[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter11",
        "title": "24.1.2 卷积",
        "content_length": 2698,
        "content": "Title: 24.1.2 卷积\nContent:\n\\section*{1. 数学卷积}\n在数学中, 卷积 (convolution) 是定义在两个函数上的运算, 表示用其中一个函数对另一个函数的形状进行的调整。这里考虑一维卷积。设 $f$ 和 $g$ 是两个可积的实值函数, 则积分\n\n$$\n\\int_{-\\infty}^{+\\infty} f(\\tau) g(t-\\tau) \\mathrm{d} \\tau\n$$\n\n定义了一个新的函数 $h(t)$, 称为 $f$ 和 $g$ 的卷积, 记作\n\n\n\\begin{equation*}\nh(t)=(f \\circledast g)(t)=\\int_{-\\infty}^{+\\infty} f(\\tau) g(t-\\tau) \\mathrm{d} \\tau \\tag{24.1}\n\\end{equation*}\n\n\n其中, 符号 $\\circledast$ 表示卷积运算。\n\n根据定义可知, 卷积满足交换律 $(f \\circledast g)(t)=(g \\circledast f)(t)$, 即有\n\n\n\\begin{equation*}\n(f \\circledast g)(t)=\\int_{-\\infty}^{+\\infty} f(t-\\tau) g(\\tau) \\mathrm{d} \\tau \\tag{24.2}\n\\end{equation*}\n\n\n例 24.1 以下是数学卷积的例子\n\n$$\ny(t)=(x \\circledast w)(t)=\\int_{-\\infty}^{+\\infty} x(\\tau) w(t-\\tau) \\mathrm{d} \\tau\n$$\n\n其中, $x(\\tau)$ 是任意给定函数, $w(t)$ 是高斯核函数。\n\n$$\nw(t)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{t^{2}}{2 \\sigma^{2}}\\right)\n$$\n\n卷积表示用高斯核函数 $w(t)$ 对给定函数 $x(\\tau)$ 进行平滑得到的结果 (见图 24.1)。\n\n数学卷积也可以自然地扩展到二维和离散的情况。具体例子参见习题。\n\n\\section*{2. 二维卷积}\n卷积神经网络中的卷积与数学卷积并不相同, 实际是数学中的互相关 (cross correlation)。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_90c63935c3d97d7235e0g-40}\n\\end{center}\n\n图 24.1 数学卷积例\n\n两个实值函数 $f$ 和 $g$ 的互相关是指\n\n\n\\begin{equation*}\n(f * g)(t)=\\int_{-\\infty}^{+\\infty} f(\\tau) g(t+\\tau) \\mathrm{d} \\tau \\tag{24.3}\n\\end{equation*}\n\n\n式中记号 $*$ 表示互相关运算。互相关不满足交换律 $(f * g)(t) \\neq(g * f)(t)$ 。可以将以上互相关自然地扩展到二维和离散的情况。\n\n卷积神经网络中的卷积一般为二维线性互相关, 用矩阵形式表示。本书称之为机器学习卷积。\n\n定义 24.1 (二维卷积) 给定一个 $I \\times J$ 输入矩阵 $\\boldsymbol{X}=\\left[x_{i j}\\right]_{I \\times J}$, 一个 $M \\times N$ 核矩阵 $\\boldsymbol{W}=\\left[w_{m n}\\right]_{M \\times N}$, 满足 $M \\ll I, N \\ll J$ 。让核矩阵在输入矩阵上从左到右再从上到下按顺序滑动, 在滑动的每一个位置, 核矩阵与输入矩阵的一个子矩阵重叠。求核矩阵与每一个子矩阵的内积, 产生一个 $K \\times L$ 输出矩阵 $\\boldsymbol{Y}=\\left[y_{k l}\\right]_{K \\times L}$, 称此运算为卷积 (convolution) 或二维卷积。写作\n\n\n\\begin{equation*}\n\\boldsymbol{Y}=\\boldsymbol{W} * \\boldsymbol{X} \\tag{24.4}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{Y}=\\left[y_{k l}\\right]_{K \\times L}$ 。\n\n\n\\begin{equation*}\ny_{k l}=\\sum_{m=1}^{M} \\sum_{n=1}^{N} w_{m, n} x_{k+m-1, l+n-1} \\tag{24.5}\n\\end{equation*}\n\n\n其中， $k=1,2, \\cdots, K, l=1,2, \\cdots, L, K=I-M+1, L=J-N+1$ 。\n\n以上是基本卷积的定义, 还有多种扩展。注意式 (24.4) 中的卷积符号是 $*, \\boldsymbol{X}$ 和 $\\boldsymbol{W}$ 的顺序是有意义的, 本书将卷积核矩阵放在前面。卷积核又被称为滤波器（filter）。\n\n比较定义式 (24.1) 和式 (24.3) 可知数学的卷积和互相关并不等价。卷积神经网络采用互相关作为 “卷积”, 主要是为了处理方便。如果数学的卷积和互相关的核矩阵都是从数据中学到, 那么效果是一样的。本书中的卷积除特别声明外均指互相关。\n\n例 24.2 给定输入矩阵 $\\boldsymbol{X}$ 和核矩阵 $\\boldsymbol{W}$ :\n\n$$\n\\boldsymbol{X}=\\left[\\begin{array}{cccc}\n3 & 2 & 0 & 1 \\\\\n0 & 2 & 1 & 2 \\\\\n2 & 0 & 0 & 3 \\\\\n2 & 3 & 1 & 2\n\\end{array}\\right], \\quad \\boldsymbol{W}=\\left[\\begin{array}{ccc}\n2 & 1 & 2 \\\\\n0 & 0 & 3 \\\\\n0 & 0 & 2\n\\end{array}\\right]\n$$\n\n求卷积 $\\boldsymbol{Y}=\\boldsymbol{W} * \\boldsymbol{X}$ 。\n\n\n\\end{CJK*}\n\\end{document}"
    },
    {
        "id": "chatcmpl-93zLELhPV9As3lka5epnFJoXKSBFx",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"数学中的卷积是如何定义的？\",\n    \"回答1\": \"在数学中, 卷积是定义在两个函数上的运算, 表示用其中一个函数对另一个函数的形状进行的调整。对于一维卷积，设 $f$ 和 $g$ 是两个可积的实值函数，则积分 $\\text{h}(t)=\\big(f \\boldsymbol{\\text{*}} g\\big)(t)=\\bigintsss_{-\\text{\\text{\\tiny{$\\boldsymbol{-}$}}} \\text{\\tiny{$\\boldsymbol{\\text{\\text{∞}}}$}}}^{\\text{\\tiny{$\\boldsymbol{+}$}}\\text{\\tiny{$\\boldsymbol{\\text{∞}}$}}} f(\\tau) g(t-\\tau) \\text{d} \\tau$ 定义了一个新的函数 $h(t)$，称为 $f$ 和 $g$ 的卷积。\",\n    \"问题2\": \"卷积满足什么性质？\",\n    \"回答2\": \"根据定义可知，卷积满足交换律，即 $(f \\boldsymbol{\\text{*}} g)(t)=(g \\boldsymbol{\\text{*}} f)(t)$。这意味着卷积操作不受函数顺序的影响，可以交换顺序而得到相同的结果。\",\n    \"问题3\": \"卷积和互相关有何区别？\",\n    \"回答3\": \"数学中的卷积和互相关并不等价。卷积神经网络采用互相关作为 “卷积”，主要是为了处理方便。如果数学的卷积和互相关的核矩阵都是从数据中学到，那么效果是一样的。本书中的卷积除特别声明外均指互相关。\",\n    \"问题4\": \"卷积神经网络中的卷积是如何定义的？\",\n    \"回答4\": \"卷积神经网络中的卷积一般为二维线性互相关，用矩阵形式表示。给定一个 $I \\times J$ 输入矩阵 $\\boldsymbol{X}$ 和一个 $M \\times N$ 核矩阵 $\\boldsymbol{W}$，满足 $M \\text{远小于} I, N \\text{远小于} J$。让核矩阵在输入矩阵上从左到右再从上到下按顺序滑动，在滑动的每一个位置，核矩阵与输入矩阵的一个子矩阵重叠。求核矩阵与每一个子矩阵的内积，产生一个 $K \\times L$ 输出矩阵 $\\boldsymbol{Y}$，称此运算为卷积或二维卷积。\",\n    \"问题5\": \"二维卷积的计算公式是什么？\",\n    \"回答5\": \"二维卷积的计算公式为 $\\boldsymbol{Y}=\\boldsymbol{W} * \\boldsymbol{X}$，其中 $y_{k l}=\\bigintsss_{m=1}^{M} \\bigintsss_{n=1}^{N} w_{m, n} x_{k+m-1, l+n-1}$，其中 $k=1,2, \\text{...}, K, l=1,2, \\text{...}, L, K=I-M+1, L=J-N+1$。\",\n    \"问题6\": \"卷积核在卷积神经网络中又被称为什么？\",\n    \"回答6\": \"在卷积神经网络中，卷积核又被称为滤波器（filter）。它们用来提取输入数据中的特征，并通过与输入数据进行卷积操作来生成输出特征图。\",\n    \"问题7\": \"为什么数学中的卷积和互相关并不等价？\",\n    \"回答7\": \"数学中的卷积和互相关并不等价，但在卷积神经网络中采用互相关作为 “卷积”，主要是为了处理方便。如果数学的卷积和互相关的核矩阵都是从数据中学到，那么效果是一样的。\",\n    \"问题8\": \"二维卷积与一维卷积有何区别？\",\n    \"回答8\": \"二维卷积与一维卷积相比，涉及到了二维输入矩阵和核矩阵，并且需要对输入矩阵进行二维滑动计算。而一维卷积则是针对一维函数进行操作。\",\n    \"问题9\": \"如何理解二维卷积中核矩阵与输入矩阵的内积运算？\",\n    \"回答9\": \"在二维卷积中，核矩阵与输入矩阵的内积运算表示了核矩阵在输入矩阵上滑动时与每一个子矩阵进行内积运算，从而生成输出矩阵。这个过程类似于对输入数据进行特征提取和信息融合。\",\n    \"问题10\": \"为什么在卷积神经网络中采用互相关作为 “卷积”？\",\n    \"回答10\": \"在卷积神经网络中采用互相关作为 “卷积”，主要是为了处理方便。如果数学的卷积和互相关的核矩阵都是从数据中学到，那么效果是一样的。因此，在实际应用中采用互相关能够简化计算并提高效率。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710737808,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1543,
            "prompt_tokens": 4279,
            "total_tokens": 5822
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]