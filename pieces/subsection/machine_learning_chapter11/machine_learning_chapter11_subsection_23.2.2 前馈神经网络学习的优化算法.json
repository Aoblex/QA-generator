[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter11",
        "title": "23.2.2 前馈神经网络学习的优化算法",
        "content_length": 4779,
        "content": "Title: 23.2.2 前馈神经网络学习的优化算法\nContent:\n\\section*{1. 非凸优化问题}\n前馈神经网络学习变成给定网络架构 $f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$ 、训练数据集 $\\mathcal{T}$ 的条件下, 最小化目标函数 $L(\\boldsymbol{\\theta})$, 得到最优参数 $\\hat{\\boldsymbol{\\theta}}$ 的优化问题 (最小化问题)。\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}} L(\\boldsymbol{\\theta})=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} L\\left(f\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right), y_{i}\\right) \\tag{23.37}\n\\end{equation*}\n\n\n前馈神经网络学习的目标函数一般是非凸函数, 优化问题是非凸优化。从前馈神经网络的等价性可以得知, 一个神经网路通常有大量等价的参数向量, 所以其学习的优化问题有大量等价的局部最优点 (最小点)。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_90c63935c3d97d7235e0g-14}\n\\end{center}\n\n图 23.18 非凸优化问题 (见文前彩图)\n\n图 23.18 示意了神经网络的非凸优化问题。参数向量是 $\\left(\\theta_{1}, \\theta_{2}\\right)$, 目标函数是 $L\\left(\\theta_{1}, \\theta_{2}\\right)$,全局最小点是深蓝色, 局部最小点是浅蓝色。因为目标函数非凸, 有许多局部最小点。\n\n\\section*{2. 梯度下降法和随机梯度下降法}\n深度学习包括前馈神经网络学习, 均使用迭代优化算法, 包括梯度下降法 (gradient descent) 和随机梯度下降法 (stochastic gradient descent), 后者更为常用 (附录 A 给出梯度下降法的一般介绍)。\n\n优化目标函数写作\n\n\n\\begin{equation*}\nL(\\boldsymbol{\\theta})=\\frac{1}{N} \\sum_{i=1}^{N} L_{i}(\\boldsymbol{\\theta})=\\frac{1}{N} \\sum_{i=1}^{N} L\\left(f\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right), y_{i}\\right) \\tag{23.38}\n\\end{equation*}\n\n\n其中, $L_{i}(\\boldsymbol{\\theta})$ 是第 $i$ 个样本的损失函数。\n\n梯度下降法首先随机初始化参数向量 $\\boldsymbol{\\theta}$; 之后针对所有样本, 通过以下公式更新参数向量 $\\boldsymbol{\\theta}$; 不断迭代, 直到收玫为止。\n\n\n\\begin{equation*}\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\frac{\\partial L(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\tag{23.39}\n\\end{equation*}\n\n\n或写作\n\n\n\\begin{equation*}\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial L_{i}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\tag{23.40}\n\\end{equation*}\n\n\n其中, $\\eta>0$ 是学习率, $\\frac{\\partial L(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}$ 是所有样本的损失函数的梯度向量, $\\frac{\\partial L_{i}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}$ 是第 $i$ 个样本的损失函数的梯度向量。算法 23.1 给出梯度下降的具体算法。\n\n梯度下降的基本想法如下。由于负梯度方向 $-\\frac{\\partial L(\\theta)}{\\partial \\theta}$ 是使函数值下降的方向, 所以每一次迭代以负梯度更新参数向量 $\\boldsymbol{\\theta}$ 的值, 从而达到减少函数值 $L(\\boldsymbol{\\theta})$ 的目的。函数极小值满足 $\\nabla L(\\boldsymbol{\\theta})=\\boldsymbol{O}$ 。在迭代过程中, 梯度向量趋近 $\\boldsymbol{O}$ 向量, 参数向量 $\\boldsymbol{\\theta}$ 也趋近极小点。学习率控制参数更新的幅度。学习率的大小需要适当, 学习率过小, 参数向量每次更新的幅度会过小, 迭代的次数会增加; 学习率过大, 参数向量每次更新的幅度会过大, 产生振荡, 迭代的次数也会增加。图 23.18 显示梯度下降的过程。\n\n\\section*{算法 23.1 (梯度下降法)}\n输入: 网络架构 $f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$, 训练数据集 $\\mathcal{T}$ 。\n\n输出: 神经网络参数向量 $\\hat{\\boldsymbol{\\theta}}$ 。\n\n超参数: 学习率 $\\eta$ 。\n\n\\begin{enumerate}\n  \\item 随机初始化参数向量 $\\boldsymbol{\\theta} ;$\n\n  \\item Do while ( $\\boldsymbol{\\theta}$ 不收敛) \\{\n\n\\end{enumerate}\n\n针对所有样本, 按照以下公式, 更新参数向量 $\\boldsymbol{\\theta}$\n\n$$\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\frac{\\partial L(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n$$\n\n\\}\n\n\\begin{enumerate}\n  \\setcounter{enumi}{2}\n  \\item 返回学习到的参数向量 $\\hat{\\boldsymbol{\\theta}}$ 。\n\\end{enumerate}\n\n梯度下降用于深度学习 (一般的机器学习) 有两个缺点。每次迭代需要计算针对所有样本的梯度向量, 计算效率不高; 得到的解可能是局部最优, 不能保证是全局最优。随机梯度下\\\\\n降能很好地解决这两个问题。\n\n随机梯度下降法首先随机打乱样本顺序, 将样本分成 $m$ 个组 (小批量), 每一组有 $n$ 个样本 (假设 $m=\\lfloor N / n\\rfloor$ ); 接着随机初始化参数向量; 之后针对每组样本, 通过以下公式更新参数向量, 并遍历所有样本组; 不断迭代, 直到收玫为止。\n\n\n\\begin{equation*}\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\frac{1}{n} \\sum_{j=1}^{n} \\frac{\\partial L_{j}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} \\tag{23.41}\n\\end{equation*}\n\n\n其中, $\\eta>0$ 是学习率, $\\frac{\\partial L_{j}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}$ 是一个组中的第 $j$ 个样本的损失函数的梯度向量。算法 23.2 给出随机梯度下降的具体算法。当 $n$ 是 1 时, 每次参数更新只使用一个样本, 是一种特殊的随机梯度下降。当 $n$ 是整体样本容量 $N$ 时, 随机梯度下降变为梯度下降（当前深度学习采用的 Adam 等优化算法, 在随机梯度下降的迭代过程中, 自适应地调整梯度向量。第 29 章对 Adam 等算法予以介绍)。\n\n\\section*{算法 23.2 (随机梯度下降法)}\n输入: 网络架构 $f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$, 训练数据集 $\\mathcal{T}$ 。\n\n输出: 神经网络参数向量 $\\hat{\\boldsymbol{\\theta}}$ 。\n\n超参数: 学习率 $\\eta$, 小批量样本容量 $n$ 。\n\n\\begin{enumerate}\n  \\item 随机打乱样本顺序, 将样本分成 $m$ 个组, 每一组有 $n$ 个样本;\n\n  \\item 随机初始化参数向量 $\\boldsymbol{\\theta}$;\n\n  \\item Do while ( $\\boldsymbol{\\theta}$ 不收敛) \\{\n\n\\end{enumerate}\n\nFor $(i=1,2, \\cdots, m)\\{$\n\n针对第 $i$ 个组的 $n$ 个样本, 按照以下公式, 更新参数向量 $\\boldsymbol{\\theta}$\n\n$$\n\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}-\\eta \\frac{1}{n} \\sum_{j=1}^{n} \\frac{\\partial L_{j}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_90c63935c3d97d7235e0g-16}\n\\end{center}\n\n\\begin{enumerate}\n  \\setcounter{enumi}{3}\n  \\item 返回学习到的参数向量 $\\hat{\\boldsymbol{\\theta}}$ 。\n\\end{enumerate}\n\n随机梯度下降可以进行分布式并行计算, 进一步提高学习效率, 特别是当训练数据量大时非常有效。具体地, 每组样本分配到不同的工作服务器 (worker) 上, 各台工作服务器基于自己的数据并行更新参数向量, 参数服务器 (parameter server) 再将所有工作服务器的参数更新结果汇总求平均, 得到一轮的训练结果。"
    },
    {
        "id": "chatcmpl-93z8FrThWMlvRs8HhYe6q2NB0DNX3",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"前馈神经网络学习的优化问题是什么？\",\n    \"回答1\": \"前馈神经网络学习的优化问题是在给定网络架构 $f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$ 和训练数据集 $\\text{𝒯}$ 的条件下，最小化目标函数 $L(\\boldsymbol{\\theta})$，从而得到最优参数 $\\boldsymbol{\\theta}$ 的非凸优化问题。\",\n    \"问题2\": \"前馈神经网络学习的目标函数一般是什么类型的函数？\",\n    \"回答2\": \"前馈神经网络学习的目标函数一般是非凸函数，因此其优化问题属于非凸优化。这意味着目标函数具有多个局部最小点，使得优化过程更加复杂。\",\n    \"问题3\": \"梯度下降法的基本思想是什么？\",\n    \"回答3\": \"梯度下降法的基本思想是通过负梯度方向 $-\\frac{\\nabla L(\\boldsymbol{\\theta})}{\\nabla \\boldsymbol{\\theta}}$ 来更新参数向量 $\\boldsymbol{\\theta}$ 的值，从而使目标函数值 $L(\\boldsymbol{\\theta})$ 不断减小。在迭代过程中，梯度向量逐渐趋近于零向量，参数向量也逐渐趋近于极小点。\",\n    \"问题4\": \"梯度下降法中的学习率起到什么作用？\",\n    \"回答4\": \"学习率控制了参数更新的幅度，即每次迭代中参数向量更新的步长。学习率大小的选择需要适当，过小会导致收敛速度缓慢，而过大则可能导致振荡或错过最优解。\",\n    \"问题5\": \"随机梯度下降法相比梯度下降法有什么优势？\",\n    \"回答5\": \"随机梯度下降法相比梯度下降法具有两个主要优势：首先，它提高了计算效率，因为每次迭代只需计算针对一个样本或一个小批量样本的梯度；其次，它能够更好地避免局部最优解，并且可以进行分布式并行计算，特别适用于大规模训练数据。\",\n    \"问题6\": \"随机梯度下降法中的小批量样本容量 $n$ 的选择有何影响？\",\n    \"回答6\": \"小批量样本容量 $n$ 的选择会影响参数更新的频率和稳定性。较小的 $n$ 可能会引入更多的随机性，但也可能导致参数更新的不稳定性；而较大的 $n$ 则可能减缓收敛速度，并且可能无法充分利用并行计算的优势。\",\n    \"问题7\": \"随机梯度下降法如何进行分布式并行计算？\",\n    \"回答7\": \"随机梯度下降法可以进行分布式并行计算，其中每组样本分配到不同的工作服务器上进行并行更新参数向量，然后参数服务器将所有工作服务器的结果汇总求平均，从而得到一轮的训练结果。这种方法能够进一步提高学习效率，特别适用于大规模训练数据。\",\n    \"问题8\": \"深度学习中梯度下降法存在哪些缺点？\",\n    \"回答8\": \"深度学习中梯度下降法存在两个主要缺点：首先，每次迭代需要计算针对所有样本的梯度向量，计算效率较低；其次，得到的解可能是局部最优解，不能保证是全局最优解。这些缺点在随机梯度下降法中得到一定程度的缓解。\",\n    \"问题9\": \"为什么随机梯度下降法更为常用？\",\n    \"回答9\": \"随机梯度下降法更为常用主要是因为它提高了计算效率，并且能够更好地避免局部最优解。此外，随机梯度下降法还可以进行分布式并行计算，在大规模训练数据时表现出较好的性能。\",\n    \"问题10\": \"前馈神经网络学习中如何选择合适的学习率？\",\n    \"回答10\": \"在前馈神经网络学习中，选择合适的学习率需要进行实验和调参。通常需要根据具体任务和数据特征来调整学习率大小，以保证参数更新的稳定性和收敛速度。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710737003,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1367,
            "prompt_tokens": 5291,
            "total_tokens": 6658
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]