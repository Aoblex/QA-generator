[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter11",
        "title": "23.1.3 前馈神经网络的表示能力",
        "content_length": 5791,
        "content": "Title: 23.1.3 前馈神经网络的表示能力\nContent:\n\\section*{1. 与其他模型的关系}\n前馈神经网络与逻辑斯谛回归模型、感知机、支持向量机等有密切关系。\n\n对于多类分类的一层神经网络, 当其输出层激活函数是软最大化函数时, 模型等价于多项逻辑斯谛回归模型。\n\n\n\\begin{equation*}\nP\\left(y_{k}=1 \\mid \\boldsymbol{x}\\right)=f(\\boldsymbol{x})=\\frac{\\mathrm{e}^{\\left(\\boldsymbol{w}_{k}^{(1) \\mathrm{T}} \\boldsymbol{x}+b_{k}^{(1)}\\right)}}{\\sum_{i=1}^{l} \\mathrm{e}^{\\left(\\boldsymbol{w}_{i}^{(1)^{\\mathrm{T}}} \\boldsymbol{x}+b_{i}^{(1)}\\right)}}, \\quad k=1,2, \\cdots, l \\tag{23.26}\n\\end{equation*}\n\n\n所以, 前馈神经网络是逻辑斯谛回归模型的扩展。注意: 前馈神经网络通常将所有 $l$ 个类别的权重和偏置作为参数使用, 而逻辑斯谛回归通常将前 $l-1$ 个类别的权重和偏置作为自由参数使用 (见6.1.4 节)。\n\n对于二类分类的一层神经网络，当其输出层激活函数是双曲正切函数时，\n\n\n\\begin{equation*}\nf(\\boldsymbol{x})=\\tanh \\left(\\boldsymbol{w}^{(1)^{\\mathrm{T}}} \\boldsymbol{x}+b^{(1)}\\right) \\tag{23.27}\n\\end{equation*}\n\n\n模型可以与感知机对应。感知机模型的定义是\n\n$$\ny= \\begin{cases}+1, & \\operatorname{sign}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}+b\\right) \\geqslant 0 \\\\ -1, & \\text { 其他 }\\end{cases}\n$$\n\n所以, 可以认为前馈神经网络是感知机的扩展。这也是前馈神经网络又被称为多层感知机的原因。\n\n对于二类分类的多层神经网络，当其输出层激活函数是双曲正切函数时,\n\n\n\\begin{align*}\n& f(\\boldsymbol{x})=\\tanh \\left(\\boldsymbol{w}^{(s)^{\\mathrm{T}}} \\boldsymbol{h}^{(s-1)}+b^{(s)}\\right)  \\tag{23.28}\\\\\n& \\boldsymbol{h}^{(s-1)}=f^{(s-1)}\\left[\\cdots f^{(2)}\\left(f^{(1)}(\\boldsymbol{x})\\right) \\cdots\\right]\n\\end{align*}\n\n\n模型可以与非线性支持向量机对应。非线性支持向量模型的定义是\n\n$$\ny= \\begin{cases}+1, & \\operatorname{sign}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\phi(\\boldsymbol{x})+b\\right) \\geqslant 0 \\\\ -1, & \\text { 其他 }\\end{cases}\n$$\n\n其中, $\\phi(x)$ 是从输入空间到特征空间的非线性映射, $\\boldsymbol{w}$ 和 $b$ 是模型的参数。前馈神经网络的前 $s-1$ 层函数 $f^{(s-1)}\\left[\\cdots f^{(2)}\\left(f^{(1)}(\\boldsymbol{x})\\right) \\cdots\\right]$ 与映射函数 $\\phi(\\boldsymbol{x})$ 对应。支持向量机学习是凸优化问题, 保证可以找到全局最优, 而前馈神经网络学习是非凸优化问题 (见 23.2 节), 不能保证找到全局最优。前馈神经网络比支持向量机有更多的参数可以调节。\n\n\\section*{2. 函数近似能力}\n前馈神经网络具有强大的函数近似能力。通用近似定理 (universal approximation theorem) 指出, 存在一个二层前馈神经网络, 具有一个线性输出层和一个隐层, 其中隐层含\\\\\n有充分数量的神经元, 激活函数为挤压函数, 这个网络可以以任意精度近似任意一个在紧的定义域上的连续函数 ${ }^{[7-8]}$ 。从这个意义上, 前馈神经网络的函数近似能力是通用的。\n\n设有实函数 $G(x): \\mathcal{R} \\rightarrow[0,1]$, 如果 $G(x)$ 是非减函数, 且满足 $\\lim _{x \\rightarrow-\\infty} G(x)=0, \\lim _{x \\rightarrow+\\infty} G(x)=$ 1 , 则称函数 $G(x)$ 为挤压函数 (squashing function)。 $\\mathrm{S}$ 型函数是一种挤压函数。\n\n后续理论研究发现, 定理的条件可以放宽, 当激活函数是多项式函数以外的其他函数时,或者当被近似函数是波莱尔可测函数时, 定理的结论依然成立。波莱尔可测函数包括连续函数、分段连续函数、阶梯函数。\n\n下面的定理是通用近似定理的一个具体形式。\n\n定理 23.1 对任意连续函数 $h:[0,1]^{n} \\rightarrow \\mathcal{R}$ 和任意 $\\varepsilon>0$, 存在一个二层前馈神经网络:\n\n$$\n\\begin{aligned}\nf(\\boldsymbol{x}) & =\\boldsymbol{\\alpha}^{\\mathrm{T}} \\sigma\\left(\\boldsymbol{W}^{\\mathrm{T}} \\boldsymbol{x}+\\boldsymbol{b}\\right) \\\\\n& =\\sum_{j} \\alpha_{j} \\sigma\\left(\\sum_{i} w_{j i} x_{i}+b_{j}\\right)\n\\end{aligned}\n$$\n\n使得对于任意 $x \\in[0,1]^{n}$, 有 $|\\boldsymbol{h}(x)-f(\\boldsymbol{x})|<\\varepsilon$ 成立。这里隐层的激活函数是 $\\mathrm{S}$ 型函数。\n\n下面给出定理 23.1 的直观解释。假设 $h(x)$ 是一个连续函数, 定义域是区间 $[0,1]$, 值域是区间 $[0,1]$, 可以用二层前馈神经网络 $f(x)$ 以任意精度近似 $h(x)$ 。(为了简单, 这里假设 $h(x)$ 取正值, 也可以取负值）\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_90c63935c3d97d7235e0g-10}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_90c63935c3d97d7235e0g-10(1)}\n\\end{center}\n\n(b)\n\n图 23.15 用阶梯函数近似连续函数\n\n如图 23.15(a) 所示, 可以用阶梯函数以任意精度近似函数 $h(x)$ 。如图 23.15(b) 所示, 假设阶梯函数第 $i$ 个分段函数是\n\n$$\ns_{i}(x)= \\begin{cases}\\alpha_{i}, & x_{i-1}<x \\leqslant x_{i} \\\\ 0, & \\text { 其他 }\\end{cases}\n$$\n\n则该分段函数可以由以下二层神经网络近似。\n\n$$\nf_{i}(x)=\\alpha_{i} \\cdot \\sigma\\left(w \\cdot x-x_{i-1}\\right)-\\alpha_{i} \\cdot \\sigma\\left(w \\cdot x-x_{i}\\right)\n$$\n\n其中隐层有两个神经元, 其激活函数是 $\\mathrm{S}$ 型函数, 输出层是线性的。这里参数 $x_{i-1}$ 和 $x_{i}$ 保证与分段函数的区间一致, 参数 $\\alpha_{i}$ 保证趋近分段函数, 参数 $w$ 控制与分段函数的趋近程度。这样, 阶梯函数的每一分段函数 $s_{i}(x)$ 都可以用一个二层神经网络 $f_{i}(x)$ 近似, 函数 $h(x)$ 整体也可以由所有 $f_{i}(x)$ 相加得到的二层神经网络 $f(x)$ 近似。\\\\\n通用近似定理叙述的是理论存在性, 并不意味着现实可行性。定理 23.1 中近似连续函数的二层前馈神经网络的隐层神经元的个数可能是非常大的, 甚至是指数级的, 参数个数也是如此, 现实中不会有足够多的数据训练这样的网络。经验上, 当前馈神经网络的层数增大时,也就是变成深度神经网络时, 可以解决这个问题。\n\n\\section*{3. 函数等价性}\n前馈神经网络 $\\boldsymbol{y}=f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$ 有大量的等价的函数, 即参数 $\\boldsymbol{\\theta}$ 不同但对相同的输入 $\\boldsymbol{x}$ 产生相同的输出 $\\boldsymbol{y}$, 而且等价函数的个数是指数级的。\n\n假设某个隐层有 $m$ 个神经元, 其所有参数由向量表示。这一层有 $m$ ! 个神经元的排列,每一个排列决定一个参数向量, 因此有 $m$ ! 种不同的参数向量。改变神经元的排列, 参数向量发生变化, 但神经网络的输入输出的映射关系不变, 这时隐层有 $m$ ! 个等价的参数向量。\n\n假设某个隐层有 $m$ 个神经元, 其所有参数由向量表示, 激活函数是双曲正切函数。双曲正切函数是奇函数, 即满足 $\\tanh (-z)=-\\tanh (z)$ 。若这一层的某一个神经元的参数以及相连的后一层神经元的参数都反号, 则对相同的输入, 神经网络的输出不变。这时隐层 (与后一层一起）有 $2^{m}$ 个等价的参数向量。如果同时考虑神经元的不同排列, 这个隐层共有 $m ! 2^{m}$ 个等价的参数向量。\n\n当神经网络有多个隐层的时候, 整体的等价函数的个数由各层的等价参数向量个数的乘积决定。\n\n\\section*{4. 网络的深度}\n前馈神经网络的深度指网络的层数, 复杂度指神经元的个数。复杂度也代表神经网络的参数个数, 因为参数个数与神经元个数成正比。深度神经网络与 “浅度神经网络” 可以有同等的表示能力, 但深度神经网络比浅度神经网络有更低的复杂度。这一点可以由逻辑门电路理论间接论证。\n\n定理 23.2 存在这样的布尔函数, 可以由深度为 $k$ 的多项式复杂度的逻辑门电路表示,等价地深度为 $k-1$ 的逻辑门电路变为指数复杂度。其中深度指从输入到输出的最长路径的长度, 复杂度指逻辑门的个数。\n\n这里给出定理的直观解释。假设有图 23.16 所示的逻辑门电路, 其深度是 3 , 复杂度是 9 。如果根据以下关系, 将 AND 的 OR 子电路 (可由合取范式表示) 转换成等价的 OR 的 AND 子电路 (可由析取范式表示):\n\n$$\n\\begin{aligned}\n\\left(x_{1}+\\bar{x}_{2}\\right)\\left(\\bar{x}_{3}+x_{4}\\right)\\left(x_{5}+x_{6}\\right)= & x_{1} \\bar{x}_{3} x_{5}+x_{1} \\bar{x}_{3} x_{6}+x_{1} x_{4} x_{5}+x_{1} x_{4} x_{6}+ \\\\\n& \\bar{x}_{2} \\bar{x}_{3} x_{5}+\\bar{x}_{2} \\bar{x}_{3} x_{6}+\\bar{x}_{2} x_{4} x_{5}+\\bar{x}_{2} x_{4} x_{6}\n\\end{aligned}\n$$\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_90c63935c3d97d7235e0g-11}\n\\end{center}\n\n图 23.16 逻辑门电路例\\\\\n那么得到图 23.17 所示的等价的逻辑门电路, 其深度是 2 , 复杂度是 17 。深的逻辑门电路复杂度低, 而等价的浅的逻辑门电路复杂度高。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_90c63935c3d97d7235e0g-12}\n\\end{center}\n\n图 23.17 等价的逻辑门电路\n\n逻辑门电路可以由前馈神经网络表示。定理 23.2 的推论是存在这样的深度神经网络, 其等价的浅度神经网络的复杂度指数级地增加。所以, 虽然深度神经网络与浅度神经网络可能有同等表示能力, 但复杂度过高的浅度神经网络现实中并不可取。\n\n在同等表示能力下, 深度神经网络比浅度神经网络有更少的参数。所以, 只需要更少的数据就可以学到, 也就是说, 深度神经网络有更低的样本复杂度 (sample complexity)。这也是深度神经网络更加强大且实用的原因。\n\n\\section*{23.2 前馈神经网络的学习算法}\n本节讲述前馈神经网络学习算法。首先给出学习问题的定义, 之后介绍学习的算法, 包括一般的随机梯度下降法和具体的反向传播算法, 以及在计算图上的实现, 最后介绍学习的技巧。"
    },
    {
        "id": "chatcmpl-93z7bphoGQkmMVmvK5x5QhSGezpkf",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"前馈神经网络与逻辑斯谛回归模型、感知机、支持向量机等模型有何密切关系？\",\n    \"回答1\": \"前馈神经网络与逻辑斯谛回归模型、感知机、支持向量机等有密切关系。具体来说，前馈神经网络可以看作是逻辑斯谛回归模型的扩展，也被称为多层感知机，同时也可以与感知机和支持向量机对应。\",\n    \"问题2\": \"前馈神经网络的函数近似能力如何？\",\n    \"回答2\": \"前馈神经网络具有强大的函数近似能力。通用近似定理指出，存在一个二层前馈神经网络，具有一个线性输出层和一个隐层，其中隐层含有充分数量的神经元，激活函数为挤压函数，这个网络可以以任意精度近似任意一个在紧的定义域上的连续函数。\",\n    \"问题3\": \"前馈神经网络的参数空间中存在哪些等价的函数？\",\n    \"回答3\": \"前馈神经网络的参数空间中存在大量的等价的函数，即参数不同但对相同的输入产生相同的输出。这种等价函数的个数是指数级的，这一特性使得前馈神经网络具有较强的灵活性和表达能力。\",\n    \"问题4\": \"前馈神经网络的深度和复杂度之间有何关系？\",\n    \"回答4\": \"前馈神经网络的深度指网络的层数，复杂度代表神经元的个数和参数个数。深度神经网络与 “浅度神经网络” 可以有同等的表示能力，但深度神经网络比浅度神经网络有更低的复杂度。这意味着在同等表示能力下，深度神经网络比浅度神经网络有更少的参数和更低的样本复杂度。\",\n    \"问题5\": \"通用近似定理是如何解释前馈神经网络的函数近似能力？\",\n    \"回答5\": \"通用近似定理叙述了存在一个二层前馈神经网络，具有一个线性输出层和一个隐层，其中隐层含有充分数量的神经元，激活函数为挤压函数，这个网络可以以任意精度近似任意一个在紧的定义域上的连续函数。这一定理直观地解释了前馈神经网络具有强大的函数近似能力。\",\n    \"问题6\": \"前馈神经网络学习算法中包括哪些内容？\",\n    \"回答6\": \"前馈神经网络学习算法包括学习问题的定义、学习算法（如一般的随机梯度下降法和具体的反向传播算法）、在计算图上的实现以及学习的技巧。\",\n    \"问题7\": \"通用近似定理对于现实中的前馈神经网络有何影响？\",\n    \"回答7\": \"通用近似定理叙述了理论上存在一个二层前馈神经网络可以以任意精度近似任意一个在紧的定义域上的连续函数。然而，在现实中，这种理论存在性并不意味着现实可行性。因此，深度神经网络在实际应用中更加实用。\",\n    \"问题8\": \"为什么深度神经网络比浅度神经网络更加强大且实用？\",\n    \"回答8\": \"在同等表示能力下，深度神经网络比浅度神经网络有更少的参数和更低的样本复杂度。这使得深度神经网络具有更强大且更实用。\",\n    \"问题9\": \"前馈神经网络与逻辑门电路之间存在怎样的关系？\",\n    \"回答9\": \"逻辑门电路可以由前馈神经网络表示。定理 23.2 指出了存在这样的深度神经网络，其等价的浅度神经网络的复杂度指数级地增加。因此，虽然深度神经网络与浅度神经网络可能有同等表示能力，但复杂度过高的浅度神经网络现实中并不可取。\",\n    \"问题10\": \"通用近似定理如何解释了前馈神经网络具有强大的函数近似能力？\",\n    \"回答10\": \"通用近似定理叙述了存在一个二层前馈神经网络，具有一个线性输出层和一个隐层，其中隐层含有充分数量的神经元，激活函数为挤压函数，这个网络可以以任意精度近似任意一个在紧的定义域上的连续函数。这一定理直观地解释了前馈神经网络具有强大的函数近似能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710736963,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1456,
            "prompt_tokens": 6617,
            "total_tokens": 8073
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]