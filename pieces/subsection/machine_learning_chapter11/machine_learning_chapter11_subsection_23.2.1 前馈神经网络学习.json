[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter11",
        "title": "23.2.1 前馈神经网络学习",
        "content_length": 4226,
        "content": "Title: 23.2.1 前馈神经网络学习\nContent:\n\\section*{1. 一般形式}\n前馈神经网络的学习和预测 ${ }^{1}$ 是如下的监督学习问题。给定训练数据集\n\n$$\n\\mathcal{T}=\\left\\{\\left(\\boldsymbol{x}_{1}, y_{1}\\right),\\left(\\boldsymbol{x}_{2}, y_{2}\\right), \\cdots,\\left(\\boldsymbol{x}_{N}, y_{N}\\right)\\right\\}\n$$\n\n其中, $\\left(\\boldsymbol{x}_{i}, y_{i}\\right), i=1,2, \\cdots, N$, 表示样本, 由输入 $\\boldsymbol{x}_{i}$ 与输出 $y_{i}$ 的对组成; $N$ 表示样本容量。学习一个前馈神经网络模型 $f(\\boldsymbol{x} ; \\hat{\\boldsymbol{\\theta}})$, 其中 $\\hat{\\boldsymbol{\\theta}}$ 是估计的神经网络的参数向量。用学到的模型对新的输入 $\\boldsymbol{x}_{N+1}$ 给出新的输出 $y_{N+1}$ 。\n\n学习时通常假设神经网络的架构已经确定, 包括网络的层数、每层的神经元数、神经元激活函数的类型。所以网络的参数已确定, 需要从数据中学习或估计的是参数值。\n\n学习问题可以形式化为以下的优化问题:\n\\footnotetext{(1) 机器学习中的训练和预测在深度学习中习惯上称为训练和推理。\n}\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}}\\left[\\sum_{i=1}^{N} L\\left(f\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right), y_{i}\\right)+\\lambda \\cdot \\Omega(f)\\right] \\tag{23.29}\n\\end{equation*}\n\n\n其中, $L(\\cdot)$ 是损失函数, $\\Omega(\\cdot)$ 是正则项, $\\lambda \\geqslant 0$ 是系数。当损失函数是对数损失函数、没有正则化时, 问题变成极大似然估计。这是前馈神经网络学习的一般形式。\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}}\\left[-\\sum_{i=1}^{N} \\log P_{\\boldsymbol{\\theta}}\\left(y_{i} \\mid \\boldsymbol{x}_{i}\\right)\\right] \\tag{23.30}\n\\end{equation*}\n\n\n这里 $\\boldsymbol{P}_{\\boldsymbol{\\theta}}(y \\mid \\boldsymbol{x})$ 表示输入 $\\boldsymbol{x}$ 给定条件下输出 $y$ 的条件概率, 由神经网络决定; $\\boldsymbol{\\theta}$ 是神经网络的参数。\n\n\\section*{2. 具体形式}\n针对不同的问题, 前馈神经网络学习的一般形式可以转化成不同的具体形式。\n\n当问题是回归时, 模型的输入是实数向量 $\\boldsymbol{x}$, 输出是实数值 $y$ 。神经网络 $f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$ 决定输入给定条件下输出的条件概率分布 $P_{\\boldsymbol{\\theta}}(y \\mid \\boldsymbol{x})$ 。假设条件概率分布 $P_{\\boldsymbol{\\theta}}(y \\mid \\boldsymbol{x})$ 遵循高斯分布:\n\n$$\nP_{\\boldsymbol{\\theta}}(y \\mid \\boldsymbol{x}) \\sim N\\left(f(\\boldsymbol{x} ; \\boldsymbol{\\theta}), \\sigma^{2}\\right)\n$$\n\n其中, $y \\in(-\\infty,+\\infty), f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$ 是均值, $\\sigma^{2}$ 是方差。学习问题 (极大似然估计) 变为优化问题:\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}}\\left[\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{N}\\left(y_{i}-f\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right)\\right)^{2}+N \\log \\sigma+\\frac{N}{2} \\log 2 \\pi\\right] \\tag{23.31}\n\\end{equation*}\n\n\n假设方差 $\\sigma^{2}$ 固定不变, 有等价的优化问题:\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}} \\sum_{i=1}^{N} \\frac{1}{2}\\left(y_{i}-f\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right)\\right)^{2} \\tag{23.32}\n\\end{equation*}\n\n\n从另一个角度看, 前馈神经网络用于回归时, 使用平方损失 (square loss) 作为损失函数, 学习进行的是平方损失的最小化。\n\n当问题是二类分类时, 模型的输入是实数向量 $\\boldsymbol{x}$, 输出是类别 $y \\in\\{0,1\\}$, 神经网络 $f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$ 决定输入给定条件下类别的条件概率分布:\n\n\n\\begin{equation*}\np=P_{\\boldsymbol{\\theta}}(y=1 \\mid \\boldsymbol{x})=f(\\boldsymbol{x} ; \\boldsymbol{\\theta}) \\tag{23.33}\n\\end{equation*}\n\n\n假设条件概率分布 $P_{\\boldsymbol{\\theta}}(y=1 \\mid \\boldsymbol{x})$ 遵循贝努利分布, 学习问题 (极大似然估计) 变为优化问题:\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}}\\left\\{-\\sum_{i=1}^{N}\\left[y_{i} \\log f(\\boldsymbol{x} ; \\boldsymbol{\\theta})+\\left(1-y_{i}\\right) \\log (1-f(\\boldsymbol{x} ; \\boldsymbol{\\theta}))\\right]\\right\\} \\tag{23.34}\n\\end{equation*}\n\n\n这时损失函数是交叉熵 (cross entropy) 损失。离散分布的交叉嫡的一般定义是 $-\\sum_{k=1}^{l} P_{k} \\log Q_{k}$, 表示经验分布和预测分布的差异, 其中 $Q_{k}$ 是预测分布的概率, $P_{k}$是经验分布的概率。\\\\\n当问题是多类分类时, 模型的输入是实数向量 $\\boldsymbol{x}$, 输出是类别 $y_{k} \\in\\{0,1\\}, k=1,2, \\cdots, l$, $\\sum_{k=1}^{l} y_{k}=1$, 神经网络 $f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$ 表示输入给定条件下类别的条件概率分布:\n\n\n\\begin{equation*}\n\\boldsymbol{p}=P_{\\boldsymbol{\\theta}}\\left(y_{k}=1 \\mid \\boldsymbol{x}\\right)=f(\\boldsymbol{x} ; \\boldsymbol{\\theta}) \\tag{23.35}\n\\end{equation*}\n\n\n假设条件概率分布 $P_{\\boldsymbol{\\theta}}\\left(y_{k}=1 \\mid \\boldsymbol{x}\\right)$ 遵循类别分布 (categorical distribution), 学习问题变为优化问题:\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\operatorname{argmin}}\\left\\{-\\sum_{i=1}^{N}\\left[\\sum_{k=1}^{l} y_{i k} \\log f(\\boldsymbol{x} ; \\boldsymbol{\\theta})\\right]\\right\\} \\tag{23.36}\n\\end{equation*}\n\n\n其中, $y_{i k} \\in\\{0,1\\}, \\sum_{k=1}^{l} y_{i k}=1, k=1,2, \\cdots, l, i=1,2, \\cdots, N$ 。所以, 前馈神经网络用于二类和多类分类时以交叉嫡为损失函数, 进行的是交叉嫡的最小化。"
    },
    {
        "id": "chatcmpl-93z7r0FlxmQrsCIKoNpcqncMN2MV4",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"前馈神经网络学习的一般形式中，学习问题如何形式化？\",\n    \"回答1\": \"前馈神经网络学习的一般形式中，学习问题可以形式化为一个优化问题，即寻找参数 $\\boldsymbol{\\theta}$ 的最优值，使得损失函数 $L(\\boldsymbol{x}_{i}, y_{i}; \\boldsymbol{\\theta})$ 和正则项 $\\boldsymbol{\\theta}$ 的组合达到最小。这可以表示为 $\\text{argmin}_{\\boldsymbol{\\theta}}\\big[\\text{损失函数} + \\text{正则项}\\big]$。\",\n    \"问题2\": \"前馈神经网络学习的一般形式中，当损失函数是对数损失函数、没有正则化时，学习问题变成了什么？\",\n    \"回答2\": \"当损失函数是对数损失函数、没有正则化时，学习问题变成了极大似然估计。这意味着在这种情况下，学习的目标是最大化观测数据出现的可能性，以估计模型参数的值。\",\n    \"问题3\": \"前馈神经网络用于回归时，学习进行的是什么最小化？\",\n    \"回答3\": \"前馈神经网络用于回归时，学习进行的是平方损失的最小化。即通过最小化实际输出与预测输出之间的差异来学习模型参数。\",\n    \"问题4\": \"前馈神经网络用于二类分类时，学习进行的是什么最小化？\",\n    \"回答4\": \"前馈神经网络用于二类分类时，学习进行的是交叉熵损失的最小化。这意味着通过最小化观测数据和预测数据之间的交叉熵来学习模型参数。\",\n    \"问题5\": \"前馈神经网络用于多类分类时，学习进行的是什么最小化？\",\n    \"回答5\": \"前馈神经网络用于多类分类时，学习进行的是交叉熵损失的最小化。通过最小化观测数据和预测数据之间的交叉熵来学习模型参数。\",\n    \"问题6\": \"在前馈神经网络学习中，什么因素决定了输入给定条件下输出的条件概率分布？\",\n    \"回答6\": \"在前馈神经网络学习中，输入给定条件下输出的条件概率分布由神经网络决定。神经网络的参数 $\\boldsymbol{\\theta}$ 决定了输入 $\\boldsymbol{x}$ 给定条件下输出 $y$ 的条件概率分布。\",\n    \"问题7\": \"在前馈神经网络学习中，如何表示输入给定条件下输出的条件概率分布遵循高斯分布？\",\n    \"回答7\": \"在前馈神经网络学习中，输入给定条件下输出的条件概率分布遵循高斯分布可以表示为 $P_{\\boldsymbol{\\theta}}(y \\text{ 为 } \\boldsymbol{x}) \\text{ 服从 } N\\big(f(\\boldsymbol{x} ; \\boldsymbol{\\theta}), \\text{ 方差 } \\boldsymbol{\\theta}^2\\big)$。\",\n    \"问题8\": \"在前馈神经网络学习中，如何表示输入给定条件下输出的条件概率分布遵循贝努利分布？\",\n    \"回答8\": \"在前馈神经网络学习中，输入给定条件下输出的条件概率分布遵循贝努利分布可以表示为 $P_{\\boldsymbol{\\theta}}(y=1 \\text{ 为 } \\boldsymbol{x})=f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$。\",\n    \"问题9\": \"在前馈神经网络学习中，如何表示输入给定条件下输出的条件概率分布遵循类别分布？\",\n    \"回答9\": \"在前馈神经网络学习中，输入给定条件下输出的条件概率分布遵循类别分布可以表示为 $\\boldsymbol{p}=P_{\\boldsymbol{\\theta}}(y_{k}=1 \\text{ 为 } \\boldsymbol{x})=f(\\boldsymbol{x} ; \\boldsymbol{\\theta})$。\",\n    \"问题10\": \"在前馈神经网络学习中，正则项 $\\boldsymbol{\\theta}$ 的作用是什么？\",\n    \"回答10\": \"在前馈神经网络学习中，正则项 $\\boldsymbol{\\theta}$ 的作用是控制模型复杂度，并且有助于防止过拟合。通过正则化项，可以使得模型更加平滑和泛化能力更强。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710736979,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1310,
            "prompt_tokens": 4786,
            "total_tokens": 6096
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]