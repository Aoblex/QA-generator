[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter5",
        "title": "9.1.1 EM 算法",
        "content_length": 5451,
        "content": "Title: 9.1.1 EM 算法\nContent:\n首先介绍一个使用 $\\mathrm{EM}$ 算法的例子。\n\n例 9.1 (三硬币模型） 假设有 3 枚硬币, 分别记作 A, B, C。这些硬币正面出现的概率分别是 $\\pi, p$ 和 $q$ 。进行如下掷硬币试验: 先郑硬币 $\\mathrm{A}$, 根据其结果选出硬币 $\\mathrm{B}$ 或硬币 $\\mathrm{C}$, 正面选硬币 $\\mathrm{B}$, 反面选硬币 $\\mathrm{C}$; 然后掷选出的硬币, 根据掷硬币的结果, 出现正面记作 1 , 出现反面记作 0 ; 独立地重复 $n$ 次试验 (这里, $n=10$ ), 观测结果如下:\n\n$$\n1,1,0,1,0,0,1,0,1,1\n$$\n\n假设只能观测到掷硬币的结果, 不能观测掷硬币的过程。问如何估计三硬币正面出现的概率,即三硬币模型的参数。\n\n解 三硬币模型可以写作\n\n\n\\begin{align*}\nP(y \\mid \\theta) & =\\sum_{z} P(y, z \\mid \\theta)=\\sum_{z} P(z \\mid \\theta) P(y \\mid z, \\theta) \\\\\n& =\\pi p^{y}(1-p)^{1-y}+(1-\\pi) q^{y}(1-q)^{1-y} \\tag{9.1}\n\\end{align*}\n\n\n这里, 随机变量 $y$ 是观测变量, 表示一次试验观测的结果是 1 或 0 ; 随机变量 $z$ 是隐变量, 表示未观测到的郑硬币 $\\mathrm{A}$ 的结果; $\\theta=(\\pi, p, q)$ 是模型参数。这一模型是以上数据的生成模型。注意, 随机变量 $y$ 的数据可以观测, 随机变量 $z$ 的数据不可观测。\n\n将观测数据表示为 $Y=\\left(Y_{1}, Y_{2}, \\cdots, Y_{n}\\right)^{\\mathrm{T}}$, 未观测数据表示为 $Z=\\left(Z_{1}, Z_{2}, \\cdots, Z_{n}\\right)^{\\mathrm{T}}$, 则观测数据的似然函数为\n\n\n\\begin{equation*}\nP(Y \\mid \\theta)=\\sum_{Z} P(Z \\mid \\theta) P(Y \\mid Z, \\theta) \\tag{9.2}\n\\end{equation*}\n\n\n即\n\n\n\\begin{equation*}\nP(Y \\mid \\theta)=\\prod_{j=1}^{n}\\left[\\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\\pi) q^{y_{j}}(1-q)^{1-y_{j}}\\right] \\tag{9.3}\n\\end{equation*}\n\n\n考虑求模型参数 $\\theta=(\\pi, p, q)$ 的极大似然估计, 即\n\n\n\\begin{equation*}\n\\hat{\\theta}=\\arg \\max _{\\theta} \\log P(Y \\mid \\theta) \\tag{9.4}\n\\end{equation*}\n\n\n这个问题没有解析解, 只有通过迭代的方法求解。EM 算法就是可以用于求解这个问题的一种迭代算法。下面给出针对以上问题的 EM 算法, 其推导过程省略。\n\nEM 算法首先选取参数的初值, 记作 $\\theta^{(0)}=\\left(\\pi^{(0)}, p^{(0)}, q^{(0)}\\right)$, 然后通过下面的步骤迭代计算参数的估计值, 直至收玫为止。第 $i$ 次迭代参数的估计值为 $\\theta^{(i)}=\\left(\\pi^{(i)}, p^{(i)}, q^{(i)}\\right)$ 。 $\\mathrm{EM}$ 算法的第 $i+1$ 次迭代如下。\n\n$\\mathrm{E}$ 步: 计算在模型参数 $\\pi^{(i)}, p^{(i)}, q^{(i)}$ 下观测数据 $y_{j}$ 来自掷硬币 $\\mathrm{B}$ 的概率。\n\n\n\\begin{equation*}\n\\mu_{j}^{(i+1)}=\\frac{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_{j}}\\left(1-p^{(i)}\\right)^{1-y_{j}}}{\\pi^{(i)}\\left(p^{(i)}\\right)^{y_{j}}\\left(1-p^{(i)}\\right)^{1-y_{j}}+\\left(1-\\pi^{(i)}\\right)\\left(q^{(i)}\\right)^{y_{j}}\\left(1-q^{(i)}\\right)^{1-y_{j}}} \\tag{9.5}\n\\end{equation*}\n\n\n$M$ 步：计算模型参数的新估计值。\n\n\\[\n\\begin{array}{r}\n\\pi^{(i+1)}=\\frac{1}{n} \\sum_{j=1}^{n} \\mu_{j}^{(i+1)} \\\\\np^{(i+1)}=\\frac{\\sum_{j=1}^{n} \\mu_{j}^{(i+1)} y_{j}}{\\sum_{j=1}^{n} \\mu_{j}^{(i+1)}} \\\\\nq^{(i+1)}=\\frac{\\sum_{j=1}^{n}\\left(1-\\mu_{j}^{(i+1)}\\right) y_{j}}{\\sum_{j=1}^{n}\\left(1-\\mu_{j}^{(i+1)}\\right)} \\tag{9.8}\n\\end{array}\n\\]\n\n进行数值计算。假设模型参数的初值取为\n\n$$\n\\pi^{(0)}=0.5, \\quad p^{(0)}=0.5, \\quad q^{(0)}=0.5\n$$\n\n由式 (9.5), 对 $y_{j}=1$ 与 $y_{j}=0$ 均有 $\\mu_{j}^{(1)}=0.5$ 。\\\\\n利用迭代公式 (9.6) 公式 (9.8) 得到:\n\n$$\n\\pi^{(1)}=0.5, \\quad p^{(1)}=0.6, \\quad q^{(1)}=0.6\n$$\n\n由式 (9.5) 得:\n\n$$\n\\mu_{j}^{(2)}=0.5, \\quad j=1,2, \\cdots, 10\n$$\n\n继续迭代, 得:\n\n$$\n\\pi^{(2)}=0.5, \\quad p^{(2)}=0.6, \\quad q^{(2)}=0.6\n$$\n\n于是得到模型参数 $\\theta$ 的极大似然估计:\n\n$$\n\\hat{\\pi}=0.5, \\quad \\hat{p}=0.6, \\quad \\hat{q}=0.6\n$$\n\n$\\pi=0.5$ 表示硬币 $\\mathrm{A}$ 是均匀的, 这一结果容易理解。\n\n如果取初值 $\\pi^{(0)}=0.4, p^{(0)}=0.6, q^{(0)}=0.7$, 那么得到的模型参数的极大似然估计是 $\\hat{\\pi}=0.4064, \\hat{p}=0.5368, \\hat{q}=0.6432$ 。这就是说, $\\mathrm{EM}$ 算法与初值的选择有关, 选择不同的初值可能得到不同的参数估计值。\n\n一般地, 用 $Y$ 表示观测随机变量的数据, $Z$ 表示隐随机变量的数据。 $Y$ 和 $Z$ 连在一起称为完全数据 (complete-data), 观测数据 $Y$ 又称为不完全数据（incomplete-data）。假设给定观测数据 $Y$, 其概率分布是 $P(Y \\mid \\theta)$, 其中 $\\theta$ 是需要估计的模型参数, 那么不完全数据 $Y$ 的似然函数是 $P(Y \\mid \\theta)$, 对数似然函数 $L(\\theta)=\\log P(Y \\mid \\theta)$; 假设 $Y$ 和 $Z$ 的联合概率分布是 $P(Y, Z \\mid \\theta)$, 那么完全数据的对数似然函数是 $\\log P(Y, Z \\mid \\theta)$ 。\n\nEM 算法通过迭代求 $L(\\theta)=\\log P(Y \\mid \\theta)$ 的极大似然估计。每次迭代包含两步: $\\mathrm{E}$ 步, 求期望; $\\mathrm{M}$ 步, 求极大化。下面来介绍 $\\mathrm{EM}$ 算法。\n\n\\section*{算法 9.1 (EM 算法)}\n输入: 观测变量数据 $Y$, 隐变量数据 $Z$, 联合分布 $P(Y, Z \\mid \\theta)$, 条件分布 $P(Z \\mid Y, \\theta)$ 。\n\n输出: 模型参数 $\\theta$ 。\n\n(1) 选择参数的初值 $\\theta^{(0)}$, 开始迭代。\n\n(2) $\\mathrm{E}$ 步: 记 $\\theta^{(i)}$ 为第 $i$ 次迭代参数 $\\theta$ 的估计值, 在第 $i+1$ 次迭代的 $\\mathrm{E}$ 步, 计算\n\n\n\\begin{align*}\nQ\\left(\\theta, \\theta^{(i)}\\right) & =E_{Z}\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\\\\n& =\\sum_{Z} \\log P(Y, Z \\mid \\theta) P\\left(Z \\mid Y, \\theta^{(i)}\\right) \\tag{9.9}\n\\end{align*}\n\n\n这里, $P\\left(Z \\mid Y, \\theta^{(i)}\\right)$ 是在给定观测数据 $Y$ 和当前的参数估计 $\\theta^{(i)}$ 下隐变量数据 $Z$ 的条件概率分布。\n\n(3) $\\mathrm{M}$ 步: 求使 $Q\\left(\\theta, \\theta^{(i)}\\right)$ 极大化的 $\\theta$, 确定第 $i+1$ 次迭代的参数的估计值 $\\theta^{(i+1)}$ 。\n\n\n\\begin{equation*}\n\\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right) \\tag{9.10}\n\\end{equation*}\n\n\n(4) 重复第 2 步和第 3 步, 直到收敛。\n\n式 (9.9) 的函数 $Q\\left(\\theta, \\theta^{(i)}\\right)$ 是 $\\mathrm{EM}$ 算法的核心, 称为 $Q$ 函数 ( $Q$ function）。\n\n定义 9.1 ( $\\boldsymbol{Q}$ 函数) 完全数据的对数似然函数 $\\log P(Y, Z \\mid \\theta)$ 关于在给定观测数据 $Y$ 和当前参数 $\\theta^{(i)}$ 下对未观测数据 $Z$ 的条件概率分布 $P\\left(Z \\mid Y, \\theta^{(i)}\\right)$ 的期望称为 $Q$ 函数, 即\n\n\n\\begin{equation*}\nQ\\left(\\theta, \\theta^{(i)}\\right)=E_{Z}\\left[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}\\right] \\tag{9.11}\n\\end{equation*}\n\n\n下面关于 EM 算法作几点说明:\n\n(1) 步骤 (1) 中参数的初值可以任意选择, 但需注意 EM 算法对初值是敏感的。\n\n(2) 步骤 (2) 中 $\\mathrm{E}$ 步求 $Q\\left(\\theta, \\theta^{(i)}\\right)$ 。 $Q$ 函数式中 $Z$ 是未观测数据, $Y$ 是观测数据。注意, $Q\\left(\\theta, \\theta^{(i)}\\right)$ 的第 1 个变元表示要极大化的参数, 第 2 个变元表示参数的当前估计值。每次迭代实际在求 $Q$ 函数及其极大。\n\n(3) 步骤 (3) 中 $\\mathrm{M}$ 步求 $Q\\left(\\theta, \\theta^{(i)}\\right)$ 的极大化, 得到 $\\theta^{(i+1)}$, 完成一次迭代 $\\theta^{(i)} \\rightarrow \\theta^{(i+1)}$ 。后面将证明每次迭代使似然函数增大或达到局部极值。\n\n(4) 步骤 (4) 给出停止迭代的条件, 一般是对较小的正数 $\\varepsilon_{1}, \\varepsilon_{2}$, 若满足\n\n$$\n\\left\\|\\theta^{(i+1)}-\\theta^{(i)}\\right\\|<\\varepsilon_{1} \\text { 或 }\\left\\|Q\\left(\\theta^{(i+1)}, \\theta^{(i)}\\right)-Q\\left(\\theta^{(i)}, \\theta^{(i)}\\right)\\right\\|<\\varepsilon_{2}\n$$\n\n则停止迭代。"
    },
    {
        "id": "chatcmpl-948ScytlhBO6LQsNmT840VT5dNhgb",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是三硬币模型？\",\n    \"回答1\": \"三硬币模型是一个概率模型，假设有3枚硬币，分别记作A, B, C。这些硬币正面出现的概率分别是 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}$ ，进行如下掷硬币试验: 先郑硬币 $\\text{A}$, 根据其结果选出硬币 $\\text{B}$ 或硬币 $\\text{C}$，正面选硬币 $\\text{B}$，反面选硬币 $\\text{C}$；然后掷选出的硬币，根据掷硬币的结果，出现正面记作 1 , 出现反面记作 0；独立地重复 $n$ 次试验，观测结果如下：$1,1,0,1,0,0,1,0,1,1$。\",\n    \"问题2\": \"三硬币模型的参数估计问题如何通过EM算法求解？\",\n    \"回答2\": \"对于三硬币模型的参数估计问题，可以通过EM算法进行求解。EM算法首先选取参数的初值，然后通过迭代的方法计算参数的估计值，直至收敛为止。具体步骤包括E步：计算在模型参数下观测数据来自掷硬币的概率；M步：计算模型参数的新估计值。通过迭代计算，最终得到模型参数的极大似然估计。\",\n    \"问题3\": \"EM算法中的E步和M步分别是如何计算的？\",\n    \"回答3\": \"在EM算法中，E步计算在模型参数下观测数据来自掷硬币的概率；而M步则计算模型参数的新估计值。具体来说，E步通过计算观测数据在当前参数下来自不同硬币的概率；M步则根据E步得到的概率值，更新模型参数的估计值。\",\n    \"问题4\": \"EM算法中的Q函数是什么？\",\n    \"回答4\": \"在EM算法中，Q函数是完全数据的对数似然函数关于在给定观测数据和当前参数估计值下对未观测数据的条件概率分布的期望。它是EM算法的核心函数，用于在每次迭代中求取极大化。\",\n    \"问题5\": \"EM算法中如何确定停止迭代？\",\n    \"回答5\": \"在EM算法中，通常通过对较小的正数进行判断来确定停止迭代。若满足某一条件，则停止迭代。一般可以通过比较参数估计值之间的差异或者比较Q函数值之间的差异来判断是否停止迭代。\",\n    \"问题6\": \"什么是观测数据和未观测数据？\",\n    \"回答6\": \"在EM算法中，观测数据指已经观测到并记录下来的数据，而未观测数据则指未被观测到或记录下来的数据。通常情况下，观测数据和未观测数据连在一起称为完全数据（complete-data），而观测数据又称为不完全数据（incomplete-data）。\",\n    \"问题7\": \"EM算法对初值选择是否敏感？\",\n    \"回答7\": \"是的，EM算法对初值选择是敏感的。不同的初值选择可能会导致不同的参数估计值。因此，在使用EM算法时需要注意选择合适的初值以获得稳定和准确的参数估计结果。\",\n    \"问题8\": \"EM算法是如何通过迭代求解极大似然估计的？\",\n    \"回答8\": \"EM算法通过迭代求解极大似然估计，每次迭代包含两步：E步求期望；M步求极大化。在E步中，计算完全数据对数似然函数关于未观测数据的条件概率分布的期望；而M步则求取使得期望极大化的参数值。通过不断迭代这两个步骤，最终得到极大似然估计。\",\n    \"问题9\": \"EM算法中Q函数和完全数据对数似然函数有什么关系？\",\n    \"回答9\": \"在EM算法中，Q函数是完全数据对数似然函数关于未观测数据条件概率分布的期望。它起到了连接完全数据对数似然函数和未观测数据条件概率分布之间关系的作用，在每次迭代中用于求取极大化。\",\n    \"问题10\": \"EM算法适用于哪些类型的问题？\",\n    \"回答10\": \"EM算法主要适用于含有隐变量（未观测数据）的概率模型参数估计问题。它能够有效地处理由隐变量引起的模型参数无法直接通过常规方法求解的情况，并且在实际应用中被广泛使用。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710772862,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1538,
            "prompt_tokens": 6056,
            "total_tokens": 7594
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]