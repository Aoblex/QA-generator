[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter5",
        "title": "9.3.2 高斯混合模型参数估计的 EM 算法",
        "content_length": 5955,
        "content": "Title: 9.3.2 高斯混合模型参数估计的 EM 算法\nContent:\n假设观测数据 $y_{1}, y_{2}, \\cdots, y_{N}$ 由高斯混合模型生成:\n\n\n\\begin{equation*}\nP(y \\mid \\theta)=\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y \\mid \\theta_{k}\\right) \\tag{9.26}\n\\end{equation*}\n\n\n其中, $\\theta=\\left(\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{K} ; \\theta_{1}, \\theta_{2}, \\cdots, \\theta_{K}\\right)$ 。我们用 $\\mathrm{EM}$ 算法估计高斯混合模型的参数 $\\theta$ 。\n\n\\section*{1. 明确隐变量, 写出完全数据的对数似然函数}\n可以设想观测数据 $y_{j}, j=1,2, \\cdots, N$, 是这样产生的: 首先根据概率 $\\alpha_{k}$ 选择第 $k$ 个高斯分布分模型 $\\phi\\left(y \\mid \\theta_{k}\\right)$, 然后由第 $k$ 个分模型的概率分布 $\\phi\\left(y \\mid \\theta_{k}\\right)$ 生成观测数据 $y_{j}$ 。这时观测数据 $y_{j}, j=1,2, \\cdots, N$, 是已知的, 反映观测数据 $y_{j}$ 来自第 $k$ 个分模型的数据是未知的, $k=1,2, \\cdots, K$, 以隐变量 $\\gamma_{j k}$ 表示, 其定义如下:\n\n\n\\begin{align*}\n& \\gamma_{j k}= \\begin{cases}1, & \\text { 第 } j \\text { 个观测来自第 } k \\text { 个分模型 } \\\\\n0, & \\text { 否则 }\\end{cases} \\\\\n& j=1,2, \\cdots, N, \\quad k=1,2, \\cdots, K \\tag{9.27}\n\\end{align*}\n\n\n其中, $\\gamma_{j k}$ 是 0-1 随机变量。\n\n有了观测数据 $y_{j}$ 及未观测数据 $\\gamma_{j k}$, 那么完全数据是\n\n$$\n\\left(y_{j}, \\gamma_{j 1}, \\gamma_{j 2}, \\cdots, \\gamma_{j K}\\right), \\quad j=1,2, \\cdots, N\n$$\n\n于是, 可以写出完全数据的似然函数:\n\n$$\n\\begin{aligned}\nP(y, \\gamma \\mid \\theta) & =\\prod_{j=1}^{N} P\\left(y_{j}, \\gamma_{j 1}, \\gamma_{j 2}, \\cdots, \\gamma_{j K} \\mid \\theta\\right) \\\\\n& =\\prod_{k=1}^{K} \\prod_{j=1}^{N}\\left[\\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)\\right]^{\\gamma_{j k}} \\\\\n& =\\prod_{k=1}^{K} \\alpha_{k}^{n_{k}} \\prod_{j=1}^{N}\\left[\\phi\\left(y_{j} \\mid \\theta_{k}\\right)\\right]^{\\gamma_{j k}} \\\\\n& =\\prod_{k=1}^{K} \\alpha_{k}^{n_{k}} \\prod_{j=1}^{N}\\left\\{\\frac{1}{\\sqrt{2 \\pi} \\sigma_{k}} \\exp \\left[-\\frac{\\left(y_{j}-\\mu_{k}\\right)^{2}}{2 \\sigma_{k}^{2}}\\right]\\right\\}^{\\gamma_{j k}}\n\\end{aligned}\n$$\n\n式中, $n_{k}=\\sum_{j=1}^{N} \\gamma_{j k}, \\sum_{k=1}^{K} n_{k}=N$ 。\n\n那么, 完全数据的对数似然函数为\n\n$$\n\\log P(y, \\gamma \\mid \\theta)=\\sum_{k=1}^{K}\\left\\{n_{k} \\log \\alpha_{k}+\\sum_{j=1}^{N} \\gamma_{j k}\\left[\\log \\frac{1}{\\sqrt{2 \\pi}}-\\log \\sigma_{k}-\\frac{1}{2 \\sigma_{k}^{2}}\\left(y_{j}-\\mu_{k}\\right)^{2}\\right]\\right\\}\n$$\n\n\\begin{enumerate}\n  \\setcounter{enumi}{1}\n  \\item EM 算法的 $\\mathrm{E}$ 步: 确定 $Q$ 函数\n\\end{enumerate}\n\n\n\\begin{align*}\nQ\\left(\\theta, \\theta^{(i)}\\right) & =E\\left[\\log P(y, \\gamma \\mid \\theta) \\mid y, \\theta^{(i)}\\right] \\\\\n& =E \\sum_{k=1}^{K}\\left\\{n_{k} \\log \\alpha_{k}+\\sum_{j=1}^{N} \\gamma_{j k}\\left[\\log \\frac{1}{\\sqrt{2 \\pi}}-\\log \\sigma_{k}-\\frac{1}{2 \\sigma_{k}^{2}}\\left(y_{j}-\\mu_{k}\\right)^{2}\\right]\\right\\} \\\\\n& =\\sum_{k=1}^{K}\\left\\{\\sum_{j=1}^{N}\\left(E \\gamma_{j k}\\right) \\log \\alpha_{k}+\\sum_{j=1}^{N}\\left(E \\gamma_{j k}\\right)\\left[\\log \\frac{1}{\\sqrt{2 \\pi}}-\\log \\sigma_{k}-\\frac{1}{2 \\sigma_{k}^{2}}\\left(y_{j}-\\mu_{k}\\right)^{2}\\right]\\right\\} \\tag{9.28}\n\\end{align*}\n\n\n这里需要计算 $E\\left(\\gamma_{j k} \\mid y, \\theta\\right)$, 记为 $\\hat{\\gamma}_{j k}$ 。\n\n$$\n\\begin{aligned}\n\\hat{\\gamma}_{j k} & =E\\left(\\gamma_{j k} \\mid y, \\theta\\right)=P\\left(\\gamma_{j k}=1 \\mid y, \\theta\\right) \\\\\n& =\\frac{P\\left(\\gamma_{j k}=1, y_{j} \\mid \\theta\\right)}{\\sum_{k=1}^{K} P\\left(\\gamma_{j k}=1, y_{j} \\mid \\theta\\right)} \\\\\n& =\\frac{P\\left(y_{j} \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)}{\\sum_{k=1}^{K} P\\left(y_{j} \\mid \\gamma_{j k}=1, \\theta\\right) P\\left(\\gamma_{j k}=1 \\mid \\theta\\right)} \\\\\n& =\\frac{\\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)}{\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)}, \\quad j=1,2, \\cdots, N, \\quad k=1,2, \\cdots, K\n\\end{aligned}\n$$\n\n$\\hat{\\gamma}_{j k}$ 是在当前模型参数下第 $j$ 个观测数据来自第 $k$ 个分模型的概率, 称为分模型 $k$ 对观测数据 $y_{j}$ 的响应度。\n\n将 $\\hat{\\gamma}_{j k}=E \\gamma_{j k}$ 及 $n_{k}=\\sum_{j=1}^{N} E \\gamma_{j k}$ 代入式 (9.28), 即得:\n\n\n\\begin{equation*}\nQ\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{k=1}^{K}\\left\\{n_{k} \\log \\alpha_{k}+\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}\\left[\\log \\frac{1}{\\sqrt{2 \\pi}}-\\log \\sigma_{k}-\\frac{1}{2 \\sigma_{k}^{2}}\\left(y_{j}-\\mu_{k}\\right)^{2}\\right]\\right\\} \\tag{9.29}\n\\end{equation*}\n\n\n\\section*{3. 确定 EM 算法的 $M$ 步}\n迭代的 $\\mathrm{M}$ 步是求函数 $Q\\left(\\theta, \\theta^{(i)}\\right)$ 对 $\\theta$ 的极大值, 即求新一轮迭代的模型参数:\n\n$$\n\\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)\n$$\n\n用 $\\hat{\\mu}_{k}, \\hat{\\sigma}_{k}^{2}$ 及 $\\hat{\\alpha}_{k}, k=1,2, \\cdots, K$, 表示 $\\theta^{(i+1)}$ 的各参数。求 $\\hat{\\mu}_{k}, \\hat{\\sigma}_{k}^{2}$ 只需将式 (9.29) 分别对 $\\mu_{k}, \\sigma_{k}^{2}$ 求偏导数并令其为 0 , 即可得到; $\\hat{\\alpha}_{k}$ 是在 $\\sum_{k=1}^{K} \\alpha_{k}=1$ 条件下求偏导数并令其为 0 得到的。结果如下:\n\n\n\\begin{gather*}\n\\hat{\\mu}_{k}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k} y_{j}}{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K  \\tag{9.30}\\\\\n\\hat{\\sigma}_{k}^{2}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}\\left(y_{j}-\\mu_{k}\\right)^{2}}{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K  \\tag{9.31}\\\\\n\\hat{\\alpha}_{k}=\\frac{n_{k}}{N}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}}{N}, \\quad k=1,2, \\cdots, K \\tag{9.32}\n\\end{gather*}\n\n\n重复以上计算, 直到对数似然函数值不再有明显的变化为止。\n\n现将估计高斯混合模型参数的 EM 算法总结如下。\n\n\\section*{算法 9.2 (高斯混合模型参数估计的EM算法)}\n输入: 观测数据 $y_{1}, y_{2}, \\cdots, y_{N}$, 高斯混合模型。\n\n输出: 高斯混合模型参数。\n\n(1) 取参数的初始值开始迭代。\n\n(2) $\\mathrm{E}$ 步: 依据当前模型参数, 计算分模型 $k$ 对观测数据 $y_{j}$ 的响应度。\n\n$$\n\\hat{\\gamma}_{j k}=\\frac{\\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)}{\\sum_{k=1}^{K} \\alpha_{k} \\phi\\left(y_{j} \\mid \\theta_{k}\\right)}, \\quad j=1,2, \\cdots, N, \\quad k=1,2, \\cdots, K\n$$\n\n(3) $\\mathrm{M}$ 步: 计算新一轮迭代的模型参数。\n\n$$\n\\begin{gathered}\n\\hat{\\mu}_{k}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k} y_{j}}{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\n\\hat{\\sigma}_{k}^{2}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}\\left(y_{j}-\\mu_{k}\\right)^{2}}{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}}, \\quad k=1,2, \\cdots, K \\\\\n\\hat{\\alpha}_{k}=\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{j k}}{N}, \\quad k=1,2, \\cdots, K\n\\end{gathered}\n$$\n\n(4) 重复第 2 步和第 3 步, 直到收敛。\n\n\\section*{$9.4 \\mathrm{EM}$ 算法的推广}\n$\\mathrm{EM}$ 算法还可以解释为 $F$ 函数 ( $F$ function) 的极大-极大算法 (maximization-maximization algorithm）, 基于这个解释有若干变形与推广, 如广义期望极大（generalized expectation maximization, GEM）算法。下面予以介绍。"
    },
    {
        "id": "chatcmpl-948VoObNVX4zrOWFR7FWPUr2DCVCR",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"高斯混合模型的观测数据生成方式是怎样的？\",\n    \"回答1\": \"观测数据 $y_{j}, j=1,2, \\text{...}, N$, 由高斯混合模型生成，其中高斯混合模型的生成方式可以表示为 $P(y \\text{ ｜ } \\theta)=\\text{∑}_{k=1}^{K} \\boldsymbol{\\text{α}}_{k} \\boldsymbol{\\text{φ}}(y \\text{ ｜ } \\theta_{k})$。\",\n    \"问题2\": \"在高斯混合模型中，隐变量 $\\boldsymbol{\\text{γ}}_{j k}$ 的含义是什么？\",\n    \"回答2\": \"在高斯混合模型中，隐变量 $\\boldsymbol{\\text{γ}}_{j k}$ 表示第 $j$ 个观测数据来自第 $k$ 个分模型的概率，其中 $\\boldsymbol{\\text{γ}}_{j k}$ 是 0-1 随机变量，其定义为 $\\boldsymbol{\\text{γ}}_{j k}= \\begin{cases}1, & \\text { 第 } j \\text { 个观测来自第 } k \\text { 个分模型 } \\\\0, & \\text { 否则 }\\boldsymbol{\\text{γ}}_{j k}$。\",\n    \"问题3\": \"完全数据的对数似然函数是如何定义的？\",\n    \"回答3\": \"完全数据的对数似然函数为 $\\text{log} P(y, \\boldsymbol{\\text{γ}} \\text{ ｜ } \\theta)=\\text{∑}_{k=1}^{K}\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\bigg{n}_{k} \\text{ log } \\boldsymbol{\\text{α}}_{k}+\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}\\bigg[\\text{log} \\frac{1}{\\boldsymbol{\\text{σ}}_{k}}-\\text{log} \\boldsymbol{\\text{σ}}_{k}-\\frac{1}{2 \\boldsymbol{\\text{σ}}_{k}^{2}}(y_{j}-\\boldsymbol{\\text{μ}}_{k})^{2}\\bigg]\\bigg$。\",\n    \"问题4\": \"EM 算法的 E 步是如何确定 $Q$ 函数的？\",\n    \"回答4\": \"EM 算法的 E 步确定 $Q$ 函数为 $Q(\\theta, \\theta^{(i)})=\\text{∑}_{k=1}^{K}\\bigg{n}_{k} \\text{ log } \\boldsymbol{\\text{α}}_{k}+\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}\\bigg[\\text{log} \\frac{1}{\\boldsymbol{\\text{σ}}_{k}}-\\text{log} \\boldsymbol{\\text{σ}}_{k}-\\frac{1}{2 \\boldsymbol{\\text{σ}}_{k}^{2}}(y_{j}-\\boldsymbol{\\text{μ}}_{k})^{2}\\bigg]$。\",\n    \"问题5\": \"EM 算法的 M 步是如何确定新一轮迭代的模型参数的？\",\n    \"回答5\": \"EM 算法的 M 步确定新一轮迭代的模型参数 $\\theta^{(i+1)}$ 的计算公式为 $\\begin{aligned}\\boldsymbol{\\text{μ}}_{k}&=\\frac{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k} y_{j}}{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}}, k=1,2, \\text{...}, K \\\\ \\boldsymbol{\\text{σ}}_{k}^{2}&=\\frac{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}(y_{j}-\\boldsymbol{\\text{μ}}_{k})^{2}}{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}}, k=1,2, \\text{...}, K \\\\ \\boldsymbol{\\text{α}}_{k}&=\\frac{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}}{N}, k=1,2, \\text{...}, K \\\\ \\\\\\boldsymbol{\\text{n}}_{k}&=\\frac{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}(y_{j}-\\boldsymbol{\\text{μ}}_{k})^{2}}{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{γ}}_{j k}}, k=1,2, \\text{...}, K \\\\ \\\\end{aligned}$。\",\n    \"问题6\": \"EM 算法的收敛条件是什么？\",\n    \"回答6\": \"EM 算法重复进行 E 步和 M 步，直到对数似然函数值不再有明显的变化为止，即达到收敛状态。\",\n    \"问题7\": \"高斯混合模型参数估计的 EM 算法具体步骤是什么？\",\n    \"回答7\": \"高斯混合模型参数估计的 EM 算法具体步骤如下：(1) 取参数的初始值开始迭代；(2) E 步：依据当前模型参数，计算分模型 $k$ 对观测数据 $y_j$ 的响应度；(3) M 步：计算新一轮迭代的模型参数；(4) 重复第 2 步和第 3 步，直到收敛。\",\n    \"问题8\": \"EM 算法还有哪些推广形式？\",\n    \"回答8\": \"EM 算法还可以解释为 $F$ 函数 ( $F$ function) 的极大-极大算法 (maximization-maximization algorithm），基于这个解释有若干变形与推广，如广义期望极大（generalized expectation maximization, GEM）算法。\",\n    \"问题9\": \"GEM 算法与标准 EM 算法有何区别？\",\n    \"回答9\": \"GEM 算法是 EM 算法的一种推广形式，它在 EM 算法的基础上进行了改进和扩展，使得在特定情况下能够更好地适应复杂的数据分布和模型结构。相比标准 EM 算法，GEM 算法具有更强的灵活性和适应性。\",\n    \"问题10\": \"在 EM 算法中，隐变量 $\\boldsymbol{\\text{γ}}_{j k}$ 的计算公式是怎样的？\",\n    \"回答10\": \"在 EM 算法中，隐变量 $\\boldsymbol{\\text{γ}}_{j k}$ 的计算公式为 $\\begin{aligned}\\boldsymbol{\\tilde{{γ}}} _{{jk}}&=E(\\boldsymbol{{γ_{{jk}}} | y, θ})=\\frac{{α_k φ(y_j | θ_k)}}{{∑_{{k=1}}{{K}}{{α_k φ(y_j | θ_k)}}}}, j=1,2, ..., N, k=1,2, ..., K \\\\end{{aligned}}\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710773060,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1893,
            "prompt_tokens": 5778,
            "total_tokens": 7671
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]