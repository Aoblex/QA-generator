[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter5",
        "title": "8.4.3 梯度提升",
        "content_length": 5217,
        "content": "Title: 8.4.3 梯度提升\nContent:\n提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时, 每一步优化是很简单的。但对一般损失函数而言, 往往每一步优化并不那么容易。针对这一问题, Freidman 提出了梯度提升 (gradient boosting) 算法。这是利用最速下降法的近似方法, 其关键是利用损失函数的负梯度在当前模型的值\n\n$$\n-\\left[\\frac{\\partial L\\left(y, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)}\n$$\n\n作为回归问题提升树算法中的残差的近似值, 拟合一个回归树。\n\n\\section*{算法 8.4 (梯度提升算法)}\n输入: 训练数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}, x_{i} \\in \\mathcal{X} \\subseteq \\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y} \\subseteq \\boldsymbol{R}$; 损失函数 $L(y, f(x))$ 。\\\\\n输出: 回归树 $\\hat{f}(x)$ 。\n\n(1) 初始化:\n\n$$\nf_{0}(x)=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right)\n$$\n\n(2) 对 $m=1,2, \\cdots, M$,\n\n(a) 对 $i=1,2, \\cdots, N$ ，计算\n\n$$\nr_{m i}=-\\left[\\frac{\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)}\n$$\n\n(b) 对 $r_{m i}$ 拟合一个回归树, 得到第 $m$ 棵树的叶结点区域 $R_{m j}, j=1,2, \\cdots, J$ 。\n\n(c) 对 $j=1,2, \\cdots, J$, 计算\n\n$$\nc_{m j}=\\arg \\min _{c} \\sum_{x_{i} \\in R_{m j}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+c\\right)\n$$\n\n(d) 更新 $f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)$\n\n(3) 得到回归树:\n\n$$\n\\hat{f}(x)=f_{M}(x)=\\sum_{m=1}^{M} \\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)\n$$\n\n算法第 1 步初始化, 估计使损失函数极小化的常数值, 它是只有一个根结点的树。第 2(a) 步计算损失函数的负梯度在当前模型的值, 将它作为残差的估计。对于平方损失函数,它就是通常所说的残差; 对于一般损失函数, 它就是残差的近似值。第 2(b) 步估计回归树叶结点区域, 以拟合残差的近似值。第 2(c) 步利用线性搜索估计叶结点区域的值, 使损失函数极小化。第 $2(\\mathrm{~d})$ 步更新回归树。第 3 步得到输出的最终模型 $\\hat{f}(x)$ 。\n\n\\section*{本 章概要}\n\\begin{enumerate}\n  \\item Boosting 是将弱学习算法提升为强学习算法的机器学习方法。在分类学习中, Boosting 通过反复修改训练数据的权值分布构建一系列基本分类器 (弱分类器), 并将这些基本分类器线性组合, 构成一个强分类器。代表性的 Boosting 是 AdaBoost 算法。\n\\end{enumerate}\n\nAdaBoost 模型是弱分类器的线性组合:\n\n$$\nf(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)\n$$\n\n\\begin{enumerate}\n  \\setcounter{enumi}{1}\n  \\item AdaBoost 算法的特点是通过迭代每次学习一个基本分类器。每次迭代中, 提高那些被前一轮分类器错误分类数据的权值, 而降低那些被正确分类的数据的权值。最后, AdaBoost 将基本分类器的线性组合作为强分类器, 其中给分类误差率小的基本分类器以大的权值, 给分类误差率大的基本分类器以小的权值。\n\n  \\item AdaBoost 的训练误差分析表明, AdaBoost 的每次迭代可以减少它在训练数据集上的分类误差率, 这说明了它作为 Boosting 的有效性。\n\n  \\item AdaBoost 算法的一个解释是该算法实际是前向分步算法的一个实现。在这个方法里,模型是加法模型, 损失函数是指数损失, 算法是前向分步算法。\n\n\\end{enumerate}\n\n每一步中极小化损失函数\n\n$$\n\\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right)\n$$\n\n得到参数 $\\beta_{m}, \\gamma_{m}$ 。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{4}\n  \\item 提升树是以分类树或回归树为基本分类器的 Boosting, 被认为是机器学习中最有效的方法之一。\n\\end{enumerate}\n\n\\section*{继续阅 读}\nBoosting 的介绍可参见文献 [1] 和文献 [2]。PAC 学习可参见文献 [3]。强可学习与弱可学习的关系可参见文献 [4]。关于 AdaBoost 的最初论文是文献 [5]。关于 AdaBoost 的前向分步加法模型的解释参见文献 [6], 提升树与梯度提升可参见文献 [6] 和文献 [7]。AdaBoost 只是用于二类分类, Schapire 与 Singer 将它扩展到多类分类问题 ${ }^{[8]}$ 。AdaBoost 与逻辑斯谛回归的关系也有相关研究 ${ }^{[9]}$ 。\n\n\\section*{习题}\n8.1 某公司招聘职员考查身体、业务能力、发展潜力这 3 项。身体分为合格 1 、不合格 0 两级, 业务能力和发展潜力分为上 1 、中 2 、下 3 三级。分类为合格 1 、不合格 -1 两类。已知 10 个人的数据, 见表 8.5 。假设弱分类器为决策树桩, 试用 AdaBoost 算法学习一个强分类器。\n\n表 8.5 应聘人员情况数据表\n\n\\begin{center}\n\\begin{tabular}{crrrrrrrrrr}\n\\hline\\hline\n & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\\\n\\hline\n身体 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 0 \\\\\n业务能力 & 1 & 3 & 2 & 1 & 2 & 1 & 1 & 1 & 3 & 2 \\\\\n发展潜力 & 3 & 1 & 2 & 3 & 3 & 2 & 2 & 1 & 1 & 1 \\\\\n分类 & -1 & -1 & -1 & -1 & -1 & -1 & 1 & 1 & -1 & -1 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n8.2 比较支持向量机、AdaBoost、逻辑斯谛回归模型的学习策略与算法。\n\n\\section*{参考文献}\n[1] FREUND Y, SCHAPIRE R E. A short introduction to boosting[J]. Journal of Japanese Society for Artificial Intelligence, 1999, 14(5): 771-780.\n\n[2] HASTIE T, TIBSHIRANI R, FRIEDMAN J. The elements of statistical learning: data mining, inference, and prediction[M]. 范明, 柴玉梅, 旨红英, 等译. Springer, 2001.\n\n[3] VALIANT L G. A theory of the learnable[J]. Communications of the ACM, 1984, 27(11): $1134-1142$.\n\n[4] SCHAPIRE R. The strength of weak learnability[J]. Machine Learning, 1990, 5(2): 197-227.\n\n[5] FREUND Y, SCHAPIRE R E. A decision-theoretic generalization of on-line learning and an application to boosting [J]. Lecture Notes in Computer Science, 1995, 904: 23-37.\n\n[6] FRIEDMAN J, HASTIE T, TIBSHIRANI R. Additive logistic regression: a statistical view of boosting (with discussions)[J]. Annals of Statistics, 2000, 28: 337-407.\n\n[7] FRIEDMAN J. Greedy function approximation: a gradient boosting machine[J]. Annals of Statistics, 2001, 29(5): 1189-1232.\n\n[8] SCHAPIRE R E, SINGER Y. Improved boosting algorithms using confidence-rated predictions [J]. Machine Learning, 1999, 37(3): 297-336.\n\n[9] COLLINS M, SCHAPIRE R E, SINGER Y. Logistic regression, AdaBoost and Bregman distances[J]. Machine Learning, 2002, 48(1-3): 253-285.\n\n\\section*{第 9 章 $\\mathrm{EM}$ 算法及其推广}\nEM 算法是一种迭代算法, 1977 年由 Dempster 等人总结提出, 用于含有隐变量 (hidden variable）的概率模型参数的极大似然估计或极大后验概率估计。EM 算法的每次迭代由两步组成: $\\mathrm{E}$ 步, 求期望 (expectation); $\\mathrm{M}$ 步, 求极大（maximization）。所以这一算法称为期望极大算法 (expectation maximization algorithm), 简称 EM 算法。本章首先叙述 EM 算法, 然后讨论 EM 算法的收玫性; 作为 EM 算法的应用, 介绍高斯混合模型的学习; 最后叙述 EM 算法的推广一GEM 算法。\n\n\\section*{$9.1 \\mathrm{EM}$ 算法的引入}\n概率模型有时既含有观测变量 (observable variable), 又含有隐变量或潜在变量 (latent variable）。如果概率模型的变量都是观测变量, 那么给定数据, 可以直接用极大似然估计法或贝叶斯估计法估计模型参数。但是, 当模型含有隐变量时, 就不能简单地使用这些估计方法。EM 算法就是含有隐变量的概率模型参数的极大似然估计法或极大后验概率估计法。我们仅讨论极大似然估计，极大后验概率估计与其类似。"
    },
    {
        "id": "chatcmpl-948RyXTt11AQL2R5uV0Ty0JIaRLb3",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"提升树利用什么算法实现学习的优化过程？\",\n    \"回答1\": \"提升树利用梯度提升（gradient boosting）算法实现学习的优化过程。这是利用最速下降法的近似方法，关键是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树。\",\n    \"问题2\": \"梯度提升算法中的第一步初始化是如何进行的？\",\n    \"回答2\": \"梯度提升算法中的第一步初始化是估计使损失函数极小化的常数值，它是只有一个根结点的树。对于回归问题，初始化公式为 $f_{0}(x)=\\text{arg} \\text{min}_{c} \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ }\\textstyle\\bigg(\\bigg) \\\\sum_{i=1}^{N} L\\big(y_{i}, c\\big)$。\",\n    \"问题3\": \"梯度提升算法中的第二步(a)是如何计算的？\",\n    \"回答3\": \"梯度提升算法中的第二步(a)是对每个样本 $i=1,2,\\\\cdots,N$ 计算残差 $r_{m i}$，其中 $r_{m i}$ 的计算公式为 $r_{m i}=-\\\\left[\\\\frac{\\nabla L\\\\left(y_{i}, f\\\\left(x_{i}\\\\right)\\\\right)}{\\nabla f\\\\left(x_{i}\\\\right)}\\\\right]_{f(x)=f_{m-1}(x)}$。\",\n    \"问题4\": \"梯度提升算法中的第二步(b)是如何进行的？\",\n    \"回答4\": \"梯度提升算法中的第二步(b)是对每个计算得到的残差 $r_{m i}$ 拟合一个回归树，得到第 $m$ 棵树的叶结点区域 $R_{m j}, j=1,2,\\\\cdots,J$。\",\n    \"问题5\": \"梯度提升算法中的第二步(c)是如何进行的？\",\n    \"回答5\": \"梯度提升算法中的第二步(c)是对每个叶结点区域 $R_{m j}$ 计算 $c_{m j}$，使得 $\\nabla c_{m j}=\\\\arg \\\\min _{c} \\\\sum_{x_{i} \\\\in R_{m j}} L\\\\left(y_{i}, f_{m-1}\\\\left(x_{i}\\\\right)+c\\\\right)$。\",\n    \"问题6\": \"梯度提升算法中的第二步(d)是如何更新回归树？\",\n    \"回答6\": \"梯度提升算法中的第二步(d)是更新回归树，得到 $f_{m}(x)=f_{m-1}(x)+\\\\sum_{j=1}^{J} c_{m j} I\\\\left(x \\\\in R_{m j}\\\\right)$。\",\n    \"问题7\": \"Boosting 方法与 AdaBoost 算法有什么关系？\",\n    \"回答7\": \"Boosting 是将弱学习算法提升为强学习算法的机器学习方法。在分类学习中，AdaBoost 算法就是 Boosting 的一种代表性实现。AdaBoost 通过反复修改训练数据的权值分布构建一系列基本分类器，并将这些基本分类器线性组合，构成一个强分类器。\",\n    \"问题8\": \"比较支持向量机、AdaBoost、逻辑斯谛回归模型的学习策略与算法。\",\n    \"回答8\": \"支持向量机、AdaBoost 和逻辑斯谛回归模型都属于监督学习方法。支持向量机通过寻找最大间隔超平面来进行分类；AdaBoost 通过迭代每次学习一个基本分类器，并将这些基本分类器线性组合构成一个强分类器；逻辑斯谛回归模型则通过拟合逻辑斯谛分布来进行分类。它们在学习策略和算法上有所不同，但都是常用的分类和预测模型。\",\n    \"问题9\": \"EM 算法与梯度提升算法有何异同？\",\n    \"回答9\": \"EM 算法和梯度提升算法都属于迭代优化算法，但应用领域和具体实现方式有所不同。EM 算法主要用于含有隐变量的概率模型参数的极大似然估计或极大后验概率估计；而梯度提升算法主要用于优化损失函数，并通过拟合残差不断改进模型。两者在目标函数和迭代更新方式上有明显差异。\",\n    \"问题10\": \"提升树和高斯混合模型有何联系？\",\n    \"回答10\": \"提升树是以分类树或回归树为基本分类器的 Boosting 方法之一，被认为是机器学习中最有效的方法之一。而高斯混合模型则是一种常用的聚类方法，用于对数据进行聚类分析。两者都属于机器学习领域，但应用场景和方法略有不同。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710772822,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1628,
            "prompt_tokens": 5826,
            "total_tokens": 7454
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]