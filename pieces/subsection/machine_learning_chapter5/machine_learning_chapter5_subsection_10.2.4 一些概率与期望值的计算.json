[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter5",
        "title": "10.2.4 一些概率与期望值的计算",
        "content_length": 2248,
        "content": "Title: 10.2.4 一些概率与期望值的计算\nContent:\n利用前向概率和后向概率, 可以得到关于单个状态和两个状态概率的计算公式。\n\n\\begin{enumerate}\n  \\item 给定模型 $\\lambda$ 和观测 $O$, 在时刻 $t$ 处于状态 $q_{i}$ 的概率。记\n\\end{enumerate}\n\n\n\\begin{equation*}\n\\gamma_{t}(i)=P\\left(i_{t}=q_{i} \\mid O, \\lambda\\right) \\tag{10.23}\n\\end{equation*}\n\n\n可以通过前向概率和后向概率计算。事实上,\n\n$$\n\\gamma_{t}(i)=P\\left(i_{t}=q_{i} \\mid O, \\lambda\\right)=\\frac{P\\left(i_{t}=q_{i}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}\n$$\n\n由前向概率 $\\alpha_{t}(i)$ 和后向概率 $\\beta_{t}(i)$ 定义可知:\n\n$$\n\\alpha_{t}(i) \\beta_{t}(i)=P\\left(i_{t}=q_{i}, O \\mid \\lambda\\right)\n$$\n\n于是得到:\n\n\n\\begin{equation*}\n\\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O \\mid \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)} \\tag{10.24}\n\\end{equation*}\n\n\n\\begin{enumerate}\n  \\setcounter{enumi}{1}\n  \\item 给定模型 $\\lambda$ 和观测 $O$, 在时刻 $t$ 处于状态 $q_{i}$ 且在时刻 $t+1$ 处于状态 $q_{j}$ 的概率。记\n\\end{enumerate}\n\n\n\\begin{equation*}\n\\xi_{t}(i, j)=P\\left(i_{t}=q_{i}, i_{t+1}=q_{j} \\mid O, \\lambda\\right) \\tag{10.25}\n\\end{equation*}\n\n\n可以通过前向概率和后向概率计算:\n\n$$\n\\xi_{t}(i, j)=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{P(O \\mid \\lambda)}=\\frac{P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} P\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)}\n$$\n\n而\n\n$$\nP\\left(i_{t}=q_{i}, i_{t+1}=q_{j}, O \\mid \\lambda\\right)=\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)\n$$\n\n所以\n\n\n\\begin{equation*}\n\\xi_{t}(i, j)=\\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)} \\tag{10.26}\n\\end{equation*}\n\n\n\\begin{enumerate}\n  \\setcounter{enumi}{2}\n  \\item 将 $\\gamma_{t}(i)$ 和 $\\xi_{t}(i, j)$ 对各个时刻 $t$ 求和, 可以得到一些有用的期望值。\n\\end{enumerate}\n\n(1) 在观测 $O$ 下状态 $i$ 出现的期望值:\n\n\n\\begin{equation*}\n\\sum_{t=1}^{T} \\gamma_{t}(i) \\tag{10.27}\n\\end{equation*}\n\n\n(2) 在观测 $O$ 下由状态 $i$ 转移的期望值:\n\n\n\\begin{equation*}\n\\sum_{t=1}^{T-1} \\gamma_{t}(i) \\tag{10.28}\n\\end{equation*}\n\n\n(3) 在观测 $O$ 下由状态 $i$ 转移到状态 $j$ 的期望值:\n\n\n\\begin{equation*}\n\\sum_{t=1}^{T-1} \\xi_{t}(i, j) \\tag{10.29}\n\\end{equation*}\n\n\n\\section*{10.3 学习算法}\n根据训练数据是包括观测序列和对应的状态序列还是只有观测序列, 隐马尔可夫模型的学习可以分别由监督学习与无监督学习实现。本节首先介绍监督学习算法, 而后介绍无监督学习算法—B Baum-Welch 算法 (也就是 EM 算法)。"
    },
    {
        "id": "chatcmpl-948gZ1bFap8BNkuQJlejYTanaP9Or",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"利用前向概率和后向概率，如何计算在时刻 $t$ 处于状态 $q_{i}$ 的概率 $\\boldsymbol{\\text{𝛾}}_{t}(i)$？\",\n    \"回答1\": \"在时刻 $t$ 处于状态 $q_{i}$ 的概率 $\\boldsymbol{\\text{𝛾}}_{t}(i)$ 可以通过前向概率和后向概率计算得到，具体公式为 $\\boldsymbol{\\text{𝛾}}_{t}(i)=\\frac{\\boldsymbol{\\text{𝛼}}_{t}(i) \\boldsymbol{\\text{𝛽}}_{t}(i)}{P(O \\text{ | } \\boldsymbol{\\text{𝜆}})}=\\frac{\\boldsymbol{\\text{𝛼}}_{t}(i) \\boldsymbol{\\text{𝛽}}_{t}(i)}{\\text{∑}_{j=1}^{N} \\boldsymbol{\\text{𝛼}}_{t}(j) \\boldsymbol{\\text{𝛽}}_{t}(j)}$。\",\n    \"问题2\": \"如何计算在时刻 $t$ 处于状态 $q_{i}$ 且在时刻 $t+1$ 处于状态 $q_{j}$ 的概率 $\\boldsymbol{\\text{ξ}}_{t}(i, j)$？\",\n    \"回答2\": \"在时刻 $t$ 处于状态 $q_{i}$ 且在时刻 $t+1$ 处于状态 $q_{j}$ 的概率 $\\boldsymbol{\\text{ξ}}_{t}(i, j)$ 可以通过前向概率和后向概率计算得到，具体公式为 $\\boldsymbol{\\text{ξ}}_{t}(i, j)=\\frac{\\boldsymbol{\\text{𝛼}}_{t}(i) a_{ij} b_j(o_{t+1}) \\boldsymbol{\\text{𝛽}}_{t+1}(j)}{\\text{∑}_{i=1}^{N} \\text{∑}_{j=1}^{N} \\boldsymbol{\\text{𝛼}}_{t}(i) a_{ij} b_j(o_{t+1}) \\boldsymbol{\\text{𝛽}}_{t+1}(j)}$。\",\n    \"问题3\": \"如何计算在观测 $O$ 下状态 $i$ 出现的期望值？\",\n    \"回答3\": \"在观测 $O$ 下状态 $i$ 出现的期望值可以通过将 $\\boldsymbol{\\text{𝛾}}_{t}(i)$ 对各个时刻 $t$ 求和得到，具体公式为 $\\text{∑}_{t=1}^{T} \\boldsymbol{\\text{𝛾}}_{t}(i)$。\",\n    \"问题4\": \"如何计算在观测 $O$ 下由状态 $i$ 转移的期望值？\",\n    \"回答4\": \"在观测 $O$ 下由状态 $i$ 转移的期望值可以通过将 $\\boldsymbol{\\text{𝛾}}_{t}(i)$ 对各个时刻 $t$ 求和得到，具体公式为 $\\text{∑}_{t=1}^{T-1} \\boldsymbol{\\text{𝛾}}_{t}(i)$。\",\n    \"问题5\": \"如何计算在观测 $O$ 下由状态 $i$ 转移到状态 $j$ 的期望值？\",\n    \"回答5\": \"在观测 $O$ 下由状态 $i$ 转移到状态 $j$ 的期望值可以通过将 $\\boldsymbol{\\text{ξ}}_{t}(i, j)$ 对各个时刻 $t$ 求和得到，具体公式为 $\\text{∑}_{t=1}^{T-1} \\boldsymbol{\\text{ξ}}_{t}(i, j)$。\",\n    \"问题6\": \"隐马尔可夫模型的学习可以分别由哪两种方式实现？\",\n    \"回答6\": \"隐马尔可夫模型的学习可以分别由监督学习与无监督学习实现。监督学习是指训练数据包括观测序列和对应的状态序列，而无监督学习是指训练数据只有观测序列。\",\n    \"问题7\": \"监督学习算法与无监督学习算法分别是如何实现隐马尔可夫模型的学习的？\",\n    \"回答7\": \"监督学习算法是根据包括观测序列和对应的状态序列的训练数据来进行模型参数估计，而无监督学习算法（如 B Baum-Welch 算法）则是根据只有观测序列的训练数据来进行模型参数估计，通常采用 EM 算法来实现。\",\n    \"问题8\": \"什么是监督学习算法？\",\n    \"回答8\": \"监督学习算法是一种机器学习方法，其训练数据包括输入数据和对应的标签（或输出），通过学习输入数据与输出之间的映射关系来进行模型训练和参数估计。\",\n    \"问题9\": \"什么是无监督学习算法？\",\n    \"回答9\": \"无监督学习算法是一种机器学习方法，其训练数据只包括输入数据而没有对应的标签（或输出），通过对输入数据的内在结构或特征进行学习来进行模型训练和参数估计。\",\n    \"问题10\": \"B Baum-Welch 算法又被称为什么名字？\",\n    \"回答10\": \"B Baum-Welch 算法也被称为 EM 算法，它是一种无监督学习算法，用于隐马尔可夫模型中参数的估计。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710773727,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1566,
            "prompt_tokens": 3861,
            "total_tokens": 5427
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]