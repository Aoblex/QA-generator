[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.2.3.3 周期性学习率调整",
        "content_length": 2061,
        "content": "Title: 7.2.3.3 周期性学习率调整\nContent:\n为了使得梯度下降法能够逃离鞍点或尖锐最小值, 一种经验性的方式是在训练过程中周期性地增大学习率. 当参数处于尖锐最小值附近时, 增大学习率有助于逃离尖锐最小值; 当参数处于平坦最小值附近时, 增大学习率依然有可能在该平坦最小值的吸引域 (Basin of Attraction) 内. 因此, 周期性地增大学习率虽然可能短期内损害优化过程, 使得网络收玫的稳定性变差, 但从长期来看有助于找到更好的局部最优解.\n\n本节介绍两种常用的周期性调整学习率的方法: 循环学习率和带热重启的随机梯度下降.\n\n循环学习率 一种简单的方法是使用循环学习率 (Cyclic Learning Rate ) [Goyal et al., 2017], 即让学习率在一个区间内周期性地增大和缩小. 通常可以使用线性缩放来调整学习率, 称为三角循环学习率 (Triangular Cyclic Learning Rate).假设每个循环周期的长度相等都为 $2 \\Delta T$, 其中前 $\\Delta T$ 步为学习率线性增大阶段,后 $\\Delta T$ 步为学习率线性缩小阶段. 在第 $t$ 次迭代时, 其所在的循环周期数 $m$ 为\n\n\n\\begin{equation*}\nm=\\left\\lfloor 1+\\frac{t}{2 \\Delta T}\\right\\rfloor \\tag{7.10}\n\\end{equation*}\n\n\n其中 $[\\cdot \\mid$ 表示“向下取整”函数. 第 $t$ 次迭代的学习率为\n\n\n\\begin{equation*}\n\\alpha_{t}=\\alpha_{\\text {min }}^{m}+\\left(\\alpha_{\\text {max }}^{m}-\\alpha_{\\text {min }}^{m}\\right)(\\max (0,1-b)) \\tag{7.11}\n\\end{equation*}\n\n\n其中 $\\alpha_{m a x}^{m}$ 和 $\\alpha_{\\text {min }}^{m}$ 分别为第 $m$ 个周期中学习率的上界和下界, 可以随着 $m$ 的增大而逐渐降低; $b \\in[0,1]$ 的计算为\n\n\n\\begin{equation*}\nb=\\left|\\frac{t}{\\Delta T}-2 m+1\\right| \\text {. } \\tag{7.12}\n\\end{equation*}\n\n\n带热重启的随机梯度下降 带热重启的随机梯度下降 (Stochastic Gradient Descent with Warm Restarts,SGDR ) [Loshchilov et al., 2017a] 是用热重启方式来 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n替代学习率衰减的方法. 学习率每间隔一定周期后重新初始化为某个预先设定值, 然后逐渐衰减. 每次重启后模型参数不是从头开始优化, 而是从重启前的参数基础上继续优化.\n\n假设在梯度下降过程中重启 $M$ 次, 第 $m$ 次重启在上次重启开始第 $T_{m}$ 个回合后进行, $T_{m}$ 称为重启周期. 在第 $m$ 次重启之前, 采用余弦衰减来降低学习率.第 $t$ 次迭代的学习率为\n\n\n\\begin{equation*}\n\\alpha_{t}=\\alpha_{\\text {min }}^{m}+\\frac{1}{2}\\left(\\alpha_{\\text {max }}^{m}-\\alpha_{\\text {min }}^{m}\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{m}} \\pi\\right)\\right), \\tag{7.13}\n\\end{equation*}\n\n\n其中 $\\alpha_{\\text {max }}^{m}$ 和 $\\alpha_{\\text {min }}^{m}$ 分别为第 $m$ 个周期中学习率的上界和下界, 可以随着 $m$ 的增大而逐渐降低; $T_{\\text {cur }}$ 为从上次重启之后的回合 (Epoch) 数. $T_{\\text {cur }}$ 可以取小数, 比如 $0.1 、 0.2$ 等, 这样可以在一个回合内部进行学习率衰减. 重启周期 $T_{m}$ 可以随着重启次数逐渐增加, 比如 $T_{m}=T_{m-1} \\times \\kappa$, 其中 $\\kappa \\geq 1$ 为放大因子.\n\n图7.5给出了两种周期性学习率调整的示例 (假设初始学习率为 1 ), 每个周期中学习率的上界也逐步衰减.\n\n\n\n(a) 三角循环学习率\n\n\n\n(b) 带热重启的余弦衰减\n\n图 7.5 周期性学习率调整"
    },
    {
        "id": "chatcmpl-9433vewM8j6v6h8mhQ5qGLVJ0ankt",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"为什么在训练过程中周期性地增大学习率有助于逃离尖锐最小值？\",\n    \"回答1\": \"当参数处于尖锐最小值附近时，增大学习率有助于逃离尖锐最小值，因为较大的学习率能够使参数跳出局部最小值并继续向更优的解决方案探索。\",\n    \"问题2\": \"在循环学习率中，如何计算第 $t$ 次迭代的学习率？\",\n    \"回答2\": \"在循环学习率中，第 $t$ 次迭代的学习率 $\\boldsymbol{\\text{α}}_{\\text{t}}$ 由公式 $\\boldsymbol{\\text{α}}_{\\text{t}}=\\boldsymbol{\\text{α}}_{\\text{min}}^{m}+\\bigg(\\boldsymbol{\\text{α}}_{\\text{max}}^{m}-\\boldsymbol{\\text{α}}_{\\text{min}}^{m}\\bigg)\\bigg(\\text{max}\\big(0,1-b\\big)\\bigg)$ 计算得到，其中 $\\boldsymbol{\\text{α}}_{\\text{max}}^{m}$ 和 $\\boldsymbol{\\text{α}}_{\\text{min}}^{m}$ 分别为第 $m$ 个周期中学习率的上界和下界，$b \\text{ 的计算为 } b=\\big|\\frac{t}{\\boldsymbol{\\text{ΔT}}}-2m+1\\big|$。\",\n    \"问题3\": \"带热重启的随机梯度下降中，如何计算第 $t$ 次迭代的学习率？\",\n    \"回答3\": \"在带热重启的随机梯度下降中，第 $t$ 次迭代的学习率 $\\boldsymbol{\\text{α}}_{\\text{t}}$ 由公式 $\\boldsymbol{\\text{α}}_{\\text{t}}=\\boldsymbol{\\text{α}}_{\\text{min}}^{m}+\\frac{1}{2}\\bigg(\\boldsymbol{\\text{α}}_{\\text{max}}^{m}-\\boldsymbol{\\text{α}}_{\\text{min}}^{m}\\bigg)\\bigg(1+\\text{cos}\\big(\\frac{T_{cur}}{T_{m}} \\boldsymbol{\\text{π}}\\big)\\bigg)$ 计算得到，其中 $\\boldsymbol{\\text{α}}_{\\text{max}}^{m}$ 和 $\\boldsymbol{\\text{α}}_{\\text{min}}^{m}$ 分别为第 $m$ 个周期中学习率的上界和下界，$T_{cur}$ 为从上次重启之后的回合数。\",\n    \"问题4\": \"周期性增大学习率可能短期内损害优化过程，但从长期来看有什么好处？\",\n    \"回答4\": \"周期性增大学习率可能短期内损害优化过程，使得网络收敛的稳定性变差。但从长期来看，这有助于找到更好的局部最优解，因为较大的学习率能够帮助模型跳出局部最小值并继续向更优的解决方案探索。\",\n    \"问题5\": \"循环学习率中的三角循环学习率是如何定义的？\",\n    \"回答5\": \"循环学习率中的三角循环学习率是通过让学习率在一个区间内周期性地增大和缩小来定义的。通常可以使用线性缩放来调整学习率，称为三角循环学习率。假设每个循环周期的长度相等都为 $2 \\boldsymbol{\\text{ΔT}}$，其中前 $\\boldsymbol{\\text{ΔT}}$ 步为学习率线性增大阶段，后 $\\boldsymbol{\\text{ΔT}}$ 步为学习率线性缩小阶段。\",\n    \"问题6\": \"带热重启的随机梯度下降中的重启周期 $T_{m}$ 是如何设定的？\",\n    \"回答6\": \"在带热重启的随机梯度下降中，重启周期 $T_{m}$ 可以随着重启次数逐渐增加，比如 $T_{m}=T_{m-1} \\times \\boldsymbol{\\text{κ}}$，其中 $\\boldsymbol{\\text{κ}} \\text{ 为放大因子}$。\",\n    \"问题7\": \"周期性学习率调整方法有哪两种常用的？\",\n    \"回答7\": \"两种常用的周期性调整学习率的方法是循环学习率和带热重启的随机梯度下降。\",\n    \"问题8\": \"为什么带热重启的随机梯度下降被称为带热重启？\",\n    \"回答8\": \"带热重启的随机梯度下降被称为带热重启是因为该方法采用热重启方式来替代学习率衰减的方法。在该方法中，学习率每间隔一定周期后重新初始化为某个预先设定值，然后逐渐衰减。每次重启后模型参数不是从头开始优化，而是从重启前的参数基础上继续优化。\",\n    \"问题9\": \"周期性学习率调整对于优化过程有何影响？\",\n    \"回答9\": \"周期性地调整学习率可能会短期内损害优化过程，使得网络收敛的稳定性变差。但从长期来看，这有助于找到更好的局部最优解，并提高模型在复杂任务上的泛化能力。\",\n    \"问题10\": \"图 7.5 中展示了哪两种周期性学习率调整方法的示例？\",\n    \"回答10\": \"图 7.5 中展示了三角循环学习率和带热重启的余弦衰减这两种周期性学习率调整方法的示例。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710752111,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1679,
            "prompt_tokens": 4034,
            "total_tokens": 5713
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]