[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "8.5.2 神经图灵机",
        "content_length": 2910,
        "content": "Title: 8.5.2 神经图灵机\nContent:\n图灵机 图灵机 (Turing Machine) 是图灵在1936 年提出的一种抽象数学模型,可以用来模拟任何可计算问题 [Turing, 1937]. 图灵机的结构如图8.8所示, 其中控制器包括状态寄存器、控制规则.\n\n\n\n图 8.8 图灵机结构示例\n\n图灵机由以下几个组件构成:\n\n（1）一条无限长的纸带: 纸带上有一个个方格, 每个方格可以存储一个符号.\n\n（2）一个符号表: 纸带上可能出现的所有符号的集合, 包含一个特殊的空白符.\n\n（3）一个读写头: 指向纸带上某个方格的指针, 每次可以向左或右移动一个位置,并可以读取、擦除、写入当前方格中的内容.\n\n（4）一个状态寄存器: 用来保存图灵机当前所处的状态, 其中包含两个特殊的状态: 起始状态和终止状态.\n\n(5) 一套控制规则: 根据当前机器所处的状态以及当前读写头所指的方格上的符号来确定读写头下一步的动作, 令机器进入一个新的状态.\n\n神经图灵机 神经图灵机 (Neural Turing Machine,NTM ) [Graves et al., 2014]主要由两个部件构成: 控制器和外部记忆. 外部记忆定义为矩阵 $M \\in \\mathbb{R}^{D \\times N}$, 这里 $N$ 是记忆片段的数量, $D$ 是每个记忆片段的大小. 控制器为一个前馈或循环神经网络. 神经图灵机中的外部记忆是可读写的,其结构如图8.9所示.\n\n在每个时刻 $t$, 控制器接受当前时刻的输入 $\\boldsymbol{x}_{t}$ 、上一时刻的输出 $\\boldsymbol{h}_{t-1}$ 和上一时刻从外部记忆中读取的信息 $\\boldsymbol{r}_{t-1}$, 并产生输出 $\\boldsymbol{h}_{t}$, 同时生成和读写外部记忆相关的三个向量: 查询向量 $\\boldsymbol{q}_{t}$ 、删除向量 $\\boldsymbol{e}_{t}$ 和增加向量 $\\boldsymbol{a}_{t}$. 然后对外部记忆 $\\mathcal{M}_{t}$ 进行读写操作,生成读向量 $\\boldsymbol{r}_{t}$ 和新的外部记忆 $M_{t+1}$.\n\n读操作 在时刻 $t$, 外部记忆的内容记为 $\\boldsymbol{M}_{t}=\\left[\\boldsymbol{m}_{t, 1}, \\cdots, \\boldsymbol{m}_{t, N}\\right]$, 读操作为从外部记忆 $\\boldsymbol{M}_{t}$ 中读取信息 $\\boldsymbol{r}_{t} \\in \\mathbb{R}^{D}$.\n\n首先通过注意力机制来进行基于内容的寻址,即\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\\begin{equation*}\n\\alpha_{t, n}=\\operatorname{softmax}\\left(s\\left(\\boldsymbol{m}_{t, n}, \\boldsymbol{q}_{t}\\right)\\right) \\tag{8.29}\n\\end{equation*}\n\n\n神经图灵机中还实现了比较复杂的基于位置的寻址方式。这里我们只介绍比较简单的基于内容的寻址方式,整个框架不变.\n\n\n\n图 8.9 神经图灵机示例\n\n其中 $\\boldsymbol{q}_{t}$ 为控制器产生的查询向量, 用来进行基于内容的寻址. 函数 $s(\\cdot, \\cdot)$ 为加性或乘性的打分函数. 注意力分布 $\\alpha_{t, n}$ 是记忆片段 $\\boldsymbol{m}_{t, n}$ 对应的权重, 并满足 $\\sum_{n=1}^{N} \\alpha_{t, n}=1$.\n\n根据注意力分布 $\\alpha_{t}$, 可以计算读向量 ( read vector) $\\boldsymbol{r}_{t}$ 作为下一个时刻控制器的输入.\n\n\n\\begin{equation*}\n\\boldsymbol{r}_{t}=\\sum_{n=1}^{N} \\alpha_{n} \\boldsymbol{m}_{t, n} \\tag{8.30}\n\\end{equation*}\n\n\n写操作 外部记忆的写操作可以分解为两个子操作: 删除和增加.\n\n首先, 控制器产生删除向量 ( erase vector) $\\boldsymbol{e}_{t}$ 和增加向量 (add vector) $\\boldsymbol{a}_{t}$,分别为要从外部记忆中删除的信息和要增加的信息. 删除操作是根据注意力分布来按比例地在每个记忆片段中删除 $\\boldsymbol{e}_{t}$, 增加操作是根据注意力分布来按比例地给每个记忆片段加入 $\\boldsymbol{a}_{t}$. 具体过程如下:\n\n\n\\begin{equation*}\n\\boldsymbol{m}_{t+1, n}=\\boldsymbol{m}_{t, n}\\left(\\mathbf{1}-\\alpha_{t, n} \\boldsymbol{e}_{t}\\right)+\\alpha_{t, n} \\boldsymbol{a}_{t}, \\quad \\forall n \\in[1, N] \\tag{8.31}\n\\end{equation*}\n\n\n通过写操作得到下一时刻的外部记忆 $\\boldsymbol{M}_{t+1}$.\n\n\\section*{8.6 基于神经动力学的联想记忆}\n结构化的外部记忆更多是受现代计算机架构的启发, 将计算和存储功能进行分离, 这些外部记忆的结构也缺乏生物学的解释性. 为了具有更好的生物学解 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n释性, 还可以将基于神经动力学 (Neurodynamics) 的联想记忆模型引入到神经网络以增加网络容量.\n\n联想记忆模型 (Associative Memory Model) 主要是通过神经网络的动态演化来进行联想,有两种应用场景.\n\n1 ) 输入的模式和输出的模式在同一空间, 这种模型叫作自联想模型 (AutoAssociative Model). 自联想模型可以通过前馈神经网络或者循环神经网络来实现, 也常称为自编码器 (Auto-Encoder, AE) . 2) 输入的模式和输出的模式不在同一空间, 这种模型叫作异联想模型 (Hetero-Associative Model ). 从广义上讲, 大部分机器学习问题都可以被看作异联想, 因此异联想模型可以作为分类器使用.\n\n联想记忆模型可以利用神经动力学的原理来实现按内容寻址的信息存储和检索.一个经典的联想记忆模型为 Hopfield 网络."
    },
    {
        "id": "chatcmpl-944YULsGF6OD3WhsApPeSJRqJRzZN",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"图灵机和神经图灵机的结构有何区别？\",\n    \"回答1\": \"图灵机包括无限长的纸带、符号表、读写头、状态寄存器和控制规则。而神经图灵机主要由控制器和外部记忆构成，外部记忆是可读写的矩阵，而控制器为一个前馈或循环神经网络。这标志着神经图灵机引入了神经网络的结构和外部记忆的概念，与传统图灵机在结构上存在明显区别。\",\n    \"问题2\": \"神经图灵机中的读操作是如何实现的？\",\n    \"回答2\": \"在神经图灵机中，读操作首先通过注意力机制进行基于内容的寻址，计算得到注意力分布 $\\boldsymbol{r}_{t}$，然后根据注意力分布计算读向量 $\\boldsymbol{r}_{t}$ 作为下一个时刻控制器的输入，具体计算公式为 $\\boldsymbol{r}_{t}=\\text{∑}_{n=1}^{N} \\boldsymbol{\\text{α}}_{n} \\boldsymbol{m}_{t, n}$。\",\n    \"问题3\": \"神经图灵机中的写操作是如何实现的？\",\n    \"回答3\": \"神经图灵机中的写操作包括删除和增加两个子操作。首先，控制器产生删除向量（erase vector）$\\boldsymbol{e}_{t}$ 和增加向量（add vector）$\\boldsymbol{a}_{t}$，然后根据注意力分布来按比例地在每个记忆片段中进行删除和增加操作，具体计算公式为 $\\boldsymbol{m}_{t+1, n}=\\boldsymbol{m}_{t, n}\\big(\\textbf{1}-\\boldsymbol{\\text{α}}_{t, n} \\boldsymbol{e}_{t}\\big)+\\boldsymbol{\\text{α}}_{t, n} \\boldsymbol{a}_{t}$。\",\n    \"问题4\": \"神经图灵机中的外部记忆是如何定义的？\",\n    \"回答4\": \"神经图灵机中的外部记忆被定义为矩阵 $M \\text{ 且 } M \\text{ 为 } \\textbf{R}^{D \\times N}$，其中 $N$ 是记忆片段的数量，$D$ 是每个记忆片段的大小。这标志着外部记忆以矩阵形式存在，并且具有可读写的特性。\",\n    \"问题5\": \"基于神经动力学的联想记忆模型有哪些应用场景？\",\n    \"回答5\": \"基于神经动力学的联想记忆模型主要有两种应用场景。一种是自联想模型（Auto-Associative Model），另一种是异联想模型（Hetero-Associative Model）。自联想模型可以通过前馈神经网络或者循环神经网络来实现，也常称为自编码器（Auto-Encoder, AE）。而异联想模型可以作为分类器使用。这表明基于神经动力学的联想记忆模型具有广泛的应用前景。\",\n    \"问题6\": \"为什么需要将基于神经动力学的联想记忆模型引入到神经网络中？\",\n    \"回答6\": \"结构化的外部记忆更多是受现代计算机架构的启发，将计算和存储功能进行分离。但这些外部记忆的结构缺乏生物学的解释性。为了具有更好的生物学解释性，可以将基于神经动力学的联想记忆模型引入到神经网络以增加网络容量。这意味着引入该模型可以使得神经网络具有更接近生物大脑的特性，并且能够更好地处理复杂信息和实现联想记忆功能。\",\n    \"问题7\": \"Hopfield 网络是什么？\",\n    \"回答7\": \"Hopfield 网络是一个经典的联想记忆模型，通过神经网络的动态演化来进行联想。它利用神经动力学的原理来实现按内容寻址的信息存储和检索。Hopfield 网络在模式识别和联想记忆领域有着重要的应用价值。\",\n    \"问题8\": \"什么是自编码器（Auto-Encoder, AE）？\",\n    \"回答8\": \"自编码器是一种用于学习数据表示（即编码）的神经网络模型。它通过将输入数据进行编码和解码，从而实现对数据特征的提取和重建。自编码器常用于特征提取、降维和数据重建等任务，在深度学习领域有着广泛的应用。\",\n    \"问题9\": \"什么是异联想模型（Hetero-Associative Model）？\",\n    \"回答9\": \"异联想模型是指输入的模式和输出的模式不在同一空间的联想记忆模型。从广义上讲，大部分机器学习问题都可以被看作异联想，因此异联想模型可以作为分类器使用。这表明异联想模型具有处理不同空间数据关联性强、分类任务等多种应用场景。\",\n    \"问题10\": \"基于内容寻址方式在神经图灵机中起到了什么作用？\",\n    \"回答10\": \"基于内容寻址方式在神经图灵机中起到了信息存储和检索的作用。通过注意力机制进行基于内容寻址，可以实现对外部记忆中信息的读取和写入操作，从而使得神经图灵机能够更好地处理复杂信息和实现联想记忆功能。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710757850,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1646,
            "prompt_tokens": 4512,
            "total_tokens": 6158
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]