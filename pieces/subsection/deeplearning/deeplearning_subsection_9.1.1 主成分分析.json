[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "9.1.1 主成分分析",
        "content_length": 3119,
        "content": "Title: 9.1.1 主成分分析\nContent:\n主成分分析 (Principal Component Analysis, PCA) 是一种最常用的数据降维方法, 使得在转换后的空间中数据的方差最大. 如图9.1所示的两维数据, 如果将这些数据投影到一维空间中, 选择数据方差最大的方向进行投影, 才能最大化数据的差异性,保留更多的原始数据信息.\n\n\n\n图 9.1 主成分分析\n\n假设有一组 $D$ 维的样本 $\\boldsymbol{x}^{(n)} \\in \\mathbb{R}^{D}, 1 \\leq n \\leq N$, 我们希望将其投影到一维空间中, 投影向量为 $\\boldsymbol{w} \\in \\mathbb{R}^{D}$. 不失一般性, 我们限制 $\\boldsymbol{w}$ 的模为 1 , 即 $\\boldsymbol{w}^{\\top} \\boldsymbol{w}=1$. 每个样本点 $\\boldsymbol{x}^{(n)}$ 投影之后的表示为\n\n\n\\begin{equation*}\nz^{(n)}=\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)} \\tag{9.1}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n用矩阵 $\\boldsymbol{X}=\\left[\\boldsymbol{x}^{(1)}, \\boldsymbol{x}^{(2)}, \\cdots, \\boldsymbol{x}^{(N)}\\right]$ 表示输入样本, $\\overline{\\boldsymbol{x}}=\\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}$ 为原始样本的中心点, 所有样本投影后的方差为\n\n\n\\begin{align*}\n\\sigma(\\boldsymbol{X} ; \\boldsymbol{w}) & =\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}-\\boldsymbol{w}^{\\top} \\overline{\\boldsymbol{x}}\\right)^{2}  \\tag{9.2}\\\\\n& =\\frac{1}{N}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{X}-\\boldsymbol{w}^{\\top} \\overline{\\boldsymbol{X}}\\right)\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{X}-\\boldsymbol{w}^{\\top} \\overline{\\boldsymbol{X}}\\right)^{\\top}  \\tag{9.3}\\\\\n& =\\boldsymbol{w}^{\\top} \\boldsymbol{\\Sigma} \\boldsymbol{w}, \\tag{9.4}\n\\end{align*}\n\n\n其中 $\\overline{\\boldsymbol{X}}=\\overline{\\boldsymbol{x}} 1_{D}^{\\top}$ 是向量 $\\overline{\\boldsymbol{x}}$ 和 $D$ 维全 1 向量 $\\mathbf{1}_{D}$ 的外积, 即有 $D$ 列 $\\overline{\\boldsymbol{x}}$ 组成的矩阵， $\\boldsymbol{\\Sigma}=\\frac{1}{N}(\\boldsymbol{X}-\\overline{\\boldsymbol{X}})(\\boldsymbol{X}-\\overline{\\boldsymbol{X}})^{\\top}$ 是原始样本的协方差矩阵.\n\n最大化投影方差 $\\sigma(\\boldsymbol{X} ; \\boldsymbol{w})$ 并满足 $\\boldsymbol{w}^{\\top} \\boldsymbol{w}=1$, 利用拉格朗日方法转换为无约束优化问题,\n\n\n\\begin{equation*}\n\\max _{\\boldsymbol{w}} \\boldsymbol{w}^{\\top} \\boldsymbol{\\Sigma} \\boldsymbol{w}+\\lambda\\left(1-\\boldsymbol{w}^{\\top} \\boldsymbol{w}\\right) \\tag{9.5}\n\\end{equation*}\n\n\n其中 $\\lambda$ 为拉格朗日乘子. 对上式求导并令导数等于 0 , 可得\n\n\n\\begin{equation*}\n\\boldsymbol{\\Sigma} \\boldsymbol{w}=\\lambda \\boldsymbol{w} . \\tag{9.6}\n\\end{equation*}\n\n\n从上式可知, $\\boldsymbol{w}$ 是协方差矩阵 $\\boldsymbol{\\Sigma}$ 的特征向量, $\\lambda$ 为特征值. 同时\n\n\n\\begin{equation*}\n\\sigma(\\boldsymbol{X} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{\\Sigma} \\boldsymbol{w}=\\boldsymbol{w}^{\\top} \\lambda \\boldsymbol{w}=\\lambda \\tag{9.7}\n\\end{equation*}\n\n\n$\\lambda$ 也是投影后样本的方差. 因此, 主成分分析可以转换成一个矩阵特征值分解问题,投影向量 $\\boldsymbol{w}$ 为矩阵 $\\boldsymbol{\\Sigma}$ 的最大特征值对应的特征向量.\n\n如果要通过投影矩阵 $\\boldsymbol{W} \\in R^{D \\times D^{\\prime}}$ 将样本投到 $D^{\\prime}$ 维空间，投影矩阵满足 $\\boldsymbol{W}^{\\top} \\boldsymbol{W}=\\boldsymbol{I}$ 为单位阵, 只需要将 $\\boldsymbol{\\Sigma}$ 的特征值从大到小排列, 保留前 $D^{\\prime}$ 个特征向量,其对应的特征向量即是最优的投影矩阵.\n\n\n\\begin{equation*}\n\\boldsymbol{\\Sigma} \\boldsymbol{W}=\\boldsymbol{W} \\operatorname{diag}(\\boldsymbol{\\lambda}) \\tag{9.8}\n\\end{equation*}\n\n\n其中 $\\lambda=\\left[\\lambda_{1}, \\cdots, \\lambda_{D^{\\prime}}\\right]$ 为 $S$ 的前 $D^{\\prime}$ 个最大的特征值.\n\n主成分分析是一种无监督学习方法, 可以作为监督学习的数据预处理方法,用来去除噪声并减少特征之间的相关性, 但是它并不能保证投影后数据的类别可分性更好. 提高两类可分性的方法一般为监督学习方法, 比如线性判别分析参见习题9-3. ( Linear Discriminant Analysis, LDA )."
    },
    {
        "id": "chatcmpl-944aauMlZVi3jOXinYfl92GOJs1hq",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"主成分分析（PCA）的主要目标是什么？\",\n    \"回答1\": \"主成分分析的主要目标是通过数据投影到新的空间中，使得数据在转换后的空间中的方差最大化。这样可以最大化数据的差异性，保留更多的原始数据信息。\",\n    \"问题2\": \"在主成分分析中，如何表示每个样本点 $\\boldsymbol{x}^{(n)}$ 投影后的结果？\",\n    \"回答2\": \"每个样本点 $\\boldsymbol{x}^{(n)}$ 投影后的结果表示为 $z^{(n)}=\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}$，其中 $\\boldsymbol{w}$ 为投影向量。\",\n    \"问题3\": \"主成分分析中的投影后样本的方差是如何计算的？\",\n    \"回答3\": \"主成分分析中投影后样本的方差由公式 $\\boldsymbol{w}^{\\top} \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{X}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}$ 计算得到。\",\n    \"问题4\": \"在主成分分析中，如何最大化投影方差并满足约束条件？\",\n    \"回答4\": \"为了最大化投影方差并满足约束条件 $\\boldsymbol{w}^{\\top} \\boldsymbol{w}=1$，可以利用拉格朗日方法将其转换为无约束优化问题，并得到优化目标函数 $\\boldsymbol{w}^{\\top} \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{w}^{\\top} \\boldsymbol{w}=1}$。\",\n    \"问题5\": \"在主成分分析中，如何得到投影向量 $\\boldsymbol{w}$ ？\",\n    \"回答5\": \"通过对优化目标函数进行求导并令导数等于 0，可以得到 $\\boldsymbol{w}$ 是协方差矩阵 $\\boldsymbol{\\boldsymbol{w}=\\frac{1}{N}\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(\\big(X}-\\bar{x}\\bar{x}^T\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\big)\\right)$。\",\n    \"问题6\": \"在主成分分析中，如何将样本投影到 $D^{\\text{'}}$ 维空间？\",\n    \"回答6\": \"要将样本投影到 $D^{\\text{'}}$ 维空间，可以通过保留协方差矩阵 $\\boldsymbol{X}$ 的前 $D^{\\text{'}}$ 个最大特征值对应的特征向量，构建投影矩阵 $\\boldsymbol{W}$。这样可以实现将样本投影到更低维度的空间。\",\n    \"问题7\": \"主成分分析适用于哪种类型的学习方法？\",\n    \"回答7\": \"主成分分析是一种无监督学习方法，可作为监督学习的数据预处理方法，用来去除噪声并减少特征之间的相关性。然而，它并不能保证投影后数据的类别可分性更好。提高两类可分性的方法一般为监督学习方法。\",\n    \"问题8\": \"在主成分分析中，如何提高两类可分性？\",\n    \"回答8\": \"要提高两类可分性一般需要采用监督学习方法，例如线性判别分析（LDA）。LDA 可以有效地提高数据的类别可分性，相比于无监督学习方法具有更强的分类能力。\",\n    \"问题9\": \"主成分分析与线性判别分析（LDA）有何区别？\",\n    \"回答9\": \"主成分分析是一种无监督学习方法，其主要目标是通过数据投影到新的空间中使得数据在转换后的空间中的方差最大化。而线性判别分析（LDA）是一种监督学习方法，其目标是寻找一个投影方向，使得同类样本的投影点尽可能接近，不同类样本的投影点尽可能远离。\",\n    \"问题10\": \"在主成分分析中，如何表示协方差矩阵 $\\boldsymbol{\\text{Σ}}$ 的特征值和特征向量？\",\n    \"回答10\": \"协方差矩阵 $\\text{Σ}$ 的特征值由向量 $\\text{λ}=\\text{[λ}_1\\text{, …, λ}_{D^{'}}\\text{]}$ 表示，而对应的特征向量构成了投影矩阵 $\\text{W}$。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710757980,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1454,
            "prompt_tokens": 4218,
            "total_tokens": 5672
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]