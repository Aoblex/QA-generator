[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "9.1.3 自编码器",
        "content_length": 2157,
        "content": "Title: 9.1.3 自编码器\nContent:\n自编码器 (Auto-Encoder, AE ) 是通过无监督的方式来学习一组数据的有效编码 (或表示).\n\n假设有一组 $D$ 维的样本 $\\boldsymbol{x}^{(n)} \\in \\mathbb{R}^{D}, 1 \\leq n \\leq N$, 自编码器将这组数据映射到特征空间得到每个样本的编码 $\\boldsymbol{z}^{(n)} \\in \\mathbb{R}^{M}, 1 \\leq n \\leq N$, 并且希望这组编码可以重构出原来的样本.\n\n自编码器的结构可分为两部分:\n\n(1) 编码器 ( Encoder) $f: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{M}$.\n\n(2) 解码器 ( Decoder) $g: \\mathbb{R}^{M} \\rightarrow \\mathbb{R}^{D}$.\n\n自编码器的学习目标是最小化重构错误 ( Reconstruction Error) :\n\n\n\\begin{align*}\n\\mathcal{L} & =\\sum_{n=1}^{N}\\left\\|\\boldsymbol{x}^{(n)}-g\\left(f\\left(\\boldsymbol{x}^{(n)}\\right)\\right)\\right\\|^{2}  \\tag{9.18}\\\\\n& =\\sum_{n=1}^{N}\\left\\|\\boldsymbol{x}^{(n)}-f \\circ g\\left(\\boldsymbol{x}^{(n)}\\right)\\right\\|^{2} \\tag{9.19}\n\\end{align*}\n\n\n如果特征空间的维度 $M$ 小于原始空间的维度 $D$, 自编码器相当于是一种降维或特征抽取方法. 如果 $M \\geq D$, 一定可以找到一组或多组解使得 $f \\circ g$ 为单位函数 (Identity Function), 并使得重构错误为 0 . 然而, 这样的解并没有太多的单位函数 $I(x)=x$.意义. 但是, 如果再加上一些附加的约束, 就可以得到一些有意义的解, 比如编码的稀疏性、取值范围, $f$ 和 $g$ 的具体形式等. 如果我们让编码只能取 $K$ 个不同的值 $(K<N)$, 那么自编码器就可以转换为一个 $K$ 类的聚类 ( Clustering) 问题.\n\n最简单的自编码器是如图9.2所示的两层神经网络. 输入层到隐藏层用来编码, 隐藏层到输出层用来解码, 层与层之间互相全连接.\n\n\n\n图 9.2 两层网络结构的自编码器\n\n对于样本 $\\boldsymbol{x}$, 自编码器的中间隐藏层的活性值为 $\\boldsymbol{x}$ 的编码, 即\n\n\n\\begin{equation*}\n\\boldsymbol{z}=f\\left(\\boldsymbol{W}^{(1)} \\boldsymbol{x}+\\boldsymbol{b}^{(1)}\\right) \\tag{9.20}\n\\end{equation*}\n\n\n自编码器的输出为重构的数据\n\n\n\\begin{equation*}\n\\boldsymbol{x}^{\\prime}=f\\left(\\boldsymbol{W}^{(2)} \\boldsymbol{z}+\\boldsymbol{b}^{(2)}\\right) \\tag{9.21}\n\\end{equation*}\n\n\n其中 $\\boldsymbol{W}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{b}^{(1)}, \\boldsymbol{b}^{(2)}$ 为网络参数, $f(\\cdot)$ 为激活函数. 如果令 $\\boldsymbol{W}^{(2)}$ 等于 $\\boldsymbol{W}^{(1)}$的转置, 即 $\\boldsymbol{W}^{(2)}=\\boldsymbol{W}^{(1)^{\\top}}$, 称为捆绑权重 (Tied Weight). 捆绑权重自编码器的参数更少, 因此更容易学习. 此外, 捆绑权重还在一定程度上起到正则化的作用.\n\n给定一组样本 $\\boldsymbol{x}^{(n)} \\in[0,1]^{D}, 1 \\leq n \\leq N$, 其重构错误为\n\n\n\\begin{equation*}\n\\left.\\mathcal{L}=\\sum_{n=1}^{N} \\| \\boldsymbol{x}^{(n)}-\\boldsymbol{x}^{(n)}\\right)\\left\\|^{2}+\\lambda\\right\\| \\boldsymbol{W} \\|_{F}^{2} \\tag{9.22}\n\\end{equation*}\n\n\n其中 $\\lambda$ 为正则化项系数. 通过最小化重构错误, 可以有效地学习网络的参数.\n\n我们使用自编码器是为了得到有效的数据表示, 因此在训练结束后, 我们一般会去掉解码器, 只保留编码器. 编码器的输出可以直接作为后续机器学习模型的输入."
    },
    {
        "id": "chatcmpl-944dmrvvrLgXUIbHfEvYWjsiYAICb",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"自编码器是通过何种方式来学习一组数据的有效编码？\",\n    \"回答1\": \"自编码器是通过无监督的方式来学习一组数据的有效编码（或表示）。它将给定的样本映射到特征空间得到每个样本的编码，并希望这组编码可以重构出原来的样本。\",\n    \"问题2\": \"自编码器的结构包括哪两部分？\",\n    \"回答2\": \"自编码器的结构包括编码器（Encoder）$f: \\boldsymbol{x} \\rightarrow \\boldsymbol{z}$ 和解码器（Decoder）$g: \\boldsymbol{z} \\rightarrow \\boldsymbol{x}$。\",\n    \"问题3\": \"自编码器的学习目标是什么？\",\n    \"回答3\": \"自编码器的学习目标是最小化重构错误（Reconstruction Error），即通过最小化重构误差来学习网络的参数，以得到有效的数据表示。\",\n    \"问题4\": \"当特征空间的维度 $M$ 小于原始空间的维度 $D$ 时，自编码器相当于是一种什么方法？\",\n    \"回答4\": \"当特征空间的维度 $M$ 小于原始空间的维度 $D$ 时，自编码器相当于是一种降维或特征抽取方法。\",\n    \"问题5\": \"自编码器如何转换为一个 $K$ 类的聚类（Clustering）问题？\",\n    \"回答5\": \"如果让编码只能取 $K$ 个不同的值 $(K<N)$，那么自编码器就可以转换为一个 $K$ 类的聚类问题。\",\n    \"问题6\": \"简述捆绑权重自编码器与普通自编码器之间的区别及优势。\",\n    \"回答6\": \"捆绑权重自编码器在参数更少的情况下更容易学习，并且在一定程度上起到正则化的作用。它通过限制解码器和编码器之间权重矩阵的关系，提高了模型的泛化能力。\",\n    \"问题7\": \"给定一组样本 $\\boldsymbol{x}^{(n)} \\text{ 且 } 1 \\text{ 为 } n \\text{ 为 } N$，其重构错误如何计算？\",\n    \"回答7\": \"给定一组样本 $\\boldsymbol{x}^{(n)} \\text{ 且 } 1 \\text{ 为 } n \\text{ 为 } N$，其重构错误可以通过计算 $\\text{Reconstruction Error}=\\frac{1}{N}\\big\\backslash\\big\\backslash\\boldsymbol{x}^{(n)}-\\boldsymbol{x}^{\\text{'(n)}}\\big\\big\\backslash\\big\\big\\backslash^{2}+\\frac{\\boldsymbol{\\text{lambda}}}{N}\\big\\backslash\\big\\backslash\\boldsymbol{W}\\big\\big\\backslash\\big\\big\\backslash_{F}^{2}$ 来得到。\",\n    \"问题8\": \"在训练结束后，为什么会去掉自编码器的解码器，只保留编码器？\",\n    \"回答8\": \"在训练结束后，会去掉解码器，只保留编码器是因为我们使用自编码器是为了得到有效的数据表示。因此，编码器的输出可以直接作为后续机器学习模型的输入。\",\n    \"问题9\": \"自编码器如何利用最小化重构错误来学习网络参数？\",\n    \"回答9\": \"通过最小化重构错误，自编码器可以调整网络参数以使得重构误差最小化，从而有效地学习数据的表示。这有助于提高模型对数据特征的捕捉能力。\",\n    \"问题10\": \"自编码器相对于监督学习有何优势？\",\n    \"回答10\": \"相对于监督学习，自编码器具有无监督学习的特点，不需要标记数据即可进行学习。这使得自编码器能够更好地适应大规模未标记数据，并且具有更强的泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710758178,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1148,
            "prompt_tokens": 3879,
            "total_tokens": 5027
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]