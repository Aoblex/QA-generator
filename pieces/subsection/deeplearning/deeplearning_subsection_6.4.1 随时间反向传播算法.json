[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "6.4.1 随时间反向传播算法",
        "content_length": 3335,
        "content": "Title: 6.4.1 随时间反向传播算法\nContent:\n随时间反向传播 ( BackPropagation Through Time, BPTT) 算法的主要思想是通过类似前馈神经网络的误差反向传播算法 [Werbos, 1990] 来计算梯度.\n\nBPTT 算法将循环神经网络看作一个展开的多层前馈网络, 其中“每一层”对应循环网络中的 “每个时刻” (见图6.2). 这样, 循环神经网络就可以按照前馈网络中的反向传播算法计算参数梯度. 在 “展开”的前馈网络中, 所有层的参数是共享的, 因此参数的真实梯度是所有 “展开层”的参数梯度之和.\n\n计算偏导数 $\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{U}}$ 先来计算公式 (6.30) 中第 $t$ 时刻损失对参数 $\\boldsymbol{U}$ 的偏导数 $\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{U}}$.\n\n因为参数 $\\boldsymbol{U}$ 和隐藏层在每个时刻 $k(1 \\leq k \\leq t)$ 的净输入 $\\boldsymbol{z}_{k}=\\boldsymbol{U} \\boldsymbol{h}_{k-1}+$ $\\boldsymbol{W} \\boldsymbol{x}_{k}+\\boldsymbol{b}$ 有关, 因此第 $t$ 时刻的损失函数 $\\mathcal{L}_{t}$ 关于参数 $u_{i j}$ 的梯度为:\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{L}_{t}}{\\partial u_{i j}}=\\sum_{k=1}^{t} \\frac{\\partial^{+} \\boldsymbol{z}_{k}}{\\partial u_{i j}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{z}_{k}} \\tag{6.31}\n\\end{equation*}\n\n\n链式法则参见公式(B.18).\n\n其中 $\\frac{\\partial^{+} \\boldsymbol{z}_{k}}{\\partial u_{i j}}$ 表示 “直接”偏导数, 即公式 $\\boldsymbol{z}_{k}=\\boldsymbol{U} \\boldsymbol{h}_{k-1}+\\boldsymbol{W} \\boldsymbol{x}_{k}+\\boldsymbol{b}$ 中保持 $\\boldsymbol{h}_{k-1}$ 不变，对 $u_{i j}$ 进行求偏导数, 得到\n\n\n\\begin{align*}\n\\frac{\\partial^{+} \\boldsymbol{z}_{k}}{\\partial u_{i j}} & =\\left[0, \\cdots,\\left[\\boldsymbol{h}_{k-1}\\right]_{j} \\cdots, 0\\right]  \\tag{6.32}\\\\\n& \\triangleq \\mathbb{\\square}_{i}\\left(\\left[\\boldsymbol{h}_{k-1}\\right]_{j}\\right), \\tag{6.33}\n\\end{align*}\n\n\n其中 $\\left[\\boldsymbol{h}_{k-1}\\right]_{j}$ 为第 $k-1$ 时刻隐状态的第 $j$ 维; $\\square_{i}(x)$ 除了第 $i$ 个元素的值为 $x$ 外, 其余都为 0 的行向量.\n\n定义误差项 $\\delta_{t, k}=\\frac{\\partial \\mathcal{L}_{t}}{\\partial z_{k}}$ 为第 $t$ 时刻的损失对第 $k$ 时刻隐藏神经层的净输入 $\\boldsymbol{z}_{k}$的导数, 则当 $1 \\leq k<t$ 时\n\n\n\\begin{align*}\n\\delta_{t, k} & =\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{z}_{k}}  \\tag{6.34}\\\\\n& =\\frac{\\partial \\boldsymbol{h}_{k}}{\\partial \\boldsymbol{z}_{k}} \\frac{\\partial \\boldsymbol{z}_{k+1}}{\\partial \\boldsymbol{h}_{k}} \\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{z}_{k+1}}  \\tag{6.35}\\\\\n& =\\operatorname{diag}\\left(f^{\\prime}\\left(\\boldsymbol{z}_{k}\\right)\\right) \\boldsymbol{U}^{\\top} \\delta_{t, k+1} \\cdot \\tag{6.36}\n\\end{align*}\n\n\n将公式(6.36)和公式 (6.33)代入公式 (6.31) 得到\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{L}_{t}}{\\partial u_{i j}}=\\sum_{k=1}^{t}\\left[\\delta_{t, k}\\right]_{i}\\left[\\boldsymbol{h}_{k-1}\\right]_{j} \\tag{6.37}\n\\end{equation*}\n\n\n将上式写成矩阵形式为\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{L}_{t}}{\\partial \\boldsymbol{U}}=\\sum_{k=1}^{t} \\delta_{t, k} \\boldsymbol{h}_{k-1}^{\\top} \\tag{6.38}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n图6.6给出了误差项随时间进行反向传播算法的示例.\n\n\n\n图 6.6 误差项随时间反向传播算法示例\n\n参数梯度 将公式(6.38) 代入到公式 (6.30), 得到整个序列的损失函数 $\\mathcal{L}$ 关于参数 $\\boldsymbol{U}$ 的梯度\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{U}}=\\sum_{t=1}^{T} \\sum_{k=1}^{t} \\delta_{t, k} \\boldsymbol{h}_{k-1}^{\\top} \\tag{6.39}\n\\end{equation*}\n\n\n同理可得, $\\mathcal{L}$ 关于权重 $\\boldsymbol{W}$ 和偏置 $\\boldsymbol{b}$ 的梯度为\n\n\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{W}} & =\\sum_{t=1}^{T} \\sum_{k=1}^{t} \\delta_{t, k} \\boldsymbol{x}_{k}^{\\top},  \\tag{6.40}\\\\\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{b}} & =\\sum_{t=1}^{T} \\sum_{k=1}^{t} \\delta_{t, k} . \\tag{6.41}\n\\end{align*}\n\n\n计算复杂度 在 BPTT算法中,参数的梯度需要在一个完整的 “前向”计算和 “反向”计算后才能得到并进行参数更新."
    },
    {
        "id": "chatcmpl-9422OXnRum03YgbVBxpVdBEyxK8QO",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"随时间反向传播算法（BPTT）的主要思想是什么？\",\n    \"回答1\": \"随时间反向传播算法的主要思想是将循环神经网络看作一个展开的多层前馈网络，其中“每一层”对应循环网络中的“每个时刻”。通过类似前馈神经网络的误差反向传播算法，计算参数梯度。\",\n    \"问题2\": \"BPTT算法中如何计算参数 $\\boldsymbol{U}$ 的偏导数 $\\frac{\\frac{\\text{d} \\text{L}_t}{\\text{d} \\boldsymbol{U}}}$？\",\n    \"回答2\": \"计算参数 $\\boldsymbol{U}$ 的偏导数 $\\frac{\\frac{\\text{d} \\text{L}_t}{\\text{d} \\boldsymbol{U}}$ 首先计算公式 (6.30) 中第 $t$ 时刻损失对参数 $\\boldsymbol{U}$ 的偏导数 $\\frac{\\frac{\\text{d} \\text{L}_t}{\\text{d} \\boldsymbol{U}}$。根据公式 (6.31)，利用链式法则和直接偏导数的定义，得到参数 $\\boldsymbol{U}$ 的偏导数表达式。\",\n    \"问题3\": \"BPTT算法中如何定义误差项 $\\boldsymbol{\\triangle}_{t, k}$？\",\n    \"回答3\": \"在BPTT算法中，定义误差项 $\\boldsymbol{\\triangle}_{t, k}$ 为第 $t$ 时刻的损失对第 $k$ 时刻隐藏神经层的净输入 $\\boldsymbol{z}_{k}$的导数。根据公式 (6.34) 和 (6.35)，通过链式法则和对角矩阵的定义，得到误差项 $\\boldsymbol{\\triangle}_{t, k}$ 的表达式。\",\n    \"问题4\": \"BPTT算法中参数梯度的计算复杂度如何？\",\n    \"回答4\": \"在BPTT算法中，参数的梯度需要在一个完整的 “前向”计算和 “反向”计算后才能得到并进行参数更新。因此，参数梯度的计算复杂度较高，需要考虑整个序列的损失函数对参数的梯度。\",\n    \"问题5\": \"BPTT算法中如何计算损失函数对权重 $\\boldsymbol{W}$ 的偏导数？\",\n    \"回答5\": \"根据公式 (6.40)，BPTT算法中计算损失函数对权重 $\\boldsymbol{W}$ 的偏导数时，需要对整个序列进行求和，并考虑每个时刻隐藏神经层的净输入。通过累加每个时刻的误差项乘以输入向量的转置，得到损失函数对权重 $\\boldsymbol{W}$ 的偏导数。\",\n    \"问题6\": \"BPTT算法中如何计算损失函数对偏置 $\\boldsymbol{b}$ 的偏导数？\",\n    \"回答6\": \"根据公式 (6.41)，BPTT算法中计算损失函数对偏置 $\\boldsymbol{b}$ 的偏导数时，同样需要对整个序列进行求和，并考虑每个时刻隐藏神经层的净输入。通过累加每个时刻的误差项，得到损失函数对偏置 $\\boldsymbol{b}$ 的偏导数。\",\n    \"问题7\": \"BPTT算法中如何将公式(6.38)写成矩阵形式？\",\n    \"回答7\": \"将公式(6.38)写成矩阵形式时，需要考虑整个序列的损失函数对参数 $\\boldsymbol{U}$ 的梯度。通过累加每个时刻的误差项乘以前一时刻隐藏状态的转置，得到公式(6.38)的矩阵形式表达。\",\n    \"问题8\": \"BPTT算法中如何将公式(6.39)代入到公式(6.30)？\",\n    \"回答8\": \"将公式(6.39)代入到公式(6.30)时，需要考虑整个序列的损失函数对参数 $\\boldsymbol{U}$ 的梯度。通过累加每个时刻的误差项乘以前一时刻隐藏状态的转置，得到整个序列的损失函数关于参数 $\\boldsymbol{U}$ 的梯度。\",\n    \"问题9\": \"BPTT算法中如何将公式(6.40)代入到公式(6.30)？\",\n    \"回答9\": \"将公式(6.40)代入到公式(6.30)时，需要考虑整个序列的损失函数对权重 $\\boldsymbol{W}$ 的梯度。通过累加每个时刻的误差项乘以输入向量的转置，得到整个序列的损失函数关于权重 $\\boldsymbol{W}$ 的梯度。\",\n    \"问题10\": \"BPTT算法中如何将公式(6.41)代入到公式(6.30)？\",\n    \"回答10\": \"将公式(6.41)代入到公式(6.30)时，需要考虑整个序列的损失函数对偏置 $\\boldsymbol{b}$ 的梯度。通过累加每个时刻的误差项，得到整个序列的损失函数关于偏置 $\\boldsymbol{b}$ 的梯度。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710748172,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1493,
            "prompt_tokens": 4319,
            "total_tokens": 5812
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]