[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "14.1.4.2 目标函数",
        "content_length": 482,
        "content": "Title: 14.1.4.2 目标函数\nContent:\n因为策略和状态转移都有一定的随机性, 所以每次试验得到的轨迹是一个随机序列, 其收获的总回报也不一样. 强化学习的目标是学习到一个策略 $\\pi_{\\theta}(a \\mid s)$来最大化期望回报 (Expected Return)，即希望智能体执行一系列的动作来获得尽可能多的平均回报.\n\n强化学习的目标函数为\n\n\n\\begin{equation*}\n\\mathcal{\\partial}(\\theta)=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[G(\\tau)]=\\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\left[\\sum_{t=0}^{T-1} \\gamma^{t} r_{t+1}\\right] \\tag{14.11}\n\\end{equation*}\n\n\n其中 $\\theta$ 为策略函数的参数.在持续式任务中, 强化学习的优化目标也可以定义为 MDP 到达平稳分布时 “即时奖励”的期望.\n\n参见习题14-1."
    },
    {
        "id": "chatcmpl-945e58llu1a9K0s7bahFgL7tzCKiB",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"强化学习的目标是什么？\",\n    \"回答1\": \"强化学习的目标是学习到一个策略 $\\boldsymbol{\\theta}(a \\text{ 在 } s)$ 来最大化期望回报（Expected Return），即希望智能体执行一系列的动作来获得尽可能多的平均回报。\",\n    \"问题2\": \"强化学习的目标函数是如何定义的？\",\n    \"回答2\": \"强化学习的目标函数为 $\\nabla(\\theta)=\\text{E}_{\\tau \\text{ 在 } p_{\\theta}(\\tau)}[G(\\tau)]=\\text{E}_{\\tau \\text{ 在 } p_{\\theta}(\\tau)}\\big[\\text{sum}_{t=0}^{T-1} \\boldsymbol{\\theta}^{t} r_{t+1}\\big]$，其中 $\\boldsymbol{\\theta}$ 为策略函数的参数。在持续式任务中，强化学习的优化目标也可以定义为 MDP 到达平稳分布时“即时奖励”的期望。\",\n    \"问题3\": \"强化学习中的期望回报是如何计算的？\",\n    \"回答3\": \"在强化学习中，期望回报是通过对轨迹的总回报进行期望计算得到的。具体地，期望回报被定义为 $\\text{E}_{\\tau \\text{ 在 } p_{\\theta}(\\tau)}\\big[\\text{sum}_{t=0}^{T-1} \\boldsymbol{\\theta}^{t} r_{t+1}\\big]$，其中 $\\tau$ 是从策略 $p_{\\theta}(\\tau)$ 中采样得到的轨迹。\",\n    \"问题4\": \"强化学习中的策略函数参数 $\\theta$ 的作用是什么？\",\n    \"回答4\": \"在强化学习中，策略函数参数 $\\theta$ 的作用是指导智能体执行动作以最大化期望回报。通过调整策略函数参数 $\\theta$，可以使智能体学习到更优秀的策略，从而获得更高的平均回报。\",\n    \"问题5\": \"强化学习中的优化目标与持续式任务有何关联？\",\n    \"回答5\": \"在持续式任务中，强化学习的优化目标可以定义为 MDP 到达平稳分布时“即时奖励”的期望。这意味着在持续式任务中，强化学习旨在最大化智能体在环境中长期获得的奖励，从而使其学会更好地执行动作以获得更高的长期回报。\",\n    \"问题6\": \"为什么每次试验得到的轨迹是一个随机序列？\",\n    \"回答6\": \"因为策略和状态转移都具有一定的随机性，所以每次试验得到的轨迹是一个随机序列。这意味着在强化学习中，智能体在环境中执行动作和状态转移都受到一定程度的随机性影响。\",\n    \"问题7\": \"强化学习中的总回报如何影响智能体的行为？\",\n    \"回答7\": \"在强化学习中，总回报反映了智能体执行一系列动作后所获得的累积奖励。智能体的目标是通过调整策略函数来最大化期望总回报，从而使其在环境中获得尽可能多的平均回报。\",\n    \"问题8\": \"强化学习中如何定义轨迹的总回报？\",\n    \"回答8\": \"在强化学习中，轨迹的总回报被定义为 $\\text{sum}_{t=0}^{T-1} \\boldsymbol{\\theta}^{t} r_{t+1}$，即对轨迹中每个时间步的即时奖励进行累积求和。这个总回报反映了智能体执行一系列动作后所获得的累积奖励。\",\n    \"问题9\": \"强化学习中如何利用期望回报来指导智能体行为？\",\n    \"回答9\": \"在强化学习中，利用期望回报来指导智能体行为是通过调整策略函数参数 $\\theta$ 来最大化期望总回报。智能体根据当前状态和策略函数输出的动作概率来执行动作，以使期望回报最大化。\",\n    \"问题10\": \"强化学习中如何定义持续式任务下的优化目标？\",\n    \"回答10\": \"在持续式任务中，强化学习的优化目标可以定义为 MDP 到达平稳分布时“即时奖励”的期望。这意味着在持续式任务下，强化学习旨在最大化智能体在环境中长期获得的奖励，从而使其学会更好地执行动作以获得更高的长期回报。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710762041,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1435,
            "prompt_tokens": 2880,
            "total_tokens": 4315
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]