[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.2.2.1 损失函数",
        "content_length": 2741,
        "content": "Title: 2.2.2.1 损失函数\nContent:\n损失函数是一个非负实数函数, 用来量化模型预测和真实标签之间的差异.下面介绍几种常用的损失函数.这里两个分布相似性的定义不太严谨, 更好的方式为KL散度或交叉熵。\n\n期望风险也经常称为期望错误 (Expected Error )\n\n0-1 损失函数 最直观的损失函数是模型在训练集上的错误率, 即 $0-1$ 损失函数 ( 0-1 Loss Function ) :\n\n\n\\begin{align*}\n\\mathcal{L}(y, f(x ; \\theta)) & = \\begin{cases}0 & \\text { if } y=f(\\boldsymbol{x} ; \\theta) \\\\\n1 & \\text { if } y \\neq f(\\boldsymbol{x} ; \\theta)\\end{cases}  \\tag{2.12}\\\\\n& =I(y \\neq f(\\boldsymbol{x} ; \\theta)), \\tag{2.13}\n\\end{align*}\n\n\n其中 $I(\\cdot)$ 是指示函数.\n\n虽然 0-1 损失函数能够客观地评价模型的好坏, 但其缺点是数学性质不是很好: 不连续且导数为 0 , 难以优化. 因此经常用连续可微的损失函数替代.\n\n平方损失函数 平方损失函数 (Quadratic Loss Function) 经常用在预测标签 $y$为实数值的任务中, 定义为\n\n\n\\begin{equation*}\n\\mathcal{L}(y, f(x ; \\theta))=\\frac{1}{2}(y-f(x ; \\theta))^{2} . \\tag{2.14}\n\\end{equation*}\n\n\n平方损失函数一般不适用于分类问题.\n\n交叉熵损失函数 交叉熵损失函数 ( Cross-Entropy Loss Function ) 一般用于分类问题. 假设样本的标签 $y \\in\\{1, \\cdots, C\\}$ 为离散的类别, 模型 $f(\\boldsymbol{x} ; \\theta) \\in[0,1]^{C}$的输出为类别标签的条件概率分布, 即\n\n\n\\begin{equation*}\np(y=c \\mid \\boldsymbol{x} ; \\theta)=f_{c}(\\boldsymbol{x} ; \\theta) \\tag{2.15}\n\\end{equation*}\n\n\n并满足\n\n\n\\begin{equation*}\nf_{c}(x ; \\theta) \\in[0,1], \\quad \\sum_{c=1}^{C} f_{c}(x ; \\theta)=1 \\tag{2.16}\n\\end{equation*}\n\n\n我们可以用一个 $C$ 维的 one-hot 向量 $\\boldsymbol{y}$ 来表示样本标签. 假设样本的标签为 $k$, 那么标签向量 $\\boldsymbol{y}$ 只有第 $k$ 维的值为 1 , 其余元素的值都为 0 . 标签向量 $\\boldsymbol{y}$ 可以看作样本标签的真实条件概率分布 $p_{r}(\\boldsymbol{y} \\mid \\boldsymbol{x})$, 即第 $c$ 维 (记为 $y_{c}, 1 \\leq c \\leq C$ ) 是类别为 $c$ 的真实条件概率. 假设样本的类别为 $k$, 那么它属于第 $k$ 类的概率为 1 , 属于其他类的概率为 0 .\n\n对于两个概率分布,一般可以用交叉熵来衡量它们的差异. 标签的真实分布 $\\boldsymbol{y}$ 和模型预测分布 $f(\\boldsymbol{x} ; \\theta)$ 之间的交叉熵为\n\n\n\\begin{align*}\n\\mathcal{L}(\\boldsymbol{y}, f(\\boldsymbol{x} ; \\theta)) & =-\\boldsymbol{y}^{\\top} \\log f(\\boldsymbol{x} ; \\theta)  \\tag{2.17}\\\\\n& =-\\sum_{c=1}^{C} y_{c} \\log f_{c}(\\boldsymbol{x} ; \\theta) . \\tag{2.18}\n\\end{align*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/} $f_{c}(x ; \\theta)$ 表示 $f(x ; \\theta)$的输出向量的第 $c$ 维.\\\\\n比如对于三分类问题, 一个样本的标签向量为 $\\boldsymbol{y}=[0,0,1]^{\\top}$, 模型预测的标签分布为 $f(\\boldsymbol{x} ; \\theta)=[0.3,0.3,0.4]^{\\top}$, 则它们的交叉嫡为 $-(0 \\times \\log (0.3)+0 \\times$ $\\log (0.3)+1 \\times \\log (0.4))=-\\log (0.4)$.\n\n因为 $\\boldsymbol{y}$ 为 one-hot 向量, 公式 (2.18) 也可以写为\n\n\n\\begin{equation*}\n\\mathcal{L}(y, f(\\boldsymbol{x} ; \\theta))=-\\log f_{y}(\\boldsymbol{x} ; \\theta) \\tag{2.19}\n\\end{equation*}\n\n\n其中 $f_{y}(\\boldsymbol{x} ; \\theta)$ 可以看作真实类别 $y$ 的似然函数. 因此, 交叉嫡损失函数也就是负对数似然函数 (Negative Log-Likelihood).\n\nHinge 损失函数 对于二分类问题, 假设 $y$ 的取值为 $\\{-1,+1\\}, f(x ; \\theta) \\in \\mathbb{R}$. Hinge 损失函数 (Hinge Loss Function) 为\n\n\n\\begin{align*}\n\\mathcal{L}(y, f(\\boldsymbol{x} ; \\theta)) & =\\max (0,1-y f(\\boldsymbol{x} ; \\theta))  \\tag{2.20}\\\\\n& \\triangleq[1-y f(\\boldsymbol{x} ; \\theta)]_{+}, \\tag{2.21}\n\\end{align*}\n\n\n其中 $[x]_{+}=\\max (0, x)$."
    },
    {
        "id": "chatcmpl-92yGeIy593ipbjiveh3AvcnK7NMIp",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"损失函数的作用是什么？\",\n    \"回答1\": \"损失函数是用来量化模型预测和真实标签之间的差异的非负实数函数。它的作用是评估模型的预测性能，帮助优化算法调整模型参数以最小化预测误差。\",\n    \"问题2\": \"0-1 损失函数的定义是什么？它的优缺点是什么？\",\n    \"回答2\": \"0-1 损失函数是模型在训练集上的错误率，即 $0-1$ 损失函数：$\\n\\\\mathcal{L}(y, f(x ; \\\\theta)) = \\\\begin{cases}0 & \\\\text { if } y=f(\\\\boldsymbol{x} ; \\\\theta) \\\\\\\\1 & \\\\text { if } y \\\\neq f(\\\\boldsymbol{x} ; \\\\theta)\\\\end{cases}$。它能客观地评价模型的好坏，但缺点是数学性质不好，不连续且导数为 0，难以优化。\",\n    \"问题3\": \"平方损失函数适用于什么类型的任务？\",\n    \"回答3\": \"平方损失函数（Quadratic Loss Function）经常用在预测标签 $y$ 为实数值的任务中。它的定义为 $\\n\\\\mathcal{L}(y, f(x ; \\\\theta))=\\\\frac{1}{2}(y-f(x ; \\\\theta))^{2}$。\",\n    \"问题4\": \"交叉熵损失函数一般用于什么类型的任务？它的计算公式是什么？\",\n    \"回答4\": \"交叉熵损失函数一般用于分类问题。对于样本的标签 $y \\n\\\\in\\\\{1, \\\\cdots, C\\\\}$ 为离散的类别，模型 $f(\\\\boldsymbol{x} ; \\\\theta) \\n\\\\in[0,1]^{C}$ 的输出为类别标签的条件概率分布。交叉熵损失函数的计算公式为 $\\n\\\\mathcal{L}(\\\\boldsymbol{y}, f(\\\\boldsymbol{x} ; \\\\theta)) = -\\\\boldsymbol{y}^{\\\\top} \\\\log f(\\\\boldsymbol{x} ; \\\\theta) = -\\\\sum_{c=1}^{C} y_{c} \\\\log f_{c}(\\\\boldsymbol{x} ; \\\\theta)$。\",\n    \"问题5\": \"Hinge 损失函数适用于什么类型的问题？它的计算公式是什么？\",\n    \"回答5\": \"Hinge 损失函数适用于二分类问题，其中 $y$ 的取值为 $\\\\{-1,+1\\\\}$，$f(x ; \\\\theta) \\n\\\\in \\\\mathbb{R}$。Hinge 损失函数的计算公式为 $\\n\\\\mathcal{L}(y, f(\\\\boldsymbol{x} ; \\\\theta)) = \\\\max (0,1-y f(\\\\boldsymbol{x} ; \\\\theta)) = [1-y f(\\\\boldsymbol{x} ; \\\\theta)]_{+}$。\",\n    \"问题6\": \"期望风险和期望错误有何关联？\",\n    \"回答6\": \"期望风险也经常称为期望错误（Expected Error）。期望风险是指在所有可能的输入上，模型预测的错误与真实标签之间的差异的期望值。因此，期望风险可以用来量化模型的整体预测性能。\",\n    \"问题7\": \"为什么经常用连续可微的损失函数替代 0-1 损失函数？\",\n    \"回答7\": \"0-1 损失函数虽然能客观地评价模型的好坏，但其缺点是数学性质不好，不连续且导数为 0，难以优化。因此，经常用连续可微的损失函数替代，以便更好地应用优化算法进行模型训练。\",\n    \"问题8\": \"交叉熵损失函数如何衡量两个概率分布之间的差异？\",\n    \"回答8\": \"交叉熵损失函数衡量标签的真实分布和模型预测分布之间的差异。它通过计算两个分布之间的交叉熵来量化它们的差异，从而评估模型在分类任务中的预测性能。\",\n    \"问题9\": \"平方损失函数为什么不适用于分类问题？\",\n    \"回答9\": \"平方损失函数经常用在预测标签为实数值的任务中，因为它对于分类问题并不合适。在分类问题中，标签通常是离散的类别，而平方损失函数更适合衡量连续数值之间的差异，因此不适用于分类任务。\",\n    \"问题10\": \"损失函数对于机器学习模型训练的重要性体现在哪些方面？\",\n    \"回答10\": \"损失函数对于机器学习模型训练非常重要。它不仅用于评估模型的预测性能，还作为优化算法的目标函数，帮助调整模型参数以最小化预测误差。选择合适的损失函数能够影响模型的收敛速度和泛化能力，直接影响模型在实际任务中的表现。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710495352,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1480,
            "prompt_tokens": 4115,
            "total_tokens": 5595
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.05,
            "presence_penalty": 0.05
        }
    }
]