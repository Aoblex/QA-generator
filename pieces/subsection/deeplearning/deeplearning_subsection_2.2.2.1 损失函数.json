[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.2.2.1 损失函数",
        "content_length": 2741,
        "content": "Title: 2.2.2.1 损失函数\nContent:\n损失函数是一个非负实数函数, 用来量化模型预测和真实标签之间的差异.下面介绍几种常用的损失函数.这里两个分布相似性的定义不太严谨, 更好的方式为KL散度或交叉熵。\n\n期望风险也经常称为期望错误 (Expected Error )\n\n0-1 损失函数 最直观的损失函数是模型在训练集上的错误率, 即 $0-1$ 损失函数 ( 0-1 Loss Function ) :\n\n\n\\begin{align*}\n\\mathcal{L}(y, f(x ; \\theta)) & = \\begin{cases}0 & \\text { if } y=f(\\boldsymbol{x} ; \\theta) \\\\\n1 & \\text { if } y \\neq f(\\boldsymbol{x} ; \\theta)\\end{cases}  \\tag{2.12}\\\\\n& =I(y \\neq f(\\boldsymbol{x} ; \\theta)), \\tag{2.13}\n\\end{align*}\n\n\n其中 $I(\\cdot)$ 是指示函数.\n\n虽然 0-1 损失函数能够客观地评价模型的好坏, 但其缺点是数学性质不是很好: 不连续且导数为 0 , 难以优化. 因此经常用连续可微的损失函数替代.\n\n平方损失函数 平方损失函数 (Quadratic Loss Function) 经常用在预测标签 $y$为实数值的任务中, 定义为\n\n\n\\begin{equation*}\n\\mathcal{L}(y, f(x ; \\theta))=\\frac{1}{2}(y-f(x ; \\theta))^{2} . \\tag{2.14}\n\\end{equation*}\n\n\n平方损失函数一般不适用于分类问题.\n\n交叉熵损失函数 交叉熵损失函数 ( Cross-Entropy Loss Function ) 一般用于分类问题. 假设样本的标签 $y \\in\\{1, \\cdots, C\\}$ 为离散的类别, 模型 $f(\\boldsymbol{x} ; \\theta) \\in[0,1]^{C}$的输出为类别标签的条件概率分布, 即\n\n\n\\begin{equation*}\np(y=c \\mid \\boldsymbol{x} ; \\theta)=f_{c}(\\boldsymbol{x} ; \\theta) \\tag{2.15}\n\\end{equation*}\n\n\n并满足\n\n\n\\begin{equation*}\nf_{c}(x ; \\theta) \\in[0,1], \\quad \\sum_{c=1}^{C} f_{c}(x ; \\theta)=1 \\tag{2.16}\n\\end{equation*}\n\n\n我们可以用一个 $C$ 维的 one-hot 向量 $\\boldsymbol{y}$ 来表示样本标签. 假设样本的标签为 $k$, 那么标签向量 $\\boldsymbol{y}$ 只有第 $k$ 维的值为 1 , 其余元素的值都为 0 . 标签向量 $\\boldsymbol{y}$ 可以看作样本标签的真实条件概率分布 $p_{r}(\\boldsymbol{y} \\mid \\boldsymbol{x})$, 即第 $c$ 维 (记为 $y_{c}, 1 \\leq c \\leq C$ ) 是类别为 $c$ 的真实条件概率. 假设样本的类别为 $k$, 那么它属于第 $k$ 类的概率为 1 , 属于其他类的概率为 0 .\n\n对于两个概率分布,一般可以用交叉熵来衡量它们的差异. 标签的真实分布 $\\boldsymbol{y}$ 和模型预测分布 $f(\\boldsymbol{x} ; \\theta)$ 之间的交叉熵为\n\n\n\\begin{align*}\n\\mathcal{L}(\\boldsymbol{y}, f(\\boldsymbol{x} ; \\theta)) & =-\\boldsymbol{y}^{\\top} \\log f(\\boldsymbol{x} ; \\theta)  \\tag{2.17}\\\\\n& =-\\sum_{c=1}^{C} y_{c} \\log f_{c}(\\boldsymbol{x} ; \\theta) . \\tag{2.18}\n\\end{align*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/} $f_{c}(x ; \\theta)$ 表示 $f(x ; \\theta)$的输出向量的第 $c$ 维.\\\\\n比如对于三分类问题, 一个样本的标签向量为 $\\boldsymbol{y}=[0,0,1]^{\\top}$, 模型预测的标签分布为 $f(\\boldsymbol{x} ; \\theta)=[0.3,0.3,0.4]^{\\top}$, 则它们的交叉嫡为 $-(0 \\times \\log (0.3)+0 \\times$ $\\log (0.3)+1 \\times \\log (0.4))=-\\log (0.4)$.\n\n因为 $\\boldsymbol{y}$ 为 one-hot 向量, 公式 (2.18) 也可以写为\n\n\n\\begin{equation*}\n\\mathcal{L}(y, f(\\boldsymbol{x} ; \\theta))=-\\log f_{y}(\\boldsymbol{x} ; \\theta) \\tag{2.19}\n\\end{equation*}\n\n\n其中 $f_{y}(\\boldsymbol{x} ; \\theta)$ 可以看作真实类别 $y$ 的似然函数. 因此, 交叉嫡损失函数也就是负对数似然函数 (Negative Log-Likelihood).\n\nHinge 损失函数 对于二分类问题, 假设 $y$ 的取值为 $\\{-1,+1\\}, f(x ; \\theta) \\in \\mathbb{R}$. Hinge 损失函数 (Hinge Loss Function) 为\n\n\n\\begin{align*}\n\\mathcal{L}(y, f(\\boldsymbol{x} ; \\theta)) & =\\max (0,1-y f(\\boldsymbol{x} ; \\theta))  \\tag{2.20}\\\\\n& \\triangleq[1-y f(\\boldsymbol{x} ; \\theta)]_{+}, \\tag{2.21}\n\\end{align*}\n\n\n其中 $[x]_{+}=\\max (0, x)$."
    },
    {
        "id": "chatcmpl-940ev7qUnpoTH97ClUfZOSj2EFq8l",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"损失函数的作用是什么？\",\n    \"回答1\": \"损失函数是用来量化模型预测和真实标签之间的差异的非负实数函数。它在训练和优化模型时起着重要的作用，帮助评估模型的性能和指导参数的更新。\",\n    \"问题2\": \"0-1 损失函数的定义是什么？\",\n    \"回答2\": \"0-1 损失函数是模型在训练集上的错误率，即 $\\text{0-1}$ 损失函数：$\\begin{cases}0 & \\text { if } y=f(\\boldsymbol{x} ; \\theta) \\\\ 1 & \\text { if } y \\neq f(\\boldsymbol{x} ; \\theta)\\text{。}$ 其中 $I(\\bullet)$ 是指示函数。\",\n    \"问题3\": \"平方损失函数适用于什么类型的任务？\",\n    \"回答3\": \"平方损失函数经常用在预测标签 $y$ 为实数值的任务中，用于量化模型预测值与真实值之间的差异。它在回归问题中常被使用。\",\n    \"问题4\": \"交叉熵损失函数适用于什么类型的任务？\",\n    \"回答4\": \"交叉熵损失函数一般用于分类问题。它衡量了模型输出的类别标签概率分布与真实标签之间的差异，常用于多分类任务。\",\n    \"问题5\": \"交叉熵损失函数中的 one-hot 向量是如何表示样本标签的？\",\n    \"回答5\": \"在交叉熵损失函数中，一个 $C$ 维的 one-hot 向量 $\\boldsymbol{y}$ 被用来表示样本标签。如果样本属于第 $k$ 类，则标签向量 $\\boldsymbol{y}$ 只有第 $k$ 维的值为 1，其余元素的值都为 0。\",\n    \"问题6\": \"交叉熵损失函数如何衡量两个概率分布之间的差异？\",\n    \"回答6\": \"交叉熵损失函数衡量了模型输出的类别标签概率分布与真实标签之间的差异。它通过计算标签的真实分布 $\\boldsymbol{y}$ 和模型预测分布 $f(\\boldsymbol{x} ; \\theta)$ 的交叉熵来量化它们之间的差异。\",\n    \"问题7\": \"Hinge 损失函数适用于什么类型的问题？\",\n    \"回答7\": \"Hinge 损失函数适用于二分类问题，其中 $y$ 的取值为 $\\big\\brace{-1,+1\\big\\text{，}} f(x ; \\theta) \\text{ 为实数。}$\",\n    \"问题8\": \"为什么经常用连续可微的损失函数替代 0-1 损失函数？\",\n    \"回答8\": \"虽然 0-1 损失函数能够客观地评价模型的好坏，但其缺点是数学性质不是很好：不连续且导数为 0，难以优化。因此经常用连续可微的损失函数替代，以便更好地进行优化和训练模型。\",\n    \"问题9\": \"期望风险通常被称为什么？\",\n    \"回答9\": \"期望风险也经常称为期望错误（Expected Error）。它是对模型在整个样本空间上表现不佳的期望程度的度量。\",\n    \"问题10\": \"KL 散度和交叉熵在衡量分布相似性时有何作用？\",\n    \"回答10\": \"$KL$ 散度和交叉熵都可以用来衡量两个概率分布之间的相似性或差异。它们提供了一种方式来量化两个分布之间的距离或信息差异，常用于评估模型输出与真实分布之间的接近程度。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710742873,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1124,
            "prompt_tokens": 4177,
            "total_tokens": 5301
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]