[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.2.3.4 小批量梯度下降法",
        "content_length": 2155,
        "content": "Title: 2.2.3.4 小批量梯度下降法\nContent:\n随机梯度下降法的一个缺点是无法充分利用计算机的并行计算能力. 小批量梯度下降法 (Mini-Batch Gradient Descent) 是批量梯度下降和随机梯度下降的折中. 每次迭代时, 我们随机选取一小部分训练样本来计算梯度并更新参数, 这样既可以兼顾随机梯度下降法的优点,也可以提高训练效率.\n\n第 $t$ 次迭代时, 随机选取一个包含 $K$ 个样本的子集 $\\mathcal{S}_{t}$, 计算这个子集上每个样本损失函数的梯度并进行平均, 然后再进行参数更新:\n\n\n\\begin{equation*}\n\\theta_{t+1} \\leftarrow \\theta_{t}-\\alpha \\frac{1}{K} \\sum_{(x, y) \\in \\mathcal{S}_{t}} \\frac{\\partial \\mathcal{L}(y, f(\\boldsymbol{x} ; \\theta))}{\\partial \\theta} . \\tag{2.29}\n\\end{equation*}\n\n\n在实际应用中, 小批量随机梯度下降法有收玫快、计算开销小的优点, 因此逐渐成为大规模的机器学习中的主要优化算法 [Bottou, 2010].\n\n\\section*{2.3 机器学习的简单示例——线性回归}\n在本节中, 我们通过一个简单的模型 (线性回归) 来具体了解机器学习的一般过程, 以及不同学习准则 (经验风险最小化、结构风险最小化、最大似然估计、最大后验估计) 之间的关系.\n\n线性回归 (Linear Regression) 是机器学习和统计学中最基础和最广泛应用的模型, 是一种对自变量和因变量之间关系进行建模的回归分析. 自变量数量为 1 时称为简单回归,自变量数量大于 1 时称为多元回归.\n\n从机器学习的角度来看, 自变量就是样本的特征向量 $\\boldsymbol{x} \\in \\mathbb{R}^{D}$ (每一维对应一个自变量), 因变量是标签 $y$, 这里 $y \\in \\mathbb{R}$ 是连续值 (实数或连续整数). 假设空间是一组参数化的线性函数\n\n\n\\begin{equation*}\nf(\\boldsymbol{x} ; \\boldsymbol{w}, b)=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b \\tag{2.30}\n\\end{equation*}\n\n\n其中权重向量 $\\boldsymbol{w} \\in \\mathbb{R}^{D}$ 和偏置 $b \\in \\mathbb{R}$ 都是可学习的参数, 函数 $f(\\boldsymbol{x} ; \\boldsymbol{w}, b) \\in \\mathbb{R}$ 也称为线性模型.\n\\footnotetext{\\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n为简单起见,我们将公式(2.30) 写为\n\n\n\\begin{equation*}\nf(\\boldsymbol{x} ; \\hat{\\boldsymbol{w}})=\\hat{\\boldsymbol{w}}^{\\top} \\hat{\\boldsymbol{x}} \\tag{2.31}\n\\end{equation*}\n\n\n其中 $\\hat{\\boldsymbol{w}}$ 和 $\\hat{\\boldsymbol{x}}$ 分别称为增广权重向量和增广特征向量:\n\n\n\\begin{align*}\n& \\hat{\\boldsymbol{x}}=\\boldsymbol{x} \\oplus 1 \\triangleq\\left[\\begin{array}{c}\n\\boldsymbol{x} \\\\\n\\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{c}\nx_{1} \\\\\n\\vdots \\\\\nx_{D} \\\\\n1\n\\end{array}\\right],  \\tag{2.32}\\\\\n& \\hat{\\boldsymbol{w}}=\\boldsymbol{w} \\oplus b \\triangleq\\left[\\begin{array}{c}\n\\boldsymbol{w} \\\\\n\\\\\nb\n\\end{array}\\right]=\\left[\\begin{array}{c}\nw_{1} \\\\\n\\vdots \\\\\nw_{D} \\\\\nb\n\\end{array}\\right], \\tag{2.33}\n\\end{align*}\n\n\n其中 $\\oplus$ 定义为两个向量的拼接操作.\n\n不失一般性, 在本章后面的描述中我们采用简化的表示方法, 直接用 $\\boldsymbol{w}$和 $\\boldsymbol{x}$ 分别表示增广权重向量和增广特征向量. 这样, 线性回归的模型简写为 $f(\\boldsymbol{x} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{x}$."
    },
    {
        "id": "chatcmpl-940kRzuF0rtxcMyYHns7gHmePUY8G",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"小批量梯度下降法与批量梯度下降法和随机梯度下降法相比有什么优势？\",\n    \"回答1\": \"小批量梯度下降法是批量梯度下降和随机梯度下降的折中，每次迭代时随机选取一小部分训练样本来计算梯度并更新参数。它兼顾了随机梯度下降法的优点，同时提高了训练效率。相比之下，小批量梯度下降法既能够更好地利用计算机的并行计算能力，又能够减少计算开销，因此在实际应用中具有收敛快、计算开销小的优势。\",\n    \"问题2\": \"小批量梯度下降法的参数更新公式是怎样的？\",\n    \"回答2\": \"在第 $t$ 次迭代时，随机选取一个包含 $K$ 个样本的子集 $\\boldsymbol{S}_{t}$，计算这个子集上每个样本损失函数的梯度并进行平均，然后进行参数更新。参数更新公式为 $\\theta_{t+1} \\text{ ← } \\theta_{t}-\\frac{\\text{α}}{K} \\text{∑}_{(x, y) \\text{ 在 } \\boldsymbol{S}_{t}} \\frac{\\text{∂} \\text{L}(y, f(\\boldsymbol{x} ; \\theta))}{\\text{∂} \\theta}$。\",\n    \"问题3\": \"线性回归模型中的自变量和因变量分别是什么？\",\n    \"回答3\": \"从机器学习的角度来看，线性回归模型中的自变量是样本的特征向量 $\\boldsymbol{x} \\text{ ∈ } \\text{ℝ}^{D}$（每一维对应一个自变量），因变量是标签 $y$，其中 $y \\text{ ∈ } \\text{ℝ}$ 是连续值（实数或连续整数）。\",\n    \"问题4\": \"线性回归模型的假设空间是怎样定义的？\",\n    \"回答4\": \"线性回归模型的假设空间是一组参数化的线性函数 $f(\\boldsymbol{x} ; \\boldsymbol{w}, b)=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b$，其中权重向量 $\\boldsymbol{w} \\text{ ∈ } \\text{ℝ}^{D}$ 和偏置 $b \\text{ ∈ } \\text{ℝ}$ 都是可学习的参数。\",\n    \"问题5\": \"线性回归模型中的增广权重向量和增广特征向量分别是什么？\",\n    \"回答5\": \"在线性回归模型中，增广权重向量 $\\boldsymbol{\\tilde{w}}$ 和增广特征向量 $\\boldsymbol{\\tilde{x}}$ 分别定义为 $\\boldsymbol{\\tilde{x}}=\\boldsymbol{x} \\bigoplus 1$ 和 $\\boldsymbol{\\tilde{w}}=\\boldsymbol{w} \\bigoplus b$，其中 $\\bigoplus$ 定义为两个向量的拼接操作。\",\n    \"问题6\": \"线性回归模型中的参数更新公式是怎样的？\",\n    \"回答6\": \"在线性回归模型中，参数更新公式为 $\\theta_{t+1} \\text{ ← } \\theta_{t}-\\frac{\\text{α}}{K} \\text{∑}_{(x, y) \\text{ 在 } \\boldsymbol{S}_{t}} \\frac{\\text{∂} \\text{L}(y, f(\\boldsymbol{x} ; \\theta))}{\\text{∂} \\theta}$。这个公式描述了在每次迭代时如何利用小批量梯度下降法来更新模型参数。\",\n    \"问题7\": \"为什么线性回归模型被称为最基础和最广泛应用的模型？\",\n    \"回答7\": \"线性回归模型被称为最基础和最广泛应用的模型，因为它是一种对自变量和因变量之间关系进行建模的回归分析。它简单直观，并且在实际应用中具有较好的适用性和可解释性，因此被广泛应用于各种领域和任务中。\",\n    \"问题8\": \"线性回归模型中的自变量数量对应着什么类型的回归？\",\n    \"回答8\": \"线性回归模型中自变量数量为 1 时称为简单回归，自变量数量大于 1 时称为多元回归。因此，自变量数量对应着线性回归模型的类型，决定了模型所属的回归类型。\",\n    \"问题9\": \"简单起见，在描述中采用简化表示方法对线性回归模型进行了怎样的简写？\",\n    \"回答9\": \"简单起见，在描述中采用简化表示方法对线性回归模型进行了简写为 $f(\\boldsymbol{x} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{x}$。这种简写方式使得模型表达更加简洁明了。\",\n    \"问题10\": \"线性回归模型中如何定义增广权重向量和增广特征向量？\",\n    \"回答10\": \"在线性回归模型中，增广权重向量 $\\boldsymbol{\\tilde{w}}$ 和增广特征向量 $\\boldsymbol{\\tilde{x}}$ 分别定义为 $\\boldsymbol{\\tilde{x}}=\\boldsymbol{x} \\bigoplus 1$ 和 $\\boldsymbol{\\tilde{w}}=\\boldsymbol{w} \\bigoplus b$，其中 $\\bigoplus$ 定义为两个向量的拼接操作。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710743215,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1605,
            "prompt_tokens": 3887,
            "total_tokens": 5492
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]