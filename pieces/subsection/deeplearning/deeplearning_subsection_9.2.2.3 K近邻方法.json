[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "9.2.2.3 K近邻方法",
        "content_length": 7361,
        "content": "Title: 9.2.2.3 K近邻方法\nContent:\n核密度估计方法中的核宽度是固定的, 因此同一个宽度可能对高密度的区域过大，而对低密度的区域过小. 一种更灵活的方式是设置一种可变宽度的区域, 并使得落入每个区域中样本数量为固定的 $K$. 要估计点 $\\boldsymbol{x}$ 的密度, 首先找到一个以 $\\boldsymbol{x}$ 为中心的球体, 使得落入球体的样本数量为 $K$, 然后根据公式(9.41), 就可以计算出点 $\\boldsymbol{x}$ 的密度. 因为落入球体的样本也是离 $\\boldsymbol{x}$ 最近的 $K$ 个样本, 所以这种方法称为 $\\mathrm{K}$ 近邻方法 (K-Nearest Neighbor Method).\n\n$\\mathrm{K}$ 近邻方法并不是一个严格的密度函数估计方法,参见习题9-5.\n\\footnotetext{\\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n在 $\\mathrm{K}$ 近邻方法中, $K$ 的选择也十分关键. 如果 $K$ 太小, 无法有效地估计密度函数;而 $K$ 太大也会使得局部的密度不准确, 并且增加计算开销.\n\n$K$ 近邻方法也经常用于分类问题, 称为 $K$ 近邻分类器 (K-Nearest Neighbor Classifier). 当 $K=1$ 时, 也称为最近邻分类器 (Nearest Neighbor Classifier).最近邻分类器的一个性质是, 当 $N \\rightarrow \\infty$ 时, 其分类错误率不超过最优分类器错误率的两倍 [Cover et al., 1967].\n\n参见习题9-7.\n\n\\section*{9.3 总结和深入阅读}\n无监督学习是一种十分重要的机器学习方法. 广义上讲, 监督学习也可以看作一类特殊的无监督学习, 即估计条件概率 $p(y \\mid \\boldsymbol{x})$. 条件概率 $p(y \\mid \\boldsymbol{x})$ 可以通过贝叶斯公式转为估计概率 $p(y)$ 和 $p(x \\mid y)$, 并通过无监督密度估计来求解.\n\n无监督学习问题主要可以分为聚类、特征学习、密度估计等几种类型. 关于聚类方面的内容,可以参考《机器学习》[周志华, 2016] 中的第 9 章.\n\n无监督特征学习是一种十分重要的表示学习方法. 当一个监督学习任务的数据比较少时, 可以通过大规模的无标注数据, 学习到一种有效的数据表示, 并有效提高监督学习的性能. 关于无监督特征学习的内容, 可以参考《机器学习》 [周志华, 2016] 中的第 10 章和《Pattern Classification》 [Duda et al., 2001] 中的第 10 章.\n\n概率密度估计方法可以分为两类: 参数方法和非参数方法. 参数方法是假设数据分布服从某种参数化的模型. 我们在本书的后面章节会陆续介绍更多的参数密度估计模型. 在第11章中, 我们通过概率图模型介绍更一般的参数密度估计方法, 包括含隐变量的参数估计方法. 当估计出一个数据分布的参数化模型后,我们可以根据这个模型来生成数据, 因此这些模型也称为生成模型. 第 12 章介绍两种比较复杂的生成模型: 玻尔兹曼机和深度信念网络. 第13章介绍两种深度生成模型:变分自编码器和对抗生成网络. 第15章介绍几种序列数据的生成模型.\n\n关于非参数密度估计方法的一般性介绍可以参考文献 [Duda et al., 2001] 和 [Bishop, 2007],理论性介绍可以参考 [Devroye et al., 1985].\n\n目前, 无监督学习并没有像监督学习那样取得广泛的成功, 其主要原因在于无监督学习缺少有效的客观评价方法, 导致很难衡量一个无监督学习方法的好坏. 无监督学习的好坏通常需要代入到下游任务中进行验证.\n\n\\section*{习题}\n习题9-1 分析主成分分析为什么具有数据降噪能力?\n\n习题 9-2 证明对于 $N$ 个样本 (样本维数 $D>N$ ) 组成的数据集, 主成分分析的有效投影子空间不超过 $N-1$ 维.\n\n习题 9-3 对于一个二分类问题, 试举例分析什么样的数据分布会使得主成分分\n\n析得到的特征反而会使得分类性能下降.\n\n习题 9-4 若数据矩阵 $\\boldsymbol{X}^{\\prime}=\\boldsymbol{X}-\\overline{\\boldsymbol{X}}$, 则对 $\\boldsymbol{X}^{\\prime}$ 奇异值分解 $\\boldsymbol{X}^{\\prime}=\\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}$, 则 $\\boldsymbol{U}$ 为主成分分析的投影矩阵.\n\n习题 9-5 举例说明, $\\mathrm{K}$ 近邻方法估计的密度函数不是严格的概率密度函数, 其在整个空间上的积分不等于 1 .\n\n习题 9-6 分析公式(9.14)和(9.15)来衡量稀疏性的效果.\n\n习题 9-7 对于一个 $C$ 类的分类问题，使用 $\\mathrm{K}$ 近邻方法估计每个类 $c(1 \\leq c \\leq C)$的密度函数 $p(\\boldsymbol{x} \\mid c)$, 并使用贝叶斯公式计算每个类的后验概率 $p(c \\mid \\boldsymbol{x})$.\n\n\\section*{参考文献}\n周志华, 2016. 机器学习[M]. 北京: 清华大学出版社.\n\nBengio Y, Lamblin P, Popovici D, et al., 2007. Greedy layer-wise training of deep networks[C]// Advances in neural information processing systems. 153-160.\n\nBishop C M, 2007. Pattern recognition and machine learning[M]. 5th edition. Springer.\n\nCover T, Hart P, 1967. Nearest neighbor pattern classification[J]. IEEE transactions on information theory, 13(1):21-27.\n\nDevroye L, Gyorfi L, 1985. Nonparametric density estimation: The $L_{1}$ view[M]. Wiley.\n\nDuda R O, Hart P E, Stork D G, 2001. Pattern classification[M]. 2nd edition. Wiley.\n\nHinton G E, Sejnowski T J, Poggio T A, 1999. Unsupervised learning: foundations of neural computation[M]. MIT press.\n\nOlshausen B A, et al., 1996. Emergence of simple-cell receptive field properties by learning a sparse code for natural images[J]. Nature, 381(6583):607-609.\n\nVincent P, Larochelle H, Bengio Y, et al., 2008. Extracting and composing robust features with denoising autoencoders[C]//Proceedings of the International Conference on Machine Learning. 1096-1103.\n\n\\section*{第 10 章 模型独立的学习方式}\n三个臭皮匠赛过诸葛亮.\n\n—谤语\n\n在前面的章节中, 我们已经介绍了机器学习的几种学习方式, 包括监督学习、无监督学习等. 这些学习方式分别可以由不同的模型和算法实现, 比如神经网络、线性分类器等. 针对一个给定的任务, 首先要准备一定规模的训练数据, 这些训练数据需要和真实数据的分布一致, 然后设定一个目标函数和优化方法, 在训练数据上学习一个模型. 此外, 不同任务的模型往往都是从零开始来训练的,一切知识都需要从训练数据中得到. 这也导致了每个任务都需要准备大量的训练数据. 在实际应用中, 我们面对的任务往往难以满足上述要求, 比如训练任务和目标任务的数据分布不一致, 训练数据过少等. 这时机器学习的应用会受到很大的局限. 并且在很多场合中, 我们也需要一个模型可以快速地适应新的任务.因此, 人们开始关注一些新的学习方式.\n\n本章介绍一些“模型独立的学习方式”,比如集成学习、协同学习、自训练、多任务学习、迁移学习、终身学习、小样本学习、元学习等. 这里 “模型独立” 是指这些学习方式不限于具体的模型, 不管是前馈神经网络、循环神经网络还是其他模型. 然而, 一种学习方式往往会对符合某种特性的模型更加青睐, 比如集成学习往往和方差大的模型组合时效果显著.\n\n\\section*{10.1 集成学习}\n给定一个学习任务, 假设输入 $\\boldsymbol{x}$ 和输出 $\\boldsymbol{y}$ 的真实关系为 $\\boldsymbol{y}=h(\\boldsymbol{x})$. 对于 $M$ 个不同的模型 $f_{1}(\\boldsymbol{x}), \\cdots, f_{M}(\\boldsymbol{x})$, 每个模型的期望错误为\n\n\n\\begin{equation*}\n\\mathcal{R}\\left(f_{m}\\right)=\\mathbb{E}_{x}\\left[\\left(f_{m}(\\boldsymbol{x})-h(\\boldsymbol{x})\\right)^{2}\\right] \\tag{10.1}\n\\end{equation*}\n\n\n\n\\begin{equation*}\n=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\epsilon_{m}(\\boldsymbol{x})^{2}\\right] \\tag{10.2}\n\\end{equation*}\n\n\n其中 $\\epsilon_{m}(\\boldsymbol{x})=f_{m}(\\boldsymbol{x})-h(\\boldsymbol{x})$ 为模型 $m$ 在样本 $\\boldsymbol{x}$ 上的错误.\n\n那么所有的模型的平均错误为\n\n\n\\begin{equation*}\n\\overline{\\mathcal{R}}(f)=\\frac{1}{M} \\sum_{m=1}^{M} \\mathbb{E}_{\\boldsymbol{x}}\\left[\\epsilon_{m}(\\boldsymbol{x})^{2}\\right] \\tag{10.3}\n\\end{equation*}\n\n\n集成学习 (Ensemble Learning) 就是通过某种策略将多个模型集成起来,通过群体决策来提高决策准确率. 集成学习首要的问题是如何集成多个模型. 比较常用的集成策略有直接平均、加权平均等.\n\n最直接的集成学习策略就是直接平均,即“投票”. 基于投票的集成模型 $F(\\boldsymbol{x})$为\n\n\n\\begin{equation*}\nF(\\boldsymbol{x})=\\frac{1}{M} \\sum_{m=1}^{M} f_{m}(\\boldsymbol{x}) \\tag{10.4}\n\\end{equation*}\n\n\n定理 10.1：对于 $M$ 个不同的模型 $f_{1}(\\boldsymbol{x}), \\cdots, f_{M}(\\boldsymbol{x})$, 其平均期望错误为 $\\overline{\\mathcal{R}}(f)$. 基于简单投票机制的集成模型 $F(\\boldsymbol{x})=\\frac{1}{M} \\sum_{m=1}^{M} f_{m}(\\boldsymbol{x}), F(\\boldsymbol{x})$ 的期望错误在 $\\frac{1}{M} \\overline{\\mathcal{R}}(f)$ 和 $\\overline{\\mathcal{R}}(f)$ 之间.\n\n证明. 根据定义,集成模型的期望错误为\n\n\n\\begin{align*}\n\\mathcal{R}(F) & =\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\frac{1}{M} \\sum_{m=1}^{M} f_{m}(\\boldsymbol{x})-h(\\boldsymbol{x})\\right)^{2}\\right]  \\tag{10.5}\\\\\n& =\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\frac{1}{M} \\sum_{m=1}^{M} \\epsilon_{m}(\\boldsymbol{x})\\right)^{2}\\right]  \\tag{10.6}\\\\\n& =\\frac{1}{M^{2}} \\mathbb{E}_{\\boldsymbol{x}}\\left[\\sum_{m=1}^{M} \\sum_{n=1}^{M} \\epsilon_{m}(\\boldsymbol{x}) \\epsilon_{n}(\\boldsymbol{x})\\right]  \\tag{10.7}\\\\\n& =\\frac{1}{M^{2}} \\sum_{m=1}^{M} \\sum_{n=1}^{M} \\mathbb{E}_{\\boldsymbol{x}}\\left[\\epsilon_{m}(\\boldsymbol{x}) \\epsilon_{n}(\\boldsymbol{x})\\right], \\tag{10.8}\n\\end{align*}\n\n\n其中 $\\mathbb{E}_{\\boldsymbol{x}}\\left[\\epsilon_{m}(\\boldsymbol{x}) \\epsilon_{n}(\\boldsymbol{x})\\right]$ 为两个不同模型错误的相关性. 如果每个模型的错误不相关, 即 $\\forall m \\neq n, \\mathbb{E}_{\\boldsymbol{x}}\\left[\\epsilon_{m}(\\boldsymbol{x}) \\epsilon_{n}(\\boldsymbol{x})\\right]=0$. 如果每个模型的错误都是相同的, 则 $\\forall m \\neq$ $n, \\epsilon_{m}(\\boldsymbol{x})=\\epsilon_{n}(\\boldsymbol{x})$. 并且, 由于 $\\epsilon_{m}(\\boldsymbol{x}) \\geq 0, \\forall m$, 可以得到\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\\begin{equation*}\n\\overline{\\mathcal{R}}(f) \\geq \\mathcal{R}(F) \\geq \\frac{1}{M} \\overline{\\mathcal{R}}(f) \\tag{10.9}\n\\end{equation*}\n\n\n即集成模型的期望错误大于等于所有模型的平均期望错误的 $1 / M$, 小于等于所有模型的平均期望错误.\n\n从定理10.1可知, 为了得到更好的集成效果, 要求每个模型之间具备一定的差异性. 并且随着模型数量的增多,其错误率也会下降,并趋近于 0 .\n\n集成学习的思想可以用一句古老的谚语来描述: “三个臭皮匠赛过诸葛亮”.但是一个有效的集成需要各个基模型的差异尽可能大. 为了增加模型之间的差异性,可以采取 Bagging 和 Boosting 这两类方法.\n\nBagging类方法 Bagging 类方法是通过随机构造训练样本、随机选择特征等方法来提高每个基模型的独立性,代表性方法有 Bagging和随机森林等.\n\nBagging ( Bootstrap Aggregating) 是通过不同模型的训练数据集的独立性来提高不同模型之间的独立性. 我们在原始训练集上进行有放回的随机采样, 得到 $M$ 个比较小的训练集并训练 $M$ 个模型,然后通过投票的方法进行模型集成.\n\n随机森林 ( Random Forest ) [Breiman, 2001] 是在 Bagging 的基础上再引入了随机特征, 进一步提高每个基模型之间的独立性. 在随机森林中, 每个基模型都是一棵决策树.\n\nBoosting 类方法 Boosting 类方法是按照一定的顺序来先后训练不同的基模型, 每个模型都针对前序模型的错误进行专门训练. 根据前序模型的结果, 来调整训练样本的权重, 从而增加不同基模型之间的差异性. Boosting 类方法是一种非常强大的集成方法, 只要基模型的准确率比随机猜测高, 就可以通过集成方法来显著地提高集成模型的准确率. Boosting 类方法的代表性方法有 AdaBoost[Freund et al., 1996]等."
    },
    {}
]