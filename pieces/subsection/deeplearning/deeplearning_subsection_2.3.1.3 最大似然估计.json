[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.3.1.3 最大似然估计",
        "content_length": 2562,
        "content": "Title: 2.3.1.3 最大似然估计\nContent:\n机器学习任务可以分为两类: 一类是样本的特征向量 $\\boldsymbol{x}$ 和标签 $y$ 之间存在未知的函数关系 $y=h(\\boldsymbol{x})$, 另一类是条件概率 $p(y \\mid \\boldsymbol{x})$ 服从某个未知分布.第2.3.1.1节中介绍的最小二乘法是属于第一类, 直接建模 $\\boldsymbol{x}$ 和标签 $y$ 之间的函数关系. 此外, 线性回归还可以从建模条件概率 $p(y \\mid \\boldsymbol{x})$ 的角度来进行参数估计.\n\n假设标签 $y$ 为一个随机变量, 并由函数 $f(\\boldsymbol{x} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{x}$ 加上一个随机噪声 $\\epsilon$决定,即\n\n这里把 $\\boldsymbol{x}$ 看作确定值的参数.\n\n\n\\begin{equation*}\ny=f(\\boldsymbol{x} ; \\boldsymbol{w})+\\epsilon, \\tag{2.45}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n共线性 (Collinearity)是指一个特征可以通过其他特征的线性组合来较准确地预测.\n\n参见习题2-4.\n\n\n\\begin{equation*}\n=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+\\epsilon \\tag{2.46}\n\\end{equation*}\n\n\n其中 $\\epsilon$ 服从均值为 0 、方差为 $\\sigma^{2}$ 的高斯分布. 这样, $y$ 服从均值为 $\\boldsymbol{w}^{\\top} \\boldsymbol{x}$ 、方差为 $\\sigma^{2}$的高斯分布:\n\n\n\\begin{align*}\np(y \\mid \\boldsymbol{x} ; \\boldsymbol{w}, \\sigma) & =\\mathcal{N}\\left(y ; \\boldsymbol{w}^{\\top} \\boldsymbol{x}, \\sigma^{2}\\right)  \\tag{2.47}\\\\\n& =\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(-\\frac{\\left(y-\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)^{2}}{2 \\sigma^{2}}\\right) . \\tag{2.48}\n\\end{align*}\n\n\n参数 $\\boldsymbol{w}$ 在训练集 $\\mathcal{D}$ 上的似然函数 ( Likelihood ) 为\n\n\n\\begin{align*}\np(\\boldsymbol{y} \\mid \\boldsymbol{X} ; \\boldsymbol{w}, \\sigma) & =\\prod_{n=1}^{N} p\\left(y^{(n)} \\mid \\boldsymbol{x}^{(n)} ; \\boldsymbol{w}, \\sigma\\right)  \\tag{2.49}\\\\\n& =\\prod_{n=1}^{N} \\mathcal{N}\\left(y^{(n)} ; \\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}, \\sigma^{2}\\right) \\tag{2.50}\n\\end{align*}\n\n\n其中 $\\boldsymbol{y}=\\left[y^{(1)}, \\cdots, y^{(N)}\\right]^{\\top}$ 为所有样本标签组成的向量, $\\boldsymbol{X}=\\left[\\boldsymbol{x}^{(1)}, \\cdots, \\boldsymbol{x}^{(N)}\\right]$ 为所有样本特征向量组成的矩阵.\n\n为了方便计算, 对似然函数取对数得到对数似然函数 (Log Likelihood ):\n\n\n\\begin{equation*}\n\\log p(\\boldsymbol{y} \\mid \\boldsymbol{X} ; \\boldsymbol{w}, \\sigma)=\\sum_{n=1}^{N} \\log \\mathcal{N}\\left(y^{(n)} ; \\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}, \\sigma^{2}\\right) \\tag{2.51}\n\\end{equation*}\n\n\n最大似然估计 (Maximum Likelihood Estimation, MLE) 是指找到一组参数 $\\boldsymbol{w}$ 使得似然函数 $p(\\boldsymbol{y} \\mid \\boldsymbol{X} ; \\boldsymbol{w}, \\sigma)$ 最大，等价于对数似然函数 $\\log p(\\boldsymbol{y} \\mid \\boldsymbol{X} ; \\boldsymbol{w}, \\sigma)$最大.\n\n\n\\begin{align*}\n\\text { 令 } \\frac{\\partial \\log p(\\boldsymbol{y} \\mid \\boldsymbol{X} ; \\boldsymbol{w}, \\sigma)}{\\partial \\boldsymbol{w}}=0, \\text { 得到 } & \\\\\n& \\boldsymbol{w}^{M L}=\\left(\\boldsymbol{X} \\boldsymbol{X}^{\\top}\\right)^{-1} \\boldsymbol{X} \\boldsymbol{y} . \\tag{2.52}\n\\end{align*}\n\n\n可以看出,最大似然估计的解和最小二乘法的解相同."
    },
    {
        "id": "chatcmpl-940obWjtzsrJ352hhNP9sZMXSDfaT",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"最大似然估计和最小二乘法有什么区别？\",\n    \"回答1\": \"最大似然估计是通过找到一组参数使得似然函数最大化来进行参数估计，而最小二乘法是直接建模特征向量和标签之间的函数关系。最大似然估计更多地关注参数使得观测数据出现的可能性最大，而最小二乘法则更加关注预测值与真实值之间的误差平方和。\",\n    \"问题2\": \"在线性回归中，标签 $y$ 是如何由特征向量 $\\boldsymbol{x}$ 和参数 $\\boldsymbol{w}$ 决定的？\",\n    \"回答2\": \"在线性回归中，标签 $y$ 由函数 $f(\\boldsymbol{x} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{x}$ 加上一个随机噪声 $\\boldsymbol{\\text{epsilon}}$ 决定，即 $y=f(\\boldsymbol{x} ; \\boldsymbol{w})+\\boldsymbol{\\text{epsilon}}$。\",\n    \"问题3\": \"什么是共线性（Collinearity）？\",\n    \"回答3\": \"共线性是指一个特征可以通过其他特征的线性组合来较准确地预测。在特征之间存在共线性时，会导致模型参数估计不稳定，增加了模型的复杂度和过拟合的风险。\",\n    \"问题4\": \"标签 $y$ 在线性回归中服从什么分布？\",\n    \"回答4\": \"在线性回归中，假设标签 $y$ 服从均值为 $\\boldsymbol{w}^{\\top} \\boldsymbol{x}$、方差为 $\\boldsymbol{\\text{sigma}}^{2}$ 的高斯分布。\",\n    \"问题5\": \"最大似然估计的核心思想是什么？\",\n    \"回答5\": \"最大似然估计的核心思想是找到一组参数使得观测数据出现的可能性最大化。通过最大化似然函数，寻找使得观测数据出现概率最大的参数值，从而进行参数估计。\",\n    \"问题6\": \"对数似然函数在最大似然估计中有什么作用？\",\n    \"回答6\": \"对数似然函数在最大似然估计中起到了简化计算和优化的作用。通过对似然函数取对数得到对数似然函数，可以将连乘转换为连加，简化了计算。同时，对数似然函数的最大化等价于原始似然函数的最大化。\",\n    \"问题7\": \"最大似然估计的求解方法是什么？\",\n    \"回答7\": \"最大似然估计通过求解对数似然函数关于参数的偏导数为零的方程，得到使得观测数据出现概率最大的参数值。具体而言，在线性回归中，最大似然估计的解为 $\\boldsymbol{w}^{M L}=\\big(\\boldsymbol{X} \\boldsymbol{X}^{\\top}\\big)^{-1} \\boldsymbol{X} \\boldsymbol{y}$。\",\n    \"问题8\": \"对于给定的训练集 $\\boldsymbol{D}$，最大似然估计是如何定义的？\",\n    \"回答8\": \"对于给定的训练集 $\\boldsymbol{D}$，参数 $\\boldsymbol{w}$ 在训练集 $\\boldsymbol{D}$ 上的似然函数定义为 $p(\\boldsymbol{y} \\big| \\boldsymbol{X} ; \\boldsymbol{w}, \\boldsymbol{\\text{sigma}})=\\bigg(\\text{连乘}_{n=1}^{N} p\\big(y^{(n)} \\big| \\boldsymbol{x}^{(n)} ; \\boldsymbol{w}, \\boldsymbol{\\text{sigma}}\\big)\\bigg)$。\",\n    \"问题9\": \"最大似然估计和最小二乘法的解有什么关系？\",\n    \"回答9\": \"可以看出，最大似然估计的解和最小二乘法的解相同。这意味着在一些情况下，通过不同的角度进行参数估计所得到的结果是一致的。\",\n    \"问题10\": \"在机器学习任务中，为什么需要进行参数估计？\",\n    \"回答10\": \"在机器学习任务中，需要进行参数估计是因为模型中的许多参数都是未知的，并且这些参数直接影响着模型对数据的拟合程度和预测能力。通过参数估计，可以利用观测数据来确定模型中的未知参数值，从而使模型更好地适应数据并具有更好的泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710743473,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1327,
            "prompt_tokens": 3853,
            "total_tokens": 5180
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]