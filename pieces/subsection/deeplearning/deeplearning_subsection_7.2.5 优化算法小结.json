[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.2.5 优化算法小结",
        "content_length": 2170,
        "content": "Title: 7.2.5 优化算法小结\nContent:\n本节介绍的几种优化方法大体上可以分为两类: 1 ) 调整学习率, 使得优化更稳定;2 ) 梯度估计修正,优化训练速度.\n\n表7.1汇总了本节介绍的几种神经网络常用优化算法.\n\n这些优化算法可以使用下面公式来统一描述概括:\n\n\n\\begin{align*}\n\\Delta \\theta_{t} & =-\\frac{\\alpha_{t}}{\\sqrt{G_{t}+\\epsilon}} M_{t}  \\tag{7.32}\\\\\nG_{t} & =\\psi\\left(\\mathbf{g}_{1}, \\cdots, \\boldsymbol{g}_{t}\\right)  \\tag{7.33}\\\\\nM_{t} & =\\phi\\left(\\mathbf{g}_{1}, \\cdots, \\boldsymbol{g}_{t}\\right) \\tag{7.34}\n\\end{align*}\n\n\n在训练循环神经网络时, 按模截断是避免梯度爆炸问题的有效方法。\n\n参见习题7-3.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n表 7.1 神经网络常用优化方法的汇总\n\n\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\n\\multicolumn{2}{|r|}{类别} & 优化算法 \\\\\n\\hline\n\\multirow{3}{*}{\n} & 固定衰减学习率 & 分段常数衰减、逆时衰减、(自然) 指数衰减、余弦衰减 \\\\\n\\hline\n & 周期性学习率 & 循环学习率、SGDR \\\\\n\\hline\n & 自适应学习率 & AdaGrad 、RMSprop $、$ AdaDelta \\\\\n\\hline\n & 梯度估计修正 & 动量法、Nesterov 加速梯度、梯度截断 \\\\\n\\hline\n & 综合方法 & Adam $\\approx$ 动量法 + RMSprop \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n其中 $\\boldsymbol{g}_{t}$ 是第 $t$ 步的梯度; $\\alpha_{t}$ 是第 $t$ 步的学习率, 可以进行衰减, 也可以不变; $\\psi(\\cdot)$是学习率缩放函数, 可以取 1 或历史梯度的模的移动平均; $\\phi(\\cdot)$ 是优化后的参数更新方向,可以取当前的梯度 $\\boldsymbol{g}_{t}$ 或历史梯度的移动平均.\n\n图7.8给出了这几种优化方法在 MNIST 数据集上收敛性的比较 (学习率为 0.001 , 批量大小为 128 ).\n\n\n\n图 7.8 不同优化方法的比较\n\n\\section*{7.3 参数初始化}\n神经网络的参数学习是一个非凸优化问题. 当使用梯度下降法来进行优化网络参数时, 参数初始值的选取十分关键, 关系到网络的优化效率和泛化能力.参数初始化的方式通常有以下三种:\n\n（1）预训练初始化: 不同的参数初始值会收敛到不同的局部最优解. 虽然这些局部最优解在训练集上的损失比较接近, 但是它们的泛化能力差异很大. 一个好的初始值会使得网络收玫到一个泛化能力高的局部最优解. 通常情况下,一个已经在大规模数据上训练过的模型可以提供一个好的参数初始值, 这种初始化方法称为预训练初始化 (Pre-trained Initialization ).\n\n预训练任务可以为监督学习或无监督学习任务. 由于无监督学习任务更容易获取大规模的训练数据, 因此被广泛采用. 预训练模型在目标任务上的学习过程也称为精调 (Fine-Tuning).\n\n(2) 随机初始化: 在线性模型的训练 (比如感知器和 Logistic 回归) 中, 我们一般将参数全部初始化为 0 . 但是这在神经网络的训练中会存在一些问题. 因为如果参数都为 0 , 在第一遍前向计算时, 所有的隐藏层神经元的激活值都相同; 在反向传播时, 所有权重的更新也都相同, 这样会导致隐藏层神经元没有区分性. 这种现象也称为对称权重现象. 为了打破这个平衡, 比较好的方式是对每个参数都随机初始化 (Random Initialization), 使得不同神经元之间的区分性更好.\n\n(3) 固定值初始化: 对于一些特殊的参数, 我们可以根据经验用一个特殊的固定值来进行初始化. 比如偏置 (Bias) 通常用 0 来初始化, 但是有时可以设置某些经验值以提高优化效率. 在 LSTM 网络的遗忘门中, 偏置通常初始化为 1 或 2 , 使得时序上的梯度变大. 对于使用 ReLU 的神经元, 有时也可以将偏置设为 0.01 , 使得 ReLU 神经元在训练初期更容易激活, 从而获得一定的梯度来进行误差反向传播.\n\n虽然预训练初始化通常具有更好的收玫性和泛化性, 但是灵活性不够, 不能在目标任务上任意地调整网络结构. 因此, 好的随机初始化方法对训练神经网络模型来说依然十分重要. 这里我们介绍三类常用的随机初始化方法: 基于固定方差的参数初始化、基于方差缩放的参数初始化和正交初始化方法."
    },
    {
        "id": "chatcmpl-943TyIBR3uomj3ZkzbLTPguMnDqoU",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"介绍了哪两类神经网络优化方法？\",\n    \"回答1\": \"介绍的几种优化方法大体上可以分为两类: 1) 调整学习率，使得优化更稳定；2) 梯度估计修正，优化训练速度。\",\n    \"问题2\": \"如何统一描述概括本节介绍的几种优化算法？\",\n    \"回答2\": \"这些优化算法可以使用公式 $\\boldsymbol{\\theta}_{t} = -\\frac{\\boldsymbol{\\theta}_{t}}{\\big(\\boldsymbol{G}_{t}+\\boldsymbol{\\text{ε}}\\big)^{\\frac{1}{2}}} \\boldsymbol{M}_{t}$ 来统一描述概括。其中 $\\boldsymbol{G}_{t} = \\boldsymbol{\\text{ψ}}\\big(\\boldsymbol{g}_{1}, \\text{...}, \\boldsymbol{g}_{t}\\big)$ 以及 $\\boldsymbol{M}_{t} = \\boldsymbol{\\text{φ}}\\big(\\boldsymbol{g}_{1}, \\text{...}, \\boldsymbol{g}_{t}\\big)$。\",\n    \"问题3\": \"在训练循环神经网络时，什么是避免梯度爆炸问题的有效方法？\",\n    \"回答3\": \"在训练循环神经网络时，按模截断是避免梯度爆炸问题的有效方法。\",\n    \"问题4\": \"神经网络参数初始化的方式通常有哪三种？\",\n    \"回答4\": \"神经网络参数初始化的方式通常有以下三种：预训练初始化、随机初始化和固定值初始化。\",\n    \"问题5\": \"预训练初始化和随机初始化各有什么特点？\",\n    \"回答5\": \"预训练初始化通常具有更好的收敛性和泛化性，但灵活性不够，不能在目标任务上任意地调整网络结构。而随机初始化方法则可以打破对称权重现象，使得不同神经元之间的区分性更好。\",\n    \"问题6\": \"为什么好的随机初始化方法对训练神经网络模型来说依然十分重要？\",\n    \"回答6\": \"好的随机初始化方法对训练神经网络模型来说依然十分重要，因为它可以打破对称权重现象，使得不同神经元之间的区分性更好，从而提高网络的表达能力和学习能力。\",\n    \"问题7\": \"在LSTM网络的遗忘门中，偏置通常如何进行初始化？\",\n    \"回答7\": \"在LSTM网络的遗忘门中，偏置通常初始化为1或2，以使时序上的梯度变大。\",\n    \"问题8\": \"表7.1汇总了哪几种神经网络常用优化算法？\",\n    \"回答8\": \"表7.1汇总了固定衰减学习率、周期性学习率、自适应学习率、梯度估计修正和综合方法等几种神经网络常用优化算法。\",\n    \"问题9\": \"图7.8给出了这几种优化方法在MNIST数据集上收敛性的比较，学习率和批量大小分别是多少？\",\n    \"回答9\": \"图7.8给出了这几种优化方法在MNIST数据集上收敛性的比较，学习率为0.001，批量大小为128。\",\n    \"问题10\": \"公式$\\boldsymbol{\\theta}_{t} = -\\frac{\\boldsymbol{\\theta}_{t}}{\\big(\\boldsymbol{G}_{t}+\\boldsymbol{\\text{ε}}\\big)^{\\frac{1}{2}}} \\boldsymbol{M}_{t}$中的符号和变量分别代表什么意义？\",\n    \"回答10\": \"公式$\\boldsymbol{\\theta}_{t} = -\\frac{\\boldsymbol{\\theta}_{t}}{\\big(\\boldsymbol{G}_{t}+\\boldsymbol{\\text{ε}}\\big)^{\\frac{1}{2}}} \\boldsymbol{M}_{t}$中的符号和变量分别代表参数更新量、学习率、历史梯度模的移动平均和优化后的参数更新方向。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710753726,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1168,
            "prompt_tokens": 4232,
            "total_tokens": 5400
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]