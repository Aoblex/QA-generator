[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "15.3.1.2 特征层",
        "content_length": 2111,
        "content": "Title: 15.3.1.2 特征层\nContent:\n特征层用于从输入向量序列 $\\boldsymbol{e}_{1}, \\cdots, \\boldsymbol{e}_{t-1}$ 中提取特征, 输出为一个可以表示历史信息的向量 $\\boldsymbol{h}_{t}$.\n\n特征层可以通过不同类型的神经网络 (比如前馈神经网络和循环神经网络等) 来实现. 常见的网络类型有以下三种:\n\n\\section*{(1) 简单平均}\n历史信息的向量 $\\boldsymbol{h}_{t}$ 为前面 $t-1$ 个词向量的平均,即\n\n\n\\begin{equation*}\n\\boldsymbol{h}_{t}=\\sum_{i=1}^{t-1} \\alpha_{i} \\boldsymbol{e}_{i} \\tag{15.24}\n\\end{equation*}\n\n\n其中 $\\alpha_{i}$ 为每个词的权重.\n\n权重 $\\alpha_{i}$ 可以和位置 $i$ 及其表示 $\\boldsymbol{e}_{i}$ 相关, 也可以无关. 为简单起见, 可以设置 $\\alpha_{i}=\\frac{1}{t-1}$. 权重 $\\alpha_{i}$ 也可以通过注意力机制来动态计算.\n\n(2) 前馈神经网络\n\n前馈神经网络要求输入的大小是固定的. 因此, 和 $\\mathrm{N}$ 元模型类似, 假设历史信息只包含前面 $N-1$ 个词. 首先将这 $N-1$ 个词向量 $\\boldsymbol{e}_{t-N+1}, \\cdots, \\boldsymbol{e}_{t-1}$ 拼接成一个 $D_{x} \\times(N-1)$ 维的向量 $\\boldsymbol{h}^{\\prime}$, 即\n\n\n\\begin{equation*}\n\\boldsymbol{h}^{\\prime}=\\boldsymbol{e}_{t-N+1} \\oplus \\cdots \\oplus \\boldsymbol{e}_{t-1} \\tag{15.25}\n\\end{equation*}\n\n\n其中 $\\oplus$ 表示向量拼接操作.\n\n然后将 $\\boldsymbol{h}^{\\prime}$ 输入到由前馈神经网络构成的隐藏层, 最后一层隐藏层的输出 $\\boldsymbol{h}_{t}$,即\n\n\n\\begin{equation*}\n\\boldsymbol{h}_{t}=g\\left(\\boldsymbol{h}^{\\prime} ; \\theta_{g}\\right) \\tag{15.26}\n\\end{equation*}\n\n\n其中 $g\\left(\\cdot ; \\theta_{g}\\right)$ 可以为全连接的前馈神经网络或卷积神经网络, $\\theta_{g}$ 为网络参数.\n\n为了增加特征的多样性和提高模型训练效率, 前馈神经网络中也可以包含跳层连接 (Skip-Layer Connection) [Bengio et al., 2003], 比如\n\n\n\\begin{equation*}\n\\boldsymbol{h}_{t}=\\boldsymbol{h}^{\\prime} \\oplus g\\left(\\boldsymbol{h}^{\\prime} ; \\theta_{g}\\right) \\text {. } \\tag{15.27}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%8F%82%E8%A7%81%E7%AC%AC8.2%E8%8A%82}{https://nndl.github.io/注意力机制参见第8.2节}.\n\n参见习题15-3.\n\n$N$ 为超参数.\n\n跳层连接是指前馈神经网络中的某一神经层可以接受来自非相邻的低层信息, 其思想和残差网络中的直连边并不完全一样. 残差网络参见第5.4.4节.\n\n\\section*{(3) 循环神经网络}\n和前馈神经网络不同, 循环神经网络可以接受变长的输入序列, 依次接受输入 $\\boldsymbol{e}_{1}, \\cdots, \\boldsymbol{e}_{t-1}$, 得到时刻 $t$ 的隐藏状态\n\n\n\\begin{equation*}\n\\boldsymbol{h}_{t}=g\\left(\\boldsymbol{h}_{t-1}, \\boldsymbol{e}_{t} ; \\theta_{g}\\right), \\tag{15.28}\n\\end{equation*}\n\n\n其中 $g(\\cdot)$ 为一个非线性函数, $\\theta_{g}$ 为循环神经网络的参数, $\\boldsymbol{h}_{0}=0$.\n\n前馈神经网络模型和循环神经网络模型的不同之处在于循环神经网络利用隐藏状态来记录以前所有时刻的信息, 而前馈神经网络只能接受前 $N-1$ 个时刻的信息."
    },
    {
        "id": "chatcmpl-946UNuCaOkLLZUriiCmzXtin9SSXE",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"特征层的作用是什么？\",\n    \"回答1\": \"特征层用于从输入向量序列中提取特征，输出为一个可以表示历史信息的向量。它在神经网络中起着重要的作用，能够帮助模型更好地理解和利用输入序列中的信息。\",\n    \"问题2\": \"简单平均是如何计算历史信息的向量 $\\boldsymbol{h}_{t}$ 的？\",\n    \"回答2\": \"简单平均计算历史信息的向量 $\\boldsymbol{h}_{t}$ 为前面 $t-1$ 个词向量的平均，即 $\\boldsymbol{h}_{t}=\\text{sum}_{i=1}^{t-1} \\boldsymbol{\\text{α}}_{i} \\boldsymbol{e}_{i}$。其中 $\\boldsymbol{\\text{α}}_{i}$ 为每个词的权重，可以根据位置 $i$ 及其表示 $\\boldsymbol{e}_{i}$ 相关或无关。\",\n    \"问题3\": \"前馈神经网络中的跳层连接是指什么？\",\n    \"回答3\": \"前馈神经网络中的跳层连接是指某一神经层可以接受来自非相邻的低层信息。它可以增加特征的多样性和提高模型训练效率，使得网络结构更加灵活和适应不同类型的任务。\",\n    \"问题4\": \"前馈神经网络中隐藏层输出 $\\boldsymbol{h}_{t}$ 是如何计算得到的？\",\n    \"回答4\": \"前馈神经网络中隐藏层输出 $\\boldsymbol{h}_{t}$ 是通过将输入向量 $\\boldsymbol{h}^{\\text{'}}$ 输入到由前馈神经网络构成的隐藏层得到的，即 $\\boldsymbol{h}_{t}=g\\big(\\boldsymbol{h}^{\\text{'}} ; \\theta_{g}\\big)$。其中 $g\\big(\\boldsymbol{\\text{·}} ; \\theta_{g}\\big)$ 可以为全连接的前馈神经网络或卷积神经网络，$\\theta_{g}$ 为网络参数。\",\n    \"问题5\": \"循环神经网络和前馈神经网络模型的主要区别是什么？\",\n    \"回答5\": \"循环神经网络可以接受变长的输入序列，并利用隐藏状态来记录以前所有时刻的信息，而前馈神经网络只能接受前 $N-1$ 个时刻的信息。这使得循环神经网络在处理序列数据时具有更强的表达能力和灵活性。\",\n    \"问题6\": \"特征层通过哪些类型的神经网络来实现？\",\n    \"回答6\": \"特征层可以通过不同类型的神经网络（比如前馈神经网络和循环神经网络等）来实现。常见的网络类型有简单平均、前馈神经网络和循环神经网络。\",\n    \"问题7\": \"简单平均中权重 $\\boldsymbol{\\text{α}}_{i}$ 的设置方式有哪些？\",\n    \"回答7\": \"简单起见，可以设置权重 $\\boldsymbol{\\text{α}}_{i}=\\frac{1}{t-1}$，也可以通过注意力机制来动态计算。这样可以根据不同需求和任务来灵活地调整权重设置方式。\",\n    \"问题8\": \"为什么需要在前馈神经网络中引入跳层连接？\",\n    \"回答8\": \"引入跳层连接可以增加特征的多样性和提高模型训练效率。跳层连接使得网络结构更加灵活，能够更好地适应不同类型的任务和数据特征，提高模型的表达能力和泛化能力。\",\n    \"问题9\": \"特征层输出向量 $\\boldsymbol{h}_{t}$ 的维度是如何确定的？\",\n    \"回答9\": \"特征层输出向量 $\\boldsymbol{h}_{t}$ 的维度取决于输入向量序列 $\\boldsymbol{e}_{1}, \\text{...}, \\boldsymbol{e}_{t-1}$ 和所采用的特征提取方法。不同的方法可能会产生不同维度的输出向量，通常需要根据具体任务和模型设计来确定输出向量的维度。\",\n    \"问题10\": \"简单平均和前馈神经网络在提取历史信息方面有何异同之处？\",\n    \"回答10\": \"简单平均通过对历史词向量进行加权平均来提取历史信息，而前馈神经网络则通过隐藏层进行信息处理和抽取特征。简单平均相对简单直接，而前馈神经网络能够更灵活地学习和表达历史信息之间的复杂关系。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710765283,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1342,
            "prompt_tokens": 3856,
            "total_tokens": 5198
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]