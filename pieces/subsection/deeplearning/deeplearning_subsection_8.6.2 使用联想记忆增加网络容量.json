[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "8.6.2 使用联想记忆增加网络容量",
        "content_length": 5405,
        "content": "Title: 8.6.2 使用联想记忆增加网络容量\nContent:\n既然联想记忆具有存储和检索功能, 我们可以利用联想记忆来增加网络容量. 和结构化的外部记忆相比, 联想记忆具有更好的生物学解释性. 比如, 我们可以将一个联想记忆模型作为部件引入 LSTM 网络中, 从而在不引入额外参数的情况下增加网络容量 [Danihelka et al., 2016]; 或者将循环神经网络中的部分连接权重作为短期记忆, 并通过一个联想记忆模型进行更新, 从而提高网络性能 [Ba et al., 2016]. 在上述的网络中, 联想记忆都是作为一个更大网络的组件, 用来增加短期记忆的容量. 联想记忆组件的参数可以使用 Hebbian 方式来学习, 也可以作为整个网络参数的一部分来学习.\n\n\\section*{8.7 总结和深入阅读}\n注意力机制是一种 (不严格的) 受人类神经系统启发的信息处理机制. 比如人视觉神经系统并不会一次性地处理所有接受到的视觉信息, 而是有选择性地处理部分信息, 从而提高其工作效率.\n\n在人工智能领域, 注意力这一概念最早是在计算机视觉中提出, 用来提取图像特征. [Itti et al., 1998] 提出了一种自下而上的注意力模型. 该模型通过提取局部的低级视觉特征, 得到一些潜在的显著 (salient) 区域. 在神经网络中, [Mnih et al., 2014] 在循环神经网络模型上使用了注意力机制来进行图像分类. [Bahdanau et al., 2014] 使用注意力机制在机器翻译任务上将翻译和对齐同时进行. 目前, 注意力机制已经在语音识别、图像标题生成、阅读理解、文本分类、机器翻译等多个任务上取得了很好的效果, 也变得越来越流行. 注意力机制的一个重要应用是自注意力. 自注意力可以作为神经网络中的一层来使用, 有效地建模长距离依赖问题 [Vaswani et al., 2017].\n\n联想记忆是人脑的重要能力, 涉及人脑中信息的存储和检索机制, 因此对人工神经网络都有着重要的指导意义. 通过引入外部记忆, 神经网络在一定程度上可以增加模型容量. 这类引入外部记忆的模型也称为记忆增强神经网络. 记忆增强神经网络的代表性模型有神经图灵机 [Graves et al., 2014]、端到端记忆网络 [Sukhbaatar et al., 2015]、动态记忆网络 [Kumar et al., 2016] 等. 此外, 基于神经动力学的联想记忆也可以作为一种外部记忆, 并具有更好的生物学解释性. [Hopfield, 1984] 将能量函数的概念引入到神经网络模型中, 提出了 Hopfield 网络. Hopfield 网络在旅行商问题上获得了当时最好结果, 引起轰动. 有一些学者 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n将联想记忆模型作为部件引入循环神经网络中来增加网络容量 [Ba et al., 2016; Danihelka et al., 2016], 但受限于联想记忆模型的存储和检索效率, 这类方法收效有限. 目前人工神经网络中的外部记忆模型结构还比较简单, 需要借鉴神经科学的研究成果,提出更有效的记忆模型,增加网络容量.\n\n\\section*{习题}\n习题 8-1 分析 LSTM 模型中隐藏层神经元数量与参数数量之间的关系.\n\n习题 8-2 分析缩放点积模型可以缓解 Softmax 函数梯度消失的原因.\n\n参见公式(8.4).\n\n习题 8-3 当将自注意力模型作为一个神经层使用时, 分析它和卷积层以及循环参见第8.3节。层在建模长距离依赖关系的效率和计算复杂度方面的差异.\n\n习题 8-4 试设计用集合、树、栈或队列来组织外部记忆,并分析它们的差异.\n\n习题 8-5 分析端到端记忆网络和神经图灵机对外部记忆操作的异同点.\n\n习题 8-6 证明 Hopfield 网络的能量函数随时间单调递减.\n\n\\section*{参考文献}\nBa J, Hinton G E, Mnih V, et al., 2016. Using fast weights to attend to the recent past[C]//Advances In Neural Information Processing Systems. 4331-4339.\n\nBahdanau D, Cho K, Bengio Y, 2014. Neural machine translation by jointly learning to align and translate[J]. ArXiv e-prints.\n\nDanihelka I, Wayne G, Uria B, et al., 2016. Associative long short-term memory[C]//Proceedings of the 33nd International Conference on Machine Learning. 1986-1994.\n\nGraves A, Wayne G, Danihelka I, 2014. Neural turing machines[J]. arXiv preprint arXiv:1410.5401. Hopfield J J, 1984. Neurons with graded response have collective computational properties like those of two-state neurons[J]. Proceedings of the national academy of sciences, 81(10):3088-3092.\n\nItti L, Koch C, Niebur E, 1998. A model of saliency-based visual attention for rapid scene analysis [J]. IEEE Transactions on Pattern Analysis \\& Machine Intelligence(11):1254-1259.\n\nKim Y, Denton C, Hoang L, et al., 2017. Structured attention networks[C]//Proceedings of 5th International Conference on Learning Representations.\n\nKohonen T, 2012. Self-organization and associative memory: volume 8[M]. Springer Science \\& Business Media.\n\nKumar A, Irsoy O, Ondruska P, et al., 2016. Ask me anything: Dynamic memory networks for natural language processing $[\\mathrm{C}] / /$ Proceedings of the 33nd International Conference on Machine Learning. 1378-1387.\n\nMnih V, Heess N, Graves A, et al., 2014. Recurrent models of visual attention[C]//Advances in Neural Information Processing Systems. 2204-2212.\n\nPlate T A, 1995. Holographic reduced representations[J]. IEEE Transactions on Neural networks, 6(3):623-641.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\nSukhbaatar S, Weston J, Fergus R, et al., 2015. End-to-end memory networks[C]//Advances in Neural Information Processing Systems. 2431-2439.\n\nThompson R F, 1975. Introduction to physiological psychology[M]. HarperCollins Publishers.\n\nTuring A M, 1937. On computable numbers, with an application to the entscheidungsproblem[J]. Proceedings of the London mathematical society, 2(1):230-265.\n\nVaswani A, Shazeer N, Parmar N, et al., 2017. Attention is all you need[C]//Advances in Neural Information Processing Systems. 6000-6010.\n\nVinyals O, Fortunato M, Jaitly N, 2015. Pointer networks[C]//Advances in Neural Information Processing Systems. 2692-2700.\n\nYang Z, Yang D, Dyer C, et al., 2016. Hierarchical attention networks for document classification. [C]//HLT-NAACL. 1480-1489.\n\n\\section*{第9章 无监督学习}\n大脑有大约 $10^{14}$ 个突触, 我们只能活大约 $10^{9}$ 秒. 所以我们有比数据更多的参数。这启发了我们必须进行大量无监督学习的想法, 因为感知输入 (包括本体感受) 是我们可以获得每秒 $10^{5}$ 维约束的唯一途径.\n\n—杰弗里$\\cdot$辛顿 (Geoffrey Hinton)\n\n2018 年图灵奖获得者\n\n无监督学习 (Unsupervised Learning, UL) 是指从无标签的数据中学习出一些有用的模式. 无监督学习算法一般直接从原始数据中学习, 不借助于任何人工给出标签或者反馈等指导信息. 如果监督学习是建立输入-输出之间的映射关系, 那么无监督学习就是发现隐藏的数据中的有价值信息, 包括有效的特征、类别、结构以及概率分布等.\n\n典型的无监督学习问题可以分为以下几类:\n\n(1) 无监督特征学习 (Unsupervised Feature Learning) 是从无标签的训练数据中挖掘有效的特征或表示. 无监督特征学习一般用来进行降维、数据可视化或监督学习前期的数据预处理.\n\n(2) 概率密度估计 (Probabilistic Density Estimation) 简称密度估计, 是根据一组训练样本来估计样本空间的概率密度. 密度估计可以分为参数密度估计和非参数密度估计. 参数密度估计是假设数据服从某个已知概率密度函数形式的分布 ( 比如高斯分布), 然后根据训练样本去估计概率密度函数的参数. 非参数密度估计是不假设数据服从某个已知分布, 只利用训练样本对密度进行估计, 可以进行任意形状密度的估计. 非参数密度估计的方法有直方图、核密度估计等.\n\n（3）聚类 (Clustering) 是将一组样本根据一定的准则划分到不同的组 (也称为簇 (Cluster)). 一个比较通用的准则是组内样本的相似性要高于组间样本的相似性. 常见的聚类算法包括 K-Means 算法、谱聚类等.\n\n特征学习也包含很多的监督学习算法, 比如线性判别分析等.这里数字是指数量级. 更早的正式描述见 [Hinton et al., 1999].\\\\\n和监督学习一样, 无监督学习方法也包含三个基本要素: 模型、学习准则和优化算法. 无监督学习的准则非常多, 比如最大似然估计、最小重构错误等. 在无监督特征学习中, 经常使用的准则为最小化重构错误, 同时也经常对特征进行一些约束, 比如独立性、非负性或稀释性等. 而在密度估计中, 经常采用最大似然估计来进行学习.\n\n本章介绍两种无监督学习问题:无监督特征学习和概率密度估计.\n\n\\section*{9.1 无监督特征学习}\n无监督特征学习是指从无标注的数据中自动学习有效的数据表示, 从而能够帮助后续的机器学习模型更快速地达到更好的性能. 无监督特征学习主要方法有主成分分析、稀疏编码、自编码器等."
    },
    {
        "id": "chatcmpl-944ZIqlSpaQimjDYJCi2ZJTkJ0XmB",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"联想记忆和结构化的外部记忆有何区别？\",\n    \"回答1\": \"联想记忆具有更好的生物学解释性，可以作为一个更大网络的组件，用来增加短期记忆的容量。相比之下，结构化的外部记忆是指通过引入外部记忆，神经网络在一定程度上可以增加模型容量，这类引入外部记忆的模型也称为记忆增强神经网络。\",\n    \"问题2\": \"如何在 LSTM 网络中利用联想记忆来增加网络容量？\",\n    \"回答2\": \"可以将一个联想记忆模型作为部件引入 LSTM 网络中，从而在不引入额外参数的情况下增加网络容量。另一种方法是将循环神经网络中的部分连接权重作为短期记忆，并通过一个联想记忆模型进行更新，从而提高网络性能。\",\n    \"问题3\": \"注意力机制在人工智能领域的应用有哪些？\",\n    \"回答3\": \"注意力机制已经在语音识别、图像标题生成、阅读理解、文本分类、机器翻译等多个任务上取得了很好的效果，并变得越来越流行。它还被用于在神经网络中进行自注意力建模，有效地建模长距离依赖问题。\",\n    \"问题4\": \"什么是无监督学习？它与监督学习有何区别？\",\n    \"回答4\": \"无监督学习是指从无标签的数据中学习出一些有用的模式，包括有效的特征、类别、结构以及概率分布等。与监督学习不同，无监督学习算法一般直接从原始数据中学习，不借助于任何人工给出标签或者反馈等指导信息。\",\n    \"问题5\": \"无监督特征学习主要方法有哪些？\",\n    \"回答5\": \"无监督特征学习主要方法包括主成分分析、稀疏编码、自编码器等。这些方法旨在从无标注的数据中自动学习有效的数据表示，帮助后续的机器学习模型更快速地达到更好的性能。\",\n    \"问题6\": \"无监督学习中常见的问题类型有哪些？\",\n    \"回答6\": \"典型的无监督学习问题包括无监督特征学习和概率密度估计。其中，无监督特征学习是从无标签的训练数据中挖掘有效的特征或表示；而概率密度估计是根据一组训练样本来估计样本空间的概率密度。\",\n    \"问题7\": \"密度估计方法中常见的两种类型是什么？它们有何区别？\",\n    \"回答7\": \"密度估计方法包括参数密度估计和非参数密度估计。参数密度估计假设数据服从某个已知概率密度函数形式的分布，然后根据训练样本去估计概率密度函数的参数；而非参数密度估计则不假设数据服从某个已知分布，只利用训练样本对密度进行估计，可以进行任意形状密度的估计。\",\n    \"问题8\": \"聚类算法是指什么？它在无监督学习中起到什么作用？\",\n    \"回答8\": \"聚类算法是将一组样本根据一定的准则划分到不同的组（也称为簇）。在无监督学习中，聚类算法可以帮助发现数据中隐藏的结构和模式，对数据进行有效的组织和分类。\",\n    \"问题9\": \"无监督学习与有监督学习相比，其模型、学习准则和优化算法有何异同？\",\n    \"回答9\": \"与监督学习一样，无监督学习也包含三个基本要素：模型、学习准则和优化算法。不同之处在于无监督学习的准则非常多，常见的准则包括最大似然估计、最小重构错误等；而在监督学习中，则是建立输入-输出之间的映射关系。\",\n    \"问题10\": \"为什么大脑需要进行大量无监督学习？\",\n    \"回答10\": \"大脑有大约 $10^{14}$ 个突触，而人只能活大约 $10^{9}$ 秒。因此，大脑需要进行大量无监督学习来处理感知输入，因为感知输入是我们可以获得每秒 $10^{5}$ 维约束的唯一途径。这启发了我们必须进行大量无监督学习以获取更多信息。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710757900,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1439,
            "prompt_tokens": 5903,
            "total_tokens": 7342
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]