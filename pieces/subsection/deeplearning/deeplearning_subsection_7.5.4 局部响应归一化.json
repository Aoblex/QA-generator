[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.5.4 局部响应归一化",
        "content_length": 2094,
        "content": "Title: 7.5.4 局部响应归一化\nContent:\n局部响应归一化 ( Local Response Normalization,LRN ) [Krizhevsky et al., 2012] 是一种受生物学启发的归一化方法, 通常用在基于卷积的图像处理上.\n\n假设一个卷积层的输出特征映射 $\\boldsymbol{Y} \\in \\mathbb{R}^{M^{\\prime} \\times N^{\\prime} \\times P}$ 为三维张量, 其中每个切片矩阵 $\\boldsymbol{Y}^{p} \\in \\mathbb{R}^{M^{\\prime} \\times N^{\\prime}}$ 为一个输出特征映射, $1 \\leq p \\leq P$.\n\n参见公式(5.24).\n\n局部响应归一化是对邻近的特征映射进行局部归一化.\n\n\n\\begin{align*}\n\\hat{\\boldsymbol{Y}}^{p} & =\\boldsymbol{Y}^{p} /\\left(k+\\alpha \\sum_{j=\\max \\left(1, p-\\frac{n}{2}\\right)}^{\\min \\left(P, p+\\frac{n}{2}\\right)}\\left(\\boldsymbol{Y}^{j}\\right)^{2}\\right)^{\\beta}  \\tag{7.65}\\\\\n& \\triangleq \\operatorname{LRN}_{n, k, \\alpha, \\beta}\\left(\\boldsymbol{Y}^{p}\\right) \\tag{7.66}\n\\end{align*}\n\n\n其中除和幂运算都是按元素运算, $n, k, \\alpha, \\beta$ 为超参, $n$ 为局部归一化的特征窗口大小. 在 AlexNet 中, 这些超参的取值为 $n=5, k=2, \\alpha=10 \\mathrm{e}^{-4}, \\beta=0.75$.\n\n局部响应归一化和层归一化都是对同层的神经元进行归一化. 不同的是, 局部响应归一化应用在激活函数之后, 只是对邻近的神经元进行局部归一化, 并且不减去均值.\n\n局部响应归一化和生物神经元中的侧抑制 ( lateral inhibition) 现象比较类似,即活跃神经元对相邻神经元具有抑制作用. 当使用 ReLU作为激活函数时, 神经元的活性值是没有限制的, 局部响应归一化可以起到平衡和约束作用. 如果一个神经元的活性值非常大, 那么和它邻近的神经元就近似地归一化为 0 , 从而起到抑制作用, 增强模型的泛化能力. 最大汇聚也具有侧抑制作用. 但最大汇聚是对同一个特征映射中的邻近位置中的神经元进行抑制, 而局部响应归一化是对同一个位置的邻近特征映射中的神经元进行抑制.\n\n邻近的神经元指对应同样位置的邻近特征映射。\n\n\\section*{7.6 超参数优化}\n在神经网络中, 除了可学习的参数之外, 还存在很多超参数. 这些超参数对网络性能的影响也很大. 不同的机器学习任务往往需要不同的超参数. 常见的超参数有以下三类:\n\n（1）网络结构, 包括神经元之间的连接关系、层数、每层的神经元数量、激活函数的类型等.\n\n（2）优化参数,包括优化方法、学习率、小批量的样本数量等.\n\n(3) 正则化系数.\n\n超参数优化 (Hyperparameter Optimization) 主要存在两方面的困难: 1 )超参数优化是一个组合优化问题, 无法像一般参数那样通过梯度下降方法来优化, 也没有一种通用有效的优化方法; 2 ) 评估一组超参数配置 (Configuration)的时间代价非常高, 从而导致一些优化方法 (比如演化算法 (Evolution Algorithm )) 在超参数优化中难以应用.\n\n假设一个神经网络中总共有 $K$ 个超参数, 每个超参数配置表示为一个向量 $x \\in X, X \\subset \\mathbb{R}^{K}$ 是超参数配置的取值空间. 超参数优化的目标函数定义为 $f(\\boldsymbol{x}): X \\rightarrow \\mathbb{R}, f(\\boldsymbol{x})$ 是衡量一组超参数配置 $\\boldsymbol{x}$ 效果的函数,一般设置为开发集上的错误率. 目标函数 $f(\\boldsymbol{x})$ 可以看作一个黑盒 (black-box) 函数, 不需要知道其具体形式. 虽然在神经网络的超参数优化中, $f(\\boldsymbol{x})$ 的函数形式已知, 但 $f(\\boldsymbol{x})$ 不是关于 $\\boldsymbol{x}$ 的连续函数, 并且 $\\boldsymbol{x}$ 不同, $f(\\boldsymbol{x})$ 的函数形式也不同, 因此无法使用梯度下降等优化方法.\n\n对于超参数的配置, 比较简单的方法有网格搜索、随机搜索、贝叶斯优化、动态资源分配和神经架构搜索."
    },
    {
        "id": "chatcmpl-943qsT50B7ZbFebDyEmw1waOZJ7WW",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"局部响应归一化与层归一化有何区别？\",\n    \"回答1\": \"局部响应归一化和层归一化都是对同层的神经元进行归一化。不同的是，局部响应归一化应用在激活函数之后，只是对邻近的神经元进行局部归一化，并且不减去均值；而层归一化是对整个层的神经元进行归一化，通过减去均值和除以标准差来实现。\",\n    \"问题2\": \"局部响应归一化的公式中的超参数 $n, k, \\text{和} \\beta$ 分别代表什么意义？\",\n    \"回答2\": \"在局部响应归一化的公式中，超参数 $n$ 代表局部归一化的特征窗口大小，$k$ 是一个常数项，$\\beta$ 是幂运算中的指数。这些超参数影响着局部响应归一化对邻近特征映射中神经元的抑制程度和平衡作用。\",\n    \"问题3\": \"局部响应归一化与生物神经元中的侧抑制现象有何相似之处？\",\n    \"回答3\": \"局部响应归一化和生物神经元中的侧抑制现象类似，即活跃神经元对相邻神经元具有抑制作用。当使用 ReLU 作为激活函数时，神经元的活性值是没有限制的。局部响应归一化可以起到平衡和约束作用，当一个神经元的活性值非常大时，它会近似地抑制邻近神经元的活性值，从而增强模型的泛化能力。\",\n    \"问题4\": \"为什么超参数优化在神经网络中存在困难？\",\n    \"回答4\": \"超参数优化在神经网络中存在困难主要有两方面原因：首先，超参数优化是一个组合优化问题，无法像一般参数那样通过梯度下降方法来优化，并且没有一种通用有效的优化方法；其次，评估一组超参数配置的时间代价非常高，导致一些优化方法在超参数优化中难以应用。\",\n    \"问题5\": \"超参数优化的目标函数 $f(\\boldsymbol{x})$ 的特点是什么？\",\n    \"回答5\": \"超参数优化的目标函数 $f(\\boldsymbol{x})$ 是衡量一组超参数配置 $\\boldsymbol{x}$ 效果的函数，通常设置为开发集上的错误率。它可以看作一个黑盒函数，不需要知道其具体形式。虽然在神经网络的超参数优化中，$f(\\boldsymbol{x})$ 的函数形式已知，但它不是关于 $\\boldsymbol{x}$ 的连续函数，并且 $\\boldsymbol{x}$ 不同，则 $f(\\boldsymbol{x})$ 的函数形式也不同。\",\n    \"问题6\": \"超参数优化中常用的简单方法有哪些？\",\n    \"回答6\": \"超参数优化中常用的简单方法包括网格搜索、随机搜索、贝叶斯优化、动态资源分配和神经架构搜索。这些方法通过不同的策略来搜索超参数空间，并寻找最优或较优的超参数配置。\",\n    \"问题7\": \"神经网络中常见的三类超参数分别是什么？\",\n    \"回答7\": \"在神经网络中，常见的三类超参数包括网络结构（如神经元之间的连接关系、层数、每层的神经元数量、激活函数的类型等）、优化参数（如优化方法、学习率、小批量的样本数量等）和正则化系数。这些超参数对网络性能具有重要影响。\",\n    \"问题8\": \"为什么需要通过很多神经元一起协作来完成复杂的功能？\",\n    \"回答8\": \"要模拟人脑的能力，单一的神经元是远远不够的。复杂的功能需要通过很多神经元一起协作来完成。这样通过一定的连接方式或信息传递方式进行协作的神经元可以看作一个网络，即神经网络。通过神经网络中多个神经元之间的协作，可以实现更加复杂和高级的信息处理和学习能力。\",\n    \"问题9\": \"Maxout 单元相比传统激活函数有什么优势？\",\n    \"回答9\": \"Maxout 单元不仅仅是净输入到输出之间的非线性映射，而是整体学习输入到输出之间的非线性映射关系。它可以看作任意凸函数的分段线性近似，并且在有限的点上是不可微的。这使得 Maxout 单元在一定程度上能够更灵活地适应复杂的非线性关系，相比传统激活函数具有更强的表达能力。\",\n    \"问题10\": \"局部响应归一化 (LRN) 通常用在哪种类型的图像处理上？\",\n    \"回答10\": \"局部响应归一化 (LRN) 通常用在基于卷积的图像处理上。它受到生物学启发，通过对邻近特征映射中神经元进行局部归一化来增强模型泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710755146,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1569,
            "prompt_tokens": 4054,
            "total_tokens": 5623
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]