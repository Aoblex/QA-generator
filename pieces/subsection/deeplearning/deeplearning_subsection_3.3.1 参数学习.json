[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "3.3.1 参数学习",
        "content_length": 5486,
        "content": "Title: 3.3.1 参数学习\nContent:\n给定 $N$ 个训练样本 $\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, Softmax 回归使用交叉熵损失函数来学习最优的参数矩阵 $\\boldsymbol{W}$.\n\n为了方便起见, 我们用 $C$ 维的 one-hot 向量 $\\boldsymbol{y} \\in\\{0,1\\}^{C}$ 来表示类别标签. 对于类别 $c$,其向量表示为\n\n\n\\begin{equation*}\n\\boldsymbol{y}=[I(1=c), I(2=c), \\cdots, I(C=c)]^{\\top} \\tag{3.37}\n\\end{equation*}\n\n\n其中 $I(\\cdot)$ 是指示函数.\n\n采用交叉熵损失函数,Softmax 回归模型的风险函数为\n\n\n\\begin{align*}\n\\mathcal{R}(\\boldsymbol{W}) & =-\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{c=1}^{C} \\boldsymbol{y}_{c}^{(n)} \\log \\hat{\\boldsymbol{y}}_{c}^{(n)}  \\tag{3.38}\\\\\n& =-\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\boldsymbol{y}^{(n)}\\right)^{\\top} \\log \\hat{\\boldsymbol{y}}^{(n)} \\tag{3.39}\n\\end{align*}\n\n\n其中 $\\hat{\\boldsymbol{y}}^{(n)}=\\operatorname{softmax}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{x}^{(n)}\\right)$ 为样本 $\\boldsymbol{x}^{(n)}$ 在每个类别的后验概率.\n\n风险函数 $\\mathcal{R}(\\boldsymbol{W})$ 关于 $\\boldsymbol{W}$ 的梯度为\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{R}(\\boldsymbol{W})}{\\partial \\boldsymbol{W}}=-\\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}\\left(\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}^{(n)}\\right)^{\\top} \\tag{3.40}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n为简单起见, 这里忽略了正则化项.\\\\\n证明. 计算公式 (3.40) 中的梯度, 关键在于计算每个样本的损失函数 $\\mathcal{L}^{(n)}(\\boldsymbol{W})=$ $-\\left(\\boldsymbol{y}^{(n)}\\right)^{\\top} \\log \\hat{\\boldsymbol{y}}^{(n)}$ 关于参数 $\\boldsymbol{W}$ 的梯度, 其中需要用到的两个导数公式为\n\n(1) 若 $\\boldsymbol{y}=\\operatorname{softmax}(\\boldsymbol{z})$, 则 $\\frac{\\partial \\boldsymbol{y}}{\\partial z}=\\operatorname{diag}(\\boldsymbol{y})-\\boldsymbol{y} \\boldsymbol{y}^{\\top}$.\n\n（2）若 $\\boldsymbol{z}=\\boldsymbol{W}^{\\top} \\boldsymbol{x}=\\left[\\boldsymbol{w}_{1}^{\\top} \\boldsymbol{x}, \\boldsymbol{w}_{2}^{\\top} \\boldsymbol{x}, \\cdots, \\boldsymbol{w}_{C}^{\\top} \\boldsymbol{x}\\right]^{\\top}$, 则 $\\frac{\\partial z}{\\partial \\boldsymbol{w}_{c}}$ 为第 $c$ 列为 $\\boldsymbol{x}$, 其余为 0 Softmax 函数的导数参见第B.4.2.2节.的矩阵.\n\n\n\\begin{align*}\n\\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{w}_{c}} & =\\left[\\frac{\\partial \\boldsymbol{w}_{1}^{\\top} \\boldsymbol{x}}{\\partial \\boldsymbol{w}_{c}}, \\frac{\\partial \\boldsymbol{w}_{2}^{\\top} \\boldsymbol{x}}{\\partial \\boldsymbol{w}_{c}}, \\cdots, \\frac{\\partial \\boldsymbol{w}_{C}^{\\top} \\boldsymbol{x}}{\\partial \\boldsymbol{w}_{c}}\\right]  \\tag{3.41}\\\\\n& =[0,0, \\cdots, \\boldsymbol{x}, \\cdots, 0]  \\tag{3.42}\\\\\n& \\triangleq \\mathbb{M}_{c}(\\boldsymbol{x}) . \\tag{3.43}\n\\end{align*}\n\n\n根据链式法则, $\\mathcal{L}^{(n)}(\\boldsymbol{W})=-\\left(\\boldsymbol{y}^{(n)}\\right)^{\\top} \\log \\hat{\\boldsymbol{y}}^{(n)}$ 关于 $\\boldsymbol{w}_{c}$ 的偏导数为\n\n\n\\begin{align*}\n& \\frac{\\partial \\mathcal{L}^{(n)}(\\boldsymbol{W})}{\\partial \\boldsymbol{w}_{c}}=-\\frac{\\partial\\left(\\left(\\boldsymbol{y}^{(n)}\\right)^{\\top} \\log \\hat{\\boldsymbol{y}}^{(n)}\\right)}{\\partial \\boldsymbol{w}_{c}}  \\tag{3.44}\\\\\n& =-\\frac{\\partial \\boldsymbol{z}^{(n)}}{\\partial \\boldsymbol{w}_{c}} \\frac{\\partial \\hat{\\boldsymbol{y}}^{(n)}}{\\partial \\boldsymbol{z}^{(n)}} \\frac{\\partial \\log \\hat{\\boldsymbol{y}}^{(n)}}{\\partial \\hat{\\boldsymbol{y}}^{(n)}} \\boldsymbol{y}^{(n)}  \\tag{3.45}\\\\\n& =-\\mathbb{M}_{c}\\left(\\boldsymbol{x}^{(n)}\\right)\\left(\\operatorname{diag}\\left(\\hat{\\boldsymbol{y}}^{(n)}\\right)-\\hat{\\boldsymbol{y}}^{(n)}\\left(\\hat{\\boldsymbol{y}}^{(n)}\\right)^{\\top}\\right)\\left(\\operatorname{diag}\\left(\\hat{\\boldsymbol{y}}^{(n)}\\right)\\right)^{-1} \\boldsymbol{y}^{(n)}  \\tag{3.46}\\\\\n& =-\\mathbb{M}_{c}\\left(\\boldsymbol{x}^{(n)}\\right)\\left(\\boldsymbol{I}-\\hat{\\boldsymbol{y}}^{(n)} \\mathbf{1}_{C}^{\\top}\\right) \\boldsymbol{y}^{(n)}  \\tag{3.47}\\\\\n& =-\\mathbb{M}_{c}\\left(\\boldsymbol{x}^{(n)}\\right)\\left(\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}^{(n)} \\mathbf{1}_{C}^{\\top} \\boldsymbol{y}^{(n)}\\right)  \\tag{3.48}\\\\\n& =-\\mathbb{M}_{c}\\left(\\boldsymbol{x}^{(n)}\\right)\\left(\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}^{(n)}\\right)  \\tag{3.49}\\\\\n& =-\\boldsymbol{x}^{(n)}\\left[\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}^{(n)}\\right]_{c} \\cdot \\tag{3.50}\n\\end{align*}\n\n\n公式(3.50)也可以表示为非向量形式,即\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{L}^{(n)}(\\boldsymbol{W})}{\\partial \\boldsymbol{w}_{c}}=-\\boldsymbol{x}^{(n)}\\left(I\\left(y^{(n)}=c\\right)-\\hat{\\boldsymbol{y}}_{c}^{(n)}\\right), \\tag{3.51}\n\\end{equation*}\n\n\n其中 $I(\\cdot)$ 是指示函数.\n\n根据公式(3.50) 可以得到\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{L}^{(n)}(\\boldsymbol{W})}{\\partial \\boldsymbol{W}}=-\\boldsymbol{x}^{(n)}\\left(\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}^{(n)}\\right)^{\\top} . \\tag{3.52}\n\\end{equation*}\n\n\n采用梯度下降法, Softmax 回归的训练过程为 : 初始化 $\\boldsymbol{W}_{0} \\leftarrow 0$, 然后通过下式进行迭代更新：\n\n\n\\begin{equation*}\n\\boldsymbol{W}_{t+1} \\leftarrow \\boldsymbol{W}_{t}+\\alpha\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)}\\left(\\boldsymbol{y}^{(n)}-\\hat{\\boldsymbol{y}}_{\\boldsymbol{W}_{t}}^{(n)}\\right)^{\\top}\\right) \\tag{3.53}\n\\end{equation*}\n\n\n其中 $\\alpha$ 是学习率, $\\hat{\\boldsymbol{y}}_{\\boldsymbol{W}_{t}}^{(n)}$ 是当参数为 $\\boldsymbol{W}_{t}$ 时, Softmax 回归模型的输出.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n要注意的是,Softmax 回归中使用的 $C$ 个权重向量是兄余的, 即对所有的权重向量都减去一个同样的向量 $\\boldsymbol{v}$, 不改变其输出结果. 因此, Softmax 回归往往需要使用正则化来约束其参数. 此外, 我们还可以利用这个特性来避免计算Softmax 函数时在数值计算上溢出问题.\n\n\\section*{3.4 感知器}\n感知器 ( Perceptron) 由 Frank Roseblatt于 1957 年提出, 是一种广泛使用的线性分类器. 感知器可谓是最简单的人工神经网络, 只有一个神经元.\n\n感知器是对生物神经元的简单数学模拟, 有与生物神经元相对应的部件, 如权重 ( 突触) 、偏置 (阈值) 及激活函数 (细胞体), 输出为 +1 或 -1 .\n\n感知器是一种简单的两类线性分类模型, 其分类准则与公式 (3.5)相同, 即\n\n\n\\begin{equation*}\n\\hat{y}=\\operatorname{sgn}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right) . \\tag{3.54}\n\\end{equation*}"
    },
    {
        "id": "chatcmpl-94188E9MuM7RxivPX4H2bTscGKAlc",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"Softmax 回归中的交叉熵损失函数是如何定义的？\",\n    \"回答1\": \"Softmax 回归模型使用交叉熵损失函数来衡量预测输出与实际标签之间的差异。具体而言，风险函数为 $\\\\mathcal{R}(\\\\boldsymbol{W})=-\\\\frac{1}{N} \\\\sum_{n=1}^{N} \\\\sum_{c=1}^{C} \\\\boldsymbol{y}_{c}^{(n)} \\\\log \\\\hat{\\\\boldsymbol{y}}_{c}^{(n)}$，其中 $\\\\hat{\\\\boldsymbol{y}}^{(n)}=\\\\operatorname{softmax}(\\\\boldsymbol{W}^{\\\\top} \\\\boldsymbol{x}^{(n)})$ 为样本 $\\\\boldsymbol{x}^{(n)}$ 在每个类别的后验概率。\",\n    \"问题2\": \"Softmax 回归中的梯度下降法是如何应用的？\",\n    \"回答2\": \"在 Softmax 回归中，采用梯度下降法进行参数学习。具体而言，通过初始化参数矩阵 $\\\\boldsymbol{W}_{0}$，然后通过迭代更新 $\\\\boldsymbol{W}_{t+1} \\\\leftarrow \\\\boldsymbol{W}_{t}+\\\\alpha\\\\left(\\\\frac{1}{N} \\\\sum_{n=1}^{N} \\\\boldsymbol{x}^{(n)}\\\\left(\\\\boldsymbol{y}^{(n)}-\\\\hat{\\\\boldsymbol{y}}_{\\\\boldsymbol{W}_{t}}^{(n)}\\\\right)^{\\\\top}\\\\right)$，其中 $\\\\alpha$ 是学习率。\",\n    \"问题3\": \"Softmax 回归中的参数学习过程中是否考虑了正则化？\",\n    \"回答3\": \"在 Softmax 回归中，由于权重向量是兄余的，即对所有的权重向量都减去一个同样的向量 $\\\\boldsymbol{v}$ 不改变其输出结果。因此，Softmax 回归往往需要使用正则化来约束其参数，以避免过拟合问题。\",\n    \"问题4\": \"感知器是什么？它与生物神经元有何相似之处？\",\n    \"回答4\": \"感知器是一种广泛使用的线性分类器，是最简单的人工神经网络，只有一个神经元。它是对生物神经元的简单数学模拟，具有与生物神经元相对应的部件，如权重（突触）、偏置（阈值）及激活函数（细胞体），输出为 +1 或 -1。\",\n    \"问题5\": \"感知器的分类准则是什么？\",\n    \"回答5\": \"感知器是一种简单的两类线性分类模型，其分类准则为 $\\\\hat{y}=\\\\operatorname{sgn}(\\\\boldsymbol{w}^{\\\\top} \\\\boldsymbol{x})$，即根据输入特征和权重向量的线性组合结果来判断样本属于哪一类。\",\n    \"问题6\": \"Softmax 回归中的交叉熵损失函数对参数矩阵 $\\\\boldsymbol{W}$ 的梯度是如何计算的？\",\n    \"回答6\": \"Softmax 回归中，风险函数 $\\\\mathcal{R}(\\\\boldsymbol{W})$ 关于 $\\\\boldsymbol{W}$ 的梯度为 $\\\\frac{\\\\partial \\\\mathcal{R}(\\\\boldsymbol{W})}{\\\\partial \\\\boldsymbol{W}}=-\\\\frac{1}{N} \\\\sum_{n=1}^{N} \\\\boldsymbol{x}^{(n)}\\\\left(\\\\boldsymbol{y}^{(n)}-\\\\hat{\\\\boldsymbol{y}}^{(n)}\\\\right)^{\\\\top}$。\",\n    \"问题7\": \"在 Softmax 回归中，如何计算每个样本的损失函数关于参数 $\\\\boldsymbol{W}$ 的梯度？\",\n    \"回答7\": \"在 Softmax 回归中，每个样本的损失函数 $\\\\mathcal{L}^{(n)}(\\\\boldsymbol{W})=-\\\\left(\\\\boldsymbol{y}^{(n)}\\\\right)^{\\\\top} \\\\log \\\\hat{\\\\boldsymbol{y}}^{(n)}$ 关于参数 $\\\\boldsymbol{W}$ 的梯度计算公式为 $\\\\frac{\\\\partial \\\\mathcal{L}^{(n)}(\\\\boldsymbol{W})}{\\\\partial \\\\boldsymbol{W}}=-\\\\boldsymbol{x}^{(n)}\\\\left(\\\\boldsymbol{y}^{(n)}-\\\\hat{\\\\boldsymbol{y}}^{(n)}\\\\right)^{\\\\top}$。\",\n    \"问题8\": \"在 Softmax 回归中，如何避免计算Softmax 函数时出现数值计算上溢出问题？\",\n    \"回答8\": \"在 Softmax 回归中，可以利用权重向量兄余的特性来避免计算Softmax 函数时出现数值计算上溢出问题。此外，也可以使用正则化来约束参数，以避免过拟合问题。\",\n    \"问题9\": \"Softmax 回归模型中的 one-hot 向量 $\\boldsymbol{y}$ 是如何表示类别标签的？\",\n    \"回答9\": \"在 Softmax 回归模型中，采用 $C$ 维的 one-hot 向量 $\\boldsymbol{y}$ 来表示类别标签。对于类别 $c$，其向量表示为 $\\boldsymbol{y}=[I(1=c), I(2=c), \\\\text{...}, I(C=c)]^{\\top}$，其中 $I(⋅)$ 是指示函数。\",\n    \"问题10\": \"Softmax 回归模型中采用的风险函数是什么？\",\n    \"回答10\": \"Softmax 回归模型采用交叉熵损失函数作为风险函数。具体而言，风险函数为 $R(W)=-\\frac{1}{N} \\textstyle\\big(\\textstyle\\bigsum_{n=1}^{N}\\textstyle\\bigsum_{c=1}^{C}\\big)y_{c}^{(n)} \\textstyle\\biglog \\big\\tilde y_{c}^{(n)}$。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710744684,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1652,
            "prompt_tokens": 5258,
            "total_tokens": 6910
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]