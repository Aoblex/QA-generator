[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "12.3.2 参数学习",
        "content_length": 830,
        "content": "Title: 12.3.2 参数学习\nContent:\n深度信念网络最直接的训练方式是最大化可观测变量的边际分布 $p(\\boldsymbol{v})$ 在训练集合上的似然. 但在深度信念网络中, 隐变量 $\\boldsymbol{h}$ 之间的关系十分复杂, 由于 “贡献度分配问题”, 很难直接学习. 即使对于简单的单层 Sigmoid 信念网络\n\n\n\\begin{equation*}\np(v=1 \\mid \\boldsymbol{h})=\\sigma\\left(b+\\boldsymbol{w}^{\\top} \\boldsymbol{h}\\right), \\tag{12.63}\n\\end{equation*}\n\n\\footnotetext{\\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n在已知可观测变量时, 其隐变量的联合后验概率 $p(\\boldsymbol{h} \\mid v)$ 不再互相独立, 因此很难精确估计所有隐变量的后验概率. 早期深度信念网络的后验概率一般通过蒙特卡罗方法或变分方法来近似估计, 但是效率比较低, 从而导致其参数学习比较困难.\n\n为了有效地训练深度信念网络, 我们将每一层的 Sigmoid 信念网络转换为受限玻尔兹曼机. 这样做的好处是隐变量的后验概率是互相独立的, 从而可以很容易地进行采样. 这样, 深度信念网络可以看作由多个受限玻尔兹曼机从下到上进行堆叠, 第 $l$ 层受限玻尔兹曼机的隐层作为第 $l+1$ 层受限玻尔兹曼机的可观测层. 进一步地, 深度信念网络可以采用逐层训练的方式来快速训练, 即从最底层开始, 每次只训练一层, 直到最后一层 [Hinton et al., 2006].\n\n深度信念网络的训练过程可以分为逐层预训练和精调两个阶段. 先通过逐层预训练将模型的参数初始化为较优的值, 再通过传统学习方法对参数进行精调."
    },
    {
        "id": "chatcmpl-945Pb6sX97wJmIVZPWDyfBV8Lpuki",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"深度信念网络最直接的训练方式是什么？\",\n    \"回答1\": \"深度信念网络最直接的训练方式是最大化可观测变量的边际分布 $p(\\boldsymbol{v})$ 在训练集合上的似然。这意味着通过最大化训练集合上可观测变量的边际概率分布的似然来进行训练。\",\n    \"问题2\": \"为什么深度信念网络中隐变量之间的关系很难直接学习？\",\n    \"回答2\": \"在深度信念网络中，隐变量 $\\boldsymbol{h}$ 之间的关系十分复杂，由于 “贡献度分配问题”，很难直接学习。即使对于简单的单层 Sigmoid 信念网络，在已知可观测变量时，其隐变量的联合后验概率 $p(\\boldsymbol{h} \\text{ 在 } v)$ 不再互相独立，因此很难精确估计所有隐变量的后验概率。\",\n    \"问题3\": \"深度信念网络的后验概率一般是如何进行近似估计的？\",\n    \"回答3\": \"早期深度信念网络的后验概率一般通过蒙特卡罗方法或变分方法来近似估计。这是因为在已知可观测变量时，其隐变量的联合后验概率不再互相独立，导致精确估计所有隐变量的后验概率比较困难。\",\n    \"问题4\": \"为什么将每一层的 Sigmoid 信念网络转换为受限玻尔兹曼机有助于有效地训练深度信念网络？\",\n    \"回答4\": \"将每一层的 Sigmoid 信念网络转换为受限玻尔兹曼机有助于有效地训练深度信念网络，因为这样做可以使隐变量的后验概率互相独立，从而可以很容易地进行采样。这种转换使得深度信念网络可以看作由多个受限玻尔兹曼机从下到上进行堆叠，进而可以采用逐层训练的方式来快速训练。\",\n    \"问题5\": \"深度信念网络是如何进行逐层训练的？\",\n    \"回答5\": \"深度信念网络可以采用逐层训练的方式来快速训练，即从最底层开始，每次只训练一层，直到最后一层。这种逐层训练方式利用了受限玻尔兹曼机结构的特点，能够有效地提高训练效率。\",\n    \"问题6\": \"深度信念网络的训练过程包括哪两个阶段？\",\n    \"回答6\": \"深度信念网络的训练过程包括逐层预训练和精调两个阶段。首先通过逐层预训练将模型的参数初始化为较优的值，然后再通过传统学习方法对参数进行精调。\",\n    \"问题7\": \"逐层预训练在深度信念网络中起到什么作用？\",\n    \"回答7\": \"逐层预训练在深度信念网络中起到将模型参数初始化为较优值的作用。通过逐层预训练，可以有效地引导模型参数朝着更优秀的初始状态发展，为后续的精调阶段提供更好的初始条件。\",\n    \"问题8\": \"为什么传统学习方法对参数进行精调在深度信念网络中是必要的？\",\n    \"回答8\": \"传统学习方法对参数进行精调在深度信念网络中是必要的，因为通过逐层预训练虽然可以得到较优的参数初始化值，但仍需要通过传统学习方法对参数进行进一步优化，以提高模型在实际数据上的表现。\",\n    \"问题9\": \"蒙特卡罗方法和变分方法在近似估计深度信念网络后验概率时有何不同？\",\n    \"回答9\": \"蒙特卡罗方法和变分方法在近似估计深度信念网络后验概率时有所不同。蒙特卡罗方法通过随机抽样来近似计算后验概率，而变分方法则通过寻找与真实后验概率分布最接近的参数化分布来进行近似。\",\n    \"问题10\": \"为什么深度信念网络中采用逐层训练方式来快速训练？\",\n    \"回答10\": \"深度信念网络中采用逐层训练方式来快速训练是因为这种方式利用了受限玻尔兹曼机结构的特点，能够有效地提高训练效率。通过逐层训练，可以更好地引导模型参数朝着更优秀的状态发展，并且能够避免一次性训练整个深度结构所带来的困难。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710761143,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1547,
            "prompt_tokens": 3264,
            "total_tokens": 4811
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]