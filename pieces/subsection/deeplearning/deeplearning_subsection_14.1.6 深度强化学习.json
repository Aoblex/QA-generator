[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "14.1.6 深度强化学习",
        "content_length": 1658,
        "content": "Title: 14.1.6 深度强化学习\nContent:\n在强化学习中, 一般需要建模策略 $\\pi(a \\mid s)$ 和值函数 $V^{\\pi}(s), Q^{\\pi}(s, a)$. 早期的强化学习算法主要关注状态和动作都是离散且有限的问题, 可以使用表格来记录这些概率. 但在很多实际问题中, 有些任务的状态和动作的数量非常多. 比如围棋的棋局有 $3^{361} \\approx 10^{170}$ 种状态, 动作 (即落子位置) 数量为 361 . 还有些任务的状态和动作是连续的. 比如在自动驾驶中, 智能体感知到的环境状态是各种传感器数据, 一般都是连续的. 动作是操作方向盘的方向 ( -90 度 $\\sim 90$ 度) 和速度控制 ( $0 \\sim 300$ 公里/小时), 也是连续的.\n\n为了有效地解决这些问题, 我们可以设计一个更强的策略函数 (比如深度神经网络), 使得智能体可以应对复杂的环境, 学习更优的策略, 并具有更好的泛化能力.\n\n深度强化学习 (Deep Reinforcement Learning) 是将强化学习和深度学习结合在一起, 用强化学习来定义问题和优化目标, 用深度学习来解决策略和值函数的建模问题, 然后使用误差反向传播算法来优化目标函数. 深度强化学习在一定程度上具备解决复杂问题的通用智能,并在很多任务上都取得了很大的成功.\n\n\\section*{14.2 基于值函数的学习方法}\n值函数是对策略 $\\pi$ 的评估. 如果策略 $\\pi$ 有限 (即状态数和动作数都有限),可以对所有的策略进行评估并选出最优策略 $\\pi^{*}$.\n\n\n\\begin{equation*}\n\\forall s, \\quad \\pi^{*}=\\underset{\\pi}{\\arg \\max } V^{\\pi}(s) . \\tag{14.22}\n\\end{equation*}\n\n\n但这种方式在实践中很难实现. 假设状态空间 $\\mathcal{S}$ 和动作空间 $\\mathcal{A}$ 都是离散且有限的,策略空间为 $|\\mathcal{A}|^{|\\mathcal{S}|}$, 往往也非常大.\n\n一种可行的方式是通过迭代的方法不断优化策略, 直到选出最优策略. 对于一个策略 $\\pi(a \\mid s)$, 其 $\\mathrm{Q}$ 函数为 $Q^{\\pi}(s, a)$, 我们可以设置一个新的策略 $\\pi^{\\prime}(a \\mid s)$,\n\n\\[\n\\pi^{\\prime}(a \\mid s)= \\begin{cases}1 & \\text { if } a=\\arg \\max _{\\hat{a}} Q^{\\pi}(s, \\hat{a})  \\tag{14.23}\\\\ 0 & \\text { otherwise }\\end{cases}\n\\]\n\n即 $\\pi^{\\prime}(a \\mid s)$ 为一个确定性的策略, 也可以直接写为\n\n\n\\begin{equation*}\n\\pi^{\\prime}(s)=\\underset{a}{\\arg \\max } Q^{\\pi}(s, a) . \\tag{14.24}\n\\end{equation*}\n\n\n如果执行 $\\pi^{\\prime}$,会有\n\n\n\\begin{equation*}\n\\forall s, \\quad V^{\\pi^{\\prime}}(s) \\geq V^{\\pi}(s) . \\tag{14.25}\n\\end{equation*}\n\n\n\\section*{参见习题14-2.}\n根据公式(14.25),我们可以通过下面方式来学习最优策略:先随机初始化一个策略, 计算该策略的值函数, 并根据值函数来设置新的策略, 然后一直反复迭代直到收敛.\n\n基于值函数的策略学习方法中最关键的是如何计算策略 $\\pi$ 的值函数,一般有动态规划或蒙特卡罗两种计算方式."
    },
    {
        "id": "chatcmpl-945q6wh5JlStMG7axqc1WvbrgPkpv",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"在深度强化学习中，为什么需要设计更强的策略函数？\",\n    \"回答1\": \"为了应对复杂的环境和学习更优的策略，并具有更好的泛化能力，需要设计一个更强的策略函数（比如深度神经网络），以有效地解决状态和动作数量非常多、连续的问题。\",\n    \"问题2\": \"深度强化学习是如何将强化学习和深度学习结合在一起的？\",\n    \"回答2\": \"深度强化学习将强化学习用于定义问题和优化目标，而将深度学习用于解决策略和值函数的建模问题。然后使用误差反向传播算法来优化目标函数，从而结合了强化学习和深度学习的优势。\",\n    \"问题3\": \"深度强化学习相比传统强化学习有哪些优势？\",\n    \"回答3\": \"深度强化学习在一定程度上具备解决复杂问题的通用智能，并在很多任务上都取得了很大的成功。相比传统强化学习，深度强化学习能够更好地处理状态和动作数量非常多、连续的问题，并具有更好的泛化能力。\",\n    \"问题4\": \"值函数在强化学习中扮演着什么样的角色？\",\n    \"回答4\": \"值函数是对策略 $\\boldsymbol{\\text{π}}$ 的评估。它可以用来评估策略的好坏，并且可以帮助选出最优策略 $\\boldsymbol{\\text{π}}^{*}$。通过值函数，可以对不同策略进行评估和比较，从而找到最优的决策方案。\",\n    \"问题5\": \"如何通过迭代方法来优化策略并选出最优策略？\",\n    \"回答5\": \"通过迭代方法，可以不断优化当前策略 $\\boldsymbol{\\text{π}}$，得到一个新的策略 $\\boldsymbol{\\text{π}}^{\\text{'}}$。新的策略 $\\boldsymbol{\\text{π}}^{\\text{'}}$ 会根据当前的 $\\boldsymbol{\\text{Q}}$ 函数来选择最优动作，从而使得对应状态的值函数增大。这样一直反复迭代直到收敛，就可以选出最优策略。\",\n    \"问题6\": \"基于值函数的策略学习方法中最关键的是什么？\",\n    \"回答6\": \"基于值函数的策略学习方法中最关键的是如何计算策略 $\\boldsymbol{\\text{π}}$ 的值函数。一般有动态规划或蒙特卡罗两种计算方式，这些方式对于有效地评估和优化策略至关重要。\",\n    \"问题7\": \"在实际问题中，为什么有些任务的状态和动作数量非常多？\",\n    \"回答7\": \"在很多实际问题中，有些任务的状态和动作数量非常多是因为任务本身非常复杂。比如围棋的棋局有 $3^{361} \\text{ 种状态}$，动作数量为 361；自动驾驶中智能体感知到的环境状态是各种传感器数据，动作是操作方向盘的方向和速度控制，都是连续且非常多样化的。\",\n    \"问题8\": \"在自动驾驶中，智能体感知到的环境状态和动作是如何描述的？\",\n    \"回答8\": \"在自动驾驶中，智能体感知到的环境状态是各种传感器数据，一般都是连续的。动作是操作方向盘的方向（-90 度 到 90 度）和速度控制（0 公里/小时 到 300 公里/小时），也是连续且多样化的。\",\n    \"问题9\": \"为什么早期的强化学习算法主要关注状态和动作都是离散且有限的问题？\",\n    \"回答9\": \"早期的强化学习算法主要关注状态和动作都是离散且有限的问题，因为这类问题可以使用表格来记录概率，并且相对容易进行建模和求解。但随着实际问题中复杂性的增加，需要更加灵活和适应性更强的方法来处理非常多状态和动作数量非常多、连续的问题。\",\n    \"问题10\": \"为什么需要将深度学习用于解决策略和值函数的建模问题？\",\n    \"回筗10\": \"将深度学习用于解决策略和值函数的建模问题是为了应对复杂环境下智能体面临的挑战。深度神经网络等更强大的模型可以更好地适应复杂环境，并具有更好的泛化能力，从而提高智能体在复杂任务中学习到更优秀的策略。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710762786,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1478,
            "prompt_tokens": 3766,
            "total_tokens": 5244
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]