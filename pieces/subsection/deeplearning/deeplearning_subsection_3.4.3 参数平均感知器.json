[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "3.4.3 参数平均感知器",
        "content_length": 2439,
        "content": "Title: 3.4.3 参数平均感知器\nContent:\n根据定理 3.1 , 如果训练数据是线性可分的, 那么感知器可以找到一个判别函数来分割不同类的数据. 如果间隔 $\\gamma$ 越大, 收玫越快. 但是感知器并不能保证找到的判别函数是最优的 ( 比如泛化能力高), 这样可能导致过拟合.\n\n感知器学习到的权重向量和训练样本的顺序相关. 在迭代次序上排在后面的错误样本比前面的错误样本, 对最终的权重向量影响更大. 比如有 1000 个训练样本, 在迭代 100 个样本后, 感知器已经学习到一个很好的权重向量. 在接下来的 899 个样本上都预测正确, 也没有更新权重向量. 但是, 在最后第 1000 个样本时预测错误,并更新了权重. 这次更新可能反而使得权重向量变差.\n\n为了提高感知器的鲁棒性和泛化能力, 我们可以将在感知器学习过程中的所有 $K$ 个权重向量保存起来, 并赋予每个权重向量 $\\boldsymbol{w}_{k}$ 一个置信系数 $c_{k}(1 \\leq$\n\n$\\left\\|\\boldsymbol{w}^{*}\\right\\|=1$. 两个向量内积的平方一定小于等于这两个向量的模的乘积。\n\n$\\boldsymbol{w}^{* \\top}\\left(y^{(n)} \\boldsymbol{x}^{(n)}\\right) \\geq \\gamma, \\forall n$\\\\\n$k \\leq K)$. 最终的分类结果通过这 $K$ 个不同权重的感知器投票决定, 这个模型也称为投票感知器 (Voted Perceptron ) [Freund et al., 1999].\n\n令 $\\tau_{k}$ 为第 $k$ 次更新权重 $\\boldsymbol{w}_{k}$ 时的迭代次数 (即训练过的样本数量), $\\tau_{k+1}$ 为下次权重更新时的迭代次数, 则权重 $\\boldsymbol{w}_{k}$ 的置信系数 $c_{k}$ 设置为从 $\\tau_{k}$ 到 $\\tau_{k+1}$ 之间间隔的迭代次数, 即 $c_{k}=\\tau_{k+1}-\\tau_{k}$. 置信系数 $c_{k}$ 越大, 说明权重 $\\boldsymbol{w}_{k}$ 在之后的训练过程中正确分类样本的数量越多, 越值得信赖.\n\n这样, 投票感知器的形式为\n\n\n\\begin{equation*}\n\\hat{y}=\\operatorname{sgn}\\left(\\sum_{k=1}^{K} c_{k} \\operatorname{sgn}\\left(\\boldsymbol{w}_{k}^{\\top} \\boldsymbol{x}\\right)\\right) \\tag{3.72}\n\\end{equation*}\n\n\n其中 $\\operatorname{sgn}(\\cdot)$ 为符号函数.\n\n投票感知器虽然提高了感知器的泛化能力, 但是需要保存 $K$ 个权重向量.在实际操作中会带来额外的开销. 因此, 人们经常会使用一个简化的版本, 通过使用 “参数平均” 的策略来减少投票感知器的参数数量, 也叫作平均感知器 (Averaged Perceptron ) [Collins, 2002]. 平均感知器的形式为\n\n\n\\begin{align*}\n\\hat{y} & =\\operatorname{sgn}\\left(\\frac{1}{T} \\sum_{k=1}^{K} c_{k}\\left(\\boldsymbol{w}_{k}^{\\top} \\boldsymbol{x}\\right)\\right)  \\tag{3.73}\\\\\n& =\\operatorname{sgn}\\left(\\frac{1}{T}\\left(\\sum_{k=1}^{K} c_{k} \\boldsymbol{w}_{k}\\right)^{\\top} \\boldsymbol{x}\\right)  \\tag{3.74}\\\\\n& =\\operatorname{sgn}\\left(\\left(\\frac{1}{T} \\sum_{t=1}^{T} \\boldsymbol{w}_{t}\\right)^{\\top} \\boldsymbol{x}\\right)  \\tag{3.75}\\\\\n& =\\operatorname{sgn}\\left(\\overline{\\boldsymbol{w}}^{\\top} \\boldsymbol{x}\\right), \\tag{3.76}\n\\end{align*}\n\n\n其中 $T$ 为迭代总回合数, $\\overline{\\boldsymbol{w}}$ 为 $T$ 次迭代的平均权重向量. 这个方法非常简单, 只需要在算法3.1中增加一个 $\\overline{\\boldsymbol{w}}$, 并且在每次迭代时都更新 $\\overline{\\boldsymbol{w}}:$\n\n\n\\begin{equation*}\n\\overline{\\boldsymbol{w}} \\leftarrow \\overline{\\boldsymbol{w}}+\\boldsymbol{w}_{t} . \\tag{3.77}\n\\end{equation*}\n\n\n但这个方法需要在处理每一个样本时都要更新 $\\overline{\\boldsymbol{w}}$. 因为 $\\overline{\\boldsymbol{w}}$ 和 $\\boldsymbol{w}_{t}$ 都是稠密向量,所以更新操作比较费时. 为了提高迭代速度, 有很多改进的方法, 让这个更新只需要在错误预测发生时才进行更新.\n\n算法3.2给出了一个改进的平均感知器算法的训练过程 [Daumé III, 2012].投票感知器是一种集成模型, 参见第10.1节."
    },
    {
        "id": "chatcmpl-941CrYWpopJnXe0uWPWTJTJSQWozJ",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"感知器在训练数据线性可分的情况下能否保证找到最优的判别函数？\",\n    \"回答1\": \"感知器在训练数据线性可分的情况下可以找到一个判别函数来分割不同类的数据。然而，感知器并不能保证找到的判别函数是最优的，这可能导致过拟合。\",\n    \"问题2\": \"感知器学习到的权重向量和训练样本的顺序之间是否存在相关性？\",\n    \"回答2\": \"感知器学习到的权重向量和训练样本的顺序是存在相关性的。在迭代次序上排在后面的错误样本对最终的权重向量影响更大。\",\n    \"问题3\": \"为了提高感知器的鲁棒性和泛化能力，采用了什么方法来改进感知器模型？\",\n    \"回答3\": \"为了提高感知器的鲁棒性和泛化能力，采用了投票感知器（Voted Perceptron）模型。该模型通过保存在感知器学习过程中的所有 $K$ 个权重向量，并赋予每个权重向量一个置信系数 $c_{k}$，最终的分类结果通过这 $K$ 个不同权重的感知器投票决定。\",\n    \"问题4\": \"什么是投票感知器（Voted Perceptron）？\",\n    \"回答4\": \"投票感知器是一种集成模型，通过保存在感知器学习过程中的所有 $K$ 个权重向量，并赋予每个权重向量一个置信系数 $c_{k}$，最终的分类结果通过这 $K$ 个不同权重的感知器投票决定。\",\n    \"问题5\": \"平均感知器（Averaged Perceptron）是如何减少投票感知器参数数量的？\",\n    \"回答5\": \"平均感知器通过使用 “参数平均” 的策略来减少投票感知器的参数数量。它采用了简化的形式，通过计算所有权重向量的加权平均值得到一个平均权重向量，从而减少了参数数量。\",\n    \"问题6\": \"平均感知器（Averaged Perceptron）模型中 $\\tau_{k}$ 和 $\\tau_{k+1}$ 分别表示什么？\",\n    \"回答6\": \"$\\tau_{k}$ 表示第 $k$ 次更新权重 $\\boldsymbol{w}_{k}$ 时的迭代次数（即训练过的样本数量），$\\tau_{k+1}$ 表示下次权重更新时的迭代次数。\",\n    \"问题7\": \"平均感知器（Averaged Perceptron）中 $\\boldsymbol{w}^{*}$ 的模长有何特点？\",\n    \"回答7\": \"$\\boldsymbol{w}^{*}$ 的模长为1，即 $\\big\\big\\boldsymbol{w}^{*}\\big\\big=1$。这意味着 $\\boldsymbol{w}^{*}$ 是单位向量。\",\n    \"问题8\": \"平均感知器（Averaged Perceptron）中如何计算最终的分类结果 $\\bar{y}$？\",\n    \"回答8\": \"平均感知器中最终的分类结果 $\\bar{y}$ 通过计算加权平均值得到，公式为 $\\bar{y}=\\text{sgn}\\big(\\frac{1}{T} \\big(\\text{sgn}\\big(\\boldsymbol{w}_{1}^{\\top} \\boldsymbol{x}\\big)+\\text{sgn}\\big(\\boldsymbol{w}_{2}^{\\top} \\boldsymbol{x}\\big)+\\text{...}+\\text{sgn}\\big(\\boldsymbol{w}_{K}^{\\top} \\boldsymbol{x}\\big)\\big)\\big)$。\",\n    \"问题9\": \"改进的平均感知器算法中如何更新 $\\bar{\\boldsymbol{w}}$？\",\n    \"回答9\": \"改进的平均感知器算法中更新 $\\bar{\\boldsymbol{w}}$ 的操作是 $\\bar{\\boldsymbol{w}} \\text{ 更新为 } \\bar{\\boldsymbol{w}}+\\boldsymbol{w}_{t}$。这意味着在处理每一个样本时都要更新 $\\bar{\\boldsymbol{w}}$。\",\n    \"问题10\": \"改进的平均感知器算法如何提高迭代速度？\",\n    \"回答10\": \"为了提高迭代速度，改进的平均感知器算法采用了一些改进方法，使得更新操作只需要在错误预测发生时才进行更新，从而减少了更新操作所需的时间。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710744977,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1264,
            "prompt_tokens": 4102,
            "total_tokens": 5366
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]