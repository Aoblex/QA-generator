[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "5.1.4.2 导数",
        "content_length": 2391,
        "content": "Title: 5.1.4.2 导数\nContent:\n假设 $\\boldsymbol{Y}=\\boldsymbol{W} \\otimes \\boldsymbol{X}$, 其中 $\\boldsymbol{X} \\in \\mathbb{R}^{M \\times N}, \\boldsymbol{W} \\in \\mathbb{R}^{U \\times V}, \\boldsymbol{Y} \\in \\mathbb{R}^{(M-U+1) \\times(N-V+1)}$,函数 $f(\\boldsymbol{Y}) \\in \\mathbb{R}$ 为一个标量函数, 则\n\n\n\\begin{align*}\n\\frac{\\partial f(\\boldsymbol{Y})}{\\partial w_{u v}} & =\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} \\frac{\\partial y_{i j}}{\\partial w_{u v}} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}}  \\tag{5.14}\\\\\n& =\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} x_{i+u-1, j+v-1} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}}  \\tag{5.15}\\\\\n& =\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} x_{u+i-1, v+j-1} . \\tag{5.16}\n\\end{align*}\n\n\n$y_{i j}=\\sum_{u, v} w_{u v} x_{i+u-1, j+v-1}$.\n\n从公式 (5.16) 可以看出, $f(\\boldsymbol{Y})$ 关于 $\\boldsymbol{W}$ 的偏导数为 $\\boldsymbol{X}$ 和 $\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}}$ 的卷积\n\n\n\\begin{equation*}\n\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{W}}=\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}} \\otimes \\boldsymbol{X} . \\tag{5.17}\n\\end{equation*}\n\n\n同理得到,\n\n\n\\begin{equation*}\n\\frac{\\partial f(\\boldsymbol{Y})}{\\partial x_{s t}}=\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} \\frac{\\partial y_{i j}}{\\partial x_{s t}} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} \\tag{5.18}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\\begin{equation*}\n=\\sum_{i=1}^{M-U+1} \\sum_{j=1}^{N-V+1} w_{S-i+1, t-j+1} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial y_{i j}} \\tag{5.19}\n\\end{equation*}\n\n\n其中当 $(s-i+1)<1$, 或 $(s-i+1)>U$, 或 $(t-j+1)<1$, 或 $(t-j+1)>V$时, $w_{s-i+1, t-j+1}=0$. 即相当于对 $\\boldsymbol{W}$ 进行了 $P=(M-U, N-V)$ 的零填充.\n\n从公式 (5.19) 可以看出, $f(\\boldsymbol{Y})$ 关于 $\\boldsymbol{X}$ 的偏导数为 $\\boldsymbol{W}$ 和 $\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}}$ 的宽卷积. 公式 (5.19) 中的卷积是真正的卷积而不是互相关, 为了一致性, 我们用互相关的 “卷积”, 即\n\n\n\\begin{align*}\n\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{X}} & =\\operatorname{rot} 180\\left(\\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}}\\right) \\tilde{\\otimes} \\boldsymbol{W}  \\tag{5.20}\\\\\n& =\\operatorname{rot} 180(\\boldsymbol{W}) \\tilde{\\otimes} \\frac{\\partial f(\\boldsymbol{Y})}{\\partial \\boldsymbol{Y}} \\tag{5.21}\n\\end{align*}\n\n\n其中 $\\operatorname{rot} 180(\\cdot)$ 表示旋转 180 度.\n\n\\section*{5.2 卷积神经网络}\n卷积神经网络一般由卷积层、汇聚层和全连接层构成."
    },
    {
        "id": "chatcmpl-941kWs6eTlLfDw95TZmKLStxruzpP",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"在给定的函数 $f(\\boldsymbol{Y})$ 中，如何计算关于 $\\boldsymbol{W}$ 的偏导数？\",\n    \"回答1\": \"根据公式 (5.16)，可以计算 $f(\\boldsymbol{Y})$ 关于 $\\boldsymbol{W}$ 的偏导数为 $\\boldsymbol{X}$ 和 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{Y}}$ 的卷积，即 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{W}}=\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{Y}} \\bigotimes \\boldsymbol{X}$。\",\n    \"问题2\": \"如何计算给定函数 $f(\\boldsymbol{Y})$ 关于 $\\boldsymbol{X}$ 的偏导数？\",\n    \"回答2\": \"根据公式 (5.19)，可以计算 $f(\\boldsymbol{Y})$ 关于 $\\boldsymbol{X}$ 的偏导数为 $\\boldsymbol{W}$ 和 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{Y}}$ 的宽卷积，即 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{X}}=\\text{rot} 180(\\boldsymbol{W}) \\tilde{\\bigotimes} \\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{Y}}$。\",\n    \"问题3\": \"在卷积神经网络中，卷积层、汇聚层和全连接层各自承担什么作用？\",\n    \"回答3\": \"卷积神经网络一般由卷积层、汇聚层和全连接层构成。卷积层用于提取输入数据的特征，汇聚层用于降低特征图的尺寸并保留重要信息，全连接层用于将提取的特征映射到输出类别。\",\n    \"问题4\": \"在给定的函数 $f(\\boldsymbol{Y})$ 中，如何计算关于 $w_{u v}$ 的偏导数？\",\n    \"回答4\": \"根据公式 (5.15)，可以计算 $f(\\boldsymbol{Y})$ 关于 $w_{u v}$ 的偏导数为 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} w_{u v}}=\\textstyle\\bigsum_{i=1}^{M-U+1} \\bigsum_{j=1}^{N-V+1} x_{i+u-1, j+v-1} \\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} y_{i j}}$。\",\n    \"问题5\": \"在给定的函数 $f(\\boldsymbol{Y})$ 中，如何计算关于 $x_{s t}$ 的偏导数？\",\n    \"回答5\": \"根据公式 (5.19)，可以计算 $f(\\boldsymbol{Y})$ 关于 $x_{s t}$ 的偏导数为 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} x_{s t}}=\\textstyle\\bigsum_{i=1}^{M-U+1} \\bigsum_{j=1}^{N-V+1} w_{S-i+1, t-j+1} \\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} y_{i j}}$。\",\n    \"问题6\": \"给定函数 $f(\\boldsymbol{Y})$ 中的卷积是真正的卷积还是互相关？\",\n    \"回答6\": \"在给定函数中，公式 (5.19) 中的卷积是真正的卷积而不是互相关。为了一致性，使用互相关的 “卷积”，即 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{X}}=\\text{rot} 180(\\boldsymbol{W}) \\tilde{\\bigotimes} \\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d} \\boldsymbol{Y}}$。\",\n    \"问题7\": \"什么是零填充（Zero Padding）？\",\n    \"回答7\": \"零填充是指在矩阵或张量的边缘周围添加零元素，以扩展其尺寸或调整其形状。在给定函数中，当 $(s-i+1)<1$, 或 $(s-i+1)>U$, 或 $(t-j+1)<1$, 或 $(t-j+1)>V$时，$w_{s-i+1, t-j+1}=0$，相当于对 $\\boldsymbol{W}$ 进行了 $(M-U, N-V)$ 的零填充。\",\n    \"问题8\": \"如何计算给定函数 $f(\\boldsymbol{Y})$ 关于 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d}\\boldsymbol{W}}$ 的偏导数？\",\n    \"回答8\": \"从公式 (5.16) 可以看出，$f(\\boldsymbol{Y})$ 关于 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d}\\boldsymbol{W}}$ 的偏导数为 $\\boldsymbol{X}$ 和 $\\frac{\\text{d} f(\\boldsymbol{Y})}{\\text{d}\\boldsymbol {Y}}$ 的卷积，即 $\\frac{\\text {d} f(\\boldsymbol {Y})}{\\text {d}\\bold symbol {W}}=\\frac{\\text {d} f(\\bold symbol {Y})}{\\text {d}\\b old symbol {Y}} \\bigotimes \\b old symbol {X}$。\",\n    \"问题9\": \"如何计算给定函数 $f(\\b old symbol {Y})$ 关于 $\\frac{\\text {d}\\b old symbol {X}}{\\text {d}\\b old symbol {s t}}$ 的偏导数？\",\n    \"回答9\": \"根据公式 (5.19)，可以计算 $f(\\b old symbol {Y})$ 关于 $\\b old symbol {X}$ 的偏导数为 $\\b old symbol {W}$ 和 $\\frac{\\text {d}\\b old symbol {f}(b old symbol {Y})}{\\text {d}\\b old symbol {Y}}$ 的宽卷积，即 $\\frac{\\text {d}\\b old symbol {f}(b old symbol {Y})}{\\text {d}\\b old symbol {X}}=\\t ext {rot }180(b old symbol {W}) \\tilde {\\bigotimes }\\frac {\\t ext {d}\\b old symbol {f}(b old symbol {Y })}{\\t ext { d}\\b old symbol { Y }}$。\",\n    \"问题10\": \"在卷积神经网络中，卷积层的作用是什么？\",\n    \"回答10\": \"卷积层用于提取输入数据的特征，通过对输入数据进行卷积操作来获取不同位置的特征信息，并且通过权重共享来减少模型参数数量，从而提高模型的效率和泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710747064,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1823,
            "prompt_tokens": 3733,
            "total_tokens": 5556
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]