[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "11.2.2.2 高斯混合模型",
        "content_length": 5227,
        "content": "Title: 11.2.2.2 高斯混合模型\nContent:\n本节介绍一个 EM 算法的应用例子: 高斯混合模型.\n\n高斯混合模型 (Gaussian Mixture Model, GMM ) 是由多个高斯分布组成的模型, 其总体密度函数为多个高斯密度函数的加权组合. 如果一个连续随机变量或连续随机向量的分布比较复杂, 那么我们通常可以用高斯混合模型来估计其分布情况.\n\n不失一般性, 这里考虑一维的情况. 假设样本 $x$ 是从 $K$ 个高斯分布中的一个分布生成的, 但是无法观测到具体由哪个分布生成. 我们引入一个隐变量 $z \\in$ $\\{1, \\cdots, K\\}$ 来表示样本 $x$ 来自于哪个高斯分布, $z$ 服从多项分布：\n\n\n\\begin{equation*}\np(z=k ; \\pi)=\\pi_{k}, \\quad 1 \\leq k \\leq K \\tag{11.50}\n\\end{equation*}\n\n\n其中 $\\pi=\\left[\\pi_{1}, \\pi_{2}, \\cdots, \\pi_{K}\\right]$ 为多项分布的参数, 并满足 $\\pi_{k} \\geq 0, \\forall k, \\sum_{k=1}^{K} \\pi_{k}=1$. $\\pi_{k}$ 表示样本 $x$ 由第 $k$ 个高斯分布生成的概率.\n\n给定 $z=k$,条件分布 $p(x \\mid z=k)$ 为高斯分布:\n\n\n\\begin{align*}\np\\left(x \\mid z=k ; \\mu_{k}, \\sigma_{k}\\right) & =\\mathcal{N}\\left(x ; \\mu_{k}, \\sigma_{k}\\right)  \\tag{11.51}\\\\\n& =\\frac{1}{\\sqrt{2 \\pi} \\sigma_{k}} \\exp \\left(-\\frac{\\left(x-\\mu_{k}\\right)^{2}}{2 \\sigma_{k}^{2}}\\right), \\tag{11.52}\n\\end{align*}\n\n\n其中 $\\mu_{k}$ 和 $\\sigma_{k}$ 分别为第 $k$ 个高斯分布的均值和方差.\n\n从高斯混合模型中生成一个样本 $x$ 的过程可以分为两步:\n\n(1) 首先根据多项分布 $p(z ; \\pi)$ 随机选取一个高斯分布.\n\n（2）假设选中第 $k$ 个高斯分布 ( 即 $z=k)$, 再从高斯分布 $\\mathcal{N}\\left(x ; \\mu_{k}, \\sigma_{k}\\right)$ 中选取一个样本 $x$.\n\n图11.12给出了高斯混合模型的图模型表示.\n\n\n\n图 11.12 高斯混合模型\n\n在高斯混合模型中,随机变量 $x$ 的概率密度函数为\n\n\n\\begin{equation*}\np(x)=\\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}\\left(x ; \\mu_{k}, \\sigma_{k}\\right) \\tag{11.53}\n\\end{equation*}\n\n\n参数估计 给定 $N$ 个由高斯混合模型生成的训练样本 $x^{(1)}, x^{(2)}, \\cdots, x^{(N)}$, 希望能学习其中的参数 $\\pi_{k}, \\mu_{k}, \\sigma_{k}, 1 \\leq k \\leq K$. 由于我们无法观测样本 $x^{(n)}$ 是从哪个高斯分布生成的, 因此无法直接用最大似然来进行参数估计.\n\n对每个样本 $x^{(n)}$, 其对数边际分布为\n\n\n\\begin{align*}\n\\log p\\left(x^{(n)}\\right) & =\\log \\sum_{z^{(n)}} p\\left(z^{(n)}\\right) p\\left(x^{(n)} \\mid z^{(n)}\\right)  \\tag{11.54}\\\\\n& =\\log \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}\\left(x^{(n)} ; \\mu_{k}, \\sigma_{k}\\right) . \\tag{11.55}\n\\end{align*}\n\n\n根据 EM 算法, 参数估计可以分为两步进行迭代:\n\n(1) $\\mathrm{E}$ 步 先固定参数 $\\mu, \\sigma$, 计算后验分布 $p\\left(z^{(n)} \\mid x^{(n)}\\right)$, 即\n\n\n\\begin{align*}\n\\gamma_{n k} & \\triangleq p\\left(z^{(n)}=k \\mid x^{(n)}\\right)  \\tag{11.56}\\\\\n& =\\frac{p\\left(z^{(n)}\\right) p\\left(x^{(n)} \\mid z^{(n)}\\right)}{p\\left(x^{(n)}\\right)}  \\tag{11.57}\\\\\n& =\\frac{\\pi_{k} \\mathcal{N}\\left(x^{(n)} ; \\mu_{k}, \\sigma_{k}\\right)}{\\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}\\left(x^{(n)} ; \\mu_{k}, \\sigma_{k}\\right)}, \\tag{11.58}\n\\end{align*}\n\n\n其中 $\\gamma_{n k}$ 定义了样本 $x^{(n)}$ 属于第 $k$ 个高斯分布的后验概率.\n\n(2) $\\mathrm{M}$ 步 令 $q(z=k)=\\gamma_{n k}$, 训练集 $\\mathcal{D}$ 的证据下界为\n\n\n\\begin{align*}\n& \\operatorname{ELBO}(\\gamma, \\mathcal{D} ; \\pi, \\mu, \\sigma)=\\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma_{n k} \\log \\frac{p\\left(x^{(n)}, z^{(n)}=k\\right)}{\\gamma_{n k}}  \\tag{11.59}\\\\\n& =\\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma_{n k}\\left(\\log \\mathcal{N}\\left(x^{(n)} ; \\mu_{k}, \\sigma_{k}\\right)+\\log \\frac{\\pi_{k}}{\\gamma_{n k}}\\right) \\tag{11.60}\n\\end{align*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\\begin{equation*}\n=\\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma_{n k}\\left(\\frac{-\\left(x-\\mu_{k}\\right)^{2}}{2 \\sigma_{k}^{2}}-\\log \\sigma_{k}+\\log \\pi_{k}\\right)+C \\tag{11.61}\n\\end{equation*}\n\n\n其中 $C$ 为和参数无关的常数.\n\n将参数估计问题转为优化问题:\n\n\n\\begin{align*}\n& \\max _{\\pi, \\mu, \\sigma} \\operatorname{ELBO}(\\gamma, \\mathcal{D} ; \\pi, \\mu, \\sigma),  \\tag{11.62}\\\\\n& \\text { s.t. } \\quad \\sum_{k=1}^{K} \\pi_{k}=1 .\n\\end{align*}\n\n\n利用拉格朗日乘数法来求解上面的等式约束优化问题, 分别求拉格朗日函数 $\\operatorname{ELBO}(\\gamma, \\mathcal{D} ; \\pi, \\mu, \\sigma)+\\lambda\\left(\\sum_{k=1}^{K} \\pi_{k}-1\\right)$ 关于 $\\pi_{k}, \\mu_{k}, \\sigma_{k}$ 的偏导数, 并令其等于 0 .可得\n\n\n\\begin{align*}\n\\pi_{k} & =\\frac{N_{k}}{N}  \\tag{11.63}\\\\\n\\mu_{k} & =\\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma_{n k} x^{(n)},  \\tag{11.64}\\\\\n\\sigma_{k}^{2} & =\\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma_{n k}\\left(x^{(n)}-\\mu_{k}\\right)^{2}, \\tag{11.65}\n\\end{align*}\n\n\n其中\n\n\n\\begin{equation*}\nN_{k}=\\sum_{n=1}^{N} \\gamma_{n k} \\tag{11.66}\n\\end{equation*}\n\n\n高斯混合模型的参数学习过程如算法11.1所示.\n\n\\section*{算法 11.1 高斯混合模型的参数学习过程}\n输入: 训练样本: $x^{(1)}, x^{(2)}, \\cdots, x^{(N)}$;\n\n1 随机初始化参数: $\\pi_{k}, \\mu_{k}, \\sigma_{k}, 1 \\leq k \\leq K$;\n\n\\section*{2 repeat}\n3\n\n$$\n/ / \\mathrm{E} \\text { 步 }\n$$\n\n固定参数, 根据公式 (11.58) 计算 $\\gamma_{n k}, 1 \\leq k \\leq K, 1 \\leq n \\leq N$;\n\n// M步\n\n4 固定 $\\gamma_{n k}$, 根据公式 (11.63)、公式(11.64) 和公式 (11.65), 计算 $\\pi_{k}, \\mu_{k}, \\sigma_{k}$, $1 \\leq k \\leq K$;\n\n5 until 对数边际分布 $\\sum_{n=1}^{N} \\log p\\left(x^{(n)}\\right)$ 收敛;输出: $\\pi_{k}, \\mu_{k}, \\sigma_{k}, 1 \\leq k \\leq K$\n\n图11.13给出一个高斯混合模型训练过程的简单示例. 给定一组数据, 我们用两个高斯分布来估计这组数据的分布情况.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\n(a) 初始化\n\n\n\n(c) 第 4 次迭代\n\n\n\n(e) 第 12 次迭代\n\n\n\n(b) 第 1 次迭代\n\n\n\n(d) 第 8 次迭代\n\n\n\n(f) 第 16 次迭代\n\n图 11.13 高斯混合模型训练过程示例\n\n\\section*{11.3 推断}\n在图模型中, 推断 (Inference ) 是指在观测到部分变量 $\\boldsymbol{e}=\\left\\{e_{1}, e_{2}, \\cdots, e_{M}\\right\\}$时, 计算其他变量的某个子集 $\\boldsymbol{q}=\\left\\{q_{1}, q_{2}, \\cdots, q_{N}\\right\\}$ 的条件概率 $p(\\boldsymbol{q} \\mid \\boldsymbol{e})$.\n\n假设一个图模型中, 除了变量 $\\boldsymbol{e} 、 \\boldsymbol{q}$ 外, 其余变量表示为 $\\boldsymbol{z}$. 根据贝叶斯公式有\n\n\n\\begin{align*}\np(\\boldsymbol{q} \\mid \\boldsymbol{e}) & =\\frac{p(\\boldsymbol{q}, \\boldsymbol{e})}{p(\\boldsymbol{e})}  \\tag{11.67}\\\\\n& =\\frac{\\sum_{\\boldsymbol{z}} p(\\boldsymbol{q}, \\boldsymbol{e}, \\boldsymbol{z})}{\\sum_{\\boldsymbol{q}, z} p(\\boldsymbol{q}, \\boldsymbol{e}, \\boldsymbol{z})} \\tag{11.68}\n\\end{align*}\n\n\n因此, 图模型的推断问题的关键为求任意一个变量子集的边际概率分布问题.\n\n在图模型中,常用的推断算法可以分为精确推断算法和近似推断算法两类."
    },
    {
        "id": "chatcmpl-9452Bw9tzgVp3gOm8qTSSU5WV7QYS",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"高斯混合模型 (GMM )的总体密度函数是怎样组成的？\",\n    \"回答1\": \"高斯混合模型 (GMM )的总体密度函数是由多个高斯密度函数的加权组合构成的，即多个高斯分布的加权组合。这种组合形式能够更好地逼近复杂连续随机变量或连续随机向量的分布情况。\",\n    \"问题2\": \"在一维情况下，高斯混合模型假设样本 $x$ 是如何生成的？\",\n    \"回答2\": \"在一维情况下，高斯混合模型假设样本 $x$ 是从 $K$ 个高斯分布中的一个分布生成的，但是无法观测到具体由哪个分布生成。为了表示样本 $x$ 来自于哪个高斯分布，引入一个隐变量 $z \\text{ 且 } z \\text{ 服从多项分布}$。\",\n    \"问题3\": \"高斯混合模型中的参数 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}$ 和 $\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm$ 的物理意义是什么？\",\n    \"回答3\": \"参数 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}}}}}}}}}}}}}}}}}$ 和 $\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm$ 分别表示第 $k$ 个高斯分布的均值和方差。它们是用来描述每个高斯分布的位置和形状特征的重要参数。\",\n    \"问题4\": \"高斯混合模型中的参数估计过程是如何进行的？\",\n    \"回答4\": \"给定 $N$ 个由高斯混合模型生成的训练样本 $x^{(1)}, x^{(2)}, \\text{...}, x^{(N)}$，希望能学习其中的参数 $\\boldsymbol{π}_{k}, \\bmu_{k}, \\bsigma_{k}, 1 \\text{ 为 } k \\text{ 为 } K$。由于无法直接观测样本 $x^{(n)}$ 是从哪个高斯分布生成的，因此参数估计采用了基于 EM 算法的迭代优化过程。\",\n    \"问题5\": \"在高斯混合模型中，$\\text{E}$ 步和 $\\text{M}$ 步分别指什么？\",\n    \"回答5\": \"$E$ 步先固定参数 $\\bmu, \\bsigma$，计算后验分布 $p(z^{(n)} \\text{ 为 } k \\text{ 为 } K \\text{ 为 } n)$；$\\text{M}$ 步固定后验概率 $\\bgamma_{n k}$，根据公式 (11.63)、公式(11.64) 和公式 (11.65)，计算参数 $\\bpi_{k}, \\bmu_{k}, \\bsigma_{k}$。\",\n    \"问题6\": \"在图模型中，推断 (Inference ) 的关键问题是什么？\",\n    \"回答6\": \"在图模型中，推断 (Inference ) 的关键问题是求任意一个变量子集的边际概率分布问题。即在观测到部分变量时，计算其他变量的某个子集的条件概率。\",\n    \"问题7\": \"推断算法通常可以分为哪两类？\",\n    \"回答7\": \"推断算法通常可以分为精确推断算法和近似推断算法两类。精确推断算法旨在准确计算边际概率分布，而近似推断算法则通过近似方法来解决复杂的推断问题。\",\n    \"问题8\": \"在高斯混合模型中，参数学习过程如何转化为优化问题？\",\n    \"回答8\": \"将参数估计问题转化为优化问题时，通过求解证据下界 $\\text{ELBO}(\\bgamma, \\bD ; \\bpi, \\bmu, \\bsigma)$ 的最大化来得到参数 $\\bpi, \\bmu, \\bsigma$ 的估计值。同时需要满足约束条件 $\\text{s.t.} \\text{ 为 } \\bgamma_{n k} \\text{ 为 } k \\text{ 为 } K=1$。\",\n    \"问题9\": \"在高斯混合模型中，参数学习过程如何进行迭代优化？\",\n    \"回答9\": \"在高斯混合模型中，参数学习过程采用 EM 算法进行迭代优化。具体包括 $\\text{E}$ 步和 $\\text{M}$ 步两个迭代过程，通过交替更新后验概率和参数值来逐步优化模型参数。\",\n    \"问题10\": \"图11.13展示了什么内容？\",\n    \"回答10\": \"图11.13展示了一个简单示例，给定一组数据，用两个高斯分布来估计这组数据的分布情况。图中展示了不同迭代次数下的训练过程和参数更新情况。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710759691,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1655,
            "prompt_tokens": 5659,
            "total_tokens": 7314
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]