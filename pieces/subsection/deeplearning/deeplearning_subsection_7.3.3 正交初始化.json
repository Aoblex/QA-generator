[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.3.3 正交初始化",
        "content_length": 5083,
        "content": "Title: 7.3.3 正交初始化\nContent:\n上面介绍的两种基于方差的初始化方法都是对权重矩阵中的每个参数进行独立采样. 由于采样的随机性, 采样出来的权重矩阵依然可能存在梯度消失或梯度爆炸问题.\n\n假设一个 $L$ 层的等宽线性网络 (激活函数为恒等函数) 为\n\n\n\\begin{equation*}\n\\boldsymbol{y}=\\boldsymbol{W}^{(L)} \\boldsymbol{W}^{(L-1)} \\cdots \\boldsymbol{W}^{(1)} \\boldsymbol{x}, \\tag{7.46}\n\\end{equation*}\n\n\n其中 $\\boldsymbol{W}^{(l)} \\in \\mathbb{R}^{M \\times M}(1 \\leq l \\leq L)$ 为神经网络的第 $l$ 层权重矩阵. 在反向传播中,误差项 $\\delta$ 的反向传播公式为 $\\delta^{(l-1)}=\\left(\\boldsymbol{W}^{(l)}\\right)^{\\top} \\delta^{(l)}$. 为了避免梯度消失或梯度爆炸问题, 我们希望误差项在反向传播中具有范数保持性 (Norm-Preserving), 即 $\\left\\|\\delta^{(l-1)}\\right\\|^{2}=\\left\\|\\delta^{(l)}\\right\\|^{2}=\\left\\|\\left(\\boldsymbol{W}^{(l)}\\right)^{\\top} \\delta^{(l)}\\right\\|^{2}$. 如果我们以均值为 0 、方差为 $\\frac{1}{M}$ 的高斯分布来随机生成权重矩阵 $\\boldsymbol{W}^{(l)}$ 中每个元素的初始值, 那么当 $M \\rightarrow \\infty$ 时, 范数保持\\\\\nHe 初始化也称为 Kaiming 初始化.这里假设每一层的偏置初始化为 0 .\n\n误差项参见公式(4.63).\\\\\n性成立. 但是当 $M$ 不足够大时, 这种对每个参数进行独立采样的初始化方式难以保证范数保持性.\n\n因此, 一种更加直接的方式是将 $\\boldsymbol{W}^{(l)}$ 初始化为正交矩阵, 即 $\\boldsymbol{W}^{(l)}\\left(\\boldsymbol{W}^{(l)}\\right)^{\\top}=$ $I$, 这种方法称为正交初始化 (Orthogonal Initialization) [Saxe et al., 2014]. 正交初始化的具体实现过程可以分为两步: 1$)$ 用均值为 0 、方差为 1 的高斯分布初始化一个矩阵; 2$)$ 将这个矩阵用奇异值分解得到两个正交矩阵, 并使用其中之一作为权重矩阵.\n\n根据正交矩阵的性质, 这个线性网络在信息的前向传播过程和误差的反向传播过程中都具有范数保持性, 从而可以避免在训练开始时就出现梯度消失或梯度爆炸现象。\n\n当在非线性神经网络中应用正交初始化时, 通常需要将正交矩阵乘以一个缩放系数 $\\rho$. 比如当激活函数为 ReLU 时, 激活函数在 0 附近的平均梯度可以近似为 0.5 . 为了保持范数不变, 缩放系数 $\\rho$ 可以设置为 $\\sqrt{2}$.\n\n\\section*{7.4 数据预处理}\n一般而言, 样本特征由于来源以及度量单位不同, 它们的尺度 (Scale) (即取值范围 ) 往往差异很大. 以描述长度的特征为例, 当用 “米”作单位时令其值为 $x$, 那么当用 “厘米” 作单位时其值为 $100 x$. 不同机器学习模型对数据特征尺度的敏感程度不一样. 如果一个机器学习算法在缩放全部或部分特征后不影响它的学习和预测, 我们就称该算法具有尺度不变性 (Scale Invariance). 比如线性分类器是尺度不变的, 而最近邻分类器就是尺度敏感的. 当我们计算不同样本之间的欧氏距离时, 尺度大的特征会起到主导作用. 因此, 对于尺度敏感的模型, 必须先对样本进行预处理, 将各个维度的特征转换到相同的取值区间, 并且消除不同特征之间的相关性,才能获得比较理想的结果.\n\n从理论上, 神经网络应该具有尺度不变性, 可以通过参数的调整来适应不同特征的尺度. 但尺度不同的输入特征会增加训练难度. 假设一个只有一层的网络 $y=\\tanh \\left(w_{1} x_{1}+w_{2} x_{2}+b\\right)$, 其中 $x_{1} \\in[0,10], x_{2} \\in[0,1]$. 之前我们提到 $\\tanh$ 函数的导数在区间 $[-2,2]$ 上是敏感的, 其余的导数接近于 0 . 因此, 如果 $w_{1} x_{1}+w_{2} x_{2}+b$ 过大或过小, 都会导致梯度过小, 难以训练. 为了提高训练效率,我们需要使 $w_{1} x_{1}+w_{2} x_{2}+b$ 在 $[-2,2]$ 区间,因此需要将 $w_{1}$ 设得小一点, 比如在 $[-0.1,0.1]$ 之间. 可以想象, 如果数据维数很多时, 我们很难这样精心去选择每一个参数. 因此, 如果每一个特征的尺度相似, 比如 $[0,1]$ 或者 $[-1,1]$, 我们就不太需要区别对待每一个参数,从而减少人工干预.\n\n除了参数初始化比较困难之外, 不同输入特征的尺度差异比较大时, 梯 \\href{https://nndl.github.io/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E5%8F%82%E8%A7%81%E7%AC%ACA.2.6.2%E8%8A%82%E3%80%82}{https://nndl.github.io/奇异值分解参见第A.2.6.2节。}\n\n正交初始化通常用在循环神经网络中循环边上的权重矩阵上.\\\\\n度下降法的效率也会受到影响. 图7.9给出了数据归一化对梯度的影响. 其中,图7.9a为未归一化数据的等高线图. 尺度不同会造成在大多数位置上的梯度方向并不是最优的搜索方向. 当使用梯度下降法寻求最优解时, 会导致需要很多次迭代才能收玫. 如果我们把数据归一化为相同尺度, 如图7.9b所示, 大部分位置的梯度方向近似于最优搜索方向. 这样, 在梯度下降求解时, 每一步梯度的方向都基本指向最小值,训练效率会大大提高.\n\n\n\n(a) 未归一化数据的梯度\n\n\n\n(b) 归一化数据的梯度\n\n图 7.9 数据归一化对梯度的影响\n\n归一化 (Normalization ) 方法泛指把数据特征转换为相同尺度的方法, 比如把数据特征映射到 $[0,1]$ 或 $[-1,1]$ 区间内, 或者映射为服从均值为 0 、方差为 1 的标准正态分布. 归一化的方法有很多种, 比如之前我们介绍的 Sigmoid型函数等都可以将不同尺度的特征挤压到一个比较受限的区间. 这里, 我们介绍几种在神经网络中经常使用的归一化方法.\n\n最小最大值归一化 最小最大值归一化 ( Min-Max Normalization) 是一种非常简单的归一化方法, 通过缩放将每一个特征的取值范围归一到 $[0,1]$ 或 $[-1,1]$之间. 假设有 $N$ 个样本 $\\left\\{\\boldsymbol{x}^{(n)}\\right\\}_{n=1}^{N}$, 对于每一维特征 $x$, 归一化后的特征为\n\n\n\\begin{equation*}\n\\hat{x}^{(n)}=\\frac{x^{(n)}-\\min _{n}\\left(x^{(n)}\\right)}{\\max _{n}\\left(x^{(n)}\\right)-\\min _{n}\\left(x^{(n)}\\right)} \\tag{7.47}\n\\end{equation*}\n\n\n其中 $\\min (x)$ 和 $\\max (x)$ 分别是特征 $x$ 在所有样本上的最小值和最大值.\n\n标准化 标准化 (Standardization ) 也叫Z值归一化 (Z-Score Normalization )，来源于统计上的标准分数. 将每一个维特征都调整为均值为 0 , 方差为 1 . 假设有 $N$ 个样本 $\\left\\{\\boldsymbol{x}^{(n)}\\right\\}_{n=1}^{N}$, 对于每一维特征 $x$, 我们先计算它的均值和方差：\n\n\n\\begin{align*}\n\\mu & =\\frac{1}{N} \\sum_{n=1}^{N} x^{(n)}  \\tag{7.48}\\\\\n\\sigma^{2} & =\\frac{1}{N} \\sum_{n=1}^{N}\\left(x^{(n)}-\\mu\\right)^{2} \\tag{7.49}\n\\end{align*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n然后, 将特征 $x^{(n)}$ 减去均值, 并除以标准差, 得到新的特征值 $\\hat{x}^{(n)}$ :\n\n\n\\begin{equation*}\n\\hat{x}^{(n)}=\\frac{x^{(n)}-\\mu}{\\sigma} \\tag{7.50}\n\\end{equation*}\n\n\n其中标准差 $\\sigma$ 不能为 0 . 如果标准差为 0 , 说明这一维特征没有任何区分性, 可以直接删掉.\n\n白化 白化 (Whitening) 是一种重要的预处理方法, 用来降低输入数据特征之间的冗余性. 输入数据经过白化处理后, 特征之间相关性较低, 并且所有特征具有相同的方差. 白化的一个主要实现方式是使用主成分分析 (Principal Component Analysis , PCA ) 方法去除掉各个成分之间的相关性.\n\n参见第9.1.1节。\n\n图7.10给出了标准归一化和 PCA 白化的比较.\\\\\n\n\n图 7.10 标准归一化和 PCA 白化\n\n\\section*{7.5 逐层归一化}\n逐层归一化 ( Layer-wise Normalization ) 是将传统机器学习中的数据归一化方法应用到深度神经网络中, 对神经网络中隐藏层的输入进行归一化, 从而使得网络更容易训练.\n\n逐层归一化可以有效提高训练效率的原因有以下几个方面:\n\n（1）更好的尺度不变性: 在深度神经网络中,一个神经层的输入是之前神经层的输出. 给定一个神经层 $l$, 它之前的神经层 $(1, \\cdots, l-1)$ 的参数变化会导致其输入的分布发生较大的改变. 当使用随机梯度下降来训练网络时, 每次参数更新都会导致该神经层的输入分布发生改变. 越高的层, 其输入分布会改变得越明显. 就像一栋高楼, 低楼层发生一个较小的偏移, 可能会导致高楼层较大的偏移. 从机器学习角度来看, 如果一个神经层的输入分布发生了改变, 那么其参数需要重新学习, 这种现象叫作内部协变量偏移 (Internal Covariate Shift . 为了缓解这个问题, 我们可以对每一个神经层的输入进行归一化操作, 使其分布保持协变量偏移参见第10.4.2节。\n\n这里的逐层归一化方法是指可以应用在深度神经网络中的任何一个中间层. 实际上并不需要对所有层进行归一化。稳定.\\\\\n把每个神经层的输入分布都归一化为标准正态分布, 可以使得每个神经层对其输入具有更好的尺度不变性. 不论低层的参数如何变化, 高层的输入保持相对稳定. 另外, 尺度不变性可以使得我们更加高效地进行参数初始化以及超参选择。\n\n(2) 更平滑的优化地形:逐层归一化一方面可以使得大部分神经层的输入处于不饱和区域, 从而让梯度变大, 避免梯度消失问题; 另一方面还可以使得神经网络的优化地形 (Optimization Landscape) 更加平滑, 以及使梯度变得更加稳定, 从而允许我们使用更大的学习率, 并提高收玫速度 [Bjorck et al., 2018; Santurkar et al., 2018].\n\n下面介绍几种比较常用的逐层归一化方法: 批量归一化、层归一化、权重归一化和局部响应归一化."
    },
    {
        "id": "chatcmpl-943oTSnptp8FHVkFtlHTmVchiwCbP",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"正交初始化与基于方差的初始化方法有何不同之处？\",\n    \"回答1\": \"正交初始化是一种更加直接的初始化方式，它将权重矩阵 $\\boldsymbol{W}^{(l)}$ 初始化为正交矩阵，即 $\\boldsymbol{W}^{(l)}\\big(\\boldsymbol{W}^{(l)}\\big)^{\\top}=I$。而基于方差的初始化方法是对权重矩阵中的每个参数进行独立采样，难以保证范数保持性。因此，正交初始化相对于基于方差的初始化方法更能避免梯度消失或梯度爆炸问题。\",\n    \"问题2\": \"在神经网络中，为什么需要对样本特征进行数据预处理？\",\n    \"回答2\": \"样本特征由于来源以及度量单位不同，它们的尺度往往差异很大。不同机器学习模型对数据特征尺度的敏感程度不一样。如果一个机器学习算法在缩放全部或部分特征后不影响它的学习和预测，我们就称该算法具有尺度不变性。因此，对样本进行数据预处理可以使得模型更具尺度不变性，获得比较理想的结果。\",\n    \"问题3\": \"神经网络应该具有尺度不变性，但为什么尺度不同的输入特征会增加训练难度？\",\n    \"回答3\": \"尺度不同的输入特征会增加训练难度的原因在于，不同尺度的输入特征会导致梯度过小或过大，从而难以训练。在神经网络中，如果每一个特征的尺度相似，就不太需要区别对待每一个参数，减少人工干预，提高训练效率。\",\n    \"问题4\": \"归一化方法中的最小最大值归一化和标准化分别是如何实现的？\",\n    \"回答4\": \"最小最大值归一化通过缩放将每一个特征的取值范围归一到 $[0,1]$ 或 $[-1,1]$之间。标准化则是将每一个维特征都调整为均值为 0 , 方差为 1。其中标准差不能为 0。这两种方法都是常用的归一化方法，在神经网络中经常使用。\",\n    \"问题5\": \"白化是如何降低输入数据特征之间的冗余性的？\",\n    \"回答5\": \"白化是通过主成分分析 (PCA) 方法去除掉各个成分之间的相关性。输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差。这种处理可以降低输入数据特征之间的冗余性。\",\n    \"问题6\": \"逐层归一化在深度神经网络中起到了什么作用？\",\n    \"回答6\": \"逐层归一化可以使得神经网络更容易训练。它可以提供更好的尺度不变性、更平滑的优化地形，并且使得大部分神经层的输入处于不饱和区域，从而避免梯度消失问题。逐层归一化方法包括批量归一化、层归一化、权重归一化和局部响应归一化。\",\n    \"问题7\": \"为什么逐层归一化可以提供更好的尺度不变性？\",\n    \"回答7\": \"逐层归一化可以使得每个神经层对其输入具有更好的尺度不变性。不论低层的参数如何变化，高层的输入保持相对稳定。这种处理可以使得神经网络更具尺度不变性，并且允许使用更大的学习率，提高收敛速度。\",\n    \"问题8\": \"在深度神经网络中，为什么需要对每一个神经层的输入进行归一化操作？\",\n    \"回答8\": \"在深度神经网络中，一个神经层的输入是之前神经层的输出。给定一个神经层 $l$，它之前的神经层 $(1, \\text{...}, l-1)$ 的参数变化会导致其输入的分布发生较大的改变。因此，对每一个神经层的输入进行归一化操作可以使得大部分神经层的输入处于不饱和区域，从而避免梯度消失问题，并且使得优化地形更加平滑。\",\n    \"问题9\": \"如何实现最小最大值归一化和标准化？\",\n    \"回答9\": \"最小最大值归一化通过缩放将每一个特征的取值范围归一到 $[0,1]$ 或 $[-1,1]$之间。标准化则是将每一个维特征都调整为均值为 0 , 方差为 1。这两种方法都是常用的归一化方法，在神经网络中经常使用。\",\n    \"问题10\": \"白化方法如何降低输入数据特征之间的冗余性？\",\n    \"回答10\": \"白化是通过主成分分析 (PCA) 方法去除掉各个成分之间的相关性。输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差。这种处理可以降低输入数据特征之间的冗余性。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710754997,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1614,
            "prompt_tokens": 6623,
            "total_tokens": 8237
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]