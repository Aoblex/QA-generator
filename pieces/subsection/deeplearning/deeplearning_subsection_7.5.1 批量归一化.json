[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.5.1 批量归一化",
        "content_length": 3335,
        "content": "Title: 7.5.1 批量归一化\nContent:\n批量归一化 ( Batch Normalization, BN ) 方法 [Ioffe et al., 2015] 是一种有效的逐层归一化方法,可以对神经网络中任意的中间层进行归一化操作.\n\n对于一个深度神经网络, 令第 $l$ 层的净输入为 $\\boldsymbol{z}^{(l)}$, 神经元的输出为 $\\boldsymbol{a}^{(l)}$, 即\n\n\n\\begin{equation*}\n\\boldsymbol{a}^{(l)}=f\\left(\\boldsymbol{z}^{(l)}\\right)=f\\left(\\boldsymbol{W} \\boldsymbol{a}^{(l-1)}+\\boldsymbol{b}\\right), \\tag{7.51}\n\\end{equation*}\n\n\n其中 $f(\\cdot)$ 是激活函数, $\\boldsymbol{W}$ 和 $\\boldsymbol{b}$ 是可学习的参数.\n\n为了提高优化效率, 就要使得净输入 $\\boldsymbol{z}^{(l)}$ 的分布一致, 比如都归一化到标准正态分布. 虽然归一化操作也可以应用在输入 $\\boldsymbol{a}^{(l-1)}$ 上, 但归一化 $\\boldsymbol{z}^{(l)}$ 更加有利于优化. 因此, 在实践中归一化操作一般应用在仿射变换 (Affine Transformation ) $\\boldsymbol{W} \\boldsymbol{a}^{(l-1)}+\\boldsymbol{b}$ 之后，激活函数之前.\n\n利用第7.4节中介绍的数据预处理方法对 $\\boldsymbol{z}^{(l)}$ 进行归一化, 相当于每一层都进行一次数据预处理, 从而加速收玫速度. 但是逐层归一化需要在中间层进行操作, 要求效率比较高, 因此复杂度比较高的白化方法就不太合适. 为了提高归一化效率,一般使用标准化将净输入 $\\boldsymbol{z}^{(l)}$ 的每一维都归一到标准正态分布.\n\n\n\\begin{equation*}\n\\hat{\\boldsymbol{z}}^{(l)}=\\frac{\\boldsymbol{z}^{(l)}-\\mathbb{E}\\left[\\boldsymbol{z}^{(l)}\\right]}{\\sqrt{\\operatorname{var}\\left(\\boldsymbol{z}^{(l)}\\right)+\\epsilon}}, \\tag{7.52}\n\\end{equation*}\n\n\n其中 $\\mathbb{E}\\left[\\boldsymbol{z}^{(l)}\\right]$ 和 $\\operatorname{var}\\left(\\boldsymbol{z}^{(l)}\\right)$ 是指当前参数下, $\\boldsymbol{z}^{(l)}$ 的每一维在整个训练集上的期望和方差. 因为目前主要的优化算法是基于小批量的随机梯度下降法, 所以准确地计算 $\\boldsymbol{z}^{(l)}$ 的期望和方差是不可行的. 因此, $\\boldsymbol{z}^{(l)}$ 的期望和方差通常用当前小批量样本集的均值和方差近似估计.\n\n$\\epsilon$ 是为了保持数值稳定性而设置的非常小的常数.批量归一化的提出动机是为了解决内部协方差偏移问题，但后来的研究者发现其主要优点是归一化会导致更平滑的优化地形 [Santurkar et al., 2018].\n\n参见习题7-6.\\\\\n给定一个包含 $K$ 个样本的小批量样本集合, 第 $l$ 层神经元的净输入 $z^{(1, l)}$, $\\cdots, z^{(K, l)}$ 的均值和方差为\n\n\n\\begin{align*}\n\\mu_{\\mathcal{B}} & =\\frac{1}{K} \\sum_{k=1}^{K} z^{(k, l)},  \\tag{7.53}\\\\\n\\sigma_{\\mathcal{B}}^{2} & =\\frac{1}{K} \\sum_{k=1}^{K}\\left(z^{(k, l)}-\\mu_{\\mathcal{B}}\\right) \\odot\\left(z^{(k, l)}-\\mu_{\\mathcal{B}}\\right) . \\tag{7.54}\n\\end{align*}\n\n\n对净输入 $\\boldsymbol{z}^{(l)}$ 的标准归一化会使得其取值集中到 0 附近, 如果使用 Sigmoid 型激活函数时, 这个取值区间刚好是接近线性变换的区间, 减弱了神经网络的非线性性质. 因此, 为了使得归一化不对网络的表示能力造成负面影响, 可以通过一个附加的缩放和平移变换改变取值区间.\n\n\n\\begin{align*}\n\\hat{z}^{(l)} & =\\frac{\\boldsymbol{z}^{(l)}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon} \\odot \\gamma+\\beta}  \\tag{7.55}\\\\\n& \\triangleq \\mathrm{BN}_{\\gamma, \\beta}\\left(\\boldsymbol{z}^{(l)}\\right) \\tag{7.56}\n\\end{align*}\n\n\n其中 $\\gamma$ 和 $\\beta$ 分别代表缩放和平移的参数向量. 从最保守的角度考虑, 可以通过标准归一化的逆变换来使得归一化后的变量可以被还原为原来的值. 当 $\\gamma=\\sqrt{\\sigma_{\\mathcal{B}}^{2}}$, $\\beta=\\mu_{\\mathcal{B}}$ 时,$\\hat{z}^{(l)}=\\boldsymbol{z}^{(l)}$.\n\n批量归一化操作可以看作一个特殊的神经层, 加在每一层非线性激活函数之前,即\n\n\n\\begin{equation*}\n\\boldsymbol{a}^{(l)}=f\\left(\\mathrm{BN}_{\\gamma, \\beta}\\left(\\boldsymbol{z}^{(l)}\\right)\\right)=f\\left(\\mathrm{BN}_{\\gamma, \\beta}\\left(\\boldsymbol{W} \\boldsymbol{a}^{(l-1)}\\right)\\right), \\tag{7.57}\n\\end{equation*}\n\n\n其中因为批量归一化本身具有平移变换, 所以仿射变换 $\\boldsymbol{W} \\boldsymbol{a}^{(l-1)}$ 不再需要偏置参数.\n\n这里要注意的是, 每次小批量样本的 $\\mu_{\\mathcal{B}}$ 和方差 $\\sigma_{\\mathcal{B}}^{2}$ 是净输入 $\\boldsymbol{z}^{(l)}$ 的函数, 而不是常量. 因此在计算参数梯度时需要考虑 $\\mu_{\\mathcal{B}}$ 和 $\\sigma_{\\mathcal{B}}^{2}$ 的影响. 当训练完成时, 用整个数据集上的均值 $\\mu$ 和方差 $\\sigma$ 来分别代替每次小批量样本的 $\\mu_{\\mathcal{B}}$ 和方差 $\\sigma_{\\mathcal{B}}^{2}$. 在实践中, $\\mu_{\\mathcal{B}}$ 和 $\\sigma_{\\mathcal{B}}^{2}$ 也可以用移动平均来计算.\n\n值得一提的是, 逐层归一化不但可以提高优化效率, 还可以作为一种隐形的正则化方法. 在训练时, 神经网络对一个样本的预测不仅和该样本自身相关, 也和同一批次中的其他样本相关. 由于在选取批次时具有随机性, 因此使得神经网络不会“过拟合”到某个特定样本, 从而提高网络的泛化能力 [Luo et al., 2018]."
    },
    {
        "id": "chatcmpl-943omxWWc3C2beebt5GMkRfyNbx2s",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"批量归一化与数据预处理方法有何区别？\",\n    \"回答1\": \"批量归一化是一种对神经网络中任意中间层进行归一化操作的方法，旨在提高优化效率。它通过标准化将净输入 $\\boldsymbol{z}^{(l)}$ 的每一维都归一到标准正态分布。而数据预处理方法则是对整个数据集进行预处理，用于加速收敛速度。虽然两者都涉及到数据的归一化，但批量归一化是应用在神经网络的中间层，而数据预处理方法是对整个数据集进行操作。\",\n    \"问题2\": \"为什么批量归一化操作一般应用在仿射变换之后，激活函数之前？\",\n    \"回答2\": \"批量归一化操作一般应用在仿射变换之后，激活函数之前，因为这样更有利于优化。通过对净输入 $\\boldsymbol{z}^{(l)}$ 进行归一化，可以使其分布更加一致，有利于提高优化效率。而且在实践中发现，这种顺序的应用可以避免对神经网络的非线性性质造成负面影响。\",\n    \"问题3\": \"批量归一化的提出动机是什么？\",\n    \"回答3\": \"批量归一化的提出动机是为了解决内部协方差偏移问题。通过标准化将净输入 $\\boldsymbol{z}^{(l)}$ 的每一维都归一到标准正态分布，可以使得其取值集中到 0 附近，从而提高优化效率。后来的研究者还发现，批量归一化会导致更平滑的优化地形，进一步证明了其有效性。\",\n    \"问题4\": \"批量归一化如何影响神经网络的表示能力？\",\n    \"回答4\": \"批量归一化不对网络的表示能力造成负面影响，因为它通过一个附加的缩放和平移变换改变取值区间，避免了对神经网络的非线性性质造成负面影响。此外，在训练时，神经网络对一个样本的预测不仅和该样本自身相关，也和同一批次中的其他样本相关，这使得神经网络不会“过拟合”到某个特定样本，从而提高网络的泛化能力。\",\n    \"问题5\": \"批量归一化操作可以看作神经网络中的哪种特殊层？\",\n    \"回答5\": \"批量归一化操作可以看作一个特殊的神经层，加在每一层非线性激活函数之前。它通过缩放和平移变换改变取值区间，并且具有平移变换功能，因此仿射变换不再需要偏置参数。\",\n    \"问题6\": \"为什么在计算参数梯度时需要考虑批量样本的均值和方差？\",\n    \"回答6\": \"在计算参数梯度时需要考虑批量样本的均值和方差，因为每次小批量样本的均值和方差是净输入 $\\boldsymbol{z}^{(l)}$ 的函数，而不是常量。因此需要考虑它们对参数梯度的影响，以保证模型训练的准确性和稳定性。\",\n    \"问题7\": \"批量归一化如何保持数值稳定性？\",\n    \"回答7\": \"为了保持数值稳定性，批量归一化引入了一个非常小的常数 $\\boldsymbol{\\text{epsilon}}$。在标准归一化公式中，分母中加入了 $\\boldsymbol{\\text{epsilon}}$，以避免出现除零错误或数值不稳定情况。\",\n    \"问题8\": \"如何使得归一化后的变量可以被还原为原来的值？\",\n    \"回答8\": \"为了使得归一化不对网络的表示能力造成负面影响，并且可以被还原为原来的值，可以通过一个附加的缩放和平移变换改变取值区间。当缩放参数 $\\boldsymbol{\\text{gamma}}$ 等于标准差 $\\boldsymbol{\\text{sigma}}_{\\boldsymbol{\\text{B}}}^2$ 时，平移参数 $\\boldsymbol{\\text{beta}}$ 等于均值 $\\boldsymbol{\\text{mu}}_{\\boldsymbol{\\text{B}}}$ 时，$\\boldsymbol{\\text{hat{z}}}^{(l)}=\\boldsymbol{\\text{z}}^{(l)}$。\",\n    \"问题9\": \"为什么在实践中使用移动平均来计算 $\\boldsymbol{\\text{mu}}_{\\boldsymbol{\\text{B}}}$ 和 $\\boldsymbol{\\text{sigma}}_{\\boldsymbol{\\text{B}}}^2$？\",\n    \"回答9\": \"在实践中使用移动平均来计算 $\\boldsymbol{\\text{mu}}_{\\boldsymbol{\\text{B}}}$ 和 $\\boldsymbol{\\text{sigma}}_{\\boldsymbol{\\text{B}}}^2$ 是为了保持模型训练过程中参数更新的稳定性。通过移动平均计算得到的均值和方差更能反映整体数据集上的统计特征，有利于模型训练过程中参数更新的稳定性。\",\n    \"问题10\": \"批量归一化如何作为隐形正则化方法？\",\n    \"回答10\": \"在训练时，神经网络对一个样本的预测不仅和该样本自身相关，也和同一批次中的其他样本相关。由于在选取批次时具有随机性，使得神经网络不会“过拟合”到某个特定样本，从而提高网络的泛化能力。这种效果使得批量归一化在实际应用中起到了隐形正则化的作用。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710755016,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1715,
            "prompt_tokens": 4716,
            "total_tokens": 6431
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]