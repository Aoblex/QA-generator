[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.2.4.3 Adam算法",
        "content_length": 1329,
        "content": "Title: 7.2.4.3 Adam算法\nContent:\nAdam算法 ( Adaptive Moment Estimation Algorithm ) [Kingma et al., 2015]可以看作动量法和 RMSprop 算法的结合, 不但使用动量作为参数更新方向, 而且可以自适应调整学习率.\n\n\\href{https://nndl.github.io/%E5%81%8F%E5%AF%BC%E6%95%B0}{https://nndl.github.io/偏导数} $\\mathfrak{g}_{t}$ 的定义参见公式(7.1).\n\nAdam 算法一方面计算梯度平方 $\\mathrm{g}_{t}^{2}$ 的指数加权平均 (和 RMSprop 算法类似), 另一方面计算梯度 $\\mathbf{g}_{t}$ 的指数加权平均 (和动量法类似).\n\n\\[\n\\begin{array}{r}\nM_{t}=\\beta_{1} M_{t-1}+\\left(1-\\beta_{1}\\right) \\boldsymbol{g}_{t}, \\\\\nG_{t}=\\beta_{2} G_{t-1}+\\left(1-\\beta_{2}\\right) \\boldsymbol{g}_{t} \\odot \\boldsymbol{g}_{t} \\tag{7.26}\n\\end{array}\n\\]\n\n其中 $\\beta_{1}$ 和 $\\beta_{2}$ 分别为两个移动平均的衰减率, 通常取值为 $\\beta_{1}=0.9, \\beta_{2}=0.99$. 我们可以把 $M_{t}$ 和 $G_{t}$ 分别看作梯度的均值 (一阶矩) 和未减去均值的方差 (二阶矩).\n\n假设 $M_{0}=0, G_{0}=0$, 那么在迭代初期 $M_{t}$ 和 $G_{t}$ 的值会比真实的均值和方差参见习题7-2.要小. 特别是当 $\\beta_{1}$ 和 $\\beta_{2}$ 都接近于 1 时, 偏差会很大. 因此, 需要对偏差进行修正.\n\n\n\\begin{align*}\n\\hat{M}_{t} & =\\frac{M_{t}}{1-\\beta_{1}^{t}}  \\tag{7.27}\\\\\n\\hat{G}_{t} & =\\frac{G_{t}}{1-\\beta_{2}^{t}} . \\tag{7.28}\n\\end{align*}\n\n\nAdam 算法的参数更新差值为\n\n\n\\begin{equation*}\n\\Delta \\theta_{t}=-\\frac{\\alpha}{\\sqrt{\\hat{G}_{t}+\\epsilon}} \\hat{M}_{t} \\tag{7.29}\n\\end{equation*}\n\n\n其中学习率 $\\alpha$ 通常设为 0.001 , 并且也可以进行衰减, 比如 $\\alpha_{t}=\\alpha_{0} / \\sqrt{t}$.\n\nAdam 算法是 RMSProp 算法与动量法的结合, 因此一种自然的 Adam 算法的改进方法是引入 Nesterov加速梯度,称为Nadam 算法[Dozat, 2016]."
    },
    {
        "id": "chatcmpl-943S1i1iIORP5V6mxmBuMdZ1cZLN2",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"Adam算法与动量法和RMSprop算法有什么关系？\",\n    \"回答1\": \"Adam算法可以看作动量法和RMSprop算法的结合。它不仅使用动量作为参数更新方向，而且可以自适应调整学习率，综合了两种算法的优点。\",\n    \"问题2\": \"Adam算法中的 $M_{t}$ 和 $G_{t}$ 分别代表什么意义？\",\n    \"回答2\": \"$M_{t}$ 可以看作梯度的均值（一阶矩），而 $G_{t}$ 可以看作未减去均值的梯度的方差（二阶矩）。它们分别用于计算梯度和梯度平方的指数加权平均，以实现自适应调整学习率。\",\n    \"问题3\": \"在Adam算法中，如何对偏差进行修正？\",\n    \"回答3\": \"为了对偏差进行修正，Adam算法引入了修正后的一阶矩 $\\text{和} \\text{二阶矩} \\text{分别为} \\frac{M_{t}}{1-\\beta_{1}^{t}} \\text{和} \\frac{G_{t}}{1-\\beta_{2}^{t}}$。这样可以在迭代初期减小偏差，特别是当 $\\beta_{1}$ 和 $\\beta_{2}$ 都接近于 1 时。\",\n    \"问题4\": \"Adam算法的参数更新差值是如何计算的？\",\n    \"回答4\": \"Adam算法的参数更新差值 $\\triangle \\theta_{t}$ 计算公式为 $\\triangle \\theta_{t}=-\\frac{\\text{学习率} \\text{通常设为} \\text{0.001}}{\\text{根号下的修正后的二阶矩}+\\text{小常数}\\text{（通常设为} \\text{\\text{10}^{-8}})} \\text{修正后的一阶矩}$。这样可以根据梯度的均值和方差来更新参数。\",\n    \"问题5\": \"学习率 $\\text{alpha}$ 在Adam算法中如何设置？\",\n    \"回答5\": \"学习率 $\\text{alpha}$ 通常设为 0.001，并且也可以进行衰减，比如 $\\text{alpha}_{t}=\\text{alpha}_{0} / \\text{根号下的迭代次数} t$。这样可以使得学习率随着迭代次数逐渐减小。\",\n    \"问题6\": \"Adam算法与Nadam算法有什么关系？\",\n    \"回答6\": \"Adam算法是RMSProp算法与动量法的结合，因此一种自然的改进方法是引入Nesterov加速梯度，称为Nadam算法。Nadam算法在Adam算法的基础上进一步引入了Nesterov加速梯度，以提高参数更新的效率和稳定性。\",\n    \"问题7\": \"Adam算法中的 $\\beta_{1}$ 和 $\\beta_{2}$ 分别代表什么意义？\",\n    \"回答7\": \"$\\beta_{1}$ 和 $\\beta_{2}$ 分别为两个移动平均的衰减率。它们用于控制梯度的一阶矩和二阶矩的权重，通常取值为 $\\beta_{1}=0.9, \\beta_{2}=0.99$。\",\n    \"问题8\": \"Adam算法中的小常数 $\\text{epsilon}$ 有何作用？\",\n    \"回答8\": \"小常数 $\\text{epsilon}$ 主要用于避免分母为零的情况，通常设为一个很小的数（如 $10^{-8}$）。它在参数更新差值计算中起到平滑和稳定计算的作用。\",\n    \"问题9\": \"Adam算法中的迭代初期 $M_{t}$ 和 $G_{t}$ 的值会受到什么影响？\",\n    \"回答9\": \"在迭代初期，$M_{t}$ 和 $G_{t}$ 的值会比真实的均值和方差要小。特别是当 $\\beta_{1}$ 和 $\\beta_{2}$ 都接近于 1 时，偏差会很大。因此，需要对偏差进行修正，引入修正后的一阶矩和二阶矩来减小偏差。\",\n    \"问题10\": \"Adam算法相比其他优化算法有哪些优势？\",\n    \"回答10\": \"Adam算法综合了动量法和RMSprop算法的优点，既能够考虑历史梯度信息（动量），又能够自适应调整学习率（RMSprop）。它在实际应用中通常能够更快地收敛并且具有较好的泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710753605,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1333,
            "prompt_tokens": 3373,
            "total_tokens": 4706
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]