[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.2.2.2 风险最小化准则",
        "content_length": 2286,
        "content": "Title: 2.2.2.2 风险最小化准则\nContent:\n一个好的模型 $f(\\boldsymbol{x} ; \\theta)$ 应当有一个比较小的期望错误, 但由于不知道真实的数据分布和映射函数, 实际上无法计算其期望风险 $\\mathcal{R}(\\theta)$. 给定一个训练集 $\\mathcal{D}=$ $\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, 我们可以计算的是经验风险 (Empirical Risk), 即在训练集上的平均损失:\n\n\n\\begin{equation*}\n\\mathcal{R}_{\\mathcal{D}}^{e m p}(\\theta)=\\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right) \\tag{2.22}\n\\end{equation*}\n\n\n因此,一个切实可行的学习准则是找到一组参数 $\\theta^{*}$ 使得经验风险最小, 即\n\n\n\\begin{equation*}\n\\theta^{*}=\\underset{\\theta}{\\arg \\min } \\mathcal{R}_{\\mathcal{D}}^{\\text {emp }}(\\theta) \\tag{2.23}\n\\end{equation*}\n\n\n这就是经验风险最小化 (Empirical Risk Minimization,ERM) 准则.\n\n过拟合 根据大数定理可知, 当训练集大小 $|\\mathcal{D}|$ 趋向于无穷大时, 经验风险就趋向于期望风险. 然而通常情况下, 我们无法获取无限的训练样本, 并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据, 不能很好地反映全部数据的真实分布. 经验风险最小化原则很容易导致模型在训练集上错误率很低, 但是在未知数据上错误率很高. 这就是所谓的过拟合 (Overfitting).\n\n定义 2.1 - 过拟合：给定一个假设空间 $\\mathcal{F}$, 一个假设 $f$ 属于 $\\mathcal{F}$, 如果存在其他的假设 $f^{\\prime}$ 也属于 $\\mathcal{F}$, 使得在训练集上 $f$ 的损失比 $f^{\\prime}$ 的损失小, 但在整个样本空间上 $f^{\\prime}$ 的损失比 $f$ 的损失小, 那么就说假设 $f$ 过度拟合训练数据 [Mitchell, 1997].经验风险也称为经验错误 ( Empirical Error ).\n\n如何选择训练样本个数可以参考PAC 学习理论,参见第2.8.1节。\\\\\n过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的.为了解决过拟合问题，一般在经验风险最小化的基础上再引入参数的正则化 ( Regularization) 来限制模型能力, 使其不要过度地最小化经验风险. 这种准则就是结构风险最小化 ( Structure Risk Minimization,SRM ) 准则:\n\n\n\\begin{align*}\n\\theta^{*} & =\\underset{\\theta}{\\arg \\min } \\mathcal{R}_{\\mathcal{D}}^{\\text {struct }}(\\theta)  \\tag{2.24}\\\\\n& =\\underset{\\theta}{\\arg \\min } \\mathcal{R}_{\\mathcal{D}}^{\\text {emp }}(\\theta)+\\frac{1}{2} \\lambda\\|\\theta\\|^{2}  \\tag{2.25}\\\\\n& =\\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(x^{(n)} ; \\theta\\right)\\right)+\\frac{1}{2} \\lambda\\|\\theta\\|^{2}, \\tag{2.26}\n\\end{align*}\n\n\n其中 $\\|\\theta\\|$ 是 $\\ell_{2}$ 范数的正则化项, 用来减少参数空间, 避免过拟合; $\\lambda$ 用来控制正则化的强度。\n\n正则化项也可以使用其他函数, 比如 $e_{1}$ 范数. $e_{1}$ 范数的引入通常会使得参数有一定稀疏性, 因此在很多算法中也经常使用. 从贝叶斯学习的角度来讲, 正则化是引入了参数的先验分布, 使其不完全依赖训练数据.\n\n和过拟合相反的一个概念是欠拟合 (Underfitting), 即模型不能很好地拟合训练数据, 在训练集上的错误率比较高. 欠拟合一般是由于模型能力不足造成的. 图2.3给出了欠拟合和过拟合的示例.\\\\\n\n\n图 2.3 欠拟合和过拟合示例\n\n总之, 机器学习中的学习准则并不仅仅是拟合训练集上的数据, 同时也要使得泛化错误最低. 给定一个训练集, 机器学习的目标是从假设空间中找到一个泛化错误较低的 “理想” 模型, 以便更好地对未知的样本进行预测, 特别是不在训练集中出现的样本. 因此, 我们可以将机器学习看作一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题."
    },
    {
        "id": "chatcmpl-940fBKtUOFQwnh4N12VobJdj8HZl5",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是期望风险和经验风险？它们之间有何区别？\",\n    \"回答1\": \"期望风险是模型在整个样本空间上的平均损失，但由于不知道真实的数据分布和映射函数，实际上无法计算其期望风险。而经验风险是在训练集上的平均损失，即模型在训练集上的表现。它们之间的区别在于期望风险考虑了整个样本空间，而经验风险只考虑了训练集。\",\n    \"问题2\": \"经验风险最小化准则是什么？它的目标是什么？\",\n    \"回答2\": \"经验风险最小化 (ERM) 准则是找到一组参数 $\\theta^{*}$ 使得经验风险最小。其目标是通过最小化在训练集上的平均损失来寻找最优的模型参数，以期望能够获得较低的期望风险。\",\n    \"问题3\": \"过拟合是指什么？它对机器学习模型有何影响？\",\n    \"回答3\": \"过拟合是指模型在训练集上表现良好，但在未知数据上表现较差的现象。它对机器学习模型的影响是导致模型在未知数据上错误率很高，从而影响模型的泛化能力。\",\n    \"问题4\": \"结构风险最小化准则是如何定义的？它与经验风险最小化有何区别？\",\n    \"回答4\": \"结构风险最小化 (SRM) 准则通过在经验风险最小化的基础上引入参数的正则化来限制模型能力，使其不要过度地最小化经验风险。它的定义包括经验风险和正则化项，而经验风险最小化只考虑在训练集上的平均损失。\",\n    \"问题5\": \"正则化项的作用是什么？为什么要引入正则化项？\",\n    \"回答5\": \"正则化项的作用是减少参数空间，避免过拟合。引入正则化项是为了限制模型的复杂度，防止模型在训练集上过度拟合，从而提高模型在未知数据上的泛化能力。\",\n    \"问题6\": \"欠拟合和过拟合分别指什么？它们之间有何区别？\",\n    \"回答6\": \"欠拟合是指模型不能很好地拟合训练数据，在训练集上的错误率比较高。而过拟合是指模型在训练集上表现良好，但在未知数据上表现较差。它们之间的区别在于欠拟合是由于模型能力不足造成的，而过拟合是由于模型能力过强或训练数据少和噪声等原因造成的。\",\n    \"问题7\": \"机器学习中的学习准则目标是什么？\",\n    \"回答7\": \"机器学习中的学习准则目标不仅仅是拟合训练集上的数据，同时也要使得泛化错误最低。给定一个训练集，机器学习的目标是从假设空间中找到一个泛化错误较低的 “理想” 模型，以便更好地对未知的样本进行预测。\",\n    \"问题8\": \"PAC 学习理论如何帮助解决过拟合问题？\",\n    \"回答8\": \"PAC 学习理论可以帮助选择适当数量的训练样本，从而帮助解决过拟合问题。通过理论分析和实践验证，PAC 学习理论提供了一种方法来确定训练样本数量，以达到对未知数据更好地泛化。\",\n    \"问题9\": \"正则化项可以使用哪些函数？它们各自有什么特点？\",\n    \"回答9\": \"正则化项可以使用 $\\text{L}_2$ 范数或 $\\text{L}_1$ 范数等函数。$\\text{L}_2$ 范数用来减少参数空间、避免过拟合；而 $\\text{L}_1$ 范数引入通常会使得参数有一定稀疏性，在很多算法中也经常使用。\",\n    \"问题10\": \"欠拟合和过拟合分别对机器学习模型有何影响？\",\n    \"回答10\": \"欠拟合会导致模型不能很好地拟合训练数据，在训练集上的错误率比较高；而过拟合会导致模型在未知数据上错误率很高。这些现象都会影响机器学习模型的泛化能力和预测性能。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710742889,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1452,
            "prompt_tokens": 4195,
            "total_tokens": 5647
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]