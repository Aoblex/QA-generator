[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.2.2.2 风险最小化准则",
        "content_length": 2370,
        "content": "Title: 2.2.2.2 风险最小化准则\nContent:\n一个好的模型 $f(\\boldsymbol{x} ; \\theta)$ 应当有一个比较小的期望错误, 但由于不知道真实的数据分布和映射函数, 实际上无法计算其期望风险 $\\mathcal{R}(\\theta)$. 给定一个训练集 $\\mathcal{D}=$ $\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, 我们可以计算的是经验风险 (Empirical Risk), 即在训练集上的平均损失:\n\n\n\\begin{equation*}\n\\mathcal{R}_{\\mathcal{D}}^{e m p}(\\theta)=\\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right) \\tag{2.22}\n\\end{equation*}\n\n\n因此,一个切实可行的学习准则是找到一组参数 $\\theta^{*}$ 使得经验风险最小, 即\n\n\n\\begin{equation*}\n\\theta^{*}=\\underset{\\theta}{\\arg \\min } \\mathcal{R}_{\\mathcal{D}}^{\\text {emp }}(\\theta) \\tag{2.23}\n\\end{equation*}\n\n\n这就是经验风险最小化 (Empirical Risk Minimization,ERM) 准则.\n\n过拟合 根据大数定理可知, 当训练集大小 $|\\mathcal{D}|$ 趋向于无穷大时, 经验风险就趋向于期望风险. 然而通常情况下, 我们无法获取无限的训练样本, 并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据, 不能很好地反映全部数据的真实分布. 经验风险最小化原则很容易导致模型在训练集上错误率很低, 但是在未知数据上错误率很高. 这就是所谓的过拟合 (Overfitting).\n\n定义 2.1 - 过拟合：给定一个假设空间 $\\mathcal{F}$, 一个假设 $f$ 属于 $\\mathcal{F}$, 如果存在其他的假设 $f^{\\prime}$ 也属于 $\\mathcal{F}$, 使得在训练集上 $f$ 的损失比 $f^{\\prime}$ 的损失小, 但在整个样本空间上 $f^{\\prime}$ 的损失比 $f$ 的损失小, 那么就说假设 $f$ 过度拟合训练数据 [Mitchell, 1997].经验风险也称为经验错误 ( Empirical Error ).\n\n如何选择训练样本个数可以参考PAC 学习理论,参见第2.8.1节。\\\\\n过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的.为了解决过拟合问题，一般在经验风险最小化的基础上再引入参数的正则化 ( Regularization) 来限制模型能力, 使其不要过度地最小化经验风险. 这种准则就是结构风险最小化 ( Structure Risk Minimization,SRM ) 准则:\n\n\n\\begin{align*}\n\\theta^{*} & =\\underset{\\theta}{\\arg \\min } \\mathcal{R}_{\\mathcal{D}}^{\\text {struct }}(\\theta)  \\tag{2.24}\\\\\n& =\\underset{\\theta}{\\arg \\min } \\mathcal{R}_{\\mathcal{D}}^{\\text {emp }}(\\theta)+\\frac{1}{2} \\lambda\\|\\theta\\|^{2}  \\tag{2.25}\\\\\n& =\\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(x^{(n)} ; \\theta\\right)\\right)+\\frac{1}{2} \\lambda\\|\\theta\\|^{2}, \\tag{2.26}\n\\end{align*}\n\n\n其中 $\\|\\theta\\|$ 是 $\\ell_{2}$ 范数的正则化项, 用来减少参数空间, 避免过拟合; $\\lambda$ 用来控制正则化的强度。\n\n正则化项也可以使用其他函数, 比如 $e_{1}$ 范数. $e_{1}$ 范数的引入通常会使得参数有一定稀疏性, 因此在很多算法中也经常使用. 从贝叶斯学习的角度来讲, 正则化是引入了参数的先验分布, 使其不完全依赖训练数据.\n\n和过拟合相反的一个概念是欠拟合 (Underfitting), 即模型不能很好地拟合训练数据, 在训练集上的错误率比较高. 欠拟合一般是由于模型能力不足造成的. 图2.3给出了欠拟合和过拟合的示例.\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_b7f820bc184c7846d78fg-044}\n\n图 2.3 欠拟合和过拟合示例\n\n总之, 机器学习中的学习准则并不仅仅是拟合训练集上的数据, 同时也要使得泛化错误最低. 给定一个训练集, 机器学习的目标是从假设空间中找到一个泛化错误较低的 “理想” 模型, 以便更好地对未知的样本进行预测, 特别是不在训练集中出现的样本. 因此, 我们可以将机器学习看作一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题."
    },
    {
        "id": "chatcmpl-92yHAKcqUILPSWG6djhRjtXVChqzL",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是经验风险和期望风险？它们之间有什么区别？\",\n    \"回答1\": \"经验风险是指在训练集上的平均损失，表示模型在已知数据上的表现。期望风险是指模型在整个样本空间上的期望损失，表示模型在未知数据上的表现。它们的区别在于经验风险是基于已知数据计算的平均损失，而期望风险是基于整个样本空间的期望损失。\",\n    \"问题2\": \"经验风险最小化准则是什么？它的目标是什么？\",\n    \"回答2\": \"经验风险最小化 (ERM) 准则是找到一组参数 $\\theta^{*}$ 使得经验风险最小，即 $\\theta^{*}=\\text{arg min}_{\\theta} \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ }(2.23)$。其目标是在训练集上找到能够使模型在已知数据上表现最好的参数。\",\n    \"问题3\": \"过拟合是什么意思？它对机器学习模型有什么影响？\",\n    \"回答3\": \"过拟合指模型在训练集上表现很好，但在未知数据上表现很差。这会导致模型在未知数据上的泛化能力不足，无法很好地适应新的数据，从而影响模型的预测能力和应用效果。\",\n    \"问题4\": \"结构风险最小化准则是如何解决过拟合问题的？\",\n    \"回答4\": \"结构风险最小化 (SRM) 准则在经验风险最小化的基础上引入参数的正则化，通过限制模型的能力来避免过拟合。其目标是找到一组参数 $\\theta^{*}$ 使得结构风险最小，即 $\\theta^{*}=\\text{arg min}_{\\theta} \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ } \\text{ }(2.24)$。通过正则化项来限制参数空间，避免模型过度拟合训练数据，提高模型的泛化能力。\",\n    \"问题5\": \"正则化项的作用是什么？它是如何控制模型复杂度的？\",\n    \"回答5\": \"正则化项通过限制参数空间来控制模型的复杂度，避免模型过度拟合训练数据。常见的正则化项如 $\\theta$ 的 $\\text{l}_2$ 范数，用来减少参数空间，避免过拟合。通过调节正则化项的系数（如 $\\theta$ 的 $\\text{l}_2$ 范数前的系数 $\\frac{1}{2} \\times \\theta^2$ 中的 $\\frac{1}{2}$），可以控制正则化的强度。\",\n    \"问题6\": \"欠拟合是什么意思？它与过拟合有何区别？\",\n    \"回答6\": \"欠拟合指模型不能很好地拟合训练数据，在训练集上表现较差。与过拟合相反，欠拟合是由于模型能力不足造成的。过拟合是模型在训练集上表现很好，但在未知数据上表现很差。\",\n    \"问题7\": \"机器学习中的泛化问题指的是什么？为什么泛化问题很重要？\",\n    \"回答7\": \"机器学习中的泛化问题指的是从有限、高维、有噪声的数据中得到更一般性规律的问题。泛化问题很重要，因为机器学习的目标不仅仅是拟合训练集上的数据，还要使得模型在未知的样本上具有较低的错误率，以便更好地对未知的样本进行预测。\",\n    \"问题8\": \"为什么在机器学习中需要考虑模型的泛化能力？\",\n    \"回答8\": \"在机器学习中需要考虑模型的泛化能力，因为模型不仅要在训练集上表现良好，还要对未知数据具有较低的错误率。只有具有良好的泛化能力的模型才能更好地适应新的数据，具有更好的预测能力和应用效果。\",\n    \"问题9\": \"正则化项可以使用哪些函数？它们各自有什么特点？\",\n    \"回答9\": \"正则化项可以使用 $\\text{l}_2$ 范数、$\\text{l}_1$ 范数等函数。$\\text{l}_2$ 范数用来减少参数空间，避免过拟合；而$\\text{l}_1$ 范数的引入通常会使得参数具有一定稀疏性，因此在很多算法中也经常使用。\",\n    \"问题10\": \"结构风险最小化准则与经验风险最小化准则有何区别？\",\n    \"回答10\": \"结构风险最小化准则在经验风险最小化的基础上引入了参数的正则化项，通过限制模型的能力来避免过拟合。相比之下，经验风险最小化准则仅关注在训练集上找到能够使模型在已知数据上表现最好的参数。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710495384,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1883,
            "prompt_tokens": 4165,
            "total_tokens": 6048
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.05,
            "presence_penalty": 0.05
        }
    }
]