[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.1.2 高维变量的非凸优化",
        "content_length": 1608,
        "content": "Title: 7.1.2 高维变量的非凸优化\nContent:\n低维空间的非凸优化问题主要是存在一些局部最优点. 基于梯度下降的优化方法会陷入局部最优点, 因此在低维空间中非凸优化的主要难点是如何选择初始化参数和逃离局部最优点. 深度神经网络的参数非常多, 其参数学习是在非常高维空间中的非凸优化问题, 其挑战和在低维空间中的非凸优化问题有所不同.\n\n鞍点 在高维空间中, 非凸优化的难点并不在于如何逃离局部最优点, 而是如何逃离鞍点 (Saddle Point) [Dauphin et al., 2014]. 鞍点的梯度是 0 , 但是在一些维度上是最高点,在另一些维度上是最低点,如图7.1所示.\n\n\n\n图 7.1 鞍点示例\n\n在高维空间中, 局部最小值 (Local Minima) 要求在每一维度上都是最低点, 这种概率非常低. 假设网络有 10,000 维参数, 梯度为 0 的点 (即驻点 (Stationary Point ））在某一维上是局部最小值的概率为 $p$, 那么在整个参数空间中, \\href{https://nndl.github.io/%E9%9E%8D%E7%82%B9%E7%9A%84%E5%8F%AB%E6%B3%95%E6%98%AF%E5%9B%A0%E4%B8%BA%E5%85%B6%E5%BD%A2%E7%8A%B6%E5%83%8F%E9%A9%AC%E9%9E%8D}{https://nndl.github.io/鞍点的叫法是因为其形状像马鞍}. 鞍点的特征是一阶梯度为 0 , 但是二阶梯度的 Hessian 矩阵不是半正定矩阵,参见定理C.2.\n\n在一般的非凸问题中, $p \\approx 0.5$.\\\\\n驻点是局部最优点的概率为 $p^{10,000}$, 这种可能性非常小. 也就是说, 在高维空间中大部分驻点都是鞍点.\n\n基于梯度下降的优化方法会在鞍点附近接近于停滞, 很难从这些鞍点中逃离. 因此, 随机梯度下降对于高维空间中的非凸优化问题十分重要, 通过在梯度方向上引入随机性, 可以有效地逃离鞍点.\n\n平坦最小值 深度神经网络的参数非常多, 并且有一定的冗余性, 这使得每单个参数对最终损失的影响都比较小, 因此会导致损失函数在局部最小解附近通常是一个平坦的区域, 称为平坦最小值 (Flat Minima) [Hochreiter et al., 1997; Li et al., 2017a]. 图7.2 给出了平坦最小值和尖锐最小值 (Sharp Minima) 的示例.\n\n\n\n(a) 平坦最小值\n\n\n\n(b) 尖锐最小值\n\n图 7.2 平坦最小值和尖锐最小值的示例 (图片来源: [Hochreiter et al., 1997])\n\n在一个平坦最小值的邻域内, 所有点对应的训练损失都比较接近, 表明我们在训练神经网络时, 不需要精确地找到一个局部最小解, 只要在一个局部最小解的邻域内就足够了. 平坦最小值通常被认为和模型泛化能力有一定的关系. 一般而言, 当一个模型收玫到一个平坦的局部最小值时, 其鲁棒性会更好, 即微小的参数变动不会剧烈影响模型能力; 而当一个模型收玫到一个尖锐的局部最小值时, 其鲁棒性也会比较差. 具备良好泛化能力的模型通常应该是鲁棒的, 因此理想的局部最小值应该是平坦的.\n\n局部最小解的等价性 在非常大的神经网络中, 大部分的局部最小解是等价的,它们在测试集上性能都比较相似. 此外, 局部最小解对应的训练损失都可能非常接近于全局最小解对应的训练损失 [Choromanska et al., 2015]. 虽然神经网络有一定概率收玫于比较差的局部最小值, 但随着网络规模增加, 网络陷入比较差的局部最小值的概率会大大降低. 在训练神经网络时, 我们通常没有必要找全局最小值,这反而可能导致过拟合."
    },
    {
        "id": "chatcmpl-942GVWpLJOzWiDbrl5KAg5l92dVk2",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"低维空间的非凸优化问题与高维空间的非凸优化问题的主要难点有何不同？\",\n    \"回答1\": \"在低维空间中，非凸优化的主要难点是如何选择初始化参数和逃离局部最优点。而在高维空间中，非凸优化的难点并不在于如何逃离局部最优点，而是如何逃离鞍点。\",\n    \"问题2\": \"在高维空间中，什么是鞍点（Saddle Point）？\",\n    \"回答2\": \"在高维空间中，鞍点是指梯度为 0，但在一些维度上是最高点，在另一些维度上是最低点的点。它的特征是一阶梯度为 0，但是二阶梯度的 Hessian 矩阵不是半正定矩阵。\",\n    \"问题3\": \"基于梯度下降的优化方法在高维空间中会遇到什么困难？\",\n    \"回答3\": \"基于梯度下降的优化方法会在鞍点附近接近于停滞，很难从这些鞍点中逃离。因此，对于高维空间中的非凸优化问题，随机梯度下降方法显得十分重要，通过在梯度方向上引入随机性，可以有效地逃离鞍点。\",\n    \"问题4\": \"什么是平坦最小值（Flat Minima）？\",\n    \"回答4\": \"平坦最小值指的是损失函数在局部最小解附近通常是一个平坦的区域。在一个平坦最小值的邻域内，所有点对应的训练损失都比较接近，表明我们在训练神经网络时，不需要精确地找到一个局部最小解，只要在一个局部最小解的邻域内就足够了。\",\n    \"问题5\": \"平坦最小值与模型泛化能力有何关系？\",\n    \"回答5\": \"平坦最小值通常被认为和模型泛化能力有一定的关系。当一个模型收敛到一个平坦的局部最小值时，其鲁棒性会更好，即微小的参数变动不会剧烈影响模型能力；而当一个模型收敛到一个尖锐的局部最小值时，其鲁棒性也会比较差。具备良好泛化能力的模型通常应该是鲁棒的，因此理想的局部最小值应该是平坦的。\",\n    \"问题6\": \"为什么在训练神经网络时通常没有必要找全局最小值？\",\n    \"回答6\": \"在非常大的神经网络中，大部分的局部最小解是等价的，它们在测试集上性能都比较相似。此外，在训练神经网络时，我们通常没有必要找全局最小值，这反而可能导致过拟合。\",\n    \"问题7\": \"驻点（Stationary Point）和局部最优点（Local Minima）在高维空间中有何不同？\",\n    \"回答7\": \"在高维空间中，驻点是局部最优点的概率非常低。假设网络有 10,000 维参数，则驻点是局部最优点的概率为 $p^{10,000}$，这种可能性非常小。因此，在高维空间中大部分驻点都是鞍点而不是局部最优点。\",\n    \"问题8\": \"为什么随着网络规模增加，在训练神经网络时网络陷入比较差的局部最小值的概率会大大降低？\",\n    \"回答8\": \"虽然神经网络有一定概率收敛于比较差的局部最小值，但随着网络规模增加，在训练神经网络时网络陷入比较差的局部最小值的概率会大大降低。这意味着大规模神经网络更容易避免陷入比较差的局部最小值。\",\n    \"问题9\": \"为什么随机梯度下降对于高维空间中的非凸优化问题十分重要？\",\n    \"回答9\": \"基于梯度下降的优化方法会在鞍点附近接近于停滞，很难从这些鞍点中逃离。因此，在高维空间中，随机梯度下降通过在梯度方向上引入随机性，可以有效地逃离鞍点。\",\n    \"问题10\": \"深度神经网络参数非常多会导致什么样的情况发生？\",\n    \"回答10\": \"深度神经网络参数非常多，并且有一定的冗余性，这使得每单个参数对最终损失的影响都比较小。这会导致损失函数在局部最小解附近通常是一个平坦的区域。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710749047,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1481,
            "prompt_tokens": 4057,
            "total_tokens": 5538
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]