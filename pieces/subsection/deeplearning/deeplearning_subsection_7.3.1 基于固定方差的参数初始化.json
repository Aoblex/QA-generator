[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.3.1 基于固定方差的参数初始化",
        "content_length": 980,
        "content": "Title: 7.3.1 基于固定方差的参数初始化\nContent:\n一种最简单的随机初始化方法是从一个固定均值 (通常为 0 ) 和方差 $\\sigma^{2}$ 的分布中采样来生成参数的初始值. 基于固定方差的参数初始化方法主要有以下两种:\n\n（1）高斯分布初始化: 使用一个高斯分布 $\\mathcal{N}\\left(0, \\sigma^{2}\\right)$ 对每个参数进行随机初始化.\n\n（2）均匀分布初始化:在一个给定的区间 $[-r, r]$ 内采用均匀分布来初始化预训练初始化通常会提升模型泛化能力的一种解释是预训练任务起到一定的正则化作用.\n\n遗忘门参见公式(6.55).\n\n随机初始化通常只应用在神经网络的权重矩阵上.\n\n这里的“固定”的含义是方差 $\\sigma^{2}$ 为一个预设值, 和神经元的输入、激活函数以及所在层数无关.\\\\\n参数. 假设随机变量 $x$ 在区间 $[a, b]$ 内均匀分布, 则其方差为\n\n\n\\begin{equation*}\n\\operatorname{var}(x)=\\frac{(b-a)^{2}}{12} \\tag{7.35}\n\\end{equation*}\n\n\n因此, 若使用区间为 $[-r, r]$ 的均匀分布来采样, 并满足 $\\operatorname{var}(x)=\\sigma^{2}$ 时, 则 $r$的取值为\n\n\n\\begin{equation*}\nr=\\sqrt{3 \\sigma^{2}} \\tag{7.36}\n\\end{equation*}\n\n\n在基于固定方差的随机初始化方法中, 比较关键的是如何设置方差 $\\sigma^{2}$. 如果参数范围取的太小, 一是会导致神经元的输出过小, 经过多层之后信号就慢慢消失了; 二是还会使得 Sigmoid 型激活函数丢失非线性的能力. 以 Sigmoid 型函数为例, 在 0 附近基本上是近似线性的. 这样多层神经网络的优势也就不存在了. 如果参数范围取的太大, 会导致输入状态过大. 对于 Sigmoid 型激活函数来说, 激活值变得饱和,梯度接近于 0 , 从而导致梯度消失问题.\n\n为了降低固定方差对网络性能以及优化效率的影响, 基于固定方差的随机初始化方法一般需要配合逐层归一化来使用.\n\n逐层归一化参见第7.5节"
    },
    {
        "id": "chatcmpl-943h9xWq3qtKlASY2UcHjn3m8H8ex",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"基于固定方差的参数初始化方法有哪两种？\",\n    \"回答1\": \"基于固定方差的参数初始化方法主要有高斯分布初始化和均匀分布初始化两种。\",\n    \"问题2\": \"高斯分布初始化和均匀分布初始化各自是如何进行参数的随机初始化的？\",\n    \"回答2\": \"高斯分布初始化使用一个高斯分布 $\\text{N}(0, \\text{σ}^2)$ 对每个参数进行随机初始化；而均匀分布初始化则在一个给定的区间 $[-r, r]$ 内采用均匀分布来初始化，其中 $r$ 的取值为 $\\text{r}=\\text{√3σ}$。\",\n    \"问题3\": \"在基于固定方差的随机初始化方法中，如何设置方差 $\\text{σ}^2$ 的取值？\",\n    \"回答3\": \"在基于固定方差的随机初始化方法中，关键是如何设置方差 $\\text{σ}^2$。如果参数范围取得太小，会导致神经元的输出过小，经过多层之后信号就慢慢消失了；而如果参数范围取得太大，会导致输入状态过大。为了降低固定方差对网络性能以及优化效率的影响，一般需要配合逐层归一化来使用。\",\n    \"问题4\": \"为什么在基于固定方差的随机初始化方法中需要配合逐层归一化来使用？\",\n    \"回答4\": \"在基于固定方差的随机初始化方法中，若参数范围取得太小或太大都会对网络性能产生负面影响。因此，为了降低固定方差对网络性能以及优化效率的影响，一般需要配合逐层归一化来使用，以保证网络训练的稳定性和收敛性。\",\n    \"问题5\": \"什么是逐层归一化？\",\n    \"回答5\": \"逐层归一化是一种用于提高神经网络训练稳定性和加速收敛速度的技术。它通过对每一层的输入进行归一化处理，使得每一层的输入分布保持稳定，有利于梯度传播和网络训练。\",\n    \"问题6\": \"逐层归一化是如何帮助提高神经网络训练稳定性和加速收敛速度的？\",\n    \"回答6\": \"逐层归一化通过对每一层的输入进行归一化处理，使得每一层的输入分布保持稳定。这有利于梯度传播和网络训练，可以减少梯度消失和爆炸问题，提高神经网络训练的稳定性和加速收敛速度。\",\n    \"问题7\": \"随机初始化通常应用在神经网络的哪些部分？\",\n    \"回答7\": \"随机初始化通常只应用在神经网络的权重矩阵上，即神经网络中的参数需要进行随机初始化。\",\n    \"问题8\": \"均匀分布下随机变量 $x$ 的方差如何计算？\",\n    \"回答8\": \"如果随机变量 $x$ 在区间 $[a, b]$ 内均匀分布，则其方差为 $\\text{var}(x)=\\frac{(b-a)^{2}}{12}$。\",\n    \"问题9\": \"为什么预训练初始化通常会提升模型泛化能力？\",\n    \"回答9\": \"预训练初始化通常会提升模型泛化能力的一个解释是预训练任务起到一定的正则化作用。通过预训练初始化，模型可以更好地学习数据特征，并且在训练过程中引入了一定程度的正则化，有助于提高模型对未知数据的泛化能力。\",\n    \"问题10\": \"固定方差对神经元输出和激活函数非线性能力有何影响？\",\n    \"回答10\": \"如果参数范围取得太小，会导致神经元输出过小，使得 Sigmoid 型激活函数丢失非线性的能力；而如果参数范围取得太大，会导致输入状态过大，对于 Sigmoid 型激活函数来说，激活值变得饱和，从而导致梯度消失问题。因此，固定方差对神经元输出和激活函数非线性能力具有重要影响。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710754543,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1328,
            "prompt_tokens": 3338,
            "total_tokens": 4666
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]