[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "13.2.6 训练",
        "content_length": 1852,
        "content": "Title: 13.2.6 训练\nContent:\n通过再参数化, 变分自编码器可以通过梯度下降法来学习参数, 从而提高变分自编码器的训练效率.\n\n给定一个数据集 $\\mathcal{D}=\\left\\{\\boldsymbol{x}^{(n)}\\right\\}_{n=1}^{N}$, 对于每个样本 $\\boldsymbol{x}^{(n)}$, 随机采样 $M$ 个变量 $\\epsilon^{(n, m)}, 1 \\leq m \\leq M$, 并通过公式(13.26)计算 $\\boldsymbol{z}^{(n, m)}$. 变分自编码器的目标函数近似为\n\n\n\\begin{equation*}\n\\mathcal{J}(\\phi, \\theta \\mid \\mathcal{D})=\\sum_{n=1}^{N}\\left(\\frac{1}{M} \\sum_{m=1}^{M} \\log p\\left(\\boldsymbol{x}^{(n)} \\mid \\boldsymbol{z}^{(n, m)} ; \\theta\\right)-\\mathrm{KL}\\left(q\\left(\\boldsymbol{z} \\mid \\boldsymbol{x}^{(n)} ; \\phi\\right), \\mathcal{N}(\\boldsymbol{z} ; \\mathbf{0}, \\boldsymbol{I})\\right)\\right) \\tag{13.27}\n\\end{equation*}\n\n\n如果采用随机梯度方法, 每次从数据集中采集一个样本 $\\boldsymbol{x}$ 和一个对应的随机变量 $\\epsilon$, 并进一步假设 $p(\\boldsymbol{x} \\mid \\boldsymbol{z} ; \\theta)$ 服从高斯分布 $\\mathcal{N}\\left(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}_{G}, \\lambda \\boldsymbol{I}\\right)$, 其中 $\\boldsymbol{\\mu}_{G}=f_{G}(\\boldsymbol{z} ; \\theta)$是生成网络的输出, $\\lambda$ 为控制方差的超参数,则目标函数可以简化为\n\n\n\\begin{equation*}\n\\mathcal{J}(\\phi, \\theta \\mid \\boldsymbol{x})=-\\frac{1}{2}\\left\\|\\boldsymbol{x}-\\mu_{G}\\right\\|^{2}-\\lambda \\mathrm{KL}\\left(\\mathcal{N}\\left(\\mu_{I}, \\sigma_{I}\\right), \\mathcal{N}(\\mathbf{0}, \\boldsymbol{I})\\right) \\tag{13.28}\n\\end{equation*}\n\n\n参见习题13-1.\n\n再参数化的另一个例子是逐层归一化, 参见第7.5节.\\\\\n其中第一项可以近似看作输入 $\\boldsymbol{x}$ 的重构正确性, 第二项可以看作正则化项, $\\lambda$ 可以看作正则化系数. 这和自编码器在形式上非常类似, 但它们的内在机理是完全不同的.\n\n变分自编码器的训练过程如图13.5所示, 其中空心矩形表示“目标函数”.\n\n\n\n图 13.5 变分自编码器的训练过程\n\n图13.6给出了在 MNIST 数据集上变分自编码器学习到的隐变量流形的可视化示例. 图13.6a是将训练集上每个样本 $\\boldsymbol{x}$ 通过推断网络映射到 2 维的隐变量空间, 图中的每个点表示 $\\mathbb{E}[\\boldsymbol{z} \\mid \\boldsymbol{x}]$, 不同颜色表示不同的数字. 图13.6b是对 2 维的标准高斯分布上进行均匀采样得到不同的隐变量 $\\boldsymbol{z}$, 然后通过生成网络产生 $\\mathbb{E}[\\boldsymbol{x} \\mid \\boldsymbol{z}]$.\n\n\n\n(a) 训练集上所有样本在隐空间上的投影\n\n\n\n(b) 隐变量 $\\boldsymbol{z}$ 在图像空间的投影\n\n图 13.6 在 MNIST 数据集上变分自编码器学习到的隐变量流形的可视化示例\n\n\\section*{13.3 生成对抗网络}"
    },
    {
        "id": "chatcmpl-945X4PMANcey9j3fv21q6BoBSQMZ4",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"再参数化是如何提高变分自编码器的训练效率的？\",\n    \"回答1\": \"再参数化通过随机采样 $M$ 个变量 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{x}^{(n)} \\text{ 且 } \\text{对于每个样本 } \\boldsymbol{x}^{(n)}}, \\text{随机采样 } M \\text{ 个变量 } \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{z}^{(n, m)}}, 1 \\text{ 为 } m \\text{ 为 } M}, \\text{ 并通过公式(13.26)计算 } \\boldsymbol{z}^{(n, m)}$。这种再参数化的方式使得变分自编码器可以通过梯度下降法来学习参数，从而提高了训练效率。\",\n    \"问题2\": \"变分自编码器的目标函数是如何定义的？\",\n    \"回答2\": \"变分自编码器的目标函数近似为 $\\begin{equation*}\\begin{aligned}&\\text{J}(\\text{φ}, \\theta | \\text{D})=\\text{∑}_{n=1}^{N}\\bigg(\\frac{1}{M} \\text{∑}_{m=1}^{M} \\text{log} p(\\text{x}^{(n)} | \\text{z}^{(n, m)} ; \\theta)-\\text{KL}(q(\\text{z} | \\text{x}^{(n)} ; \\text{φ}), \\text{N}(\\text{z} ; \\textbf{0}, \\textbf{I})\\bigg)\\bigg) \\tag{13.27}\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)\\bigg)$。其中第一项是输入 $\\text{x}$ 的重构正确性，第二项是正则化项，$\\text{λ}$ 是正则化系数。\",\n    \"问题3\": \"随机梯度方法下，变分自编码器的目标函数如何简化？\",\n    \"回答3\": \"在随机梯度方法下，每次从数据集中采集一个样本 $\\text{x}$ 和一个对应的随机变量 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{x}}}}}}}$，并假设 $p(\\text{x} | \\text{z} ; \\theta)$ 服从高斯分布 $\\text{N}(\\boldsymbol{x} | \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{x}}}}}}}}}, \\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\textbf{$\\t...}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}})$，其中 $\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出，$\\bar{x}=f_{G}(\\bar{z} ; \\theta)$ 是生成网络的输出。目标函数可以简化为 $\\begin{equation*}\\begin{aligned}&J(\\theta, φ | x)=-\\frac{1}{2}\\big|\\bar{x}-μ_{G}\\big|^{2}-λKL(N(μ_{I}, σ_{I}), N(0, I)) \\tag{13.28}\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。\\big)。$\",\n    \"问题4\": \"再参数化和逐层归一化都是什么？它们有何异同之处？\",\n    \"回答4\": \"再参数化和逐层归一化都是用于提高神经网络训练效率和性能的技术。再参数化通过随机采样来学习参数，而逐层归一化则是一种用于加速深度神经网络收敛速度的技术。它们的不同之处在于应用领域和具体实现方式不同，但都旨在提高神经网络的训练效率和性能。\",\n    \"问题5\": \"变分自编码器训练过程中目标函数起到什么作用？\",\n    \"回答5\": \"变分自编码器训练过程中，目标函数起到了衡量重构正确性和正则化项的作用。它通过最小化重构误差和正则化项来优化模型参数，从而实现对数据分布的有效建模和特征学习。\",\n    \"问题6\": \"生成对抗网络与变分自编码器有何异同之处？\",\n    \"回答6\": \"生成对抗网络（GAN）和变分自编码器（VAE）都是用于生成模型的技术，但它们的内在机理和实现方式完全不同。GAN 通过博弈论中的对抗训练来学习生成模型和判别模型，而 VAE 则通过最大化数据似然和最小化编码空间的 KL 散度来学习模型参数。\",\n    \"问题7\": \"变分自编码器在 MNIST 数据集上学习到的隐变量流形有什么特点？\",\n    \"回答7\": \"在 MNIST 数据集上，变分自编码器学习到的隐变量流形具有明显的特征：（a）将训练集上每个样本 $\\boldsymbol{x}$ 通过推断网络映射到 2 维的隐变量空间后，不同数字对应的点呈现出明显的聚类特性；（b）在 2 维的标准高斯分布上进行均匀采样得到不同的隐变量 $\\boldsymbol{z}$ 后，通过生成网络产生 $\\bar{x}$ 的投影呈现出清晰可辨认的数字特征。\",\n    \"问题8\": \"图 13.5 中空心矩形表示“目标函数”，这个表示有何含义？\",\n    \"回答8\": \"图 13.5 中空心矩形表示“目标函数”是指训练过程中优化的目标，即通过调整模型参数使得目标函数达到最小值。这个表示意味着在训练过程中需要不断优化目标函数以提高模型性能。\",\n    \"问题9\": \"再参数化和逐层归一化对神经网络训练有何影响？\",\n    \"回答9\": \"再参数化和逐层归一化都能够提高神经网络训练效率和性能。再参数化通过随机采样来学习参数，从而加速模型收敛；而逐层归一化则能够加速深度神经网络收敛速度，减少梯度消失问题。它们共同作用于提高神经网络训练效率和性能。\",\n    \"问题10\": \"变分自编码器与传统自编码器相比有何优势？\",\n    \"回答10\": \"与传统自编码器相比，变分自编码器具有更强大的建模能力和泛化能力。它能够有效地学习数据分布并进行特征学习，并且通过引入随机采样来实现更灵活、更有效地训练过程。这使得变分自编码器在实际应用中具有更广泛的适用性和更好的性能表现。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710761606,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 2402,
            "prompt_tokens": 3648,
            "total_tokens": 6050
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]