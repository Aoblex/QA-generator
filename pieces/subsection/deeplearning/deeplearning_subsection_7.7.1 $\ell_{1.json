[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.7.1 $\\ell_{1",
        "content_length": 1729,
        "content": "Title: 7.7.1 $\\ell_{1\nContent:\n$ 和 $\\ell_{2}$ 正则化}\n$\\ell_{1}$ 和 $\\ell_{2}$ 正则化是机器学习中最常用的正则化方法, 通过约束参数的 $\\ell_{1}$ 和 $\\ell_{2}$范数来减小模型在训练数据集上的过拟合现象.\n\n通过加入 $e_{1}$ 和 $e_{2}$ 正则化, 优化问题可以写为\n\n\n\\begin{equation*}\n\\theta^{*}=\\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right)+\\lambda e_{p}(\\theta) \\tag{7.68}\n\\end{equation*}\n\n\n其中 $\\mathcal{L}(\\cdot)$ 为损失函数, $N$ 为训练样本数量, $f(\\cdot)$ 为待学习的神经网络, $\\theta$ 为其参数, $\\ell_{p}$ 为范数函数, $p$ 的取值通常为 $\\{1,2\\}$ 代表 $\\ell_{1}$ 和 $\\ell_{2}$ 范数, $\\lambda$ 为正则化系数.\n\n带正则化的优化问题等价于下面带约束条件的优化问题,\n\n\n\\begin{equation*}\n\\theta^{*}=\\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right) \\tag{7.69}\n\\end{equation*}\n\n\n\n\\begin{equation*}\n\\text { s.t. } \\quad \\ell_{p}(\\theta) \\leq 1 \\text {. } \\tag{7.70}\n\\end{equation*}\n\n\n$\\ell_{1}$ 范数在零点不可导, 因此经常用下式来近似:\n\n\n\\begin{equation*}\n\\ell_{1}(\\theta)=\\sum_{d=1}^{D} \\sqrt{\\theta_{d}^{2}+\\epsilon} \\tag{7.71}\n\\end{equation*}\n\n\n其中 $D$ 为参数数量, $\\epsilon$ 为一个非常小的常数.\n\n图7.11给出了不同范数约束条件下的最优化问题示例. 红线表示函数 $e_{p}=$ $1, \\mathcal{F}$ 为函数 $f(\\theta)$ 的等高线 (为简单起见, 这里用直线表示). 可以看出, $\\ell_{1}$ 范数的约束通常会使得最优解位于坐标轴上, 从而使得最终的参数为稀疏性向量.范数参见第A.1.3节.\n\n参见第C.1.2节.\n\n参见第2.8.1节。\n\n过度参数化是指模型参数的数量远远大于训练数据的数量.\\\\\n\n\n图 7.11 不同范数约束条件下的最优化问题示例\n\n一种折中的正则化方法是同时加入 $e_{1}$ 和 $\\ell_{2}$ 正则化, 称为弹性网络正则化 ( Elastic Net Regularization ) [Zou et al., 2005], .\n\n\n\\begin{equation*}\n\\theta^{*}=\\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}\\left(y^{(n)}, f\\left(\\boldsymbol{x}^{(n)} ; \\theta\\right)\\right)+\\lambda_{1} \\ell_{1}(\\theta)+\\lambda_{2} \\ell_{2}(\\theta) \\tag{7.72}\n\\end{equation*}\n\n\n其中 $\\lambda_{1}$ 和 $\\lambda_{2}$ 分别为两个正则化项的系数."
    },
    {
        "id": "chatcmpl-944EYf0KTdNtg76mI0C77xlUG5kpX",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是 $\\text{L}_1$ 和 $\\text{L}_2$ 正则化？\",\n    \"回答1\": \"$\\text{L}_1$ 和 $\\text{L}_2$ 正则化是机器学习中最常用的正则化方法，通过约束参数的 $\\text{L}_1$ 和 $\\text{L}_2$ 范数来减小模型在训练数据集上的过拟合现象。\",\n    \"问题2\": \"带正则化的优化问题可以写为什么形式？\",\n    \"回答2\": \"带正则化的优化问题可以写为 $\\theta^{*}=\\text{arg}\\text{min}_{\\theta} \\frac{1}{N} \\text{sum}_{n=1}^{N} \\text{L}(y^{(n)}, f(\\boldsymbol{x}^{(n)} ; \\theta))+\\text{λ} e_{p}(\\theta)$ 的形式，其中 $\\text{L}(\\bullet)$ 为损失函数，$N$ 为训练样本数量，$f(\\bullet)$ 为待学习的神经网络，$\\theta$ 为其参数，$e_{p}$ 为范数函数，$\\text{λ}$ 为正则化系数。\",\n    \"问题3\": \"带正则化的优化问题等价于什么形式的优化问题？\",\n    \"回答3\": \"带正则化的优化问题等价于带约束条件的优化问题，即 $\\theta^{*}=\\text{arg}\\text{min}_{\\theta} \\frac{1}{N} \\text{sum}_{n=1}^{N} \\text{L}(y^{(n)}, f(\\boldsymbol{x}^{(n)} ; \\theta))$，s.t. $e_{p}(\\theta) \\text{≤ 1}$。\",\n    \"问题4\": \"为什么在 $\\text{L}_1$ 范数中需要使用近似形式？\",\n    \"回答4\": \"$\\text{L}_1$ 范数在零点不可导，因此经常使用近似形式 $\\text{L}_1(\\theta)=\\text{sum}_{d=1}^{D} \\text{sqrt}(\\theta_{d}^{2}+\\text{ε})$ 来代替。其中 $D$ 为参数数量，$\\text{ε}$ 为一个非常小的常数。\",\n    \"问题5\": \"图 7.11 中展示了什么内容？\",\n    \"回答5\": \"图 7.11 展示了不同范数约束条件下的最优化问题示例。红线表示函数 $e_{p}=1$，$\\text{F}$ 为函数 $f(\\theta)$ 的等高线。可以观察到，$\\text{L}_1$ 范数的约束通常会使得最优解位于坐标轴上，从而使得最终的参数为稀疏性向量。\",\n    \"问题6\": \"弹性网络正则化是什么？\",\n    \"回答6\": \"弹性网络正则化是一种折中的正则化方法，同时加入 $\\text{L}_1$ 和 $\\text{L}_2$ 正则化。其优化问题形式为 $\\theta^{*}=\\text{arg}\\text{min}_{\\theta} \\frac{1}{N} \\text{sum}_{n=1}^{N} \\text{L}(y^{(n)}, f(\\boldsymbol{x}^{(n)} ; \\theta))+\\text{λ}_{1} \\text{L}_1(\\theta)+\\text{λ}_{2} \\text{L}_2(\\theta)$。\",\n    \"问题7\": \"弹性网络正则化相比单一 $\\text{L}_1$ 或 $\\text{L}_2$ 正则化有何优势？\",\n    \"回答7\": \"弹性网络正则化相比单一 $\\text{L}_1$ 或 $\\text{L}_2$ 正则化具有更好的折中效果。同时考虑两种正则化项，能够更好地平衡稀疏性和参数之间的关系，从而提高模型的泛化能力。\",\n    \"问题8\": \"$\\theta^{*}$ 在带正则化的优化问题中代表什么？\",\n    \"回答8\": \"$\\theta^{*}$ 在带正则化的优化问题中代表模型参数的最优解，通过最小化损失函数和正则化项来得到最佳的参数取值。\",\n    \"问题9\": \"在机器学习中，为什么需要使用正则化方法？\",\n    \"回答9\": \"在机器学习中，需要使用正则化方法来减小模型在训练数据集上的过拟合现象。通过约束参数的范数，可以有效控制模型的复杂度，提高模型对未知数据的泛化能力。\",\n    \"问题10\": \"损失函数在带正则化的优化问题中起到什么作用？\",\n    \"回答10\": \"损失函数在带正则化的优化问题中衡量了模型预测值与真实值之间的差异，通过最小化损失函数来调整模型参数以使其更好地拟合训练数据集。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710756614,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1353,
            "prompt_tokens": 3553,
            "total_tokens": 4906
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]