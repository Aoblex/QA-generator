[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "11.5.4.3 吉布斯采样",
        "content_length": 11318,
        "content": "Title: 11.5.4.3 吉布斯采样\nContent:\n吉布斯采样 (Gibbs Sampling) 是一种有效地对高维空间中的分布进行采样的 MCMC 方法, 可以看作 Metropolis-Hastings 算法的特例. 吉布斯采样使用全条件概率 (Full Conditional Probability) 作为提议分布来依次对每个维度进行采样, 并设置接受率为 $A=1$.\n\n对于一个 $M$ 维的随机向量 $\\boldsymbol{X}=\\left[X_{1}, X_{2}, \\cdots, X_{M}\\right]^{\\top}$, 其第 $m$ 个变量 $X_{m}$ 的全条件概率为\n\n\n\\begin{align*}\np\\left(x_{m} \\mid \\boldsymbol{x}_{\\backslash m}\\right) & \\triangleq P\\left(X_{m}=x_{m} \\mid X_{\\backslash m}=\\boldsymbol{x}_{\\backslash m}\\right)  \\tag{11.116}\\\\\n& =p\\left(x_{m} \\mid x_{1}, x_{2}, \\cdots, x_{m-1}, x_{m+1}, \\cdots, x_{M}\\right), \\tag{11.117}\n\\end{align*}\n\n\n其中 $\\boldsymbol{x}_{\\backslash m}=\\left[x_{1}, x_{2}, \\cdots, x_{m-1}, x_{m+1}, \\cdots, x_{M}\\right]^{\\top}$ 表示除 $X_{m}$ 外其他变量的取值.\n\n吉布斯采样可以按照任意的顺序根据全条件分布依次对每个变量进行采样. 假设从一个随机的初始化状态 $\\boldsymbol{x}^{(0)}=\\left[x_{1}^{(0)}, x_{2}^{(0)}, \\cdots, x_{M}^{(0)}\\right]^{\\top}$ 开始, 按照下标顺序依次对 $M$ 个变量进行采样.\n\n\n\\begin{align*}\nx_{1}^{(1)} & \\sim p\\left(x_{1} \\mid x_{2}^{(0)}, x_{3}^{(0)}, \\cdots, x_{M}^{(0)}\\right),  \\tag{11.118}\\\\\nx_{2}^{(1)} & \\sim p\\left(x_{2} \\mid x_{1}^{(1)}, x_{3}^{(0)} \\cdots, x_{M}^{(0)}\\right),  \\tag{11.119}\\\\\n\\vdots &  \\tag{11.120}\\\\\nx_{M}^{(1)} & \\sim p\\left(x_{M} \\mid x_{1}^{(1)}, x_{2}^{(1)} \\cdots, x_{M-1}^{(1)}\\right),  \\tag{11.121}\\\\\n\\vdots &  \\tag{11.122}\\\\\nx_{1}^{(t)} & \\sim p\\left(x_{1} \\mid x_{2}^{(t-1)}, x_{3}^{(t-1)}, \\cdots, x_{M}^{(t-1)}\\right),  \\tag{11.123}\\\\\nx_{2}^{(t)} & \\sim p\\left(x_{2} \\mid x_{1}^{(t)}, x_{3}^{(t-1)} \\cdots, x_{M}^{(t-1)}\\right), \\\\\n\\vdots & \\\\\nx_{M}^{(t)} & \\sim p\\left(x_{M} \\mid x_{1}^{(t)}, x_{2}^{(t)} \\cdots, x_{M-1}^{(t)}\\right),\n\\end{align*}\n\n\n其中 $x_{m}^{(t)}$ 是第 $t$ 次迭代时变量 $X_{m}$ 的采样.\\\\\n吉布斯采样的每单步采样也构成一个马尔可夫链. 假设每个单步 (采样维度为第 $m$ 维 $)$ 的状态转移概率 $q\\left(\\boldsymbol{x} \\mid \\boldsymbol{x}^{\\prime}\\right)$ 为\n\n\\[\nq\\left(\\boldsymbol{x} \\mid \\boldsymbol{x}^{\\prime}\\right)=\\left\\{\\begin{array}{cl}\n\\frac{p(\\boldsymbol{x})}{p\\left(\\boldsymbol{x}_{\\backslash m}^{\\prime}\\right)} & \\text { if } \\quad \\boldsymbol{x}_{\\backslash m}=\\boldsymbol{x}_{\\backslash m}^{\\prime}  \\tag{11.124}\\\\\n0 & \\text { otherwise, }\n\\end{array}\\right.\n\\]\n\n其中边际分布 $p\\left(\\boldsymbol{x}_{\\backslash m}^{\\prime}\\right)=\\sum_{x_{m}^{\\prime}} p\\left(\\boldsymbol{x}^{\\prime}\\right)$, 等式 $\\boldsymbol{x}_{\\backslash m}=\\boldsymbol{x}_{\\backslash m}^{\\prime}$ 表示 $x_{k}=x_{k}^{\\prime}, \\forall k \\neq m$, 因此有 $p\\left(\\boldsymbol{x}_{\\backslash m}^{\\prime}\\right)=p\\left(\\boldsymbol{x}_{\\backslash m}\\right)$, 并可以得到\n\n\n\\begin{equation*}\np\\left(\\boldsymbol{x}^{\\prime}\\right) q\\left(\\boldsymbol{x} \\mid \\boldsymbol{x}^{\\prime}\\right)=p\\left(\\boldsymbol{x}^{\\prime}\\right) \\frac{p(\\boldsymbol{x})}{p\\left(\\boldsymbol{x}_{\\backslash i}^{\\prime}\\right)}=p(\\boldsymbol{x}) \\frac{p\\left(\\boldsymbol{x}^{\\prime}\\right)}{p\\left(\\boldsymbol{x}_{\\backslash i}\\right)}=p(\\boldsymbol{x}) q\\left(\\boldsymbol{x}^{\\prime} \\mid \\boldsymbol{x}\\right) . \\tag{11.125}\n\\end{equation*}\n\n\n根据细致平稳条件, 公式 (11.124) 中定义的状态转移概率 $q\\left(x \\mid x^{\\prime}\\right)$ 的马尔可夫链的平稳分布为 $p(\\boldsymbol{x})$. 随着迭代次数 $t$ 的增加, 样本 $\\boldsymbol{x}^{(t)}=\\left[x_{1}^{(t)}, x_{2}^{(t)} \\cdots, x_{M}^{(t)}\\right]^{\\top}$将收敛于概率分布 $p(\\boldsymbol{x})$.\n\n\\section*{11.6 总结和深入阅读}\n概率图模型提供了一个用图形来描述概率模型的框架, 这种可视化方法使我们可以更加容易地理解复杂模型的内在性质. 目前, 概率图模型已经是一个非常庞大的研究领域, 涉及众多的模型和算法. 很多机器学习模型也都可以用概率图模型来描述. 图11.16给出了概率图模型所涵盖的内容.\n\n\n\n图 11.16 概率图模型所涵盖内容的简单概括\n\n在本章中, 我们只介绍了部分内容. 要更全面深入地了解概率图模型, 可以阅读《Probabilistic Graphical Models: Principles and Techniques》[Koller et al., 2009] 《Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference》[Pearl, 2014] 或《Pattern Recognition and Machine Learning》 [Bishop, 2007] 中的相关章节.\\\\\n概率图模型中最基本的假设是条件独立性. 图形化表示直观地描述了随机变量之间的条件独立性, 有利于将复杂的概率模型分解为简单模型的组合, 并更好地理解概率模型的表示、推断、学习等方法.\n\n20 世纪 90 年代末, 概率图模型的研究逐步成熟. 到 21 世纪, 图模型在机器学习、计算机视觉、自然语言处理等领域开始不断发展壮大. 其中比较有代表性的模型有: 条件随机场 [Lafferty et al., 2001]、潜在狄利克雷分配 (Latent Dirichlet Allocation ) [Blei et al., 2003] 等. 此外, 图模型的结构学习也一直是非常重要但极具挑战性的研究方向.\n\n图模型与神经网络的关系 图模型和神经网络有着类似的网络结构, 但两者也有很大的不同. 图模型的节点是随机变量, 其图结构的主要功能是描述变量之间的依赖关系, 一般是稀疏连接. 使用图模型的好处是可以有效地进行统计推断.而神经网络中的节点是神经元, 是一个计算节点. 图模型中的每个变量一般有着明确的解释, 变量之间依赖关系一般是人工来定义. 而神经网络中的单个神经元则没有直观的解释. 如果将神经网络中每个神经元看作一个二值随机变量, 那神经网络就变成一个Sigmoid 信念网络.\n\n神经网络是判别模型, 直接用来分类. 而图模型不但可以是判别模型, 也可以是生成模型. 生成模型不但可以用来生成样本, 也可以通过贝叶斯公式用来做分类. 图模型的参数学习的目标函数为似然函数或条件似然函数, 若包含隐变量则通常通过 EM 算法来求解. 而神经网络参数学习的目标函数为交叉熵或平方误差等损失函数.\n\n目前, 神经网络和概率图模型的结合越来越紧密.一方面我们可以利用神经网络强大的表示能力和拟合能力来建模图模型中的推断问题 (比如变分自编码器, 第13.2节), 生成问题 (比如生成对抗网络, 第13.3节), 或势能函数 (比如 LSTM+CRF 模型 [Lample et al., 2016; Ma et al., 2016] ); 另一方面可以利用图模型的算法来解决复杂结构神经网络中的学习和推断问题, 比如图神经网络 ( Graph Neural Network, GNN ) [Gilmer et al., 2017; Li et al., 2015; Scarselli et al., 2009] 和结构化注意力 [Kim et al., 2017].\n\n\\section*{习题}\n习题 11-1 根据贝叶斯网络的定义,证明图11.3中的四种因果关系.\n\n习题 11-2 证明公式(11.9).\n\n参见公式(11.9).\n\n习题 11-3 根据公式(11.37), 推导线性链条件随机场 (参见公式(11.25)) 的参数更新公式.\\\\\n习题 11-4 证明仅当 $q(\\boldsymbol{z})=p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta)$ 时, 对数边际似然函数 $\\log p(\\boldsymbol{x} ; \\theta)$ 和其下界 $\\operatorname{ELBO}(q, \\boldsymbol{x} ; \\theta)$ 相等.\n\n习题 11-5 在高斯混合分布的参数估计中, 证明 $\\mathrm{M}$ 步中的参数更新公式, 即公式 (11.63)、公式(11.64)和公式(11.65).\n\n习题11-6 考虑一个伯努利混合分布,即\n\n\n\\begin{equation*}\np(x ; \\mu, \\pi)=\\sum_{k=1}^{K} \\pi_{k} p\\left(x ; \\mu_{k}\\right) \\tag{11.126}\n\\end{equation*}\n\n\n其中 $p\\left(x ; \\mu_{k}\\right)=\\mu_{k}^{x}\\left(1-\\mu_{k}\\right)^{(1-x)}$ 为伯努利分布.\n\n给定一组训练集合 $D=\\left\\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\right\\}$, 若用 $\\mathrm{EM}$ 算法来进行参数估计，推导其每步的参数更新公式.\n\n习题 11-7 在变分推断的目标公式(11.82)中，试分析为什么使用的 KL 散度是 $\\operatorname{KL}(q(\\boldsymbol{z}) \\| p(\\boldsymbol{z} \\mid \\boldsymbol{x}))$ 而不是 $\\mathrm{KL}(p(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\| q(\\boldsymbol{z}))$.\n\n习题 11-8 在图11.2a 的有向图中, 分析按不同的消除顺序计算边际概率 $p\\left(x_{3}\\right)$ 时的计算复杂度.\n\n习题 11-9 在树结构的图模型上应用信念传播算法时,推导其消息计算公式.\n\n习题 11-10 证明若分布 $p(x)$ 存在累积分布函数的逆函数 $\\mathrm{cdf}^{-1}(y), y \\in[0,1]$, 且随机变量 $\\xi$ 为 $[0,1]$ 区间上的均匀分布,则 $\\operatorname{cdf}^{-1}(\\xi)$ 服从分布 $p(x)$.\n\n\\section*{参考文献}\nBaum L E, Petrie T, 1966. Statistical inference for probabilistic functions of finite state markov chains [J]. The annals of mathematical statistics, 37(6):1554-1563.\n\nBerger A L, Pietra V J D, Pietra S A D, 1996. A maximum entropy approach to natural language processing[J]. Computational linguistics, 22(1):39-71.\n\nBishop C M, 2007. Pattern recognition and machine learning[M]. 5th edition. Springer.\n\nBlei D M, Ng A Y, Jordan M I, 2003. Latent dirichlet allocation[J]. Journal of machine Learning research, 3(Jan):993-1022.\n\nDella Pietra S, Della Pietra V, Lafferty J, 1997. Inducing features of random fields[J]. IEEE transactions on pattern analysis and machine intelligence, 19(4):380-393.\n\nGilmer J, Schoenholz S S, Riley P F, et al., 2017. Neural message passing for quantum chemistry[J]. arXiv preprint arXiv:1704.01212.\n\nKim Y, Denton C, Hoang L, et al., 2017. Structured attention networks[C]//Proceedings of 5th International Conference on Learning Representations.\n\nKoller D, Friedman N, 2009. Probabilistic graphical models: principles and techniques[M]. MIT press.\n\nLafferty J D, McCallum A, Pereira F C N, 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data[C]//Proceedings of the Eighteenth International Conference on Machine Learning.\n\n\\href{https://nndl.github.io/%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%B7%B7%E5%90%88%E5%88%86%E5%B8%83%E5%8F%82%E8%A7%81%E7%AC%ACD.2.1.1%E8%8A%82%E3%80%82}{https://nndl.github.io/伯努利混合分布参见第D.2.1.1节。}\n\n参见第11.5.1节\n\nLample G, Ballesteros M, Subramanian S, et al., 2016. Neural architectures for named entity recognition[J]. arXiv preprint arXiv:1603.01360.\n\nLauritzen S L, Spiegelhalter D J, 1988. Local computations with probabilities on graphical structures and their application to expert systems[J]. Journal of the Royal Statistical Society. Series B (Methodological):157-224.\n\nLi Y, Tarlow D, Brockschmidt M, et al., 2015. Gated graph sequence neural networks[J]. arXiv preprint arXiv:1511.05493.\n\nMa X, Hovy E, 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf[J]. arXiv preprint arXiv:1603.01354.\n\nNeal R M, 1992. Connectionist learning of belief networks[J]. Artificial intelligence, 56(1):71-113.\n\nPearl J, 2014. Probabilistic reasoning in intelligent systems: networks of plausible inference[M]. Elsevier.\n\nScarselli F, Gori M, Tsoi A C, et al., 2009. The graph neural network model[J]. IEEE Transactions on Neural Networks, 20(1):61-80.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\\section*{第 12 章 深度信念网络}\n计算的目的不在于数据, 而在于洞察事物.\n\n—理查德$\\cdot$卫斯里・汉明 ( Richard Wesley Hamming )\n\n1968 年图灵奖获得者\n\n对于一个复杂的数据分布, 我们往往只能观测到有限的局部特征, 并且这些特征通常会包含一定的噪声. 如果要对这个数据分布进行建模, 就需要挖掘出可观测变量之间复杂的依赖关系,以及可观测变量背后隐藏的内部表示.\n\n本章介绍一种可以有效学习变量之间复杂依赖关系的概率图模型 (深度信念网络) 以及两种相关的基础模型 (玻尔兹曼机和受限玻尔兹曼机). 深度信念网络中包含很多层的隐变量, 可以有效地学习数据的内部特征表示, 也可以作为一种有效的非线性降维方法. 这些学习到的内部特征表示包含了数据的更高级的、有价值的信息, 因此十分有助于后续的分类和回归等任务.\n\n玻尔兹曼机和深度信念网络都是生成模型, 借助隐变量来描述复杂的数据分布. 作为概率图模型, 玻尔兹曼机和深度信念网络的共同问题是推断和学习问题. 因为这两种模型都比较复杂, 并且都包含隐变量, 它们的推断和学习一般通过 MCMC 方法来进行近似估计. 这两种模型和神经网络有很强的对应关系, 在一定程度上也称为随机神经网络 (Stochastic Neural Network,SNN).\n\n\\section*{12.1 玻尔兹曼机}\n玻尔兹曼机 (Boltzmann Machine) 是一个随机动力系统 (Stochastic Dynamical System ), 每个变量的状态都以一定的概率受到其他变量的影响. 玻尔兹曼机可以用概率无向图模型来描述. 一个具有 $K$ 个节点 (变量) 的玻尔兹曼机满足以下三个性质:\n\n（1）每个随机变量是二值的, 所有随机变量可以用一个二值的随机向量\n\n动力系统 (Dynamical System ) 是数学上的一个概念, 用来描述一个空间中所有点随时间的变化情况, 比如钟摆晃动、水的流动等.\\\\\n$\\boldsymbol{X} \\in\\{0,1\\}^{K}$ 来表示, 其中可观测变量表示为 $\\boldsymbol{V}$, 隐变量表示为 $\\boldsymbol{H}$.\n\n（2）所有节点之间是全连接的. 每个变量 $X_{i}$ 都依赖于所有其他变量 $\\boldsymbol{X}_{\\backslash i}$.\n\n（3）每两个变量之间的互相影响 $\\left(X_{i} \\rightarrow X_{j}\\right.$ 和 $\\left.X_{j} \\rightarrow X_{i}\\right)$ 是对称的.\n\n图12.1给出了一个包含 3 个可观测变量和 3 个隐变量的玻尔兹曼机.\n\n\n\n图 12.1 一个有六个变量的玻尔兹曼机\n\n随机向量 $\\boldsymbol{X}$ 的联合概率由玻尔兹曼分布得到, 即\n\n\n\\begin{equation*}\np(\\boldsymbol{x})=\\frac{1}{Z} \\exp \\left(\\frac{-E(\\boldsymbol{x})}{T}\\right) \\tag{12.1}\n\\end{equation*}\n\n\n其中 $Z$ 为配分函数, $T$ 表示温度, 能量函数 $E(\\boldsymbol{x})$ 的定义为\n\n\n\\begin{align*}\nE(\\boldsymbol{x}) & \\triangleq E(\\boldsymbol{X}=\\boldsymbol{x}) \\\\\n& =-\\left(\\sum_{i<j} w_{i j} x_{i} x_{j}+\\sum_{i} b_{i} x_{i}\\right), \\tag{12.2}\n\\end{align*}\n\n\n其中 $w_{i j}$ 是两个变量 $x_{i}$ 和 $x_{j}$ 之间的连接权重, $x_{i} \\in\\{0,1\\}$ 表示状态, $b_{i}$ 是变量 $x_{i}$的偏置.\n\n如果两个变量 $X_{i}$ 和 $X_{j}$ 的取值都为 1 时,一个正的权重 $w_{i j}>0$ 会使得坡尔兹曼机的能量下降, 发生的概率变大; 相反, 一个负的权重会使得玻尔兹曼机的能量上升, 发生的概率变小. 因此, 如果令玻尔兹曼机中的每个变量 $X_{i}$ 代表一个基本假设, 其取值为 1 或 0 分别表示模型接受或拒绝该假设, 那么变量之间连接的权重代表了两个假设之间的弱约束关系 [Ackley et al., 1985]. 连接权重为可正可负的实数. 一个正的连接权重表示两个假设可以互相支持. 也就是说, 如果一个假设被接受, 另一个也很可能被接受. 相反, 一个负的连接权重表示两个假设不能同时被接受.\n\n玻尔兹曼机可以用来解决两类问题. 一类是搜索问题: 当给定变量之间的连接权重时, 需要找到一组二值向量, 使得整个网络的能量最低. 另一类是学习问题: 当给定变量的多组观测值时,学习网络的最优权重.这也是玻尔兹曼机名称的由来. 为简单起见,这里我们把玻尔兹曼常数 $k$ 吸收到温度 $T$ 中. 玻尔兹曼分布取自其提出者、奥地利物理学家路德维希・玻尔兹曼 ( Ludwig Boltzmann, 1844 1906), 他在 1868 年研究热平衡气体的统计力学时首次提出了这一分布.\n\n\\section*{数学小知识 I 玻尔兹曼分布}\n在统计力学中, 玻尔兹曼分布 (Boltzmann Distribution) 是描述粒子处于特定状态下的概率, 是关于状态能量与系统温度的函数. 一个粒子处于状态 $\\alpha$ 的概率 $p_{\\alpha}$ 是关于状态能量与系统温度的函数:\n\n\n\\begin{equation*}\np_{\\alpha}=\\frac{1}{Z} \\exp \\left(\\frac{-E_{\\alpha}}{k T}\\right), \\tag{12.3}\n\\end{equation*}\n\n\n其中 $E_{\\alpha}$ 为状态 $\\alpha$ 的能量, $k$ 为玻尔兹曼常量, $T$ 为系统温度, $\\exp \\left(\\frac{-E_{\\alpha}}{k T}\\right)$ 称为玻尔兹曼因子 (Boltzmann Factor ), 是没有归一化的概率. $Z$ 为归一化因子, 通常称为配分函数 ( Partition Function ), 是对系统所有状态进行总和, $Z=\\sum_{\\alpha} \\exp \\left(\\frac{-E_{\\alpha}}{k T}\\right)$.\n\n玻尔兹曼分布的一个性质是两个状态的概率比仅仅依赖于两个状态能量的差值, 即\n\n\n\\begin{equation*}\n\\frac{p_{\\alpha}}{p_{\\beta}}=\\exp \\left(\\frac{E_{\\beta}-E_{\\alpha}}{k T}\\right) \\tag{12.4}\n\\end{equation*}"
    },
    {}
]