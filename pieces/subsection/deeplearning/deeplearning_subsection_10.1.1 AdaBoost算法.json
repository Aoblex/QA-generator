[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "10.1.1 AdaBoost算法",
        "content_length": 4585,
        "content": "Title: 10.1.1 AdaBoost算法\nContent:\nBoosting 类集成模型的目标是学习一个加性模型 (Additive Model)\n\n\n\\begin{equation*}\nF(\\boldsymbol{x})=\\sum_{m=1}^{M} \\alpha_{m} f_{m}(\\boldsymbol{x}) \\tag{10.10}\n\\end{equation*}\n\n\n其中 $f_{m}(\\boldsymbol{x})$ 为弱分类器 (Weak Classifier), 或基分类器 (Base Classifier), $\\alpha_{m}$为弱分类器的集成权重, $F(\\boldsymbol{x})$ 称为强分类器 (Strong Classifier).\n\nBoosting 类方法的关键是如何训练每个弱分类器 $f_{m}(\\boldsymbol{x})$ 及其权重 $\\alpha_{m}$. 为了提高集成的效果, 应当尽量使得每个弱分类器的差异尽可能大. 一种有效的算法是采用迭代的方法来学习每个弱分类器, 即按照一定的顺序依次训练每个弱分类器. 假设已经训练了 $m$ 个弱分类器, 在训练第第 $m+1$ 个弱分类器时, 增加已有弱分类器分错样本的权重, 使得第 $m+1$ 个弱分类器 “更关注” 于已有弱分类器 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n分错的样本. 这样增加每个弱分类器的差异, 最终提升集成分类器的准确率. 这种方法称为AdaBoost (Adaptive Boosting) 算法.\n\nAdaBoost 算法是一种迭代式的训练算法, 通过改变数据分布来提高弱分类器的差异. 在每一轮训练中, 增加分错样本的权重, 减少分对样本的权重, 从而得到一个新的数据分布.\n\n以二分类为例, 弱分类器 $f_{m}(x) \\in\\{+1,-1\\}$, AdaBoost 算法的训练过程如算法10.1所示. 最初赋予每个样本同样的权重. 在每一轮迭代中, 根据当前的样本权重训练一个新的弱分类器. 然后根据这个弱分类器的错误率来计算其集成权重,并调整样本权重.\n\n算法 10.1 二分类的 AdaBoost 算法\n\n输入: 训练集 $\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, 迭代次数 $M$\n\n1 初始样本权重: $w_{1}^{(n)} \\leftarrow \\frac{1}{N}, \\forall n \\in[1, N]$;\n\n2 for $m=1 \\cdots M$ do\n\n3 按照样本权重 $w_{m}^{(1)}, \\cdots, w_{m}^{(N)}$, 学习弱分类器 $f_{m}$;\n\n4 计算弱分类器 $f_{m}$ 在数据集上的加权错误 $\\epsilon_{m}$;\n\n5 计算分类器的集成权重:\n\n$$\n\\alpha_{m} \\leftarrow \\frac{1}{2} \\log \\frac{1-\\epsilon_{m}}{\\epsilon_{m}} ;\n$$\n\n6\n\n调整样本权重：\n\n$$\nw_{m+1}^{(n)} \\leftarrow w_{m}^{(n)} \\exp \\left(-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right), \\forall n \\in[1, N]\n$$\n\n7 end\n\n输出: $F(\\boldsymbol{x})=\\operatorname{sgn}\\left(\\sum_{m=1}^{M} \\alpha_{m} f_{m}(\\boldsymbol{x})\\right)$\n\nAdaBoost算法的统计学解释 AdaBoost算法也可以看作一种分步 ( Stage-Wise )优化的加性模型 [Friedman et al., 2000], 其损失函数定义为\n\n\n\\begin{align*}\n\\mathcal{L}(F) & =\\exp (-y F(\\boldsymbol{x}))  \\tag{10.11}\\\\\n& =\\exp \\left(-y \\sum_{m=1}^{M} \\alpha_{m} f_{m}(\\boldsymbol{x})\\right) \\tag{10.12}\n\\end{align*}\n\n\n其中 $y, f_{m}(\\boldsymbol{x}) \\in\\{+1,-1\\}$.\n\n假设经过 $m-1$ 次迭代,得到\n\n\n\\begin{equation*}\nF_{m-1}(\\boldsymbol{x})=\\sum_{t=1}^{m-1} \\alpha_{t} f_{t}(\\boldsymbol{x}) \\tag{10.13}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n则第 $m$ 次迭代的目标是找一个 $\\alpha_{m}$ 和 $f_{m}(\\boldsymbol{x})$ 使得下面的损失函数最小.\n\n\n\\begin{equation*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right)=\\sum_{n=1}^{N} \\exp \\left(-y^{(n)}\\left(F_{m-1}\\left(\\boldsymbol{x}^{(n)}\\right)+\\alpha_{m} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right)\\right) . \\tag{10.14}\n\\end{equation*}\n\n\n令 $w_{m}^{(n)}=\\exp \\left(-y^{(n)} F_{m-1}\\left(\\boldsymbol{x}^{(n)}\\right)\\right)$, 则损失函数可以写为\n\n\n\\begin{equation*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right)=\\sum_{n=1}^{N} w_{m}^{(n)} \\exp \\left(-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right) \\tag{10.15}\n\\end{equation*}\n\n\n因为 $y, f_{m}(\\boldsymbol{x}) \\in\\{+1,-1\\}$, 有\n\n\n\\begin{equation*}\ny f_{m}(\\boldsymbol{x})=1-2 I\\left(y \\neq f_{m}(\\boldsymbol{x})\\right) \\tag{10.16}\n\\end{equation*}\n\n\n其中 $I(x)$ 为指示函数.\n\n将损失函数在 $-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)=0$ 处进行二阶泰勒展开, 有\n\n\n\\begin{align*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right) & =\\sum_{n=1}^{N} w_{m}^{(n)}\\left(1-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)+\\frac{1}{2} \\alpha_{m}^{2}\\right)  \\tag{10.17}\\\\\n& \\propto \\alpha_{m} \\sum_{n=1}^{N} w_{m}^{(n)} I\\left(y^{(n)} \\neq f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right) . \\tag{10.18}\n\\end{align*}\n\n\n从上式可以看出, 当 $\\alpha_{m}>0$ 时, 最优的分类器 $f_{m}(\\boldsymbol{x})$ 为使得在样本权重为 $w_{m}^{(n)}, 1 \\leq n \\leq N$ 时的加权错误率最小的分类器.\n\n在求解出 $f_{m}(\\boldsymbol{x})$ 之后, 公式(10.15)可以写为\n\n\n\\begin{align*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right) & =\\sum_{y^{(n)}=f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)} w_{m}^{(n)} \\exp \\left(-\\alpha_{m}\\right)+\\sum_{y^{(n)} \\neq f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)} w_{m}^{(n)} \\exp \\left(\\alpha_{m}\\right)  \\tag{10.19}\\\\\n& \\propto\\left(1-\\epsilon_{m}\\right) \\exp \\left(-\\alpha_{m}\\right)+\\epsilon_{m} \\exp \\left(\\alpha_{m}\\right), \\tag{10.20}\n\\end{align*}\n\n\n其中 $\\epsilon_{m}$ 为分类器 $f_{m}(\\boldsymbol{x})$ 的加权错误率,\n\n\n\\begin{equation*}\n\\epsilon_{m}=\\frac{\\sum_{y^{(n)} \\neq f_{m}\\left(x^{(n)}\\right)} w_{m}^{(n)}}{\\sum_{n} w_{m}^{(n)}} \\tag{10.21}\n\\end{equation*}\n\n\n求上式关于 $\\alpha_{m}$ 的导数并令其为 0 , 得到\n\n\n\\begin{equation*}\n\\alpha_{m}=\\frac{1}{2} \\log \\frac{1-\\epsilon_{m}}{\\epsilon_{m}} \\tag{10.22}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\\section*{10.2 自训练和协同训练}\n监督学习往往需要大量的标注数据, 而标注数据的成本比较高. 因此, 利用大量的无标注数据来提高监督学习的效果有着十分重要的意义. 这种利用少量标注数据和大量无标注数据进行学习的方式称为半监督学习 (Semi-Supervised Learning,SSL ) . 本节介绍两种半监督学习算法: 自训练和协同训练."
    },
    {
        "id": "chatcmpl-944mX6B3t899IIZVGHy3sSlHHEl5V",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"Boosting 类集成模型的目标是什么？\",\n    \"回答1\": \"Boosting 类集成模型的目标是学习一个加性模型 $F(\\boldsymbol{x})=\\text{∑}_{m=1}^{M} \\boldsymbol{\\text{α}}_{m} f_{m}(\\boldsymbol{x})$，其中 $f_{m}(\\boldsymbol{x})$ 为弱分类器（Weak Classifier）或基分类器（Base Classifier），$\\boldsymbol{\\text{α}}_{m}$ 为弱分类器的集成权重，$F(\\boldsymbol{x})$ 称为强分类器（Strong Classifier）。\",\n    \"问题2\": \"Boosting 类方法的关键是什么？\",\n    \"回答2\": \"Boosting 类方法的关键是如何训练每个弱分类器 $f_{m}(\\boldsymbol{x})$ 及其权重 $\\boldsymbol{\\text{α}}_{m}$。为了提高集成的效果，应当尽量使得每个弱分类器的差异尽可能大。一种有效的算法是采用迭代的方法来学习每个弱分类器，即按照一定的顺序依次训练每个弱分类器。\",\n    \"问题3\": \"什么是AdaBoost算法？\",\n    \"回答3\": \"AdaBoost 算法是一种迭代式的训练算法，通过改变数据分布来提高弱分类器的差异。在每一轮训练中，增加分错样本的权重，减少分对样本的权重，从而得到一个新的数据分布。\",\n    \"问题4\": \"以二分类为例，AdaBoost 算法的训练过程是怎样的？\",\n    \"回答4\": \"以二分类为例，AdaBoost 算法的训练过程包括初始化样本权重，然后在每一轮迭代中，根据当前的样本权重训练一个新的弱分类器。接着根据这个弱分类器的错误率来计算其集成权重，并调整样本权重。\",\n    \"问题5\": \"AdaBoost算法的统计学解释是什么？\",\n    \"回答5\": \"AdaBoost算法也可以看作一种分步（Stage-Wise）优化的加性模型，其损失函数定义为 $\\boldsymbol{\\text{L}}(F)=\\text{exp}(-y F(\\boldsymbol{x}))$。假设经过 $m-1$ 次迭代，得到 $F_{m-1}(\\boldsymbol{x})=\\text{∑}_{t=1}^{m-1} \\boldsymbol{\\text{α}}_{t} f_{t}(\\boldsymbol{x})$，则第 $m$ 次迭代的目标是找一个 $\\boldsymbol{\\text{α}}_{m}$ 和 $f_{m}(\\boldsymbol{x})$ 使得损失函数最小。\",\n    \"问题6\": \"半监督学习指什么？\",\n    \"回答6\": \"半监督学习是利用少量标注数据和大量无标注数据进行学习的方式。监督学习往往需要大量的标注数据，而标注数据的成本比较高。因此，利用大量的无标注数据来提高监督学习的效果有着十分重要的意义。\",\n    \"问题7\": \"半监督学习中有哪两种算法？\",\n    \"回答7\": \"半监督学习中有两种算法：自训练和协同训练。\",\n    \"问题8\": \"自训练和协同训练算法分别是如何工作的？\",\n    \"回答8\": \"自训练算法利用已有标记数据训练模型，然后使用该模型对未标记数据进行预测，并将预测结果中置信度较高的样本加入到已标记数据中。协同训练算法则是通过多个初始假设模型相互交互地生成新的训练数据和新的假设模型，以提高模型性能。\",\n    \"问题9\": \"在AdaBoost算法中，如何计算分类器的集成权重 $\\boldsymbol{\\text{α}}_{m}$？\",\n    \"回答9\": \"在AdaBoost算法中，计算分类器的集成权重 $\\boldsymbol{\\text{α}}_{m}$ 的公式为 $\\boldsymbol{\\text{α}}_{m} \\rightarrow \\frac{1}{2} \\text{log} \\frac{1-\\boldsymbol{\\text{ε}}_{m}}{\\boldsymbol{\\text{ε}}_{m}}$，其中 $\\boldsymbol{\\text{ε}}_{m}$ 为分类器 $f_{m}(\\boldsymbol{x})$ 的加权错误率。\",\n    \"问题10\": \"在AdaBoost算法中，如何调整样本权重 $w_{m+1}^{(n)}$？\",\n    \"回答10\": \"在AdaBoost算法中，调整样本权重 $w_{m+1}^{(n)}$ 的公式为 $w_{m+1}^{(n)} \\rightarrow w_{m}^{(n)} \\text{exp}\\big(-\\boldsymbol{\\text{α}}_{m} y^{(n)} f_{m}\\big(\\boldsymbol{x}^{(n)}\\big)\\big)$。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710758721,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1403,
            "prompt_tokens": 5129,
            "total_tokens": 6532
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]