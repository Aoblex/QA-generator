[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "10.1.1 AdaBoost算法",
        "content_length": 4585,
        "content": "Title: 10.1.1 AdaBoost算法\nContent:\nBoosting 类集成模型的目标是学习一个加性模型 (Additive Model)\n\n\n\\begin{equation*}\nF(\\boldsymbol{x})=\\sum_{m=1}^{M} \\alpha_{m} f_{m}(\\boldsymbol{x}) \\tag{10.10}\n\\end{equation*}\n\n\n其中 $f_{m}(\\boldsymbol{x})$ 为弱分类器 (Weak Classifier), 或基分类器 (Base Classifier), $\\alpha_{m}$为弱分类器的集成权重, $F(\\boldsymbol{x})$ 称为强分类器 (Strong Classifier).\n\nBoosting 类方法的关键是如何训练每个弱分类器 $f_{m}(\\boldsymbol{x})$ 及其权重 $\\alpha_{m}$. 为了提高集成的效果, 应当尽量使得每个弱分类器的差异尽可能大. 一种有效的算法是采用迭代的方法来学习每个弱分类器, 即按照一定的顺序依次训练每个弱分类器. 假设已经训练了 $m$ 个弱分类器, 在训练第第 $m+1$ 个弱分类器时, 增加已有弱分类器分错样本的权重, 使得第 $m+1$ 个弱分类器 “更关注” 于已有弱分类器 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n分错的样本. 这样增加每个弱分类器的差异, 最终提升集成分类器的准确率. 这种方法称为AdaBoost (Adaptive Boosting) 算法.\n\nAdaBoost 算法是一种迭代式的训练算法, 通过改变数据分布来提高弱分类器的差异. 在每一轮训练中, 增加分错样本的权重, 减少分对样本的权重, 从而得到一个新的数据分布.\n\n以二分类为例, 弱分类器 $f_{m}(x) \\in\\{+1,-1\\}$, AdaBoost 算法的训练过程如算法10.1所示. 最初赋予每个样本同样的权重. 在每一轮迭代中, 根据当前的样本权重训练一个新的弱分类器. 然后根据这个弱分类器的错误率来计算其集成权重,并调整样本权重.\n\n算法 10.1 二分类的 AdaBoost 算法\n\n输入: 训练集 $\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, 迭代次数 $M$\n\n1 初始样本权重: $w_{1}^{(n)} \\leftarrow \\frac{1}{N}, \\forall n \\in[1, N]$;\n\n2 for $m=1 \\cdots M$ do\n\n3 按照样本权重 $w_{m}^{(1)}, \\cdots, w_{m}^{(N)}$, 学习弱分类器 $f_{m}$;\n\n4 计算弱分类器 $f_{m}$ 在数据集上的加权错误 $\\epsilon_{m}$;\n\n5 计算分类器的集成权重:\n\n$$\n\\alpha_{m} \\leftarrow \\frac{1}{2} \\log \\frac{1-\\epsilon_{m}}{\\epsilon_{m}} ;\n$$\n\n6\n\n调整样本权重：\n\n$$\nw_{m+1}^{(n)} \\leftarrow w_{m}^{(n)} \\exp \\left(-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right), \\forall n \\in[1, N]\n$$\n\n7 end\n\n输出: $F(\\boldsymbol{x})=\\operatorname{sgn}\\left(\\sum_{m=1}^{M} \\alpha_{m} f_{m}(\\boldsymbol{x})\\right)$\n\nAdaBoost算法的统计学解释 AdaBoost算法也可以看作一种分步 ( Stage-Wise )优化的加性模型 [Friedman et al., 2000], 其损失函数定义为\n\n\n\\begin{align*}\n\\mathcal{L}(F) & =\\exp (-y F(\\boldsymbol{x}))  \\tag{10.11}\\\\\n& =\\exp \\left(-y \\sum_{m=1}^{M} \\alpha_{m} f_{m}(\\boldsymbol{x})\\right) \\tag{10.12}\n\\end{align*}\n\n\n其中 $y, f_{m}(\\boldsymbol{x}) \\in\\{+1,-1\\}$.\n\n假设经过 $m-1$ 次迭代,得到\n\n\n\\begin{equation*}\nF_{m-1}(\\boldsymbol{x})=\\sum_{t=1}^{m-1} \\alpha_{t} f_{t}(\\boldsymbol{x}) \\tag{10.13}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n则第 $m$ 次迭代的目标是找一个 $\\alpha_{m}$ 和 $f_{m}(\\boldsymbol{x})$ 使得下面的损失函数最小.\n\n\n\\begin{equation*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right)=\\sum_{n=1}^{N} \\exp \\left(-y^{(n)}\\left(F_{m-1}\\left(\\boldsymbol{x}^{(n)}\\right)+\\alpha_{m} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right)\\right) . \\tag{10.14}\n\\end{equation*}\n\n\n令 $w_{m}^{(n)}=\\exp \\left(-y^{(n)} F_{m-1}\\left(\\boldsymbol{x}^{(n)}\\right)\\right)$, 则损失函数可以写为\n\n\n\\begin{equation*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right)=\\sum_{n=1}^{N} w_{m}^{(n)} \\exp \\left(-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right) \\tag{10.15}\n\\end{equation*}\n\n\n因为 $y, f_{m}(\\boldsymbol{x}) \\in\\{+1,-1\\}$, 有\n\n\n\\begin{equation*}\ny f_{m}(\\boldsymbol{x})=1-2 I\\left(y \\neq f_{m}(\\boldsymbol{x})\\right) \\tag{10.16}\n\\end{equation*}\n\n\n其中 $I(x)$ 为指示函数.\n\n将损失函数在 $-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)=0$ 处进行二阶泰勒展开, 有\n\n\n\\begin{align*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right) & =\\sum_{n=1}^{N} w_{m}^{(n)}\\left(1-\\alpha_{m} y^{(n)} f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)+\\frac{1}{2} \\alpha_{m}^{2}\\right)  \\tag{10.17}\\\\\n& \\propto \\alpha_{m} \\sum_{n=1}^{N} w_{m}^{(n)} I\\left(y^{(n)} \\neq f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)\\right) . \\tag{10.18}\n\\end{align*}\n\n\n从上式可以看出, 当 $\\alpha_{m}>0$ 时, 最优的分类器 $f_{m}(\\boldsymbol{x})$ 为使得在样本权重为 $w_{m}^{(n)}, 1 \\leq n \\leq N$ 时的加权错误率最小的分类器.\n\n在求解出 $f_{m}(\\boldsymbol{x})$ 之后, 公式(10.15)可以写为\n\n\n\\begin{align*}\n\\mathcal{L}\\left(\\alpha_{m}, f_{m}(\\boldsymbol{x})\\right) & =\\sum_{y^{(n)}=f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)} w_{m}^{(n)} \\exp \\left(-\\alpha_{m}\\right)+\\sum_{y^{(n)} \\neq f_{m}\\left(\\boldsymbol{x}^{(n)}\\right)} w_{m}^{(n)} \\exp \\left(\\alpha_{m}\\right)  \\tag{10.19}\\\\\n& \\propto\\left(1-\\epsilon_{m}\\right) \\exp \\left(-\\alpha_{m}\\right)+\\epsilon_{m} \\exp \\left(\\alpha_{m}\\right), \\tag{10.20}\n\\end{align*}\n\n\n其中 $\\epsilon_{m}$ 为分类器 $f_{m}(\\boldsymbol{x})$ 的加权错误率,\n\n\n\\begin{equation*}\n\\epsilon_{m}=\\frac{\\sum_{y^{(n)} \\neq f_{m}\\left(x^{(n)}\\right)} w_{m}^{(n)}}{\\sum_{n} w_{m}^{(n)}} \\tag{10.21}\n\\end{equation*}\n\n\n求上式关于 $\\alpha_{m}$ 的导数并令其为 0 , 得到\n\n\n\\begin{equation*}\n\\alpha_{m}=\\frac{1}{2} \\log \\frac{1-\\epsilon_{m}}{\\epsilon_{m}} \\tag{10.22}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\\section*{10.2 自训练和协同训练}\n监督学习往往需要大量的标注数据, 而标注数据的成本比较高. 因此, 利用大量的无标注数据来提高监督学习的效果有着十分重要的意义. 这种利用少量标注数据和大量无标注数据进行学习的方式称为半监督学习 (Semi-Supervised Learning,SSL ) . 本节介绍两种半监督学习算法: 自训练和协同训练."
    },
    {
        "id": "chatcmpl-92yk8y7HdANLhW0uhtojqkcDmI20s",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"Boosting 类集成模型的加性模型 $F(\\boldsymbol{x})$ 是如何定义的？\",\n    \"回答1\": \"$F(\\boldsymbol{x})$ 被定义为 $F(\\boldsymbol{x})=\\text{∑}_{m=1}^{M} \\boldsymbol{\\text{α}}_{m} f_{m}(\\boldsymbol{x})$，其中 $f_{m}(\\boldsymbol{x})$ 为弱分类器（Weak Classifier）或基分类器（Base Classifier），$\\boldsymbol{\\text{α}}_{m}$ 为弱分类器的集成权重，$F(\\boldsymbol{x})$ 称为强分类器（Strong Classifier）。\",\n    \"问题2\": \"Boosting 类方法中如何训练每个弱分类器 $f_{m}(\\boldsymbol{x})$ 及其权重 $\\boldsymbol{\\text{α}}_{m}$？\",\n    \"回答2\": \"为了提高集成的效果, 应当尽量使得每个弱分类器的差异尽可能大。一种有效的算法是采用迭代的方法来学习每个弱分类器，即按照一定的顺序依次训练每个弱分类器。在训练第 $m+1$ 个弱分类器时，增加已有弱分类器分错样本的权重，使得第 $m+1$ 个弱分类器“更关注”于已有弱分类器分错的样本。这样增加每个弱分类器的差异，最终提升集成分类器的准确率。\",\n    \"问题3\": \"AdaBoost 算法的训练过程是怎样的？\",\n    \"回答3\": \"以二分类为例，AdaBoost 算法的训练过程如下：首先赋予每个样本同样的权重。在每一轮迭代中，根据当前的样本权重训练一个新的弱分类器。然后根据这个弱分类器的错误率来计算其集成权重，并调整样本权重。最终输出一个强分类器 $F(\\boldsymbol{x})$。\",\n    \"问题4\": \"AdaBoost 算法的统计学解释是什么？\",\n    \"回答4\": \"AdaBoost 算法也可以看作一种分步（Stage-Wise）优化的加性模型，其损失函数定义为 $\\boldsymbol{\\text{L}}(F)=\\text{exp}(-y F(\\boldsymbol{x}))$。在每一轮迭代中，目标是找到一个 $\\boldsymbol{\\text{α}}_{m}$ 和 $f_{m}(\\boldsymbol{x})$ 使得损失函数最小。这涉及到调整样本权重和计算加权错误率等步骤。\",\n    \"问题5\": \"自训练和协同训练是什么？\",\n    \"回答5\": \"自训练和协同训练是两种半监督学习算法。自训练利用少量标注数据和大量无标注数据进行学习，而协同训练则是一种利用多个模型相互协作进行学习的方法。\",\n    \"问题6\": \"什么是半监督学习？\",\n    \"回答6\": \"半监督学习是利用少量标注数据和大量无标注数据进行学习的方式。它旨在通过无标注数据来提高监督学习的效果，以降低标注数据的成本。\",\n    \"问题7\": \"为什么半监督学习具有重要意义？\",\n    \"回答7\": \"监督学习往往需要大量的标注数据，而标注数据的成本比较高。因此，利用大量的无标注数据来提高监督学习的效果具有十分重要的意义。半监督学习能够充分利用无标注数据，提高模型的泛化能力和表达能力。\",\n    \"问题8\": \"损失函数 $\\boldsymbol{\\text{L}}(F)$ 的统计学解释是什么？\",\n    \"回答8\": \"损失函数 $\\boldsymbol{\\text{L}}(F)$ 的统计学解释涉及到对加性模型的优化过程，其中需要找到使损失函数最小化的 $\\boldsymbol{\\text{α}}_{m}$ 和 $f_{m}(\\boldsymbol{x})$。这一过程涉及到样本权重、加权错误率等统计学概念。\",\n    \"问题9\": \"在 AdaBoost 算法中，如何调整样本权重？\",\n    \"回答9\": \"在 AdaBoost 算法中，调整样本权重的过程是通过公式 $w_{m+1}^{(n)} \\text{←} w_{m}^{(n)} \\text{exp}(-\\boldsymbol{\\text{α}}_{m} y^{(n)} f_{m}(\\boldsymbol{x}^{(n)})$ 实现的。这一过程根据当前弱分类器的表现来调整样本权重，以便下一轮迭代能够更好地关注分错的样本。\",\n    \"问题10\": \"为什么需要使得每个弱分类器的差异尽可能大？\",\n    \"回答10\": \"为了提高集成的效果，应当尽量使得每个弱分类器的差异尽可能大。这样可以增加集成模型对复杂模式和特征的捕捉能力，从而提升集成分类器的准确率和泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710497180,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1494,
            "prompt_tokens": 5067,
            "total_tokens": 6561
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.1,
            "presence_penalty": 0.1
        }
    }
]