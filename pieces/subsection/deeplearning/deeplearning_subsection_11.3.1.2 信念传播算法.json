[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "11.3.1.2 信念传播算法",
        "content_length": 3399,
        "content": "Title: 11.3.1.2 信念传播算法\nContent:\n信念传播 ( Belief Propagation, BP ) 算法, 也称为和积 (Sum-Product)算法或消息传递（Message Passing）算法，是将变量消除法中的和积（SumProduct ) 操作看作消息 (Message), 并保存起来, 这样可以节省大量的计算资源.\n\n我们先介绍链式结构上的信念传播算法.\n\n\n\n本节以无向图为例来介绍信念传播算法, 但其同样适用于有向图.\n\n图 11.14 无向马尔可夫链的消息传递过程\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n以图11.14所示的无向马尔可夫链为例, 其联合概率 $p(\\boldsymbol{x})$ 为\n\n\n\\begin{align*}\np(\\boldsymbol{x}) & =\\frac{1}{Z} \\prod_{c \\in \\mathcal{C}} \\phi_{c}\\left(\\boldsymbol{x}_{c}\\right)  \\tag{11.73}\\\\\n& =\\frac{1}{Z} \\prod_{t=1}^{T-1} \\phi\\left(x_{t}, x_{t+1}\\right) \\tag{11.74}\n\\end{align*}\n\n\n其中 $\\boldsymbol{\\phi}\\left(x_{t}, x_{t+1}\\right)$ 是定义在团 $\\left(x_{t}, x_{t+1}\\right)$ 上的势能函数.\n\n第 $t$ 个变量的边际概率 $p\\left(x_{t}\\right)$ 为\n\n\n\\begin{align*}\np\\left(x_{t}\\right) & =\\sum_{x_{1}} \\cdots \\sum_{x_{t-1}} \\sum_{x_{t+1}} \\cdots \\sum_{x_{T}} p(\\boldsymbol{x})  \\tag{11.75}\\\\\n& =\\frac{1}{Z} \\sum_{x_{1}} \\cdots \\sum_{x_{t-1}} \\sum_{x_{t+1}} \\cdots \\sum_{x_{T}} \\prod_{t=1}^{T-1} \\phi\\left(x_{t}, x_{t+1}\\right) . \\tag{11.76}\n\\end{align*}\n\n\n假设每个变量取 $K$ 个值, 不考虑归一化项, 通过公式 (11.76) 计算边际分布需要 $K^{T-1}$ 次加法以及 $K^{T-1} \\times(T-1)$ 次乘法.\n\n根据乘法的分配律,边际概率 $p\\left(x_{t}\\right)$ 可以通过下面方式进行计算:\n\n\n\\begin{align*}\np\\left(x_{t}\\right)= & \\frac{1}{Z}\\left(\\sum_{x_{1}} \\cdots \\sum_{x_{t-1}} \\prod_{j=1}^{t-1} \\phi\\left(x_{j}, x_{j+1}\\right)\\right) \\cdot\\left(\\sum_{x_{t+1}} \\cdots \\sum_{x_{T}} \\prod_{j=t}^{T-1} \\phi\\left(x_{j}, x_{j+1}\\right)\\right) \\\\\n= & \\frac{1}{Z}\\left(\\sum_{x_{t-1}} \\phi\\left(x_{t-1}, x_{t}\\right) \\cdots\\left(\\sum_{x_{2}} \\phi\\left(x_{2}, x_{3}\\right)\\left(\\sum_{x_{1}} \\phi\\left(x_{1}, x_{2}\\right)\\right)\\right)\\right) . \\\\\n& \\left(\\sum_{x_{t+1}} \\phi\\left(x_{t}, x_{t+1}\\right) \\cdots\\left(\\sum_{x_{T-1}} \\phi\\left(x_{T-2}, x_{T-1}\\right)\\left(\\sum_{x_{T}} \\phi\\left(x_{T-1}, x_{T}\\right)\\right)\\right)\\right) \\\\\n= & \\frac{1}{Z} \\mu_{t-1, t}\\left(x_{t}\\right) \\mu_{t+1, t}\\left(x_{t}\\right), \\tag{11.77}\n\\end{align*}\n\n\n其中 $\\mu_{t-1, t}\\left(x_{t}\\right)$ 定义为变量 $X_{t-1}$ 向变量 $X_{t}$ 传递的消息, 定义为\n\n\n\\begin{equation*}\n\\mu_{t-1, t}\\left(x_{t}\\right) \\triangleq \\sum_{x_{t-1}} \\phi\\left(x_{t-1}, x_{t}\\right) \\mu_{t-2, t-1}\\left(x_{t-1}\\right) \\tag{11.78}\n\\end{equation*}\n\n\n$\\mu_{t+1, t}\\left(x_{t}\\right)$ 是变量 $X_{t+1}$ 向变量 $X_{t}$ 传递的消息, 定义为\n\n\n\\begin{equation*}\n\\mu_{t+1, t}\\left(x_{t}\\right) \\triangleq \\sum_{x_{t+1}} \\phi\\left(x_{t}, x_{t+1}\\right) \\mu_{t+2, t+1}\\left(x_{t+1}\\right) . \\tag{11.79}\n\\end{equation*}\n\n\n$\\mu_{t-1, t}\\left(x_{t}\\right)$ 和 $\\mu_{t+1, t}\\left(x_{t}\\right)$ 都可以递归计算. 因此, 边际概率 $p\\left(x_{t}\\right)$ 的计算复杂度减少为 $O\\left(T K^{2}\\right)$. 如果要计算整个序列上所有变量的边际概率, 不需要将消息传递的过程重复 $T$ 次,因为其中每两个相邻节点上的消息是相同的.\n\n链式结构图模型的消息传递过程为:\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n（1）依次计算前向传递的消息 $\\mu_{t-1, t}\\left(x_{t}\\right), t=1, \\cdots, T-1$.\n\n（2）依次计算反向传递的消息 $\\mu_{t+1, t}\\left(x_{t}\\right), t=T-1, \\cdots, 1$.\n\n(3) 在任意节点 $t$ 上计算配分函数 $Z$,\n\n\n\\begin{equation*}\nZ=\\sum_{x_{t}} \\mu_{t-1, t}\\left(x_{t}\\right) \\mu_{t+1, t}\\left(x_{t}\\right) \\tag{11.80}\n\\end{equation*}\n\n\n这样,我们可以通过公式(11.77)计算所有变量的边际概率.\n\n树结构上的信念传播算法 信念传播算法也可以推广到具有树结构的图模型上. 如果一个有向图满足任意两个变量只有一条路径 (忽略方向), 且只有一个没有父节点的节点, 那么这个有向图为树结构, 其中唯一没有父节点的节点称为根节点. 如果一个无向图满足任意两个变量只有一条路径, 那么这个无向图也为树结构. 在树结构的无向图中,任意一个节点都可以作为根节点.\n\n树结构图模型的消息传递过程为:\n\n（1）从叶子节点到根节点依次计算并传递消息.\n\n（2）从根节点开始到叶子节点,依次计算并传递消息.\n\n（3）在每个节点上计算所有接收消息的乘积（如果是无向图还需要归一化), 就得到了所有变量的边际概率.\n\n如果图结构中存在环路, 可以使用联合树算法 (Junction Tree Algorithm ) [Lauritzen et al., 1988] 来将图结构转换为无环图."
    },
    {
        "id": "chatcmpl-9457lEAbgYUwRepNsskJiTJmhGqxx",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"信念传播算法和变量消除法有何区别？\",\n    \"回答1\": \"信念传播算法将变量消除法中的和积操作看作消息，并保存起来，从而节省大量的计算资源。相比之下，变量消除法需要进行指数级别的加法和乘法运算，而信念传播算法通过消息传递的方式降低了计算复杂度。\",\n    \"问题2\": \"在无向图中，信念传播算法的消息传递过程是如何进行的？\",\n    \"回答2\": \"在无向图中，信念传播算法的消息传递过程包括依次计算前向传递的消息 $\\boldsymbol{\\text{μ}}_{t-1, t}\\big(x_{t}\\big), t=1, \\text{...}, T-1$ 和反向传递的消息 $\\boldsymbol{\\text{μ}}_{t+1, t}\\big(x_{t}\\big), t=T-1, \\text{...}, 1$。然后在任意节点 $t$ 上计算配分函数 $Z$，通过这一过程可以计算所有变量的边际概率。\",\n    \"问题3\": \"在信念传播算法中，如何计算边际概率 $p\\big(x_{t}\\big)$ 的复杂度被降低到 $O\\big(T K^{2}\\big)$？\",\n    \"回答3\": \"在信念传播算法中，边际概率 $p\\big(x_{t}\\big)$ 的计算复杂度被降低到 $O\\big(T K^{2}\\big)$ 是通过递归计算消息 $\\boldsymbol{\\text{μ}}_{t-1, t}\\big(x_{t}\\big)$ 和 $\\boldsymbol{\\text{μ}}_{t+1, t}\\big(x_{t}\\big)$ 实现的。这使得边际概率的计算复杂度大大降低。\",\n    \"问题4\": \"树结构图模型上的信念传播算法的消息传递过程是怎样的？\",\n    \"回答4\": \"在树结构图模型上，信念传播算法的消息传递过程包括从叶子节点到根节点依次计算并传递消息，然后从根节点开始到叶子节点依次计算并传递消息。最后，在每个节点上计算所有接收消息的乘积（如果是无向图还需要归一化），就得到了所有变量的边际概率。\",\n    \"问题5\": \"如何将具有环路的图结构转换为无环图以应用信念传播算法？\",\n    \"回答5\": \"如果图结构中存在环路，可以使用联合树算法 (Junction Tree Algorithm) 来将图结构转换为无环图。这样就能够应用信念传播算法进行高效的推断和计算。\",\n    \"问题6\": \"信念传播算法相对于其他推断方法有哪些优势？\",\n    \"回答6\": \"相对于其他推断方法，信念传播算法具有较低的计算复杂度和高效的消息传递机制。它能够有效地处理大规模图模型，并且在树结构和无向图模型上表现出良好的性能。\",\n    \"问题7\": \"在信念传播算法中，什么是配分函数 $Z$ 的作用？\",\n    \"回答7\": \"在信念传播算法中，配分函数 $Z$ 的作用是用于计算所有变量的边际概率。它通过对前向和后向传递的消息进行乘积来实现对边际概率的计算。\",\n    \"问题8\": \"无向图和有向图在信念传播算法中有何异同？\",\n    \"回答8\": \"信念传播算法同样适用于无向图和有向图。在无向图中，它通过消息传递实现对边际概率的计算；而在有向图中，也可以通过类似的消息传递方式实现对边际概率的推断。\",\n    \"问题9\": \"信念传播算法如何节省计算资源？\",\n    \"回答9\": \"信念传播算法将变量消除法中的和积操作看作消息，并保存起来，从而节省大量的计算资源。它通过高效的消息传递机制和复杂度降低的计算方式实现了对边际概率的高效推断。\",\n    \"问题10\": \"在信念传播算法中，如何实现对所有变量边际概率的高效计算？\",\n    \"回答10\": \"在信念传播算法中，通过前向和后向消息传递以及配分函数 $Z$ 的计算，可以高效地实现对所有变量边际概率的计算。这种高效计算方式使得信念传播算法在大规模图模型上具有良好的性能和应用价值。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710760037,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1378,
            "prompt_tokens": 4595,
            "total_tokens": 5973
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]