[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "1.5.3 神经网络的发展历史",
        "content_length": 13770,
        "content": "Title: 1.5.3 神经网络的发展历史\nContent:\n神经网络的发展大致经过五个阶段.\n\n第一阶段: 模型提出 第一阶段为 1943 年 1969 年, 是神经网络发展的第一个高潮期. 在此期间,科学家们提出了许多神经元模型和学习规则.\n\n1943 年, 心理学家 Warren McCulloch 和数学家 Walter Pitts 最早提出了一种基于简单逻辑运算的人工神经网络, 这种神经网络模型称为 MP 模型, 至此开启了人工神经网络研究的序幕. 1948 年, Alan Turing 提出了一种“B型图灵机”. “B型图灵机” 可以基于Hebbian 法则来进行学习. 1951年, McCulloch 和 Pitts 的学生 Marvin Minsky 建造了第一台神经网络机 SNARC. [Rosenblatt, 1958] 提出了一种可以模拟人类感知能力的神经网络模型, 称为感知器 (Perceptron ), 并提出了一种接近于人类学习过程 (迭代、试错) 的学习算法.\n\n在这一时期,神经网络以其独特的结构和处理信息的方法,在许多实际应用领域 (自动控制、模式识别等) 中取得了显著的成效.\n\n第二阶段:冰河期 第二阶段为 1969 年 1983 年, 是神经网络发展的第一个低谷期. 在此期间,神经网络的研究处于长年停滞及低潮状态.\n\n1969 年, Marvin Minsky 出版《感知器》一书, 指出了神经网络的两个关键缺陷: 一是感知器无法处理 “异或” 回路问题; 二是当时的计算机无法支持处理大感知器参见第3.4节.\n\n在本书中, 人工神经网络主要是作为一种映射函数, 即机器学习中的模型.\n\nHebbian 法则参见第8.6.1节.\n\nMarvin Minsky ( 1927 2016 ), 人工智能领域最重要的领导者和创新者之一, 麻省理工学院人工智能实验室的创始人之一。因其在人工智能领域的贡献, 于 1969 年获得图灵奖.\\\\\n型神经网络所需要的计算能力. 这些论断使得人们对以感知器为代表的神经网络产生质疑,并导致神经网络的研究进入了十多年的“冰河期”.\n\n但在这一时期，依然有不少学者提出了很多有用的模型或算法. 1974 年,哈佛大学的 Paul Werbos 发明反向传播算法 (BackPropagation, BP) [Werbos, 1974], 但当时未受到应有的重视. 1980 年, 福岛邦彦提出了一种带卷积和子采样操作的多层神经网络: 新知机 (Neocognitron) [Fukushima, 1980]. 新知机的提出是受到了动物初级视皮层简单细胞和复杂细胞的感受野的启发. 但新知机并没有采用反向传播算法, 而是采用了无监督学习的方式来训练, 因此也没有引起足够的重视.\n\n第三阶段: 反向传播算法引起的复兴 第三阶段为 1983 年 1995 年, 是神经网络发展的第二个高潮期. 这个时期中, 反向传播算法重新激发了人们对神经网络的兴趣.\n\n1983 年, 物理学家 John Hopfield 提出了一种用于联想记忆 (Associative Memory) 的神经网络, 称为Hopfield 网络. Hopfield 网络在旅行商问题上取得了当时最好结果, 并引起了轰动. 1984 年, Geoffrey Hinton 提出一种随机化版本的 Hopfield 网络,即玻尔兹曼机（Boltzmann Machine）.\n\n真正引起神经网络第二次研究高潮的是反向传播算法. 20 世纪 80 年代中期, 一种连接主义模型开始流行, 即分布式并行处理 ( Parallel Distributed Processing, PDP ) 模型 [McClelland et al., 1986]. 反向传播算法也逐渐成为 PDP 模型的主要学习算法. 这时, 神经网络才又开始引起人们的注意, 并重新成为新的研究热点. 随后, [LeCun et al., 1989] 将反向传播算法引入了卷积神经网络, 并在手写体数字识别上取得了很大的成功 [LeCun et al., 1998]. 反向传播算法是迄今最为成功的神经网络学习算法. 目前在深度学习中主要使用的自动微分可以看作反向传播算法的一种扩展.\n\n然而, 梯度消失问题 (Vanishing Gradient Problem) 阻碍神经网络的进一步发展, 特别是循环神经网络. 为了解决这个问题, [Schmidhuber, 1992] 采用两步来训练一个多层的循环神经网络:1) 通过无监督学习的方式来逐层训练每一层循环神经网络,即预测下一个输入;2) 通过反向传播算法进行精调.\n\n第四阶段: 流行度降低 第四阶段为 1995 年 2006 年, 在此期间, 支持向量机和其他更简单的方法 (例如线性分类器) 在机器学习领域的流行度逐渐超过了神经网络.\n\n虽然神经网络可以很容易地增加层数、神经元数量, 从而构建复杂的网络,但其计算复杂性也会随之增长. 当时的计算机性能和数据规模不足以支持训练大规模神经网络. 在 20 世纪 90 年代中期, 统计学习理论和以支持向量机为代表的机器学习模型开始兴起. 相比之下, 神经网络的理论基础不清晰、优化困难、可 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\nHopfield 网络参见第8.6.1节。玻尔兹曼机参见第12.1节。\n\n自动微分参见第4.5.3节。\\\\\n解释性差等缺点更加凸显, 因此神经网络的研究又一次陷入低潮.\n\n第五阶段: 深度学习的崛起 第五阶段为从 2006 年开始至今, 在这一时期研究者逐渐掌握了训练深层神经网络的方法, 使得神经网络重新崛起.\n\n[Hinton et al., 2006] 通过逐层预训练来学习一个深度信念网络, 并将其权重作为一个多层前馈神经网络的初始化权重, 再用反向传播算法进行精调. 这种 “预训练 + 精调” 的方式可以有效地解决深度神经网络难以训练的问题. 随着深度神经网络在语音识别 [Hinton et al., 2012] 和图像分类 [Krizhevsky et al., 2012]等任务上的巨大成功, 以神经网络为基础的深度学习迅速崛起. 近年来, 随着大规模并行计算以及 GPU 设备的普及, 计算机的计算能力得以大幅提高. 此外, 可供机器学习的数据规模也越来越大. 在强大的计算能力和海量的数据规模支持下, 计算机已经可以端到端地训练一个大规模神经网络, 不再需要借助预训练的方式. 各大科技公司都投入巨资研究深度学习, 神经网络迎来第三次高潮.\n\n\\section*{1.6 本书的知识体系}\n本书主要对神经网络和深度学习所涉及的知识提出一个较全面的基础性介绍. 本书的知识体系如图1.6所示, 可以分为三大块: 机器学习、神经网络和概率图模型.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_b7f820bc184c7846d78fg-031}\n\\end{center}\n\n图 1.6 本书的知识体系\n\n本书的知识体系在各章节中的安排如下:\n\n机器学习 机器学习可以分为监督学习、无监督学习和强化学习. 第 2 章对机器学习进行概述, 使读者能够了解机器学习的基本概念以及三要素: 模型、学习准 \\href{https://nndl.github.io/%E6%B7%B1%E5%BA%A6%E4%BF%A1%E5%BF%B5%E7%BD%91%E7%BB%9C%E5%8F%82%E8%A7%81%E7%AC%AC12.3%E8%8A%82%E3%80%82}{https://nndl.github.io/深度信念网络参见第12.3节。}\\\\\n则和优化算法, 并以线性回归为例来讲述不同学习算法之间的关联. 第 3 章主要介绍一些基本的线性模型. 这两章都以监督学习为主进行介绍. 第 9 章介绍了一些无监督学习方法, 包括无监督特征学习和概率密度估计. 第 10 章中介绍了一些和模型无关的机器学习方法. 第14章介绍了深度强化学习的知识.\n\n神经网络 神经网络作为一类非线性的机器学习模型, 可以更好地实现输入和输出之间的映射. 第 4 章到第 6 章分别讲述三种主要的神经网络模型: 前馈神经网络、卷积神经网络和循环神经网络. 第6章也简单介绍了一种更一般性的网络: 图网络. 第 7 章介绍神经网络的优化与正则化方法. 第 8 章介绍神经网络中的注意力机制和外部记忆.\n\n概率图模型 概率图模型为机器学习提供了一个更加便捷的描述框架. 第11章介绍了概率图模型的基本概念, 包括模型表示、学习和推断. 目前深度学习和概率图模型的融合已经十分流行. 第12章介绍了两种概率图模型: 玻尔兹曼机和深度信念网络. 第13章和第15章分别介绍两种概率生成模型: 深度生成模型和序列生成模型.\n\n由于深度学习涉及非常多的研究领域, 因此很多知识无法进行追根溯源并深入介绍. 每章最后一节都提供了一些参考文献, 读者可根据需要通过深入阅读来了解这些知识. 此外, 本书的附录中介绍了一些深度学习涉及的数学知识, 包括线性代数、微积分、数学优化、概率论和信息论等.\n\n\\section*{1.7 常用的深度学习框架}\n在深度学习中, 一般通过误差反向传播算法来进行参数学习. 采用手工方式来计算梯度再写代码实现的方式会非常低效, 并且容易出错. 此外, 深度学习模型需要的计算机资源比较多,一般需要在 CPU 和 GPU 之间不断进行切换, 开发难度也比较大. 因此,一些支持自动梯度计算、无缝 CPU 和 GPU 切换等功能的深度学习框架就应运而生. 比较有代表性的框架包括: Theano、Caffe、TensorFlow、 Pytorch、飞桨 ( PaddlePaddle)、Chainer和 MXNet 等 ${ }^{1}$.\n\n( 1) Theano ${ }^{2}$ : 由蒙特利尔大学的 Python 工具包, 用来高效地定义、优化和计算张量数据的数学表达式. Theano 可以透明地使用 GPU 和高效的符号微分.\n\n(2) $\\mathrm{Caffe}^{3}$ : 由加州大学伯克利分校开发的针对卷积神经网络的计算框架,\n\\footnotetext{1 更全面的深度学习框架介绍可以参考https://en.wikipedia.org/wiki/Comparison\\_of\\_deep\\_ learning\\_software.\n\n$2 \\mathrm{http} / /$ \\href{http://www.deeplearning.net/software/theano}{www.deeplearning.net/software/theano}\n\n3 全称为 Convolutional Architecture for Fast Feature Embedding, \\href{http://caffe.berkeleyvision.org}{http://caffe.berkeleyvision.org} \\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n\n虽然这里将神经网络结构大体上分为三种类型, 但是大多数网络都是复合型结构, 即一个神经网络中包括多种网络结构。\n\n自动梯度计算参见第4.5节。\n\nTheano 项目目前已停止维护.\\\\\n主要用于计算机视觉. Caffe 用 $\\mathrm{C}++$ 和 Python 实现, 但可以通过配置文件来实现所要的网络结构, 不需要编码.\n\n(3) TensorFlow ${ }^{1}$ : 由 Google 公司开发的深度学习框架, 可以在任意具备 CPU 或者 GPU 的设备上运行. TensorFlow 的计算过程使用数据流图来表示. TensorFlow 的名字来源于其计算过程中的操作对象为多维数组, 即张量 (Tensor ). TensorFlow 1.0 版本采用静态计算图, 2.0 版本之后也支持动态计算图.\n\n(4) PyTorch ${ }^{2}$ : 由 Facebook、NVIDIA、Twitter 等公司开发维护的深度学习框架, 其前身为 Lua 语言的 Torch ${ }^{3}$. PyTorch 也是基于动态计算图的框架, 在需要动态改变神经网络结构的任务中有着明显的优势.\n\n( 5 ) 飞桨 (PaddlePaddle $)^{4}$ : 由百度开发的一个高效和可扩展的深度学习框架, 同时支持动态图和静态图. 飞桨提供强大的深度学习并行技术, 可以同时支持稠密参数和稀疏参数场景的超大规模深度学习并行训练, 支持千亿规模参数和数百个节点的高效并行训练.\n\n(6) MindSpore ${ }^{5}$ : 由华为开发的一种适用于端边云场景的新型深度学习训练/推理框架. MindSpore 为 Ascend AI 处理器提供原生支持, 以及软硬件协同优化.\n\n( 7 ) Chainer ${ }^{6}$ : 一个最早采用动态计算图的深度学习框架, 其核心开发团队为来自日本的一家机器学习创业公司 Preferred Networks. 和 Tensorflow、 Theano、Caffe等框架使用的静态计算图相比, 动态计算图可以在运行时动态地构建计算图,因此非常适合进行一些复杂的决策或推理任务.\n\n（8 ) $\\mathrm{MXNet}^{7}$ : 由亚马逊、华盛顿大学和卡内基・梅隆大学等开发维护的深度学习框架. MXNet 支持混合使用符号和命令式编程来最大化效率和生产率,并可以有效地扩展到多个GPU 和多台机器.\n\n在这些基础框架之上, 还有一些建立在这些框架之上的高度模块化的神经网络库, 使得构建一个神经网络模型就像搭积木一样容易. 其中比较有名的模块化神经网络框架有: 1) 基于 TensorFlow 和 Theano 的 $\\mathrm{Keras}^{8} ; 2$ ）基于 Theano 的 Lasagne $\\left.{ }^{9} ; 3\\right)$ 面向图结构数据的 $\\mathrm{DGL}^{10}$.\n\\footnotetext{1 \\href{https://www.tensorflow.org}{https://www.tensorflow.org}\n\n2 \\href{http://pytorch.org}{http://pytorch.org}\n\n3 \\href{http://torch.ch}{http://torch.ch}\n\n4 Parallel Distributed Deep Learning,\\href{http://paddlepaddle.org/}{http://paddlepaddle.org/}\n\n5 \\href{https://www.mindspore.cn/}{https://www.mindspore.cn/}\n\n6 \\href{https://chainer.org}{https://chainer.org}\n\n7 \\href{https://mxnet.apache.org}{https://mxnet.apache.org}\n\n8 \\href{http://keras.io/}{http://keras.io/}\n\n9 \\href{https://github.com/Lasagne/Lasagne}{https://github.com/Lasagne/Lasagne}\n\n${ }^{10}$ Deep Graph Library, 支持 PyTorch、MXNet 和 TensorFlow, \\href{https://www.dgl.ai/}{https://www.dgl.ai/}. \\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n\n\\section*{1.8 总结和深入阅读}\n要理解深度学习的意义或重要性, 就得从机器学习或者是人工智能的更广的视角来分析. 在传统机器学习中, 除了模型和学习算法外, 特征或表示也是影响最终学习效果的重要因素, 甚至在很多的任务上比算法更重要. 因此, 要开发一个实际的机器学习系统, 人们往往需要花费大量的精力去尝试设计不同的特征以及特征组合, 来提高最终的系统能力,这就是所谓的特征工程问题.\n\n如何自动学习有效的数据表示成为机器学习中的关键问题. 早期的表示学习方法, 比如特征抽取和特征选择, 都是人工引入一些主观假设来进行学习的.这种表示学习不是端到端的学习方式, 得到的表示不一定对后续的机器学习任务有效. 而深度学习是将表示学习和预测模型的学习进行端到端的学习, 中间不需要人工干预. 深度学习所要解决的问题是贡献度分配问题, 而神经网络恰好是解决这个问题的有效模型. 套用马克思的一句名言 , “金银天然不是货币, 但货币天然是金银”, 我们可以说, 神经网络天然不是深度学习, 但深度学习天然是神经网络.\n\n目前, 深度学习主要以神经网络模型为基础, 研究如何设计模型结构, 如何有效地学习模型的参数, 如何优化模型性能以及在不同任务上的应用等. [Bengio et al., 2013] 给出了一个很好的表示学习综述. 若希望全面了解人工神经网络和深度学习的知识, 可以参考《Deep Learning》[Goodfellow et al., 2016] 以及文献 [Bengio, 2009]. 关于神经网络的历史可以参考文献 [Anderson et al., 2000].斯坦福大学的 CS231 $n^{1}$ 和 CS224n 2 是两门非常好的深度学习入门课程, 分别从计算机视觉和自然语言处理两个角度来讲授深度学习的基础知识和最新进展.\n\n深度学习的研究进展非常迅速. 因此, 最新的文献一般会发表在学术会议上. 和深度学习相关的学术会议主要有:\n\n（1）国际表示学习会议 ${ }^{3}$ (International Conference on Learning Representations, ICLR ): 主要聚焦于深度学习.\n\n(2) 神经信息处理系统年会4 (Annual Conference on Neural Information Processing Systems, NeurIPS ): 交叉学科会议, 但偏重于机器学习. 主要包括神经信息处理、统计方法、学习理论以及应用等.\n\n(3) 国际机器学习会议 5 (International Conference on Machine Learning, ICML ) : 机器学习顶级会议, 深度学习作为近年来的热点, 也占据了 ICML\n\n1 \\href{http://cs231n.stanford.edu}{http://cs231n.stanford.edu}\n\n2 \\href{http://web.stanford.edu/class/cs224n/}{http://web.stanford.edu/class/cs224n/}\n\n3 \\href{http://www.iclr.cc}{http://www.iclr.cc}\n\n4 \\href{https://nips.cc}{https://nips.cc}\n\n5 \\href{https://icml.cc}{https://icml.cc}\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\\section*{的很大比例.}\n（4）国际人工智能联合会议 ${ }^{1}$ (International Joint Conference on Artificial Intelligence, IJCAI ) : 人工智能领域最顶尖的综合性会议. 历史悠久, 从 1969 年开始举办.\n\n（5）美国人工智能协会年会 ${ }^{2}$ (AAAI Conference on Artificial Intelligence, AAAI ) : 人工智能领域的顶级会议,每年二月份左右召开,地点一般在北美.\n\n另外, 人工智能的很多子领域也都有非常好的专业学术会议. 在计算机视觉领域, 有计算机视觉与模式识别大会 (IEEE Conference on Computer Vision and Pattern Recognition, CVPR) 和国际计算机视觉会议 (International Comference on Computer Vision, ICCV ). 在自然语言处理领域, 有计算语言学年会 (Annual Meeting of the Association for Computational Linguistics, ACL)和自然语言处理实证方法大会 (Conference on Empirical Methods in Natural Language Processing, EMNLP) 等.\n\n\\section*{第 2 章 机器学习概述}\n机器学习是对能通过经验自动改进的计算机算法的研究.\n\n——汤姆・米切尔 (Tom Mitchell ) [Mitchell, 1997]\n\n通俗地讲, 机器学习 (Machine Learning, ML) 就是让计算机从数据中进行自动学习, 得到某种知识 (或规律). 作为一门学科, 机器学习通常指一类问题以及解决这类问题的方法, 即如何从观测数据 (样本) 中寻找规律, 并利用学习到的规律 (模型) 对未知或无法观测的数据进行预测.\n\n在早期的工程领域, 机器学习也经常称为模式识别 ( Pattern Recognition, PR ), 但模式识别更偏向于具体的应用任务, 比如光学字符识别、语音识别、人脸识别等. 这些任务的特点是, 对于我们人类而言, 这些任务很容易完成, 但我们不知道自己是如何做到的, 因此也很难人工设计一个计算机程序来完成这些任务.一个可行的方法是设计一个算法可以让计算机自己从有标注的样本上学习其中的规律, 并用来完成各种识别任务. 随着机器学习技术的应用越来越广, 现在机器学习的概念逐渐替代模式识别,成为这一类问题及其解决方法的统称.\n\n以手写体数字识别为例, 我们需要让计算机能自动识别手写的数字. 比如图 2.1 中的例子, 将 5 识别为数字 5 , 将 6 识别为数字 6 . 手写数字识别是一个经典的机器学习任务, 对人来说很简单, 但对计算机来说却十分困难. 我们很难总结每个数字的手写体特征, 或者区分不同数字的规则, 因此设计一套识别算法是一项几乎不可能的任务. 在现实生活中, 很多问题都类似于手写体数字识别这类问题, 比如物体识别、语音识别等. 对于这类问题, 我们不知道如何设计一个计算机程序来解决, 即使可以通过一些启发式规则来实现, 其过程也是极其复杂的. 因此, 人们开始尝试采用另一种思路, 即让计算机 “看” 大量的样本, 并从中学习到一些经验, 然后用这些经验来识别新的样本. 要识别手写体数字, 首先通过人工标注大量的手写体数字图像 (即每张图像都通过人工标记了它是什么数字), 这些图像作为训练数据, 然后通过学习算法自动生成一套模型, 并依靠它来识别新\\\\\n的手写体数字. 这个过程和人类学习过程也比较类似, 我们教小孩子识别数字也是这样的过程. 这种通过数据来学习的方法就称为机器学习的方法.\n\n\\begin{center}\n\\begin{tabular}{lllllllllllllllllllll}\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 \\\\\n3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 3 \\\\\n4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 \\\\\n5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\\\\n6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 & 6 \\\\\n7 & 7 & 7 & 7 & 7 & 7 & 7 & 77 & 7 & 7 & 7 & 7 & 7 & 7 & 7 & 7 & 7 & 7 & 7 & 7 \\\\\n8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 \\\\\n9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 & 9 \\\\\n\\end{tabular}\n\\end{center}\n\n图 2.1 手写体数字识别示例 ( 图片来源: [LeCun et al., 1998])\n\n本章先介绍机器学习的基本概念和基本要素, 并较详细地描述一个简单的机器学习例子——线性回归.\n\n\\section*{2.1 基本概念}\n首先我们以一个生活中的例子来介绍机器学习中的一些基本概念: 样本、特征、标签、模型、学习算法等. 假设我们要到市场上购买芒果, 但是之前毫无挑选芒果的经验,那么如何通过学习来获取这些知识?\n\n首先, 我们从市场上随机选取一些芒果, 列出每个芒果的特征 (Feature),包括颜色、大小、形状、产地、品牌, 以及我们需要预测的标签 (Label). 标签可以是连续值 (比如关于芒果的甜度、水分以及成熟度的综合打分), 也可以是离散值 (比如 “好”“坏”两类标签). 这里, 每个芒果的标签可以通过直接品尝来获得,也可以通过请一些经验丰富的专家来进行标记.\n\n我们可以将一个标记好特征以及标签的芒果看作一个样本 (Sample), 也经常称为示例 (Instance).\n\n一组样本构成的集合称为数据集 (Data Set). 一般将数据集分为两部分:训练集和测试集. 训练集 (Training Set) 中的样本是用来训练模型的, 也叫训练样本 ( Training Sample), 而测试集 (Test Set) 中的样本是用来检验模型好坏的, 也叫测试样本 ( Test Sample ).\n\n我们通常用一个 $D$ 维向量 $\\boldsymbol{x}=\\left[x_{1}, x_{2}, \\cdots, x_{D}\\right]^{\\top}$ 表示一个芒果的所有特征构成的向量, 称为特征向量 (Feature Vector), 其中每一维表示一个特征. 而芒果的标签通常用标量 $y$ 来表示.\n\n\\href{https://nndl.github.io/%E7%89%B9%E5%BE%81%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%A7%B0%E4%B8%BA%E5%B1%9E%E6%80%A7}{https://nndl.github.io/特征也可以称为属性} (Attribute).\n\n在很多领域, 数据集也经常称为语料库 (Corpus ).\n\n并不是所有的样本特征都是数值型, 需要通过转换表示为特征向量,参见第2.6节.\\\\\n假设训练集 $\\mathcal{D}$ 由 $N$ 个样本组成, 其中每个样本都是独立同分布的 (Identically and Independently Distributed, IID ), 即独立地从相同的数据分布中抽取的, 记为\n\n\n\\begin{equation*}\n\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(1)}, y^{(1)}\\right),\\left(\\boldsymbol{x}^{(2)}, y^{(2)}\\right), \\cdots,\\left(\\boldsymbol{x}^{(N)}, y^{(N)}\\right)\\right\\} \\tag{2.1}\n\\end{equation*}\n\n\n给定训练集 $\\mathcal{D}$, 我们希望让计算机从一个函数集合 $\\mathcal{F}=\\left\\{f_{1}(\\boldsymbol{x}), f_{2}(\\boldsymbol{x}), \\cdots\\right\\}$ 中自动寻找一个 “最优” 的函数 $f^{*}(\\boldsymbol{x})$ 来近似每个样本的特征向量 $\\boldsymbol{x}$ 和标签 $y$ 之间的真实映射关系. 对于一个样本 $\\boldsymbol{x}$, 我们可以通过函数 $f^{*}(\\boldsymbol{x})$ 来预测其标签的值\n\n\n\\begin{equation*}\n\\hat{y}=f^{*}(\\boldsymbol{x}), \\tag{2.2}\n\\end{equation*}\n\n\n或标签的条件概率\n\n\n\\begin{equation*}\n\\hat{p}(y \\mid \\boldsymbol{x})=f_{y}^{*}(\\boldsymbol{x}) \\tag{2.3}\n\\end{equation*}\n\n\n如何寻找这个 “最优” 的函数 $f^{*}(\\boldsymbol{x})$ 是机器学习的关键, 一般需要通过学习算法 (Learning Algorithm) $\\mathcal{A}$ 来完成. 这个寻找过程通常称为学习 (Learning)或训练 (Training) 过程.\n\n这样, 下次从市场上买芒果 (测试样本) 时, 可以根据芒果的特征, 使用学习到的函数 $f^{*}(\\boldsymbol{x})$ 来预测芒果的好坏. 为了评价的公正性, 我们还是独立同分布地抽取一组芒果作为测试集 $\\mathcal{D}^{\\prime}$, 并在测试集中所有芒果上进行测试, 计算预测结果的准确率\n\n\n\\begin{equation*}\n\\operatorname{Acc}\\left(f^{*}(\\boldsymbol{x})\\right)=\\frac{1}{\\left|\\mathcal{D}^{\\prime}\\right|} \\sum_{(\\boldsymbol{x}, y) \\in \\mathcal{D}^{\\prime}} I\\left(f^{*}(\\boldsymbol{x})=y\\right) \\tag{2.4}\n\\end{equation*}\n\n\n其中 $I(\\cdot)$ 为指示函数, $\\left|\\mathcal{D}^{\\prime}\\right|$ 为测试集大小.\n\n图2.2给出了机器学习的基本流程. 对一个预测任务, 输入特征向量为 $\\boldsymbol{x}$, 输出标签为 $y$, 我们选择一个函数集合 $\\mathcal{F}$, 通过学习算法 $\\mathcal{A}$ 和一组训练样本 $\\mathcal{D}$, 从 $\\mathcal{F}$中学习到函数 $f^{*}(\\boldsymbol{x})$. 这样对新的输入 $\\boldsymbol{x}$, 就可以用函数 $f^{*}(\\boldsymbol{x})$ 进行预测.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_b7f820bc184c7846d78fg-039}\n\\end{center}\n\n图 2.2 机器学习系统示例\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\\section*{2.2 机器学习的三个基本要素}\n机器学习是从有限的观测数据中学习 (或 “猜测”) 出具有一般性的规律, 并可以将总结出来的规律推广应用到未观测样本上. 机器学习方法可以粗略地分为三个基本要素:模型、学习准则、优化算法."
    },
    {}
]