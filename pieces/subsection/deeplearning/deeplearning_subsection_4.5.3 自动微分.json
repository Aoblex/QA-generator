[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "4.5.3 自动微分",
        "content_length": 5789,
        "content": "Title: 4.5.3 自动微分\nContent:\n自动微分 (Automatic Differentiation, AD) 是一种可以对一个 (程序) 函数进行计算导数的方法. 符号微分的处理对象是数学表达式, 而自动微分的处理对象是一个函数或一段程序.\n\n自动微分的基本原理是所有的数值计算可以分解为一些基本操作，包含 ,,$+- \\times, /$ 和一些初等函数 exp, $\\log , \\sin , \\cos$ 等, 然后利用链式法则来自动计算一个复合函数的梯度.\n\n为简单起见, 这里以一个神经网络中常见的复合函数的例子来说明自动微分的过程. 令复合函数 $f(x ; w, b)$ 为\n\n\n\\begin{equation*}\nf(x ; w, b)=\\frac{1}{\\exp (-(w x+b))+1} \\tag{4.74}\n\\end{equation*}\n\n\n其中 $x$ 为输入标量, $w$ 和 $b$ 分别为权重和偏置参数.\n\n首先, 我们将复合函数 $f(x ; w, b)$ 分解为一系列的基本操作, 并构成一个计算图 (Computational Graph). 计算图是数学运算的图形化表示. 计算图中的每个非叶子节点表示一个基本操作, 每个叶子节点为一个输入变量或常量. 图4.8给出了当 $x=1, w=0, b=0$ 时复合函数 $f(x ; w, b)$ 的计算图, 其中连边上的红色数字表示前向计算时复合函数中每个变量的实际取值.\n\n\n\n图 4.8 复合函数 $f(x ; w, b)$ 的计算图\n\n从计算图上可以看出, 复合函数 $f(x ; w, b)$ 由 6 个基本函数 $h_{i}, 1 \\leq i \\leq 6$ 组成. 如表4.2所示, 每个基本函数的导数都十分简单, 可以通过规则来实现.\n\n\\href{https://nndl.github.io/%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%9C%A8%E5%8E%9F%E5%A7%8B%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81%E8%BF%9B%E8%A1%8C%E5%BE%AE%E5%88%86}{https://nndl.github.io/自动微分可以直接在原始程序代码进行微分}, 因此自动微分成为目前大多数深度学习框架的首选。\\\\\n表 4.2 复合函数 $f(x ; w, b)$ 的 6 个基本函数及其导数\n\n\\begin{center}\n\\begin{tabular}{lll}\n\\hline\n\\multicolumn{1}{c}{函数} & 导数 &  \\\\\n\\hline\n$h_{1}=x \\times w$ & $\\frac{\\partial h_{1}}{\\partial w}=x$ & $\\frac{\\partial h_{1}}{\\partial x}=w$ \\\\\n$h_{2}=h_{1}+b$ & $\\frac{\\partial h_{2}}{\\partial h_{1}}=1$ & $\\frac{\\partial h_{2}}{\\partial b}=1$ \\\\\n$h_{3}=h_{2} \\times-1$ & $\\frac{\\partial h_{3}}{\\partial h_{2}}=-1$ &  \\\\\n$h_{4}=\\exp \\left(h_{3}\\right)$ & $\\frac{\\partial h_{4}}{\\partial h_{3}}=\\exp \\left(h_{3}\\right)$ &  \\\\\n$h_{5}=h_{4}+1$ & $\\frac{\\partial h_{5}}{\\partial h_{4}}=1$ &  \\\\\n$h_{6}=1 / h_{5}$ & $\\frac{\\partial h_{6}}{\\partial h_{5}}=-\\frac{1}{h_{5}^{2}}$ &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n整个复合函数 $f(x ; w, b)$ 关于参数 $w$ 和 $b$ 的导数可以通过计算图上的节点 $f(x ; w, b)$ 与参数 $w$ 和 $b$ 之间路径上所有的导数连乘来得到, 即\n\n\n\\begin{align*}\n& \\frac{\\partial f(x ; w, b)}{\\partial w}=\\frac{\\partial f(x ; w, b)}{\\partial h_{6}} \\frac{\\partial h_{6}}{\\partial h_{5}} \\frac{\\partial h_{5}}{\\partial h_{4}} \\frac{\\partial h_{4}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}} \\frac{\\partial h_{1}}{\\partial w}  \\tag{4.75}\\\\\n& \\frac{\\partial f(x ; w, b)}{\\partial b}=\\frac{\\partial f(x ; w, b)}{\\partial h_{6}} \\frac{\\partial h_{6}}{\\partial h_{5}} \\frac{\\partial h_{5}}{\\partial h_{4}} \\frac{\\partial h_{4}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial b} \\tag{4.76}\n\\end{align*}\n\n\n以 $\\frac{\\partial f(x ; w, b)}{\\partial w}$ 为例, 当 $x=1, w=0, b=0$ 时, 可以得到\n\n\n\\begin{align*}\n\\left.\\frac{\\partial f(x ; w, b)}{\\partial w}\\right|_{x=1, w=0, b=0} & =\\frac{\\partial f(x ; w, b)}{\\partial h_{6}} \\frac{\\partial h_{6}}{\\partial h_{5}} \\frac{\\partial h_{5}}{\\partial h_{4}} \\frac{\\partial h_{4}}{\\partial h_{3}} \\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial h_{1}} \\frac{\\partial h_{1}}{\\partial w}  \\tag{4.77}\\\\\n& =1 \\times-0.25 \\times 1 \\times 1 \\times-1 \\times 1 \\times 1  \\tag{4.78}\\\\\n& =0.25 . \\tag{4.79}\n\\end{align*}\n\n\n如果函数和参数之间有多条路径, 可以将这多条路径上的导数再进行相加,得到最终的梯度.\n\n按照计算导数的顺序, 自动微分可以分为两种模式: 前向模式和反向模式.\n\n前向模式 前向模式是按计算图中计算方向的相同方向来递归地计算梯度. 以 $\\frac{\\partial f(x ; w, b)}{\\partial w}$ 为例, 当 $x=1, w=0, b=0$ 时, 前向模式的累积计算顺序如下:\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\\begin{gather*}\n\\frac{\\partial h_{1}}{\\partial w}=x=1,  \\tag{4.80}\\\\\n\\frac{\\partial h_{2}}{\\partial w}=\\frac{\\partial h_{2}}{\\partial h_{1}} \\frac{\\partial h_{1}}{\\partial w}=1 \\times 1=1  \\tag{4.81}\\\\\n\\frac{\\partial h_{3}}{\\partial w}=\\frac{\\partial h_{3}}{\\partial h_{2}} \\frac{\\partial h_{2}}{\\partial w}=-1 \\times 1, \\tag{4.82}\n\\end{gather*}\n\n\n\\[\n\\begin{array}{r}\n\\frac{\\partial h_{6}}{\\partial w}=\\frac{\\partial h_{6}}{\\partial h_{5}} \\frac{\\partial h_{5}}{\\partial w}=-0.25 \\times-1=0.25, \\\\\n\\frac{\\partial f(x ; w, b)}{\\partial w}=\\frac{\\partial f(x ; w, b)}{\\partial h_{6}} \\frac{\\partial h_{6}}{\\partial w}=1 \\times 0.25=0.25 . \\tag{4.84}\n\\end{array}\n\\]\n\n反向模式 反向模式是按计算图中计算方向的相反方向来递归地计算梯度. 以 $\\frac{\\partial f(x ; w, b)}{\\partial w}$ 为例, 当 $x=1, w=0, b=0$ 时, 反向模式的累积计算顺序如下:\n\n\n\\begin{align*}\n& \\frac{\\partial f(x ; w, b)}{\\partial h_{6}}=1,  \\tag{4.85}\\\\\n& \\frac{\\partial f(x ; w, b)}{\\partial h_{5}}=\\frac{\\partial f(x ; w, b)}{\\partial h_{6}} \\frac{\\partial h_{6}}{\\partial h_{5}}=1 \\times-0.25,  \\tag{4.86}\\\\\n& \\frac{\\partial f(x ; w, b)}{\\partial h_{4}}=\\frac{\\partial f(x ; w, b)}{\\partial h_{5}} \\frac{\\partial h_{5}}{\\partial h_{4}}=-0.25 \\times 1=-0.25,  \\tag{4.87}\\\\\n& \\quad \\vdots  \\tag{4.88}\\\\\n& \\frac{\\partial f(x ; w, b)}{\\partial w}=\\frac{\\partial f(x ; w, b)}{\\partial h_{1}} \\frac{\\partial h_{1}}{\\partial w}=0.25 \\times 1=0.25 .\n\\end{align*}\n\n\n前向模式和反向模式可以看作应用链式法则的两种梯度累积方式. 从反向模式的计算顺序可以看出,反向模式和反向传播的计算梯度的方式相同.\n\n对于一般的函数形式 $f: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}^{M}$, 前向模式需要对每一个输入变量都进行一遍遍历, 共需要 $N$ 遍. 而反向模式需要对每一个输出都进行一个遍历, 共需要 $M$ 遍. 当 $N>M$ 时, 反向模式更高效. 在前馈神经网络的参数学习中, 风险函数为 $f: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}$, 输出为标量, 因此采用反向模式为最有效的计算方式, 只需要一遍计算。\n\n静态计算图和动态计算图 计算图按构建方式可以分为静态计算图 (Static Computational Graph) 和动态计算图 (Dynamic Computational Graph). 静态计算图是在编译时构建计算图, 计算图构建好之后在程序运行时不能改变, 而动态计算图是在程序运行时动态构建. 两种构建方式各有优缺点. 静态计算图在构建时可以进行优化, 并行能力强, 但灵活性比较差. 动态计算图则不容易优化, 当不同输入的网络结构不一致时,难以并行计算,但是灵活性比较高.\n\n符号微分和自动微分 符号微分和自动微分都利用计算图和链式法则来自动求解导数. 符号微分在编译阶段先构造一个复合函数的计算图, 通过符号计算得到导数的表达式, 还可以对导数表达式进行优化, 在程序运行阶段才代入变量的具体数值来计算导数. 而自动微分则无须事先编译, 在程序运行阶段边计算边记录计算图, 计算图上的局部梯度都直接代入数值进行计算, 然后用前向或反向模式来计算最终的梯度.\n\n图4.9给出了符号微分与自动微分的对比.\n\n\\href{https://nndl.github.io/%E5%9C%A8%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E9%87%8C}{https://nndl.github.io/在目前深度学习框架里}, Theano 和 Tensorflow 采用的是静态计算图, 而 DyNet、 Chainer 和 PyTorch 采用的是动态计算图. Tensorflow 2.0 也支持了动态计算图.\n\n\n\n图 4.9 符号微分与自动微分对比\n\n\\section*{4.6 优化问题}\n神经网络的参数学习比线性模型要更加困难, 主要原因有两点: 1 ) 非凸优化问题和 2$)$ 梯度消失问题."
    },
    {
        "id": "chatcmpl-941ZFlslf89RV1RA88BuEcdmik7YL",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"自动微分与符号微分的计算方式有何区别？\",\n    \"回答1\": \"符号微分在编译阶段先构造一个复合函数的计算图, 通过符号计算得到导数的表达式, 还可以对导数表达式进行优化, 在程序运行阶段才代入变量的具体数值来计算导数。而自动微分则无须事先编译, 在程序运行阶段边计算边记录计算图, 计算图上的局部梯度都直接代入数值进行计算, 然后用前向或反向模式来计算最终的梯度。\",\n    \"问题2\": \"自动微分是如何利用链式法则来计算一个复合函数的梯度的？\",\n    \"回答2\": \"自动微分利用链式法则来计算一个复合函数的梯度。整个复合函数关于参数 $w$ 和 $b$ 的导数可以通过计算图上的节点 $f(x ; w, b)$ 与参数 $w$ 和 $b$ 之间路径上所有的导数连乘来得到。\",\n    \"问题3\": \"在自动微分中，前向模式和反向模式分别是如何计算梯度的？\",\n    \"回答3\": \"前向模式是按计算图中计算方向的相同方向来递归地计算梯度，需要对每一个输入变量都进行一遍遍历。而反向模式是按计算图中计算方向的相反方向来递归地计算梯度，需要对每一个输出都进行一个遍历。当输入变量数量 $N$ 大于输出数量 $M$ 时，反向模式更高效。\",\n    \"问题4\": \"静态计算图和动态计算图各有哪些特点？\",\n    \"回答4\": \"静态计算图在编译时构建计算图, 可以进行优化, 并行能力强, 但灵活性比较差。动态计算图是在程序运行时动态构建, 灵活性高, 但不容易优化，当不同输入的网络结构不一致时难以并行计算。\",\n    \"问题5\": \"为什么神经网络的参数学习比线性模型更加困难？\",\n    \"回答5\": \"神经网络的参数学习比线性模型更加困难主要原因有两点: 1) 非凸优化问题和 2) 梯度消失问题。神经网络通常具有大量参数和非线性结构，导致优化过程中存在多个局部最优解，增加了优化的难度；同时，梯度消失问题也会导致深层神经网络训练困难。\",\n    \"问题6\": \"自动微分是如何帮助深度学习框架进行参数学习的？\",\n    \"回答6\": \"自动微分可以直接在原始程序代码进行微分，因此成为目前大多数深度学习框架的首选。它能够自动计算复合函数的梯度，帮助深度学习框架高效地进行参数学习和优化。\",\n    \"问题7\": \"为什么深度学习框架中采用反向模式是最有效的计算方式？\",\n    \"回答7\": \"在前馈神经网络的参数学习中，风险函数为 $f: \\text{R}^{N} \\rightarrow \\text{R}$，输出为标量，因此采用反向模式为最有效的计算方式，只需要一遍计算。当输入变量数量 $N$ 大于输出数量 $M$ 时，反向模式更高效。\",\n    \"问题8\": \"自动微分和符号微分在深度学习框架中有何不同的应用？\",\n    \"回答8\": \"自动微分无须事先编译，在程序运行阶段边计算边记录计算图，并直接代入数值进行计算。而符号微分在编译阶段先构造一个复合函数的计算图, 通过符号计算得到导数的表达式，在程序运行阶段才代入变量的具体数值来计算导数。\",\n    \"问题9\": \"自动微分中采用前向模式和反向模式的梯度累积方式有何区别？\",\n    \"回答9\": \"前向模式是按计算图中计算方向的相同方向来递归地计算梯度，需要对每一个输入变量都进行一遍遍历。而反向模式是按计算图中计算方向的相反方向来递归地计算梯度，需要对每一个输出都进行一个遍历。\",\n    \"问题10\": \"为什么深度学习框架中既有静态计算图又有动态计算图？\",\n    \"回答10\": \"深度学习框架中既有静态计算图又有动态计算图是因为两种构建方式各有优缺点。静态计算图在编译时可以进行优化，并行能力强；而动态计算图灵活性高，但不容易优化。选择静态或动态计算图取决于具体应用场景和需求。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710746365,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1490,
            "prompt_tokens": 5897,
            "total_tokens": 7387
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]