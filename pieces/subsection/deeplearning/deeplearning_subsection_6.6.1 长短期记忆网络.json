[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "6.6.1 长短期记忆网络",
        "content_length": 4517,
        "content": "Title: 6.6.1 长短期记忆网络\nContent:\n长短期记忆网络 ( Long Short-Term Memory Network, LSTM ) [Gers et al., 2000; Hochreiter et al., 1997] 是循环神经网络的一个变体, 可以有效地解决简单循环神经网络的梯度爆炸或消失问题.\n\n在公式 (6.50)的基础上,LSTM 网络主要改进在以下两个方面:\n\n新的内部状态 LSTM 网络引入一个新的内部状态 (internal state ) $\\boldsymbol{c}_{t} \\in \\mathbb{R}^{D}$ 专门进行线性的循环信息传递, 同时 (非线性地) 输出信息给隐藏层的外部状态 $\\boldsymbol{h}_{t} \\in \\mathbb{R}^{D}$. 内部状态 $\\boldsymbol{c}_{t}$ 通过下面公式计算:\n\n\n\\begin{align*}\n\\boldsymbol{c}_{t} & =\\boldsymbol{f}_{t} \\odot \\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\odot \\tilde{\\boldsymbol{c}}_{t},  \\tag{6.51}\\\\\n\\boldsymbol{h}_{t} & =\\boldsymbol{o}_{t} \\odot \\tanh \\left(\\boldsymbol{c}_{t}\\right) \\tag{6.52}\n\\end{align*}\n\n\n其中 $\\boldsymbol{f}_{t} \\in[0,1]^{D} 、 \\boldsymbol{i}_{t} \\in[0,1]^{D}$ 和 $\\boldsymbol{o}_{t} \\in[0,1]^{D}$ 为三个门 ( gate ) 来控制信息传递的路径; $\\odot$ 为向量元素乘积; $\\boldsymbol{c}_{t-1}$ 为上一时刻的记忆单元; $\\tilde{\\boldsymbol{c}}_{t} \\in \\mathbb{R}^{D}$ 是通过非线性函数得到的候选状态:\n\n\n\\begin{equation*}\n\\tilde{\\boldsymbol{c}}_{t}=\\tanh \\left(\\boldsymbol{W}_{c} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{c} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{c}\\right) . \\tag{6.53}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/%E5%8F%82%E8%A7%81%E4%B9%A0%E9%A2%986-3}{https://nndl.github.io/参见习题6-3}.\n\n还有一种增加记忆容量的方法是增加一些额外的存储单元: 外部记忆,参见第8.5节.\\\\\n在每个时刻 $t$, LSTM 网络的内部状态 $\\boldsymbol{c}_{t}$ 记录了到当前时刻为止的历史信息.\n\n门控机制 在数字电路中, 门 (gate) 为一个二值变量 $\\{0,1\\}, 0$ 代表关闭状态, 不许任何信息通过; 1 代表开放状态,允许所有信息通过.\n\nLSTM 网络引入门控机制 (Gating Mechanism) 来控制信息传递的路径.公式 (6.51) 和公式 (6.52) 中三个 “门” 分别为输入门 $\\boldsymbol{i}_{t}$ 、遗忘门 $\\boldsymbol{f}_{t}$ 和输出门 $\\boldsymbol{o}_{t}$. 这三个门的作用为\n\n(1) 遗忘门 $\\boldsymbol{f}_{t}$ 控制上一个时刻的内部状态 $\\boldsymbol{c}_{t-1}$ 需要遗忘多少信息.\n\n(2) 输入门 $\\boldsymbol{i}_{t}$ 控制当前时刻的候选状态 $\\boldsymbol{c}_{t}$ 有多少信息需要保存.\n\n(3) 输出门 $\\boldsymbol{o}_{t}$ 控制当前时刻的内部状态 $\\boldsymbol{c}_{t}$ 有多少信息需要输出给外部状态 $\\boldsymbol{h}_{t}$.\n\n当 $\\boldsymbol{f}_{t}=0, \\boldsymbol{i}_{t}=1$ 时, 记忆单元将历史信息清空, 并将候选状态向量 $\\tilde{\\boldsymbol{c}}_{t}$ 写入.但此时记忆单元 $\\boldsymbol{c}_{t}$ 依然和上一时刻的历史信息相关. 当 $\\boldsymbol{f}_{t}=1, \\boldsymbol{i}_{t}=0$ 时, 记忆单元将复制上一时刻的内容,不写入新的信息.\n\nLSTM 网络中的 “门” 是一种 “软” 门, 取值在 $(0,1)$ 之间, 表示以一定的比例允许信息通过. 三个门的计算方式为:\n\n\n\\begin{align*}\n\\boldsymbol{i}_{t} & =\\sigma\\left(\\boldsymbol{W}_{i} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{i} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{i}\\right)  \\tag{6.54}\\\\\n\\boldsymbol{f}_{t} & =\\sigma\\left(\\boldsymbol{W}_{f} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{f} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{f}\\right)  \\tag{6.55}\\\\\n\\boldsymbol{o}_{t} & =\\sigma\\left(\\boldsymbol{W}_{o} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{o} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{o}\\right) \\tag{6.56}\n\\end{align*}\n\n\n其中 $\\sigma(\\cdot)$ 为 Logistic 函数, 其输出区间为 $(0,1), \\boldsymbol{x}_{t}$ 为当前时刻的输入, $\\boldsymbol{h}_{t-1}$ 为上一时刻的外部状态.\n\n图6.7给出了 LSTM 网络的循环单元结构, 其计算过程为: 1 ) 首先利用上一时刻的外部状态 $\\boldsymbol{h}_{t-1}$ 和当前时刻的输入 $\\boldsymbol{x}_{t}$, 计算出三个门, 以及候选状态 $\\left.\\tilde{\\boldsymbol{c}}_{t} ; 2\\right)$结合遗忘门 $\\boldsymbol{f}_{t}$ 和输入门 $\\boldsymbol{i}_{t}$ 来更新记忆单元 $\\left.\\boldsymbol{c}_{t} ; 3\\right)$ 结合输出门 $\\boldsymbol{o}_{t}$, 将内部状态的信息传递给外部状态 $\\boldsymbol{h}_{t}$.\n\n\n\n图 6.7 LSTM 网络的循环单元结构\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n通过 LSTM 循环单元, 整个网络可以建立较长距离的时序依赖关系. 公式 (6.51) 公式(6.56) 可以简洁地描述为\n\n\n\\begin{align*}\n{\\left[\\begin{array}{c}\n\\tilde{\\boldsymbol{c}}_{t} \\\\\n\\boldsymbol{o}_{t} \\\\\n\\boldsymbol{i}_{t} \\\\\n\\boldsymbol{f}_{t}\n\\end{array}\\right] } & =\\left[\\begin{array}{c}\n\\tanh \\\\\n\\sigma \\\\\n\\sigma \\\\\n\\sigma\n\\end{array}\\right]\\left(\\boldsymbol{W}\\left[\\begin{array}{c}\n\\boldsymbol{x}_{t} \\\\\n\\boldsymbol{h}_{t-1}\n\\end{array}\\right]+\\boldsymbol{b}\\right),  \\tag{6.57}\\\\\n\\boldsymbol{c}_{t} & =\\boldsymbol{f}_{t} \\odot \\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\odot \\tilde{\\boldsymbol{c}}_{t},  \\tag{6.58}\\\\\n\\boldsymbol{h}_{t} & =\\boldsymbol{o}_{t} \\odot \\tanh \\left(\\boldsymbol{c}_{t}\\right), \\tag{6.59}\n\\end{align*}\n\n\n其中 $\\boldsymbol{x}_{t} \\in \\mathbb{R}^{M}$ 为当前时刻的输入, $\\boldsymbol{W} \\in \\mathbb{R}^{4 D \\times(M+D)}$ 和 $\\boldsymbol{b} \\in \\mathbb{R}^{4 D}$ 为网络参数.\n\n记忆 循环神经网络中的隐状态 $\\boldsymbol{h}$ 存储了历史信息, 可以看作一种记忆 (Memory ）。在简单循环网络中, 隐状态每个时刻都会被重写, 因此可以看作一种短期记忆 (Short-Term Memory)。在神经网络中, 长期记忆 (Long-Term Memory ) 可以看作网络参数, 隐含了从训练数据中学到的经验, 其更新周期要远远慢于短期记忆. 而在 LSTM 网络中, 记忆单元 $\\boldsymbol{c}$ 可以在某个时刻捕捉到某个关键信息, 并有能力将此关键信息保存一定的时间间隔. 记忆单元 $\\boldsymbol{c}$ 中保存信息的生命周期要长于短期记忆 $\\boldsymbol{h}$, 但又远远短于长期记忆, 因此称为长短期记忆 ( Long Short-Term Memory ) .\n\n一般在深度网络参数学习时, 参数初始化的值一般都比较小. 但是在训练 LSTM 网络时, 过小的值会使得遗忘门的值比较小. 这意味着前一时刻的信息大部分都丢失了, 这样网络很难捕捉到长距离的依赖信息. 并且相邻时间间隔的梯度会非常小, 这会导致梯度弥散问题. 因此遗忘的参数初始值一般都设得比较大,其偏置向量 $\\boldsymbol{b}_{f}$ 设为 1 或 2 ."
    },
    {
        "id": "chatcmpl-9425Lj664oiTbd64ijoENbJR4kvJ3",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"长短期记忆网络与简单循环神经网络相比有哪些改进？\",\n    \"回答1\": \"长短期记忆网络引入了一个新的内部状态 $\\boldsymbol{c}_{t}$ 专门进行线性的循环信息传递，并通过三个门来控制信息传递的路径。这种改进有效地解决了简单循环神经网络的梯度爆炸或消失问题。\",\n    \"问题2\": \"LSTM 网络中的内部状态 $\\boldsymbol{c}_{t}$ 是如何计算得到的？\",\n    \"回答2\": \"LSTM 网络中的内部状态 $\\boldsymbol{c}_{t}$ 通过公式 $\\boldsymbol{c}_{t}=\\boldsymbol{f}_{t} \\bigodot \\boldsymbol{c}_{t-1}+\\boldsymbol{i}_{t} \\bigodot \\tilde{\\boldsymbol{c}}_{t}$ 计算得到，其中 $\\boldsymbol{f}_{t}$ 和 $\\boldsymbol{i}_{t}$ 是控制信息传递路径的门，$\\tilde{\\boldsymbol{c}}_{t}$ 是通过非线性函数得到的候选状态。\",\n    \"问题3\": \"LSTM 网络中的输出状态 $\\boldsymbol{h}_{t}$ 是如何计算得到的？\",\n    \"回答3\": \"LSTM 网络中的输出状态 $\\boldsymbol{h}_{t}$ 通过公式 $\\boldsymbol{h}_{t}=\\boldsymbol{o}_{t} \\bigodot \\tanh \\big(\\boldsymbol{c}_{t}\\big)$ 计算得到，其中 $\\boldsymbol{o}_{t}$ 是控制信息输出路径的门，$\\tanh$ 为双曲正切函数。\",\n    \"问题4\": \"LSTM 网络中的门控机制有什么作用？\",\n    \"回答4\": \"LSTM 网络引入门控机制来控制信息传递的路径。其中遗忘门 $\\boldsymbol{f}_{t}$ 控制上一个时刻的内部状态 $\\boldsymbol{c}_{t-1}$ 需要遗忘多少信息，输入门 $\\boldsymbol{i}_{t}$ 控制当前时刻的候选状态 $\\boldsymbol{c}_{t}$ 有多少信息需要保存，输出门 $\\boldsymbol{o}_{t}$ 控制当前时刻的内部状态 $\\boldsymbol{c}_{t}$ 有多少信息需要输出给外部状态 $\\boldsymbol{h}_{t}$。\",\n    \"问题5\": \"LSTM 网络中的门是如何计算得到的？\",\n    \"回答5\": \"LSTM 网络中的三个门（遗忘门、输入门和输出门）分别通过公式计算得到：$\\boldsymbol{i}_{t}=\\text{sigmoid}\\big(\\boldsymbol{W}_{i} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{i} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{i}\\big)$、$\\boldsymbol{f}_{t}=\\text{sigmoid}\\big(\\boldsymbol{W}_{f} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{f} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{f}\\big)$ 和 $\\boldsymbol{o}_{t}=\\text{sigmoid}\\big(\\boldsymbol{W}_{o} \\boldsymbol{x}_{t}+\\boldsymbol{U}_{o} \\boldsymbol{h}_{t-1}+\\boldsymbol{b}_{o}\\big)$。\",\n    \"问题6\": \"LSTM 网络如何解决梯度爆炸或消失问题？\",\n    \"回答6\": \"LSTM 网络通过引入门控机制和新的内部状态 $\\boldsymbol{c}_{t}$ 来有效地解决简单循环神经网络的梯度爆炸或消失问题。遗忘门和输入门能够控制信息传递路径，从而更好地捕捉长距离依赖信息，并且避免梯度弥散问题。\",\n    \"问题7\": \"LSTM 网络中记忆单元和隐状态分别代表什么？\",\n    \"回答7\": \"在 LSTM 网络中，记忆单元 $\\boldsymbol{c}$ 存储了历史信息，可以看作一种长期记忆；而隐状态 $\\boldsymbol{h}$ 存储了短期信息，可以看作一种短期记忆。记忆单元中保存信息的生命周期要长于短期记忆，但又远远短于长期记忆，因此称为长短期记忆。\",\n    \"问题8\": \"LSTM 网络中遗忘门参数初始化值为什么需要设定比较大？\",\n    \"回答8\": \"在训练 LSTM 网络时，过小的遗忘门参数初始值会导致前一时刻的信息大部分丢失，使网络难以捕捉长距离依赖信息，并且会导致梯度弥散问题。因此，遗忘门参数初始值一般设定比较大，其偏置向量也会设为较大值，以确保网络能够更好地捕捉长距离依赖信息。\",\n    \"问题9\": \"LSTM 网络如何建立较长距离的时序依赖关系？\",\n    \"回答9\": \"通过 LSTM 循环单元, 整个网络可以建立较长距离的时序依赖关系。LSTM 网络引入了新的内部状态和门控机制来控制信息传递路径，从而能够更好地捕捉并维持较长距离的时序依赖关系。\",\n    \"问题10\": \"LSTM 网络中三个门是什么作用？\",\n    \"回答10\": \"LSTM 网络中的三个门（遗忘门、输入门和输出门）分别控制着上一个时刻的内部状态需要遗忘多少信息、当前时刻的候选状态有多少信息需要保存以及当前时刻的内部状态有多少信息需要输出给外部状态。这些门通过软门控机制来以一定比例允许信息通过，从而实现对信息传递路径的精细控制。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710748355,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1679,
            "prompt_tokens": 5177,
            "total_tokens": 6856
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]