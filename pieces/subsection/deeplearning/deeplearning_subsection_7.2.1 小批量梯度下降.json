[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.2.1 小批量梯度下降",
        "content_length": 1662,
        "content": "Title: 7.2.1 小批量梯度下降\nContent:\n在训练深度神经网络时, 训练数据的规模通常都比较大. 如果在梯度下降时, 每次迭代都要计算整个训练数据上的梯度, 这就需要比较多的计算资源. 另外大规模训练集中的数据通常会非常冗余, 也没有必要在整个训练集上计算梯度. 因此, 在训练深度神经网络时, 经常使用小批量梯度下降法 (Mini-Batch Gradient Descent ).\n\n令 $f(x ; \\theta)$ 表示一个深度神经网络, $\\theta$ 为网络参数, 在使用小批量梯度下降进行优化时, 每次选取 $K$ 个训练样本 $\\mathcal{S}_{t}=\\left\\{\\left(\\boldsymbol{x}^{(k)}, \\boldsymbol{y}^{(k)}\\right)\\right\\}_{k=1}^{K}$. 第 $t$ 次迭代 (Iteration )时损失函数关于参数 $\\theta$ 的偏导数为\n\n\n\\begin{equation*}\n\\mathfrak{g}_{t}(\\theta)=\\frac{1}{K} \\sum_{(\\boldsymbol{x}, \\boldsymbol{y}) \\in \\mathcal{S}_{t}} \\frac{\\partial \\mathcal{L}(\\boldsymbol{y}, f(\\boldsymbol{x} ; \\theta))}{\\partial \\theta} \\tag{7.1}\n\\end{equation*}\n\n\n这里的损失函数忽略了正则化项. 加上 $e_{p}$ 正则化的损失函数参见第7.7.1节。\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n其中 $\\mathcal{L}(\\cdot)$ 为可微分的损失函数, $K$ 称为批量大小 ( Batch Size ).\n\n第 $t$ 次更新的梯度 $\\mathrm{g}_{t}$ 定义为\n\n\n\\begin{equation*}\n\\mathbf{g}_{t} \\triangleq \\mathfrak{g}_{t}\\left(\\theta_{t-1}\\right) \\tag{7.2}\n\\end{equation*}\n\n\n使用梯度下降来更新参数，\n\n\n\\begin{equation*}\n\\theta_{t} \\leftarrow \\theta_{t-1}-\\alpha \\mathbf{g}_{t}, \\tag{7.3}\n\\end{equation*}\n\n\n其中 $\\alpha>0$ 为学习率.\n\n每次迭代时参数更新的差值 $\\Delta \\theta_{t}$ 定义为\n\n\n\\begin{equation*}\n\\Delta \\theta_{t} \\triangleq \\theta_{t}-\\theta_{t-1} \\tag{7.4}\n\\end{equation*}\n\n\n$\\Delta \\theta_{t}$ 和梯度 $\\mathbf{g}_{t}$ 并不需要完全一致. $\\Delta \\theta_{t}$ 为每次迭代时参数的实际更新方向, 即 $\\theta_{t}=\\theta_{t-1}+\\Delta \\theta_{t}$. 在标准的小批量梯度下降中, $\\Delta \\theta_{t}=-\\alpha \\mathrm{g}_{t}$.\n\n从上面公式可以看出，影响小批量梯度下降法的主要因素有: 1) 批量大小 $K 、 2)$ 学习率 $\\alpha 、 3$ ) 梯度估计. 为了更有效地训练深度神经网络, 在标准的小批量梯度下降法的基础上, 也经常使用一些改进方法以加快优化速度, 比如如何选择批量大小、如何调整学习率以及如何修正梯度估计. 我们分别从这三个方面来介绍在神经网络优化中常用的算法. 这些改进的优化算法也同样可以应用在批量或随机梯度下降法上."
    },
    {
        "id": "chatcmpl-942J8mSqKDNlCsk260ERBXu8zW6fW",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"小批量梯度下降法相对于整体梯度下降有什么优势？\",\n    \"回答1\": \"小批量梯度下降法在训练深度神经网络时，每次迭代只需计算选取的 $K$ 个训练样本上的梯度，而不是整个训练数据集上的梯度。这样可以节省计算资源，并且在大规模训练集中避免不必要的冗余计算，从而提高了计算效率。\",\n    \"问题2\": \"小批量梯度下降法中的批量大小 $K$ 有何含义？\",\n    \"回答2\": \"小批量梯度下降法中的批量大小 $K$ 表示每次迭代时选取的训练样本数量。通过调整批量大小 $K$，可以影响梯度估计的准确性和优化速度。\",\n    \"问题3\": \"小批量梯度下降法中的损失函数 $\\text{L}(\\boldsymbol{y}, f(\\boldsymbol{x} ; \\theta))$ 忽略了哪一项？\",\n    \"回答3\": \"小批量梯度下降法中的损失函数 $\\text{L}(\\boldsymbol{y}, f(\\boldsymbol{x} ; \\theta))$ 忽略了正则化项。在实际应用中，可以加上正则化项来控制模型的复杂度和泛化能力。\",\n    \"问题4\": \"小批量梯度下降法中的学习率 $\\text{α}$ 的作用是什么？\",\n    \"回答4\": \"小批量梯度下降法中的学习率 $\\text{α}$ 用于控制参数更新的步长，即每次迭代时参数沿着梯度方向更新的幅度。合适的学习率能够平衡收敛速度和优化稳定性。\",\n    \"问题5\": \"小批量梯度下降法中的参数更新公式是怎样的？\",\n    \"回答5\": \"小批量梯度下降法使用梯度下降来更新参数，即 $\\theta_{t} \\text{ ← } \\theta_{t-1}-\\text{α} \\textbf{g}_{t}$，其中 $\\text{α}>0$ 为学习率，$\\textbf{g}_{t}$ 为第 $t$ 次更新的梯度。\",\n    \"问题6\": \"小批量梯度下降法中的参数更新差值 $\\triangle \\theta_{t}$ 的含义是什么？\",\n    \"回答6\": \"小批量梯度下降法中的参数更新差值 $\\triangle \\theta_{t}$ 表示每次迭代时参数的实际更新方向，即 $\\theta_{t}=\\theta_{t-1}+\\triangle \\theta_{t}$。它反映了参数在每次迭代中的变化。\",\n    \"问题7\": \"影响小批量梯度下降法效果的主要因素有哪些？\",\n    \"回答7\": \"影响小批量梯度下降法效果的主要因素包括批量大小 $K$、学习率 $\\text{α}$ 和梯度估计。合理选择这些因素能够影响优化速度和收敛性能。\",\n    \"问题8\": \"为了更有效地训练深度神经网络，除了标准的小批量梯度下降法，还经常使用哪些改进方法？\",\n    \"回答8\": \"为了更有效地训练深度神经网络，除了标准的小批量梯度下降法，还经常使用一些改进方法以加快优化速度。这些改进方法包括如何选择批量大小、如何调整学习率以及如何修正梯度估计。\",\n    \"问题9\": \"在神经网络优化中常用的算法主要从哪三个方面进行改进？\",\n    \"回答9\": \"在神经网络优化中常用的算法主要从批量大小选择、学习率调整以及梯度估计修正这三个方面进行改进。这些改进的优化算法也同样可以应用在批量或随机梯度下降法上。\",\n    \"问题10\": \"改进的优化算法对神经网络模型有何影响？\",\n    \"回筗10\": \"改进的优化算法使得神经网络模型在训练过程中能够更快地收敛并获得更好的性能。通过合理选择批量大小、调整学习率以及修正梯度估计等方法，可以提高模型的训练效率和泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710749210,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1350,
            "prompt_tokens": 3626,
            "total_tokens": 4976
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]