[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "14.1.2 强化学习定义",
        "content_length": 1211,
        "content": "Title: 14.1.2 强化学习定义\nContent:\n我们先描述强化学习的任务定义.\n\n在强化学习中, 有两个可以进行交互的对象:智能体和环境.\n\n（1）智能体 (Agent）可以感知外界环境的状态 (State) 和反馈的奖励 (Reward), 并进行学习和决策. 智能体的决策功能是指根据外界环境的状态来做出不同的动作 (Action), 而学习功能是指根据外界环境的奖励来调整策略.\n\n（2）环境 (Environment) 是智能体外部的所有事物, 并受智能体动作的影响而改变其状态, 并反馈给智能体相应的奖励.\n\n强化学习的基本要素包括:\n\n（1）状态 $s$ 是对环境的描述,可以是离散的或连续的,其状态空间为 $\\mathcal{S}$.\n\n(2) 动作 $a$ 是对智能体行为的描述，可以是离散的或连续的，其动作空间为 $\\mathcal{A}$.\n\n（3）策略 $\\pi(a \\mid s)$ 是智能体根据环境状态 $s$ 来决定下一步动作 $a$ 的函数.\n\n(4) 状态转移概率 $p\\left(s^{\\prime} \\mid s, a\\right)$ 是在智能体根据当前状态 $s$ 做出一个动作 $a$ 之后, 环境在下一个时刻转变为状态 $s^{\\prime}$ 的概率.\n\n（5）即时奖励 $r\\left(s, a, s^{\\prime}\\right)$ 是一个标量函数, 即智能体根据当前状态 $s$ 做出动作 $a$ 之后, 环境会反馈给智能体一个奖励, 这个奖励也经常和下一个时刻的状态 $s^{\\prime}$ 有关.\n\n策略 智能体的策略 (Policy) 就是智能体如何根据环境状态 $s$ 来决定下一步的动作 $a$, 通常可以分为确定性策略 (Deterministic Policy) 和随机性策略 (Stochastic Policy ) 两种.\n\n确定性策略是从状态空间到动作空间的映射函数 $\\pi: \\mathcal{S} \\rightarrow \\mathcal{A}$. 随机性策略表示在给定环境状态时, 智能体选择某个动作的概率分布.\n\n\n\\begin{align*}\n\\pi(a \\mid s) & \\triangleq p(a \\mid s),  \\tag{14.1}\\\\\n\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) & =1 . \\tag{14.2}\n\\end{align*}\n\n\n通常情况下, 强化学习一般使用随机性策略. 随机性策略可以有很多优点: 1 ) 在学习时可以通过引入一定随机性更好地探索环境;2) 随机性策略的动作具参考利用-探索策略. 参见第14.2.2节。有多样性, 这一点在多个智能体博亦时也非常重要. 采用确定性策略的智能体总是对同样的环境做出相同的动作,会导致它的策略很容易被对手预测."
    },
    {
        "id": "chatcmpl-945c39IAfgIN2IOMrxIP4pCDTNEZS",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"在强化学习中，智能体和环境的交互对象有哪两个？\",\n    \"回答1\": \"在强化学习中，智能体（Agent）和环境（Environment）是可以进行交互的两个对象。智能体可以感知外界环境的状态和反馈的奖励，并进行学习和决策；而环境是智能体外部的所有事物，受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励。\",\n    \"问题2\": \"强化学习中的状态和动作分别是如何定义的？\",\n    \"回答2\": \"在强化学习中，状态 $s$ 是对环境的描述，可以是离散的或连续的，其状态空间为 $\\boldsymbol{\\text{S}}$；动作 $a$ 是对智能体行为的描述，可以是离散的或连续的，其动作空间为 $\\boldsymbol{\\text{A}}$。\",\n    \"问题3\": \"什么是智能体的策略？它有哪两种类型？\",\n    \"回答3\": \"智能体的策略（Policy）是智能体如何根据环境状态 $s$ 来决定下一步的动作 $a$。它通常可以分为确定性策略（Deterministic Policy）和随机性策略（Stochastic Policy）两种。\",\n    \"问题4\": \"强化学习中的状态转移概率 $p\\big(s^{\\text{'}} \\big| s, a\\big)$ 是如何定义的？\",\n    \"回答4\": \"在强化学习中，状态转移概率 $p\\big(s^{\\text{'}} \\big| s, a\\big)$ 是在智能体根据当前状态 $s$ 做出一个动作 $a$ 之后，环境在下一个时刻转变为状态 $s^{\\text{'}}$ 的概率。\",\n    \"问题5\": \"即时奖励在强化学习中起到什么作用？\",\n    \"回答5\": \"即时奖励是一个标量函数，即智能体根据当前状态 $s$ 做出动作 $a$ 之后，环境会反馈给智能体一个奖励。这个奖励也经常和下一个时刻的状态 $s^{\\text{'}}$ 有关。即时奖励在强化学习中用于指导智能体学习和调整策略。\",\n    \"问题6\": \"什么是确定性策略？它是如何定义的？\",\n    \"回答6\": \"确定性策略是从状态空间到动作空间的映射函数 $\\boldsymbol{\\text{π}}: \\boldsymbol{\\text{S}} \\rightarrow \\boldsymbol{\\text{A}}$。它表示在给定环境状态时，智能体选择某个动作的确定性映射关系。\",\n    \"问题7\": \"为什么在强化学习中通常使用随机性策略？\",\n    \"回答7\": \"在强化学习中通常使用随机性策略有很多优点：1) 在学习时可以通过引入一定随机性更好地探索环境；2) 随机性策略具有参考利用-探索策略，有多样性，在多个智能体博弈时也非常重要。\",\n    \"问题8\": \"强化学习中的策略函数 $\\boldsymbol{\\text{π}}(a \\big| s)$ 是如何定义的？\",\n    \"回答8\": \"强化学习中的策略函数 $\\boldsymbol{\\text{π}}(a \\big| s)$ 定义为 $p(a \\big| s)$，表示在给定环境状态时，智能体选择某个动作的概率分布。\",\n    \"问题9\": \"强化学习中状态和动作空间的描述有何特点？\",\n    \"回答9\": \"在强化学习中，状态和动作空间的描述可以是离散的或连续的。状态空间描述了环境可能处于的各种状态，而动作空间描述了智能体可能采取的各种行为。\",\n    \"问题10\": \"采用确定性策略和随机性策略对于智能体行为有何不同影响？\",\n    \"回答10\": \"采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致其策略很容易被对手预测。而采用随机性策略则具有更好地探索环境、多样性等优点，在多个智能体博弈时也非常重要。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710761915,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1336,
            "prompt_tokens": 3540,
            "total_tokens": 4876
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]