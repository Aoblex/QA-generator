[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "13.3.5.2 模型坞塌",
        "content_length": 3816,
        "content": "Title: 13.3.5.2 模型坞塌\nContent:\n如果使用公式(13.33)作为生成网络的目标函数, 将最优判别器 $D^{\\star}$ 代入, 可以得到\n\n\n\\begin{align*}\n\\mathcal{L}^{\\prime} & \\left(G \\mid D^{\\star}\\right)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\theta}(\\boldsymbol{x})}\\left[\\log D^{\\star}(\\boldsymbol{x})\\right]  \\tag{13.42}\\\\\n& =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\theta}(\\boldsymbol{x})}\\left[\\log \\frac{p_{r}(\\boldsymbol{x})}{p_{r}(\\boldsymbol{x})+p_{\\theta}(\\boldsymbol{x})} \\cdot \\frac{p_{\\theta}(\\boldsymbol{x})}{p_{\\theta}(\\boldsymbol{x})}\\right]  \\tag{13.43}\\\\\n& =-\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\theta}(\\boldsymbol{x})}\\left[\\log \\frac{p_{\\theta}(\\boldsymbol{x})}{p_{r}(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\theta}(\\boldsymbol{x})}\\left[\\log \\frac{p_{\\theta}(\\boldsymbol{x})}{p_{r}(\\boldsymbol{x})+p_{\\theta}(\\boldsymbol{x})}\\right]  \\tag{13.44}\\\\\n& =-\\mathrm{KL}\\left(p_{\\theta}, p_{r}\\right)+\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\theta}(\\boldsymbol{x})}\\left[\\log \\left(1-D^{\\star}(\\boldsymbol{x})\\right)\\right]  \\tag{13.45}\\\\\n& =-\\mathrm{KL}\\left(p_{\\theta}, p_{r}\\right)+2 \\mathrm{JS}\\left(p_{r}, p_{\\theta}\\right)-2 \\log 2-\\mathbb{E}_{\\boldsymbol{x} \\sim p_{r}(x)}\\left[\\log D^{\\star}(\\boldsymbol{x})\\right], \\tag{13.46}\n\\end{align*}\n\n\n根据公式(13.41)\n\n其中后两项和生成网络无关. 因此\n\n\n\\begin{equation*}\n\\underset{\\theta}{\\arg \\max } \\mathcal{L}^{\\prime}\\left(G \\mid D^{\\star}\\right)=\\underset{\\theta}{\\arg \\min } \\mathrm{KL}\\left(p_{\\theta}, p_{r}\\right)-2 \\mathrm{JS}\\left(p_{r}, p_{\\theta}\\right) \\tag{13.47}\n\\end{equation*}\n\n\n其中 $\\mathrm{JS}$ 散度 $\\mathrm{JS}\\left(p_{\\theta}, p_{r}\\right) \\in[0, \\log 2]$ 为有界函数, 因此生成网络的目标更多的是受逆向 KL 散度 $\\mathrm{KL}\\left(p_{\\theta}, p_{r}\\right)$ 影响, 使得生成网络更倾向于生成一些更 “安全” 的样本, 从而造成模型坞塌 ( Model Collapse ) 问题.\n\n前向和逆向 $K L$ 散度 因为 $K L$ 散度是一种非对称的散度, 在计算真实分布 $p_{r}$ 和模型分布 $p_{\\theta}$ 之间的 KL 散度时, 按照顺序不同, 有两种 KL 散度: 前向 KL 散度 ( Forward KL divergence ) $\\mathrm{KL}\\left(p_{r}, p_{\\theta}\\right)$ 和逆向 KL 散度 ( Reverse KL divergence ) $\\mathrm{KL}\\left(p_{\\theta}, p_{r}\\right)$. 前向和逆向 $\\mathrm{KL}$ 散度分别定义为\n\n\n\\begin{align*}\n& \\mathrm{KL}\\left(p_{r}, p_{\\theta}\\right)=\\int p_{r}(\\boldsymbol{x}) \\log \\frac{p_{r}(\\boldsymbol{x})}{p_{\\theta}(\\boldsymbol{x})} \\mathrm{d} \\boldsymbol{x},  \\tag{13.48}\\\\\n& \\mathrm{KL}\\left(p_{\\theta}, p_{r}\\right)=\\int p_{\\theta}(\\boldsymbol{x}) \\log \\frac{p_{\\theta}(\\boldsymbol{x})}{p_{r}(\\boldsymbol{x})} \\mathrm{d} \\boldsymbol{x} . \\tag{13.49}\n\\end{align*}\n\n\n图13.11给出数据真实分布为一个高斯混合分布, 模型分布为一个单高斯分布时, 使用前向和逆向 KL 散度来进行模型优化的示例. 黑色曲线为真实分布 $p_{r}$的等高线,红色曲线为模型分布 $p_{\\theta}$ 的等高线.\n\n在前向 KL 散度中，\n\n（1）当 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 而 $p_{\\theta}(\\boldsymbol{x})>0$ 时, $p_{r}(\\boldsymbol{x}) \\log \\frac{p_{r}(\\boldsymbol{x})}{p_{\\theta}(\\boldsymbol{x})} \\rightarrow 0$. 不管 $p_{\\theta}(\\boldsymbol{x})$ 如何取值, 都对前向 KL 散度的计算没有贡献.得非常大.\n\n(2) 当 $p_{r}(\\boldsymbol{x})>0$ 而 $p_{\\theta}(\\boldsymbol{x}) \\rightarrow 0$ 时, $p_{r}(\\boldsymbol{x}) \\log \\frac{p_{r}(\\boldsymbol{x})}{p_{\\theta}(\\boldsymbol{x})} \\rightarrow \\infty$, 前向 KL 散度会变\n\n因此, 前向 KL散度会鼓励模型分布 $p_{\\theta}(\\boldsymbol{x})$ 尽可能覆盖所有真实分布 $p_{r}(\\boldsymbol{x})>$ 0 的点,而不用回避 $p_{r}(\\boldsymbol{x}) \\approx 0$ 的点.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n\n\n图 13.11 前向和逆向 KL 散度\n\n在逆向 KL 散度中，\n\n(1) 当 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 而 $p_{\\theta}(\\boldsymbol{x})>0$ 时, $p_{\\theta}(\\boldsymbol{x}) \\log \\frac{p_{\\theta}(\\boldsymbol{x})}{p_{r}(\\boldsymbol{x})} \\rightarrow \\infty$. 即当 $p_{r}(\\boldsymbol{x})$ 接近于 0 ,而 $p_{\\theta}(\\boldsymbol{x})$ 有一定的密度时, 逆向 KL 散度会变得非常大.\n\n(2) 当 $p_{\\theta}(\\boldsymbol{x}) \\rightarrow 0$ 时,不管 $p_{r}(\\boldsymbol{x})$ 如何取值, $p_{\\theta}(\\boldsymbol{x}) \\log \\frac{p_{\\theta}(\\boldsymbol{x})}{p_{r}(\\boldsymbol{x})} \\rightarrow 0$.\n\n因此, 逆向 $\\mathrm{KL}$ 散度会鼓励模型分布 $p_{\\theta}(\\boldsymbol{x})$ 尽可能避开所有真实分布 $p_{r}(\\boldsymbol{x}) \\approx$ 0 的点, 而不需要考虑是否覆盖所有真实分布 $p_{r}(\\boldsymbol{x})>0$ 的点."
    },
    {
        "id": "chatcmpl-945bCDstzP5ifX5rTe2OtTXk3ibIg",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"生成网络的目标函数中的 $\\text{KL}$ 散度和 $\\text{JS}$ 散度分别代表什么意义？\",\n    \"回答1\": \"生成网络的目标函数中的 $\\text{KL}$ 散度代表生成分布 $p_{\\theta}(\\boldsymbol{x})$ 与真实分布 $p_{r}(\\boldsymbol{x})$ 之间的差异，而 $\\text{JS}$ 散度则表示两个分布之间的相似程度。因此，$\\text{KL}$ 散度和 $\\text{JS}$ 散度在目标函数中分别体现了生成网络与真实分布之间的差异和相似性。\",\n    \"问题2\": \"生成网络的目标函数中的 $\\text{KL}$ 散度和 $\\text{JS}$ 散度如何影响生成网络的优化过程？\",\n    \"回答2\": \"生成网络的目标函数中的 $\\text{KL}$ 散度会鼓励模型分布 $p_{\\theta}(\\boldsymbol{x})$ 尽可能覆盖所有真实分布 $p_{r}(\\boldsymbol{x})>0$ 的点，而不用回避 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 的点。而 $\\text{JS}$ 散度则表示两个分布之间的相似程度，因此会影响生成网络更倾向于生成一些更 “安全” 的样本。这些因素共同影响着生成网络的优化过程。\",\n    \"问题3\": \"前向和逆向 $\\text{KL}$ 散度分别代表什么意义？\",\n    \"回答3\": \"前向 $\\text{KL}$ 散度 $\\text{KL}\\big(p_{r}, p_{\\theta}\\big)$ 表示真实分布 $p_{r}(\\boldsymbol{x})$ 对模型分布 $p_{\\theta}(\\boldsymbol{x})$ 的期望信息损失，而逆向 $\\text{KL}$ 散度 $\\text{KL}\\big(p_{\\theta}, p_{r}\\big)$ 则表示模型分布 $p_{\\theta}(\\boldsymbol{x})$ 对真实分布 $p_{r}(\\boldsymbol{x})$ 的期望信息损失。这两种散度分别从不同角度衡量了两个分布之间的差异。\",\n    \"问题4\": \"前向和逆向 $\\text{KL}$ 散度在模型优化中有何不同影响？\",\n    \"回答4\": \"前向 $\\text{KL}$ 散度会鼓励模型分布 $p_{\\theta}(\\boldsymbol{x})$ 尽可能覆盖所有真实分布 $p_{r}(\\boldsymbol{x})>0$ 的点，而不用回避 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 的点。逆向 $\\text{KL}$ 散度则会鼓励模型分布 $p_{\\theta}(\\boldsymbol{x})$ 尽可能避开所有真实分布 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 的点，而不需要考虑是否覆盖所有真实分布 $p_{r}(\\boldsymbol{x})>0$ 的点。这两种散度在模型优化中产生不同的影响。\",\n    \"问题5\": \"为什么生成网络更倾向于受逆向 $\\text{KL}$ 散度影响，从而导致模型坍塌问题？\",\n    \"回答5\": \"由于逆向 $\\text{KL}$ 散度会鼓励模型分布 $p_{\\theta}(\\boldsymbol{x})$ 尽可能避开所有真实分布 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 的点，而不需要考虑是否覆盖所有真实分布 $p_{r}(\\boldsymbol{x})>0$ 的点。这使得生成网络更倾向于生成一些更 “安全” 的样本，从而导致模型坍塌问题。\",\n    \"问题6\": \"在前向 $\\text{KL}$ 散度中，当 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 而 $p_{\\theta}(\\boldsymbol{x})>0$ 时会发生什么情况？\",\n    \"回答6\": \"在前向 $\\text{KL}$ 散度中，当 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 而 $p_{\\theta}(\\boldsymbol{x})>0$ 时，$p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 而 $p_{\\theta}(\\boldsymbol{x})>0$ 时，对前向 $\\text{KL}$ 散度的计算没有贡献。这意味着无论 $p_{\\theta}(\\boldsymbol{x})$ 如何取值，都对前向 $\\text{KL}$ 散度的计算没有贡献。\",\n    \"问题7\": \"在前向 $\\text{KL}$ 散度中，当 $p_{r}(\\boldsymbol{x})>0$ 而 $p_{\\theta}(\\boldsymbol{x}) \\rightarrow 0$ 时会发生什么情况？\",\n    \"回答7\": \"在前向 $\\text{KL}$ 散度中，当 $p_{r}(\\boldsymbol{x})>0$ 而 $p_{\\theta}(\\boldsymbol{x}) \\rightarrow 0$ 时，前向 $\\text{KL}$ 散度会变得非常大。这意味着前向 $\\text{KL}$ 散度会鼓励模型分布 $p_{\\theta}(\\boldsymbol{x})$ 尽可能覆盖所有真实分布 $p_{r}(\\boldsymbol{x})>$ 0 的点。\",\n    \"问题8\": \"在逆向 $\\text{KL}$ 散度中，当 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 而 $p_{\\theta}(\\boldsymbol{x})>0$ 时会发生什么情况？\",\n    \"回答8\": \"在逆向 $\\text{KL}$ 散度中，当 $p_{r}(\\boldsymbol{x}) \\rightarrow 0$ 而 $p_{\\theta}(\\boldsymbol{x})>0$ 时，$\\text{逆向 KL}$ 散度会变得非常大。即当 $p_{r}(\\boldsymbol{x})$ 接近于 0 ，而 $p_{\\theta}(\\boldsymbol{x})$ 有一定的密度时，逆向 KL 散度会变得非常大。\",\n    \"问题9\": \"在逆向 $\\text{KL}$ 散度中，当 $p_{\\theta}(\\boldsymbol{x}) \\rightarrow 0$ 时会发生什么情况？\",\n    \"回答9\": \"在逆向 KL 散度中，当 $p_{\\theta}(\\boldsymbol{x}) \\rightarrow 0$ 时，不管 $p_{r}(\\boldsymbol{x})$ 如何取值, $p_{\\theta}(\\boldsymbol{x}) \\rightarrow 0$. 这意味着当模型分布接近于零时，不管真实分布如何取值，对逆向 KL 散度的计算没有贡献。\",\n    \"问题10\": \"图13.11中黑色曲线和红色曲线分别代表什么？\",\n    \"回筗10\": \"图13.11中黑色曲线为真实分布$p_r$(x)的等高线,红色曲线为模型分布$p_\\theta$(x)的等高线。黑色曲线展示了真实数据的概率密度分布情况，而红色曲线展示了生成模型对数据的概率密度估计情况。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710761862,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1972,
            "prompt_tokens": 4445,
            "total_tokens": 6417
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]