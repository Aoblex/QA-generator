[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.2.3.3 随机梯度下降法",
        "content_length": 1210,
        "content": "Title: 2.2.3.3 随机梯度下降法\nContent:\n在公式 (2.27) 的梯度下降法中, 目标函数是整个训练集上的风险函数, 这种方式称为批量梯度下降法 ( Batch Gradient Descent,BGD ). 批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和. 当训练集中的样本数量 $N$ 很大时, 空间复杂度比较高, 每次迭代的计算开销也很大.\n\n在机器学习中, 我们假设每个样本都是独立同分布地从真实数据分布中随机抽取出来的，真正的优化目标是期望风险最小. 批量梯度下降法相当于是从真实数据分布中采集 $N$ 个样本, 并由它们计算出来的经验风险的梯度来近似期望风险的梯度. 为了减少每次迭代的计算复杂度, 我们也可以在每次迭代时只采集一个样本, 计算这个样本损失函数的梯度并更新参数, 即随机梯度下降法 (Stochastic Gradient Descent, SGD ). 当经过足够次数的迭代时, 随机梯度下降也可以收敛到局部最优解 [Nemirovski et al., 2009].\n\n随机梯度下降法的训练过程如算法2.1所示.\n\n\\section*{算法 2.1 随机梯度下降法}\n输入: 训练集 $\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, 验证集 $\\mathcal{V}$, 学习率 $\\alpha$\n\n1 随机初始化 $\\theta$;\n\n\\section*{2 repeat}\n3\n\n$4 \\quad$ for $n=1 \\cdots N$ do随机梯度下降法也叫作增量梯度下降法。\\\\\n5\n\n6\n\n7 end\n\n8 until 模型 $f(\\boldsymbol{x} ; \\theta)$ 在验证集 $\\mathcal{V}$ 上的错误率不再下降;\n\n输出: $\\theta$\n\n从训练集 $\\mathcal{D}$ 中选取样本 $\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)$;\n\n$\\theta \\leftarrow \\theta-\\alpha \\frac{\\partial \\mathcal{L}\\left(\\theta ; x^{(n)}, y^{(n)}\\right)}{\\partial \\theta} ;$\n\n// 更新参数\n\nhttps:/nndl.github.io/\\\\\n批量梯度下降和随机梯度下降之间的区别在于, 每次迭代的优化目标是对所有样本的平均损失函数还是对单个样本的损失函数. 由于随机梯度下降实现简单, 收玫速度也非常快, 因此使用非常广泛. 随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声. 在非凸优化问题中, 随机梯度下降更容易逃离局部最优点."
    },
    {
        "id": "chatcmpl-940k8EzvIaZBY5pNg7ZMC4jaNxaKd",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"批量梯度下降法和随机梯度下降法的计算复杂度有何区别？\",\n    \"回答1\": \"批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和，因此当训练集中的样本数量 $N$ 很大时，空间复杂度比较高，每次迭代的计算开销也很大。而随机梯度下降法在每次迭代时只需要计算一个样本的损失函数的梯度，因此计算复杂度相对较低。\",\n    \"问题2\": \"随机梯度下降法是如何近似期望风险的梯度的？\",\n    \"回答2\": \"随机梯度下降法相当于是从真实数据分布中采集 $N$ 个样本，并由它们计算出来的经验风险的梯度来近似期望风险的梯度。在每次迭代时，随机梯度下降法通过计算单个样本损失函数的梯度来逼近期望风险的梯度。\",\n    \"问题3\": \"随机梯度下降法的训练过程中如何更新参数？\",\n    \"回答3\": \"在随机梯度下降法的训练过程中，从训练集 $\\boldsymbol{D}$ 中选取样本 $\\big(\\boldsymbol{x}^{(n)}, y^{(n)}\\big)$ 后，通过参数更新公式 $\\theta \\rightarrow \\theta-\\text{α} \\frac{\\text{∂}\\boldsymbol{L}\\big(\\theta ; \\boldsymbol{x}^{(n)}, y^{(n)}\\big)}{\\text{∂}\\theta}$ 来更新参数，其中 $\\text{α}$ 为学习率。\",\n    \"问题4\": \"随机梯度下降法在机器学习中有何优势？\",\n    \"回答4\": \"随机梯度下降法实现简单，收敛速度非常快，因此在机器学习中使用非常广泛。相比批量梯度下降，随机梯度下降能够更快地进行参数更新和模型优化，尤其对于大规模数据集和高维特征空间具有明显优势。\",\n    \"问题5\": \"随机梯度下降法相对于批量梯度下降法引入了什么因素？\",\n    \"回答5\": \"随机梯度下降法相当于在批量梯度下降的梯度上引入了随机噪声。由于每次迭代只计算单个样本损失函数的梯度，因此引入了随机性，使得优化过程更加灵活和多样化。\",\n    \"问题6\": \"在非凸优化问题中，随机梯度下降法相比批量梯度下降法有何优势？\",\n    \"回答6\": \"在非凸优化问题中，随机梯度下降更容易逃离局部最优点。由于引入了随机噪声并且每次迭代只计算单个样本损失函数的梯度，随机梯度下降能够更灵活地搜索参数空间，并有更大可能性找到全局最优解。\",\n    \"问题7\": \"什么是批量梯度下降法和随机梯度下降法之间的主要区别？\",\n    \"回答7\": \"批量梯度下降法每次迭代的优化目标是对所有样本的平均损失函数，而随机梯度下降法每次迭代的优化目标是对单个样本的损失函数。这导致两者在参数更新方式、收敛速度和计算复杂度等方面存在显著差异。\",\n    \"问题8\": \"为什么需要在随机梯度下降法中使用学习率？\",\n    \"回答8\": \"在随机梯度下降法中使用学习率是为了控制参数更新的步长。学习率可以影响参数更新的幅度，过大或过小的学习率都可能导致优化过程出现问题。合适的学习率能够使得模型更快地收敛到最优解。\",\n    \"问题9\": \"如何判断随机梯度下降法是否收敛到局部最优解？\",\n    \"回答9\": \"经过足够次数的迭代后，可以观察模型 $f(\\boldsymbol{x} ; \\theta)$ 在验证集 $\\boldsymbol{V}$ 上的错误率。如果错误率不再下降，则可以判断模型已经收敛到局部最优解。这种方法可以作为判断随机梯度下降是否收敛的一种指标。\",\n    \"问题10\": \"算法2.1展示了随机梯度下降法的训练过程，请问该算法中哪一步是实现参数更新的关键步骤？\",\n    \"回答10\": \"在算法2.1中，实现参数更新的关键步骤是从训练集 $\\boldsymbol{D}$ 中选取样本 $\\big(\\boldsymbol{x}^{(n)}, y^{(n)}\\big)$ 后，通过参数更新公式 $\\theta \\rightarrow \\theta-\\text{α} \\frac{\\text{∂}\\boldsymbol{L}\\big(\\theta ; \\boldsymbol{x}^{(n)}, y^{(n)}\\big)}{\\text{∂}\\theta}$ 来更新参数，其中 $\\text{α}$ 为学习率。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710743196,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1611,
            "prompt_tokens": 3496,
            "total_tokens": 5107
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]