[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "5.2.4 卷积网络的整体结构",
        "content_length": 1958,
        "content": "Title: 5.2.4 卷积网络的整体结构\nContent:\n一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成. 目前常用的卷积网络整体结构如图5.9所示. 一个卷积块为连续 $M$ 个卷积层和 $b$ 个汇聚层 ( $M$ 通常设置为 $2 \\sim 5, b$ 为 0 或 1 ). 一个卷积网络中可以堆叠 $N$ 个连续的卷积块, 然后在后面接着 $K$ 个全连接层 ( $N$ 的取值区间比较大, 比如 $1 \\sim 100$ 或者更大; $K$ 一般为 $0 \\sim 2$ ).\n\n\n\n图 5.9 常用的卷积网络整体结构\n\n目前, 卷积网络的整体结构趋向于使用更小的卷积核 $($ 比如 $1 \\times 1$ 和 $3 \\times 3$ )以及更深的结构 ( 比如层数大于 50 ). 此外, 由于卷积的操作性越来越灵活 ( 比如不同的步长), 汇聚层的作用也变得越来越小, 因此目前比较流行的卷积网络中, 汇聚层的比例正在逐渐降低, 趋向于全卷积网络.\n\n\\section*{5.3 参数学习}\n在卷积网络中, 参数为卷积核中权重以及偏置. 和全连接前馈网络类似, 卷积网络也可以通过误差反向传播算法来进行参数学习.\n\n在全连接前馈神经网络中, 梯度主要通过每一层的误差项 $\\delta$ 进行反向传播,并进一步计算每层参数的梯度.\n\n参见公式(4.63).\n\n在卷积神经网络中, 主要有两种不同功能的神经层: 卷积层和汇聚层. 而参数为卷积核以及偏置, 因此只需要计算卷积层中参数的梯度.\n\n不失一般性, 对第 $l$ 层为卷积层, 第 $l-1$ 层的输入特征映射为 $\\mathcal{X}^{(l-1)} \\in$ $\\mathbb{R}^{M \\times N \\times D}$, 通过卷积计算得到第 $l$ 层的特征映射净输入 $z^{(l)} \\in \\mathbb{R}^{M^{\\prime} \\times N^{\\prime} \\times P}$. 第 $l$ 层这里假设汇聚层中没有参数.\n\n参见公式(5.23).的第 $p(1 \\leq p \\leq P)$ 个特征映射净输入\n\n\n\\begin{equation*}\n\\boldsymbol{Z}^{(l, p)}=\\sum_{d=1}^{D} \\boldsymbol{W}^{(l, p, d)} \\otimes \\boldsymbol{X}^{(l-1, d)}+b^{(l, p)} \\tag{5.28}\n\\end{equation*}\n\n\n其中 $\\boldsymbol{W}^{(l, p, d)}$ 和 $b^{(l, p)}$ 为卷积核以及偏置. 第 $l$ 层中共有 $P \\times D$ 个卷积核和 $P$ 个偏置, 可以分别使用链式法则来计算其梯度.\n\n根据公式 (5.17) 和公式 (5.28), 损失函数 $\\mathcal{L}$ 关于第 $l$ 层的卷积核 $\\boldsymbol{W}^{(l, p, d)}$ 的偏参见公式(5.17).导数为\n\n\n\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{W}^{(l, p, d)}} & =\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{Z}^{(l, p)}} \\otimes \\boldsymbol{X}^{(l-1, d)}  \\tag{5.29}\\\\\n& =\\delta^{(l, p)} \\otimes \\boldsymbol{X}^{(l-1, d)} \\tag{5.30}\n\\end{align*}\n\n\n其中 $\\delta^{(l, p)}=\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{Z}^{(l, p)}}$ 为损失函数关于第 $l$ 层的第 $p$ 个特征映射净输入 $\\boldsymbol{Z}^{(l, p)}$ 的偏导数.\n\n同理可得, 损失函数关于第 $l$ 层的第 $p$ 个偏置 $b^{(l, p)}$ 的偏导数为\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{L}}{\\partial b^{(l, p)}}=\\sum_{i, j}\\left[\\delta^{(l, p)}\\right]_{i, j} \\tag{5.31}\n\\end{equation*}\n\n\n在卷积网络中, 每层参数的梯度依赖其所在层的误差项 $\\delta^{(l, p)}$."
    },
    {
        "id": "chatcmpl-941pDF70hUMOvR3mNxbkpWVVzcMc0",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"卷积网络的整体结构包括哪些主要层？\",\n    \"回答1\": \"一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成。一个卷积块通常包括连续 $M$ 个卷积层和 $b$ 个汇聚层，而一个卷积网络中可以堆叠 $N$ 个连续的卷积块，然后在后面接着 $K$ 个全连接层。\",\n    \"问题2\": \"目前卷积网络整体结构的趋势是什么？\",\n    \"回答2\": \"目前卷积网络整体结构趋向于使用更小的卷积核（比如 $1 \\times 1$ 和 $3 \\times 3$）以及更深的结构（比如层数大于 50）。此外，由于卷积的操作性越来越灵活，汇聚层的作用也变得越来越小，因此目前比较流行的卷积网络中，汇聚层的比例正在逐渐降低，趋向于全卷积网络。\",\n    \"问题3\": \"在卷积网络中，参数学习是如何进行的？\",\n    \"回答3\": \"在卷积网络中，参数为卷积核中权重以及偏置。类似于全连接前馈神经网络，卷积网络也可以通过误差反向传播算法来进行参数学习。主要有两种不同功能的神经层：卷积层和汇聚层。参数为卷积核以及偏置，因此只需要计算卷积层中参数的梯度。\",\n    \"问题4\": \"在卷积神经网络中，损失函数关于卷积核的偏导数是如何计算的？\",\n    \"回答4\": \"损失函数 $\\text{L}$ 关于第 $l$ 层的卷积核 $\\boldsymbol{W}^{(l, p, d)}$ 的偏导数可以通过链式法则计算得到。具体而言，损失函数关于第 $l$ 层的第 $p$ 个特征映射净输入 $\\boldsymbol{Z}^{(l, p)}$ 的偏导数为 $\\frac{\\text{d} \\text{L}}{\\text{d} \\boldsymbol{W}^{(l, p, d)}} = \\frac{\\text{d} \\text{L}}{\\text{d} \\boldsymbol{Z}^{(l, p)}} \\bigotimes \\boldsymbol{X}^{(l-1, d)}$。\",\n    \"问题5\": \"在卷积神经网络中，损失函数关于偏置的偏导数是如何计算的？\",\n    \"回答5\": \"在卷积神经网络中，损失函数关于第 $l$ 层的第 $p$ 个偏置 $b^{(l, p)}$ 的偏导数可以通过对误差项 $\\boldsymbol{\\text{δ}}^{(l, p)}$ 进行求和得到。具体表达式为 $\\frac{\\text{d} \\text{L}}{\\text{d} b^{(l, p)}} = \\text{∑}_{i,j}\\big[\\boldsymbol{\\text{δ}}^{(l, p)}\\big]_{i,j}$。\",\n    \"问题6\": \"在卷积网络中，每层参数的梯度依赖于什么？\",\n    \"回答6\": \"在卷积网络中，每层参数的梯度依赖其所在层的误差项 $\\boldsymbol{\\text{δ}}^{(l, p)}$。对于损失函数关于第 $l$ 层的第 $p$ 个特征映射净输入 $\\boldsymbol{Z}^{(l, p)}$ 的偏导数，可以通过误差项和输入特征映射计算得到。\",\n    \"问题7\": \"什么是全连接前馈神经网络？\",\n    \"回答7\": \"全连接前馈神经网络是一种基本的神经网络结构，其中每一层神经元与下一层的所有神经元都有连接。这种结构使得信息能够从输入层传递到输出层，并且能够通过误差反向传播算法进行参数学习。\",\n    \"问题8\": \"汇聚层在当前流行的卷积网络中起到怎样的作用？\",\n    \"回答8\": \"由于卷积操作越来越灵活，汇聚层的作用在当前流行的卷积网络中变得越来越小。因此，汇聚层的比例正在逐渐降低，趋向于全卷积网络。这意味着在当前流行的卷积网络中，汇聚层对特征提取和信息处理起到了较小的作用。\",\n    \"问题9\": \"为什么目前常用的卷积网络整体结构趋向于更小的卷积核和更深的结构？\",\n    \"回答9\": \"目前常用的卷积网络整体结构趋向于更小的卷积核和更深的结构是因为这样能够提高模型对特征和信息的抽象能力，并且有助于提高模型对复杂数据和任务的表达能力和泛化能力。\",\n    \"问题10\": \"全连接层在典型的卷积网络整体结构中起到什么作用？\",\n    \"回答10\": \"在典型的卷积网络整体结构中，全连接层用于将前面堆叠得到的特征进行整合和映射，从而实现对最终输出结果的预测和分类。全连接层通常位于整个卷积网络结构的最后一部分，起到了整合和输出预测结果的作用。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710747355,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1650,
            "prompt_tokens": 3988,
            "total_tokens": 5638
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]