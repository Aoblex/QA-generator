[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "15.5.3.1 层次化 Softmax",
        "content_length": 3015,
        "content": "Title: 15.5.3.1 层次化 Softmax\nContent:\n我们先来考虑使用两层的树结构来组织词表, 即将词表中的词分成 $K$ 组, 并且每一个词只能属于一个分组, 每组大小为 $\\frac{|\\mathcal{V}|}{K}$. 假设词 $w$ 所属的组为 $c(w)$, 则\n\n\n\\begin{align*}\np(w \\mid \\tilde{h}) & =p(w, c(w) \\mid \\tilde{h})  \\tag{15.51}\\\\\n& =p(w \\mid c(w), \\tilde{h}) p(c(w) \\mid \\tilde{h}) \\tag{15.52}\n\\end{align*}\n\n\n其中 $p(c(w) \\mid \\tilde{h})$ 是给定历史信息 $\\tilde{h}$ 条件下, 类 $c(w)$ 的后验概率, $p(w \\mid c(w), \\tilde{h})$ 是给定历史信息 $\\tilde{h}$ 和类 $c(w)$ 条件下, 词 $w$ 的后验概率. 因此,一个词的概率可以分解为两个概率 $p(w \\mid c(w), \\tilde{h})$ 和 $p(c(w) \\mid \\tilde{h})$ 的乘积, 它们可以分别利用神经网络来估计, 这样计算 Softmax 函数时分别只需要做 $\\frac{|\\mathcal{V}|}{K}$ 和 $K$ 次求和, 从而大大提高了 Softmax 函数的计算速度.\n\n一般对于词表大小 $|\\mathcal{V}|$, 我们将词平均分到 $\\sqrt{|\\mathcal{V}|}$ 个分组中, 每组 $\\sqrt{|\\mathcal{V}|}$ 个词.这样通过一层的分组, 我们可以将 Softmax 计算加速 $\\frac{1}{2} \\sqrt{|\\mathcal{V}|}$ 倍. 比如当词表大小为 40,000 时, 将词表中所有词分到 200 组, 每组 200 个词. 这样只需要计算两次 200 类的 Softmax, 比直接计算 40,000 类的 Softmax 加快 100 倍.\n\n为了进一步降低 Softmax 函数的计算复杂度, 我们可以使用更深层的树结构来组织词汇表. 假设用二叉树来组织词表中的所有词, 二叉树的叶子节点代表 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n词表中的词, 非叶子节点表示不同层次上的类别. 图15.4给出了平衡二叉树和霍夫曼编码二叉树的示例.\n\n\n\n(a) 平衡树\n\n\n\n(b) 霍夫曼编码树\n\n图 15.4 层次化树结构\n\n如果我们将二叉树上所有左连边标记为 0 , 右连边标记为 1 . 每一个词可以用根节点到它所在的叶子之间路径上的标记来进行编码. 图15.4a中所示的四个词的编码分别为:\n\n\n\\begin{equation*}\nv_{1}=00, \\quad v_{2}=01, \\quad v_{3}=10, \\quad v_{4}=11 \\tag{15.53}\n\\end{equation*}\n\n\n假设词 $v$ 在二叉树上从根节点到其所在叶子节点的路径长度为 $M$, 其编码可以表示一个位向量 ( bit vector ): $\\left[b_{1}, \\cdots, b_{M}\\right]^{\\top}$. 词 $v$ 的条件概率为\n\n\n\\begin{align*}\nP(v \\mid \\tilde{h}) & =p\\left(b_{1}, \\cdots, b_{M} \\mid \\tilde{h}\\right)  \\tag{15.54}\\\\\n& =\\prod_{m=1}^{M} p\\left(b_{m} \\mid b_{1}, \\cdots, b_{m-1}, \\tilde{h}\\right),  \\tag{15.55}\\\\\n& =\\prod_{m=1}^{M} p\\left(b_{m} \\mid b_{m-1}, \\tilde{h}\\right) . \\tag{15.56}\n\\end{align*}\n\n\n由于 $b_{m} \\in\\{0,1\\}$ 为二值变量, 可以将 $p\\left(b_{m} \\mid b_{m-1}, \\tilde{h}\\right)$ 看作二分类问题, 使用 Logistic 回归来进行预测 :\n\n\n\\begin{equation*}\np\\left(b_{m}=1 \\mid b_{m-1}, \\tilde{h}\\right)=\\sigma\\left(\\boldsymbol{w}_{n\\left(b_{m-1}\\right)}^{\\top} \\boldsymbol{h}+\\boldsymbol{b}_{n\\left(b_{m-1}\\right)}\\right), \\tag{15.57}\n\\end{equation*}\n\n\n其中 $n\\left(b_{m-1}\\right)$ 为词 $v$ 在树 $T$ 上的路径上的第 $m-1$ 个节点.\n\n若使用平衡二叉树来进行分组, 则条件概率估计可以转换为 $\\log _{2}|\\mathcal{V}|$ 个二分类问题. 这时原始预测模型中的 Softmax 函数可以用 Logistic 函数代替, 计算效率可以加速 $\\frac{|\\mathcal{V}|}{\\log _{2}|\\mathcal{V}|}$ 倍.\n\n将词表中的词按照树结构进行组织, 有以下几种转换方式:\n\n（1）利用人工整理的词汇层次结构, 比如利用 WordNet[Miller, 1995] 系统中的 “IS-A” 关系 (即上下位关系). 例如, “狗” 是 “动物” 的下位词. 因为 WordNet 的层次化结构不是二叉树, 因此需要通过进一步聚类来转换为二叉树.\n\n（2）使用霍夫曼编码 ( Huffman Coding ). 霍夫曼编码对出现概率高的词使用较短的编码, 出现概率低的词使用较长的编码. 因此训练速度会更快. 霍夫曼编码的算法如算法15.1所示.\n\n算法 15.1 霍夫曼编码树构建算法\n\n输入: 词表: $\\mathcal{V}$\n\n1 初始化: 为每个词 $v$ 建立一个叶子节点, 其概率为词的出现频率;\n\n2 将所有的叶子节点放入集合 $\\mathcal{S}$ 中;\n\nwhile $|\\mathcal{S}|>1$ do\n\n4 从集合 $\\mathcal{S}$ 中选择两个概率最低的节点 $n_{1}$ 和 $n_{2}$;\n\n5 构建一个新节点 $n^{\\prime}$, 并将 $n_{1}$ 和 $n_{2}$ 作为 $n^{\\prime}$ 的左右子节点;\n\n6 新节点 $n^{\\prime}$ 的概率为 $n_{1}$ 和 $n_{2}$ 的概率之和;\n\n7 将新节点 $n^{\\prime}$ 加入集合 $\\mathcal{S}$ 中,并把 $n_{1}$ 和 $n_{2}$ 从集合 $\\mathcal{S}$ 中移除;\n\n8 end\n\n9 集合 $\\mathcal{S}$ 中最后一个节点为 $n$;\n\n输出: 以 $n$ 为根节点的二叉树 $T$"
    },
    {
        "id": "chatcmpl-946k3ilgWvwwFUNB09J0wjoUDH6VS",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"使用两层的树结构来组织词表有什么作用？\",\n    \"回答1\": \"使用两层的树结构可以将词表中的词分成 $K$ 组，每个词只能属于一个分组，从而实现对词汇的层次化组织。这种组织方式有助于提高 Softmax 函数的计算速度，通过分组计算可以大大加快 Softmax 函数的计算速度。\",\n    \"问题2\": \"词 $w$ 的概率是如何分解的？\",\n    \"回答2\": \"词 $w$ 的概率可以分解为两个概率 $p(w \\text{ 且 } c(w) \\text{ 在 } \\tilde{h} \\text{ 条件下})$ 和 $p(c(w) \\text{ 在 } \\tilde{h} \\text{ 条件下})$ 的乘积。这样的分解方式有助于利用神经网络分别估计这两个概率，从而提高 Softmax 函数的计算效率。\",\n    \"问题3\": \"如何使用更深层的树结构来降低 Softmax 函数的计算复杂度？\",\n    \"回答3\": \"为了进一步降低 Softmax 函数的计算复杂度，可以使用更深层的树结构来组织词汇表。假设用二叉树来组织词表中的所有词，二叉树的叶子节点代表词表中的词，非叶子节点表示不同层次上的类别。通过这种方式，可以将原始预测模型中的 Softmax 函数用 Logistic 函数代替，从而加速计算效率。\",\n    \"问题4\": \"如何将词表中的词按照树结构进行组织？\",\n    \"回答4\": \"将词表中的词按照树结构进行组织有两种转换方式：一是利用人工整理的词汇层次结构，例如利用 WordNet 系统中的 “IS-A” 关系；二是使用霍夫曼编码。霍夫曼编码对出现概率高的词使用较短的编码，出现概率低的词使用较长的编码，从而提高训练速度。\",\n    \"问题5\": \"二叉树上每一个词如何进行编码？\",\n    \"回答5\": \"如果我们将二叉树上所有左连边标记为 0 , 右连边标记为 1 , 每一个词可以用根节点到它所在的叶子之间路径上的标记来进行编码。这样每个词在二叉树上的路径长度为 $M$，其编码可以表示为一个位向量（bit vector）$\\big[b_{1}, \\text{...}, b_{M}\\big]^{\\top}$。\",\n    \"问题6\": \"词 $v$ 的条件概率是如何计算的？\",\n    \"回答6\": \"词 $v$ 的条件概率可以表示为 $\\text{P}(v \\text{ 在 } \\tilde{h} \\text{ 条件下})=\\text{P}\\big(b_{1}, \\text{...}, b_{M} \\text{ 在 } \\tilde{h}\\big)=\\text{∏}_{m=1}^{M} \\text{P}\\big(b_{m} \\text{ 在 } b_{1}, \\text{...}, b_{m-1}, \\tilde{h}\\big)$。由于 $b_{m} \\text{ 是二值变量}$，因此可以将 $\\text{P}\\big(b_{m} \\text{ 在 } b_{m-1}, \\tilde{h}\\big)$ 看作二分类问题，使用 Logistic 回归来进行预测。\",\n    \"问题7\": \"平衡二叉树在条件概率估计中起到了什么作用？\",\n    \"回答7\": \"如果使用平衡二叉树来进行分组，则条件概率估计可以转换为 $\\text{log}_2|\\text{V}|$ 个二分类问题。这时原始预测模型中的 Softmax 函数可以用 Logistic 函数代替，从而加速计算效率。\",\n    \"问题8\": \"霍夫曼编码树构建算法是如何进行的？\",\n    \"回答8\": \"霍夫曼编码树构建算法首先为每个词建立一个叶子节点，并初始化其概率为词的出现频率。然后选择两个概率最低的节点构建新节点，并将新节点加入集合中，重复该过程直到集合中只剩下一个节点。最后以最后一个节点为根节点构建霍夫曼编码树。\",\n    \"问题9\": \"霍夫曼编码对训练速度有什么影响？\",\n    \"回答9\": \"霍夫曼编码对训练速度有积极影响。它对出现概率高的词使用较短的编码，出现概率低的词使用较长的编码，从而提高训练速度。\",\n    \"问题10\": \"如何利用 WordNet 中的 “IS-A” 关系来组织词汇层次结构？\",\n    \"回答10\": \"利用 WordNet 中的 “IS-A” 关系来组织词汇层次结构需要通过进一步聚类来转换为二叉树。例如，“狗” 是 “动物” 的下位词，因此需要通过聚类和转换操作将 WordNet 的层次化结构转换为二叉树结构。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710766255,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1534,
            "prompt_tokens": 4607,
            "total_tokens": 6141
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]