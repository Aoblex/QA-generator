[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "13.2.2 推断网络",
        "content_length": 3108,
        "content": "Title: 13.2.2 推断网络\nContent:\n为简单起见，假设 $q(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\phi)$ 是服从对角化协方差的高斯分布，\n\n\n\\begin{equation*}\nq(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\phi)=\\mathcal{N}\\left(\\boldsymbol{z} ; \\mu_{I}, \\sigma_{I}^{2} \\boldsymbol{I}\\right) \\tag{13.6}\n\\end{equation*}\n\n\\footnotetext{\\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n其中 $\\mu_{I}$ 和 $\\sigma_{I}^{2}$ 是高斯分布的均值和方差, 可以通过推断网络 $f_{I}(\\boldsymbol{x} ; \\phi)$ 来预测.\n\n\\[\n\\left[\\begin{array}{c}\n\\mu_{I}  \\tag{13.7}\\\\\n\\sigma_{I}^{2}\n\\end{array}\\right]=f_{I}(x ; \\phi)\n\\]\n\n其中推断网络 $f_{I}(\\boldsymbol{x} ; \\phi)$ 可以是一般的全连接网络或卷积网络, 比如一个两层的神经网络,\n\n\n\\begin{align*}\n\\boldsymbol{h} & =\\sigma\\left(\\boldsymbol{W}^{(1)} \\boldsymbol{x}+\\boldsymbol{b}^{(1)}\\right),  \\tag{13.8}\\\\\n\\mu_{I} & =\\boldsymbol{W}^{(2)} \\boldsymbol{h}+\\boldsymbol{b}^{(2)}  \\tag{13.9}\\\\\n\\sigma_{I}^{2} & =\\operatorname{softplus}\\left(\\boldsymbol{W}^{(3)} \\boldsymbol{h}+\\boldsymbol{b}^{(3)}\\right) \\tag{13.10}\n\\end{align*}\n\n\n其中 $\\boldsymbol{\\phi}$ 代表所有的网络参数 $\\left\\{\\boldsymbol{W}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{W}^{(3)}, \\boldsymbol{b}^{(1)}, \\boldsymbol{b}^{(2)}, \\boldsymbol{b}^{(3)}\\right\\}, \\sigma$ 和 softplus 为激\n\n$\\operatorname{softplus}(x)=\\log (1+$ $\\left.\\mathrm{e}^{x}\\right)$.活函数. 这里使用 softplus 激活函数是由于方差总是非负的. 在实际实现中, 也可以用一个线性层 ( 不需要激活函数 ) 来预测 $\\log \\left(\\sigma_{I}^{2}\\right)$.\n\n推断网络的目标 推断网络的目标是使得 $q(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\phi)$ 尽可能接近真实的后验 $p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta)$,\n\n需要找到一组网络参数 $\\phi^{*}$ 来最小化两个分布的 KL 散度, 即\n\n\n\\begin{equation*}\n\\phi^{*}=\\underset{\\phi}{\\arg \\min } \\mathrm{KL}(q(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\phi), p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta)) \\tag{13.11}\n\\end{equation*}\n\n\n然而, 直接计算上面的 KL 散度是不可能的, 因为 $p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta)$ 一般无法计算. 传统方法是利用采样或者变分法来近似推断. 基于采样的方法效率很低且估计也不是很准确, 所以一般使用的是变分推断方法, 即用简单的分布 $q$ 去近似复杂的分布 $p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta)$. 但是, 在深度生成模型中, $p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta)$ 通常比较复杂, 很难用简单分布去近似. 因此,我们需要找到一种间接计算方法.\n\n根据公式 (13.3) 可知, 变分分布 $q(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\phi)$ 与真实后验 $p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta)$ 的 KL 散度等于对数边际似然 $\\log p(x ; \\theta)$ 与其下界 $\\operatorname{ELBO}(q, \\boldsymbol{x} ; \\theta, \\phi)$ 的差, 即\n\n\n\\begin{equation*}\n\\operatorname{KL}(q(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\boldsymbol{\\phi}), p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta))=\\log p(\\boldsymbol{x} ; \\theta)-E L B O(q, \\boldsymbol{x} ; \\theta, \\boldsymbol{\\phi}), \\tag{13.12}\n\\end{equation*}\n\n\n因此,推断网络的目标函数可以转换为\n\n\n\\begin{align*}\n\\phi^{*} & =\\underset{\\phi}{\\arg \\min } \\operatorname{KL}(q(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\phi), p(\\boldsymbol{z} \\mid \\boldsymbol{x} ; \\theta))  \\tag{13.13}\\\\\n& =\\underset{\\phi}{\\arg \\min } \\log p(\\boldsymbol{x} ; \\theta)-E L B O(q, \\boldsymbol{x} ; \\theta, \\phi)  \\tag{13.14}\\\\\n& =\\underset{\\phi}{\\arg \\max } \\operatorname{ELBO}(q, \\boldsymbol{x} ; \\theta, \\phi), \\tag{13.15}\n\\end{align*}\n\n\n即推断网络的目标转换为寻找一组网络参数 $\\phi^{*}$ 使得证据下界 $\\operatorname{ELBO}(q, \\boldsymbol{x} ; \\theta, \\phi)$最大,这和变分推断中的转换类似.\n\n变分推断参见第11.4节。\n\n可以看作 EM 算法中的 E步。\n\n第一项与 $\\phi$ 无关.\n\n参见公式(11.85)."
    },
    {
        "id": "chatcmpl-945TVjqw1yNVBl9C4T4hKI2CLPlxB",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"推断网络中的 $q(\\boldsymbol{z} \\text{ 与 } \\boldsymbol{x} ; \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}}}}}}}}}}}}}}}}}}}}}}}$ 服从什么分布？\",\n    \"回答1\": \"$q(\\boldsymbol{z} \\text{ 与 } \\boldsymbol{x} ; \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\text{是服从对角化协方差的高斯分布，即}$\\text{N}\\big(\\textbf{z} ; \\textbf{$\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm\\bm$}, \\textbf{$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$\\b$I}\\big)$。\"}  \r\r    \r    \r    \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r   \r    \r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n\r\r\r\n",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710761385,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 488,
            "prompt_tokens": 4147,
            "total_tokens": 4635
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]