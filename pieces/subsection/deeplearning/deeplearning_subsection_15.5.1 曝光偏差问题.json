[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "15.5.1 曝光偏差问题",
        "content_length": 1648,
        "content": "Title: 15.5.1 曝光偏差问题\nContent:\n在自回归生成模型中, 第 $t$ 步的输入为模型生成的前缀序列 $\\hat{\\boldsymbol{x}}_{1:(t-1)}$. 而在训练时, 我们使用的前缀序列是训练集中的真实数据 $\\boldsymbol{x}_{1:(t-1)}$, 而不是模型预测的 $\\hat{\\boldsymbol{x}}_{1:(t-1)}$. 这种学习方式也称为教师强制 (Teacher Forcing) [Williams et al., 1989].\n\n这种教师强制的学习方式存在协变量偏移问题. 在训练时, 每步的输入 $\\boldsymbol{x}_{1:(t-1)}$ 来自于真实数据分布 $p_{r}\\left(\\boldsymbol{x}_{1:(t-1)}\\right)$; 而在测试时, 每步的输入 $\\hat{\\boldsymbol{x}}_{1:(t-1)}$ 来自协变量偏移问题参见第10.4.2节.于模型分布 $p_{\\theta}\\left(\\boldsymbol{x}_{1:(t-1)}\\right)$. 由于模型分布 $p_{\\theta}\\left(\\boldsymbol{x}_{1:(t-1)}\\right)$ 和真实数据分布 $p_{r}\\left(\\boldsymbol{x}_{1:(t-1)}\\right)$并不严格一致, 因此存在协变量偏移问题. 一旦在预测前缀 $\\hat{\\boldsymbol{x}}_{1:(t-1)}$ 的过程中存在错误, 会导致错误传播, 使得后续生成的序列也会偏离真实分布. 这个问题称为曝光偏差 (Exposure Bias) 问题.\n\n计划采样 为了缓解曝光偏差问题, 我们可以在训练时混合使用真实数据和模型生成数据. 在第 $t$ 步时, 模型随机使用真实数据 $x_{t-1}$ 或前一步生成的词 $\\hat{x}_{t-1}$ 作为输入.\n\n令 $\\epsilon \\in[0,1]$ 为一个控制替换率的超参数, 在每一步时, 以 $\\epsilon$ 的概率使用真实数据 $x_{t-1}$, 以 $1-\\epsilon$ 的概率使用生成数据 $\\hat{x}_{t-1}$. 当令 $\\epsilon=1$ 时, 训练和最大似然估计一样,使用真实数据; 当令 $\\epsilon=0$ 时,训练时全部使用模型生成数据.\n\n直觉上, 如果一开始训练时的 $\\epsilon$ 过小, 模型相当于在噪声很大的数据上训练,会导致模型性能变差, 并且难以收玫. 因此, 一个较好的策略是在训练初期赋予 $\\epsilon$ 较大的值, 随着训练次数的增加逐步减小 $\\epsilon$ 的取值. 这种策略称为计划采样 ( Scheduled Sampling ) [Bengio et al., 2015].\n\n令 $\\epsilon_{i}$ 为在第 $i$ 次迭代时的替换率, 在计划采样中可以通过下面几种方法来逐步减小 $\\epsilon$ 的取值.\n\n(1) 线性衰减: $\\epsilon_{i}=\\max (\\epsilon, k-c i)$, 其中 $\\epsilon$ 为最小的替换率, $k$ 和 $c$ 分别为初始值和衰减率。\n\n(2) 指数衰减: $\\epsilon_{i}=k^{i}$, 其中 $k<1$ 为初始替换率.\n\n(3) 逆 Sigmoid 衰减: $\\epsilon_{i}=k /(k+\\exp (i / k))$, 其中 $k \\geq 1$ 来控制衰减速度.\n\n计划采样的一个缺点是过度纠正, 即在每一步中不管输入如何选择, 目标输出依然是来自于真实数据. 这可能使得模型预测一些不正确的序列. 比如一个真实的序列是 “吃饭”, 如果在第一步生成时使用模型预测的词是 “喝”, 模型就会强制记住“喝饭”这个不正确的序列."
    },
    {
        "id": "chatcmpl-946g24lG1JvhwgCcdxsIXxRafn3il",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"教师强制（Teacher Forcing）学习方式存在什么问题？\",\n    \"回答1\": \"教师强制的学习方式存在协变量偏移问题。在训练时，输入来自于真实数据分布 $p_{r}\\big(\\boldsymbol{x}_{1:(t-1)}\\big)$；而在测试时，输入来自于模型分布 $p_{\\theta}\\big(\\boldsymbol{x}_{1:(t-1)}\\big)$。由于两者并不严格一致，导致了协变量偏移问题。\",\n    \"问题2\": \"曝光偏差（Exposure Bias）问题是指什么？\",\n    \"回答2\": \"曝光偏差问题指的是在预测前缀序列时存在错误，会导致错误传播，使得后续生成的序列也会偏离真实分布。这是由于训练时使用的前缀序列来自于真实数据分布，而在预测时使用的前缀序列来自于模型分布，导致了偏差问题。\",\n    \"问题3\": \"计划采样（Scheduled Sampling）是如何缓解曝光偏差问题的？\",\n    \"回答3\": \"计划采样通过在训练时混合使用真实数据和模型生成数据来缓解曝光偏差问题。在每一步时，以一定的概率使用真实数据或生成数据作为输入，通过调整替换率的超参数 $\\boldsymbol{\\text{ε}}$ 来平衡使用真实数据和模型生成数据。\",\n    \"问题4\": \"计划采样中的线性衰减、指数衰减和逆 Sigmoid 衰减分别是如何调整替换率的？\",\n    \"回答4\": \"在计划采样中，线性衰减通过 $\\boldsymbol{\\text{ε}}_{i}=\\text{max}(\\boldsymbol{\\text{ε}}, k-c i)$ 的方式逐步减小替换率；指数衰减通过 $\\boldsymbol{\\text{ε}}_{i}=k^{i}$ 的方式逐步减小替换率；逆 Sigmoid 衰减通过 $\\boldsymbol{\\text{ε}}_{i}=k /(k+\\text{exp}(i / k))$ 的方式逐步减小替换率。\",\n    \"问题5\": \"为什么需要逐步减小替换率 $\\boldsymbol{\\text{ε}}$ 的取值？\",\n    \"回答5\": \"逐步减小替换率 $\\boldsymbol{\\text{ε}}$ 的取值是为了在训练初期赋予较大的值，随着训练次数的增加逐步减小 $\\boldsymbol{\\text{ε}}$ 的取值。这样的策略有助于平衡模型在噪声数据上的训练和模型性能的提升。\",\n    \"问题6\": \"计划采样中存在的一个缺点是什么？\",\n    \"回答6\": \"计划采样的一个缺点是过度纠正，即在每一步中不管输入如何选择，目标输出依然是来自于真实数据。这可能使得模型预测一些不正确的序列，导致模型性能下降。\",\n    \"问题7\": \"什么是自回归生成模型中的教师强制（Teacher Forcing）学习方式？\",\n    \"回答7\": \"自回归生成模型中的教师强制学习方式是指在训练时使用的前缀序列来自于真实数据分布 $p_{r}\\big(\\boldsymbol{x}_{1:(t-1)}\\big)$，而不是模型预测的前缀序列 $\\boldsymbol{\\text{hat{x}}}_{1:(t-1)}$。这种学习方式用于训练自回归生成模型。\",\n    \"问题8\": \"协变量偏移问题是如何影响自回归生成模型的训练和测试过程？\",\n    \"回答8\": \"协变量偏移问题影响了自回归生成模型的训练和测试过程。在训练时，输入来自于真实数据分布；而在测试时，输入来自于模型分布。由于两者并不严格一致，导致了协变量偏移问题。\",\n    \"问题9\": \"为什么需要缓解曝光偏差问题？\",\n    \"回答9\": \"需要缓解曝光偏差问题是因为在预测前缀序列时存在错误会导致错误传播，使得后续生成的序列也会偏离真实分布。通过缓解曝光偏差问题可以提高自回归生成模型的预测准确性。\",\n    \"问题10\": \"计划采样中的替换率 $\\boldsymbol{\\text{ε}}$ 如何影响模型训练过程？\",\n    \"回答10\": \"计划采样中的替换率 $\\boldsymbol{\\text{ε}}$ 影响着模型在训练过程中对真实数据和生成数据的使用比例。通过调整替换率 $\\boldsymbol{\\text{ε}}$ 的大小，可以平衡模型在噪声数据上的训练和模型性能的提升。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710766006,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1460,
            "prompt_tokens": 3753,
            "total_tokens": 5213
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]