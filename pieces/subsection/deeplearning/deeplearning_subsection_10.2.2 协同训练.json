[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "10.2.2 协同训练",
        "content_length": 5732,
        "content": "Title: 10.2.2 协同训练\nContent:\n协同训练 ( Co-Training) 是自训练的一种改进方法, 通过两个基于不同视角 (view) 的分类器来互相促进. 很多数据都有相对独立的不同视角. 比如互联网上的每个网页都由两种视角组成: 文字内容 (text) 和指向其他网页的链接 \\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n( hyperlink ). 如果要确定一个网页的类别, 既可以根据文字内容来判断, 也可根据网页之间的链接关系来判断.\n\n假设一个样本 $\\boldsymbol{x}=\\left[\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}\\right]$, 其中 $\\boldsymbol{x}_{1}$ 和 $\\boldsymbol{x}_{2}$ 分别表示两种不同视角 $V_{1}$ 和 $V_{2}$ 的特征, 并满足下面两个假设. 1 ) 条件独立性: 给定样本标签 $y$ 时, 两种特征条件独立 $\\left.p\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2} \\mid y\\right)=p\\left(\\boldsymbol{x}_{1} \\mid y\\right) p\\left(\\boldsymbol{x}_{2} \\mid y\\right) ; 2\\right)$ 充足和觉余性: 当数据充分时, 每种视角的特征都足以单独训练出一个正确的分类器. 令 $y=g(x)$ 为需要学习的真实映射函数, $f_{1}$ 和 $f_{2}$ 分别为两个视角的分类器, 有\n\n\n\\begin{equation*}\n\\exists f_{1}, f_{2}, \\quad \\forall \\boldsymbol{x} \\in \\mathcal{X}, \\quad f_{1}\\left(\\boldsymbol{x}_{1}\\right)=f_{2}\\left(\\boldsymbol{x}_{2}\\right)=g(\\boldsymbol{x}), \\tag{10.23}\n\\end{equation*}\n\n\n其中 $X$ 为样本 $\\boldsymbol{x}$ 的取值空间.\n\n算法10.3给出了协同训练的训练过程. 协同算法要求两种视角是条件独立的. 如果两种视角完全一样, 则协同训练退化成自训练算法.\n\n\n\n由于不同视角的条件独立性, 在不同视角上训练出来的模型就相当于从不同视角来理解问题, 具有一定的互补性. 协同训练就是利用这种互补性来进行自训练的一种方法. 首先在训练集上根据不同视角分别训练两个模型 $f_{1}$ 和 $f_{2}$, 然后 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n用 $f_{1}$ 和 $f_{2}$ 在无标注数据集上进行预测, 各选取预测置信度比较高的样本加入训练集, 重新训练两个不同视角的模型,并不断重复这个过程.\n\n\\section*{10.3 多任务学习}\n一般的机器学习模型都是针对单一的特定任务, 比如手写体数字识别、物体检测等. 不同任务的模型都是在各自的训练集上单独学习得到的. 如果有两个任务比较相关, 它们之间会存在一定的共享知识, 这些知识对两个任务都会有所帮助. 这些共享的知识可以是表示 (特征)、模型参数或学习算法等. 目前, 主流的多任务学习方法主要关注表示层面的共享.\n\n多任务学习 (Multi-task Learning) 是指同时学习多个相关任务, 让这些任务在学习过程中共享知识, 利用多个任务之间的相关性来改进模型在每个任务上的性能和泛化能力. 多任务学习可以看作一种归纳迁移学习 (Inductive Transfer Learning ), 即通过利用包含在相关任务中的信息作为归纳偏置（Inductive Bias ) 来提高泛化能力 [Caruana, 1997].\n\n共享机制 多任务学习的主要挑战在于如何设计多任务之间的共享机制. 在传统的机器学习算法中, 引入共享的信息是比较困难的, 通常会导致模型变得复杂. 但是在神经网络模型中, 模型共享变得相对比较容易. 深度神经网络模型提供了一种很方便的信息共享方式, 可以很容易地进行多任务学习. 多任务学习的共享机制比较灵活, 有很多种共享模式. 图10.1给出了多任务学习中四种常见的共享模式, 其中 $A 、 B$ 和 $C$ 表示三个不同的任务, 红色框表示共享模块, 蓝色框表示任务特定模块.\n\n这四种常见的共享模式分别为:\n\n（1）硬共享模式：让不同任务的神经网络模型共同使用一些共享模块 (一般是低层) 来提取一些通用特征, 然后再针对每个不同的任务设置一些私有模块 (一般是高层) 来提取一些任务特定的特征.\n\n(2) 软共享模式: 不显式地设置共享模块, 但每个任务都可以从其他任务中 “窃取”一些信息来提高自己的能力. 窃取的方式包括直接复制使用其他任务的隐状态,或使用注意力机制来主动选取有用的信息.\n\n（3）层次共享模式:一般神经网络中不同层抽取的特征类型不同, 低层一般抽取一些低级的局部特征, 高层抽取一些高级的抽象语义特征. 因此如果多任务学习中不同任务也有级别高低之分, 那么一个合理的共享模式是让低级任务在低层输出,高级任务在高层输出.\n\n（4）共享-私有模式: 一个更加分工明确的方式是将共享模块和任务特定 (私有) 模块的责任分开. 共享模块捕捉一些跨任务的共享特征, 而私有模块只 \\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\n(a) 硬共享模式\n\n\n\n(c) 层次共享模式\n\n\n\n(b) 软共享模式\n\n\n\n(d) 共享-私有模式\n\n图 10.1 多任务学习中四种常见的共享模式\n\n捕捉和特定任务相关的特征. 最终的表示由共享特征和私有特征共同构成.\n\n学习步骤 在多任务学习中, 每个任务都可以有自己单独的训练集. 为了让所有任务同时学习,我们通常会使用交替训练的方式来“近似”地实现同时学习.\n\n假设有 $M$ 个相关任务,第 $m$ 个任务的训练集为 $\\mathcal{D}_{m}$, 包含 $N_{m}$ 个样本.\n\n\n\\begin{equation*}\n\\mathcal{D}_{m}=\\left\\{\\left(\\boldsymbol{x}^{(m, n)}, y^{(m, n)}\\right)\\right\\}_{n=1}^{N_{m}}, \\tag{10.24}\n\\end{equation*}\n\n\n其中 $\\boldsymbol{x}^{(m, n)}$ 和 $y^{(m, n)}$ 表示第 $m$ 个任务中的第 $n$ 个样本以及它的标签.\n\n假设这 $M$ 个任务对应的模型分别为 $f_{m}(\\boldsymbol{x} ; \\theta), 1 \\leq m \\leq M$, 多任务学习的联合目标函数为所有任务损失函数的线性加权.\n\n\n\\begin{equation*}\n\\mathcal{L}(\\theta)=\\sum_{m=1}^{M} \\sum_{n=1}^{N_{m}} \\eta_{m} \\mathcal{L}_{m}\\left(f_{m}\\left(x^{(m, n)} ; \\theta\\right), y^{(m, n)}\\right) \\tag{10.25}\n\\end{equation*}\n\n\n其中 $\\mathcal{L}_{m}(\\cdot)$ 为第 $m$ 个任务的损失函数, $\\eta_{m}$ 是第 $m$ 个任务的权重, $\\theta$ 表示包含了共享模块和私有模块在内的所有参数. 权重可以根据不同任务的重要程度来赋值,也可以根据任务的难易程度来赋值. 通常情况下, 所有任务设置相同的权重, 即 $\\eta_{m}=1,1 \\leq m \\leq M$.\n\n多任务学习的流程可以分为两个阶段:\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n（1）联合训练阶段:每次迭代时,随机挑选一个任务, 然后从这个任务中随机选择一些训练样本, 计算梯度并更新参数. 多任务学习中联合训练阶段的具体过程如算法10.4所示.\n\n\n\n（2）单任务精调阶段: 基于多任务学习得到的参数, 分别在每个单独任务进行精调 (Fine-Tuning). 其中单任务精调阶段为可选阶段. 当多个任务的差异性比较大时, 在每个单任务上继续优化参数可以进一步提升模型能力.\n\n多任务学习通常可以获得比单任务学习更好的泛化能力, 主要有以下几个原因:\n\n(1) 多任务学习在多个任务的数据集上进行训练, 比单任务学习的训练集更大. 由于多个任务之间有一定的相关性, 因此多任务学习相当于是一种隐式的数据增强, 可以提高模型的泛化能力.\n\n（2 ）多任务学习中的共享模块需要兼顾所有任务, 这在一定程度上避免了模型过拟合到单个任务的训练集,可以看作一种正则化.\n\n（3）既然一个好的表示通常需要适用于多个不同任务, 多任务学习的机制 参见第1.3节.使得它会比单任务学习获得更好的表示.\n\n（4）在多任务学习中, 每个任务都可以“选择性” 利用其他任务中学习到的隐藏特征, 从而提高自身的能力.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\\section*{10.4 迁移学习}\n标准机器学习的前提假设是训练数据和测试数据的分布是相同的. 如果不满足这个假设, 在训练集上学习到的模型在测试集上的表现会比较差. 而在很多实际场景中, 经常碰到的问题是标注数据的成本十分高, 无法为一个目标任务准备足够多相同分布的训练数据. 因此, 如果有一个相关任务已经有了大量的训练数据, 虽然这些训练数据的分布和目标任务不同, 但是由于训练数据的规模比较大, 我们假设可以从中学习某些可以泛化的知识, 那么这些知识对目标任务会有一定的帮助. 如何将相关任务的训练数据中的可泛化知识迁移到目标任务上, 就是迁移学习 ( Transfer Learning) 要解决的问题.\n\n具体而言, 假设一个机器学习任务 $\\mathcal{T}$ 的样本空间为 $x \\times y$, 其中 $x$ 为输入空间, $y$ 为输出空间, 其概率密度函数为 $p(x, y)$. 为简单起见, 这里设 $x$ 为 $D$ 维实数空间的一个子集, $y$ 为一个离散的集合.\n\n$p(x, y)=P(X=$ $x, Y=y)$.\n\n一个样本空间及其分布可以称为一个领域 (Domain ) : $\\mathcal{D}=(\\mathcal{X}, \\boldsymbol{y}, p(\\boldsymbol{x}, y)$ ).给定两个领域, 如果它们的输入空间、输出空间或概率分布中至少一个不同, 那么这两个领域就被认为是不同的. 从统计学习的观点来看, 一个机器学习任务 $\\mathcal{T}$定义为在一个领域 $\\mathcal{D}$ 上的条件概率 $p(y \\mid \\boldsymbol{x})$ 的建模问题.\n\n迁移学习是指两个不同领域的知识迁移过程, 利用源领域 (Source Domain) $\\mathcal{D}_{S}$ 中学到的知识来帮助目标领域 (Target Domain) $\\mathcal{D}_{T}$ 上的学习任务. 源领域的训练样本数量一般远大于目标领域.\n\n表10.1给出了迁移学习和标准机器学习的比较.\n\n表 10.1 迁移学习和标准机器学习的比较\n\n\\begin{center}\n\\begin{tabular}{c|c|c}\n\\hline\n学习类型 & 样本空间 & 概率分布 \\\\\n\\hline\n标准机器学习 & $x_{S}=x_{T}, y_{S}=y_{T}$ & $p_{S}(\\boldsymbol{x}, y)=p_{T}(\\boldsymbol{x}, y)$ \\\\\n\\hline\n迁移学习 & $x_{S} \\neq x_{T}$ 或 $y_{S} \\neq y_{T}$ 或 $p_{S}(\\boldsymbol{x}, y) \\neq p_{T}(\\boldsymbol{x}, y)$ &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n迁移学习根据不同的迁移方式又分为两个类型: 归纳迁移学习 (Inductive Transfer Learning) 和转导迁移学习 (Transductive Transfer Learning). 这两个类型分别对应两个机器学习的范式: 归纳学习 (Inductive Learning) 和转导学习 (Transductive Learning) [Vapnik, 1998]. 一般的机器学习都是指归纳学习, 即希望在训练数据集上学习到使得期望风险 (即真实数据分布上的错误率)最小的模型. 而转导学习的目标是学习一种在给定测试集上错误率最小的模型,期望风险参见第2.2.2节。在训练阶段可以利用测试集的信息.\n\\footnotetext{\\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n归纳迁移学习是指在源领域和任务上学习出一般的规律, 然后将这个规律迁移到目标领域和任务上; 而转导迁移学习是一种从样本到样本的迁移, 直接利用源领域和目标领域的样本进行迁移学习."
    },
    {}
]