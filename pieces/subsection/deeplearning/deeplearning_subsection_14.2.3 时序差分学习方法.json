[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "14.2.3 时序差分学习方法",
        "content_length": 3520,
        "content": "Title: 14.2.3 时序差分学习方法\nContent:\n蒙特卡罗方法一般需要拿到完整的轨迹, 才能对策略进行评估并更新模型,因此效率也比较低. 时序差分学习 (Temporal-Difference Learning) 方法是蒙特卡罗方法的一种改进, 通过引入动态规划算法来提高学习效率 [Sutton et al., 2018]. 时序差分学习方法是模拟一段轨迹, 每行动一步 (或者几步), 就利用贝尔曼方程来评估行动前状态的价值.\n\n首先, 将蒙特卡罗方法中 $\\mathrm{Q}$ 函数 $\\hat{Q}^{\\pi}(s, a)$ 的估计改为增量计算的方式, 假设第 $N$ 次试验后值函数 $\\hat{Q}_{N}^{\\pi}(s, a)$ 的平均为\n\n\n\\begin{align*}\n\\hat{Q}_{N}^{\\pi}(s, a) & =\\frac{1}{N} \\sum_{n=1}^{N} G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(n)}\\right)  \\tag{14.32}\\\\\n& =\\frac{1}{N}\\left(G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(N)}\\right)+\\sum_{n=1}^{N-1} G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(n)}\\right)\\right)  \\tag{14.33}\\\\\n& =\\frac{1}{N}\\left(G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(N)}\\right)+(N-1) \\hat{Q}_{N-1}^{\\pi}(s, a)\\right)  \\tag{14.34}\\\\\n& =\\hat{Q}_{N-1}^{\\pi}(s, a)+\\frac{1}{N}\\left(G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(N)}\\right)-\\hat{Q}_{N-1}^{\\pi}(s, a)\\right) \\tag{14.35}\n\\end{align*}\n\n\n其中 $\\tau_{s_{0}=s, a_{0}=a}$ 表示轨迹 $\\tau$ 的起始状态和动作为 $s, a$.\n\n值函数 $\\hat{Q}^{\\pi}(s, a)$ 在第 $N$ 试验后的平均等于第 $N-1$ 试验后的平均加上一个增量. 更一般性地, 我们将权重系数 $\\frac{1}{N}$ 改为一个比较小的正数 $\\alpha$. 这样每次采用一个新的轨迹 $\\tau_{s_{0}=s, a_{0}=a}$, 就可以更新 $\\hat{Q}^{\\pi}(s, a)$.\n\n\n\\begin{equation*}\n\\hat{Q}^{\\pi}(s, a) \\leftarrow \\hat{Q}^{\\pi}(s, a)+\\alpha\\left(G\\left(\\tau_{s_{0}=s, a_{0}=a}\\right)-\\hat{Q}^{\\pi}(s, a)\\right), \\tag{14.36}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E5%8F%82%E8%A7%81%E7%AC%AC11.5.3%E8%8A%82%E3%80%82}{https://nndl.github.io/重要性采样参见第11.5.3节。}\n\n当时序差分学习方法中每次更新的动作数为最大步数时, 就等价于蒙特卡罗方法.\\\\\n其中增量 $\\delta \\triangleq G\\left(\\tau_{s_{0}=s, a_{0}=a}\\right)-\\hat{Q}^{\\pi}(s, a)$ 称为蒙特卡罗误差, 表示当前轨迹的真实回报 $G\\left(\\tau_{s_{0}=s, a_{0}=a}\\right)$ 与期望回报 $\\hat{Q}^{\\pi}(s, a)$ 之间的差距.\n\n在公式(14.36)中, $G\\left(\\tau_{s_{0}=s, a_{0}=a}\\right)$ 为一次试验的完整轨迹所得到的总回报. 为了提高效率, 可以借助动态规划的方法来计算 $G\\left(\\tau_{s_{0}=s, a_{0}=a}\\right)$, 而不需要得到完整的轨迹. 从 $s, a$ 开始, 采样下一步的状态和动作 $\\left(s^{\\prime}, a^{\\prime}\\right)$, 并得到奖励 $r\\left(s, a, s^{\\prime}\\right)$, 然后利用贝尔曼方程来近似估计 $G\\left(\\tau_{s_{0}=s, a_{0}=a}\\right)$,\n\n\n\\begin{align*}\nG\\left(\\tau_{s_{0}=s, a_{0}=a, s_{1}=s^{\\prime}, a_{1}=a^{\\prime}}\\right) & =r\\left(s, a, s^{\\prime}\\right)+\\gamma G\\left(\\tau_{s_{0}=s^{\\prime}, a_{0}=a^{\\prime}}\\right)  \\tag{14.37}\\\\\n& \\approx r\\left(s, a, s^{\\prime}\\right)+\\gamma \\hat{Q}^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right), \\tag{14.38}\n\\end{align*}\n\n\n其中 $\\hat{Q}^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)$ 是当前的 $\\mathrm{Q}$ 函数的近似估计.\n\n参见习题14-4.\n\n结合公式(14.36)和公式(14.38), 有\n\n\n\\begin{equation*}\n\\hat{Q}^{\\pi}(s, a) \\leftarrow \\hat{Q}^{\\pi}(s, a)+\\alpha\\left(r\\left(s, a, s^{\\prime}\\right)+\\gamma \\hat{Q}^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)-\\hat{Q}^{\\pi}(s, a)\\right), \\tag{14.39}\n\\end{equation*}\n\n\n因此, 更新 $\\hat{Q}^{\\pi}(s, a)$ 只需要知道当前状态 $s$ 和动作 $a$ 、奖励 $r\\left(s, a, s^{\\prime}\\right)$ 、下一步的状态 $s^{\\prime}$ 和动作 $a^{\\prime}$. 这种策略学习方法称为SARSA 算法 (State Action Reward State Action, SARSA ) [Rummery et al., 1994].\n\nSARSA 算法的学习过程如算法14.3所示, 其采样和优化的策略都是 $\\pi^{\\epsilon}$, 因此是一种同策略算法. 为了提高计算效率, 我们不需要对环境中所有的 $s, a$ 组合进行穷举, 并计算值函数. 只需要将当前的探索 $\\left(s, a, r, s^{\\prime}, a^{\\prime}\\right)$ 中 $s^{\\prime}, a^{\\prime}$ 作为下一次估计的起始状态和动作.\n\n\\\\\n时序差分学习是强化学习的主要学习方法, 其关键步骤就是在每次迭代中优化 $\\mathrm{Q}$ 函数来减少现实 $r+\\gamma Q\\left(s^{\\prime}, a^{\\prime}\\right)$ 和预期 $Q(s, a)$ 的差距. 这和动物学习的机制十分相像. 在大脑神经元中, 多巴胺的释放机制和时序差分学习十分吻合. [Schultz, 1998] 的一个实验中, 通过监测猴子大脑释放的多巴胺浓度, 发现如果猴子获得比预期更多的果汁，或者在没有预想到的时间喝到果汁, 多巴胺释放大增. 如果没有喝到本来预期的果汁, 多巴胺的释放就会大减. 多巴胺的释放, 来自对于实际奖励和预期奖励的差异,而不是奖励本身.\n\n时序差分学习方法和蒙特卡罗方法的主要不同为: 蒙特卡罗方法需要一条完整的路径才能知道其总回报, 也不依赖马尔可夫性质; 而时序差分学习方法只需要一步, 其总回报需要通过马尔可夫性质来进行近似估计."
    },
    {
        "id": "chatcmpl-946QwCboZPIhxsCIlb8Omyp0hveFj",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"时序差分学习方法相对于蒙特卡罗方法有哪些改进？\",\n    \"回答1\": \"时序差分学习方法相对于蒙特卡罗方法的改进之一是通过引入动态规划算法来提高学习效率。它不需要拿到完整的轨迹就能对策略进行评估并更新模型，因此具有更高的效率。\",\n    \"问题2\": \"时序差分学习方法中的增量计算方式是如何定义的？\",\n    \"回答2\": \"时序差分学习方法中的增量计算方式将值函数 $\\text{Q}^{\\text{π}}(s, a)$ 的估计改为增量计算的方式。具体而言，每次采用一个新的轨迹 $\\tau_{s_{0}=s, a_{0}=a}$，就可以通过增量计算更新 $\\text{Q}^{\\text{π}}(s, a)$，公式为 $\\text{Q}^{\\text{π}}(s, a) \\rightarrow \\text{Q}^{\\text{π}}(s, a)+\\text{α}\\big(G\\big(\\tau_{s_{0}=s, a_{0}=a}\\big)-\\text{Q}^{\\text{π}}(s, a)\\big)$。\",\n    \"问题3\": \"时序差分学习方法中的蒙特卡罗误差是指什么？\",\n    \"回答3\": \"时序差分学习方法中的蒙特卡罗误差表示当前轨迹的真实回报 $G\\big(\\tau_{s_{0}=s, a_{0}=a}\\big)$ 与期望回报 $\\text{Q}^{\\text{π}}(s, a)$ 之间的差距。在公式中，增量 $\\text{δ} \\triangleq G\\big(\\tau_{s_{0}=s, a_{0}=a}\\big)-\\text{Q}^{\\text{π}}(s, a)$ 即为蒙特卡罗误差。\",\n    \"问题4\": \"时序差分学习方法中的贝尔曼方程是如何用于近似估计总回报的？\",\n    \"回答4\": \"时序差分学习方法中，贝尔曼方程被用于近似估计总回报。从当前状态 $s$ 和动作 $a$ 开始，采样下一步的状态和动作 $\\big(s^{\\text{′}}, a^{\\text{′}}\\big)$，并得到奖励 $r\\big(s, a, s^{\\text{′}}\\big)$。然后利用贝尔曼方程来近似估计总回报 $G\\big(\\tau_{s_{0}=s, a_{0}=a}\\big)$，公式为 $G\\big(\\tau_{s_{0}=s, a_{0}=a, s_{1}=s^{\\text{′}}, a_{1}=a^{\\text{′}}}\\big) = r\\big(s, a, s^{\\text{′}}\\big)+\\text{γ} \\text{Q}^{\\text{π}}\\big(s^{\\text{′}}, a^{\\text{′}}\\big)$。\",\n    \"问题5\": \"SARSA 算法中的策略学习方法是什么？\",\n    \"回答5\": \"SARSA 算法中的策略学习方法称为同策略算法。在 SARSA 算法中，采样和优化的策略都是 $\\text{π}^{\\text{ε}}$。这种策略学习方法使得 SARSA 算法能够更加高效地进行学习和优化。\",\n    \"问题6\": \"时序差分学习方法和蒙特卡罗方法的主要不同点是什么？\",\n    \"回答6\": \"时序差分学习方法和蒙特卡罗方法的主要不同点之一是，蒙特卡罗方法需要一条完整的路径才能知道其总回报，也不依赖马尔可夫性质；而时序差分学习方法只需要一步，其总回报需要通过马尔可夫性质来进行近似估计。\",\n    \"问题7\": \"时序差分学习方法如何减少现实回报和预期回报之间的差距？\",\n    \"回答7\": \"时序差分学习方法在每次迭代中优化 $\\text{Q}$ 函数来减少现实回报 $r+\\text{γ} \\text{Q}\\big(s^{\\text{′}}, a^{\\text{′}}\\big)$ 和预期 $\\text{Q}(s, a)$ 的差距。这类似于动物学习的机制，在大脑神经元中，多巴胺的释放机制和时序差分学习十分吻合。\",\n    \"问题8\": \"时序差分学习方法如何利用动态规划来提高效率？\",\n    \"回答8\": \"为了提高效率，时序差分学习方法利用动态规划的方法来计算总回报，而不需要得到完整的轨迹。它通过采样下一步的状态和动作，并利用贝尔曼方程来近似估计总回报，从而提高了学习效率。\",\n    \"问题9\": \"时序差分学习方法中的增量计算方式如何更新值函数 $\\text{Q}^{\\text{π}}(s, a)$？\",\n    \"回答9\": \"时序差分学习方法中，每次采用一个新的轨迹 $\\tau_{s_{0}=s, a_{0}=a}$，就可以通过增量计算更新值函数 $\\text{Q}^{\\text{π}}(s, a)$。更新公式为 $\\text{Q}^{\\text{π}}(s, a) \\rightarrow \\text{Q}^{\\text{π}}(s, a)+\\text{α}\\big(G\\big(\\tau_{s_{0}=s, a_{0}=a}\\big)-\\text{Q}^{\\text{π}}(s, a)\\big)$。\",\n    \"问题10\": \"时序差分学习方法和动物大脑神经元中多巴胺释放机制有何相似之处？\",\n    \"回答10\": \"时序差分学习方法和动物大脑神经元中多巴胺释放机制相似之处在于，它们都是通过对实际奖励和预期奖励之间的差异进行调节来实现学习。在大脑神经元中，多巴胺的释放机制和时序差分学习十分吻合，都是基于对奖励预期和实际奖励之间的比较来进行调节。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710765070,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1839,
            "prompt_tokens": 4905,
            "total_tokens": 6744
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]