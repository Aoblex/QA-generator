[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "13.3.2.2 生成网络",
        "content_length": 747,
        "content": "Title: 13.3.2.2 生成网络\nContent:\n生成网络 (Generator Network) 的目标刚好和判别网络相反, 即让判别网络将自己生成的样本判别为真实样本.\n\n\n\\begin{align*}\n& \\max _{\\theta}\\left(\\mathbb{E}_{\\boldsymbol{z} \\sim p(\\boldsymbol{z})}[\\log D(G(\\boldsymbol{z} ; \\theta) ; \\boldsymbol{\\phi})]\\right)  \\tag{13.33}\\\\\n= & \\min _{\\theta}\\left(\\mathbb{E}_{\\boldsymbol{z} \\sim p(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z} ; \\theta) ; \\boldsymbol{\\phi}))]\\right) . \\tag{13.34}\n\\end{align*}\n\n\n上面的这两个目标函数是等价的. 但是在实际训练时,一般使用前者, 因为其梯度性质更好. 我们知道, 函数 $\\log (x), x \\in(0,1)$ 在 $x$ 接近 1 时的梯度要比接近 0 时的梯度小很多, 接近 “饱和” 区间. 这样, 当判别网络 $D$ 以很高的概率认为生成网络 $G$ 产生的样本是 “假” 样本, 即 $(1-D(G(\\boldsymbol{z} ; \\theta) ; \\phi)) \\rightarrow 1$, 这时目标函数关于 $\\theta$ 的梯度反而很小,从而不利于优化.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}"
    },
    {
        "id": "chatcmpl-945ZWNRMsJDJszKkmMQ7ZYNZdDdz0",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"生成网络的目标是什么？\",\n    \"回答1\": \"生成网络的目标是让判别网络将生成的样本判别为真实样本，即通过生成网络生成的样本尽可能地逼近真实样本，以欺骗判别网络。\",\n    \"问题2\": \"生成网络的优化目标函数是什么？\",\n    \"回答2\": \"生成网络的优化目标函数可以表示为 $\\text{max}_{\\theta}\\big(\\text{E}_{\\boldsymbol{z} \\text{ 从 } p(\\boldsymbol{z}) \\text{ 中采样}}[\\text{log } D(G(\\boldsymbol{z} ; \\theta) ; \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\theta})])}\\big)$。这个目标函数的等价形式是 $\\text{min}_{\\theta}\\big(\\text{E}_{\\boldsymbol{z} \\text{ 从 } p(\\boldsymbol{z}) \\text{ 中采样}}[\\text{log }(1-D(G(\\boldsymbol{z} ; \\theta) ; \\boldsymbol{\\theta}))])\\big)$。\",\n    \"问题3\": \"为什么在实际训练时一般使用生成网络的前者优化目标函数？\",\n    \"回答3\": \"在实际训练时一般使用生成网络的前者优化目标函数，因为其梯度性质更好。当判别网络认为生成网络产生的样本是“假”样本时，目标函数关于 $\\theta$ 的梯度较小，有利于优化。而后者在这种情况下梯度较大，不利于优化。\",\n    \"问题4\": \"生成网络的优化目标函数中的 $\\theta$ 代表什么？\",\n    \"回答4\": \"$\\theta$ 代表生成网络中的参数，通过调整这些参数来使得生成网络能够产生更逼近真实样本的合成样本。\",\n    \"问题5\": \"为什么要让判别网络将生成的样本判别为真实样本？\",\n    \"回答5\": \"通过让判别网络将生成的样本判别为真实样本，可以使得生成网络产生的合成样本更加逼近真实样本，从而提高生成网络的性能和效果。\",\n    \"问题6\": \"在生成网络中，为什么要使用期望值来计算优化目标函数？\",\n    \"回答6\": \"在生成网络中使用期望值来计算优化目标函数是因为生成网络的输入 $\\boldsymbol{z}$ 是从潜在空间中采样得到的随机向量，通过对潜在空间中所有可能取值的采样进行期望值计算，可以更全面地评估生成网络的性能。\",\n    \"问题7\": \"生成网络和判别网络之间的关系是什么？\",\n    \"回答7\": \"生成网络和判别网络是对抗生成网络（GANs）中的两个重要组成部分。它们通过对抗学习的方式相互影响和优化，使得生成网络能够不断改进产生更逼近真实样本的合成样本，而判别网络则不断提高对真实和合成样本的判别能力。\",\n    \"问题8\": \"在实际训练中，为什么需要考虑梯度性质来选择优化目标函数？\",\n    \"回答8\": \"在实际训练中需要考虑梯度性质来选择优化目标函数是因为梯度性质直接影响了优化算法的收敛速度和稳定性。选择梯度性质较好的优化目标函数能够更快地使得模型收敛并获得更好的性能。\",\n    \"问题9\": \"对于生成网络和判别网络之间的关系，如何理解两者相互对抗学习？\",\n    \"回答9\": \"在对抗生成网络中，生成网络和判别网络通过对抗学习相互影响和优化。生成网络不断改进产生更逼近真实样本的合成样本，以欺骗判别网络；而判别网络则不断提高对真实和合成样本的判别能力，以区分真假样本。这种相互对抗学习推动了两个网络不断提升，并最终达到一种动态平衡状态。\",\n    \"问题10\": \"在对抗生成网络中，如何理解判别网络认为生成网络产生的“假”样本时对于优化过程的影响？\",\n    \"回答10\": \"当判别网络认为生成网络产生的“假”样本时，即 $(1-D(G(\\boldsymbol{z} ; \\theta) ; \\theta)) \\rightarrow 1$，这时目标函数关于 $\\theta$ 的梯度反而很小，从而不利于优化。因此，在优化过程中需要考虑如何调整生成网络使得判别网络更难区分出“假”样本，以获得更好的训练效果。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710761758,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1469,
            "prompt_tokens": 2987,
            "total_tokens": 4456
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]