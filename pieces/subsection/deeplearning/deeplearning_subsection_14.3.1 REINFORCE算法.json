[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "14.3.1 REINFORCE算法",
        "content_length": 1242,
        "content": "Title: 14.3.1 REINFORCE算法\nContent:\n公式 (14.54) 中, 期望可以通过采样的方法来近似. 根据当前策略 $\\pi_{\\theta}$, 通过随机游走的方式来采集多个轨迹 $\\tau^{(1)}, \\tau^{(2)}, \\cdots, \\tau^{(N)}$, 其中每一条轨迹 $\\tau^{(n)}=$ $s_{0}^{(n)}, a_{0}^{(n)}, s_{1}^{(n)}, a_{1}^{(n)}, \\cdots$. 这样, 策略梯度 $\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta}$ 可以写为\n\n\n\\begin{equation*}\n\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N}\\left(\\sum_{t=0}^{T-1} \\frac{\\partial}{\\partial \\theta} \\log \\pi_{\\theta}\\left(a_{t}^{(n)} \\mid s_{t}^{(n)}\\right) \\gamma^{t} G_{\\tau_{t: T}^{(n)}}\\right) \\tag{14.56}\n\\end{equation*}\n\n\n结合随机梯度上升算法, 我们可以每次采集一条轨迹, 计算每个时刻的梯度并更新参数, 这称为REINFORCE算法[Williams, 1992], 如算法14.6所示.\n\n\\section*{算法 14.6 REINFORCE 算法}\n输入: 状态空间 $\\mathcal{S}$,动作空间 $\\mathcal{A}$, 可微分的策略函数 $\\pi_{\\theta}(a \\mid s)$, 折扣率 $\\gamma$, 学习率 $\\alpha$;\n\n1 随机初始化参数 $\\theta$;\n\n2 repeat\n\n3 根据策略 $\\pi_{\\theta}(a \\mid s)$ 生成一条轨迹: $\\tau=s_{0}, a_{0}, s_{1}, a_{1}, \\cdots, s_{T-1}, a_{T-1}, s_{T}$;\n\n4 for $t=0$ to $T$ do\n\n5 计算 $G\\left(\\tau_{t: T}\\right)$; $\\theta \\leftarrow \\theta+\\alpha \\gamma^{t} G\\left(\\tau_{t: T}\\right) \\frac{\\partial}{\\partial \\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) ; \\quad$ // 更新策略函数参数 end\n\n8 until $\\pi_{\\theta}$ 收敛;\n\n输出:策略 $\\pi_{\\theta}$"
    },
    {
        "id": "chatcmpl-946RhGvR0AgPJyb1bmxYHtlJJAG2n",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"公式 (14.54) 中的期望如何通过采样的方法来近似？\",\n    \"回答1\": \"根据当前策略 $\\boldsymbol{\\theta}$，通过随机游走的方式来采集多个轨迹 $\\tau^{(1)}, \\tau^{(2)}, \\text{...}, \\tau^{(N)}$，其中每一条轨迹 $\\tau^{(n)}=s_{0}^{(n)}, a_{0}^{(n)}, s_{1}^{(n)}, a_{1}^{(n)}, \\text{...}$。然后策略梯度 $\\frac{\\text{d} \\text{J}(\\boldsymbol{\\theta})}{\\text{d} \\boldsymbol{\\theta}}$ 可以通过对每个轨迹中的动作概率和回报进行加权求和来近似。\",\n    \"问题2\": \"REINFORCE算法中的策略梯度是如何计算的？\",\n    \"回答2\": \"在REINFORCE算法中，策略梯度 $\\frac{\\text{d} \\text{J}(\\boldsymbol{\\theta})}{\\text{d} \\boldsymbol{\\theta}}$ 可以通过对每条轨迹中每个时刻的动作概率和回报进行加权求和来近似。具体地，公式 (14.56) 中的梯度计算包括对每个时刻的动作概率的对数梯度乘以相应时刻的折扣回报，并对所有轨迹进行平均。\",\n    \"问题3\": \"REINFORCE算法中的参数更新是如何进行的？\",\n    \"回答3\": \"在REINFORCE算法中，参数更新是通过随机梯度上升算法实现的。每次采集一条轨迹后，根据该轨迹中每个时刻的动作概率和回报计算梯度，并利用学习率进行参数更新。这样可以逐步优化策略函数，使其能够更好地产生高回报的动作序列。\",\n    \"问题4\": \"REINFORCE算法中的折扣率 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{gamma}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}$ 对于参数更新有何影响？\",\n    \"回答4\": \"折扣率 $\\boldsymbol{\\beta}$ 在参数更新中起到了平衡当前奖励和未来奖励的作用。通过折扣率 $\\beta$ 的引入，可以使得未来时刻的回报对当前时刻的动作产生影响，从而更好地考虑长期奖励。较大的折扣率会使得未来奖励对当前参数更新产生更大影响，而较小的折扣率则更注重当前奖励。\",\n    \"问题5\": \"REINFORCE算法中的学习率 $\\boldsymbol{\\beta}$ 对于参数更新有何影响？\",\n    \"回答5\": \"学习率 $\\beta$ 决定了参数更新的步长大小。较大的学习率会导致参数更新幅度较大，可能会出现不稳定甚至发散的情况；而较小的学习率则可能导致收敛速度过慢。因此，选择合适的学习率对于保证算法收敛性和效率非常重要。\",\n    \"问题6\": \"REINFORCE算法中的收敛条件是什么？\",\n    \"回答6\": \"REINFORCE算法中通常使用策略函数的更新幅度或者策略函数本身的变化量作为收敛条件。当策略函数参数变化足够小，或者达到一定迭代次数后，认为策略函数已经收敛。\",\n    \"问题7\": \"REINFORCE算法中如何生成一条轨迹？\",\n    \"回答7\": \"根据当前策略 $\\boldsymbol{\\theta}$，可以通过随机游走的方式来生成一条轨迹。从初始状态开始，根据当前策略选择动作并转移到下一个状态，如此循环直至终止状态。这样就得到了一条完整的轨迹，用于计算梯度并更新参数。\",\n    \"问题8\": \"REINFORCE算法与随机梯度上升算法有何关联？\",\n    \"回答8\": \"REINFORCE算法是基于随机梯度上升算法实现的。在每次采集一条轨迹后，根据该轨迹中每个时刻的动作概率和回报计算梯度，并利用学习率进行参数更新。这与随机梯度上升算法类似，都是通过梯度上升来逐步优化目标函数。\",\n    \"问题9\": \"REINFORCE算法如何利用随机游走来近似期望？\",\n    \"回答9\": \"根据当前策略 $\\boldsymbol{\\theta}$，通过随机游走的方式来采集多个轨迹 $\\tau^{(1)}, \\tau^{(2)}, \\text{...}, \\tau^{(N)}$。然后利用这些轨迹中每个时刻的动作概率和回报进行加权求和，从而近似期望。\",\n    \"问题10\": \"REINFORCE算法如何利用随机游走来优化策略函数？\",\n    \"回答10\": \"通过随机游走采集多个轨迹后，可以计算每个时刻的梯度并利用学习率进行参数更新。这样可以逐步优化策略函数，使其能够更好地产生高回报的动作序列。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710765117,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1676,
            "prompt_tokens": 3238,
            "total_tokens": 4914
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]