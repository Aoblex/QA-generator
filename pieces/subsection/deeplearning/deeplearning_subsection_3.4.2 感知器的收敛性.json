[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "3.4.2 感知器的收敛性",
        "content_length": 2841,
        "content": "Title: 3.4.2 感知器的收敛性\nContent:\n[Novikoff, 1963] 证明对于两类问题,如果训练集是线性可分的,那么感知器算法可以在有限次迭代后收玫. 然而, 如果训练集不是线性可分的, 那么这个算法则不能确保会收敛.\n\n当数据集是两类线性可分时, 对于训练集 $\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, 其中 $\\boldsymbol{x}^{(n)}$ 为参见定义3.1.样本的增广特征向量, $y^{(n)} \\in\\{-1,1\\}$, 那么存在一个正的常数 $\\gamma(\\gamma>0)$ 和权重向量 $\\boldsymbol{w}^{*}$, 并且 $\\left\\|\\boldsymbol{w}^{*}\\right\\|=1$, 对所有 $n$ 都满足 $\\left(\\boldsymbol{w}^{*}\\right)^{\\top}\\left(y^{(n)} \\boldsymbol{x}^{(n)}\\right) \\geq \\gamma$. 我们可以证明如下定理.\n\n定理 3.1 - 感知器收玫性: 给定训练集 $\\mathcal{D}=\\left\\{\\left(\\boldsymbol{x}^{(n)}, y^{(n)}\\right)\\right\\}_{n=1}^{N}$, 令 $R$ 是训练集中最大的特征向量的模,即\n\n$$\nR=\\max _{n}\\left\\|x^{(n)}\\right\\| \\text {. }\n$$\n\n如果训练集 $\\mathcal{D}$ 线性可分, 两类感知器的参数学习算法3.1的权重更新次数不超过 $\\frac{R^{2}}{\\gamma^{2}}$.\n\n证明. 感知器的权重向量的更新方式为\n\n\n\\begin{equation*}\n\\boldsymbol{w}_{k}=\\boldsymbol{w}_{k-1}+y^{(k)} \\boldsymbol{x}^{(k)} \\tag{3.59}\n\\end{equation*}\n\n\n其中 $\\boldsymbol{x}^{(k)}, y^{(k)}$ 表示第 $k$ 个错误分类的样本.\n\n因为初始权重向量为 0 , 在第 $K$ 次更新时感知器的权重向量为\n\n\n\\begin{equation*}\n\\boldsymbol{w}_{K}=\\sum_{k=1}^{K} y^{(k)} \\boldsymbol{x}^{(k)} \\tag{3.60}\n\\end{equation*}\n\n\n分别计算 $\\left\\|\\boldsymbol{w}_{K}\\right\\|^{2}$ 的上下界:\n\n(1) $\\left\\|\\boldsymbol{w}_{K}\\right\\|^{2}$ 的上界为\n\n\n\\begin{align*}\n\\left\\|\\boldsymbol{w}_{K}\\right\\|^{2} & =\\left\\|\\boldsymbol{w}_{K-1}+y^{(K)} \\boldsymbol{x}^{(K)}\\right\\|^{2}  \\tag{3.61}\\\\\n& =\\left\\|\\boldsymbol{w}_{K-1}\\right\\|^{2}+\\left\\|y^{(K)} \\boldsymbol{x}^{(K)}\\right\\|^{2}+2 y^{(K)} \\boldsymbol{w}_{K-1}^{\\top} \\boldsymbol{x}^{(K)} \\\\\n& \\leq\\left\\|\\boldsymbol{w}_{K-1}\\right\\|^{2}+R^{2}  \\tag{3.63}\\\\\n& \\leq\\left\\|\\boldsymbol{w}_{K-2}\\right\\|^{2}+2 R^{2}  \\tag{3.64}\\\\\n& \\leq K R^{2} \\tag{3.65}\n\\end{align*}\n\n\n(3.62) $\\quad y_{k} \\boldsymbol{w}_{K-1}^{\\top} x^{(K)} \\leq 0$.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n(2) $\\left\\|\\boldsymbol{w}_{K}\\right\\|^{2}$ 的下界为\n\n\n\\begin{align*}\n\\left\\|\\boldsymbol{w}_{K}\\right\\|^{2} & =\\left\\|_{\\left\\|\\boldsymbol{w}^{*}\\right\\|^{2}} \\cdot\\right\\| \\boldsymbol{w}_{K} \\|^{2}  \\tag{3.66}\\\\\n& \\geq\\left\\|\\boldsymbol{w}^{* \\top} \\boldsymbol{w}_{K}\\right\\|^{2}  \\tag{3.67}\\\\\n& =\\left\\|\\boldsymbol{w}^{* \\top} \\sum_{k=1}^{K}\\left(y^{(k)} \\boldsymbol{x}^{(k)}\\right)\\right\\|^{2}  \\tag{3.68}\\\\\n& =\\left\\|\\sum_{k=1}^{K} \\boldsymbol{w}^{* \\top}\\left(y^{(k)} \\boldsymbol{x}^{(k)}\\right)\\right\\|^{2}  \\tag{3.69}\\\\\n& \\geq K^{2} \\gamma^{2} . \\tag{3.70}\n\\end{align*}\n\n\n由公式(3.65)和公式(3.70), 得到\n\n\n\\begin{equation*}\nK^{2} \\gamma^{2} \\leq\\left\\|\\boldsymbol{w}_{K}\\right\\|^{2} \\leq K R^{2} \\tag{3.71}\n\\end{equation*}\n\n\n取最左和最右的两项, 进一步得到, $K^{2} \\gamma^{2} \\leq K R^{2}$. 然后两边都除 $K$, 最终得到 $K \\leq \\frac{R^{2}}{\\gamma^{2}}$. 因此,在线性可分的条件下,算法3.1会在 $\\frac{R^{2}}{\\gamma^{2}}$ 步内收玫.\n\n虽然感知器在线性可分的数据上可以保证收玫, 但其存在以下不足:\n\n(1) 在数据集线性可分时, 感知器虽然可以找到一个超平面把两类数据分开,但并不能保证其泛化能力.\n\n（2）感知器对样本顺序比较敏感. 每次迭代的顺序不一致时, 找到的分割超平面也往往不一致.\n\n（3）如果训练集不是线性可分的,就永远不会收敛."
    },
    {
        "id": "chatcmpl-941BWbyjZbWbQ72C3UgnbVunpmqdN",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"感知器算法在什么情况下可以保证收敛？\",\n    \"回答1\": \"感知器算法可以在有限次迭代后收敛的情况是当训练集是线性可分的时。在这种情况下，存在一个正的常数 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}$ 和权重向量 $\\boldsymbol{w}^{*}$，使得对所有 $n$ 都满足 $\\big(\\boldsymbol{w}^{*}\\big)^{\\top}\\big(y^{(n)} \\boldsymbol{x}^{(n)}\\big) \\text{ 为 } \\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于} \\text{ }\\text{大于等于}$。\",\n    \"问题2\": \"定理 3.1中的感知器收敛性定理是如何定义的？\",\n    \"回答2\": \"定理 3.1 - 感知器收敛性: 给定训练集 $\\big\\big\\big(\\boldsymbol{x}^{(n)}, y^{(n)}\\big)\\big\\big\\big)_{n=1}^{N}$，如果训练集 $\\big\\big\\big(\\boldsymbol{x}^{(n)}, y^{(n)}\\big)\\big\\big\\big)_{n=1}^{N}$ 线性可分，那么两类感知器的参数学习算法3.1的权重更新次数不超过 $\\frac{R^{2}}{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}R^{2}}$。\",\n    \"问题3\": \"感知器的权重向量更新方式是怎样的？\",\n    \"回答3\": \"感知器的权重向量的更新方式为 $\\boldsymbol{w}_{k}=\\boldsymbol{w}_{k-1}+y^{(k)} \\boldsymbol{x}^{(k)}$，其中 $\\big(\\boldsymbol{x}^{(k)}, y^{(k)}\\big)$ 表示第 $k$ 个错误分类的样本。\",\n    \"问题4\": \"感知器算法在数据集线性可分时的权重更新次数上下界是如何计算的？\",\n    \"回答4\": \"感知器算法在数据集线性可分时的权重更新次数上下界计算如下：上界为 $K R^{2}$，下界为 $K^{2}\\big(\\frac{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}\\big)$。\",\n    \"问题5\": \"感知器算法在数据集线性可分时存在哪些不足之处？\",\n    \"回答5\": \"在数据集线性可分时，感知器存在以下不足：（1）虽然可以找到一个超平面把两类数据分开，但不能保证其泛化能力；（2）对样本顺序比较敏感，每次迭代的顺序不一致时，找到的分割超平面也往往不一致；（3）如果训练集不是线性可分的，则永远不会收敛。\",\n    \"问题6\": \"Novikoff 在 1963 年的研究证明了什么？\",\n    \"回答6\": \"Novikoff 在 1963 年的研究证明了对于两类问题,如果训练集是线性可分的,那么感知器算法可以在有限次迭代后收敛。然而, 如果训练集不是线性可分的, 那么这个算法则不能确保会收敛。\",\n    \"问题7\": \"训练集线性可分时，感知器算法的权重更新次数与什么相关？\",\n    \"回答7\": \"训练集线性可分时，感知器算法的权重更新次数与训练集中最大的特征向量的模 $R$ 相关，具体表现为不超过 $\\frac{R^{2}}{\\beta^{2}}$ 次。\",\n    \"问题8\": \"感知器算法对样本顺序敏感的特点会导致什么问题？\",\n    \"回答8\": \"感知器算法对样本顺序比较敏感。每次迭代的顺序不一致时，找到的分割超平面也往往不一致。这可能导致模型对输入数据的微小变化产生较大影响，降低了模型的稳定性和泛化能力。\",\n    \"问题9\": \"感知器算法在非线性可分数据集上存在怎样的局限性？\",\n    \"回答9\": \"如果训练集不是线性可分的, 则感知器算法不能确保会收敛。这意味着在非线性可分数据集上，感知器算法可能无法找到合适的超平面将数据正确分类，从而无法达到收敛状态。\",\n    \"问题10\": \"感知器算法在何种情况下无法保证收敛？\",\n    \"回答10\": \"感知器算法无法保证收敛的情况是当训练集不是线性可分时。在这种情况下，该算法不能确保会收敛，因为无法找到合适的超平面将数据正确分类。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710744894,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1761,
            "prompt_tokens": 4231,
            "total_tokens": 5992
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]