[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "3.5.1 参数学习",
        "content_length": 2576,
        "content": "Title: 3.5.1 参数学习\nContent:\n为了找到最大间隔分割超平面, 将公式 (3.86) 的目标函数写为凸优化问题\n\n\n\\begin{equation*}\n\\min _{\\boldsymbol{w}, b} \\quad \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2} \\tag{3.87}\n\\end{equation*}\n\n\n$$\n\\text { s.t. } \\quad 1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right) \\leq 0, \\quad \\forall n \\in\\{1, \\cdots, N\\} \\text {. }\n$$\n\n使用拉格朗日乘数法, 公式 (3.87) 的拉格朗日函数为\n\n\n\\begin{equation*}\n\\Lambda(\\boldsymbol{w}, b, \\lambda)=\\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+\\sum_{n=1}^{N} \\lambda_{n}\\left(1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)\\right) \\tag{3.88}\n\\end{equation*}\n\n\n其中 $\\lambda_{1} \\geq 0, \\cdots, \\lambda_{N} \\geq 0$ 为拉格朗日乘数. 计算 $\\Lambda(\\boldsymbol{w}, b, \\lambda)$ 关于 $\\boldsymbol{w}$ 和 $b$ 的导数, 并令其等于 0 ,得到\n\n\n\\begin{align*}\n\\boldsymbol{w} & =\\sum_{n=1}^{N} \\lambda_{n} y^{(n)} \\boldsymbol{x}^{(n)},  \\tag{3.89}\\\\\n0 & =\\sum_{n=1}^{N} \\lambda_{n} y^{(n)} . \\tag{3.90}\n\\end{align*}\n\n\n将公式(3.89)代入公式(3.88), 并利用公式 (3.90), 得到拉格朗日对偶函数\n\n\n\\begin{equation*}\n\\Gamma(\\lambda)=-\\frac{1}{2} \\sum_{n=1}^{N} \\sum_{m=1}^{N} \\lambda_{m} \\lambda_{n} y^{(m)} y^{(n)}\\left(\\boldsymbol{x}^{(m)}\\right)^{\\top} \\boldsymbol{x}^{(n)}+\\sum_{n=1}^{N} \\lambda_{n} . \\tag{3.91}\n\\end{equation*}\n\n\n支持向量机的主优化问题为凸优化问题, 满足强对偶性, 即主优化问题可以通过最大化对偶函数 $\\max _{\\lambda \\geq 0} \\Gamma(\\lambda)$ 来求解. 对偶函数 $\\Gamma(\\lambda)$ 是一个凹函数, 因此最大化对偶函数是一个凸优化问题, 可以通过多种凸优化方法来进行求解, 得到拉格朗日乘数的最优值 $\\lambda^{*}$. 但由于其约束条件的数量为训练样本数量, 一般的优化方法代价比较高, 因此在实践中通常采用比较高效的优化方法, 比如序列最小优化 (Sequential Minimal Optimization,SMO ) 算法 [Platt, 1998]等.\n\n根据 $\\mathrm{KKT}$ 条件中的互补松弛条件, 最优解满足 $\\lambda_{n}^{*}\\left(1-y^{(n)}\\left(\\boldsymbol{w}^{* \\top} \\boldsymbol{x}^{(n)}+b^{*}\\right)\\right)=$ 0 . 如果样本 $\\boldsymbol{x}^{(n)}$ 不在约束边界上, $\\lambda_{n}^{*}=0$, 其约束失效; 如果样本 $\\boldsymbol{x}^{(n)}$ 在约束边界上, $\\lambda_{n}^{*} \\geq 0$. 这些在约束边界上的样本点称为支持向量 (Support Vector), 即离决策平面距离最近的点.\n\n在计算出 $\\lambda^{*}$ 后, 根据公式 (3.89) 计算出最优权重 $\\boldsymbol{w}^{*}$, 最优偏置 $b^{*}$ 可以通过任选一个支持向量 $(\\tilde{\\boldsymbol{x}}, \\tilde{y})$ 计算得到:\n\n\n\\begin{equation*}\nb^{*}=\\tilde{y}-\\boldsymbol{w}^{* \\top} \\tilde{\\boldsymbol{x}} \\tag{3.92}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n最优参数的支持向量机的决策函数为\n\n\n\\begin{align*}\nf(\\boldsymbol{x}) & =\\operatorname{sgn}\\left(\\boldsymbol{w}^{*^{\\top}} \\boldsymbol{x}+b^{*}\\right)  \\tag{3.93}\\\\\n& =\\operatorname{sgn}\\left(\\sum_{n=1}^{N} \\lambda_{n}^{*} y^{(n)}\\left(\\boldsymbol{x}^{(n)}\\right)^{\\top} \\boldsymbol{x}+b^{*}\\right) . \\tag{3.94}\n\\end{align*}\n\n\n支持向量机的决策函数只依赖于 $\\lambda_{n}^{*}>0$ 的样本点, 即支持向量.\n\n支持向量机的目标函数可以通过 SMO 等优化方法得到全局最优解, 因此比其他分类器的学习效率更高. 此外, 支持向量机的决策函数只依赖于支持向量,与训练样本总数无关, 分类速度比较快."
    },
    {
        "id": "chatcmpl-941EpsNoh8yyyruFzu9fFmNTl6KtR",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"支持向量机的目标函数是什么？\",\n    \"回答1\": \"支持向量机的目标函数是 $\\frac{1}{2}\\big\\bracevert\\boldsymbol{w}\\big\\bracevert^{2}$，其中 $\\boldsymbol{w}$ 是权重向量。\",\n    \"问题2\": \"使用拉格朗日乘数法，支持向量机的目标函数对应的拉格朗日函数是什么？\",\n    \"回答2\": \"使用拉格朗日乘数法，支持向量机的目标函数对应的拉格朗日函数是 $\\frac{1}{2}\\big\\bracevert\\boldsymbol{w}\\big\\bracevert^{2}+\\text{∑}_{n=1}^{N} \\text{λ}_{n}\\big(1-y^{(n)}\\big(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\big)\\big)$。\",\n    \"问题3\": \"支持向量机的拉格朗日对偶函数是如何定义的？\",\n    \"回答3\": \"支持向量机的拉格朗日对偶函数定义为 $\\text{Γ}(\\text{λ})=-\\frac{1}{2} \\text{∑}_{n=1}^{N} \\text{∑}_{m=1}^{N} \\text{λ}_{m} \\text{λ}_{n} y^{(m)} y^{(n)}\\big(\\boldsymbol{x}^{(m)}\\big)^{\\top} \\boldsymbol{x}^{(n)}+\\text{∑}_{n=1}^{N} \\text{λ}_{n}$。\",\n    \"问题4\": \"支持向量机的主优化问题满足什么性质？\",\n    \"回答4\": \"支持向量机的主优化问题满足强对偶性，即主优化问题可以通过最大化对偶函数 $\\text{Γ}(\\text{λ})$ 来求解。\",\n    \"问题5\": \"支持向量机的决策函数依赖于哪些样本点？\",\n    \"回答5\": \"支持向量机的决策函数只依赖于 $\\text{λ}_{n}^{*}>0$ 的样本点，即支持向量。\",\n    \"问题6\": \"在计算出最优拉格朗日乘数 $\\text{λ}^{*}$ 后，如何计算出最优权重 $\\boldsymbol{w}^{*}$ 和最优偏置 $b^{*}$？\",\n    \"回答6\": \"在计算出最优拉格朗日乘数 $\\text{λ}^{*}$ 后，可以通过公式 $b^{*}=\\tilde{y}-\\boldsymbol{w}^{* \\top} \\tilde{\\boldsymbol{x}}$ 计算得到最优偏置 $b^{*}$，其中 $(\\tilde{\\boldsymbol{x}}, \\tilde{y})$ 是任选一个支持向量。\",\n    \"问题7\": \"为什么在实践中通常采用比较高效的优化方法来求解支持向量机的主优化问题？\",\n    \"回答7\": \"由于支持向量机的约束条件数量为训练样本数量，一般的优化方法代价比较高。因此在实践中通常采用比较高效的优化方法，比如序列最小优化 (SMO) 算法等。\",\n    \"问题8\": \"根据KKT条件中的互补松弛条件，最优解满足什么条件？\",\n    \"回答8\": \"根据KKT条件中的互补松弛条件，最优解满足 $\\text{λ}_{n}^{*}\\big(1-y^{(n)}\\big(\\boldsymbol{w}^{*^{\\top}} \\boldsymbol{x}^{(n)}+b^{*}\\big)\\big)=0$。如果样本 $\\boldsymbol{x}^{(n)}$ 不在约束边界上，$\\text{λ}_{n}^{*}=0$，其约束失效；如果样本 $\\boldsymbol{x}^{(n)}$ 在约束边界上，$\\text{λ}_{n}^{*} \\textgreater= 0$。\",\n    \"问题9\": \"支持向量机的决策函数公式是什么？\",\n    \"回答9\": \"支持向量机的决策函数公式是 $f(\\boldsymbol{x})=\\text{sgn}\\big(\\boldsymbol{w}^{*^{\\top}} \\boldsymbol{x}+b^{*}\\big)$。\",\n    \"问题10\": \"支持向量机相比其他分类器有哪些优势？\",\n    \"回答10\": \"支持向量机的目标函数可以通过高效的优化方法得到全局最优解，学习效率更高。此外，支持向量机的决策函数只依赖于支持向量，与训练样本总数无关，因此分类速度比较快。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710745099,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1277,
            "prompt_tokens": 4044,
            "total_tokens": 5321
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]