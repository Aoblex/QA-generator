[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "14.2.2 蒙特卡罗方法",
        "content_length": 2231,
        "content": "Title: 14.2.2 蒙特卡罗方法\nContent:\n在很多应用场景中, 马尔可夫决策过程的状态转移概率 $p\\left(s^{\\prime} \\mid s, a\\right)$ 和奖励函数 $r\\left(s, a, s^{\\prime}\\right)$ 都是未知的. 在这种情况下, 我们一般需要智能体和环境进行交互,并收集一些样本, 然后再根据这些样本来求解马尔可夫决策过程最优策略. 这种模型未知, 基于采样的学习算法也称为模型无关的强化学习 ( Model-Free Reinforcement Learning ) 算法.\n\n$\\mathrm{Q}$ 函数 $Q^{\\pi}(s, a)$ 是初始状态为 $s$, 并执行动作 $a$ 后所能得到的期望总回报:\n\n\n\\begin{equation*}\nQ^{\\pi}(s, a)=\\mathbb{E}_{\\tau \\sim p(\\tau)}\\left[G\\left(\\tau_{s_{0}=s, a_{0}=a}\\right)\\right] \\tag{14.29}\n\\end{equation*}\n\n\n其中 $\\tau_{s_{0}=s, a_{0}=a}$ 表示轨迹 $\\tau$ 的起始状态和动作为 $s, a$.\n\n如果模型未知, $\\mathrm{Q}$ 函数可以通过采样来进行计算, 这就是蒙特卡罗方法. 对于一个策略 $\\pi$, 智能体从状态 $s$, 执行动作 $a$ 开始, 然后通过随机游走的方法来探索环境, 并计算其得到的总回报. 假设我们进行 $N$ 次试验, 得到 $N$ 个轨迹 $\\tau^{(1)}, \\tau^{(2)}, \\cdots, \\tau^{(N)}$ ，其总回报分别为 $G\\left(\\tau^{(1)}\\right), G\\left(\\tau^{(2)}\\right), \\cdots, G\\left(\\tau^{(N)}\\right)$. Q 函数可以近似为\n\n\n\\begin{equation*}\nQ^{\\pi}(s, a) \\approx \\hat{Q}^{\\pi}(s, a)=\\frac{1}{N} \\sum_{n=1}^{N} G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(n)}\\right) \\tag{14.30}\n\\end{equation*}\n\n\n当 $N \\rightarrow \\infty$ 时, $\\hat{Q}^{\\pi}(s, a) \\rightarrow Q^{\\pi}(s, a)$.\n\n在近似估计出 $\\mathrm{Q}$ 函数 $\\hat{Q}^{\\pi}(s, a)$ 之后, 就可以进行策略改进. 然后在新的策略下重新通过采样来估计 $\\mathrm{Q}$ 函数,并不断重复, 直至收玫.\n\n利用和探索 但在蒙特卡罗方法中, 如果采用确定性策略 $\\pi$, 每次试验得到的轨迹是一样的, 只能计算出 $Q^{\\pi}(s, \\pi(s))$, 而无法计算其他动作 $a^{\\prime}$ 的 $\\mathrm{Q}$ 函数, 因此也无法进一步改进策略. 这样情况仅仅是对当前策略的利用 (exploitation), 而缺失了对环境的探索 (exploration), 即试验的轨迹应该尽可能覆盖所有的状态和动作,以找到更好的策略.\n\n为了平衡利用和探索, 我们可以采用 $\\epsilon$-贪心法 ( $\\epsilon$-greedy Method ). 对于一参见第14.2.4节。\n\n模型无关的强化学习也叫作无模型的强化学习.\n\n$\\mathrm{Q}$ 函数的定义参见公式(14.19).\n\n这也可以看作一个多臂赌博机问题.\\\\\n个目标策略 $\\pi$,其对应的 $\\epsilon$-贪心法策略为\n\n\\[\n\\pi^{\\epsilon}(s)=\\left\\{\\begin{array}{cc}\n\\pi(s), & \\text { 按概率 } 1-\\epsilon,  \\tag{14.31}\\\\\n\\text { 随机选择 } \\mathcal{A} \\text { 中的动作, } & \\text { 按概率 } \\epsilon .\n\\end{array}\\right.\n\\]\n\n这样, $\\epsilon$-贪心法将一个仅利用的策略转为带探索的策略. 每次选择动作 $\\pi(s)$ 的概率为 $1-\\epsilon+\\frac{\\epsilon}{|\\mathcal{A}|}$, 其他动作的概率为 $\\frac{\\epsilon}{|\\mathcal{A}|}$.\n\n同策略 在蒙特卡罗方法中, 如果采样策略是 $\\pi^{\\epsilon}(s)$, 不断改进策略也是 $\\pi^{\\epsilon}(s)$ 而不是目标策略 $\\pi(s)$. 这种采样与改进策略相同 (即都是 $\\pi^{\\epsilon}(s)$ ) 的强化学习方法叫作同策略 ( On-Policy ) 方法.\n\n异策略 如果采样策略是 $\\pi^{\\epsilon}(s)$, 而优化目标是策略 $\\pi$, 可以通过重要性采样, 引入重要性权重来实现对目标策略 $\\pi$ 的优化. 这种采样与改进分别使用不同策略的强化学习方法叫作异策略 (Off-Policy ) 方法."
    },
    {
        "id": "chatcmpl-946MhNyFhnEpe9vLNAHNQXpluPtPW",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"蒙特卡罗方法中的 Q 函数是如何定义的？\",\n    \"回答1\": \"在蒙特卡罗方法中，Q 函数 $Q^{\\text{π}}(s, a)$ 是初始状态为 $s$，并执行动作 $a$ 后所能得到的期望总回报。其定义为 $Q^{\\text{π}}(s, a)=\\text{E}_{\\tau \\text{~} p(\\tau)}\\big[G\\big(\\tau_{s_{0}=s, a_{0}=a}\\big)\\big]$。\",\n    \"问题2\": \"蒙特卡罗方法中如何通过采样来近似估计 Q 函数？\",\n    \"回答2\": \"在蒙特卡罗方法中，对于一个策略 π，智能体从状态 $s$，执行动作 $a$ 开始，然后通过随机游走的方法来探索环境，并计算其得到的总回报。通过进行 $N$ 次试验，得到 $N$ 个轨迹 $\\tau^{(1)}, \\tau^{(2)}, \\text{...}, \\tau^{(N)}$ ，其总回报分别为 $G\\big(\\tau^{(1)}\\big), G\\big(\\tau^{(2)}\\big), \\text{...}, G\\big(\\tau^{(N)}\\big)$。Q 函数可以近似为 $\\frac{1}{N} \\text{Σ}_{n=1}^{N} G\\big(\\tau_{s_{0}=s, a_{0}=a}^{(n)}\\big)$。\",\n    \"问题3\": \"蒙特卡罗方法中如何进行策略改进？\",\n    \"回答3\": \"在蒙特卡罗方法中，利用近似估计出的 Q 函数 $\\text{Q}^{\\text{π}}(s, a)$ 进行策略改进。然后在新的策略下重新通过采样来估计 Q 函数，并不断重复，直至收敛。\",\n    \"问题4\": \"为什么在蒙特卡罗方法中需要平衡利用和探索？\",\n    \"回答4\": \"在蒙特卡罗方法中，如果采用确定性策略 π，每次试验得到的轨迹是一样的，只能计算出 $Q^{\\text{π}}(s, \\text{π}(s))$，而无法计算其他动作 $a^{\\text{'}}$ 的 Q 函数。这导致缺失了对环境的探索，即试验的轨迹应该尽可能覆盖所有的状态和动作，以找到更好的策略。\",\n    \"问题5\": \"什么是 $\\text{ε}$-贪心法（$\\text{ε}$-greedy Method）？\",\n    \"回答5\": \"在蒙特卡罗方法中，为了平衡利用和探索，可以采用 $\\text{ε}$-贪心法。对于一参见第14.2.4节。目标策略 π，其对应的 $\\text{ε}$-贪心法策略为 $\\text{π}^{\\text{ε}}(s)=\\bigg\\begin{cases}\\text{π}(s), & \\text{按概率 } 1-\\text{ε}, \\\\ \\text{随机选择 } \\text{A} \\text{ 中的动作, } & \\text{按概率 } \\text{ε} .\\bigg.$\",\n    \"问题6\": \"蒙特卡罗方法中的同策略（On-Policy）方法和异策略（Off-Policy）方法有什么区别？\",\n    \"回答6\": \"在蒙特卡罗方法中，同策略方法指采样与改进策略相同（即都是 $\\text{π}^{\\text{ε}}(s)$）的强化学习方法。而异策略方法指采样策略是 $\\text{π}^{\\text{ε}}(s)$，而优化目标是策略 π，可以通过重要性采样引入重要性权重来实现对目标策略 π 的优化。\",\n    \"问题7\": \"蒙特卡罗方法中的模型无关强化学习和无模型强化学习有何联系？\",\n    \"回答7\": \"模型无关强化学习也叫作无模型强化学习。这种算法基于采样来进行学习和决策，而不依赖对环境状态转移概率和奖励函数的具体模型。\",\n    \"问题8\": \"蒙特卡罗方法中如何利用 $\\text{Q}$ 函数进行策略改进？\",\n    \"回答8\": \"在蒙特卡罗方法中，利用近似估计出的 $\\text{Q}$ 函数 $\\text{Q}^{\\text{π}}(s, a)$ 进行策略改进。然后在新的策略下重新通过采样来估计 $\\text{Q}$ 函数，并不断重复，直至收敛。\",\n    \"问题9\": \"蒙特卡罗方法中如何平衡利用和探索？\",\n    \"回答9\": \"在蒙特卡罗方法中，为了平衡利用和探索，可以采用 $\\text{ε}$-贪心法。这种方法将一个仅利用的策略转为带探索的策略，以便更好地发现更好的策略。\",\n    \"问题10\": \"蒙特卡罗方法中如何进行 Q 函数的近似计算？\",\n    \"回答10\": \"在蒙特卡罗方法中，如果模型未知，则可以通过采样来进行 Q 函数的近似计算。通过多次试验得到多个轨迹，并计算其总回报，从而近似估计出 Q 函数。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710764807,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1594,
            "prompt_tokens": 4089,
            "total_tokens": 5683
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]