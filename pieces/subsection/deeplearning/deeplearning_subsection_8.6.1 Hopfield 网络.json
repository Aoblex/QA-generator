[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "8.6.1 Hopfield 网络",
        "content_length": 3283,
        "content": "Title: 8.6.1 Hopfield 网络\nContent:\n本书中之前介绍的神经网络都是作为一种机器学习模型的输入-输出映射函数, 其参数学习方法是通过梯度下降方法来最小化损失函数. 除了作为机器学习模型外,神经网络还可以作为一种记忆的存储和检索模型.\n\nHopfield 网络 (Hopfield Network) 是一种循环神经网络模型, 由一组互相连接的神经元组成. Hopfield 网络也可以认为是所有神经元都互相连接的不分层的神经网络. 每个神经元既是输入单元, 又是输出单元, 没有隐藏神经元. 一个神经元和自身没有反馈相连,不同神经元之间连接权重是对称的.\n\n图8.10给出了 Hopfield 网络的结构示例.\n\n\n\n图 8.10 四个节点的 Hopfield 网络神经动力学是将神经网络作为非线性动力系统, 研究其随时间变化的规律以及稳定性等问题.\\\\\n假设一个Hopfield 网络有 $M$ 个神经元, 第 $i$ 个神经元的更新规则为\n\n\\[\ns_{i}= \\begin{cases}+1 & \\text { if } \\sum_{j=1}^{M} w_{i j} s_{j}+b_{i} \\geq 0  \\tag{8.32}\\\\ -1 & \\text { otherwise }\\end{cases}\n\\]\n\n其中 $w_{i j}$ 为神经元 $i$ 和 $j$ 之间的连接权重, $b_{i}$ 为偏置.\n\n连接权重 $w_{i j}$ 有以下性质:\n\n\\[\n\\begin{array}{lll}\nw_{i i} & =0 & \\forall i \\in[1, M]  \\tag{8.33}\\\\\nw_{i j} & =w_{j i} & \\forall i, j \\in[1, M] .\n\\end{array}\n\\]\n\nHopfield 网络的更新可以分为异步和同步两种方式. 异步更新是指每次更新一个神经元，神经元的更新顺序可以是随机或事先固定的. 同步更新是指一次更新所有的神经元, 需要有一个时钟来进行同步. 第 $t$ 时刻的神经元状态为 $\\boldsymbol{s}_{t}=\\left[s_{t, 1}, s_{t, 2}, \\cdots, s_{t, M}\\right]^{\\top}$, 其更新规则为\n\n\n\\begin{equation*}\n\\boldsymbol{s}_{t}=f\\left(\\boldsymbol{W} \\boldsymbol{s}_{t-1}+\\boldsymbol{b}\\right) \\tag{8.34}\n\\end{equation*}\n\n\n其中 $\\boldsymbol{s}_{0}=\\boldsymbol{x}, \\boldsymbol{W}=\\left[w_{i j}\\right]_{M \\times M}$ 为连接权重, $\\boldsymbol{b}=\\left[b_{i}\\right]_{M \\times 1}$ 为偏置向量, $f(\\cdot)$ 为非线性阶跃函数.\n\n能量函数 在 Hopfield 网络中, 我们给每个不同的网络状态定义一个标量属性,称为 “能量”. Hopfield 网络的能量函数 (Energy Function) $E$ 定义为\n\n\n\\begin{align*}\nE & =-\\frac{1}{2} \\sum_{i, j} w_{i j} s_{i} s_{j}-\\sum_{i} b_{i} s_{i}  \\tag{8.35}\\\\\n& =-\\frac{1}{2} \\boldsymbol{s}^{\\top} \\boldsymbol{W} \\boldsymbol{s}-\\boldsymbol{b}^{\\top} \\boldsymbol{s} . \\tag{8.36}\n\\end{align*}\n\n\nHopfield 网络是稳定的, 即能量函数经过多次迭代后会达到收玫状态. 权重对称是一个重要特征, 因为它保证了能量函数在神经元激活时单调递减, 而不对称的权重可能导致周期性振荡或者混乱.\n\n给定一个外部输入, 网络经过演化, 会达到某个稳定状态. 这些稳定状态称为吸引点 (Attractor). 在一个Hopfield 网络中, 通常有多个吸引点, 每个吸引点为一个能量的局部最优点.\n\n图8.11给出了 Hopfield 网络的能量函数. 红线为网络能量的演化方向, 蓝点为吸引点.\n\n\n\n图 8.11 Hopfield 网络的能量函数\n\n联想记忆 Hopfield 网络存在有限的吸引点 (Attractor), 即能量函数的局部最小点. 每个吸引点 $\\boldsymbol{u}$ 都对应一个 “管辖”区域 $\\mathcal{R}_{\\boldsymbol{u}}$. 若输入向量 $\\boldsymbol{x}$ 落入这个区域, 网络最终会收玫到 $\\boldsymbol{u}$. 因此, 我们可以把吸引点看作网络中存储的模式 ( Pattern ).\n\n将网络输入 $\\boldsymbol{x}$ 作为起始状态，随时间收敛到吸引点 $\\boldsymbol{u}$ 上的过程作为检索过程. 即使输入向量 $\\boldsymbol{x}$ 只包含部分信息或包含噪声, 只要其位于对应存储模式的 “吸引”区域内, 那么随着时间演化, 网络最终会收玫到其对应的存储模式. 因此, Hopfield 的检索是基于内容寻址的检索, 具有联想记忆能力.\n\n信息存储 信息存储是指将一组向量 $\\boldsymbol{x}_{1}, \\cdots, \\boldsymbol{x}_{N}$ 存储在网络中的过程. 存储过程主要是调整神经元之间的连接权重, 因此可以看作一种学习过程. Hopfield 网络的学习规则有很多种. 一种最简单的学习方式为: 神经元 $i$ 和 $j$ 之间的连接权重通过下面公式得到\n\n\n\\begin{equation*}\nw_{i j}=\\frac{1}{N} \\sum_{n=1}^{N} x_{i}^{(n)} x_{j}^{(n)} \\tag{8.37}\n\\end{equation*}\n\n\n其中 $x_{i}^{(n)}$ 是第 $n$ 个输入向量的第 $i$ 维特征. 如果 $x_{i}$ 和 $x_{j}$ 在输入向量中相同的概率越多, 则 $w_{i j}$ 越大. 这种学习规则和人脑神经网络的学习方式十分类似. 在人脑神经网络中, 如果两个神经元经常同时激活, 则它们之间的连接加强; 如果两个神经元经常不同时激活, 则连接消失. 这种学习方式称为赫布规则 ( Hebbian Rule,或 Hebb’s Rule).\n\n存储容量 对于联想记忆模型来说, 存储容量为其能够可靠地存储和检索模式的最大数量. 对于数量为 $M$ 的互相连接的二值神经元网络, 其总状态数为 $2^{M}$,其中可以作为有效稳定点的状态数量就是其存储容量. 模型容量一般与网络结构和学习方式有关. Hopfield 网络的最大容量为 $0.14 M$, 玻尔兹曼机的容量为 $0.6 M$, 但是其学习效率比较低, 需要非常长时间的演化才能达到均衡状态. 通玻尔兹曼机参见过改进学习算法, Hopfield 网络的最大容量可以达到 $O(M)$. 如果允许高阶 (阶数为 $K)$ 连接, 比如三个神经元连接关系, 其稳定存储的最大容量为 $O\\left(M^{K-1}\\right)$. [Plate, 1995] 引入复数运算, 有效地提高了网络容量. 总体上讲, 通过改进网络结 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n构、学习方式以及引入更复杂的运算 ( 比如复数、量子操作), 可以有效改善联想记忆网络的容量."
    },
    {
        "id": "chatcmpl-944YvxtHce6PQPv8C1cv38nMLUtFR",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"Hopfield 网络与之前介绍的神经网络在作用上有何不同？\",\n    \"回答1\": \"Hopfield 网络除了作为机器学习模型的输入-输出映射函数外，还可以作为一种记忆的存储和检索模型。这意味着 Hopfield 网络具有存储和检索记忆模式的能力，与之前介绍的神经网络在作用上有所不同。\",\n    \"问题2\": \"Hopfield 网络的结构特点是什么？\",\n    \"回答2\": \"Hopfield 网络由一组互相连接的神经元组成，可以认为是所有神经元都互相连接的不分层的神经网络。每个神经元既是输入单元，又是输出单元，没有隐藏神经元。此外，一个神经元和自身没有反馈相连，不同神经元之间连接权重是对称的。\",\n    \"问题3\": \"Hopfield 网络中的更新规则是怎样的？\",\n    \"回答3\": \"Hopfield 网络中，每个神经元的更新规则为 $s_{i}= \\begin{cases}+1 & \\text { if } \\text{...} \\tag{8.32}\\\\ -1 & \\text { otherwise }\\big\\rbrace$，其中 $w_{i j}$ 为神经元 $i$ 和 $j$ 之间的连接权重，$b_{i}$ 为偏置。\",\n    \"问题4\": \"Hopfield 网络的能量函数是如何定义的？\",\n    \"回答4\": \"Hopfield 网络的能量函数 $E$ 定义为 $E =-\\frac{1}{2} \\boldsymbol{s}^{\\top} \\boldsymbol{W} \\boldsymbol{s}-\\boldsymbol{b}^{\\top} \\boldsymbol{s}$。其中 $\\boldsymbol{s}$ 为神经元状态向量，$\\boldsymbol{W}$ 为连接权重，$\\boldsymbol{b}$ 为偏置向量。\",\n    \"问题5\": \"Hopfield 网络的稳定性体现在哪些方面？\",\n    \"回答5\": \"Hopfield 网络是稳定的，即能量函数经过多次迭代后会达到收敛状态。其中权重对称是一个重要特征，保证了能量函数在神经元激活时单调递减，而不对称的权重可能导致周期性振荡或混乱。\",\n    \"问题6\": \"Hopfield 网络中的吸引点是指什么？\",\n    \"回答6\": \"Hopfield 网络中存在有限的吸引点 (Attractor)，即能量函数的局部最小点。每个吸引点 $\\boldsymbol{u}$ 都对应一个 “管辖”区域 $\\text{...}$。如果输入向量 $\\boldsymbol{x}$ 落入这个区域，网络最终会收敛到 $\\boldsymbol{u}$。\",\n    \"问题7\": \"Hopfield 网络的检索过程基于什么原理？\",\n    \"回答7\": \"将网络输入 $\\boldsymbol{x}$ 作为起始状态，随时间收敛到吸引点 $\\boldsymbol{u}$ 上的过程作为检索过程。即使输入向量 $\\boldsymbol{x}$ 只包含部分信息或包含噪声，只要其位于对应存储模式的 “吸引”区域内，那么随着时间演化，网络最终会收敛到其对应的存储模式。\",\n    \"问题8\": \"Hopfield 网络中信息存储过程主要通过什么方式实现？\",\n    \"回答8\": \"信息存储是指将一组向量 $\\boldsymbol{x}_{1}, \\text{...}, \\boldsymbol{x}_{N}$ 存储在网络中的过程。存储过程主要通过调整神经元之间的连接权重来实现。Hopfield 网络的学习规则中，连接权重通过简单公式计算得到。\",\n    \"问题9\": \"Hopfield 网络的存储容量与何相关？\",\n    \"回答9\": \"对于联想记忆模型来说，存储容量为其能够可靠地存储和检索模式的最大数量。Hopfield 网络的存储容量一般与网络结构和学习方式有关，并且可以通过改进网络结构、学习方式以及引入更复杂的运算来提高网络容量。\",\n    \"问题10\": \"Hopfield 网络与其他模型相比，在存储容量上有何特点？\",\n    \"回答10\": \"Hopfield 网络的最大容量为 $0.14 M$，相比之下玻尔兹曼机的容量为 $0.6 M$。然而，通过改进学习算法和引入复数运算等方式，Hopfield 网络的最大容量可以达到 $O(M)$。这表明 Hopfield 网络在存储容量上具有一定优势，并且具有进一步提高容量的潜力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710757877,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1380,
            "prompt_tokens": 4910,
            "total_tokens": 6290
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]