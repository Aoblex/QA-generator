[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "3.5.3 软间隔",
        "content_length": 9817,
        "content": "Title: 3.5.3 软间隔\nContent:\n在支持向量机的优化问题中, 约束条件比较严格. 如果训练集中的样本在特征空间中不是线性可分的, 就无法找到最优解. 为了能够容忍部分不满足约束的样本, 我们可以引入松弛变量 (Slack Variable) $\\xi$, 将优化问题变为\n\n\n\\begin{equation*}\n\\min _{\\boldsymbol{w}, b} \\quad \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{n=1}^{N} \\xi_{n} \\tag{3.99}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\ns.t. $\\quad 1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)-\\xi_{n} \\leq 0, \\quad \\forall n \\in\\{1, \\cdots, N\\}$\n\n$\\xi_{n} \\geq 0, \\quad \\forall n \\in\\{1, \\cdots, N\\}$\n\n其中参数 $C>0$ 用来控制间隔和松弛变量惩罚的平衡. 引入松弛变量的间隔称为软间隔 (Soft Margin ). 公式(3.99)也可以表示为经验风险 + 正则化项的形式:\n\n\n\\begin{equation*}\n\\min _{\\boldsymbol{w}, b} \\quad \\sum_{n=1}^{N} \\max \\left(0,1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)\\right)+\\frac{1}{2 C}\\|\\boldsymbol{w}\\|^{2}, \\tag{3.100}\n\\end{equation*}\n\n\n其中可以把 $\\max \\left(0,1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)\\right)$ 看作损失函数, 称为 Hinge 损失函数 参见公式(2.20).\n\n( Hinge Loss Function ), 把 $\\frac{1}{2 C}\\|\\boldsymbol{w}\\|^{2}$ 看作正则化项, $\\frac{1}{C}$ 是正则化系数.\n\n软间隔支持向量机的参数学习和原始支持向量机类似, 其最终决策函数也只和支持向量有关, 即满足 $1-y^{(n)}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}^{(n)}+b\\right)-\\xi_{n}=0$ 的样本.\n\n\\section*{3.6 损失函数对比}\n本章介绍了三种二分类模型:Logistic 回归、感知器和支持向量机. 虽然它们的决策函数相同, 但由于使用了不同的损失函数以及相应的优化方法, 导致它们在实际任务上的表现存在一定的差异.\n\n为了比较这些损失函数, 我们统一定义类别标签 $y \\in\\{+1,-1\\}$, 并定义 $f(\\boldsymbol{x} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b$. 这样对于样本 $(\\boldsymbol{x}, y)$, 若 $y f(\\boldsymbol{x} ; \\boldsymbol{w})>0$, 则分类正确; 若 $y f(\\boldsymbol{x} ; \\boldsymbol{w})<0$, 则分类错误. 这样, 为了方便比较这些模型, 我们可以将它们的损失函数都表述为定义在 $y f(\\boldsymbol{x} ; \\boldsymbol{w})$ 上的函数.\n\nLogistic 回归的损失函数可以改写为\n\n\n\\begin{align*}\n\\mathcal{L}_{L R} & =-I(y=1) \\log \\sigma(f(\\boldsymbol{x} ; \\boldsymbol{w}))-I(y=-1) \\log (1-\\sigma(f(\\boldsymbol{x} ; \\boldsymbol{w})))  \\tag{3.101}\\\\\n& =-I(y=1) \\log \\sigma(f(\\boldsymbol{x} ; \\boldsymbol{w}))-I(y=-1) \\log \\sigma(-f(\\boldsymbol{x} ; \\boldsymbol{w}))  \\tag{3.102}\\\\\n& =-\\log \\sigma(y f(\\boldsymbol{x} ; \\boldsymbol{w})) \\\\\n& =\\log (1+\\exp (-y f(\\boldsymbol{x} ; \\boldsymbol{w}))) . \\tag{3.104}\n\\end{align*}\n\n\n$$\n\\text { (3.103) } y \\in\\{+1,-1\\} \\text {. }\n$$\n\n$I(\\cdot)$ 为指示函数.\n\n$1-\\sigma(x)=\\sigma(-x)$.\n\n$y \\in\\{+1,-1\\}$.\n\n感知器的损失函数为\n\n\n\\begin{equation*}\n\\mathcal{L}_{p}=\\max (0,-y f(\\boldsymbol{x} ; \\boldsymbol{w})) . \\tag{3.105}\n\\end{equation*}\n\n\n软间隔支持向量机的损失函数为\n\n\n\\begin{equation*}\n\\mathcal{L}_{\\text {hinge }}=\\max (0,1-y f(\\boldsymbol{x} ; \\boldsymbol{w})) . \\tag{3.106}\n\\end{equation*}\n\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n平方损失可以重写为\n\n\n\\begin{align*}\n\\mathcal{L}_{\\text {squared }} & =(y-f(\\boldsymbol{x} ; \\boldsymbol{w}))^{2}  \\tag{3.107}\\\\\n& =1-2 y f(\\boldsymbol{x} ; \\boldsymbol{w})+(y f(\\boldsymbol{x} ; \\boldsymbol{w}))^{2}  \\tag{3.108}\\\\\n& =(1-y f(\\boldsymbol{x} ; \\boldsymbol{w}))^{2} . \\tag{3.109}\n\\end{align*}\n\n\n图3.7给出了不同损失函数的对比. 对于二分类来说, 当 $y f(\\boldsymbol{x} ; \\boldsymbol{w})>0$ 时, 分类器预测正确, 并且 $y f(\\boldsymbol{x} ; \\boldsymbol{w})$ 越大, 模型的预测越正确; 当 $y f(\\boldsymbol{x} ; \\boldsymbol{w})<0$ 时, 分类器预测错误, 并且 $y f(\\boldsymbol{x} ; \\boldsymbol{w})$ 越小, 模型的预测越错误. 因此,一个好的损失函数应该随着 $y f(\\boldsymbol{x} ; \\boldsymbol{w})$ 的增大而减少. 从图3.7中看出, 除了平方损失, 其他损失函数都比较适合于二分类问题.\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_b7f820bc184c7846d78fg-089}\n\\end{center}\n\n图 3.7 不同损失函数的对比\n\n\\section*{3.7 总结和深入阅读}\n和回归问题不同, 分类问题中的目标标签 $y$ 是离散的类别标签, 因此分类问题中的决策函数需要输出离散值或是标签的后验概率. 线性分类模型一般是一个广义线性函数，即一个或多个线性判别函数加上一个非线性激活函数. 所谓 “线性”是指决策边界由一个或多个超平面组成.\n\n表3.1给出了几种常见的线性模型的比较. 在 Logistic 回归和 Softmax 回归中, $\\boldsymbol{y}$ 为类别的 one-hot 向量表示; 在感知器和支持向量机中, $y$ 为 $\\{+1,-1\\}$.\\\\\n表 3.1 几种常见的线性模型对比\n\n\\begin{center}\n\\begin{tabular}{llll}\n\\hline\n线性模型 & 激活函数 & 损失函数 & 优化方法 \\\\\n\\hline\n线性回归 & - & $\\left(y-\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)^{2}$ & 最小二乘、梯度下降 \\\\\nLogistic 回归 & $\\sigma\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)$ & $\\boldsymbol{y} \\log \\sigma\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)$ & 梯度下降 \\\\\nSoftmax 回归 & $\\operatorname{softmax}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{x}\\right)$ & $\\boldsymbol{y} \\log \\operatorname{softmax}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{x}\\right)$ & 梯度下降 \\\\\n感知器 & $\\operatorname{sgn}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)$ & $\\max \\left(0,-y \\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)$ & 随机梯度下降 \\\\\n支持向量机 & $\\operatorname{sgn}\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)$ & $\\max \\left(0,1-y \\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)$ & 二次规划、SMO 等 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nLogistic 回归是一种概率模型, 其通过使用 Logistic 函数来将一个实数值映射到 $[0,1]$ 之间. 事实上, 还有很多函数也可以达到此目的, 比如正态分布的累积概率密度函数, 即 probit 函数. 这些知识可以参考 《Pattern Recognition and Machine Learning 》》[Bishop, 2007]的第 4 章.\n\n感知器作为一种最简单的神经网络, 其学习算法也非常直观有效 [Rosenblatt, 1958]. [Freund et al., 1999] 提出了使用核技巧改进感知器学习算法, 并用投票感知器来提高泛化能力. [Collins, 2002] 将感知器算法扩展到结构化学习,给出了相应的收玫性证明, 并且提出一种更加有效并且实用的参数平均化策略.\n\n要深入了解支持向量机以及核方法, 可以参考文献《Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond 》 [Scholkopf et al., 2001].\n\n\\section*{习题}\n习题 3-1 证明在两类线性分类中,权重向量 $\\boldsymbol{w}$ 与决策平面正交.\n\n习题 3-2 在线性空间中, 证明一个点 $\\boldsymbol{x}$ 到平面 $f(\\boldsymbol{x} ; \\boldsymbol{w})=\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b=0$ 的距离为 $|f(\\boldsymbol{x} ; \\boldsymbol{w})| /\\|\\boldsymbol{w}\\|$.\n\n习题 3-3 在线性分类中, 决策区域是凸的. 即若点 $x_{1}$ 和 $x_{2}$ 被分为类别 $c$, 则点 $\\rho \\boldsymbol{x}_{1}+(1-\\rho) \\boldsymbol{x}_{2}$ 也会被分为类别 $c$, 其中 $\\rho \\in(0,1)$.\n\n习题 3-4 给定一个多分类的数据集,证明:1) 如果数据集中每个类的样本都和除该类之外的样本是线性可分的, 则该数据集一定是线性可分的; 2 ) 如果数据集中每两个类的样本是线性可分的,则该数据集不一定是线性可分的.\n\n习题 3-5 在 Logistic 回归中, 是否可以用 $\\hat{y}=\\sigma\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{x}\\right)$ 去逼近正确的标签 $y$, 并用平方损失 $(y-\\hat{y})^{2}$ 最小化来优化参数 $\\boldsymbol{w}$ ?\\\\\n习题 3-6 在 Softmax 回归的风险函数 (公式 (3.39)) 中, 如果加上正则化项会有什么影响?\n\n习题 3-7 验证平均感知器训练算法3.2中给出的平均权重向量的计算方式和公式 (3.77)等价.\n\n习题3-8 证明定理3.2.\n\n习题 3-9 若数据集线性可分, 证明支持向量机中将两类样本正确分开的最大间隔分割超平面存在且唯一.\n\n习题 3-10 验证公式(3.97).\n\n习题 3-11 在软间隔支持向量机中, 试给出原始优化问题的对偶问题, 并列出其 KKT条件.\n\n\\section*{参考文献}\nBishop C M, 2007. Pattern recognition and machine learning[M]. 5th edition. Springer.\n\nCollins M, 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms $[\\mathrm{C}] / /$ Proceedings of the conference on Empirical methods in natural language processing. 1-8.\n\nDaumé III H, 2012. A course in machine learning[EB/OL]. \\href{http://ciml.info/}{http://ciml.info/}.\n\nFreund Y, Schapire R E, 1999. Large margin classification using the perceptron algorithm[J]. Machine learning, 37(3):277-296.\n\nNovikoff A B, 1963. On convergence proofs for perceptrons[R]. DTIC Document.\n\nPlatt J, 1998. Sequential minimal optimization: A fast algorithm for training support vector machines[R]. 21.\n\nRosenblatt F, 1958. The perceptron: a probabilistic model for information storage and organization in the brain.[J]. Psychological review, 65(6):386.\n\nScholkopf B, Smola A J, 2001. Learning with kernels: support vector machines, regularization, optimization, and beyond[M]. MIT press.\n\n\\section*{第二部分}\n\\section*{基础模型}\n\\section*{第 4 章 前馈神经网络}\n神经网络是一种大规模的并行分布式处理器, 天然具有存储并使用经验知识的能力. 它从两个方面上模拟大脑: (1) 网络获取的知识是通过学习来获取的; (2) 内部神经元的连接强度,即突触权重, 用于储存获取的知识.\n\n一西蒙$\\cdot$赫金 ( Simon Haykin ) [Haykin, 1994]\n\n人工神经网络 (Artificial Neural Network, ANN) 是指一系列受生物学和神经科学启发的数学模型. 这些模型主要是通过对人脑的神经元网络进行抽象,构建人工神经元, 并按照一定拓扑结构来建立人工神经元之间的连接, 来模拟生物神经网络. 在人工智能领域, 人工神经网络也常常简称为神经网络 (Neural Network,NN ) 或神经模型 (Neural Model).\n\n神经网络最早是作为一种主要的连接主义模型. 20 世纪 80 年代中后期, 最流行的一种连接主义模型是分布式并行处理 (Parallel Distributed Processing, PDP ) 模型 [McClelland et al., 1986], 其有 3 个主要特性: 1 ) 信息表示是分布式的 (非局部的 );2) 记忆和知识是存储在单元之间的连接上; 3 ) 通过逐渐改变单元之间的连接强度来学习新的知识.\n\n连接主义的神经网络有着多种多样的网络结构以及学习方法, 虽然早期模型强调模型的生物学合理性 (Biological Plausibility), 但后期更关注对某种特定认知能力的模拟, 比如物体识别、语言理解等. 尤其在引入误差反向传播来改进其学习能力之后, 神经网络也越来越多地应用在各种机器学习任务上. 随着训练数据的增多以及 (并行) 计算能力的增强, 神经网络在很多机器学习任务上已经取得了很大的突破, 特别是在语音、图像等感知信号的处理上, 神经网络表现出了卓越的学习能力.\\\\\n在本章中, 我们主要关注采用误差反向传播来进行学习的神经网络, 即作为一种机器学习模型的神经网络. 从机器学习的角度来看, 神经网络一般可以看作一个非线性模型, 其基本组成单元为具有非线性激活函数的神经元, 通过大量神经元之间的连接, 使得神经网络成为一种高度非线性的模型. 神经元之间的连接权重就是需要学习的参数, 可以在机器学习的框架下通过梯度下降方法来进行学习.\n\n\\section*{4.1 神经元}\n人工神经元 (Artificial Neuron), 简称神经元 (Neuron), 是构成神经网络的基本单元, 其主要是模拟生物神经元的结构和特性, 接收一组输入信号并产生输出.\n\n生物学家在 20 世纪初就发现了生物神经元的结构. 一个生物神经元通常具有多个树突和一条轴突. 树突用来接收信息, 轴突用来发送信息. 当神经元所获得的输入信号的积累超过某个阈值时, 它就处于兴奋状态, 产生电脉冲. 轴突尾端有许多末梢可以给其他神经元的树突产生连接 (突触), 并将电脉冲信号传递给其他神经元.\n\n1943 年, 心理学家 McCulloch 和数学家 Pitts 根据生物神经元的结构, 提出了一种非常简单的神经元模型, MP 神经元 [McCulloch et al., 1943]. 现代神经网络中的神经元和 MP 神经元的结构并无太多变化. 不同的是, MP 神经元中的激活函数 $f$ 为 0 或 1 的阶跃函数, 而现代神经元中的激活函数通常要求是连续可导的函数.\n\n假设一个神经元接收 $D$ 个输入 $x_{1}, x_{2}, \\cdots, x_{D}$, 令向量 $\\boldsymbol{x}=\\left[x_{1} ; x_{2} ; \\cdots ; x_{D}\\right]$ 来表示这组输入, 并用净输入 (Net Input) $z \\in \\mathbb{R}$ 表示一个神经元所获得的输入信号 $\\boldsymbol{x}$ 的加权和，\n\n\n\\begin{align*}\nz & =\\sum_{d=1}^{D} w_{d} x_{d}+b  \\tag{4.1}\\\\\n& =\\boldsymbol{w}^{\\top} \\boldsymbol{x}+b \\tag{4.2}\n\\end{align*}\n\n\n其中 $\\boldsymbol{w}=\\left[w_{1} ; w_{2} ; \\cdots ; w_{D}\\right] \\in \\mathbb{R}^{D}$ 是 $D$ 维的权重向量, $b \\in \\mathbb{R}$ 是偏置.\n\n净输入 $z$ 在经过一个非线性函数 $f(\\cdot)$ 后, 得到神经元的活性值 (Activation) $a$,\n\n\n\\begin{equation*}\na=f(z) \\text {, } \\tag{4.3}\n\\end{equation*}\n\n\n其中非线性函数 $f(\\cdot)$ 称为激活函数 (Activation Function).\n\n图4.1给出了一个典型的神经元结构示例.\n\n\\href{https://nndl.github.io/%E5%90%8E%E9%9D%A2%E6%88%91%E4%BB%AC%E4%BC%9A%E4%BB%8B%E7%BB%8D%E4%B8%80%E7%A7%8D%E7%94%A8%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%AE%B0%E5%BF%86%E5%AD%98%E5%82%A8%E5%92%8C%E6%A3%80%E7%B4%A2%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C}{https://nndl.github.io/后面我们会介绍一种用来进行记忆存储和检索的神经网络}, 参见第8.6节.净输入也叫净活性值 (Net Activation).\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_b7f820bc184c7846d78fg-095}\n\\end{center}\n\n输人\n\n图 4.1 典型的神经元结构\n\n激活函数 激活函数在神经元中非常重要的. 为了增强网络的表示能力和学习能力, 激活函数需要具备以下几点性质:\n\n（1）连续并可导 (允许少数点上不可导) 的非线性函数. 可导的激活函数可以直接利用数值优化的方法来学习网络参数.\n\n（2 ）激活函数及其导函数要尽可能的简单, 有利于提高网络计算效率.\n\n（3）激活函数的导函数的值域要在一个合适的区间内, 不能太大也不能太小,否则会影响训练的效率和稳定性.\n\n下面介绍几种在神经网络中常用的激活函数."
    },
    {}
]