[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "2.6.2 深度学习方法",
        "content_length": 4488,
        "content": "Title: 2.6.2 深度学习方法\nContent:\n传统的特征抽取一般是和预测模型的学习分离的. 我们会先通过主成分分析或线性判别分析等方法抽取出有效的特征, 然后再基于这些特征来训练一个具体的机器学习模型.\n\n如果我们将特征的表示学习和机器学习的预测学习有机地统一到一个模型中, 建立一个端到端的学习算法, 就可以有效地避免它们之间准则的不一致性.这种表示学习方法称为深度学习 (Deep Learning, DL ). 深度学习方法的难点是如何评价表示学习对最终系统输出结果的贡献或影响, 即贡献度分配问题. 目前比较有效的模型是神经网络, 即将最后的输出层作为预测学习, 其他层作为表示学习.\n\n\\section*{2.7 评价指标}\n为了衡量一个机器学习模型的好坏, 需要给定一个测试集, 用模型对测试集中的每一个样本进行预测, 并根据预测结果计算评价分数.\n\n对于分类问题, 常见的评价标准有准确率、精确率、召回率和 $\\mathrm{F}$ 值等. 给定测试集 $\\mathcal{T}=\\left\\{\\left(\\boldsymbol{x}^{(1)}, y^{(1)}\\right), \\cdots,\\left(\\boldsymbol{x}^{(N)}, y^{(N)}\\right)\\right\\}$, 假设标签 $y^{(n)} \\in\\{1, \\cdots, C\\}$, 用学习好的模型 $f\\left(\\boldsymbol{x} ; \\theta^{*}\\right)$ 对测试集中的每一个样本进行预测, 结果为 $\\left\\{\\hat{y}^{(1)}, \\cdots, \\hat{y}^{(N)}\\right\\}$.\n\n准确率 最常用的评价指标为准确率 (Accuracy)：\n\n\n\\begin{equation*}\n\\mathcal{A}=\\frac{1}{N} \\sum_{n=1}^{N} I\\left(y^{(n)}=\\hat{y}^{(n)}\\right) \\tag{2.73}\n\\end{equation*}\n\n\n其中 $I(\\cdot)$ 为指示函数.\n\n错误率 和准确率相对应的就是错误率 (Error Rate):\n\n\n\\begin{align*}\n\\mathcal{E} & =1-\\mathcal{A}  \\tag{2.74}\\\\\n& =\\frac{1}{N} \\sum_{n=1}^{N} I\\left(y^{(n)} \\neq \\hat{y}^{(n)}\\right) . \\tag{2.75}\n\\end{align*}\n\n\n\\href{https://nndl.github.io/%E5%8F%82%E8%A7%81%E7%AC%AC1.3%E8%8A%82%E3%80%82}{https://nndl.github.io/参见第1.3节。}\n\n由于中文的模糊性, 关于准确率和下文提到的精确率的定义在有些文献中刚好相反, 具体含义需要根据上下文进行判断.\\\\\n精确率和召回率 准确率是所有类别整体性能的平均, 如果希望对每个类都进行性能估计, 就需要计算精确率 (Precision) 和召回率 (Recall). 精确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值, 在机器学习的评价中也被大量使用.\n\n对于类别 $c$ 来说,模型在测试集上的结果可以分为以下四种情况：\n\n（1）真正例 (True Positive, TP ): 一个样本的真实类别为 $c$ 并且模型正确地预测为类别 $c$. 这类样本数量记为\n\n\n\\begin{equation*}\nT P_{c}=\\sum_{n=1}^{N} I\\left(y^{(n)}=\\hat{y}^{(n)}=c\\right) \\tag{2.76}\n\\end{equation*}\n\n\n（2）假负例 (False Negative, FN ): 一个样本的真实类别为 $c$, 模型错误地预测为其他类. 这类样本数量记为\n\n\n\\begin{equation*}\nF N_{c}=\\sum_{n=1}^{N} I\\left(y^{(n)}=c \\wedge \\hat{y}^{(n)} \\neq c\\right) \\tag{2.77}\n\\end{equation*}\n\n\n（3）假正例 (False Positive, FP ): 一个样本的真实类别为其他类, 模型错误地预测为类别 $c$. 这类样本数量记为\n\n\n\\begin{equation*}\nF P_{c}=\\sum_{n=1}^{N} I\\left(y^{(n)} \\neq c \\wedge \\hat{y}^{(n)}=c\\right) . \\tag{2.78}\n\\end{equation*}\n\n\n（4）真负例 (True Negative, TN ): 一个样本的真实类别为其他类, 模型也预测为其他类. 这类样本数量记为 $T N_{c}$. 对于类别 $c$ 来说, 这种情况一般不需要关注.\n\n这四种情况的关系可以用如表2.3所示的混淆矩阵 (Confusion Matrix) 来\n\n\\begin{center}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n &  & \\multicolumn{2}{|c|}{预测类别} \\\\\n\\hline\n &  & $\\hat{y}=c$ & $\\hat{y} \\neq c$ \\\\\n\\hline\n\\multirow{2}{*}{真实类别} & $y=c$ & $T P_{c}$ & $F N_{c}$ \\\\\n\\hline\n & $y \\neq c$ & $F P_{c}$ & $T N_{c}$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n表示.\n\n表 2.3 类别 $c$ 的预测结果的混淆矩阵\n\n根据上面的定义,我们可以进一步定义查准率、查全率和 $\\mathrm{F}$ 值.\n\n精确率 (Precision), 也叫精度或查准率, 类别 $c$ 的查准率是所有预测为类别 $c$ 的样本中预测正确的比例:\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n\n\\begin{equation*}\n\\mathcal{P}_{c}=\\frac{T P_{c}}{T P_{c}+F P_{c}} \\tag{2.79}\n\\end{equation*}\n\n\n召回率 (Recall), 也叫查全率, 类别 $c$ 的查全率是所有真实标签为类别 $c$ 的样本中预测正确的比例:\n\n\n\\begin{equation*}\n\\mathcal{R}_{c}=\\frac{T P_{c}}{T P_{c}+F N_{c}} \\tag{2.80}\n\\end{equation*}\n\n\n$\\mathrm{F}$ 值 (F Measure) 是一个综合指标, 为精确率和召回率的调和平均:\n\n\n\\begin{equation*}\n\\mathcal{F}_{c}=\\frac{\\left(1+\\beta^{2}\\right) \\times \\mathcal{P}_{c} \\times \\mathcal{R}_{c}}{\\beta^{2} \\times \\mathcal{P}_{c}+\\mathcal{R}_{c}} \\tag{2.81}\n\\end{equation*}\n\n\n其中 $\\beta$ 用于平衡精确率和召回率的重要性, 一般取值为 $1 . \\beta=1$ 时的 $\\mathrm{F}$ 值称为 $\\mathrm{F} 1$值, 是精确率和召回率的调和平均.\n\n宏平均和微平均 为了计算分类算法在所有类别上的总体精确率、召回率和 F1 值, 经常使用两种平均方法, 分别称为宏平均 (Macro Average) 和微平均 (Micro Average ) [Yang, 1999].\n\n宏平均是每一类的性能指标的算术平均值:\n\n\n\\begin{align*}\n\\mathcal{P}_{\\text {macro }} & =\\frac{1}{C} \\sum_{c=1}^{C} \\mathcal{P}_{c},  \\tag{2.82}\\\\\n\\mathcal{R}_{\\text {macro }} & =\\frac{1}{C} \\sum_{c=1}^{C} \\mathcal{R}_{c},  \\tag{2.83}\\\\\n\\mathcal{F} 1_{\\text {macro }} & =\\frac{2 \\times \\mathcal{P}_{\\text {macro }} \\times R_{\\text {macro }}}{P_{\\text {macro }}+R_{\\text {macro }}} . \\tag{2.84}\n\\end{align*}\n\n\n值得注意的是, 在有些文献上 $F 1$ 值的宏平均为 $\\mathcal{F} 1_{\\text {macro }}=\\frac{1}{C} \\sum_{c=1}^{C} \\mathcal{F} 1_{c}$.\n\n微平均是每一个样本的性能指标的算术平均值. 对于单个样本而言, 它的精确率和召回率是相同的 (要么都是 1 , 要么都是 0 ). 因此精确率的微平均和召回率的微平均是相同的. 同理, $\\mathrm{F} 1$ 值的微平均指标是相同的. 当不同类别的样本数量不均衡时, 使用宏平均会比微平均更合理些. 宏平均会更关注小类别上的评价指标.\n\n在实际应用中, 我们也可以通过调整分类模型的阈值来进行更全面的评价,比如 AUC (Area Under Curve )、ROC (Receiver Operating Characteristic) 曲线、PR (Precision-Recall) 曲线等. 此外, 很多任务还有自己专门的评价方式, 比如 TopN 准确率.\n\n交叉验证 交叉验证 (Cross-Validation) 是一种比较好的衡量机器学习模型的统计分析方法, 可以有效避免划分训练集和测试集时的随机性对评价结果造成的影响. 我们可以把原始数据集平均分为 $K$ 组不重复的子集, 每次选 $K-1$ 组子 K一般大于 3.集作为训练集,剩下的一组子集作为验证集. 这样可以进行 $K$ 次试验并得到 $K$ 个模型,将这 $K$ 个模型在各自验证集上的错误率的平均作为分类器的评价.\n\n关于更详细的模型评价指标, 可以参考《机器学习》[周志华, 2016]的第 2 章。\n\n\\section*{2.8 理论和定理}\n在机器学习中, 有一些非常有名的理论或定理, 对理解机器学习的内在特性非常有帮助."
    },
    {
        "id": "chatcmpl-93vh0DQTFtuiPYMivPn1d26bFvllD",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"传统的特征抽取和深度学习方法的主要区别是什么？\",\n    \"回答1\": \"传统的特征抽取一般是和预测模型的学习分离的，通过主成分分析或线性判别分析等方法抽取出有效的特征，然后再基于这些特征来训练一个具体的机器学习模型。而深度学习方法将特征的表示学习和机器学习的预测学习有机地统一到一个模型中，建立一个端到端的学习算法，避免了它们之间准则的不一致性。\",\n    \"问题2\": \"深度学习方法中的难点是什么？\",\n    \"回答2\": \"深度学习方法的难点是如何评价表示学习对最终系统输出结果的贡献或影响，即贡献度分配问题。这意味着在深度学习中，如何准确评估表示学习对最终预测结果的贡献或影响是一个具有挑战性的问题。\",\n    \"问题3\": \"在机器学习中，常见的分类问题评价标准有哪些？\",\n    \"回答3\": \"常见的分类问题评价标准包括准确率、精确率、召回率和 $\\text{F}$ 值等。这些指标可以用来衡量分类模型在测试集上的性能表现，对于不同类型的任务都具有重要意义。\",\n    \"问题4\": \"请解释精确率和召回率的定义及其在机器学习中的作用。\",\n    \"回答4\": \"精确率是所有预测为某一类别并且预测正确的样本占所有预测为该类别的样本比例，召回率是所有真实标签为某一类别并且预测正确的样本占所有真实标签为该类别的样本比例。精确率和召回率是用来衡量分类模型对每个类别进行性能估计的重要指标，在信息检索、统计学分类领域以及机器学习评价中被广泛使用。\",\n    \"问题5\": \"请解释 $\\text{F}$ 值及其在机器学习中的作用。\",\n    \"回答5\": \"$\\text{F}$ 值是精确率和召回率的调和平均，用于综合衡量分类模型对每个类别的性能表现。它是一个综合指标，能够平衡考虑精确率和召回率，$\\text{F}$ 值越高表示模型在对应类别上的性能越好。\",\n    \"问题6\": \"宏平均和微平均在机器学习中有什么作用？\",\n    \"回答6\": \"宏平均和微平均是用于计算分类算法在所有类别上总体精确率、召回率和 $\\text{F}$ 值的两种方法。宏平均是每一类的性能指标的算术平均值，而微平均是每一个样本的性能指标的算术平均值。它们能够帮助综合评价分类模型在多个类别上的整体性能。\",\n    \"问题7\": \"交叉验证在机器学习中起到什么作用？\",\n    \"回答7\": \"交叉验证是一种比较好的衡量机器学习模型的统计分析方法，可以有效避免划分训练集和测试集时的随机性对评价结果造成的影响。通过交叉验证，可以得到多个模型并综合评估其性能，提高对模型泛化能力和稳定性的评估。\",\n    \"问题8\": \"在实际应用中，还有哪些其他常见的评价方式？\",\n    \"回答8\": \"在实际应用中，除了常见的评价指标外，还可以通过调整分类模型的阈值来进行更全面的评价，比如 AUC（Area Under Curve）、ROC（Receiver Operating Characteristic）曲线、PR（Precision-Recall）曲线等。此外，很多任务还有自己专门的评价方式，比如 TopN 准确率等。\",\n    \"问题9\": \"在机器学习中有哪些非常有名的理论或定理？\",\n    \"回答9\": \"在机器学习中，有一些非常有名的理论或定理，对理解机器学习的内在特性非常有帮助。这些理论或定理包括各种关于模型训练、泛化能力、收敛性等方面的重要理论成果。\",\n    \"问题10\": \"为了衡量一个机器学习模型的好坏需要做什么？\",\n    \"回答10\": \"为了衡量一个机器学习模型的好坏，需要给定一个测试集，并用模型对测试集中每一个样本进行预测，并根据预测结果计算评价分数。通过评价指标如准确率、精确率、召回率、$\\text{F}$ 值等来综合衡量模型在测试集上的性能表现。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710723782,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1490,
            "prompt_tokens": 5589,
            "total_tokens": 7079
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]