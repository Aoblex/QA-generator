[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "14.2.1.2 值迭代算法",
        "content_length": 2656,
        "content": "Title: 14.2.1.2 值迭代算法\nContent:\n策略迭代算法中的策略评估和策略改进是交替轮流进行, 其中策略评估也是通过一个内部迭代来进行计算, 其计算量比较大. 事实上, 我们不需要每次计算出每次策略对应的精确的值函数, 也就是说内部迭代不需要执行到完全收玫.\n\n值迭代 (Value Iteration ) 算法将策略评估和策略改进两个过程合并, 来直接计算出最优策略. 最优策略 $\\pi^{*}$ 对应的值函数称为最优值函数, 其中包括最优状态值函数 $V^{*}(s)$ 和最优状态-动作值函数 $Q^{*}(s, a)$, 它们之间的关系为\n\n\n\\begin{equation*}\nV^{*}(s)=\\max _{a} Q^{*}(s, a) . \\tag{14.26}\n\\end{equation*}\n\n\n根据贝尔曼方程, 我们可以通过迭代的方式来计算最优状态值函数 $V^{*}(s)$ 和最优状态-动作值函数 $Q^{*}(s, a)$ :\n\\footnotetext{\\href{https://nndl.github.io/}{https://nndl.github.io/}\n}\n\n\n\\begin{equation*}\nV^{*}(s)=\\max _{a} \\mathbb{E}_{s^{\\prime} \\sim p\\left(s^{\\prime} \\mid s, a\\right)}\\left[r\\left(s, a, s^{\\prime}\\right)+\\gamma V^{*}\\left(s^{\\prime}\\right)\\right] \\text {, } \\tag{14.27}\n\\end{equation*}\n\n\n\n\\begin{equation*}\nQ^{*}(s, a)=\\mathbb{E}_{s^{\\prime} \\sim p\\left(s^{\\prime} \\mid s, a\\right)}\\left[r\\left(s, a, s^{\\prime}\\right)+\\gamma \\max _{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\right] \\tag{14.28}\n\\end{equation*}\n\n\n参见习题14-3.\n\n这两个公式称为贝尔曼最优方程 (Bellman Optimality Equation ).\n\n值迭代算法通过直接优化贝尔曼最优方程 (见公式 (14.27)), 迭代计算最优值函数. 值迭代算法如算法14.2所示.\n\n\\section*{算法 14.2 值迭代算法}\n输入: $\\operatorname{MDP}$ 五元组: $\\mathcal{S}, \\mathcal{A}, P, r, \\gamma ;$\n\n1 初始化: $\\forall s \\in \\mathcal{S}, V(s)=0$;\n\n2 repeat\n\n$3 \\mid \\forall s, V(s) \\leftarrow \\max _{a} \\mathbb{E}_{s^{\\prime} \\sim p\\left(s^{\\prime} \\mid s, a\\right)}\\left[r\\left(s, a, s^{\\prime}\\right)+\\gamma V\\left(s^{\\prime}\\right)\\right]$;\n\n4 until $\\forall s, V(s)$ 收玫;\n\n5 根据公式 (14.19) 计算 $Q(s, a)$;\n\n$6 \\forall s, \\pi(s)=\\arg \\max _{a} Q(s, a)$;\n\n输出: 策略 $\\pi$\n\n策略迭代算法 VS 值迭代算法 在策略迭代算法中, 每次迭代的时间复杂度最大为 $O\\left(|\\mathcal{S}|^{3}|\\mathcal{A}|^{3}\\right)$, 最大迭代次数为 $|\\mathcal{A}|^{|\\mathcal{S}|}$. 而在值迭代算法中, 每次迭代的时间复杂度最大为 $O\\left(|\\mathcal{S}|^{2}|\\mathcal{A}|\\right)$, 但迭代次数要比策略迭代算法更多.\n\n策略迭代算法是根据贝尔曼方程来更新值函数, 并根据当前的值函数来改进策略. 而值迭代算法是直接使用贝尔曼最优方程来更新值函数, 收玫时的值函数就是最优的值函数, 其对应的策略也就是最优的策略.\n\n值迭代算法和策略迭代算法都需要经过非常多的迭代次数才能完全收敛.在实际应用中, 可以不必等到完全收玫. 这样, 当状态和动作数量有限时, 经过有限次迭代就可以收敛到近似最优策略.\n\n基于模型的强化学习算法实际上是一种动态规划方法. 在实际应用中有以下两点限制:\n\n(1) 要求模型已知, 即要给出马尔可夫决策过程的状态转移概率 $p\\left(s^{\\prime} \\mid s, a\\right)$和奖励函数 $r\\left(s, a, s^{\\prime}\\right)$. 但实际应用中这个要求很难满足. 如果我们事先不知道模型, 那么可以先让智能体与环境交互来估计模型, 即估计状态转移概率和奖励函数. 一个简单的估计模型的方法为 R-max [Brafman et al., 2002], 通过随机游走的方法来探索环境. 每次随机一个策略并执行, 然后收集状态转移和奖励的样本. 在收集一定的样本后, 就可以通过统计或监督学习来重构出马尔可夫决策过程. 但是, 这种基于采样的重构过程的复杂度也非常高, 只能应用于状态数非常少的场合.\n\n(2) 效率问题, 即当状态数量较多时, 算法效率比较低. 但在实际应用中, 很 \\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n多问题的状态数量和动作数量非常多. 比如, 围棋有 $19 \\times 19=361$ 个位置, 每个位置有黑子、白子或无子三种状态, 整个棋局有 $3^{361} \\approx 10^{170}$ 种状态. 动作 (即落子位置) 数量为 361. 不管是值迭代还是策略迭代, 以当前计算机的计算能力, 根本无法计算. 一种有效的方法是通过一个函数 (比如神经网络) 来近似计算值函数,以减少复杂度, 并提高泛化能力."
    },
    {
        "id": "chatcmpl-946MLJVmDqN7gCusrkYiTxUgualO1",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"策略迭代算法中的策略评估和策略改进是如何进行的？\",\n    \"回答1\": \"策略迭代算法中的策略评估和策略改进是交替轮流进行的。策略评估通过一个内部迭代来计算，不需要执行到完全收敛。而策略改进则是根据当前的值函数来更新策略。\",\n    \"问题2\": \"值迭代算法是如何计算最优策略的？\",\n    \"回答2\": \"值迭代算法将策略评估和策略改进两个过程合并，直接计算出最优策略。最优策略 $\\boldsymbol{\\text{π}}^{*}$ 对应的值函数称为最优值函数，通过迭代计算最优状态值函数 $V^{*}(s)$ 和最优状态-动作值函数 $Q^{*}(s, a)$，并根据贝尔曼方程来更新值函数。\",\n    \"问题3\": \"最优状态值函数 $V^{*}(s)$ 和最优状态-动作值函数 $Q^{*}(s, a)$ 之间有怎样的关系？\",\n    \"回答3\": \"最优状态值函数 $V^{*}(s)$ 等于对应状态下最大的动作值函数 $Q^{*}(s, a)$。即 $V^{*}(s)=\\text{max}_{a} Q^{*}(s, a)$。\",\n    \"问题4\": \"值迭代算法中的贝尔曼最优方程是如何定义的？\",\n    \"回答4\": \"值迭代算法通过直接优化贝尔曼最优方程来迭代计算最优值函数。具体而言，贝尔曼最优方程包括更新最优状态值函数 $V^{*}(s)$ 和最优状态-动作值函数 $Q^{*}(s, a)$ 的公式。\",\n    \"问题5\": \"值迭代算法和策略迭代算法在时间复杂度上有何区别？\",\n    \"回答5\": \"在策略迭代算法中，每次迭代的时间复杂度最大为 $O\\big(|\\boldsymbol{\\text{S}}|^{3}|\\boldsymbol{\\text{A}}|^{3}\\big)$，而在值迭代算法中，每次迭代的时间复杂度最大为 $O\\big(|\\boldsymbol{\\text{S}}|^{2}|\\boldsymbol{\\text{A}}|\\big)$。然而，值迭代算法需要更多的迭代次数才能收敛。\",\n    \"问题6\": \"值迭代算法和策略迭代算法在更新值函数和改进策略时有何不同？\",\n    \"回答6\": \"策略迭代算法是根据贝尔曼方程来更新值函数，并根据当前的值函数来改进策略。而值迭代算法是直接使用贝尔曼最优方程来更新值函数，收敛时的值函数就是最优的值函数，对应的策略也就是最优的策略。\",\n    \"问题7\": \"在实际应用中，为什么可以不必等到完全收敛？\",\n    \"回答7\": \"在实际应用中，当状态和动作数量有限时，经过有限次迭代就可以收敛到近似最优策略。因此，并不需要等到完全收敛，这样可以节省计算资源并提高效率。\",\n    \"问题8\": \"基于模型的强化学习算法在实际应用中存在哪些限制？\",\n    \"回答8\": \"基于模型的强化学习算法要求模型已知，即要给出马尔可夫决策过程的状态转移概率和奖励函数。然而，在实际应用中这个要求很难满足。另外，当状态数量较多时，算法效率比较低。\",\n    \"问题9\": \"如何解决基于模型的强化学习算法中模型已知和效率低的问题？\",\n    \"回答9\": \"当模型未知时，可以让智能体与环境交互来估计模型，即估计状态转移概率和奖励函数。另一种有效方法是通过一个函数（比如神经网络）来近似计算值函数，以减少复杂度，并提高泛化能力。\",\n    \"问题10\": \"为什么基于模型的强化学习算法只能应用于状态数非常少的场合？\",\n    \"回答10\": \"基于模型的强化学习算法只能应用于状态数非常少的场合，因为基于采样的重构过程的复杂度非常高，无法应用于状态数量较多或者动作数量较多的场景。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710764785,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1348,
            "prompt_tokens": 4412,
            "total_tokens": 5760
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]