[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.3.2.1 Xavier初始化",
        "content_length": 2558,
        "content": "Title: 7.3.2.1 Xavier初始化\nContent:\n假设在一个神经网络中, 第 $l$ 层的一个神经元 $a^{(l)}$, 其接收前一层的 $M_{l-1}$ 个神经元的输出 $a_{i}^{(l-1)}, 1 \\leq i \\leq M_{l-1}$,\n\n\n\\begin{equation*}\na^{(l)}=f\\left(\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)}\\right) \\tag{7.37}\n\\end{equation*}\n\n\n其中 $f(\\cdot)$ 为激活函数, $w_{i}^{(l)}$ 为参数, $M_{l-1}$ 是第 $l-1$ 层神经元个数. 为简单起见,这里令激活函数 $f(\\cdot)$ 为恒等函数, 即 $f(x)=x$.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n假设 $w_{i}^{(l)}$ 和 $a_{i}^{(l-1)}$ 的均值都为 0 , 并且互相独立, 则 $a^{(l)}$ 的均值为\n\n\n\\begin{equation*}\n\\mathbb{E}\\left[a^{(l)}\\right]=\\mathbb{E}\\left[\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)}\\right]=\\sum_{i=1}^{M_{l-1}} \\mathbb{E}\\left[w_{i}^{(l)}\\right] \\mathbb{E}\\left[a_{i}^{(l-1)}\\right]=0 . \\tag{7.38}\n\\end{equation*}\n\n\n$a^{(l)}$ 的方差为\n\n\n\\begin{align*}\n\\operatorname{var}\\left(a^{(l)}\\right) & =\\operatorname{var}\\left(\\sum_{i=1}^{M_{l-1}} w_{i}^{(l)} a_{i}^{(l-1)}\\right)  \\tag{7.39}\\\\\n& =\\sum_{i=1}^{M_{l-1}} \\operatorname{var}\\left(w_{i}^{(l)}\\right) \\operatorname{var}\\left(a_{i}^{(l-1)}\\right)  \\tag{7.40}\\\\\n& =M_{l-1} \\operatorname{var}\\left(w_{i}^{(l)}\\right) \\operatorname{var}\\left(a_{i}^{(l-1)}\\right) \\tag{7.41}\n\\end{align*}\n\n\n也就是说, 输入信号的方差在经过该神经元后被放大或缩小了 $M_{l-1} \\operatorname{var}\\left(w_{i}^{(l)}\\right)$ 倍.为了使得在经过多层网络后, 信号不被过分放大或过分减弱, 我们尽可能保持每个神经元的输入和输出的方差一致. 这样 $M_{l-1} \\operatorname{var}\\left(w_{i}^{(l)}\\right)$ 设为 1 比较合理, 即\n\n\n\\begin{equation*}\n\\operatorname{var}\\left(w_{i}^{(l)}\\right)=\\frac{1}{M_{l-1}} \\tag{7.42}\n\\end{equation*}\n\n\n同理, 为了使得在反向传播中, 误差信号也不被放大或缩小, 需要将 $w_{i}^{(l)}$ 的方差保持为\n\n\n\\begin{equation*}\n\\operatorname{var}\\left(w_{i}^{(l)}\\right)=\\frac{1}{M_{l}} . \\tag{7.43}\n\\end{equation*}\n\n\n作为折中, 同时考虑信号在前向和反向传播中都不被放大或缩小, 可以设置\n\n\n\\begin{equation*}\n\\operatorname{var}\\left(w_{i}^{(l)}\\right)=\\frac{2}{M_{l-1}+M_{l}} \\tag{7.44}\n\\end{equation*}\n\n\n在计算出参数的理想方差后, 可以通过高斯分布或均匀分布来随机初始化参数. 若采用高斯分布来随机初始化参数, 连接权重 $w_{i}^{(l)}$ 可以按 $\\mathcal{N}\\left(0, \\frac{2}{M_{l-1}+M_{l}}\\right)$的高斯分布进行初始化. 若采用区间为 $[-r, r]$ 的均匀分布来初始化 $w_{i}^{(l)}$, 则 $r$ 的取值为 $\\sqrt{\\frac{6}{M_{l-1}+M_{l}}}$. 这种根据每层的神经元数量来自动计算初始化参数方差的方法称为Xavier初始化[Glorot et al., 2010].\n\n虽然在 Xavier 初始化中我们假设激活函数为恒等函数, 但是 Xavier 初始化也适用于 Logistic 函数和 Tanh 函数. 这是因为神经元的参数和输入的绝对值通常比较小, 处于激活函数的线性区间. 这时 Logistic 函数和 Tanh 函数可以近似为线性函数. 由于 Logistic 函数在线性区间的斜率约为 0.25 , 因此其参数初始化的方差约为 $16 \\times \\frac{2}{M_{l-1}+M_{l}}$. 在实际应用中, 使用 Logistic 函数或 Tanh 函数的神经层通常将方差 $\\frac{2}{M_{l-1}+M_{l}}$ 乘以一个缩放因子 $\\rho$.\n\n参见习题7-4.\n\n参见公式(7.36).\n\nXavier初始化方法中, Xavier 是发明者 Xavier Glorot 的名字. Xavier 初始化也称为 Glorot 初始化.\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\n\n$\\rho$ 根据经验设定."
    },
    {
        "id": "chatcmpl-943ivVwm7EOseGOKd3RzyvU9zfoum",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"Xavier初始化的主要目标是什么？\",\n    \"回答1\": \"Xavier初始化的主要目标是使得在神经网络的前向传播和反向传播过程中，信号不被过分放大或过分减弱。为了实现这一目标，Xavier初始化方法尽可能保持每个神经元的输入和输出的方差一致，从而提高神经网络的训练稳定性。\",\n    \"问题2\": \"Xavier初始化中如何设置参数的理想方差？\",\n    \"回答2\": \"在Xavier初始化中，为了使得信号在前向和反向传播中都不被过分放大或缩小，参数的理想方差可以设置为 $\\frac{2}{M_{l-1}+M_{l}}$。这样可以保持信号的稳定性，有利于神经网络的训练和收敛。\",\n    \"问题3\": \"Xavier初始化中采用高斯分布来随机初始化参数时，连接权重 $w_{i}^{(l)}$ 的初始化遵循怎样的分布？\",\n    \"回答3\": \"在Xavier初始化中，采用高斯分布来随机初始化参数时，连接权重 $w_{i}^{(l)}$ 可以按 $\\text{N}\\big(0, \\frac{2}{M_{l-1}+M_{l}}\\big)$ 的高斯分布进行初始化。这种初始化方式有助于实现参数的合适分布，从而提高神经网络的训练效果。\",\n    \"问题4\": \"Xavier初始化中采用均匀分布来随机初始化参数时，连接权重 $w_{i}^{(l)}$ 的初始化范围如何确定？\",\n    \"回答4\": \"在Xavier初始化中，采用区间为 $[-r, r]$ 的均匀分布来初始化 $w_{i}^{(l)}$，其中 $r$ 的取值为 $\\text{根号下}\\big{\\frac{6}{M_{l-1}+M_{l}}\\big}$。通过这种方式，可以确保参数的初始化范围合理，并有利于神经网络的训练和收敛。\",\n    \"问题5\": \"Xavier初始化方法适用于哪些激活函数？\",\n    \"回答5\": \"虽然在 Xavier 初始化中假设激活函数为恒等函数，但是 Xavier 初始化也适用于 Logistic 函数和 Tanh 函数。这是因为神经元的参数和输入通常比较小，处于激活函数的线性区间。因此，Logistic 函数和 Tanh 函数在这种情况下可以近似为线性函数。\",\n    \"问题6\": \"Xavier初始化方法中的缩放因子 $\\rho$ 是如何确定的？\",\n    \"回答6\": \"在实际应用中，使用 Logistic 函数或 Tanh 函数的神经层通常将方差 $\\frac{2}{M_{l-1}+M_{l}}$ 乘以一个缩放因子 $\\rho$。这个缩放因子 $\\rho$ 根据经验设定，可以根据具体任务和数据特点进行调整。\",\n    \"问题7\": \"Xavier初始化方法中的 Xavier 是指什么？\",\n    \"回答7\": \"Xavier 初始化方法中的 Xavier 是指发明者 Xavier Glorot 的名字。因此，Xavier 初始化也称为 Glorot 初始化，以纪念其发明者。\",\n    \"问题8\": \"Xavier初始化方法如何保持每个神经元的输入和输出的方差一致？\",\n    \"回答8\": \"为了使得在经过多层网络后，信号不被过分放大或过分减弱，Xavier初始化方法尽可能保持每个神经元的输入和输出的方差一致。通过设置参数的理想方差为 $\\frac{2}{M_{l-1}+M_{l}}$，可以实现这一目标。\",\n    \"问题9\": \"Xavier初始化方法对神经网络训练有什么影响？\",\n    \"回答9\": \"Xavier初始化方法有助于提高神经网络的训练稳定性和收敛速度。通过合理设置参数的初始分布和方差，可以减少梯度消失或梯度爆炸问题，从而提高神经网络模型的训练效果。\",\n    \"问题10\": \"Xavier初始化方法如何影响信号在前向传播和反向传播中的变化？\",\n    \"回答10\": \"Xavier初始化方法通过合理设置参数的初始方差，使得信号在前向传播和反向传播过程中不被过分放大或过分减弱。这有利于保持信号稳定性，并提高神经网络模型在训练过程中的表现。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710754653,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1305,
            "prompt_tokens": 4134,
            "total_tokens": 5439
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]