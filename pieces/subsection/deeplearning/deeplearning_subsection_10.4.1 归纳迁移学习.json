[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "10.4.1 归纳迁移学习",
        "content_length": 1982,
        "content": "Title: 10.4.1 归纳迁移学习\nContent:\n在归纳迁移学习中, 源领域和目标领域有相同的输入空间 $x_{S}=\\mathcal{X}_{T}$, 输出空间可以相同也可以不同, 源任务和目标任务一般不相同 $\\mathcal{J}_{S} \\neq \\mathcal{T}_{T}$, 即 $p_{S}(y \\mid \\boldsymbol{x}) \\neq$ $p_{T}(y \\mid \\boldsymbol{x})$. 一般而言, 归纳迁移学习要求源领域和目标领域是相关的, 并且源领域 $\\mathcal{D}_{S}$ 有大量的训练样本, 这些样本可以是有标注的样本, 也可以是无标注样本.\n\n（1）当源领域只有大量无标注数据时，源任务可以转换为无监督学习任务, 比如自编码和密度估计任务. 通过这些无监督任务学习一种可迁移的表示, 然后再将这种表示迁移到目标任务上. 这种学习方式和自学习 (Self-Taught Learning ) [Raina et al., 2007] 以及半监督学习比较类似. 比如在自然语言处理领域, 由于语言相关任务的标注成本比较高, 很多自然语言处理任务的标注数据都比较少, 这导致了在这些自然语言处理任务上经常会受限于训练样本数量而无法充分发挥深度学习模型的能力. 同时, 由于我们可以低成本地获取大规模的无标注自然语言文本, 因此一种自然的迁移学习方式是将大规模文本上的无监督学习 (比如语言模型) 中学到的知识迁移到一个新的目标任务上. 从早期的预训练词向量 ( 比如 word2vec [Mikolov et al., 2013] 和 GloVe [Pennington et al., 2014] 等) 到句子级表示 ( 比如 ELMO [Peters et al., 2018]、OpenAI GPT [Radford et al., 2018] 以及 BERT[Devlin et al., 2018]等) 都对自然语言处理任务有很大的促进作用.\n\n（2）当源领域有大量的标注数据时, 可以直接将源领域上训练的模型迁移到目标领域上. 比如在计算机视觉领域有大规模的图像分类数据集 ImageNet [Deng et al., 2009]. 由于在 ImageNet 数据集上有很多预训练的图像分类模型, 比如 AlexNet[Krizhevsky et al., 2012]、VGG [Simonyan et al., 2014]和 ResNet[He et al., 2016]等, 我们可以将这些预训练模型迁移到目标任务上.\n\n在归纳迁移学习中, 由于源领域的训练数据规模非常大, 这些预训练模型通常有比较好的泛化性, 其学习到的表示通常也适用于目标任务. 归纳迁移学习一般有下面两种迁移方式:\n\n（1）基于特征的方式:将预训练模型的输出或者是中间隐藏层的输出作为特征直接加入到目标任务的学习模型中. 目标任务的学习模型可以是一般的浅层分类器 ( 比如支持向量机等) 或一个新的神经网络模型.\n\n（2）精调的方式: 在目标任务上复用预训练模型的部分组件, 并对其参数进行精调 ( Fine-Tuning ).\n\n\\href{https://nndl.github.io/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%8F%82%E8%A7%81%E7%AC%AC9.1.3%E8%8A%82}{https://nndl.github.io/自编码器参见第9.1.3节}. 概率密度估计参见第9.2节。\\\\\n假设预训练模型是一个深度神经网络, 这个预训练网络中每一层的可迁移性也不尽相同 [Yosinski et al., 2014]. 通常来说, 网络的低层学习一些通用的低层特征, 中层或高层学习抽象的高级语义特征, 而最后几层一般学习和特定任务相关的特征. 因此, 根据目标任务的自身特点以及和源任务的相关性, 可以有针对性地选择预训练模型的不同层来迁移到目标任务中.\n\n将预训练模型迁移到目标任务上通常会比从零开始学习的方式更好, 主要体现在以下三点 [Torrey et al., 2010]: 1 ) 初始模型的性能一般比随机初始化的模型要好;2) 训练时模型的学习速度比从零开始学习要快, 收玫性更好;3) 模型的最终性能更好,具有更好的泛化性.\n\n归纳迁移学习和多任务学习也比较类似, 但有下面两点区别: 1) 多任务学习是同时学习多个不同任务, 而归纳迁移学习通常分为两个阶段, 即源任务上的学习阶段和目标任务上的迁移学习阶段;2) 归纳迁移学习是单向的知识迁移,希望提高模型在目标任务上的性能,而多任务学习是希望提高所有任务的性能."
    },
    {
        "id": "chatcmpl-944tYNoVPcKTvorgjaBBABy5oPWeJ",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"归纳迁移学习中的源领域和目标领域有何要求？\",\n    \"回答1\": \"在归纳迁移学习中，源领域和目标领域要求有相同的输入空间 $x_{S}=\\boldsymbol{X}_{T}$，输出空间可以相同也可以不同。同时，源任务和目标任务一般不相同 $\\boldsymbol{J}_{S} \\neq \\boldsymbol{T}_{T}$。\",\n    \"问题2\": \"当源领域只有大量无标注数据时，如何进行归纳迁移学习？\",\n    \"回答2\": \"当源领域只有大量无标注数据时，可以将源任务转换为无监督学习任务，例如自编码和密度估计任务。通过这些无监督任务学习一种可迁移的表示，然后再将这种表示迁移到目标任务上。\",\n    \"问题3\": \"在计算机视觉领域中，如何利用归纳迁移学习进行模型迁移？\",\n    \"回答3\": \"在计算机视觉领域中，如果源领域有大量的标注数据，可以直接将源领域上训练的模型迁移到目标领域上。例如，可以将在 ImageNet 数据集上预训练的图像分类模型迁移到目标任务上。\",\n    \"问题4\": \"归纳迁移学习中的两种迁移方式分别是什么？\",\n    \"回答4\": \"归纳迁移学习一般有基于特征的方式和精调的方式。基于特征的方式是将预训练模型的输出或者是中间隐藏层的输出作为特征直接加入到目标任务的学习模型中。而精调的方式是在目标任务上复用预训练模型的部分组件，并对其参数进行精调（Fine-Tuning）。\",\n    \"问题5\": \"预训练模型在归纳迁移学习中通常具有怎样的优势？\",\n    \"回答5\": \"预训练模型通常具有比较好的泛化性，其学习到的表示也适用于目标任务。将预训练模型迁移到目标任务上通常会比从零开始学习的方式更好，主要体现在初始模型的性能、训练速度和最终性能等方面。\",\n    \"问题6\": \"归纳迁移学习和多任务学习有何区别？\",\n    \"回答6\": \"归纳迁移学习通常分为两个阶段，即源任务上的学习阶段和目标任务上的迁移学习阶段；而多任务学习是同时学习多个不同任务。此外，归纳迁移学习是单向的知识迁移，希望提高模型在目标任务上的性能；而多任务学习是希望提高所有任务的性能。\",\n    \"问题7\": \"为什么在自然语言处理领域经常会使用归纳迁移学习？\",\n    \"回答7\": \"在自然语言处理领域，由于语言相关任务的标注成本比较高，很多自然语言处理任务的标注数据都比较少。因此，通过归纳迁移学习可以将大规模文本上的无监督学习中学到的知识迁移到一个新的目标任务上，从而充分发挥深度学习模型的能力。\",\n    \"问题8\": \"预训练模型中不同层次的可迁移性有何特点？\",\n    \"回答8\": \"预训练模型中每一层的可迁移性不尽相同。通常来说，网络的低层学习一些通用的低层特征，中层或高层学习抽象的高级语义特征，而最后几层一般学习和特定任务相关的特征。根据目标任务的自身特点以及和源任务的相关性，可以有针对性地选择预训练模型的不同层来迁移到目标任务中。\",\n    \"问题9\": \"如何利用大规模无标注自然语言文本进行归纳迁移学习？\",\n    \"回答9\": \"通过大规模无标注自然语言文本进行无监督学习（比如语言模型）中学到的知识可以被迁移到一个新的目标任务上。这种方式在自然语言处理领域取得了很大促进作用。\",\n    \"问题10\": \"归纳迁移学习相对于从零开始学习有哪些优势？\",\n    \"回答10\": \"将预训练模型迁移到目标任务上通常会比从零开始学习的方式更好，主要体现在初始模型的性能、训练速度和最终性能等方面。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710759156,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1419,
            "prompt_tokens": 4243,
            "total_tokens": 5662
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]