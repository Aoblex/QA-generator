[
    {
        "strategy": "subsection",
        "source": "deeplearning",
        "title": "7.7.4 丢弃法",
        "content_length": 1922,
        "content": "Title: 7.7.4 丢弃法\nContent:\n当训练一个深度神经网络时, 我们可以随机丢弃一部分神经元 (同时丢弃其对应的连接边) 来避免过拟合, 这种方法称为丢弃法 (Dropout Method) [Srivastava et al., 2014]. 每次选择丢弃的神经元是随机的. 最简单的方法是设置一个固定的概率 $p$. 对每一个神经元都以概率 $p$ 来判定要不要保留. 对于一个神经层 $\\boldsymbol{y}=f(\\boldsymbol{W} \\boldsymbol{x}+\\boldsymbol{b})$, 我们可以引入一个掩蔽函数 $\\operatorname{mask}(\\cdot)$ 使得 $\\boldsymbol{y}=$ $f(\\boldsymbol{W} \\operatorname{mask}(\\boldsymbol{x})+\\boldsymbol{b})$. 掩蔽函数 $\\operatorname{mask}(\\cdot)$ 的定义为\n\n\\[\n\\operatorname{mask}(\\boldsymbol{x})= \\begin{cases}\\boldsymbol{m} \\odot \\boldsymbol{x} & \\text { 当训练阶段时 }  \\tag{7.74}\\\\ p \\boldsymbol{x} & \\text { 当测试阶段时 }\\end{cases}\n\\]\n\n其中 $\\boldsymbol{m} \\in\\{0,1\\}^{D}$ 是丢弃掩码 (Dropout Mask), 通过以概率为 $p$ 的伯努利分 $D$ 为输入 $\\boldsymbol{x}$ 的维度.布随机生成. 在训练时, 激活神经元的平均数量为原来的 $p$ 倍. 而在测试时, 所有的神经元都是可以激活的, 这会造成训练和测试时网络的输出不一致. 为了缓解这个问题, 在测试时需要将神经层的输入 $\\boldsymbol{x}$ 乘以 $p$, 也相当于把不同的神经网络做了平均. 保留率 $p$ 可以通过验证集来选取一个最优的值. 一般来讲, 对于隐藏层的神经元, 其保留率 $p=0.5$ 时效果最好, 这对大部分的网络和任务都比较有效. 当 $p=0.5$ 时, 在训练时有一半的神经元被丢弃, 只剩余一半的神经元是可以激活的, 随机生成的网络结构最具多样性. 对于输入层的神经元, 其保留率通常设为更接近 1 的数, 使得输入变化不会太大. 对输入层神经元进行丢弃时, 相当于给数据增加噪声, 以此来提高网络的鲁棒性.\n\n丢弃法一般是针对神经元进行随机丢弃, 但是也可以扩展到对神经元之间的连接进行随机丢弃 [Wan et al., 2013], 或每一层进行随机丢弃. 图7.12给出了一个网络应用丢弃法后的示例.\n\n\n\n(a) 标准网络\n\n\n\n(b) 应用丢弃法后的网络\n\n图 7.12 丢弃法示例\n\n\\href{https://nndl.github.io/}{https://nndl.github.io/}\\\\\n集成学习角度的解释 每做一次丢弃, 相当于从原始的网络中采样得到一个子网络. 如果一个神经网络有 $n$ 个神经元, 那么总共可以采样出 $2^{n}$ 个子网络. 每次迭代都相当于训练一个不同的子网络, 这些子网络都共享原始网络的参数. 那么, 最终的网络可以近似看作集成了指数级个不同网络的组合模型.\n\n贝叶斯学习角度的解释 丢弃法也可以解释为一种贝叶斯学习的近似 [Gal et al., 2016a]. 用 $y=f(\\boldsymbol{x} ; \\theta)$ 来表示要学习的神经网络, 贝叶斯学习是假设参数 $\\theta$ 为随机向量, 并且先验分布为 $q(\\theta)$, 贝叶斯方法的预测为\n\n\n\\begin{align*}\n\\mathbb{E}_{q(\\theta)}[y] & =\\int_{q} f(\\boldsymbol{x} ; \\theta) q(\\theta) d \\theta  \\tag{7.75}\\\\\n& \\approx \\frac{1}{M} \\sum_{m=1}^{M} f\\left(\\boldsymbol{x}, \\theta_{m}\\right) \\tag{7.76}\n\\end{align*}\n\n\n其中 $f\\left(\\boldsymbol{x}, \\theta_{m}\\right)$ 为第 $m$ 次应用丢弃方法后的网络, 其参数 $\\theta_{m}$ 为对全部参数 $\\theta$ 的一次采样."
    },
    {
        "id": "chatcmpl-944HXpLvzY201liUpufKrOITkzwI9",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"丢弃法（Dropout Method）是用来解决深度神经网络训练中的什么问题？\",\n    \"回答1\": \"丢弃法是用来解决深度神经网络训练中的过拟合问题。通过随机丢弃一部分神经元和其对应的连接边，可以提高模型的泛化能力，避免过拟合现象。\",\n    \"问题2\": \"丢弃法中的掩蔽函数 $\\text{mask}(\\boldsymbol{x})$ 是如何定义的？\",\n    \"回答2\": \"掩蔽函数 $\\text{mask}(\\boldsymbol{x})$ 的定义是：在训练阶段时，$\\text{mask}(\\boldsymbol{x})=\\boldsymbol{m} \\bigodot \\boldsymbol{x}$；在测试阶段时，$\\text{mask}(\\boldsymbol{x})=p \\boldsymbol{x}$。其中 $\\boldsymbol{m} \\text{ 是丢弃掩码（Dropout Mask）}$，通过以概率为 $p$ 的伯努利分布随机生成。\",\n    \"问题3\": \"在训练时，丢弃法对激活神经元的数量有什么影响？\",\n    \"回答3\": \"在训练时，丢弃法会导致激活神经元的平均数量为原来的 $p$ 倍。当 $p=0.5$ 时，在训练时有一半的神经元被丢弃，只剩余一半的神经元是可以激活的，从而增加了网络结构的多样性。\",\n    \"问题4\": \"为什么在测试时需要将神经层的输入乘以 $p$？\",\n    \"回答4\": \"在测试时需要将神经层的输入乘以 $p$，相当于对不同的神经网络做了平均。这是为了缓解训练和测试时网络输出不一致的问题，保持模型在不同阶段的一致性。\",\n    \"问题5\": \"丢弃法如何通过验证集来选择最优的保留率 $p$？\",\n    \"回答5\": \"保留率 $p$ 可以通过验证集来选取一个最优的值。一般来讲，对于隐藏层的神经元，其保留率 $p=0.5$ 时效果最好，而对输入层的神经元，其保留率通常设为更接近 1 的数。\",\n    \"问题6\": \"丢弃法如何提高网络的鲁棒性？\",\n    \"回答6\": \"当对输入层神经元进行丢弃时，相当于给数据增加噪声，从而提高了网络的鲁棒性。这有助于使模型对输入变化具有更好的适应能力。\",\n    \"问题7\": \"除了针对神经元进行随机丢弃，丢弃法还可以扩展到哪些方面？\",\n    \"回答7\": \"除了针对神经元进行随机丢弃外，丢弃法还可以扩展到对神经元之间的连接进行随机丢弃，或者扩展到每一层进行随机丢弃。\",\n    \"问题8\": \"丢弃法如何从集成学习角度进行解释？\",\n    \"回答8\": \"从集成学习角度来看，每次做一次丢弃相当于从原始网络中采样得到一个子网络。这些子网络共享原始网络的参数，最终可以近似看作集成了指数级个不同网络的组合模型。\",\n    \"问题9\": \"丢弃法如何从贝叶斯学习角度进行解释？\",\n    \"回答9\": \"从贝叶斯学习角度来看，丢弃法可以解释为一种贝叶斯学习的近似。通过应用丢弃方法后得到多个子网络，并将这些子网络参数进行平均，可以近似得到贝叶斯方法的预测。\",\n    \"问题10\": \"在贝叶斯学习角度下，丢弃法如何进行预测？\",\n    \"回答10\": \"在贝叶斯学习角度下，丢弃法进行预测时，通过对多次应用丢弃方法后得到的子网络参数进行平均，从而得到预测结果。这相当于对参数进行采样并求期望值。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710756799,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1255,
            "prompt_tokens": 3931,
            "total_tokens": 5186
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]