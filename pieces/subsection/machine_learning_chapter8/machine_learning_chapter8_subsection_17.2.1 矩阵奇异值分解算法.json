[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter8",
        "title": "17.2.1 矩阵奇异值分解算法",
        "content_length": 4618,
        "content": "Title: 17.2.1 矩阵奇异值分解算法\nContent:\n\\section*{1. 单词-文本矩阵}\n给定文本集合 $\\boldsymbol{D}=\\left\\{d_{1}, d_{2}, \\cdots, d_{n}\\right\\}$ 和单词集合 $\\boldsymbol{W}=\\left\\{w_{1}, w_{2}, \\cdots, w_{m}\\right\\}$ 。潜在语义分析首先将这些数据表示成一个单词-文本矩阵:\n\n\\[\n\\boldsymbol{X}=\\left[\\begin{array}{cccc}\nx_{11} & x_{12} & \\cdots & x_{1 n}  \\tag{17.12}\\\\\nx_{21} & x_{22} & \\cdots & x_{2 n} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\nx_{m 1} & x_{m 2} & \\cdots & x_{m n}\n\\end{array}\\right]\n\\]\n\n这是一个 $m \\times n$ 矩阵, 元素 $x_{i j}$ 表示单词 $w_{i}$ 在文本 $d_{j}$ 中出现的频数或权值。\n\n\\section*{2. 截断奇异值分解}\n潜在语义分析根据确定的话题个数 $k$ 对单词-文本矩阵 $\\boldsymbol{X}$ 进行截断奇异值分解:\n\n\\[\n\\boldsymbol{X} \\approx \\boldsymbol{U}_{k} \\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}=\\left[\\begin{array}{llll}\n\\boldsymbol{u}_{1} & \\boldsymbol{u}_{2} & \\cdots & \\boldsymbol{u}_{k}\n\\end{array}\\right]\\left[\\begin{array}{lllll}\n\\sigma_{1} & & &  \\tag{17.13}\\\\\n& \\sigma_{2} & & \\\\\n& & \\ddots & \\\\\n& & & \\sigma_{k}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\boldsymbol{v}_{1}^{\\mathrm{T}} \\\\\n\\boldsymbol{v}_{2}^{\\mathrm{T}} \\\\\n\\vdots \\\\\n\\boldsymbol{v}_{k}^{\\mathrm{T}}\n\\end{array}\\right]\n\\]\n\n式中 $k \\leqslant n \\leqslant m ; \\boldsymbol{U}_{k}$ 是 $m \\times k$ 矩阵, 它的列由 $\\boldsymbol{X}$ 的前 $k$ 个互相正交的左奇异向量组成; $\\boldsymbol{\\Sigma}_{k}$是 $k$ 阶对角方阵, 对角元素为前 $k$ 个最大奇异值; $\\boldsymbol{V}_{k}$ 是 $n \\times k$ 矩阵, 它的列由 $\\boldsymbol{X}$ 的前 $k$ 个互相正交的右奇异向量组成。\n\n\\section*{3. 话题向量空间}\n在单词-文本矩阵 $\\boldsymbol{X}$ 的截断奇异值分解式 (17.13) 中, 矩阵 $\\boldsymbol{U}_{k}$ 的每一个列向量 $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2}, \\cdots, \\boldsymbol{u}_{k}$ 表示一个话题, 称为话题向量。由这 $k$ 个话题向量张成一个子空间:\n\n$$\n\\boldsymbol{U}_{k}=\\left[\\begin{array}{llll}\n\\boldsymbol{u}_{1} & \\boldsymbol{u}_{2} & \\cdots & \\boldsymbol{u}_{k}\n\\end{array}\\right]\n$$\n\n称为话题向量空间。\n\n\\section*{4. 文本的话题空间表示}\n有了话题向量空间, 接着考虑文本在话题空间的表示。将式 (17.13) 写作\n\n$$\n\\begin{aligned}\n& \\boldsymbol{X}=\\left[\\begin{array}{llll}\n\\boldsymbol{x}_{1} & \\boldsymbol{x}_{2} & \\cdots & \\boldsymbol{x}_{n}\n\\end{array}\\right] \\approx \\boldsymbol{U}_{k} \\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}} \\\\\n& =\\left[\\begin{array}{llll}\n\\boldsymbol{u}_{1} & \\boldsymbol{u}_{2} & \\cdots & \\boldsymbol{u}_{k}\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sigma_{1} & & & \\\\\n& \\sigma_{2} & & \\\\\n& & \\ddots & \\\\\n& & & \\sigma_{k}\n\\end{array}\\right]\\left[\\begin{array}{cccc}\nv_{11} & v_{21} & \\cdots & v_{n 1} \\\\\nv_{12} & v_{22} & \\cdots & v_{n 2} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n& & & \\\\\nv_{1 k} & v_{2 k} & \\cdots & v_{n k}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{llll}\n\\boldsymbol{u}_{1} & \\boldsymbol{u}_{2} & \\cdots & \\boldsymbol{u}_{k}\n\\end{array}\\right]\\left[\\begin{array}{cccc}\n\\sigma_{1} v_{11} & \\sigma_{1} v_{21} & \\cdots & \\sigma_{1} v_{n 1} \\\\\n\\sigma_{2} v_{12} & \\sigma_{2} v_{22} & \\cdots & \\sigma_{2} v_{n 2} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\sigma_{k} v_{1 k} & \\sigma_{k} v_{2 k} & \\cdots & \\sigma_{k} v_{n k}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n其中，\n\n$$\n\\boldsymbol{u}_{l}=\\left[\\begin{array}{c}\nu_{1 l} \\\\\nu_{2 l} \\\\\n\\vdots \\\\\nu_{m l}\n\\end{array}\\right], \\quad l=1,2, \\cdots, k\n$$\n\n由式 (17.14) 知, 矩阵 $\\boldsymbol{X}$ 的第 $j$ 列向量 $\\boldsymbol{x}_{j}$ 满足\n\n\n\\begin{align*}\n\\boldsymbol{x}_{j} & \\approx \\boldsymbol{U}_{k}\\left(\\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}\\right)_{j} \\\\\n& =\\left[\\begin{array}{llll}\n\\boldsymbol{u}_{1} & \\boldsymbol{u}_{2} & \\cdots & \\boldsymbol{u}_{k}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\sigma_{1} v_{j 1} \\\\\n\\sigma_{2} v_{j 2} \\\\\n\\vdots \\\\\n\\sigma_{k} v_{j k}\n\\end{array}\\right] \\\\\n& =\\sum_{l=1}^{k} \\sigma_{l} v_{j l} \\boldsymbol{u}_{l}, \\quad j=1,2, \\cdots, n \\tag{17.15}\n\\end{align*}\n\n\n式中 $\\left(\\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}\\right)_{j}$ 是矩阵 $\\left(\\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}\\right)$ 的第 $j$ 列向量。式 (17.15) 是文本 $d_{j}$ 的近似表达式, 由 $k$ 个话题向量 $\\boldsymbol{u}_{l}$ 的线性组合构成。矩阵 $\\left(\\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}\\right)$ 的每一个列向量\n\n$$\n\\left[\\begin{array}{c}\n\\sigma_{1} v_{11} \\\\\n\\sigma_{2} v_{12} \\\\\n\\vdots \\\\\n\\sigma_{k} v_{1 k}\n\\end{array}\\right], \\quad\\left[\\begin{array}{c}\n\\sigma_{1} v_{21} \\\\\n\\sigma_{2} v_{22} \\\\\n\\vdots \\\\\n\\sigma_{k} v_{2 k}\n\\end{array}\\right], \\cdots,\\left[\\begin{array}{c}\n\\sigma_{1} v_{n 1} \\\\\n\\sigma_{2} v_{n 2} \\\\\n\\vdots \\\\\n\\sigma_{k} v_{n k}\n\\end{array}\\right]\n$$\n\n是一个文本在话题向量空间的表示。\n\n综上，可以通过对单词-文本矩阵的奇异值分解进行潜在语义分析\n\n\n\\begin{equation*}\n\\boldsymbol{X} \\approx \\boldsymbol{U}_{k} \\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}=\\boldsymbol{U}_{k}\\left(\\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}\\right) \\tag{17.16}\n\\end{equation*}\n\n\n得到话题空间 $U_{k}$, 以及文本在话题空间的表示 $\\left(\\boldsymbol{\\Sigma}_{k} \\boldsymbol{V}_{k}^{\\mathrm{T}}\\right)$ 。"
    },
    {
        "id": "chatcmpl-947KOjejCAxcXP3L1ILGi2hKjTf5R",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"单词-文本矩阵和普通矩阵有何不同？\",\n    \"回答1\": \"单词-文本矩阵是由给定文本集合和单词集合构成的矩阵，其中元素 $x_{i j}$ 表示单词 $w_{i}$ 在文本 $d_{j}$ 中出现的频数或权值。与普通矩阵相比，单词-文本矩阵具有特定的语义信息，反映了单词在不同文本中的出现情况，因此在潜在语义分析中具有特殊的意义。\",\n    \"问题2\": \"截断奇异值分解是如何对单词-文本矩阵进行分解的？\",\n    \"回答2\": \"截断奇异值分解是根据确定的话题个数 $k$ 对单词-文本矩阵 $\\boldsymbol{X}$ 进行分解，得到近似矩阵 $\\boldsymbol{U}_{k} \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{V}_{k}^{\\top}}}}}}}}}}}}}}}}}}}}}$ 。其中 $\\boldsymbol{U}_{k}$ 是 $m \\times k$ 矩阵，$\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{V}_{k}^{\\top}}}}}}}}}$ 是 $n \\times k$ 矩阵，它们分别由 $\\boldsymbol{X}$ 的前 $k$ 个互相正交的左奇异向量和右奇异向量组成。\",\n    \"问题3\": \"话题向量空间是如何定义的？\",\n    \"回答3\": \"话题向量空间由截断奇异值分解得到的矩阵 $\\boldsymbol{U}_{k}$ 的列向量 $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2}, \\text{...}, \\boldsymbol{u}_{k}$ 张成，每个列向量表示一个话题。这个子空间称为话题向量空间，用于表示文本数据中的话题信息。\",\n    \"问题4\": \"文本在话题空间的表示是如何计算得到的？\",\n    \"回答4\": \"文本在话题空间的表示是通过截断奇异值分解得到的近似矩阵 $\\boldsymbol{U}_{k} \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{V}_{k}^{\\top}}}}}}}}}}}}}}}$ 计算得到。具体而言，矩阵 $\\boldsymbol{X}$ 的第 $j$ 列向量 $\\boldsymbol{x}_{j}$ 可以通过线性组合得到，表示为 $\\boldsymbol{x}_{j} \\text{约等于} \\text{...}$。\",\n    \"问题5\": \"潜在语义分析是如何利用奇异值分解进行的？\",\n    \"回答5\": \"潜在语义分析通过对单词-文本矩阵进行截断奇异值分解，得到近似矩阵 $\\boldsymbol{U}_{k} \\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{V}_{k}^{\\top}}}}}}}}}$ ，从而得到话题空间 $U_{k}$ 和文本在话题空间的表示。这一过程利用了奇异值分解的特性，将原始数据映射到更低维度的空间中，实现了对数据语义信息的提取。\",\n    \"问题6\": \"截断奇异值分解中的参数 $k$ 有何含义？\",\n    \"回答6\": \"$k$ 是截断奇异值分解中确定的话题个数，它决定了最终近似矩阵的维度。较小的 $k$ 可以实现对数据压缩和降维，同时保留主要的语义信息；较大的 $k$ 则可以更精确地表示原始数据，但也可能包含更多噪音和冗余信息。\",\n    \"问题7\": \"奇异值分解与主题模型有何关联？\",\n    \"回答7\": \"奇异值分解可以用于潜在语义分析中提取数据的主题信息，从而实现对文本数据中隐藏主题的挖掘。通过截断奇异值分解得到的话题向量空间和文本在话题空间的表示，可以帮助理解文本数据中隐藏的语义结构和主题信息。\",\n    \"问题8\": \"潜在语义分析如何帮助理解文本数据中隐藏的语义结构？\",\n    \"回答8\": \"潜在语义分析通过奇异值分解将原始文本数据映射到更低维度的话题向量空间中，从而实现对文本数据中隐藏的语义结构和主题信息的挖掘和理解。这有助于发现文本数据中隐藏的语义关联和主题特征，为进一步的文本挖掘和理解提供了基础。\",\n    \"问题9\": \"截断奇异值分解对于大规模文本数据处理有何优势？\",\n    \"回答9\": \"截断奇异值分解能够实现对大规模文本数据进行降维和压缩，并提取其中隐藏的主题信息。通过降低数据维度和去除噪音信息，可以更高效地处理大规模文本数据，并从中获取有价值的语义信息和主题特征。\",\n    \"问题10\": \"潜在语义分析与自然语言处理领域有何关系？\",\n    \"回答10\": \"潜在语义分析是自然语言处理领域中重要的技术手段之一，用于挖掘和理解大规模文本数据中隐藏的语义结构和主题信息。它为自然语言处理任务（如文本分类、信息检索、情感分析等）提供了有效的特征提取和数据理解方法。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710768508,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1726,
            "prompt_tokens": 4856,
            "total_tokens": 6582
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]