[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter8",
        "title": "17.3.1 非负矩阵分解",
        "content_length": 1724,
        "content": "Title: 17.3.1 非负矩阵分解\nContent:\n若一个矩阵的所有元素非负, 则称该矩阵为非负矩阵, 若 $\\boldsymbol{X}$ 是非负矩阵, 则记作 $\\boldsymbol{X} \\geqslant 0$ 。给定一个非负矩阵 $\\boldsymbol{X} \\geqslant 0$, 找到两个非负矩阵 $\\boldsymbol{W} \\geqslant 0$ 和 $\\boldsymbol{H} \\geqslant 0$, 使得\n\n\n\\begin{equation*}\n\\boldsymbol{X} \\approx \\boldsymbol{W} \\boldsymbol{H} \\tag{17.17}\n\\end{equation*}\n\n\n即将非负矩阵 $\\boldsymbol{X}$ 分解为两个非负矩阵 $\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 的乘积的形式, 称为非负矩阵分解。因为 $\\boldsymbol{W} \\boldsymbol{H}$ 与 $\\boldsymbol{X}$ 完全相等很难实现, 所以只要求 $\\boldsymbol{W} \\boldsymbol{H}$ 与 $\\boldsymbol{X}$ 近似相等。\n\n假设非负矩阵 $\\boldsymbol{X}$ 是 $m \\times n$ 矩阵, 非负矩阵 $\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 分别为 $m \\times k$ 矩阵和 $k \\times n$ 矩阵。假设 $k<\\min (m, n)$, 即 $\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 小于原矩阵 $\\boldsymbol{X}$, 所以非负矩阵分解是对原数据的压缩。\n\n由式 (17.17) 知, 矩阵 $\\boldsymbol{X}$ 的第 $j$ 列向量 $\\boldsymbol{x}_{j}$ 满足\n\n\n\\begin{align*}\n\\boldsymbol{x}_{j} & \\approx \\boldsymbol{W} \\boldsymbol{h}_{j} \\\\\n& =\\left[\\begin{array}{llll}\n\\boldsymbol{w}_{1} & \\boldsymbol{w}_{2} & \\cdots & \\boldsymbol{w}_{k}\n\\end{array}\\right]\\left[\\begin{array}{c}\nh_{1 j} \\\\\nh_{2 j} \\\\\n\\vdots \\\\\nh_{k j}\n\\end{array}\\right] \\\\\n& =\\sum_{l=1}^{k} h_{l j} \\boldsymbol{w}_{l}, \\quad j=1,2, \\cdots, n \\tag{17.18}\n\\end{align*}\n\n\n其中, $\\boldsymbol{h}_{j}$ 是矩阵 $\\boldsymbol{H}$ 的第 $j$ 列, $\\boldsymbol{w}_{l}$ 是矩阵 $\\boldsymbol{W}$ 的第 $l$ 列, $h_{l j}$ 是 $\\boldsymbol{h}_{j}$ 的第 $l$ 个元素, $l=1,2, \\cdots, k$ 。\n\n式 (17.18) 表明, 矩阵 $\\boldsymbol{X}$ 的第 $j$ 列 $\\boldsymbol{x}_{j}$ 可以由矩阵 $\\boldsymbol{W}$ 的 $k$ 个列 $\\boldsymbol{w}_{l}$ 的线性组合逼近, 线性组合的系数是矩阵 $\\boldsymbol{H}$ 的第 $j$ 列 $\\boldsymbol{h}_{j}$ 的元素。这里矩阵 $\\boldsymbol{W}$ 的列向量为一组基, 矩阵 $\\boldsymbol{H}$ 的列向量为线性组合系数。称 $\\boldsymbol{W}$ 为基矩阵, $\\boldsymbol{H}$ 为系数矩阵。非负矩阵分解旨在用较少的基向量、系数向量来表示较大的数据矩阵。"
    },
    {
        "id": "chatcmpl-947MIrVy7JAif90zFzUtS3KwrtaRH",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是非负矩阵？\",\n    \"回答1\": \"若一个矩阵的所有元素非负，则称该矩阵为非负矩阵。对于一个非负矩阵 $\\boldsymbol{X}$，可以记作 $\\boldsymbol{X} \\text{ 为 } \\boldsymbol{X} \\text{ 的所有元素非负}$。\",\n    \"问题2\": \"什么是非负矩阵分解？\",\n    \"回答2\": \"给定一个非负矩阵 $\\boldsymbol{X} \\text{ 为 } \\boldsymbol{X} \\text{ 的所有元素非负}$，非负矩阵分解旨在找到两个非负矩阵 $\\boldsymbol{W} \\text{ 为 } \\boldsymbol{W} \\text{ 的所有元素非负}$ 和 $\\boldsymbol{H} \\text{ 为 } \\boldsymbol{H} \\text{ 的所有元素非负}$，使得 $\\boldsymbol{X}$ 可以近似表示为 $\\boldsymbol{W} \\boldsymbol{H}$ 的乘积形式。\",\n    \"问题3\": \"非负矩阵分解的目标是什么？\",\n    \"回答3\": \"非负矩阵分解的目标是通过较少的基向量和系数向量来表示较大的数据矩阵。即将原始的非负矩阵 $\\boldsymbol{X}$ 分解为两个非负矩阵 $\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 的乘积形式，以实现对原数据的压缩和近似表示。\",\n    \"问题4\": \"在非负矩阵分解中，如何表示矩阵近似相等关系？\",\n    \"回答4\": \"在非负矩阵分解中，通过式 (17.17) 表示矩阵 $\\boldsymbol{X}$ 可以近似表示为 $\\boldsymbol{W} \\boldsymbol{H}$ 的乘积形式。这里的近似相等关系意味着虽然 $\\boldsymbol{W} \\boldsymbol{H}$ 与 $\\boldsymbol{X}$ 完全相等很难实现，但我们希望它们尽可能接近。\",\n    \"问题5\": \"在非负矩阵分解中，如何表示矩阵的列向量近似表示？\",\n    \"回答5\": \"在非负矩阵分解中，矩阵 $\\boldsymbol{X}$ 的第 $j$ 列向量 $\\boldsymbol{x}_{j}$ 可以由矩阵 $\\boldsymbol{W}$ 的 $k$ 个列向量的线性组合逼近，其中线性组合的系数是矩阵 $\\boldsymbol{H}$ 的第 $j$ 列向量 $\\boldsymbol{h}_{j}$ 的元素。这里的线性组合表示了对原始数据的近似表示。\",\n    \"问题6\": \"在非负矩阵分解中，$\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 分别代表什么？\",\n    \"回答6\": \"$\\boldsymbol{W}$ 被称为基矩阵，它是用来表示原始数据的基向量集合；$\\boldsymbol{H}$ 被称为系数矩阵，它包含了用来线性组合基向量以逼近原始数据的系数向量集合。\",\n    \"问题7\": \"在非负矩阵分解中，为什么要求$\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 是非负矩阵？\",\n    \"回答7\": \"要求$\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 是非负矩阵是因为我们希望分解后得到的基向量和系数向量都能够保持非负性。这样可以更好地保留原始数据的特征，并且符合实际场景中许多数据都具有非负性的特点。\",\n    \"问题8\": \"在非负矩阵分解中，如何理解对原数据的压缩？\",\n    \"回答8\": \"在非负矩阵分解中，假设 $k<\\text{min}(m, n)$，即 $\\boldsymbol{W}$ 和 $\\boldsymbol{H}$ 小于原矩阵 $\\boldsymbol{X}$。这意味着通过较少的基向量和系数向量来表示较大的数据矩阵，实现了对原数据的压缩。\",\n    \"问题9\": \"在非负矩阵分解中，如何理解基向量和系数向量对原始数据的表示作用？\",\n    \"回答9\": \"在非负矩阵分解中，$\\boldsymbol{W}$ 被称为基矩阵，它包含了用来表示原始数据的基向量集合；$\\boldsymbol{H}$ 被称为系数矩阵，它包含了用来线性组合基向量以逼近原始数据的系数向量集合。通过这种方式，可以用较少的基向量和系数向量来表示较大的数据矩阵。\",\n    \"问题10\": \"在非负矩阵分解中，为什么要求$k<\\text{min}(m, n)$？\",\n    \"回答10\": \"$k<\\text{min}(m, n)$ 的要求意味着我们希望通过较少的基向量和系数向量来表示较大的数据矩阵。这样可以实现对原数据的压缩，并且能够更好地逼近原始数据。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710768626,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1533,
            "prompt_tokens": 3578,
            "total_tokens": 5111
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]