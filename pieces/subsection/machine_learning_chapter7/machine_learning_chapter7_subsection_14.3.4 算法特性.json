[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter7",
        "title": "14.3.4 算法特性",
        "content_length": 4560,
        "content": "Title: 14.3.4 算法特性\nContent:\n\\section*{1. 总体特点}\n$k$ 均值聚类有以下特点: 基于划分的聚类方法; 类别数 $k$ 事先指定; 以欧氏距离平方表示样本之间的距离, 以中心或样本的均值表示类别; 以样本和其所属类的中心之间的距离的总和为最优化的目标函数; 得到的类别是平坦的、非层次化的; 算法是迭代算法, 不能保证得到全局最优。\n\n\\section*{2. 收敛性}\n$k$ 均值聚类属于启发式方法, 不能保证收玫到全局最优, 初始中心的选择会直接影响聚类结果。注意, 类中心在聚类的过程中会发生移动, 但是往往不会移动太大, 因为在每一步,样本被分到与其最近的中心的类中。\n\n\\section*{3. 初始类的选择}\n选择不同的初始中心会得到不同的聚类结果。针对上面的例 14.2, 如果改变两个类的初始中心, 比如选择 $m_{1}^{(0)}=x_{1}$ 和 $m_{2}^{(0)}=x_{5}$, 那么 $x_{2}, x_{3}$ 会分到 $G_{1}^{(0)}, x_{4}$ 会分到 $G_{2}^{(0)}$, 形成聚类结果 $G_{1}^{(1)}=\\left\\{x_{1}, x_{2}, x_{3}\\right\\}, G_{2}^{(1)}=\\left\\{x_{4}, x_{5}\\right\\}$ 。中心是 $m_{1}^{(1)}=(0.33,0.67)^{\\mathrm{T}}, m_{2}^{(1)}=(5,1)^{\\mathrm{T}}$ 。继续迭代, 聚类结果仍然是 $G_{1}^{(2)}=\\left\\{x_{1}, x_{2}, x_{3}\\right\\}, G_{2}^{(2)}=\\left\\{x_{4}, x_{5}\\right\\}$ 。聚类停止。\n\n对于初始中心的选择, 可以用层次聚类对样本进行聚类, 得到 $k$ 个类时停止, 然后从每个类中选取一个与中心距离最近的点。\n\n\\section*{4. 类别数 $k$ 的选择}\n$k$ 均值聚类中的类别数 $k$ 值需要预先指定, 而在实际应用中最优的 $k$ 值是不知道的。解决这个问题的一个方法是尝试用不同的 $k$ 值聚类，检验各自得到的聚类结果的质量，推测最\\\\\n优的 $k$ 值。聚类结果的质量可以用类的平均直径来衡量。一般地, 类别数变小时, 平均直径会增加; 类别数变大超过某个值以后, 平均直径会不变, 而这个值正是最优的 $k$ 值。图 14.3 说明类别数与平均直径的关系。实验时, 可以采用二分查找, 快速找到最优的 $k$ 值。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_690a9c6ba63c19ff9106g-09}\n\\end{center}\n\n图 14.3 类别数与平均直径的关系\n\n\\section*{本章概要}\n\\begin{enumerate}\n  \\item 聚类是针对给定的样本, 依据它们属性的相似度或距离, 将其归并到若干个 “类”或 “簇” 的数据分析问题。一个类是样本的一个子集。直观上，相似的样本聚集在同类，不相似的样本分散在不同类。\n\n  \\item 距离或相似度度量在聚类中起着重要作用。\n\n\\end{enumerate}\n\n常用的距离度量有闵可夫斯基距离, 包括欧氏距离、曼哈顿距离、切比雪夫距离以及马哈拉诺比斯距离。常用的相似度度量有相关系数、夹角余弦。\n\n用距离度量相似度时, 距离越小表示样本越相似; 用相关系数时, 相关系数越大表示样本越相似。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{2}\n  \\item 类是样本的子集, 比如有如下基本定义:\n\\end{enumerate}\n\n用 $G$ 表示类或簇, 用 $x_{i}, x_{j}$ 等表示类中的样本, 用 $d_{i j}$ 表示样本 $x_{i}$ 与样本 $x_{j}$ 之间的距离。如果对任意的 $x_{i}, x_{j} \\in G$, 有\n\n$$\nd_{i j} \\leqslant T\n$$\n\n则称 $G$ 为一个类或簇。\n\n描述类的特征的指标有中心、直径、散布矩阵、协方差矩阵。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{3}\n  \\item 聚类过程中用到的类与类之间的距离也称为连接。类与类之间的距离包括最短距离、最长距离、中心距离、平均距离。\n\n  \\item 层次聚类假设类别之间存在层次结构, 将样本聚到层次化的类中。层次聚类又有聚合或自下而上、分裂或自上而下两种方法。\n\n\\end{enumerate}\n\n聚合聚类开始将每个样本各自分到一个类, 之后将相距最近的两类合并, 建立一个新的类, 重复此操作直到满足停止条件, 得到层次化的类别。分裂聚类开始将所有样本分到一个类, 之后将已有类中相距最远的样本分到两个新的类, 重复此操作直到满足停止条件, 得到层次化的类别。\\\\\n聚合聚类需要预先确定下面三个要素:\n\n(1) 距离或相似度;\n\n(2) 合并规则;\n\n(3) 停止条件。\n\n根据这些概念的不同组合, 就可以得到不同的聚类方法。\\\\\n6. $k$ 均值聚类是常用的聚类算法, 有以下特点: 基于划分的聚类方法; 类别数 $k$ 事先指定; 以欧氏距离平方表示样本之间的距离或相似度, 以中心或样本的均值表示类别; 以样本和其所属类的中心之间的距离的总和为优化的目标函数; 得到的类别是平坦的、非层次化的;算法是迭代算法, 不能保证得到全局最优。\n\n对于 $k$ 均值聚类算法, 首先选择 $k$ 个类的中心, 将样本分到与中心最近的类中, 得到一个聚类结果; 然后计算每个类的样本的均值, 作为类的新的中心; 重复以上步骤, 直到收玫为止。\n\n\\section*{继续阅 读}\n聚类的方法很多，各种方法的详细介绍可参见文献 [1] 和文献 [2]。层次化聚类的方法可参见文献 [2], $k$ 均值聚类可参见文献 [3] 和文献 [4]。 $k$ 均值聚类的扩展有 X-means ${ }^{[5]}$ 。其他常用的聚类方法还有基于混合分布的方法, 如高斯混合模型与 EM 算法; 基于密度的方法, 如 DBScan ${ }^{[6]}$; 基于谱聚类的方法, 如 Normalized Cuts ${ }^{[7]}$ 。以上方法是对样本的聚类, 也有对样本与属性同时聚类的方法, 如 Co-Clustering ${ }^{[8]}$ 。\n\n\\section*{习 题}\n14.1 试写出分裂聚类算法, 自上而下地对数据进行聚类, 并给出其算法复杂度。\n\n14.2 证明类或簇的四个定义中, 第一个定义可推出其他三个定义。\n\n14.3 证明式 (14.21) 成立, 即 $k$ 均值的可能解的个数是指数级的。\n\n14.4 比较 $k$ 均值聚类与高斯混合模型加 EM 算法的异同。\n\n\\section*{参考文献}\n[1] JAIN A, DUBES R. Algorithms for clustering data[M]. Prentice-Hall, 1988.\n\n[2] AGGARWAL C C, REDDY C K. Data clustering: algorithms and applications $[M]$. CRC Press, 2013.\n\n[3] MACQUEEN J B. Some methods for classification and analysis of multivariate observations[C]// Procceedings of 5th Symposium on Mathematical Statistics and Probability. 1967: 396-410.\n\n[4] HASTIE T, TIBSHIRANI R, FRIEDMAN J. The elements of statistical learning: data mining, inference, and prediction[M]. 范明, 柴玉梅, 旨红英, 等译. Springer, 2001.\n\n[5] PElleG D, MOORE A W. X-means: extending $K$-means with efficient estimation of the number of clusters[C]//Proceedings of ICML. 2000: 727-734.\n\n[6] ESTER M, KRIEGEL H, SANDER J, et al. A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of ACM SIGKDD. 1996: 226-231.\n\n[7] SHI J, MALIK J. Normalized cuts and image segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22(8): 888-905.\n\n[8] DHILLON I S. Co-clustering documents and words using bipartite spectral graph partitioning[C]//Proceedings of ACM SIGKDD. 2001: 269-274.\n\n\\section*{第 15 章 奇异值分解}\n奇异值分解 (singular value decomposition, SVD) 是一种矩阵因子分解方法, 是线性代数的概念, 但在机器学习中被广泛使用, 成为其重要工具。本书介绍的主成分分析、潜在语义分析都用到奇异值分解。\n\n任意一个 $m \\times n$ 矩阵, 都可以表示为三个矩阵的乘积 (因子分解) 形式, 分别是 $m$ 阶正交矩阵、由降序排列的非负的对角线元素组成的 $m \\times n$ 矩形对角矩阵和 $n$ 阶正交矩阵, 称为该矩阵的奇异值分解。矩阵的奇异值分解一定存在, 但不唯一。奇异值分解可以看作是矩阵数据压缩的一种方法, 即用因子分解的方式近似地表示原始矩阵, 这种近似是在平方损失意义下的最优近似。\n\n15.1 节讲述矩阵奇异值分解的定义与基本定理, 叙述奇异值分解的紧凑和截断形式、几何解释、主要性质; 15.2 节讲述奇异值分解的算法; 15.3 节论述奇异值分解是矩阵的一种最优近似方法。\n\n\\section*{15.1 奇异值分解的定义与性质}"
    },
    {
        "id": "chatcmpl-93z0K5Z4oLYOfid1zVVrdhvpHkVKU",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是$k$均值聚类的总体特点？\",\n    \"回答1\": \"$k$均值聚类是一种基于划分的聚类方法，其中类别数$k$事先指定。它以欧氏距离平方表示样本之间的距离，以中心或样本的均值表示类别，并以样本和其所属类的中心之间的距离的总和为最优化的目标函数。得到的类别是平坦的、非层次化的，而且算法是迭代算法，不能保证得到全局最优。\",\n    \"问题2\": \"$k$均值聚类属于什么类型的方法？它的收敛性如何？\",\n    \"回答2\": \"$k$均值聚类属于启发式方法，不能保证收敛到全局最优。初始中心的选择会直接影响聚类结果。在聚类过程中，类中心会发生移动，但往往不会移动太大，因为在每一步，样本被分到与其最近的中心的类中。\",\n    \"问题3\": \"对于$k$均值聚类，初始类的选择对聚类结果有何影响？\",\n    \"回答3\": \"选择不同的初始中心会得到不同的聚类结果。改变初始中心会导致不同的样本被分到不同的类中，从而影响最终的聚类结果。\",\n    \"问题4\": \"在$k$均值聚类中，如何选择合适的类别数$k$？\",\n    \"回答4\": \"$k$均值聚类中的类别数$k$值需要预先指定，而在实际应用中最优的$k$值是不知道的。一种方法是尝试用不同的$k$值进行聚类，检验各自得到的聚类结果的质量，推测最优的$k$值。聚类结果的质量可以用类的平均直径来衡量。\",\n    \"问题5\": \"描述一下层次聚类方法以及其两种不同的实现方式。\",\n    \"回答5\": \"层次聚类假设类别之间存在层次结构，将样本聚到层次化的类中。层次聚类有两种实现方式：聚合或自下而上和分裂或自上而下。在聚合聚类中，开始将每个样本各自分到一个类，之后将相距最近的两个类合并，重复此操作直到满足停止条件；而在分裂聚类中，开始将所有样本分到一个类，之后将已有类中相距最远的样本分到两个新的类，重复此操作直到满足停止条件。\",\n    \"问题6\": \"什么是奇异值分解（SVD）？它在机器学习中有什么应用？\",\n    \"回答6\": \"奇异值分解（SVD）是一种矩阵因子分解方法，在机器学习中被广泛使用。任意一个$m \\times n$矩阵都可以表示为三个矩阵的乘积形式，分别是$m$阶正交矩阵、由降序排列的非负对角线元素组成的$m \\times n$矩形对角矩阵和$n$阶正交矩阵。SVD可以看作是矩阵数据压缩的一种方法，在平方损失意义下提供最优近似。\",\n    \"问题7\": \"在$k$均值聚类算法中，如何选择初始中心？\",\n    \"回答7\": \"对于$k$均值聚类算法，可以首先选择$k$个类的中心，将样本分到与中心最近的类中，得到一个初始聚类结果。通常可以采用层次聚类对样本进行聚类，并从每个类中选取一个与中心距离最近的点作为初始中心。\",\n    \"问题8\": \"如何衡量聚类结果的质量以确定最优的$k$值？\",\n    \"回答8\": \"一种方法是尝试用不同的$k$值进行聚类，检验各自得到的聚类结果的质量。通常可以用类的平均直径来衡量聚类结果的质量。通过观察平均直径随着$k$值变化的趋势，可以推测最优的$k$值。\",\n    \"问题9\": \"除了$k$均值聚类外，还有哪些常用的聚类方法？\",\n    \"回答9\": \"除了$k$均值聚类外，常用的聚类方法还包括基于混合分布的方法（如高斯混合模型与EM算法）、基于密度的方法（如DBScan）、基于谱聚类的方法（如Normalized Cuts）等。这些方法在处理不同类型数据和场景时具有各自特点和优势。\",\n    \"问题10\": \"在实际应用中，如何确定最优的$k$值？\",\n    \"回答10\": \"在实际应用中，可以尝试用不同的$k$值进行聚类，并检验各自得到的聚类结果质量。一种常见方法是采用二分查找来快速找到最优的$k$值。通过这种方式可以有效地确定最适合数据特征和任务要求的最优$k$值。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710736512,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1524,
            "prompt_tokens": 5927,
            "total_tokens": 7451
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]