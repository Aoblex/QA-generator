[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter3",
        "title": "6.3.2 拟牛顿法",
        "content_length": 5356,
        "content": "Title: 6.3.2 拟牛顿法\nContent:\n最大熵模型学习还可以应用牛顿法或拟牛顿法, 参阅附录 B。\n\n对于最大熵模型而言,\n\n$$\nP_{w}(y \\mid x)=\\frac{\\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}{\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)}\n$$\n\n目标函数为\n\n$$\n\\min _{w \\in \\boldsymbol{R}^{n}} f(w)=\\sum_{x} \\tilde{P}(x) \\log \\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)-\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)\n$$\n\n梯度为\n\n$$\ng(w)=\\left(\\frac{\\partial f(w)}{\\partial w_{1}}, \\frac{\\partial f(w)}{\\partial w_{2}}, \\cdots, \\frac{\\partial f(w)}{\\partial w_{n}}\\right)^{\\mathrm{T}}\n$$\n\n其中，\n\n$$\n\\frac{\\partial f(w)}{\\partial w_{i}}=\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f_{i}(x, y)-E_{\\tilde{P}}\\left(f_{i}\\right), \\quad i=1,2, \\cdots, n\n$$\n\n相应的拟牛顿法 BFGS 算法如下。\n\n\\section*{算法 6.2 (最大摘模型学习的 BFGS 算法)}\n输入: 特征函数 $f_{1}, f_{2}, \\cdots, f_{n}$, 经验分布 $\\tilde{P}(x, y)$, 目标函数 $f(w)$, 梯度 $g(w)=\\nabla f(w)$,精度要求 $\\varepsilon$ 。\n\n输出: 最优参数值 $w^{*}$, 最优模型 $P_{w^{*}}(y \\mid x)$ 。\n\n(1) 选定初始点 $w^{(0)}$, 取 $B_{0}$ 为正定对称矩阵, 置 $k=0$;\n\n(2) 计算 $g_{k}=g\\left(w^{(k)}\\right)$, 若 $\\left\\|g_{k}\\right\\|<\\varepsilon$, 则停止计算, 得 $w^{*}=w^{(k)}$; 否则, 转步骤 (3);\n\n(3) 由 $B_{k} p_{k}=-g_{k}$ 求出 $p_{k}$;\n\n(4) 一维搜索: 求 $\\lambda_{k}$ 使得\n\n$$\nf\\left(w^{(k)}+\\lambda_{k} p_{k}\\right)=\\min _{\\lambda \\geqslant 0} f\\left(w^{(k)}+\\lambda p_{k}\\right)\n$$\n\n(5) 置 $w^{(k+1)}=w^{(k)}+\\lambda_{k} p_{k}$;\n\n（6）计算 $g_{k+1}=g\\left(w^{(k+1)}\\right)$, 若 $\\left\\|g_{k+1}\\right\\|<\\varepsilon$, 则停止计算, 得 $w^{*}=w^{(k+1)}$; 否则, 按下式求出 $B_{k+1}$ :\n\n$$\nB_{k+1}=B_{k}+\\frac{y_{k} y_{k}^{\\mathrm{T}}}{y_{k}^{\\mathrm{T}} \\delta_{k}}-\\frac{B_{k} \\delta_{k} \\delta_{k}^{\\mathrm{T}} B_{k}}{\\delta_{k}^{\\mathrm{T}} B_{k} \\delta_{k}}\n$$\n\n其中,\n\n$$\ny_{k}=g_{k+1}-g_{k}, \\quad \\delta_{k}=w^{(k+1)}-w^{(k)}\n$$\n\n(7) 置 $k=k+1$, 转步骤 $(3)$ 。\n\n\\section*{本章概 要}\n\\begin{enumerate}\n  \\item 逻辑斯谛回归模型是由以下条件概率分布表示的分类模型, 可以用于二类或多类分类。\n\\end{enumerate}\n\n$$\n\\begin{gathered}\nP(Y=k \\mid x)=\\frac{\\exp \\left(w_{k} \\cdot x\\right)}{1+\\sum_{k=1}^{K-1} \\exp \\left(w_{k} \\cdot x\\right)}, \\quad k=1,2, \\cdots, K-1 \\\\\nP(Y=K \\mid x)=\\frac{1}{1+\\sum_{k=1}^{K-1} \\exp \\left(w_{k} \\cdot x\\right)}\n\\end{gathered}\n$$\n\n这里, $x$ 为输入特征, $w$ 为特征的权值。\n\n逻辑斯谛回归模型源自逻辑斯谛分布, 其分布函数 $F(x)$ 是 $\\mathrm{S}$ 形函数。逻辑斯谛回归模型是由输入的线性函数表示的输出的对数几率模型。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{1}\n  \\item 最大熵模型是由以下条件概率分布表示的分类模型, 也可以用于二类或多类分类。\n\\end{enumerate}\n\n$$\n\\begin{aligned}\n& P_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right) \\\\\n& Z_{w}(x)=\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n\\end{aligned}\n$$\n\n其中, $Z_{w}(x)$ 是规范化因子, $f_{i}$ 为特征函数, $w_{i}$ 为特征的权值。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{2}\n  \\item 最大熵模型可以由最大熵原理推导得出。最大熵原理是概率模型学习或估计的一个准则。最大嫡原理认为在所有可能的概率模型 (分布) 的集合中, 摘最大的模型是最好的模型。\n\\end{enumerate}\n\n最大嫡原理应用到分类模型的学习中, 有以下约束最优化问题:\n\n$$\n\\begin{array}{ll}\n\\min & -H(P)=\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\log P(y \\mid x) \\\\\n\\text { s.t. } & P\\left(f_{i}\\right)-\\tilde{P}\\left(f_{i}\\right)=0, \\quad i=1,2, \\cdots, n \\\\\n& \\sum_{y} P(y \\mid x)=1\n\\end{array}\n$$\n\n求解此最优化问题的对偶问题得到最大熵模型。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{3}\n  \\item 逻辑斯谛回归模型与最大嫡模型都属于对数线性模型。\n\n  \\item 逻辑斯谛回归模型及最大熵模型学习一般采用极大似然估计或正则化的极大似然估计。逻辑斯谛回归模型及最大熵模型学习可以形式化为无约束最优化问题, 求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法。\n\n\\end{enumerate}\n\n\\section*{继续阅读}\n逻辑斯谛回归的介绍参见文献 [1], 最大嫡模型的介绍参见文献 [2] 和文献 [3]。逻辑斯谛回归模型与朴素贝叶斯模型的关系参见文献 [4], 逻辑斯谛回归模型与 AdaBoost 的关系参见文献 [5], 逻辑斯谛回归模型与核函数的关系参见文献 [6]。\n\n\\section*{习 题}\n6.1 确认逻辑斯谛分布属于指数分布族。\n\n6.2 写出逻辑斯谛回归模型学习的梯度下降算法。\n\n6.3 写出最大嫡模型学习的 DFP 算法 (关于一般的 DFP 算法参见附录 B)。\n\n\\section*{参考文献}\n[1] BERGER A, DELLA PIETRA S D, PIETRA V D. A maximum entropy approach to natural language processing[J]. Computational Linguistics, 1996, 22(1): 39-71.\n\n[2] BERGER A. The improved iterative scaling algorithm: a gentle introduction[R/OL]. http:// \\href{http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/scaling.ps}{www.cs.cmu.edu/afs/cs/user/aberger/www/ps/scaling.ps}.\n\n[3] HASTIE T, TIBSHIRANI R, FRIEDMAN J. The elements of statistical learning: data mining, inference, and prediction[M]. 范明, 柴玉梅, 处红英, 等译. Springer, 2001.\n\n[4] MITCHELL T M. Machine learning[M]. 曾华军, 张银奎, 等译. McGraw-Hill Companies, Inc. 1997.\n\n[5] COLLINS M, SCHAPIRE R E, SINGER Y. Logistic regression, AdaBoost and Bregman distances[J]. Machine Learning, 2002, 48(1-3): 253-285.\n\n[6] CANU S, SMOLA A J. Kernel method and exponential family[J]. Neurocomputing, 2005, 69: $714-720$.\n\n\\section*{第 7 章 支持向量机}\n支持向量机（support vector machines, SVM）是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器, 间隔最大使它有别于感知机; 支持向量机还包括核技巧, 这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化, 可形式化为一个求解凸二次规划 (convex quadratic programming) 的问题, 也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。\n\n支持向量机学习方法包含构建由简至繁的模型: 线性可分支持向量机 (linear support vector machine in linearly separable case) 、线性支持向量机 (linear support vector machine）以及非线性支持向量机（non-linear support vector machine）。简单模型是复杂模型的基础, 也是复杂模型的特殊情况。当训练数据线性可分时, 通过硬间隔最大化 (hard margin maximization), 学习一个线性的分类器, 即线性可分支持向量机, 又称为硬间隔支持向量机; 当训练数据近似线性可分时, 通过软间隔最大化 (soft margin maximization), 也学习一个线性的分类器, 即线性支持向量机, 又称为软间隔支持向量机; 当训练数据线性不可分时,通过使用核技巧 (kernel trick) 及软间隔最大化, 学习非线性支持向量机。\n\n当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，核函数（kernel function）表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。这样的方法称为核技巧。核方法 (kernel method) 是比支持向量机更为一般的机器学习方法。\n\nCortes 与 Vapnik 提出线性支持向量机, Boser、Guyon 与 Vapnik 又引入核技巧, 提出非线性支持向量机。\n\n本章按照上述思路介绍 3 类支持向量机、核函数及一种快速学习算法——序列最小最优化算法 (SMO)。\n\n\\section*{7.1 线性可分支持向量机与硬间隔最大化}"
    },
    {
        "id": "chatcmpl-93yqyGIKLntmNatgin1cixtYNO1u8",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"最大熵模型中的目标函数是如何定义的？\",\n    \"回答1\": \"最大熵模型中的目标函数定义为 $\\text{min}_{w \\text{ 在 } \\boldsymbol{R}^{n}} f(w)=\\text{sum}_{x} \\tilde{P}(x) \\text{log} \\text{sum}_{y} \\text{exp}\\big(\\text{sum}_{i=1}^{n} w_{i} f_{i}(x, y)\\big)-\\text{sum}_{x, y} \\tilde{P}(x, y) \\text{sum}_{i=1}^{n} w_{i} f_{i}(x, y)$。\",\n    \"问题2\": \"拟牛顿法 BFGS 算法中的步骤主要包括哪些内容？\",\n    \"回答2\": \"拟牛顿法 BFGS 算法的步骤主要包括：(1) 选定初始点 $w^{(0)}$, 取 $B_{0}$ 为正定对称矩阵, 置 $k=0$; (2) 计算 $g_{k}=g\\big(w^{(k)}\\big)$, 若 $\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big|g_{k}\\big|< \\text{精度要求} \\right)$, 则停止计算, 得 $w^{*}=w^{(k)}$; 否则, 转步骤 (3); (3) 由 $B_{k} p_{k}=-g_{k}$ 求出 $p_{k}$; (4) 一维搜索: 求 $\\text{lambda}_{k}$ 使得 $f(w^{(k)}+\\text{lambda}_{k} p_{k})=\\text{min}_{lambda \\text{geqslant} 0} f(w^{(k)}+\\text{lambda} p_{k})$; (5) 置 $w^{(k+1)}=w^{(k)}+\\text{lambda}_{k} p_{k}$; （6）计算 $g_{k+1}=g(w^{(k+1)})$, 若 $\\big|g_{k+1}\\big|< \\text{精度要求}$, 则停止计算, 得 $w^{*}=w^{(k+1)}$; 否则, 按下式求出 $B_{k+1}$ : $B_{k+1}=B_{k}+\\frac{y_{k} y_{k}^{\\top}}{y_{k}^{\\top} \\text{delta}_{k}}-\\frac{B_{k} \\text{delta}_{k} \\text{delta}_{k}^{\\top} B_{k}}{\\text{delta}_{k}^{\\top} B_{k} \\text{delta}_{k}}$；其中，$y_{k}=g_{k+1}-g_{k}, \\text{delta}_{k}=w^{(k+1)}-w^{(k)}$；(7) 置 $k=k+1$, 转步骤 $(3)$。\",\n    \"问题3\": \"最大熵模型与逻辑斯谛回归模型有何异同？\",\n    \"回答3\": \"最大熵模型和逻辑斯谛回归模型都是用于分类的模型，但它们的条件概率分布表达形式不同。最大熵模型使用了特征函数和特征权值来表示条件概率分布，而逻辑斯谛回归模型则是通过输入的线性函数表示输出的对数几率模型。因此，它们在建模和表达上存在一定的差异。\",\n    \"问题4\": \"支持向量机（SVM）的学习策略是什么？\",\n    \"回答4\": \"支持向量机（SVM）的学习策略是间隔最大化，即寻找一个定义在特征空间上的间隔最大的线性分类器。这使得支持向量机能够更好地处理分类问题，尤其在处理高维数据和非线性数据时具有优势。\",\n    \"问题5\": \"核函数在支持向量机中起到什么作用？\",\n    \"回答5\": \"核函数在支持向量机中起到将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。这样的方法称为核技巧。\",\n    \"问题6\": \"逻辑斯谛回归模型与最大熵模型学习一般采用哪些方法？\",\n    \"回答6\": \"逻辑斯谛回归模型与最大熵模型学习一般采用极大似然估计或正则化的极大似然估计。这些方法能够有效地估计模型参数，并在实际应用中得到了广泛的应用。\",\n    \"问题7\": \"拟牛顿法 BFGS 算法中的精度要求是如何定义的？\",\n    \"回答7\": \"拟牛顿法 BFGS 算法中的精度要求通常表示为 $\\text{epsilon}$，它是一个用户设定的参数，用于控制算法迭代过程中收敛的精度。当梯度 $g_k$ 的范数小于 $\\text{epsilon}$ 时，算法停止计算并得到最优参数值 $w^* = w^{(k)}$。\",\n    \"问题8\": \"最大熵原理在分类模型学习中起到什么作用？\",\n    \"回答8\": \"最大熵原理是概率模型学习或估计的一个准则。它认为在所有可能的概率模型（分布）的集合中，摘最大的模型是最好的模型。因此，在分类模型学习中，最大熵原理被应用到约束最优化问题中，以求解出满足约束条件下摘最大的概率模型。\",\n    \"问题9\": \"逻辑斯谛回归模型与最大嫡模型都属于哪种类型的模型？\",\n    \"回答9\": \"逻辑斯谛回归模型与最大嫡模型都属于对数线性模型。它们通过对输入进行线性函数变换，并应用对数几率函数来建立输出与输入之间的关系，因此属于对数线性模型。\",\n    \"问题10\": \"支持向量机学习方法包含哪些构建模型？\",\n    \"回答10\": \"支持向量机学习方法包含构建由简至繁的模型，包括线性可分支持向量机、线性支持向量机以及非线性支持向量机。这些不同类型的模型能够更好地适应不同类型和复杂度的数据特征，提高了支持向量机在实际应用中的灵活性和适应性。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710735932,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1949,
            "prompt_tokens": 5926,
            "total_tokens": 7875
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]