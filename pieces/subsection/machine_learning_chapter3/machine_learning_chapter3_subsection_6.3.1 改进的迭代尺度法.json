[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter3",
        "title": "6.3.1 改进的迭代尺度法",
        "content_length": 5293,
        "content": "Title: 6.3.1 改进的迭代尺度法\nContent:\n改进的迭代尺度法 (improved iterative scaling, IIS) 是一种最大嫡模型学习的最优化算法。\n\n已知最大熵模型为\n\n$$\nP_{w}(y \\mid x)=\\frac{1}{Z_{w}(x)} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n$$\n\n其中,\n\n$$\nZ_{w}(x)=\\sum_{y} \\exp \\left(\\sum_{i=1}^{n} w_{i} f_{i}(x, y)\\right)\n$$\n\n对数似然函数为\n\n$$\nL(w)=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} w_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log Z_{w}(x)\n$$\n\n目标是通过极大似然估计学习模型参数, 即求对数似然函数的极大值 $\\hat{w}$ 。\n\nIIS 的想法是: 假设最大熵模型当前的参数向量是 $w=\\left(w_{1}, w_{2}, \\cdots, w_{n}\\right)^{\\mathrm{T}}$, 我们希望找到一个新的参数向量 $w+\\delta=\\left(w_{1}+\\delta_{1}, w_{2}+\\delta_{2}, \\cdots, w_{n}+\\delta_{n}\\right)^{\\mathrm{T}}$, 使得模型的对数似然函数值增大。如果能有这样一种参数向量更新的方法 $\\tau: w \\rightarrow w+\\delta$, 那么就可以重复使用这一方法, 直至找到对数似然函数的最大值。\n\n对于给定的经验分布 $\\tilde{P}(x, y)$, 模型参数从 $w$ 到 $w+\\delta$, 对数似然函数的改变量是\n\n$$\n\\begin{aligned}\nL(w+\\delta)-L(w) & =\\sum_{x, y} \\tilde{P}(x, y) \\log P_{w+\\delta}(y \\mid x)-\\sum_{x, y} \\tilde{P}(x, y) \\log P_{w}(y \\mid x) \\\\\n& =\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\log \\frac{Z_{w+\\delta}(x)}{Z_{w}(x)}\n\\end{aligned}\n$$\n\n利用不等式\n\n$$\n-\\log \\alpha \\geqslant 1-\\alpha, \\quad \\alpha>0\n$$\n\n建立对数似然函数改变量的下界:\n\n$$\n\\begin{aligned}\nL(w+\\delta)-L(w) & \\geqslant \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\frac{Z_{w+\\delta}(x)}{Z_{w}(x)} \\\\\n& =\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y \\mid x) \\exp \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)\n\\end{aligned}\n$$\n\n将右端记为\n\n$$\nA(\\delta \\mid w)=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y \\mid x) \\exp \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)\n$$\n\n于是有\n\n$$\nL(w+\\delta)-L(w) \\geqslant A(\\delta \\mid w)\n$$\n\n即 $A(\\delta \\mid w)$ 是对数似然函数改变量的一个下界。\n\n如果能找到适当的 $\\delta$ 使下界 $A(\\delta \\mid w)$ 提高, 那么对数似然函数也会提高。然而, 函数 $A(\\delta \\mid w)$ 中的 $\\delta$ 是一个向量, 含有多个变量, 不易同时优化。IIS 试图一次只优化其中一个变量 $\\delta_{i}$, 而固定其他变量 $\\delta_{j}, i \\neq j$ 。\n\n为达到这一目的, IIS 进一步降低下界 $A(\\delta \\mid w)$ 。具体地, IIS 引进一个量 $f^{\\#}(x, y)$ :\n\n$$\nf^{\\#}(x, y)=\\sum_{i} f_{i}(x, y)\n$$\n\n因为 $f_{i}$ 是二值函数, 故 $f^{\\#}(x, y)$ 表示所有特征在 $(x, y)$ 出现的次数。这样, $A(\\delta \\mid w)$ 可以改写为\n\n\n\\begin{align*}\nA(\\delta \\mid w)= & \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1- \\\\\n& \\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y \\mid x) \\exp \\left(f^{\\#}(x, y) \\sum_{i=1}^{n} \\frac{\\delta_{i} f_{i}(x, y)}{f \\#(x, y)}\\right) \\tag{6.30}\n\\end{align*}\n\n\n利用指数函数的凸性以及对任意 $i$, 有 $\\frac{f_{i}(x, y)}{f \\#(x, y)} \\geqslant 0$ 且 $\\sum_{i=1}^{n} \\frac{f_{i}(x, y)}{f \\#(x, y)}=1$ 这一事实, 根据 Jensen 不等式, 得到:\n\n$$\n\\exp \\left(\\sum_{i=1}^{n} \\frac{f_{i}(x, y)}{f^{\\#}(x, y)} \\delta_{i} f^{\\#}(x, y)\\right) \\leqslant \\sum_{i=1}^{n} \\frac{f_{i}(x, y)}{f^{\\#}(x, y)} \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right)\n$$\n\n于是式 (6.30) 可改写为\n\n\n\\begin{align*}\nA(\\delta \\mid w) \\geqslant & \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1- \\\\\n& \\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y \\mid x) \\sum_{i=1}^{n}\\left(\\frac{f_{i}(x, y)}{f^{\\#}(x, y)}\\right) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right) \\tag{6.31}\n\\end{align*}\n\n\n记不等式 (6.31) 右端为\n\n$$\nB(\\delta \\mid w)=\\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n} \\delta_{i} f_{i}(x, y)+1-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y \\mid x) \\sum_{i=1}^{n}\\left(\\frac{f_{i}(x, y)}{f^{\\#}(x, y)}\\right) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right)\n$$\n\n于是得到:\n\n$$\nL(w+\\delta)-L(w) \\geqslant B(\\delta \\mid w)\n$$\n\n这里, $B(\\delta \\mid w)$ 是对数似然函数改变量的一个新的 (相对不紧的) 下界。\n\n求 $B(\\delta \\mid w)$ 对 $\\delta_{i}$ 的偏导数:\n\n$\\frac{\\partial B(\\delta \\mid w)}{\\partial \\delta_{i}}=\\sum_{x, y} \\tilde{P}(x, y) f_{i}(x, y)-\\sum_{x} \\tilde{P}(x) \\sum_{y} P_{w}(y \\mid x) f_{i}(x, y) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right)$\n\n在式 (6.32) 里, 除 $\\delta_{i}$ 外不含任何其他变量。令偏导数为 0 得到:\n\n\n\\begin{equation*}\n\\sum_{x, y} \\tilde{P}(x) P_{w}(y \\mid x) f_{i}(x, y) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right)=E_{\\tilde{P}}\\left(f_{i}\\right) \\tag{6.33}\n\\end{equation*}\n\n\n于是, 依次对 $\\delta_{i}$ 求解方程 (6.33) 可以求出 $\\delta$ 。\n\n这就给出了一种求 $w$ 的最优解的迭代算法, 即改进的迭代尺度算法 IIS。\n\n\\section*{算法 6.1 (改进的迭代尺度算法 IIS)}\n输入: 特征函数 $f_{1}, f_{2}, \\cdots, f_{n}$, 经验分布 $\\tilde{P}(X, Y)$, 模型 $P_{w}(y \\mid x)$ 。\n\n输出: 最优参数值 $w_{i}^{*}$, 最优模型 $P_{w^{*}}$ 。\n\n(1) 对所有 $i \\in\\{1,2, \\cdots, n\\}$, 取初值 $w_{i}=0$ 。\n\n(2) 对每一个 $i \\in\\{1,2, \\cdots, n\\}$ :\n\n(a) 令 $\\delta_{i}$ 是方程\n\n$$\n\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) f_{i}(x, y) \\exp \\left(\\delta_{i} f^{\\#}(x, y)\\right)=E_{\\tilde{P}}\\left(f_{i}\\right)\n$$\n\n的解, 这里,\n\n$$\nf^{\\#}(x, y)=\\sum_{i=1}^{n} f_{i}(x, y)\n$$\n\n(b) 更新 $w_{i}$ 值: $w_{i} \\leftarrow w_{i}+\\delta_{i}$ 。\n\n(3) 如果不是所有 $w_{i}$ 都收玫, 重复步骤 (2)。\n\n这一算法关键的一步是步骤 (a), 即求解方程 (6.33) 中的 $\\delta_{i}$ 。如果 $f(x, y)$ 是常数, 即对任何 $x, y$, 有 $f^{\\#}(x, y)=M$, 那么 $\\delta_{i}$ 可以显式地表示成\n\n\n\\begin{equation*}\n\\delta_{i}=\\frac{1}{M} \\log \\frac{E_{\\tilde{P}}\\left(f_{i}\\right)}{E_{P}\\left(f_{i}\\right)} \\tag{6.34}\n\\end{equation*}\n\n\n如果 $f^{\\#}(x, y)$ 不是常数, 那么必须通过数值计算求 $\\delta_{i}$ 。简单有效的方法是牛顿法。以 $g\\left(\\delta_{i}\\right)=0$ 表示方程 (6.33), 牛顿法通过迭代求得 $\\delta_{i}^{*}$, 使得 $g\\left(\\delta_{i}^{*}\\right)=0$ 。迭代公式是\n\n\n\\begin{equation*}\n\\delta_{i}^{(k+1)}=\\delta_{i}^{(k)}-\\frac{g\\left(\\delta_{i}^{(k)}\\right)}{g^{\\prime}\\left(\\delta_{i}^{(k)}\\right)} \\tag{6.35}\n\\end{equation*}\n\n\n只要适当选取初始值 $\\delta_{i}^{(0)}$, 由于 $\\delta_{i}$ 的方程 (6.33) 有单根, 因此牛顿法恒收玫, 而且收玫速度很快。"
    },
    {
        "id": "chatcmpl-93yq2K8LkNd7cc8fUHSuI1uBLmrR6",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"改进的迭代尺度法是用于解决什么类型的最优化问题？\",\n    \"回答1\": \"改进的迭代尺度法是一种用于解决最大熵模型学习的最优化算法。它通过极大似然估计学习模型参数，即求对数似然函数的极大值 $\\boldsymbol{\\text{w}}$。\",\n    \"问题2\": \"在改进的迭代尺度法中，如何更新模型的参数向量 $w$？\",\n    \"回答2\": \"在改进的迭代尺度法中，假设当前的参数向量是 $w=\\big(w_{1}, w_{2}, \\text{...}, w_{n}\\big)^{\\top}$，通过引入一个新的参数向量更新方法 $\\tau: w \\rightarrow w+\\boldsymbol{\\text{δ}}$。其中，$\\boldsymbol{\\text{δ}}=\\big(\\boldsymbol{\\text{δ}}_{1}, \\boldsymbol{\\text{δ}}_{2}, \\text{...}, \\boldsymbol{\\text{δ}}_{n}\\big)^{\\top}$ 是一个新的参数向量，使得模型的对数似然函数值增大。\",\n    \"问题3\": \"在改进的迭代尺度法中，如何建立对数似然函数改变量的下界？\",\n    \"回答3\": \"在改进的迭代尺度法中，利用不等式 $-\\text{log} \\boldsymbol{\\text{α}} \\boldsymbol{\\text{≥}} 1-\\boldsymbol{\\text{α}}, \\boldsymbol{\\text{α}}>0$ 建立对数似然函数改变量的下界。具体地，引入一个新的下界 $A(\\boldsymbol{\\text{δ}} \\boldsymbol{\\text{∣}} w)$ 来表示对数似然函数改变量。\",\n    \"问题4\": \"在改进的迭代尺度法中，为什么需要降低对数似然函数改变量的下界？\",\n    \"回答4\": \"在改进的迭代尺度法中，降低对数似然函数改变量的下界是为了使得求解过程更加简单和高效。通过降低下界 $A(\\boldsymbol{\\text{δ}} \\boldsymbol{\\text{∣}} w)$，可以更容易地优化参数向量 $\\boldsymbol{\\text{δ}}$，从而提高对数似然函数的值。\",\n    \"问题5\": \"在改进的迭代尺度法中，如何求解参数向量 $\\boldsymbol{\\text{δ}}$ 以提高对数似然函数？\",\n    \"回答5\": \"在改进的迭代尺度法中，通过引入一个新的量 $f^{\\flat}(x, y)=\\text{∑}_{i} f_{i}(x, y)$，将对数似然函数改变量的下界 $A(\\boldsymbol{\\text{δ}} \\boldsymbol{\\text{∣}} w)$ 改写为一个新的相对不紧的下界 $B(\\boldsymbol{\\text{δ}} \\boldsymbol{\\text{∣}} w)$。然后通过求解方程 $\\frac{\\tilde{P}(x, y) P_{w}(y \\boldsymbol{∣} x) f_{i}(x, y) \\text{exp}(\\boldsymbol{δ}_{i} f^{\\flat}(x, y))}{\\tilde{P}(x) \\text{∑}_{y} P_{w}(y \\boldsymbol{∣} x) \\text{exp}(\\boldsymbol{δ}_{i} f^{\\flat}(x, y))}=E_{\\tilde{P}}(f_{i})$ 来求解参数向量 $\\boldsymbol{\\text{δ}}$。\",\n    \"问题6\": \"改进的迭代尺度法中使用了哪种方法来求解参数向量 $\\boldsymbol{\\text{δ}}$？\",\n    \"回答6\": \"在改进的迭代尺度法中，如果特征函数 $f(x, y)$ 是常数，则可以使用牛顿法来显式地求解参数向量 $\\boldsymbol{\\text{δ}}$。如果特征函数 $f(x, y)$ 不是常数，则可以通过数值计算和牛顿法来求解参数向量 $\\boldsymbol{\\text{δ}}$。\",\n    \"问题7\": \"算法 6.1 (改进的迭代尺度算法 IIS) 的输入和输出分别是什么？\",\n    \"回答7\": \"算法 6.1 (改进的迭代尺度算法 IIS) 的输入包括特征函数 $f_{1}, f_{2}, \\text{...}, f_{n}$ 和经验分布 $\\tilde{P}(X, Y)$，以及模型 $P_{w}(y \\boldsymbol{∣} x)$。输出则是最优参数值 $w_{i}^{*}$ 和最优模型 $P_{w^{*}}$。\",\n    \"问题8\": \"算法 6.1 (改进的迭代尺度算法 IIS) 中关键的一步是什么？\",\n    \"回",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710735874,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1310,
            "prompt_tokens": 5723,
            "total_tokens": 7033
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]