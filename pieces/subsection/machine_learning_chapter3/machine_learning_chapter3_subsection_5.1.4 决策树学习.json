[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter3",
        "title": "5.1.4 决策树学习",
        "content_length": 1944,
        "content": "Title: 5.1.4 决策树学习\nContent:\n假设给定训练数据集\n\n$$\nD=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}\n$$\n\n其中, $x_{i}=\\left(x_{i}^{(1)}, x_{i}^{(2)}, \\cdots, x_{i}^{(n)}\\right)^{\\mathrm{T}}$ 为输入实例 (特征向量), $n$ 为特征个数, $y_{i} \\in\\{1,2, \\cdots, K\\}$为类标记, $i=1,2, \\cdots, N, N$ 为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型, 使它能够对实例进行正确的分类。\n\n决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树 (即能对训练数据进行正确分类的决策树) 可能有多个, 也可能一个都没有。我们需要的是一个与训练数据矛盾较小的决策树, 同时具有很好的泛化能力。从另一个角度看, 决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_aa878f82ee3a63be2358g-02(1)}\n\\end{center}\n\n(a) 特征空间划分\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_aa878f82ee3a63be2358g-02(2)}\n\\end{center}\n\n(b) 条件概率分布\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_aa878f82ee3a63be2358g-02}\n\\end{center}\n\n(c) 决策树\n\n图 5.2 决策树对应于条件概率分布\n\n个。我们选择的条件概率模型应该不仅对训练数据有很好的拟合, 而且对未知数据有很好的预测。\n\n决策树学习用损失函数表示这一目标。如下所述, 决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。\n\n当损失函数确定以后, 学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是 NP 完全问题, 所以现实中决策树学习算法通常采用启发式方法, 近似求解这一最优化问题。这样得到的决策树是次最优 (sub-optimal) 的。\n\n决策树学习的算法通常是一个递归地选择最优特征, 并根据该特征对训练数据进行分割, 使得对各个子数据集有一个最好的分类的过程。这一过程对应对特征空间的划分, 也对应决策树的构建。首先, 构建根结点, 将所有训练数据都放在根结点。选择一个最优特征, 按照这一特征将训练数据集分割成子集, 使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类, 那么构建叶结点, 并将这些子集分到所对应的叶结点中去; 如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割, 构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上, 即都有了明确的类。这就生成了一棵决策树。\n\n以上方法生成的决策树可能对训练数据有很好的分类能力, 但对未知的测试数据未必有\\\\\n很好的分类能力, 即可能发生过拟合现象。我们需要对已生成的树自下而上进行剪枝, 将树变得更简单, 从而使它具有更好的泛化能力。具体地, 就是去掉过于细分的叶结点, 使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。\n\n如果特征数量很多, 也可以在决策树学习开始的时候对特征进行选择, 只留下对训练数据有足够分类能力的特征。\n\n可以看出, 决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应不同复杂度的概率模型。决策树的生成对应于模型的局部选择, 决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。\n\n决策树学习常用的算法有 ID3、C4.5 与 CART, 下面结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。\n\n\\section*{5.2 特征选择}"
    },
    {
        "id": "chatcmpl-93ylHx6BtNRlzTw9vfCNCeYTFRewD",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"决策树学习的目标是什么？\",\n    \"回答1\": \"决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使其能够对实例进行正确的分类。\",\n    \"问题2\": \"决策树学习本质上是从训练数据集中归纳出什么？\",\n    \"回答2\": \"决策树学习本质上是从训练数据集中归纳出一组分类规则，即与训练数据集不相矛盾的决策树，使得它能够对实例进行正确分类。\",\n    \"问题3\": \"决策树学习的损失函数通常是什么？\",\n    \"回答3\": \"决策树学习的损失函数通常是正则化的极大似然函数，用于表示学习的目标。决策树学习的策略是以损失函数为目标函数的最小化。\",\n    \"问题4\": \"决策树学习算法通常采用何种方法来近似求解最优化问题？\",\n    \"回答4\": \"因为从所有可能的决策树中选取最优决策树是 NP 完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优 (sub-optimal) 的。\",\n    \"问题5\": \"决策树学习算法中特征选择的作用是什么？\",\n    \"回答5\": \"特征选择在决策树学习算法中起着重要作用，它可以在决策树学习开始时对特征进行选择，只留下对训练数据有足够分类能力的特征，从而提高模型的泛化能力。\",\n    \"问题6\": \"决策树的生成过程对应于模型的局部选择还是全局选择？\",\n    \"回答6\": \"决策树的生成对应于模型的局部选择，它是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。\",\n    \"问题7\": \"决策树的剪枝过程对应于模型的局部选择还是全局选择？\",\n    \"回答7\": \"决策树的剪枝过程对应于模型的全局选择。它通过自下而上进行剪枝，将过于细分的叶结点变得更简单，从而使得决策树具有更好的泛化能力。\",\n    \"问题8\": \"决策树学习常用的算法有哪些？\",\n    \"回答8\": \"决策树学习常用的算法有 ID3、C4.5 与 CART。这些算法分别叙述了决策树学习的特征选择、决策树的生成和剪枝过程。\",\n    \"问题9\": \"决策树生成过程中如何进行特征空间划分？\",\n    \"回答9\": \"在决策树生成过程中，首先构建根结点，将所有训练数据都放在根结点。然后选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。\",\n    \"问题10\": \"为什么需要对已生成的决策树进行剪枝？\",\n    \"回答10\": \"已生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据未必有很好的分类能力，即可能发生过拟合现象。因此需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使其具有更好的泛化能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710735579,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1227,
            "prompt_tokens": 4268,
            "total_tokens": 5495
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]