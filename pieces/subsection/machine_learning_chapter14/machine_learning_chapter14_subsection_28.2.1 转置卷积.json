[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter14",
        "title": "28.2.1 转置卷积",
        "content_length": 4164,
        "content": "Title: 28.2.1 转置卷积\nContent:\n\\section*{1. 转置卷积的定义}\n转置卷积 (transposed convolution) 也称为微步卷积 (fractionally strided convolution)或反卷积 (deconvolution) ${ }^{1}$, 在图像生成网络、图像自动编码器等模型中广泛使用。卷积可以用于图像数据尺寸的缩小, 而转置卷积可以用于图像数据尺寸的放大, 又分别称为下采样和上采样 (参见第 24 章)。\n\n卷积运算可以表示为线形变换。假设有核矩阵为以下矩阵 $\\boldsymbol{W}$ 、填充为 0 、步幅为 1 的卷积运算。\n\n\\[\n\\boldsymbol{W}=\\left[\\begin{array}{lll}\nw_{11} & w_{12} & w_{13}  \\tag{28.11}\\\\\nw_{21} & w_{22} & w_{23} \\\\\nw_{31} & w_{32} & w_{33}\n\\end{array}\\right]\n\\]\n\n图 28.3 显示以上卷积运算的过程, 蓝色格点表示输入矩阵, 绿色格点表示输出矩阵, 深色部分表示具体的卷积计算。输入矩阵的大小是 $4 \\times 4$, 输出矩阵的大小是 $2 \\times 2$, 这个卷积进行的是下采样。\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-12}\n\n图 28.3 卷积例 (见文前彩图)\n\n构建矩阵 $\\boldsymbol{C}$ :\n\n$\\left[\\begin{array}{cccccccccccccccc}w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 & 0 & 0 & 0 & 0 \\\\ 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33} & 0 \\\\ 0 & 0 & 0 & 0 & 0 & w_{11} & w_{12} & w_{13} & 0 & w_{21} & w_{22} & w_{23} & 0 & w_{31} & w_{32} & w_{33}\\end{array}\\right]$\n\n考虑基于矩阵 $\\boldsymbol{C}$ 的线性变换, 其输入是输入矩阵展开的向量, 输出是输出矩阵展开的向量。这个线性变换对应神经网络前一层到后一层的信号传递 (正向传播), 而以上卷积运算表示在这个线性变换中。\n\n另一方面, 考虑基于转置矩阵 $C^{\\mathrm{T}}$ 的线性变换。这个线性变换对应神经网络后一层到前一层的信号传递 (反向传播)。事实上, 存在另一个卷积运算, 表示在基于转置矩阵 $\\boldsymbol{C}^{\\mathrm{T}}$ 的线性变换中, 其核矩阵为以下矩阵:\n\\footnotetext{(1) 反卷积是容易引起误解的名称, 因为它不是卷积的逆运算。\n}\n\n\\[\n\\operatorname{rot} 180(\\boldsymbol{W})=\\left[\\begin{array}{lll}\nw_{33} & w_{32} & w_{31}  \\tag{28.12}\\\\\nw_{23} & w_{22} & w_{21} \\\\\nw_{13} & w_{12} & w_{11}\n\\end{array}\\right]\n\\]\n\n称这个卷积为转置卷积。这个转置卷积是核矩阵为 $\\operatorname{rot} 180(\\boldsymbol{W})$ 、填充为 2 、步幅为 1 的卷积运算。这里 rot180 表示矩阵 180 度旋转, 卷积计算时对输入矩阵进行全填充。\n\n图 28.4 显示以上转置卷积运算的过程, 蓝色格点表示输入矩阵, 绿色格点表示输出矩阵, 虚线部分表示填充, 深色部分表示具体的卷积计算。输入矩阵的大小是 $2 \\times 2$, 输出矩阵的大小是 $4 \\times 4$, 转置卷积进行的是上采样。\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-13}\n\n图 28.4 转置卷积例 (见文前彩图)\n\n原始卷积和转置卷积是相互对应、互为反向的运算, 注意不是逆运算。这个关系的直观解释是在卷积神经网络的两层之间，正向和反向的传播（不考虑基于激活函数的非线性变换）都是卷积运算，相互对应，方向相反。\n\n给定任意一个以 $\\boldsymbol{W}$ 为核矩阵的卷积, 可以构建一个以 $\\operatorname{rot} 180(\\boldsymbol{W})$ 为核矩阵的转置卷积。卷积核和转置卷积核之间有 $\\operatorname{rot} 180(\\operatorname{rot} 180(\\boldsymbol{W}))=\\boldsymbol{W}$ 成立, 相应地, 矩阵和转置矩阵之间有 $\\left(\\boldsymbol{C}^{\\mathrm{T}}\\right)^{\\mathrm{T}}=\\boldsymbol{C}$ 成立。\n\n\\section*{2. 转置卷积的大小}\n首先, 计算原始卷积的大小。这里考虑简单的情况。假设输入矩阵是方阵, 卷积核矩阵也是方阵。设 $I$ 是输入矩阵的尺寸, $K$ 是卷积核的尺寸, $P$ 是填充的尺寸, $S$ 是步幅。输出矩阵的尺寸 $O$ 满足\n\n\n\\begin{equation*}\nO=\\frac{I+2 P-K}{S}+1 \\tag{28.13}\n\\end{equation*}\n\n\n这里考虑可以整除的情况。式 (28.13) 可以改为对应的形式:\n\n$$\nI=\\frac{[O+(O-1)(S-1)]+2(K-P-1)-K}{1}+1\n$$\n\n接着, 计算转置卷积的大小。设 $I^{\\prime}$ 是输入矩阵的尺寸, $K^{\\prime}$ 是卷积核的尺寸, $P^{\\prime}$ 是填充的尺寸, $S^{\\prime}$ 是步幅。输出矩阵的尺寸 $O^{\\prime}$ 满足\n\n\n\\begin{equation*}\nO^{\\prime}=\\frac{I^{\\prime}+2 P^{\\prime}-K^{\\prime}}{S^{\\prime}}+1 \\tag{28.14}\n\\end{equation*}\n\n\n这里也考虑可以整除的情况。转置卷积的输出矩阵尺寸 $O^{\\prime}$ 与原始卷积的输入矩阵尺寸 $I$ 相\\\\\n同。因此, 可以推算, 当 $S=1, P=0$ 时, 转置卷积的大小和原始卷积的大小之间有以下关系成立:\n\n$$\n\\begin{gathered}\nI^{\\prime}=O, P^{\\prime}=K-1, K^{\\prime}=K, S^{\\prime}=1 \\\\\nO^{\\prime}=O+K-1\n\\end{gathered}\n$$\n\n图 28.3 的卷积有 $I=4, K=3, S=1, P=0, O=2$ 。图 28.4 的转置卷积有 $I^{\\prime}=2, K^{\\prime}=$ $3, S^{\\prime}=1, P^{\\prime}=2, O^{\\prime}=4$ 。\n\n\\section*{3. 转置卷积的上采样}\n可以通过增大卷积的步幅 $S>1$ 实现下采样, 即将大尺寸的输入矩阵降低为小尺寸的输出矩阵。相反, 也可以通过减小转置卷积的步幅 $S^{\\prime}<1$ 实现上采样, 即将小尺寸的输入矩阵提高为大尺寸的输出矩阵。采用 $S^{\\prime}<1$ 的步幅, 实际是在输入矩阵的相邻两行之间插入适当数量的 0 行向量, 相邻的两列之间插入适当数量的 0 列向量。转置卷积中经常使用这样的处理, 这是被称为微步卷积的原因。\n\n图 28.5 给出一个转置卷积的例子。原始卷积输入矩阵尺寸为 5 , 卷积核尺寸为 3 , 步幅为 2 , 填充尺寸为 0 , 输出矩阵尺寸为 4 , 即 $I=5, K=3, S=2, P=0, O=2$ 。转置卷积实际是在输入矩阵的相邻的两行之间插入一行 0 向量, 相邻的两列之间插入一列 0 向量。转置卷积实际的输入矩阵 (插入 0 向量后) 尺寸为 3 , 卷积核尺寸为 3 , 实际的步幅为 1 , 填充为 2 , 输出矩阵尺寸为 5 , 即 $\\hat{I}^{\\prime}=3, K^{\\prime}=3, \\hat{S}^{\\prime}=1, P^{\\prime}=2, O^{\\prime}=5$ 。\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_2ac75b94a84b35ee81e4g-14}\n\n图 28.5 转置卷积例 (见文前彩图)\n\n当 $S=2, P=0$ 时，转置卷积的大小和原始卷积的大小之间有以下关系成立:\n\n$$\n\\begin{gathered}\n\\hat{I}^{\\prime}=O+(O-1), P^{\\prime}=K-1, K^{\\prime}=K, \\hat{S}^{\\prime}=1 \\\\\nO^{\\prime}=2(O-1)+K\n\\end{gathered}\n$$"
    },
    {
        "id": "chatcmpl-940JaVuU3eDLbbOPP2MNJ2PdDfB5e",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"转置卷积与原始卷积的作用有何区别？\",\n    \"回答1\": \"卷积可以用于图像数据尺寸的缩小，而转置卷积可以用于图像数据尺寸的放大，又分别称为下采样和上采样。因此，转置卷积和原始卷积在图像处理中具有相反的作用。\",\n    \"问题2\": \"如何表示卷积运算的核矩阵 $\\boldsymbol{W}$？\",\n    \"回答2\": \"卷积运算的核矩阵 $\\boldsymbol{W}$ 可以表示为一个矩阵，例如 $\\boldsymbol{W}=\\big[\\begin{smallmatrix}w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33}\\big]$。\",\n    \"问题3\": \"转置卷积的输出矩阵尺寸 $O^{\\text{'}}$ 与原始卷积的输入矩阵尺寸 $I$ 之间有什么关系？\",\n    \"回答3\": \"转置卷积的输出矩阵尺寸 $O^{\\text{'}}$ 与原始卷积的输入矩阵尺寸 $I$ 相同。当 $S=1, P=0$ 时，转置卷积的大小和原始卷积的大小之间有关系 $O^{\\text{'}}=O+K-1$。\",\n    \"问题4\": \"转置卷积是如何实现上采样的？\",\n    \"回答4\": \"转置卷积可以通过减小步幅 $S^{\\text{'}}<1$ 实现上采样，即将小尺寸的输入矩阵提高为大尺寸的输出矩阵。采用 $S^{\\text{'}}<1$ 的步幅实际上是在输入矩阵的相邻两行之间插入适当数量的 0 行向量，相邻的两列之间插入适当数量的 0 列向量。\",\n    \"问题5\": \"转置卷积中经常使用哪种处理方式来实现上采样？\",\n    \"回答5\": \"转置卷积中经常使用在输入矩阵的相邻两行之间插入一行 0 向量，相邻的两列之间插入一列 0 向量的处理方式来实现上采样。这种处理方式也被称为微步卷积。\",\n    \"问题6\": \"转置卷积与原始卷积之间存在怎样的对应关系？\",\n    \"回答6\": \"给定任意一个以 $\\boldsymbol{W}$ 为核矩阵的卷积，可以构建一个以 $\\text{rot} 180(\\boldsymbol{W})$ 为核矩阵的转置卷积。这两者之间存在对应关系 $\\text{rot} 180(\\text{rot} 180(\\boldsymbol{W}))=\\boldsymbol{W}$。\",\n    \"问题7\": \"如何计算原始卷积和转置卷积的输出矩阵尺寸？\",\n    \"回答7\": \"对于简单情况，可以通过公式 $O=\\frac{I+2P-K}{S}+1$ 计算原始卷积的输出矩阵尺寸；而对于转置卷积，可以通过公式 $O^{\\text{'}}=\\frac{I^{\\text{'}}+2P^{\\text{'}}-K^{\\text{'}}}{S^{\\text{'}}}+1$ 计算输出矩阵尺寸。\",\n    \"问题8\": \"转置卷积和原始卷积在神经网络中分别承担什么作用？\",\n    \"回答8\": \"原始卷积用于图像数据尺寸的缩小，而转置卷积则用于图像数据尺寸的放大，分别对应下采样和上采样。它们在神经网络中分别用于不同尺寸数据的处理和特征提取。\",\n    \"问题9\": \"转置卷积和原始卷积之间存在怎样的关系？\",\n    \"回答9\": \"转置卷积和原始卷积是相互对应、互为反向的运算，但并不是逆运算。它们在神经网络中分别对应正向传播和反向传播，相互对应且方向相反。\",\n    \"问题10\": \"如何表示转置矩阵 $\\boldsymbol{C}^{\\text{T}}$ 的线性变换？\",\n    \"回答10\": \"转置矩阵 $\\boldsymbol{C}^{\\text{T}}$ 的线性变换可以表示为基于 $\\boldsymbol{C}^{\\text{T}}$ 的线性变换，其输入是输入矩阵展开的向量，输出是输出矩阵展开的向量。这个线性变换对应神经网络后一层到前一层的信号传递（反向传播）。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710741550,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1408,
            "prompt_tokens": 5629,
            "total_tokens": 7037
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]