[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter14",
        "title": "28.1.3 理论分析",
        "content_length": 5635,
        "content": "Title: 28.1.3 理论分析\nContent:\n不考虑网络参数, 将 GAN 学习的极小极大问题写成\n\n\n\\begin{equation*}\n\\min _{G} \\max _{D} L(G, D)=\\min _{G} \\max _{D}\\left\\{E_{\\boldsymbol{x} \\sim P_{\\text {data }(\\boldsymbol{x})}}[\\log D(\\boldsymbol{x})]+E_{\\boldsymbol{z} \\sim P_{\\text {seed }}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z}))]\\}\\right. \\tag{28.4}\n\\end{equation*}\n\n\n定理 28.1 当生成网络固定为 $\\bar{G}$ 时, 问题 (28.4) 变成以下最大化问题:\n\n$$\n\\max _{D} L(\\bar{G}, D)=\\max _{D}\\left\\{E_{\\boldsymbol{x} \\sim P_{\\text {data }(\\boldsymbol{x})}}[\\log D(\\boldsymbol{x})]+E_{\\boldsymbol{z} \\sim P_{\\text {seed }}(\\boldsymbol{z})}[\\log (1-D(\\bar{G}(\\boldsymbol{z}))]\\}\\right.\n$$\n\n该最大化问题的解一判别网络 $D_{G}^{*}$ 满足以下关系:\n\n\n\\begin{equation*}\nD_{G}^{*}(\\boldsymbol{x})=\\frac{P_{\\text {data }}(\\boldsymbol{x})}{P_{\\text {data }}(\\boldsymbol{x})+P_{\\text {gen }}(\\boldsymbol{x})} \\tag{28.5}\n\\end{equation*}\n\n\n证明\n\n\n\\begin{align*}\nL(\\bar{G}, D) & =\\int_{\\boldsymbol{x}} P_{\\text {data }}(\\boldsymbol{x}) \\log D(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}+\\int_{\\boldsymbol{z}} P_{\\text {seed }}(\\boldsymbol{z}) \\log (1-D(\\bar{G}(\\boldsymbol{z}))) \\mathrm{d} \\boldsymbol{z} \\\\\n& =\\int_{\\boldsymbol{x}} P_{\\text {data }}(\\boldsymbol{x}) \\log D(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}+\\int_{\\boldsymbol{x}} P_{\\text {gen }}(\\boldsymbol{x}) \\log (1-D(\\boldsymbol{x})) \\mathrm{d} \\boldsymbol{x} \\tag{28.6}\n\\end{align*}\n\n\n式 (28.6) 达到最大值的判别网络表示为 $D_{G}^{*}$, 则有式 (28.5) 成立。这是因为, 针对任意的 $(a, b) \\in \\mathcal{R}^{2} \\backslash(0,0)$, 函数 $f(x)=a \\log x+b \\log (1-x), x \\in(0,1)$, 当 $x=\\frac{a}{a+b}$ 时取最大值。函数 $D(\\boldsymbol{x})$ 在 $\\operatorname{supp}\\left(P_{\\text {data }}(\\boldsymbol{x})\\right) \\cup \\operatorname{supp}\\left(P_{\\text {gen }}(\\boldsymbol{x})\\right)$ 之外无须定义。\n\n定理 28.2 当判别网络固定为 $D_{G}^{*}$ 时, 问题 (28.4) 变成以下最小化问题:\n\n$$\n\\min _{G} L\\left(G, D_{G}^{*}\\right)=\\min _{G}\\left\\{E_{\\boldsymbol{x} \\sim P_{\\text {data }(\\boldsymbol{x})}}\\left[\\log D_{G}^{*}(\\boldsymbol{x})\\right]+E_{\\boldsymbol{z} \\sim P_{\\text {seed }}(\\boldsymbol{z})}\\left[\\log \\left(1-D_{G}^{*}(G(\\boldsymbol{z}))\\right]\\right\\}\\right.\n$$\n\n该最小化问题的解——生成网络 $G^{*}$ 满足以下关系:\n\n\n\\begin{equation*}\nP_{\\text {gen }}^{*}(\\boldsymbol{x})=P_{\\text {data }}(\\boldsymbol{x}) \\tag{28.7}\n\\end{equation*}\n\n\n最小值是 $-2 \\log 2$ 。\n\n\\section*{证明}\n\n\\begin{align*}\nL\\left(G, D_{G}^{*}\\right) & =\\int_{\\boldsymbol{x}} P_{\\text {data }}(\\boldsymbol{x}) \\log D_{G}^{*}(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}+\\int_{\\boldsymbol{z}} P_{\\text {seed }}(\\boldsymbol{z}) \\log \\left(1-D_{G}^{*}(G(\\boldsymbol{z}))\\right) \\mathrm{d} \\boldsymbol{z} \\\\\n& =\\int_{\\boldsymbol{x}} P_{\\text {data }}(\\boldsymbol{x}) \\log D_{G}^{*}(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}+\\int_{\\boldsymbol{x}} P_{\\text {gen }}(\\boldsymbol{x}) \\log \\left(1-D_{G}^{*}(\\boldsymbol{x})\\right) \\mathrm{d} \\boldsymbol{x} \\\\\n& =\\int_{\\boldsymbol{x}} P_{\\text {data }}(\\boldsymbol{x}) \\log \\frac{P_{\\text {data }}(\\boldsymbol{x})}{P_{\\text {data }}(\\boldsymbol{x})+P_{\\text {gen }}(\\boldsymbol{x})} \\mathrm{d} \\boldsymbol{x}+\\int_{\\boldsymbol{x}} P_{\\text {gen }}(\\boldsymbol{x}) \\log \\left(\\frac{P_{\\text {gen }}(\\boldsymbol{x})}{P_{\\text {data }}(\\boldsymbol{x})+P_{\\text {gen }}(\\boldsymbol{x})}\\right) \\mathrm{d} \\boldsymbol{x} \\\\\n& =\\mathrm{KL}\\left(P_{\\text {data }}(\\boldsymbol{x}) \\| \\frac{P_{\\text {data }}(\\boldsymbol{x})+P_{\\text {gen }}(\\boldsymbol{x})}{2}\\right)+\\mathrm{KL}\\left(P_{\\text {gen }}(\\boldsymbol{x}) \\| \\frac{P_{\\text {data }}(\\boldsymbol{x})+P_{\\text {gen }}(\\boldsymbol{x})}{2}\\right)-2 \\log 2 \\\\\n& =\\log \\mathrm{JS}\\left(P_{\\text {data }}(\\boldsymbol{x}) \\| P_{\\text {gen }}(\\boldsymbol{x})\\right)-2 \\log 2 \\tag{28.8}\n\\end{align*}\n\n\n$\\mathrm{JS}(P \\| Q)$ 是两个概率分布 $P$ 和 $Q$ 之间的 Jessen-Shannon 散度, 当且仅当两个概率分布相同时, 取最小值 0 。所以, 式 (28.8) 当且仅当 $P_{\\mathrm{gen}}(\\boldsymbol{x})=P_{\\mathrm{data}}(\\boldsymbol{x})$ 时达到最小值, 且最小值为 $-2 \\log 2$ 。达到最小值的生成分布表示为 $P_{\\mathrm{gen}}^{*}(\\boldsymbol{x})$, 即有式 (28.7) 成立。\n\n理论上的最优解（即纳什均衡状态）满足：\n\n\n\\begin{gather*}\nP_{\\text {gen }}^{*}(\\boldsymbol{x})=P_{\\text {data }}(\\boldsymbol{x})  \\tag{28.9}\\\\\nD^{*}(\\boldsymbol{x})=\\frac{1}{2} \\tag{28.10}\n\\end{gather*}\n\n\n也就是生成网络可以以与训练数据相同的分布生成数据, 判别网络无法辨别数据是来自训练数据还是生成的数据。以上定理只是表示理论上最优解存在。实际上, 生成网络和判别网络需要用参数 $\\boldsymbol{\\theta}$ 和 $\\boldsymbol{\\varphi}$ 表示, 算法 28.1 不能保证求得最优解。\n\n图 28.2 示意 GAN 的学习过程。图中下面横线表示生成网络输入 $\\boldsymbol{z}$ 的分布, 这里假设是均匀分布。中间横线表示生成网络输出 $\\boldsymbol{x}$ 的分布。两条横线之间的有向实线表示生成网络的映射 $\\boldsymbol{x}=G(\\boldsymbol{z} ; \\boldsymbol{\\theta})$ 。上面黑色点线表示真实数据分布 $P_{\\mathrm{data}}(\\boldsymbol{x})$, 绿色实线表示生成数据分布 $P_{\\mathrm{gen}}(\\boldsymbol{x})$, 蓝色点线表示判别网络判别分布 $D(\\boldsymbol{x})$ 。训练初始, 生成数据分布和真实数据分布相差较远, 判别网络的判别概率也不准确 (图 28.2(a))。生成网络固定判别网络训练后, 其判别概率趋于 $D_{G}^{*}(\\boldsymbol{x})=\\frac{P_{\\mathrm{data}}(\\boldsymbol{x})}{P_{\\mathrm{data}}(\\boldsymbol{x})+P_{\\mathrm{gen}}(\\boldsymbol{x})}($ 图 28.2(b))。判别网络固定生成网络训练后, 其生成数据分布和真实数据分布趋于接近 (图 28.2(c))。训练收玫后, 生成网络达到最优 $P_{\\text {gen }}^{*}(\\boldsymbol{x})=P_{\\text {data }}(\\boldsymbol{x})$, 判别网络也达到最优 $D^{*}(\\boldsymbol{x})=\\frac{1}{2}($ 图 28.2(d))。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11}\n\\end{center}\n\n(a)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11(1)}\n\\end{center}\n\n(b)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11(3)}\n\\end{center}\n\n(c)\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_2ac75b94a84b35ee81e4g-11(2)}\n\\end{center}\n\n(d)\n\n图 28.2 GAN 的学习过程 (见文前彩图)\n\nGAN 的模型训练并不容易, 需要一定的技巧。有很多改进的模型被提出, 包括 W-GAN (Wasserstein GAN)。\n\n\\section*{28.2 图像生成中的应用}\n可以使用 GAN 技术从图像数据中学习生成网络, 用于图像数据的自动生成。比如, 训练数据是人脸图片, 可以学习 GAN, 自动生成 “人脸” 的图片。本节介绍常用的用于图像生成的 GAN 模型 DCGAN, 先讲解 DCGAN 使用的转置卷积。"
    },
    {
        "id": "chatcmpl-940IJypuSau5MvQsvyAQl8hbzCVXD",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"极小极大问题与生成对抗网络（GAN）的学习有什么关系？\",\n    \"回答1\": \"极小极大问题是用来描述 GAN 学习过程的数学形式。通过极小极大问题，GAN 试图找到生成网络和判别网络之间的最优博弈策略，使得生成网络能够生成逼真的数据样本，而判别网络能够准确地区分真实数据和生成数据。\",\n    \"问题2\": \"在定理 28.1 中，当生成网络固定为 $\\bar{G}$ 时，问题 (28.4) 变成了什么类型的问题？\",\n    \"回答2\": \"在定理 28.1 中，当生成网络固定为 $\\bar{G}$ 时，问题 (28.4) 变成了最大化问题，即最大化判别网络 $D$ 的损失函数 $L(\\bar{G}, D)$。\",\n    \"问题3\": \"定理 28.1 中的判别网络 $D_{G}^{*}$ 满足怎样的关系？\",\n    \"回答3\": \"定理 28.1 中的判别网络 $D_{G}^{*}$ 满足关系 $D_{G}^{*}(\\boldsymbol{x})=\\frac{P_{\\text {data }}(\\boldsymbol{x})}{P_{\\text {data }}(\\boldsymbol{x})+P_{\\text {gen }}(\\boldsymbol{x})}$。\",\n    \"问题4\": \"在定理 28.2 中，当判别网络固定为 $D_{G}^{*}$ 时，问题 (28.4) 变成了什么类型的问题？\",\n    \"回答4\": \"在定理 28.2 中，当判别网络固定为 $D_{G}^{*}$ 时，问题 (28.4) 变成了最小化问题，即最小化生成网络 $G$ 的损失函数 $L\\big(G, D_{G}^{*}\\big)$。\",\n    \"问题5\": \"定理 28.2 中的生成网络 $G^{*}$ 满足怎样的关系？\",\n    \"回答5\": \"定理 28.2 中的生成网络 $G^{*}$ 满足关系 $P_{\\text {gen }}^{*}(\\boldsymbol{x})=P_{\\text {data }}(\\boldsymbol{x})$。\",\n    \"问题6\": \"在证明中提到的 $\\text{JS}(P\\big|\\big|Q)$ 是什么意思？\",\n    \"回答6\": \"$\\text{JS}(P\\big|\\big|Q)$ 是两个概率分布 $P$ 和 $Q$ 之间的 Jessen-Shannon 散度，用于衡量两个概率分布之间的相似度。当且仅当两个概率分布相同时，$\\text{JS}(P\\big|\\big|Q)$ 取最小值 0。\",\n    \"问题7\": \"证明中提到的 $\\text{KL}(P\\big|\\big|Q)$ 是什么意思？\",\n    \"回答7\": \"$\\text{KL}(P\\big|\\big|Q)$ 是概率分布 $P$ 相对于概率分布 $Q$ 的 Kullback-Leibler 散度，用于衡量两个概率分布之间的差异。它表示在使用概率分布 $Q$ 来拟合概率分布 $P$ 时产生的信息损失。\",\n    \"问题8\": \"在图 28.2 中展示了 GAN 的学习过程，它是如何描述 GAN 的训练过程的？\",\n    \"回答8\": \"图 28.2 展示了 GAN 的学习过程。从初始训练到训练收敛，图中描述了生成数据分布和真实数据分布逐渐接近、判别网络判别概率趋于 $\\frac{1}{2}$ 的过程。这反映了 GAN 在训练过程中逐步提升生成数据质量和判别准确度的过程。\",\n    \"问题9\": \"GAN 的模型训练有何困难？\",\n    \"回答9\": \"GAN 的模型训练并不容易，需要一定的技巧。由于 GAN 学习涉及到生成网络和判别网络之间的博弈过程，存在训练不稳定、模式崩溃等问题。因此，需要通过改进模型或算法来解决这些困难。\",\n    \"问题10\": \"GAN 技术在图像生成中有哪些应用？\",\n    \"回答10\": \"GAN 技术可以应用于图像数据中学习生成网络，用于图像数据的自动生成。例如，可以使用 GAN 学习人脸图片数据，并生成逼真的“人脸”图片。此外，在图像合成、风格迁移等领域也有广泛应用。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710741471,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1312,
            "prompt_tokens": 5362,
            "total_tokens": 6674
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]