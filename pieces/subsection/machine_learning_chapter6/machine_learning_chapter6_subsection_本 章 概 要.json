[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter6",
        "title": "本 章 概 要",
        "content_length": 3875,
        "content": "Title: 本 章 概 要\nContent:\n\\begin{enumerate}\n  \\item 隐马尔可夫模型是关于时序的概率模型, 描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列, 再由各个状态随机生成一个观测从而产生观测序列的过程。\n\\end{enumerate}\n\n隐马尔可夫模型由初始状态概率向量 $\\pi$ 、状态转移概率矩阵 $A$ 和观测概率矩阵 $B$ 决定。因此, 隐马尔可夫模型可以写成 $\\lambda=(A, B, \\pi)$ 。\n\n隐马尔可夫模型是一个生成模型, 表示状态序列和观测序列的联合分布, 但是状态序列是隐藏的, 不可观测的。\n\n隐马尔可夫模型可以用于标注, 这时状态对应着标记。标注问题是指给定观测序列预测其对应的标记序列。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{1}\n  \\item 概率计算问题。给定模型 $\\lambda=(A, B, \\pi)$ 和观测序列 $O=\\left(o_{1}, o_{2}, \\cdots, o_{T}\\right)$, 计算在模型 $\\lambda$ 下观测序列 $O$ 出现的概率 $P(O \\mid \\lambda)$ 。前向-后向算法通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。\n\n  \\item 学习问题。已知观测序列 $O=\\left(o_{1}, o_{2}, \\cdots, o_{T}\\right)$, 估计模型 $\\lambda=(A, B, \\pi)$ 参数, 使得在该模型下观测序列概率 $P(O \\mid \\lambda)$ 最大。即用极大似然估计的方法估计参数。Baum-Welch 算法, 也就是 EM 算法可以高效地对隐马尔可夫模型进行训练。它是一种无监督学习算法。\n\n  \\item 预测问题。已知模型 $\\lambda=(A, B, \\pi)$ 和观测序列 $O=\\left(o_{1}, o_{2}, \\cdots, o_{T}\\right)$, 求对给定观测序列条件概率 $P(I \\mid O)$ 最大的状态序列 $I=\\left(i_{1}, i_{2}, \\cdots, i_{T}\\right)$ 。维特比算法应用动态规划高效地求解最优路径, 即概率最大的状态序列。\n\n\\end{enumerate}\n\n\\section*{继续阅 读}\n隐马尔可夫模型的介绍可参见文献 [1] 和文献 [2], 特别地, 文献 [1] 是经典的介绍性论文。关于 Baum-Welch 算法可参见文献 [3] 和文献 [4]。可以认为概率上下文无关文法 (probabilistic context-free grammar) 是隐马尔可夫模型的一种推广, 隐马尔可夫模型的不可观测数据是状态序列, 而概率上下文无关文法的不可观测数据是上下文无关文法树 ${ }^{[5]}$ 。动态贝叶斯网络 (dynamic Bayesian network) 是定义在时序数据上的贝叶斯网络, 它包含隐马尔可夫模型, 是一种特例 $[6]$ 。\n\n\\section*{习 题}\n10.1 给定盒子和球组成的隐马尔可夫模型 $\\lambda=(A, B, \\pi)$, 其中,\n\n$$\nA=\\left[\\begin{array}{lll}\n0.5 & 0.2 & 0.3 \\\\\n0.3 & 0.5 & 0.2 \\\\\n0.2 & 0.3 & 0.5\n\\end{array}\\right], \\quad B=\\left[\\begin{array}{cc}\n0.5 & 0.5 \\\\\n0.4 & 0.6 \\\\\n0.7 & 0.3\n\\end{array}\\right], \\quad \\pi=(0.2,0.4,0.4)^{\\mathrm{T}}\n$$\n\n设 $T=4, O=$ (红, 白, 红, 白), 试用后向算法计算 $P(O \\mid \\lambda)$ 。\n\n10.2 考虑盒子和球组成的隐马尔可夫模型 $\\lambda=(A, B, \\pi)$, 其中,\n\n$$\nA=\\left[\\begin{array}{lll}\n0.5 & 0.1 & 0.4 \\\\\n0.3 & 0.5 & 0.2 \\\\\n0.2 & 0.2 & 0.6\n\\end{array}\\right], \\quad B=\\left[\\begin{array}{ll}\n0.5 & 0.5 \\\\\n0.4 & 0.6 \\\\\n0.7 & 0.3\n\\end{array}\\right], \\quad \\pi=(0.2,0.3,0.5)^{\\mathrm{T}}\n$$\n\n设 $T=8, O=$ (红, 白, 红, 红, 白, 红, 白, 白), 用前向-后向概率计算 $P\\left(i_{4}=q_{3} \\mid O, \\lambda\\right)$ 。\n\n10.3 在习题 10.1 中, 试用维特比算法求最优路径 $I^{*}=\\left(i_{1}^{*}, i_{2}^{*}, i_{3}^{*}, i_{4}^{*}\\right)$ 。\n\n10.4 试用前向概率和后向概率推导\n\n$$\nP(O \\mid \\lambda)=\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad t=1,2, \\cdots, T-1\n$$\n\n10.5 比较维特比算法中变量 $\\delta$ 的计算和前向算法中变量 $\\alpha$ 的计算的主要区别。\n\n\\section*{参考文献}\n[1] RABINER L, JUANG B. An introduction to hidden Markov Models[J]. IEEE ASSP Magazine, 1986, 3(1): 4-16.\n\n[2] RABINER L. A tutorial on hidden Markov models and selected applications in speech recognition[J]. Proceedings of IEEE, 1989, 77(2): 257-286.\n\n[3] BAUM L, et al. A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains[J]. Annals of Mathematical Statistics, 1970, 41: 164-171.\n\n[4] BILMES J A. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models[Z/OL]. \\href{http://ssli.ee.washington.edu/}{http://ssli.ee.washington.edu/} bilmes/ mypubs/bilmes1997-em.pdf.\n\n[5] LARI K, YOUNG S J. Applications of stochastic context-free grammars using the InsideOutside algorithm[J]. Computer Speech \\& Language, 1991, 5(3): 237-257.\n\n[6] GHAHRAmANI Z. Learning dynamic Bayesian networks[J]. Lecture Notes in Computer Science, 1997, 1387: 168-197.\n\n\\section*{第 11 章 条件随机场}\n条件随机场 (conditional random field, CRF) 是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型, 其特点是假设输出随机变量构成马尔可夫随机场。条件随机场可以用于不同的预测问题, 本书仅论及它在标注问题的应用。因此主要讲述线性链 (linear chain) 条件随机场, 这时, 问题变成了由输入序列对输出序列预测的判别模型, 形式为对数线性模型, 其学习方法通常是极大似然估计或正则化的极大似然估计。线性链条件随机场应用于标注问题是由 Lafferty 等人于 2001 年提出的。\n\n本章首先介绍概率无向图模型, 然后叙述条件随机场的定义和各种表示方法, 最后介绍条件随机场的 3 个基本问题: 概率计算问题、学习问题和预测问题。\n\n\\section*{11.1 概率无向图模型}\n概率无向图模型 (probabilistic undirected graphical model) 又称为马尔可夫随机场 （Markov random field）, 是一个可以由无向图表示的联合概率分布。本节首先叙述概率无向图模型的定义，然后介绍概率无向图模型的因子分解。"
    },
    {
        "id": "chatcmpl-9472GUJDutHQyZLWdnMuilVxElJ4s",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"隐马尔可夫模型与传统概率模型有何不同？\",\n    \"回答1\": \"隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列，再由各个状态随机生成一个观测从而产生观测序列的过程。与传统概率模型不同的是，隐马尔可夫模型包含了隐藏状态序列和观测序列之间的联合分布，其中状态序列是不可观测的。\",\n    \"问题2\": \"隐马尔可夫模型的参数由哪些部分决定？\",\n    \"回答2\": \"隐马尔可夫模型的参数由初始状态概率向量 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}\\boldsymbol{A}$ 和观测概率矩阵 $\\boldsymbol{B}$ 决定。因此，隐马尔可夫模型可以写成 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}=\\big(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{\\boldsymbol{pi}}\\big)$。\",\n    \"问题3\": \"隐马尔可夫模型可以用于解决哪些问题？\",\n    \"回答3\": \"隐马尔可夫模型可以用于标注问题，其中状态对应着标记。标注问题是指给定观测序列预测其对应的标记序列。此外，隐马尔可夫模型还可以用于概率计算问题和预测问题。\",\n    \"问题4\": \"前向-后向算法在隐马尔可夫模型中的作用是什么？\",\n    \"回答4\": \"前向-后向算法通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。它能够有效地计算在给定模型下观测序列出现的概率 $P(O \\text{ 在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } \\text{在 } T)$。\",\n    \"问题5\": \"Baum-Welch 算法的作用是什么？\",\n    \"回答5\": \"Baum-Welch 算法（也就是 EM 算法）可以高效地对隐马尔可夫模型进行训练，用于估计模型参数。它是一种无监督学习算法，通过极大似然估计的方法估计参数，使得在该模型下观测序列概率 $P(O \\text{ 在 } O)$ 最大。\",\n    \"问题6\": \"维特比算法在隐马尔可夫模型中的作用是什么？\",\n    \"回答6\": \"维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列。它能够根据给定模型和观测序列，求得对给定观测序列条件概率 $P(I \\text{ 在 } O)$ 最大的状态序列 $I=\\big(i_{1}, i_{2}, i_{3}, i_{4}\\big)$。\",\n    \"问题7\": \"条件随机场与隐马尔可夫模型有何异同？\",\n    \"回答7\": \"条件随机场是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫随机场。与隐马尔可夫模型相比，条件随机场更加灵活，适用于不同的预测问题，并且可以用于线性链条件随机场等多种形式。\",\n    \"问题8\": \"概率无向图模型与条件随机场有何联系？\",\n    \"回答8\": \"概率无向图模型（也称为马尔可夫随机场）是一个可以由无向图表示的联合概率分布。条件随机场可以看作是一种特殊形式的概率无向图模型，其特点是假设输出随机变量构成马尔可夫随机场。因此，条件随机场属于概率无向图模型的一种。\",\n    \"问题9\": \"线性链条件随机场主要应用于解决什么类型的问题？\",\n    \"回答9\": \"线性链条件随机场主要应用于由输入序列对输出序列预测的判别模型。它适用于标注问题，其中问题变成了由输入序列对输出序列预测的判别模型，形式为对数线性模型。\",\n    \"问题10\": \"维特比算法和前向算法中变量计算的主要区别是什么？\",\n    \"回答10\": \"维特比算法中变量 $\\boldsymbol{\\beta}$ 的计算和前向算法中变量 $\\boldsymbol{\\beta}$ 的计算主要区别在于维特比算法通过动态规划求解最优路径，即概率最大的状态序列；而前向算法主要用于计算给定模型下观测序列出现的概率 $P(O \\text{ 在 } O)$。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710767384,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1659,
            "prompt_tokens": 4965,
            "total_tokens": 6624
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]