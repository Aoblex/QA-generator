[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter6",
        "title": "11.4.1 改进的迭代尺度法",
        "content_length": 5919,
        "content": "Title: 11.4.1 改进的迭代尺度法\nContent:\n已知训练数据集, 由此可知经验概率分布 $\\tilde{P}(X, Y)$ 。可以通过极大化训练数据的对数似然函数来求模型参数。\n\n训练数据的对数似然函数为\n\n$$\nL(w)=L_{\\tilde{P}}\\left(P_{w}\\right)=\\log \\prod_{x, y} P_{w}(y \\mid x)^{\\tilde{P}(x, y)}=\\sum_{x, y} \\tilde{P}(x, y) \\log P_{w}(y \\mid x)\n$$\n\n当 $P_{w}$ 是一个由式 (11.15) 和式 (11.16) 给出的条件随机场模型时, 对数似然函数为\n\n$$\n\\begin{aligned}\nL(w) & =\\sum_{x, y} \\tilde{P}(x, y) \\log P_{w}(y \\mid x) \\\\\n& =\\sum_{x, y}\\left[\\tilde{P}(x, y) \\sum_{k=1}^{K} w_{k} f_{k}(y, x)-\\tilde{P}(x, y) \\log Z_{w}(x)\\right] \\\\\n& =\\sum_{j=1}^{N} \\sum_{k=1}^{K} w_{k} f_{k}\\left(y_{j}, x_{j}\\right)-\\sum_{j=1}^{N} \\log Z_{w}\\left(x_{j}\\right)\n\\end{aligned}\n$$\n\n改进的迭代尺度法通过迭代的方法不断优化对数似然函数改变量的下界, 达到极大化对数似然函数的目的。假设模型的当前参数向量为 $w=\\left(w_{1}, w_{2}, \\cdots, w_{K}\\right)^{\\mathrm{T}}$, 向量的增量为 $\\delta=\\left(\\delta_{1}, \\delta_{2}, \\cdots, \\delta_{K}\\right)^{\\mathrm{T}}$, 更新参数向量为 $w+\\delta=\\left(w_{1}+\\delta_{1}, w_{2}+\\delta_{2}, \\cdots, w_{K}+\\delta_{K}\\right)^{\\mathrm{T}}$ 。在每步迭代过程中, 改进的迭代尺度法通过依次求解式 (11.36) 和式 (11.37), 得到 $\\delta=\\left(\\delta_{1}, \\delta_{2}, \\cdots, \\delta_{K}\\right)^{\\mathrm{T}}$ 。推导可参考本书 6.3 .1 节。\n\n关于转移特征 $t_{k}$ 的更新方程为\n\n\n\\begin{align*}\nE_{\\tilde{P}}\\left[t_{k}\\right]= & \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n+1} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\\\\n= & \\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n+1} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\exp \\left(\\delta_{k} T(x, y)\\right), \\\\\n& k=1,2, \\cdots, K_{1} \\tag{11.36}\n\\end{align*}\n\n\n关于状态特征 $s_{l}$ 的更新方程为\n\n\n\\begin{align*}\nE_{\\tilde{P}}\\left[s_{l}\\right]= & \\sum_{x, y} \\tilde{P}(x, y) \\sum_{i=1}^{n+1} s_{l}\\left(y_{i}, x, i\\right) \\\\\n= & \\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n} s_{l}\\left(y_{i}, x, i\\right) \\exp \\left(\\delta_{K_{1}+l} T(x, y)\\right), \\\\\n& \\quad l=1,2, \\cdots, K_{2} \\tag{11.37}\n\\end{align*}\n\n\n这里, $T(x, y)$ 是在数据 $(x, y)$ 中出现的所有特征数的总和:\n\n\n\\begin{equation*}\nT(x, y)=\\sum_{k} f_{k}(y, x)=\\sum_{k=1}^{K} \\sum_{i=1}^{n+1} f_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\tag{11.38}\n\\end{equation*}\n\n\n\\section*{算法 11.1 (条件随机场模型学习的改进的迭代尺度法)}\n输入: 特征函数 $t_{1}, t_{2}, \\cdots, t_{K_{1}}, s_{1}, s_{2}, \\cdots, s_{K_{2}}$, 经验分布 $\\tilde{P}(x, y)$ 。\n\n输出: 参数估计值 $\\hat{w}$, 模型 $P_{\\hat{w}}$ 。\n\n(1) 对所有 $k \\in\\{1,2, \\cdots, K\\}$, 取初值 $w_{k}=0$ 。\n\n(2) 对每一 $k \\in\\{1,2, \\cdots, K\\}$ :\n\n（a）当 $k=1,2, \\cdots, K_{1}$ 时, 令 $\\delta_{k}$ 是方程\n\n$$\n\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n+1} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\exp \\left(\\delta_{k} T(x, y)\\right)=E_{\\tilde{P}}\\left[t_{k}\\right]\n$$\n\n的解;\n\n当 $k=K_{1}+l, l=1,2, \\cdots, K_{2}$ 时, 令 $\\delta_{K_{1}+l}$ 是方程\n\n$$\n\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n} s_{l}\\left(y_{i}, x, i\\right) \\exp \\left(\\delta_{K_{1}+l} T(x, y)\\right)=E_{\\tilde{P}}\\left[s_{l}\\right]\n$$\n\n的解, 式中 $T(x, y)$ 由式 (11.38) 给出。\n\n(b) 更新 $w_{k}$ 值: $w_{k} \\leftarrow w_{k}+\\delta_{k}$ 。\n\n(3) 如果不是所有 $w_{k}$ 都收玫, 重复步骤 (2)。\n\n在式 (11.36) 和式 (11.37) 中, $T(x, y)$ 表示数据 $(x, y)$ 中的特征总数, 对不同的数据 $(x, y)$取值可能不同。为了处理这个问题, 定义松他特征\n\n\n\\begin{equation*}\ns(x, y)=S-\\sum_{i=1}^{n+1} \\sum_{k=1}^{K} f_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\tag{11.39}\n\\end{equation*}\n\n\n式中 $S$ 是一个常数。选择足够大的常数 $S$ 使得对训练数据集的所有数据 $(x, y), s(x, y) \\geqslant 0$成立。这时特征总数可取 $S$ 。\n\n由式 (11.36), 对于转移特征 $t_{k}, \\delta_{k}$ 的更新方程是\n\n\n\\begin{equation*}\n\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n+1} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\exp \\left(\\delta_{k} S\\right)=E_{\\tilde{P}}\\left[t_{k}\\right] \\tag{11.40}\n\\end{equation*}\n\n\n\n\\begin{equation*}\n\\delta_{k}=\\frac{1}{S} \\log \\frac{E_{\\tilde{P}}\\left[t_{k}\\right]}{E_{P}\\left[t_{k}\\right]} \\tag{11.41}\n\\end{equation*}\n\n\n其中，\n\n\n\\begin{equation*}\nE_{P}\\left(t_{k}\\right)=\\sum_{x} \\tilde{P}(x) \\sum_{i=1}^{n+1} \\sum_{y_{i-1}, y_{i}} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\frac{\\alpha_{i-1}^{\\mathrm{T}}\\left(y_{i-1} \\mid x\\right) M_{i}\\left(y_{i-1}, y_{i} \\mid x\\right) \\beta_{i}\\left(y_{i} \\mid x\\right)}{Z(x)} \\tag{11.42}\n\\end{equation*}\n\n\n同样由式 (11.37), 对于状态特征 $s_{l}, \\delta_{k}$ 的更新方程是\n\n\n\\begin{gather*}\n\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n} s_{l}\\left(y_{i}, x, i\\right) \\exp \\left(\\delta_{K_{1}+l} S\\right)=E_{\\tilde{P}}\\left[s_{l}\\right]  \\tag{11.43}\\\\\n\\delta_{K_{1}+l}=\\frac{1}{S} \\log \\frac{E_{\\tilde{P}}\\left[s_{l}\\right]}{E_{P}\\left[s_{l}\\right]} \\tag{11.44}\n\\end{gather*}\n\n\n其中，\n\n\n\\begin{equation*}\nE_{P}\\left(s_{l}\\right)=\\sum_{x} \\tilde{P}(x) \\sum_{i=1}^{n} \\sum_{y_{i}} s_{l}\\left(y_{i}, x, i\\right) \\frac{\\alpha_{i}^{\\mathrm{T}}\\left(y_{i} \\mid x\\right) \\beta_{i}\\left(y_{i} \\mid x\\right)}{Z(x)} \\tag{11.45}\n\\end{equation*}\n\n\n以上算法称为算法 $\\mathrm{S}$ 。在算法 $\\mathrm{S}$ 中需要使常数 $S$ 取足够大, 这样一来, 每步迭代的增量向量会变大, 算法收玫会变慢。算法 $\\mathrm{T}$ 试图解决这个问题, 对每个观测序列 $x$ 计算其特征总数最大值 $T(x)$ :\n\n\n\\begin{equation*}\nT(x)=\\max _{y} T(x, y) \\tag{11.46}\n\\end{equation*}\n\n\n利用前向-后向递推公式, 可以很容易地计算 $T(x)=t$ 。这时, 关于转移特征参数的更新方程可以写成\n\n\n\\begin{align*}\nE_{\\tilde{P}}\\left[t_{k}\\right] & =\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n+1} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\exp \\left(\\delta_{k} T(x)\\right) \\\\\n& =\\sum_{x} \\tilde{P}(x) \\sum_{y} P(y \\mid x) \\sum_{i=1}^{n+1} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right) \\exp \\left(\\delta_{k} T(x)\\right) \\\\\n& =\\sum_{x} \\tilde{P}(x) a_{k, t} \\exp \\left(\\delta_{k} t\\right) \\\\\n& =\\sum_{t=0}^{T_{\\max }} a_{k, t} \\beta_{k}^{t} \\tag{11.47}\n\\end{align*}\n\n\n这里, $a_{k, t}$ 是特征 $t_{k}$ 的期待值, $\\delta_{k}=\\log \\beta_{k}$ 。 $\\beta_{k}$ 是多项式方程 (11.47) 唯一的实根, 可以用牛顿法求得, 从而求得相关的 $\\delta_{k}$ 。\n\n同样, 关于状态特征的参数更新方程可以写成\n\n$$\nE_{\\tilde{P}}\\left[s_{l}\\right]=\\sum_{x, y} \\tilde{P}(x) P(y \\mid x) \\sum_{i=1}^{n} s_{l}\\left(y_{i}, x, i\\right) \\exp \\left(\\delta_{K_{1}+l} T(x)\\right)\n$$\n\n\n\\begin{align*}\n& =\\sum_{x} \\tilde{P}(x) \\sum_{y} P(y \\mid x) \\sum_{i=1}^{n} s_{l}\\left(y_{i}, x, i\\right) \\exp \\left(\\delta_{K_{1}+l} T(x)\\right) \\\\\n& =\\sum_{x} \\tilde{P}(x) b_{l, t} \\exp \\left(\\delta_{k} t\\right) \\\\\n& =\\sum_{t=0}^{T_{\\max }} b_{l, t} \\gamma_{l}^{t} \\tag{11.48}\n\\end{align*}\n\n\n这里, $b_{l, t}$ 是特征 $s_{l}$ 的期望值, $\\delta_{l}=\\log \\gamma_{l}, \\gamma_{l}$ 是多项式方程 (11.48) 唯一的实根, 也可以用牛顿法求得。"
    },
    {
        "id": "chatcmpl-947F93TrMAJhs0uPqzTGQHZ6zSI9N",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"如何通过极大化训练数据的对数似然函数来求模型参数？\",\n    \"回答1\": \"可以通过对数似然函数 $L(w)$ 的极大化来求模型参数。对数似然函数是训练数据的对数似然函数，表示了模型参数和训练数据的关系。通过极大化对数似然函数，可以得到最优的模型参数。\",\n    \"问题2\": \"改进的迭代尺度法是如何优化对数似然函数改变量的下界的？\",\n    \"回答2\": \"改进的迭代尺度法通过迭代的方法不断优化对数似然函数改变量的下界，从而达到极大化对数似然函数的目的。在每步迭代过程中，该方法通过依次求解更新方程，得到参数向量的增量，从而优化对数似然函数。\",\n    \"问题3\": \"在算法中，如何更新转移特征和状态特征的参数向量？\",\n    \"回答3\": \"在算法中，更新转移特征和状态特征的参数向量需要依次求解更新方程。对于转移特征 $t_{k}$ 和状态特征 $s_{l}$，需要分别求解相应的更新方程，得到参数向量的增量 $\\boldsymbol{\\beta}$ 和 $\\boldsymbol{\\theta}$。\",\n    \"问题4\": \"为什么需要选择足够大的常数 $S$ 来处理特征总数的问题？\",\n    \"回答4\": \"选择足够大的常数 $S$ 可以确保对训练数据集的所有数据 $(x, y)$，特征总数 $s(x, y)$ 都大于等于零。这样可以避免更新过程中出现负值情况，同时保证算法收敛性和稳定性。\",\n    \"问题5\": \"算法 $\\text{S}$ 中如何计算观测序列 $x$ 的特征总数最大值 $T(x)$？\",\n    \"回答5\": \"算法 $\\text{S}$ 中可以利用前向-后向递推公式来计算观测序列 $x$ 的特征总数最大值 $T(x)$。通过计算得到 $T(x)$ 的值，可以更好地处理特征总数的问题。\",\n    \"问题6\": \"在算法中，如何求解转移特征和状态特征参数向量的增量 $\\boldsymbol{\\beta}$ 和 $\\boldsymbol{\\theta}$？\",\n    \"回答6\": \"在算法中，求解转移特征和状态特征参数向量的增量 $\\boldsymbol{\\beta}$ 和 $\\boldsymbol{\\theta}$ 需要利用牛顿法来求得多项式方程唯一的实根。通过牛顿法求解得到实根，进而求得相应的参数向量增量。\",\n    \"问题7\": \"算法 $\\text{T}$ 相比算法 $\\text{S}$ 有什么优势？\",\n    \"回答7\": \"相比算法 $\\text{S}$，算法 $\\text{T}$ 试图解决了选择足够大常数 $S$ 导致每步迭代增量变大、收敛变慢的问题。通过计算观测序列 $x$ 的特征总数最大值 $T(x)$，可以更好地处理特征总数问题，并且提高了算法的收敛速度和稳定性。\",\n    \"问题8\": \"在式 (11.40) 中，$\\boldsymbol{\\beta}_k$ 的计算公式是什么？\",\n    \"回答8\": \"在式 (11.40) 中，$\\boldsymbol{\\beta}_k$ 的计算公式为 $\\boldsymbol{\\beta}_k = \\frac{1}{S} \\text{log} \\frac{E_{\\tilde{P}}[t_k]}{E_{P}[t_k]}$。其中 $E_{\\tilde{P}}[t_k]$ 表示转移特征 $t_k$ 的期望值，$E_{P}[t_k]$ 表示经验分布下转移特征 $t_k$ 的期望值。\",\n    \"问题9\": \"在式 (11.43) 中，$\\boldsymbol{\\theta}_{K_1+l}$ 的计算公式是什么？\",\n    \"回答9\": \"在式 (11.43) 中，$\\boldsymbol{\\theta}_{K_1+l}$ 的计算公式为 $\\boldsymbol{\\theta}_{K_1+l} = \\frac{1}{S} \\text{log} \\frac{E_{\\tilde{P}}[s_l]}{E_{P}[s_l]}$。其中 $E_{\\tilde{P}}[s_l]$ 表示状态特征 $s_l$ 的期望值，$E_{P}[s_l]$ 表示经验分布下状态特征 $s_l$ 的期望值。\",\n    \"问题10\": \"如何利用前向-后向递推公式计算观测序列 $x$ 的特征总数最大值 $T(x)$？\",\n    \"回答10\": \"利用前向-后向递推公式可以很容易地计算观测序列 $x$ 的特征总数最大值 $T(x)=\\text{max}_{y} T(x, y)$。通过递推公式进行计算可以得到观测序列 $x$ 的最大特征总数值。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710768183,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1431,
            "prompt_tokens": 5976,
            "total_tokens": 7407
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]