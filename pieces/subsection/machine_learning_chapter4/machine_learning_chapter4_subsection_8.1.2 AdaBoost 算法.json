[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter4",
        "title": "8.1.2 AdaBoost 算法",
        "content_length": 3529,
        "content": "Title: 8.1.2 AdaBoost 算法\nContent:\n现在叙述 AdaBoost 算法。假设给定一个二类分类的训练数据集\n\n$$\nT=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}\n$$\n\n其中, 每个样本点由实例与标记组成。实例 $x_{i} \\in \\mathcal{X} \\subseteq \\boldsymbol{R}^{n}$, 标记 $y_{i} \\in \\mathcal{Y}=\\{-1,+1\\}, \\mathcal{X}$ 是实例空间, $\\mathcal{Y}$ 是标记集合。AdaBoost 利用以下算法, 从训练数据中学习一系列弱分类器或基本分类器, 并将这些弱分类器线性组合成为一个强分类器。\n\n\\section*{算法 8.1 (AdaBoost)}\n输入: 训练数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$, 其中 $x_{i} \\in \\mathcal{X} \\subseteq \\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y}=$ $\\{-1,+1\\}$; 弱学习算法。\n\n输出: 最终分类器 $G(x)$ 。\n\n(1) 初始化训练数据的权值分布\n\n$$\nD_{1}=\\left(w_{11}, \\cdots, w_{1 i}, \\cdots, w_{1 N}\\right), \\quad w_{1 i}=\\frac{1}{N}, \\quad i=1,2, \\cdots, N\n$$\n\n(2) 对 $m=1,2, \\cdots, M$,\n\n(a) 使用具有权值分布 $D_{m}$ 的训练数据集学习, 得到基本分类器:\n\n$$\nG_{m}(x): \\mathcal{X} \\rightarrow\\{-1,+1\\}\n$$\n\n(b) 计算 $G_{m}(x)$ 在训练数据集上的分类误差率:\n\n\n\\begin{equation*}\ne_{m}=\\sum_{i=1}^{N} P\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right)=\\sum_{i=1}^{N} w_{m i} I\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right) \\tag{8.1}\n\\end{equation*}\n\n\n(c) 计算 $G_{m}(x)$ 的系数:\n\n\n\\begin{equation*}\n\\alpha_{m}=\\frac{1}{2} \\log \\frac{1-e_{m}}{e_{m}} \\tag{8.2}\n\\end{equation*}\n\n\n这里的对数是自然对数。\n\n(d) 更新训练数据集的权值分布:\n\n\n\\begin{equation*}\nD_{m+1}=\\left(w_{m+1,1}, \\cdots, w_{m+1, i}, \\cdots, w_{m+1, N}\\right) \\tag{8.3}\n\\end{equation*}\n\n\n\n\\begin{equation*}\nw_{m+1, i}=\\frac{w_{m i}}{Z_{m}} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right), \\quad i=1,2, \\cdots, N \\tag{8.4}\n\\end{equation*}\n\n\n这里, $Z_{m}$ 是规范化因子, 且\n\n\n\\begin{equation*}\nZ_{m}=\\sum_{i=1}^{N} w_{m i} \\exp \\left(-\\alpha_{m} y_{i} G_{m}\\left(x_{i}\\right)\\right) \\tag{8.5}\n\\end{equation*}\n\n\n它使 $D_{m+1}$ 成为一个概率分布。\n\n（3）构建基本分类器的线性组合\n\n\n\\begin{equation*}\nf(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x) \\tag{8.6}\n\\end{equation*}\n\n\n得到最终分类器:\n\n\n\\begin{align*}\nG(x) & =\\operatorname{sign}(f(x)) \\\\\n& =\\operatorname{sign}\\left(\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)\\right) \\tag{8.7}\n\\end{align*}\n\n\n对 AdaBoost 算法作如下说明:\n\n(1) 步骤 (1) 假设训练数据集具有均匀的权值分布, 即每个训练样本在基本分类器的学习中作用相同, 这一假设保证第 1 步能够在原始数据上学习基本分类器 $G_{1}(x)$ 。\n\n(2) 步骤 (2) 中 AdaBoost 反复学习基本分类器, 在每一轮 $m=1,2, \\cdots, M$ 顺次地执行下列操作:\n\n(a) 使用当前分布 $D_{m}$ 加权的训练数据集学习基本分类器 $G_{m}(x)$ 。\n\n(b) 计算基本分类器 $G_{m}(x)$ 在加权训练数据集上的分类误差率:\n\n\n\\begin{align*}\ne_{m} & =\\sum_{i=1}^{N} P\\left(G_{m}\\left(x_{i}\\right) \\neq y_{i}\\right) \\\\\n& =\\sum_{G_{m}\\left(x_{i}\\right) \\neq y_{i}} w_{m i} \\tag{8.8}\n\\end{align*}\n\n\n这里, $w_{m i}$ 表示第 $m$ 轮中第 $i$ 个实例的权值, $\\sum_{i=1}^{N} w_{m i}=1$ 。这表明, $G_{m}(x)$ 在加权的训练数据集上的分类误差率是被 $G_{m}(x)$ 误分类样本的权值之和, 由此可以看出数据权值分布 $D_{m}$与基本分类器 $G_{m}(x)$ 的分类误差率的关系。\n\n(c) 计算基本分类器 $G_{m}(x)$ 的系数 $\\alpha_{m} \\circ \\alpha_{m}$ 表示 $G_{m}(x)$ 在最终分类器中的重要性。由式 (8.2) 可知, 当 $e_{m} \\leqslant \\frac{1}{2}$ 时, $\\alpha_{m} \\geqslant 0$, 并且 $\\alpha_{m}$ 随着 $e_{m}$ 的减小而增大, 所以分类误差率越小的基本分类器在最终分类器中的作用越大。\n\n(d) 更新训练数据的权值分布为下一轮作准备。式 (8.4) 可以写成\n\n$$\nw_{m+1, i}= \\begin{cases}\\frac{w_{m i}}{Z_{m}} \\mathrm{e}^{-\\alpha_{m}}, & G_{m}\\left(x_{i}\\right)=y_{i} \\\\ \\frac{w_{m i}}{Z_{m}} \\mathrm{e}^{\\alpha_{m}}, & G_{m}\\left(x_{i}\\right) \\neq y_{i}\\end{cases}\n$$\n\n由此可知, 被基本分类器 $G_{m}(x)$ 误分类样本的权值得以扩大, 而被正确分类样本的权值得以\n\n缩小。两相比较, 由式 (8.2) 知误分类样本的权值被放大 $\\mathrm{e}^{2 \\alpha_{m}}=\\frac{1-e_{m}}{e_{m}}$ 倍。因此, 误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据, 而不断改变训练数据权值的分布,使得训练数据在基本分类器的学习中起不同的作用, 这是 AdaBoost 的一个特点。\n\n（3）步骤 (3) 中线性组合 $f(x)$ 实现 $M$ 个基本分类器的加权表决。系数 $\\alpha_{m}$ 表示了基本分类器 $G_{m}(x)$ 的重要性, 这里, 所有 $\\alpha_{m}$ 之和并不为 $1 \\circ f(x)$ 的符号决定实例 $x$ 的类, $f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是 AdaBoost 的另一特点。"
    },
    {
        "id": "chatcmpl-93w68TNfCNuD6oo2aG8tgeAARNWqu",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"AdaBoost 算法的输入数据集 $T$ 的组成是怎样的？\",\n    \"回答1\": \"AdaBoost 算法的输入数据集 $T$ 由训练样本组成，每个样本点包括实例 $x_{i} \\text{ 和 } y_{i}$，其中实例 $x_{i} \\text{ 属于实例空间 } \\boldsymbol{X} \\text{ 的子集 } \\boldsymbol{R}^{n}$，标记 $y_{i} \\text{ 属于标记集合 } \\boldsymbol{Y}=\\big\\\\{-1,+1\\big\\big\\text{。}$\",\n    \"问题2\": \"AdaBoost 算法中的步骤 (1) 初始化训练数据的权值分布是如何进行的？\",\n    \"回答2\": \"AdaBoost 算法中的步骤 (1) 初始化训练数据的权值分布为 $D_{1}=\\big(w_{11}, \\text{...}, w_{1 i}, \\text{...}, w_{1 N}\\big)$，其中 $w_{1 i}=\\frac{1}{N}$，表示假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同。\",\n    \"问题3\": \"在 AdaBoost 算法中，如何计算基本分类器 $G_{m}(x)$ 在训练数据集上的分类误差率？\",\n    \"回答3\": \"在 AdaBoost 算法中，计算基本分类器 $G_{m}(x)$ 在训练数据集上的分类误差率使用公式 $e_{m}=\\text{sum}_{i=1}^{N} P\\big(G_{m}\\big(x_{i}\\big) \\neq y_{i}\\big)=\\text{sum}_{i=1}^{N} w_{m i} I\\big(G_{m}\\big(x_{i}\\big) \\neq y_{i}\\big)$。\",\n    \"问题4\": \"在 AdaBoost 算法中，如何计算基本分类器 $G_{m}(x)$ 的系数 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{m}}$？\",\n    \"回答4\": \"在 AdaBoost 算法中，计算基本分类器 $G_{m}(x)$ 的系数使用公式 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\frac{1}{2}} \\text{log} \\frac{1-e_{m}}{e_{m}}}$。\",\n    \"问题5\": \"AdaBoost 算法中的步骤 (3) 中线性组合 $f(x)$ 的作用是什么？\",\n    \"回答5\": \"AdaBoost 算法中的步骤 (3) 中线性组合 $f(x)=\\text{sum}_{m=1}^{M} \\frac{1}{2} \\text{log} \\frac{1-e_{m}}{e_{m}} G_{m}(x)$ 实现了 $M$ 个基本分类器的加权表决，得到最终分类器 $G(x)=\\text{sign}(f(x))$。这使得最终分类器能够更准确地对实例进行分类。\",\n    \"问题6\": \"AdaBoost 算法中为什么需要反复学习基本分类器？\",\n    \"回答6\": \"AdaBoost 算法需要反复学习基本分类器是为了不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用。这样可以逐步提升模型对误分类样本的关注，从而提高整体分类器的性能。\",\n    \"问题7\": \"在 AdaBoost 算法中，系数 $\\beta_m$ 表示了什么？\",\n    \"回答7\": \"在 AdaBoost 算法中，系数 $\\beta_m$ 表示了基本分类器 $G_m(x)$ 在最终分类器中的重要性。当分类误差率 $e_m \\text{小于等于} \\frac{1}{2}$ 时，$\\beta_m \\text{大于等于} 0$，并且随着 $e_m$ 的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。\",\n    \"问题8\": \"AdaBoost 算法中规范化因子 $Z_m$ 的作用是什么？\",\n    \"回答8\": \"在 AdaBoost 算法中，规范化因子 $Z_m$ 的作用是使更新后的训练数据权值分布 $D_{m+1}$ 成为一个概率分布。它通过对被基本分类器误分类样本的权值进行放大，从而调整了下一轮学习中各样本的重要性。\",\n    \"问题9\": \"AdaBoost 算法中为什么要构建基本分类器的线性组合？\",\n    \"回答9\": \"AdaBoost 算法构建基本分类器的线性组合是为了得到最终分类器。通过将各个基本分类器按照其重要性进行加权组合，可以得到更准确和稳定的整体分类效果。\",\n    \"问题10\": \"AdaBoost 算法相比其他算法有哪些优势？\",\n    \"回答10\": \"相比其他算法，AdaBoost 算法具有较高的精度和泛化能力。它能够通过反复学习基本分类器，并根据其表现调整样本权重，逐步提升模型对误分类样本的关注，从而提高整体模型性能。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710725340,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1609,
            "prompt_tokens": 4770,
            "total_tokens": 6379
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]