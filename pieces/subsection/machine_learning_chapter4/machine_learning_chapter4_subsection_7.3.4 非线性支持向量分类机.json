[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter4",
        "title": "7.3.4 非线性支持向量分类机",
        "content_length": 2788,
        "content": "Title: 7.3.4 非线性支持向量分类机\nContent:\n如上所述，利用核技巧，可以将线性分类的学习方法应用到非线性分类问题中去。将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数。\n\n定义 7.8 (非线性支持向量机) 从非线性分类训练集，通过核函数与软间隔最大化或凸二次规划 $(7.95) \\sim(7.97)$ 学习得到的分类决策函数\n\n\n\\begin{equation*}\nf(x)=\\operatorname{sign}\\left(\\sum_{i=1}^{N} \\alpha_{i}^{*} y_{i} K\\left(x, x_{i}\\right)+b^{*}\\right) \\tag{7.94}\n\\end{equation*}\n\n\n称为非线性支持向量机, $K(x, z)$ 是正定核函数。\n\n下面叙述非线性支持向量机学习算法。\n\n\\section*{算法 7.4 (非线性支持向量机学习算法)}\n输入: 训练数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$, 其中 $x_{i} \\in \\mathcal{X}=\\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y}=$ $\\{-1,+1\\}, i=1,2, \\cdots, N$ 。\n\n输出: 分类决策函数。\n\n(1) 选取适当的核函数 $K(x, z)$ 和适当的参数 $C$, 构造并求解最优化问题\n\n\\[\n\\begin{array}{ll}\n\\min _{\\alpha} & \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K\\left(x_{i}, x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\\\\n\\text { s.t. } & \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n& 0 \\leqslant \\alpha_{i} \\leqslant C, \\quad i=1,2, \\cdots, N \\tag{7.97}\n\\end{array}\n\\]\n\n求得最优解 $\\alpha^{*}=\\left(\\alpha_{1}^{*}, \\alpha_{2}^{*}, \\cdots, \\alpha_{N}^{*}\\right)^{\\mathrm{T}}$ 。\n\n(2) 选择 $\\alpha^{*}$ 的一个正分量 $0<\\alpha_{j}^{*}<C$, 计算\n\n$$\nb^{*}=y_{j}-\\sum_{i=1}^{N} \\alpha_{i}^{*} y_{i} K\\left(x_{i}, x_{j}\\right)\n$$\n\n（3）构造决策函数:\n\n$$\nf(x)=\\operatorname{sign}\\left(\\sum_{i=1}^{N} \\alpha_{i}^{*} y_{i} K\\left(x, x_{i}\\right)+b^{*}\\right)\n$$\n\n当 $K(x, z)$ 是正定核函数时, 问题 $(7.95) \\sim(7.97)$ 是凸二次规划问题, 解是存在的。\n\n\\section*{7.4 序列最小最优化算法}\n本节讨论支持向量机学习的实现问题。我们知道, 支持向量机的学习问题可以形式化为求解凸二次规划问题。这样的凸二次规划问题具有全局最优解, 并且有许多最优化算法可以用于这一问题的求解。但是当训练样本容量很大时, 这些算法往往变得非常低效, 以致无法使用。所以, 如何高效地实现支持向量机学习就成为一个重要的问题。目前人们已提出许多快速实现算法。本节讲述其中的序列最小最优化 (sequential minimal optimization, SMO) 算法, 这种算法于 1998 年由 Platt 提出。\n\nSMO 算法要解如下凸二次规划的对偶问题:\n\n\\[\n\\begin{array}{ll}\n\\min _{\\alpha} & \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K\\left(x_{i}, x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\\\\n\\text { s.t. } & \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n& 0 \\leqslant \\alpha_{i} \\leqslant C, \\quad i=1,2, \\cdots, N \\tag{7.100}\n\\end{array}\n\\]\n\n在这个问题中, 变量是拉格朗日乘子, 一个变量 $\\alpha_{i}$ 对应于一个样本点 $\\left(x_{i}, y_{i}\\right)$; 变量的总数等于训练样本容量 $N$ 。\n\nSMO 算法是一种启发式算法, 其基本思路是: 如果所有变量的解都满足此最优化问题的 KKT 条件 (Karush-Kuhn-Tucker conditions), 那么这个最优化问题的解就得到了。因为 KKT 条件是该最优化问题的充分必要条件。否则, 选择两个变量, 固定其他变量, 针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解, 因为这会使得原始二次规划问题的目标函数值变得更小。重要的是, 这时子问题可以通过解析方法求解, 这样就可以大大提高整个算法的计算速度。子问题有两个变量,一个是违反 KKT 条件最严重的那一个, 另一个由约束条件自动确定。如此, SMO 算法将原问题不断分解为子问题并对子问题求解, 进而达到求解原问题的目的。\n\n注意, 子问题的两个变量中只有一个是自由变量。假设 $\\alpha_{1}, \\alpha_{2}$ 为两个变量, $\\alpha_{3}, \\alpha_{4}, \\cdots, \\alpha_{N}$固定, 那么由等式约束 (7.99) 可知:\n\n$$\n\\alpha_{1}=-y_{1} \\sum_{i=2}^{N} \\alpha_{i} y_{i}\n$$\n\n如果 $\\alpha_{2}$ 确定, 那么 $\\alpha_{1}$ 也随之确定, 所以子问题中同时更新两个变量。\n\n整个 SMO 算法包括两个部分: 求解两个变量二次规划的解析方法和选择变量的启发式方法。"
    },
    {
        "id": "chatcmpl-93w5WQtSIVBjxViARhkIZyku8PVO1",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"非线性支持向量机与线性支持向量机的主要区别是什么？\",\n    \"回答1\": \"非线性支持向量机通过核函数与软间隔最大化或凸二次规划学习得到分类决策函数，而线性支持向量机则直接利用内积进行分类。这意味着非线性支持向量机能够处理非线性分类问题，而线性支持向量机只能处理线性分类问题。\",\n    \"问题2\": \"非线性支持向量机的分类决策函数是如何定义的？\",\n    \"回答2\": \"非线性支持向量机的分类决策函数定义为 $f(x)=\\text{sign}\\bigg(\\text{∑}_{i=1}^{N} \\boldsymbol{\\text{α}}_{i}^{*} y_{i} K(x, x_{i})+b^{*}\\bigg)$，其中 $K(x, z)$ 是正定核函数。这个函数利用训练得到的拉格朗日乘子 $\\boldsymbol{\\text{α}}^{*}$ 和偏置 $b^{*}$ 对新样本进行分类。\",\n    \"问题3\": \"非线性支持向量机学习算法的输入和输出是什么？\",\n    \"回答3\": \"非线性支持向量机学习算法的输入是训练数据集 $T=\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big{(}x_{1}, y_{1}\\big), \\text{...}, \\text{(}x_{N}, y_{N}\\text{)}$，其中 $x_{i} \\text{在} \\boldsymbol{R}^{n}$，$y_{i} \\text{在} \\boldsymbol{Y}=\\text{{{-}}1,+1}$，$i=1,2, \\text{...}, N$。输出是分类决策函数，用于对新样本进行分类。\",\n    \"问题4\": \"非线性支持向量机学习算法中的最优化问题是如何定义的？\",\n    \"回答4\": \"非线性支持向量机学习算法中的最优化问题定义为 $\\text{min}_{\\boldsymbol{\\text{α}}} \\frac{1}{2} \\text{∑}_{i=1}^{N} \\text{∑}_{j=1}^{N} \\boldsymbol{\\text{α}}_{i} \\boldsymbol{\\text{α}}_{j} y_{i} y_{j} K(x_{i}, x_{j})-\\text{∑}_{i=1}^{N} \\boldsymbol{\\text{α}}_{i}$，$\\text{s.t.}$ $\\text{∑}_{i=1}^{N} \\boldsymbol{\\text{α}}_{i} y_{i}=0$，$0 \\text{{≤}} \\boldsymbol{\\text{α}}_{i} \\text{{≤}} C, i=1,2, \\text{...}, N$。这个问题通过选择适当的核函数和参数 $C$，构造并求解以获得最优解 $\\boldsymbol{\\text{α}}^{*}$。\",\n    \"问题5\": \"什么是序列最小最优化 (SMO) 算法？\",\n    \"回答5\": \"序列最小最优化 (SMO) 算法是一种启发式算法，用于解决凸二次规划的对偶问题。其基本思路是将原始问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。这种算法通过选择两个变量构建一个二次规划问题，并通过解析方法求解子问题，以提高整个算法的计算速度。\",\n    \"问题6\": \"为什么需要快速实现支持向量机学习算法？\",\n    \"回答6\": \"当训练样本容量很大时，传统的最优化算法往往变得非常低效，甚至无法使用。因此，需要快速实现支持向量机学习算法来提高计算效率，使得支持向量机在处理大规模数据时仍能保持较高的效率和实用性。\",\n    \"问题7\": \"SMO 算法中的基本思路是什么？\",\n    \"回答7\": \"SMO 算法的基本思路是如果所有变量的解都满足最优化问题的 KKT 条件，那么这个最优化问题的解就得到了。否则，选择两个变量构建一个二次规划问题，并通过解析方法求解子问题。这种分解和求解子问题的方式可以大大提高整个算法的计算速度。\",\n    \"问题8\": \"SMO 算法中如何选择变量并构造子问题？\",\n    \"回答8\": \"在 SMO 算法中，选择两个变量，并固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解。重要的是，这时子问题可以通过解析方法求解，从而提高整个算法的计算速度。\",\n    \"问题9\": \"SMO 算法中如何更新变量？\",\n    \"回答9\": \"在 SMO 算法中，如果选择了两个变量 $\\boldsymbol{\\text{α}}_{1}, \\boldsymbol{\\text{α}}_{2}$ 并固定其他变量，则由等式约束可知 $\\boldsymbol{\\text{α}}_{1}$ 与 $\\boldsymbol{\\text{α}}_{2}$ 同时更新。如果 $\\boldsymbol{\\text{α}}_{2}$ 确定，则 $\\boldsymbol{\\text{α}}_{1}$ 也随之确定。\",\n    \"问题10\": \"SMO 算法相比传统最优化算法有哪些优势？\",\n    \"回答10\": \"相比传统最优化算法，SMO 算法在处理支持向量机学习问题时能够更高效地实现。它通过选择两个变量构建子问题，并采用解析方法求解子问题，从而提高了整个算法的计算速度。这使得 SMO 算法在处理大规模数据时具有更好的效率和实用性。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710725302,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1708,
            "prompt_tokens": 4414,
            "total_tokens": 6122
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]