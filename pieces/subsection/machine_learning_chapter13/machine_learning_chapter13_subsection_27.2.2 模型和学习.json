[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter13",
        "title": "27.2.2 模型和学习",
        "content_length": 4599,
        "content": "Title: 27.2.2 模型和学习\nContent:\n\\section*{1. 模型}\nBERT 是双向 Transformer 编码器表示 (bidirectional encoder representations from Transformers) 的缩写。BERT 的模型基于 Transformer 的编码器, 是双向语言模型。BERT 的预训练主要是掩码语言模型化, 使用大规模语料基于去噪自动编码器原理进行模型的参数估计, 学习的目标是复原给定的掩码单词序列中被掩码的每一个单词。学习和预测都是非自回归过程 (non-autoregressive process)。\n\nBERT 模型有以下结构。输入是两个合并的单词序列。\n\n$$\n<\\operatorname{cls}>, x_{1}, x_{2}, \\cdots, x_{m-1},<\\operatorname{sep}>, x_{m+1}, x_{m+2}, \\cdots, x_{m+n-1},<\\operatorname{sep}>\n$$\n\n其中, $x_{1}, x_{2}, \\cdots, x_{m-1}$ 是第一个单词序列, $x_{m+1}, x_{m+2}, \\cdots, x_{m+n-1}$ 是第二个单词序列, $\\langle\\mathrm{cls}>$ 是表示类别的特殊字符, $<\\mathrm{sep}>$ 是表示序列分割的特殊字符, 合并的单词序列共有 $m+n+1$ 个单词和字符。每一个单词序列是一个句子或一段文章。首先经过输入层,产生初始的单词表示向量的序列, 记作矩阵 $\\boldsymbol{H}^{(0)}$ :\n\n\n\\begin{equation*}\n\\boldsymbol{H}^{(0)}=\\boldsymbol{X}+\\boldsymbol{S}+\\boldsymbol{E} \\tag{27.22}\n\\end{equation*}\n\n\n其中, 矩阵 $\\boldsymbol{X}$ 表示单词的词嵌入的序列 $\\boldsymbol{X}=\\left(\\boldsymbol{x}_{0}, \\boldsymbol{x}_{1}, \\cdots, \\boldsymbol{x}_{m+n}\\right)$; 矩阵 $\\boldsymbol{E}$ 表示单词的位置嵌入的序列 $\\boldsymbol{E}=\\left(e_{0}, e_{1}, \\cdots, e_{m+n}\\right)$; 矩阵 $\\boldsymbol{S}$ 是区别前后单词序列的标记序列 $\\boldsymbol{S}=$ $(\\boldsymbol{a}, \\boldsymbol{a}, \\cdots \\boldsymbol{a}, \\boldsymbol{b}, \\boldsymbol{b}, \\cdots, \\boldsymbol{b})$, 含有 $m+1$ 个向量 $\\boldsymbol{a}$ 和 $n$ 个向量 $\\boldsymbol{b} \\circ \\boldsymbol{X}, \\boldsymbol{E}, \\boldsymbol{S}, \\boldsymbol{H}^{(0)}$ 是 $d \\times(m+n+1)$矩阵, 设词嵌入、位置嵌入、标记向量的维度是 $d$ 。图 27.4 显示的是 BERT 模型输入层的计算。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-38}\n\\end{center}\n\n图 27.4 BERT 模型输入层的计算\n\n使用拼接的单词序列 (两个单词序列) 作为输入是让 BERT 不仅能用于以一个文本为输入的任务, 如文本分类, 也能用于以两个文本为输入的任务, 如文本匹配。\n\n之后经过 $L$ 个编码层, 得到单词的表示向量的序列, 记作 $\\boldsymbol{H}^{(L)}$ :\n\n\n\\begin{equation*}\n\\boldsymbol{H}^{(L)}=\\text { transformer_encoder }\\left(\\boldsymbol{H}^{(0)}\\right) \\tag{27.23}\n\\end{equation*}\n\n\n具体地,\n\n$$\n\\boldsymbol{H}^{(L)}=\\left(\\boldsymbol{h}_{0}^{(L)}, \\boldsymbol{h}_{1}^{(L)}, \\cdots, \\boldsymbol{h}_{m+n}^{(L)}\\right)\n$$\n\n其中, $\\boldsymbol{h}_{i}^{(L)}$ 是第 $i$ 个位置的单词的表示向量。BERT 模型中, 在每一层, 每一个位置的表示向量是该位置的单词基于之前位置和之后位置的上下文的表示 (contextualized representation)。\n\nBERT 模型的输出是在合并的单词序列的各个位置上的条件概率, 第 $i$ 个位置的单词 (包括特殊字符) 的条件概率 $p_{i}$ 定义为\n\n\n\\begin{equation*}\nP_{\\boldsymbol{\\theta}}\\left(x_{i} \\mid x_{0}, \\cdots, x_{i-1}, x_{i+1}, \\cdots, x_{m+n}\\right)=\\operatorname{softmax}\\left(\\boldsymbol{W}_{x}^{\\mathrm{T}} \\boldsymbol{h}_{i}^{(L)}\\right)=\\frac{\\exp \\left(\\boldsymbol{w}_{x_{i}}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{i}^{(L)}\\right)}{\\sum_{x_{i}^{\\prime}} \\exp \\left(\\boldsymbol{w}_{\\boldsymbol{x}_{i}^{\\prime}}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{i}^{(L)}\\right)} \\tag{27.24}\n\\end{equation*}\n\n\n其中, $x_{0}, \\cdots, x_{i-1}, x_{i+1}, \\cdots, x_{m+n}$ 是其他位置的单词, $x_{i}$ 是当前位置的单词, $\\boldsymbol{W}_{x}$ 表示所有单词的权重矩阵, $\\theta$ 表示模型的参数。\n\n图 27.4 显示的是 BERT 模型的架构, 其中输入层进行式 (27.22) 的计算, 编码层整体进行式 (27.23) 的计算, 输出层进行式 (27.24) 的计算。BERT 利用 Transformer 编码器对语言的内容进行层次化的组合式的表示。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-39}\n\\end{center}\n\n图 27.5 BERT 模型的架构\n\nBERT 中编码层的多头自注意力都是双向的, 也就是各个位置的单词针对其他位置的单词都进行自注意力计算, 这一点与 GPT 不同。图 27.6 比较了 BERT 和 GPT 中表示之间\\\\\n关系的差异。BERT 中每一层每一个位置的表示都是由下一层所有位置的表示组合而成, 而 GPT 中每一层每一个位置的表示都是由下一层之前所有位置的表示组合而成。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-40(1)}\n\\end{center}\n\nBERT\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-40}\n\\end{center}\n\nGPT\n\n图 27.6 BERT 模型和 GPT 模型的比较\n\nBERT 模型有三个超参数: 编码层的层数 $L$ 、头的个数 $h$ 、模型的维度 $d$ 。 BERT Base 模型取 $L=12, h=12, d=768$ 。输入合并单词序列的最大长度一般是 128 。\n\n\\section*{2. 预训练}\n预训练数据的每个样本由两个单词序列 A 和 B 合并组成, 中间由特殊字符 $<\\mathrm{sep}>$ 分割。 $50 \\%$ 的样本中 $\\mathrm{A}$ 和 $\\mathrm{B}$ 是同一篇文章中的连续文本, $50 \\%$ 的样本中 $\\mathrm{A}$ 和 $\\mathrm{B}$ 来自不同篇文章。在每一个样本的合并单词序列中, 随机选择 15\\% 的位置进行掩码操作。对于掩码操作,在选择的 $15 \\%$ 的位置上, 有 $80 \\%$ 的单词替换为特殊字符 , 有 $10 \\%$ 的单词随机替换为其他单词, 剩下 $10 \\%$ 的单词保持不变。\n\nBERT 模型的预训练由两部分组成, 掩码语言模型化 (mask language modeling) 和下句预测 (next sentence prediction)。掩码语言模型化的目标是复原输入单词序列中被掩码的单词。可以看作是去噪自动编码器学习, 对被掩码的单词独立地进行复原。下句预测的目标是判断输入单词序列是否来自同一篇文章。这里说的下句未必是一个自然句, 也可以是多个自然句。掩码单词序列表示为 $\\tilde{\\boldsymbol{x}}$ 。\n\n掩码语言模型化在每一个掩码位置计算条件概率 (式 (27.4)):\n\n\n\\begin{equation*}\nP_{\\theta}\\left(x_{i} \\mid \\tilde{x}_{0}, \\tilde{x}_{1}, \\cdots, \\tilde{x}_{m+n}\\right)=\\operatorname{softmax}\\left(\\boldsymbol{W}_{x}^{\\mathrm{T}} \\boldsymbol{h}_{i}^{(L)}\\right)=\\frac{\\exp \\left(\\boldsymbol{w}_{x_{i}}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{i}^{(L)}\\right)}{\\sum_{x_{i}^{\\prime}} \\exp \\left(\\boldsymbol{w}_{x_{i}^{\\prime}}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{i}^{(L)}\\right)} \\tag{27.25}\n\\end{equation*}\n\n\n\n\\end{document}"
    },
    {
        "id": "chatcmpl-93wMQgDvv75kJB4j64YPnTzYJpdTI",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"BERT 模型的输入结构是怎样的？\",\n    \"回答1\": \"BERT 模型的输入是两个合并的单词序列，其中包括特殊字符 $<\\text{cls}>$、$<\\text{sep}>$，以及两个单词序列 $x_{1}, x_{2}, \\text{...}, x_{m-1}$ 和 $x_{m+1}, x_{m+2}, \\text{...}, x_{m+n-1}$。这种输入结构使得 BERT 不仅能用于以一个文本为输入的任务，如文本分类，也能用于以两个文本为输入的任务，如文本匹配。\",\n    \"问题2\": \"BERT 模型中的编码层是如何得到单词的表示向量序列的？\",\n    \"回答2\": \"经过 $L$ 个编码层后，BERT 模型得到单词的表示向量序列 $\\boldsymbol{H}^{(L)}$，通过计算 $\\boldsymbol{H}^{(L)}=\\text{transformer_encoder}\\big(\\boldsymbol{H}^{(0)}\\big)$ 得到。在每一层，每一个位置的表示向量是该位置的单词基于之前位置和之后位置的上下文的表示。\",\n    \"问题3\": \"BERT 模型的输出是什么？\",\n    \"回答3\": \"BERT 模型的输出是在合并的单词序列的各个位置上的条件概率，即第 $i$ 个位置的单词（包括特殊字符）的条件概率 $p_{i}$。这些条件概率由公式 $P_{\\boldsymbol{\\theta}}\\big(x_{i} \\big| x_{0}, \\text{...}, x_{i-1}, x_{i+1}, \\text{...}, x_{m+n}\\big)=\\text{softmax}\\big(\\boldsymbol{W}_{x}^{\\top} \\boldsymbol{h}_{i}^{(L)}\\big)$ 给出。\",\n    \"问题4\": \"BERT 模型中的预训练由哪两部分组成？\",\n    \"回答4\": \"BERT 模型的预训练由掩码语言模型化（mask language modeling）和下句预测（next sentence prediction）两部分组成。掩码语言模型化的目标是复原输入单词序列中被掩码的单词，而下句预测的目标是判断输入单词序列是否来自同一篇文章。\",\n    \"问题5\": \"在 BERT 模型中，掩码语言模型化是如何计算条件概率的？\",\n    \"回答5\": \"在 BERT 模型中，掩码语言模型化在每一个掩码位置计算条件概率 $P_{\\theta}\\big(x_{i} \\big| \\tilde{x}_{0}, \\tilde{x}_{1}, \\text{...}, \\tilde{x}_{m+n}\\big)=\\text{softmax}\\big(\\boldsymbol{W}_{x}^{\\top} \\boldsymbol{h}_{i}^{(L)}\\big)$。这意味着对于每一个掩码位置，都计算其被掩码单词复原的条件概率。\",\n    \"问题6\": \"BERT 模型中有哪些超参数？\",\n    \"回答6\": \"BERT 模型有三个超参数: 编码层的层数 $L$、头的个数 $h$、模型的维度 $d$。例如，BERT Base 模型取 $L=12, h=12, d=768$。\",\n    \"问题7\": \"为什么 BERT 模型使用拼接的单词序列作为输入？\",\n    \"回答7\": \"使用拼接的单词序列作为输入使得 BERT 不仅能用于以一个文本为输入的任务，如文本分类，也能用于以两个文本为输入的任务，如文本匹配。这种输入结构增加了模型的适用性和灵活性。\",\n    \"问题8\": \"BERT 模型中编码层与 GPT 模型有何不同？\",\n    \"回答8\": \"在 BERT 中，每一层每一个位置的表示都是由下一层所有位置的表示组合而成；而在 GPT 中，每一层每一个位置的表示都是由下一层之前所有位置的表示组合而成。这表明 BERT 中编码层的多头自注意力都是双向的，与 GPT 不同。\",\n    \"问题9\": \"在 BERT 预训练中，对于掩码操作有怎样的处理方式？\",\n    \"回答9\": \"在 BERT 预训练中，在选择 $15\\text{%}$ 的位置进行掩码操作。其中有 $80\\text{%}$ 的单词被替换为特殊字符，有 $10\\text{%}$ 的单词随机替换为其他单词，剩下 $10\\text{%}$ 的单词保持不变。这种处理方式使得模型能够学习到对被掩码单词独立地进行复原。\",\n    \"问题10\": \"BERT 模型中输入合并单词序列长度通常设定为多少？\",\n    \"回答10\": \"BERT 模型中输入合并单词序列长度通常设定为 128。这意味着对于较长的文本输入，可能需要进行截断或其他处理以满足模型输入要求。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710726350,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1432,
            "prompt_tokens": 5209,
            "total_tokens": 6641
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]