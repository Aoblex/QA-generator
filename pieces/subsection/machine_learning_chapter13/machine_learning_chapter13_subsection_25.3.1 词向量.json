[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter13",
        "title": "25.3.1 词向量",
        "content_length": 4380,
        "content": "Title: 25.3.1 词向量\nContent:\n\\section*{1. 词向量的定义}\n向量或单词向量 (word vector) 是指表示自然语言的单词的实数向量。把自然语言的单词映射到实数向量空间也称作词嵌入或单词嵌入（word embedding）。词向量空间的维度远小于单词表的大小。词向量的内积或余弦表示单词间的相似性。自然语言处理中, 通常输入是一个句子, 句子中的每一个单词用词向量表示。词向量表示属于分布式表示。\n\n机器学习中概念 (特征) 的表示方法起着重要作用, 直接影响到学习的效果和效率。图 25.11 的例子通过数字的不同表示法对算法的影响间接说明这一点。Hinton 提出了分布式表示 (distributed representation) 和局部式表示 (local representation) 的概念, 指出分布式表示作为神经网络学习的概念的表示方法具有许多优点 ${ }^{[10]}$ 。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-03}\n\\end{center}\n\n图 25.11 表示对算法产生影响的例子\n\n假设有 $K$ 个概念, 可以用两种方法表示。一种是 $K$ 维独热向量。每一个概念由一个 $K$维 $0 / 1$ 向量表示, 概念对应的维度取值为 1 , 其他维度取值为 0 。称这种概念表示方法为局部式表示。另一种是用 $N$ 维 $0 / 1$ 向量或者 $N$ 维实数向量, 这时 $\\log _{2} K<N \\ll K$ 。每一个概念\\\\\n由一个 $N$ 维 $0 / 1$ 向量或者 $N$ 维实数向量表示。称这种概念的表示方法为分布式表示。因为这种表示由向量的所有维度组合而成, 所以是 “分布式的”。分布式表示的向量可以是神经网络的某一层的输出, 其中每一维对应一个神经元。\n\n分布式表示与局部式表示相比有诸多优点。首先, 容易表示相似性, 用于机器学习可以提高模型的学习泛化能力。当表示是 $0 / 1$ 向量时可以用汉明距离 (Hamming distance), 当表示是实数向量时可以用内积或余弦, 很方便地计算概念之间的相似度, 使学到的模型能对相似的输入产生相似输出。其次, 表示的效率高。分布式表示中的维度 $K$ 远远小于局部式表示中的维度 $N$ 。再次, 拥有稳健性 (robustness)。由噪声等带来的表示在一定范围内的变化往往不会对相似度计算产生太大影响。最后, 拥有可扩展性。有新增概念时, 可以比较容易地将其表示加入到已有的表示中, 不需要改变表示的框架 (增加维度)。事实上, 有大量证据证明生物神经网络中的表示也是分布式的。\n\n词向量是分布式表示。图 25.12 给出一个简单的例子。假设只有三个单词, 有 3 维的局部式表示, 2 维的分布式表示。图左侧显示的是局部式表示, 右侧是分布式表示。可以看出, 单词的分布式表示, 也就是词向量, 可以用实数空间中的内积或余弦更好地描述单词之间的语义相似性。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-04}\n\\end{center}\n\n局部式表示\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-04(1)}\n\\end{center}\n\n分布式表示\n\n图 25.12 词向量表示的例子\n\n\\section*{2. 词向量的学习}\n在具体的学习任务中, 词向量可以作为模型的一部分同模型一起学习, 也可以预先学好然后在学习中固定使用。前者适合训练数据多的情况, 后者适合训练数据少的情况。词向量的预先学习通过无监督学习进行。有多种方法, 这里介绍常用的跳元模型加负采样 (skip-gram model with negative sampling) 方法, 简称跳元模型 (skip-gram)。\n\n词向量学习方法的基本想法是在大量的语料中收集单词和上下文的共现数据, 从共现数据中学习每一个单词的词向量, 这里的上下文是指在文章中以一个单词为中心前后固定窗口内出现的所有单词。比如, 单词是 “高兴”, 从句子 “小朋友们高兴得手舞足蹈” 中可以获得窗口内四个单词组成的上下文 “小朋友、们、得、手舞足蹈”。可以从共现的上下文的单词 “手舞足蹈” “小朋友” 等学习单词 “高兴” 的语义表示。参见表 25.1 。\n\n假设所有单词的集合是 $\\mathcal{W}$, 所有上下文的集合是 $\\mathcal{C}$ 。定义下面的单词和上下文的共现模型, 代表单词 $w \\in \\mathcal{W}$ 和上下文 $c \\in \\mathcal{C}$ 共现的概率:\n\n\n\\begin{equation*}\nP(d=1 \\mid w, c)=\\frac{1}{1+\\exp (-\\boldsymbol{w} \\cdot \\boldsymbol{c})} \\tag{25.39}\n\\end{equation*}\n\n\n\n\\begin{equation*}\nP(d=0 \\mid w, c)=\\frac{1}{1+\\exp (\\boldsymbol{w} \\cdot \\boldsymbol{c})} \\tag{25.40}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$ 是维度为 $l$ 的参数向量。实际是判断共现与否的分类模型。\n\n表 25.1 单词和上下文共现数据的例子\n\n\\begin{center}\n\\begin{tabular}{c|c|c|c}\n\\hline\n & 高兴 & 愉快 & 生气 \\\\\n\\hline\n$\\{$ 小朋友、们、得、手舞足蹈 $\\}$ & 55 &  &  \\\\\n\\hline\n$\\{$ 听、令人、的、音乐 $\\}$ & 8 & 120 &  \\\\\n\\hline\n$\\{$ 单词句、描写、的、心情 $\\}$ & 4 & 11 & 1 \\\\\n\\hline\n$\\{$ 让、人、的、缺点 $\\}$ &  &  & 87 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n针对大量单词和上下文共现数据, 定义基于共现模型预测的目标函数, 使用随机梯度下降进行优化, 学习共现模型的参数向量 $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$ 。目标函数是\n\n\n\\begin{equation*}\n\\sum_{w} \\sum_{c} f(w, c)\\left(-\\log P(d=1 \\mid w, c)-k \\cdot E_{\\bar{c} \\in P(c)} \\log P(d=0 \\mid w, \\bar{c})\\right) \\tag{25.41}\n\\end{equation*}\n\n\n其中, $f(w, c)$ 表示单词 $w$ 和上下文 $c$ 在共现数据中出现的次数, $w$ 和 $c$ 的一次共现看作是一个正样本, 随机采样 $k$ 个 $w$ 未出现的上下文 $\\bar{c}, w$ 和 $\\bar{c}$ 组成 $k$ 个负样本。\n\n这样得到的每一个单词 $w$ 的参数向量 $\\boldsymbol{w}$ 就是该单词的词向量。直观上通过学习得到参数向量 $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$ 能很好地说明共现数据, 其中的参数向量 $\\boldsymbol{w}$ 是从共现数据角度对单词 $w$ 的解释。\n\n跳元模型还有以下解释。定义单词 $w$ 和上下文 $c$ 之间的互信息 (mutual information):\n\n\n\\begin{equation*}\nI(w, c)=\\log \\frac{P(w, c)}{P(w) P(c)} \\tag{25.42}\n\\end{equation*}\n\n\n其中, $P(w, c)$ 是 $w$ 和 $c$ 的共现概率, $P(w)$ 是 $w$ 的出现概率, $P(c)$ 是 $c$ 的出现概率。互信息的值越大，表示单词和上下文越相关。互信息 $I(w, c)$ 从共现数据计算。\n\n\n\\begin{equation*}\nI(w, c)=\\log \\frac{f(w, c) N}{f(w) f(c)} \\tag{25.43}\n\\end{equation*}\n\n\n其中, $f(w, c)$ 是 $w$ 和 $c$ 的共现频率, $f(w)$ 是 $w$ 的频率, $f(c)$ 是 $c$ 的频率, $N$ 是样本容量。\n\n所有单词和上下文的互信息减去一个常量 $\\log k$, 构成矩阵 $\\boldsymbol{M}$ :\n\n\n\\begin{equation*}\n\\boldsymbol{M}=\\left(m_{i j}\\right), \\quad m_{i j}=I\\left(w_{i}, c_{j}\\right)-\\log k \\tag{25.44}\n\\end{equation*}\n\n\n其中的 $k$ 与式 (25.41) 中的 $k$ 相同。可以证明, 对目标函数 (25.41) 的优化等价于对矩阵 $M($ 式 $(25.44))$ 的矩阵分解:\n\n\n\\begin{equation*}\n\\boldsymbol{M}=\\boldsymbol{W} \\cdot \\boldsymbol{C}^{\\mathrm{T}} \\tag{25.45}\n\\end{equation*}\n\n\n得到的矩阵 $\\boldsymbol{W}$ 的行向量就是单词的词向量。设 $\\boldsymbol{M}$ 是 $m \\times n$ 矩阵, $\\boldsymbol{W}$ 是 $m \\times l$ 矩阵, $\\boldsymbol{C}$ 是 $n \\times l$ 矩阵, 这里有 $l \\ll m, l \\ll n$ 。所以, 跳元模型得到的词向量是对单词与上下文的互信息进行压缩得到的表示。这里的矩阵分解是通过随机梯度下降得到的, 而不是奇异值分解和非负矩阵分解。详细见第 15 章的奇异值分解和第 17 章的非负矩阵分解。"
    },
    {
        "id": "chatcmpl-93wJhW5zd83WfLjn65UD2yXUDElkp",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是词向量？它在自然语言处理中有什么作用？\",\n    \"回答1\": \"词向量是指表示自然语言的单词的实数向量，也称为词嵌入或单词嵌入。它将自然语言的单词映射到实数向量空间，维度远小于单词表的大小。在自然语言处理中，词向量可以用于表示句子中的每一个单词，属于分布式表示，能够通过内积或余弦表示单词间的相似性。\",\n    \"问题2\": \"分布式表示和局部式表示有什么区别？分布式表示相比局部式表示有哪些优点？\",\n    \"回答2\": \"分布式表示是指用 $N$ 维 $0 / 1$ 向量或者 $N$ 维实数向量表示概念，其中 $\\text{log}_2 K<N \\text{且} N \\text{远远小于} K$。而局部式表示是指用 $K$ 维独热向量表示概念，每一个概念由一个 $K$ 维 $0 / 1$ 向量表示。分布式表示相比局部式表示具有容易表示相似性、表示效率高、拥有稳健性和可扩展性等优点。\",\n    \"问题3\": \"词向量的学习方法是怎样的？\",\n    \"回答3\": \"在词向量的学习方法中，通过大量的语料收集单词和上下文的共现数据，从共现数据中学习每一个单词的词向量。常用的方法是跳元模型加负采样方法，通过随机梯度下降进行优化，学习共现模型的参数向量 $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$ 。目标函数是通过共现模型预测的目标函数进行优化，学习得到参数向量 $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$。\",\n    \"问题4\": \"跳元模型中的共现模型是如何定义的？\",\n    \"回答4\": \"跳元模型中的共现模型定义了单词 $w \\text{和} c$ 共现的概率，其中 $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$ 是维度为 $l$ 的参数向量。实际上是判断共现与否的分类模型。\",\n    \"问题5\": \"跳元模型中的目标函数是如何定义的？\",\n    \"回答5\": \"跳元模型中的目标函数通过共现数据计算得到每个单词的参数向量 $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$ ，目标函数是对共现数据中出现次数 $f(w, c)$ 的负对数似然损失函数，通过随机梯度下降进行优化。\",\n    \"问题6\": \"互信息在跳元模型中起着什么作用？\",\n    \"回答6\": \"互信息在跳元模型中用于衡量单词和上下文之间的相关性，通过共现数据计算得到。互信息越大，表示单词和上下文越相关。在跳元模型中，互信息被用来构成矩阵 $\\boldsymbol{M}$ ，并进行矩阵分解以得到单词的词向量。\",\n    \"问题7\": \"为什么分布式表示相比局部式表示更容易表示相似性？\",\n    \"回答7\": \"分布式表示相比局部式表示更容易表示相似性，因为当表示是 $0 / 1$ 向量时可以用汉明距离，当表示是实数向量时可以用内积或余弦来方便地计算概念之间的相似度，使学到的模型能对相似的输入产生相似输出。\",\n    \"问题8\": \"词向量在具体学习任务中可以如何应用？\",\n    \"回答8\": \"在具体学习任务中，词向量可以作为模型的一部分同模型一起学习，也可以预先学好然后在学习中固定使用。前者适合训练数据多的情况，后者适合训练数据少的情况。词向量的预先学习通过无监督学习进行。\",\n    \"问题9\": \"词向量如何从共现数据中学习每一个单词的语义表示？\",\n    \"回答9\": \"在跳元模型中，通过大量语料收集单词和上下文的共现数据，并从共现数据中学习每一个单词的语义表示。通过共现数据计算得到每个单词的参数向量 $\\boldsymbol{w}$ 和 $\\boldsymbol{c}$ ，使得参数向量能很好地说明共现数据。\",\n    \"问题10\": \"为什么跳元模型得到的词向量是对单词与上下文的互信息进行压缩得到的表示？\",\n    \"回答10\": \"跳元模型得到的词向量是对单词与上下文的互信息进行压缩得到的表示，因为目标函数 (25.41) 的优化等价于对矩阵 $\\boldsymbol{M}$ 的矩阵分解。这种矩阵分解通过随机梯度下降得到，将互信息进行压缩得到了最终的单词表示。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710726181,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1505,
            "prompt_tokens": 5699,
            "total_tokens": 7204
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]