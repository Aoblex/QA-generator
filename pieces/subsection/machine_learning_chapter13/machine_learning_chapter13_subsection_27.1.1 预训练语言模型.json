[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter13",
        "title": "27.1.1 预训练语言模型",
        "content_length": 3019,
        "content": "Title: 27.1.1 预训练语言模型\nContent:\n在实际应用中使用的深度学习主要还是监督学习, 如在自然语言处理中的文本分类、文本序列标注。在具体的任务中需要有标注数据, 普遍规律是标注数据质量越高和数量越大,学到的模型的准确率就越高。但问题是数据的标注成本通常很高, 实际应用中往往很难获取大量的高质量标注数据。另外, 不同的任务需要不同的标注数据, 标注数据在任务之间很难通用。预训练语言模型是为解决这个问题而开发的用于自然语言处理的深度学习方法。\n\n预训练语言模型的基本想法如下: 基于神经网络, 如 Transformer 的编码器或解码器 (见第 26 章), 实现语言模型, 以计算语言的生成概率。首先使用大规模的语料通过无监督学习的方式学习模型的参数, 称为预训练 (pre-training), 得到的模型可以有效地表示自然语言的\n\n(1) 这种学习方式也被称为自监督学习 (self-supervised learning)。自监督学习并没有严格的定义, 这里仍使用无监督学习。\\\\\n特征; 之后将模型用于一个具体任务, 使用少量的标注数据通过监督学习的方式进一步学习模型的参数, 称为微调 (fine tuning), 任务称为下游任务 (downstream task)。预训练使用通用的语料统一进行, 微调使用各个下游任务的标注数据分别进行。微调 (下游任务) 的模型有时在预训练模型的基础上增加新的参数。\n\nTransformer 具有强大的语言表示能力, 大规模语料包含丰富的语言表达 (这样的无标注数据可以较容易地获取), 加之大规模深度学习的训练系统变得越来越高效, 所以学习得到的预训练语言模型可以有效地表示语言的词汇、句法和语义特征。这样, 当预训练语言模型用于下游任务时, 只需要标注少量的数据训练就可以达到很高的准确率。预训练语言模型已成为当前语言理解和语言生成的核心技术。\n\n有代表性的预训练语言模型有 GPT 和 BERT。表 27.1 比较了 GPT 和 BERT 的主要特点, 其主要区别在于模型的架构和预训练方式。\n\n表 27.1 GPT 和 BERT 的比较\n\n\\begin{center}\n\\begin{tabular}{c|c|c}\n\\hline\n & GPT & BERT \\\\\n\\hline\n语言模型类型 & 单向语言模型 & 双向语言模型 \\\\\n\\hline\n模型架构 & Transformer 解码器 & Transformer 编码器 \\\\\n\\hline\n预训练方式 & 语言模型化 & 掩码语言模型化 \\\\\n\\hline\n预训练原理 & 序列概率估计 & 去噪自动编码器 \\\\\n\\hline\n下游任务 & 语言理解、语言生成 & 语言理解 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nGPT 是单向语言模型 (unidirectional language model), 从一个方向对单词序列建模, 方向为从左到右或者从右到左, 由 Transformer 的解码器实现。假设有单词序列 $\\boldsymbol{x}=$ $x_{1}, x_{2}, \\cdots, x_{n}$, 在单词序列的各个位置上, 单向语言模型具有以下单词生成的条件概率:\n\n\n\\begin{equation*}\nP\\left(x_{i} \\mid x_{1}, x_{2}, \\cdots, x_{i-1}\\right), \\quad i=1,2, \\cdots, n \\tag{27.1}\n\\end{equation*}\n\n\n每一个位置的单词依赖于之前位置的单词。可以使用单向语言模型计算单词序列 $x=$ $x_{1}, x_{2}, \\cdots, x_{n}$ 的生成概率。\n\nBERT 是双向语言模型 (bidirectional language model), 从两个方向同时对单词序列建模, 由 Transformer 的编码器实现。在单词序列的各个位置上, 双向语言模型具有以下单词生成的条件概率:\n\n\n\\begin{equation*}\nP\\left(x_{i} \\mid x_{1}, \\cdots, x_{i-1}, x_{i+1}, \\cdots, x_{n}\\right), \\quad i=1,2, \\cdots, n \\tag{27.2}\n\\end{equation*}\n\n\n每一个位置的单词依赖于之前位置和之后位置的单词。不可以使用双向语言模型直接计算单词序列 $\\boldsymbol{x}=x_{1}, x_{2}, \\cdots, x_{n}$ 的生成概率。\n\nGPT 的预训练通过语言模型化 (language modeling) 的方式进行, 基于序列概率估计。对给定的单词序列 $\\boldsymbol{x}=x_{1}, x_{2}, \\cdots, x_{n}$, 计算以下负对数似然函数或交叉嫡, 并通过其最小化估计模型的参数。\n\n\n\\begin{equation*}\n-\\log P(\\boldsymbol{x})=-\\sum_{i=1}^{n} \\log P_{\\boldsymbol{\\theta}}\\left(x_{i} \\mid x_{1}, x_{2}, \\cdots, x_{i-1}\\right) \\tag{27.3}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{\\theta}$ 表示 GPT 模型的参数。\n\nBERT 的预训练主要通过掩码语言模型化 (mask language modeling) 的方式进行, 可以认为基于后叙去噪自动编码器。假设单词序列 $\\boldsymbol{x}=x_{1}, x_{2}, \\cdots, x_{n}$ 中有若干个单词被随机掩码, 也就是被改为特殊字符 $<\\mathrm{mask}>$, 得到掩码单词序列 $\\tilde{\\boldsymbol{x}}$, 假设被掩码的几个单词是 $\\overline{\\boldsymbol{x}}$ 。计算以下负对数似然函数, 并通过其最小化估计模型的参数。\n\n\n\\begin{equation*}\n-\\log P(\\overline{\\boldsymbol{x}} \\mid \\tilde{\\boldsymbol{x}}) \\approx-\\sum_{i=1}^{n} \\delta_{i} \\log P_{\\boldsymbol{\\theta}}\\left(x_{i} \\mid \\tilde{\\boldsymbol{x}}\\right) \\tag{27.4}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{\\theta}$ 表示 BERT 模型的参数; $\\delta_{i}$ 取值为 1 或 0 , 表示是否对位置 $i$ 的单词进行掩码处理。\n\nGPT 适合于语言生成, 也可以用于语言理解。BERT 只能用于语言理解。语言理解是指对自然语言进行分析的处理, 如文本分类、文本匹配、文本序列标注。语言生成是指产生自然语言的处理, 可以是无条件的, 也可以是有条件的, 基于语言、图像等输入, 如机器翻译、图像标题生成。"
    },
    {
        "id": "chatcmpl-93wLUeNokmBvDQOGg1LKu5Ksym0Nt",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"深度学习中的监督学习主要应用于哪些领域？\",\n    \"回答1\": \"深度学习中的监督学习主要应用于自然语言处理中的文本分类、文本序列标注等领域。在这些具体任务中，需要有标注数据来训练模型，而标注数据的质量和数量会直接影响到模型的准确率。\",\n    \"问题2\": \"为什么在实际应用中很难获取大量的高质量标注数据？\",\n    \"回答2\": \"在实际应用中很难获取大量的高质量标注数据，主要是因为数据的标注成本通常很高。另外，不同的任务需要不同的标注数据，而标注数据在任务之间很难通用，这也增加了获取大量高质量标注数据的难度。\",\n    \"问题3\": \"预训练语言模型是如何解决标注数据不足的问题的？\",\n    \"回答3\": \"预训练语言模型通过无监督学习的方式，在大规模语料上学习模型的参数，然后将模型用于具体任务时，只需要使用少量的标注数据进行微调，从而解决了标注数据不足的问题。\",\n    \"问题4\": \"GPT 和 BERT 分别属于什么类型的语言模型？\",\n    \"回答4\": \"GPT 是单向语言模型（unidirectional language model），而 BERT 是双向语言模型（bidirectional language model）。它们分别采用不同的方式对单词序列进行建模，从而具有不同的语言表示能力。\",\n    \"问题5\": \"GPT 和 BERT 的预训练方式有何区别？\",\n    \"回答5\": \"GPT 的预训练方式是通过语言模型化（language modeling）进行，基于序列概率估计。而 BERT 的预训练方式主要通过掩码语言模型化（mask language modeling）进行，基于后叙去噪自动编码器。\",\n    \"问题6\": \"GPT 和 BERT 在下游任务上有何区别？\",\n    \"回答6\": \"GPT 在下游任务上可以用于语言理解和语言生成，而 BERT 只能用于语言理解。语言理解是指对自然语言进行分析的处理，如文本分类、文本匹配、文本序列标注。而语言生成是指产生自然语言的处理，如机器翻译、图像标题生成。\",\n    \"问题7\": \"GPT 和 BERT 的模型架构分别是什么？\",\n    \"回答7\": \"GPT 的模型架构是基于 Transformer 解码器实现的，而 BERT 的模型架构是基于 Transformer 编码器实现的。它们采用不同的架构来实现单向和双向语言建模。\",\n    \"问题8\": \"为什么预训练语言模型可以有效地表示自然语言的特征？\",\n    \"回答8\": \"预训练语言模型可以有效地表示自然语言的特征，主要是因为大规模语料包含丰富的语言表达，并且大规模深度学习的训练系统变得越来越高效。这使得学习得到的预训练语言模型能够有效地表示语言的词汇、句法和语义特征。\",\n    \"问题9\": \"为什么在具体任务中只需要少量标注数据就可以达到很高准确率？\",\n    \"回答9\": \"在具体任务中只需要少量标注数据就可以达到很高准确率，主要是因为预训练语言模型已经通过大规模无监督学习学习到了丰富的语言特征和知识，使得在下游任务中只需要少量标注数据就能够快速适应并取得较高准确率。\",\n    \"问题10\": \"为什么预训练语言模型成为当前语言理解和生成的核心技术？\",\n    \"回答10\": \"预训练语言模型成为当前语言理解和生成的核心技术，主要是因为它能够通过无监督学习有效地学习到丰富的语言特征和知识，并且在下游任务中只需要少量标注数据就能够取得较高准确率，从而大大提高了自然语言处理任务的效率和性能。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710726292,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1275,
            "prompt_tokens": 4643,
            "total_tokens": 5918
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]