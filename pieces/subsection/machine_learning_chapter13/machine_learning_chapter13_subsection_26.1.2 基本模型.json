[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter13",
        "title": "26.1.2 基本模型",
        "content_length": 1769,
        "content": "Title: 26.1.2 基本模型\nContent:\n基本模型的编码器和解码器是循环神经网络。编码器根据给定输入的单词序列产生其状态的序列, 并且以状态序列为中间表示序列。编码器将最终位置的中间表示传递到解码器。解码器根据得到的中间表示决定其状态的序列以及输出的单词序列。基本模型实际是一个有条件的 RNN 语言模型。RNN 通常是 LSTM 和 GRU, LSTM 和 GRU 可以更好地刻画长距离依存关系。\n\n基本模型的编码器是 RNN, 如 LSTM, 状态是\n\n\n\\begin{equation*}\n\\boldsymbol{h}_{j}=a\\left(\\boldsymbol{x}_{j}, \\boldsymbol{h}_{j-1}\\right), \\quad j=1,2, \\cdots, m \\tag{26.4}\n\\end{equation*}\n\n\n这里 $\\boldsymbol{h}_{j}$ 是当前位置的状态; $\\boldsymbol{h}_{j-1}$ 是前一个位置的状态; $\\boldsymbol{x}_{j}$ 是当前位置的输入单词的词向量; $a$ 是处理单元, 如 LSTM 单元; 假设 $\\boldsymbol{h}_{0}=\\boldsymbol{O}$ 。\n\n解码器也是 RNN, 如 LSTM, 状态是\n\n\n\\begin{equation*}\ns_{i}=a\\left(\\boldsymbol{y}_{i-1}, s_{i-1}\\right), \\quad i=1,2, \\cdots, n \\tag{26.5}\n\\end{equation*}\n\n\n这里 $s_{i}$ 是当前位置的状态; $s_{i-1}$ 是前一个位置的状态; $\\boldsymbol{y}_{i-1}$ 是前一个位置的输出单词的词向量; $a$ 是处理单元, 如 LSTM 单元。输出是\n\n\n\\begin{equation*}\n\\boldsymbol{p}_{i}=g\\left(\\boldsymbol{s}_{i}\\right), \\quad i=1,2, \\cdots, n \\tag{26.6}\n\\end{equation*}\n\n\n这里 $s_{i}$ 是当前位置的状态; $\\boldsymbol{p}_{i}$ 是当前位置的输出; $g$ 是输出层函数, 由线性变换和软最大化函数组成。 $p_{i}$ 表示的是下一个位置单词出现的条件概率。\n\n编码器将其最终状态 $\\boldsymbol{h}_{m}$ 作为整个输入单词序列的表示传递给解码器。解码器将 $\\boldsymbol{h}_{m}$ 作为解码器的初始状态 $s_{0}$, 决定其状态序列, 以及输出单词序列。\n\n\n\\begin{equation*}\ns_{0}=\\boldsymbol{h}_{m} \\tag{26.7}\n\\end{equation*}\n\n\n这意味着解码器只依赖于编码器最终位置的中间表示。\n\n图 26.2 显示基本模型的架构。图中矩形表示函数及其输出。基本模型整体是一种特殊的 RNN, 或者一种特殊的语言模型。在前面 $m$ 个和后面 $n$ 个位置都有状态, 在前面 $m$ 个位置没有输出, 在后面 $n$ 个位置有输出。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-15}\n\\end{center}\n\n图 26.2 序列到序列学习基本模型\n\n序列到序列学习可以用于机器翻译、对话生成、文本摘要等应用。图 26.3 给出用基本模型进行机器翻译的例子。机器翻译将一个语言的句子转化为另一个语言的句子, 两者语义相同。对话生成中系统针对用户发话产生回复, 两者形成一轮对话。文本摘要将一个长的文本转换为一个短的文本, 使后者概括前者的内容。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-15(1)}\n\\end{center}\n\n图 26.3 机器翻译的例子"
    },
    {
        "id": "chatcmpl-93wKJGkgJnGgUi8OsZ1kZnVf6VOSA",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"基本模型的编码器和解码器分别采用了什么类型的循环神经网络？\",\n    \"回答1\": \"基本模型的编码器和解码器都采用了循环神经网络（RNN），其中常用的 RNN 类型包括 LSTM 和 GRU。编码器根据输入的单词序列产生状态的序列，而解码器根据得到的中间表示决定其状态的序列以及输出的单词序列。\",\n    \"问题2\": \"基本模型中编码器的状态是如何计算得到的？\",\n    \"回答2\": \"基本模型中编码器的状态 $\\boldsymbol{h}_{j}$ 是通过处理单元 $a$（如 LSTM 单元）对当前位置的输入单词的词向量 $\\boldsymbol{x}_{j}$ 和前一个位置的状态 $\\boldsymbol{h}_{j-1}$ 进行计算得到的，具体公式为 $\\boldsymbol{h}_{j}=a\\big(\\boldsymbol{x}_{j}, \\boldsymbol{h}_{j-1}\\big)$。\",\n    \"问题3\": \"基本模型中解码器的输出是如何计算得到的？\",\n    \"回答3\": \"基本模型中解码器的输出 $\\boldsymbol{p}_{i}$ 是通过输出层函数 $g$ 对当前位置的状态 $s_{i}$ 进行计算得到的，具体公式为 $\\boldsymbol{p}_{i}=g\\big(\\boldsymbol{s}_{i}\\big)$。这里 $g$ 由线性变换和软最大化函数组成，表示下一个位置单词出现的条件概率。\",\n    \"问题4\": \"基本模型中解码器初始状态 $s_0$ 的计算依赖于什么？\",\n    \"回答4\": \"基本模型中解码器初始状态 $s_0$ 的计算依赖于编码器最终位置的中间表示 $\\boldsymbol{h}_{m}$，具体公式为 $s_{0}=\\boldsymbol{h}_{m}$。这意味着解码器只依赖于编码器最终位置的中间表示来确定初始状态。\",\n    \"问题5\": \"基本模型整体架构是一种特殊的什么类型模型？\",\n    \"回答5\": \"基本模型整体是一种特殊的循环神经网络（RNN）或者一种特殊的语言模型。在该模型中，前面 $m$ 个和后面 $n$ 个位置都有状态，在前面 $m$ 个位置没有输出，在后面 $n$ 个位置有输出。\",\n    \"问题6\": \"序列到序列学习可以应用于哪些领域或任务？\",\n    \"回答6\": \"序列到序列学习可以应用于机器翻译、对话生成、文本摘要等领域或任务。例如，机器翻译将一个语言的句子转化为另一个语言的句子，对话生成中系统针对用户发话产生回复，文本摘要将一个长的文本转换为一个短的文本，使后者概括前者的内容。\",\n    \"问题7\": \"为什么在序列到序列学习中需要使用循环神经网络？\",\n    \"回答7\": \"在序列到序列学习中需要使用循环神经网络，因为循环神经网络能够更好地处理序列数据，并且能够捕捉输入序列和输出序列之间复杂的依赖关系。这对于诸如机器翻译、对话生成等需要处理文本序列输入和输出的任务非常重要。\",\n    \"问题8\": \"图 26.2 中所示基本模型架构有何特点？\",\n    \"回答8\": \"图 26.2 中所示基本模型架构是一种特殊的 RNN 模型，具有前面 $m$ 个和后面 $n$ 个位置都有状态，在前面 $m$ 个位置没有输出，在后面 $n$ 个位置有输出的特点。该架构能够有效地处理输入和输出序列之间复杂的关系。\",\n    \"问题9\": \"为什么基本模型被称为有条件的 RNN 语言模型？\",\n    \"回答9\": \"基本模型被称为有条件的 RNN 语言模型，是因为它在生成输出时受到输入序列信息的条件约束。换句话说，解码器生成输出单词序列时会根据给定输入单词序列产生的状态信息进行决策，因此是一种有条件的语言模型。\",\n    \"问题10\": \"LSTM 和 GRU 在基本模型中起到了什么作用？\",\n    \"回答10\": \"LSTM 和 GRU 在基本模型中作为处理单元 $a$ 起到了关键作用，用于计算编码器和解码器中每个位置的状态。它们能够更好地刻画长距离依存关系，从而提高了基本模型对复杂语义关系的建模能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710726219,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1383,
            "prompt_tokens": 3689,
            "total_tokens": 5072
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]