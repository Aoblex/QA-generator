[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter13",
        "title": "26.2.1 注意力",
        "content_length": 2692,
        "content": "Title: 26.2.1 注意力\nContent:\n脑科学和心理学中的注意力是指人脑根据自己的意识有选择地对信息进行处理的机制。深度学习中的注意力更多的是受其启发而开发的相似或相关向量检索的计算方法。在深度学习中注意力经常被用于概念组合的表示的计算, 比如, 自然语言处理中单词组合的表示的计算。\n\n定义 26.1 (注意力) 假设有键-值数据库 (key-value store), 存储键-值对数据 $\\left\\{\\left(\\boldsymbol{k}_{1}, \\boldsymbol{v}_{1}\\right)\\right.$, $\\left.\\left(k_{2}, \\boldsymbol{v}_{2}\\right), \\cdots,\\left(\\boldsymbol{k}_{n}, \\boldsymbol{v}_{n}\\right)\\right\\}$, 其中每一个键-值对 $\\left(\\boldsymbol{k}_{i}, \\boldsymbol{v}_{i}\\right)$ 的键和值都是实数向量。另有查询 (query) $\\boldsymbol{q}$, 也是实数向量。向量 $\\boldsymbol{q}$ 和 $\\boldsymbol{k}_{i}$ 的维度相同, 向量 $\\boldsymbol{k}_{i}$ 和 $\\boldsymbol{v}_{i}$ 的维度一般也相同。考虑从键-值数据库中搜索与查询 $\\boldsymbol{q}$ 相似的键所对应的值。注意力是实现检索的一种计算方法。计算查询 $\\boldsymbol{q}$ 和各个键 $\\boldsymbol{k}_{i}$ 的归一化相似度 $\\alpha\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)$, 以归一化相似度为权重, 计算各个值 $\\boldsymbol{v}_{i}$ 的\\\\\n加权平均 $v$, 将计算结果 $v$ 作为检索结果返回。\n\n\n\\begin{equation*}\n\\boldsymbol{v}=\\sum_{i=1}^{n} \\alpha\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right) \\cdot \\boldsymbol{v}_{i} \\tag{26.8}\n\\end{equation*}\n\n\n满足\n\n$$\n\\sum_{i=1}^{n} \\alpha\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)=1\n$$\n\n图 26.4 显示注意力机制。归一化的权重称作注意力权重, 一般通过软最大化计算。\n\n\n\\begin{equation*}\n\\alpha\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)=\\frac{e\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)}{\\sum_{j=1}^{n} e\\left(\\boldsymbol{q}, \\boldsymbol{k}_{j}\\right)} \\tag{26.9}\n\\end{equation*}\n\n\n其中, $e\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)$ 是查询 $\\boldsymbol{q}$ 和键 $\\boldsymbol{k}_{i}$ 的相似度。相似度计算可以有多种方法, 包括加法注意力和乘法注意力。乘法注意力要求查询和键向量的维度相同, 而加法注意力没有这个要求。乘法注意力比加法注意力计算效率更高。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-16}\n\\end{center}\n\n图 26.4 注意力机制\n\n加法注意力使用一层神经网络计算相似度：\n\n\n\\begin{equation*}\ne\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)=\\sigma\\left(\\boldsymbol{w}^{\\mathrm{T}} \\cdot\\left[\\boldsymbol{q} ; \\boldsymbol{k}_{i}\\right]+b\\right) \\tag{26.10}\n\\end{equation*}\n\n\n其中, $\\sigma$ 是 $\\mathrm{S}$ 型函数, 输入是 $\\boldsymbol{q}$ 和 $\\boldsymbol{k}_{i}$ 的拼接, [;] 表示向量的拼接。\n\n乘法注意力使用内积或尺度变换的内积计算相似度:\n\n\n\\begin{align*}\n& e\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)=\\boldsymbol{q}^{\\mathrm{T}} \\cdot \\boldsymbol{k}_{i}  \\tag{26.11}\\\\\n& e\\left(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\right)=\\frac{\\boldsymbol{q}^{\\mathrm{T}} \\cdot \\boldsymbol{k}_{i}}{\\sqrt{d}} \\tag{26.12}\n\\end{align*}\n\n\n其中, $d$ 是向量 $\\boldsymbol{q}$ 和 $\\boldsymbol{k}_{i}$ 的维度。尺度变换保证相似度的取值在一定范围内, 避免学习时发生梯度消失。\n\n注意力将与键相似的值的组合作为检索结果, 是一种 “软的” 而不是 “硬的” 检索。对于\\\\\n一般的键-值数据库检索, 键、值、查询都是符号, 而对于注意力计算, 键、值、查询都是实数向量。极端情况下, 如果向量都是独热向量, 注意力等价于一般的键-值数据库检索。\n\n注意力的模型复杂度, 也就是参数个数, 不随键-值数据库规模的增大而增大。比如, 使用加法注意力时, 参数只有 $\\boldsymbol{w}$ 和 $b$ 。\n\n注意力是深度学习的重要手段, 因为可以通过注意力, 基于已有的表示 (查询), 有选择地搜索相似的表示 (键), 并将其对应的表示 (值) 组合起来, 从而将注意力作为产生表示的组合的基本运算。"
    },
    {
        "id": "chatcmpl-93wKbhx9R9BfTJ9ofmIlbwxXehQyk",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"注意力在深度学习中的作用是什么？\",\n    \"回答1\": \"注意力在深度学习中被用于基于已有的表示（查询），有选择地搜索相似的表示（键），并将其对应的表示（值）组合起来。通过注意力机制，可以实现对信息的有选择性处理和提取，从而将注意力作为产生表示的组合的基本运算。\",\n    \"问题2\": \"在注意力机制中，如何计算查询和键之间的归一化相似度？\",\n    \"回答2\": \"在注意力机制中，计算查询 $\\boldsymbol{q}$ 和各个键 $\\boldsymbol{k}_{i}$ 的归一化相似度 $\\text{α}\\big(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\big)$，以归一化相似度为权重，计算各个值 $\\boldsymbol{v}_{i}$ 的加权平均 $v$，将计算结果 $v$ 作为检索结果返回。归一化相似度的计算公式为 $\\text{α}\\big(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\big)=\\frac{e\\big(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\big)}{\\text{∑}_{j=1}^{n} e\\big(\\boldsymbol{q}, \\boldsymbol{k}_{j}\\big)}$。\",\n    \"问题3\": \"加法注意力和乘法注意力之间有什么区别？\",\n    \"回答3\": \"加法注意力使用一层神经网络计算相似度，公式为 $e\\big(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\big)=\\text{σ}\\big(\\boldsymbol{w}^{\\text{T}} \\text{⋅}\\big[\\boldsymbol{q} ; \\boldsymbol{k}_{i}\\big]+b\\big)$，其中 $\\text{σ}$ 是 $\\text{S}$ 型函数。而乘法注意力使用内积或尺度变换的内积计算相似度，公式为 $e\\big(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\big)=\\boldsymbol{q}^{\\text{T}} \\text{⋅} \\boldsymbol{k}_{i}$ 或 $e\\big(\\boldsymbol{q}, \\boldsymbol{k}_{i}\\big)=\\frac{\\boldsymbol{q}^{\\text{T}} \\text{⋅} \\boldsymbol{k}_{i}}{\\text{√}{d}}$。乘法注意力要求查询和键向量的维度相同，而加法注意力没有这个要求。\",\n    \"问题4\": \"为什么乘法注意力比加法注意力计算效率更高？\",\n    \"回答4\": \"乘法注意力比加法注意力计算效率更高是因为乘法注意力不需要额外的神经网络层来计算相似度，直接使用内积或尺度变换的内积进行计算。这使得乘法注意力在计算过程中更加简洁高效。\",\n    \"问题5\": \"在注意力机制中，如何保证归一化相似度的取值在一定范围内？\",\n    \"回答5\": \"在乘法注意力中，通过尺度变换的内积计算相似度时，除以 $\\text{√}{d}$ 可以保证相似度的取值在一定范围内，避免学习时发生梯度消失。这种尺度变换能够有效地控制相似度的取值范围。\",\n    \"问题6\": \"注意力机制与一般的键-值数据库检索有何区别？\",\n    \"回答6\": \"一般的键-值数据库检索是硬检索，即根据给定的键直接查找对应的值。而注意力机制是软检索，它通过计算查询与各个键的相似度，并将与键相似的值进行加权平均作为检索结果。因此，注意力机制具有更灵活、更智能的信息检索方式。\",\n    \"问题7\": \"注意力机制对于深度学习模型的模型复杂度有何影响？\",\n    \"回答7\": \"注意力机制对于深度学习模型的模型复杂度不随键-值数据库规模的增大而增大。例如，在使用加法注意力时，参数只有 $\\boldsymbol{w}$ 和 $b$。这意味着引入注意力机制不会显著增加模型的参数数量，从而降低了模型复杂度。\",\n    \"问题8\": \"在自然语言处理中，注意力机制通常被用于哪些方面？\",\n    \"回答8\": \"在自然语言处理中，注意力机制通常被用于概念组合的表示计算，比如单词组合的表示计算。通过注意力机制，可以实现对单词组合等信息的有选择性提取和表征。\",\n    \"问题9\": \"为什么说注意力是深度学习的重要手段？\",\n    \"回答9\": \"注意力是深度学习的重要手段，因为它可以通过基于已有的表示（查询），有选择地搜索相似的表示（键），并将其对应的表示（值）组合起来。这种灵活、智能的信息处理方式使得深度学习模型能够更好地适应复杂任务和数据特征。\",\n    \"问题10\": \"在图 26.4 中显示了怎样的内容？\",\n    \"回答10\": \"图 26.4 显示了一个关于注意力机制的示意图，其中展示了通过归一化权重进行软最大化计算，并将与键相似的值进行加权平均作为检索结果。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710726237,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1545,
            "prompt_tokens": 4081,
            "total_tokens": 5626
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]