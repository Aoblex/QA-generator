[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter13",
        "title": "27.1.2 模型和学习",
        "content_length": 4522,
        "content": "Title: 27.1.2 模型和学习\nContent:\n\\section*{1. 模型}\nGPT 是生成式预训练 (generative pre-training) 的缩写。GPT 的模型基于 Transformer 的解码器 (1), 是单向语言模型。GPT 的预训练就是语言模型化, 使用大规模语料基于序列概率估计原理进行模型的参数估计, 学习的目标是预测给定单词序列中的每一个单词。学习和预测都是自回归过程 (autoregressive process)。\n\nGPT 模型有以下结构。输入是单词序列 $x_{1}, x_{2}, \\cdots, x_{n}$, 可以是一个句子或一段文章。首先经过输入层, 产生初始的单词表示向量的序列, 记作矩阵 $\\boldsymbol{H}^{(0)}$ :\n\n\n\\begin{equation*}\n\\boldsymbol{H}^{(0)}=\\boldsymbol{X}+\\boldsymbol{E} \\tag{27.5}\n\\end{equation*}\n\n\n其中, 矩阵 $\\boldsymbol{X}$ 表示单词的词嵌入 (表示单词的实数向量) 的序列 $\\boldsymbol{X}=\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}, \\cdots \\boldsymbol{x}_{n}\\right)$, 矩阵 $\\boldsymbol{E}$表示单词的位置嵌入 (表示位置的实数向量) 的序列 $\\boldsymbol{E}=\\left(\\boldsymbol{e}_{1}, \\boldsymbol{e}_{2}, \\cdots \\boldsymbol{e}_{n}\\right) 。 \\boldsymbol{X}, \\boldsymbol{E}, \\boldsymbol{H}^{(0)}$ 是 $d \\times n$矩阵, 设词嵌入和位置嵌入向量的维度是 $d$ 。图 27.1 显示的是 GPT 模型输入层的计算。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-33}\n\\end{center}\n\n图 27.1 GPT 模型输入层的计算\n\n(1) 这里的 Transformer 解码器的每一层只包含自注意力子层和前馈网络子层, 不包含编码器和解码器之间的注意力子层。\\\\\n之后经过 $L$ 个解码层, 得到单词表示向量的序列, 记作矩阵 $\\boldsymbol{H}^{(L)}$ :\n\n\n\\begin{equation*}\n\\boldsymbol{H}^{(L)}=\\text { transformer_decoder }\\left(\\boldsymbol{H}^{(0)}\\right) \\tag{27.6}\n\\end{equation*}\n\n\n具体地，\n\n$$\n\\boldsymbol{H}^{(L)}=\\left(\\boldsymbol{h}_{1}^{(L)}, \\boldsymbol{h}_{2}^{(L)}, \\cdots, \\boldsymbol{h}_{n}^{(L)}\\right)\n$$\n\n其中, $\\boldsymbol{h}_{i}^{(L)}$ 是第 $i$ 个位置的单词表示向量。GPT 模型中, 在每一层, 每一个位置的表示向量是该位置的单词基于之前位置的上下文的表示 (contextualized representation)。注意: 一般的词向量是不依赖于上下文的表示 (见第 25 章)。\n\nGPT 模型的输出是在单词序列各个位置上的条件概率, 第 $i$ 个位置的单词的条件概率 $p_{i}$ 定义为\n\n\n\\begin{equation*}\nP_{\\boldsymbol{\\theta}}\\left(x_{i} \\mid x_{1}, x_{2}, \\cdots, x_{i-1}\\right)=\\operatorname{softmax}\\left(\\boldsymbol{W}_{x}^{\\mathrm{T}} \\boldsymbol{h}_{i}^{(L)}\\right)=\\frac{\\exp \\left(\\boldsymbol{w}_{x_{i}}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{i}^{(L)}\\right)}{\\sum_{x_{i}^{\\prime}} \\exp \\left(\\boldsymbol{w}_{\\boldsymbol{x}_{i}^{\\prime}}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{\\boldsymbol{i}}^{(L)}\\right)} \\tag{27.7}\n\\end{equation*}\n\n\n其中, $x_{1}, x_{2}, \\cdots, x_{i-1}$ 是之前位置的单词序列, $x_{i}$ 是当前位置的单词, $\\boldsymbol{W}_{x}$ 表示所有单词的权重矩阵, $\\boldsymbol{\\theta}$ 表示模型的参数。\n\n图 27.2 显示的是 GPT 模型的架构, 其中输入层进行式 (27.5) 的计算, 解码层整体进行式 (27.6) 的计算, 输出层进行式 (27.7) 的计算。GPT 利用 Transformer 解码器对语言的内容进行层次化的组合式的表示。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-34}\n\\end{center}\n\n图 27.2 GPT 模型的架构\n\nGPT 中解码层的多头自注意力都是单向的, 也就是各个位置的单词只针对之前所有位置的单词进行自注意力计算。\n\nGPT 模型有三个超参数: 解码层的层数 $L$ 、头的个数 $h$ 、模型的维度 $d \\circ$ 取 $L=12, h=$ $12, d=768$ 。输入单词序列的最大长度一般是 128 。\n\n\\section*{2. 预训练}\n预训练时, 估计模型的参数, 使模型对单词序列数据有准确的预测。损失函数是负对数似然函数或交叉嫡 (式 (27.3))。\n\n\n\\begin{equation*}\nL_{\\mathrm{PT}}=-\\sum_{i=1}^{n} \\log P_{\\boldsymbol{\\theta}}\\left(x_{i} \\mid x_{1}, x_{2}, \\cdots, x_{i-1}\\right) \\tag{27.8}\n\\end{equation*}\n\n\n其中, $\\theta$ 是模型的参数, 通过预训练估计得到, 作为下游任务模型的初始值。整个预训练通过 Transformer 解码器的学习进行, 包括强制教学、掩码注意力、反向传播。\n\n\\section*{3. 微调}\n微调时, 进一步调节参数, 使模型对下游任务有准确的预测。假设下游任务是文本分类,输入是单词序列 $\\boldsymbol{x}^{\\prime}=x_{1}, x_{2}, \\cdots, x_{m}$, 输出是类别 $y$, 计算条件概率 $P\\left(y \\mid x_{1}, x_{2}, \\cdots, x_{m}\\right)$ :\n\n\n\\begin{equation*}\nP_{\\boldsymbol{\\theta}, \\boldsymbol{\\phi}}\\left(y \\mid x_{1}, x_{2}, \\cdots, x_{m}\\right)=\\operatorname{softmax}\\left(\\boldsymbol{W}_{y}^{\\mathrm{T}} \\boldsymbol{h}_{m}^{(L)}\\right)=\\frac{\\exp \\left(\\boldsymbol{w}_{y}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{m}^{(L)}\\right)}{\\sum_{y^{\\prime}} \\exp \\left(\\boldsymbol{w}_{y^{\\prime}}^{\\mathrm{T}} \\cdot \\boldsymbol{h}_{m}^{(L)}\\right)} \\tag{27.9}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{h}_{m}^{(L)}$ 是第 $L$ 个解码层最后位置的单词的表示向量, $\\boldsymbol{W}_{y}$ 是类别的权重矩阵, $\\phi$ 表示分类的参数。\n\n损失函数包括两部分 ( $\\lambda$ 是系数):\n\n\n\\begin{equation*}\nL_{\\mathrm{FT}}=L_{\\mathrm{CLS}}+\\lambda \\cdot L_{\\mathrm{LM}} \\tag{27.10}\n\\end{equation*}\n\n\n一个是分类的损失函数:\n\n\n\\begin{equation*}\nL_{\\mathrm{CLS}}=-\\log P_{\\boldsymbol{\\theta}, \\boldsymbol{\\phi}}\\left(y \\mid \\boldsymbol{x}^{\\prime}\\right) \\tag{27.11}\n\\end{equation*}\n\n\n另一个是语言模型化的损失函数:\n\n\n\\begin{equation*}\nL_{\\mathrm{LM}}=-\\sum_{j=1}^{m} \\log P_{\\boldsymbol{\\theta}}\\left(x_{j} \\mid x_{1}, x_{2}, \\cdots, x_{j-1}\\right) \\tag{27.12}\n\\end{equation*}\n\n\n前者是微调的主要部分。微调中, 预训练模型的参数 $\\boldsymbol{\\theta}$ 作为初始值, 在这个过程中得到进一步学习, 同时分类的参数 $\\phi$ 也得到学习。\n\n如果下游任务是生成, 针对输入单词序列是 $x_{1}, x_{2}, \\cdots, x_{m}$, 进一步调节模型的参数, 使得模型对之有准确的预测。损失函数只有语言模型化的部分。\n\n$$\nL_{\\mathrm{FT}}=L_{\\mathrm{LM}}\n$$\n\n\\section*{4. 模型特点}\nGPT 的模型是单向语言模型, 而不是双向语言模型。学习 (预训练) 和预测的过程都是自回归的, 保证学习和预测的一致。可用于语言生成, 而用于语言理解时不具备优势。因为语\\\\\n言理解中, 输入是一个句子或一段文章, 从两个方向同时对语言建模更加合理。BERT 可以解决这个问题。"
    },
    {
        "id": "chatcmpl-93wLnQaMDMrpHKxWxWZza2blQlyva",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"GPT 模型的输入层结构是怎样的？\",\n    \"回答1\": \"GPT 模型的输入层结构包括经过输入层产生初始的单词表示向量的序列，记作矩阵 $\\boldsymbol{H}^{(0)}$，其中矩阵 $\\boldsymbol{X}$ 表示单词的词嵌入的序列，矩阵 $\\boldsymbol{E}$ 表示单词的位置嵌入的序列。这些矩阵都是 $d \\times n$ 矩阵，其中 $d$ 是词嵌入和位置嵌入向量的维度，$n$ 是单词序列的长度。\",\n    \"问题2\": \"GPT 模型中的解码层是如何计算得到单词表示向量的序列的？\",\n    \"回答2\": \"GPT 模型中经过 $L$ 个解码层，得到单词表示向量的序列 $\\boldsymbol{H}^{(L)}$。在每一层，每一个位置的表示向量是该位置的单词基于之前位置的上下文的表示。这是通过 Transformer 解码器进行计算得到的。\",\n    \"问题3\": \"GPT 模型中输出层的条件概率是如何定义的？\",\n    \"回答3\": \"GPT 模型中输出层的条件概率 $p_{i}$ 定义为 $P_{\\boldsymbol{\\theta}}\\big(x_{i} \\big| x_{1}, x_{2}, \\text{...}, x_{i-1}\\big)=\\text{softmax}\\big(\\boldsymbol{W}_{x}^{\\top} \\boldsymbol{h}_{i}^{(L)}\\big)$。其中，$x_{1}, x_{2}, \\text{...}, x_{i-1}$ 是之前位置的单词序列，$x_{i}$ 是当前位置的单词，$\\boldsymbol{W}_{x}$ 表示所有单词的权重矩阵，$\\boldsymbol{\\theta}$ 表示模型的参数。\",\n    \"问题4\": \"GPT 模型中有哪些超参数？\",\n    \"回答4\": \"GPT 模型有三个超参数: 解码层的层数 $L$、头的个数 $h$、模型的维度 $d$。通常取 $L=12, h=12, d=768$。此外，输入单词序列的最大长度一般是 128。\",\n    \"问题5\": \"预训练和微调在 GPT 模型中分别指什么？\",\n    \"回答5\": \"预训练时，估计模型的参数，使模型对单词序列数据有准确的预测。而微调时，则进一步调节参数，使模型对下游任务有准确的预测。预训练通过 Transformer 解码器进行学习，包括强制教学、掩码注意力、反向传播。\",\n    \"问题6\": \"在微调中，损失函数包括哪些部分？\",\n    \"回答6\": \"在微调中，损失函数包括两部分：分类的损失函数和语言模型化的损失函数。其中分类的损失函数用于下游任务中对类别进行预测，而语言模型化的损失函数用于进一步调节模型参数。\",\n    \"问题7\": \"GPT 模型相比双向语言模型有什么特点？\",\n    \"回答7\": \"GPT 模型是单向语言模型，学习和预测过程都是自回归的。这保证了学习和预测的一致性。它适用于语言生成，但在语言理解方面并不具备优势。相比之下，双向语言模型能够同时对语言进行双向建模，在语言理解方面更加合理。\",\n    \"问题8\": \"GPT 模型如何利用 Transformer 解码器对语言内容进行表示？\",\n    \"回答8\": \"GPT 利用 Transformer 解码器对语言内容进行层次化的组合式表示。解码层中的多头自注意力都是单向的，各个位置的单词只针对之前所有位置的单词进行自注意力计算。\",\n    \"问题9\": \"为什么 GPT 模型中输入单词序列长度一般取 128？\",\n    \"回答9\": \"GPT 模型中输入单词序列长度一般取 128 是出于对计算效率和内存消耗的考虑。较短的输入序列长度可以更好地适应当前计算资源和硬件设备，并且能够更高效地进行模型训练和推理。\",\n    \"问题10\": \"GPT 模型与 BERT 模型相比有哪些区别？\",\n    \"回答10\": \"GPT 模型是单向语言模型，适用于语言生成；而 BERT 模型是双向语言模型，能够同时对语言进行双向建模，在语言理解方面更加合理。因此，在不同应用场景下，选择合适的模型能够更好地满足任务需求。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710726311,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1399,
            "prompt_tokens": 5199,
            "total_tokens": 6598
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]