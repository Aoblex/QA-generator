[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter13",
        "title": "26.1.1 序列到序列学习",
        "content_length": 2851,
        "content": "Title: 26.1.1 序列到序列学习\nContent:\n序列到序列学习是将一个输入的单词序列转换为另一个输出的单词序列的任务, 比如一个句子到另一个句子。不失一般性, 这里用单词序列作为例子, 也可以是字的序列或者符号的序列。序列到序列模型表示的是序列到序列的映射。\n\n假设输入的单词序列是 $x_{1}, x_{2}, \\cdots, x_{m}$, 输出的单词序列是 $y_{1}, y_{2}, \\cdots, y_{n}$, 单词都来自词\\\\\n表。给定输入单词序列条件下输出单词序列的条件概率是\n\n\n\\begin{equation*}\nP\\left(y_{1}, y_{2}, \\cdots, y_{n} \\mid x_{1}, x_{2}, \\cdots, x_{m}\\right)=\\prod_{i=1}^{n} P\\left(y_{i} \\mid y_{1}, y_{2}, \\cdots, y_{i-1}, x_{1}, x_{2}, \\cdots, x_{m}\\right) \\tag{26.1}\n\\end{equation*}\n\n\n其中, $P\\left(y_{i} \\mid y_{1}, y_{2}, \\cdots, y_{i-1}, x_{1}, x_{2}, \\cdots, x_{m}\\right)$ 是输出序列第 $i$ 个位置上单词出现的条件概率。设 $P\\left(y_{1} \\mid y_{0}, x_{1}, x_{2}, \\cdots, x_{m}\\right)=P\\left(y_{1} \\mid x_{1}, x_{2}, \\cdots, x_{m}\\right)$ 。\n\n序列到序列学习是有条件的语言生成, 即在给定单词序列 $x_{1}, x_{2}, \\cdots, x_{m}$ 的条件下, 生成单词序列 $y_{1}, y_{2}, \\cdots, y_{n}$ 。模型是条件语言模型 (conditional language model), $P\\left(y_{i} \\mid y_{1}, y_{2}, \\cdots\\right.$, $\\left.y_{i-1}, x_{1}, x_{2}, \\cdots, x_{m}\\right)$, 预测给定输入序列及已生成输出序列的条件下, 下一个位置上单词出现的条件概率。\n\n序列到序列模型由编码器网络和解码器网络组成。编码器将输入单词序列 $x_{1}, x_{2}, \\cdots, x_{m}$转换成中间表示序列 $\\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\cdots, \\boldsymbol{z}_{m}$, 每一个中间表示是一个实数向量。解码器根据中间表示序列 $\\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\cdots, \\boldsymbol{z}_{m}$ 依次生成输出单词序列 $y_{1}, y_{2}, \\cdots, y_{n}$ 。前者的过程称为编码, 后者的过程称为解码。\n\n编码器网络可以写作\n\n\n\\begin{equation*}\n\\left(\\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\cdots, \\boldsymbol{z}_{m}\\right)=F\\left(x_{1}, x_{2}, \\cdots, x_{m}\\right) \\tag{26.2}\n\\end{equation*}\n\n\n其中, $x_{1}, x_{2}, \\cdots, x_{m}$ 是输入单词序列, $\\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\cdots, \\boldsymbol{z}_{m}$ 是中间表示序列。编码器定义在整体输入单词序列上。解码器网络可以写作\n\n\n\\begin{equation*}\nP\\left(y_{i} \\mid y_{1}, y_{2}, \\cdots, y_{i-1}, x_{1}, x_{2}, \\cdots, x_{m}\\right)=G\\left(y_{1}, y_{2}, \\cdots, y_{i-1}, \\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\cdots, \\boldsymbol{z}_{m}\\right) \\tag{26.3}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\cdots, \\boldsymbol{z}_{m}$ 是中间表示序列, $y_{1}, y_{2}, \\cdots, y_{i-1}$ 是已生成的输出单词序列, $P\\left(y_{i} \\mid y_{1}, y_{2}, \\cdots\\right.$, $\\left.y_{i-1}, x_{1}, x_{2}, \\cdots, x_{m}\\right)$ 是待生成的单词的条件概率。解码器定义在单词输出序列的每一个位置上。\n\n图 26.1 显示由编码器和解码器组成的序列到序列学习的框架。编码器可以 “看到” 整个输入单词序列, 而解码器只能 “看到” 已生成的输出单词序列, 不能 “看到” 待生成的输出单词序列。解码是自回归过程 (1), 编码可以是自回归过程也可以是非自回归过程。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_5ec6446031f017adfacbg-13}\n\\end{center}\n\n图 26.1 序列到序列学习框架\n\\footnotetext{(1) 最近有研究将解码过程也作为非自回归实现, 这里不予介绍。\n}\n序列到序列学习有几个特点: 编码器和解码器联合训练、反向传播、强制教学 (teacher forcing)。学习时, 训练数据的每一个样本由一个输入单词序列和一个输出单词序列组成。利用大量样本通过端到端学习的方式进行模型的参数估计, 包括编码器和解码器的参数估计。因为编码器的输出是解码器的输入, 二者连接在一起, 所以参数估计可以通过反向传播进行。与通常的语言模型学习一样, 输出序列每一个位置的单词的条件概率的学习 (式 (26.3)) 使用训练数据中之前所有位置的单词。学习是强制教学的, 可以在所有位置上并行处理。与通常的语言模型学习不同的是还有输入单词序列, 学习依赖于整个输入单词序列。\n\n序列到序列学习的预测 (生成) 通常使用束搜索 (beam search)。目标是计算给定输入单词序列条件下概率最大的输出单词序列, 束搜索用递归的方法近似计算条件概率最大的 $k$ 个输出单词序列, 其中 $k$ 为束宽。"
    },
    {
        "id": "chatcmpl-93wK3p8Gl8l54wwpyEERFKwelgeJk",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是序列到序列学习的任务？\",\n    \"回答1\": \"序列到序列学习是将一个输入的单词序列转换为另一个输出的单词序列的任务，比如将一个句子翻译成另一个句子。它可以应用于单词序列、字的序列或者符号的序列。\",\n    \"问题2\": \"在序列到序列学习中，如何表示给定输入单词序列条件下输出单词序列的条件概率？\",\n    \"回答2\": \"给定输入单词序列条件下输出单词序列的条件概率可以表示为 $P\\big(y_{1}, y_{2}, \\text{...}, y_{n} \\big| x_{1}, x_{2}, \\text{...}, x_{m}\\big)=\\bigg(\\text{连乘}_{i=1}^{n} P\\big(y_{i} \\big| y_{1}, y_{2}, \\text{...}, y_{i-1}, x_{1}, x_{2}, \\text{...}, x_{m}\\big)\\bigg)$。\",\n    \"问题3\": \"编码器网络和解码器网络分别有什么作用？\",\n    \"回答3\": \"编码器网络将输入单词序列转换成中间表示序列，而解码器网络根据中间表示序列逐步生成输出单词序列。编码过程称为编码，解码过程称为解码。\",\n    \"问题4\": \"如何表示编码器网络的过程？\",\n    \"回答4\": \"编码器网络可以表示为 $\\big(\\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\text{...}, \\boldsymbol{z}_{m}\\big)=F\\big(x_{1}, x_{2}, \\text{...}, x_{m}\\big)$，其中 $x_{1}, x_{2}, \\text{...}, x_{m}$ 是输入单词序列，$\\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\text{...}, \\boldsymbol{z}_{m}$ 是中间表示序列。\",\n    \"问题5\": \"解码器网络的条件概率计算公式是什么？\",\n    \"回答5\": \"解码器网络的条件概率计算公式为 $P\\big(y_{i} \\big| y_{1}, y_{2}, \\text{...}, y_{i-1}, x_{1}, x_{2}, \\text{...}, x_{m}\\big)=G\\big(y_{1}, y_{2}, \\text{...}, y_{i-1}, \\boldsymbol{z}_{1}, \\boldsymbol{z}_{2}, \\text{...}, \\boldsymbol{z}_{m}\\big)$。\",\n    \"问题6\": \"在图 26.1 中，编码器和解码器组成的框架有什么特点？\",\n    \"回答6\": \"在图 26.1 中，编码器可以 '看到' 整个输入单词序列，而解码器只能 '看到' 已生成的输出单词序列，不能 '看到' 待生成的输出单词序列。解码是自回归过程，而编码可以是自回归过程也可以是非自回归过程。\",\n    \"问题7\": \"在序列到序列学习中，训练数据的每一个样本由什么组成？\",\n    \"回答7\": \"在序列到序列学习中，训练数据的每一个样本由一个输入单词序列和一个输出单词序列组成。通过大量样本进行端到端学习，包括编码器和解码器的参数估计。\",\n    \"问题8\": \"在预测（生成）阶段，通常使用什么方法来计算给定输入单词序列条件下概率最大的输出单词序列？\",\n    \"回答8\": \"在预测（生成）阶段，通常使用束搜索（beam search）来计算给定输入单词序列条件下概率最大的输出单词序列。束搜索用递归的方法近似计算条件概率最大的 $k$ 个输出单词序列，其中 $k$ 为束宽。\",\n    \"问题9\": \"在序列到序列学习中，为什么需要反向传播和强制教学？\",\n    \"回答9\": \"在序列到序列学习中，编码器的输出是解码器的输入，二者连接在一起，因此参数估计可以通过反向传播进行。此外，在学习时使用强制教学，可以在所有位置上并行处理，并依赖于整个输入单词序列。\",\n    \"问题10\": \"在式 (26.1) 中，$P\\big(y_{i} \\big| y_{1}, y_{2}, \\text{...} , y_{i-1}, x_{1}, x_{2}, \\text{...} , x_{m}\\big)$ 表示什么意义？\",\n    \"回答10\": \"$P\\big(y_{i} \\big| y_{1}, y_{2}, \\text{...} , y_{i-1}, x_{1}, x_{2}, \\text{...} , x_{m}\\big)$ 表示在给定输入单词序列和已生成的输出单词序列条件下，在位置 $i$ 上生成下一个单词的条件概率。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710726203,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1395,
            "prompt_tokens": 4409,
            "total_tokens": 5804
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]