[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter12",
        "title": "24.1.5 卷积神经网络性质",
        "content_length": 2921,
        "content": "Title: 24.1.5 卷积神经网络性质\nContent:\n\\section*{1. 表示效率}\n卷积神经网络的表示和学习效率比前馈神经网络高。首先层与层之间的连接是稀疏的,因为卷积代表的是稀疏连接, 比全连接的数目大幅减少, 如图 24.15 所示。\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_fc25db59b1a18f350d07g-13}\n\n图 24.15 用卷积层代替全连接层 (见文前彩图)\n\n图中显示的是一维卷积\n\n其次同一层的卷积的参数是共享的, 卷积核在前一层的各个位置上滑动计算, 在所有位置上具有相同的参数, 这样就大幅减少了参数的数量。另外, 每一层内的卷积运算可以并行处理, 这样也可以加快学习和推理的速度。\n\n\\section*{2. 不变性}\n设 $f(\\boldsymbol{x})$ 是以 $\\boldsymbol{x}$ 为输入的函数, $\\tau(\\boldsymbol{x})$ 是对 $\\boldsymbol{x}$ 的变换, 如平移变换、旋转变换、缩放变换。如果满足以下关系,\n\n\n\\begin{equation*}\nf(\\boldsymbol{x})=f(\\tau(\\boldsymbol{x})) \\tag{24.18}\n\\end{equation*}\n\n\n则称函数 $f(\\cdot)$ 对变换 $\\tau(\\cdot)$ 具有不变性。如果 $\\tau(\\cdot)$ 表示的是平移变换、旋转变换、缩放变换, 则函数 $f(\\cdot)$ 具有平移不变性、旋转不变性、缩放不变性。\n\n卷积神经网络具有平移不变性, 但不能严格保证; 不具有旋转不变性、缩放不变性。这意味着在图像识别中, 图像中的物体平行移动位置也能被识别。在图像识别中, 往往通过数据增强的方法提高卷积神经网络的旋转不变性和缩放不变性。\n\n例 24.10 图 24.16 给出从两张图片中进行特征抽取的例子。两张图片中都包含 $\\mathrm{L}$ 字,但位置发生了平移。通过卷积和汇聚运算, 可以分别抽取出两张图片中的这个特征, 卷积使\\\\\n用表示 L 字的卷积核, 汇聚使用最大汇聚。所以, 这里的卷积和汇聚运算对特征抽取具有平移不变性。\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_fc25db59b1a18f350d07g-14}\n\n图 24.16 卷积和汇聚运算实现的特征抽取具有平移不变性\n\n下面给出三个不变性的严格定义。在平面上的点的坐标 $(x, y)$ 通过以下矩阵表示的变换变成新的坐标 $\\left(x^{\\prime}, y^{\\prime}\\right)$,则分别称变换为平移变换、旋转变换、缩放变换。\n\n(1) 平移变换\n\n$$\n\\left[\\begin{array}{c}\nx^{\\prime} \\\\\ny^{\\prime} \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n1 & 0 & t_{x} \\\\\n0 & 1 & t_{y} \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right]\n$$\n\n其中, $t_{x}$ 和 $t_{y}$ 分别表示点在 $x$ 轴和 $y$ 轴方向平移的幅度。\n\n(2) 旋转变换\n\n$$\n\\left[\\begin{array}{c}\nx^{\\prime} \\\\\ny^{\\prime} \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\cos \\theta & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right]\n$$\n\n其中, $\\theta$ 表示点围绕原点旋转的角度。\n\n(3) 缩放变换\n\n$$\n\\left[\\begin{array}{c}\nx^{\\prime} \\\\\ny^{\\prime} \\\\\n1\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\ns_{x} & 0 & 0 \\\\\n0 & s_{y} & 0 \\\\\n0 & 0 & 1\n\\end{array}\\right]\\left[\\begin{array}{l}\nx \\\\\ny \\\\\n1\n\\end{array}\\right]\n$$\n\n其中, $s_{x}$ 和 $s_{y}$ 分别表示点在 $x$ 轴和 $y$ 轴方向缩放的尺度。\n\n\\section*{3. 感受野}\n卷积神经网络利用卷积实现了图像处理需要的特征的表示。前端的神经元表示的是局部\\\\\n的特征, 如物体的轮廓; 后端的神经元表示的是全局的特征, 如物体的部件, 可以更好地对图像数据进行预测。\n\n卷积神经网络通过特殊的函数表示和学习实现了自己的感受野机制。卷积神经网络的感受野是指其神经元涵盖的输入矩阵的部分 (二维图像的区域)。图 24.17 显示的网络有两个卷积层, 输入层是二维图像。第一层的绿色神经元的感受野是输入层的绿色区域, 第二层的黄色神经元的感受野是输入层的整个区域。感受野是从神经元的输出到输入反向看过去得到的结果。卷积核加激活函数产生的感受野具有与生物视觉系统中的感受野相似的特点。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_fc25db59b1a18f350d07g-15}\n\\end{center}\n\n输入层\n\n图 24.17 卷积神经网络中的感受野 (见文前彩图)\n\n考虑卷积神经网络全部由卷积层组成的情况, 神经元的感受野的大小有以下关系成立。证明留作习题。\n\n\n\\begin{equation*}\nR^{(l)}=1+\\sum_{j=1}^{l}\\left(F^{(j)}-1\\right) \\prod_{i=0}^{j-1} S^{(i)} \\tag{24.19}\n\\end{equation*}\n\n\n设输入矩阵和卷积核都呈正方形。 $R^{(l)} \\times R^{(l)}$ 表示第 $l$ 层的神经元的感受野的大小, $F^{(j)} \\times F^{(j)}$表示第 $j$ 层的卷积核的大小, $S^{(i)}$ 表示第 $i$ 层卷积的步幅, 设 $S^{(0)}=1$ 。\n\n\\section*{24.2 卷积神经网络的学习算法}\n卷积神经网络的学习算法也是反向传播算法, 与前馈神经网络学习的反向传播算法相似, 不同点在于正向和反向传播基于卷积函数。"
    },
    {
        "id": "chatcmpl-93w0yJR6lKlpLox6iMXSH6r7PMx35",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"卷积神经网络与前馈神经网络在表示效率上有何区别？\",\n    \"回答1\": \"卷积神经网络的表示和学习效率比前馈神经网络高。首先，卷积神经网络层与层之间的连接是稀疏的，因为卷积代表的是稀疏连接，比全连接的数目大幅减少。其次，同一层的卷积的参数是共享的，这样就大幅减少了参数的数量。另外，每一层内的卷积运算可以并行处理，这也可以加快学习和推理的速度。\",\n    \"问题2\": \"什么是函数的不变性？卷积神经网络具有哪些不变性？\",\n    \"回答2\": \"函数的不变性指函数对特定变换具有稳定不变的特性。卷积神经网络具有平移不变性，但不能严格保证；不具有旋转不变性、缩放不变性。这意味着在图像识别中，图像中的物体平行移动位置也能被识别。在图像识别中，往往通过数据增强的方法提高卷积神经网络的旋转不变性和缩放不变性。\",\n    \"问题3\": \"卷积神经网络中的感受野是指什么？它与神经元的输出有何关系？\",\n    \"回答3\": \"卷积神经网络中的感受野是指其神经元涵盖的输入矩阵的部分（二维图像的区域）。感受野是从神经元的输出到输入反向看过去得到的结果。卷积核加激活函数产生的感受野具有与生物视觉系统中的感受野相似的特点。\",\n    \"问题4\": \"卷积神经网络全部由卷积层组成时，神经元的感受野大小如何计算？\",\n    \"回答4\": \"当卷积神经网络全部由卷积层组成时，神经元的感受野大小 $R^{(l)}$ 由以下关系式给出：$R^{(l)}=1+\\text{∑}_{j=1}^{l}\\big(F^{(j)}-1\\big) \\text{∏}_{i=0}^{j-1} S^{(i)}$。其中，$F^{(j)}$ 表示第 $j$ 层的卷积核大小，$S^{(i)}$ 表示第 $i$ 层卷积的步幅。\",\n    \"问题5\": \"卷积神经网络的学习算法与前馈神经网络学习算法有何异同？\",\n    \"回答5\": \"卷积神经网络的学习算法也是反向传播算法，与前馈神经网络学习的反向传播算法相似。不同点在于正向和反向传播基于卷积函数，这使得卷积神经网络更适合处理图像等二维数据。\",\n    \"问题6\": \"为什么卷积神经网络在表示效率上比前馈神经网络高？\",\n    \"回答6\": \"卷积神经网络在表示效率上比前馈神经网络高主要体现在两个方面：首先，层与层之间的连接是稀疏的，因为卷积代表的是稀疏连接，比全连接的数目大幅减少；其次，同一层的卷积参数是共享的，在所有位置上具有相同的参数，这样就大幅减少了参数的数量。\",\n    \"问题7\": \"为什么在图像识别中需要提高卷积神经网络的旋转不变性和缩放不变性？\",\n    \"回答7\": \"在图像识别中，需要提高卷积神经网络的旋转不变性和缩放不变性是因为图像中物体可能发生旋转或缩放而导致特征发生改变。通过提高旋转不变性和缩放不变性，可以使得模型更加鲁棒地识别出图像中物体的特征。\",\n    \"问题8\": \"感受野是如何影响卷积神经网络对图像数据进行预测？\",\n    \"回答8\": \"感受野影响着卷积神经网络对图像数据进行预测。前端的神经元表示局部特征，后端的神经元表示全局特征。通过特殊函数表示和学习实现了自己的感受野机制，使得卷积核加激活函数产生的感受野具有与生物视觉系统中感受野相似的特点。\",\n    \"问题9\": \"平移、旋转、缩放变换分别对应着哪些不变性？\",\n    \"回答9\": \"平移、旋转、缩放变换分别对应着平移不变性、旋转不变性、缩放不变性。如果函数对这些变换具有稳定不变的特性，则称函数具有相应的不变性。\",\n    \"问题10\": \"为什么在图像识别中往往通过数据增强方法提高卷积神经网络的旋转不变性和缩放不变性？\",\n    \"回答10\": \"在图像识别中通过数据增强方法提高卷积神经网络的旋转不变性和缩放不变性是为了使得模型更加鲁棒地识别出图像中物体特征。通过引入更多样化、更全面化的数据增强方式，可以使得模型更好地适应各种旋转和缩放情况下的图像特征。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710725020,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1660,
            "prompt_tokens": 4660,
            "total_tokens": 6320
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]