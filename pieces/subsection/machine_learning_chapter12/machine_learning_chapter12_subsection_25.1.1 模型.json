[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter12",
        "title": "25.1.1 模型",
        "content_length": 4998,
        "content": "Title: 25.1.1 模型\nContent:\n\\section*{1. 模型定义}\n考虑序列数据的预测问题。给定输入的实数向量序列 $x_{1}, x_{2}, \\cdots, \\boldsymbol{x}_{T}$; 在第 $t=$ $1,2, \\cdots, T$ 个位置上 ${ }^{1}$, 对实数向量 $\\boldsymbol{x}_{t}$ 进行预测, 给出概率分布 $\\boldsymbol{p}_{t}$; 整体产生输出的概率向量序列 $\\boldsymbol{p}_{1}, \\boldsymbol{p}_{2}, \\cdots, \\boldsymbol{p}_{T}$ 。\n\n一个朴素的方法是用前馈神经网络完成这个任务。假设序列数据的长度固定, 将输入的\n\\footnotetext{(1) 当数据是时间序列数据时称为第 $t=1,2, \\cdots, T$ 个时刻。\n}\n实数向量序列拼接, 作为前馈神经网络的输入, 将输出的概率向量序列拼接, 作为前馈神经网络的输出。这个方法在序列数据预测, 特别是时间序列预测上存在两个问题。一个是序列长度问题。序列数据的长度通常是可变的, 而前馈神经网络的输入层的宽度是固定的, 需要对数据进行截断或补齐处理 (比如用 $O$ 向量补齐)。但无论如何, 不易处理任意长度的序列数据。另一个是局部特征的表示和学习问题。序列数据通常在不同位置上有相似的局部特征,而前馈神经网络对不同位置的局部特征是分开表示和学习的, 产生冗余, 会降低表示和学习的效率。\n\n循环神经网络的基本想法是: 在序列数据的每一个位置上重复使用相同的前馈神经网络, 并将相邻位置的神经网络连接起来; 用前馈神经网络隐层的输出表示当前位置的 “状态”,假设当前位置的状态依赖于当前位置的输入和之前位置的状态。这样就可以表示和学习序列数据中的局部和全局特征, 并解决以上两个问题。\n\n循环神经网络的基本模型是简单循环神经网络 (simple recurrent neural network, S-RNN)。下面给出定义。\n\n定义 25.1 (简单循环神经网络) 称以下的神经网络为简单循环神经网络。神经网络以序列数据 $x_{1}, x_{2}, \\cdots, x_{T}$ 为输入, 每一项是一个实数向量。在每一个位置上重复使用同一个神经网络结构。在第 $t$ 个位置上 $(t=1,2, \\cdots, T)$, 神经网络的隐层或中间层以 $\\boldsymbol{x}_{\\boldsymbol{t}}$ 和 $\\boldsymbol{h}_{t-1}$ 为输入, 以 $\\boldsymbol{h}_{t}$ 为输出，其间有以下关系成立:\n\n\n\\begin{equation*}\n\\boldsymbol{h}_{t}=\\tanh \\left(\\boldsymbol{U} \\cdot \\boldsymbol{h}_{t-1}+\\boldsymbol{W} \\cdot \\boldsymbol{x}_{t}+\\boldsymbol{b}\\right) \\tag{25.1}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{x}_{t}$ 表示第 $t$ 个位置上的输入, 是一个实数向量 $\\left(x_{t, 1}, x_{t, 2}, \\cdots, x_{t, n}\\right)^{\\mathrm{T}} ; \\boldsymbol{h}_{t-1}$ 表示第 $t-1$个位置的状态, 也是一个实数向量 $\\left(h_{t-1,1}, h_{t-1,2}, \\cdots, h_{t-1, m}\\right)^{\\mathrm{T}} ; \\boldsymbol{h}_{t}$ 表示第 $t$ 个位置的状态 $\\left(h_{t, 1}, h_{t, 2}, \\cdots, h_{t, m}\\right)^{\\mathrm{T}}$, 也是一个实数向量; $\\boldsymbol{U}, \\boldsymbol{W}$ 是权重矩阵; $\\boldsymbol{b}$ 是偏置向量。神经网络的输出层以 $h_{t}$ 为输入, $\\boldsymbol{p}_{t}$ 为输出, 有以下关系成立:\n\n\n\\begin{equation*}\n\\boldsymbol{p}_{t}=\\operatorname{softmax}\\left(\\boldsymbol{V} \\cdot \\boldsymbol{h}_{t}+\\boldsymbol{c}\\right) \\tag{25.2}\n\\end{equation*}\n\n\n其中, $\\boldsymbol{p}_{t}$ 表示第 $t$ 个位置上的输出, 是一个概率向量 $\\left(p_{t, 1}, p_{t, 2}, \\cdots, p_{t, l}\\right)^{\\mathrm{T}}$, 满足 $p_{t, i} \\geqslant 0$ $(i=1,2, \\cdots, l), \\sum_{i=1}^{l} p_{t, i}=1 ; \\boldsymbol{V}$ 是权重矩阵; $\\boldsymbol{c}$ 是偏置向量。神经网络输出序列数据 $\\boldsymbol{p}_{1}, \\boldsymbol{p}_{2}, \\cdots, \\boldsymbol{p}_{T}$, 每一项是一个概率向量。\n\n以上公式还可以写作\n\n\n\\begin{gather*}\n\\boldsymbol{r}_{\\boldsymbol{t}}=\\boldsymbol{U} \\cdot \\boldsymbol{h}_{t-1}+\\boldsymbol{W} \\cdot \\boldsymbol{x}_{t}+\\boldsymbol{b}  \\tag{25.3}\\\\\n\\boldsymbol{h}_{t}=\\tanh \\left(\\boldsymbol{r}_{\\boldsymbol{t}}\\right)  \\tag{25.4}\\\\\n\\boldsymbol{z}_{t}=\\boldsymbol{V} \\cdot \\boldsymbol{h}_{t}+\\boldsymbol{c}  \\tag{https://cdn.mathpix.com/cropped/2024_03_13_fc25db59b1a18f350d07g-31.jpg?height=164&width=302&top_left_y=7257&top_left_x=5219}\\\\\n\\boldsymbol{p}_{t}=\\operatorname{softmax}\\left(\\boldsymbol{z}_{t}\\right) \\tag{25.6}\n\\end{gather*}\n\n\n其中, $\\boldsymbol{r}_{\\boldsymbol{t}}$ 是隐层的净输入向量, $z_{t}$ 是输出层的净输入向量。隐层的激活函数通常是双曲正切函数, 也可以是其他激活函数; 输出层的激活函数通常是软最大化函数 (见第 23 章)。\\\\\n这里神经网络在每一个位置都产生输出, 也可以只在最后一个位置产生输出, 是一种特殊情况。通常假定 $h_{0}=\\boldsymbol{O}$, 即 $\\boldsymbol{h}_{0}$ 是每一个元素为 0 的向量。\n\n图 25.1 给出简单循环神经网络 S-RNN 的架构。可以看作是在序列数据上展开的 (unfolded) 前馈神经网络, 其中参数在各个位置共享。图 25.2 给出 S-RNN 的折叠 (folded)形式。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_fc25db59b1a18f350d07g-32(1)}\n\\end{center}\n\n图 25.1 简单循环神经网络架构 (展开形式)\n\n偏置向量被省去\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_fc25db59b1a18f350d07g-32}\n\\end{center}\n\n图 25.2 简单循环神经网络架构 (折叠形式)\n\n偏置向量被省去\n\n\\section*{2. 模型特点}\n循环神经网络 (1)的定义中不仅涉及输入的空间和输出的空间, 而且涉及状态的空间, 并且状态的空间起着重要作用。这一点与一般的前馈神经网络不同。S-RNN 对依次给定的输入的实数向量序列 $\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}, \\cdots, \\boldsymbol{x}_{T}$, 首先依次生成状态的实数向量序列 $\\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\cdots, \\boldsymbol{h}_{T}$, 然后再依次生成输出的概率向量序列 $\\boldsymbol{p}_{1}, \\boldsymbol{p}_{2}, \\cdots, \\boldsymbol{p}_{T}$ 。在这个过程中, 起核心作用的是式 (25.1) 的非线性变换, 意味着当前位置的状态 $\\boldsymbol{h}_{t}$ 由当前位置的输入 $\\boldsymbol{x}_{t}$ 和之前位置的状态 $\\boldsymbol{h}_{t-1}$ 决定。按顺序反向递归, 可以看出每一个位置的状态表示的是到这个位置为止的序列数据的局部特征及全局特征, 也称作短距离依存关系和长距离依存关系。\n\n循环神经网络是自回归模型 (auto-regressive model), 也就是说, 在序列数据的每一个位置上的预测只使用之前位置的信息, 适用于时间序列的预测。循环神经网络的计算需要在序列数据上依次进行。循环神经网络的优点是可以处理任意长度的序列数据, 缺点是不能进行并行化处理以提高计算效率。\n\n循环神经网络具有强大的表示能力, 是动态系统 (dynamical system) 的通用模型。自然界和人工界随时间变化的动态系统有如下基本形式。\n\n(1) 这里讨论一般的循环神经网络, Jordan 类型的循环神经网络 (见习题 25.1) 有不同的特点。\n\n$$\n\\begin{gathered}\n\\boldsymbol{s}(t)=F(\\boldsymbol{x}(t), \\boldsymbol{s}(t-1)) \\\\\n\\boldsymbol{y}(t)=G(\\boldsymbol{s}(t))\n\\end{gathered}\n$$\n\n这里 $\\boldsymbol{s}(t)$ 表示系统在时刻 $t$ 的状态, $\\boldsymbol{x}(t)$ 表示系统在时刻 $t$ 的输入, $\\boldsymbol{s}(t-1)$ 表示系统在时刻 $(t-1)$ 的状态, $\\boldsymbol{y}(t)$ 表示系统在时刻 $t$ 的输出, $F(\\cdot)$ 表示系统的状态函数, $G(\\cdot)$ 表示系统的输出函数。也就是说, 系统的当前状态由当前的输入和之前的状态决定, 而系统当前的输出由当前的状态决定。如果系统的初始状态以及在每一个时刻的输入依次确定, 那么系统的每一个时刻的状态也就依次确定, 每一个时刻的输出也依次确定。整个过程是一个确定性的过程, 由状态函数和输出函数决定。动态系统的核心是状态之间按时间顺序的依存关系。可以看出, 简单循环神经网络 S-RNN 是描述动态系统的非线性模型 (见式 (25.1) 和式 (25.2))。\n\n循环神经网络也是计算的通用模型, 可以模拟图灵机。理论证明, 图灵机可计算的任何函数都可以通过有限规模的循环网络来计算 ${ }^{[9]}$ 。也就是说, 循环神经网络可以计算任意可计算的函数, 这里说的函数可计算是指丘奇-图灵论题定义的函数可计算。"
    },
    {
        "id": "chatcmpl-93w1ZUG6GnoYxIIdwc28bnIjxazmc",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"前馈神经网络在序列数据预测中存在哪些问题？\",\n    \"回答1\": \"前馈神经网络在序列数据预测中存在两个问题。首先是序列长度问题，因为序列数据的长度通常是可变的，而前馈神经网络的输入层的宽度是固定的，需要对数据进行截断或补齐处理。其次是局部特征的表示和学习问题，因为序列数据通常在不同位置上有相似的局部特征，而前馈神经网络对不同位置的局部特征是分开表示和学习的，导致冗余并降低了表示和学习的效率。\",\n    \"问题2\": \"简单循环神经网络 (S-RNN) 的基本模型是如何定义的？\",\n    \"回答2\": \"简单循环神经网络 (S-RNN) 的基本模型定义包括两个关键公式。首先，在每一个位置上，通过非线性变换得到当前位置的状态 $\\boldsymbol{h}_{t}$，其中 $\\boldsymbol{h}_{t}$ 由当前位置的输入 $\\boldsymbol{x}_{t}$ 和之前位置的状态 $\\boldsymbol{h}_{t-1}$ 决定。其次，在输出层，通过 softmax 函数得到当前位置的输出概率向量 $\\boldsymbol{p}_{t}$。\",\n    \"问题3\": \"简单循环神经网络 (S-RNN) 的计算过程具有什么特点？\",\n    \"回答3\": \"简单循环神经网络 (S-RNN) 是自回归模型，即在序列数据的每一个位置上的预测只使用之前位置的信息，适用于时间序列的预测。其计算需要在序列数据上依次进行，无法进行并行化处理以提高计算效率。\",\n    \"问题4\": \"简单循环神经网络 (S-RNN) 在描述动态系统方面有何特点？\",\n    \"回答4\": \"简单循环神经网络 (S-RNN) 描述动态系统的非线性模型，其中核心是状态之间按时间顺序的依存关系。每一个位置的状态表示了到这个位置为止的序列数据的局部特征及全局特征，也称作短距离依存关系和长距离依存关系。\",\n    \"问题5\": \"简单循环神经网络 (S-RNN) 与一般前馈神经网络有何不同之处？\",\n    \"回答5\": \"简单循环神经网络 (S-RNN) 不仅涉及输入的空间和输出的空间，还涉及状态的空间，并且状态的空间起着重要作用。与一般前馈神经网络不同，S-RNN 对依次给定的输入实数向量序列首先生成状态实数向量序列，然后再生成输出概率向量序列。在这个过程中，当前位置的状态由当前的输入和之前的状态决定。\",\n    \"问题6\": \"简单循环神经网络 (S-RNN) 的输出层采用了什么激活函数？\",\n    \"回答6\": \"简单循环神经网络 (S-RNN) 的输出层采用 softmax 函数作为激活函数，用于得到当前位置的输出概率向量 $\\boldsymbol{p}_{t}$。\",\n    \"问题7\": \"简单循环神经网络 (S-RNN) 的优点和缺点分别是什么？\",\n    \"回答7\": \"简单循环神经网络 (S-RNN) 的优点是可以处理任意长度的序列数据，并具有强大的表示能力，是动态系统的通用模型。然而，其缺点是无法进行并行化处理以提高计算效率。\",\n    \"问题8\": \"简单循环神经网络 (S-RNN) 与 Jordan 类型的循环神经网络有何不同之处？\",\n    \"回答8\": \"简单循环神经网络 (S-RNN) 与 Jordan 类型的循环神经网络在结构和特点上有所不同。Jordan 类型的循环神经网络具有不同的状态更新方式和输出方式，并且具有不同的动态系统描述形式。\",\n    \"问题9\": \"简单循环神经网络 (S-RNN) 可以模拟哪种计算模型？\",\n    \"回答9\": \"理论证明显示，简单循环神经网络 (S-RNN) 可以模拟图灵机。也就是说，S-RNN 可以计算任意可计算的函数，这里说的函数可计算是指丘奇-图灵论题定义的函数可计算。\",\n    \"问题10\": \"简单循环神经网络 (S-RNN) 在序列数据预测中如何解决了前馈神经网络存在的问题？\",\n    \"回答10\": \"简单循环神经网络 (S-RNN) 通过重复使用相同的前馈神经网络结构，并将相邻位置的神经网络连接起来，在每一个位置上生成状态实数向量序列，并解决了前馈神经网络存在的序列长度问题和局部特征表示和学习问题。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710725057,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1432,
            "prompt_tokens": 5749,
            "total_tokens": 7181
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]