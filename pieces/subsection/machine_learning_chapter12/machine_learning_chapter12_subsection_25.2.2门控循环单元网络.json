[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter12",
        "title": "25.2.2门控循环单元网络",
        "content_length": 1960,
        "content": "Title: 25.2.2门控循环单元网络\nContent:\n\\section*{1. 模型定义}\n门控循环单元 (GRU) 是对 LSTM 进行简化得到的模型。效果相当, 但计算效率更高。GRU 有两个门一一更新门 (update gate) 和重置门 (reset gate), 不使用记忆元。\n\n定义 25.3 (门控循环单元) 以下的循环神经网络称为门控循环单元网络。在循环网络的每一个位置上有状态及重置门、更新门, 构成一个单元。第 $t$ 个位置上 $(t=1,2, \\cdots, T)$ 的单元是以当前位置的输入 $\\boldsymbol{x}_{t}$, 之前位置的状态 $h_{t-1}$ 为输入, 以当前位置的状态 $h_{t}$ 为输出的函数，按以下方式计算。\n\n\n\\begin{gather*}\n\\boldsymbol{r}_{t}=\\sigma\\left(\\boldsymbol{U}_{r} \\cdot \\boldsymbol{h}_{t-1}+\\boldsymbol{W}_{r} \\cdot \\boldsymbol{x}_{t}+\\boldsymbol{b}_{r}\\right)  \\tag{25.27}\\\\\n\\boldsymbol{z}_{t}=\\sigma\\left(\\boldsymbol{U}_{z} \\cdot \\boldsymbol{h}_{t-1}+\\boldsymbol{W}_{z} \\cdot \\boldsymbol{x}_{t}+\\boldsymbol{b}_{z}\\right)  \\tag{25.28}\\\\\n\\tilde{\\boldsymbol{h}}_{t}=\\tanh \\left(\\boldsymbol{U}_{h} \\cdot \\boldsymbol{r}_{t} \\odot \\boldsymbol{h}_{t-1}+\\boldsymbol{W}_{h} \\cdot \\boldsymbol{x}_{t}+\\boldsymbol{b}_{h}\\right)  \\tag{25.29}\\\\\n\\boldsymbol{h}_{t}=\\left(\\boldsymbol{1}-\\boldsymbol{z}_{t}\\right) \\odot \\tilde{\\boldsymbol{h}}_{t}+\\boldsymbol{z}_{t} \\odot \\boldsymbol{h}_{t-1} \\tag{25.30}\n\\end{gather*}\n\n\n这里 $\\boldsymbol{r}_{t}$ 是重置门, $z_{t}$ 是更新门, $\\tilde{h}_{t}$ 是中间结果。状态 $\\boldsymbol{h}_{t}$ 、重置门 $i_{t}$ 、更新门 $\\boldsymbol{f}_{t}$ 都是向量, 其维度相同。\n\n图 25.7 是 GRU 网络的架构图。整体是一个循环神经网络, 在每一个位置有输入 $\\boldsymbol{x}_{t}$ 、状态 $\\boldsymbol{h}_{t}$ 、输出 $\\boldsymbol{p}_{t}$ 。状态信息在 GRU 单元之间传递。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_fc25db59b1a18f350d07g-40}\n\\end{center}\n\n图 25.7 GRU 网络的架构图\n\n图 25.8 显示的是 GRU 单元的结构。第 $t$ 个位置上单元的输入是当前位置的数据 $\\boldsymbol{x}_{t}$ 、之前位置的状态 $\\boldsymbol{h}_{t-1}$, 输出是当前位置的状态 $\\boldsymbol{h}_{t}$ 。重置门 $\\boldsymbol{r}_{t}$ 、更新门 $\\boldsymbol{z}_{t}$ 有相同的结构, 都是以当前位置的输入 $\\boldsymbol{x}_{t}$ 和之前位置的状态 $\\boldsymbol{h}_{t-1}$ 为输入的函数 (式 (25.27) 和式 (25.28))。首先计算中间结果 $\\tilde{\\boldsymbol{h}}_{t}$, 是以当前位置的输入 $\\boldsymbol{x}_{t}$ 、重置门 $\\boldsymbol{r}_{t}$ 为系数的之前位置的状态 $\\boldsymbol{h}_{t-1}$ 为输入的函数, 其中系数计算是向量的逐元素积 (式 (25.29))。然后计算当前位置的状态 $\\boldsymbol{h}_{t}$, 是中间\n\n\n\\end{document}"
    },
    {
        "id": "chatcmpl-93w4TyzVvcIPTtXmwSyCg5TN3E9o5",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"门控循环单元 (GRU) 和 LSTM 之间有什么关系？\",\n    \"回答1\": \"门控循环单元 (GRU) 是对 LSTM 进行简化得到的模型。虽然效果相当，但 GRU 的计算效率更高。GRU 与 LSTM 都属于循环神经网络的一种，但在结构和计算效率上有所不同。\",\n    \"问题2\": \"门控循环单元网络中的重置门和更新门分别起到什么作用？\",\n    \"回答2\": \"在门控循环单元网络中，重置门 $\\boldsymbol{r}_{t}$ 的作用是控制如何将当前位置的输入 $\\boldsymbol{x}_{t}$ 和之前位置的状态 $\\boldsymbol{h}_{t-1}$ 结合起来计算中间结果 $\\tilde{\\boldsymbol{h}}_{t}$；更新门 $\\boldsymbol{z}_{t}$ 的作用是控制如何将中间结果 $\\tilde{\\boldsymbol{h}}_{t}$ 与之前位置的状态 $\\boldsymbol{h}_{t-1}$ 结合得到当前位置的状态 $\\boldsymbol{h}_{t}$。\",\n    \"问题3\": \"门控循环单元网络中的状态 $\\boldsymbol{h}_{t}$ 是如何计算得到的？\",\n    \"回答3\": \"在门控循环单元网络中，状态 $\\boldsymbol{h}_{t}$ 的计算涉及重置门 $\\boldsymbol{r}_{t}$、更新门 $\\boldsymbol{z}_{t}$ 和中间结果 $\\tilde{\\boldsymbol{h}}_{t}$。具体计算方式为 $\\boldsymbol{h}_{t}=\\big(\\boldsymbol{1}-\\boldsymbol{z}_{t}\\big) \\bigodot \\tilde{\\boldsymbol{h}}_{t}+\\boldsymbol{z}_{t} \\bigodot \\boldsymbol{h}_{t-1}$。\",\n    \"问题4\": \"门控循环单元网络中的重置门和更新门是如何计算得到的？\",\n    \"回答4\": \"在门控循环单元网络中，重置门 $\\boldsymbol{r}_{t}$ 和更新门 $\\boldsymbol{z}_{t}$ 的计算都涉及当前位置的输入 $\\boldsymbol{x}_{t}$、之前位置的状态 $\\boldsymbol{h}_{t-1}$，以及相应的权重矩阵和偏置项。具体计算方式分别为 $\\boldsymbol{r}_{t}=\\text{sigmoid}\\big(\\boldsymbol{U}_{r} \\boldsymbol{h}_{t-1}+\\boldsymbol{W}_{r} \\boldsymbol{x}_{t}+\\boldsymbol{b}_{r}\\big)$ 和 $\\boldsymbol{z}_{t}=\\text{sigmoid}\\big(\\boldsymbol{U}_{z} \\boldsymbol{h}_{t-1}+\\boldsymbol{W}_{z} \\boldsymbol{x}_{t}+\\boldsymbol{b}_{z}\\big)$。\",\n    \"问题5\": \"门控循环单元网络相比传统循环神经网络有哪些优势？\",\n    \"回答5\": \"门控循环单元 (GRU) 相比传统循环神经网络具有计算效率更高的优势。它通过引入更新门和重置门，能够更好地控制信息的流动和捕捉长距离依赖关系，从而在保持模型性能的同时减少了参数量和计算量。\",\n    \"问题6\": \"图 25.7 中展示了什么内容？\",\n    \"回答6\": \"图 25.7 展示了 GRU 网络的架构图。整体是一个循环神经网络，在每一个位置有输入 $\\boldsymbol{x}_{t}$、状态 $\\boldsymbol{h}_{t}$、输出 $\\boldsymbol{p}_{t}$。状态信息在 GRU 单元之间传递，展示了 GRU 网络的整体结构。\",\n    \"问题7\": \"图 25.8 中展示了什么内容？\",\n    \"回答7\": \"图 25.8 展示了 GRU 单元的结构。它说明了第 $t$ 个位置上单元的输入、输出以及重置门和更新门的结构和作用。通过图 25.8 可以更直观地理解 GRU 单元内部各部分之间的关系和计算过程。\",\n    \"问题8\": \"门控循环单元网络中的中间结果 $\\tilde{\\boldsymbol{h}}_{t}$ 是如何计算得到的？\",\n    \"回答8\": \"在门控循环单元网络中，中间结果 $\\tilde{\\boldsymbol{h}}_{t}$ 的计算涉及重置门 $\\boldsymbol{r}_{t}$、之前位置的状态 $\\boldsymbol{h}_{t-1}$ 和当前位置的输入 $\\boldsymbol{x}_{t}$。具体计算方式为 $\\tilde{\\boldsymbol{h}}_{t}=\\text{tanh}\\big(\\boldsymbol{U}_{h} \\boldsymbol{r}_{t} \\bigodot \\boldsymbol{h}_{t-1}+\\boldsymbol{W}_{h} \\boldsymbol{x}_{t}+\\boldsymbol{b}_{h}\\big)$。\",\n    \"问题9\": \"门控循环单元网络中的状态信息是如何在 GRU 单元之间传递的？\",\n    \"回答9\": \"在门控循环单元网络中，状态信息是通过 GRU 单元之间相互传递实现的。每个位置上的状态信息会根据重置门和更新门进行调控，然后传递给下一个位置上的 GRU 单元，从而实现状态信息在整个网络中的传递和更新。\",\n    \"问题10\": \"为什么称 GRU 是对 LSTM 进行简化得到的模型？\",\n    \"回答10\": \"GRU 是对 LSTM 进行简化得到的模型，主要体现在 GRU 取消了 LSTM 中的记忆元（cell state），并引入了更新门和重置门来更有效地捕捉长距离依赖关系。这种简化使得 GRU 在保持模型性能的同时减少了参数量和计算量，提高了计算效率。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710725237,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1613,
            "prompt_tokens": 3592,
            "total_tokens": 5205
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]