[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter12",
        "title": "24.1.4 卷积神经网络",
        "content_length": 5520,
        "content": "Title: 24.1.4 卷积神经网络\nContent:\n\\section*{1. 模型定义}\n卷积神经网络是包含卷积运算的一种特殊前馈神经网络。卷积神经网络一般由卷积层、汇聚层和全连接层构成。卷积神经网络架构如图 24.11 所示。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_fc25db59b1a18f350d07g-10}\n\\end{center}\n\n图 24.11 卷积神经网络架构\n\n\\section*{定义 24.3 (卷积神经网络)}\n卷积神经网络是具有以下特点的神经网络。输入是张量表示的数据, 输出是标量, 表示分类或回归的预测值。经过多个卷积层, 有时中间经过汇聚层, 最后经过全连接层。每层的输入是张量 (包括矩阵) 表示的特征图, 输出也是张量 (包括矩阵) 表示的特征图。\n\n卷积层进行基于卷积函数的仿射变换和基于激活函数的非线性变换。假设第 $l$ 层是卷积层, 则第 $l$ 层的计算如下:\n\n\n\\begin{gather*}\n\\boldsymbol{Z}^{(l)}=\\boldsymbol{W}^{(l)} * \\boldsymbol{X}^{(l-1)}+\\boldsymbol{b}^{(l)}  \\tag{24.10}\\\\\n\\boldsymbol{X}^{(l)}=a\\left(\\boldsymbol{Z}^{(l)}\\right) \\tag{24.11}\n\\end{gather*}\n\n\n这里 $\\boldsymbol{X}^{(l-1)}$ 是输入的 $I \\times J \\times K$ 张量, $\\boldsymbol{X}^{(l)}$ 是输出的 $I^{\\prime} \\times J^{\\prime} \\times K^{\\prime}$ 张量, $\\boldsymbol{W}^{(l)}$ 是卷积核的 $M \\times N \\times K \\times K^{\\prime}$ 张量, $\\boldsymbol{b}^{(l)}$ 是偏置的 $I^{\\prime} \\times J^{\\prime} \\times K^{\\prime}$ 张量, $\\boldsymbol{Z}^{(l)}$ 是净输入的 $I^{\\prime} \\times J^{\\prime} \\times K^{\\prime}$ 张量, $a(\\cdot)$ 是激活函数。可以认为式 (24.10) 和式 (24.11) 表示的变换由一组函数决定, 也就是第 $l$ 层的神经元, 一个神经元对应输出张量 $\\boldsymbol{X}^{(l)}$ 的一个元素。另一方面, 输入张量 $\\boldsymbol{X}^{(l-1)}$也就是第 $l-1$ 层的输出张量由第 $l-1$ 层的神经元决定。当 $\\boldsymbol{X}^{(l-1)}$ 的元素到 $\\boldsymbol{X}^{(l)}$ 的元素之间存在映射关系时，对应的神经元之间存在连接。\n\n汇聚层进行汇聚运算。假设第 $l$ 层是汇聚层, 则第 $l$ 层的计算如下:\n\n\n\\begin{equation*}\n\\boldsymbol{X}^{(l)}=\\operatorname{pooling}\\left(\\boldsymbol{X}^{(l-1)}\\right) \\tag{24.12}\n\\end{equation*}\n\n\n这里 $\\boldsymbol{X}^{(l-1)}$ 是输入的 $I \\times J \\times K$ 张量, $\\boldsymbol{X}^{(l)}$ 是输出的 $I^{\\prime} \\times J^{\\prime} \\times K$ 张量, $\\operatorname{pooling}(\\cdot)$ 是汇聚运算。\\\\\n可以认为式 (24.12) 表示的是基于神经元的变换（汇聚加恒等）。输入张量 $\\boldsymbol{X}^{(l-1)}$ 由第 $l-1$ 层的神经元决定, 输出张量 $\\boldsymbol{X}^{(l)}$ 由第 $l$ 层的神经元决定。当 $\\boldsymbol{X}^{(l-1)}$ 的元素到 $\\boldsymbol{X}^{(l)}$ 的元素之间存在映射关系时, 对应的神经元之间存在连接。\n\n全连接的第 $l$ 层是前馈神经网络的一层, 进行仿射变换和非线性变换。\n\n\n\\begin{gather*}\n\\boldsymbol{z}^{(l)}=\\boldsymbol{W}^{(l)} \\boldsymbol{x}^{(l-1)}+\\boldsymbol{b}^{(l)}  \\tag{24.13}\\\\\n\\boldsymbol{x}^{(l)}=a\\left(\\boldsymbol{z}^{(l)}\\right) \\tag{24.14}\n\\end{gather*}\n\n\n这里 $\\boldsymbol{x}^{(l-1)}$ 是 $N$ 维输入向量, 是由张量展开得到的; $\\boldsymbol{x}^{(l)}$ 是 $M$ 维输出向量; $\\boldsymbol{W}^{(l)}$ 是 $M \\times N$权重矩阵; $\\boldsymbol{b}^{(l)}$ 是 $M$ 维偏置向量; $\\boldsymbol{z}^{(l)}$ 是 $M$ 维净输入向量; $a(\\cdot)$ 是激活函数。全连接的最后一层输出的是标量。\n\n卷积神经网络中的所有参数, 包括卷积核的权重和偏置、全连接的权重和偏置, 都通过学习获得。\n\n卷积神经网络也可以只有卷积层和全连接层, 而没有汇聚层。步幅大于 1 的卷积运算也可以起到下采样作用, 以代替汇聚运算。为了达到更好的预测效果, 设计上的原则通常是使用更小的卷积核 (如 $3 \\times 3$ ) 和更深的结构, 前端使用少量的卷积核, 后端使用大量的卷积核。\n\n可以将 $I \\times J \\times K$ 张量展开成 $K$ 个 $I \\times J$ 矩阵, 将 $I^{\\prime} \\times J^{\\prime} \\times K^{\\prime}$ 张量展开成 $K^{\\prime}$ 个 $I^{\\prime} \\times J^{\\prime}$矩阵。卷积层计算也可以写作\n\n\n\\begin{gather*}\n\\boldsymbol{Z}_{k^{\\prime}}^{(l)}=\\sum_{k} \\boldsymbol{W}_{k, k^{\\prime}}^{(l)} * \\boldsymbol{X}_{k}^{(l-1)}+\\boldsymbol{b}_{k^{\\prime}}^{(l)}  \\tag{24.15}\\\\\n\\boldsymbol{X}_{k^{\\prime}}^{(l)}=a\\left(\\boldsymbol{Z}_{k^{\\prime}}^{(l)}\\right) \\tag{24.16}\n\\end{gather*}\n\n\n这里 $\\boldsymbol{X}_{k}^{(l-1)}$ 是输入的第 $k$ 个 $I \\times J$ 矩阵, $\\boldsymbol{X}_{k^{\\prime}}^{(l)}$ 是输出的第 $k^{\\prime}$ 个 $I^{\\prime} \\times J^{\\prime}$ 矩阵, $\\boldsymbol{W}_{k, k^{\\prime}}^{(l)}$ 是二维卷积核的第 $k \\times k^{\\prime}$ 个 $M \\times N$ 矩阵, $\\boldsymbol{b}_{k^{\\prime}}^{(l)}$ 是偏置的第 $k^{\\prime}$ 个 $I^{\\prime} \\times J^{\\prime}$ 矩阵, $Z_{k^{\\prime}}^{(l)}$ 是净输入的第 $k^{\\prime}$个 $I^{\\prime} \\times J^{\\prime}$ 矩阵。图 24.12 显示卷积层的输入和输出张量 (特征图)。\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_fc25db59b1a18f350d07g-11}\n\n图 24.12 卷积层的输入和输出张量\n\n每次对 $K$ 个 $I \\times J$ 矩阵同时进行卷积运算得到 1 个 $I^{\\prime} \\times J^{\\prime}$ 矩阵, 整体计算 $K^{\\prime}$ 次得到 $K^{\\prime}$个 $I^{\\prime} \\times J^{\\prime}$ 矩阵, 卷积核是 $K^{\\prime}$ 个 $M \\times N \\times K$ 张量。输入和输出张量的深度分别是 $K$ 和 $K^{\\prime}$ 。\n\n可以将 $I \\times J \\times K$ 张量展开成 $K$ 个 $I \\times J$ 矩阵, 将 $I^{\\prime} \\times J^{\\prime} \\times K$ 张量展开成 $K$ 个 $I^{\\prime} \\times J^{\\prime}$矩阵。汇聚层计算也可以写作\n\n\n\\begin{equation*}\n\\boldsymbol{X}_{k}^{(l)}=\\operatorname{pooling}\\left(\\boldsymbol{X}_{k}^{(l-1)}\\right) \\tag{24.17}\n\\end{equation*}\n\n\n这里 $\\boldsymbol{X}_{k}^{(l-1)}$ 是输入的是第 $k$ 个 $I \\times J$ 矩阵, $\\boldsymbol{X}_{k}^{(l)}$ 是输出的第 $k$ 个 $I^{\\prime} \\times J^{\\prime}$ 矩阵。图 24.13 显示汇聚层的输入和输出张量 (特征图)。\\\\\n\\includegraphics[max width=\\textwidth, center]{2024_03_13_fc25db59b1a18f350d07g-12}\n\n图 24.13 汇聚层的输入和输出张量\n\n汇聚运算对 $K$ 个 $I \\times J$ 矩阵分别进行, 得到 $K$ 个 $I^{\\prime} \\times J^{\\prime}$ 矩阵, 汇聚核是 $K$ 个 $M \\times N$矩阵。输入和输出张量的深度都是 $K$ 。\n\n卷积神经网络的特点可以由每一层的输入和输出张量体现, 所以习惯上用输入和输出张量表示其架构。\n\n\\section*{2. 模型例子}\n下面是一个简单的卷积神经网络的例子。这个 CNN 模型与 LeCun 提出的 LeNet 模型有相近的架构和规模。该模型在手写数字识别上达到很高的准确率, 是卷积神经网络最基本的模型。整个网络由两个卷积层、两个汇聚层、两个全连接层、一个输出层组成 (图 24.14)。表 24.1 列出了卷积层、汇聚层、全连接层、输出层的超参数, 输出特征图的大小, 其中 $F$表示卷积核或汇聚核的大小, $S$ 表示步幅, $W$ 表示权重矩阵的大小, $B$ 表示偏置向量的长度。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_fc25db59b1a18f350d07g-12(1)}\n\\end{center}\n\n图 $24.14 \\mathrm{CNN}$ 模型的网络架构\\\\\n表 24.1 CNN 模型的规模\n\n\\begin{center}\n\\begin{tabular}{c|c|c}\n\\hline\n & 超参数 & 输出特征图大小 \\\\\n\\hline\n输入 & $F=5 \\times 5 \\times 3 \\times 8, S=1$ & $32 \\times 32 \\times 3$ \\\\\n\\hline\nConv1 & $F=2 \\times 2 \\times 8, S=2$ & $28 \\times 28 \\times 8$ \\\\\n\\hline\nPool1 & $F=5 \\times 5 \\times 8 \\times 16, S=1$ & $14 \\times 14 \\times 8$ \\\\\n\\hline\nConv2 & $F=2 \\times 2 \\times 16, S=2$ & $10 \\times 10 \\times 16$ \\\\\n\\hline\nPool2 & $W=400 \\times 120, \\quad B=120$ & $5 \\times 5 \\times 16$ \\\\\n\\hline\nFC1 & $W=120 \\times 84, \\quad B=84$ & $120 \\times 1$ \\\\\n\\hline\nFC2 & $W=84 \\times 10$ & $10 \\times 1$ \\\\\n\\hline\nSoftmax &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}"
    },
    {
        "id": "chatcmpl-93w0eCWsl0EvwJMqOJTF8IK3oqjoA",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"卷积神经网络的架构包括哪些主要层？\",\n    \"回答1\": \"卷积神经网络的架构一般包括卷积层、汇聚层和全连接层。卷积层用于进行卷积运算，汇聚层用于进行汇聚运算，全连接层用于进行仿射变换和非线性变换。\",\n    \"问题2\": \"卷积神经网络中的卷积层是如何进行计算的？\",\n    \"回答2\": \"卷积层进行基于卷积函数的仿射变换和基于激活函数的非线性变换。对于第 $l$ 层的计算，输入是张量表示的特征图 $\\boldsymbol{X}^{(l-1)}$，输出也是张量表示的特征图 $\\boldsymbol{X}^{(l)}$。计算公式为 $\\boldsymbol{Z}^{(l)}=\\boldsymbol{W}^{(l)} * \\boldsymbol{X}^{(l-1)}+\\boldsymbol{b}^{(l)}$ 和 $\\boldsymbol{X}^{(l)}=a\\big(\\boldsymbol{Z}^{(l)}\\big)$。\",\n    \"问题3\": \"汇聚层在卷积神经网络中起到什么作用？\",\n    \"回答3\": \"汇聚层进行汇聚运算，其作用是对输入的特征图进行降维处理，从而减少参数数量和计算量，同时保留主要特征。这有助于提高模型的计算效率和泛化能力。\",\n    \"问题4\": \"全连接层在卷积神经网络中扮演什么角色？\",\n    \"回答4\": \"全连接层是前馈神经网络的一层，用于进行仿射变换和非线性变换。它将卷积和汇聚后得到的特征图展开成向量形式，并通过权重矩阵和偏置向量进行线性变换和非线性变换，最终输出标量结果。\",\n    \"问题5\": \"卷积神经网络中的参数是如何获得的？\",\n    \"回答5\": \"卷积神经网络中的所有参数，包括卷积核的权重和偏置、全连接层的权重和偏置，都通过学习过程获得。这意味着这些参数会根据训练数据不断调整以最小化损失函数，从而使得模型能够更好地拟合数据。\",\n    \"问题6\": \"卷积神经网络中为什么可以只有卷积层和全连接层，而没有汇聚层？\",\n    \"回答6\": \"在卷积神经网络中，步幅大于 1 的卷积运算也可以起到下采样作用，以代替汇聚运算。因此，可以通过调整卷积操作的步幅来实现降维效果，从而不需要使用汇聚层。这种设计原则通常能够在保持模型性能的同时减少参数数量和计算量。\",\n    \"问题7\": \"卷积神经网络中如何设计更深的结构以达到更好的预测效果？\",\n    \"回答7\": \"为了达到更好的预测效果，通常设计上的原则是使用更小的卷积核（如 $3 \\times 3$）和更深的结构。前端使用少量的卷积核，后端使用大量的卷积核。这种设计可以帮助网络更好地捕捉数据中的特征，并提高模型的表达能力。\",\n    \"问题8\": \"在卷积神经网络中，如何将张量展开成矩阵进行计算？\",\n    \"回答8\": \"可以将 $I \\times J \\times K$ 张量展开成 $K$ 个 $I \\times J$ 矩阵，将 $I^{\\text{'}} \\times J^{\\text{'}} \\times K^{\\text{'}}$ 张量展开成 $K^{\\text{'}}$ 个 $I^{\\text{'}} \\times J^{\\text{'}}$ 矩阵。这样就可以将卷积层和汇聚层的计算转化为矩阵之间的运算。\",\n    \"问题9\": \"卷积神经网络中如何表示卷积核和特征图之间的映射关系？\",\n    \"回答9\": \"在卷积神经网络中，输入张量由上一层的神经元决定，输出张量由当前层的神经元决定。当输入张量的元素到输出张量的元素之间存在映射关系时，对应的神经元之间存在连接。\",\n    \"问题10\": \"简单描述一下图中所示 CNN 模型的网络架构及规模。\",\n    \"回答10\": \"图中所示 CNN 模型包括两个卷积层、两个汇聚层、两个全连接层和一个输出层。其中包括超参数、输出特征图大小等信息。该模型在手写数字识别上取得了很高的准确率，并且是卷积神经网络最基本的模型之一。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710725000,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1479,
            "prompt_tokens": 6010,
            "total_tokens": 7489
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]