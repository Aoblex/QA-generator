[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter2",
        "title": "2.3.1 感知机学习算法的原始形式",
        "content_length": 4201,
        "content": "Title: 2.3.1 感知机学习算法的原始形式\nContent:\n感知机学习算法是对以下最优化问题的算法。给定一个训练数据集\n\n$$\nT=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}\n$$\n\n其中, $x_{i} \\in \\mathcal{X}=\\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y}=\\{-1,1\\}, i=1,2, \\cdots, N$, 求参数 $w, b$, 使其为以下损失函数极小化问题的解:\n\n\n\\begin{equation*}\n\\min _{w, b} L(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right) \\tag{2.5}\n\\end{equation*}\n\n\n其中, $M$ 为误分类点的集合。\n\n感知机学习算法是误分类驱动的, 具体采用随机梯度下降法 (stochastic gradient descent）。首先, 任意选取一个超平面 $w_{0}, b_{0}$, 然后用梯度下降法不断地极小化目标函数（式 (2.5））。极小化过程中不是一次使 $M$ 中所有误分类点的梯度下降, 而是一次随机选取一个误分类点使其梯度下降。\n\n假设误分类点集合 $M$ 是固定的, 那么损失函数 $L(w, b)$ 的梯度由\n\n$$\n\\begin{aligned}\n\\nabla_{w} L(w, b) & =-\\sum_{x_{i} \\in M} y_{i} x_{i} \\\\\n\\nabla_{b} L(w, b) & =-\\sum_{x_{i} \\in M} y_{i}\n\\end{aligned}\n$$\n\n给出。\n\n随机选取一个误分类点 $\\left(x_{i}, y_{i}\\right)$, 对 $w, b$ 进行更新:\n\n\n\\begin{gather*}\nw \\leftarrow w+\\eta y_{i} x_{i}  \\tag{2.6}\\\\\nb \\leftarrow b+\\eta y_{i} \\tag{2.7}\n\\end{gather*}\n\n\n式中 $\\eta(0<\\eta \\leqslant 1)$ 是步长, 在机器学习中又称为学习率 (learning rate)。这样, 通过迭代可以期待损失函数 $L(w, b)$ 不断减小, 直到为 0 。综上所述, 得到如下算法:\n\n\\section*{算法 2.1 (感知机学习算法的原始形式)}\n输入: 训练数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$, 其中 $x_{i} \\in \\mathcal{X}=\\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y}=$ $\\{-1,+1\\}, i=1,2, \\cdots, N$; 学习率 $\\eta(0<\\eta \\leqslant 1)$ 。\n\n输出: $w, b$; 感知机模型 $f(x)=\\operatorname{sign}(w \\cdot x+b)$ 。\n\n(1) 选取初值 $w_{0}, b_{0}$;\n\n(2) 在训练集中选取数据 $\\left(x_{i}, y_{i}\\right)$;\n\n(3) 如果 $y_{i}\\left(w \\cdot x_{i}+b\\right) \\leqslant 0$, 则\n\n$$\n\\begin{aligned}\n& w \\leftarrow w+\\eta y_{i} x_{i} \\\\\n& b \\leftarrow b+\\eta y_{i}\n\\end{aligned}\n$$\n\n（4）转至步骤 (2)，直至训练集中没有误分类点。\n\n这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时, 则调整 $w, b$ 的值, 使分离超平面向该误分类点的一侧移动, 以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。\n\n算法 2.1 是感知机学习的基本算法, 对应于后面的对偶形式, 称为原始形式。感知机学习算法简单且易于实现。\n\n例 2.1 如图 2.2 所示的训练数据集, 其正实例点是 $x_{1}=(3,3)^{\\mathrm{T}}, x_{2}=(4,3)^{\\mathrm{T}}$, 负实例点是 $x_{3}=(1,1)^{\\mathrm{T}}$, 试用感知机学习算法的原始形式求感知机模型 $f(x)=\\operatorname{sign}(w \\cdot x+b)$ 。这里, $w=\\left(w^{(1)}, w^{(2)}\\right)^{\\mathrm{T}}, x=\\left(x^{(1)}, x^{(2)}\\right)^{\\mathrm{T}}$ 。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_c4e863166a24653ffd7cg-17}\n\\end{center}\n\n图 2.2 感知机示例\n\n解 构建最优化问题:\n\n$$\n\\min _{w, b} L(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right)\n$$\n\n按照算法 2.1 求解 $w, b, \\eta=1$ 。\n\n(1) 取初值 $w_{0}=0, b_{0}=0$;\n\n(2) 对 $x_{1}=(3,3)^{\\mathrm{T}}, y_{1}\\left(w_{0} \\cdot x_{1}+b_{0}\\right)=0$, 未能被正确分类, 更新 $w, b$ :\n\n$$\nw_{1}=w_{0}+y_{1} x_{1}=(3,3)^{\\mathrm{T}}, b_{1}=b_{0}+y_{1}=1\n$$\n\n得到线性模型:\n\n$$\nw_{1} \\cdot x+b_{1}=3 x^{(1)}+3 x^{(2)}+1\n$$\n\n(3) 对 $x_{1}, x_{2}$, 显然, $y_{i}\\left(w_{1} \\cdot x_{i}+b_{1}\\right)>0$, 被正确分类, 不修改 $w, b$; 对 $x_{3}=(1,1)^{\\mathrm{T}}$, $y_{3}\\left(w_{1} \\cdot x_{3}+b_{1}\\right)<0$, 被误分类, 更新 $w, b:$\n\n$$\nw_{2}=w_{1}+y_{3} x_{3}=(2,2)^{\\mathrm{T}}, b_{2}=b_{1}+y_{3}=0\n$$\n\n得到线性模型:\n\n$$\nw_{2} \\cdot x+b_{2}=2 x^{(1)}+2 x^{(2)}\n$$\n\n如此继续下去, 直到\n\n$$\n\\begin{gathered}\nw_{7}=(1,1)^{\\mathrm{T}}, b_{7}=-3 \\\\\nw_{7} \\cdot x+b_{7}=x^{(1)}+x^{(2)}-3\n\\end{gathered}\n$$\n\n对所有数据点 $y_{i}\\left(w_{7} \\cdot x_{i}+b_{7}\\right)>0$, 没有误分类点, 损失函数达到极小。\n\n分离超平面为 $x^{(1)}+x^{(2)}-3=0$, 感知机模型为 $f(x)=\\operatorname{sign}\\left(x^{(1)}+x^{(2)}-3\\right)$ 。\n\n迭代过程见表 2.1。\n\n表 2.1 例 2.1 求解的迭代过程\n\n\\begin{center}\n\\begin{tabular}{ccccc}\n\\hline\n迭代次数 & 误分类点 & $w$ & $b$ & $w \\cdot x+b$ \\\\\n\\hline\n0 &  & 0 & 0 & 0 \\\\\n1 & $x_{1}$ & $(3,3)^{\\mathrm{T}}$ & 1 & $3 x^{(1)}+3 x^{(2)}+1$ \\\\\n2 & $x_{3}$ & $(2,2)^{\\mathrm{T}}$ & 0 & $2 x^{(1)}+2 x^{(2)}$ \\\\\n3 & $x_{3}$ & $(1,1)^{\\mathrm{T}}$ & -1 & $x^{(1)}+x^{(2)}-1$ \\\\\n4 & $x_{3}$ & $(0,0)^{\\mathrm{T}}$ & -2 & -2 \\\\\n5 & $x_{1}$ & $(3,3)^{\\mathrm{T}}$ & -1 & $3 x^{(1)}+3 x^{(2)}-1$ \\\\\n6 & $x_{3}$ & $(2,2)^{\\mathrm{T}}$ & -2 & $2 x^{(1)}+2 x^{(2)}-2$ \\\\\n7 & $x_{3}$ & $(1,1)^{\\mathrm{T}}$ & -3 & $x^{(1)}+x^{(2)}-3$ \\\\\n8 & 0 & $(1,1)^{\\mathrm{T}}$ & -3 & $x^{(1)}+x^{(2)}-3$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n这是在计算中误分类点先后取 $x_{1}, x_{3}, x_{3}, x_{3}, x_{1}, x_{3}, x_{3}$ 得到的分离超平面和感知机模型。如果在计算中误分类点依次取 $x_{1}, x_{3}, x_{3}, x_{3}, x_{2}, x_{3}, x_{3}, x_{3}, x_{1}, x_{3}, x_{3}$, 那么得到的分离超平面是 $2 x^{(1)}+x^{(2)}-5=0$ 。\n\n可见, 感知机学习算法由于采用不同的初值或选取不同的误分类点, 解可以不同。"
    },
    {
        "id": "chatcmpl-93zyt9abmPMm5u0jpvucRlaZHmWGM",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"感知机学习算法的目标是什么？\",\n    \"回答1\": \"感知机学习算法的目标是求参数 $w, b$，使损失函数 $L(w, b)=-\\text{∑}_{x_{i} \\text{在} M} y_{i}\\big(w \\bullet x_{i}+b\\big)$ 极小化，其中 $M$ 为误分类点的集合。\",\n    \"问题2\": \"感知机学习算法采用什么方法进行优化？\",\n    \"回答2\": \"感知机学习算法采用随机梯度下降法进行优化。具体来说，首先任意选取一个超平面 $w_{0}, b_{0}$，然后通过随机梯度下降法不断地极小化目标函数 $L(w, b)$。在极小化过程中，不是一次使 $M$ 中所有误分类点的梯度下降，而是随机选取一个误分类点使其梯度下降。\",\n    \"问题3\": \"感知机学习算法中的学习率 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}}$ 是什么作用？\",\n    \"回答3\": \"学习率 $\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{\\boldsymbol{0<\\text{η} \\text{≤} 1}$ 是步长，在机器学习中又称为学习率。它控制着参数 $w, b$ 在每一步更新时的幅度，影响着算法收敛的速度和稳定性。\",\n    \"问题4\": \"感知机学习算法的更新规则是怎样的？\",\n    \"回答4\": \"对于随机选取的一个误分类点 $\\big(x_{i}, y_{i}\\big)$，感知机学习算法通过更新参数 $w, b$ 的方式来减小损失函数。更新规则为 $w \\rightarrow w+\\text{η} y_{i} x_{i}$ 和 $b \\rightarrow b+\\text{η} y_{i}$。\",\n    \"问题5\": \"感知机学习算法的直观解释是什么？\",\n    \"回答5\": \"感知机学习算法的直观解释是当一个实例点被误分类时，即位于分离超平面的错误一侧时，调整 $w, b$ 的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距禒，直至超平面越过该误分类点使其被正确分类。\",\n    \"问题6\": \"感知机学习算法的基本算法对应于后面的哪种形式？\",\n    \"回答6\": \"感知机学习算法的基本算法对应于后面的对偶形式，称为原始形式。它是感知机学习算法的基本形式之一，简单且易于实现。\",\n    \"问题7\": \"在例 2.1 中，感知机学习算法如何求解感知机模型？\",\n    \"回答7\": \"在例 2.1 中，感知机学习算法通过迭代过程不断更新参数 $w, b$，直到损失函数达到极小值。具体来说，在每次迭代中，根据选取的误分类点和更新规则，逐步调整参数 $w, b$ 直至损失函数为 0。最终得到了分离超平面和感知机模型。\",\n    \"问题8\": \"在例 2.1 中，感知机学习算法如何处理误分类点？\",\n    \"回答8\": \"在例 2.1 中，当一个实例点被误分类时，即位于分离超平面的错误一侧时，感知机学习算法会调整 $w, b$ 的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距禒。这样不断迭代直至所有数据点都被正确分类。\",\n    \"问题9\": \"在例 2.1 中，感知机学习算法如何确定分离超平面和感知机模型？\",\n    \"回答9\": \"在例 2.1 中，感知机学习算法通过迭代过程不断更新参数 $w, b$ 直至损失函数达到极小值。最终得到了分离超平面和感知机模型。通过调整参数 $w, b$ 的值，使分离超平面能够正确地将数据点进行分类。\",\n    \"问题10\": \"在例 2.1 中，感知机学习算法有哪些因素会影响最终结果？\",\n    \"回答10\": \"在例 2.1 中，感知机学习算法中初值的选取、误分类点的顺序等因素会影响最终结果。不同的初值或选取不同的误分类点可能会导致不同的解。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710740267,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1511,
            "prompt_tokens": 5188,
            "total_tokens": 6699
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]