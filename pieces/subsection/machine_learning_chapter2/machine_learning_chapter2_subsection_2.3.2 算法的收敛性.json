[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter2",
        "title": "2.3.2 算法的收敛性",
        "content_length": 4303,
        "content": "Title: 2.3.2 算法的收敛性\nContent:\n现在证明, 对于线性可分数据集, 感知机学习算法原始形式收玫, 即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。\n\n为了便于叙述与推导, 将偏置 $b$ 并入权重向量 $w$, 记作 $\\hat{w}=\\left(w^{\\mathrm{T}}, b\\right)^{\\mathrm{T}}$, 同样也将输入向量加以扩充, 加进常数 1 , 记作 $\\hat{x}=\\left(x^{\\mathrm{T}}, 1\\right)^{\\mathrm{T}}$ 。这样, $\\hat{x} \\in \\boldsymbol{R}^{n+1}, \\hat{w} \\in \\boldsymbol{R}^{n+1}$ 。显然, $\\hat{w} \\cdot \\hat{x}=w \\cdot x+b$ 。\n\n定理 2.1 (Novikoff) 设训练数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}$ 是线性可分的, 其中 $x_{i} \\in \\mathcal{X}=\\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y}=\\{-1,+1\\}, i=1,2, \\cdots, N$, 则\n\n(1) 存在满足条件 $\\left\\|\\hat{w}_{\\mathrm{opt}}\\right\\|=1$ 的超平面 $\\hat{w}_{\\mathrm{opt}} \\cdot \\hat{x}=w_{\\mathrm{opt}} \\cdot x+b_{\\mathrm{opt}}=0$ 将训练数据集完全正确分开; 且存在 $\\gamma>0$, 对所有 $i=1,2, \\cdots, N$, 有\n\n\n\\begin{equation*}\ny_{i}\\left(\\hat{w}_{\\mathrm{opt}} \\cdot \\hat{x}_{i}\\right)=y_{i}\\left(w_{\\mathrm{opt}} \\cdot x_{i}+b_{\\mathrm{opt}}\\right) \\geqslant \\gamma \\tag{2.8}\n\\end{equation*}\n\n\n(2) 令 $R=\\max _{1 \\leqslant i \\leqslant N}\\left\\|\\hat{x}_{i}\\right\\|$, 则感知机算法 2.1 在训练数据集上的误分类次数 $k$ 满足不等式\n\n\n\\begin{equation*}\nk \\leqslant\\left(\\frac{R}{\\gamma}\\right)^{2} \\tag{2.9}\n\\end{equation*}\n\n\n证明 (1) 由于训练数据集是线性可分的, 按照定义 2.2 , 存在超平面可将训练数据集完全正确分开, 取此超平面为 $\\hat{w}_{\\mathrm{opt}} \\cdot \\hat{x}=w_{\\mathrm{opt}} \\cdot x+b_{\\mathrm{opt}}=0$, 使 $\\left\\|\\hat{w}_{\\mathrm{opt}}\\right\\|=1$ 。由于对有限的 $i=1,2, \\cdots, N$, 均有\n\n$$\ny_{i}\\left(\\hat{w}_{\\mathrm{opt}} \\cdot \\hat{x}_{i}\\right)=y_{i}\\left(w_{\\mathrm{opt}} \\cdot x_{i}+b_{\\mathrm{opt}}\\right)>0\n$$\n\n所以存在\n\n$$\n\\gamma=\\min _{i}\\left\\{y_{i}\\left(w_{\\mathrm{opt}} \\cdot x_{i}+b_{\\mathrm{opt}}\\right)\\right\\}\n$$\n\n使\n\n$$\ny_{i}\\left(\\hat{w}_{\\mathrm{opt}} \\cdot \\hat{x}_{i}\\right)=y_{i}\\left(w_{\\mathrm{opt}} \\cdot x_{i}+b_{\\mathrm{opt}}\\right) \\geqslant \\gamma\n$$\n\n(2) 感知机算法从 $\\hat{w}_{0}=0$ 开始, 如果实例被误分类, 则更新权重。令 $\\hat{w}_{k-1}$ 是第 $k$ 个误分类实例之前的扩充权重向量, 即\n\n$$\n\\hat{w}_{k-1}=\\left(w_{k-1}^{\\mathrm{T}}, b_{k-1}\\right)^{\\mathrm{T}}\n$$\n\n则第 $k$ 个误分类实例的条件是\n\n\n\\begin{equation*}\ny_{i}\\left(\\hat{w}_{k-1} \\cdot \\hat{x}_{i}\\right)=y_{i}\\left(w_{k-1} \\cdot x_{i}+b_{k-1}\\right) \\leqslant 0 \\tag{2.10}\n\\end{equation*}\n\n\n若 $\\left(x_{i}, y_{i}\\right)$ 是被 $\\hat{w}_{k-1}=\\left(w_{k-1}^{\\mathrm{T}}, b_{k-1}\\right)^{\\mathrm{T}}$ 误分类的数据, 则 $w$ 和 $b$ 的更新是\n\n$$\n\\begin{aligned}\n& w_{k} \\leftarrow w_{k-1}+\\eta y_{i} x_{i} \\\\\n& b_{k} \\leftarrow b_{k-1}+\\eta y_{i}\n\\end{aligned}\n$$\n\n即\n\n\n\\begin{equation*}\n\\hat{w}_{k}=\\hat{w}_{k-1}+\\eta y_{i} \\hat{x}_{i} \\tag{2.11}\n\\end{equation*}\n\n\n下面推导不等式 (2.12) 及不等式 (2.13):\n\n\n\\begin{equation*}\n\\hat{w}_{k} \\cdot \\hat{w}_{\\text {opt }} \\geqslant k \\eta \\gamma \\tag{2.12}\n\\end{equation*}\n\n\n由式 (2.11) 及式 (2.8) 得:\n\n$$\n\\begin{aligned}\n\\hat{w}_{k} \\cdot \\hat{w}_{\\mathrm{opt}} & =\\hat{w}_{k-1} \\cdot \\hat{w}_{\\mathrm{opt}}+\\eta y_{i} \\hat{w}_{\\mathrm{opt}} \\cdot \\hat{x}_{i} \\\\\n& \\geqslant \\hat{w}_{k-1} \\cdot \\hat{w}_{\\mathrm{opt}}+\\eta \\gamma\n\\end{aligned}\n$$\n\n由此递推即得不等式 (2.12):\n\n\n\\begin{gather*}\n\\hat{w}_{k} \\cdot \\hat{w}_{\\text {opt }} \\geqslant \\hat{w}_{k-1} \\cdot \\hat{w}_{\\text {opt }}+\\eta \\gamma \\geqslant \\hat{w}_{k-2} \\cdot \\hat{w}_{\\text {opt }}+2 \\eta \\gamma \\geqslant \\cdots \\geqslant k \\eta \\gamma \\\\\n\\left\\|\\hat{w}_{k}\\right\\|^{2} \\leqslant k \\eta^{2} R^{2} \\tag{2.13}\n\\end{gather*}\n\n\n由式 (2.11) 及式 (2.10) 得:\n\n$$\n\\begin{aligned}\n\\left\\|\\hat{w}_{k}\\right\\|^{2} & =\\left\\|\\hat{w}_{k-1}\\right\\|^{2}+2 \\eta y_{i} \\hat{w}_{k-1} \\cdot \\hat{x}_{i}+\\eta^{2}\\left\\|\\hat{x}_{i}\\right\\|^{2} \\\\\n& \\leqslant\\left\\|\\hat{w}_{k-1}\\right\\|^{2}+\\eta^{2}\\left\\|\\hat{x}_{i}\\right\\|^{2} \\\\\n& \\leqslant\\left\\|\\hat{w}_{k-1}\\right\\|^{2}+\\eta^{2} R^{2} \\\\\n& \\leqslant\\left\\|\\hat{w}_{k-2}\\right\\|^{2}+2 \\eta^{2} R^{2} \\leqslant \\cdots \\\\\n& \\leqslant k \\eta^{2} R^{2}\n\\end{aligned}\n$$\n\n结合不等式 $(2.12)$ 及式 (2.13) 即得:\n\n$$\n\\begin{aligned}\n& k \\eta \\gamma \\leqslant \\hat{w}_{k} \\cdot \\hat{w}_{\\text {opt }} \\leqslant\\left\\|\\hat{w}_{k}\\right\\|\\left\\|\\hat{w}_{\\text {opt }}\\right\\| \\leqslant \\sqrt{k} \\eta R \\\\\n& k^{2} \\gamma^{2} \\leqslant k R^{2}\n\\end{aligned}\n$$\n\n于是\n\n$$\nk \\leqslant\\left(\\frac{R}{\\gamma}\\right)^{2}\n$$\n\n定理表明, 误分类的次数 $k$ 是有上界的, 经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。也就是说, 当训练数据集线性可分时, 感知机学习算法原始形式迭代是收玫的。但是例 2.1 说明, 感知机学习算法存在许多解, 这些解既依赖于初值的选择, 也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面, 需要对分离超平面增加约束条件。这就是第 7 章将要讲述的线性支持向量机的想法。当训练集线性不可分时, 感知机学习算法不收玫，迭代结果会发生振荡。"
    },
    {
        "id": "chatcmpl-9400DlAi9jlv3HgPcQNrI7nGvDze2",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"感知机学习算法原始形式的收敛性是指什么？\",\n    \"回答1\": \"感知机学习算法原始形式的收敛性指经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。这意味着在线性可分数据集的情况下，感知机学习算法能够收敛并找到一个能够完全正确分类训练数据集的超平面。\",\n    \"问题2\": \"为什么将偏置 $b$ 并入权重向量 $w$，并对输入向量加以扩充？\",\n    \"回答2\": \"将偏置 $b$ 并入权重向量 $w$，并对输入向量加以扩充是为了方便叙述与推导。通过这样的处理，可以将偏置和权重统一表示为扩充后的权重向量 $\\boldsymbol{\\tilde{w}}$ 和扩充后的输入向量 $\\boldsymbol{\\tilde{x}}$，从而简化算法描述和数学推导。\",\n    \"问题3\": \"定理 2.1 (Novikoff) 中的不等式 (2.9) 描述了什么内容？\",\n    \"回答3\": \"定理 2.1 (Novikoff) 中的不等式 (2.9) 描述了感知机算法在训练数据集上的误分类次数 $k$ 与数据集特征的最大范数 $R$ 和一个正数 $\\rho$ 的关系。具体而言，不等式表明误分类次数 $k$ 满足 $k \\text{ 不大于 } \\big(\\frac{R}{\\rho}\\big)^{2}$。\",\n    \"问题4\": \"感知机算法如何更新权重和偏置以纠正误分类实例？\",\n    \"回答4\": \"感知机算法从 $\\boldsymbol{\\tilde{w}}_{0}=0$ 开始，如果实例被误分类，则更新权重和偏置。对于第 $k$ 个误分类实例，更新规则为 $w_{k} \\text{ 更新为 } w_{k-1}+\\boldsymbol{\\tilde{\\text{η}}} y_{i} x_{i}$，$b_{k} \\text{ 更新为 } b_{k-1}+\\boldsymbol{\\tilde{\\text{η}}} y_{i}$，其中 $\\boldsymbol{\\tilde{\\text{η}}}$ 为学习率。\",\n    \"问题5\": \"不等式 (2.12) 和不等式 (2.13) 分别描述了什么内容？\",\n    \"回答5\": \"不等式 (2.12) 描述了感知机算法中更新后的权重向量 $\\boldsymbol{\\tilde{w}}_{k}$ 与最优超平面的内积关系，而不等式 (2.13) 则描述了更新后的权重向量 $\\boldsymbol{\\tilde{w}}_{k}$ 的范数与误分类次数 $k$ 的关系。\",\n    \"问题6\": \"定理 2.1 (Novikoff) 的主要结论是什么？\",\n    \"回答6\": \"定理 2.1 (Novikoff) 的主要结论是当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。即存在一个满足条件的超平面将训练数据完全正确分开，并且误分类次数有上界。这表明在线性可分情况下，感知机学习算法能够收敛并找到唯一的超平面。\",\n    \"问题7\": \"感知机学习算法存在哪些问题？\",\n    \"回答7\": \"感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。此外，在训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生振荡。为了解决这些问题，需要对分离超平面增加约束条件，这是线性支持向量机的核心思想。\",\n    \"问题8\": \"为什么需要将偏置 $b$ 并入权重向量 $w$ 进行处理？\",\n    \"回答8\": \"将偏置 $b$ 并入权重向量 $w$ 进行处理是为了简化算法描述和数学推导。通过将偏置和权重统一表示为扩充后的权重向量 $\\boldsymbol{\\tilde{w}}$ 和扩充后的输入向量 $\\boldsymbol{\\tilde{x}}$，可以更方便地进行推导和计算。\",\n    \"问题9\": \"感知机学习算法如何更新权重和偏置以纠正误分类实例？\",\n    \"回答9\": \"感知机算法从 $\\boldsymbol{\\tilde{w}}_{0}=0$ 开始, 如果实例被误分类, 则更新权重。更新规则为 $w_{k} \\text{ 更新为 } w_{k-1}+\\boldsymbol{\\tilde{\\text{η}}} y_{i} x_{i}$, $b_{k} \\text{ 更新为 } b_{k-1}+\\boldsymbol{\\tilde{\\text{η}}} y_{i}$, 其中 $\\boldsymbol{\\tilde{\\text{η}}}$ 为学习率。\",\n    \"问题10\": \"定理 2.1 (Novikoff) 的结论对于感知机学习算法有何启示？\",\n    \"回答10\": \"定理 2.1 (Novikoff) 的结论表明，在线性可分数据集上，感知机学习算法原始形式迭代是收敛的，并且存在一个满足条件的超平面将训练数据完全正确分开。这启示我们在实际应用中可以利用该结论来判断是否适合使用感知机学习算法，并且在处理线性可分数据时能够更好地利用其特性。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710740349,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1605,
            "prompt_tokens": 4989,
            "total_tokens": 6594
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]