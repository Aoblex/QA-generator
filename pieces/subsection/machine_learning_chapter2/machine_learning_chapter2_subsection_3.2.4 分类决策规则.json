[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter2",
        "title": "3.2.4 分类决策规则",
        "content_length": 995,
        "content": "Title: 3.2.4 分类决策规则\nContent:\n$k$ 近邻法中的分类决策规则往往是多数表决, 即由输入实例的 $k$ 个邻近的训练实例中的多数类决定输入实例的类。\n\n多数表决规则 (majority voting rule) 有如下解释: 如果分类的损失函数为 0-1 损失函数, 分类函数为\n\n$$\nf: \\boldsymbol{R}^{n} \\rightarrow\\left\\{c_{1}, c_{2}, \\cdots, c_{K}\\right\\}\n$$\n\n那么误分类的概率是\n\n$$\nP(Y \\neq f(X))=1-P(Y=f(X))\n$$\n\n对给定的实例 $x \\in \\mathcal{X}$, 其最近邻的 $k$ 个训练实例点构成集合 $N_{k}(x)$ 。如果涵盖 $N_{k}(x)$ 的区域的类别是 $c_{j}$, 那么误分类率是\n\n$$\n\\frac{1}{k} \\sum_{x_{i} \\in N_{k}(x)} I\\left(y_{i} \\neq c_{j}\\right)=1-\\frac{1}{k} \\sum_{x_{i} \\in N_{k}(x)} I\\left(y_{i}=c_{j}\\right)\n$$\n\n要使误分类率最小即经验风险最小, 就要使 $\\sum_{x_{i} \\in N_{k}(x)} I\\left(y_{i}=c_{j}\\right)$ 最大, 所以多数表决规则等价于经验风险最小化。\n\n\\section*{$3.3 k$ 近邻法的实现: $k d$ 树}\n实现 $k$ 近邻法时, 主要考虑的问题是如何对训练数据进行快速 $k$ 近邻搜索。这点在特征空间的维数大及训练数据容量大时尤其必要。\n\n$k$ 近邻法最简单的实现方法是线性扫描 (linear scan), 这时要计算输入实例与每一个训练实例的距离。当训练集很大时, 计算非常耗时, 这种方法是不可行的。\n\n为了提高 $k$ 近邻搜索的效率, 可以考虑使用特殊的结构存储训练数据, 以减少计算距离的次数。具体方法很多, 下面介绍其中的 $k d$ 树 ( $k d$ tree) 方法 (1)。树的名称。\n\n(1) $k d$ 树是存储 $k$ 维空间数据的树结构, 这里的 $k$ 与 $k$ 近邻法的 $k$ 意义不同, 为了与习惯一致, 本书仍用 $k d$"
    },
    {
        "id": "chatcmpl-9401pxzeJGGk1tJnkR7OHubdbe88v",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"在$k$近邻法中，多数表决规则是如何决定输入实例的类别的？\",\n    \"回答1\": \"多数表决规则是由输入实例的$k$个邻近的训练实例中的多数类来决定输入实例的类别。即选择$k$个邻近训练实例中出现最多次的类别作为输入实例的类别。\",\n    \"问题2\": \"多数表决规则的解释中提到了什么类型的损失函数和分类函数？\",\n    \"回答2\": \"多数表决规则的解释中提到了0-1损失函数和分类函数。其中，0-1损失函数用于衡量分类的误差，而分类函数$f: \\boldsymbol{R}^{n} \\rightarrow\\big\\\\{c_{1}, c_{2}, \\text{...}, c_{K}\\big\\\\}$用于将输入实例映射到类别集合中的某一类别。\",\n    \"问题3\": \"如何计算给定实例$x$的误分类率？\",\n    \"回答3\": \"对给定的实例$x \\text{ 在 } \\boldsymbol{X}$，其最近邻的$k$个训练实例点构成集合$N_{k}(x)$。如果涵盖$N_{k}(x)$的区域的类别是$c_{j}$，那么误分类率可以通过计算$\\frac{1}{k} \\text{ 乘以 } \\text{求和}_{x_{i} \\text{ 在 } N_{k}(x)} I\\big(y_{i} \\neq c_{j}\\big)$来得到。\",\n    \"问题4\": \"多数表决规则与经验风险最小化有什么等价性？\",\n    \"回答4\": \"多数表决规则等价于经验风险最小化。具体来说，要使误分类率最小即经验风险最小，就要使$\\text{求和}_{x_{i} \\text{ 在 } N_{k}(x)} I\\big(y_{i}=c_{j}\\big)$最大。\",\n    \"问题5\": \"在$k$近邻法中，为什么需要考虑如何对训练数据进行快速$k$近邻搜索？\",\n    \"回答5\": \"在$k$近邻法中，需要考虑如何对训练数据进行快速$k$近邻搜索是因为在特征空间的维数大及训练数据容量大时，线性扫描方法计算非常耗时，不可行。因此，需要寻找更高效的方法来进行$k$近邻搜索。\",\n    \"问题6\": \"简要介绍一下$k d$树（$k d$ tree）方法。\",\n    \"回答6\": \"$k d$树是存储$k$维空间数据的树结构。它是一种特殊的结构存储训练数据的方法，旨在减少计算距离的次数，从而提高$k$近邻搜索的效率。\",\n    \"问题7\": \"线性扫描方法在什么情况下不可行？\",\n    \"回答7\": \"线性扫描方法在训练集很大时不可行。因为当训练数据容量大时，计算输入实例与每一个训练实例的距离将会非常耗时。\",\n    \"问题8\": \"为什么需要使用特殊的结构存储训练数据来提高$k$近邻搜索的效率？\",\n    \"回答8\": \"需要使用特殊的结构存储训练数据来提高$k$近邻搜索的效率是因为在特征空间的维数大及训练数据容量大时，线性扫描方法计算非常耗时。通过使用特殊结构存储训练数据，可以减少计算距离的次数，从而提高$k$近邻搜索的效率。\",\n    \"问题9\": \"$k d$树方法如何帮助提高$k$近邻搜索的效率？\",\n    \"回答9\": \"$k d$树方法通过存储$k$维空间数据的树结构，可以减少计算距离的次数，从而帮助提高$k$近邻搜索的效率。它能够在特征空间维度较高且训练数据容量较大时，更快地找到输入实例的$k$个最近邻居。\",\n    \"问题10\": \"$k d$树与$k$近邻法中的$k$有何不同？\",\n    \"回答10\": \"$k d$树中的$k$与$k$近邻法中的$k$意义不同。在$k d$树中，$k d$表示存储$k$维空间数据的树结构；而在$k$近邻法中，$k$表示选择输入实例的最近$k$个邻近训练实例。因此，在这两个概念中，$k$具有不同的含义。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710740449,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1367,
            "prompt_tokens": 3325,
            "total_tokens": 4692
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]