[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter2",
        "title": "1.6.2 泛化误差上界",
        "content_length": 4852,
        "content": "Title: 1.6.2 泛化误差上界\nContent:\n学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的, 简称为泛化误差上界 (generalization error bound)。具体来说, 就是通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣。泛化误差上界通常具有以下性质: 它是样本容量的函数, 当样本容量增加时, 泛化误差上界趋于 0 ; 它是假设空间容量 (capacity) 的函数, 假设空间容量越大, 模型就越难学, 泛化误差上界就越大。\n\n下面给出一个简单的泛化误差上界的例子: 二类分类问题的泛化误差上界。\n\n考虑二类分类问题。已知训练数据集 $T=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}, N$ 是样本容量, $T$ 是从联合概率分布 $P(X, Y)$ 独立同分布产生的, $X \\in \\boldsymbol{R}^{n}, Y \\in\\{-1,+1\\}$ 。假设空间是函数的有限集合 $\\mathcal{F}=\\left\\{f_{1}, f_{2}, \\cdots, f_{d}\\right\\}, d$ 是函数个数。设 $f$ 是从 $\\mathcal{F}$ 中选取的函数。损失函数是 0-1 损失。关于 $f$ 的期望风险和经验风险分别是\n\n\n\\begin{gather*}\nR(f)=E[L(Y, f(X))]  \\tag{1.28}\\\\\n\\hat{R}(f)=\\frac{1}{N} \\sum_{i=1}^{N} L\\left(y_{i}, f\\left(x_{i}\\right)\\right) \\tag{1.29}\n\\end{gather*}\n\n\n经验风险最小化函数是\n\n\n\\begin{equation*}\nf_{N}=\\arg \\min _{f \\in \\mathcal{F}} \\hat{R}(f) \\tag{1.30}\n\\end{equation*}\n\n\n$f_{N}$ 依赖训练数据集的样本容量 $N$ 。人们更关心的是 $f_{N}$ 的泛化能力\n\n\n\\begin{equation*}\nR\\left(f_{N}\\right)=E\\left[L\\left(Y, f_{N}(X)\\right)\\right] \\tag{1.31}\n\\end{equation*}\n\n\n下面讨论从有限集合 $\\mathcal{F}=\\left\\{f_{1}, f_{2}, \\cdots, f_{d}\\right\\}$ 中任意选出的函数 $f$ 的泛化误差上界。\n\n定理 1.1 (泛化误差上界) 对二类分类问题，当假设空间是有限个函数的集合 $\\mathcal{F}=$ $\\left\\{f_{1}, f_{2}, \\cdots, f_{d}\\right\\}$ 时, 对任意一个函数 $f \\in \\mathcal{F}$, 至少以概率 $1-\\delta, 0<\\delta<1$, 以下不等式成立:\n\n\n\\begin{equation*}\nR(f) \\leqslant \\hat{R}(f)+\\varepsilon(d, N, \\delta) \\tag{1.32}\n\\end{equation*}\n\n\n其中,\n\n\n\\begin{equation*}\n\\varepsilon(d, N, \\delta)=\\sqrt{\\frac{1}{2 N}\\left(\\log d+\\log \\frac{1}{\\delta}\\right)} \\tag{1.33}\n\\end{equation*}\n\n\n不等式 (1.32) 左端 $R(f)$ 是泛化误差, 右端即为泛化误差上界。在泛化误差上界中, 第 1 项是训练误差, 训练误差越小, 泛化误差也越小。第 2 项 $\\varepsilon(d, N, \\delta)$ 是 $N$ 的单调递减函数,当 $N$ 趋于无穷时趋于 0 ; 同时它也是 $\\sqrt{\\log d}$ 阶的函数, 假设空间 $\\mathcal{F}$ 包含的函数越多, 其值越大。\\\\\n证明 在证明中要用到 Hoeffding 不等式, 先叙述如下。\n\n设 $X_{1}, X_{2}, \\cdots, X_{N}$ 是独立随机变量, 且 $X_{i} \\in\\left[a_{i}, b_{i}\\right], i=1,2, \\cdots, N ; \\bar{X}$ 是 $X_{1}, X_{2}, \\cdots, X_{N}$的经验均值, 即 $\\bar{X}=\\frac{1}{N} \\sum_{i=1}^{N} X_{i}$, 则对任意 $t>0$, 以下不等式成立:\n\n\n\\begin{align*}\n& P(\\bar{X}-E(\\bar{X}) \\geqslant t) \\leqslant \\exp \\left[-\\frac{2 N^{2} t^{2}}{\\sum_{i=1}^{N}\\left(b_{i}-a_{i}\\right)^{2}}\\right]  \\tag{1.34}\\\\\n& P(E(\\bar{X})-\\bar{X} \\geqslant t) \\leqslant \\exp \\left[-\\frac{2 N^{2} t^{2}}{\\sum_{i=1}^{N}\\left(b_{i}-a_{i}\\right)^{2}}\\right] \\tag{1.35}\n\\end{align*}\n\n\nHoeffding 不等式的证明省略, 这里用来推导泛化误差上界。\n\n对任意函数 $f \\in \\mathcal{F}, \\hat{R}(f)$ 是 $N$ 个独立的随机变量 $L(Y, f(X))$ 的样本均值, $R(f)$ 是随机变量 $L(Y, f(X))$ 的期望值。如果损失函数取值于区间 $[0,1]$, 即对所有 $i,\\left[a_{i}, b_{i}\\right]=[0,1]$, 那么由 Hoeffding 不等式 (1.35) 不难得知, 对 $\\varepsilon>0$, 以下不等式成立:\n\n\n\\begin{equation*}\nP(R(f)-\\hat{R}(f) \\geqslant \\varepsilon) \\leqslant \\exp \\left(-2 N \\varepsilon^{2}\\right) \\tag{1.36}\n\\end{equation*}\n\n\n由于 $\\mathcal{F}=\\left\\{f_{1}, f_{2}, \\cdots, f_{d}\\right\\}$ 是一个有限集合, 故\n\n$$\n\\begin{aligned}\nP(\\exists f \\in \\mathcal{F}: R(f)-\\hat{R}(f) \\geqslant \\varepsilon) & =P\\left(\\bigcup_{f \\in \\mathcal{F}}\\{R(f)-\\hat{R}(f) \\geqslant \\varepsilon\\}\\right) \\\\\n& \\leqslant \\sum_{f \\in \\mathcal{F}} P(R(f)-\\hat{R}(f) \\geqslant \\varepsilon) \\\\\n& \\leqslant d \\exp \\left(-2 N \\varepsilon^{2}\\right)\n\\end{aligned}\n$$\n\n或者等价地, 对任意 $f \\in \\mathcal{F}$, 有\n\n\n\\begin{equation*}\nP(R(f)-\\hat{R}(f)<\\varepsilon) \\geqslant 1-d \\exp \\left(-2 N \\varepsilon^{2}\\right) \\tag{1.37}\n\\end{equation*}\n\n\n令\n\n\n\\begin{equation*}\n\\delta=d \\exp \\left(-2 N \\varepsilon^{2}\\right) \\tag{1.38}\n\\end{equation*}\n\n\n则\n\n$$\nP(R(f)<\\hat{R}(f)+\\varepsilon) \\geqslant 1-\\delta\n$$\n\n即至少以概率 $1-\\delta$ 有 $R(f)<\\hat{R}(f)+\\varepsilon$, 其中 $\\varepsilon$ 由式 (1.38) 得到, 即为式 (1.33)。\n\n从泛化误差上界可知:\n\n\n\\begin{equation*}\nR\\left(f_{N}\\right) \\leqslant \\hat{R}\\left(f_{N}\\right)+\\varepsilon(d, N, \\delta) \\tag{1.39}\n\\end{equation*}\n\n\n其中, $\\varepsilon(d, N, \\delta)$ 由式 (1.33) 定义, $f_{N}$ 由式 (1.30) 定义。\\\\\n以上讨论的只是假设空间包含有限个函数情况下的泛化误差上界, 对一般的假设空间要找到泛化误差上界就没有这么简单, 这里不作介绍。\n\n\\section*{1.7 生成模型与判别模型}\n监督学习的任务就是学习一个模型, 应用这一模型, 对给定的输入预测相应的输出。这个模型的一般形式为决策函数:\n\n$$\nY=f(X)\n$$\n\n或者条件概率分布:\n\n$$\nP(Y \\mid X)\n$$\n\n监督学习方法又可以分为生成方法 (generative approach) 和判别方法 (discrimina-tive approach)。所学到的模型分别称为生成模型 (generative model) 和判别模型 (discriminative model）。\n\n生成方法原理上由数据学习联合概率分布 $P(X, Y)$, 然后求出条件概率分布 $P(Y \\mid X)$ 作为预测的模型, 即生成模型:\n\n\n\\begin{equation*}\nP(Y \\mid X)=\\frac{P(X, Y)}{P(X)} \\tag{1.40}\n\\end{equation*}\n\n\n这样的方法之所以称为生成方法, 是因为模型表示了给定输入 $X$ 产生输出 $Y$ 的生成关系。典型的生成模型有朴素贝叶斯法和隐马尔可夫模型, 将在后面章节进行相关讲述。\n\n判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(Y \\mid X)$ 作为预测的模型,即判别模型。判别方法关心的是对给定的输入 $X$, 应该预测什么样的输出 $Y$ 。典型的判别模型包括: $k$ 近邻法、感知机、逻辑斯谛回归模型、最大摘模型、支持向量机、提升方法和条件随机场等, 将在后面章节讲述。\n\n在监督学习中, 生成方法和判别方法各有优缺点, 适合于不同条件下的学习问题。\n\n生成方法的特点: 生成方法可以还原出联合概率分布 $P(X, Y)$, 而判别方法不能; 生成方法的学习收玫速度更快, 即当样本容量增加的时候, 学到的模型可以更快地收玫于真实模型; 当存在隐变量时, 仍可以用生成方法学习, 此时判别方法就不能用。\n\n判别方法的特点: 判别方法直接学习的是条件概率分布 $P(Y \\mid X)$ 或决策函数 $f(X)$, 直接面对预测, 往往学习的精确率更高; 由于直接学习 $P(Y \\mid X)$ 或 $f(X)$, 可以对数据进行各种程度上的抽象、定义特征并使用特征, 因此可以简化学习问题。\n\n\\section*{1.8 监督学习应用}\n监督学习的应用主要在三个方面: 分类问题、标注问题和回归问题。"
    },
    {
        "id": "chatcmpl-93zxMwehA1BBb5bpfI9bk6X6RABua",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是泛化误差上界，它通常具有哪些性质？\",\n    \"回答1\": \"泛化误差上界是通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣。它通常具有以下性质：它是样本容量的函数，当样本容量增加时，泛化误差上界趋于 0；它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。\",\n    \"问题2\": \"在二类分类问题中，如何定义经验风险和期望风险？\",\n    \"回答2\": \"对于二类分类问题，已知训练数据集 $T=\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big(x_{1}, y_{1}\\right), \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, \\text{...}, x_{N}\\right)$，$N$ 是样本容量, $T$ 是从联合概率分布 $P(X, Y)$ 独立同分布产生的, $X \\boldsymbol{R}^{n}$, $Y \\boldsymbol{-1,+1}$ 。损失函数是 0-1 损失。关于 $f$ 的期望风险和经验风险分别是 $R(f)=E[L(Y, f(X))]$ 和 $\\bar{R}(f)=\\\\frac{1}{N} \\\\sum_{i=1}^{N} L(y_{i}, f(x_{i}))$。\",\n    \"问题3\": \"生成方法和判别方法在监督学习中有什么区别？\",\n    \"回答3\": \"生成方法原理上由数据学习联合概率分布 $P(X, Y)$, 然后求出条件概率分布 $P(Y \\\\\\\\_mid X)$ 作为预测的模型, 即生成模型。而判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(Y \\\\mid X)$ 作为预测的模型,即判别模型。生成方法表示了给定输入 $X$ 产生输出 $Y$ 的生成关系，而判别方法关心的是对给定的输入 $X$, 应该预测什么样的输出 $Y$。\",\n    \"问题4\": \"生成方法和判别方法各有什么优缺点？\",\n    \"回答4\": \"生成方法可以还原出联合概率分布 $P(X, Y)$, 而判别方法不能；生成方法的学习收敛速度更快, 即当样本容量增加的时候, 学到的模型可以更快地收敛于真实模型；当存在隐变量时, 仍可以用生成方法学习, 此时判别方法就不能用。判别方法直接学习的是条件概率分布 $P(Y \\\\mid X)$ 或决策函数 $f(X)$, 直接面对预测, 往往学习的精确率更高；由于直接学习 $P(Y \\\\mid X)$ 或 $f(X)$, 可以对数据进行各种程度上的抽象、定义特征并使用特征, 因此可以简化学习问题。\",\n    \"问题5\": \"监督学习的应用主要在哪三个方面？\",\n    \"回答5\": \"监督学习的应用主要在分类问题、标注问题和回归问题。\",\n    \"问题6\": \"Hoeffding 不等式是如何用来推导泛化误差上界的？\",\n    \"回答6\": \"Hoeffding 不等式被用来推导泛化误差上界。对任意函数 $f \\\\in \\\\mathcal{F}$, $\\bar{R}(f)$ 是 $N$ 个独立的随机变量 $L(Y, f(X))$ 的样本均值, $R(f)$ 是随机变量 $L(Y, f(X))$ 的期望值。如果损失函数取值于区间 $[0,1]$, 即对所有 $i,\\big[a_{i}, b_{i}\\big]=\\\\big[0,1\\\\big]$, 那么由 Hoeffding 不等式不难得知, 对 $\\\\\\\\varepsilon>0$, 以下不等式成立: $P(R(f)-\\\\bar{R}(f) \\\\geqslant \\\\varepsilon) \\\\leqslant \\\\exp \\\\left(-2 N \\\\varepsilon^{2}\\\\right)$. 由于 $\\\\\\\\\\\\\\\\mathcal{F}=\\\\left\\\\{f_{1}, f_{2}, \\\\cdots, f_{d}\\\\right\\\\}$ 是一个有限集合, 故 $P(\\\\exists f \\\\in \\\\mathcal{F}: R(f)-\\\\bar{R}(f) \\\\geqslant \\\\varepsilon) =P\\\\left(\\\\bigcup_{f \\\\in \\\\mathcal{F}}\\\\left\\\\{R(f)-\\\\bar{R}(f) \\\\geqslant \\\\varepsilon\\\\right\\\\}\\\\right) \\\\leqslant \\\\sum_{f \\\\in \\\\mathcal{F}} P(R(f)-\\\\bar{R}(f) \\\\geqslant \\\\varepsilon) \\\\leqslant d \\\\exp \\\\left(-2 N \\\\varepsilon^{2}\\\\right)$ 或者等价地, 对任意 $f \\\\in \\\\mathcal{F}$, 有 $P(R(f)-\\\\bar{R}(f)<\\\\varepsilon) \\\\geqslant 1-d \\\\exp \\\\left(-2 N \\\\varepsilon^{2}\\\\right)$. 这样就得到了泛化误差上界。\",\n    \"问题7\": \"为什么人们更关心生成模型和判别模型中的判别模型？\",\n    \"回答7\": \"人们更关心判别模型，因为判别方法直接学习的是条件概率分布 $P(Y | X)$ 或决策函数 $f(X)$，直接面对预测，往往学习的精确率更高。由于直接学习条件概率分布或决策函数，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。\",\n    \"问题8\": \"泛化误差上界中的第一项和第二项分别代表什么意义？\",\n    \"回答8\": \"在泛化误差上界中，第一项是训练误差，训练误差越小，泛化误差也越小。第二项 $\\\\\\\\\\\\\\\\varepsilon(d, N, \\\\\\\\delta)$ 是 $\\\\\\\\\\\\\\\\sqrt{{1}/{2 N}}(\\\\\\\\log d+\\\\\\\\log {{1}/{\\\\\\\\delta}})$ 阶的函数，表示假设空间 $\\\\\\\\\\\\\\\\mathcal{{F}}$ 包含的函数越多，其值越大。\",\n    \"问题9\": \"监督学习中生成方法和判别方法各有哪些优点？\",\n    \"回答9\": \"生成方法可以还原出联合概率分布 $P(X, Y)$，而判别方法不能；生成方法的学习收敛速度更快，即当样本容量增加时，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。判别方法直接学习的是条件概率分布 $P(Y | X)$ 或决策函数 $f(X)$，直接面对预测，往往学习的精确率更高；由于直接学习条件概率分布或决策函数，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。\",\n    \"问题10\": \"在监督学习中，为什么需要通过研究泛化误差的概率上界来比较不同学习方法的优劣？\",\n    \"回答10\": \"通过研究泛化误差的概率上界来比较不同学习方法的优劣是因为泛化能力分析往往是通过研究泛化误差的概率上界进行的。通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣，从而评估它们在处理未见过数据时的表现。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710740172,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 2370,
            "prompt_tokens": 5726,
            "total_tokens": 8096
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]