[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter2",
        "title": "2.2.1 数据集的线性可分性",
        "content_length": 2149,
        "content": "Title: 2.2.1 数据集的线性可分性\nContent:\n定义 2.2 (数据集的线性可分性) 给定一个数据集\n\n$$\nT=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}\n$$\n\n其中, $x_{i} \\in \\mathcal{X}=\\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y}=\\{+1,-1\\}, i=1,2, \\cdots, N$, 如果存在某个超平面 $S$\n\n$$\nw \\cdot x+b=0\n$$\n\n能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧, 即对所有 $y_{i}=+1$ 的实例 $i$, 有 $w \\cdot x_{i}+b>0$, 对所有 $y_{i}=-1$ 的实例 $i$, 有 $w \\cdot x_{i}+b<0$, 则称数据集 $T$ 为线性可分数据集 (linearly separable data set); 否则, 称数据集 $T$ 线性不可分。\n\n\\section*{2.2 .2 感知机学习策略}\n假设训练数据集是线性可分的, 感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面, 即确定感知机模型参数 $w, b$,需要确定一个学习策略, 即定义 (经验) 损失函数并将损失函数极小化。\\\\\n损失函数的一个自然选择是误分类点的总数。但是, 这样的损失函数不是参数 $w, b$ 的连续可导函数, 不易优化。损失函数的另一个选择是误分类点到超平面 $S$ 的总距离, 这是感知机所采用的。为此, 首先写出输入空间 $\\boldsymbol{R}^{n}$ 中任一点 $x_{0}$ 到超平面 $S$ 的距离:\n\n$$\n\\frac{1}{\\|w\\|}\\left|w \\cdot x_{0}+b\\right|\n$$\n\n这里, $\\|w\\|$ 是 $w$ 的 $L_{2}$ 范数。\n\n其次, 对于误分类的数据 $\\left(x_{i}, y_{i}\\right)$,\n\n$$\n-y_{i}\\left(w \\cdot x_{i}+b\\right)>0\n$$\n\n成立。因为当 $w \\cdot x_{i}+b>0$ 时, $y_{i}=-1$; 而当 $w \\cdot x_{i}+b<0$ 时, $y_{i}=+1$ 。 因此, 误分类点 $x_{i}$ 到超平面 $S$ 的距离是\n\n$$\n-\\frac{1}{\\|w\\|} y_{i}\\left(w \\cdot x_{i}+b\\right)\n$$\n\n假设超平面 $S$ 的误分类点集合为 $M$, 那么所有误分类点到超平面 $S$ 的总距离为\n\n$$\n-\\frac{1}{\\|w\\|} \\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right)\n$$\n\n不考虑 $\\frac{1}{\\|w\\|}$, 就得到感知机学习的损失函数 ${ }^{(1)}$ 。\n\n给定训练数据集\n\n$$\nT=\\left\\{\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\cdots,\\left(x_{N}, y_{N}\\right)\\right\\}\n$$\n\n其中, $x_{i} \\in \\mathcal{X}=\\boldsymbol{R}^{n}, y_{i} \\in \\mathcal{Y}=\\{+1,-1\\}, i=1,2, \\cdots, N$ 。感知机 $\\operatorname{sign}(w \\cdot x+b)$ 学习的损失函数定义为\n\n\n\\begin{equation*}\nL(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(w \\cdot x_{i}+b\\right) \\tag{2.4}\n\\end{equation*}\n\n\n其中, $M$ 为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。\n\n显然, 损失函数 $L(w, b)$ 是非负的。如果没有误分类点, 损失函数值是 0 。而且, 误分类点越少，误分类点离超平面越近，损失函数值就越小。一个特定的样本点的损失函数在误分类时是参数 $w, b$ 的线性函数, 在正确分类时是 0 。因此, 给定训练数据集 $T$, 损失函数 $L(w, b)$是 $w, b$ 的连续可导函数。\n\n感知机学习的策略是在假设空间中选取使损失函数式 (2.4) 最小的模型参数 $w, b$, 即感知机模型。\n\n\\section*{2.3 感知机学习算法}\n感知机学习问题转化为求解损失函数式 (2.4) 的最优化问题, 最优化的方法是随机梯度下降法。本节叙述感知机学习的具体算法, 包括原始形式和对偶形式, 并证明在训练数据线\n\n(1) 第 7 章中会介绍 $y(w \\cdot x+b)$ 称为样本点的函数间隔。\\\\\n性可分条件下感知机学习算法的收敛性。"
    },
    {
        "id": "chatcmpl-93zyeNUgJmJjG0jEmXmRJfjVjruOq",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是数据集的线性可分性？\",\n    \"回答1\": \"给定一个数据集 $T$，其中每个数据点 $x_{i} \\text{ 为 } \\boldsymbol{R}^{n}$，标签 $y_{i} \\text{ 为 } \\text{+1 或 -1}$。如果存在某个超平面 $S$ 能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 $y_{i}=+1$ 的实例 $i$，有 $w \\times x_{i}+b>0$，对所有 $y_{i}=-1$ 的实例 $i$，有 $w \\times x_{i}+b<0$，则称数据集 $T$ 为线性可分数据集；否则，称数据集 $T$ 为线性不可分。\",\n    \"问题2\": \"感知机学习的目标是什么？\",\n    \"回答2\": \"假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数 $w, b$，需要确定一个学习策略，并将损失函数极小化。\",\n    \"问题3\": \"感知机学习中采用的损失函数是什么？\",\n    \"回答3\": \"感知机学习中采用的损失函数是经验风险函数，定义为 $L(w, b)=-\\text{∑}_{x_{i} \\text{ 在 } M} y_{i}(w \\times x_{i}+b)$，其中 $M$ 为误分类点的集合。这个损失函数是非负的，当没有误分类点时值为 0，而且误分类点越少，离超平面越近，损失函数值就越小。\",\n    \"问题4\": \"感知机学习的损失函数为什么是参数 $w, b$ 的连续可导函数？\",\n    \"回答4\": \"给定训练数据集 $T$，损失函数 $L(w, b)$ 是参数 $w, b$ 的连续可导函数。一个特定的样本点的损失函数在误分类时是参数 $w, b$ 的线性函数，在正确分类时是 0。因此，损失函数在参数空间中具有连续可导性。\",\n    \"问题5\": \"感知机学习的策略是如何选取模型参数？\",\n    \"回筗5\": \"感知机学习的策略是在假设空间中选取使损失函数最小化的模型参数 $w, b$。即通过最小化经验风险函数来确定感知机模型。\",\n    \"问题6\": \"感知机学习算法采用什么方法进行最优化？\",\n    \"回筗6\": \"感知机学习算法采用随机梯度下降法进行最优化。通过不断迭代更新模型参数 $w, b$，使得损失函数逐渐减小，从而求解损失函数式的最优化问题。\",\n    \"问题7\": \"什么是样本点的函数间隔？\",\n    \"回筗7\": \"$y(w \\times x+b)$ 称为样本点的函数间隔。其中，$y$ 为样本点的真实标签，$w \\times x+b$ 为样本点到超平面 $S$ 的有符号距禿。\",\n    \"问题8\": \"感知机学习算法在训练数据线性可分条件下是否收敛？\",\n    \"回筗8\": \"在训练数据线性可分条件下，感知机学习算法是收敛的。通过不断迭代更新模型参数 $w, b$，最终可以找到一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。\",\n    \"问题9\": \"误分类点到超平面的总距禂是如何计算得到损失函数？\",\n    \"回筗9\": \"误分类点到超平面的总距离是通过计算所有误分类点到超平面的距离之和得到的。具体而言，对于误分类的数据 $\\big(x_{i}, y_{i}\\big)$，其距离为 $-\\frac{1}{\\big\\backslash w\\big\\backslash} y_{i}\\big(w \\times x_{i}+b\\big)$。因此，误分类点到超平面的总距离可以表示为 $-\\frac{1}{\\big\\backslash w\\big\\backslash} \\text{∑}_{x_{i} \\text{ 在 } M} y_{i}\\big(w \\times x_{i}+b\\big)$。\",\n    \"问题10\": \"感知机学习中采用哪种损失函数作为经验风险函数？\",\n    \"回筗10\": \"感知机学习中采用误分类点到超平面的总距离作为经验风险函数。具体而言，损失函数定义为 $L(w, b)=-\\text{∑}_{x_{i} \\text{ 在 } M} y_{i}\\big(w \\times x_{i}+b\\big)$。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710740252,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1427,
            "prompt_tokens": 4079,
            "total_tokens": 5506
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]