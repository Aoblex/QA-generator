[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter2",
        "title": "3.3.2 搜索 $k d$ 树",
        "content_length": 3885,
        "content": "Title: 3.3.2 搜索 $k d$ 树\nContent:\n下面介绍如何利用 $k d$ 树进行 $k$ 近邻搜索。可以看到, 利用 $k d$ 树可以省去对大部分数据点的搜索, 从而减少搜索的计算量。这里以最近邻为例加以叙述, 同样的方法可以应用到 $k$近邻。\n\n给定一个目标点, 搜索其最近邻。首先找到包含目标点的叶结点; 然后从该叶结点出发,依次回退到父结点; 不断查找与目标点最邻近的结点, 当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。\n\n包含目标点的叶结点对应包含目标点的最小超矩形区域。以此叶结点的实例点作为当前最近点, 目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部（参阅\n\n(1) $x^{(1)}=6$ 是中位数, 但 $x^{(1)}=6$ 上没有数据点, 故选 $x^{(1)}=7$ 。\\\\\n图 3.5)。然后返回当前结点的父结点, 如果父结点的另一子结点的超矩形区域与超球体相交,那么在相交的区域内寻找与目标点更近的实例点。如果存在这样的点, 将此点作为新的当前最近点。算法转到更上一级的父结点, 继续上述过程。如果父结点的另一子结点的超矩形区域与超球体不相交, 或不存在比当前最近点更近的点, 则停止搜索。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_c4e863166a24653ffd7cg-30}\n\\end{center}\n\n图 3.5 通过 $k d$ 树搜索最近邻\n\n下面叙述用 $k d$ 树的最近邻搜索算法。\n\n\\section*{算法 3.3 (用 $k d$ 树的最近邻搜索)}\n输入: 已构造的 $k d$ 树, 目标点 $x$ 。\n\n输出: $x$ 的最近邻。\n\n(1) 在 $k d$ 树中找出包含目标点 $x$ 的叶结点: 从根结点出发, 递归地向下访问 $k d$ 树。若目标点 $x$ 当前维的坐标小于切分点的坐标, 则移动到左子结点, 否则移动到右子结点, 直到子结点为叶结点为止。\n\n(2) 以此叶结点为 “当前最近点”。\n\n(3) 递归地向上回退, 在每个结点进行以下操作:\n\n(a) 如果该结点保存的实例点比当前最近点距离目标点更近, 则以该实例点为 “当前最近点”。\n\n(b) 当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地, 检查另一子结点对应的区域是否与以目标点为球心、以目标点与 “当前最近点” 间的距离为半径的超球体相交。如果相交,可能在另一个子结点对应的区域内存在距目标点更近的点, 移动到另一个子结点。接着,递归地进行最近邻搜索。如果不相交, 向上回退。\n\n(4) 当回退到根结点时, 搜索结束。最后的 “当前最近点” 即为 $x$ 的最近邻点。\n\n如果实例点是随机分布的, $k d$ 树搜索的平均计算复杂度是 $O(\\log N)$, 这里 $N$ 是训练实例数。 $k d$ 树更适用于训练实例数远大于空间维数时的 $k$ 近邻搜索。当空间维数接近训练实例数时, 它的效率会迅速下降, 几乎接近线性扫描。\n\n下面通过一个例题来说明搜索方法。\n\n例 3.3 给定一个如图 3.5 所示的 $k d$ 树, 根结点为 $\\mathrm{A}$, 其子结点为 $\\mathrm{B}, \\mathrm{C}$ 等。树上共存储 7 个实例点, 另有一个输入目标实例点 $\\mathrm{S}$, 求 $\\mathrm{S}$ 的最近邻。\\\\\n解 首先在 $k d$ 树中找到包含点 $\\mathrm{S}$ 的叶结点 $\\mathrm{D}$ (图中的右下区域), 以点 $\\mathrm{D}$ 作为近似最近邻。真正最近邻一定在以点 $\\mathrm{S}$ 为中心通过点 $\\mathrm{D}$ 的圆的内部。然后返回结点 $\\mathrm{D}$ 的父结点 $\\mathrm{B}$, 在结点 $\\mathrm{B}$ 的另一子结点 $\\mathrm{F}$ 的区域内搜索最近邻。结点 $\\mathrm{F}$ 的区域与圆不相交, 不可能有最近邻点。继续返回上一级父结点 $\\mathrm{A}$, 在结点 $\\mathrm{A}$ 的另一子结点 $\\mathrm{C}$ 的区域内搜索最近邻。结点 $\\mathrm{C}$ 的区域与圆相交, 该区域在圆内的实例点有点 $\\mathrm{E}$, 点 $\\mathrm{E}$ 比点 $\\mathrm{D}$ 更近, 成为新的最近邻近似。最后得到点 $\\mathrm{E}$ 是点 $\\mathrm{S}$ 的最近邻。\n\n\\section*{本章概要}\n\\begin{enumerate}\n  \\item $k$ 近邻法是基本且简单的分类与回归方法。 $k$ 近邻法的基本做法是: 对给定的训练实例点和输入实例点, 首先确定输入实例点的 $k$ 个最近邻训练实例点, 然后利用这 $k$ 个训练实例点的类的多数来预测输入实例点的类。\n  \\item $k$ 近邻模型对应于基于训练数据集对特征空间的一个划分。 $k$ 近邻法中, 当训练集、距离度量、 $k$ 值及分类决策规则确定后, 其结果唯一确定。\n  \\item $k$ 近邻法三要素: 距离度量、 $k$ 值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的 $L_{p}$ 距离。 $k$ 值小时, $k$ 近邻模型更复杂; $k$ 值大时, $k$ 近邻模型更简单。 $k$ 值的选择反映了对近似误差与估计误差之间的权衡, 通常由交叉验证选择最优的 $k$ 。常用的分类决策规则是多数表决, 对应于经验风险最小化。\n  \\item $k$ 近邻法的实现需要考虑如何快速搜索 $k$ 个最近邻点。 $k d$ 树是一种便于对 $k$ 维空间中的数据进行快速检索的数据结构。 $k d$ 树是二叉树, 表示对 $k$ 维空间的一个划分, 其每个结点对应于 $k$ 维空间划分中的一个超矩形区域。利用 $k d$ 树可以省去对大部分数据点的搜索,从而减少搜索的计算量。\n\\end{enumerate}\n\n\\section*{继续阅读}\n$k$ 近邻法由 Cover 与 Hart 提出 [1]。 $k$ 近邻法相关的理论在文献 [2] 和文献 [3] 中已有论述。 $k$ 近邻法的扩展可参考文献 [4]。 $k d$ 树及其他快速搜索算法可参考文献 [5]。关于 $k$ 近邻法的介绍可参考文献 $[2]$ 。\n\n\\section*{习 题}\n3.1 参照图 3.1, 在二维空间中给出实例点, 画出 $k$ 为 1 和 2 时的 $k$ 近邻法构成的空间划分, 并对其进行比较, 体会 $k$ 值选择与模型复杂度及预测精确率的关系。\n\n3.2 利用例题 3.2 构造的 $k d$ 树求点 $x=(3,4.5)^{\\mathrm{T}}$ 的最近邻点。\n\n3.3 参照算法 3.3 , 写出输出为 $x$ 的 $k$ 近邻的算法。\n\n\\section*{参考文献}\n[1] COVER T, HART P. Nearest neighbor pattern classification[J]. IEEE Transactions on Information Theory, 1967, 13(1): 21-27.\n\n[2] HASTIE T, TIBSHIRANI R, FRIEDMAN J. The elements of statistical learning: data mining, inference, and prediction[M]. 范明, 柴玉梅, 处红英, 等译. Springer, 2001.\n\n[3] FRIEDMAN J. Flexible metric nearest neighbor classification[J]. Technical Report, 1994.\n\n[4] WEINBERGER K Q, BLITZER J, SAUL L K. Distance metric learning for large margin nearest neighbor classification[C]//Proceedings of the NIPS. 2005.\n\n[5] SAMET H. The design and analysis of spatial data structures[M]. Reading, MA: AddisonWesley, 1990.\n\n\\section*{第 4 章朴素贝叶斯法}\n朴素贝叶斯 (naïve Bayes) 法是基于贝叶斯定理与特征条件独立假设的分类方法 (1)。对于给定的训练数据集, 首先基于特征条件独立假设学习输入输出的联合概率分布; 然后基于此模型, 对给定的输入 $x$, 利用贝叶斯定理求出后验概率最大的输出 $y$ 。朴素贝叶斯法实现简单, 学习与预测的效率都很高, 是一种常用的方法。\n\n本章叙述朴素贝叶斯法, 包括朴素贝叶斯法的学习与分类、朴素贝叶斯法的参数估计算法。\n\n\\section*{4.1 朴素贝口十斯法的学习与分类}"
    },
    {
        "id": "chatcmpl-9407NP1vsjtjnZUVTaL01gfMWU0sp",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"利用 $k d$ 树进行 $k$ 近邻搜索相比传统搜索有什么优势？\",\n    \"回答1\": \"利用 $k d$ 树进行 $k$ 近邻搜索可以省去对大部分数据点的搜索，从而减少搜索的计算量。通过限制搜索在空间的局部区域上，能够大幅提高搜索效率。\",\n    \"问题2\": \"描述用 $k d$ 树进行最近邻搜索的算法流程。\",\n    \"回答2\": \"首先在 $k d$ 树中找出包含目标点 $x$ 的叶结点，然后以此叶结点为 “当前最近点”。接着递归地向上回退，在每个结点进行操作：(a) 如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为 “当前最近点”；(b) 检查该子结点的父结点的另一子结点对应的区域是否有更近的点，如果相交，则移动到另一个子结点，继续递归地进行最近邻搜索。当回退到根结点时，搜索结束，最后的 “当前最近点” 即为 $x$ 的最近邻点。\",\n    \"问题3\": \"为什么 $k d$ 树更适用于训练实例数远大于空间维数时的 $k$ 近邻搜索？\",\n    \"回答3\": \"$k d$ 树更适用于训练实例数远大于空间维数时的 $k$ 近邻搜索，因为如果实例点是随机分布的，$k d$ 树搜索的平均计算复杂度是 $O(\\text{log} N)$，其中 $N$ 是训练实例数。这使得 $k d$ 树在高维空间中能够更有效地减少搜索计算量，提高搜索效率。\",\n    \"问题4\": \"朴素贝叶斯法与 $k d$ 树有何联系？\",\n    \"回答4\": \"朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法，而 $k d$ 树是一种便于对 $k$ 维空间中的数据进行快速检索的数据结构。虽然两者在功能和应用领域上有所不同，但它们都是机器学习领域中常用的方法，用于解决不同类型的问题。\",\n    \"问题5\": \"朴素贝叶斯法的学习与分类过程分别是怎样的？\",\n    \"回答5\": \"朴素贝叶斯法的学习过程是基于特征条件独立假设学习输入输出的联合概率分布；分类过程是对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$。朴素贝叶斯法实现简单，学习与预测的效率都很高。\",\n    \"问题6\": \"在朴素贝叶斯法中，为什么要基于特征条件独立假设学习输入输出的联合概率分布？\",\n    \"回答6\": \"朴素贝叶斯法基于特征条件独立假设学习输入输出的联合概率分布是为了简化模型，并降低计算复杂度。虽然这一假设在现实情况下可能并不成立，但在实际应用中，朴素贝叶斯法仍然能够取得不错的分类效果。\",\n    \"问题7\": \"朴素贝叶斯法中常用的距离度量是什么？\",\n    \"回答7\": \"朴素贝叶斯法中常用的距离度量是欧氏距离及更一般的 $L_{p}$ 距离。选择合适的距离度量对于模型性能和分类效果具有重要影响。\",\n    \"问题8\": \"在朴素贝叶斯法中，如何选择最优的 $k$ 值？\",\n    \"回答8\": \"在朴素贝叶斯法中，选择最优的 $k$ 值通常由交叉验证来确定。选择合适的 $k$ 值反映了对近似误差与估计误差之间的权衡。\",\n    \"问题9\": \"朴素贝叶斯法如何实现快速搜索 $k$ 个最近邻点？\",\n    \"回答9\": \"朴素贝叶斯法通常利用 $k d$ 树这种数据结构来实现快速搜索 $k$ 个最近邻点。通过构建和利用 $k d$ 树，可以有效地减少对大部分数据点的搜索，从而提高搜索效率。\",\n    \"问题10\": \"除了朴素贝叶斯法外，还有哪些方法可用于快速检索高维空间中的数据？\",\n    \"回答10\": \"除了朴素贝叶斯法中利用 $k d$ 树进行快速检索外，还有其他方法如局部敏感哈希（LSH）等可用于快速检索高维空间中的数据。这些方法都旨在解决高维空间下数据检索效率低下的问题。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710740793,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1510,
            "prompt_tokens": 5586,
            "total_tokens": 7096
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]