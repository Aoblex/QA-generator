[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter2",
        "title": "1.5.1 正则化",
        "content_length": 1118,
        "content": "Title: 1.5.1 正则化\nContent:\n模型选择的典型方法是正则化（regularization）。正则化是结构风险最小化策略的实现,是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数, 模型越复杂, 正则化值就越大。比如, 正则化项可以是模型参数向量的范数。\n\n正则化一般具有如下形式:\n\n\n\\begin{equation*}\n\\min _{f \\in \\mathcal{F}} \\frac{1}{N} \\sum_{i=1}^{N} L\\left(y_{i}, f\\left(x_{i}\\right)\\right)+\\lambda J(f) \\tag{1.24}\n\\end{equation*}\n\n\n其中, 第 1 项是经验风险, 第 2 项是正则化项, $\\lambda \\geqslant 0$ 为调整两者之间关系的系数。\n\n正则化项可以取不同的形式。例如, 在回归问题中, 损失函数是平方损失, 正则化项可以是参数向量的 $L_{2}$ 范数:\n\n\n\\begin{equation*}\nL(w)=\\frac{1}{N} \\sum_{i=1}^{N}\\left(f\\left(x_{i} ; w\\right)-y_{i}\\right)^{2}+\\frac{\\lambda}{2}\\|w\\|^{2} \\tag{1.25}\n\\end{equation*}\n\n\n这里, $\\|w\\|$ 表示参数向量 $w$ 的 $L_{2}$ 范数。\n\n正则化项也可以是参数向量的 $L_{1}$ 范数:\n\n\n\\begin{equation*}\nL(w)=\\frac{1}{N} \\sum_{i=1}^{N}\\left(f\\left(x_{i} ; w\\right)-y_{i}\\right)^{2}+\\lambda\\|w\\|_{1} \\tag{1.26}\n\\end{equation*}\n\n\n这里, $\\|w\\|_{1}$ 表示参数向量 $w$ 的 $L_{1}$ 范数。\n\n第 1 项的经验风险较小的模型可能较复杂 (有多个非零参数), 这时第 2 项的模型复杂度会较大。正则化的作用是选择经验风险与模型复杂度同时较小的模型。\n\n正则化符合奥卡姆剃刀 (Occam's razor) 原理。奥卡姆剃刀原理应用于模型选择时变为以下想法: 在所有可能选择的模型中, 能够很好地解释已知数据并且十分简单才是最好的模型, 也就是应该选择的模型。从贝叶斯估计的角度来看, 正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率, 简单的模型有较大的先验概率。"
    },
    {
        "id": "chatcmpl-93zsU0HC0NPObFtKWM9diFFZBg4AG",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"正则化在模型选择中起到什么作用？\",\n    \"回答1\": \"正则化是结构风险最小化策略的实现，通过在经验风险上加一个正则化项或罚项来选择经验风险与模型复杂度同时较小的模型。它有助于防止过拟合，促使模型更好地泛化到新数据上。\",\n    \"问题2\": \"正则化项的一般形式是怎样的？\",\n    \"回答2\": \"正则化一般具有如下形式：$\\frac{1}{N} \\text{ 为 } i=1 \\text{ 到 } N \\text{ 的损失函数} L\\big(y_{i}, f\\big(x_{i}\\big)\\big)+\\text{调整两者之间关系的系数 } \\text{ 的正则化项 } J(f)$。\",\n    \"问题3\": \"正则化项可以采取哪些不同的形式？\",\n    \"回答3\": \"正则化项可以采取不同的形式，例如在回归问题中，损失函数是平方损失时，正则化项可以是参数向量的 $L_{2}$ 范数或 $L_{1}$ 范数。\",\n    \"问题4\": \"为什么正则化符合奥卡姆剃刀原理？\",\n    \"回答4\": \"正则化符合奥卡姆剃刀原理，因为它促使选择能够很好地解释已知数据并且十分简单的模型。在所有可能选择的模型中，简单的模型更有可能是最好的模型，也就是应该选择的模型。\",\n    \"问题5\": \"正则化项对应于贝叶斯估计中的哪个概念？\",\n    \"回答5\": \"从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。复杂的模型有较小的先验概率，而简单的模型有较大的先验概率。\",\n    \"问题6\": \"正则化项中的 $\\frac{\\text{1}}{\\text{N}} \\text{ 为 } i=1 \\text{ 到 } N \\text{ 的损失函数} L\\big(y_{i}, f\\big(x_{i}\\big)\\big)$ 代表什么意义？\",\n    \"回答6\": \"$\\frac{\\text{1}}{\\text{N}} \\text{ 为 } i=1 \\text{ 到 } N \\text{ 的损失函数} L\\big(y_{i}, f\\big(x_{i}\\big)\\big)$ 表示经验风险，即模型在训练数据集上的平均损失。它衡量了模型对已知数据的拟合程度。\",\n    \"问题7\": \"为什么正则化值随着模型复杂度增加而增大？\",\n    \"回答7\": \"正则化值随着模型复杂度增加而增大，因为正则化项一般是模型复杂度的单调递增函数。当模型越复杂时，正则化值也越大，从而惩罚复杂度较高的模型。\",\n    \"问题8\": \"为什么需要在经验风险上加入正则化项？\",\n    \"回答8\": \"需要在经验风险上加入正则化项是为了在模型选择中平衡经验风险和模型复杂度。通过调整两者之间关系的系数 $\\text{ 的大小}$ ，可以有效地控制模型拟合数据和模型复杂度之间的权衡。\",\n    \"问题9\": \"奥卡姆剃刀原理在模型选择中有何重要作用？\",\n    \"回答9\": \"奥卡姆剃刀原理在模型选择中起到重要作用，它指导着我们应该倾向于选择能够很好地解释已知数据并且十分简单的模型。这有助于避免过度拟合和提高模型泛化能力。\",\n    \"问题10\": \"调整系数 $\\text{ 的大小}$ 对正则化项和经验风险之间的关系有何影响？\",\n    \"回答10\": \"调整系数 $\\text{ 的大小}$ 对正则化项和经验风险之间的关系具有重要影响。通过调整系数 $\\text{ 的大小}$ ，可以平衡经验风险和模型复杂度之间的权衡，从而影响最终选择的模型。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710739870,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1256,
            "prompt_tokens": 3370,
            "total_tokens": 4626
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]