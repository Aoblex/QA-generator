[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter10",
        "title": "20.4.4 算法总结",
        "content_length": 5423,
        "content": "Title: 20.4.4 算法总结\nContent:\n根据上面的推导给出 LDA 的变分 EM 算法。\n\n\\section*{算法 20.5 (LDA 的变分 EM 算法)}\n输入: 给定文本集合 $D=\\left\\{w_{1}, w_{2}, \\cdots, w_{m}, \\cdots, w_{M}\\right\\}$ 。\n\n输出: 变分参数 $\\gamma, \\eta$, 模型参数 $\\alpha, \\varphi$ 。\n\n交替迭代 E 步和 $M$ 步, 直到收玫。\n\n\\section*{(1) $\\mathrm{E}$ 步}\n固定模型参数 $\\alpha, \\varphi$, 通过关于变分参数 $\\gamma, \\eta$ 的证据下界的最大化, 估计变分参数 $\\gamma, \\eta$ 。具体见算法 20.4。\n\n(2) $\\mathrm{M}$ 步\n\n固定变分参数 $\\gamma, \\eta$, 通过关于模型参数 $\\alpha, \\varphi$ 的证据下界的最大化, 估计模型参数 $\\alpha, \\varphi$ 。具体算法见式 (20.63) 和式 (20.67)。\n\n根据变分参数 $(\\gamma, \\eta)$ 可以估计模型参数 $\\theta=\\left(\\theta_{1}, \\theta_{2}, \\cdots, \\theta_{m}, \\cdots, \\theta_{M}\\right), z=\\left(z_{1}, z_{2}, \\cdots\\right.$, $\\left.z_{m}, \\cdots, z_{M}\\right)$ 。\n\n以上介绍的是图 20.7 中简化 LDA 模型的变分 EM 算法, 图 20.4 中完整 LDA 模型的变分 $\\mathrm{EM}$ 算法作为推广可以类似地导出。\n\n\\section*{本章概要}\n\\begin{enumerate}\n  \\item 狄利克雷分布的概率密度函数为\n\\end{enumerate}\n\n$$\np(\\theta \\mid \\alpha)=\\frac{\\Gamma\\left(\\sum_{i=1}^{k} \\alpha_{i}\\right)}{\\prod_{i=1}^{k} \\Gamma\\left(\\alpha_{i}\\right)} \\prod_{i=1}^{k} \\theta_{i}^{\\alpha_{i}-1}\n$$\n\n(1) 牛顿法的介绍可参照附录 B。\\\\\n其中, $\\sum_{i=1}^{k} \\theta_{i}=1, \\theta_{i} \\geqslant 0, \\alpha=\\left(\\alpha_{1}, \\alpha_{2}, \\cdots, \\alpha_{k}\\right), \\alpha_{i}>0, i=1,2, \\cdots, k \\circ$ 狄利克雷分布是多项分布的共轭先验。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{1}\n  \\item 潜在狄利克雷分配 (LDA) 是文本集合的生成概率模型。模型假设话题由单词的多项分布表示, 文本由话题的多项分布表示, 单词分布和话题分布的先验分布都是狄利克雷分布。LDA 模型属于概率图模型, 可以由板块表示法表示。LDA 模型中, 每个话题的单词分布、每个文本的话题分布、文本的每个位置的话题是隐变量, 文本的每个位置的单词是观测变量。\n\n  \\item LDA 生成文本集合的过程如下:\n\n\\end{enumerate}\n\n（1）话题的单词分布：随机生成所有话题的单词分布, 话题的单词分布是多项分布, 其先验分布是狄利克雷分布。\n\n（2）文本的话题分布：随机生成所有文本的话题分布, 文本的话题分布是多项分布, 其先验分布是狄利克雷分布。\n\n(3) 文本的内容: 随机生成所有文本的内容。在每个文本的每个位置, 按照文本的话题分布随机生成一个话题，再按照该话题的单词分布随机生成一个单词。\n\n\\begin{enumerate}\n  \\setcounter{enumi}{3}\n  \\item LDA 模型的学习与推理不能直接求解。通常采用的方法是吉布斯抽样算法和变分 EM 算法, 前者是蒙特卡罗法而后者是近似算法。\n\n  \\item LDA 的收缩的吉布斯抽样算法的基本想法如下。目标是对联合概率分布 $p(w, z, \\theta$, $\\varphi \\mid \\alpha, \\beta)$ 进行估计。通过积分求和将隐变量 $\\theta$ 和 $\\varphi$ 消掉, 得到边缘概率分布 $p(w, z \\mid \\alpha, \\beta)$; 对概率分布 $p(w \\mid z, \\alpha, \\beta)$ 进行吉布斯抽样, 得到分布 $p(w \\mid z, \\alpha, \\beta)$ 的随机样本; 再利用样本对变量 $z, \\theta$ 和 $\\varphi$ 的概率进行估计, 最终得到 LDA 模型 $p(w, z, \\theta, \\varphi \\mid \\alpha, \\beta)$ 的参数估计。具体算法如下: 对给定的文本单词序列, 每个位置上随机指派一个话题, 整体构成话题系列; 然后循环执行以下操作, 对整个文本序列进行扫描, 在每一个位置上计算在该位置上的话题的满条件概率分布，然后进行随机抽样，得到该位置的新的话题，指派给这个位置。\n\n  \\item 变分推理的基本想法如下。假设模型是联合概率分布 $p(x, z)$, 其中 $x$ 是观测变量（数据), $z$ 是隐变量。目标是学习模型的后验概率分布 $p(z \\mid x)$ 。考虑用变分分布 $q(z)$ 近似条件概率分布 $p(z \\mid x)$, 用 KL 散度计算两者的相似性, 找到与 $p(z \\mid x)$ 在 KL 散度意义下最近的 $q^{*}(z)$,用这个分布近似 $p(z \\mid x)$ 。假设 $q(z)$ 中的 $z$ 的所有分量都是互相独立的。利用 Jensen 不等式得到 KL 散度的最小化可以通过证据下界的最大化实现。因此, 变分推理变成求解以下证据下界最大化问题:\n\n\\end{enumerate}\n\n$$\nL(q, \\theta)=E_{q}[\\log p(x, z \\mid \\theta)]-E_{q}[\\log q(z)]\n$$\n\n\\begin{enumerate}\n  \\setcounter{enumi}{6}\n  \\item LDA 的变分 EM 算法如下: 针对 LDA 模型, 定义变分分布, 应用变分 EM 算法。目标是对证据下界 $L(\\gamma, \\eta, \\alpha, \\varphi)$ 进行最大化, 其中 $\\alpha$ 和 $\\varphi$ 是模型参数, $\\gamma$ 和 $\\eta$ 是变分参数。交替迭代 E 步和 $\\mathrm{M}$ 步, 直到收玫。\n\\end{enumerate}\n\n(1) $\\mathrm{E}$ 步: 固定模型参数 $\\alpha, \\varphi$, 通过关于变分参数 $\\gamma, \\eta$ 的证据下界的最大化, 估计变分参数 $\\gamma, \\eta$ 。\n\n(2) $\\mathrm{M}$ 步: 固定变分参数 $\\gamma, \\eta$, 通过关于模型参数 $\\alpha, \\varphi$ 的证据下界的最大化, 估计模型参数 $\\alpha, \\varphi$ 。\n\n\\section*{继续阅 读}\nLDA 的原始论文是文献 [1] 和文献 [2], LDA 的吉布斯抽样算法见文献 [3] 文献 [5], 变分 EM 算法见文献 [2]。变分推理的介绍可参考文献 [6]。LDA 的分布式学习算法有文献 [7],快速学习算法有文献 [8], 在线学习算法有文献 [9]。\n\n\\section*{习 题}\n20.1 推导狄利克雷分布数学期望公式。\n\n20.2 针对 17.2.2 节的文本例子, 使用 LDA 模型进行话题分析。\n\n20.3 找出 LDA 的吉布斯抽样算法、变分 EM 算法中利用狄利克雷分布的部分, 思考 LDA 中使用狄利克雷分布的重要性。\n\n20.4 给出 LDA 的吉布斯抽样算法和变分 EM 算法的算法复杂度。\n\n20.5 证明变分 EM 算法收敛。\n\n\\section*{参考文献}\n[1] BLEI D M, NG A Y, JORDAN M I. Latent Dirichlet allocation[C]//Advances in Neural Information Processing Systems 14. MIT Press, 2002.\n\n[2] BLEI D M, NG A Y, JORDAN M I. Latent Dirichlet allocation[J]. Journal of Machine Learning Research, 2003, 3: 933-1022.\n\n[3] GRIFFITHS T L, STEYVERS M. Finding scientific topics[J]. Proceedings of the National Academy of Science, 2004, 101: 5228-5235.\n\n[4] STEYVERS M, GRIFFITHS T. Probabilistic topic models[C]//Landauer T, McNamara D, Dennis S, et al. Handbook of Latent Semantic Analysis. Psychology Press, 2014.\n\n[5] GREGOR HEINRICH. Parameter estimation for text analysis[J]. Technical note, 2004.\n\n[6] BLEI D M, KUCUKELBIR A, MCAULIFFE J D. Variational inference: a review for statisticians $[\\mathrm{J}]$. Journal of the American Statistical Association, 2017, 112(518).\n\n[7] NEWMAN D, SMYTH P, WELLING M, et al. Distributed inference for latent Dirichlet allocation[J]. Advances in Neural Information Processing Systems, 2008: 1081-1088.\n\n[8] PORTEOUS I, NEWMAN D, IHLER A, et al. Fast collapsed Gibbs sampling for latent Dirichlet allocation[C]//Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2008: 569-577.\n\n[9] HOFFMAN M, BACH F R, BLEI D M. Online learning for latent Dirichlet allocation[J]. Advances in Neural Information Processing Systems, 2010: 856-864.\n\n\\section*{第 21 章 PageRank 算法}\n在实际应用中许多数据都以图 (graph) 的形式存在, 比如, 互联网、社交网络都可以看作是一个图。图数据上的机器学习具有理论与应用上的重要意义。PageRank 算法是图的链接分析（link analysis）的代表性算法, 属于图数据上的无监督学习方法。\n\nPageRank 算法最初作为互联网网页重要度的计算方法, 于 1996 年由 Page 和 Brin 提出, 并用于谷歌搜索引擎的网页排序。事实上, PageRank 可以定义在任意有向图上, 后来被应用到社会影响力分析、文本摘要等多个问题。\n\nPageRank 算法的基本想法是在有向图上定义一个随机游走模型, 即一阶马尔可夫链,描述随机游走者沿着有向图随机访问各个结点的行为。在一定条件下, 极限情况访问每个结点的概率收玫到平稳分布, 这时各个结点的平稳概率值就是其 PageRank 值, 表示结点的重要度。PageRank 是递归定义的, PageRank 的计算可以通过迭代算法进行。\n\n本章 21.1 节给出 PageRank 的定义, 21.2 节叙述 PageRank 的计算方法, 包括常用的幂法 (power method)。"
    },
    {
        "id": "chatcmpl-93zPvCgD8NAWP9LgELr884ys0Jgui",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"LDA 的变分 EM 算法的输入和输出是什么？\",\n    \"回答1\": \"LDA 的变分 EM 算法的输入是给定文本集合 $D=\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big[w_{1}, w_{2}, \\text{...}, w_{m}, \\text{...}, w_{M}\\big]\\right.$，输出是变分参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$ 和模型参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$。\",\n    \"问题2\": \"LDA 的变分 EM 算法中的 E 步和 M 步分别是如何进行的？\",\n    \"回答2\": \"在 LDA 的变分 EM 算法中，E 步中固定模型参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$，通过关于变分参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$ 的证据下界的最大化，估计变分参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$；M 步中固定变分参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$，通过关于模型参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$ 的证据下界的最大化，估计模型参数 $\\boldsymbol{\\theta}, \\boldsymbol{\\beta}$。\",\n    \"问题3\": \"LDA 模型中的狄利克雷分布有什么重要性？\",\n    \"回答3\": \"LDA 模型中使用狄利克雷分布的重要性体现在吉布斯抽样算法和变分 EM 算法中。狄利克雷分布作为多项分布的共轭先验，在 LDA 模型的推断过程中起到了重要作用，帮助对模型参数进行估计和推断。\",\n    \"问题4\": \"LDA 的吉布斯抽样算法和变分 EM 算法的算法复杂度是怎样的？\",\n    \"回答4\": \"LDA 的吉布斯抽样算法和变分 EM 算法的算法复杂度取决于文本集合的大小和模型参数的维度。一般来说，吉布斯抽样算法的复杂度较高，而变分 EM 算法相对更为高效，但具体复杂度需要根据具体情况进行评估。\",\n    \"问题5\": \"变分推理在 LDA 中起到了什么作用？\",\n    \"回答5\": \"在 LDA 中，变分推理用于近似求解后验概率分布 $p(z \\text{ 丨 } x)$。通过最大化证据下界来近似条件概率分布 $p(z \\text{ 丨 } x)$，从而实现对模型参数和变分参数的学习和推断。\",\n    \"问题6\": \"PageRank 算法最初是为了解决什么问题？\",\n    \"回答6\": \"PageRank 算法最初是为了解决互联网网页重要度的计算问题，用于谷歌搜索引擎的网页排序。后来被应用到社会影响力分析、文本摘要等多个领域。\",\n    \"问题7\": \"PageRank 算法是如何定义在有向图上的随机游走模型的？\",\n    \"回答7\": \"PageRank 算法在有向图上定义一个随机游走模型，描述随机游走者沿着有向图随机访问各个结点的行为。在一定条件下，访问每个结点的概率收敛到平稳分布，这时各个结点的平稳概率值就是其 PageRank 值，表示结点的重要度。\",\n    \"问题8\": \"PageRank 算法的计算方法包括哪些常用技术？\",\n    \"回答8\": \"PageRank 算法的计算方法包括常用的幂法（power method），通过迭代算法进行计算。幂法是一种求解特征值和特征向量的数值方法，在 PageRank 计算中被广泛应用。\",\n    \"问题9\": \"PageRank 算法在实际应用中有哪些扩展和应用？\",\n    \"回答9\": \"除了最初解决互联网网页重要度计算问题外，PageRank 算法后来被应用到社会影响力分析、文本摘要等多个领域。它也被扩展应用到其他图数据上的无监督学习方法。\",\n    \"问题10\": \"PageRank 算法在实际应用中有哪些扩展和应用？\",\n    \"回答10\": \"除了最初解决互联网网页重要度计算问题外，PageRank 算法后来被应用到社会影响力分析、文本摘要等多个领域。它也被扩展应用到其他图数据上的无监督学习方法。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710738099,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1443,
            "prompt_tokens": 5876,
            "total_tokens": 7319
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]