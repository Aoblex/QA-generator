[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter10",
        "title": "22.1.3 基础机器学习方法",
        "content_length": 3961,
        "content": "Title: 22.1.3 基础机器学习方法\nContent:\n矩阵分解基于不同假设：SVD 基于正交假设，即分解得到的左右矩阵是正交矩阵，中间矩阵是非负对角矩阵; 非负矩阵分解基于非负假设, 即分解得到的左右矩阵皆是非负矩阵。\n\n含有隐变量的概率模型的学习有两种方法: 迭代计算方法、随机抽样方法。EM 算法和变分推理 (包括变分 EM 算法) 属于迭代计算方法, 吉布斯抽样属于随机抽样方法。变分 EM 算法是 EM 算法的推广。\n\n矩阵的特征值与特征向量求解方法中, 幂法是常用的算法。\n\n表 22.2 总结了含隐变量概率模型的学习方法的特点。\\\\\n表 22.2 含有隐变量概率模型的学习方法的特点\n\n\\begin{center}\n\\begin{tabular}{c|l|c|c|c|c}\n\\hline\n\\multicolumn{1}{c|}{算法} & \\multicolumn{1}{|c|}{基本原理} & 收敛性 & 收敛速度 & 实现难易度 & 适合问题 \\\\\n\\hline\n$\\mathrm{EM}$ 算法 & \\begin{tabular}{l}\n迭代计算、后验概 \\\\\n率估计 \\\\\n\\end{tabular} & 收敛于局部最优 & 较快 & 容易 & 简单模型 \\\\\n\\hline\n变分推理 & \\begin{tabular}{l}\n迭代计算、后验概 \\\\\n率近似估计 \\\\\n\\end{tabular} & 收敛于局部最优 & 较慢 & 较复杂 & 复杂模型 \\\\\n\\hline\n吉布斯抽样 & \\begin{tabular}{l}\n随机抽样、后验概 \\\\\n率估计 \\\\\n\\end{tabular} & \\begin{tabular}{l}\n依概率收敛于全局 \\\\\n最优 \\\\\n\\end{tabular} & 较慢 & 容易 & 复杂模型 \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n\\section*{22.2 话题模型之间的关系和特点}\n在本书介绍的四种话题模型 LSA, NMF, PLSA 和 LDA 中, 前两者是非概率模型, 后两者是概率模型。下面讨论它们之间的关系（细节可参考文献 [1] 和文献 [2]）。\n\n可以从矩阵分解的统一框架看 LSA, NMF 和 PLSA。在这个框架下, 通过最小化一般化 Bregman 散度进行有约束的矩阵分解 $D=U V$, 得到这三个话题模型:\n\n$$\n\\min _{U, V} B(D \\| U V)\n$$\n\n这里 $B(D \\| U V)$ 表示 $D$ 和 $U V$ 之间的一般化 Bregman 散度 (generalized Bregman divergence), 当且仅当两者相等时取值为 0 。一般化 Bregman 散度包含平方损失、KL 散度等。三个话题模型拥有三种不同的具体形式。表 22.3 给出了三个话题模型的损失函数和约束的公式, 其中 PLSA 的矩阵 $D$ 需要进行归一化 $\\sum_{m, n} d_{m n}=1$ 。\n\n表 22.3 矩阵分解的角度看话题模型\n\n\\begin{center}\n\\begin{tabular}{cccc}\n\\hline\n方法 & 一般损失函数 $B(D \\| U V)$ & 矩阵 $U$ 的约束条件 & 矩阵 $V$ 的约束条件 \\\\\n\\hline\nLSA & $\\|D-U V\\|_{F}^{2}$ & $U^{\\mathrm{T}} U=I$ & $V V^{\\mathrm{T}}=\\Lambda^{2}$ \\\\\nNMF & $\\|D-U V\\|_{F}^{2}$ & $u_{m k} \\geqslant 0$ & $v_{k n} \\geqslant 0$ \\\\\nPLSA & $\\sum_{m n} d_{m n} \\log \\frac{d_{m n}}{(U V)_{m n}}$ & $U^{\\mathrm{T}} 1=1$ & $V^{\\mathrm{T}} 1=1$ \\\\\n &  & $u_{m k} \\geqslant 0$ & $v_{k n} \\geqslant 0$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n话题模型 LSA 和 NMF 是非概率模型, 但也有概率模型解释。可以从概率图模型的统一框架看 LSA, NMF, PLSA 和 LDA。在这个框架下, 认为文本由概率模型生成, 基于不同的假设得到四个不同的话题模型。四个话题模型有不同的概率图模型定义。对于 LSA 和 NMF,每个文本 $d_{n}$ 由高斯分布 $P\\left(d_{n} \\mid U, v_{n}\\right) \\propto \\exp \\left(-\\left\\|d_{n}-U v_{n}\\right\\|^{2}\\right)$ 生成, 其参数是 $U$ 和 $v_{n}$, 共有 $N$ 个文本, 如图 22.2 所示。两个话题模型有不同的约束条件, 表 22.4 给出约束条件的公式。\n\n\\begin{center}\n\\includegraphics[max width=\\textwidth]{2024_03_13_3182641e7f8cf23e4171g-30}\n\\end{center}\n\n图 22.2 话题模型 LSA 和 NMF 的概率图模型表示\\\\\n表 22.4 话题模型 LSA 和 NMF 的约束条件\n\n\\begin{center}\n\\begin{tabular}{ccc}\n\\hline\n方法 & 变量 $u_{k}$ 的约束条件 & 变量 $v_{n}$ 的约束条件 \\\\\n\\hline\nLSA & 正交 & 正交 \\\\\nNMF & $u_{m k} \\geqslant 0$ & $v_{k n} \\geqslant 0$ \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n\\section*{参考文献}\n[1] SINGH A P, GORDON G J. A unified view of matrix factorization models[M]//Daelemans W, Goethals B, Morik K. Machine Learning and Knowledge Discovery in Databases. Berlin: Springer, 2008.\n\n[2] WANG Q, XU J, LI H, et al. Regularized latent semantic indexing: a new approach to largescale topic modeling[J]. ACM Transactions on Information Systems (TOIS), 2013, 31(1), 5.\\\\\n第 3 离深度学习\n\n\\section*{第 23 章 前馈神经网络}\n人工神经网络（artificial neural network）或神经网络（neural network）是受生物神经网络启发而发明的由神经元连接组成的网络状机器学习模型。前馈神经网络 (feedforward neural network) 或多层感知机 (multilayer perceptron, MLP) 是最具代表性的神经网络, 主要用于监督学习, 如分类和回归。\n\n前馈神经网络由多层神经元组成, 层间的神经元相互连接, 层内的神经元不连接。其信息处理机制是: 前一层神经元通过层间连接向后一层神经元传递信号, 因为信号是从前往后转递的, 所以是 “前馈的” 信息处理网络。这里, 神经元是对多个输入信号 (实数向量) 进行非线性转换产生一个输出信号 (实数值) 的函数, 整个神经网络是对多个输入信号 (实数向量)进行多次非线性转换产生多个输出信号 (实数向量) 的复合函数。每一个神经元的函数含有参数, 神经网络的神经元的参数通过学习得到。当前馈神经网络的层数达到一定数量时 (一般大于 2), 又称为深度神经网络 (deep neural network, DNN)。\n\n前馈神经网络学习算法是反向传播 (back propagation) 算法, 是随机梯度下降法的具体实现。学习的损失函数通常在分类时是交叉嫡损失, 在回归时是平方损失, 其最小化等价于极大似然估计。学习的正则化方法包括早停法 (early stopping)、暂退法 (dropout)。\n\nMcCulloch 和 Pitts 于 1943 年提出了最初的人工神经网络模型; Rosenblatt 于 1958 年发明了感知机, 可以看作是前馈神经网络的前身; Rumelhart 等于 1986 年重新开发了反向传播算法, 用于前馈神经网络学习; Hinton 于 2006 年提出了深度学习的概念, 指包括深度神经网络 (DNN) 等复杂神经网络的机器学习。\n\n本章 23.1 节讲述前馈神经网络的模型, 23.2 节叙述前馈神经网络学习的算法, 23.3 节叙述前馈神经网络学习的正则化方法。\n\n\\section*{23.1 前馈神经网络的模型}\n神经网络是由神经元连接组成的网络, 采用不同类型的神经元以及神经元的不同连接方法可以构建出不同的网络结构, 也就是不同的神经网络模型。本节讲述前馈神经网络的基本模型。首先给出前馈神经网络的定义, 接着介绍具体例子, 最后讨论前馈神经网络的表示能力。"
    },
    {
        "id": "chatcmpl-93zZC2IE3yIXRqeeq8P2WWxkiMaK8",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"矩阵分解中 SVD 和非负矩阵分解的假设有何区别？\",\n    \"回答1\": \"SVD 基于正交假设，即分解得到的左右矩阵是正交矩阵，中间矩阵是非负对角矩阵；而非负矩阵分解基于非负假设，即分解得到的左右矩阵皆是非负矩阵。这两种假设在矩阵分解过程中对矩阵的性质和约束条件有所不同。\",\n    \"问题2\": \"含有隐变量的概率模型的学习有哪两种方法？\",\n    \"回答2\": \"含有隐变量的概率模型的学习有两种方法：迭代计算方法和随机抽样方法。其中，EM 算法和变分推理属于迭代计算方法，吉布斯抽样属于随机抽样方法。\",\n    \"问题3\": \"幂法在求解矩阵特征值与特征向量中扮演着什么角色？\",\n    \"回答3\": \"幂法是常用的算法，用于求解矩阵的特征值和特征向量。它通过迭代计算得到矩阵的主要特征值和对应的特征向量，是一种常用的特征值求解方法。\",\n    \"问题4\": \"LSA、NMF 和 PLSA 之间有何关系？\",\n    \"回答4\": \"LSA、NMF 和 PLSA 可以从矩阵分解的统一框架进行统一看待。通过最小化一般化 Bregman 散度进行有约束的矩阵分解 $D=U V$，得到这三个话题模型。它们拥有三种不同的具体形式，但可以在统一框架下进行统一理解。\",\n    \"问题5\": \"LSA 和 NMF 是概率模型还是非概率模型？\",\n    \"回答5\": \"LSA 和 NMF 是非概率模型。它们可以从概率图模型的统一框架看作是由概率模型生成，但本身属于非概率模型。\",\n    \"问题6\": \"前馈神经网络的信息处理机制是什么？\",\n    \"回答6\": \"前馈神经网络的信息处理机制是：前一层神经元通过层间连接向后一层神经元传递信号。因为信号是从前往后传递的，所以是“前馈”的信息处理网络。\",\n    \"问题7\": \"前馈神经网络学习算法是什么？\",\n    \"回答7\": \"前馈神经网络学习算法是反向传播（back propagation）算法，它是随机梯度下降法的具体实现。通过不断调整网络参数来最小化损失函数，实现对神经网络模型的学习和训练。\",\n    \"问题8\": \"前馈神经网络学习的正则化方法有哪些？\",\n    \"回答8\": \"前馈神经网络学习的正则化方法包括早停法（early stopping）和暂退法（dropout）。这些方法可以帮助提高神经网络模型的泛化能力和避免过拟合问题。\",\n    \"问题9\": \"人工神经网络的发展历程中有哪些重要事件？\",\n    \"回答9\": \"人工神经网络的发展历程中包括了一系列重要事件：McCulloch 和 Pitts 于 1943 年提出了最初的人工神经网络模型；Rosenblatt 于 1958 年发明了感知机；Rumelhart 等于 1986 年重新开发了反向传播算法；Hinton 于 2006 年提出了深度学习的概念。\",\n    \"问题10\": \"前馈神经网络和深度神经网络之间有何区别？\",\n    \"回答10\": \"前馈神经网络当层数达到一定数量时（一般大于 2），又称为深度神经网络（deep neural network, DNN）。因此，深度神经网络是前馈神经网络在层数上的一个扩展，具有更深层次的信息处理能力。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710738674,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1242,
            "prompt_tokens": 5380,
            "total_tokens": 6622
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]