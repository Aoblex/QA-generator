[
    {
        "strategy": "subsection",
        "source": "machine_learning_chapter9",
        "title": "20.3.2 算法的主要部分",
        "content_length": 3728,
        "content": "Title: 20.3.2 算法的主要部分\nContent:\n根据上面的分析, 问题转化为对后验概率分布 $p(z \\mid w, \\alpha, \\beta)$ 的吉布斯抽样, 该分布表示在所有文本的单词序列给定条件下所有可能话题序列的条件概率。这里先给出该分布的表达式, 之后给出该分布的满条件分布表达式。\n\n\\section*{1. 抽样分布的表达式}\n首先有关系\n\n\n\\begin{equation*}\np(z \\mid w, \\alpha, \\beta)=\\frac{p(w, z \\mid \\alpha, \\beta)}{p(w \\mid \\alpha, \\beta)} \\propto p(w, z \\mid \\alpha, \\beta) \\tag{20.20}\n\\end{equation*}\n\n\n这里变量 $w, \\alpha$ 和 $\\beta$ 已知, 分母相同, 可以不予考虑。联合分布 $p(w, z \\mid \\alpha, \\beta)$ 的表达式可以进一步分解为\n\n\n\\begin{equation*}\np(w, z \\mid \\alpha, \\beta)=p(w \\mid z, \\alpha, \\beta) p(z \\mid \\alpha, \\beta)=p(w \\mid z, \\beta) p(z \\mid \\alpha) \\tag{20.21}\n\\end{equation*}\n\n\n两个因子可以分别处理。\n\n推导第一个因子 $p(w \\mid z, \\beta)$ 的表达式。首先\n\n\n\\begin{equation*}\np(w \\mid z, \\varphi)=\\prod_{k=1}^{K} \\prod_{v=1}^{V} \\varphi_{k v}^{n_{k v}} \\tag{20.22}\n\\end{equation*}\n\n\n其中, $\\varphi_{k v}$ 是第 $k$ 个话题生成单词集合第 $v$ 个单词的概率, $n_{k v}$ 是数据中第 $k$ 个话题生成第 $v$ 个单词的次数。于是\n\n(1) 原理上也可以考虑整体吉布斯抽样 (full Gibbs sampling), 但算法更加复杂。\n\n\n\\begin{align*}\np(w \\mid z, \\beta) & =\\int p(w \\mid z, \\varphi) p(\\varphi \\mid \\beta) \\mathrm{d} \\varphi \\\\\n& =\\int \\prod_{k=1}^{K} \\frac{1}{\\mathrm{~B}(\\beta)} \\prod_{v=1}^{V} \\varphi_{k v}^{n_{k v}+\\beta_{v}-1} \\mathrm{~d} \\varphi \\\\\n& =\\prod_{k=1}^{K} \\frac{1}{\\mathrm{~B}(\\beta)} \\int \\prod_{v=1}^{V} \\varphi_{k v}^{n_{k v}+\\beta_{v}-1} \\mathrm{~d} \\varphi \\\\\n& =\\prod_{k=1}^{K} \\frac{\\mathrm{B}\\left(n_{k}+\\beta\\right)}{\\mathrm{B}(\\beta)} \\tag{20.23}\n\\end{align*}\n\n\n其中, $n_{k}=\\left\\{n_{k 1}, n_{k 2}, \\cdots, n_{k V}\\right\\}$ 。\n\n第二个因子 $p(z \\mid \\alpha)$ 的表达式可以类似推导。首先\n\n\n\\begin{equation*}\np(z \\mid \\theta)=\\prod_{m=1}^{M} \\prod_{k=1}^{K} \\theta_{m k}^{n_{m k}} \\tag{20.24}\n\\end{equation*}\n\n\n其中, $\\theta_{m k}$ 是第 $m$ 个文本生成第 $k$ 个话题的概率, $n_{m k}$ 是数据中第 $m$ 个文本生成第 $k$ 个话题的次数。于是\n\n\n\\begin{align*}\np(z \\mid \\alpha) & =\\int p(z \\mid \\theta) p(\\theta \\mid \\alpha) \\mathrm{d} \\theta \\\\\n& =\\int \\prod_{m=1}^{M} \\frac{1}{\\mathrm{~B}(\\alpha)} \\prod_{k=1}^{K} \\theta_{m k}^{n_{m k}+\\alpha_{k}-1} \\mathrm{~d} \\theta \\\\\n& =\\prod_{m=1}^{M} \\frac{1}{\\mathrm{~B}(\\alpha)} \\int \\prod_{k=1}^{K} \\theta_{m k}^{n_{m k}+\\alpha_{k}-1} \\mathrm{~d} \\theta \\\\\n& =\\prod_{m=1}^{M} \\frac{\\mathrm{B}\\left(n_{m}+\\alpha\\right)}{\\mathrm{B}(\\alpha)} \\tag{20.25}\n\\end{align*}\n\n\n其中, $n_{m}=\\left\\{n_{m 1}, n_{m 2}, \\cdots, n_{m K}\\right\\}$ 。由式 (20.23) 和式 (20.25) 得:\n\n\n\\begin{equation*}\np(z, w \\mid \\alpha, \\beta)=\\prod_{k=1}^{K} \\frac{\\mathrm{B}\\left(n_{k}+\\beta\\right)}{\\mathrm{B}(\\beta)} \\cdot \\prod_{m=1}^{M} \\frac{\\mathrm{B}\\left(n_{m}+\\alpha\\right)}{\\mathrm{B}(\\alpha)} \\tag{20.26}\n\\end{equation*}\n\n\n故由式 (20.20) 和式 (20.26) 得收缩的吉布斯抽样分布的公式:\n\n\n\\begin{equation*}\np(z \\mid w, \\alpha, \\beta) \\propto \\prod_{k=1}^{K} \\frac{\\mathrm{B}\\left(n_{k}+\\beta\\right)}{\\mathrm{B}(\\beta)} \\cdot \\prod_{m=1}^{M} \\frac{\\mathrm{B}\\left(n_{m}+\\alpha\\right)}{\\mathrm{B}(\\alpha)} \\tag{20.27}\n\\end{equation*}\n\n\n\\section*{2. 满条件分布的表达式}\n分布 $p(z \\mid w, \\alpha, \\beta)$ 的满条件分布可以写成\n\n\n\\begin{equation*}\np\\left(z_{i} \\mid z_{-i}, w, \\alpha, \\beta\\right)=\\frac{1}{Z_{z_{i}}} p(z \\mid w, \\alpha, \\beta) \\tag{20.28}\n\\end{equation*}\n\n\n这里 $w_{i}$ 表示所有文本的单词序列的第 $i$ 个位置的单词, $z_{i}$ 表示单词 $w_{i}$ 对应的话题, $i=$ $(m, n), i=1,2, \\cdots, I, z_{-i}=\\left\\{z_{j}: j \\neq i\\right\\}, Z_{z_{i}}$ 表示分布 $p(z \\mid w, \\alpha, \\beta)$ 对变量 $z_{i}$ 的边缘化因子。式 (20.28) 是在所有文本单词序列、其他位置话题序列给定条件下第 $i$ 个位置的话题的条件概率分布。由式 (20.27) 和式 (20.28) 可以推出:\n\n\n\\begin{equation*}\np\\left(z_{i} \\mid z_{-i}, w, \\alpha, \\beta\\right) \\propto \\frac{n_{k v}+\\beta_{v}}{\\sum_{v=1}^{V}\\left(n_{k v}+\\beta_{v}\\right)} \\cdot \\frac{n_{m k}+\\alpha_{k}}{\\sum_{k=1}^{K}\\left(n_{m k}+\\alpha_{k}\\right)} \\tag{20.29}\n\\end{equation*}\n\n\n其中, 第 $m$ 个文本的第 $n$ 个位置的单词 $w_{i}$ 是单词集合的第 $v$ 个单词, 其话题 $z_{i}$ 是话题集合的第 $k$ 个话题; $n_{k v}$ 表示第 $k$ 个话题中第 $v$ 个单词的计数, 但减去当前单词的计数; $n_{m k}$ 表示第 $m$ 个文本中第 $k$ 个话题的计数, 但减去当前单词的话题的计数。"
    },
    {
        "id": "chatcmpl-948OFB2hRWk7Z9mHw4PUnNxqjUnGl",
        "choices": [
            {
                "finish_reason": "stop",
                "index": 0,
                "logprobs": null,
                "message": {
                    "content": "{\n    \"问题1\": \"什么是吉布斯抽样？\",\n    \"回答1\": \"吉布斯抽样是一种用于从多维概率分布中抽取样本的方法。它通过依次对每个变量进行抽样，利用其他变量的当前值来生成下一个变量的值，从而逼近目标分布。\",\n    \"问题2\": \"如何表示后验概率分布 $p(z \\text{ ｜ } w, \\text{α, β})$ 的表达式？\",\n    \"回答2\": \"后验概率分布 $p(z \\text{ ｜ } w, \\text{α, β})$ 的表达式可以表示为 $p(z \\text{ ｜ } w, \\text{α, β}) \\text{ ∝ } p(w, z \\text{ ｜ } \\text{α, β})$，其中 $p(w, z \\text{ ｜ } \\text{α, β})$ 是在所有文本的单词序列给定条件下所有可能话题序列的条件概率。\",\n    \"问题3\": \"推导第一个因子 $p(w \\text{ ｜ } z, \\text{β})$ 的表达式是如何进行的？\",\n    \"回答3\": \"首先，$p(w \\text{ ｜ } z, \\text{β})$ 的表达式可以表示为 $\\text{∫} p(w \\text{ ｜ } z, \\text{ϕ}) p(ϕ \\text{ ｜ } \\text{β}) dϕ$。通过对 $ϕ$ 的积分和一系列推导，最终得到了 $p(w \\text{ ｜ } z, \\text{β})$ 的表达式。\",\n    \"问题4\": \"推导第二个因子 $p(z \\text{ ｜ } α)$ 的表达式是如何进行的？\",\n    \"回答4\": \"首先，$p(z \\text{ ｜ } α)$ 的表达式可以表示为 $\\text{∫} p(z \\text{ ｜ } θ) p(θ \\text{ ｜ } α) dθ$。通过对 $θ$ 的积分和一系列推导，最终得到了 $p(z \\text{ ｜ } α)$ 的表达式。\",\n    \"问题5\": \"收缩的吉布斯抽样分布的公式是什么？\",\n    \"回答5\": \"收缩的吉布斯抽样分布的公式可以表示为 $p(z \\text{ ｜ } w, \\text{α, β}) ∝ \\text{∏}_{k=1}^{K} \\frac{\\text{B}(n_{k}+\\text{β})}{\\text{B}(\\text{β})} ∙ \\text{∏}_{m=1}^{M} \\frac{\\text{B}(n_{m}+\\text{α})}{\\text{B}(\\text{α})}$，其中 $n_{k}$ 和 $n_{m}$ 分别表示相关计数。\",\n    \"问题6\": \"满条件分布的表达式是如何定义的？\",\n    \"回答6\": \"满条件分布的表达式可以表示为 $p(z_{i} \\text{ ｜ } z_{-i}, w, \\text{α, β}) = \\frac{1}{Z_{z_{i}}} p(z \\text{ ｜ } w, \\text{α, β})$，其中 $z_{i}$ 表示单词对应的话题，$z_{-i}$ 表示其他位置话题序列，$Z_{z_{i}}$ 表示分布 $p(z \\text{ ｜ } w, \\text{α, β})$ 对变量 $z_{i}$ 的边缘化因子。\",\n    \"问题7\": \"满条件分布的表达式中的边缘化因子 $Z_{z_{i}}$ 是如何计算得到的？\",\n    \"回答7\": \"$Z_{z_{i}}$ 是通过对分布 $p(z \\text{ ｜ } w, \\text{α, β})$ 进行边缘化得到的。它通常是一个归一化因子，用于确保满条件分布的概率和为1。\",\n    \"问题8\": \"满条件分布的表达式中的计数项 $n_{k v}$ 和 $n_{m k}$ 分别代表什么意义？\",\n    \"回答8\": \"$n_{k v}$ 表示第 $k$ 个话题中第 $v$ 个单词的计数，但减去当前单词的计数；$n_{m k}$ 表示第 $m$ 个文本中第 $k$ 个话题的计数，但减去当前单词的话题的计数。\",\n    \"问题9\": \"满条件分布的表达式中的公式 $p(z_{i} \\text{ ｜ } z_{-i}, w, \\text{α, β})$ 如何推导得到？\",\n    \"回答9\": \"$p(z_{i} \\text{ ｜ } z_{-i}, w, \\text{α, β}) ∝ \\frac{n_{k v}+\\beta_{v}}{\\text{∑}_{v=1}^{V}(n_{k v}+\\beta_{v})} ∙ \\frac{n_{m k}+\\text{α}_{k}}{\\text{∑}_{k=1}^{K}(n_{m k}+\\text{α}_{k})}$ 是通过对满条件分布进行推导得到的，其中涉及相关计数和超参数。\",\n    \"问题10\": \"在吉布斯抽样中，为什么需要使用满条件分布？\",\n    \"回答10\": \"在吉布斯抽样中，使用满条件分布可以方便地对每个变量进行抽样，利用其他变量的当前值来生成下一个变量的值，从而逼近目标分布。这有助于提高抽样效率和准确性。\"\n}",
                    "role": "assistant",
                    "function_call": null,
                    "tool_calls": null
                }
            }
        ],
        "created": 1710772591,
        "model": "gpt-3.5-turbo-1106",
        "object": "chat.completion",
        "system_fingerprint": "fp_4aaaf0dc94",
        "usage": {
            "completion_tokens": 1528,
            "prompt_tokens": 4654,
            "total_tokens": 6182
        },
        "parameters": {
            "model": "gpt-3.5-turbo-1106",
            "temperature": 0.2,
            "frequency_penalty": 0.15,
            "presence_penalty": 0.15
        }
    }
]