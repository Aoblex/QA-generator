[
    {
        "strategy": "section",
        "source": "978-3-030-70578-7",
        "level": 3,
        "title": "_Section 2.5_",
        "content_length": 6875,
        "content": "Title: _Section 2.5_\nContent: **Problem 2.7**: Prove Theorem 2.5.1 for the case of an \\(n\\)-dimensional sample space. [The condition that the cumulative distribution function is nondecreasing is replaced by \\(P\\{x_{1}<X_{1}\\leq x_{1}^{\\prime},\\ldots,x_{n}<X_{n}\\leq x_{n}^{\\prime}\\}\\geq 0\\); the condition that it is continuous on the right can be stated as \\(\\lim_{m\\to\\infty}F(x_{1}+1/m,\\ldots,x_{n}+1/m)=F(x_{1},\\ldots,x_{n})\\).]\n\n**Problem 2.8**: Let \\(\\mathcal{X}=\\mathcal{Y}\\times\\mathcal{T}\\), and suppose that \\(P_{0}\\), \\(P_{1}\\) are two probability distributions given by\n\n\\[dP_{0}(y,t) = f(y)g(t)\\,d\\mu(y)\\,d\\nu(t),\\] \\[dP_{1}(y,t) = h(y,t)\\,d\\mu(y)\\,d\\nu(t),\\]\n\nwhere \\(h(y,t)/f(y)g(t)<\\infty\\). Then under \\(P_{1}\\) the probability density of \\(Y\\) with respect to \\(\\mu\\) is\n\n\\[p_{1}^{Y}(y)=f(y)E_{0}\\left[\\frac{h(y,T)}{f(y)g(T)}\\,\\Big{|}\\,Y=y\\right].\\]\n\n[MISSING_PAGE_FAIL:71]\n\nis also \\({\\cal A}_{0}\\)-measurable. (ii): Let \\(\\lambda=\\sum_{j=1}^{\\infty}c_{j}P_{\\theta_{j}}\\) be equivalent to \\({\\cal P}\\). Then pairwise sufficiency of \\(T\\) implies for any \\(\\theta_{0}\\) that \\(dP_{\\theta_{0}}/(dP_{\\theta_{0}}+d\\lambda)\\) and hence \\(dP_{\\theta_{0}}/d\\lambda\\) is a measurable function of \\(T\\).]\n\n**Problem 2.12**: If a statistic \\(T\\) is sufficient for \\({\\cal P}\\), then for every function \\(f\\) which is \\(({\\cal A},\\,P_{\\theta})\\)-integrable for all \\(\\theta\\in\\Omega\\) there exists a determination of the conditional expectation function \\(E_{\\theta}[f(X)\\mid t]\\) that is independent of \\(\\theta\\). [If \\({\\cal X}\\) is Euclidean, this follows from Theorems 2.5.2 and 2.6.1. In general, if \\(f\\) is nonnegative there exists a nondecreasing sequence of simple nonnegative functions \\(f_{n}\\) tending to \\(f\\). Since the conditional expectation of a simple function can be taken to be independent of \\(\\theta\\) by Lemma 2.4.1(i), the desired result follows from Lemma 2.4.1(iv).]\n\n**Problem 2.13**: For a decision problem with a finite number of decisions, the class of procedures depending on a sufficient statistic \\(T\\) only is essentially complete. [For Euclidean sample spaces this follows from Theorem 2.5.1 without any restriction on the decision space. For the present case, let a decision procedure be given by \\(\\delta(x)=(\\delta^{(1)}(x),\\,\\ldots,\\,\\delta^{(m)}(x))\\) where \\(\\delta^{(i)}(x)\\) is the probability with which decision \\(d_{i}\\) is taken when \\(x\\) is observed. If \\(T\\) is sufficient and \\(\\eta^{(i)}(t)=E[\\delta^{(i)}(X)\\mid t]\\), the procedures \\(\\delta\\) and \\(\\eta\\) have identical risk functions.] [More general versions of this result are discussed, for example, by Elfving (1952), Bahadur (1955), Burkholder (1961), LeCam (1964), and Roy and Ramamoorthi (1979).]\n\n**Problem 2.14**: Let \\(X_{i}\\) (\\(i=1,\\,\\ldots,\\,s\\)) be independently distributed with Poisson distribution \\(P(\\lambda_{i})\\), and let \\(T_{0}=\\sum X_{j}\\), \\(T_{i}=X_{i}\\), \\(\\lambda=\\sum\\lambda_{j}\\). Then \\(T_{0}\\) has the Poisson distribution \\(P(\\lambda)\\), and the conditional distribution of \\(T_{1},\\,\\ldots,\\,T_{s-1}\\) given \\(T_{0}=t_{0}\\) is the multinomial distribution (2.34) with \\(n=t_{0}\\) and \\(p_{i}=\\lambda_{i}/\\lambda\\).\n\n**Problem 2.15**: _Life testing._ Let \\(X_{1},\\,\\ldots,\\,X_{n}\\) be independently distributed with exponential density \\((2\\theta)^{-1}e^{-x/2\\theta}\\) for \\(x\\geq 0\\), and let the ordered \\(X\\)'s be denoted by \\(Y_{1}\\leq Y_{2}\\leq\\cdots\\leq Y_{n}\\). It is assumed that \\(Y_{1}\\) becomes available first, then \\(Y_{2}\\), and so on, and that observation is continued until \\(Y_{r}\\) has been observed. This might arise, for example, in life testing where each \\(X\\) measures the length of life of, say, an electron tube, and \\(n\\) tubes are being tested simultaneously. Another application is to the disintegration of radioactive material, where \\(n\\) is the number of atoms, and observation is continued until \\(r\\)\\(\\alpha\\)-particles have been emitted.\n\n* The joint distribution of \\(Y_{1},\\,\\ldots,\\,Y_{r}\\) is an exponential family with density \\[\\frac{1}{(2\\theta)^{r}}\\,\\frac{n!}{(n-r)!}\\exp\\left[-\\frac{\\sum\\limits_{i=1} ^{r}y_{i}+(n-r)y_{r}}{2\\theta}\\right],\\qquad 0\\leq y_{1}\\leq\\cdots\\leq y_{r}.\\]* The distribution of \\([\\sum_{i=1}^{r}Y_{i}+(n-r)Y_{r}]/\\theta\\) is \\(\\chi^{2}\\) with \\(2r\\) degrees of freedom.\n* Let \\(Y_{1}\\), \\(Y_{2},\\ldots\\) denote the time required until the first, second, \\(\\ldots\\) event occurs in a Poisson process with parameter \\(1/2\\theta^{\\prime}\\) (see Problem 1.1). Then \\(Z_{1}=Y_{1}/\\theta^{\\prime}\\), \\(Z_{2}=(Y_{2}-Y_{1})/\\theta^{\\prime}\\), \\(Z_{3}=(Y_{3}-Y_{2})/\\theta^{\\prime}\\), \\(\\ldots\\) are independently distributed as \\(\\chi^{2}\\) with \\(2\\) degrees of freedom, and the joint density \\(Y_{1}\\), \\(\\ldots\\), \\(Y_{r}\\) is an exponential family with density \\[\\frac{1}{(2\\theta^{\\prime})^{r}}\\exp\\left(-\\frac{y_{r}}{2\\theta^{\\prime}} \\right),\\hskip 19.916929pt0\\leq y_{1}\\leq\\cdots\\leq y_{r}.\\] The distribution of \\(Y_{r}/\\theta^{\\prime}\\) is again \\(\\chi^{2}\\) with \\(2r\\) degrees of freedom.\n* The same model arises in the application to life testing if the number \\(n\\) of tubes is held constant by replacing each burned-out tube with a new one, and if \\(Y_{1}\\) denotes the time at which the first tube burns out, \\(Y_{2}\\) the time at which the second tube burns out, and so on, measured from some fixed time.\n\n[(ii): The random variables \\(Z_{i}=(n-i+1)(Y_{i}-Y_{i-1})/\\theta\\) (\\(i=1,2,\\ldots,r\\)) are independently distributed as \\(\\chi^{2}\\) with \\(2\\) degrees of freedom, and \\([\\sum_{i=1}^{r}Y_{i}+(n-r)Y_{r}/\\theta=\\sum_{i=1}^{r}Z_{i}\\).]\n\n**Problem 2.16**: For any \\(\\theta\\) which is an interior point of the natural parameter space, the expectations and covariances of the statistics \\(T_{j}\\) in the exponential family (2.35) are given by\n\n\\[E\\left[T_{j}(X)\\right]=-\\frac{\\partial\\log C(\\theta)}{\\partial\\theta_{j}} \\hskip 19.916929pt(j=1,\\ldots,k),\\]\n\n\\[E\\left[T_{i}(X)T_{j}(X)\\right]-\\left[ET_{i}(X)ET_{j}(X)\\right]=-\\frac{ \\partial^{2}\\log C(\\theta)}{\\partial\\theta_{i}\\partial\\theta_{j}}\\hskip 19.916929pt (i,j=1,\\ldots,k).\\]\n\n**Problem 2.17**: Let \\(\\Omega\\) be the natural parameter space of the exponential family (2.35), and for any fixed \\(t_{r+1}\\), \\(\\ldots\\), \\(t_{k}\\) (\\(r<k\\)) let \\(\\Omega^{\\prime}_{\\theta_{1}\\ldots\\theta_{r}}\\) be the natural parameter space of the family of conditional distributions given \\(T_{r+1}=t_{r+1}\\), \\(\\ldots\\), \\(T_{k}=t_{k}\\).\n\n* Then \\(\\Omega^{\\prime}_{\\theta_{1},\\ldots,\\theta_{r}}\\) contains the projection \\(\\Omega_{\\theta_{1},\\ldots,\\theta_{r}}\\) of \\(\\Omega\\) onto \\(\\theta_{1}\\), \\(\\ldots\\), \\(\\theta_{r}\\).\n* An example in which \\(\\Omega_{\\theta_{1},\\ldots,\\theta_{r}}\\) is a proper subset of \\(\\Omega^{\\prime}_{\\theta_{1},\\ldots,\\theta_{r}}\\) is the family of densities \\[p_{\\theta_{1}\\theta_{2}}(x,y)=C(\\theta_{1},\\theta_{2})\\exp(\\theta_{1}x+\\theta _{2}y-xy),\\hskip 19.916929ptx,y>0.\\]"
    },
    {
        "raw_response": "{\n  \"问题1\": \"在统计学中，什么是累积分布函数连续且不减的定理（Theorem 2.5.1）？\",\n  \"回答1\": \"累积分布函数连续且不减的定理指出，在n维样本空间中，当累积分布函数满足一定条件时，其概率测度可以表示为一系列区间的概率之和。这一定理对于推导概率分布的性质和应用具有重要意义。\",\n  \"问题2\": \"在统计学中，什么是充分统计量（sufficient statistic）？\",\n  \"回答2\": \"充分统计量是指在已知充分统计量的条件下，样本所提供的关于参数的所有信息都得到了完全捕捉。换句话说，充分统计量将样本中含有的参数信息提炼出来，可以代替整个样本对参数进行推断。\",\n  \"问题3\": \"在统计学中，给定概率分布为P0和P1时，如何计算在P1下Y关于测度μ的概率密度函数？\",\n  \"回答3\": \"在概率分布为P0和P1的情况下，可以通过计算P1下Y关于测度μ的条件期望来得到Y关于μ的概率密度函数。具体而言，使用P0的期望值和P1与P0的比值，可以计算得到Y关于μ的概率密度函数。\",\n  \"问题4\": \"在统计学中，什么是条件期望函数（conditional expectation function）？\",\n  \"回答4\": \"条件期望函数是指在给定某一条件下，对随机变量的期望进行的条件推断。在统计学中，通过条件期望函数可以更好地理解随机变量在不同条件下的期望表现，具有重要的理论和实际应用价值。\",\n  \"问题5\": \"在统计学中，为什么充分统计量对于决策问题的类别具有重要意义？\",\n  \"回答5\": \"充分统计量对于决策问题的类别具有重要意义，因为充分统计量可以帮助简化决策问题的复杂性，从而使得决策程序更加简洁和高效。使用充分统计量可以大大减少决策问题的处理难度，提高决策的精确度。\",\n  \"问题6\": \"在统计学中，什么是多项分布（multinomial distribution）？\",\n  \"回答6\": \"多项分布是指在多个相互独立的试验中，每个试验有多个可能结果的概率分布。在统计学中，多项分布常用于描述多分类问题中各个类别的出现概率，具有广泛的应用。\",\n  \"问题7\": \"在统计学中，指数家族（exponential family）的期望和协方差如何计算？\",\n  \"回答7\": \"对于指数家族中的统计量，其期望和协方差可以通过对对数概率密度函数进行一阶和二阶偏导数计算而得。这些期望和协方差的计算对于理解指数家族的性质和在推断中的应用具有重要意义。\",\n  \"问题8\": \"在统计学中，什么是条件分布（conditional distribution）？\",\n  \"回答8\": \"条件分布是指在给定某一条件下，随机变量的分布情况。通过条件分布，可以更好地理解在特定条件下随机变量的分布特征，对于理解随机现象的多样性和推断分析具有重要意义。\",\n  \"问题9\": \"在统计学中，如何计算指数家族中的条件分布的自然参数空间？\",\n  \"回答9\": \"计算指数家族中的条件分布的自然参数空间可以通过将自然参数投影到相应的条件参数空间来实现。这一过程可以帮助理解在给定条件下的参数空间结构，对于推断和决策问题的分析具有重要意义。\",\n  \"问题10\": \"在统计学中，指数家族中的随机变量如何表现为卡方分布（chi-square distribution）？\",\n  \"回答10\": \"在指数家族中，随机变量可以表现为卡方分布，即满足卡方分布特性。通过卡方分布的性质，可以更好地理解和描述指数家族中的随机变量的分布特征，对于推断和决策问题具有重要意义。\"\n}"
    }
]