[MISSING_PAGE_EMPTY:1]

_Springer Texts in Statistics (STS)_ includes advanced textbooks from 3rd- to 4th-year undergraduate courses to 1st- to 2nd-year graduate courses. Exercise sets should be included. The series editors are currently Geneva I. Allen, Richard D. De Veaux, and Rebecca Nugent. Stephen Fienberg, George Casella, and Ingram Olkin were editors of the series for many years.

More information about this series at [http://www.springer.com/series/417](http://www.springer.com/series/417)Ronald Christensen

## Plane Answers to Complex Questions

The Theory of Linear Models

Fifth EditionRonald Christensen

Department of Mathematics and Statistics

University of New Mexico

Albuquerque, NM, USA

ISSN 1431-875X

ISSN 2197-4136 (electronic)

Springer Texts in Statistics

ISBN 978-3-030-32096-6

ISBN 978-3-030-32097-3 (eBook)

[https://doi.org/10.1007/978-3-030-32097-3](https://doi.org/10.1007/978-3-030-32097-3)

4th edition \(\copyright\) Springer Science+Business Media, LLC 2011

3rd edition \(\copyright\) Springer Science+Business Media New York, 2002

2nd edition \(\copyright\) Springer Science+Business Media New York, 1996

1st edition \(\copyright\) Springer Science+Business Media New York, 1987

5th edition \(\copyright\) Springer Nature Switzerland AG 2020

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

This Springer imprint is published by the registered company Springer Nature Switzerland AG

The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandTo Dad, Mom, Sharon, Fletch, and Don 

## Preface to the Fifth Edition

I prepared the fifth edition of _Plane Answers (PA-V)_ in conjunction with a new edition of _Advanced Linear Modeling (ALM-III)_ (Christensen, 2019). The emphasis in both revisions was to include more material on _Statistical Learning_. _ALM-III_ has far more changes in it than _PA-V_. (_ALM-III_ is about 50% longer than _ALM-II_.) In _ALM-III_, all but the first three chapters (and Chapter 13) are devoted to dependent data. I regretfully concluded that almost all of the mixed models chapter in _PA_ needed to go into _ALM-III_. The one exception is that I moved the discussion of BLUP into Chapter 6 of _PA-V_.

The biggest changes in _PA-V_ are listed below:

* Section 1.3 has been restructured to isolate the more difficult parts.
* Section 2.9 is a new section on biased estimation and the variance-bias tradeoff.
* Subsection 3.2.1 is a short new subsection that introduces the importance of small \(F\) statistics.
* A new Exercise 3.7b helps establish Fieller's method prior to its application in Exercises 6.9.1, 2, 3.
* Section 4.1 contains some cleaner notation for one-way ANOVA computations.
* Section 5.1 is a new section containing my overall view of common multiple comparison procedures.
* Subsubsection 6.3.3.2 discusses best predictors for loss functions other than squared error.
* Section 6.6 now contains the discussion of BLUP.
* The section on polynomial regression and one-way ANOVA now contains a table of polynomial contrasts.
* Subsection 7.5.3 contains new material on characterizing the interaction space in an unbalanced two-way ANOVA.

* Subsection 9.1.1 introduces ACOVA ideas for models with dependent or heteroscedastic data.
* I thought about just deleting Section 9.3 but opted for attempting to make it more relevant.
* Section 11.2 has some new results on checking whether models qualify as generalized split plot models.
* New Subsection 11.2.3 addresses the analysis of (generalized) split plot designs when there is missing data in the subplots.
* As mentioned, mixed models got moved to _ALM-III_ because it fits naturally into _ALM-III_'s emphasis on dependent data.
* Subsection 12.4.1 includes additional discussion of testing for heteroscedasticity.
* Subsection 12.4.2 introduces the Huber-White sandwich estimator.
* I changed the order of the chapters on variable selection and collinearity from previous versions of _PA_ in order to smooth the presentation with _ALM-III_.
* The collinearity chapter contains a new Section 13.2 on variance inflation factors.
* The singular value decomposition of a matrix \(X\), Theorem 13.3.1, has been generalized and the relationship between ridge regression and principal component regression explicated.
* Section 13.6 is a new section on different approaches to estimation. While this stands on its own, it also motivates material in _ALM-III_.
* Section 14.1 now discusses information criteria for model selection as well as cost complexity pruning.
* Section 14.2 has examples illustrating issues with larger data sets.
* Section 14.3 contains more discussion of variable selection.
* Section 14.4 introduces _boosting_, _bagging_ and the random part of _random forests_. The application of these subjects is closely related to nonparametric regression as discussed in Chapter 1 of _ALM-III_.
* Appendix B has a number of refinements in the results. I also decided to rename "orthogonal matrices" as "orthonormal matrices" because it is a clearly better name.
* Appendix D has a new section on identifiability.

A big part of the effort in producing _PA-V_ was just cleaning the text. After 30 years you would think, by now, I would be happy with it.

While _PA_ is a book on Linear Model Theory, Christensen (2015) illustrates the use of most of the theory presented in this book. There are a number of related topics discussed on my website [http://www.stat.unm.edu/](http://www.stat.unm.edu/)\(\sim\)fletcher/ in various places. These include computer code for the applications book as well as for _ALM-III_, cf. [http://www.stat.unm.edu/](http://www.stat.unm.edu/)\(\sim\)fletcher/Rcode.pdf and [http://www.stat.unm.edu/](http://www.stat.unm.edu/)\(\sim\)fletcher/R-ALMIIII.pdf.

I have quite assiduously avoided doing asymptotic theory in _PA_, and that remains true in _PA-V_. There are many sources that discuss asymptotics for linear model theory. The appendix to Christensen and Lin (2015) uses a number of the most important results.

I would like to thank Fletcher Christensen and Joe Cavanaugh both of whom I have used as a sounding board for years on linear model issues. I thank Mohammad Hattab for numerous suggestions. Since the last edition of the book, Steve Fienberg and Ingram Olkin have both died. They were the subject matter editors for Springer when \(PA\) was first published in 1987. I sent the book to a lot of publishers and Steve was the only person who took seriously the efforts of an assistant professor from Montana State University. (Steve had been one of my professors at the University of Minnesota.) He recommended it to Ingram who both liked it a lot and gave me a large number of suggestions (virtually all of which remain in the book). I owe both of them a great debt!

Please note that while most of this book's examples, exercises, and figures draw from real data and are cited accordingly, a few of them are based on simulated data.

Some people think that Plane Answers is an example of the old maxim, "If all you have is a hammer, everything looks like a nail." I prefer to think that if you have a good enough hammer, almost everything actually is a nail.

Ronald Christensen

Albuquerque, New Mexico, 2018

## Preface to the Fourth Edition

As with the prefaces to the second and third editions, this focuses on changes to the previous edition. The preface to the first edition discusses the core of the book.

Two substantial changes have occurred in Chapter 3. Subsection 3.3.2 uses a simplified method of finding the reduced model and includes some additional discussion of applications. In testing the generalized least squares models of Section 3.8, even though the data may not be independent or homoscedastic, there are conditions under which the standard \(F\) statistic (based on those assumptions) still has the standard \(F\) distribution under the reduced model. Section 3.8 contains a new subsection examining such conditions.

The major change in the fourth edition has been a more extensive discussion of best prediction and associated ideas of \(R^{2}\) in Sections 6.3 and 6.4. It also includes a nice result that justifies traditional uses of residual plots. One portion of the new material is viewing best predictors (best linear predictors) as perpendicular projections of the dependent random variable \(y\) into the space of random variables that are (linear) functions of the predictor variables \(x\). A new subsection on inner products and perpendicular projections for more general spaces facilitates the discussion. While these ideas were not new to me, their inclusion here was inspired by deLaubenfels (2006).

Section 9.1 has an improved discussion of least squares estimation in ACOVA models. A new Section 9.5 examines Milliken and Graybill's generalization of Tukey's one degree of freedom for nonadditivity test.

A new Section 10.5 considers estimable parameters that can be known with certainty when \(C(X)\not\subset C(V)\) in a general Gauss-Markov model. It also contains a relatively simple way to estimate estimable parameters that are not known with certainty. The nastier parts in Sections 10.1-10.4 are those that provide sufficient generality to allow \(C(X)\not\subset C(V)\). The approach of Section 10.5 seems more appealing.

In Sections 12.4 and 12.6, the point is now made that ML and REML methods can also be viewed as method of moments or estimating equations procedures.

The biggest change in Chapter 13 is a new title. The plots have been improved and extended. At the end of Section 13.6, some additional references are given on case deletions for correlated data as well as an efficient way of computing case deletion diagnostics for correlated data.

The old Chapter 14 has been divided into two chapters, the first on variable selection and the second on collinearity and alternatives to least squares estimation. Chapter 15 includes a new section on penalized estimation that discusses both ridge and lasso estimation and their relation to Bayesian inference. There is also a new section on orthogonal distance regression that finds a regression line by minimizing orthogonal distances, as opposed to least squares, which minimizes vertical distances.

Appendix D now contains a short proof of the claim: If the random vectors \(x\) and \(y\) are independent, then any vector-valued functions of them, say \(g(x)\) and \(h(y)\), are also independent.

Another significant change is that I wanted to focus on Fisherian inference, rather than the previous blend of Fisherian and Neyman-Pearson inference. In the interests of continuity and conformity, the differences are soft-pedaled in most of the book. They arise notably in new comments made after presenting the traditional (one-sided) \(F\) test in Section 3.2 and in a new Subsection 5.6.1 on multiple comparisons. The Fisherian viewpoint is expanded in Appendix F, which is where it primarily occurred in the previous edition. But the change is most obvious in Appendix E. In all previous editions, Appendix E existed just in case readers did not already know the material. While I still expect most readers to know the "how to" of Appendix E, I no longer expect most to be familiar with the "why" presented there.

Other minor changes are too numerous to mention and, of course, I have corrected all of the typographic errors that have come to my attention. Comments by Jarrett Barber led me to clean up Definition 2.1.1 on identifiability.

My thanks to Fletcher Christensen for general advice and for constructing Figures 10.1 and 10.2. (Little enough to do for putting a roof over his head all those years. :-)

Ronald Christensen

Albuquerque, New Mexico, 2010

### Preface to the Third Edition

The third edition of _Plane Answers_ includes fundamental changes in how some aspects of the theory are handled. Chapter 1 includes a new section that introduces generalized linear models. Primarily, this provides a definition so as to allow comments on how aspects of linear model theory extend to generalized linear models.

For years, I have been unhappy with the concept of estimability. Just because you cannot get a linear unbiased estimate of something does not mean you cannot estimate it. For example, it is obvious how to estimate the ratio of two contrasts in an ANOVA, just estimate each one and take their ratio. The real issue is that if the model matrix \(X\) is not of full rank, the parameters are not identifiable. Section 2.1 now introduces the concept of identifiability and treats estimability as a special case of identifiability. This change also resulted in some minor changes in Section 2.2.

In the second edition, Appendix F presented an alternative approach to dealing with linear parametric constraints. In this edition, I have used the new approach in Section 3.3. I think that both the new approach and the old approach have virtues, so I have left a fair amount of the old approach intact.

Chapter 8 contains a new section with a theoretical discussion of models for factorial treatment structures and the introduction of special models for homologous factors. This is closely related to the changes in Section 3.3.

In Chapter 9, reliance on the normal equations has been eliminated from the discussion of estimation in ACOVA models--something I should have done years ago! In the previous editions, Exercise 9.3 has indicated that Section 9.1 should be done with projection operators, not normal equations. I have finally changed it. (Now Exercise 9.3 is to redo Section 9.1 with normal equations.)

Appendix F now discusses the meaning of small \(F\) statistics. These can occur because of model lack of fit that exists in an unsuspected location. They can also occur when the mean structure of the model is fine but the covariance structure has been misspecified.

In addition, there are various smaller changes including the correction of typographical errors. Among these are very brief introductions to nonparametric regression and generalized additive models, as well as Bayesian justifications for the mixed model equations and classical ridge regression. I will let you discover the other changes for yourself.

Ronald Christensen

Albuquerque, New Mexico, 2001

## Preface to the Second Edition

The second edition of _Plane Answers_ has many additions and a couple of deletions. New material includes additional illustrative examples in Appendices A and B and Chapters 2 and 3, as well as discussions of Bayesian estimation, near replicate lack of fit tests, testing the independence assumption, testing variance components, the interblock analysis for balanced incomplete block designs, nonestimable constraints, analysis of unreplicated experiments using normal plots, tensors, and properties of Kronecker products and Vec operators. The book contains an improved discussion of the relation between ANOVA and regression, and an improved presentation of general Gauss-Markov models. The primary material that has been deleted are the discussions of weighted means and of log-linear models. The material on log-linear models was included in Christensen (1997), so it became redundant here. Generally, I have tried to clean up the presentation of ideas wherever it seemed obscure to me.

Much of the work on the second edition was done while on sabbatical at the University of Canterbury in Christchurch, New Zealand. I would particularly like to thank John Deely for arranging my sabbatical. Through their comments and criticisms, four people were particularly helpful in constructing this new edition. I would like to thank Wes Johnson, Snehalata Huzurbazar, Ron Butler, and Vance Berger.

Ronald Christensen

Albuquerque, New Mexico, 1996

## Preface to the First Edition

This book was written to rigorously illustrate the practical application of the projective approach to linear models. To some, this may seem contradictory. I contend that it is possible to be both rigorous and illustrative, and that it is possible to use the projective approach in practical applications. Therefore, unlike many other books on linear models, the use of projections and subspaces does not stop after the general theory. They are used wherever I could figure out how to do it. Solving normal equations and using calculus (outside of maximum likelihood theory) are anathema to me. This is because I do not believe that they contribute to the understanding of linear models. I have similar feelings about the use of side conditions. Such topics are mentioned when appropriate and thenceforward avoided like the plague.

On the other side of the coin, I just as strenuously reject teaching linear models with a coordinate free approach. Although Joe Eaton assures me that the issues in complicated problems frequently become clearer when considered free of coordinate systems, my experience is that too many people never make the jump from coordinate free theory back to practical applications. I think that coordinate free theory is better tackled after mastering linear models from some other approach. In particular, I think it would be very easy to pick up the coordinate free approach after learning the material in this book. See Eaton (1983) for an excellent exposition of the coordinate free approach.

By now it should be obvious to the reader that I am not very opinionated on the subject of linear models. In spite of that fact, I have made an effort to identify sections of the book where I express my personal opinions.

Although in recent revisions I have made an effort to cite more of the literature, the book contains comparatively few references. The references are adequate to the needs of the book, but no attempt has been made to survey the literature. This was done for two reasons. First, the book was begun about 10 years ago, right after I finished my Masters degree at the University of Minnesota. At that time, I was not aware of much of the literature. The second reason is that this book emphasizes a particular point of view. A survey of the literature would best be done on the literature's own terms. In writing this, I ended up reinventing a lot of wheels. My apologies to anyone whose work I have overlooked.

### Using the Book

This book has been extensively revised, and the last five chapters were written at Montana State University. At Montana State, we require a year of Linear Models for all of our statistics graduate students. In our three-quarter course, I usually end the first quarter with Chapter 4 or in the middle of Chapter 5. At the end of winter quarter, I have finished Chapter 9. I consider the first nine chapters to be the corematerial of the book. I go quite slowly because all of our Masters students are required to take the course. For Ph.D. students, I think a one-semester course might be the first nine chapters, and a two-quarter course might have time to add some topics from the remainder of the book.

I view the chapters after 9 as a series of important special topics from which instructors can choose material but which students should have access to even if their course omits them. In our third quarter, I typically cover (at some level) Chapters 11 to 14. The idea behind the special topics is not to provide an exhaustive discussion but rather to give a basic introduction that will also enable readers to move on to more detailed works such as Cook and Weisberg (1982) and Haberman (1974).

Appendices A-E provide required background material. My experience is that the student's greatest stumbling block is linear algebra. I would not dream of teaching out of this book without a thorough review of Appendices A and B.

The main prerequisite for reading this book is a good background in linear algebra. The book also assumes knowledge of mathematical statistics at the level of, say, Lindgren or Hogg and Craig. Although I think a mathematically sophisticated reader could handle this book without having had a course in statistical methods, I think that readers who have had a methods course will get much more out of it.

The exercises in this book are presented in two ways. In the original manuscript, the exercises were incorporated into the text. The original exercises have not been relocated. It has been my practice to assign virtually all of these exercises. At a later date, the editors from Springer-Verlag and I agreed that other instructors might like more options in choosing problems. As a result, a section of additional exercises was added to the end of the first nine chapters and some additional exercises were added to other chapters and appendices. I continue to recommend requiring nearly all of the exercises incorporated in the text. In addition, I think there is much to be learned about linear models by doing, or at least reading, the additional exercises.

Many of the exercises are provided with hints. These are primarily designed so that I can quickly remember how to do them. If they help anyone other than me, so much the better.

### Acknowledgements

I am a great believer in books. The vast majority of my knowledge about statistics has been obtained by starting at the beginning of a book and reading until I covered what I had set out to learn. I feel both obligated and privileged to thank the authors of the books from which I first learned about linear models: Daniel and Wood, Draper and Smith, Scheffe, and Searle.

In addition, there are a number of people who have substantially influenced particular parts of this book. Their contributions are too diverse to specify, but I should mention that, in several cases, their influence has been entirely by means of their written work. (Moreover, I suspect that in at least one case, the person in question will be loath to find that his writings have come to such an end as this.) I would like to acknowledge Kit Bingham, Carol Bittinger, Larry Blackwood, Dennis Cook, Somesh Das Gupta, Seymour Geisser, Susan Groshen, Shelby Haberman, David Harville, Cindy Hertzler, Steve Kachman, Kinley Larrtz, Dick Lund, Ingram Olkin, S. R. Searle, Anne Torbeyns, Sandy Weisberg, George Zyskind, and all of my students. Three people deserve special recognition for their pains in advising me on the manuscript: Robert Boik, Steve Fienberg, and Wes Johnson.

The typing of the first draft of the manuscript was done by Laura Cranmer and Donna Stickney.

I would like to thank my family: Sharon, Fletch, George, Doris, Gene, and Jim, for their love and support. I would also like to thank my friends from graduate school who helped make those some of the best years of my life.

Finally, there are two people without whom this book would not exist: Frank Martin and Don Berry. Frank because I learned how to think about linear models in a course he taught. This entire book is just an extension of the point of view that I developed in Frank's class. And Don because he was always there ready to help--from teaching my first statistics course to being my thesis adviser and everywhere in between.

Since I have never even met some of these people, it would be most unfair to blame anyone but me for what is contained in the book. (Of course, I will be more than happy to accept any and all praise.) Now that I think about it, there may be one exception to the caveat on blame. If you don't like the diatribe on prediction in Chapter 6, you might save just a smidgen of blame for Seymour (even though he did not see it before publication).

## References

* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* Christensen (2019) Christensen, R. (2019). _Advanced linear modeling III: Statistical learning and dependent data_ (3rd ed.). Springer-Verlag, New York.
* Christensen & Lin (2015) Christensen, R. & Lin, Y. (2015). Lack-of-fit tests based on partial sums of residuals. _Communications in Statistics, Theory and Methods_, _44_ 2862-2880.
* Cook & Weisberg (1982) Cook, R. D., & Weisberg, S. (1982). _Residuals and influence in regression_. New York: Chapman and Hall.
* deLaubenfels (2006) deLaubenfels, R. (2006). The victory of least squares and orthogonality in statistics. _The American Statistician_, _60_, 315-321.
* Eaton (1983) Eaton, M. L. (1983). _Multivariate statistics: a vector space approach_. New York: Wiley. Reprinted in 2007 by IMS Lecture Notes-Monograph Series.
* Haberman (1974) Haberman, S. J. (1974). The Analysis of Frequency Data. University of Chicago Press, Chicago.
* Haberman (1987)

[MISSING_PAGE_FAIL:17]

	* 3.3 Testing Linear Parametric Functions
		* 3.3.1 A Generalized Test Procedure
		* 3.3.2 Testing an Unusual Class of Hypotheses
	* 3.4 Discussion
	* 3.5 Testing Single Degrees of Freedom in a Given Subspace
	* 3.6 Breaking a Sum of Squares into Independent Components
		* 3.6.1 General Theory
		* 3.6.2 Two-Way ANOVA
	* 3.7 Confidence Regions
	* 3.8 Tests for Generalized Least Squares Models
		* 3.8.1 Conditions for Simpler Procedures
	* 3.9 Additional Exercises
* 3.10 References
* 4 One-Way ANOVA
	* 4.1 Analysis of Variance
	* 4.2 Estimating and Testing Contrasts
	* 4.3 Additional Exercises
* 5 Multiple Comparison Techniques
	* 5.1 Basic Ideas
	* 5.2 Scheffe's Method
	* 5.3 Least Significant Difference Method
	* 5.4 Bonferroni Method
	* 5.5 Tukey's Method
	* 5.6 Multiple Range Tests: Newman-Keuls and Duncan
	* 5.7 Summary
		* 5.7.1 Fisher Versus Neyman-Pearson
	* 5.8 Additional Exercises
		* 5.8.1 References
* 6 Regression Analysis
	* 6.1 Simple Linear Regression
	* 6.2 Multiple Regression
		* 6.2.1 Partitioned Model
		* 6.2.2 Nonparametric Regression and Generalized Additive Models
	* 6.3 General Prediction Theory
		* 6.3.1 Discussion
		* 6.3.2 General Prediction
		* 6.3.3 Best Prediction
		* 6.3.4 Best Linear Prediction
		* 6.3.5 Inner Products and Orthogonal Projections in General Spaces
	* 6.4 Multiple Correlation

###### Contents

		* 6.4.1 Squared Predictive Correlation
	* 6.5 Partial Correlation Coefficients
	* 6.6 Best Linear Unbiased Prediction
	* 6.7 Testing Lack of Fit
		* 6.7.1 The Traditional Test
		* 6.7.2 Near Replicate Lack of Fit Tests
		* 6.7.3 Partitioning Methods
		* 6.7.4 Nonparametric Methods
	* 6.8 Polynomial Regression and One-Way ANOVA
	* 6.9 Additional Exercises
* 7 Multifactor Analysis of Variance
	* 7.1 Balanced Two-Way ANOVA Without Interaction
		* 7.1.1 Contrasts
	* 7.2 Balanced Two-Way ANOVA with Interaction
		* 7.2.1 Interaction Contrasts
	* 7.3 Polynomial Regression and the Balanced Two-Way ANOVA ANOVA
	* 7.4 Two-Way ANOVA with Proportional Numbers
	* 7.5 Two-Way ANOVA with Unequal Numbers: General Case
		* 7.5.1 Without Interaction
		* 7.5.2 Interaction
		* 7.5.3 Characterizing the Interaction Space
	* 7.6 Three or More Way Analyses
		* 7.6.1 Balanced Analyses
		* 7.6.2 Unbalanced Analyses
	* 7.7 Additional Exercises
	* 7.8 Experimental Design Models
	* 8.1 Completely Randomized Designs
	* 8.2 Randomized Complete Block Designs: Usual Theory
	* 8.3 Latin Square Designs
	* 8.4 Factorial Treatment Structures
	* 8.5 More on Factorial Treatment Structures
	* 8.6 Additional Exercises
* 9 Analysis of Covariance
	* 9.1 Estimation of Fixed Effects
		* 9.1.1 Generalized Least Squares
	* 9.2 Estimation of Error and Tests of Hypotheses
	* 9.3 Another Adjusted Model and Missing Data
	* 9.4 Balanced Incomplete Block Designs
* 9.5 Testing a Nonlinear Full Model 276 9.6 Additional Exercises 277 10 General Gauss-Markov Models 281 10.1 BLUEs with an Arbitrary Covariance Matrix 282 10.2 Geometric Aspects of Estimation 288 10.3 Hypothesis Testing 291 10.4 Least Squares Consistent Estimation 296 10.5 Perfect Estimation and More 304 11.1.1 The \(\mathbf{Split}\) Plot Models 313 11.1 A Cluster Sampling Model 314 11.2 Generalized Split Plot Models 318 11.2.1 Estimation and Testing of Estimable Functions 322 11.2.2 Testing Models 326 11.2.3 Unbalanced Subplots 328 11.3 The Split Plot Design 328 11.4 Identifying the Appropriate Error 332 11.4.1 Subsampling 332 11.4.2 Two-Way ANOVA with Interaction 335 11.5 Exercise: An Unusual Split Plot Analysis 337 References
* 12 Model Diagnostics 341 12.1 Leverage 344 12.1.1 Mahalanobis Distances 345 12.1.2 Diagonal Elements of the Projection Operator 347 12.1.3 Examples 348 12.2 Checking Normality 354 12.2.1 Other Applications for Normal Plots 360 12.3 Checking Independence 362 12.3.1 Serial Correlation 363 12.4 Heteroscedasticity and Lack of Fit 369 12.4.1 Heteroscedasticity 369 12.4.2 Huber-White (Robust) Sandwich Estimator 374 12.4.3 Lack of Fit 377 12.4.4 Residual Plots 380 12.5 Updating Formulae and Predicted Residuals 380 12.6 Outliers and Influential Observations 384 12.7 Transformations

###### Contents

* 13 Collinearity and Alternative Estimates
	* 13.1 Defining Collinearity
	* 13.2 Tolerance and Variance Inflation Factors
	* 13.3 Regression in Canonical Form and on Principal Components
		* 13.3.1 Regression in Canonical Form
		* 13.3.2 Principal Component Regression
		* 13.3.3 Generalized Inverse Regression
	* 13.4 Classical Ridge Regression
		* 13.4.1 Ridge Applied to Principal Components
	* 13.5 More on Mean Squared Error
	* 13.6 Robust Estimation and Alternative Distance Measures
	* 13.7 Orthogonal Regression
* 14 Variable Selection
	* 14.1 All Possible Regressions and Best Subset Regression
		* 14.1.1 \(R^{2}\)
		* 14.1.2 Adjusted \(R^{2}\)
		* 14.1.3 Mallows's \(C_{p}\)
		* 14.1.4 Information Criteria: AIC, BIC
		* 14.1.5 Cost Complexity Pruning
	* 14.2 Stepwise Regression
		* 14.2.1 Traditional Forward Selection
		* 14.2.2 Backward Elimination
		* 14.2.3 Other Methods
	* 14.3 Discussion of Traditional Variable Selection Techniques
		* 14.3.1 \(R^{2}\)
		* 14.3.2 Influential Observations
		* 14.3.3 Exploratory Data Analysis
		* 14.3.4 Multiplicities
		* 14.3.5 Predictive Models
		* 14.3.6 Overfitting
	* 14.4 Modern Forward Selection: Boosting, Bagging, and Random Forests
		* 14.4.1 Boosting
		* 14.4.2 Bagging
		* 14.4.3 Random Forests

## Appendix A Vector Spaces

**Appendix B: Matrix Results**

B.1 Basic Ideas

B.2 Eigenvalues and Related Results

B.3 Projections

B.4 Miscellaneous Results

B.5 Properties of Kronecker Products and Vec Operators

B.6 Tensors

B.7 Exercises

**Appendix C: Some Univariate Distributions**

**Appendix D: Multivariate Distributions**

D.1 Identifiability

**Appendix E: Inference for One Parameter**

E.1 Testing

E.2 \(P\) Values

E.3 Confidence Intervals

E.4 Final Comments on Significance Testing

**Appendix F: Significantly Insignificant Tests**

F.1 Lack of Fit and Small \(F\) Statistics

F.2 The Effect of Correlation and Heteroscedasticity on \(F\) Statistics

**Appendix G: Randomization Theory Models**

G.1 Simple Random Sampling

G.2 Completely Randomized Designs

G.3 Randomized Complete Block Designs.

**References**

**Author Index**

**Subject Index**

## Chapter 1 Introduction

### 1.1 Abstract

This chapter introduces the general linear model, illustrating how it subsumes a variety of standard applied models. It also introduces random vectors and matrices and the distributions that will be used with them. Finally, it introduces generalized linear models, which are different from general linear models. It is important to be familiar with the background in Appendices A through E before getting into the book.

This book is about the theory of linear models. A typical model considered is

\[Y=X\beta+e,\]

where \(Y\) is an \(n\times 1\) vector of random observations, \(X\) is an \(n\times p\) matrix of known constants called the _model_ (or _design_) _matrix_, \(\beta\) is a \(p\times 1\) vector of unobservable fixed parameters, and \(e\) is an \(n\times 1\) vector of unobservable random errors. Both \(Y\) and \(e\) are random vectors.

_Linear models specify the expected value of the observed data \(Y\) as a linear function of the parameter vector \(\beta\)._ To be a linear model the errors must have mean zero, i.e., \(\mathrm{E}(e)=0\). In a _standard linear model_ we assume that the errors have a common unknown variance and are uncorrelated, i.e., \(\mathrm{Cov}(e)=\sigma^{2}I\) where \(\sigma^{2}\) is some unknown parameter. (The operations \(\mathrm{E}(\cdot)\) and \(\mathrm{Cov}(\cdot)\) will be defined formally in the next section.)

Although our primary object is to explore models that can be used to predict future observable events, much of our effort will be devoted to drawing inferences, in the form of point estimates, tests, and confidence regions, about the parameters \(\beta\) and \(\sigma^{2}\). In order to get tests and confidence regions for a standard linear model, we will assume that the errors are independent with normal distributions, i.e., \(e\) has an \(n\)-dimensional normal distribution with mean vector 0 and covariance matrix \(\sigma^{2}I\), written \(e\sim N(0,\sigma^{2}I)\).

The essence of linear model theory is to decompose the observations \(Y\) into \(Y=\hat{Y}+\hat{e}\). Here \(\hat{Y}\) is a vector of _fitted values_ that contains all the information for estimating the unknown parameter vector \(\beta\). It is somewhere in the vector space spanned by the columns of the model matrix \(X\), i.e., \(\hat{Y}\in C(X)\). With any good statistical procedure, it is necessary to investigate whether the assumptions that have been made are reasonable. \(\hat{e}\) is a vector of _residuals_ that contains the information used for checking the assumptions built into the standard linear model and for estimating parameters associated with the errors. In standard linear model theory, the residuals are used to estimate the unknown variance parameter \(\sigma^{2}\). Methods for evaluating the validity of the assumptions will consist of both formal statistical tests and the informal examination of residuals. We will also consider the issue of how to select a model when several alternative models seem plausible.

Applications of linear models often fall into two special cases: _Regression Analysis_ and _Analysis of Variance_. Regression Analysis refers to models in which the matrix \(X^{\prime}X\) is nonsingular. Analysis of Variance (_ANOVA_) models are models in which the model matrix consists entirely of zeros and ones. ANOVA models are sometimes called classification models but in recent years that name has been co-opted for models in which the components of \(Y\) are binary.

#### Example 1.0.1 Simple Linear Regression.

Consider the model

\[y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i},\]

\(i=1,\ldots,6\), \((x_{1},x_{2},x_{3},x_{4},x_{5},x_{6})=(1,2,3,4,5,6)\), where the \(e_{i}\)s are independent \(N(0,\sigma^{2})\). In matrix notation we can write this as

\[\begin{bmatrix}y_{1}\\ y_{2}\\ y_{3}\\ y_{4}\\ y_{5}\\ y_{6}\end{bmatrix}=\begin{bmatrix}1&1\\ 1&2\\ 1&3\\ 1&4\\ 1&5\\ 1&6\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}+\begin{bmatrix}e_{1}\\ e_{2}\\ e_{3}\\ e_{4}\\ e_{5}\\ e_{6}\end{bmatrix}\]

\[Y=X\qquad\beta+\quad e.\]

#### Example 1.0.2 One-Way Analysis of Variance.

The model

\[y_{ij}=\mu+\alpha_{i}+e_{ij},\]

\(i=1,\ldots,3,\;\;j=1,\ldots,\;N_{i}\), \((N_{1},N_{2},N_{3})=(3,1,2)\), where the \(e_{ij}\)s are independent \(N\big{(}0,\sigma^{2}\big{)}\), can be written as \[\begin{bmatrix}y_{11}\\ y_{12}\\ y_{13}\\ y_{21}\\ y_{31}\\ y_{32}\end{bmatrix}=\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}+\begin{bmatrix}e_{11}\\ e_{12}\\ e_{13}\\ e_{21}\\ e_{31}\\ e_{32}\end{bmatrix}\]

\(Y\) = \(X\) \(\beta\) + \(e\).

Examples 1.0.1 and 1.0.2 will be used to illustrate concepts in Chapters 2 and 3.

There are a couple of useful alternative methods for writing \(Y=X\beta+e\). Write \(X\) in terms of its columns and its rows as

\[X_{n\times p}=[X_{1},\ldots,X_{p}]=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}.\]

This leads to

\[Y=\sum_{i=1}^{p}\beta_{j}X_{j}+e\]

and also, listing the elements of \(Y\) and \(e\) in the obvious way,

\[y_{i}=x_{i}^{\prime}\beta+e_{i},\quad i=1,\ldots,n.\]

The approach taken here emphasizes the use of vector spaces, subspaces, orthogonality, and projections. These and other topics in linear algebra are reviewed in Appendices A and B. _It is absolutely vital that the reader be familiar with the material presented in the first two appendices._ Appendix C contains the definitions of some commonly used distributions. Much of the notation used in the book is set in Appendices A, B, and C. To develop the distribution theory necessary for tests and confidence regions, it is necessary to study properties of the multivariate normal distribution and properties of quadratic forms. We begin with a discussion of random vectors and matrices in Section 1.

**Exercise 1.1**  Write the following models in matrix notation:

(a) Multiple regression

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\beta_{3}x_{i3}+e_{i},\]

\(i=1,\ldots,6\).

(b) Two-way ANOVA with interaction

\[y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\gamma_{ij}+e_{ijk},\]\(i=1,2,3\), \(j=1,2\), \(k=1,2\).

(c) Two-way analysis of covariance (ACOVA) with no interaction

\[y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\gamma x_{ijk}+e_{ijk},\]

\(i=1,2,3\), \(j=1,2\), \(k=1,2\).

(d) Multiple polynomial regression

\[y_{i}=\beta_{00}+\beta_{10}x_{i1}+\beta_{01}x_{i2}+\beta_{20}x_{i1}^{2}+\beta_ {02}x_{i2}^{2}+\beta_{11}x_{i1}x_{i2}+e_{i},\]

\(i=1,\ldots,6\).

### Random Matrices and Vectors

A random matrix is just a matrix that consists of random variables. In general take \(W=[w_{ij}]\) where each \(w_{ij}\), \(i=1,\ldots,r\), \(j=1,\ldots,c\), is a random variable. The expected value of \(W\) is taken elementwise, i.e.,

\[\mathrm{E}(W)\equiv[\mathrm{E}(w_{ij})].\]

Clearly, if \(W_{1}\) and \(W_{2}\) are random matrices of the same dimensions,

\[\mathrm{E}(W_{1}+W_{2})=\mathrm{E}(W_{1})+\mathrm{E}(W_{2}).\]

**Theorem 1.1.1**: _If \(W\) is an \(r\times c\) random matrix and \(A=[a_{ij}]\), \(B=[b_{ij}]\) and \(C=[c_{ij}]\) are fixed matrices of conformable sizes then_

\[\mathrm{E}(AWB+C)=A\mathrm{E}(W)B+C.\]

_Proof_ We show that every element of the matrix on the left equals every element of the matrix on the right. Take \(Q\equiv AWB+C\). It is not too difficult to see that the matrix \(Q\) consists of random variables

\[q_{ij}=c_{ij}+\sum_{h=1}^{r}\sum_{k=1}^{c}a_{ih}w_{hk}b_{kj}.\]

By the linearity of expectations for random variables,

\[\mathrm{E}(q_{ij})=c_{ij}+\sum_{h=1}^{r}\sum_{k=1}^{c}a_{ih}\mathrm{E}(w_{hk}) b_{kj}.\]

The right-hand side is the \(ij\) element of \(A\mathrm{E}(W)B+C\).

Let \(y_{1},\ldots,y_{n}\) be random variables with \(\mathrm{E}(y_{i})=\mu_{i}\), \(\mathrm{Var}(y_{i})=\sigma_{ii}\), and \(\mathrm{Cov}(y_{i},y_{j})=\sigma_{ij}\equiv\sigma_{ji}\). Writing the random variables as an \(n\)-dimensional vector \(Y\), the expected value of \(Y\) is

\[\mathrm{E}(Y)=\mathrm{E}\begin{bmatrix}y_{1}\\ y_{2}\\ \vdots\\ y_{n}\end{bmatrix}=\begin{bmatrix}\mathrm{E}(y_{1})\\ \mathrm{E}(y_{2})\\ \vdots\\ \mathrm{E}(y_{n})\end{bmatrix}=\begin{bmatrix}\mu_{1}\\ \mu_{2}\\ \vdots\\ \mu_{n}\end{bmatrix}\equiv\mu.\]

The definition of the _covariance matrix_ of \(Y\) is

\[\mathrm{Cov}(Y)\equiv\mathrm{E}\begin{bmatrix}(Y-\mu)(Y-\mu)^{\prime}\end{bmatrix} =\begin{bmatrix}\sigma_{11}&\sigma_{12}&\cdots&\sigma_{1n}\\ \sigma_{21}&\sigma_{22}&\cdots&\sigma_{2n}\\ \vdots&\vdots&\ddots&\vdots\\ \sigma_{n1}&\sigma_{n2}&\cdots&\sigma_{nn}\end{bmatrix}.\]

A random vector is referred to as singular or nonsingular depending on whether its covariance matrix is singular or nonsingular. Sometimes the covariance matrix is called the _variance-covariance matrix_ or the _dispersion matrix_.

From Theorem 1.1.1, if \(Y\) is an \(n\)-dimensional random vector, \(A\) is a fixed \(r\times n\) matrix, and \(b\) is a fixed vector in \(\mathbf{R}^{r}\), then

\[\mathrm{E}(AY+b)=A\mathrm{E}(Y)+b\]

and

\[\mathrm{Cov}(AY+b)=A\mathrm{Cov}(Y)A^{\prime}.\]

This last equality can be used to show that for any random vector \(Y\), \(\mathrm{Cov}(Y)\) is nonnegative definite. It follows that \(Y\) is nonsingular if and only if \(\mathrm{Cov}(Y)\) is positive definite.

**Exercise 1.2**: Show that \(\mathrm{Cov}(AY+b)=A\mathrm{Cov}(Y)A^{\prime}\).

**Exercise 1.3**: Show that \(\mathrm{Cov}(Y)\) is nonnegative definite for any random vector \(Y\). Hint: Consider the variance of fixed linear combinations of \(Y\), say \(a^{\prime}Y\).

The covariance of two random vectors with possibly different dimensions can be defined. If \(W_{r\times 1}\) and \(Y_{s\times 1}\) are random vectors with \(\mathrm{E}(W)=\gamma\) and \(\mathrm{E}(Y)=\mu\), then the covariance of \(W\) and \(Y\) is the \(r\times s\) matrix

\[\mathrm{Cov}(W,Y)\equiv\mathrm{E}[(W-\gamma)(Y-\mu)^{\prime}].\]In particular, \(\operatorname{Cov}(Y,\,Y)=\operatorname{Cov}(Y)\). If \(A\) and \(B\) are fixed matrices, and \(a\) and \(b\) are fixed vectors of conformable sizes, Theorem 1.1.1 quickly yields

\[\operatorname{Cov}(AW+a,\,BY+b)=A\operatorname{Cov}(W,\,Y)\,B^{\prime}.\]

Another simple consequence of the definition is:

**Theorem 1.1.2**: _If \(A\) and \(B\) are fixed matrices and \(W\) and \(Y\) are random vectors, and if \(AW\) and \(BY\) are both vectors in \(\mathbf{R}^{n}\), then, assuming that the expectations exist,_

\[\operatorname{Cov}(AW+BY)=A\operatorname{Cov}(W)A^{\prime}+B\operatorname{Cov }(Y)\,B^{\prime}+A\operatorname{Cov}(W,\,Y)\,B^{\prime}+B\operatorname{Cov}( Y,\,W)\,A^{\prime}.\]

_Proof_ Without loss of generality we can assume that \(\operatorname{E}(W)=0\) and \(\operatorname{E}(Y)=0\):

\[\operatorname{Cov}(AW+BY) = \operatorname{E}[(AW+BY)(AW+BY)^{\prime}]\] \[= A\operatorname{E}[WW^{\prime}]A^{\prime}+B\operatorname{E}[YY^ {\prime}]B^{\prime}+A\operatorname{E}[WY^{\prime}]B^{\prime}+B\operatorname{E }[YW^{\prime}]A^{\prime}\] \[= A\operatorname{Cov}(W)A^{\prime}+B\operatorname{Cov}(Y)B^{ \prime}+A\operatorname{Cov}(W,\,Y)\,B^{\prime}+B\operatorname{Cov}(Y,\,W)\,A^ {\prime}.\]

\(\square\)

**Exercise 1.4**: Let \(M\) be the perpendicular projection operator onto \(C(X)\). Show that \((I-M)\) is the perpendicular projection operator onto \(C(X)^{\perp}\). Find \(\operatorname{tr}(I-M)\) in terms of \(r(X)\). (This problem has become even easier in this edition of the book.)

**Exercise 1.5**: For a linear model \(Y=X\beta+e\); \(\operatorname{E}(e)=0\), show that \(\operatorname{E}(Y)=X\beta\). For a _standard linear model_\(Y=X\beta+e\); \(\operatorname{E}(e)=0\); \(\operatorname{Cov}(e)=\sigma^{2}I\), show that \(\operatorname{Cov}(Y)=\sigma^{2}I\).

**Exercise 1.6**: For a linear model \(Y=X\beta+e\), \(\operatorname{E}(e)=0\), \(\operatorname{Cov}(e)=\sigma^{2}I\), the residuals are

\[\hat{e}=Y-X\hat{\beta}=(I-M)Y,\]

where \(M\) is the perpendicular projection operator onto \(C(X)\). Find

(a) \(\operatorname{E}(\hat{e})\).

(b) \(\operatorname{Cov}(\hat{e})\).

(c) \(\operatorname{Cov}(\hat{e},\,MY)\).

(d) \(\operatorname{E}(\hat{e}^{\prime}\hat{e})\).

(e) Show that \(\hat{e}^{\prime}\hat{e}=Y^{\prime}Y-(Y^{\prime}M)Y\).

[Note: In Chapter 2 we will show that for a least squares estimate of \(\beta\), say \(\hat{\beta}\), we have \(MY=X\hat{\beta}\).]

(f) Rewrite (c) and (e) in terms of \(\hat{\beta}\).

### 1.2 Multivariate Normal Distributions

It is assumed that the reader is familiar with the basic ideas of multivariate distributions. A summary of these ideas is contained in Appendix D.

Let \(Z=[z_{1},\ldots,z_{n}]^{\prime}\) be a random vector with \(z_{1},\ldots,z_{n}\)_independent identically distributed (i.i.d.)_\(N(0,1)\) random variables. Note that \(\mathrm{E}(Z)=0\) and \(\mathrm{Cov}(Z)=I\). Now consider the transformation \(AZ+\mu\), for some fixed \(r\times n\) matrix \(A\), and some fixed \(r\) vector \(\mu\).

**Definition 1.2.1**: \(Y\) has an \(r\)-dimensional _multivariate normal distribution_ if \(Y\) has the same distribution as \(AZ+\mu\), i.e., \(Y\sim AZ+\mu\), for some \(n\), some fixed \(r\times n\) matrix \(A\), and some fixed \(r\) vector \(\mu\). \(\mathrm{E}(Y)=\mu\) and \(\mathrm{Cov}(Y)=AA^{\prime}\equiv V\). We indicate the multivariate normal distribution of \(Y\) by writing \(Y\sim N(\mu,V)\).

It is not clear that the notation \(Y\sim N(\mu,V)\) is well defined, i.e., that a multivariate normal distribution depends only on its mean vector and covariance matrix. We can pick \(A\) and \(B\) so that \(V=AA^{\prime}=BB^{\prime}\) where \(A\neq B\). In that case, we do not know whether to take \(Y\sim AZ+\mu\) or \(Y\sim BZ+\mu\). In fact, the number of columns in \(A\) and \(B\) need not even be the same, so the length of the vector \(Z\) could change between \(Y\sim AZ+\mu\) and \(Y\sim BZ+\mu\). We need to show that it does not matter which characterization is used. We now give such an argument based on characteristic functions. The argument is based on the fact that any two random vectors with the same characteristic function have the same distribution. Appendix D contains the definition of the characteristic function of a random vector.

**Theorem 1.2.2**: _If \(Y\sim N(\mu,V)\) and \(W\sim N(\mu,V)\), then \(Y\) and \(W\) have the same distribution._

_Proof_ Observe that

\[\varphi_{Z}(t)\equiv\mathrm{E}[\exp(it^{\prime}Z)]=\prod_{j=1}^{n}\mathrm{E}[ \exp(i\sharp z_{j})]=\prod_{j=1}^{n}\exp(-t_{j}^{2}/2)=\exp(-t^{\prime}t/2).\]

Define \(Y\sim AZ+\mu\), where \(AA^{\prime}=V\). The characteristic function of \(Y\) is

\[\varphi_{Y}(t) = \mathrm{E}[\exp(it^{\prime}Y)]=\mathrm{E}[\exp(it^{\prime}[AZ+\mu ])]\] \[= \exp(it^{\prime}\mu)\varphi_{Z}(A^{\prime}t)\] \[= \exp(it^{\prime}\mu)\exp(-t^{\prime}AA^{\prime}t/2)\] \[= \exp(it^{\prime}\mu-t^{\prime}Vt/2).\]

Similarly,\[\varphi_{W}(t)=\exp(it^{\prime}\mu-t^{\prime}Vt/2).\]

Since the characteristic functions are the same, \(Y\sim W\). 

In other words, if \(Y\) has a multivariate normal distribution, the distribution is entirely determined by the mean vector \(\mu\) and the covariance matrix \(V\) because those are the only parameters in the characteristic function.

Suppose that \(Y\) is nonsingular and that \(Y\sim N(\mu,\,V)\); then \(Y\) has a density. By definition, \(Y\) nonsingular means precisely that \(V\) is positive definite. By Corollary B.23, we can write \(V=AA^{\prime}\), with \(A\) nonsingular. Since \(Y\sim AZ+\mu\) involves a nonsingular transformation of the random vector \(Z\), which has a known density, it is quite easy to find the density of \(Y\). The density is

\[f(y)=(2\pi)^{-n/2}[\det(V)]^{-1/2}\exp[-(y-\mu)^{\prime}V^{-1}(y-\mu)/2],\]

where \(\det(V)\) is the determinant of \(V\). Figure 1 shows isobars of a normal density.

#### Exercise 1.7

Show that the function \(f(y)\) given above is the density of \(Y\) when \(Y\sim N(\mu,\,V)\) and \(V\) is nonsingular. Hint: If \(Z\) has density \(f_{Z}(z)\) and \(Y=G(Z)\), the

Figure 1: Two-dimensional normal density isobars: \(\mu=(1,2)^{\prime}\), \(v_{11}=1.0\), \(v_{12}=0.9\), \(v_{22}=2.0\)

density of \(Y\) is

\[f_{Y}(y)=f_{Z}[G^{-1}(y)]|\text{det}(\mathbf{d}G^{-1})|,\]

where \(\mathbf{d}G^{-1}\) is the derivative (matrix of partial derivatives) of \(G^{-1}\) evaluated at \(y\).

An important and useful result is that for random vectors having a joint multivariate normal distribution, the condition of having zero covariance is equivalent to the condition of independence.

**Theorem 1.2.3**: _If \(Y\sim N(\mu,\,V)\,\text{and}\,\,Y=\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}\), then \(\text{Cov}(Y_{1},\,Y_{2})=0\) if and only if \(Y_{1}\) and \(Y_{2}\) are independent._

_Proof_  Partition \(V\) and \(\mu\) to conform with \(Y\), giving \(V=\begin{bmatrix}V_{11}&V_{12}\\ V_{21}&V_{22}\end{bmatrix}\) and \(\mu=\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix}\). Note that \(V_{12}=V_{21}^{\prime}=\text{Cov}(Y_{1},\,Y_{2})\).

\(\Leftarrow\) If \(Y_{1}\) and \(Y_{2}\) are independent,

\[V_{12}=\text{E}[(Y_{1}-\mu_{1})(Y_{2}-\mu_{2})^{\prime}]=\text{E}(Y_{1}-\mu_{ 1})\text{E}(Y_{2}-\mu_{2})^{\prime}=0.\]

\(\Rightarrow\) Suppose \(\text{Cov}(Y_{1},\,Y_{2})=0\), so that \(V_{12}=V_{21}^{\prime}=0\). Using the definition of multivariate normality, we will construct a version of \(Y\) in which it is clear that \(Y_{1}\) and \(Y_{2}\) are independent. Given the uniqueness established in Theorem 1.2.2, this is sufficient to establish independence of \(Y_{1}\) and \(Y_{2}\).

Since \(Y\) is multivariate normal, by definition we can write \(Y\sim AZ+\mu\), where \(A\) is an \(r\,\times\,n\) matrix. Partition \(A\) in conformance with \(\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}\) as \(A=\begin{bmatrix}A_{1}\\ A_{2}\end{bmatrix}\) so that

\[V=\begin{bmatrix}V_{11}&V_{12}\\ V_{21}&V_{22}\end{bmatrix}=\begin{bmatrix}A_{1}A_{1}^{\prime}&A_{1}A_{2}^{ \prime}\\ A_{2}A_{1}^{\prime}&A_{2}A_{2}^{\prime}\end{bmatrix}.\]

Because \(V_{12}=0\), we have \(A_{1}A_{2}^{\prime}=0\) and

\[V=\begin{bmatrix}A_{1}A_{1}^{\prime}&0\\ 0&A_{2}A_{2}^{\prime}\end{bmatrix}.\]

Now let \(z_{1},\,z_{2},\,\ldots,\,z_{2n}\) be i.i.d. \(N(0,1)\). Define the random vectors \(Z_{1}=[z_{1},\,\ldots,\,z_{n}]^{\prime}\), \(Z_{2}=[z_{n+1},\,\ldots,\,z_{2n}]^{\prime}\), and

\[Z_{0}=\begin{bmatrix}Z_{1}\\ Z_{2}\end{bmatrix}.\]

Note that \(Z_{1}\) and \(Z_{2}\) are independent. Now consider the random vector \[W=\begin{bmatrix}A_{1}&0\\ 0&A_{2}\end{bmatrix}Z_{0}+\mu.\]

By definition, \(W\) is multivariate normal with \(\mathrm{E}(W)=\mu\) and

\[\mathrm{Cov}(W) =\begin{bmatrix}A_{1}&0\\ 0&A_{2}\end{bmatrix}\begin{bmatrix}A_{1}&0\\ 0&A_{2}\end{bmatrix}^{\prime}\] \[=\begin{bmatrix}A_{1}A_{1}^{\prime}&0\\ 0&A_{2}A_{2}^{\prime}\end{bmatrix}\] \[=V.\]

We have shown that \(W\sim N(\mu,\,V)\) and by assumption \(Y\sim N(\mu,\,V)\). By Theorem 1.2.2, \(W\) and \(Y\) have exactly the same distribution; thus

\[Y\sim\begin{bmatrix}A_{1}&0\\ 0&A_{2}\end{bmatrix}Z_{0}+\mu.\]

It follows that \(Y_{1}\sim[A_{1},0]Z_{0}+\mu_{1}=A_{1}Z_{1}+\mu_{1}\) and \(Y_{2}\sim[0,\,A_{2}]Z_{0}+\mu_{2}=A_{2}Z_{2}+\mu_{2}\). The joint distribution of \((Y_{1},\,Y_{2})\) is the same as the joint distribution of \((A_{1}Z_{1}+\mu_{1},\,A_{2}Z_{2}+\mu_{2})\). However, \(Z_{1}\) and \(Z_{2}\) are independent; thus \(A_{1}Z_{1}+\mu_{1}\) and \(A_{2}Z_{2}+\mu_{2}\) are independent, and it follows that \(Y_{1}\) and \(Y_{2}\) are independent. 

**Exercise 1.8**: Show that if \(Y\) is an \(r\)-dimensional random vector with \(Y\sim N(\mu,\,V)\) and if \(B\) is a fixed \(n\times r\) matrix, then \(BY\sim N(B\mu,\,BVB^{\prime})\).

In linear model theory, Theorem 1.2.3 is often applied to establish independence of two linear transformations of the data vector \(Y\).

**Corollary 1.2.4**: _If \(Y\sim N(\mu,\,\sigma^{2}I)\) and if \(AB^{\prime}=0\), then \(AY\) and \(BY\) are independent. The result also holds if \(Y\sim N(\mu,\,V)\) and \(AVB^{\prime}=0\)._

_Proof_ Consider the distribution of \(\begin{bmatrix}A\\ B\end{bmatrix}Y\). By Exercise 1.8, the joint distribution of \(AY\) and \(BY\) is multivariate normal. Since \(\mathrm{Cov}(AY,\,BY)=\sigma^{2}AI\,B^{\prime}=\sigma^{2}AB^{\prime}=0\), Theorem 1.2.3 implies that \(AY\) and \(BY\) are independent. 

**Exercise 1.9**: For a standard linear model \(Y=X\beta+e\), with \(e\sim N(0,\sigma^{2}I)\), show that \(Y\sim N(X\beta,\,\sigma^{2}I)\).

### 1.3 Distributions of Quadratic Forms

In this section, quadratic forms are defined, the expectation of a quadratic form is found, and a series of results on independence and chi-squared distributions are given.

**Definition 1.3.1**: Let \(Y\) be an \(n\)-dimensional random vector and let \(A\) be an \(n\times n\) matrix. A _quadratic form_ is a random variable defined by \(Y^{\prime}AY\) for some \(Y\) and \(A\).

Note that since \(Y^{\prime}AY\) is a scalar, \(Y^{\prime}AY=Y^{\prime}A^{\prime}Y=Y^{\prime}(A+A^{\prime})Y/2\). Since \((A+A^{\prime})/2\) is always a symmetric matrix, we can, without loss of generality, _restrict ourselves to quadratic forms where \(A\) is symmetric_.

The next result is extremely useful.

**Theorem 1.3.2**: _If \(\operatorname{E}(Y)=\mu\) and \(\operatorname{Cov}(Y)=V\), then \(\operatorname{E}(Y^{\prime}AY)=\operatorname{tr}(AV)+\mu^{\prime}A\mu\)._

_Proof_

\[(Y-\mu)^{\prime}A(Y-\mu) =Y^{\prime}AY-\mu^{\prime}AY-Y^{\prime}A\mu+\mu^{\prime}A\mu,\] \[\operatorname{E}[(Y-\mu)^{\prime}A(Y-\mu)] =\operatorname{E}[Y^{\prime}AY]-\mu^{\prime}A\mu-\mu^{\prime}A\mu+ \mu^{\prime}A\mu,\]

so \(\operatorname{E}[Y^{\prime}AY]=\operatorname{E}[(Y-\mu)^{\prime}A(Y-\mu)]+\mu^ {\prime}A\mu\).

It is easily seen that for any random square matrix \(W\), \(\operatorname{E}(\operatorname{tr}(W))=\operatorname{tr}(\operatorname{E}(W))\). Thus

\[\operatorname{E}[(Y-\mu)^{\prime}A(Y-\mu)] =\operatorname{E}(\operatorname{tr}[(Y-\mu)^{\prime}A(Y-\mu)])\] \[=\operatorname{E}(\operatorname{tr}[A(Y-\mu)(Y-\mu)^{\prime}])\] \[=\operatorname{tr}(\operatorname{E}[A(Y-\mu)(Y-\mu)^{\prime}])\] \[=\operatorname{tr}(AE[(Y-\mu)(Y-\mu)^{\prime}])\] \[=\operatorname{tr}(AV).\]

Substitution gives

\[\operatorname{E}(Y^{\prime}AY)=\operatorname{tr}(AV)+\mu^{\prime}A\mu.\]

**Exercise 1.10a**: If \(W\) and \(Y\) are random vectors with \(\operatorname{E}(W)=\gamma\) and \(\operatorname{E}(Y)=\mu\), show that

\[\operatorname{E}(Y^{\prime}AW)=\operatorname{tr}\left[AC\text{cov}(W,Y)\right] +\mu^{\prime}A\gamma.\]

We now proceed to give results on chi-squared distributions and independence of quadratic forms. Note that by Definition C.1 and Theorem 1.2.3, if \(Z\) is an \(n\)-dimensional random vector and \(Z\sim N(\mu,I)\), then \(Z^{\prime}Z\sim\chi^{2}(n,\mu^{\prime}\mu/2)\).

**Theorem 1.3.3**: _If \(Y\) is a random vector with \(Y\sim N(\mu,\,I)\) and if \(M\) is any perpendicular projection matrix, then \(Y^{\prime}MY\sim\chi^{2}[r(M),\,\mu^{\prime}M\mu/2]\)._

_Proof_ Let \(r(M)=r\) and let \(o_{1},\,\ldots,\,o_{r}\) be an orthonormal basis for \(C(M)\). Let \(O=[o_{1},\,\ldots,\,o_{r}]\) so that \(M=OO^{\prime}\). We now have \(Y^{\prime}MY=Y^{\prime}OO^{\prime}Y=(O^{\prime}Y)^{\prime}(O^{\prime}Y)\), where \(O^{\prime}Y\sim N(O^{\prime}\mu,\,O^{\prime}IO)\). The columns of \(O\) are orthonormal, so \(O^{\prime}O\) is an \(r\times r\) identity matrix, and by definition \((O^{\prime}Y)^{\prime}(O^{\prime}Y)\sim\chi^{2}(r,\,\mu^{\prime}OO^{\prime} \mu/2)\) where \(\mu^{\prime}OO^{\prime}\mu=\mu^{\prime}M\mu\). \(\square\)

Observe that if \(Y\sim N\left(\mu,\sigma^{2}I\right)\), then \([1/\sigma]Y\sim N\left([1/\sigma]\mu,\,I\right)\) and \(Y^{\prime}MY/\sigma^{2}\sim\chi^{2}\left[r(M),\,\mu^{\prime}M\mu/2\sigma^{2}\right]\).

Theorem 1.3.6 provides a generalization of Theorem 1.3.3 that is valid for an arbitrary covariance matrix. The next two lemmas are used in the proof of Theorem 1.3.6. The first is also useful in establishing the asymptotic distribution of Pearson's chi-squared statistic for multinomial data. The second is a useful fact whenever dealing with singular covariance matrices.

**Lemma 1.3.4**: _If \(Y\sim N(\mu,\,M)\) with \(\mu\in C(M)\) and if \(M\) is a perpendicular projection matrix, then \(Y^{\prime}Y\sim\chi^{2}[r(M),\,\mu^{\prime}\mu/2]\)._

_Proof_ Let \(O\) have \(r\) orthonormal columns with \(M=OO^{\prime}\). Since \(\mu\in C(M)\), \(\mu=Ob\). Let \(W\sim N(b,\,I)\), then \(Y\sim O\,W\). Since \(O^{\prime}O=I_{r}\) is also a perpendicular projection matrix, the previous theorem gives \(Y^{\prime}Y\sim W^{\prime}O^{\prime}OW\sim\chi^{2}(r,\,b^{\prime}O^{\prime}Ob/2)\). The proof is completed by observing that \(r=r(M)\) and \(b^{\prime}O^{\prime}Ob=\mu^{\prime}\mu\). \(\square\)

The following lemma establishes that, if \(Y\) is a singular random variable, then there exists a proper subset of \({\bf R}^{n}\) that contains \(Y\) with probability 1.

**Lemma 1.3.5**: _If \({\rm E}(Y)=\mu\) and \({\rm Cov}(Y)=V\), then \({\rm Pr}[(Y-\mu)\in C(V)]=1\)._

_Proof_ Without loss of generality, assume \(\mu=0\). Let \(M_{V}\) be the perpendicular projection operator onto \(C(V)\); then \(Y=M_{V}Y+(I-M_{V})Y\). Clearly, \({\rm E}[(I-M_{V})Y]=0\) and \({\rm Cov}[(I-M_{V})Y]=(I-M_{V})V(I-M_{V})\)=0. Thus, \({\rm Pr}[(I-M_{V})\)\(Y=0]=1\) and \({\rm Pr}[Y=M_{V}Y]=1\). Since \(M_{V}Y\in C(V)\), we are done. \(\square\)

A property that holds with probability one is said to hold _almost surely_. Thus it is almost sure that \((Y-\mu)\in C(V)\), or we might write \((Y-\mu)\in C(V)\) a.s.

**Exercise 1.10b**: Show that if \(Y\) is a random vector and if \({\rm E}(Y)=0\) and \({\rm Cov}(Y)=0\), then \({\rm Pr}[Y=0]=1\). Hint: For a random variable \(w\) with \({\rm Pr}[w\geq 0]=1\) and \(k>0\), show that \({\rm Pr}[w\geq k]\leq{\rm E}(w)/k\). Apply this result to \(Y^{\prime}Y\).

**Theorem 1.3.6**: _If \(Y\sim N(\mu,\,V)\), then \(Y^{\prime}AY\sim\chi^{2}[\operatorname{tr}(AV),\,\mu^{\prime}A\mu/2]\) provided that (1) \(VAVAV=VAV\), (2) \(\mu^{\prime}AVA\mu=\mu^{\prime}A\mu\), and (3) \(VAVA\mu=VA\mu\)._

_Proof_ _The general proof is given in the next subsection._ Here we give a proof for \(V\) positive definite.

For \(V\) nonsingular, condition (1) of the theorem holds if and only if \(AVA=A\), and \(AVA=A\) implies conditions (2) and (3). Also, write \(V=QQ^{\prime}\) for \(Q\) nonsingular. \(AVA=A\) implies that \(Q^{\prime}AQQ^{\prime}AQ=Q^{\prime}AVAQ=Q^{\prime}AQ\), so \(Q^{\prime}AQ\) is a ppo. Also, \(AVA=A\) implies that \([Q^{\prime}A]^{\prime}[Q^{\prime}A]=A\). Now consider \(Q^{\prime}AY\sim N(Q^{\prime}A\mu,\,Q^{\prime}AVAQ)=N(Q^{\prime}A\mu,\,Q^{ \prime}AQ)\). Since \(Q^{\prime}AQ\) is a ppo, Lemma 1.3.4 applies if \(Q^{\prime}A\mu\in C(Q^{\prime}AQ)\). But \(Q^{\prime}A\mu=[Q^{\prime}AQ]Q^{-1}\mu\in C(Q^{\prime}AQ)\). Applying Lemma 1.3.4,

\[Y^{\prime}AY=(Q^{\prime}AY)^{\prime}(Q^{\prime}AY)\sim\chi^{2}\left[r(Q^{ \prime}AQ),(Q^{\prime}A\mu)^{\prime}(Q^{\prime}A\mu)/2\right].\]

However,

\[r(Q^{\prime}AQ)=\operatorname{tr}(Q^{\prime}AQ)=\operatorname{tr}(AQQ^{\prime })=\operatorname{tr}(AV)\]

and

\[(Q^{\prime}A\mu)^{\prime}(Q^{\prime}A\mu)=\mu^{\prime}AQ^{\prime}QA\mu=\mu^{ \prime}AVA\mu=\mu^{\prime}A\mu,\]

so we are done. \(\square\)

Even if the three conditions of the theorem are not satisfied, it is possible to compute the distribution of the quadratic form, cf. the R package CompQuadForm, the references cited in its documentation, and Exercise 1.5.7.

One particularly useful result of the theorem follows.

**Corollary 1.3.6a**: _If \(Y\sim N(\mu,\,V)\) and \(\mu\in C(V)\), then_

\[Y^{\prime}V^{-}Y\sim\chi^{2}[r(V),\,\mu^{\prime}V^{-}\mu/2].\]

_Proof_  The three conditions in Theorem 1.3.6 are satisfied so \(Y^{\prime}V^{-}Y\sim\chi^{2}[\operatorname{tr}(V^{-}V),\,\mu^{\prime}V^{-}\mu /2]\). Because \(VV^{-}\) is idempotent, \(\operatorname{tr}(V^{-}V)=\operatorname{tr}(VV^{-})=r(VV^{-})\). To see that \(r(VV^{-})=r(V)\), note that the two matrices have the same column space:

\[C(V)=C(VV^{-}V)\subset C(VV^{-})\subset C(V).\]

The next three theorems establish conditions under which quadratic forms are independent. Theorem 1.3.7 examines the important special case in which the covariance matrix is a multiple of the identity matrix. In addition to considering independence of quadratic forms, the theorem also examines independence between quadratic forms and linear transformations of the random vector.

**Theorem 1.3.7**: _If \(A\) is symmetric and in (2) \(B\) is symmetric, \(Y\sim N(\mu,\sigma^{2}I)\), and \(BA=0\), then_

1. \(Y^{\prime}AY\) _and_ \(BY\) _are independent,_
2. \(Y^{\prime}AY\) _and_ \(Y^{\prime}BY\) _are independent,_

_The result also holds if \(Y\sim N(\mu,\,V)\) and \(BVA=0\)._

_Proof_ By Corollary 1.2.4, if \(BA=0\), \(BY\) and \(AY\) are independent or, more generally, if \(BVA=0\), \(BY\) and \(AY\) are independent. In addition, as discussed near the end of Appendix D, any function of \(AY\) is independent of any function of \(BY\). Since \(Y^{\prime}AY=Y^{\prime}AA^{-}AY\) and \(Y^{\prime}BY=Y^{\prime}BB^{-}BY\) are functions of \(AY\) and \(BY\), the theorem holds.

\(\Box\)

#### Results for General Covariance Matrices

_Proof of Theorem_ 1.3.6. By Lemma 1.3.5, for the purpose of finding the distribution of \(Y^{\prime}AY\), we can assume that \(Y=\mu+e\), where \(e\in C(V)\). Using conditions (1), (2), and (3) of the theorem and the fact that \(e=Vb\) for some \(b\),

\[Y^{\prime}AY = \mu^{\prime}A\mu+\mu^{\prime}Ae+e^{\prime}A\mu+e^{\prime}Ae\] \[= \mu^{\prime}AVA\mu+\mu^{\prime}AVAe+e^{\prime}AVA\mu+e^{\prime} AVAe\] \[= Y^{\prime}(AVA)Y.\]

Write \(V=QQ^{\prime}\) so that \(Y^{\prime}AY=(Q^{\prime}AY)^{\prime}(Q^{\prime}AY)\), where \(Q^{\prime}AY\sim N(Q^{\prime}A\mu\), \(Q^{\prime}AVAQ)\). If we can show that \(Q^{\prime}AVAQ\) is a perpendicular projection matrix and that \(Q^{\prime}A\mu\in C(Q^{\prime}AVAQ)\), then \(Y^{\prime}AY\) will have a chi-squared distribution by Lemma 1.3.4.

Since \(V\) is nonnegative definite, we can write \(Q=Q_{1}Q_{2}\), where \(Q_{1}\) has orthonormal columns and \(Q_{2}\) is nonsingular. It follows that

\[Q_{2}^{-1}Q_{1}^{\prime}V=Q_{2}^{-1}Q_{1}^{\prime}[Q_{1}Q_{2}Q^{\prime}]=Q^{ \prime}.\]

Applying this result, \(VAVAV=VAV\)implies that \(Q^{\prime}AVAQ=Q^{\prime}AQ\). Now \(Q^{\prime}AVAQ=(Q^{\prime}AQ)(Q^{\prime}AQ)\), so \(Q^{\prime}AQ\) is idempotent and symmetric and \(Q^{\prime}AQ=Q^{\prime}AVAQ\) so \(Q^{\prime}AVAQ\) is a perpendicular projection operator.

From the preceding paragraph, to see that \(Q^{\prime}A\mu\in C(Q^{\prime}AVAQ)\) it suffices to show that \(Q^{\prime}AQQ^{\prime}A\mu=Q^{\prime}A\mu\). Note that \(VAVA\mu=VA\mu\) implies that \(Q^{\prime}AVA\mu=Q^{\prime}A\mu\). However, since \(Q^{\prime}AVA\mu=Q^{\prime}AQQ^{\prime}A\mu\), we are done.

The noncentrality parameter is one-half of

\[(Q^{\prime}A\mu)^{\prime}(Q^{\prime}A\mu)=\mu^{\prime}AVA\mu=\mu^{\prime}A\mu.\]The degrees of freedom are

\[r(Q^{\prime}AVAQ)=r(Q^{\prime}AQ)=\operatorname{tr}(Q^{\prime}AQ)=\operatorname{ tr}(AQQ^{\prime})=\operatorname{tr}(AV).\qed\]

The final two theorems provide conditions for independence of quadratic forms under general covariance matrices. It is possible for the conditions in these theorems to hold with \(AVB\neq 0\).

**Theorem 1.3.8**: _If \(Y\sim N(\mu,\,V)\), \(A\) and \(B\) are nonnegative definite, and \(VAV\)\(BV=0\), then \(Y^{\prime}AY\) and \(Y^{\prime}BY\) are independent._

_Proof_ Since \(A\) and \(B\) are nonnegative definite, we can write \(A=RR^{\prime}\) and \(B=SS^{\prime}\). We can also write \(V=QQ^{\prime}\).

\(Y^{\prime}AY=(R^{\prime}Y)^{\prime}(R^{\prime}Y)\) and \(Y^{\prime}BY=(S^{\prime}Y)^{\prime}(S^{\prime}Y)\) are independent

\[\begin{array}{ll}&\text{if}\ \ \ R^{\prime}Y\text{ and }S^{\prime}Y\text{ are independent}\\ &\text{iff}\ \operatorname{Cov}(R^{\prime}Y,S^{\prime}Y)=0\\ &\text{iff}\ \ R^{\prime}VS=0\\ &\text{iff}\ \ R^{\prime}QQ^{\prime}S=0\\ &\text{iff}\ \ C(Q^{\prime}S)\perp C(Q^{\prime}R).\end{array}\]

Since \(C(AA^{\prime})=C(A)\) for any \(A\), we have

\[\begin{array}{ll}C(Q^{\prime}S)\perp C(Q^{\prime}R)&\text{iff}\ \ C(Q^{ \prime}SS^{\prime}Q)\perp C(Q^{\prime}RR^{\prime}Q)\\ &\text{iff}\ \ [Q^{\prime}SS^{\prime}Q][Q^{\prime}RR^{\prime}Q]=0\\ &\text{iff}\ \ Q^{\prime}BVAQ=0\\ &\text{iff}\ \ C(Q)\perp C(BVAQ)\\ &\text{iff}\ \ C(QQ^{\prime})\perp C(BVAQ)\\ &\text{iff}\ \ QQ^{\prime}BVAQ=0\\ &\text{iff}\ \ VBVAQ=0.\end{array}\]

Repeating similar arguments for the right side gives \(VBVAQ=0\) iff \(VBVAV=0\).

**Theorem 1.3.9**: _If \(Y\sim N(\mu,\,V)\) and (1) \(VAVBV=0\), (2) \(VAVB\mu=0\), (3) \(VBVA\mu=0\), (4) \(\mu^{\prime}AVB\mu=0\), and conditions (1), (2), and (3) from Theorem 1.3.6 hold for both \(Y^{\prime}AY\) and \(Y^{\prime}BY\), then \(Y^{\prime}AY\) and \(Y^{\prime}BY\) are independent._

**Exercise 1.11**: Prove Theorem 1.3.9.

Hints: Let \(V=QQ^{\prime}\) and write \(Y=\mu+QZ\), where \(Z\sim N(0,\,I)\). Using \(\qed\) to indicate independence, show that \[\begin{bmatrix}Q^{\prime}AQZ\\ \mu^{\prime}AQZ\end{bmatrix}\quad\coprod\quad\begin{bmatrix}Q^{\prime}BQZ\\ \mu^{\prime}BQZ\end{bmatrix}\]

and that, say, \(Y^{\prime}AY\) is a function \(Q^{\prime}AQZ\) and \(\mu^{\prime}AQZ\).

Note that Theorem 1.3.8 applies immediately if \(AY\) and \(BY\) are independent, i.e., if \(AVB=0\). In something of a converse, if \(V\) is nonsingular, the condition \(VAVBV=0\) is equivalent to \(AVB=0\); so the theorem applies only when \(AY\) and \(BY\) are independent. However, if \(V\) is singular, the conditions of Theorems 1.3.8 and 1.3.9 can be satisfied even when \(AY\) and \(BY\) are not independent.

### Generalized Linear Models

We now give a brief introduction to _generalized linear models_. On occasion through the rest of the book, reference will be made to various properties of linear models that extend easily to generalized linear models. See McCullagh and Nelder (1989) or Christensen (1997) for more extensive discussions of generalized linear models and their applications. First it must be noted that a _general linear model_ is a linear model but a _generalized_ linear model is a generalization of the concept of a linear model. Generalized linear models include linear models as a special case but also include logistic regression, exponential regression, and gamma regression as special cases. Additionally, log-linear models for multinomial data are closely related to generalized linear models.

Consider a random vector \(Y\) with \(\operatorname{E}(Y)=\mu\). Let \(h\) be an arbitrary function on the real numbers and, for a vector \(v=(v_{1},\ldots,v_{n})^{\prime}\), define the vector function

\[h(v)\equiv\begin{bmatrix}h(v_{1})\\ \vdots\\ h(v_{n})\end{bmatrix}.\]

The primary idea of a generalized linear model is specifying that

\[\mu=h(X\beta),\]

where \(h\) is a known invertible function and \(X\) and \(\beta\) are defined as for linear models. The inverse of \(h\) is called the _link_ function. In particular, linear models use the identity function \(h(v)=v\), logistic regression uses the logistic transform \(h(v)=e^{v}/(1+e^{v})\), and both exponential regression and log-linear models use the exponential transform \(h(v)=e^{v}\). Their link functions are, respectively, the identity, logit, and log transforms. Because the linear structure \(X\beta\) is used in generalized linear models, many of the analysis techniques used for linear models can be easily extended to generalized linear models.

Typically, in a generalized linear model it is assumed that the \(y_{i}\)s are independent and each follows a distribution having density or mass function of the form

\[f(y_{i}|\theta_{i},\phi;w_{i})=\exp\left\{\frac{w_{i}}{\phi}[\theta_{i}y_{i}-r( \theta_{i})]\right\}g(y_{i},\phi,w_{i}), \tag{1}\]

where \(r(\cdot)\) and \(g(\cdot,\cdot,\cdot)\) are known functions and \(\theta_{i},\phi,\) and \(w_{i}\) are scalars. By assumption, \(w_{i}\) is a fixed known number. Typically, it is a known weight that indicates knowledge about a pattern in the variabilities of the \(y_{i}\)s. \(\phi\) is either known or is an unknown parameter, but for some purposes is always treated like it is known. It is related to the variance of \(y_{i}\). The parameter \(\theta_{i}\) is related to the mean of \(y_{i}\). For standard linear models, the assumption is that the \(y_{i}\)s are independent \(N(\theta_{i},\phi/w_{i})\), with \(\phi\equiv\sigma^{2}\) and \(w_{i}\equiv 1\). The standard assumption of logistic regression is that the \(N_{i}y_{i}\)s are distributed as independent binomials with \(N_{i}\) trials, success probability

\[\mathrm{E}(y_{i})\equiv\mu_{i}\equiv p_{i}=e^{\theta_{i}}/[1+e^{\theta_{i}}],\]

\(w_{i}=N_{i}\), and \(\phi=1\). Log-linear models fit into this framework when one assumes that the \(y_{i}\)s are independent Poisson with mean \(\mu_{i}=e^{\theta_{i}}\), \(w_{i}=1\), and \(\phi=1\). Note that in these cases the mean is some function of \(\theta_{i}\) and that \(\phi\) is merely related to the variance. Note also that in the three examples, the \(h\) function has already appeared, even though these distributions have not yet incorporated the linear structure of the generalized linear model.

To investigate the relationship between the \(\theta_{i}\) parameters and the linear structure \(x_{i}^{\prime}\beta\), where \(x_{i}^{\prime}\) is the \(i\)th row of \(X\), let \(\dot{r}(\theta_{i})\) be the derivative \(dr(\theta_{i})/d\theta_{i}\). It can be shown that

\[\mathrm{E}(y_{i})\equiv\mu_{i}=\dot{r}(\theta_{i}).\]

Thus, another way to think about the modeling process is that

\[\mu_{i}=h(x_{i}^{\prime}\beta)=\dot{r}(\theta_{i}),\]

where both \(h\) and \(\dot{r}\) are invertible. In matrix form, write \(\theta=(\theta_{1},\ldots,\theta_{n})^{\prime}\) so that

\[X\beta=h^{-1}(\mu)=h^{-1}\left[\dot{r}(\theta)\right]\quad\text{and}\quad\dot {r}^{-1}\left[h(X\beta)\right]=\dot{r}^{-1}\left(\mu\right)=\theta.\]

The special case of \(h(\cdot)=\dot{r}(\cdot)\) gives \(X\beta=\theta\). This is known as a _canonical generalized linear model_, or as using a _canonical link function_. The three examples given earlier are all examples of canonical generalized linear models. Linear models with normally distributed data are canonical generalized linear models. Logistic regression is the canonical model having \(N_{i}y_{i}\) distributed Binomial(\(N_{i}\), \(\mu_{i}\)) for known \(N_{i}\) with \(h^{-1}(\mu_{i})\equiv\log(\mu_{i}/[1-\mu_{i}])\). Another canonical generalized linear model has \(y_{i}\) distributed Poisson(\(\mu_{i}\)) with \(h^{-1}(\mu_{i})\equiv\log(\mu_{i})\).

### Additional Exercises

#### Exercise 1.5.1

Let \(Y=(y_{1},y_{2},y_{3})^{\prime}\) be a random vector. Suppose that \(\mathrm{E}(Y)\in\mathcal{M}\), where \(\mathcal{M}\) is defined by

\[\mathcal{M}=\{(a,a-b,2b)^{\prime}|a,b\in\mathbf{R}\}.\]

* Show that \(\mathcal{M}\) is a vector space.
* Find a basis for \(\mathcal{M}\).
* Write a linear model for this problem (i.e., find \(X\) such that \(Y=X\beta+e\), \(\mathrm{E}(e)=0\)).
* If \(\beta=(\beta_{1},\beta_{2})^{\prime}\) in part (c), find two vectors \(r=(r_{1},r_{2},r_{3})^{\prime}\) and \(s=(s_{1},s_{2},s_{3})^{\prime}\) such that \(\mathrm{E}(r^{\prime}Y)=r^{\prime}X\beta=\beta_{1}\) and \(\mathrm{E}(s^{\prime}Y)=\beta_{2}\). Find another vector \(t=(t_{1},t_{2},t_{3})^{\prime}\) with \(r\neq t\) but \(\mathrm{E}(t^{\prime}Y)=\beta_{1}\).

#### Exercise 1.5.2

Let \(Y=(y_{1},y_{2},y_{3})^{\prime}\) with \(Y\sim N(\mu,V)\), where

\[\mu=(5,6,7)^{\prime}\]

and

\[V=\begin{bmatrix}2&0&1\\ 0&3&2\\ 1&2&4\end{bmatrix}.\]

Find

* the marginal distribution of \(y_{1}\),
* the joint distribution of \(y_{1}\) and \(y_{2}\),
* the conditional distribution of \(y_{3}\) given \(y_{1}=u_{1}\) and \(y_{2}=u_{2}\),
* the conditional distribution of \(y_{3}\) given \(y_{1}=u_{1}\),
* the conditional distribution of \(y_{1}\) and \(y_{2}\) given \(y_{3}=u_{3}\),
* the correlations \(\rho_{12}\), \(\rho_{13}\), \(\rho_{23}\),
* the distribution of

\[Z=\begin{bmatrix}2&1&0\\ 1&1&1\end{bmatrix}Y+\begin{bmatrix}-15\\ -18\end{bmatrix},\]
* the characteristic functions of \(Y\) and \(Z\).

#### Exercise 1.5.3

The density of \(Y=(y_{1},y_{2},y_{3})^{\prime}\) is

\[(2\pi)^{-3/2}|V|^{-1/2}e^{-Q/2},\]

where

\[Q=2y_{1}^{2}+y_{2}^{2}+y_{3}^{2}+2y_{1}y_{2}-8y_{1}-4y_{2}+8.\]Find \(V^{-1}\) and \(\mu\).

**Exercise 1.5.4**: Let \(Y\sim N(J\mu,\sigma^{2}I)\) and let \(O=\left[n^{-1/2}J,\,O_{1}\right]\) be an orthonormal matrix.

1. Find the distribution of \(O^{\prime}Y\).
2. Show that \(\bar{y}.=(1/n)J^{\prime}Y\) and that \(s^{2}=Y^{\prime}O_{1}O_{1}^{\prime}Y/(n-1)\).
3. Show that \(\bar{y}.\) and \(s^{2}\) are independent.

Hint: Show that \(Y^{\prime}Y=Y^{\prime}O\,O^{\prime}Y=Y^{\prime}(1/n)J\,J^{\prime}Y+Y^{\prime} O_{1}O_{1}^{\prime}Y\).

**Exercise 1.5.5**: Let \(Y=(y_{1},\,y_{2})^{\prime}\) have a \(N(0,\,I)\) distribution. Show that if

\[A=\begin{bmatrix}1&a\\ a&1\end{bmatrix}\qquad B=\begin{bmatrix}1&b\\ b&1\end{bmatrix},\]

then the conditions of Theorem 1.3.7 implying independence of \(Y^{\prime}AY\) and \(Y^{\prime}BY\) are satisfied only if \(|a|=1/|b|\) and \(a=-b\). What are the possible choices for \(a\) and \(b\)?

**Exercise 1.5.6**: Let \(Y=(y_{1},\,y_{2},\,y_{3})^{\prime}\) have a \(N(\mu,\sigma^{2}I)\) distribution. Consider the quadratic forms defined by the matrices \(M_{1},\,M_{2},\) and \(M_{3}\) given below.

1. Find the distribution of each \(Y^{\prime}M_{i}Y\).
2. Show that the quadratic forms are pairwise independent.
3. Show that the quadratic forms are mutually independent.

\[M_{1}=\frac{1}{3}J_{3}^{3},\qquad M_{2}=\frac{1}{14}\begin{bmatrix}9&-3&-6\\ -3&1&2\\ -6&2&4\end{bmatrix},\]

\[M_{3}=\frac{1}{42}\begin{bmatrix}1&-5&4\\ -5&25&-20\\ 4&-20&16\end{bmatrix}.\]

**Exercise 1.5.7**: Let \(A\) be symmetric, \(Y\sim N(0,\,V)\), and \(w_{1},\,\ldots,\,w_{s}\) be independent \(\chi^{2}(1)\) random variables. Show that for some value of \(s\) and some numbers \(\lambda_{i}\), \(Y^{\prime}AY\sim\sum_{i=1}^{s}\lambda_{i}w_{i}\). Hint: \(Y\sim QZ\) so \(Y^{\prime}AY\sim Z^{\prime}Q^{\prime}AQZ\). Write \(Q^{\prime}AQ=PD(\lambda_{i})\,P^{\prime}\).

**Exercise 1.5.8**: Show that

1. for Example 1.0.1 the perpendicular projection operator onto \(C(X)\) is \[M=\frac{1}{6}J_{6}^{6}+\frac{1}{70}\begin{bmatrix}25&15&5&-5&-15&-25\\ 15&9&3&-3&-9&-15\\ 5&3&1&-1&-3&-5\\ -5&-3&-1&1&3&5\\ -15&-9&-3&3&9&15\\ -25&-15&-5&5&15&25\end{bmatrix};\](b) for Example 1.0.2 the perpendicular projection operator onto \(C(X)\) is

\[M=\left[\begin{array}{cccccc}1/3&1/3&1/3&0&0&0\\ 1/3&1/3&1/3&0&0&0\\ 1/3&1/3&1/3&0&0&0\\ 0&0&0&1&&0\\ 0&0&0&0&1/2&1/2\\ 0&0&0&0&1/2&1/2\end{array}\right].\]

## References

* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* McCullagh & Nelder (1989) McCullagh, P., & Nelder, J. A. (1989). _Generalized linear models_ (2nd ed.). London: Chapman and Hall.

## Chapter 2 Estimation

There is a huge body of literature available on estimation and testing in linear models. A few books dealing with the subject are Arnold (1981), Eaton (1983), Graybill (1976), Harville (2018), Monahan (2008), Rao (1973), Ravishanker and Dey (2002), Rencher and Schaalje (2008), Scheffe (1959), Searle (1971), Seber (1966, 1977, 2015), and Wichura (2006).

### Identifiability and Estimability

A key issue in linear model theory is figuring out which parameters can be estimated and which cannot. We will see that what can be estimated are functions of the parameters that are _identifiable_. Linear functions of the parameters that are identifiable are called _estimable_ and have linear unbiased estimators. These concepts also have natural applications to generalized linear models. The definitions used here are tailored to (generalized) linear models but our definition of an identifiable parameterization coincides with more common definitions of identifiability; cf. Appendix D.1. The distribution of \(Y\) should completely determine \(\operatorname{E}(Y)\) along with some covariance parameter(s) [typically \(\sigma^{2}\)] but the question is whether \(\beta\) is uniquely determined by \(\operatorname{E}(Y)\).

Consider the general linear model

\[Y=X\beta+e,\ \ \ \operatorname{E}(e)=0,\]

where again \(Y\) is an \(n\times 1\) vector of observations, \(X\) is an \(n\times p\) matrix of known constants, \(\beta\) is a \(p\times 1\) vector of unobservable parameters, and \(e\) is an \(n\times 1\) vector of unobservable random errors whose distribution does not depend on \(\beta\). We can only learn about \(\beta\) through \(X\beta\). If \(x^{\prime}_{i}\) is the \(i\)th row of \(X\), \(x^{\prime}_{i}\beta\) is the \(i\)th row of \(X\beta\) and we can only learn about \(\beta\) through the \(x^{\prime}_{i}\beta\)s. \(X\beta\) can be thought of as a vector of inner products between \(\beta\) and a spanning set for \(C(X^{\prime})\). Thus, we can learn about inner products between \(\beta\) and \(C(X^{\prime})\). In particular, when \(\lambda\) is a \(p\times 1\) vector of known constants, we can learn about functions \(\lambda^{\prime}\beta\) where \(\lambda\in C(X^{\prime})\), i.e., where \(\lambda=X^{\prime}\rho\) for some vector \(\rho\). These are precisely the estimable functions of \(\beta\). We now give more formal arguments leading us to focus on functions \(\lambda^{\prime}\beta\) where \(\lambda^{\prime}=\rho^{\prime}X\) or, more generally, vectors \(\Lambda^{\prime}\beta\) where \(\Lambda^{\prime}=P^{\prime}X\).

In general, a _parameterization_ for the \(n\times 1\) mean vector \(\operatorname{E}(Y)\) consists of writing \(\operatorname{E}(Y)\) as a function of some parameters \(\beta\), say,

\[\operatorname{E}(Y)=f(\beta).\]

A general linear model is a parameterization

\[\operatorname{E}(Y)=X\beta\]because \(\mathrm{E}(Y)=\mathrm{E}(X\beta+e)=X\beta+\mathrm{E}(e)=X\beta\). A parameterization is identifiable if knowing \(\mathrm{E}(Y)\) tells you the parameter vector \(\beta\).

**Definition 2.1.1**.: The parameter \(\beta\) is _identifiable_ if for any \(\beta_{1}\) and \(\beta_{2}\), \(f(\beta_{1})=f(\beta_{2})\) implies \(\beta_{1}=\beta_{2}\). If \(\beta\) is identifiable, we say that the parameterization \(f(\beta)\) is identifiable. Moreover, a vector-valued function \(g(\beta)\) is identifiable if \(f(\beta_{1})=f(\beta_{2})\) implies \(g(\beta_{1})=g(\beta_{2})\). If the parameterization is not identifiable but nontrivial identifiable functions exist, then the parameterization is said to be _partially identifiable_.

The key point is that if \(\beta\) or a function \(g(\beta)\) is not identifiable, it is simply impossible for one to know what it is based on knowing \(\mathrm{E}(Y)\). From a statistical perspective, we are considering models for the mean vector with the idea of collecting data that will allow us to estimate \(\mathrm{E}(Y)\). If actually knowing \(\mathrm{E}(Y)\) is not sufficient to tell us the value of \(\beta\) or \(g(\beta)\), no amount of data is ever going to let us estimate them.

In regression models, i.e., models for which \(r(X)=p\), the parameters are identifiable. In this case, \(X^{\prime}X\) is nonsingular, so if \(X\beta_{1}=X\beta_{2}\), then

\[\beta_{1}=(X^{\prime}X)^{-1}X^{\prime}X\beta_{1}=(X^{\prime}X)^{-1}X^{\prime} X\beta_{2}=\beta_{2}\]

and identifiability holds.

For models in which \(r(X)=r<p\), there exist \(\beta_{1}\neq\beta_{2}\) but \(X\beta_{1}=X\beta_{2}\), so the parameters are not identifiable.

For general linear models, the only functions of the parameters that are identifiable are functions of \(X\beta\). This follows from the next result.

**Theorem 2.1.2**.: _A function \(g(\beta)\) is identifiable if and only if \(g(\beta)\) is a function of \(f(\beta)\)._

Proof.: \(g(\beta)\) being a function of \(f(\beta)\) means that for some function \(g_{*}\), \(g(\beta)=g_{*}[f(\beta)]\) for all \(\beta\); or, equivalently, it means that for any \(\beta_{1}\neq\beta_{2}\) such that \(f(\beta_{1})=f(\beta_{2})\), \(g(\beta_{1})=g(\beta_{2})\).

Clearly, if \(g(\beta)=g_{*}[f(\beta)]\) and \(f(\beta_{1})=f(\beta_{2})\), then \(g(\beta_{1})=g_{*}[f(\beta_{1})]=g_{*}[f(\beta_{2})]=g(\beta_{2})\), so \(g(\beta)\) is identifiable.

Conversely, if \(g(\beta)\) is not a function of \(f(\beta)\), there exists \(\beta_{1}\neq\beta_{2}\) such that \(f(\beta_{1})=f(\beta_{2})\) but \(g(\beta_{1})\neq g(\beta_{2})\). Hence, \(g(\beta)\) is not identifiable. 

It is reasonable to estimate any identifiable function. Thus, in a linear model it is reasonable to estimate any function of \(X\beta\). It is not reasonable to estimate nonidentifiable functions, because you simply do not know what you are estimating.

The traditional idea of estimability in linear models can now be presented. Estimable functions are linear functions of \(\beta\) that are identifiable.

**Definition 2.1.3**.: A vector-valued linear function of \(\beta\), say, \(\Lambda^{\prime}\beta\), is _estimable_ if \(\Lambda^{\prime}\beta=P^{\prime}X\beta\) for some matrix \(P\).

Actually, an identifiable linear function of \(\beta\) is a function \(g_{*}(X\beta)\), but since the composite function is linear and \(X\beta\) is linear, the function \(g_{*}\) must be linear, and we can write it as a matrix \(P^{\prime}\).

Clearly, if \(\Lambda^{\prime}\beta\) is estimable, it is identifiable and therefore it is a reasonable thing to estimate. However, estimable functions are not the only functions of \(\beta\) that are reasonable to estimate. For example, the ratio of two estimable functions is not estimable, but it is identifiable, so the ratio is reasonable to estimate. You _can_ estimate many functions that are not "estimable." What you cannot do is estimate nonidentifiable functions.

Unfortunately, the term "nonestimable" is often used to mean something other than "not being estimable." You can be "not estimable" by being either not linear or not identifiable. In particular, a linear function that is "not estimable" is automatically nonidentifiable. However, nonesimable is often _taken to mean_ a linear function that is not identifiable. In other words, some authors (perhaps, on occasion, even this one) presume that nonestimable functions are linear, so that nonestimability and nonidentifiability become equivalent.

It should be noted that the concepts of identifiability and estimability are based entirely on the assumption that \(\operatorname{E}(Y)=X\beta\). Identifiability and estimability do not depend on \(\operatorname{Cov}(Y)=\operatorname{Cov}(e)\) (as long as the covariance matrix is not also a function of \(\beta\)).

An important property of estimable functions \(\Lambda^{\prime}\beta=P^{\prime}X\beta\) is that although \(P\) need not be unique, its perpendicular projection (columnwise) onto \(C(X)\) is unique. Let \(P_{1}\) and \(P_{2}\) be matrices with \(\Lambda^{\prime}=P_{1}^{\prime}X=P_{2}^{\prime}X\), then

\[MP_{1}=X(X^{\prime}X)^{-}X^{\prime}P_{1}=X(X^{\prime}X)^{-}\Lambda=X(X^{ \prime}X)^{-}X^{\prime}P_{2}=MP_{2}.\]

_Example 2.1.4_.: In the simple linear regression model of Example 1.0.1, \(\beta_{1}\) is estimable because

\[\frac{1}{35}(-5,-3,-1,1,3,5)\begin{bmatrix}1&1\\ 1&2\\ 1&3\\ 1&4\\ 1&5\\ 1&6\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}=(0,1)\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}=\beta_{1}.\]

\(\beta_{0}\) is also estimable. Note that\[\frac{1}{6}(1,1,1,1,1,1)\begin{bmatrix}1&1\\ 1&2\\ 1&3\\ 1&4\\ 1&5\\ 1&6\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}=\beta_{0}+\frac{7}{2}\beta_{1},\]

so

\[\beta_{0} =\left(\beta_{0}+\frac{7}{2}\beta_{1}\right)-\frac{7}{2}\beta_{1}\] \[=\left[\frac{1}{6}\begin{pmatrix}1\\ 1\\ 1\\ 1\\ 1\end{pmatrix}-\frac{7}{2}\left(\frac{1}{35}\right)\begin{pmatrix}-5\\ -3\\ -1\\ 1\\ 3\\ 5\end{pmatrix}\right]^{\prime}\begin{bmatrix}1&1\\ 1&2\\ 1&3\\ 1&4\\ 1&5\\ 1&6\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}\] \[=\frac{1}{30}(20,14,8,2,-4,-10)\begin{bmatrix}1&1\\ 1&2\\ 1&3\\ 1&4\\ 1&5\\ 1&6\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}.\]

For any fixed number \(x\), \(\beta_{0}+\beta_{1}x\) is estimable because it is a linear combination of estimable functions.

#### Example 2.1.5

In the one-way ANOVA model of Example 1.0.2, we can estimate parameters like \(\mu+\alpha_{1}\), \(\alpha_{1}-\alpha_{3}\), and \(\alpha_{1}+\alpha_{2}-2\alpha_{3}\). Observe that

\[(1,0,0,0,0,0)\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}=\mu+\alpha_{1},\]

\[(1,0,0,0,-1,0)\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}=\alpha_{1}-\alpha_{3},\]

but also \[\left(\frac{1}{3},\frac{1}{3},\frac{1}{3},0,\frac{-1}{2},-\frac{-1}{2}\right) \begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\\ \end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}=\alpha_{1}-\alpha_{3},\]

and

\[(1,0,0,1,-2,0)\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\\ \end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}=\alpha_{1}+\alpha_{2}-2\alpha_{3}.\]

We have given two vectors \(\rho_{1}\) and \(\rho_{2}\) with \(\rho_{i}^{\prime}X\beta=\alpha_{1}-\alpha_{3}\). Using \(M\) given in Exercise 1.5.8b, the reader can verify that \(M\rho_{1}=M\rho_{2}\).

In the one-way analysis of covariance model,

\[y_{ij}=\mu+\alpha_{i}+\gamma\,x_{ij}+e_{ij},\quad\text{E}(e_{ij})=0,\]

\(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\), \(x_{ij}\) is a known predictor variable and \(\gamma\) is its unknown coefficient. \(\gamma\) is generally identifiable but \(\mu\) and the \(\alpha_{i}\)s are not. The following results allow one to tell whether or not an individual parameter is identifiable.

**Proposition 2.1.6a** _For a linear model, write \(X\beta=\sum_{k=1}^{p}X_{k}\beta_{k}\) where the \(X_{k}\)s are the columns of \(X\). An individual parameter \(\beta_{j}\) is not identifiable if and only if there exist scalars \(\alpha_{k}\) such that \(X_{j}=\sum_{k\neq j}X_{k}\alpha_{k}\)._

_Proof_ To show that the condition on \(X\) implies nonidentifiability, it is enough to show that there exist \(\beta\) and \(\beta_{*}\) with \(X\beta=X\beta_{*}\) but \(\beta_{j}\neq\beta_{*j}\). The condition \(X_{j}=\sum_{k\neq j}X_{k}\alpha_{k}\) is equivalent to there existing a vector \(\alpha\) with \(\alpha_{j}\neq 0\) and \(X\alpha=0\). Let \(\beta_{*}=\beta+\alpha\) and the proof is complete.

Rather than showing that when \(\beta_{j}\) is not identifiable, the condition on \(X\) holds, we show the contrapositive, i.e., that when the condition on \(X\) does not hold, \(\beta_{j}\) is identifiable. If there do not exist such \(\alpha_{k}\)s, then whenever \(X\alpha=0\), we must have \(\alpha_{j}=0\). In particular, if \(X\beta=X\beta_{*}\), then \(X(\beta-\beta_{*})=0\), so \((\beta_{j}-\beta_{*j})=0\) and \(\beta_{j}\) is identifiable. \(\square\)

**Proposition 2.1.6b** _For a linear model, write \(X\beta=\sum_{k=1}^{p}X_{k}\beta_{k}\) where the \(X_{k}\)s are the columns of \(X\). An individual parameter \(\beta_{j}\) is identifiable if and only if for any scalars \(\alpha_{k}\) such that \(0=\sum_{k}X_{k}\alpha_{k}\), \(\alpha_{j}=0\).__Proof_ The condition \(0=\sum_{k}X_{k}\alpha_{k}\) with \(\alpha_{j}=0\) is equivalent to there existing a vector \(\alpha\) with \(X\alpha=0\) and \(\alpha_{j}=0\). To show that this condition on \(X\) implies identifiability, assume \(\beta\) and \(\beta_{*}\) have \(X\beta=X\beta_{*}\). Then \(X\left(\beta-\beta_{*}\right)=0\), so the \(j\)th component of \(\beta-\beta_{*}\) is \(0\) and \(\beta_{j}=\beta_{*j}\).

Let \(\alpha\) be any vector with \(X\alpha=0\). For any \(\beta\) define \(\beta_{*}\equiv\beta+\alpha\). Clearly, \(X\beta=X\beta_{*}\) so by identifiability \(\beta_{j}=\beta_{*j}=\beta_{j}+\alpha_{j}\) which implies \(\alpha_{j}=0\). \(\square\)

The concepts of identifiability and estimability apply with little change to generalized linear models. In generalized linear models, the distribution of \(Y\) is either completely determined by \(\mathrm{E}(Y)\) or it is determined by \(\mathrm{E}(Y)\) along with another parameter \(\phi\) that is unrelated to the parameterization of \(\mathrm{E}(Y)\). A generalized linear model has \(\mathrm{E}(Y)=h(X\beta)\). By Theorem 2.1.2, a function \(g(\beta)\) is identifiable if and only if it is a function of \(h(X\beta)\). However, the function \(h(\cdot)\) is assumed to be invertible, so \(g(\beta)\) is identifiable if and only if it is a function of \(X\beta\). A vector-valued linear function of \(\beta\), say, \(\Lambda^{\prime}\beta\) is identifiable if \(\Lambda^{\prime}\beta=P^{\prime}X\beta\) for some matrix \(P\), hence Definition 2.1.3 applies as well to define estimability for generalized linear models as it does for linear models. Propositions 2.1.6a, 2.1.6b also applies without change.

Finally, the concept of estimability in linear models can be related to the existence of linear unbiased estimators. A linear function of the parameter vector \(\beta\), say \(\lambda^{\prime}\beta\), is estimable if and only if it admits a linear unbiased estimate.

**Definition 2.1.7**: An estimate \(f(Y)\) of \(g(\beta)\) is _unbiased_ if \(\mathrm{E}[f(Y)]=g(\beta)\) for any \(\beta\).

**Definition 2.1.8**: \(f(Y)\) is a _linear estimate_ of \(\lambda^{\prime}\beta\) if \(f(Y)=a_{0}+a^{\prime}Y\) for some scalar \(a_{0}\) and vector \(a\).

Technically, \(a_{0}+a^{\prime}Y\) is an _affine_ transformation of \(Y\) and \(a^{\prime}Y\) is linear. We now see that unbiased linear estimates have to be linear transformations.

**Proposition 2.1.9**: _An estimate \(a_{0}+a^{\prime}Y\) is unbiased for \(\lambda^{\prime}\beta\) if and only if \(a_{0}=0\) and \(a^{\prime}X=\lambda^{\prime}\)._

_Proof_\(\Leftarrow\) If \(a_{0}=0\) and \(a^{\prime}X=\lambda^{\prime}\), then \(\mathrm{E}(a_{0}+a^{\prime}Y)=0+a^{\prime}X\beta=\lambda^{\prime}\beta\).

\(\Rightarrow\) If \(a_{0}+a^{\prime}Y\) is unbiased, \(\lambda^{\prime}\beta=\mathrm{E}(a_{0}+a^{\prime}Y)=a_{0}+a^{\prime}X\beta\), for any \(\beta\). Subtracting \(a^{\prime}X\beta\) from both sides gives

\[(\lambda^{\prime}-a^{\prime}X)\beta=a_{0}\]

for any \(\beta\). If \(\beta=0\), then \(a_{0}=0\). Thus the vector \(\lambda-X^{\prime}a\) is orthogonal to any vector \(\beta\). This can only occur if \(\lambda-X^{\prime}a=0\); so \(\lambda^{\prime}=a^{\prime}X\). \(\square\)

**Corollary 2.1.10**: \(\lambda^{\prime}\beta\) _is estimable if and only if there exists \(\rho\) such that \(\operatorname{E}(\rho^{\prime}Y)=\lambda^{\prime}\beta\) for any \(\beta\)._

### Estimation: Least Squares

Consider the linear model

\[Y=X\beta+e,\ \ \ \operatorname{E}(e)=0.\]

Suppose we want to estimate \(\operatorname{E}(Y)\). We know that \(\operatorname{E}(Y)=X\beta\), but \(\beta\) is unknown; so all we really know is that

\[\operatorname{E}(Y)\in C(X)\equiv\{v|v=X\beta\text{ for some }\beta\in \mathbb{R}^{p}\}.\]

To estimate \(\operatorname{E}(Y)\), we might take the vector in \(C(X)\) that is closest to \(Y\). By definition then, an estimate \(\hat{\beta}\) is a least squares estimate if \(X\hat{\beta}\) is the vector in \(C(X)\) that is closest to \(Y\). In other words, \(\hat{\beta}\) is a _least squares estimate (LSE)_ of \(\beta\) if

\[(Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})=\min_{\beta}(Y-X\beta)^{\prime}(Y-X \beta).\]

For a vector \(\Lambda^{\prime}\beta\), a least squares estimate is defined as \(\Lambda^{\prime}\hat{\beta}\) for any least squares estimate \(\hat{\beta}\).

In this section, least squares estimates are characterized and uniqueness and unbiasedness properties of least squares estimates are given. For standard linear models an unbiased estimate of \(\sigma^{2}\) is presented. Finally, at the end of the section, the geometry associated with least squares estimation and unbiased estimation of \(\sigma^{2}\) is discussed. The geometry provides good intuition for \(n\)-dimensional problems but the geometry can only be visualized in three dimensions. In other words, although it is a fine pedagogical tool, the geometry can only be spelled out for three or fewer data points. The fundamental goal of this book is to build the theory of linear models on vector space generalizations of these fundamentally geometric concepts. We now establish the _fundamental theorem of least squares estimation_, that the vector in \(C(X)\) that is closest to \(Y\) is the perpendicular projection of \(Y\) onto \(C(X)\).

**Theorem 2.2.1**: \(\hat{\beta}\) _is a least squares estimate of \(\beta\) if and only if \(X\hat{\beta}=MY\), where \(M\) is the perpendicular projection operator onto \(C(X)\)._

_Proof_ We will show that

\[(Y-X\beta)^{{}^{\prime}}(Y-X\beta)=(Y-MY)^{{}^{\prime}}(Y-MY)+(MY-X\beta)^{{}^ {\prime}}(MY-X\beta).\]Both terms on the righthand side are nonnegative, and the first term does not depend on \(\beta\). \((Y-X\beta)^{\prime}(Y-X\beta)\) is minimized by minimizing \((MY-X\beta)^{\prime}(MY-X\beta)\). This is the squared distance between \(MY\) and \(X\beta\). The distance is zero if and only if \(MY=X\beta\), which proves the theorem. We now establish the equation.

\[(Y-X\beta)^{\prime}(Y-X\beta) = (Y-MY+MY-X\beta)^{\prime}(Y-MY+MY-X\beta)\] \[= (Y-MY)^{\prime}(Y-MY)+(Y-MY)^{\prime}(MY-X\beta)\] \[+(MY-X\beta)^{\prime}(Y-MY)+(MY-X\beta)^{\prime}(MY-X\beta).\]

However, \((Y-MY)^{\prime}(MY-X\beta)=Y^{\prime}(I-M)MY-Y^{\prime}(I-M)X\beta=0\) because \((I-M)M=0\) and \((I-M)X=0\). Similarly, \((MY-X\beta)^{\prime}(Y-MY)=0\). 

**Corollary 2.2.2**: \((X^{\prime}X)^{-}X^{\prime}Y\) _is a least squares estimate of \(\beta\)._

In Example 1.0.2, with \(M\) given in Exercise 1.5.8b, it is not difficult to see that

\[MY=\left[\begin{array}{c}\bar{y}_{1}.\\ \bar{y}_{1}.\\ \bar{y}_{1}.\\ \bar{y}_{21}.\\ \bar{y}_{3}.\\ \bar{y}_{3}.\\ \end{array}\right]=\left[\begin{array}{c}1\ 1\ 0\ 0\\ 1\ 1\ 0\ 0\\ 1\ 0\ 0\\ 1\ 0\ 1\ 0\\ 1\ 0\ 0\ 1\\ 1\ 0\ 0\ 0\ 1\\ \end{array}\right]\left[\begin{array}{c}0\\ \bar{y}_{1}.\\ y_{21}\\ \bar{y}_{3}.\\ \end{array}\right]=\left[\begin{array}{c}1\ 1\ 0\ 0\\ 1\ 1\ 0\ 0\\ 1\ 1\ 0\ 0\\ 1\ 0\ 1\ 0\\ 1\ 0\ 0\ 1\\ 1\ 0\ 0\ 0\ 1\\ \end{array}\right]\left[\begin{array}{c}\bar{y}_{1}.\\ 0\\ y_{21}-\bar{y}_{1}.\\ \bar{y}_{3}.\\ \end{array}\right].\]

Thus, both \(\hat{\beta}_{1}=(0,\bar{y}_{1}.,\,y_{21},\,\bar{y}_{3}.)^{\prime}\) and \(\hat{\beta}_{2}=(\bar{y}_{1}.,\,0,\,y_{21}-\bar{y}_{1}.,\,\bar{y}_{3}.-\bar{y}_ {1}.)^{\prime}\) are least squares estimates of \(\beta\). From Example 2.1.5,

\[\alpha_{1}-\alpha_{3}=(1,\,0,\,0,\,0,\,-1,\,0)X\beta=(1/3,\,1/3,\,1/3,\,0,\,-1/ 2,\,-1/2)X\beta.\]

The least squares estimates are

\[(1,\,0,\,0,\,0,\,-1,\,0)X\hat{\beta}=(1,\,0,\,0,\,0,\,-1,\,0)MY=\bar{y}_{1}.- \bar{y}_{3}.,\]

but also

\[(1/3,\,1/3,\,1/3,\,0,\,-1/2,\,-1/2)X\hat{\beta}=(1/3,\,1/3,\,1/3,\,0,\,-1/2,\, -1/2)MY\\ =\bar{y}_{1}.-\bar{y}_{3}.\]

Moreover, these estimates do not depend on the choice of least squares estimates. Either \(\hat{\beta}_{1}\) or \(\hat{\beta}_{2}\) gives this result.

In Example 1.0.1, \(X^{\prime}X\) has a true inverse, so the unique least squares estimate of \(\beta\) is

\[\hat{\beta}=\left(X^{\prime}X\right)^{-1}X^{\prime}Y=\left[\begin{array}{c}6 &21\\ 21&91\\ \end{array}\right]^{-1}\left[\begin{array}{c}1\ 1\ 1\ 1\ 1\ 1\\ 1\ 2\ 3\ 4\ 5\ 6\\ \end{array}\right]Y.\]An immediate result of Theorem 2.2.1, the uniqueness of perpendicular projection operators (Proposition B.34), and Theorem 2.1.2 as applied to linear models, is that the least squares estimate of any identifiable function is unique. Any least squares estimates \(\hat{\beta}_{1}\) and \(\hat{\beta}_{2}\) have \(X\hat{\beta}_{1}=X\hat{\beta}_{2}\), so if \(g(\beta)\) is identifiable, \(g(\hat{\beta}_{1})=g(\hat{\beta}_{2})\). In particular, least squares estimates of estimable functions are unique.

**Corollary 2.2.3**: _The unique least squares estimate of \(\rho^{\prime}X\beta\) is \(\rho^{\prime}MY\)._

Recall that a vector-valued linear function of the parameters, say \(\Lambda^{\prime}\beta\), is estimable if and only if \(\Lambda^{\prime}=P^{\prime}X\) for some matrix \(P\). The unique least squares estimate of \(\Lambda^{\prime}\beta\) is then \(P^{\prime}MY=\Lambda^{\prime}\hat{\beta}\).

We now show that the least squares estimate of \(\lambda^{\prime}\beta\) is unique only if \(\lambda^{\prime}\beta\) is estimable.

**Theorem 2.2.4**: \(\lambda^{\prime}=\rho^{\prime}X\) _if \(\lambda^{\prime}\hat{\beta}_{1}=\lambda^{\prime}\hat{\beta}_{2}\) for any \(\hat{\beta}_{1}\), \(\hat{\beta}_{2}\) that satisfy \(X\hat{\beta}_{1}=X\hat{\beta}_{2}=MY\)._

_Proof_  Decompose \(\lambda\) into vectors in \(C(X^{\prime})\) and its orthogonal complement. Let \(N\) be the perpendicular projection operator onto \(C(X^{\prime})\); then we can write \(\lambda=X^{\prime}\rho_{1}+(I-N)\rho_{2}\). We want to show that \((I-N)\rho_{2}=0\). This will be done by showing that \((I-N)\rho_{2}\) is orthogonal to every vector in \(\mathbb{R}^{p}\).

By assumption, \(\lambda^{\prime}(\hat{\beta}_{1}-\hat{\beta}_{2})=0\), and we know that \(\rho_{1}^{\prime}X(\hat{\beta}_{1}-\hat{\beta}_{2})=0\); so we must have \(\rho_{2}^{\prime}(I-N)(\hat{\beta}_{1}-\hat{\beta}_{2})=0\), and this holds for any least squares estimates \(\hat{\beta}_{1}\), \(\hat{\beta}_{2}\).

Let \(\hat{\beta}_{1}\) be any least squares estimate and take \(v\) such that \(v\perp C(X^{\prime})\), then \(\hat{\beta}_{2}=\hat{\beta}_{1}-v\) is a least squares estimate. This follows because \(X\hat{\beta}_{2}=X\hat{\beta}_{1}-Xv=X\hat{\beta}_{1}=MY\). Substituting above gives \(0=\rho_{2}^{\prime}(I-N)(\hat{\beta}_{1}-\hat{\beta}_{2})=\rho_{2}^{\prime}(I -N)v\) for any \(v\perp C(X^{\prime})\). Moreover, by definition of \(N\) for any \(v\in C(X^{\prime})\), \((I-N)v=0\). It follows that \(\rho_{2}^{\prime}(I-N)v=0\) for any \(v\in\mathbb{R}^{p}\) and thus \((I-N)\rho_{2}=0\). \(\square\)

When \(\beta\) is not identifiable, sometimes _side conditions_ are arbitrarily imposed on the parameters to allow "estimation" of nonidentifiable parameters. Imposing side conditions amounts to choosing one particular least squares estimate of \(\beta\). In our earlier discussion of estimation for Example 1.0.2, we presented two sets of parameter estimates. The first estimate, \(\hat{\beta}_{1}\), arbitrarily imposed \(\mu=0\) and \(\hat{\beta}_{2}\) arbitrarily imposed \(\alpha_{1}=0\). Side conditions determine a particular least squares estimate by introducing a nonidentifiable, typically a linear nonestimable, constraint on the parameters. With \(r\equiv r(X)<p\), one needs \(p-r\) individual side conditions to identify the parameters and thus allow "estimation" of the otherwise nonidentifiable parameters. Initially, the model was overparameterized. A linear nonestimable constraint is chosen to remove the ambiguity. Fundamentally, one choice of side conditions is as good as any other. See the discussion near Corollary 3.3.8 for further explication of linear nonestimable constraints. The use of a side condition in one-way ANOVA is also considered in Chapter 4.

When \(n<p\), \(\beta\) in never estimable.

Personally, I find it silly to pretend that nonidentifiable functions of the parameters can be estimated. To do so requires strong assumptions that should be made explicit, cf. Christensen (2018). The one good thing about imposing arbitrary side conditions is that they allow computer programs to print out parameter estimates. But different programs use different (equally valid) side conditions, so the printed estimates may differ from program to program. Fortunately, the estimates should agree on all estimable (and, more generally, identifiable) functions of the parameters.

_Least squares estimation is not a statistical procedure!_ Its justification as an optimal estimate is geometric, not statistical. Much of this chapter is devoted to establishing the statistical properties of these geometrically optimal estimates. First, we note that least squares estimates of estimable functions are unbiased.

**Proposition 2.2.5**: _If \(\lambda^{\prime}=\rho^{\prime}X\), then \(\operatorname{E}(\rho^{\prime}MY)=\lambda^{\prime}\beta\)._

_Proof_ \(\operatorname{E}(\rho^{\prime}MY)=\rho^{\prime}ME(Y)=\rho^{\prime}MX\beta= \rho^{\prime}X\beta=\lambda^{\prime}\beta\). \(\square\)

None of our results on least squares estimation have involved the standard assumption that \(\operatorname{Cov}(e)=\sigma^{2}I\). Least squares provides unique estimates of identifiable functions and unbiased estimates of estimable functions, regardless of the covariance structure. The next three sections establish that least squares estimates have good statistical properties when \(\operatorname{Cov}(e)=\sigma^{2}I\).

We now consider unbiased estimation of the variance parameter \(\sigma^{2}\). First write the _fitted values_ (also called the _predicted values_)

\[\hat{Y}\equiv X\hat{\beta}=MY\]

and the _residuals_

\[\hat{e}\equiv Y-X\hat{\beta}=(I-M)Y.\]

The data vector \(Y\) can be decomposed as

\[Y=\hat{Y}+\hat{e}=MY+(I-M)Y.\]

The perpendicular projection of \(Y\) onto \(C(X)\) (i.e., \(MY\)) provides an estimate of \(X\beta\). Note that \(MY=MX\beta+Me=X\beta+Me\) so that \(MY\) equals \(X\beta\) plus some error where \(\operatorname{E}(Me)=ME(e)=0\). Similarly, \((I-M)Y=(I-M)X\beta+(I-M)e=(I-M)e\), so \((I-M)Y\) depends only on the error vector \(e\). Since \(\sigma^{2}\) is a property of the error vector, it is reasonable to use \((I-M)Y\) to estimate \(\sigma^{2}\).

**Theorem 2.2.6**: _Let \(r(X)=r\) and \(\operatorname{Cov}(e)=\sigma^{2}I\), then \(Y^{\prime}(I-M)Y/(n-r)\) is an unbiased estimate of \(\sigma^{2}\).__Proof_ From Theorem 1.3.2 and the facts that \(\operatorname{E}(Y)=X\beta\) and \(\operatorname{Cov}(Y)=\sigma^{2}I\), we have

\[\operatorname{E}\bigl{[}Y^{\prime}(I-M)Y\bigr{]}=\operatorname{tr}\bigl{[} \sigma^{2}(I-M)\bigr{]}+\beta^{\prime}X^{\prime}(I-M)X\beta.\]

However, \(\operatorname{tr}\bigl{[}\sigma^{2}(I-M)\bigr{]}=\sigma^{2}\operatorname{tr}( I-M)=\sigma^{2}\operatorname{r}(I-M)=\sigma^{2}\operatorname{(}n-r)\) and \((I-M)X=0\), so \(\beta^{\prime}X^{\prime}(I-M)X\beta=0\); therefore,

\[\operatorname{E}\bigl{[}Y^{\prime}(I-M)Y\bigr{]}=\sigma^{2}\operatorname{(}n-r)\]

and

\[\operatorname{E}\bigl{[}Y^{\prime}(I-M)Y/(n-r)\bigr{]}=\sigma^{2}.\]

\(Y^{\prime}(I-M)Y=\hat{e}^{\prime}\hat{e}\) is called the _sum of squares for error_ (_SSE_). It is the squared length of the residual vector \((I-M)Y\). \(Y^{\prime}(I-M)Y/(n-r)\) is called the _mean squared error_ (_MSE_). It is the squared length of \((I-M)Y\) divided by the rank of \((I-M)\). In a sense, the _MSE_ is the average squared length of \((I-M)Y\), where the average is over the number of dimensions in \(C(I-M)\). The rank of \(I-M\) is called the _degrees of freedom for error_, denoted \(dfE\).

For Example 1.0.1,

\[SSE=(y_{1}-\hat{\beta}_{0}-\hat{\beta}_{1}1)^{2}+(y_{2}-\hat{\beta}_{0}-\hat{ \beta}_{1}2)^{2}+\cdots+(y_{6}-\hat{\beta}_{0}-\hat{\beta}_{1}6)^{2}\]

and \(MSE=SSE/(6-2)\). For Example 1.0.2,

\[SSE=(y_{11}-\bar{y}_{1}.)^{2}+(y_{12}-\bar{y}_{1}.)^{2}+ (y_{13}-\bar{y}_{1}.)^{2}\] \[+(y_{21}-y_{21})^{2}+(y_{31}-\bar{y}_{3}.)^{2}+(y_{32}-\bar{y}_{3 }.)^{2}\]

and \(MSE=SSE/(6-3)\).

Finally, one can think about the geometry of least squares estimation in three dimensions. Consider a rectangular table. (Yes, that furniture you have in your kitchen!) Take one corner of the table to be the origin. Take \(C(X)\) as the two-dimensional subspace determined by the surface of the table. \(Y\) can be any vector originating at the origin, i.e., any point in three-dimensional space. The linear model says that \(\operatorname{E}(Y)=X\beta\), which just says that \(\operatorname{E}(Y)\) is somewhere on the surface of the table. The least squares estimate \(MY=X\hat{\beta}\) is the perpendicular projection of \(Y\) onto the table surface. The residual vector \((I-M)Y\) is the vector starting at the origin, perpendicular to the surface of the table, that reaches the same height as \(Y\). Another way to think of the residual vector is to connect the ends of \(MY\) and \(Y\) with a line segment (that is perpendicular to the surface of the table) but then shift the line segment along the surface (keeping it perpendicular) until the line segment has one end at the origin. The residual vector is the perpendicular projection of \(Y\) onto \(C(I-M)\), that is, the projection onto the orthogonal complement of the table surface. The orthogonal complement is the one-dimension space in the vertical direction that goes through the origin. Because the orthogonal complement has only one dimension, _MSE_ is just the squared length of the residual vector.

Alternatively, one could take \(C(X)\) to be a one-dimensional subspace determined by an edge of the table that includes the origin. The linear model now says that \(\mathrm{E}(Y)\) is somewhere on this edge of the table. \(MY=X\hat{\beta}\) is found by dropping a perpendicular from \(Y\) to the edge of the table. If you connect \(MY\) and \(Y\), you essentially get the residual vector \((I-M)Y\), except that the line segment has to be shifted down the edge so that it has one end at the origin. The residual vector is perpendicular to the \(C(X)\) edge of the table, but typically would not be perpendicular to the surface of the table. \(C(I-M)\) is now the plane that contains everything (through the origin) that is perpendicular to the \(C(X)\) edge of the table. In other words, \(C(I-M)\) is the two-dimensional space determined by the vertical direction and the _other_ edge of the table that goes through the origin. _MSE_ is the squared length of the residual vector divided by 2, because \(C(I-M)\) is a two-dimensional space.

### Estimation: Best Linear Unbiased

Another criterion for estimation of \(\lambda^{\prime}\beta\) is to choose the best linear unbiased estimate (_BLUE_) of \(\lambda^{\prime}\beta\). We prove the Gauss-Markov theorem that least squares estimates are best linear unbiased estimates.

**Definition 2.3.1**: \(a^{\prime}Y\) is a _best linear unbiased estimate_ of \(\lambda^{\prime}\beta\) if \(a^{\prime}Y\) is unbiased and if for any other linear unbiased estimate \(b^{\prime}Y\), \(\mathrm{Var}(a^{\prime}Y)\leq\mathrm{Var}(b^{\prime}Y)\).

**Gauss-Markov Theorem 2.3.2**: _Consider the standard linear model_

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I.\]

_If \(\lambda^{\prime}\beta\) is estimable, then the least squares estimate of \(\lambda^{\prime}\beta\) is a BLUE of \(\lambda^{\prime}\beta\)._

_Proof_ Let \(M\) be the perpendicular projection operator onto \(C(X)\). Since \(\lambda^{\prime}\beta\) is an estimable function, let \(\lambda^{\prime}=\rho^{\prime}X\) for some \(\rho\). We need to show that if \(a^{\prime}Y\) is an unbiased estimate of \(\lambda^{\prime}\beta\), then \(\mathrm{Var}(a^{\prime}Y)\geq\mathrm{Var}(\rho^{\prime}MY)\). Since \(a^{\prime}Y\) is unbiased for \(\lambda^{\prime}\beta\), \(\lambda^{\prime}\beta=\mathrm{E}(a^{\prime}Y)=a^{\prime}X\beta\) for any value of \(\beta\). Therefore \(\rho^{\prime}X=\lambda^{\prime}=a^{\prime}X\). Write

\[\mathrm{Var}(a^{\prime}Y) = \mathrm{Var}(a^{\prime}Y-\rho^{\prime}MY+\rho^{\prime}MY)\] \[= \mathrm{Var}(a^{\prime}Y-\rho^{\prime}MY)+\mathrm{Var}(\rho^{ \prime}MY)+2\mathrm{Cov}\big{[}(a^{\prime}Y-\rho^{\prime}MY),\rho^{\prime}MY \big{]}.\]

Since \(\mathrm{Var}(a^{\prime}Y-\rho^{\prime}MY)\geq 0\), if we show that \(\mathrm{Cov}\big{[}(a^{\prime}Y-\rho^{\prime}MY),\rho^{\prime}MY\big{]}=0\), then \(\mathrm{Var}(a^{\prime}Y)\geq\mathrm{Var}(\rho^{\prime}MY)\) and the theorem holds.

We now show that \(\mathrm{Cov}\big{[}(a^{\prime}Y-\rho^{\prime}MY),\rho^{\prime}MY\big{]}=0\).

\[\begin{array}{rl}\operatorname{Cov}\bigl{[}(a^{\prime}Y-\rho^{\prime}MY),\,\rho^{ \prime}MY\bigr{]}&=\operatorname{Cov}\bigl{[}(a^{\prime}-\rho^{\prime}M)Y,\,\rho^ {\prime}MY\bigr{]}\\ &=(a^{\prime}-\rho^{\prime}M)\operatorname{Cov}(Y)M\rho\\ &=\sigma^{2}(a^{\prime}-\rho^{\prime}M)M\rho\\ &=\sigma^{2}(a^{\prime}M-\rho^{\prime}M)\rho.\end{array}\]

As shown above, \(a^{\prime}X=\rho^{\prime}X\), and since we can write \(M=X(X^{\prime}X)^{-}X^{\prime}\), we have \(a^{\prime}M=\rho^{\prime}M\). It follows that \(\sigma^{2}(a^{\prime}M-\rho^{\prime}M)\rho=0\) as required. 

Corollary 2.3.3: _If \(\sigma^{2}>0\), there exists a unique BLUE for any estimable function \(\lambda^{\prime}\beta\)._

Proof: Let \(\lambda^{\prime}=\rho^{\prime}X\), and recall from Section 1 that the vector \(\rho^{\prime}M\) is uniquely determined by \(\lambda^{\prime}\). In the proof of Theorem 2.3.2, it was shown that for an arbitrary linear unbiased estimate \(a^{\prime}Y\),

\[\operatorname{Var}(a^{\prime}Y)=\operatorname{Var}(\rho^{\prime}MY)+ \operatorname{Var}(a^{\prime}Y-\rho^{\prime}MY).\]

If \(a^{\prime}Y\) is a BLUE of \(\lambda^{\prime}\beta\), it must be true that \(\operatorname{Var}(a^{\prime}Y-\rho^{\prime}MY)=0\). It is easily seen that

\[0=\operatorname{Var}(a^{\prime}Y-\rho^{\prime}MY)=\operatorname{Var}\bigl{[}(a ^{\prime}-\rho^{\prime}M)Y\bigr{]}=\sigma^{2}(a-M\rho)^{\prime}(a-M\rho).\]

For \(\sigma^{2}>0\), this occurs if and only if \(a-M\rho=0\), which is equivalent to the condition \(a=M\rho\). 

### Estimation: Maximum Likelihood

Another criterion for choosing estimates of \(\beta\) and \(\sigma^{2}\) is maximum likelihood. The likelihood function is derived from the joint density of the observations by considering the parameters as variables and the observations as fixed at their observed values. If we assume \(Y\sim N\bigl{(}X\beta,\sigma^{2}I\bigr{)}\), then the _maximum likelihood estimates_ (_MLE_s) of \(\beta\) and \(\sigma^{2}\) are obtained by maximizing

\[(2\pi)^{-n/2}[\det(\sigma^{2}I)]^{-1/2}\exp\bigl{[}-(Y-X\beta)^{\prime}(Y-X \beta)/2\sigma^{2}\bigr{]}. \tag{1}\]

Equivalently, the log of the likelihood can be maximized. The log of (1) is

\[\frac{-n}{2}\log(2\pi)-\frac{1}{2}\log[(\sigma^{2})^{n}]-(Y-X\beta)^{\prime}(Y -X\beta)/2\sigma^{2}.\]

For every value of \(\sigma^{2}\), the log-likelihood is maximized by taking \(\beta\) to minimize \((Y-X\beta)^{\prime}(Y-X\beta)\), i.e., least squares estimates are MLEs. To estimate \(\sigma^{2}\) we can substitute \(Y^{\prime}(I-M)Y=(Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})\) for \((Y-X\beta)^{\prime}(Y-X\beta)\) and differentiate with respect to \(\sigma^{2}\) to get \(Y^{\prime}(I-M)Y/n\) as the MLE of \(\sigma^{2}\).

The MLE of \(\sigma^{2}\) is rarely used in practice. The _MSE_ is the standard estimate of \(\sigma^{2}\). For almost any purpose except point estimation of \(\sigma^{2}\), it is immaterial whether the _MSE_ or the MLE is used. They lead to identical confidence intervals and tests for \(\sigma^{2}\). They also lead to identical confidence regions and tests for estimable functions of \(\beta\). It should be emphasized that it is not appropriate to substitute the MLE for the _MSE_ and then form confidence intervals and tests as if the _MSE_ were being used.

### Estimation: Minimum Variance Unbiased

In Section 3, it was shown that least squares estimates give best estimates among the class of linear unbiased estimates. If the error vector is normally distributed, least squares estimates are best estimates among all unbiased estimates, not just the linear unbiased estimates. In particular, with normal errors, the best estimates happen to be linear estimates. As in Section 3, a best unbiased estimate is taken to be an unbiased estimate with minimum variance.

It is not the purpose of this monograph to develop the theory of minimum variance unbiased estimation. However, we will outline the application of this theory to linear models. See Lehmann (1983, Sections 1.4, 1.5) and Lehmann (1986, Sections 2.6, 2.7, 4.3) for a detailed discussion of the definitions and theorems used here. Our model is

\[Y=X\beta+e,\hskip 14.226378pte\sim N\big{(}0,\sigma^{2}I\big{)}\,.\]

**Definition 2.5.1**: A vector-valued sufficient statistic \(T(Y)\) is said to be _complete_ if \(\mathrm{E}[h(T(Y))]=0\) for all \(\beta\) and \(\sigma^{2}\) implies that \(\Pr[h(T(Y))=0]=1\) for all \(\beta\) and \(\sigma^{2}\).

**Theorem 2.5.2**: _If \(T(Y)\) is a complete sufficient statistic, then \(f(T(Y))\) is a minimum variance unbiased estimate (MVUE) of \(\mathrm{E}[f(T(Y))]\)._

_Proof_ Suppose \(g(Y)\) is an unbiased estimate of \(\mathrm{E}[f(T(Y))]\). By the Rao-Blackwell theorem (see Cox and Hinkley 1974),

\[\mathrm{Var}(\mathrm{E}[g(Y)|T(Y)])\leq\mathrm{Var}(g(Y))\,.\]

Since \(\mathrm{E}[g(Y)|T(Y)]\) is unbiased, \(\mathrm{E}[f(T(Y))-\mathrm{E}[g(Y)|T(Y)]]=0\). By completeness of \(T(Y)\), \(\Pr[f(T(Y))=\mathrm{E}[g(Y)|T(Y)]]=1\). It follows that \(\mathrm{Var}(f(T(Y)))\leq\mathrm{Var}(g(Y))\)We wish to use the following result from Lehmann (1983, pp. 28, 46):

**Theorem 2.5.3**: _Let \(\theta=(\theta_{1},\ldots,\theta_{s})^{\prime}\) and let \(Y\) be a random vector with probability density function_

\[f(Y)=c(\theta)\exp\!\left[\sum_{i=1}^{s}\theta_{i}T_{i}(Y)\right]h(Y);\]

_then \(T(Y)=(T_{1}(Y),\,T_{2}(Y),\,\ldots,\,T_{s}(Y))^{\prime}\) is a complete sufficient statistic provided that neither \(\theta\) nor \(T(Y)\) satisfy any linear constraints._

Suppose \(r(X)=r<p\), then the theorem cannot apply to \(X^{\prime}Y\) because, for \(b\perp C(X^{\prime}),\,Xb=0\); so \(b^{\prime}X^{\prime}Y\) is subject to a linear constraint. We need to consider the following reparameterization. Let \(Z\) be an \(n\times r\) matrix whose columns form a basis for \(C(X)\). For some matrix \(U\), we have \(X=ZU\). Let \(\lambda^{\prime}\beta\) be an estimable function. Then for some \(\rho\), \(\lambda^{\prime}\beta=\rho^{\prime}X\beta=\rho^{\prime}ZU\beta\). Define \(\gamma=U\beta\) and consider the linear model

\[Y=Z\gamma+e,\hskip 19.916929pte\sim N\big{(}0,\sigma^{2}I\big{)}\,.\]

The usual estimate of \(\lambda^{\prime}\beta=\rho^{\prime}Z\gamma\) is \(\rho^{\prime}MY\) regardless of the parameterization used. We will show that \(\rho^{\prime}MY\) is a minimum variance unbiased estimate of \(\lambda^{\prime}\beta\). The density of \(Y\) can be written

\[f(Y) = (2\pi)^{-n/2}\left(\sigma^{2}\right)^{-n/2}\exp\!\left[-(Y-Z \gamma)^{\prime}(Y-Z\gamma)/2\sigma^{2}\right]\] \[= C_{1}(\sigma^{2})\exp\!\left[-\left(Y^{\prime}Y-2\gamma^{\prime} Z^{\prime}Y+\gamma^{\prime}Z^{\prime}Z\gamma\right)/2\sigma^{2}\right]\] \[= C_{2}(\gamma,\sigma^{2})\exp\!\left[(-1/2\sigma^{2})Y^{\prime}Y+ (\sigma^{-2}\gamma^{\prime})(Z^{\prime}Y)\right].\]

This is the form of Theorem 2.5.3. There are no linear constraints on the parameters \((-1/2\sigma^{2},\,\gamma_{1}/\sigma^{2},\,\ldots,\,\gamma_{r}/\sigma^{2})\) nor on \((Y^{\prime}Y,\,Y^{\prime}Z)^{\prime}\), so \((Y^{\prime}Y,\,Y^{\prime}Z)^{\prime}\) is a complete sufficient statistic. An unbiased estimate of \(\lambda^{\prime}\beta=\rho^{\prime}X\beta\) is \(\rho^{\prime}MY=\rho^{\prime}Z(Z^{\prime}Z)^{-1}Z^{\prime}Y\). \(\rho^{\prime}MY\) is a function of \(Z^{\prime}Y\), so it is a minimum variance unbiased estimate. Moreover, \(Y^{\prime}(I-M)Y/(n-r)\) is an unbiased estimate of \(\sigma^{2}\) and \(Y^{\prime}(I-M)Y=Y^{\prime}Y-(Y^{\prime}Z)(Z^{\prime}Z)^{-1}(Z^{\prime}Y)\) is a function of the complete sufficient statistic \((Y^{\prime}Y,\,Y^{\prime}Z)^{\prime}\), so \(MSE\) is a minimum variance unbiased estimate. We have established the following result:

**Theorem 2.5.4**: _MSE is a minimum variance unbiased estimate of \(\sigma^{2}\) and \(\rho^{\prime}MY\) is a minimum variance unbiased estimate of \(\rho^{\prime}X\beta\) whenever \(e\sim N\big{(}0,\sigma^{2}I\big{)}\)._

### Sampling Distributions of Estimates

If we continue to assume that \(Y\sim N\big{(}X\beta,\sigma^{2}I\big{)}\), the distributions of estimates are straightforward. The least squares estimate of \(\Lambda^{\prime}\beta\) where \(\Lambda^{\prime}=P^{\prime}X\) is \(\Lambda^{\prime}\hat{\beta}=P^{\prime}MY\). The distribution of \(P^{\prime}MY\) is \(N\big{(}P^{\prime}X\beta,\sigma^{2}P^{\prime}MIMP\big{)}\) or, equivalently,

\[P^{\prime}MY\sim N\big{(}\Lambda^{\prime}\beta,\sigma^{2}P^{\prime}MP\big{)}\;.\]

Since \(M=X(X^{\prime}X)^{-}X^{\prime}\), we can also write

\[\Lambda^{\prime}\hat{\beta}\sim N\big{(}\Lambda^{\prime}\beta,\sigma^{2} \Lambda^{\prime}(X^{\prime}X)^{-}\Lambda\big{)}\;.\]

Two special cases are of interest. First, the estimate of \(X\beta\) is

\[\hat{Y}\equiv MY\sim N\big{(}X\beta,\sigma^{2}M\big{)}\;.\]

Second, if \((X^{\prime}X)\) is nonsingular, \(\beta\) is estimable and

\[\hat{\beta}\sim N\big{(}\beta,\sigma^{2}(X^{\prime}X)^{-1}\big{)}\;.\]

In Section 2 it was shown that the mean square error \(Y^{\prime}(I-M)Y/(n-r)\) is an unbiased estimate of \(\sigma^{2}\). We now show that \(Y^{\prime}(I-M)Y/\sigma^{2}\sim\chi^{2}(n-r)\). Clearly \(Y/\sigma\sim N(X\beta/\sigma,I)\), so by Theorem 1.3.3

\[Y^{\prime}(I-M)Y/\sigma^{2}\sim\chi^{2}\big{(}r(I-M),\,\beta^{\prime}X^{ \prime}(I-M)X\beta/2\sigma^{2}\big{)}\;.\]

We have already shown that \(r(I-M)=n-r\) and \(\beta^{\prime}X^{\prime}(I-M)X\beta/2\sigma^{2}=0\). Moreover, by Theorem 1.3.7, \(MY\) and \(Y^{\prime}(I-M)Y\) are independent.

**Exercise 2.1**: Show that for \(\lambda^{\prime}\beta\) estimable,

\[\frac{\lambda^{\prime}\hat{\beta}-\lambda^{\prime}\beta}{\sqrt{MSE\lambda^{ \prime}(X^{\prime}X)^{-}\lambda}}\sim t(dfE).\]

Find the form of an \(\alpha\) level test of \(H_{0}:\lambda^{\prime}\beta=0\) and the form for a \((1-\alpha)100\%\) confidence interval for \(\lambda^{\prime}\beta\). Hint: The test and confidence interval can be found using the methods of Appendix E.

**Exercise 2.2**: Let \(y_{11},\,y_{12},\,\ldots,\,y_{1r}\) be \(N(\mu_{1},\,\sigma^{2})\) and \(y_{21},\,y_{22},\,\ldots,\,y_{2s}\) be \(N(\mu_{2},\,\sigma^{2})\) with all \(y_{ij}\)s independent. Write this as a linear model. For the rest of the problem use the results of Chapter 2. Find estimates of \(\mu_{1},\mu_{2},\mu_{1}-\mu_{2}\), and \(\sigma^{2}\). Using Appendix E and Exercise 2.1, form an \(\alpha=0.01\) test for \(H_{0}:\mu_{1}=\mu_{2}\). Similarly, form 95% confidence intervals for \(\mu_{1}-\mu_{2}\) and \(\mu_{1}\). What is the test for \(H_{0}:\mu_{1}=\mu_{2}+\Delta\), where \(\Delta\)is some known fixed quantity? How do these results compare with the usual analysis for two independent samples with a common variance?

**Exercise 2.3**.: Let \(y_{1}\), \(y_{2}\),..., \(y_{n}\) be independent \(N(\mu,\sigma^{2})\). Write a linear model for these data. For the rest of the problem use the results of Chapter 2, Appendix E, and Exercise 2.1. Form an \(\alpha=0.01\) test for \(H_{0}:\mu=\mu_{0}\), where \(\mu_{0}\) is some known fixed number and form a 95% confidence interval for \(\mu\). How do these results compare with the usual analysis for one sample?

**Exercise 2.4**.: Use Corollary 1.3.6a to show that if \(\Lambda^{\prime}=P^{\prime}X\) then

\[\frac{(\Lambda^{\prime}\hat{\beta})^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}(\Lambda^{\prime}\hat{\beta})}{\sigma^{2}}\sim\chi^{2}[r(\Lambda),\,(\Lambda^{\prime}\beta)^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-}\Lambda] ^{-}(\Lambda^{\prime}\beta)]\]

Hints: To show that \(\Lambda^{\prime}\beta\in C(\Lambda^{\prime}(X^{\prime}X)^{-}\Lambda)\) and \(r(\Lambda)=r(\Lambda^{\prime}(X^{\prime}X)^{-}\Lambda)\), establish that \(C(\Lambda^{\prime})=C(P^{\prime}M)=C(P^{\prime}MP)\). Showing \(X^{\prime}Pb=0\) iff \(MPb=0\) implies that \(C(P^{\prime}X)^{\perp}=C(P^{\prime}M)^{\perp}\).

### Generalized Least Squares

A slightly more general linear model than the one considered so far is

\[Y=X\beta+e,\ \ \ \ \ \ \mathrm{E}(e)=0,\ \ \ \ \ \ \mathrm{Cov}(e)=\sigma^{2}V, \tag{1}\]

where \(V\) is some known positive definite matrix. By Corollary B.23, we can write \(V=Q\,Q^{\prime}\) for some nonsingular matrix \(Q\). It follows that \(Q^{-1}V\,Q^{\prime-1}=I\).

Instead of analyzing model (1), we analyze the equivalent model,

\[\tilde{Y}=\tilde{X}\beta+\tilde{e} \tag{2}\]

where

\[\tilde{Y}\equiv Q^{-1}Y,\ \ \ \tilde{X}\equiv Q^{-1}X,\ \ \ \tilde{e}\equiv Q^{-1 }e.\]

For model (2),

\[\mathrm{E}(\tilde{e})=\mathrm{E}(Q^{-1}e)=0\]

and

\[\mathrm{Cov}(\tilde{e})=\mathrm{Cov}(Q^{-1}e)=\sigma^{2}Q^{-1}V\,Q^{-1\prime} =\sigma^{2}I.\]

The transformed model (2) satisfies the assumptions made in the previously developed theory. For the transformed model, the least squares estimates minimize \[\|\tilde{Y}-\tilde{X}\beta\|^{2} \equiv (\tilde{Y}-\tilde{X}\beta)^{\prime}(\tilde{Y}-\tilde{X}\beta)\] \[= \left(Q^{-1}Y-Q^{-1}X\beta\right)^{\prime}\left(Q^{-1}Y-Q^{-1}X\beta\right)\] \[= \left[Q^{-1}(Y-X\beta)\right]^{\prime}\left[Q^{-1}(Y-X\beta)\right]\] \[= \left(Y-X\beta\right)^{\prime}Q^{-1\prime}Q^{-1}\left(Y-X\beta\right)\] \[= \left(Y-X\beta\right)^{\prime}V^{-1}\left(Y-X\beta\right)\equiv \|Y-X\beta\|_{V^{-1}}^{2}.\]

As functions of \(Y\) (rather than \(\tilde{Y}\)), the estimates of \(\beta\) that minimize this criterion are called _generalized least squares estimates_ because instead of minimizing the squared distance between \(Y\) and \(X\beta\), a generalized squared distance determined by \(V^{-1}\) is minimized. Generalized least squares is a concept in linear model theory and should not be confused with generalized linear models. To differentiate from generalized least squares, the least squares estimation of Section 2 is sometimes called _ordinary least squares (OLS)_.

**Theorem 2.7.1**:
1. \(\lambda^{\prime}\beta\) _is estimable in model (1) if and only if_ \(\lambda^{\prime}\beta\) _is estimable in model (2)._
2. \(\hat{\beta}\) _is a generalized least squares estimate of_ \(\beta\) _if and only if_ \[X\left(X^{\prime}V^{-1}X\right)^{-}X^{\prime}V^{-1}Y=X\hat{\beta}.\] _For any estimable function there exists a unique generalized least squares estimate._
3. _For an estimable function_ \(\lambda^{\prime}\beta\)_, the generalized least squares estimate is the BLUE of_ \(\lambda^{\prime}\beta\)_._
4. _If_ \(e\sim N(0,\sigma^{2}V)\)_, then for any estimable function_ \(\lambda^{\prime}\beta\)_, the generalized least squares estimate is the minimum variance unbiased estimate._
5. _If_ \(e\sim N(0,\sigma^{2}V)\)_, then any generalized least squares estimate of_ \(\beta\) _is a maximum likelihood estimate of_ \(\beta\)_._

_Proof_

1. If \(\lambda^{\prime}\beta\) is estimable in model (1), we can write \[\lambda^{\prime}=\rho^{\prime}X=\left(\rho^{\prime}Q\right)Q^{-1}X;\] so \(\lambda^{\prime}\beta\) is estimable in model (2). If \(\lambda^{\prime}\beta\) is estimable in model (2), then \(\lambda^{\prime}=\rho^{\prime}Q^{-1}X=\left(\rho^{\prime}Q^{-1}\right)X\); so \(\lambda^{\prime}\beta\) is estimable in model (1).

2. By Theorem 2.2.1, the generalized least squares estimates (i.e., the least squares estimates for model (2)) satisfy the equation \[Q^{-1}X\left(X^{\prime}Q^{-1\prime}Q^{-1}X\right)^{-}X^{\prime}Q^{-1\prime}Q^{ -1}Y=\tilde{X}(\tilde{X}^{\prime}\tilde{X})^{-}\tilde{X}^{\prime}\tilde{Y}= \tilde{X}\hat{\beta}=Q^{-1}X\hat{\beta}.\]

Simplifying and multiplying through on the left by \(Q\) gives the equivalent condition \[X\left(X^{\prime}V^{-1}X\right)^{-}X^{\prime}V^{-1}Y=X\hat{\beta}.\]

From Theorem 2.2.3, generalized least squares estimates of estimable functions are unique.

(c) From Theorem 2.3.2 as applied to model (2), the generalized least squares estimate of \(\lambda^{\prime}\beta\) is the BLUE of \(\lambda^{\prime}\beta\) among all unbiased linear combinations of the vector \(Q^{-1}Y\). However, any linear combination, in fact any function, of \(Y\) can be obtained from \(Q^{-1}Y\) because \(Q^{-1}\) is invertible. Thus, the generalized least squares estimate is the BLUE.

(d) Applying Theorem 2.5.2 to model (2) establishes that the generalized least squares estimate is the MVUE from among unbiased estimates that are functions of \(Q^{-1}Y\). Since \(Q\) is nonsingular, any function of \(Y\) can be written as a function of \(Q^{-1}Y\); so the generalized least squares estimate is the minimum variance unbiased estimate.

(e) The likelihood functions from models (1) and (2) are equal up to a constant of proportionality. From model (2), a generalized least squares estimate \(\hat{\beta}\) maximizes the likelihood among all functions of \(Q^{-1}Y\), but since \(Q\) is nonsingular, \(\hat{\beta}\) maximizes the likelihood among all functions of \(Y\). 

Theorem 2.7.1(b) is the generalized least squares equivalent of Theorem 2.2.1. Theorem 2.2.1 relates \(X\hat{\beta}\) to the perpendicular projection of \(Y\) onto \(C(X)\). Theorem 2.7.1(b) also relates \(X\hat{\beta}\) to a projection of \(Y\) onto \(C(X)\), but in Theorem 2.7.1(b) the projection is not the perpendicular projection. If we write

\[A=X\left(X^{\prime}V^{-1}X\right)^{-}X^{\prime}V^{-1}, \tag{3}\]

then the condition in Theorem 2.7.1(b) is

\[AY=X\hat{\beta}.\]

We wish to show that \(A\) is a projection operator onto \(C(X)\). The perpendicular projection operator onto \(C(Q^{-1}X)\) is

\[Q^{-1}X\left[(Q^{-1}X)^{\prime}(Q^{-1}X)\right]^{-}(Q^{-1}X)^{\prime}.\]

By the definition of a projection operator,

\[Q^{-1}X\left[(Q^{-1}X)^{\prime}(Q^{-1}X)\right]^{-}(Q^{-1}X)^{\prime}Q^{-1}X=Q ^{-1}X.\]

This can also be written as

\[Q^{-1}AX=Q^{-1}X.\]

Multiplying on the left by \(Q\) gives\[AX=X. \tag{4}\]

From (3) and (4), we immediately have

\[AA=A,\]

so \(A\) is a projection matrix. From (3), \(C(A)\subset C(X)\) and from (4), \(C(X)\subset C(A)\); so \(C(A)=C(X)\) and we have proven:

**Proposition 2.7.2**: _A is a projection operator onto \(C(X)\)._

For an estimable function \(\lambda^{\prime}\beta\) with \(\lambda^{\prime}=\rho^{\prime}X\), the generalized least squares estimate is \(\lambda^{\prime}\hat{\beta}=\rho^{\prime}AY\). This result is analogous to the ordinary least squares result in Corollary 2.2.3. To obtain tests and confidence intervals for \(\lambda^{\prime}\beta\), we need to know \(\operatorname{Cov}(X\hat{\beta})\).

**Proposition 2.7.3**: \(\operatorname{Cov}(X\hat{\beta})=\sigma^{2}X\left(X^{\prime}V^{-1}X\right)^{- }X^{\prime}\)_._

_Proof_ \(\operatorname{Cov}(X\hat{\beta})=\operatorname{Cov}(AY)=\sigma^{2}AVA^{\prime}\). From (3) and (4) it is easily seen (cf. Exercise 2.5) that

\[AVA^{\prime}=AV=VA^{\prime}.\]

In particular, \(AV=X\left(X^{\prime}V^{-1}X\right)^{-}X^{\prime}\). \(\square\)

**Corollary 2.7.4**: _If \(\lambda^{\prime}\beta\) is estimable, then the generalized least squares estimate has \(\operatorname{Var}(\lambda^{\prime}\hat{\beta})=\sigma^{2}\lambda^{\prime} \left(X^{\prime}V^{-1}X\right)^{-}\lambda\)._

**Exercise 2.5**:
* Show that \(AVA^{\prime}=AV=VA^{\prime}\).
* Show that \(A^{\prime}V^{-1}A=A^{\prime}V^{-1}=V^{-1}A\).
* Show that \(A\) is the same for any choice of \(\left(X^{\prime}V^{-1}X\right)^{-}\).

It is necessary to have an estimate of \(\sigma^{2}\). From model (2),

\[SSE = \left(Q^{-1}Y\right)^{\prime}\left[I-Q^{-1}X\left[(Q^{-1}X)^{ \prime}(Q^{-1}X)\right]^{-}(Q^{-1}X)^{\prime}\right](Q^{-1}Y)\] \[= Y^{\prime}V^{-1}Y-Y^{\prime}V^{-1}X\left(X^{\prime}V^{-1}X\right) ^{-}X^{\prime}V^{-1}Y\] \[= Y^{\prime}(I-A)^{\prime}V^{-1}(I-A)Y=\|(I-A)Y\|_{V^{-1}}^{2}.\]

Note that \((I-A)Y\) is the vector of residuals \(Y-X\hat{\beta}\), so the \(SSE\) is a quadratic form in the residuals. Because \(Q\) is nonsingular, \(r(Q^{-1}X)=r(X)\). It follows from model (2) that an unbiased estimate of \(\sigma^{2}\) is obtained from \[MSE=Y^{\prime}(I-A)^{\prime}V^{-1}(I-A)Y\big{/}\left[n-r(X)\right].\]

With normal errors, this is also the minimum variance unbiased estimate of \(\sigma^{2}\).

Suppose that \(e\) is normally distributed. From Theorem 1.3.7 applied to model (2), the _MSE_ is independent of \(Q^{-1}X\hat{\beta}\). Since \(X\hat{\beta}\) is a function of \(Q^{-1}X\hat{\beta}\), the _MSE_ is independent of \(X\hat{\beta}\). Moreover, \(X\hat{\beta}\) is normally distributed and \(SSE/\sigma^{2}\) has a chi-squared distribution.

A particular application of these results is that, for an estimable function \(\lambda^{\prime}\beta\),

\[\frac{\lambda^{\prime}\hat{\beta}-\lambda^{\prime}\beta}{\sqrt{MSE\lambda^{ \prime}\left(X^{\prime}V^{-1}X\right)^{-}\lambda}}\sim t(n-r(X)).\]

Given this distribution, tests and confidence intervals involving \(\lambda^{\prime}\beta\) can be obtained as in Appendix E.

We now give a result that determines when generalized least squares estimates are (ordinary) least squares estimates. The result will be generalized in Theorem 10.4.5. The generalization changes it to an if and only if statement for arbitrary covariance matrices.

**Proposition 2.7.5**: _If \(V\) is nonsingular and \(C(V\!X)\subset C(X)\), then least squares estimates are BLUEs._

_Proof_ The proof proceeds by showing that \(A\equiv X(X^{\prime}V^{-1}X)^{-}X^{\prime}V^{-1}\) is the perpendicular projection operator onto \(C(X)\). We already know that \(A\) is a projection operator onto \(C(X)\), so all we need to establish is that if \(w\perp C(X)\), then \(Aw=0\).

\(V\) being nonsingular implies that the null spaces of \(V\!X\) and \(X\) are identical, so \(r(V\!X)=r(X)\), cf. Exercise 2.11.7. With \(C(V\!X)\subset C(X)\), we must have \(C(V\!X)=C(X)\). \(C(V\!X)=C(X)\) implies that for some matrices \(B_{1}\) and \(B_{2}\), \(V\!X\!B_{1}=X\) and \(V\!X=X\!B_{2}\). Multiplying through by \(V^{-1}\) in both equations gives \(X\!B_{1}\!=\!V^{-1}X\) and \(X=V^{-1}X\!B_{2}\), so \(C(X)=C(V^{-1}X)\). It follows immediately that \(C(X)^{\perp}=C(V^{-1}X)^{\perp}\). Now, \(w\perp C(X)\) if and only if \(w\perp C(V^{-1}X)\), so

\[Aw=\left[X(X^{\prime}V^{-1}X)^{-}X^{\prime}V^{-1}\right]w=X(X^{\prime}V^{-1}X) ^{-}\left[X^{\prime}V^{-1}w\right]=0.\]

Frequently in regression analysis, \(V\) is a diagonal matrix, in which case generalized least squares is referred to as _weighted least squares (WLS)_. Considerable simplification results.

The following result will be useful in Section 10. It is essentially the Pythagorean theorem and can be used directly to show Theorem 2.7.1(b), that \(X\hat{\beta}\) is a generalized least squares estimate if and only if \(X\hat{\beta}=AY\).

**Lemma 2.7.6**: _Let \(A=X(X^{\prime}V^{-1}X)^{-}X^{\prime}V^{-1}\), then_

\[(Y-X\beta)V^{-1}(Y-X\beta) = (Y-AY)^{\prime}V^{-1}(Y-AY)+(AY-X\beta)V^{-1}(AY-X\beta)\] \[= (Y-AY)^{\prime}V^{-1}(Y-AY)+(\hat{\beta}-\beta)^{\prime}(X^{ \prime}V^{-1}X)(\hat{\beta}-\beta)\]

_where \(\hat{\beta}=(X^{\prime}V^{-1}X)^{-}X^{\prime}V^{-1}Y\)._

_Proof_ Following the proof of Theorem 2.2.2, write \((Y-X\beta)=(Y-AY)+(AY-X\beta)\) and eliminate cross product terms using Exercise 2.5 and

\[(I-A)^{\prime}V^{-1}(AY-X\beta)=V^{-1}(I-A)(AY-X\beta)=0.\]

**Exercise 2.6**: (a) Show that \(A\) is the projection operator onto \(C(X)\) along \(C\left(V^{-1}X\right)^{\perp}\).

(b) Show that \(A\) is the perpendicular projection operator onto \(C(X)\) when the inner product between two vectors \(x\) and \(y\) is defined as \(x^{\prime}V^{-1}y\). Hint: Recall the discussion after Definition B.50.

(c) Show that \(C(VX)\subset C(X)\) if and only if \(V=XU_{0}X^{\prime}+WU_{1}W^{\prime}\) for some \(U_{0}\) and \(U_{1}\) nonnegative definite with \(X^{\prime}W=0\) and \(r(W)=n-r(X)\). This is _Rao's simple covariance structure_.

(d) Show that \(\operatorname{Cov}[MY,(I-M)Y]=0\) when Rao's simple covariance structure holds.

### Normal Equations

An alternative method for finding least squares estimates of the parameter \(\beta\) in the model

\[Y=X\beta+e,\ \ \ \ \ \ \operatorname{E}(e)=0\]

is to find solutions of what are called the _normal equations_. The normal equations are defined as

\[X^{\prime}X\beta=X^{\prime}Y.\]

They are usually arrived at by setting equal to zero the partial derivatives of \((Y-X\beta)^{\prime}(Y-X\beta)\) with respect to \(\beta\).

Corollary 2.8.2 shows that solutions of the normal equations are least squares estimates of \(\beta\). Recall that, by Theorem 2.2.1, least squares estimates are solutions of \(X\beta=MY\).

**Theorem 2.8.1**: \(\hat{\beta}\) _is a least squares estimate of \(\beta\) if and only if \((Y-X\hat{\beta})\perp C(X)\).__Proof_  Since \(M\) is the perpendicular projection operator onto \(C(X)\), \((Y-X\hat{\beta})\perp C(X)\) if and only if \(M(Y-X\hat{\beta})=0\), i.e, if and only if \(MY=X\hat{\beta}\). \(\square\)

**Corollary 2.8.2**: \(\hat{\beta}\) _is a least squares estimate of \(\beta\) if and only if \(X^{\prime}X\hat{\beta}=X^{\prime}Y\)._

_Proof_ \(X^{\prime}X\hat{\beta}=X^{\prime}Y\) if and only if \(X^{\prime}(Y-X\hat{\beta})=0\), which occurs if and only if \((Y-X\hat{\beta})\perp C(X)\). \(\square\)

For generalized least squares problems, the normal equations are found from model (2.7.2). The normal equations simplify to

\[X^{\prime}V^{-1}X\beta=X^{\prime}V^{-1}Y.\]

### Variance-Bias Tradeoff

For a standard linear model the least squares estimates are the best linear unbiased estimates and for multivariate normal data they are the best unbiased estimates. They are best in the sense of having smallest variances. However, it turns out that you can often get better point estimates by incorporating a little bias. A little bit of bias can sometimes eliminate a great deal of variance, making for an overall better estimate.

Suppose the standard linear model

\[Y=X\beta+e,\ \ \ \mbox{E}(e)=0,\ \ \ \mbox{Cov}(e)=\sigma^{2}I, \tag{1}\]

is correct and consider fitting a _reduced_ model

\[Y=X_{0}\gamma+e,\ \ \ \mbox{with}\ C(X_{0})\subset C(X). \tag{2}\]

If \(\mbox{E}(Y)\neq X_{0}\gamma\), using the reduced linear model to estimate \(X\beta\) creates bias. In this section we examine how even incorrect reduced models can improve estimation if the bias they introduce is small relative to the variability in the model.

_Example 2.9.1_  Consider fitting a linear model with an intercept and three predictors. I am going to fit the full model using ordinary least squares. You, however, think that the regression coefficients for the second and third variable should be the same and that they should be half of the coefficient for the first variable. You incorporate that into your model. If you are correct, your fitted values will be twice as good as mine! But even if you are wrong, if you are close to being correct, your fitted values will still be better than mine. We now explore these claims in some generality.

Under the standard linear model (1), the best fitted values one could ever have are \(X\beta\) but we don't know \(\beta\). For estimated fitted values, say, \(F(Y)\), their quality can be 

[MISSING_PAGE_EMPTY:5361]

model is true, the bias is zero. But even when the reduced model is not true, if a reduced model with \(r(X_{0})\) substantially smaller than \(r(X)\) is close to being true, specifically if

\[\left\|X\beta\right.-M_{0}X\beta\left\|^{2}\right.<\sigma^{2}[r(X)-r(X_{0})],\]

the fitted values of the reduced model will be better estimates than the original least squares estimates. And don't forget that if the full model is not a regression, the estimates of all (estimable) identifiable functions have to be (linear) functions of whatever fitted values we use.

In Example 2.9.1, if the squared distance between the truth, \(X\beta\), and the expected value of your reduced model estimate, \(M_{0}X\beta\), is less than \(2\sigma^{2}\), you will do better than me. Of course we cannot know how close the truth is to the reduced model expected value, but in Subsection 14.1.3 we will see that Mallow's \(C_{p}\) statistic estimates \(r(X_{0})+\left\|X\beta\right.-M_{0}X\beta\left\|^{2}\right./\sigma^{2}\), so it gives us an idea about how much better (or worse) a reduced model is doing than the full model. In the context of variable selection, dropping predictor variables with regression coefficients close to zero should result in improved fitted values because the reduced model without those predictors should have \(M_{0}X\beta\doteq X\beta\).

Most biased estimation methods used in practice are immensely more complicated than this. The variable selection methods discussed in Chapter 14 are perhaps the most widely used methods of creating biased estimates. They use the data to determine an appropriate reduced model, so \(X_{0}\) is actually a function of \(Y\), say \(X_{0}(Y)\). The computations made here for a fixed \(X_{0}\) no longer apply. The models considered in Section 9.5 have model matrices that depend on \(Y\) in a particular fashion that makes them somewhat more tractable.

The alternative estimates discussed in Chapter 13 and _ALM-III_, Chapter 2 are also biased. If the choice of components in principal component regression is made without reference to \(Y\), then computations similar to those made here are possible. Ridge regression is also relatively tractable. _ALM-III_, Chapter 2 discusses other penalized least squares estimates. Their effects are harder to evaluate.

The Bayesian methods discussed in the next section, when using a proper prior on \(\beta\), also provide biased estimates. Whether or not they actually improve the estimates, in the sense discussed here, depends on how well the prior reflects reality.

#### Exercise 2.7

Consider the prediction of further observations \(Y_{new}\) from our standard linear model, i.e., new observations that are independent of the original observations. We assume that for some new model matrix \(X_{new}\), \(\operatorname{E}(Y_{new})=X_{new}\beta\) and \(\operatorname{Cov}(Y_{new})=\sigma^{2}I\) for an appropriate sized identity matrix. A predictor of \(Y_{new}\) is, say, \(G(Y)\). If the best predictor minimizes \(\operatorname{E}\left\{[Y_{new}\,-\,G(Y)]^{\prime}[Y_{new}\,-\,G(Y)]\right\}\), show that the best predictor minimizes \(\operatorname{E}\left\{[G(Y)-X_{new}\beta]^{\prime}[G(Y)-X_{new}\beta]\right\}\) so that the prediction problem reduces to an estimation problem.

#### Estimable Functions

Suppose we want to estimate some vector \(\Lambda^{\prime}\beta\). In Exercise 2.7, \(\Lambda^{\prime}=X_{new}\). For this vector to be estimable, we need \(\Lambda^{\prime}=P^{\prime}X\) for some \(P\). Recall that any such \(P\) can be replaced by the uniquely determined matrix \(MP\).

The mean squared error of estimation from an arbitrary estimate \(\tilde{\beta}\) is

\[\operatorname{E}\left[(\Lambda^{\prime}\tilde{\beta}-\Lambda^{\prime}\beta)^{ \prime}(\Lambda^{\prime}\tilde{\beta}-\Lambda^{\prime}\beta)\right].\]

Using the full model (1), the least squares estimate is \(\Lambda^{\prime}\hat{\beta}=P^{\prime}MY\) with

\[\operatorname{E}\left[(\Lambda^{\prime}\hat{\beta}-\Lambda^{\prime }\beta)^{\prime}(\Lambda^{\prime}\hat{\beta}-\Lambda^{\prime}\beta)\right] =\operatorname{E}\|P^{\prime}M(Y-X\beta)\|^{2}\] \[=\operatorname{tr}[M^{\prime}P(\sigma^{2}I)P^{\prime}M]\] \[=\sigma^{2}\operatorname{tr}[P^{\prime}MP]\] \[=\sigma^{2}\operatorname{tr}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda].\]

Fitting the reduced model (2), a possibly biased estimate of \(\Lambda^{\prime}\beta\) is \(P^{\prime}X_{0}\hat{\gamma}=P^{\prime}M_{0}Y\) with

\[\operatorname{E}\|P^{\prime}(M_{0}Y-X\beta)\|^{2} =\sigma^{2}\operatorname{tr}[M_{0}PP^{\prime}M_{0}]+\beta^{ \prime}X^{\prime}(I-M_{0})PP^{\prime}(I-M_{0})X\beta\] \[=\sigma^{2}\operatorname{tr}[P^{\prime}M_{0}P]+\|P^{\prime}(I-M_{ 0})X\beta\|^{2}.\]

The reduced model provides better estimates if

\[\beta^{\prime}X^{\prime}(I-M_{0})PP^{\prime}(I-M_{0})X\beta<\sigma^{2} \operatorname{tr}[P^{\prime}(M-M_{0})P].\]

If \(C(MP)\subset C(X_{0})\), the reduced model provides an unbiased estimate of \(\Lambda^{\prime}\beta\) and the reduced model gives no worse an estimate than the full model. If \(C(MP)\perp C(X_{0})\), the reduced model estimate is 0, so the variance term in the mean squared error is 0 but the bias of the expected squared error of estimation is \(\|\Lambda^{\prime}\beta\|^{2}\). The "estimate" 0 has better expected squared estimation error if \(\|\Lambda^{\prime}\beta\|^{2}<\sigma^{2}\operatorname{tr}[\Lambda^{\prime}(X^ {\prime}X)^{-}\Lambda]\). In general some combination of these phenomena occur because an arbitrary \(MP\) can be written as \(MP=MP_{1}+MP_{2}\) with \(C(MP_{1})\subset C(X_{0})\) and \(C(MP_{2})\perp C(X_{0})\).

#### Exercise 2.8

Show that if \(X_{0}\hat{\gamma}\) provides improved estimates relative to \(X\hat{\beta}\), then for any \(P\), \(P^{\prime}M_{0}Y\) is at least as good as \(P^{\prime}MY\).

### Bayesian Estimation

Bayesian estimation incorporates the analyst's subjective information about a problem into the analysis. It appears to be the only logically consistent method of analysis, but not the only useful one. Some people object to the loss of objectivity that results from using the analyst's subjective information, but either the data are strong enough for reasonable people to agree on their interpretation or, if not, analysts should be using their subjective (prior) information for making appropriate decisions related to the data.

There is a vast literature on Bayesian statistics. Three fundamental works are de Finetti (1974,1975), Jeffreys (1961), and Savage (1954). Good elementary introductions to the subject are Lindley (1971) and Berry (1996). A few of the well-known books on the subject are Berger (1993), Box and Tiao (1973), DeGroot (1970), Geisser (1993), Gelman et al. (2013), Raiffa and Schlaifer (1961), Robert (2007), and Zellner (1971). My favorite is now Christensen, Johnson, Branscum, and Hanson (2010), which also includes many more references to many more excellent books. Christensen et al. contains a far more extensive discussion of Bayesian linear models than this relatively short section.

Consider the linear model

\[Y=X\beta+e,\ \ \ e\sim N(0,\sigma^{2}I),\]

where \(r(X)=r\). It will be convenient to consider a full rank reparameterization of this model,

\[Y=Z\gamma+e,\ \ \ e\sim N(0,\sigma^{2}I),\]

where \(C(X)=C(Z)\). As in Sections 1.2 and 2.4, this determines a density for \(Y\) given \(\gamma\) and \(\sigma^{2}\), say, \(f(Y|\gamma,\sigma^{2})\). For a Bayesian analysis, we must have a joint density for \(\gamma\) and \(\sigma^{2}\), say \(p(\gamma,\sigma^{2})\). This distribution reflects the analyst's beliefs, prior to collecting data, about the process of generating the data. We will actually specify this distribution conditionally as \(p(\gamma,\sigma^{2})=p(\gamma|\sigma^{2})p(\sigma^{2})\). In practice, it is difficult to specify these distributions for \(\gamma\) given \(\sigma^{2}\) and \(\sigma^{2}\). Convenient choices that have minimal impact on (most aspects of) the analysis are the (improper) reference priors \(p(\gamma|\sigma^{2})=1\) and \(p(\sigma^{2})=1/\sigma^{2}\). These are improper in that neither prior density integrates to 1. Although these priors are convenient, a true Bayesian analysis requires the specification of proper prior distributions.

Specifying prior information is difficult, particularly about such abstract quantities as regression coefficients. A useful tool in specifying prior information is to think in terms of the mean of potential observations. For example, we could specify a vector of predictor variables, say \(\tilde{z}_{i}\), and specify the distribution for the mean of observations having those predictor variables. With covariates \(\tilde{z}_{i}\), the mean of potential observables is \(\tilde{z}_{i}^{\prime}\gamma\). Typically, we assume that \(\tilde{z}_{i}^{\prime}\gamma\) has a \(N(\tilde{y}_{i},\sigma^{2}/\tilde{w}_{i})\) distribution. One way to think about \(\tilde{y}_{i}\) and \(\tilde{w}_{i}\) is that \(\tilde{y}_{i}\) is a prior guess for what one would see with covariates \(\tilde{z}_{i}\), and \(\tilde{w}_{i}\) is the number of observations this guess is worth. To specify a proper priordistribution for \(\gamma\) given \(\sigma^{2}\), we specify independent priors at vectors \(\bar{z}_{i}\), \(i=1\),..., \(r\), where \(r\) is the dimension of \(\gamma\). As will be seen later, under a mild condition this prior specification leads easily to a proper prior distribution on \(\gamma\). Although choosing the \(\bar{z}_{i}\)s and specifying the priors may be difficult to do, it is much easier to do than trying to specify an intelligent joint prior directly on \(\gamma\). If one wishes to specify only partial prior information, one can simply choose fewer than \(r\) vectors \(\bar{z}_{i}\) and the analysis follows as outlined below. In fact, using the reference prior for \(\gamma\) amounts to not choosing any \(\bar{z}_{i}\)s. Again, the analysis follows as outlined below. Bedrick et al. (1996) and Christensen et al. (2010) discuss these techniques in more detail.

I believe that the most reasonable way to specify a proper prior on \(\sigma^{2}\) is to think in terms of the variability of potential observables around some fixed mean. Unfortunately, the implications of this idea are not currently as well understood as they are for the related technique of specifying the prior for \(\gamma\) indirectly through priors on means of potential observables. For now, we will simply consider priors for \(\sigma^{2}\) that are inverse gamma distributions, i.e., distributions in which \(1/\sigma^{2}\) has a gamma distribution. An inverse gamma distribution has two parameters, \(a\) and \(b\). One can think of the prior as being the equivalent of \(2a\) (prior) observations with a prior guess for \(\sigma^{2}\) of \(b/a\).

It is convenient to write \(\tilde{Y}=(\bar{y}_{1}\),..., \(\tilde{y}_{r})^{\prime}\) and \(\tilde{Z}\) as the \(r\,\times\,r\) matrix with \(i\)th row \(\bar{z}^{\prime}_{i}\). In summary, our distributional assumptions are

\[Y|\gamma,\sigma^{2}\ \sim\ N(Z\gamma,\sigma^{2}I),\] \[\tilde{Z}\gamma|\sigma^{2}\ \sim\ N(\tilde{Y},\sigma^{2}D^{-1}( \tilde{w})),\] \[\sigma^{2}\ \sim\ InvGa(a,b).\]

We assume that \(\tilde{Z}\) is nonsingular, so that the second of these induces the distribution

\[\gamma\,|\sigma^{2}\sim\ N(\tilde{Z}^{-1}\tilde{Y},\sigma^{2}\tilde{Z}^{-1}D^ {-1}(\tilde{w})\tilde{Z}^{-1}{}^{\prime}).\]

Actually, any multivariate normal distribution for \(\gamma\) given \(\sigma^{2}\) will lead to essentially the same analysis as given here.

A Bayesian analysis is based on finding the distribution of the parameters given the data, i.e., \(p(\gamma,\sigma^{2}|Y)\). This is accomplished by using _Bayes's theorem_, which states that

\[p(\gamma,\sigma^{2}|Y)=\frac{f(Y|\gamma,\sigma^{2})p(\gamma,\sigma^{2})}{f(Y)}.\]

If we know the numerator of the fraction, we can obtain the denominator by

\[f(Y)=\int f(Y|\gamma,\sigma^{2})p(\gamma,\sigma^{2})\,d\gamma\ d\sigma^{2}.\]

In fact, because of this relationship, we only need to know the numerator up to a constant multiple, because any multiple will cancel in the fraction.

Later in this section we will show that

\[p(\gamma,\sigma^{2}|Y) \propto\left(\sigma^{2}\right)^{-(n+r)/2}p(\sigma^{2})\] \[\quad\times\exp\left\{\frac{-1}{2\sigma^{2}}\left[(\gamma-\hat{ \gamma})^{\prime}(Z^{\prime}Z+\bar{Z}^{\prime}D(\tilde{w})\bar{Z})(\gamma- \hat{\gamma})\right]\right\} \tag{1}\] \[\quad\times\exp\left\{\frac{-1}{2\sigma^{2}}\left[(Y-Z\hat{\gamma })^{\prime}(Y-Z\hat{\gamma})+(\tilde{Y}-\tilde{Z}\hat{\gamma})^{\prime}D( \tilde{w})(\tilde{Y}-\tilde{Z}\hat{\gamma})\right]\right\},\]

where

\[\hat{\gamma}=\left(Z^{\prime}Z+\tilde{Z}^{\prime}D(\tilde{w})\tilde{Z}\right)^ {-1}\left[Z^{\prime}Y+\tilde{Z}^{\prime}D(\tilde{w})\tilde{Y}\right]. \tag{2}\]

The joint posterior (post data) density is the righthand side of (1) divided by its integral with respect to \(\gamma\) and \(\sigma^{2}\).

The form (1) for the joint distribution given the data is not particularly useful. What we really need are the marginal distributions of \(\sigma^{2}\) and functions \(\rho^{\prime}Z\gamma\equiv\rho^{\prime}X\beta\), and predictive distributions for new observations.

As will be shown, the Bayesian analysis turns out to be quite consistent with the frequentist analysis. For the time being, we use the reference prior \(p(\sigma^{2})=1/\sigma^{2}\). In our model, we have \(\tilde{Z}\gamma\) random, but it is convenient to think of \(\tilde{Y}\) as being \(r\) independent prior observations with mean \(\tilde{Z}\gamma\) and weights \(\tilde{w}\). Now consider the generalized least squares model

\[\begin{bmatrix}Y\\ \tilde{Y}\end{bmatrix}=\begin{bmatrix}Z\\ \tilde{Z}\end{bmatrix}\gamma+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix},\;\;\;\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}\sim N\left(\begin{bmatrix}0_{n\times 1}\\ 0_{r\times 1}\end{bmatrix},\sigma^{2}\begin{bmatrix}I_{n}&0\\ 0&D^{-1}(\tilde{w})\end{bmatrix}\right). \tag{3}\]

This generalized least squares model can also be written as, say,

\[Y_{*}=Z_{*}\gamma+e_{*},\;\;\;e_{*}\sim N(0,\sigma^{2}V_{*}).\]

The generalized least squares estimate from this model is \(\hat{\gamma}\) as given in (2). In the Bayesian analysis, \(\hat{\gamma}\) is the expected value of \(\gamma\) given \(Y\). Let _BMSE_ denote the mean squared error from the (Bayesian) generalized least squares model with _BdfE_ degrees of freedom for error.

In the frequentist generalized least squares analysis, for fixed \(\gamma\) with random \(\hat{\gamma}\) and _BMSE_,

\[\frac{\lambda^{\prime}\hat{\gamma}-\lambda^{\prime}\gamma}{\sqrt{\textit{BMSE }\lambda^{\prime}\left(Z^{\prime}Z+\bar{Z}^{\prime}D(\tilde{w})\bar{Z} \right)^{-1}\lambda}}\sim t\left(\textit{BdfE}\right).\]

In the Bayesian analysis the same distribution holds, but for fixed \(\hat{\gamma}\) and _BMSE_ with random \(\gamma\). Frequentist confidence intervals for \(\lambda^{\prime}\gamma\) are identical to Bayesian posterior probability intervals for \(\lambda^{\prime}\gamma\). Note that for estimating a function \(\rho^{\prime}X\beta\), simply write it as \(\rho^{\prime}X\beta=\rho^{\prime}Z\gamma\) and take \(\lambda^{\prime}=\rho^{\prime}Z\).

In the frequentist generalized least squares analysis, for fixed \(\sigma^{2}\) and random _BMSE_,

\[\frac{(\textit{BdfE})\textit{BMSE}}{\sigma^{2}}\sim\chi^{2}(\textit{BdfE}).\]

In the Bayesian analysis, the same distribution holds, but for fixed _BMSE_ and random \(\sigma^{2}\). Confidence intervals for \(\sigma^{2}\) are identical to Bayesian posterior probability intervals for \(\sigma^{2}\).

In the frequentist generalized least squares analysis, a prediction interval for a future independent observation \(y_{0}\) with predictor vector \(z_{0}\) and weight 1 is based on the distribution

\[\frac{y_{0}-z_{0}^{\prime}\hat{\gamma}}{\sqrt{\textit{BMSE}\left[1+z_{0}^{ \prime}\left(Z^{\prime}Z+\bar{Z}^{\prime}D(\tilde{w})\bar{Z}\right)^{-1}z_{0} \right]}}\sim t\,(\textit{BdfE}), \tag{4}\]

where \(\hat{\gamma}\) and _BMSE_ are random and \(y_{0}\) is independent of \(Y\) for given \(\gamma\) and \(\sigma^{2}\), see Exercise 2.11.1. In the Bayesian analysis, the same distribution holds, but for fixed \(\hat{\gamma}\) and _BMSE_. Standard prediction intervals for \(y_{0}\) are identical to Bayesian prediction intervals.

If we specify an improper prior on \(\gamma\) using fewer than \(r\) vectors \(\bar{z}_{i}\), these relationships between generalized least squares and the Bayesian analysis remain valid. In fact, for the reference prior on \(\gamma\), i.e., choosing no \(\bar{z}_{i}\)s, the generalized least squares model reduces to the usual model \(Y=X\beta+e\), \(e\sim N(0,\sigma^{2}I)\), and the Bayesian analysis becomes analogous to the usual ordinary least squares analysis.

In the generalized least squares model (3), \(\textit{BdfE}=n\). If we take \(\sigma^{2}{\sim}InvGa(a,b)\), the only changes in the Bayesian analysis are that _BMSE_ changes to \([(\textit{BdfE})\textit{BMSE}+2b]/(\textit{BdfE}+2a)\) and the degrees of freedom for the \(t\) and \(\chi^{2}\) distributions change from _BdfE_ to \(\textit{BdfE}+2a\). With reference priors for both \(\gamma\) and \(\sigma^{2}\), \(\textit{BdfE}=n-r\), as in ordinary least squares.

Schafer (1987) presented data on 93 individuals at the Harris Bank of Chicago in 1977. The response is beginning salary in hundreds of dollars. There are four covariates: sex, years of education, denoted EDUC, years of experience, denoted EXP, and time at hiring as measured in months after 1-1-69, denoted TIME. This is a regression, so we can take \(Z\equiv X\), \(\gamma\equiv\beta\), and write \(\bar{X}\equiv\bar{Z}\). With an intercept in the model, Johnson, Bedrick, and I began by specifying five covariate vectors \(\tilde{x}_{i}^{\prime}=(1,\textit{SEX}_{i},\textit{EDUC}_{i},\textit{EXP}_{i}, \textit{TIME}_{i})\), say, \((1,\,0,\,8,\,0,\,0)\), \((1,\,1,\,8,\,0,\,0)\), \((1,\,1,\,16,\,0,\,0)\), \((1,\,1,\,16,\,30,\,0)\), and \((1,\,1,\,8,\,0,\,36)\), where a SEX value of 1 indicates a male. For example, the vector \((1,\,0,\,8,\,0,\,0)\) corresponds to a male with 8 years of education, no previous experience, and starting work on 1-1-69. Thinking about the mean salary for each set of covariates, we chose \(\bar{y}^{\prime}=(40,\,40,\,60,\,70,\,50)\)which reflects a prior belief that starting salaries are the same for equally qualified men and women and a belief that salary is increasing as a function of EDUC, EXP, and TIME. The weights \(\tilde{w}_{i}\) are all chosen to be 0.4, so that in total the prior carries the same weight as two sampled observations. The induced prior on \(\beta\) given \(\sigma^{2}\) has mean vector (20, 0, 2.50, 0.33, 0.28)\({}^{\prime}\) and standard deviation vector (2.74, 2.24, 0.28, 0.07, 0.06)\({}^{\prime}\). (This analysis can be performed on standard regression software using the weights of prior observations and weights of 1 for actual observations.)

To illustrate partial prior information, we consider the same example, only with the fifth "prior observation" deleted. In this instance, the prior does not reflect any information about the response at TIMEs other than 0. The prior is informative about the mean responses at the first four covariate vectors but is noninformative (the prior is constant) for the mean response at the fifth covariate vector. Moreover, the prior is constant for the mean response with any other choice of fifth vector, provided this vector is linearly independent of the other four. (All such vectors must have a nonzero value for the time component.) In this example, the induced improper distribution on \(\beta\) has the same means and standard deviations for \(\beta_{1}\), \(\beta_{2}\), \(\beta_{3}\), and \(\beta_{4}\), but is flat for \(\beta_{5}\).

We specify a prior on \(\sigma^{2}\) worth \(2a=2\) observations with a prior guess for \(\sigma^{2}\) of \(b/a=25\). The prior guess reflects our belief that a typical salary has a standard deviation of 5.

Using our informative prior, the posterior mean of \(\beta\) is

\[\hat{\beta}=(33.68,6.96,1.02,0.18,0.23)^{\prime}\]

with \(\mathit{BdfE}=95\) and \(\mathit{BMSE}=2404/95=25.3053\). The standard deviations for \(\beta\) are \(\sqrt{95/93}(310,114,23.43,6.76,5.01)^{\prime}/100\). In the partially informative case discussed above, the posterior mean is

\[\hat{\beta}=(34.04,7.11,0.99,0.17,0.23),\]

\(\mathit{BdfE}=94\), \(\mathit{BMSE}=2383/94=25.3511\), and the standard deviations for \(\beta\) are \(\sqrt{94/92}(313,116,23.78,6.80,5.06)^{\prime}/100\). Using the standard reference prior, i.e., using ordinary least squares without prior data, \(\hat{\beta}=(35.26,7.22,0.90,0.15,0.23)^{\prime}\), \(\mathit{BdfE}=88\), \(\mathit{BMSE}=2266/88=25.75\), and the standard deviations for \(\beta\) are \(\sqrt{88/86}(328,118,24.70,7.05,5.20)^{\prime}/100\). The 95% prediction interval for \(x_{0}=(1,0,10,3.67,7)^{\prime}\) with a weight of 1 is \((35.97,56.34)\) for the informative prior, \((35.99,56.38)\) with partial prior information, and \((36.27,56.76)\) for the reference prior.

#### Distribution Theory

For the time being, we will assume relation (1) for the joint posterior and use it to arrive at marginal distributions. Afterwards, we will establish relation (1).

The distribution theory for the Bayesian analysis involves computations unlike anything else done in this book. It requires knowledge of some basic facts.

The density of a gamma distribution with parameters \(a>0\) and \(b>0\) is

\[g(\tau)=\frac{b^{a}}{\Gamma(a)}\tau^{a-1}\exp[-b\tau]\]

for \(\tau>0\). A \(Gamma(n/2,1/2)\) distribution is the same as a \(\chi^{2}(n)\) distribution. We will not need the density of an inverse gamma, only the fact that \(y\) has a \(Gamma(a,b)\) distribution if and only if \(1/y\) has an \(InvGa(a,b)\) distribution. The improper reference distribution corresponds to \(a=0,b=0\).

The \(t\) distribution is defined in Appendix C. The density of a \(t(n)\) distribution is

\[g(w)=\left[1+\frac{w^{2}}{n}\right]^{-(n+1)/2}\Gamma\left(\frac{n+1}{2}\right) \Big{/}\left[\Gamma\left(\frac{n}{2}\right)\sqrt{n\pi}\right].\]

Eliminating the constants required to make the density integrate to 1 gives

\[g(w)\propto\left[1+\frac{w^{2}}{n}\right]^{-(n+1)/2}.\]

Bayesian linear models involve multivariate \(t\) distributions, cf. DeGroot (1970, Section 5.6). Let \(W\sim N(\mu,V)\), \(Q\sim\chi^{2}(n)\), with \(W\) and \(Q\) independent. Then if

\[Y\sim(W-\mu)\frac{1}{\sqrt{Q/n}}+\mu,\]

by definition

\[Y\sim t(n,\mu,V)\,.\]

For an \(r\) vector \(Y\), a multivariate \(t(n)\) distribution with center \(\mu\) and dispersion matrix \(V\) has density

\[g(y)=\left[1+\frac{1}{n}(y-\mu)^{\prime}V^{-1}(y-\mu)\right]^{-( n+r)/2}\\ \times\Gamma\left(\frac{n+r}{2}\right)\Big{/}\left\{\Gamma\left( \frac{n}{2}\right)(n\pi)^{r/2}[\det(V)]^{1/2}\right\}\,.\]

This distribution has mean \(\mu\) for \(n>1\) and covariance matrix \([n/(n-2)]V\) for \(n>2\). To get noninteger degrees of freedom \(a\), just replace \(Q/n\) in the definition with \(bT/a\), where \(T\sim Gamma(a/2,b/2)\) independent of \(W\).

Note that from the definition of a multivariate \(t\),\[\frac{\lambda^{\prime}Y-\lambda^{\prime}\mu}{\sqrt{\lambda^{\prime}V\lambda}}\sim \frac{(\lambda^{\prime}W-\lambda^{\prime}\mu)/\sqrt{\lambda^{\prime}V\lambda}}{ \sqrt{Q/n}}\sim t(n). \tag{5}\]

Proceeding with the Bayesian analysis, to find the marginal posterior of \(\gamma\) let

\[Q=\left[(Y-Z\hat{\gamma})^{\prime}(Y-Z\hat{\gamma})+(\tilde{Y}- \tilde{Z}\hat{\gamma})^{\prime}D(\tilde{w})(\tilde{Y}-\tilde{Z}\hat{\gamma})\right]\] \[+\left[(\gamma-\hat{\gamma})^{\prime}\ (Z^{\prime}Z+\tilde{Z}^{ \prime}D(\tilde{w})\tilde{Z})(\gamma-\hat{\gamma})\right].\]

From (1),

\[p(\gamma|Y)\propto\int\left(\sigma^{2}\right)^{-(n+r)/2}p(\sigma^{2})\exp\left\{ \frac{-1}{2\sigma^{2}}Q\right\}d\sigma^{2}.\]

Transforming \(\sigma^{2}\) into \(\tau=1/\sigma^{2}\) gives \(\sigma^{2}=1/\tau\) and \(d\sigma^{2}=|-\tau^{-2}|d\tau.\) Thus

\[p(\gamma|Y)\propto\int\left(\tau\right)^{(n+r)/2}p(1/\tau)\exp\left\{-\tau Q/ 2\right\}\tau^{-2}d\tau.\]

Note that if \(\sigma^{2}\) has an inverse gamma distribution with parameters \(a\) and \(b\), then \(\tau\) has a gamma distribution with parameters \(a\) and \(b\); so \(p(1/\tau)\tau^{-2}\) is a gamma density and

\[p(\gamma|Y)\propto\int\left(\tau\right)^{(n+r+2a-2)/2}\exp\left\{-\tau(Q+2b)/2 \right\}d\tau.\]

The integral is a gamma integral, e.g., the gamma density given earlier integrates to 1, so

\[p(\gamma|Y)\propto\Gamma[(n+r+2a)/2]\bigg{/}\left[(Q+2b)/2\right]^{(n+r+2a)/2}\]

or

\[p(\gamma|Y)\propto[Q+2b]^{-(n+r+2a)/2}.\]

We can rewrite this as

\[p(\gamma|Y) \propto\left[(BdfE)(BMSE)+2b+(\gamma-\hat{\gamma})^{\prime}\left( Z^{\prime}Z+\tilde{Z}^{\prime}D(\tilde{w})\tilde{Z}\right)(\gamma-\hat{\gamma}) \right]^{-(n+r+2a)/2}\] \[\propto\left[1+\frac{1}{n+2a}\frac{(\gamma-\hat{\gamma})^{\prime }\left(Z^{\prime}Z+\tilde{Z}^{\prime}D(\tilde{w})\tilde{Z}\right)(\gamma-\hat {\gamma})}{[(BdfE)(BMSE)+2b]/(n+2a)}\right]^{-(n+2a+r)/2},\]

so

\[\gamma|Y\sim t\bigg{(}n+2a,\hat{\gamma},\frac{(BdfE)(BMSE)+2b}{n+2a}\left(Z^{ \prime}Z+\tilde{Z}^{\prime}D(\tilde{w})\tilde{Z}\right)^{-1}\bigg{)}.\]

Together with (5), this provides the posterior distribution of \(\lambda^{\prime}\gamma\).

Now consider the marginal (posterior) distribution of \(\sigma^{2}\).

\[p(\sigma^{2}|Y) \propto\left(\sigma^{2}\right)^{-n/2}p(\sigma^{2})\] \[\quad\times\exp\left\{\frac{-1}{2\sigma^{2}}\left[(Y-Z\hat{\gamma}) ^{\prime}(Y-Z\hat{\gamma})+(\tilde{Y}-\tilde{Z}\hat{\gamma})^{\prime}D(\tilde{w })(\tilde{Y}-\tilde{Z}\hat{\gamma})\right]\right\}\] \[\quad\times\int\left(\sigma^{2}\right)^{-r/2}\exp\left\{\frac{-1} {2\sigma^{2}}\left[(\gamma-\hat{\gamma})^{\prime}(Z^{\prime}Z+\tilde{Z}^{\prime }D(\tilde{w})\tilde{Z})(\gamma-\hat{\gamma})\right]\right\}d\gamma.\]

The term being integrated is proportional to a normal density, so the integral is a constant that does not depend on \(\sigma^{2}\). Hence,

\[p(\sigma^{2}|Y) \propto\left(\sigma^{2}\right)^{-n/2}p(\sigma^{2})\] \[\quad\times\exp\left\{\frac{-1}{2\sigma^{2}}\,\left[(Y-Z\hat{ \gamma})^{\prime}(Y-Z\hat{\gamma})+(\tilde{Y}-\tilde{Z}\hat{\gamma})^{\prime }D(\tilde{w})(\tilde{Y}-\tilde{Z}\hat{\gamma})\right]\right\}\]

or, using the generalized least squares notation,

\[p(\sigma^{2}|Y)\propto\left(\sigma^{2}\right)^{-n/2}p(\sigma^{2})\exp\left[ \frac{-1}{2\sigma^{2}}(BdfE)(BMSE)\right].\]

We transform to the _precision_, \(\tau\equiv 1/\sigma^{2}\). The \(InvGa(a,b)\) distribution for \(\sigma^{2}\) yields

\[p(\tau|Y) \propto\left(\tau\right)^{n/2}\left(\tau\right)^{a-1}\exp[-b\tau ]\exp\left[\frac{-\tau}{2}(BdfE)(BMSE)\right]\] \[=\left(\tau\right)^{[(n+2a)/2]-1}\exp\left[-\frac{2b+(BdfE)(BMSE) }{2}\tau\right];\]

so

\[\tau|Y\sim Gamma\left(\frac{n+2a}{2},\,\frac{2b+(BdfE)(BMSE)}{2}\right).\]

It is not difficult to show that

\[[2b+(BdfE)(BMSE)]\tau|Y\sim Gamma\left(\frac{n+2a}{2},\frac{1}{2}\right),\]

i.e.,

\[\frac{2b+(BdfE)(BMSE)}{\sigma^{2}}\Big{|}Y\sim Gamma\left(\frac{n+2a}{2},\, \frac{1}{2}\right).\]

As mentioned earlier, for the reference distribution with \(a=0\), \(b=0\),

\[\frac{(BdfE)(BMSE)}{\sigma^{2}}\Big{|}Y\sim Gamma\left(\frac{n}{2},\,\frac{1}{ 2}\right)=\chi^{2}(n).\]

Finally, we establish relation (1).

\[p(\gamma,\sigma^{2}|Y) \propto f(Y|\gamma,\sigma^{2})p(\gamma|\sigma^{2})p(\sigma^{2})\] \[\propto \left\{\left(\sigma^{2}\right)^{-n/2}\exp\left[-(Y-Z\gamma)^{ \prime}(Y-Z\gamma)/2\sigma^{2}\right]\right\}\] \[\times\left\{\left(\sigma^{2}\right)^{-r/2}\exp\left[-\left( \gamma-\tilde{Z}^{-1}\tilde{Y}\right)^{\prime}\left(\tilde{Z}^{\prime}D(\tilde {w})\tilde{Z}\right)\left(\gamma-\tilde{Z}^{-1}\tilde{Y}\right)\left/2\sigma^{ 2}\right]\right\}\] \[\times p(\sigma^{2})\]

Most of the work involves simplifying the terms in the exponents. We isolate those terms, deleting the multiple \(-1/2\sigma^{2}\).

\[(Y-Z\gamma)^{\prime}(Y-Z\gamma)+\left(\gamma-\tilde{Z}^{-1}\tilde {Y}\right)^{\prime}\left(\tilde{Z}^{\prime}D(\tilde{w})\tilde{Z}\right)\left( \gamma-\tilde{Z}^{-1}\tilde{Y}\right)\] \[= (Y-Z\gamma)^{\prime}(Y-Z\gamma)+(\tilde{Y}-\tilde{Z}\gamma)^{ \prime}D(\tilde{w})(\tilde{Y}-\tilde{Z}\gamma)\] \[= \left(\begin{bmatrix}Y\\ \tilde{Y}\end{bmatrix}-\begin{bmatrix}Z\\ \tilde{Z}\end{bmatrix}\gamma\right)^{\prime}\begin{bmatrix}I&0\\ 0&D(\tilde{w})\end{bmatrix}\left(\begin{bmatrix}Y\\ \tilde{Y}\end{bmatrix}-\begin{bmatrix}Z\\ \tilde{Z}\end{bmatrix}\gamma\right).\]

Write

\[Y_{*}=\begin{bmatrix}Y\\ \tilde{Y}\end{bmatrix},\quad Z_{*}=\begin{bmatrix}Z\\ \tilde{Z}\end{bmatrix},\quad V_{*}=\begin{bmatrix}I&0\\ 0&D^{-1}(\tilde{w})\end{bmatrix}\]

and apply Lemma 2.7.6 to get

\[(Y-Z\gamma)^{\prime}(Y-Z\gamma)+\left(\gamma-\tilde{Z}^{-1}\tilde {Y}\right)^{\prime}\left(\tilde{Z}^{\prime}D(\tilde{w})\tilde{Z}\right)\left( \gamma-\tilde{Z}^{-1}\tilde{Y}\right)\] \[= (Y_{*}-Z_{*}\gamma)^{\prime}V_{*}^{-1}(Y_{*}-Z_{*}\gamma)\] \[= (Y_{*}-A_{*}Y_{*})^{\prime}V_{*}^{-1}(Y_{*}-A_{*}Y_{*})+(\hat{ \gamma}-\gamma)^{\prime}(Z_{*}^{\prime}V_{*}^{-1}Z_{*})(\hat{\gamma}-\gamma),\]

where \(A_{*}=Z_{*}(Z_{*}^{\prime}V_{*}^{-1}Z_{*})^{-1}Z_{*}^{\prime}V_{*}^{-1}\) and

\[\hat{\gamma} = (Z_{*}^{\prime}V_{*}^{-1}Z_{*})^{-1}Z_{*}^{\prime}V_{*}^{-1}Y_{*}\] \[= \left(Z^{\prime}Z+\tilde{Z}^{\prime}D(\tilde{w})\tilde{Z}\right) ^{-1}\left[Z^{\prime}Y+\tilde{Z}^{\prime}D(\tilde{w})\tilde{Y}\right].\]

Finally, observe that

\[(Y_{*}-A_{*}Y_{*})^{\prime}V_{*}^{-1}(Y_{*}-A_{*}Y_{*})+(\hat{ \gamma}-\gamma)^{\prime}(Z_{*}^{\prime}V_{*}^{-1}Z_{*})(\hat{\gamma}-\gamma)\] \[= \left[(Y-Z\hat{\gamma})^{\prime}(Y-Z\hat{\gamma})+(\tilde{Y}- \tilde{Z}\hat{\gamma})^{\prime}D(\tilde{w})(\tilde{Y}-\tilde{Z}\hat{\gamma})\right]\] \[+ \left[(\gamma-\hat{\gamma})^{\prime}(Z^{\prime}Z+\tilde{Z}^{ \prime}D(\tilde{w})\tilde{Z})(\gamma-\hat{\gamma})\right].\]

Substitution gives (1).

**Exercise 2.9** : Prove relation (4).

### Additional Exercises

**Exercise 2.11.1**: _Prediction_**. Consider a regression model \(Y\)=\(X\beta+e\), \(e\sim N(0\), \(\sigma^{2}I)\) and suppose that we want to predict the value of a future observation, say \(y_{0}\), that will be independent of \(Y\) and be distributed \(N(x_{0}^{{}^{\prime}}\beta,\)\(\sigma^{2})\).

(a) Find the distribution of

\[\frac{y_{0}-x_{0}^{{}^{\prime}}\hat{\beta}}{\sqrt{MSE\left[1+x_{0}^{{}^{\prime }}(X^{{}^{\prime}}X)^{-1}x_{0}\right]}}.\]

(b) Find a 95% _prediction interval_ for \(y_{0}\).

Hint: A prediction interval is similar to a confidence interval except that, rather than finding parameter values that are consistent with the data and the model, one finds new observations \(y_{0}\) that are consistent with the data and the model as determined by an \(\alpha\) level test.

(c) Let \(\eta\in(0,0.5]\). The \(100\eta\)th percentile of the distribution of \(y_{0}\) is, say, \(\gamma(\eta)=x_{0}^{{}^{\prime}}\beta+z(\eta)\sigma\). (Note that \(z(\eta)\) is a negative number.) Find a \((1-\alpha)100\%\) lower confidence bound for \(\gamma(\eta)\). In reference to the distribution of \(y_{0}\), this lower confidence bound is referred to as a lower \(\eta\)_tolerance point_ with confidence coefficient \((1-\alpha)100\%\). For example, if \(\eta=0.1\), \(\alpha=0.05\), and \(y_{0}\) is the octane value of a batch of gasoline manufactured under conditions \(x_{0}\), then we are 95% confident that no more than 10% of all batches produced under \(x_{0}\) will have an octane value below the tolerance point.

Hint: Use a noncentral \(t\) distribution based on \(x_{0}^{{}^{\prime}}\hat{\beta}-\gamma(\eta)\).

Comment: For more detailed discussions of prediction and tolerance (and we all know that tolerance is a great virtue), see Geisser (1993), Aitchison and Dunsmore (1975), and Guttman (1970).

**Exercise 2.11.2**: Consider the model

\[Y=X\beta+Xb+e,\text{ \ \ \ \ \ E}(e)=0,\text{ \ \ \ \ \ Cov}(e)=\sigma^{2}I,\]

where \(b\) is a known vector and \(Xb\) is often called an _offset_. Show that Proposition 2.1.3 is not valid for this model by producing a linear unbiased estimate of \(\rho^{{}^{\prime}}X\beta\), say \(a_{0}+a^{\prime}Y\), for which \(a_{0}\neq 0\).

Hint: Modify \(\rho^{{}^{\prime}}MY\).

**Exercise 2.11.3**: Consider the model \(y_{i}=\beta_{1}x_{i1}+\beta_{2}x_{i2}+e_{i}\), \(e_{i}\)s i.i.d. \(N(0,\sigma^{2})\). Use the data given below to answer (a) through (d). Show your work, i.e., do not use a regression or general linear models computer program.

(a) Estimate \(\beta_{1}\), \(\beta_{2}\), and \(\sigma^{2}\).

(b)  Give 95% confidence intervals for \(\beta_{1}\) and \(\beta_{1}+\beta_{2}\). (c)  Perform an \(\alpha=0.01\) test for \(H_{0}:\beta_{2}=3\). (d)  Find an appropriate \(P\) value for the test of \(H_{0}:\beta_{1}-\beta_{2}=0\).

\[\begin{array}{c|ccccccccc}\text{obs.}&1&2&3&4&5&6&7&8\\ \hline y&82&79&74&83&80&81&84&81\\ x_{1}&10&9&9&11&11&10&10&12\\ x_{2}&15&14&13&15&14&14&16&13\\ \end{array}\]

**Exercise 2.11.4**: Consider the model \(y_{i}=\beta_{1}x_{i1}+\beta_{2}x_{i2}+e_{i},e_{i}\)s i.i.d. \(N(0,\sigma^{2})\). There are 15 observations and the sum of the squared observations is \(Y^{\prime}Y=3.03\). Use the normal equations given below to answer parts (a) through (c).

(a)  Estimate \(\beta_{1}\), \(\beta_{2}\), and \(\sigma^{2}\). (b)  Give 98% confidence intervals for \(\beta_{2}\) and \(\beta_{2}-\beta_{1}\). (c)  Perform an \(\alpha=0.05\) test for \(H_{0}:\beta_{1}=0.5\). The normal equations are

\[\left[\begin{array}{cc}15.00&374.50\\ 374.50&9482.75\end{array}\right]\left[\begin{array}{c}\beta_{1}\\ \beta_{2}\end{array}\right]=\left[\begin{array}{c}6.03\\ 158.25\end{array}\right].\]

**Exercise 2.11.5**: Consider the model

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\beta_{11}x_{i1}^{2}+\beta_{22} x_{i2}^{2}+\beta_{12}x_{i1}x_{i2}+e_{i},\]

where the predictor variables take on the following values.

\[\begin{array}{c|ccccccccc}i&1&2&3&4&5&6&7\\ \hline x_{i1}&1&1&-1&-1&0&0&0\\ x_{i2}&1&-1&1&-1&0&0&0\\ \end{array}\]

Show that \(\beta_{0}\), \(\beta_{1}\), \(\beta_{2}\), \(\beta_{11}+\beta_{22}\), \(\beta_{12}\) are estimable and find (nonmatrix) algebraic forms for the estimates of these parameters. Find the _MSE_ and the standard errors of the estimates.

**Exercise 2.11.6**: Consider the linear model

\[\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}=\begin{bmatrix}J_{N_{1}}\\ J_{N_{2}}\end{bmatrix}\mu+e,\quad\text{E}(e)=0,\quad\text{Cov}\left(\begin{bmatrix} e_{1}\\ e_{2}\end{bmatrix}\right)=\begin{bmatrix}\sigma_{1}^{2}I_{N_{1}}&0\\ 0&\sigma_{2}^{2}I_{N_{2}}\end{bmatrix}.\]

Find the BLUE \(\hat{\mu}\) and \(\text{Var}(\hat{\mu})\) assuming that the variance parameters are known. How does the result change if \(\sigma_{1}^{2}\) is unknown but \(\sigma_{2}^{2}/\sigma_{1}^{2}\) is known?

**Exercise 2.11.7** : For \(V\) positive definite and \(C(VX)\subset C(X)\) use Proposition B.12 and the results in Section B.4 to show that

\[r(VX)=r(VXX^{\prime})=r(XX^{\prime})=r(X).\]

Hint: Show \(\mathcal{N}(VXX^{\prime})=\mathcal{N}(XX^{\prime})\).

**Exercise 2.11.8** : Use Theorem 2.2.4 and its proof to show that there is a unique least squares estimate in \(C(X^{\prime})\), say \(\hat{\beta}_{0}\), and that all other least squares estimates are obtained by \(\hat{\beta}=\hat{\beta}_{0}+v\) for some \(v\perp C(X^{\prime})\).

## References

* [Aitchison & DunsmoreAitchison & Dunsmore1975] Aitchison, J., & Dunsmore, I. R. (1975). _Statistical prediction analysis_. Cambridge: Cambridge University Press.
* [ArnoldArnold1981] Arnold, S. F. (1981). _The theory of linear models and multivariate analysis_. New York: Wiley.
* [Bedrick, Christensen & JohnsonBedrick et al.1996] Bedrick, E. J., Christensen, R., & Johnson, W. (1996). A new perspective on priors for generalized linear models. _Journal of the American Statistical Association_, _91_, 1450-1460.
* [BergerBerger1993] Berger, J. O. (1993). _Statistical decision theory and Bayesian analysis_ (Revised 2nd ed.). New York: Springer.
* [BerryBerry1996] Berry, D. A. (1996). _Statistics: A Bayesian perspective_. Belmont: Duxbery.
* [Box & TiaoBox and TiaoBox and Tiao1973] Box, G. E. P., & Tiao, G. C. (1973). _Bayesian inference in statistical analysis_. New York: Wiley.
* [ChristensenChristensen2018] Christensen, R. (2018). Another look at linear hypothesis testing in dense high-dimensional linear models. [http://www.stat.unm.edu/~fletcher/AnotherLook.pdf](http://www.stat.unm.edu/~fletcher/AnotherLook.pdf)
* [Christensen, Johnson, Branscum, & HansonChristensen et al.2010] Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton: Chapman and Hall/CRC Press.
* [Cox & HinkleyCox & Hinkley1974] Cox, D. R., & Hinkley, D. V. (1974). _Theoretical statistics_. London: Chapman and Hall.
* [de Finettide Finetti1974] de Finetti, B. (1974, 1975). _Theory of probability_ (Vols. 1, 2). New York: Wiley.
* [DeGrootDeGroot1970] DeGroot, M. H. (1970). _Optimal statistical decisions_. New York: McGraw-Hill.
* [EatonEaton1983] Eaton, M. L. (1983). _Multivariate statistics: A vector space approach_. New York: Wiley. Reprinted in 2007 by IMS Lecture Notes-Monograph Series.
* [GeisserGeisser1993] Geisser, S. (1993). _Predictive inference: An introduction_. New York: Chapman and Hall.
* [Gelman, Carlin, Stern, Dunson, Vehtari, & RubinGelman et al.2013] Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). _Bayesian data analysis_ (3rd ed.). Boca Raton: Chapman and Hall/CRC.
* [GraybillGraybill1976] Graybill, F. A. (1976). _Theory and application of the linear model_. North Scituate: Duxbury Press.
* [GuttmanGuttman1970] Guttman, I. (1970). _Statistical tolerance regions_. New York: Hafner Press.
* [HarvilleHarville2018] Harville, D. A. (2018). _Linear models and the relevant distributions and matrix algebra_. Boca Raton: CRC Press.
* [JeffreysJeffreys1961] Jeffreys, H. (1961). _Theory of probability_ (3rd ed.). London: Oxford University Press.
* [LehmannLehmann1983] Lehmann, E. L. (1983). _Theory of point estimation_. New York: Wiley.
* [LehmannLehmann1986] Lehmann, E. L. (1986). _Testing statistical hypotheses_ (2nd ed.). New York: Wiley.
* [LindleyLindley1971] Lindley, D. V. (1971). _Bayesian statistics: A review_. Philadelphia: SIAM.
* [MonahanMonahan2008] Monahan, J. F. (2008). _A primer on linear models_. Boca Raton: Chapman & Hall/CRC Press.
* [Raiffa & SchlaiferRaiffa & SchlaiferRaiffa & SchlaiferRaiffa & Schlaifer1961] Raiffa, H., & Schlaifer, R. (1961). _Applied statistical decision theory_. Boston: Division of Research, Graduate School of Business Administration, Harvard University.
* [Raiffa & SchlaiferRaiffa & Schlaifer1961]* Rao (1973) Rao, C. R. (1973). _Linear statistical inference and its applications_ (2nd ed.). New York: Wiley.
* Ravishanker & Dey (2002) Ravishanker, N., & Dey, D. (2002). _A first course in linear model theory_. Boca Raton: Chapman and Hall/CRC Press.
* Rencher & Schaalje (2008) Rencher, A. C., & Schaalje, G. B. (2008). _Linear models in statistics_ (2nd ed.). New York: Wiley.
* Robert (2007) Robert, C. P. (2007). _The Bayesian choice: From decision-theoretic foundations to computational implementation_ (2nd ed.). New York: Springer.
* Savage (1954) Savage, L. J. (1954). _The foundations of statistics_. New York: Wiley.
* Schafer (1987) Schafer, D. W. (1987). Measurement error diagnostics and the sex discrimination problem. _Journal of Business and Economic Statistics_, 5, 529-537.
* Scheffe (1959) Scheffe, H. (1959). _The analysis of variance_. New York: Wiley.
* Searle (1971) Searle, S. R. (1971). _Linear models_. New York: Wiley.
* Seber (1966) Seber, G. A. F. (1966). _The linear hypothesis: A general theory_. London: Griffin.
* Seber (1977) Seber, G. A. F. (1977). _Linear regression analysis_. New York: Wiley.
* Seber (2015) Seber, G. A. F. (2015). _The linear model and hypothesis: A general theory_. New York: Springer.
* Wichura (2006) Wichura, M. J. (2006). _The coordinate-free approach to linear models_. New York: Cambridge University Press.
* Zellner (1971) Zellner, A. (1971). _An introduction to Bayesian inference in econometrics_. New York: Wiley.

## Chapter 3 Testing

### 3.1 More About Models

For estimation in the standard linear model

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I,\]we have found that the crucial item needed is \(M\), the perpendicular projection operator onto \(C(X)\). For convenience, we will call \(C(X)\) the _estimation space_ and \(C(X)^{\perp}\) the _error space_. \(I\,-\,M\) is the perpendicular projection operator onto the error space. In a profound sense, any two linear models with the same estimation space are the same model. For example, any two such models will give the same fitted (predicted) values for the observations, hence the same residuals and the same estimate of \(\sigma^{2}\).

Suppose we have two linear models for a vector of observations, say \(Y=X_{1}\beta_{1}+e_{1}\) and \(Y=X_{2}\beta_{2}+e_{2}\) with \(C(X_{1})=C(X_{2})\). For these alternative parameterizations, i.e., reparameterizations, \(M\) does not depend on which of \(X_{1}\) or \(X_{2}\) is used; it depends only on \(C(X_{1})[=C(X_{2})]\). Thus, the _MSE_ does not change, and the least squares estimate of \(\mathrm{E}(Y)\) is \(\hat{Y}=MY=X_{1}\hat{\beta}_{1}=X_{2}\hat{\beta}_{2}\).

The expected values of the observations are fundamental to estimation in linear models. Identifiable parameteric functions are functions of the expected values. Attention is often restricted to estimable functions, i.e., functions \(\rho^{\prime}X\beta\) where \(X\beta=\mathrm{E}(Y)\). The key idea in estimability is restricting estimation to linear combinations of the rows of \(\mathrm{E}(Y)\). \(\mathrm{E}(Y)\) depends only on the choice of \(C(X)\), whereas the vector \(\beta\) depends on the particular choice of \(X\).

Consider again the two models discussed above. If \(\lambda_{1}^{\prime}\beta_{1}\) is estimable, then \(\lambda_{1}^{\prime}\beta_{1}=\rho^{\prime}X_{1}\beta_{1}=\rho^{\prime}E(Y)\) for some \(\rho\). This estimable function is the same linear combination of the rows of \(\mathrm{E}(Y)\) as \(\rho^{\prime}E(Y)=\rho^{\prime}X_{2}\beta_{2}=\lambda_{2}^{\prime}\beta_{2}\). These are really the same estimable function, but they are written with different parameters. This estimable function has a unique least squares estimate, \(\rho^{\prime}MY\).

#### 3.1.1 One-Way ANOVA.

Two parameterizations for a one-way ANOVA are commonly used. They are

\[y_{ij}=\mu+\alpha_{i}+e_{ij}\]

and

\[y_{ij}=\mu_{i}+e_{ij}.\]

It is easily seen that these models determine the same estimation space. The estimates of \(\sigma^{2}\) and \(\mathrm{E}(y_{ij})\) are identical in the two models. One convenient aspect of these models is that the relationships between the two sets of parameters are easily identified. In particular,

\[\mu_{i}=\mathrm{E}(y_{ij})=\mu+\alpha_{i}.\]

It follows that the mean of the \(\mu_{i}\)s equals \(\mu\) plus the mean of the \(\alpha_{i}\)s, i.e., \(\bar{\mu}.=\mu+\bar{\alpha}.\). It also follows that \(\mu_{1}-\mu_{2}=\alpha_{1}-\alpha_{2}\), etc. The parameters in the two models are different, but they are related. Any estimable function in one model determines a corresponding estimable function in the other model. These functions have the same estimate. Chapter 4 contains a detailed examination of these models.

#### Example 3.1.2 Simple Linear Regression.

The models

\[y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\]

and

\[y_{i}=\gamma_{0}+\gamma_{1}(x_{i}-\bar{x}.)+e_{i}\]

have the same estimation space (\(\bar{x}.\) is the mean of the \(x_{i}\)s). Since

\[\beta_{0}+\beta_{1}x_{i}=\mathrm{E}(y_{i})=\gamma_{0}+\gamma_{1}(x_{i}-\bar{x}.) \tag{1}\]

for all \(i\), it is easily seen from averaging over \(i\) that

\[\beta_{0}+\beta_{1}\bar{x}.=\gamma_{0}.\]

Substituting \(\beta_{0}=\gamma_{0}-\beta_{1}\bar{x}.\) into (1) leads to

\[\beta_{1}(x_{i}-\bar{x}.)=\gamma_{1}(x_{i}-\bar{x}.)\]

and, if the \(x_{i}\)s are not all identical,

\[\beta_{1}=\gamma_{1}.\]

These models are examined in detail in Section 6.1.

When the estimation spaces \(C(X_{1})\) and \(C(X_{2})\) are the same, write \(X_{1}=X_{2}T\) to get

\[X_{1}\beta_{1}=X_{2}T\beta_{1}=X_{2}\beta_{2}. \tag{2}\]

Estimable functions are equivalent in the two models: \(\Lambda^{\prime}_{1}\beta_{1}=P^{\prime}X_{1}\beta_{1}=P^{\prime}X_{2}\beta_{2} =\Lambda^{\prime}_{2}\beta_{2}.\) It also follows from equation (2) that the parameterizations must satisfy the relation

\[\beta_{2}=T\beta_{1}+v \tag{3}\]

for some \(v\in C(X^{\prime}_{2})^{\perp}.\) In general, neither of the parameter vectors \(\beta_{1}\) or \(\beta_{2}\) is uniquely defined but, to the extent that either parameter vector is defined, equation (3) establishes the relationship between them. A unique parameterization for, say, the \(X_{2}\) model occurs if and only if \(X^{\prime}_{2}X_{2}\) is nonsingular. In such a case, the columns of \(X_{2}\) form a basis for \(C(X_{2})\), so the matrix \(T\) is uniquely defined. In this case, the vector \(v\) must be zero because \(C(X^{\prime}_{2})^{\perp}=\{0\}.\) An alternative and detailed presentation of equivalent linear models, both the reparameterizations considered here and the equivalences between constrained and unconstrained models considered in subsequent sections, is given by Peixoto (1993).

Basically, the \(\beta\) parameters in

\[Y=X\beta+e,\;\;\;\mathrm{E}(e)=0,\;\;\;\mathrm{Cov}(e)=\sigma^{2}I\]are either a convenience or a nuisance, depending on what we are trying to do. Having \(\mathrm{E}(e)=0\) gives \(\mathrm{E}(Y)=X\beta\), but since \(\beta\) is unknown, this is merely saying that \(\mathrm{E}(Y)\) is _some_ linear combination of the columns of \(X\). The essence of the model is that

\[\mathrm{E}(Y)\in C(X),\quad\mathrm{Cov}(Y)=\sigma^{2}I.\]

As long as we do not change \(C(X)\), we can change \(X\) itself to suit our convenience.

### Testing Models

In this section, the basic theory for testing a linear model against a reduced model is presented. A generalization of the basic procedure is also presented.

Testing in linear models typically reduces to putting a constraint on the estimation space. We start with a (full) model that we know (assume) to be valid,

\[Y=X\beta+e,\quad e\sim N(0,\sigma^{2}I). \tag{1}\]

Our wish is to reduce this model, i.e., we wish to know if some simpler model gives an acceptable fit to the data. Consider whether the model

\[Y=X_{0}\gamma+e,\quad e\sim N(0,\sigma^{2}I),\quad C(X_{0})\subset C(X) \tag{2}\]

is acceptable. Clearly, if model (2) is correct, then model (1) is also correct. The question is whether (2) is correct.

The procedure of testing full and reduced models is a commonly used method in statistics.

Example 3.2.0 (a): _One-Way ANOVA._

The full model is

\[y_{ij}=\mu+\alpha_{i}+e_{ij}.\]

To test for no treatment effects, i.e., to test that the \(\alpha_{i}\)s are extraneous, the reduced model simply eliminates the treatment effects. The reduced model is

\[y_{ij}=\gamma+e_{ij}.\]

Additionally, consider testing \(H_{0}:\alpha_{1}-\alpha_{3}=0\) in Example 1.0.2. The full model is

\[\begin{bmatrix}y_{11}\\ y_{12}\\ y_{13}\\ y_{21}\\ y_{31}\\ y_{32}\end{bmatrix}=\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}+\begin{bmatrix}e_{11}\\ e_{12}\\ e_{13}\\ e_{21}\\ e_{31}\\ e_{32}\end{bmatrix}.\]We can rewrite this as

\[Y=\mu\begin{bmatrix}1\\ 1\\ 1\\ 1\\ 1\\ 1\end{bmatrix}+\alpha_{1}\begin{bmatrix}1\\ 1\\ 1\\ 0\\ 0\\ 0\end{bmatrix}+\alpha_{2}\begin{bmatrix}0\\ 0\\ 0\\ 1\\ 0\\ 0\end{bmatrix}+\alpha_{3}\begin{bmatrix}0\\ 0\\ 0\\ 1\\ 1\end{bmatrix}+e.\]

If we impose the constraint \(H_{0}:\alpha_{1}-\alpha_{3}=0\), i.e., \(\alpha_{1}=\alpha_{3}\), we get

\[Y=\mu J+\alpha_{1}\begin{bmatrix}1\\ 1\\ 1\\ 0\\ 0\end{bmatrix}+\alpha_{2}\begin{bmatrix}0\\ 0\\ 0\\ 1\\ 0\\ 0\end{bmatrix}+\alpha_{1}\begin{bmatrix}0\\ 0\\ 0\\ 1\\ 1\end{bmatrix}+e,\]

or

\[Y=\mu J+\alpha_{1}\begin{pmatrix}\begin{bmatrix}1\\ 1\\ 1\\ 0\\ 0\\ 0\end{bmatrix}+\begin{bmatrix}0\\ 0\\ 0\\ 1\\ 1\end{bmatrix}\end{pmatrix}+\alpha_{2}\begin{bmatrix}0\\ 0\\ 0\\ 1\\ 0\end{bmatrix}+e,\]

or

\[\begin{bmatrix}y_{11}\\ y_{12}\\ y_{13}\\ y_{21}\\ y_{31}\\ y_{32}\end{bmatrix}=\begin{bmatrix}1&1&0\\ 1&1&0\\ 1&1&0\\ 1&1&0\\ 1&1&0\end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\end{bmatrix}+\begin{bmatrix}e_{11}\\ e_{12}\\ e_{13}\\ e_{21}\\ e_{31}\\ e_{32}\end{bmatrix}.\]

This is the reduced model determined by \(H_{0}:\alpha_{1}-\alpha_{3}=0\). However, the parameters \(\mu\), \(\alpha_{1}\), and \(\alpha_{2}\) no longer mean what they did in the full model.

(b) _Multiple Regression._

Consider the full model

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\beta_{3}x_{i3}+e_{i}.\]

For a simultaneous test of whether the variables \(x_{1}\) and \(x_{3}\) are adding significantly to the explanatory capability of the regression model, simply eliminate the variables \(x_{1}\) and \(x_{3}\) from the model. The reduced model is

\[y_{i}=\gamma_{0}+\gamma_{2}x_{i2}+e_{i}.\]Now write the original model matrix as \(X=[J,\,X_{1},\,X_{2},\,X_{3}]\), so

\[Y=[J,\,X_{1},\,X_{2},\,X_{3}]\begin{bmatrix}\beta_{0}\\ \beta_{1}\\ \beta_{2}\\ \beta_{3}\end{bmatrix}+e=\beta_{0}J+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{3}X_{ 3}+e.\]

Consider the hypothesis \(H_{0}:\beta_{2}-\beta_{3}=0\) or \(H_{0}:\beta_{2}=\beta_{3}\). The reduced model is

\[Y =\beta_{0}J+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{2}X_{3}+e\] \[=\beta_{0}J+\beta_{1}X_{1}+\beta_{2}(X_{2}+X_{3})+e\] \[=[J,\,X_{1},\,X_{2}+X_{3}]\begin{bmatrix}\beta_{0}\\ \beta_{1}\\ \beta_{2}\end{bmatrix}+e.\]

However, these \(\beta\) parameters no longer mean what they did in the original model, so it is better to write the model as

\[Y=[J,\,X_{1},\,X_{2}+X_{3}]\begin{bmatrix}\gamma_{0}\\ \gamma_{1}\\ \gamma_{2}\end{bmatrix}+e.\]

The distribution theory for testing models is given in Theorem 3.2.1. Before stating those results, we discuss the intuitive background of the test based only on the assumptions about the first two moments. Let \(M\) and \(M_{0}\) be the perpendicular projection operators onto \(C(X)\) and \(C(X_{0})\), respectively. Note that with \(C(X_{0})\subset C(X)\), \(M-M_{0}\) is the perpendicular projection operator onto the orthogonal complement of \(C(X_{0})\) with respect to \(C(X)\), that is, onto \(C(M-M_{0})=C(X_{0})_{C(X)}^{\perp}\), see Theorem B.47.

Under model (1), the estimate of \(\mathrm{E}(Y)\) is \(X\hat{\beta}=MY\). Under model (2), the estimate is \(X_{0}\hat{\gamma}=M_{0}Y\). Recall that the validity of model (2) implies the validity of model (1); so if model (2) is true, \(MY\) and \(M_{0}Y\) are estimates of the same quantity. This suggests that the difference between the two estimates, \(X\hat{\beta}-X_{0}\hat{\gamma}=MY-M_{0}Y=(M-M_{0})Y\), should be reasonably small. Under model (2), the difference is just error because \(\mathrm{E}[(M-M_{0})Y]=0\).

On the other hand, a large difference between the estimates suggests that \(MY\) and \(M_{0}Y\) are estimating different things. By assumption, \(MY\) is always an estimate of \(\mathrm{E}(Y)\); so \(M_{0}Y\) must be estimating something different, namely, \(M_{0}\mathrm{E}(Y)\neq\mathrm{E}(Y)\). If \(M_{0}Y\) is not estimating \(\mathrm{E}(Y)\), model (2) cannot be true because model (2) implies that \(M_{0}Y\) is an estimate of \(\mathrm{E}(Y)\).

The decision about whether model (2) is appropriate hinges on deciding whether the vector \(X\hat{\beta}-X_{0}\hat{\gamma}=(M-M_{0})Y\) is large. An obvious measure of the size of \((M-M_{0})Y\) is its squared length, \([(M-M_{0})Y]^{\prime}[(M-M_{0})Y]=Y^{\prime}(M-M_{0})Y\). However, the length of \((M-M_{0})Y\) is also related to the relative sizes of \(C(M)\) and \(C(M_{0})\). It is convenient (not crucial) to adjust for this factor. As a measure of the size of \((M-M_{0})Y\), we use the value

\[Y^{\prime}(M-M_{0})Y\big{/}r(M-M_{0}).\]

Even though we have an appropriate measure of the size of \((M-M_{0})Y\), we still need some idea of how large the measure will be both when model (2) is true and when model (2) is not true. Using only the assumption that model (1) is true, Theorem 1.3.2 implies that

\[\mbox{E}[Y^{\prime}(M-M_{0})Y\big{/}r(M-M_{0})]=\sigma^{2}+\beta^{\prime}X^{ \prime}(M-M_{0})X\beta\big{/}r(M-M_{0}).\]

If model (2) is also true, \(X\beta=X_{0}\gamma\) and \((M-M_{0})X_{0}=0\); so the expected value of \(Y^{\prime}(M-M_{0})Y\big{/}r(M-M_{0})\) is \(\sigma^{2}\). If \(\sigma^{2}\) were known, our intuitive argument would be complete. If \(Y^{\prime}(M-M_{0})Y\big{/}r(M-M_{0})\) is not much larger than \(\sigma^{2}\), then we have observed something that is consistent with the validity of model (2). Values that are much larger than \(\sigma^{2}\) indicate that model (2) is false because they suggest that \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta\big{/}r(M-M_{0})>0\), i.e., \(X\beta\neq M_{0}X\beta\).

Typically, we do not know \(\sigma^{2}\), so the obvious thing to do is to estimate it. Since model (1) is assumed to be true, the obvious estimate is the \(MSE\equiv Y^{\prime}(1-M)Y/r(1-M)\). Now, values of \(Y^{\prime}(M-M_{0})Y\big{/}r(M-M_{0})\) that are much larger than \(MSE\) cause us to doubt the validity of model (2). Equivalently, values of the _test statistic_

\[\frac{Y^{\prime}(M-M_{0})Y\big{/}r(M-M_{0})}{MSE}\]

that are considerably larger than 1 cause precisely the same doubts.

We now examine the behavior of this test statistic when model (2) is not correct but model (1) is. \(Y^{\prime}(M-M_{0})Y/r(M-M_{0})\) and \(MSE\) are each estimates of their expected values, so the test statistic obviously provides an estimate of the ratio of their expected values. Recalling \(\mbox{E}[Y^{\prime}(M-M_{0})Y/r(M-M_{0})]\) from above and that \(\mbox{E}(MSE)=\sigma^{2}\), the test statistic gives an estimate of \(1+\beta^{\prime}X^{\prime}(M-M_{0})X\beta\big{/}r(M-M_{0})\sigma^{2}\). The term \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta\) is crucial to evaluating the behavior of the test statistic when model (1) is valid but model (2) is not, cf. the noncentrality parameter in Theorem 3.2.1, part \(i\). Note that \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta=[X\beta-M_{0}X\beta]^{\prime}[X\beta-M _{0}X\beta]\) is the squared length of the difference between \(X\beta\) (i.e., E(Y)) and \(M_{0}X\beta\) (the projection of \(X\beta\) onto \(C(X_{0})\)). If \(X\beta-M_{0}X\beta\) is large (relative to \(\sigma^{2}\)), then model (2) is very far from being correct, and the test statistic will tend to be large. On the other hand, if \(X\beta-M_{0}X\beta\) is small (relative to \(\sigma^{2}\)), then model (2), although not correct, is a good approximation to the correct model. In this case the test statistic will tend to be a little larger than it would be if model (2) were correct, but the effect will be very slight. In other words, if \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta\big{/}r(M-M_{0})\sigma^{2}\) is small, it is unreasonable to expect any test to work very well.

One can think about the geometry of all this in three dimensions. As in Section 2.2, consider a rectangular table. Take one corner of the table to be the origin. Take \(C(X)\) as the two-dimensional subspace determined by the surface of the table and take \(C(X_{0})\) to be a one-dimensional subspace determined by an edge of the table that includes the origin. \(Y\) can be any vector originating at the origin, i.e., any point in three-dimensional space. The full model (1) says that \(\operatorname{E}(Y)=X\beta\), which just says that \(\operatorname{E}(Y)\) is somewhere on the surface of the table. The reduced model (2) says that \(\operatorname{E}(Y)\) is somewhere on the \(C(X_{0})\) edge of the table. \(MY=X\hat{\beta}\) is the perpendicular projection of \(Y\) onto the table surface. \(M_{0}Y=X_{0}\hat{\gamma}\) is the perpendicular projection of \(Y\) onto the \(C(X_{0})\) edge of the table. The residual vector \((I-M)Y\) is the perpendicular projection of \(Y\) onto the vertical line through the origin.

If \(MY\) is close to the \(C(X_{0})\) edge of the table, it must be close to \(M_{0}Y\). This is the behavior we would expect if the reduced model is true, i.e., if \(X\beta=X_{0}\gamma\). The difference between the two estimates, \(MY-M_{0}Y\), is a vector that is on the table, but perpendicular to the \(C(X_{0})\) edge. In fact, the table edge through the origin that is perpendicular to the \(C(X_{0})\) edge is the orthogonal complement of \(C(X_{0})\) with respect to \(C(X)\), that is, it is \(C(X_{0})^{\perp}_{C(X)}=C(M-M_{0})\). The difference between the two estimates is \(MY-M_{0}Y=(M-M_{0})Y\), which is the perpendicular projection of \(Y\) onto \(C(M-M_{0})\). If \((M-M_{0})Y\) is large, it suggests that the reduced model is not true. To decide if \((M-M_{0})Y\) is large, we find its average (mean) squared length, where the average is computed relative to the dimension of \(C(M-M_{0})\), and compare that to the averaged squared length of the residual vector \((I-M)Y\) (i.e., the _MSE_). In our three-dimensional example, the dimensions of both \(C(M-M_{0})\) and \(C(I-M)\) are 1. If \((M-M_{0})Y\) is, on average, much larger than \((I-M)Y\), we reject the reduced model.

If the true (but unknown) \(X\beta\) happens to be far from the \(C(X_{0})\) edge of the table, it will be very easy to see that the reduced model is not true. This occurs because \(MY\) will be near \(X\beta\), which is far from anything in \(C(X_{0})\) so, in particular, it will be far from \(M_{0}Y\). Remember that the meaning of "far" depends on \(\sigma^{2}\) which is estimated by the _MSE_. On the other hand, if \(X\beta\) happens to be near, but not on, the \(C(X_{0})\) edge of the table, it will be very hard to tell that the reduced model is not true because \(MY\) and \(M_{0}Y\) will tend to be close together. On the other hand, if \(X\beta\) is near, but not on, the \(C(X_{0})\) edge of the table, using the incorrect reduced model may not create great problems, cf. Section 2.9.

To this point, the discussion has been based entirely on the assumptions \(Y=X\beta+e\), \(\operatorname{E}(e)=0\), \(\operatorname{Cov}(e)=\sigma^{2}I\). We now quantify the precise behavior of the test statistic for normal errors.

**Theorem 3.2.1**: _Consider a full model_

\[Y=X\beta+e,\ \ \ e\sim N(0,\sigma^{2}I)\]

_that holds for some values of \(\beta\) and \(\sigma^{2}\) and a reduced model_\[Y=X_{0}\gamma+e,\ \ \ e\sim N(0,\sigma^{2}I),\ \ \ C(X_{0})\subset C(X).\]

_(i) If the full model is true,_

\[\frac{Y^{\prime}(M-M_{0})Y/r(M-M_{0})}{Y^{\prime}(I-M)Y/r(I-M)}\sim F\left(r(M- M_{0}),r(I-M),\beta^{\prime}X^{\prime}(M-M_{0})X\beta/2\sigma^{2}\right).\]

_(ii) If the reduced model is true,_

\[\frac{Y^{\prime}(M-M_{0})Y/r(M-M_{0})}{Y^{\prime}(I-M)Y/r(I-M)}\sim F(r(M-M_{0} ),r(I-M),0)\,.\]

_When the full model is true, this distribution holds only if the reduced model is true._

_Proof_ (i) Since \(M\) and \(M_{0}\) are the perpendicular projection matrices onto \(C(X)\) and \(C(X_{0})\), \(M-M_{0}\) is the perpendicular projection matrix onto \(C(M-M_{0})\), cf. Theorem B.47. As in Section 2.6,

\[\frac{Y^{\prime}(I-M)Y}{\sigma^{2}}\sim\chi^{2}(r(I-M))\]

and from Theorem 1.3.3 on the distribution of quadratic forms,

\[\frac{Y^{\prime}(M-M_{0})Y}{\sigma^{2}}\sim\chi^{2}\left(r(M-M_{0}),\beta^{ \prime}X^{\prime}(M-M_{0})X\beta/2\sigma^{2}\right).\]

Theorem 1.3.7 establishes that \(Y^{\prime}(M-M_{0})Y\) and \(Y^{\prime}(I-M)Y\) are independent because

\[(M-M_{0})(I-M) = M-M_{0}-M+M_{0}M\] \[= M-M_{0}-M+M_{0}=0.\]

Finally, part (i) of the theorem holds by Definition C.3.

(ii) It suffices to show that \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta=0\) if and only if \(\mathrm{E}(Y)=X_{0}\gamma\) for some \(\gamma\).

\(\Leftarrow\) If \(\mathrm{E}(Y)=X_{0}\gamma\), we have \(\mathrm{E}(Y)=X\beta\) for some \(\beta\) because \(C(X_{0})\subset C(X)\). In particular, \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta=\gamma^{\prime}X^{\prime}_{0}(M-M_{0} )X_{0}\gamma\), but since \((M-M_{0})X_{0}=X_{0}-X_{0}=0\), we have \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta=0\).

\(\Rightarrow\) If \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta=0\), then \([(M-M_{0})X\beta]^{\prime}\{(M-M_{0})X\beta\}=0\). Since for any \(x,x^{\prime}x=0\) if and only if \(x=0\), we have \((M-M_{0})X\beta=0\,\mathrm{or}\,X\beta=M_{0}X\beta=X_{0}(X^{\prime}_{0}X_{0})^ {-}X^{\prime}_{0}X\beta\). Taking \(\gamma=(X^{\prime}_{0}X_{0})^{-}X^{\prime}_{0}X\beta\), we have \(\mathrm{E}(Y)=X_{0}\gamma\). \(\square\)

People typically reject the hypothesis

\[H_{0}:\mathrm{E}(Y)=X_{0}\gamma\ \ \ \ \ \ \text{for some $\gamma$},\]for large observed values of the test statistic. The informal second moment arguments given prior to Theorem 3.2.1 suggest rejecting large values and the existence a positive noncentrality parameter in Theorem 3.2.1(i) would shift the (central) \(F\) distribution to the right which also suggests rejecting large values. Both of these arguments depend on the full model being true. Theorem 3.2.1(ii) provides a distribution for the test statistic under the reduced (null) model, so under the conditions of the theorem this test of

\[H_{0}:\mathrm{E}(Y)\in C(X_{0})\]

rejects \(H_{0}\) at level \(\alpha\) if

\[\frac{Y^{\prime}(M-M_{0})Y/r(M-M_{0})}{Y^{\prime}(I-M)Y/r(I-M)}>F(1-\alpha,r(M- M_{0}),r(I-M))\,.\]

\(P\) values are then _reported_ as the probability from an \(F(r(M-M_{0}),r(I-M))\) distribution of being at least as large as the observed value of the test statistic.

This test procedure is "non-Fisherian" in that it assumes more than just the null (reduced) model being true. The decision on when to reject the null model depends on the full model being true. In fact, test statistic values near 0 (reported \(P\) values near 1) can be just as interesting as large test statistic values (reported \(P\) values near 0). Large reported \(P\) values often need to be closer to 1 to be interesting than small reported \(P\) values need to be close to 0.

Personally, I don't consider these _reported_\(P\) values to be real \(P\) values, although they are not without their uses. _Appendix F discusses the significance of small test statistics and some foundational issues related to this test_. For those willing to assume that the full model is true, the \(F\) test is the _generalized likelihood ratio test_ (see Exercise 3.1) and the _uniformly most powerful invariant (UMPI) test_ (see Lehmann, 1986, Chapter 7 and [http://www.stat.unm.edu/~fletcher/UMPI.pdf](http://www.stat.unm.edu/~fletcher/UMPI.pdf)).

In practice it is often easiest to use the following approach to obtain the test statistic: Observe that \((M-M_{0})=(I-M_{0})-(I-M)\). If we can find the error sums of squares, \(Y^{\prime}(I-M_{0})Y\) from the model \(Y=X_{0}\gamma+e\) and \(Y^{\prime}(I-M)Y\) from the model \(Y=X\beta+e\), then the difference is \(Y^{\prime}(I-M_{0})Y-Y^{\prime}(I-M)Y=Y^{\prime}(M-M_{0})Y\), which is the numerator sum of squares for the \(F\) test. Unless there is some simplifying structure to the model matrix (as in cases we will examine later), it is usually easier to obtain the error sums of squares for the two models than to find \(Y^{\prime}(M-M_{0})Y\) directly.

Example 3.2.2: Consider the model matrix given at the end of this example. It is for the unbalanced analysis of variance \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\), where \(i=1\), \(2\), \(3\), \(j=1\), \(2\), \(3\), \(k=1\),..., \(N_{ij}\), \(N_{11}=N_{12}=N_{21}=3\), \(N_{13}=N_{22}=N_{23}=N_{31}=N_{32}=2\). Here we have written \(Y=X\beta+e\) with \(Y=[y_{111},\,y_{112},\,\ldots,\,y_{332}]^{\prime},\beta=[\mu,\,\alpha_{1}, \alpha_{2},\,\alpha_{3},\,\eta_{1},\,\eta_{2},\,\eta_{3}]^{\prime}\), and \(e=[e_{111},\,e_{112},\,\ldots,\,e_{332}]^{\prime}\). We can now test to see if the model \(y_{ijk}=\mu+\alpha_{i}+e_{ijk}\) is an adequate representation of the data simply by dropping the last three columns from the model matrix. We can also test \(\mu+e_{ijk}\) by dropping the last six columns of the model matrix. In either case, the test is based on comparing the error sum of squares for the reduced model with that of the full model.

\[X=\begin{bmatrix}1&1&0&0&1&0&0\\ 1&1&0&0&1&0&0\\ 1&1&0&0&1&0&0\\ 1&1&0&0&0&1&0\\ 1&1&0&0&0&1&0\\ 1&1&0&0&0&0&1\\ 1&1&0&0&0&0&1\\ 1&0&1&0&1&0&0\\ 1&0&1&0&1&0&0\\ 1&0&1&0&0&1&0\\ 1&0&1&0&0&0&1\\ 1&0&1&0&0&0&1\\ 1&0&1&0&0&0&1\\ 1&0&0&1&1&0&0\\ 1&0&0&1&0&1&0\\ 1&0&0&1&0&0&1\\ 1&0&0&1&0&0&1\\ 1&0&0&1&0&0&1\\ 1&0&0&1&0&0&1\\ 1&0&0&1&0&0&1\\ 1&0&0&1&0&0&1\\ 1&0&0&1&0&0&1\\ \end{bmatrix}.\]

#### Small Test Statistics

When testing the reduced model, what happens when the full model \(Y=X\beta+e\) is not true? In particular, what happens if \(\operatorname{E}(Y)\in C(X_{0})+C(I-M)\), i.e., \(\operatorname{E}(Y)=X_{0}\gamma+W_{0}\delta_{0}\) where \(W_{0}\delta_{0}\in C(I-M)\)? In that case,

\[\operatorname{E}[Y^{\prime}(M-M_{0})Y] =\sigma^{2}\text{tr}(M-M_{0})+(X_{0}\gamma+W_{0}\delta_{0})^{ \prime}(M-M_{0})(X_{0}\gamma+W_{0}\delta_{0})\] \[=\sigma^{2}[r(X)-r(X_{0})]+0\]

but

\[\operatorname{E}[Y^{\prime}(I-M)Y] =\sigma^{2}\text{tr}(I-M)+(X_{0}\gamma+W_{0}\delta_{0})^{\prime}( I-M)(X_{0}\gamma+W_{0}\delta_{0})\] \[=\sigma^{2}[n-r(X)]+(W_{0}\delta_{0})^{\prime}(W_{0}\delta_{0}),\]

so the \(F\) statistic is an estimate of \(1/\{1+\delta_{0}^{\prime}W_{0}^{\prime}W_{0}\delta_{0}/\sigma^{2}[n-r(X)]\}\) which gets small (close to 0) when \(\|W_{0}\delta_{0}\|^{2}\) is large. Appendix F goes into more detail about small \(F\) statistics.

#### A Generalized Test Procedure

Before considering tests of parametric functions, we consider a generalization of the test procedure outlined earlier. Assume that the model \(Y=X\beta+e\) is correct. We want to test the adequacy of a model \(Y=X_{0}\gamma+Xb+e\), where \(C(X_{0})\subset C(X)\) and \(Xb\) is some known vector. In generalized linear model terminology, \(Xb\) is called an _offset_.

##### Multiple Regression.

Consider the model

\[Y=[J,X_{1},X_{2},X_{3}]\begin{bmatrix}\beta_{0}\\ \beta_{1}\\ \beta_{2}\\ \beta_{3}\end{bmatrix}+e=\beta_{0}J+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{3}X_ {3}+e.\]

To test \(H_{0}:\beta_{2}=\beta_{3}+5\), \(\beta_{1}=0\), write the reduced model as

\[\begin{split} Y&=\beta_{0}J+(\beta_{3}+5)X_{2}+\beta_{3}X_{3}+e \\ &=\beta_{0}J+\beta_{3}(X_{2}+X_{3})+5X_{2}+e\\ &=[J,X_{2}+X_{3}]\begin{bmatrix}\beta_{0}\\ \beta_{3}\end{bmatrix}+5X_{2}+e.\end{split} \tag{3}\]

Alternatively, we could write the reduced model as

\[\begin{split} Y&=\beta_{0}J+\beta_{2}X_{2}+(\beta_{2}-5)X_{3}+e \\ &=\beta_{0}J+\beta_{2}(X_{2}+X_{3})-5X_{3}+e\\ &=[J,X_{2}+X_{3}]\begin{bmatrix}\beta_{0}\\ \beta_{2}\end{bmatrix}-5X_{3}+e.\end{split} \tag{4}\]

We will see that both reduced models lead to the same test.

The model \(Y=X\beta+e\) can be rewritten as \(Y-Xb=X\beta-Xb+e\). Since \(Xb\in C(X)\), this amounts to a reparameterization, \(Y-Xb=X\beta^{*}+e\), where \(\beta^{*}=\beta-b\). Since \(Xb\) is known, \(Y-Xb\) is still an observable random variable.

The reduced model \(Y=X_{0}\gamma+Xb+e\) can be rewritten as \(Y-Xb=X_{0}\gamma+e\). The question of testing the adequacy of the reduced model is now a straightforward application of our previous theory. The distribution of the test statistic is

\[\frac{(Y-Xb)^{\prime}(M-M_{0})(Y-Xb)/r(M-M_{0})}{(Y-Xb)^{\prime}( I-M)(Y-Xb)/r(I-M)}\\ \sim F\big{(}r(M-M_{0}),r(I-M),\,\beta^{*^{\prime}}X^{\prime}(M-M _{0})X\beta^{*}/2\sigma^{2}\big{)}\,.\]

The noncentrality parameter is zero if and only if \(0=\beta^{*^{\prime}}X^{\prime}(M-M_{0})X\beta^{*}=[(M-M_{0})(X\beta-Xb)]^{ \prime}[(M-M_{0})(X\beta-Xb)]\), which occurs if and only if\(M_{0}(X\beta-Xb)=0\) or \(X\beta=M_{0}(X\beta-Xb)+Xb\). The last condition is nothing more or less than that the reduced model is valid with \(\gamma=(X_{0}^{\prime}X_{0})^{-}X_{0}(X\beta-Xb)\), a fixed unknown parameter.

Note also that, since \((I-M)X=0\), in the denominator of the test statistic \((Y-Xb)^{\prime}(I-M)(Y-Xb)=Y^{\prime}(I-M)Y\), which is the \(SSE\) from the original full model. Moreover, the numerator sum of squares is

\[(Y-Xb)^{\prime}(M-M_{0})(Y-Xb)=(Y-Xb)^{\prime}(I-M_{0})(Y-Xb)-Y^{\prime}(I-M)Y,\]

which can be obtained by subtracting the \(SSE\) of the original full model from the \(SSE\) of the reduced model. To see this, write \(I-M_{0}=(I-M)+(M-M_{0})\).

_Example 3.2.3 Continued_. The numerator sum of squares for testing model (4) is \((Y+5X_{3})^{\prime}(M-M_{0})(Y+5X_{3})\). But \((M-M_{0})[X_{2}+X_{3}]=0\), so, upon observing that \(5X_{3}=-5X_{2}+5[X_{2}+X_{3}]\),

\[(Y+5X_{3})^{\prime}(M-M_{0})(Y+5X_{3})\] \[=(Y-5X_{2}+5[X_{2}+X_{3}])^{\prime}(M-M_{0})(Y-5X_{2}+5[X_{2}+X_{ 3}])\] \[=(Y-5X_{2})^{\prime}(M-M_{0})(Y-5X_{2}),\]

which is the numerator sum of squares for testing model (3). In fact, models (3) and (4) are equivalent because the only thing different about them is that one uses \(5X_{2}\) and the other uses \(-5X_{3}\); but the only difference between these terms is \(5[X_{2}+X_{3}]\in C(X_{0})\).

The phenomenon illustrated in this example is a special case of a general result. Consider the model \(Y=X_{0}\gamma+Xb+e\) for some unknown \(\gamma\) and known \(b\) and suppose \(X(b-b_{*})\in C(X_{0})\) for known \(b_{*}\). The model E(\(Y\)) = \(X_{0}\gamma+Xb\) holds if and only if E(\(Y\)) = \(X_{0}\gamma+X(b-b_{*})+Xb_{*}\), which holds if and only if E(\(Y\)) = \(X_{0}\gamma_{*}+Xb_{*}\) for some unknown \(\gamma_{*}\).

**Exercise 3.1**: (a) Show that the \(F\) test developed in the first part of this section is equivalent to the (generalized) likelihood ratio test for the reduced versus full models, cf. Casella and Berger (2002, Subsection 8.2.1). (b) Find an \(F\) test for \(H_{0}:X\beta=X\beta_{0}\) where \(\beta_{0}\) is known. (c) Construct a full versus reduced model test when \(\sigma^{2}\) has a known value \(\sigma_{0}^{2}\).

**Exercise 3.2**: Redo the tests in Exercise 2.2 using the theory of Section 3.2. Write down the models and explain the procedure.

**Exercise 3.3**: Redo the tests in Exercise 2.3 using the procedures of Section 3.2. Write down the models and explain the procedure.

Hints: (a) Let \(A\) be a matrix of zeros, the generalized inverse of \(A\), \(A^{-}\), can be anything at all because \(AA^{-}A=A\) for any choice of \(A^{-}\). (b) There is no reason why \(X_{0}\) cannot be a matrix of zeros.

**Exercise 3.4**: When testing submodels of a largest model it is considered good practice to use the mean squared error from the largest model in the denominator of the test. Consider three linear models \(\operatorname{E}(Y)=X_{0}\gamma\), \(\operatorname{E}(Y)=X_{1}\delta\), \(\operatorname{E}(Y)=X\beta\) with \(C(X_{0})\subset C(X_{1})\subset C(X)\) and ppos \(M_{0}\), \(M_{1}\), \(M\), respectively. If \(Y\sim N(X_{1}\delta\), \(\sigma^{2}I)\), find the distribution of

\[\frac{Y^{\prime}(M_{1}-M_{0})Y/r(M_{1}-M_{0})}{Y^{\prime}(I-M)Y/r(I-M)}.\]

### Testing Linear Parametric Functions

In this section, the theory of testing linear parametric functions is presented. A basic test procedure and a generalized test procedure are given. These procedures are analogous to the model testing procedures of Section 2. In the course of this presentation, the important concept of the constraint imposed by an hypothesis is introduced. Finally, a class of hypotheses that is rarely used for linear models but commonly used with log-linear models is given along with results that define the appropriate testing procedure.

Consider a general linear model

\[Y=X\beta+e \tag{1}\]

with \(X\) an \(n\times p\) matrix. A key aspect of this model is that \(\beta\) is allowed to be any vector in \(\mathbf{R}^{p}\). Additionally, consider an hypothesis concerning a linear function, say \(\Lambda^{\prime}\beta=0\). The null model to be tested is

\[H_{0}:Y=X\beta+e\ \ \ \text{and}\ \ \ \Lambda^{\prime}\beta=0.\]

We need to find a reduced model that corresponds to this.

The constraint \(\Lambda^{\prime}\beta=0\) can be stated in an infinite number of ways. Observe that \(\Lambda^{\prime}\beta=0\) holds if and only if \(\beta\perp C(\Lambda)\); so if \(\Gamma\) is another matrix with \(C(\Gamma)=C(\Lambda)\), the constraint can also be written as \(\beta\perp C(\Gamma)\) or \(\Gamma^{\prime}\beta=0\).

To identify the reduced model, pick a matrix \(U\) such that

\[C(U)=C(\Lambda)^{\perp},\]

then \(\Lambda^{\prime}\beta=0\) if and only if \(\beta\perp C(\Lambda)\) if and only if \(\beta\in C(U)\), which occurs if and only if for some vector \(\gamma\),

\[\beta=U\gamma. \tag{2}\]Substituting (2) into the linear model gives the reduced model

\[Y=XU\gamma+e\]

or, letting \(X_{0}\equiv XU\),

\[Y=X_{0}\gamma+e. \tag{3}\]

Note that \(C(X_{0})\subset C(X)\). Proposition B.54 establishes that the particular choice of \(U\) is irrelevant to defining \(C(X_{0})\). Moreover, the reduced model does not depend on \(\operatorname{Cov}(Y)\) or the exact distribution of \(e\), it only depends on \(\operatorname{E}(Y)=X\beta\) and the constraint \(\Lambda^{\prime}\beta=0\).

If \(e\sim N(0,\,\sigma^{2}I)\), the reduced model (3) allows us to test \(\Lambda^{\prime}\beta=0\) by applying the results of Section 2. If \(C(X_{0})=C(X)\), the constraint involves only a reparameterization and there is nothing to test. In other words, if \(C(X_{0})=C(X)\), then \(\Lambda^{\prime}\beta=0\) involves only arbitrary side conditions that do not affect the model.

#### Example 3.3.1

Consider the one-way analysis of variance model

\[\begin{bmatrix}y_{11}\\ y_{12}\\ y_{13}\\ y_{21}\\ y_{31}\\ y_{32}\end{bmatrix}=\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\end{bmatrix}\beta+e,\]

where \(\beta=(\mu,\,\alpha_{1},\,\alpha_{2},\,\alpha_{3})^{\prime}\). The parameters in this model are not uniquely defined because the rank of \(X\) is less than the number of columns.

Let \(\lambda_{1}^{\prime}=(0,\,1,\,0,\,-1)\). The contrast \(\lambda_{1}^{\prime}\beta\) is estimable, so the hypothesis \(\alpha_{1}-\alpha_{3}=\lambda_{1}^{\prime}\beta=0\) determines an estimable constraint. To obtain \(C(\lambda_{1})^{\perp}=C(U)\), one can pick

\[U=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&1\\ 0&1&0\end{bmatrix},\]

which yields

\[XU=\begin{bmatrix}1&1&0\\ 1&1&0\\ 1&1&0\\ 1&0&1\\ 1&1&0\\ 1&1&0\end{bmatrix}. \tag{4}\]

This is a real restriction on \(C(X)\), i.e., \(C(XU)\neq C(X)\).

Let \(\lambda_{2}^{\prime}=(0,1,1,1)\). A nonestimable linear constraint for a one-way analysis of variance is that

\[\alpha_{1}+\alpha_{2}+\alpha_{3}=\lambda_{2}^{\prime}\beta=0.\]

Consider two choices for \(U\) with \(C(\lambda_{2})^{\perp}=C(U)\), i.e.,

\[U_{1}=\begin{bmatrix}1&0&0\\ 0&1&1\\ 0&-1&1\\ 0&0&-2\end{bmatrix}\quad\text{and}\quad U_{2}=\begin{bmatrix}1&0&0\\ 0&1&1\\ 0&0&-2\\ 0&-1&1\end{bmatrix}.\]

These yield

\[XU_{1}=\begin{bmatrix}1&1&1\\ 1&1&1\\ 1&1&1\\ 1&-1&1\\ 1&0&-2\\ 1&0&-2\end{bmatrix}\quad\text{and}\quad XU_{2}=\begin{bmatrix}1&1&1\\ 1&1&1\\ 1&1&1\\ 1&0&-2\\ 1&-1&1\\ 1&-1&1\end{bmatrix}.\]

Note that \(C(X)=C(XU_{1})=C(XU_{2})\). The models determined by \(XU_{1}\) and \(XU_{2}\) are equivalent linear models but have different parameterizations, say \(Y=XU_{1}\xi_{1}+e\) and \(Y=XU_{2}\xi_{2}+e\), with \(\xi_{i}=(\xi_{i1},\,\xi_{i2},\,\xi_{i3})^{\prime}\). Transform \(\xi_{i}\) to the original \(\beta\) parameterization using (2), for example,

\[\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}=\beta=U_{1}\xi_{1}=\begin{bmatrix}\xi_{11}\\ \xi_{12}+\xi_{13}\\ -\xi_{12}+\xi_{13}\\ -2\xi_{13}\end{bmatrix}.\]

Both \(\xi_{i}\) parameterizations lead to \(\alpha_{1}+\alpha_{2}+\alpha_{3}=0\). Thus both determine the same specific choice for the parameterization of the original one-way analysis of variance model.

Similar results hold for alternative nonidentifiable constraints such as \(\alpha_{1}=0\). As will be established later, any nonidentifiable constraint leaves the estimation space unchanged and therefore yields the same estimates of estimable functions.

Now consider the joint constraint \(\Lambda_{1}^{\prime}\beta=0\), where

\[\Lambda_{1}^{\prime}=\begin{bmatrix}\lambda_{1}^{\prime}\\ \lambda_{2}^{\prime}\end{bmatrix}.\]

\(\lambda_{1}^{\prime}\beta\) is a contrast, so it is estimable; therefore \(\Lambda_{1}^{\prime}\beta\) has estimable aspects. One choice of \(U\) with \(C(\Lambda_{1})^{\perp}=C(U)\) is

\[U_{3}=\begin{bmatrix}1&0\\ 0&1\\ 0&-2\\ 0&1\end{bmatrix}.\]This gives

\[XU_{3}=\begin{bmatrix}1&1\\ 1&1\\ 1&1\\ 1&-2\\ 1&1\\ 1&1\end{bmatrix},\]

and the estimation space is the same as in (4), where only the contrast \(\lambda_{1}^{\prime}\beta\) was assumed equal to zero.

A constraint equivalent to \(\Lambda_{1}^{\prime}\beta=0\) is \(\Lambda_{2}^{\prime}\beta=0\), where

\[\Lambda_{2}^{\prime}=\begin{bmatrix}\lambda_{3}^{\prime}\\ \lambda_{2}^{\prime}\end{bmatrix}=\begin{bmatrix}0&2&1&0\\ 0&1&1&1\end{bmatrix}.\]

The constraints are equivalent because \(C(\Lambda_{1})=C(\Lambda_{2})\). (Note that \(\lambda_{3}-\lambda_{2}=\lambda_{1}\).) Neither \(\lambda_{3}^{\prime}\beta\) nor \(\lambda_{2}^{\prime}\beta\) is estimable, so separately they would affect only the parameterization of the model. However, \(\Lambda_{1}^{\prime}\beta=0\) involves an estimable constraint, so \(\Lambda_{2}^{\prime}\beta=0\) also has an estimable part to the constraint. The concept of the estimable part of a constraint will be examined in detail later.

#### Estimable Constraints

We have established a perfectly general method for identifying the reduced model determined by an arbitrary linear constraint \(\Lambda^{\prime}\beta=0\), and thus we have a general method for testing \(\Lambda^{\prime}\beta=0\) by applying the results of Section 2. Next, we will examine the form of the test statistic when \(\Lambda^{\prime}\beta\) is estimable. Afterwards, we present results showing that there is little reason to consider nonestimable linear constraints.

When \(\Lambda^{\prime}\beta\) is estimable, so that \(\Lambda^{\prime}=P^{\prime}X\) for some \(P\), rather than finding \(U\) we can find the numerator projection operator for testing \(\Lambda^{\prime}\beta=0\) in terms of \(P\) and \(M\). Better yet, we can find the numerator sum of squares in terms of \(\Lambda\) and any least squares estimate \(\hat{\beta}\). Recall from Section 2 that the numerator sum of squares is \(Y^{\prime}(M-M_{0})Y\), where \(M-M_{0}\) is the perpendicular projection operator onto the orthogonal complement of \(C(X_{0})\) with respect to \(C(X)\). In other words, \(M-M_{0}\) is a perpendicular projection operator with \(C(M-M_{0})=C(X_{0})_{C(X)}^{\perp}\). For testing an estimable parametric hypothesis with \(\Lambda^{\prime}=P^{\prime}X\), we now show that the perpendicular projection operator onto \(C(MP)\) is also the perpendicular projection operator onto the orthogonal complement of \(C(X_{0})\) with respect to \(C(X)\), i.e., that \(C(MP)=C(X_{0})_{C(X)}^{\perp}\). It follows immediately that the numerator sum of squares in the test is \(Y^{\prime}M_{MP}Y\), where \(M_{MP}\equiv MP(P^{\prime}MP)^{-}P^{\prime}M\) is the perpendicular projection operator onto \(C(MP)\). In particular, from Section 2

\[\frac{Y^{\prime}M_{MP}Y/r(M_{MP})}{Y^{\prime}(I-M)Y/r(I-M)}\sim F\big{(}r(M_{MP }),r(I-M),\,\beta^{\prime}X^{\prime}M_{MP}X\beta/2\sigma^{2}\big{)}\,. \tag{5}\]Proposition 3.3.2 provides a formal proof of the necessary result. However, after the proof, we give an alternative justification based on finding the reduced model associated with the constraint. This reduced model argument differs from the one given at the beginning of the section in that it only applies to estimable constraints.

**Proposition 3.3.2**: _With \(U\) and \(P\) defined for \(\Lambda^{\prime}\beta=0\),_

\[C(M-M_{0})=C(X_{0})_{C(X)}^{\perp}\equiv C(XU)_{C(X)}^{\perp}=C(MP).\]

_Proof_ From Section 2, we already know that \(C(M-M_{0})=C(X_{0})_{C(X)}^{\perp}\). Since \(X_{0}\equiv XU\), we need only establish that \(C(XU)_{C(X)}^{\perp}=C(MP)\).

If \(x\in C(XU)_{C(X)}^{\perp},\text{then}\,0=x^{\prime}XU,\text{so}\,X^{\prime}x \perp C(U)\,\text{and}\,X^{\prime}x\in C(\Lambda)=C(X^{\prime}P)\). It follows that

\[x=Mx=[X(X^{\prime}X)^{-}]X^{\prime}x\in C([X(X^{\prime}X)^{-}]X^{\prime}P)=C(MP).\]

Conversely, if \(x\in C(MP)\), then \(x=MPb\) for some \(b\) and

\[x^{\prime}XU=b^{\prime}P^{\prime}MXU=b^{\prime}P^{\prime}XU=b^{\prime}\Lambda^ {\prime}U=0,\]

so \(x\in C(XU)_{C(X)}^{\perp}\). \(\square\)

Earlier, we found the reduced model matrix \(X_{0}=XU\) directly and then, for \(\Lambda^{\prime}=P^{\prime}X\), we showed that \(C(MP)=C(X_{0})_{C(X)}^{\perp}\), which led to the numerator sum of squares. An alternative derivation of the test arrives at \(C(M-M_{0})=C(X_{0})_{C(X)}^{\perp}=C(MP)\) more directly for estimable constraints. The reduced model is

\[Y=X\beta+e\ \ \ \text{and}\ \ \ P^{\prime}X\beta=0,\]

or

\[Y=X\beta+e\ \ \ \text{and}\ \ \ P^{\prime}MX\beta=0,\]

or

\[\text{E}(Y)\in C(X)\ \ \ \text{and}\ \ \ \text{E}(Y)\perp C(MP),\]

or

\[\text{E}(Y)\in C(X)\cap\ C(MP)^{\perp}.\]

The reduced model matrix \(X_{0}\) must satisfy \(C(X_{0})\)=\(C(X)\cap\ C(MP)^{\perp}\equiv C(MP)_{C(X)}^{\perp}\). It follows immediately that \(C(X_{0})_{C(X)}^{\perp}=C(MP)\). Moreover, it is easily seen that \(X_{0}\) can be taken as \(X_{0}=(I-M_{MP})X\).

**Theorem 3.3.3**: \(C[(I-M_{MP})X]=C(X)\cap\ C(MP)^{\perp}\)Proof.: First, assume \(x\in C(X)\) and \(x\perp C(MP)\). Write \(x=Xb\) for some \(b\) and note that \(M_{MP}x=0\). It follows that \(x=(I-M_{MP})x=(I-M_{MP})Xb\), so \(x\in C[(I-M_{MP})X]\).

Conversely, if \(x=(I-M_{MP})Xb\) for some \(b\), then clearly \(x\in C(X)\) and \(x^{\prime}MP=b^{\prime}X^{\prime}(I-M_{MP})MP=0\) because \((I-M_{MP})MP=0\) 

Note also that \(C(X)\cap\ C(MP)^{\perp}=C(X)\cap\ C(P)^{\perp}\).

_Example 3.3.4_.: To illustrate these ideas, consider testing \(H_{0}:\alpha_{1}-\alpha_{3}=0\) in Example 3.3.1. The constraint can be written

\[0=\alpha_{1}-\alpha_{3}=\left(\frac{1}{3},\frac{1}{3},\frac{1}{3},0,\frac{-1}{ 2},\frac{-1}{2}\right)\left[\begin{matrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\end{matrix}\right]\left[\begin{matrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{matrix}\right],\]

so \(P^{\prime}=(1/3,1/3,1/3,0,-1/2,-1/2)\). We need \(C(X)\cap\ C(MP)^{\perp}\). Note that vectors in \(C(X)\) have the form \((a,a,a,b,c,c)^{\prime}\) for any \(a,b,c\), so \(P=MP\). Vectors in \(C(MP)^{\perp}\) are \(v=(v_{11},v_{12},v_{13},v_{21},v_{31},v_{32})^{\prime}\) with \(P^{\prime}v=\bar{v}_{1}.-\bar{v}_{3}.=0\). Vectors in \(C(X)\cap\ C(MP)^{\perp}\) have the first three elements identical, the last two elements identical, and the average of the first three equal to the average of the last two, i.e., they have the form \((a,a,a,b,a,a)^{\prime}\). A spanning set for this space is given by the columns of

\[X_{0}=\left[\begin{matrix}1&1&0\\ 1&1&0\\ 1&1&0\\ 1&0&1\\ 1&1&0\\ 1&1&0\end{matrix}\right].\]

As seen earlier, another choice for \(P\) is \((1,0,0,0,-1,0)^{\prime}\). Using \(M\) from Exercise 1.5.8b, this choice for \(P\) also leads to \(MP=(1/3,1/3,1/3,0,-1/2,-1/2)^{\prime}\). To compute \((I-M_{MP})X\), observe that

\[M_{MP}=\frac{1}{(1/3)+(1/2)}\left[\begin{matrix}1/9&1/9&1/9&0&-1/6&-1/6\\ 1/9&1/9&1/9&0&-1/6&-1/6\\ 1/9&1/9&1/9&0&-1/6&-1/6\\ 0&0&0&0&0&0\\ -1/6&-1/6&-1/6&0&1/4&1/4\\ -1/6&-1/6&-1/6&0&1/4&1/4\end{matrix}\right]\]\[=\frac{1}{5}\left[\begin{matrix}2/3&2/3&2/3&0&-1&-1\\ 2/3&2/3&2/3&0&-1&-1\\ 2/3&2/3&2/3&0&-1&-1\\ 0&0&0&0&0&0\\ -1&-1&-1&0&3/2&3/2\\ -1&-1&-1&0&3/2&3/2\\ \end{matrix}\right].\]

Then

\[(I-M_{MP})X=X-M_{MP}X\\ =X-\frac{1}{5}\left[\begin{matrix}0&2&0&-2\\ 0&2&0&-2\\ 0&2&0&-2\\ 0&0&0&0\\ 0&-3&0&3\\ 0&-3&0&3\\ \end{matrix}\right]=\left[\begin{matrix}1&3/5&0&2/5\\ 1&3/5&0&2/5\\ 1&3/5&0&2/5\\ 1&3/5&0&2/5\\ 1&0&1&0\\ 1&3/5&0&2/5\\ 1&3/5&0&2/5\\ \end{matrix}\right],\]

which has the same column space as \(X_{0}\) given earlier.

We have reduced the problem of finding \(X_{0}\) to that of finding \(C(X)\cap\ C(MP)^{\perp}\), which is just the orthogonal complement of \(C(MP)\) with respect to \(C(X)\). By Corollary B.48, \(C(X)\cap\ C(MP)^{\perp}=C(M-M_{MP})\), so \(M-M_{MP}\) is another valid choice for \(X_{0}\). For Example 3.3.1 with \(H_{0}:\alpha_{1}-\alpha_{3}=0\), \(M\) was given in Exercise 1.5.8b and \(M_{MP}\) was given earlier, so

\[M-M_{MP}=\left[\begin{matrix}1/5&1/5&1/5&0&1/5&1/5\\ 1/5&1/5&1/5&0&1/5&1/5\\ 1/5&1/5&1/5&0&1/5&1/5\\ 0&0&0&1&0&0\\ 1/5&1/5&1/5&0&1/5&1/5\\ 1/5&1/5&1/5&0&1/5&1/5\\ \end{matrix}\right].\]

This matrix has the same column space as the other choices of \(X_{0}\) that have been given.

For \(\Lambda^{\prime}\beta\) estimable, we now rewrite the test statistic in (5) in terms of \(\Lambda\) and \(\hat{\beta}\). First, we wish to show that \(r(\Lambda)=r(M_{MP})\). It suffices to show that \(r(\Lambda)=r(MP)\). Writing \(\Lambda=X^{\prime}P\), we see that for any vector \(b\), \(X^{\prime}Pb=0\) if and only if \(Pb\perp C(X)\), which occurs if and only if \(MPb=0\). It follows that \(C(P^{\prime}X)^{\perp}=C(P^{\prime}M)^{\perp}\) so that \(C(P^{\prime}X)=C(P^{\prime}M)\), \(r(P^{\prime}X)=r(P^{\prime}M)\), and \(r(\Lambda)=r(X^{\prime}P)=r(MP)\).

Now rewrite the quadratic form \(Y^{\prime}M_{MP}Y\). Recall that since \(X\hat{\beta}=MY\), we have \(\Lambda^{\prime}\hat{\beta}=P^{\prime}X\hat{\beta}=P^{\prime}MY\). Substitution gives

\[Y^{\prime}M_{MP}Y =Y^{\prime}MP(P^{\prime}MP)^{-}P^{\prime}MY\] \[=\hat{\beta}^{\prime}\Lambda(P^{\prime}X(X^{\prime}X)^{-}X^{ \prime}P)^{-}\Lambda^{\prime}\hat{\beta}\] \[=\hat{\beta}^{\prime}\Lambda[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}\Lambda^{\prime}\hat{\beta}.\]The test statistic in (5) becomes

\[\frac{\hat{\beta}^{\prime}\,\Lambda[\,\Lambda^{\prime}(X^{\prime}X)^{-}\,\Lambda]^ {-}\,\Lambda^{\prime}\hat{\beta}/r(\Lambda)}{MSE}.\]

A similar argument shows that the noncentrality parameter in (5) can be written as \(\beta^{\prime}\,\Lambda[\,\Lambda^{\prime}(X^{\prime}X)^{-}\,\Lambda]^{-}\, \Lambda^{\prime}\,\beta/2\sigma^{2}\). The test statistic consists of three main parts: _MSE_, \(\Lambda^{\prime}\hat{\beta}\), and the generalized inverse of \(\Lambda^{\prime}(X^{\prime}X)^{-}\,\Lambda\). Note that \(\sigma^{2}\,\Lambda^{\prime}(X^{\prime}X)^{-}\,\Lambda=\mathrm{Cov}(\Lambda^{ \prime}\hat{\beta})\). _These facts give an alternative method of deriving tests. One can simply find the estimate \(\Lambda^{\prime}\hat{\beta}\), the covariance matrix of the estimate, and the MSE._

For a single degree of freedom hypothesis \(H_{0}:\lambda^{\prime}\beta=0\), the numerator takes the especially nice form

\[\hat{\beta}^{\prime}\lambda[\lambda^{\prime}(X^{\prime}X)^{-}\lambda]^{-1} \lambda^{\prime}\hat{\beta}=(\lambda^{\prime}\hat{\beta})^{2}/[\lambda^{\prime }(X^{\prime}X)^{-}\lambda];\]

so the \(F\) test becomes: reject \(H_{0}:\lambda^{\prime}\beta=0\) if

\[\frac{(\lambda^{\prime}\hat{\beta})^{2}}{MSE[\lambda^{\prime}(X^{\prime}X)^{- }\lambda]}>F(1-\alpha,1,dfE)\;,\]

which is just the square of the \(t\) test that could be derived from the sampling distributions of the least squares estimate and the _MSE_, cf. Exercise 2.1. The _sum of squares for testing \(H_{0}:\lambda^{\prime}\beta=0\)_ is defined as

\[SS(\lambda^{\prime}\beta)\equiv\frac{(\lambda^{\prime}\hat{\beta})^{2}}{\lambda ^{\prime}(X^{\prime}X)^{-}\lambda}=\frac{(\rho^{\prime}MY)^{2}}{\rho^{\prime} M\rho}=Y^{\prime}[M\rho(\rho^{\prime}M\rho)^{-1}\rho^{\prime}M]Y\equiv Y^{ \prime}M_{M\rho}Y,\]

so that the test statistic is \(SS(\lambda^{\prime}\beta)/MSE\).

**Definition 3.3.5**: The condition \(\mathrm{E}(Y)\perp C(MP)\) is called the _constraint_ on the model caused (imposed) by \(\Lambda^{\prime}\beta=0\), where \(\Lambda^{\prime}=P^{\prime}X\). As a shorthand, we will call \(C(MP)\) the constraint caused by \(\Lambda^{\prime}\beta=0\). If \(C(MP)\subset\mathcal{M}\subset C(X)\), we say that \(C(MP)\) is the constraint on \(\mathcal{M}\) caused by \(\Lambda^{\prime}\beta=0\). If \(\Lambda^{\prime}\beta=0\) puts a constraint on \(\mathcal{M}\), we say that \(\Lambda^{\prime}\beta=0\) is an hypothesis in \(\mathcal{M}\).

**Exercise 3.5**:
1. Show that \(\beta^{\prime}X^{\prime}M_{MP}X\beta=0\) if and only if \(\Lambda^{\prime}\beta=0\).
2. Show that a necessary and sufficient condition for \(\rho^{\prime}_{1}X\beta=0\) and \(\rho^{\prime}_{2}X\beta=0\) to determine orthogonal constraints on the model is that \(\rho^{\prime}_{1}M\rho_{2}=0\).

**Exercise 3.6**: In testing a reduced model \(Y=X_{0}\gamma+e\) against a full model \(Y=X\beta+e\), what linear parametric function of the parameters is being tested?

### Theoretical Complements

If, rather than testing the constraint \(\Lambda^{\prime}\beta=0\), our desire is to estimate \(\beta\) subject to the constraint, simply estimate \(\gamma\) in model (3) and use \(\hat{\beta}=U\hat{\gamma}\). In this _constrained estimation_, the estimates automatically satisfy \(\Lambda^{\prime}\hat{\beta}=0\). Moreover, estimable functions \(\Gamma^{\prime}\beta=Q^{\prime}X\beta\) are equivalent to \(\Gamma^{\prime}U\gamma=Q^{\prime}XU\gamma\), and optimal estimates of \(\gamma\) are transformed into optimal estimates of \(\beta\).

We now examine the implications of testing \(\Lambda^{\prime}\beta=0\) when \(\Lambda^{\prime}\beta\) is not estimable. Recall that we began this section by finding the reduced model associated with such a constraint, so we already have a general method for performing such tests.

The first key result is that in defining a linear constraint there is no reason to use anything but estimable functions, because only estimable functions induce a real constraint on \(C(X)\). Theorem 3.3.6 identifies the _estimable part_ of \(\Lambda^{\prime}\beta\), say \(\Lambda^{\prime}_{0}\beta\), and implies that \(\Lambda^{\prime}_{0}\beta=0\) gives the same reduced model as \(\Lambda^{\prime}\beta=0\). \(\Lambda_{0}\) is a matrix chosen so that \(C(\Lambda)\cap\ C(X^{\prime})=C(\Lambda_{0})\). With such a choice, \(\Lambda^{\prime}\beta=0\) implies that \(\Lambda^{\prime}_{0}\beta=0\) but \(\Lambda^{\prime}_{0}\beta\) is estimable because \(C(\Lambda_{0})\subset C(X^{\prime})\), so \(\Lambda^{\prime}_{0}=P^{\prime}_{0}X\) for some \(P_{0}\).

**Theorem 3.3.6**: _If \(C(\Lambda)\cap\ C(X^{\prime})=C(\Lambda_{0})\) and \(C(U_{0})=C(\Lambda_{0})^{\perp}\), then \(C(XU)\)\(=C(XU_{0})\). Thus \(\Lambda^{\prime}\beta=0\) and \(\Lambda^{\prime}_{0}\beta=0\) induce the same reduced model._

_Proof_ \(C(\Lambda_{0})\subset C(\Lambda)\), so \(C(U)\equiv C(\Lambda)^{\perp}\subset C(\Lambda_{0})^{\perp}=C(U_{0})\) and \(C(XU)\subset C(XU_{0})\).

To complete the proof, we show that there cannot be any vectors in \(C(XU_{0})\) that are not in \(C(XU)\). In particular, we show that there are no nontrivial vectors in \(C(XU_{0})\) that are orthogonal to \(C(XU)\), i.e., if \(v\in C(XU)_{C(XU_{0})}^{\perp}\) then \(v=0\). If \(v\in C(XU)_{C(XU_{0})}^{\perp}\), then \(v^{\prime}XU=0\), so \(X^{\prime}v\perp C(U)\) and \(X^{\prime}v\in C(\Lambda)\). But also note that \(X^{\prime}v\in C(X^{\prime})\), so \(X^{\prime}v\in C(\Lambda)\cap C(X^{\prime})=C(\Lambda_{0})\). This implies that \(X^{\prime}v\perp C(U_{0})\), so \(v\perp C(XU_{0})\). We have shown that the vector \(v\) which, by assumption, is in \(C(XU_{0})\), is also orthogonal to \(C(XU_{0})\). The only such vector is the \(0\) vector. \(\square\)

Nontrivial estimable constraints always induce a real constraint on the column space.

**Proposition 3.3.7**: _If \(\Lambda^{\prime}\beta\) is estimable and \(\Lambda\neq 0\), then \(\Lambda^{\prime}\beta=0\) implies that \(C(XU)\neq C(X)\)._

_Proof_ With \(\Lambda^{\prime}=P^{\prime}X\), the definition of \(U\) gives \(0=\Lambda^{\prime}U=P^{\prime}XU=P^{\prime}MXU\), so \(C(XU)\perp C(MP)\). Both are subspaces of \(C(X)\); therefore if \(C(MP)\neq\{0\}\), we have \(C(X)\neq C(XU)\). To see that \(C(MP)\neq\{0\}\), note \(P^{\prime}MX=\Lambda^{\prime}\neq 0\), so \(C(MP)\) is not orthogonal to \(C(X)\) and \(C(MP)\neq\{0\}\). \(\square\)

Note that Proposition 3.3.7 also implies that whenever the estimable part \(\Lambda_{0}\) is different from \(0\), there is always a real constraint on the column space.

Corollary 3.3.8 establishes that \(\Lambda^{\prime}\beta\) has no estimable part if and only if the constraint does not affect the model. If the constraint does not affect the model, it merely defines a reparameterization, in other words, it merely specifies arbitrary side conditions. The corollary follows from the observation made about \(\Lambda_{0}\) after Proposition 3.3.7 and taking \(\Lambda_{0}=0\) in Theorem 3.3.6.

**Corollary 3.3.8**: \(C(\Lambda)\cap C(X^{\prime})=\{0\}\) _if and only if \(C(XU)=C(X)\)._

In particular, if \(\Lambda^{\prime}\beta\) is not estimable, we can obtain the numerator sum of squares for testing \(\Lambda^{\prime}\beta=0\) either by finding \(X_{0}=XU\) directly and using it to get \(M-M_{0}\), or by finding \(\Lambda_{0}\), writing \(\Lambda^{\prime}_{0}=P^{\prime}_{0}X\), and using \(M_{M^{\prime}\!\!P_{0}}\). But as noted earlier, there is no reason to have \(\Lambda^{\prime}\beta\) not estimable.

One final point worth noting. Each component of \(\Lambda^{\prime}\beta\) may be nonestimable but the joint constraint may contain an estimable component. For example, in one-way ANOVA, \(y_{ij}=\mu+\alpha_{i}+e_{ij}\), by itself the constraint \(\alpha_{1}=0\) is nonestimable. Similarly, \(\alpha_{2}=0\) is a nonestimable constraint. But together they imply that the estimable contrast \(\alpha_{1}-\alpha_{2}\) equals 0.

#### A Generalized Test Procedure

We now consider hypotheses of the form \(\Lambda^{\prime}\beta=d\) where \(d\in C(\Lambda^{\prime})\) so that the equation \(\Lambda^{\prime}\beta=d\) is solvable. (If the rows of \(\Lambda^{\prime}\) are linearly independent, the equation is always solvable.) Let \(b\) be such a solution. Note that

\[\Lambda^{\prime}\beta=\Lambda^{\prime}b=d\]

if and only if

\[\Lambda^{\prime}(\beta-b)=0\]

if and only if

\[(\beta-b)\perp C(\Lambda).\]

Again picking a matrix \(U\) such that

\[C(U)=C(\Lambda)^{\perp},\]

\(\Lambda^{\prime}(\beta-b)=0\) if and only if

\[(\beta-b)\in C(U),\]

which occurs if and only if for some vector \(\gamma\)

\[(\beta-b)=U\gamma.\]Multiplying both sides by \(X\) gives

\[X\beta-Xb=XU\gamma\]

or

\[X\beta=XU\gamma+Xb.\]

We can now substitute this into the linear model to get the reduced model

\[Y=XU\gamma+Xb+e,\]

or, letting \(X_{0}\equiv XU\),

\[Y=X_{0}\gamma+Xb+e. \tag{6}\]

Recall that \(b\) is a vector we can find, so \(Xb\) is a known (offset) vector. Exercise 3.9.8 establishes that the particular choice of \(b\) is irrelevant to the test. To estimate \(\beta\) subject to the linear constraint \(\Lambda^{\prime}\beta=d\), take \(\hat{\beta}=U\hat{\gamma}+b\).

The test for reduced models such as (6) was developed in Section 2. For nonestimable linear hypotheses, use that theory directly. If \(\Lambda^{\prime}=P^{\prime}X\), then \(C(X_{0})_{C(X)}^{\perp}=C(MP)\) and the test statistic is easily seen to be

\[\frac{(Y-Xb)^{\prime}M_{MP}(Y-Xb)/r(M_{MP})}{(Y-Xb)^{\prime}(I-M)(Y-Xb)/r(I-M)}.\]

Note that \(\Lambda^{\prime}\beta=d\) imposes the constraint \(\operatorname{E}(Y-Xb)\perp C(MP)\), so once again we could refer to \(C(MP)\) as the constraint imposed by the hypothesis.

We did not specify the solution \(b\) to \(\Lambda^{\prime}\beta=d\) that should be used. For \(\Lambda^{\prime}\beta\) estimable, it is easy (unlike the general case) to see that the test does not depend on the choice of \(b\). As mentioned in the previous section, \((Y-Xb)^{\prime}(I-M)(Y-Xb)=Y^{\prime}(I-M)Y\), so the denominator of the test is just the _MSE_ and does not depend on \(b\). The numerator term \((Y-Xb)^{\prime}M_{MP}(Y-Xb)\) equals \((\Lambda^{\prime}\hat{\beta}-d)^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}(\Lambda^{\prime}\hat{\beta}-d)\). The test statistic can be written as

\[\frac{(\Lambda^{\prime}\hat{\beta}-d)^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{ -}\Lambda]^{-}(\Lambda^{\prime}\hat{\beta}-d)/r(\Lambda)}{MSE}. \tag{7}\]

For \(\Lambda^{\prime}\beta\) estimable, the linear model \(Y=X_{0}\gamma+Xb+e\) implies that \(\Lambda^{\prime}\beta=d\), but for nonestimable linear constraints, there are infinitely many constraints that result in the same reduced model. (If you think of nonestimable linear constraints as including arbitrary side conditions, that is not surprising.) In particular, if \(\Lambda^{\prime}\beta=d\), the same reduced model results if we take \(\Lambda^{\prime}\beta=d_{0}\) where \(d_{0}=d+\Lambda^{\prime}v\) and \(v\perp C(X^{\prime})\). Note that, in this construction, if \(\Lambda^{\prime}\beta\) is estimable, \(d_{0}=d\) for any \(v\).

We now present an application of the test statistic (7). The results are given without justification, but they should seem similar to results from a statistical methods course.

Example 3.3.9: Consider the balanced two-way ANOVA without interaction model

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk},\]

\(i=1,\ldots,a,j=1,\ldots,b,k=1,\ldots,N.\) (The analysis for this model is presented in Section 7.1.) We examine the test of the null hypothesis

\[H_{0}:\sum_{i=1}^{a}\lambda_{i}\alpha_{i}=4\ \ \text{ and }\ \ \sum_{j=1}^{b} \gamma_{j}\eta_{j}=7,\]

where \(\sum_{i=1}^{a}\lambda_{i}=0=\sum_{j=1}^{b}\gamma_{j}.\) The hypothesis is simultaneously specifying the values of a contrast in the \(\alpha_{i}\)s and a contrast in the \(\eta_{j}\)s.

In terms of the model \(Y=X\beta+e,\) we have

\[\beta=[\mu,\alpha_{1},\ldots,\alpha_{a},\eta_{1},\ldots,\eta_{b}]^{\prime}\]

\[\Lambda^{\prime}=\begin{bmatrix}0&\lambda_{1}&\cdots&\lambda_{a}&0&\cdots&0\\ 0&0&\cdots&0&\gamma_{1}&\cdots&\gamma_{b}\end{bmatrix}\]

\[d=\begin{bmatrix}4\\ 7\end{bmatrix}\]

\[\Lambda^{\prime}\hat{\beta}=\begin{bmatrix}\sum_{i=1}^{a}\lambda_{i}\bar{y}_{ i..}\\ \sum_{j=1}^{b}\gamma_{j}\bar{y}_{\cdot j.}\end{bmatrix}\]

\[\text{Cov}(\Lambda^{\prime}\hat{\beta})/\sigma^{2}=\Lambda^{\prime}(X^{\prime }X)^{-}\Lambda=\begin{bmatrix}\sum_{i=1}^{a}\lambda_{i}^{2}/bN&0\\ 0&\sum_{j=1}^{b}\gamma_{j}^{2}/aN\end{bmatrix}.\]

The diagonal elements of the covariance matrix are just the variances of the estimated contrasts. The off-diagonal elements are zero because this is a balanced two-way ANOVA, hence the estimates of the \(\alpha\) contrast and the \(\eta\) contrast are independent. We will see in Chapter 7 that these contrasts define orthogonal constraints in the sense of Definition 3.3.5, so they are often referred to as being orthogonal parameters.

There are two linearly independent contrasts being tested, so \(r(\Lambda)=2.\) The test statistic is

\[\frac{1}{2MSE}\left[\sum_{i=1}^{a}\lambda_{i}\bar{y}_{i..}-4\quad \sum_{j=1}^{b}\gamma_{j}\bar{y}_{\cdot j.}-7\right]\\ \times\begin{bmatrix}\frac{bN}{\sum_{i=1}^{a}\lambda_{i}^{2}}& \mathbf{0}\\ \mathbf{0}&\frac{aN}{\sum_{j=1}^{b}\gamma_{j}^{2}}\end{bmatrix}\begin{bmatrix} \sum_{j=1}^{a}\lambda_{i}\bar{y}_{i..}-4\\ \sum_{j=1}^{b}\gamma_{j}\bar{y}_{\cdot j.}-7\end{bmatrix}\]\[\frac{1}{2MSE}\left[\frac{\left(\sum_{i=1}^{a}\lambda_{i}\bar{y}_{i..}-4\right)^{2} }{\sum_{i=1}^{a}\lambda_{i}^{2}/bN}+\frac{\left(\sum_{j=1}^{b}\gamma_{j}\bar{y} _{j..}-7\right)^{2}}{\sum_{j=1}^{b}\gamma_{j}^{2}/aN}\right].\]

Note that the term \(\left(\sum_{i=1}^{a}\lambda_{i}\bar{y}_{i..}-4\right)^{2}/\left(\sum_{i=1}^{a} \lambda_{i}^{2}/bN\right)\) is, except for subtracting the 4, the sum of squares for testing \(\sum_{i=1}^{a}\lambda_{i}\alpha_{i}=0\). We are subtracting the 4 because we are testing \(\sum_{i=1}^{a}\lambda_{i}\alpha_{i}=4\). Similarly, we have a term that is very similar to the sum of squares for testing \(\sum_{j=1}^{b}\gamma_{j}\eta_{j}=0\). The test statistic takes the average of these sums of squares and divides by the _MSE_. The test is then defined by reference to an \(F(2,dfE,0)\) distribution.

#### Testing an Unusual Class of Hypotheses

Occasionally, a valid linear hypothesis \(\Lambda^{\prime}\beta=d\) is considered where \(d\) is not completely known but involves other parameters. (This is the linear structure involved in creating a logistic regression model from a log-linear model.) For \(\Lambda^{\prime}\beta=d\) to give a valid linear hypothesis, \(\Lambda^{\prime}\beta=d\) must put a restriction on \(C(X)\), so that when the hypothesis is true, \(\mathrm{E}(Y)\) lies in some subspace of \(C(X)\).

Let \(X_{1}\) be such that \(C(X_{1})\subset C(X)\) and consider an hypothesis

\[P^{\prime}X\beta=P^{\prime}X_{1}\delta\]

for some parameter vector \(\delta\). We seek an appropriate reduced model for such an hypothesis.

Note that the hypothesis occurs if and only if

\[P^{\prime}M(X\beta-X_{1}\delta)=0,\]

which occurs if and only if

\[(X\beta-X_{1}\delta)\perp C(MP),\]

which occurs if and only if

\[(X\beta-X_{1}\delta)\in C(MP)_{C(X)}^{\perp}.\]

As discussed earlier in this section, we choose \(X_{0}\) so that \(C(MP)_{C(X)}^{\perp}=C(X_{0})\). The choice of \(X_{0}\) does not depend on \(X_{1}\). Using \(X_{0}\), the hypothesis occurs if and only if for some \(\gamma\)

\[(X\beta-X_{1}\delta)=X_{0}\gamma.\]

Rewriting these terms, we see that

\[X\beta=X_{0}\gamma+X_{1}\delta\]which is the mean structure for the reduced model. In other words, assuming the null hypothesis is equivalent to assuming a reduced model

\[Y=X_{0}\gamma+X_{1}\delta+e.\]

To illustrate, consider a linear model for pairs of observations \((y_{1j},y_{2j})\), \(j=1,\ldots,N\). Write \(Y=(y_{11},\ldots,y_{1N},y_{21},\ldots,y_{2N})^{\prime}\). Initially, we will impose no structure on the means so that \(\operatorname{E}(y_{ij})=\mu_{ij}\). We are going to consider an hypothesis for the differences between the pairs,

\[\mu_{1j}-\mu_{2j}=z_{j}^{\prime}\delta\]

for some known predictor vector \(z_{j}\). Of course we could just fit a linear model to the differences \(y_{1j}-y_{2j}\), but we want to think about comparing such a model to models that are not based on the differences.

The conditions just specified correspond to a linear model \(Y=X\beta+e\) in which \(X=I_{2N}\) and \(\beta=(\mu_{11},\ldots,\mu_{2N})^{\prime}\). Write

\[P=\begin{bmatrix}I_{N}\\ -I_{N}\end{bmatrix}\quad\text{and}\quad X_{1}=\begin{bmatrix}Z\\ 0\end{bmatrix}\]

where \(Z^{\prime}=[z_{1},\ldots,z_{N}]\). Then the hypothesis for the differences can be specified as

\[P^{\prime}X\beta=P^{\prime}X_{1}\delta=Z\delta.\]

Finally, it is not difficult to see that a valid choice of \(X_{0}\) is

\[X_{0}=\begin{bmatrix}I_{N}\\ I_{N}\end{bmatrix}.\]

It follows that, under the reduced model

\[X\beta\equiv I\beta=\begin{bmatrix}I&Z\\ I&0\end{bmatrix}\begin{bmatrix}\gamma\\ \delta\end{bmatrix}\]

or that the reduced model is

\[Y=\begin{bmatrix}I&Z\\ I&0\end{bmatrix}\begin{bmatrix}\gamma\\ \delta\end{bmatrix}+e.\]

This relationship is of particular importance in the analysis of frequency data. The model \(y_{ij}=\mu_{ij}+e_{ij}\) is analogous to a saturated log-linear model. The hypothesis \(\mu_{1j}-\mu_{2j}=\alpha_{0}+\alpha_{1}t_{j}\equiv z_{j}^{\prime}\delta\) is analogous to the hypothesis that a simple linear logit model in a predictor \(t\) holds. We have found the vector space such that restricting the (saturated) log-linear model to that space gives the logit model, see Christensen (1997) for more details.

### Discussion

The reason that we considered testing models first and then discussed testing parametric functions by showing them to be changes in models is because, in _general,_ only model testing is ever performed. This is not to say that parametric functions are not tested as such, but that parametric functions are only tested in special cases. In particular, parametric functions can easily be tested in balanced ANOVA problems and one-way ANOVAs. Multifactor ANOVA designs with unequal numbers of observations in the treatment cells, as illustrated in Example 3.2.2, are best analyzed by considering alternative models. In fact, Christensen (2015) was written (largely) to illustrate that balanced ANOVA methods can be applied to unbalanced ANOVA by recasting the procedures as identifying relevant reduced models.

Even in regression models, where all the parameters are estimable, it is often more enlightening to think in terms of model selection. Of course, in regression there is a special relationship between the parameters and the model matrix. For the model \(y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdots+\beta_{p-1}x_{i}\,_{p-1} +e\), the model matrix can be written as \(X=[J,\,X_{1},\,\ldots,\,X_{p-1}]\), where \(X_{j}=[x_{1j},\,x_{2j},\,\ldots,\,x_{nj}]^{\prime}\). The test of \(H_{0}:\beta_{j}=0\) is obtained by just leaving \(X_{j}\) out of the model matrix.

Another advantage of the method of testing models is that it is often easy in simple but nontrivial cases to see immediately what new model is generated by a null hypothesis. This was illustrated in Examples 3.2.0 and 3.2.3.

#### 3.4.1 One-Way ANOVA.

Consider the model \(y_{ij}=\mu+\alpha_{i}+e_{ij}\), \(i=1,\,2,\,3\), \(j=1,\,\ldots,\,N_{i}\), \(N_{1}=N_{3}=3\), \(N_{2}=2\). In matrix terms this is

\[Y=\left[\begin{array}{cccc}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ \end{array}\right]\left[\begin{array}{c}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\\ \end{array}\right]+e.\]

Let the null hypothesis be \(\alpha_{1}=\mu+2\alpha_{2}\). Writing \(X=[J,\,X_{1},\,X_{2},\,X_{3}]\) and

\[\mathrm{E}(Y)=X\beta=\mu J+\alpha_{1}X_{1}+\alpha_{2}X_{2}+\alpha_{3}X_{3},\]

the reduced model is easily found by substituting \(\mu+2\alpha_{2}\) for \(\alpha_{1}\) which leads to

\[\mathrm{E}(Y)=\mu(J+X_{1})+\alpha_{2}(2X_{1}+X_{2})+\alpha_{3}X_{3}.\]

This gives the reduced model\[Y=\left[\begin{array}{ccc}2&2&0\\ 2&2&0\\ 2&2&0\\ 1&1&0\\ 1&1&0\\ 1&0&1\\ 1&0&1\\ 1&0&1\\ 1&0&1\\ \end{array}\right]\left[\begin{array}{c}\gamma_{0}\\ \gamma_{1}\\ \gamma_{2}\\ \end{array}\right]+e.\]

For Examples 3.2.0, 3.2.3, and 3.4.1, it would be considerable work to go through the procedure developed in Section 3 to test the hypotheses. In fairness, it should be added that for these special cases, there is no need to go through the general procedures of Section 3 to get the tests (assuming that you get the necessary computer output for the regression problem).

### Testing Single Degrees of Freedom in a Given Subspace

Consider a two-way ANOVA model \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\). Suppose we want to look at contrasts in the \(\alpha_{i}\)s and \(\eta_{j}\)s. For analyzing a balanced two-way ANOVA it would be very convenient if estimates and tests for contrasts in the \(\alpha_{i}\)s, say, could be based on the projection operator associated with dropping the \(\alpha_{i}\)s out of the one-way ANOVA model \(y_{ijk}=\mu+\alpha_{i}+e_{ijk}\) rather than the projection operator for the two-way ANOVA model. One convenience is that the projection operator for the one-way model turns out to be much simpler than the projection operator for the two-way model. A second convenience is that orthogonality of the projection operators for dropping the \(\alpha_{i}\)s and \(\eta_{j}\)s in the balanced two-way model leads to independence between estimates of contrasts in the \(\alpha_{i}\)s and \(\eta_{j}\)s. We would also like to establish that orthogonal contrasts (contrasts that define orthogonal constraints) in the \(\alpha_{i}\)s, say, depend only on the projection operator for dropping the \(\alpha_{i}\)s in the one-way model.

With these ultimate goals in mind, we now examine, in general, estimates, tests, and orthogonality relationships between single degree of freedom hypotheses that put a constraint on a particular subspace.

Consider a perpendicular projection operator \(M_{*}\) used in the numerator of a test statistic. In the situation of testing a model \(Y=X\beta+e\) against a reduced model \(Y=X_{0}\gamma+e\) with \(C(X_{0})\subset C(X)\), if \(M\) and \(M_{0}\) are the perpendicular projection operators onto \(C(X)\) and \(C(X_{0})\), respectively, then \(M_{*}=M-M_{0}\). For testing the estimable parametric hypothesis \(\Lambda^{\prime}\beta=0\), if \(\Lambda^{\prime}=P^{\prime}X\), then \(M_{*}=M_{MP}\).

We want to examine the problem of testing a single degree of freedom hypothesis in \(C(M_{*})\). Let \(\lambda^{\prime}=\rho^{\prime}X\). Then, by Definition 3.3.2, \(\lambda^{\prime}\beta=0\) puts a constraint on \(C(M_{*})\) if and only if \(M\rho\in C(M_{*})\). If \(M\rho\in C(M_{*})\), then \(M\rho=M_{*}M\rho=M_{*}\rho\) because \(MM_{*}=M_{*}\). It follows that the estimate of \(\lambda^{\prime}\beta\) is \(\rho^{\prime}M_{*}Y\) because \(\rho^{\prime}M_{*}Y=\rho^{\prime}MY\). From Section 3, the test statistic for \(H_{0}:\lambda^{\prime}\beta=0\) is \[\frac{Y^{\prime}M_{*}\rho(\rho^{\prime}M_{*}\rho)^{-1}\rho^{\prime}M_{*}Y}{MSE}= \frac{(\rho^{\prime}M_{*}Y)^{2}/\rho^{\prime}M_{*}\rho}{MSE},\]

where \(MSE=Y^{\prime}(I-M)Y/r(I-M)\) and \(r(M_{*}\rho(\rho^{\prime}M_{*}\rho)^{-1}\rho^{\prime}M_{*})=1\).

Let \(\lambda_{1}^{\prime}=\rho_{1}^{\prime}X\) and \(\lambda_{2}^{\prime}=\rho_{2}^{\prime}X\), and let the hypotheses \(\lambda_{1}^{\prime}\beta=0\) and \(\lambda_{2}^{\prime}\beta=0\) define orthogonal constraints on the model. The constraints are, respectively, \(\mathrm{E}(Y)\perp M\rho_{1}\) and \(\mathrm{E}(Y)\perp M\rho_{2}\). These constraints are said to be orthogonal if the vectors \(M\rho_{1}\) and \(M\rho_{2}\) are orthogonal. This occurs if and only if \(\rho_{1}^{\prime}M\rho_{2}=0\). If \(\lambda_{1}^{\prime}\beta=0\) and \(\lambda_{2}^{\prime}\beta=0\) both put constraints on \(C(M_{*})\), then orthogonality is equivalent to \(0=\rho_{1}^{\prime}M\rho_{2}=\rho_{1}^{\prime}M_{*}\rho_{2}\).

We have now shown that for any estimable functions that put constraints on \(C(M_{*})\), estimates, tests, and finding orthogonal constraints in \(C(M_{*})\) require only the projection operator \(M_{*}\) and the _MSE_.

**Exercise 3.7a**  Show that \(\rho^{\prime}MY=\rho^{\prime}[M\rho(\rho^{\prime}M\rho)^{-}\rho^{\prime}M]Y\) so that to estimate \(\rho^{\prime}X\beta\), one only needs the perpendicular projection of \(Y\) onto \(C(M\rho)\).

### Breaking a Sum of Squares into Independent Components

We now present a general theory that includes, as special cases, the breaking down of the treatment sum of squares in a one-way ANOVA into sums of squares for orthogonal contrasts and the breaking of the sum of squares for the model into independent sums of squares as in an ANOVA table. This is an important device in statistical analyses.

Frequently, a reduced model matrix \(X_{0}\) is a submatrix of \(X\). This is true for the initial hypotheses considered in both cases of Example 3.2.0 and for Example 3.2.2. If we can write \(X=[X_{0},\,X_{1}]\), it is convenient to write \(SSR(X_{1}|X_{0})\equiv Y^{\prime}(M-M_{0})Y\). \(SSR(X_{1}|X_{0})\) is called the sum of squares for regressing \(X_{1}\) after \(X_{0}\). We will also write \(SSR(X)\equiv Y^{\prime}MY\), the sum of squares for regressing on \(X\). Similarly, \(SSR(X_{0})\equiv Y^{\prime}M_{0}Y\). The \(SSR(\cdot)\) notation is one way of identifying sums of squares for tests. Other notations exist, and one alternative will soon be introduced. Note that \(SSR(X)=SSR(X_{0})+SSR(X_{1}|X_{0})\), which constitutes a breakdown of \(SSR(X)\) into two parts. If \(e\sim N(0,\,\sigma^{2}I)\), these two parts are independent.

We begin with a general theory and conclude with a discussion of breaking down the sums of squares in a two-way ANOVA model \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\). The projection operators used in the numerator sums of squares for dropping the \(\alpha_{i}\)s and \(\eta_{j}\)s are orthogonal if and only if the numerator sum of squares for dropping the \(\eta_{j}\)s out of the two-way model is the same as the numerator sum of squares for dropping the \(\eta_{j}\)s out of the one-way ANOVA model \(y_{ijk}=\mu+\eta_{j}+e_{ijk}\).

#### General Theory

We now present a general theory that is based on finding an orthonormal basis for a subspace of the estimation space. (This subspace could be the entire estimation space.) We discuss two methods of doing this. The first is a direct method involving identifying the subspace and choosing an orthonormal basis. The second method determines an orthonormal basis indirectly by examining single degree of freedom hypotheses and the constraints imposed by those hypotheses.

Our general model is \(Y=X\beta+e\) with \(M\) the perpendicular projection operator onto \(C(X)\). Let \(M_{*}\) be any perpendicular projection operator with \(C(M_{*})\subset C(X)\). Then \(M_{*}\) defines a test statistic

\[\frac{Y^{\prime}M_{*}Y/r(M_{*})}{Y^{\prime}(I-M)Y/r(I-M)}\]

for testing the reduced model, say, \(Y=(M-M_{*})\gamma+e\). If \(r(M_{*})=r\), then we will show that we can break the sum of squares based on \(M_{*}\) (i.e., \(Y^{\prime}M_{*}Y\)) into as many as \(r\) independent sums of squares whose sum will equal \(Y^{\prime}M_{*}Y\). By using \(M_{*}\) in the numerator of the test, we are testing whether the subspace \(C(M_{*})\) is adding anything to the predictive (estimative) ability of the model. What we have done is break \(C(X)\) into two orthogonal parts, \(C(M_{*})\) and \(C(M-M_{*})\). In this case, \(C(M-M_{*})\) is the estimation space under \(H_{0}\) and we can call \(C(M_{*})\) the _test space_. \(C(M_{*})\) is a space that will contain only error if \(H_{0}\) is true but which is part of the estimation space under the full model. Note that the error space under \(H_{0}\) is \(C(I-(M-M_{*}))\), but \(I-(M-M_{*})=(I-M)+M_{*}\) so that \(C(I-M)\) is part of the error space under both models.

We now break \(C(M_{*})\) into \(r\) orthogonal subspaces. Take an orthonormal basis for \(C(M_{*})\), say \(R_{1}\), \(R_{2}\),..., \(R_{r}\). Note that, using Gram-Schmidt, \(R_{1}\) can be any normalized vector in \(C(M_{*})\). It is the statistician's choice. \(R_{2}\) can then be any normalized vector in \(C(M_{*})\) orthogonal to \(R_{1}\), etc. Let \(R=[R_{1}\), \(R_{2}\),..., \(R_{r}]\), then as in Theorem B.35,

\[M_{*}=RR^{\prime}=[R_{1},\ldots,R_{r}]\begin{bmatrix}R_{1}^{\prime}\\ \vdots\\ R_{r}^{\prime}\end{bmatrix}=\sum_{i=1}^{r}R_{i}R_{i}^{\prime}.\]

Let \(M_{i}\equiv R_{i}R_{i}^{\prime}\), then \(M_{i}\) is a perpendicular projection operator in its own right and \(M_{i}M_{j}=0\) for \(i\neq j\) because of the orthogonality of the \(R_{i}\)s.

The goal of this section is to break up the sum of squares into independent components. By Theorem 1.3.7, the sums of squares \(Y^{\prime}M_{i}Y\) and \(Y^{\prime}M_{j}Y\) are independent for any \(i\neq j\) because \(M_{i}M_{j}=0\). Also, \(Y^{\prime}M_{*}Y=\sum_{i=1}^{r}Y^{\prime}M_{i}Y\) simply because \(M_{*}=\sum_{i=1}^{r}M_{i}\). Moreover, since \(r(M_{i})=1\),\[\frac{Y^{\prime}M_{i}Y}{Y^{\prime}(I-M)Y/r(I-M)}\sim F(1,r(I-M),\,\beta^{\prime}X^{ \prime}M_{i}X\beta/2\sigma^{2}).\]

In a one-way ANOVA, \(Y^{\prime}M_{*}Y\) corresponds to the treatment sum of squares while the \(Y^{\prime}M_{i}Y\)s correspond to the sums of squares for a set of orthogonal contrasts, cf. Example 3.6.2 below.

We now consider the correspondence between the hypothesis tested using \(Y^{\prime}M_{*}Y\) and those tested using the \(Y^{\prime}M_{i}Y\)s. Because \(M_{*}\) and the \(M_{i}\)s are nonnegative definite,

\[0=\beta^{\prime}X^{\prime}M_{*}X\beta=\sum_{i=1}^{r}\beta^{\prime}X^{\prime}M_ {i}X\beta\]

if and only if \(\beta^{\prime}X^{\prime}M_{i}X\beta=0\) for all \(i\) if and only if \(R_{i}^{\prime}X\beta=0\) for all \(i\). Thus, the null hypothesis that corresponds to the test based on \(M_{*}\) is true if and only if the null hypotheses \(R_{i}^{\prime}X\beta=0\) corresponding to all the \(M_{i}\)s are true. Equivalently, if the null hypothesis corresponding to \(M_{*}\) is not true, we have

\[0<\beta^{\prime}X^{\prime}M_{*}X\beta=\sum_{i=1}^{r}\beta^{\prime}X^{\prime}M_ {i}X\beta.\]

Again, since \(M_{*}\) and the \(M_{i}\)s are nonnegative definite, this occurs if and only if at least one of the terms \(\beta^{\prime}X^{\prime}M_{i}X\beta\) is greater than zero. Thus the null hypothesis corresponding to \(M_{*}\) is not true if and only if at least one of the hypotheses corresponding to the \(M_{i}\)s is not true. Thinking in terms of a one-way ANOVA, these results correspond to stating that (1) the hypothesis of no treatment effects is true if and only if all the contrasts in a set of orthogonal contrasts are zero or, equivalently, (2) the hypothesis of no treatment effects is not true if and only if at least one contrast in a set of orthogonal contrasts is not zero.

We have broken \(Y^{\prime}M_{*}Y\) into \(r\) independent parts. It is easy to see how to break it into less than \(r\) parts. Suppose \(r=7\). We can break \(Y^{\prime}M_{*}Y\) into three parts by looking at projections onto only three subspaces. For example, \(Y^{\prime}M_{*}Y=Y^{\prime}(M_{1}+M_{3}+M_{6})Y+Y^{\prime}(M_{2}+M_{7})Y+Y^{ \prime}(M_{4}+M_{5})Y\), where we have used three projection operators \(M_{1}+M_{3}+M_{6}\), \(M_{2}+M_{7}\), and \(M_{4}+M_{5}\). Note that these three projection operators are orthogonal, so the sums of squares are independent. By properly choosing \(R\), an ANOVA table can be developed using this idea.

Example 3.6.1: _One-Way ANOVA._

In this example we examine breaking up the treatment sum of squares in a one-way ANOVA. Consider the model \(y_{ij}=\mu+\alpha_{i}+e_{ij}\), \(i=1,\,2,\,3\), \(j=1,\,2,\,3\). In matrix terms this is \[Y=\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&1&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ \end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{bmatrix}+e. \tag{1}\]

Denote the model matrix \(X=[J,\,X_{1},\,X_{2},\,X_{3}]\). To test \(H_{0}:\alpha_{1}=\alpha_{2}=\alpha_{3}\), the reduced model is clearly

\[Y=J\mu+e.\]

The projection operator for the test is \(M_{*}=M-[1/n]J\,J^{\prime}\). The test space is \(C(M_{*})=C(M-[1/n]J\,J^{\prime})\), i.e., the test space is the set of all vectors in \(C(X)\) that are orthogonal to a column of ones. The test space can be obtained by using Gram-Schmidt to remove the effect of \(J\) from the last three columns of the model matrix, that is, \(C(M_{*})\) is spanned by the columns of

\[\begin{bmatrix}2&-1&-1\\ 2&-1&-1\\ 2&-1&-1\\ -1&2&-1\\ -1&2&-1\\ -1&-1&2\\ -1&-1&2\end{bmatrix},\]

which is a rank 2 matrix. The statistician is free to choose \(R_{1}\) within \(C(M_{*})\). \(R_{1}\) could be a normalized version of

\[\begin{bmatrix}2\\ 2\\ 2\\ -1\\ -1\\ -1\\ -1\\ -1\end{bmatrix}+\begin{bmatrix}-1\\ -1\\ -1\\ 2\\ 2\\ -1\\ -1\end{bmatrix}=\begin{bmatrix}1\\ 1\\ 1\\ 1\\ 1\\ 1\\ -2\\ -2\\ -2\end{bmatrix},\]

which was chosen as \(X_{1}+X_{2}\) with the effect of \(J\) removed. \(R_{2}\) must be the only normalized vector left in \(C(M_{*})\) that is orthogonal to \(R_{1}\). \(R_{2}\) is a normalized version of \([1,\,1,\,1,\,-1,\,-1,\,-1,\,0,\,0,\,0]^{\prime}\). The sum of squares for testing \(H_{0}:\alpha_{1}=\alpha_{2}=\alpha_{3}\) is \(Y^{\prime}R_{1}R_{1}^{\prime}Y+Y^{\prime}R_{2}R_{2}^{\prime}Y\).

Using the specified form of \(R_{1},\,Y^{\prime}M_{1}Y\) is the numerator sum of squares for testing

\[0=R_{1}^{\prime}X\beta\,\propto(0,\,3,\,3,\,-6)\left[\begin{matrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\end{matrix}\right]=6\left(\frac{\alpha_{1}+\alpha_{2}}{2}-\alpha_{ 3}\right).\]

Similarly,

\[R_{2}^{\prime}X\beta\,\propto\alpha_{1}-\alpha_{2}.\]

\(R_{1}\) was chosen so that \(Y^{\prime}R_{2}R_{2}^{\prime}Y\) would be the sum of squares for testing \(H_{0}:\alpha_{1}=\alpha_{2}\).

The discussion thus far has concerned itself with directly choosing an orthonormal basis for \(C(M_{*})\). An equivalent approach to the problem of finding an orthogonal breakdown is in terms of single degree of freedom hypotheses \(\lambda^{\prime}\beta=0\).

If we choose any \(r\) single degree of freedom hypotheses \(\lambda_{1}^{\prime}\beta\)=\(\cdots=\lambda_{r}^{\prime}\beta\)=0 with \(\rho_{k}^{\prime}X=\lambda_{k}^{\prime},\,\,M\rho_{k}\in C(M_{*})\), and \(\rho_{k}^{\prime}M\rho_{h}=0\) for all \(k\neq h\), then the vectors \(M\rho_{k}/\sqrt{\rho_{k}^{\prime}M\rho_{k}}\) form an orthonormal basis for \(C(M_{*})\). The projection operators are \(M_{k}\equiv M\rho_{k}(\rho_{k}^{\prime}M\rho_{k})^{-1}\rho_{k}^{\prime}M\). The sums of squares for these hypotheses,

\[Y^{\prime}M_{k}Y=Y^{\prime}M\rho_{k}(\rho_{k}^{\prime}M\rho_{k})^{-1}\rho_{k}^ {\prime}MY=\frac{(\rho_{k}^{\prime}MY)^{2}}{\rho_{k}^{\prime}M\rho_{k}}=\frac{ (\rho_{k}M_{*}Y)^{2}}{\rho_{k}^{\prime}M_{*}\rho_{k}},\]

form an orthogonal breakdown of \(Y^{\prime}M_{*}Y\).

As shown in Section 3, the sum of squares for testing \(\lambda_{k}^{\prime}\beta=0\) can be found from \(\lambda_{k}\), \(\hat{\beta}\), and \((X^{\prime}X)^{-}\) as \((\lambda_{k}^{\prime}\hat{\beta})/\lambda_{k}^{\prime}(X^{\prime}X)^{-}\lambda_ {k}\). In many ANOVA problems, the condition \(\rho_{k}^{\prime}M\rho_{h}=0\) can be checked by considering an appropriate function of \(\lambda_{k}\) and \(\lambda_{h}\). It follows that, in many problems, an orthogonal breakdown can be obtained without actually finding the vectors \(\rho_{1},\,\ldots,\,\rho_{r}\).

#### 3.6.2 One-Way ANOVA.

Consider the model \(y_{ij}=\mu+\alpha_{i}+e_{ij},i=1,\,\ldots,\,t,\,j=1,\,\ldots,\,N_{i}\). Let \(Y^{\prime}M_{*}Y\) correspond to the sum of squares for treatments (i.e., the sum of squares for testing \(\alpha_{1}=\cdots=\alpha_{t}\)). The hypotheses \(\lambda_{k}^{\prime}\beta=0\) correspond to contrasts \(c_{k1}\alpha_{1}+\cdots+c_{kt}\alpha_{t}=0\), where \(c_{k1}+\cdots+c_{kt}=0\). In Chapter 4, it will be shown that contrasts are estimable functions and that any contrast imposes a constraint on the space for testing equality of treatments. In other words, Chapter 4 shows that the \(\lambda_{k}^{\prime}\beta\)s can be contrasts and that if they are contrasts, then \(M\rho_{k}\in C(M_{*})\). In Chapter 4 it will also be shown that the condition for orthogonality, \(\rho_{k}^{\prime}M\rho_{h}=0\), reduces to the condition \(c_{k1}c_{h1}/N_{1}+\cdots+c_{kt}c_{ht}/N_{t}=0\). If the contrasts are orthogonal, then the sums of squares for the contrasts add up to the sums of squares for treatments, and the sums of squares for the contrasts are independent.

#### Two-Way ANOVA

We discuss the technique of breaking up sums of squares as it applies to the two-way ANOVA model of Example 3.2.2. The results really apply to any two-way ANOVA with unequal numbers. The sum of squares for the full model is \(Y^{\prime}MY\)(by definition). We can break this up into three parts, one for fitting the \(\eta_{j}\)s after having fit the \(\alpha_{i}\)s and \(\mu\), one for fitting the \(\alpha_{i}\)s after fitting \(\mu\), and one for fitting \(\mu\). In Example 3.2.2, the model is \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\), \(i=1\), \(2\), \(3\), \(j=1\), \(2\), \(3\). The seven columns of \(X\) correspond to the elements of \(\beta=[\mu,\alpha_{1},\alpha_{2},\alpha_{3},\eta_{1},\eta_{2},\eta_{3}]^{\prime}\) and were given earlier. Let \(J=(1,\ldots,1)^{\prime}\) be the first column of \(X\). Let \(X_{0}\) be a matrix consisting of the first four columns of \(X\), those corresponding to \(\mu,\alpha_{1},\alpha_{2},\) and \(\alpha_{3}\). Take \(M\) and \(M_{0}\) corresponding to \(X\) and \(X_{0}\). It is easy to see that \((1/21)JJ^{\prime}\) is the perpendicular projection operator onto \(C(J)\).

Since \(J\in C(X_{0})\subset C(X)\), we can write, with \(n=21\),

\[Y^{\prime}MY=Y^{\prime}\frac{1}{n}JJ^{\prime}Y+Y^{\prime}\left(M_{0}-\frac{1}{ n}JJ^{\prime}\right)Y+Y^{\prime}\left(M-M_{0}\right)Y,\]

where \((1/n)JJ^{\prime}\), \(M_{0}-(1/n)JJ^{\prime}\), and \(M-M_{0}\) are all perpendicular projection matrices. Since \(X_{0}\) is obtained from \(X\) by dropping the columns corresponding to the \(\eta_{j}\)s, \(Y^{\prime}(M-M_{0})Y\) is the sum of squares used to test the full model against the reduced model with the \(\eta_{j}\)s left out. Recalling our technique of looking at the differences in error sums of squares, we write \(Y^{\prime}(M-M_{0})Y\equiv R(\eta|\alpha,\,\mu)\). \(R(\eta|\alpha,\,\mu)\) is the reduction in (error) sum of squares due to fitting the \(\eta_{j}\)s after fitting \(\mu\) and the \(\alpha_{i}\)s, or, more simply, the sum of squares due to fitting the \(\eta_{j}\)s after the \(\alpha_{i}\)s and \(\mu\). Similarly, if we wanted to test the model \(y_{ijk}=\mu+e_{ijk}\) against \(y_{ijk}=\mu+\alpha_{i}+e_{ijk}\), we would use \(Y^{\prime}(M_{0}-[1/n]JJ^{\prime})Y\equiv R(\alpha|\mu)\), i.e. the sum of squares for fitting the \(\alpha_{i}\)s after \(\mu\). Finally, to test \(y_{ijk}=\mu+e_{ijk}\) against \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\), we would use \(Y^{\prime}(M-[1/n]JJ^{\prime})Y\equiv R(\alpha,\,\eta|\mu)\). Note that \(R(\alpha,\,\eta|\mu)=R(\eta|\alpha,\,\mu)+R(\alpha|\mu)\).

The notations \(SSR(\cdot)\) and \(R(\cdot)\) are different notations for essentially the same idea. The \(SSR(\cdot)\) notation emphasizes variables and is often used in regression problems. The \(R(\cdot)\) notation emphasizes parameters and is frequently used in analysis of variance problems.

Alternatively, we could have chosen to develop the results in this discussion by comparing the model \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\) to the model \(y_{ijk}=\mu+\eta_{j}+e_{ijk}\). Then we would have taken \(X_{0}\) as columns 1, 5, 6, and 7 of the \(X\) matrix instead of columns 1, 2, 3, and 4. This would have led to terms such as \(R(\eta|\mu)\), \(R(\alpha|\eta,\,\mu)\), and \(R(\alpha,\,\eta|\mu)\). In general, these two analyses _will not_ be the same. Typically, \(R(\eta|\alpha,\,\mu)\neq R(\eta|\mu)\) and \(R(\alpha|\mu)\neq R(\alpha|\eta,\,\mu)\). There do exist cases (e.g., balanced two-way ANOVA models) where the order of the analysis has no effect. Specifically, if the columns of the \(X\) matrix associated with \(\alpha\) and those associated with \(\eta\) are orthogonal after somehow fitting \(\mu\), then \(R(\eta|\alpha,\,\mu)=R(\eta|\mu)\) and \(R(\alpha|\mu)=R(\alpha|\eta,\,\mu)\). In Section 7.4 we establish that these relationships hold if and only if the data display _proportional numbers_.

As mentioned, the preceding discussion applies to all two-way ANOVA models. We now state precisely the sense in which the columns for \(\alpha\) and \(\eta\) need to be orthogonal. Let \(X_{0}\) be the columns of \(X\) associated with \(\mu\) and the \(\alpha_{i}\)s, and let \(X_{1}\) be the columns of \(X\) associated with \(\mu\) and the \(\eta_{j}\)s. Let \(M_{0}\) and \(M_{1}\) be the projection operators onto \(C(X_{0})\) and \(C(X_{1})\), respectively. We will show that \(R(\eta|\alpha,\,\mu)=R(\eta|\mu)\) for all \(Y\) if and only if \(C(M_{1}-[1/n]JJ^{\prime})\perp C(M_{0}-[1/n]JJ^{\prime})\), i.e., \((M_{1}-[1/n]JJ^{\prime})(M_{0}-[1/n]JJ^{\prime})=0\).

Since \(R(\eta|\mu)=Y^{\prime}(M_{1}-[1/n]JJ^{\prime})Y\) and \(R(\eta|\alpha,\,\mu)=Y^{\prime}(M-M_{0})Y\), it suffices to show the next proposition.

**Proposition 3.6.3**: _In two-way ANOVA, \((M_{1}-[1/n]JJ^{\prime})=(M-M_{0})\) if and only if \((M_{1}-[1/n]JJ^{\prime})(M_{0}-[1/n]JJ^{\prime})=0\)._

_Proof_\(\Rightarrow\) If \((M_{1}-[1/n]JJ^{\prime})=(M-M_{0})\), then

\[(M_{1}-[1/n]JJ^{\prime})(M_{0}-[1/n]JJ^{\prime})=(M-M_{0})(M_{0}-[1/n]JJ^{ \prime})=0\]

because \(J\in C(M_{0})\subset C(M)\).

\(\Leftarrow\) To simplify notation, let

\[M_{\alpha}\equiv(M_{0}-[1/n]JJ^{\prime})\ \ \ \text{and}\ \ \ M_{\eta}\equiv(M_{1}-[1/n]JJ^{\prime}).\]

We know that \(M=[1/n]JJ^{\prime}+M_{\alpha}+(M-M_{0})\). If we could show that \(M=[1/n]JJ^{\prime}+M_{\alpha}+M_{\eta}\), we would be done.

\([1/n]JJ^{\prime}+M_{\alpha}+M_{\eta}\) is symmetric and is easily seen to be idempotent since \(0=M_{\eta}M_{\alpha}=M_{\alpha}M_{\eta}\). It suffices to show that \(C\big{[}(1/n)JJ^{\prime}+M_{\alpha}+M_{\eta}\big{]}=C(X)\). Clearly, \(C\big{[}(1/n)JJ^{\prime}+M_{\alpha}+M_{\eta}\big{]}\subset C(X)\).

Suppose now that \(v\in C(X)\). Since \(C(M_{0})=C(X_{0})\) and \(C(M_{1})=C(X_{1})\), if we let \(Z=[M_{0},\,M_{1}]\), then \(C(Z)=C(X)\) and \(v=Zb=M_{0}b_{0}+M_{1}b_{1}\). Since \(J\in C(X_{0})\) and \(J\in C(X_{1})\), it is easily seen that \(M_{\alpha}M_{1}=M_{\alpha}M_{\eta}=0\) and \(M_{\eta}M_{0}=0\). Observe that

\[\begin{split}\left[\frac{1}{n}JJ^{\prime}+M_{\alpha}+M_{\eta} \right]v&=\big{[}M_{0}+M_{\eta}\big{]}\,M_{0}b_{0}+[M_{1}+M_{ \alpha}]\,M_{1}b_{1}\\ &=M_{0}b_{0}+M_{1}b_{1}=v,\end{split}\]

so \(C(X)\subset C\big{[}(1/n)JJ^{\prime}+M_{\alpha}+M_{\eta}\big{]}\). \(\square\)

The condition \((M_{1}-[1/n]JJ^{\prime})(M_{0}-[1/n]JJ^{\prime})=0\) is equivalent to what follows. Using the Gram-Schmidt orthogonalization algorithm, make all the columns corresponding to the \(\alpha\)s and \(\eta\)s orthogonal to \(J\). Now, if the transformed \(\alpha\) columns are orthogonal to the transformed \(\eta\) columns, then \(R(\eta|\alpha,\,\mu)=R(\eta|\mu)\). In other words, check the condition \(X^{\prime}_{0}(I-[1/n]JJ^{\prime})X_{1}=0\). In particular, this occurs in a balanced two-way ANOVA model, see Section 7.1, and is the crux of the proportional numbers argument in Section 7.4.

From the symmetry of the problem, it follows that \(R(\alpha|\eta,\,\mu)=R(\alpha|\mu)\) whenever \(R(\eta|\alpha,\,\mu)=R(\eta|\mu)\).

### Confidence Regions

Consider the problem of finding a confidence region for the estimable parametric vector \(\Lambda^{\prime}\beta\). A \((1-\alpha)100\%\) confidence region for \(\Lambda^{\prime}\beta\) consists of all the vectors \(d\) that would not be rejected by an \(\alpha\) level test of \(\Lambda^{\prime}\beta=d\). That is to say, a \((1-\alpha)100\%\) confidence region for \(\Lambda^{\prime}\beta\) consists of all the vectors \(d\) that are consistent with the data and the full model as determined by an \(\alpha\) level test of \(\Lambda^{\prime}\beta=d\). Based on the distribution theory of Section 2 and the algebraic simplifications of Section 3, the \((1-\alpha)100\%\) confidence region consists of all the vectors \(d\) that satisfy the inequality

\[\frac{[\Lambda^{\prime}\hat{\beta}-d]^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{ -}\Lambda]^{-}[\Lambda^{\prime}\hat{\beta}-d]/r(\Lambda)}{MSE}\leq F(1-\alpha,\,r(\Lambda),\,r(I-M)). \tag{1}\]

These vectors form an ellipsoid that is degenerate when \(r(\Lambda)\) is less than its number of columns.

Alternative forms for the confidence region are

\[[\Lambda^{\prime}\hat{\beta}-d]^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}[\Lambda^{\prime}\hat{\beta}-d]\leq r(\Lambda)\ MSE\ F(1-\alpha, \,r(\Lambda),\,r(I-M))\]

and

\[[P^{\prime}MY-d]^{\prime}(P^{\prime}MP)^{-}[P^{\prime}MY-d]\leq r(MP)\ MSE\ F(1-\alpha,\,r(MP),\,r(I-M)).\]

For regression problems we can get a considerable simplification. If we take \(P^{\prime}=(X^{\prime}X)^{-1}X^{\prime}\), then we have \(\Lambda^{\prime}=P^{\prime}X=I_{p}\) and \(\Lambda^{\prime}\beta=\beta=d\). Using these in (1) and renaming the placeholder variable \(d\) as \(\beta\) gives

\[[\Lambda^{\prime}\hat{\beta}-d]^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}[\Lambda^{\prime}\hat{\beta}-d] = (\hat{\beta}-\beta)^{\prime}[(X^{\prime}X)^{-1}]^{-1}(\hat{\beta}-\beta)\] \[= (\hat{\beta}-\beta)^{\prime}(X^{\prime}X)(\hat{\beta}-\beta)\]

with \(r(\Lambda)=r(I_{p})=r(X)\). The confidence region is thus the set of all \(\beta\)s satisfying

\[(\hat{\beta}-\beta)^{\prime}(X^{\prime}X)(\hat{\beta}-\beta)\leq p\ MSE\ F(1-\alpha,\,p,n-p).\]

**Exercise 3.7b** _Fieller's method_.

For a one-dimensional estimable function \(\lambda^{\prime}\beta\), use the quadratic formula to show that the confidence region agrees with the usual \(t(dfE)\) confidence interval discussed in Section 2.6.

### Tests for Generalized Least Squares Models

We now consider the problem of deriving tests for the model of Section 2.7. For testing, we take the generalized least squares model as

\[Y=X\beta+e,\ \ \ e\sim N\big{(}0,\sigma^{2}V\big{)}\, \tag{1}\]

where \(V\) is a known positive definite matrix. As in Section 2.7, we can write \(V=QQ^{\prime}\) for \(Q\) nonsingular. The model

\[\tilde{Y}=\tilde{X}\beta+\tilde{e},\ \ \ \tilde{e}\sim N\big{(}0,\sigma^{2}I \big{)} \tag{2}\]

where

\[\tilde{Y}\equiv Q^{-1}Y,\ \ \ \tilde{X}\equiv Q^{-1}X,\ \ \ \tilde{e}\equiv Q^{-1 }e.\]

is analyzed instead of model (1).

First consider the problem of testing model (1) against a reduced model, say

\[Y=X_{0}\beta_{0}+e,\ \ \ e\sim N\big{(}0,\sigma^{2}V\big{)}\,\ \ \ C(X_{0}) \subset C(X). \tag{3}\]

The reduced model can be transformed to

\[\tilde{Y}=\tilde{X}_{0}\beta_{0}+\tilde{e},\ \ \ \tilde{e}\sim N\big{(}0, \sigma^{2}I\big{)} \tag{4}\]

where

\[\tilde{X}_{0}\equiv Q^{-1}X.\]

The test of model (3) against model (1) is performed by testing model (4) against model (2). To test model (4) against model (2), we need to know that model (4) is a reduced model relative to model (2). In other words, we need to show that \(C(Q^{-1}X_{0})\subset C(Q^{-1}X)\). From model (3), \(C(X_{0})\subset C(X)\), so there exists a matrix \(U\) such that \(X_{0}=XU\). It follows immediately that \(Q^{-1}X_{0}=Q^{-1}XU\); hence \(C(\tilde{X}_{0})=C(Q^{-1}X_{0})\subset C(Q^{-1}X)=C(\tilde{X})\).

Recall from Section 2.7 that \(A=X(X^{\prime}V^{-1}X)^{-}X^{\prime}V^{-1}\) and that for model (1)

\[MSE=Y^{\prime}(I-A)^{\prime}V^{-1}(I-A)Y\Big{/}[n-r(X)].\]

Define \(A_{0}=X_{0}(X_{0}^{\prime}V^{-1}X_{0})^{-}X_{0}^{\prime}V^{-1}\). The test comes from the following distributional result.

**Theorem 3.8.1**: \[\mbox{(i)}\ \ \frac{Y^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})Y/[r(X)-r(X_ {0})]}{MSE}\sim F(r(X)-r(X_{0}),n-r(X),\pi)\,\]_where \(\pi=\beta^{\prime}X^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})X\beta/2\sigma^{2}\)._

(ii) \(\beta^{\prime}X^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})X\beta=0\) _if and only if \(\operatorname{E}(Y)\in C(X_{0})\)._

_Proof_ (i) Theorem 3.2.1 applied to models (2) and (4) gives the appropriate test statistic. It remains to show that part (i) involves the same test statistic. Exercise 3.8 is to show that \(Y^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})Y/[r(X)-r(X_{0})]\) is the appropriate numerator mean square.

(ii) From part (i) and Theorem 3.2.1 applied to models (2) and (4),

\[\beta^{\prime}X^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})X\beta=0\]

if and only if \(\operatorname{E}(Q^{-1}Y)\in C(Q^{-1}X_{0})\). \(\operatorname{E}(Q^{-1}Y)\in C(Q^{-1}X_{0})\) if and only if \(\operatorname{E}(Y)\in C(X_{0})\). \(\square\)

**Exercise 3.8**: Show that \(Y^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})Y/[r(X)-r(X_{0})]\) is the appropriate numerator mean square for testing model (4) against model (2).

The intuition behind the test based on Theorem 3.8.1 is essentially the same as that behind the usual test (which was discussed in Section 2). The usual test is based on the difference \(MY-M_{0}Y=(M-M_{0})Y\). \(MY\) is the estimate of \(\operatorname{E}(Y)\) from the full model, and \(M_{0}Y\) is the estimate of \(\operatorname{E}(Y)\) from the reduced model. The difference between these estimates indicates how well the reduced model fits. If the difference is large, the reduced model fits poorly; if the difference is small, the reduced model fits relatively well. To determine whether the difference vector is large or small, the squared length of the vector, as measured in Euclidean distance, is used. The squared length of \((M-M_{0})Y\) reduces to the usual form \(Y^{\prime}(M-M_{0})Y\). The basis of the test is to quantify how large the difference vector must be before there is some assurance that the difference between the vectors is due to more than just the variability of the data.

For generalized least squares models, the estimate of \(\operatorname{E}(Y)\) from the full model is \(AY\) and the estimate of \(\operatorname{E}(Y)\) from the reduced model is \(A_{0}Y\). The difference between these vectors, \(AY-A_{0}Y=(A-A_{0})Y\), indicates how well the reduced model fits. The test is based on the squared length of the vector \((A-A_{0})Y\), but the length of the vector is no longer measured in terms of Euclidean distance. The inverse of the covariance matrix is used to define a distance measure appropriate to generalized least squares models. Specifically, the squared distance between two vectors \(u\) and \(v\) is defined to be \((u-v)^{\prime}V^{-1}(u-v)\). Note that with this distance measure, the generalized least squares estimate \(AY\) is the vector in \(C(X)\) that is closest to \(Y\), i.e., \(AY\) is the perpendicular projection onto \(C(X)\) (cf. Section 2.7).

It should be noted that if \(V=I\), then \(A=M\), \(A_{0}=M_{0}\), and the test is exactly as in Section 2. Also as in Section 2, the key term in the numerator of the test statistic, \(Y^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})Y\), can be obtained as the difference between the _SSE_ for the reduced model and the _SSE_ for the full model.

Now consider testing parametric functions. If \(\Lambda^{\prime}\beta\) is an estimable parametric vector, then \(\Lambda^{\prime}=P^{\prime}X\), \(\Lambda^{\prime}\hat{\beta}=P^{\prime}AY\), and the test of the hypothesis \(\Lambda^{\prime}\beta=0\) can be obtained from the following result:

**Theorem 3.8.2**: \[\mbox{(i)}\;\;\;\frac{\hat{\beta}^{\prime}\Lambda\left[\Lambda^{\prime}(X^{ \prime}V^{-1}X)^{-}\Lambda\right]^{-}\Lambda^{\prime}\hat{\beta}/r(\Lambda)}{MSE }\sim F(r(\Lambda),n-r(X),\pi)\,,\]

_where \(\pi=\beta^{\prime}\Lambda\left[\Lambda^{\prime}(X^{\prime}V^{-1}X)^{-}\Lambda \right]^{-}\Lambda^{\prime}\beta/2\sigma^{2}\)._

\(\mbox{(ii)}\;\;\;\beta^{\prime}\Lambda\left[\Lambda^{\prime}(X^{\prime}V^{-1}X) ^{-}\Lambda\right]^{-}\Lambda^{\prime}\beta=0\) _if and only if \(\Lambda^{\prime}\beta=0\)._

_Proof \(\Lambda^{\prime}\beta\)_ is estimable in model (1) if and only if \(\Lambda^{\prime}\beta\) is estimable in model (2). \(\Lambda^{\prime}\hat{\beta}\) is the least squares estimate of \(\Lambda^{\prime}\beta\) from model (2), and \(\sigma^{2}\Lambda^{\prime}(X^{\prime}V^{-1}X)^{-}\Lambda\) is the covariance matrix of \(\Lambda^{\prime}\hat{\beta}\). The result follows immediately from Section 3 applied to model (2). \(\Box\)

Note that \(\Lambda^{\prime}\beta=0\) defines the same reduced model as in Section 3 but the test of the reduced model changes. Just as in Section 3 for ordinary least squares models, Theorem 3.8.2 provides a method of finding tests for generalized least squares models. To test \(\Lambda^{\prime}\beta=0\), one need only find \(\Lambda^{\prime}\hat{\beta}\), \(\mbox{Cov}(\Lambda^{\prime}\hat{\beta})\), and _MSE_. If these can be found, the test follows immediately.

We have assumed that \(V\) is a known matrix. Since the results depend on \(V\), they would seem to be of little use if \(V\) were not known. Nevertheless, the validity of the results does not depend on \(V\) being known. In Chapter 11, we will consider cases where \(V\) is not known, but where \(V\) and \(X\) are related in such a way that the results of this section can be used. In Chapter 11, we will need the distribution of the numerators of the test statistics.

**Theorem 3.8.3**: \[\mbox{(i)}\;\;Y^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})Y/\sigma^{2}\sim \chi^{2}(r(X)-r(X_{0}),\pi)\,,\]

_where \(\pi\equiv\beta^{\prime}X^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})X\beta/2 \sigma^{2}\), and \(\mbox{E}(Y)\in C(X_{0})\) if and only if \(\pi=0\)._

_For \(\Lambda^{\prime}\beta\) estimable, \(\hat{\beta}^{\prime}\Lambda\left[\Lambda^{\prime}(X^{\prime}V^{-1}X)^{-} \Lambda\right]^{-}\Lambda^{\prime}\hat{\beta}/\sigma^{2}\sim\chi^{2}(r(\Lambda ),\pi)\,,\)_

_where \(\pi\equiv\beta^{\prime}\Lambda\left[\Lambda^{\prime}(X^{\prime}V^{-1}X)^{-} \Lambda\right]^{-}\Lambda^{\prime}\beta/2\sigma^{2}\), and \(\Lambda^{\prime}\beta=0\) if and only if \(\pi=0\)._

_Proof \(\;\;\;\)_ The results follow from Sections 3.2 and 3.3 applied to model (2). \(\Box\)

**Exercise 3.9**: Show that \(Y^{\prime}(A-A_{0})^{\prime}V^{-1}(A-A_{0})Y\) equals the difference in the _SSE_s for models (3) and (1).

#### Conditions for Simpler Procedures

Just as Proposition 2.7.5 establishes that least squares estimates can be BLUEs even when \(\operatorname{Cov}(Y)\equiv\sigma^{2}V\neq\sigma^{2}I\), there exist conditions where \(\operatorname{Cov}(Y)\equiv\sigma^{2}V\neq\sigma^{2}I\) but under which the \(F\) statistic of Section 3.2 still has an \(F(r(M-M_{0}),r(I-M))\) distribution under the null model with multivariate normal errors. In particular, when testing \(Y=X\beta+e\) versus the reduced model \(Y=X_{0}\gamma+e\) where \(\operatorname{Cov}(Y)\equiv V=\sigma^{2}[I+X_{0}B^{\prime}+BX_{0}^{\prime}]\), the standard central \(F\) distribution continues to hold under the null hypothesis. In fact, the matrix \(B\) can even contain unknown parameters without affecting the validity of this result.

To obtain the result on \(F\) tests, one need only check that the usual numerator and denominator have the same independent \(\chi^{2}\) distributions under the null hypothesis as established in the proof of Theorem 3.2.1. This can be demonstrated by applying Theorems 1.3.6 and 1.3.8. In particular, \(Y^{\prime}(M-M_{0})Y/\sigma^{2}\sim\chi^{2}(r(M-M_{0}))\) because, with this \(V\),

\[\left[\frac{1}{\sigma^{2}}(M-M_{0})\right]V\left[\frac{1}{\sigma^{2}}(M-M_{0}) \right]=\frac{1}{\sigma^{2}}(M-M_{0}).\]

Similarly, \(Y^{\prime}(I-M)Y/\sigma^{2}\sim\chi^{2}(r(I-M))\) because

\[\left[\frac{1}{\sigma^{2}}(I-M)\right]V\left[\frac{1}{\sigma^{2}}(I-M)\right]= \frac{1}{\sigma^{2}}(I-M).\]

Finally, independence follows from the fact that

\[\left[\frac{1}{\sigma^{2}}(M-M_{0})\right]V\left[\frac{1}{\sigma^{2}}(I-M) \right]=0.\]

Moreover, the arguments given in Huynh and Feldt (1970) should generalize to establish that the \(F\) distribution holds only if \(V\) has the form indicated.

Of course, it is not clear whether \(\sigma^{2}[I+X_{0}B^{\prime}+BX_{0}^{\prime}]\) is positive definite, as good covariance matrices should be. However, \(X_{0}B^{\prime}+BX_{0}^{\prime}\) is symmetric, so it has real eigenvalues, and if the negative of its smallest eigenvalue is less than 1, \(I+X_{0}B^{\prime}+BX_{0}^{\prime}\) will be positive definite.

A special case of this covariance structure has \(X_{0}B^{\prime}+BX_{0}^{\prime}=X_{0}B_{0}X_{0}^{\prime}\) for some \(B_{0}\). In this special case, it is sufficient (but not necessary) to have \(B_{0}\) nonnegative definite. In fact, if a positive definite \(V\) has Rao's simple covariance structure from Exercise 2.6 relative to the reduced model, it also has the \(I+X_{0}B_{0}X_{0}^{\prime}\) structure. In this special case, not only do standard \(F\) tests apply, but least squares estimates are BLUEs in both models because Proposition 2.7.5 applies. Yet, in general with \(V=I+X_{0}B^{\prime}+BX_{0}^{\prime}\), it is possible to use the standard \(F\) tests even though least squares does not give BLUEs.

To illustrate the ideas, consider a balanced two-way ANOVA without interaction or replication, \(y_{ij}=\mu+\alpha_{i}+\eta_{j}+e_{ij}\), \(i=1,\ldots,a\), \(j=1,\ldots,b\). _In this context, we think about the \(\alpha_{i}\)s as block effects, so there are a blocks and \(b\) treatments_. We explore situations in which observations within each block are correlated, but the usual \(F\) test for treatment effects continues to apply. Write the linear model in matrix form as

\[Y=X\beta+e=[J_{ab},X_{\alpha},X_{\eta}]\begin{bmatrix}\mu\\ \alpha\\ \eta\end{bmatrix}+e.\]

Here \(\alpha=(\alpha_{1},\ldots,\alpha_{a})^{\prime}\) and \(\eta=(\eta_{1},\ldots,\eta_{b})^{\prime}\). The test of no treatment effects uses the reduced model

\[Y=X_{0}\gamma+e=[J_{ab},X_{\alpha}]\begin{bmatrix}\mu\\ \alpha\end{bmatrix}+e.\]

In the first illustration given below, \(V=I+X_{\alpha}B_{0*}X_{\alpha}^{\prime}\) for some \(B_{0*}\). In the second illustration, \(V=I+X_{\alpha}B_{*}^{\prime}+B_{*}X_{\alpha}^{\prime}\). In both cases it suffices to write \(V\) using \(X_{\alpha}\) rather than \(X_{0}\). This follows because \(C(X_{0})=C(X_{\alpha})\), so we can always write \(X_{0}B^{\prime}=X_{\alpha}B_{*}^{\prime}\).

One covariance structure that is commonly used involves _compound symmetry_, that is, independence between blocks, homoscedasticity, and constant correlation within blocks. In other words,

\[\text{Cov}(y_{ij},y_{i^{\prime}j^{\prime}})=\begin{cases}\sigma_{*}^{2}&\text{ if }i=i^{\prime},j=j^{\prime}\\ \sigma_{*}^{2}\rho&\text{ if }i=i^{\prime},j\neq j^{\prime}\\ 0&\text{ if }i\neq i^{\prime}.\end{cases}\]

One way to write this covariance matrix is as

\[\sigma^{2}V=\sigma_{*}^{2}(1-\rho)I+\sigma_{*}^{2}\rho X_{\alpha}X_{\alpha}^{ \prime}.\]

In the context, \(\sigma^{2}\) from the general theory is \(\sigma_{*}^{2}(1-\rho)\) from the example and \(B_{0*}\equiv[\rho/(1-\rho)]I_{a}\).

A more general covariance structure is

\[\text{Cov}(y_{ij},y_{i^{\prime}j^{\prime}})=\begin{cases}\sigma^{2}(1+2\delta_ {j})&\text{if }i=i^{\prime},j=j^{\prime}\\ \sigma^{2}(\delta_{j}+\delta_{j^{\prime}})&\text{if },i=i^{\prime},j\neq j^{\prime}\\ 0&\text{if }i\neq i^{\prime}.\end{cases}\]

We want to find \(B\) so that this covariance structure can be written as \(\sigma^{2}[I+X_{0}B^{\prime}+BX_{0}^{\prime}]\). It suffices to show that for some \(B_{*}\) we can write \(V=I+X_{\alpha}B_{*}^{\prime}+B_{*}X_{\alpha}^{\prime}\). In the balanced two-way ANOVA without interaction or replication, when \(Y=[y_{11},y_{12},\ldots,y_{ab}]^{\prime}\), \(X_{\alpha}\) can be written using Kronecker products as \[X_{\alpha}=[I_{a}\otimes J_{b}]=\begin{bmatrix}J_{b}&0&\cdots&0\\ 0&J_{b}&&\vdots\\ \vdots&&\ddots&\\ 0&\cdots&&J_{b}\end{bmatrix}.\]

Now define \(\delta=(\delta_{1},\ldots,\delta_{b})^{\prime}\) and take

\[B_{*}=[I_{a}\otimes\delta]=\begin{bmatrix}\delta&0&\cdots&0\\ 0&\delta&&\vdots\\ \vdots&&\ddots&\\ 0&\cdots&&\delta\end{bmatrix}.\]

With these choices, it is not difficult to see that the covariance structure specified earlier has

\[V=I_{ab}+[I_{a}\otimes J_{b}][I_{a}\otimes\delta]^{\prime}+[I_{a}\otimes\delta ][I_{a}\otimes J_{b}]^{\prime}.\]

This second illustration is similar to a discussion in Huynh and Feldt (1970). The split plot models of Chapter 11 involve covariance matrices with compound symmetry, so they are _similar_ in form to the first illustration that involved \(\sigma^{2}I+X_{0}B_{0}X_{0}^{\prime}.\) The results here establish that the \(F\) tests in the subplot analyses of Chapter 11 could still be obtained when using the more general covariance structures considered here.

### 3.9 Additional Exercises

##### Exercise 3.9.1

Consider the model \(y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+e_{i},e_{i}\)s i.i.d. \(N(0,\sigma^{2}).\) Use the data given below to answer (a) and (b).

\begin{tabular}{l|r r r r r} \hline Obs. & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline \(y\) & \(-2\) & 7 & 2 & 5 & 8 & \(-1\) \\ \(x_{1}\) & \(4\) & \(-1\) & 2 & 0 & \(-2\) & 3 \\ \(x_{2}\) & \(2\) & \(-3\) & 0 & \(-2\) & \(-4\) & 1 \\ \hline \end{tabular}

* Find \(SSR(X_{1},X_{2}|J)=R(\beta_{1},\beta_{2}|\beta_{0}).\)
* Are \(\beta_{0},\)\(\beta_{1},\) and \(\beta_{2}\) estimable?

##### Exercise 3.9.2

For a standard linear model, find the form of the generalized likelihood ratio test of \(H_{0}:\sigma^{2}=\sigma_{0}^{2}\) versus \(H_{A}:\sigma^{2}\neq\sigma_{0}^{2}\) in terms of rejectingwhen some function of \(SSE/\sigma_{0}^{2}\) is small. Show that the test makes sense in that it rejects for both large and small values of \(SSE/\sigma_{0}^{2}\).

#### Exercise 3.9.3

Consider a set of seemingly unrelated regression equations

\[Y_{i}=X_{i}\beta_{i}+e_{i},\quad e_{i}\sim N\big{(}0,\sigma^{2}I\big{)}\,,\]

\(i=1\),..., \(r\), where \(X_{i}\) is an \(n_{i}\times p\) matrix and the \(e_{i}\)s are independent. Find the test for \(H_{0}:\beta_{1}=\cdots=\beta_{r}\).

#### Exercise 3.9.4

(a) Using the notation of Section 3.3 and in particular of model (3.3.6), show that the least squares estimate of \(\beta\) subject to the constraint \(\Lambda^{\prime}\beta=d\) is \(\hat{\beta}=U\hat{\gamma}+b\).

(b) What happens to the test of \(\Lambda^{\prime}\beta=d\) if \(\Lambda^{\prime}\beta\) has no estimable part?

#### Exercise 3.9.5

Consider the model

\[Y=X\beta+e,\quad\text{E}(e)=0,\quad\text{Cov}(e)=\sigma^{2}I, \tag{1}\]

with the additional restriction

\[\Lambda^{\prime}\beta=d,\]

where \(d=\Lambda^{\prime}b\) for some (known) vector \(b\) and \(\Lambda^{\prime}=P^{\prime}X\). Model (1) with the additional restriction is equivalent to the model

\[(Y-Xb)=(M-M_{MP})\gamma+e. \tag{2}\]

If the parameterization of model (1) is particularly appropriate, then we might be interested in estimating \(X\beta\) subject to the restriction \(\Lambda^{\prime}\beta=d\). To do this, write

\[X\beta=\text{E}(Y)=(M-M_{MP})\gamma+Xb,\]

and define the BLUE of \(\lambda^{\prime}\beta=\rho^{\prime}X\beta\) in the restricted version of (1) to be \(\rho^{\prime}(M-M_{MP})\hat{\gamma}+\rho^{\prime}Xb\), where \(\rho^{\prime}(M-M_{MP})\hat{\gamma}\) is the BLUE of \(\rho^{\prime}(M-M_{MP})\gamma\) in model (2). Let \(\hat{\beta}_{1}\) be the least squares estimate of \(\beta\) in the unrestricted version of model (1). Show that the BLUE of \(\lambda^{\prime}\beta\) in the restricted version of model (1) is

\[\lambda^{\prime}\hat{\beta}_{1}-\Big{[}\text{Cov}(\lambda^{\prime}\hat{\beta} _{1},\,\Lambda^{\prime}\hat{\beta}_{1})\Big{]}\Big{[}\text{Cov}(\Lambda^{ \prime}\hat{\beta}_{1})\Big{]}^{-}\,(\Lambda^{\prime}\hat{\beta}_{1}-d), \tag{3}\]

where the covariance matrices are computed as in the unrestricted version of model (1).

Hint: This exercise is actually nothing more than simplifying the terms in (3) to show that it equals \(\rho^{\prime}(M-M_{MP})\hat{\gamma}+\rho^{\prime}Xb\).

Note: The result in (3) is closely related to best linear prediction, cf. Section 6.3.

**Exercise 3.9.6**: Discuss how the testing procedures from this chapter would change if you actually knew the variance \(\sigma^{2}\).

**Exercise 3.9.7**: Consider a linear model \(Y=X\beta+e\), \(\mathrm{E}(e)=0\) with the (not necessarily estimable) linear constraint \(\Lambda^{\prime}\beta=d\). Consider two solutions to the constraint, \(b_{1}\) and \(b_{2}\), so that \(\Lambda^{\prime}b_{k}=d\), \(k=1,2\). Define appropriate least squares fitted values \(\hat{Y}_{k}\) from the model \(Y=X_{0}\gamma+Xb_{k}+e\) where \(X_{0}=XU\) with \(C(U)=C(\Lambda)^{\perp}\). Show that \(\hat{Y}_{1}=\hat{Y}_{2}\). Hint: After finding \(\hat{Y}_{k}\), show that \((I-M_{0})X(b_{1}-b_{2})=0\).

**Exercise 3.9.8**: Under the conditions of Exercise 3.9.7, show that \((Y-Xb_{1})^{\prime}(M-M_{0})(Y-Xb_{1})=(Y-Xb_{2})^{\prime}(M-M_{0})(Y-Xb_{2})\).

## References

* Casella Berger (2002) Casella, G., & Berger, R. L. (2002). _Statistical inference_ (2nd ed.). Pacific Grove: Duxbury Press.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* Huynh & Feldt (1970) Huynh, H., & Feldt, L. S. (1970). Conditions under which mean square ratios in repeated measurements designs have exact \(F\)-distributions. _Journal of the American Statistical Association_, _65_, 1582-1589.
* Peixoto (1993) Peixoto, J. L. (1993). Four equivalent definitions of reparameterizations and restrictions in linear models. _Communications in Statistics_, \(A\), _22_, 283-299.

## Chapter 4 One-Way ANOVA

In this and the following chapters, we apply the general theory of linear models to various special cases. This chapter considers the analysis of one-way ANOVA models. A one-way ANOVA model can be written

\[y_{ij}=\mu+\alpha_{i}+e_{ij},\hskip 14.226378pti=1,\ldots,t,\hskip 14.226378ptj=1, \ldots,N_{i}, \tag{4.1}\]

where \(\text{E}(e_{ij})=0\), \(\text{Var}(e_{ij})=\sigma^{2}\), and \(\text{Cov}(e_{ij},e_{i^{\prime}j^{\prime}})=0\) when \((i,\,j)\neq(i^{\prime},\,j^{\prime})\). For finding tests and confidence intervals, the \(e_{ij}\)s are assumed to have a multivariate normal distribution. Here \(\alpha_{i}\) is an effect for \(y_{ij}\) belonging to the \(i\)th group of observations. Group effects are often called _treatment effects_ because one-way ANOVA models are used to analyze completely randomized experimental designs.

Section 1 is devoted primarily to deriving the ANOVA table for a one-way ANOVA. The ANOVA table in this case is a device for presenting the sums of squares necessary for testing the reduced model

\[y_{ij}=\mu+e_{ij},\hskip 14.226378pti=1,\ldots,t,\hskip 14.226378ptj=1, \ldots,N_{i}, \tag{4.2}\]

against model (1). This test is equivalent to testing the hypothesis \(H_{0}:\alpha_{1}=\cdots=\alpha_{t}\).

The main tool needed for deriving the analysis of model (1) is the perpendicular projection operator. The first part of Section 1 is devoted to finding \(M\). Since the \(y\)s in model (1) are identified with two subscripts, it will be necessary to develop notation that allows the rows of a vector to be denoted by two subscripts. Once \(M\) is found, some comments are made about estimation and the role of side conditions in estimation. Finally, the perpendicular projection operator for testing \(H_{0}:\alpha_{1}=\cdots=\alpha_{t}\) is found and the ANOVA table is presented. Section 2 is an examination of contrasts. First, contrasts are defined and discussed. Estimation and testing procedures are presented. Orthogonal contrasts are defined and applications of Sections 3.5 and 3.6 aregiven. Fortunately, many balanced multifactor analysis of variance problems can be analyzed by repeatedly using the analysis for a one-way analysis of variance. For that reason, the results of this chapter are particularly important.

### 4.1 Analysis of Variance

In linear model theory, the main tools we need are perpendicular projection matrices. Our first project in this section is finding the perpendicular projection matrix for a one-way ANOVA model. We will then discuss estimation, side conditions, and the ANOVA table.

Usually, the one-way ANOVA model is written

\[y_{ij}=\mu+\alpha_{i}+e_{ij},\ \ \ \ \ i=1,\ldots,t,\ \ \ j=1,\ldots,N_{i}.\]

Let \(n=\sum_{i=1}^{t}N_{i}\). Although the notation \(N_{i}\) is standard, we will sometimes use \(N(i)\) instead. Thus, \(N(i)\equiv N_{i}\). We proceed to find the perpendicular projection matrix \(M=X(X^{\prime}X)^{-}X^{\prime}\).

#### Example 4.1.1

In any particular example, the matrix manipulations necessary for finding \(M\) are simple. Suppose \(t=3\), \(N_{1}=5\), \(N_{2}=3\), \(N_{3}=3\). In matrix notation the model can be written

\[\begin{bmatrix}y_{11}\\ y_{12}\\ y_{13}\\ y_{14}\\ y_{15}\\ y_{21}\\ y_{22}\\ y_{23}\\ y_{31}\\ y_{32}\\ y_{33}\\ \end{bmatrix}=\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&1&0&0\\ 1&0&1&0\\ 1&0&1&0\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ 1&0&0&1\\ \end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\\ \end{bmatrix}+e.\]

To find the perpendicular projection matrix \(M\), first find

\[X^{\prime}X=\begin{bmatrix}11&5&3&3\\ 5&5&0&0\\ 3&0&3&0\\ 3&0&0&3\\ \end{bmatrix}.\]By checking that \((X^{\prime}X)(X^{\prime}X)^{-}(X^{\prime}X)=X^{\prime}X\), it is easy to verify that

\[(X^{\prime}X)^{-}=\begin{bmatrix}0&0&0&0\\ 0&1/5&0&0\\ 0&0&1/3&0\\ 0&0&0&1/3\end{bmatrix}.\]

Then

\[M = X(X^{\prime}X)^{-}X^{\prime}\] \[= X\left[\begin{array}{cccccccc}0&0&0&0&0&0&0&0&0&0&0\\ 1/5&1/5&1/5&1/5&1/5&0&0&0&0&0\\ 0&0&0&0&0&1/3&1/3&0&0&0\\ 0&0&0&0&0&0&0&0&1/3&1/3&1/3\end{array}\right]\] \[= \left[\begin{array}{cccccccc}1/5&1/5&1/5&1/5&1/5&0&0&0&0&0\\ 1/5&1/5&1/5&1/5&1/5&0&0&0&0&0\\ 1/5&1/5&1/5&1/5&0&0&0&0&0\\ 1/5&1/5&1/5&1/5&1/5&0&0&0&0&0\\ 0&0&0&0&1/3&1/3&1/3&0&0&0\\ 0&0&0&0&0&1/3&1/3&1/3&0&0\\ 0&0&0&0&0&1/3&1/3&1/3&0&0&0\\ 0&0&0&0&0&0&0&1/3&1/3&1/3\\ 0&0&0&0&0&0&0&0&1/3&1/3&1/3\\ 0&0&0&0&0&0&0&0&1/3&1/3&1/3\\ 0&0&0&0&0&0&0&0&1/3&1/3&1/3\end{array}\right].\]

Thus, in this example, \(M\) is Blk \(\operatorname{diag}[N_{i}^{-1}J_{N(i)}^{N(i)}]\), where \(J_{r}^{c}\) is a matrix of 1s with \(r\) rows and \(c\) columns. In fact, we will see below that this is the general form for \(M\) in a one-way ANOVA when the observation vector \(Y\) has subscripts changing fastest on the right.

A somewhat easier way of finding \(M\) is as follows. Let \(Z\) be the model matrix for the alternative one-way analysis of variance model

\[y_{ij}=\mu_{i}+e_{ij},\]

\(i=1,\ldots,t\), \(j=1,\ldots,N_{i}\). (See Example 3.1.1.) \(Z\) is then just a matrix consisting of the last \(t\) columns of \(X\), i.e., \(X=[J,Z]\). Clearly \(C(X)=C(Z)\), \(Z^{\prime}Z=\operatorname{Diag}(N_{1},\,N_{2},\,\ldots,N_{t})\), and \((Z^{\prime}Z)^{-1}=\operatorname{Diag}(N_{1}^{-1},\,N_{2}^{-1},\,\ldots,N_{t}^ {-1})\). It is easy to see that \(Z(Z^{\prime}Z)^{-}Z^{\prime}=\operatorname{Blk}\,\operatorname{diag}[N_{i}^{- 1}J_{N(i)}^{N(i)}]\).

In particular, write the observations in the \(i\)th group as

\[Y_{i}\equiv\begin{bmatrix}y_{i1}\\ \vdots\\ y_{iN(i)}\end{bmatrix}.\]

Now we can write the one-way ANOVA linear model as \[\begin{bmatrix}Y_{1}\\ Y_{2}\\ \vdots\\ Y_{t}\end{bmatrix}=\begin{bmatrix}J_{N(1)}&J_{N(1)}&0&\cdots&0\\ J_{N(2)}&0&J_{N(2)}&&0\\ \vdots&\vdots&&\ddots&\\ J_{N(t)}&0&0&&J_{N(t)}\end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \vdots\\ \alpha_{t}\end{bmatrix}+e\]

or

\[\begin{bmatrix}Y_{1}\\ Y_{2}\\ \vdots\\ Y_{t}\end{bmatrix}=\begin{bmatrix}J_{N(1)}&0&\cdots&0\\ 0&J_{N(2)}&&0\\ \vdots&&\ddots&\\ 0&0&&J_{N(t)}\end{bmatrix}\begin{bmatrix}\mu_{1}\\ \mu_{2}\\ \vdots\\ \mu_{t}\end{bmatrix}+e.\]

It is not difficult to see that

\[M=\begin{bmatrix}\frac{1}{N(1)}J_{N(1)}^{N(1)}&0&\cdots&0\\ 0&\frac{1}{N(2)}J_{N(2)}^{N(2)}&&0\\ \vdots&&\ddots&\\ 0&0&&\frac{1}{N(t)}J_{N(t)}^{N(t)}\end{bmatrix}.\]

It follows that \(X\hat{\beta}=MY\) reduces to, depending on the parameterization,

\[\begin{bmatrix}(\hat{\mu}+\hat{\alpha}_{1})J_{N(1)}\\ (\hat{\mu}+\hat{\alpha}_{2})J_{N(2)}\\ \vdots\\ (\hat{\mu}+\hat{\alpha}_{t})J_{N(t)}\end{bmatrix}=\begin{bmatrix}J_{N(1)}&J_{N( 1)}&0&\cdots&0\\ J_{N(2)}&0&J_{N(2)}&&0\\ \vdots&\vdots&&\ddots&\\ J_{N(t)}&0&0&&J_{N(t)}\end{bmatrix}\begin{bmatrix}\hat{\mu}\\ \alpha_{1}\\ \vdots\\ \alpha_{t}\end{bmatrix}\\ =\begin{bmatrix}\frac{1}{N(1)}J_{N(1)}^{N(1)}&0&\cdots&0\\ 0&\frac{1}{N(2)}J_{N(2)}N(2)&&0\\ \vdots&&\ddots&\\ 0&0&&\frac{1}{N(t)}J_{N(t)}^{N(t)}\end{bmatrix}\begin{bmatrix}Y_{1}\\ Y_{2}\\ \vdots\\ Y_{t}\end{bmatrix}=\begin{bmatrix}\bar{y}_{1}.&J_{N(1)}\\ \bar{y}_{2}.&J_{N(2)}\\ \vdots\\ \bar{y}_{t}.&J_{N(t)}\end{bmatrix}\]

or

\[\begin{bmatrix}\hat{\mu}_{1}J_{N(1)}\\ \hat{\mu}_{2}J_{N(2)}\\ \vdots\\ \hat{\mu}_{t}J_{N(t)}\end{bmatrix}=\begin{bmatrix}J_{N(1)}&0&\cdots&0\\ 0&J_{N(2)}&&0\\ \vdots&&\ddots&\\ 0&0&&J_{N(t)}\end{bmatrix}\begin{bmatrix}\hat{\mu}_{1}\\ \hat{\mu}_{2}\\ \vdots\\ \hat{\mu}_{t}\end{bmatrix}\\ =\begin{bmatrix}\frac{1}{N(1)}J_{N(1)}^{N(1)}&0&\cdots&0\\ 0&\frac{1}{N(2)}J_{N(2)}N(2)&&0\\ \vdots&&\ddots&\\ 0&0&&\frac{1}{N(t)}J_{N(t)}^{N(t)}\end{bmatrix}\begin{bmatrix}Y_{1}\\ Y_{2}\\ \vdots\\ Y_{t}\end{bmatrix}=\begin{bmatrix}\bar{y}_{1}.&J_{N(1)}\\ \bar{y}_{2}.&J_{N(2)}\\ \vdots\\ \bar{y}_{t}.&J_{N(t)}\end{bmatrix}.\]We now present a rigorous derivation of these results that does not require the \(Y\) vector to be written in any particular order. (This is important for our later examination of multifactor ANOVA.) The ideas involved in Example 4.1.1 are perfectly general. A similar computation can be performed for any values of \(t\) and the \(N_{i}\)s. The difficulty in a rigorous general presentation lies entirely in being able to write down the model in matrix form. The elements of \(Y\) are the \(y_{ij}\)s. The \(y_{ij}\)s have two subscripts, so a pair of subscripts must be used to specify each row of the vector \(Y\). The elements of the model matrices \(X\) and \(Z\) are determined entirely by knowing the order in which the \(y_{ij}\)s have been listed in the \(Y\) vector. For example, the row of \(Z\) corresponding to \(y_{ij}\) would have a 1 in the \(i\)th column and 0s everywhere else. Clearly, it will also be convenient to use a pair of subscripts to specify the rows of the model matrices.

If

\[Y^{\prime}=(y_{11},y_{12},\ldots,y_{1N(1)},y_{21},\ldots,y_{tN(t)}),\]

then \(y_{21}\) is the \(N_{1}+1\) row of \(Y\) and \(y_{ij}\) is the \((N_{1}+\cdots+N_{i-1}+j)\)th row of \(Y\). In conformance with \(Y\), write any other vector \(S\) as \(S=[s_{ij}]\), where it is understood that for \(j=1,\ldots,N_{i}\), \(s_{ij}\) denotes the \((N_{1}+\cdots+N_{i-1}+j)\)th row of \(S\). But the \(S=[s_{ij}]\) notation for a vector does not require a particular ordering for the entries of \(Y\). The vector \(Y=[y_{ij}]\) can have any ordering and other vectors \(S\) use the same ordering. The discussion of tensors in Appendix B may help the reader feel more comfortable with this use of subscripts.

To specify the model matrices \(X\) and \(Z\) we must identify the columns of \(X\) and \(Z\). Write \(X=[J,X_{1},X_{2},\ldots,X_{t}]\) and \(Z=[X_{1},X_{2},\ldots,X_{t}]\). Note that the \(k\)th column of \(Z\) can be written

\[X_{k}=[t_{ij}],\qquad\mbox{where $t_{ij}=\delta_{ik}$} \tag{1}\]

with \(\delta_{ik}\) equal to 0 if \(i\neq k\) and 1 if \(i=k\). This means that if the observation in the \(ij\) row belongs to the \(k\)th group, the \(ij\) row of \(X_{k}\) is 1. If not, the \(ij\) row of \(X_{k}\) is zero.

Our goal is to find \(M=Z(Z^{\prime}Z)^{-1}Z^{\prime}\). To do this we need to find \((Z^{\prime}Z)\) and \((Z^{\prime}Z)^{-1}\). Noting that \((X_{k})^{\prime}(X_{q})\) is a real number, we can write the elements of \(Z^{\prime}Z\) as

\[(Z^{\prime}Z)=\left[(X_{k})^{\prime}(X_{q})\right]_{t\times t}.\]

Now, from (1)

\[(X_{k})^{\prime}(X_{k})=\sum_{ij}\delta_{ik}\delta_{ik}=\sum_{i=1}^{t}\sum_{j =1}^{N_{i}}\delta_{ik}=\sum_{i=1}^{t}N_{i}\delta_{ik}=N_{k}\]

and for \(k\neq q\)

\[(X_{k})^{\prime}(X_{q})=\sum_{i=1}^{t}\sum_{j=1}^{N_{i}}\delta_{ik}\delta_{iq }=\sum_{i=1}^{t}N_{i}\delta_{ik}\delta_{iq}=0.\]

It follows that

\[(Z^{\prime}Z)=\mbox{Diag}(N_{i})\]and clearly

\[(Z^{\prime}Z)^{-1}=\text{Diag}(N_{i}^{-1}).\]

We can now find \(Z(Z^{\prime}Z)^{-1}\).

\[Z(Z^{\prime}Z)^{-1} =[X_{1},X_{2},\ldots,X_{t}]\text{Diag}(N_{i}^{-1})\] \[=\left[N_{1}^{-1}X_{1},N_{2}^{-1}X_{2},\ldots,N_{t}^{-1}X_{t} \right].\]

Finally, we are in a position to find \(M=Z(Z^{\prime}Z)^{-1}Z^{\prime}\). We denote the columns of an \(n\times n\) matrix using the convention introduced above for denoting rows, i.e., by using two subscripts. Then the matrix \(M\) can be written

\[M=[m_{ij,i^{\prime}j^{\prime}}].\]

We now find the entries of this matrix. Note that \(m_{ij,i^{\prime}j^{\prime}}\) is the \(ij\) row of \(Z(Z^{\prime}Z)^{-1}\) times the \(i^{\prime}j^{\prime}\) column of \(Z^{\prime}\) (i.e., the \(i^{\prime}j^{\prime}\) row of \(Z\)). The \(ij\) row of \(Z(Z^{\prime}Z)^{-1}\) is \((N_{1}^{-1}\delta_{i1},\ldots,N_{t}^{-1}\delta_{it})\). The \(i^{\prime}j^{\prime}\) row of \(Z\) is \((\delta_{i^{\prime}1},\ldots,\delta_{i^{\prime}t})\). The product is

\[m_{ij,i^{\prime}j^{\prime}} =\sum_{k=1}^{t}N_{k}^{-1}\delta_{ik}\delta_{i^{\prime}k}\] \[=N_{i}^{-1}\delta_{ii^{\prime}}.\]

If \(Y\) is listed in the usual order with the second subscript changing fastest, these values of \(m_{ij,i^{\prime}j^{\prime}}\) determine a block diagonal matrix

\[M=\text{Blk diag}(N_{i}^{-1}J_{N(i)}^{N(i)})\]

just as in Example 4.1.1, but these values apply to any ordering of the entries in \(Y\).

To reiterate, the notation and methods developed above are somewhat unusual, but they are necessary for giving a rigorous treatment of ANOVA models. The device of indicating the rows of vectors with multiple subscripts will be used extensively in later discussions of multifactor ANOVA. The arguments given above apply to any order of specifying the entries in a vector \(S=[s_{ij}]\); they do not depend on having \(S=(s_{11},s_{12},\ldots,s_{tN(t)})^{\prime}\). If we specified some other ordering, we would still get the perpendicular projection matrix \(M\); however, \(M\) might no longer be block diagonal.

**Exercise 4.1**: To develop some facility with this notation, let

\[T_{r}=[t_{ij}],\qquad\text{where }t_{ij}=\delta_{ir}-\frac{N_{r}}{n}\]

for \(r=1,\ldots,t\). Find \(T_{r}^{\prime}T_{r}\), \(T_{r}^{\prime}T_{s}\) for \(s\neq r\), and \(J^{\prime}T_{r}\).

A very important application of this notation is in characterizing the vector \(MY\). As discussed in Section 3.1, the vector \(MY\) is the base from which all estimates of parametric functions are found. A second important application involves the projection operator

\[M_{\alpha}=M-\frac{1}{n}J_{n}^{n}\,.\]

\(M_{\alpha}\) is useful in testing hypotheses and is especially important in the analysis of multifactor ANOVAs. It is therefore necessary to have a characterization of \(M_{\alpha}\).

**Exercise 4.2**: Show that

\[MY=[t_{ij}],\qquad\text{where }t_{ij}=\bar{y}_{i}.\]

and

\[M_{\alpha}Y=[u_{ij}],\qquad\text{where }u_{ij}=\bar{y}_{i}.-\bar{y}_{..}\,.\]

Hint: Write \(M_{\alpha}Y=MY-(\frac{1}{n}J_{n}^{n})Y\).

These characterizations \(MY\) and \(M_{\alpha}Y\) tell us how to find \(Mv\) and \(M_{\alpha}v\) for any vector \(v\). In fact, they completely characterize the perpendicular projection operators \(M\) and \(M_{\alpha}\).

_Example 4.1.1 Continued._ In this example,

\[MY=(\bar{y}_{1}.,\,\bar{y}_{1}.,\,\bar{y}_{1}.,\,\bar{y}_{1}.,\,\bar{y}_{2}., \,\bar{y}_{2}.,\,\bar{y}_{2}.,\,\bar{y}_{3}.,\,\bar{y}_{3}.,\,\bar{y}_{3}.)^{\prime}\]

and

\[M_{\alpha}Y=(\bar{y}_{1}.-\bar{y}.,\,\bar{y}_{1}.-\bar{y}.,\bar{y }_{1}.-\bar{y}.,\,\bar{y}_{1}.-\bar{y}.,\,\bar{y}_{1}.-\bar{y}.,\\ \bar{y}_{2}.-\bar{y}.,\,\bar{y}_{2}.-\bar{y}.,\,\bar{y}_{2}.-\bar {y}.,\,\bar{y}_{3}.-\bar{y}.,\,\bar{y}_{3}.-\bar{y}.,\,\bar{y}_{3}.-\bar{y}.)^{ \prime}.\]

We can now obtain a variety of estimates. Recall that estimable functions are linear combinations of the rows of \(X\beta\), e.g., \(\rho^{\prime}X\beta\). Since

\[X\beta=\big{(}\mu+\alpha_{1},\,\mu+\alpha_{1},\,\mu+\alpha_{1}, \,\mu+\alpha_{1},\\ \mu+\alpha_{2},\,\mu+\alpha_{2},\,\mu+\alpha_{2},\,\mu+\alpha_{3},\,\mu+\alpha_{3},\,\mu+\alpha_{3}\big{)}^{\prime},\]

if \(\rho^{\prime}\) is taken to be \(\rho^{\prime}=(1,\,0,\,0,\,0,\,0,\,0,\,0,\,0,\,0,\,0,\,0)^{\prime}\), then it is easily seen that \(\mu+\alpha_{1}=\rho^{\prime}X\beta\) is estimable. The estimate of \(\mu+\alpha_{1}\) is \(\rho^{\prime}MY=\bar{y}_{1}..\) Similarly, the estimates of \(\mu+\alpha_{2}\) and \(\mu+\alpha_{3}\) are \(\bar{y}_{2}.\) and \(\bar{y}_{3}.\), respectively. The contrast \(\alpha_{1}-\alpha_{2}\) can be obtained as \(\rho^{\prime}X\beta\) using \(\rho^{\prime}=(1,\,0,\,0,\,0,\,-1,\,0,\,0,\,0,\,0,\,0)^{\prime}\). The estimate of \(\alpha_{1}-\alpha_{2}\) is \(\rho^{\prime}MY=\bar{y}_{1}.-\bar{y}_{2}.\) Note that for this contrast \(\rho^{\prime}MY=\rho^{\prime}M_{\alpha}Y\).

Estimation is as easy in a general one-way ANOVA as it is in Example 4.1.1. We have found \(M\) and \(MY\), and it is an easy matter to see that, for instance, \(\mu+\alpha_{i}\) is estimable and the estimate of \(\mu+\alpha_{i}\) is

\[\{\hat{\mu}+\hat{\alpha}_{i}\}=\bar{y}_{i}.\,.\]

The notation \(\{\hat{\mu}+\hat{\alpha}_{i}\}\) will be used throughout this chapter to denote the estimate of \(\mu+\alpha_{i}\).

For computational purposes, it is often convenient to present one particular set of least squares estimates. In one-way ANOVA, the traditional _side condition_ on the parameters is \(\sum_{i=1}^{t}N_{i}\alpha_{i}=0\). With this condition, one obtains

\[\mu=\frac{1}{n}\sum_{i=1}^{t}N_{i}(\mu+\alpha_{i})\]

and an estimate

\[\hat{\mu}=\frac{1}{n}\sum_{i=1}^{t}N_{i}\{\hat{\mu}+\hat{\alpha}_{i}\}=\frac{1 }{n}\sum_{i=1}^{t}\frac{N_{i}}{N_{i}}\sum_{j=1}^{N_{i}}y_{ij}=\bar{y}_{..},\]

which is the mean of all the observations. Similarly,

\[\hat{\alpha}_{i}=\{\hat{\mu}+\hat{\alpha}_{i}\}-\hat{\mu}=\bar{y}_{i}.-\bar{y} _{..}.\]

The traditional side condition is no longer the most often used. Exercise 4.9 involves finding parameter estimates that satisfy a different commonly used side condition. Fortunately, all side conditions lead to the same estimates of identifiable functions, so one choice of a side condition is as good as any other. The best choice of a side condition is the most convenient choice. However, different side conditions do lead to different "estimates" of nonidentifiable parameters. Do not be lulled into believing that an arbitrary side condition allows you to say anything meaningful about nonidentifiable parameters. That is just silly!

We now derive the analysis of variance table for the one-way ANOVA. The analysis of variance table is a device for displaying an orthogonal breakdown of the _total sum of squares_ of the data (_SSTot_), i.e., \(Y^{\prime}Y\). More often, the total sum of squares corrected for fitting the _grand mean_ is broken down. The _sum of squares for fitting the grand mean_ (_SSGM_) is just the sum of squares accounted for by the model

\[Y=J\mu+e \tag{2}\]

This is also known as the _correction factor_\(C\) so

\[Y^{\prime}[1/n]J_{n}^{n}Y=SSGM=C=SSR(J)=R(\mu).\]The _total sum of squares corrected for the grand mean_ (\(SSTot-C\)) is the error sum of squares in model (2), i.e., \(Y^{\prime}\left(I-[1/n]J_{n}^{n}\right)Y=Y^{\prime}Y-C\). Included in an ANOVA table is information to identify the sums of squares (Source), the degrees of freedom for the sums of squares (\(df\)), the sums of squares (\(SS\)), and the mean squares (\(MS\)). The mean squares are just the sums of squares divided by their degrees of freedom. Sometimes the expected values of the mean squares are included. From the expected mean squares, the hypotheses tested by the various sums of squares can be identified. Recall that, when divided by \(\sigma^{2}\), the sums of squares have \(\chi^{2}\) distributions and that there is a very close relationship between the expected mean square, the expected sum of squares, the noncentrality parameter of the \(\chi^{2}\) distribution, and the noncentrality parameter of an \(F\) distribution with the mean square in the numerator. In particular, if the expected mean square is \(\sigma^{2}+\pi/df\), then the noncentrality parameter is \(\pi/2\sigma^{2}\). Assuming the full model is true, the null hypothesis being tested is that the noncentrality parameter of the \(F\) distribution is zero.

The usual orthogonal breakdown for a one-way ANOVA is to isolate the effect of the grand mean (\(\mu\)), and then the effect of fitting the groups (\(\alpha_{i}s\)) after fitting the mean. The _sum of squares for groups_ (\(SSGrps\)) is just what is left after removing the sum of squares for \(\mu\) from the sum of squares for the model. In other words, the sum of squares for groups is the sum of squares for testing the reduced model (4.0.2) against model (4.0.1). As we have seen earlier, the projection operator for fitting the grand mean is based on the first column of \(X\), i.e., \(J\). The projection operator is \((1/n)J_{n}^{n}=(1/n)JJ^{\prime}\). The projection operator for the groups sum of squares is then

\[M_{\alpha}=M-\frac{1}{n}J_{n}^{n}.\]

The sum of squares for fitting groups after \(\mu\), \(Y^{\prime}M_{\alpha}Y\), is the difference between the sum of squares for fitting the full model, \(Y^{\prime}MY\), and the sum of squares for fitting the model with just the mean, \(Y^{\prime}\left([1/n]J_{n}^{n}\right)Y\).

Table 4.1 gives an ANOVA table and indicates some common notation for the entries.

**Exercise 4.3** : Verify that the estimate of \(\mu+\alpha_{i}\) is \(\bar{y}_{i}\). and that the algebraic formulas for the sums of squares in the ANOVA table are correct.

Hint: To find, for example, \(Y\left(M-[1/n]J_{n}^{n}\right)Y=Y^{\prime}M_{\alpha}Y\), use Exercise 4.2 to get \(M_{\alpha}Y\) and recall that \(Y^{\prime}M_{\alpha}Y=[M_{\alpha}Y]^{\prime}\left[M_{\alpha}Y\right]\).

**Exercise 4.4** : Verify that the formulas for expected mean squares in the ANOVA table are correct. Hint: Use Theorem 1.3.2 and Exercise 4.3.

The techniques suggested for Exercises 4.3 and 4.4 are very useful. The reader should make a point of remembering them.

### Estimating and Testing Contrasts

In this section, contrasts are defined and characterizations of contrasts are given. Estimates of contrasts are found. The numerator sum of squares necessary for doing an \(F\) test of a contrast and the form of the \(F\) test are given. The form of a confidence interval for a contrast is presented and the idea of orthogonal contrasts is discussed. Finally, the results of Section 3.5 are reviewed by deriving them anew for the one-way ANOVA model.

A contrast in the one-way ANOVA (4.0.1) is a function \(\sum_{i=1}^{t}\lambda_{i}\alpha_{i}\), with \(\sum_{i=1}^{t}\lambda_{i}=0\). In other words, the vector \(\lambda^{\prime}\) in \(\lambda^{\prime}\beta\) is \((0,\lambda_{1},\lambda_{2},\ldots,\lambda_{t})\) and \(\lambda^{\prime}J_{t+1}=0\). To establish that \(\lambda^{\prime}\beta\) is estimable, we need to find \(\rho\) such that \(\rho^{\prime}X=\lambda^{\prime}\). Write

\begin{table}
\begin{tabular}{l c c} \hline  & \multicolumn{2}{c}{Matrix notation} \\ \hline Source & \(df\) & \(SS\) \\ \hline Grand mean & 1 & \(Y^{\prime}\left(\frac{1}{n}J_{n}^{n}\right)Y\) \\ Groups & \(t-1\) & \(Y^{\prime}\left(M-\frac{1}{n}J_{n}^{n}\right)Y\) \\ Error & \(n-t\) & \(Y^{\prime}(I-M)Y\) \\ \hline Total & \(n\) & \(Y^{\prime}Y\) \\ Source & \(SS\) & \(\mathrm{E}(MS)\) \\ \hline Grand mean & \(SSGM\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}\left(\frac{1}{n}J_{n}^{n}\right)X\beta\) \\ Groups & \(SSGrps\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}\left(M-\frac{1}{n}J_{n}^{n}\right)X\beta/ (t-1)\) \\ Error & \(SSE\) & \(\sigma^{2}\) \\ \hline Total & \(SSTot\) \\ \hline \end{tabular}
\end{table}
Table 4.1: One-way analysis of variance table\[\rho^{\prime} = (\lambda_{1}/N_{1},\ldots,\lambda_{1}/N_{1},\lambda_{2}/N_{2},\ldots, \lambda_{2}/N_{2},\lambda_{3}/N_{3},\ldots,\lambda_{t}/N_{t})\] \[= [(\lambda_{1}/N_{1})J^{\prime}_{N(1)},(\lambda_{2}/N_{2})J^{\prime }_{N(2)},\ldots,(\lambda_{t}/N_{t})J^{\prime}_{N(t)}]\]

where \(\rho^{\prime}\) is a \(1\times n\) vector and the string of \(\lambda_{i}/N_{i}\)s is \(N_{i}\) long. In the alternate notation that uses two subscripts to denote a row of a vector, we have

\[\rho=[t_{ij}],\qquad\mbox{where }t_{ij}=\lambda_{i}/N_{i}. \tag{1}\]

Recall from Section 2.1 that, while other choices of \(\rho\) may exist with \(\rho^{\prime}X=\lambda^{\prime}\), the vector \(M\rho\) is unique. As shown in Exercise 4.10, for \(\rho\) as in (1), \(\rho\in C(X)\); so \(\rho=M\rho\). Thus, for any contrast \(\lambda^{\prime}\beta\), the vector \(M\rho\) has the structure

\[M\rho=[t_{ij}],\qquad\mbox{where }t_{ij}=\lambda_{i}/N_{i}. \tag{2}\]

We now show that the contrasts are precisely the estimable functions that do not involve \(\mu\). Note that since \(J\) is the column of \(X\) associated with \(\mu\), \(\rho^{\prime}X\beta\) does not involve \(\mu\) if and only if \(\rho^{\prime}J=0\).

**Proposition 4.2.1**: \(\rho^{\prime}X\beta\) _is a contrast if and only if \(\rho^{\prime}J=0\)._

_Proof_ Clearly, a contrast does not involve \(\mu\), so \(\rho^{\prime}J=0\). Conversely, if \(\rho^{\prime}J=0\), then \(\rho^{\prime}X\beta=\rho^{\prime}[J,\,Z]\beta\) does not involve \(\mu\); so we need only show that \(0=\rho^{\prime}XJ_{t+1}\). This follows because \(XJ_{t+1}=2J_{n}\), and we know that \(\rho^{\prime}J_{n}=0\). \(\square\)

We now show that the contrasts are the estimable functions that impose constraints on \(C(M_{\alpha})\). Recall that the constraint imposed on \(C(X)\) by \(\rho^{\prime}X\beta=0\) is that \(\mbox{E}(Y)\in C(X)\) and \(\mbox{E}(Y)\perp M\rho\), i.e., \(\mbox{E}(Y)\) is constrained to be orthogonal to \(M\rho\). By definition, \(\rho^{\prime}X\beta\) puts a constraint on \(C(M_{\alpha})\) if \(M\rho\in C(M_{\alpha})\).

**Proposition 4.2.2**: \(\rho^{\prime}X\beta\) _is a contrast if and only if \(M\rho\in C(M_{\alpha})\)._

_Proof_ Using Proposition 4.2.1 and \(J\in C(X)\), we see that \(\rho^{\prime}X\beta\) is a contrast if and only if \(0=\rho^{\prime}J=\rho^{\prime}MJ\), i.e., \(J\perp M\rho\). However, \(C(M_{\alpha})\) is everything in \(C(X)\) that is orthogonal to \(J\); thus \(J\perp M\rho\) if and only if \(M\rho\in C(M_{\alpha})\). \(\square\)

Finally, we can characterize \(C(M_{\alpha})\).

**Proposition 4.2.3**: \(C(M_{\alpha})=\Big{\{}\rho\Big{|}\rho=[t_{ij}],\ t_{ij}=\lambda_{i}/N_{i},\ \sum_{i=1}^{t}\lambda_{i}=0\Big{\}}.\)__

_Proof_ Any vector \(\rho\) with the structure of (1) and \(\sum_{i}\lambda_{i}=0\) has \(\rho^{\prime}J=0\) and by Proposition 4.2.1 determines a contrast \(\rho^{\prime}X\beta\). By Proposition 4.2.2, \(M\rho\in C(M_{\alpha})\). However, vectors that satisfy (1) also satisfy \(M\rho=\rho\), so \(\rho\in C(M_{\alpha})\). Conversely, if \(\rho\in C(M_{\alpha})\), then \(\rho^{\prime}J=0\); so \(\rho^{\prime}X\beta\) determines a contrast. It follows that \(M\rho\) must be of the form (2), where \(\lambda_{1}+\cdots+\lambda_{t}=0\). However, since \(\rho\in C(M_{\alpha})\), \(M\rho=\rho\); so \(\rho\) must be of the form (1) with \(\lambda_{1}+\cdots+\lambda_{t}=0\).

**Exercise 4.5**  Show that \(\alpha_{1}=\alpha_{2}=\cdots=\alpha_{t}\) if and only if all contrasts are zero.

We now consider estimation and testing for contrasts. The least squares estimate of a contrast \(\sum_{i=1}^{t}\lambda_{i}\alpha_{i}\) is easily obtained. Let \(\hat{\mu}\), \(\hat{\alpha}_{1}\),..., \(\hat{\alpha}_{t}\) be any choice of least squares estimates for the nonidentifiable parameters \(\mu,\alpha_{1}\),..., \(\alpha_{t}\). Since \(\sum_{i=1}^{t}\lambda_{i}=0\), we can write

\[\sum_{i=1}^{t}\lambda_{i}\hat{\alpha}_{i}=\sum_{i=1}^{t}\lambda_{i}\{\hat{\mu} +\hat{\alpha}_{i}\}=\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}.,\]

because \(\mu+\alpha_{i}\) is estimable and its unique least squares estimate is \(\bar{y}_{i}.\). This result can also be seen by examining \(\rho^{\prime}Y=\rho^{\prime}MY\) for the \(\rho\) given earlier in (1). To test the hypothesis that \(\lambda^{\prime}\beta=0\), we have seen that the numerator of the \(F\) test statistic is \((\rho^{\prime}MY)^{2}/\rho^{\prime}M\rho\). However, \(\rho^{\prime}MY=\rho^{\prime}X\hat{\beta}=\lambda^{\prime}\hat{\beta}=\sum_{ i=1}^{t}\lambda_{i}\bar{y}_{i}.\) We also need to find \(\rho^{\prime}M\rho\). The easiest way is to observe that, since \(M\rho\) has the structure of (2),

\[\rho^{\prime}M\rho =[M\rho]^{\prime}[M\rho]\] \[=\sum_{i=1}^{t}\sum_{j=1}^{N_{i}}\lambda_{i}^{2}/N_{i}^{2}\] \[=\sum_{i=1}^{t}\lambda_{i}^{2}/N_{i}.\]

The numerator sum of squares for testing the contrast is

\[SS(\lambda^{\prime}\beta)\equiv\left(\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}. \right)^{2}\Big{/}\left(\sum_{i=1}^{t}\lambda_{i}^{2}/N_{i}\right).\]

The \(\alpha\) level test for \(H_{0}:\sum_{i=1}^{t}\lambda_{i}\alpha_{i}=0\) is to reject \(H_{0}\) if

\[\frac{\left(\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}.\right)^{2}\Big{/}\left(\sum_ {i=1}^{t}\lambda_{i}^{2}/N_{i}\right)}{MSE}>F(1-\alpha,1,dfE).\]

Equivalently, \(\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}\). has a normal distribution, \(\mathrm{E}\!\left(\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}.\right)=\sum_{i=1}^{t} \lambda_{i}(\mu+\alpha_{i})=\sum_{i=1}^{t}\lambda_{i}\alpha_{i}\), and \(\mathrm{Var}\!\left(\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}.\right)=\sum_{i=1}^{ t}\lambda_{i}^{2}\mathrm{Var}(\bar{y}_{i}.)=\sigma^{2}\sum_{i=1}^{t}\lambda_{i}^{2} /N_{i}\), so we have a \(t\) test available. The \(\alpha\) level test is to reject \(H_{0}\) if

\[\frac{\left|\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}.\right|}{\sqrt{MSE\sum_{i=1}^ {t}\lambda_{i}^{2}/N_{i}}}>t\Big{(}1-\frac{\alpha}{2},dfE\Big{)}\,.\]

Note that since \(\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i}.=\rho^{\prime}MY\) is a function of \(MY\) and \(MSE\) is a function of \(Y^{\prime}(I-M)Y\), we have the necessary independence for the \(t\) test. In fact, all tests and confidence intervals follow as in Exercise 2.1.

In order to break up the sums of squares for groups into \(t-1\) orthogonal single degree of freedom sums of squares, we need to find \(t-1\) contrasts \(\lambda_{1}^{\prime}\beta\), \(\ldots,\lambda_{t-1}^{\prime}\beta\) with the property that \(\rho_{r}^{\prime}M\rho_{s}=0\) for \(r\neq s\), where \(\rho_{r}^{\prime}X=\lambda_{r}^{\prime}\) (see Section 3.6). Let \(\lambda_{r}^{\prime}=(0,\lambda_{r1},\ldots,\lambda_{rt})\) and recall that \(M\rho_{r}\) has the structure of (2). The condition required is

\[0 = \rho_{r}^{\prime}M\rho_{s}\] \[= [M\rho_{r}]^{\prime}[M\rho_{s}]\] \[= \sum_{i=1}^{t}\sum_{j=1}^{N_{i}}(\lambda_{ri}/N_{i})(\lambda_{ si}/N_{i})\] \[= \sum_{i=1}^{t}\lambda_{ri}\lambda_{si}/N_{i}.\]

With any set of contrasts \(\sum_{i=1}^{t}\lambda_{ri}\alpha_{i}\), \(r=1,\ldots,t-1\), for which \(0=\sum_{i=1}^{t}\lambda_{ri}\lambda_{si}/N_{i}\) for all \(r\neq s\), we have a set of \(t-1\) orthogonal constraints on the test space so that the sums of squares for the contrasts add up to the _SSGrps_. Contrasts that determine orthogonal constraints are referred to as _orthogonal contrasts_.

In later analyses, we will need to use the fact that the analysis developed here depends only on the projection matrix onto the space for testing \(\alpha_{1}=\alpha_{2}=\cdots=\alpha_{t}\). That projection matrix is \(M_{\alpha}=M-(1/n)J_{n}^{n}\). Note that \(M=(1/n)J_{n}^{n}+M_{\alpha}\). For any contrast \(\lambda^{\prime}\beta\) with \(\rho^{\prime}X=\lambda^{\prime}\), we know that \(\rho^{\prime}J_{n}=0\). It follows that \(\rho^{\prime}M=\rho^{\prime}(1/n)J_{n}^{n}+\rho^{\prime}M_{\alpha}=\rho^{ \prime}M_{\alpha}\). There are two main uses for this fact. First,

\[\sum_{i=1}^{t}\lambda_{i}\hat{\alpha}_{i}=\sum_{i=1}^{t}\lambda_{i}\bar{y}_{i.}=\rho^{\prime}MY=\rho^{\prime}M_{\alpha}Y,\]

\[\sum_{i=1}^{t}\lambda_{i}^{2}/N_{i}=\lambda^{\prime}(X^{\prime}X)^{-}\lambda= \rho^{\prime}M\rho=\rho^{\prime}M_{\alpha}\rho,\]

so estimation, and therefore tests, depend only on the projection \(M_{\alpha}\). Second, the condition for contrasts to give an orthogonal breakdown of _SSGrps_ is

\[0=\sum_{i=1}^{t}\lambda_{ri}\lambda_{si}/N_{i}=\rho_{r}^{\prime}M\rho_{s}= \rho_{r}^{\prime}M_{\alpha}\rho_{s},\]

which depends only on \(M_{\alpha}\). This is just a specific example of the theory of Section 3.5.

**Exercise 4.6**: Using the theories of Sections 3.3 and 2.6, respectively, find the \(F\) test and the \(t\) test for the hypothesis \(H_{0}:\sum_{i=1}^{t}\lambda_{i}\alpha_{i}=d\) in terms of the _MSE_, the \(\bar{y}_{i}.\)s, and the \(\lambda_{i}\)s.

**Exercise 4.7**: Suppose \(N_{1}=N_{2}=\cdots=N_{t}\equiv N\). Rewrite the ANOVA table incorporating any simplifications due to this assumption.

**Exercise 4.8**: If \(N_{1}=N_{2}=\cdots=N_{t}\equiv N\), show that two contrasts \(\lambda_{1}^{\prime}\beta\) and \(\lambda_{2}^{\prime}\beta\) are orthogonal if and only if \(\lambda_{1}^{\prime}\lambda_{2}=0\).

**Exercise 4.9**: Find the least squares estimates of \(\mu\), \(\alpha_{1}\), and \(\alpha_{t}\) using the side condition \(\alpha_{1}=0\). (The other commonly used side condition is \(\alpha_{t}=0\).)

**Exercise 4.10**: Using \(\rho\) as defined by (1) and \(X\) as defined in Section 1, especially (4.1.1), show that

(a) \(\rho^{\prime}X=\lambda^{\prime}\), where \(\lambda^{\prime}=(0,\lambda_{1},\ldots,\lambda_{t})\).

(b) \(\rho\in C(X)\).

(c) For any scalar \(k\), the test of \(k\lambda^{\prime}\beta=0\) is the same as the test for \(\lambda^{\prime}\beta=0\).

### Additional Exercises

**Exercise 4.3.1**: An experiment was conducted to see which of four brands of blue jeans were most resistant to wearing out as a result of students kneeling before their linear models instructor begging for additional test points. In a class of 32 students, 8 students were randomly assigned to each brand of jeans. Before being informed of their test score, each student was required to fall to his/her knees and crawl 3 m to the instructor's desk. This was done after each of 5 mid-quarter and 3 final exams. (The jeans were distributed along with each of the 8 test forms and were collected again 36 h after grades were posted.) A fabric wear score was determined for each pair of jeans. The scores are listed below.

(a): Give an ANOVA table for these data, and perform and interpret the \(F\) test for the differences between brands.

(b): Brands 2 and 3 were relatively inexpensive, while Brands 1 and 4 were very costly. Based on these facts, determine an appropriate set of orthogonal contrasts to consider in this problem. Find the sums of squares for the contrasts.

(c): What conclusions can be drawn from these data? Perform any additional computations that may be necessary

**Exercise 4.3.2** After the final exam of spring quarter, 30 of the subjects of the previous experiment decided to test the sturdiness of 3 brands of sport coats and 2 brands of shirts. In this study, sturdiness was measured as the length of time before tearing when the instructor was hung by his collar out of his second-story office window. Each brand was randomly assigned to 6 students, but the instructor was occasionally dropped before his collar tore, resulting in some missing data. The data are listed below.

\begin{tabular}{l l l l l l l} Coat 1: & 2.34 & 2.46 & 2.83 & 2.04 & 2.69 & \\ Coat 2: & 2.64 & 3.00 & 3.19 & 3.83 & & & \\ Coat 3: & 2.61 & 2.07 & 2.80 & 2.58 & 2.98 & 2.30 \\ Shirt 1: & 1.32 & 1.62 & 1.92 & 0.88 & 1.50 & 1.30 \\ Shirt 2: & 0.41 & 0.83 & 0.53 & 0.32 & 1.62 & \\ \end{tabular}

(a)  Give an ANOVA table for these data, and perform and interpret the \(F\) test for the differences between brands.

(b)  Test whether, on average, these brands of coats are sturdier than these brands of shirts.

(c)  Give three contrasts that are mutually orthogonal and orthogonal to the contrast used in (b). Compute the sums of squares for all four contrasts.

(d)  Give a 95% confidence interval for the difference in sturdiness between shirt Brands 1 and 2. Is one brand significantly sturdier than the other?

## Chapter 5 Multiple Comparison Techniques

In analyzing a linear model we can examine as many single degree of freedom hypotheses as we want. If we test all of these hypotheses at, say, the 0.05 level, then the (weak) _experimentwise error rate_ (the probability of rejecting at least one of these hypotheses when all are true) will be greater than 0.05. Multiple comparison techniques are methods of performing the tests so that if all the hypotheses are true, then the probability of rejecting any of the hypotheses is no greater than some specified value, i.e., the experimentwise error rate is controlled.

A multiple comparison method can be said to be more powerful than a competing method if both methods control the experimentwise error rate at the same level, but the method in question rejects hypotheses more often than its competitor. Being more powerful, in this sense, is a mixed blessing. If one admits the idea that a null hypothesis really can be true (an idea that I am often loath to admit), then the purpose of a multiple comparison procedure is to identify which hypotheses are true and which are false. The more powerful of two multiple comparison procedures will be more likely to correctly identify hypotheses that are false as being false. It will also be more likely to incorrectly identify hypotheses that are true as being false.

A related issue is that of examining the data before deciding on the hypotheses. If the data have been examined, an hypothesis may be chosen to test because it looks as if it is likely to be significant. The nominal significance level of such a test is invalid. In fact, when doing multiple tests by any standard method, nearly all the nominal significance levels are invalid. For some methods, however, selecting the hypotheses after examining the data make the error levels intolerably bad.

The sections of this chapter contain discussions of individual multiple comparison methods. Section 1 provides a more formal background to the ideas just introduced. The specific methods discussed are Scheffe's method, the Least Significant Difference (LSD) method, the Bonferroni method, Tukey's Honest Significant Difference(HSD) method, and multiple range tests. The section on multiple range tests examines both the Newman-Keuls method and Duncan's method. The final section of the chapter compares the various methods. For a more complete discussion of multiple comparison methods, see Miller (1981), Hochberg and Tamhane (1987), Hsu (1996), or Bretz, Hothorn, and Westfall (2011). Christensen (1996) discusses the methods of this chapter at a more applied level, discusses Dunnett's method for comparing treatments with a control, and discusses Ott's analysis of means method.

### Basic Ideas

The Neyman-Pearson theory of hypothesis testing is based on the idea of controlling the probability of Type I error, i.e., the probability of rejecting a null hypothesis when it is true. If you have multiple hypotheses to test, you have multiple chances for Type I error, so one might be interested in controlling the probability of rejecting any of the null hypotheses when they are true. Multiple testing methods seek to do this. A multiple testing method declares each hypothesis as accepted or rejected but it seeks to place some control on the overall rate of making Type I errors.

Probability of Type I error is an hypothesis testing (Neyman-Pearson) concept, not a significance testing (Fisherian) concept, so we feel free to discuss accepting null hypotheses. In this book the term _multiple comparisons_ is used synonymously with multiple testing, although the former term is more properly restricted to analysis of variance problems.

Multiple testing is a complicated problem. Suppose we are interested in a collection of single degree of freedom null hypotheses \(H_{0j}:\lambda_{j}^{\prime}\beta=0\), \(j=1,\ldots,s\). Obviously, we would like to know which hypotheses are true and which are false. There are \(2^{s}\) possibilities as to which are true and false. For each of those possibilities, we can draw either the correct conclusion or an incorrect conclusion. Thus there are \(4^{s}\) different conditions a set of tests for these hypotheses can take. Ideally, for each \(j\) we would like to know the probability of rejecting when \(H_{0j}\) is true and of accepting when \(H_{0j}\) is false. But we would also like to know what the collective probability of rejecting any of the true null hypotheses is, as well as knowing the collective probability of accepting any of the false hypotheses. Alas, that would require us to know which ones are true and which ones are not true. If we knew that, we would not be testing.

Rather than trying to deal with all of the possibilities, we focus attention on one specific aspect of this problem. Assuming that all of \(H_{0j}:\lambda_{j}^{\prime}\beta=0\), \(j=1,\ldots,s\) are true, we look at methods for which the probability of rejecting any of them will be less than or equal to a specified number \(\alpha\). This is often referred to as controlling the _(weak) experimentwise error rate_ at \(\alpha\).

Define \(\Lambda=[\lambda_{1},\ldots,\lambda_{s}]\). Clearly, \(\Lambda^{\prime}\beta=0\) if and only if \(\lambda_{j}^{\prime}\beta=0\), \(j=1,\ldots,s\). Moreover, both of these conditions are equivalent to \(\lambda^{\prime}\beta=0\) for every \(\lambda\in C(\Lambda)\). Equivalently, there exists a \(\lambda\in C(\Lambda)\) such that \(\lambda^{\prime}\beta\neq 0\) if and only if \(\Lambda^{\prime}\beta\neq 0\) which, since the \(\lambda_{j}\)s are a spanning set for \(C(\Lambda)\), occurs if and only if there exists a \(j\) such that \(\lambda^{\prime}_{j}\beta\neq 0\). Indeed, we do not even need to define \(\Lambda=[\lambda_{1},\ldots,\lambda_{s}]\) as long as we pick a \(\Lambda\) with

\[C(\Lambda)=\text{span}\{\lambda_{1},\ldots,\lambda_{s}\}.\]

Henceforth, _any reference within this chapter to \(\lambda^{\prime}\beta\) presumes that \(\lambda\in C(\Lambda)\)_.

If you are testing \(\lambda^{\prime}\beta=0\) for any \(\lambda\in C(\Lambda)\), you need to be sure that your procedure behaves in a reasonable way when \(\lambda\) is chosen as some projection of \(\hat{\beta}\) into \(C(\Lambda)\). This is far less of a problem when you are only looking at a prespecified collection of hypotheses \(\lambda^{\prime}_{j}\beta=0\), \(j=1,\ldots,s\). More on this later.

All of the multiple testing methods of which I am aware fit into the following algorithm:

1. Test \(H_{0}:\Lambda^{\prime}\beta=0\) at level \(\alpha\).
2. If this overall test is not significant, quit and go home. None of the \(\lambda^{\prime}\beta\)s are declared significant.
3. If this overall test is significant, use some _reasonable_ method to decide which \(\lambda^{\prime}\beta\)s to declare significant.

Whether the method used in step 3 is "reasonable" or not, this algorithm controls the experimentwise error rate for testing \(\lambda^{\prime}_{j}\beta=0\), \(j=1,\ldots,s\) or even for testing \(\lambda^{\prime}\beta=0\) for any \(\lambda\in C(\Lambda)\). Based on step 1, if the null hypotheses are all true, the probability of rejecting any of them is no more than \(\alpha\). The trick is to find a method for declaring significance in step 3 that behaves in ways that you like. You also need to choose a test for step 1. Typically, the procedure in step 3 is related to the choice of test in step 1, but there is no compelling reason that it needs to be.

The difficulty in step 3 arises when some \(j\)s have \(\lambda^{\prime}_{j}\beta=0\) but some have \(\lambda^{\prime}_{j}\beta\neq 0\). The latter may get you into step 3 where you would like to get the decision correct for each \(\lambda^{\prime}_{j}\beta\). More generally, the \(\lambda_{j}\)s with \(\lambda^{\prime}_{j}\beta=0\) define some subspace of \(C(\Lambda)\) where \(\lambda^{\prime}\beta=0\) but any \(\lambda\in C(\Lambda)\) outside of that subspace has \(\lambda^{\prime}\beta\neq 0\). Ideally we would like to make the correct decision on every \(\lambda^{\prime}\beta\). Choosing a multiple testing method is largely a choice about how hard you want to make it to reject an hypothesis \(\lambda^{\prime}\beta=0\). If you make it hard to reject, you will get most of the \(\lambda^{\prime}\beta=0\)s correct but can make many mistakes when \(\lambda^{\prime}\beta\neq 0\). If you make it easy to reject an hypothesis \(\lambda^{\prime}\beta=0\), you will get more of the \(\lambda^{\prime}\beta\neq 0\)s correct but make more mistakes when \(\lambda^{\prime}\beta=0\).

To illustrate the algorithm, we assume that \(\Lambda^{\prime}\beta\) is estimable. This implies that each \(\lambda^{\prime}_{j}\beta\) is estimable and that \(\lambda^{\prime}\beta\) is estimable whenever \(\lambda\in C(\Lambda)\). Recall that the sum of squares for testing an estimable \(\lambda^{\prime}\beta=0\) is

\[SS(\lambda^{\prime}\beta)\equiv\frac{(\lambda^{\prime}\hat{\beta})^{2}}{ \lambda^{\prime}(X^{\prime}X)^{-\lambda}}=\frac{(\rho^{\prime}MY)^{2}}{\rho^{ \prime}M\rho}=Y^{\prime}[M\rho(\rho^{\prime}M\rho)^{-1}\rho^{\prime}M]Y \equiv Y^{\prime}M_{M\rho}Y,\]

where \(\lambda^{\prime}=\rho^{\prime}X\).

The _Least Significant Difference (LSD)_ method often (I think incorrectly) associated with Fisher, amounts to 1. Check the truth of \[\frac{(A^{\prime}\hat{\beta})^{\prime}[A^{\prime}(X^{\prime}X)^{-}\Lambda]^{-}(A^{ \prime}\hat{\beta})/r(\Lambda)}{MSE}>F[1-\alpha,r(\Lambda),n-r(X)].\]
2. If the inequality is not true, quit and go home. None of the \(\lambda^{\prime}\beta\)s are declared significant.
3. If the inequality is true, declare \(\lambda^{\prime}\beta\) significantly different from 0 if and only if \[\frac{SS(\lambda^{\prime}\beta)}{MSE}>F[1-\alpha,1,n-r(X)].\]

Note that step 3 is just the usual \(\alpha\) level \(F\) test for \(\lambda^{\prime}\beta=0\). Certainly, this step 3 qualifies as a reasonable method but it makes it easy to reject. Collective experience indicates that the method works reasonably well if you restrict yourself in step 3 to testing only a prespecified finite collection \(\lambda^{\prime}_{j}\beta=0,\ j=1,\ldots,s\). Experience indicates that this method performs very badly, in the sense that it rejects too many hypotheses \(\lambda^{\prime}\beta=0\) that are true, if you use it to test arbitrarily chosen \(\lambda^{\prime}\beta\).

_Scheffe's method_ amounts to

1. Check the truth of \[\frac{(A^{\prime}\hat{\beta})^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}(A^{\prime}\hat{\beta})/r(\Lambda)}{MSE}>F[1-\alpha,r(\Lambda),n-r (X)].\] (1)
2. If the inequality is not true, quit and go home. None of the \(\lambda^{\prime}\beta\)s are declared significant.
3. If the inequality is true, declare \(\lambda^{\prime}\beta\) significantly different from 0 if and only if \[\frac{SS(\lambda^{\prime}\beta)/r(\Lambda)}{MSE}>F[1-\alpha,r(\Lambda),n-r(X)].\]

The basis of Scheffe's method is replacing \((A^{\prime}\hat{\beta})^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-}\Lambda]^{-} (A^{\prime}\hat{\beta})^{\prime}\) in (1) with \(SS(\lambda^{\prime}\beta)\). We will show later that

\[(A^{\prime}\hat{\beta})^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-}\Lambda]^{-} (A^{\prime}\hat{\beta})^{\prime}\geq SS(\lambda^{\prime}\beta), \tag{2}\]

so it is impossible to reject \(\lambda^{\prime}\beta=0\) unless you have previously rejected \(A^{\prime}\beta=0\). In fact, you can skip steps 1 and 2 of the algorithm because they take care of themselves. We will also show that there exists a \(\lambda\in C(\Lambda)\) for which equality occurs in (2). This ensures that if you get past step 1, in step 3 there always exists a \(\lambda^{\prime}\beta=0\) that would be rejected. Unfortunately, there is no assurance that any of the specific \(\lambda^{\prime}_{j}\beta=0\) hypotheses will be rejected, even though we are reasonably confident that they cannot all be true.

Scheffe's procedure makes it very difficult to reject a \(\lambda^{\prime}\beta=0\). Collective experience indicates that this method performs well if you use it to test arbitrary \(\lambda^{\prime}\beta\)(including ones suggested by the data) but is too conservative if your interest is only in testing a prespecified finite collection \(\lambda^{\prime}_{j}\beta=0\), \(j=1,\ldots,s\). Too conservative means not finding enough of the \(\lambda^{\prime}_{j}\beta\)s that are different from 0.

_The Bonferroni method_ fits the algorithm somewhat awkwardly. Pick numbers \(\alpha_{j}>0\) such that \(\sum_{j=1}^{s}\alpha_{j}=\alpha\). Typically, one picks \(\alpha_{j}=\alpha/s\). The \(\alpha\) level test of \(\Lambda^{\prime}\beta=0\) is rejected if any of the following statements are true:

\[\frac{SS(\lambda^{\prime}_{j}\beta)}{MSE}>F[1-\alpha_{j},1,n-r(X)],\]

\(j=1,\ldots,s\). As will be shown later, typically this is not an exact \(\alpha\) level test but has a null hypothesis probability of rejecting \(\Lambda^{\prime}\beta=0\) that is less than or equal to \(\alpha\). The multiple testing method is

1. Check if any of the following statements are true: \[\frac{SS(\lambda^{\prime}_{j}\beta)}{MSE}>F[1-\alpha_{j},1,n-r(X)],\] (3) \(j=1,\ldots,s\)
2. If none of the inequalities is true, quit and go home. None of the \(\lambda^{\prime}_{j}\beta\)s are declared significant.
3. For each \(j\), if inequality (3) is true, declare \(\lambda^{\prime}_{j}\beta\) significantly different from 0,

I have never seen this used to test arbitrary \(\lambda^{\prime}\beta=0\) for \(\lambda\in C(\Lambda)\) although if you take \(\alpha_{j}\equiv\alpha/s\) it is obvious how to do so. Typically, Bonferroni makes it harder to reject \(\lambda^{\prime}\beta=0\) than LSD but easier to reject than Scheffe.

As discussed in Christensen (1996, Chapter 6) [but _not_ in the second edition, Christensen (2015)], for a balanced one-way ANOVA with \(\Lambda^{\prime}\beta=0\) denoting equality of group means, there are several other tests available for \(\Lambda^{\prime}\beta=0\). The best known of these is the test based on the Studentized range statistic which leads naturally to Tukey's _Honest Significant Difference (HSD)_ method but also to the Newman-Keuls multiple range method. The difference between these two is that they use different reasonable methods at step 3. Dunnett's many-one \(t\) statistic method is based on a test designed to have good power for detecting differences between the mean of a control group and any of the other group means. Otts' analysis of means method, which is closely related to Shewhart's control charts, is based on a version of the Studentized maximum modulus statistic that is designed to have good power for detecting differences between the mean of a group and the average of the other group means. The test is appropriate when most groups should have the same mean, but occasionally a weird group pops up. Typically, these tests are used only for a finite number of hypotheses that are naturally associated with the form of the test statistic, specifically, \(\mu_{j}-\mu_{k}\), \(j\neq k\) for Tukey, \(\mu_{1}-\mu_{j}\), \(j\neq 1\) for Dunnet, and \(\mu_{j}-\bar{\mu}\). \(\propto\mu_{j}-\mbox{mean}_{k\neq j}\mu_{k}\). In fact, step 3 is typically only defined for the naturally associated hypotheses. Scheffe (1959) discusses how to extend Tukey's method to testing arbitrary contrasts \(\lambda^{\prime}\beta\), and finds that Tukey's method works worse than his for arbitrary contrasts.

To be honest, all of these multiple testing methods except LSD are defined in such a say that you never have to think about steps 1 or 2. They are defined so that it is impossible to reject in step 3 unless you have already rejected in step one. The value of the algorithm is that it emphasizes how much freedom one has to choose a reasonable procedure in step 3.

A more general multiple testing problem evaluates \(H_{0j}:\lambda_{j}^{\prime}\beta=d_{j}\), \(j=1\),..., \(s\), for known values \(d_{j}\). For this problem to make sense we need to know that there exists, and that we can actually find, a vector \(b\) such that \(d_{j}=\lambda_{j}^{\prime}b\), \(j=1\),..., \(s\). Collectively, the testing problem is \(\Lambda^{\prime}\beta=d\) where \(d=(d_{1}\),..., \(d_{s})^{\prime}\) and there exists \(b\) such that \(\Lambda^{\prime}b=d\). The three procedures that we outlined continue to hold with the following substitutions:

\[(\Lambda^{\prime}\hat{\beta})^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}(\Lambda^{\prime}\hat{\beta})^{\prime}\quad\rightarrow\quad( \Lambda^{\prime}\hat{\beta}-d)^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}(\Lambda^{\prime}\hat{\beta}-d)\]

and

\[SS(\lambda^{\prime}\beta)\quad\rightarrow\quad\frac{[\lambda^{\prime}(\hat{ \beta}-b)]^{2}}{\lambda^{\prime}(X^{\prime}X)^{-}\lambda}=\frac{[\rho^{\prime }M(Y-Xb)]^{2}}{\rho^{\prime}M\rho}\]

so that

\[SS(\lambda_{j}^{\prime}\beta)=\frac{(\lambda_{j}^{\prime}\hat{\beta}-d_{j})^{ 2}}{\lambda^{\prime}(X^{\prime}X)^{-}\lambda}.\]

We can define multiple confidence intervals for the \(\lambda_{j}^{\prime}\beta\)s. The interval for \(\lambda_{j}^{\prime}\beta\) is the set of all values \(d_{j}\) for which \(\lambda_{j}^{\prime}\beta=d_{j}\) is not rejected. Unfortunately, the intersection of such intervals can contain nonsensical values. If \(\lambda_{1}=2\lambda_{2}\), and if the interval for \(\lambda_{1}^{\prime}\beta\) is \((a,b)\), then the interval for \(\lambda_{2}^{\prime}\beta\) is \((2a,2b)\). Collectively these define a rectangle of reasonable values for the pair \((\lambda_{1}^{\prime}\beta,\lambda_{2}^{\prime}\beta)\) defined by the endpoints \((a,2a)\), \((a,2b)\), \((b,2b)\), and \((b,2a)\). For our choice of \(\lambda_{1}\) and \(\lambda_{2}\), every point in that rectangle in nonsensical except for those on the line segment from \((a,2a)\) to \((b,2b)\).

### Scheffe's Method

_Scheffe's method_ of multiple comparisons is an omnibus technique that allows one to test any and all single degree of freedom hypotheses that put constraints on a given subspace. It provides the assurance that the experimentwise error rate will not exceed a given level \(\alpha\). Typically, this subspace will be for fitting a set of parameters after fitting the mean, and in ANOVA problems is some sort of treatment space.

It is, of course, easy to find silly methods of doing multiple comparisons. One could, for example, always accept the null hypothesis. However, if the subspace is of value in fitting the model, Scheffe's method assures us that there is at least one hypothesis in the subspace that will be rejected. That is, if the \(F\) test is significant for testing that the subspace adds to the model, then there exists a linear hypothesis, putting a constraint on the subspace, that will be deemed significant by Scheffe's method.

We look at single degree of freedom estimable hypotheses \(\lambda^{\prime}\beta=0\) that put constraints on a given subspace. Call that subspace the test space, say, \(\mathcal{M}_{T}\subset C(X)\) with ppo \(M_{T}\). A test space can result from testing a reduced model \(Y=X_{0}\gamma+e\) with \(C(X_{0})\subset C(X)\) and \(\mathcal{M}_{T}=C(X_{0})_{C(X)}^{\perp}\) so that \(M_{T}=M-M_{0}\). Another convenient way of defining the test space is by defining an estimable hypothesis \(\Lambda^{\prime}\beta=0\) with \(\Lambda^{\prime}=P^{\prime}X\). In that case \(\mathcal{M}_{T}=C(MP)\) so that \(M_{T}=M_{MP}\). In either case we restrict ourselves to looking at \(\lambda^{\prime}\beta\) with \(\lambda^{\prime}=\rho^{\prime}X\) and \(M\rho\in\mathcal{M}_{T}\). In the former case, \(\lambda\) must have \(M\rho\perp C(X_{0})\) and in the later case \(\lambda\in C(\Lambda)\).

Scheffe's method is that an hypothesis \(H_{0}:\lambda^{\prime}\beta=0\) is rejected if

\[\frac{SS(\lambda^{\prime}\beta)/s}{MSE}>F(1-\alpha,s,\,dfE),\]

where \(SS(\lambda^{\prime}\beta)\) is the sum of squares for the usual test of the hypothesis, \(s\) is the dimension of the subspace \(\mathcal{M}_{T}\), and \(\lambda^{\prime}\beta=0\) is assumed to put a constraint on the subspace.

In terms of a one-way analysis of variance where the subspace is the space for testing equality of the group means, Scheffe's method applies to testing whether all contrasts equal zero. With \(t\) groups, a contrast is deemed significantly different from zero at the \(\alpha\) level if the sum of squares for the contrast divided by \(t-1\) and the _MSE_ is greater than \(F(1-\alpha,\,t-1,\,dfE)\).

Theorem 5.1.1 given below leads immediately to the key properties of Scheffe's method. Recall that if \(\rho^{\prime}X\beta=0\) puts a constraint on a subspace, then \(M\rho\) is an element of that subspace. Theorem 5.1.1 shows that the \(F\) test for the subspace rejects if and only if the Scheffe test rejects the single degree of freedom hypothesis \(\rho^{\prime}X\beta=0\) for some \(\rho\) with \(M\rho\) in the subspace. The proof is accomplished by finding a vector in the subspace having the property that the sum of squares for testing the corresponding one degree of freedom hypothesis equals the sum of squares for the entire space. Of course, the particular vector that has this property depends on \(Y\). To emphasize this dependence on \(Y\), the vector is denoted \(m_{Y}\). In the proof of the theorem, \(m_{Y}\) is seen to be just the projection of \(Y\) onto the subspace (hence the use of the letter \(m\) in the notation).

It follows that for a one-way ANOVA there is always a contrast for which the contrast sum of squares equals the sum of squares for groups. The exact nature of this contrast depends on \(Y\) and often the contrast is completely uninterpretable. Nevertheless, the existence of such a contrast establishes that Scheffe's method rejects for some contrast if and only if the test for equality of group means is rejected.

**Theorem 5.2.1**: _Consider the linear model \(Y=X\beta+e\) and let \(M_{T}\) be the perpendicular projection operator onto some subspace of \(C(X)\). Let \(r(M_{T})=s\). Then_

\[\frac{Y^{\prime}M_{T}Y/s}{MSE}>F(1-\alpha,s,\,dfE)\]

_if and only if there exists a vector \(m_{Y}\) such that \(Mm_{Y}\in C(M_{T})\) and_\[\frac{SS(m^{\prime}_{Y}X\beta)/s}{MSE}>F(1-\alpha,s,dfE).\]

_Proof_ \(\Rightarrow\) We want to find a vector \(m_{Y}\) so that if the \(F\) test for the subspace is rejected, then \(Mm_{Y}\) is in \(C(M_{T})\), and the hypothesis \(m^{\prime}_{Y}X\beta=0\) is rejected by Scheffe's method. If we find \(m_{Y}\) within \(C(M_{T})\) and \(SS(m^{\prime}_{Y}X\beta)=Y^{\prime}M_{T}Y\), we are done. Let \(m_{Y}=M_{T}Y\).

As in Section 3.5, \(SS(m^{\prime}_{Y}X\beta)=Y^{\prime}M_{T}m_{Y}[m^{\prime}_{Y}M_{T}m_{Y}]^{-1}m^{ \prime}_{Y}M_{T}Y\). Since \(M_{T}m_{Y}=M_{T}M_{T}Y=M_{T}Y=m_{Y}\), we have \(SS(m^{\prime}_{Y}X\beta)=Y^{\prime}m_{Y}[m^{\prime}_{Y}m_{Y}]^{-1}m^{\prime}_{ Y}Y=(Y^{\prime}M_{T}Y)^{2}/Y^{\prime}M_{T}Y=Y^{\prime}M_{T}Y\), and we are finished.

\(\Leftarrow\) We prove the contrapositive, i.e., if \(Y^{\prime}M_{T}Y/s\ MSE\leq F(1-\alpha,s,dfE)\), then for any \(\rho\) such that \(M\rho\in C(M_{T})\), we have \(SS(\rho^{\prime}X\beta)/s\ MSE\leq F(1-\alpha,s,dfE)\). To see this, observe that

\[SS(\rho^{{}^{\prime}}X\beta)=Y^{\prime}[M\rho(\rho^{{}^{\prime}}M\rho)^{-1} \rho^{{}^{\prime}}M]Y.\]

Since \([M\rho(\rho^{{}^{\prime}}M\rho)^{-1}\rho^{{}^{\prime}}M]\) is the perpendicular projection matrix onto a subspace of \(C(M_{T})\),

\[Y^{\prime}[M\rho(\rho^{{}^{\prime}}M\rho)^{-1}\rho^{{}^{\prime}}M]Y\leq Y^{ \prime}M_{T}Y\]

and we are done. \(\square\)

For an application of this Theorem, see Exercise 5.8.5

\(Y^{\prime}M_{T}Y\) is the sum of squares for testing the reduced model \(Y=(M-M_{T})\gamma+e\). If this null model is true,

\[\Pr\left[\frac{Y^{\prime}M_{T}Y}{s\ MSE}>F(1-\alpha,s,dfE)\right]=\alpha.\]

The theorem therefore implies that the experimentwise error rate for testing all hypotheses \(\rho^{\prime}X\beta=0\) with \(M\rho\in C(M_{T})\) is exactly \(\alpha\). More technically, we wish to test the hypotheses

\[H_{0}:\lambda^{\prime}\beta=0\ \ \ \mbox{for}\ \lambda\in\{\lambda|\lambda^{ \prime}=\rho^{{}^{\prime}}X\ \mbox{with}\ M\rho\in C(M_{T})\}.\]

The theorem implies that

\[\Pr\Bigl{[}\frac{SS(\lambda^{\prime}\beta)/s}{MSE}>F(1-\alpha,s,dfE)\ \mbox{for some}\ \lambda,\lambda^{\prime}=\rho^{{}^{\prime}}X,M\rho\in C(M_{T})\Bigr{]}=\alpha,\]

so the experimentwise error rate is \(\alpha\). The theorem also implies that if the omnibus \(F\) test rejects, there exists some single degree of freedom test that will be rejected. Note that which single degree of freedom tests are rejected depends on what the data are, as should be expected.

Scheffe's method can also be used for testing a subset of the set of all hypotheses putting a constraint on \(C(M_{T})\). For testing a subset, the experimentwise error rate will be no greater than \(\alpha\) and typically much below \(\alpha\). The primary problem with using Scheffe's method is that, for testing a finite number of hypotheses, the experimentwise error rate is so much below the nominal rate of \(\alpha\) that the procedure has very little power. (On the other hand, you can be extra confident, when rejecting with Scheffe's method, that you are not making a Type I error.)

Suppose that we want to test

\[H_{0}:\lambda_{k}^{\prime}\beta=0,\quad k=1,\ldots,r.\]

The constraints imposed by these hypotheses are \(M\rho_{k}\), \(k=1,\ldots,r\), where \(\lambda_{k}^{\prime}=\rho_{k}^{\prime}X\). If \(C(M_{T})\) is chosen so that \(C(M\rho_{1},\ldots,M\rho_{r})\subset C(M_{T})\), then by the previous paragraph, if \(H_{0}\) is true,

\[\Pr\Bigl{[}\frac{SS(\lambda_{k}^{\prime}\beta)/s}{MSE}>F(1-\alpha,s,dfE)\mbox{ for some }k,\;\;k=1,\ldots,r\Bigr{]}\leq\alpha.\]

For testing a finite number of hypotheses, it is possible to reject the overall \(F\) test but not reject for any of the specific hypotheses.

We now show that the most efficient procedure is to choose \(C(M\rho_{1},\ldots,M\rho_{r})=C(M_{T})\). In particular, given that a subspace contains the necessary constraints, the smaller the rank of the subspace, the more powerful is Scheffe's procedure. Consider two subspaces, one of rank \(s\) and another of rank \(t\), where \(s>t\). Both procedures guarantee that the experimentwise error rate is no greater than \(\alpha\). The more powerful procedure is the one that rejects more often. Based on the rank \(s\) subspace, Scheffe's method rejects if

\[SS(\lambda^{\prime}\beta)/MSE>sF(1-\alpha,s,dfE).\]

For the rank \(t\) subspace, the method rejects if

\[SS(\lambda^{\prime}\beta)/MSE>tF(1-\alpha,t,dfE).\]

With \(s>t\), by Theorem C.4,

\[sF(1-\alpha,s,dfE)\geq tF(1-\alpha,t,dfE).\]

One gets more rejections with the rank \(t\) space, hence it gives a more powerful procedure.

_Example 5.2.2_ _One-Way ANOVA_.

Consider the model

\[y_{ij}=\mu+\alpha_{i}+e_{ij},\quad e_{ij}\mbox{s i.i.d. }N(0,\sigma^{2}),\]\(i=1,2,3,4,\)\(j=1,\ldots,N.\) To test the three contrast hypotheses

\[\lambda_{1}^{\prime}\beta = \alpha_{1}+\alpha_{2}-\alpha_{3}-\alpha_{4}=0,\] \[\lambda_{2}^{\prime}\beta = \alpha_{1}-\alpha_{2}+\alpha_{3}-\alpha_{4}=0,\] \[\lambda_{3}^{\prime}\beta = \alpha_{1}+0+0-\alpha_{4}=0,\]

we can observe that the contrasts put constraints on the space for testing \(H_{0}:\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}\) which has rank 3. We can apply Scheffe's method: reject \(H_{0}:\lambda_{k}^{\prime}\beta=0\) if

\[\frac{SS(\lambda_{k}^{\prime}\beta)/3}{MSE}>F(1-\alpha,\,3,\,4(N-1)).\]

A more efficient method is to notice that \(\lambda_{1}^{\prime}\beta+\lambda_{2}^{\prime}\beta=2\lambda_{3}^{\prime}\beta.\) This is true for any \(\beta\), so \(\lambda_{1}^{\prime}+\lambda_{2}^{\prime}=2\lambda_{3}^{\prime}\) and, using (4.2.2), \(M\rho_{1}+M\rho_{2}=2M\rho_{3}.\) Since \(\lambda_{1}\) and \(\lambda_{2}\) are linearly independent, \(M\rho_{1}\) and \(M\rho_{2}\) are also; thus \(C(M\rho_{1},\,M\rho_{2},\,M\rho_{3})\) is a rank 2 space and Scheffe's method can be applied as: reject \(H_{0}:\lambda_{k}^{\prime}\beta=0\) if

\[\frac{SS(\lambda_{k}^{\prime}\beta)/2}{MSE}>F(1-\alpha,\,2,\,4(N-1)).\]

One virtue of Scheffe's method is that since it is really a test of all the hypotheses in a subspace, you can look at the data to help you pick an hypothesis and the test remains valid.

Scheffe's method can also be used to find simultaneous confidence intervals. To show this we need some additional structure for the problem. Let \(X=[X_{0},\,X_{1}]\) and let \(\beta^{\prime}=[\beta_{0}^{\prime},\,\beta_{1}^{\prime}]\), so that

\[Y=X_{0}\beta_{0}+X_{1}\beta_{1}+e.\]

Let \(M\) and \(M_{0}\) be the perpendicular projection operators onto \(C(X)\) and \(C(X_{0})\), respectively, and let \(M_{T}=M-M_{0}.\) We seek to find simultaneous confidence intervals for all estimable functions \(\rho^{\prime}X_{1}\beta_{1}.\) Note that \(\rho^{\prime}X_{1}\beta_{1}\) is estimable if and only if \(\rho^{\prime}X_{0}=0,\) which occurs if and only if \(0=M_{0}\rho=M\rho-M_{T}\rho,\) i.e., \(M\rho=M_{T}\rho.\) It follows that if \(\rho^{\prime}X_{1}\beta_{1}\) is an estimable function, then \(\rho^{\prime}X_{1}\beta_{1}=\rho^{\prime}MX_{1}\beta_{1}=\rho^{\prime}M_{T}X_{ 1}\beta_{1}.\) Conversely, for any vector \(\rho,\,\rho^{\prime}M_{T}(X_{0}\beta_{0}+X_{1}\beta_{1})=\rho^{\prime}M_{T}X_{ 1}\beta_{1}\) is an estimable function. Proceeding as in Section 3.7, and observing that \(M_{T}X\beta=M_{T}X_{0}\beta_{0}+M_{T}X_{1}\beta_{1}=M_{T}X_{1}\beta_{1},\) we have

\[\frac{(Y-X_{1}\beta_{1})^{\prime}M_{T}(Y-X_{1}\beta_{1})/r(M_{T})}{MSE}\sim F( r(M_{T}),\,dfE,\,0),\]

so that

\[\Pr\Bigl{[}\frac{(Y-X_{1}\beta_{1})^{\prime}M_{T}(Y-X_{1}\beta_{1})/r(M_{T})}{MSE }\leq F(1-\alpha,\,r(M_{T}),\,dfE)\Bigr{]}=1-\alpha\]

or, equivalently,\[1-\alpha=\Pr\biggl{[}\frac{(Y-X_{1}\beta_{1})^{\prime}M_{T}\rho(\rho^{ \prime}M_{T}\rho)^{-1}\rho^{\prime}M_{T}(Y-X_{1}\beta_{1})/r(M_{T})}{MSE}\\ \leq F(1-\alpha,r(M_{T}),dfE)\text{ for all }\rho\biggr{]}\]

\[=\Pr\bigl{[}\,|\rho^{\prime}M_{T}Y-\rho^{\prime}M_{T}X_{1}\beta_{1}|\\ \leq\sqrt{(\rho^{\prime}M_{T}\rho)(MSE)r(M_{T})F(1-\alpha,r(M_{T} ),dfE)}\text{ for all }\rho\bigr{]}.\]

This leads to obvious confidence intervals for all functions \(\rho^{\prime}M_{T}X_{1}\beta_{1}\) and thus to confidence intervals for arbitrary estimable functions \(\rho^{\prime}X_{1}\beta_{1}\).

#### 5.2.3 One-Way ANOVA.

Consider the model \(y_{ij}=\mu+\alpha_{i}+e_{ij}\), \(e_{ij}\)s independent \(N(0,\sigma^{2})\), \(i=1,\ldots,t\), \(j=1,\ldots,N_{i}\), and the space for testing \(\alpha_{1}=\alpha_{2}=\cdots=\alpha_{t}\). The linear functions that put constraints on that space are the contrasts. Scheffe's method indicates that \(H_{0}:\sum_{i=1}^{t}\lambda_{i}\alpha_{i}=0\) should be rejected if

\[\frac{(\sum\lambda_{i}\bar{y}_{i}.)^{2}/(\sum\lambda_{i}^{2}/N_{i})}{(t-1)MSE }>F(1-\alpha,t-1,dfE).\]

To find confidence intervals for contrasts, write \(X=[J,X_{1}]\) and \(\beta^{\prime}=[\mu,\beta_{1}^{\prime}]\), where \(\beta_{1}^{\prime}=[\alpha_{1},\ldots,\alpha_{t}]\). We can get simultaneous confidence intervals for estimable functions \(\rho^{\prime}X_{1}\beta_{1}\). As discussed in Chapter 4, the estimable functions \(\rho^{\prime}X_{1}\beta_{1}\) are precisely the contrasts. The simultaneous \((1-\alpha)100\%\) confidence intervals have limits

\[\sum\lambda_{i}\bar{y}_{i}.\pm\sqrt{(t-1)F(1-\alpha,t-1,dfE)MSE\left(\sum \lambda_{i}^{2}/N_{i}\right)}.\]

### 5.3 Least Significant Difference Method

The _Least Significant Difference (LSD)_ method is a general technique for testing a fixed number of hypotheses \(\lambda_{k}^{\prime}\beta=0\), \(k=1,\ldots,r\), chosen without looking at the data. The constraints imposed by these hypotheses generate some subspace. (Commonly, one identifies the subspace first and picks hypotheses that will generate it.) The technique involves three steps. First, do an \(\alpha\) level \(F\) test for whether the subspace adds to the model. If this omnibus \(F\) test is not significant, we can conclude that the data are consistent with \(\lambda_{k}^{\prime}\beta=0\), \(k=1,\ldots,r\). If the \(F\) test is significant, we want to identify which hypotheses are not true. To do this, test each hypothesis \(\lambda_{k}^{\prime}\beta=0\) with a \(t\) test (or an equivalent \(F\) test) at the \(\alpha\) level.

The experimentwise error rate is controlled by using the \(F\) test for the subspace. When all of the hypotheses are true, the probability of identifying any of them as false is no more than \(\alpha\), because \(\alpha\) is the probability of rejecting the omnibus \(F\) test. Although the omnibus \(F\) test is precisely a test of \(\lambda_{k}^{\prime}\beta=0\), \(k=1,\ldots,r\), even if the \(F\) test is rejected, the LSD method may not reject any of the specific hypotheses being considered. For this reason, the experimentwise error rate is less than \(\alpha\).

The LSD method is more powerful than Scheffe's method. If the hypotheses generate a space of rank \(s\), then Scheffe's method rejects if \(SS(\lambda_{k}^{\prime}\beta)/MSE>sF(1-\alpha,s,dfE)\). The LSD rejects if \(SS(\lambda_{k}^{\prime}\beta)/MSE>F(1-\alpha,1,dfE)\). By Theorem C.4, \(sF(1-\alpha,s,dfE)>F(1-\alpha,1,dfE)\), so the LSD method will reject more often than Scheffe's method. Generally, the LSD method is more powerful than other methods for detecting when \(\lambda_{k}^{\prime}\beta\neq 0\); but if \(\lambda_{k}^{\prime}\beta=0\), it is more likely than other methods to incorrectly identify the hypothesis as being different from zero.

Note that it is not appropriate to use an \(F\) test for a space that is larger than the space generated by the \(r\) hypotheses. Such an \(F\) test can be significant for reasons completely unrelated to the hypotheses, thus invalidating the experimentwise error rate.

#### Exercise 5.1

Consider the ANOVA model

\[y_{ij}=\mu+\alpha_{i}+e_{ij},\]

\(i=1,\ldots,t,j=1,\ldots,N\), with the \(e_{ij}\)s independent \(N(0,\sigma^{2})\). Suppose it is desired to test the hypotheses \(\alpha_{i}=\alpha_{i^{\prime}}\) for all \(i\neq i^{\prime}\). Show that, if the \(F\) test for group effects is significant, there is one number, called the LSD, so that the least significant difference rejects \(\alpha_{i}=\alpha_{i^{\prime}}\) precisely when

\[|\bar{y_{i}}.-\bar{y_{i^{\prime}}}.|>LSD.\]

#### Exercise 5.2

In the model of Exercise 5.1, let \(t=4\). Suppose we want to use the LSD method to test contrasts labeled \(A\), \(B\), and \(C\) defined by

\[\begin{array}{ccccc}\mbox{Name}&\lambda_{1}&\lambda_{2}&\lambda_{3}&\lambda_ {4}\\ \hline A&1&1&-1&-1\\ B&0&0&1&-1\\ C&1/3&1/3&1/3&-1\end{array}\]

Describe the procedure. Give test statistics for each test that is to be performed. Hint: The contrasts define a two-dimensional space.

### Bonferroni Method

Suppose we have chosen, before looking at the data, a set of \(r\) hypotheses to test, say, \(\lambda_{k}^{\prime}\beta=0\), \(k=1\),..., \(r\). The _Bonferroni method (BSD)_ consists of rejecting \(H_{0}:\lambda_{k}^{\prime}\beta=0\) if

\[\frac{SS(\lambda_{k}^{\prime}\beta)}{MSE}>F\left(1-\frac{\alpha}{r},\,1,\,dfE \right).\]

The Bonferroni method simply reduces the significance level of each individual test so that the sum of the significance levels is no greater than \(\alpha\). (In fact, the reduced significance levels do not have to be \(\alpha/r\) as long as the sum of the individual significance levels is \(\alpha\).)

This method rests on a Bonferroni inequality. For sets \(A_{1}\),..., \(A_{r}\), \(\Pr(\bigcup_{k=1}^{r}A_{k})\leq\sum_{k=1}^{r}\Pr(A_{k})\). (This inequality is nothing more than the statement that a probability measure is finitely subadditive.) If all the hypotheses \(\lambda_{k}^{\prime}\beta=0\), \(k=1\),..., \(r\) are true, then the experimentwise error rate is

\[\Pr\left(SS(\lambda_{k}^{\prime}\beta)>MSEF\left(1-\frac{\alpha}{ r},\,1,\,dfE\right)\text{ for some }k\right)\] \[=\Pr\left(\bigcup_{k=1}^{r}\left[SS(\lambda_{k}^{\prime}\beta)> MSEF\left(1-\frac{\alpha}{r},\,1,\,dfE\right)\right]\right)\] \[\leq\sum_{k=1}^{r}\Pr\left(SS(\lambda_{k}^{\prime}\beta)>MSEF \left(1-\frac{\alpha}{r},\,1,\,dfE\right)\right)\] \[=\sum_{k=1}^{r}\frac{\alpha}{r}=\alpha.\]

If the hypotheses to be tested are chosen after looking at the data, the individual significance levels of \(\alpha/r\) are invalid, so the experimentwise error rate has not been controlled.

Given that the subspace \(F\) test is rejected, the LSD method is more powerful than the Bonferroni method because \(F\left(1-\frac{\alpha}{r},1,\,dfE\right)>F(1-\alpha,1,\,dfE)\). The Bonferroni method is designed to handle a finite number of hypotheses, so it is not surprising that it is usually a more powerful method than Scheffe's method for testing the \(r\) hypotheses if \(r\) is not too large.

### Tukey's Method

Tukey's method, also known as the _Honest Significant Difference (HSD)_ method, is designed to compare all pairs of means for a set of independent normally distributed random variables with a common variance. (Meaning no disrespect to agreat statistician, John Tukey, I've never been able to think of this as anything other than Honest John's Significant Difference.) Let \(y_{i}\sim N(\mu_{i},\sigma^{2}),i=1,\ldots,t\), let the \(y_{i}\)s be independent, and let \(S^{2}\) be an estimate of \(\sigma^{2}\) with \(S^{2}\) independent of the \(y_{i}\)s and

\[\frac{vS^{2}}{\sigma^{2}}\sim\chi^{2}(v).\]

Tukey's method depends on knowing the distribution of the _Studentized range_ when \(\mu_{1}=\mu_{2}=\cdots=\mu_{t}\), i.e., we need to know that

\[Q\equiv\frac{\max_{i}y_{i}-\min_{i}y_{i}}{S}\sim Q(t,v)\]

and we need to be able to find percentage points of the \(Q(t,v)\) distribution. These are programmed into a lot of statistical software and are tabled in many books on statistical methods, e.g., Christensen (1996, 2015), Snedecor and Cochran (1980), and Kutner, Nachtsheim, Neter, and Li (2005).

If the observed value of \(Q\) is too large, the null hypothesis \(H_{0}:\mu_{1}=\cdots=\mu_{t}\) should be rejected. That is because any differences in the \(\mu_{i}\)s will tend to make the range large relative to the distribution of the range when all the \(\mu_{i}\)s are equal. Since the hypothesis \(H_{0}:\mu_{1}=\cdots=\mu_{t}\) is equivalent to the hypothesis \(H_{0}:\mu_{i}=\mu_{j}\) for all \(i\) and \(j\), we can use the Studentized range test to test all pairs of means. Reject the hypothesis that \(H_{0}:\mu_{i}=\mu_{j}\) if

\[\frac{|y_{i}-y_{j}|}{S}>Q(1-\alpha,t,v),\]

where \(Q(1-\alpha,t,v)\) is the \((1-\alpha)100\) percentage point of the \(Q(t,v)\) distribution. If \(H_{0}:\mu_{i}=\mu_{j}\) for all \(i\) and \(j\) is true, then at least one of these tests will reject \(H_{0}\) if and only if

\[\frac{\max_{i}y_{i}-\min_{i}y_{i}}{S}>Q(1-\alpha,t,v),\]

which happens with probability \(\alpha\). Thus the experimentwise error rate is exactly \(\alpha\).

#### Balanced Two-Way ANOVA

Consider the model

\[y_{ijk}=\mu+\alpha_{i}+\beta_{j}+e_{ijk},\ \ \ e_{ijk}\mbox{s i.i.d. }N(0, \sigma^{2}),\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), \(k=1,\ldots,N\). Suppose we want to test the hypotheses \(H_{0}:\beta_{j}=\beta_{j^{\prime}}\) for all \(j\neq j^{\prime}\). Consider the \(\bar{y}_{.j}\). values. Here

\[\bar{y}_{.j.}\sim N\big{(}\mu+\bar{\alpha}_{.}+\beta_{j},\sigma^{2}/aN\big{)}\]and the \(\bar{y}_{.j.}\)s are independent because \(\bar{y}_{.j.}\) depends only on \(\bar{e}_{.j.}\) for its randomness; and since \(\bar{e}_{.j.}\) and \(\bar{e}_{.j^{\prime}.}\) are based on disjoint sets of the \(e_{ijk}\)s, they must be independent. We will see in Section 7.1 that the \(\bar{y}_{.j.}\)s are least squares estimates of the \(\mu+\bar{\alpha}.+\beta_{j}\)s, so the \(\bar{y}_{.j.}\)s must be independent of the \(MSE\). It follows quickly that if \(H_{0}:\beta_{1}=\cdots=\beta_{b}\) is true, then

\[\frac{\max_{j}\bar{y}_{.j.}-\min_{j}\bar{y}_{.j.}}{\sqrt{MSE/aN}}\sim Q(b,\,dfE);\]

and we reject \(H_{0}:\beta_{j}=\beta_{j^{\prime}}\) if

\[|\bar{y}_{.j.}-\bar{y}_{.j^{\prime}.}|>Q(1-\alpha,\,b,\,dfE)\sqrt{MSE/aN}.\]

Note that the Studentized range provides a competitor to the usual analysis of variance \(F\) test for the hypothesis \(H_{0}:\beta_{1}=\cdots=\beta_{b}\), cf. [http://www.stat.unm.edu/~fletcher/UMPI.pdf](http://www.stat.unm.edu/~fletcher/UMPI.pdf). Also, Tukey's method is only applicable when all the means being used are based on the same number of observations.

### Multiple Range Tests: Newman-Keuls and Duncan

The _Newman-Keuls multiple range method_ is a competitor to the Tukey method. It looks at all pairs of means. In fact, it amounts to a sequential version of Tukey's method. Using the notation of the previous section, order the \(y_{i}\)s from smallest to largest, say

\[y_{(1)}\leq y_{(2)}\leq\cdots\leq y_{(t)},\]

and define \(\mu_{(i)}\equiv\mu_{j}\) when \(y_{(i)}=y_{j}\). Note that the \(\mu_{(i)}\)s need not be ordered in any particular way. However, the Newman-Keuls method acts as if the \(\mu_{(i)}\)s are also ordered. With this notation, we can write the Studentized range as

\[Q=\frac{y_{(t)}-y_{(1)}}{S}.\]

The Newman-Keuls method rejects \(H_{0}:\mu_{(t)}=\mu_{(1)}\) if \(y_{(t)}-y_{(1)}>SQ(1-\alpha,\,t,\,v)\). If this hypothesis is not rejected, stop. All means are considered equal. If this hypothesis is rejected, we continue.

The next step tests two hypotheses. \(H_{0}:\mu_{(t-1)}=\mu_{(1)}\) is rejected if \(y_{(t-1)}-y_{(1)}>SQ(1-\alpha,\,t-1,\,v)\). \(H_{0}:\mu_{(t)}=\mu_{(2)}\) is rejected if \(y_{(t)}-y_{(2)}>SQ(1-\alpha,\,t-1,\,v)\). If \(\mu_{(t-1)}=\mu_{(1)}\) is not rejected, then \(\mu_{(1)}\), \(\mu_{(2)}\),..., \(\mu_{(t-1)}\) are assumed to be equal, and no more tests concerning only those means are performed. Similar conclusions hold if \(\mu_{(t)}=\mu_{(2)}\) is not rejected. If either hypothesis is rejected, the next round of hypotheses is considered.

[MISSING_PAGE_FAIL:158]

Duncan has a multiple range test that is similar to Newman-Keuls but where the \(\alpha\) levels for the various rounds of tests keep decreasing. In fact, _Duncan's method_ is exactly the same as Newman-Keuls except that the \(\alpha\) levels used when taking values from the table of the Studentized range are different. Duncan suggests using a \(1-(1-\alpha)^{p-1}\) level test when comparing a set of \(p\) means. If there is a total of \(t\) means to be compared, Duncan's method only controls the experimentwise error rate at \(1-(1-\alpha)^{t-1}\). For \(\alpha=0.05\) and \(t=6\), Duncan's method can only be said to have an experimentwise error rate of \(0.23\). As Duncan suggests, his method should only be performed when a corresponding omnibus \(F\) test has been found significant. This two stage procedure may be a reasonable compromise between the powers of the LSD and Newman-Keuls methods.

### Summary

The emphasis in this chapter has been on controlling the experimentwise error rate. We have made some mention of power and the fact that increased power can be a mixed blessing. The really difficult problem for multiple comparison procedures is not in controlling the experimentwise error rate, but in carefully addressing the issues of power and the sizes of individual tests.

The discussion of Duncan's multiple range test highlights an interesting fact about multiple comparison methods. Any method of rejecting hypotheses, if preceded by an appropriate omnibus \(F\) test, is a valid multiple comparison procedure, valid in the sense that the experimentwise error rate is controlled. For example, if you do an \(F\) test first and stop if the \(F\) test does not reject, you can then (1) reject all individual hypotheses if the analysis is being performed on your mother's birth date, (2) reject no individual hypotheses on other dates. As stupid as this is, the experimentwise error rate is controlled. Intelligent choice of a multiple comparison method also involves consideration of the error rates (probabilities of type I errors) for the individual hypotheses. The main question is: If not all of the hypotheses are true, how many of the various kinds of mistakes do the different methods make?

In a one-way ANOVA with 12 groups and no mean differences, doing unadjusted \(\alpha=0.05\) level tests you could easily find \(3\doteq\binom{12}{2}\times 0.05\) phantom differences that look significant. None of the methods discussed here are likely to do that. If you had 13 groups with exactly 1 mean different from the other 12 (and substantially so), LSD is likely to find the 12 real differences but may also find 3 phantom differences. None of the other discussed methods (except Duncan) is likely find the phantom differences.

A reasonable goal might be to have the experimentwise error rate and the error rates for the individual hypotheses all no greater than \(\alpha\). The Scheffe, LSD, Bonferroni, Tukey, and Newman-Keuls methods all seem to aim at this goal. The Duncan method does not seem to accept this goal.

Suppose we want \(\alpha\) level tests of the hypotheses \[H_{0}:\lambda^{\prime}_{k}\beta=0,\ \ \ \ \ k\in\Omega.\]

A reasonable procedure is to reject an hypothesis if

\[SS(\lambda^{\prime}_{k}\beta)/MSE>C\]

for some value \(C\). For example, the LSD method takes \(C=F(1-\alpha,1,dfE)\). If \(\Omega\) consists of all the hypotheses in a \(t\)-dimensional space, Scheffe's method takes \(C=tF(1-\alpha,t,dfE)\). If \(\Omega\) is a finite set, say \(\Omega=\{1,\ldots,r\}\), then the Bonferroni method takes \(C=F(1-\alpha/r,1,dfE)\).

To control the level of the individual test \(H_{0}:\lambda^{\prime}_{k}\beta=0\), one needs to pick \(C\) as the appropriate percentile of the distribution of \(SS(\lambda^{\prime}_{k}\beta)/MSE\). At one extreme, if one ignores everything else that is going on and if \(\lambda^{\prime}_{k}\beta\) was chosen without reference to the data, the appropriate distribution for \(SS(\lambda^{\prime}_{k}\beta)/MSE\) is \(F(1,dfE)\). At the other extreme, if one picks \(\lambda^{\prime}_{k}\beta\) so that \(SS(\lambda^{\prime}_{k}\beta)\) is maximized in a \(t\)-dimensional space, then the appropriate distribution for \(SS(\lambda^{\prime}_{k}\beta)/MSE\) is \(t\) times an \(F(t,dfE)\); it is clear that the probability of rejecting any hypothesis other than that associated with maximizing \(SS(\lambda^{\prime}_{k}\beta)\) must be less than \(\alpha\). Thus, in the extremes, we are led to the LSD and Scheffe methods. What one really needs is the distribution of \(SS(\lambda^{\prime}_{k}\beta)/MSE\) given \(\lambda^{\prime}_{k}\beta=0\), and all the information contained in knowing \(\lambda^{\prime}_{j}\beta\) for \(j\in\Omega-\{k\}\) and that \(SS(\lambda^{\prime}_{j}\beta)\) for \(j\in\Omega-\{k\}\) will also be observed. Since the desired distribution will depend on the \(\lambda^{\prime}_{j}\beta\)s, and they will never be known, there is no hope of achieving this goal.

The quality of the LSD method depends on how many hypotheses are to be tested. If only one hypothesis is to be tested, LSD is the method of choice. If all of the hypotheses in a subspace are to be tested, LSD is clearly a bad choice for testing the hypothesis that maximizes \(SS(\lambda^{\prime}_{k}\beta)\) and also a bad choice for testing other hypotheses that look likely to be significant. For testing a reasonably small number of hypotheses that were chosen without looking at the data, the LSD method seems to keep the levels of the individual tests near the nominal level of \(\alpha\). (The fact that the individual hypotheses are tested only if the omnibus \(F\) test is rejected helps keep the error rates near their nominal levels.) However, as the number of hypotheses to be tested increases, the error rate of the individual tests can increase greatly. The LSD method is not very responsive to the problem of controlling the error level of each individual test, but it is very powerful in detecting hypotheses that are not zero.

Scheffe's method puts an upper bound of \(\alpha\) on the probability of type I error for each test, but for an individual hypothesis chosen without examining the data, the true probability of type I error is typically far below \(\alpha\). Scheffe's method controls the type I error but at a great cost in the power of each test.

The Bonferroni method uses the same distributions for \(SS(\lambda^{\prime}_{k}\beta)/MSE\), \(k\in\Omega\), as the LSD method uses. The difference is in the different ways of controlling the experimentwise error rate. Bonferroni reduces the size of each individual test, while LSD uses an overall \(F\) test. The Bonferroni method, since it reduces the size of each test, does a better job of controlling the error rate for each individual hypothesis than does the LSD method. This is done at the cost of reducing the power relative to LSD. For a reasonable number of hypotheses, the Bonferroni method tends to be more powerful than Scheffe's method and tends to have error levels nearer the nominal than Scheffe's method.

A similar evaluation can be made of the methods for distinguishing between pairs of means. The methods that are most powerful have the highest error rates for individual hypotheses. From greatest to least power, the methods seem to rank as LSD, Duncan, Newman-Keuls, Tukey. Scheffe's method should rank after Tukey's. The relative position of Bonferroni's method is unclear.

When deciding on a multiple comparison method, one needs to decide on the importance of correctly identifying nonzero hypotheses (high power) relative to the importance of incorrectly identifying zero hypotheses as being nonzero (controlling the type I error). With high power, one will misidentify some zero hypotheses as being nonzero. When controlling the type I error, one is more likely not to identify some nonzero hypotheses as being nonzero.

Table 5.1 contains a summary of the methods considered in this chapter. It lists the hypotheses for which each method is appropriate, the method by which the experimentwise error rate is controlled, and comments on the relative powers and probabilities of type I error (error rates) for testing individual hypotheses.

\begin{table}
\begin{tabular}{p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}} \hline Method & Hypotheses & Control & Comments \\ \hline Scheffe & Any and all hypotheses constraining a particular subspace & \(F\) test for subspace & Lowest error rate and power of any method. Good for data snooping. HSD better for pairs of means \\ \hline LSD & Any and all hypotheses constraining a particular subspace & \(F\) test for subspace & Highest error rate and power of any method. Best suited for a finite number of hypotheses \\ \hline Bonferroni & Any finite set of hypotheses & Bonferroni inequality & Most flexible method. Often similar to HSD for pairs of means \\ \hline Tukey’s HSD & All differences between pairs of means & Studentized range test & Lowest error rate and power for pairs of means \\ \hline Newman–Keuls & All differences between pairs of means & Studentized range test & Error rate and power intermediate between HSD and Duncan \\ \hline Duncan & All differences between pairs of means & Studentized range test or \(F\) test & Error rate and power intermediate between Newman–Keuls and LSD \\ \hline \end{tabular}
\end{table}
Table 5.1: Summary of Multiple Comparison Methods

**Exercise 5.3**  Show that for testing all hypotheses in a six-dimensional space with 30 degrees of freedom for error, if the subspace \(F\) test is omitted and the nominal LSD level is \(\alpha=0.005\), then the true error rate must be less than 0.25.

Hint: Try to find a Scheffe rejection region that is comparable to the LSD rejection region.

#### Fisher Versus Neyman-Pearson

I have _tried_ to maintain a Fisherian view towards statistical inference in this edition of the book. However, I think multiple comparison procedures are fundamentally a tool of Neyman-Pearson testing. Fisherian testing is about measuring the evidence against the null model, while Neyman-Pearson testing is about controlling error rates. Controlling the experimentwise error rate seems anti-Fisherian to me.

Fisher is often credited with (blamed for) the LSD method. However, Fisher (1935, Chapter 24) did not worry about the experimentwise error rate when making multiple comparisons using his least significant difference method in analysis of variance. He did, however, worry about drawing inappropriate conclusions by using an invalid null distribution for tests determined by examining the data. In particular, Fisher proposed a Bonferroni correction when comparing the largest and smallest sample means.

If you are going to look at all pairs of means in a balanced ANOVA, then the appropriate distribution for comparing the largest and smallest sample means is the Studentized range. It gives the appropriate \(P\) value. An appropriate \(P\) value for other comparisons is difficult, but \(P\) values based on the Studentized range should be conservative (larger than the true \(P\) value). Similar arguments can be made for other procedures.

### Additional Exercises

**Exercise 5.8.1**  Compare all pairs of means for the blue jeans exercise of Chapter 4. Use the following methods:

* Scheffe's method, \(\alpha=0.01\),
* LSD method, \(\alpha=0.01\),
* Bonferroni method, \(\alpha=0.012\),
* Tukey's HSD method, \(\alpha=0.01\),
* Newman-Keuls method, \(\alpha=0.01\).

**Exercise 5.8.2**  Test whether the four orthogonal contrasts you chose for the blue jeans exercise of Chapter 4 equal zero. Use all of the appropriate multiple comparisonmethods discussed in this chapter to control the experimentwise error rate at \(\alpha=0.05\) (or thereabouts).

##### Exercise 5.8.3

Compare all pairs of means in the coat-shirt exercise of Chapter 4. Use all of the appropriate multiple comparison methods discussed in this chapter to control the experimentwise error rate at \(\alpha=0.05\) (or thereabouts).

##### Exercise 5.8.4

Suppose that in a balanced one-way ANOVA the group means \(\tilde{y}_{1\cdot}\),..., \(\tilde{y}_{l\cdot}\) are not independent but have some nondiagonal covariance matrix \(V\). How can Tukey's HSD method be modified to accommodate this situation?

##### Exercise 5.8.5

For an unbalanced one-way ANOVA, give the contrast coefficients for the contrast whose sum of squares equals the sum of squares for groups. Show the equality of the sums of squares. Hint: Recall that in Exercise 4.2 we found the form of \(M_{\alpha}Y\) and that equation (4.2.2) allows one to read off the coefficients of any contrast determined by a vector in \(C(M_{\alpha})\).

## References

* Bretz et al. (2011) Bretz, F., Hothorn, T., & Westfall, P. (2011). _Multiple comparisons using R_. Boca Raton: Chapman and Hall/CRC.
* Christensen (1996) Christensen, R. (1996). _Analysis of variance, design, and regression: Applied statistical methods_. London: Chapman and Hall.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* Fisher (1935) Fisher, R. A. (1935). _The design of experiments_, (9th ed., 1971). New York: Hafner Press.
* Hochberg & Tamhane (1987) Hochberg, Y., & Tamhane, A. (1987). _Multiple comparison procedures_. New York: Wiley.
* Hsu (1996) Hsu, J. C. (1996). _Multiple comparisons: Theory and methods_. London: Chapman and Hall.
* Kutner et al. (2005) Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2005). _Applied linear statistical models_ (5th ed.). New York: McGraw-Hill Irwin.
* Miller (1981) Miller, R. G, Jr. (1981). _Simultaneous statistical inference_ (2nd ed.). New York: Springer.
* Scheffe (1959) Scheffe, H. (1959). _The analysis of variance_. New York: Wiley.
* Snedecor & Cochran (1980) Snedecor, G. W., & Cochran, W. G. (1980). _Statistical methods_ (7th ed.). Ames: Iowa State University Press.
* Snedecor & Cochran (1981)

## Chapter 6 Regression Analysis

the natural estimates of these correlation coefficients can be obtained from standard regression results. Section 6 introduces _best linear unbiased prediction (BLUP)_. (In previous editions this material was in Section 12.2.) Section 7 examines testing for _lack of fit_. Finally, Section 8 establishes the basis of the relationship between polynomial regression and polynomial contrasts in one-way ANOVA.

There is additional material, spread throughout the book, that relates to the material in this chapter. Section 2 examines a partitioned model. Partitioned models are treated in general in Sections 9.1 and 9.2. Chapter 9 also contains an exercise that establishes the basis of the sweep operator used in regression computations. The results of Section 7 are extended in Section 7.3 to relate polynomial regression with polynomial contrasts in a two-way ANOVA. Finally, Chapters 12-14 are concerned with topics that are traditionally considered part of regression analysis.

There are a number of fine books available on regression analysis. Those that I refer to most often are Cook and Weisberg (1999), Daniel and Wood (1980), Draper and Smith (1998), and Weisberg (2014).

### Simple Linear Regression

The model for simple linear regression is \(y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\) or \(Y=X\beta+e\), where \(\beta^{\prime}=[\beta_{0},\,\beta_{1}]\) and

\[X^{\prime}=\begin{bmatrix}1&1&\cdots&1\\ x_{1}&x_{2}&\cdots&x_{n}\end{bmatrix}.\]

Often it is easier to work with the alternative model \(y_{i}=\gamma_{0}+\gamma_{1}(x_{i}-\bar{x}.)+e_{i}\) or \(Y=X_{*}\gamma+e\), where \(\gamma^{\prime}=[\gamma_{0},\,\gamma_{1}]\) and

\[X^{\prime}_{*}=\begin{bmatrix}1&1&\cdots&1\\ x_{1}-\bar{x}.&x_{2}-\bar{x}.&\cdots&x_{n}-\bar{x}.\end{bmatrix}.\]

Note that \(C(X)=C(X_{*})\). In fact, letting

\[U=\begin{bmatrix}1&-\bar{x}.\\ 0&1\end{bmatrix},\]

we have \(X_{*}=XU\) and \(X_{*}U^{-1}=X\). Moreover, \(\operatorname{E}(Y)=X\beta=X_{*}\gamma=XU\gamma\). Letting \(P^{\prime}=(X^{\prime}X)^{-1}X^{\prime}\) leads to

\[\beta=P^{\prime}X\beta=P^{\prime}XU\gamma=U\gamma.\]

In particular,

\[\begin{bmatrix}\beta_{0}\\ \beta_{1}\end{bmatrix}=\begin{bmatrix}\gamma_{0}-\gamma_{1}\bar{x}.\\ \gamma_{1}\end{bmatrix}.\](See also Example 3.1.2.)

To find the least squares estimates and the projection matrix, observe that

\[X_{*}^{\prime}X_{*}=\begin{bmatrix}n&0\\ 0&\sum_{i=1}^{n}(x_{i}-\bar{x}_{\cdot})^{2}\end{bmatrix},\]

\[(X_{*}^{\prime}X_{*})^{-1}=\begin{bmatrix}1/n&0\\ 0&1\Big{/}\sum_{i=1}^{n}(x_{i}-\bar{x}_{\cdot})^{2}\end{bmatrix},\]

and, since the inverse of \(X_{*}^{\prime}X_{*}\) exists, the estimate of \(\gamma\) is

\[\hat{\gamma}=(X_{*}^{\prime}X_{*})^{-1}X_{*}^{\prime}Y=\begin{bmatrix}\bar{y}.\\ \sum_{i=1}^{n}(x_{i}-\bar{x}_{\cdot})y_{i}\Big{/}\sum_{i=1}^{n}(x_{i}-\bar{x}_ {\cdot})^{2}\end{bmatrix}.\]

Moreover,

\[\hat{\beta}=U\hat{\gamma},\]

so the least squares estimate of \(\beta\) is

\[\hat{\beta}=\begin{bmatrix}\bar{y}.-\hat{\gamma}_{1}\bar{x}_{\cdot}\\ \hat{\gamma}_{1}\end{bmatrix}.\]

The projection matrix \(M=X_{*}(X_{*}^{\prime}X_{*})^{-1}X_{*}^{\prime}\) is

\[\begin{bmatrix}\frac{1}{n}+\frac{(x_{1}-\bar{x}_{\cdot})^{2}}{\sum_{i=1}^{n}(x _{i}-\bar{x}_{\cdot})^{2}}&\cdots&\frac{1}{n}+\frac{(x_{1}-\bar{x}_{\cdot})(x_ {n}-\bar{x}_{\cdot})}{\sum_{i=1}^{n}(x_{i}-\bar{x}_{\cdot})^{2}}\\ \vdots&&\vdots\\ \frac{1}{n}+\frac{(x_{1}-\bar{x}_{\cdot})(x_{n}-\bar{x}_{\cdot})}{\sum_{i=1}^{n }(x_{i}-\bar{x}_{\cdot})^{2}}&\cdots&\frac{1}{n}+\frac{(x_{n}-\bar{x}_{\cdot}) ^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x}_{\cdot})^{2}}\end{bmatrix}.\]

The covariance matrix of \(\hat{\gamma}\) is \(\sigma^{2}(X_{*}^{\prime}X_{*})^{-1}\); for \(\hat{\beta}\) it is \(\sigma^{2}(X^{\prime}X)^{-1}\). The usual tests and confidence intervals follow immediately upon assuming that \(e\sim N(0,\sigma^{2}I)\).

A natural generalization of the simple linear regression model \(y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\) is to expand it into a polynomial, say

\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\cdots+\beta_{p-1}x_{i}^{p- 1}+e_{i}.\]

Although polynomial regression has some special features that will be discussed later, at a fundamental level it is simply a linear model that involves an intercept and \(p-1\) predictor variables. It is thus a special case of multiple regression, the model treated in the next section.

**Exercise 6.1** : For simple linear regression, find the \(MSE\), \(\text{Var}(\hat{\beta}_{0})\), \(\text{Var}(\hat{\beta}_{1})\), and \(\text{Cov}(\hat{\beta}_{0},\hat{\beta}_{1})\).

**Exercise 6.2**: Use Scheffe's method of multiple comparisons to derive the _Working-Hotelling simultaneous confidence band_ for a simple linear regression line \(\mathrm{E}(y)=\beta_{0}+\beta_{1}x\).

### Multiple Regression

Multiple regression is any regression problem with \(p\geq 2\) that is not simple linear regression. If we take as our model \(Y=X\beta+e\), we have

\[\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}Y,\] \[\mathrm{Cov}\Big{(}\hat{\beta}\Big{)} = \sigma^{2}(X^{\prime}X)^{-1}X^{\prime}IX(X^{\prime}X)^{-1}=\sigma ^{2}(X^{\prime}X)^{-1},\] \[SSR(X) = Y^{\prime}MY=\hat{\beta}^{\prime}(X^{\prime}X)\hat{\beta},\] \[SSE = Y^{\prime}(I-M)Y,\] \[dfE = r(I-M)=n-p.\]

Since \(\beta\) is estimable, any linear function \(\lambda^{\prime}\beta\) is estimable. If \(Y\sim N(X\beta,\sigma^{2}I)\), tests and confidence intervals based on

\[\frac{\lambda^{\prime}\hat{\beta}-\lambda^{\prime}\beta}{\sqrt{MSE\lambda^{ \prime}(X^{\prime}X)^{-1}\lambda}}\sim t(dfE)\]

are available.

Suppose we write the regression model as

\[Y=[X_{1},\ldots,X_{p}]\begin{bmatrix}\beta_{1}\\ \vdots\\ \beta_{p}\end{bmatrix}+e.\]

If we let \(\lambda^{\prime}_{j}=(0,\ldots,0,1,0,\ldots,0)\), with the \(1\) in the \(j\)th place, we have

\[\frac{\hat{\beta}_{j}-\beta_{j}}{\sqrt{MSE\lambda^{\prime}_{j}(X^{\prime}X)^{- 1}\lambda_{j}}}\sim t(dfE),\]

where \(\lambda^{\prime}_{j}(X^{\prime}X)^{-1}\lambda_{j}\) is the \(j\)th diagonal element of \((X^{\prime}X)^{-1}\). This yields a test of the hypothesis \(H_{0}:\beta_{j}=0\). It is important to remember that this \(t\) test is equivalent to the \(F\) test for testing the reduced model\[Y=[X_{1},\ldots,X_{j-1},X_{j+1},\ldots,X_{p}]\left[\begin{array}{c}\beta_{1}\\ \vdots\\ \beta_{j-1}\\ \beta_{j+1}\\ \vdots\\ \beta_{p}\end{array}\right]+e\]

against the full regression model. The \(t\) and \(F\) tests for \(\beta_{j}=0\) depend on all of the other variables in the regression model. Add or delete any other variable in the model and the tests change.

\(SSR(X)\) can be broken down into single degree of freedom components:

\[SSR(X) = SSR(X_{1})+SSR(X_{2}|X_{1})+SSR(X_{3}|X_{1},X_{2})+\cdots+SSR(X_ {p}|X_{1},\ldots,X_{p-1})\] \[= R(\beta_{1})+R(\beta_{2}|\beta_{1})+R(\beta_{3}|\beta_{1},\beta_ {2})+\cdots+R(\beta_{p}|\beta_{1},\ldots,\beta_{p-1}).\]

Of course, any permutation of the subscripts \(1,\ldots,p\) gives another breakdown. The interpretation of these terms is somewhat unusual. For instance, \(SSR(X_{3}|X_{1},X_{2})\)_is not_ the sum of squares for testing any very interesting hypothesis about the full regression model. \(SSR(X_{3}|X_{1},X_{2})\) is the sum of squares needed for testing the model

\[Y=[X_{1},X_{2}]\left[\begin{array}{c}\beta_{1}\\ \beta_{2}\end{array}\right]+e\]

against the larger model

\[Y=[X_{1},X_{2},X_{3}]\left[\begin{array}{c}\beta_{1}\\ \beta_{2}\\ \beta_{3}\end{array}\right]+e.\]

This breakdown is useful in that, for instance,

\[SSR(X_{p-1},X_{p}|X_{1},\ldots,X_{p-2})=SSR(X_{p}|X_{1},\ldots,X_{p-1})+SSR(X_ {p-1}|X_{1},\ldots,X_{p-2}).\]

\(SSR(X_{p-1},X_{p}|X_{1},\ldots,X_{p-2})\) is the sum of squares needed to test \(H_{0}:\beta_{p}=\beta_{p-1}=0\).

The _usual multiple regression model_ is assumed to have a column of 1s in the model matrix. In that case, the model can be written

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\cdots+\beta_{p-1}x_{ip-1}+e_{ i}.\]

An analysis of variance table is often written for testing this model against the model

\[y_{i}=\beta_{0}+e_{i},\]

for which the model matrix consists only of a column of 1s. The table is given below.

The _SSReg_ from the table can be rewritten as \(\hat{\beta}^{\prime}(X^{\prime}X)\hat{\beta}-C\), where \(C\) is the correction factor, i.e., \(C\equiv Y^{\prime}([1/n]J_{n}^{n})Y=n(\bar{y}.)^{2}\).

_Example 6.2.1_ Consider the data given in Table 6.1 on the heating requirements for a factory. There are 25 observations on a dependent variable \(y\) (the number of pounds of steam used per month) and 2 independent variables, \(x_{1}\) (the average atmospheric temperature for the month in \({}^{\circ}F\)) and \(x_{2}\) (the number of operating days in the month). The predictor variables are from Draper and Smith (1998).

The parameter estimates, standard errors, and \(t\) statistics for testing whether each parameter equals zero are

\begin{table}
\begin{tabular}{c c c c c c c} \hline Obs. no. & \(x_{1}\) & \(x_{2}\) & \(y\) & Obs. no. & \(x_{1}\) & \(x_{2}\) & \(y\) \\ \hline
1 & 35.3 & 20 & 17.8270 & 14 & 39.1 & 19 & 19.0198 \\
2 & 29.7 & 20 & 17.0443 & 15 & 46.8 & 23 & 20.6128 \\
3 & 30.8 & 23 & 15.6764 & 16 & 48.5 & 20 & 20.7972 \\
4 & 58.8 & 20 & 26.0350 & 17 & 59.3 & 22 & 28.1459 \\
5 & 61.4 & 21 & 28.3908 & 18 & 70.0 & 22 & 33.2510 \\
6 & 71.3 & 22 & 31.1388 & 19 & 70.0 & 11 & 30.4711 \\
7 & 74.4 & 11 & 32.9019 & 20 & 74.5 & 23 & 36.1130 \\
8 & 76.7 & 23 & 37.7660 & 21 & 72.1 & 20 & 35.3671 \\
9 & 70.7 & 21 & 31.9286 & 22 & 58.1 & 21 & 25.7301 \\
10 & 57.5 & 20 & 24.8575 & 23 & 44.6 & 20 & 19.9729 \\
11 & 46.4 & 20 & 21.0482 & 24 & 33.4 & 20 & 16.6504 \\
12 & 28.9 & 21 & 15.3141 & 25 & 28.6 & 22 & 16.5597 \\
13 & 28.1 & 21 & 15.2673 & & & & \\ \hline \end{tabular}
\end{table}
Table 6.1: Steam data As will be seen from the ANOVA table below, the \(t\) statistics have 22 degrees of freedom. There is a substantial effect for variable \(x_{1}\). The \(P\) value for \(\beta_{2}\) is approximately 0.10. The estimated covariance matrix for the parameter estimates is \(MSE\)\((X^{\prime}X)^{-1}\). _MSE_ is given in the ANOVA table below. The matrix \((X^{\prime}X)^{-1}\) is

\begin{tabular}{c|r r r}  & \(\beta_{0}\) & \(\beta_{1}\) & \(\beta_{2}\) \\ \hline \(\beta_{0}\) & 2.77875 & \(-0.01124\) & \(-0.10610\) \\ \(\beta_{1}\) & \(-0.01124\) & 0.00015 & 0.00018 \\ \(\beta_{2}\) & \(-0.10610\) & 0.00018 & 0.00479 \\ \end{tabular}

The analysis of variance table is

\begin{tabular}{l r r r r}  & \multicolumn{4}{c}{ANOVA} \\ Source & \(df\) & \(SS\) & \(MS\) & \(F\) \\ \hline \(\beta_{0}\) & 1 & 15270.78 & 15270.78 & \\ Regression & 2 & 1259.32 & 629.66 & 298 \\ Error & 22 & 46.50 & 2.11 & \\ \hline Total & 25 & 16576.60 & & \\ \end{tabular}

The \(F\) statistic is huge. There is a very significant effect due to fitting the regression variables (after fitting a mean value to the data). One breakdown of the sum of squares for regression is

\[SSR(X_{1}|J) = 1252.62\] \[SSR(X_{2}|X_{1},J) = 6.70.\]

#### Partitioned Model

We now partition the usual model matrix and parameter vector in order to get a multiple regression analogue of the alternative model for simple linear regression. This alternative model is often discussed in regression analysis and is necessary for establishing, in the next section, the relationship between multiple regression and best linear prediction. However, the results in this subsection do not actually require the linear model to be a regression, only that the model have an intercept term.

We rewrite the usual model \(Y=X\beta+e\) as

\[Y=[J,Z]\begin{bmatrix}\beta_{0}\\ \beta_{*}\end{bmatrix}+e,\]

where \(\beta_{*}=[\beta_{1},\ldots,\beta_{p-1}]^{\prime}\) and

\[Z=\begin{bmatrix}x_{11}&\cdots&x_{1p-1}\\ \vdots&\ddots&\vdots\\ x_{n1}&\cdots&x_{np-1}\end{bmatrix}\]

[MISSING_PAGE_EMPTY:5465]

Finally, from the formula for _SSReg_ developed earlier, the normal equations, and the fact that \(\hat{\gamma}_{*}=\hat{\beta}_{*}\), we get

\[SSReg = Y^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Z\left[Z^{\prime} \left(I-\frac{1}{n}J_{n}^{n}\right)Z\right]^{-}Z^{\prime}\left(I-\frac{1}{n}J_ {n}^{n}\right)Y\] \[= \hat{\beta}_{*}^{\prime}Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n} \right)Z\left[Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Z\right]^{-}Z^{ \prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Z\hat{\beta}_{*}\] \[= \hat{\beta}_{*}^{\prime}Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n} \right)Z\hat{\beta}_{*},\]

where the last equality follows from the definition of a generalized inverse.

#### Nonparametric Regression and Generalized Additive Models

In general, regression models assume \(y_{i}=m(x_{i})+e_{i}\), \(\mathrm{E}(e_{i})=0\), where \(x_{i}\) is a \(p\) vector of known predictor variables and \(m(\cdot)\) is some function. If \(m(x_{i})=x_{i}^{\prime}\beta\), the model is linear. If \(m(x_{i})=h(x_{i}^{\prime}\beta)\) for a known function \(h\), the model is generalized linear as discussed in Section 1.4. If \(m(x_{i})=h(x_{i};\,\beta)\), for a known \(h\) depending on the predictor variables \(x_{i}\) and some parameters \(\beta\), the model is nonlinear regression, see Christensen (2015, Chapter 23).

In _nonparametric regression_, \(m\) is not assumed to fall into any such parametric family. However, by making weak assumptions about \(m(x)\), one can often write it as \(m(x)=\sum_{j=1}^{\infty}\beta_{j}\phi_{j}(x)\) for some class of known functions \(\{\phi_{j}\}\). When \(x\) is a scalar, examples of such classes include polynomials, cosines, and wavelets. In practice, one fits a _linear model_

\[y_{i}=\sum_{j=1}^{p}\beta_{j}\phi_{j}(x_{i})+e_{i}, \tag{1}\]

where \(p\) is large enough to capture the interesting behavior of \(m(\cdot)\), see _ALM-III_, Chapter 1. Note that in the linear model, each of the terms \(\phi_{j}(x_{i})\) is a predictor variable, i.e., the \(j\)th column of the model matrix is \([\phi_{j}(x_{1}),\,\cdots,\,\phi_{j}(x_{n})]^{\prime}\).

This "_basis function_" approach to nonparametric regression involves fitting one large, complicated linear model. Other approaches involve fitting many simple linear models. For example, the _lowess fit (locally weighted scatterplot smoother)_ begins estimation of \(m(x)\) by fitting a weighted linear regression to some fraction of the \(x_{i}\)s that are nearest \(x\) with weights proportional to the distance from \(x\). The linear model is used only to estimate one point, \(m(x)\). One performs a separate fit for a large number of points \(x_{g}\), and to estimate all of \(m(x)\) just connect the dots in the graph of \([x_{g},\,\hat{m}(x_{g})]\). Kernel estimation is similar but the weights are determined by a kernel function and the estimate is just a weighted average, not the result of fitting a weighted line or plane.

Unfortunately, these methods quickly run into a "curse of dimensionality." With the basis function approach and only one predictor variable, it might take, for example, \(p=8\) functions to get an adequate approximation to \(m(\cdot)\). No problem! With 2 predictor variables, we could expect to need approximately \(p=8^{2}=64\) predictor variables in the linear model. Given a few hundred observations, this is doable. However, with 5 predictor variables, we could expect to need about \(p=8^{5}=32,768\) functions.

One way to get around this curse of dimensionality is to fit _generalized additive models_. For example, with 3 predictor variables, \(x=(x_{1},x_{2},x_{3})^{\prime}\), we might expect to need \(p=8^{3}=512\) terms to approximate \(m(\cdot)\). To simplify the problem, we might assume that \(m(\cdot)\) follows a generalized additive model such as

\[m(x)=f_{1}(x_{1})+f_{23}(x_{2},x_{3}) \tag{2}\]

or

\[m(x)=f_{12}(x_{1},x_{2})+f_{23}(x_{2},x_{3}). \tag{3}\]

If we need 8 terms to approximate \(f_{1}(\cdot)\) and 64 terms to approximate each of the \(f_{jk}(\cdot,\cdot)\)s, the corresponding linear model for (2) involves fitting only 72 predictor variables, and for model (3) only \(128-8=120\) rather than \(8^{3}=512\). (Fitting \(f_{12}\) and \(f_{23}\) typically duplicates fitting of an \(f_{2}\) term.)

With the same 8 term approximations and 5 predictor variables, a generalized additive model that includes all of the possible \(f_{jk}(\cdot,\cdot)\)s involves only 526 terms, rather than the 32,768 required by a full implementation of a nonparametric regression.

I should repeat that my use of 8 terms per dimension is merely an illustration. One might need 5 terms, or 10 terms, or 15 terms. And in two dimensions, one might need more or less than \(8^{2}\) terms. But these computations illustrate the magnitude of the problem. It should also be noted that with alternative (nonlinear) methods of fitting generalized additive models it may be necessary to fit lower order terms, i.e., instead of model (3), fit

\[m(x)=f_{1}(x_{1})+f_{2}(x_{2})+f_{3}(x_{3})+f_{12}(x_{1},x_{2})+f_{23}(x_{2},x_ {3}).\]

Another problem with the basis function approach is that it can lead to very strange results if the number of functions \(p\) being fitted is too large. It is well known, see for example Christensen (2015, Section 8.2), that fitting high order polynomials can be a bad thing to do. For \(x\) a scalar predictor, by a high order polynomial we mean a polynomial in which the number of polynomial terms \(p\) is close to the number of distinct \(x_{i}\) values in the data. If the \(x_{i}\) values are all distinct, a high order polynomial will fit every observed data point almost perfectly. The price of this almost perfect fit to the data is that _between the \(x_{i}\) values_ the polynomial can do very weird and inappropriate things. Thus, we have a model that will work poorly for future predictions. This behavior is not unique to polynomials. It occurs with all the standard classes of functions. (It seems that the problems can be ameliorated by fitting smaller models on subsets of the data, e.g. wavelets or splines, even though these retain the form in (1).) Similar problems occur when the \(x_{i}\) values are not all distinct.

_ALM-III_, Chapter 1 contains much more information on nonparametric regression.

### 6.3 General Prediction Theory

General prediction theory provides another approach to regression analysis. It ties in closely with linear model theory but also with generalized linear models, nonlinear regression, and nonparametric regression.

One of the great things about writing a technical book is that if you do enough good mathematics, people will put up with you spouting off once in a while. In this section, we take up the subject of prediction and its application to linear model theory. To me, prediction is what science is all about, and I cannot resist the urge to spout off. If you just want to get to work, skip down to the subsection headed "General Prediction."

#### Discussion

There is a fundamental philosophical conflict in statistics between Bayesians (who incorporate subjective beliefs into their analyses) and non-Bayesians. On the philosophical issues, the Bayesians seem to me to have much the better of the argument. (Yes, hard as it may be to believe, the author of this book is a philosophical Bayesian.) However, in the practice of 20th Century statistics, the non-Bayesians carried the day. Perhaps the most difficult philosophical aspect of Bayesian statistics for non-Bayesians to accept is the incorporation of subjective beliefs. Many scientists believe that objectivity is of paramount importance, and classical statistics maintains the semblance of objectivity. (In fact, classical statistics is rife with subjective inputs. Choosing an experimental design, choosing independent variables to consider in regression, and any form of data snooping such as choosing contrasts after looking at the data are examples.)

As Smith (1986) has pointed out, this concept of objectivity is very elusive. Objectivity is almost indistinguishable from the idea of consensus. If all the "clear thinking" people agree on something, then the consensus is (for the time being) "objective" reality.

Fortunately, the essence of science is not objectivity; it is repeatability. The object of scientific statistical inference is not the examination of parameters (too often created by and for statisticians so that they have something to draw inferences about). The object of scientific statistical inference is the (correct) prediction of future observable events. (I bet you were wondering what all this had to do with prediction.) Parameters are at best a convenience, and parameters are at their best when they are closely related to prediction (e.g., probabilities of survival). Geisser (1971, 1993) gives excellent discussions of the predictive approach.

In this book the emphasis has been placed on models rather than on parameters. Now you know why. Models can be used for prediction. They are an endproduct. Parameters are an integral part of most models, but they are a tool and not an end in themselves. Christensen (1995) gives a short discussion on the relation among models, prediction, and testing. Having now convinced a large portion of the statistical community of the unreliability of my ideas, I shall return to the issue at hand.

#### General Prediction

Suppose we have random variables \(y\), \(x_{1}\), \(x_{2}\),..., \(x_{p-1}\). Regression can be viewed as the problem of predicting \(y\) from the values of \(x_{1}\),..., \(x_{p-1}\). We will examine this problem and consider its relationship to the linear model theory approach to regression. Let \(x\) be the vector \(x=(x_{1}\),..., \(x_{p-1})^{\prime}\). A reasonable criterion for choosing a predictor of \(y\) is to pick a predictor \(f(x)\) that minimizes the mean squared error, \(\mbox{E}[y\,-\,f(x)]^{2}\). (The _MSE_ defined in linear model theory is a function of the observations that estimates the theoretical mean squared error defined here.) Note that, unlike standard linear model theory, the expected value is taken over the joint distribution of \(y\) and \(x\).

The use of an expected squared error criterion presupposes the existence of first and second moments for \(y\) and \(f(x)\). Let \(\mbox{E}(y)\equiv\mu_{y}\), \(\mbox{Var}(y)\equiv\sigma_{y}^{2}\equiv\sigma_{yy}\) and let \(\mbox{E}[f(x)]\equiv\mu_{f}\), \(\mbox{Var}[f(x)]\equiv\sigma_{ff}\), and \(\mbox{Cov}[y\), \(f(x)]\equiv\sigma_{yf}\), with similar notations for other functions of \(x\), e.g., \(m(x)\) has \(\sigma_{ym}\equiv\mbox{Cov}[y\), \(m(x)]\).

The remainder of this section consists of three subsections. The next examines best predictors, i.e., those functions \(f(x)\) that do the best job of predicting \(y\). Without knowing (or being able to estimate) the joint distribution of \(x\) and \(y\), we cannot find the best predictor, so in Subsection 4 we examine the best predictors among functions \(f(x)\) that are restricted to be linear functions of \(x\). These best linear predictors depend only on the means and covariances of the random variables and are thus relatively easy to estimate. Subsection 4 also examines the relationship between best linear predictors and least squares estimation. Both the best predictor and the best linear predictor can be viewed as orthogonal projections in an appropriate vector space, a subject that is commented on earlier but is amplified in Subsection 5.

#### Best Prediction

We now establish that the best predictor is the conditional expectation of \(y\) given \(x\). See Appendix D for definitions and results about conditional expectations.

**Theorem 6.3.1**: _Let \(m(x)\equiv\mathrm{E}(y|x)\). Then for any other predictor \(f(x)\), \(\mathrm{E}{\mathrm{I}}y-m(x)]^{2}\leq\mathrm{E}[y-f(x)]^{2}\); thus \(m(x)\) is the best predictor of \(y\)._

_Proof_

\[\mathrm{E}[y-f(x)]^{2} =\mathrm{E}[y-m(x)+m(x)-f(x)]^{2}\] \[=\mathrm{E}[y-m(x)]^{2}+\mathrm{E}[m(x)-f(x)]^{2}+2\mathrm{E}[[y- m(x)][m(x)-f(x)]].\]

Since both \(\mathrm{E}[y-m(x)]^{2}\) and \(\mathrm{E}[m(x)-f(x)]^{2}\) are nonnegative, it is enough to show that \(\mathrm{E}\{[y-m(x)][m(x)-f(x)]\}=0\). Consider this expectation conditional on \(x\).

\[\mathrm{E}\{[y-m(x)][m(x)-f(x)]\} =\mathrm{E}\big{(}\mathrm{E}\{[y-m(x)][m(x)-f(x)]|x\}\big{)}\] \[=\mathrm{E}\big{(}[m(x)-f(x)]\mathrm{E}\{[y-m(x)]|x\}\big{)}\] \[=\mathrm{E}\big{(}[m(x)-f(x)]0\big{)}=0\]

where \(\mathrm{E}\{[y-m(x)]|x\}=0\) because \(\mathrm{E}(y|x)=m(x)\). 

The goal of most predictive procedures is to find, or rather estimate, the function \(\mathrm{E}(y|x)\equiv m(x)\). Suppose we have a random sample \((x_{i}^{\prime},\,y_{i})\), \(i=1,\,\ldots,\,n\). In linear regression, usually \(m(x_{i})=\alpha+x_{i}^{\prime}\beta\) with \(\alpha\) and \(\beta\) unknown. A generalized linear model assumes a distribution for \(y\) given \(x\) and that \(\mathrm{E}(y_{i}|x_{i})=m(x_{i})=h(\alpha+x_{i}^{\prime}\beta)\) for known \(h\) and unknown \(\alpha\) and \(\beta\). Here \(h\) is just the inverse of the link function. The standard nonlinear regression model is a more general version of these. It uses \(m(x_{i})=h(x_{i};\,\alpha,\,\beta)\), where \(h\) is some known function but \(\alpha\) and \(\beta\) are unknown. The conditional mean structure of all three parametric models is that of the nonlinear regression model: \(m(x)=h(x;\,\alpha,\,\beta)\), \(h\) known. We then become interested in estimating \(\alpha\) and \(\beta\). Evaluating whether we have the correct "known" form for \(h\) is a question of whether lack of fit exists, see Section 7. Nonparametric regression is unwilling to assume a functional form \(h(x;\,\alpha,\,\beta)\). The standard nonparametric regression model is \(y_{i}=m(x_{i})+e_{i}\) where, conditional on the \(x_{i}\)s, the \(e_{i}\)s are independent with mean \(0\) and variance \(\sigma^{2}\). In nonparametric regression, \(m\) is completely unknown, but often it is assumed to be continuous over a closed bounded interval.

All of these versions of regression involve estimating whatever parts of \(m(x)\) are not assumed to be known. On the other hand, best prediction _theory_ treats \(m(x)\) as a known function, so for models involving \(\alpha\) and \(\beta\) it treats them as known.

##### Residuals

In Theorem 6.3.3, we present a result that does two things. First, it provides a justification for the residual plots used in Chapter 12 to identify lack of fit. Second, as discussed in Subsection 5, it establishes that \(\mathrm{E}(y|x)\) can be viewed as the perpendicular projection of \(y\) into the space of random variables, say \(f(x)\), that are functions of \(x\) alone, have mean \(\mathrm{E}\{f(x)\}=E[y]\), and a finite variance, see also deLaubenfels(2006). Before doing this, we need to establish some covariance and correlation properties.

**Proposition 6.3.2**.: \(\operatorname{Cov}[y,\,f(x)]=\operatorname{Cov}[m(x),\,f(x)]\)_. In particular, \(\operatorname{Cov}[y,\,m(x)]=\operatorname{Var}[m(x)]=\sigma_{mm}\) and \(\operatorname{Corr}^{2}[y,\,m(x)]=\sigma_{mm}/\sigma_{yy}\)._

Proof.: Recall that, from the definition of conditional expectation, \(\operatorname{E}[m(x)]=\mu_{y}\).

\[\operatorname{Cov}[y,\,f(x)] =\operatorname{E}_{yx}\left[[y-\mu_{y}]f(x)\right\}\] \[=\operatorname{E}_{yx}\left[[y-m(x)+m(x)-\mu_{y}]f(x)\right\}\] \[=\operatorname{E}_{yx}\left[[y-m(x)]f(x)\right]+\operatorname{E} _{yx}\left[[m(x)-\mu_{y}]f(x)\right\}\] \[=\operatorname{E}_{x}\left(\operatorname{E}_{y|x}\left[[y-m(x)]f( x)\right]\right)+\operatorname{E}_{x}\left(\operatorname{E}_{y|x}\left[[m(x)-\mu_{y}]f(x) \right]\right)\] \[=\operatorname{E}_{x}(0)+\operatorname{E}_{x}\left[[m(x)-\mu_{y}] f(x)\right\}\] \[=\operatorname{Cov}[m(x),\,f(x)].\qed\]

Now consider an arbitrary predictor \(\tilde{y}(x)\) and its residual \(y-\tilde{y}(x)\). We show that \(\tilde{y}(x)\) is the best predictor if and only if it is unbiased and its residuals are uncorrelated with any function of \(x\).

**Theorem 6.3.3**.: _Let \(\tilde{y}(x)\) be any predictor, then \(\operatorname{E}[\tilde{y}(x)]=\mu_{y}\) and \(\operatorname{Cov}[f(x),\,y-\tilde{y}(x)]=0\) for any function \(f\) if and only if \(\operatorname{E}(y|x)=\tilde{y}(x)\) almost surely._

Proof.: \(\Leftarrow\operatorname{If}\operatorname{E}(y|x)\equiv m(x)=\tilde{y}(x), \operatorname{E}[\tilde{y}(x)]=\mu_{y}\) and an immediate consequence of Proposition 2 is that \(\operatorname{Cov}[f(x),\,y-m(x)]=0\) for any function \(f\).

\(\Rightarrow\) Now suppose that \(\operatorname{E}[\tilde{y}(x)]=\mu_{y}\) and \(\operatorname{Cov}[f(x),\,y-\tilde{y}(x)]=0\) for any function \(f\). To show that \(\tilde{y}(x)=m(x)\) a.s., it is enough to note that \(\operatorname{E}[\tilde{y}(x)-m(x)]=\mu_{y}-\mu_{y}=0\) and to show that \(\operatorname{Var}[\tilde{y}(x)-m(x)]=0\). Thinking of \(f(x)=\tilde{y}(x)-m(x)\) in the covariance conditions, observe that by Proposition 2 and the assumption of the theorem,

\[\operatorname{Var}[\tilde{y}(x)-m(x)] =\operatorname{Cov}[\tilde{y}(x)-m(x),\,\tilde{y}(x)-m(x)]\] \[=\operatorname{Cov}[[y-m(x)]-[y-\tilde{y}(x)],[\tilde{y}(x)-m(x)]]\] \[=\operatorname{Cov}[[y-m(x)],[\tilde{y}(x)-m(x)]]-\operatorname{ Cov}[[y-\tilde{y}(x)],[\tilde{y}(x)-m(x)]]\] \[=0-0.\qed\]

In fitting standard linear models, or any other regression procedure, we typically obtain fitted values \(\hat{y}_{i}\) corresponding to the observed data \(y_{i}\), from which we can obtain residuals \(y_{i}-\hat{y}_{i}\). According to Theorem 6.3.3, if the fitted values are coming from the best predictor, plotting the residuals against any function of the predictor vector \(x_{i}\) should display zero correlation. Thus, if we plot the residuals against some function \(f(x_{i})\) of the predictors and observe a correlation, we obviously do not have the best predictor, so we should try fitting some other model. In particular, regardless of how nonlinear the original regression model might have been, adding a linear term to the model using \(f(x_{i})\) as the predictor should improve the fit of the model. Unfortunately, there is no reason to think that adding such a linear term will get you to the best predictor. Finally, it should be noted that a common form of lack of fit detected in residual plots is a parabolic shape, which does not necessarily suggest a nonzero correlation with the predictor used in the plot. However, when the residual plot is a parabola, a plot of the residuals versus the (suitably centered) squared predictor will display a nonzero correlation.

Finally, if we plot the residuals against some predictor variable \(z\) that is not part of \(x\), the fact that we would make such a plot suggests that we really want the best predictor \(m(x,z)\) rather than \(m(x)\), although if we originally left \(z\) out, we probably suspect that \(m(x)=m(x,z)\). A linear relationship between the residuals and \(z\) indicates that the estimated regression function \(\hat{m}(x)\) is not an adequate estimate of the best predictor \(m(x,z)\).

##### Other Loss Functions

Our focus has been, and will continue to be, on squared error prediction loss. However, there are a number of other loss functions (somewhat less) commonly used for prediction. These include weighted squared error \(L[y,\tilde{y}(x)]=w(y)[y-\tilde{y}(x)]^{2}\) wherein \(w(y)>0\), absolute error \(L[y,\tilde{y}(x)]=|y-\tilde{y}(x)|\), and, when \(y\) is binary, _Hamming_ loss \(L[y,\tilde{y}(x)]=I[y\neq\tilde{y}(x)]\) wherein \(I\)(statement) is 1 if the statement is true and 0 if the statement is false. [http://www.stat.unm.edu/~fletcher/DecisionTheory.pdf](http://www.stat.unm.edu/~fletcher/DecisionTheory.pdf) links to a document "Decision Theory and Prediction" that proves the following statements.

* For data \((x^{\prime},y)\), \(y\in\mathbf{R}\), and \(L[y,\tilde{y}(x)]=w(y)[y-\tilde{y}(x)]^{2}\), the best predictor is \(\hat{y}=\text{E}[w(y)y|x]/\text{E}[w(y)|x]\).
* For data \((x^{\prime},y)\), \(y\in\mathbf{R}\), and \(L[y,\tilde{y}(x)]=|y-\tilde{y}(x)|\), the best predictor is \(\hat{y}\equiv\text{Median}(y|x)\).
* For data \((x^{\prime},y)\), \(y\in\{0,1\}\) and \(L[y,\tilde{y}(x)]=I[y\neq\tilde{y}(x)]\), the best predictor is \[\hat{y}(x)\equiv\begin{cases}0&\text{if }\Pr(y=0|x)>0.5\\ 1&\text{if }\Pr(y=0|x)<0.5.\end{cases}\]

The proofs of these results are identical to proofs that establish the optimal Bayesian estimators for the first two loss functions and the optimal Bayesian hypothesis test in the last case.

Binary logistic regression involves \(y\in\{0,1\}\) and seeks to estimate \(\Pr(y=1|x)\). Since this determines the entire (estimated) conditional distribution of \(y\) given \(x\), it leads immediately to estimated best predictors under any of the predictive loss functions.

#### Best Linear Prediction

The ideas of best linear prediction and best linear unbiased prediction (see Section 6) are very important. As will be seen here (and in _ALM-III_), best linear prediction theory has important applications in standard linear models. The theory has traditionally been taught as part of multivariate analysis (cf. Anderson 2003). It is important for general stochastic processes (cf. Doob 1953), mixed linear models (cf. McCulloch et al. 2008; Hodges 2013; or _ALM-III_, Chapter 5), time series (cf. Shumway and Stoffer 2011; Brockwell and Davis 1991; or _ALM-III_, Chapters 6, 7), spatial data (cf. Cressie 1993; Cressie and Wikle 2011; Ripley 1981; or _ALM-III_, Chapter 8), principal component analysis (cf. Johnson and Wichern 2007 or _ALM-III_, Chapter 14), and it is the basis for linear Bayesian methods (cf. Hartigan 1969).

In order to use the results on best prediction, one needs to know \(\mathrm{E}(y|x)\), which generally requires knowledge of the joint distribution of \((y,x_{1},x_{2},\ldots,x_{p-1})^{\prime}\). If the joint distribution is not available but the means, variances, and covariances are known, we can find the best linear predictor of \(y\). We seek a linear predictor \(\alpha+x^{\prime}\beta\) that minimizes \(\mathrm{E}[y-\alpha-x^{\prime}\beta]^{2}\) for all scalars \(\alpha\) and \((p-1)\times 1\) vectors \(\beta\).

In addition to our earlier assumptions that first and second moments for \(y\) exist, we now also assume the existence of \(\mathrm{E}(x)\equiv\mu_{x}\), \(\mathrm{Cov}(x)\equiv V_{xx}\), and \(\mathrm{Cov}(x,y)\equiv V_{xy}=V_{yx}^{\prime}\equiv\mathrm{Cov}(y,x)^{\prime}\).

Let \(\beta_{*}\) be a solution to \(V_{xx}\beta=V_{xy}\), then we will show that the function

\[\hat{E}(y|x)\equiv\mu_{y}+(x-\mu_{x})^{\prime}\beta_{*}\]

is a best linear predictor of \(y\) based on \(x\). \(\hat{E}(y|x)\) is also called _the linear expectation of \(y\) given \(x\)_. (Actually, it is the linear expectation of \(y\) given \(x\) and a random variable that is constant with probability 1.) Note that when \(V_{xx}\) is singular, there are an infinite number of vectors \(\beta_{*}\) that solve \(V_{xx}\beta=V_{xy}\), but by using Lemma 1.3.5 we can show that all such solutions give the same best linear predictor with probability 1. The idea is that since \((x-\mu_{x})\in C(V_{xx})\) with probability 1, for some random \(b\), \((x-\mu_{x})=V_{xx}b\) with probability 1, so

\[(x-\mu_{x})^{\prime}\beta_{*}=b^{\prime}V_{xx}\beta_{*}=b^{\prime}V_{xy},\]

which does not depend on the choice of \(\beta_{*}\) with probability 1.

**Theorem 6.3.4**: \(\hat{E}(y|x)\) _is a best linear predictor of \(y\)._

_Proof_ Define \(\eta\) so that \(\alpha=\eta-\mu_{x}^{\prime}\beta\). An arbitrary linear predictor is \(f(x)\equiv\alpha+x^{\prime}\beta=\eta+(x-\mu_{x})^{\prime}\beta\).

\[\mathrm{E}[y-f(x)]^{2} = \mathrm{E}[y-\hat{E}(y|x)+\hat{E}(y|x)-f(x)]^{2}\] \[= \mathrm{E}[y-\hat{E}(y|x)]^{2}+\mathrm{E}[\hat{E}(y|x)-f(x)]^{2}\] \[+2\mathrm{E}[\{y-\hat{E}(y|x)\}[\hat{E}(y|x)-f(x)]\}.\]If we show that \(\mathrm{E}\{[y-\hat{E}(y|x)][\hat{E}(y|x)-f(x)]\}=0\), the result follows almost immediately. In that case,

\[\mathrm{E}[y-f(x)]^{2}=\mathrm{E}[y-\hat{E}(y|x)]^{2}+\mathrm{E}[\hat{E}(y|x)-f( x)]^{2}.\]

To find \(f(x)\) that minimizes the left-hand side, observe that both terms on the right are nonnegative, the first term does not depend on \(f(x)\), and the second term is minimized by taking \(f(x)=\hat{E}(y|x)\).

We now show that \(\mathrm{E}\{[y-\hat{E}(y|x)][\hat{E}(y|x)-f(x)]\}=0\).

\[\mathrm{E}\{[y-\hat{E}(y|x)][\hat{E}(y|x)-f(x)]\}\] \[=\mathrm{E}\big{(}\{y-[\mu_{y}+(x-\mu_{x})^{\prime}\beta_{*}]\}[ \{\mu_{y}+(x-\mu_{x})^{\prime}\beta_{*}]-[\eta+(x-\mu_{x})^{\prime}\beta]\} \big{)}\] \[=\mathrm{E}\big{(}\{(y-\mu_{y})-(x-\mu_{x})^{\prime}\beta_{*}\} \{(\mu_{y}-\eta)+(x-\mu_{x})^{\prime}(\beta_{*}-\beta)\}\] \[=\mathrm{E}\big{[}(y-\mu_{y})(\mu_{y}-\eta)-(x-\mu_{x})^{\prime} \beta_{*}(\mu_{y}-\eta)\] \[\quad+(y-\mu_{y})(x-\mu_{x})^{\prime}(\beta_{*}-\beta)-(x-\mu_{x} )^{\prime}\beta_{*}(x-\mu_{x})^{\prime}(\beta_{*}-\beta)\big{]}\] \[=(\mu_{y}-\eta)\mathrm{E}[(y-\mu_{y})]-\mathrm{E}[(x-\mu_{x})^{ \prime}]\beta_{*}(\mu_{y}-\eta)\] \[\quad+\mathrm{E}[(y-\mu_{y})(x-\mu_{x})^{\prime}](\beta_{*}-\beta )-\mathrm{E}[\beta_{*}^{\prime}(x-\mu_{x})(x-\mu_{x})^{\prime}(\beta_{*}-\beta)]\] \[=0-0+V_{yx}(\beta_{*}-\beta)-\beta_{*}^{\prime}\mathrm{E}[(x-\mu_ {x})(x-\mu_{x})^{\prime}](\beta_{*}-\beta)\] \[=V_{yx}(\beta_{*}-\beta)-\beta_{*}^{\prime}V_{xx}(\beta_{*}-\beta).\]

However, by definition, \(\beta_{*}^{\prime}V_{xx}=V_{yx}\); so

\[V_{yx}(\beta_{*}-\beta)-\beta_{*}^{\prime}V_{xx}(\beta_{*}-\beta)=V_{yx}(\beta _{*}-\beta)-V_{yx}(\beta_{*}-\beta)=0.\qed\]

It is of interest to note that if \((y,x^{\prime})^{\prime}\) has a multivariate normal distribution, then the best linear predictor is the best predictor. Morrison (2004) contains a discussion of conditional expectations for multivariate normals.

The following proposition will be used in Section 6 to develop the theory of best linear unbiased prediction.

**Proposition 6.3.5**: \(\mathrm{E}[y-\alpha-x^{\prime}\beta]^{2}=\mathrm{E}[y-\hat{E}(y|x)]^{2}+ \mathrm{E}[\hat{E}(y|x)-\alpha-x^{\prime}\beta]^{2}\)_._

_Proof_ The result is part of the proof of Theorem 6.3.4. \(\qed\)

We show that best linear predictors are essentially unique. In other words, we show that to minimize \(\mathrm{E}[\hat{E}(y|x)-\eta-(x-\mu_{x})^{\prime}\beta]^{2}\), you need \(\mu_{y}=\eta\) and \(\beta\) must be a solution to \(V_{xx}\beta=V_{xy}\). It is not difficult to show that

\[\mathrm{E}[\hat{E}(y|x)-\eta-(x-\mu_{x})^{\prime}\beta]^{2}=(\mu_{y}-\eta)^{2 }+\mathrm{E}[(x-\mu_{x})^{\prime}\beta_{*}-(x-\mu_{x})^{\prime}\beta]^{2}.\]

Clearly, minimization requires \(\mu_{y}=\eta\) and \(\mathrm{E}[(x-\mu_{x})^{\prime}\beta_{*}-(x-\mu_{x})^{\prime}\beta]^{2}=0\). The best linear predictors will be essentially unique if we can show that \(\mathrm{E}[(x-\mu_{x})^{\prime}\beta_{*}-(x-\mu_{x})^{\prime}\beta]^{2}=0\) implies that \(\beta\) must be a solution to \(V_{xx}\beta=V_{xy}\). Observe that \[\begin{split}\mathrm{E}[(x-\mu_{x})^{\prime}\beta_{*}-(x-\mu_{x})^{ \prime}\beta]^{2}&=\mathrm{E}[(x-\mu_{x})^{\prime}(\beta_{*}-\beta) ]^{2}\\ &=\mathrm{Cov}[(x-\mu_{x})^{\prime}(\beta_{*}-\beta)]\\ &=(\beta_{*}-\beta)^{\prime}V_{xx}(\beta_{*}-\beta).\end{split}\]

Write \(V_{xx}=Q\,Q^{\prime}\) with \(C(V_{xx})=C(Q)\). Then \((\beta_{*}-\beta)^{\prime}V_{xx}(\beta_{*}-\beta)=0\) if and only if \((\beta_{*}-\beta)^{\prime}Q\,Q^{\prime}(\beta_{*}-\beta)=0\) if and only if \(Q^{\prime}(\beta_{*}-\beta)=0\) if and only if \((\beta_{*}-\beta)\perp C(Q)=C(V_{xx})\) if and only if \(V_{xx}(\beta_{*}-\beta)=0\) if and only if \(V_{xx}\beta=V_{xx}\beta_{*}=V_{xy}\). So \(\beta\) must be a solution.

The variance of the prediction error \(y-\hat{E}(y|x)\) is given in Section 5. (Actually, the covariance matrix for a bivariate prediction is given.)

##### Relation to Least Squares Estimation

Next, we examine the correspondence between this theory and linear model regression theory. Suppose we have \(n\) observations on the vector \((y,x^{\prime})^{\prime}=(y,x_{1},x_{2},\ldots,x_{p-1})^{\prime}\). We can write these as \((y_{i},x_{i}^{\prime})^{\prime}=(y_{i},x_{i1},x_{i2},\ldots,x_{i,p-1})^{\prime}\), \(i=1,\ldots,n\). In matrix notation write \(Y=(y_{1},y_{2},\ldots,y_{n})^{\prime}\) and \(Z=[x_{ij}]\), \(i=1,\ldots,n\), \(j=1,\ldots,p-1\). (\(Z\) plays the same role as \(Z\) did in Section 2 on multiple regression.) The usual unbiased estimates for \(V_{xx}\) and \(V_{xy}\) can be written as

\[\begin{split} S_{xx}&=\frac{1}{n-1}\,Z^{\prime} \left(I-\frac{1}{n}\,J_{n}^{n}\right)Z&=\frac{1}{n-1}\,\sum_{i=1 }^{n}(x_{i}-\bar{x}.)(x_{i}-\bar{x}.)^{\prime}\\ S_{xy}&=\frac{1}{n-1}\,Z^{\prime}\left(I-\frac{1}{n }\,J_{n}^{n}\right)Y&=\frac{1}{n-1}\,\sum_{i=1}^{n}(x_{i}-\bar{x}.)(y_{i}-\bar{y}.).\end{split}\]

The usual unbiased estimates of \(\mu_{y}\) and \(\mu_{x}\) are

\[\begin{split}\bar{y}.&=\frac{1}{n}\,J_{1}^{n}\,Y= \frac{1}{n}\sum_{i=1}^{n}y_{i}\\ \bar{x}^{\prime}&=\frac{1}{n}\,J_{1}^{n}\,Z=\left( \frac{1}{n}\,\sum_{i=1}^{n}x_{i1},\ldots,\frac{1}{n}\,\sum_{i=1}^{n}x_{ip-1} \right)^{\prime}.\end{split}\]

The obvious estimate for the best linear predictor of \(y\) is \[\hat{y}=\bar{y}.+(x-\bar{x}.)^{\prime}\hat{\beta}_{*},\]

where \(\hat{\beta}_{*}\) is a solution to \(S_{xx}\beta_{*}=S_{xy}\), i.e., it solves \(Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Z\beta_{*}=Z^{\prime}\left(I- \frac{1}{n}J_{n}^{n}\right)Y\). From the results of Section 2, \(\bar{y}.=\hat{\gamma}_{0}\) and any solution of \(Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)\)\(Z\beta_{*}=Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Y\) is a least squares estimate of \(\beta_{*}=\gamma_{*}\). Thus, the natural estimates of the parameters in the best linear predictor are the least squares estimates from the mean corrected regression model considered in the previous section.

##### 6.3.4.2 Residuals

Finally, we include a result for best linear predictors that is analogous to Theorem 6.3.3 for best predictors. First, if we have residuals from the best linear predictor, they will be uncorrelated with any linear combination of the predictor variables. Second, \(\hat{E}(y|x)\) can be viewed as the perpendicular projection of \(y\) into the space of random variables \(f(x)\) that are linear functions of \(x\) and have \(\operatorname{E}[f(x)]=E[y]\), cf. Subsection 5.

**Theorem 6.3.6**: _Suppose \(\bar{y}(x)\) is any linear predictor, then \(\operatorname{E}[\tilde{y}(x)]=\mu_{y}\) and \(\operatorname{Cov}[f(x),\,y-\tilde{y}(x)]=0\) for any linear function \(f\) if and only if \(\hat{E}(y|x)=\tilde{y}(x)\) almost surely._

_Proof_ Let \(f(x)\equiv\eta+(x-\mu_{x})^{\prime}\beta\) for some \(\eta\) and \(\beta\).

\(\Leftarrow\) For \(\hat{E}(y|x)=\tilde{y}(x)\), \(\operatorname{E}[\tilde{y}(x)]=\mu_{y}\) and

\[\operatorname{Cov}[\eta+(x-\mu_{x})^{\prime}\beta,(y-\mu_{y})-(x- \mu_{x})^{\prime}\beta_{*}] =\beta^{\prime}V_{xy}-\beta^{\prime}V_{xx}\beta_{*}\] \[=\beta^{\prime}V_{xy}-\beta^{\prime}V_{xy}=0.\]

\(\Rightarrow\) From \(\operatorname{E}[\tilde{y}(x)]=\mu_{y}\), we can write \(\tilde{y}=\mu_{y}+(x-\mu_{x})^{\prime}\delta\) for some \(\delta\). If

\[0 = \operatorname{Cov}[\eta+(x-\mu_{x})^{\prime}\beta,(y-\mu_{y})-(x- \mu_{x})^{\prime}\delta]\] \[= \beta^{\prime}V_{xy}-\beta^{\prime}V_{xx}\delta\]

for any \(\beta\), then \(V_{xy}=V_{xx}\delta\), so \(\tilde{y}(x)=\hat{E}(y|x)\) with probability 1. \(\square\)

As mentioned earlier, the theory of best linear predictors also comes up in developing the theory of best linear unbiased predictors (BLUPs), which is an important subject in models with dependent \(y_{i}\)s, such as random effects models and spatial data models. In random effects models, part of the \(\beta\) vector in \(Y=X\beta+e\) is assumed to be random and unobservable. BLUPs are discussed in Section 6. Random effects models and spatial models are mentioned in Section 6 and discussed more fully in _ALM-III_.

The result of the next exercise will be used in Section 5 on partial correlations.

**Exercise 6.3**  For predicting \(y=(y_{1},\ldots,y_{q})^{\prime}\) from \(x=(x_{1},\ldots,x_{p-1})^{\prime}\) we say that a predictor \(f(x)\) is best if the scalar \(\operatorname{E}\{[y-f(x)]^{\prime}[y-f(x)]\}\) is minimized. Show that with simple modifications, Theorems 6.3.1 and 6.3.4 hold for the extended problem, as does Proposition 6.3.5.

#### Inner Products and Orthogonal Projections in General Spaces

In most of this book, we define orthogonality and length using the Euclidean inner product in \(\mathbf{R}^{n}\). For two vectors \(x\) and \(y\), the Euclidean inner product is \(x^{\prime}y\), so \(x\perp y\) if \(x^{\prime}y=0\) and the length of \(x\) is \(\|x\|=\sqrt{x^{\prime}x}\). In Section B.3 we discussed, in detail, perpendicular projection operators relative to this inner product. We established that the projection of a vector \(Y\) into \(C(X)\) is \(\hat{Y}\equiv MY\). It also follows from Theorems 2.2.1 and 2.8.1 that \(\hat{Y}\) is the unique vector in \(C(X)\) that satisfies \((Y-\hat{Y})\perp C(X)\). This last property is sometimes used to define what it means for \(\hat{Y}\) to be the perpendicular projection of \(Y\) into \(C(X)\). We use this concept to extend the application of perpendicular projections to more general vector spaces.

More generally in \(\mathbf{R}^{n}\), we can use any positive definite matrix \(W\) to define an inner product between \(x\) and \(y\) as \(x^{\prime}Wy\). As before, \(x\) and \(y\) are orthogonal if their inner product \(x^{\prime}Wy\) is zero and the length of \(x\) is the square root of its inner product with itself, now \(\|x\|_{W}\equiv\sqrt{x^{\prime}Wx}\). As argued in Section B.3, any idempotent matrix is always a projection operator, but which one is the perpendicular projection operator depends on the inner product. As can be seen from Proposition 2.7.2 and Exercise 2.5, the matrix \(A\equiv X(X^{\prime}WX)^{-}X^{\prime}W\) is an oblique projection operator onto \(C(X)\) for the Euclidean inner product, but it is the perpendicular projection operator onto \(C(X)\) with the inner product defined using the matrix \(W\). It is not too difficult to see that \(AY\) is the unique vector in \(C(X)\) that satisfies \((Y-AY)\perp_{W}C(X)\), i.e., \((Y-AY)^{\prime}WX=0\).

These ideas can be applied in very general spaces. In particular, they can be applied to the concepts of prediction introduced in this section. For example, we can define the inner product between two random variables \(y\) and \(x\) with mean \(0\) and finite variances as the \(\operatorname{Cov}(x,y)\). In this case, \(\operatorname{Var}(x)\) plays the role of the squared length of the random variable and the standard deviation is the length. Two random variables are orthogonal if they are uncorrelated.

Now consider a vector of random variables \(x=(x_{1},\ldots,x_{p-1})^{\prime}\) and the space of all functions \(f(x)\) into \(\mathbf{R}^{1}\) that have mean \(0\) and variances. We showed in Theorem 6.3.3 that \(m(x)\equiv\operatorname{E}(y|x)\) is the unique function of \(x\) having mean \(\mu_{y}\) for which \(\operatorname{Cov}[y-m(x),\)\(f(x)]=0\) for any \(f(x)\). Thus, as alluded to above, \(m(x)-\mu_{y}\) satisfies a property often used to _define_ the perpendicular projection of \(y-\mu_{y}\) into the space of functions of \(x\) that have mean \(0\) and variances. Alternatively, we can think of \(m(x)\) as the perpendicular projection of \(y\) into the space of functions of \(x\) that have mean \(\mu_{y}\) and variances.

We can also consider a reduced space of random variables, the linear functions of \(x\), i.e., \(f(x)=\alpha+x^{\prime}\beta\). In Theorem 6.3.6 we show that \(\operatorname{Cov}[y-\hat{E}(y|x),\alpha+x^{\prime}\beta]=0\) for any linear function of \(x\), so once again, the best linear predictor is the perpendicular projection of \(y\) into the linear functions of \(x\) with mean \(\mu_{y}\).

We now generalize the definitions of an inner product space, orthogonality, and orthogonal projection.

**Definition A.15** (Alternate): A vector space \(\mathcal{X}\) is an inner product space if for any \(x,\,y\in\mathcal{X}\), there exists a symmetric bilinear function \(\langle x,\,y\rangle\) into \(\mathbf{R}\) with \(\langle x,\,x\rangle>0\) for any \(x\neq 0\) and \(\langle x,\,x\rangle=0\) when \(x=0\). A bilinear function has the properties that for any scalars \(a_{1}\), \(a_{2}\) and any vectors \(x_{1}\), \(x_{2}\), \(y\), \(\langle a_{1}x_{1}+a_{2}x_{2},\,y\rangle=a_{1}\langle x_{1},\,y\rangle+a_{2} \langle x_{2},\,y\rangle\) and \(\langle y,\,a_{1}x_{1}+a_{2}x_{2}\rangle=a_{1}\langle y,\,x_{1}\rangle+a_{2} \langle y,\,x_{2}\rangle\). A symmetric function has \(\langle x_{1},\,x_{2}\rangle=\langle x_{2},\,x_{1}\rangle\). The vectors \(x\) and \(y\) are orthogonal if \(\langle x,\,y\rangle=0\) and the squared length of \(x\) is \(\langle x,\,x\rangle\). The perpendicular projection of \(y\) into a subspace \(\mathcal{X}_{0}\) of \(\mathcal{X}\) is defined as the unique vector \(y_{0}\in\mathcal{X}_{0}\) with the property that \(\langle y-y_{0},\,x\rangle=0\) for any \(x\in\mathcal{X}_{0}\).

Note that the set of mean zero, finite variance, real-valued functions of random \(x\) and \(y\) form a vector space under Definition A.1 and an inner product space using the covariance of any two such functions as the inner product. Both the set of mean \(0\), finite variance functions of \(x\) and the set of mean \(0\) linear functions of \(x\) are subspaces, so \(y\) can be projected into either subspace.

We now relate Definition A.15 (Alternate) to the concept of a perpendicular projection operator.

**Exercise 6.4**: Consider an inner product space \(\mathcal{X}\) and a subspace \(\mathcal{X}_{0}\). Any vector \(y\in\mathcal{X}\) can be written uniquely as \(y=y_{0}+y_{1}\) with \(y_{0}\in\mathcal{X}_{0}\) and \(y_{1}\perp\mathcal{X}_{0}\). Let \(M(x)\) be a linear operator on \(\mathcal{X}\) in the sense that for any \(x\in\mathcal{X}\), \(M(x)\in\mathcal{X}\) and for any scalars \(a_{1}\), \(a_{2}\) and any vectors \(x_{1}\), \(x_{2}\), \(M(a_{1}x_{1}+a_{2}x_{2})=a_{1}M(x_{1})+a_{2}M(x_{2})\). \(M(x)\) is defined to be a perpendicular projection operator onto \(\mathcal{X}_{0}\) if for any \(x_{0}\in\mathcal{X}_{0}\), \(M(x_{0})=x_{0}\), and for any \(x_{1}\perp\mathcal{X}_{0}\), \(M(x_{1})=0\). Using Definition A.15 (Alternate), show that for any vector \(y\), \(M(y)\) is the perpendicular projection of \(y\) into \(\mathcal{X}_{0}\).

### Multiple Correlation

The coefficient of determination, denoted \(R^{2}\), is a commonly used measure of the predictive ability of a model. Computationally, it is most often defined as

\[R^{2}=\frac{SSReg}{SSTot-C},\]

so it is the proportion of the total variability explained by the independent variables. The greater the proportion of the total variability in the data that is explained by the model, the better the ability to predict with that model. The use and possible abuse of \(R^{2}\) as a tool for comparing models is discussed in Chapter 14, however it should be noted here that since \(R^{2}\) is a measure of the predictive ability of a model, \(R^{2}\) does not give direct information about whether a model fits the data properly. Demonstrably bad models can have very high \(R^{2}\)s and perfect models can have low \(R^{2}\)s.

We now define the multiple correlation, characterize it in terms of the best linear predictor, and show that \(R^{2}\) is the natural estimate of it. Subsection 6.4.1 generalizes the idea of \(R^{2}\) from best linear prediction to best prediction.

Recall that the correlation between two random variables, say \(x_{1}\) and \(x_{2}\), is

\[\operatorname{Corr}(x_{1},x_{2})=\operatorname{Cov}(x_{1},x_{2})\Big{/}\sqrt{ \operatorname{Var}(x_{1})\operatorname{Var}(x_{2})}.\]

The multiple correlation of \(y\) and \((x_{1},x_{2},\ldots,x_{p-1})^{\prime}=x\) is the maximum of \(\operatorname{Corr}(y,\alpha+x^{\prime}\beta)\) over all \(\alpha\) and \(\beta\). Note that

\[\operatorname{Cov}(y,\alpha+x^{\prime}\beta)=V_{yx}\beta=\beta_{*}^{\prime}V_{ xx}\beta,\]

where \(\beta_{*}\) is defined as in Subsection 6.3.4 and

\[\operatorname{Var}(\alpha+x^{\prime}\beta)=\beta^{\prime}V_{xx}\beta.\]

In particular, \(\operatorname{Cov}(y,\alpha+x^{\prime}\beta_{*})=\beta_{*}^{\prime}V_{xx} \beta_{*}=\operatorname{Var}(\alpha+x^{\prime}\beta_{*})\). The Cauchy-Schwarz inequality (see Exercise B.0) says that

\[\left(\sum_{i=1}^{t}r_{i}s_{i}\right)^{2}\leq\sum_{i=1}^{t}r_{i}^{2}\sum_{i=1} ^{t}s_{i}^{2}.\]

Since \(V_{xx}=RR^{\prime}\) for some matrix \(R\), the Cauchy-Schwarz inequality gives

\[\left(\beta_{*}^{\prime}V_{xx}\beta\right)^{2}\leq\left(\beta^{\prime}V_{xx} \beta\right)\left(\beta_{*}^{\prime}V_{xx}\beta_{*}\right).\]

Considering the squared correlation gives

\[\operatorname{Corr}^{2}(y,\alpha+x^{\prime}\beta) =\left(\beta_{*}^{\prime}V_{xx}\beta\right)^{2}\Big{/}\left(\beta ^{\prime}V_{xx}\beta\right)\sigma_{y}^{2}\] \[\leq\beta_{*}^{\prime}V_{xx}\beta_{*}\big{/}\sigma_{y}^{2}\] \[=\left(\beta_{*}^{\prime}V_{xx}\beta_{*}\right)^{2}\big{/}\left( \beta_{*}^{\prime}V_{xx}\beta_{*}\right)\sigma_{y}^{2}\] \[=\operatorname{Corr}^{2}(y,\alpha+x^{\prime}\beta_{*})\]

and the squared multiple correlation between \(y\) and \(x\) equals

\[\operatorname{Corr}^{2}(y,\alpha+x^{\prime}\beta_{*})=\beta_{*}^{\prime}V_{xx} \beta_{*}\big{/}\sigma_{y}^{2}\equiv\mathcal{R}^{2}.\]If we have observations (\(y_{i}\), \(x_{i1}\), \(x_{i2}\),..., \(x_{i,\,p-1})^{\prime}\), \(i=1\),..., \(n\), the usual estimate of \(\sigma_{y}^{2}\) can be written as

\[s_{y}^{2}=\frac{1}{n-1}Y^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Y=\frac{1} {n-1}\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}.\]

Using equivalences derived earlier, the natural estimate of the squared multiple correlation coefficient between \(y\) and \(x\) is

\[\frac{\hat{\beta}_{*}^{\prime}S_{xx}\hat{\beta}_{*}}{s_{y}^{2}}=\frac{\hat{ \beta}_{*}^{\prime}Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Z\hat{\beta}_ {*}}{Y^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Y}=\frac{SSReg}{SSTot-C}.\]

It is worth noticing that \(SSTot-C=SSReg+SSE\) and that

\[SSReg/SSE = SSReg/[(SSE+SSReg)-SSReg]\] \[= SSReg/[SSTot-C-SSReg]\] \[= \frac{SSReg}{[SSTot-C][1-R^{2}]}\] \[= R^{2}/[1-R^{2}].\]

For normally distributed data, the \(\alpha\) level \(F\) test for \(H_{0}:\beta_{1}=\beta_{2}=\cdots=\beta_{p-1}=0\) is to reject \(H_{0}\) if

\[\frac{n-p}{p-1}\frac{R^{2}}{1-R^{2}}>F(1-\alpha,\,p-1,n-p).\]

_Example 6.4.1_ From the information given in Example 6.2.1, the coefficient of determination can be found. From the ANOVA table in Example 6.2.1, we get

\[SSReg=1259.32\]

\[SSTot-C=SSTotal-SS(\beta_{0})=16576.60-15270.78=1305.82\]

and

\[R^{2}=1259.32/1305.82=0.964.\]

This is a very high value for \(R^{2}\) and indicates that the model has very substantial predictive ability. However, before you conclude that the model actually fits the data well, see Example 6.6.3.

**Exercise 6.5**:
1. Show that for a linear model with an intercept, \(R^{2}\) is simply the square of the sample correlation between the data \(y_{i}\) and the predicted values \(\hat{y}_{i}\).
2. It is a well known fact that if \(Q\sim F(r_{1},r_{2})\) then \[\frac{r_{1}Q/r_{2}}{1+r_{1}Q/r_{2}}\sim\text{Beta}(r_{1}/2,r_{2}/2).\]

Show that under the intercept only model \(R^{2}\) has a beta distribution and find its expected value and variance.
3. If \(p\) is increasing with \(n\), such that \(p/n\to f\), with \(0\leq f<1\), what happens to \(R^{2}\) as \(n\), \(p\to\infty\), assuming that the intercept only model is true?

This discussion has focused on \(R^{2}\), which estimates the squared multiple correlation coefficient, a measure of the predictive ability of the best linear predictor. We now consider an analogous measure for the best predictor.

#### Squared Predictive Correlation

As in Subsection 6.3.2, consider an arbitrary predictor \(\tilde{y}(x)\). This is a function of \(x\) alone and not a function of \(y\).

The _squared predictive correlation_ of \(\tilde{y}(x)\) is \(\text{Corr}^{2}[y,\tilde{y}(x)]\). The highest squared predictive correlation is obtained by using the best predictor. Note that in the special case where the best predictor \(m(x)\) is also the best linear predictor, the highest squared predictive correlation equals the squared multiple correlation coefficient.

**Theorem 6.4.2**: \(\text{Corr}^{2}[y,\tilde{y}(x)]\leq\text{Corr}^{2}[y,m(x)]\equiv\mathsf{R}^{2}\)_._

_Proof_ By Cauchy-Schwarz, \((\sigma_{m\tilde{y}})^{2}\leq\sigma_{mm}\sigma_{\tilde{y}\tilde{y}}\), so \((\sigma_{m\tilde{y}})^{2}/\sigma_{\tilde{y}\tilde{y}}\leq\sigma_{mm}\). Using Proposition 6.3.2,

\[\text{Corr}^{2}[y,\tilde{y}(x)]=\frac{(\sigma_{y\tilde{y}})^{2}}{\sigma_{yy} \sigma_{\tilde{y}\tilde{y}}}=\frac{(\sigma_{m\tilde{y}})^{2}}{\sigma_{yy} \sigma_{\tilde{y}\tilde{y}}}\leq\frac{\sigma_{mm}}{\sigma_{yy}}=\text{Corr}^{2 }[y,m(x)].\qed\]

Theorem 6.4.2 is also established in Rao (1973, Section 4g.1). From Theorem 6.4.2, the best regression function \(m(x)\) has the highest squared predictive correlation. When we have perfect prediction, the highest squared predictive correlation is 1. In other words, if the conditional variance of \(y\) given \(x\) is 0, then \(y=m(x)\) a.s., and the highest squared predictive correlation is the correlation of \(m(x)\) with itself, which is 1. On the other hand, if there is no regression relationship, i.e., if \(m(x)=\mu_{y}\) a.s., then \(\sigma_{mm}=0\), and the highest squared predictive correlation is 0.

We would now like to show that as the squared predictive correlation increases, we get increasingly better prediction. First we need to deal with the fact that high squared predictive correlations can be achieved by bad predictors. Just because \(\tilde{y}(x)\) is highly correlated with \(y\) does not mean that \(\tilde{y}(x)\) is actually close to \(y\). Recall that \(\tilde{y}(x)\) is simply a random _variable_ that is being used to predict \(y\). As such, \(\tilde{y}(x)\) is a linear predictor of \(y\), that is, \(\tilde{y}(x)=0+1\tilde{y}(x)\). We can apply Theorem 6.3.4 to this random variable to obtain a linear predictor that is at least as good as \(\tilde{y}(x)\), namely

\[\hat{y}(x)=\mu_{y}+\frac{\sigma_{y\tilde{y}}}{\sigma_{\tilde{y}\tilde{y}}}[ \tilde{y}(x)-\mu_{\tilde{y}}].\]

We refer to such predictors as _linearized predictors_. Note that \(\mathrm{E}[\hat{y}(x)]\equiv\mu_{\hat{y}}=\mu_{y}\),

\[\sigma_{\hat{y}\tilde{y}}\equiv\mathrm{Var}[\hat{y}(x)]=\left(\frac{\sigma_{y \tilde{y}}}{\sigma_{\tilde{y}\tilde{y}}}\right)^{2}\sigma_{\tilde{y}\tilde{y} }=\frac{(\sigma_{y\tilde{y}})^{2}}{\sigma_{\tilde{y}\tilde{y}}},\]

and

\[\sigma_{y\tilde{y}}\equiv\mathrm{Cov}[y,\,\hat{y}(x)]=\frac{\sigma_{y\tilde{y }}}{\sigma_{\tilde{y}\tilde{y}}}\sigma_{y\tilde{y}}=\frac{(\sigma_{y\tilde{y} })^{2}}{\sigma_{\tilde{y}\tilde{y}}}.\]

In particular, \(\sigma_{\hat{y}\tilde{y}}=\sigma_{y\tilde{y}}\), so the squared predictive correlation of \(\hat{y}(x)\) is

\[\mathrm{Corr}^{2}[y,\,\hat{y}(x)]=\frac{\sigma_{\hat{y}\hat{y}}}{\sigma_{yy}}.\]

In addition, the direct measure of the goodness of prediction for \(\hat{y}(x)\) is

\[\mathrm{E}[y-\hat{y}(x)]^{2}=\sigma_{yy}-2\sigma_{y\hat{y}}+\sigma_{\hat{y} \tilde{y}}=\sigma_{yy}-\sigma_{\hat{y}\tilde{y}}.\]

This leads directly to the next result.

**Theorem 6.4.2**: _For two linearized predictors \(\hat{y}_{1}(x)\) and \(\hat{y}_{2}(x)\), the squared predictive correlation of \(\hat{y}_{2}(x)\) is higher if and only if \(\hat{y}_{2}(x)\) is a better predictor._

_Proof \(\sigma_{\hat{y}_{1}\hat{y}_{1}}/\sigma_{yy}<\sigma_{\hat{y}_{2}\hat{y}_{2}}/ \sigma_{yy}\)_ if and only if \(\sigma_{\hat{y}_{1}\hat{y}_{1}}<\sigma_{\hat{y}_{2}\hat{y}_{2}}\) if and only if \(\sigma_{yy}-\sigma_{\hat{y}_{2}\hat{y}_{2}}<\sigma_{yy}-\sigma_{\hat{y}_{1} \hat{y}_{1}}\). \(\square\)_

_It should be noted that linearizing \(m(x)\) simply returns \(m(x)\)._

For any predictor \(\tilde{y}(x)\), no matter how one arrives at it, to estimate the squared predictive correlation from i.i.d. data \((y_{i},x_{i}^{\prime})\), simply compute the squared sample correlation between \(y_{i}\) and its predictor of \(\tilde{y}(x_{i})\). This should also work fine for \(y_{i}\)s that are conditionally uncorrelated and homoscedastic given the \(x_{i}\)s.

### Partial Correlation Coefficients

Many regression programs have options available to the user that depend on the values of the sample partial correlation coefficients. The partial correlation is defined in terms of two random variables of interest, say \(y_{1}\) and \(y_{2}\), and several auxiliary variables, say \(x_{1},\ldots,x_{p-1}\). The partial correlation coefficient of \(y_{1}\) and \(y_{2}\) given \(x_{1}\),..., \(x_{p-1}\), written \(\rho_{y\cdot x}\), is a measure of the linear relationship between \(y_{1}\) and \(y_{2}\) after taking the effects of \(x_{1}\),..., \(x_{p-1}\) out of both variables.

Writing \(y=(y_{1},y_{2})^{\prime}\) and \(x=(x_{1},\ldots,x_{p-1})^{\prime}\), Exercise 6.3 indicates that the best linear predictor of \(y\) given \(x\) is

\[\hat{E}(y|x)=\mu_{y}+\beta_{*}^{\prime}(x-\mu_{x}),\]

where \(\beta_{*}\) is a solution of \(V_{xx}\beta_{*}=V_{xy}\) and \(V_{xy}\) is now a \((p-1)\times 2\) matrix. We take the effects of \(x\) out of \(y\) by looking at the prediction error

\[y-\hat{E}(y|x)=(y-\mu_{y})-\beta_{*}^{\prime}(x-\mu_{x}),\]

which is a 2 \(\times\) 1 random vector. The partial correlation is simply the correlation between the two components of this vector and is readily obtained from the covariance matrix. We now find the prediction error covariance matrix. Let \(\text{Cov}(y)\equiv V_{yy}\).

\[\text{Cov}[(y-\mu_{y})-\beta_{*}^{\prime}(x-\mu_{x})] =\text{Cov}(y-\mu_{y})+\beta_{*}^{\prime}\text{Cov}(x-\mu_{x}) \beta_{*}\] \[\quad-\text{Cov}(y-\mu_{y},x-\mu_{x})\beta_{*}\] \[\quad-\beta_{*}^{\prime}\text{Cov}(x-\mu_{x},y-\mu_{y})\] \[=V_{yy}+\beta_{*}^{\prime}V_{xx}\beta_{*}-V_{yx}\beta_{*}-\beta_{ *}^{\prime}V_{xy}\] \[=V_{yy}+\beta_{*}^{\prime}V_{xx}\beta_{*}-\beta_{*}^{\prime}V_{xx }\beta_{*}-\beta_{*}^{\prime}V_{xx}\beta_{*}\] \[=V_{yy}-\beta_{*}^{\prime}V_{xx}\beta_{*}.\]

Since \(V_{xx}\beta_{*}=V_{xy}\) and, for any generalized inverse, \(V_{xx}V_{xx}^{-}V_{xx}=V_{xx}\),

\[\text{Cov}[(y-\mu_{y})-\beta_{*}^{\prime}(x-\mu_{x})] =V_{yy}-\beta_{*}^{\prime}V_{xx}V_{xx}^{-}V_{xx}\beta_{*}\] \[=V_{yy}-V_{yx}V_{xx}^{-}V_{xy}. \tag{1}\]

If we have a sample of the \(y\)s and \(x\)s, say \(y_{i1}\), \(y_{i2}\), \(x_{i1}\), \(x_{i2}\),..., \(x_{i,p-1}\), \(i=1\),..., \(n\), we can estimate the covariance matrix in the usual way. The relationship with the linear regression model of Section 2 is as follows: Let

\[Y=\begin{bmatrix}y_{11}&y_{12}\\ \vdots&\vdots\\ y_{n1}&y_{n2}\end{bmatrix}=[Y_{1},Y_{2}]\]\[Z=\begin{bmatrix}x_{11}&\cdots&x_{1p-1}\\ \vdots&\ddots&\vdots\\ x_{n1}&\cdots&x_{np-1}\end{bmatrix}.\]

The usual estimate of \(V_{yy}-V_{yx}V_{xx}^{-}V_{xy}\) is \((n-1)^{-1}\) times

\[Y^{\prime}\bigg{(}I-\frac{1}{n}J_{n}^{n}\bigg{)}Y-Y^{\prime}\bigg{(}I-\frac{1 }{n}J_{n}^{n}\bigg{)}Z\left[Z^{\prime}\bigg{(}I-\frac{1}{n}J_{n}^{n}\bigg{)}Z \right]^{-}Z^{\prime}\bigg{(}I-\frac{1}{n}J_{n}^{n}\bigg{)}Y.\]

From Section 2, we know that this is the same as

\[Y^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Y-Y^{\prime}\left(M-\frac{1}{n}J_ {n}^{n}\right)Y=Y^{\prime}\left(I-M\right)Y,\]

where \(M\) is the perpendicular projection operator onto \(C([J,Z])\). Remembering that \(Y^{\prime}(I-M)Y=[(I-M)Y]^{\prime}[(I-M)Y]\), we can see that the estimate of \(\rho_{y\cdot x}\), written \(r_{y\cdot x}\) and called the _sample partial correlation coefficient_, is just the sample correlation coefficient between the residuals of fitting \(Y_{1}=[J,Z]\beta_{1}+e_{1}\) and the residuals of fitting \(Y_{2}=[J,Z]\beta_{2}+e_{2}\), i.e.,

\[r_{y\cdot x}=\frac{Y_{1}^{\prime}(I-M)Y_{2}}{\sqrt{Y_{1}^{\prime}(I-M)Y_{1}Y_{ 2}^{\prime}(I-M)Y_{2}}}.\]

The square of the sample partial correlation coefficient, often called the coefficient of partial determination, has a nice relationship to another linear model. Consider fitting \(Y_{1}=[J,Z,Y_{2}]\gamma+e\). Because \(C[(I-M)Y_{2}]\) is the orthogonal complement of \(C([J,Z])\) with respect to \(C([J,Z,Y_{2}])\), the sum of squares for testing whether \(Y_{2}\) adds to the model is

\[SSR(Y_{2}|J,Z)=Y_{1}^{\prime}(I-M)Y_{2}[Y_{2}^{\prime}(I-M)Y_{2}]^{-1}Y_{2}^{ \prime}(I-M)Y_{1}.\]

Since \(Y_{2}^{\prime}(I-M)Y_{2}\) is a scalar, it is easily seen that

\[r_{y\cdot x}^{2}=\frac{SSR(Y_{2}|J,Z)}{SSE(J,Z)},\]

where \(SSE(J,Z)=Y_{1}^{\prime}(I-M)Y_{1}\), the sum of squares for error when fitting the model \(Y_{1}=[J,Z]\beta+e\).

Finally, for normally distributed data we can do a \(t\) test of the null hypothesis \(H_{0}:\rho_{y\cdot x}=0\). If \(H_{0}\) is true, then

\[\sqrt{n-p-1}\ r_{y\cdot x}\bigg{/}\sqrt{1-r_{y\cdot x}^{2}}\sim t(n-p-1).\]See Exercise 6.7 for a proof of this result.

#### Example 6.5.1

Using the data of Example 6.2.1, the coefficient of partial determination (squared sample partial correlation coefficient) between \(y\) and \(x_{2}\) given \(x_{1}\) can be found:

\[SSR(X_{2}|J,X_{1})=6.70,\]

\[SSE(J,X_{1})=SSE(J,X_{1},X_{2})+SSR(X_{2}|J,X_{1})=46.50+6.70=53.20,\]

\[r_{y\cdot x}^{2}=6.70/53.20=0.1259.\]

The absolute value of the \(t\) statistic for testing whether \(\rho_{y2\cdot 1}=0\) can also be found. In this application we are correcting \(Y\) and \(X_{2}\) for only one variable \(X_{1}\), so \(p-1=1\) and \(p=2\). The formula for the absolute \(t\) statistic becomes

\[\sqrt{25-2-1}\sqrt{0.1259}/\sqrt{1-0.1259}=1.78.\]

Note that this is precisely the \(t\) statistic reported for \(\beta_{2}\) in Example 6.2.1.

##### Exercise 6.6

Assume that \(V_{xx}\) is nonsingular. Show that \(\rho_{y\cdot x}=0\) if and only if the best linear predictor of \(y_{1}\) based on \(x\) and \(y_{2}\) equals the best linear predictor of \(y_{1}\) based on \(x\) alone.

##### Exercise 6.7

If \((y_{i1},y_{i2},x_{i1},x_{i2},\ldots,x_{i,p-1})^{\prime}\), \(i=1,\ldots,n\) are independent \(N(\mu,V)\), find the distribution of \(\sqrt{n-p-1}\)\(r_{y\cdot x}\)\(\sqrt{1-r_{y\cdot x}^{2}}\) when \(\rho_{y\cdot x}=0\). Hint: Use the linear model \(\operatorname{E}(Y_{1}|X,Y_{2})\in C(J,X,Y_{2})\), i.e., \(Y_{1}=[J,X,Y_{2}]\gamma+e\), to find a distribution conditional on \(X\), \(Y_{2}\). Note that the distribution does not depend on the values of \(X\) and \(Y_{2}\), so it must also be the unconditional distribution. Note also that from Exercise 6.5 and the equality between the conditional expectation and the best linear predictor for multivariate normals that we have \(\rho_{y\cdot x}=0\) if and only if the regression coefficient of \(Y_{2}\) is zero.

Finally, the usual concept of partial correlation, which looks at the correlation between the components of \(y-\hat{E}(y|x)\), i.e., the residuals based on best linear prediction, can be generalized to a concept based on examining the correlations between the components of \(y-\operatorname{E}(y|x)\), the residuals from the best predictor.

### Best Linear Unbiased Prediction

In this section we consider the general theory of best linear unbiased prediction. In a series of examples, this theory will be used to examine prediction in standard linear model theory, Kriging of spatial data, and prediction of random effects in mixed models. The last two subjects are treated in more depth in _ALM-III_.

Consider a set of random variables \(y_{i}\), \(i=0\), \(1\),..., \(n\). We want to use \(y_{1}\),..., \(y_{n}\) to predict \(y_{0}\). Let \(Y=(y_{1}\),..., \(y_{n})^{\prime}\). In Section 6.3 it was shown that the best predictor (BP) of \(y_{0}\) is \(\mathrm{E}(y_{0}|Y)\). Typically, the joint distribution of \(y_{0}\) and \(Y\) is not available, so \(\mathrm{E}(y_{0}|Y)\) cannot be found. However, if the means and covariances of the \(y_{i}\)s are available, then we can find the best linear predictor (BLP) of \(y_{0}\). Let \(\mathrm{Cov}(Y)\equiv V\), \(\mathrm{Cov}(Y,\,y_{0})\equiv V_{y0}\), \(\mathrm{E}(y_{i})\equiv\mu_{i}\), \(i=0\), \(1\),..., \(n\), and \(\mu\equiv(\mu_{1}\),..., \(\mu_{n})^{\prime}\). Again from Subsection 6.3.4, the BLP of \(y_{0}\) is

\[\hat{E}(y_{0}|Y)\equiv\mu_{0}+\delta_{*}^{\prime}(Y-\mu), \tag{1}\]

where \(\delta_{*}\) satisfies \(V\delta_{*}=V_{y0}\).

We now want to weaken the assumption that \(\mu\) and \(\mu_{0}\) are known. Since the prediction is based on \(Y\) and there are as many unknown parameters in \(\mu\) as there are observations in \(Y\), we need to impose some structure on the mean vector \(\mu\) before we can generalize the theory. This will be done by specifying a linear model for \(Y\). Since \(y_{0}\) is being predicted and has not been observed, it is necessary either to know \(\mu_{0}\) or to know that \(\mu_{0}\) is related to \(\mu\) in a specified manner. In the theory below, it will be assumed that \(\mu_{0}\) is related to the linear model for \(Y\).

Suppose that a vector of known concomitant variables \(x_{i}^{\prime}=(x_{i1},\ldots,x_{ip})\) is associated with each random observation \(y_{i}\), \(i=0\),..., \(n\). We impose structure on the \(\mu_{i}\)s by assuming that \(\mu_{i}=x_{i}^{\prime}\beta\) for some vector of unknown parameters \(\beta\) and all \(i=0\), \(1\),..., \(n\).

We can now reset our notation in terms of linear model theory. Let

\[X=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}.\]

The observed vector \(Y\) satisfies the linear model

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=V. \tag{2}\]

With this additional structure, the BLP of \(y_{0}\) given in (1) becomes

\[\hat{E}(y_{0}|Y)=x_{0}^{\prime}\beta+\delta_{*}^{\prime}(Y-X\beta), \tag{3}\]

where again \(V\delta_{*}=V_{y0}\).

The standard assumption that \(\mu\) and \(\mu_{0}\) are known now amounts to the assumption that \(\beta\) is known. It is this assumption that we renounce. By weakening the assumptions, we consequently weaken the predictor. If \(\beta\) is known, we can find the best linear predictor. When \(\beta\) is unknown, we find the best linear unbiased predictor.

Before proceeding, a technical detail must be mentioned. To satisfy estimability conditions, we need to assume that \(x_{0}^{\prime}=\rho^{\prime}X\) for some vector \(\rho\). Frequently, model (2) will be a regression model, so choosing \(\rho^{\prime}=x_{0}^{\prime}(X^{\prime}X)^{-1}X^{\prime}\) will suffice. In applications to mixed models, \(x_{0}^{\prime}=0\), so \(\rho=0\) will suffice.

**Definition 6.6.1**: A _predictor_\(f(Y)\) of \(y_{0}\) is said to be _unbiased_ if

\[\operatorname{E}[f(Y)]=\operatorname{E}(y_{0}).\]

Although \(\operatorname{E}[\operatorname{E}(y_{0}|Y)]=\operatorname{E}(y_{0})\) and \(\operatorname{E}[\hat{E}(y_{0}|Y)]=\operatorname{E}(y_{0})\), in this context neither of \(\operatorname{E}(y_{0}|Y)\) and \(\hat{E}(y_{0}|Y)\) are unbiased predictors because they are not predictors. We do not know what they are, so we cannot use them to predict.

**Definition 6.6.2**: \(a_{0}+a^{\prime}Y\) is a _best linear unbiased predictor_ of \(y_{0}\) if \(a_{0}+a^{\prime}Y\) is unbiased and if, for any other unbiased predictor \(b_{0}+b^{\prime}Y\),

\[\operatorname{E}\bigl{[}y_{0}-a_{0}-a^{\prime}Y\bigr{]}^{2}\leq\operatorname{E }\bigl{[}y_{0}-b_{0}-b^{\prime}Y\bigr{]}^{2}.\]

**Theorem 6.6.3**: _The best linear unbiased predictor of \(y_{0}\) is \(x_{0}^{\prime}\hat{\beta}+\delta_{*}^{\prime}(Y-X\hat{\beta})\), where \(V\delta_{*}=V_{y0}\) and \(X\hat{\beta}\) is a BLUE of \(X\beta\)._

_Proof_ The technique of the proof is to change the prediction problem into an estimation problem and then to use the theory of best linear unbiased estimation. Consider an arbitrary linear unbiased predictor of \(y_{0}\), say \(b_{0}+b^{\prime}Y\). By Proposition 6.3.5,

\[\operatorname{E}[y_{0}-b_{0}-b^{\prime}Y]^{2}=\operatorname{E}[y_{0}-\hat{E}(y_ {0}|Y)]^{2}+\operatorname{E}[\hat{E}(y_{0}|Y)-b_{0}-b^{\prime}Y]^{2};\]

so it is enough to find \(b_{0}+b^{\prime}Y\) that minimizes \(\operatorname{E}[\hat{E}(y_{0}|Y)-b_{0}-b^{\prime}Y]^{2}\).

From the definition of \(\hat{E}(y_{0}|Y)\) and unbiasedness of \(b_{0}+b^{\prime}Y\), we have

\[0=\operatorname{E}[\hat{E}(y_{0}|Y)-b_{0}-b^{\prime}Y].\]

Substituting from equation (3) gives

\[0=\operatorname{E}[x_{0}^{\prime}\beta+\delta_{*}^{\prime}(Y-X\beta)-b_{0}-b^ {\prime}Y].\]

This relationship holds if and only if \(b_{0}+(b-\delta_{*})^{\prime}Y\) is a linear unbiased estimate of \(x_{0}^{\prime}\beta-\delta_{*}^{\prime}X\beta\). By Proposition 2.1.9,

\[b_{0}=0;\]

so the term we are trying to minimize is \[\mathrm{E}[\hat{E}(y_{0}|Y)-b_{0}-b^{\prime}Y]^{2} = \mathrm{E}[(b-\delta_{*})^{\prime}Y-(x_{0}^{\prime}\beta-\delta_{*} ^{\prime}X\beta)]^{2}\] \[= \mathrm{Var}\big{[}(b-\delta_{*})^{\prime}Y\big{]}.\]

Because \((b-\delta_{*})^{\prime}Y\) is a linear unbiased estimate, to minimize the variance choose \(b\) so that \((b-\delta_{*})^{\prime}Y=x_{0}^{\prime}\hat{\beta}-\delta_{*}^{\prime}X\hat{\beta}\) is a BLUE of \(x_{0}^{\prime}\beta-\delta_{*}^{\prime}X\beta\). It follows that the best linear unbiased predictor of \(y_{0}\) is

\[b^{\prime}Y=x_{0}^{\prime}\hat{\beta}+\delta_{*}^{\prime}(Y-X\hat{\beta}).\qed\]

Of course if we did not know the covariance matrices, the BLUP would not be a predictor either. It should not be overlooked that both \(\hat{\beta}\) and \(\delta_{*}\) depend crucially on \(V\). When all inverses exist, \(\hat{\beta}=(X^{\prime}V^{-1}X)^{-1}X^{\prime}V^{-1}Y\) and \(\delta_{*}=V^{-1}V_{y0}\). However, this proof remains valid when either inverse does not exist.

It is also of value to know the prediction variance of the BLUP. Let \(\mathrm{Var}(y_{0})\equiv\sigma_{0}^{2}\).

\[\mathrm{E}\Big{[}y_{0}-x_{0}^{\prime}\hat{\beta}-\delta_{*}^{ \prime}(Y-X\hat{\beta})\Big{]}^{2}\] \[= \mathrm{E}\Big{[}y_{0}-\hat{E}(y_{0}|Y)\Big{]}^{2}+\mathrm{E} \Big{[}\hat{E}(y_{0}|Y)-x_{0}^{\prime}\hat{\beta}-\delta_{*}^{\prime}(Y-X\hat{ \beta})\Big{]}^{2}\] \[= \mathrm{E}\Big{[}y_{0}-\hat{E}(y_{0}|Y)\Big{]}^{2}+\mathrm{Var} \Big{[}x_{0}^{\prime}\hat{\beta}-\delta_{*}^{\prime}X\hat{\beta}\Big{]}\] \[= \sigma_{0}^{2}-V_{y0}^{\prime}V^{-}V_{y0}+(x_{0}^{\prime}-\delta_ {*}^{\prime}X)(X^{\prime}V^{-}X)^{-}(x_{0}-X^{\prime}\delta_{*}),\]

or, writing the BLUP as \(b^{\prime}Y\), the prediction variance becomes

\[\mathrm{E}\left[y_{0}-b^{\prime}Y\right]^{2}=\sigma_{0}^{2}-2b^{\prime}V_{y0}+ b^{\prime}Vb.\]

If for some reason \(\mathrm{E}(y_{0})\) is known, we can still find a BLUP. If \(\mathrm{E}(y_{0})=0\), the theory holds merely by taking \(x_{0}=0\). If \(\mathrm{E}(y_{0})=\mu_{0}\), the more precise way to proceed is to go back into the proof replacing \(x_{0}^{\prime}\beta\) with \(\mu_{0}\) and observe that the proof continues to hold if we choose \(b_{0}=\mu_{0}\). A simpler but less precise way to proceed is to instead predict \(y_{*}\equiv y_{0}-\mu_{0}\) which has mean zero and notice that

\[\hat{y}_{0}-\mu_{0}\equiv\hat{y}_{*}=\delta_{*}^{\prime}(Y-X\hat{\beta})\]

so that

\[\hat{y}_{0}=\mu_{0}+\delta_{*}^{\prime}(Y-X\hat{\beta}).\]

In either case, the prediction variance is

\[\mathrm{E}\Big{[}y_{0}-\mu_{0}-\delta_{*}^{\prime}(Y-X\hat{\beta})\Big{]}^{2} =\sigma_{0}^{2}-V_{y0}^{\prime}V^{-}V_{y0}+\delta_{*}^{\prime}X(X^{\prime}V^{- }X)^{-}X^{\prime}\delta_{*}.\]_Example 6.6.4_: _Prediction in Standard Linear Models._

In the standard linear model the \(y_{i}\)s have zero covariance and identical variances. Thus, model (2) is satisfied with \(V=\sigma^{2}I\). A new observation \(y_{0}\) would typically have zero covariance with the previous data, so \(V_{y0}=0\). It follows that \(\delta_{*}=0\) and the BLUP of \(y_{0}\) is \(x_{0}^{\prime}\hat{\beta}\). This is just the BLUE of \(\mathrm{E}(y_{0})=x_{0}^{\prime}\beta\).

_Example 6.6.5_: _Spatial Data and Kriging._

If \(y_{i}\) is an observation taken at some point in space, then \(x_{i}^{\prime}\) typically contains the coordinates of the point. In dealing with spatial data, finding the BLUP is often called _Kriging_. The real challenge with spatial data is getting some idea of the covariance matrices \(V\) and \(V_{y0}\). See _ALM-III_, Chapter 8 for a more detailed discussion with additional references.

**Exercise 6.8**: Prove three facts about Kriging.

(a) If \(b^{\prime}Y\) is the BLUP of \(y_{0}\) and if \(x_{i1}=1\), \(i=0\), \(1\), \(\ldots\), \(n\), then \(b^{\prime}J=1\).

(b) If \((y_{0},x_{0}^{\prime})=(y_{i}\), \(x_{i}^{\prime})\) for some \(i\geq 1\), then the BLUP of \(y_{0}\) is just \(y_{i}\).

(c) If \(V\) is nonsingular and \(b^{\prime}Y\) is the BLUP of \(y_{0}\), then there exists a vector \(\gamma\) such that the following equation is satisfied:

\[\begin{bmatrix}V&X\\ X^{\prime}&0\end{bmatrix}\begin{bmatrix}b\\ \gamma\end{bmatrix}=\begin{bmatrix}V_{y0}\\ x_{0}\end{bmatrix}.\]

Typically, this equation will have a unique solution.

Hint: Recall that \(x_{0}^{\prime}=\rho^{\prime}X\) and that \(X(X^{\prime}V^{-1}X)^{-}X^{\prime}V^{-1}\) is a projection operator onto \(C(X)\).

_Example 6.6.6_: _Mixed Model Prediction._

A mixed model (is similar to the analysis of covariance models of Chapter 9 but) has

\[Y=X\beta+Z\gamma+e, \tag{4}\]

where \(X\) and \(Z\) are known matrices, \(\beta\) is an unobservable vector of fixed effects, but where \(\gamma\) is an unobservable vector of random effects with \(\mathrm{E}(\gamma)=0\), \(\mathrm{Cov}(\gamma)\equiv D\), and \(\mathrm{Cov}(\gamma,e)=0\). Let \(\mathrm{Cov}(e)\equiv R\).

We wish to find the BLUP of \(\lambda^{\prime}\gamma\) based on the data \(Y\). The vector \(\lambda\) can be any known vector. Note that \(\mathrm{E}(\lambda^{\prime}\gamma)=0\); so we can let \(y_{0}=\lambda^{\prime}\gamma\) and \(x_{0}^{\prime}=0\). Also,

\[V\equiv\mathrm{Cov}(Y)=\mathrm{Cov}(Z\gamma+e)=ZDZ^{\prime}+R\]\[V_{y0} = \text{Cov}(Y,\lambda^{\prime}\gamma)\] \[= \text{Cov}(Z\gamma+e,\lambda^{\prime}\gamma)\] \[= \text{Cov}(Z\gamma,\lambda^{\prime}\gamma)\] \[= ZD\lambda.\]

The BLUP of \(\lambda^{\prime}\gamma\) is

\[\delta^{\prime}_{*}(Y-X\hat{\beta}),\]

where \(X\hat{\beta}\) is the BLUE of \(X\beta\) and \(\delta_{*}\) satisfies \((ZDZ^{\prime}+R)\delta_{*}=ZD\lambda\). The matrices \(X,\,Z\), and \(\lambda\) are all known. As in Kriging, the practical challenge is to get some idea of the covariance matrices. In this case, we need \(D\) and \(R\). Estimates of \(D\) and \(R\) are one byproduct of variance component estimation. _ALM-III_, Chapter 5 contains much more information on mixed models.

**Exercise 6.9**: If \(\lambda^{\prime}_{1}\beta\) is estimable, find the best linear unbiased predictor of \(\lambda^{\prime}_{1}\beta+\lambda^{\prime}_{2}\gamma\). For this problem, \(b_{0}+b^{\prime}Y\) is unbiased if \(\text{E}(b_{0}+b^{\prime}Y)=\text{E}(\lambda^{\prime}_{1}\beta+\lambda^{ \prime}_{2}\gamma)\). The best predictor minimizes \(\text{E}[b_{0}+b^{\prime}Y-\lambda^{\prime}_{1}\beta-\lambda^{\prime}_{2} \gamma]^{2}\).

**Exercise 6.10**: Assuming the results of Exercise 6.3, show that the BLUP of the random vector \(\Lambda^{\prime}\gamma\) is \(Q^{\prime}(Y-X\hat{\beta})\), where \(V\,Q=ZD\Lambda\).

### Testing Lack of Fit

Suppose we have a linear model \(Y=X\beta+e\) and we suspect that the model is an inadequate explanation of the data. The obvious thing to do to correct the problem is to add more predictor variables to the model, i.e, fit a model \(Y=Z\gamma+e\), where \(C(X)\subset C(Z)\). Two questions present themselves: (1) how does one choose \(Z\), and (2) is there really a lack of fit? Given a choice for \(Z\), the second of these questions can be addressed by testing \(Y=X\beta+e\) against \(Y=Z\gamma+e\). This is referred to as a _test for lack of fit_. Since \(Z\) is chosen so that \(Y=Z\gamma+e\) will actually fit the data (or at least fit the data better than \(Y=X\beta+e\)), the error sum of squares for the model \(Y=Z\gamma+e\), say \(SSE(Z)\), can be called the _sum of squares for pure error_, _SSPE_. The difference \(SSE(X)-SSE(Z)\) is used for testing lack of fit, so \(SSE(X)-SSE(Z)\) is called the _sum of squares for lack of fit_, _SSLF_.

In general, there are few theoretical guidelines for choosing \(Z\). A common situation is where there are measured variables not currently included in the model and it is necessary to select additional variables to include in the model. Variable selection techniques are discussed in Chapter 14. In this section, we discuss the problem of testing lack of fit when there are no other variables available for inclusion in the model. With no other variables available, the model matrix \(X\) must be used as the basis for choosing \(Z\). We will present four approaches. The first is the traditional method based on having a model matrix \(X\) in which some of the rows are identical. A second approach is based on identifying clusters of rows in \(X\) that are nearly identical. A third approach examines different subsets of the data. Finally, we briefly mention a nonparametric regression approach to testing lack of fit. Christensen (2015) covers similar material at a more applied level.

One final note. This section is in the chapter on regression because testing lack of fit has traditionally been considered as a topic in regression analysis. Nowhere in this section do we assume that \(X^{\prime}X\) is nonsingular. _The entire discussion holds for general linear models._

#### The Traditional Test

To discuss the traditional approach that originated with Fisher (1922), we require notation for identifying which rows of the model matrix are identical. A model with replications can be written

\[y_{ij}=x^{\prime}_{i}\beta+e_{ij},\]

where \(\beta\) is the vector of parameters, \(x^{\prime}_{i}=(x_{i1},\ldots,x_{ip})\), \(i=1,\cdots,c\), and \(j=1,\cdots,N_{i}\). We presume that \(x^{\prime}_{i}\neq x^{\prime}_{k}\) for \(i\neq k\). Using the notation of Chapter 4 in which a pair of subscripts is used to denote a row of a vector or a row of the model matrix, we have \(Y=[y_{ij}]\) and

\[X=[w^{\prime}_{ij}],\quad\quad\text{where }w^{\prime}_{ij}=x^{\prime}_{i}.\]

The idea of pure error, when there are rows of \(X\) that are replicated, is that if several observations have the same mean value, the variability about that mean value is in some sense pure error. The problem is to estimate the mean value. If we estimate the mean value in the \(i\)th group with \(x^{\prime}_{i}\hat{\beta}\), then estimate the variance for the group by looking at the deviations about the estimated mean value, and finally pool the estimates from the different groups, we get \(MSE(X)\). Now consider a more general model, \(Y=Z\gamma+e\), where \(C(X)\subset C(Z)\) and \(Z\) is chosen so that

\[Z=[z^{\prime}_{ij}],\quad\quad\text{where }z^{\prime}_{ij}=v^{\prime}_{i}\]

for some vectors \(v^{\prime}_{i}\), \(i=1,\ldots,c\). Two rows of \(Z\) are the same if and only if the corresponding rows of \(X\) are the same. Thus, the groups of observations that had the same mean value in the original model still have the same mean value in the generalized model. If there exists a lack of fit, we hope that the more general model gives a more accurate estimate of the mean value.

It turns out that there exists a most general model \(Y=Z\gamma+e\) that satisfies the condition that two rows of \(Z\) are the same if and only if the corresponding rows of \(X\) are the same. We will refer to the property that rows of \(Z\) are identical if and only if the corresponding rows of \(X\) are identical as \(X\) and \(Z\) having the same _row structure_.

\(X\) was defined to have \(c\) distinct rows, therefore \(r(X)\leq c\). Since \(Z\) has the same row structure as \(X\), we also have \(r(Z)\leq c\). The most general matrix \(Z\), the one with the largest column space, will have \(r(Z)=c\). We need to find \(Z\) with \(C(X)\subset C(Z)\), \(r(Z)=c\), and the same row structure as \(X\). We also want to show that the column space is the same for any such \(Z\).

Let \(Z\) be the model matrix for the model \(y_{ij}=\mu_{i}+e_{ij}\), \(i=1,\ldots,c\), \(j=1,\ldots,N_{i}\). If we let \(z_{ij,k}\) denote the element in the \(ij\)th row and \(k\)th column of \(Z\), then from Chapter 4

\[Z=[z_{ij,k}],\ \ \ \ \ \mbox{ where }z_{ij,k}=\delta_{ik}.\]

\(Z\) is a matrix where the \(k\)th column is \(0\) everywhere except that it has \(1\)s in rows that correspond to the \(y_{kj}\)s. Since the values of \(z_{ij,k}\) do not depend on \(j\), it is clear that \(Z\) has the same row structure as \(X\). Since the \(c\) columns of \(Z\) are linearly independent, we have \(r(Z)=c\), and it is not difficult to see that \(X=M_{Z}X\), where \(M_{Z}\) is the perpendicular projection operator onto \(C(Z)\); so we have \(C(X)\subset C(Z)\).

In fact, because of the form of \(Z\) and \(M_{Z}\), it is clear that any matrix \(Z_{1}\) with the same row structure as \(X\) must have \(Z_{1}=M_{Z}Z_{1}\) and \(C(Z_{1})\subset C(Z)\). If \(r(Z_{1})=c\), then it follows that \(C(Z_{1})=C(Z)\) and the column space of the most general model \(Y=Z\gamma+e\) does not depend on the specific choice of \(Z\).

If one is willing to assume that the lack of fit is not due to omitting some variable that, if included, would change the row structure of the model (i.e., if one is willing to assume that the row structure of the true model is the same as the row structure of \(X\)), then the true model can be written \(Y=W\delta+e\) with \(C(X)\subset C(W)\subset C(Z)\). It is easily seen that the lack-of-fit test statistic based on \(X\) and \(Z\) has a noncentral \(F\) distribution and if \(Y=X\beta+e\) is the true model, the test statistic has a central \(F\) distribution.

The computations for this lack-of-fit test are quite simple. With the choice of \(Z\) indicated, \(C(Z)\) is just the column space for a one-way ANOVA.

\[SSPE\equiv SSE(Z)=Y^{\prime}(I-M_{Z})Y=\sum_{i=1}^{c}\sum_{j=1}^{N_{i}}(y_{ij}- \bar{y}_{i}.)^{2}.\]

With \(M_{Z}Y=(\bar{y}_{1},\ldots\bar{y}_{1},\bar{y}_{2},\ldots\bar{y}_{2},\ldots, \bar{y}_{c},\ldots\bar{y}_{c})^{\prime}\), \(\hat{y}_{i}=x_{i}^{\prime}\hat{\beta}\) and \(MY=(\hat{y}_{1},\ldots\hat{y}_{1},\hat{y}_{2},\ldots\hat{y}_{2},\ldots,\hat{y} _{c},\ldots\hat{y}_{c})^{\prime}\), the sum of squares for lack of fit is

\[\begin{array}{rl}SSLF\equiv Y^{\prime}(M_{Z}-M)Y&=[(M_{Z}-M)Y][(M_{Z}-M)Y] \\ &=\sum_{i=1}^{c}\sum_{j=1}^{N_{i}}(\bar{y}_{i}.-\hat{y}_{i})^{2}=\sum_{i=1}^{ c}N_{i}(\bar{y}_{i}.-\hat{y}_{i})^{2}.\end{array}\]

**Exercise 6.11**: Show that if \(M\) is the perpendicular projection operator onto \(C(X)\) with \[X=\begin{bmatrix}w^{\prime}_{1}\\ \vdots\\ w^{\prime}_{n}\end{bmatrix}\ \ \ \text{and}\ \ \ M=\begin{bmatrix}T^{\prime}_{1}\\ \vdots\\ T^{\prime}_{n}\end{bmatrix},\]

then \(w_{i}=w_{j}\) if and only if \(T_{i}=T_{j}\).

**Exercise 6.12**: Discuss the application of the traditional lack-of-fit test to the problem where \(Y=X\beta+e\) is a simple linear regression model.

As we have seen, in the traditional method of testing for lack of fit, the row structure of the model matrix \(X\) completely determines the choice of \(Z\). Now, suppose that none of the rows of \(X\) are identical. It is still possible to have lack of fit, but the traditional method no longer applies.

#### Near Replicate Lack of Fit Tests

Another set of methods for testing lack of fit is based on mimicking the traditional lack-of-fit test. With these methods, rows of the model matrix that are nearly replicates are identified. One way of identifying near replicates is to use a hierarchical clustering algorithm (see Gnanadesikan 1977) to identify rows of the model matrix that are near one another. Tests for lack of fit using near replicates are reviewed by Neill and Johnson (1984). The theory behind such tests is beautifully explained in Christensen (1989, 1991). (OK, so I'm biased in favor of this particular author.) Miller et al. (1998, 1999) provide a theoretical basis for choosing near replicate clusters. Pimentel et al. (2017) propose an alternative method for defining near replicates.

Christensen (1991) suggests that a very good all-purpose near replicate lack-of-fit test was introduced by Shillington (1979). Write the regression model in terms of \(c\) clusters of near replicates with the \(i\)th cluster containing \(N_{i}\) cases, say

\[y_{ij}=x^{\prime}_{ij}\beta+e_{ij}, \tag{1}\]

\(i=1,\ldots,c,j=1,\ldots,N_{i}\). Note that at this point we have done nothing to the model except play with the subscripts; model (1) is just the original model. Shillington's test involves finding means of the predictor variables in each cluster and fitting the model

\[y_{ij}=\bar{x}^{\prime}_{i}.\beta+e_{ij}. \tag{2}\]

The numerator for Shillington's test is then the numerator mean square used in comparing this model to the one-way ANOVA model

\[y_{ij}=\mu_{i}+e_{ij}.\]However, the denominator mean square for Shillington's test is the mean squared error from fitting the model

\[y_{ij}=x_{ij}^{\prime}\beta+\mu_{i}+e_{ij}. \tag{4}\]

It is not difficult to see that if model (1) holds, then Shillington's test statistic has a central \(F\) distribution with the appropriate degrees of freedom. Christensen (1989, 1991) gives details and explains why this should be a good all-purpose test--even though it is not the optimal test for either of the alternatives developed by Christensen. The near replicate lack-of-fit test proposed in Christensen (1989) is the test of model (1) against model (4). The test proposed in Christensen (1991) uses the same numerator as Shillington's test, but a denominator sum of squares that is the _SSE_ from model (1) minus the numerator sum of squares from Shillington's test. Both of Christensen's tests are optimal for certain types of lack of fit. If the clusters consist of exact replicates, then all of these tests reduce to the traditional test.

#### Example 6.2.1

Using the data of Example 6.2.1 we illustrate the near replicate lack-of-fit tests. Near replicates were chosen visually by plotting \(x_{1}\) versus \(x_{2}\). The near replicates are presented below.

\begin{tabular}{l c c c|c c c c}  & \multicolumn{6}{c}{Near Replicate Clusters for Steam Data} \\ Obs. & & & Near & Obs. & & & Near \\ no. & \(x_{1}\) & \(x_{2}\) & rep. & no. & \(x_{1}\) & \(x_{2}\) & rep. \\ \hline
1 & 35.3 & 20 & 2 & 14 & 39.1 & 19 & 11 \\
2 & 29.7 & 20 & 2 & 15 & 46.8 & 23 & 12 \\
3 & 30.8 & 23 & 9 & 16 & 48.5 & 20 & 3 \\
4 & 58.8 & 20 & 4 & 17 & 59.3 & 22 & 13 \\
5 & 61.4 & 21 & 5 & 18 & 70.0 & 22 & 7 \\
6 & 71.3 & 22 & 7 & 19 & 70.0 & 11 & 1 \\
7 & 74.4 & 11 & 1 & 20 & 74.5 & 23 & 8 \\
8 & 76.7 & 23 & 8 & 21 & 72.1 & 20 & 14 \\
9 & 70.7 & 21 & 10 & 22 & 58.1 & 21 & 5 \\
10 & 57.5 & 20 & 4 & 23 & 44.6 & 20 & 3 \\
11 & 46.4 & 20 & 3 & 24 & 33.4 & 20 & 2 \\
12 & 28.9 & 21 & 6 & 25 & 28.6 & 22 & 15 \\
13 & 28.1 & 21 & 6 & & & & \\ \end{tabular}

Fitting models (1) through (4) gives the following results:

\begin{tabular}{l c c c c} Model & (1) & (2) & (3) & (4) \\ \hline \(dfE\) & 22 & 22 & 10 & 9 \\ _SSE_ & 46.50 & 50.75 & 12.136 & 7.5 \\ \end{tabular}

Shillington's test is

\[F_{S}=\frac{[50.75-12.136]/[22-10]}{7.5/9}=3.8614>3.073=F(0.95,12,9).\]

Christensen's (1989) test is

\[F_{89}=\frac{[46.50-7.5]/[22-9]}{7.5/9}=3.6000>3.048=F(0.95,13,9).\]

Christensen's (1991) test is

\[F_{91}=\frac{[50.75-12.136]/[22-10]}{[46.5-(50.75-12.136)]/[22-(22-10)]}=4.0804 >2.913=F(0.95,12,10).\]

All three tests indicate a lack of fit.

In this example, all of the tests behaved similarly. Christensen (1991) shows that the tests can be quite different and that for the single most interesting type of lack of fit, the 91 test will typically be more powerful than Shillington's test, which is typically better than the 89 test.

Christensen's 89 test is similar in spirit to "nonparametric" lack-of-fit tests based on spanning (or basis) functions used to approximate general regression models, see Subsection 6.2.2. In particular, it is similar to adding Haar wavelets as additional predictor variables to model (1) except that Haar wavelets amount to adding indicator variables for a predetermined partition of the space of predictor variables while the near replicate methods use the observed predictors to suggest where indicator variables are needed. See Subsection 4 and _ALM-III_, Section 1.8 for additional discussion of these nonparametric approaches.

#### Partitioning Methods

Another way to use \(X\) in determining a more general matrix \(Z\) is to partition the data. Write

\[X=\begin{bmatrix}X_{1}\\ X_{2}\end{bmatrix},\quad Y=\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}.\]

The model \(Y=Z\gamma+e\) can be chosen with

\[Z=\begin{bmatrix}X_{1}&0\\ 0&X_{2}\end{bmatrix}.\]Clearly, \(C(X)\subset C(Z)\). We again refer to the difference \(SSE(X)-SSE(Z)\) as the \(SSLF\) and, in something of an abuse of the concept "pure," we continue to call \(SSE(Z)\) the \(SSPE\).

**Exercise 6.13**: Let \(M_{i}\) be the perpendicular projection operator onto \(C(X_{i})\), \(i=1\), \(2\). Show that the perpendicular projection operator onto \(C(Z)\) is

\[M_{Z}=\begin{bmatrix}M_{1}&0\\ 0&M_{2}\end{bmatrix}.\]

Show that \(SSE(Z)=SSE(X_{1})+SSE(X_{2})\), where \(SSE(X_{i})\) is the sum of squares for error from fitting \(Y_{i}=X_{i}\beta_{i}+e_{i}\), \(i=1\), \(2\).

If there is no lack of fit for \(Y=X\beta+e\), since \(C(X)\subset C(Z)\), the test statistic will have a central \(F\) distribution. Suppose that there is lack of fit and the true model is, say, \(Y=W\delta+e\). It is unlikely that \(W\) will have the property that \(C(X)\subset C(W)\subset C(Z)\), which would ensure that the test statistic has a noncentral \(F\) distribution. In general, if there is lack of fit, the test statistic has a doubly noncentral \(F\) distribution. (A doubly noncentral \(F\) is the ratio of two independent noncentral chi-squareds divided by their degrees of freedom.) The idea behind the lack-of-fit test based on partitioning the data is the hope that \(X_{1}\) and \(X_{2}\) will be chosen so that the combined fit of \(Y_{1}=X_{1}\beta_{1}+e_{1}\) and \(Y_{2}=X_{2}\beta_{2}+e_{2}\) will be qualitatively better than the fit of \(Y=X\beta+e\). Thus, it is hoped that the noncentrality parameter of the numerator chi-squared will be larger than the noncentrality parameter of the denominator chi-squared.

**Example 6.7.2**: Let \(Y=X\beta+e\) be the simple linear regression \(y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\), \(i=1\), \(\ldots\), \(2r\), with \(x_{1}\leq x_{2}\leq\cdots\leq x_{2r}\). Suppose that the lack of fit is due to the true model being \(y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+e_{i}\), so the true curve is a parabola. Clearly, one can approximate a parabola better with two lines than with one line. The combined fit of \(y_{i}=\eta_{0}+\eta_{1}x_{i}+e_{i}\), \(i=1\), \(\ldots\), \(r\), and \(y_{i}=\tau_{0}+\tau_{1}x_{i}+e_{i}\), \(i=r+1\), \(\ldots\), \(2r\), should be better than the unpartitioned fit.

**Example 6.7.3**: We now test the model used in Example 6.2.1 for lack of fit using the partitioning method. The difficulty with the partitioning method lies in finding some reasonable way to partition the data. Fortunately for me, I constructed this example, so I know where the lack of fit is and I know a reasonable way to partition the data. (The example was constructed just like Example 12.4.4. The construction is explained in Chapter 12.) I partitioned the data based on the variable \(x_{1}\). Any case that had a value of \(x_{1}\) less than 24 went into one group. The remaining cases went into the other group. This provided a group of 12 cases with small \(x_{1}\) values and a group of 13 cases with large \(x_{1}\) values. The sum of squares for error for the small group was \(SSE(S)=2.925\) with 9 degrees of freedom and the sum of squares for error for the large group was \(SSE(L)=13.857\) with 10 degrees of freedom. Using the error from Example 6.2.1, we get \[\begin{array}{l}\mathit{SSPE}=13.857+2.925=16.782,\\ \mathit{dfPE}=10+9=19,\\ \mathit{MSPE}=0.883,\\ \\ \mathit{SSLF}=46.50-16.78=29.72,\\ \mathit{dfLF}=22-19=3,\\ \mathit{MSLF}=9.91,\\ \mathit{F}=9.91/0.883=11.22.\\ \end{array}\]

This has 3 degrees of freedom in the numerator, 19 degrees of freedom in the denominator, is highly significant, and indicates a definite lack of fit. But remember, I knew that the lack of fit was related to \(x_{1}\), so I could pick an effective partition.

Recall from Example 6.4.1 that the \(R^{2}\) for Example 6.2.1 is 0.964, which indicates a very good predictive model. In spite of the high \(R^{2}\), we are still able to establish a lack of fit using both the partitioning method and the near replicate method.

The partitioning method can easily be extended, cf. Atwood and Ryan (1977). For example, one could select three partitions of the data and write

\[X=\begin{bmatrix}X_{1}\\ X_{2}\\ X_{3}\end{bmatrix},\quad Y=\begin{bmatrix}Y_{1}\\ Y_{2}\\ Y_{3}\end{bmatrix},\quad Z=\begin{bmatrix}X_{1}&0&0\\ 0&X_{2}&0\\ 0&0&X_{3}\end{bmatrix}.\]

The lack-of-fit test would proceed as before. Note that the partitioning method is actually a generalization of the traditional method. If the partition of the data consists of the different sets of identical rows of the model matrix, then the partitioning method gives the traditional lack-of-fit test. The partitioning method can also be used to give a near replicate lack-of-fit test with the partitions corresponding to the clusters of near replicates. As mentioned earlier, it is not clear in general how to choose an appropriate partition.

Utts (1982) presented a particular partition to be used in what she called the Rainbow Test (for lack of fit). She suggests selecting a set of rows from \(X\) that are centrally located to serve as \(X_{1}\), and placing each row of \(X\) not in \(X_{1}\) into a separate set that consists only of that row. With this partitioning, each of the separate sets determined by a single row corresponds to \(p\) columns of \(Z\) that are zero except for the entries in that row. These \(p\) columns are redundant. Eliminating unnecessary columns allows the \(Z\) matrix to be rewritten as

\[Z=\begin{bmatrix}X_{1}&0\\ 0&I\end{bmatrix}.\]From Exercise 6.13, it is immediately seen that \(SSE(Z)=SSE(X_{1})\); thus, the Rainbow Test amounts to testing \(Y=X\beta+e\) against \(Y_{1}=X_{1}\beta+e_{1}\), see also Section 9.3. To select the matrix \(X_{1}\), Utts suggests looking at the diagonal elements of \(M=[m_{ij}]\). The smallest values of the \(m_{ii}\)s are the most centrally located data points, cf. Section 12.1. The author's experience indicates that the Rainbow Test works best when one is quite selective about the points included in the central partition.

_Example 6.7.4 Continued_. First consider the Rainbow Test using half of the data set. The variables \(x_{1}\), \(x_{2}\), and an intercept were fitted to the 12 cases that had the smallest \(m_{ii}\) values. This gave a \(SSE=16.65\) with 9 degrees of freedom. The Rainbow Test mean square for lack of fit, mean square for pure error, and \(F\) statistic are

\[MSLF=(46.50-16.65)/(22-9)=2.296,\]

\[MSPE=16.65/9=1.850,\]

\[F=2.296/1.850=1.24.\]

The \(F\) statistic is nowhere near being significant. Now consider taking the quarter of the data with the smallest \(m_{ii}\) values. These 6 data points provide a \(SSE=0.862\) with 3 degrees of freedom.

\[MSLF=(46.50-0.862)/(22-3)=2.402,\]

\[MSPE=0.862/3=0.288,\]

\[F=2.402/0.288=8.35.\]

In spite of the fact that this has only 3 degrees of freedom in the denominator, the \(F\) statistic is reasonably significant. \(F(0.95,19,3)\) is approximately 8.67 and \(F(0.90,19,3)\) is about 5.19.

#### Nonparametric Methods

As discussed in Subsection 6.2.2, one approach to nonparametric regression is to fit very complicated linear models using "basis" functions. One particular application of this approach to nonparametric regression is the fitting of moderate (as opposed to low or high) order polynomials. _ALM-III_, Chapter 1 provides more details of this general approach to nonparametric regression and in particular Section 1.8 discusses testing lack of fit. Fundamentally, the idea is to test the original linear model \(y_{i}=x^{\prime}_{i}\beta+e_{i}\) against a larger model that incorporates nonparametric regression components, i.e.,\(y_{i}=x_{i}^{\prime}\beta+\sum_{j=1}^{q}\gamma_{j}\phi_{j}(x_{i})+e_{i}\). For high dimensional problems, the larger model may need to involve generalized additive functions.

**Exercise 6.14**: Test the model \(y_{ij}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+e_{ij}\) for lack of fit using the data:

\[\begin{array}{c|rrr r r}x_{i}&1.00&2.00&0.00&-3.00&2.50\\ \hline y_{ij}&3.41&22.26&-1.74&79.47&37.96\\ &2.12&14.91&1.32&80.04&44.23\\ &6.26&23.41&-2.55&81.63\\ &18.39&\end{array}\]

**Exercise 6.15**: Using the following data, test the model \(y_{ij}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+e_{ij}\) for lack of fit. Explain and justify your method.

\[\begin{array}{c c|rrr r}X_{1}&X_{2}&Y&X_{1}&X_{2}&Y\\ \hline 31&9.0&122.41&61&2.2&70.08\\ 43&8.0&115.12&36&4.7&66.42\\ 50&2.8&64.90&52&9.4&150.15\\ 38&5.0&64.91&38&1.5&38.15\\ 38&5.1&74.52&41&1.0&45.67\\ 51&4.6&75.02&41&5.0&68.66\\ 41&7.2&101.36&52&4.5&76.15\\ 57&4.0&74.45&29&2.7&36.20\\ 46&2.5&56.22&\end{array}\]

### Polynomial Regression and One-Way ANOVA

Polynomial regression is the special case of fitting a model

\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\cdots+\beta_{p-1}x_{i}^{p-1} +e_{i},\]

i.e.,

\[Y=\begin{bmatrix}1&x_{1}&x_{1}^{2}&\cdots&x_{1}^{p-1}\\ 1&x_{2}&x_{2}^{2}&\cdots&x_{2}^{p-1}\\ \vdots&\vdots&\vdots&&\vdots\\ 1&x_{n}&x_{n}^{2}&\cdots&x_{n}^{p-1}\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{1}\\ \vdots\\ \beta_{p-1}\end{bmatrix}+e.\]

All of the standard multiple regression results hold, but there are some additional issues to consider. For instance, one should think very hard about whether it makes sense to test \(H_{0}:\beta_{j}=0\) for any \(j\) other than \(j=p-1\). Frequently, the model

\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\cdots+\beta_{j-1}x_{i}^{j-1}+\beta_{j+1}x_{i}^ {j+1}+\cdots+\beta_{p-1}x_{i}^{p-1}+e_{i}\]is not very meaningful. Typically, it only makes sense to test the coefficient of the highest order term in the polynomial. One would only test \(\beta_{j}=0\) if it had already been decided that \(\beta_{j+1}=\cdots=\beta_{p-1}=0\).

Sometimes, polynomial regression models are fitted using orthogonal polynomials. This is a procedure that allows one to perform all the appropriate tests on the \(\beta_{j}\)s without having to fit more than one regression model. The technique uses the Gram-Schmidt algorithm to orthogonalize the columns of the model matrix and then fits a model to the orthogonalized columns. Since Gram-Schmidt orthogonalizes vectors sequentially, the matrix with orthogonal columns can be written \(T=XP\), where \(P\) is a nonsingular upper triangular matrix. The model \(Y=X\beta+e\) is equivalent to \(Y=T\gamma+e\) with \(\gamma=P^{-1}\beta\). \(P^{-1}\) is also an upper triangular matrix, so \(\gamma_{j}\) is a linear function of \(\beta_{j}\), \(\beta_{j+1}\),..., \(\beta_{p-1}\). The test of \(H_{0}:\gamma_{p-1}=0\) is equivalent to the test of \(H_{0}:\beta_{p-1}=0\). If \(\beta_{j+1}=\beta_{j+2}=\cdots=\beta_{p-1}=0\), then the test of \(H_{0}:\gamma_{j}=0\) is equivalent to the test of \(H_{0}:\beta_{j}=0\). In other words, the test of \(H_{0}:\gamma_{j}=0\) is equivalent to the test of \(H_{0}:\beta_{j}=0\) in the model \(y_{i}=\beta_{0}+\cdots+\beta_{j}x_{i}^{j}+e_{i}\). However, because the columns of \(T\) are orthogonal, the sum of squares for testing \(H_{0}:\gamma_{j}=0\) depends only on the column of \(T\) associated with \(\gamma_{j}\). It is not necessary to do any additional model fitting to obtain the test.

An algebraic expression for the orthogonal polynomials being fitted is available in the row vector

\[[1,x,x^{2},\ldots,x^{p-1}]P. \tag{1}\]

The \(p-1\) different polynomials that are contained in this row vector are orthogonal only in that the coefficients of the polynomials were determined so that \(XP\) has columns that are orthogonal. As discussed above, the test of \(\gamma_{j}=0\) is the same as the test of \(\beta_{j}=0\) when \(\beta_{j+1}=\cdots=\beta_{p-1}=0\). The test of \(\gamma_{j}=0\) is based on the \((j+1)\)st column of the matrix \(T\). \(\beta_{j}\) is the coefficient of the \((j+1)\)st column of \(X\), i.e., the \(j\)th degree term in the polynomial. By analogy, _the \((j+1)\)st column of \(T\) is called the \(j\)th degree orthogonal polynomial_.

Polynomial regression has some particularly interesting relationships with the problem of estimating pure error and with one-way ANOVA problems. It is clear that for all values of \(p\), the row structure of the model matrices for the polynomial regression models is the same, i.e., if \(x_{i}=x_{i^{\prime}}\), then \(x_{i}^{k}=x_{i^{\prime}}^{k}\); so the \(i\) and \(i^{\prime}\) rows of \(X\) are the same regardless of the order of the polynomial. Suppose there are \(q\) distinct values of \(x_{i}\) in the model matrix. The most general polynomial that can be fitted must give a rank \(q\) model matrix; thus the most general model must be

\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\cdots+\beta_{q-1}x_{i}^{q-1}+e_{i}.\]

It also follows from the previous section that the column space of this model is exactly the same as the column space for fitting a one-way ANOVA with \(q\) treatments.

Using double subscript notation with \(i=1,\ldots,q\), \(j=1,\ldots,N_{i}\), the models

\[y_{ij}=\beta_{0}+\beta_{1}x_{i}+\cdots+\beta_{q-1}x_{i}^{q-1}+e_{ij} \tag{2}\]\[y_{ij}=\mu+\alpha_{i}+e_{ij}\]

are equivalent. Since both \(\beta_{0}\) and \(\mu\) are parameters corresponding to a column of \(1\)s, the tests of \(H_{0}:\beta_{1}=\cdots=\beta_{q-1}=0\) and \(H_{0}:\alpha_{1}=\cdots=\alpha_{q}\) are identical. Both tests look at the orthogonal complement of \(J_{n}\) with respect to \(C(X)\), where \(C(X)\) is the column space for either of the models. Using the ideas of Section 3.6, one way to break this space up into \(q-1\) orthogonal one-degree-of-freedom hypotheses is to look at the orthogonal polynomials for \(i=1,\ldots,q-1\). As seen in Section 4.2, any vector in \(C(X)\) that is orthogonal to \(J\) determines a contrast in the \(\alpha_{i}\)s. In particular, each orthogonal polynomial corresponds to a contrast in the \(\alpha_{i}\)s.

Finding a set of \(q-1\) orthogonal contrasts amounts to finding an orthogonal basis for \(C(M_{\alpha})\). If we write \(T=[T_{0},\ldots,T_{q-1}]\), then \(T_{1},\ldots,T_{q-1}\) is an orthogonal basis for \(C(M_{\alpha})\). Given these vectors in \(C(M_{\alpha})\), we can use Proposition 4.2.3 to read off the corresponding contrasts. Moreover, the test for dropping, say, \(T_{j}\) from the model is the test of \(H_{0}:\gamma_{j}=0\), which is just the test that the corresponding contrast is zero. Note that testing this contrast is not of interest unless \(\beta_{j+1}=\cdots=\beta_{q-1}=0\) or, equivalently, if \(\gamma_{j+1}=\cdots=\gamma_{q-1}=0\) or, equivalently, if all the higher order polynomial contrasts are zero.

**Definition 6.8.1**  The orthogonal contrasts determined by the orthogonal polynomials are called the _polynomial contrasts_. The contrast corresponding to the first degree orthogonal polynomial is called the _linear contrast_. The contrasts for higher degree orthogonal polynomials are called the _quadratic_, _cubic_, _quartic_, etc., contrasts.

Using Proposition 4.2.3, if we identify an orthogonal polynomial as a vector \(\rho\in C(M_{\alpha})\), then the corresponding contrast can be read off. For example, the second column of the model matrix for (2) is \(X_{1}=[t_{ij}]\), where \(t_{ij}=x_{i}\) for all \(i\) and \(j\). If we orthogonalize this with respect to \(J\), we get the linear orthogonal polynomial. Letting

\[\bar{x}.=\sum_{i=1}^{q}N_{i}x_{i}\Big{/}\sum_{i=1}^{q}N_{i}\]

leads to the linear orthogonal polynomial

\[T_{1}=[w_{ij}],\quad\quad\text{where }w_{ij}=x_{i}-\bar{x}.\]

From Section 4.2, this vector corresponds to a contrast \(\sum\lambda_{i}\alpha_{i}\), where \(\lambda_{i}/N_{i}=x_{i}-\bar{x}.\). Solving for \(\lambda_{i}\) gives

\[\lambda_{i}=N_{i}(x_{i}-\bar{x}.).\]

The sum of squares for testing \(H_{0}:\beta_{1}=\gamma_{1}=0\) is \[\left[\sum_{i}N_{i}(x_{i}-\bar{x}.)\bar{y}_{i}.\right]^{2}\Big{/}\left[\sum_{i}N_{ i}(x_{i}-\bar{x}.)^{2}\right].\]

And, of course, one would not do this test unless it had already been established that \(\beta_{2}=\cdots=\beta_{q-1}=0\).

As with other directions in vector spaces and linear hypotheses, orthogonal polynomials and orthogonal polynomial contrasts are only of interest up to constant multiples. In applying the Gram-Schmidt theorem to obtain orthogonal polynomials, we really do not care about normalizing the columns. It is the sequential orthogonalization that is important. In the example of the linear contrast, we did not bother to normalize anything.

It is well known (see Exercise 6.16) that, if \(N_{i}=N\) for all \(i\) and the quantitative levels \(x_{i}\) are equally spaced, then the orthogonal polynomial contrasts (up to constant multiples) depend only on \(q\). For any value of \(q\), the contrasts can be tabled; see Table 6.2 or, for example, Snedecor and Cochran (1980).

Although it is difficult to derive the tabled contrasts directly, one can verify the appropriateness of the tabled contrasts. Again we appeal to Chapter 4. Let \([J,\,Z]\) be the model matrix for the one-way ANOVA and let \(X\) be the model matrix for model (2). The model matrix for the orthogonal polynomial model is \(T=XP\). With \(C(X)=C(Z)\), we can write \(T=ZB\) for some matrix \(B\). Writing the \(q\,\times\,q\) matrix \(B\) as \(B=\big{[}b_{0},\ldots,b_{q-1}\big{]}\) with \(b_{k}=\big{(}b_{1k},\ldots,b_{qk}\big{)}^{\prime}\), we will show that for \(k\geq 1\) and \(N_{i}=N\), the \(k\)th degree orthogonal polynomial contrast is \(b_{k}^{\prime}\alpha\), where \(\alpha=\big{(}\alpha_{1},\ldots,\alpha_{q}\big{)}^{\prime}\). To see this, note that the \(k\)th degree orthogonal polynomial is \(T_{k}=Zb_{k}\). The first column of \(X\) is a column of \(1\)s and \(T\) is a successive orthogonalization of the columns of \(X\); so \(J_{n}=Zb_{0}\) and for \(k\geq 1\), \(Zb_{k}\perp J_{n}\). It follows that \(b_{0}=J_{q}\) and, from Chapter 4, for \(k\geq 1\), \(Zb_{k}\in C(M_{\alpha})\). Thus, for \(k\geq 1\), \(Zb_{k}\) determines a contrast \((Zb_{k})^{\prime}Z\alpha\equiv\sum_{i=1}^{q}\lambda_{i}\alpha_{i}\). However, \((Zb_{k})^{\prime}Z\alpha=b_{k}^{\prime}Z^{\prime}Z\alpha=b_{k}^{\prime}\mbox {Diag}(N_{i})\alpha\). The contrast coefficients \(\lambda_{1},\ldots,\lambda_{q}\) satisfy \(b_{ik}N_{i}=\lambda_{i}\). When \(N_{i}=N\), the contrast is \(\sum_{i}\lambda_{i}\alpha_{i}=N\sum_{i}b_{ik}\alpha_{i}=Nb_{k}^{\prime}\alpha\). Orthogonal polynomial contrasts are defined only up to constant multiples, so the \(k\)th degree orthogonal polynomial contrast is also \(b_{k}^{\prime}\alpha\).

Given a set of contrast vectors \(b_{1},\ldots,b_{q-1}\), we can check whether these are the orthogonal polynomial contrasts. Simply compute the corresponding matrix \(T=ZB\) and check whether this constitutes a successive orthogonalization of the columns of \(X\).

Ideas similar to these will be used in Section 9.4 to justify the use of tabled contrasts in the analysis of balanced incomplete block designs. These ideas also relate to Section 7.3 on Polynomial Regression and the Balanced Two-Way ANOVA. Finally, these ideas relate to nonparametric regression as discussed in Subsection 6.2.2. There, polynomials were used as an example of a class of functions that can be used to approximate arbitrary continuous functions. Other examples mentioned were cosines and wavelets. The development given in this section for polynomials can be mimicked for any approximating class of functions.

For completeness, an alternative idea of orthogonal polynomials should be mentioned. In equation (1), rather than using the matrix \(P\) that transforms the columns of \(X\) into \(T\) with orthonormal columns, one could instead choose a \(P_{0}\) so that the transformed functions are orthogonal in an appropriate function space. The Legendre polynomials are such a collection. The fact that such orthogonal polynomials do not depend on the specific \(x_{i}\)s in the data is both an advantage and a disadvantage. It is an advantage in that they are well known and do not have to be derived for each unique set of \(x_{i}\)s. It is a disadvantage in that, although they display better numerical properties than the unadjusted polynomials, since \(T_{0}=XP_{0}\) typically does not have (exactly) orthonormal columns, these polynomials will not display the precise features exploited earlier in this section.

\begin{table}
\begin{tabular}{c c c|c c c|c c c}  & \multicolumn{2}{c}{\(q=3\)} & \multicolumn{2}{c}{\(q=4\)} & \multicolumn{2}{c}{\(q=5\)} \\ L & Q & C & L & Q & C & L & Q & C \\ \hline \(-1\) & \(1\) & & \(-3\) & \(1\) & \(-1\) & \(-2\) & \(2\) & \(-1\) \\ \(0\) & \(-2\) & & \(-1\) & \(-1\) & \(3\) & \(-1\) & \(-1\) & \(2\) \\ \(1\) & \(1\) & & \(1\) & \(-1\) & \(-3\) & \(0\) & \(-2\) & \(0\) \\  & & & \(3\) & \(1\) & \(1\) & \(1\) & \(-1\) & \(-2\) \\  & & & & & & & \(2\) & \(2\) & \(1\) \\ \hline  & \multicolumn{2}{c}{\(q=6\)} & \multicolumn{2}{c}{\(q=7\)} & \multicolumn{2}{c}{\(q=8\)} \\ L & Q & C & L & Q & C & L & Q & C \\ \hline \(-5\) & \(5\) & \(-5\) & \(3\) & \(5\) & \(-1\) & \(-7\) & \(7\) & \(-7\) \\ \(-3\) & \(-1\) & \(7\) & \(2\) & \(0\) & \(1\) & \(-5\) & \(1\) & \(5\) \\ \(-1\) & \(-4\) & \(4\) & \(1\) & \(-3\) & \(1\) & \(-3\) & \(-3\) & \(7\) \\ \(1\) & \(-4\) & \(-4\) & \(0\) & \(-4\) & \(0\) & \(-1\) & \(-5\) & \(3\) \\ \(3\) & \(-1\) & \(-7\) & \(1\) & \(-3\) & \(-1\) & \(1\) & \(-5\) & \(-3\) \\ \(5\) & \(5\) & \(5\) & \(2\) & \(0\) & \(-1\) & \(3\) & \(-3\) & \(-7\) \\  & & & & \(3\) & \(5\) & \(1\) & \(5\) & \(1\) & \(-5\) \\  & & & & & & \(7\) & \(7\) \\ \hline  & \multicolumn{2}{c}{\(q=9\)} & \multicolumn{2}{c}{\(q=10\)} & \multicolumn{2}{c}{\(q=11\)} \\ L & Q & C & L & Q & C & L & Q & C \\ \hline \(-4\) & \(28\) & \(-14\) & \(-9\) & \(6\) & \(-42\) & \(-5\) & \(15\) & \(-30\) \\ \(-3\) & \(7\) & \(7\) & \(-7\) & \(2\) & \(14\) & \(-4\) & \(6\) & \(6\) \\ \(-2\) & \(-8\) & \(13

**Exercise 6.16**: (a) Find the model matrix for the orthogonal polynomial model \(Y=T\gamma+e\) corresponding to the model

\[y_{ij}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\beta_{3}x_{i}^{3}+e_{ij},\]

\(i=1,\,2,\,3,\,4,\,j=1,\,\ldots,\,N\), where \(x_{i}=a+(i-1)t\).

Hint: First consider the case \(N=1\).

(b) For the model \(y_{ij}=\mu+\alpha_{i}+e_{ij}\), \(i=1,\,2,\,3,\,4,\,j=1,\,\ldots,\,N\), and for \(k=1,\,2,\,3\), find the contrast \(\sum\lambda_{ik}\alpha_{i}\) such that the test of \(H_{0}:\sum\lambda_{ik}\alpha_{i}=0\) is the same as the test of \(H_{0}:\gamma_{k}=0\), i.e., find the polynomial contrasts.

**Exercise 6.17**: Repeat Exercise 6.16 with \(N=2\) and \(x_{1}=2\), \(x_{2}=3\), \(x_{3}=5\), \(x_{4}=8\).

### Additional Exercises

The first three exercises involve using _Fieller's method_ for finding confidence intervals, cf. Exercise 3.7b.

**Exercise 6.9.1**: _Calibration._

Consider the regression model \(Y=X\beta+e\), \(e\sim N(0,\,\sigma^{2}I)\) and suppose that we are interested in a future observation, say \(y_{0}\), that will be independent of \(Y\) and have mean \(x_{0}^{\prime}\beta\). In previous work with this situation, \(y_{0}\) was not yet observed but the corresponding vector \(x_{0}\) was known. The calibration problem reverses these assumptions. Suppose that we have observed \(y_{0}\) and wish to infer what the corresponding vector \(x_{0}\) might be.

A typical calibration problem might involve two methods of measuring some quantity: \(y\), a cheap and easy method, and \(x\), an expensive but very accurate method. Data are obtained to establish the relationship between \(y\) and \(x\). Having done this, future measurements are made with \(y\) and the calibration relationship is used to identify what the value of \(y\) really means. For example, in sterilizing canned food, \(x\) would be a direct measure of the heat absorbed into the can, while \(y\) might be the number of bacterial spores of a certain strain that are killed by the heat treatment. (Obviously, one needs to be able to measure the number of spores in the can both before and after heating.)

Consider now the simplest calibration model, \(y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\), \(e_{i}\)s i.i.d. \(N(0,\,\sigma^{2}),i=1,\,2,\,3,\,\ldots,\,n\). Suppose that \(y_{0}\) is observed and that we wish to estimate the corresponding value \(x_{0}\) (\(x_{0}\) is viewed as a parameter here).

(a) Find the MLEs of \(\beta_{0}\), \(\beta_{1}\), \(x_{0}\), and \(\sigma^{2}\).

Hint: This is a matter of showing that the obvious estimates are MLEs.

Suppose now that a series of observations \(y_{01},\ldots,y_{0r}\) were taken, all of which correspond to the same \(x_{0}\). Find the MLEs of \(\beta_{0}\), \(\beta_{1}\), \(x_{0}\), and \(\sigma^{2}\).

Hint: Only the estimate of \(\sigma^{2}\) changes form.

(c) Based on one observation \(y_{0}\), find a \((1-\alpha)100\%\) confidence interval for \(x_{0}\). When does such an interval exist?

Hint: Use an \(F(1,n-2)\) distribution based on \((y_{0}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{0})^{2}\).

Comment: Aitchison and Dunsmore (1975) discuss calibration in considerable detail, including a comparison of different methods.

**Exercise 6.9.2**: _Maximizing a Quadratic Response_. Consider the model, \(y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+e_{i},e_{i}\)s i.i.d. \(N(0,\sigma^{2}),i=1,2,3,\ldots,n\). Let \(x_{0}\) be the value at which the function \(\mathrm{E}(y)=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}\) is maximized (or minimized).

(a) Find the maximum likelihood estimate of \(x_{0}\).

(b) Find a \((1-\alpha)100\%\) confidence interval for \(x_{0}\). Does such an interval always exist?

Hint: Use an \(F(1,n-3)\) distribution based on \((\hat{\beta}_{1}+2\hat{\beta}_{2}x_{0})^{2}\).

Comment: The problem of finding values of the independent variables that maximize (or minimize) the expected \(y\) value is a basic problem in the field of response surface methods. See Box et al. (1978) or Christensen (2001, Chapter 8) or [http://www.stat.unm.edu/~fletcher/TopicsInDesign](http://www.stat.unm.edu/~fletcher/TopicsInDesign)) for an introduction to the subject. Box and Draper (1987) give a detailed treatment.

**Exercise 6.9.3**: _Two-Phase Linear Regression_. Consider the problem of sterilizing canned pudding. As the pudding is sterilized by a heat treatment, it is simultaneously cooked. If you have ever cooked pudding, you know that it starts out soupy and eventually thickens. That, dear reader, is the point of this little tale. Sterilization depends on the transfer of heat to the pudding and the rate of transfer depends on whether the pudding is soupy or gelatinous. On an appropriate scale, the heating curve is linear in each phase. The question is, "Where does the line change phases?"

Suppose that we have collected data \((y_{i},x_{i})\), \(i=1,\ldots,n+m\), and that we know that the line changes phases between \(x_{n}\) and \(x_{n+1}\). The model \(y_{i}=\beta_{10}+\beta_{11}x_{i}+e_{i}\), \(e_{i}\)s i.i.d. \(N(0,\sigma^{2})\), \(i=1,\ldots,n\), applies to the first phase and the model \(y_{i}=\beta_{20}+\beta_{21}x_{i}+e_{i}\), \(e_{i}\)s i.i.d. \(N(0,\sigma^{2})\), \(i=n+1,\ldots,n+m\), applies to the second phase. Let \(\gamma\) be the value of \(x\) at which the lines intersect.

(a) Find estimates of \(\beta_{10}\), \(\beta_{11}\), \(\beta_{20}\), \(\beta_{21}\), \(\sigma^{2}\), and \(\gamma\).

Hint: \(\gamma\) is a function of the other parameters.

(b) Find a \((1-\alpha)100\%\) confidence interval for \(\gamma\). Does such an interval always exist?

Hint: Use an \(F(1,n+m-4)\) distribution based on \[\left[(\hat{\beta}_{10}+\hat{\beta}_{11}\gamma)-(\hat{\beta}_{20}+\hat{\beta}_{21 }\gamma)\right]^{2}.\]

Comment: Hinkley (1969) has treated the more realistic problem in which it is not known between which \(x_{i}\) values the intersection occurs.

**Exercise 6.9.4**: Consider the model \(y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\), \(e_{i}\)s i.i.d. \(N(0,\sigma^{2}/w_{i})\), \(i=1,\,2,\,3,\,\ldots,\,n\), where the \(w_{i}\)s are known numbers. Derive algebraic formulas for \(\hat{\beta}_{0}\), \(\hat{\beta}_{1}\), \(\mbox{Var}(\hat{\beta}_{0})\), and \(\mbox{Var}(\hat{\beta}_{1})\).

**Exercise 6.9.5**: Consider the model \(y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\), \(e_{i}\)s i.i.d. \(N(0,\sigma^{2})\), \(i=1,\,2,\,3,\,\ldots,\,n\). If the \(x_{i}\)s are restricted to be in the closed interval \([-10,\,15]\), determine how to choose the \(x_{i}\)s to minimize

(a) \(\mbox{Var}(\hat{\beta}_{0})\).

(b) \(\mbox{Var}(\hat{\beta}_{1})\).

(c) How would the choice of the \(x_{i}\)s change if they were restricted to the closed interval \([-10,\,10]\)?

**Exercise 6.9.6**: Find \(\mbox{E}[y-\hat{E}(y|x)]^{2}\) in terms of the variances and covariances of \(x\) and \(y\). Give a "natural" estimate of \(\mbox{E}\!\left[y-\hat{E}(y|x)\right]^{2}\).

**Exercise 6.9.7**: Test whether the data of Example 6.2.1 indicate that the multiple correlation coefficient is different from zero.

**Exercise 6.9.8**: Test whether the data of Example 6.2.1 indicate that the partial correlation coefficient \(\rho_{y1\cdot 2}\) is different from zero.

**Exercise 6.9.9**: Show that

(a) \(\rho_{12\cdot 3}=\frac{\rho_{12}-\rho_{13}\rho_{23}}{\sqrt{1-\rho_{13}^{2}} \sqrt{1-\rho_{23}^{2}}}\)

(b) \(\rho_{12\cdot 34}=\frac{\rho_{12\cdot 4}-\rho_{13\cdot 4}\rho_{23\cdot 4}}{\sqrt{1-\rho_{13\cdot 4}^{2}}\sqrt{1-\rho_{23\cdot 4}^{2}}}\).

**Exercise 6.9.10**: Show that in Section 2, \(\gamma_{*}=\beta_{*}\) and \(\beta_{0}=\gamma_{0}-(1/n)J_{1}^{n}Z\gamma_{*}\).

Hint: Examine the corresponding argument given in Section 1 for simple linear regression.

[MISSING_PAGE_FAIL:213]

* Morrison (2004) Morrison, D. F. (2004). _Multivariate statistical methods_ (4th ed.). Pacific Grove: Duxbury Press.
* a review. _Communications in Statistics, Part A
- Theory and Methods_, _13_, 485-511.
* Pimentel et al. (2017) Pimentel, S. D., Small, D. S., & Rosenbaum, P. R. (2017). An exact test of fit for the Gaussian linear model using optimal nonbipartite matching. _Technometrics_, _59_, 330-337.
* Rao (1973) Rao, C. R. (1973). _Linear statistical inference and its applications_ (2nd ed.). New York: Wiley.
* Ripley (1981) Ripley, B. D. (1981). _Spatial statistics_. New York: Wiley.
* Shillington (1979) Shillington, E. R. (1979). Testing lack of fit in regression without replication. _Canadian Journal of Statistics_, \(7\), 137-146.
* Shumway & Stoffer (2011) Shumway, R. H., & Stoffer, D. S. (2011). _Time series analysis and its applications: With R examples_ (3rd ed.). New York: Springer.
* Smith (1986) Smith, A. F. M. (1986). Comment on an article by B. Efron. _The American Statistician_, _40_, 10.
* Snedecor & Cochran (1980) Snedecor, G. W., & Cochran, W. G. (1980). _Statistical methods_ (7th ed.). Ames: Iowa State University Press.
* Utts (1982) Utts, J. (1982). The rainbow test for lack of fit in regression. _Communications in Statistics--Theory and Methods_, _11_, 2801-2815.
* Weisberg (2014) Weisberg, S. (2014). _Applied linear regression_ (4th ed.). New York: Wiley.

## Chapter 7 Multifactor Analysis of Variance

**Abstract** This chapter presents the analysis of multifactor ANOVA models. The first three sections deal with the balanced two-way ANOVA model. Section 1 examines the no interaction model. Section 2 examines the model with interaction. Section 3 discusses the relationship between polynomial regression and the balanced two-way ANOVA model. Sections 4 and 5 discuss unbalanced two-way ANOVA models. Section 4 treats the special case of proportional numbers. Section 5 examines the general case. Finally, Section 6 extends the earlier results of the chapter to models with more than two factors. A review of the tensor concepts in Appendix B may aid the reader of this chapter.

### 7.1 Balanced Two-Way ANOVA Without Interaction

The balanced two-way ANOVA without interaction model is generally written

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}, \tag{1}\]

\(i=1,\ldots,a,j=1,\ldots,b,k=1,\ldots,N\).

#### Example 7.1.1

Suppose \(a=3\), \(b=2\), \(N=4\). In matrix terms write

\[\begin{bmatrix}y_{111}\\ y_{112}\\ y_{113}\\ y_{114}\\ y_{121}\\ y_{122}\\ y_{123}\\ y_{124}\\ y_{211}\\ y_{212}\\ y_{213}\\ y_{214}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{221}\\ y_{222}\\ y_{223}\\ y_{224}\\ y_{311}\\ y_{312}\\ y_{313}\\ y_{313}\\ y_{314}\\ y_{321}\\ y_{322}\\ y_{323}\\ y_{324}\\ y_{324}\end{bmatrix}=\begin{bmatrix}1&1&0&0&1&0\\ 1&1&0&0&1&0\\ 1&1&0&0&1&0\\ 1&1&0&0&1&0\\ 1&1&0&0&0&1\\ 1&1&0&0&0&1\\ 1&1&0&0&0&1\\ 1&1&0&0&0&1\\ 1&0&0&0&1\\ 1&0&1&0&1&0\\ 1&0&1&0&0&1\\ 1&0&1&0&0&1\\ 1&0&1&0&0&1\\ 1&0&1&0&0&1\\ 1&0&0&1&0&1\\ 1&0&0&1&0&1\\ 1&0&0&1&0&1\\ 1&0&0&1&0&1\\ 1&0&0&1&0&1\\ 1&0&0&1&0&1\\ 1&0&0&1&0&1\\ 1&0&0&1&0&1\\ \end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\\ \eta_{1}\\ \eta_{2}\end{bmatrix}+e.\]

In general, we can write the model as

\[Y=[X_{0},\,X_{1},\,\ldots,\,X_{a},\,X_{a+1},\,\ldots,\,X_{a+b}]\begin{bmatrix} \mu\\ \alpha_{1}\\ \vdots\\ \alpha_{a}\\ \eta_{1}\\ \vdots\\ \eta_{b}\end{bmatrix}+e.\]

Write \(n=ab\,N\) and the observation vector as \(Y=[y_{ijk}]\), where the three subscripts \(i\), \(j\), and \(k\) denote a row of the vector. (Multiple subscripts denoting the rows and columns of matrices were introduced in Chapter 4.) The model matrix of the balanced two-way ANOVA model has

\[\begin{array}{rcl}X_{0}&=&J,\\ X_{r}&=&[t_{ijk}],&t_{ijk}=\delta_{ir},&r=1,\,\ldots,\,a,\\ X_{a+s}&=&[t_{ijk}],&t_{ijk}=\delta_{js},&s=1,\,\ldots,\,b,\end{array}\]

where \(\delta_{gh}=1\) if \(g=h\), and \(0\) otherwise. This is just a formal way of writing down model matrices that look like the one in Example 7.1.1. For example, an observation \(y_{rst}\) is subject to the effects of \(\alpha_{r}\) and \(\eta_{s}\). The _rst_ row of the column \(X_{r}\) needs to be \(1\)so that \(\alpha_{r}\) is added to \(y_{rst}\) and the _rst_ row of \(X_{a+s}\) needs to be 1 so that \(\eta_{s}\) is added to \(y_{rst}\). The _rst_ rows of the columns \(X_{j}\), \(j=1,\ldots,a\), \(j\neq r\) and \(X_{a+j}\), \(j=1,\ldots,b\), \(j\neq s\) need to be 0 so that none of the \(\alpha_{j}\)s other than \(\alpha_{r}\), nor \(\eta_{j}\)s other than \(\eta_{s}\), are added to \(y_{rst}\). The definition of the columns \(X_{r}\) and \(X_{a+s}\) given above ensures that this occurs.

The analysis of this model is based on doing two separate one-way ANOVAs. It is frequently said that this can be done because the treatments (groups) \(\alpha\) and \(\eta\) are orthogonal. This is true in the sense that after fitting \(\mu\), the column space for the \(\alpha\)s is orthogonal to the column space for the \(\eta\)s. (See the discussion surrounding Proposition 3.6.3.) To investigate this further, consider a new matrix

\[Z=[Z_{0},Z_{1},\ldots,Z_{a},Z_{a+1},\ldots,Z_{a+b}],\]

where

\[Z_{0}=X_{0}=J\]

and

\[Z_{r}=X_{r}-\frac{X_{r}^{\prime}J}{J^{\prime}J}J,\]

for \(r=1,\ldots,a+b\). Here we have used Gram-Schmidt to eliminate the effect of \(J\) (the column associated with \(\mu\)) from the rest of the columns of \(X\). Since \(J^{\prime}J=abN\), \(X_{r}^{\prime}J=bN\) for \(r=1,\ldots,a\), and \(X_{a+s}^{\prime}J=aN\) for \(s=1,\ldots,b\), we have

\[Z_{r} = X_{r}-\frac{1}{a}J,\ \ \ r=1,\ldots,a,\] \[Z_{a+s} = X_{a+s}-\frac{1}{b}J,\ \ \ s=1,\ldots,b.\]

Observe that

\[C(X)=C(X_{0},X_{1},\ldots,X_{a},X_{a+1},\ldots,X_{a+b})\\ =C(Z_{0},Z_{1},\ldots,Z_{a},Z_{a+1},\ldots,Z_{a+b})=C(Z),\]

\[C(X_{0},X_{1},\ldots,X_{a})=C(Z_{0},Z_{1},\ldots,Z_{a}),\]

\[C(X_{0},X_{a+1},\ldots,X_{a+b})=C(Z_{0},Z_{a+1},\ldots,Z_{a+b}),\]

\[Z_{0}\perp Z_{r},\ \ \ r=1,\ldots,a+b,\]

and

\[C(Z_{1},\ldots,Z_{a})\ \perp\ C(Z_{a+1},\ldots,Z_{a+b}).\]

To see the last of these, observe that for \(r=1,\ldots,a\) and \(s=1,\ldots,b\),\[Z^{\prime}_{a+s}Z_{r} = \sum_{ijk}(\delta_{js}-1/b)(\delta_{ir}-1/a)\] \[= \sum_{ijk}\delta_{js}\delta_{ir}-\sum_{ijk}\delta_{js}\frac{1}{a}- \sum_{ijk}\delta_{ir}\frac{1}{b}+\sum_{ijk}\frac{1}{ab}\] \[= \sum_{ij}N\delta_{js}\delta_{ir}-\sum_{j}\delta_{js}\frac{aN}{a}- \sum_{i}\delta_{ir}\frac{bN}{b}+\frac{abN}{ab}\] \[= N-aN/a-bN/b+N=0.\]

We have decomposed \(C(X)\) into three orthogonal parts, \(C(Z_{0})\), \(C(Z_{1},\ldots,Z_{a})\), and \(C(Z_{a+1},\ldots,Z_{a+b})\). \(M\), the perpendicular projection operator onto \(C(X)\), can be written as the matrix sum of the perpendicular projection matrices onto these three spaces. By appealing to the one-way ANOVA, we can actually identify these projection matrices.

\(C([X_{0},\,X_{1},\,\ldots,\,X_{a}])\) is the column space for the one-way ANOVA model

\[y_{ijk}=\mu+\alpha_{i}+e_{ijk}, \tag{2}\]

where the subscripts \(j\) and \(k\) are both used to indicate replications. Similarly, \(C([X_{0},\,X_{a+1},\,\ldots,\,X_{a+b}])\) is the column space for the one-way ANOVA model

\[y_{ijk}=\mu+\eta_{j}+e_{ijk}, \tag{3}\]

where the subscripts \(i\) and \(k\) are both used to indicate replications. If one actually writes down the matrix \([X_{0},\,X_{a+1},\,\ldots,\,X_{a+b}]\), it looks a bit different from the usual form of a one-way ANOVA model matrix because the rows have been permuted out of the convenient order generally used.

Let \(M_{\alpha}\) be the projection matrix used to test for no group effects in model (2). \(M_{\alpha}\) is the perpendicular projection matrix onto \(C(Z_{1},\,\ldots,\,Z_{a})\). Similarly, if \(M_{\eta}\) is the projection matrix for testing no group effects in model (3), then \(M_{\eta}\) is the perpendicular projection matrix onto \(C(Z_{a+1},\,\ldots,\,Z_{a+b})\). It follows that

\[M=\frac{1}{n}J^{n}_{n}+M_{\alpha}+M_{\eta}.\]

By comparing models, we see, for instance, that the test for \(H_{0}:\alpha_{1}=\cdots=\alpha_{a}\) is based on

\[\frac{Y^{\prime}M_{\alpha}Y/r(M_{\alpha})}{Y^{\prime}(I-M)Y/r(I-M)}.\]

It is easy to see that \(r(M_{\alpha})=a-1\) and \(r(I-M)=n-a-b+1\). \(Y^{\prime}M_{\alpha}Y\) can be found as in Chapter 4 by appealing to the analysis of the one-way ANOVA model (2). In particular, since pairs \(jk\) identify replications,

\[M_{\alpha}Y=[t_{ijk}],\qquad\text{where }t_{ijk}=\bar{y}_{i,\,\cdot}-\bar{y}_{ \,\cdot\,\cdot} \tag{4}\]\[SS(\alpha)\equiv Y^{\prime}M_{\alpha}Y=[M_{\alpha}Y]^{\prime}[M_{\alpha}Y]=bN\sum_{i= 1}^{a}(\bar{y}_{i\cdots}-\bar{y}_{\cdots})^{2}.\]

Expected mean squares can also be found by appealing to the one-way ANOVA

\[\mathrm{E}(Y^{\prime}M_{\alpha}Y)=\sigma^{2}(a-1)+\beta^{\prime}X^{\prime}M_{ \alpha}X\beta.\]

Substituting \(\mu+\alpha_{i}+\eta_{j}\) for \(y_{ijk}\) in (4) gives

\[M_{\alpha}X\beta=[t_{ijk}],\qquad\text{where }t_{ijk}=\alpha_{i}-\bar{\alpha}.\]

and thus

\[\mathrm{E}\left[Y^{\prime}M_{\alpha}Y/(a-1)\right]=\sigma^{2}+\frac{bN}{a-1} \sum_{i=1}^{a}(\alpha_{i}-\bar{\alpha}.)^{2}.\]

Similar results hold for testing \(H_{0}:\eta_{1}=\cdots=\eta_{b}\).

The _SSE_ can be found using (4) and the facts that

\[M_{\eta}Y=[t_{ijk}],\qquad\text{where }t_{ijk}=\bar{y}_{\cdot j\cdot}-\bar{y}_{ \cdots}\]

and

\[\frac{1}{n}J_{n}^{n}Y=[t_{ijk}],\qquad\text{where }t_{ijk}=\bar{y}_{\cdots}.\]

Because \((I-M)Y=Y-(1/n)J_{n}^{n}Y-M_{\alpha}Y-M_{\eta}Y\),

\[(I-M)Y=[t_{ijk}]\]

where

\[t_{ijk} =y_{ijk}-\bar{y}_{\cdots}-(\bar{y}_{i\cdots}-\bar{y}_{\cdots})-( \bar{y}_{\cdot j\cdot}-\bar{y}_{\cdots})\] \[=y_{ijk}-\bar{y}_{i\cdots}-\bar{y}_{\cdot j\cdot}+\bar{y}_{\cdots}.\]

Finally,

\[SSE =Y^{\prime}(I-M)Y=[(I-M)Y]^{\prime}[(I-M)Y]\] \[=\sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{N}(y_{ijk}-\bar{y}_{i \cdot}-\bar{y}_{\cdot j\cdot}+\bar{y}_{\cdots})^{2}.\]

The analysis of variance table is given in Table 7.1.

\begin{table}
\begin{tabular}{l c c} \hline  & Matrix Notation \\ \hline Source & \(df\) & \(SS\) \\ \hline Grand Mean & 1 & \(Y^{\prime}\left(\frac{1}{n}J_{n}^{n}\right)Y\) \\ Groups(\(\alpha\)) & \(a-1\) & \(Y^{\prime}M_{\alpha}Y\) \\ Groups(\(\eta\)) & \(b-1\) & \(Y^{\prime}M_{\eta}Y\) \\ Error & \(n-a-b+1\) & \(Y^{\prime}(I-M)Y\) \\ \hline Total & \(n=abN\) & \(Y^{\prime}Y\) \\ Source & \(SS\) & E(\(MS\)) \\ \hline Grand Mean & \(SSGM\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}\left(\frac{1}{n}J_{n}^{n}\right)X\beta\) \\ Groups(\(\alpha\)) & \(SS(\alpha)\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}M_{\alpha}X\beta/(a-1)\) \\ Groups(\(\eta\)) & \(SS(\eta)\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}M_{\eta}X\beta/(b-1)\) \\ Error & \(SSE\) & \(\sigma^{2}\) \\ \hline Total & \(SSTot\) \\ \hline \end{tabular} 
\begin{tabular}{l c c} \hline Source & \(df\) & \(SS\) \\ \hline Grand Mean & \(dfGM\) & \(n^{-1}y_{\cdot\cdot\cdot}^{2}=n\tilde{y}_{\cdot\cdot\cdot}^{2}\) \\ Groups(\(\alpha\)) & \(df(\alpha)\) & \(bN\sum_{i=1}^{a}(\tilde{y}_{\cdot\cdot\cdot}-\tilde{y}_{\cdot\cdot\cdot})^{2}\) \\ Groups(\(\eta\)) & \(df(\eta)\) & \(aN\sum_{j=1}^{b}\left(\tilde{y}_{\cdot\cdot\cdot}-\tilde{y}_{\cdot\cdot\cdot} \right)^{2}\) \\ Error & \(dfE\) & \(\sum_{ijk}\left(y_{ijk}-\tilde{y}_{\cdot\cdot\cdot}-\tilde{y}_{\cdot\cdot\cdot} +\tilde{y}_{\cdot\cdot\cdot}\right)^{2}\) \\ \hline Total & \(dfTot\) & \(\sum_{ijk}y_{ijk}^{2}\) \\ Source & \(MS\) & E(\(MS\)) \\ \hline Grand Mean & \(SSGM\) & \(\sigma^{2}+abN(\mu+\tilde{\alpha}_{\cdot\cdot}+\tilde{\eta}_{\cdot\cdot})^{2}\) \\ Groups(\(\alpha\)) & \(SS(\alpha)/(a-1)\) & \(\sigma^{2}+bN\sum_{i=1}^{a}(\alpha_{i}-\tilde{\alpha}_{\cdot\cdot})^{2}/(a-1)\) \\ Groups(\(\eta\)) & \(SS(\eta)/(b-1)\) & \(\sigma^{2}+aN\sum_{j=1}^{b}\left(\eta_{j}-\tilde{\eta}_{\cdot\cdot}\right)^{2}/ (b-1)\) \\ Error & \(SSE/(n-a-b+1)\) & \(\sigma^{2}\) \\ \hline \end{tabular}
\end{table}
Table 1: Balanced two-way analysis of variance table with no interaction

#### Contrasts

We wish to show that estimation and testing of contrasts in a balanced two-way ANOVA is done exactly as in the one-way ANOVA: by ignoring the fact that a second type of group exists. This follows from showing that, say, a contrast in the \(\alpha_{i}\)s involves a constraint on \(C(M_{\alpha})\), and \(C(M_{\alpha})\) is defined by the one-way ANOVA without the \(\eta_{j}\)s.

**Theorem 7.1.2**: _Let \(\lambda^{\prime}\beta\) be estimable and \(\rho^{\prime}X=\lambda^{\prime}\). Then \(\lambda^{\prime}\beta\) is a contrast in the \(\alpha_{i}s\) if and only if \(\rho^{\prime}M=\rho^{\prime}M_{\alpha}\). In this case, \(\lambda^{\prime}\hat{\beta}=\rho^{\prime}MY=\rho^{\prime}M_{\alpha}Y\), which is the estimate from the one-way ANOVA ignoring the \(\eta_{j}s\)._

_Proof_ Let \(\lambda^{\prime}\beta=\sum_{i=1}^{a}c_{i}\alpha_{i}\) with \(\sum_{i=1}^{a}c_{i}=0\). Thus, \(\lambda^{\prime}=(0,c_{1},\ldots,c_{a},0,\ldots,0)\) and \(\lambda^{\prime}J_{a+b+1}=0\). To have such a \(\lambda\) is to have \(\rho\) with

\[\rho^{\prime}X_{i}=0,\quad i=0,a+1,a+2,\ldots,a+b,\]

which happens if and only if \(\rho\) is orthogonal to \(C(Z_{0},Z_{a+1},\ldots,Z_{a+b})=C(M-M_{\alpha})\). In other words, \(\rho^{\prime}(M-M_{\alpha})=0\) and \(\rho^{\prime}M=\rho^{\prime}M_{\alpha}\). \(\square\)

One interpretation of this result is that having a contrast in the \(\alpha_{i}\)s equal to zero puts a constraint on \(C(X)\) that requires \(\mathrm{E}(Y)\in C(X)\) and \(\mathrm{E}(Y)\perp M_{\alpha}\rho\). Clearly this constitutes a constraint on \(C(M_{\alpha})\), the space for the \(\alpha\) groups. Another interpretation is that estimation or testing of a contrast in the \(\alpha_{i}\)s is done using \(M_{\alpha}\), which is exactly the way it is done in a one-way ANOVA ignoring the \(\eta_{j}\)s.

Specifically, if we have a contrast \(\sum_{i=1}^{a}c_{i}\alpha_{i}\), then the corresponding vector \(M_{\alpha}\rho\) is

\[M_{\alpha}\rho=[t_{ijk}],\qquad\text{where }t_{ijk}=c_{i}/bN.\]

The estimated contrast is \(\rho^{\prime}M_{\alpha}Y=\sum_{i=1}^{a}c_{i}\bar{y}_{i}\).. having a variance of \(\sigma^{2}\rho^{\prime}M_{\alpha}\rho=\sigma^{2}\sum_{i=1}^{a}c_{i}^{2}/bN\) and a sum of squares for testing \(H_{0}:\sum_{i=1}^{a}c_{i}\alpha_{i}=0\) of

\[\left(\sum_{i=1}^{a}c_{i}\bar{y}_{i}..\right)^{2}\Big{/}\left(\sum_{i=1}^{a}c_ {i}^{2}/bN\right).\]

To get two orthogonal constraints on \(C(M_{\alpha})\), as in the one-way ANOVA, take \(\rho_{1}\) and \(\rho_{2}\) such that \(\rho_{1}^{\prime}X\beta\) and \(\rho_{2}^{\prime}X\beta\) are contrasts in the \(\alpha_{i}\)s and \(\rho_{1}^{\prime}M_{\alpha}\rho_{2}=0\). If \(\rho_{j}^{\prime}X\beta=\sum_{i=1}^{a}c_{ji}\alpha_{i}\), then, as shown for the one-way ANOVA, \(\rho_{1}^{\prime}M_{\alpha}\rho_{2}=0\) if and only if \(\sum_{i=1}^{a}c_{1i}c_{2i}=0\).

In the balanced two-way ANOVA without interaction, if \(N\) is greater than 1, we have a row structure to the model matrix with \(ab\) distinct rows. This allows estimation of pure error and lack of fit. The balanced two-way ANOVA with interaction retains the row structure and is equivalent to a large one-way ANOVA with \(ab\) groups.

[MISSING_PAGE_FAIL:222]

The columns \(X_{0},\ldots,X_{a+b}\) are exactly the same as for (7.1.1), the model without interaction. The key fact to notice is that

\[C(X)=C(X_{a+b+1},\ldots,X_{a+b+ab});\]

this will be shown rigorously later. We can write an equivalent model using just \(X_{a+b+1},\ldots,X_{a+b+ab}\), say

\[y_{ijk}=\mu_{ij}+e_{ijk}.\]

This is just a one-way ANOVA model with \(ab\) groups and is sometimes called the _cell means model_.

We can decompose \(C(X)\) into four orthogonal parts based on the identity

\[M=\frac{1}{n}J_{n}^{n}+M_{\alpha}+M_{\eta}+M_{\gamma},\]

where

\[M_{\gamma}\equiv M-\frac{1}{n}J_{n}^{n}-M_{\alpha}-M_{\eta}.\]

\(M_{\alpha}\) and \(M_{\eta}\) come from the no interaction model. Thus, as discussed earlier, \(M_{\alpha}\) and \(M_{\eta}\) each come from a one-way ANOVA model. Since \(M\) also comes from a one-way ANOVA model, we can actually find \(M_{\gamma}\). The interaction space is defined as \(C(M_{\gamma})\). The sum of squares for interaction is \(Y^{\prime}M_{\gamma}Y\). It is just the sum of squares left over after explaining as much as possible with \(\mu\), the \(\alpha_{i}\)s, and the \(\eta_{j}\)s. The degrees of freedom for the interaction are

\[r(M_{\gamma}) = r\bigg{(}M-\frac{1}{n}J_{n}^{n}-M_{\alpha}-M_{\eta}\bigg{)}\] \[= ab-1-(a-1)-(b-1)=(a-1)(b-1).\]

The algebraic formula for the interaction sum of squares can be found as follows:

\[SS(\gamma)\equiv Y^{\prime}M_{\gamma}Y=[M_{\gamma}Y]^{\prime}[M_{\gamma}Y], \tag{1}\]

where

\[M_{\gamma}Y=\bigg{(}M-\frac{1}{n}J_{n}^{n}-M_{\alpha}-M_{\eta}\bigg{)}Y=MY-\frac {1}{n}J_{n}^{n}Y-M_{\alpha}Y-M_{\eta}Y.\]

With \(M\) the projection operator for a one-way ANOVA, all of the terms on the right of the last equation have been characterized, so \[\left(M-\frac{1}{n}J_{n}^{n}-M_{\alpha}-M_{\eta}\right)Y=[t_{ijk}],\]

where

\[t_{ijk} = \bar{y}_{ij\cdot}-\bar{y}_{\cdot\cdot\cdot}-(\bar{y}_{i\cdot}-\bar {y}_{\cdot\cdot\cdot})-(\bar{y}_{\cdot j\cdot}-\bar{y}_{\cdot\cdot\cdot\cdot})\] \[= \bar{y}_{ij\cdot}-\bar{y}_{i\cdot}-\bar{y}_{\cdot j\cdot}+\bar{y} _{\cdot\cdot\cdot\cdot}.\]

It follows immediately from (1) that

\[\begin{split} SS(\gamma)&=\sum_{i=1}^{a}\sum_{j=1}^{b }\sum_{k=1}^{N}[\bar{y}_{ij\cdot}-\bar{y}_{i\cdot\cdot}-\bar{y}_{\cdot j\cdot }+\bar{y}_{\cdot\cdot\cdot}]^{2}\\ &=N\sum_{i=1}^{a}\sum_{j=1}^{b}[\bar{y}_{ij\cdot}-\bar{y}_{i\cdot \cdot}-\bar{y}_{\cdot j\cdot}+\bar{y}_{\cdot\cdot\cdot}]^{2}.\end{split}\]

The expected value of \(Y^{\prime}M_{\gamma}Y\) is \(\sigma^{2}(a-1)(b-1)+\beta^{\prime}X^{\prime}M_{\gamma}X\beta\). The second term is a quadratic form in the \(\gamma_{ij}\)s because \(\left(M-M_{\alpha}-M_{\eta}-[1/n]J_{n}^{n}\right)X_{r}=0\) for \(r=0\), \(1\),..., \(a+b\). The algebraic form of \(\beta^{\prime}X^{\prime}M_{\gamma}X\beta\) can be found by substituting \(\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}\) for \(y_{ijk}\) in \(SS(\gamma)\). Simplification gives

\[\beta^{\prime}X^{\prime}M_{\gamma}X\beta=N\sum_{ij}[\gamma_{ij}-\bar{\gamma}_{i \cdot}-\bar{\gamma}_{\cdot j\cdot}-\bar{\gamma}_{\cdot\cdot}]^{2}.\]

The expected values of \(Y^{\prime}M_{\alpha}Y\), \(Y^{\prime}M_{\eta}Y\), and \(Y^{\prime}(1/n)J_{n}^{n}Y\) are now different from those found for the no interaction model. As above, algebraic forms for the expected values can be computed by substituting for the \(y_{ijk}\)s in the algebraic forms for the sums of squares. For instance,

\[\text{E}(Y^{\prime}M_{\alpha}Y) = \sigma^{2}(a-1)+\beta^{\prime}X^{\prime}M_{\alpha}X\beta=\sigma^{ 2}(a-1)+bN\sum_{i=1}^{a}(\alpha_{i}+\bar{\gamma}_{i\cdot}-\bar{\alpha}_{\cdot \cdot}-\bar{\gamma}_{\cdot\cdot})^{2},\]

which depends on the \(\gamma_{ij}\)s and not just the \(\alpha_{i}\)s. This implies that the standard test is not a test that the \(\alpha_{i}\)s are all equal; it is a test that the \((\alpha_{i}+\bar{\gamma}_{i\cdot})\)s are all equal. In fact, since the column space associated with the \(\gamma_{ij}\)s spans the entire space, i.e., \(C(X_{a+b+1},\ldots,\)\(X_{a+b+ab})=C(X)\), all estimable functions of the parameters are functions of the \(\gamma_{ij}\)s. To see this, note that if \(\lambda^{\prime}\beta\) is not a function of the \(\gamma_{ij}\)s, but is estimable, then \(\rho^{\prime}X_{i}=0,i=a+b+1,\ldots,a+b+ab\), and hence \(\lambda^{\prime}=\rho^{\prime}X=0\); so \(\lambda^{\prime}\beta\) is identically zero.

If we impose the "usual" side conditions, \(\alpha_{\cdot}=\eta_{\cdot}=\gamma_{i\cdot}=\gamma_{\cdot j}=0\), we obtain, for example,

\[\text{E}(Y^{\prime}M_{\alpha}Y)=\sigma^{2}(a-1)+bN\sum_{i=1}^{a}\alpha_{i}^{2},\]which looks nice but serves no purpose other than to hide the fact that these new \(\alpha_{i}\) terms are averages over any interactions that exist.

As we did for a two-way ANOVA without interaction, we can put a single degree of freedom constraint on, say, \(C(M_{\alpha})\) by choosing a function \(\lambda^{\prime}\beta\) such that \(\lambda^{\prime}=\rho^{\prime}X\) and \(\rho^{\prime}M=\rho^{\prime}M_{\alpha}\). However, such a constraint no longer yields a contrast in the \(\alpha_{i}\)s. To examine \(\rho^{\prime}M_{\alpha}X\beta\), we examine the nature of \(M_{\alpha}X\beta\). Since \(M_{\alpha}Y\) is a vector whose rows are made up of terms like \((\tilde{y}_{i..}-\tilde{y}...)\), algebraic substitution gives \(M_{\alpha}X\beta\) as a vector whose rows are terms like \((\alpha_{i}+\tilde{y}_{i..}-\tilde{\alpha}.-\tilde{y}_{..})\). \(\lambda^{\prime}\beta=\rho^{\prime}M_{\alpha}X\beta\) will be a contrast in these terms or, equivalently, in the \((\alpha_{i}+\tilde{y}_{i..})\)s. Such contrasts are generally hard to interpret. A contrast in the terms \((\alpha_{i}+\tilde{y}_{i..})\) will be called a contrast in the \(\alpha\) space. Similarly, a contrast in the terms \((\eta_{j}+\tilde{y}_{j..})\) will be called a contrast in the \(\eta\) space.

There are two fundamental approaches to analyzing a two-way ANOVA with interaction. In both methods, the test for whether interaction adds to the two-way without interaction model is performed. If this is not significant, the interactions are tentatively assumed to be zero. If the effect of the interaction terms is significant, the easiest approach is to do the entire analysis as a one-way ANOVA. The alternative approach consists of trying to interpret the contrasts in \(C(M_{\alpha})\) and \(C(M_{\eta})\) and examining constraints in the interaction space.

#### Interaction Contrasts

We now consider how to define and test constraints on the interaction space. The hypothesis \(H_{0}:\lambda^{\prime}\beta=0\) puts a constraint on the interaction space if and only if \(\lambda^{\prime}=\rho^{\prime}X\) has the property \(\rho^{\prime}M=\rho^{\prime}\left(M-M_{\alpha}-M_{\eta}-[1/n]J_{n}^{n}\right)\). To find hypotheses that put constraints on the interaction space, it suffices to find \(\rho\perp C\left(M_{\alpha}+M_{\eta}+[1/n]J_{n}^{n}\right)\) or, alternately, \(\rho^{\prime}X_{i}=0\), \(i=0\), \(\ldots,a+b\).

The goal of the following discussion is to characterize vectors in the interaction space, i.e., to characterize the vectors \(M\rho\) that have the property \(\rho^{\prime}X_{i}=0\) for \(i=0\), \(\ldots,a+b\). A convenient way to do this is to characterize the vectors \(\rho\) that have two properties: (1) \(M\rho=\rho\), and (2) \(\rho^{\prime}X_{i}=0\) for \(i=0\), \(\ldots,a+b\). The second property ensures that \(M\rho\) is in the interaction space.

First we find a class of vectors that are contained in the interaction space. From this class of vectors we will get a class of orthogonal bases for the interaction space. The class of vectors and the class of orthogonal bases are found by combining a contrast in the \(\alpha\) space with a contrast in the \(\eta\) space. This method leads naturally to the standard technique of examining interactions. Finally, a second class of vectors contained in the interaction space will be found. This class contains the first class as a special case. The second class is closed under linear combinations, so the second class is a vector space that contains a basis for the interaction space but which is also contained in the interaction space. It follows that the second class is precisely the interaction space.

At this point a problem arises. It is very convenient to write down ANOVA models as was done in Example 7.2.1, with indices to the right in \(y_{ijk}\) changing fastest. It is easy to see what the model looks like and that it can be written in a similar manner for any choices of \(a\), \(b\), and \(N\). In the example it would be easy to find a vector that is in the interaction space, and it would be easy to see that the technique could be extended to cover any two-way ANOVA problem. Although it is easy to see how to write down models as in the example, it is awkward to develop a notation for it. It is also less than satisfying to have a proof that depends on the way in which the model is written down. Consequently, the material on finding a vector in the interaction space will be presented in three stages: an application to Example 7.2.1, a comment on how that argument can be generalized, and finally a rigorous presentation.

_Example 7.2.2_  In the model of Example 7.2.1, let \(d^{\prime}=(d_{1},\,d_{2},\,d_{3})=(1,\,2,\,-3)\) and \(c^{\prime}=(c_{1},\,c_{2})=(1,\,-1)\). The \(d_{i}\)s determine a contrast in the \(\alpha\) space and the \(c_{j}\)s determine a contrast in the \(\eta\) space. Consider

\[[d^{\prime}\otimes c^{\prime}] =[c_{1}d_{1},\,c_{2}d_{1},\,c_{1}d_{2},\,c_{2}d_{2},\,c_{1}d_{3},\,c_{2}d_{3}]\] \[=[1,\,-1,\,2,\,-2,\,-3,\,3];\]

and since \(N=4\), let

\[\rho^{\prime}\] \[=\frac{1}{4}[d^{\prime}\otimes c^{\prime}]\otimes J_{1}^{4}=\frac {1}{4}[c_{1}d_{1}J_{1}^{4},c_{2}d_{1}J_{1}^{4},\ldots,c_{2}d_{3}J_{1}^{4}]\] \[=\frac{1}{4}[1,\,1,\,1,\,-1,\,-1,\,-1,\,-1,\,2,\,2,\,2,\,2,\,-2,\, -2,\,-2,\,-3,\,-3,\,-3,\,3,\,3,\,3,3].\]

It is easily seen that \(\rho\) is orthogonal to the first six columns of \(X\); thus \(M\rho\) is in the interaction space. However, it is also easily seen that \(\rho\in C(X)\), so \(M\rho=\rho\). The vector \(\rho\) is itself a vector in the interaction space.

Extending the argument to an arbitrary two-way ANOVA written in standard form, let \(d^{\prime}=(d_{1},\,\ldots,\,d_{a})\) with \(\sum_{i=1}^{a}d_{i}=0\) and \(c^{\prime}=(c_{1},\,\ldots,\,c_{b})\) with \(\sum_{j=1}^{b}c_{j}=0\). The \(d_{i}\)s can be thought of as determining a contrast in the \(\alpha\) space and the \(c_{j}\)s as determining a contrast in the \(\eta\) space. Let \(\rho^{\prime}=(1/N)[d^{\prime}\otimes c^{\prime}]\otimes J_{1}^{N}\), i.e.,

\[\rho^{\prime}=\frac{1}{N}(c_{1}d_{1}J_{1}^{N},\,c_{2}d_{1}J_{1}^{N},\ldots,\,c _{b}d_{1}J_{1}^{N},\,c_{1}d_{2}J_{1}^{N},\ldots,\,c_{b}d_{a}J_{1}^{N}).\]

It is clear that \(\rho^{\prime}X_{i}=0\) for \(i=0\),..., \(a+b\) and \(\rho\in C(X_{a+b+1},\,\ldots,\,X_{a+b+ab})\) so \(\rho^{\prime}M=\rho^{\prime}\) and \(\rho^{\prime}X\beta=0\) puts a constraint on the correct space.

To make the argument completely rigorous, with \(d\) and \(c\) defined as above, take

\[\rho=[\rho_{ijk}],\quad\text{ where }\rho_{ijk}=\frac{1}{N}(d_{i}c_{j}).\]Using the characterizations of \(X_{0}\),..., \(X_{a+b}\) from Section 1 we get

\[\rho^{\prime}X_{0}=\sum_{i}\sum_{j}\sum_{k}\frac{1}{N}(d_{i}c_{j})=\sum_{i}d_{i} \sum_{j}c_{j}=0;\]

for \(r=1,\ldots,a\), we get

\[\rho^{\prime}X_{r}=\sum_{i}\sum_{j}\sum_{k}\frac{1}{N}(d_{i}c_{j})\delta_{ir}=d _{r}\sum_{j}c_{j}=0;\]

and for \(s=1,\ldots,b\), we get

\[\rho^{\prime}X_{a+s}=\sum_{i}\sum_{j}\sum_{k}\frac{1}{N}(d_{i}c_{j})\delta_{js} =c_{s}\sum_{i}d_{i}=0.\]

This shows that \(M\rho\) is in the interaction space.

To show that \(M\rho=\rho\), we show that \(\rho\in C(X)\). We need to characterize \(C(X)\). The columns of \(X\) that correspond to the \(\gamma_{ij}\)s are the vectors \(X_{a+b+1}\),..., \(X_{a+b+ab}\). Reindex these as \(X_{(1,1)}\), \(X_{(1,2)}\),..., \(X_{(1,b)}\),..., \(X_{(2,1)}\),..., \(X_{(a,b)}\). Thus \(X_{(i,j)}\) is the column of \(X\) corresponding to \(\gamma_{ij}\). We can then write

\[X_{(r,s)}=[t_{ijk}],\ \ \ \ \ \mbox{ where }t_{ijk}=\delta_{(i,j)(r,s)}\]

and \(\delta_{(i,j)(r,s)}=1\) if \((i,j)=(r,s)\), and 0 otherwise. It is easily seen that

\[X_{0}=\sum_{i=1}^{a}\sum_{j=1}^{b}X_{(i,j)},\]

\[X_{r}=\sum_{j=1}^{b}X_{(r,j)},\ \ \ r=1,\ldots,a,\]

\[X_{a+s}=\sum_{i=1}^{a}X_{(i,s)},\ \ \ s=1,\ldots,b.\]

This shows that \(C(X)=C(X_{(1,1)}\), \(X_{(1,2)}\),..., \(X_{(a,b)})\). (In the past, we have merely claimed this result.) It is also easily seen that

\[\rho=\sum_{i=1}^{a}\sum_{j=1}^{b}\frac{d_{i}c_{j}}{N}X_{(i,j)},\]

so that \(\rho\in C(X)\).

We have found a class of vectors contained in the interaction space. If we find \((a-1)(b-1)\) such vectors, say \(\rho_{r}\), \(r=1,\ldots,(a-1)(b-1)\), where

\[\rho_{r}^{\prime}\left(M-M_{\alpha}-M_{\eta}-[1/n]J_{n}^{\eta}\right)\rho_{s}=0\]

for any \(r\neq s\), then we will have an orthogonal basis for \(C(M-M_{\alpha}-M_{\eta}-[1/n]J_{n}^{\eta})\). Consider now another pair of contrasts for \(\alpha\) and \(\eta\), say \(d^{*}=(d_{1}^{*},\ldots,d_{a}^{*})^{\prime}\) and \(c^{*}=(c_{1}^{*},\ldots,c_{b}^{*})^{\prime}\), where one, say \(c^{*}=(c_{1}^{*},\ldots,c_{b}^{*})^{\prime}\), is orthogonal to the corresponding contrast in the other pair. We can write

\[\rho_{*}=[\rho_{ijk}^{*}],\qquad\mbox{where }\rho_{ijk}^{*}=d_{i}^{*}c_{j}^{*}/N,\]

and we know that \(\sum_{i=1}^{a}d_{i}^{*}=\sum_{j=1}^{b}c_{j}^{*}=\sum_{j=1}^{b}c_{j}c_{j}^{*}=0\). With our choices of \(\rho\) and \(\rho_{*}\),

\[\rho^{{}^{\prime}}\left(M-M_{\alpha}-M_{\eta}-[1/n]J_{n}^{\eta}\right)\rho_{*} = \rho^{{}^{\prime}}\rho_{*}\] \[= N^{-2}\sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{N}d_{i}d_{i}^{*}c_ {j}c_{j}^{*}\] \[= N^{-1}\sum_{i=1}^{a}d_{i}d_{i}^{*}\sum_{j=1}^{b}c_{j}c_{j}^{*}\] \[= 0.\]

Since there are \((a-1)\) orthogonal ways of choosing \(d^{\prime}=(d_{1},\ldots,d_{a})\) and \((b-1)\) orthogonal ways of choosing \(c^{\prime}=(c_{1},\ldots,c_{b})\), there are \((a-1)(b-1)\) orthogonal vectors \(\rho\) that can be chosen in this fashion. This provides the desired orthogonal breakdown of the interaction space.

To actually compute the estimates of these parametric functions, recall that \(M\) is the perpendicular projection operator for a one-way ANOVA. With \(\rho\) chosen as above,

\[\rho^{{}^{\prime}}X\beta=\sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{N}\frac{d_{i} c_{j}}{N}\gamma_{ij}=\sum_{i=1}^{a}\sum_{j=1}^{b}d_{i}c_{j}\gamma_{ij}\,.\]

Its estimate reduces to

\[\rho^{{}^{\prime}}MY=\sum_{i=1}^{a}\sum_{j=1}^{b}d_{i}c_{j}\bar{y}_{ij}.\]

and its variance to

\[\sigma^{2}\rho^{{}^{\prime}}M\rho=\sigma^{2}\rho^{{}^{\prime}}\rho=\sigma^{2} \sum_{i=1}^{a}\sum_{j=1}^{b}d_{i}^{2}c_{j}^{2}/N.\]A handy method for computing these is to write out the following two-way table:

\[\begin{array}{c|ccccc}&c_{1}&c_{2}&\cdots&c_{b}&h_{i}\\ \hline d_{1}&\bar{y}_{11}.&\bar{y}_{12}.&\cdots&\bar{y}_{1b}.&h_{1}\\ d_{2}&\bar{y}_{21}.&\bar{y}_{22}.&\cdots&\bar{y}_{2b}.&h_{2}\\ \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\ d_{a}&\bar{y}_{a1}.&\bar{y}_{a2}.&\cdots&\bar{y}_{ab}.&h_{a}\\ \hline g_{j}&g_{1}&g_{2}&\cdots&g_{b}&\\ \end{array}\]

where \(h_{i}=\sum_{j=1}^{b}c_{j}\bar{y}_{ij}\). and \(g_{j}=\sum_{i=1}^{a}d_{i}\bar{y}_{ij}\). We can then write

\[\rho^{\prime}MY=\sum_{j=1}^{b}c_{j}g_{j}=\sum_{i=1}^{a}d_{i}h_{i}.\]

Unfortunately, not all contrasts in the interaction space can be defined as illustrated here. The \((a-1)(b-1)\) orthogonal vectors that we have discussed finding form a basis for the interaction space, so any linear combination of these vectors is also in the interaction space. However, not all of these linear combinations can be written with the method based on two contrasts.

Let \(Q=[q_{ij}]\) be any \(a\times b\) matrix such that \(J_{a}^{\prime}Q=0\) and \(QJ_{b}=0\) (i.e., \(q_{i}.=0=q_{\cdot j}\)). If the model is written down in the usual manner, the vectors in the interaction space are the vectors of the form \(\rho=(1/N)\mbox{Vec}(Q^{\prime})\otimes J_{N}\). In general, we write the vector \(\rho\) with triple subscript notation as

\[\rho=[\rho_{ijk}],\ \ \ \ \ \mbox{where }\rho_{ijk}=q_{ij}/N\ \ \ \mbox{with }q_{i}.=q_{\cdot j}=0. \tag{2}\]

First, note that linear combinations of vectors with this structure retain the structure; thus vectors of this structure form a vector space. Vectors \(\rho\) with this structure are in \(C(X)\), and it is easily seen that \(\rho^{\prime}X_{i}\)=0 for \(i\)=0, 1,..., \(a+b\). Thus, a vector with this structure is contained in the interaction space. Note also that the first method of finding vectors in the interaction space using a pair of contrasts yields a vector of the structure that we are currently considering, so the vector space alluded to above is both contained in the interaction space and contains a basis for the interaction space. It follows that the interaction space is precisely the set of all vectors with the form (2).

#### Exercise 7.1

Prove the claims of the previous paragraph. In particular, show that linear combinations of the vectors presented retain their structure, that the vectors are orthogonal to the columns of \(X\) corresponding to the grand mean and the group effects, and that the vectors based on contrasts have the same structure as the vector given above.

For estimation and tests of single-degree-of-freedom hypotheses in the interaction space, it is easily seen, with \(\rho\) taken as above, that 

\begin{table}
\begin{tabular}{l c c} \hline  & Matrix Notation \\ \hline Source & \(df\) & \(SS\) \\ \hline Grand Mean & 1 & \(Y^{\prime}\left(\frac{1}{n}J_{n}^{n}\right)Y\) \\ Groups(\(\alpha\)) & \(a-1\) & \(Y^{\prime}M_{\alpha}Y\) \\ Groups(\(\eta\)) & \(b-1\) & \(Y^{\prime}M_{\eta}Y\) \\ Interaction(\(\gamma\)) & \((a-1)(b-1)\) & \(Y^{\prime}\left(M-M_{\alpha}-M_{\eta}-\frac{1}{n}J_{n}^{n}\right)Y\) \\ Error & \(n-ab\) & \(Y^{\prime}(I-M)Y\) \\ \hline Total & \(n=abN\) & \(Y^{\prime}Y\) \\ Source & \(SS\) & E(\(MS\)) \\ \hline Grand Mean & \(SSGM\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}\left(\frac{1}{n}J_{n}^{n}\right)X\beta\) \\ Groups(\(\alpha\)) & \(SS(\alpha)\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}M_{\alpha}X\beta/(a-1)\) \\ Groups(\(\eta\)) & \(SS(\eta)\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}M_{\eta}X\beta/(b-1)\) \\ Interaction(\(\gamma\)) & \(SS(\gamma)\) & \(\sigma^{2}+\beta^{\prime}X^{\prime}M_{\gamma}X\beta/(a-1)(b-1)\) \\ Error & \(SSE\) & \(\sigma^{2}\) \\ \hline Total & \(SSTot\) \\ \hline \multicolumn{3}{c}{Algebraic Notation} \\ \hline Source & \(df\) & \(SS\) \\ \hline Grand Mean & \(dfGM\) & \(n^{-1}y_{,..}^{2}=n\tilde{y}_{,..}^{2}\) \\ Groups(\(\alpha\)) & \(df(\alpha)\) & \(bN\sum_{i=1}^{a}\left(\tilde{y}_{i..}-\tilde{y}_{,..}\right)^{2}\) \\ Groups(\(\eta\)) & \(df(\eta)\) & \(aN\sum_{j=1}^{b}\left(\tilde{y}_{,j..}-\tilde{y}_{,..}\right)^{2}\) \\ Interaction(\(\gamma\)) & \(df(\gamma)\) & \(N\sum_{ij}\left(\tilde{y}_{ij..}-\tilde{y}_{,i..}-\tilde{y}_{,j..}+\tilde{y}_ {,..}\right)^{2}\) \\ Error & \(dFE\) & \(\sum_{ijk}\left(y_{ijk}-\tilde{y}_{ij.j}\right)^{2}\) \\ \hline Total & \(dfTot\) & \(\sum_{ijk}y_{ijk}^{2}\) \\ Source & \(MS\) & E(\(MS\)) \\ \hline Grand Mean & \(SSGM\) & \(\sigma^{2}+abN(\mu+\tilde{\alpha}_{,}+\tilde{\eta}_{,}+\tilde{\gamma}_{,})^{2}\) \\ Groups(\(\alpha\)) & \(SS(\alpha)/(a-1)\) & \(\sigma^{2}+\frac{bN}{a-1}\sum_{i}\left(\alpha_{i}+\tilde{\gamma}_{i..}-\tilde {\alpha}_{,}-\tilde{\gamma}_{,}\right)^{2}\) \\ Groups(\(\eta\)) & \(SS(\eta)/(b-1)\) & \(\sigma^{2}+\frac{bN}{a-1}\sum_{j}\left(\eta_{j}+\tilde{\gamma}_{,j}-\tilde{ \eta}_{,}-\tilde{\gamma}_{,}\right)^{2}\) \\ Interaction(\(\gamma\)) & \(SS(\gamma)/df(\gamma)\) & \(\sigma^{2}+\frac{N}{df(\gamma)}\sum_{ij}\left(\gamma_{ij}-\tilde{\gamma}_{i..}- \tilde{\gamma}_{,j}+\tilde{\gamma}_{,}\right)^{2}\) \\ Error & \(SSE/(n-ab)\) & \(\sigma^{2}\) \\ \hline \end{tabular}
\end{table}
Table 7: Balanced two-way analysis of variance table with interaction\[\rho^{\prime}X\beta=\sum_{i=1}^{a}\sum_{j=1}^{b}q_{ij}\gamma_{ij}\;,\]

\[\rho^{\prime}MY=\sum_{i=1}^{a}\sum_{j=1}^{b}q_{ij}\tilde{y}_{ij}\;,\]

\[\mbox{Var}(\rho^{\prime}MY)=\sigma^{2}\sum_{i=1}^{a}\sum_{j=1}^{b}q_{ij}^{2}/N\;.\]

Table 7.2 gives the ANOVA table for the balanced two-way ANOVA with interaction. Note that if \(N=1\), there will be no pure error term available. In that case, it is often _assumed_ that the interactions add nothing to the model, so that the mean square for interactions can be used as an estimate of error for the two-way ANOVA without interaction. See Example 12.2.4 for a graphical procedure that addresses this problem.

**Exercise 7.2**  Does the statement "the interactions add nothing to the model" mean that \(\gamma_{11}=\gamma_{12}=\cdots=\gamma_{ab}\)? If it does, justify the statement. If it does not, what does the statement mean?

Two final comments on exploratory work with interactions. If the \((a-1)(b-1)\) degree-of-freedom \(F\) test for interactions is not significant, then neither Scheffe's method nor the LSD method will allow us to claim significance for any contrast in the interactions. Bonferroni's method may give significance, but it is unlikely. Nevertheless, if our goal is to explore the data, there may be _suggestions_ of possible interactions. For example, if you work with interactions long enough, you begin to think that some interaction contrasts have reasonable interpretations. If such a contrast exists that accounts for the bulk of the interaction sum of squares, and if the corresponding \(F\) test approaches significance, then it would be unwise to ignore this possible source of interaction. (As a word of warning though, recall that there always exists an interaction contrast, usually uninterpretable, that accounts for the entire sum of squares for interaction.) Second, in exploratory work it is very useful to plot the cell means. For example, one can plot the points \((i,\,\tilde{y}_{ij}.)\) for each value of \(j\), connecting the points to give \(b\) different curves, one for each \(j\). If the \(\alpha\) groups correspond to levels of some quantitative factor \(x_{i}\), plot the points \((x_{i},\,\tilde{y}_{ij}.)\) for each value of \(j\). If there are no interactions, the plots for different values of \(j\) should be (approximately) parallel. (If no interactions are present, the plots estimate plots of the points \((i,\,\mu+\alpha_{i}+\eta_{j})\). These plots are parallel for all \(j\).) Deviations from a parallel set of plots can suggest possible sources of interaction. The data are suggesting possible interaction contrasts, so if valid tests are desired, use Scheffe's method. Finally, it is equally appropriate to plot the points \((j,\,\tilde{y}_{ij}.)\) for all \(i\) or a corresponding set of points using quantitative levels associated with the \(\eta\) groups.

### Polynomial Regression and the Balanced

Two-Way ANOVA

Consider first the balanced two-way ANOVA without interaction. Suppose that the \(i\)th level of the \(\alpha\) groups corresponds to some number \(w_{i}\) and that the \(j\)th level of the \(\eta\) groups corresponds to some number \(z_{j}\). We can write vectors taking powers of \(w_{i}\) and \(z_{j}\). For \(r=1,\ldots,a-1\) and \(s=1,\ldots,b-1\), write

\[W^{r}=[t_{ijk}],\qquad\text{where }t_{ijk}=w_{i}^{r}\,,\]

\[Z^{s}=[t_{ijk}],\qquad\text{where }t_{ijk}=z_{j}^{s}\,.\]

Note that \(W^{0}=Z^{0}=J\).

Example 7.3.1: Consider the model \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\), \(i=1,2,3\), \(j=1,2\), \(k=1,2\). Suppose that the \(\alpha\) groups are 1, 2, and 3 pounds of fertilizer and that the \(\eta\) groups are 5 and 7 pounds of manure. Then, if we write \(Y=(y_{111},y_{112},y_{121},y_{122},y_{211},\ldots,y_{322})^{\prime}\), we have

\[W^{1}=(1,1,1,1,2,2,2,2,3,3,3,3)^{\prime},\]

\[W^{2}=(1,1,1,1,4,4,4,4,9,9,9,9)^{\prime},\]

\[Z^{1}=(5,5,7,7,5,5,7,7,5,5,7,7)^{\prime}.\]

From the discussion of Section 6.8 on polynomial regression and one-way ANOVA, we have

\[C(J,W^{1},\ldots,W^{a-1})=C(X_{0},X_{1},\ldots,X_{a}),\] \[C(J,Z^{1},\ldots,Z^{b-1})=C(X_{0},X_{a+1},\ldots,X_{a+b}),\]

and thus

\[C(J,W^{1},\ldots,W^{a-1},Z^{1},\ldots,Z^{b-1})=C(X_{0},X_{1},\ldots,X_{a+b}).\]

Fitting the two-way ANOVA is the same as fitting a joint polynomial in \(w_{i}\) and \(z_{j}\). Writing the models out algebraically, the model

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk},\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), \(k=1,\ldots,N\), is equivalent to

\[y_{ijk}=\beta_{0,0}+\beta_{1,0}w_{i}+\cdots+\beta_{a-1,0}w_{i}^{a-1}+\beta_{0, 1}z_{j}+\cdots+\beta_{0,b-1}z_{j}^{b-1}+e_{ijk},\]\(i=1,\,\ldots,\,a,\,\,j=1,\,\ldots,\,b,\,k=1,\,\ldots,\,N\). The correspondence between contrasts and orthogonal polynomials remains valid. In particular, if the \(w_{i}\)s or \(z_{j}\)s are equally spaced, the contrasts in Table 6.2 can be used.

We now show that the interaction model

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+e_{ijk}\]

is equivalent to the model

\[y_{ijk}=\sum_{i=0}^{a-1}\sum_{j=0}^{b-1}\beta_{rs}w_{i}^{r}z_{j}^{s}+e_{ijk}.\]

Example 7.3.1 Continued.The model \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+e_{ijk}\) is equivalent to \(y_{ijk}=\beta_{00}+\beta_{10}w_{i}+\beta_{20}w_{i}^{2}+\beta_{01}z_{j}+\beta_{ 11}w_{i}z_{j}+\beta_{21}w_{i}^{2}z_{j}+e_{ijk}\), where \(w_{1}=1\), \(w_{2}=2\), \(w_{3}=3\), \(z_{1}=5\), \(z_{2}=7\).

The model matrix for this polynomial model can be written

\[S= [J_{n}^{1},\,W^{1},\,W^{2},\ldots,\,W^{a-1},\,Z^{1},\ldots,\,Z^{b -1},\] \[W^{1}Z^{1},\ldots,\,W^{1}Z^{b-1},\,W^{2}Z^{1},\ldots,\,W^{a-1}Z^{ b-1}],\]

where for any two vectors in \(\mathbf{R}^{n}\), say \(U=(u_{1},\,\ldots,\,u_{n})^{\prime}\) and \(V=(v_{1},\,\ldots,\,v_{n})^{\prime}\), we define \(VU\) to be the vector \((v_{1}u_{1},\,v_{2}u_{2},\,\ldots,\,v_{n}u_{n})^{\prime}\).

To establish the equivalence of the models, it is enough to notice that the row structure of \(X=[X_{0},\,X_{1},\,\ldots,\,X_{a+b+ab}]\) is the same as the row structure of \(S\) and that \(r(X)=ab=r(S)\). \(C(X)\) is the column space of the one-way ANOVA model that has a separate effect for each distinct set of rows in \(S\). From our discussion of pure error and lack of fit in Section 6.7, \(C(S)\subset C(X)\). Since \(r(S)=r(X)\), we have \(C(S)=C(X)\); thus the models are equivalent.

We would like to characterize the test in the interaction space that is determined by, say, the quadratic contrast in the \(\alpha\) space and the cubic contrast in the \(\eta\) space. We want to show that it is the test for \(W^{2}Z^{3}\), i.e., that it is the test for \(H_{0}:\beta_{23}=0\) in the model \(y_{ijk}=\beta_{00}+\beta_{10}w_{i}+\beta_{20}w_{i}^{2}+\beta_{01}z_{j}+\beta_{ 02}z_{j}^{2}+\beta_{03}z_{j}^{3}+\beta_{11}w_{i}z_{j}+\beta_{12}w_{i}z_{j}^{2} +\beta_{13}w_{i}z_{j}^{3}+\beta_{21}w_{i}^{2}z_{j}+\beta_{22}w_{i}^{2}z_{j}^{ 2}+\beta_{23}w_{i}^{2}z_{j}^{3}+e_{ijk}\). In general, we want to be able to identify the columns \(W^{r}Z^{s}\), \(r\geq 1\), \(s\geq 1\), with vectors in the interaction space. Specifically, we would like to show that the test of \(W^{r}Z^{s}\) adding to the model based on \(C([W^{i}Z^{j}:i=0,\,\ldots,\,r,\,j=0,\,\ldots,\,s])\) is precisely the test of the vector in the interaction space defined by the \(r\)th degree polynomial contrast in the \(\alpha\) space and the \(s\)th degree polynomial contrast in the \(\eta\) space. Note that the test of the \(r\)th degree polynomial contrast in the \(\alpha\) space is a test of whether the column \(W^{r}\) adds to the model based on \(C(J,\,W^{1},\,\ldots,\,W^{r})\), and that the test of the \(s\)th degree polynomial contrast in the \(\eta\) space is a test of whether the column \(Z^{s}\) adds to the model based on \(C(J,\,Z^{1},\ldots,\,Z^{s})\).

It is important to remember that the test for \(W^{r}Z^{s}\) adding to the model is not a test for \(W^{r}Z^{s}\) adding to the full model. It is a test for \(W^{r}Z^{s}\) adding to the model spanned by the vectors \(W^{i}Z^{j}\), \(i=0,\ldots,r\), \(j=0,\ldots,s\), where \(W^{0}=Z^{0}=J_{n}^{1}\). As discussed above, the test of the vector in the interaction space corresponding to the quadratic contrast in the \(\alpha\) space and the cubic contrast in the \(\eta\) space should be the test of whether \(W^{2}Z^{3}\) adds to a model with \(C(J_{n}^{1},\,W^{1},\,W^{2},\,Z^{1},\,Z^{2},\,Z^{3},\,W^{1}Z^{1},\,W^{1}Z^{2}, \,W^{1}Z^{3},\,W^{2}Z^{1},\,W^{2}Z^{2},\,W^{2}Z^{3})\). Intuitively, this is reasonable because the quadratic and cubic contrasts are being fitted after all terms of smaller order.

As observed earlier, if \(\lambda^{\prime}=\rho^{\prime}X\), then the constraint imposed by \(\lambda^{\prime}\beta=0\) is \(M\rho\) and the test of \(\lambda^{\prime}\beta=0\) is the test for dropping \(M\rho\) from the model. Using the Gram-Schmidt algorithm, find \(R_{0,0}\), \(R_{1,0}\),..., \(R_{a-1,0}\), \(R_{0,1}\),..., \(R_{0,b-1}\), \(R_{1,1}\),..., \(R_{a-1,b-1}\) by orthonormalizing, in order, the columns of \(S\). Recall that the polynomial contrasts in the \(\alpha_{i}\)s correspond to the vectors \(R_{i,0}\), \(i=1,\ldots,\,a-1\), and that \(C(R_{1,0},\ldots,\,R_{a-1,0})=C(M_{\alpha})\). Similarly, the polynomial contrasts in the \(\eta_{j}\)s correspond to the vectors \(R_{0,j}\)\(j=1,\ldots,\,b-1\) and \(C(R_{0,1},\ldots,\,R_{0,a-1})=C(M_{\eta})\). If the test of \(\lambda^{\prime}\beta=0\) is to test whether, say, \(Z^{s}\) adds to the model after fitting \(Z^{j}\), \(j=1,\ldots,\,s-1\), we must have \(C(M\rho)=C(R_{0,s})\). Similar results hold for the vectors \(W^{r}\).

First, we need to examine the relationship between vectors in \(C(M_{\alpha})\) and \(C(M_{\eta})\) with vectors in the interaction space. A contrast in the \(\alpha\) space is defined by, say, (\(d_{1},\ldots,d_{a}\)), and a contrast in the \(\eta\) space by, say, (\(c_{1},\ldots,c_{b}\)). From Chapter 4, if we define

\[\rho_{1}=[t_{ijk}],\qquad\mbox{where}\ t_{ijk}=d_{i}/Nb\]

and

\[\rho_{2}=[t_{ijk}],\qquad\mbox{where}\ t_{ijk}=c_{j}/Na,\]

then \(\rho_{1}\in C(M_{\alpha})\), \(\rho_{2}\in C(M_{\eta})\), \(\rho_{1}X\beta=\sum_{i=1}^{a}d_{i}(\alpha_{i}+\tilde{\gamma}_{i\cdot})\), and \(\rho_{2}X\beta=\sum_{j=1}^{b}c_{j}\) (\(\eta_{j}+\tilde{\gamma}_{\cdot j}\)). The vector \(\rho_{1}\rho_{2}\) is

\[\rho_{1}\rho_{2}=[t_{ijk}]\qquad\mbox{where}\ t_{ijk}=N^{-2}(ab)^{-1}d_{i}c_{ j}.\]

This is proportional to a vector in the interaction space corresponding to (\(d_{1},\ldots,\,d_{a}\)) and (\(c_{1},\ldots,c_{b}\)).

From this argument, it follows that since \(R_{r,0}\) is the vector (constraint) in \(C(M_{\alpha})\) for testing the \(r\)th degree polynomial contrast and \(R_{0,s}\) is the vector in \(C(M_{\eta})\) for testing the \(s\)th degree polynomial contrast, then \(R_{r,0}R_{0,s}\) is a vector in the interaction space. Since the polynomial contrasts are defined to be orthogonal, and since \(R_{r,0}\) and \(R_{0,s}\) are defined by polynomial contrasts, our discussion in the previous section about orthogonal vectors in the interaction space implies that the set \(\{R_{r,0}R_{0,s}|r=1,\ldots,a-1,s=1,\ldots,b-1\}\) is an orthogonal basis for the interaction space. Moreover, with \(R_{r,0}\) and \(R_{0,s}\) orthonormal,

\[[R_{r,0}R_{0,s}]^{\prime}[R_{r,0}R_{0,s}]=1/abN.\]

We now check to see that \((abN)Y^{\prime}[R_{r,0}R_{0,s}][R_{r,0}R_{0,s}]^{\prime}Y/MSE\) provides a test of the correct thing, i.e., that \(W^{r}Z^{s}\) adds to a model containing all lower order terms. Since, by Gram-Schmidt, for some \(a_{i}\)s and \(b_{j}\)s we have \(R_{r,0}=a_{0}W^{r}+a_{1}W^{r-1}+\cdots+a_{r-1}W^{1}+a_{r}J_{n}^{1}\) and \(R_{0,s}=b_{0}Z^{s}+b_{1}Z^{s-1}+\cdots+b_{s-1}Z^{1}+b_{s}J_{n}^{1}\), we also have

\[R_{r,0}R_{0,s}=a_{0}b_{0}W^{r}Z^{s}+\sum_{j=1}^{s}b_{j}Z^{s-j}W^{r}+\sum_{i=1}^ {r}a_{i}W^{r-i}Z^{s}+\sum_{j=1}^{s}\sum_{i=1}^{r}a_{i}b_{j}Z^{s-j}W^{r-i}.\]

Letting \(R_{1,0}=R_{0,1}=J\), it follows immediately that

\[C(R_{i,0}R_{0,j}:i=0,\ldots,r,j=0,\ldots,s)\subset C(W^{i}Z^{j}:i=0,\ldots,r,j= 0,\ldots,s).\]

However, the vectors listed in each of the sets are linearly independent and the number of vectors in each set is the same, so the ranks of the column spaces are the same and

\[C(R_{i,0}R_{0,j}:i=0,\ldots,r,j=0,\ldots,s)=C(W^{i}Z^{j}:i=0,\ldots,r,j=0, \ldots,s).\]

The vectors \(R_{i,0}R_{0,j}\) are orthogonal, so \(abN[R_{r,0}R_{0,s}][R_{r,0}R_{0,s}]^{\prime}\) is the projection operator for testing if \(W^{r}Z^{s}\) adds to the model after fitting all terms of lower order. Since \(R_{r,0}R_{0,s}\) was found as the vector in the interaction space corresponding to the \(r\)th orthogonal polynomial contrast in the \(\alpha_{i}\)s and the \(s\)th orthogonal polynomial contrast in the \(\eta_{j}\)s, the technique for testing if \(W^{r}Z^{s}\) adds to the appropriate model is a straightforward test of an interaction contrast.

### Two-Way ANOVA with Proportional Numbers

Consider the model

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk},\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), \(k=1,\ldots,N_{ij}\). We say that the model has _proportional numbers_ if, for \(i,i^{\prime}=1,\ldots,a\) and \(j\), \(j^{\prime}=1,\ldots,b\),

\[N_{ij}/N_{ij^{\prime}}=N_{i^{\prime}j}/N_{i^{\prime}j^{\prime}}.\]The special case of \(N_{ij}=N\) for all \(i\) and \(j\) is the balanced two-way ANOVA.

The analysis with proportional numbers is in the same spirit as the analysis with balance presented in Section 1. After fitting the mean, \(\mu\), the column space for the \(\alpha\) groups and the column space for the \(\eta\) groups are orthogonal. Before showing this we need a result on the \(N_{ij}\)s.

**Lemma 7.4.1**: _The \(N_{ij}s\) are proportional if and only if for any \(r=1,\ldots,a\), and \(s=1,\ldots,b\),_

\[N_{rs}=N_{r}.N_{\cdot s}/N_{\cdot\cdot\cdot}\,. \tag{2}\]

_Proof_  If the numbers are proportional,

\[N_{ij}N_{rs}=N_{rj}N_{is}\,.\]

Summing over \(i\) and \(j\) yields

\[N_{rs}N_{\cdot\cdot}=N_{rs}\sum_{i=1}^{a}\sum_{j=1}^{b}N_{ij}=\sum_{i=1}^{a} \sum_{j=1}^{b}N_{rj}N_{is}=N_{r}.N_{\cdot s}.\]

Dividing by \(N_{\cdot\cdot}\) gives the result. If (2) holds, substitution establishes (1). \(\square\)

We now show that the \(\alpha\) and \(\eta\) spaces are orthogonal after correcting for \(\mu\) if and only if the model has proportional numbers. As in Section 1, we can write the model matrix as \(X=[X_{0},\,X_{1},\ldots,\,X_{a+b}]\), where \(X_{0}=J_{n}^{1}\),

\[\begin{array}{ccc}X_{r}=[t_{ijk}],&t_{ijk}=\delta_{ir},&r=1,\ldots,a;\\ X_{a+s}=[u_{ijk}],&u_{ijk}=\delta_{js},&s=1,\ldots,b.\end{array}\]

Orthogonalizing with respect to \(J_{n}^{1}\) gives

\[\begin{array}{ccc}Z_{r}=X_{r}-\frac{N_{r\cdot}}{N_{\cdot\cdot}}J,&r=1,\ldots, a;\\ Z_{a+s}=X_{a+s}-\frac{N_{\cdot s}}{N_{\cdot\cdot}}J,&s=1,\ldots,b.\end{array}\]

It is easily seen that for \(r=1,\ldots,a,s=1,\ldots,b\),

\[Z_{a+s}^{\prime}Z_{r}=N_{rs}-N_{\cdot s}\frac{N_{r\cdot}}{N_{\cdot\cdot}}-N_{r \cdot}\frac{N_{\cdot s}}{N_{\cdot\cdot}}+N_{\cdot\cdot}\frac{N_{r\cdot}}{N_{ \cdot\cdot}}\frac{N_{\cdot s}}{N_{\cdot\cdot\cdot}}=N_{rs}-N_{r\cdot}N_{\cdot s }/N_{\cdot\cdot}\,.\]

If follows immediately that \(Z_{a+s}^{\prime}Z_{r}=0\) for all \(r\) and \(s\) if and only if \(N_{rs}=N_{r\cdot}N_{\cdot s}/N_{\cdot\cdot}\) for all \(r\) and \(s\).

**Exercise 7.3**  Find the ANOVA table for the two-way ANOVA without interaction model when there are proportional numbers. Find the least squares estimate of a contrast in the \(\alpha_{i}\)s. Find the variance of the contrast and give a definition of orthogonal contrasts that depends only on the contrast coefficients and the \(N_{ij}\)s. If the \(\alpha_{i}\)s correspond to levels of a quantitative factor, say \(x_{i}\)s, find the linear contrast.

The analysis when interaction is included is similar. It is based on repeated use of the one-way ANOVA.

**Exercise 7.4**  Using proportional numbers, find the ANOVA table for the two-way ANOVA with interaction model.

### Two-Way ANOVA with Unequal Numbers: General Case

Without balance or proportional numbers, there is no simplification of the model, so, typically, \(R(\alpha|\mu,\,\eta)\neq R(\alpha|\mu)\) and \(R(\eta|\mu,\alpha)\neq R(\eta|\mu)\). We are forced to analyze the model on the basis of the general theory alone.

#### Without Interaction

First consider the two-way ANOVA without interaction

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk},\]

\(i=1,\,\ldots,\,a,\,\,j=1,\,\ldots,\,b,\,\,k=1,\,\ldots,\,N_{ij}\). Note that we have not excluded the possibility that \(N_{ij}=0\).

One approach to analyzing a two-way ANOVA is by model selection. Consider \(R(\alpha|\mu,\,\eta)\) and \(R(\eta|\mu,\alpha)\). If both of these are large, the model is taken as

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}.\]

If, say, \(R(\alpha|\mu,\,\eta)\) is large and \(R(\eta|\mu,\alpha)\) is not, then the model

\[y_{ijk}=\mu+\alpha_{i}+e_{ijk}\]

should be appropriate; however, a further test of \(\alpha\) based on \(R(\alpha|\mu)\) may give contradictory results. If \(R(\alpha|\mu)\) is small and \(R(\alpha,\,\eta|\mu)\) is large, we have a problem: no model seems to fit. The groups, \(\alpha\) and \(\eta\), are together having an effect, but neither seems to be helping individually. A model with \(\mu\) and \(\alpha\) is inappropriate because\(R(\alpha|\mu)\) is small. A model with \(\mu\) and \(\eta\) is inappropriate because if \(\mu\) and \(\eta\) are in the model and \(R(\alpha|\mu,\,\eta)\) is large, we need \(\alpha\) also. However, the model with \(\mu\), \(\alpha\), and \(\eta\) is inappropriate because \(R(\eta|\mu,\,\alpha)\) is small. Finally, the model with \(\mu\) alone is inappropriate because \(R(\alpha,\,\eta|\mu)\) is large. Thus, every model that we can consider is inappropriate by some criterion. If \(R(\alpha,\,\eta|\mu)\) had been small, the best choice probably would be a model with only \(\mu\); however, some would argue that all of \(\mu,\,\alpha\), and \(\eta\) should be included on the basis of \(R(\alpha|\mu,\,\eta)\) being large.

Fortunately, it is difficult for these situations to arise. Suppose \(R(\alpha|\mu,\,\eta)=8\) and \(R(\alpha|\mu)=6\), both with 2 degrees of freedom. Let \(R(\eta|\mu,\,\alpha)=10\) with 4 degrees of freedom, and \(MSE=1\) with 30 degrees of freedom. The 0.05 test for \(\alpha\) after \(\mu\) and \(\eta\) is

\[[8/2]/1=4>3.32=F(0.95,\,2,\,30).\]

The test for \(\alpha\) after \(\mu\) is

\[[6/2]/1=3<3.32=F(0.95,\,2,\,30).\]

The test for \(\eta\) after \(\mu\) and \(\alpha\) is

\[[10/4]/1=2.5<2.69=F(0.95,\,4,\,30).\]

The test for \(\alpha\) and \(\eta\) after \(\mu\) is based on \(R(\alpha,\,\eta|\mu)=R(\alpha|\mu)+R(\eta|\mu,\,\alpha)=6+10=16\) with 2 + 4 = 6 degrees of freedom. The test is

\[[16/6]/1=2.67>2.42=F(0.95,\,6,\,30).\]

Although the tests are contradictory, the key point is that the \(P\) values for all four tests are about 0.05. For the first and last tests, the \(P\) values are just below 0.05; for the second and third tests, the \(P\) values are just above 0.05. The real information is not which tests are rejected and which are not rejected; the valuable information is that all four \(P\) values are approximately 0.05. All of the sums of squares should be considered to be either significantly large or not significantly large.

Because of the inflexible nature of hypothesis tests, we have chosen to discuss sums of squares that are either large or small, without giving a precise definition of what it means to be either large or small. The essence of any definition of large and small should be that the sum of two large quantities should be large and the sum of two small quantities should be small. For example, since \(R(\alpha,\,\eta|\mu)=R(\alpha|\mu)+R(\eta|\mu,\,\alpha)\), it should be impossible to have \(R(\alpha|\mu)\) small and \(R(\eta|\mu,\,\alpha)\) small, but \(R(\alpha,\,\eta|\mu)\) large. This consistency can be achieved by the following definition that exchanges one undefined term for another: _Large means significant or nearly significant_.

With this approach to the terms large and small, the contradiction alluded to above does not exist. The contradiction was based on the fact that with \(R(\alpha|\mu,\,\eta)\) large, \(R(\eta|\mu,\,\alpha)\) small, \(R(\alpha|\mu)\) small, and \(R(\alpha,\,\eta|\mu)\) large, no model seemed to fit.

However, this situation is impossible. If \(R(\eta|\mu,\alpha)\) is small and \(R(\alpha|\mu)\) is small, then \(R(\alpha,\eta|\mu)\) must be small; and the model with \(\mu\) alone fits. (Note that since \(R(\alpha|\mu,\eta)\) is large but \(R(\alpha,\eta|\mu)\) is small, we must have \(R(\eta|\mu)\) small; otherwise, two large quantities would add up to a small quantity.)

I find the argument of the previous paragraph convincing. Having built a better mousetrap, I expect the world to beat a path to my door. Unfortunately, I suspect that when the world comes, it will come carrying tar and feathers. It is my impression that in this situation most statisticians would prefer to use the model with all of \(\mu\), \(\alpha\), and \(\eta\). In any case, the results are sufficiently unclear that further data collection would probably be worthwhile.

Table 3 contains some suggested model choices based on various sums of squares. Many of the suggestions are potentially controversial; these are indicated by asterisks.

The example that has been discussed throughout this section is the case in the fourth row and second column of Table 3. The entry in the second row and fourth column is similar.

The entry in the second row and second column is the case where each effect is important after fitting the other effect, but neither is important on its own. My inclination is to choose the model based on the results of examining \(R(\alpha,\eta|\mu)\). On the other hand, it is sometimes argued that since the full model gives no basis for dropping either \(\alpha\) or \(\eta\) individually, the issue of dropping them both should not be addressed.

The entry in the third row and third column is very interesting. Each effect is important on its own, but neither effect is important after fitting the other effect. The corresponding models (2 and 3 in the table) are not hierarchical, i.e., neither column

\begin{table}
\begin{tabular}{c c c|c c c} \hline \multicolumn{6}{c}{Table Entries Are Models as Numbered Below} \\ \hline  & & \(R(\alpha|\mu,\eta)\) & L & & S \\ \(R(\eta|\mu,\alpha)\) & \(R(\alpha|\mu)\) & \(R(\eta|\mu)\) & L & S & L & S \\ \hline L & L & & 1 & 1 & 3* & I \\  & S & & 1 & 4,1* & 3 & 4,1* \\ S & L & & 2* & 2 & 1,2,3* & 2 \\  & S & & I & 4,1* & 3 & 4 \\ \hline \multicolumn{6}{l}{L indicates that the sum of squares is large.} \\ \multicolumn{6}{l}{S indicates that the sum of squares is small.} \\ \multicolumn{6}{l}{I indicates that the sum of squares is impossible.} \\ \multicolumn{6}{l}{* indicates that the model choice is debatable.} \\ \hline Models: & & & & & \\
1 & \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\) & & & & \\
2 & \(y_{ijk}=\mu+\alpha_{i}+e_{ijk}\) & & & & \\
3 & \(y_{ijk}=\mu+\eta_{j}+e_{ijk}\) & & & & \\
4 & \(y_{ijk}=\mu+e_{ijk}\) & & & & \\ \hline \end{tabular}
\end{table}
Table 3: Suggested model selections: two-way analysis of variance without interaction (unequal numbers)space is contained in the other, so there is no way of testing which is better. From a testing standpoint, I can see no basis for choosing the full model; but since both effects have been shown to be important, many argue that both effects belong in the model. One particular argument is that the structure of the model matrix is hiding what would otherwise be a significant effect. As will be seen in Chapter 13, with collinearity in the model matrix that is quite possible.

The entries in the third row, first column and first row, third column are similar. Both effects are important by themselves, but one of the effects is not important after fitting the other. Clearly, the effect that is always important must be in the model. The model that contains only the effect for which both sums of squares are large is an adequate substitute for the full model. On the other hand, the arguments in favor of the full model from the previous paragraph apply equally well to this situation.

Great care needs to be used in all the situations where the choice of model is unclear. With unequal numbers, the possibility of collinearity in the model matrix (see Chapter 13) must be dealt with. If collinearity exists, it will affect the conclusions that can be made about the models. Of course, when the conclusions to be drawn are questionable, additional data collection would seem to be appropriate.

_An alternative to selecting a model based on \(F\) tests,_ one that I have grown to like and to use in other books, _is to select a model based on the \(C_{p}\) statistic or one of the other model selection criteria that are discussed in Chapter 14_.

The discussion of model selection has been predicated on the assumption that there is no interaction. Some of the more bizarre situations that come up in model selection are more likely to arise if there is an interaction that is being ignored. A test for interaction should be made whenever possible. Of course, just because the test gives no evidence of interaction does not mean that interaction does not exist, or even that it is small enough so that it will not affect model selection.

Finally, it should be recalled that model selection is not the only possible goal. One may accept the full model and only seek to interpret it. For the purpose of interpreting the full model, \(R(\alpha|\mu)\) and \(R(\eta|\mu)\) are not very enlightening. In terms of the full model, the hypotheses that can be tested with these sums of squares are complicated functions of both the \(\alpha\)s and the \(\eta\)s. The exact nature of these hypotheses under the full model can be obtained from the formulae given below for the model with interaction.

#### Interaction

We now consider the two-way model with interaction. The model can be written

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+e_{ijk}, \tag{1}\]

\(i=1,\ldots,a,\)\(j=1,\ldots,b,\)\(k=1,\ldots,N_{ij}\). However, for most purposes, we do not recommend using this parameterization of the model. The full rank _cell means_ parameterization\[y_{ijk}=\mu_{ij}+e_{ijk} \tag{2}\]

is much easier to work with. The interaction model has the column space of a one-way ANOVA with unequal numbers.

Model (1) lends itself to two distinct orthogonal breakdowns of the sum of squares for the model. These are

\[R(\mu),\;\;R(\alpha|\mu),\;\;R(\eta|\mu,\alpha),\;\;R(\gamma|\mu,\alpha,\eta)\]

and

\[R(\mu),\;\;R(\eta|\mu),\;\;R(\alpha|\mu,\eta),\;\;R(\gamma|\mu,\alpha,\eta).\]

If \(R(\gamma|\mu,\alpha,\eta)\) is small, one can work with the reduced model. If \(R(\gamma|\mu,\alpha,\eta)\) is large, the full model must be retained. Just as with the balanced model, the \(F\) tests for \(\alpha\) and \(\eta\) test hypotheses that involve the interactions. Using the parameterization of model (2), the hypothesis associated with the test using \(R(\alpha|\mu)\) is that for all \(i\) and \(i^{\prime}\),

\[\sum_{j=1}^{b}N_{ij}\mu_{ij}/N_{i\cdot}=\sum_{j=1}^{b}N_{i^{\prime}j}\mu_{i^{ \prime}j}/N_{i^{\prime}\cdot}.\]

The hypothesis associated with the test using \(R(\alpha|\mu,\eta)\) is that for all \(i\),

\[\sum_{j=1}^{b}N_{ij}\mu_{ij}-\sum_{i^{\prime}=1}^{a}\sum_{j=1}^{b}N_{ij}N_{i^{ \prime}j}\mu_{i^{\prime}j}/N_{\cdot j}=0.\]

Since \(\mu_{ij}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}\), the formulae above can be readily changed to involve the alternative parameterization. Similarly, by dropping the \(\gamma_{ij}\), one can get the hypotheses associated with the sums of squares in the no interaction model. Neither of these hypotheses appears to be very interesting. The first of them is the hypothesis of equality of the weighted averages, taken over \(j\), of the \(\mu_{ij}\)s. It is unclear why one should be interested in weighted averages of the \(\mu_{ij}\)s when the weights are the sample sizes. For many purposes, a more reasonable hypothesis would seem to be that the simple averages, taken over \(j\), are all equal. The second hypothesis above is almost uninterpretable. In terms of model selection, these tests for "main effects" do not make much sense when there is interaction. The reduced models corresponding to such tests are not easy to find, so it is not surprising that the parametric hypotheses corresponding to these tests are not very interpretable.

Of course testing either \(\alpha_{1}=\cdots=\alpha_{a}\) or \(\eta_{1}=\cdots=\eta_{b}\) makes no sense in an interaction model because neither constraint puts a restriction on \(C(X)\). What the previous paragraph discussed was what hypotheses are being tested when using certain sums of squares.

A better idea seems to be to choose tests based directly on model (2). For example, the hypothesis of equality of the simple averages \(\bar{\mu}_{i}\). (i.e., \(\bar{\mu}_{i}\). = \(\bar{\mu}_{i^{\prime}}\). for \(i\), \(i^{\prime}=1\),..., \(a\)) is easily tested using the fact that the interaction model is really just a one-way ANOVA with unequal numbers.

At this point we should say a word about computational techniques, or rather explain why we will not discuss computational techniques. The difficulty in computing sums of squares for a two-way ANOVA with interaction and unequal numbers lies in computing \(R(\alpha|\mu,\,\eta)\) and \(R(\eta|\mu,\,\alpha)\). The sums of squares that are needed for the analysis are \(R(\gamma|\mu,\,\alpha,\,\eta)\), \(R(\alpha|\mu,\,\eta)\), \(R(\eta|\mu,\,\alpha)\), \(R(\alpha|\mu)\), \(R(\eta|\mu)\), and \(SSE\). \(SSE\) is the error computed as in a one-way ANOVA. \(R(\alpha|\mu)\) and \(R(\eta|\mu)\) are computed as in a one-way ANOVA, and the interaction sum of squares can be written as \(R(\gamma|\mu,\,\alpha,\,\eta)=R(\gamma|\mu)-R(\eta|\mu,\,\alpha)-R(\alpha|\mu)\), where \(R(\gamma|\mu)\) is computed as in a one-way ANOVA. Thus, only \(R(\alpha|\mu,\,\eta)\) and \(R(\eta|\mu,\,\alpha)\) present difficulties.

There are formulae available for computing \(R(\alpha|\mu,\,\eta)\) and \(R(\eta|\mu,\,\alpha)\), but the formulae are both nasty and of little use. With the advent of high speed computing, the very considerable difficulties in obtaining these sums of squares have vanished. Moreover, the formulae add very little to one's understanding of the method of analysis. The key idea in the analysis is that for, say \(R(\eta|\mu,\,\alpha)\), the \(\eta\) effects are being fitted after the \(\alpha\) effects. The general theory of linear models provides methods for finding the sums of squares and there is little simplification available in the special case.

Of course, there is more to an analysis than just testing group effects. As mentioned above, if there is evidence of interactions, probably the simplest approach is to analyze the \(\mu_{ij}\) model. If there are no interactions, then one is interested in testing contrasts in the main effects. Just as in the one-way ANOVA, it is easily seen that all estimable functions of the group effects will be contrasts; however, there is no assurance that all contrasts will be estimable. To test a contrast, say \(\alpha_{1}-\alpha_{2}=0\), the simplest method is to fit a model that does not distinguish between \(\alpha_{1}\) and \(\alpha_{2}\), see Example 3.2.0 and Section 3.4 and Christensen (2015) contains many similar examples. In the two-way ANOVA with unequal numbers, the model that does not distinguish between \(\alpha_{1}\) and \(\alpha_{2}\) may or may not have a different column space from that of the unrestricted model.

Scheffe (1959) (cf. Bailey 1953) reports on an experiment in which infant female rats were given to foster mothers for nursing, and the weights, in grams, of the infants were measured after 28 days. The two factors in the experiment were the genotypes of the infants and the genotypes of the foster mothers. In the experiment, an entire litter was given to a single foster mother. The variability within each litter was negligible compared to the variability between different litters, so the analysis was performed on the litter averages. Table 4 contains the data.

We use the two-way ANOVA model

\[y_{ijk}=G+L_{i}+M_{j}+[LM]_{ij}+e_{ijk},\]where \(L\) indicates the effect of the litter genotype and \(M\) indicates the effect of the foster mother genotype. Tables 7.5 and 7.6 contain, respectively, the sums of squares for error for a variety of submodels and the corresponding reductions in sums of squares for error and \(F\) tests. In discussing these data Christensen (2015, Section 14.1) adds a column of \(C_{p}\) statistics to Table 7.5 and eliminates Table 7.6.

Some percentiles of the \(F\) distribution that are of interest in evaluating the statistics of Table 7.6 are \(F(0.9,9,45)=1.78\), \(F(0.99,3,45)=4.25\), and \(F(0.95,6,45)=2.23\). Clearly, the litter-mother interaction and the main effect for litters can be dropped from model (3). However, the main effect for mothers is important. The smallest model that fits the data is

\[y_{ijk}=G+M_{j}+e_{ijk}.\]

\begin{table}
\begin{tabular}{c|c c c c} \hline Genotype of & \multicolumn{4}{c}{Genotype of Foster Mother} \\ Litter & A & F & I & J \\ \hline A & 61.5 & 55.0 & 52.5 & 42.0 \\  & 68.2 & 42.0 & 61.8 & 54.0 \\  & 64.0 & 60.2 & 49.5 & 61.0 \\  & 65.0 & & 52.7 & 48.2 \\  & 59.7 & & & 39.6 \\ F & 60.3 & 50.8 & 56.5 & 51.3 \\  & 51.7 & 64.7 & 59.0 & 40.5 \\  & 49.3 & 61.7 & 47.2 & \\  & 48.0 & 64.0 & 53.0 & \\  & & 62.0 & & \\ I & 37.0 & 56.3 & 39.7 & 50.0 \\  & 36.3 & 69.8 & 46.0 & 43.8 \\  & 68.0 & 67.0 & 61.3 & 54.5 \\  & & & 55.3 & \\  & & & 55.7 & \\ J & 59.0 & 59.5 & 45.2 & 44.8 \\  & 57.4 & 52.8 & 57.0 & 51.5 \\  & 54.0 & 56.0 & 61.4 & 53.0 \\  & 47.0 & & & 42.0 \\  & & & & 54.0 \\ \hline \end{tabular}
\end{table}
Table 7.4: Infant rats’ weight gain with foster mothersThis is just a one-way ANOVA model and can be analyzed as such. By analogy with balanced two-factor ANOVAs, tests of contrasts might be best performed using \(MSE([LM])\) rather than \(MSE([M])=3329/57=58.40\).

The mean values for the foster mother genotypes are reported in Table 7.7, along with the number of observations for each mean. It would be appropriate to continue the analysis by comparing all pairs of means. This can be done with either Scheffe's method, Bonferroni's method, or the LSD method. The LSD method with \(\alpha=0.05\) establishes that genotype \(J\) is distinct from genotypes \(A\) and \(F\). (Genotypes \(F\) and \(I\) are almost distinct.) Bonferroni's method with \(\alpha=0.06\) establishes that \(J\) is distinct from \(F\) and that \(J\) is almost distinct from \(A\).

#### Exercise 7.5

Analyze the following data as a two-factor ANOVA where the subscripts \(i\) and \(j\) indicate the two factors.

\begin{table}
\begin{tabular}{c c c c} \hline \multicolumn{2}{c}{Reduction in \(SSE\)} & \(df\) & \(MS\) & \(F\)* \\ \hline \(R(LM|L,M,G)\) & \(=3265-2441=824\) & 9 & 91.6 & 1.688 \\ \(R(M|L,G)\) & \(=4040-3265=775\) & 3 & 258.3 & 4.762 \\ \(R(L|G)\) & \(=4100-4040=60\) & 3 & 20.0 & 0.369 \\ \(R(L|M,G)\) & \(=3329-3265=64\) & 3 & 21.3 & 0.393 \\ \(R(M|G)\) & \(=4100-3329=771\) & 3 & 257. & 4.738 \\ \(R(L,M|G)\) & \(=4100-3265=835\) & 6 & 139.2 & 2.565 \\ \hline \end{tabular}

* All \(F\) statistics calculated using \(MSE([LM])=2441/45=54.244\) in the denominator

\end{table}
Table 7.6: \(F\) tests for fitting models to the data of Table 7.4

\begin{table}
\begin{tabular}{c c c c} \hline \multicolumn{2}{c}{Foster Mother} & Parameter & \(N_{.j}\) & Estimate \\ \hline \(A\) & \(G\) + \(M_{1}\) & 16 & 55.400 \\ \(F\) & \(G\) + \(M_{2}\) & 14 & 58.700 \\ \(I\) & \(G\) + \(M_{3}\) & 16 & 53.363 \\ \(J\) & \(G\) + \(M_{4}\) & 15 & 48.680 \\ \hline \end{tabular}
\end{table}
Table 7.7: Mean values for foster mother genotypesThe dependent variable is a mathematics ineptitude score. The first factor (_i_) identifies economics majors, anthropology majors, and sociology majors, respectively. The second factor (_j_) identifies whether the student's high school background was rural (1) or urban (2).

**Exercise 7.6**: Analyze the following data as a two-factor ANOVA where the subscripts \(i\) and \(j\) indicate the two factors.

\begin{tabular}{c c|c c c}  & & & \multicolumn{2}{c}{\(y_{ijk}s\)} \\  & \(i\) & 1 & 2 & 3 \\ \hline \(j\) & 1 & 1.620 & 2.228 & 2.999 \\  & & 1.669 & 3.219 & 1.615 \\  & & 1.155 & 4.080 & \\  & & 2.182 & & \\  & & 3.545 & & \\  & 2 & 1.342 & 3.762 & 2.939 \\  & & 0.687 & 4.207 & 2.245 \\  & & 2.000 & 2.741 & 1.527 \\  & & 1.068 & & 0.809 \\  & & 2.233 & & 1.942 \\  & & 2.664 & & \\  & & 1.002 & & \\ \end{tabular}

The dependent variable is again a mathematics ineptitude score and the levels of the first factor identify the same three majors as in Exercise 7.5. In these data, the second factor identifies whether the student is lefthanded (1) or righthanded (2).

#### Characterizing the Interaction Space

In a two-way ANOVA with interaction and unbalanced numbers, the cell-means parameterization is

\[y_{ijk}=\mu_{ij}+\varepsilon_{ijk},\ \ \ \ \ \ i=1,\ldots,a,\ \ \ j=1,\ldots,b,\ \ \ k=1,\ldots,N_{ij},\]

which is really just a one-way ANOVA with unequal numbers and the pair of subscripts \(ij\) identifying the \(ab\) different groups. The traditional parameterization is \(\mu_{ij}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}\). Write the cell-means model in matrix form as \(Y=X\tilde{\mu}+e\). \(X\) has \(ab\) columns and the \(rs\) column has the form

\[X_{rs}=[t_{ijk}],\ \ \ \text{with}\ \ \ t_{ijk}=\delta_{(i,j)(r,s)}\]

where for any two symbols \(g\) and \(h\), the kronecker delta has

\[\delta_{gh}=\begin{cases}1&\text{if }g=h\\ 0&\text{if }g\neq h.\end{cases}\]

Note that

\[C(X)=\left\{v|v=[v_{ijk}],\ \ \ \text{with}\ \ \ v_{ijk}=\mu_{ij}\ \text{for some }\mu_{ij}\right\}.\]

Similar to the balanced case, we will see that the interaction space is the set of vectors

\[T=[t_{ijk}],\ \ \ \text{with}\ \ \ t_{ijk}=q_{ij}/N_{ij}\ \ \ \ \ \ \text{where}\ \ \ q_{i\cdot}=0=q_{\cdot j},\ \ \text{for all }i,j.\]

An interaction contrast is \(T^{\prime}X\tilde{\mu}=\sum_{ijk}q_{ij}\mu_{ij}/N_{ij}=\sum_{ij}q_{ij}\mu_{ij} =\sum_{ij}q_{ij}\gamma_{ij}\). Clearly, \(T\in C(X)\), so \(T^{\prime}MY=T^{\prime}Y\) where \(M\) is the perpendicular projection operator onto \(C(X)\). It follows that the least squares estimate of \(T^{\prime}X\tilde{\mu}\) is \(T^{\prime}Y=\sum_{ij}q_{ij}\bar{y}_{ij}\). These are essentially the same results as for balanced ANOVA.

To see that vectors \(T\) characterize the interaction space, write the corresponding main effects model

\[Y=J\mu+X_{\alpha}\alpha+X_{\eta}\eta+e.\]

The matrix \(X_{\alpha}\) has columns

\[X_{r}=[t_{ijk}],\ \ \ t_{ijk}=\delta_{ir}\ \ \ \ \ \ r=1,\ldots,a\]

The matrix \(X_{\eta}\) has columns

\[X_{a+s}=[t_{ijk}],\ \ \ t_{ijk}=\delta_{js}\ \ \ \ \ s=1,\ldots,b.\]Note that the \(ab\) columns of \(X\) are generated by multiplying elementwise the \(a\) columns of \(X_{\alpha}\) and the \(b\) columns of \(X_{\eta}\) because \(\delta_{ir}\delta_{js}=\delta_{(i,\,j)(r,s)}\). By definition, the interaction space is \(C(J,\,X_{\alpha},\,X_{\eta})_{C(X)}^{\perp}\). Thus, the characterization of the interaction space results from vectors of the form \(T\) spanning a space of sufficient rank [\((a-1)(b-1)\) when \(N_{ij}>0\) for all \(ij\)], having \(T\in C(X)\), and \(X^{\prime}_{h}T=0\), \(h=1,\,\ldots,\,a+b\). That \(X^{\prime}_{h}T=0\) follows from the definitions and arithmetic. Note that to have orthogonal interaction contrasts we need the \(T\) vectors corresponding to the interaction contrasts to be orthogonal.

Note also that all interaction contrasts are contrasts in the \(\mu_{ij}\) one-way model, but not all \(\mu_{ij}\) contrasts are interaction contrasts. As in Chapter 4, an arbitrary element of the \(\mu_{ij}\) contrast space is

\[S=[s_{ijk}],\quad\text{with}\ \ \ s_{ijk}=s_{ij}/N_{ij}\quad\quad\text{where}\ \ \ s_{..}=0.\]

In addition to the interaction space being a subset of the contrast space, there is an \(a-1\) dimensional subspace of the contrast space consisting of vectors

\[T_{\alpha}=[t_{ijk}],\quad\text{with}\ \ \ t_{ijk}=c_{i}/bN_{ij}\quad\quad\text{ where}\ \ \ c.=0\]

and a \(b-1\) dimensional subspace of vectors

\[T_{\eta}=[t_{ijk}],\quad\text{with}\ \ \ t_{ijk}=d_{j}/aN_{ij}\quad\quad\text{ where}\ \ \ d.=0.\]

These define contrasts in the interaction model of

\[T^{\prime}_{\alpha}X\tilde{\mu}=\sum_{ij}c_{i}\mu_{ij}/b=\sum_{i}c_{i}\tilde{ \mu}_{i..}=\sum_{i}c_{i}(\alpha_{i}+\tilde{\gamma}_{i..})\]

with estimate \(T^{\prime}_{\alpha}Y=\sum_{i}c_{i}\tilde{\gamma}_{i}\) where \(\tilde{\gamma}_{i}\equiv\sum_{j}\tilde{\gamma}_{ij..}/b\) which need not equal \(\tilde{\gamma}_{i..}\) and

\[T^{\prime}_{\eta}X\tilde{\mu}=\sum_{ij}d_{j}\mu_{ij}/a=\sum_{j}d_{j}\tilde{ \mu}_{..j}=\sum_{j}d_{j}(\eta_{j}+\tilde{\gamma}_{..j})\]

with estimate \(T^{\prime}_{\eta}Y=\sum_{j}d_{j}\tilde{\gamma}_{j}\) where \(\tilde{\gamma}_{j}\equiv\sum_{i}\tilde{\gamma}_{ij..}/a\) which need not equal \(\tilde{\gamma}_{i..}\). Under proportional numbers, these three spaces are orthogonal but in general they merely intersect in the zero vector.

For the main effects model, the vectors \(T_{\alpha}\) and \(T_{\eta}\) typically are not in \(C(J,\,X_{\alpha},\,X_{\eta})\), so although, for example, \(T^{\prime}_{\alpha}[J\mu\,+\,X_{\alpha}\alpha\,+\,X_{\eta}\eta]=\sum_{i}c_{i} \alpha_{i}\), the estimate is _not_\(T^{\prime}_{\alpha}Y\), it is \(T^{\prime}_{\alpha}M_{0}Y\) where \(M_{0}\) is the ppo onto \(C(J,\,X_{\alpha},\,X_{\eta})\).

### Three or More Way Analyses

#### Balanced Analyses

With balanced or proportional numbers, the analyses for more general models follow the same patterns as those of the two-way ANOVA. (Proportional numbers can be defined in terms of complete independence in higher dimensional tables, see Fienberg 1980 or Christensen 1997.) Consider, for example, the model

\[y_{ijkm}=\mu+\alpha_{i}+\eta_{j}+\gamma_{k}+(\alpha\eta)_{ij}+(\alpha\gamma)_{ ik}+(\eta\gamma)_{jk}+(\alpha\eta\gamma)_{ijk}+e_{ijkm},\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), \(k=1,\ldots,c\), \(m=1,\ldots,N\). The sums of squares for the main effects of \(\alpha\), \(\eta\), and \(\gamma\) are based on the one-way ANOVA ignoring all other effects, e.g.,

\[SS(\eta)=acN\sum_{j=1}^{b}(\bar{y}_{\cdot j\cdot}-\bar{y}_{\cdot\ldots})^{2}. \tag{1}\]

The sums of squares for the two-way interactions, \(\alpha\eta\), \(\alpha\gamma\), and \(\eta\gamma\), are obtained as in the two-way ANOVA by ignoring the third effect, e.g.,

\[SS(\alpha\gamma)=bN\sum_{i=1}^{a}\sum_{k=1}^{c}(\bar{y}_{i\cdot k\cdot}-\bar{y }_{i\cdot\ldots}-\bar{y}_{\cdot\cdot k\cdot}+\bar{y}_{\cdot\ldots})^{2}. \tag{2}\]

The sum of squares for the three-way interaction \(\alpha\eta\gamma\) is found by subtracting all of the other sums of squares (including the grand mean's) from the sum of squares for the full model. Note that the full model is equivalent to the one-way ANOVA model

\[y_{ijkm}=\mu_{ijk}+e_{ijkm}.\]

Sums of squares and their associated projection operators are defined from reduced models. For example, \(M_{\eta}\) is the perpendicular projection operator for fitting the \(\eta\)s after \(\mu\) in the model

\[y_{ijkm}=\mu+\eta_{j}+e_{ijkm}.\]

The subscripts \(i\), \(k\), and \(m\) are used to indicate the replications in this one-way ANOVA. \(SS(\eta)\) is defined by

\[SS(\eta)=Y^{\prime}M_{\eta}Y.\]

The algebraic formula for \(SS(\eta)\) was given in (1). Similarly, \(M_{\alpha\gamma}\) is the projection operator for fitting the \((\alpha\gamma)\)s after \(\mu\), the \(\alpha\)s, and the \(\gamma\)s in the model\[y_{ijkm}=\mu+\alpha_{i}+\gamma_{k}+(\alpha\gamma)_{ik}+e_{ijkm}.\]

In this model, the subscripts \(j\) and \(m\) are used to indicate replication. The sum of squares \(SS(\alpha\gamma)\) is

\[SS(\alpha\gamma)=Y^{\prime}M_{\alpha\gamma}Y.\]

The algebraic formula for \(SS(\alpha\gamma)\) was given in (2). Because all of the projection operators (except for the three-factor interaction) are defined on the basis of reduced models that have previously been discussed, the sums of squares take on the familiar forms indicated above.

The one new aspect of the model that we are considering is the inclusion of the three-factor interaction. As mentioned above, the sum of squares for the three-factor interaction is just the sum of squares that is left after fitting everything else, i.e.,

\[SS(\alpha\eta\gamma)\equiv R\left[(\alpha\eta\gamma)|\mu,\alpha,\eta,\gamma, (\alpha\eta),(\alpha\gamma),(\eta\gamma)\right].\]

The space for the three-factor interaction is the orthogonal complement (with respect to the space for the full model) of the space for the model that includes all factors except the three-factor interaction. Thus, the space for the three-factor interaction is orthogonal to everything else. (This is true even when the numbers are not balanced.)

In order to ensure a nice analysis, we need to show that the spaces associated with all of the projection operators are orthogonal and that the projection operators add up to the perpendicular projection operator onto the space for the full model. First, we show that \(C(M_{\mu},M_{\alpha},M_{\eta},M_{\gamma},M_{\alpha\eta},M_{\alpha\gamma},M_{ \eta\gamma})\) is the column space for the model

\[y_{ijkm}=\mu+\alpha_{i}+\eta_{j}+\gamma_{k}+(\alpha\eta)_{ij}+(\alpha\gamma)_{ ik}+(\eta\gamma)_{jk}+e_{ijkm},\]

and that all the projection operators are orthogonal. That the column spaces are the same follows from the fact that the column space of the model without the three-factor interaction is precisely the column space obtained by combining the column spaces of all of the two-factor with interaction models. Combining the spaces of the projection operators is precisely combining the column spaces of all the two-factor with interaction models. That the spaces associated with all of the projection operators are orthogonal follows easily from the fact that all of the spaces come from reduced models. For the reduced models, characterizations have been given for the various spaces. For example,

\[C(M_{\eta})=\{v|v=[v_{ijkm}],\ \mbox{where}\ v_{ijkm}=d_{j}\ \mbox{for some}\ d_{1},\ldots,d_{b}\ \mbox{with}\ d.=0\}.\]

Similarly,

\[C(M_{\alpha\gamma})= \{w|w=[w_{ijkm}],\ \mbox{where}\ w_{ijkm}=r_{ik}\ \mbox{for some}\ r_{ik}\] \[\mbox{with}\ r_{i}.=r_{k}=0\ \mbox{for}\ i=1,\ldots,a,k=1,\ldots,c\}.\]With these characterizations, it is a simple matter to show that the projection operators define orthogonal spaces.

Let \(M\) denote the perpendicular projection operator for the full model. The projection operator onto the interaction space, \(M_{\alpha\eta\gamma}\), has been defined as

\[M_{\alpha\eta\gamma}\equiv M-[M_{\mu}+M_{\alpha}+M_{\eta}+M_{\gamma}+M_{\alpha \eta}+M_{\alpha\gamma}+M_{\eta\gamma}];\]

thus,

\[M=M_{\mu}+M_{\alpha}+M_{\eta}+M_{\gamma}+M_{\alpha\eta}+M_{\alpha\gamma}+M_{ \eta\gamma}+M_{\alpha\eta\gamma},\]

where the spaces of all the projection operators on the right side of the equation are orthogonal to each other.

**Exercise 7.7**: Show that \(C(M_{\eta})\perp C(M_{\alpha\gamma})\) and that \(C(M_{\eta\gamma})\perp C(M_{\alpha\gamma})\). Give an explicit characterization of a typical vector in \(C(M_{\alpha\eta\gamma})\) and show that your characterization is correct.

If the \(\alpha\), \(\eta\), and \(\gamma\) effects correspond to quantitative levels of some factor, the three-way ANOVA corresponds to a polynomial in three variables. The main effects and the two-way interactions can be dealt with as before. The three-way interaction can be broken down into contrasts such as the linear-by-linear-by-quadratic.

#### Unbalanced Analyses

For unequal numbers, the analysis can be performed by comparing models.

**Example 7.6.1**: Table 7.8 is derived from Scheffe (1959) and gives the moisture content (in grams) for samples of a food product made with three kinds of salt (\(A\)), three amounts of salt (\(B\)), and two additives (\(C\)). The amounts of salt, as measured in moles, are equally spaced. The two numbers listed for some group combinations are replications. We wish to analyze these data.

We will consider these data as a three-factor ANOVA. From the structure of the replications, the ANOVA has unequal numbers. The general model for a three-factor ANOVA with replications is

\[y_{ijkm}=G+A_{i}+B_{j}+C_{k}+[AB]_{ij}+[AC]_{ik}+[BC]_{jk}+[ABC]_{ijk}+e_{ijkm}.\]

Our first priority is to find out which interactions are important. Table 7.9 contains the sum of squares for error and the degrees of freedom for error for all models that include all of the main effects. Each model is identified in the table by the highest order terms in the model (cf. Table 7.5, Section 5). Readers familiar with methods for fitting log-linear models (cf. Fienberg 1980 or Christensen 1997) will notice a correspondence between Table 7.9 and similar displays used in fitting three-dimensional contingency tables. The analogies between selecting log-linear models and selecting models for unbalanced ANOVA are pervasive. Christensen (2015, Section 16.1) gives a more expansive version of this data analysis.

All of the models have been compared to the full model using \(F\) statistics in Table 7.9. It takes neither a genius nor an \(F\) table to see that the only models that fit the data are the models that include the \([AB]\) interaction. A number of other comparisons can be made among models that include \([AB]\). These are \([AB][AC][BC]\) versus \([AB][AC]\), \([AB][AC][BC]\) versus \([AB][C]\), \([AB][AC]\) versus \([AB][C]\), and \([AB][BC]\) versus \([AB][C]\). None of the comparisons show any lack of fit. The last two comparisons are illustrated below.

\[[AB][AC]\text{ versus }[AB][C]:\]

\[R(AC|AB,C)=45.75-45.18=0.57,\]

\[F=(0.57/2)/2.3214=0.123.\]

\begin{table}
\begin{tabular}{l r r r r r r r r} \hline \multicolumn{1}{c}{\(A\) (salt)} & \multicolumn{3}{c}{1} & \multicolumn{3}{c}{2} & \multicolumn{3}{c}{3} \\ \hline \(B\) (amount salt) & 1 & 2 & 3 & 1 & 2 & 3 & 1 & 2 & 3 \\ \hline \multirow{4}{*}{\(C\) (additive)} & 1 & 8 & 17 & 22 & 7 & 26 & 34 & 10 & 24 & 39 \\  & & 13 & 20 & 10 & 24 & & 9 & & 36 \\  & & & & & & & & & \\  & 2 & 5 & 11 & 16 & 3 & 17 & 32 & 5 & 16 & 33 \\  & & 4 & 10 & 15 & 5 & 19 & 29 & 4 & & 34 \\ \hline \end{tabular}
\end{table}
Table 7.8: Moisture content of a food product

\begin{table}
\begin{tabular}{l r r r} \hline Model & \multicolumn{1}{c}{\(SSE\)} & \multicolumn{1}{c}{\(df\)} & \multicolumn{1}{c}{\(F\)*} \\ \hline \([ABC]\) & 32.50 & 14 & — \\ \([AB][AC][BC]\) & 39.40 & 18 & 0.743 \\ \([AB][AC]\) & 45.18 & 20 & 0.910 \\ \([AB][BC]\) & 40.46 & 20 & 0.572 \\ \([AC][BC]\) & 333.2 & 22 & 16.19 \\ \([AB][C]\) & 45.75 & 22 & 0.713 \\ \([AC][B]\) & 346.8 & 24 & 13.54 \\ \([BC][A]\) & 339.8 & 24 & 13.24 \\ \([A][B][C]\) & 351.1 & 26 & 11.44 \\ \hline \end{tabular}

*The \(F\) statistics are for testing each model against the model with a three-factor interaction, i.e., \([ABC]\). The denominator of each \(F\) statistic is \(MSE([ABC])=32.50/14=2.3214\)

\end{table}
Table 7.9: Statistics for fitting models to the data of Table 7.8\[[AB][BC]\ \text{versus}\ [AB][C]:\]

\[R(BC|AB,\ C)=45.75-40.46=5.29,\]

\[F=(5.29/2)/2.3214=1.139.\]

Note that, by analogy to the commonly accepted practice for balanced ANOVAs, all tests have been performed using \(MSE([ABC])\) in the denominator, i.e., the estimate of pure error from the full model.

The smallest model that seems to fit the data adequately is \([AB][C]\). The \(F\) statistics for comparing \([AB][C]\) to the larger models are all extremely small. Writing out the model \([AB][C]\), it is

\[y_{ijkm}=G+A_{i}+B_{j}+C_{k}+[AB]_{ij}+e_{ijkm}.\]

We need to examine the \([AB]\) interaction. Since the levels of \(B\) are quantitative, a model that is equivalent to \([AB][C]\) is a model that includes the main effects for \(C\) but, instead of fitting an interaction in \(A\) and \(B\), fits a separate regression equation in the levels of \(B\) for each level of \(A\). Let \(x_{j}\), \(j=1\), \(2\), \(3\), denote the levels of \(B\). There are three levels of \(B\), so the most general polynomial we can fit is a second-degree polynomial in \(x_{j}\). Since the levels of salt were equally spaced, it does not matter much what we use for the \(x_{j}\)s. The computations were performed using \(x_{1}=1\), \(x_{2}=2\), \(x_{3}=3\). In particular, the model \([AB][C]\) was reparameterized as

\[y_{ijkm}=A_{i0}+A_{i1}x_{j}+A_{i2}x_{j}^{2}+C_{k}+e_{ijkm}. \tag{3}\]

With a notation similar to that used in Table 7.9, the \(SSE\) and the \(dfE\) are reported in Table 7.10 for model (3) and three reduced models.

Note that the \(SSE\) and \(df\) reported in Table 7.10 for \([A_{0}][A_{1}][A_{2}][C]\) are identical to the values reported in Table 7.9 for \([AB][C]\). This, of course, must be true if the models are merely reparameterizations of one another. First we want to establish whether the quadratic effects are necessary in the regressions. To do this we test

\begin{table}
\begin{tabular}{l c c} \hline Model & \(SSE\) & \(df\) \\ \hline \([A_{0}][A_{1}][A_{2}][C]\) & 45.75 & 22 \\ \([A_{0}][A_{1}][C]\) & 59.98 & 25 \\ \([A_{0}][A_{1}]\) & 262.0 & 26 \\ \([A_{0}][C]\) & 3130. & 28 \\ \hline \end{tabular}
\end{table}
Table 7.10: Additional statistics for fitting models to the data of Table 7.8\[[A_{0}][A_{1}][A_{2}][C]\ \text{versus}\ [A_{0}][A_{1}][C]:\]

\[R(A_{2}|A_{1},\,A_{0},\,C)=59.98-45.75=14.23,\]

\[F=(14.23/3)/2.3214=2.04.\]

Since \(F(0.95,\,3,\,14)=3.34\), there is no evidence of any nonlinear effects.

At this point it might be of interest to test whether there is any linear effect. This is done by testing \([A_{0}][A_{1}][C]\) against \([A_{0}][C]\). The statistics needed for this test are given in Table 7.10. Instead of actually doing the test, recall that no models in Table 7.9 fit the data unless they included the \([AB]\) interaction. If we eliminated the linear effects, we would have a model that involved none of the \([AB]\) interaction. (The model \([A_{0}][C]\) is identical to the ANOVA model \([A][C]\).) We already know that such models do not fit.

Finally, we have never explored the possibility that there is no main effect for \(C\). This can be done by testing

\[[A_{0}][A_{1}][C]\ \text{versus}\ [A_{0}][A_{1}]:\]

\[R(C|A_{1},\,A_{0})=262.0-59.98=202,\]

\[F=(202/1)/2.3214=87.\]

Obviously, there is a substantial main effect for \(C\), the type of food additive.

Our conclusion is that the model \([A_{0}][A_{1}][C]\) is the smallest model yet considered that adequately fits the data. This model indicates that there is an effect for the type of additive and a linear relationship between amount of salt and moisture content. The slope and intercept of the line may depend on the type of salt. (The intercept of the line also depends on the type of additive.) Table 7.11 contains parameter estimates and standard errors for the model. All estimates in the example use the side condition \(C_{1}=0\).

\begin{table}
\begin{tabular}{c c c} \hline Parameter & Estimate & S.E. \\ \hline \(A_{10}\) & 3.350 & 1.375 \\ \(A_{11}\) & 5.85 & 0.5909 \\ \(A_{20}\) & \(-3.789\) & 1.237 \\ \(A_{21}\) & 13.24 & 0.5909 \\ \(A_{30}\) & \(-4.967\) & 1.231 \\ \(A_{31}\) & 14.25 & 0.5476 \\ \(C_{1}\) & 0. & none \\ \(C_{2}\) & \(-5.067\) & 0.5522 \\ \hline \end{tabular}
\end{table}
Table 7.11: Parameter estimates and standard errors for the model \(y_{ijkm}=A_{i0}+A_{i1}x_{j}+C_{k}+e_{ijkm}\)Note that, in lieu of the \(F\) test, the test for the main effect \(C\) could be performed by looking at \(t=-5.067/0.5522=-9.176\). Moreover, we should have \(t^{2}=F\). The \(t\) statistic squared is 84, while the \(F\) statistic reported earlier is 87. The difference is due to the fact that the S.E. reported uses the \(MSE\) for the model being fitted, while in performing the \(F\) test we used the \(MSE([ABC])\).

Are we done yet? No! The parameter estimates suggest some additional questions. Are the slopes for salts 2 and 3 the same, i.e., is \(A_{21}=A_{31}?\) In fact, are the entire lines for salts 2 and 3 the same, i.e., are \(A_{21}=A_{31}\), \(A_{20}=A_{30}\)? We can fit models that incorporate these assumptions.

\[\begin{array}{l r}\mbox{Model}&SSE&df\\ \hline[A_{0}][A_{1}][C]&59.98&25\\ [A_{0}][A_{1}][C],\,A_{21}=A_{31}&63.73&26\\ [A_{0}][A_{1}][C],\,A_{21}=A_{31},\,A_{20}=A_{30}&66.97&27\\ \end{array}\]

It is a small matter to check that there is no lack of fit displayed by any of these models. The smallest model that fits the data is now \([A_{0}][A_{1}][C]\), \(A_{21}=A_{31}\), \(A_{20}=A_{30}\). Thus there seems to be no difference between salts 2 and 3, but salt 1 has a different regression than the other two salts. (We did not actually test whether salt 1 is different, but if salt 1 had the same slope as the other two, then there would be no interaction, and we know that interaction exists.) There is also an effect for the food additives. The parameter estimates and standard errors for the final model are given in Table 7.12.

Figure 7.1 shows the fitted values for the final model. The two lines for a given additive are shockingly close when \(x=1\), which makes me wonder if \(j=1\) is the condition of no salt being used. Scheffe does not say.

Are we done yet? Probably not. We have not even considered the validity of the assumptions. Are the errors normally distributed? Are the variances the same for every group combination? Some methods for addressing these questions are discussed in Chapter 12. Technically, we need to ask whether \(C_{1}=C_{2}\) in this new model. A quick look at the estimate and standard error for \(C_{2}\) answers the question in the negative. We also have not asked whether \(A_{10}=A_{20}\). Personally, I find this last question so uninteresting that I would be loath to examine it. However, a look at the estimates and standard errors suggests that the answer is no. A more interesting question is whether \(A_{10}\,+\,A_{11}\,=\,A_{20}\,+\,A_{21}\), i.e. that the two lines for different salts but the same additive take on the same value at \(x=1\). It is pretty clear from Figure 7.1 that there will be no evidence against this hypothesis that was suggested by the data, cf. Exercise 7.7.6. Christensen (2015, Section 16.1) considers this issue in more detail.

As mentioned in Example 7.6.1, the correspondences between log-linear models and unbalanced ANOVAs are legion. Often these correspondences have been overlooked. We have just considered a three-factor unbalanced ANOVA. What would we do with a four-factor or five-factor ANOVA? There is a considerable amount of literature in log-linear model theory about how to select models when there are a large number of factors. In particular, Benedetti and Brown (1978) and Christensen (1997) have surveyed strategies for selecting log-linear models. Those strategies can be applied to unbalanced ANOVAs with equal success.

I might also venture a personal opinion that statisticians tend not to spend enough time worrying about what high-dimensional ANOVA models actually mean. Log-linear model theorists do worry about interpreting their models. Wermuth (1976) has developed a method of searching among log-linear models that have nice interpretations, cf. also Christensen (1997). I believe that her method could be applied equally well to ANOVA models.

Figure 7.1: Fitted values for moisture content data

[MISSING_PAGE_EMPTY:5550]

#### Exercise 7.7.2

An experiment was conducted to examine thrust forces when drilling under different conditions. Data were collected for four drilling speeds and three feeds.

\[\begin{array}{c|cccc}&&\omit\span\omit\span\omit\span\omit\span\omit\text{Speed}\\ \text{Feed}&100&250&400&550\\ \hline&121&98&83&58\\ &124&108&81&59\\ 0.005&104&87&88&60\\ &124&94&90&66\\ &110&91&86&56\\ &329&291&281&265\\ &331&265&278&265\\ 0.010&324&295&275&269\\ &338&288&276&260\\ &332&297&287&251\\ &640&569&551&487\\ &600&575&552&481\\ &612&565&570&487\\ &620&573&546&500\\ &623&588&569&497\\ \end{array}\]

Analyze these data.

#### Exercise 7.7.3

Consider the model

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+e_{ijk},\]

\(i=1\), \(2\), \(3\), \(4\), \(j=1\), \(2\), \(3\), \(k=1\), \(\dots\), \(N_{ij}\), where for \(i\neq 1\neq j\), \(N_{ij}=N\), and \(N_{11}=2N\). This model could arise from an experimental design having \(\alpha\) treatments of No Treatment (NT), \(a_{1}\), \(a_{2}\), \(a_{3}\) and \(\eta\) treatments of NT, \(b_{1}\), \(b_{2}\). This gives a total of 12 treatments: NT, \(a_{1}\), \(a_{2}\), \(a_{3}\), \(b_{1}\), \(a_{1}b_{1}\), \(a_{2}b_{1}\), \(a_{3}b_{1}\)\(b_{2}\), \(a_{1}b_{2}\), \(a_{2}b_{2}\), and \(a_{3}b_{2}\). Since NT is a control, it might be of interest to compare all of the treatments to NT. If NT is to play such an important role in the analysis, it is reasonable to take more observations on NT than on the other treatments. Find sums of squares for testing

(a) no differences between \(a_{1}\), \(a_{2}\), \(a_{3}\),

(b) no differences between \(b_{1}\), \(b_{2}\),

(c) no \(\{a_{1}\), \(a_{2}\), \(a_{3}\}\times\{b_{1}\), \(b_{2}\}\) interaction,

(d) no differences between NT and the averages of \(a_{1}\), \(a_{2}\), and \(a_{3}\) when there is interaction,

(e) no differences between NT and the average of \(a_{1}\), \(a_{2}\), and \(a_{3}\) when there is no interaction present,

(f) no differences between NT and the average of \(b_{1}\) and \(b_{2}\) when there is interaction,(g) no differences between NT and the average of \(b_{1}\) and \(b_{2}\) when there is no interaction present.

Discuss the orthogonality relationships among the sums of squares. For parts (e) and (g), use the assumption of no interaction. Do not just repeat parts (d) and (f)!

**Exercise 7.7.4**: Consider the linear model \(y_{ij}=\mu+\alpha_{i}+\eta_{j}+e_{ij},i=1,\ldots,a\), \(j=1,\ldots,b\). As in Section 1, write \(X=[X_{0},\,X_{1},\,\ldots,\,X_{a},\,X_{a+1},\,\ldots,\,X_{a+b}]\). If we write the observations in the usual order, we can use Kronecker products to write the model matrix. Write \(X=[J,\,X_{*},\,X_{**}]\), where \(X_{*}=[X_{1},\,\ldots,\,X_{a}]\), and \(X_{**}=[X_{a+1},\,\ldots,\,X_{a+b}]\). Using Kronecker products, \(X_{*}=[I_{a}\otimes J_{b}]\), and \(X_{**}=[J_{a}\otimes I_{b}]\). In fact, with \(n=ab\), \(J=J_{n}=[J_{a}\otimes J_{b}]\). Use Kronecker products to show that \(X^{\prime}_{*}(I-[1/n]J^{n}_{n})X_{**}=0\). In terms of Section 1, this is the same as showing that \(C(Z_{1},\,\ldots,\,Z_{a})\perp C(Z_{a+1},\,\ldots,\,Z_{a+b})\). Also show that \([(1/a)J^{a}_{a}\otimes I_{b}]\) is the perpendicular projection operator onto \(C(X_{**})\) and that \(M_{\eta}=[(1/a)J^{a}_{a}\otimes(I_{b}-(1/b)J^{b}_{b})]\).

**Exercise 7.7.5**: Consider the balanced two-way ANOVA with interaction model \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+e_{ijk},\ i=1,\,\ldots,\,a,\ j=1, \,\ldots,\,b,\ k=1,\,\ldots,\,N\), with \(e_{ijk}\)s independent \(N(0,\,\sigma^{2})\). Find \(\mathrm{E}[Y^{\prime}(\frac{1}{n}J^{n}_{n}+M_{\alpha})Y]\) in terms of \(\mu\), the \(\alpha_{i}\)s, the \(\eta_{j}\)s, and the \(\gamma_{ij}\)s.

**Exercise 7.7.6**: For Example 7.6.1, develop a test for \(H_{0}:A_{10}+A_{11}=A_{20}+A_{21}\).

**Exercise 7.7.7**: Assuming that the reader is familiar with the analysis of two-way tables of counts and treating the \(N_{ij}\)s as a two-way table of counts, show that proportional numbers hold if and only if the \(N_{ij}\) fit the model of independence perfectly which occurs if and only if all of the odds ratios in the table equal 1.

## Bibliography

* Bailey (1953) Bailey, D. W. (1953). _The inheritance of maternal influences on the growth of the rat_. Ph.D. thesis, University of California.
* Benedetti & Brown (1978) Benedetti, J. K., & Brown, M. B. (1978). Strategies for the selection of log-linear models. _Biometrics_, _34_, 680-686.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* Fienberg (1980) Fienberg, S. E. (1980). _The analysis of cross-classified categorical data_ (2nd ed.). Cambridge: MIT Press.
* Scheffe (1959) Scheffe, H. (1959). _The analysis of variance_. New York: Wiley.
* Wermuth (1976) Wermuth, N. (1976). Model search among multiplicative models. _Biometrics_, _32_, 253-264.

## Chapter 8 Experimental Design Models

### 8.1 Abstract

In this chapter we examine three models used to analyze results obtained from three specific experimental designs. The designs are the completely randomized design, the randomized complete block design, and the Latin square design. We also examine a particularly effective method of defining treatments for situations in which several factors are of interest.

The design of an experiment is extremely important. It identifies the class of linear models that can be used to analyze the results of the experiment. Perhaps more accurately, the design identifies large classes of models that are inappropriate for the analysis of the experiment. Finding appropriate models can be difficult even in well-designed experiments.

_Randomization_, that is, the random application of treatments to experimental units, provides a philosophical justification for inferring that the treatments actually cause the experimental results. In addition to the models considered here, Appendix G examines estimation for completely randomized designs and randomized complete block designs under an alternative error structure that is derived from the act of randomization.

_Blocking_ is one of the most important concepts in experimental design. Blocking is used to reduce variability for the comparison of treatments. Blocking consists of grouping experimental units that will act similarly into blocks and then (randomly) applying the treatments to the units in each block. A _randomized complete block design_ is one in which each block has exactly the same number of units as there are treatments. Each treatment is randomly applied once in each block. If there are more units than treatments, we can have a complete block with replications. If there are fewer units than treatments, we have an _incomplete block design_. A major issue in the subject of experimental design is how one should assign treatments to blocks in an incomplete block design. Latin square designs are complete block designs that incorporate two separate forms of blocking.

The analysis of balanced incomplete block designs is derived in Section 9.4. Alternative methods for blocking designs are examined in the exercises for Section 11.1. For an excellent discussion of the concepts underlying the design of experiments, see Fisher (1935) or Cox (1958). For more detailed discussion of the design and analysis of experiments, there are many good books including Kempthorne (1952), Cochran and Cox (1957), John (1971), Hinkelmann and Kempthorne (2005, 2008), Casella (2008), Wu and Hamada (2009), Oehlert (2010), or [blush, blush] Christensen (1996, 2015). Cox and Reid (2000) take an interesting and different tack to the subject. My website contains design material deleted from _PA-V_, _ALM-III_, and Christensen (2015) plus some new material ([http://www.stat.unm.edu/~fletcher/TopicsInDesign](http://www.stat.unm.edu/~fletcher/TopicsInDesign)).

### Completely Randomized Designs

The simplest experimental design is the _completely randomized design_ (CRD). It involves no blocking. The experimental technique is simply to decide how many observations are to be taken on each treatment, obtain an adequate number of experimental units, and apply the treatments to the units randomly. The standard model for this design is

\[y_{ij}=\mu+\alpha_{i}+e_{ij},\]

\(i=1,\ldots,a,\quad j=1,\ldots,N_{i},\quad\mathrm{E}(e_{ij})=0,\quad\mathrm{Var }(e_{ij})=\sigma^{2},\quad\mathrm{Cov}(e_{ij},e_{i^{\prime}j^{\prime}})=0\) if (\(i,j\)) \(\neq\) (\(i^{\prime},j^{\prime}\)). This is a one-way ANOVA model and is analyzed as such.

### Randomized Complete Block Designs: Usual Theory

The model usually assumed for a _randomized complete block design_ (RCB) is

\[y_{ij}=\mu+\alpha_{i}+\beta_{j}+e_{ij},\]

\(i=1,\ldots,a,j=1,\ldots,b,\mathrm{E}(e_{ij})=0,\mathrm{Var}(e_{ij})=\sigma^{2},\mathrm{Cov}(e_{ij},e_{i^{\prime}j^{\prime}})=0\,\mathrm{if}\,(i,j)\neq(i^{ \prime},j^{\prime})\). The \(\beta_{j}\)s stand for an additive effect for each block; the \(\alpha_{i}\)s are an additive effect for each treatment. It is assumed that any block-treatment interaction is error so that an estimate of \(\sigma^{2}\) is available. This randomized complete block model is just a two-way ANOVA without interaction.

In this chapter, we are presenting models that are generally used for analyzing experiments conducted with certain standard experimental designs. Many people believe that these models are useful only because they are good approximations to linear models derived from the random application of the treatments to experimental units. This randomization theory leads to the conclusion that there is no valid test for block effects. On the other hand, there is nothing in the two-way ANOVA model given above to keep one from testing block effects. It is my opinion that this contradiction arises simply because the two-way ANOVA model is not very appropriate for a randomized complete block design. For example, a basic idea of blocking is that results within a block should be more alike than results in different blocks. The two-way ANOVA, with only an additive effect for blocks, is a very simplistic model. Another key idea of blocking is that it reduces the variability of treatment comparisons. It is not clear how the existence of additive block effects reduces variability in a two-way ANOVA. The question of how blocking achieves variance reduction is addressed in Exercise 8.1. Exercises 11.4-11.6 discuss alternative models for complete block designs.

Appendix G defines linear models for completely randomized designs and randomized complete block designs that are based on randomization theory. It is shown, using Theorem 10.4.5 (or Proposition 2.7.5), that least squares estimates are BLUEs for the randomization theory models.

**Exercise 8.1**: Using a randomized complete block design is supposed to reduce the variability of treatment comparisons. If the randomized complete block model is taken as

\[y_{ij}=\mu+\alpha_{i}+\beta_{j}+e_{ij},\quad e_{ij}\text{s i.i.d. }N(0,\sigma^{2}),\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), argue that the corresponding variance for a completely randomized design should be \(\sigma^{2}+\sum_{j=1}^{b}(\mu_{j}-\bar{\mu}.)^{2}/b\), where \(\mu_{i}=\mu+\beta_{j}\).

Hint: Figure out what population a completely randomized design would have to be sampled from.

### Latin Square Designs

A _Latin square_ is a design that allows for treatment effects and two different kinds of block effects. The number of treatments must equal the number of blocks of each kind. On occasion, the design is used with two kinds of treatments (each with the same number of levels) and one block effect.

_Example 8.3.1_: A \(4\times 4\) Latin square has four treatments, say, \(T_{1}\), \(T_{2}\), \(T_{3}\), \(T_{4}\). Consider the block effects as row effects, \(R_{1}\), \(R_{2}\), \(R_{3}\), and \(R_{4}\) and column effects \(C_{1}\), \(C_{2}\), \(C_{3}\), and \(C_{4}\). We can diagram one example of a \(4\times 4\) Latin square as The key idea is that each treatment occurs once in each row and once in each column.

The model for an \(a\times a\) Latin square design is

\[y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\gamma_{k}+e_{ijk},\]

E(\(e_{ijk}\)) = 0, Var(\(e_{ijk}\)) = \(\sigma^{2}\), Cov(\(e_{ijk}\), \(e_{i^{\prime}j^{\prime}k^{\prime}}\)) = 0 if (\(i\), \(j\), \(k\)) \(\neq\) (\(i^{\prime}\), \(j^{\prime}\), \(k^{\prime}\)). The correspondence between the rows, columns, and treatments and the effects in the model is: \(\alpha_{i}\) is the effect for row \(R_{i}\), \(\beta_{j}\) is the effect for column \(C_{j}\), and \(\gamma_{k}\) is the effect for treatment \(T_{k}\). _The subscripting in this model is unusual._ The key point is that in a Latin square if you know the row and the column, the Latin square design tells you what the treatment is, so the three subscripts do not vary freely. In particular, we can specify \(i=1\),..., \(a\), \(j=1\),..., \(a\), \(k\in\{1\), 2,..., \(a\}\) and \(k=f\)(\(i\), \(j\)), where for each \(i\), \(f\)(\(i\), \(j\)) is a one-to-one function of \(\{1\), \(2\),..., \(a\}\) onto itself, and the same is true for each \(j\). As in the case of the randomized complete block design, this model makes no distinction between treatment effects and the two sets of block effects.

To derive the analysis of the Latin square model, we need to show that after fitting \(\mu\), the spaces for the three main effects are orthogonal. Before proceeding, note again that the terms \(y_{ijk}\) are overindexed. There are \(a^{2}\) terms but \(a^{3}\) possible combinations of the indices. Any two of the indices serve to identify all of the observations. For example, the mean of all \(a^{2}\) observations is

\[\bar{y}_{\ldots}=\frac{1}{a^{2}}\sum_{i=1}^{a}\sum_{j=1}^{a}y_{ijk}=\frac{1}{a ^{2}}\sum_{i=1}^{a}\sum_{k=1}^{a}y_{ijk}=\frac{1}{a^{2}}\sum_{j=1}^{a}\sum_{k= 1}^{a}y_{ijk}.\]

We will use triple index notation to describe the rows of the model matrix. Write the model matrix as \(X=[X_{0}\), \(X_{1}\),..., \(X_{3a}]\), where \(X_{0}=J\),

\[X_{r} = [u_{ijk}],\hskip 14.226378ptu_{ijk}=\delta_{ir},\hskip 14.226378ptr =1,\ldots,a,\] \[X_{a+s} = [u_{ijk}],\hskip 14.226378ptu_{ijk}=\delta_{js},\hskip 14.226378pts =1,\ldots,a,\] \[X_{2a+t} = [u_{ijk}],\hskip 14.226378ptu_{ijk}=\delta_{kt},\hskip 14.226378ptt =1,\ldots,a.\]

#### Example 8.3.2

The model for the \(4\times 4\) Latin square of Example 8.3.1 is

\[\begin{bmatrix}y_{111}\\ y_{122}\\ y_{133}\\ y_{144}\\ y_{212}\\ y_{223}\\ y_{234}\\ y_{241}\\ y_{313}\\ y_{324}\\ y_{331}\\ y_{331}\\ y_{342}\\ y_{414}\\ y_{421}\\ y_{432}\\ y_{433}\\ y_{443}\\ \end{bmatrix}=\begin{bmatrix}1&1&0&0&0&1&0&0&0&0&0\\ 1&1&0&0&0&0&1&0&0&0&1&0&0\\ 1&1&0&0&0&0&0&1&0&0&0&0&1&0\\ 1&1&0&0&0&0&0&0&1&0&0&0&0&1\\ 1&0&1&0&0&1&0&0&0&0&1&0&0&0\\ 1&0&1&0&0&0&1&0&0&0&0&1&0\\ 1&0&1&0&0&0&1&0&0&0&0&1&0\\ 1&0&1&0&0&0&0&1&0&0&0&0&1\\ 1&0&1&0&0&0&0&0&1&0&0&0&1\\ 1&0&0&1&0&0&0&0&1&0&0&0&0\\ 1&0&0&1&0&0&1&0&0&1&0&0&0\\ 1&0&0&0&1&0&0&0&1&0&0&1&0\\ 1&0&0&0&1&0&0&0&1&0&0&1&0\\ \end{bmatrix}\begin{bmatrix}\mu\\ \alpha_{1}\\ \alpha_{2}\\ \alpha_{3}\\ \alpha_{4}\\ \beta_{1}\\ \beta_{2}\\ \beta_{3}\\ \beta_{4}\\ \gamma_{1}\\ \gamma_{2}\\ \gamma_{3}\\ \gamma_{4}\\ \end{bmatrix}+e.\]

Orthogonalizing columns \(1\) to \(3a\) of \(X\) with respect to \(J\) gives the matrix \(Z\) with columns

\[Z_{0} =X_{0}\] \[Z_{i} =X_{i}-\frac{X_{i}^{\prime}J}{J^{\prime}J}J=X_{i}-\frac{a}{a^{2}} J,\quad i=1,\ldots,3a.\]

The three spaces \(C(Z_{1},\ldots,Z_{a})\), \(C(Z_{a+1},\ldots,Z_{2a})\), and \(C(Z_{2a+1},\ldots,Z_{3a})\) are orthogonal. For example, with \(r=1,\ldots,a\) and \(t=1,\ldots,a\),

\[Z_{r}^{\prime}Z_{2a+t} =\sum_{i=1}^{a}\sum_{k=1}^{a}\left(\delta_{ir}-\frac{1}{a} \right)\left(\delta_{kt}-\frac{1}{a}\right)\] \[=\sum_{i=1}^{a}\sum_{k=1}^{a}\delta_{ir}\delta_{kt}-\sum_{i=1}^{a }\sum_{k=1}^{a}\delta_{ir}/a-\sum_{i=1}^{a}\sum_{k=1}^{a}\delta_{kt}/a+\sum_ {i=1}^{a}\sum_{k=1}^{a}1/a^{2}\] \[=1-1-1+1\] \[=0.\]

Similar computations establish the other orthogonality relationships.

Because of the orthogonality, the sum of squares for dropping, say, the \(\alpha_{i}\)s from the model is just the sum of squares for dropping the \(\alpha_{i}\)s from a one-way ANOVA model that ignores the \(\beta_{j}\) and \(\gamma_{k}\) effects. The ANOVA table isEstimation and testing in one of the treatment (block) spaces depends only on the appropriate projection operator. Since we have the usual one-way ANOVA projection operators, estimation and testing are performed in the usual way.

The Latin square model assumes that the (\(\alpha\beta\)), (\(\alpha\gamma\)), (\(\beta\gamma\)), and (\(\alpha\beta\gamma\)) interactions are nonexistent. This assumption is necessary in order to obtain an estimate of error. If an outside estimate of \(\sigma^{2}\) is available, it might be hoped that the interactions could be examined. Unfortunately, it is impossible to tell from which interaction the degrees of freedom called "error" come from. For example, in the 4 \(\times\) 4 Latin square of the example, the (\(\alpha\beta\)) interaction can be broken up into 3 degrees of freedom for \(\gamma\) and 6 degrees of freedom for error. Since a similar result holds for each of the interactions, the 6 degrees of freedom for error involve all of the interactions.

#### Exercise 8.2

In the 4 \(\times\) 4 Latin square of the examples, show that the 9 degrees of freedom for (\(\alpha\beta\)) interaction are being divided into 3 degrees of freedom for \(\gamma\) and 6 degrees of freedom for error.

A _Graeco-Latin square_ is a Latin square in which a second group of \(a\) treatments has been applied so that each treatment in the second group occurs once in each row, once in each column, and once with each of the treatments from the first group.

#### Exercise 8.3

Derive the analysis for the Graeco-Latin square given below. Use the model \(y_{hijk}=\mu+\alpha_{h}+\beta_{i}+\gamma_{j}+\eta_{k}+e_{hijk}\).

\begin{tabular}{c|c c c c c}  & \(C_{1}\) & \(C_{2}\) & \(C_{3}\) & \(C_{4}\) & \(C_{5}\) \\ \hline \(R_{1}\) & \(T_{1}\tau_{1}\) & \(T_{2}\tau_{3}\) & \(T_{3}\tau_{5}\) & \(T_{4}\tau_{2}\) & \(T_{5}\tau_{4}\) \\ \(R_{2}\) & \(T_{2}\tau_{2}\) & \(T_{3}\tau_{4}\) & \(T_{4}\tau_{1}\) & \(T_{5}\tau_{3}\) & \(T_{1}\tau_{5}\) \\ \(R_{3}\) & \(T_{3}\tau_{3}\) & \(T_{4}\tau_{5}\) & \(T_{5}\tau_{2}\) & \(T_{1}\tau_{4}\) & \(T_{2}\tau_{1}\) \\ \(R_{4}\) & \(T_{4}\tau_{4}\) & \(T_{5}\tau_{1}\) & \(T_{1}\tau_{3}\) & \(T_{2}\tau_{5}\) & \(T_{3}\tau_{2}\) \\ \(R_{5}\) & \(T_{5}\tau_{5}\) & \(T_{1}\tau_{2}\) & \(T_{2}\tau_{4}\) & \(T_{3}\tau_{1}\) & \(T_{4}\tau_{3}\) \\ \end{tabular}

Extend the analysis to an arbitrary \(a\times a\) Graeco-Latin square.

### Factorial Treatment Structures

For each experimental design considered in this chapter, we have assumed the existence of "\(a\)" treatments. Sometimes the treatments are chosen in such a way that the treatment space can be conveniently broken into orthogonal subspaces. One of the most common methods of doing this is to choose treatments with factorial structure.

Suppose that two or more different kinds of treatments are of interest. Each kind of treatment is called a _factor_. Each factor is of interest at some number of different levels. A very efficient way of gaining information on all of the levels of all of the factors is to use what is called a _factorial design_. In a factorial design the treatments are taken to be all possible combinations of the levels of the different factors. Since a factorial design refers only to the treatment structure, factorial designs can be used with all of the designs considered in this chapter as well as with balanced incomplete block designs (cf. Section 9.4) and split plot designs (cf. Section 11.3).

#### _Example 8.4.1_

An experiment is to be conducted examining the effects of fertilizer on potato yields. Of interest are two kinds of fertilizer, a nitrogen-based fertilizer and a phosphate-based fertilizer. The two types of fertilizer are factors. The nitrogen fertilizer is to be examined at two levels: no nitrogen fertilizer (\(n_{0}\)) and a single dose of nitrogen fertilizer (\(n_{1}\)). The phosphate fertilizer has three levels: no phosphate fertilizer (\(p_{0}\)), a single dose of phosphate (\(p_{1}\)), and a double dose of phosphate (\(p_{2}\)). The treatments are taken to be all six of the possible combinations:

\[n_{0}p_{0}\quad n_{0}p_{1}\quad n_{0}p_{2}\quad n_{1}p_{0}\quad n_{1}p_{1}\quad n _{1}p_{2}.\]

The use of treatments with factorial structure has a number of advantages. One is that it allows study of the interrelationships (interactions) between the factors. In Example 8.4.1, it is possible to examine whether the levels of phosphate have different effects depending on whether or not nitrogen was applied. Another advantage is that if there are no interactions, the experimental material is used very efficiently. Suppose that there is no interaction between nitrogen and phosphate. The effect of nitrogen is the difference in yields between experimental units that have the same level of phosphate but different levels of nitrogen, that is,

\[n_{0}p_{0}-n_{1}p_{0},\]

\[n_{0}p_{1}-n_{1}p_{1},\]

and

\[n_{0}p_{2}-n_{1}p_{2}.\]

In each case, the only difference in the pair of treatments is the difference in nitrogen. At the same time, the difference in, say, the effects of \(p_{1}\) and \(p_{2}\) can be examined by looking at the differences\[n_{0}p_{1}-n_{0}p_{2}\]

and

\[n_{1}p_{1}-n_{1}p_{2}.\]

The same treatments are used to obtain estimates of both the nitrogen effect and the phosphate effect.

We now examine the analysis of designs having factorial treatment structure. Consider a randomized complete block design with \(c\) blocks, where the treatments are all combinations of two factors, say \(A\) and \(B\). Suppose factor \(A\) has \(a\) levels and factor \(B\) has \(b\) levels. The total number of treatments is \(ab\). Rewriting the model of Section 2 with \(\tau_{h}\) denoting treatment effects and \(\xi_{k}\) denoting block effects, we get

\[y_{hk}=\mu+\tau_{h}+\xi_{k}+e_{hk}, \tag{1}\]

\(h=1\),..., \(ab\), \(k=1\),..., \(c\). In this model, each treatment is indicated by an index \(h\). Since the treatments consist of all combinations of two factors, it makes sense to use two indices to identify treatments: one index for each factor. With this idea we can rewrite model (1) as

\[y_{ijk}=\mu+\tau_{ij}+\xi_{k}+e_{ijk}, \tag{2}\]

\(i=1\),..., \(a\), \(j=1\),..., \(b\), \(k=1\),..., \(c\).

**Exercise 8.4**  For model (1), \(C(M_{\tau})\) is given by Proposition 4.2.3. What is \(C(M_{\tau})\) in the notation of model (2)?

The orthogonal breakdown of the treatment space follows from a reparameterization of model (2). Model (2) can be rewritten as

\[y_{ijk}=\mu+\alpha_{i}+\beta_{j}+(\alpha\beta)_{ij}+\xi_{k}+e_{ijk}. \tag{3}\]

The new parameterization is simply \(\tau_{ij}=\alpha_{i}+\beta_{j}+(\alpha\beta)_{ij}\). Using the ideas of Sections 7.1 and 7.2, the treatment space \(C(M_{\tau})\) can be broken up into three orthogonal subspaces: one for factor \(A\), \(C(M_{\alpha})\), one for factor \(B\), \(C(M_{\beta})\), and one for interaction, \(C(M_{\alpha\beta})\). The analysis of model (3) follows along the lines of Chapter 7. Model (3) is just a balanced three-way ANOVA in which some of the interactions (namely, any interactions that involve blocks) have been thrown into the error.

In practice, it is particularly important to be able to relate contrasts in the treatments (\(\tau_{ij}\)s) to contrasts in the main effects (\(\alpha_{i}\)s and \(\beta_{j}\)s) and contrasts in the interactions ((\(\alpha\beta\))\({}_{ij}\)s). The relationship is demonstrated in Exercise 8.5. The relationship is illustrated in the following example.

_Example 8.4.2_ Consider the treatments of Example 8.4.1. There is one contrast in nitrogen,

The two orthogonal polynomial contrasts in phosphate are:

\[\begin{array}{l|rrr}&p_{0}&p_{1}&p_{2}\\ \hline P\ \mathrm{linear}&-1&0&1\\ P\ \mathrm{quadratic}&1&-2&1\\ \end{array}\]

The main effect contrasts define two orthogonal interaction contrasts: \(N-P\) linear

\[\begin{array}{l|rrr}&p_{0}&p_{1}&p_{2}\\ \hline n_{0}&-1&0&1\\ n_{1}&1&0&-1\\ \end{array}\]

and \(N-P\) quadratic

\[\begin{array}{l|rrr}&p_{0}&p_{1}&p_{2}\\ \hline n_{0}&1&-2&1\\ n_{1}&-1&2&-1\\ \end{array}\]

The corresponding contrasts in the six treatments are:

\[\begin{array}{l|rrr}&n_{0}p_{0}&n_{0}p_{1}&n_{0}p_{2}&n_{1}p_{0}&n_{1}p_{1} &n_{1}p_{2}\\ \hline N&1&1&-1&-1&-1\\ P\ \mathrm{linear}&-1&0&1&-1&0&1\\ P\ \mathrm{quadratic}&1&-2&1&1&-2&1\\ N\ -P\ \mathrm{linear}&-1&0&1&1&0&-1\\ N\ -P\ \mathrm{quadratic}&1&-2&1&-1&2&-1\\ \end{array}\]

It is easily verified that these five contrasts are orthogonal.

**Exercise 8.5** Show that the contrasts in the \(\tau_{ij}\)s corresponding to the contrasts \(\sum\lambda_{i}\alpha_{i}\) and \(\sum\sum\lambda_{i}\eta_{j}(\alpha\beta)_{ij}\) are \(\sum\sum\lambda_{i}\tau_{ij}\) and \(\sum\sum\lambda_{i}\eta_{j}\tau_{ij}\), respectively.

Hint: Any contrast in the \(\tau_{ij}\)s corresponds to a vector in \(C(M_{\tau})\), just as any contrast in the \(\alpha_{i}\)s corresponds to a vector in \(C(M_{\alpha})\subset C(M_{\tau})\). Recall that contrasts are only defined up to constant multiples; and that contrasts in the \(\alpha_{i}\)s also involve the interactions when interaction exists.

### More on Factorial Treatment Structures

We now present a more theoretical discussion of factorial treatment structures and some interesting new models.

For two factors, say \(\alpha\) at \(s\) levels and \(\eta\) at \(t\) levels, there are a total of \(p\equiv st\) treatment groups. Start by considering a linear model for the data \(y_{ijk}=(\alpha\eta)_{ij}+e_{ijk}\). In matrix form, the linear model \(Y=X\beta+e\) has \(X\) as the indicator matrix used for a one-way ANOVA (cell means) model and \(\beta=[(\alpha\eta)_{11},\ldots,(\alpha\eta)_{st}]^{\prime}\) containing a separate effect for every combination of the two factors.

More generally, if we have \(r\) factors \(\alpha_{i}\) with \(s_{i}\) levels respectively, the factorial structure defines \(p=\prod_{i=1}^{r}s_{i}\) groups. We again take \(X\) to be the one-way ANOVA indicator matrix and define \(\beta=[\alpha_{i_{1},\ldots,i_{r}}]\) as a vector providing a separate effect for each combination of the factors.

The idea is that factorial models are defined by specifying a linear structure for the group parameters. This consists of putting a linear constraint on \(\beta\), say \(\beta=U_{p\times q}\gamma\) for some known matrix \(U\), cf. Section 3.3. For example, in the two-factor model, one such linear constraint is to force an additive main effects model for the data, \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\). In matrix terms using Kronecker products, this amounts to specifying that

\[\beta=\begin{pmatrix}[J_{s}\otimes J_{t}],[I_{s}\otimes J_{t}],[J_{s}\otimes I _{t}]\end{pmatrix}\begin{bmatrix}\mu\\ \alpha\\ \eta\end{bmatrix}\equiv U\gamma, \tag{1}\]

where \(\alpha=(\alpha_{1},\ldots,\alpha_{s})^{\prime}\) and \(\eta=(\eta_{1},\ldots,\eta_{t})^{\prime}\). As in Section 3.3, the linear model for the data has been transformed to \(Y=XU\gamma+e\), where the model matrix is now \(XU\), which is a reduced model relative to the original, i.e., \(C(XU)\subset C(X)\). Note that if we define a linear structure for the parameters, \(\beta=U\gamma\) and a reduced structure, i.e., \(\beta=U_{0}\gamma_{0}\), where \(C(U_{0})\subset C(U)\), then this also defines a reduced linear model for the data in that \(C(XU_{0})\subset C(XU)\).

I have no difficulty considering all such models to be factorial models, but McCullagh (2000) proposes a more stringent definition involving "selection invariance." The idea is that if you drop various indices from various factors, the model should somehow remain invariant. This idea can be executed in the following way: Begin with a linear structure \(\beta=U\gamma\) but partition \(U\) into sets of columns \(U=[U_{0},\,U_{1},\ldots,U_{m}]\) and then specify that

\[U_{i}=[V_{i1}\otimes V_{i2}\otimes\cdots\otimes V_{ir}]. \tag{2}\]

Such structures should be sufficient to satisfy the basic idea of selection invariance. However, as will be seen later, other interesting selection invariant factorial models require us to consider matrices \(U_{i}\) that are linear combinations of matrices with the structure (2).

An interesting aspect of factorial structures is dealing with factors that have the same levels. Such factors are called _homologous_. Example 7.5.1 involves genotypes of mothers and genotypes of litters, but the genotypes are identical for the mothers and the litters, so it provides an example of homologous factors. In fact, Christensen (2015, Section 14.4) uses that data to illustrate fitting the homologous factor models discussed below.

In the additive two-factor model \(y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk}\) with \(s=t\) and homologous factors, we might consider situations such as _symmetric additive effects_ where \(y_{ijk}=\mu+\alpha_{i}+\alpha_{j}+e_{ijk}\) or _alternating (skew symmetric) additive effects_ where \(y_{ijk}=\mu+\alpha_{i}-\alpha_{j}+e_{ijk}\). As illustrated in (1), we can write the additive model for the parameters in matrix form as

\[\beta=U\gamma=[J_{st},U_{1},U_{2}]\begin{bmatrix}\mu\\ \alpha\\ \eta\end{bmatrix}.\]

We can now specify symmetric additive effects by specifying \(\alpha=\eta\) to get

\[\beta=[J_{st},U_{1},U_{2}]\begin{bmatrix}\mu\\ \alpha\\ \alpha\end{bmatrix}=[J_{st},(U_{1}+U_{2})]\begin{bmatrix}\mu\\ \alpha\\ \alpha\end{bmatrix},\]

thus defining the linear model \(Y=X[J_{st},(U_{1}+U_{2})]\begin{bmatrix}\mu\\ \alpha\end{bmatrix}+e\). Similarly, specifying alternating additive effects \(\alpha=-\eta\) leads to \(Y=X[J_{st},(U_{1}-U_{2})]\begin{bmatrix}\mu\\ \alpha\end{bmatrix}+e\). In a \(3\times 3\) example with \(\beta=[(\alpha\eta)_{11},(\alpha\eta)_{12},\ldots,(\alpha\eta)_{33}]^{\prime}\), the additive main effects model has

\[U=[J_{st},U_{1},U_{2}]=\begin{bmatrix}1&1&0&0&1&0&0\\ 1&1&0&0&0&1&0\\ 1&1&0&0&0&0&1\\ 1&0&1&0&1&0&0\\ 1&0&1&0&0&1&0\\ 1&0&1&0&0&0&1\\ 1&0&0&1&1&0&0\\ 1&0&0&1&0&1&0\\ 1&0&0&1&0&1&0\\ 1&0&0&1&0&0&1\end{bmatrix}.\]

The symmetric additive effects and alternating additive effects models have \[[J_{st},U_{1}+U_{2}]=\begin{bmatrix}1&2&0&0\\ 1&1&1&0\\ 1&1&0&1\\ 1&1&1&0\\ 1&0&2&0\\ 1&0&1&1\\ 1&1&0&1\\ 1&0&1&1\\ 1&0&0&2\end{bmatrix},\quad[J_{st},(U_{1}-U_{2})]=\begin{bmatrix}1&0&0&0\\ 1&1&-1&0\\ 1&1&0&-1\\ 1&-1&1&0\\ 1&0&0&0\\ 1&0&1&-1\\ 1&-1&0&1\\ 1&0&-1&1\\ 1&0&0&0\end{bmatrix},\]

respectively. Given the simple structure of the original one-way ANOVA matrix \(X\), the reduced model matrices \(X[J_{st},(U_{1}+U_{2})]\) and \(X[J_{st},(U_{1}-U_{2})]\) have structures very similar to \([J_{st},(U_{1}+U_{2})]\) and \([J_{st},(U_{1}-U_{2})]\). However, these linear structures for the parameters are not of the form (2), hence the need to consider linear combinations of terms like those in (2).

Models specifying such things as simple symmetry \((\alpha\eta)_{ij}=(\alpha\eta)_{ji}\) can also be specified quite easily by defining an appropriate \(U\) matrix, e.g.,

\[\begin{bmatrix}(\alpha\eta)_{11}\\ (\alpha\eta)_{12}\\ (\alpha\eta)_{13}\\ (\alpha\eta)_{21}\\ (\alpha\eta)_{22}\\ (\alpha\eta)_{23}\\ (\alpha\eta)_{31}\\ (\alpha\eta)_{32}\\ (\alpha\eta)_{33}\end{bmatrix}=\beta=U\phi=\begin{bmatrix}1&0&0&0&0&0\\ 0&1&0&0&0&0\\ 0&1&0&0&0&0\\ 0&0&0&1&0&0\\ 0&0&0&0&1&0\\ 0&0&1&0&0&0\\ 0&0&0&0&1&0\\ 0&0&0&0&1&0\\ 0&0&0&0&0&1\end{bmatrix}\begin{bmatrix}\phi_{11}\\ \phi_{12}\\ \phi_{13}\\ \phi_{22}\\ \phi_{23}\\ \phi_{33}\end{bmatrix}.\]

These also fit into the class of linear combinations of matrices with the pattern (2), e.g., the column of \(U\) associated with \(\phi_{23}\) can be written as

\[\left(\begin{bmatrix}0\\ 1\\ 0\end{bmatrix}\otimes\begin{bmatrix}0\\ 0\\ 1\end{bmatrix}\right)+\left(\begin{bmatrix}0\\ 0\\ 1\end{bmatrix}\otimes\begin{bmatrix}0\\ 1\\ 0\end{bmatrix}\right).\]

These ideas also apply to generalized linear models. For example, in log-linear models, symmetry is sometimes an interesting model, cf. Christensen (1997, Exercise 2.7.10), and the symmetric additive effects model is a (not the) model of marginal homogeneity, cf. Christensen (1997, Exercise 10.8.6). See McCullagh (2000) for a more extensive and theoretical treatment of these ideas.

### Additional Exercises

#### Exercise 8.6.1

A study was performed to examine the effect of two factors on increasing muscle mass in weight lifters. The first factor was dietary protein level. The levels were use of a relatively low protein diet (_L_) and use of a high protein diet (_H_). The second factor was the use of anabolic steroids. The first level consisted of no steroid use (_N_) and the second level involved the use of steroids (_S_). Subjects were chosen so that one subject was in each combination of four height groups (_i_) and four weight groups (_j_). Treatments are identified as _LN_, _LS_, _HN_, and _HS_. The dependent variable is a measure of increase in muscle mass during the treatment period. The study was replicated in two different years. The height groups and weight groups changed from the first year to the second year. The design and data are listed below. Heights are the columns of the squares and weights are the rows. Analyze the data.

#### Exercise 8.6.2

Show that the set of indices \(i=1,\ldots,a,\)\(j=1,\ldots,a,\) and \(k=(i+j+a-1)\operatorname{mod}(a)\) determines a Latin square design.

Hint: Recall that \(t\operatorname{mod}(a)\) means \(t\) modulo \(a\) and is defined as the remainder when \(t\) is divided by \(a\).

## References

* [CasellaCasella2008] Casella, G. 2008. Statistical design. New York: Springer.
* [ChristensenChristensen1996] Christensen, R. 1996. Analysis of variance, design, and regression: Applied statistical methods. London: Chapman and Hall.
* [ChristensenChristensen1997] Christensen, R. 1997. Log-linear models and logistic regression (2nd ed.). New York: Springer.
* [ChristensenChristensen1998]* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* Cochran & Cox (1957) Cochran, W. G., & Cox, G. M. (1957). _Experimental designs_ (2nd ed.). New York: Wiley.
* Cox (1958) Cox, D. R. (1958). _Planning of experiments_. New York: Wiley.
* Cox & Reid (2000) Cox, D. R., & Reid, N. (2000). _The theory of the design of experiments_. Boca Raton: Chapman and Hall/CRC.
* Fisher (1935) Fisher, R. A. (1935). _The design of experiments_, (9th ed., 1971). New York: Hafner Press.
* Hinkelmann & Kempthorne (2005) Hinkelmann, K., & Kempthorne, O. (2005). _Design and analysis of experiments: Volume 2, advanced experimental design_. Hoboken: Wiley.
* Hinkelmann & Kempthorne (2008) Hinkelmann, K., & Kempthorne, O. (2008). _Design and analysis of experiments: Volume 1, introduction to experimental design_ (2nd ed.). Hoboken: Wiley.
* John (1971) John, P. W. M. (1971). _Statistical design and analysis of experiments_. New York: Macmillan.
* Kempthorne (1952) Kempthorne, O. (1952). _Design and analysis of experiments_. Huntington: Krieger.
* McCullagh (2000) McCullagh, P. (2000). Invariance and factorial models, with discussion. _Journal of the Royal Statistical Society, Series B_, 62, 209-238.
* Oehlert (2010) Oehlert, G. W. (2010). _A first course in design and analysis of experiments_. [http://users.stat.umn.edu/~gary/book/fcdae.pdf](http://users.stat.umn.edu/~gary/book/fcdae.pdf).
* Wu & Hamada (2009) Wu, C. F. J., & Hamada, M. S. (2009). _Experiments: Planning, analysis, and optimization_ (2nd ed.). New York: Wiley.

## Chapter 9 Analysis of Covariance

This chapter examines the analysis of partition models, also known as analysis of covariance, a method traditionally used for improving the analysis of designed experiments. Sections 1 and 2 present the theory of estimation and testing for general partitioned models. Sections 3 and 4 present nontraditional applications of the theory. Section 3 applies the partitioned model results to the problem of fixing up balanced ANOVA problems that have lost their balance due to the existence of some missing data. Although applying analysis of covariance to missing data problems is not a traditional experimental design application, it is an application that was used for quite some time until computational improvements made it largely unnecessary. Section 4 uses the analysis of covariance results to derive the analysis for balanced incomplete block designs. Section 5 presents Milliken and Graybill's (1970) test of a linear model versus a nonlinear alternative. I personally find the techniques of Sections 1 and 2 to be some of the most valuable tools available for deriving results in linear model theory.

Traditionally, analysis of covariance (ACOVA) has been used as a tool in the analysis of designed experiments. Suppose one or more measurements are made on a group of experimental units. In an agricultural experiment, such a measurement might be the amount of nitrogen in each plot of ground prior to the application of any treatments. In animal husbandry, the measurements might be the height and weight of animals before treatments are applied. One way to use such information is to create blocks of experimental units that have similar values of the measurements. Analysis of covariance uses a different approach. In analysis of covariance, an experimental design is chosen that does not depend on these supplemental observations. The concomitant observations come into play as regression variables that are added to the basic experimental design model.

The goal of analysis of covariance is the same as the goal of blocking. The regression variables are used to _reduce the variability of treatment comparisons_. In this traditional context, comparisons among treatments remain the primary goal of the analysis. Exercises 9.1 and 9.5 are important practical illustrations of how this isaccomplished. Snedecor and Cochran (1980, Chapter 18) discuss the practical uses of analysis of covariance. Cox (1958, Chapter 4) discusses the proper role of concomitant observations in experimental design. _Biometrics_ has devoted two entire issues to analysis of covariance: Volume 13, Number 3, 1957 and Volume 38, Number 3, 1982.

From a theoretical point of view, analysis of covariance involves the analysis of a model with a _partitioned model_ matrix, say

\[Y = [X,Z]\left[ {\begin{array}{*{20}c} \beta \\ \gamma \\ \end{array} } \right] + e,\]

where \(X\) is an \(n\times p\) matrix, \(Z\) is an \(n\times s\) matrix, E(\(e\)) = 0, and Cov(\(e\)) = \(\sigma^{2}I\). Analysis of covariance is a technique for analyzing model (1) based on the analysis of the reduced model

\[Y = X\delta + e.\]

The point is that model (2) should be a model whose analysis is relatively easy. In traditional applications, \(X\) is taken as the model matrix for a balanced analysis of variance. The \(Z\) matrix can be anything, but traditionally consists of columns of regression variables. In essence we generalize and deepen the partitioning ideas used in Subsection 6.2.1.

The practical application of general linear model theory is prohibitively difficult without a computer program to perform the worst of the calculations. There are, however, special cases: notably, simple linear regression, one-way ANOVA, and balanced multifactor ANOVA, in which the calculations are not prohibitive. Analysis of covariance allows computations of the BLUEs and the _SSE_ for model (1) by performing several analyses on tractable special cases plus finding the generalized inverse of an \(s\times s\) matrix. Since finding the generalized inverse of anything bigger than, say, a 3 \(\times\) 3 matrix is difficult for hand calculations, one would typically not want more than three columns in the \(Z\) matrix for such purposes.

As mentioned earlier, in the traditional application of performing an ANOVA while adjusting for the effect of some regression variables, the primary interest is in the ANOVA. The regression variables are there only to sharpen the analysis. The inference on the ANOVA part of the model is performed after fitting the regression variables. To test whether the regression variables really help to sharpen the analysis, they should be tested after fitting the ANOVA portion of the model. The basic computation for performing these tests is finding the _SSE_ for model (1). This implicitly provides a method for finding the _SSE_ for submodels of model (1). Appropriate tests are performed by comparing the _SSE_ for model (1) to the _SSE_s of the various submodels.

### Estimation of Fixed Effects

To obtain least squares estimates, we break the estimation space of model (9.0.1) into two orthogonal parts. As usual, let \(M\) be the perpendicular projection operator onto \(C(X)\). Note that \(C(X,Z)=C[X,(I-M)Z]\). One way to see this is that from model (9.0.1)\[\mathrm{E}(Y)=X\beta+Z\gamma=X\beta+MZ\gamma+(I-M)Z\gamma=[X,MZ]\begin{bmatrix} \beta\\ \gamma\end{bmatrix}+(I-M)Z\gamma.\]

Since \(C(X)=C([X,MZ])\), clearly model (9.0.1) holds if and only if \(\mathrm{E}(Y)\in C[X,(I-M)Z]\).

Let \(\mathcal{P}\) denote the perpendicular projection matrix onto \(C([X,Z])=C([X,(I-M)Z])\). Since the two sets of column vectors in \([X,(I-M)Z]\) are orthogonal, the perpendicular projection matrix for the entire space is the sum of the perpendicular projection matrices for the subspaces \(C(X)\) and \(C[(I-M)Z]\), cf. Theorem B.45. Thus,

\[\mathcal{P}=M+(I-M)Z\begin{bmatrix}Z^{\prime}(I-M)Z\end{bmatrix}^{-}Z^{\prime }(I-M)\]

and write

\[M_{2}\equiv(I-M)Z\begin{bmatrix}Z^{\prime}(I-M)Z\end{bmatrix}^{-}Z^{\prime}(I -M).\]

Least squares estimates satisfy \(X\hat{\beta}+Z\hat{\gamma}=\mathcal{P}Y=MY+M_{2}Y\).

We now consider estimation of estimable functions of \(\gamma\) and \(\beta\). The formulae are simpler if we incorporate the estimate

\[\hat{\gamma}\equiv\begin{bmatrix}Z^{\prime}(I-M)Z\end{bmatrix}^{-}Z^{\prime} (I-M)Y, \tag{1}\]

which we will later show to be a least squares estimate.

First consider an estimable function of \(\gamma\), say, \(\xi^{\prime}\gamma\). For this to be estimable, there exists a vector \(\rho\) such that \(\xi^{\prime}\gamma=\rho^{\prime}[X\beta+Z\gamma]\). For this equality to hold for all \(\beta\) and \(\gamma\), we must have \(\rho^{\prime}X=0\) and \(\rho^{\prime}Z=\xi^{\prime}\). The least squares estimate of \(\xi^{\prime}\gamma\) is

\[\rho^{\prime}\mathcal{P}Y =\rho^{\prime}\left\{M+M_{2}\right\}Y\] \[=\rho^{\prime}MY+\rho^{\prime}(I-M)Z\begin{bmatrix}Z^{\prime}(I- M)Z\end{bmatrix}^{-}Z^{\prime}(I-M)Y\] \[=0+\rho^{\prime}Z\hat{\gamma}\] \[=\xi^{\prime}\hat{\gamma}.\]

The penultimate equality stems from the fact that \(\rho^{\prime}X=0\) implies \(\rho^{\prime}M=0\).

An arbitrary estimable function of \(\beta\), say, \(\lambda^{\prime}\beta\), has \(\lambda^{\prime}\beta=\rho^{\prime}[X\beta+Z\gamma]\) for some \(\rho\). For this equality to hold for all \(\beta\) and \(\gamma\), we must have \(\rho^{\prime}X=\lambda^{\prime}\) and \(\rho^{\prime}Z=0\). As a result, the least squares estimate is

\[\rho^{\prime}\mathcal{P}Y =\rho^{\prime}\left\{M+M_{2}\right\}Y\] \[=\rho^{\prime}MY+\rho^{\prime}(I-M)Z\begin{bmatrix}Z^{\prime}(I- M)Z\end{bmatrix}^{-}Z^{\prime}(I-M)Y\] \[=\rho^{\prime}MY+\rho^{\prime}(I-M)Z\hat{\gamma}\] \[=\rho^{\prime}MY-\rho^{\prime}MZ\hat{\gamma}\] \[=\rho^{\prime}M(Y-Z\hat{\gamma})\] \[\equiv\lambda^{\prime}\hat{\beta}.\]Define

\[X\hat{\beta}\equiv M(Y-Z\hat{\gamma}). \tag{2}\]

We now establish that

\[X\hat{\beta}+Z\hat{\gamma}=\mathcal{P}Y\]

so that \(\hat{\gamma}\) is a least squares estimate of \(\gamma\) and and \(X\hat{\beta}\) is a least squares estimate of \(X\beta\). Write

\[X\hat{\beta}+Z\hat{\gamma} =M(Y-Z\hat{\gamma})+Z\hat{\gamma}\] \[=MY+(I-M)Z\hat{\gamma}\] \[=MY+M_{2}Y=\mathcal{P}Y.\]

Often in ACOVA, the \(X\) matrix comes from a model that is simple to analyze, like one-way ANOVA or a balanced multifactor ANOVA. If the model \(Y=X\beta+e\) has simple formula for computing an estimate of some function \(\lambda^{\prime}\beta=\rho^{\prime}X\beta\), say a contrast, then that simple formula must be incorporated into \(\rho^{\prime}MY\). Under conditions that we will explore, \(\lambda^{\prime}\beta\) is also an estimable function under the ACOVA model \(Y=X\beta+Z\gamma+e\) and, with the same vector \(\rho\), the estimate is \(\lambda^{\prime}\hat{\beta}=\rho^{\prime}M(Y-Z\hat{\gamma})\). That means that the same (simple) computational procedure that was applied to the data \(Y\) in order to estimate \(\lambda^{\prime}\beta\) in \(Y=X\beta+e\) can also be applied to \(Y-Z\hat{\gamma}\) to estimate \(\lambda^{\prime}\beta\) in \(Y=X\beta+Z\gamma+e\), see Exercise 9.1. We now explore the conditions necessary to make this happen, along with other issues related to estimability in ACOVA models.

Often \(Z\) consists of columns of regression variables, in which case \(Z^{\prime}(I-M)Z\) is typically nonsingular. In this _nonsingular case_, both \(\gamma\) and \(X\beta\) are estimable. In particular,

\[\gamma=\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I-M)[X\beta+Z\gamma]\]

with estimate

\[\hat{\gamma} =\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I-M)\mathcal{P}Y\] \[=\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I-M)[M+M_{2}]Y\] \[=\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I-M)M_{2}Y\] \[=\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I-M)Y.\]

\(X\) is traditionally the model matrix for an ANOVA model, so \(\beta\) is usually not estimable. However, when \(Z^{\prime}(I-M)Z\) is nonsingular, \(X\beta\) is estimable in the ACOVA model. Observe that

\[\left\{I-Z\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I-M) \right\}\left[X\beta+Z\gamma\right]\] \[=X\beta+Z\gamma-Z\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I- M)Z\gamma\] \[=X\beta+Z\gamma-Z\gamma\] \[=X\beta.\]Thus, in the nonsingular case, anything that is estimable in \(Y=X\beta+e\) is also estimable in the ACOVA model. In particular, if \(\lambda^{\prime}=\rho^{\prime}X\), the estimate of \(\lambda^{\prime}\beta\) in \(Y=X\beta+e\) is \(\rho^{\prime}MY\). Clearly, for the ACOVA model,

\[\rho^{\prime}\left\{I-Z\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}(I-M) \right\}\left[X\beta+Z\gamma\right]=\rho^{\prime}X\beta=\lambda^{\prime}\beta\]

and the estimate is

\[\lambda^{\prime}\hat{\beta}=\rho^{\prime}\left\{I-Z\left[Z^{\prime}(I-M)Z \right]^{-1}Z^{\prime}(I-M)\right\}\mathcal{P}Y.\]

We now show that \(\lambda^{\prime}\hat{\beta}=\rho^{\prime}M(Y-Z\hat{\gamma})\). As mentioned earlier, the beauty of this result is that if we know how to estimate \(\lambda^{\prime}\beta\) in \(Y=X\beta+e\) using \(Y\), exactly the same method applied to \(Y-Z\hat{\gamma}\) will give the estimate in the ACOVA model.

As discussed earlier, an estimable function \(\lambda^{\prime}\beta\), has \(\lambda^{\prime}\beta=\tilde{\rho}^{\prime}[X\beta+Z\gamma]\) for some \(\tilde{\rho}\) with \(\tilde{\rho}^{\prime}X=\lambda^{\prime}\) and \(\tilde{\rho}^{\prime}Z=0\). Also as before, the least squares estimate is

\[\tilde{\rho}^{\prime}\mathcal{P}Y=\tilde{\rho}^{\prime}M(Y-Z\hat{\gamma}).\]

In the nonsingular case, if \(\rho\) is any vector that has \(\rho^{\prime}X=\lambda^{\prime}\), we can turn it into a vector \(\tilde{\rho}\) that has both \(\tilde{\rho}^{\prime}X=\lambda^{\prime}\) and \(\tilde{\rho}^{\prime}Z=0\), simply by defining

\[\tilde{\rho}^{\prime}=\rho^{\prime}\left\{I-Z\left[Z^{\prime}(I-M)Z\right]^{- 1}Z^{\prime}(I-M)\right\}.\]

Moreover,

\[\tilde{\rho}^{\prime}M(Y-Z\hat{\gamma})=\rho^{\prime}M(Y-Z\hat{\gamma}),\]

so the same estimation procedure applied to \(Y\) in \(Y=X\beta+e\) gets applied to \(Y-Z\hat{\gamma}\) in \(Y=X\beta+Z\gamma+e\) when estimating the estimable function \(\lambda^{\prime}\beta\).

In general, if \(\left[Z^{\prime}(I-M)Z\right]\) is singular, neither \(\gamma\) nor \(X\beta\) are estimable. The estimable functions of \(\gamma\) will be those that are linear functions of \((I-M)Z\gamma\). This is shown below.

**Proposition 9.1.1**: \(\xi^{\prime}\gamma\) _is estimable if and only if \(\xi^{\prime}=\rho^{\prime}(I-M)Z\) for some vector \(\rho\)._

_Proof_ If \(\xi^{\prime}\gamma\) is estimable, there exists \(\rho\) such that \(\xi^{\prime}\gamma=\rho^{\prime}[X,Z]\begin{bmatrix}\beta\\ \gamma\end{bmatrix}\), so \(\rho^{\prime}[X,Z]=(0,\xi^{\prime})\) and \(\rho^{\prime}X=0\). Therefore, \(\xi^{\prime}=\rho^{\prime}Z=\rho^{\prime}(I-M)Z\). Conversely, if \(\xi^{\prime}=\rho^{\prime}(I-M)Z\) then \(\rho^{\prime}(I-M)[X,Z]\begin{bmatrix}\beta\\ \gamma\end{bmatrix}=\xi^{\prime}\gamma\). \(\square\)

Proposition 9.1.1 is phrased in terms of estimating a function of \(\gamma\), but it also applies with appropriate changes to estimation of \(\beta\).

Finally, if \(X\beta\) and \(\gamma\) are estimable, that is, if \((I-M)Z\) is of full rank, it is easy to see that \[\text{Cov}\!\left[\begin{matrix}X\hat{\beta}\\ \hat{\gamma}\end{matrix}\right]=\sigma^{2}\left[\begin{matrix}M+MZ\!\left[Z^{ \prime}(I-M)Z\right]^{-1}Z^{\prime}M&-MZ\!\left[Z^{\prime}(I-M)Z\right]^{-1}\\ -\left[Z^{\prime}(I-M)Z\right]^{-1}Z^{\prime}M&\left[Z^{\prime}(I-M)Z\right]^{- 1}\end{matrix}\right]\!.\]

**Exercise 9.0**: In the ACOVA model (9.0.1), suppose \(\lambda^{\prime}\beta=\rho^{\prime}X\beta\) is estimable so that

\[\text{Var}(\lambda^{\prime}\hat{\beta})=\sigma^{2}\rho^{\prime}\left\{M+MZ\! \left[Z^{\prime}(I-M)Z\right]^{-}Z^{\prime}M\right\}\rho\,.\]

In the standard model without covariates \(Y=X\beta+e\), \(\text{Var}(\lambda^{\prime}\hat{\beta})=\sigma^{2}\rho^{\prime}M\rho\), which seems to be a smaller number than the ACOVA variance. If the point of ACOVA is to sharpen up the analysis, the ACOVA variance should be smaller. Explain away this seeming contradiction. Hint: Does \(\sigma^{2}\) mean the same thing in each model?

#### Generalized Least Squares

Consider the linear model

\[Y=X\beta+Z\gamma+e,\ \ \ \ \ E(e)=0,\ \ \ \ \ \text{Cov}(e)=\sigma^{2}V.\]

As in Section 2.7, generalized least squares estimates are BLUEs and satisfy

\[X\hat{\beta}+Z\hat{\gamma}=\mathcal{A}Y\]

where \(\mathcal{A}\) is the oblique projection operator onto \(C(X,Z)\) along \(C[V^{-1}(X,Z)]^{\perp}\).

The matrix

\[A\equiv X[X^{\prime}V^{-1}X]^{-}X^{\prime}V^{-1}\]

is the oblique projection operator onto \(C(X)\) along \(C[V^{-1}X]^{\perp}\), i.e., if \(v\in C(X)\), \(Av=v\), and if \(v\in C[V^{-1}X]^{\perp}\), \(Av=0\). From Exercise 2.5,

\[(I-A)^{\prime}V^{-1}(I-A)=(I-A)^{\prime}V^{-1}=V^{-1}(I-A). \tag{3}\]

To see that

\[\mathcal{A}=A+(I-A)Z[Z^{\prime}(I-A)^{\prime}V^{-1}(I-A)Z]^{-}Z^{\prime}(I-A) ^{\prime}V^{-1} \tag{4}\]

simply write out

\[\mathcal{A}=[X,(I-A)Z]\left\{[X,(I-A)Z]^{\prime}V^{-1}[X,(I-A)Z]\right\}^{-}[ X,(I-A)Z]^{\prime}V^{-1}\]

and simplify using (3), \((I-A)X=0\), and the fact that a generalized inverse of a block diagonal matrix is the block diagonal of the generalized inverses.

Given (4) it is easy to see that

\[\hat{\gamma}=[Z^{\prime}(I-A)^{\prime}V^{-1}(I-A)Z]^{-}Z^{\prime}(I-A)^{\prime}V^{ -1}(I-A)Y\]

and

\[X\hat{\beta}=A(Y-Z\hat{\gamma})\]

provide generalized least squares estimates.

### Estimation of Error and Tests of Hypotheses

The estimate of the variance \(\sigma^{2}\) is the _MSE_. We will find \(SSE=Y^{\prime}(I-\mathcal{P})Y\) in terms of \(M\) and \(Z\). The error sum of squares is

\[Y^{\prime}(I-\mathcal{P})Y = Y^{\prime}[I-M-M_{2}]Y\] \[= Y^{\prime}\Big{[}(I-M)-(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}Z^ {\prime}(I-M)\Big{]}Y\] \[= Y^{\prime}(I-M)Y-Y^{\prime}(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{ -}Z^{\prime}(I-M)Y.\]

Using the notation \(E_{AB}\equiv A^{\prime}(I-M)B\), we have

\[Y^{\prime}(I-\mathcal{P})Y=E_{YY}-E_{YZ}E_{ZZ}^{-}E_{ZY}.\]

Note the similarity of this structure to the covariance matrix of the best linear predictor's prediction error given in (6.5.1).

##### _Example 9.2.1_

Consider a balanced two-way analysis of variance with no replication or interaction and one covariate (regression variable, concomitant variable, supplemental observation) \(z\). The analysis of covariance model can be written

\[y_{ij}=\mu+\alpha_{i}+\eta_{j}+\gamma z_{ij}+e_{ij},\]

\(i=1,\ldots,a,j=1,\ldots,b\). \(X\) is the model matrix for the balanced two-way ANOVA without replication or interaction,

\[y_{ij}=\mu+\alpha_{i}+\eta_{j}+e_{ij},\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), and \(Z\) is an \(ab\times 1\) matrix that contains the values of \(z_{ij}\). The sum of squares for error in the covariate analysis is \(E_{YY}-E_{YZ}^{2}/E_{ZZ}\), where

\[E_{YY}=\sum_{i=1}^{a}\sum_{j=1}^{b}\left(y_{ij}-\bar{y}_{i}.-\bar{y}_{.j}+\bar{ y}_{.}\right)^{2},\]\[E_{YZ}=E_{ZY}=\sum_{i=1}^{a}\sum_{j=1}^{b}\left(y_{ij}-\bar{y}_{i.}-\bar{y}_{.j}+ \bar{y}_{.}\right)\left(z_{ij}-\bar{z}_{i.}-\bar{z}_{.j}+\bar{z}_{.}\right),\]

\[E_{ZZ}=\sum_{i=1}^{a}\sum_{j=1}^{b}\left(z_{ij}-\bar{z}_{i.}-\bar{z}_{.j}+\bar{ z}_{.}\right)^{2}.\]

Tests for analysis of covariance models are found by considering the reductions in sums of squares for error due to the models. For instance, if \(C(X_{0})\subset C(X)\) and we want to test the reduced model

\[Y=X_{0}\beta_{0}+Z\gamma+e\]

against the full model (9.0.1), the test statistic is

\[\frac{\left[Y^{\prime}(I-\mathcal{P}_{0})Y-Y^{\prime}(I-\mathcal{P})Y\right]/ \left[r(X,Z)-r(X_{0},Z)\right]}{\left[Y^{\prime}(I-\mathcal{P})Y\right]/ \left[n-r(X,Z)\right]},\]

where \(\mathcal{P}_{0}\) is the perpendicular projection operator onto \(C(X_{0},Z)\). We have already found \(Y^{\prime}(I-\mathcal{P})Y\). If \(M_{0}\) is the perpendicular projection operator onto \(C(X_{0})\),

\[Y^{\prime}(I-\mathcal{P}_{0})Y=Y^{\prime}(I-M_{0})Y-Y^{\prime}(I-M_{0})Z\left[ Z^{\prime}(I-M_{0})Z\right]^{-}Z^{\prime}(I-M_{0})Y.\]

In the old days, these computations were facilitated by writing an analysis of covariance table.

Example 9.2.1 ContinuedThe analysis of covariance table is given below in matrix notation. Recall that, for example,

\[Y^{\prime}M_{\alpha}Y = b\sum_{i=1}^{a}\left(\bar{y}_{i.}-\bar{y}_{.}\right)^{2},\] \[Y^{\prime}M_{\alpha}Z = b\sum_{i=1}^{a}\left(\bar{y}_{i.}-\bar{y}_{.}\right)\left(\bar{z} _{i.}-\bar{z}_{.}\right),\] \[Z^{\prime}M_{\alpha}Z = b\sum_{i=1}^{a}\left(\bar{z}_{i.}-\bar{z}_{.}\right)^{2}.\]\[\begin{array}{lccccc}&&&&\text{ACOVA Table}\\ \text{Source}&df&SS_{YY}&SS_{YZ}&SS_{ZZ}\\ \hline\text{Grand Mean}&1&Y^{\prime}\frac{1}{n}J_{n}^{n}Y&Y^{\prime}\frac{1}{n}J_{n }^{n}Z&Z^{\prime}\frac{1}{n}J_{n}^{n}Z\\ \text{Treatments }(\alpha)&a-1&Y^{\prime}M_{\alpha}Y&Y^{\prime}M_{\alpha}Z&Z^{ \prime}M_{\alpha}Z\\ \text{Treatments }(\eta)&b-1&Y^{\prime}M_{\eta}Y&Y^{\prime}M_{\eta}Z&Z^{ \prime}M_{\eta}Z\\ \text{Error}&n-a-b+1&Y^{\prime}(I-M)Y&Y^{\prime}(I-M)Z&Z^{\prime}(I-M)Z\\ \end{array}\]

If we want to test \(H_{0}:\eta_{1}=\eta_{2}=\cdots=\eta_{b}\), the error sum of squares under the reduced model is

\[\begin{array}{l}\left[Y^{\prime}(I-M)Y+Y^{\prime}M_{\eta}Y\right]-\left[Y^{ \prime}(I-M)Z+Y^{\prime}M_{\eta}Z\right]\\ \times\left[Z^{\prime}(I-M)Z+Z^{\prime}M_{\eta}Z\right]^{-}\left[Z^{\prime}(I- M)Y+Z^{\prime}M_{\eta}Y\right].\end{array}\]

All of these terms are available from the ACOVA table. With more than one covariate, the terms in the \(SS_{YZ}\) and \(SS_{ZZ}\) columns of the table would be matrices and it would be more involved to compute \(\left[Z^{\prime}(I-M)Z+Z^{\prime}M_{\eta}Z\right]^{-}\).

**Exercise 9.1**: Consider a one-way ANOVA with one covariate. The model is

\[y_{ij}=\mu+\alpha_{i}+\xi x_{ij}+e_{ij},\]

\(i=1,\ldots,t,\ j=1,\ldots,N_{i}\). Find the BLUE of the contrast \(\sum_{i=1}^{t}\lambda_{i}\alpha_{i}\). Find the variance of the contrast.

**Exercise 9.2**: Consider the problem of estimating \(\beta_{p}\) in the regression model

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{p}x_{ip}+e_{i}. \tag{2}\]

Let \(r_{i}\) be the _ordinary residual_ from fitting

\[y_{i}=\alpha_{0}+\alpha_{1}x_{i1}+\cdots+\alpha_{p-1}x_{ip-1}+e_{i}\]

and \(s_{i}\) be the residual from fitting

\[x_{ip}=\gamma_{0}+\gamma_{1}x_{i1}+\cdots+\gamma_{p-1}x_{ip-1}+e_{i}.\]

Show that the least squares estimate of \(\beta_{p}\) is \(\hat{\xi}\) from fitting the model

\[r_{i}=\xi s_{i}+e_{i},\ \ \ i=1,\ldots,n, \tag{3}\]

that the _SSE_ from models (2) and (3) are the same, and that \((\hat{\beta}_{0},\ldots,\hat{\beta}_{p-1})^{\prime}=\hat{\alpha}-\hat{\beta}_{p}\hat {\gamma}\) with \(\hat{\alpha}=(\hat{\alpha}_{0},\ldots,\hat{\alpha}_{p-1})^{\prime}\) and \(\hat{\gamma}=(\hat{\gamma}_{0},\ldots,\hat{\gamma}_{p-1})^{\prime}\). Discuss the usefulness of these results for computing regression estimates. (These are the key results behind the _sweep operator_ that is often used in regression computations.) What happens to the results if \(r_{i}\) is replaced by \(y_{i}\) in model (3)?

**Exercise 9.3**.: Suppose \(\lambda_{1}^{\prime}\beta\) and \(\lambda_{2}^{\prime}\gamma\) are estimable in model (9.0.1). Use the normal equations to find find least squares estimates of \(\lambda_{1}^{\prime}\beta\) and \(\lambda_{2}^{\prime}\gamma\).

Hint: Reparameterize the model as \(X\beta+Z\gamma=X\delta+(I-M)Z\gamma\) and use the normal equations on the reparameterized model. Note that \(X\delta=X\beta+MZ\gamma\).

**Exercise 9.4**.: Derive the test for model (9.0.1) versus the reduced model \(Y=X\beta+Z_{0}\gamma_{0}+e\), where \(C(Z_{0})\subset C(Z)\). Describe how the procedure would work for testing \(H_{0}:\gamma_{2}=0\) in the model \(y_{ij}=\mu+\alpha_{i}+\eta_{j}+\gamma_{1}z_{ij1}+\gamma_{2}z_{ij2}+e_{ij}\), \(i=1,\ldots,a\), \(j=1,\ldots,b\).

**Exercise 9.5**.: An experiment was conducted with two treatments. There were four levels of the first treatment and five levels of the second treatment. Besides the data \(y\), two covariates were measured, \(x_{1}\) and \(x_{2}\). The data are given below. Analyze the data with the assumption that there is no interaction between the treatments.

\[\begin{array}{rccccc|rrr}i&j&y_{ij}&x_{1ij}&x_{2ij}&i&j&y_{ij}&x_{1ij}&x_{2 ij}\\ \hline 1&1&27.8&5.3&9&3&1&22.4&3.0&13\\ &2&27.8&5.2&11&2&21.0&4.5&12\\ &3&26.2&3.6&13&3&30.6&5.4&18\\ &4&24.8&5.2&17&4&25.4&6.6&21\\ &5&17.8&3.6&10&5&15.9&4.1&9\\ \end{array}\]

\[\begin{array}{rccccc|rrr}2&1&19.6&4.7&12&4&1&14.1&5.4&10\\ &2&28.4&5.8&17&2&29.5&6.8&18\\ &3&26.3&3.3&22&3&29.2&5.3&22\\ &4&18.3&4.1&8&4&21.5&6.2&9\\ &5&20.8&5.7&11&5&25.5&6.4&22\\ \end{array}\]

### Another Adjusted Model and Missing Data

Exercise 9.2 looked at the sweep operator that is based on least squares fitting of

\[(I-M)Y=(I-M)Z\gamma+e.\]

This gives the ACOVA least squares estimates of \(\gamma\) and the ACOVA _SSE_. LaMotte (2014) expands on this basic idea.

Another adjusted model is to fit

\[(Y-Z\hat{\gamma})=X\beta+e,\]where \(\hat{\gamma}\) is a least squares estimate from the ACOVA model. It is clear from Section 1 that a least squares fit of this model gives a least squares ACOVA estimate of \(X\beta\). It is less clear that it also gives the ACOVA _SSE_. With \(\hat{\gamma}=[Z^{\prime}(I-M)Z]^{-}Z^{\prime}(I-M)Y\), the _SSE_ from the adjusted model (1) is

\[(Y-Z\hat{\gamma})^{\prime}(I-M)(Y-Z\hat{\gamma})\] \[=Y^{\prime}(I-M)Y-2\hat{\gamma}^{\prime}Z^{\prime}(I-M)Y+\hat{ \gamma}^{\prime}Z^{\prime}(I-M)Z\hat{\gamma}\] \[=Y^{\prime}(I-M)Y-2Y^{\prime}(I-M)Z[Z^{\prime}(I-M)Z]^{-}Z^{ \prime}(I-M)Y\] \[\qquad+Y^{\prime}(I-M)Z[Z^{\prime}(I-M)Z]^{-}[Z^{\prime}(I-M)Z][Z^ {\prime}(I-M)Z]^{-}Z^{\prime}(I-M)Y\] \[=Y^{\prime}(I-M)Y-Y^{\prime}(I-M)Z[Z^{\prime}(I-M)Z]^{-}Z^{\prime }(I-M)Y,\]

A difficulty with model (1) is that you have to do the ACOVA analysis first to get \(\hat{\gamma}\). Another is that, unless \(X\beta\) is estimable in the ACOVA, we need to worry about ACOVA estimability rather than estimability in (1). One application of this model is in dealing with missing data.

When a few observations are missing from, say, a balanced multifactor design, the balance is lost and the analysis would seem to be quite complicated. One use of the analysis of covariance is to allow the analysis with missing data to be performed using results for the original balanced design. With modern computing power, these days we would probably just fit the unbalanced model.

Consider an original design

\[Y=X\beta+e,\]

with \(Y=(y_{1},\ldots,y_{n})^{\prime}\). For each missing observation \(y_{i}\), include a covariate \(z_{t}=(0,\ldots,0,1,0,\ldots,0)^{\prime}\) with the \(1\) in the \(i\)th place. Set each \(y_{i}\) that is missing equal to zero. Before applying the adjusted model (1), consider the ACOVA model.

We wish to show that the _SSE_ in this ACOVA model equals the _SSE_ in the model with the missing observations deleted. The _MSE_ in the covariance model will also equal the _MSE_ in the model with deletions. In the covariance model, although we are artificially adding observations by setting missing observations to zero, we are also removing those degrees of freedom from the error by adding covariates.

Suppose \(r\) observations are missing. Without loss of generality, we can assume that the last \(r\) observations are missing. The \(n\times r\) matrix of covariates can be written

\[Z=\left[\begin{array}{c}0\\ I_{r}\end{array}\right],\]

where \(I_{r}\) is an \(r\times r\) identity matrix and \(0\) is an \((n-r)\times r\) matrix of zeros. Let \(X\) be the \(n\times p\) model matrix for the model with no missing observations and let \(X_{*}\) be the \((n-r)\times p\) model matrix for the model with the missing observations deleted. Again we can assume that

\[X=\left[\begin{array}{c}X_{*}\\ X_{r}\end{array}\right],\]where \(X_{r}\) is the \(r\times p\) matrix whose rows are the rows of \(X\) corresponding to the missing observations. The analysis of covariance model

\[Y=X\beta+Z\gamma+e\]

can now be written as

\[Y=\begin{bmatrix}X_{*}&0\\ X_{r}&I_{r}\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}+e.\]

Notice that

\[C\begin{pmatrix}\begin{bmatrix}X_{*}&0\\ X_{r}&I_{r}\end{bmatrix}\end{pmatrix}=C\begin{pmatrix}\begin{bmatrix}X_{*}&0\\ 0&I_{r}\end{bmatrix}\end{pmatrix}.\]

Let \(M_{*}\) be the perpendicular projection operator onto \(C(X_{*})\) and let \(\mathcal{P}\) be the perpendicular projection operator onto

\[C\begin{pmatrix}\begin{bmatrix}X_{*}&0\\ X_{r}&I_{r}\end{bmatrix}\end{pmatrix}.\]

It is easy to see that

\[\mathcal{P}=\begin{bmatrix}M_{*}&0\\ 0&I_{r}\end{bmatrix}.\]

Writing \(Y\) as \(Y^{\prime}=[Y^{\prime}_{*},0]\), we find that

\[Y^{\prime}(I-\mathcal{P})Y=\begin{bmatrix}Y^{\prime}_{*}&0\end{bmatrix}\begin{bmatrix} I-M_{*}&0\\ 0&0\end{bmatrix}\begin{bmatrix}Y_{*}\\ 0\end{bmatrix}=Y^{\prime}_{*}(I-M_{*})Y_{*}.\]

Since \(Y^{\prime}(I-\mathcal{P})Y\) is the _SSE_ from the covariate model and \(Y^{\prime}_{*}(I-M_{*})Y_{*}\) is the _SSE_ for the model with the missing observations dropped, we are done. Note that the values we put in for the missing observations do not matter for computing the _SSE_. (The justification in Subsection 6.7.3 for Utts's Rainbow Test for Lack of Fit is similarly based on the idea of including a separate indicator for every case being deleted but there we observed the data being deleted.) Tests of hypotheses can be conducted by comparing _SSE_s for different models.

With \(Y^{\prime}=[Y^{\prime}_{*},0]\), estimation will be the same in both models. The least squares estimate of

\[\begin{bmatrix}X_{*}\beta\\ X_{r}\beta+\gamma\end{bmatrix}\]

is

\[\mathcal{P}Y=\begin{bmatrix}M_{*}Y_{*}\\ 0\end{bmatrix}.\]

Any estimable function in the model \(Y_{*}=X_{*}\beta+e_{*}\) is estimable in the covariate model, and the estimates are the same. The function \(\rho^{\prime}_{*}X_{*}\beta\) equals \(\rho^{\prime}(X\beta+Z\gamma)\)where \(\rho^{\prime}=[\rho^{\prime}_{*},0]\) and \(0\) is a \(1\times r\) matrix of zeros. Thus, \(\rho^{\prime}\mathcal{P}Y=\rho^{\prime}_{*}M_{*}Y_{*}\). In particular, \(X_{*}\hat{\beta}\) is the same in both models and if \(X_{r}\beta\) is estimable in the \(X_{*}\) model, \(-\hat{\gamma}=X_{r}\hat{\beta}\), the predicted values of \(X_{r}\beta\) from the \(X_{*}\) model.

#### Exercise 9.6

Show that

\[\mathcal{P}=\begin{bmatrix}M_{*}&0\\ 0&I_{r}\end{bmatrix}\]

and that if \(X_{r}=P^{\prime}X_{*}\) for some \(P\), then \(\gamma\) and \(X\beta\) are both estimable in the missing data ACOVA. What happens to estimability in the ACOVA model when the \(X\) model involves factors and you are missing all the observations on one level of a factor?

An alternative approach to the missing value problem is based on finding substitutes for the missing values. The substitutes are chosen so that if one acts like the substitutes are real data, the correct \(SSE\) is computed. (The degrees of freedom for error must be corrected.)

Setting the problem up as before, we have

\[Y=X\beta+Z\gamma+e,\]

and the estimate \(\hat{\gamma}\) can be found. It is proposed to treat \(Y-Z\hat{\gamma}=[Y^{\prime}_{*},-\hat{\gamma}^{\prime}]^{\prime}\) as the data from an experiment with model matrix \(X\), i.e., fit the adjusted model (1). We already know that this gives the correct \(SSE\) and least squares estimates of \(X\beta\). The variance of an estimate, say \(\rho^{\prime}X\hat{\beta}\), needs to be calculated as in an analysis of covariance. It is \(\sigma^{2}(\rho^{\prime}M\rho+\rho^{\prime}MZ[Z^{\prime}(I-M)Z]^{-1}Z^{\prime} M\rho)=\sigma^{2}\rho^{\prime}_{*}M_{*}\rho_{*}\), not the naive value of \(\sigma^{2}\rho^{\prime}M\rho\).

For \(r=1\) missing observation and a variety of balanced designs, formulae have been obtained for \(-\hat{\gamma}\) and are available in many older statistical methods books.

#### Exercise 9.7

Derive \(-\hat{\gamma}\) for a randomized complete block design when \(r=1\).

### Balanced Incomplete Block Designs

The analysis of covariance technique can be used to develop the analysis of a balanced incomplete block design. Suppose that a design is to be set up with \(b\) blocks and \(t\) treatments, but the number of treatments that can be observed in any block is \(k\), where \(k<t\). One natural way to proceed would be to find a design where each pair of treatments occurs together in the same block a fixed number of times, say \(\lambda\). Such a design is called a _balanced incomplete block (BIB) design_.

Let \(r\) be the common number of replications for each treatment. There are two well-known facts about the parameters introduced so far. First, the total number of experimental units in the design must be the number of blocks times the number of units in each block, i.e., \(bk\), but the total number of units must also be the number of treatments times the number of times we observe each treatment, i.e., \(tr\); thus

\[tr=bk. \tag{1}\]

Second, the number of within block comparisons between any given treatment and the other treatments is fixed. One way to count this is to multiply the number of other treatments (\(t-1\)) by the number of times each occurs in a block with the given treatment (\(\lambda\)). Another way to count it is to multiply the number of other treatments in a block (\(k-1\)) times the number of blocks that contain the given treatment (\(r\)). Therefore,

\[(t-1)\lambda=r(k-1). \tag{2}\]

The analysis that follows treats both the treatment and block effects as fixed. Previous editions of this book contained a section on the _recovery of interblock information_ in which the block effects were considered random. That section now only exists on my website as part of a larger work _Topics in Experimental Design_, cf. [http://www.stat.unm.edu/~fletcher/TopicsInDesign](http://www.stat.unm.edu/~fletcher/TopicsInDesign).

An experiment was conducted to examine the effects of fertilizers on potato yields. Six treatments (\(A\), \(B\), \(C\), \(D\), \(E\), and \(F\)) were used but blocks were chosen that contained only five experimental units. The experiment was performed using a balanced incomplete block design with six blocks. The potato yields (in pounds) along with the mean yield for each block are reported in Table 1.

The six treatments consist of all of the possible combinations of two factors. One factor was that a nitrogen-based fertilizer was either applied (\(n_{1}\)) or not applied (\(n_{0}\)). The other factor was that a phosphate-based fertilizer was either not applied (\(p_{0}\)), applied in a single dose (\(p_{1}\)), or applied in a double dose (\(p_{2}\)). In terms of the factorial structure, the six treatments are \(A=n_{0}p_{0}\), \(B=n_{0}p_{1}\), \(C=n_{0}p_{2}\), \(D=n_{1}p_{0}\), \(E=n_{1}p_{1}\), and \(F=n_{1}p_{2}\). From the information in Table 1, it is a simple matter to check that \(t=6\), \(b=6\), \(k=5\), \(r=5\), and \(\lambda=4\). After deriving the theory for balanced incomplete block designs, we will return to these data and analyze them.

\begin{table}
\begin{tabular}{c|c c c c c c|c} Block & & & & & & Block & Means \\
1 & \(E\) 583 & \(B\) 512 & \(F\) 661 & \(A\) 399 & \(C\) 525 & 536.0 \\
2 & \(B\) 439 & \(C\) 460 & \(D\) 424 & \(E\) 497 & \(F\) 592 & 482.4 \\
3 & \(A\) 334 & \(E\) 466 & \(C\) 492 & \(B\) 431 & \(D\) 355 & 415.6 \\
4 & \(F\) 570 & \(D\) 433 & \(E\) 514 & \(C\) 448 & \(A\) 344 & 461.8 \\
5 & \(D\) 402 & \(A\) 417 & \(B\) 420 & \(F\) 626 & \(E\) 615 & 496.0 \\
6 & \(C\) 450 & \(F\) 490 & \(A\) 268 & \(D\) 375 & \(B\) 347 & 386.0 \\ \end{tabular}
\end{table}
Table 1: Potato yields in pounds for six fertilizer treatmentsThe fixed effects balanced incomplete block model can be written as

\[y_{ij}=\mu+\beta_{i}+\tau_{j}+e_{ij},\qquad e_{ij}\text{s i.i.d. }N(0,\sigma^{2}), \tag{3}\]

where \(i=1,\ldots,b\) and \(j\in D_{i}\) or \(j=1,\ldots,t\) and \(i\in A_{j}\). \(D_{i}\) is the set of indices for the treatments in block \(i\). \(A_{j}\) is the set of indices for the blocks in which treatment \(j\) occurs. Note that there are \(k\) elements in each set \(D_{i}\) and \(r\) elements in each set \(A_{j}\).

In applying the analysis of covariance, we will use the balanced one-way ANOVA determined by the grand mean and the blocks to help analyze the model with covariates. The covariates are taken as the columns of the model matrix associated with the treatments. Writing (3) in matrix terms, we get

\[Y=[X,Z]\begin{bmatrix}\beta\\ \tau\end{bmatrix}+e,\]

\[\beta^{\prime}\equiv(\mu,\,\beta_{1},\ldots,\beta_{b})\,,\qquad\tau^{\prime} \equiv(\tau_{1},\ldots,\tau_{t})\,.\]

Note that in performing the analysis of covariance for this model our primary interest lies in the coefficients of the covariates, i.e., the treatment effects. To perform an analysis of covariance, we need to find \(Y^{\prime}(I-M)Z\) and \([Z^{\prime}(I-M)Z]^{-}\).

First, find \(Z^{\prime}(I-M)Z\). There are \(t\) columns in \(Z\); write \(Z=[Z_{1},\ldots,Z_{t}]\). The rows of the \(m\)th column indicate the presence or absence of the \(m\)th treatment. Using two subscripts to denote the rows of vectors, we can write the \(m\)th column as

\[Z_{m}=[z_{ij,m}],\qquad\text{where }z_{ij,m}=\delta_{jm}\]

and \(\delta_{jm}\) is \(1\) if \(j=m\), and \(0\) otherwise. In other words, \(Z_{m}\) is \(0\) for all rows except the \(r\) rows that correspond to an observation on treatment \(m\); those \(r\) rows are \(1\).

To get the \(t\times t\) matrix \(Z^{\prime}(I-M)Z\), we find each individual element of \(Z^{\prime}Z\) and \(Z^{\prime}MZ\). This is done by finding \(Z^{\prime}_{s}Z_{m}\) and \(Z^{\prime}_{s}M^{\prime}MZ_{m}\) for all values of \(m\) and \(s\). If \(m=s\), we get

\[Z^{\prime}_{m}Z_{m}=\sum_{i}\sum_{j}(z_{ij,m})^{2}=\sum_{j=1}^{t}\sum_{i\in A_ {j}}\delta_{jm}=\sum_{j=1}^{t}r\delta_{jm}=r.\]

Now, if \(m\neq s\), because each observation has only one treatment associated with it, either \(z_{ij,s}\) or \(z_{ij,m}\) equals \(0\); so for \(s\neq m\),

\[Z^{\prime}_{s}Z_{m}=\sum_{i}\sum_{j}(z_{ij,s})(z_{ij,m})=\sum_{j=1}^{t}\sum_{i \in A_{j}}\delta_{js}\delta_{jm}=\sum_{j=1}^{t}r\delta_{js}\delta_{jm}=0.\]

Thus the matrix \(Z^{\prime}Z\) is \(rI_{t}\), where \(I_{t}\) is a \(t\times t\) identity matrix.

Recall that in this problem, \(X\) is the model matrix for a one-way ANOVA where the groups of the ANOVA are the blocks of the BIB design and there are \(k\) observationson each group. Using two subscripts to denote each row and each column of a matrix, we can write the projection matrix as in Section 4.1,

\[M=[v_{ij,i^{\prime}j^{\prime}}],\qquad\text{where }v_{ij,i^{\prime}j^{\prime}}=\frac{1}{k}\delta_{ii^{\prime}}.\]

Let

\[MZ_{m}=[d_{ij,m}].\]

Then

\[d_{ij,m} =\sum_{i^{\prime}j^{\prime}}v_{ij,i^{\prime}j^{\prime}}z_{i^{ \prime}j^{\prime},m}\] \[=\sum_{j^{\prime}=1}^{t}\sum_{i^{\prime}\in A_{j^{\prime}}}\frac{ 1}{k}\delta_{ii^{\prime}}\delta_{j^{\prime}m}\] \[=\sum_{j^{\prime}=1}^{t}\delta_{j^{\prime}m}\sum_{i^{\prime}\in A _{j^{\prime}}}\frac{1}{k}\delta_{ii^{\prime}}\] \[=\sum_{i^{\prime}\in A_{m}}\frac{1}{k}\delta_{ii^{\prime}}\] \[\equiv\frac{1}{k}\delta_{i}(A_{m}),\]

where \(\delta_{i}(A_{m})\) is 1 if \(i\in A_{m}\) and 0 otherwise. In other words, if treatment \(m\) is in block \(i\), then all \(k\) of the units in block \(i\) have \(d_{ij,m}=1/k\). If treatment \(m\) is not in block \(i\), all \(k\) of the units in block \(i\) have \(d_{ij,m}=0\). Since treatment \(m\) is contained in exactly \(r\) blocks,

\[Z_{m}^{\prime}M^{\prime}MZ_{m} =\sum_{ij}(d_{ij,m})^{2}=\sum_{i=1}^{b}\sum_{j\in D_{i}}k^{-2} \delta_{i}(A_{m})\] \[=\sum_{i=1}^{b}(k/k^{2})\delta_{i}(A_{m})=\frac{r}{k}.\]

Since, for \(s\neq m\), there are \(\lambda\) blocks in which both treatments \(s\) and \(m\) are contained,

\[Z_{s}^{\prime}M^{\prime}MZ_{m} =\sum_{ij}(d_{ij,s})(d_{ij,m})=\sum_{i=1}^{b}\sum_{j\in D_{i}}(1/ k^{2})\delta_{i}(A_{s})\delta_{i}(A_{m})\] \[=\sum_{i=1}^{b}(k/k^{2})\delta_{i}(A_{s})\delta_{i}(A_{m})=\frac{ \lambda}{k}.\]It follows that the matrix \(Z^{\prime}MZ\) has values \(r/k\) down the diagonal and values \(\lambda/k\) off the diagonal. This can be written as

\[Z^{\prime}MZ=\frac{1}{k}\left[(r-\lambda)I+\lambda J^{t}_{t}\right].\]

Finally, we can now write

\[Z^{\prime}(I-M)Z = Z^{\prime}Z-Z^{\prime}MZ\] \[= rI-k^{-1}\left[(r-\lambda)I+\lambda J^{t}_{t}\right]\] \[= k^{-1}\left[(r(k-1)+\lambda)I-\lambda J^{t}_{t}\right].\]

This matrix can be simplified further. Define

\[W\equiv I-(1/t)J^{t}_{t}. \tag{4}\]

Note that \(W\) is a perpendicular projection operator and that equation (2) gives

\[r(k-1)+\lambda=\lambda t.\]

With these substitutions, we obtain

\[Z^{\prime}(I-M)Z=(\lambda/k)\left[tI-J^{t}_{t}\right]=(\lambda t/k)\left[I-(1 /t)J^{t}_{t}\right]=(\lambda t/k)W.\]

We need to find a generalized inverse of \(Z^{\prime}(I-M)Z\). Because \(W\) is a projection operator, it is easily seen that

\[\left[Z^{\prime}(I-M)Z\right]^{-}=(k/\lambda t)W. \tag{5}\]

We also need to be able to find the \(1\times t\) vector \(Y^{\prime}(I-M)Z\). The vector \((I-M)Y\) has elements \((y_{ij}-\bar{y}_{i}.)\), so

\[Y^{\prime}(I-M)Z_{m}=\sum_{ij}(y_{ij}-\bar{y}_{i}.)z_{ij,m}=\sum_{j=1}^{t} \delta_{jm}\sum_{i\in A_{j}}(y_{ij}-\bar{y}_{i}.)=\sum_{i\in A_{m}}(y_{im}- \bar{y}_{i}.).\]

Define

\[Q_{m}\equiv\sum_{i\in A_{m}}(y_{im}-\bar{y}_{i}.).\]

Then

\[Y^{\prime}(I-M)Z=(Q_{1},\ldots,Q_{t}).\]

Since the \(\beta\) effects are for blocks, our primary interests are in estimable functions \(\xi^{\prime}\tau\) and in estimating \(\sigma^{2}\). From (9.1.1) and Proposition 9.1.1, write \[\xi^{\prime}=\rho^{\prime}(I-M)Z\]

to get

\[\xi^{\prime}\hat{\tau}=\rho^{\prime}(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}Z^{ \prime}(I-M)Y\]

and, from (9.2.1),

\[SSE=Y^{\prime}(I-M)Y-Y^{\prime}(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}Z^{ \prime}(I-M)Y.\]

Both of these formulae involve the term \((I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}\). This term can be simplified considerably. Note that since the columns of \(Z\) are 0s and 1s, indicating the presence or absence of a treatment effect,

\[ZJ_{t}^{1}=J_{n}^{1}.\]

Because \(M\) is defined from a one-way ANOVA,

\[0=(I-M)J_{n}^{1}=(I-M)ZJ_{t}^{1}.\]

From (5), (4), and (6), it is easily seen that

\[(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}=(k/\lambda t)(I-M)Z.\]

Using this fact, we get that the BLUE of \(\xi^{\prime}\tau\) is

\[\xi^{\prime}\hat{\tau} = \rho^{\prime}(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}Z^{\prime}(I- M)Y\] \[= \rho^{\prime}(k/\lambda t)(I-M)ZZ^{\prime}(I-M)Y\] \[= (k/\lambda t)\xi^{\prime}(Q_{1},\,\ldots,\,Q_{t})^{\prime}\] \[= (k/\lambda t)\sum_{j=1}^{t}\xi_{j}Q_{j}.\]

Many computer programs, e.g., Minitab, present _adjusted treatment means_ (\(k/\lambda t\)) \(Q_{j}+\tilde{y}_{\cdot}\). that can be used to estimate contrasts, because \(\sum_{j}\xi_{j}\tilde{y}_{\cdot\cdot}=0\). The variance of the estimate of the contrast is

\[\text{Var}\big{(}\xi^{\prime}\hat{\tau}\big{)} = \sigma^{2}\rho^{\prime}(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}Z^{ \prime}(I-M)\rho\] \[= \sigma^{2}(k/\lambda t)\xi^{\prime}\hat{\xi}.\]

From the estimate and the variance, it is a simple matter to see that

\[SS\big{(}\xi^{\prime}\tau\big{)}=(k/\lambda t)\left[\sum_{j=1}^{t}\xi_{j}Q_{j} \right]^{2}\sqrt[2]{\xi^{\prime}\hat{\xi}}.\]The error sum of squares is

\[\begin{array}{l}SSE=Y^{\prime}(I-M)Y-Y^{\prime}(I-M)Z\left[Z^{\prime}(I-M)Z\right] ^{-}Z^{\prime}(I-M)Y\\ =Y^{\prime}(I-M)Y-(k/\lambda t)Y^{\prime}(I-M)ZZ^{\prime}(I-M)Y\\ =\sum_{ij}(y_{ij}-\bar{y}_{i\cdot})^{2}-\frac{k}{\lambda t}\sum_{j=1}^{t}Q_{j}^ {2}\,.\end{array}\]

**Exercise 9.8**: Show that \(\xi^{\prime}\tau\) is estimable if and only if \(\xi^{\prime}\tau\) is a contrast. Hint: One direction is easy. For the other direction, show that for \(\xi^{\prime}=(\xi_{1},\ldots,\xi_{t})\),

\[\xi^{\prime}=(k/\lambda t)\xi^{\prime}Z^{\prime}(I-M)Z.\]

**Exercise 9.9**: Show that if \(\xi^{\prime}\tau\) and \(\eta^{\prime}\tau\) are contrasts and that if \(\xi^{\prime}\eta=0\), then \(\xi^{\prime}\tau=0\) and \(\eta^{\prime}\tau=0\) put orthogonal constraints on \(C(X,Z)\), i.e., the treatment sum of squares can be broken down with orthogonal contrasts in the usual way. Hint: Let \(\xi^{\prime}=\rho_{1}^{\prime}[X,Z]\) and \(\eta^{\prime}=\rho_{2}^{\prime}[X,Z]\). Show that

\[\rho_{1}^{\prime}(I-M)Z\left[Z^{\prime}(I-M)Z\right]^{-}Z^{\prime}(I-M)\rho_{2 }=0.\]

Suppose that the treatments have quantitative levels, say \(x_{1},\ldots,x_{t}\), that are equally spaced. Model (3) can be reparameterized as

\[y_{ij}=\mu+\beta_{i}+\gamma_{1}x_{j}+\gamma_{2}x_{j}^{2}+\cdots+\gamma_{t-1}x_ {j}^{t-1}+e_{ij}.\]

We would like to show that the orthogonal polynomial contrasts for the balanced incomplete block design are the same as for a balanced one-way ANOVA. In other words, tabled polynomial contrasts, which are useful in balanced ANOVAs, can also be used to analyze balanced incomplete block designs. More generally, the treatments in a BIB may have a factorial structure with quantitative levels in some factor (e.g., Example 9.4.1). We would like to establish that the polynomial contrasts in the factor can be used in the usual way to analyze the data.

Because this is a balanced incomplete block design, \(Z\) is the model matrix for a balanced one-way ANOVA (without a grand mean). As in Section 7.3, define orthogonal polynomials \(T=ZB\) by ignoring blocks. (Here we are not interested in \(J\) as an orthogonal polynomial, so we take \(B\) as a \(t\times t-1\) matrix.) Write \(B=[b_{1},\ldots,b_{t-1}]\). If treatments are levels of a single quantitative factor, then the \(b_{j}\)s are tabled orthogonal polynomial contrasts. If the treatments have factorial structure, the \(b_{j}\)s are obtained from tabled contrasts as in the continuation of Example 9.4.1 below. The important fact is that the \(b_{j}\)s are readily obtained. Note that \(J_{t}^{\prime}b_{j}=0\) for all \(j\), and \(b_{i}^{\prime}b_{j}=0\) for \(i\neq j\).

A model with treatments replaced by regression variables can be written \(Y=X\beta+T\eta+e\), where \(\eta=(\eta_{1},\ldots,\eta_{t-1})^{\prime}\). For a simple treatment structure, \(\eta_{j}\) would be the coefficient for a \(j\)th degree polynomial. For a factorial treatment structure, \(\eta_{j}\) could be the coefficient for some cross-product term. The key points are that the hypothesis \(\eta_{j}=0\) corresponds to some hypothesis that can be interpreted as in Section 6.7 or Section 7.3, and that the columns of \(T\) are orthogonal.

As we have seen, the model \(Y=X\beta+T\eta+e\) is equivalent to the model \(Y=X\delta+(I-M)T\eta+e\), where \(\eta\) is identical in the two models. Thus the test of \(\eta_{j}=0\) can be performed in the second model. In the second model, \(\hat{\eta}\) is independent of \(\hat{\delta}\) because of the orthogonality. If the columns of \((I-M)T\) are orthogonal, the estimates of the \(\eta_{j}\)s are independent. Finally, and most importantly, we can show that the contrast in the \(\tau\)s that corresponds to testing \(\eta_{j}=0\) is simply \(b^{\prime}_{j}\tau\), where \(\tau=(\tau_{1},\ldots,\tau_{t})^{\prime}\).

To show that the columns of \((I-M)T\) are orthogonal, it suffices to show that \(T^{\prime}(I-M)T\) is diagonal.

\[T^{\prime}(I-M)T = B^{\prime}Z^{\prime}(I-M)ZB\] \[= (\lambda t/k)B^{\prime}WB\] \[= (\lambda t/k)B^{\prime}B.\]

The last equality follows from the definition of \(W\) and the fact that \(J^{\prime}_{t}b_{j}=0\) for all \(j\). Note that \(B^{\prime}B\) is diagonal because \(b^{\prime}_{i}b_{j}=0\) for \(i\neq j\).

Finally, the contrast that corresponds to testing \(\eta_{j}=0\) is \(\rho^{\prime}(I-M)Z\tau\), where \(\rho\) is the \(j\)th column of \((I-M)T\), i.e., \(\rho=(I-M)Zb_{j}\). This is true because \((I-M)T\) has orthogonal columns. The contrast is then

\[[(I-M)Zb_{j}]^{\prime}(I-M)Z\tau = b^{\prime}_{j}Z^{\prime}(I-M)Z\tau\] \[= (\lambda t/k)b^{\prime}_{j}W\tau\] \[= (\lambda t/k)b^{\prime}_{j}\tau\]

or, equivalently, the contrast is

\[b^{\prime}_{j}\tau.\]

We now apply these results to the analysis of the data in Example 9.4.1.

Example 9.4.1 Continued.: The computation of the \(Q_{m}\)s is facilitated by the following table.

\[\begin{array}{lcccccccc}\text{Treatment}&n_{0}p_{0}&n_{0}p_{1}&n_{0}p_{2}&n_ {1}p_{0}&n_{1}p_{1}&n_{1}p_{2}\\ \hline\sum_{i\in A_{m}}y_{im}&1762.0&2149.0&2375.0&1989.0&2675.0&2939.0\\ \sum_{i\in A_{m}}\bar{y}_{i}.&2295.4&2316.0&2281.8&2241.8&2391.8&2362.2\\ Q_{m}&-533.4&-167.0&93.2&-252.8&283.2&576.8\end{array}\]

An analysis of variance table can be computed.

\begin{tabular}{l r r r r r}  & \multicolumn{4}{c}{ANOVA} \\ Source & \multicolumn{1}{c}{\(df\)} & \multicolumn{1}{c}{\(SS\)} & \multicolumn{1}{c}{\(MS\)} & \multicolumn{1}{c}{\(F\)} \\ \hline Blocks (Ignoring Trts) & 5 & 74857.77 & 14971.553 & \\ Treatments (After Blks) & 5 & 166228.98 & 33245.797 & 31.97 \\ Error & 19 & 19758.22 & 1039.906 & \\ \hline Total & 29 & 260844.97 & & & \\ \end{tabular}

Clearly, there are significant differences among the treatments. These can be explored further by examining contrasts. The factorial structure of the treatments suggests looking at nitrogen effects, phosphate effects, and interaction effects. With two levels of nitrogen, the only available contrast is \((1)n_{0}+(-1)n_{1}\). Phosphate was applied at quantitative levels 0, 1, and 2. The linear contrast in phosphate is \((-1)p_{0}+(0)p_{1}+(1)p_{2}\). The quadratic contrast in phosphate is \((1)p_{0}+(-2)p_{1}+(1)p_{2}\). Combining these to obtain interaction contrasts and rewriting them as contrasts in the original six treatments gives \(b_{1},\ldots,b_{5}\).

\begin{tabular}{l r r r r r r}  & \multicolumn{4}{c}{\(b_{j}\)s} \\ Treatments & \(n_{0}p_{0}\) & \(n_{0}p_{1}\) & \(n_{0}p_{2}\) & \(n_{1}p_{0}\) & \(n_{1}p_{1}\) & \(n_{1}p_{2}\) \\ \hline \(N\) & 1 & 1 & 1 & \(-1\) & \(-1\) & \(-1\) \\ \(P\) linear & \(-1\) & 0 & 1 & \(-1\) & 0 & 1 \\ \(P\) quadratic & 1 & \(-2\) & 1 & 1 & \(-2\) & 1 \\ \(N\)\(-\)\(P\) linear & \(-1\) & 0 & 1 & 1 & 0 & \(-1\) \\ \(N\)\(-\)\(P\) quadratic & 1 & \(-2\) & 1 & \(-1\) & 2 & \(-1\) \\ \end{tabular}

\begin{tabular}{l r r r r r} Source & \multicolumn{1}{c}{\(df\)} & \multicolumn{1}{c}{\(SS\)} & \multicolumn{1}{c}{\(F\)} \\ \hline \(N\) & 1 & 51207.20 & 49.24 & \\ \(P\) linear & 1 & 110443.67 & 106.21 & \\ \(P\) quadratic & 1 & 2109.76 & 2.03 & \\ \(N\)\(-\)\(P\) linear & 1 & 2146.30 & 2.06 & \\ \(N\)\(-\)\(P\) quadratic & 1 & 322.06 & 0.31 & \\ \end{tabular}

The conclusions to be drawn are clear. There is a substantial increase in yields due to adding the nitrogen-based fertilizer. For the dosages of phosphate used, there is a definite increasing linear relationship between amount of phosphate and yield of potatoes. There is no evidence of any interaction.

Note that the linear relationship between phosphate and yield is an approximation that holds in some neighborhood of the dosages used in the experiment. It is well known that too much fertilizer will actually kill most plants. In particular, no potato plant will survive having an entire truckload of phosphate dumped on it.

**Exercise 9.10**: Derive the analysis for a Latin square with one row missing. Hint: This problem is at the end of Section 9.4, not Section 9.3.

**Exercise 9.11**: Eighty wheat plants were grown in each of 5 different fields. Each of 6 individuals (\(A\), \(B\), \(C\), \(D\), \(E\), and \(F\)) were asked to pick 8 "representative"

[MISSING_PAGE_EMPTY:5588]

Now fit the model

\[Y=X\beta+\tilde{Z}\gamma+e\]

treating \(\tilde{Z}\) as a known model matrix that does not depend on \(Y\). Let \(\mathcal{P}\) be the perpendicular projection operator onto \(C(X,\tilde{Z})\) and write \(\mathcal{P}=M+\tilde{M}_{2}\) as in Section 1. The usual \(F\) test for \(H_{0}:\gamma=0\) is based on

\[F=\frac{Y^{\prime}\tilde{M}_{2}Y/r[(I-M)\tilde{Z}]}{Y^{\prime}(I-\mathcal{P})Y /[n-r(X,\tilde{Z})]}\sim F(r[(I-M)\tilde{Z}],n-r[X,\tilde{Z}]). \tag{2}\]

To show that (2) really holds under the null hypothesis of model (1), consider the distribution of \(Y\) given \(\tilde{Y}\). Write

\[Y=MY+(I-M)Y=\tilde{Y}+(I-M)Y.\]

Under the null model, \(\tilde{Y}\) and \((I-M)Y\) are independent and

\[(I-M)Y\sim N[0,\sigma^{2}(I-M)],\]

so

\[Y|\tilde{Y}\sim N[\tilde{Y},\sigma^{2}(I-M)].\]

Use the general results from Section 1.3 that involve checking conditions like _VAVAV_ = _VAV_ and _VAVBV_ = 0 to establish that the \(F\) statistic has the stated \(F(r[(I-M)\tilde{Z}],n-r[X,\tilde{Z}])\) distribution conditional on \(\tilde{Y}\). Finally, by assumption, the degrees of freedom for the \(F\) distribution do not depend on \(\tilde{Y}\), so the conditional distribution does not depend on \(\tilde{Y}\) and it must also be the unconditional distribution.

##### Exercise 9.12

Prove that display (2) is true.

##### Additional Exercises

##### Exercise 9.6.1

Sulzberger (1953) and Williams (1959) examined the maximum compressive strength parallel to the grain (\(y\)) of 10 hoop trees and how it was affected by temperature. A covariate, the moisture content of the wood (\(x\)), was also measured. Analyze the data, which are reported below 

**Exercise 9.6.2**  Suppose that in Exercise 7.7.1 on motor oil pricing, the observation on store 7, brand \(H\) was lost. Treat the stores as blocks in a randomized complete block design. Plug in an estimate of the missing value and analyze the data without correcting the \(MSTrts\) or any variance estimates. Compare the results of this approximate analysis to the results of a correct analysis.

**Exercise 9.6.3**  The missing value procedure that consists of analyzing the model \((Y-Z\dot{\gamma})=X\beta+e\) has been shown to give the correct _SSE_ and BLUEs; however, sums of squares explained by the model are biased upwards. For a randomized complete block design with \(a\) treatments and \(b\) blocks and the observation in the \(c\), \(d\) cell missing, show that the correct mean square for treatments is the naive (biased) mean square treatments minus \([y_{\cdot d}-(a-1)\hat{y}_{cd}]^{2}/a(a-1)^{2}\), where \(y_{\cdot d}\) is the sum of all actual observations in block \(d\), and \(\hat{y}_{cd}\) is the pseudo-observation (the nonzero element of \(Z\dot{\gamma}\)).

**Exercise 9.6.4**  State whether each design given below is a balanced incomplete block design, and if so, give the values of \(b\), \(t\), \(k\), \(r\), and \(\lambda\).

(a) The experiment involves 5 treatments: \(A\), \(B\), \(C\), \(D\), and \(E\). The experiment is laid out as follows.

\begin{tabular}{c c|c c} Block & Treatments & Block & Treatments \\ \hline
1 & \(A\), \(B\), \(C\) & 6 & \(A\), \(B\), \(D\) \\
2 & \(A\), \(B\), \(E\) & 7 & \(A\), \(C\), \(D\) \\
3 & \(A\), \(D\), \(E\) & 8 & \(A\), \(C\), \(E\) \\
4 & \(B\), \(C\), \(D\) & 9 & \(B\), \(C\), \(E\) \\
5 & \(C\), \(D\), \(E\) & 10 & \(B\), \(D\), \(E\) \\ \end{tabular}

## References

* Christensen (1996) Christensen, R. (1996). _Analysis of variance, design, and regression: Applied statistical methods_. London: Chapman and Hall.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* Christensen & Utts (1992) Christensen, R., & Utts, J. (1992). Testing for nonadditivity in log-linear and logit models. _Journal of Statistical Planning and Inference_, _33_, 333-343.
* Cox (1958) Cox, D. R. (1958). _Planning of experiments_. New York: Wiley.
* LaMotte (2014) LaMotte, L. R. (2014). The Gram-Schmidt construction as a basis for linear models. _The American Statistician_, _68_, 52-55.
* Mandel (1961) Mandel, J. (1961). Nonadditivity in two-way analysis of variance. _Journal of the American Statistical Association_, _56_, 878-888.
* Mandel (1971) Mandel, J. (1971). A new analysis of variance model for nonadditive data. _Technometrics_, _13_, 1-18.
* Milliken & Graybill (1970) Milliken, G. A., & Graybill, F. A. (1970). Extensions of the general linear hypothesis model. _Journal of the American Statistical Association_, _65_, 797-807.
* Rao (1973) Rao, C. R. (1973). _Linear statistical inference and its applications_ (2nd ed.). New York: Wiley.
* Snedecor & Cochran (1980) Snedecor, G. W., & Cochran, W. G. (1980). _Statistical methods_ (7th ed.). Ames: Iowa State University Press.
* St Laurent (1990) St. Laurent, R. T. (1990). The equivalence of the Milliken-Graybill procedure and the score test. _The American Statistician_, _44_, 36-37.
* Sulzberger (1953) Sulzberger, P. H. (1953). The effects of temperature on the strength of wood, plywood and glued joints. Department of supply, Report ACA-46, Aeronautical Research Consultative Committee, Australia.
* Tukey (1949) Tukey, J. W. (1949). One degree of freedom for nonadditivity. _Biometrics_, \(5\), 232-242.
* Williams (1959) Williams, E. J. (1959). _Regression analysis_. New York: Wiley.

## Chapter 10 General Gauss-Markov Models

Section 2 contains a discussion of the geometry of estimation for category (d). In particular, it points out the need for a consistency criterion and the crucial role of projection operators in linear unbiased estimation. Section 3 examines the problem of testing a model against a reduced model for category (d). Section 4 discusses the extension of least squares estimation to category (d) in light of the consistency requirement of Section 2. Section 4 also contains the very important result that least squares estimates are BLUEs if and only if \(C(V\!X)\subset C(X)\).

Section 5 considers estimable parameters that can be known with certainty when \(C(X)\not\subset C(V)\) and a relatively simple way to estimate estimable parameters that are not known with certainty. Some of the nastier parts in Sections 1 through 4 are those that provide sufficient generality to allow \(C(X)\not\subset C(V)\). The simpler approach of Section 5 seems to obviate the need for much of that. Gross (2004) surveyed important results in linear models with possibly singular covariance matrices.

### BLUEs with an Arbitrary Covariance Matrix

Consider the model

\[Y=X\beta+e,\quad\text{E}(e)=0,\quad\text{Cov}(e)=\sigma^{2}V, \tag{1}\]

where \(V\) is a known matrix. We want to find the best linear unbiased estimate of \(\text{E}(Y)\).

**Definition 10.1.1**.: Let \(\Lambda^{\prime}\) be an \(r\times p\) matrix with \(\Lambda^{\prime}=P^{\prime}X\) for some \(P\). An estimate \(B_{0}Y\) is called a _best linear unbiased estimate_ (_BLUE_) of \(\Lambda^{\prime}\beta\) if

1. \(\text{E}(B_{0}Y)=\Lambda^{\prime}\beta\) for any \(\beta\), and
2. if \(BY\) is any other linear unbiased estimate of \(\Lambda^{\prime}\beta\), then for any \(r\times 1\) vector \(\xi\) \[\text{Var}(\xi^{\prime}B_{0}Y)\leq\text{Var}(\xi^{\prime}BY).\]

**Exercise 10.1**.: Show that \(A_{0}Y\) is a BLUE of \(X\beta\) if and only if, for every estimable function \(\lambda^{\prime}\beta\) such that \(\rho^{\prime}X=\lambda^{\prime}\), \(\rho^{\prime}A_{0}Y\) is a BLUE of \(\lambda^{\prime}\beta\).

**Exercise 10.2**.: Show that if \(\Lambda^{\prime}=P^{\prime}X\) and if \(A_{0}Y\) is a BLUE of \(X\beta\), then \(P^{\prime}A_{0}Y\) is a BLUE of \(\Lambda^{\prime}\beta\).

In the case of a general covariance matrix \(V\), it is a good idea to reconsider what the linear model (1) is really saying. The obvious thing it is saying is that \(\text{E}(Y)\in C(X)\). From Lemma 1.3.5, the model also says that \(e\in C(V)\) or, in other notation, \(Y\in C(X,\,V)\). If \(V\) is a nonsingular matrix, \(C(V)=\mathbf{R}^{n}\); so the conditions on \(e\) and \(Y\) are really meaningless. When \(V\) is a singular matrix, the conditions on \(e\) and \(Y\) are extremely important.

For any matrix \(A\), let \(M_{A}\) denote the perpendicular projection matrix onto \(C(A)\). \(M\) without a subscript is \(M_{X}\). Any property that holds with probability 1 will be said to hold almost surely (a.s.). For example, Lemma 1.3.5 indicates that \(e\in C(V)\) a.s. and, adding \(X\beta\) to \(Y-X\beta\), the lemma gives \(Y\in C(X,\,V)\) a.s.

If \(R\) and \(S\) are any two random vectors with \(R=S\) a.s., then \(\mathrm{E}(R)=\mathrm{E}(S)\) and \(\mathrm{Cov}(R)=\mathrm{Cov}(S)\). In particular, if \(R=S\) a.s. and \(R\) is a BLUE, then \(S\) is also a BLUE.

Results on estimation will be established by comparing the estimation problem in model (1) to the estimation problem in two other, more easily analyzed, models.

Before proceeding to the first theorem on estimation, recall the eigenvector decomposition of the symmetric, nonnegative definite matrix \(V\). One can pick matrices \(E\) and \(D\) so that \(VE=ED\). Here \(D=\mathrm{Diag}(d_{i})\), where the \(d_{i}\)s are the, say \(m\), positive eigenvalues of \(V\) (with the correct multiplicities). \(E\) can be chosen as a matrix of orthonormal columns with the \(i\)th column an eigenvector corresponding to \(d_{i}\). Define \(D^{1/2}\equiv\mathrm{Diag}\bigl{(}\sqrt{d_{i}}\bigr{)}\). Write

\[Q\equiv ED^{1/2}\quad\text{and}\quad Q^{-}\equiv D^{-1/2}E^{\prime}.\]

Useful facts are

1. \(C(V)=C(E)=C(Q)\)
2. \(M_{V}=EE^{\prime}=QQ^{-}\)
3. \(I_{m}=Q^{-}Q\)
4. \(V=QQ^{\prime}\)
5. \(QQ^{-}Q=Q\)
6. \(Q^{-}VQ^{-^{\prime}}=I_{m}\)
7. \(Q^{-^{\prime}}Q^{-}=V^{-}\).

Consider the linear model

\[Q^{-}Y=Q^{-}X\beta+Q^{-}e,\quad\mathrm{E}(Q^{-}e)=0,\quad\mathrm{Cov}(Q^{-}e) =\sigma^{2}I_{m}. \tag{2}\]

Models (1) and (2) are equivalent when \(C(X)\subset C(V)\). Clearly, (2) can be obtained from (1) by multiplying on the left by \(Q^{-}\). Moreover, with \(C(X)\subset C(V)\), each of \(Y\), \(C(X)\), and \(e\) are contained in \(C(V)\) a.s.; so multiplying (2) on the left by \(Q\) gives \(M_{V}Y=M_{V}X\beta+M_{V}e\), which is model (1) a.s. Note that \(Q^{-}Y\in\mathbf{R}^{m}\) and that the Gauss-Markov theorem can be used to get estimates in model (2). Moreover, if \(C(X)\subset C(V)\), then \(X=M_{V}X=QQ^{-}X\); so \(X\beta\) is estimable in model (2).

**Theorem 10.1.2**: _If \(C(X)\subset C(V)\), then \(A_{0}Y\) is a BLUE of \(X\beta\) in model (1) if and only if \((A_{0}Q)\,Q^{-}Y\) is a BLUE of \(X\beta\) in model (2)._

_Proof_ If this theorem is to make any sense at all, we need to first show that \(\mathrm{E}(A_{0}Y)=X\beta\) iff \(\mathrm{E}(A_{0}QQ^{-}Y)=X\beta\). Recall that \(Y\in C(X,\,V)\) a.s., so in this special case where \(C(X)\subset C(V)\), we have \(Y\in C(V)\) a.s. Thus, for the purposes of finding expected values and covariances, we can assume that \(Y=M_{V}Y\). Let \(B\) be an arbitrary \(n\times n\) matrix. Then \(\mathrm{E}(BY)=\mathrm{E}(BM_{V}Y)=\mathrm{E}(BQQ^{-}Y)\), so \(\mathrm{E}(A_{0}Y)=X\beta\) iff \(\mathrm{E}(A_{0}QQ^{-}Y)=X\beta\). It is also handy to know the following fact:

\[\mathrm{Var}(\rho^{{}^{\prime}}BY)=\mathrm{Var}(\rho^{{}^{\prime}}BM_{V}Y)= \mathrm{Var}(\rho^{{}^{\prime}}BQQ^{-}Y).\]

Now suppose that \(A_{0}Y\) is a BLUE for \(X\beta\) in model (1). We show that \(A_{0}QQ^{-}Y\) is a BLUE of \(X\beta\) in model (2). Let \(BQ^{-}Y\) be another unbiased estimate of \(X\beta\) in model (2). Then \(X\beta=\mathrm{E}(BQ^{-}Y)\), so \(BQ^{-}Y\) is an unbiased estimate of \(X\beta\) in model (1) and, since \(A_{0}Y\) is a BLUE in model (1), \(\mathrm{Var}(\rho^{{}^{\prime}}A_{0}QQ^{-}Y)=\mathrm{Var}(\rho^{{}^{\prime}}A _{0}Y)\leq\mathrm{Var}(\rho^{{}^{\prime}}B\,Q^{-}Y)\). Thus \(A_{0}QQ^{-}Y\) is a BLUE of \(X\beta\) in model (2).

Conversely, suppose that \(A_{0}QQ^{-}Y\) is a BLUE of \(X\beta\) in model (2). Let \(BY\) be an unbiased estimate for \(X\beta\) in model (1). Then \(BQQ^{-}Y\) is unbiased for \(X\beta\) in model (2) and \(\mathrm{Var}(\rho^{{}^{\prime}}A_{0}Y)=\mathrm{Var}(\rho^{{}^{\prime}}A_{0}QQ^ {-}Y)\leq\mathrm{Var}(\rho^{{}^{\prime}}BQQ^{-}Y)=\mathrm{Var}(\rho^{{}^{\prime }}BY)\); so \(\rho^{{}^{\prime}}A_{0}Y\) is a BLUE of \(X\beta\) in model (1). 

Note that with \(C(X)\subset C(V)\), \(A_{0}Y=A_{0}M_{V}Y=A_{0}QQ^{-}Y\) a.s., so Theorem 10.1.2 is really saying that \(A_{0}Y\) is a BLUE in model (1) if and only if \(A_{0}Y\) is a BLUE in model (2). The virtue of Theorem 10.1.2 is that we can actually find a BLUE for \(X\beta\) in model (2). From Exercises 10.1 and 10.2, a BLUE of \(X\beta=QQ^{-}X\beta\) from model (2) is

\[X\hat{\beta} = QM_{Q^{-}X}Q^{-}Y \tag{3}\] \[= Q\left[Q^{-}X(X^{\prime}Q^{-\prime}Q^{-}X)^{-}X^{\prime}Q^{- \prime}\right]Q^{-}Y\] \[= M_{V}X(X^{\prime}V^{-}X)^{-}X^{\prime}V^{-}Y\] \[= X(X^{\prime}V^{-}X)^{-}X^{\prime}V^{-}Y.\]

It is useful to observe that we can get a BLUE from any choice of \(V^{-}\) and \((X^{\prime}V^{-}X)^{-}\). First, notice that \(X^{\prime}V^{-}X\) does not depend on \(V^{-}\). Since \(C(X)\subset C(V)\), we can write \(X=VC\) for some matrix \(C\). Then \(X^{\prime}V^{-}X=C^{\prime}VV^{-}VC=C^{\prime}VC\). Second, \(Q^{-}X(X^{\prime}Q^{-\prime}Q^{-}X)^{-}X^{\prime}Q^{-\prime}\) does not depend on the choice of \((X^{\prime}Q^{-\prime}Q^{-}X)^{-}\). Therefore, \(X(X^{\prime}V^{-}X)^{-}X^{\prime}V^{-}\) does not depend on the choice of \((X^{\prime}V^{-}X)^{-}\). Moreover, for any \(Y\in C(V)\), \(X(X^{\prime}V^{-}X)^{-}X^{\prime}V^{-}Y\) does not depend on the choice of \(V^{-}\). To see this, write \(Y=Vb\). Then \(X^{\prime}V^{-}Y=(C^{\prime}V)V^{-}(Vb)=C^{\prime}Vb\). Since \(Y\in C(V)\) a.s., \(X(X^{\prime}V^{-}X)^{-}X^{\prime}V^{-}Y\) is a BLUE of \(X\beta\) for any choices of \(V^{-}\) and \((X^{\prime}V^{-}X)^{-}\).

To obtain the general estimation result for arbitrary singular \(V\), consider the linear model

\[Y_{1}=X\beta+e_{1},\quad\mathrm{E}(e_{1})=0,\quad\mathrm{Cov}(e_{1})=\sigma^{ 2}\left(V+XUX^{\prime}\right), \tag{4}\]

where \(U\) is any symmetric nonnegative definite matrix.

**Theorem 10.1.3**: \(A_{0}Y\) _is a BLUE for \(X\beta\) in model (1) if and only if \(A_{0}Y_{1}\) is a BLUE for \(X\beta\) in model (4)._Proof.: Clearly, for any matrix \(B,BY\) is unbiased if and only if \(BY_{1}\) is unbiased and both are equivalent to the condition \(BX=X\). In the remainder of the proof, \(B\) will be an arbitrary matrix with \(BX=X\) so that \(BY\) and \(BY_{1}\) are arbitrary linear unbiased estimates of \(X\beta\) in models (1) and (4), respectively. The key fact in the proof is that

\[\begin{array}{rcl}\mbox{Var}(\rho^{\prime}BY_{1})&=&\sigma^{2}\rho^{\prime}B \left(V+XUX^{\prime}\right)B^{\prime}\rho\\ &=&\sigma^{2}\rho^{\prime}BVB^{\prime}\rho+\rho^{\prime}BXUX^{\prime}B^{\prime }\rho\\ &=&\mbox{Var}(\rho^{\prime}BY)+\sigma^{2}\rho^{\prime}XUX^{\prime}\rho.\end{array}\]

Now suppose that \(A_{0}Y\) is a BLUE for \(X\beta\) in model (1). Then

\[\mbox{Var}(\rho^{\prime}A_{0}Y)\leq\mbox{Var}(\rho^{\prime}BY).\]

Adding \(\sigma^{2}\rho^{\prime}XUX^{\prime}\rho\) to both sides we get

\[\mbox{Var}(\rho^{\prime}A_{0}Y_{1})\leq\mbox{Var}(\rho^{\prime}BY_{1}),\]

and since \(BY_{1}\) is an arbitrary linear unbiased estimate, \(A_{0}Y_{1}\) is a BLUE.

Conversely, suppose \(A_{0}Y_{1}\) is a BLUE for \(X\beta\) in model (4). Then

\[\mbox{Var}(\rho^{\prime}A_{0}Y_{1})\leq\mbox{Var}(\rho^{\prime}BY_{1}),\]

or

\[\mbox{Var}(\rho^{\prime}A_{0}Y)+\sigma^{2}\rho^{\prime}XUX^{\prime}\rho\leq \mbox{Var}(\rho^{\prime}BY)+\sigma^{2}\rho^{\prime}XUX^{\prime}\rho.\]

Subtracting \(\sigma^{2}\rho^{\prime}XUX^{\prime}\rho\) from both sides we get

\[\mbox{Var}(\rho^{\prime}A_{0}Y)\leq\mbox{Var}(\rho^{\prime}BY),\]

so \(A_{0}Y\) is a BLUE. 

As with Theorem 10.1.2, this result is useful because a BLUE for \(X\beta\) can actually be found in one of the models. Let \(T=V+XUX^{\prime}\). If \(U\) is chosen so that \(C(X)\subset C(T)\), then Theorem 10.1.2 applies to model (4) and a BLUE for \(X\beta\) is \(X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}Y\). Exercise 10.3 establishes that such matrices \(U\) exist. Since this is an application of Theorem 10.1.2, \(X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}Y\) does not depend on the choice of \((X^{\prime}T^{-}X)^{-}\) and, for \(Y\in C(T)\), it does not depend on the choice of \(T^{-}\). Proposition 10.1.4 below shows that \(C(X)\subset C(T)\) implies \(C(T)=C(X,\,V)\); so \(Y\in C(T)\) a.s., and any choice of \(T^{-}\) gives a BLUE.

Exercise 10.3.: The BLUE of \(X\beta\) can be obtained by taking \(T=V+XX^{\prime}\). Prove this by showing that

1. \(C(X)\subset C(T)\) if and only if \(TT^{-}X=X\), and 2. if \(T=V+XX^{\prime}\), then \(TT^{-}X=X\).

Hint: Searle and Pukelsheim (1987) base a proof of (b) on

\[\begin{array}{l}0=(I-TT^{-})T(I-TT^{-})^{\prime}\\ \phantom{0}=(I-TT^{-})V(I-TT^{-})^{\prime}+(I-TT^{-})XX^{\prime}(I-TT^{-})^{ \prime}\end{array}\]

and the fact that the last term is the sum of two nonnegative definite matrices.

**Proposition 10.1.4**: _If \(C(X)\subset C(T)\), then \(C(V)\subset C(T)\) and \(C(X,\,V)=C(T)\)._

_Proof_ We know that \(C(X)\subset C(T)\), so \(X=TG\) for some matrix \(G\). If \(v\in C(V)\), then \(v=Vb_{1}\), and \(v=Vb_{1}+XUX^{\prime}b_{1}-XUX^{\prime}b_{1}=Tb_{1}-TG(UX^{\prime}b_{1})\); so \(v\in C(T)\). Thus, \(C(X,\,V)\subset C(T)\). But clearly, by the definition of \(T\), \(C(T)\subset C(X,\,V)\); so we have \(C(X,\,V)=C(T)\). \(\square\)

To complete our characterization of best linear unbiased estimates, we will show that for practical purposes BLUEs of \(X\beta\) are unique.

**Theorem 10.1.5**: _Let \(AY\) and \(BY\) be BLUEs of \(X\beta\) in model (1), then \(\Pr(AY=BY)=1\)._

_Proof_ It is enough to show that \(\Pr[(A-B)Y=0]=1\). We need only observe that \(\operatorname{E}[(A-B)Y]=0\) and show that for any \(\rho\), \(\operatorname{Var}[\rho^{\prime}(A-B)Y]=0\).

Remembering that \(\operatorname{Var}(\rho^{\prime}AY)=\operatorname{Var}(\rho^{\prime}BY)\), we first consider the unbiased estimate of \(X\beta\), \(\frac{1}{2}(A+B)Y\):

\[\operatorname{Var}(\rho^{\prime}AY)\leq\operatorname{Var}\biggl{(}\rho^{ \prime}\frac{1}{2}(A+B)Y\biggr{)}\]

but

\[\operatorname{Var}\biggl{(}\rho^{\prime}\frac{1}{2}(A+B)Y\biggr{)}=\frac{1}{4 }\left[\operatorname{Var}(\rho^{\prime}AY)+\operatorname{Var}(\rho^{\prime}BY )+2\operatorname{Cov}(\rho^{\prime}AY,\,\rho^{\prime}BY)\right],\]

so

\[\operatorname{Var}(\rho^{\prime}AY)\leq\frac{1}{2}\operatorname{Var}(\rho^{ \prime}AY)+\frac{1}{2}\operatorname{Cov}(\rho^{\prime}AY,\,\rho^{\prime}BY).\]

Simplifying, we find that

\[\operatorname{Var}(\rho^{\prime}AY)\leq\operatorname{Cov}(\rho^{\prime}AY,\, \rho^{\prime}BY).\]

Now look at \(\operatorname{Var}\bigl{(}\rho^{\prime}(A-B)Y\bigr{)}\),\(0\leq\operatorname{Var}\bigl{(}\rho^{\prime}(A-B)Y\bigr{)}=\operatorname{Var}( \rho^{\prime}AY)+\operatorname{Var}(\rho^{\prime}BY)-2\operatorname{Cov}(\rho^{ \prime}AY,\,\rho^{\prime}BY)\leq 0\). 

Finally, the most exact characterization of a BLUE is the following:

**Theorem 10.1.6**: _If \(AY\) and \(BY\) are BLUEs for \(X\beta\) in model (1), then \(AY=BY\) for any \(Y\in C(X,\,V)\)._

_Proof_ It is enough to show that \(AY=BY\), first when \(Y\in C(X)\) and then when \(Y\in C(V)\). When \(Y\in C(X)\), by unbiasedness \(AY=BY\).

We want to show that \(AY=BY\) for all \(Y\in C(V)\). Let \(\mathcal{M}=\{Y\in C(V)|AY=BY\}\). It is easily seen that \(\mathcal{M}\) is a vector space, and clearly \(\mathcal{M}\subset C(V)\). If \(C(V)\subset\mathcal{M}\), then \(\mathcal{M}=C(V)\), and we are done.

From unbiasedness, \(AY=X\beta+Ae\) and \(BY=X\beta+Be\), so \(AY=BY\) if and only if \(Ae=Be\). From Theorem 10.1.5, \(\operatorname{Pr}(Ae=Be)=1\). We also know that \(\operatorname{Pr}(e\in C(V))=1\). Therefore, \(\operatorname{Pr}(e\in\mathcal{M})=1\).

Computing the covariance of \(e\) we find that

\(\sigma^{2}V=\int ee^{\prime}dP=\int_{e\in\mathcal{M}}ee^{\prime}dP\);

so, by Exercise 10.4, \(C(V)\subset\mathcal{M}\). 

**Exercise 10.4**: Ferguson (1967, Section 2.7, page 74) proves the following:

**Lemma (3)**: _If \(S\) is a convex subset of \(\mathbb{R}^{n}\), and \(Z\) is an \(n\)-dimensional random vector for which \(\operatorname{Pr}(Z\in S)=1\) and for which \(\operatorname{E}(Z)\) exists and is finite, then \(\operatorname{E}(Z)\in S\)._

Use this lemma to show that

\(C(V)=C\biggl{[}\int_{e\in\mathcal{M}}ee^{\prime}dP\biggr{]}\subset\mathcal{M}\).

After establishing Theorem 10.1.5, we only need to find any one BLUE, because for any observations that have a chance of happening, all BLUEs give the same estimates. Fortunately, we have already shown how to obtain a BLUE, so this section is finished. There is only one problem. Best linear unbiased estimates might be pure garbage. We pursue this issue in the next section.

### Geometric Aspects of Estimation

The linear model

\[Y=X\beta+e,\ \ \ \ \mathrm{E}(e)=0,\ \ \ \ \mathrm{Cov}(e)=\sigma^{2}V, \tag{1}\]

says two important things:

1. \(\mathrm{E}(Y)=X\beta\in C(X)\), and
2. \(\Pr(e\in C(V))=1\).

Note that (b) also says something about \(\mathrm{E}(Y)\):

1. \(X\beta=Y-e\in Y+C(V)\) a.s.

Intuitively, any reasonable estimate of \(\mathrm{E}(Y)\), say \(X\hat{\beta}\), should satisfy the following definition for consistency.

**Definition 10.2.1**: An estimate \(\widehat{X\beta}\) of \(X\beta\) is called a _consistent_ estimate if

1. \(\widehat{X\beta}\in C(X)\) for any \(Y\), and
2. \(\widehat{X\beta}\in Y+C(V)\) for any \(Y\in C(X,V)\).

\(\widehat{X\beta}\) is called _almost surely consistent_ if conditions (i) and (ii) hold almost surely.

Note that this concept of consistency is distinct from the usual large sample idea of consistency. The idea is that a consistent estimate, in our sense, is consistent with respect to conditions (a) and (b\({}^{\prime}\)).

_Example 10.2.2_: Consider the linear model determined by

\[X=\left[\begin{array}{cc}1&0\\ 0&1\\ 0&0\end{array}\right]\ \ \ \ \ \text{and}\ \ \ \ \ V=\left[\begin{array}{cc} \frac{1}{2}&0&\frac{1}{2}\\ 0&1&0\\ \frac{1}{2}&0&\frac{1}{2}\end{array}\right].\]

If this model is graphed with coordinates \((x,\,y,\,z)\), then \(C(X)\) is the \(x,\,y\) plane and \(C(V)\) is the plane determined by the \(y\)-axis and the line [\(x=z,\,y=0\)]. (See Figure 10.1.)

Suppose that \(Y=(7,\,6,\,2)^{\prime}\). Then (see Figure 10.2) \(\mathrm{E}(Y)\) is in \(C(X)\) (the \(x,\,y\) plane) and also in \(Y+C(V)\) (which is the plane \(C(V)\) in Figure 10.1, translated over until it contains \(Y\)). The intersection of \(C(X)\) and \(Y+C(V)\) is the line [\(x=5,\,z=0\)], so any consistent estimate of \(X\beta\) will be in the line [\(x=5,\,z=0\)]. To see this, note that \(C(X)\) consists of vectors with the form \((a,\,b,\,0)^{\prime}\), and \(Y+C(V)\) consists of vectors like \((7,\,6,\,2)^{\prime}+(c,\,d,\,c)^{\prime}\). The intersection is those vectors with \(c=-2\), so they are of the form \((5,\,6+d,\,0)^{\prime}\) or \((5,\,b,\,0)^{\prime}\).

The problem with BLUEs of \(X\beta\) is that there is no apparent reason why a BLUE should be consistent. The class of linear unbiased estimates (LUEs) is very broad. It consists of all estimates \(AY\) with \(AX=X\). There are many linear unbiased estimates that are not consistent. For example, \(Y\) itself is a LUE and it satisfies condition (ii) of consistency; however, one would certainly not want to use it as an estimate.

Figure 10.2: Consistent estimation for Example 10.2.2

Figure 10.1: Estimation space and singular covariance space for Example 10.2.2

Example 10.2.2: Continued._MY_ is a LUE and satisfies condition (i) of consistency; however, with \(Y=(7,6,2)^{\prime}\), \(MY=(7,6,0)^{\prime}\), but \((7,6,0)^{\prime}\) is not in \(C(X)\cap[Y+C(V)]=[x=5,z=0]\). (See Figure 10.2.)

Before showing that BLUEs of \(X\beta\) are almost surely consistent, several _observations_ will be made. The object is to explicate the case \(C(X)\subset C(V)\), give alternative conditions that can be used in place of conditions (i) and (ii) of consistency, and display the importance of projections in linear estimation.

1. In practice, when a covariance matrix other than \(\sigma^{2}I\) is appropriate, most of the time the condition \(C(X)\subset C(V)\) will be satisfied. When \(C(X)\subset C(V)\), then \(Y\in C(X,V)=C(V)\) a.s., so the condition \(\widehat{X\beta}\in Y+C(V)\) a.s. merely indicates that \(\widehat{X\beta}\in C(V)\) a.s. In fact, any estimate of \(X\beta\) satisfying condition (i) of consistency also satisfies condition (ii). (Note: The last claim is not restricted to linear estimates or even unbiased estimates.)
2. An estimate \(AY\) satisfies condition (i) of consistency if and only if \(A=XB\) for some matrix \(B\).
3. An estimate \(AY\) satisfies condition (ii) of consistency if and only if \((I-A)Y\in C(V)\) for any \(Y\in C(X,V)\).
4. If \(AY\) is a LUE of \(X\beta\), then \(AY\) satisfies condition (ii) of consistency if and only if \(AY\in C(V)\) for any \(Y\in C(V)\), i.e., iff \(C(AV)\subset C(V)\). _Proof_\(Y\in C(X,V)\) iff \(Y=x+v\) for \(x\in C(X)\) and \(v\in C(V)\). \((I-A)Y=(x-x)+(v-Av)\). \(v-Av\in C(V)\) iff \(Av\in C(V)\). \(\Box\)
5. \(AY\) is a LUE satisfying condition (i) if and only if \(A\) is a projection matrix (not necessarily perpendicular) onto \(C(X)\). _Proof_ From unbiasedness, \(AX=X\), and from (b), \(A=XB\). Hence \(AA=AXB=XB=A\), and so \(A\) is idempotent and a projection onto \(C(A)\). Since \(AX=X\), we have \(C(X)\subset C(A)\); and since \(A=XB\), we have \(C(A)\subset C(X)\). Therefore, \(C(A)=C(X)\). \(\Box\)

**Exercise 10.5**: Prove observations (a), (b), and (c).

Notice that in general all consistent linear unbiased estimates (CLUEs) are projections onto \(C(X)\) and that if \(C(X)\subset C(V)\), all projections onto \(C(X)\) are CLUEs. This goes far to show the importance of projection matrices in estimation. In particular, the BLUEs that we actually found in the previous section satisfy condition (i) by observation (b). Thus, by observation (e) they are projections onto \(C(X)\).

Before proving the main result, we need the following proposition:

Proposition 10.2.3: _For \(T=V+XUX^{\prime}\) with \(U\) nonnegative definite and \(A=X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}\) with \(C(X)\subset C(T)\), \(AV=VA^{\prime}\)._Proof.: Consider the symmetric matrix \(X(X^{\prime}T^{-}X)^{-}X^{\prime}\), where the generalized inverses are taken as in Corollary B.41. By the choice of \(T\), write \(X=TG\).

\[X^{\prime} = G^{\prime}T=G^{\prime}TT^{-}T=X^{\prime}T^{-}T,\] \[X(X^{\prime}T^{-}X)^{-}X^{\prime} = X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}T=A(V+X\!UX^{\prime}).\]

Since \(A\) is a projection operator onto \(C(X)\), \(AXUX^{\prime}=X\!UX^{\prime}\); thus,

\[X(X^{\prime}T^{-}X)^{-}X^{\prime}-X\!UX^{\prime}=AV.\]

The lefthand side is symmetric. 

**Proposition 10.2.4**.: _There exists a BLUE of \(X\beta\) in model (1) that is a CLUE._

Proof.: \(AY=X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}Y\) is a BLUE satisfying condition (i). In Proposition 10.2.3, we proved that \(AV=VA^{\prime}\). By observation (d), the proposition holds. 

**Theorem 10.2.5**.: _If \(AY\) is a BLUE of \(X\beta\) in model (1), then \(AY\) is almost surely consistent._

Proof.: By Proposition 10.2.4, a consistent BLUE exists. By Theorem 10.1.5, any BLUE must almost surely satisfy consistency. 

### Hypothesis Testing

We consider the problem of testing the model

\[Y=X\beta+e,\ \ \ e\sim N\big{(}0,\sigma^{2}V\big{)} \tag{1}\]

against the reduced model

\[Y=X_{0}\gamma+e,\ \ \ e\sim N\big{(}0,\sigma^{2}V\big{)}\,, \tag{2}\]

where \(C(X_{0})\subset C(X)\). In particular, we use the approach of looking at reductions in the sum of squares for error. First we need to define what we mean by the sum of squares for error.

In the remainder of this section, let \(A=X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}\), where \(T^{-}\) and \((X^{\prime}T^{-}X)^{-}\) are chosen (for convenience) as in Corollary B.41 and as usual \(T=V+X\!UX^{\prime}\) for some nonnegative definite matrix \(U\) such that \(C(X)\subset C(T)\). \(AY\) is a BLUE of \(X\beta\) in model (1). We will define the sum of squares for error (_SSE_) in model (1) as

\[SSE=Y^{\prime}(I-A)^{\prime}T^{-}(I-A)Y. \tag{3}\]

The idea of using a quadratic form in the residuals to estimate the error is reasonable, and any quadratic form in the residuals, when normalized, gives an unbiased estimate of \(\sigma^{2}\). The terminology _SSE_ literally comes from the use of the quadratic form \(Y^{\prime}(I-A)^{\prime}(I-A)Y\). In particular, if \(V=I\), then

\[Y^{\prime}(I-A)^{\prime}(I-A)Y/\sigma^{2}=Y^{\prime}(I-A)^{\prime}[\sigma^{2}I ]^{-1}(I-A)Y\]

has a \(\chi^{2}\) distribution and is independent of \(AY\). For an arbitrary covariance matrix, an analogous procedure would be to use \(Y^{\prime}(I-A)^{\prime}V^{-}(I-A)Y\) to get an estimate of \(\sigma^{2}\) and develop tests. To simplify computations, we have chosen to define _SSE_ as in (3). However, we will show that for \(Y\in C(X,\,V)\), _SSE_\(=Y^{\prime}(I-A)^{\prime}V^{-}(I-A)Y\) for any choice of \(V^{-}\).

The sum of squares error in model (2) (_SSE_\({}_{0}\)) is defined similarly. Let \(T_{0}=V+X_{0}U_{0}X^{\prime}_{0}\) for a nonnegative definite matrix \(U_{0}\) for which \(C(X_{0})\subset C(T_{0})\). Let \(A_{0}=X_{0}(X^{\prime}_{0}T^{-}_{0}X_{0})^{-}X^{\prime}_{0}T^{-}_{0}\), where again for convenience take \(T^{-}_{0}\) and \((X^{\prime}_{0}T^{-}_{0}X_{0})^{-}\) as in Corollary B.41.

\[SSE_{0}=Y^{\prime}(I-A_{0})^{\prime}T^{-}_{0}(I-A_{0})Y.\]

Even though \(V\) is the same in models (1) and (2), things are complicated by the fact that \(T\) and \(T_{0}\) may be different.

The test will be, for some constant \(K\), based on

\[K\ (SSE_{0}-SSE)\ \big{/}SSE,\]

which will be shown to have an \(F\) distribution. We will use Theorem 1.3.6 and Theorem 1.3.9 to obtain distribution results.

**Theorem 10.3.1**: _If \(Y\in C(T)\), then_

_(a)_ \(Y^{\prime}(I-A)^{\prime}T^{-}(I-A)Y=Y^{\prime}(I-A)^{\prime}V^{-}(I-A)Y\) _for any_ \(V^{-}\)_,_

_(b)_ \(Y^{\prime}(I-A)^{\prime}T^{-}(I-A)Y=0\) _if and only if_ \((I-A)Y=0\)_._

_Proof_ The proof is given after Proposition 10.3.6. 

These results also hold when \(A\) an \(T\) are replaced by \(A_{0}\) and \(T_{0}\). Denote \(C\equiv(I-A)^{\prime}T^{-}(I-A)\) and \(C_{0}\equiv(I-A_{0})^{\prime}T^{-}_{0}(I-A_{0})\). Thus _SSE_\(=Y^{\prime}CY\) and _SSE_\({}_{0}=Y^{\prime}C_{0}Y\).

**Theorem 10.3.2**:
1. _Y'CY/_s_2__2__2_ \(\sim\chi^{2}(\operatorname{tr}(CV),\,0)\)_._ _If_ \(X\beta\in C(X_{0},\,V)\)_, then_ 2. \(Y^{\prime}(C_{0}-C)Y/\sigma^{2}\sim\chi^{2}(\operatorname{tr}(C_{0}V-CV),\, \beta^{\prime}X^{\prime}C_{0}X\beta)\) _and_ 3. \(Y^{\prime}CY\) _and_ \(Y^{\prime}(C_{0}-C)Y\) _are independent._

Proof.: The proof consists of checking the conditions in Theorems 1.3.6 and 1.3.9. There are many conditions to be checked. This is done in Lemmas 10.3.7 through 10.3.9 at the end of the section. 

The last result before stating the test establishes behavior of the distributions under the two models. Model (2) is true if and only if \(X\beta\in C(X_{0})\).

**Theorem 10.3.3**:
1. _If_ \(X\beta\in C(X_{0})\)_, then_ \(\Pr(Y\in C(X_{0},\,V))=1\) _and_ \(\beta^{\prime}X^{\prime}C_{0}X\beta=0\)_._
2. _If_ \(X\beta\notin C(X_{0})\)_, then either_ \(X\beta\in C(X_{0},\,V)\) _and_ \(\beta^{\prime}X^{\prime}C_{0}X\beta>0\) _or_ \(X\beta\notin C(X_{0},\,V)\) _and_ \(\Pr(Y\notin C(X_{0},\,V))=1\)__

Proof.:
1. The first part is clear, the second is because \(C_{0}X_{0}=0\).
2. If \(X\beta\notin C(X_{0})\), then either \(X\beta\in C(X_{0},\,V)\) or \(X\beta\notin C(X_{0},\,V)\). If \(X\beta\in C(X_{0},\,V)\), then by Theorem 10.3.1b, \(\beta^{\prime}X^{\prime}C_{0}X\beta=0\) if and only if \((I-A_{0})X\beta=0\) or \(X\beta=A_{0}X\beta\). Since \(X\beta\notin C(X_{0})\), \(\beta^{\prime}X^{\prime}C_{0}X\beta>0\). If \(X\beta\notin C(X_{0},\,V)\), suppose \(e\in C(V)\) and \(Y\in C(X_{0},\,V)\), then \(X\beta=Y-e\in C(X_{0},\,V)\), a contradiction. Therefore either \(e\notin C(V)\) or \(Y\notin C(X_{0},\,V)\). Since \(\Pr(e\in C(V))=1\), we must have \(\Pr(Y\notin C(X_{0},\,V))=1\). 

The test at the \(\alpha\) level is to reject \(H_{0}\) that model (2) is adequate if \(Y\notin C(X_{0},\,V)\) or if

\[\frac{(SSE_{0}-SSE)/\operatorname{tr}[(C_{0}-C)V]}{SSE/\operatorname{tr}(CV) }>F(1-\alpha,\operatorname{tr}[(C_{0}-C)V],\operatorname{tr}(CV)).\]

This is an \(\alpha\) level test because, under \(H_{0}\), \(\Pr(Y\notin C(X_{0},\,V))=0\). The power of the test is at least as great as that of a noncentral \(F\) test and is always greater than \(\alpha\) because if \(\beta^{\prime}X^{\prime}C_{0}X\beta=0\) under the alternative, Theorem 10.3.3 ensures that the test will reject with probability 1.

In the next section we consider extensions of least squares and conditions under which such extended least squares estimates are best estimates.

Proofs of Theorems 10.3.1 and 10.3.2.: Before proceeding with the proofs of the theorems, we need some background results.

**Proposition 10.3.4**: \(A^{\prime}T^{-}A=A^{\prime}T^{-}=T^{-}A\)_._

_Proof_

\[A^{\prime}T^{-} = T^{-}X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}=T^{-}A,\] \[A^{\prime}T^{-}A = T^{-}X(X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}X(X^{\prime}T^{-}X)^{-} X^{\prime}T^{-};\]

but, as discussed earlier, \(A\) does not depend on the choice of \((X^{\prime}T^{-}X)^{-}\) and \((X^{\prime}T^{-}X)^{-}X^{\prime}T^{-}X(X^{\prime}T^{-}X)^{-}\) is a generalized inverse of \(X^{\prime}T^{-}X\), so \(A^{\prime}T^{-}A=T^{-}A\). \(\square\)

**Corollary 10.3.5**: \((I-A)^{\prime}T^{-}(I-A)=(I-A)^{\prime}T^{-}=T^{-}(I-A)\)_._

In the remainder of this discussion we will let \(X=TG\) and \(V=TBT\). (The latter comes from Proposition 10.1.4 and symmetry.)

**Proposition 10.3.6**: \((a)\)__\(VT^{-}(I-A)=TT^{-}(I-A)\)_,_

\((b)\)__\(VT^{-}(I-A)V=(I-A)V\)_._

_Proof_ From Corollary 10.3.5 and unbiasedness, i.e., \(AX=X\),

\[TT^{-}(I-A)=T(I-A)^{\prime}T^{-}=V(I-A)^{\prime}T^{-}=VT^{-}(I-A).\]

With \(V=TBT\) and, from Proposition 10.2.3, \(AV\) symmetric, we have

\[VT^{-}(I-A)V = TT^{-}(I-A)V=TT^{-}V(I-A)^{\prime}=TT^{-}TBT(I-A)^{\prime}\] \[=TBT(I-A)^{\prime}=V(I-A)^{\prime}=(I-A)V.\]

Proof of Theorem 10.3.1. Recalling Proposition 10.1.4, if \(Y\in C(T)\), write \(Y=Xb_{1}+Vb_{2}\).

(a) Using Proposition 10.2.3,

\[Y^{\prime}(I-A)^{\prime}T^{-}(I-A)Y = (Xb_{1}+Vb_{2})^{\prime}(I-A)^{\prime}T^{-}(I-A)(Xb_{1}+Vb_{2})\] \[= b_{2}^{\prime}V(I-A)^{\prime}T^{-}(I-A)Vb_{2}\] \[= b_{2}^{\prime}(I-A)VT^{-}(I-A)Vb_{2}\] \[= b_{2}^{\prime}(I-A)(I-A)Vb_{2}\] \[= b_{2}^{\prime}(I-A)V(I-A)^{\prime}b_{2}\] \[= b_{2}^{\prime}(I-A)VV^{-}V(I-A)^{\prime}b_{2}\] \[= b_{2}^{\prime}V(I-A)^{\prime}V^{-}(I-A)Vb_{2}\] \[= Y^{\prime}(I-A)^{\prime}V^{-}(I-A)Y.\]2. From the proof of (a), \[Y^{\prime}(I-A)^{\prime}T^{-}(I-A)Y=b^{\prime}_{2}(I-A)V(I-A)^{\prime}b_{2}.\] Recall that we can write \(V=EDE^{\prime}\) with \(E^{\prime}E=I\), \(D=\operatorname{Diag}(d_{i})\), and \(d_{i}>0\) for all \(i\). \[Y^{\prime}(I-A)^{\prime}T^{-}(I-A)Y=0\quad\text{iff}\quad\ b^{\prime}_{2}(I-A) V(I-A)^{\prime}b_{2}=0\] iff \(E^{\prime}(I-A)^{\prime}b_{2}=0\) iff \((I-A)^{\prime}b_{2}\perp C(E)\) iff \((I-A)^{\prime}b_{2}\perp C(V)\) iff \(V(I-A)^{\prime}b_{2}=0\) iff \((I-A)Vb_{2}=0\) iff \((I-A)Y=0\). 

The following lemmas constitute the proof of Theorem 10.3.2.

**Lemma 10.3.7**:
1. _CVC_ \(=\) _C and_ \(C_{0}VC_{0}=C_{0}\)_,_
2. _CVC_\({}_{0}V=\) _CV,_
3. _VCVCV_ \(=\) _VCV,_ \(\beta^{\prime}X^{\prime}CVCX\beta=\beta^{\prime}X^{\prime}CX\beta=0\)_,_ _VCVCX_\(\beta=VCX\beta=0\)_._

_Proof_

1. Using Corollary 10.3.5 and Proposition 10.3.6, \(CVC=CVT^{-}(I-A)=CTT^{-}(I-A)=(I-A)^{\prime}T^{-}TT^{-}(I-A)=(I-A)^{\prime}T^{ -}(I-A)=C\).
2. A similar argument gives the second equality: \(CVC_{0}V=CVT^{-}_{0}(I-A_{0})V=C(I-A_{0})V=T^{-}(I-A)(I-A_{0})V=T^{-}(I-A)V=CV\).
3. The equalities in (c) follow from (a) and the fact that \(CX=T^{-}(I-A)X=0\). 

Note that Lemma 10.3.7c leads directly to Theorem 10.3.2a. We now establish the conditions necessary for Theorem 10.3.2b.

**Lemma 10.3.8**:
1. \(V(C_{0}-C)V(C_{0}-C)V=V(C_{0}-C)V\)_,_
2. \(\beta^{\prime}X^{\prime}(C_{0}-C)V(C_{0}-C)X\beta=\beta^{\prime}X^{\prime}(C_{ 0}-C)X\beta=\beta^{\prime}X^{\prime}C_{0}X\beta\)_,_
3. _if_ \(X\beta\in C(X_{0},\,V)\)_, then_ \(V(C_{0}-C)V(C_{0}-C)X\beta=V(C_{0}-C)X\beta\)_._

_Proof_ From parts (a) and (b) of Lemma 10.3.7,\[V(C_{0}-C)V(C_{0}-C)V = VC_{0}VC_{0}V-VC_{0}VCV-VCVC_{0}V+VCVCV\] \[= VC_{0}V-VCV-VCV+VCV\] \[= VC_{0}V-VCV\] \[= V(C_{0}-C)V.\]

To show (b), we need only show that

\[X^{\prime}(C_{0}-C)V(C_{0}-C)X=X^{\prime}(C_{0}-C)X.\]

Since \(CX=0\), this is equivalent to showing \(X^{\prime}C_{0}VC_{0}X=X^{\prime}C_{0}X\). The result is immediate from part (a) of Lemma 10.3.7.

To show (c), we need to show that

\[V(C_{0}-C)V(C_{0}-C)X\beta=V(C_{0}-C)X\beta.\]

Since \(CX=0\), it is enough to show that \(VC_{0}VC_{0}X\beta-VCVC_{0}X\beta=VC_{0}X\beta\). With \(VC_{0}VC_{0}X\beta=VC_{0}X\beta\), we only need \(VCVC_{0}X\beta=0\). Since \(X\beta\in C(T_{0})\), by assumption, \((I-A_{0})X\beta=T_{0}\gamma\) for some \(\gamma\),

\[VCVC_{0}X\beta = VCVT_{0}^{-}(I-A_{0})X\beta=VCT_{0}T_{0}^{-}(I-A_{0})X\beta\] \[= VCT_{0}T_{0}^{-}T_{0}\gamma=VCT_{0}\gamma=VC(I-A_{0})X\beta\] \[= VT^{-}(I-A)(I-A_{0})X\beta=VT^{-}(I-A)X\beta=0.\]

To establish Theorem 10.3.2c, use the following lemma:

**Lemma 10.3.9**:
1. _VCV_(_\(C_{0}-C)V=0\)_,_
2. _VCV_(_\(C_{0}-C)X\beta=0\) _if_ \(X\beta\in C(X_{0},\,V)\)_,_
3. \(V(C_{0}-C)VCX\beta=0\)_,_
4. \(\beta^{\prime}X^{\prime}(C_{0}-C)VCX\beta=0\)_._

_Proof_ For part (a), \(VCV(C_{0}-C)V=VCVC_{0}V-VCVCV=VCV-VCV\). Parts (c) and (d) follow because \(CX=0\); also, (b) becomes the condition \(VCVC_{0}X\beta=0\), as was shown in the proof of Lemma 10.3.8. 

### Least Squares Consistent Estimation

**Definition 10.4.1** : An estimate \(\tilde{\beta}\) of \(\beta\) is said to be _consistent_ if \(X\tilde{\beta}\) is a consistent estimate of \(X\beta\). \(\hat{\beta}\) is said to be a _least squares consistent estimate of_\(\beta\) if for any other consistent estimate \(\tilde{\beta}\) and any \(Y\in C(X,\,V)\),

\[(Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})\leq(Y-X\tilde{\beta})^{\prime}(Y-X \tilde{\beta}).\]How does this differ from the usual definition of least squares? In the case where \(C(X)\subset C(V)\), it hardly differs at all. Any estimate \(\tilde{\beta}\) will have \(X\tilde{\beta}\in C(X)\), and, as we observed earlier, when \(C(X)\subset C(V)\), any estimate of \(X\beta\) satisfying condition (i) of consistency (Definition 10.2.1) also satisfies condition (ii). The main difference between consistent least squares and least squares is that we are restricting ourselves to consistent estimates of \(X\beta\). As we saw earlier, estimates that are not consistent are just not reasonable, so we are not losing anything. (Recall Example 10.2.2, in which the least squares estimate was not consistent.) The other difference between consistent least squares estimation and regular least squares is that the current definition restricts \(Y\) to \(C(X,\,V)\). In the case where \(C(X)\subset C(V)\), this restriction would not be necessary because a least squares estimate will have to be a least squares consistent estimate. In the general case, we need to actually use condition (ii) of consistency. Condition (ii) was based on the fact that \(e\in C(V)\) a.s. Since \(e\) cannot be observed, we used the related fact that \(Y\in C(X,\,V)\) a.s., and made condition (ii) apply only when \(Y\in C(X,\,V)\).

**Theorem 10.4.2a**.: _If \(AY\) is a CLUE and \((I-A)r\perp C(X)\cap\,C(V)\) for any \(r\in C(X,\,V)\), then any \(\hat{\beta}\) satisfying \(AY=X\hat{\beta}\) is a least squares consistent estimate._

_Proof_ Let \(\tilde{\beta}\) be any consistent estimate,

\[\begin{array}{rcl}(Y-X\tilde{\beta})^{\prime}(Y-X\tilde{\beta})&=&(Y-AY+AY-X \tilde{\beta})^{\prime}(Y-AY+AY-X\tilde{\beta})\\ &=&Y^{\prime}(I-A)^{\prime}(I-A)Y+(AY-X\tilde{\beta})^{\prime}(AY-X\tilde{ \beta})\\ &&\quad+2Y^{\prime}(I-A)^{\prime}(AY-X\tilde{\beta}).\end{array}\]

It is enough to show that

\[Y^{\prime}(I-A)^{\prime}(AY-X\tilde{\beta})=0\mbox{ for }Y\in C(X,\,V).\]

Note that \(AY-X\tilde{\beta}\in C(X)\). Also observe that since \(AY\) and \(X\tilde{\beta}\) are consistent, \(AY-X\tilde{\beta}=(Y-X\tilde{\beta})-(I-A)Y\in C(V)\) for \(Y\in C(X,\,V)\). Thus \(AY-X\tilde{\beta}\in C(X)\cap C(V)\). For any \(Y\in C(X,\,V)\), we have

\[(I-A)Y\perp C(X)\cap\,C(V);\]

so \(Y^{\prime}(I-A)^{\prime}(AY-X\tilde{\beta})=0\). 

We now give a formula for a least squares consistent estimate. Choose \(V_{0}\) with orthonormal columns such that \(C(V_{0})=C(X)\cap\,C(V)\). Also choose \(V_{1}\) with orthonormal columns such that \(C(V_{1})\perp C(V_{0})\) and \(C(V_{0},\,V_{1})=C(V)\). It is easily seen that \(MY-MV_{1}\hat{\gamma}\) is a CLUE, where \(\hat{\gamma}=[V_{1}^{\prime}(I-M)V_{1}]^{-1}V_{1}^{\prime}(I-M)Y\). Observe that

\[C(X,\,V)=C(X,\,V_{0},\,V_{1})=C(X,\,(I-M)V_{1}).\]To put the computations into a somewhat familiar form, consider the analysis of covariance model

\[Y=[X,\,(I-M)\,V_{1}]\left[\begin{array}{c}\beta\\ \gamma\end{array}\right]+e,\;\;\;\mbox{E}(e)=0. \tag{1}\]

We are interested in the least squares estimate of \(\mbox{E}(Y)\), so the error vector \(e\) is of no interest except that \(\mbox{E}(e)=0\). The least squares estimate of \(\mbox{E}(Y)\) is \(MY+(I-M)\,V_{1}\hat{\gamma}\), where \(\hat{\gamma}=[V_{1}^{\prime}(I-M)\,V_{1}]^{-1}\,V_{1}^{\prime}(I-M)Y\). It turns out that \(MY-MV_{1}\hat{\gamma}\) is a CLUE for \(X\beta\) in model (10.1.1).

First, \(MY-MV_{1}\hat{\gamma}\) is unbiased, because \(\mbox{E}(MY)=X\beta\) and \(\mbox{E}(\hat{\gamma})=0\). \(\mbox{E}(\hat{\gamma})\) is found by replacing \(Y\) with \(X\beta\) in the formula for \(\hat{\gamma}\), but the product of \((I-M)X\beta=0\). Second, it is clear that for any \(Y\)

\[MY-MV_{1}\hat{\gamma}\,\in C(X).\]

Finally, it is enough to show that if \(Y\in C(V)\), then \(MY-MV_{1}\hat{\gamma}\in C(V)\). If \(Y\in C(V)\), then \(Y\in C(X,\,(I-M)\,V_{1})=C(X,\,V)\). Therefore, the least squares estimate of \(\mbox{E}(Y)\) in model (1) is \(Y\) itself. We have two characterizations of the least squares estimate, and equating them gives

\[MY+(I-M)\,V_{1}\hat{\gamma}=Y\]

or

\[MY-MV_{1}\hat{\gamma}=Y-V_{1}\hat{\gamma}.\]

Now, it is clear that \(V_{1}\hat{\gamma}\in C(V)\); so if \(Y\in C(V)\), then \(MY-MV_{1}\hat{\gamma}\in C(V)\).

**Proposition 10.4.3**: _If \(\hat{\beta}\) is an estimate with \(X\hat{\beta}=MY-MV_{1}\hat{\gamma}\), then \(\hat{\beta}\) is a least squares consistent estimate._

_Proof_ By Theorem 10.4.2a, it is enough to show that

\[(Y-MY+MV_{1}\hat{\gamma})\perp C(V_{0})\]

for any \(Y\in C(X,\,V)\). Let \(w\in C(V_{0})\); then, since \(Mw=w\),

\[w^{\prime}(Y-MY+MV_{1}\hat{\gamma}) =w^{\prime}Y-w^{\prime}MY+w^{\prime}MV_{1}\hat{\gamma}\] \[=w^{\prime}Y-w^{\prime}Y+w^{\prime}V_{1}\hat{\gamma}\] \[=w^{\prime}V_{1}\hat{\gamma}\] \[=0.\]

**Theorem 10.4.2b**.: _If \(\hat{\beta}\) is a least squares consistent estimate, then \((Y-X\hat{\beta})\perp C(X)\cap\ C(V)\) for any \(Y\in C(X,\,V)\); and if \(\tilde{\beta}\) is any other least squares consistent estimate, \(X\hat{\beta}=X\tilde{\beta}\) for any \(Y\in C(X,\,V)\)._

Proof.: Let \(A\) be the matrix determined by \(AY=MY-MV_{1}\hat{\gamma}\) for all \(Y\). We show that \(AY=X\hat{\beta}\) for any \(Y\in C(X,\,V)\) and are done.

We know, by Definition 10.4.1, that

\[(Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})=(Y-AY)^{\prime}(Y-AY)\ \ \mbox{ for }Y\in C(X,\,V).\]

As in the proof of Theorem 10.4.2a, we also know that, for \(Y\in C(X,\,V)\),

\[(Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})=(Y-AY)^{\prime}(Y-AY)+(AY-X\hat{ \beta})^{\prime}(AY-X\hat{\beta}).\]

Therefore,

\[(AY-X\hat{\beta})^{\prime}(AY-X\hat{\beta})=0;\]

hence

\[AY-X\hat{\beta}=0\]

or

\[AY=X\hat{\beta}\ \ \mbox{ for }Y\in C(X,\,V).\qed\]

Together, Theorems 10.4.2a and 10.4.2b give an "if and only if" condition for \(\hat{\beta}\) to be a least squares CLUE.

In the future, any CLUE of \(X\beta\), say \(AY\), that satisfies \((I-A)r\perp C(X)\cap\ C(V)\) for any \(r\in C(X,\,V)\) will be referred to as a least squares CLUE of \(X\beta\). The most important result on least squares CLUEs is:

**Theorem 10.4.4**.: _If in model (10.1.1), \(C(VV_{0})\subset C(V_{0})\), then a least squares CLUE of \(X\beta\) is a best CLUE of \(X\beta\) (and hence a BLUE of \(X\beta\))._

Proof.: Let \(AY\) be a least squares CLUE and \(BY\) any other CLUE. We need to show that, for any vector \(\rho\),

\[\mbox{Var}(\rho^{\prime}AY)\leq\mbox{Var}(\rho^{\prime}BY).\]

We can decompose \(\mbox{Var}(\rho^{\prime}BY)\),

\[\mbox{Var}(\rho^{\prime}BY) = \mbox{Var}[\rho^{\prime}(B-A)Y+\rho^{\prime}AY]\] \[= \mbox{Var}[\rho^{\prime}(B-A)Y]+\mbox{Var}(\rho^{\prime}AY)+2 \mbox{Cov}[\rho^{\prime}(B-A)Y,\rho^{\prime}AY].\]

Since variances are nonnegative, it suffices to show that

\[0=\mbox{Cov}[\rho^{\prime}(B-A)Y,\rho^{\prime}AY]=\sigma^{2}\rho^{\prime}(B-A )VA^{\prime}\rho.\]First, we will establish that it is enough to show that the covariance is zero when \(\rho\in C(X)\cap\,C(V)\). Let \(M_{0}\) be the perpendicular projection matrix onto \(C(X)\cap C(V)\). Then \(\rho=M_{0}\rho+(I-M_{0})\rho\) and

\[\rho^{\prime}(B-A)VA^{\prime}\rho = \rho^{\prime}(B-A)VA^{\prime}M_{0}\rho+\rho^{\prime}(B-A)VA^{ \prime}(I-M_{0})\rho\] \[= \rho^{\prime}M_{0}(B-A)VA^{\prime}M_{0}\rho+\rho^{\prime}(I-M_{0}) (B-A)VA^{\prime}M_{0}\rho\] \[+\rho^{\prime}M_{0}(B-A)VA^{\prime}(I-M_{0})\rho\] \[+\rho^{\prime}(I-M_{0})(B-A)VA^{\prime}(I-M_{0})\rho.\]

It turns out that all of these terms except the first is zero. Since \(AY\) and \(BY\) are CLUEs, unbiasedness and observation (d) in Section 2 give \(C(AV)\subset C(X)\cap\,C(V)\) and \(C(BV)\subset C(X)\cap\,C(V)\). By orthogonality,

\[VA^{\prime}(I-M_{0})\rho=0\ \ \ {\rm and}\ \ \ \rho^{\prime}(I-M_{0})(B-A)V=0;\]

so

\[\rho^{\prime}(B-A)VA^{\prime}\rho=\rho^{\prime}M_{0}(B-A)VA^{\prime}M_{0}\rho.\]

Henceforth, assume \(\rho\in C(X)\cap\,C(V)\).

To obtain the final result, observe that since \(AY\) is a least squares CLUE, any column of \((I-A)V\) is orthogonal to \(\rho\); so

\[V(I-A)^{\prime}\rho=0\ \ \ {\rm and}\ \ \ V\rho=VA^{\prime}\rho.\]

Since, by assumption, \(C(VV_{0})\subset C(V_{0})\), we also have \(V\rho\in C(X)\cap\,C(V)\). The covariance term is \(\rho^{\prime}(B-A)VA^{\prime}\rho=\rho^{\prime}(B-A)V\rho\). Since \(AY\) and \(BY\) are unbiased and \(V\rho\in C(X)\), \((B-A)V\rho=0\); hence

\[\rho^{\prime}(B-A)VA^{\prime}\rho=0.\qed\]

As mentioned in the statement of the theorem, a best CLUE is a BLUE. That occurs because all BLUEs have the same variance and there is a BLUE that is a CLUE.

As would be expected, when \(C(X)\subset C(V)\), a least squares CLUE will equal \(MY\) for all \(Y\in C(V)\). When \(C(X)\subset C(V)\), then \(C(X)=C(V_{0})\); so \(MV_{1}=0\) and \(MY-MV_{1}\hat{\gamma}=MY\).

The following theorem characterizes when ordinary least squares estimates are BLUEs.

**Theorem 10.4.5**: _The following conditions are equivalent:_

1. \(C(V\!X)\subset C(X)\)_,_
2. \(C(VV_{0})\subset C(V_{0})\) _and_ \(X^{\prime}V_{1}=0\)_,_
3. \(MY\) _is a BLUE for_ \(X\beta\)_Proof_ Let \(X_{1}\) be a matrix with orthonormal columns such that \(C(X)=C(V_{0},\,X_{1})\) and \(V_{0}^{\prime}X_{1}=0\). Also let

\[V=[V_{0},\,V_{1}]\left[\begin{array}{cc}B_{11}&B_{12}\\ B_{21}&B_{22}\end{array}\right]\left[\begin{array}{c}V_{0}^{\prime}\\ V_{1}^{\prime}\end{array}\right];\]

so \(V=V_{0}B_{11}V_{0}^{\prime}+V_{1}B_{22}V_{1}^{\prime}+V_{0}B_{12}V_{1}^{\prime} +V_{1}B_{21}V_{0}^{\prime}\). By symmetry, \(B_{12}=B_{21}^{\prime}\). Recall that \(V_{0}\) and \(V_{1}\) also have orthonormal columns.

\(a\Rightarrow b\)

Clearly \(C(V\!X)\subset C(V)\); so if \(C(V\!X)\subset C(X)\), we have \(C(V\!X)\subset C(V_{0})\). It is easily seen that

\[C(V\!X)\subset C(V_{0})\mbox{ if and only if }C(V\!V_{0})\subset C(V_{0}) \mbox{ and }C(V\!X_{1})\subset C(V_{0}).\]

We show that \(C(V\!X_{1})\subset C(V_{0})\) implies that \(X_{1}^{\prime}V=0\); hence \(X_{1}^{\prime}V_{1}=0\) and \(X^{\prime}V_{1}=0\). First \(V\!X_{1}=V_{1}B_{22}V_{1}^{\prime}X_{1}+V_{0}B_{12}V_{1}^{\prime}X_{1}\); we show that both terms are zero. Consider \(V\!V_{0}=V_{0}B_{11}+V_{1}B_{21}\); since \(C(V\!V_{0})\subset C(V_{0})\), we must have \(V_{1}B_{21}=0\). By symmetry, \(B_{12}V_{1}^{\prime}=0\) and \(V_{0}B_{12}V_{1}^{\prime}X_{1}=0\). To see that \(V\!X_{1}=V_{1}B_{22}V_{1}^{\prime}X_{1}=0\), observe that since \(C(V\!X_{1})\subset C(V_{0})\), it must be true that \(C(V_{1}B_{22}V_{1}^{\prime}X_{1})\subset C(V_{0})\). However, \(C(V_{1}B_{22}V_{1}^{\prime}X_{1})\subset C(V_{1})\) but \(C(V_{0})\) and \(C(V_{1})\) are orthogonal, so \(V_{1}B_{22}\)\(V_{1}^{\prime}X_{1}=0\).

\(b\Rightarrow a\)

If \(X^{\prime}V_{1}=0\), then \(X_{1}^{\prime}V_{1}=0\) and \(X_{1}^{\prime}V=0\). Write \(X=V_{0}B_{0}+X_{1}B_{1}\) so \(V\!X=V\!V_{0}B_{0}+V\!X_{1}B_{1}=V\!V_{0}B_{0}\). Thus,

\[C(V\!X)=C(V\!V_{0}B_{0})\subset C(V\!V_{0})\subset C(V\!V_{0})\subset C(X).\]

\(b\Rightarrow c\)

If \(C(V\!V_{0})\subset C(V_{0})\), then \(MY-MV_{1}\hat{\gamma}\) is a BLUE. Since \(X^{\prime}V_{1}=0\), \(MV_{1}=0\) and \(MY\) is a BLUE.

\(c\Rightarrow b\)

If \(AY\) and \(BY\) are BLUEs, then \(AY\!=\!BY\) for \(Y\!\in\!C(X,\,V)\). As in Proposition 10.2.3, there exists a BLUE, say \(AY\), such that \(AV=VA^{\prime}\). Since \(MY\) is a BLUE and \(C(V)\subset C(X,\,V)\), \(AV=MV\) and \(MV=VM\). Finally, \(V\!V_{0}=VM\!V_{0}=MV\!V_{0}\), so \(C(V\!V_{0})=C(MV\!V_{0})\subset C(X)\). Since \(C(V\!V_{0})\subset C(V)\), we have \(C(VV\!V_{0})\subset C(V_{0})\).

From Theorem 10.4.4 we know that a least squares CLUE is a BLUE; hence \(MY=MY-MV_{1}\hat{\gamma}\) for \(Y\!\in\!C(X,\,V)=C(X,\,V_{1})\). Since

\[\hat{\gamma}=\left[V_{1}^{\prime}(I-M)\,V_{1}\right]^{-1}V_{1}^{\prime}(I-M)Y,\]

we must have \(0=MV_{1}\left[V_{1}^{\prime}(I-M)\,V_{1}\right]^{-1}V_{1}^{\prime}(I-M)\,V_{1} =MV_{1}\). Thus \(0=X^{\prime}V_{1}\).

In the proof of \(a\Rightarrow b\), it was noted that \(V_{1}B_{21}=0\). That means we can write \(V=V_{0}B_{11}V_{0}^{\prime}+V_{1}B_{22}V_{1}^{\prime}\). For ordinary least squares estimates to be BLUEs, \(C(V)\) must admit an orthogonal decomposition into a subspace contained in \(C(X)\) and a subspace orthogonal to \(C(X)\). Moreover, the error term \(e\) in model (10.1.1) must have \(e=e_{0}+e_{1}\), where \(\operatorname{Cov}(e_{0},\,e_{1})=0\), \(\operatorname{Cov}(e_{0})=V_{0}B_{11}V_{0}^{\prime}\), and \(\operatorname{Cov}(e_{1})=V_{1}B_{22}V_{1}^{\prime}\). Thus, with probability 1, the error can be written as the sum of two orthogonal vectors, both in \(C(V)\), and one in \(C(X)\). The two vectors must also be uncorrelated.

**Exercise 10.6**: Show that \(V_{1}^{\prime}(I-M)V_{1}\) is invertible.

Answer: The columns of \(V_{1}\) form a basis, so \(0=V_{1}b\) iff \(b=0\). Also \((I-M)V_{1}b=0\) iff \(V_{1}b\in C(X)\), but \(V_{1}b\in C(X)\) iff \(b=0\) by choice of \(V_{1}\). Thus, \((I-M)V_{1}b=0\) iff \(b=0\); hence \((I-M)V_{1}\) has full column rank and \(V_{1}^{\prime}(I-M)V_{1}\) is invertible.

**Exercise 10.7**: Show that if \(MY-MV_{1}\hat{\gamma}\) is a BLUE of \(X\beta\), then \(C(V_{0})\subset C(V_{0})\).

Hint: Multiply on the right by \(V_{0}\) after showing that

\[\Big{[}M-MV_{1}\,\big{[}V_{1}^{\prime}(I-M)V_{1}\big{]}^{-1}\,V_{1 }^{\prime}(I-M)\Big{]}V\\ =V\Big{[}M-(I-M)V_{1}\,\big{[}V_{1}^{\prime}(I-M)V_{1}\big{]}^{-1 }\,V_{1}^{\prime}M\Big{]}.\]

We include a result that allows one to find the matrix \(V_{1}\), and thus find least squares CLUEs.

**Proposition 10.4.6**: \(r\perp C(X)\,\cap\,C(V)\) _if and only if \(r\in C(I-M,\,I-M_{V})\)._

_Proof_ If \(r\in C(I-M,\,I-M_{V})\), then write \(r=(I-M)r_{1}+(I-M_{V})r_{2}\).

Let \(w\in C(X)\,\cap\,C(V)\) so that \(w=M_{V}w=Mw\). We need to show that \(w^{\prime}r=0\). Observe that

\[w^{\prime}r =w^{\prime}(I-M)r_{1}+w^{\prime}(I-M_{V})r_{2}\] \[=w^{\prime}M(I-M)r_{1}+w^{\prime}M_{V}(I-M_{V})r_{2}\] \[=0.\]

The vector space here is, say, \(\mathbf{R}^{n}\). Let \(r[C(X)\,\cap\,C(V)]=m\). From the above result, \(C(I-M,\,I-M_{V})\) is orthogonal to \(C(X)\,\cap\,C(V)\). It is enough to show that the rank of \(C(I-M,\,I-M_{V})\) is \(n-m\). If this is not the case, there exists a vector \(w\neq 0\) such that \(w\perp C(I-M,\,I-M_{V})\) and \(w\perp C(X)\,\cap\,C(V)\).

Since \(w\perp C(I-M,\,I-M_{V})\), we have \((I-M)w=0\) or \(w=Mw\in C(X)\). Similarly, \(w=M_{V}w\in C(V)\); so \(w\in C(X)\,\cap\,C(V)\), a contradiction.

To find \(V_{1}\), one could use Gram-Schmidt to first get an orthonormal basis for \(C(I-M,\,I-M_{V})\), then extend this to \(\mathbf{R}^{n}\). The extension is a basis for \(C(V_{0})\). Finally, extend the basis for \(C(V_{0})\) to an orthonormal basis for \(C(V)\). The extension is a basis for \(C(V_{1})\). A basis for \(C(X_{1})\) can be found by extending the basis for \(C(V_{0})\) to a basis for \(C(X)\).

**Exercise 10.8**: Give the general form for a BLUE of \(X\beta\) in model (10.1.1).

Hint: Add something to a particular BLUE.

**Exercise 10.9**: From inspecting Figure 10.2, give the least squares CLUE for Example 10.2.2. Do not do any matrix manipulations.

_Remark_: Suppose that we are analyzing the model \(Y=X\beta+e\), \(\operatorname{E}(e)=0\), \(\operatorname{Cov}(e)=\varSigma(\theta)\), where \(\varSigma(\theta)\) is some nonnegative definite matrix depending on a vector of unknown parameters \(\theta\). The special case where \(\varSigma(\theta)=\sigma^{2}V\) is what we have been considering so far. It is clear that if \(\theta\) is known, our current theory gives BLUEs. If it happens to be the case that for any value of \(\theta\) the BLUEs are identical, then the BLUEs are known even though \(\theta\) may not be. This is precisely what we have been doing with \(\varSigma(\theta)=\sigma^{2}V\). We have found BLUEs for any value of \(\sigma^{2}\), and they do not depend on \(\sigma^{2}\). Another important example of this occurs when \(C(\varSigma(\theta)X)\subset C(X)\) for any \(\theta\). In this case, least squares estimates are BLUEs for any \(\theta\), and least squares estimates do not depend on \(\theta\), so it does not matter that \(\theta\) is unknown. The split plot design model is one in which the covariance matrix depends on two parameters, but for any value of those parameters, least squares estimates are BLUEs. Such models are examined in the next chapter.

**Exercise 10.10**: Show that ordinary least squares estimates are best linear unbiased estimates in the model \(Y=X\beta+e\), \(\operatorname{E}(e)=0\), \(\operatorname{Cov}(e)=V\) if the columns of \(X\) are eigenvectors of \(V\).

**Exercise 10.11**: Use Definition B.31 and Proposition 10.4.6 to show that \(M_{C(X)\cap C(V)}=M-M_{W}\) where \(C(W)=C[M(I-M_{V})]\).

Another way to find \(V_{0}\) is due to Rao and Mitra (1971, pp. 118, 119). \(V_{0}=X\tilde{U}\) where \(C(\tilde{U})=C[X^{\prime}(I-M_{V})]^{\perp}\). Note that \((I-M_{V})X\tilde{U}=0\), so \(C(X\tilde{U})\subset C(V_{0})\). To see that \(C(V_{0})\subset C(X\tilde{U})\), take \(v\in C(V_{0})\). Then \(v=Xb\) for some \(b\) but also \((I-M_{V})Xb=0\), so \(b\in C[X^{\prime}(I-M_{V})]^{\perp}\) and \(b=\tilde{U}\gamma\) for some \(\gamma\), hence \(v=X\tilde{U}\gamma\). Note that \((I-M_{V})\) can be replaced with any matrix \(B\) having \(C(B)=C(V)^{\perp}\) and also that \(r(V_{0})=r(X)+r(V)-r(X,\,V)\).

### Perfect Estimation and More

One of the interesting things about the linear model

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}V, \tag{1}\]

is that when \(C(X)\not\subset C(V)\) you can learn things about the parameters with probability 1. We identify these estimable functions of the parameters and consider the process of estimating those parameters that are not perfectly known.

Earlier, to treat \(C(X)\not\subset C(V)\), we obtained estimates and tests by replacing \(V\) in model (1) with \(T\) where \(C(X)\subset C(T)\). Although we showed that the procedure works, it is probably not the first thing one would think to do. In this section, after isolating the estimable functions of \(X\beta\) that can be known perfectly, we replace model (1) with a new model \(\tilde{Y}=V_{0}\gamma+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}V\) in which \(C(V_{0})\subset C(V)\) and \(\tilde{Y}\) is just \(Y\) minus a perfectly known component of \(X\beta\). I find it far more intuitive to make adjustments to the model matrix \(X\), something we do regularly in defining reduced and restricted models, than to adjust the covariance matrix \(V\). Additional details are given in Christensen and Lin (2010).

Example 10.5.1: Consider a one-sample model \(y_{i}=\mu+\varepsilon_{i}\), \(i=1,\,2,\,3\), with uncorrelated observations but in which the second and third observations have variance 0. If this model is correct, you obviously should have \(\mu=y_{2}=y_{3}\) with probability 1. The matrices for model (1) are

\[Y=\left[\begin{array}{c}y_{1}\\ y_{2}\\ y_{3}\end{array}\right],\quad X=\left[\begin{array}{c}1\\ 1\\ 1\end{array}\right],\quad V=\left[\begin{array}{cc}1&0&0\\ 0&0&0\\ 0&0&0\end{array}\right].\]

All of our examples in this section use this same \(Y\) vector.

The first thing to do with models having \(C(X)\not\subset C(V)\) is to see whether they are even plausible for the data. In particular, Lemma 1.3.5 implies that \(\mathrm{Pr}[(Y-X\beta)\in C(V)]=1\) so that \(Y\in C(X,\,V)\) a.s. This should be used as a model-checking device. If \(Y\notin C(X,\,V)\), you clearly have the wrong model.

Example 10.5.1: Continued. In this example, if \(y_{2}\neq y_{3}\) we obviously have the wrong model. It is easily seen that

\[C(X,\,V)=C\left(\left[\begin{array}{c}0\\ 1\\ 1\end{array}\right],\left[\begin{array}{c}1\\ 0\\ 0\end{array}\right]\right)\]

and if \(y_{2}\neq y_{3}\), \(Y\notin C(X,\,V)\).

To identify the estimable parameters that can be known perfectly, let \(Q\) be a full rank matrix with

\[C(Q)=C(V)^{\perp}.\]

Note that if \(C(X)\not\subset C(V)\), then \(Q^{\prime}X\neq 0\). Actually, the contrapositive is more obvious, if \(Q^{\prime}X=0\) then

\[C(X)\subset C(Q)^{\perp}=\left[C(V)^{\perp}\right]^{\perp}=C(V).\]

The fact that \(Q^{\prime}X\neq 0\) means that the estimable function \(Q^{\prime}X\beta\) is nontrivial. Note also that \(C(X)\not\subset C(V)\) implies that \(V\) must be singular. If \(V\) were nonsingular, then \(C(V)=\mathbf{R}^{n}\) and \(C(X)\) has to be contained in it.

We can now identify the estimable functions that are known perfectly. Because \(\Pr[(Y-X\beta)\in C(V)]=1\), clearly \(\Pr[Q^{\prime}(Y-X\beta)=0]=1\) and \(Q^{\prime}Y=Q^{\prime}X\beta\) a.s. Therefore, whenever \(C(X)\not\subset C(V)\), there are nontrivial estimable functions of \(\beta\) that we can learn without error. Moreover, \(\operatorname{Cov}(Q^{\prime}Y)=\sigma^{2}Q^{\prime}V\,Q=0\) and only linear functions of \(Q^{\prime}Y\) will have 0 covariance matrices, so only linear functions of \(Q^{\prime}X\beta\) will be estimated perfectly.

**Exercise 10.13**: Show that \(\operatorname{Cov}(B^{\prime}Y)=0\) iff \(B^{\prime}=B_{*}^{\prime}Q^{\prime}\) for some matrix \(B_{*}\).

Hint: First, decompose \(B\) into the sum of two matrices \(B_{0}\) and \(B_{1}\) with \(C(B_{0})\subset C(V)\) and \(C(B_{1})\perp C(V)\). Then use a singular value decomposition of \(V\) to show that \(B_{0}^{\prime}VB_{0}=0\) iff \(B_{0}=0\).

If \(Q^{\prime}X\) has full column rank, we can actually learn all of \(X\beta\) without error. In that case, with probability 1 we can write

\[X\beta=X\left(X^{\prime}QQ^{\prime}X\right)^{-1}\left[X^{\prime}QQ^{\prime}X \right]\beta=X(X^{\prime}QQ^{\prime}X)^{-1}X^{\prime}QQ^{\prime}Y.\]

For convenience, define \(A\) so that \(AY\equiv X(X^{\prime}QQ^{\prime}X)^{-1}X^{\prime}QQ^{\prime}Y\), in which case \(X\beta=AY\) a.s. In particular, it is easy to see that \(\operatorname{E}[AY]=X\beta\) and \(\operatorname{Cov}[AY]=0\). We now illustrate these matrix formulations in some simple examples to show that the matrix results give obviously correct answers.

_Example 10.5.1 Continued._ Using the earlier forms for \(X\) and \(V\),

\[Q=\begin{bmatrix}0&0\\ 1&0\\ 0&1\end{bmatrix}.\]

It follows that

\[Q^{\prime}X=\begin{bmatrix}1\\ 1\end{bmatrix}\]and \(Q^{\prime}Y=Q^{\prime}X\beta\) reduces to

\[\begin{bmatrix}y_{2}\\ y_{3}\end{bmatrix}=\mu J_{2}\ \ \ \text{a.s.}\]

Obviously, for this to be true, \(y_{2}=y_{3}\) a.s. Moreover, since \(Q^{\prime}X\) is full rank, upon observing that \(X^{\prime}QQ^{\prime}X=2\) we can compute

\[\mu J_{3}=X\beta=AY=[(y_{2}+y_{3})/2]J_{3}\ \ \ \text{a.s.}\]

_Example 10.5.2_  This is a two-sample problem, with the first two observations from sample one and the third from sample two. Again, observations two and three have 0 variance. Clearly, with probability 1, \(\mu_{1}=y_{2}\) and \(\mu_{2}=y_{3}\). The key matrices are

\[X=\begin{bmatrix}1&0\\ 1&0\\ 0&1\end{bmatrix},\quad V=\begin{bmatrix}1&0&0\\ 0&0&0\\ 0&0&0\end{bmatrix},\quad Q=\begin{bmatrix}0&0\\ 1&0\\ 0&1\end{bmatrix}.\]

Since \(\beta=[\mu_{1},\,\mu_{2}]^{\prime}\) in the two-sample problem and

\[Q^{\prime}X=\begin{bmatrix}1&0\\ 0&1\end{bmatrix},\]

we have

\[\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix}=Q^{\prime}X\beta=Q^{\prime}Y=\begin{bmatrix}y_{2}\\ y_{3}\end{bmatrix}\ \ \ \text{a.s.}\]

In particular, with probability 1,

\[\begin{bmatrix}\mu_{1}\\ \mu_{1}\\ \mu_{2}\end{bmatrix}=X\beta=AY=\begin{bmatrix}y_{2}\\ y_{2}\\ y_{3}\end{bmatrix}.\]

_Example 10.5.3_  This is a one-sample problem similar to Example 10.5.1 except that now the first two observations have variance 1 but the third has variance 0. The key matrices are

\[X=\begin{bmatrix}1\\ 1\\ 1\end{bmatrix},\quad V=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&0\end{bmatrix},\quad Q=\begin{bmatrix}0\\ 0\\ 1\end{bmatrix}.\]

With \(Q^{\prime}X=1\) we get \(Q^{\prime}X\beta=\mu\) equaling \(Q^{\prime}Y=y_{3}\) with probability 1. Moreover, since \(X^{\prime}QQ^{\prime}X=1\) we can easily compute

\[\mu J_{3}=X\beta=AY=y_{3}J_{3}.\]The next two examples do not have \(Q^{\prime}X\) with full rank, so they actually have something to estimate.

Example 10.5.4: Consider a two-sample problem similar to Example 10.5.2 except that now the first two observations have variance 1 but the third has variance 0. The key matrices are

\[X=\begin{bmatrix}1&0\\ 1&0\\ 0&1\end{bmatrix},\quad V=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&0\end{bmatrix},\quad Q=\begin{bmatrix}0\\ 0\\ 1\end{bmatrix}.\]

With \(\beta=[\mu_{1},\,\mu_{2}]^{\prime}\) and

\[Q^{\prime}X=\begin{bmatrix}0&1\end{bmatrix},\]

we get

\[\mu_{2}=\begin{bmatrix}0&1\end{bmatrix}\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix}=Q^{\prime}X\beta=Q^{\prime}Y=y_{3}\ \ \ \text{a.s.}\]

Clearly, \(y_{3}=\mu_{2}\) a.s. but \(\mu_{1}\) would be estimated with the average \((y_{1}+y_{2})/2\). More on this later.

Example 10.5.5: Finally, in the two-sample problem we move the second observation from the first group to the second group. This time

\[X=\begin{bmatrix}1&0\\ 0&1\\ 0&1\end{bmatrix},\quad V=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&0\end{bmatrix},\quad Q=\begin{bmatrix}0\\ 0\\ 1\end{bmatrix},\]

with

\[Q^{\prime}X=\begin{bmatrix}0&1\end{bmatrix}.\]

Again, with probability 1,

\[\mu_{2}=\begin{bmatrix}0&1\end{bmatrix}\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix}=Q^{\prime}X\beta=Q^{\prime}Y=y_{3},\]

so \(y_{3}=\mu_{2}\) a.s. But this time, only \(y_{1}\) would be used to estimate \(\mu_{1}\), and \(y_{2}\) is of no value for estimating the means. However, \(y_{2}\) could be used to estimate an unknown variance via \((y_{2}-\mu_{2})^{2}=(y_{2}-y_{3})^{2}\). Similar results on estimating the variance apply in all the examples except Example 10.5.4.

To get perfect estimation of _anything_, we need \(C(X)\not\subset C(V)\). To get perfect estimation of _everything_ we need \(C(X)\,\cap\,C(V)=\{0\}\). In other words, to get perfect estimation of \(X\beta\) we need \(Q^{\prime}X\) with full column rank and to get \(Q^{\prime}X\) with full column rank, we need \(C(X)\,\cap\,C(V)=\{0\}\).

**Proposition 10.5.6**: _For \(X\) of full column rank, \(Q^{\prime}X\) is of full column rank if and only if \(C(X)\cap\ C(V)=\{0\}\)._

_Proof_ This is a special case of Lemma 10.5.7. \(\square\)

Although \(C(X)\cap\ C(V)=\{0\}\) is actually a necessary and sufficient condition for perfect estimation of \(X\beta\), with the methods we have illustrated it is not obviously sufficient. For our current method, we need \(Q^{\prime}X\) to have full column rank, which obviously will not happen if \(X\) is not full rank. Fortunately, we can always simply choose \(X\) to have full column rank. In addition, we close this section with the mathematics needed to deal with arbitrary \(X\).

Now consider models in which some aspect of \(X\beta\) is known but some aspect is not. In particular, we know that \(Q^{\prime}X\beta\) is known, but how do we estimate the rest of \(X\beta\)? As discussed above, we must now consider the case where \(C(X)\cap\ C(V)\neq\{0\}\). Write \(\beta=\beta_{0}+\beta_{1}\) with \(\beta_{0}\in C(X^{\prime}Q)\) and \(\beta_{1}\perp C(X^{\prime}Q)\). We show that \(X\beta_{0}\) is known, so that we need only estimate \(X\beta_{1}\) to learn all that can be learned about \(X\beta\). These methods make no assumption about \(r(X)\).

In fact, \(\beta_{0}\) is known, not just \(X\beta_{0}\). Let \(P_{X^{\prime}Q}\) be the ppo onto \(C(X^{\prime}Q)\). By the definition of \(\beta_{0}\) as part of an (unique) orthogonal decomposition, with probability 1,

\[\beta_{0}=P_{X^{\prime}Q}\beta=X^{\prime}Q[Q^{\prime}XX^{\prime}Q]^{-}Q^{\prime }X\beta=X^{\prime}Q[Q^{\prime}XX^{\prime}Q]^{-}Q^{\prime}Y.\]

Since the perpendicular projection operator does not depend on the choice of generalized inverse, neither does \(\beta_{0}\).

Now we show how to estimate \(X\beta_{1}\). Let \(V_{0}\) be such that \(C(V_{0})=C(X)\cap\ C(V)\). Proposition 10.4.6 can be used to find \(V_{0}\). Note that \(\beta_{1}\perp C(X^{\prime}Q)\) iff \(Q^{\prime}X\beta_{1}=0\) iff \(X\beta_{1}\perp C(Q)\) iff \(X\beta_{1}\in C(V)\) iff \(X\beta_{1}\in C(V_{0})\) iff \(X\beta_{1}=V_{0}\gamma\) for some \(\gamma\). Since \(X\beta_{0}\) is fixed and known, it follows that \(\operatorname{E}(Y-X\beta_{0})=X\beta_{1}\in C(V_{0})\) and \(\operatorname{Cov}(Y-X\beta_{0})=\sigma^{2}V\), so we can estimate \(X\beta_{1}\) by fitting

\[Y-X\beta_{0}=V_{0}\gamma+e,\quad\operatorname{E}(e)=0,\quad\operatorname{Cov} (e)=\sigma^{2}V, \tag{2}\]

and taking

\[X\hat{\beta}_{1}\equiv V_{0}\hat{\gamma}=V_{0}(V_{0}^{\prime}V^{-}V_{0})^{-}V_ {0}V^{-}(Y-X\beta_{0}),\]

cf. Section 2. Under normality, tests are also relatively easy to construct.

_Example 10.5.4 Continued._ Using the earlier versions of \(X\), \(V\), \(Q\), and \(Q^{\prime}X\), observe that

\[C(V_{0})=C\left(\begin{bmatrix}1\\ 1\\ 0\end{bmatrix}\right),\qquad C(X^{\prime}Q)=C\left(\begin{bmatrix}0\\ 1\end{bmatrix}\right).\]

It follows that with \(\beta=[\mu_{1},\,\mu_{2}]^{\prime}\),\[\beta_{0}=\begin{bmatrix}0\\ \mu_{2}\end{bmatrix},\qquad\beta_{1}=\begin{bmatrix}\mu_{1}\\ 0\end{bmatrix}.\]

Thus, since we already know that \(\mu_{2}=y_{3}\) a.s., \(X\beta_{0}=[0,0,\mu_{2}]^{\prime}=[0,0,y_{3}]^{\prime}\) and \(X\beta_{1}=[\mu_{1},\mu_{1},0]^{\prime}\). Finally, model (2) reduces, with probability 1, to

\[Y-X\beta_{0}=\begin{bmatrix}y_{1}\\ y_{2}\\ y_{3}-\mu_{2}\end{bmatrix}=\begin{bmatrix}y_{1}\\ y_{2}\\ 0\end{bmatrix}=\begin{bmatrix}1\\ 1\\ 0\end{bmatrix}\gamma+e.\]

Recalling that \(X\beta_{1}\equiv V_{0}\gamma\), it is easily seen in this example that the BLUE of \(\mu_{1}\equiv\gamma\) is \((y_{1}+y_{2})/2\).

This theory applied to Example 10.5.1 is quite degenerate, but it still works.

_Example 10.5.1 Continued._ Using the earlier versions of \(X\), \(V\), \(Q\), and \(Q^{\prime}X\), observe that

\[C(V_{0})=C\left(\begin{bmatrix}0\\ 0\\ 0\end{bmatrix}\right),\qquad C(X^{\prime}Q)=C\left(\begin{bmatrix}1&1\end{bmatrix} \right).\]

It follows that

\[\beta_{0}=\mu,\qquad\beta_{1}=0.\]

Since we already know that \(\mu=y_{2}=y_{3}\) a.s.,

\[X\beta_{0}=\begin{bmatrix}\mu\\ \mu\\ \mu\end{bmatrix}=\begin{bmatrix}y_{2}\\ y_{2}\\ y_{2}\end{bmatrix}=\begin{bmatrix}y_{3}\\ y_{3}\\ y_{3}\end{bmatrix}=\begin{bmatrix}y_{2}\\ y_{2}\\ y_{3}\end{bmatrix}\quad\text{a.s.}\]

and \(X\beta_{1}=[0,0,0]^{\prime}\). Finally, model (2) reduces, with probability 1, to

\[Y-X\beta_{0}=\begin{bmatrix}y_{1}-\mu\\ y_{2}-\mu\\ y_{3}-\mu\end{bmatrix}=\begin{bmatrix}y_{1}-y_{2}\\ 0\\ 0\end{bmatrix}=\begin{bmatrix}0\\ 0\\ 0\end{bmatrix}\gamma+e,\]

which provides us with one degree of freedom for estimating \(\sigma^{2}\) using either of \((y_{1}-y_{i})^{2},i=2,3\).

The results in the early part of this section on perfect estimation of \(X\beta\) required \(X\) to be of full rank. That is never a very satisfying state of affairs. Rather than assuming \(X\) to be of full rank and considering whether \(Q^{\prime}X\) is also of full rank, the more general condition for estimating \(X\beta\) perfectly is that \(r(X)=r(Q^{\prime}X)\). Moreover, with \(A\equiv X(X^{\prime}QQ^{\prime}X)^{-}X^{\prime}QQ^{\prime}\), we always have perfect estimation of \(AX\beta\) because \(AX\beta\) is a linear function of \(Q^{\prime}X\beta=Q^{\prime}Y\) a.s. but for perfect estimation of \(X\beta\) we need

\[X\beta=AX\beta=AY\quad\text{a.s.}\]for any \(\beta\) which requires \(A\) to be a projection operator onto \(C(X)\). This added generality requires some added work.

**Lemma 10.5.7**:
1. \(r(X)=r(Q^{\prime}X)\) _iff for any \(b\), \(Q^{\prime}Xb=0\) implies \(Xb=0\)._
2. \(r(X)=r(Q^{\prime}X)\) _iff_ \(C(X)\cap\ C(V)=\{0\}\)_._

_Proof_ Proof of (a): Recall that \(r(Q^{\prime}X)=r(X)\) iff \(r[\mathcal{N}(Q^{\prime}X)]=r[\mathcal{N}(X)]\). Since the null spaces have \(\mathcal{N}(X)\subset\mathcal{N}(Q^{\prime}X)\), it is enough to show that \(\mathcal{N}(Q^{\prime}X)=\mathcal{N}(X)\) is equivalent to the condition that for any \(b\), \(Q^{\prime}Xb=0\) implies \(Xb=0\) and, in particular, it is enough to show that \(\mathcal{N}(Q^{\prime}X)\subset\mathcal{N}(X)\) is equivalent to the condition. But by the very definition of the null spaces, \(\mathcal{N}(Q^{\prime}X)\subset\mathcal{N}(X)\) is equivalent to the condition that for any \(b\) we have \(Q^{\prime}Xb=0\) implies that \(Xb=0\).

Proof of (b): Note that for any \(b\),

\[Q^{\prime}Xb=0\ \ \ \text{iff}\ \ \ Xb\perp C(Q)\ \ \ \text{iff}\ \ \ Xb\in C(Q)^{\perp}=\left[C(V)^{\perp}\right]^{\perp}=C(V),\]

so

\[Q^{\prime}Xb=0\ \ \ \text{iff}\ \ \ Xb\in C(V)\ \ \ \text{iff}\ \ \ Xb\in C(X)\cap\ C(V).\]

If follows immediately that if \(C(X)\cap\ C(V)=\{0\}\), then \(Q^{\prime}Xb=0\) implies \(Xb=0\) and \(r(X)=r(Q^{\prime}X)\). It also follows immediately that since \(Q^{\prime}Xb=0\) is equivalent to having \(Xb\in C(X)\cap\ C(V)\), the condition that \(Q^{\prime}Xb=0\) implies \(Xb=0\) means that the only vector in \(C(X)\cap\ C(V)\) is the \(0\) vector. \(\square\)

**Proposition 10.5.8**: _If \(r(X)=r(Q^{\prime}X)\), the matrix \(A\equiv X(X^{\prime}QQ^{\prime}X)^{-}X^{\prime}QQ^{\prime}\) is a projection operator onto \(C(X)\)._

_Proof_ By its definition we clearly have \(C(A)\subset C(X)\), so it is enough to show that \(AX=X\).

Let \(M_{Q^{\prime}X}\) be the ppo onto \(C(Q^{\prime}X)\). Note that for any \(b\), \(Xb-AXb\in C(X)\). Moreover, from the definitions of \(A\) and \(M_{Q^{\prime}X}\),

\[Q^{\prime}Xb-Q^{\prime}AXb=Q^{\prime}Xb-M_{Q^{\prime}X}Q^{\prime}Xb=0.\]

Writing

\[0=Q^{\prime}Xb-M_{Q^{\prime}X}Q^{\prime}Xb=Q^{\prime}X\left[I-(X^{\prime}QQ^{ \prime}X)^{-}X^{\prime}QQ^{\prime}X\right]b,\]

by the condition \(r(X)=r(Q^{\prime}X)\) and Lemma 10.5.7a, we have

\[0=X\left[I-(X^{\prime}QQ^{\prime}X)^{-}X^{\prime}QQ^{\prime}X\right]b=Xb-AXb,\]

hence \(X=AX\)

**Exercise 10.14**: Show that the results in this section do not depend on the particular choice of \(Q\).

**Exercise 10.15**: Let \(C(V_{0})=C(X)\cap\,C(V)\), \(C(X)=C(V_{0},\,X_{1})\), \(C(V)=C(V_{0},\,V_{1})\) with the columns of \(V_{0}\), \(V_{1}\), and \(X_{1}\) being orthonormal. Show that the columns of \([V_{0},\,V_{1},\,X_{1}]\) are linearly independent.

Hint: Write \(V_{0}b_{0}+V_{1}b_{1}+X_{1}b_{2}=0\) and show that \(b_{i}=0\), \(i=0\), \(1\), \(2\). In particular, write

\[0.5V_{0}b_{0}+V_{1}b_{1}=-(0.5V_{0}b_{0}+X_{1}b_{2}),\]

\(0.5V_{0}b_{0}+V_{1}b_{1}\in C(V)\,\mbox{and}-(0.5V_{0}b_{0}+X_{1}b_{2})\in C(X)\) so the vector is in \(C(V_{0})=C(X)\cap\,C(V)\).

## References

* Christensen & Lin (2010) Christensen, R., & Lin, Y. (2010). Linear models that allow perfect estimation. _Statistical Papers_, _54_, 695-708.
* Ferguson (1967) Ferguson, T. S. (1967). _Mathematical statistics: A decision theoretic approach_. New York: Academic.
* Gross (2004) Gross, J. (2004). The general Gauss-Markov model with possibly singular dispersion matrix. _Statistical Papers_, _25_, 311-336.
* Rao & Mitra (1971) Rao, C. R., & Mitra, S. K. (1971). _Generalized inverse of matrices and its applications_. New York: Wiley.
* Searle & Pukelsheim (1987) Searle, S. R., & Pukelsheim, F. (1987). Estimation of the mean vector in linear models, _Technical Report BU-912-M, Biometrics Unit_. Ithaca, NY: Cornell University.

## Chapter 11 Split Plot Models

enough blocks available that can accommodate all of the treatment combinations. In split plot designs, this means that there are not enough subplots per whole plot so that all treatment combinations could be applied at the subplot level. If, in addition, there are not enough whole plots so that each treatment combination could be applied to a whole plot, then a split plot design is an attractive option.

Mathematically, the key characteristic of a split plot model is the covariance structure. Typically, observations taken on the subplots of any particular whole plot are assumed to have a constant positive correlation. Observations taken on different whole plots are assumed to be uncorrelated.

The main purpose of this chapter is to derive the analysis for split plot models. In Section 1, we consider a special cluster sampling model. The cluster sampling model has the same covariance structure as a split plot model. In Section 2, we consider ways of generalizing the cluster sampling model that allow for an easy analysis of the data. The discussion in Section 2 is really an examination of generalized split plot models. Section 3 derives the analysis for the traditional split plot model by using the results of Section 2. Section 4 discusses the issues of identifying an appropriate error term and of subsampling.

Sections 1 and 2 are closely related to Christensen (1984) and (1987), respectively. In fact, Christensen (1987) is probably easier to read than Section 2 because it includes more introductory material and fewer of the mathematical details. Closely related work is contained in Monlezun and Blouin (1988) and Mathew and Sinha (1992). Christensen (2015) examines practical issues associated with applying this theory. A general review of methods for analyzing cluster sampling models is given in Skinner, Holt, and Smith (1989).

### A Cluster Sampling Model

A commonly used technique in survey sampling is _cluster sampling_ (also called _two-stage sampling_). This technique is applied when the population to be sampled consists of some kind of clusters. The sample is obtained by taking a random sample of clusters and then taking a random sample of the individuals within each of the sampled clusters. For example, suppose it was desired to sample the population of grade school students in Montana. One could take a random sample of grade schools in the state, and then for each school that was chosen take a random sample of the students in the school. One complication of this method is that students from the same school will tend to be more alike than students from different schools. In general, there will be a nonnegative correlation among the individual units within a cluster.

_General Cluster Sampling Models_

Suppose \(n\) observations are available from a two-stage sample with \(c\) clusters. From each cluster, \(m_{i}\) units are sampled and variables \(y,x_{1},\ldots,x_{p}\) are obtained. Since observations in a cluster are typically not independent, we will consider the linear model

\[Y=X\beta+e,\ \ \ e\sim N\big{(}0,\sigma^{2}V\big{)}\,,\]where \(X\) is \(n\times p\) of rank \(r\) and (assuming the elements of \(Y\) are listed by clusters) \(V\) is the block diagonal matrix

\[V=\text{Blk diag}(V_{i}),\]

where \(V_{i}\) is an \(m_{i}\times m_{i}\)_intraclass correlation_ matrix

\[V_{i}=\left[\begin{array}{cccc}1&\rho&\cdots&\rho\\ \rho&1&\cdots&\rho\\ \vdots&\vdots&\ddots&\vdots\\ \rho&\rho&\cdots&1\end{array}\right].\]

If we let \(J_{m(i)}\) be an \(m_{i}\times 1\) vector of ones, then

\[V=(1-\rho)I+\rho\text{Blk diag}(J_{m(i)}J^{\prime}_{m(i)}).\]

Now let \(X_{1}\) be an \(n\times c\) matrix of indicator variables for the clusters. In other words, a row of the \(i\)th column of \(X_{1}\) is 1 if the row corresponds to an observation from the \(i\)th cluster and 0 otherwise. It follows that \(X_{1}X^{\prime}_{1}=\text{Blk diag}(J_{m(i)}J^{\prime}_{m(i)})\), so

\[V=(1-\rho)I+\rho X_{1}X^{\prime}_{1}. \tag{1}\]

In fact, equation (1) holds even if the elements of \(Y\) are not listed by cluster.

We can now provide an interesting condition for when ordinary least squares (OLS) estimates are best linear unbiased estimates (BLUEs) in cluster sampling models. Recall from Theorem 10.4.5 that OLS estimates are BLUEs if and only if \(C(VX)\subset C(X)\). This condition holds if and only if \(C(X_{1}X^{\prime}_{1}X)\subset C(X)\). Since the columns of \(X_{1}\) are indicator variables for the clusters, \(X_{1}X^{\prime}_{1}X\) takes each column of \(X\), computes the cluster totals, and replaces each component with the corresponding cluster total. Thus, OLS estimates are BLUEs if and only if for any variable in the model, the variable formed by replacing each component with the corresponding cluster total is also, either implicitly or explicitly, contained in the model.

_A Special Cluster Sampling Model_

We now consider a particular cluster sampling model for which OLS estimates are BLUEs, and for which tests and confidence intervals are readily available for the most interesting parameters. Consider a model in which \(X\) can be written as \(X=[X_{1},\,X_{2}]\), where \(X_{1}\) is again the matrix of indicator variables for the clusters. Rewriting the linear model as

\[Y=[X_{1},\,X_{2}]\left[\begin{array}{c}\alpha\\ \gamma\end{array}\right]+e \tag{2}\]

leads to the interpretation that the \(\alpha_{i}\)s are separate cluster effects. Typically, one would not be very interested in these cluster effects. One's primary interest would be in the vector \(\gamma\).

It is easily seen that \(C(VX)\subset C(X)\), so OLS estimates are BLUEs. The noteworthy thing about this model is that inference on the parameter vector \(\gamma\) can proceed just as when \(\text{Cov}(Y)=\sigma^{2}I\). Treating (2) as an analysis of covariance model, we obtain for any estimable function \(\lambda^{\prime}\gamma\)

\[\lambda^{\prime}\hat{\gamma}=\lambda^{\prime}\left[X_{2}^{\prime}(I-M_{1})X_{2} \right]^{-}X_{2}^{\prime}(I-M_{1})Y,\]

where \(M_{1}\) is the perpendicular projection matrix onto \(C(X_{1})\).

The variance of \(\lambda^{\prime}\hat{\gamma}\) is

\[\text{Var}(\lambda^{\prime}\hat{\gamma})=\sigma^{2}\lambda^{\prime}\left[X_{2}^ {\prime}(I-M_{1})X_{2}\right]^{-}X_{2}^{\prime}(I-M_{1})V(I-M_{1})X_{2}\left[X_ {2}^{\prime}(I-M_{1})X_{2}\right]^{-}\lambda. \tag{3}\]

From (1) observe that

\[V(I-M_{1})=(1-\rho)(I-M_{1}).\]

Substitution into (3) gives

\[\text{Var}(\lambda^{\prime}\hat{\gamma})=\sigma^{2}(1-\rho)\lambda^{\prime} \left[X_{2}^{\prime}(I-M_{1})X_{2}\right]^{-}\lambda, \tag{4}\]

which, except for the term \((1-\rho)\), is the variance from assuming \(\text{Cov}(Y)=\sigma^{2}I\).

**Exercise 11.1**: Prove that equation (4) is true.

The mean square error (_MSE_) from ordinary least squares provides an independent estimate of \(\sigma^{2}(1-\rho)\). Let \(M=X(X^{\prime}X)^{-}X^{\prime}\), so \(MSE=Y^{\prime}(I-M)Y/(n-r)\).

\[\text{E}(MSE)=(n-r)^{-1}\sigma^{2}\text{tr}[(I-M)V].\]

Since \(C(X_{1})\subset C(X)\), from (1) we have

\[(I-M)V=(I-M)(1-\rho)I=(1-\rho)(I-M).\]

But, \(\text{tr}[(1-\rho)(I-M)]=(1-\rho)(n-r)\), so

\[\text{E}(MSE)=\sigma^{2}(1-\rho).\]

**Theorem 11.1.1**: \(\quad\)__

_(i)_ \(\quad Y^{\prime}(I-M)Y/\sigma^{2}(1-\rho)\sim\chi^{2}(n-r,0)\)_._

_(ii)_ \(\quad MSE\) _and_ \(X\hat{\beta}\) _are independent. In particular, MSE and_ \(\lambda^{\prime}\hat{\gamma}\) _are independent._

**Exercise 11.2**  Prove Theorem 11.1.1.

Hint: For (i), use Theorem 1.3.6. For (ii), show that \(\operatorname{Cov}[(I-M)Y,\,MY]=0\).

These results provide a basis for finding tests and confidence intervals for an estimable function \(\lambda^{\prime}\gamma\). We might also want to consider doing \(F\) tests. Suppose we want to test some vector of estimable restrictions on \(\gamma\), say \(\Lambda^{\prime}\gamma=0\). The test can be derived from \(\Lambda^{\prime}\hat{\gamma}\), \(\operatorname{Cov}(\Lambda^{\prime}\hat{\gamma})\), and _MSE_, using Theorem 11.1.1 and Corollary 3.8.3. In particular,

**Theorem 11.1.2**

_(i)_ \(\frac{\left(\Lambda^{\prime}\hat{\gamma}\right)^{\prime}\left(\Lambda^{\prime} \left[X_{2}^{\prime}(I-M_{1})X_{2}\right]^{-}\Lambda\right)^{-}\left(\Lambda^ {\prime}\hat{\gamma}\right)/r(\Lambda)}{MSE}\sim F(r(\Lambda),\,n-r,\pi)\),

_where \(\pi=\left(\Lambda^{\prime}\gamma\right)^{\prime}\left(\Lambda^{\prime}\left[X_ {2}^{\prime}(I-M_{1})X_{2}\right]^{-}\Lambda\right)^{-}\left(\Lambda^{\prime} \gamma\right)/2\sigma^{2}(1-\rho)\)._

_(ii)_ \(\left(\Lambda^{\prime}\gamma\right)^{\prime}\left(\Lambda^{\prime}\left[X_{2}^ {\prime}(I-M_{1})X_{2}\right]^{-}\Lambda\right)^{-}\left(\Lambda^{\prime} \gamma\right)=0\) _if and only if \(\Lambda^{\prime}\gamma=0\)._

An alternative to testing linear parametric functions is testing models. To test model (2) against a reduced model, say

\[Y=X_{0}\beta_{0}+e,\quad C(X_{1})\subset C(X_{0})\subset C(X),\]

the test is the usual \(\operatorname{Cov}(Y)=\sigma^{2}I\) test. With \(M_{0}=X_{0}(X_{0}^{\prime}X_{0})^{-}X_{0}^{\prime}\), we have:

**Theorem 11.1.3**

_(i)_ \(\frac{Y^{\prime}(M-M_{0})Y/[r(X)-r(X_{0})]}{MSE}\sim F(r(X)-r(X_{0}),\,n-r,\pi)\),

_where \(\pi=\beta_{0}^{\prime}X^{\prime}(M-M_{0})X\beta_{0}/2\sigma^{2}(1-\rho)\)._

_(ii)_ \(\beta_{0}^{\prime}X^{\prime}(M-M_{0})X\beta_{0}=0\) _if and only if \(\operatorname{E}(Y)\in C(X_{0})\)._

_Proof_ For part (i), see Exercise 11.3. Part (ii) follows exactly as in Theorem 3.2.1. \(\square\)

**Exercise 11.3**  Prove Theorem 11.1.3(i).

In summary, for a model that includes separate fixed effects for each cluster, the ordinary least squares fit gives optimal estimates of all effects and valid estimates of standard errors for all effects not involving the cluster effects. If normal distributions are assumed, the usual \(\operatorname{Cov}(Y)=\sigma^{2}I\) tests and confidence intervals are valid unlessthe cluster effects are involved. If the cluster effects are not of interest, the entire analysis can be performed with ordinary least squares. This substantially reduces the effort required to analyze the data.

The assumption that the \(\alpha_{i}\)s are fixed effects is necessary for the result to hold. However, if additional random cluster effects are added to the model so that there are both fixed and random cluster effects, then the basic structure of the covariance matrix remains unchanged and the optimality of ordinary least squares is retained.

#### Exercise 11.4

The usual model for a randomized complete block design was given in Section 8.2 as

\[y_{ij}=\mu+\alpha_{i}+\beta_{j}+e_{ij},\]

\(i=1,\ldots,a,\;\;j=1,\ldots,b,\;\;\mbox{Var}(e_{ij})=\sigma^{2},\;\;\mbox{and }\;\mbox{Cov}(e_{ij},e_{i^{\prime}j^{\prime}})=0\;\;\mbox{for }\;(i,j)\neq(i^{\prime},j^{\prime})\). The \(\beta_{j}\)s are considered as fixed block effects. Consider now a model

\[y_{ij}=\mu+\alpha_{i}+\beta_{j}+\eta_{j}+e_{ij}.\]

The \(\eta_{j}\)s are independent \(N(0,\sigma_{2}^{2})\) and the \(e_{ij}\)s are independent \(N(0,\sigma_{1}^{2})\). The \(\eta_{j}\)s and \(e_{ij}\)s are also independent. The block effects are now \((\beta_{j}+\eta_{j})\). There is a fixed component and a random component with mean zero in each block effect. Use the results of this section to derive an analysis for this model. Give an ANOVA table and discuss interval estimates for contrasts in the \(\alpha_{i}\)s.

#### Exercise 11.5

An alternative model for a block design is

\[y_{ij}=\mu+\alpha_{i}+\beta_{j}+e_{ij}, \tag{5}\]

where the \(\beta_{j}\)s are independent \(N(0,\sigma_{2}^{2})\) and the \(\beta_{j}\)s and \(e_{ij}\)s are independent. If this model is used for a balanced incomplete block design, the BLUEs are different than they are when the \(\beta_{j}\)s are assumed to be fixed. Discuss the appropriateness of the analysis based on this model in light of the results of Exercise 11.4.

#### Exercise 11.6

Show that when using model (5) for a randomized complete block design, the BLUE of a contrast in the \(\alpha_{i}\)s is the same regardless of whether the \(\beta_{j}\)s are assumed random or fixed. Show that the estimates of a contrast's variance are the same.

### Generalized Split Plot Models

By placing additional conditions on model (11.1.2), we can get a simple analysis of the cluster effects while retaining a simple analysis for the noncluster effects. The analysis of the cluster effects corresponds to the analysis of whole-plot treatments in a split plot model. The noncluster effects relate to effects on the subplot level.

_Generalized split plot models_ are models obtained by imposing additional structure on the cluster sampling model (11.1.2). This additional structure involves simplifying the covariance matrix and modeling the whole-plot (cluster) effects. First, a condition on the whole plots is discussed. The condition is that the number of observations in each whole plot is the same. This condition simplifies the covariance matrix considerably. Next, the whole-plot effects are modeled by assuming a reduced model that does not allow separate effects for each whole plot. As part of the modeling process, a condition is imposed on the model matrix of the reduced model that ensures that least squares estimates are BLUEs. The problem of drawing inferences about generalized split plot models is discussed in two parts, (1) estimation and testing of estimable functions and (2) testing reduced models. A condition that allows for a simple analysis of the whole-plot effects is mentioned, a discussion of how to identify generalized split plot models is given, and finally some computational methods are presented.

_The Covariance Matrix_

Writing \(Y\) so that observations in each whole plot are listed contiguously, we can rewrite the covariance matrix of model (11.1.2) as

\[\sigma^{2}V=\sigma^{2}\left[(1-\rho)I+\rho M_{1}[\text{Blk diag}(m_{i}I_{m(i)}) \right]\right],\]

where \(I_{m(i)}\) is an \(m_{i}\times m_{i}\) identity matrix and \(M_{1}\) is the perpendicular projection matrix onto \(C(X_{1})\). This follows from Section 1 because \(M_{1}=\text{Blk diag}(m_{i}^{-1}J_{m(i)}\)\(J^{\prime}_{m(i)})\). This characterization of \(V\) is not convenient in itself because of the Blk diag (\(m_{i}I_{m(i)}\)) term. For example, the expected value of a quadratic form, say \(Y^{\prime}AY\), is \(\text{E}(Y^{\prime}AY)=\sigma^{2}\text{tr}(AV)+\beta^{\prime}X^{\prime}AX\beta\). The trace of \(AV\) is not easy to compute. To simplify the subsequent analysis, we impose

**Condition 11.2.1**: _All whole plots (clusters) are of the same size, say \(m_{i}=m\) for all \(i\)._

It follows that

\[V=(1-\rho)I+m\rho M_{1}. \tag{1}\]

This form for \(V\) will be assumed in the remainder of Section 2.

_Modeling the Whole Plots_

Model (11.1.2) includes \(X_{1}\) which determines separate effects for each whole plot (cluster). Modeling the cluster effects consists of imposing structure on those effects. This is done by putting a constraint on \(C(X_{1})\). The simplest way to do this is by postulating a reduced model, say

\[Y=Z\beta_{*}+e,\quad Z=[X_{*},X_{2}]\,,\quad C(X_{*})\subset C(X_{1}). \tag{2}\]Partitioning \(\beta_{*}\) in conformance with \(Z\), write

\[\beta_{*}^{\prime}=\left[\delta^{\prime},\gamma^{\prime}\right].\]

Remember, this is not the same \(\gamma\) as in (11.1.2), but it is the coefficient for \(X_{2}\), just as in (11.1.2). Define the perpendicular projection operators onto \(C(Z)\) and \(C(X_{*})\) as \(M_{Z}\) and \(M_{*}\), respectively.

In Section 1, it was shown that least squares estimates were BLUEs for model (11.1.2). We are now dealing with a different model, model (2), so another proof is required. To check if least squares estimates are BLUEs, we need to see whether \(C(VZ)\subset C(Z)\). Since by equation (1), \(V=(1-\rho)I+m\rho M_{1}\), we have \(VZ=(1-\rho)Z+m\rho M_{1}Z\). Clearly, it is enough to check whether \(C(M_{1}Z)\subset C(Z)\). This is true for a special case.

**Proposition 11.2.2**: _Let \(\mathcal{M}\) and \(\mathcal{N}\) be subspaces of \(C(Z)\). If \(C(Z)=\mathcal{M}+\mathcal{N}\), where \(\mathcal{M}\subset C(X_{1})\) and \(\mathcal{N}\perp C(X_{1})\), then \(C(M_{1}Z)\subset C(Z)\)._

_Proof_ For any \(v\in C(Z)\), write \(v=v_{1}+v_{2}\), where \(v_{1}\in\mathcal{M}\) and \(v_{2}\in\mathcal{N}\). \(M_{1}v=M_{1}v_{1}+M_{1}v_{2}=v_{1}\), but \(v_{1}\in\mathcal{M}\subset C(Z)\). \(\square\)

A condition that is easy to check is

**Condition 11.2.3**: \(C(Z)=C[X_{*},(I-M_{1})X_{2}]\) _and \(C(X_{*})\subset C(X_{1})\)._

If Condition 11.2.3 holds, then Proposition 11.2.2 applies with \(\mathcal{M}=C(X_{*})\) and \(\mathcal{N}=C[(I-M_{1})X_{2}]\). If Proposition 11.2.2 applies, then least squares estimates are BLUEs. In fact, if \(C(X_{*})\subset C(X_{1})\), all we really have to check is whether \(C(M_{1}X_{2})\subset C(X_{*})\).

#### Examples

_Example 11.2.4A_ Let whole plots be denoted by the subscripts \(i\) and \(j\), and let subplots have the subscript \(k\). Let the dependent variable be \(y_{ijk}\) and let \(x_{ijk1}\), \(x_{ijk2}\), and \(x_{ijk3}\) be three covariates. The model given below is a generalized split plot model (see Exercise 11.7.):

\[y_{ijk} = \mu+\omega_{i}+\gamma_{1}\bar{x}_{ij\cdot 1}+\gamma_{21}\bar{x}_ {ij\cdot 2}+\eta_{ij} \tag{3}\] \[+\tau_{k}+(\omega\tau)_{ik}+\gamma_{22}(x_{ijk2}-\bar{x}_{ij\cdot 2 })+\gamma_{3}(x_{ijk3}-\bar{x}_{ij\cdot 3})\] \[+e_{ijk},\]

\(i=1\), \(\ldots\), \(a\), \(j=1\), \(\ldots\), \(N_{i}\), \(k=1\), \(\ldots\), \(m\). The \(\eta_{ij}\)s and \(e_{ijk}\)s are all independent with \(\eta_{ij}\sim N(0,\sigma_{w}^{2})\) and \(e_{ijk}\sim N(0,\sigma_{s}^{2})\). With these assumptions

\[\sigma^{2}=\sigma_{w}^{2}+\sigma_{s}^{2}\]and

\[\rho=\sigma_{w}^{2}/(\sigma_{w}^{2}+\sigma_{s}^{2}).\]

The \(\omega_{i}\)s are treatment effects for a one-way ANOVA with unequal numbers in the whole plots. The whole-plot treatments can obviously be generalized to include multifactor ANOVAs with unequal numbers. The \(\omega_{i}\)s, \(\gamma_{1}\), \(\gamma_{21}\), and \(\mu\) make up the \(\delta\) vector. The \(\tau_{k}\)s, \((\omega\tau)_{ik}\)s, \(\gamma_{22}\), and \(\gamma_{3}\) make up the vector \(\gamma\) from model (2). Note that the covariate used with \(\gamma_{22}\) could be changed to \(x_{ijk2}\) without changing \(C(Z)\) or invalidating Condition 11.2.3.

##### Exercise 11.7

Verify that Condition 11.2.3 holds for model (3).

It is easy to establish whether the \(m_{i}\)s are all equal and it is easy to pick an \(X_{*}\) with \(C(X_{*})\subset C(X_{1})\) because each column of \(X_{*}\) just has to take the same value for every subplot in a given whole plot. The difficulty in establishing Condition 11.2.3 is showing that \(C(M_{1}X_{2})\subset C(X_{*})\).

When the whole-plot model involves a balanced ANOVA, the orthogonality relationships discussed in Chapter 7 can often be used to establish Condition 11.2.3. In particular, whenever \(C(J)\subset C(X_{*})\), Condition 11.2.3 holds if

\[X_{1}^{\prime}[I-(1/n)J_{n}^{n}]X_{2}=0.\]

Equivalent conditions are

\[X_{1}^{\prime}X_{2}=X_{1}^{\prime}(1/n)J_{n}^{n}X_{2},\]

and also

\[C(M_{1}X_{2})=C[(1/n)J_{n}^{n}X_{2}]\]

which implies

\[C[(I-M_{1})X_{2}]=C[(I-(1/n)J_{n}^{n}]X_{2}].\]

These conditions are easy to check on a computer, but they may not apply when including something as simple as a whole-plot treatment by subplot treatment interaction.

##### Example 11.2.4b

If whole-plot treatments exist in the model and if the subplot model contains _only_ balanced subplot treatments and whole-plot treatment by subplot interaction, then Condition 3 always holds. Let \(i\) identify the whole-plot treatments, let the pair \(ij\) identify the whole plots, and \(k\) identify the subplots (each of which gets a different subplot treatment). Let \(X_{*1}\) identify the whole-plot treatments, \(X_{21}\) identify the subplot treatments, and \(X_{22}\) identify the whole-plot by subplot interaction effect, i.e., \(X_{*}=[X_{*1},X_{*2}]\), where we don't care what \(X_{*2}\) is, and \(X_{2}=[X_{21},X_{22}]\). We show that \(C(M_{1}X_{21})\subset C(J)\) and \(C(M_{1}X_{22})\subset C(X_{*1})\).

Write the \(r\)th column of \(X_{*1}\) as

\[X_{*1r}=[t_{ijk}],\quad t_{ijk}=\delta_{ir}.\]

Then, as for any one-way ANOVA, \(M_{1}Y\) just averages observations within the whole-plots, i.e.,

\[M_{1}Y=[t_{ijk}]\quad t_{ijk}=\bar{y}_{ij}.\]

Most importantly, the formula for \(M_{1}Y\) determines what \(M_{1}\) does to any vector.

Write the \(s\)th column of \(X_{21}\) as

\[X_{21s}=[t_{ijk}],\quad t_{ijk}=\delta_{ks}.\]

Then

\[M_{1}X_{21s}=[t_{ijk}],\quad t_{ijk}=1/m,\]

so \(M_{1}X_{21s}\in C(J)\). Similarly, write the \(rs\) column of \(X_{22}\) as

\[X_{22rs}=[t_{ijk}],\quad t_{ijk}=\delta_{(i,k)(r,s)}.\]

Then

\[M_{1}X_{22rs}=[t_{ijk}],\quad t_{ijk}=(1/m)\sum_{k=1}^{m}\delta_{(i,k)(r,s)}=( 1/m)\delta_{ir},\]

so \(M_{1}X_{22rs}\in C(X_{*1})\).

Of course this result generalizes immediately whenever the whole-plot treatments or subplot treatments have factorial treatment structure and it can serve as a tool for demonstrating Condition 11.2.3 in models with a more general \(X_{2}\).

#### Estimation and Testing of Estimable Functions

We now discuss estimation and testing for model (2). Under Condition 11.2.1 and Condition 11.2.3, least squares estimates are BLUEs. Define

\[M_{2}\equiv(I-M_{1})X_{2}\left[X_{2}^{\prime}(I-M_{1})X_{2}\right]^{-}X_{2}^{ \prime}(I-M_{1}).\]

From Condition 11.2.3, the perpendicular projection operator onto \(C(Z)\) is

\[M_{Z}=M_{*}+M_{2}. \tag{4}\]

Given the perpendicular projection operator, the least squares estimates can be found in the usual way.

First, consider drawing inferences about \(\gamma\). For estimable functions of \(\gamma\), the estimates are exactly as in Section 1. In both cases, the estimates depend only on \(M_{2}Y\). (See Proposition 9.1.1.) Since model (11.1.2) is a larger model than model (11.1.2) [i.e., \(C(Z)\subset C(X)\)], model (11.1.2) remains valid. It follows that all of the distributional results in Section 1 remain valid. In particular,

\[Y^{\prime}(I-M)Y/\sigma^{2}(1-\rho)\sim\chi^{2}(n-r(X),0), \tag{5}\]

and \(Y^{\prime}(I-M)Y/[n-r(X)]\) is an unbiased estimate of \(\sigma^{2}(1-\rho)\). In split plot models, \(\sigma^{2}(1-\rho)\) is called the _subplot error variance_. \(Y^{\prime}(I-M)Y\) is called the _sum of squares for subplot error_ [\(SSE(s)\)] and \(Y^{\prime}(I-M)Y/[n-r(X)]\) is the _mean square for subplot error_ [\(MSE(s)\)].

The results in Section 1 are for functions of \(\gamma\) that are estimable in model (11.1.2). We now show that \(\lambda^{\prime}\gamma\) is estimable in (11.1.2) if and only if \(\lambda^{\prime}\gamma\) is estimable in (2). The argument is given for real-valued estimable functions, but it clearly applies to vector-valued estimable functions.

First, suppose that \(\lambda^{\prime}\gamma\) is estimable in (11.1.2). Then there exists a vector \(\xi\) such that \(\lambda^{\prime}=\xi^{\prime}X_{2}\) and \(\xi^{\prime}X_{1}=0\). It follows immediately that \(\lambda^{\prime}=\xi^{\prime}X_{2}\) and \(\xi^{\prime}X_{*}=0\), so \(\lambda^{\prime}\gamma\) is estimable in model (2).

Now suppose that \(\lambda^{\prime}\gamma\) is estimable in model (2). There exists a vector \(\xi\) such that \(\xi^{\prime}X_{*}=0\) and \(\xi^{\prime}X_{2}=\lambda^{\prime}\). Using equation (4) and \(\xi^{\prime}M_{*}=0\), it is easily seen that

\[\lambda^{\prime}=\xi^{\prime}X_{2}=\xi^{\prime}M_{Z}X_{2}=\xi^{\prime}M_{2}X_{2}\]

and, since \(M_{2}X_{1}=0\),

\[\xi^{\prime}M_{2}X_{1}=0.\]

The vector \(\xi^{\prime}M_{2}\) satisfies the two conditions needed to show that \(\lambda^{\prime}\gamma\) is estimable in (11.1.2). Thus, inferences about estimable functions \(\lambda^{\prime}\gamma\) can be made exactly as in Section 1.

Drawing inferences about \(\delta\) is trickier. The projection operator \(M_{Z}\) is the sum of two orthogonal projection operators \(M_{*}\) and \(M_{2}\). Estimation of \(\lambda^{\prime}\gamma\) can be accomplished easily because the estimate depends on \(M_{2}Y\) alone. Similarly, estimable functions whose estimates depend on \(M_{*}Y\) alone can be handled simply. The problem lies in identifying which estimable functions have estimates that depend on \(M_{*}Y\) alone. Since estimable functions of \(\gamma\) depend on \(M_{2}Y\), any estimable function with an estimate that depends on \(M_{*}Y\) alone must involve \(\delta\). (Of course, there exist estimable functions that depend on both \(M_{*}Y\) and \(M_{2}Y\).) Later, a condition will be discussed that forces all estimable functions of \(\delta\) to depend only on \(M_{*}Y\). With this condition, we have a convenient dichotomy in that estimates of functions of \(\delta\) depend on the perpendicular projection operator \(M_{*}\), and estimates of functions of \(\gamma\) depend on the perpendicular projection operator \(M_{2}\). The condition referred to is convenient, but it is not necessary for having a generalized split plot model.

As discussed in Chapter 3 the question of whether the estimate of an estimable function, say \(\Lambda^{\prime}\beta_{*}\), depends only on \(M_{*}Y\) is closely related to the constraint on the model imposed by the hypothesis \(\Lambda^{\prime}\beta_{*}=0\). In particular, if \(\Lambda^{\prime}=P^{\prime}Z\), then the constraint imposed by \(\Lambda^{\prime}\beta_{*}=0\) is \(\operatorname{E}(Y)\perp C(M_{Z}P)\), and \(\Lambda^{\prime}\hat{\beta}_{*}\) depends on \(M_{*}Y\) if and only if \(C(M_{Z}P)\subset C(M_{*})=C(X_{*})\). _In the discussion that follows, \(\Lambda^{\prime}\beta_{*}=0\) is assumed to put a constraint on \(C(X_{*})\)._

We seek to derive an \(F\) test for \(\Lambda^{\prime}\beta_{*}=0\). From Corollary 3.8.3,

\[\left(\Lambda^{\prime}\hat{\beta}_{*}\right)^{\prime}\left[\operatorname{Cov} \left(\Lambda^{\prime}\hat{\beta}_{*}\right)\right]^{-}\left(\Lambda^{\prime} \hat{\beta}_{*}\right)\sim\chi^{2}\left(r(\Lambda),\left(\Lambda^{\prime}\beta _{*}\right)^{\prime}\left[\operatorname{Cov}\left(\Lambda^{\prime}\hat{\beta}_ {*}\right)\right]^{-}\left(\Lambda^{\prime}\beta_{*}\right)/2\right).\]

We need the covariance of \(\Lambda^{\prime}\hat{\beta}_{*}\). Note that \(\Lambda^{\prime}\hat{\beta}_{*}=P^{\prime}M_{Z}Y=P^{\prime}M_{*}Y\). From equation (1) and the fact that \(C(X_{*})\subset C(X_{1})\), it is easily seen that

\[M_{*}V=\left[(1-\rho)+m\rho\right]M_{*}; \tag{6}\]

so

\[\operatorname{Cov}\left(\Lambda^{\prime}\hat{\beta}_{*}\right) =\sigma^{2}P^{\prime}M_{*}VM_{*}P=\sigma^{2}\left[(1-\rho)+m\rho \right]P^{\prime}M_{*}P\] \[=\sigma^{2}\left[(1-\rho)+m\rho\right]P^{\prime}M_{Z}P\] \[=\sigma^{2}\left[(1-\rho)+m\rho\right]\Lambda^{\prime}\left(Z^{ \prime}Z\right)^{-}\Lambda,\]

which, except for the term \((1-\rho)+m\rho\), is the usual covariance of \(\Lambda^{\prime}\hat{\beta}_{*}\) from a standard linear model. We can get an \(F\) test of \(\Lambda^{\prime}\beta_{*}=0\) if we can find an independent chi-squared estimate of \(\sigma^{2}\left[(1-\rho)+m\rho\right]\).

**Theorem 11.2.4**: _Under model (2),_

\[Y^{\prime}(M_{1}-M_{*})Y/\sigma^{2}\left[(1-\rho)+m\rho\right]\sim\chi^{2}(r(X _{1})-r(X_{*}),0)\;.\]

_Proof_ Observe that

\[M_{1}V=\left[(1-\rho)+m\rho\right]M_{1}. \tag{7}\]

Using equations (6) and (7), it is easy to check the conditions of Theorem 1.3.6. It remains to show that \(\beta_{*}^{\prime}Z^{\prime}\left(M_{1}-M_{*}\right)Z\beta_{*}=0\). Recall that \(M_{Z}=M_{*}+M_{2}\), and note that \(M=M_{1}+M_{2}\). It follows that \(M_{1}-M_{*}=M-M_{Z}\). Clearly, \((M-M_{Z})Z\beta_{*}=0\). \(\square\)

The quadratic form \(Y^{\prime}(M_{1}-M_{*})Y\) is called the _sum of squares for whole-plot (cluster) error_. This is denoted \(SSE(w)\). An unbiased estimate of \(\sigma^{2}\left[(1-\rho)+m\rho\right]\) is available from

\[MSE(w)\equiv Y^{\prime}(M_{1}-M_{*})Y\Big{/}\left[r(X_{1})-r(X_{*})\right].\]

To complete the derivation for the \(F\) test of \(\Lambda^{\prime}\beta_{*}=0\), we need to show that \(\Lambda^{\prime}\hat{\beta}_{*}\) and \(Y^{\prime}(M_{1}-M_{*})Y\) are independent. It suffices to note that \[\begin{array}{rl}\text{Cov}(M_{*}Y,(M_{1}-M_{*})Y)&=\sigma^{2}M_{*}V(M_{1}-M_{*}) \\ &=\sigma^{2}\left[(1-\rho)+m\rho\right]M_{*}(M_{1}-M_{*})\\ &=0.\end{array}\]

The \(F\) test is based on the distributional result

\[\frac{\left(\Lambda^{\prime}\hat{\beta}_{*}\right)^{\prime}\left[\Lambda^{ \prime}(Z^{\prime}Z)^{-}\Lambda\right]^{-}\left(\Lambda^{\prime}\hat{\beta}_{ *}\right)/r(\Lambda)}{MSE(w)}\sim F(r(\Lambda),r(X_{1})-r(X_{*}),\pi)\;,\]

where

\[\pi=\left(\Lambda^{\prime}\beta_{*}\right)^{\prime}\left[\Lambda^{\prime}(Z^{ \prime}Z)^{-}\Lambda\right]^{-}\left(\Lambda^{\prime}\beta_{*}\right)/2\sigma ^{2}\left[(1-\rho)+m\rho\right]\]

and \(\left(\Lambda^{\prime}\beta_{*}\right)^{\prime}\left[\Lambda^{\prime}(Z^{ \prime}Z)^{-}\Lambda\right]^{-}\left(\Lambda^{\prime}\beta_{*}\right)=0\) if and only if \(\Lambda^{\prime}\beta_{*}=0\).

The argument establishing the independence of \(\Lambda^{\prime}\hat{\beta}_{*}\) and \(MSE(w)\) can be extended to establish the independence of all the distinct statistics being used.

**Theorem 11.2.5**: \(M_{*}Y\)_, \(M_{2}Y\), \(SSE(w)\), and \(SSE(s)\) are mutually independent._

_Proof_  Since the joint distribution of \(Y\) is multivariate normal, it suffices to use equations (6) and (7) to establish that the covariance between any pair of \(M_{*}Y\), \(M_{2}Y\), \((M_{1}-M_{*})Y\), and \((I-M)Y\) is 0. \(\square\)

To summarize the results so far, if the linear model satisfies Conditions 11.2.1 and 11.2.3, then (a) least squares estimates are BLUEs, (b) inferences about estimable functions \(\lambda^{\prime}\gamma\) can be made in the usual way (i.e., just as if \(V=I\)) with the exception that the estimate of error is taken to be \(MSE(s)\), and (c) inferences about estimable functions of \(\Lambda^{\prime}\beta_{*}\) that put a constraint on \(C(X_{*})\) can be drawn in the usual way, except that \(MSE(w)\) is used as the estimate of error.

As mentioned, it is not clear what kind of estimable functions put a constraint on \(C(X_{*})\). Two ways of getting around this problem will be discussed. As mentioned above, one way is to place another condition on the model matrix \(Z\) of model (2), a condition that forces the estimable functions of \(\delta\) to put constraints on \(C(X_{*})\). A second approach, that requires no additional conditions, is to abandon the idea of testing estimable functions and to look at testing models.

_Inferences About \(\delta\)_

 One of the problems with generalized split plot models is in identifying the hypotheses that put constraints on \(C(X_{*})\). In general, such hypotheses can involve both the \(\delta\) and the \(\gamma\) parameters. For example, suppose that \(C(X_{*})\cap C(X_{2})\) contains a nonzero vector \(\xi\). Then, since \(M_{Z}\xi=\xi\in C(X_{*})\), the hypothesis \(\xi^{\prime}X_{*}\delta+\xi^{\prime}X_{2}\gamma=0\) puts a constraint on \(C(X_{*})\). However, \(\xi^{\prime}X_{*}\delta+\xi^{\prime}X_{2}\gamma\) involves both the \(\delta\) and \(\gamma\) parameters because, with \(\xi\in C(X_{*})\cap C(X_{2})\), neither \(\xi^{\prime}X_{*}\) nor \(\xi^{\prime}X_{2}\) is 0. The most common example of this phenomenon occurs when \(X_{*}\) and \(X_{2}\) are chosen so that \(C(X_{*})\) and \(C(X_{2})\) both contain a column of 1s (i.e., \(J_{n}\)). It follows that inferences about the grand mean, \(n^{-1}J_{n}^{\prime}Z\beta_{*}\), are made using \(MSE(w)\).

The condition stated below ensures that any estimable function of the \(\delta\)s puts a constraint on \(C(X_{*})\). This condition is typically satisfied when \(Z\) is the model matrix for a balanced multifactor ANOVA (with some of the interactions possibly deleted).

**Condition 11.2.6**: _For \(v\in C(X)\), if \(v\perp C(X_{2})\), then \(v\in C(X_{1})\). Suppose that \(\lambda^{\prime}\delta\) is an estimable function. Then \(\lambda^{\prime}=\xi^{\prime}X_{*}\) and \(\xi^{\prime}X_{2}=0\) for some \(\xi\in C(Z)\). Since \(\xi^{\prime}X_{2}=0\), Condition 11.2.7 implies that \(\xi\in C(X_{1})\); thus \(M_{2}\xi=0\). Finally, by (4),_

\[M_{Z}\xi=M_{*}\xi\in C(X_{*}).\]

_Thus, estimable functions of \(\delta\) put a constraint on \(C(X_{*})\) and inferences about such functions are made using \(M_{*}Y\) and \(MSE(w)\)._

#### Testing Models

We will now examine the problem of testing model (2) against reduced models. To look at the complete problem, we will discuss both reduced models that put a constraint on \(C(X_{*})\) and reduced models that put a constraint on \(C(X_{2})\). For both kinds of reduced models, the tests are analogous to those developed in Section 3.2. A reduced model that puts a constraint on \(C(X_{*})\) can be tested by comparing the \(SSE(w)\) for model (2) with the \(SSE(w)\) for the reduced model. The difference in \(SSE(w)\)s is divided by the difference in the ranks of the design matrices to give a numerator mean square for the test. The denominator mean square is \(MSE(w)\) from model (2). The test for a reduced model that puts a constraint on \(C(X_{2})\) is performed in a similar fashion using \(SSE(s)\) and \(MSE(s)\). In the discussion below, specific models and notation are presented to justify these claims.

First, consider a reduced model that puts a constraint on \(C(X_{*})\). The reduced model is a model of the form

\[Y=Z_{0}\xi+e,\quad Z_{0}=[X_{0*},X_{2}]\,,\quad C(X_{0*})\subset C(X_{*}). \tag{8}\]

Let \(M_{0}\equiv Z_{0}(Z_{0}^{\prime}Z_{0})^{-}Z_{0}^{\prime}\) and \(M_{0*}\equiv X_{0*}(X_{0*}^{\prime}X_{0*})^{-}X_{0*}^{\prime}\). If the equivalent of Condition 11.2.3 holds for model (8), then

\[\frac{Y^{\prime}(M_{*}-M_{0*})Y/[r(X_{*})-r(X_{0*})]}{MSE(w)}\sim F\left(r(X_{* })-r(X_{0*}),r(X_{1})-r(X_{*}),\pi\right),\]

where

\[\pi=\beta_{*}^{\prime}Z^{\prime}(M_{*}-M_{0*})Z\beta_{*}\big{/}2\sigma^{2}[(1- \rho)+m\rho]\]

and \(\beta_{*}^{\prime}Z^{\prime}(M_{*}-M_{0*})Z\beta_{*}=0\) if and only if \(\mathrm{E}(Y)\in C(Z_{0})\).

These results follow from Theorems 11.2.5 and 11.2.6 and Corollary 3.8.3 upon noticing two things: First, in Corollary 3.8.3, \(A-A_{0}=M_{Z}-M_{0}=M_{*}-M_{0*}\). Second, from equation (6), \(M_{*}=[(1-\rho)+m\rho]M_{*}V^{-1}\), with a similar result holding for \(M_{0*}V^{-1}\).

The other kind of reduced model that can be treated conveniently is a reduced model that puts a constraint on \(C(X_{2})\). The reduced model is written as

\[Y=Z_{0}\xi+e,\quad Z_{0}=[X_{*},X_{3}]\,,\quad C(X_{3})\subset C(X_{2}). \tag{9}\]

If the equivalent of Condition 11.2.3 holds for model (9), write \(M_{0}\) as before and \(M_{3}\equiv(I-M_{1})X_{3}\left[X_{3}^{\prime}(I-M_{1})X_{3}\right]^{-}X_{3}^{ \prime}(I-M_{1})\). Then

\[\frac{Y^{\prime}(M_{2}-M_{3})Y/[r(M_{2})-r(M_{3})]}{MSE(s)}\sim F\left(r(M_{2} )-r(M_{3}),n-r(X),\pi\right),\]

where

\[\pi=\beta_{*}^{\prime}Z^{\prime}(M_{2}-M_{3})Z\beta_{*}/2\sigma^{2}(1-\rho)\]

and \(\beta_{*}^{\prime}Z^{\prime}(M_{2}-M_{3})Z\beta_{*}=0\) if and only if \(\mathrm{E}(Y)\in C(Z_{0})\).

These results follow from Theorem 11.2.6, Corollary 3.8.3, and relation (5) upon noticing that \(A-A_{0}=M_{Z}-M_{0}=M_{2}-M_{3}\); and, since \(M_{2}V=(1-\rho)M_{2}\), we have \((1-\rho)^{-1}M_{2}=M_{2}V^{-1}\), and a similar result for \(M_{3}\).

##### Identifying Generalized Split Plot Models

There are only two conditions necessary for having a generalized split plot model, Condition 11.2.1 and Condition 11.2.3. The form of generalized split plot models can be read from these conditions. Condition 11.2.1 requires that an equal number of observations be obtained within each whole plot. Condition 11.2.3 requires \(C(Z)=C(X_{*},(I-M_{1})X_{2})\), where \(C(X_{*})\subset C(X_{1})\). Since \(C(X_{1})\) is the column space that allows a separate effect for each cluster, \(X_{*}\delta\) can be anything that treats all of the observations in a given whole plot the same. The matrix \(X_{2}\) can contain the columns for any ANOVA effects that are balanced within whole plots. \(X_{2}\) can also contain any columns that are orthogonal to \(C(X_{1})\). Model (3) in Example 11.2.4a displays these characteristics.

##### Computations

The simplest way to actually fit generalized split plot models would seem to be to fit both models (2) and (11.1.2) using an ordinary least squares computer program. Fitting model (2) provides least squares estimates of \(\delta\) and \(\gamma\). Fitting model (2) also provides \(Y^{\prime}(I-M_{Z})Y\) as the reported _SSE_. This reported _SSE_ is not appropriate for any inferences, but, as seen below, it can be used to obtain the whole-plot sum of squares error. Fitting model (11.1.2) provides least squares estimates of \(\alpha\) and \(\gamma\) and the reported _SSE_ is \(Y^{\prime}(I-M)Y\). If the model is a generalized split plot model, the two estimates of \(\gamma\) should be identical. Since \(Y(I-M)Y\) is the _SSE_(_s_) (sum of squares for subplot error), any conclusions about \(\gamma\) obtained from fitting (11.1.2) will be appropriate. To obtain _SSE_(_w_) (sum of squares for whole-plot error), note that

\[Y^{\prime}(I-M_{Z})Y-Y^{\prime}(I-M)Y = Y^{\prime}(M-M_{Z})Y\] \[= Y^{\prime}(M_{1}+M_{2}-M_{*}-M_{2})Y\] \[= Y^{\prime}(M_{1}-M_{*})Y.\]

Thus, all of the computationally intensive work can be performed on standard computer programs.

**Exercise 11.8**: Give detailed proofs of the test statistic's distribution for

(a) testing model (8) against model (2),

(b) testing model (9) against model (2).

**Exercise 11.9**: Show that model (11.1.5) for a randomized complete block design is a generalized split plot model.

#### Unbalanced Subplots

Generalized split plot models can have any structure whatsoever in the whole plots but the subplot model needs to display certain orthogonality with the whole plots and the subplots have to be balanced, i.e., the whole plots must contain the same number of observations \(m\). If the whole plots are not balanced, a generalized split plot model does not exist. However, if you are willing to abandon the whole-plot analysis, the results of Section 1 provide a subplot analysis. And after all, the whole-plot analysis is rarely as interesting as the subplot analysis. Christensen (2015, Subsection 19.2.1) illustrates the methodology (although on data that are actually balanced).

### The Split Plot Design

The traditional model for a split plot design is a special case of the model presented in Section 2. We will present the split plot model and a model equivalent to model (11.1.2). We will use the balance of the split plot design to argue that Conditions 11.2.1, 11.2.3, and 11.2.7 hold. The arguments based on the balance of the split plot model are similar to those presented in Subsection 7.6.1. Statistical inferences are based on least squares estimates and quadratic forms in corresponding perpendicular projection matrices. The balance of the split plot model dictates results that are very similar to those described in Sections 7.1, 7.2, and 7.6.

The traditional split plot model involves a randomized complete block design in the whole plots. In fact, a completely randomized design or a Latin square in the whole plots leads to an analogous analysis. Suppose that there are \(r\) blocks of \(t\) whole plots available. Within each block, a different (whole plot) treatment is applied to each whole plot. Let \(\mu\) denote a grand mean, \(\xi\)s denote block effects, and \(\omega\)s denote whole-plot treatment effects. Let each whole plot be divided into \(m\) subplots with a different (subplot) treatment applied to each subplot. The \(\tau\)s denote subplot treatment effects, and the (\(\omega\tau\))s denote interaction effects between the whole-plot treatments and the subplot treatments. The split plot model has two sources of error, whole plot to whole plot variation denoted by \(\eta\), and subplot to subplot variation denoted by \(e\). The split plot model is

\[y_{ijk}=\mu+\xi_{i}+\omega_{j}+\eta_{ij}+\tau_{k}+(\omega\tau)_{jk}+e_{ijk}, \tag{1}\]

\(i=1,\ldots,r,\ j=1,\ldots,t\), \(k=1,\ldots,m\), \(\eta_{ij}\)s independent \(N(0,\sigma_{w}^{2})\), \(e_{ijk}\)s independent \(N(0,\sigma_{s}^{2})\). The \(\eta_{ij}\)s and \(e_{ijk}\)s are assumed to be independent. We can combine the error terms as \(\varepsilon_{ijk}=\eta_{ij}+e_{ijk}\). Writing the vector of errors as \(\varepsilon=[\varepsilon_{111},\varepsilon_{112},\ldots,\varepsilon_{rtm}]^{\prime}\), we get

\[\text{Cov}(\varepsilon) = \text{Blk diag}[\sigma_{s}^{2}I_{m}+\sigma_{w}^{2}J_{m}^{m}]\] \[= \sigma^{2}[(1-\rho)I+m\rho M_{1}],\]

where \(\sigma^{2}=\sigma_{w}^{2}+\sigma_{s}^{2}\) and \(\rho=\sigma_{w}^{2}/(\sigma_{w}^{2}+\sigma_{s}^{2})\). In a split plot model, the whole plots are considered as the different combinations of \(i\) and \(j\). There are \(m\) observations for each whole plot, so Condition 11.2.1 holds.

The model that includes separate effects \(\alpha_{ij}\) for each whole plot can be written as

\[y_{ijk}=\alpha_{ij}+\eta_{ij}+\tau_{k}+(\omega\tau)_{jk}+e_{ijk}\,.\]

Combining the error terms gives

\[y_{ijk}=\alpha_{ij}+\tau_{k}+(\omega\tau)_{jk}+\varepsilon_{ijk}\,,\]

or, using a parameterization with interactions,

\[y_{ijk}=\mu+\xi_{i}+\omega_{j}+(\xi\omega)_{ij}+\tau_{k}+(\omega\tau)_{jk}+ \varepsilon_{ijk}\,. \tag{2}\]

From Section 2, \(C(X_{1})\) is the space spanned by the columns associated with the \(\alpha_{ij}\)s and \(C(X_{2})\) is the space spanned by the columns associated with the \(\tau_{k}\)s and \((\omega\tau)_{jk}\)s. \(C(X)\) is the column space for model (2). Using the parameterization of model (2) and the notation and results of Subsection 7.6.1 gives

\[C(X_{1}) = C(M_{\mu}+M_{\xi}+M_{\omega}+M_{\xi\omega}),\] \[C(X_{2}) = C(M_{\mu}+M_{\omega}+M_{\tau}+M_{\omega\tau}),\] \[C(X) = C(M_{\mu}+M_{\xi}+M_{\omega}+M_{\xi\omega}+M_{\tau}+M_{\omega \tau}).\]Recall that all of the \(M\) matrices on the right-hand sides are perpendicular projection matrices, and that all are mutually orthogonal. In particular, \(M=M_{\mu}+M_{\xi}+M_{\omega}+M_{\xi\omega}+M_{\xi\omega}+M_{\tau}+M_{\omega\tau}\) and \(M_{1}=M_{\mu}+M_{\xi}+M_{\omega}+M_{\xi\omega}\).

The split plot model (1) is a reduced model relative to model (2). The \(\xi\omega\) interactions are dropped to create the split plot model. \(C(X_{*})\) is the space spanned by the columns associated with \(\mu\), the \(\xi_{i}\)s, and the \(\omega_{j}\)s. Again using results from 7.6.1,

\[C(X_{*}) = C(M_{\mu}+M_{\xi}+M_{\omega}),\] \[C[(I-M_{1})X_{2}] = C(M_{\tau}+M_{\omega\tau}),\] \[C(Z) = C(M_{\mu}+M_{\xi}+M_{\omega}+M_{\tau}+M_{\omega\tau}).\]

Clearly, Condition 11.2.3 applies.

In fact, even Condition 11.2.7 holds, so that estimable functions of the \(\xi\)s and \(\omega\)s are tested using \(MSE(w)\). To check Condition 11.2.7, it suffices to show that if \(v\in C(X)\) and \(v\perp C(X_{2})\), then \(v\in C(X_{1})\). If \(v\in C(X)\), then \(Mv=v\). If \(v\perp C(X_{2})\), then \((M_{\mu}+M_{\omega}+M_{\tau}+M_{\omega\tau})v=0\). Thus

\[v=Mv = (M_{\mu}+M_{\xi}+M_{\omega}+M_{\xi\omega}+M_{\tau}+M_{\omega\tau})v\] \[= (M_{\xi}+M_{\xi\omega})v.\]

But, \(v=(M_{\xi}+M_{\xi\omega})v\in C(X_{1})\). It should be noted that with \((\omega\tau)\) interaction in the model, contrasts in the \(\omega\)s and \(\tau\)s are not estimable. For example, the usual procedure gives estimates of contrasts in the \(\omega_{j}+\overline{(\omega\tau)}_{j}\).s. Without \((\omega\tau)\) interaction, contrasts in the \(\omega\)s become estimable. In either case, the estimates are obtained using \(M_{*}\).

We can now write out an ANOVA table.

\begin{tabular}{l l l l} Source & \(df\) & \(SS\) & E(\(MS\)) \\ \hline \(\mu\) & \(r(M_{\mu})\) & \(Y^{\prime}M_{\mu}Y\) & \((\sigma_{s}^{2}+m\sigma_{w}^{2})+\beta_{*}^{\prime}Z^{\prime}M_{\mu}Z\beta_{*} /r(M_{\mu})\) \\ \(\xi\) & \(r(M_{\xi})\) & \(Y^{\prime}M_{\xi}Y\) & \((\sigma_{s}^{2}+m\sigma_{w}^{2})+\beta_{*}^{\prime}Z^{\prime}M_{\xi}Z\beta_{*} /r(M_{\xi})\) \\ \(\omega\) & \(r(M_{\omega})\) & \(Y^{\prime}M_{\omega}Y\) & \((\sigma_{s}^{2}+m\sigma_{w}^{2})+\beta_{*}^{\prime}Z^{\prime}M_{\omega}Z\beta_{*} /r(M_{\omega})\) \\ error 1 & \(r(M_{\xi\omega})\) & \(Y^{\prime}M_{\xi\omega}Y\) & \(\sigma_{s}^{2}+m\sigma_{w}^{2}\) \\ \(\tau\) & \(r(M_{\tau})\) & \(Y^{\prime}M_{\tau}Y\) & \(\sigma_{s}^{2}+\beta_{*}^{\prime}Z^{\prime}M_{\tau}Z\beta_{*}/r(M_{\tau})\) \\ \(\omega\tau\) & \(r(M_{\omega\tau})\) & \(Y^{\prime}M_{\omega\tau}Y\) & \(\sigma_{s}^{2}+\beta_{*}^{\prime}Z^{\prime}M_{\omega\tau}Z\beta_{*}/r(M_{ \omega\tau})\) \\ error 2 & \(r(I-M)\) & \(Y^{\prime}(I-M)Y\) & \(\sigma_{s}^{2}\) \\ \hline Total & \(n\) & \(Y^{\prime}Y\) \\ \end{tabular} Note that \(\sigma^{2}[(1-\rho)+m\rho]=\sigma_{s}^{2}+m\sigma_{w}^{2}\) and \(\sigma^{2}(1-\rho)=\sigma_{s}^{2}\). Algebraically, we can write the table as \[A = tm\sum_{i}(\xi_{i}-\bar{\xi}.)^{2}/(r-1),\] \[B = rm\sum_{j}[\omega_{j}+\overline{(\omega\tau)}_{j}.-\tilde{\omega}. -\overline{(\omega\tau)}_{..}]^{2}/(t-1),\] \[C = rt\sum_{k}[\tau_{k}+\overline{(\omega\tau)}_{..k}-\bar{\tau}.- \overline{(\omega\tau)}_{..}]^{2}/(m-1),\] \[D = r\sum_{jk}[(\omega\tau)_{jk}-\overline{(\omega\tau)}_{j}.- \overline{(\omega\tau)}_{..k}+\overline{(\omega\tau)}_{..}]^{2}/(t-1)(m-1).\]

Tests and confidence intervals for contrasts in the \(\tau_{k}\)s and \((\omega\tau)_{jk}\)s are based on the usual least squares estimates and use the mean square from the "error 2" line for an estimate of variance. Tests and confidence intervals in the \(\xi_{i}\)s and \(\omega_{j}\)s also use least squares estimates, but use the mean square from the "error 1" line for an estimate of variance. Note that, even though there is no interaction in the whole-plot analysis, contrasts in the \(\omega_{j}\)s are really contrasts in the \([\omega_{j}+\overline{(\omega\tau)}_{j}.]\)s when interaction is present.

Finally, a word about missing data. If one or more whole plots are missing, the data can still be analyzed as in Section 2. If one or more subplots are missing, the data can still be analyzed as in Section 1; however, with missing subplots, some sort of ad hoc analysis for the whole-plot effects must be used.

**Exercise 11.10**: Consider the table of meansLet \(\sum_{k=1}^{m}d_{k}=0\). For any fixed \(j\), find a confidence interval for \(\sum_{k=1}^{m}d_{k}\mu_{jk}\), where \(\mu_{jk}=\mu+\hat{\xi}.+\omega_{j}+\tau_{k}+(\omega\tau)_{jk}\). (Hint: The estimate of the variance comes from the "error 2" line.) Let \(\sum_{j=1}^{t}c_{j}=0\). Why is it not possible to find a confidence interval for \(\sum_{j=1}^{t}c_{j}\mu_{jk}\)?

### Identifying the Appropriate Error

Statistics is all about drawing conclusions from data that are subject to error. One of the crucial problems in statistics is identifying and estimating the appropriate error so that valid conclusions can be made. The importance of this issue has long been recognized by the statistics community. The necessity of having a valid estimate of error is one of the main points in _The Design of Experiments_, Fisher's (1935) seminal work.

The key feature of split plot models is that they involve two separate sources of variation. The analysis involves two separate estimates of error, and a correct analysis requires that the estimates be used appropriately. If the existence of two separate sources of variability is not noticed, the estimate of error will probably be obtained by pooling the sums of squares for the two separate errors. The pooled estimate of error will generally be too small for comparing treatments applied to whole plots and too large for comparing treatments applied to subplots. The whole-plot treatments will appear more significant than they really are. The subplot treatments will appear less significant than they really are. The interactions between whole-plot and subplot treatments will also appear less significant than they really are.

The problem of identifying the appropriate error is a difficult one. In this section, some additional examples of the problem are discussed. First, the problem of subsampling is considered. The section ends with a discussion of the appropriate error for testing main effects in the presence of interactions.

#### Subsampling

One of the most commonly used, misused, and abused of models is the subsampling model.

In an agricultural experiment, one treatment is applied to each of 6 pastures. The experiment involves 4 different treatments, so there are a total of 24 pastures in the experiment. On each pasture 10 observations are taken. The 10 observations taken on each pasture are referred to as subsamples. Each observation is subject to two kinds of variability: (1) pasture to pasture variability and (2) within pasture variability. Note, however, that in comparing the 10 observations taken on a given pasture, there is no pasture variability. The correct model for this experiment involves error terms for both kinds of variability. Typically the model is taken as

\[y_{ijk}=\mu+\omega_{i}+\eta_{ij}+e_{ijk}, \tag{1}\]

\(i=1,2,3,4\), \(j=1,\ldots,6,k=1,\ldots,10\), \(\eta_{ij}\)s i.i.d. \(N(0,\sigma_{w}^{2})\), \(e_{ijk}\)s i.i.d. \(N(0,\sigma_{s}^{2})\), and the \(\eta_{ij}\)s and \(e_{ijk}\)s are independent. In this model, \(\sigma_{w}^{2}\) is the pasture to pasture variance and \(\sigma_{s}^{2}\) is the within pasture or subsampling variance.

As will be seen below, the statistical analysis of model (1) acts like there is only 1 observation on each pasture. That 1 observation is the mean of the 10 actual observations that were taken. If the analysis acts like only 1 observation was taken on a pasture, why should an experimenter trouble to take 10 observations? Why not take just 1 observation on each pasture?

With 1 real observation on each pasture, the statistical analysis is subject to the whole weight of the within pasture variability. By taking 10 observations on a pasture and averaging them to perform the analysis, the mean of the 10 observations still has the full pasture to pasture variability, but the within pasture variability (variance) is cut by a factor of 10. The experiment is being improved by reducing the variability of the treatment estimates, but that improvement comes only in the reduction of the within pasture variability. The effects of pasture to pasture variability are not reduced by subsampling.

Rather than subsampling, it would be preferable to use more pastures. Using more pastures reduces the effects of both kinds of variability. Unfortunately, doing that is often not feasible. In the current example, the same reduction in within pasture variation without subsampling would require the use of 240 pastures instead of the 24 pastures that are used in the experiment with subsampling. In practice, obtaining 24 pastures for an experiment can be difficult. Obtaining 240 pastures can be well high impossible.

A general balanced subsampling model is

\[y_{ijk}=\mu+\omega_{i}+\eta_{ij}+e_{ijk}, \tag{2}\]

\[\text{E}(\eta_{ij})=\text{E}(e_{ijk})=0,\quad\text{Var}(\eta_{ij})=\sigma_{w} ^{2},\quad\text{Var}(e_{ijk})=\sigma_{s}^{2},\]

\(i=1,\ldots,t\), \(j=1,\ldots,r\), \(k=1,\ldots,m\) with distinct \(\eta_{ij}\)s and \(e_{ijk}\)s uncorrelated. By checking Conditions 11.2.1, 11.2.3, and 11.2.7, it is easily seen that this is a generalized split plot model with \(X_{2}\) vacuous. An ANOVA table can be written:where \(A=rm\sum_{i}(\omega_{i}-\bar{\omega}.)^{2}/(t-1)\). The entire analysis is performed as a standard one-way ANOVA. The variance of \(\bar{y}_{i\cdots}\) is \((\sigma_{s}^{2}+m\sigma_{w}^{2})/rm\), so the "error 1" line is used for all tests and confidence intervals.

As mentioned above, an equivalent analysis can be made with the averages of the observations in each subsample. The model based on the averages is

\[\bar{y}_{ij\cdots}=\mu+\omega_{i}+\xi_{ij},\;\;\;\;\mbox{E}(\xi_{ij})=0,\;\;\; \;\mbox{Var}(\xi_{ij})=[\sigma_{w}^{2}+(\sigma_{s}^{2}/m)],\]

\(i=1,\ldots,t\), \(j=1,\ldots,r\). It is easily seen that this one-way AVOVA gives exactly the same tests and confidence intervals as those obtained from model (2).

One of the most common mistakes in statistical practice is to mistake subsampling for independent replication in an experiment. Example 11.4.1 involves 6 independent replications, i.e., the 6 pastures to which each treatment is applied. The 10 observations on a given pasture are not independent because the random effect for pastures is the same for all of them. The _incorrect_ model that is often analyzed instead of model (2) is

\[y_{ij}=\mu+\omega_{i}+e_{ij}, \tag{3}\]

\(i=1,\ldots,t\), \(j=1,\ldots,rm\). The effect of analyzing model (3) is that the "error 1" and "error 2" lines of the ANOVA are pooled together. Since the expected mean square for "error 2" is only \(\sigma_{s}^{2}\), the pooled mean square error is inappropriately small and all effects will appear to be more significant than they really are.

Subsampling can be an important tool, especially when the variability between subsamples is large. However, it is important to remember that subsampling is to be used in addition to independent replication. It does not eliminate the need for an adequate number of independent replicates. In terms of Example 11.4.1, an experimenter should first decide on a reasonable number of pastures and then address the question of how many observations to take within a pasture. If the pasture to pasture variability is large compared to the within pasture variability, subsampling will be of very limited value. If the within pasture variability is large, subsampling can be extremely worthwhile.

The existence of subsampling can usually be determined by carefully identifying the treatments and the experimental units to which the treatments are applied. In the agricultural example, identifying the subsampling structure was easy. Treatments were applied to pastures. If differences between treatments are to be examined, then differences between pastures with the same treatment must be error.

Lest the reader think that identifying subsampling is easy, let us try to confuse the issue. The analysis that has been discussed is based on the assumption that pasture to pasture variability is error, but now suppose that the experimenter has an interest in the pastures. For example, different pastures have different fertilities, so some pastures are better than others. If differences in pastures are of interest, it may be reasonable to think of the \(\eta_{ij}\)s as fixed effects, in which case there is no subsampling and the ANOVA table gives only one error line. The expected mean squares are:The mean square for \(\eta\) can be used to test whether there are any differences between pastures that have the same treatment. The _MSE_(\(\omega\)) provides a test of whether the treatment effects added to their average pasture effects are different. The test from _MSE_(\(\omega\)) may not be very interesting if there are different pasture effects. In summary, the analysis depends crucially on whether the \(\eta_{ij}\)s are assumed to be random or fixed. When the \(\omega\) treatments are of primary importance it makes sense to treat the \(\eta\) effects as random.

#### Two-Way ANOVA with Interaction

The balanced two-way ANOVA with interaction model from Section 7.2 is

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+e_{ijk}, \tag{4}\]

\(i=1,\ldots,a,\)\(j=1,\ldots,b,\)\(k=1,\ldots,N\). First, the question of subsampling in a two-way ANOVA will be addressed. The discussion of subsampling leads to an examination of independent replication, and to the question of whether the interactions should be considered fixed or random. The discussion of identifying the appropriate error begins with an example:

We want to investigate the effects of 4 fertilizers and 6 herbicides on arable plots used for raising cane (as in sugar cane). There are a total of \(4\times 6=24\) treatment combinations. If each treatment combination is applied to 5 plots, then model (4) is appropriate with \(a=4\), \(b=6\), and \(N=5\).

Now consider an alternative experimental design that is easily confused with this. Suppose each treatment combination is applied to 1 plot and 5 observations are taken on each plot. There is no independent replication. The 5 observations on each plot are subsamples. Comparisons within plots do not include plot to plot variability. If model (4) is used to analyze such data, the _MSE_ is actually the estimated subsampling variance. It is based on comparisons within plots. An analysis based on model (4) will inflate the significance of all effects.

The appropriate model for this subsampling design is

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+\xi_{ij}+e_{ijk},\]where \(\xi_{ij}\sim N(0,\sigma_{w}^{2})\) and \(e_{ijk}\sim N(0,\sigma_{s}^{2})\). Note that the indices on \(\gamma\) and \(\xi\) are exactly the same. It is impossible to tell interactions apart from plot to plot variability. As a result, unless the interactions can be assumed nonexistent, there is no appropriate estimate of error available in this experiment.

In the example, two extreme cases were considered, one case with no subsampling and one case with no independent replication. Of course, any combination of subsampling and independent replication can be used. In the example, the designs were clearly stated so that the subsampling structures were clear. In practice, this rarely occurs. When presented with data that have been collected, it can be very difficult to identify how the experiment was designed. (I have claimed for many years that the hardest part of any statistical consulting job is to get the clients to tell you what they actually did--as opposed to what _they_ think is important.)

Designs without independent replications are actually quite common. When confronted with a two-factor ANOVA without any independent replication, the fixed interaction effects are generally assumed to be zero so that an analysis can be performed. This is precisely the assumption made in Chapter 8 in analyzing the Randomized Complete Block model. An alternate way of phrasing this idea is that any interaction effects that exist must be due to error. This idea that interaction effects can themselves be errors is important. If the interactions are errors, then model (4) needs to be changed. The standard assumption would be that the \(\gamma_{ij}\)s are independent \(N(0,\sigma_{w}^{2})\) random variables.

Note that the assumption of random \(\gamma_{ij}\)s does not imply the existence of subsampling. Subsampling is a property of the experimental design. What is being discussed is simply a choice about how to model interactions. Should they be modeled as fixed effects, or should they be modeled as random errors? One guideline is based on the repeatability of the results. If the pattern of interactions should be the same in other similar studies, then the interactions are fixed. If there is little reason to believe that the pattern of interactions would be the same in other studies, then the interactions would seem to be random.

The analysis of model (4) with random interactions is straightforward. The model is a generalized split plot model with \(X_{2}\) vacuous. The mean square for interactions becomes the mean square for "error 1." The mean square error from assuming fixed interactions becomes the mean square for "error 2." The analysis for main effects uses the mean square "error 1" exclusively as the estimate of error.

Although this is not a subsampling model, it does involve two sources of variation: (1) variation due to interactions, and (2) variation due to independent replication (i.e., variation from experimental unit to experimental unit). It seems to be difficult to reduce the effects on comparisons among treatments of the variability due to interactions. The effect of variation due to experimental units can be reduced by taking additional independent replications.

### Exercise: An Unusual Split Plot Analysis

Cornell (1988) considered data on the production of vinyl for automobile seat covers. Different blends involve various plasticizers, stabilizers, lubricants, pigments, fillers, drying agents, and resins. The current data involve 5 blends of vinyl.

The 5 blends represent various percentages of 3 plasticizers that together make up 40.7% of the product. The first plasticizer is restricted to be between 19.1% and 34.6% of the product. The second plasticizer is restricted to be between 0% and 10.2% of the product. The third plasticizer is restricted to be between 6.1% and 11.4% of the product. Changing these restrictions to fractions of the 40.7% of the total, we get

\[0.47\leq x_{1}\leq 0.85,\quad 0\leq x_{2}\leq 0.25,\quad 0.15\leq x_{3}\leq 0.28.\]

The 5 blends are

\[\begin{array}{ccc}\text{Blend}&(x_{1},x_{2},x_{3})\\ \hline 1&(0.85,\ 0.000,\ 0.150)\\ 2&(0.72,\ 0.000,\ 0.280)\\ 3&(0.60,\ 0.250,\ 0.150)\\ 4&(0.47,\ 0.250,\ 0.280)\\ 5&(0.66,\ 0.125,\ 0.215)\\ \end{array}\]

Note that the first four blends have all combinations of \(x_{2}\) and \(x_{3}\) at their extremes with \(x_{1}\) values decreasing at about 0.13 per blend. Blend 5 is in the center of the other blends. In particular, for \(i=1\), 2, 3, the \(x_{i}\) value of blend 5 is the mean of the other four \(x_{i}\) values. Eight groups of the five different blends were prepared.

The first group of 5 blends was run with the production process set for a high rate of extrusion (\(z_{1}=1\)) and a low drying temperature (\(z_{2}=-1\)). The process was then reset for low extrusion rate and high drying temperature (\(z_{1}=-1,z_{2}=1\)), and another group of 5 blends was run. For subsequent runs of 5 blends, the process was set for \(z_{1}=-1\), \(z_{2}=-1\), and \(z_{1}=1\), \(z_{2}=1\) to finish the first replication. Later, the second replication was run in the order \(z_{1}=-1\), \(z_{2}=1\); \(z_{1}=1,z_{2}=1\); \(z_{1}=1,z_{2}=-1\); \(z_{1}=-1\), \(z_{2}=-1\). The data are presented in Table 11.1

[MISSING_PAGE_FAIL:354]

* Christensen (1987) Christensen, R. (1987). The analysis of two-stage sampling data by ordinary least squares. _Journal of the American Statistical Association_, _82_, 492-498.
* Cornell (1988) Cornell, J. A. (1988). Analyzing mixture experiments containing process variables. A split plot approach. _Journal of Quality Technology_, _20_, 2-23.
* Fisher (1935) Fisher, R. A. (1935). _The design of experiments_, (9th ed., 1971). New York: Hafner Press.
* Mathew & Sinha (1992) Mathew, T., & Sinha, B. K. (1992). Exact and optimum tests in unbalanced split-plot designs under mixed and random models. _Journal of the American Statistical Association_, _87_, 192-200.
* Monlezun & Blouin (1988) Monlezun, C. J., & Blouin, D. C. (1988). A general nested split-plot analysis of covariance. _Journal of the American Statistical Association_, _83_, 818-823.
* Skinner et al. (1989) Skinner, C. J., Holt, D., & Smith, T. M. F. (1989). _Analysis of complex surveys_. New York: Wiley.

## Chapter 12 Model Diagnostics

This chapter focuses on methods for evaluating the assumptions made in a standard linear model and on the use of transformations to correct such problems.

This book deals with linear model theory and as such we have largely assumed that the data are good and that the models are true. Unfortunately, good data are rare and true models are even rarer. (George Box would have us believe that true models are nonexistent.) Chapters 12-14 discuss some additional tools used by statisticians to deal with the problems presented by real data.

All models are based on assumptions. We typically assume that \(\operatorname{E}(Y)\) has a linear structure, that the observations are independent, that the variance is the same for each observation, and that the observations are normally distributed. In truth, these assumptions will probably never be correct. It is our hope that if we check the assumptions and if the assumptions look plausible, then the mathematical methods presented here will work quite well.

If the assumptions are checked and found to be implausible, we need to have alternate ways of analyzing the data. In Section 2.7, Section 3.8, Chapters 10, and 11, we discussed the analysis of linear models with general covariance matrices. If an approximate covariance matrix can be found, the methods presented earlier can be used. (See also _ALM-III_ and in particular the discussion of the deleterious effects of estimating covariance matrices in Chapter 4 (Christensen 2001, Section 6.5).) Another approach is to find a transformation of the data so that the assumptions seem plausible for a standard linear model in the transformed data.

The primary purpose of this chapter is to present methods of identifying when there may be trouble with the assumptions. Analysis of the residuals is the method most often used for detecting invalidity of the assumptions. Residuals are used to check for nonnormality of errors, nonindependence, lack of fit, heteroscedasticity (inequality) of variances, and outliers (unusual data). They also help identify influential observations.

The vector of residuals is, essentially, an estimate of \(e\) in the model

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I.\]

The residual vector is

\[\hat{e}\equiv Y-X\hat{\beta}=(I-M)Y,\]

with

\[\mathrm{E}(\hat{e})=(I-M)X\beta=0\]

and

\[\mathrm{Cov}(\hat{e})=(I-M)\sigma^{2}I(I-M)^{\prime}=\sigma^{2}(I-M).\]

For many of the techniques that we will discuss, the residuals are standardized so that their variances are about 1. The _standardized residuals_ are

\[r_{i}\equiv\hat{e}_{i}\big{/}\sqrt{MSE(1-m_{ii})},\]

where \(m_{ii}\) is the \(i\)th diagonal element of \(M\) and \(\hat{e}=[\hat{e}_{1},\ldots,\hat{e}_{n}]^{\prime}\).

When checking for nonnormality or heteroscedastic variances, it is important to use the standardized residuals rather than unstandardized residuals. As just seen, the ordinary residuals have heteroscedastic variances. Before they are useful in checking for equality of the variances of the observations, they need to be standardized. Moreover, methods for detecting nonnormality are often sensitive to inequality of variances, so the use of ordinary residuals can make it appear that the errors are not normal even when they are.

Once computer programs used \(\hat{e}/\sqrt{MSE}\) as standardized residuals, but that seems to be a thing of the past. That method was inferior to the standardization given above because it ignores the fact that the variances of the residuals are not all equal. The standardized residuals are also sometimes called the _Studentized residuals_, but that term is also sometimes used for another quantity discussed in Section 6.

Influential observations have been mentioned. What are they? One idea is that an observation is influential if it greatly affects the fitted regression equation. Influential observations are not intrinsically good or bad, but they are always important. Typically, influential observations are outliers: data points that are, in some sense, far from the other data points being analyzed. This can happen in two ways. First, the \(y\) value associated with a particular row of the \(X\) matrix can be unlike what would be expected from examining the rest of the data. Second, a particular row of the \(X\) matrix can be unlike any of the other rows of the \(X\) matrix.

A frequently used method for analyzing residuals is to plot the (standardized) residuals against various other variables. We now consider an example that will be used throughout this chapter to illustrate the use of residual plots. In the example, we consider a model that will later be perturbed in various ways. This model and its perturbations will provide residuals that can be plotted to show characteristics of residual plots and the effects of unsatisfied assumptions.

_Example 12.0.1_ Draper and Smith (1998) presented an example with 25 observations on a dependent variable, pounds of steam used by a company per month, and two predictor variables: \(x_{1}\), the average atmospheric temperature for the month (in \({}^{\circ}\)F); and \(x_{2}\), the number of operating days in the month. The values of \(x_{1}\) and \(x_{2}\) are listed in Table 1.1.

Draper and Smith's fitted equation is

\[y=9.1266-0.0724x_{1}+0.2029x_{2}.\]

Our examples will frequently be set up so that the _true_ model is

\[y_{i}=9.1266-0.0724x_{i1}+0.2029x_{i2}+e_{i}. \tag{1}\]

The vector \(Y=(y_{1},\ldots,y_{25})^{\prime}\) can be obtained by generating the \(e_{i}\)s and adding the terms on the right-hand side of the equation. Once \(Y\) is obtained, the equation \(y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+e_{i}\) can be fitted by least squares and the residuals computed. By generating errors that have independent identical normal distributions, nonnormal distributions, serial correlations, or unequal variances, we can examine how residual plots should look when these conditions exist. By fitting models with incorrect mean structure, we can examine how residual plots for detecting lack of fit should look.

The material in this chapter is presented with applications to regression models in mind, but can be applied to ANOVA models with little modification. Excellent discussions of residuals and influential observations are found in Cook and Weisberg (1982) [downloadable from [http://www.stat.umn.edu/rir](http://www.stat.umn.edu/rir)], Atkinson (1985), and elsewhere. Cook and Weisberg (1994, 1999) and Cook (1998) give extensive discussions of regression graphics.

**Exercise 12.1** Show that for a standard linear model \(\hat{e}\) is the BLUP of \(e\).

\begin{table}
\begin{tabular}{l c c|c c c} \hline Obs. & & & Obs. & & \\ no. & \(x_{1}\) & \(x_{2}\) & no. & \(x_{1}\) & \(x_{2}\) \\ \hline
1 & 35.3 & 20 & 14 & 39.1 & 19 \\
2 & 29.7 & 20 & 15 & 46.8 & 23 \\
3 & 30.8 & 23 & 16 & 48.5 & 20 \\
4 & 58.8 & 20 & 17 & 59.3 & 22 \\
5 & 61.4 & 21 & 18 & 70.0 & 22 \\
6 & 71.3 & 22 & 19 & 70.0 & 11 \\
7 & 74.4 & 11 & 20 & 74.5 & 23 \\
8 & 76.7 & 23 & 21 & 72.1 & 20 \\
9 & 70.7 & 21 & 22 & 58.1 & 21 \\
10 & 57.5 & 20 & 23 & 44.6 & 20 \\
11 & 46.4 & 20 & 24 & 33.4 & 20 \\
12 & 28.9 & 21 & 25 & 28.6 & 22 \\
13 & 28.1 & 21 & & & \\ \hline \end{tabular}
\end{table}
Table 1.1: Steam data 

### Leverage

A data point (case) that corresponds to a row of the \(X\) matrix that is "unlike" the other rows is said to have high _leverage_. In this section we will define the Mahalanobis distance and use it as a basis for identifying rows of \(X\) that are unusual. It will be shown that for regression models that include an intercept (a column of 1s), the diagonal elements of the ppo \(M\) and the Mahalanobis distances are equivalent measures. In particular, a diagonal element of the ppo is an increasing function of the corresponding Mahalanobis distance. (There are some minor technical difficulties with this claim when \(X\) is not a full rank matrix.) This equivalence justifies the use of the diagonal elements to measure the abnormality of a row of the model matrix. _The diagonal elements of the ppo are the standard tool used for measuring leverage_.

In addition to their interpretation as a measure of abnormality, it will be shown that the diagonal elements of the projection matrix can have direct implications on the fit of a regression model. A diagonal element of the projection matrix that happens to be near 1 (the maximum possible) will force the estimated regression equation to go very near the corresponding \(y\) value. Thus, cases with extremely large diagonal elements have considerable influence on the estimated regression equation. It will be shown through examples that diagonal elements that are large, but not near 1, can also have substantial influence.

High leverage points are not necessarily bad. If a case with high leverage is consistent with the remainder of the data, then the case with high leverage causes no problems. In fact, the case with high leverage can greatly reduce the variability of the least squares fit. In other words, with an essentially correct model and good data, high leverage points actually help the analysis.

On the other hand, _high leverage points are dangerous_. The regression model that one chooses is rarely the true model. Usually it is only an approximation to the truth. High leverage points can change a good approximate model into a bad approximate model. An approximate model is likely to work well only on data that are limited to some particular range of values. It is unreasonable to expect to find a model that works well in places where very little data were collected. By definition, high leverage points exist where very little data were collected, so one would not expect them to be modeled well. Ironically, just the opposite result usually occurs. The high leverage points are often fit very well, while the fit of the other data is often harmed. The model for the bulk of the data is easily distorted to accommodate the high leverage points. When high leverage points are identified, the researcher is often left to decide between a bad model for the entire data or a good model for a more limited problem.

The purpose of this discussion of cases with high leverage is to make one point. If some data were collected in unusual places, then the appropriate goal may be to find a good approximate model for the area in which the bulk of the data were collected. This is not to say that high leverage points should always be thrown out of a data set. High leverage points need to be handled with care, and the implications of excluding high leverage points from a particular data set need to be thoroughly examined. High leverage points can be the most important cases in the entire data.

We begin by defining the Mahalanobis distance and establishing its equivalence to the diagonal elements of the projection operator. This will be followed by an examination of diagonal elements that are near 1. The section closes with a series of examples.

##### Mahalanobis Distances

The _Mahalanobis distance_ measures how far a random vector is from the middle of its distribution. For this purpose, we will think of the rows of the matrix \(X\) as a sample of vectors from some population. Although this contradicts our assumption that the matrix \(X\) is fixed and known, our only purpose is to arrive at a reasonable summary measure of the distance of each row from the other rows. The Mahalanobis distance provides such a measure. The notation and ideas involved in estimating Mahalanobis distances are similar to those used in estimating best linear predictors. Estimation of best linear predictors was discussed in Subsection 6.3.4. In particular, we write the \(i\)th row of \(X\) as \((1,x_{i}^{\prime})\) so that the corresponding linear model contains an intercept.

Let \(x\) be a random vector.

**Definition 12.1.1** Let \(\operatorname{E}(x)=\mu\) and \(\operatorname{Cov}(x)=V\). The _squared Mahalanobis distance_ is

\[D^{2}\equiv(x-\mu)^{\prime}V^{-1}(x-\mu).\]

For a sample \(x_{1}\),..., \(x_{n}\), the relative distances of the observations from the center of the distribution can be measured by the squared distances

\[D_{i}^{2}=(x_{i}-\mu)^{\prime}V^{-1}(x_{i}-\mu),\quad i=1,\ldots,n.\]

Usually, \(\mu\) and \(V\) are not available, so they must be estimated. Similar to Subsubsection 6.3.4.1, write

\[Z=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix},\]

estimate \(\mu\) with \(\bar{x}^{\prime}=(1/n)J_{1}^{n}Z\), and estimate \(V\) with

\[S=\frac{1}{n-1}\left[\sum_{i=1}^{n}(x_{i}-\bar{x}.)(x_{i}-\bar{x}.)^{\prime} \right]=\frac{1}{n-1}Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Z.\]

**Definition 12.1.2** The _estimated squared Mahalanobis distance_ for the \(i\)th case in a sample of vectors \(x_{1}\),..., \(x_{n}\) is \[\hat{D}_{i}^{2}\equiv(x_{i}-\bar{x}.)^{\prime}S^{-1}(x_{i}-\bar{x}.).\]

Note that the values \(\hat{D}_{i}^{2}\) are precisely the diagonal elements of

\[(n-1)\left(I-\frac{1}{n}J_{n}^{n}\right)Z\left[Z^{\prime}\left(I-\frac{1}{n}J_{ n}^{n}\right)Z\right]^{-1}Z^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right).\]

Our interest in these definitions is that for a regression model \(Y=X\beta+e\), the distance of the \(i\)th row of \(X\) from the other rows can be measured by the estimated squared Mahalanobis distance \(\hat{D}_{i}^{2}\). In this context we think of the rows of \(X\) as a sample from some population. As mentioned earlier, when the model has an intercept, the diagonal elements of \(M\) and the estimated squared Mahalanobis distances are equivalent measures. When an intercept is included in the model, \(X\) can be written as \(X=[J,Z]\). Since the rows of \(J\) are identical, the matrix \(S\), defined for the entire matrix \(X\), is singular. Thus, \(S^{-1}\) does not exist and the estimated Mahalanobis distances are not defined. Instead, we measure the relative distances of the rows of \(Z\) from their center.

Theorem 1.3: _Consider the linear model \(Y=X\beta+e\), where \(X\) is a full rank matrix and \(X=[J,Z]\). Then,_

\[m_{ii}=\frac{1}{n}+\frac{\hat{D}^{2}}{n-1}.\]

_Proof_ The theorem follows immediately from the fact that

\[M=\frac{1}{n}J_{n}^{n}+\left(I-\frac{1}{n}J_{n}^{n}\right)Z\left[Z^{\prime} \left(I-\frac{1}{n}J_{n}^{n}\right)Z\right]^{-1}Z^{\prime}\left(I-\frac{1}{n} J_{n}^{n}\right)\]

(cf. Sections 6.2 and 9.1). The inverse in the second term of the right-hand side exists because \(X\), and therefore \(\left(I-\frac{1}{n}J_{n}^{n}\right)Z\), are full rank matrices. \(\square\)

From this theorem, it is clear that the rows with the largest squared Mahalanobis distances are precisely the rows with the largest diagonal elements of the perpendicular projection matrix. For identifying high leverage cases in a regression model with an intercept, using the diagonal elements of \(M\) is equivalent to using the squared Mahalanobis distances.

Of course, for regression models that do not include an intercept, using the squared Mahalanobis distances is not equivalent to using the diagonal elements of the projection matrix. For such models it would probably be wise to examine both measures of leverage. The diagonal elements of the projection matrix are either given by or easily obtained from many computer programs. The information in the squared Mahalanobis distances can be obtained by the artifice of adding an intercept to the model and obtaining the diagonal elements of the projection matrix for the augmented model.

For linear models in which \(X\) is not of full rank, similar definitions could be made using a generalized inverse of the (estimated) covariance matrix rather than the inverse.

##### Exercise 12.2

Show that for a regression model that does not contain an intercept, the diagonal elements of the perpendicular projection operator are equivalent to the estimated squared Mahalanobis distances computed with the assumption that \(\mu=0\).

##### Diagonal Elements of the Projection Operator

Having established that the diagonal elements of \(M\) are a reasonable measure of how unusual a row of \(X\) is, we are left with the problem of calibrating the measure. How big does \(m_{ii}\) have to be before we need to worry about it? The following proposition indirectly establishes several facts that allow us to provide some guidelines.

**Proposition 12.1.4**: _For any \(i\)_

\[m_{ii}(1-m_{ii})=\sum_{j\neq i}m_{ij}^{2}.\]

_Proof_ Because \(M\) is a symmetric idempotent matrix,

\[m_{ii}=\sum_{j=1}^{n}m_{ij}m_{ji}=\sum_{j=1}^{n}m_{ij}^{2}=m_{ii}^{2}+\sum_{j \neq i}m_{ij}^{2}.\]

Subtracting gives

\[m_{ii}(1-m_{ii})=m_{ii}-m_{ii}^{2}=\sum_{j\neq i}m_{ij}^{2}.\]

The term on the right-hand side of Proposition 12.1.4 is a sum of squared terms. This must be nonnegative, so \(m_{ii}(1-m_{ii})\geq 0\). It follows immediately that the \(m_{ii}\)s must lie between 0 and 1.

Since the largest value that an \(m_{ii}\) can take is 1, any value near 1 indicates a point with extremely high leverage. Other values, considerably less than 1, can also indicate high leverage. Because \(\operatorname{tr}(M)=r(M)\), the average value of the \(m_{ii}\)s is \(p/n\). Any \(m_{ii}\) value that is substantially larger than \(p/n\) indicates a point with high leverage. Some useful but imprecise terminology is set in the following definition.

**Definition 12.1.5**: Any case that corresponds to a row of the model matrix that is unlike the other rows is called an _outlier in the design space_ (or estimation space). Any case corresponding to an \(m_{ii}\) substantially larger than \(p/n\) is called a case with _high leverage_. Any case corresponding to an \(m_{ii}\) near 1 is called a case with _extremely high leverage_.

Points with extremely high leverage have dramatic effects. If \(m_{ii}\) happens to be near 1, then \(m_{ii}(1-m_{ii})\) must be near zero and the right-hand side of Proposition 12.1.4 must be near zero. Since the right-hand side is a sum of squared terms, for all \(j\neq i\) the terms \(m_{ij}\) must be near zero. As will be shown, this causes a point with extremely high leverage to dominate the fitting process.

Let \(\rho_{i}\) be a vector of zeros with a 1 in the \(i\)th row so that \(x^{\prime}_{i}=\rho^{\prime}_{i}X\). The mean for the \(i\)th case is \(\rho^{\prime}_{i}X\beta=x^{\prime}_{i}\beta\), which is estimated by \(x^{\prime}_{i}\hat{\beta}=\rho^{\prime}_{i}MY=\sum_{j=1}^{n}m_{ij}y_{j}\). If \(m_{ii}\) is close to 1, \(m_{ij}\) is close to zero for all \(j\neq i\); thus, the rough approximation \(\rho^{\prime}_{i}MY\doteq y_{i}\) applies. This approximation is by no means unusual. It simply says that the model fits well; however, the fact that the estimate largely ignores observations other than \(y_{i}\) indicates that something strange is occurring. Since \(m_{ii}\) is near 1, \(x^{\prime}_{i}\) is far from the other rows of \(X\). Thus, there is little information available about behavior at \(x^{\prime}_{i}\) other than the observation \(y_{i}\).

The fact that \(y_{i}\) is fit reasonably well has important implications for the estimated regression equation \(f(x)=x^{\prime}\hat{\beta}\). This function, when evaluated at \(x_{i}\), must be near \(y_{i}\). Regardless of whether \(y_{i}\) is an aberrant observation or whether a different approximate model is needed for observations taken near \(x_{i}\), the estimated regression equation will adjust itself to fit \(y_{i}\) reasonably well. If necessary, the estimated regression equation will ignore the structure of the other data points in order to get a reasonable fit to the point with extremely high leverage. Thus, points with extremely high leverage have the potential to influence the estimated regression equation a great deal.

#### Examples

##### Simple Linear Regression

Consider the model \(y_{i}=\beta_{0}+\beta_{1}x_{i}+e_{i}\), \(i=1,\ldots,6\), \(x_{1}=1\), \(x_{2}=2\), \(x_{3}=3\), \(x_{4}=4\), \(x_{5}=5\), \(x_{6}=15\). \(x_{6}\) is far from the other \(x\) values, so it should be a case with high leverage. In particular, \(m_{66}=0.936\), so case six is an extremely high leverage point. (Section 6.1 gives a formula for \(M\).)

For \(i=1\),..., 5, data were generated using the model

\[y_{i}=2+3x_{i}+e_{i},\]

with the \(e_{i}\)s independent \(N(0,1)\) random variates. The \(y\) values actually obtained were \(y_{1}=7.455\), \(y_{2}=7.469\), \(y_{3}=10.366\), \(y_{4}=14.279\), \(y_{5}=17.046\). The model was fit under three conditions: (1) with the sixth case deleted, (2) with \(y_{6}=47=2+3(x_{6})\), and (3) with \(y_{6}=11=2+3(x_{3})\). The results of fitting the simple linear regression model are summarized in the table below.

Figure 12.1 contains a scatterplot of the data under condition (c) and the fitted lines for conditions (a) and (c). Under conditions (a) and (b), reasonably consistent fits are obtained. In particular, the extremely high leverage case has little effect on point estimation in these situations where the data are good and the model is true. (The high leverage case could have a large effect in decreasing the size of interval estimates.)

Under condition (c), when the model is no longer valid for case six, the fit is grossly different from those obtained under conditions (a) and (b). When the extremely high leverage case is inconsistent with the rest of the data, the fit of the regression equation is dominated by that case. Note that the first five cases lead us to expect a value for \(y_{6}\) in the mid-40s, but with \(y_{6}=11\), the fitted value is 13.11, close to 11 and far from the mid-40s. Finally, the fit of the model, as measured by the _MSE_, is good under conditions (a) and (b), but much poorer under condition (c).

Other things being equal, under condition (b) it would be wise to include case six in the analysis. Under condition (c), it might be wiser not to try to model all the data, but rather to model only the first five cases and admit that little is known about behavior at \(x\) values substantially larger than 5. Unfortunately, in order to distinguish between conditions (b) and (c), the true model must be known, which in practice is not the case.

Finally, a word about residuals for case six. Under condition (b), the regression equation is right on the data point, \(\hat{e}_{6}=0.2\), and \(r_{6}=0.611\). On the other hand,

Figure 12.1: Scatterplot and fitted lines for Example 12.1.6

for condition (c) there is a reasonable amount of error. The residual is \(\hat{e}_{6}=-2.11\). Compared to the other residuals (cf. Figure 12.1), \(\hat{e}_{6}\) is not large, but neither is it extremely small. The standardized residuals have a standard deviation of about 1, so the standardized residual for case six, \(r_{6}=-1.918\), is quite substantial.

Another useful tool is to look at the predicted residuals. For case six, this involves looking at the predicted value with case six deleted, 43.91, and comparing that to the observed value. For condition (b), the predicted residual is 3.09. For condition (c), the predicted residual is \(-32.91\), which seems immense. To get a better handle on what these numbers mean, we need to standardize the predicted residuals. Predicted residuals are discussed in more detail in Sections 5 and 6. Proposition 12.6.1 gives a simple formula for computing standardized predicted residuals. Using this formula, the standardized predicted residual for case six under condition (b) is \(t_{6}=0.556\), which is very reasonable. For condition (c), it is \(t_{6}=-5.86\), which is large enough to cause concern. (Section 6 explains why \(t_{6}\) is not huge under condition (c).)

#### Example 12.1.7

We now modify Example 12.1.6 by adding a second point far away from the bulk of the data. This is done in two ways. First, the extremely high leverage point is replicated with a second observation taken at \(x=15\). Second, a high leverage point is added that is smaller than the bulk of the data. In both cases, the \(y\) values for the first five cases remain unchanged.

With two observations at \(x=15\), \(m_{66}=m_{77}=0.48\). This is well above the value \(p/n=2/7=0.29\). In particular, the diagonal values for the other five cases are all less than 0.29.

To illustrate the effect of the high leverage points on the regression equation, conditions (b) and (c) of the previous example were combined. In other words, the two \(y\) values for \(x=15\) were taken as 11 and 47. The estimated regression equation becomes \(\hat{y}=6.783+1.5140x\). The slope of the line is about halfway between the slopes under conditions (b) and (c). More importantly, the predicted value for \(x=15\) is 29.49. The regression equation is being forced near the mean of \(y_{6}\) and \(y_{7}\). (The mean is 29.)

One of the salient points in this example is the effect on the root mean squared error. Under condition (b), where the high leverage point was consistent with the other data, the root mean squared error was 1.293. Under condition (c), where the high leverage point was grossly inconsistent with the other data, the root mean squared error was 4.293. In this case, with two high leverage points, one consistent with the bulk of the data and one not, the root mean squared error is 11.567. This drastic change is because, with two \(y\) values so far apart, one point almost has to be an outlier. Having an outlier in the \(y\)s at a high leverage point has a devastating effect on all estimates, especially the estimated error.

In the second illustration, \(x_{7}\) was taken as \(-9\). This was based on adding a second observation as far to the left of the bulk of the data as \(x_{6}=15\) is to the right. The leverages are \(m_{66}=m_{77}=0.63\). Again, this value is well above \(2/7\) and is far above the other diagonal values, which are around 0.15.

To illustrate the effect of high leverage on estimation, the \(y\) values were taken as \(y_{6}=y_{7}=11\). The estimated regression equation was \(11.0906+0.0906x\). The root mean squared error was \(3.921\). The \(t\) statistic for testing that the slope equaled zero was \(0.40\).

In essence, the data in the second illustration have been reduced to three points: a point \(x=-9\) with a \(y\) value of \(11\), a point \(x=15\) with a \(y\) value of \(11\), and a point \(x=3\) (the mean of \(x_{1}\) to \(x_{5}\)) with a \(y\) value of \(11.523\) (the mean of \(y_{1}\) to \(y_{5}\)). Compared to the high leverage points at \(-9\) and \(15\), the five points near \(3\) are essentially replicates.

Both of these scenarios illustrate situations where the leverages contain gaps. The first illustration has no points with leverages between \(0.28\) and \(0.48\). The second has no leverages between \(0.16\) and \(0.63\). Such _gaps in the leverages indicate that the predictor variables contain clusters of observations that are separated from each other._

The final example of this section illustrates that high leverage points are model dependent. Since our measure of leverage is based on the perpendicular projection operator onto \(C(X)\), it is not surprising that changing the model can affect the leverage of a case. The example below is a polynomial regression where a particular case does not have high leverage for fitting a line, but does have high leverage for fitting a parabola.

#### 2.1.8 Quadratic Regression

Consider fitting the models

\[y_{i} = \beta_{0}+\beta_{1}x_{i}+e_{i},\] \[y_{i} = \gamma_{0}+\gamma_{1}x_{i}+\gamma_{2}x_{i}^{2}+e_{i},\]

\(i=1,\ldots,7\). The values of the \(x_{i}\)s used were \(x_{1}=-10\), \(x_{2}=-9\), \(x_{3}=-8\), \(x_{4}=0\), \(x_{5}=8\), \(x_{6}=9\), \(x_{7}=10\). Note that the value of \(x_{4}\) appears to be in the center of the data. For fitting a straight line, that appearance is correct. For fitting a line, the leverage of the fourth case is \(0.14\).

The model matrix for the quadratic model is

\[X=\begin{bmatrix}1&-10&100\\ 1&-9&81\\ 1&-8&64\\ 1&0&0\\ 1&8&64\\ 1&9&81\\ 1&10&100\end{bmatrix}.\]Note that the choice of the \(x_{i}\)s makes the second column of \(x\) orthogonal to the other two columns. An orthonormal basis for \(C(X)\) is easily obtained, and thus the diagonal elements of \(M\) are also easily obtained. The value of \(m_{44}\) is 0.84, which is quite large. From inspecting the third column of the model matrix, it is clear that the fourth case is unlike the rest of the data.

To make the example more specific, for \(i\neq 4\) data were generated from the model

\[\begin{array}{rcl}y_{i}&=&19.6+0.4x_{i}-0.1x_{i}^{2}+e_{i}\\ &=&-0.1(x_{i}-2)^{2}+20+e_{i},\end{array}\]

with the \(e_{i}\)s independent \(N(0,\,1)\) random variables. The values 0, 11.5, and 19.6 were used for \(y_{4}\). These values were chosen to illustrate a variety of conditions. The value 19.6 is consistent with the model given above. In particular, \(19.6=\mathrm{E}(y_{4})\). The value \(11.5=\mathrm{E}(y_{2}+y_{6})/2\) should give a fit that is nearly linear. The value 0 is simply a convenient choice that is likely to be smaller than any other observation. The \(Y\) vector obtained was

\[Y=(6.230,\,8.275,\,8.580,\,y_{4},\,16.249,\,14.791,\,14.024)^{\prime}.\]

Figure 12.2 contains a scatterplot of the data that includes all three values for \(y_{4}\) as well as a plot of the true regression curve.

The linear and quadratic models were fitted with all three of the \(y_{4}\) values and with the fourth case deleted from the model. For all models fitted, the coefficient of the linear term was 0.4040. As mentioned above, the second column (the linear column) of the matrix \(X\) is orthogonal to the other columns. Thus, for any value of \(y_{4}\) the linear coefficient will be the same for the quadratic model and the linear model. The linear coefficient does not depend on the value of \(y_{4}\) because \(x_{4}=0\). Also, because \(x_{4}=0\), the predicted value for \(y_{4}\) is just the intercept of the line. Fitting simple linear regressions resulted in the following:

\[\begin{array}{rcl}&\mbox{Linear Fits}\\ y_{4}&\hat{y}_{4}=\hat{\beta}_{0}&\sqrt{MSE}&dfE\\ \hline\mbox{deleted}&11.36&1.263&4\\ 0.0&9.74&4.836&5\\ 11.5&11.38&1.131&5\\ 19.6&12.54&3.594&5\end{array}\]

As designed, the fits for \(y_{4}\) deleted and \(y_{4}=11.5\) are almost identical. The other values of \(y_{4}\) serve merely to move the intercept up or down a bit. They do not move the line enough so that the predicted value \(\hat{y}_{4}\) is close to the observed value \(y_{4}\).

[MISSING_PAGE_EMPTY:5662]

It is important to note that the problems caused by high leverage points are not unique to fitting models by least squares. When fitting by least squares, high leverage points are fit well, because it is assumed that the model is correct for all of the data points. Least squares accommodates all the data points, including the points with high leverage. If a model, say model A, fits the bulk of the data, but a different model, say model B, is necessary to explain the data when including the cases with high leverage, then the error of fitting model A to the high leverage cases is likely to be large. Any method of fitting models (c.f. Section 13.6 and 13.7) that seeks to minimize errors will modify the fit so that those large errors do not occur. Thus, any fitting mechanism forces the fitted model to do a reasonable job of fitting all of the data. Since the high leverage cases must be fit reasonably well and since, by definition, data are sparse near the high leverage points, the high leverage points are often fit extremely well.

### Checking Normality

We give a general discussion of the problem of checking normality for a random sample and then relate it to the analysis of residuals. Suppose \(v_{1},\ldots,v_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\) and \(z_{1},\ldots,z_{n}\) are i.i.d. \(N(0,1)\). Ordering these from smallest to largest gives the order statistics \(v_{(1)}\leq\cdots\leq v_{(n)}\) and \(z_{(1)}\leq\cdots\leq z_{(n)}\). The expected values of the standard normal order statistics are \(\mathrm{E}[z_{(1)}],\ldots,\mathrm{E}[z_{(n)}]\). Since the \(v_{i}\)s are normal, \([v_{(i)}-\mu]/\sigma\sim z_{(i)}\) and we should have the approximate equality, \([v_{(i)}-\mu]/\sigma\doteq\mathrm{E}[z_{(i)}]\) or \(v_{(i)}\doteq\sigma\mathrm{E}[z_{(i)}]+\mu\).

Suppose now that \(v_{1},\ldots,v_{n}\) are observed and we want to see if they are a random sample from a normal distribution. If the \(v_{i}\)s are from a normal distribution, a graph of the pairs \((\mathrm{E}[z_{(i)}],v_{(i)})\) should be an approximate straight line. If the graph is not an approximate straight line, nonnormality is indicated. These graphs are variously called _rankit plots_, _normal plots_, or _q-q plots_.

To make the graph, one needs the values \(\mathrm{E}[z_{(i)}]\). These values, often called _rankits_, _normal scores_, or _theoretical (normal) quantiles_, are frequently approximated as follows. Let

\[\Phi(x)=\int_{-\infty}^{x}(2\pi)^{-1/2}\exp[-t^{2}/2]dt.\]

\(\Phi(x)\) is the cumulative distribution function for a standard normal random variable. Let \(u\) have a uniform distribution on the interval (0, 1). Write \(u\sim U(0,1)\). It can be shown that

\[\Phi^{-1}(u)\sim N(0,1).\]

If \(z_{1},\ldots,z_{n}\) are i.i.d. \(N(0,1)\) and \(u_{1},\ldots,u_{n}\) are i.i.d. \(U(0,1)\), then

\[z_{(i)}\sim\Phi^{-1}(u_{(i)}),\]

and

\[\mathrm{E}[z_{(i)}]=\mathrm{E}[\Phi^{-1}(u_{(i)})].\]

[MISSING_PAGE_FAIL:370]

the real question is not whether the data are nonnormal, but whether they are sufficiently nonnormal to invalidate a normal approximation. This is a more difficult question to address. See Arnold (1981, Chapter 10) for a discussion of _asymptotic consistency_ of least squares estimates.

Shapiro and Wilk (1965) developed a formal test for normality related to normal plots. Unfortunately, the test involves knowing the inverse of the covariance matrix of \(z_{(1)},\ldots,z_{(n)}\). An excellent approximation to their test was suggested by Shapiro and Francia (1972). The approximate test statistic is the square of the sample correlation coefficient computed from the pairs (E[\(z_{(i)}\)], \(v_{(i)}\)), \(i=1,\ldots,n\). Let

\[W^{\prime}=\left(\sum_{i=1}^{n}\mbox{E}[z_{(i)}]v_{(i)}\right)^{2}\Big{/}\sum _{i=1}^{n}(\mbox{E}[z_{(i)}])^{2}\sum_{i=1}^{n}(v_{(i)}-\bar{v})^{2}.\]

(Note: \(\sum_{i=1}^{n}\mbox{E}[z_{(i)}]=0\) by symmetry.) If \(W^{\prime}\) is large, there is no evidence of nonnormality. Small values of \(W^{\prime}\) are inconsistent with the hypothesis that the data are a random sample from a normal distribution. Approximate percentage points for the distribution of \(W^{\prime}\) are given in Christensen (2015) and many other places.

To test for normality in linear models, the \(v_{i}\)s are generally replaced by the \(r_{i}\)s, i.e., the standardized residuals.

Figure 12: Normal plots for normal data, \(n=10\)

Figure 12.4: Normal plots for normal data, \(n=25\)

Figure 12.5: Normal plots for normal data, \(n=50\)

_Example 12.2.2_: The \(W^{\prime}\) statistics were computed for the \(Eis\) and \(Ri\)s from Example 12.2.1 with \(n=25\). The values are listed below.

\begin{tabular}{c c c} \(i\) & \(W^{\prime}(E)\) & \(W^{\prime}(R)\) \\ \hline
1 & 0.966 & 0.951 \\
2 & 0.975 & 0.982 \\
3 & 0.980 & 0.980 \\
4 & 0.973 & 0.968 \\
5 & 0.978 & 0.973 \\
6 & 0.981 & 0.975 \\
7 & 0.945 & 0.964 \\
8 & 0.955 & 0.947 \\
9 & 0.946 & 0.948 \\ \end{tabular}

Note that, in this example (as in previous editions of the book), the \(W^{\prime}\) values do not seem to be either systematically higher or lower when computed on the residuals

Figure 12.6: Normal plots for normal data and corresponding standardized residual plots

[MISSING_PAGE_EMPTY:5668]

**Exercise 12.4** Using the model of Example 12.2.2, estimate the power of detecting a \(t(3)\) with \(\alpha=0.05\) by simulation.

#### Other Applications for Normal Plots

We close this section with two variations on the use of normal rankit plots.

Consider an ANOVA, say

\[y_{ijk}=\mu_{ij}+e_{ijk},\]

\(i=1,\ldots,a,\ j=1,\ldots,b,\ k=1,\ldots,N_{ij}\). Rather than using residuals to check whether the \(e_{ijk}\)s are i.i.d. \(N(0,\sigma^{2})\), for each pair \(i,\ j\) we can check whether \(y_{ij1},\ldots,y_{ijN_{ij}}\) are i.i.d. \(N(\mu_{ij},\sigma^{2})\). The model assumes that for each group \(ij\), the \(N_{ij}\) observations are a random sample from a normal population. Each group can be checked for normality individually. This leads to forming \(ab\) normal plots, one for each group. Of course, these plots will only work well if the \(N_{ij}\)s are reasonably large.

We now present graphical methods for evaluating the interaction in a two-way ANOVA with only one observation in each cell. The model is

\[y_{ij}=\mu+\alpha_{i}+\eta_{j}+(\alpha\eta)_{ij}+e_{ij},\]

\(i=1,\ldots,a,\ j=1,\ldots,b\). Here the \(e_{ij}\)s are assumed to be i.i.d. \(N(0,\sigma^{2})\). With one observation per cell, the \((\alpha\eta)_{ij}\)s are confounded with the \(e_{ij}\)s; the effects of interaction cannot be separated from those of error.

As was mentioned in Section 7.2, the interactions in this model are often assumed to be nonexistent so that an analysis of the main effects can be performed. As an alternative to assuming no interaction, one can evaluate graphically an orthogonal set of \((a-1)(b-1)\) interaction contrasts, say \(\lambda^{\prime}_{rs}\beta\). If there are no interactions, the values \(\lambda^{\prime}_{rs}\hat{\beta}\)/\(\sqrt[3]{\lambda^{\prime}_{rs}(X^{\prime}X)^{-}\lambda_{rs}}\) are i.i.d. \(N(0,\sigma^{2})\). Recall that for an interaction contrast, \(\lambda^{\prime}\beta=\sum_{ij}q_{ij}(\alpha\eta)_{ij}\),

\[\lambda^{\prime}\hat{\beta}=\sum_{ij}q_{ij}\ y_{ij}\]

and

\[\lambda^{\prime}(X^{\prime}X)^{-}\lambda=\sum_{ij}q_{ij}^{2}.\]The graphical procedure is to order the \(\lambda^{\prime}_{rs}\hat{\beta}\!\!\!\!/\!\sqrt{\lambda^{\prime}_{rs}(X^{\prime}X)^ {-}\lambda_{rs}}\) values and form a normal rankit plot. If there are no interactions, the plot should be linear. Often there will be some estimated interactions that are near zero and some that are clearly nonzero. The near zero interactions should fall on a line, but clearly nonzero interactions will show up as deviations from the line. Contrasts that do not fit the line are identified as nonzero interaction contrasts (without having executed a formal test).

The interactions that fit on a line are used to estimate \(\sigma^{2}\). This can be done in either of two ways. First, an estimate of the slope of the linear part of the graph can be used as an estimate of the standard deviation \(\sigma\). Second, sums of squares for the contrasts that fit the line can be averaged to obtain a mean squared error.

Both methods of estimating \(\sigma^{2}\) are open to criticism. Consider the slope estimate of \(\sigma\) and, in particular, assume that \((a-1)(b-1)=12\) and that there are three nonzero contrasts all yielding large positive values of \(\lambda^{\prime}_{rs}\hat{\beta}\!\!\!\!/\!\sqrt{\lambda^{\prime}_{rs}(X^{ \prime}X)^{-}\lambda_{rs}}\). In this case, the ninth largest value is plotted against the ninth largest rankit. Unfortunately, we do not know that the ninth largest value is the ninth largest observation in a random sample of size 12. If we could correct for the nonzero means of the three largest contrasts, what we observed as the ninth largest value could become anything from the ninth to the twelfth largest value. To estimate \(\sigma\), we need to plot the mean adjusted statistics \(\left(\lambda^{\prime}_{rs}\hat{\beta}-\lambda^{\prime}_{rs}\beta\right)\!\! \!\!/\!\sqrt{\lambda^{\prime}_{rs}(X^{\prime}X)^{-}\lambda_{rs}}\). We know that 9 of the 12 values \(\lambda^{\prime}_{rs}\beta\) are zero. The ninth largest value of \(\lambda^{\prime}_{rs}\hat{\beta}\!\!\!/\!\!\sqrt{\lambda^{\prime}_{rs}(X^{ \prime}X)^{-}\lambda_{rs}}\) can be any of the order statistics of the mean adjusted values \(\left(\lambda^{\prime}_{rs}\hat{\beta}-\lambda^{\prime}_{rs}\beta\right)\!\! \!/\!\!\sqrt{\lambda^{\prime}_{rs}(X^{\prime}X)^{-}\lambda_{rs}}\) between 9 and 12. The graphical method assumes that extreme values of \(\lambda^{\prime}_{rs}\hat{\beta}\!\!\!/\!\!\sqrt{\lambda^{\prime}_{rs}(X^{ \prime}X)^{-}\lambda_{rs}}\) are also extreme values of the mean adjusted statistics. There is no justification for this assumption. If the ninth largest value were really the largest of the 12 mean adjusted statistics, then plotting the ninth largest value rather than the ninth largest mean adjusted value against the ninth largest rankit typically indicates a slope that is larger than \(\sigma\). Thus the graphical procedure tends to overestimate the variance. Alternatively, the ninth largest value may not seem to fit the line and so, inappropriately, be declared nonzero. These problems should be ameliorated by dropping the three clearly nonzero contrasts and replotting the remaining contrasts as if they were a sample of size 9. In fact, the replotting method will tend to have a downward bias, as discussed in the next paragraph.

The criticism of the graphical procedure was based on what happens when there are nonzero interaction contrasts. The criticism of the mean squared error procedure is based on what happens when there are no nonzero interaction contrasts. In this case, if one erroneously identifies contrasts as being nonzero, the remaining contrasts have been selected for having small absolute values of \(\lambda^{\prime}_{rs}\hat{\beta}\!\!\!/\!\!\sqrt{\lambda^{\prime}_{rs}(X^{ \prime}X)^{-}\lambda_{rs}}\) or, equivalently, for having small sums of squares. Averaging a group of sums of squares that were chosen to be small clearly underestimates \(\sigma^{2}\). The author's inclination is to use the mean squared error criterion and try very hard to avoid erroneously identifying zero interactions as nonzero. This avoids the problem of estimating the slope of the normal plot. Simulation envelopes such as those discussed by Atkinson (1985, Chapter 4) can be very helpful in deciding which contrasts to identify as nonzero.

Some authors contend that, for visual reasons, normal plots should be replaced with plots that do not involve the sign of the contrasts, see Atkinson (1981, 1982). Rather than having a graphical procedure based on the values \(\lambda_{rs}^{\prime}\hat{\beta}/\sqrt[3]{\lambda_{rs}^{\prime}(X^{\prime}X)^{-} \lambda_{rs}}\), the squared values, i.e., the sums of squares for the \(\lambda_{rs}^{\prime}\beta\) contrasts, can be used. When there are no interactions,

\[\frac{SS(\lambda_{rs}^{\prime}\beta)}{\sigma^{2}}\sim\chi^{2}(1).\]

The contrasts are orthogonal so, with no interactions, the values \(SS(\lambda_{rs}^{\prime}\beta)\) form a random sample from a \(\sigma^{2}\chi^{2}(1)\) distribution. Let \(w_{1}\),..., \(w_{r}\) be i.i.d \(\chi^{2}(1)\), where \(r=(a-1)(b-1)\). Compute the expected order statistics \(\text{E}[w_{(i)}]\) and plot the pairs \(\big{(}\text{E}[w_{(i)}],\,SS(\lambda_{(i)}^{\prime}\beta)\big{)}\), where \(SS(\lambda_{(i)}^{\prime}\beta)\) is the \(i\)th smallest of the sums of squares. With no interactions, this should be an approximate straight line through zero with slope \(\sigma^{2}\). For nonzero contrasts, \(SS(\lambda_{rs}^{\prime}\beta)\) has a distribution that is \(\sigma^{2}\) times a _noncentral_\(\chi^{2}(1)\). Values of \(SS(\lambda_{rs}^{\prime}\beta)\) that are substantially above the linear portion of the graph indicate nonzero contrasts. A graphical estimate of \(\sigma^{2}\) is available from the sums of squares that fit on a line; this has bias problems similar to that of a normal plot. The theoretical quantiles \(\text{E}[w_{(i)}]\) can be approximated by evaluating the inverse of the \(\chi^{2}(1)\) cdf at \(i/(n+1)\).

A corresponding method for estimating \(\sigma\), based on the square roots of the sums of squares, is called a _half-normal plot_. The expected order statistics are often approximated as \(\Phi^{-1}((n+i)/(2n+1))\).

These methods can be easily extended to handle other situations in which there is no estimate of error available. In fact, this graphical method was first proposed by Daniel (1959) for analyzing \(2^{n}\) factorial designs. Daniel (1976) also contains a useful discussion. Christensen (1996) and [http://www.stat.unm.edu/~fletcher/TopicsInDesign](http://www.stat.unm.edu/~fletcher/TopicsInDesign) illustrate some of these ideas. Lenth (2015) argues that methods exist superior to these.

### Checking Independence

Lack of independence occurs when \(\text{Cov}(e)\) is not diagonal. One reason that good methods for evaluating independence are difficult to develop is that, unlike the other assumptions involved in \(e\sim N(0,\sigma^{2}I)\), independence is not a property of the population in question. Independence is a property of the way that the population is sampled. As a result, there is no way to check independence without thinking hard about the method of sampling. Identifying lack of independence is closely related to identifying lack of fit. For example, consider data from a randomized complete block (RCB) experiment being analyzed with a one-way analysis of variance model that ignores blocks. If the blocks have fixed effects, the one-way model suffers from lack of fit. If the blocks effects are random with a common mean, the one-way model suffers from lack of independence. We begin with a general discussion of ideas for testing the independence assumption based upon Christensen and Bedrick (1997). This is followed by a subsection on detecting serial correlation.

A key idea in checking independence is the formation of rational subgroups. To evaluate whether a group of numbers form a random sample from a population, Shewhart (1931) proposed using control charts for means. The means being charted were to be formed from rational subgroups of the observations that were obtained under essentially identical conditions. Shewhart (1939, p. 42) suggests that a control chart is less a test for whether data form a random sample and more an operational definition of what it means to have a random sample. It is easily seen that a means chart based on rational subgroups is sensitive to lack of independence, lack of fit (nonconstant mean), inequality of variances, and nonnormality. In analyzing linear models, statisticians seek assurance that any lack of independence or other violations of the assumptions are not so bad as to invalidate their conclusions. Essentially, statisticians need an operational definition of when traditional linear model theory can be applied.

As used with linear models, rational subgroups are simply clusters of observations. They can be clustered in time, or in space, by having similar predictor variables, by being responses on the same individual, or by almost anything that the sampling scheme suggests could make observations within a cluster more alike than observations outside a cluster. To test for lack of independence, the near replicate lack-or-fit tests presented in Subsection 6.7.2 can be used. Simply replace the clusters of near replicates with clusters of rational subgroups determined by the sampling scheme. Christensen and Bedrick (1997) found that the analysis of covariance test, i.e., the Christensen (1989) test, worked well in a wide variety of situations, though the Shillington test often worked better when the clusters were very small. Of course, specialized tests for specific patterns of nonindependence can work much better than these general tests _when the specific patterns are appropriate_.

##### Serial Correlation

An interesting case of nonindependence is serial correlation. This occurs frequently when observations \(y_{1}\), \(y_{2}\),..., \(y_{n}\) are taken serially at equally spaced time periods. A model often used when the observations form such a time series is a symmetric _Toeplitz matrix_

\[{\rm Cov}(e)=\sigma^{2}\left[\begin{array}{ccccccc}1&\rho_{1}&\rho_{2}& \cdots&\rho_{n-3}&\rho_{n-2}&\rho_{n-1}\\ \rho_{1}&1&\rho_{1}&\ddots&\ddots&\rho_{n-3}&\rho_{n-2}\\ \rho_{2}&\rho_{1}&1&\ddots&\ddots&\ddots&\rho_{n-3}\\ \vdots&\ddots&\ddots&\ddots&\ddots&\ddots&\vdots\\ \rho_{n-3}&\ddots&\ddots&\ddots&1&\rho_{1}&\rho_{2}\\ \rho_{n-2}&\rho_{n-3}&\ddots&\ddots&\rho_{1}&1&\rho_{1}\\ \rho_{n-1}&\rho_{n-2}&\rho_{n-3}&\cdots&\rho_{2}&\rho_{1}&1\end{array}\right],\]

where \(\rho_{1}\), \(\rho_{2}\),..., \(\rho_{n-1}\) are such that \({\rm Cov}(e)\) is positive definite. Typically, only the first few of \(\rho_{1}\), \(\rho_{2}\),..., \(\rho_{n-1}\) will be substantially different from zero. One way of detecting serial correlation is to plot \(r_{i}\) versus \(i\). If, say, \(\rho_{1}\) and \(\rho_{2}\) are positive and \(\rho_{3}\),..., \(\rho_{n}\) are near zero, a plot of \(r_{i}\) versus \(i\) may have oscillations, but residuals that are adjacent should be close. If \(\rho_{1}\) is negative, then \(\rho_{2}\) must be positive. The plot of \(r_{i}\) versus \(i\) in this case may or may not show overall oscillations, but adjacent residuals should be oscillating rapidly. An effective way to detect a nonzero \(\rho_{s}\) is to plot \((r_{i},r_{i+s})\) for \(i=1\),..., \(n-s\) or to compute the corresponding sample correlation coefficient from these pairs.

A special case of serial correlation is \(\rho_{s}=\rho^{s}\) for some parameter \(\rho\) between \(-1\) and \(1\). This AR(1), i.e., autoregressive order \(1\), covariance structure can be obtained by assuming (1) \(e_{1}\sim N(0,\sigma^{2})\), and (2) for \(i>1\), \(e_{i+1}=\rho e_{i}+v_{i+1}\), where \(v_{2}\),..., \(v_{n}\) are i.i.d. \(N(0,(1-\rho^{2})\sigma^{2})\) and \(v_{i+1}\) is independent of \(e_{i}\) for all \(i\). Other models for serial correlation are discussed in Christensen et al. (2010, Section 10.3). Most are based on ARMA time series models, cf. _ALM-III_, Chapter 7.

#### Example 12.3.1

For \(\rho\) equal to \(-0.9\), \(-0.5\), \(0.5\), and \(0.9\), serially correlated error vectors were generated as just described. Dependent variable values \(y\) were obtained using (12.0.1) and the model (12.2.2) was fitted, giving a standardized residual vector \(r\). For all values of \(\rho\), the standardized residuals are plotted against their observation numbers. Within each figure, \(z_{1}\), \(z_{2}\),..., \(z_{n}\) are i.i.d. \(N(0,1)\) with \(e_{1}=z_{1}\) and \(v_{i}=\sqrt{1-\rho^{2}}z_{i}\), so only \(\rho\) changes. Figures 12.7, 12.8 and 12.9 give three independent sets of plots. Note that when \(\rho\) is positive, adjacent observations remain near one another. The overall pattern tends to oscillate slowly. When \(\rho\) is negative, the observations oscillate very rapidly; adjacent observations tend to be far apart, but observations that are one apart (e.g., \(e_{i}\) and \(e_{i+2}\)) are fairly close.

Figures 12.10, 12.11, 12.12, 12.13 and 12.14 contain plots with \(\rho=0\). Figure 12.10 is in the same form as Figures 12.7, 12.8 and 12.9. The other figures use a different style. Comparing Figure 12.10 with Figures 12.7, 12.8 and 12.9, it does not seem easy to distinguish between \(\rho=0\) and moderate correlations like \(\rho=\pm 0.5\).

Figure 12.8: Serial correlation standardized residual plots

Figure 12.9: Serial correlation standardized residual plots

Figures 12.10, 12.11, 12.12, 12.13 and 12.14 are of interest not only for illustrating a lack of serial correlation, but also as examples of what the plots in Section 4 should look like, i.e., these are standardized residual plots when all the model assumptions are valid. Note that the horizontal axis is not specified, because the residual plots should show no correlation, regardless of what they are plotted against. It is interesting to try to detect patterns in Figures 12.10, 12.11, 12.12, 12.13 and 12.14 because the

Figure 12.11: Standardized residual plots when model assumptions are valid

Figure 12.10: Serial correlation standardized residual plots with uncorrelated data

human eye is good at detecting/creating patterns, even though none exist in these plots.

Durbin and Watson (1951) provided an approximate test for the hypothesis \(\rho=0\). The Durbin-Watson test statistic is

\[d=\sum_{i=1}^{n-1}(\hat{e}_{i+1}-\hat{e}_{i})^{2}\Big{/}\sum_{i=1}^{n}\hat{e}_{i} ^{2}.\]

Here \(d\) is an estimate of \(\sum_{i=1}^{n-1}(e_{i+1}-e_{i})^{2}/\sum_{i=1}^{n}e_{i}^{2}\). For an AR(1) structure,

\[e_{i+1}-e_{i}=\rho e_{i}+v_{i+1}-e_{i}=(1-\rho)e_{i}+v_{i+1},\]

so we have

\[{\rm E}[e_{i+1}-e_{i}]^{2}={\rm E}[(1-\rho)e_{i}+v_{i+1}]^{2}=(1-\rho)^{2} \sigma^{2}+(1-\rho^{2})\sigma^{2}=2(1-\rho)\sigma^{2},\]

and

\[{\rm E}[e_{i}^{2}]=\sigma^{2}.\]

It follows that \(\sum_{i=1}^{n-1}(\hat{e}_{i+1}-\hat{e}_{i})^{2}\) should, for some constant \(K_{1}\), estimate \(K_{1}(1-\rho)\sigma^{2}\), and \(\sum_{i=1}^{n}\hat{e}_{i}^{2}\) estimates \(K_{2}\sigma^{2}\). \(d\) is a rough estimate of \((K_{1}/K_{2})(1-\rho)\) or \(K[1-\rho]\). If \(\rho=0\), \(d\) should be near \(K\). If \(\rho>0\), \(d\) will tend to be small. If \(\rho<0\), \(d\) will tend to be large.

The exact distribution of \(d\) varies with \(X\). For example, \(\sum_{i=1}^{n-1}(\hat{e}_{i+1}-\hat{e}_{i})^{2}\) is just a quadratic form in \(\hat{e}\), say \(\hat{e}A\hat{e}=Y^{\prime}(I-M)A(I-M)Y\). It takes little effort to see that \(A\) is a very simple matrix. By Theorem 1.3.2,

\[{\rm E}(\hat{e}A\hat{e})={\rm tr}[(I-M)A(I-M){\rm Cov}(Y)].\]

Thus, even the expected value of the numerator of \(d\) depends on the model matrix. Since the distribution depends on \(X\), it is not surprising that the exact distribution of \(d\) varies with \(X\).

Figure 14: Standardized residual plots when model assumptions are valid

**Exercise 12.5**  Show that \(d\) is approximately equal to \(2(1-r_{a})\), where \(r_{a}\) is the sample (auto)correlation between the pairs \((\hat{e}_{i+1},\hat{e}_{i})\)\(i=1,\ldots,n-1\).

### 12.4 Heteroscedasticity and Lack of Fit

Heteroscedasticity refers to having unequal variances. In particular, an independent heteroscedastic model has

\[\text{Cov}(e)=\text{Diag}(\sigma_{i}^{2}).\]

Lack of fit refers to having an incorrect model for \(\text{E}(Y)\). In Section 6.7 on testing lack of fit, we viewed this as having an insufficiently general model matrix. When lack of fit occurs, \(\text{E}(e)\equiv\text{E}(Y-X\beta)\neq 0\). Both heteroscedasticity and lack of fit are diagnosed by plotting the standardized residuals against any variable of choice. The chosen variable may be case numbers, time sequence, any predictor variable included in the model, any predictor variable not included in the model, or the predicted values \(\hat{Y}=MY\). If there is no lack of fit or heteroscedasticity, the residual plots should form a horizontal band. The plots in Section 3 with \(\rho=0.0\) are examples of such plots when the horizontal axis has equally spaced entries.

#### Heteroscedasticity

A horn-shaped pattern in a residual plot indicates that the variance of the observations is increasing or decreasing with the other variable.

_Example 12.4.1_  Twenty-five i.i.d. \(N(0,1)\) random variates, \(z_{1},\ldots,z_{25}\) were generated and \(y\) values were computed using (12.0.1) with \(e_{i}=x_{i}1z_{i}/60\). The variance of the \(e_{i}\)s increase as the \(x_{i}1\)s increase.

Figure 12.15 plots the standardized residuals \(R1\) against \(\hat{Y}\) and \(X_{2}\). The plot of \(R1\) versus \(\hat{Y}\) shows something of a horn shape, but it opens to the left and is largely dependent on one large residual with a \(\hat{y}\) of about 8.3. The plot against \(X_{2}\) shows very little. It is difficult to detect any pattern in either plot. In (b) the two relatively small values of \(x_{2}\) don't help. The top left component of Figure 12.16 plots R1 against \(X_{1}\) where you can detect a pattern but, again, by no means an obvious one. The impression of a horn shape opening to the right is due almost entirely to one large residual near \(x_{1}=70\). The remaining plots in Figure 12.16 as well as the plots in Figures 12.17 and 12.18 are independent replications. Often you can detect the horn shape but sometimes you cannot.

_Example 12.4.2_ To illustrate horn shapes that open to the left, Example 12.4.1 was repeated using \(e_{i}=60z_{i}/x_{i1}\). With these \(e_{i}\)s, the variance decreases as \(x_{1}\) increases. The plots are contained in Figures 12.19, 12.20 and 12.21. In Figure 12.19a of \(R1\) versus \(\hat{Y}\), we see a horn opening to the right. Note that from (12.0.1), if \(x_{2}\) is held constant, \(y\) increases as \(x_{1}\) decreases. In the plot, \(x_{2}\) is not being held constant, but the relationship still appears. There is little to see in Figure 12.19b \(R1\) versus \(X_{2}\). The plot of \(R1\) versus \(X_{1}\) in the top left of Figure 12.20 shows a horn opening to the left. The remaining plots in Figure 12.20 as well as the plots in Figure 12.21 are independent replications.

Although plotting the residuals seems to be the standard method for examining heteroscedasticity of variances, Examples 12.4.1 and 12.4.2 indicate that residual plots are far from foolproof.

Figure 12.15: Variance increasing with \(x_{1}\)

For one-way ANOVA models (and equivalent models such as two-way ANOVA with interaction), there are formal tests of heteroscedasticity available. The best known of these are Hartley's, Cochran's, and Bartlett's tests. The tests are based on the sample variances for the individual groups, say \(s_{1}^{2},\,\ldots,\,s_{t}^{2}\). Hartley's and Cochran's tests require equal sample sizes for each group; Bartlett's test does not. Hartley's test statistic is \(\max_{i}s_{i}^{2}/\min_{i}s_{i}^{2}\). Cochran's test statistic is \(\max_{i}s_{i}^{2}/\sum_{i=1}^{t}s_{i}^{2}\). Bartlett's

Figure 12.17: Variance increasing with \(x_{1}\)

Figure 12.16: Variance increasing with \(x_{1}\)

test statistic is \((n-t)\log\bar{s}_{i}^{2}-\sum_{i}(N_{i}-1)\log s_{i}^{2}\). Descriptions of these and other tests can be found in Mandansky (1988).

The partitioning ideas of Subsection 6.7.3 can be exploited to apply these tests to more general problems than one-way ANOVA. Simply replace \(s_{i}^{2}\) and \(N_{i}-1\) with \(MSE(i)\) and \(dfE(i)\) where \(MSE(i)\) and \(dfE(i)\) are computed from fitting the model to the data in the \(i\)th partition set. Bartlett's test can be applied with any partition sets; the other tests require the partition sets have equal sample sizes. If there are only two partition sets, the ratio of the \(MSE\)s provides a two-sided \(F\) test for equality of variances over the partition. In Example 6.7.3 the \(F\) statistic is \((13.857/10)/(2.925/9)=4.26\) which is an odd, but not tremendously odd, value for an \(F(10,\,9)\) distribution. The test is two-sided because small ratios are as indicative as large ones (and there is no good reason to assign either mean square to the numerator).

All of these tests are based on the assumption that the data are normally distributed, and the tests are quite notoriously sensitive to the invalidity of that assumption. For nonnormal data, the tests frequently reject the hypothesis of all variances being equal even when all variances are, in fact, equal. This is important because \(t\) and \(F\) tests tend not to be horribly sensitive to nonnormality. In other words, if the data are not normally distributed (and they never are), the data may be close enough to being normally distributed so that the \(t\) and \(F\) tests are approximately correct. However, the nonnormality may be enough to make Hartley's, Cochran's, and Bartlett's tests reject, so that the data analyst worries about a nonexistent problem of heteroscedasticity, cf. Box (1953).

More recently tests have been proposed that involve fitting models to functions of the residuals. The Breusch-Pagan/Cook-Weisberg test involves fitting a regression model to the squared residuals from the original fitted model. No regression

Figure 12.18: Variance increasing with \(x_{1}\)

effects suggest homoscedasticity. Significant predictors in the model for the squared residuals suggest that those predictors are related to heteroscedasticity. Rather than the squared residuals, models can also be fitted to the absolute values of the original residuals. In particular, when the original model is a one-way ANOVA, the popular Levene test is the \(F\) test for group differences in fitting an auxiliary one-way ANOVA to the absolute values of the original residuals. Again, this is generalizable to fitting arbitrary models on partition sets. It has been suggested that Levene's test is less sensitive to nonnormality than the comparable Cochran, Hartley, and Bartlett tests, cf. [https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm).

#### 12.6a

Show that \({\rm E}[\hat{e}_{i}^{2}/(1-m_{ii})]=\sigma^{2}\) under a standard linear model. Justify the Breusch-Pagan/Cook-Weisberg approach when applied to the squares of the standardized residuals.

Figure 12.19: Variance decreasing with \(x_{1}\)

#### Huber-White (Robust) Sandwich Estimator

Sandwich estimators get used in complicated situations to estimate the variability of point estimates. The idea is that useful point estimates are often obtained from an incorrect model but, if we suspect the model is incorrect, we still want valid estimates

Figure 12.21: Variance decreasing with \(x_{1}\)

Figure 12.20: Variance decreasing with \(x_{1}\)of their variability. It is of interest to see how sandwich estimates work for linear models.

It is often convenient to estimate \(\beta\) using least squares. For a standard linear model, least squares is optimal. But we want to estimate the variability of the estimates under a true heteroscedastic model. In other words, consider the problem of estimating the covariance matrix of least squares estimates when \(\operatorname{Cov}(Y)\neq\sigma^{2}I\) and, more specifically, under an heteroscedastic model \(\operatorname{Cov}(Y)=D(\sigma_{i}^{2})\). Our discussion follows closely that of Freedman (2006).

Assume the model

\[Y=X\beta+e,\qquad\operatorname{E}(e)=0,\qquad\operatorname{Cov}(e)=V.\]

The generalized least squares estimates, the BLUEs and, under normality, the MLEs and the minimum variance unbiased estimates all satisfy

\[X\hat{\beta}_{G}=X\left[X^{\prime}V^{-1}X\right]^{-1}X^{\prime}V^{-1}Y\]

But frequently we do not know \(V\).

One option, which is attractive when \(n\) is large relative to \(r(X)\), is to simply continue using least squares. The least squares estimate \(\hat{\beta}\) does not require knowledge of \(V\) and is still unbiased but it is inefficient in the sense that it has larger than necessary variability. However, when \(n\) is large relative to \(r(X)\), even inefficient estimates can be good estimates. If we use least squares, we still need to estimate the variability of our estimates and in particular,

\[\operatorname{Cov}(\hat{\beta})=\left[X^{\prime}X\right]^{-1}X^{\prime}VX \left[X^{\prime}X\right]^{-1}.\]

But to estimate this covariance matrix, we still need an estimate of \(V\).

As discussed in _ALM-III_ Chapter 4, we can create a parametric model for \(V\), say \(V(\theta)\), for some \(s\) vector of parameters \(\theta\). This allows us to estimate the parameters with \(\hat{\theta}\) and the covariance matrix with \(\hat{V}\equiv V(\hat{\theta})\), which immediately gives an estimate for the covariance matrix for \(\hat{\beta}\) of

\[\widehat{\operatorname{Cov}}(\hat{\beta})=\left[X^{\prime}X\right]^{-1}X^{ \prime}\hat{V}X\left[X^{\prime}X\right]^{-1}.\]

But if we go to all this trouble, rather than using least squares, we might as well use the _empirical estimate_ of \(\beta\) defined by,

\[X\tilde{\beta}=X\left[X^{\prime}\hat{V}^{-1}X\right]^{-1}X^{\prime}\hat{V}^{-1 }Y.\]

Unfortunately, as discussed in _ALM-III_ Section 4.7, the obvious way of estimating the covariance matrix of \(\tilde{\beta}\) leads to underestimating the variability.

Now consider a different approach, one that is specifically _excluded_ in _ALM-III_. Consider a model for \(\operatorname{Cov}(Y)\) that depends on \(\operatorname{E}(Y)\), say \(V(X\beta)\). Having the covariance matrix depend on \(\beta\) (as opposed to a distinct vector \(\theta\)) complicates the estimation of \(X\beta\). The standard estimation methods do not apply except maximum likelihood, and the nature of the likelihood equations changes dramatically. Nonetheless, we can easily estimate the covariance matrix of least squares estimates by looking at

\[\widehat{\text{Cov}}(\hat{\beta})=\left[X^{\prime}X\right]^{-1}X^{\prime}V(X\hat {\beta})X\left[X^{\prime}X\right]^{-1}.\]

The sandwich estimator is something of a compromise between these two approaches. Assume a heteroscedastic, uncorrelated model \(V=D(\sigma_{i}^{2})\). Note that, if we knew \(\beta\), we could get an unbiased estimate of \(V\) by simply observing that \(\text{E}(y_{i}-x_{i}^{\prime}\beta)^{2}=\sigma_{i}^{2}\), or

\[\text{E}\left[D^{2}(Y-X\beta)\right]=D(\sigma_{i}^{2}).\]

This suggests using

\[\hat{V}=D^{2}(Y-X\hat{\beta})\]

to estimate \(V\) and gives the _Huber-White robust sandwich estimator_ of the covariance of \(\hat{\beta}\),

\[\widehat{\text{Cov}}(\hat{\beta})=\left[X^{\prime}X\right]^{-1}X^{\prime}D^{ 2}(Y-X\hat{\beta})X\left[X^{\prime}X\right]^{-1}.\]

Unfortunately, \(D^{2}(Y-X\hat{\beta})\) is a _horrible_ estimate of \(D(\sigma_{i}^{2})\). Each \(\sigma_{i}^{2}\) is being estimated by \((y_{i}-x_{i}^{\prime}\hat{\beta})^{2}\) which is a worse estimate than \((y_{i}-x_{i}^{\prime}\beta)^{2}\) which is based on a single observation. Moreover, the model

\[Y=X\beta+e,\qquad\text{E}(e)=0,\qquad\text{Cov}(e)=D(\sigma_{i}^{2}).\]

has \(n+r(X)\) parameters. Why would anyone think that you could do a good job estimating that many parameters from \(n\) observations? Freedman (2006) repeated emphasized that Huber is not to blame for the misuse of his ideas.

The following exercise involves a situation where the sandwich estimator can be made to work. The model is a special case of the longitudinal models discussed in _ALM-III_ Section 5.6.

#### Exercise 12.6b

Consider a collection of \(r\) uncorrelated regression models

\[Y_{k}=X_{k}\beta+e_{k},\qquad\text{E}(e_{k})=0,\qquad\text{Cov}(e_{k})=V,\]

each with \(N\) observations. Find the least squares estimate \(\hat{\beta}\) from all the data. Find an estimate of \(V\). Find the sandwich estimator for \(\text{Cov}(\hat{\beta})\). Why is this a reasonable estimate? Hint: \(V=\text{E}[(Y_{k}-X_{k}\beta)(Y_{k}-X_{k}\beta)^{\prime}]\).

#### Lack of Fit

An additional use of residual plots is to identify lack of fit. The assumption is that \(\text{E}(e)=0\), so any _systematic_ pattern in the residuals (including a horn shape) can indicate lack of fit. Most commonly, one looks for a linear or quadratic trend in the residuals. Such trends indicate the existence of effects that have not been removed from the residuals, i.e., effects that have not been accounted for in the model.

Theorems 6.3.3 and 6.3.6 indicate that the residuals should be uncorrelated with any function of the predictor variables when we have the best possible model. So a nonzero correlation between the residuals and any other variable, say \(z\), indicates that something is wrong with the model. If \(z\) were not originally in the model, then it needs to be. A quadratic relationship between the residuals and \(z\) is indicative of a nonzero correlation between the residuals and \(z^{2}\), although the linear relationship might be much clearer when plotting the residuals against a standardized version of \(z\), say \(z-\bar{z}\).

For examining heteroscedasticity, the standardized residuals need to be used because the ordinary residuals are themselves heteroscedastic. For examining lack of fit, the ordinary residuals are preferred but we often use the standardized residuals for convenience.

#### Data were generated using (12.0.1) and the incorrect model

\[y_{i}=\beta_{0}+\beta_{2}x_{i2}+e_{i}\]

was fitted. In the independently generated Figures 12.22 and 12.23, the ordinary residuals \(\hat{e}\) and the standardized residuals \(R\) are plotted against \(x_{1}\) in (a) and (b), respectively. The decreasing trends in the residual plots indicate that \(x_{1}\) may be worth adding to the model.

Part (c) of the figures contains an _added variable plot_. To obtain it, find the ordinary residuals, say \(\hat{e}(x_{1})\), from fitting

\[x_{i1}=\gamma_{0}+\gamma_{2}x_{i2}+e_{i}.\]

By Exercise 9.2, a plot of \(\hat{e}\) versus \(\hat{e}(x_{1})\) gives an exact graphical display of the effect of adding \(x_{1}\) to the model.

The disadvantage of added variable plots is that it is time consuming to adjust the predictor variables under consideration for the variables already in the model. It is more convenient to plot residuals against predictor variables that have not been adjusted. As in Example 12.4.3, such plots are often informative but could, potentially, be misleading.

[MISSING_PAGE_EMPTY:5687]

[MISSING_PAGE_EMPTY:5688]

[MISSING_PAGE_EMPTY:5689]

Let \(X_{[i]}\) and \(Y_{[i]}\) be \(X\) and \(Y\) with the \(i\)th row deleted. Write \(x^{\prime}_{i}\) for the \(i\)th row of \(X\) and

\[\hat{\beta}_{[i]}\equiv(X^{\prime}_{[i]}X_{[i]})^{-1}X^{\prime}_{[i]}Y_{[i]}\]

for the estimate of \(\beta\) without the \(i\)th case. The predicted residual is defined as

\[\hat{e}_{[i]}\equiv y_{i}-x^{\prime}_{i}\hat{\beta}_{[i]}.\]

The predicted residuals are useful in checking for outliers. They are also used for model selection. The Predicted REsidual Sum of Squares (PRESS) is defined as

\[\text{PRESS}\equiv\sum_{i=1}^{n}\hat{e}_{[i]}^{2}.\]

Models with relatively low values of the PRESS statistic should be better than models with high PRESS statistics. It is tempting to think that PRESS is a more valid measure of how well a model fits than _SSE_, because PRESS predicts values not used in fitting the model. This reasoning may seem less compelling after the updating formula for the predicted residuals has been established.

The predicted residuals can also be used to check normality, heteroscedasticity, and lack of fit in the same way that the usual residuals are used. For these purposes they should be standardized. Their variances are

\[\text{Var}(\hat{e}_{[i]}) = \sigma^{2}+\sigma^{2}x^{\prime}_{i}(X^{\prime}_{[i]}X_{[i]})^{-1 }x_{i}\] \[= \sigma^{2}\left[1+x^{\prime}_{i}(X^{\prime}_{[i]}X_{[i]})^{-1}x_ {i}\right].\]

A reasonable estimate of \(\sigma^{2}\) is _MSE\({}_{[i]}\)_, the mean squared error for the model with the \(i\)th case deleted. Alternatively, \(\sigma^{2}\) could be estimated with the regular _MSE_. If _MSE_ is used, then the standardized predicted residuals are identical to the standardized residuals (see Exercise 12.6c). Standardized predicted residuals will be discussed again in Section 6. A more useful formula for \(\text{Var}(\hat{e}_{[i]})\) is given in Proposition 12.5.4.

We now present a series of results that establish the updating formulae for models with one deleted case.

**Proposition 12.5.1**: _Let \(A\) be a \(p\times p\) nonsingular matrix, and let \(a\) and \(b\) be \(q\times p\) rank \(q\) matrices. Then, if all inverses exist,_

\[(A+a^{\prime}b)^{-1}=A^{-1}-A^{-1}a^{\prime}(I+bA^{-1}a^{\prime})^{-1}bA^{-1}.\]

_Proof_ This is a special case of Proposition B.56. \(\square\)

The application of Proposition 12.5.1 is 

**Corollary 12.5.2**: \[(X^{\prime}_{[i]}X_{[i]})^{-1}=(X^{\prime}X)^{-1}+[(X^{\prime}X)^{-1}x_{i}x^{ \prime}_{i}(X^{\prime}X)^{-1}]\big{/}[1-x^{\prime}_{i}(X^{\prime}X)^{-1}x_{i}].\]

_Proof_ The corollary follows from noticing that \(X^{\prime}_{[i]}X_{[i]}=(X^{\prime}X-x_{i}x^{\prime}_{i})\). \(\square\)

**Proposition 12.5.3**: \[\hat{\beta}_{[i]}=\hat{\beta}-[(X^{\prime}X)^{-1}x_{i}\hat{e}_{i}]\big{/}(1-m_ {ii}).\]

_Proof_ First, note that \(x^{\prime}_{i}(X^{\prime}X)^{-1}x_{i}=m_{ii}\) and \(X^{\prime}_{[i]}Y_{[i]}=X^{\prime}Y-x_{i}y_{i}\). Now, from Corollary 12.5.2,

\[\hat{\beta}_{[i]} = (X^{\prime}_{[i]}X_{[i]})^{-1}X^{\prime}_{[i]}Y_{[i]}\] \[= (X^{\prime}_{[i]}X_{[i]})^{-1}(X^{\prime}Y-x_{i}y_{i})\] \[= \hat{\beta}-(X^{\prime}X)^{-1}x_{i}y_{i}+\Big{[}(X^{\prime}X)^{-1 }x_{i}x^{\prime}_{i}\hat{\beta}-(X^{\prime}X)^{-1}x_{i}x^{\prime}_{i}(X^{\prime }X)^{-1}x_{i}y_{i}\Big{]}\big{/}(1-m_{ii}).\]

Writing \((X^{\prime}X)^{-1}x_{i}y_{i}\) as \((X^{\prime}X)^{-1}x_{i}y_{i}/(1-m_{ii})-m_{ii}(X^{\prime}X)^{-1}x_{i}y_{i}/(1- m_{ii})\), it is easily seen that

\[\hat{\beta}_{[i]} = \hat{\beta}-[(X^{\prime}X)^{-1}x_{i}(y_{i}-x^{\prime}_{i}\hat{ \beta})]/(1-m_{ii})\] \[+\big{[}m_{ii}(X^{\prime}X)^{-1}x_{i}y_{i}-(X^{\prime}X)^{-1}x_{i }m_{ii}y_{i}\big{]}/(1-m_{ii})\] \[= \hat{\beta}-[(X^{\prime}X)^{-1}x_{i}\hat{e}_{i}]/(1-m_{ii}).\] \(\square\)

The predicted residuals can now be written in a simple way.

**Proposition 12.5.4**: \[\hat{e}_{[i]}=\hat{e}_{i}/(1-m_{ii}).\]

_(b)_ \(\operatorname{Var}(\hat{e}_{[i]})=\sigma^{2}/(1-m_{ii}).\)__

_Proof_

(a) \[\hat{e}_{[i]} = y_{i}-x^{\prime}_{i}\hat{\beta}_{[i]}\] \[= y_{i}-x^{\prime}_{i}\bigg{[}\hat{\beta}-\frac{(X^{\prime}X)^{-1}x _{i}\hat{e}_{i}}{1-m_{ii}}\bigg{]}\] \[= \hat{e}_{i}+m_{ii}\hat{e}_{i}/(1-m_{ii})\] \[= \hat{e}_{i}/(1-m_{ii}).\] (b) This follows from having \(\hat{e}_{[i]}=\hat{e}_{i}/(1-m_{ii})\) and \(\operatorname{Var}(\hat{e}_{i})=\sigma^{2}(1-m_{ii})\). \(\square\)The PRESS statistic can now be written as

\[\text{PRESS}=\sum_{i=1}^{n}\hat{e}_{i}^{2}\big{/}(1-m_{ii})^{2}.\]

The value of \(\hat{e}_{i}^{2}\big{/}(1-m_{ii})^{2}\) will usually be large when \(m_{ii}\) is near 1. Model selection with PRESS puts a premium on having models in which observations with extremely high leverage are fitted very well. As will be discussed in Chapter 14, when fitting a model after going through a procedure to select a good model, the fitted model tends to be very optimistic in the sense of indicating much less variability than is appropriate. Model selection using the PRESS statistic tends to continue that phenomenon, cf. Picard and Cook (1984) and Picard and Berk (1990).

Later, we will also need the sum of squares for error with the \(i\)th case deleted, say \(SSE_{[i]}\).

**Proposition 12.5.5**: \(SSE_{[i]}=SSE-\hat{e}_{i}^{2}\big{/}(1-m_{ii}).\)__

_Proof_ By definition,

\[\begin{array}{rl}SSE_{[i]}&=\,Y_{[i]}^{\prime}Y_{[i]}-Y_{[i]}^{\prime}X_{[i] }(X_{[i]}^{\prime}X_{[i]})^{-1}X_{[i]}^{\prime}Y_{[i]}\\ &=\,(Y^{\prime}Y-y_{i}^{2})-Y_{[i]}^{\prime}X_{[i]}\hat{\beta}_{[i]}.\end{array}\]

The second term can be written

\[\begin{array}{rl}Y_{[i]}^{\prime}X_{[i]}\hat{\beta}_{[i]}&=\,(Y^{\prime}X-y_{i }x_{i}^{\prime})\left\{\hat{\beta}-[(X^{\prime}X)^{-1}x_{i}\hat{e}_{i}]\big{/} (1-m_{ii})\right\}\\ &=\,Y^{\prime}X\hat{\beta}-y_{i}x_{i}^{\prime}\hat{\beta}-x_{i}^{\prime}\hat{ \beta}\hat{e}_{i}\big{/}(1-m_{ii})+y_{i}m_{ii}\hat{e}_{i}\big{/}(1-m_{ii})\\ &=\,Y^{\prime}MY-y_{i}x_{i}^{\prime}\hat{\beta}+y_{i}\hat{e}_{i}\big{/}(1-m_{ ii})-x_{i}^{\prime}\hat{\beta}\hat{e}_{i}\big{/}(1-m_{ii})\\ &\quad-y_{i}\hat{e}_{i}\big{/}(1-m_{ii})+y_{i}m_{ii}\hat{e}_{i}\big{/}(1-m_{ ii})\\ &=\,Y^{\prime}MY-y_{i}x_{i}^{\prime}\hat{\beta}+\hat{e}_{i}^{2}\big{/}(1-m_{ ii})-y_{i}\hat{e}_{i}\\ &=\,Y^{\prime}MY+\hat{e}_{i}^{2}\big{/}(1-m_{ii})-y_{i}^{2}.\end{array}\]

Therefore,

\[\begin{array}{rl}SSE_{[i]}&=\,Y^{\prime}Y-y_{i}^{2}-[Y^{\prime}MY+\hat{e}_{i }^{2}\big{/}(1-m_{ii})-y_{i}^{2}]\\ &=\,Y^{\prime}(I-M)Y-\hat{e}_{i}^{2}\big{/}(1-m_{ii})\\ &=\,SSE-\hat{e}_{i}^{2}\big{/}(1-m_{ii}).\end{array}\]

**Exercise 12.6c**: Show that the standardized predicted residuals with \(\sigma^{2}\) estimated by \(MSE\) are the same as the standardized residuals.

### Outliers and Influential Observations

Realistically, the purpose of fitting a linear model is to get a (relatively) succinct summary of the important features of the data. Rarely is the chosen linear model really correct. Usually, the linear model is no more than a rough approximation to reality.

Outliers are cases that do not seem to fit the chosen linear model. There are two kinds of outliers. Outliers may occur because the predictor variables for the case are unlike the predictor variables for the other cases. These are cases with high leverage. If we think of the linear model as being an approximation to reality, the approximation may be quite good within a certain range of the predictor variables, but poor outside that range. A few cases that fall outside the range of good approximation can greatly distort the fitted model and lead to a bad fit, even on the range of good approximation. Outliers of this kind are referred to as _outliers in the model space (or design or estimation space)_.

The other kind of outliers are those due to bizarre values of the dependent variable. These may occur because of gross measurement error, or from recording the data incorrectly. Not infrequently, data are generated from a mixture process. In other words, the data fit one pattern most of the time, but occasionally data with a different pattern are generated. Often it is appropriate to identify the different kinds of data and model them separately. If the vast majority of data fit a common pattern, there may not be enough of the rare observations for a complete analysis; but it is still important to identify such observations. In fact, these rare observations can be more important than all of the other data.

Not only is it important to be able to identify outliers, but it must also be decided whether such observations should be included when fitting the model. If they are left out, one gets an approximation to what usually happens, and one must be aware that something unusual will happen every so often.

Outliers in the design space are identified by their leverages, the \(m_{ii}\)s. Bizarre values of \(y_{i}\) can often be identified by their standardized residuals. Large standardized residuals indicate outliers. Typically, these are easily spotted in residual plots. However, if a case with an unusual \(y_{i}\) also has high leverage, the standardized residual may not be large. If there is only one bizarre value of \(y_{i}\), it should be easy to identify by examining all the cases with either a large standardized residual or high leverage. With more than one bizarre value, they _may_ mask each other. (I believe that careful examination of the leverages will almost always identify possible masking, cf. the comments in Section 1 on gaps in the leverage values.)

An alternative to examining the standardized residuals is to examine the _standardized predicted residuals_, also known as the _standardized deleted residuals_, the \(t\)_residuals_, and sometimes (as in the R programming language) the _Studentized residuals_. The standardized predicted residuals are

\[t_{i}\equiv\frac{\hat{e}_{[i]}}{\sqrt{MSE_{[i]}/(1-m_{ii})}}=\frac{y_{i}-x_{i}^ {\prime}\hat{\beta}_{[i]}}{\sqrt{MSE_{[i]}/(1-m_{ii})}}.\]Since \(y_{i}\), \(\hat{\beta}_{[i]}\), and \(MSE_{[i]}\) are independent,

\[t_{i}\sim t(n-p-1),\]

where \(p=r(X)\). This allows a formal \(t\) test for whether the value \(y_{i}\) is consistent with the rest of the data. Actually, this procedure is equivalent to examining the standardized residuals, but using the \(t(n-p-1)\) distribution is more convenient than using the appropriate distribution for the \(r_{i}\)s, cf. Cook and Weisberg (1982).

When all the values \(t_{i}\), \(i=1,\ldots,n\), are computed, the large values will naturally be singled out for testing. The appropriate test statistic is actually \(\max_{i}|t_{i}|\). The null distribution of this statistic is quite different from a \(t(n-p-1)\). Fortunately, Bonferroni's inequality provides an appropriate, actually a conservative, \(P\) value by multiplying the \(P\) value from a \(t(n-p-1)\) distribution by \(n\). Alternatively, for an \(\alpha\) level test, use a critical value of \(t(1-\alpha/2n,n-p-1)\).

If \(y_{i}\) corresponds to a case with extremely high leverage, the standard error for the predicted residual, \(\sqrt{MSE_{[i]}/(1-m_{ii})}\), will be large, and it will be difficult to reject the \(t\) test. Recall from Example 12.1.6 that under condition (c) the \(y\) value for case six is clearly discordant. Although the absolute \(t\) value is quite large, \(t_{6}=-5.86\) with \(m_{66}=0.936\), it is smaller than one might expect, considering the obvious discordance of case six. In particular, the absolute \(t\) value is smaller than the critical point for the Bonferroni method with \(\alpha=0.05\). (The critical point is \(t(1-0.025/6,3)=6.23\).) Of course, with three degrees of freedom, the power of this test is very small. A larger \(\alpha\) level would probably be more appropriate. Using the Bonferroni method with \(\alpha=0.10\) leads to rejection.

The updating formula for \(t_{i}\) is

**Proposition 12.6.1**: \[t_{i}=r_{i}\sqrt{\frac{n-p-1}{n-p-r_{i}^{2}}}.\]

_Proof_ Using the updating formulae of Section 5,

\[t_{i} = \hat{\varepsilon}_{[i]}\Big{/}\sqrt{MSE_{[i]}/(1-m_{ii})}\] \[= \hat{\varepsilon}_{[i]}\sqrt{(1-m_{ii})}\Big{/}\sqrt{MSE_{[i]}}\] \[= r_{i}\sqrt{MSE}\Big{/}\sqrt{MSE_{[i]}}\] \[= r_{i}\sqrt{(n-p-1)/(n-p)}\sqrt{SSE/SSE_{[i]}}\] \[= r_{i}\sqrt{(n-p-1)/(n-p)}\sqrt{SSE/[SSE-\hat{\varepsilon}_{i}^{ 2}/(1-m_{ii})]}\] \[= r_{i}\sqrt{(n-p-1)/(n-p)}\sqrt{1/[1-r_{i}^{2}/(n-p)]}\] \[= r_{i}\sqrt{(n-p-1)/(n-p-r_{i}^{2})}.\]As indicated earlier, \(t_{i}\) really contains the same information as \(r_{i}\).

A test that a given set of \(y\) values does not fit the model is easily available from general linear model theory. Suppose that the \(r\) observations \(i=n-r+1,\ldots,n\) are suspected of being outliers. The model \(Y=X\beta+e\) can be written with

\[Y=\begin{bmatrix}Y_{0}\\ Y_{1}\end{bmatrix},\quad X=\begin{bmatrix}X_{0}\\ X_{1}\end{bmatrix},\quad e=\begin{bmatrix}e_{0}\\ e_{1}\end{bmatrix},\]

where \(Y_{1}\), \(X_{1}\), and \(e_{1}\) each have \(r\) rows. If \(Z=\begin{bmatrix}0\\ I_{r}\end{bmatrix}\), then the model with the possible outliers deleted

\[Y_{0}=X_{0}\beta+e_{0} \tag{12.1}\]

and the model

\[Y=X\beta+Z\gamma+e \tag{12.2}\]

are equivalent for estimating \(\beta\) and \(\sigma^{2}\). A test of the reduced model \(Y=X\beta+e\) against the full model \(Y=X\beta+Z\gamma+e\) is rejected if

\[\frac{(SSE-SSE_{0})/r}{MSE_{0}}>F(1-\alpha,r,n-p-r).\]

If the test is rejected, the \(r\) observations appear to contain outliers. Note that this procedure is essentially the same as Utts's Rainbow Test for lack of fit discussed in Section 6.7. The difference is in how one identifies the cases to be eliminated.

In my opinion, the two most valuable tools for identifying outliers are the \(m_{ii}\)s and the \(t_{i}\)s. It would be unusual to have outliers in the design space without large values of the \(m_{ii}\)s. Such outliers would have to be "far" from the other data without the Mahalanobis distance being large. For bizarre values of the \(y_{i}\)s, it is useful to determine whether the value is so bizarre that it could not reasonably come from the model under consideration. The \(t_{i}\)s provide a test of exactly what is needed.

Cook (1977) presented a distance measure that combines the standardized residual with the leverage to get a single measure of the influence a case has on the fit of the regression model. Cook's distance (\(C_{i}\)) measures the statistical distance between \(\hat{\beta}\) and \(\hat{\beta}_{[i]}\). It is defined as

\[C_{i}\equiv\frac{(\hat{\beta}_{[i]}-\hat{\beta})^{\prime}(X^{\prime}X)(\hat{ \beta}_{[i]}-\hat{\beta})}{pMSE}.\]

Written as a function of the standardized residual and the leverage, Cook's distance is:

**Proposition 12.6.2**: \(C_{i}=r_{i}^{2}[m_{ii}/p(1-m_{ii})]\)

**Exercise 12.7**  Prove Proposition 12.6.2.

From Proposition 12.6.2 it can be seen that Cook's distance takes the size of the standardized residual and rescales it based on the leverage of the case. For an extremely high leverage case, the squared standardized residual gets multiplied by a very large number. For low leverage cases the multiplier is very small. Another interpretation of Cook's distance is that it is a standardized version of how far the predicted values \(X\hat{\beta}\) move when the \(i\)th case is deleted.

In Section 1, after establishing that the \(m_{ii}\)s were a reasonable measure of leverage, it was necessary to find guidelines for what particular values of \(m_{ii}\) meant. This can also be done for Cook's distance. Cook's distance can be calibrated in terms of confidence regions. Recall that a \((1-\alpha)100\%\) confidence region for \(\beta\) is

\[\left\{\beta\Big{|}\frac{(\beta-\hat{\beta})^{\prime}(X^{\prime}X)(\beta-\hat{ \beta})}{pMSE}<F(1-\alpha,\,p,\,n-p)\right\}.\]

If \(C_{i}\doteq F(0.75,\,p,\,n-p)\), then deleting the \(i\)th case moves the estimate of \(\beta\) to the edge of a \(75\%\) confidence region for \(\beta\) based on \(\hat{\beta}\). This is a very substantial move. Since \(F(0.5,\,p,\,n-p)\doteq 1\), any case with \(C_{i}>1\) probably has above average influence. Note that \(C_{i}\) does not actually have an \(F\) distribution. While many people consider such calibrations a necessity, other people, including the author, prefer simply to examine those cases with distances that are substantially larger than the other distances.

Cook's distance can be modified in an obvious way to measure the influence of any set of observations. Cook and Weisberg (1982) give a more detailed discussion of all of the topics in this section.

Updating formulae and case deletion diagnostics for linear models with general covariance matrices are discussed by Christensen, Johnson, and Pearson (1992, 1993), Christensen, Pearson, and Johnson (1992), and by Haslett and Hayes (1998) and Martin (1992). A nice review of these procedures is given by Shi and Chen (2009).

Haslett (1999) establishes two results that can greatly simplify case deletion diagnostics for correlated data. First, he shows that an analysis based on \(Y_{[i]}\), \(X_{[i]}\), and \(V_{[ii]}\) (the covariance matrix with the \(i\)th row and column removed) is the same as an analysis based on \(\tilde{Y}(i)\), \(X\), and \(V\) where

\[\tilde{Y}(i)=\left[\begin{array}{c}\hat{y}_{i}(Y_{[i]})\\ Y_{[i]}\end{array}\right]\]

and \(\hat{y}_{i}(Y_{[i]})\) is the BLUP of \(y_{i}\) based on \(Y_{[i\,\downarrow}\). Second, he shows that there is a relatively simple way to find a matrix \(P_{i}\) such that \(\tilde{Y}(i)=P_{i}Y\).

### Transformations

If the residuals suggest nonnormality, heteroscedasticity of variances, or lack of fit, a transformation of the \(y_{i}\)s may alleviate the problem. Cook and Weisberg (1982) and Atkinson (1985) give extensive discussions of transformations. Only a brief review is presented here.

Picking a transformation is often a matter of trial and error. Different transformations are tried until one is found for which the residuals seem reasonable. Three more systematic methods of choosing transformations will be discussed: _Box-Cox power transformations_, _variance stabilizing transformations_, and the generalized least squares approach of Grizzle et al. (1969).

Box and Cox (1964) suggested a systematic method of picking transformations. They suggested using the family of transformations

\[y^{(\lambda)}=\begin{cases}(y^{\lambda}-1)/\lambda,&\lambda\neq 0\\ \log y,&\lambda=0\end{cases}\]

and choosing \(\lambda\) by maximum likelihood. Convenient methods of executing this procedure are discussed in Cook and Weisberg (1982), Weisberg (2014), and Christensen (2015). They and Atkinson (1985) also discuss _constructed variable tests_ for whether a Box-Cox transformation is needed. Constructed variable tests are generalizations of the famous Tukey (1949) one-degree-of-freedom test for nonadditivity.

If the distribution of the \(y_{i}\)s is known, the commonly used _variance stabilizing transformations_ can be tried (cf. Rao 1973, Section 6g, or Christensen 2015). For example,

\begin{tabular}{l l} if \(y_{i}\sim\)Binomial(\(N_{i}\), \(p_{i}\)), & use \(\arcsin(\sqrt{y_{i}}/N_{i})\), \\ if \(y_{i}\sim\)Poisson(\(\lambda_{i}\)), & use \(\sqrt{y_{i}}\), \\ if \(y_{i}\) has \(\sigma_{i}/\)E(\(y_{i}\)) constant, & use \(\log(y_{i})\). \\ \end{tabular}

More generally, \(\arcsin(\sqrt{y_{i}/N_{i}})\) can be tried for any problem where \(y_{i}\) is a count between 0 and \(N_{i}\) or a proportion, \(\sqrt{y_{i}}\) can be used for any problem where \(y_{i}\) is a count, and \(\log(y_{i})\) can be used if \(y_{i}\) is a count or amount.

The transformation \(\log(y_{i})\) is also frequently used because, for a linear model in \(\log(y_{i})\), the additive effects of the predictor variables transform to multiplicative effects on the original scale. If multiplicative effects seem reasonable, the log transformation may be appropriate.

As an alternative to the variance stabilizing transformations, there exist generalized linear models specifically designed for treating binomial and Poisson data. For Poisson data there exists a well developed theory for fitting log-linear models. One branch of the theory of log-linear models is the theory of logistic regression, which is used to analyze binomial data. As shown by Grizzle et al. (1969), generalized least squares methods can be used to fit log-linear models to Poisson data and logistic regression models to binomial data. The method involves both a transformation of the dependent variable and weights. The appropriate transformation and weights are:\[\begin{array}{llll}\text{Distribution of }y_{i}&\text{Transformation}&\text{Weights}\\ \hline\text{Poisson}(\lambda_{i})&\log y_{i}&y_{i}\\ \text{Binomial}(N_{i},\ p_{i})&\log(y_{i}/[N_{i}-y_{i}])&y_{i}(N_{i}-y_{i})/N_{i} \end{array}\]

With these weights, the asymptotic variance of the transformed data is 1. Standard errors for regression coefficients are computed as usual except that no estimate of \(\sigma^{2}\) is required (\(\sigma^{2}\) is known to be 1). Since \(\sigma^{2}\) is known, \(t\) tests and \(F\) tests are replaced with normal tests and chi-squared tests. In particular, if the linear model fits the data and the observations \(y_{i}\) are large, the _SSE_ has an asymptotic chi-squared distribution with the usual degrees of freedom. If the _SSE_ is too large, a lack of fit is indicated. Tests of various models can be performed by comparing the difference in the _SSE_s for the model. The difference in the _SSE_s has an asymptotic chi-squared distribution with the usual degrees of freedom. If the difference is too large, then the smaller model is deemed inadequate. Unlike the lack-of-fit test, these model comparison tests are typically valid if \(n\) is large even when the individual \(y_{i}\)s are not. Fitting log-linear and logistic models both by generalized least squares and by maximum likelihood is discussed in detail by Christensen (1997).

#### Exercise 12.8

The data given below were first presented by Brownlee (1965) and have subsequently appeared in Daniel and Wood (1980), Draper and Smith (1998), and Andrews (1974), among other places. The data consist of measurements taken on 21 successive days at a plant that oxidizes ammonia into nitric acid. The dependent variable \(y\) is stack loss. It is 10 times the percentage of ammonia that is lost (in the form of unabsorbed nitric oxides) during the oxidation process. There are three predictor variables: \(x_{1}\), \(x_{2}\), and \(x_{3}\). The first predictor variable is air flow into the plant. The second predictor variable is the cooling water temperature as it enters the countercurrent nitric oxide absorption tower. The third predictor variable is a coded version of the nitric acid concentration in the absorbing liquid. Analyze these data giving special emphasis to residual analysis and influential observations.

\[\begin{array}{llllll}\text{Obs.}&x_{1}&x_{2}&x_{3}&y&\text{Obs.}&x_{1}&x_{2}& x_{3}&y\\ \hline 1&80&27&89&42&12&58&17&88&13\\ 2&80&27&88&37&13&58&18&82&11\\ 3&75&25&90&37&14&58&19&93&12\\ 4&62&24&87&28&15&50&18&89&8\\ 5&62&22&87&18&16&50&18&86&7\\ 6&62&23&87&18&17&50&19&72&8\\ 7&62&24&93&19&18&50&19&79&8\\ 8&62&24&93&20&19&50&20&80&9\\ 9&58&23&87&15&20&56&20&82&15\\ 10&58&18&80&14&21&70&20&91&15\\ 11&58&18&89&14&&&&\\ \end{array}\]

#### Exercise 12.9

For testing whether one observation \(y_{i}\) is an outlier, show that the \(F\) statistic is equal to the squared standardized predicted residual.

## References

* [AndrewsAndrews1974] Andrews, D. F. (1974). A robust method for multiple regression. _Technometrics_, _16_, 523-531.
* [ArnoldArnold1981] Arnold, S. F. (1981). _The theory of linear models and multivariate analysis_. New York: Wiley.
* [AtkinsonAtkinson1981] Atkinson, A. C. (1981). Two graphical displays for outlying and influential observations in regression. _Biometrika_, _68_, 13-20.
* [AtkinsonAtkinson1982] Atkinson, A. C. (1982). Regression diagnostics, transformations and constructed variables (with discussion). _Journal of the Royal Statistical Society, Series B_, _44_, 1-36.
* [AtkinsonAtkinson1985] Atkinson, A. C. (1985). _Plots, transformations, and regression: An introduction to graphical methods of diagnostic regression analysis_. Oxford: Oxford University Press.
* [BlomBlom1958] Blom, G. (1958). _Statistical estimates and transformed beta variates_. New York: Wiley.
* [BoxBox1953] Box, G. E. P. (1953). Non-normality and tests on variances. _Biometrika_, _40_, 318-335.
* [BoxBoxBox and CoxBox1964] Box, G. E. P., & Cox, D. R. (1964). An analysis of transformations. _Journal of the Royal Statistical Society, Series B_, _26_, 211-246.
* [BrownleeBrownlee1965] Brownlee, K. A. (1965). _Statistical theory and methodology in science and engineering_ (2nd ed.). New York: Wiley.
* [ChristensenChristensen1989] Christensen, R. (1989). Lack of fit tests based on near or exact replicates. _The Annals of Statistics_, _17_, 673-683.
* [ChristensenChristensen1996] Christensen, R. (1996). _Analysis of variance, design, and regression: Applied statistical methods_. London: Chapman and Hall.
* [ChristensenChristensen1997] Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* [ChristensenChristensen2001] Christensen, R. (2001). _Advanced linear modeling: Multivariate, time series, and spatial data; nonparametric regression, and response surface maximization_ (2nd ed.). New York: Springer.
* [ChristensenChristensen2015] Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* [Christensen and BedrickChristensen1997] Christensen, R., Bedrick, E. J. (1997). Testing the independence assumption in linear models. _Journal of the American Statistical Association_, _92_, 1006-1016.
* [ChristensenJohnson and PearsonChristensen1992] Christensen, R., Johnson, W., & Pearson, L. M. (1992). Prediction diagnostics for spatial linear models. _Biometrika_, _79_, 583-591.
* [ChristensenJohnson and PearsonChristensen1993] Christensen, R., Johnson, W., & Pearson, L. M. (1993). Covariance function diagnostics for spatial linear models. _Mathematical Geology_, _25_, 145-160.
* [ChristensenJohnson, Branscum and HansonChristensen2010] Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton: Chapman and Hall/CRC Press.
* [ChristensenPearson and JohnsonChristensen1992] Christensen, R., Pearson, L. M., & Johnson, W. (1992). Case deletion diagnostics for mixed models. _Technometrics_, _34_, 38-45.
* [CookCook1977] Cook, R. D. (1977). Detection of influential observations in linear regression. _Technometrics_, _19_, 15-18.
* [CookCook1998] Cook, R. D. (1998). _Regression graphics: Ideas for studying regressions through graphics_. New York: Wiley.
* [Cook and WeisbergCook and Weisberg1982] Cook, R. D., & Weisberg, S. (1982). _Residuals and influence in regression_. New York: Chapman and Hall.
* [Cook and WeisbergCook and Weisberg1994] Cook, R. D., & Weisberg, S. (1994). _An introduction to regression graphics_. New York: Wiley.
* [Cook and WeisbergCook and Weisberg1999] Cook, R. D., & Weisberg, S. (1999). _Applied regression including computing and graphics_. New York: Wiley.
* [DanielDaniel1959] Daniel, C. (1959). Use of half-normal plots in interpreting factorial two-level experiments. _Technometrics_, \(1\), 311-341.
* [DanielDaniel1976] Daniel, C. (1976). _Applications of statistics to industrial experimentation_. New York: Wiley.
* [DanielWood1980] Daniel, C., & Wood, F. S. (1980). _Fitting equations to data_ (2nd ed.). New York: Wiley.
* [Draper and SmithDraper and Smith1998] Draper, N., & Smith, H. (1998). _Applied regression analysis_ (3rd ed.). New York: Wiley.
* [DuanDuan1981] Duan, N. (1981). Consistency of residual distribution functions. Working Draft No. 801-1-HHS (106B-80010), Rand Corporation, Santa Monica, CA.
* [Duan and SmithDuan and Smith2001]* [Durbin WatsonDurbin and Watson1951] Durbin, J. Watson, G. S. 1951. Testing for serial correlation in least squares regression II. Biometrika38159-179.
* [Freedman2006] Freedman, D. A. 2006. On the so-called "Huber sandwich estimator" and "robust standard errors". The American Statistician60299-302.
* [Grizzle et al.1969] Grizzle, J. E., Starmer, C. F., Koch, G. G. 1969. Analysis of categorical data by linear models. Biometrics25489-504.
* [Haslett Haslett1999] Haslett, J. 1999. A simple derivation of deletion diagnostic results for the general linear model with correlated errors. Journal of the Royal Statistical Society, Series B61603-609.
* [Haslett HayesHaslett Hayes1998] Haslett, J. Hayes, K. 1998. Residuals for the linear model with general covariance structure. Journal of the Royal Statistical Society, Series B60201-215.
* [Lenthenth2015] Lenth, R. V. 2015. The case against normal plots of effects (with discussion). Journal of Quality Technology4791-97.
* [MandanskyMandansky1988] Mandansky, A. 1988. Prescriptions for working statisticians. New York: Springer.
* Theory and Methods211183-1212.
* [Picard Berk and Berk1990] Picard, R. R. Berk, K. N. 1990. Data splitting. The American Statistician44140-147.
* [Picard CookPicard Cook1984] Picard, R. R. Cook, R. D. 1984. Cross-validation of regression models. Journal of the American Statistical Association79575-583.
* [Rao Rao1973] Rao, C. R. 1973. Linear statistical inference and its applications (2nd ed.). New York: Wiley.
* [SearleSearle1988] Searle, S. R. 1988. Parallel lines in residual plots. The American Statistician42211.
* [Shapiro and FranciaShapiro and Francia1972] Shapiro, S. S. Francia, R. S. 1972. An approximate analysis of variance test for normality. Journal of the American Statistical Association67215-216.
* [Shapiro and WilkShapiro and Wilk1965] Shapiro, S. S. Wilk, M. B. 1965. An analysis of variance test for normality (complete samples). Biometrika52591-611.
* [Shewhart1931] Shewhart, W. A. 1931. Economic control of quality. New York: Van Nostrand.
* [Shewhart1939] Shewhart, W. A. 1939. Statistical method from the viewpoint of quality control. Graduate School of the Department of Agriculture, Washington. Reprint (1986), Dover, New York.
* [Shi ChenShi Chen2009] Shi, L. Chen, G. 2009. Influence measures for general linear models with correlated errors. The American Statistician6340-42.
* [Stefanski.1997] Stefanski, L. A. 2007. Residual (sur)realism. The American Statistician61163-177.
* [Tukey Tukey1949] Tukey, J. W. 1949. One degree of freedom for nonadditivity. Biometrics5232-242.
* [WeisbergWeisberg2014] Weisberg, S. 2014. Applied linear regression (4th ed.). New York: Wiley.

## Chapter 13 Collinearity and Alternative Estimates

### 13 Collinearity and Alternative Estimates

This chapter deals with problems caused by having predictor variables that are very nearly redundant. It examines estimation methods developed for dealing with those problems and then goes on to introduce a variety of alternatives to least squares estimation including robust and penalized (regularized) estimates. Penalized estimation is discussed in more detail in _ALM-III_.

_Collinearity_ or multicollinearity refers to the problem in regression analysis of the columns of the model matrix being nearly linear dependent. Ideally, this is no problem at all. There are numerical difficulties associated with the actual computations, but there are no theoretical difficulties. If, however, one has any doubts about the accuracy of the model matrix, the analysis could be in deep trouble.

Section 1 discusses what collinearity is and what problems it can cause. Section 2 defines tolerance and variance inflation factors and relates them to our definition of collinearity. Sections 3 and 4 introduce four techniques for dealing with collinearity. These are regression in canonical form, principal component regression, generalized inverse regression, and classical ridge regression. The methods, other than classical ridge regression, are essentially the same. Classical ridge regression is a version of penalized least squares estimation, something that is discussed in more detail in _ALM-III_ Chapter 2. Section 5 presents additional comments on the potential benefits of biased estimation. These results are specifically for regression in canonical form. Finally, Sections 6 and 7 present alternatives to least squares estimation. Least squares minimizes the vertical Euclidean distances between the dependent variable and the regression surface. Section 6 considers alternative methods of measuring that vertical distance. Section 7 considers minimizing the Euclidean perpendicular distance between the dependent variable and the regression surface.

The estimation methods presented in this chapter and the next typically result in biased estimates, cf. Section 2.9. When using biased estimates it might be reasonable to continue to use the original least squares residuals for estimating variability (provided the model has sufficient degrees of freedom for Error). Standard statistical methods, when applied with biased estimates, especially those from the datadriven reduced models discussed in Chapter 14, tend to be overly optimistic in that they underestimate the appropriate variability. Data driven reduced models tend to be chosen precisely because they display small variability! For example, a prediction interval from the full model should be appropriate, if unnecessarily wide. A prediction interval from a data selected reduced model is probably unrealistically narrow, although more accurately centered. The residuals from the original (full) model should provide a more accurate estimate of the variance. A prediction interval computed from the reduced model, except with the _MSE_ and \(dfE\) from the reduced model replaced by their corresponding values from the full model, might retain the advantages of biased estimation for \(\beta\) without the inappropriate downward bias in variability. But this is not a mathematical result! The mathematics becomes immensely complicated when, as in a data driven reduced model, the model matrix itself becomes a function of \(Y\).

If the final (biased) estimate of \(\beta\) is a function of the least squares estimate \(\hat{\beta}\), under multivariate normal errors the final estimate still has to be independent of the least squares residuals. If the final estimate is a linear function of \(\hat{\beta}\), the least squares residuals are uncorrelated with it even without assuming normality. This is true for penalized (regularized) estimates like the ridge regression estimates of Section 4 (see _ALM-III_ Section 2.1). Independence is also true for any estimates of estimable functions from a reduced model because they are functions of the full model least squares estimates, i.e., \(X_{0}\hat{\beta}_{0}=M_{0}Y=M_{0}MY=M_{0}X\hat{\beta}\), but not necessarily for data driven reduced models where \(X_{0}\) is a function of \(Y\).

In this book, the term "mean squared error" (_MSE_) has generally denoted the quantity \(Y^{\prime}(I-M)Y/r(I-M)\). This is a sample quantity, a function of the data. In Chapter 6, when discussing prediction, we needed a theoretical concept of the mean squared error. Fortunately, up until this point we have not needed to discuss both the sample quantity and the theoretical one at the same time. To discuss variable selection methods and techniques of dealing with collinearity, we will need both concepts simultaneously. To reduce confusion, we will refer to \(Y^{\prime}(I-M)Y/r(I-M)\) as the _residual mean square_ (_RMS_) and \(Y^{\prime}(I-M)Y\) as the _residual sum of squares_ (_RSS_). Since \(Y^{\prime}(I-M)Y=[(I-M)Y]^{\prime}[(I-M)Y]\) is the sum of the squared residuals, this is a very natural nomenclature.

### Defining Collinearity

In this section we define the problem of collinearity. The approach taken is to quantify the idea of having columns of the model matrix that are "nearly linearly dependent." The effects of near linear dependencies are examined. The section concludes with establishing the relationship between the definition given here and another commonly used concept of collinearity.

Suppose we have a standard regression model

\[Y=X\beta+e,\quad\text{E}(e)=0,\quad\text{Cov}(e)=\sigma^{2}I,\]where \(Y\) is \(n\times 1\), \(X=[X_{1},\ldots,X_{p}]\) is \(n\times p\), \(\beta\) is \(p\times 1\), and \(r(X)=p\). Frequently, \(X_{1}\equiv J\), which is known without error. The essence of model (1) is that \(\mbox{E}(Y)\in C(X)\), \(\mbox{Cov}(Y)=\sigma^{2}I\). Suppose that the model matrix consists of some predictor variables, say \(x_{1}\), \(x_{2}\),..., \(x_{p}\), that are measured with some small error. (If any variables are measured without error, e.g., \(X_{1}\equiv J\), adjustments to the discussion are needed.) A near linear dependence in the observed model matrix \(X\) could mean a real linear dependence in the underlying model matrix of variables measured without error. Let \(X_{*}\) be the underlying model matrix. If the columns of \(X_{*}\) are linearly dependent, there exists an infinite number of least squares estimates for the true regression coefficients. If \(X\) is nearly linearly dependent, the estimated regression coefficients may not be meaningful and may be highly variable.

The real essence of this particular problem is that \(C(X)\) is too large. Generally, we hope that in some sense, \(C(X)\) is close to \(C(X_{*})\). Regression should work well precisely when this is the case. However, when \(X_{*}\) has linearly dependent columns, \(X\) typically will not. Thus \(r(X)>r(X_{*})\). \(C(X_{*})\) may be close to some proper subspace of \(C(X)\), but \(C(X)\) has extra dimensions. By pure chance, these extra dimensions could be very good at explaining the \(Y\) vector that happened to be observed. In this case, we get an apparently good fit that has no real world significance.

The extra dimensions of \(C(X)\) are due to the existence of vectors \(b\) such that \(X_{*}b=0\) but \(Xb\neq 0\). If the errors in \(X\) are small, then \(Xb\) should be approximately zero. We would like to say that a vector \(w\) in \(C(X)\) is ill-defined if there exists \(b\) such that \(w=Xb\) is approximately zero. The vector \(w\) is approximately the zero vector when its length is near zero. Unfortunately, multiplying \(w\) by a scalar can increase or decrease the length of the vector arbitrarily, while not changing the direction determined within \(C(X)\). To rectify this, we can restrict attention to vectors \(b\) with \(b^{\prime}b=1\) (i.e., the length of \(b\) is 1), or, equivalently, we make the following:

**Definition 13.1.1**  A vector \(w=Xb\) is said to be \(\varepsilon\)_ill-defined_ if \(w^{\prime}w/b^{\prime}b=b^{\prime}X^{\prime}Xb/b^{\prime}b<\varepsilon\). The matrix \(X\) is \(\varepsilon\) ill-defined if any vector in \(C(X)\) is \(\varepsilon\) ill-defined. We use the terms "ill-defined" and "ill-conditioned" interchangeably.

In a model with an intercept, say \(X=[J,Z]\), typically we would be interested in whether the columns of \(Z\) are collinear after correcting for their mean values, i.e., evaluating whether \([I-(1/n)JJ^{\prime}]Z\) is \(\varepsilon\) ill-defined. In a traditional ACOVA with model matrix \([X,Z]\), when \(X\) is not subject to errors we typically would be interested in whether \((I-M)Z\) is \(\varepsilon\) ill-defined. (If individuals randomly fall into groups, rather than being placed into groups, the group indicators in \(X\) may be subject to error.)

The assumption of a real linear dependence in the \(X_{*}\) matrix is a strong one. We now indicate how that assumption can be weakened. Let \(X=X_{*}+\Delta\), where the elements of \(\Delta\) are uniformly small errors. Consider the vector \(Xb\). (For simplicity, assume \(b^{\prime}b=1\).) The corresponding direction in the underlying model matrix is \(X_{*}b\).

Note that \(b^{\prime}X^{\prime}Xb=b^{\prime}X^{\prime}_{*}X_{*}b+2b^{\prime}\Delta^{ \prime}X_{*}b+b^{\prime}\Delta^{\prime}\Delta b\). The vector \(\Delta^{\prime}b\) is short; so if \(Xb\) and \(X_{*}b\) are of reasonable size, they have about the same length. Also\[b^{\prime}X^{\prime}_{*}Xb=b^{\prime}X^{\prime}_{*}X_{*}b+b^{\prime}X^{\prime}_{*} \Delta b, \tag{2}\]

where \(b^{\prime}X^{\prime}_{*}\Delta b\) is small; so the angle between \(Xb\) and \(X_{*}b\) should be near zero. (For any two vectors \(x\) and \(y\), let \(\theta\) be the angle between \(x\) and \(y\). Then \(x^{\prime}y=\sqrt{x^{\prime}x}\sqrt{y^{\prime}y}\cos\theta\).) On the other hand, if \(Xb\) is ill-defined, \(X_{*}b\) will also be small, and the term \(b^{\prime}X^{\prime}_{*}\Delta b\) could be a substantial part of \(b^{\prime}X^{\prime}_{*}Xb\). Thus, the angle between \(Xb\) and \(X_{*}b\) could be substantial. In that case, the use of the direction \(Xb\) is called into question because it may be substantially different from the underlying direction \(X_{*}b\). In practice, we generally cannot know if the angle between \(Xb\) and \(X_{*}b\) is large. Considerable care must be taken when using a direction in \(C(X)\) that is ill-defined.

If \(X_{1}=J\), which has no measurement error, we typically orthogonalize all the other columns of \(X\) to \(J\), i.e., we subtract their means, prior to evaluating collinearity. If the squared length of a (mean adjusted) column of \(X\) is less than \(\varepsilon\), that direction will be \(\varepsilon\) ill-defined, regardless of what other vectors are in \(C(X)\). To avoid this we typically rescale (standardize) the (mean adjusted) columns of \(X\) to have a constant length before evaluating collinearity. I think that one should not be dogmatic about the issue of whether the columns of \(X\) should be adjusted for their mean values or rescaled but see Belsley (1984) along with its excellent discussion papers and, more recently, Velilla (2018) and Christensen (2018).

The intercept term is frequently handled separately from all other variables in techniques for dealing with collinearity. The model

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{p-1}x_{i\;p-1}+e_{i} \tag{3}\]

\[Y=[J,Z]\begin{bmatrix}\beta_{0}\\ \beta_{*}\end{bmatrix}+e\]

is often rewritten as

\[y_{i}=\alpha+\beta_{1}(x_{i1}-\bar{x}_{\cdot 1})+\ldots+\beta_{p-1}(x_{i\;p-1}- \bar{x}_{\cdot p-1})+e_{i}, \tag{4}\]

or

\[Y=\begin{bmatrix}J,\left(I-\frac{1}{n}J_{n}^{n}\right)Z\end{bmatrix}\begin{bmatrix}\alpha\\ \beta_{*}\end{bmatrix}+e,\]

where \(\bar{x}_{\cdot j}=n^{-1}\sum_{i=1}^{n}x_{ij}\). It is easily seen that \(\beta_{*}\) is the same in both models, but \(\beta_{0}\neq\alpha\). We dispense with the concept of the underlying model matrix and define

\[X_{*}\equiv\left(I-\frac{1}{n}J_{n}^{n}\right)Z.\]

Because of orthogonality, \(\hat{\alpha}=\bar{y}\). and \(\beta_{*}\) can be estimated from the model

\[Y_{*}=X_{*}\beta_{*}+e, \tag{5}\]

where \(Y_{*}^{\prime}\equiv[y_{1}-\bar{y}_{\cdot},y_{2}-\bar{y}_{\cdot},\ldots,y_{n}-\bar{y}_{\cdot}]\).

Frequently, the scales of the \(x\) variables are also standardized. Let \(q_{j}^{2}\equiv\sum_{i=1}^{n}(x_{ij}-\bar{x}_{\cdot j})^{2}\). Model (5) is equivalent to

\[Y_{*}=X_{*}\mbox{Diag}(q_{j}^{-1})\gamma_{*}+e, \tag{6}\]

where \(\gamma_{*}=\mbox{Diag}(q_{j})\beta_{*}\). In model (6), the model matrix is \(X_{*}\mbox{Diag}(q_{j}^{-1})\).

Typically in techniques for dealing with collinearity, model (6) is assumed. Sometimes model (5). Rarely model (3). To retain full generality, our discussion uses model (3), but all the results apply to models (5) and (6). Note that the matrix \(X_{*}^{\prime}X_{*}\) is proportional to the sample covariance matrix of the \(X\)s when \((x_{i1},\ldots,x_{i\ p-1})\), \(i=1,\ldots,n\), is thought of as a sample of size \(n\) from a \(p-1\) dimensional random vector. \(\mbox{Diag}(q_{j}^{-1})X_{*}^{\prime}X_{*}\mbox{Diag}(q_{j}^{-1})\) is the sample correlation matrix.

We now present the relationship between \(\varepsilon\) ill-defined matrices and other commonly used methods of identifying ill-conditioned matrices.

One of the main tools in the examination of collinearity is the examination of the eigenvalues of \(X^{\prime}X\). We discuss the relationship between Definition 13.1.1 and an eigen-analysis of \(X^{\prime}X\).

Recall that \(X\) has linearly dependent columns if and only if \(X^{\prime}X\) is singular, which happens if and only if \(X^{\prime}X\) has a zero eigenvalue. One often-used (but I think unintuitive) definition is that the columns of \(X\) are nearly linearly dependent if \(X^{\prime}X\) has at least one small eigenvalue. Suppose that \(v_{1}\),..., \(v_{p}\) is an orthogonal set of eigenvectors for \(X^{\prime}X\) corresponding to the eigenvalues \(\delta_{1}\), \(\delta_{2}\),..., \(\delta_{p}\). Then \(v_{1}\),..., \(v_{p}\) form a basis for \({\bf R}^{p}\). If \(\delta_{i}<\varepsilon\), then \(\delta_{i}=v_{i}^{\prime}X^{\prime}Xv_{i}/v_{i}^{\prime}v_{i}\); so \(Xv_{i}\) is a direction in \(C(X)\) that is \(\varepsilon\) ill-defined. Conversely, if an \(\varepsilon\) ill-defined vector exists, we show that at least one of the \(\delta_{i}\)s must be less than \(\varepsilon\). Let \(w=Xd\) be \(\varepsilon\) ill-defined. Write \(d=\sum_{i=1}^{p}\alpha_{i}v_{i}\). Then \(w^{\prime}w=d^{\prime}X^{\prime}Xd=\sum_{i=1}^{p}\alpha_{i}^{2}\delta_{i}\) and \(d^{\prime}d=\sum_{i=1}^{p}\alpha_{i}^{2}\). Since \(\sum_{i=1}^{p}\alpha_{i}^{2}\delta_{i}/\sum_{i=1}^{p}\alpha_{i}^{2}<\varepsilon\), and since the \(\delta_{i}\)s are all nonnegative, at least one of the \(\delta_{i}\)s must be less than \(\varepsilon\). We have proved:

**Theorem 13.1.2**: _The matrix \(X\) is \(\varepsilon\) ill-defined if and only if \(X^{\prime}X\) has an eigenvalue less than \(\varepsilon\)._

The orthogonal eigenvectors of \(X^{\prime}X\) lead to a useful breakdown of \(C(X)\) into orthogonal components. Let \(\delta_{1}\),..., \(\delta_{r}\) be eigenvalues of at least \(\varepsilon\) and \(\delta_{r+1}\),..., \(\delta_{p}\) eigenvalues less than \(\varepsilon\). It is easily seen that \(Xv_{r+1}\),..., \(Xv_{p}\) form an orthogonal basis for a subspace of \(C(X)\) in which all vectors are \(\varepsilon\) ill-defined. The space spanned by \(Xv_{1}\),..., \(Xv_{r}\) is a subspace in which none of the vectors are \(\varepsilon\) ill-defined. These two spaces are orthogonal complements with respect to \(C(X)\). (Note that by taking a linear combination of a vector in each of the orthogonal subspaces one can get a vector that is \(\varepsilon\) ill-defined, but is in neither subspace.) As discussed later, the vectors \(Xv_{j}\) are the principal component vectors if \(X\) is centered (i.e., \(X=X_{*}\)) or centered and scaled (i.e., \(X=X_{*}D(q_{j}^{-1})\)).

A second commonly used method of identifying ill-conditioned matrices was presented by Belsley et al. (1980). See also Belsley (1991). They use the _conditionnumber_ (_CN_) as the basis of their definition of collinearity. If the columns of \(X\) are rescaled to have length 1, the condition number is

\[CN\equiv\sqrt{\max_{i}\delta_{i}\Big{/}\min_{i}\delta_{i}}.\]

Large values of the condition number indicate that \(X\) is ill-conditioned.

**Theorem 13.1.3**: _(a) If \(CN>\sqrt{p/\varepsilon}\), then \(X\) is \(\varepsilon\) ill-defined._

_(b) If \(X\) is \(\varepsilon\) ill-defined, then \(CN>\sqrt{1/\varepsilon}\)._

_Proof_ See Exercise 13.2. \(\Box\)

If the columns of \(X\) have not been rescaled to have length 1, the \(CN\) is inappropriate. If \(X\) has two orthogonal columns, with one of length \(10^{3}\) and one of length \(10^{-3}\), the condition number is \(10^{6}\). Rescaling would make the columns of \(X\) orthonormal, which is the ideal of noncollinearity. (Note that such an \(X\) matrix is also ill-defined for any \(\varepsilon>10^{-6}\).)

**Exercise 13.1**  Show that any linear combination of ill-defined orthonormal eigenvectors is ill-defined. In particular, if \(w=X(av_{i}+bv_{j})\), then

\[\frac{w^{\prime}w}{(av_{i}+bv_{j})^{\prime}(av_{i}+bv_{j})}\leq\max(\delta_{i },\delta_{j}).\]

**Exercise 13.2**  Prove Theorem 13.1.3.

Hint: For a model matrix with columns of length 1, \(\mbox{tr}(X^{\prime}X)=p\). It follows that \(1\leq\max_{i}\delta_{i}\leq p\).

### Tolerance and Variance Inflation Factors

Regression assumes that the model matrix in \(Y=X\beta+e\) has full rank. Mathematically, either the columns of \(X\) are linearly independent or they are not. In practice, computational difficulties arise if the columns of \(X\) are nearly linearly dependent, e.g., if \(Xb\doteq 0\) for some \(b\) with \(b^{\prime}b=1\). In this section we focus on an equivalent idea, that one column of \(X\) can be nearly reproduced by the other columns, i.e., when Proposition 2.1.6_almost_ holds.

Most computer programs for doing regression analysis fit the predictor variables sequentially. _Tolerance_ is a concept used to determine whether a variable is too collinear with the previous variables to allow its entry into the model because it will present numerical difficulties. The relationship between the concepts of tolerance and ill-defined vectors is complex.

Suppose the model \(Y=X\beta+e\) has an intercept, say,

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{p-1}x_{i\;p-1}+e_{i},\]

and we are considering adding variable \(x_{p}\) to the model. To check the tolerance associated with adding \(x_{p}\), fit

\[x_{ip}=\alpha_{0}+\alpha_{1}x_{i1}+\cdots+\alpha_{p-1}x_{i\;p-1}+e_{i}. \tag{1}\]

If the coefficient of determination \(R^{2}(p)\) from this model is high, the column vectors of \(X\), say \(J\), \(X_{1},\ldots,X_{p-1}\), are nearly linearly dependent with \(X_{p}\equiv[x_{1p},\ldots,x_{np}]^{\prime}\). The tolerance of \(x_{p}\) (relative to \(x_{1}\),..., \(x_{p-1}\)) is defined as the value

\[T_{p}\equiv 1-R^{2}(p)\]

for fitting model (1). If the tolerance is too small, variable \(x_{p}\) is not used. Often, in a computer program, the user can define which values of the tolerance should be considered too small.

More generally, the tolerance of predictor variable \(x_{j}\) is defined as

\[T_{j}\equiv 1-R^{2}(j)\]

where \(R^{2}(j)\) is the coefficient of determination from regressing \(X_{j}\) on an intercept and all of the other variables. \(R^{2}(j)\), being a measure of predictive ability, examines the predictive ability after correcting _all_ variables for their means.

A closely related concept is that of the _variance inflation factor (VIF)_ associated with the regression coefficient \(\gamma_{j}\) in the model

\[y_{i}=\gamma_{0}+\gamma_{1}x_{i1}+\cdots+\gamma_{p}x_{ip}+e_{i}.\]

The variance inflation factor is defined as

\[\mathit{VIF}_{j}\equiv 1/T_{j}.\]

The VIF can be viewed as the variance of \(\hat{\gamma_{j}}\) divided by the variance that \(\hat{\gamma_{j}}\) would have if all the predictors were uncorrelated, which is also the variance of \(\hat{\eta_{j}}\) in the simple linear regression \(y_{i}=\eta_{0}+\eta_{j}x_{ij}+e_{i}\). Specifically, from Chapter 9 and for \(j=p\) (only because we have that notation already set),

\[\mathit{VIF}_{p}=\frac{X_{p}^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)X_{p}} {X_{p}(I-M)X_{p}}=\frac{\mathrm{Var}(\hat{\gamma_{p}})}{\mathrm{Var}(\hat{\eta _{p}})}.\]Recall that \(M\) is the ppo onto \(C(X)=C(J,\,X_{1},\ldots,\,X_{p-1})\).

While the above definition of \(T_{j}\) is usually appropriate, in the discussion of Section 1, where \(X_{1}\) is not assumed to be the intercept, it would be more appropriate to define tolerance as

\[T_{j}=\mbox{sum of squares error}/\mbox{sum of squares total(uncorrected)}\]

for regressing \(X_{j}\) on the other variables. If it is decided to adjust all variables for their means as in model (13.1.5), this becomes the usual definition. _In the following discussion of the relationship between tolerance and \(\varepsilon\) ill-defined, we will use this alternative definition of \(T_{j}\)._

In a more subtle way, \(T_{j}\) also adjusts for the scale of \(X_{j}=[x_{1j},\ldots,\,x_{nj}]^{\prime}\). The scale adjustment comes because \(T_{j}\) is the ratio of two squared lengths. This issue arises again in our later analysis.

Rewrite model (1) as

\[X_{p}=X\alpha+e. \tag{2}\]

The new (orthogonal) dimension added to \(C(X)\) by including the variable \(x_{p}\) in the regression is the vector of residuals from fitting model (2), i.e., \(X_{p}-X\hat{\alpha}\). Our question is whether \(X_{p}-X\hat{\alpha}\) is ill-defined within \(C(X,\,X_{p})\).

By the alternative definition,

\[T_{p} = X^{\prime}_{p}(I-M)X_{p}\Big{/}X^{\prime}_{p}X_{p}\] \[= X^{\prime}_{p}(I-M)X_{p}\Big{/}[X^{\prime}_{p}(I-M)X_{p}+X^{ \prime}_{p}MX_{p}].\]

\(T_{p}\) is small when \(X^{\prime}_{p}(I-M)X_{p}\) is small relative to \(X^{\prime}_{p}MX_{p}\).

The residual vector, \(X_{p}-X\hat{\alpha}\), is \(\varepsilon\) ill-defined in \(C(X,\,X_{p})\) if

\[\varepsilon>\frac{(X_{p}-X\hat{\alpha})^{\prime}(X_{p}-X\hat{\alpha})}{1+\hat{ \alpha}^{\prime}\hat{\alpha}}=\frac{X^{\prime}_{p}(I-M)X_{p}}{1+\hat{\alpha}^{ \prime}\hat{\alpha}}.\]

It is not too difficult to see, cf. Exercise 13.3a, that

\[\delta_{1}\frac{X^{\prime}_{p}(I-M)X_{p}}{\delta_{1}+X^{\prime}_{p}MX_{p}}\leq \frac{X^{\prime}_{p}(I-M)X_{p}}{1+\hat{\alpha}^{\prime}\hat{\alpha}}\leq\delta _{p}\frac{X^{\prime}_{p}(I-M)X_{p}}{\delta_{p}+X^{\prime}_{p}MX_{p}}, \tag{3}\]

where \(\delta_{1}\) and \(\delta_{p}\) are the smallest and largest eigenvalues of \(X^{\prime}X\). Note that for positive \(a\) and \(b,a/(a+b)<\varepsilon\) implies \(a/(\delta_{p}+b)<a/b<\varepsilon/(1-\varepsilon)\); so it follows immediately that if \(T_{p}<\varepsilon\), \(X_{p}-X\hat{\alpha}\) is \(\delta_{p}\varepsilon/(1-\varepsilon)\) ill-defined. On the other hand, if \(X_{p}-X\hat{\alpha}\) is \(\varepsilon\) ill-defined, some algebra shows that

\[T_{p}<\frac{\delta_{1}\varepsilon}{(\delta_{1}+\varepsilon)X^{\prime}_{p}X_{p }}+\frac{\varepsilon}{\delta_{1}+\varepsilon}.\]By picking \(\varepsilon\) small, the tolerance can be made small. This bound depends on the squared length of \(X_{p}\). In practice, however, if \(X_{p}\) is short, \(X_{p}\) may not have small tolerance, but the vector of residuals may be ill-defined. If \(X_{p}\) is standardized so that \(X^{\prime}_{p}X_{p}=1\), this problem is eliminated. Standardizing the columns of \(X\) also ensures that there are some reasonable limits on the values of \(\delta_{1}\) and \(\delta_{p}\). (One would assume in this context that \(X\) is not ill-defined.)

##### Exercise 13.3a

Use a singular value decomposition \(X^{\prime}X=PD(\delta_{i})P^{\prime}\) to show that \(v^{\prime}(X^{\prime}X)^{-1}v/\delta_{p}\leq v^{\prime}(X^{\prime}X)^{-2}v\leq v ^{\prime}(X^{\prime}X)^{-1}v/\delta_{1}\), where \(\delta_{1}\leq\cdots\leq\delta_{p}\) and \(v\) is any vector. Use this to show inequality (3). Hints: The inequalities are reversed if you invert the numbers. Focus on rewriting \(\hat{\alpha}^{\prime}\hat{\alpha}\).

##### Exercise 13.3b

Writing model (2) as \(X_{p}=J\alpha_{0}+Z\alpha_{*}+e\) and using the original definition of tolerance \(T_{p}=X_{p}(I-M)X_{p}/X_{p}[I-(1/n)JJ^{\prime}]X_{p}\), find the relationship between tolerance and having \(X_{p}-X\hat{\alpha}\) be \(\varepsilon\) ill-defined in \(C\{[I-(1/n)JJ^{\prime}](X,X_{p})\}\).

### Regression in Canonical Form and on Principal Components

#### Regression in Canonical Form

Regression in canonical form involves transforming the \(Y\) and \(\beta\) vectors so that the model matrix is particularly nice. Regression in canonical form is closely related to two procedures that have been proposed for dealing with collinearity. To transform the regression problem we need

**Theorem 13.3.1**: **The Singular Value Decomposition.**

_Let \(X\) be an \(n\times p\) matrix with rank \(s\). Then \(X\) can be written as_

\[X=\mathit{ULV},\]

_where \(U\) is \(n\times s\), \(L\) is \(s\times s\), \(V\) is \(p\times s\), and_

\[L\equiv\mathit{Diag}(\lambda_{j}).\]

_The \(\lambda_{j}\)s are the positive square roots of the positive eigenvalues (singular values) of \(X^{\prime}X\) and \(X\!X^{\prime}\) (i.e., \(\lambda_{j}^{2}=\delta_{j}\)). The columns of \(V\) are \(s\) orthonormal eigenvectors of \(X^{\prime}X\) corresponding to the positive eigenvalues with_

\[X^{\prime}X\!V=VL^{2},\]and the columns of \(U\) are \(s\) orthonormal eigenvectors of \(X\!X^{\prime}\) with_

\[X\!X^{\prime}U=U\!L^{2}.\]

_Proof_ We can pick \(L\) and \(V\) as indicated in the theorem. We need to show that we can find \(U\) so that the theorem holds. (Orthonormal eigenvectors of \(X\!X^{\prime}\) are not unique and not just any set of them will do!) If we take \(U=X\!VL^{-1}\), then

\[X\!X^{\prime}U=X\!X^{\prime}X\!VL^{-1}=X\!VL^{2}L^{-1}=X\!VL=X\!VL^{-1}L^{2}=U \!L^{2};\]

so the columns of \(U\) are eigenvectors of \(X\!X^{\prime}\) corresponding to the \(\lambda_{i}^{2}\)s. The columns of \(U\) are orthonormal because the columns of \(V\) are orthonormal and

\[U^{\prime}U=L^{-1}V^{\prime}X^{\prime}X\!VL^{-1}=L^{-1}V^{\prime}VL^{2}L^{-1}=L ^{-1}I_{s}L=I_{s}.\]

Having found \(U\) we need to show that \(X=U\!LV\). Note that

\[U^{\prime}X\!V=U^{\prime}X\!VL^{-1}L=U^{\prime}U\!L=L,\]

thus

\[U\!U^{\prime}X\!V^{\prime}=U\!LV^{\prime}.\]

From Appendix B.2 and Proposition B.51, \(C(U)=C(X)\) and \(C(V)=C(X^{\prime})\); so by Theorem B.35, \(UU^{\prime}\) is the ppo onto \(C(X)\) and \(VV^{\prime}\) is the ppo onto \(C(X^{\prime})\), hence \(X=U\!LV\). \(\square\)

We can now derive the canonical form of a regression problem. Consider the standard linear model

\[Y=X\beta+e,\ \ \ \mathrm{E}(e)=0,\ \ \ \mathrm{Cov}(e)=\sigma^{2}I. \tag{1}\]

Write \(X=U\!LV^{\prime}\) as in Theorem 13.3.1 and write \(U_{*}=[U,U_{1}]\), where the columns of \(U_{*}\) are an orthonormal basis for \(\mathbf{R}^{n}\). Transform model (1) to

\[U^{\prime}_{*}Y=U^{\prime}_{*}X\beta+U^{\prime}_{*}e. \tag{2}\]

Let \(Y_{*}=U^{\prime}_{*}Y\) and \(e_{*}=U^{\prime}_{*}e\). Then

\[\mathrm{E}(e_{*})=0,\ \ \ \mathrm{Cov}(e_{*})=\sigma^{2}U^{\prime}_{*}U_{*}= \sigma^{2}I.\]

Using Theorem 13.3.1 again,

\[U^{\prime}_{*}X\beta=U^{\prime}_{*}U\!LV^{\prime}\beta=\begin{bmatrix}U^{ \prime}\\ U^{\prime}_{1}\end{bmatrix}U\!LV^{\prime}\beta=\begin{bmatrix}L\\ 0\end{bmatrix}V^{\prime}\beta.\]

Reparameterizing by letting \(\gamma\equiv V^{\prime}\beta\) gives the canonical regression model \[Y_{*}=\left[\begin{matrix}L\\ 0\end{matrix}\right]\gamma+e_{*},\quad\text{E}(e_{*})=0,\quad\text{Cov}(e_{*})= \sigma^{2}I. \tag{3}\]

Since this was obtained by a nonsingular transformation of model (1), it contains all of the information in model (1).

Estimation of parameters becomes trivial in this model:

\[\hat{\gamma}=(L^{-1},0)Y_{*}=L^{-1}U^{\prime}Y,\]

\[\text{Cov}(\hat{\gamma})=\sigma^{2}L^{-2},\]

\[RSS=Y_{*}^{\prime}\left[\begin{matrix}0&0\\ 0&I_{n-p}\end{matrix}\right]Y_{*},\]

\[RMS=\sum_{i=s+1}^{n}y_{*i}^{2}\Big{/}(n-p).\]

In particular, the estimate of \(\gamma_{j}\) is \(\hat{\gamma}_{j}=y_{*j}/\lambda_{j}\), and the variance of \(\hat{\gamma}_{j}\) is \(\sigma^{2}/\lambda_{j}^{2}\). The estimates \(\hat{\gamma}_{j}\) and \(\hat{\gamma}_{k}\) have zero covariance if \(j\neq k\).

Models (1) and (3) are also equivalent to the (principal component) model

\[Y=UL\gamma+e, \tag{4}\]

so, writing \(U=[u_{1},\ldots,u_{s}]\) and \(V=[v_{1},\ldots,v_{s}]\), \(\gamma_{j}\) is the coefficient in the direction \(\lambda_{j}u_{j}\). If \(\lambda_{j}\) is small, \(\lambda_{j}u_{j}=Xv_{j}\) is ill-defined, and the variance of \(\hat{\gamma}_{j}\), \(\sigma^{2}/\lambda_{j}^{2}\), is large. Since the variance is large, it will be difficult to reject \(H_{0}:\gamma_{j}=0\). If the data are consistent with \(\gamma_{j}=0\), life is great. We conclude that there is no evidence of an effect in the ill-defined direction \(\lambda_{j}u_{j}\). If \(H_{0}:\gamma_{j}=0\) is rejected, we have to weigh the evidence that the direction \(u_{j}\) is important in explaining \(Y\) against the evidence that the ill-defined direction \(u_{j}\) should not be included in \(C(X)\).

Regression in canonical form can, of course, be applied to models (13.1.5) and (13.1.6), where the predictor variables have been standardized.

#### Exercise 13.3c

Show the following.

(1) \(Y^{\prime}MY=\sum_{i=1}^{s}y_{*i}^{2}\).

(2) \(Y^{\prime}(I-M)Y=\sum_{i=s+1}^{n}y_{*i}^{2}\).

(3) \(\hat{\beta}^{\prime}\hat{\beta}=\sum_{i=1}^{s}y_{*i}^{2}\big{/}\lambda_{i}^{2}\).

(4) If \(\lambda_{1}^{2}\leq\cdots\leq\lambda_{s}^{2}\), then

\[\lambda_{1}^{2}\frac{Y^{\prime}(I-M)Y}{\lambda_{1}^{2}+Y^{\prime}MY}\leq\frac{ Y^{\prime}(I-M)Y}{1+\hat{\beta}^{\prime}\hat{\beta}}\leq\lambda_{s}^{2}\frac{Y^{ \prime}(I-M)Y}{\lambda_{s}^{2}+Y^{\prime}MY}.\]

#### Principal Component Regression

If we take as our original model a standardized version such as (13.1.5) or (13.1.6), the model matrix _UL_ of model (4) has columns that are the principal components of the multivariate data set \((x_{i1},\ldots,x_{i\ p-1})^{\prime}\), \(i=1,\ldots,n\). See Johnson and Wichern (2007) or _ALM-III_, Chapter 14 for a discussion of principal components.

If the direction \(u_{j}\) is ill-defined, we may decide that the direction should not be used for estimation. Not using the direction \(u_{j}\) amounts to setting \(\gamma_{j}=0\) in model (4). If ill-defined directions are not to be used, and if ill-defined is taken to mean that \(\lambda_{j}<\sqrt{\varepsilon}\) for some small value of \(\varepsilon\), then we can take as our estimate of \(\gamma\), \(\tilde{\gamma}=(\tilde{\gamma_{1}},\ldots,\tilde{\gamma_{p}})^{\prime}\), where

\[\tilde{\gamma_{j}}=\begin{cases}\hat{\gamma_{j}},&\text{if }\lambda_{j}\geq\sqrt{\varepsilon}\\ 0,&\text{if }\lambda_{j}<\sqrt{\varepsilon}.\end{cases}\]

As an estimate of \(\beta\) in the original model (1) we can use \(\tilde{\beta}=V\tilde{\gamma}\). This is reasonable in a regression model with \(p=s\) because \(V^{\prime}\beta=\gamma\); so \(V\gamma=VV^{\prime}\beta=\beta\). This procedure for obtaining an estimate of \(\beta\) is referred to as _principal component regression (PCR)_, see also Subsection 13.4.1. Christensen (2015, Section 11.6) contains a numerical example.

Mohammad Hattab and Gabriel Huerta have brought to my attention that principal component regression is also a viable method for fitting models with \(n<p\). In that case it is simpler to use the eigenvalues and vectors of _XX_\({}^{\prime}\) rather than \(X^{\prime}X\).

#### Generalized Inverse Regression

To deal with collinearity, Marquardt (1970) suggested using the estimate

\[\tilde{\beta}\equiv(X^{\prime}X)_{r}^{-}X^{\prime}Y,\]

where

\[(X^{\prime}X)_{r}^{-}\equiv\sum_{j=p-r+1}^{p}v_{j}v_{j}^{\prime}/\lambda_{j}^{ 2},\]

and the \(\lambda_{j}\)s are written so that \(\lambda_{1}\leq\lambda_{2}\leq\cdots\leq\lambda_{p}\). \((X^{\prime}X)_{r}^{-}\) would be the (Moore-Penrose) generalized inverse of \((X^{\prime}X)\) if \(r(X^{\prime}X)=r\), i.e., if \(0=\lambda_{1}=\cdots=\lambda_{p-r}\). Since \(X=\textit{ULV}^{\prime}\),

\[\tilde{\beta}=(X^{\prime}X)_{r}^{-}X^{\prime}Y=\sum_{j=p-r+1}^{p}v_{j}v_{j}^{ \prime}\textit{ULU}^{\prime}Y/\lambda_{j}^{2}=\sum_{j=p-r+1}^{p}v_{j}\hat{ \gamma_{j}}.\]This is essentially the same procedure as principal component regression. One method chooses \(r\) principal components and the other uses all the principal components that have sufficiently large eigenvalues. Marquardt originally suggested this as an alternative to classical ridge regression, which is the subject of the next section.

### Classical Ridge Regression

_Ridge regression_ was originally proposed by Hoerl and Kennard (1970) as a method to deal with collinearity. Now it is more commonly viewed as a form of penalized likelihood estimation, which makes it a form of Bayesian estimation. In this section, we consider the traditional view of ridge regression. _ALM-III_, Chapter 2 relates ridge regression to the more general issue of penalized estimation.

Hoerl and Kennard (1970) looked at the mean squared error, \(\text{E}[(\hat{\beta}-\beta)^{\prime}(\hat{\beta}-\beta)]\), for estimating \(\beta\) with least squares. This is the expected value of a quadratic form in \((\hat{\beta}-\beta)\). \(\text{E}(\hat{\beta}-\beta)=0\) and \(\text{Cov}(\hat{\beta}-\beta)=\sigma^{2}(X^{\prime}X)^{-1}\); so by Theorem 1.3.2

\[\text{E}[(\hat{\beta}-\beta)^{\prime}(\hat{\beta}-\beta)]=\text{tr}[\sigma^{2} (X^{\prime}X)^{-1}].\]

If \(\lambda_{1}^{2}\),..., \(\lambda_{p}^{2}\) are the eigenvalues of \((X^{\prime}X)\), we see that \(\text{tr}[(X^{\prime}X)^{-1}]=\sum_{j=1}^{p}\lambda_{j}^{-2}\); so

\[\text{E}[(\hat{\beta}-\beta)^{\prime}(\hat{\beta}-\beta)]=\sigma^{2}\sum_{j=1} ^{p}\lambda_{j}^{-2}.\]

If some of the values \(\lambda_{j}^{2}\) are small, the mean squared error will be large.

Hoerl and Kennard suggested using the estimate

\[\tilde{\beta}\equiv(X^{\prime}X+kI)^{-1}X^{\prime}Y, \tag{1}\]

where \(k\) is some fixed scalar. The choice of \(k\) will be discussed briefly later. The consequences of using this estimate are easily studied in the canonical regression model. The canonical regression model (13.2.3) is

\[Y_{*}=\begin{bmatrix}L\\ 0\end{bmatrix}\gamma+e_{*}.\]

The ridge regression estimate is

\[\tilde{\gamma}=(L^{\prime}L+kI)^{-1}[L^{\prime},0]Y_{*}=(L^{2}+kI)^{-1}L^{2} \hat{\gamma}. \tag{2}\]

In particular,

\[\tilde{\gamma}_{j}=\frac{\lambda_{j}^{2}}{\lambda_{j}^{2}+k}\hat{\gamma}_{j}.\]If \(\lambda_{j}\) is small, \(\tilde{\gamma}_{j}\) will be shrunk toward zero. If \(\lambda_{j}\) is large, \(\tilde{\gamma}_{j}\) will change relatively little from \(\tilde{\gamma}_{j}\).

Exercise 13.4 illustrates the relationship between ridge regression performed on the canonical model and ridge regression performed on the usual model. The transformation matrix \(V\) is defined as in Section 3.

**Exercise 13.4**.: Use equations (1) and (2) to show that

(a) \(\tilde{\beta}=V\tilde{\gamma}\),

(b) \(\mbox{E}[(\tilde{\beta}-\beta)^{\prime}(\tilde{\beta}-\beta)]=\mbox{E}[(\tilde {\gamma}-\gamma)^{\prime}(\tilde{\gamma}-\gamma)]\).

The estimate \(\tilde{\beta}\) has expected mean square

\[\mbox{E}[(\tilde{\beta}-\beta)^{\prime}(\tilde{\beta}-\beta)]= \sigma^{2}\mbox{tr}[(X^{\prime}X+kI)^{-1}X^{\prime}X(X^{\prime}X+kI)^{-1}]\] \[+\beta^{\prime}\left\{(X^{\prime}X+kI)^{-1}X^{\prime}X-I\right\}^ {\prime}\left\{(X^{\prime}X+kI)^{-1}X^{\prime}X-I\right\}\beta.\]

Writing \(X^{\prime}X=VL^{2}V^{\prime}\), \(I=V\tilde{V}^{\prime}\), and in the second term \(I=(X^{\prime}X+kI)^{-1}(X^{\prime}X+kI)\), so that \((X^{\prime}X+kI)^{-1}X^{\prime}X-I=-(X^{\prime}X+kI)^{-1}kI\), this can be simplified to

\[\mbox{E}[(\tilde{\beta}-\beta)^{\prime}(\tilde{\beta}-\beta)]=\sigma^{2}\sum_{ j=1}^{p}\lambda_{j}^{2}/(\lambda_{j}^{2}+k)^{2}+k^{2}\beta^{\prime}(X^{\prime}X+kI)^{-2}\beta.\]

The derivative of this with respect to \(k\) at \(k=0\) can be shown to be negative. Since \(k=0\) is least squares estimation, in terms of mean squared error there exists \(k>0\) that gives better estimates of \(\beta\) than least squares. Unfortunately, the particular values of such \(k\) are not known.

Frequently, a _ridge trace_ is used to determine \(k\). A ridge trace is a simultaneous plot of the estimated regression coefficients (which are functions of \(k\)) against \(k\). The value of \(k\) is chosen so that the regression coefficients change little for any larger values of \(k\). In addition to providing a good overall review of ridge regression, Draper and van Nostrand (1979) provide references to criticisms that have been raised against the ridge trace.

Because the mean squared error, \(\mbox{E}[(\tilde{\beta}-\beta)^{\prime}(\tilde{\beta}-\beta)]\), puts equal weight on each regression coefficient, it is often suggested that ridge regression be used only on the rescaled model (13.1.6).

The ridge regression technique admits obvious generalizations. One is to use \(\tilde{\beta}=(X^{\prime}X+K)^{-1}X^{\prime}Y\), where \(K=\mbox{Diag}(k_{j})\). The ridge estimates for canonical regression become

\[\tilde{\gamma}_{j}=\frac{\lambda_{j}^{2}}{\lambda_{j}^{2}+k_{j}}\hat{\gamma}_{ j}.\]

The ridge regression estimate (1) can also be arrived at from a Bayesian argument. With \(Y=X\beta+e\) and \(e\sim N(0,\sigma^{2}I)\), incorporating prior information of the form \(\beta|\sigma^{2}\sim N[0,(\sigma^{2}/k)I]\) leads to fitting a version of (2.10.3) that has

[MISSING_PAGE_FAIL:421]

**Exercise 13.6**  It could be argued that the canonical model should be standardized before applying ridge regression. Define an appropriate standardization so that under the standardization

\[\tilde{\gamma}_{j}=\frac{1}{1+k_{j}}\hat{\gamma}_{j}.\]

#### Ridge Applied to Principal Components

The standard regression model is

\[Y=X\beta+e.\]

Using the notation of the singular value decomposition, (ignoring complications due to centering and scaling) the principal component (PC) regression model is

\[Y=\tilde{X}\gamma+e,\ \ \ \ \ \ \tilde{X}\equiv XV=UL.\]

These are equivalent models because \(C(X)=C(\tilde{X})\). In particular

\[\begin{array}{l}\text{E}(Y)=\tilde{X}\gamma=UL\gamma=UL\gammaV\gamma\\ =XV\gamma=X\beta.\end{array}\]

As a result we can transform _any_ estimate of \(\gamma\) into an estimate of \(\beta\) through

\[\beta=V\gamma.\]

(If the model is not a regression, life is not so simple because \(\beta\) can be any vector with \(\beta=V\gamma+w\) for \(w\perp C(X^{\prime})\).) Traditional PCR involves dropping ill-defined directions from \(\tilde{X}\) but here we apply ridge regression to all of the principal components.

First we want to examine least squares and ridge regression estimates on the PCR model because the behavior of the ridge estimates is very clear there. The least squares estimates are

\[\hat{\gamma}\equiv(\tilde{X}^{\prime}\tilde{X})^{-1}\tilde{X}^{\prime}Y=L^{-1} U^{\prime}Y. \tag{3}\]

The ridge regression estimates turn out to be

\[\hat{\gamma}_{R}\equiv(\tilde{X}^{\prime}\tilde{X}+kI)^{-1}\tilde{X}^{\prime}Y= D\left(\frac{\lambda_{i}^{2}}{\lambda_{i}^{2}+k}\right)\hat{\gamma}. \tag{4}\]

In _ALM-III_ we discuss something called the kernel trick which amounts to fitting _XX_'_e_ in place of \(X\beta\). Since Proposition B.51 establishes \(C(X\!X^{\prime})=C(X)\), the kernel-trick model is equivalent to the original model. The point of the kernel trick is that insome situations it is easier to specify \(XX^{\prime}\) than to specify \(X\). In the kernel-trick model, any estimate of \(\eta\) can be transformed into an estimate of \(\beta\) via \(\beta=X^{\prime}\eta\). Note that if \(\hat{\eta}\) is any least squares estimate, \(\hat{\beta}\equiv X^{\prime}\hat{\eta}\) will be the unique least squares estimate in \(C(X^{\prime})\), cf. Exercise 2.11.8. In applications the kernel trick is typically used with some form of biased estimation, so we also investigate ridge regression applied to the kernel-trick model.

We also want to apply the kernel trick to the PC model, thus we fit \(\tilde{X}\tilde{X}^{\prime}\delta\) in place of \(\tilde{X}\gamma\). Moreover we want find the ridge estimate for the kernel-trick PC model. Applying ridge regression to the kernel-trick PC model gives

\[\hat{\delta}_{KR}\equiv(\tilde{X}\tilde{X}^{\prime}\tilde{X}\tilde{X}^{\prime} +kI)^{-1}\tilde{X}\tilde{X}^{\prime}Y\]

and transforming the estimate back to the PC model eventually gives

\[\hat{\gamma}_{KR}\equiv\tilde{X}^{\prime}\hat{\delta}_{KR}=\tilde{X}^{\prime} (\tilde{X}\tilde{X}^{\prime}\tilde{X}\tilde{X}^{\prime}+kI)^{-1}\tilde{X} \tilde{X}^{\prime}Y=D\left(\frac{\lambda_{i}^{4}}{\lambda_{i}^{4}+k}\right) \hat{\gamma}. \tag{5}\]

That the two ridge PC estimates are both diagonal transformations of the least squares PC estimates elucidates their relationship. The notation for the ridge estimates suppresses their dependence on the tuning parameter \(k\) but it is now clear from (4) and (5) that if you pick any tuning parameter, say \(k_{R}\), to use in the ridge estimate \(\hat{\gamma}_{R}\), there exists a (different) tuning parameter \(k_{KR}\) that gives \(\hat{\gamma}_{R}=\hat{\gamma}_{KR}\), and vice versa.

It is not obvious but the ridge estimates on the PC model transform directly to the ridge estimates on the original model. In particular, the least squares estimates for the original model are

\[\hat{\beta}\equiv(X^{\prime}X)^{-1}X^{\prime}Y=V\hat{\gamma}. \tag{6}\]

The ridge estimates are

\[\hat{\beta}_{R}\equiv(X^{\prime}X+kI)^{-1}X^{\prime}Y=V\hat{\gamma}_{R}. \tag{7}\]

Even the kernel-trick ridge estimates are a direct transformation of the kernel-trick PC ridge estimates,

\[\hat{\beta}_{KR}\equiv X^{\prime}(XX^{\prime}XX^{\prime}+kI)^{-1}XX^{\prime}Y= V\hat{\gamma}_{KR}. \tag{8}\]

**Exercise 13.7**  Prove equations (3) through (8). Hint: All but (5) and (8) follow directly from the characterizations \(X=ULV\) and \(\tilde{X}=UL\). Those two involve applying Proposition B.56 to find, say, the inverse of \([kI+X(X^{\prime}X)X^{\prime}]\) prior to using the characterizations.

### More on Mean Squared Error

In Section 2.9 we showed that the biased estimates obtained from reduced models can provide better estimates than the least squares estimates obtained from the full model. We now explore more general results for the canonical regression model.

For the canonical regression model, Goldstein and Smith (1974) have shown that for \(0\leq h_{j}\leq 1\), if \(\tilde{\gamma}_{j}\) is a general _shrinkage estimator_ defined by

\[\tilde{\gamma}_{j}=h_{j}\hat{\gamma}_{j}\]

and if

\[\frac{{\gamma}_{j}^{2}}{{\rm Var}(\hat{\gamma}_{j})}<\frac{1+h_{j}}{1-h_{j}}, \tag{1}\]

then \(\tilde{\gamma}_{j}\) is a better estimate than \(\hat{\gamma}_{j}\) in that

\[{\rm E}(\tilde{\gamma}_{j}-\gamma_{j})^{2}\leq{\rm E}(\hat{\gamma}_{j}-\gamma _{j})^{2}.\]

In particular, if

\[\gamma_{j}^{2}<{\rm Var}(\hat{\gamma}_{j})=\sigma^{2}\big{/}\lambda_{j}^{2}, \tag{2}\]

then \(\tilde{\gamma}_{j}=0\) is a better estimate than \(\hat{\gamma}_{j}\). Estimating \(\sigma^{2}\) with \(RMS\) and \(\gamma_{j}\) with \(\hat{\gamma}_{j}\) leads to taking \(\tilde{\gamma}_{j}=0\) if the absolute \(t\) statistic for testing \(H_{0}:\gamma_{j}=0\) is less than 1. This is only an approximation to condition (2), so taking \(\tilde{\gamma}_{j}=0\) for larger values of the \(t\) statistic may well be justified.

For generalized ridge regression, \(h_{j}=\lambda_{j}^{2}\big{/}(\lambda_{j}^{2}+k_{j})\), and condition (1) becomes

\[\gamma_{j}^{2}<\frac{\sigma^{2}}{\lambda_{j}^{2}}\frac{2\lambda_{j}^{2}+k_{j}} {k_{j}}=\sigma^{2}\left[\frac{2}{k_{j}}+\frac{1}{\lambda_{j}^{2}}\right].\]

If \(\lambda_{j}^{2}\) is small, almost any value of \(k_{j}\) will give an improvement over least squares. If \(\lambda_{j}^{2}\) is large, only very small values of \(k_{j}\) will give an improvement.

Note that, since the \(\hat{\gamma}_{j}\)s are unbiased, the \(\tilde{\gamma}_{j}\)s will, in general, be biased estimates.

### Robust Estimation and Alternative Distance Measures

_Robust_ estimates of \(\beta\) have less sensitivity to outlying \(y_{i}\) values than least squares estimates, see, for example, Huber and Ronchetti (2009). Robust estimates work better when the distribution of the \(y_{i}\)s has fatter tails than the normal distribution, e.g. Laplace, logistic, \(t(df)\). Optimal estimates for such distributions tend to be nonlinear. In standard linear models least squares estimates are BLUEs, so theyshould be reasonable, if not optimal, for most errors that are i.i.d.. The robust estimates discussed here are still sensitive to high leverage \(x_{i}\) vectors.

Write the \(n\times p\) model matrix as

\[X=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}.\]

As in Section 2.2, least squares estimates minimize

\[\|Y-X\beta\|^{2}=\sum_{i=1}^{n}(y_{i}-x_{i}^{\prime}\beta)^{2}. \tag{1}\]

This is a geometric estimation criterion but we showed that for a standard linear model with \(\operatorname{Cov}(Y)=\sigma^{2}I\) the least squares estimates are BLUEs and if \(Y\) also has a multivariate normal distribution the least squares estimates have other optimal properties. Because the squared distance in (1) is always nonnegative, it is equivalent (but less convenient) to minimize

\[\|Y-X\beta\|=\sqrt{\sum_{i=1}^{n}(y_{i}-x_{i}^{\prime}\beta)^{2}}.\]

In Section 2.7 we discussed generalized least squares estimates that minimized, for some positive definite matrix \(W\),

\[\|Y-X\beta\|_{W}^{2}\equiv(Y-X\beta)^{\prime}W(Y-X\beta).\]

In particular we showed that if \(\operatorname{Cov}(Y)=\sigma^{2}V\), the choice \(W=V^{-1}\) leads to BLUEs and if \(Y\) also has a multivariate normal distribution these generalized least squares estimates have other optimal properties. With \(W\) positive definite, it is equivalent to minimize \(\|Y-X\beta\|_{W}\).

The simplest form of generalized least squares estimation is weighted least squares estimation in which \(W\) is a positive definite diagonal matrix, i.e., \(W=D(w)\) for a vector \(w=(w_{1},\ldots,w_{n})^{\prime}\) with \(w_{i}>0\). In this case

\[\|Y-X\beta\|_{D(w)}^{2}=(Y-X\beta)^{\prime}D(w)(Y-X\beta)=\sum_{i=1}^{n}w_{i}( y_{i}-x_{i}^{\prime}\beta)^{2}. \tag{2}\]

When \(\operatorname{Cov}(Y)=\sigma^{2}D(v_{i})\), the optimal weights in (2) are \(w_{i}=1/v_{i}\).

For an arbitrary \(n\) vector \(v\), the measures \(\|v\|\) and \(\|v\|_{W}\) provide alternative definitions for the length of a vector. A wide variety of estimates for \(\beta\) can be obtained by defining yet other concepts of the length of a vector. One of the most common concepts of length used in mathematics is \(\mathbf{L}^{p}\) length in which, for \(p\geq 1\),

\[\|v\|_{p}\equiv\left[\sum_{i=1}^{n}|v_{i}|^{p}\right]^{1/p}.\]

There is also

\[\|v\|_{\infty}\equiv\max_{i}\{|v_{1}|,\ldots,|v_{n}|\}.\]

In this book, we have used the notation

\[\|\cdot\|\equiv\|\cdot\|_{2}.\]

A minimum \(\mathbf{L}^{p}\) estimate of \(\beta\) minimizes the distance \(\|Y-X\beta\|_{p}\) or, equivalently, minimizes \(\left(\|Y-X\beta\|_{p}\right)^{p}\). Not that I have ever seen anyone do it, but one could even estimate \(\beta\) by minimizing \(\|Y-X\beta\|_{\infty}\). When \(1\leq p<2\), minimum \(\mathbf{L}^{p}\) estimates are robust to unusual \(y_{i}\) values. Taking \(p>2\) makes the estimates _more_ sensitive to unusual \(y_{i}\) values, something statisticians rarely want. (Hence my never seeing anyone use minimum \(\mathbf{L}^{\infty}\) estimation.) But recall that when estimating the mean of a thin tailed distribution, like the uniform, it is the most extreme observations that provide the most information.

Minimum \(\mathbf{L}^{p}\) estimation provides an immediate analogy to finding weighted least squares estimates: just minimize \(\sum_{i=1}^{n}w_{i}|y_{i}-x_{i}^{\prime}\beta|^{p}\) for positive \(w_{i}\)s. Unfortunately, for \(p\neq 2,\mathbf{L}^{p}\) distances do not readily generalize to incorporate relationships between observations as does generalized least squares estimation. When not using a quadratic form to define length, it is not clear how to include an entire positive definite matrix that totally redefines the concept of distance. (As opposed to weighted versions that merely rescale individual variables.) One thing you could do is apply minimum \(\mathbf{L}^{p}\) estimation to model (2.7.2).

In the search for good robust estimates, people have gone well past the use of minimum weighted \(\mathbf{L}^{p}\) estimation with \(1\leq p<2\). _M-estimates_ involve choosing a nonnegative loss function \(\mathcal{L}(y,u)\) and weights and then picking \(\tilde{\beta}\) to minimize the weighted sum of the losses, i.e.,

\[\sum_{i=1}^{n}w_{i}\mathcal{L}(y_{i},x_{i}^{\prime}\tilde{\beta})=\min_{\beta} \sum_{i=1}^{n}w_{i}\mathcal{L}(y_{i},x_{i}^{\prime}\beta). \tag{3}\]

Whether this gives robust estimation or not depends on the choice of loss function.

The M in M-estimation is an allusion to maximum likelihood type estimates. The loss function \(\mathcal{L}(y,u)=(y-u)^{2}\), with equal weights, leads to least squares and thus to maximum likelihood estimates for standard linear models with multivariate normal data. In generalized linear models for binomial data, wherein \(y_{i}\) denotes the proportion of successes, maximum likelihood estimates of \(\beta\) can typically be cast as minimizing a weighted sum of losses where the weights equal the binomial sample sizes, cf. _ALM-III_, Section 13.1. Even Support Vector Machines can be cast as estimating \(\beta\) in \(x^{\prime}\beta\) by minimizing a sum of losses, cf. _ALM-III_, Section 13.5.

In linear models, loss functions typically take the form

\[\mathcal{L}(y,u)=\mathcal{L}(y-u).\]

If \(\mathcal{L}(\xi)\) is differentiable everywhere, Newton-Raphson can be used to find the minimizing value. One of the more famous families of robust loss functions is _Tukey's biweight_ which is typically defined by its derivative:

\[\mathbf{d}_{\xi}\mathcal{L}_{c}(\xi)\equiv\begin{cases}\dot{\xi}\left(1-\frac{ \dot{\xi}^{2}}{c^{2}}\right)&\text{if }|\xi|<c\\ 0&\text{if }|\xi|\geq c\end{cases}\]

for some scale factor \(c\).

#### Exercise 13.8

Find the derivative of the loss function for least squares and minimum \(\mathbf{L}^{1}\) estimation. Graph them along with the \(c=1\) biweight derivative.

### Orthogonal Regression

Suppose we have bivariate data \((x_{i},\,y_{i})\) and want to fit a line \(\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x\). Rather than using least squares (that minimizes vertical distances to the line) we will do orthogonal regression that minimizes perpendicular distances to the line. We will run this line through the point \((\bar{x}.,\,\bar{y}.)\) so we need only worry about the slope of the line.

Transform \((x_{i},\,y_{i})^{\prime}\) into \(v_{i}\equiv(x_{i}-\bar{x}.,\,y_{i}-\bar{y}.)^{\prime}\). We want to find a vector \(a\), or more properly a one-dimensional column space \(C(a)\), that minimizes the squared perpendicular distances between the data \(v_{i}\) and the regression line that consists of all multiples of the vector \(a\). We need to take \(v_{i}\) and project it onto the line, that is, project it into \(C(a)\). The projection is \(M_{a}v_{i}\). The squared perpendicular distance between the data and the line is \(\|v_{i}-M_{a}v_{i}\|^{2}=\|(I-M_{a})v_{i}\|^{2}=v_{i}^{\prime}(I-M_{a})v_{i}\). It will become important later to recognize this as the squared length of the perpendicular projection of \(v_{i}\) onto the orthogonal complement of the regression surface vector space. In any case, we need to pick the line so as to minimize the sum of all these squared distances, i.e., so that \(\sum_{i=1}^{n}v_{i}^{\prime}(I-M_{a})v_{i}\) is minimized. However, if \(a\) minimizes \(\sum_{i=1}^{n}v_{i}^{\prime}(I-M_{a})v_{i}\), it maximizes \(\sum_{i=1}^{n}v_{i}^{\prime}M_{a}v_{i}\). Note also that \[\max_{a}\sum_{i=1}^{n}v_{i}^{\prime}M_{a}v_{i} = \max_{a}\sum_{i=1}^{n}v_{i}^{\prime}a(a^{\prime}a)^{-1}a^{\prime}v _{i}\] \[= \max_{a}\frac{1}{a^{\prime}a}\sum_{i=1}^{n}a^{\prime}v_{i}v_{i}^{ \prime}a\] \[= \max_{a}\frac{1}{a^{\prime}a}a^{\prime}\left(\sum_{i=1}^{n}v_{i}v _{i}^{\prime}\right)a\] \[= \max_{a}\frac{n-1}{a^{\prime}a}a^{\prime}Sa,\]

where \(S\) is the sample covariance matrix of the data.

It is enough to find \(\hat{a}\) such that

\[\frac{\hat{a}^{\prime}S\hat{a}}{\hat{a}^{\prime}\hat{a}}=\max_{a}\frac{a^{ \prime}Sa}{a^{\prime}a}.\]

It is well-known (_ALM-III_, Proposition 12.7.4) that this max is achieved by eigenvectors associated with the largest eigenvalue of \(S\). In particular, if we pick a maximizing eigenvector \(\hat{a}\) to be \(\hat{a}^{\prime}=(1,\hat{\beta})\), then \(\hat{\beta}\) is the orthogonal regression slope estimate. The estimated line becomes

\[\hat{y}=\bar{y}.+\hat{\beta}(x-\bar{x}.).\]

Technically, this occurs because for the fitted values to fall on the regression line, they must themselves determine a maximizing eigenvector, i.e., \(\hat{y}\) is _defined_ so that

\[\begin{bmatrix}x-\bar{x}.\\ \hat{y}-\bar{y}.\end{bmatrix}\equiv(x-\bar{x}.)\begin{bmatrix}1\\ \hat{\beta}\end{bmatrix}.\]

Normally, we would find the eigenvector corresponding to the largest eigenvalue computationally, but our problem can be solved analytically. To find the maximizing eigenvector we need to solve the matrix equation

\[\begin{bmatrix}s_{x}^{2}-\lambda&s_{xy}\\ s_{xy}&s_{y}^{2}-\lambda\end{bmatrix}\begin{bmatrix}1\\ \hat{\beta}\end{bmatrix}=\begin{bmatrix}0\\ 0\end{bmatrix}. \tag{1}\]

This simplifies to the set of equations

\[\lambda=s_{x}^{2}+s_{xy}\hat{\beta} \tag{2}\]

and

\[s_{xy}+(s_{y}^{2}-\lambda)\hat{\beta}=0. \tag{3}\]

Substituting \(\lambda\) from (2) into (3),\[s_{xy}+(s_{y}^{2}-s_{x}^{2})\hat{\beta}-s_{xy}\hat{\beta}^{2}=0. \tag{4}\]

Applying the quadratic formula gives

\[\hat{\beta}=\frac{-(s_{y}^{2}-s_{x}^{2})\pm\sqrt{(s_{y}^{2}-s_{x}^{2})^{2}+4s_{ xy}^{2}}}{-2s_{xy}}=\frac{(s_{y}^{2}-s_{x}^{2})\pm\sqrt{(s_{y}^{2}-s_{x}^{2})^{2}+4 s_{xy}^{2}}}{2s_{xy}}.\]

Substituting \(\hat{\beta}\) back into (2), the larger of the two values of \(\lambda\) corresponds to

\[\hat{\beta}=\frac{(s_{y}^{2}-s_{x}^{2})+\sqrt{(s_{y}^{2}-s_{x}^{2})^{2}+4s_{xy} ^{2}}}{2s_{xy}},\]

which is our slope estimate.

If you find the eigenvalues and eigenvectors computationally, remember that eigenvectors for a given eigenvalue (essentially) form a vector space. (Eigenvectors are not allowed to be 0.) Thus, a reported eigenvector \((a_{1},a_{2})^{\prime}\) for the largest eigenvalue also determines the eigenvector we want, \((1,a_{2}/a_{1})^{\prime}\).

It turns out that we can also get least squares estimates by modifying the matrix equations (1). Consider

\[\begin{bmatrix}s_{x}^{2}+[s_{y}^{2}-s_{xy}^{2}/s_{x}^{2}]-\lambda&s_{xy}\\ s_{xy}&s_{y}^{2}-\lambda\end{bmatrix}\begin{bmatrix}1\\ \hat{\beta}\end{bmatrix}=\begin{bmatrix}0\\ 0\end{bmatrix}\]

or equivalently,

\[\begin{bmatrix}s_{x}^{2}+[s_{y}^{2}-s_{xy}^{2}/s_{x}^{2}]&s_{xy}\\ s_{xy}&s_{y}^{2}\end{bmatrix}\begin{bmatrix}1\\ \hat{\beta}\end{bmatrix}=\lambda\begin{bmatrix}1\\ \hat{\beta}\end{bmatrix}.\]

Rather than solving this for \(\hat{\beta}\), simply observe that one solution is \(\lambda=s_{x}^{2}+s_{y}^{2}\) and \((1,\hat{\beta})^{\prime}=(1,s_{xy}/s_{x}^{2})^{\prime}\). Note that \([s_{y}^{2}-s_{xy}^{2}/s_{x}^{2}]=[(n-2)/(n-1)]MSE\) where \(MSE\) is the mean squared error from the least squares fit of \(y_{i}=\beta_{0}+\beta_{1}x_{i}+\varepsilon_{i}\).

To generalize the orthogonal regression procedure to multiple regression we need a more oblique approach. With data \((x_{i}^{\prime},y_{i})\) consisting of \(p\)-dimensional vectors, a linear regression surface corresponds to a \((p-1)\)-dimensional hyperplane so that if we specify a \((p-1)\) vector of predictor variables \(x\), we know the fitted value \(\hat{y}\) because it is the point corresponding to \(x\) on the hyperplane. By considering data \(v_{i}^{\prime}\equiv(x_{i}^{\prime}-\bar{x}^{\prime},y_{i}-\bar{y}.)\), the regression surface goes through the origin and becomes a \((p-1)\)-dimensional vector space. Rather than specify the \((p-1)\)-dimensional space, it is easier to find the orthogonal complement which is one-dimensional. Writing the one-dimensional space as \(C(a)\) for a \(p\) vector \(a\), the regression surface will be \(C(a)^{\perp}\). The squared distances from the \(v_{i}\)s to the \((p-1)\)-dimensional regression space are now \(v_{i}^{\prime}M_{a}v_{i}\), so we want to minimize \(\sum_{i=1}^{n}v_{i}^{\prime}M_{a}v_{i}\). Similar to our earlier argument,\[\min_{a}\sum_{i=1}^{n}v_{i}^{\prime}M_{a}v_{i}=\min_{a}\frac{n-1}{a^{\prime}a}a^{ \prime}Sa,\]

with \(S\) being the sample covariance matrix of the complete data. However, now we obtain our fitted values differently. The minimum is achieved by choosing the eigenvector \(\hat{a}=(\hat{\beta}^{\prime},-1)^{\prime}\) corresponding to the smallest eigenvalue. Our fitted values \(\hat{y}\) now must determine vectors that are orthogonal to this eigenvector, so they satisfy

\[\left[\hat{\beta}^{\prime}-1\right]\left[\begin{matrix}x-\bar{x}.\\ \hat{y}-\bar{y}.\end{matrix}\right]=0\]

or

\[\hat{y}=\bar{y}.+\hat{\beta}^{\prime}(x-\bar{x}.).\]

As illustrated earlier for \(p=2\), any eigenvector (for the smallest eigenvalue) reported by a computer program is easily rescaled to \((\hat{\beta}^{\prime},-1)^{\prime}\)

Finally, this approach better give the same answers for simple linear regression that we got from our first procedure. It is not difficult to see that

\[\left[\begin{matrix}s_{x}^{2}-\lambda&s_{xy}\\ s_{xy}&s_{y}^{2}-\lambda\end{matrix}\right]\left[\begin{matrix}\hat{\beta}\\ -1\end{matrix}\right]=\left[\begin{matrix}0\\ 0\end{matrix}\right]\]

once again leads to equation (4) but now

\[\hat{\beta}=\frac{(s_{y}^{2}-s_{x}^{2})+\sqrt{(s_{y}^{2}-s_{x}^{2})^{2}+4s_{xy }^{2}}}{2s_{xy}}\]

corresponds to the smallest eigenvalue. If you think about it, with distinct eigenvalues, the eigenvector \((1,\hat{\beta})^{\prime}\) corresponding to the largest eigenvalue must be orthogonal to any eigenvector for the smallest eigenvalue, and \((\hat{\beta},-1)^{\prime}\) is orthogonal to \((1,\hat{\beta})^{\prime}\).

Like least squares, this procedure is a geometric justification for an estimate, not a statistical justification. In Chapter 2 we showed that least squares estimates have statistical optimality properties like being BLUEs, MVUEs, and MLEs. The ideas used here are similar to those needed for looking at the separating hyperplanes often used to motivate support vector machines, see Moguerza and Munoz (2006) or Zhu (2008).

## Bibliography

* (1)
* Belsley (1984) Belsley, D. A. (1984). Demeaning conditioning diagnostics through centering (with discussion). _The American Statistician_, _38_, 73-77.
* Belsley (1991) Belsley, D. A. (1991). _Collinearity diagnostics: Collinearity and weak data in regression_. New York: Wiley.

* Belsley et al. (1980) Belsley, D. A., Kuh, E., & Welsch, R. E. (1980). _Regression diagnostics: Identifying influential data and sources of collinearity_. New York: Wiley.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton, FL: Chapman and Hall/CRC Press.
* Christensen (2018) Christensen, R. (2018). Comment on "A note on collinearity diagnostics and centering" by Velilla (2018). _The American Statistician_, _72_, 114-117.
* Draper & van Nostrand (1979) Draper, N. R., & van Nostrand, R. C. (1979). Ridge regression and James-Stein estimation: Review and comments. _Technometrics_, _21_, 451-466.
* Goldstein & Smith (1974) Goldstein, M., & Smith, A. F. M. (1974). Ridge-type estimators for regression analysis. _Journal of the Royal Statistical Society, Series B_, _26_, 284-291.
* Hoerl & Kennard (1970) Hoerl, A. E., & Kennard, R. (1970). Ridge regression: Biased estimation for non-orthogonal problems. _Technometrics_, _12_, 55-67.
* Huber & Ronchetti (2009) Huber, P. J., & Ronchetti, E. M. (2009). _Robust statistics_ (2nd ed.). New York: Wiley.
* Johnson & Wichern (2007) Johnson, R. A., & Wichern, D. W. (2007). _Applied multivariate statistical analysis_ (6th ed.). Englewood Cliffs, NJ: Prentice-Hall.
* Marquardt (1970) Marquardt, D. W. (1970). Generalized inverses, ridge regression, biased linear estimation, and nonlinear estimation. _Technometrics_, _12_, 591-612.
* Moguerza & Munoz (2006) Moguerza, J. M., & Munoz, A. (2006). Support vector machines with applications. _Statistical Science_, _21_, 322-336.
* Mosteller & Tukey (1977) Mosteller, F., & Tukey, J. W. (1977). _Data analysis and regression_. Reading, MA: Addison-Wesley.
* Velilla (2018) Velilla, S. (2018). A note on collinearity diagnostics and centering. _The American Statistician_, _72_, 140-146.
* Zhu (2008) Zhu, M. (2008). Kernels and ensembles: Perspectives on statistical learning. _The American Statistician_, _62_, 97-109.

## Chapter 14 Variable Selection

if one has the time and money to compute all of them, it may be difficult to assimilate that much information.

A more efficient procedure than computing all possible regressions is to choose a criterion for ranking how well different candidate models fit and to compute only the best fitting models. Typically, one would want to _identify several of the best fitting models and investigate them further_. The computing effort for this _best subset regression_ method is still considerable. Most computer programs put limits on how large \(s\) can be. For example, the R package leaps and Minitab 18 both require \(s\leq 31\). Section 1 discusses criteria for choosing the best models.

An older group of methods is stepwise regression. These methods consider the efficacy of adding or deleting individual variables to a model that is currently under consideration. These methods have the flaw of considering variables only one at a time. For example, there is no reason to believe that the best two variables to add to a model are the one variable that adds most to the model followed by the one variable that adds the most to this augmented model. The flaw of stepwise procedures is also their virtue. Because computations go one variable at a time, they are relatively easy. Stepwise methods are considered in Section 2. Section 3 discusses the best subset and traditional stepwise approaches to variable selection.

Problems with \(s\) large relative to \(n\) seem to have become more important with the rise of computing power. In the early '90s I was involved in discussions of data on ceramic superconductors. Collecting the data involved destroying the object and each superconductor cost $20,000. Naturally \(n\) was small and naturally they measured everything they could on every superconductor, so \(s\) was larger than \(n\). Even in situations where the cost per unit is not large, if it easy to measure large numbers of variables on each unit, people do so.

Cook et al. (2013, 2015) and Tarpey et al. (2015) are a few of many works that discuss asymptotics when the number of predictors \(s\) increases as a fixed percentage of the sample size, but even those problems are much more tractable than having \(s\) as large or larger than \(n\). If \(s\) is close to \(n\), fitting the full model (1) may be unrealistic or unwise. It seems to me that a crucial factor is having an adequate number of degrees of freedom for error. If \(s+1=n\), a regression model will be saturated, i.e. \(dFE=0\) and \(\hat{Y}=Y\). When \(s+1>n\), the model can no longer be a regression model because \((X^{\prime}X)^{-1}\) will not exist.

I am not aware of any truly good general methods for handling data with really large numbers of predictor variables. When the predictor variables are actual measurements, I suspect that the best way to handle \(s>n\) is to model the covariance structure of the predictor variables with fewer parameters than there are data points and then use best linear prediction theory as the basis for analysis. That idea seems less appropriate when the predictors in the model include many nonlinear functions of some original measured predictor variables, as occurs in many nonparametric regression models, cf. _ALM-III_, Chapter 1.

Despite its obvious faults, forward selection seems to be the best available general procedure for dealing with large \(s\). (Although I am intrigued by the potential of PCR.) Section 4 discusses some modern ideas on how to improve forward selection when \(s\) is large.

Another popular method of variable selection is to employ the _lasso (least absolute shrinkage and selection operator)_. The lasso was discussed briefly in _PA-IV_ and is discussed more extensively in _ALM-III_, Chapter 2 but it is not discussed here.

### 14.1 All Possible Regressions and Best Subset Regression

There is little to say about the all possible regressions technique. The efficient computation of all possible regressions is due to Schatzoff et al. (1968). Their algorithm was a major advance. Further advances have made this method obsolete. It is a waste of money to compute all possible regressions. One should only compute those regressions that consist of the best subsets of the predictor variables.

The efficient computation of the best regressions is due to Furnival and Wilson (1974). "Best" is defined by ranking models on the basis of some measure of how well they fit. The most commonly used of these measures were \(R^{2}\), adjusted \(R^{2}\), and Mallows's \(C_{p}\). In recent years _AIC_ and _BIC_ have become increasingly popular measures. Except for \(R^{2}\), all of these criteria introduce a penalty for fitting more parameters. _Cost complexity pruning_ determines the best model by using cross-validation to determine the most appropriate penalty. All of these criteria are discussed in the subsections that follow. Although nominally discussed for regression models, _all of these measures are trivially adapted to general linear models by replacing the number of columns in model matrices by their ranks_.

Although the criteria for identifying best models are traditionally used in the context of finding the best subsets among all possible regression models, they can be used to identify the best within any collection of linear or generalized linear models. For example, Christensen (2015) uses the \(C_{p}\) statistic to identify best unbalanced ANOVA models, Christensen (1997) used AIC to identify best ANOVA-like log-linear models for categorical data, and in the next section we mention using them on the sequences of models created by stepwise regression procedures.

#### 14.1.1 \(R^{2}\)

The coefficient of determination, \(R^{2}\), was discussed in Section 6.4. It is computed as

\[R^{2}=\frac{SSReg}{SSTot-C}\]

and is just the ratio of the variability in \(y\) explained by the regression to the total variability of \(y\). \(R^{2}\) measures how well a regression model predicts (fits) the data as compared to just fitting a mean to the data. If one has two models with, say, \(p\) independent variables, other things being equal, the model with the higher \(R^{2}\) will be the better model.

Using \(R^{2}\) is not a valid way to compare models with different numbers of independent variables. With \(s>p\), the \(R^{2}\) for the full model

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{s}x_{is}+e_{i} \tag{1}\]

must be at least as large as the \(R^{2}\) for the candidate model

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{p}x_{ip}+e_{i} \tag{2}\]

The full model has all the variables in the candidate model plus more, so

\[SSReg(1)\geq SSReg(2)\]

and

\[R^{2}(1)\geq R^{2}(2).\]

Typically, if the \(R^{2}\) criterion is chosen, a program for doing best subset regression will print out the models with the highest \(R^{2}\) for each possible value of \(p\), the number of predictor variables. It is the use of the \(R^{2}\) criterion in best subset regression that makes computing all possible regressions obsolete. The \(R^{2}\) criterion fits all the good models one could ever want. In fact, it probably fits too many models.

#### Adjusted \(R^{2}\)

The adjusted \(R^{2}\) is a modification of \(R^{2}\) so that it can be used to compare models with different numbers of predictor variables. For a candidate model with \(p\) predictor variables plus an intercept, the adjusted \(R^{2}\) is defined as

\[\text{Adj }R^{2}=1-\frac{n-1}{n-p-1}\left(1-R^{2}\right).\]

Define \(s_{y}^{2}=(SSTot-C)/(n-1)\), then \(s_{y}^{2}\) is the sample variance of the \(y_{i}\)s ignoring any regression structure. It is easily seen (Exercise 14.1) that

\[\text{Adj }R^{2}=1-\frac{RMS}{s_{y}^{2}}.\]

As in the previous chapter, to avoid confusion with theoretical expected (mean) squared errors, we refer to the unbiased variance estimate as the residual mean square. The best models based on the Adj \(R^{2}\) criterion are those models with the smallest residual mean squares.

As a method of identifying sets of good models, this is very attractive. The models with the smallest residual mean squares should be among the best models. However, the model with the smallest residual mean square may very well not be the best model.

Consider the question of deleting some variables from a model. In particular, consider testing model (2) against model (1). If the \(F\) statistic is greater than 1, then deleting the variables will increase the residual mean square. By the adjusted \(R^{2}\) criterion, the variables should not be deleted. (See Exercise 14.2.) However, unless the \(F\) value is substantially greater than 1, the variables probably should be deleted. The Adj \(R^{2}\) criterion tends to include too many variables in the model.

##### 14.1.1 Show that Adj \(R^{2}=1-(RMS/s_{y}^{2})\).

##### 14.2.2 Exercise 14.2

Consider testing the candidate regression model (2) against (1). Show that \(F>1\) if and only if the Adj \(R^{2}\) for model (1) is greater than the Adj \(R^{2}\) for model (2).

Some people define a _predicted_\(R^{2}\) as \(R^{2}\equiv 1-[PRESS/(n-1)s_{y}^{2}]\) where PRESS was discussed in Section 12.5.

##### 14.1.3 Mallows's \(C_{p}\)

Suppose the full model \(y_{i}=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{s}x_{is}+e_{i}\), i.e. \(Y=X\beta+e\), is correct. In variable selection the first problem is that some of the \(\beta_{j}\)s may be zero. As discussed in Section 2.9, we can even get better fitted values by eliminating variables that merely have small \(\beta_{j}\)s. Rather than trying to identify which \(\beta_{j}\)s are zero, Mallows suggested that the appropriate criterion for evaluating a reduced candidate model \(Y=X_{0}\gamma+e\) is via its mean squared error for estimating \(X\beta\), i.e.,

\[\mbox{E}\big{[}(X_{0}\hat{\gamma}-X\beta)^{\prime}(X_{0}\hat{\gamma}-X\beta) \big{]}\,.\]

This is the same criterion used in Section 2.9. As mentioned earlier, to distinguish between this use of the term "mean squared error" and the estimate of the variance in the full model we write \(R\mbox{SS}(\beta)\equiv Y^{\prime}(I-M)Y\) for the residual sum of squares and \(RMS(\beta)\equiv Y^{\prime}(I-M)Y/r(I-M)\) for the residual mean square. The statistics \(R\mbox{SS}(\gamma)\) and \(RMS(\gamma)\) are the corresponding quantities for the model \(Y=X_{0}\gamma+e\).

The quantity

\[(X_{0}\hat{\gamma}-X\beta)^{\prime}(X_{0}\hat{\gamma}-X\beta)\]

is a quadratic form in the vector \((X_{0}\hat{\gamma}-X\beta)\). Writing

\[M_{0}=X_{0}(X_{0}^{\prime}X_{0})^{-}X_{0}^{\prime}\]

gives

\[(X_{0}\hat{\gamma}-X\beta)=M_{0}Y-X\beta,\]\[\begin{array}{c} {\rm E}(X_{0}\hat{\gamma}-X\beta)=M_{0}X\beta-X\beta=-(I-M_{0})X\beta,\\ {\rm Cov}(X_{0}\hat{\gamma}-X\beta)=\sigma^{2}M_{0}.\end{array}\]

From Theorem 1.3.2

\[{\rm E}\big{[}(X_{0}\hat{\gamma}-X\beta)^{{}^{\prime}}(X_{0}\hat{\gamma}-X\beta )\big{]}=\sigma^{2}{\rm tr}(M_{0})+\beta^{{}^{\prime}}X^{{}^{\prime}}(I-M_{0}) X\beta.\]

We do not know \(\sigma^{2}\) or \(\beta\), but we can estimate the mean squared error. First note that

\[{\rm E}\big{[}Y^{\prime}(I-M_{0})Y\big{]}=\sigma^{2}{\rm tr}(I-M_{0})+\beta^{ \prime}X^{\prime}(I-M_{0})X\beta;\]

so

\[\begin{array}{c} {\rm E}\big{[}(X_{0}\hat{\gamma}-X\beta)^{\prime}(X_{0}\hat{\gamma}-X\beta) \big{]}=\sigma^{2}{\rm tr}(M_{0})+{\rm E}\big{[}Y^{\prime}(I-M_{0})Y\big{]}- \sigma^{2}{\rm tr}(I-M_{0})\\ =\sigma^{2}\left[2{\rm tr}(M_{0})-n\right]+{\rm E}\big{[}Y^{\prime}(I-M_{0})Y \big{]}.\end{array}\]

With \(p+1={\rm tr}(M_{0})=r(X_{0})\), an unbiased estimate of the mean squared error is

\[RMS(\beta)[2(p+1)-n]+RSS(\gamma).\]

Mallows's \(C_{p}\) statistic simply rescales the estimated mean squared error,

\[C_{p}\equiv\frac{RSS(\gamma)}{RMS(\beta)}-[n-2(p+1)].\]

The models with the smallest values of \(C_{p}\) have the smallest estimated mean squared error and should be among the best models for the data.

\(C_{p}\) _allows us to estimate which reduced models will give better \(\beta\) estimates through incorporating bias_. (A standard reduced model biases \(\beta\) towards \(0\). Incorporating a known offset vector \(Xb\) into a reduced model biases \(\beta\) towards \(b\).)

\(C_{p}\) is the only measure discussed in this section that involves fitting a full model.

**Exercise 14.3** : For the specific candidate model (2) that drops some of the variables in (1), it is easy to see how an estimate of \(\gamma\) determines an estimate of \(\beta\) in (1); some of the \(\hat{\beta}_{j}\)s are forced to be \(0\). For a candidate model specified only by \(C(X_{0})\subset C(X)\), show how an estimate of \(\gamma\) determines an estimate of \(\beta\).

**Exercise 14.4** :
1. Consider the \(F\) statistic for testing model (2) against model (1). Show that \[C_{p}=(s-p)(F-2)+s+1.\]
2. If \({\rm E}(Y)=X_{1}\delta\) has \(r(X_{1})=r\) and \(C(X_{0})\subset C(X_{1})\subset C(X)\), show that \[\frac{[RSS(\gamma)-RSS(\delta)]/[r-p]}{RMS(\beta)}>2\]

if and only if \(C_{p}\) for the \(\gamma\) model is greater than \(C_{p}\) for the \(\delta\) model.

**Exercise 14.5**: _Give an informal argument to show that if \(Y=X_{0}\gamma^{\prime}+e\) is a correct model, then the value of \(C_{p}\) should be around \(p+1\). Provide a formal argument for this fact. Show that if \((n-s)>3\), then \(\operatorname{E}(C_{p})=(p+1)+2(s-p)/(n-s-3)\). To do this you need to know that if \(W\sim F(u,v,0)\), then \(\operatorname{E}(W)=v/(v-2)\) for \(v>2\). For large values of \(n\) (relative to \(s\) and \(p\)), what is the approximate value of \(\operatorname{E}(C_{p})\)?_

#### Information Criteria: AIC, BIC

Unlike the previous criteria that depend only on second moment assumptions, information criteria depend on the likelihood of the data.

As seen in Section 2.4, the log-likelihood for a standard linear model under normal theory is

\[\ell(\beta,\sigma^{2})=\frac{-n}{2}\log(2\pi)-\frac{n}{2}\log[\sigma^{2}]-(Y-X \beta)^{\prime}(Y-X\beta)/2\sigma^{2}.\]

The maximum likelihood estimates of \(\beta\) and \(\sigma^{2}\) are the least squares \(\hat{\beta}\) and \(\hat{\sigma}^{2}\equiv RSS/n\), so

\[-2\ell(\hat{\beta},\hat{\sigma}^{2}) = n\log(2\pi)+n\log[(\hat{\sigma}^{2})]+(Y-X\hat{\beta})^{\prime}( Y-X\hat{\beta})/\hat{\sigma}^{2}\] \[= n\log(2\pi)+n\log(RSS/n)+RSS/(RSS/n).\] \[= n\log(2\pi)+n\log(RSS)-n\log(n)+n.\]

The _Akaike Information Criterion (AIC)_ is \(-2\) times the maximum of the log-likelihood plus \(2\) times the number of parameters in the model. Better models have smaller AIC values.

For a candidate regression model with \(p\) predictors plus an intercept plus an unknown variance, the MLEs are \(\hat{\gamma},\hat{\sigma}^{2}_{\gamma}=RSS(\gamma)/n\), so

\[AIC = -2\ell(\hat{\gamma},\hat{\sigma}^{2}_{\gamma})+2(p+2)\] \[= n\log(2\pi)+n\log[RSS(\gamma)]-n\log(n)+n+2(p+2).\] \[= \{n\log(2\pi)-n\log(n)+n+4\}+n\log[RSS(\gamma)]+2p.\]

Everything in the first term of the last line is a constant that does not depend on the particular model, so, for comparing candidate models with intercepts, effectively

\[AIC=n\log[RSS(\gamma)]+2p.\]If you want to use AIC to compare models with different data distributions, the constant term needs to be included.

While it is somewhat advantageous that AIC can be computed without worrying about the existence of a largest model, it is of interest to compare how AIC and \(C_{p}\) work when both are applicable. Using the notation for \(C_{p}\) statistics, AIC picks candidate models with small values of

\[RSS(\gamma)e^{2p/n}=\exp(AIC/n),\]

whereas \(C_{p}\) picks models with small values of

\[RSS(\gamma)+2pRMS(\beta).\]

**Exercise 14.6** Find the AIC for a linear model in which the variance is known. Show that the \(C_{p}\) statistic is a simple function of the known variance AIC but where the variance is estimated from the largest available model.

Asymptotically, something similar to Exercise 14.4a kicks in for AIC where a larger model is preferred if the \(F\) statistic is above 2. Again using the notation of the previous subsection, suppose \(s\) and \(p\) are fixed but \(n\) is large. The full model is preferred if \(AIC(\gamma)>AIC(\beta)\), or

\[1<\frac{e^{AIC(\gamma)}}{e^{AIC(\beta)}}=\frac{RSS(\gamma)}{RSS(\beta)}e^{-2(s -p)/n}\]

or

\[e^{2(s-p)/n}<\frac{RSS(\gamma)}{RSS(\beta)}.\]

For \(a\) close to 0, \(e^{a}\doteq 1+a\), so when \(n\) is large an approximate condition for preferring the larger model is

\[\left[1+\frac{2(s-p)}{n}\right]<\frac{RSS(\gamma)}{RSS(\beta)},\]

or

\[2<\frac{n}{s-p}\left[\frac{RSS(\gamma)}{RSS(\beta)}-1\right],\]

or

\[2<\frac{n}{n-s-1}\frac{n-s-1}{s-p}\left[\frac{RSS(\gamma)-RSS(\beta)}{RSS( \beta)}\right]=\frac{n}{n-s-1}F.\]

For large \(n\) this is approximately the same condition as for \(C_{p}\).

For small to moderate samples, many prefer to use a bias corrected form of AIC for evaluating candidate models,\[AICc\equiv AIC+\frac{2(p+2)(p+3)}{n-p-3}.\]

cf. Sugiura (1978), Hurvich and Tsai (1989), Bedrick and Tsai (1994), and Cavanaugh (1997). It is commonly suggested to use AICc when any of the candidate models have \(n/(p+2)<40\).

Schwarz (1978) presented an asymptotic _Bayesian information criterion (BIC)_ which is \(-2\) times the maximum of the log-likelihood plus \(\log(n)\) times the number of model parameters. For a standard normal theory regression candidate model with an intercept,

\[\begin{array}{l}BIC=-2\ell(\hat{\gamma},\hat{\sigma}_{\gamma}^{2})+(p+2)\log( n)\\ \phantom{\frac{1}{2}}=\{n\log(2\pi)-(n-2)\log(n)+n\}+n\log RSS+p\log(n),\end{array}\]

or effectively,

\[\begin{array}{l}BIC=n\log RSS+p\log(n).\end{array}\]

The derivation of BIC from Bayesian principals is described in Christensen et al. (2010).

BIC places much greater demands on variables to be included. When comparing nested models with \(p\) and \(s\) predictors, it chooses the larger model when, approximately,

\[\log(n)<F.\]

(The asymptotics kick in much slower than for AIC, which depends on \((1/n)\to 0\), because BIC relies on the much slower convergence of \(\log(n)/n\to 0\).)

Exercise 14.7: Show that, for a given value of \(p\), the \(R^{2}\), Adj \(R^{2}\), \(C_{p}\), AIC, AICc, and BIC criteria all induce the same rankings of candidate models.

#### Cost Complexity Pruning

_Cost complexity pruning_ is a related, but more complicated, way of determining the best model within a collection of candidate models. The collection of models can be all possible models or just the sequences of models determined by a stepwise regression method as discussed in the next section. For a given value of \(\alpha\), pick the model from the collection that minimizes \(R\mbox{\rm{S}}\Sigma+\alpha p\). The complexity comes because the value of \(\alpha\) is chosen by cross-validation. Find the best model for a large number of \(\alpha\) values and then choose a final \(\alpha\), and thus a final model, by cross-validation.

Specifically, randomly divide the data into \(K\) equal sized subgroups of observations, leave out one subgroup and apply the procedure to the other \(K-1\). Using the \(K-1\) subgroups as data, find the model within the collection that minimizes \(R\mbox{\rm{S}}\Sigma+\alpha p\), fit the model, predict the results in the omitted subgroup, and find the mean of the squared prediction errors, i.e., _MSPE_. (Mean Squared _Prediction_ Error;_not_ the Mean Squared Pure Error discussed in Chapter 6.) In cost complexity pruning you do this for a large number of different \(\alpha\) values to get a _MSPE_ for each \(\alpha\), i.e., _MSPE_(\(\alpha\)). Cycle through, leaving out a different subgroup each time, to get \(K\) different means of squared prediction errors, i.e., _MSPEk_(\(\alpha\)), \(k=1,\ldots,K\). Pick \(\hat{\alpha}\) to minimize \(\sum_{k=1}^{K}\textit{MSPE}_{k}(\alpha)\). The best model is the model that minimizes \(\textit{RSS}+\hat{\alpha}\textit{p}\) when fitted to all the data.

James et al. (2013) discuss cost complexity pruning in the context of fitting regression trees. The term "pruning" originates from hopping off tree limbs, not from devouring desiccated plums.

### Stepwise Regression

Stepwise regression methods involve adding or deleting variables one at a time.

#### Traditional Forward Selection

Suppose we have variables \(y\), \(x_{1}\), \(x_{2}\), and \(x_{3}\) and the current model is

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+e_{i}.\]

In forward selection we must choose between adding variables \(x_{2}\) and \(x_{3}\). Fit the models

\[y_{i} = \beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+e_{i},\] \[y_{i} = \beta_{0}+\beta_{1}x_{i1}+\beta_{3}x_{i3}+e_{i}.\]

Choose the model with the higher \(R^{2}\). Equivalently, one could look at the \(t\) (or \(F\)) statistics for testing \(H_{0}:\beta_{2}=0\) and \(H_{0}:\beta_{3}=0\) and choose the model that gives the larger absolute value of the statistic. One could also look at \(r_{y2:1}\) and \(r_{y3:1}\) and pick the variable that gives the larger absolute value for the partial correlation.

_Traditional forward selection_ sequentially adds variables to the model. Since this is a sequential procedure, the model in question is constantly changing. At any stage in the selection process, forward selection adds the variable that when added:

1. gives the largest absolute \(t\) statistic,
2. gives the largest \(F\) statistic,
3. gives the smallest \(P\) value,
4. increases \(R^{2}\) the most,
5. decreases _RSS_ the most,6. gives the smallest \(C_{p}\),
7. gives the smallest AIC,
8. gives the smallest BIC,
9. has the highest absolute partial correlation with \(y\) given the variables in the current model.

These criteria are equivalent because we are only considering the addition of one new variable.

#### Exercise 14.8

Show that the nine enumerated criteria for selecting a variable are equivalent.

Traditional forward selection stops adding variables when one of three things happens:

1. \(p^{*}\) variables have been added,
2. all absolute \(t\) statistics for adding variables not in the model are less than \(t^{*}\),
3. the tolerance is too small for all variables not in the model.

The user (or programmer) picks the values of \(p^{*}\) and \(t^{*}\) and the tolerance limit. Tolerance was discussed in the previous chapter. No variable is ever added if its tolerance is too small, regardless of its absolute \(t\) statistic. Traditionally, one just uses the model that stopped the process. Alternatively, one could use a model selection criterion to pick the best among the models that forward selection has produced.

Although the nine criteria for adding variables are equivalent, stopping rules can get tricky. For example, a stopping rule based on having all \(P\) values above some fixed number is not equivalent to any stopping rule based on having all \(|t|\) statistics below some fixed number because the \(P\) values depend on the residual degrees of freedom which keep changing. Reasonable stopping rules based on AIC or BIC might be when these no longer decrease or one can stop when Adj. \(R^{2}\) fails to increase. Unfortunately, such stopping rules remove the flexibility that a self-selected stopping rule has to control the extent to which forward selection explores the set of all models. A stopping rule for forward selection based on \(C_{p}\) is rarely appropriate because there is rarely a largest model under consideration. (I would never use forward selection if I could fit a reasonable full model.)

The forward selection process is typically started with the initial model

\[y_{i}=\beta_{0}+e_{i}.\]

#### Example 14.2.2

Big Data.

The data involve 1000 observations on a dependent variable and 100 predictor variables. By big data standards, this is a small set of big data. I performed forward selection with a stopping rule that the \(P\) value to enter the model must be below \(\alpha=0.05\). The table of parameters follows.

The model has a horrible \(R^{2}\) of 0.0287 but it seems that there are several variables that help explain the data.

_None of these variables has any actual effect!_ With two exceptions, all the 101,000 observations are i.i.d. standard normal. The first exception is that the \(y\) observations have a signal added to them. The other exception is that \(x_{49}\) consists of a small multiple of the signal buried within (the negative of) the white noise in \(x_{100}\). The signal in \(x_{49}\) is almost hopelessly lost within that white noise, except for the fact that \(x_{49}+x_{100}\) is precisely the small multiple of the signal. If you have both \(x_{49}\) and \(x_{100}\) in the model, you can extract the signal, but neither \(x_{49}\) nor \(x_{100}\) by itself helps. Any model that contains the two important variables will have \(R^{2}=1.0000\).

I also ran the forward selection stopping when, to get a variable added, its \(P\) value must be below \(\alpha=0.25\) (the Minitab default). The idea of the higher cutoff value is to try to get important combinations of variables, like \(x_{49}\) and \(x_{100}\), into the model so that their joint effect can be seen. It did not work here. The final model included the 5 predictors from \(\alpha=0.05\) plus 17 additional worthless predictors. Again, the model gives poor prediction with \(R^{2}=0.0717\). When using a large \(\alpha\) value, it seems inappropriate to blindly use the final model produced, since it will include a lot of variables with little predictive power. A far better procedure would be to select the best among the sequence of models produced or the best subset of the final model. While these are better procedures, in this example they are better procedures for producing garbage.

Even with \(\alpha=0.75\) and a final model that includes 79 of the 100 predictors, forward selection still did not manage to pick up both of the predictors necessary for getting a good model. Using a large \(\alpha\) increases our chances of picking up the two good predictors, but it picks up a lot of junk predictors also. If we are lucky enough to pick up our two worthwhile predictors, clearly we would want to go back and eliminate the obvious junk. Of course \(\alpha=1\) will always find the two important variables because it will always fit the full model at the end of the sequence. But if you can fit the full model, you should be doing backward elimination or, better yet, best subset selection.

The way this example was constructed, \(\alpha\) is pretty much the probability of finding a good prediction model using forward selection. I did not go looking for a large \(P\) value, but the \(P\) value associated with \(x_{100}\) was particularly large. (Recall that, by itself, \(x_{100}\) is unrelated to \(y\) and independent of all the predictors except \(x_{49}\), so it's \(P\) remains pretty stable in any model that excludes \(x_{49}\). Moreover, because the signal is _buried_ in \(x_{49}\), the same is true about \(x_{49}\) in any model that excludes \(x_{100}\).) Ifthe \(P\) value for \(x_{100}\) is below \(\alpha\), forward selection should find a good model in this example.

The problem of finding "important" effects that are actually meaningless is ubiquitous with big data. It is no accident that with \(\alpha=0.05\) we found about 5% of the meaningless predictors to be significant. It is no accident that with \(\alpha=0.25\) we found about 25% of the meaningless predictors in our model. The more tests you perform, the more meaningless things will look statistically significant. With any big data where nothing is related, you can always find something that looks related. In exploring big data, the usual standards of statistical significance do not apply.

As mentioned earlier, stopping rules based on AIC or BIC failing to decrease or Adj. \(R^{2}\) failing to increase, lack the flexibility in exploring the space of possible models that one gets by selecting a \(P\) value or an absolute \(t\) statistic for inclusion.

#### Backward Elimination

Backward elimination sequentially deletes variables from the model. At any stage in the selection process, it deletes the variable with the smallest absolute \(t\) statistic or \(F\) statistic or equivalent criterion. Backward elimination often stops deleting variables when:

1. \(p_{*}\) variables have been eliminated,
2. the smallest absolute \(t\) statistic for eliminating a variable is greater than \(t_{*}\).

The user can usually specify \(p_{*}\) and \(t_{*}\) in a computer program. Often, the process is stopped when the \(P\) value associated with \(|t|\) is too small. Stopping rules are also based on AIC or BIC failing to decrease or Adj. \(R^{2}\) failing to increase. Traditionally, one just uses the model that stopped the process. Alternatively, one could use a model selection criterion to pick the best among the sequence of models that backward elimination produced.

The initial model in the backward elimination procedure is the model with all of the predictor variables included,

\[y_{i}=\beta_{0}+\beta_{1}x_{i1}+\cdots+\beta_{s}x_{is}+e_{i}.\]

_Backward elimination should give an adequate model_. We assume that the process is started with an adequate model, and so only variables that add nothing are eliminated. The model arrived at may, however, be far from the most succinct. On the other hand, _there is no reason to believe that forward selection gives even an adequate model_.

Since we are starting with an adequate model, unlike forward selection, there is little reason to choose a stopping rule that helps us to explore the space of possible models.

#### 2.2.3 Big Data Continued.

I applied backward selection to the data with a \(P\) value cutoff of (the Minitab default) \(\alpha=0.10\)

\begin{tabular}{l r r r r} \multicolumn{5}{l}{Table of Coefficients: Backward Elimination, \(\alpha=0.10\).} \\ Predictor & \(\hat{\gamma_{i}}\) & SE(\(\hat{\gamma_{i}}\)) & \(t\) & \(P\) \\ \hline Constant & 0.0458 & 0.0604 & 0.76 & 0.449 \\ \(x_{2}\) & 0.0669 & 0.0306 & 2.19 & 0.029 \\ \(x_{34}\) & \(-0.0577\) & 0.0302 & \(-1.91\) & 0.057 \\ \(x_{48}\) & 0.0632 & 0.0308 & 2.05 & 0.040 \\ \(x_{49}\) & 999879 & 104 & 9572.43 & 0.000 \\ \(x_{59}\) & \(-0.0572\) & 0.0311 & \(-1.84\) & 0.066 \\ \(x_{66}\) & 0.0680 & 0.0299 & 2.27 & 0.023 \\ \(x_{95}\) & \(-0.0546\) & 0.0293 & \(-1.86\) & 0.063 \\ \(x_{98}\) & \(-0.0752\) & 0.0303 & \(-2.49\) & 0.013 \\ \(x_{100}\) & 999879 & 104 & 9572.41 & 0.000 \\ \end{tabular}

This is an extreme example so the \(|t|\) statistics for the two important variables leap out. (Their \(P\) values do not!) But the point of this example is not that backward elimination found and kept the important variables. The point of this example is that backward elimination still finds 4 worthless variables that look significant by traditional standards and another 3 worthless variables that one would normally consider to be of marginal significance.

Unlike forward selection, backward elimination gives a good predictive model, one with \(R^{2}=1.00\). The residual mean square is \(RMS=0.906\) which is disturbingly below the correct value 1, even with \(dFE=990\). (Based on the asymptotic normal approximation to the \(\chi^{2}\), \(RMS\) is more than two standard deviations below the true value.) Fitting the full model there are 899 degrees of freedom for error. From the 98 worthless predictor variables there are another 98 sums of squares with 1 degree of freedom that could all go into the error. The seven largest of those 98 sums of squares have been assigned to the model and the 91 smallest have been assigned to the error. Even when starting with 899 degrees of freedom for error from the full model that truly estimate the error, adding in the 91 smallest and leaving out the 7 biggest sums for squares biases the estimated error downward in the fitted backward elimination model.

If the final model from backward elimination is sufficiently small, one could apply best subset selection to the variables in the final model. But that is unlikely to accomplish anything that changing the \(\alpha\) level could not accomplish and it is highly unlikely to eliminate all of the worthless predictor variables. The bigger the data set, the more stringent our requirements for significance should be.

#### Other Methods

Traditional forward selection is such an obviously faulty method that several improvements have been recommended. These consist of introducing rules for eliminating and exchanging variables. Four rules for adding, deleting, and exchanging variables follow.

1. Add the variable with the largest absolute \(t\) value if that value is greater than _t_*.
2. Delete the variable with the smallest absolute \(t\) value if that value is less than _t_*.
3. A variable not in the model is exchanged for a variable in the model if the exchange increases \(R\)2.
4. The largest \(R\)2 for each size model considered so far is saved. Delete a variable if the deletion gives a model with \(R\)2 larger than any other model of the same size.

These rules can be used in combination. For example, 1 then 2, 1 then 2 then 3, 1 then 4, or 1 then 4 then 3. Again, no variable is ever added if its tolerance is too small.

Basically, these rules are just an attempt to get forward selection to look at a broader collection of models and it would be wise to select the best among the sequence of models generated.

#### Big Data Continued

Minitab's default stepwise procedure began with the intercept model, concluded with 17 variables, none of which were the important two.

### Discussion of Traditional Variable Selection Techniques

Stepwise regression methods are fast, easy, cheap, and readily available. When the number of observations, \(n\), is less than the number of variables, \(s\) + 1, forward selection or a modification of it is the only available method for variable selection. Backward elimination and best subset regression assume that one can fit the model that includes all the predictor variables. This is not possible when \(n\) < \(s\) + 1. In fact the use of \(t\) statistics, or anything equivalent to them, is probably unwise unless they are associated with a reasonable number of residual degrees of freedom.

There are serious problems with stepwise methods. They do not give the best model (based on any of the criteria we have discussed). In fact, stepwise methods can give models that contain none of the variables that are in the best regressions. That is because, as mentioned earlier, they handle variables one at a time. Another problem is nontechnical. The user of a stepwise regression program will end up with one model. The user may be inclined to think that this is _the_ model. It probably is not. In fact, _the_ model probably does not exist. Even though Adjusted \(R\)2, Mallows's \(C_{p}\), AIC, AICc, and BIC all define a unique best model, and could be subject to thesame problem, best subset regression programs generally present several of the best models.

A problem with variable selection methods is that they tend to give models that appear to be better than they really are. For example, the Adjusted \(R^{2}\) criterion chooses the model with the smallest _RMS_. Because one has selected the smallest _RMS_, the _RMS_ for that model is biased toward being too small. Almost any measure of the fit of a model is related to the _RMS_, so the fit of the model will appear to be better than it is. If one could sample the data over again and fit the same model, the _RMS_ would almost certainly be larger, perhaps substantially so.

When using Mallows's \(C_{p}\) statistic, if one wants to exploit the virtues of biased estimation, one often picks models with the smallest value of \(C_{p}\). This can be justified by the fact that the model with the smallest \(C_{p}\) is the model with the smallest estimated expected squared error. However, as suggested by Exercise 14.5, if you are looking for a correct model the target value of \(C_{p}\) is the number of predictors, so it seems to make little sense to pick the model with the smallest \(C_{p}\). It seems that one should pick models for which \(C_{p}\) is close to the number of predictors. (I pick models with small \(C_{p}\).)

The result of Exercise 14.7, that for a fixed number of predictor variables the best regression criteria are equivalent, is interesting because the various criteria can be viewed as simply different methods of penalizing models that include more variables. The penalty is needed because models with more variables necessarily explain as much or more variation (have as high or higher \(R^{2}\)s).

#### 14.3.1 \(R^{2}\)

\(R^{2}\) is a good statistic for measuring the predictive ability of a model. \(R^{2}\) is also a good statistic for comparing models. That is what we used it for here. But the actual value of \(R^{2}\) should not be overemphasized when it is being used to identify correct models (rather than models that are merely useful for prediction). If you have data with a lot of variability, it is possible to have a very good fit to the underlying regression model without having a high \(R^{2}\). For example, if the _R_SS admits a decomposition into pure error and lack of fit, it is possible to have very little lack of fit while having a substantial pure error so that \(R^{2}\) is small while the fit is good.

If transformations of the dependent variable \(y\) are considered, it is inappropriate to compare \(R^{2}\) for models based on different transformations. For example, it is possible for a transformation to increase \(R^{2}\) without really increasing the predictive ability of the model. One way to check whether this is happening is to compare the width of confidence intervals for predicted values after transforming them to a common scale.

To compare models based on different transformations of \(y\), say \(y_{1}=f_{1}(y)\) and \(y_{2}=f_{2}(y)\), fit models to the transformed data to obtained predicted values \(\hat{y}_{1}\) and \(\hat{y}_{2}\). Return these to the original scale with \(\tilde{y}_{1}=f_{1}^{-1}(\hat{y}_{1})\) and \(\tilde{y}_{2}=f_{2}^{-1}(\hat{y}_{2})\). Finally, define \(R_{1}^{2}\) as the squared sample correlation between the \(y\)s and the \(\tilde{y}_{1}\)s and define \(R_{2}^{2}\) as the squared sample correlation between the \(y\)s and the \(\tilde{y}_{2}\)s. These \(R^{2}\) values are comparable (and particularly so when the number of parameters in the two fitted models are comparable).

#### Influential Observations

Influential observations are a problem in any regression analysis. Variable selection techniques involve fitting lots of models, so the problem of influential observations is multiplied. Recall that an influential observation in one model is not necessarily influential in a different model.

Some statisticians think that the magnitude of the problem of influential observations is so great as to reject all variable selection techniques. They argue that the models arrived at from variable selection techniques depend almost exclusively on the influential observations and have little to do with any real world effects. Most statisticians, however, approve of the judicious use of variable selection techniques. (But then, by definition, everyone will approve of the _judicious_ use of anything.)

##### Exploratory Data Analysis

John W. Tukey, among others, has emphasized the difference between exploratory and confirmatory data analysis. Briefly, _exploratory data analysis (EDA)_ deals with situations in which you are trying to find out what is going on in a set of data. Confirmatory data analysis is for proving what you already think is going on. EDA frequently involves looking at lots of graphs. _Confirmatory data analysis_ looks at things like tests and confidence intervals. Strictly speaking, you cannot do both exploratory data analysis and confirmatory data analysis on the same set of data.

Variable selection is an exploratory technique. If you know what variables are important, you do not need variable selection and should not use it. When you do use variable selection, if the model is fitted with the same set of data that determined the variable selection, then the model you eventually decide on will give biased estimates and invalid tests and confidence intervals. The biased estimates may very well be better point estimates than a full or correct model gives but tests and confidence intervals are usually over optimistic. Because you typically pick a candidate model partially because it has \(RMS(\gamma)<RMS(\beta)\), confidence intervals are too narrow and tests are too significant.

If you can fit the model with all predictor variables and still have a reasonable \(dFE\), it might be reasonable to perform tests and confidence intervals using least squares on the full model but use biased methods for point estimation and prediction. (With many predictors, you still need to use multiple comparison methods.) The alternative seems to be to use asymptotic or ad hoc methods for inference based directly on biasedestimates. (Bayesians have the best of both worlds in that a proper Bayesian analysis both uses biased estimation and has exact small sample inference methods.)

One solution to this problem of selecting variables and fitting parameters with the same data is to divide the data into two parts. Do an exploratory analysis on one part and then a confirmatory analysis on the other. To do this well requires a lot of data. It also demonstrates the problem of influential observations. Depending on where the influential observations are, you can get pretty strange results. The PRESS statistic was designed to be used in procedures similar to this. However, as we have seen, the PRESS statistic is highly sensitive to influential observations.

#### Multiplicities

Methods of statistical inference were originally developed for situations where data were collected to investigate one thing. The methods work well on the one thing. In reality, even the best studies are designed to look at multiple questions, so the original methods need adjustment. Hence the need for the multiple comparisons methods of Chapter 5.

In an awful lot of studies, people collect data and muck around with it to see what they can find that is interesting. To paraphrase Seymour Geisser, they ransack the data. When mucking around with a lot of data, if you see something interesting, there is a good chance it is just random variation. Even if there is something there, the true effect is probably smaller than it looks. In this context, if you require a statistical test to show that something is important, it probably isn't important. We saw this with the Big Data examples of the previous section and those were examples with very clear structures. The general problem in mucking with the data is that to adjust for ransacking you need to keep track of _everything_ you looked at that could _possibly_ have been interesting. And we are just not psychologically equipped to do that.

#### Predictive Models

Predictive statistical models are based on correlation rather than causation. They work just fine as long as _nothing (important) has changed_ from when the data were collected. You wake up, hear the shower on, you know your dad is making breakfast. Hearing the shower is a good predictor of Dad making breakfast. If you wake up from a nap and hear the shower at 2 in the afternoon, do you think Dad will be making breakfast?

What is the causation behind this prediction? Mom showering? It being 7am? Mom having to be to work at 8?

You cannot figure out what a change does to a system without changing the system! Yet _everybody_ wants to do just that. They want to solve problems by collecting more data on present conditions. The world doesn't work that way. Without changing the system, you only have (hopefully intelligent) guesswork. But guesswork has limited value. Evaluating data from current conditions may provide ideas about what changes to try but it provides no assurance of what those changes will accomplish.

#### Overfitting

A big problem with having \(s\) large relative to \(n\) is the tendency to overfit. _Overfitting_ is the phenomenon of fitting a model with so many parameters that the model looks like it fits the data well, e.g. has a high \(R^{2}\), but does a poor job of predicting future data. Fitting any model with \(r(X)=n\) gives \(R^{2}=1\), so it is easy to overfit regression models just by taking an \(X\) with \(r(X)\doteq n\). Our discussion of variable selection was about making \(X\) smaller (turning \(X\) into a well chosen \(X_{0}\)), so as not to overfit the data. If \(r(X)\doteq n\), forward selection could be applied to try to avoid overfitting. When fitting regression trees and other multiple nonparametric regression models (cf. _ALM-III_, Chapter 1), the set of potential predictor variables is huge and forward selection is used to pick variables that seem appropriate.

Under normal theory for a standard linear model

\[\text{E}(RMS)=\sigma^{2},\qquad\text{Var}(RMS)=2\sigma^{4}/dfE,\]

so the coefficient of variation (CV) is \(\sqrt{2/dfE}\). For _RMS_ is to be a decent estimate, we need CV reasonably small. With \(dfE=2,\,8,\,18\), CV is 1, 1/2, 1/3, so I would like at least 18 \(dfE\), 8 might be tolerable, and using 2 or less is fraught with danger. (I don't want to think about how often I have failed to live up to that prescription.)

Christensen (2015, Section 8.2) shows how bad predictions can be from overfitted models but it incidentally shows how bad the estimated variances are from those overfitted models. For everything except the simple linear regression, his estimated variances were well below the target value of 1.

Some rules I have seen to avoid overfitting require \(n\geq 10p\), \(n\geq 15p\), or \(n\geq 50+8p\). These seem like they should work but they seem awfully stringent.

### Modern Forward Selection: Boosting, Bagging, and Random Forests

For many years, forward selection was dismissed as the poor sibling of variable selection. Forward selection provides no assurance that it will find anything like the best models. Backward elimination, since it begins with a presumably reasonable full model and only does reasonable things to that model, should arrive at a decent model. Looking at the "best" subsets of variables seems like the best thing to do.

But backward elimination and best subset selection both require being able to fit a reasonable full model.

If the number of predictor variables \(s\) is big enough so that \(r(X)=n\), we have a saturated full model. Least squares then gives \(\hat{Y}=Y\), \(SSE=0\), \(d{fE}=0\), and the model will be over-fitted so that predictions of new observations typically are poor. Whenever \(d{fE}\) is small, we have probably over-fitted, making our full-model results dubious. In problems with \(s\doteq n\) or \(s\,>\,n\), forward selection, poor as it is, is about the only game in town. (Principal component regression is another.) _Boosting_, _Bagging_, and _Random Forests_ are more recently developed methods of forward selection by which one can use over-fitting of models to improve predictions. Despite all of the difficulties that arose in our Big Data examples given earlier, by the standards of this section those examples have extremely well behaved data because \(s<<n\). Nonetheless, I do not _believe_ that the improvements presented here are capable of overcoming the specific foreward selection problem built into the Big Data examples, namely that the importance of the pair of variables is not detectable from either variable separately. (Randomly picking variables for a full model could solve the problem with that example.)

Boosting is a biased estimation technique associated with forward selection. Bagging (bootstrap aggregation) involves use of the _bootstrap_ to get more broad based estimates. Random forests are a modification of bagging.

Forward selection starts with some relatively small model and defines a sequence of larger and larger models. The two key features are (1) how to decide which variable gets added next and (2) when to stop adding variables. The traditional method of forward selection ranks variables based on the absolute value of the \(t\) statistic for adding them to the current model (or some equivalent statistic) and chooses the highest ranked variable.

If the predictor variables happen to have equal sample variances, forward selection could use the regression coefficients themselves to rank variables, rather than their associated \(t\) statistics. In general, using regression coefficients rather than \(|t|\) statistics does not seem like a great idea, but in my _quite limited_ experience, the procedure works remarkably similar to Tibshirani's (1996) lasso for standardized predictors.

My primary references for this section were James et al. (2013), Hastie et al. (2016), and Efron and Hastie (2016). This section is different from any other in the book because it contains quite a few of my speculations (clearly marked as such) about how these or related methods _might_ work. (The first such speculation occurred in the previous paragraph.)

#### Boosting

The forward selection method known as _boosting_ involves a sequence of model matrices \(X_{j}\) with ppos \(M_{j}\). The procedure depends on choices for integers \(d\) and \(B\) and a scalar \(k\).

Perform a forward selection from among the predictor vectors in \(X\) to obtain a model with \(d\) predictors,

\[Y=X_{1}\beta_{1}+e.\]

From this obtain fitted values and residuals,

\[\hat{Y}_{1}=M_{1}Y;\quad\hat{e}_{1}=Y-\hat{Y}_{1}.\]

Perform another forward selection using \(\hat{e}_{1}\) as the dependent variable to obtain another model with \(d\) predictors,

\[\hat{e}_{1}=X_{2}\beta_{2}+e.\]

From this obtain fitted values

\[\tilde{Y}_{2}=M_{2}\hat{e}_{1}=M_{2}(I-M_{1})Y.\]

Define overall fitted values

\[\hat{Y}_{2}=\hat{Y}_{1}+k\tilde{Y}_{2}\]

and residuals,

\[\hat{e}_{2}=Y-\hat{Y}_{2}=(I-kM_{2})(I-M_{1})Y.\]

In general, given residuals \(\hat{e}_{j}\) perform a forward selection using \(\hat{e}_{j}\) as the dependent variable to obtain another model with \(d\) predictors,

\[\hat{e}_{j}=X_{j+1}\beta_{j+1}+e.\]

From this obtain fitted values

\[\tilde{Y}_{j+1}=M_{j+1}\hat{e}_{j}.\]

Define overall fitted values

\[\hat{Y}_{j+1}=\hat{Y}_{j}+k\tilde{Y}_{j+1}\]

and residuals,

\[\hat{e}_{j+1}=Y-\hat{Y}_{j+1}=(I-kM_{j+1})\cdots(I-kM_{2})(I-M_{1})Y.\]

The procedure stops when \(j\) reaches the predetermined value \(B\). The accepted wisdom seems to be that picking a stopping point \(B\) that is too large can still result in overfitting the model.

If \(k=1\), so that no shrinkage of the estimates is involved, boosting seems like just a lousy way of fitting

\[Y=[X_{1},\ldots,X_{B}]\delta+e.\]Next we present two adjusted methods that give least squares estimates when \(k=1\).

##### 14.4.1.1 Alternatives

Collect all of the possible predictors into the matrix \(X\). We use ideas related to the sweep operator discussed in Chapter 9. Perform a forward selection from the columns of \(X\) to obtain a model with \(d\) predictors,

\[Y=X_{1}\beta_{1}+e.\]

From this obtain fitted values and residuals,

\[\hat{Y}_{1}=M_{1}Y;\quad\hat{e}_{1}=Y-\hat{Y}_{1}.\]

Adjust all the columns of \(X\) into \(\tilde{X}_{2}=(I-M_{1})X\). Note that \(X\) contains the columns of \(X_{1}\) but these are zeroed out in \(\tilde{X}_{2}\) and should not be eligible for future selection. This adjustment is not a hideously expensive thing to do. The single expensive operation is computing \((X_{1}^{\prime}X_{1})^{-1}\). The other operations are numerous but individually inexpensive.

Perform a forward selection using \(\hat{e}_{1}\) as the dependent variable and \(\tilde{X}_{2}\) as the matrix of possible variables to obtain another model with \(d\) predictors,

\[\hat{e}_{1}=X_{2}\beta_{2}+e.\]

Notice that \(C(X_{1})\perp C(X_{2})\) so \(M_{1}M_{2}=0\). From this obtain overall fitted values

\[\hat{Y}_{2}=\hat{Y}_{1}+kM_{2}\hat{e}_{1}=(M_{1}+kM_{2})Y\]

and residuals,

\[\hat{e}_{2}=Y-\hat{Y}_{2}=(I-M_{1}-kM_{2})Y.\]

In general, given fitted values \(\hat{Y}_{j}\), residuals \(\hat{e}_{j}\) and the matrices \(X_{j}\) and \(\tilde{X}_{j}\), construct the possible additions \(\tilde{X}_{j+1}\equiv(I-M_{j})\tilde{X}_{j}\) in which all variables that are already in the model will have been zeroed out.

Perform a forward selection using \(\hat{e}_{j}\) as the dependent variable with the columns of \(\tilde{X}_{j+1}\) as potential predictors to obtain another model with \(d\) predictors,

\[\hat{e}_{j}=X_{j+1}\beta_{j+1}+e.\]

Again, all of the \(C(X_{k})\)s are orthogonal. From this model obtain overall fitted values

\[\hat{Y}_{j+1}=\hat{Y}_{j}+kM_{j+1}\hat{e}_{j}=[M_{1}+k(M_{2}+\cdots+M_{j+1})]Y\]and residuals

\[\hat{e}_{j+1}=Y-\hat{Y}_{j+1}=(I-kM_{j+1})\hat{e}_{j}.\]

The accepted wisdom that, picking a stopping point \(B\) that is too large can still result in overfitting the model, seems related to the fact that the penalty term \(k\) remains the same for every step after the first. An alternative method, akin to exponential smoothing, might do better by defining

\[\hat{Y}_{j+1}=\hat{Y}_{j}+k^{j}\hat{Y}_{j+1}=(M_{1}+kM_{2}+\cdots+k^{j}M_{j+1})Y.\]

The alternatives mentioned here are based on the idea that it is the shrinkage of estimates that is valuable in boosting. Although I don't see how it could be true, it is possible that the very awkwardness of adding nonorthogonalized variables could be of some benefit. Boosting was originally developed for binomial regression problems and I _suspect_ behaves quite differently there.

#### Bagging

Bagging is a technique described by Hastie et al. (2016, p. 282) as "how to use the bootstrap to improve the estimate or prediction." We will see that bagging can be useful but cannot be a panacea.

The fundamental idea of _bagging_ (as I see it) follows: Suppose you have an algorithm for fitting a model to a set of data with \(n\) observations. In bagging this should be an algorithm that tends to overfit the data. Take a random sample with replacement of size \(n\) from your data. Apply your algorithm on this sample of data and obtain your desired results: predictions or estimates. Do this repeatedly for many random samples and average your predictions/estimates over these samples. These averages are the result of bagging. The hope is that these averages will be better predictions and estimates than the results of the original algorithm applied just once to the original data. We will see, in a simple example, that the better the algorithm, the less likely this is to be true. But in situations where we do not know how to create a good algorithm for the particular data, bagging can provide valuable improvements.

With reasonably large collections of data, the gold standard for determining the quality of a predictive model seems to be: (1) randomly pull out a set of _test data_, (2) use the remaining _training data_ to develop the predictive model, and (3) evaluate the predictive model by seeing how well it predicts the test data. This is frequently used to compare the quality of various methods of developing predictive models. In this context, overfitting consists of fitting a model that explains the training data very well but does a poor job of predicting the test data.

If we think of the collection of observed data as the entire population of possible data, and randomly select the test data, then the training data is also just a random sample from the population. It should display the same predictive relationships as the overall population. If the goal is to predict a random sample (the test sample)from the population, why not use random samples from the population to develop a predictor? Moreover, we want to use overfitting to help, rather than hinder, the predictive process.

The idea is to start with a model selection procedure that is capable of modeling the salient features in the data. In multiple nonparametric regression (_ALM-III_, Chapter 1) that involves constructing additional predictor variables that make the number of predictor variables \(s\) very large indeed. Different nonparametric regression procedures have different modeling capabilities (they create different model matrices \(X\)), so the best choice of a procedure depends on the nature of the data in ways that we will rarely understand beforehand. But merely having an \(X\) matrix with \(s>>n\) is not enough. To develop a prediction model we must have some variable selection scheme, presumably a form of forward selection, that includes enough predictor variables to capture the salient features of the data, and to do this consistently seems to require some overfitting.

Take a random sample from the training data and overfit a model on it. This ensures that the salient features that are present in all samples are caught but overfitting will also include features that are unique to the particular sample being fitted. Do this for many random samples (say \(B\)) and average the results. The hope is that the salient features will appear in a similar fashion in every sample but that the unique (random) features that occur from overfitting particular samples will average themselves out over the process of repeated sampling.

Bagging is a very complicated procedure. In fact, it is notorious for providing (good) predictions that are uninterpretable. We now examine an extremely simple example of bagging to explore how it actually works.

##### A Simple Example

Consider a random variable \(y\). With no potential predictor variables available, the best predictor (BP) under squared error prediction loss is \(\text{E}(y)=\mu_{y}\equiv\mu\). Now suppose we have a random sample from \(y\), say \(Y_{n\times 1}\). The best nonparametric estimate of \(\mu\) is \(\bar{y}_{\cdot}=J^{\prime}Y/n\). We know it is the BLUE but for a class of distributions that includes nearly all continuous distributions that have a mean, \(\bar{y}_{\cdot}\) is minimum variance unbiased, cf. Fraser (1957). If the distribution of \(y\) is normal, \(\bar{y}_{\cdot}\) is again minimum variance unbiased but it is much easier to be unbiased for normal distributions than it is for all continuous distributions. For uniform distributions with unknown limits, the best unbiased estimate of the expected value is the midrange. For symmetric distributions with heavy tails, e.g. Laplace (double exponential), the median tends to be a good estimate. In this context, \(\bar{y}_{\cdot}\) is always going to be a reasonably good estimate of the BP. Generally, for light-tailed symmetric distributions, like the uniform with unknown limits, the midrange can be a good estimate of the BP but the median will be less good. For heavy-tailed symmetric distributions, the median can be a good estimate of the BP but the midrange will be less good. Neither the median nor the midrange are linear functions of the data.

Bagging brings an element of averaging into the estimates that has virtually no effect on the linear estimate \(\bar{y}\). and cannot improve an optimal estimate, but bagging can substantially improve a poor estimate and can even improve good but suboptimal estimates.

#### Example 14.4.1

In the spirit of fitting a number of parameters that is a large proportion of the number of observations (and just to be able to perform the computations), suppose we have a simple random sample of size \(n=3\) with order statistics \(y_{(1)}<y_{(2)}<y_{(3)}\). The best predictor is the population mean, which we want to estimate. The midrange \((y_{(1)}+y_{(3)})/2\) is optimal for uniform distributions with unknown end points. The median \(y_{(2)}\) works well for heavy tailed distributions. The sample mean \((y_{(1)}+y_{(2)}+y_{(3)})/3\) is optimal for normal data or extremely broad (nonparametric) families of distributions.

Normally one would not bootstrap a sample this small but the small sample size allows us to examine what Hastie et al. (2016) refer to as the _"true" bagging estimate_. With only 3 observations, bootstrapping takes samples from a population that has 27 equally probable outcomes: Three of the outcomes are \(\{y_{(j)},y_{(j)},y_{(j)}\}\) for \(j=1,2,3\). Six of the outcomes are reorderings of \(\{y_{(1)},y_{(2)},y_{(3)}\}\). The other 18 outcomes involve the three reorderings one can get from samples of the form \(\{y_{(j)},y_{(j)},y_{(k)}\}\), \(j=1,2,3\), \(k\neq j\) there being six distinct outcomes of this form.

From each of the 27 outcomes in the bootstrap population we can compute the sample mean, median, and midrange statistics. The bootstrap procedure actually provides an estimate (one that we can make arbitrarily good by picking \(B\) large) of the expected value over the 27 equally probable outcomes of these statistics (sample mean, median and midrange). We want to know what function of the observed data the bootstrap is estimating, because that is the function of the data that the bootstrap uses to estimate the BP (as \(B\to\infty\)).

The expected value of the sample mean is easily seen to be \((9y_{(1)}+9y_{(2)}+9y_{(3)})/27\), so unsurprisingly the bootstrap of the sample mean is estimating the sample mean of the original data. The bootstrap expected value of the sample median is \((7y_{(1)}+13y_{(2)}+7y_{(3)})/27\), which is a symmetric weighted average of the original observations; one that puts more weight on the middle observation. The bootstrap expected value of the midrange is \((10y_{(1)}+7y_{(2)}+10y_{(3)})/27\), which is again a symmetric weighted average of the original observations but one that puts less weight on the middle observation.

Another way to think about this is that the bagged median is estimating

\[(14/27)\text{midrange}+(13/27)\text{median}\]

and the bagged midrange is estimating

\[(20/27)\text{midrange}+(7/27)\text{median}.\]But perhaps more importantly, _both of them are closer to the sample mean than they were originally_.

For a uniform distribution, where the midrange is optimal, the bagged midrange estimate will be less good because it puts too much weight on the middle observation. However, the bagged median will be better than the median because the bagged median puts more weight on the midrange.

When the median is good, the bagged median will be less good because it puts more weight on the extreme observations. However the bagged midrange will be better than the midrange because the bagged midrange puts more weight on the median.

Bagging the sample mean is a waste of effort. Because the sample mean is the best nonparametric estimate, the sample mean is never going to be too bad.

_If you don't know the distribution of the data, which you almost never do, you might as well use the sample mean and bagging is irrelevant. Bagging would be useful if for some reason you cannot use the sample mean._

The three estimates we examined were all unbiased for symmetric distributions. Lets look at a biased estimate of the mean. Consider the estimate \((y_{(2)}+y_{(3)})/2\) which is clearly biased above the mean for symmetric distributions. The bootstrapped estimate has expected value (\(4y_{(1)}+10y_{(2)}+13y_{(3)})/27\), which, while still heavily biased above the mean, is considerably less biased than the original estimate.

##### 14.4.2.2 Discussion

The prediction problem is to estimate the best predictor, \(\operatorname{E}(y|x)\). The more you know about the conditional distribution of \(y\) given \(x\), the easier the problem becomes. By expanding our definition of \(x\), e.g. incorporating polynomials, we can often ensure that \(\operatorname{E}(y|x)\) is approximately linear in \(x\). If we have enough data to fit a full model, we should. For homoscedastic, uncorrelated data, least squares estimates are BLUEs, so they are probably about as good as we can do to start, but then we may be able to get better point estimates by incorporating bias. If \(s>n\), we cannot fit the full model in any meaningful way, so we need some way of constructing a predictive model and typically that involves some form of forward selection. We know forward selection does not work well, so it is unlikely to give good estimates of the best predictor. When you have poor estimates, bagging seems to be good at improving them by averaging them over more of the data. (My _hope_ is that the bagging estimates will move them closer to appropriate estimates for the [unfitable] full model.) While it is relatively easy to see that bagging has no systematic effect on estimates that are linear functions of the data, forward selection is a profoundly nonlinear estimation process.

#### Random Forests

The _random forest_ idea modifies the forward selection procedure in conjunction with bagging. The name derives from applying the idea to regression trees.

Divide the predictor variables into \(G\) groups. The modification to forward selection is that instead of considering all of the variables as candidates for selection, one randomly chooses \(m\) of the \(G\) groups as candidates for forward selection. If you were only fitting one model, that would be a disastrous idea, but in the context of bagging, all of the important variables should show up often. Typically one takes \(m\doteq G/3\) or \(m\doteq\sqrt{G}\).

##### Polynomial Regression.

Division into \(G\) groups occurs naturally in polynomial regression and many other nonparametric regression procedures that, like polynomial regression, begin with, say \(Q\), measured predictor variables and define functions of those measured variables. A full polynomial model on \(Q\) measured variables \(x_{1},\ldots,x_{Q}\) is

\[y_{i}=\sum_{j_{1}=0}^{d_{1}}\cdots\sum_{j_{Q}=0}^{d_{Q}}\beta_{j_{i}\cdots j_{s }}\,x_{i1}^{j_{1}}\cdots x_{iQ}^{j_{Q}}+\varepsilon_{i},\]

so the total number of predictor variables is \(s=\prod_{j=1}^{Q}d_{j}\). The \(G\) groups can conveniently be taken as \(x_{k}^{j_{k}}:j_{k}=1,\ldots,d_{k}\) for \(k=1,\ldots,Q\) which makes \(G=Q\). Instead of considering all of the variables \(x_{k}^{j_{k}}\), \(j_{k}=1,\ldots,d_{k}\), \(k=1,\ldots,Q\) as candidates for selection, one randomly chooses \(m\) of the \(Q\) groups as candidates for forward selection. (Forward selection typically will not result in a hierarchical polynomial that contains all the lower order terms for every term in the polynomial. Although good arguments can be made for using hierarchical polynomials, they seem inconsistent with the spirit of forward selection.)

##### Big Data.

Divide the 100 predictors into 9 groups all but one having 11 predictor variables. Randomly pick 3 groups to be included in the variable selection. In this example, with almost everything being independent, it is hard to see how the random forest idea is going to help. Much of the time the two worthwhile predictors will not even be available for selection in the model. And even when the two good predictors are both available, nothing has happened that will increase their chances of being selected. Remember, we have to have an algorithm that randomly gets one of the two predictors into the model. Once one of them is in the model, forward selection will find the second variable (if it is available to find). So the random forest idea (or bagging alone) does not seem to help in this example, but then forward selection on these data is not a method well suited for finding the salient characteristics at the expense of some overfitting.

I would be tempted to define subgroups of variables for which a random selection would give something I consider a plausible full model and rather than averaging them all, actually look for good ones. But in the era of big data, there seems to be a premium on procedures you can run without having to supervise them.

##### Exercise 14.9

Compare the results of using the various model selection techniques on the data of Exercise 13.5.

## References

* Bedrick & Tsai (1994) Bedrick, E. J., & Tsai, C.-L. (1994). Model selection for multivariate regression in small samples. _Biometrics_, _50_, 226-231.
* Cavanaugh (1997) Cavanaugh, J. E. (1997). Unifying the derivations of the Akaike and corrected Akaike information criteria. _Statistics and Probability Letters_, _31_, 201-208.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton, FL: Chapman and Hall/CRC Pres.
* Christensen et al. (2010) Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton, FL: Chapman and Hall/CRC Press.
* Cook et al. (2013) Cook, R. D., Forzani, L., & Rothman, A. J. (2013). Prediction in abundant high-dimensional linear regression. _Electronic Journal of Statistics_, \(7\), 3059-3088.
* Cook et al. (2015) Cook, R. D., Forzani, L., & Rothman, A. J. (2015). Letter to the editor. _The American Statistician_, _69_, 253-254.
* Efron & Hastie (2016) Efron, B., & Hastie, T. (2016). _Computer age statistical inference: Algorithms, evidence, and data science_. Cambridge: Cambridge University Press.
* Fraser (1957) Fraser, D. A. S. (1957). _Nonparametric methods in statistics_. New York: Wiley.
* Furnival & Wilson (1974) Furnival, G. M., & Wilson, R. W. (1974). Regression by leaps and bounds. _Technometrics_, _16_, 499-511.
* Hastie et al. (2016) Hastie, T., Tibshirani, R., & Friedman, J. (2016). _The elements of statistical learning: Data mining, inference, and prediction_ (2nd ed.). New York: Springer.
* Hurvich & Tsai (1989) Hurvich, C. M., & Tsai, C.-L. (1989). Regression and time series model selection in small samples. _Biometrika_, _76_, 297-307.
* James et al. (2013) James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). _An introduction to statistical learning_. New York: Springer.
* Schatzoff et al. (1968) Schatzoff, M., Tsao, R., & Fienberg, S. (1968). Efficient calculations of all possible regressions. _Technometrics_, _10_, 768-779.
* Schwarz (1978) Schwarz, G. (1978). Estimating the dimension of a model. _Annals of Statistics_, \(6\), 461-464.
* Sugiura (1978) Sugiura, N. (1978). Further analysis of the data by Akaike's information criterion and the finite corrections. _Communications in Statistics, Part A, Theory and Methods_, \(7\), 13-26.
* Tarpey et al. (2015) Tarpey, T., Ogden, R., Petkova, E., & Christensen, R. (2015). Reply. _The American Statistician_, _69_, 254-255.

## Appendix A Vector Spaces

This appendix reviews some of the basic definitions and properties of vector spaces. It presumes some basic knowledge of matrices.

**Definition A.1**.: A set \(\mathcal{M}\) is a _vector space_ if, for any \(x\), \(y\), \(z\in\mathcal{M}\) and scalars \(\alpha\), \(\beta\), operations of vector addition and scalar multiplication are defined such that:

(1) \((x+y)+z=x+(y+z)\).

(2) \(x+y=y+x\).

(3) There exists a vector \(0\in\mathcal{M}\) such that \(x+0=x=0+x\) for any \(x\in\mathcal{M}\).

(4) For any \(x\in\mathcal{M}\), there exists \(y\equiv-x\) such that \(x+y=0=y+x\).

(5) \(\alpha(x+y)=\alpha x+\alpha y\).

(6) \((\alpha+\beta)x=\alpha x+\beta x\).

(7) \((\alpha\beta)x=\alpha(\beta x)\).

(8) There exists a scalar \(\xi\) such that \(\xi x=x\). (Typically, \(\xi=1\).)

We rely on context to distinguish between the vector \(0\in\mathcal{M}\) and the scalar \(0\in\mathbf{R}\). In most of our applications, we assume \(\mathcal{M}\subset\mathbf{R}^{n}\). Vectors in \(\mathbf{R}^{n}\) will be considered as \(n\times 1\) matrices. The \(0\) vector referred to in Definition A.1 is just an \(n\times 1\) matrix of zeros.

**Definition A.2**.: Let \(\mathcal{M}\) be a vector space, and let \(\mathcal{N}\) be a set with \(\mathcal{N}\subset\mathcal{M}\). \(\mathcal{N}\) is a _subspace_ of \(\mathcal{M}\) if \(\mathcal{N}\) is a vector space using the same definitions of vector addition and scalar multiplication as for \(\mathcal{M}\).

Thinking of vectors in three dimensions as \((x,y,z)^{\prime}\), where \(w^{\prime}\) denotes the _transpose_ of a matrix \(w\). The subspace consisting of the \(z\) axis is

\[\left\{\begin{pmatrix}0\\ 0\\ z\end{pmatrix}\ \middle|\ z\in\mathbf{R}\right\}.\]The subspace consisting of the \(x\), \(y\) plane is

\[\left\{\begin{pmatrix}x\\ y\\ 0\end{pmatrix}\,\middle|\,\,x,\,y\in\mathbf{R}\right\}.\]

The subspace consisting of the plane that is perpendicular to the line \(x=y\) in the \(x\), \(y\) plane is

\[\left\{\begin{pmatrix}x\\ -x\\ z\end{pmatrix}\,\middle|\,\,x,\,z\in\mathbf{R}\right\}.\]

**Theorem A.3**: _Let \(\mathcal{M}\) be a vector space, and let \(\mathcal{N}\) be a nonempty subset of \(\mathcal{M}\). If \(\mathcal{N}\) is closed under vector addition and scalar multiplication, then \(\mathcal{N}\) is a subspace of \(\mathcal{M}\)._

**Theorem A.4**: _Let \(\mathcal{M}\) be a vector space, and let \(x_{1},\ldots,x_{r}\) be in \(\mathcal{M}\). The set of all linear combinations of \(x_{1},\ldots,x_{r}\), i.e., \(\{v\,|\,v=\alpha_{1}x_{1}+\cdots+\alpha_{r}x_{r},\,\alpha_{i}\in\mathbf{R}\}\), is a subspace of \(\mathcal{M}\)._

**Definition A.5**: The set of all linear combinations of \(x_{1},\ldots,x_{r}\) is called the _space spanned by \(x_{1},\ldots,x_{r}\)_. If \(\mathcal{N}\) is a subspace of \(\mathcal{M}\), and \(\mathcal{N}\) equals the space spanned by \(x_{1},\ldots,x_{r}\), then \(\{x_{1},\ldots,x_{r}\}\) is called a _spanning set_ for \(\mathcal{N}\).

The space spanned by the vectors

\[x_{1}=\begin{pmatrix}1\\ 1\\ 1\end{pmatrix},\quad x_{2}=\begin{pmatrix}1\\ 0\\ 0\end{pmatrix}\]

consists of all vectors of the form \((a,\,b,\,b)^{\prime}\), where \(a\) and \(b\) are any real numbers.

Let \(A\) be an \(n\,\times\,p\) matrix. Each column of \(A\) is a vector in \(\mathbf{R}^{n}\). The space spanned by the columns of \(A\) is called the _column space_ of \(A\) and written \(C(A)\). (Some people refer to \(C(A)\) as the _range space_ of \(A\) and write it \(R(A)\).) If \(B\) is an \(n\,\times\,r\) matrix, then \(C(A,\,B)\) is the space spanned by the \(p\,+\,r\) columns of \(A\) and \(B\).

**Definition A.6**: Let \(x_{1},\ldots,x_{r}\) be vectors in \(\mathcal{M}\). If there exist scalars \(\alpha_{1},\ldots,\alpha_{r}\) not all zero so that \(\sum\alpha_{i}x_{i}=0\), then \(x_{1},\ldots,x_{r}\) are _linearly dependent_. If such \(\alpha_{i}\)s do not exist, i.e., if \(\sum\alpha_{i}x_{i}=0\) implies that \(\alpha_{i}=0\) for all \(i\), then \(x_{1},\ldots,x_{r}\) are _linearly independent_.

**Definition A.7**: If \(\mathcal{N}\) is a subspace of \(\mathcal{M}\) and if \(\{x_{1},\ldots,x_{r}\}\) is a linearly independent spanning set for \(\mathcal{N}\), then \(\{x_{1},\ldots,x_{r}\}\) is called a _basis_ for \(\mathcal{N}\).

**Theorem A.8**: _If \(\mathcal{N}\) is a subspace of \(\mathcal{M}\), all bases for \(\mathcal{N}\) have the same number of vectors._

**Definition A.9**: The _rank_ of a subspace \(\mathcal{N}\) is the number of elements in a basis for \(\mathcal{N}\). The rank is written \(r(\mathcal{N})\). If \(A\) is a matrix, the rank of \(C(A)\) is called the rank of \(A\) and is written \(r(A)\), i.e., \(r(A)\equiv r[C(A)]\).

Exercise B.24 establishes that \(r(A)=r(A^{\prime})\).

**Theorem A.10**: _If \(v_{1},\ldots,v_{r}\) is a basis for \(\mathcal{N}\), and \(x\in\mathcal{N}\), then the characterization \(x=\sum_{i=1}^{r}\alpha_{i}v_{i}\) is unique._

_Proof_ Suppose \(x=\sum_{i=1}^{r}\alpha_{i}v_{i}\) and \(x=\sum_{i=1}^{r}\beta_{i}v_{i}\). Then \(0=\sum_{i=1}^{r}(\alpha_{i}-\beta_{i})v_{i}\). Since the vectors \(v_{i}\) are linearly independent, \(\alpha_{i}-\beta_{i}=0\) for all \(i\). \(\square\)

The vectors

\[x_{1}=\begin{pmatrix}1\\ 1\\ \end{pmatrix},\quad x_{2}=\begin{pmatrix}1\\ 0\\ 0\\ \end{pmatrix},\quad x_{3}=\begin{pmatrix}2\\ 3\\ 3\\ \end{pmatrix}\]

are linearly dependent because \(0=3x_{1}-x_{2}-x_{3}\). Any two of \(x_{1},x_{2},x_{3}\) form a basis for the space of vectors with the form \((a,b,b)^{\prime}\). This space has rank 2.

**Definition A.11**: Let \(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\) be vector subspaces. The sum of \(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\) is \(\mathcal{N}_{1}+\mathcal{N}_{2}\equiv\{x|x=x_{1}+x_{2}\text{ for some }x_{1}\in\mathcal{N}_{1},x_{2}\in\mathcal{N}_{2}\}\). This is sometimes called the _direct sum_ and written \(\mathcal{N}_{1}\oplus\mathcal{N}_{2}\).

**Theorem A.12**: \(\mathcal{N}_{1}+\mathcal{N}_{2}\) _is a vector space and \(C(A,B)=C(A)+C(B)\)._

**Theorem A.13**: _Let \(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\) be vector subspaces. If \(\mathcal{N}_{1}\cap\mathcal{N}_{2}=\{0\}\), then any vector \(x\in\mathcal{N}_{1}+\mathcal{N}_{2}\) has a unique decomposition \(x=x_{1}+x_{2}\) with \(x_{1}\in\mathcal{N}_{1}\) and \(x_{2}\in\mathcal{N}_{2}\)._

_Proof_ Let \(x=x_{1}+x_{2}\), \(x_{1}\in\mathcal{N}_{1}\), \(x_{2}\in\mathcal{N}_{2}\) and \(x=y_{1}+y_{2}\), \(y_{1}\in\mathcal{N}_{1}\), \(y_{2}\in\mathcal{N}_{2}\). Then \(x_{1}+x_{2}=x=y_{1}+y_{2}\), so \(x_{1}-y_{1}=y_{2}-x_{2}\equiv v\). But \(v\equiv x_{1}-y_{1}\in\mathcal{N}_{1}\) and \(v=y_{2}-x_{2}\in\mathcal{N}_{2}\), so \(v\in\mathcal{N}_{1}\cap\mathcal{N}_{2}=\{0\}\) and \(x_{1}=y_{1}\) and \(y_{2}=x_{2}\). \(\square\)

**Theorem A.14**: _Let \(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\) be vector subspaces. If \(\mathcal{N}_{1}\cap\mathcal{N}_{2}=\{0\}\), then \(r(\mathcal{N}_{1}+\mathcal{N}_{2})=r(\mathcal{N}_{1})+r(\mathcal{N}_{2})\).__Proof_ Let \(v_{1},\ldots,v_{r}\) be a basis for \(\mathcal{N}_{1}\) and \(w_{1},\ldots,w_{s}\) be a basis for \(\mathcal{N}_{2}\). It suffices to show that \(v_{1},\ldots,v_{r},w_{1},\ldots,w_{s}\) is a basis for \(\mathcal{N}_{1}+\mathcal{N}_{2}\). Clearly, \(v_{1},\ldots,v_{r},w_{1},\ldots,w_{s}\) is a spanning set for \(\mathcal{N}_{1}+\mathcal{N}_{2}\). If the vectors are linearly independent, the result is proven.

Suppose \(0=\sum_{i=1}^{r}\alpha_{i}v_{i}+\sum_{j=1}^{s}\beta_{j}w_{j}\) which implies that

\[\sum_{i=1}^{r}\alpha_{i}v_{i}=\sum_{j=1}^{s}-\beta_{j}w_{j}.\]

The left-hand side of the equation is a vector in \(\mathcal{N}_{1}\) and the right-hand side is a vector in \(\mathcal{N}_{2}\), so it is a vector in both \(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\), hence must be in the intersection which, by assumption, is the 0 vector. Since both \(0=\sum_{i=1}^{r}\alpha_{i}v_{i}\) and \(0=\sum_{j=1}^{s}\beta_{j}w_{j}\) and the \(v_{i}\)s and \(w_{j}\)s are each a basis, we must have \(0=\alpha_{1}=\cdots=\alpha_{r}=\beta_{1}=\cdots=\beta_{s}\), hence \(v_{1},\ldots,v_{r},w_{1},\ldots,w_{s}\) are linearly independent and a basis. It follows immediately that \(r(\mathcal{N}_{1}+\mathcal{N}_{2})=r+s=r(\mathcal{N}_{1})+r(\mathcal{N}_{2})\). \(\square\)

**Definition A.15**: The (Euclidean) _inner product_ between two vectors \(x\) and \(y\) in \(\mathbf{R}^{n}\) is \(x^{\prime}y\). Two vectors \(x\) and \(y\) are _orthogonal_ (written \(x\perp y\)) if \(x^{\prime}y=0\). Two subspaces \(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\) are orthogonal if \(x\in\mathcal{N}_{1}\) and \(y\in\mathcal{N}_{2}\) implies that \(x^{\prime}y=0\). \(\{x_{1},\ldots,x_{r}\}\) is an _orthogonal basis_ for a space \(\mathcal{N}\) if \(\{x_{1},\ldots,x_{r}\}\) is a basis for \(\mathcal{N}\) and for \(i\neq j\), \(x_{i}^{\prime}x_{j}=0\). \(\{x_{1},\ldots,x_{r}\}\) is an _orthonormal basis_ for \(\mathcal{N}\) if \(\{x_{1},\ldots,x_{r}\}\) is an orthogonal basis and \(x_{i}^{\prime}x_{i}=1\) for \(i=1,\ldots,r\). _The terms orthogonal and perpendicular are used interchangeably_. The _length_ of a vector \(x\) is \(\|x\|\equiv\sqrt{x^{\prime}x}\). The _distance_ between two vectors \(x\) and \(y\) is the length of their difference, i.e., \(\|x-y\|\).

The lengths of the vectors given earlier are

\[\|x_{1}\|=\sqrt{1^{2}+1^{2}+1^{2}}=\sqrt{3},\quad\|x_{2}\|=1,\quad\|x_{3}\|= \sqrt{22}\doteq 4.7.\]

If \(x=(2,1)^{\prime}\), its length is \(\|x\|=\sqrt{2^{2}+1^{2}}=\sqrt{5}\). If \(y=(3,2)^{\prime}\), the distance between \(x\) and \(y\) is the length of \(x-y=(2,1)^{\prime}-(3,2)^{\prime}=(-1,-1)^{\prime}\), which is \(\|x-y\|=\sqrt{(-1)^{2}+(-1)^{2}}=\sqrt{2}\).

Just prior to Section B.4 and in Sections 2.7 and 6.3 we discuss more general versions of the concepts of inner product and length. In particular, a more general version of Definition A.15 is given in Subsection 6.3.5. The remaining results and definitions in this appendix are easily extended to general inner products.

Our emphasis on orthogonality and our need to find orthogonal projection matrices make both the following theorem and its proof fundamental tools in linear model theory:

**Theorem A.16**.: **The Gram-Schmidt Theorem.**

_Let \(\mathcal{N}\) be a space with basis \(\{x_{1},\ldots,x_{r}\}\). There exists an orthonormal basis for \(\mathcal{N}\), say \(\{y_{1},\ldots,y_{r}\}\), with \(y_{s}\) in the space spanned by \(x_{1},\ldots,x_{s}\), \(s=1,\ldots,r\)._

Proof.: The Gram-Schmidt algorithm defines the \(y_{i}\)s inductively:

\[y_{1} = x_{1}\Big{/}\sqrt{x_{1}^{\prime}x_{1}}\,,\] \[w_{s} = x_{s}-\sum_{i=1}^{s-1}(x_{s}^{\prime}y_{i})y_{i}\,,\] \[y_{s} = w_{s}\Big{/}\sqrt{w_{s}^{\prime}w_{s}}\,.\]

See Exercise A.1. 

This result is written using the Euclidean inner product, but terms like \(x_{s}^{\prime}y_{i}\) can be replaced with any general inner product, say, \(\langle x_{s},y_{i}\rangle\).

You can also apply the Gram-Schmidt algorithm to an arbitrary spanning set, rather than to a basis. It still gives an orthonormal basis for the space originally spanned but it involves bookkeeping issues. If the spanning set has linear dependencies, some of the vectors \(w_{j}\) in the algorithm will be 0 vectors, so they need to be dropped from the orthonormal basis.

The vectors

\[x_{1}=\begin{pmatrix}1\\ 1\\ 1\end{pmatrix},\quad x_{2}=\begin{pmatrix}1\\ 0\\ 0\end{pmatrix}\]

are a basis for the space of vectors with the form \((a,b,b)^{\prime}\). To orthonormalize this basis, take \(y_{1}=x_{1}/\sqrt{3}\). Then take

\[w_{2}=\begin{pmatrix}1\\ 0\\ 0\end{pmatrix}-\frac{1}{\sqrt{3}}\begin{pmatrix}1/\sqrt{3}\\ 1/\sqrt{3}\\ 1/\sqrt{3}\end{pmatrix}=\begin{pmatrix}2/3\\ -1/3\\ -1/3\end{pmatrix}\,.\]

Finally, normalize \(w_{2}\) to give

\[y_{2}=w_{2}\Big{/}\sqrt{6/9}=(2/\sqrt{6},-1/\sqrt{6},-1/\sqrt{6})^{\prime}.\]

Note that another orthonormal basis for this space consists of the vectors

\[z_{1}=\begin{pmatrix}0\\ 1/\sqrt{2}\\ 1/\sqrt{2}\end{pmatrix},\quad z_{2}=\begin{pmatrix}1\\ 0\\ 0\end{pmatrix}.\]The result of Gram-Schmidt depends on the order in which you list the vectors in the basis. If you change the order, typically you get a different orthonormal basis. In fact, the \(z_{i}\)s are the vectors you get if you change the order of the two \(x_{i}\)s.

**Definition A.17**.: For \(\mathcal{N}\) a subspace of \(\mathcal{M}\), let \(\mathcal{N}^{\perp}_{\mathcal{M}}\equiv\{y\in\mathcal{M}|y\perp\mathcal{N}\}\). \(\mathcal{N}^{\perp}_{\mathcal{M}}\) is called the _orthogonal complement_ of \(\mathcal{N}\) with respect to \(\mathcal{M}\). If \(\mathcal{M}\) is taken as \(\mathbf{R}^{n}\), then \(\mathcal{N}^{\perp}\equiv\mathcal{N}^{\perp}_{\mathbf{R}^{n}}\) is simply referred to as the orthogonal complement of \(\mathcal{N}\).

**Corollary A.18**.: _For any subspace \(\mathcal{N}\), \(\mathcal{N}\cap\mathcal{N}^{\perp}=\{0\}\)._

Proof.: For any \(x\in\mathcal{N}\cap\mathcal{N}^{\perp}\), the vector must be orthogonal to itself, so \(x^{\prime}x=0\) and \(x=0\). 

**Theorem A.19**.: _Let \(\mathcal{M}\) be a vector space, and let \(\mathcal{N}\) be a subspace of \(\mathcal{M}\). \(\mathcal{N}^{\perp}_{\mathcal{M}}\) is a subspace of \(\mathcal{M}\) and \(\mathcal{M}=\mathcal{N}+\mathcal{N}^{\perp}_{\mathcal{M}}\)._

Before proving Theorem A.19 we state and prove the following corollary and give examples.

**Corollary A.20**.: _Any vector \(x\in\mathcal{M}\) can be written uniquely as \(x=x_{1}+x_{2}\) with \(x_{1}\in\mathcal{N}\) and \(x_{2}\in\mathcal{N}^{\perp}_{\mathcal{M}}\). Moreover, \(r\left(\mathcal{M}\right)=r\left(\mathcal{N}\right)+r\left(\mathcal{N}^{\perp} _{\mathcal{M}}\right)\)._

Proof.: If \(\mathcal{M}=\mathcal{N}+\mathcal{N}^{\perp}_{\mathcal{M}}\), the results are immediate from applying Corollary A.18 and Theorems A.13 and A.14. 

Let \(\mathcal{M}=\mathbf{R}^{3}\) and let \(\mathcal{N}\) be the space of vectors with the form \((a,b,b)^{\prime}\). It is not difficult to see that the orthogonal complement of \(\mathcal{N}\) consists of vectors of the form \((0,c,-c)^{\prime}\). Any vector \((x,y,z)^{\prime}\) can be written uniquely as

\[\begin{pmatrix}x\\ y\\ z\end{pmatrix}=\begin{pmatrix}x\\ (y+z)/2\\ (y+z)/2\end{pmatrix}+\begin{pmatrix}0\\ (y-z)/2\\ -(y-z)/2\end{pmatrix}.\]

The space of vectors with form \((a,b,b)^{\prime}\) has rank 2, and the space \((0,c,-c)^{\prime}\) has rank 1.

For additional examples, let

\[X_{0}=\begin{bmatrix}1\\ 1\\ 1\end{bmatrix}\quad\text{and}\quad X=\begin{bmatrix}1&1\\ 1&2\\ 1&3\end{bmatrix}.\]

In this case,\[C(X_{0})^{\perp}=C\left(\left[\begin{array}{cc}-1&1\\ 0&-2\\ 1&1\end{array}\right]\right),\ \ \ C(X_{0})_{C(X)}^{\perp}=C\left(\left[\begin{array}{c} -1\\ 0\\ 1\end{array}\right]\right),\]

and

\[C(X)^{\perp}=C\left(\left[\begin{array}{c}1\\ -2\\ 1\end{array}\right]\right).\]

Proof of Theorem a.19.: It is easily seen that \(\mathcal{N}_{\mathcal{M}}^{\perp}\) is a subspace by checking Theorem A.3. Since \(\mathcal{N}\) and \(\mathcal{N}_{\mathcal{M}}^{\perp}\) are subspaces of \(\mathcal{M}\), we have \(\mathcal{N}+\mathcal{N}_{\mathcal{M}}^{\perp}\subset\mathcal{M}\). To show equality it remains to show that \(\mathcal{M}\subset\mathcal{N}+\mathcal{N}_{\mathcal{M}}^{\perp}\), i.e., if \(x\in\mathcal{M}\), then \(x\in\mathcal{N}+\mathcal{N}_{\mathcal{M}}^{\perp}\).

Let \(r(\mathcal{M})=n\) and \(r(\mathcal{N})=r\). Let \(v_{1}\),..., \(v_{r}\) be a basis for \(\mathcal{N}\) and extend this with \(w_{1}\),..., \(w_{n-r}\) to a basis for \(\mathcal{M}\). (Alternatively, take \(w_{1}\),..., \(w_{n}\) a basis for \(\mathcal{M}\) but define the spanning set \(\{v_{1}\),..., \(v_{r}\), \(w_{1}\),..., \(w_{n}\}\) for \(\mathcal{M}\).) Apply Gram-Schmidt to get \(v_{1}^{*},\ldots,v_{r}^{*}\), \(w_{1}^{*},\ldots,w_{n-r}^{*}\) an orthonormal basis for \(\mathcal{M}\) with \(v_{1}^{*},\ldots,v_{r}^{*}\) an orthonormal basis for \(\mathcal{N}\). By construction \(\{w_{1}^{*}\),..., \(w_{n-r}^{*}\}\subset\mathcal{N}_{\mathcal{M}}^{\perp}\).

If \(x\in\mathcal{M}\), then

\[x=\sum_{i=1}^{r}\alpha_{i}v_{i}^{*}+\sum_{j=1}^{n-r}\beta_{j}w_{j}^{*}.\]

Let \(x_{0}\equiv\sum_{i=1}^{r}\alpha_{i}v_{i}^{*}\) and \(x_{1}\equiv\sum_{j=1}^{n-r}\beta_{j}w_{j}^{*}\). Then \(x_{0}\in\mathcal{N}\), \(x_{1}\in\mathcal{N}_{\mathcal{M}}^{\perp}\), and \(x=x_{0}+x_{1}\), so \(x\in\mathcal{N}+\mathcal{N}_{\mathcal{M}}^{\perp}\). 

From Corollary A.20 we have \(r\left(\mathcal{N}_{\mathcal{M}}^{\perp}\right)=n-r\), so \(w_{1}^{*},\ldots,w_{n-r}^{*}\) must be a basis for \(\mathcal{N}_{\mathcal{M}}^{\perp}\). This relies on the fact that if the \(w_{j}^{*}\)s are all in \(\mathcal{N}_{\mathcal{M}}^{\perp}\) and if they are linearly independent with the same number of vectors as in a basis for the \(\mathcal{N}_{\mathcal{M}}^{\perp}\), then they must be a spanning set for \(\mathcal{N}_{\mathcal{M}}^{\perp}\). But we have not shown directly that \(w_{1}^{*}\),..., \(w_{n-r}^{*}\) is a spanning set for \(\mathcal{N}_{\mathcal{M}}^{\perp}\). We do that now.

If \(x\in\mathcal{N}_{\mathcal{M}}^{\perp}\), because \(x\in\mathcal{M}\) write

\[x=\sum_{i=1}^{r}\alpha_{i}v_{i}^{*}+\sum_{j=1}^{n-r}\beta_{j}w_{j}^{*}.\]

Since \(x\in\mathcal{N}_{\mathcal{M}}^{\perp}\) and \(v_{k}^{*}\in\mathcal{N}\) for \(k=1,\ldots,r\),\[0=x^{\prime}v_{k}^{*} =\left(\sum_{i=1}^{r}\alpha_{i}v_{i}^{*}+\sum_{j=1}^{n-r}\beta_{j}w_{ j}^{*}\right)^{\prime}v_{k}^{*}\] \[=\sum_{i=1}^{r}\alpha_{i}v_{i}^{*\prime}v_{k}^{*}+\sum_{j=1}^{n-r} \beta_{j}w_{j}^{*\prime}v_{k}^{*}\] \[=\alpha_{k}v_{k}^{*\prime}v_{k}^{*}=\alpha_{k}.\]

Thus \(x=\sum_{j=1}^{n-r}\beta_{j}w_{j}^{*}\), implying that \(\{w_{1}^{*},\ldots,w_{n-r}^{*}\}\) is a spanning set and a basis for \(\mathcal{N}_{\mathcal{M}}^{\perp}\).

**Theorem A.21**: _Let \(\mathcal{N}_{1}\) and \(\mathcal{N}_{2}\) be subspaces, then \((\mathcal{N}_{1}\cap\mathcal{N}_{2})^{\perp}=\mathcal{N}_{1}^{\perp}+ \mathcal{N}_{2}^{\perp}\)._

_Proof_ This is a restatement of Proposition 10.4.6. The proof is given there. \(\square\)

## Exercises

**Exercise A.1**: Give a detailed proof of the Gram-Schmidt theorem.

Questions A.2 through A.13 involve the following matrices:

\[A=\begin{bmatrix}1&1&0&0\\ 1&1&0&0\\ 0&0&1&0\\ 0&0&1&1\end{bmatrix},\quad B=\begin{bmatrix}1&0&0\\ 1&0&0\\ 0&1&0\\ 0&0&1\end{bmatrix},\quad D=\begin{bmatrix}1&0\\ 1&0\\ 2&5\\ 0&0\end{bmatrix},\quad E=\begin{bmatrix}1&2\\ 1&2\\ 2&7\\ 0&0\end{bmatrix},\]

\[F=\begin{bmatrix}1&5&6\\ 1&5&6\\ 0&7&2\\ 0&0&9\end{bmatrix},\quad G=\begin{bmatrix}1&0&5&2\\ 1&0&5&2\\ 2&5&7&9\\ 0&0&0&3\end{bmatrix},\quad H=\begin{bmatrix}1&0&2&2&6\\ 1&0&2&2&6\\ 7&9&3&9&-1\\ 0&0&0&3&-7\end{bmatrix},\]

\[K=\begin{bmatrix}1&0&0\\ 1&0&0\\ 1&1&0\\ 1&0&1\end{bmatrix},\quad L=\begin{bmatrix}2&0&0\\ 2&0&0\\ 1&1&0\\ 1&0&1\end{bmatrix},\quad N=\begin{bmatrix}1\\ 2\\ 3\\ 4\end{bmatrix}.\]

**Exercise A.2**: Is the space spanned by the columns of \(A\) the same as the space spanned by the columns of \(B\)? How about the spaces spanned by the columns of \(K\), \(L\), \(F\), \(D\), and \(G\)?

**Exercise A.3**: Give a matrix whose column space contains \(C(A)\).

**Exercise A.4**: Give two matrices whose column spaces contain \(C(B)\).

**Exercise A.5**: Which of the following equalities are valid: \(C(A)=C(A,\,D)\), \(C(D)=C(A,\,B)\), \(C(A,\,N)=C(A)\), \(C(N)=C(A)\), \(C(A)=C(F)\), \(C(A)=C(G)\), \(C(A)=C(H)\), \(C(A)=C(D)\)?

**Exercise A.6**: Which of the following matrices have linearly independent columns: \(A\), \(B\), \(D\), \(N\), \(F\), \(H\), \(G\)?

**Exercise A.7**: Give a basis for the space spanned by the columns of each of the following matrices: \(A\), \(B\), \(D\), \(N\), \(F\), \(H\), \(G\).

**Exercise A.8**: Give the ranks of \(A\), \(B\), \(D\), \(E\), \(F\), \(G\), \(H\), \(K\), \(L\), \(N\).

**Exercise A.9**: Which of the following matrices have columns that are mutually orthogonal: \(B\), \(A\), \(D\)?

**Exercise A.10**: Give an orthogonal basis for the space spanned by the columns of each of the following matrices: \(A\), \(D\), \(N\), \(K\), \(H\), \(G\).

**Exercise A.11**: Find \(C(A)^{\perp}\) and \(C(B)^{\perp}\) (with respect to \({\bf R}^{4}\)).

**Exercise A.12**: Find two linearly independent vectors in the orthogonal complement of \(C(D)\) (with respect to \({\bf R}^{4}\)).

**Exercise A.13**: Find a vector in the orthogonal complement of \(C(D)\) with respect to \(C(A)\).

**Exercise A.14**: Find an orthogonal basis for the space spanned by the columns of

\[X=\left[\matrix{1&1&4\cr 1&2&1\cr 1&3&0\cr 1&4&0\cr 1&5&1\cr 1&6&4\cr} \right].\]

**Exercise A.15**: For \(X\) as above, find two linearly independent vectors in the orthogonal complement of \(C(X)\) (with respect to \({\bf R}^{6}\)).

**Exercise A.16**: Let \(X\) be an \(n\times p\) matrix. Prove or disprove the following statement: Every vector in \(\mathbb{R}^{n}\) is in either \(C(X)\) or \(C(X)^{\perp}\) or both.

**Exercise A.17**: For any matrix \(A\), prove that \(C(A)\) and the null space of \(A^{\prime}\) are orthogonal complements. Note: The null space is defined in Definition B.11.

## Appendix B Matrix Results

**Abstract** This appendix reviews standard ideas in matrix theory with emphasis given to important results that are less commonly taught in a junior/senior level linear algebra course. The appendix begins with basic definitions and results. A section devoted to eigenvalues and their applications follows. This section contains a number of standard definitions, but it also contains a number of very specific results that are unlikely to be familiar to people with only an undergraduate background in linear algebra. The third section is devoted to an intense (brief but detailed) examination of projections and their properties. The appendix closes with some miscellaneous results, some results on Kronecker products and Vec operators, and an introduction to tensors.

### Basic Ideas

**Definition B.1**  Any matrix with the same number of rows and columns is called a _square matrix_.

**Definition B.2**  Let \(A=[a_{ij}]\) be a matrix. The _transpose_ of \(A\), written \(A^{\prime}\), is the matrix \(A^{\prime}=[b_{ij}]\), where \(b_{ij}=a_{ji}\).

**Definition B.3**  If \(A=A^{\prime}\), then \(A\) is called _symmetric_. Note that only square matrices can be symmetric.

**Definition B.4**  If \(A=[a_{ij}]\) is a square matrix and \(a_{ij}=0\) for \(i\neq j\), then \(A\) is a _diagonal matrix_. If \(\lambda_{1},\ldots,\lambda_{n}\) are scalars, then \(D(\lambda_{j})\) and \(\text{Diag}(\lambda_{j})\) are used to indicate an \(n\times n\) matrix \(D=[d_{ij}]\) with \(d_{ij}=0\), \(i\neq j\), and \(d_{ii}=\lambda_{i}\). If \(\lambda\equiv(\lambda_{1},\ldots,\lambda_{n})^{\prime}\), then \(D(\lambda)\equiv D(\lambda_{j})\). A diagonal matrix with all 1s on the diagonal is called an _identity matrix_ and is denoted \(I\). Occasionally, \(I_{n}\) is used to denote an \(n\times n\) identity matrix.

If \(A=[a_{ij}]\) is \(n\times p\) and \(B=[b_{ij}]\) is \(n\times q\), we can write an \(n\times(p+q)\) matrix \(C=[A,\,B]\), where \(c_{ij}=a_{ij}\), \(i=1,\ldots,n\), \(j=1,\ldots,p\), and \(c_{ij}=b_{i,j-p}\), \(i=1,\ldots,n,j=p+1,\ldots,p+q\). This notation can be extended in obvious ways, e.g., \(C^{\prime}=\begin{bmatrix}A^{\prime}\\ B^{\prime}\end{bmatrix}\).

**Definition B.5**.: Let \(A=[a_{ij}]\) be an \(r\times c\) matrix and \(B=[b_{ij}]\) be an \(s\times d\) matrix. The _Kronecker product_ of \(A\) and \(B\), written \(A\otimes B\), is an \(r\times c\) matrix of \(s\times d\) matrices. The matrix in the \(i\)th row and \(j\)th column is \(a_{ij}B\). In total, \(A\otimes B\) is an \(rs\times cd\) matrix.

**Definition B.6**.: Let \(A\) be an \(r\times c\) matrix. Write \(A=[A_{1},\,A_{2},\ldots,\,A_{c}]\), where \(A_{i}\) is the \(i\)th column of \(A\). The _Vec_ operator stacks the columns of \(A\) into an \(rc\times 1\) vector; thus,

\[[\text{Vec}(A)]^{\prime}=[A_{1}^{\prime},\,A_{2}^{\prime},\ldots,\,A_{c}^{ \prime}].\]

_Example B.7_.: \[A=\begin{bmatrix}1&4\\ 2&5\end{bmatrix},\quad B=\begin{bmatrix}1&3\\ 0&4\end{bmatrix},\]

\[A\otimes B=\begin{bmatrix}1&\begin{pmatrix}1&3\\ 0&4\end{pmatrix}&4\begin{pmatrix}1&3\\ 0&4\end{pmatrix}\\ 2&\begin{pmatrix}1&3\\ 0&4\end{pmatrix}&5\begin{pmatrix}1&3\\ 0&4\end{pmatrix}\end{bmatrix}=\begin{bmatrix}1&3&4&12\\ 0&4&0&16\\ 2&6&5&15\\ 0&8&0&20\end{bmatrix},\]

\[\text{Vec}(A)=[1,\,2,\,4,\,5]^{\prime}.\]

**Definition B.8**.: Let \(A\) be an \(n\times n\) matrix. \(A\) is _nonsingular_ if there exists a matrix \(A^{-1}\) such that \(A^{-1}A=I=AA^{-1}\). If no such matrix exists, then \(A\) is singular. If \(A^{-1}\) exists, it is called the _inverse_ of \(A\).

**Theorem B.9**.: _An \(n\times n\) matrix \(A\) is nonsingular if and only if \(r(A)=n\), i.e., the columns of \(A\) form a basis for \(\mathbf{R}^{n}\)._

**Corollary B.10**.: \(A_{n\times n}\) _is singular if and only if there exists \(x\neq 0\) such that \(Ax=0\)._

For any matrix \(A\), the set of all \(x\) such that \(Ax=0\) is easily seen to be a vector space.

**Definition B.11**.: The set of all \(x\) such that \(Ax=0\) is called the _null space_ of \(A\) and written \(\mathcal{N}(A)\).

**Theorem B.12**: _If \(A\) is \(n\times n\) and \(r(A)=r\), then the null space of \(A\) has rank \(n-r\)._

### Eigenvalues and Related Results

The material in this section deals with eigenvalues and eigenvectors either in the statements of the results or in their proofs. Again, this is meant to be a brief review of important concepts; but, in addition, there are a number of specific results that may be unfamiliar.

**Definition B.13**: The scalar \(\lambda\) is an _eigenvalue_ of \(A_{n\times n}\) if \(A-\lambda I\) is singular. \(\lambda\) is an eigenvalue of _multiplicity_\(s\) if the rank of the null space of \(A-\lambda I\) is \(s\). A nonzero vector \(x\) is an _eigenvector_ of \(A\) corresponding to the eigenvalue \(\lambda\) if \(x\) is in the null space of \(A-\lambda I\), i.e., if \(Ax=\lambda x\). Eigenvalues are also called _singular values_ and _characteristic roots_.

For example,

\[\begin{bmatrix}2&1\\ 1&2\end{bmatrix}\begin{pmatrix}1\\ 1\end{pmatrix}=3\begin{pmatrix}1\\ 1\end{pmatrix}\]

and

\[\begin{bmatrix}2&1\\ 1&2\end{bmatrix}\begin{pmatrix}-1\\ 1\end{pmatrix}=1\begin{pmatrix}-1\\ 1\end{pmatrix}.\]

Combining the two equations gives

\[\begin{bmatrix}2&1\\ 1&2\end{bmatrix}\begin{bmatrix}1&-1\\ 1&1\end{bmatrix}=\begin{bmatrix}1&-1\\ 1&1\end{bmatrix}\begin{bmatrix}3&0\\ 0&1\end{bmatrix}.\]

Note that if \(\lambda\neq 0\) is an eigenvalue of \(A\), the eigenvectors corresponding to \(\lambda\) (along with the vector \(0\)) form a subspace of \(C(A)\). For example, if \(Ax_{1}=\lambda x_{1}\) and \(Ax_{2}=\lambda x_{2}\), then \(A(x_{1}+x_{2})=\lambda(x_{1}+x_{2})\), so the set of eigenvectors is closed under vector addition. Similarly, it is closed under scalar multiplication, so it forms a subspace (except that eigenvectors cannot be \(0\) and every subspace contains \(0\)). If \(\lambda=0\), the subspace is the null space of \(A\).

If \(A\) is a symmetric matrix, and \(\gamma\) and \(\lambda\) are distinct eigenvalues, then the eigenvectors corresponding to \(\lambda\) and \(\gamma\) are orthogonal. To see this, let \(x\) be an eigenvector for \(\lambda\) and \(y\) an eigenvector for \(\gamma\). Then \(\lambda x^{\prime}y=x^{\prime}Ay=\gamma x^{\prime}y\), which can happen only if \(\lambda=\gamma\) or if \(x^{\prime}y=0\). Since \(\lambda\) and \(\gamma\) are distinct, we have \(x^{\prime}y=0\).

Let \(\lambda_{1},\ldots,\lambda_{r}\) be the distinct nonzero eigenvalues of a symmetric matrix \(A\) with respective multiplicities \(s(1),\ldots,s(r)\). Let \(v_{i1},\ldots,v_{is(i)}\) be a basis for the space of eigenvectors of \(\lambda_{i}\). We want to show that \(v_{11},\)\(v_{12},\ldots,\)\(v_{rs(r)}\) is a basis for \(C(A)\)Suppose \(v_{11}\), \(v_{12}\),..., \(v_{rs(r)}\) is not a basis. Since \(v_{ij}\in C(A)\) and the \(v_{ij}\)s are linearly independent, we can pick \(x\in C(A)\) with \(x\perp v_{ij}\) for all \(i\) and \(j\). Note that since \(Av_{ij}=\lambda_{i}v_{ij}\), we have \((A)^{p}v_{ij}=(\lambda_{i})^{p}v_{ij}\). In particular, \(x^{\prime}(A)^{p}v_{ij}=x^{\prime}(\lambda_{i})^{p}v_{ij}=(\lambda_{i})^{p}x^ {\prime}v_{ij}=0\), so \(A^{p}x\perp v_{ij}\) for any \(i\), \(j\), and \(p\). The vectors \(x\), \(Ax\), \(A^{2}x\),... cannot all be linearly independent, so there exists a smallest value \(k\leq n\) such that

\[A^{k}x+b_{k-1}A^{k-1}x+\cdots+b_{0}x=0.\]

Since there is a solution to this, for some real number \(\mu\) we can write the equation as

\[(A-\mu I)\left(A^{k-1}x+\gamma_{k-2}A^{k-2}x+\cdots+\gamma_{0}x\right)=0,\]

and \(\mu\) is an eigenvalue. (See Exercise B.1.) An eigenvector for \(\mu\) is \(y=A^{k-1}x+\gamma_{k-2}A^{k-2}x+\cdots+\gamma_{0}x\). Clearly, \(y\perp v_{ij}\) for any \(i\) and \(j\). Since \(k\) was chosen as the smallest value to get linear dependence, we have \(y\neq 0\). If \(\mu\neq 0\), \(y\) is an eigenvector that does not correspond to any of \(\lambda_{1}\),..., \(\lambda_{r}\), a contradiction. If \(\mu=0\), we have \(Ay=0\); and since \(A\) is symmetric, \(y\) is a vector in \(C(A)\) that is orthogonal to every other vector in \(C(A)\), i.e., \(y^{\prime}y=0\) but \(y\neq 0\), a contradiction. We have proven

**Theorem B.14**: _If \(A\) is a symmetric matrix, then there exists a basis for \(C(A)\) consisting of eigenvectors of nonzero eigenvalues. If \(\lambda\) is a nonzero eigenvalue of multiplicity \(s\), then the basis will contain \(s\) eigenvectors for \(\lambda\)._

If \(\lambda\) is an eigenvalue of \(A\) with multiplicity \(s\), then we can think of \(\lambda\) as being an eigenvalue \(s\) times. With this convention, the rank of \(A\) is the number of nonzero eigenvalues. The total number of eigenvalues is \(n\) if \(A\) is an \(n\times n\) matrix.

For a symmetric matrix \(A\), if we use eigenvectors corresponding to the zero eigenvalue, we can get a basis for \(\mathbf{R}^{n}\) consisting of eigenvectors. We already have a basis for \(C(A)\), and the eigenvectors of \(0\) are the null space of \(A\). For \(A\) symmetric, \(C(A)\) and the null space of \(A\) are orthogonal complements. Let \(\lambda_{1}\),..., \(\lambda_{n}\) be the eigenvalues of a symmetric matrix \(A\). Let \(v_{1}\),..., \(v_{n}\) denote a basis of eigenvectors for \(\mathbf{R}^{n}\), with \(v_{i}\) being an eigenvector for \(\lambda_{i}\) for any \(i\).

**Theorem B.15**: _If \(A\) is symmetric, there exists an orthonormal basis for \(\mathbf{R}^{n}\) consisting of eigenvectors of \(A\)._

_Proof_ Assume \(\lambda_{i1}=\cdots=\lambda_{ik}\) are all the \(\lambda_{i}\)s equal to any particular value \(\lambda\), and let \(v_{i1}\),..., \(v_{ik}\) be a basis for the space of eigenvectors for \(\lambda\). By Gram-Schmidt there exists an orthonormal basis \(w_{i1}\),..., \(w_{ik}\) for the space of eigenvectors corresponding to \(\lambda\). If we do this for each distinct eigenvalue, we get a collection of orthonormal sets that form a basis for \(\mathbf{R}^{n}\). Since, as we have seen, for \(\lambda_{i}\neq\lambda_{j}\), any eigenvector for \(\lambda_{i}\) is orthogonal to any eigenvector for \(\lambda_{j}\), the basis is orthonormal.

**Definition B.16**.: A square matrix \(P\) is _orthonormal_ (more often called _orthogonnal_) if \(P^{\prime}=P^{-1}\). Note that if \(P\) is orthonormal, so is \(P^{\prime}\).

Some examples of orthonormal matrices are

\[P_{1}=\tfrac{1}{\sqrt{6}}\begin{bmatrix}\sqrt{2}&-\sqrt{3}&1\\ \sqrt{2}&0&-2\\ \sqrt{2}&\sqrt{3}&1\end{bmatrix},\quad P_{2}=\tfrac{1}{\sqrt{2}}\begin{bmatrix} 1&1\\ 1&-1\end{bmatrix},\]

\[P_{3}=\begin{bmatrix}1&0&0\\ 0&-1&0\\ 0&0&1\end{bmatrix}.\]

**Theorem B.17**.: \(P_{n\times n}\) _is orthonormal if and only if the columns of \(P\) form an orthonormal basis for \(\mathbf{R}^{n}\)._

Proof.: \(\Leftarrow\) It is clear that if the columns of \(P\) form an orthonormal basis for \(\mathbf{R}^{n}\), then \(P^{\prime}P=I\).

\(\Rightarrow\) Since \(P\) is nonsingular, the columns of \(P\) form a basis for \(\mathbf{R}^{n}\). Since \(P^{\prime}P=I\), the basis is orthonormal. 

**Corollary B.18**.: \(P_{n\times n}\) _is orthonormal if and only if the rows of \(P\) form an orthonormal basis for \(\mathbf{R}^{n}\)._

Proof.: \(P\) is orthonormal if and only if \(P^{\prime}\) is orthonormal if and only if the columns of \(P^{\prime}\) are an orthonormal basis if and only if the rows of \(P\) are an orthonormal basis. 

**Theorem B.19**.: _If \(A\) is an \(n\times n\) symmetric matrix, then there exists an orthonormal matrix \(P\) such that \(P^{\prime}AP=Diag(\lambda_{i})\), where \(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\) are the eigenvalues of \(A\)._

Proof.: Let \(v_{1},v_{2},\ldots,v_{n}\) be an orthonormal set of eigenvectors of \(A\) corresponding, respectively, to \(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\). Let \(P=[v_{1},\ldots,v_{n}]\). Then

\[P^{\prime}AP =\begin{bmatrix}v_{1}^{\prime}\\ \vdots\\ v_{n}^{\prime}\end{bmatrix}[Av_{1},\ldots,Av_{n}]\] \[=\begin{bmatrix}v_{1}^{\prime}\\ \vdots\\ v_{n}^{\prime}\end{bmatrix}[\lambda_{1}v_{1},\ldots,\lambda_{n}v_{n}]\] \[=\begin{bmatrix}\lambda_{1}v_{1}^{\prime}v_{1}&\ldots&\lambda_{n} v_{1}^{\prime}v_{n}\\ \vdots&\ddots&\vdots\\ \lambda_{1}v_{n}^{\prime}v_{1}&\ldots&\lambda_{n}v_{n}^{\prime}v_{n}\end{bmatrix}\] \[=Diag(\lambda_{i}).\]The _singular value decomposition_ for a symmetric matrix is given by the following corollary.

Corollary B.20: \(A=PD(\lambda_{i})P^{\prime}\)_._

For example, using results illustrated earlier,

\[\begin{bmatrix}2&1\\ 1&2\end{bmatrix}=\begin{bmatrix}1/\sqrt{2}&-1/\sqrt{2}\\ 1/\sqrt{2}&1/\sqrt{2}\end{bmatrix}\begin{bmatrix}3&0\\ 0&1\end{bmatrix}\begin{bmatrix}1/\sqrt{2}&1/\sqrt{2}\\ -1/\sqrt{2}&1/\sqrt{2}\end{bmatrix}.\]

Definition B.21: A symmetric matrix \(A\) is _positive (nonnegative) definite_ if, for any nonzero vector \(v\in\mathbf{R}^{n}\), \(v^{\prime}Av\) is positive (nonnegative).

Theorem B.22: \(A\) _is nonnegative definite if and only if there exists a square matrix \(Q\) such that \(A=QQ^{\prime}\)._

_Proof_ \(\Rightarrow\) We know that there exists \(P\) orthonormal with \(P^{\prime}AP=\operatorname{Diag}(\lambda_{i})\). The \(\lambda_{i}\)s must all be nonnegative, because if \(e_{j}^{\prime}=(0,\ldots,0,1,0,\ldots,0)\) with the \(1\) in the \(j\)th place and we let \(v=Pe_{j}\), then \(0\leq v^{\prime}Av=e_{j}^{\prime}\operatorname{Diag}(\lambda_{i})e_{j}= \lambda_{j}\). Let \(Q=P\operatorname{Diag}\bigl{(}\sqrt{\lambda_{i}}\bigr{)}\). Then, since \(P\operatorname{Diag}(\lambda_{i})P^{\prime}=A\), we have

\[QQ^{\prime}=P\operatorname{Diag}(\lambda_{i})P^{\prime}=A.\]

\(\Leftarrow\) If \(A=QQ^{\prime}\), then \(v^{\prime}Av=(Q^{\prime}v)^{\prime}(Q^{\prime}v)\geq 0\). \(\square\)

Corollary B.23: \(A\) _is positive definite if and only if \(Q\) is nonsingular for any choice of \(Q\)._

_Proof_ There exists \(v\neq 0\) such that \(v^{\prime}Av=0\) if and only if there exists \(v\neq 0\) such that \(Q^{\prime}v=0\), which occurs if and only if \(Q^{\prime}\) is singular. The contrapositive of this is that \(v^{\prime}Av>0\) for all \(v\neq 0\) if and only if \(Q^{\prime}\) is nonsingular. \(\square\)

In the interest of brevity, I have dropped Theorem B.24, Corollary B.25, and Corollary B.26 that appeared in earlier editions.

Definition B.27: Let \(A=[a_{ij}]\) be an \(n\times n\) matrix. The _trace_ of \(A\) is \(\operatorname{tr}(A)=\sum_{i=1}^{n}a_{ii}\).

Theorem B.28: _For matrices \(A_{r\times s}\) and \(B_{s\times r}\), \(\operatorname{tr}(AB)=\operatorname{tr}(BA)\).__Proof_ See Exercise B.8. 

**Theorem B.29**: _If \(A_{n\times n}\) is a symmetric matrix, \(\operatorname{tr}(A)=\sum_{i=1}^{n}\lambda_{i}\), where \(\lambda_{1},\ldots,\lambda_{n}\) are the eigenvalues of \(A\)._

_Proof_ \(A=PD(\lambda_{i})P^{\prime}\) with \(P\) orthonormal

\[\operatorname{tr}(A) = \operatorname{tr}[PD(\lambda_{i})P^{\prime}]=\operatorname{tr}[D( \lambda_{i})P^{\prime}P]\] \[= \operatorname{tr}[D(\lambda_{i})]=\sum_{i=1}^{n}\lambda_{i}.\]

To illustrate, we saw earlier that the matrix \(\begin{bmatrix}2&1\\ 1&2\end{bmatrix}\) had eigenvalues of 3 and 1. In fact, a stronger result than Theorem B.29 is true. We give it without proof.

**Theorem B.30**: \(\operatorname{tr}(A)=\sum_{i=1}^{n}\lambda_{i}\)_, where \(\lambda_{1},\ldots,\lambda_{n}\) are the eigenvalues of \(A\). Moreover, the determinant of \(A\) is \(\text{det}(A)=\prod_{i=1}^{n}\lambda_{i}\)._

### Projections

This section is devoted primarily to a discussion of perpendicular projection operators. It begins with their definition, some basic properties, and two important characterizations: Theorems B.33 and B.35. A third important characterization, Theorem B.44, involves generalized inverses. Generalized inverses are defined, briefly studied, and applied to projection operators. The section continues with the examination of the relationships between two perpendicular projection operators and closes with discussions of the Gram-Schmidt theorem, eigenvalues of projection operators, and oblique (nonperpendicular) projection operators.

We begin by defining a _perpendicular projection operator (ppo)_ onto an arbitrary space. To be consistent with later usage, we denote the arbitrary space \(C(X)\) for some matrix \(X\).

**Definition B.31**: \(M\) is a perpendicular projection operator (matrix) onto \(C(X)\) if and only if

1. \(v\in C(X)\) implies \(Mv=v\) (projection),
2. \(w\perp C(X)\) implies \(Mw=0\) (perpendicularity).

For example, consider the subspace of \(\mathbf{R}^{2}\) determined by vectors of the form \((2a,a)^{\prime}\). It is not difficult to see that the orthogonal complement of this subspace consists of vectors of the form \((b,-2b)^{\prime}\). The perpendicular projection operator onto the \((2a,a)^{\prime}\) subspace is

\[M=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}.\]

To verify this note that

\[M\begin{pmatrix}2a\\ a\end{pmatrix}=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}\begin{pmatrix}2a\\ a\end{pmatrix}=\begin{pmatrix}(0.8)2a+0.4a\\ (0.4)2a+0.2a\end{pmatrix}=\begin{pmatrix}2a\\ a\end{pmatrix}\]

and

\[M\begin{pmatrix}b\\ -2b\end{pmatrix}=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}\begin{pmatrix}b\\ -2b\end{pmatrix}=\begin{pmatrix}0.8b+0.4(-2b)\\ 0.4b+0.2(-2b)\end{pmatrix}=\begin{pmatrix}0\\ 0\end{pmatrix}.\]

Notationally, \(M\) is used to indicate the ppo onto \(C(X)\). If \(A\) is another matrix, \(M_{A}\) denotes the ppo onto \(C(A)\). Thus, \(M\equiv M_{X}\). When \(X\) has a subscript we typically write the ppo onto \(C(X_{0})\) as \(M_{0}\equiv M_{X_{0}}\) and, similarly, \(M_{1}\equiv M_{X_{1}}\), but often \(M_{2}\neq M_{X_{2}}\).

**Proposition B.32**: _If \(M\) is a perpendicular projection operator onto \(C(X)\), then \(C(M)=C(X)\)._

_Proof_ See Exercise B.2. \(\square\)

Note that both columns of

\[M=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}\]

have the form \((2a,a)^{\prime}\).

**Theorem B.33**: \(M\) _is a perpendicular projection operator on \(C(M)\) if and only if \(MM=M\) and \(M^{\prime}=M\)._

_Proof_\(\Rightarrow\) Write \(v=v_{1}+v_{2}\), where \(v_{1}\in C(M)\) and \(v_{2}\perp C(M)\), and let \(w=w_{1}+w_{2}\) with \(w_{1}\in C(M)\) and \(w_{2}\perp C(M)\). Since \((I-M)v=(I-M)v_{2}=v_{2}\) and \(Mw=Mw_{1}=w_{1}\), we get

\[w^{\prime}M^{\prime}(I-M)v=w_{1}^{\prime}M^{\prime}(I-M)v_{2}=w_{1}^{\prime}v_ {2}=0.\]

This is true for any \(v\) and \(w\), so we have \(M^{\prime}(I-M)=0\) or \(M^{\prime}=M^{\prime}M\). Since \(M^{\prime}M\) is symmetric, \(M^{\prime}\) must also be symmetric, and this implies that \(M=MM\).

\(\Leftarrow\) If \(M^{2}=M\) and \(v\in C(M)\), then since \(v=Mb\) we have \(Mv=MMb=Mb=v\). If \(M^{\prime}=M\) and \(w\perp C(M)\), then \(Mw=M^{\prime}w=0\) because the columns of \(M\) are in \(C(M)\).

In our example,

\[MM=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}=M\]

and

\[M=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}=M^{\prime}.\]

**Proposition B.34**: _Perpendicular projection operators are unique._

_Proof_ Let \(M\) and \(P\) be perpendicular projection operators onto some space \(\mathcal{M}\). Let \(v\in\mathbf{R}^{n}\) and write \(v=v_{1}+v_{2}\), \(v_{1}\in\mathcal{M}\), \(v_{2}\perp\mathcal{M}\). Since \(v\) is arbitrary and \(Mv=v_{1}=Pv\), we have \(M=P\). \(\square\)

For any matrix \(X\), we will now find two ways to characterize the perpendicular projection operator onto \(C(X)\). The first method depends on the Gram-Schmidt theorem; the second depends on the concept of a generalized inverse.

**Theorem B.35**: _Let \(o_{1},\ldots,o_{r}\) be an orthonormal basis for \(C(X)\), and let \(O=[o_{1},\ldots,o_{r}]\). Then \(OO^{\prime}=\sum_{i=1}^{r}o_{i}o_{i}^{\prime}\) is the perpendicular projection operator onto \(C(X)\)._

_Proof_\(O\,O^{\prime}\) is symmetric and \(OO^{\prime}OO^{\prime}=OI_{r}O^{\prime}=OO^{\prime}\); so, by Theorem B.33, it only remains to show that \(C(OO^{\prime})=C(X)\). Clearly \(C(OO^{\prime})\subset C(O)=C(X)\). On the other hand, if \(v\in C(O)\), then \(v=Ob\) for some vector \(b\in\mathbf{R}^{r}\) and \(v=Ob=OI_{r}b=OO^{\prime}Ob\); so clearly \(v\in C(OO^{\prime})\). \(\square\)

For example, to find the perpendicular projection operator for vectors of the form \((2a,a)^{\prime}\), we can find an orthonormal basis. The space has rank 1 and to normalize \((2a,a)^{\prime}\), we must have

\[1=(2a,a)\begin{pmatrix}2a\\ a\end{pmatrix}=4a^{2}+a^{2}=5a^{2};\]

so \(a^{2}=1/5\) and \(a=\pm 1/\sqrt{5}\). If we take \((2/\sqrt{5},\,1/\sqrt{5})^{\prime}\) as our orthonormal basis, then

\[M=\begin{pmatrix}2/\sqrt{5}\\ 1/\sqrt{5}\end{pmatrix}(2/\sqrt{5},\,1/\sqrt{5})=\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix},\]

as was demonstrated earlier.

One use of Theorem B.35 is that, given a matrix \(X\), one can use the Gram-Schmidt theorem to get an orthonormal basis for \(C(X)\) and thus obtain the perpendicular projection operator.

We now examine properties of generalized inverses. Generalized inverses are a generalization on the concept of the inverse of a matrix. Although the most common use of generalized inverses is in solving systems of linear equations, our interest liesprimarily in their relationship to projection operators. The discussion below is given for an arbitrary matrix \(A\).

**Definition B.36**.: A _generalized inverse_ of a matrix \(A\) is any matrix \(G\) such that \(AGA=A\). The notation \(A^{-}\) is used to indicate a generalized inverse of \(A\).

**Theorem B.37**.: _If \(A\) is nonsingular, the unique generalized inverse of \(A\) is \(A^{-1}\)._

_Proof_ \(AA^{-1}A=IA=A\), so \(A^{-1}\) is a generalized inverse. If \(AA^{-}A=A\), then \(AA^{-}=AA^{-}AA^{-1}=AA^{-1}=I\); so \(A^{-}\) is the inverse of \(A\). 

**Theorem B.38**.: _For any symmetric matrix \(A\), there exists a generalized inverse of \(A\)._

_Proof_  There exists \(P\) orthonormal so that \(P^{\prime}AP=D(\lambda_{i})\) and \(A=PD(\lambda_{i})P^{\prime}\). Let

\[\gamma_{i}=\begin{cases}1/\lambda_{i},&\text{if }\lambda_{i}\neq 0\\ 0,&\text{if }\lambda_{i}=0,\end{cases}\]

and \(G=PD(\gamma_{i})P^{\prime}\). We now show that \(G\) is a generalized inverse of \(A\). \(P\) is orthonormal, so \(P^{\prime}P=I\) and

\[AGA = PD(\lambda_{i})P^{\prime}PD(\gamma_{i})P^{\prime}PD(\lambda_{i} )P^{\prime}\] \[= PD(\lambda_{i})D(\gamma_{i})D(\lambda_{i})P^{\prime}\] \[= PD(\lambda_{i})P^{\prime}\] \[= A.\]

Although this is the only existence result we really need, later we will show that generalized inverses exist for arbitrary matrices.

**Theorem B.39**.: _If \(G_{1}\) and \(G_{2}\) are generalized inverses of \(A\), then so is \(G_{1}AG_{2}\)._

_Proof_ \(A(G_{1}AG_{2})A=(AG_{1}A)G_{2}A=AG_{2}A=A\). 

For \(A\) symmetric, \(A^{-}\) need not be symmetric.

_Example B.40_.: Consider the matrix

\[\begin{bmatrix}a&b\\ b&b^{2}/a\end{bmatrix}.\]

It has a generalized inverse

\[\begin{bmatrix}1/a&-1\\ 1&0\end{bmatrix},\]and in fact, by considering the equation

\[\begin{bmatrix}a&b\\ b&b^{2}/a\end{bmatrix}\begin{bmatrix}r&s\\ t&u\end{bmatrix}\begin{bmatrix}a&b\\ b&b^{2}/a\end{bmatrix}=\begin{bmatrix}a&b\\ b&b^{2}/a\end{bmatrix},\]

it can be shown that if \(r=1/a\), then any solution of \(at+as+bu=0\) gives a generalized inverse.

**Corollary B.41**: _For a symmetric matrix \(A\), there exists \(A^{-}\) such that \(A^{-}AA^{-}=A^{-}\) and \((A^{-})^{\prime}=A^{-}\)._

_Proof_ Take \(A^{-}\) as the generalized inverse in the proof of Theorem B.38. Clearly, \(A^{-}=PD(\gamma_{i})P^{\prime}\) is symmetric and

\[A^{-}AA^{-}=PD(\gamma_{i})P^{\prime}PD(\lambda_{i})P^{\prime}PD(\gamma_{i})P^{ \prime}=PD(\gamma_{i})D(\lambda_{i})D(\gamma_{i})P^{\prime}=PD(\gamma_{i})P^{ \prime}=A^{-}.\]

\(\square\)

**Definition B.42**: A generalized inverse \(A^{-}\) for a matrix \(A\) that has the property \(A^{-}AA^{-}=A^{-}\) is said to be _reflexive_.

Corollary B.41 establishes the existence of a reflexive generalized inverse for any symmetric matrix.

Generalized inverses are of interest in that they provide an alternative to the characterization of perpendicular projection matrices given in Theorem B.35. The two results immediately below characterize the perpendicular projection matrix onto \(C(X)\).

**Lemma B.43**: _If \(G\) and \(H\) are generalized inverses of \((X^{\prime}X)\), then_

_(i)_ \(XGX^{\prime}X=XHX^{\prime}X=X\)_,_

_(ii)_ \(XGX^{\prime}=XHX^{\prime}\)_._

_Proof_ For \(v\in\mathbb{R}^{n}\), let \(v=v_{1}+v_{2}\) with \(v_{1}\in C(X)\) and \(v_{2}\perp C(X)\). Also let \(v_{1}=Xb\) for some vector \(b\). Then

\[v^{\prime}XGX^{\prime}X=v_{1}^{\prime}XGX^{\prime}X=b^{\prime}(X^{\prime}X)G(X^ {\prime}X)=b^{\prime}(X^{\prime}X)=v^{\prime}X.\]

Since \(v\) and \(G\) are arbitrary, we have shown (i).

To see (ii), observe that for the arbitrary vector \(v\) above,

\[XGX^{\prime}v=XGX^{\prime}Xb=XHX^{\prime}Xb=XHX^{\prime}v.\]Since \(X^{\prime}X\) is symmetric, there exists a generalized inverse \((X^{\prime}X)^{-}\) that is symmetric. For this generalized inverse, \(X(X^{\prime}X)^{-}X^{\prime}\) is symmetric; so, by the above lemma, \(X(X^{\prime}X)^{-}X^{\prime}\) must be symmetric for any choice of \((X^{\prime}X)^{-}\).

**Theorem B.44**: \(X(X^{\prime}X)^{-}X^{\prime}\) _is the perpendicular projection operator onto \(C(X)\)._

_Proof_ We need to establish conditions (i) and (ii) of Definition B.31. (i) For \(v\in C(X)\), write \(v=Xb\), so by Lemma B.43, \(X(X^{\prime}X)^{-}X^{\prime}v=X(X^{\prime}X)^{-}X^{\prime}Xb=Xb=v\). (ii) If \(w\perp C(X)\), \(X(X^{\prime}X)^{-}X^{\prime}w=0\). \(\square\)

For example, one spanning set for the subspace of vectors with the form \((2a,a)^{\prime}\) is \((2,1)^{\prime}\). It follows that

\[M={2\choose 1}\left[(2,1)\,{2\choose 1}\right]^{-1}(2,1)=\left[{0.8\atop 0.4 \quad 0.2}\right],\]

as was shown earlier.

The next five results examine the relationships between two perpendicular projection matrices.

**Theorem B.45**: _Let \(M_{1}\) and \(M_{2}\) be perpendicular projection matrices on \({\bf R}^{n}\). \((M_{1}+M_{2})\) is the perpendicular projection matrix onto \(C(M_{1},M_{2})\) if and only if \(C(M_{1})\perp C(M_{2})\)._

_Proof_\(\Leftarrow\) If \(C(M_{1})\perp C(M_{2})\), then \(M_{1}M_{2}=M_{2}M_{1}=0\). Because

\[(M_{1}+M_{2})^{2}=M_{1}^{2}+M_{2}^{2}+M_{1}M_{2}+M_{2}M_{1}=M_{1}^{2}+M_{2}^{2 }=M_{1}+M_{2}\]

and

\[(M_{1}+M_{2})^{\prime}=M_{1}^{\prime}+M_{2}^{\prime}=M_{1}+M_{2},\]

\(M_{1}+M_{2}\) is the perpendicular projection matrix onto \(C(M_{1}+M_{2})\). Clearly \(C(M_{1}+M_{2})\subset C(M_{1},M_{2})\). To see that \(C(M_{1},M_{2})\subset C(M_{1}+M_{2})\), write \(v=M_{1}b_{1}+M_{2}b_{2}\). Then, because \(M_{1}M_{2}=M_{2}M_{1}=0\), \((M_{1}+M_{2})v=v\). Thus, \(C(M_{1},M_{2})=C(M_{1}+M_{2})\).

\(\Rightarrow\) If \(M_{1}+M_{2}\) is a perpendicular projection matrix, then

\[(M_{1}+M_{2}) = (M_{1}+M_{2})^{2}=M_{1}^{2}+M_{2}^{2}+M_{1}M_{2}+M_{2}M_{1}\] \[= M_{1}+M_{2}+M_{1}M_{2}+M_{2}M_{1}.\]

Thus, \(M_{1}M_{2}+M_{2}M_{1}=0\).

Multiplying by \(M_{1}\) gives \(0=M_{1}^{2}M_{2}+M_{1}M_{2}M_{1}=M_{1}M_{2}+M_{1}M_{2}M_{1}\) and thus \(-M_{1}M_{2}M_{1}=M_{1}M_{2}\). Since \(-M_{1}M_{2}M_{1}\) is symmetric, so is \(M_{1}M_{2}\). This gives \(M_{1}M_{2}=(M_{1}M_{2})^{\prime}=M_{2}M_{1}\), so the condition \(M_{1}M_{2}+M_{2}M_{1}=0\) becomes \(2(M_{1}M_{2})=0\) or \(M_{1}M_{2}=0\). By symmetry, this says that the columns of \(M_{1}\) are orthogonal to the columns of \(M_{2}\). 

**Theorem B.46**: _If \(M_{1}\) and \(M_{2}\) are symmetric, \(C(M_{1})\perp C(M_{2})\), and \((M_{1}+M_{2})\) is a perpendicular projection matrix, then \(M_{1}\) and \(M_{2}\) are perpendicular projection matrices._

_Proof_

\[(M_{1}+M_{2})=(M_{1}+M_{2})^{2}=M_{1}^{2}+M_{2}^{2}+M_{1}M_{2}+M_{2}M_{1}.\]

Since \(M_{1}\) and \(M_{2}\) are symmetric with \(C(M_{1})\perp C(M_{2})\), we have \(M_{1}M_{2}+M_{2}M_{1}=0\) and \(M_{1}+M_{2}=M_{1}^{2}+M_{2}^{2}\). Rearranging gives \(M_{2}-M_{2}^{2}=M_{1}^{2}-M_{1}\), so \(C(M_{2}-M_{2}^{2})=C(M_{1}^{2}-M_{1})\). Now \(C(M_{2}-M_{2}^{2})\subset C(M_{2})\) and \(C(M_{1}^{2}-M_{1})\subset C(M_{1})\), so \(C(M_{2}-M_{2}^{2})\perp C(M_{1}^{2}-M_{1})\). The only way a vector space can be orthogonal to itself is if it consists only of the zero vector. Thus, \(M_{2}-M_{2}^{2}=M_{1}^{2}-M_{1}=0\), and \(M_{2}=M_{2}^{2}\) and \(M_{1}=M_{1}^{2}\). 

**Theorem B.47**: _Let \(M\) and \(M_{0}\) be perpendicular projection matrices with \(C(M_{0}){\subset}C(M)\). Then \(M-M_{0}\) is the perpendicular projection matrix onto \(C(M_{0})^{\perp}_{C(M)}\)._

_Proof_ Since \(C(M_{0})\subset C(M)\), \(MM_{0}=M_{0}\) and, by symmetry, \(M_{0}M=M_{0}\). Checking the conditions of Theorem B.33, we see that \((M-M_{0})^{2}=M^{2}-MM_{0}-M_{0}M+M_{0}^{2}=M-M_{0}-M_{0}+M_{0}=M-M_{0}\), and \((M-M_{0})^{\prime}=M-M_{0}\), so \(M-M_{0}\) is a ppo onto \(C(M-M_{0})\).

To see that \(C(M-M_{0})=C(M_{0})^{\perp}_{C(M)}\) note that \(C(M-M_{0})\perp C(M_{0})\), because \((M-M_{0})M_{0}=MM_{0}-M_{0}^{2}=M_{0}-M_{0}=0\). Thus, \(C(M-M_{0})\subset C(M_{0})^{\perp}_{C(M)}\). If \(x\in C(M)\) and \(x\perp C(M_{0})\), then \(x=Mx=(M-M_{0})x+M_{0}x=(M-M_{0})x\). Thus, \(x\in C(M-M_{0})\) and \(C(M_{0})^{\perp}_{C(M)}\subset C(M-M_{0})\). 

**Corollary B.48**: \(C(M-M_{0})=C(M_{0})^{\perp}_{C(M)}\)_._

**Corollary B.49**: \(r(M)=r(M_{0})+r(M-M_{0})\)_._

One particular application of these results involves \(I\), the perpendicular projection operator onto \({\bf R}^{n}\). For any other perpendicular projection operator \(M\), \(I-M\) is the perpendicular projection operator onto the orthogonal complement of \(C(M)\) with respect to \({\bf R}^{n}\). For example, the subspace of vectors with the form \((2a,a)^{\prime}\) has an orthogonal complement consisting of vectors with the form \((b,-2b)^{\prime}\). With \(M\) as given earlier,

\[I-M=\begin{bmatrix}1&0\\ 0&1\end{bmatrix}-\begin{bmatrix}0.8&0.4\\ 0.4&0.2\end{bmatrix}=\begin{bmatrix}0.2&-0.4\\ -0.4&0.8\end{bmatrix}.\]Note that

\[(I-M)\begin{pmatrix}b\\ -2b\end{pmatrix}=\begin{pmatrix}b\\ -2b\end{pmatrix}\quad\text{and}\quad(I-M)\begin{pmatrix}2a\\ a\end{pmatrix}=0;\]

so by definition \(I-M\) is the perpendicular projection operator onto the space of vectors with the form \((b,-2b)^{\prime}\).

At this point, we examine the relationship between perpendicular projection operations and the Gram-Schmidt theorem (Theorem A.16). Recall that in the Gram-Schmidt theorem, \(x_{1},\ldots,x_{r}\) denotes the original basis and \(y_{1},\ldots,y_{r}\) denotes the orthonormal basis. Let

\[M_{s}=\sum_{i=1}^{s}y_{i}y_{i}^{\prime}.\]

Applying Theorem B.35, \(M_{s}\) is the ppo onto \(C(x_{1},\ldots,x_{s})\). Now define

\[w_{s+1}=(I-M_{s})x_{s+1}.\]

Thus, \(w_{s+1}\) is the perpendicular projection of \(x_{s+1}\) onto the orthogonal complement of \(C(x_{1},\ldots,x_{s})\). Finally, \(y_{s+1}\) is just \(w_{s+1}\) normalized.

Consider the eigenvalues of a perpendicular projection operator \(M\). Let \(v_{1},\ldots,v_{r}\) be a basis for \(C(M)\). Then \(Mv_{i}=v_{i}\), so \(v_{i}\) is an eigenvector of \(M\) with eigenvalue \(1\). In fact, \(1\) is an eigenvalue of \(M\) with multiplicity \(r\). Now, let \(w_{1},\ldots,w_{n-r}\) be a basis for \(C(M)^{\perp}\). \(Mw_{j}=0\), so \(0\) is an eigenvalue of \(M\) with multiplicity \(n-r\). We have completely characterized the \(n\) eigenvalues of \(M\). Since \(\operatorname{tr}(M)\) equals the sum of the eigenvalues, we have \(\operatorname{tr}(M)=r(M)\).

In fact, if \(A\) is an \(n\times n\) matrix with \(A^{2}=A\), any basis for \(C(A)\) is a basis for the space of eigenvectors for the eigenvalue \(1\). The null space of \(A\) is the space of eigenvectors for the eigenvalue \(0\). The rank of \(A\) and the rank of the null space of \(A\) add to \(n\), and \(A\) has \(n\) eigenvalues, so all the eigenvalues are accounted for. Again, \(\operatorname{tr}(A)=r(A)\).

#### Definition B.50

(a) If \(A\) is a square matrix with \(A^{2}=A\), then \(A\) is called _idempotent_.

(b) Let \(\mathcal{N}\) and \(\mathcal{M}\) be two spaces with \(\mathcal{N}\cap\mathcal{M}=\{0\}\) and \(r(\mathcal{N})+r(\mathcal{M})=n\). The square matrix \(A\) is a _projection operator_ onto \(\mathcal{N}\) along \(\mathcal{M}\) if \(1\)) \(Av=v\) for any \(v\in\mathcal{N}\), and \(2\)) \(Aw=0\) for any \(w\in\mathcal{M}\).

If the square matrix \(A\) has the property that \(Av=v\) for any \(v\in C(A)\), then \(A\) is the projection operator (matrix) onto \(C(A)\) along \(C(A^{\prime})^{\perp}\). (Note that \(C(A^{\prime})^{\perp}\) is the null space of \(A\).) It follows immediately that if \(A\) is idempotent, then \(A\) is a projection operator onto \(C(A)\) along \(\mathcal{N}(A)=C(A^{\prime})^{\perp}=C(I-A)\), see Exercise B.22.

The uniqueness of projection operators can be established like it was for perpendicular projection operators. Note that \(x\in\mathbf{R}^{n}\) can be written uniquely as \(x=v+w\)for \(v\in{\cal N}\) and \(w\in{\cal M}\), i.e., \({\bf R}^{n}={\cal N}+{\cal M}\). To see this, take basis matrices for the two spaces, say \(N\) and \(M\), respectively. The result follows from observing that \([N,\,M]\) is a basis matrix for \({\bf R}^{n}\). Because of the rank conditions, \([N,\,M]\) is an \(n\,\times\,n\) matrix. It is enough to show that the columns of \([N,\,M]\) must be linearly independent.

\[0=[N,\,M]{b\brack c}=Nb+Mc\]

implies \(Nb=M(-c)\) which, since \({\cal N}\cap{\cal M}=\{0\}\), can only happen when \(Nb=0=M(-c)\), which, because they are basis matrices, can only happen when \(b=0=(-c)\), which implies that \({b\brack c}=0\), and we are done.

Any projection operator that is not a perpendicular projection is referred to as an _oblique projection operator_.

To show that a matrix \(A\) is a projection operator onto an arbitrary space, say \(C(X)\), it is necessary to show that \(C(A)=C(X)\) and that for \(x\in C(X)\), \(Ax=x\). A typical proof runs in the following pattern. First, show that \(Ax=x\) for any \(x\in C(X)\). This also establishes that \(C(X)\subset C(A)\). To finish the proof, it suffices to show that \(Av\in C(X)\) for any \(v\in{\bf R}^{n}\) because this implies that \(C(A)\subset C(X)\).

In this book, our use of the word "perpendicular" is based on the standard inner product that defines Euclidean distance. In other words, for two vectors \(x\) and \(y\), their inner product is \(x^{\prime}y\). By definition, the vectors \(x\) and \(y\) are orthogonal if their inner product is \(0\). In fact, for any two vectors \(x\) and \(y\), let \(\theta\) be the angle between \(x\) and \(y\). Then \(x^{\prime}y=\sqrt{x^{\prime}x}\sqrt{y^{\prime}y}\,\cos\theta\). The length of a vector \(x\) is defined as the square root of the inner product of \(x\) with itself, i.e., \(\|x\|\equiv\sqrt{x^{\prime}x}\). The distance between two vectors \(x\) and \(y\) is the length of their difference, i.e., \(\|x-y\|\).

These concepts can be generalized. For a positive definite matrix \(B\), we can define an inner product between \(x\) and \(y\) as \(x^{\prime}By\). As before, \(x\) and \(y\) are orthogonal if their inner product is \(0\) and the length of \(x\) is the square root of its inner product with itself (now \(\|x\|_{B}\equiv\sqrt{x^{\prime}Bx}\)). As argued above, any idempotent matrix is always a projection operator, but which one is the perpendicular projection operator depends on the inner product. As can be seen from Proposition 2.7.2 and Exercise 2.5, the matrix \(X(X^{\prime}BX)^{-}X^{\prime}B\) is an oblique projection onto \(C(X)\) for the standard inner product; but it is the perpendicular projection operator onto \(C(X)\) with the inner product defined using the matrix \(B\).

### Miscellaneous Results

**Proposition B.51**: _For any matrix \(X\), \(C(XX^{\prime})=C(X)\)._

_Proof_ Clearly \(C(XX^{\prime})\subset C(X)\), so we need to show that \(C(X)\subset C(XX^{\prime})\). Let \(x\in C(X)\). Then \(x=Xb\) for some \(b\). Write \(b=b_{0}+b_{1}\), where \(b_{0}\in C(X^{\prime})\) and \(b_{1}\perp C(X^{\prime})\). Clearly, \(Xb_{1}=0\), so we have \(x=Xb_{0}\). But \(b_{0}=X^{\prime}d\) for some \(d\); so \(x=Xb_{0}=XX^{\prime}d\) and \(x\in C(XX^{\prime})\).

**Corollary B.52**: _For any matrix \(X\), \(r(XX^{\prime})=r(X)\)._

_Proof_ See Exercise B.4. \(\Box\)

**Corollary B.53**: _If \(X_{n\times p}\) has \(r(X)=p\), then the \(p\,\times\,p\) matrix \(X^{\prime}X\) is nonsingular._

_Proof_ See Exercise B.5. \(\Box\)

**Proposition B.54**:
* _If_ \(C(U_{1})\subset C(U_{2})\)_, then_ \(C(XU_{1})\subset C(XU_{2})\)_._
* _If_ \(C(U_{1})=C(U_{2})\)_, then_ \(C(XU_{1})=C(XU_{2})\)_._
* \(C(XB)\subset C(X)\)__
* _If_ \(B\) _is nonsingular,_ \(C(XB)=C(X)\)_._

_Proof_ (a) Take \(v\in C(XU_{1})\). For some \(\gamma_{1},v=XU_{1}\gamma_{1}\). Because, \(C(U_{1})\subset C(U_{2})\), there exists \(\gamma_{2}\) so that \(U_{1}\gamma_{1}=U_{2}\gamma_{2}\). Clearly, \(v=XU_{1}\gamma_{1}=XU_{2}\gamma_{2}\in C(XU_{2})\). (b) Use (a) as is and with the roles of \(U_{1}\) and \(U_{2}\) reversed. (c) This is immediate from the definition of a column space but also, in a) take \(U_{1}=B\) and \(U_{2}=I\). (d) If \(B\) is nonsingular, \(C(B)=C(I)\) and use (b). \(\Box\)

It follows immediately from Proposition B.54 that, for \(B\) nonsingular, the perpendicular projection operators onto \(C(XB)\) and \(C(X)\) are identical.

We now show that generalized inverses always exist.

**Theorem B.55**: _For any matrix \(X\), there exists a generalized inverse \(X^{-}\)._

_Proof_ We know that \((X^{\prime}X)^{-}\) exists. Set \(X^{-}=(X^{\prime}X)^{-}X^{\prime}\). Then \(XX^{-}X=X(X^{\prime}X)^{-}X^{\prime}X=X\) because \(X(X^{\prime}X)^{-}X^{\prime}\) is a projection matrix onto \(C(X)\). \(\Box\)

Note that for any \(X^{-}\), the matrix \(XX^{-}\)is idempotent and hence a projection operator.

**Proposition B.56**: _When all inverses exist,_

\[[A+BCD]^{-1}=A^{-1}-A^{-1}B\left[C^{-1}+DA^{-1}B\right]^{-1}DA^{-1}.\]

_Proof_ If all inverses exist

\[[A+BCD]\left[A^{-1}-A^{-1}B\left[C^{-1}+DA^{-1}B\right]^{-1}DA^{-1}\right]\]

\[=I-B\left[C^{-1}+DA^{-1}B\right]^{-1}DA^{-1}+BCDA^{-1}\]

\[-BCDA^{-1}B\left[C^{-1}+DA^{-1}B\right]^{-1}DA^{-1}\]

\[I-B\left[I+CDA^{-1}B\right]\left[C^{-1}+DA^{-1}B\right]^{-1}DA^{-1}+BCDA^{-1}\]

\[I-BCDA^{-1}+BCDA^{-1}=I.\]

**Proposition B.57**: _Let \(P\) be a projection operator (idempotent), and let \(a\) and \(b\) be real numbers. Then_

\[[aI+bP]^{-1}=\frac{1}{a}\left[I-\frac{b}{a+b}P\right].\]

_Proof_

\[\frac{1}{a}\left[I-\frac{b}{a+b}P\right][aI+bP]=\frac{1}{a}\left[aI+bP-\frac{ ab}{a+b}P-\frac{b^{2}}{a+b}P\right]=I.\]

When we study linear models, we frequently need to refer to matrices and vectors that consist entirely of 1s. Such matrices are denoted by the letter \(J\) with various subscripts and superscripts to specify their dimensions. \(J_{r}^{c}\) is an \(r\times c\) matrix of 1s. The subscript indicates the number of rows and the superscript indicates the number of columns. If there is only one column, the superscript may be suppressed, e.g., \(J_{r}\equiv J_{r}^{1}\). In a context where we are dealing with vectors in \(\mathbf{R}^{n}\), the subscript may also be suppressed, e.g., \(J\equiv J_{n}\equiv J_{n}^{1}\).

A matrix of 0s is always denoted by 0.

### Properties of Kronecker Products and Vec Operators

Kronecker products and Vec operators are extremely useful in multivariate analysis and some approaches to variance component estimation. (Both are discussed in _ALM-III._) They are also often used in writing balanced ANOVA models. We now present their basic algebraic properties.

1. If the matrices are of conformable sizes, \([A\otimes(B+C)]=[A\otimes B]+[A\otimes C]\).
2. If the matrices are of conformable sizes, \([(A+B)\otimes C]=[A\otimes C]+[B\otimes C]\).
3. If \(a\) and \(b\) are scalars, \(ab[A\otimes B]=[aA\otimes bB]\).
4. If the matrices are of conformable sizes, \([A\otimes B][C\otimes D]=[AC\otimes BD]\).
5. The transpose of a Kronecker product matrix is \([A\otimes B]^{\prime}=[A^{\prime}\otimes B^{\prime}]\).
6. The generalized inverse of a Kronecker product matrix is \([A\otimes B]^{-}=[A^{-}\otimes B^{-}]\).
7. For two vectors \(v\) and \(w\), \(\operatorname{Vec}(vw^{\prime})=w\otimes v\).
8. For a matrix \(W\) and conformable matrices \(A\) and \(B\), \(\operatorname{Vec}(AWB^{\prime})=[B\otimes A]\operatorname{Vec}(W)\).
9. For conformable matrices \(A\) and \(B\), \(\operatorname{Vec}(A)^{\prime}\operatorname{Vec}(B)=\operatorname{tr}(A^{ \prime}B)\).
10. The Vec operator commutes with any matrix operation that is performed elementwise. For example, \(\operatorname{E}\{\operatorname{Vec}(W)\}=\operatorname{Vec}\{\operatorname{E} (W)\}\) when \(W\) is a random matrix. Similarly, for conformable matrices \(A\) and \(B\) and scalar \(\phi\), \(\operatorname{Vec}(A+B)=\operatorname{Vec}(A)+\operatorname{Vec}(B)\) and \(\operatorname{Vec}(\phi A)=\phi\operatorname{Vec}(A)\).
11. If \(A\) and \(B\) are positive definite, then \(A\otimes B\) is positive definite.

Most of these are well-known facts and easy to establish. Two of them are somewhat more unusual. Proofs for Items 8 and 11 are given in _ALM-III_, Appendix A.2 and in earlier editions of this book.

### Tensors

Tensors are simply an alternative notation for writing vectors. This notation has substantial advantages when dealing with quadratic forms and when dealing with more general concepts than quadratic forms. Our main purpose in discussing them here is simply to illustrate how flexibly subscripts can be used in writing vectors.

Consider a vector \(Y=(y_{1},\ldots,y_{n})^{\prime}\). The tensor notation for this is simply \(y_{i}\). We can write another vector \(a=(a_{1},\ldots,a_{n})^{\prime}\) as \(a_{i}\). When written individually, the subscript is not important. In other words, \(a_{i}\) is the same vector as \(a_{j}\). Note that the length of these vectors needs to be understood from the context. Just as when we write \(Y\) and \(a\) in conventional vector notation, there is nothing in the notation \(y_{i}\) or \(a_{i}\) to tell us how many elements are in the vector.

If we want the inner product \(a^{\prime}Y\), in tensor notation we write \(a_{i}y_{i}\). Here we are using something called the _summation convention_. Because the subscripts on \(a_{i}\) and \(y_{i}\) are the same, \(a_{i}y_{i}\) is taken to mean \(\sum_{i=1}^{n}a_{i}y_{i}\). If, on the other hand, we wrote \(a_{i}y_{j}\), this means something completely different. \(a_{i}y_{j}\) is an alternative notation for the Kronecker product \([a\otimes Y]=(a_{1}y_{1},\ldots,a_{1}y_{n},a_{2}y_{1},\ldots,a_{n}y_{n})^{\prime}\). In \([a\otimes Y]\equiv a_{i}y_{j}\), we have two subscripts identifying the rows of the vector.

Now, suppose we want to look at a quadratic form \(Y^{\prime}AY\), where \(Y\) is an \(n\) vector and \(A\) is \(n\times n\). One way to rewrite this is

\[Y^{\prime}AY=\sum_{i=1}^{n}\sum_{j=1}^{n}y_{i}a_{ij}y_{j}=\sum_{i=1}^{n}\sum_{ j=1}^{n}a_{ij}y_{i}y_{j}=\text{Vec}(A)^{\prime}[Y\otimes Y].\]

(From Property B.5.8 we also have \(Y^{\prime}AY=[Y^{\prime}\otimes Y^{\prime}]\text{Vec}(A)\).) Here we have rewritten the quadratic form as a linear combination of the elements in the vector \([Y\otimes Y]\). The linear combination is determined by the elements of the vector \(\text{Vec}(A)\). In tensor notation, this becomes quite simple. Using the summation convention in which objects with the same subscript are summed over,

\[Y^{\prime}AY=y_{i}a_{ij}y_{j}=a_{ij}y_{i}y_{j}.\]

The second term just has the summation signs removed, but the third term, which obviously gives the same sum as the second, is actually the tensor notation for \(\text{Vec}(A)^{\prime}[Y\otimes Y]\). Again, \(\text{Vec}(A)=(a_{11},a_{21},a_{31},\ldots,a_{nn})^{\prime}\) uses two subscripts to identify rows of the vector. Obviously, if you had a need to consider things like \[\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n}a_{ijk}y_{i}y_{j}y_{k}\equiv a_{ijk}y_{i} y_{j}y_{k},\]

the tensor version \(a_{ijk}y_{i}y_{j}y_{k}\) saves some work.

There is one slight complication in how we have been writing things. Suppose \(A\) is not symmetric and we have another \(n\) vector \(W\). Then we might want to consider

\[W^{\prime}AY=\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}a_{ij}y_{j}.\]

From item 8 in the previous subsection,

\[W^{\prime}AY=\mbox{Vec}(W^{\prime}AY)=[Y^{\prime}\otimes W^{\prime}]\mbox{Vec} (A).\]

Alternatively,

\[W^{\prime}AY=\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}a_{ij}y_{j}=\sum_{i=1}^{n}\sum_{j =1}^{n}a_{ij}y_{j}w_{i}=\mbox{Vec}(A)^{\prime}[Y\otimes W]\]

or \(W^{\prime}AY=Y^{\prime}A^{\prime}W=\mbox{Vec}(A^{\prime})^{\prime}[W\otimes Y]\). However, with \(A\) nonsymmetric, \(W^{\prime}A^{\prime}Y=\mbox{Vec}(A^{\prime})^{\prime}[Y\otimes W]\) is typically different from \(W^{\prime}AY\). The Kronecker notation requires that care be taken in specifying the order of the vectors in the Kronecker product, and whether or not to transpose \(A\) before using the Vec operator. In tensor notation, \(W^{\prime}AY\) is simply \(w_{i}a_{ij}y_{j}\). In fact, the orders of the vectors can be permuted in any way; so, for example, \(a_{ij}y_{j}w_{i}\) means the same thing. \(W^{\prime}A^{\prime}Y\) is simply \(w_{i}a_{ji}y_{j}\). The tensor notation and the matrix notation require less effort than the Kronecker notation.

For our purposes, the real moral here is simply that the subscripting of an individual vector does not matter. We can write a vector \(Y=(y_{1},\ldots,y_{n})^{\prime}\) as \(Y=[y_{k}]\) (in tensor notation as simply \(y_{k}\)), or we can write the same \(n\) vector as \(Y=[y_{ij}]\) (in tensor notation, simply \(y_{ij}\)), where, as long as we know the possible values that \(i\) and \(j\) can take on, the actual order in which we list the elements is not of much importance. Thus, if \(i=1\),..., \(t\) and \(j=1\),..., \(N_{i}\), with \(n=\sum_{i=1}^{t}N_{i}\), it really does not matter if we write a vector \(Y\) as \((y_{1},\ldots,y_{n})\), or \((y_{11},\ldots,y_{1N_{1}},y_{21},\ldots,y_{tN_{i}})^{\prime}\) or \((y_{t1},\ldots,y_{tN_{t}},y_{t-1,1},\ldots,y_{1N_{1}})^{\prime}\) or in any other fashion we may choose, as long as we keep straight which row of the vector is which. Thus, a linear combination \(a^{\prime}Y\) can be written \(\sum_{k=1}^{n}a_{k}y_{k}\) or \(\sum_{i=1}^{t}\sum_{j=1}^{N_{i}}a_{ij}y_{ij}\). In tensor notation, the first of these is simply \(a_{k}y_{k}\) and the second is \(a_{ij}y_{ij}\). These ideas become very handy in examining analysis of variance models, where the standard approach is to use multiple subscripts to identify the various observations. The subscripting has no intrinsic importance; the only thing that matters is knowing which row is which in the vectors. The subscripts are an aid in this identification, but they do not create any problems. We can still put all of the observations into a vector and use standard operations on them.

### Exercises

**Exercise B.0**

(a) Let \(X=[X_{0},\,X_{1}]\) with \(M\) and \(M_{0}\) the ppos onto \(C(X)\) and \(C(X_{0})\), respectively. Show that \((I-M_{0})X_{1}[X^{\prime}_{0}(I-M_{0})X_{1}]^{-}X^{\prime}_{1}(I-M_{0})\) is the ppo onto \(C(X_{0})^{\perp}_{C(X)}\).

(b) Let \(r\) and \(s\) be two \(n\) vectors. Let \(M_{r}\) be the ppo onto \(C(r)\), then \(s^{\prime}(I-M_{r})s\geq 0\). Use this fact to prove the Cauchy-Schwarz inequality,

\[(s^{\prime}r)^{2}\leq s^{\prime}s\ r^{\prime}r.\]

**Exercise B.1**

(a) Show that

\[A^{k}x+b_{k-1}A^{k-1}x+\cdots+b_{0}x=(A-\mu I)\left(A^{k-1}x+\tau_{k-2}A^{k-2} x+\cdots+\tau_{0}x\right)=0,\]

where \(\mu\) is any nonzero solution of \(b_{0}+b_{1}w+\cdots+b_{k}w^{k}=0\) with \(b_{k}=1\) and \(\tau_{j}=-(b_{0}+b_{1}\mu+\cdots+b_{j}\mu^{j})/\mu^{j+1}\), \(j=0,\ldots,k\).

(b) Show that if the only root of \(b_{0}+b_{1}w+\cdots+b_{k}w^{k}\) is zero, then the factorization in (a) still holds.

(c) The solution \(\mu\) used in (a) need not be a real number, in which case \(\mu\) is a complex eigenvalue and the \(\tau_{i}\)s are complex; so the eigenvector is complex. Show that with \(A\) symmetric, \(\mu\) must be real because the eigenvalues of \(A\) must be real. In particular, assume that

\[A(y+iz)=(\lambda+i\gamma)(y+iz),\]

for \(y\), \(z\), \(\lambda\), and \(\gamma\) real vectors and scalars, respectively, set \(Ay=\lambda y-\gamma z\), \(Az=\gamma y+\lambda z\), and examine \(z^{\prime}Ay=y^{\prime}Az\).

**Exercise B.2** Prove Proposition B.32.

**Exercise B.3** Show that any nonzero symmetric matrix \(A\) can be written as \(A=PDP^{\prime}\), where \(C(A)=C(P)\), \(P^{\prime}P=I\), and \(D\) is nonsingular.

**Exercise B.4** Prove Corollary B.52.

**Exercise B.5** Prove Corollary B.53.

**Exercise B.6** Show \(\operatorname{tr}(cI_{n})=nc\).

**Exercise B.7**: Let \(a\), \(b\), \(c\), and \(d\) be real numbers. If \(ad-bc\neq 0\), find the inverse of

\[\begin{bmatrix}a&b\\ c&d\end{bmatrix}.\]

**Exercise B.8**: Prove Theorem B.28, i.e., let \(A\) be an \(r\times s\) matrix, let \(B\) be an \(s\times r\) matrix, and show that \(\operatorname{tr}(AB)=\operatorname{tr}(BA)\).

**Exercise B.9**: Determine whether the matrices given below are positive definite, nonnegative definite, or neither.

\[\begin{bmatrix}3&2&-2\\ 2&2&-2\\ -2&-2&10\end{bmatrix},\quad\begin{bmatrix}26&-2&-7\\ -2&4&-6\\ -7&-6&13\end{bmatrix},\quad\begin{bmatrix}26&2&13\\ 2&4&6\\ 13&6&13\end{bmatrix},\quad\begin{bmatrix}3&2&-2\\ 2&-2&-2\\ -2&-2&10\end{bmatrix}.\]

**Exercise B.10**: Show that the matrix \(B\) given below is positive definite, and find a matrix \(Q\) such that \(B=QQ^{\prime}\). (Hint: The first row of \(Q\) can be taken as \((1,-1,0)\).)

\[B=\begin{bmatrix}2&-1&1\\ -1&1&0\\ 1&0&2\end{bmatrix}.\]

**Exercise B.11**: Let

\[A=\begin{bmatrix}2&0&4\\ 1&5&7\\ 1&-5&-3\end{bmatrix},\quad B=\begin{bmatrix}1&0&0\\ 0&0&1\\ 0&1&0\end{bmatrix},\quad C=\begin{bmatrix}1&4&1\\ 2&5&1\\ -3&0&1\end{bmatrix}.\]

Use Theorem B.35 to find the perpendicular projection operator onto the column space of each matrix.

**Exercise B.12**: Show that for a perpendicular projection matrix \(M\),

\[\sum_{i}\sum_{j}m_{ij}^{2}=r(M).\]

**Exercise B.13**: Prove that if \(M=M^{\prime}M\), then \(M=M^{\prime}\) and \(M=M^{2}\).

**Exercise B.14**: Let \(M_{1}\) and \(M_{2}\) be perpendicular projection matrices, and let \(M_{0}\) be a perpendicular projection operator onto \(C(M_{1})\cap C(M_{2})\). Show that the following are equivalent:

(a) \(M_{1}M_{2}=M_{2}M_{1}\).

(b) \(M_{1}M_{2}=M_{0}\).

(c) \(\left\{C(M_{1})\cap[C(M_{1})\cap C(M_{2})]^{\perp}\right\}\perp\left\{C(M_{2})\cap[ C(M_{1})\cap C(M_{2})]^{\perp}\right\}\).

Hints: (i) Show that \(M_{1}M_{2}\) is a projection operator. (ii) Show that \(M_{1}M_{2}\) is symmetric. (iii) Note that \(C(M_{1})\cap[C(M_{1})\cap C(M_{2})]^{\perp}=C(M_{1}-M_{0})\).

**Exercise B.15**: Let \(M_{1}\) and \(M_{2}\) be perpendicular projection matrices. Show that

(a) the eigenvalues of \(M_{1}M_{2}\) are no greater than 1 in absolute value (they may be complex);

(b) \(\operatorname{tr}(M_{1}M_{2})\leq r(M_{1}M_{2})\).

Hints: For part (a) show that with \(x^{\prime}Mx\equiv\|Mx\|^{2}\), \(\|Mx\|\leq\|x\|\) for any perpendicular projection operator \(M\). Use this to show that if \(M_{1}M_{2}x=\lambda x\), then \(\|M_{1}M_{2}x\|\geq|\lambda|\ \|M_{1}M_{2}x\|\).

**Exercise B.16**: For vectors \(x\) and \(y\), let \(M_{x}=x(x^{\prime}x)^{-1}x^{\prime}\) and \(M_{y}=y(y^{\prime}y)^{-1}y^{\prime}\). Show that \(M_{x}M_{y}=M_{y}M_{x}\) if and only if \(C(x)=C(y)\) or \(x\perp y\).

**Exercise B.17**: Consider the matrix

\[A=\begin{bmatrix}0&1\\ 0&1\end{bmatrix}.\]

(a) Show that \(A\) is a projection matrix.

(b) Is \(A\) a perpendicular projection matrix? Why or why not?

(c) Describe the space that \(A\) projects onto and the space that \(A\) projects along. Sketch these spaces.

(d) Find another projection operator onto the space that \(A\) projects onto.

**Exercise B.18**: Let \(A\) be an arbitrary projection matrix. Show that \(C(I-A)=C(A^{\prime})^{\perp}\).

Hints: Recall that \(C(A^{\prime})^{\perp}\) is the null space of \(A\). Show that \((I-A)\) is a projection matrix.

**Exercise B.19**: Show that if \(A^{-}\) is a generalized inverse of \(A\), then so is

\[G=A^{-}AA^{-}+(I-A^{-}A)B_{1}+B_{2}(I-AA^{-})\]

for any choices of \(B_{1}\) and \(B_{2}\) with conformable dimensions.

**Exercise B.20**: Let \(A\) be positive definite with eigenvalues \(\lambda_{1}\),..., \(\lambda_{n}\). Show that \(A^{-1}\) has eigenvalues \(1/\lambda_{1}\),..., \(1/\lambda_{n}\) and the same eigenvectors as \(A\).

## Appendix B Matrix Results

**Exercise B.21**  For \(A\) nonsingular, let

\[A=\begin{bmatrix}A_{11}&A_{12}\\ A_{21}&A_{22}\end{bmatrix},\]

and let \(A_{1\cdot 2}=A_{11}-A_{12}A_{22}^{-1}A_{21}\). Show that if all inverses exist,

\[A^{-1}=\begin{bmatrix}A_{1\cdot 2}^{-1}&-A_{1\cdot 2}^{-1}A_{12}A_{22}^{-1} \\ \\ -A_{22}^{-1}A_{21}A_{1\cdot 2}^{-1}&A_{22}^{-1}+A_{22}^{-1}A_{21}A_{1\cdot 2}^{-1}A_{12}A_{22}^{-1}\end{bmatrix}\]

and that

\[A_{22}^{-1}+A_{22}^{-1}A_{21}A_{1\cdot 2}^{-1}A_{12}A_{22}^{-1}=\begin{bmatrix}A_{22}-A_{21}A_{11}^{-1}A_{12} \end{bmatrix}^{-1}.\]

**Exercise B.22**  Show that if \(A\) is idempotent, then \(\mathcal{N}(A)=C(I-A)\). Hint: Show that each set is contained in the other.

**Exercise B.23**  Consider the vectors that are the columns of a matrix \(X_{n\times p}\) with \(r(X)=r\). A _rotation_ of these vectors keeps their lengths the same and the angles between them the same. In other words, it keeps all of the inner products between them the same. If \(P\) is an orthonormal matrix, then \(Z=PX\) is a rotation of the columns of X because \(Z^{\prime}Z=X^{\prime}P^{\prime}PX=X^{\prime}X\).

Rotating vectors within a subspace is more difficult. Let the \(n\times r\) matrix \(Q\) have columns that are an orthonormal basis for \(C(X)\). Write \(X=QB\). Show that a rotation of the columns of \(X\) that remains within \(C(X)\) is obtained from \(Z=QPB\) where \(P\) is an \(r\times r\) orthonormal matrix.

**Exercise B.24**  Show that \(r(X)=r(X^{\prime})\). Hints: Let \(X\) be \(n\times p\) with \(r(X^{\prime})=r\). Let \(\tilde{X}\) be an \(r\times p\) matrix with the rows of \(\tilde{X}\) forming a basis for \(C(X^{\prime})\). Write \(X=B\tilde{X}\) and argue that \(r(X)\leq r=r(X^{\prime})\). Reverse the roles of \(X\) and \(X^{\prime}\).

[MISSING_PAGE_FAIL:493]

\[W=\frac{X}{\sqrt{Y/n}}\]

has a _noncentral \(t\) distribution_ with \(n\) degrees of freedom and noncentrality parameter \(\mu\). Write \(W\sim t(n,\mu)\). If \(\mu=0\), we say that the distribution is a central \(t\) distribution and write \(W\sim t(n)\). The \(100\alpha\)th percentile of a \(t(n)\) distribution is denoted \(t(\alpha,n)\).

**Definition C.3**.: Let \(X\sim\chi^{2}(r,\gamma)\) and \(Y\sim\chi^{2}(s,0)\) with \(X\) and \(Y\) independent. Then

\[W=\frac{X/r}{Y/s}\]

has a _noncentral \(F\) distribution_ with \(r\) numerator and \(s\) denominator degrees of freedom and noncentrality parameter \(\gamma\). Write \(W\sim F(r,s,\gamma)\). If \(\gamma=0\), write \(W\sim F(r,s)\) for the central \(F\) distribution. The \(100\alpha\)th percentile of \(F(r,s)\) is denoted \(F(\alpha,r,s)\).

As indicated, if the noncentrality parameter of any of these distributions is zero, the distribution is referred to as a _central distribution_ (e.g., central \(F\) distribution). The central distributions are those commonly used in statistical methods courses. If any of these distributions is not specifically identified as a noncentral distribution, it should be assumed to be a central distribution.

It is easily seen from Definition C.1 that any noncentral chi-squared distribution _tends_ to be larger than the central chi-squared distribution with the same number of degrees of freedom. Similarly, from Definition C.3, a noncentral \(F\) tends to be larger than the corresponding central \(F\) distribution. (These ideas are made rigorous in Exercise C.1.) The fact that the noncentral \(F\) distribution tends to be larger than the corresponding central \(F\) distribution is the basis for many of the tests used in linear models. Typically, test statistics are used that have a central \(F\) distribution if the reduced (null) model is true and a noncentral \(F\) distribution if the full model is true but the null model is not. Since the noncentral \(F\) distribution tends to be larger, large values of the test statistic are more consistent with the full model than with the null. Thus, the form of an appropriate rejection region when the full model is true is to reject the null hypothesis for large values of the test statistic.

The power of these \(F\) tests is simply a function of the noncentrality parameter. Given a value for the noncentrality parameter, there is no theoretical difficulty in finding the power of an \(F\) test. The power simply involves computing the probability of the rejection region when the probability distribution is a noncentral \(F\). Davies (1980) gives an algorithm for making these and more general computations.

We now prove a theorem about central \(F\) distributions that will be useful in Chapter 5.

**Theorem C.4**.: _If \(s>t\), then \(s\,F(1-\alpha,s,v)\geq t\,F(1-\alpha,t,v)\)._Proof: Let \(X\sim\chi^{2}(s)\), \(Y\sim\chi^{2}(t)\), and \(Z\sim\chi^{2}(v)\). Let \(Z\) be independent of \(X\) and \(Y\). Note that \((X/s)\big{/}(Z/v)\) has an \(F(s,\,v)\) distribution; so \(s\,F(1-\alpha,\,s,\,v)\) is the \(100(1-\alpha)\) percentile of the distribution of \(X\big{/}(Z/v)\). Similarly, \(t\,F(1-\alpha,\,t,\,v)\) is the \(100(1-\alpha)\) percentile of the distribution of \(Y\big{/}(Z/v)\).

We will first argue that to prove the theorem it is enough to show that

\[\Pr\left[X\leq d\right]\leq\Pr\left[Y\leq d\right] \tag{1}\]

for all real numbers \(d\). We will then show that (1) is true.

If (1) is true, if \(c\) is any real number, and if \(Z=z\), by independence we have

\[\Pr\left[X\leq cz/v\right]=\Pr\left[X\leq cz/v|Z=z\right]\leq\Pr\left[Y\leq cz /v|Z=z\right]=\Pr\left[Y\leq cz/v\right].\]

Taking expectations with respect to \(Z\),

\[\Pr\left[X\big{/}(Z/v)\leq c\right] = \text{E}\left(\Pr\left[X\leq cz/v|Z=z\right]\right)\] \[\leq \text{E}\left(\Pr\left[Y\leq cz/v|Z=z\right]\right)\] \[= \Pr\left[Y\big{/}(Z/v)\leq c\right].\]

Since the cumulative distribution function (cdf) for \(X\big{/}(Z/v)\) is always no greater than the cdf for \(Y\big{/}(Z/v)\), the point at which a probability of \(1-\alpha\) is attained for \(X\big{/}(Z/v)\) must be no less than the similar point for \(Y\big{/}(Z/v)\). Therefore,

\[s\,F(1-\alpha,\,s,\,v)\geq t\,F(1-\alpha,\,t,\,v).\]

To see that (1) holds, let \(Q\) be independent of \(Y\) and \(Q\sim\chi^{2}(s-t)\). Then, because \(Q\) is nonnegative,

\[\Pr\left[X\leq d\right]=\Pr\left[Y+\,Q\leq d\right]\leq\Pr\left[Y\leq d\right].\]

## Exercise

Definition 0.5: Consider two random variables \(W_{1}\) and \(W_{2}\). \(W_{2}\) is said to be _stochastically larger_ than \(W_{1}\) if for every real number \(w\)

\[\Pr\left[W_{1}>w\right]\leq\Pr\left[W_{2}>w\right].\]

If for some random variables \(W_{1}\) and \(W_{2}\), \(W_{2}\) is stochastically larger than \(W_{1}\), then we also say that the distribution of \(W_{2}\) is stochastically larger than the distribution of \(W_{1}\).

**Exercise C.1**  Show that a noncentral chi-squared distribution is stochastically larger than the central chi-squared distribution with the same degrees of freedom. Show that a noncentral \(F\) distribution is stochastically larger than the corresponding central \(F\) distribution.

## Appendix D Multivariate Distributions

**Abstract** This appendix reviews properties of multivariate distributions. It also examines the concept of identifiable parameters.

Let \((x_{1},\ldots,x_{n})^{\prime}\) be a random vector. The joint cumulative distribution function (cdf) of \((x_{1},\ldots,x_{n})^{\prime}\) is

\[F(u_{1},\ldots,u_{n})\equiv\Pr\left[x_{1}\leq u_{1},\ldots,x_{n}\leq u_{n} \right].\]

If \(F(u_{1},\ldots,u_{n})\) is the cdf of a discrete random variable, we can define a (joint) probability mass function

\[f(u_{1},\ldots,u_{n})\equiv\Pr\left[x_{1}=u_{1},\ldots,x_{n}=u_{n}\right].\]

If \(F(u_{1},\ldots,u_{n})\) admits the \(n\)th order mixed partial derivative, then we can define a (joint) density function

\[f(u_{1},\ldots,u_{n})\equiv\frac{\partial^{n}}{\partial u_{1}\cdots\partial u _{n}}F(u_{1},\ldots,u_{n}).\]

The cdf can be recovered from the density as

\[F(u_{1},\ldots,u_{n})=\int_{-\infty}^{u_{1}}\cdots\int_{-\infty}^{u_{n}}f(w_{1 },\ldots,w_{n})dw_{1}\cdots dw_{n}.\]

For a function \(g(\cdot)\) of \((x_{1},\ldots,x_{n})^{\prime}\) into **R**, the expected value is defined as

\[\mathrm{E}\left[g(x_{1},\ldots,x_{n})\right]=\int_{-\infty}^{\infty}\cdots\int _{-\infty}^{\infty}g(u_{1},\ldots,u_{n})f(u_{1},\ldots,u_{n})du_{1}\cdots du_{ n}.\]

We might also write this as \(\mathrm{E}_{x}\left[g(x)\right]\).

We now consider relationships between two random vectors, say \(x=(x_{1},\ldots,x_{n})^{\prime}\) and \(y{=}(y_{1},\ldots,y_{m})^{\prime}\). Assume that the joint vector \((x^{\prime},y^{\prime})^{\prime}{=}(x_{1},\ldots,x_{n},y_{1},\ldots,y_{m})^{\prime}\) has a density function

\[f_{x,y}(u,v)\equiv f_{x,y}(u_{1},\ldots,u_{n},v_{1},\ldots,v_{m}).\]

Similar definitions and results hold if \((x^{\prime},y^{\prime})^{\prime}\) has a probability mass function.

The distribution of one random vector, say \(x\), ignoring the other vector, \(y\), is called the _marginal distribution_ of \(x\). The marginal cdf of \(x\) can be obtained by substituting the value \(+\infty\) into the joint cdf for all of the \(y\) variables:

\[F_{x}(u)=F_{x,y}(u_{1},\ldots,u_{n},+\infty,\ldots,+\infty).\]

The marginal density can be obtained either by partial differentiation of \(F_{x}(u)\) or by integrating the joint density over the \(y\) variables:

\[f_{x}(u)=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}f_{x,y}(u_{1}, \ldots,u_{n},v_{1},\ldots,v_{m})dv_{1}\cdots dv_{m}.\]

The conditional density of a vector, say \(x\), given the value of the other vector, say \(y=v\), is obtained by dividing the density of \((x^{\prime},y^{\prime})^{\prime}\) by the density of \(y\) evaluated at \(v\), i.e.,

\[f_{x|y}(u|v)\equiv f_{x,y}(u,v)\big{/}f_{y}(v).\]

The conditional density is a well-defined density, so expectations with respect to it are well defined. Let \(g\) be a function from \({\bf R}^{n}\) into \({\bf R}\),

\[{\rm E}[g(x)|y=v]=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}g(u)f_{x| y}(u|v)du,\]

where \(du\equiv du_{1}du_{2}\cdots du_{n}\). Sometimes we write

\[{\rm E}_{x|y=v}[g(x)]\equiv{\rm E}[g(x)|y=v]\,.\]

The standard properties of expectations hold for conditional expectations. For example, with \(a\) and \(b\) real,

\[{\rm E}[ag_{1}(x)+bg_{2}(x)|y=v]=a{\rm E}[g_{1}(x)|y=v]+b{\rm E}[g_{2}(x)|y=v]\,.\]

The conditional expectation of \({\rm E}[g(x)|y=v]\) is a function of the value \(v\). Since \(y\) is random, we can consider \({\rm E}[g(x)|y=v]\) as a random variable. In this context we write \({\rm E}[g(x)|y]\) or \({\rm E}_{x|y}[g(x)]\). An important property of conditional expectations is

\[{\rm E}[g(x)]={\rm E}[\,{\rm E}[g(x)|y]\,]\,.\]

To see this, note that \(f_{x|y}(u|v)f_{y}(v)=f_{x,y}(u,v)\) and \[\begin{split}\operatorname{E}[\operatorname{E}[g(x)|y]\,]& =\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}\operatorname{E}[g(x)|y=v]\,f_{y}(v)dv\\ &=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}\left[\int_ {-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}g(u)f_{x|y}(u|v)du\right]f_{y}(v)dv\\ &=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}g(u)\,f_{x|y}(u|v)f_{y}(v)du\,dv\\ &=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}g(u)\,f_{x,y}(u,v)du\,dv\\ &=\operatorname{E}[g(x)]\,.\end{split}\]

In fact, both the notion of conditional expectation and this result can be generalized. Consider a function \(g(x,\,y)\) from \(\mathbf{R}^{n+m}\) into \(\mathbf{R}\). If \(y=v\), we can define \(\operatorname{E}[g(x,\,y)|y=v]\) in a natural manner. If we consider \(y\) as random, we write \(\operatorname{E}[g(x,\,y)|y]\). It can be easily shown that

\[\operatorname{E}[g(x,\,y)]=\operatorname{E}[\operatorname{E}[g(x,\,y)|y]]\,.\]

A function of \(x\) or \(y\) alone can also be considered as a function from \(\mathbf{R}^{n+m}\) into \(\mathbf{R}\).

A second important property of conditional expectations is that if \(h(y)\) is a function from \(\mathbf{R}^{m}\) into \(\mathbf{R}\), we have

\[\operatorname{E}[h(y)g(x,\,y)|y]=h(y)\operatorname{E}[g(x,\,y)|y]\,. \tag{1}\]

This follows because if \(y=v\),

\[\begin{split}\operatorname{E}[h(y)g(x,\,y)|y=v]&= \int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}h(v)g(u,v)f_{x|y}(u|v)du\\ &=h(v)\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{\infty}g(u,v)f_ {x|y}(u|v)du\\ &=h(v)\operatorname{E}[g(x,\,y)|y=v]\,.\end{split}\]

This is true for all \(v\), so (1) holds. In particular, if \(g(x,\,y)\equiv 1\), we get

\[\operatorname{E}[h(y)|y]=h(y).\]

Finally, we can extend the idea of conditional expectation to a function \(g(x,\,y)\) from \(\mathbf{R}^{n+m}\) into \(\mathbf{R}^{s}\). Write \(g(x,\,y)=[g_{1}(x,\,y),\,\ldots,\,g_{s}(x,\,y)]^{\prime}\). Then define

\[\operatorname{E}[g(x,\,y)|y]=\left(\operatorname{E}[g_{1}(x,\,y)|y]\,,\,\ldots, \,\operatorname{E}[g_{s}(x,\,y)|y]\right)^{\prime}.\]

If their densities exist, two random vectors are _independent_ if and only if their joint density is equal to the product of their marginal densities, i.e., \(x\) and \(y\) are independent if and only if \[f_{x,y}(u,v)=f_{x}(u)f_{y}(v).\]

Note that if \(x\) and \(y\) are independent,

\[f_{x|y}(u|v)=f_{x}(u).\]

_If the random vectors \(x\) and \(y\) are independent, then any (reasonable) vector-valued functions of them, say \(g(x)\) and \(h(y)\)_, _are also independent_. This follows easily from a more general definition of the independence of two random vectors: The random vectors \(x\) and \(y\) are independent if for any two (reasonable) sets \(A\) and \(B\),

\[\Pr[x\in A,y\in B]=\Pr[x\in A]\Pr[y\in B].\]

To prove that functions of random variables are independent, recall that the set inverse of a function \(g(u)\) on a set \(A_{0}\) is \(g^{-1}(A_{0})\equiv\{u|g(u)\in A_{0}\}\). That \(g(x)\) and \(h(y)\) are independent follows from the fact that for any (reasonable) sets \(A_{0}\) and \(B_{0}\),

\[\Pr[g(x)\in A_{0},h(y)\in B_{0}] =\Pr[x\in g^{-1}(A_{0}),y\in h^{-1}(B_{0})]\] \[=\Pr[x\in g^{-1}(A_{0})]\Pr[y\in h^{-1}(B_{0})]\] \[=\Pr[g(x)\in A_{0}]\Pr[h(y)\in B_{0}].\]

By "reasonable" I mean things that satisfy the mathematical definitions of being measurable.

The _characteristic function_ of a random vector \(x=(x_{1},\ldots,x_{n})^{\prime}\) is a function from \({\bf R}^{n}\) to \({\bf C}\), the complex numbers. It is defined by

\[\varphi_{x}(t_{1},\ldots,t_{n})=\int_{-\infty}^{\infty}\cdots\int_{-\infty}^{ \infty}\exp\!\left[i\sum_{j=1}^{n}t_{j}u_{j}\right]f_{x}(u_{1},\ldots,u_{n})du _{1}\cdots du_{n}.\]

We are interested in characteristic functions because if \(x=(x_{1},\ldots,x_{n})^{\prime}\) and \(y=(y_{1},\ldots,y_{n})^{\prime}\) are random vectors and if

\[\varphi_{x}(t_{1},\ldots,t_{n})=\varphi_{y}(t_{1},\ldots,t_{n})\]

for all \((t_{1},\ldots,t_{n})\), then \(x\) and \(y\) have the same distribution.

For convenience, we have assumed the existence of densities. With minor modifications, the definitions and results of this appendix hold for any probability defined on \({\bf R}^{n}\).

### Identifiability

For better or worse (usually worse) much of statistical practice focuses on estimating and testing parameters. Identifiability is a property that ensures that this process is a sensible one.

Consider a collection of probability distributions \(Y\sim P_{\theta},\theta\in\Theta\). The parameter \(\theta\) merely provides the name (index) for each distribution in the collection. Identifiability ensures that each distribution has a unique name/index.

**Definition D.1**: The parameterization \(\theta\in\Theta\) is _identifiable_ if \(Y_{1}\sim P_{\theta_{1}},Y_{2}\sim P_{\theta_{2}}\), and \(Y_{1}\sim Y_{2}\) imply that \(\theta_{1}=\theta_{2}\).

Being identifiable is easily confused with the concept of being well defined.

**Definition D.2**: The parameterization \(\theta\in\Theta\) is _well defined_ if \(Y_{1}\sim P_{\theta_{1}},Y_{2}\sim P_{\theta_{2}}\), and \(\theta_{1}=\theta_{2}\) imply that \(Y_{1}\sim Y_{2}\).

The problem with not being identifiable is that some distributions have more than one name. Observed data give you information about the correct distribution and thus about the correct name. Typically, the more data you have, the more information you have about the correct name. Estimation is about getting close to the correct name and testing hypotheses is about deciding which of two lists contains the correct name. If a distribution has more than one name, it could be in both lists. (Significance testing is about whether it seems plausible that a name is on a list, so identifiability seems less of an issue.) If a distribution has more than one name, does getting close to one of those names really help? In applications to linear models, typically distributions have only one name or they have an infinite number of names.

The ideas are roughly this. If the distributions are well defined and I know that Wesley O. Johnson (\(\theta_{1}\)) and O. Wesley Johnson (\(\theta_{2}\)) are the same person (\(\theta_{1}=\theta_{2}\)), then, say, any collection of blood pressure readings on Wesley O. should look pretty much the same as comparable readings on O. Wesley. They would be two samples from the same distribution. Identifiability is the following: if all the samples I have taken or ever could take on Wesley O. look pretty much the same as samples on O. Wesley, then Wesley O. would have to be the same person as O. Wesley. (The reader might consider whether personhood is actually an identifiable parameter for blood pressure.)

For the multivariate normal distributions of Section 1.2, being well defined is the requirement that if \(Y_{1}\sim N\) (\(\mu_{1}\), \(V_{1}\)), \(Y_{2}\sim N\) (\(\mu_{2}\), \(V_{2}\)), and \(\mu_{1}=\mu_{2}\) and \(V_{1}=V_{2}\), then \(Y_{1}\sim Y_{2}\). Theorem 1.2.2 establishes that the mean and covariance of a multivariate normal determine the distribution. Being identifiable is that if \(Y_{1}\sim N\) (\(\mu_{1}\), \(V_{1}\)), \(Y_{2}\sim N\) (\(\mu_{2}\), \(V_{2}\)), and \(Y_{1}\sim Y_{2}\), then \(\mu_{1}=\mu_{2}\) and \(V_{1}=V_{2}\). Obviously, two random vectors with the same distribution have to have the same mean vector and covariance matrix. But life gets more complicated.

The more interesting problem for multivariate normality is a model

\[Y\sim N\left[F(\beta),\,V(\phi)\right]\]

where \(F\) and \(V\) are known functions of parameter vectors \(\beta\) and \(\phi\). To show that \(\beta\) and \(\phi\) are identifiable we need to consider

\[Y_{1}\sim N\left[F(\beta_{1}),\,V(\phi_{1})\right],\qquad Y_{2}\sim N\left[F( \beta_{2}),\,V(\phi_{2})\right]\]

and show that if \(Y_{1}\sim Y_{2}\) then \(\beta_{1}=\beta_{2}\) and \(\phi_{1}=\phi_{2}\). From our earlier discussion, if \(Y_{1}\sim Y_{2}\) then \(F(\beta_{1})=F(\beta_{2})\) and \(V(\phi_{1})=V(\phi_{2})\). We need to check that \(F(\beta_{1})=F(\beta_{2})\) implies \(\beta_{1}=\beta_{2}\) and that \(V(\phi_{1})=V(\phi_{2})\) implies \(\phi_{1}=\phi_{2}\).

Section 2.1 gives an extensive discussion of when the mean parameterization is identifiable, i.e., when \(F(\beta_{1})=F(\beta_{2})\) implies \(\beta_{1}=\beta_{2}\). There we defined identifiable functions of \(\beta\) as those that are functions of \(F(\beta)\).

In this book, we mostly consider simple models for the covariance parameterization \(V(\phi)\); models that are clearly identifiable because they involve at most one scalar parameter. We consider \(V(\phi)\equiv\sigma^{2}I\), and \(V(\phi)\equiv\sigma^{2}V\) where \(V\) is a known nonnegative definite matrix, and, again with known \(V\), \(V(\phi)\equiv V\), which involves no parameterization. For example, if \(V(\phi_{1})\equiv\sigma_{1}^{2}I=V(\phi_{2})\equiv\sigma_{2}^{2}I\), we must have \(\phi_{1}\equiv\sigma_{1}^{2}=\phi_{2}\equiv\sigma_{2}^{2}\). As long as \(V\) is not the zero matrix, the covariance parameterizations in this book are identifiable.

_ALM-III_ examines many commonly used models for \(V(\phi)\) using similar notation to that used here. In particular, linear covariance parameterizations of the form \(\sum_{r=0}^{s}\phi_{r}V_{r}\) for nonnegative \(\phi_{r}\)s and nonnegative definite known \(V_{r}\)s are identifiable if and only if the \(V_{r}\)s are linearly independent, i.e., the Vec(\(V_{r}\))s are linearly independent. The covariance matrices of Chapter 11 fall into this category.

## Exercise

**Exercise D.1**: Let \(x\) and \(y\) be independent. Show that

* \(\operatorname{E}[g(x)|y]=\operatorname{E}[g(x)]\);
* \(\operatorname{E}[g(x)h(y)]=\operatorname{E}[g(x)]\operatorname{E}[h(y)]\).

## Appendix E Inference for One Parameter

AbstractSince the third edition of this book, I have thought hard about the philosophy of testing as a basis for non-Bayesian statistical inference, cf. Christensen (2005, 2008). This appendix has been modified accordingly. The approach taken is one I call Fisherian, as opposed to the Neyman-Pearson approach. The theory presented here has no formal role for alternative hypotheses. A more extensive discussion of these ideas appears in Chapter 3 of Christensen (2015).

A significance testing problem is essentially a form of proof by contradiction. We have a _null model_ for the data and we determine whether the observed data seem to contradict that null model or whether they are consistent with it. If the data contradict the null model, something must be wrong with the null model. Having data consistent with the null model certainly does not suggest that the null model is correct but may suggest that the model is tentatively adequate. The catch is that we rarely get an absolute contradiction to the null model, so we use probability to determine the extent to which the data seem inconsistent with the null model.

In the current discussion, _it is convenient to break the null model into two parts: a general model for the data and a particular statement about a single parameter of interest, called the null hypothesis (H0)._

Many statistical tests and confidence intervals for a single parameter are applications of the same theory. (Tests and confidence intervals for variances are an exception.) To use this theory we need to know four things: [1] The unobservable _parameter_ of interest (_Par_). [2] The _estimate_ of the parameter (_Est_). [3] The _standard error_ of the estimate (SE(_Est_)), wherein SE(_Est_) is typically an estimate of the standard deviation of _Est_, but if we happened to know the actual standard deviation, we would be happy to use it. And [4] an appropriate _reference distribution_. Specifically, we need the distribution of

\[\frac{Est-Par}{\text{SE}(Est)}.\]

If the SE(_Est_) is estimated, the reference distribution is usually the \(t\) distribution withsome known number of degrees of freedom \(df\), say, \(t(df)\). If the SE(\(Est\)) is known, then the distribution is usually the standard normal distribution, i.e., a \(t(\infty)\). In some problems (e.g., problems involving the binomial distribution) large sample results are used to get an approximate distribution and then the technique proceeds as if the approximate distribution were correct. When appealing to large sample results, the known distribution of part [4] is the standard normal (although I suspect that a \(t(df)\) distribution with a reasonable, finite number of degrees of freedom would give more realistic results).

_These four required items are derived from the model for the data_ (although sometimes the standard error incorporates the null hypothesis). For convenience, we may refer to these four items as "the model."

The \(1-\alpha\) percentile of a distribution is the point that cuts off the top \(\alpha\) of the distribution. For a \(t\) distribution, denote this \(t(1-\alpha,df)\) as seen in Figure 1. Formally, we can write

\[\Pr\left[\frac{Est-Par}{\text{SE}(Est)}\geq t(1-\alpha,df)\right]=\alpha.\]

By symmetry about zero, we also have

\[\Pr\left[\frac{Est-Par}{\text{SE}(Est)}\leq-t(1-\alpha,df)\right]=\alpha.\]

To keep the discussion as simple as possible, numerical examples have been restricted to one-sample normal theory. However, the results also apply to inferenceson each individual mean and the difference between the means in two-sample problems, contrasts in analysis of variance, coefficients in regression, and, in general, to one-dimension estimable parametric functions in arbitrary linear models.

### Testing

We want to test the null hypothesis

\[H_{0}:\mathit{Par}=m,\]

where \(m\) is some known number. In _significance (Fisherian) testing_, we cannot do that. _What we can do_ is test the null model, which is the combination of the model and the null hypothesis. The test is based on the assumption that both the model and \(H_{0}\) are true. As mentioned earlier, it is rare that data contradict the null model absolutely, so we check to see if the data seem inconsistent with the null model.

What kind of data are inconsistent with the null model? Consider the _test statistic_

\[\frac{\mathit{Est}-m}{\mathrm{SE}(\mathit{Est})}.\]

With \(m\) known, the test statistic is an observable random variable. If the null model is true, the test statistic has a known \(t(df)\) distribution as illustrated in Figure 1. The \(t(df)\) distribution is likely to give values near 0 and is increasingly less likely to give values far from 0. Therefore, weird data, i.e., those that are most inconsistent with the null model, are large positive and large negative values of \([\mathit{Est}-m]/\mathrm{SE}(\mathit{Est})\). The density (shape) of the \(t(df)\) distribution allows us to order the possible values of the test statistic in terms of how weird they are relative to the null model.

To decide on a formal test, we need to decide which values of the test statistic will cause us to reject the null model and which will not. In other words, "How weird must data be before we question the null model?" We solve this problem by picking a small probability \(\alpha\) that determines a _rejection region_, sometimes called a _critical region_. The rejection region consists of the weirdest test statistic values under the null model, but is restricted to have a probability of only \(\alpha\) under the null model. Since a \(t(df)\) distribution is symmetric about 0 and the density decreases as we go away from 0, the \(\alpha\) critical region consists of points less than \(-t(1-\alpha/2,df)\) and points larger than \(t(1-\alpha/2,df)\). In other words, the \(\alpha\) level test for the model with \(H_{0}:\mathit{Par}=m\) is to reject the null model if

\[\frac{\mathit{Est}-m}{\mathrm{SE}(\mathit{Est})}\geq t\Big{(}1-\frac{\alpha}{2 },df\Big{)}\]

or if

\[\frac{\mathit{Est}-m}{\mathrm{SE}(\mathit{Est})}\leq-t\Big{(}1-\frac{\alpha}{ 2},df\Big{)}.\]This is equivalent to rejecting the null model if

\[\frac{|Est-m|}{\text{SE}(Est)}\geq t\left(1-\frac{\alpha}{2},df\right).\]

What causes us to reject the null model? Either having a true model that is so different from the null that the data look "weird," or having the null model true and getting unlucky with the data.

Observing weird data, i.e., data that are inconsistent with the null model, gives us cause to question the validity of the null model. Specifying a small \(\alpha\) level merely ensures that everything in the rejection region really constitutes weird data. More properly, specifying a small \(\alpha\) level is our means of determining what constitutes weird data. Although \(\alpha\) can be viewed as a probability, it is better viewed as a measure of how weird the data must be relative to the null model before we will reject. We want \(\alpha\) small so that we only reject the null model for data that are truly weird, but we do not want \(\alpha\) so small that we fail to reject the null model even when very strange data occur.

Rejecting the null model means that _either_ the null hypothesis _or_ the model is deemed incorrect. Only if we are confident that the model is correct can we conclude that the null hypothesis is wrong. If we want to make conclusions about the null hypothesis, it is important to do everything possible to assure ourselves that the model is reasonable.

If we do not reject the null model, we merely have data that are consistent with the null model. That in no way implies that the null model is true. Many other models will also be consistent with the data. Typically, \(Par=m+0.00001\) fits the data about as well as the null model. Not rejecting the test does not imply that the null model is true any more than rejecting the null model implies that the underlying model is true.

Example 1: Suppose that 16 independent observations are taken from a normal population. Test \(H_{0}:\mu=20\) with \(\alpha\) level 0.01. The observed values of \(\bar{y}.\) and \(s^{2}\) were 19.78 and 0.25, respectively.

\([1]\)\(Par=\mu\),

\([2]\)\(Est=\bar{y}.\),

\([3]\)\(\text{SE}(Est)=\sqrt{s^{2}/16}\). In this case, the SE(\(Est\)) is estimated.

\([4]\)\([Est-Par]/\text{SE}(Est)=[\bar{y}.-\mu]/\sqrt{s^{2}/16}\) has a \(t\,(15)\) distribution.

With \(m=20\), the \(\alpha=0.01\) test is to reject the \(H_{0}\) model if

\[|\bar{y}.-20|/[s/4]\geq 2.947=t\,(0.995,15).\]

Having \(\bar{y}.=19.78\) and \(s^{2}=0.25\), we reject if

\[\frac{|19.78-20|}{\sqrt{.25/16}}\geq 2.947.\]Since \(|19.78-20|/\sqrt{.25/16}=|-1.76|\) is less than 2.947, we do not reject the null model at the \(\alpha=0.01\) level.

_Nobody actually does this!_ Or at least, nobody should do it. Although this procedure provides a philosophical basis for our statistical inferences, there are two other procedures, both based on this, that give uniformly more information. This procedure requires us to specify the model, the null hypothesis parameter value \(m\), and the \(\alpha\) level. For a fixed model and a fixed null parameter \(m\), \(P\) values are more informative because they allow us to report test results for all \(\alpha\) levels. Alternatively, for a fixed model and a fixed \(\alpha\) level, confidence intervals report the values of all parameters that are consistent with the model and the data. (Parameter values that are inconsistent with the model and the data are those that would be rejected, assuming the model is true.) We now discuss these other procedures.

### \(P\) Values

_The \(P\) value of a test is the probability under the null model of seeing data as weird or weirder than we actually saw_. Weirdness is determined by the distribution of the test statistic. If the observed value of the test statistic from Section 1 is \(t_{obs}\), then the \(P\) value is the probability of seeing data as far or farther from 0 than \(t_{obs}\). In general, we do not know if \(t_{obs}\) will be positive or negative, but its distance from 0 is \(|t_{obs}|\). The \(P\) value is the probability that a \(t(df)\) distribution is less than or equal to \(-|t_{obs}|\) or greater than or equal to \(|t_{obs}|\).

In Example E.1, the value of the test statistic is \(-1.76\). Since \(t(0.95,\,15)=1.75\), the \(P\) value of the test is approximately (just smaller than) \(0.10\). An \(\alpha=0.10\) test would use the \(t(0.95,\,15)\) value.

It is not difficult to see that the \(P\) value is the \(\alpha\) level at which the test would just barely be rejected. So if \(P\leq\alpha\), the null model is rejected, and if \(P>\alpha\), the data are deemed consistent with the null model. Knowing the \(P\) value lets us do all \(\alpha\) level tests of the null model. In fact, historically and philosophically, \(P\) values come before \(\alpha\) level tests. Rather than noticing that the \(\alpha\) level test has this relationship with \(P\) values, it is more general to define the \(\alpha\) level test as rejecting precisely when \(P\leq\alpha\). We can then observe that, for our setup, the \(\alpha\) level test has the form given in Section 1.

While an \(\alpha\) level constitutes a particular choice about how weird the data must be before we decide to reject the null model, the \(P\) value measures the evidence against the null hypothesis. The smaller the \(P\) value, the more evidence against the null model.

### Confidence Intervals

A \((1-\alpha)100\%\)_confidence interval (CI)_ for \(Par\) is defined to be the set of all parameter values \(m\) that would not be rejected by an \(\alpha\) level test. In Section 1 we gave the rule for when an \(\alpha\) level test of \(H_{0}:Par=m\) rejects. Conversely, the null model will not be rejected if

\[-t\Big{(}1-\frac{\alpha}{2},df\Big{)}<\frac{Est-m}{\mathrm{SE}(Est)}<t\Big{(}1- \frac{\alpha}{2},df\Big{)}\,. \tag{1}\]

Some algebra, given later, establishes that we do not reject the null model if and only if

\[Est-t\Big{(}1-\frac{\alpha}{2},df\Big{)}\,\mathrm{SE}(Est)<m<Est+t\Big{(}1- \frac{\alpha}{2},df\Big{)}\,\mathrm{SE}(Est). \tag{2}\]

_This interval consists of all the parameter values \(m\) that are consistent with the data and the model as determined by an \(\alpha\) level test_. The endpoints of the CI can be written

\[Est\pm t\Big{(}1-\frac{\alpha}{2},df\Big{)}\,\mathrm{SE}(Est).\]

On occasion (as with binomial data), when doing an \(\alpha\) level test or a \(P\) value, we may let the standard error depend on the null hypothesis. To obtain a confidence interval using this approach, we need a standard error that does not depend on \(m\).

_Example E.2_ We have 10 independent observations from a normal population with variance 6. \(\bar{y}\). is observed to be 17. We find a 95% CI for \(\mu\), the mean of the population.

[1] \(Par=\mu\),

[3] \(Est=\bar{y}\).,

[3] \(\mathrm{SE}(Est)=\sqrt{6/10}\). In this case, \(\mathrm{SE}(Est)\) is known and not estimated.

[4] \([Est-Par]/\mathrm{SE}(Est)=[\bar{y}.-\mu]\big{/}\sqrt{6/10}\sim N(0,\,1)=t(\infty)\).

The confidence coefficient is 95% = \((1-\alpha)100\%\), so \(1-\alpha=0.95\) and \(\alpha=0.05\). The percentage point from the normal distribution that we require is \(t\Big{(}1-\frac{\alpha}{2},\infty\Big{)}\) = \(t\,(0.975,\infty)\) = \(1.96\). The limits of the 95% CI are, in general,

\[\bar{y}.\pm 1.96\sqrt{6/10}\]

or, since \(\bar{y}.=17\),

\[17\pm 1.96\sqrt{6/10}.\]

The \(\mu\) values in the interval (15.48, 18.52) are consistent with the data and the normal random sampling model as determined by an \(\alpha=0.05\) test.

To see that statements (1) and (2) are algebraically equivalent, the argument runs as follows:

\[-t\Big{(}1-\frac{\alpha}{2},df\Big{)}<\frac{Est-m}{\mathrm{SE}(Est)}<t\Big{(}1- \frac{\alpha}{2},df\Big{)}\]

if and only if \(-t\big{(}1-\frac{\alpha}{2},df\big{)}\,\mathrm{SE}(Est)<Est-m<t\big{(}1-\frac{ \alpha}{2},df\big{)}\,\mathrm{SE}(Est)\); if and only if \(t\big{(}1-\frac{\alpha}{2},df\big{)}\,\mathrm{SE}(Est)>-Est+m>-t\big{(}1- \frac{\alpha}{2},df\big{)}\,\mathrm{SE}(Est)\); if and only if \(Est+t\big{(}1-\frac{\alpha}{2},df\big{)}\,\mathrm{SE}(Est)>m>Est-t\big{(}1- \frac{\alpha}{2},df\big{)}\,\mathrm{SE}(Est)\); if and only if \(Est-t\big{(}1-\frac{\alpha}{2},df\big{)}\,\mathrm{SE}(Est)<m<Est+t\big{(}1- \frac{\alpha}{2},df\big{)}\,\mathrm{SE}(Est)\).

### Final Comments on Significance Testing

The most arbitrary element in Fisherian testing is the choice of a test statistic. Although alternative hypotheses do not play a formal role in significance testing, interesting possible alternative hypotheses do inform the choice of test statistic.

For example, in linear models we often test a full model \(Y=X\beta+e\) against a reduced model \(Y=X_{0}\gamma+e\), with \(e\sim N\,(0,\sigma^{2}I)\) and \(C(X_{0})\subset C(X)\). Although we choose a test statistic based on comparing these models, the significance test is only a test of whether the data are consistent with the reduced model (and is a two-sided \(F\) test when \(r(X)-r(X_{0})\geq 3\)). Rejecting the \(F\) test does not suggest that the full model is correct, it only suggests that the reduced model is wrong. Nonetheless, it is of interest to see how the test behaves if the full model is correct. But models other than the full model can also cause the test to reject, see Appendix F, especially Section F.2. For example, it is of interest to examine the _power_ of a test. The power of an \(\alpha\) level test at some alternative model is the probability of rejecting the null model when the alternative model is true. But in significance testing, there is no thought of accepting any alternative model. Any number of things can cause the rejection of the null model. Similar comments hold for testing generalized linear models.

When testing a null model based on a single parameter hypothesis \(H_{0}:\,Par=m\), interesting possible alternatives include \(Par\neq m\). Our test statistic is designed to be sensitive to these alternatives, but problems with the null model other than \(Par\neq m\) can cause us to reject the null model.

In general, a test statistic can be any function of the data for which the distribution under the null model is known (or can be approximated). But finding a usable test statistic can be difficult. Having to choose between alternative test statistics for the same null model is something of a luxury. For example, to test the null model with equal means in a balanced one-way ANOVA, we can use either the \(F\) test of Chapter 4 or the Studentized range test of Section 5.5.

## Appendix F Significantly Insignificant Tests

**Abstract** Computer programs for fitting linear models typical focus on the significance of large \(F\) statistics. This appendix discusses why one should always be concerned about observing \(F\) statistics very near 0 (when the numerator degrees of freedom are 3 or more).

Philosophically, the test of a null model occurs almost in a vacuum. Either the data contradict the null model or they are consistent with it. The discussion of model testing in Section 3.2 largely assumes that the full model is true. While it is interesting to explore the behavior of the \(F\) test statistic when the full model is true, and indeed it is reasonable and appropriate to choose a test statistic that will work well when the full model is true, the act of rejecting the null model in no way implies that the full model is true. It is perfectly reasonable that the null (reduced) model can be rejected when the full model is false.

Throughout this book we have examined standard approaches to testing in which \(F\) tests are rejected only for large values. The rationale for this is based on the full model being true. We now examine the significance of small \(F\) statistics. Small \(F\) statistics can be caused by an unsuspected lack of fit or, when the mean structure of the reduced model is correct, they can be caused by not accounting for negatively correlated data or not accounting for heteroscedasticity. We also demonstrate that large \(F\) statistics can be generated by not accounting for positively correlated data or heteroscedasticity, even when the mean structure of the reduced model is correct.

Christensen (1995, 2005, 2008) argues that (non-Bayesian) testing should be viewed as an exercise in examining whether or not the data are consistent with a particular (predictive) model. While possible alternative hypotheses may drive the choice of a test statistic, any unusual values of the test statistic should be considered important. By this standard, perhaps the only general way to decide which values of the test statistic are unusual is to identify as unusual those values that have small probabilities or small densities under the model being tested.

The \(F\) test statistic is driven by the idea of testing the reduced model against the full model. However, given the test statistic, any unusual values of that statisticshould be recognized as indicating data that are inconsistent with the model being tested. If the full model is true, values of \(F\) much larger than 1 are inconsistent with the reduced model. Values of \(F\) much larger than 1 are consistent with the full model but, as we shall see, they are consistent with other models as well. Similarly, (when the numerator degrees of freedom are 3 or more) values of \(F\) much smaller than 1 are also inconsistent with the reduced model and we will examine models that can generate small \(F\) statistics.

I have been hesitant to discuss what I think of as a Fisherian \(F\) test, since nobody actually performs them. (That includes me, because it is so much easier to use the reported \(P\) values provided by standard computer programs.) Although the test statistic comes from considering both the reduced (null) model and the full model, once the test statistic is chosen, the full model no longer plays a role. From Theorem 3.2.1(ii), if the reduced model is true,

\[F\equiv\frac{Y^{\prime}(M-M_{0})Y/r(M-M_{0})}{Y^{\prime}(I-M)Y/r(I-M)}\sim F(r( M-M_{0}),r(I-M),0)\;.\]

We use the density to define "weird" values of the \(F\) distribution. The smaller the density, the weirder the observation. Write \(r_{1}\equiv r(M-M_{0})\) and \(r_{2}\equiv r(I-M)\), denote the density \(g(f|r_{1},r_{2})\), and let \(F_{obs}\) denote the observed value of the \(F\) statistic. Since the \(P\) value of a test is the probability under the null model of seeing data as weird or weirder than we actually saw, and weirdness is defined by the density, the \(P\) value of the test is

\[P=\Pr[g(F|r_{1},r_{2})\leq g(F_{obs}|r_{1},r_{2})],\]

wherein \(F_{obs}\) is treated as fixed and known. This is computed under the only distribution we have, the \(F(r_{1},r_{2})\) distribution. An \(\alpha\) level test is defined as rejecting the null model precisely when \(P\leq\alpha\).

If \(r_{1}>2\), the \(F(r_{1},r_{2})\) density has the familiar shape that starts at 0, rises to a maximum in the vicinity of 1, and drops back down to zero for large values. Unless \(F_{obs}\) happens to be the mode, there are two values \(f_{1}<f_{2}\) that have

\[g(F_{obs}|r_{1},r_{2})=g(f_{1}|r_{1},r_{2})=g(f_{2}|r_{1},r_{2}).\]

(One of \(f_{1}\) and \(f_{2}\) will be \(F_{obs}\).) In this case, the \(P\) value reduces to

\[P=\Pr[F\leq f_{1}]+\Pr[F\geq f_{2}].\]

In other words, the Fisherian \(F\) test is a two-sided \(F\) test, rejecting both for very small and very large values of \(F_{obs}\). For \(r_{1}=1\), 2, the Fisherian test agrees with the usual test because then the \(F(r_{1},r_{2})\) density starts high and decreases as \(f\) gets larger.

I should also admit that there remain open questions about the appropriateness of using densities, rather than actual probabilities, to define the weirdness of observations. In fact, I have long speculated whether _Fisher's "z" distribution_, i.e., \(z\equiv\log(F)/2\) might not provide a more appropriate density for significance testing than the \(F\) density. The remainder of this appendix is closely related to Christensen (2003). See also Hogfeldt (1979).

## Appendix F List of Fit and Small \(F\) Statistics

The standard assumption in testing models is that there is a full model \(Y=X\beta+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}I\) that fits the data. We then test the adequacy of a reduced model \(Y=X_{0}\gamma+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}I\) in which \(C(X_{0})\subset C(X)\), cf. Section 3.2. Based on second moment arguments, the test statistic is a ratio of variance estimates. We construct an unbiased estimate of \(\sigma^{2}\), \(Y^{\prime}(I-M)Y/r(I-M)\), and another statistic \(Y^{\prime}(M-M_{0})Y/r(M-M_{0})\) that has \(\mathrm{E}[Y^{\prime}(M-M_{0})Y/r(M-M_{0})]=\sigma^{2}+\beta^{\prime}X^{\prime }(M-M_{0})X\beta/r(M-M_{0})\). Under the assumed covariance structure, this second statistic is an unbiased estimate of \(\sigma^{2}\) if and only if the reduced model is correct. The test statistic

\[F=\frac{Y^{\prime}(M-M_{0})Y/r(M-M_{0})}{Y^{\prime}(I-M)Y/r(I-M)}\]

is a (biased) estimate of

\[\frac{\sigma^{2}+\beta^{\prime}X^{\prime}(M-M_{0})X\beta/r(M-M_{0})}{\sigma^{ 2}}=1+\frac{\beta^{\prime}X^{\prime}(M-M_{0})X\beta}{\sigma^{2}\,r(M-M_{0})}.\]

Under the null model, \(F\) is an estimate of the number 1. When the full model is true, values of \(F\) much larger than 1 suggest that \(F\) is estimating something larger than 1, which suggests that \(\beta^{\prime}X^{\prime}(M-M_{0})X\beta/\sigma^{2}\,r(M-M_{0})>0\), something that occurs if and only if the reduced model is false. The standard normality assumption leads to an exact central \(F\) distribution for the test statistic under the null model, so we are able to quantify how unusual it is to observe any \(F\) statistic greater than 1. Although the test is based on second moment considerations, under the normality assumption it is also the generalized likelihood ratio test, see Exercise 3.1, and a uniformly most powerful invariant test, see Lehmann (1986, Section 7.1).

In testing lack of fit, the same basic ideas apply except that we start with the (reduced) model \(Y=X\beta+e\). The ideal situation would be to know that if \(Y=X\beta+e\) has the wrong mean structure, then a model of the form

\[Y=X\beta+W\delta+e,\quad C(W)\perp C(X) \tag{1}\]

fits the data where assuming \(C(W)\perp C(X)\) creates no loss of generality. Unfortunately, there is rarely anyone to tell us the true matrix \(W\). Lack-or-fit testing is largely about constructing a full model, say, \(Y=X_{*}\beta_{*}+e\) with \(C(X)\subset C(X_{*})\) based on reasonable assumptions about the nature of any lack of fit. The test for lack of fit is simply the test of \(Y=X\beta+e\) against the constructed model \(Y=X_{*}\beta_{*}+e\). Typically, the constructed full model involves somehow generalizing the structure already observed in \(Y=X\beta+e\). Section 6.7 discusses the rationale for several choices of constructed full models. For example, the traditional lack-or-fit test for simple linear regression begins with the replication model \(y_{ij}=\beta_{0}+\beta_{1}x_{i}+e_{ij}\), \(i=1\),..., \(a\), \(j=1\),..., \(N_{i}\). It then assumes \(\mathrm{E}(y_{ij})=f(x_{i})\) for some function \(f(\cdot)\), in otherwords, it assumes that the several observations associated with \(x_{i}\) have the same expected value. Making no additional assumptions leads to fitting the full model \(y_{ij}=\mu_{i}+e_{ij}\) and the traditional lack-or-fit test. Another way to think of this traditional test views the reduced model relative to the one-way ANOVA as having only the linear contrast important. The traditional lack-or-fit test statistic becomes

\[F=\frac{SSTrts-SS(lin)}{a-2}\bigg{/}MSE, \tag{2}\]

where \(SS(lin)\) is the sum of squares for the linear contrast. If there is no lack of fit in the reduced model, \(F\) should be near 1. If lack of fit exists because the more general mean structure of the one-way ANOVA fits the data better than the simple linear regression model, the \(F\) statistic tends to be larger than 1.

Unfortunately, if the lack of fit exists because of features that are not part of the original model, generalizing the structure observed in \(Y=X\beta+e\) is often inappropriate. Suppose that the simple linear regression model is balanced, i.e., all \(N_{i}=N\), that for each \(i\) the data are taken in time order \(t_{1}<t_{2}<\cdots<t_{N}\), and that the lack of fit is due to the true model being

\[y_{ij}=\beta_{0}+\beta_{1}x_{i}+\delta t_{j}+e_{ij},\;\;\;\delta\neq 0. \tag{3}\]

Thus, depending on the sign of \(\delta\), the observations within each group are subject to an increasing or decreasing trend. Note that in this model, for fixed \(i\), the E(\(y_{ij}\))s are _not_ the same for all \(j\), thus invalidating the assumption of the traditional test. In fact, this causes the traditional lack of fit test to have a _small_\(F\) statistic. One way to see this is to view the problem in terms of a balanced two-way ANOVA. The true model (3) is a special case of the two-way ANOVA model \(y_{ij}=\mu+\alpha_{i}+\eta_{j}+e_{ij}\) in which the only nonzero terms are the linear contrast in the \(\alpha_{i}\)s and the linear contrast in the \(\eta_{j}\)s. Under model (3), the numerator of the statistic (2) gives an unbiased estimate of \(\sigma^{2}\) because _SSTrts_ in (2) is \(SS(\alpha)\) for the two-way model and the only nonzero \(\alpha\) effect is being eliminated from the treatments. However, the mean squared error in the denominator of (2) is a weighted average of the error mean square from the two-way model and the mean square for the \(\eta_{j}\)s in the two-way model. The sum of squares for the significant linear contrast in the \(\eta_{j}\)s from model (3) is included in the error term of the lack-or-fit test (2), thus biasing the error term to estimate something larger than \(\sigma^{2}\). In particular, the denominator has an expected value of \(\sigma^{2}+\delta^{2}a\sum_{j=1}^{N}(t_{j}-\overline{t}.)^{2}/a(N-1)\). Thus, if the appropriate model is (3), the statistic in (2) estimates \(\sigma^{2}/[\sigma^{2}+\delta^{2}a\sum_{j=1}^{N}(t_{j}-\overline{t}.)^{2}/a(N- 1)]\) which is a number that is less than 1. Values of \(F\) much smaller than 1, i.e., very near 0, are consistent with a lack of fit that exists within the groups of the one-way ANOVA. Note that in this balanced case, true models involving interaction terms, e.g., models like

\[y_{ij}=\beta_{0}+\beta_{1}x_{i}+\delta t_{j}+\gamma x_{i}t_{j}+e_{ij},\]also tend to make the \(F\) statistic small if either \(\delta\neq 0\) or \(\gamma\neq 0\). Finally, if there exists lack of fit both between the groups of observations and within the groups, if can be very difficult to identify. For example, if \(\beta_{2}\neq 0\) and either \(\delta\neq 0\) or \(\gamma\neq 0\) in the true model

\[y_{ij}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\delta t_{j}+\gamma x_{i}t_ {j}+e_{ij},\]

there is both a traditional lack of fit between the groups (the significant \(\beta_{2}x_{i}^{2}\) term) and lack of fit within the groups (\(\delta t_{j}+\gamma x_{i}t_{j}\)). In this case, neither the numerator nor the denominator in (2) is an estimate of \(\sigma^{2}\).

More generally, start with a model \(Y=X\beta+e\). This is tested against a larger model \(Y=X_{*}\beta_{*}+e\) with \(C(X)\subset C(X_{*})\), regardless of where the larger model comes from. The \(F\) statistic is

\[F=\frac{Y^{\prime}(M_{*}-M)Y/r(M_{*}-M)}{Y^{\prime}(I-M_{*})Y/r(I-M_{*})}.\]

We assume that the true model is (1). The \(F\) statistic estimates 1 if the original model \(Y=X\beta+e\) is correct. It estimates something greater than 1 if the larger model \(Y=X_{*}\beta_{*}+e\) is correct, i.e., if \(W\delta\in C(X)_{C(X_{*})}^{\perp}\). \(F\) estimates something less than 1 if \(W\delta\in C(X_{*})^{\perp}\), i.e., if \(W\delta\) is actually in the error space of the larger model, because then the numerator estimates \(\sigma^{2}\) but the denominator estimates

\[\sigma^{2}+\delta^{\prime}W^{\prime}(I-M_{*})W\delta/r(I-M_{*})=\sigma^{2}+ \delta^{\prime}W^{\prime}W\delta/r(I-M_{*}).\]

If \(W\delta\) is in neither of \(C(X)_{C(X_{*})}^{\perp}\) nor \(C(X_{*})^{\perp}\), it is not clear how the test will behave because neither the numerator nor the denominator estimates \(\sigma^{2}\). Christensen (1989, 1991) contains related discussion of these concepts.

The main point is that, when testing a full model \(Y=X\beta+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}I\) against a reduced model \(Y=X_{0}\gamma+e\), \(C(X_{0})\subset C(X)\), if the \(F\) statistic is small, it suggests that \(Y=X_{0}\gamma+e\) may suffer from lack of fit in which the lack of fit exists in the error space of \(Y=X\beta+e\). We will see in the next section that other possible explanations for a small \(F\) statistic are the existence of "negative correlation" in the data or heteroscedasticity.

### The Effect of Correlation and Heteroscedasticity

on \(F\) Statistics

The test of a reduced model assumes that the full model \(Y=X\beta+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}I\) holds and tests the adequacy of a reduced model \(Y=X_{0}\gamma+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}I\), \(C(X_{0})\subset C(X)\). Rejecting the reduced model does not imply that the full model is correct. The mean structure of the reduced model may be perfectly valid, but the \(F\) statistic can become large or small because the assumed covariance structure is incorrect.

We begin with a concrete example, one-way ANOVA. Let \(i=1,\ldots,a,\ j=1,\ldots,N,\) and \(n\equiv aN\). Consider a reduced model \(y_{ij}=\mu+e_{ij}\) which in matrix terms we write \(Y=J\mu+e\), and a full model \(y_{ij}=\mu_{i}+e_{ij}\), which we write \(Y=Z\gamma+e\). In matrix terms the usual one-way ANOVA \(F\) statistic is

\[F=\frac{Y^{\prime}[M_{Z}-(1/n)J_{n}^{n}]Y/(a-1)}{Y^{\prime}(I-M_{Z})Y/a(N-1)}. \tag{1}\]

We now assume that the true model is \(Y=J\mu+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}V\) and examine the behavior of the \(F\) statistic (1).

For a homoscedastic balanced one-way ANOVA we want to characterize the concepts of overall positive correlation, positive correlation within groups, and positive correlation for evaluating differences between groups. Consider first a simple example with \(a=2\), \(N=2\). The first two observations are a group and the last two are a group. Consider a covariance structure

\[V_{1}=\left[\begin{array}{cccc}1&0.9&0.1&0.09\\ 0.9&1&0.09&0.1\\ 0.1&0.09&1&0.9\\ 0.09&0.1&0.9&1\end{array}\right].\]

There is an overall positive correlation, high positive correlation between the two observations in each group, and weak positive correlation between the groups. A second example,

\[V_{2}=\left[\begin{array}{cccc}1&0.1&0.9&0.09\\ 0.1&1&0.09&0.9\\ 0.9&0.09&1&0.1\\ 0.09&0.9&0.1&1\end{array}\right],\]

has an overall positive correlation but weak positive correlation between the two observations in each group, with high positive correlation between some observations in different groups.

We now make a series of definitions for homoscedastic balanced one-way ANOVA based on the projection operators in (1) and \(V\). Overall positive correlation is characterized by \(\mathrm{Var}(\bar{y}_{..})>\sigma^{2}/n\), which in matrix terms is written

\[n\frac{\mathrm{Var}(\bar{y}_{..})}{\sigma^{2}}=\mathrm{tr}[(1/n)JJ^{\prime}V] >\frac{1}{n}\mathrm{tr}(V)\mathrm{tr}[(1/n)JJ^{\prime}]=\frac{1}{n}\mathrm{tr} (V). \tag{2}\]

Overall negative correlation is characterized by the reverse inequality. For homoscedastic models the term \(\mathrm{tr}(V)/n\) is 1. For heteroscedastic models the term on the right is the average variance of the observations divided by \(\sigma^{2}\).

Positive correlation within groups is characterized by \(\sum_{i=1}^{a}\text{Var}(\bar{y}_{i}.)/a>\sigma^{2}/N\), which in matrix terms is written

\[\sum_{i=1}^{a}N\frac{\text{Var}(\bar{y}_{i}.)}{\sigma^{2}}=\text{tr}[M_{Z}V]> \frac{1}{n}\text{tr}(V)\text{tr}[M_{Z}]=\frac{a}{n}\text{tr}(V). \tag{3}\]

Negative correlation within groups is characterized by the reverse inequality.

Positive correlation for evaluating differences between groups is characterized by

\[\frac{\sum_{i=1}^{a}\text{Var}(\bar{y}_{i}.-\bar{y}.)}{a}>\frac{a-1}{a}\frac{ \sigma^{2}}{N}.\]

Note that equality obtains if \(V=I\). In matrix terms, this is written

\[\frac{N}{\sigma^{2}}\sum_{i=1}^{a}\text{Var}(\bar{y}_{i}.-\bar{y}.)=\text{tr}([M_{Z}-(1/n)JJ^{\prime}]V)\\ >\frac{1}{n}\text{tr}(V)\text{tr}[M_{Z}-(1/n)JJ^{\prime}]=\frac{ a-1}{n}\text{tr}(V) \tag{4}\]

and negative correlation for evaluating differences between groups is characterized by the reverse inequality. If all the observations in different groups are uncorrelated, there will be positive correlation for evaluating differences between groups if and only if there is positive correlation within groups. This follows because having a block diagonal covariance matrix \(\sigma^{2}V\) implies that \(\text{tr}(M_{Z}V)=\text{tr}[(1/N)Z^{\prime}VZ]=a\text{tr}[(1/n)JJ^{\prime}VJ]= a\text{tr}[(1/n)JJ^{\prime}V]\).

For our example \(V_{1}\),

\[2.09=(1/4)[4(2.09)]=\text{tr}[(1/n)J_{n}^{n}V_{1}]>\frac{1}{n}\text{tr}(V_{1}) =4/4=1,\]

so there is an overall positive correlation,

\[3.8=2(1/2)[3.8]=\text{tr}[M_{Z}V_{1}]>\frac{a}{n}\text{tr}(V_{1})=(2/4)4=2,\]

so there is positive correlation within groups, and

\[1.71=3.8-2.09=\text{tr}([M_{Z}-(1/n)J_{n}^{n}]V_{1})>\frac{a-1}{n}\text{tr}(V _{1})=(1/4)4=1,\]

so there is positive correlation for evaluating differences between groups.

For the second example \(V_{2}\),

\[2.09=(1/4)[4(2.09)]=\text{tr}[(1/n)J_{n}^{n}V_{2}]>\frac{1}{n}\text{tr}(V_{2}) =4/4=1,\]so there is an overall positive correlation,

\[2.2=2(1/2)[2.2]=\text{tr}[M_{Z}V_{2}]>\frac{a}{n}\text{tr}(V_{2})=(2/4)4=2,\]

so there is positive correlation within groups, but

\[0.11=2.2-2.09=\text{tr}([M_{Z}-(1/n)J_{n}^{n}]V_{2})<\frac{a-1}{n}\text{tr}(V_{ 2})=(1/4)4=1,\]

so positive correlation for evaluating differences between groups does not exist.

The existence of positive correlation within groups and positive correlation for evaluating differences between groups causes the one-way ANOVA \(F\) statistic in (1) to get large even when there are no differences in the group means. Assuming that the correct model is \(Y=J\mu+e\), \(\text{E}(e)=0\), \(\text{Cov}(e)=\sigma^{2}V\), by Theorem 1.3.1, the numerator of the \(F\) statistic estimates

\[\text{E}\{Y^{\prime}[M_{Z}-(1/n)J_{n}^{n}]Y/(a-1)\}=\text{tr}[[M_ {Z}-(1/n)J_{n}^{n}]V)/(a-1)\\ >\frac{a-1}{n}\text{tr}(V)/(a-1)=\text{tr}(V)/n\]

and the denominator of the \(F\) statistic estimates

\[\text{E}\{Y^{\prime}(I-M_{Z})Y/a(N-1)\} =\text{tr}\{[I-M_{Z}]V\}/a(N-1)\] \[=(\text{tr}\{V\}-\text{tr}\{[M_{Z}]V\})/a(N-1)\] \[<\left(\text{tr}\{V\}-\frac{a}{n}\text{tr}(V)\right)/a(N-1)\] \[=\frac{n-a}{n}\text{tr}(V)/a(N-1)=\text{tr}(V)/n.\]

In (1), \(F\) is an estimate of

\[\frac{\text{E}\{Y^{\prime}[M_{Z}-(1/n)J_{n}^{n}]Y/(a-1)\}}{\text{E}\{Y^{ \prime}(I-M_{Z})Y/a(N-1)\}}=\frac{\text{tr}\{[M_{Z}-(1/n)J_{n}^{n}]V\}/(a-1)} {\text{tr}\{[I-M_{Z}]V\}/a(N-1)}\\ >\frac{\text{tr}(V)/n}{\text{tr}(V)/n}=1,\]

so having both positive correlation within groups and positive correlation for evaluating differences between groups tends to make \(F\) statistics large. Exactly analogous computations show that having both negative correlation within groups and negative correlation for evaluating differences between groups tends to make \(F\) statistics less than 1.

Another example elucidates some additional points. Suppose the observations have the AR(1) correlation structure discussed in Subsection 12.3.1:\[V_{3}=\begin{bmatrix}1&\rho&\rho^{2}&\rho^{3}\\ \rho&1&\rho&\rho^{2}\\ \rho^{2}&\rho&1&\rho\\ \rho^{3}&\rho^{2}&\rho&1\end{bmatrix}.\]

Using the same grouping structure as before, when \(0<\rho<1\), we have overall positive correlation because

\[1+\frac{\rho}{2}(3+2\rho+\rho^{2})=\operatorname{tr}[(1/n)JJ^{\prime}V_{3}]>1,\]

and we have positive correlation within groups because

\[2(1+\rho)=\operatorname{tr}[M_{Z}V_{3}]>2.\]

If \(-1<\rho<0\), the inequalities are reversed. Similarly, for \(-1<\rho<0\) we have negative correlation for evaluating differences between groups because

\[1+\frac{\rho}{2}(1-2\rho-\rho^{2})^{2}=\operatorname{tr}([M_{Z}-(1/n)JJ^{ \prime}]V_{3})<1.\]

However, we only get positive correlation for evaluating differences between groups when \(0<\rho<\sqrt{2}-1\). Thus, for negative \(\rho\) we tend to get small \(F\) statistics, for \(0<\rho<\sqrt{2}-1\) we tend to get large \(F\) statistics, and for \(\sqrt{2}-1<\rho<1\) the result is not clear.

To illustrate, suppose \(\rho=1\) and the observations all have the same mean, then with probability 1, all the observations are equal and, in particular, \(\bar{y}_{i.}=\bar{y}_{..}\) with probability 1. It follows that

\[0=\frac{\sum_{i=1}^{a}\operatorname{Var}(\bar{y}_{i.}-\bar{y}_{..})}{a}<\frac{ a-1}{a}\frac{\sigma^{2}}{N}\]

and no positive correlation exists for evaluating differences between groups. More generally, for very strong positive correlations, both the numerator and the denominator of the \(F\) statistic estimate numbers close to 0 and both are smaller than they would be under \(V=I\). On the other hand, it is not difficult to see that, for \(\rho=-1\), the \(F\) statistic is 0.

In the balanced heteroscedastic one-way ANOVA, \(V\) is diagonal. This generates equality between the left sides and right sides of (2), (3), and (4), so under heteroscedasticity \(F\) still estimates the number 1. We now generalize the ideas of within group correlation and correlation for evaluating differences between groups, and see that heteroscedasticity can affect unbalanced one-way ANOVA.

In general, we test a full model \(Y=X\beta+e\), \(\operatorname{E}(e)=0\), \(\operatorname{Cov}(e)=\sigma^{2}I\) against a reduced model \(Y=X_{0}\gamma+e\), in which \(C(X_{0})\subset C(X)\). We examine the \(F\) statistic when the true model is \(Y=X_{0}\gamma+e\), \(\operatorname{E}(e)=0\), \(\operatorname{Cov}(e)=\sigma^{2}V\). Using arguments similar to those for balanced one-way ANOVA, having\[\text{tr}[M\,V]>\frac{1}{n}\text{tr}(V)\text{tr}[M]=\frac{r(X)}{n}\text{tr}(V)\]

and

\[\text{tr}([M-M_{0}]V)>\frac{1}{n}\text{tr}(V)\text{tr}[M-M_{0}]=\frac{r(X)-r(X_{ 0})}{n}\text{tr}(V)\]

causes large \(F\) statistics even when the mean structure of the reduced model is true, and reversing the inequalities causes small \(F\) statistics. These are merely sufficient conditions so that the tests intuitively behave certain ways. The actual behavior of the tests under normal distributions can be determined numerically, cf. Christensen and Bedrick (1997).

These covariance conditions can be caused by patterns of positive and negative correlations as discussed earlier, but they can also be caused by heteroscedasticity. For example, consider the behavior of the unbalanced one-way ANOVA \(F\) test when the observations are uncorrelated but heteroscedastic. For concreteness, assume that \(\text{Var}(y_{ij})=\sigma_{i}^{2}\). Because the observations are uncorrelated, we need only check the condition

\[\text{tr}[M\,V]\equiv\text{tr}[M_{Z}\,V]>\frac{1}{n}\text{tr}(V)\text{tr}[M_{ Z}]=\frac{a}{n}\text{tr}(V),\]

which amounts to

\[\sum_{i=1}^{a}\sigma_{i}^{2}\big{/}a>\sum_{i=1}^{a}\frac{N_{i}}{n}\sigma_{i}^{ 2}.\]

Thus, when the groups' means are equal, \(F\) statistics will get large if many observations are taken in groups with small variances and few observations are taken on groups with large variances. \(F\) statistics will get small if the reverse relationship holds.

The general condition

\[\text{tr}[M\,V]>\frac{1}{n}\text{tr}(V)\text{tr}[M]=\frac{r(X)}{n}\text{tr}(V)\]

is equivalent to

\[\frac{\sum_{i=1}^{n}\text{Var}(x_{i}^{\prime}\hat{\beta})}{r(X)}>\frac{\sum_{i =1}^{n}\text{Var}(y_{i})}{n}.\]

So, under homoscedasticity, positive correlation in the full model amounts to having an average variance for the predicted values (averaging over the rank of the covariance matrix of the predicted values) that is larger than the common variance of the observations. Negative correlation in the full model involves reversing the inequality. Similarly, having positive correlation for distinguishing the full model from the reduced model means

\[\frac{\sum_{i=1}^{n}\text{Var}(x_{i}^{\prime}\hat{\beta}-x_{0i}^{\prime}\hat{ \gamma})}{r(X)-r(X_{0})}=\frac{\text{tr}[(M-M_{0})V]}{r(M-M_{0})}>\frac{\text {tr}(V)}{n}=\frac{\sum_{i=1}^{n}\text{Var}(y_{i})}{n}.\]

## Appendix G Randomization Theory Models

**Abstract** This appendix introduces randomization theory models in which, rather than assuming the existence of an error term with certain properties, the random variability in the data is constructed either by random sampling from a population or by randomly assigning treatments to experimental units.

The division of labor in statistics has traditionally designated randomization theory as an area of nonparametric statistics. Randomization theory is also of special interest in the theory of experimental design because randomization has been used to justify the analysis of designed experiments.

It can be argued that the linear models given in Chapter 8 are merely good approximations to more appropriate models based on randomization theory. One aspect of this argument is that the \(F\) tests based on the theory of normal errors are a good approximation to randomization (permutation) tests. Investigating this is beyond the scope of a linear models book, cf. Hinkelmann and Kempthorne (2005) and Puri and Sen (1971). Another aspect of the approximation argument is that the BLUEs under randomization theory are precisely the least squares estimates. By Theorem 10.4.5, to establish this we need to show that \(C(V\!X)\subset C(X)\) for the model

\[Y=X\beta+e,\ \ \ \mathrm{E}(e)=0,\ \ \ \mathrm{Cov}(e)=V,\]

where \(V\) is the covariance matrix under randomization theory. This argument will be examined here for two experimental design models: the model for a completely randomized design and the model for a randomized complete block design. First, we introduce the subject with a discussion of simple random sampling.

### Simple Random Sampling

Randomization theory for a simple random sample assumes that observations \(y_{i}\) are picked at random (without replacement) from a larger finite population. Suppose the elements of the population are \(s_{1},s_{2},\ldots,s_{N}\). We can define elementary samplingrandom variables for \(i=1,\ldots,n\) and \(j=1,\ldots,N\),

\[\delta_{j}^{i}=\begin{cases}1,&\text{if }y_{i}=s_{j}\\ 0,&\text{otherwise}.\end{cases}\]

Under simple random sampling without replacement

\[\text{E}[\delta_{j}^{i}]=\Pr[\delta_{j}^{i}=1]=\frac{1}{N}.\]

\[\text{E}[\delta_{j}^{i}\delta_{j^{\prime}}^{i^{\prime}}]=\Pr[\delta_{j}^{i} \delta_{j^{\prime}}^{i^{\prime}}=1]=\begin{cases}1/N,&\text{if }(i,j)=(i^{\prime},j^{\prime})\\ 1/N(N-1),&\text{if }i\neq i^{\prime}\text{ and }j\neq j^{\prime}\\ 0,&\text{otherwise}.\end{cases}\]

If we write \(\mu=\sum_{j=1}^{N}s_{j}/N\) and \(\sigma^{2}=\sum_{j=1}^{N}(s_{j}-\mu)^{2}/N\), then

\[y_{i}=\sum_{j=1}^{N}\delta_{j}^{i}s_{j}=\mu+\sum_{j=1}^{N}\delta_{j}^{i}(s_{j} -\mu).\]

Letting \(e_{i}=\sum_{j=1}^{N}\delta_{j}^{i}(s_{j}-\mu)\) gives the linear model

\[y_{i}=\mu+e_{i}.\]

The population mean \(\mu\) is a fixed unknown constant. The \(e_{i}\)s have the properties

\[\text{E}[e_{i}]=\text{E}\!\left[\sum_{j=1}^{N}\delta_{j}^{i}(s_{j}-\mu)\right] =\sum_{j=1}^{N}\text{E}\!\left[\delta_{j}^{i}\right](s_{j}-\mu)=\sum_{j=1}^{N} (s_{j}-\mu)\big{/}N=0,\]

\[\text{Var}(e_{i})=\text{E}[e_{i}^{2}]=\sum_{j=1}^{N}\sum_{j^{\prime}=1}^{N}(s_ {j}-\mu)(s_{j^{\prime}}-\mu)\text{E}[\delta_{j}^{i}\delta_{j^{\prime}}^{i}]= \sum_{j=1}^{N}(s_{j}-\mu)^{2}\big{/}N=\sigma^{2}.\]

For \(i\neq i^{\prime}\),

\[\text{Cov}(e_{i},e_{i^{\prime}}) =\text{E}[e_{i}e_{i^{\prime}}]=\sum_{j=1}^{N}\sum_{j^{\prime}=1}^ {N}(s_{j}-\mu)(s_{j^{\prime}}-\mu)\text{E}[\delta_{j}^{i}\delta_{j^{\prime}}^ {i^{\prime}}]\] \[=[N(N-1)]^{-1}\sum_{j\neq j^{\prime}}(s_{j}-\mu)(s_{j^{\prime}}-\mu)\] \[=[N(N-1)]^{-1}\left(\left[\sum_{j=1}^{N}(s_{j}-\mu)\right]^{2}- \sum_{j=1}^{N}(s_{j}-\mu)^{2}\right)\] \[=-\sigma^{2}\big{/}(N-1).\]In matrix terms, the linear model can be written

\[Y=J\mu+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}V,\]

where

\[V=\begin{bmatrix}1&-(N-1)^{-1}&-(N-1)^{-1}&\cdots&-(N-1)^{-1}\\ -(N-1)^{-1}&1&-(N-1)^{-1}&\cdots&-(N-1)^{-1}\\ -(N-1)^{-1}&-(N-1)^{-1}&1&\cdots&-(N-1)^{-1}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ -(N-1)^{-1}&-(N-1)^{-1}&-(N-1)^{-1}&\cdots&1\end{bmatrix}.\]

Clearly \(VJ=\left[(N-n)/(N-1)\right]J\), so the BLUE of \(\mu\) is \(\bar{y}\)..

### Completely Randomized Designs

Suppose that there are \(t\) treatments, each to be randomly assigned to \(N\) units out of a collection of \(n=tN\) experimental units. A one-way ANOVA model for this design is

\[y_{ij}=\mu_{i}+e_{ij}, \tag{1}\]

\(i=1\),..., \(t\), \(j=1\),..., \(N\). Suppose further that the \(i\)th treatment has an effect \(\tau_{i}\) and that the experimental units without treatment effects would have readings \(s_{1}\),..., \(s_{n}\). The elementary sampling random variables are

\[\delta_{k}^{ij}=\begin{cases}1,&\text{if replication $j$ of treatment $i$ is assigned to unit $k$}\\ 0,&\text{otherwise.}\end{cases}\]

With this restricted random sampling,

\[\mathrm{E}[\delta_{k}^{ij}]=\mathrm{Pr}[\delta_{k}^{ij}=1]=\frac{1}{n}\]

\[\mathrm{E}[\delta_{k}^{ij}\delta_{k^{\prime}}^{i^{\prime}j^{\prime}}]=\mathrm{ Pr}[\delta_{k}^{ij}\delta_{k^{\prime}}^{i^{\prime}j^{\prime}}=1]=\begin{cases}1/n,&\text{if $(i,j,k)=(i^{\prime},j^{\prime},k^{\prime})$}\\ 1/n(n-1),&\text{if $k\neq k^{\prime}$ and $(i,j)\neq(i^{\prime},j)^{\prime}$}\\ 0,&\text{otherwise.}\end{cases}\]

We can write

\[y_{ij}=\tau_{i}+\sum_{k=1}^{n}\delta_{k}^{ij}s_{k}.\]Taking \(\mu=\sum_{k=1}^{n}s_{k}/n\) and \(\mu_{i}=\mu+\tau_{i}\) gives

\[y_{ij}=\mu_{i}+\sum_{k=1}^{n}\delta_{k}^{ij}(s_{k}-\mu).\]

To obtain the linear model (1), let \(\text{sp}\,e_{ij}=\sum_{k=1}^{n}\delta_{k}^{ij}(s_{k}-\mu)\). Write \(\sigma^{2}=\sum_{k=1}^{n}(s_{k}-\mu)^{2}/n\). Then

\[\text{E}[e_{ij}]=\text{E}\!\left[\sum_{k=1}^{n}\delta_{k}^{ij}(s_{k}-\mu) \right]=\sum_{k=1}^{n}\text{E}\!\left[\delta_{k}^{ij}\right](s_{k}-\mu)=\sum_{ k=1}^{n}(s_{k}-\mu)\big{/}n=0,\]

\[\text{Var}(e_{ij})=\text{E}[e_{ij}^{2}]=\sum_{k=1}^{n}\sum_{k^{\prime}=1}^{n}( s_{k}-\mu)(s_{k^{\prime}}-\mu)\text{E}[\delta_{k}^{ij}\delta_{k^{\prime}}^{ ij}]=\sum_{k=1}^{n}(s_{k}-\mu)^{2}\big{/}n=\sigma^{2}.\]

For \((i,j)\neq(i^{\prime},j^{\prime})\),

\[\text{Cov}(e_{ij},e_{i^{\prime}j^{\prime}}) =\text{E}[e_{ij}e_{i^{\prime}j^{\prime}}]=\sum_{k=1}^{n}\sum_{k^ {\prime}=1}^{n}(s_{k}-\mu)(s_{k^{\prime}}-\mu)\text{E}[\delta_{k}^{ij}\delta_{ k^{\prime}}^{i^{\prime}j^{\prime}}]\] \[=[n(n-1)]^{-1}\sum_{k\neq k^{\prime}}(s_{k}-\mu)(s_{k^{\prime}}-\mu)\] \[=[n(n-1)]^{-1}\left(\left[\sum_{k=1}^{n}(s_{k}-\mu)\right]^{2}- \sum_{k=1}^{n}(s_{k}-\mu)^{2}\right)\] \[=-\sigma^{2}\big{/}(n-1).\]

In matrix terms, writing \(Y=(y_{11},y_{12},\dots,y_{tN})^{\prime}\), we get

\[Y=X\begin{bmatrix}\mu_{1}\\ \vdots\\ \mu_{t}\end{bmatrix}+e,\ \ \text{E}(e)=0,\ \ \ \text{Cov}(e)=\sigma^{2}V,\]

where

\[V =\begin{bmatrix}1&-1/(n-1)&-1/(n-1)&\cdots&-1/(n-1)\\ -1/(n-1)&1&-1/(n-1)&\cdots&-1/(n-1)\\ -1/(n-1)&-1/(n-1)&1&\cdots&-1/(n-1)\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ -1/(n-1)&-1/(n-1)&-1/(n-1)&\cdots&1\end{bmatrix}\] \[=\frac{n}{n-1}I-\frac{1}{n-1}J_{n}^{n}.\]It follows that

\[VX=\frac{n}{n-1}X-\frac{1}{n-1}J_{n}^{n}X.\]

Since \(J\in C(X)\), \(C(VX)\subset C(X)\), and least squares estimates are BLUEs. Standard errors for estimable functions can be found as in Section 11.1 using the fact that this model involves only one cluster.

**Exercise G.1**: Establish whether least squares estimates are BLUEs in a completely randomized design with unequal numbers of observations on the treatments.

### Randomized Complete Block Designs

Suppose there are \(a\) treatments and \(b\) blocks. The experimental units must be grouped into \(b\) blocks, each of \(a\) units. Let the experimental unit effects be \(s_{kj}\), \(k=1\),..., \(a\), \(j=1\),..., \(b\). Treatments are assigned at random to the \(a\) units in each block. The elementary sampling random variables are

\[\delta_{kj}^{i}=\begin{cases}1,&\text{if treatment $i$ is assigned to unit $k$ in block $j$}\\ 0,&\text{otherwise}.\end{cases}\]

\[\text{E}[\delta_{kj}^{i}]=\text{Pr}[\delta_{kj}^{i}=1]=\frac{1}{a}.\]

\[\text{E}[\delta_{kj}^{i}\delta_{k^{\prime}j^{\prime}}^{i^{\prime}}]=\text{Pr}[ \delta_{kj}^{i}\delta_{k^{\prime}j^{\prime}}^{i^{\prime}}=1]=\begin{cases}1/a,& \text{if $(i,j,k)=(i^{\prime},j^{\prime},k^{\prime})$}\\ 1/a^{2},&\text{if $j\neq j^{\prime}$}\\ 1/a(a-1),&\text{if $j=j^{\prime},k\neq k^{\prime},i\neq i^{\prime}$}\\ 0,&\text{otherwise}.\end{cases}\]

If \(\alpha_{i}\) is the additive effect of the \(i\)th treatment and \(\beta_{j}\equiv\bar{s}_{.j}\), then

\[y_{ij}=\alpha_{i}+\beta_{j}+\sum_{k=1}^{a}\delta_{kj}^{i}(s_{kj}-\beta_{j}).\]

Letting \(e_{ij}=\sum_{k=1}^{a}\delta_{kj}^{i}(s_{kj}-\beta_{j})\) gives the linear model

\[y_{ij}=\alpha_{i}+\beta_{j}+e_{ij}. \tag{1}\]

The column space of the design matrix for this model is precisely that of the model considered in Section 8.3. Let \(\sigma_{j}^{2}=\sum_{k=1}^{a}(s_{kj}-\beta_{j})^{2}/a\). Then\[\text{E}[e_{ij}]=\sum_{k=1}^{a}(s_{kj}-\beta_{j})/a=0,\]

\[\text{Var}(e_{ij}) =\sum_{k=1}^{a}\sum_{k^{\prime}=1}^{a}(s_{kj}-\beta_{j})(s_{k^{ \prime}j}-\beta_{j})\text{E}[\delta_{kj}^{i}\delta_{k^{\prime}j}^{i}]\] \[=\sum_{k=1}^{a}(s_{kj}-\beta_{j})^{2}/a=\sigma_{j}^{2}.\]

For \(j\neq j^{\prime}\),

\[\text{Cov}(e_{ij},e_{i^{\prime}j^{\prime}}) =\sum_{k=1}^{a}\sum_{k^{\prime}=1}^{a}(s_{kj}-\beta_{j})(s_{k^{ \prime}j^{\prime}}-\beta_{j^{\prime}})\text{E}[\delta_{kj}^{i}\delta_{k^{ \prime}j^{\prime}}^{i^{\prime}}]\] \[=a^{-2}\sum_{k=1}^{a}(s_{kj}-\beta_{j})\sum_{k^{\prime}=1}^{a}(s_ {k^{\prime}j^{\prime}}-\beta_{j^{\prime}})\] \[=0.\]

For \(j=j^{\prime},i\neq i^{\prime}\),

\[\text{Cov}(e_{ij},e_{i^{\prime}j^{\prime}}) =\sum_{k=1}^{a}\sum_{k^{\prime}=1}^{a}(s_{kj}-\beta_{j})(s_{k^{ \prime}j}-\beta_{j})\text{E}[\delta_{kj}^{i}\delta_{k^{\prime}j}^{i^{\prime}}]\] \[=\sum_{k\neq k^{\prime}}(s_{kj}-\beta_{j})(s_{k^{\prime}j}-\beta_ {j})/a(a-1)\] \[=[a(a-1)]^{-1}\left(\left[\sum_{k=1}^{a}(s_{kj}-\beta_{j})\right]^ {2}-\sum_{k=1}^{a}(s_{kj}-\beta_{j})^{2}\right)\] \[=-\sigma_{j}^{2}/(a-1).\]

Before proceeding, we show that although the terms \(\beta_{j}\) are not known, the differences among these are known constants under randomization theory. For any unit \(k\) in block \(j\), some treatment is assigned, so \(\sum_{i=1}^{a}\delta_{kj}^{i}=1\).

\[\tilde{y}_{.j} =\frac{1}{a}\left[\sum_{i=1}^{a}\left(\alpha_{i}+\beta_{j}+\sum_{ k=1}^{a}\delta_{kj}^{i}(s_{kj}-\beta_{j})\right)\right]\] \[=\frac{1}{a}\left[\sum_{i=1}^{a}\alpha_{i}+a\beta_{j}+\sum_{k=1}^ {a}(s_{kj}-\beta_{j})\sum_{i=1}^{a}\delta_{kj}^{i}\right]\] \[=\tilde{\alpha}.+\beta_{j}+\sum_{k=1}^{a}(s_{kj}-\beta_{j})\] \[=\tilde{\alpha}.+\beta_{j}.\]Therefore, \(\bar{y}_{.j}-\bar{y}_{.j^{\prime}}=\beta_{j}-\beta_{j^{\prime}}=\bar{s}_{.j}-\bar{ s}_{.j^{\prime}}\). Since these differences are fixed and known, there is no basis for a test of \(H_{0}:\beta_{1}=\cdots=\beta_{b}\). In fact, the linear model is not just model (1) but model (1) subject to these estimable constraints on the \(\beta\)s.

To get best linear unbiased estimates we need to assume that \(\sigma_{1}^{2}=\sigma_{2}^{2}=\cdots=\sigma_{b}^{2}=\sigma^{2}\). We can now write the linear model in matrix form and establish that least squares estimates of treatment means and contrasts in the \(\alpha_{i}\)s are BLUEs. In the discussion that follows, we use notation from Section 7.1. Model (1) can be rewritten

\[Y=X\eta+e,\ \ \ \mbox{E}(e)=0,\ \ \ \mbox{Cov}(e)=V, \tag{2}\]

where \(\eta=[\mu,\alpha_{1},\ldots,\alpha_{a},\beta_{1},\ldots,\beta_{b}]^{\prime}\). If we let \(X_{2}\) be the columns of \(X\) corresponding to \(\beta_{1}\),..., \(\beta_{b}\), then (cf. Section 11.1)

\[V=\sigma^{2}\left[a/(a-1)\right]\left[I-(1/a)X_{2}X_{2}^{\prime}\right]=\sigma ^{2}\left[a/(a-1)\right]\left[I-M_{\mu}-M_{\beta}\right].\]

If model (2) were the appropriate model, checking that \(C(V\!X)\subset C(X)\) would be trivial based on the fact that \(C(X_{2})\subset C(X)\). However, we must account for the estimable constraints on the model discussed above. In particular, consider

\[M_{\beta}X\eta=[t_{ij}],\]

where

\[t_{ij}=\beta_{j}-\bar{\beta}.=\bar{y}_{.j}-\bar{y}_{.\ldots}=\bar{s}_{.j}-\bar {s}_{.\ldots}\.\]

This is a fixed known quantity. Proceeding as in Section 3.3, the model is subject to the estimable constraint

\[M_{\beta}X\eta=M_{\beta}Y.\]

Normally a constraint has the form \(\Lambda^{\prime}\beta=d\), where \(d\) is known. Here \(d=M_{\beta}Y\), which appears to be random but, as discussed, \(M_{\beta}Y\) is not random; it is fixed and upon observing \(Y\) it is known.

The equivalent reduced model involves \(X_{0}=(I-M_{MP})X=(I-M_{\beta})X\) and a known vector \(Xb=M_{\beta}Y\). Thus, the constrained model is equivalent to

\[(Y-M_{\beta}Y)=(I-M_{\beta})X\gamma+e. \tag{3}\]

We want to show that least squares estimates of contrasts in the \(\alpha\)s based on \(Y\) are BLUEs with respect to this model. First we show that least squares estimates from model (3) based on \((Y-M_{\beta}Y)=(I-M_{\beta})Y\) are BLUEs. We need to show that

\[C(V(I-M_{\beta})X)=C[(I-M_{\mu}-M_{\beta})(I-M_{\beta})X]\subset C[(I-M_{\beta })X].\]

Because \((I-M_{\mu}-M_{\beta})(I-M_{\beta})=(I-M_{\mu}-M_{\beta})\), we have

\[C(V(I-M_{\beta})X)=C[(I-M_{\mu}-M_{\beta})X],\]and because \(C(I-M_{\mu}-M_{\beta})\subset C(I-M_{\beta})\) we have

\[C[(I-M_{\mu}-M_{\beta})X]\subset C[(I-M_{\beta})X].\]

To finish the proof that least squares estimates based on \(Y\) are BLUEs, note that the estimation space for model (3) is \(C[(I-M_{\beta})X]=C(M_{\mu}+M_{\alpha})\). BLUEs are based on

\[(M_{\mu}+M_{\alpha})(I-M_{\beta})Y=(M_{\mu}+M_{\alpha})Y.\]

Thus, any linear parametric function in model (2) that generates a constraint on \(C(M_{\mu}+M_{\alpha})\) has a BLUE based on \((M_{\mu}+M_{\alpha})Y\) (cf. Exercise 3.9.5). In particular, this is true for contrasts in the \(\alpha\)s. Standard errors for estimable functions are found in a manner analogous to Section 11.1. This is true even though model (3) is not the form considered in Section 11.1 and is a result of the orthogonality relationships that are present.

The assumption that \(\sigma_{1}^{2}=\sigma_{2}^{2}=\cdots=\sigma_{b}^{2}\) is a substantial one. Least squares estimates without this assumption are unbiased, but may be far from optimal. It is important to choose blocks so that their variances are approximately equal.

**Exercise G.2**: Find the standard error for a contrast in the \(\alpha_{i}\)s of model (1).

## Bibliography

* (1)
* (2)Christensen, R. (1989). Lack of fit tests based on near or exact replicates. _The Annals of Statistics_, _17_, 673-683.
* (3)
* (4)Christensen, R. (1991). Small sample characterizations of near replicate lack of fit tests. _Journal of the American Statistical Association_, _86_, 752-756.
* (5)
* (6)Christensen, R. (1995). Comment on Inman (1994). _The American Statistician_, _49_, 400.
* (7)
* (8)Christensen, R. (2003). Significantly insignificant\(F\) tests. _The American Statistician_, _57_, 27-32.
* (9)
* (10)Christensen, R. (2005). Testing Fisher, Neyman, Pearson, and Bayes. _The American Statistician_, _59_, 121-126.
* (11)
* (12)Christensen, R. (2008). Review of _Principals of statistical inference_ by D. R. Cox. _Journal of the American Statistical Association_, _103_, 1719-1723.
* (13)
* (14)Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton, FL: Chapman and Hall/CRC Pres.
* (15)
* (16)Christensen, R., & Bedrick, E. J. (1997). Testing the independence assumption in linear models. _Journal of the American Statistical Association_, _92_, 1006-1016.
* (17)
* (18)Davies, R. B. (1980). The distribution of linear combinations of \(\chi^{2}\) random variables. _Applied Statistics_, _29_, 323-333.
* (19)
* (20)Hinkelmann, K., & Kempthorne, O. (2005). _Design and analysis of experiments: Volume 2, Advanced experimental design_. Hoboken, NJ: Wiley.
* (21)
* (22)Hogfeldt, P. (1979). On low \(F\)-test values in linearmodels. _Scandinavian Journal of Statistics_, \(6\), 175-178.
* (23)
* (24)Lehmann, E. L. (1986). _Testing statistical hypotheses_ (2nd ed.). New York: Wiley.
* (25)
* (26)Puri, M. L., & Sen, P. K. (1971). _Nonparametric methods in multivariate analysis_. New York: Wiley.
* (27)
* (28)Rao, C. R. (1973). _Linear statistical inference and its applications_ (2nd ed.). New York: Wiley.

## Author Index

Aitchison, J., 57, 192

Anderson, T. W., 160

Andrews, D.F., 389

Arnold, S. F., 22, 356

Atkinson, A. C., 343, 361, 362, 388

Atwood, C. L., 184

B Bailey, D. W., 224

Bedrick, E. J., 16, 49, 51, 87, 230, 233, 237, 252, 276, 362, 363, 389, 421, 426, 508

Belsley, D. A., 396-398

Benedetti, J. K., 237

Berger, J. O., 48, 73

Berger, R. L., 73

Berk, K. N., 383

Berry, D. A., 48

Blom, G., 355

Blouin, D. C., 314

Box, G. E. P., 48, 192, 372, 388

Branscum, A., 48, 49, 364, 427

Bretz, F., 124

Brockwell, P. J., 160

Brownlee, K. A., 389

Brown, M. B., 237

C Carlin, J. B., 48

Casella, G., 73, 242

Cavanaugh, J. E., 426

Chen, G., 387

Christensen, R., 16, 31, 48, 49, 87, 88, 124, 127, 136, 145, 153, 154, 156, 178,

c Springer Nature Switzerland AG 2020

R. Christensen, _Plane Answers to Complex Questions_, Springer Texts in Statistics, [https://doi.org/10.1007/978-3-030-32097-3](https://doi.org/10.1007/978-3-030-32097-3)
Feldt, L. S., 101-103 Ferguson, T. S., 287 Fienberg, S. E., 230, 233, 421 Fisher, R. A., 107, 125, 142, 145, 178, 242, 332, 500 Forzani, L., 420 Francia, R. S., 356 Fraser, D. A. S., 442 Freedman, D. A., 375, 376 Friedman, J., 438, 441, 443 Furnival, G. M., 421

GGeisser, S., 48, 57, 156, 436 Gelman, A., 48 Gnanadesikan, R., 180 Goldstein, M., 410 Graybill, F. A., 22, 255, 276 Grizzle, J. E., 388 Gross, J., 282 Guttman, I., 57

H Haberman, S. J., xvi Hamada, M. S., 242 Hanson, T. E., 48, 49, 364, 427 Hartigan, J., 160 Harville, D. A., 22 Haslett, J., 387 Hastie, T., 428, 438, 441, 443 Hayes, K., 387 Hinkelmann, K., 242, 509 Hinkley, D. V., 35, 193 Hochberg, Y., 124 Hodges, J. S., 160 Hoerl, A. E., 405 Hogfeldt, P., 500 Holt, D., 314 Hothorn, T., 124 Hsu, J. C., 124 Huber, P. J., 410 Hunter, J. S., 192 Hunter, W. G., 192 Hunter, W. G., 192 Hurvich, C. M., 426 Huynh, H., 101-103 Huynh, H., 101-103

James, G., 428, 438 Jeffreys, H., 48 John, P. W. M., 242 Johnson, D. E., 180 Johnson, R. A., 160, 404 Johnson, W., 48, 49, 362-364, 387, 427, 508 Kempthorne, O., 242, 509 Kennard, R., 405 Koch, G. G., 388 Kuh, E., 397, 398 Kutner, M. H., 136

Lafotte, L. R., 264 Lehmann, E. L., 35, 36, 70, 501 Lenth, R. V., 362 Lin, Y., 304 Lindley, D. V., 48 Li, W., 136

Mandansky, A., 372 Mandel, J., 276 Marquardt, D. W., 404, 405 Martin, R. J., 387 Mathew, T., 314 McCullagh, P., 16, 250, 252 McCulloch, C. E., 160 Miller, F. R., 180 Miller, R. G., Jr., 124 Milliken, G. A., 255, 276 Mitra, S. K., 303 Moguerza, J. M., 416 Monlezun, C. J., 314 Morrison, Donald F., 161 Mosteller, F., 407 Munoz, A., 416

Nachtsheim, C. J., 136 Neill, J. W., 180 Nelder, J. A., 16 Netzer, J., 136 Neuhaus, J. M., 160* [44] Pearson, L. M., 387
* [45] Peixoto, J. L., 63
* [46] Petkova, E., 420
* [47] Picard, R. R., 383
* [48] Pukelsheim, F., 286
* [49] Puri, M. L., 509
* [50]

**R**

Raiffa, H., 48
* [51] Rao, C. R., 22, 35, 43, 101, 168, 276, 303, 388, 481
* [52] Ravishanker, N., 22
* [53] Reid, N., 242
* [54] Rencher, A. C., 22
* [55] Ripley, B. D., 160
* [56] Robert, C. P., 48
* [57] Ronchetti, E. M., 410
* [58] Rothman, A. J., 420
* [59] Rubin, D. B., 48
* [60] Ryan, T. A., Jr., 184

**S**

Savage, L. J., 48
* [61] Schaalje, G. B., 22
* [62] Schafer, D. W., 51
* [63] Schatzoff, M., 421
* [64] Scheffe, H., 22, 123, 126-135, 139-142, 148, 213, 224, 226, 232, 236
* [65] Schlaifer, R., 48
* [66] Schwarz, G., 166, 168, 427, 476
* [67] Searle, S. R., 22, 160, 286, 380
* [68] Seber, G. A. F., 22
* [69] Sen, P. K., 509
* [70] Shapiro, S. S., 356
* [71] Sherfey, B. W., 180
* [72] Shewhart, W. A., 127, 363
* [73] Shi, L., 387
* [74] Shillington, E. R., 180-182, 363
* [75] Shumway, R. H., 160
* [76] Sinha, B. K., 314
* [77] Skinner, C. J., 314
* [78] Smith, A. F. M., 155, 410
* [79] Smith, H., 146, 150, 342, 343, 389
* [80] Smith, T. M. F., 314
* [81] Snedecor, G. W., 136, 189, 256
* [82] Starmer, C. F., 388
* [83] Stefanski, L. A., 380
* [84] Stern, H. S., 48
* [85] St. Laurent, R. T., 276
* [86] Stoffer, D. S., 160
* [87] Sugiura, N., 426
* [88] Sulzberger, P. H., 277

**T**

Tamhane, A., 124
* [89] Tarpey, T., 420
* [90] Tiao, G. C., 48
* [91] Tibshirani, R., 428, 438, 441, 443
* [92] Tsai, C.-L., 426
* [93] Tsao, R., 421
* [94] Tukey, J. W., 123, 127, 135-139, 141-143, 276, 388, 407, 413, 435

**U**

Utts, J., 184, 185, 266, 276, 386

**V**

Van Nostrand, R. C., 406
* [95] Vehtari, A., 48
* [96] Velilla, S., 396

**W**

Watson, G. S., 367
* [97] Weisberg, S., 146, 343, 372, 373, 380, 385, 387, 388
* [98] Welsch, R. E., 397, 398
* [99] Wermuth, N., 237
* [100] Westfall, P., 124
* [101] Wichern, D. W., 160, 404
* [102] Wichura, M. J., 22
* [103] Wikle, C. K., 160
* [104] Wilk, M. B., 356
* [105] Williams, E. J., 277
* [106] Wilson, R. W., 421
* [107] Witten, D., 428, 438
* [108] Wood, F. S., 146, 389
* [109] Wu, C. F. J., 242

**Z**

Zellner, A., 48
* [110] Zhu, M., 416

[MISSING_PAGE_EMPTY:5825]

two-factor balanced with interaction, 204 balanced with quantitative factors, 214 balanced without interaction, 197 proportional numbers, 217 unbalanced, 219 Analysis of variance table, 115, 150, 212 Angle between vectors, 471 ANOVA, 2 ANOVA table, 202, 212 Assumptions, 341, 493, 501 Asymptotic consistency, 355 Asymptotic results, viii chi-squared distribution, 481 \(F\) distribution, 482 \(t\) distribution, 482 Central distribution, 482 Central limit theorem, viii Change of scale, 397 Characteristic function, 7, 488 Characteristic root, 459 Chi-squared distribution, 11, 37, 69, 100, 292, 293, 316, 323, 324, 481 Classification models, 2 Cluster error, 324 Cluster sampling, 314 Coefficient of determination, 165, 421 partial determination, 171 variation, 388 Collinearity, 393 Column space, 448 Comparisons, 116 Completely randomized design, 242 Complete statistic, 35, 36 Compound symmetry, 102 Concomitant variable, 261 Condition number, 398 Confidence bands, 148 ellipsoid, 97 interval, 37, 496 simultaneous, 132 region, 97 Confirmatory data analysis, 435 Consistent, 288, 355 Constrained estimation, 82, 84, 104 Constraint on, 81 Constraints estimable, 77, 84, 259 estimation under, 82, 84, 104 imposed by hypothesis, 81, 84 linear, 74 nonestimable, 24 nonidentifiable, 75, 76, 83, 84, 114, 120, 206 Constructed variable tests, 388 Contrasts balanced incomplete blocks (BIBs), 271 one-way, 116 orthogonal, 94, 119 polynomial, 188, 215 two-way with interaction, 207, 214 two-way without interaction, 203 Cook's distance, 386 Corrected AIC, 427Correction factor, 114, 150 Correlation coefficient, 166, 193 multiple, 166 partial, 170, 193, 429 serial, 363 Cost complexity pruning, 427 Counts, 388 Covariance, 5, 160 analysis of, 255 Covariance matrix, 5 Covariance parameterization, 489 linear, 490 Covariate, 261 CRD, 242 Critical region, 493 Cross-validation, 427

**D**

Degrees of freedom, 481 for error, 32 Deleted residual, 380 Design matrix, 1 Design space, 62 Determinant, 8, 463 Diagnostics, 341 Diagonal matrix, 457 Dispersion matrix, 5 Distance measures, 165, 345, 450, 471 Distributions chi-squared, 11, 481 \(F\), 482 doubly noncentral, 183 gamma, 49, 53 multivariate normal, 7 \(t\), 482 Duncan's multiple range test, 124, 139 Dunnett's method, 124 Durbin-Watson test, 367

**E**

EDA, 435 Eigenvalue, 459 Eigenvector, 459 Empirical estimate, 375 Error degrees of freedom, 32 Error mean square, 32 Error rate, 123 Error space, 62 Estimable, 22, 24, 39 Estimable constraints, 77, 84, 259 Estimable part, 82 Estimation Bayesian, 48, 405, 406 best linear unbiased (BLUE), 33, 39, 174 consistent linear unbiased (CLUE), 290 general Gauss-Markov models, 281 generalized least squares (GLS), 39, 281 generalized split plot, 322, 325 least squares, 28, 42, 300, 315, 319 maximum likelihood, 34, 39 minimum variance unbiased, 35, 39 ordinary least squares (OLS), 28, 42, 300, 315, 319 simple least squares, 28, 42, 300, 315, 319 unbiased, 27 for variance, 31 uniformly minimum variance unbiased (UMVU), 35, 39 variance, unbiased, 31 weighted least squares, 42 with constraints, 82, 84, 104 Estimation space, 62 Expected mean squares, 115, 201, 206 Expected squared error, 156, 394 Expected values, 485 quadratic forms, 11 random vectors and matrices, 4 Experimental unit, 241 Experimentwise error rate, 123, 124, 142 Exploratory data analysis, 435 Exponential regression, 16

**F**

Factor, 247 Factorial design, 247, 250 Factorial experiment, 247 Factorial treatment structure, 247, 250 Fieller's method, 97, 191 Fisherian testing, 70, 142, 491, 493, 499, 500 Fisher significant difference (FSD), 133 Fisher's "z" distribution, 500 Fitted values, 31, 369 definition, 2 Forward selection, 429 Full model, 64, 68, 500 Fundamental theorem of least squares estimation, 28

**G**

Gamma distribution, 49, 53 Gamma regression, 16 Gaussian distribution, 7Gauss-Markov Theorem, 33 General Gauss-Markov estimation, 281 testing, 291 Generalized additive models, 154, 186 Generalized inverse, 466 general form, 478 Generalized inverse regression, 404 Generalized least squares estimation, 39 testing, 98 Generalized likelihood ratio test, 70, 73, 104 Generalized linear model, 16, 22, 27, 153, 157, 413 Generalized split plot models, 319 General linear model, 16 Graeco-Latin squares, 246 Gram-Schmidt orthogonalization, 91, 187, 199, 451, 460, 465, 470 Grand mean, 114 Greedy algorithm, 419

**H**

Hamming loss, 159 Heterogeneous variances, 342 Heteroscedastic, 342, 507 High leverage, 347 Homologous factors, 251 Homoscedastic, 504 HSD, 124, 135 Huber-White estimator, 376 Huynh-Feldt condition, 102

**I**

Idempotent matrix, 470 Identifiable, 22, 23, 489 Identity matrix, 457 Ill-conditioned, 398 Ill-conditioned model matrix, 395 i.i.d, 7 Ill-defined, 395 Incomplete blocks, 241, 267 Independence, 9, 13, 230 contingency table, 240 linear, 448 random vectors, 487 Independent identically distributed, 7 Influential observation, 342 Information criteria, 425 Inner product, 43, 164, 450, 471 Interaction

BIB designs, 275 contrasts, 207 factorial treatment structure, 247 plot, 213 split plot designs, 329 test, 205 three-way ANOVA, 230, 232 two-way ANOVA, 207, 215, 222 Interval estimation, 37, 496 Intraclass correlation, 102, 315 Invariance, 70 Inverse matrix, 458

**J**

Joint distribution, 485

**K**

Kernel trick, 409 Kriging, 176 Kronecker product, 240, 250, 458, 473, 474

**L**

Lack of fit, 177, 204, 369, 499 near replicate tests, 180 partitioning tests, 182 residual analysis, 157, 377 traditional test, 178 Lasso, 421 Latin square design, 243 Least squares consistent estimate, 296 estimate, 28, 319 generalized, 39, 98, 281 ordinary, 39, 42, 300, 315, 328 simple, 39, 42, 300, 315, 328 weighted, 42 Legendre polynomials, 190 Length, 165, 450, 471 Leverage, 344, 347 Likelihood function, 34, 40 Likelihood ratio test, 70, 73, 104 Linear combination, 62, 448 Linear constraint, 74 Linear covariance parameterization, 490 Linear dependence, 448 Linear estimable function, 24 Linear estimate, 27 Linear independence, 448 Linear model standard, 1 Locally weighted scatterplot smoother, 153 Logistic regression, 16, 87, 159, 388Logit model, 16, 87, 388

Log-linear model, 16, 87, 388

Lowess, 153

LSD, 123, 133

LSE, 28

Mahalanobis distance, 345

Mallows's \(C_{P}\), 424

Marginal distribution, 486

Matrix

design, 1

diagonal, 457

generalized inverse, 466

idempotent, 470

identity, 457

inverse, 458

model, 1

nonnegative definite, 462

orthogonal, 461

orthonormal, 461

partitioned, 458

inverse, 479

positive definite, 462

projection, 470

oblique, 471

perpendicular, 463

square, 457

symmetric, 457

zero, 473

Maximum likelihood estimates

generalized least squares models, 39

standard models, 34

Mean squared error, 32

population, 156, 394

Mean squared groups, 115

M-estimates, 412

Milliken and Graybill test, 276

Minimum variance unbiased estimate, 35

Missing data, 265

Mixed model, 313

MLEs, 34, 425

Model matrix, 1

Models, 1

analysis of covariance, 255

analysis of variance

balanced incomplete block (BIB),

267

multifactor, 230

one-way, 107

three-way, 230

two-way, 197, 204, 214, 217, 219

balanced incomplete block (BIB) design,

267

cell means, 205, 223

cluster sampling, 314

completely randomized design (CRD),

242

estimable constraints, 104

experimental design, 241

full, 64, 68

general Gauss-Markov, 281

generalized least squares, 38, 98

generalized split plot, 319

Graeco-Latin square, 246

Latin square, 243

randomization theory, 509

randomized complete block (RCB)

design, 242, 318, 328

reduced, 64, 68

split plot design, 328

subsampling, 332

Model selection, 419

Multicollinearity, 393

Multicfactor structures, 230

Multiple comparisons, 123, 124

Multiple correlation coefficient, 166

Multiple range method, 137, 139

Multiple regression, 148

Multiple testing, 124

Multivariate distribution, 485

Multivariate normal, 7

Newman-Keuls multiple range test, 124, 137

Neyman-Pearson testing, 70, 142, 491, 499

Noncentral

chi-squared distribution, 481

\(F\) distribution, 482

\(t\) distribution, 482

Noncentrality parameter, 69, 98, 481

Nonestimable, 24

Nonestimable constraints, 30

Nonidentifiable, 23, 24

Nonidentifiable constraints, 75, 76, 83, 84,

114, 120, 206

Nonnegative definite matrix, 462

Nonnormality, 342

Nonparametric methods, 509

Nonparametric regression, 153, 186

Nonsingular case, 258

Nonsingular covariance matrix, 38, 98, 281

Nonsingular distribution, 5Nonsingular matrix, 458

Normal distribution, 7

Normal equations, 43

Normality, test for, 356

Normal plot, 354

Normal score, 354

Null hypothesis, 491

Null model, 70, 491, 500

Null space, 458, 479

**O**

Oblique projection, 478

Oblique projection operator, 471

Odds ratio, 240

Offset, 57, 72, 73, 84, 105, 424

OLS, 39

One sample, 38, 73

One-way analysis of variance (ANOVA), 107

Optimal allocation of \(x\) values, 193

Ordinary least squares, 39, 42, 300, 328

Ordinary residual, 6, 263, 342

Orthogonal, 165, 450, 471

basis, 450

complement, 452, 476

constraints, 90, 91

contrasts, 94, 119

distance regression, 413

matrix, 461

polynomials, 187, 215

projection, 165, 463, 471

Orthonormal

basis, 450, 451, 453, 460, 461, 465

matrix, 359, 461, 479

Outliers

in dependent variable, 380, 384

in the design space, 347, 384

in the estimation space, 347, 384

Overfitting, 437

**P**

Parameter, 1, 22, 491

Parameterization, 22

Partial correlation coefficient, 170, 193, 429

Partial determination, coefficient of, 171

Partially identifiable, 23

Partitioned matrices, 458

inverse, 479

Partitioned model, 256

PC, 404

PC, 404, 408

Penalized estimation, 405

Percentile, 481

Perfect estimation, 304

Perpendicular, 165, 450, 471

projection, 157, 163

projection operator (ppo), 165, 344, 463,

471

Poisson distribution, 16, 388

Polynomial contrasts, 188, 215

Polynomial regression, 147, 186, 214

Positive definite matrix, 462

Power, 70, 497

Power transformations, 388

Predicted \(R\)2, 423

Predicted residual, 350, 380

Predicted REsidual Sum of Squares (PRESS), 381

Predicted values, 31, 369

Prediction, 155, 172

best linear predictor (BLP), 160, 173

best linear unbiased predictor (BLUP), 163, 172, 174, 343

best predictor (BP), 156, 173

Prediction interval, 57

Predictor, 174

PRESS, 381

Principal component, 404

Principal component regression (PCR), 404,

408

Probability distribution, 485

Projection

oblique, 471

perpendicular, 157, 163, 165, 463

Projection operator, 470

Proportional numbers, 95, 217, 230

Proportions, 388

Pure error, 177, 203

**Q**

q-q plot, 354

Quadratic forms, 11, 474

distribution, 12, 13

expectation, 11

independence, 14, 15

Quadratic formula, 415

Quadratic formula, 354

Quadratic factors, 188, 214

**R**

Random effects, 176

Randomization, 241

Randomization theory, 509Randomized complete block design, 241, 242, 318, 328 Random matrix, 4 Random vector, 5 Range, 136 Range space, 448 Rank, 449 Rankit plot, 354 Rao's simple covariance structure, 43, 101 RCB, 242 Recovery of interblock information, 268 Reduced model, 44, 64, 68, 500 Reduction in sums of squares, 95 Reference distribution, 491 Reflexive generalized inverse, 467 Regression analysis definition, 2 in canonical form, 401 multiple regression, 148 nonparametric, 153 polynomial, 186, 214 simple linear regression, 146 Regression model, 145, 420 Regularization, 405 Rejection region, 70, 482, 493 Reparameterization, 36, 62, 63, 72, 73, 75, 83, 146, 151, 264 Residual mean square, 394, 422 Residual plots, 342 heteroscedasticity, 369 lack of fit, 377 normality, 354 serial correlation, 364, 380 Residuals, 6, 31 definition, 2 deleted, 350, 380 predicted, 350, 380 standardized, 342 standardized deleted, 384 standardized predicted residuals, 384 Studentized, 342 Residual sum of squares, 394 Response surface, 192 Ridge regression, 405, 410 Ridge trace, 406 Robust regression, 411 Rotation, 479 Row structure, 178

Sample partial correlation coefficient, 171 Sandwich estimator, 376 Scaling the model matrix, 397 Scheffe's method, 123, 128, 148 Sequential fitting, 90, 95, 149 Sequential sums of squares, 90, 95, 149 Serial correlation, 363 test, 367 Side conditions, 30, 75, 76, 83, 84, 114, 120, 206 estimation under, 82, 84, 104 Significance testing, 70, 142, 491, 493, 499, 500 Simple covariance structure, 43, 101 Simple least squares, 39 Simple linear regression, 146 Simultaneous confidence intervals, 132 Simultaneous inference, 123, 148 Singular covariance matrix, 281 Singular distribution, 5 Singular value, 459 Singular value decomposition, 401, 462 Skew symmetric additive effects, 251 Spanning set, 448 Spanning space, 448 Spatial data, 176 Split plot designs, 313, 328 generalized, 319 Split plot model, 103 Squared predictive correlation, 168 Square matrix, 457 Standard error, 491 Standardized deleted residual, 384 Standardized predicted residual, 384 Standardized residuals, 342 Standard linear model, 1, 6, 10 Stepwise regression, 428 Stochastically larger, 483 Studentized range, 136 Studentized residuals, 342, 384 Student's \(t\), 37, 50, 57, 482, 492 Subplot analysis, 313, 328 Subplot error, 323 Subsampling, 332 Subspace, 447 Summation convention, 474 Sum of squares contrast, 118 error, 32 for regressing, 90 for testing \(H_{0}:\lambda^{\prime}\beta=0\), 81 groups, 115 reduction in, 95 regression, 150 total, 114Supplemental observations, 261 Support vector machine, 413 Sweep operator, 264, 440 Symmetric additive effects, 251 Symmetric matrix, 457

**T**

Tensor, 474 Tests, 61, 493 \(\alpha\) level, 70, 493, 500 Durbin-Watson, 367 generalized likelihood ratio, 70, 73, 104 independence, 362 lack of fit, 177 Milliken and Graybill, 276 models cluster sampling, 317 general Gauss-Markov, 291 generalized least squares, 98 generalized split plot, 326 standard, 64 multiple comparisons, 123 normality, 356 one-parameter, 491 parametric functions cluster sampling models, 317 generalized least squares models, 100 generalized split plot models, 322, 325 standard models, 74 serial correlation, 367 single degree of freedom, 37, 89 Tukey's one degree of freedom, 276 variances, 104 Wilk-Shapiro, 356 Test space, 91 Test statistic, 67, 70, 72, 77, 81, 84, 89, 91, 99, 100, 493, 499, 500 Three-way ANOVA, 230 Toeplitz matrix, 363 Tolerance, 399, 429 Tolerance point, 57 Topics in Design, 242, 268 Trace, 462 Transformations, 388 Box-Cox, 388 Grizzle, Starmer, Koch, 388 power, 388 variance stabilizing, 388 Transpose, 447, 457 Tukey's biweight, 413 Tukey's HSD, 124, 135 Tukey's one degree of freedom, 276 Tukey's one degree of freedom for nonadditivity, 388 Two independent samples, 37, 73 Two-phase linear regression, 192 Two-stage sampling, 314 Two-way ANOVA, 197, 204, 214, 217, 219 **U**

UVMVU, 39 Unbalanced ANOVA, 219, 232 Unbiased estimate, 27, 31, 33, 35, 282, 293 Unbiased predictor, 174 Unequal numbers, 219, 232 Uniformly minimum variance unbiased (UMVU), 35 Uniformly minimum variance unbiased (UMVU) estimate, 35 Uniformly most powerful invariant (UMPI) test, 70 Unreplicated experiments, 360 Updating formulae, 380 Usual constraints, 114 Usual multiple regression model, 149 Utts's rainbow test, 184, 266, 386 **V**

Variable selection, 419, 421, 428, 433 Variance component, 313 Variance-covariance matrix, 5 Variance estimation Bayesian, 50, 55 general Gauss-Markov models, 292, 293 generalized least squares models, 41 standard models, 31, 35, 36 Variance inflation factor, 399 Variance stabilizing transformations, 388 Vec operator, 458, 473 Vector, 447 angle between, 471 space, 447 VIF, 399

**W**

Weakest link pruning, 427 Weak experimentwise error rate, 123, 124 Weighted least squares, 42 Well defined parameterization, 489* [14] Whole plot, 313
* [15] Whole-plot analysis, 318, 319, 325, 326, 328
* [16] Whole-plot error, 324
* [17] Wilk-Shapiro test, 356
* [18] WLS, 42
* [19] Working-Hotelling confidence bands, 148
* [20]