[MISSING_PAGE_EMPTY:1]

_Advisors_:

George Casella Stephen Fienberg Ingram OlkinSpringer Texts in Statistics

_Alfred:_ Elements of Statistics for the Life and Social Sciences

_Athreya and Lahiri:_ Measure Theory and Probability Theory

_Berger._ An Introduction to Probability and Stochastic Processes

_Bilodeau and Brenner._ Theory of Multivariate Statistics

_Blom:_ Probability and Statistics: Theory and Applications

_Brockwell and Davis:_ Introduction to Times Series and Forecasting, Second Edition

_Carmona:_ Statistical Analysis of Financial Data in S-Plus

_Chow and Teicher._ Probability Theory: Independence, Interchangeability,

Martingales, Third Edition

_Christensen:_ Advanced Linear Modeling: Multivariate, Time Series, and

Spatial Data--Nonparametric Regression and Response Surface

Maximization, Second Edition

_Christensen:_ Log-Linear Models and Logistic Regression, Second Edition

_Christensen:_ Plane Answers to Complex Questions: The Theory of Linear

Models, Third Edition

_Creighton:_ A First Course in Probability Models and Statistical Inference

_Davis:_ Statistical Methods for the Analysis of Repeated Measurements

_Dean and Voss:_ Design and Analysis of Experiments

_du Toit, Steyn, and Stumpf:_ Graphical Exploratory Data Analysis

_Durrett:_ Essentials of Stochastic Processes

_Edwards:_ Introduction to Graphical Modelling, Second Edition

_Finkelstein and Levin:_ Statistics for Lawyers

_Flury:_ A First Course in Multivariate Statistics

_Ghosh, Delampady and Samanta:_ An Introduction to Bayesian Analysis:

Theory and Methods

_Gut:_ Probability: A Graduate Course

_Heiberger and Holland:_ Statistical Analysis and Data Display:

An Intermediate Course with Examples in S-PLUS, R, and SAS

_Jobson:_ Applied Multivariate Data Analysis, Volume I: Regression and

Experimental Design

_Jobson:_ Applied Multivariate Data Analysis, Volume II: Categorical and

Multivariate Methods

_Kalbfleisch:_ Probability and Statistical Inference, Volume I: Probability,

Second Edition

_Kalbfleisch:_ Probability and Statistical Inference, Volume II: Statistical

Inference, Second Edition

_Karr:_ Probability

_Keyfitz:_ Applied Mathematical Demography, Second Edition

_Kiefer:_ Introduction to Statistical Inference

_Kokoska and Nevison:_ Statistical Tables and Formulae

_Kulkarni:_ Modeling, Analysis, Design, and Control of Stochastic Systems

_Lange:_ Applied Probability

_Lange:_ Optimization

_Lehmann:_ Elements of Large-Sample Theory

_(continued after index)_Krishna B. Athreya

Sounendra N. Lahiri

Measure Theory

and Probability TheoryKrishna B. Athreya

Department of Mathematics and

Department of Statistics

Iowa State University

Ames, IA 50011

kba@iastate.edu

Soundra N. Lahiri

Department of Statistics

Iowa State University

Ames, IA 50011

snlahiri@iastate.edu

_Editorial Board_

George Casella

Department of Statistics

University of Florida

Gainesville, FL 32611-8545

Stephen Fienberg

Department of Statistics

Carnegie Mellon University

Pittsburgh, PA 15213-3890

Ingram Olkin

Department of Statistics

Stanford University

Stanford, CA 94305

USA

Library of Congress Control Number: 2006922767

ISBN-10: 0-387-32903-X

e-ISBN: 0-387-35434-4

ISBN-13: 978-0387-32903-1

Printed on acid-free paper.

(c)2006 Springer Science+Business Media, LLC

All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excepts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.

The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.

Printed in the United States of America. (MVY)

9 8 7 6 5 4 3 2 1

springer.comDedicated to our wives

Krishna S. Athreya and Publai Banerjee

and

to the memory of

Uma Mani Athreya and Narayani Ammal

## Preface

This book arose out of two graduate courses that the authors have taught during the past several years; the first one being on measure theory followed by the second one on advanced probability theory.

The traditional approach to a first course in measure theory, such as in Royden (1988), is to teach the Lebesgue measure on the real line, then the differentation theorems of Lebesgue, \(L^{p}\)-spaces on \(\mathbb{R}\), and do general measure at the end of the course with one main application to the construction of product measures. This approach does have the pedagogic advantage of seeing one concrete case first before going to the general one. But this also has the disadvantage in making many students' perspective on measure theory somewhat narrow. It leads them to think only in terms of the Lebesgue measure on the real line and to believe that measure theory is intimately tied to the topology of the real line. As students of statistics, probability, physics, engineering, economics, and biology know very well, there are mass distributions that are typically nonuniform, and hence it is useful to gain a general perspective.

This book attempts to provide that general perspective right from the beginning. The opening chapter gives an informal introduction to measure and integration theory. It shows that the notions of \(\sigma\)-algebra of sets and countable additivity of a set function are dictated by certain very natural approximation procedures from practical applications and that they are not just some abstract ideas. Next, the general extension theorem of Caratheory is presented in Chapter 1. As immediate examples, the construction of the large class of Lebesgue-Stieltjes measures on the real line and Euclidean spaces is discussed, as are measures on finite and countablespaces. Concrete examples such as the classical Lebesgue measure and various probability distributions on the real line are provided. This is further developed in Chapter 6 leading to the construction of measures on sequence spaces (i.e., sequences of random variables) via Kolmogorov's consistency theorem.

After providing a fairly comprehensive treatment of measure and integration theory in the first part (Introduction and Chapters 1-5), the focus moves onto probability theory in the second part (Chapters 6-13). The feature that distinguishes probability theory from measure theory, namely, the notion of independence and dependence of random variables (i.e., measureable functions) is carefully developed first. Then the laws of large numbers are taken up. This is followed by convergence in distribution and the central limit theorems. Next the notion of conditional expectation and probability is developed, followed by discrete parameter martingales. Although the development of these topics is based on a rigorous measure theoretic foundation, the heuristic and intuitive backgrounds of the results are emphasized throughout. Along the way, some applications of the results from probability theory to proving classical results in analysis are given. These include, for example, the density of normal numbers on (0,1) and the Wierstrass approximation theorem. These are intended to emphasize the benefits of studying both areas in a rigorous and combined fashion. The approach to conditional expectation is via the mean square approximation of the "unknown" given the "known" and then a careful approximation for the \(L^{1}\)-case. This is a natural and intuitive approach and is preferred over the "black box" approach based on the Radon-Nikodym theorem.

The final part of the book provides a basic outline of a number of special topics. These include Markov chains including Markov chain Monte Carlo (MCMC), Poisson processes, Brownian motion, bootstrap theory, mixing processes, and branching processes. The first two parts can be used for a two-semester sequence, and the last part could serve as a starting point for a seminar course on special topics.

This book presents the basic material on measure and integration theory and probability theory in a self-contained and step-by-step manner. It is hoped that students will find it accessible, informative, and useful and also that they will be motivated to master the details by carefully working out the text material as well as the large number of exercises. The authors hope that the presentation here is found to be clear and comprehensive without being intimidating.

Here is a quick summary of the various chapters of the book. After giving an informal introduction to the ideas of measure and integration theory, the construction of measures starting with set functions on a small class of sets is taken up in Chapter 1 where the Caratheodory extension theorem is proved and then applied to construct Lebesgue-Stieltjes measures. Integration theory is taken up in Chapter 2 where all the basic convergence theorems including the MCT, Fatou, DCT, BCT, Egorov's, and Scheffe's are proved. Included here are also the notion of uniform integrability and the classical approximation theorem of Lusin and its use in \(L^{p}\)-approximation by smooth functions. The third chapter presents basic inequalities for \(L^{p}\)-spaces, the Riesz-Fischer theorem, and elementary theory of Banach and Hilbert spaces. Chapter 4 deals with Radon-Nikodym theory via the Riesz representation on \(L^{2}\)-spaces and its application to differentiation theorems on the real line as well as to signed measures. Chapter 5 deals with product measures and the Fubini-Tonelli theorems. Two constructions of the product measure are presented: one using the extension theorem and another via iterated integrals. This is followed by a discussion on convolutions, Laplace transforms, Fourier series, and Fourier transforms. Kolmogorov's consistency theorem for the construction of stochastic processes is taken up in Chapter 6 followed by the notion of independence in Chapter 7. The laws of large numbers are presented in a unified manner in Chapter 8 where the classical Kolmogorov's strong law as well as Etemadi's strong law are presented followed by Marcinkiewicz-Zygmund laws. There are also sections on renewal theory and ergodic theorems. The notion of weak convergence of probability measures on \(\mathbb{R}\) is taken up in Chapter 9, and Chapter 10 introduces characteristic functions (Fourier transform of probability measures), the inversion formula, and the Levy-Cramer continuity theorem. Chapter 11 is devoted to the central limit theorem and its extensions to stable and infinitely divisible laws. Chapter 12 discusses conditional expectation and probability where an \(L^{2}\)-approach followed by an approximation to \(L^{1}\) is presented. Discrete time martingales are introduced in Chapter 13 where the basic inequalities as well as convergence results are developed. Some applications to random walks are indicated as well. Chapter 14 discusses discrete time Markov chains with a discrete state space first. This is followed by discrete time Markov chains with general state spaces where the regeneration approach for Harris chains is carefully explained and is used to derive the basic limit theorems via the iid cycles approach. There are also discussions of Feller Markov chains on Polish spaces and Markov chain Monte Carlo methods. An elementary treatment of Brownian motion is presented in Chapter 15 along with a treatment of continuous time jump Markov chains. Chapters 16-18 provide brief outlines respectively of the bootstrap theory, mixing processes, and branching processes. There is an Appendix that reviews basic material on elementary set theory, real and complex numbers, and metric spaces.

Here are some suggestions on how to use the book.

1. For a one-semester course on real analysis (i.e., measure end integration theory), material up to Chapter 5 and the Appendix should provide adequate coverage with Chapter 6 being optional.
2. A one-semester course on advanced probability theory for those with the necessary measure theory background could be based on Chapters 6-13 with a selection of topics from Chapters 14-18.

3. A one-semester course on combined treatment of measure theory and probability theory could be built around Chapters 1, 2, Sections 3.1-3.2 of Chapter 3, all of Chapter 4 (Section 4.2 optional), Sections 5.1 and 5.2 of Chapter 5, Chapters 6, 7, and Sections 8.1, 8.2, 8.3 (Sections 8.5 and 8.6 optional) of Chapter 8. Such a course could be followed by another that includes some coverage of Chapters 9-12 before moving on to other areas such as mathematical statistics or martingales and financial mathematics. This will be particularly useful for graduate programs in statistics.
4. A one-semester course on an introduction to stochastic processes or a seminar on special topics could be based on Chapters 14-18.

A word on the numbering system used in the book. Statements of results (i.e., Theorems, Corollaries, Lemmas, and Propositions) are numbered consecutively within each section, in the format \(a.b.c\), where \(a\) is the chapter number, \(b\) is the section number, and \(c\) is the counter. Definitions, Examples, and Remarks are numbered individually within each section, also of the form \(a.b.c\), as above. Sections are referred to as \(a.b\) where \(a\) is the chapter number and \(b\) is the section number. Equation numbers appear on the right, in the form \((b.c)\), where \(b\) is the section number and \(c\) is the equation number. Equations in a given chapter \(a\) are referred to as \((b.c)\) within the chapter but as \((a.b.c)\) outside chapter \(a\). Problems are listed at the end of each chapter in the form \(a.c\), where \(a\) is the chapter number and \(c\) is the problem number.

In the writing of this book, material from existing books such as Apostol (1974), Billingsley (1995), Chow and Teicher (2001), Chung (1974), Durrett (2004), Royden (1988), and Rudin (1976, 1987) has been freely used. The authors owe a great debt to these books. The authors have used this material for courses taught over several years and have benefited greatly from suggestions for improvement from students and colleagues at Iowa State University, Cornell University, the Indian Institute of Science, and the Indian Statistical Institute. We are grateful to them.

Our special thanks go to Dean Issacson, Ken Koehler, and Justin Peters at Iowa State University for their administrative support of this long project. Krishna Athreya would also like to thank Cornell University for its support.

We are most indebted to Sharon Shepard who typed and retyped several times this book, patiently putting up with our never-ending "final" versions. Without her patient and generous help, this book could not have been written. We are also grateful to Denise Riker who typed portions of an earlier version of this book.

John Kimmel of Springer got the book reviewed at various stages. The referee reports were very helpful and encouraging. Our grateful thanks to both John Kimmel and the referees.

We have tried hard to make this book free of mathematical and typographical errors and misleading or ambiguous statements, but we are aware that there will still be many such remaining that we have not caught. We will be most grateful to receive such corrections and suggestions for improvement. They can be e-mailed to us at _kba@iastate.edu_ or _snlahiri@iastate.edu_.

On a personal note, we would like to thank our families for their patience and support. Krishna Athreya would like to record his profound gratitude to his maternal granduncle, the late Shri K. Venkatarama Iyer, who opened the door to mathematical learning for him at a crucial stage in high school, to the late Professor D. Basu of the Indian Statistical Institute who taught him to think probabilistically, and to Professor Samuel Karlin of Stanford University for initiating him into research in mathematics.

K. B. Athreya

S. N. Lahiri

May 12, 2006

###### Contents

* 1 Measures
	* 1.1 Classes of sets
	* 1.2 Measures
	* 1.3 The extension theorems and Lebesgue-Stieltjes measures
		* 1.3.1 Caratheodory extension of measures
		* 1.3.2 Lebesgue-Stieltjes measures on \(\mathbb{R}\)
		* 1.3.3 Lebesgue-Stieltjes measures on \(\mathbb{R}^{2}\)
		* 1.3.4 More on extension of measures
	* 1.4 Completeness of measures
	* 1.5 Problems
* 2 Integration
	* 2.1 Measurable transformations
	* 2.2 Induced measures, distribution functions
		* 2.2.1 Generalizations to higher dimensions
	* 2.3 Integration
	* 2.4 Riemann and Lebesgue integrals
	* 2.5 More on convergence
	* 2.6 Problems

## 3 \(L^{p}\)-Spaces

\(\phantom{a}\)3.1 Inequalities \(\phantom{a}\)3.2 \(L^{p}\)-Spaces \(\phantom{a}\)3.2.1 Basic properties \(\phantom{a}\)3.2.2 Dual spaces \(\phantom{a}\)3.3 Banach and Hilbert spaces \(\phantom{a}\)3.3.1 Banach spaces \(\phantom{a}\)3.3.2 Linear transformations \(\phantom{a}\)3.3.3 Dual spaces \(\phantom{a}\)3.3.4 Hilbert space \(\phantom{a}\)3.4 Problems
* 4 Differentiation \(\phantom{a}\)4.1 The Lebesgue-Radon-Nikodym theorem \(\phantom{a}\)4.2 Signed measures \(\phantom{a}\)4.3 Functions of bounded variation \(\phantom{a}\)4.4 Absolutely continuous functions on \(\mathbb{R}\) \(\phantom{a}\)4.5 Singular distributions \(\phantom{a}\)4.5.1 Decomposition of a cdf \(\phantom{a}\)4.5.2 Cantor ternary set \(\phantom{a}\)4.5.3 Cantor ternary function \(\phantom{a}\)4.6 Problems
* 5 Product Measures, Convolutions, and Transforms \(\phantom{a}\)5.1 Product spaces and product measures \(\phantom{a}\)5.2 Fubini-Tonelli theorems \(\phantom{a}\)5.3 Extensions to products of higher orders \(\phantom{a}\)5.4 Convolutions \(\phantom{a}\)5.4.1 Convolution of measures on \(\bigl{(}\mathbb{R},\mathcal{B}(\mathbb{R})\bigr{)}\)5.4.2 Convolution of sequences \(\phantom{a}\)5.4.3 Convolution of functions in \(L^{1}(\mathbb{R})\)5.4.4 Convolution of functions and measures \(\phantom{a}\)5.5 Generating functions and Laplace transforms \(\phantom{a}\)5.6 Fourier series \(\phantom{a}\)5.7 Fourier transforms on \(\mathbb{R}\) \(\phantom{a}\)5.8 Plancherel transform \(\phantom{a}\)5.9 Problems
* 6 Probability Spaces \(\phantom{a}\)6.1 Kolmogorov's probability model \(\phantom{a}\)6.2 Random variables and random vectors \(\phantom{a}\)6.3 Kolmogorov's consistency theorem \(\phantom{a}\)6.4 Problems
* 7 Independence	* 7.1 Independent events and random variables
	* 7.2 Borel-Cantelli lemmas, tail \(\sigma\)-algebras, and Kolmogorov's zero-one law
	* 7.3 Problems
* 8 Laws of Large Numbers
	* 8.1 Weak laws of large numbers
	* 8.2 Strong laws of large numbers
	* 8.3 Series of independent random variables
	* 8.4 Kolmogorov and Marcinkiewz-Zygmund SLLNs
	* 8.5 Renewal theory
		* 8.5.1 Definitions and basic properties
		* 8.5.2 Wald's equation
		* 8.5.3 The renewal theorems
		* 8.5.4 Renewal equations
		* 8.5.5 Applications
	* 8.6 Ergodic theorems
		* 8.6.1 Basic definitions and examples
		* 8.6.2 Birkhoff's ergodic theorem
	* 8.7 Law of the iterated logarithm
	* 8.8 Problems
* 9 Convergence in Distribution
	* 9.1 Definitions and basic properties
	* 9.2 Vague convergence, Helly-Bray theorems, and tightness
	* 9.3 Weak convergence on metric spaces
	* 9.4 Skorohod's theorem and the continuous mapping theorem
	* 9.5 The method of moments and the moment problem
		* 9.5.1 Convergence of moments
		* 9.5.2 The method of moments
		* 9.5.3 The moment problem
	* 9.6 Problems
* 10 Characteristic Functions
	* 10.1 Definition and examples
	* 10.2 Inversion formulas
	* 10.3 Levy-Cramer continuity theorem
	* 10.4 Extension to \(\mathbb{R}^{k}\)
	* 10.5 Problems
* 11 Central Limit Theorems
	* 11.1 Lindeberg-Feller theorems
	* 11.2 Stable distributions
	* 11.3 Infinitely divisible distributions
	* 11.4 Refinements and extensions of the CLT
11.4.1 The Berry-Esseen theorem * 11.4.2 Edgeworth expansions * 11.4.3 Large deviations * 11.4.4 The functional central limit theorem * 11.4.5 Empirical process and Brownian bridge * 11.5 Problems
* 12 Conditional Expectation and Conditional Probability
	* 12.1 Conditional expectation: Definitions and examples
	* 12.2 Convergence theorems
	* 12.3 Conditional probability
	* 12.4 Problems
* 13 Discrete Parameter Martingales
	* 13.1 Definitions and examples
	* 13.2 Stopping times and optional stopping theorems
	* 13.3 Martingale convergence theorems
	* 13.4 Applications of martingale methods
		* 13.4.1 Supercritical branching processes
		* 13.4.2 Investment sequences
		* 13.4.3 A conditional Borel-Cantelli lemma
		* 13.4.4 Decomposition of probability measures
		* 13.4.5 Kakutani's theorem
		* 13.4.6 de Finetti's theorem
	* 13.5 Problems
* 14 Markov Chains and MCMC
	* 14.1 Markov chains: Countable state space
		* 14.1.1 Definition
		* 14.1.2 Examples
		* 14.1.3 Existence of a Markov chain
		* 14.1.4 Limit theory
	* 14.2 Markov chains on a general state space
		* 14.2.1 Basic definitions
		* 14.2.2 Examples
		* 14.2.3 Chapman-Kolmogorov equations
		* 14.2.4 Harris irreducibility, recurrence, and minorization
		* 14.2.5 The minorization theorem
		* 14.2.6 The fundamental regeneration theorem
		* 14.2.7 Limit theory for regenerative sequences
		* 14.2.8 Limit theory of Harris recurrent Markov chains
		* 14.2.9 Markov chains on metric spaces
	* 14.3 Markov chain Monte Carlo (MCMC)
		* 14.3.1 Introduction
		* 14.3.2 Metropolis-Hastings algorithm

#### 14.3.3 The Gibbs sampler

14.4 Problems
15 Stochastic Processes * 15.1 Continuous time Markov chains * 15.1.1 Definition * 15.1.2 Kolmogorov's differential equations * 15.1.3 Examples * 15.1.4 Limit theorems * 15.2 Brownian motion * 15.2.1 Construction of SBM * 15.2.2 Basic properties of SBM * 15.2.3 Some related processes * 15.2.4 Some limit theorems * 15.2.5 Some sample path properties of SBM * 15.2.6 Brownian motion and martingales * 15.2.7 Some applications * 15.2.8 The Black-Scholes formula for stock price option * 15.3 Problems
16 Limit Theorems for Dependent Processes * 16.1 A central limit theorem for martingales * 16.2 Mixing sequences * 16.2.1 Mixing coefficients * 16.2.2 Coupling and covariance inequalities * 16.3 Central limit theorems for mixing sequences * 16.4 Problems
17 The Bootstrap * 17.1 The bootstrap method for independent variables * 17.1.1 A description of the bootstrap method * 17.1.2 Validity of the bootstrap: Sample mean * 17.1.3 Second order correctness of the bootstrap * 17.1.4 Bootstrap for lattice distributions * 17.1.5 Bootstrap for heavy tailed random variables * 17.2 Inadequacy of resampling single values under dependence * 17.3 Block bootstrap * 17.4 Properties of the MBB * 17.4.1 Consistency of MBB variance estimators * 17.4.2 Consistency of MBB cdf estimators * 17.4.3 Second order properties of the MBB * 17.5 Problems
18 Branching Processes * 18.1 Bienyeme-Galton-Watson branching process * 18.2	* 18.2 BGW process: Multitype case
	* 18.3 Continuous time branching processes
	* 18.4 Embedding of Urn schemes in continuous time branching processes
	* 18.5 Problems
	* 18.4 Advanced Calculus: A Review
	* 18.1 Elementary set theory
		* 18.1.1 Set operations
		* 18.1.2 The principle of induction
		* 18.1.3 Equivalence relations
	* 18.2 Real numbers, continuity, differentiability, and integration
		* 18.2.1 Real numbers
		* 18.2.2 Sequences, series, limits, limsup, liminf
		* 18.2.3 Continuity and differentiability
		* 18.2.4 Riemann integration
	* 18.3 Complex numbers, exponential and trigonometric functions
	* 18.4 Metric spaces
		* 18.4.1 Basic definitions
		* 18.4.2 Continuous functions
		* 18.4.3 Compactness
		* 18.4.4 Sequences of functions and uniform convergence
	* 18.5 Problems
	* 18.2 List of Abbreviations and Symbols
	* 18.1 Abbreviations
	* 18.2 Symbols
Measures and Integration: An Informal Introduction

For many students who are learning measure and integration theory for the first time, the notions of a \(\sigma\)-algebra of subsets of a set \(\Omega\), countable additivity of a set function \(\lambda\), measurability of a function, the definition of an integral, and the interchange of limits and integration are not easy to understand and often seem not so intuitive. The goals of this informal introduction to this subject are (1) to show that the notions of \(\sigma\)-algebra and countable additivity are logical consequences of certain natural approximation procedures; (2) the dividends for the assumption of these two properties are great, and they lead to a nice and natural theory that is also very powerful for the handling of limits. Of course, as the saying goes, the devil is in the details. After this informal introduction, the necessary details are given in the next few sections. It is hoped that after this heuristic explanation of the subject, the motivation for and the process of mastering the details on the part of the students will be forthcoming.

**What is Measure Theory?**

A simple answer is that it is a theory about the distribution of mass over a set \(\mathbb{S}\). If the mass is uniformly distributed and \(\mathbb{S}\) is an Euclidean space \(\mathbb{R}^{k}\), it is the theory of Lebesgue measure on \(\mathbb{R}^{k}\) (i.e., length in \(\mathbb{R}\), area in \(\mathbb{R}^{2}\), volume in \(\mathbb{R}^{3}\), etc.). Probability theory is concerned with the case when \(\mathbb{S}\) is the sample space of a random experiment and the total mass is one. Consider the following example.

Imagine an open field \(\mathbb{S}\) and a snowy night. At daybreak one goes to the field to measure the amount of snow in as many of the subsets of \(\mathbb{S}\) as possible. Suppose now that one has the tools to measure the snow exactly on a class of subsets, such as triangles, rectangles, circular shapes, elliptic shapes, etc., no matter how small. It is natural to try to approximate oddly-shaped regions by combinations of these "standard shapes," and then use a limiting process to obtain a measure for the oddly-shaped regions and reach some limit for such sets. Let \(\mathcal{B}\) denote the class of subsets of \(\mathbb{S}\) whose measure is obtained this way and let \(\lambda(B)\) denote the amount of snow in each \(B\in\mathcal{B}\). Call \(\mathcal{B}\) the class of all (snow) measurable sets and \(\lambda(B)\) the measure (of snow) on \(B\) for each \(B\in\mathcal{B}\). It is reasonable to expect that the following properties of \(\mathcal{B}\) and \(\lambda(\cdot)\) hold:

**Properties of \(\mathcal{B}\)**

* \(A\in\mathcal{B}\Rightarrow A^{c}\in\mathcal{B}\) (i.e., if one can measure the amount of snow on \(A\) and knows the total amount on \(\mathbb{S}\), then one knows the amount of snow on \(A^{c}\)).
* \(A_{1},A_{2}\in\mathcal{B}\Rightarrow A_{1}\cup A_{2}\in\mathcal{B}\) (i.e., if one can measure the amount of snow on \(A_{1}\) and \(A_{2}\), then one can do the same for \(A_{1}\cup A_{2}\)).
* If \(\{A_{n}:n\geq 1\}\subset\mathcal{B}\), and \(A_{n}\subset A_{n+1}\) for all \(n\geq 1\), then \(\lim_{n\to\infty}A_{n}\equiv\bigcup_{n=1}^{\infty}A_{n}\in\mathcal{B}\) (i.e., if one can measure the amount of snow on \(A_{n}\) for each \(n\geq 1\) on an _increasing sequence_ of sets, then one can do so on the limit of \(A_{n}\)).
* \(\mathcal{C}\subset\mathcal{B}\) where \(\mathcal{C}\) is the class of nice sets such as triangles, squares, etc., that one started with.

**Properties of \(\lambda(\cdot)\)**

* \(\lambda(A)\geq 0\) for \(A\in\mathcal{B}\) (i.e., the amount of snow on any set is nonnegative!)
* If \(A_{1},A_{2}\in B,A_{1}\cap A_{2}=\emptyset,\lambda(A_{1}\cup A_{2})=\lambda(A _{1})+\lambda(A_{2})\) (i.e., the amounts of snow on two disjoint sets simply add up! This property of \(\lambda\) is referred to as _finite additivity_).
* If \(\{A_{n}:n\geq 1\}\subset\mathcal{B}\), are such that \(A_{n}\subset A_{n+1}\) for all \(n\), then \(\lambda(\lim_{n\to\infty}A_{n})=\lambda(\bigcup_{n=1}^{\infty}A_{n})=\lim_{n \to\infty}\lambda(A_{n})\) (i.e., if we can approximate a set \(A\) by an increase sequence of sets \(\{A_{n}\}_{n\geq 1}\) from \(\mathcal{B}\), then \(\lambda(A)=\lim_{n\to\infty}\lambda(A_{n})\). This property of \(\lambda\) is referred to as _monotone continuity from below_, or _m.c.f.b._ in short).

This last assumption (iii) is what guarantees that different approximations lead to consistent limits. Thus, if there are two increasing sequences \(\{A^{\prime}_{n}\}_{n\geq 1}\) and \(\{A^{{}^{\prime\prime}}_{n}\}_{n\geq 1}\) having the same limit \(A\) but \(\{\lambda(A^{\prime}_{n})\}_{n\geq 1}\) and \(\{\lambda(A^{{}^{\prime\prime}}_{n})\}_{n\geq 1}\) have different limits, then the approximating procedures are not consistent.

It turns out that the above set of reasonable and natural assumptions lead to a very rich and powerful theory that is widely applicable.

A triplet \((\mathbb{S},\mathcal{B},\lambda)\) that satisfies the above two sets of assumptions is called a _measure space_. The assumptions on \(\mathcal{B}\) and \(\lambda\) are _equivalent_ to the following:

**On \(\mathcal{B}\)**

\(\mathcal{B}\)(i)\({}^{\prime}\): \(\emptyset\), the empty set, lies in \(\mathcal{B}\)

\(\mathcal{B}\)(ii)\({}^{\prime}\): \(A\in\mathcal{B}\Rightarrow A^{c}\in\mathcal{B}\) (same as (i) before)

\(\mathcal{B}\)(iii)\({}^{\prime}\): \(A_{1},A_{2},\ldots\in\mathcal{B}\Rightarrow\cup_{i}A_{i}\in\mathcal{B}\) (combines (ii) and (iii) above) (_Closure under countable unions_).

**On \(\lambda\)**

\(\lambda\)(i)\({}^{\prime}\): \(\lambda(\cdot)\geq 0\) (same as (i) before) and \(\lambda(\emptyset)=0\).

\(\lambda\)(ii)\({}^{\prime}\): \(\lambda(\cup_{n\geq 1}A_{n})=\sum_{n=1}^{\infty}\lambda(A_{n})\) if \(\{A_{n}\}_{n\geq 1}\subset\mathcal{B}\) are _pairwise disjoint_, i.e., \(A_{i}\cap A_{j}=\emptyset\) for \(i\neq j\) (_Countable additivity_).

Any collection \(\mathcal{B}\) of subsets of \(\mathbb{S}\) that satisfies \(\mathcal{B}\)(i)\({}^{\prime}\), \(\mathcal{B}\)(ii)\({}^{\prime}\), \(\mathcal{B}\)(iii)\({}^{\prime}\) above is called a _\(\sigma\)-algebra_. Any set function \(\lambda\) on a \(\sigma\)-algebra \(\mathcal{B}\) that satisfies \(\lambda\)(i)\({}^{\prime}\) and \(\lambda\)(ii)\({}^{\prime}\) above is called a _measure_. Thus, a _measure space_ is a triplet \((\mathbb{S},\mathcal{B},\lambda)\) where \(\mathbb{S}\) is a nonempty set, \(\mathcal{B}\) is a _\(\sigma\)-algebra_ of subsets of \(\mathbb{S}\) and \(\lambda\) is a _measure_ on \(\mathcal{B}\). Notice that the \(\sigma\)-algebra structure on \(\mathcal{B}\) and the countable additivity of \(\lambda\) are necessary consequences of the very natural assumptions (i), (ii), and (iii) on \(\mathcal{B}\) and \(\lambda\) defined at the beginning.

It is not often the case that one is given \(\mathcal{B}\) and \(\lambda\) explicitly. Typically, one starts with a small collection \(\mathcal{C}\) of subsets of \(\mathbb{S}\) that have properties resembling intervals or rectangles and a set function \(\lambda\) on \(\mathcal{C}\). Then, \(\mathcal{B}\) is the smallest \(\sigma\)-algebra containing \(\mathcal{C}\) obtained from \(\mathcal{C}\) by various operations such as countable unions, intersections, and their limits. The key properties on \(\mathcal{C}\) that one needs are:

* \(A,B\in\mathcal{C}\Rightarrow A\cap B\in\mathcal{C}\) (e.g., intersection of intervals is an interval).
* \(A\in\mathcal{C}\Rightarrow A^{c}\) is a finite union of sets from \(\mathcal{C}\) (e.g., the complement of an interval is the union of two intervals or an interval itself).

A collection \(\mathcal{C}\) satisfying (i) and (ii) is called a _semialgebra_. The function \(\lambda\) on \(\mathcal{B}\) is an extension of \(\lambda\) on \(\mathcal{C}\). For this extension to be a measure on \(\mathcal{B}\), the conditions needed are

* \(\lambda(A)\geq 0\quad\text{for all}\quad A\in\mathcal{C}\)
* If \(A_{1},A_{2},\ldots\in\mathcal{C}\) are pairwise disjoint and \(A=\bigcup_{n\geq 1}A_{n}\in\mathcal{C}\), then \(\lambda(A)=\sum_{n=1}^{\infty}\lambda(A_{n})\).

There is a result, known as the _extension theorem_, that says that given such a pair \((\mathcal{C},\lambda)\), it is possible to extend \(\lambda\) to \(\mathcal{B}\), the smallest \(\sigma\)-algebra containing \(\mathcal{C}\), such that \((\mathbb{S},\mathcal{B},\lambda)\) is a measure space. Actually, it does more. It constructs a \(\sigma\)-algebra \(\mathcal{B}^{*}\) larger than \(\mathcal{B}\) and a measure \(\lambda^{*}\) on \(\mathcal{B}^{*}\) such that \((\mathbb{S},\mathcal{B}^{*},\lambda^{*})\) is a larger measure space, \(\lambda^{*}\) coincides with \(\lambda\) on \(\mathcal{C}\) and it provides nice approximation theorems. For example, the following approximation result is available:

If \(B\in\mathcal{B}^{*}\) with \(\lambda^{*}(B)<\infty\), then for every \(\epsilon>0\), \(B\) can be approximated by a finite union of sets from \(\mathcal{C}\), i.e., there exist sets \(A_{1},\ldots,A_{k}\in\mathcal{C}\) with \(k<\infty\) such that \(\lambda^{*}(A\triangle B)<\epsilon\) where \(A\equiv\bigcup_{i=1}^{k}A_{i}\) and \(A\triangle B=(A\cap B^{c})\cup(A^{c}\cap B)\), the _symmetric difference_ between \(A\) and \(B\).

That is, in principle, every (measurable) set \(B\) of finite measure (i.e., \(B\) belonging to \(\mathcal{B}^{*}\) with \(\lambda^{*}(B)<\infty\)) is nearly a finite union of (elementary) sets that belong to \(\mathcal{C}\). For example if \(\mathbb{S}=\mathbb{R}\) and \(\mathcal{C}\) is the class of intervals, then every measurable set of finite measure is nearly a finite union of disjoint bounded open intervals.

The following are some concrete examples of the above extension procedure.

**Theorem:** (_Lebesgue-Stieltjes measures on \(\mathbb{R}\)_). Let \(F:\mathbb{R}\to\mathbb{R}\) satisfy_

* \(x_{1}<x_{2}\Rightarrow F(x_{1})\leq F(x_{2})\) _(nondecreasing);_
* \(F(x)=F(x+)\equiv\lim_{y\downarrow x}F(y)\) _for all_ \(x\in\mathbb{R}\) _(i.e.,_ \(F(\cdot)\) _is right continuous)._

_Let \(\mathcal{C}\) be the class of sets of the form \((a,b]\), or \((b,\infty)\), \(-\infty\leq a<b<\infty\). Then, there exists a measure \(\mu_{F}\) defined on \(\mathcal{B}\equiv\mathcal{B}(\mathbb{R})\), the smallest \(\sigma\)-algebra generated by \(\mathcal{C}\) such that_

\[\mu_{F}((a,b])=F(b)-F(a)\quad\mbox{for all}\quad-\infty<a<b<\infty.\]

The \(\sigma\)-algebra \(\mathcal{B}\equiv\mathcal{B}(\mathbb{R})\) is called the _Borel \(\sigma\)-algebra of_ \(\mathbb{R}\).

**Corollary:** _There exists a measure \(m\) on \(\mathcal{B}(\mathbb{R})\) such that \(m(I)=\) the length of \(I\), for any interval \(I\)._

**Proof:** Take \(F(x)\equiv x\) in the above theorem.

This measure is called the _Lebesgue measure on \(\mathbb{R}\)_. \(\Box\)

**Corollary:** _There exists a measure \(\lambda\) on \(\mathcal{B}(\mathbb{R})\) such that_

\[\lambda((a,b])=\frac{1}{\sqrt{2\pi}}\int_{a}^{b}e^{-x^{2}/2}dx.\]

**Proof:** Take \(F(x)=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{-u^{2}/2}du\), \(x\in\mathbb{R}\).

This measure is called the _standard normal probability measure_ on \(\mathbb{R}\). \(\Box\)

**Theorem:** (_Lebesgue-Stieltjes measures on \(\mathbb{R}^{2}\)_). Let \(F:\mathbb{R}^{2}\to\mathbb{R}\) be a function satisfying the following:_

* \((\)_Monotonicity_\()\) _For_ \(x=(x_{1},x_{2})^{\prime},y=(y_{1},y_{2})^{\prime}\) _with_ \(x_{i}\leq y_{i}\) _for_ \(i=1,2\)_,_ \(\big{(}\Delta F\big{)}(x,y)\equiv F(y_{1},y_{2})-F(x_{1},y_{2})-F(y_{1},x_{2})+ F(x_{1},x_{2})\geq 0\)_._
* \((\)_Continuity from above_\()\)__\(F(x)=\lim\limits_{y_{i}\downarrow x_{i},i=1,2}F(y)\) _for all_ \(x\in\mathbb{R}^{2}\)_._

_Let \(\mathcal{C}\) be the class of all rectangles of the form \((a,b]\equiv(a_{1},b_{1}]\times(a_{2},b_{2}]\) with \(a=(a_{1},a_{2})^{\prime},b=(b_{1},b_{2})^{\prime}\in\mathbb{R}^{2}\). Then there exists a measure \(\mu_{F}\), defined on the \(\sigma\)-algebra \(\mathcal{B}\equiv\mathcal{B}(\mathbb{R}^{2})\), generated by \(\mathcal{C}\), such that_

\[\mu_{F}((a,b])=\Big{(}\Delta F\Big{)}(a,b).\]

The above theorems have a converse that says that every measure on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\) that is finite on bounded sets arises from some function \(F\) (called a _distribution function_) and is, therefore, a _Lebesgue-Stieltjes measure_.

Here is another simple example of a measure space (with discrete \(\mathbb{S}\)).

**Example:** Let \(\mathbb{S}=\{s_{1},s_{2},\ldots,s_{k}\},k\leq\infty\), and let \(\mathcal{B}=\mathcal{P}(\mathbb{S})\), the power set of \(\mathbb{S}\), i.e., the collection of all possible subsets of \(\mathbb{S}\). Let \(p_{1},p_{2},\ldots\) be nonnegative numbers. Let

\[\lambda(A)\equiv\sum_{1\leq i\leq k}p_{i}I_{A}(s_{i}),\]

where \(I_{A}\) is the indicator function of the set \(A\), defined by \(I_{A}(s)=1\) if \(s\in A\) and \(0\) otherwise. It is easy to verify that \((\mathbb{S},\mathcal{B},\lambda)\) is a measure space and also that every measure \(\lambda\) on \(\mathcal{B}\) arises this way.

**What is Integration Theory?**

In short, it is a theory about weighted sums of functions on a set \(\mathbb{S}\) when the weights are specified by a mass distribution \(\lambda\). Here is a more detailed answer.

Let \((\mathbb{S},\mathcal{B},\lambda)\) be a measure space. Suppose \(f:\mathbb{S}\to\mathbb{R}\) is a _simple function_, i.e., \(f\) is such that \(f(\mathbb{S})\) is a finite set \(\{a_{1},a_{2},\ldots,a_{k}\}\). It is reasonable to define the weighted sum of \(f\) with respect to \(\lambda\) as \(\sum_{i=1}^{k}a_{i}\lambda(A_{i})\) where \(A_{i}=f^{-1}\{a_{i}\}\). Of course, for this to be well defined, one needs \(A_{i}\) to be in \(\mathcal{B}\) and \(\lambda(A_{i})<\infty\) for all \(i\) such that \(a_{i}\neq 0\).

Notice that the quantity \(\sum_{i=1}^{k}a_{i}\lambda(A_{i})\) remains the same whether the \(a_{i}\)'s are distinct or not. Call this the integral of \(f\) with respect to \(\lambda\) and denote this by \(\int fd\lambda\). If \(f\) and \(g\) are simple, then for \(\alpha,\beta\in\mathbb{R}\), \(\int(\alpha f+\beta g)d\lambda=\alpha\int fdx+\beta\int gd\lambda\). Now how should one define \(\int fd\lambda\) (integral of \(f\) with respect to \(\lambda\)) for a nonsimple \(f\)? The answer, of course, is to "approximate" by simple functions. Let \(f\) be a nonnegative function. To define the integral of \(f\), one would like to approximate \(f\) by _simple_ functions. It turns out that a necessary and sufficient condition for this is that for any \(a\in\mathbb{R}\), the set \(\{s:f(s)\leq a\}\) is in \(\mathcal{B}\). Such a function \(f\) is called _measurable_ with respect to \(\mathcal{B}\) or \(\mathcal{B}\)-_measurable_ or simply, _measurable_ (if \(\mathcal{B}\) is kept fixed throughout). Let \(f\) be a nonnegative \(\mathcal{B}\) measurable function. Then there exists a sequence \(\{f_{n}\}_{n\geq 1}\) of _simple nonnegative_ functions such that for each \(s\in\mathbb{S}\), \(\{f_{n}(s)\}_{n\geq 1}\) is a nondecreasing sequence converging to \(f(s)\). It is now natural to define the weighted sum of \(f\) with respect to \(\lambda\), i.e., the _integral of \(f\) with respect to \(\lambda\)_, denoted by \(\int fd\lambda\), as

\[\int fd\lambda=\lim_{n\to\infty}\int f_{n}d\lambda.\]

An immediate question is: _Is the right side the same for all such approximating sequences \(\{f_{n}\}_{n\geq 1}\)?_ The answer is a _yes_; it is guaranteed by the very natural assumption imposed on \(\lambda\) that it is finitely additive and monotone continuous from below, i.e. \(\lambda\)(ii) and \(\lambda\)(iii) (or equivalently, that \(\lambda\) is countably additive, i.e., \(\lambda\)(ii)\({}^{\prime}\)).

One can strengthen this to a stronger result known as the _monotone convergence theorem_, a key result that in turn leads to two other major convergence results.

**The monotone convergence theorem (MCT):** _Let \((\mathbb{S},\mathcal{B},\lambda)\) be a measure space and let \(f_{n}:\mathbb{S}\to\mathbb{R}_{+}\), \(n\geq 1\) be a sequence of nonnegative \(\mathcal{B}\)-measurable functions (not necessarily simple) such that for all \(s\in\mathbb{S}\),_

* \(f_{n}(s)\leq f_{n+1}(s),\quad\mbox{for all}\quad n\geq 1,\quad\mbox{and}\)__
* \(\lim_{n\to\infty}f_{n}(s)=f(s)\)_._

_Then \(f\) is \(\mathcal{B}\)-measurable and \(\int fd\lambda=\lim_{n\to\infty}\int f_{n}d\lambda\)._

This says that the integral and the limit can be interchanged for monotone nondecreasing nonnegative \(\mathcal{B}\)-measurable functions. Note that if \(f_{n}=I_{A_{n}}\), the indicator function of a set \(A_{n}\) and if \(A_{n}\subset A_{n+1}\) for each \(n\), then the MCT is the same as m.c.f.b. (cf. property \(\lambda\)(ii)). _Thus, the very natural assumption of m.c.f.b. yields a basic convergence result that makes the integration theory so elegant and powerful._

To extend the definition of \(\int fd\lambda\) to a real valued, \(\mathcal{B}\)-measurable function \(f:\mathbb{S}\to\mathbb{R}\), one uses the simple idea that \(f\) can be decomposed as \(f=f^{+}-f^{-}\) where \(f^{+}(s)=\max\{f(s),0\}\) and \(f^{-}(s)=\max\{-f(s),0\}\), \(s\in\mathbb{S}\). Since both \(f^{+}\) and \(f^{-}\) are nonnegative and \(\mathcal{B}\)-measurable, \(\int f^{+}d\lambda\) and \(\int f^{-}d\lambda\) are both well defined. Now set

\[\int fd\lambda=\int f^{+}d\lambda-\int f^{-}d\lambda,\]

provided at least one of the two terms on the right is finite. The function \(f\) is said to be _integrable_ with respect to (w.r.t.) \(\lambda\) if both \(\int f^{+}d\lambda\) and \(f^{-}d\lambda\) are finite or, equivalently, if \(\int|f|d\lambda<\infty\). The following is a consequence of the MCT.

**Fatou's lemma:** _Let \(\{f_{n}\}_{n\geq 1}\) be a sequence of nonnegative \(\mathcal{B}\)-measurable functions on a measure space \((\mathbb{S},\mathcal{B},\lambda)\). Then_

\[\int\liminf_{n\to\infty}f_{n}d\lambda\leq\liminf_{n\to\infty}\int f_{n}d\lambda.\]

This in turn leads to

**(Lebesgue's) dominated convergence theorem (DCT):** _Let \(\{f_{n}\}_{n\geq 1}\) be a sequence of \(\mathcal{B}\)-measurable functions from a measure space \((\mathbb{S},\mathcal{B},\lambda)\) to \(\mathbb{R}\) and let \(g\) be a \(\mathcal{B}\)-measurable nonnegative integrable function on \((\mathbb{S},\mathcal{B},\lambda)\). Suppose that for each \(s\) in \(\mathbb{S}\),_

* \(|f_{n}(s)|\leq g(s)\quad\mbox{for all}\quad n\geq 1\quad\mbox{ and}\)__
* \(\lim_{n\to\infty}f_{n}(s)=f(s)\)_._

_Then, \(f\) is integrable and_

\[\lim_{n\to\infty}\int f_{n}d\lambda=\int fd\lambda=\int\lim_{n\to\infty}f_{n}d\lambda.\]

Thus some very natural assumptions on \(\mathcal{B}\) and \(\lambda\) lead to an interesting measure and integration theory that is quite general and that allows the interchange of limits and integrals under fairly general conditions. A systematic treatment of the measure and integration theory is given in the next five chapters.

## Chapter 1 Measures

Section 1.1 deals with algebraic operations on subsets of a given nonempty set \(\Omega\). Section 1.2 treats nonnegative set functions on classes of sets and defines the notion of a measure on an algebra. Section 1.3 treats the extension theorem, and Section 1.4 deals with completeness of measures.

### 1.1 Classes of sets

Let \(\Omega\) be a nonempty set and \(\mathcal{P}(\Omega)\equiv\{A:A\subset\Omega\}\) be the _power set of \(\Omega\)_, i.e., the class of all subsets of \(\Omega\).

**Definition 1.1.1**: **:**__A collection of sets \(\mathcal{F}\subset\mathcal{P}(\Omega)\) is called an _algebra_ if (a) \(\Omega\in\mathcal{F}\), (b) \(A\in\mathcal{F}\) implies \(A^{c}\in\mathcal{F}\), and (c) \(A,B\in\mathcal{F}\) implies \(A\cup B\in\mathcal{F}\) (i.e., closure under pairwise unions).

Thus, an algebra is a class of sets containing \(\Omega\) that is closed under complementation and pairwise (and hence finite) unions. It is easy to see that one can equivalently define an algebra by requiring that properties (a), (b) hold and that the property

\[\left(\mathrm{c}\right)^{\prime}\quad A,B\in\mathcal{F}\Rightarrow A\cap B\in \mathcal{F}\]

holds (i.e. closure under finite intersections).

**Definition 1.1.2:** A class \({\cal F}\subset{\cal P}(\Omega)\) is called a \(\sigma\)-_algebra_ if it is an algebra and if it satisfies

\[(d)\quad A_{n}\in{\cal F}\quad\mbox{for}\quad n\geq 1\Rightarrow\bigcup_{n\geq 1 }A_{n}\in{\cal F}.\]

Thus, a \(\sigma\)-algebra is a class of subsets of \(\Omega\) that contains \(\Omega\) and is closed under complementation and countable unions. As pointed out in the introductory chapter, a \(\sigma\)-algebra can be alternatively defined as an algebra that is closed under monotone unions as the following shows.

**Proposition 1.1.1:** _Let \({\cal F}\subset{\cal P}(\Omega)\). Then \({\cal F}\) is a \(\sigma\)-algebra if and only if \({\cal F}\) is an algebra and satisfies_

\[A_{n}\in{\cal F},A_{n}\subset A_{n+1}\quad\mbox{for all}\quad n\Rightarrow \bigcup_{n\geq 1}A_{n}\in{\cal F}.\]

**Proof:** The 'only if' part is obvious. For the 'if' part, let \(\{B_{n}\}_{n=1}^{\infty}\subset{\cal F}\). Then, since \({\cal F}\) is an algebra, \(A_{n}\equiv\bigcup_{j=1}^{n}B_{j}\in{\cal F}\) for all \(n\). Further, \(A_{n}\subset A_{n+1}\) for all \(n\) and \(\bigcup_{n\geq 1}B_{n}=\bigcup_{n\geq 1}A_{n}\). Since by hypothesis \(\cup_{n}A_{n}\in{\cal F}\), \(\cup_{n}B_{n}\in{\cal F}\). \(\Box\)

Here are some examples of algebras and \(\sigma\)-algebras.

**Example 1.1.1:** Let \(\Omega=\{a,b,c,d\}\). Consider the classes

\[{\cal F}_{1}=\{\Omega,\emptyset,\{a\}\}\]

and

\[{\cal F}_{2}=\{\Omega,\emptyset,\{a\},\{b,c,d\}\}.\]

Then, \({\cal F}_{2}\) is an algebra (and also a \(\sigma\)-algebra), but \({\cal F}_{1}\) is not an algebra, since \(\{a\}^{c}\not\in{\cal F}_{1}\).

**Example 1.1.2:** Let \(\Omega\) be any nonempty set and let

\[{\cal F}_{3}={\cal P}(\Omega)\equiv\{A:A\subset\Omega\},\quad\mbox{the power set of}\quad\Omega\]

and

\[{\cal F}_{4}=\{\Omega,\emptyset\}.\]

Then, it is easy to check that both \({\cal F}_{3}\) and \({\cal F}_{4}\) are \(\sigma\)-algebras. The latter \(\sigma\)-algebra is often called the _trivial \(\sigma\)-algebra_ on \(\Omega\) (Problem 1.1).

From the definition it is clear that any \(\sigma\)-algebra is also an algebra and thus \({\cal F}_{2},{\cal F}_{3},{\cal F}_{4}\) are examples of algebras, too. The following is an example of an algebra that is not a \(\sigma\)-algebra.

**Example 1.1.3:** Let \(\Omega\) be a nonempty set, and let \(|A|\) denote the number of elements of a set \(A\subset\Omega\). Define.

\[{\cal F}_{5}=\{A\subset\Omega:\mbox{ either }|A|\mbox{ is finite or }|A^{c}|\mbox{ is finite}\}.\]Then, note that (i) \(\Omega\in{\cal F}_{5}\) (since \(|\Omega^{c}|=|\emptyset|=0\))), (ii) \(A\in{\cal F}_{5}\) implies \(A^{c}\in{\cal F}_{5}\) (if \(|A|<\infty\), then \(|(A^{c})^{c}|=|A|<\infty\) and if \(|A^{c}|<\infty\), then \(A^{c}\in{\cal F}_{5}\) trivially). Next, suppose that \(A,B\in{\cal F}_{5}\). If either \(|A|<\infty\) or \(|B|<\infty\), then

\[|A\cap B|\leq\min\{|A|,|B|\}<\infty,\]

so that \(A\cap B\in{\cal F}_{5}\). On the other hand, if both \(|A^{c}|<\infty\) and \(|B^{c}|<\infty\), then

\[|(A\cap B)^{c}|=|A^{c}\cup B^{c}|\leq|A^{c}|+|B^{c}|<\infty,\]

implying that \(A\cap B\in{\cal F}_{5}\). Thus, property (c)\({}^{\prime}\) holds, and \({\cal F}_{5}\) is an algebra. However, if \(|\Omega|=\infty\), then \({\cal F}_{5}\) is not a \(\sigma\)-algebra. To see this, suppose that \(|\Omega|=\infty\) and \(\{\omega_{1},\omega_{2},\ldots\}\subset\Omega\). Then, by definition, \(A_{i}=\{\omega_{i}\}\in{\cal F}_{5}\) for all \(i\geq 1\), but \(A\equiv\bigcup_{i=1}^{\infty}A_{2i-1}=\{\omega_{1},\omega_{3},\ldots\}\not\in{ \cal F}_{5}\), since \(|A|=|A^{c}|=\infty\).

**Example 1.1.4:** Let \(\Omega\) be a nonempty set and let

\[{\cal F}_{6}=\{A\subset\Omega:A\mbox{ is countable or }A^{c}\mbox{ is countable}\}.\]

Then, it is easy to show that \({\cal F}_{6}\) is a \(\sigma\)-algebra (Problem 1.3).

Suppose \(\{{\cal F}_{\theta}:\theta\in\Theta\}\) is a family of \(\sigma\)-algebras on \(\Omega\). From the definition, it follows that the intersection \(\bigcap_{\theta\in\Theta}{\cal F}_{\theta}\) is a \(\sigma\)-algebra, no matter how large the index set \(\Theta\) is (Problem 1.4). However, the union of two \(\sigma\)-algebras may not even be an algebra (Problem 1.5). For the development of measure theory and probability theory, the concept of a \(\sigma\)-algebra plays a crucial role. In many instances, given an arbitrary collection of subsets of \(\Omega\), one would like to extend it to a possibly larger class that is a \(\sigma\)-algebra. This leads to the following definition.

**Definition 1.1.3:** If \({\cal A}\) is a class of subsets of \(\Omega\), then the \(\sigma\)-algebra generated by \({\cal A}\), denoted by \(\sigma\langle{\cal A}\rangle\), is defined as

\[\sigma\langle{\cal A}\rangle=\bigcap_{{\cal F}\in{\cal I}({\cal A})}{\cal F},\]

where \({\cal I}({\cal A})\equiv\{{\cal F}:{\cal A}\subset{\cal F}\) and \({\cal F}\) is a \(\sigma\)-algebra on \(\Omega\}\) is the collection of all \(\sigma\)-algebras containing the class \({\cal A}\).

Note that since the power set \({\cal P}(\Omega)\) contains \({\cal A}\) and is itself a \(\sigma\)-algebra, the collection \({\cal I}({\cal A})\) is not empty and hence, the intersection in the above definition is well defined.

**Example 1.1.5:** In the setup of Example 1.1.1, \(\sigma\langle{\cal F}_{1}\rangle={\cal F}_{2}\) (why?).

A particularly useful class of \(\sigma\)-algebras are those generated by open sets of a topological space. These are called Borel \(\sigma\)-algebras. A _topological space_ is a pair \(({\mathbb{S}},{\cal T})\) where \({\mathbb{S}}\) is a nonempty set and \({\cal T}\) is a collection of subsets of \({\mathbb{S}}\) such that (i) \({\mathbb{S}}\in{\cal T}\), (ii) \({\cal O}_{1},{\cal O}_{2}\in{\cal T}\Rightarrow{\cal O}_{1}\cap{\cal O}_{2}\in {\cal T}\), and (iii) \(\{{\cal O}_{\alpha}:\alpha\in I\}\subset{\cal T}\Rightarrow\bigcup_{\alpha\in I }{\cal O}_{\alpha}\in{\cal T}\). Elements of \({\cal T}\) are called _open sets_.

A _metric space_ is a pair \((\mathbb{S},d)\) where \(\mathbb{S}\) is a nonempty set and \(d\) is a function from \(\mathbb{S}\times\mathbb{S}\) to \(\mathbb{R}^{+}\) satisfying (i) \(d(x,y)=d(y,x)\) for all \(x,y\) in \(\mathbb{S}\), (ii) \(d(x,y)=0\) iff \(x=y\), and (iii) \(d(x,z)\leq d(x,y)+d(y,z)\) for all \(x,y,z\) in \(\mathbb{S}\). Property (iii) is called the triangle inequality. The function \(d\) is called a metric on \(\mathbb{S}\) (cf. see A.4).

Any Euclidean space \(\mathbb{R}^{n}(1\leq n<\infty)\) is a metric space under any one of the following metrics:

* For \(1\leq p<\infty\), \(d_{p}(x,y)=\Big{(}\sum\limits_{i=1}^{n}|x_{i}-y_{i}|^{p}\Big{)}^{1/p}\).
* \(d_{\infty}(x,y)=\max\limits_{1\leq i\leq n}|x_{i}-y_{i}|\).
* For \(0<p<1\), \(d_{p}(x,y)=\Big{(}\sum\limits_{i=1}^{n}|x_{i}-y_{i}|^{p}\Big{)}\).

A metric space \((\mathbb{S},d)\) is a topological space where a set \(\mathcal{O}\) is open if for all \(x\in\mathcal{O}\), there is an \(\epsilon>0\) such that \(B(x,\epsilon)\equiv\{y:d(y,x)<\epsilon\}\subset\mathcal{O}\).

**Definition 1.1.4:** The _Borel \(\sigma\)-algebra_ on a topological space \(\mathbb{S}\) (in particular, on a metric space or an Euclidean space) is defined as the \(\sigma\)-algebra generated by the collection of open sets in \(\mathbb{S}\).

**Example 1.1.6:** Let \(\mathcal{B}(\mathbb{R}^{k})\) denote the Borel \(\sigma\)-algebra on \(\mathbb{R}^{k}\), \(1\leq k<\infty\). Then,

\[\mathcal{B}(\mathbb{R}^{k})\equiv\sigma\langle\{A:A\mbox{ is an open subset of }\mathbb{R}^{k}\}\rangle\]

is also generated by each of the following classes of sets

\[\mathcal{O}_{1} = \{(a_{1},b_{1})\times\ldots\times(a_{k},b_{k}):-\infty\leq a_{i} <b_{i}\leq\infty,1\leq i\leq k\};\] \[\mathcal{O}_{2} = \{(-\infty,x_{1})\times\cdots\times(-\infty,x_{k}):x_{1},\ldots,x _{k}\in\mathbb{R}\};\] \[\mathcal{O}_{3} = \{(a_{1},b_{1})\times\ldots\times(a_{k},b_{k}):a_{i},b_{i}\in \mathcal{Q},a_{i}<b_{i},1\leq i\leq k\};\] \[\mathcal{O}_{4} = \{(-\infty,x_{1})\times\ldots\times(-\infty,x_{k}):x_{1},\ldots,x _{k}\in\mathcal{Q}\},\]

where \(\mathcal{Q}\) denotes the set of all rational numbers.

To show this, note that \(\sigma\langle\mathcal{O}_{i}\rangle\ \subset\mathcal{B}(\mathbb{R}^{k})\) for \(i=1,2,3,4\), and hence, it is enough to show that \(\sigma\langle\mathcal{O}_{i}\rangle\supset\mathcal{B}(\mathbb{R}^{k})\). Let \(\mathcal{G}\) be a \(\sigma\)-algebra containing \(\mathcal{O}_{3}\). Observe that given any open set \(A\subset\mathbb{R}^{k}\), there exist a sequence of sets \(\{B_{n}\}_{n\geq 1}\) in \(\mathcal{O}_{3}\) such that \(A=\bigcup_{n\geq 1}B_{n}\) (Problem 1.9). Since \(\mathcal{G}\) is a \(\sigma\)-algebra and \(B_{n}\in\mathcal{G}\) for all \(n\geq 1\), \(A\in\mathcal{G}\). Thus, \(\mathcal{G}\) is a \(\sigma\)-algebra containing all open subsets of \(\mathbb{R}^{k}\), and hence \(\mathcal{G}\supset\mathcal{B}(\mathbb{R}^{k})\). Hence, it follows that

\[\mathcal{B}(\mathbb{R}^{k})\supset\sigma\langle\mathcal{O}_{1}\rangle\ \supset\sigma\langle\mathcal{O}_{3}\rangle\ =\ \bigcap_{\mathcal{G}:\mathcal{G}\supset\mathcal{O}_{3}}\ \mathcal{G}\ \supset\mathcal{B}(\mathbb{R}^{k}).\]

[MISSING_PAGE_EMPTY:29]

Next, let \(\lambda_{2}({\cal C})\equiv\{A:A\in\lambda\langle{\cal C}\rangle,A\cap B\in\lambda \langle{\cal C}\rangle\) for all \(B\in\lambda\langle{\cal C}\rangle\}\). Then \(\lambda_{2}({\cal C})\) is a \(\lambda\)-system and by the previous step \({\cal C}\subset\lambda_{2}({\cal C})\subset\lambda\langle{\cal C}\rangle\). Hence, it follows that \(\lambda_{2}({\cal C})=\lambda\langle{\cal C}\rangle\), i.e., \(\lambda\langle{\cal C}\rangle\) is closed under intersection. This completes the proof of the theorem. \(\Box\)

**Corollary 1.1.3:**_If \({\cal C}\) is a \(\pi\)-system and \({\cal L}\) is a \(\lambda\)-system containing \({\cal C}\), then \({\cal L}\supset\sigma\langle{\cal C}\rangle\)._

**Remark 1.1.1:** There are several equivalent definitions of \(\lambda\)-systems; see, for example, Billingsley (1995). A closely related concept is that of a monotone class; see, for example, Chung (1974).

### 2 Measures

A _set function_ is an extended real valued function defined on a class of subsets of a set \(\Omega\). Measures are nonnegative set functions that, intuitively speaking, measure the content of a subset of \(\Omega\). As explained in Section 2 of the introductory chapter, a measure has to satisfy certain natural requirements, such as the measure of the union of a countable collection of _disjoint_ sets is the _sum_ of the measures of the individual sets. Formally, one has the following definition.

**Definition 1.2.1:** Let \(\Omega\) be a nonempty set and \({\cal F}\) be an algebra on \(\Omega\). Then, a set function \(\mu\) on \({\cal F}\) is called a _measure_ if

* \(\mu(A)\in[0,\infty]\) for all \(A\in{\cal F}\);
* \(\mu(\emptyset)=0\);
* for any disjoint collection of sets \(A_{1},A_{2},\ldots,\in{\cal F}\) with \(\bigcup_{n\geq 1}A_{n}\in{\cal F}\), \[\mu\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)}=\sum_{n=1}^{\infty}\mu(A_{n}).\]

As discussed in Section 2 of the introductory chapter, these conditions on \(\mu\) are equivalent to finite additivity and monotone continuity from below.

**Proposition 1.2.1:**_Let \(\Omega\) be a nonempty set and \({\cal F}\) be an algebra of subsets of \(\Omega\) and \(\mu\) be a set function on \({\cal F}\) with values in \([0,\infty]\) and with \(\mu(\emptyset)=0\). Then, \(\mu\) is a measure iff \(\mu\) satisfies_

* \((iii)^{\prime}_{a}:\)__\((\)_finite additivity_\()\) _for all_ \(A_{1},A_{2}\in{\cal F}\) _with_ \(A_{1}\cap A_{2}=\emptyset\)_,_ \(\mu(A_{1}\cup A_{2})=\mu(A_{1})+\mu(A_{2}),\) _and_
* \((iii)^{\prime}_{b}:\)__\((\)_monotone continuity from below or_, _m.c.f.b._, _in short_\()\) _for any collection_ \(\{A_{n}\}_{n\geq 1}\) _of sets in_ \({\cal F}\) _such that_ \(A_{n}\subset A_{n+1}\) _for all_ \(n\geq 1\)_and_ \(\bigcup_{n\geq 1}A_{n}\in{\cal F}\)_,_

\[\mu\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)}=\lim_{n\to\infty}\mu(A_{n}).\]

**Proof:** Let \(\mu\) be a measure on \({\cal F}\). Since \(\mu\) satisfies (iii), taking \(A_{3},A_{4},\ldots\) to be \(\emptyset\) yields \(({\rm iii})^{\prime}_{a}\). This implies that for \(A\) and \(B\) in \({\cal F}\), \(A\subset B\Rightarrow\mu(B)=\mu(A)+\mu(B\setminus A)\geq\mu(A)\), i.e., \(\mu\) is _monotone_. To establish \(({\rm iii})^{\prime}_{b}\), note that if \(\mu(A_{n})=\infty\) for some \(n=n_{0}\), then \(\mu(A_{n})=\infty\) for all \(n\geq n_{0}\) and \(\mu(\bigcup_{n\geq 1}A_{n})=\infty\) and \(({\rm iii})^{\prime}_{b}\) holds in this case. Hence, suppose that \(\mu(A_{n})<\infty\) for all \(n\geq 1\). Setting \(B_{n}=A_{n}\setminus A_{n-1}\) for \(n\geq 1\) (with \(A_{0}=\emptyset\)), by \(({\rm iii})^{\prime}_{a}\), \(\mu(B_{n})=\mu(A_{n})-\mu(A_{n-1})\). Since \(\{B_{n}\}_{n\geq 1}\) is a _disjoint_ collection of sets in \({\cal F}\) with \(\bigcup_{n\geq 1}B_{n}=\bigcup_{n\geq 1}A_{n}\), by (iii)

\[\mu\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)} = \mu\Big{(}\bigcup_{n\geq 1}B_{n}\Big{)}=\sum_{n=1}^{\infty}\mu(B _{n})=\lim_{N\to\infty}\sum_{n=1}^{N}\left[\mu(A_{n})-\mu(A_{n-1})\right]\] \[= \lim_{N\to\infty}\mu(A_{N}),\]

and so \(({\rm iii})^{\prime}_{b}\) holds also in this case.

Conversely, let \(\mu\) satisfy \(\mu(\emptyset)=0\) and \(({\rm iii})^{\prime}_{a}\) and \(({\rm iii})^{\prime}_{b}\). Let \(\{A_{n}\}_{n\geq 1}\) be a disjoint collection of sets in \({\cal F}\) with \(\bigcup_{i\geq 1}A_{i}\in{\cal F}\). Let \(C_{n}=\bigcup_{j=1}^{n}A_{j}\) for \(n\geq 1\). Since \({\cal F}\) is an algebra, \(C_{n}\in{\cal F}\) for all \(n\geq 1\). Also, \(C_{n}\subset C_{n+1}\) for all \(n\geq 1\). Hence, \(\bigcup_{n\geq 1}C_{n}=\bigcup_{j\geq 1}A_{j}\). By \(({\rm iii})^{\prime}_{b}\),

\[\mu\Big{(}\bigcup_{j\geq 1}A_{j}\Big{)} = \mu\Big{(}\bigcup_{n\geq 1}C_{n}\Big{)}=\lim_{n\to\infty}\mu(C_{n})\] \[= \lim_{n\to\infty}\sum_{j=1}^{n}\mu(A_{j})\quad({\rm by\ ({\rm iii })^{\prime}_{a}})\] \[= \sum_{j=1}^{\infty}\mu(A_{j}).\]

Thus, (iii) holds. \(\Box\)

**Remark 1.2.1:** The definition of a measure given in Definition 1.2.1 is valid when \({\cal F}\) is a \(\sigma\)-algebra. However, very often one may start with a measure on an algebra \({\cal A}\) and then extend it to a measure on the \(\sigma\)-algebra \(\sigma\langle{\cal A}\rangle\). This is why the definition of a measure on an algebra is given here. In the same vein, one may begin with a definition of a measure on a class of subsets of \(\Omega\) that 

**Definition 1.2.2:** A measure \(\mu\) is called _finite_ or _infinite_ according as \(\mu(\Omega)<\infty\) or \(\mu(\Omega)=\infty\). A finite measure with \(\mu(\Omega)=1\) is called a _probability_ measure. A measure \(\mu\) on a \(\sigma\)-algebra \(\mathcal{F}\) is called \(\sigma\)_-finite_ if there exist a countable collection of sets \(A_{1},A_{2},\ldots,\in\mathcal{F}\), not necessarily disjoint, such that

\[\mbox{(a) }\bigcup_{n\geq 1}A_{n}=\Omega\quad\mbox{and}\quad\mbox{(b) }\mu(A_{n})<\infty\quad\mbox{for all}\quad n\geq 1.\]

Here are some examples of measures.

**Example 1.2.1:** (_The counting measure_). Let \(\Omega\) be a nonempty set and \(\mathcal{F}_{3}=\mathcal{P}(\Omega)\) be the set of all subsets of \(\Omega\) (cf. Example 1.1.2). Define

\[\mu(A)=|A|,\quad A\in\mathcal{F}_{3},\]

where \(|A|\) denotes the number of elements in \(A\). It is easy to check that \(\mu\) satisfies the requirements (a)-(c) of a measure. This measure \(\mu\) is called the _counting measure_ on \(\Omega\). Note that \(\mu\) is finite iff \(\Omega\) is finite and it is \(\sigma\)-finite if \(\Omega\) is countably infinite.

**Example 1.2.2:** (_Discrete probability measures_). Let \(\omega_{1},\omega_{2},\ldots,\in\Omega\) and \(p_{1},p_{2},\ldots\in[0,1]\) be such that \(\sum_{i=1}^{\infty}p_{i}=1\). Define for any \(A\subset\Omega\)

\[P(A)=\sum_{i=1}^{\infty}p_{i}I_{A}(\omega_{i}),\]

where \(I_{A}(\cdot)\) denotes the indicator function of a set \(A\), defined by \(I_{A}(\omega)=0\) or \(1\) according as \(\omega\not\in A\) or \(\omega\in A\). For any disjoint collection of sets \(A_{1},A_{2},\ldots\in\mathcal{P}(\Omega)\),

\[P\bigg{(}\bigcup_{i=1}^{\infty}A_{i}\bigg{)} = \sum_{j=1}^{\infty}p_{j}I_{\bigcup_{i=1}^{\infty}A_{i}}(\omega_{j})\] \[= \sum_{j=1}^{\infty}p_{j}\bigg{(}\sum_{i=1}^{\infty}I_{A_{i}}( \omega_{j})\bigg{)}\] \[= \sum_{i=1}^{\infty}\bigg{(}\sum_{j=1}^{\infty}p_{j}I_{A_{i}}( \omega_{j})\bigg{)}\] \[= \sum_{i=1}^{\infty}P(A_{i}),\]

where interchanging the order of summation is permissible since the summands are nonnegative. This shows that \(P\) is a probability measure on \(\mathcal{P}(\Omega)\).

**Example 1.2.3:** (_Lebesgue-Stieltjes measures on \(\mathbb{R}\)_). As mentioned in the previous chapter (cf. Section 2), a large class of measures on the Borel \(\sigma\)-algebra \(\mathcal{B}(\mathbb{R})\) of subsets of \(\mathbb{R}\), known as the _Lebesgue-Stieltjes measures_, arise from nondecreasing right continuous functions \(F:\mathbb{R}\to\mathbb{R}\). For each such \(F\), the corresponding measure \(\mu_{F}\) satisfies \(\mu_{F}((a,b])=F(b)-F(a)\) for all \(-\infty<a<b<\infty\). The construction of these \(\mu_{F}\)'s via the extension theorem will be discussed in the next section. Also, note that if \(A_{n}=(-n,n)\), \(n=1,2,\ldots\), then \(\mathbb{R}=\bigcup_{n\geq 1}A_{n}\) and \(\mu_{F}(A_{n})<\infty\) for each \(n\geq 1\) (such measures are called Radon measures) and thus, \(\mu_{F}\) is necessarily \(\sigma\)-finite.

**Proposition 1.2.2:** _Let \(\mu\) be a measure on an algebra \(\mathcal{F}\), and let \(A,B,A_{1},\ldots,A_{k}\in\mathcal{F},1\leq k<\infty\). Then,_

* \((\)_Monotonicity_\()\)__\(\mu(A)\leq\mu(B)\) _if_ \(A\subset B;\)__
* \((\)_Finite subadditivity_\()\)__\(\mu(A_{1}\cup\ldots\cup A_{k})\leq\mu(A_{1})+\ldots+\mu(A_{k});\)__
* \((\)_Inclusion-exclusion formula_\()\) _If_ \(\mu(A_{i})<\infty\) _for all_ \(i=1,\ldots,k\)_, then_ \[\mu(A_{1}\cup\ldots\cup A_{k}) = \sum_{i=1}^{k}\mu(A_{i})-\sum_{1\leq i<j<k}\mu(A_{i}\cap A_{j})\] \[+\ldots+(-1)^{k-1}\mu(A_{1}\cap\ldots\cap A_{k}).\]

**Proof:**\(\mu(B)=\mu(A\cup(A^{c}\cap B))=\mu(A)+\mu(B\setminus A)\geq\mu(A)\), by (a) and (c) of Definition 1.2.1. This proves (i).

To prove (ii), note that if either \(\mu(A)\) or \(\mu(B)\) is finite, then \(\mu(A\cap B)<\infty\), by (i). Hence, using the countable additivity property (c), we have

\[\mu(A\cup B) = \mu(A)+\mu(B\setminus A) \tag{2.1}\] \[= \mu(A)+[\mu(B\setminus A)+\mu(A\cap B)]-\mu(A\cap B)\] \[= \mu(A)+\mu(B)-\mu(A\cap B).\]

Hence, (ii) follows from (2.1) and by induction.

To prove (iii), note that the case \(k=2\) follows from (2.1). Next, suppose that (iii) holds for all sets \(A_{1},\ldots,A_{k}\in\mathcal{F}\) with \(\mu(A_{i})<\infty\) for all \(i=1,\ldots,k\) for some \(k=n\), \(n\in\mathbb{N}\). To show that it holds for \(k=n+1\), note that by (2.1),

\[\mu\bigg{(}\bigcup_{i=1}^{n+1}A_{i}\bigg{)}\] \[= \mu\bigg{(}\bigcup_{i=1}^{n}A_{i}\bigg{)}+\mu(A_{n+1})-\mu\bigg{(} \bigcup_{i=1}^{n}(A_{i}\cap A_{n+1})\bigg{)}\]1. Measures \[= \bigg{\{}\sum_{i=1}^{n}\mu(A_{i})-\sum_{1<i<j\leq n}\mu(A_{i}\cap A_{ j})+\cdots+(-1)^{n-1}\mu(A_{1}\cap\ldots\cap A_{n})\bigg{\}}\] \[\mbox{}+\mu(A_{n+1})-\bigg{[}\sum_{i=1}^{n}\mu(A_{i}\cap A_{n+1})- \sum_{1\leq i<j\leq n}\mu(A_{i}\cap A_{j}\cap A_{n+1})\] \[\mbox{}+\cdots+(-1)^{n-1}\mu(A_{1}\cap\ldots\cap A_{n+1})\bigg{]}\] \[= \sum_{i=1}^{n+1}\mu(A_{i})-\sum_{1\leq i<j\leq n+1}\mu(A_{i}\cap A _{j})+\cdots+(-1)^{n}\mu\bigg{(}\bigcap_{j=1}^{n+1}A_{j}\bigg{)}.\]

By induction, this completes the proof of Proposition 1.2.2. \(\Box\)

In Proposition 1.2.1, it was shown that a set function \(\mu\) on an algebra \({\cal F}\) is a measure iff it is finitely additive and monotone continuous from below. A natural question is: if \(\mu\) is a measure on \({\cal F}\) and \(\{A_{n}\}_{n\geq 1}\) is a collection of decreasing sets in \({\cal F}\) with \(A\equiv\bigcap_{n\geq 1}A_{n}\) also in \({\cal F}\), does the relation \(\mu(A)=\lim_{n\to\infty}\mu(A_{n})\) hold, i.e., does _monotone continuity from above_ hold? The answer is positive under the assumption \(\mu(A_{n_{0}})<\infty\) for some \(n_{0}\in{\mathbb{N}}\). It turns out that, in general, this assumption cannot be dropped (Problem 1.18).

**Proposition 1.2.3:** _Let \(\mu\) be a measure on an algebra \({\cal F}\)._

1. \((\)_Monotone continuity from above_\()\) _Let_ \(\{A_{n}\}_{n\geq 1}\) _be a sequence of sets in_ \({\cal F}\) _such that_ \(A_{n+1}\subset A_{n}\) _for all_ \(n\geq 1\) _and_ \(A\equiv\bigcap_{n\geq 1}A_{n}\in{\cal F}\)_. Also, let_ \(\mu(A_{n_{0}})<\infty\) _for some_ \(n_{0}\in{\mathbb{N}}\)_. Then,_ \[\lim_{n\to\infty}\mu(A_{n})=\mu(A).\]
2. \((\)_Countable subadditivity_\()\) _If_ \(\{A_{n}\}_{n\geq 1}\) _is a sequence of sets in_ \({\cal F}\) _such that_ \(\bigcup_{n\geq 1}A_{n}\in{\cal F}\)_, then_ \[\mu\bigg{(}\bigcup_{n=1}^{\infty}A_{n}\bigg{)}\leq\sum_{n=1}^{\infty}\mu(A_{n}).\]

**Proof:** To prove part (i), without loss of generality (w.l.o.g.), assume that \(n_{0}=1\), i.e., \(\mu(A_{1})<\infty\). Let \(C_{n}=A_{1}\setminus A_{n}\) for \(n\geq 1\), and \(C_{\infty}=A_{1}\setminus A\). Then \(C_{n}\) and \(C_{\infty}\) belong to \({\cal F}\) and \(C_{n}\uparrow C_{\infty}\). By Proposition 1.2.1 \((\)iii\()^{{}^{\prime}}_{b}\), (i.e., by the m.c.f.b. property), \(\mu(C_{n})\uparrow\mu(C_{\infty})\) and by \((\)iii\()^{{}^{\prime}}_{a}\), (i.e., finite additivity), \(\mu(C_{n})=\mu(A_{1})-\mu(A_{n})\) for all \(1\leq n<\infty\), due to the fact \(\mu(A_{1})<\infty\). This proves (i).

To prove part (ii), let \(D_{n}=\bigcup_{i=1}^{n}A_{i},n\geq 1\). Then, \(D_{n}\uparrow D\equiv\bigcup_{i\geq 1}A_{i}\). Hence, by m.c.f.b. and finite subadditivity,

\[\mu(D)=\lim_{n\to\infty}\mu(D_{n})\leq\lim_{n\to\infty}\sum_{i=1}^{n}\mu(A_{i}) =\sum_{n=1}^{\infty}\mu(A_{n}).\]

**Theorem 1.2.4:** (_Uniqueness of measures_). _Let \(\mu_{1}\) and \(\mu_{2}\) be two finite measures on a measurable space \((\Omega,\mathcal{F})\). Let \(\mathcal{C}\subset\mathcal{F}\) be a \(\pi\)-system such that \(\mathcal{F}=\sigma\langle\mathcal{C}\rangle\). If \(\mu_{1}(C)=\mu_{2}(C)\) for all \(C\in\mathcal{C}\) and \(\mu_{1}(\Omega)=\mu_{2}(\Omega)\), then \(\mu_{1}(A)=\mu_{2}(A)\) for all \(A\in\mathcal{F}\)._

**Proof:** Let \(\mathcal{L}\equiv\{A:A\in\mathcal{F},\ \mu_{1}(A)=\mu_{2}(A)\}\). It is easy to verify that \(\mathcal{L}\) is a \(\lambda\)-system. Since \(\mathcal{C}\subset\mathcal{L}\), by Theorem 1.1.2, \(\mathcal{L}=\sigma\langle\mathcal{C}\rangle=\mathcal{F}\). \(\Box\)

### 1.3 The extension theorems and Lebesgue-Stieltjes measures

As discussed earlier, in many situations, one starts with a given set function \(\mu\) defined on a small class \(\mathcal{C}\) of subsets of a set \(\Omega\) and then wants to extend \(\mu\) to a larger class \(\mathcal{M}\) by some approximation procedure. In this section, a general result in this direction, known as the _extension theorem_, is established. This is then applied to the construction of Lebesgue-Stieltjes measures on Euclidean spaces. For another application, see Chapter 6.

#### 1.3.1 Caratheodory extension of measures

**Definition 1.3.1:** Let \(\Omega\) be a nonempty set and let \(\mathcal{P}(\Omega)\) be the power set of \(\Omega\). A class \(\mathcal{C}\subset\mathcal{P}(\Omega)\) is called a _semialgebra_ if (i) \(A,B\in\mathcal{C}\Rightarrow A\cap B\in\mathcal{C}\) and (ii) for any \(A\in\mathcal{C}\), there exist sets \(B_{1},B_{2},\ldots,B_{k}\in\mathcal{C}\), for some \(1\leq k<\infty\), such that \(B_{i}\cap B_{j}=\emptyset\) for \(i\neq j\), and \(A^{c}=\bigcup_{i=1}^{k}B_{i}\).

**Example 1.3.1:**\(\Omega=\mathbb{R},\ \mathcal{C}\equiv\{(a,b],(b,\infty):-\infty\leq a,b<\infty\}\).

**Example 1.3.2:**\(\Omega=\mathbb{R},\ \mathcal{C}\equiv\{I:I\ \mbox{is an interval}\}\). An interval \(I\) in \(\mathbb{R}\) is a set in \(\mathbb{R}\) such that \(a\), \(b\in I\), \(a<b\Rightarrow(a,b)\subset I\).

**Example 1.3.3:**\(\Omega=\mathbb{R}^{k}\), \(\mathcal{C}\equiv\{I_{1}\times I_{2}\times\ \times I_{k}:I_{j}\ \mbox{is an interval in $\mathbb{R}$ for}\ 1\leq j\leq k\}\).

Recall that a collection \(\mathcal{A}\subset\mathcal{P}(\Omega)\) is an algebra if it is closed under finite union and complementation. It is easily verified (Problem 1.19) that the smallest algebra containing a semialgebra \(\mathcal{C}\) is \(\mathcal{A}(\mathcal{C})\equiv\{A:A=\bigcup_{i=1}^{k}B_{i},B_{i}\in\mathcal{C}\ \mbox{for}\ i=1,\ldots,k,k<\infty,\}\), i.e., the class of finite unions of sets from \(\mathcal{C}\).

**Definition 1.3.2:** A set function \(\mu\) on a semialgebra \(\mathcal{C}\), taking values in \(\mathbb{R}_{+}\equiv[0,\infty]\), is called a _measure_ if (i) \(\mu(\emptyset)=0\) and (ii) for any sequence of sets \(\{A_{n}\}_{n\geq 1}\subset\mathcal{C}\) with \(\bigcup_{n\geq 1}A_{n}\in\mathcal{C}\), and \(A_{i}\cap A_{j}=\emptyset\) for \(i\neq j\), \(\mu(\bigcup_{n\geq 1}A_{n})=\sum_{n=1}^{\infty}\mu(A_{n})\).

**Proposition 1.3.1:** _Let \(\mu\) be a measure on a semialgebra \(\mathcal{C}\). Let \(\mathcal{A}\equiv\mathcal{A}(\mathcal{C})\) be the smallest algebra generated by \(\mathcal{C}\). For each \(A\in\mathcal{A}\), set_

\[\bar{\mu}(A)=\sum_{i=1}^{k}\mu(B_{i}),\]

_if the set \(A\) has the representation \(A=\bigcup_{i=1}^{k}B_{i}\) for some \(B_{1},\ldots,B_{k}\in\mathcal{C},k<\infty\) with \(B_{i}\cap B_{j}=\emptyset\) for \(i\neq j\). Then,_

* \(\bar{\mu}\) _is independent of the representation of_ \(A\) _as_ \(A=\bigcup_{i=1}^{k}B_{i}\)_;_
* \(\bar{\mu}\) _is finitely additive on_ \(\mathcal{A}\)_, i.e.,_ \(A,B\in\mathcal{A},A\cap B=\emptyset\Rightarrow\bar{\mu}(A\cup B)=\bar{\mu}(A) +\bar{\mu}(B)\)_; and_
* \(\bar{\mu}\) _is countably additive on_ \(\mathcal{A}\)_, i.e., if_ \(A_{n}\in\mathcal{A}\) _for all_ \(n\geq 1\)_,_ \(A_{i}\cap A_{j}=\emptyset\) _for all_ \(i=j\)_, and_ \(\bigcup_{n\geq 1}A_{n}\in\mathcal{A}\)_, then_ \[\bar{\mu}\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)}=\sum_{n=1}^{\infty}\bar{\mu}(A_{n}).\]

**Proof:** Parts (i) and (ii) are easy to verify. Turning to part (iii), let each \(n\geq 1\), \(A_{n}=\bigcup_{j=1}^{k_{n}}B_{nj}\), \(B_{nj}\in\mathcal{C}\), \(\{B_{nj}\}_{j=1}^{k_{n}}\) disjoint. Since \(\bigcup_{n\geq 1}A_{n}\in\mathcal{A}\) then exist disjoint sets \(\{B_{i}\}_{i=1}^{k}\subset\mathcal{C}\) such that \(\bigcup_{n\geq 1}A_{n}=\bigcup_{i=1}^{k}B_{i}\). Now

\[B_{i} = B_{i}\cap\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)}=\bigcup_{n\geq 1}( B_{i}\cap A_{n})\] \[= \bigcup_{n\geq 1}\bigcup_{j=1}^{k_{n}}(B_{i}\cap B_{nj}).\]

Since for all \(i\), \(B_{i}\in\mathcal{C}\), \(B_{i}\cap B_{nj}\in\mathcal{C}\) for all \(j,n\) and \(\mu\) is a measure on \(\mathcal{C}\)

\[\mu(B_{i})=\sum_{n\geq 1}\sum_{j=1}^{k_{n}}\mu(B_{i}\cap B_{nj}).\]

Thus,

\[\bar{\mu}\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)} = \sum_{i=1}^{k}\mu(B_{i})=\sum_{i=1}^{k}\sum_{n\geq 1}\bigg{(} \sum_{j=1}^{k_{n}}\mu(B_{i}\cap B_{nj})\bigg{)}\] \[= \sum_{i=1}^{k}\mu(B_{i})=\sum_{j=1}^{k_{n}}\mu(B_{i})=\sum_{j=1 }^{k_{n}}\mu(B_{i}\cap B_{nj}).\]

Thus,

\[\bar{\mu}\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)} = \sum_{i=1}^{k}\mu(B_{i})=\sum_{j=1}^{k}\mu(B_{i}\cap B_{nj}).

\[\mu^{*}(E)=\mu^{*}(E\cap A)+\mu^{*}(E\cap A^{c})\mbox{ for all }E\subset\Omega. \tag{3.3}\]

In other words, an analog of (3.2) should hold in every portion \(E\) of \(\Omega\) for a set \(A\) to be \(\mu^{*}\)-measurable.

It can be shown (Problem 1.20) that \(\mu^{*}\) defined in (3.1) satisfies:

\[\mu^{*}(\emptyset) = 0, \tag{3.4}\] \[A\subset B \Rightarrow \mu^{*}(A)\leq\mu^{*}(B), \tag{3.5}\]

and for any \(\{A_{n}\}_{n\geq 1}\subset{\cal P}(\Omega)\),

\[\mu^{*}\Big{(}\bigcup_{n\geq 1}A_{n}\Big{)}\leq\sum_{n=1}^{\infty}\mu^{*}(A_{n })\,. \tag{3.6}\]

**Definition 1.3.5:** Any set function \(\mu^{*}:{\cal P}(\Omega)\to\bar{\mathbb{R}}_{+}\equiv[0,\infty]\) satisfying (3.4)-(3.6) is called an _outer measure on \(\Omega\)_.

The following result (due to C. Caratheodory) yields a measure space on \(\Omega\) starting from a general outer measure \(\mu^{*}\) that need not arise from a measure \(\mu\) as in (3.1).

**Theorem 1.3.2:**_Let \(\mu^{*}\) be an outer measure on \(\Omega\), i.e., it satisfies (3.4)-(3.6). Let \({\cal M}\equiv{\cal M}_{\mu^{*}}\equiv\{A:A\) is \(\mu^{*}\)-measurable, i.e., \(A\) satisfies (3.3)\(\}\). Then_

* \({\cal M}\) _is a_ \(\sigma\)_-algebra,_
* \(\mu^{*}\) _restricted to_ \({\cal M}\) _is a measure, and_
* \(\mu^{*}(A)=0\Rightarrow{\cal P}(A)\subset{\cal M}\)_._

**Proof:** From (3.3), it follows that \(\emptyset\in{\cal M}\) and that \(A\in{\cal M}\Rightarrow A^{c}\in{\cal M}\). Next, it will be shown that \({\cal M}\) is closed under finite unions. Let \(A_{1},A_{2}\in{\cal M}\). Then, for any \(E\subset\Omega\),

\[\mu^{*}(E) = \mu^{*}(E\cap A_{1})+\mu^{*}(E\cap A_{1}^{c})\quad\mbox{(since $A_{1} \in{\cal M}$)}\] \[= \mu^{*}(E\cap A_{1}\cap A_{2})+\mu^{*}(E\cap A_{1}\cap A_{2}^{c})\] \[+\mu^{*}(E\cap A_{1}^{c}\cap A_{2})+\mu^{*}(E\cap A_{1}^{c}\cap A _{2}^{c})\quad\mbox{(since $A_{2}\in{\cal M}$)}.\]

But \((A_{1}\cap A_{2})\cup(A_{1}\cap A_{2}^{c})\cup(A_{1}^{c}\cap A_{2})=A_{1}\cup A _{2}\). Since \(\mu^{*}\) is subadditive, it follows that

\[\mu^{*}(E\cap(A_{1}\cup A_{2}))\leq\mu^{*}(E\cap A_{1}\cap A_{2})+\mu^{*}(E \cap A_{1}\cap A_{2}^{c})+\mu^{*}(E\cap A_{1}^{c}\cap A_{2}).\]

Thus

\[\mu^{*}(E)\geq\mu^{*}(E\cap(A_{1}\cup A_{2}))+\mu^{*}(E\cap(A_{1}\cup A_{2})^{ c}).\]

The subadditivity of \(\mu^{*}\) yields the opposite inequality and so, \(A_{1}\cup A_{2}\in{\cal M}\) and hence, \({\cal M}\) is an algebra. To show that \({\cal M}\) is a \(\sigma\)-algebra, it suffices to show that \({\cal M}\) is closed under monotone unions, i.e., \(A_{n}\in{\cal M},A_{n}\subset A_{n+1}\) for all \(n\geq 1\Rightarrow A\equiv\bigcup_{n\geq 1}A_{n}\in{\cal M}\). Let \(B_{1}=A_{1}\) and \(B_{n}=A_{n}\cap A_{n-1}^{c}\)

[MISSING_PAGE_EMPTY:39]

implies \(\mu^{*}(E)\geq\mu^{*}(E\cap B^{c})+\mu^{*}(E\cap B)\). The opposite inequality holds by the subadditivity of \(\mu^{*}\). So \(B\in{\cal M}\), and (iii) is proved. \(\Box\)

**Definition 1.3.6:** A measure space \((\Omega,{\cal F},\nu)\) is called _complete_ if for any \(A\in{\cal F}\) with \(\nu(A)=0\Rightarrow{\cal P}(A)\subset{\cal F}\).

Thus, by part (iii) of the above theorem, \((\Omega,{\cal M}_{\mu^{*}},\mu^{*})\) is a _complete measure space_. Now the above theorem is applied to a \(\mu^{*}\) that is generated by a given measure \(\mu\) on a semialgebra \({\cal C}\) via (3.1).

**Theorem 1.3.3:** (_Caratheodory's extension theorem_). _Let \(\mu\) be a measure on a semialgebra \({\cal C}\) and let \(\mu^{*}\) be the set function induced by \(\mu\) as defined by (3.1). Then,_

* \(\mu^{*}\) _is an outer measure,_
* \({\cal C}\subset{\cal M}_{\mu^{*}}\)_, and_
* \(\mu^{*}=\mu\) _on_ \({\cal C}\)_, where_ \({\cal M}_{\mu^{*}}\) _is as in Theorem_ 1.3.2_._

**Proof:** The proof of (i) involves verifying (3.4)-(3.6), which is left as an exercise (Problem 1.20). To prove (ii), let \(A\in{\cal C}\). Let \(E\subset\Omega\) and \(\{A_{n}\}_{n\geq 1}\subset{\cal C}\) be such that \(E\subset\bigcup_{n\geq 1}A_{n}\). Then, for all \(i\in\mathbb{N}\), \(A_{i}=(A_{i}\cap A)\cup(A_{i}\cap B_{1})\cup\ldots\cup(A_{i}\cap B_{k})\) where \(B_{1},\ldots,B_{k}\) are disjoint sets in \({\cal C}\) such that \(\bigcup_{j=1}^{k}B_{j}=A^{c}\). Since \(\mu\) is finitely additive on \({\cal C}\),

\[\mu(A_{i}) = \mu(A_{i}\cap A)+\sum_{j=1}^{k}\mu(A_{i}\cap B_{j})\] \[\Rightarrow\quad\sum_{n=1}^{\infty}\mu(A_{n}) = \sum_{n=1}^{\infty}\mu(A_{n}\cap A)+\sum_{n=1}^{\infty}\sum_{j=1} ^{k}\mu(A_{n}\cap B_{j})\] \[\geq \mu^{*}(E\cap A)+\mu^{*}(E\cap A^{c}),\]

since \(\{A_{n}\cap A\}_{n\geq 1}\) and \(\{A_{n}\cap B_{j}:1\leq j\leq k,n\geq 1\}\) are both countable subcollections of \({\cal C}\) whose unions cover \(E\cap A\) and \(E\cap A^{c}\), respectively. From the definition of \(\mu^{*}(E)\), it now follows that

\[\mu^{*}(E)\geq\mu^{*}(E\cap A)+\mu^{*}(E\cap A^{c}).\]

Now the subadditivity of \(\mu^{*}\) completes the proof of part (ii).

To prove (iii), let \(A\in{\cal C}\). Then, by definition, \(\mu^{*}(A)\leq\mu(A)\). If \(\mu^{*}(A)=\infty\), then \(\mu(A)=\infty=\mu^{*}(A)\). If \(\mu^{*}(A)<\infty\), then by the definition of 'infimum,' for any \(\epsilon>0\), there exists \(\{A_{n}\}_{n\geq 1}\subset{\cal C}\) such that \(A\subset\bigcup_{n\geq 1}A_{n}\) and

\[\mu^{*}(A)\leq\sum_{n=1}^{\infty}\mu(A_{n})\leq\mu^{*}(A)+\epsilon.\]But \(A=A\cap(\bigcup_{n\geq 1}A_{n})=\bigcup_{n\geq 1}(A\cap A_{n})\). Note that the set function \(\bar{\mu}\) defined in Proposition 1.3.1 is a measure on \(\mathcal{A}(\mathcal{C})\) and it coincides with \(\mu\) on \(\mathcal{C}\). Since \(A,A\cap A_{n}\in\mathcal{C}\) for all \(n\geq 1\), by Proposition 1.2.3 (b) applied to \(\bar{\mu}\),

\[\mu(A)=\bar{\mu}(A)\leq\sum_{n=1}^{\infty}\bar{\mu}(A\cap A_{n})\leq\sum_{n=1}^ {\infty}\bar{\mu}(A_{n})=\sum_{n=1}^{\infty}\mu(A_{n})\leq\mu^{*}(A)+\epsilon.\]

(Alternately, w.l.o.g., assume that \(\{A_{n}\}_{n\geq 1}\) are disjoint. Then \(A=A\cap\bigcup_{n\geq 1}A_{n}=\bigcup_{n\geq 1}(A\cap A_{n})\). Since \(A\cap A_{n}\in\mathcal{C}\) for all \(n\), by the countable additivity of \(\mu\) on \(\mathcal{C}\), \(\mu(A)=\sum_{n=1}^{\infty}\mu(A\cap A_{n})\leq\sum_{n=1}^{\infty}\mu(A_{n})= \mu^{*}(A)+\epsilon.\)) Since \(\epsilon>0\) is arbitrary, this yields \(\mu(A)\leq\mu^{*}(A)\). \(\Box\)

Thus, given a measure \(\mu\) on a semialgebra \(\mathcal{C}\subset\mathcal{P}(\Omega)\), there is a complete measure space \((\Omega,\mathcal{M}_{\mu^{*}},\mu^{*})\) such that \(\mathcal{M}_{\mu^{*}}\supset\mathcal{C}\) and \(\mu^{*}\) restricted to \(\mathcal{C}\) equals \(\mu\). For this reason, \(\mu^{*}\) is called an _extension_ of \(\mu\). The measure space \((\Omega,\mathcal{M}_{\mu^{*}},\mu^{*})\) is called the _Caratheodory extension_ of \(\mu\). Since \(\mathcal{M}_{\mu^{*}}\) is a \(\sigma\)-algebra and contains \(\mathcal{C}\), \(\mathcal{M}_{\mu^{*}}\) must contain \(\sigma\langle\mathcal{C}\rangle\), the \(\sigma\)-algebra generated by \(\mathcal{C}\), and thus, \((\Omega,\sigma\langle\mathcal{C}\rangle,\mu^{*})\) is also a measure space. However, the latter may _not_ be _complete_ (see Section 1.4).

Now the above method is applied to the construction of Lebesgue-Stieltjes measures on \(\mathbb{R}\) and \(\mathbb{R}^{2}\).

#### 1.3.2 Lebesgue-Stieltjes measures on \(\mathbb{R}\)

Let \(F:\mathbb{R}\to\mathbb{R}\) be nondecreasing. For \(x\in\mathbb{R}\), let \(F(x+)\equiv\lim_{y\downarrow x}F(y)\), and \(F(x-)\equiv\lim_{y\uparrow x}F(y)\). Set \(F(\infty)=\lim_{x\uparrow\infty}F(x)\) and \(F(-\infty)=\lim_{x\downarrow-\infty}F(y)\). Let

\[\mathcal{C}\equiv\Big{\{}(a,b]:-\infty\leq a\leq b<\infty\Big{\}}\cup\Big{\{}( a,\infty):-\infty\leq a<\infty\Big{\}}. \tag{3.7}\]

Define

\[\mu_{F}((a,b]) = F(b+)-F(a+),\] \[\mu_{F}((a,\infty)) = F(\infty)-F(a+). \tag{3.8}\]

Then, it may be verified that

1. \(\mathcal{C}\) is a semialgebra;
2. \(\mu_{F}\) is a measure on \(\mathcal{C}\). (For (ii), one needs to use the Heine-Borel theorem. See Problems 1.22 and 1.23.)

Let \((\mathbb{R},\mathcal{M}_{\mu^{*}_{F}},\mu^{*}_{F})\) be the Caratheodory extension of \(\mu_{F}\), i.e., the measure space constructed as in the above two theorems.

**Definition 1.3.7:** Let \(F:\mathbb{R}\to\mathbb{R}\) be nondecreasing. The (measure) space \((\mathbb{R},\mathcal{M}_{\mu^{*}_{F}},\mu^{*}_{F})\) is called a _Lebesgue-Stieltjes measure space_ and \(\mu^{*}_{F}\) is the _Lebesgue-Stieltjes measure_ generated by \(F\).

Since \(\sigma\langle\mathcal{C}\rangle=\mathcal{B}(\mathbb{R})\), the class of all Borel sets of \(\mathbb{R}\), every Lebesgue-Stieltjes measure \(\mu^{*}_{F}\) is also a measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Note also that \(\mu^{*}_{F}\) is finite on bounded intervals.

Conversely, given any _Radon measure_\(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\), i.e., a measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) that is finite on bounded intervals, set

\[F(x)=\left\{\begin{array}{ll}\mu((0,x])&\mbox{if }x>0\\ 0&\mbox{if }x=0\\ -\mu((x,0])&\mbox{if }x\leq 0.\end{array}\right.\]

Then \(\mu_{F}=\mu\) on \(\mathcal{C}\). By the uniqueness of the extension (discussed later in this section, see also Theorem 1.2.4), it follows that \(\mu^{*}_{F}\) coincides with \(\mu\) on \(\mathcal{B}(\mathbb{R})\). Thus, every Radon measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) is necessarily a Lebesgue-Stieltjes measure.

**Definition 1.3.8:** (_Lebesgue Measure on \(\mathbb{R}\)_). When \(F(x)\equiv x,x\in\mathbb{R}\), the measure \(\mu^{*}_{F}\) is called the _Lebesgue measure_ and the \(\sigma\)-algebra \(\mathcal{M}_{\mu^{*}_{F}}\) is called the class of _Lebesgue measurable sets_.

The Lebesgue measure will be denoted by \(m(\cdot)\) or \(\mu_{L}(\cdot)\). Given below are some important results on \(m(\cdot)\).

* It follows from equation (3.1), that \(\mu^{*}_{F}(\{x\})=F(x+)-F(x-)\) and hence \(=0\) if \(F\) is continuous at \(x\). Thus \(m(\{x\})\equiv 0\) on \(\mathbb{R}\).
* By countable additivity of \(m(\cdot)\), \(m(A)=0\) for any countable set \(A\).
* (_Cantor set_). There exists an uncountable set \(C\) such that \(m(C)=0\). An example is the _Cantor set_ constructed as follows: Start with \(I_{0}=[0,1]\). Delete the open middle third, i.e., \(\left(\frac{1}{3},\frac{2}{3}\right)\). Next from the closed intervals \(I_{11}=[0,\frac{1}{3}]\) and \(I_{12}=[\frac{2}{3},1]\) delete the open middle thirds, i.e., \(\left(\frac{1}{9},\frac{2}{9}\right)\) and \(\left(\frac{7}{9},\frac{2}{9}\right)\), respectively. Repeat this process of deleting the middle third from each of the remaining closed intervals. Thus at stage \(n\) there will be \(2^{n-1}\) new closed intervals and \(2^{n-1}\) deleted open intervals, each of length \(\frac{1}{3^{n}}\). Let \(U_{n}\) denote the union of all the deleted open intervals at the \(n\)th stage. Then \(\{U_{n}\}_{n\geq 1}\) are disjoint open sets. Let \(U\equiv\bigcup_{n\geq 1}U_{n}\). By countable additivity \[m(U)=\sum_{n=1}^{\infty}m(U_{n})=\sum_{n=1}^{\infty}\frac{2^{n-1}}{3^{n}}=1.\] Let \(C\equiv[0,1]-U\). Since \(U\) is open and [0,1] is closed, \(C\) is nonempty. It can be shown that \(C\equiv\{x:x=\sum_{1}^{\infty}\frac{a_{i}}{3^{i}},a_{i}=0\mbox{ or }2\}\) (Problem1.33). Thus \(C\) can be mapped in (1-1) manner on to the set of all sequences \(\{\delta_{i}\}_{i\geq 1}\) such that \(\delta_{i}=0\) or \(2\). But this set is uncountable. Since \(m([0,1])=1\), it follows that \(m(C)=0\). For more properties of the Cantor set, see Rudin (1976) and Chapter 4.
* \(m(\cdot)\) is invariant under reflection and translation. That is, for any \(E\) in \(\mathcal{B}(\mathbb{R})\), \[m(-E)=m(E)\quad\text{and}\quad m(E+c)=m(E)\] for all \(c\) in \(\mathbb{R}\) where \(-E=\{-x:x\in E\}\) and \(E+c=\{y:y=x+c,x\in E\}\). This follows from Theorem 1.2.4 and the fact that the claim holds for intervals (cf. Problem 2.48).
* There exists a subset \(A\subset\mathbb{R}\) such that \(A\not\in\mathcal{M}_{m}\). That is, there exists a _non-Lebesgue measurable set_. The proof of this requires the use of the axiom of choice (cf. A.1). For a proof see Royden (1988).

#### 1.3.3 Lebesgue-Stieltjes measures on \(\mathbb{R}^{2}\)

Let \(F:\mathbb{R}^{2}\to\mathbb{R}\) satisfy

\[F(a_{2},b_{2})-F(a_{2},b_{1})-F(a_{1},b_{2})+F(a_{1},b_{1})\geq 0, \tag{3.9}\]

and

\[F(a_{2},b_{2})-F(a_{1},b_{1})\geq 0, \tag{3.10}\]

for all \(a_{1}\leq a_{2}\), \(b_{1}\leq b_{2}\). Extend \(F\) to \(\bar{\mathbb{R}}^{2}\) by appropriate limiting procedure. Let

\[\mathcal{C}_{2}\equiv\{I_{1}\times I_{2}:I_{1},I_{2}\in\mathcal{C}\}, \tag{3.11}\]

where \(\mathcal{C}\) is as in (3.7). Next, for \(I_{1}=(a_{1},a_{2}]\), \(I_{2}=(b_{1},b_{2}]\), \(-\infty<a_{1},a_{2},b_{1},b_{2}<\infty\), set

\[\mu_{F}(I_{1}\times I_{2})\equiv F(a_{2}+,b_{2}+)-F(a_{2}+,b_{1}+)-(F(a_{1}+,b_ {2}+)+F(a_{1}+,b_{1}+)), \tag{3.12}\]

where for any \(a,b\in\mathbb{R}\), \(F(a+,b+)\) is defined as

\[F(a+,b+)\equiv\lim_{a^{\prime}\downarrow a,b^{\prime}\downarrow b}F(a^{\prime},b^{\prime}).\]

Note that by (3.10), the limit exists and hence, \(F(a+,b+)\) is well defined. Further, by (3.9), the right side of (3.12) is nonnegative. Next extend the definition of \(\mu_{F}\) to unbounded sets in \(\mathcal{C}_{2}\) by the limiting procedure:

\[\mu_{F}(I_{1}\times I_{2})=\lim_{n\to\infty}\mu_{F}\big{(}(I_{1}\times I_{2}) \cap J_{n}\big{)}, \tag{3.13}\]

where \(J_{n}=(-n,n]\times(-n,n]\). Then it may be verified (Problems 1.24, 1.25) that 1. Measures
2. \({\cal C}_{2}\) is a semialgebra
3. \(\mu_{F}\) is a measure on \({\cal C}_{2}\).

Let \((\mathbb{R}^{2},{\cal M}_{\mu^{*}_{F}},\mu^{*}_{F})\) be the measure space constructed as in the above two theorems. The measure \(\mu^{*}_{F}\) is called the _Lebesgue-Stieltjes measure_ generated by \(F\) and \({\cal M}_{\mu^{*}_{F}}\) a _Lebesgue-Stieltjes \(\sigma\)-algebra_. Again, in this case, \({\cal M}_{\mu^{*}}\) includes the \(\sigma\)-algebra \(\sigma({\cal C})\equiv{\cal B}(\mathbb{R}^{2})\) and so \((\mathbb{R}^{2},{\cal B}(\mathbb{R}^{2}),\mu^{*}_{F})\) is also a measure space. If \(F(a,b)=ab\), then \(\mu_{F}\) is called the _Lebesgue measure on \(\mathbb{R}^{2}\)_.

A similar construction holds for any \(\mathbb{R}^{k}\), \(k<\infty\).

#### More on extension of measures

Next the uniqueness of the extension \(\mu^{*}\) and approximation of the \(\mu^{*}\)-measure of a set in \({\cal M}_{\mu^{*}}\) by that of a set from the algebra \({\cal A}\equiv{\cal A}({\cal C})\) are considered. As in the case of measures defined on an algebra, a measure \(\mu\) on a semialgebra \({\cal C}\subset{\cal P}(\Omega)\) is said to be \(\sigma\)-_finite_ if there exists a _countable_ collection \(\{A_{n}\}_{n\geq 1}\subset{\cal C}\) such that (i) \(\mu(A_{n})<\infty\) for each \(n\geq 1\) and (ii) \(\bigcup_{n\geq 1}A_{n}=\Omega\). The following approximation result holds.

**Theorem 1.3.4:** _Let \(A\in{\cal M}_{\mu^{*}}\) and \(\mu^{*}(A)<\infty\). Then, for each \(\epsilon>0\), there exist \(B_{1},B_{2},\ldots,B_{k}\in{\cal C}\), \(k<\infty\) with \(B_{i}\cap B_{j}=\emptyset\) for \(1\leq i\neq j\leq k\), such that_

\[\mu^{*}\bigg{(}A\bigtriangleup\bigcup_{j=1}^{k}B_{j}\bigg{)}<\epsilon,\]

_where for any two sets \(E_{1}\) and \(E_{2}\), \(E_{1}\bigtriangleup E_{2}\) is the symmetric difference of \(E_{1}\) and \(E_{2}\), defined by \(E_{1}\bigtriangleup E_{2}\equiv(E_{1}\cap E_{2}^{c})\cup(E_{1}^{c}\cap E_{2})\)._

**Proof:** By definition of \(\mu^{*}\), \(\mu^{*}(A)<\infty\) implies that for every \(\epsilon>0\), there exist \(\{B_{n}\}_{n\geq 1}\subset{\cal C}\) such that \(A\subset\bigcup_{n\geq 1}B_{n}\) and

\[\mu^{*}(A)\leq\sum_{n=1}^{\infty}\mu(B_{n})\leq\mu^{*}(A)+\epsilon/2<\infty.\]

Since \(B_{n}\in{\cal C}\), \(B_{n}^{c}\) is a finite union of disjoint sets from \({\cal C}\). W.l.o.g., it can be assumed that \(\{B_{n}\}_{n\geq 1}\) are disjoint. (Otherwise, one can consider the sequence \(B_{1},B_{2}\cap B_{1}^{c},B_{3}\cap B_{2}^{c}\cap B_{1}^{c},\ldots\).) Next, \(\sum_{n=1}^{\infty}\mu(B_{n})<\infty\) implies that for every \(\epsilon>0\), there exists \(k\in\mathbb{N}\) such that \(\sum_{n=k+1}^{\infty}\mu(B_{n})<\frac{\epsilon}{2}\). Since both \(A\) and \(\bigcup_{j=1}^{k}B_{j}\) are subsets of \(\bigcup_{n\geq 1}B_{n}\),

\[\mu^{*}\bigg{(}A\bigtriangleup\bigg{[}\bigcup_{j=1}^{k}B_{j}\bigg{]}\bigg{)} \leq\mu^{*}\bigg{(}A^{c}\cap\bigg{[}\bigcup_{j=1}^{\infty}B_{j}\bigg{]}\bigg{)} +\mu^{*}\bigg{(}\bigcup_{j=k+1}^{\infty}B_{j}\bigg{)}.\]But since \(\mu^{*}\) is a measure on \((\Omega,\mathcal{M}_{\mu^{*}})\), \(\mu^{*}(\bigcup_{n\geq 1}B_{n})=\mu^{*}(A)+\mu^{*}((\bigcup_{n\geq 1}B_{n})\cap A^{c})\). Further, since \(\mu^{*}(A)<\infty\),

\[\mu^{*}\Big{(}\Big{[}\bigcup_{n\geq 1}B_{n}\Big{]}\cap A^{c}\Big{)}\] \[= \mu^{*}\Big{(}\bigcup_{n\geq 1}B_{n}\Big{)}-\mu^{*}(A))\] \[\leq \sum_{n=1}^{\infty}\mu^{*}(B_{n})-\mu^{*}(A),\quad\mbox{(since $ \mu^{*}$ is countably subadditive)}\] \[= \sum_{n=1}^{\infty}\mu(B_{n})-\mu^{*}(A),\quad\mbox{(since $\mu^{*}= \mu$ on $\mathcal{C}$)}\] \[< \frac{\epsilon}{2}\quad\mbox{(by the choice of $\{B_{n}\}_{n\geq 1}$)}.\]

Also, by the definition of \(k\),

\[\mu^{*}\bigg{(}\bigcup_{j=k+1}^{\infty}B_{j}\bigg{)}\leq\sum_{j=k+1}^{\infty} \mu^{*}(B_{j})=\sum_{j=k+1}^{\infty}\mu(B_{j})<\frac{\epsilon}{2}.\]

Thus, \(\mu^{*}(A\bigtriangleup[\bigcup_{j=1}^{k}B_{j}])<\epsilon\). This completes the proof of the theorem. \(\Box\)

Thus, every \(\mu^{*}\)-measurable set of finite measure is nearly a finite union of disjoint elements from the semialgebra \(\mathcal{C}\). This was enunciated by J.E. Littlewood as the first of his three principles of approximation (the other two being: every Lebesgue measurable function is nearly continuous (cf. Theorem 2.5.12) and every almost everywhere convergent sequence of functions on a finite measure space is nearly uniformly convergent (Egorov's theorem) cf. Theorem 2.5.11).

One may strengthen Theorem 1.3.4 to prove the following result on regularity of _Radon_ measures on \(\big{(}\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k})\big{)}\) (cf. Problem 1.32). See also Rudin (1987), Chapter 2.

**Corollary 1.3.5:** (_Regularity of measures_). _Let \(\mu\) be a Radon measure on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\) for some \(k\in\mathbb{N}\), i.e., \(\mu(A)<\infty\) for all bounded sets \(A\in\mathcal{B}(\mathbb{R}^{k})\). Let \(A\in\mathcal{B}(\mathbb{R}^{k})\) be such that \(\mu(A)<\infty\). Then, for each \(\epsilon>0\), there exist a compact set \(K\) and an open set \(G\) such that \(K\subset A\subset G\) and \(\mu(G\setminus K)<\epsilon\)._

The following uniqueness result can be established by an application of the above approximation theorem (Theorem 1.3.4) or by applying the \(\pi\)-\(\lambda\) theorem (Corollary 1.1.3), as in Theorem 1.2.4 (Problem 1.26).

**Theorem 1.3.6:** (_Uniqueness_). _Let \(\mu\) be a \(\sigma\)-finite measure on a semialgebra \(\mathcal{C}\). Let \(\nu\) be a measure on the measurable space \((\Omega,\sigma\langle\mathcal{C}\rangle)\) such that \(\nu=\mu\) on \(\mathcal{C}\). Then \(\nu=\mu^{*}\) on \(\sigma\langle\mathcal{C}\rangle\)._An application of Theorem 1.3.4 yields an useful approximation result known as Lusin's theorem (see Theorem 2.1.3 in Section 2.1) for approximating Borel measurable functions by continuous functions.

### 1.4 Completeness of measures

Recall from Definition 1.3.6 that a measure space \((\Omega,\mathcal{F},\mu)\) is called _complete_ if for any \(A\in\mathcal{F}\), \(\mu(A)=0,\ B\subset A\ \Rightarrow B\in\mathcal{F}\). That is, for any set \(A\) in \(\mathcal{F}\) whose \(\mu\) measure is zero, all subsets of a set \(A\) are also in \(\mathcal{F}\). For example, the very construction of the Lebesgue-Stieltjes measure for a nondecreasing \(F\) on \(\mathbb{R}\), discussed in Section 1.3, yields a complete measure space \((\mathbb{R},\mathcal{M}_{\mu^{*}_{F}},\mu^{*}_{F})\). The Borel \(\sigma\)-algebra \(\mathcal{B}(\mathbb{R})\) is a sub-\(\sigma\)-algebra of \(\mathcal{M}_{\mu^{*}_{F}}\) and \((\mathbb{R},\mathcal{B}(\mathbb{R}),\mu^{*}_{F})\) need not be complete. For example, if \(\mu_{F}\) is the Lebesgue measure, then the Cantor set \(C\) (Section 1.3.2) has measure \(0\) and hence \(\mathcal{M}\equiv\) the Lebesgue \(\sigma\)-algebra contains the power set of \(C\) and hence has cardinality larger than that of \(\mathbb{R}\). It can be shown that the cardinality of \(\mathcal{B}(\mathbb{R})\) is the same as that of \(\mathbb{R}\) (see Hewitt and Stromberg (1965)). For another example, if \(F\) is a degenerate distribution at \(0\), i.e., \(F(x)=0\) for \(x<0\) and \(F(x)=1\) for \(x\geq 0\), then \(\mathcal{M}_{\mu^{*}_{F}}=\mathcal{P}(\mathbb{R})\), the power set of \(\mathbb{R}\) (Problem 1.28), and hence \((\mathbb{R},\mathcal{B}(\mathbb{R}),\mu_{F})\) is not complete. The same is true for any discrete distribution function. However, it is always possible to complete an incomplete measure space \((\Omega,\mathcal{F},\mu)\) by adding new sets to \(\mathcal{F}\). This procedure is discussed below.

**Theorem 1.4.1**: _Let \((\Omega,\mathcal{F},\mu)\) be a measure space. Let \(\tilde{\mathcal{F}}\equiv\{A:B_{1}\subset A\subset B_{2}\) for some \(B_{1},B_{2}\in\mathcal{F}\) satisfying \(\mu(B_{2}\setminus B_{1})=0\}\). For any \(A\in\tilde{\mathcal{F}}\), set \(\tilde{\mu}(A)=\mu(B_{1})=\mu(B_{2})\) for any pair of sets \(B_{1},B_{2}\in\mathcal{F}\) with \(B_{1}\subset A\subset B_{2}\) and \(\mu(B_{2}\setminus B_{1})=0\). Then_

* \(\tilde{\mathcal{F}}\) _is a_ \(\sigma\)_-algebra and_ \(\mathcal{F}\subset\tilde{\mathcal{F}}\)_,_
* \(\tilde{\mu}\) _is well defined,_
* \((\Omega,\tilde{\mathcal{F}},\tilde{\mu})\) _is a complete measure space and_ \(\tilde{\mu}=\mu\) _on_ \(\mathcal{F}\)_._

**Proof:**

* Since \(A\in\tilde{\mathcal{F}}\), there exist \(B_{1},B_{2}\in\mathcal{F},\ B_{1}\subset A\subset B_{2}\) and \(\mu(B_{2}\setminus B_{1})=0\). Clearly, \(B_{2}^{c}\subset A^{c}\subset B_{1}^{c}\), and \(B_{1}^{c},B_{2}^{c}\in\mathcal{F}\) and \(\mu(B_{1}^{c}\setminus B_{2}^{c})=\mu(B_{2}\setminus B_{1})=0\) and so \(A^{c}\in\tilde{\mathcal{F}}\). Next, let \(\{A_{n}\}_{n=1}^{\infty}\subset\tilde{\mathcal{F}}\) and \(A=\bigcup_{n\geq 1}A_{n}\). Then, for each \(n\) there exist \(B_{1n}\) and \(B_{2n}\) in \(\mathcal{F}\) such that \(B_{1n}\subset A_{n}\subset B_{2n}\) and \(\mu(B_{2n}\setminus B_{1n})=0\). Let \(B_{1}=\bigcup_{n\geq 1}B_{1n}\) and \(B_{2}=\bigcup_{n\geq 1}B_{2n}\). Then \(B_{1}\subset A\subset B_{2}\), \(B_{1},B_{2}\in\mathcal{F}\) and \(\tilde{B_{2}}\setminus B_{1}\subset\bigcup_{n\geq 1}(B_{2n}\setminus B_{1n})\) and hence \(\mu(B_{2}\setminus B_{1})\leq\sum_{n=1}^{\infty}\mu(B_{2n}\setminus B_{1n})=0\). Thus, \(A\in\tilde{\mathcal{F}}\) and hence \(\tilde{\mathcal{F}}\) is a \(\sigma\)-algebra. Clearly, \(\mathcal{F}\subset\tilde{\mathcal{F}}\) since for every \(A\in\mathcal{F}\), one may take \(B_{1}=B_{2}=A\).

* Let \(B_{1}\subset A\subset B_{2}\), \(B_{1}^{\prime}\subset A\subset B_{2}^{\prime}\), \(B_{1},B_{1}^{\prime},B_{2},B_{2}^{\prime}\in{\cal F}\) and \(\mu(B_{2}\setminus B_{1})=0=\mu(B_{2}^{\prime}\setminus B_{1}^{\prime})\). Then \(B_{1}\cup B_{1}^{\prime}\subset A\subset B_{2}\cap B_{2}^{\prime}\) and \((B_{2}\cap B_{2}^{\prime})\setminus(B_{1}\cup B_{1}^{\prime})=(B_{2}\cap B_{2} ^{\prime})\cap(B_{1}^{c}\cap B_{1}^{{}^{\prime}c})\subset B_{2}\cap B_{1}^{c}\). Thus \[\mu([B_{2}\cap B_{2}^{\prime}]\setminus[B_{1}\cup B_{1}^{\prime}])=0.\] Hence, \(\mu(B_{2})=\mu(B_{1})+\mu(B_{2}\setminus B_{1})=\mu(B_{1})\leq\mu(B_{1}\cup B _{1}^{\prime})=\mu(B_{2}\cap B_{2}^{\prime})\leq\mu(B_{2}^{\prime})\). By symmetry \(\mu(B_{2}^{\prime})\leq\mu(B_{2})\) and so \(\mu(B_{2})=\mu(B_{2}^{\prime})\). But \(\mu(B_{2})=\mu(B_{1})\) and \(\mu(B_{2}^{\prime})=\mu(B_{1}^{\prime})\) and also all four quantities agree.
* It remains to show that \(\tilde{\mu}\) is countably additive and complete on \(\tilde{\cal F}\). Let \(\{A_{n}\}_{n=1}^{\infty}\) be a disjoint sequence of sets from \(\tilde{\cal F}\) and let \(A=\bigcup_{n\geq 1}A_{n}\). Let \(\{B_{1n}\}_{n\geq 1},\{B_{2n}\}_{n\geq 1},B_{1},B_{2}\) be as in the proof of (i). Then, the fact that \(\{A_{n}\}_{n=1}^{\infty}\) are disjoint implies \(\{B_{1n}\}_{n=1}^{\infty}\) are also disjoint. And since \(B_{1}=\bigcup_{n\geq 1}B_{1n}\) and \(\mu\) is a measure on \((\Omega,{\cal F})\), \[\mu(B_{1})\equiv\sum_{n=1}^{\infty}(B_{1n}).\] Also, by definition of \(B_{1n}\)'s, \(\mu(B_{1n})=\tilde{\mu}(A_{n})\) for all \(n\geq 1\), and by (i), \(\tilde{\mu}(A)=\mu(B_{1})\). Thus, \[\tilde{\mu}(A)=\mu(B_{1})=\sum_{n=1}^{\infty}(B_{1n})=\sum_{n=1}^{\infty} \tilde{\mu}(A_{n}),\] establishing the countable additivity of \(\tilde{\mu}\). Next, suppose that \(A\in\tilde{\cal F}\) and \(\tilde{\mu}(A)=0\). Then there exist \(B_{1},B_{2}\in{\cal F}\) such that \(B_{1}\subset A\subset B_{2}\) and \(\mu(B_{2}\setminus B_{1})=0\). Further, by definition of \(\tilde{\mu}\), \(\mu(B_{2})=\tilde{\mu}(A)=0\). If \(D\subset A\), then \(\emptyset\subset D\subset B_{2}\) and \(\mu(B_{2}\setminus\emptyset)=0\). Therefore, \(D\in\tilde{\cal F}\) and hence \((\Omega,\tilde{\cal F},\tilde{\mu})\) is complete. Finally, if \(A\in{\cal F}\), then take \(B_{1}=B_{2}=A\) and so, \(\tilde{\mu}(A)=\mu(B_{1})=\mu(A)\), and thus, \(\tilde{\mu}=\mu\) on \({\cal F}\). Hence, the proof of the theorem is complete. \(\Box\)

### Problems

1. Let \(\Omega\) be a nonempty set. Show that \({\cal F}\equiv\{\Omega,\emptyset\}\) and \({\cal G}={\cal P}(\Omega)\equiv\{A:A\subset\Omega\}\) are both \(\sigma\)-algebras.
2. Let \(\Omega\) be a finite set, i.e., the number of elements in \(\Omega\) is finite. Let \({\cal F}\subset{\cal P}(\Omega)\) be an algebra. Show that \({\cal F}\) is a \(\sigma\)-algebra.
3. Show that \({\cal F}_{6}\) in Example 1.1.4 is a \(\sigma\)-algebra.

[MISSING_PAGE_FAIL:48]

1.14 Let \(\mathcal{A}_{1},\mathcal{A}_{2}\) and \(\mathcal{A}_{3}\) denote, respectively, the class of triangles, discs, and pentagons in \(\mathbb{R}^{2}\). Show that \(\sigma\langle\mathcal{A}_{i}\rangle\equiv\mathcal{B}(\mathbb{R}^{2})\). Thus, the \(\sigma\)-algebra \(\mathcal{B}(\mathbb{R}^{2})\) (and similarly \(\mathcal{B}(\mathbb{R}^{k})\)) can be generated by starting with various classes of sets of different shapes and geometry.
1.15 Let \(\Omega\) be a nonempty set and \(\mathcal{B}\) be a \(\sigma\)-algebra on \(\Omega\). Let \(A\subset\Omega\) and \(\mathcal{B}_{A}\equiv\{B\cap A:B\in\mathcal{B}\}\). Show that \(\mathcal{B}_{A}\) is a \(\sigma\)-algebra on \(A\). The \(\sigma\)-algebra \(\mathcal{B}_{A}\) is called the _trace \(\sigma\)-algebra of \(\mathcal{B}\) on \(A\)_.
1.16 Let \(\Omega=\mathbb{R}\) and \(\mathcal{F}\) be the collection of all finite unions of disjoint intervals of the form \((a,b]\cap\mathbb{R}\), \(-\infty\leq a<b\leq\infty\). Show that \(\mathcal{F}\) is an algebra but not a \(\sigma\)-algebra.
1.17 Let \(\Omega\) be a nonempty set and \(\{A_{i}\}_{i\in\mathbb{N}}\) be a sequence of subsets of \(\Omega\) such that \(A_{i+1}\subset A_{i}\) for all \(i\in\mathbb{N}\). Verify that \(\mathcal{A}=\{A_{i}:i\in\mathbb{N}\}\) is a \(\pi\)-system and also determine \(\lambda\langle\mathcal{A}\rangle\), the \(\lambda\)-system generated by \(\mathcal{A}\).
1.18 Let \(\Omega\equiv\mathbb{N}\), \(\mathcal{F}=\mathcal{P}(\Omega)\), and \(A_{n}=\{j:j\in\mathbb{N},j\geq n\},n\in\mathbb{N}\). Let \(\mu\) be the counting measure on \((\Omega,\mathcal{F})\). Verify that \(\lim_{n\to\infty}\mu(A_{n})\neq\mu(\bigcap_{n\geq 1}A_{n})\).
1.19 Let \(\Omega\) be a nonempty set and let \(\mathcal{C}\subset\mathcal{P}(\Omega)\) be a semialgebra. Let \[\mathcal{A}(\mathcal{C})\equiv\Big{\{}A:A=\bigcup_{i=1}^{k}B_{i}:B_{i}\in \mathcal{C},i=1,2,\ldots,k,k\in\mathbb{N}\Big{\}}.\] 1. Show that \(\mathcal{A}(\mathcal{C})\) is the smallest algebra containing \(\mathcal{C}\). 2. Show also that \(\sigma\langle\mathcal{C}\rangle=\sigma\langle\mathcal{A}(\mathcal{C})\rangle\).
1.20 Let \(\mu^{*}\) be as in (3.1) of Section 1.3. Verify (3.4)-(3.6). (**Hint:** Fix \(0<\epsilon<\infty\). If \(\mu^{*}(A_{n})<\infty\) for all \(n\in\mathbb{N}\), then find, for each \(n\), a cover \(\{A_{nj}\}_{j\geq 1}\subset\mathcal{C}\) such that \(\mu^{*}(A_{n})\leq\sum_{j=1}^{\infty}\mu(A_{nj})+\frac{\epsilon}{2^{n}}\).)
1.21 Prove Proposition 1.3.1.
1.22 Let \(F:\mathbb{R}\to\mathbb{R}\) be nondecreasing. Let \((a,b],(a_{n},b_{n}],n\in\mathbb{N}\) be intervals in \(\mathbb{R}\) such that \((a,b]=\bigcup_{n\geq 1}(a_{n},b_{n}]\) and \(\{(a_{n},b_{n}]:n\geq 1\}\) are disjoint. Let \(\mu_{F}(\cdot)\) be as in (3.8). Show that \(\mu_{F}((a,b])=\sum_{n=1}^{\infty}\mu_{F}((a_{n},b_{n}])\) by completing the following steps: 1. Let \(G(x)\equiv F(x+)\) for all \(x\in\mathbb{R}\) and let \(G(\pm\infty)=F(\pm\infty)\). Verify that \(G(\cdot)\) is nondecreasing and right continuous on \(\mathbb{R}\) and that for any \(A\) in \(\mathcal{C}\), \(\mu_{F}(A)=\mu_{G}(A)\). 2. In view of (a), assume w.l.o.g. that \(F(\cdot)\) is right continuous. Show that for any \(k\in\mathbb{N}\), \[F(b)-F(a)\geq\sum_{i=1}^{k}(F(b_{i})-F(a_{i})),\]and conclude that \[F(b)-F(a)\geq\sum_{i=1}^{\infty}(F(b_{i})-F(a_{i})).\]
3. To prove the reverse inequality, fix \(\eta>0.\) Choose \(c>a\) and \(d_{n}>b_{n},\)\(n\geq 1\) such that such that \(F(c)-F(a)<\eta,\)\([c,b]\subset\bigcup_{n\geq 1}(a_{n},d_{n})\) and \(F(d_{n})-F(b_{n})<\eta/2^{n}\) for all \(n\in\mathbb{N}.\) Next, apply the Heine-Borel theorem to the interval \([c,b]\) and the open cover \(\{(a_{n},d_{n})\}_{n\geq 1}\) and extract a finite cover \(\{(a_{i},d_{i})\}_{i=1}^{k}\) for \([c,b].\) W.l.o.g., assume that \(c\in(a_{1},d_{1})\) and \(b\in(a_{k},d_{k}).\) Now verify that \[F(b)-F(a) \leq \sum_{j=1}^{k}(F(b_{j})-F(a_{j}))+2\eta\] \[\leq \sum_{j=1}^{\infty}(F(b_{j})-F(a_{j}))+2\eta.\]

1.23 Extend the above arguments to the case when \((a,b]\) and \((a_{i},b_{i}],\)\(i\geq 1\) are not necessarily bounded intervals.

1.24 Verify that \(\mathcal{C}_{2},\) defined in (3.11), is a semialgebra.

1.25 1. Verify that the limit in (3.13) exists. 2. Extend the arguments in Problems 1.22 and 1.23 to verify that \(\mu_{F}\) of (3.12) and (3.13) is a measure on \(\mathcal{C}_{2}.\)

1.26 Establish Theorem 1.3.6 by completing the following:

1. Suppose first that \(\nu(\Omega)<\infty.\) Verify that \(\mathcal{L}\equiv\{A:A\in\sigma\langle\mathcal{C}\rangle,\)\(\mu^{*}(A)=\nu(A)\}\) is a \(\lambda\) system and use the \(\pi\)-\(\lambda\) theorem. 2. Extend the above to the \(\sigma\)-finite case.

1.27 Prove Corollary 1.3.5 for Lebesgue measure \(m(\cdot).\)

1.28 Let \(F\) be a discrete distribution function, i.e., \(F\) is of the form \[F(x)\equiv\sum_{j=1}^{\infty}a_{j}I(x_{j}\leq x),\quad x\in\mathbb{R},\] where \(0<a_{j}<\infty,\)\(\sum_{j\geq 1}a_{j}=1,\)\(x_{j}\in\mathbb{R},\)\(j\geq 1.\) Show that \(\mathcal{M}_{\mu_{F}^{*}}=\mathcal{P}(\mathbb{R}).\) (**Hint:** Show that \(\mu_{F}^{*}(A^{c})=0,\) where \(A\equiv\{x_{j}:j\geq 1\},\) and use the fact that for any \(B\subset\mathbb{R},\)\(B\cap A\in\mathcal{B}(\mathbb{R}).\))1.29 Let \[F(x)=\left\{\begin{array}{lll}0&\mbox{for}&x<0\\ x&\mbox{for}&0\leq x\leq 1\\ 1&\mbox{for}&x>0.\end{array}\right.\] Show that \({\cal M}_{\mu^{*}_{F}}\equiv\{A:A\in{\cal P}({\mathbb{R}}),A\cap[0,1]\in{\cal M}\}\), where \({\cal M}\) is the \(\sigma\)-algebra of Lebesgue measurable sets as in Definition 1.3.7.
1.30 Let \(F(\cdot)=\frac{1}{2}\Phi(\cdot)+\frac{1}{2}F_{P}(\cdot)\) where \(\Phi(\cdot)\) is the standard normal cdf, i.e., \(\Phi(x)\equiv\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{-u^{2}/2}Au\) and \(F_{P}(x)\equiv\sum_{k=0}^{\infty}e^{-2}\frac{2^{k}}{k!}I^{(k)}_{(-\infty,x]}\), \(x\in{\mathbb{R}}\). Let \(F_{1}=\Phi\), \(F_{2}=F_{P}\) and \(F_{3}=F\). Let \(A_{1}=(0,1)\), \(A_{2}=\{x:x\in{\mathbb{R}},\sin x\in(0,\frac{1}{2})\}\), \(A_{3}=\{x:\mbox{ for some integers }a_{0},a_{1},\ldots,a_{k}\), \(k<\infty\), \(\sum_{i=0}^{k}a_{i}x^{i}=0\}\), the set of all algebraic numbers. Compute \(\mu_{F_{i}}(A_{j})\), \(1\leq i\), \(j\leq 3\).
1.31 Let \(\mu\) be a measure on a semialgebra \({\cal C}\subset{\cal P}(\Omega)\) where \(\Omega\) is a nonempty set. Let \(\mu^{*}\) be the outer measure generated by \(\mu\) and let \({\cal M}_{\mu^{*}}\) be the \(\sigma\)-algebra of \(\mu^{*}\)-measurable sets as defined in Theorem 1.3.3. 1. Show that for all \(A\subset\Omega\), there exists a \(B\in\sigma\langle{\cal C}\rangle\) such that \(A\subset B\) and \(\mu^{*}(A)=\mu^{*}(B)\). 2. (**Hint:** If \(\mu^{*}(A)=\infty\), take \(B\) to be \(\Omega\). If \(\mu^{*}(A)<\infty\), use the definition of \(\mu^{*}\) to show that for each \(n\geq 1\), there exists \(\{B_{nj}\}_{j\geq 1}\subset{\cal C}\) such that \(A\subset B_{n}\equiv\bigcup_{j\geq 1}B_{nj}\), \(\mu^{*}(A)\leq\sum_{j=1}^{\infty}\mu(B_{nj})<\mu^{*}(A)+\frac{1}{n}\). Take \(B=\bigcap_{n\geq 1}B_{n}\).) 3. Show that for all \(A\in{\cal M}_{\mu^{*}}\) with \(\mu^{*}(A)<\infty\), there exists \(B\in\sigma\langle{\cal C}\rangle\) such that \(A\subset B\) and \(\mu^{*}(B\setminus A)=0\). (**Hint:** Use (a) and the relation \(B=A\cup(B\setminus A)\) with \(A\) and \(B\setminus A=B\cap A^{c}\) in \({\cal M}_{\mu^{*}}\).) 4. Show that if \(\mu\) is \(\sigma\)-finite (i.e., there exist sets \(\Omega_{n},n\geq 1\) in \({\cal C}\) with \(\mu(\Omega_{n})<\infty\) for all \(n\geq 1\) and \(\bigcup_{n\geq 1}\Omega_{n}=\Omega\)), then in (b), the hypothesis that \(\mu^{*}(A)<\infty\) can be dropped. (**Hint:** Assume w.l.o.g. that \(\{\Omega_{n}\}_{n\geq 1}\) are disjoint. Apply (b) to \(\{A_{n}\equiv A\cap\Omega_{n}\}_{n\geq 1}\).) 5. Show that if \(\mu\) is \(\sigma\)-finite, then \(A\in{\cal M}_{\mu^{*}}\) iff there exist sets \(B_{1},B_{2}\in\sigma\langle{\cal C}\rangle\) such that \(B_{1}\subset A\subset B_{2}\) and \(\mu^{*}(B_{2}\setminus B_{1})=0\). (**Hint:** Apply (c) to both \(A\) and \(A^{c}\).) This shows that \({\cal M}_{\mu^{*}}\) is the completion of \(\sigma\langle{\cal C}\rangle\) w.r.t. \(\mu^{*}\).
1.32 (_An outline of a proof of Corollary 1.3.5_). Let \(({\mathbb{R}},{\cal M}_{\mu^{*}_{F}},\mu^{*}_{F})\) be a Lebesgue Stieltjes measure space generated by a right continuous and nondecreasing function \(F:{\mathbb{R}}\to{\mathbb{R}}\).

1. Measures 1. Show that \(A\in{\cal M}_{\mu^{*}_{F}}\) iff there exist Borel sets \(B_{1}\) and \(B_{2}\in{\cal B}(\mathbb{R})\) such that \(B_{1}\subset A\subset B_{2}\) and \(\mu^{*}_{F}(B_{2}\setminus B_{1})=0\). (**Hint:** Take \({\cal C}\) to be the semialgebra \({\cal C}=\{(a,b]:-\infty\leq a\leq b<\infty\}\cup\{(b,\infty):-\infty\leq b<\infty\}\) and apply Problem 1.31 (d).) 2. Let \(A\in{\cal M}_{\mu^{*}_{F}}\) with \(\mu^{*}_{F}(A)<\infty\). Show that for any \(\epsilon>0\), there exist a finite number of bounded open intervals \(I_{j},j=1,2,\ldots,k\) such that \(\mu^{*}_{F}(A\bigtriangleup\bigcup_{j=1}^{k}I_{j})<\epsilon\). (**Outline:** _Claim: For any \(B\in{\cal C}\) with \(\mu_{F}(B)<\infty\), there exists an open interval \(I\) such that \(\mu^{*}_{F}(I\bigtriangleup B)<\epsilon\)._ To see this, note that if \(B=(a,b],\ -\infty\leq a<b<\infty\), then one may choose \(b^{\prime}>b\) such that \(F(b^{\prime})-F(b)<\epsilon\). Now, with \(I=(a,b^{\prime})\), \(\mu^{*}_{F}(I\bigtriangleup B)=\mu^{*}_{F}((b,b^{\prime}))=\mu_{F}((b,b^{ \prime}))\leq F(b^{\prime})-F(b)<\epsilon\). If \(B=(b,\infty)\) and \(\mu_{F}(B)<\infty\), then there exists \(b^{\prime}>b\) such that \(F(\infty)-F(b^{\prime}-)<\epsilon\). Hence, with \(I=(b,b^{\prime})\), \(\mu^{*}_{F}(I\bigtriangleup B)=\mu^{*}_{F}([b^{\prime},\infty))=F(\infty)-F(b^ {\prime}-)<\epsilon\). This proves the claim. Next, By Theorem 1.3.4, for all \(\epsilon>0\), there exist \(B_{1},B_{2},\ldots,B_{k}\in{\cal C}\) such that \(\mu^{*}_{F}(A\bigtriangleup\bigcup_{j=1}^{k}B_{j})<\epsilon/2\). For each \(B_{j}\), find \(I_{j}\), a bounded open interval such that \(\mu^{*}_{F}(B_{j}\bigtriangleup I_{j})<\frac{\epsilon}{2^{j}}\). Since \((A_{1}\cup A_{2})\bigtriangleup(C_{1}\cup C_{2})\subset(A_{1}\bigtriangleup C _{1})\cup(A_{2}\bigtriangleup C_{2})\) for any \(A_{1},A_{2},C_{1},C_{2}\subset\Omega\), it follows that \[\mu^{*}_{F}\Big{(}\Big{[}\bigcup_{j=1}^{k}B_{j}\Big{]}\bigtriangleup\Big{[} \bigcup_{j=1}^{k}I_{j}\Big{]}\Big{)}<\sum_{j=1}^{k}\mu^{*}_{F}(B_{j}\bigtriangleup I _{j})<\frac{\epsilon}{2}.\] Hence, \(\mu^{*}_{F}(A\bigtriangleup[\bigcup_{j=1}^{k}I_{j}])<\epsilon\).) 3. Let \(A\in{\cal M}_{\mu^{*}_{F}}\) with \(\mu^{*}_{F}(A)<\infty\). Show that for every \(\epsilon>0\), there exists an open set \(O\) such that \(A\subset O\) and \(\mu^{*}_{F}(O\setminus A)<\epsilon\). (**Hint:** By definition of \(\mu^{*}_{F}\), for every \(\epsilon>0\), there exist \(\{B_{j}\}_{j\geq 1}\subset{\cal C}\) such that \(A\subset\bigcup_{j\geq 1}B_{j}\) and \(\mu^{*}_{F}(A)\leq\sum_{j=1}^{\infty}\mu_{F}(B_{j})\leq\mu^{*}_{F}(A)+\epsilon\). Now as in (b), there exist open intervals \(I_{j}\) such that \(B_{j}\subset I_{j}\) and \(\mu^{*}_{F}(I_{j}\setminus B_{j})<\epsilon/2^{j}\) for all \(j\geq 1\). Then \(A\subset\bigcup_{j=1}^{\infty}B_{j}\subset\bigcup_{j=1}^{\infty}I_{j}\equiv O\). Also, \(\mu^{*}_{F}(O)=\mu^{*}_{F}(A)+\mu^{*}_{F}(O\setminus A)\Rightarrow\mu^{*}_{F}( O\setminus A)=\mu^{*}_{F}(O)-\mu^{*}(A)<2\epsilon\) (since \(\mu^{*}(O)\leq\sum_{j=1}^{\infty}\mu^{*}(I_{j})=\sum_{j=1}^{\infty}\mu^{*}_{F}( B_{j})+\epsilon\leq\mu^{*}_{F}(A)+2\epsilon<\infty\)).) 4. Extend (c) to all \(A\in{\cal M}_{\mu^{*}_{F}}\). (**Hint:** Let \(A_{i}=A\cap[i,i+1]\), \(i\in\mathbb{Z}\). Apply (c) to \(A_{i}\) with \(\epsilon_{i}=\frac{\epsilon}{2^{|i|+1}}\) and take unions.) 5. Show that for all \(A\in{\cal M}_{\mu^{*}_{F}}\) and for all \(\epsilon>0\), there exist a closed set \(C\) and an open set \(O\) such that \(C\subset A\subset O\) and \(\mu^{*}_{F}(A)<\infty\). (**Hint:** Let \(A_{i}=A\cap[i,i+1]\), \(i\in\mathbb{Z}\). Apply (d) to \(A_{i}\) with \(\epsilon_{i}=\frac{\epsilon}{2^{|i|+1}}\) and take unions.)\(\mu_{F}^{*}(O\setminus A)\leq\epsilon\), \(\mu^{*}(A\setminus C)<\epsilon\). (**Hint:** Apply (d) to \(A\) and \(A^{c}\).) (f) Show that for all \(A\in{\cal M}_{\mu_{F}^{*}}\) with \(\mu_{F}^{*}(A)<\infty\) and for all \(\epsilon>0\), there exist a closed and bounded set \(F\subset A\) such that \(\mu_{F}^{*}(A\setminus F)<\epsilon\) and an open set \(O\) with \(A\subset O\) such that \(\mu_{F}^{*}(O\setminus A)<\epsilon\). (**Hint:** Apply (d) to \(A\cap[-M,M]\) where \(M\) is chosen so that \(\mu_{F}^{*}(A\cap[-M,M]^{c})<\epsilon\). Why is this possible?) **Remark:** Thus for all \(A\in{\cal M}_{\mu_{F}^{*}}\) with \(\mu_{F}^{*}(A)<\infty\) and for all \(\epsilon>0\), there exist a compact set \(K\subset A\) and an open set \(O\supset A\) such that \(\mu_{F}^{*}(A\setminus K)<\epsilon\) and \(\mu_{F}^{*}(O\setminus A)<\epsilon\). The first property is called _inner regularity of \(\mu_{F}^{*}\)_ and the second property is called _outer regularity_ of \(\mu_{F}^{*}\). (g) Show that for all \(A\in{\cal M}_{\mu_{F}^{*}}\) with \(\mu_{F}^{*}(A)<\infty\) and for all \(\epsilon>0\), there exists a continuous function \(g_{\epsilon}\) with compact support (i.e., \(g_{\epsilon}(x)\) is zero for \(|x|\) large) such that \[\mu_{F}^{*}(A\bigtriangleup\ g_{\epsilon}^{-1}\{1\})<\epsilon.\] (**Hint:** For any bounded open interval \((a,b)\), let \(\eta>0\) be such that \(\mu_{F}((a,a+\eta])+\mu_{F}([b-\eta,b))<\epsilon\). Next define \[g_{\epsilon}(x)=\left\{\begin{array}{lcl}1&&\mbox{if}\quad a+\eta\leq x\leq b -\eta\\ 0&&\mbox{if}\quad x\not\in(a,b)\\ \mbox{linear}&&\mbox{over}\quad[a,a+\eta]\cup[b-\eta,b].\end{array}\right.\] Then \(g_{\epsilon}\) is continuous with compact support. Also, \(g^{-1}\{1\}=[a+\eta,b-\eta]\) and \((a,b)\bigtriangleup g^{-1}\{1\}=(a,a+\eta)\cup(b-\eta,b)\). So \(\mu_{F}\{(a,b)\bigtriangleup g_{\epsilon}^{-1}(1)\}|<\epsilon\), proving the claim for \(A=(a,b)\). The general case follows from (b).) (h) Show that for all \(A\in{\cal M}_{\mu_{F}^{*}}\) and for all \(\epsilon>0\), there exists a continuous function \(g_{\epsilon}\) (not necessarily with compact support) such that \(\mu_{F}^{*}(A\bigtriangleup g_{\epsilon}^{-1}\{1\})<\epsilon\) (i.e., drop the condition \(\mu_{F}^{*}(A)<\infty\)). (**Hint:** Let \(A_{k}=A\cap[k,k+1]\), \(k\in{\mathbb{Z}}\). Find \(g_{k}:{\mathbb{R}}\to{\mathbb{R}}\) continuous with support in \(\big{(}k-\frac{1}{8},k+\frac{9}{8}\big{)}\) such that \(\mu_{F}^{*}(I_{A_{k}}\neq g_{k})<\frac{\epsilon}{2^{|k|+1}}\). Let \(g=\sum_{k\in{\mathbb{Z}}}g_{k}\). Note that for any \(x\in{\mathbb{R}}\), at most two \(g_{k}(x)\neq 0\) and so \(g\) is continuous. Also, \(\mu_{F}^{*}(I_{A}\neq g)\leq\sum_{k\in{\mathbb{Z}}}\mu_{F}^{*}(I_{A_{k}}\neq g _{k})<\epsilon\).)
1. [label=.33]
2. Let \(C\) be the Cantor set in [0,1] as defined in Section 1.3.2. 1. Show that \[C=\Big{\{}x:x=\sum_{i=1}^{\infty}\frac{a_{i}}{3^{i}},\ a_{i}\in\{0,2\}\Big{\}}.\]and hence that \(C\) is uncountable. 2. Show that \[C+C\equiv\{x+y:x,y\in C\}=[0,2].\]Integration

### 2.1 Measurable transformations

Oftentimes, one is not interested in the full details of a measure space \((\Omega,\mathcal{F},\mu)\) but only in certain functions defined on \(\Omega\). For example, if \(\Omega\) represents the outcomes of 10 tosses of a fair coin, one may only be interested in knowing the number of heads in the 10 tosses. It turns out that to assign measures (probabilities) to sets (events) involving such functions, one can allow only certain functions (called measurable functions) that satisfy some 'natural' restrictions, specified in the following definitions.

**Definition 2.1.1:** Let \(\Omega\) be a nonempty set and let \(\mathcal{F}\) be a \(\sigma\)-algebra on \(\Omega\). Then the pair \((\Omega,\mathcal{F})\) is called a _measurable space_. If \(\mu\) is a measure on \((\Omega,\mathcal{F})\), then the triplet \((\Omega,\mathcal{F},\mu)\) is called a _measure space_. If in addition, \(\mu\) is a probability measure, then \((\Omega,\mathcal{F},\mu)\) is called a _probability space_.

**Definition 2.1.2:**

1. Let \((\Omega,\mathcal{F})\) be a measurable space. Then a function \(f:\Omega\) to \(\mathbb{R}\) is called _\(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable_ (or \(\mathcal{F}\)-_measurable_) if for each \(a\) in \(\mathbb{R}\) \[f^{-1}\big{(}(-\infty,a]\big{)}\equiv\{\omega:f(\omega)\leq a\}\in\mathcal{F}.\] (1.1)
2. Let \((\Omega,\mathcal{F},P)\) be a probability space. Then a function \(X:\Omega\to\mathbb{R}\) is called a _random variable_, if the event \[X^{-1}((-\infty,a])\equiv\{\omega:X(\omega)\leq a\}\in\mathcal{F}\]for each \(a\) in \(\mathbb{R}\), i.e., a random variable is a real valued \(\mathcal{F}\)-measurable function on a probability space \((\Omega,\mathcal{F},P)\).

It will be shown later that condition (1.1) on \(f\) is equivalent to the stronger condition that \(f^{-1}(A)\in\mathcal{F}\) for all Borel sets \(A\in\mathcal{B}(\mathbb{R})\). Since for any Borel set \(A\in\mathcal{B}(\mathbb{R})\), \(f^{-1}(A)\) is a member of the underlying \(\sigma\)-algebra \(\mathcal{F}\), one can assign a measure to the set \(f^{-1}(A)\) using a measure \(\mu\) on \((\Omega,\mathcal{F})\). Note that for an _arbitrary_ function \(T\) from \(\Omega\to\mathbb{R}\), \(T^{-1}(A)\) need not be a member of \(\mathcal{F}\) and hence such an assignment may _not_ be possible. Thus, condition (1.1) on real valued mappings is a 'natural' requirement while dealing with measure spaces.

The following definition generalizes (1.1) to maps between two measurable spaces.

**Definition 2.1.3:** Let \((\Omega_{i},\mathcal{F}_{i}),i=1,2\) be measurable spaces. Then, a mapping \(T:\Omega_{1}\to\Omega_{2}\) is called _measurable with respect to the \(\sigma\)-algebras \(\langle\mathcal{F}_{1},\mathcal{F}_{2}\rangle\)_ (or \(\langle\mathcal{F}_{1},\mathcal{F}_{2}\rangle\)_-measurable_) if

\[T^{-1}(A)\in\mathcal{F}_{1}\quad\mbox{for all}\quad A\in\mathcal{F}_{2}.\]

Thus, \(X\) is a random variable on a probability space \((\Omega,\mathcal{F},P)\) iff \(X\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable. Some examples of measurable transformations are given below.

**Example 2.1.1:** Let \(\Omega=\{a,b,c,d\},\mathcal{F}_{2}=\{\Omega,\emptyset,\{a\},\{b,c,d\}\}\) and let \(\mathcal{F}_{3}=\) the set of all subsets of \(\Omega\). Define the mappings \(T_{i}:\Omega\to\Omega,i=1,2\), by

\[T_{1}(\omega)\equiv a\mbox{ for }\omega\in\Omega\]

and

\[T_{2}(\omega)=\left\{\begin{array}{ll}a&\mbox{if }\omega=a,b\\ c&\mbox{if }\omega=c,d.\end{array}\right.\]

Then, \(T_{1}\) is \(\langle\mathcal{F}_{2},\mathcal{F}_{3}\rangle\)-measurable since for any \(A\in\mathcal{F}_{3}\), \(T_{1}^{-1}(A)=\Omega\) or \(\emptyset\) according as \(a\in A\) or \(a\not\in A\). By similar arguments, it follows that \(T_{2}\) is \(\langle\mathcal{F}_{3},\mathcal{F}_{2}\rangle\)-measurable. However, \(T_{2}\) is not \(\langle\mathcal{F}_{2},\mathcal{F}_{3}\rangle\)-measurable since \(T_{2}^{-1}(\{a\})=\{a,b\}\not\in\mathcal{F}_{2}\).

As this simple example shows, measurability of a given mapping critically depends on the \(\sigma\)-algebras on its domain and range spaces. In general, if \(T\) is \(\langle\mathcal{F}_{1},\mathcal{F}_{2}\rangle\)-measurable, then \(T\) is \(\langle\tilde{\mathcal{F}}_{1},\mathcal{F}_{2}\rangle\)-measurable for any \(\sigma\)-algebra \(\tilde{\mathcal{F}}_{1}\supset\mathcal{F}_{1}\) and \(T\) is \(\langle\mathcal{F}_{1},\tilde{\mathcal{F}}_{2}\rangle\)-measurable for any \(\tilde{\mathcal{F}}_{2}\subset\mathcal{F}_{2}\).

**Example 2.1.2:** Let \(T:\mathbb{R}\to\mathbb{R}\) be defined as

\[T(x)=\left\{\begin{array}{ll}\sin 2x&\mbox{if }x>0\\ 1+\cos x&\mbox{if }x\leq 0.\end{array}\right.\]Is \(T\) measurable w.r.t. the Borel \(\sigma\)-algebras \(\langle{\cal B}({\mathbb{R}}),{\cal B}({\mathbb{R}})\rangle\)? If one is to apply the definition directly, one must check that \(T^{-1}(A)\in{\cal B}({\mathbb{R}})\) for all \(A\in{\cal B}({\mathbb{R}})\). However, finding \(T^{-1}(A)\) for all Borel sets \(A\) is not an easy task. In many instances like this one, verification of the measurability property of a given mapping by directly using the definition can be difficult. In such situations, one may use some easy-to-verify sufficient conditions. Some results of this type are given below.

**Proposition 2.1.1:** _Let \((\Omega_{i},{\cal F}_{i})\), \(i=1,2,3\) be measurable spaces._

* _Suppose that_ \({\cal F}_{2}=\sigma\langle{\cal A}\rangle\) _for some class of subsets_ \({\cal A}\) _of_ \(\Omega_{2}\)_. If_ \(T:\Omega_{1}\to\Omega_{2}\) _is such that_ \(T^{-1}(A)\in{\cal F}_{1}\) _for all_ \(A\in{\cal A}\)_, then_ \(T\) _is_ \(\langle{\cal F}_{1},{\cal F}_{2}\rangle\)_-measurable._
* _Suppose that_ \(T_{1}:\Omega_{1}\to\Omega_{2}\) _is_ \(\langle{\cal F}_{1},{\cal F}_{2}\rangle\)_-measurable and_ \(T_{2}:\Omega_{2}\to\Omega_{3}\) _is_ \(\langle{\cal F}_{2},{\cal F}_{3}\rangle\)_-measurable. Let_ \(T=T_{2}\circ T_{1}:\Omega_{1}\to\Omega_{3}\) _denote the composition of_ \(T_{1}\) _and_ \(T_{2}\)_, defined by_ \(T(\omega_{1})=T_{2}(T_{1}(\omega_{1})),\ \omega_{1}\in\Omega_{1}\)_. Then,_ \(T\) _is_ \(\langle{\cal F}_{1},{\cal F}_{3}\rangle\)_-measurable._

**Proof:**

* Define the collection of sets \[{\cal F}=\{A\in{\cal F}_{2}:T^{-1}(A)\in{\cal F}_{1}\}.\] Then,
* \(T^{-1}(\Omega_{2})=\Omega_{1}\in{\cal F}_{1}\Rightarrow\Omega_{2}\in{\cal F}\).
* If \(A\in{\cal F}\), then \(T^{-1}(A)\in{\cal F}_{1}\Rightarrow(T^{-1}(A))^{c}\in{\cal F}_{1}\Rightarrow T ^{-1}(A^{c})=(T^{-1}(A))^{c}\in{\cal F}_{1}\), implying \(A^{c}\in{\cal F}\).
* If \(A_{1},A_{2},\ldots,\in{\cal F}\), then, \(T^{-1}(A_{i})\in{\cal F}_{1}\) for all \(i\geq 1\). Since \({\cal F}_{1}\) is a \(\sigma\)-algebra, \(T^{-1}(\bigcup_{n\geq 1}A_{n})=\bigcup_{n\geq 1}T^{-1}(A_{n})\in{\cal F}_{1}\). Thus, \(\bigcup_{n\geq 1}A_{n}\in{\cal F}\). (See also Problem 2.1 on de Morgan's laws.) Hence, by (a), (b), (c), \({\cal F}\) is a \(\sigma\)-algebra and by hypothesis \({\cal A}\subset{\cal F}\). Hence, \({\cal F}_{2}=\sigma\langle{\cal A}\rangle\subset{\cal F}\subset{\cal F}_{2}\). Thus, \({\cal F}={\cal F}_{2}\) and \(T\) is \(\langle{\cal F}_{1},{\cal F}_{2}\rangle\)-measurable.
* Let \(A\in{\cal F}_{3}\). Then, \(T_{2}^{-1}(A)\in{\cal F}_{2}\), since \(T_{2}\) is \(\langle{\cal F}_{2},{\cal F}_{3}\rangle\)-measurable. Also, by the \(\langle{\cal F}_{1},{\cal F}_{2}\rangle\)-measurability of \(T_{1}\), \(T^{-1}(A)=T_{1}^{-1}(T_{2}^{-1}(A))\in{\cal F}_{1}\), showing that \(T\) is \(\langle{\cal F}_{1},{\cal F}_{3}\rangle\)-measurable. \(\Box\)

**Proposition 2.1.2:** _For any \(k,p\in{\mathbb{N}}\), if \(f:{\mathbb{R}}^{p}\to{\mathbb{R}}^{k}\) is continuous, then \(f\) is \(\langle{\cal B}({\mathbb{R}}^{p}),{\cal B}({\mathbb{R}}^{k})\rangle\)-measurable._

**Proof:** Let \({\cal A}=\{A:A\) is an open set in \({\mathbb{R}}^{k}\}\). Then, by the continuity of \(f\), \(f^{-1}(A)\) is open and hence, is in \({\cal B}({\mathbb{R}}^{p})\) (cf. Section A.4). Thus,\(\mathbb{R}^{p}\) for all \(A\in\mathcal{A}\). Since \(\mathcal{B}(\mathbb{R}^{k})=\sigma\langle\mathcal{A}\rangle\), by Proposition 2.1.1 (a), \(f\) is \(\langle\mathcal{B}(\mathbb{R}^{p}),\mathcal{B}(\mathbb{R}^{k})\rangle\)-measurable. \(\Box\)

Although the converse to the above proposition is not true, a result due to Lusin says that except on a set of small measure, \(f\) coincides with a continuous function. This is stronger than the statement that except on set of small measure, \(f\) is close to a continuous function. For the statement and proof of Lusin's theorem, see Theorem 2.5.12.

**Proposition 2.1.3:** _Let \(f_{1},\ldots,f_{k}\)\((k\in\mathbb{N})\) be \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable transformations from \(\Omega\) to \(\mathbb{R}\). Then,_

* \(f=(f_{1},\ldots,f_{k})\) _is_ \(\langle\mathcal{F},\mathcal{B}(\mathbb{R}^{k})\rangle\)_-measurable._
* \(g=f_{1}+\ldots+f_{k}\) _is_ \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)_-measurable._
* \(h\equiv\prod_{i=1}^{k}f_{i}\) _is_ \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)_-measurable._
* _Let_ \(p\in\mathbb{N}\) _and let_ \(\psi:\mathbb{R}^{k}\to\mathbb{R}^{p}\) _be continuous. Then,_ \(\xi\equiv\psi\circ f\) _is_ \(\langle\mathcal{F},\mathcal{B}(\mathbb{R}^{p})\rangle\)_-measurable, where_ \(f=(f_{1},\ldots,f_{k})\)_._

**Proof:** To prove (i), note that for any rectangle \(R=(a_{1},b_{1})\times\ldots\times(a_{k},b_{k})\),

\[f^{-1}(R) = \{\omega\in\Omega:a_{1}<f_{1}(\omega)<b_{1},\ldots,a_{k}<f_{k}( \omega)<b_{k}\}\] \[= \bigcap_{i=1}^{k}\{\omega\in\Omega:a_{i}<f_{1}(\omega)<b_{i}\}\] \[= \bigcap_{i=1}^{k}f_{i}^{-1}(a_{i},b_{i})\in\mathcal{F},\]

since each \(f_{i}\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable. Hence, by Proposition 2.1.1 (i), \(f\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R}^{k})\rangle\)-measurable. To prove (ii), note that the function \(g_{1}(x)\equiv x_{1}+\ldots+x_{k}\), \(x=(x_{1},\ldots,x_{k})\in\mathbb{R}^{k}\) is continuous on \(\mathbb{R}^{k}\), and hence, by Proposition 2.1.2, is \(\langle\mathcal{B}(\mathbb{R}^{k}),\mathcal{B}(\mathbb{R})\rangle\)-measurable. Since \(g=g_{1}\circ f\), \(g\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable, by Proposition 2.1.1 (ii). The proofs of (iii) and (iv) are similar to that of (ii) and hence, are omitted. \(\Box\)

**Corollary 2.1.4:** _The collection of \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable functions from \(\Omega\) to \(\mathbb{R}\) is closed under pointwise addition and multiplication as well as under scalar multiplication._

The proof of Corollary 2.1.4 is omitted.

In view of the above, writing the function \(T\) of Example 2.1.2 as

\[T(x)=(\sin 2x)I_{(0,\infty)}(x)+(1+\cos x)I_{(-\infty,0]}(x),\]

\(x\in\mathbb{R}\), the \(\langle\mathcal{B}(\mathbb{R}),\mathcal{B}(\mathbb{R})\rangle\)-measurability of \(T\) follows. Note that here \(T\) is not continuous over \(\mathbb{R}\) but only piecewise continuous (see also Problem 2.2).

Next, measurability of the limit of a sequence of measurable functions is considered. Let \(\bar{\mathbb{R}}=\mathbb{R}\cup\{+\infty,-\infty\}\) denote the extended real line and let \(\mathcal{B}(\bar{\mathbb{R}})\equiv\sigma\langle\mathcal{B}(\mathbb{R})\cup\{+ \infty\}\cup\{-\infty\}\rangle\) denote the extended Borel \(\sigma\)-algebra on \(\bar{\mathbb{R}}\).

**Proposition 2.1.5:** _For each \(n\in\mathbb{N}\), let \(f_{n}:\Omega\to\bar{\mathbb{R}}\) be a \(\langle\mathcal{F},\mathcal{B}(\bar{\mathbb{R}})\rangle\)-measurable function._

* _Then, each of the functions_ \(\sup_{n\in\mathbb{N}}f_{n}\)_,_ \(\inf_{n\in\mathbb{N}}f_{n}\)_,_ \(\limsup_{n\to\infty}f_{n}\)_, and_ \(\liminf_{n\to\infty}f_{n}\) _is_ \(\langle\mathcal{F},\mathcal{B}(\bar{\mathbb{R}})\rangle\)_-measurable._
* _The set_ \(A\equiv\{\omega:\lim_{n\to\infty}f_{n}(\omega)\) _exists and is finite_\(\}\) _lies in_ \(\mathcal{F}\) _and the function_ \(h\equiv(\lim_{n\to\infty}f_{n})\cdot I_{A}\) _is_ \(\langle\mathcal{F},\mathcal{B}(\bar{\mathbb{R}})\rangle\)_-measurable._

**Proof:**

* Let \(g=\sup_{n\geq 1}f_{n}\). To show that \(g\) is \(\langle\mathcal{F},\mathcal{B}(\bar{\mathbb{R}})\rangle\)-measurable, it is enough to show that \(\{\omega:g(\omega)\leq r\}\in\mathcal{F}\) for all \(r\in\mathbb{R}\) (cf. Problem 2.4). Now, for any \(r\in\mathbb{R}\), \[\{\omega:g(\omega)\leq r\} = \bigcap_{n=1}^{\infty}\{\omega:f_{n}(\omega)\leq r\}\] \[= \bigcap_{n=1}^{\infty}f_{n}^{-1}((-\infty,r])\in\mathcal{F},\] since \(f_{n}^{-1}((-\infty,r])\in\mathcal{F}\) for all \(n\geq 1\), by the measurability of \(f_{n}\). Next note that \(\inf_{n\geq 1}f_{n}=-\sup_{n\geq 1}(-f_{n})\) and hence, \(\inf_{n\geq 1}f_{n}\) is \(\langle\mathcal{F},\mathcal{B}(\bar{\mathbb{R}})\rangle\)-measurable. To prove the measurability of \(\limsup_{n\to\infty}f_{n}\), define the functions \(g_{m}=\sup_{n\geq m}f_{n}\), \(m\geq 1\). Then, \(g_{m}\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable for each \(m\geq 1\) and since \(g_{m}\) is nonincreasing in \(m\), \(\inf_{m\geq 1}g_{m}\equiv\limsup_{n\to\infty}f_{n}\) is also \(\langle\mathcal{F},\mathcal{B}(\bar{\mathbb{R}})\rangle\)-measurable. A similar argument works for \(\liminf_{n\to\infty}f_{n}\).
* Let \(h_{1}=\limsup_{n\to\infty}f_{n}\) and \(h_{2}=\liminf_{n\to\infty}f_{n}\), and define \(\tilde{h}_{i}=h_{i}I_{\mathbb{R}}(h_{i})\), \(i=1,2\). Note that \(\tilde{h}_{1}-\tilde{h}_{2}\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable. Hence, \[\{\omega:\lim_{n\to\infty}f_{n}(\omega)\quad\text{exists and is finite}\}\] \[= \{\omega:-\infty<\limsup_{n\to\infty}f_{n}(\omega)=\liminf_{n\to \infty}f_{n}(\omega)<\infty\}\] \[= \{\omega:-\infty<h_{2}(\omega)=h_{1}(\omega)<\infty\}\] \[= \{\omega:\tilde{h}_{1}(\omega)=\tilde{h}_{2}(\omega)\}\cap\{ \omega:h_{1}(\omega)<\infty,h_{2}(\omega)>-\infty\}\] \[= (\tilde{h}_{1}-\tilde{h}_{2})^{-1}(\{0\})\cap\{\omega:h_{1}( \omega)<\infty,h_{2}(\omega)>-\infty\}\in\mathcal{F}.\] Finally, note that \(h=h_{1}I_{A}\). \(\Box\)

**Definition 2.1.4:** Let \(\{f_{\lambda}:\lambda\in\Lambda\}\) be a family of mappings from \(\Omega_{1}\) into \(\Omega_{2}\) and let \({\cal F}_{2}\) be a \(\sigma\)-algebra on \(\Omega_{2}.\) Then,

\[\sigma\langle\{f_{\lambda}^{-1}(A):A\in{\cal F}_{2},\lambda\in\Lambda\}\rangle\]

is called the \(\sigma\)_-algebra generated by_\(\{f_{\lambda}:\lambda\in\Lambda\}\) (w.r.t. \({\cal F}_{2}\)) and is denoted by \(\sigma\langle\{f_{\lambda}:\lambda\in\Lambda\}\rangle.\)

Note that \(\sigma\langle\{f_{\lambda}:\lambda\in\Lambda\}\rangle\) is the smallest \(\sigma\)-algebra on \(\Omega_{1}\) that makes all \(f_{\lambda}\)'s measurable w.r.t. \({\cal F}_{2}\) on \(\Omega_{2}.\)

**Example 2.1.3:** Let \(f=I_{A}\) for some set \(A\subset\Omega_{1}\) and \(\Omega_{2}={\mathbb{R}}\) and \({\cal F}_{2}={\cal B}({\mathbb{R}}).\) Then,

\[\sigma\langle\{f\}\rangle=\sigma\langle\{A\}\rangle=\{\Omega_{1},\emptyset,A,A ^{c}\}.\]

**Example 2.1.4:** Let \(\Omega_{1}={\mathbb{R}}^{k},\)\(\Omega_{2}={\mathbb{R}},\)\({\cal F}_{2}={\cal B}({\mathbb{R}}),\) and for \(1\leq i\leq k,\) let \(f_{i}:\Omega_{1}\rightarrow\Omega_{2}\) be defined as

\[f_{i}(x_{1},\ldots,x_{k})=x_{i},\qquad(x_{1},\ldots,x_{k})\in\Omega_{1}={ \mathbb{R}}^{k}.\]

Then, \(\sigma\langle\{f_{i}:1\leq i\leq k\}\rangle={\cal B}({\mathbb{R}}^{k}).\)

To show this, note that any measurable rectangle \(A_{1}\times\ldots\times A_{k}\) can be written as \(A_{1}\times\ldots\times A_{k}=\bigcap_{i=1}^{k}f_{i}^{-1}(A_{i})\) and hence \(A_{1}\times\ldots\times A_{k}\in\sigma\langle\{f_{i}:1\leq i\leq k\}\rangle\) for all \(A_{1},\ldots,A_{k}\in{\mathbb{R}}.\) Since \({\mathbb{R}}^{k}\) is generated by the collection of all measurable rectangles, \({\cal B}({\mathbb{R}}^{k})\subset\sigma\langle\{f_{i}:1\leq i\leq k\}\rangle.\) Conversely, for any \(A\in{\cal B}({\mathbb{R}})\) and for any \(1\leq i\leq k,\)\(f_{i}^{-1}(A)={\mathbb{R}}\times\ldots\times A\times\ldots\times{\mathbb{R}}\) (with \(A\) in the \(i\)th position) is in \({\cal B}({\mathbb{R}}^{k}).\) Therefore, \(\sigma\langle\{f_{i}:1\leq i\leq k\}\rangle=\sigma\langle\{f_{i}^{-1}(A):A\in{ \mathbb{R}},1\leq i\leq k\}\rangle\subset{\cal B}({\mathbb{R}}^{k}).\) Hence, \(\sigma\langle\{f_{i}:1\leq i\leq k\}\rangle={\cal B}({\mathbb{R}}^{k}).\)

**Proposition 2.1.6:** _Let \(\{f_{\lambda}:\lambda\in\Lambda\}\) be an uncountable collection of maps from \(\Omega_{1}\) to \(\Omega_{2}.\) Then for any \(B\in\sigma\langle\{f_{\lambda}:\lambda\in\Lambda\}\rangle\), there exists a countable set \(\Lambda_{B}\subset\Lambda\) such that \(B\in\sigma\langle\{f_{\lambda}:\lambda\in\Lambda_{B}\}\rangle\)._

**Proof:** The proof of this result is left as an exercise (Problem 2.5). \(\Box\)

### 2.2 Induced measures, distribution functions

Suppose \(X\) is a random variable defined on a probability space \((\Omega,{\cal F},P).\) Then \(P\) governs the probabilities assigned to events like \(X^{-1}([a,b]),-\infty<a<b<\infty.\) Since \(X\) takes values in the real line, one should be able to express such probabilities only as a function of the set \([a,b].\) Clearly, since \(X\) is \(\langle{\cal F},{\mathbb{R}}\rangle\)-measurable, \(X^{-1}(A)\in{\cal F}\) for all \(A\in{\cal B}({\mathbb{R}})\) and the function

\[P_{X}(A)\equiv P(X^{-1}(A)) \tag{2.1}\]is a set function defined on \({\cal B}({\mathbb{R}})\). Is this a (probability) measure on \({\cal B}({\mathbb{R}})\)? The following proposition answers the question more generally.

**Proposition 2.2.1:** _Let \((\Omega_{i},{\cal F}_{i}),\ i=1,2\) be measurable spaces and let \(T:\Omega_{1}\to\Omega_{2}\) be a \(\langle{\cal F}_{1},{\cal F}_{2}\rangle\)-measurable mapping from \(\Omega_{1}\) to \(\Omega_{2}\). Then, for any measure \(\mu\) on \((\Omega_{1},{\cal F}_{1})\), the set function \(\mu T^{-1}\), defined by_

\[\mu T^{-1}(A)\equiv\mu(T^{-1}(A)),\ \ A\in{\cal F}_{2} \tag{2.2}\]

_is a measure on \({\cal F}_{2}\)._

**Proof:** It is easy to check that \(\mu T^{-1}\) satisfies the three conditions for being a measure. The details are left as an exercise (cf. Problem 2.9).

**Definition 2.2.1:** The measure \(\mu T^{-1}\) is called the _measure induced by \(T\)_ (or the _induced measure of \(T\)_) on \({\cal F}_{2}\).

In particular, if \(\mu(\Omega_{1})=1\), then \(\mu T^{-1}(\Omega_{2})=1\). Hence, the set function \(P\) defined in (2.1) is indeed a probability measure on \(({\mathbb{R}},{\cal B}({\mathbb{R}}))\).

**Definition 2.2.2:** For a random variable \(X\) defined on a probability space \((\Omega,{\cal F},P)\), the _probability distribution_ of \(X\) (or the _law_ of \(X\)), denoted by \(P_{X}\) (say), is the induced measure of \(X\) under \(P\) on \({\mathbb{R}}\), as defined in (2.2).

In introductory courses on probability and statistics, one defines probabilities of events like '\(X\in[a,b]\)' by using the probability mass function for discrete random variables and the probability density function for 'continuous' random variables. The measure-theoretic definition above allows one to treat both these cases as well as the case of'mixed' distributions under a unified framework.

**Definition 2.2.3:** The _cumulative distribution function_ (or _cdf_ in short) _of a random variable \(X\)_ is defined as

\[F_{X}(x)\equiv P_{X}((-\infty,x]),\ \ x\in{\mathbb{R}}. \tag{2.3}\]

**Proposition 2.2.2:** _Let \(F\) be the cdf of a random variable \(X\)._

* _For_ \(x_{1}<x_{2}\)_,_ \(F(x_{1})\leq F(x_{2})\) _(i.e.,_ \(F\) _is nondecreasing on_ \({\mathbb{R}}\)_)._
* _For_ \(x\) _in_ \({\mathbb{R}}\)_,_ \(F(x)=\lim_{y\downarrow x}F(y)\) _(i.e.,_ \(F\) _is right continuous on_ \({\mathbb{R}}\)_)._
* \(\lim_{x\to-\infty}F(x)=0\) _and_ \(\lim_{x\to+\infty}F(x)=1\)_._

**Proof:** For \(x_{1}<x_{2},(-\infty,x_{1}]\subset(-\infty,x_{2}]\). Since \(P_{X}\) is a measure on \({\cal B}({\mathbb{R}})\),

\[F(x_{1})=P_{X}((-\infty,x_{1}])\leq P_{X}((-\infty,x_{2}])=F(x_{2}),\]

proving (i).

To prove (ii), let \(x_{n}\downarrow x\). Then, the sets \((-\infty,x_{n}]\downarrow(-\infty,x]\), and \(P_{X}((-\infty,x_{1}])=P(X\leq x_{1})\leq 1\). Hence, using the monotone continuity from above of the measure \(P_{X}\) (m.c.f.a.) (cf. Proposition 1.2.3), one gets

\[\lim_{n\to\infty}F(x_{n})=\lim_{n\to\infty}P_{X}((-\infty,x_{n}])=P_{X}((- \infty,x])=F(x).\]

Next consider part (iii). Note that if \(x_{n}\downarrow-\infty\) and \(y_{n}\uparrow\infty\), then \((-\infty,x_{n}]\downarrow\emptyset\) and \((-\infty,y_{n}]\uparrow(-\infty,\infty)\). Hence, part (iii) follows the m.c.f.a. and the m.c f.b. properties of \(P_{X}\) (cf. Propositions 1.2.1 and 1.2.3). \(\Box\)

**Definition 2.2.4:** A function \(F:\mathbb{R}\to\mathbb{R}\) satisfying (i), (ii), and (iii) of Proposition 2.2.2 is called a _cumulative distribution function_ (or _cdf_ for short).

Thus, given a random variable \(X\), its cdf \(F_{X}\) satisfies properties (i), (ii), (iii) of Proposition 2.2.2. Conversely, given a cdf \(F\), one can construct a probability space \((\Omega,\mathcal{F},P)\) and a random variable \(X\) on it such that the cdf of \(X\) is \(F\). Indeed, given a cdf \(F\), note that by Theorem 1.3.3 and Definition 1.3.7, there exists a (Lebesgue-Stieltjes) probability measure \(\mu_{F}\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \(\mu_{F}((-\infty,x])=F(x)\). Now define \(X\) to be the identity map on \(\mathbb{R}\), i.e., let \(X(x)\equiv x\) for all \(x\in\mathbb{R}\). Then, \(X\) is a random variable on the probability space \((\mathbb{R},\mathcal{B}(\mathbb{R}),\mu_{F})\) with probability distribution \(P_{X}=\mu_{F}\) and cdf \(F_{X}=F\).

In addition to (i), (ii) and (iii) of Proposition 2.2.2, it is easy to verify that for any \(x\) in \(\mathbb{R}\),

\[P(X<x)=F_{X}(x-)\equiv\lim_{y\uparrow x}F_{X}(y)\]

and hence

\[P(X=x)=F_{X}(x)-F_{X}(x-). \tag{2.4}\]

Thus, the function \(F_{X}(\cdot)\) has a jump at \(x\) iff \(P(X=x)>0\). Since a monotone function from \(\mathbb{R}\) to \(\mathbb{R}\) can have only jump discontinuities and only a countable number of them (cf. Problem 2.11), for any random variable \(X\), the set \(\{a\in\mathbb{R}:P(X=a)>0\}\) is countable. This leads to the following definitions.

**Definition 2.2.5:**

1. A random variable \(X\) is called _discrete_ if there exists a countable set \(A\subset\mathbb{R}\) such that \(P(X\in A)=1\).
2. A random variable \(X\) is called _continuous_ if \(P(X=x)=0\) for all \(x\in\mathbb{R}\).

Note that \(X\) is continuous iff \(F_{X}\) is continuous on all of \(\mathbb{R}\) and \(X\) is discrete iff the sum of all the jumps of its cdf \(F_{X}\) is one. It may also be noted that if \(F_{X}\) is a step function, then \(X\) is discrete but not conversely. For example, consider the case where the set \(A\) in the above definition is the set of all rational numbers.

It turns out that a given cdf may be written as a weighted sum of a discrete and a continuous cdfs. Let \(F\) be a cdf. Let \(A\equiv\{x:p(x)\equiv F(x)-F(x-)>0\}\). As remarked earlier, \(A\) is at most countable. Write \(\alpha=\sum_{y\in A}p(y)\) and let \(\tilde{F}_{d}(x)=\sum_{y\in A}p(y)I_{(-\infty,x]}(y)\), and \(\tilde{F}_{c}(x)=F(x)-\tilde{F}_{d}(x)\). It is easy to verify that \(\tilde{F}_{c}(\cdot)\) is continuous on \(\mathbb{R}\). If \(\alpha=0\), then \(F(x)=\tilde{F}_{c}(x)\) and \(F\) is continuous. If \(\alpha=1\), then \(F=\tilde{F}_{d}(x)\) and \(F\) is discrete. If \(0<\alpha<1\), \(F(\cdot)\) can be written as

\[F(x)=\alpha F_{d}(x)+(1-\alpha)F_{c}(x), \tag{2.5}\]

where \(F_{d}(x)\equiv\alpha^{-1}\tilde{F}_{d}(x)\) and \(F_{c}(x)\equiv(1-\alpha)^{-1}\tilde{F}_{c}(x)\) are both cdfs, with \(F_{d}\) being discrete and \(F_{c}\) being continuous. For a further decomposition of \(F_{c}(\cdot)\) into absolutely continuous and singular continuous components, see Chapter 4.

#### Generalizations to higher dimensions

Induced distributions of random vectors and the associated cdfs are briefly considered in this section. Let \(X=(X_{1},X_{2},\ldots,X_{k})\) be a \(k\)-dimensional random vector defined on a probability space \((\Omega,\mathcal{F},P)\). The probability distribution \(P_{X}\) of \(X\) is the induced probability measure on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\), defined by (cf. (2.1))

\[P_{X}(A)\equiv P(X^{-1}(A))\quad A\in\mathcal{B}(\mathbb{R}^{k})\,. \tag{2.6}\]

The cdf \(F_{X}\) of \(X\) is now defined by

\[F_{X}(x)=P(X\leq x),\quad x\in\mathbb{R}^{k}\,, \tag{2.7}\]

where for any \(x=(x_{1},x_{2},\ldots,x_{k})\) and \(y=(y_{1},y_{2},\ldots,y_{k})\) in \(\mathbb{R}^{k}\), \(x\leq y\) means that \(x_{i}\leq y_{i}\) for all \(i=1,\ldots,k\).

The extension of Proposition 2.2.2 to the \(k\)-dimensional case is notationally involved. Here, an analog of Proposition 2.2.2 for the bivariate case, i.e., for \(k=2\) is stated.

**Proposition 2.2.3:** _Let \(F\) be the cdf of a bivariate random vector \(X=(X_{1},X_{2})\)._

1. _Then, for any_ \(x=(x_{1},x_{2})\leq y=(y_{1},y_{2})\)_,_ \[F(y_{1},y_{2})-F(x_{1},y_{2})-F(y_{1},x_{2})+F(x_{1},x_{2})\geq 0.\] (2.8)
2. _For any_ \(x=(x_{1},x_{2})\in\mathbb{R}^{2}\)_,_ \[\lim_{y_{1}\downarrow x_{1},y_{2}\downarrow x_{2}}F(y_{1},y_{2})=F(x_{1},x_{2}),\] _i.e.,_ \(F\) _is right continuous on_ \(\mathbb{R}^{2}\)* \(\lim_{x_{1}\to-\infty}F(x_{1},a)=\lim_{x_{2}\to-\infty}F(a,x_{2})=0\) _for all_ \(a\in\mathbb{R}\)_;_ \(\lim_{x_{1}\to\infty,x_{2}\to\infty}F(x_{1},x_{2})=1\)_._
* _For any_ \(a\in\mathbb{R}\)_,_ \(\lim_{y\uparrow\infty}F(a,y)=P(X_{1}\leq a)\) _and_ \(\lim_{y\uparrow\infty}F(y,a)=P(X_{2}\leq a)\)_._

**Proof:** Clearly,

\[0 \leq P(X\in(x_{1},y_{1}]\times(x_{2},y_{2}])\] \[= P(x_{1}<X_{1}\leq y_{1},x_{2}<X_{2}\leq y_{2})\] \[= P(X_{1}\leq y_{1},x_{2}<X_{2}\leq y_{2})-P(X_{1}\leq x_{1},x_{2 }<X_{2}\leq y_{2})\] \[= P(X_{1}\leq y_{1},X_{2}\leq y_{2})-P(X_{1}\leq y_{1},X_{2}\leq x _{2})\] \[\qquad\qquad-[P(X_{1}\leq x_{1},X_{2}\leq y_{2})-P(X_{1}\leq x_{1 },X_{2}\leq x_{2})]\] \[= F(y_{1},y_{2})-F(y_{1},x_{2})-F(x_{1},y_{2})+F(x_{1},x_{2}).\]

This proves (i).

To prove (ii), note that for any sequence \(y_{in}\downarrow x_{i},i=1,2\), the sets \(A_{n}=(-\infty,y_{1n}]\times(-\infty,y_{2n}]\downarrow A\equiv(-\infty,x_{1}] \times(-\infty,x_{2}]\). Hence, by m.c.f.a property of a probability measure,

\[F(y_{1n},y_{2n})=P(X\in A_{n})\downarrow P(A)=F(x_{1},x_{2}).\]

For (iii), note that \((-\infty,x_{1n}]\times(-\infty,a]\downarrow\emptyset\) for any sequence \(x_{1n}\downarrow-\infty\) and for any \(a\in\mathbb{R}\). Hence, again by the m.c.f.a. property,

\[F(x_{1n},a)\downarrow 0\quad\mbox{as}\quad n\to\infty\,.\]

By similar arguments, \(F(a,x_{2n})\downarrow 0\) whenever \(x_{2n}\downarrow-\infty\). To prove the last relation in (iii), apply the m.c.f.b. property to the sets \((-\infty,x_{1n}]\times(-\infty,x_{2n}]\uparrow\mathbb{R}^{2}\) for \(x_{1n}\uparrow\infty,x_{2n}\uparrow\infty\).

The proof of part (iv) is similar. \(\Box\)

Note that any function satisfying properties (i), (ii), (iii) of Proposition 2.2.3 determines a probability measure uniquely. This follows from the discussions in Section 1.3, as (1.3.9) and (1.3.10) follow from (i) and (iii) (Problem 2.12). For a general \(k\geq 1\), an analog of property (i) above is cumbersome to write down explicitly. Indeed, for any \(x\leq y\), now a sum involving \(2^{k}\)-terms must be nonnegative. However, properties (ii), (iii), and (iv) can be extended in an obvious way to the \(k\)-dimensional case. See Problem 2.13 for a precise statement. Also, for a general \(k\geq 1\), functions satisfying the properties listed in Problem 2.13 uniquely determine a probability measure on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\).

### 2.3 Integration

Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(f:\Omega\to\mathbb{R}\) be a measurable function. The goal of this section is to define the integral of \(f\) with respect to the measure \(\mu\) and establish some basic convergence results. The integral of a nonnegative function taking only _finitely_ many values is defined first, which is then extended to all nonnegative measurable functions by approximation from below. Finally, the integral of an arbitrary measurable function is defined using the decomposition of the function into its positive and negative parts.

**Definition 2.3.1:** A function \(f:\Omega\to\bar{\mathbb{R}}\equiv[-\infty,\infty]\) is called _simple_ if there exist a finite set (of distinct elements) \(\{c_{1},\ldots,c_{k}\}\in\bar{\mathbb{R}}\) and sets \(A_{1},\ldots,A_{k}\in\mathcal{F}\), \(k\in\mathbb{N}\) such that \(f\) can be written as

\[f=\sum_{i=1}^{k}c_{i}I_{A_{i}}\,. \tag{3.1}\]

**Definition 2.3.2:** (_The integral of a simple nonnegative function_). Let \(f:\Omega\to\bar{\mathbb{R}}_{+}\equiv[0,\infty]\) be a simple nonnegative function on \((\Omega,\mathcal{F},\mu)\) with the representation (3.1). The _integral_ of \(f\) w.r.t. \(\mu\), denoted by \(\int fd\mu\), is defined as

\[\int fd\mu\equiv\sum_{i=1}^{k}c_{i}\mu(A_{i})\,. \tag{3.2}\]

Here and in the following, the relation

\[0\cdot\infty=0\]

is adopted as a convention.

It may be verified that the value of the integral in (3.2) does not depend on the representation of \(f\). That is, if \(f\) can be expressed as \(f=\sum_{j=1}^{l}d_{j}I_{B_{j}}\) for some \(d_{1},\ldots,d_{l}\in\bar{\mathbb{R}}_{+}\) (not necessarily distinct) and for some sets \(B_{1},\ldots,B_{l}\in\mathcal{F}\), then \(\sum_{i=1}^{k}c_{i}\mu(A_{i})=\sum_{j=1}^{l}d_{j}\mu(B_{j})\), so that the value of the integral remains unchanged (Problem 2.17). Also note that for the \(f\) in Definition 2.3.2,

\[0\leq\int fd\mu\leq\infty.\]

The following proposition is an easy consequence of the definition and the above remark.

**Proposition 2.3.1:** _Let \(f\) and \(g\) be two simple nonnegative functions on \((\Omega,\mathcal{F},\mu)\). Then_

* \((\)_Linearity_\()\) _For_ \(\alpha\geq 0,\beta\geq 0,\int(\alpha f+\beta g)d\mu=\alpha\int fd\mu+\beta\int gd\mu.\)__
* \((\)_Monotonicity_\()\) _If_ \(f\geq g\) _a.e._ \((\mu)\)_, i.e.,_ \(\mu(\{\omega:\omega\in\Omega,f(\omega)<g(\omega)\})=0\)_, then_ \(\int fd\mu\geq\int gd\mu.\)___._
* _If_ \(f=g\) _a.e._ \((\mu)\)_, i.e., if_ \(\mu(\{\omega:\omega\in\Omega,f(\omega)\neq g(\omega)\})=0\)_, then_ \(\int fd\mu=\int gd\mu\)_._

**Definition 2.3.3:** (_The integral of a nonnegative measurable function_). Let \(f:\Omega\rightarrow\bar{\mathbb{R}}_{+}\) be a nonnegative measurable function on \((\Omega,\mathcal{F},\mu)\). The integral of \(f\) with respect to \(\mu\), also denoted by \(\int fd\mu\), is defined as

\[\int fd\mu=\lim_{n\rightarrow\infty}\int f_{n}d\mu, \tag{3.3}\]

where \(\{f_{n}\}_{n\geq 1}\) is _any_ sequence of nonnegative simple functions such that \(f_{n}(\omega)\uparrow f(\omega)\) for all \(\omega\).

Note that by Proposition 2.3.1 (ii), the sequence \(\{\int f_{n}d\mu\}_{n\geq 1}\) is nondecreasing, and hence the right side of (3.3) is well defined. That the right side of (3.3) is the _same_ for all such approximating sequences of functions needs to be established and is the content of the following proposition. The proof of this proposition exploits in a crucial way the m.c.f.b. and the finite additivity of the set function \(\mu\) (or, equivalently, the countable additivity of \(\mu\)).

**Proposition 2.3.2:** _Let \(\{f_{n}\}_{n\geq 1}\) and \(\{g_{n}\}_{n\geq 1}\) be two sequences of simple nonnegative measurable functions on \((\Omega,\mathcal{F},\mu)\) to \(\bar{\mathbb{R}}_{+}\) such that as \(n\rightarrow\infty\), \(f_{n}(\omega)\uparrow f(\omega)\) and \(g_{n}(\omega)\uparrow f(\omega)\) for all \(\omega\in\Omega\). Then_

\[\lim_{n\rightarrow\infty}\int f_{n}d\mu=\lim_{n\rightarrow\infty}\int g_{n}d\mu. \tag{3.4}\]

**Proof:** Fix \(N\in\mathbb{N}\) and \(0<\rho<1\). It will now be shown that

\[\lim_{n\rightarrow\infty}\int f_{n}d\mu\geq\rho\int g_{N}d\mu. \tag{3.5}\]

Suppose that \(g_{N}\) has the representation \(g_{N}\equiv\sum_{i=1}^{k}d_{i}I_{B_{i}}\). Let \(D_{n}=\{\omega\in\Omega:f_{n}(\omega)\geq\rho g_{N}(\omega)\},\ n\geq 1\). Since \(f_{n}(\omega)\uparrow f(\omega)\) for all \(\omega\), \(D_{n}\uparrow D\equiv\{\omega:f(\omega)\geq\rho g_{N}(\omega)\}\) (Problem 2.18 (b)). Also since \(g_{N}(\omega)\leq f(\omega)\) and \(0<\rho<1,D=\Omega\). Now writing \(f_{n}=f_{n}I_{D_{n}}+f_{n}I_{D_{n}^{c}}\), it follows from Proposition 2.3.1 that

\[\int f_{n}d\mu \geq \int f_{n}I_{D_{n}}d\mu\geq\rho\int g_{N}I_{D_{n}}d\mu \tag{3.6}\] \[= \rho\sum_{i=1}^{k}d_{i}\mu(B_{i}\cap D_{n}).\]

By the m.c.f.b. property, for each \(i\in\mathbb{N}\), \(\mu(B_{i}\cap D_{n})\uparrow\mu(B_{i}\cap\Omega)=\mu(B_{i})\) as \(n\rightarrow\infty\). Since the sequence \(\{\int f_{n}d\mu\}_{n\geq 1}\) is nondecreasing, taking limitsin (3.6), yields (3.5). Next, letting \(\rho\uparrow 1\) yields \(\lim_{n\to\infty}\int f_{n}d\mu\geq\int g_{N}d\mu\) for each \(N\in\mathbb{N}\) and hence,

\[\lim_{n\to\infty}\int f_{n}d\mu\geq\lim_{n\to\infty}\int g_{n}d\mu\,.\]

By symmetry, (3.4) follows and hence, the proof is complete. \(\Box\)

**Remark 2.3.1:** It is easy to verify that Proposition 2.3.2 remains valid if \(\{f_{n}\}_{n\geq 1}\) and \(\{g_{n}\}_{n\geq 1}\) increase to \(f\) a.e. \((\mu)\).

Given a nonnegative measurable function \(f\), one can always construct a nondecreasing sequence \(\{f_{n}\}_{n\geq 1}\) of nonnegative simple functions such that \(f_{n}(\omega)\uparrow f(\omega)\) for all \(\omega\in\bar{\Omega}\) in the following manner. Let \(\{\delta_{n}\}_{n\geq 1}\) be a sequence of positive real numbers and let \(\{N_{n}\}_{n\geq 1}\) be a sequence of positive integers such that as \(n\to\infty\), \(\delta_{n}\downarrow 0,N_{n}\uparrow\infty\) and \(N_{n}\delta_{n}\uparrow\infty\). Further, suppose that the sequence \(P_{n}\equiv\{j\delta_{n}:j=0,1,2,\ldots,N_{n}\}\) is nested, i.e., \(P_{n}\subset P_{n+1}\) for each \(n\geq 1\). Now set

\[f_{n}(\omega)=\left\{\begin{array}{lll}j\delta_{n}&\mbox{if}&j\delta_{n}\leq f (\omega)<(j+1)\delta_{n},\;j=0,1,2,\ldots,(N_{n}-1)\\ N_{n}\delta_{n}&\mbox{if}&f(\omega)\geq N_{n}\delta_{n}\,.\end{array}\right. \tag{3.7}\]

A specific choice of \(\delta_{n}\) and \(N_{n}\) is given by \(\delta_{n}=2^{-n},N_{n}=n2^{n}\).

Thus, with the above choice of \(\{f_{n}\}_{n\geq 1}\) in the definition of the _Lebesgue integral_\(\int fd\mu\) in (3.3), the _range_ of \(f\) is subdivided into intervals of decreasing lengths. This is in contrast to the definition of the _Riemann integral_ of \(f\) over a bounded interval, which is defined via subdividing the _domain_ of \(f\) into finer subintervals.

**Remark 2.3.2:** In some cases it may be more appropriate to choose the approximating sequence \(\{f_{n}\}_{n\geq 1}\) in a manner different from (3.7). For example, let \(\Omega=\{\omega_{i}:i\geq 1\}\) be a countable set, \({\cal F}={\cal P}(\Omega)\), the power set of \(\Omega\), and let \(\mu\) be a measure on \((\Omega,{\cal F})\). Then any function \(f:\Omega\to\mathbb{R}_{+}\equiv[0,\infty)\) is measurable and the integral \(\int fd\mu\) coincides with the sum \(\sum_{i=1}^{\infty}f(\omega_{i})\mu(\{\omega_{i}\})\) as can be seen by choosing the approximating sequence \(\{f_{n}\}_{n\geq 1}\) as

\[f_{n}(\omega_{i})=\left\{\begin{array}{lll}f(\omega_{i})&\mbox{for}&i=1,2, \ldots,n\\ 0&\mbox{for}&i>n.\end{array}\right.\]

**Remark 2.3.3:** The integral of a nonnegative measurable function can be alternatively defined as

\[\int fd\mu=\sup\Big{\{}\int gd\mu:g\quad\mbox{nonnegative and simple},\;g\leq f \Big{\}}.\]

The equivalence of this to (3.3) is seen as follows. Clearly the right side above, say, \(M\) is greater than or equal to \(\int fd\mu\) as in (3.3). Conversely,there exist a sequence \(\{g_{n}\}_{n\geq 1}\) of simple nonnegative functions with \(g_{n}\leq f\) for all \(n\geq 1\) such that \(\lim_{n}\int g_{n}d\mu\) equals the supremum \(M\) defined above. Now set \(h_{n}=\max\{g_{j}:1\leq j\leq n\}\), \(n\geq 1\). Now it can be verified that for each \(n\geq 1\), \(h_{n}\) is nonnegative, simple, and satisfies \(h_{n}\uparrow f\) and also that \(\int h_{n}d\mu\) converges to \(M\) (Problem 2.19 (b)).

**Corollary 2.3.3:** _Let \(f\) and \(g\) be two nonnegative measurable functions on \((\Omega,\mathcal{F},\mu)\). Then, the conclusions of Proposition 2.3.1 remain valid for such \(f\) and \(g\)._

**Proof:** This follows from Proposition 2.3.1 for nonnegative simple functions and Definition 2.3.3. \(\Box\)

The definition of the integral \(\int fd\mu\) of a nonnegative measurable function \(f\) in (3.3) makes it possible to interchange limits and integration in a fairly routine manner. In particular, the following key result is a direct consequence of the definition.

**Theorem 2.3.4:** (_The monotone convergence theorem or MCT_). _Let \(\{f_{n}\}_{n\geq 1}\) and \(f\) be nonnegative measurable functions on \((\Omega,\mathcal{F},\mu)\) such that \(f_{n}\uparrow f\) a.e. \((\mu)\). Then_

\[\int fd\mu=\lim_{n\to\infty}\int f_{n}d\mu. \tag{3.8}\]

**Remark 2.3.4:** The important difference between (3.4) and (3.8) is that in (3.8), the \(f_{n}\)'s need not be simple.

**Proof:** It is similar to the proof of Proposition 2.3.2. Let \(\{g_{n}\}_{n\geq 1}\) be a sequence of nonnegative simple functions on \((\Omega,\mathcal{F},\mu)\) such that \(g_{n}(\omega)\uparrow f(\omega)\) for all \(\omega\). By hypothesis, there exists a set \(A\in\mathcal{F}\) such that \(\mu(A^{c})=0\) and for \(\omega\) in \(A\), \(f_{n}(\omega)\uparrow f(\omega)\). Fix \(k\in\mathbb{N}\) and \(0<\rho<1\). Let \(D_{n}=\{\omega:\omega\in A,\ f_{n}(\omega)\geq\rho g_{k}(\omega)\}\), \(n\geq 1\). Then, \(D_{n}\uparrow D\equiv\{\omega:\omega\in A,f(\omega)\geq\rho g_{k}(\omega)\}\). Since \(g_{k}(\omega)\leq f(\omega)\) for all \(\omega\), it follows that \(D=A\). Now, by Corollary 2.3.3,

\[\int f_{n}d\mu\geq\int f_{n}I_{D_{n}}d\mu\geq\rho\int g_{k}I_{D_{n}}d\mu\quad \mbox{for all}\quad n\geq 1.\]

By m.c.f.b., \(\int g_{k}I_{D_{n}}d\mu\uparrow\int g_{k}I_{A}d\mu=\int g_{k}d\mu\) as \(n\to\infty\), yielding

\[\liminf_{n\to\infty}\int f_{n}d\mu\geq\rho\int g_{k}d\mu\]

for all \(0<\rho<1\) and all \(k\in\mathbb{N}\). Letting \(\rho\uparrow 1\) first and then \(k\uparrow\infty\), from (3.3) one gets

\[\liminf_{n\to\infty}\int f_{n}d\mu\geq\int fd\mu.\]By monotonicity (Corollary 2.3.3),

\[\int f_{n}d\mu\leq\int fd\mu\quad\mbox{for all}\quad n\geq 1\]

and so the proof is complete. \(\Box\)

**Corollary 2.3.5:** _Let \(\{h_{n}\}_{n\geq 1}\) be a sequence of nonnegative measurable functions on a measure space \((\Omega,{\cal F},\mu)\). Then_

\[\int\left(\sum_{n=1}^{\infty}h_{n}\right)d\mu=\sum_{n=1}^{\infty}\int h_{n}d \mu\,.\]

**Proof:** Let \(f_{n}=\sum_{i=1}^{n}h_{i},n\geq 1\), and let \(f=\sum_{i=1}^{\infty}h_{i}\). Then, \(0\leq f_{n}\uparrow f\). By the MCT,

\[\int f_{n}d\mu\uparrow\int fd\mu.\]

But by Corollary 2.3.3,

\[\int f_{n}d\mu=\sum_{i=1}^{n}\int h_{i}d\mu.\]

Hence, the result follows. \(\Box\)

**Corollary 2.3.6:** _Let \(f\) be a nonnegative measurable function on a measurable space \((\Omega,{\cal F},\mu)\). For \(A\in{\cal F}\), let_

\[\nu(A)\equiv\int fI_{A}d\mu.\]

_Then, \(\nu\) is a measure on \((\Omega,{\cal F})\)._

**Proof:** Let \(\{A_{n}\}_{n\geq 1}\) be a sequence of disjoint sets in \({\cal F}\). Let \(h_{n}=fI_{A_{n}}\) for \(n\geq 1\). Then by Corollary 2.3.5,

\[\nu(\bigcup_{n\geq 1}A_{n}) = \int fI_{[\bigcup_{n\geq 1}A_{n}]}d\mu=\int f\cdot\bigg{[}\sum_{ n=1}^{\infty}I_{A_{n}}\bigg{]}d\mu\] \[= \int\bigg{[}\sum_{n=1}^{\infty}h_{n}\bigg{]}d\mu=\sum_{n=1}^{ \infty}\int h_{n}d\mu=\sum_{n=1}^{\infty}\nu(A_{n}).\]

\(\Box\)

**Remark 2.3.5:** Notice that \(\mu(A)=0\Rightarrow\nu(A)=0\). In this case \(\nu\) is said to be _dominated by_ or _absolutely continuous with respect to \(\mu\)_. The Radon-Nikodym theorem (see Chapter 4) provides a converse to this. That is, if \(\nu\) and \(\mu\) are two measures on a measurable space \((\Omega,{\cal F})\) such that \(\nu\) is dominated by \(\mu\) and \(\mu\) is \(\sigma\)-finite, then there exists a nonnegative measurable function \(f\) such that \(\nu(A)=\int fI_{A}d\mu\) for all \(A\) in \(\mathcal{F}\). This \(f\) is called a _Radon-Nikodym derivative_ (or _a density_) of \(\nu\) with respect to \(\mu\) and is denoted by \(\frac{d\nu}{d\mu}\).

**Theorem 2.3.7:** (_Fatou's lemma_). _Let \(\{f_{n}\}_{n\geq 1}\) be a sequence of nonnegative measurable functions on \((\Omega,\mathcal{F},\mu)\). Then_

\[\liminf_{n\to\infty}\int f_{n}d\mu\geq\int\liminf_{n\to\infty}f_{n}d\mu. \tag{3.9}\]

**Proof:** Let \(g_{n}(\omega)\equiv\inf\{f_{j}(\omega):j\geq n\}\). Then \(\{g_{n}\}_{n\geq 1}\) is a sequence of nonnegative, nondecreasing measurable functions on \((\Omega,\mathcal{F},\mu)\) such that \(g_{n}(\omega)\uparrow g(\omega)\equiv\liminf_{n\to\infty}f_{n}(\omega)\). By the MCT,

\[\int g_{n}d\mu\uparrow\int gd\mu.\]

But by monotonicity

\[\int f_{n}d\mu\geq\int g_{n}d\mu\text{ for each }n\geq 1,\]

and hence, (3.9) follows. \(\Box\)

**Remark 2.3.6:** In (3.9), the inequality can be strict. For example, take \(f_{n}=I_{[n,\infty)},\ n\geq 1\), on the measure space \((\mathbb{R},\mathcal{B}(\mathbb{R}),m)\) where \(m\) is the Lebesgue measure. For another example, consider \(f_{n}=nI_{[0,\frac{1}{n}]},\ n\geq 1\), on the finite measure space \(([0,1],\mathcal{B}([0,1]),m)\).

**Definition 2.3.4:** (_The integral of a measurable function_).  Let \(f\) be a real valued measurable function on a measure space \((\Omega,\mathcal{F},\mu)\). Let \(f^{+}=fI_{\{f\geq 0\}}\) and \(f^{-}=-fI_{\{f<0\}}\). The integral of \(f\) with respect to \(\mu\), denoted by \(\int fd\mu\), is defined as

\[\int fd\mu=\int f^{+}d\mu-\int f^{-}d\mu,\]

provided that at least one of the integrals on the right side is finite.

**Remark 2.3.7:** Note that both \(f^{+}\) and \(f^{-}\) are nonnegative measurable functions and \(f=f^{+}-f^{-}\) and \(|f|=f^{+}+f^{-}\). Further, the integrals \(\int f^{+}d\mu\) and \(\int f^{-}d\mu\) in Definition 2.3.4 are defined via Definition 2.3.3.

**Definition 2.3.5:** (_Integrable functions_). A measurable function \(f\) on a measure space \((\Omega,\mathcal{F},\mu)\) is said to be _integrable_ with respect to \(\mu\) if \(\int|f|d\mu<\infty\).

Since \(|f|=f^{+}+f^{-}\), it follows that \(f\) is _integrable_ iff both \(f^{+}\) and \(f^{-}\) are integrable, i.e., \(\int f^{+}d\mu<\infty\) and \(\int f^{-}d\mu<\infty\). In the following, whenever the integral of \(f\) or its integrability is discussed, the measurability of \(f\) will be assumed to hold.

**Remark on notation:** The \(\int fd\mu\) is also written as

\[\int_{\Omega}f(\omega)\mu(d\omega)\quad\mbox{and}\quad\int_{\Omega}f(\omega)d\mu (\omega).\]

**Definition 2.3.6:** Let \(f\) be a measurable function on a measure space \((\Omega,\mathcal{F},\mu)\) and \(A\in\mathcal{F}.\) Then integral of \(f\) over \(A\) with respect to \(\mu,\) denoted by \(\int_{A}fd\mu,\) is defined as

\[\int_{A}fd\mu\equiv\int fI_{A}d\mu, \tag{3.10}\]

provided the right side is well defined.

**Definition 2.3.7:** (\(L^{p}\)-_spaces_). Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(0<p\leq\infty.\) Then \(L^{p}(\Omega,\mathcal{F},\mu)\) is defined as

\[L^{p}(\Omega,\mathcal{F},\mu) \equiv \left\{f:|f|^{p}\mbox{ is integrable with respect to }\mu\right\}\] \[= \left\{f:\int|f|^{p}d\mu<\infty\right\}\quad\mbox{for}\quad 0<p<\infty,\]

and

\[L^{\infty}(\Omega,\mathcal{F},\mu)\equiv\left\{f:\mu(\{|f|>K\})=0\quad\mbox{ for some}\quad K\in(0,\infty)\right\}.\]

The following is an extension of Proposition 2.3.1 to integrable functions.

**Proposition 2.3.8:** _Let \(f\), \(g\in L^{1}(\Omega,\mathcal{F},\mu).\) Then_

1. \(\int(\alpha f+\beta g)d\mu=\alpha\int fd\mu+\beta\int gd\mu\) _for any_ \(\alpha,\beta\in\mathbb{R}.\)__
2. \(f\geq g\) _a.e._ \((\mu)\Rightarrow\int fd\mu\geq\int gd\mu.\)__
3. \(f=g\) _a.e._ \((\mu)\Rightarrow\int fd\mu=\int gd\mu.\)__

**Proof:** It is easy to verify (Problem 2.32) that if \(h=h_{1}-h_{2}\) where \(h_{1}\) and \(h_{2}\) are nonnegative functions in \(L^{1}(\Omega,\mathcal{F},\mu),\) then \(h\) is also in \(L^{1}(\Omega,\mathcal{F},\mu)\) and

\[\int hd\mu=\int h_{1}d\mu-\int h_{2}d\mu. \tag{3.11}\]

Note that \(h\equiv\alpha f+\beta g\) can be written as

\[(a^{+}-\alpha^{-})(f^{+}-f^{-})+(\beta^{+}-\beta^{-})(g^{+}-g^{-})\] \[= (\alpha^{+}f^{+}+\alpha^{-}f^{-}+\beta^{+}g^{+}+\beta^{-}g^{-})\] \[-(\alpha^{+}f^{-}+\alpha^{-}f^{+}+\beta^{+}g^{-}+\beta^{-}g^{+})\] \[= h_{1}-h_{2},\quad\mbox{say}.\]Since \(f\), \(g\in L^{1}(\Omega,\mathcal{F},\mu)\), it follows that \(h_{1}\) and \(h_{2}\in L^{1}(\Omega,\mathcal{F},\mu)\). Further, they are nonnegative and by (3.11), \(h\in L^{1}(\Omega,\mathcal{F},\mu)\) and

\[\int hd\mu=\int h_{1}d\mu-\int h_{2}d\mu.\]

Now apply Proposition 2.3.1 to each of the terms on the right side and regroup the terms to get

\[\int hd\mu=\alpha\int fd\mu+\beta\int gd\mu.\]

Proofs of (ii) and (iii) are left as an exercise. \(\Box\)

**Remark 2.3.8:** By Proposition 2.3.8, if \(f\) and \(g\in L^{1}(\Omega,\mathcal{F},\mu)\), then so does \(\alpha f+\beta g\) for all \(\alpha,\beta\in\mathbb{R}\). Thus, \(L^{1}(\Omega,\mathcal{F},\mu)\) is a vector space over \(\mathbb{R}\). Further, if one sets

\[\|f\|_{1}\equiv\int|f|d\mu,\]

(and identifies a function \(f\) with its equivalence class under the relation \(f\sim g\) iff \(f=g\) a.e. \((\mu)\)), then \(\|\cdot\|_{1}\) defines a norm on \(L^{1}(\Omega,\mathcal{F},\mu)\) and makes it a _normed linear space_ (cf. Chapter 3). A similar remark also holds for \(L^{p}(\Omega,\mathcal{F},\mu)\) for \(1<p\leq\infty\).

Next note that by Proposition 2.3.8, if \(f=0\) a.e. \((\mu)\), then \(\int fd\mu=0\). However, the converse is not true. But if \(f\) is nonnegative a.e. \((\mu)\), then the converse is true as shown below.

**Proposition 2.3.9:** _Let \(f\) be a measurable function on \((\Omega,\mathcal{F},\mu)\) and let \(f\) be nonnegative a.e. \((\mu)\). Then_

\[\int fd\mu=0\quad\mbox{iff}\quad f=0\quad\mbox{a.e.}\quad(\mu).\]

**Proof:** It is enough to prove the "only if" part. Let \(D=\{\omega:f(\omega)>0\}\) and \(D_{n}=\{\omega:f(\omega)>\frac{1}{n}\}\), \(n\geq 1\). Then \(D=\bigcup_{n\geq 1}D_{n}\). Since \(f\geq fI_{D_{n}}\) a.e. \((\mu)\),

\[0=\int fd\mu\geq\int fI_{D_{n}}d\mu\geq\frac{1}{n}\mu(D_{n})\Rightarrow\mu(D_{ n})=0\quad\mbox{for each}\quad n\geq 1.\]

Also \(D_{n}\uparrow D\) and so by m.c.f.b.,

\[\mu(D)=\lim_{n\to\infty}\mu(D_{n})=0\,.\]

Hence, Proposition 2.3.9 follows. \(\Box\)

A dual to the above proposition is the next one.

**Proposition 2.3.10:** _Let \(f\in L^{1}(\Omega,{\cal F},\mu)\). Then, \(|f|<\infty\) a.e. \((\mu)\)._

**Proof:** Let \(C_{n}=\{\omega:|f(\omega)|>n\}\), \(n\geq 1\) and let \(C=\{\omega:|f(\omega)|=\infty\}\). Then \(C_{n}\downarrow C\) and

\[\int|f|d\mu\geq\int|f|I_{C_{n}}d\mu\geq n\mu(C_{n})\Rightarrow\mu(C_{n})\leq \frac{\int|f|d\mu}{n}\,.\]

Since \(\int|f|d\mu<\infty\), \(\lim_{n\rightarrow\infty}\mu(C_{n})=0\). Hence, by m.c.f.a., \(\mu(C)=\lim_{n\rightarrow\infty}\mu(C_{n})=0\). \(\Box\)

The next result is a useful convergence theorem for integrals.

**Theorem 2.3.11:** (_The extended dominated convergence theorem or EDCT_). Let \((\Omega,{\cal F},\mu)\) be a measure space and let \(f_{n},\,g_{n}:\Omega\rightarrow{\mathbb{R}}\) be \(\langle{\cal F},{\mathbb{R}}\rangle\)-measurable functions such that \(|f_{n}|\leq g_{n}\) a.e. \((\mu)\) for all \(n\geq 1\). Suppose that_

* \(g_{n}\to g\) _a.e._ \((\mu)\) _and_ \(f_{n}\to f\) _a.e._ \((\mu)\)_;_
* \(g_{n},\,g\in L^{1}(\Omega,{\cal F},\mu)\) _and_ \(\int|g_{n}|d\mu\rightarrow\int|g|d\mu\) _as_ \(n\rightarrow\infty\)_. Then,_ \(f\in L^{1}(\Omega,{\cal F},\mu)\)_,_ \[\lim_{n\rightarrow\infty}\int f_{n}d\mu=\int fd\mu\quad\mbox{and}\quad\lim_{n \rightarrow\infty}\int|f_{n}-f|d\mu=0.\] (3.12)

Two important special cases of Theorem 2.3.11 will be stated next. When \(g_{n}=g\) for all \(n\geq 1\), one has the standard version of the dominated convergence theorem.

**Corollary 2.3.12:** (_Lebesgue's dominated convergence theorem, or DCT_). _If \(|f_{n}|\leq g\) a.e. \((\mu)\) for all \(n\geq 1\), \(\int gd\mu<\infty\) and \(f_{n}\to f\) a.e. \((\mu)\), then \(f\in L^{1}(\Omega,{\cal F},\mu)\),_

\[\lim_{n\rightarrow\infty}\int f_{n}d\mu=\int fd\mu\quad\mbox{and}\quad\lim_{n \rightarrow\infty}\int|f_{n}-f|d\mu=0. \tag{3.13}\]

**Corollary 2.3.13:** (_The bounded convergence theorem, or BCT_). _Let \(\mu(\Omega)<\infty\). If there exist a \(0<k<\infty\) such that \(|f_{n}|\leq k\) a.e. \((\mu)\) and \(f_{n}\to f\) a.e. \((\mu)\)_, _then_

\[\lim_{n\rightarrow\infty}\int f_{n}d\mu=\int fd\mu\quad\mbox{and}\quad\lim_{n \rightarrow\infty}\int|f_{n}-f|d\mu=0. \tag{3.14}\]

**Proof:** Take \(g(\omega)\equiv k\) for all \(\omega\in\Omega\) in the previous corollary. \(\Box\)

**Proof of Theorem 2.3.11:** By Fatou's lemma,

\[\int|f|d\mu\leq\liminf_{n\rightarrow\infty}\int|f_{n}|d\mu\leq\liminf_{n \rightarrow\infty}\int|g_{n}|d\mu=\int|g|d\mu<\infty.\]Hence, \(f\) is integrable. For proving the second part, let \(h_{n}=f_{n}+g_{n}\) and \(\gamma_{n}=g_{n}-f_{n},n\geq 1\). Then, \(\{h_{n}\}_{n\geq 1}\) and \(\{\gamma_{n}\}_{n\geq 1}\) are sequences of nonnegative integrable functions. By Fatou's lemma and (ii),

\[\int(f+g)d\mu = \int\liminf_{n\to\infty}\,h_{n}d\mu\] \[\leq \liminf_{n\to\infty}\int h_{n}d\mu\] \[= \liminf_{n\to\infty}\left[\int g_{n}d\mu+\int f_{n}d\mu\right]\] \[= \int gd\mu+\liminf_{n\to\infty}\int f_{n}d\mu.\]

Similarly,

\[\int(g-f)d\mu\leq\int gd\mu-\limsup_{n\to\infty}\int f_{n}d\mu.\]

By Proposition 2.3.8, \(\int(g\pm f)d\mu=\int gd\mu\pm\int fd\mu\). Hence,

\[\int fd\mu\leq\liminf_{n\to\infty}\int f_{n}d\mu\]

and

\[\limsup_{n\to\infty}\int f_{n}d\mu\leq\int fd\mu\]

yielding that \(\lim_{n\to\infty}\int f_{n}d\mu=\int fd\mu\). For the last part, apply the above argument to \(f_{n}\) and \(g_{n}\) replaced by \(\tilde{f}_{n}\equiv|f-f_{n}|\) and \(\tilde{g}_{n}\equiv g_{n}+|f|\), respectively. \(\Box\)

**Theorem 2.3.14:** (_An approximation theorem_). _Let \(\mu_{F}\) be a Lebesgue-Stieltjes measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Let \(f\in L^{p}(\mathbb{R},\mathcal{B}(\mathbb{R}),\mu_{F})\), \(0<p<\infty\). Then, for any \(\delta>0\), there exist a step function \(h\) and a continuous function \(g\) with compact support (i.e., \(g\) vanishes outside a bounded interval) such that_

\[\int|f-h|^{p}d\mu<\delta, \tag{3.15}\]

\[\int|f-g|^{p}d\mu<\delta, \tag{3.16}\]

_where a step function \(h\) is a function of the form \(h=\sum_{i=1}^{k}c_{i}I_{A_{i}}\) with \(k<\infty\), \(c_{1},c_{2},\ldots,c_{k}\in\mathbb{R}\) and \(A_{1},A_{2},\ldots,A_{k}\) being bounded disjoint intervals._

**Proof:** Let \(f_{n}(\cdot)=f(\cdot)I_{B_{n}}(\cdot)\) where \(B_{n}=\{x:|x|\leq n,|f(x)|\leq n\}\). By the DCT, for every \(\epsilon>0\), there exists an \(N_{\epsilon}\) such that for all \(n\geq N_{\epsilon}\),

\[\int|f-f_{n}|^{p}d\mu_{F}<\epsilon. \tag{3.17}\]Since \(|f_{N_{\epsilon}}(\cdot)|\leq N_{\epsilon}\) on \([-N_{\epsilon},N_{\epsilon}]\), for any \(\eta>0\), there exists a simple function \(\tilde{f}\) such that

\[\sup\{|f_{N_{\epsilon}}(x)-\tilde{f}(x)|:x\in\mathbb{R}\}<\eta.\quad\mbox{(cf. \eqref{eq:f})} \tag{3.18}\]

Next, using Problem 1.32 (b), one can show that for any \(\eta>0\), there exists a step function \(h\) (depending on \(\eta\)) such that

\[\int|\tilde{f}_{N_{\epsilon}}-h|^{p}d\mu_{F}<\eta. \tag{3.19}\]

Since \(f-h=f-f_{N_{\epsilon}}+f_{N_{\epsilon}}-\tilde{f}+\tilde{f}-h\),

\[|f-h|^{p}\leq C_{p}\Big{(}|f-f_{N_{\epsilon}}|^{p}+|f_{N_{\epsilon}}-\tilde{f} |^{p}+|\tilde{f}-h|^{p}\Big{)},\]

where \(C_{p}\) is a constant depending only on \(p\).

This in turn yields, from (3.17)-(3.19),

\[\int|f-h|^{p}d\mu_{F}\leq C_{p}(\epsilon+(\mu_{F}\{x:|x|\leq N_{\epsilon}\}) \eta^{p}+\eta). \tag{3.20}\]

Given \(\delta>0\), choose \(\epsilon>0\) first and then \(\eta>0\) such that the right side of (3.20) above is less than \(\delta\).

Next, given any step function \(h\) and \(\eta>0\) there exists a continuous function \(g\) with compact support such that \(\mu_{F}\{x:h(x)\neq g(x)\}<\eta\) (cf. Problem 1.32 (g)). Now (3.16) follows from (3.15). \(\Box\)

**Remark 2.3.9:** The approximation (3.16) remains valid if \(g\) is restricted to the class of all infinitely differentiable functions with compact support. Further it remains valid for \(0<p<\infty\) for Lebesgue-Stieltjes measures on any Euclidean space.

**Remark 2.3.10:** The above approximation theorem fails for \(p=\infty\). For example, consider the function \(f(x)\equiv 1\) in \(L^{\infty}(m)\).

### 2.4 Riemann and Lebesgue integrals

Let \(f\) be a real valued bounded function on a bounded interval \([a,b]\). Recall the definition of the _Riemann integral_. Let \(P=\{x_{0},x_{1},\ldots,x_{n}\}\) be a finite _partition_ of \([a,b]\), i.e., \(x_{0}=a<x_{1}<x_{2}<x_{n-1}<x_{n}=b\) and \(\Delta=\Delta(P)\equiv\max\{(x_{i+1}-x_{i}):0\leq i\leq n-1\}\) be the _diameter_ of \(P\). Let \(M_{i}=\sup\{f(x):x_{i}\leq x\leq x_{i+1}\}\) and \(m_{i}=\inf\{f(x):x_{i}\leq x\leq x_{i+1}\}\), \(i=0,1,\ldots,n-1\).

**Definition 2.4.1:** The _upper_- and _lower-Riemann sums_ of \(f\) w.r.t. the partition \(P\) are, respectively, defined as

\[U(f,P)\equiv\sum_{i=0}^{n-1}M_{i}\cdot(x_{i+1}-x_{i}) \tag{4.1}\]and

\[L(f,P)\equiv\sum_{i=0}^{n-1}m_{i}\cdot(x_{i+1}-x_{i}). \tag{4.2}\]

It is easy to verify that if \(Q=\{y_{0},y_{1},\ldots,y_{k}\}\) is another partition satisfying \(P\subset Q\), then \(U(f,P)\geq U(f,Q)\geq L(f,Q)\geq L(f,P)\). Let \({\cal P}\) denote the collection of all finite partitions of \([a,b]\).

**Definition 2.4.2:** The _upper-Riemann integral_\(\overline{\int}f\) is defined as

\[\overline{\int}f=\inf_{P\in{\cal P}}U(f,P) \tag{4.3}\]

and the _lower-Riemann integral_\(\underline{\int}f\), by

\[\underline{\int}f=\sup_{P\in{\cal P}}L(f,P). \tag{4.4}\]

It can be shown (cf. Problem 2.23) that if \(\{P_{n}\}_{n\geq 1}\) is any sequence of partitions such that \(\Delta(P_{n})\to 0\) as \(n\to\infty\) and \(P_{n}\subset P_{n+1}\) for each \(n\geq 1\), then \(U(P_{n},f)\downarrow\overline{\int}f\) and \(L(P_{n},f)\uparrow\underline{\int}f\).

**Definition 2.4.3:**\(f\) is said to be _Riemann integrable_ if

\[\overline{\int}f=\underline{\int}f. \tag{4.5}\]

The common value is denoted by \(\oint_{[a,b]}f\).

Fix a sequence \(\{P_{n}\}_{n\geq 1}\) of partitions such that \(P_{n}\subset P_{n+1}\) for all \(n\geq 1\) and \(\Delta(P_{n})\to 0\) as \(n\to\infty\). Let \(P_{n}=\{x_{n0}=a<x_{n1}<x_{n2}\ldots<x_{nk_{n}}=b\}\). For \(i=0,1,\ldots,k_{n}-1\), let

\[\phi_{n}(x) \equiv \sup\{f(t):x_{i}\leq t\leq x_{i+1}\},\ \ x\in[x_{i},x_{i+1})\] \[\psi_{n}(x) \equiv \inf\{f(t):x_{i}\leq t\leq x_{i+1}\},\ \ x\in[x_{i},x_{i+1})\]

and let \(\phi_{n}(b)=\psi_{n}(b)=0\). Then, \(\phi_{n}\) and \(\psi_{n}\) are step functions on \([a,b]\) and hence, are Borel measurable. Further, since \(f\) is bounded, so are \(\phi_{n}\) and \(\psi_{n}\) and hence are integrable on \([a,b]\) w.r.t. the Lebesgue measure \(m\). The Lebesgue integrals of \(\phi_{n}\) and \(\psi_{n}\) are given by \(\int_{[a,b]}\phi_{n}dm=U(P_{n},f)\) and \(\int_{[a,b]}\psi_{n}dm=L(P_{n},f)\).

It can be shown (Problem 2.24) that for all \(x\not\in\bigcup_{n\geq 1}P_{n}\), as \(n\to\infty\),

\[\phi_{n}(x)\to\phi(x)\equiv\lim_{\delta\downarrow 0}\ \sup\{f(y):|y-x|<\delta\} \tag{4.6}\]

and

\[\psi_{n}(x)\to\psi(x)\equiv\lim_{\delta\downarrow 0}\ \inf\{f(y):|y-x|<\delta\} \tag{4.7}\]Thus, \(\phi\) and \(\psi\), being limits of Borel measurable functions (except possibly on a countable set), are Borel measurable. By the BCT (Corollary 2.3.13),

\[\overline{\int}f=\lim_{n\to\infty}\int\phi_{n}dm=\int\phi dm\]

and

\[\underline{\int}f=\lim_{n\to\infty}\int\psi_{n}dm=\int\psi dm.\]

Thus, \(f\) is Riemann integrable on \([a,b]\), iff \(\int\phi dm=\int\psi dm\), iff \(\int(\phi-\psi)dm=0\). Since \(\phi(x)\geq f(x)\geq\psi(x)\) for all \(x\), this holds iff \(\phi=f=\psi\) a.e \([m]\). It can be shown that \(f\) is continuous at \(x_{0}\) iff \(\phi(x_{0})=f(x_{0})=\psi(x_{0})\) (Problem 2.8). Summarizing the above discussion, one gets the following theorem.

Theorem 2.4.1: Let \(f\) be a bounded function on a bounded interval \([a,b]\). Then \(f\) is Riemann integrable on \([a,b]\) iff \(f\) is continuous a.e. (m) on \([a,b]\). In this case, \(f\) is Lebesgue integrable on \([a,b]\) and the Lebesgue integral \(\int_{[a,b]}fdm\) equals the Riemann integral \(\oint_{[a,b]}f\), i.e., the two integrals coincide.

It should be noted that Lebesgue integrability need not imply Riemann integrability. For example, consider \(f(x)\equiv I_{\mathbb{Q}_{1}}(x)\) where \(\mathbb{Q}_{1}\) is the set of rationals in \([0,1]\) (Problem 2.25).

The functions \(\phi\) and \(\psi\) defined in (4.6) and (4.7) above are called, respectively, the _upper_ and the _lower envelopes_ of the function \(f\). They are _semicontinuous_ in the sense that for each \(\alpha\in\mathbb{R}\), the sets \(\{x:\phi(x)<\alpha\}\) and \(\{x:\psi(x)>\alpha\}\) are open (cf. Problem 2.8).

Remark Remark 2.4.1: The key difference in the definitions of Riemann and Lebesgue integrals is that in the former the _domain of \(f\)_ is partitioned while in the latter the _range of \(f\)_ is partitioned.

### 2.5 More on convergence

Let \(\{f_{n}\}_{n\geq 1}\) and \(f\) be measurable functions from a measure space \((\Omega,\mathcal{F},\mu)\) to \(\bar{\mathbb{R}}\), the set of extended real numbers. There are several notions of convergence of \(\{f_{n}\}_{n\geq 1}\) to \(f\). The following two have been discussed earlier.

Definition Definition 2.5.1: \(\{f_{n}\}_{n\geq 1}\) converges to \(f\) pointwise _if_

\[\lim_{n\to\infty}f_{n}(\omega)=f(\omega)\quad\text{for all}\quad\omega\text{ in }\Omega.\]

Definition Definition 2.5.2: \(\{f_{n}\}_{n\geq 1}\) converges to \(f\) almost everywhere \((\mu)\), denoted by \(f_{n}\to f\) a.e. \((\mu)\), if there exists a set \(B\) in \(\mathcal{F}\) such that \(\mu(B)=0\) and

\[\lim_{n\to\infty}f_{n}(\omega)=f(\omega)\quad\text{for all}\quad\omega{\in}B ^{c}. \tag{5.1}\]Now consider some more notions of convergence.

**Definition 2.5.3:**\(\{f_{n}\}_{n\geq 1}\) _converges to \(f\) in measure_ (w.r.t. \(\mu\)), denoted by \(f_{n}\longrightarrow^{m}f\), if for each \(\epsilon>0\),

\[\lim_{n\to\infty}\mu\big{(}\{|f_{n}-f|>\epsilon\}\big{)}=0. \tag{5.2}\]

**Definition 2.5.4:** Let \(0<p<\infty\). Then, \(\{f_{n}\}_{n\geq 1}\) _converges to \(f\) in \(L^{p}(\mu)\)_, denoted by \(f_{n}\longrightarrow^{L^{p}}f\), if \(\int|f_{n}|^{p}d\mu<\infty\) for all \(n\geq 1\), \(\int|f|^{p}d\mu<\infty\) and

\[\lim_{n\to\infty}\int|f_{n}-f|^{p}d\mu=0. \tag{5.3}\]

Clearly, (5.3) is equivalent to \(\|f_{n}-f\|_{p}\to 0\) as \(n\to\infty\), where for any \(\mathcal{F}\)-measurable function \(g\) and any \(0<p<\infty\),

\[\|g\|_{p}=\Big{(}\int|g|^{p}d\mu\Big{)}^{\min\{\frac{1}{p},1\}}. \tag{5.4}\]

For \(p=1\), this is also called _convergence in absolute deviation_ and for \(p=2\), _convergence in mean square_.

**Definition 2.5.5:**\(\{f_{n}\}_{n\geq 1}\) _converges to \(f\) uniformly_ (over \(\Omega\)) if

\[\lim_{n\to\infty}\ \sup\{|f_{n}(\omega)-f(\omega)|:\omega\in\Omega\}=0. \tag{5.5}\]

**Definition 2.5.6:**\(\{f_{n}\}_{n\geq 1}\) _converges to \(f\) in \(L^{\infty}(\mu)\)_ if

\[\lim_{n\to\infty}\|f_{n}-f\|_{\infty}=0, \tag{5.6}\]

where for any \(\mathcal{F}\)-measurable function \(g\) on \((\Omega,\mathcal{F},\mu)\),

\[\|g\|_{\infty}=\inf\big{\{}K:K\in(0,\infty),\mu(\{|g|>K\})=0\big{\}}. \tag{5.7}\]

**Definition 2.5.7:**\(\{f_{n}\}_{n\geq 1}\) _converges to \(f\) nearly uniformly_ (\(\mu\)) if for every \(\epsilon>0\), there exists a set \(A\in\mathcal{F}\) such that \(\mu(A)<\epsilon\) and on \(A^{c}\), \(f_{n}\to f\) uniformly, i.e., \(\sup\{|f_{n}(\omega)-f(\omega)|:\omega\in A^{c}\}\to 0\) as \(n\to\infty\).

The notion of convergence in Definition 2.5.7 is also called almost uniform convergence in some books (cf. Royden (1988)). The sequence \(f_{n}\equiv nI_{[0,1/n]}\) on \((\Omega=[0,1],\mathcal{B}([0,1]),m)\) converges to \(f\equiv 0\) nearly uniformly but not in \(L^{\infty}(m)\).

When \(\mu\) is a probability measure, there is another useful notion of convergence, known as convergence in distribution, that is defined in terms of the induced measures \(\{\mu f_{n}^{-1}\}_{n\geq 1}\) and \(\mu f^{-1}\). This notion of convergence will be treated in detail in Chapter 9.

Next, the connections between some of these notions of convergence are explored.

**Theorem 2.5.1:**_Suppose that \(\mu(\Omega)<\infty\). Then, \(f_{n}\to f\) a.e. \((\mu)\) implies \(f_{n}\longrightarrow^{m}f\)._

The proof is left as an exercise (Problem 2.26). The hypothesis that '\(\mu(\Omega)<\infty\)' in Theorem 2.5.1 cannot be dispensed with as seen by taking \(f_{n}=I_{[n,\infty)}\) on \(\mathbb{R}\) with Lebesgue measure. Also, \(f_{n}\longrightarrow^{m}f\) does not imply \(f_{n}\to f\) a.e. \((\mu)\) (Problem 2.46), but the following holds.

**Theorem 2.5.2:**_Let \(f_{n}\longrightarrow^{m}f\). Then, there exists a subsequence \(\{n_{k}\}_{k\geq 1}\) such that \(f_{n_{k}}\to f\) a.e. \((\mu)\)._

**Proof:** Since \(f_{n}\longrightarrow^{m}f\), for each integer \(k\geq 1\), there exists an \(n_{k}\) such that for all \(n\geq n_{k}\)

\[\mu\Big{(}\big{\{}|f_{n}-f|>2^{-k}\big{\}}\Big{)}<2^{-k}. \tag{5.8}\]

W.l.o.g., that assume \(n_{k+1}>n_{k}\quad\mbox{for all}\quad k\geq 1\). Let \(A_{k}=\{|f_{n_{k}}-f|>2^{-k}\}\). By Corollary 2.3.5,

\[\int\left(\sum_{k=1}^{\infty}I_{A_{k}}\right)d\mu=\sum_{k=1}^{\infty}\int I_{A _{k}}d\mu=\sum_{k=1}^{\infty}\mu(A_{k}),\]

which, by (5.8), is finite. Hence, by Proposition 2.3.10, \(\sum_{k=1}^{\infty}I_{A_{k}}<\infty\) a.e. \((\mu)\). Now observe that \(\sum_{k=1}^{\infty}I_{A_{k}}(\omega)<\infty\Rightarrow|f_{n_{k}}(\omega)-f( \omega)|\leq 2^{-k}\) for all \(k\) large \(\Rightarrow\lim_{k\to\infty}f_{n_{k}}(\omega)=f(\omega)\). Thus, \(f_{n_{k}}\to f\) a.e. \((\mu)\). \(\Box\)

**Remark 2.5.1:** From the above result it follows that the extended dominated convergence theorem (Theorem 2.3.11) remains valid if convergence a.e. of \(\{f_{n}\}_{n\geq 1}\) and of \(\{g_{n}\}_{n\geq 1}\) are replaced by convergence in measure for both (Problem 2.37).

**Theorem 2.5.3:**_Let \(\{f_{n}\}_{n\geq 1}\), \(f\) be measurable functions on a measure space \((\Omega,\mathcal{F},\mu)\). Let \(f_{n}\longrightarrow^{L^{p}}f\) for some \(0<p<\infty\). Then \(f_{n}\longrightarrow^{m}f\)._

**Proof:** For each \(\epsilon>0\), let \(A_{n}=\{|f_{n}-f|\geq\epsilon\}\), \(n\geq 1\). Then

\[\int|f_{n}-f|^{p}d\mu\geq\int_{A_{n}}|f_{n}-f|^{p}d\mu\geq\epsilon^{p}\mu(A_{n }).\]

Since \(f_{n}\to f\) in \(L^{p},\ \int|f_{n}-f|^{p}d\mu\to 0\) and hence, \(\mu(A_{n})\to 0\). \(\Box\)

It turns out that \(f_{n}\longrightarrow^{m}f\) need not imply \(f_{n}\longrightarrow^{L^{p}}f\), even if \(\{f_{n}:n\geq 1\}\cup\{f\}\) is contained in \(L^{p}(\Omega,\mathcal{F},\mu)\). For example, let \(f_{n}=nI_{[0,\frac{1}{n}]}\) and \(f\equiv 0\) on the Lebesgue space \(([0,1],\mathcal{B}([0,1]),m)\), where \(m\) is the Lebesgue measure. Then \(f_{n}\longrightarrow^{m}f\) but \(\int|f_{n}-f|\equiv 1\) for all \(n\geq 1\). However, under some additional conditions, convergence in measure does imply convergence in \(L^{p}\). Here are two results in this direction.

**Theorem 2.5.4:** (_Scheffe's theorem_). Let \(\{f_{n}\}_{n\geq 1},f\) be a collection of nonnegative measurable functions on a measure space \((\Omega,\mathcal{F},\mu)\). Let \(f_{n}\to f\) a.e. \((\mu)\), \(\int f_{n}d\mu\rightarrow\int fd\mu\) and \(\int fd\mu<\infty\). Then_

\[\lim_{n\rightarrow\infty}\int|f_{n}-f|d\mu=0.\]

**Proof:** Let \(g_{n}=f-f_{n},\ n\geq 1\). Since \(f_{n}\to f\) a.e. \((\mu)\), both \(g_{n}^{+}\) and \(g_{n}^{-}\) go to zero a.e. \((\mu)\). Further, \(0\leq g_{n}^{+}\leq f\) and by hypothesis \(\int fd\mu<\infty\). Thus, by the DCT, it follows that

\[\int g_{n}^{+}d\mu\to 0.\]

Next, note that by hypothesis, \(\int g_{n}d\mu\to 0\). Thus, \(\int g_{n}^{-}d\mu=\int g_{n}^{+}d\mu-\int g_{n}d\mu\to 0\) and hence, \(\int|g_{n}|d\mu=\int g_{n}^{+}d\mu+\int g_{n}^{-}d\mu\to 0\). \(\Box\)

**Corollary 2.5.5:** _Let \(\{f_{n}\}_{n\geq 1}\), \(f\) be probability density functions on a measure space \((\Omega,\mathcal{F},\mu)\). That is, for all \(n\geq 1\), \(\int f_{n}d\mu=\int fd\mu=1\) and \(f_{n}\), \(f\geq 0\) a.e. \((\mu)\). If \(f_{n}\to f\) a.e. \((\mu)\), then_

\[\lim_{n\rightarrow\infty}\int|f_{n}-f|d\mu=0.\]

**Remark 2.5.2:** The above theorem and the corollary remain valid if the convergence of \(f_{n}\) to \(f\) a.e. \((\mu)\) is replaced by \(f\longrightarrow^{m}f\).

**Corollary 2.5.6:** _Let \(\{p_{nk}\}_{k\geq 1}\), \(n=1,2,\ldots\) and \(\{p_{k}\}_{k\geq 1}\) be sequences of nonnegative numbers satisfying \(\sum_{k=1}^{\infty}p_{nk}=1=\sum_{k=1}^{\infty}p_{k}\). Let \(p_{nk}\to p_{k}\) as \(n\rightarrow\infty\) for each \(k\geq 1\). Then \(\sum_{k=1}^{\infty}|p_{nk}-p_{k}|\to 0\)._

**Proof:** Apply Corollary 2.5.5 with \(\mu=\) the counting measure on \((\mathbb{N},\mathcal{P}(\mathbb{N}))\). \(\Box\)

A more general result in this direction that does not require \(f_{n}\), \(f\) to be nonnegative involves the concept of _uniform integrability_. Let \(\{f_{\lambda}:\lambda\in\Lambda\}\) be a collection of functions in \(L^{1}(\Omega,\mathcal{F},\mu)\). Then for each \(\lambda\in\Lambda\), by the DCT and the integrability of \(f_{\lambda}\),

\[a_{\lambda}(t)\equiv\int_{\{|f_{\lambda}|>t\}}|f_{\lambda}|d\mu\to 0\quad \mbox{as}\quad t\rightarrow\infty. \tag{5.9}\]

The notion of uniform integrability requires that the integrals \(a_{\lambda}(t)\) go to zero uniformly in \(\lambda\in\Lambda\) as \(t\rightarrow\infty\).

[MISSING_PAGE_FAIL:81]

\[= \int h_{t}\big{(}2|f_{\lambda}|\big{)}I\big{(}|f_{\lambda}|\geq|g_{ \gamma}|\big{)}d\mu+\int h_{t}\big{(}2|g_{\gamma}|\big{)}I\big{(}|f_{\lambda}|<|g _{\gamma}|\big{)}d\mu\] \[\leq 2\int_{\{|f_{\lambda}|>t/2\}}|f_{\lambda}|d\mu+2\int_{\{|g_{ \gamma}|>t/2\}}|g_{\gamma}|d\mu\] \[\leq 2[a(t/2)+b(t/2)].\]

By hypothesis, both \(a(t)\) and \(b(t)\to 0\) as \(t\to\infty\), thus proving (iv).

Next consider (v). Since \(\{f_{\lambda}\}:\lambda\in\Lambda\}\) is UI, there exists a \(T>0\) such that

\[\sup_{\lambda\in\Lambda}\int h_{T}\big{(}|f_{\lambda}|\big{)}d\mu\leq 1.\]

Hence,

\[\sup_{\lambda\in\Lambda}\int|f_{\lambda}|d\mu = \sup_{\lambda\in\Lambda}\left\{\int_{\{|f_{\lambda}|\leq T\}}|f_ {\lambda}|d\mu+\int h_{T}(|f_{\lambda}|)d\mu\right\}\] \[\leq T\mu(\Omega)+1<\infty.\]

This completes the proof of the proposition. \(\Box\)

**Remark 2.5.3:** In the above proposition, part (ii) can be improved as follows: _Let \(\phi:\mathbb{R}_{+}\to\mathbb{R}_{+}\) be nondecreasing and \(\frac{\phi(x)}{x}\uparrow\infty\) as \(x\uparrow\infty\). If \(\sup_{\lambda\in\Lambda}\int\phi(|f_{\lambda}|)d\mu<\infty\), then \(\{f_{\lambda}:\lambda\in\Lambda\}\) is U\(I\)_ (Problem 2.27). A converse to this result is true. That is, if \(\{f_{\lambda}:\lambda\in\Lambda\}\) is UI then there exists such a function \(\phi\). Some examples of such \(\phi\)'s are \(\phi(x)=x^{k},\ \ k>1,\ \ \ \phi(x)=x(\log x)^{\beta}I(x>1),\ \beta>0\), and \(\phi(x)=\exp(\beta x),\ \beta>0\)._

In part (v) of Proposition 2.5.7, (5.11) does not imply UI. For example, consider the sequence of functions \(f_{n}=nI_{[0,\frac{1}{n}]},n=1,2,\ldots\) on \([0,1]\). On the other hand, (5.11) with an additional condition becomes necessary and sufficient for UI.

**Proposition 2.5.8:** _Let \(f\in L^{1}(\Omega,\mathcal{F},\mu)\). Then for every \(\epsilon>0\), there exists a \(\delta>0\) such that \(\mu(A)<\delta\Rightarrow\int_{A}|f|d\mu<\epsilon\)._

**Proof:** Fix \(\epsilon>0\). By the DCT, there exists a \(t>0\) such that \(\int_{\{|f|>t\}}|f|d\mu<\epsilon/2\). Hence, for any \(A\in\mathcal{F}\) with \(\mu(A)\leq\delta\equiv\frac{\epsilon}{2t}\),

\[\int_{A}|f|d\mu \leq \int_{A\cap\{|f|\leq t\}}|f|d\mu+\int_{\{|f|>t\}}|f|d\mu\] \[\leq t\mu(A)+\int_{\{|f|>t\}}|f|d\mu\] \[\leq \epsilon,\]

proving the claim. \(\Box\)The above proposition shows that for every \(f\in L^{1}(\Omega,{\cal F},\mu)\), the measure (cf. Corollary 2.3.6)

\[\nu_{|f|}(A)\equiv\int_{A}|f|d\mu \tag{5.12}\]

on \((\Omega,{\cal F})\) satisfies the condition that \(\nu_{|f|}(A)\) is small if \(\mu(A)\) is small, i.e., for every \(\epsilon>0\), there exists a \(\delta>0\) such that \(\mu(A)<\delta\Rightarrow\nu_{|f|}(A)<\epsilon\). This property is referred to as the _absolute continuity_ of the measure \(\nu_{f}\) w.r.t. \(\mu\).

**Definition 2.5.9:** Given a family \(\{f_{\lambda}:\lambda\in\Lambda\}\subset L^{1}(\Omega,{\cal F},\mu)\), the measures \(\{\nu_{|f_{\lambda}|}:\lambda\in\Lambda\}\) as defined in (5.12) above are _uniformly absolutely continuous_ w.r.t. \(\mu\) (or _u.a.c. (\(\mu\)_), in short) if for every \(\epsilon>0\), there exists a \(\delta>0\) such that

\[\mu(A)<\delta\Rightarrow\sup\Big{\{}\nu_{|f_{\lambda}|}(A):\lambda\in\Lambda \Big{\}}<\epsilon.\]

**Theorem 2.5.9:**_Let \(\{f_{\lambda}:\lambda\in\Lambda\}\subset L^{1}(\Omega,{\cal F},\mu)\) and \(\mu(\Omega)<\infty\). Then, \(\{f_{\lambda}:\lambda\in\Lambda\}\) is UI iff \(\sup_{\lambda\in\Lambda}\int|f_{\lambda}|d\mu<\infty\) and \(\big{\{}\nu_{|f_{\lambda}|}(\cdot):\lambda\in\Lambda\big{\}}\) is u.a.c. \((\mu)\)._

**Proof:** Let \(\big{\{}f_{\lambda}:\lambda\in\Lambda\big{\}}\) be UI. Then, since \(\mu(\Omega)<\infty\), \(L_{1}\) boundedness of \(\{f_{\lambda}:\lambda\in\Lambda\}\) follows from Proposition 2.5.7 (v). To establish u.a.c. \((\mu)\), fix \(\epsilon>0\). By UI, there exists an \(N\) such that

\[\sup_{\lambda\in\Lambda}\int_{\{|f_{\lambda}|>N\}}|f_{\lambda}|d\mu<\epsilon/2.\]

Let \(\delta=\frac{\epsilon}{2N}\) and let \(A\in{\cal F}\) be such that \(\mu(A)<\delta\). Then, as in the proof of Proposition 2.5.8 above,

\[\sup_{\lambda\in\Lambda}\int_{A}|f_{\lambda}|d\mu<\epsilon\,\,\,\mbox{if}\,\, \mu(A)<\delta_{\epsilon}.\]

Conversely, suppose \(\big{\{}f_{\lambda}:\lambda\in\Lambda\big{\}}\) is \(L^{1}\) bounded and u.a.c. \((\mu)\). Then, for every \(\epsilon>0\), there exists a \(\delta_{\epsilon}>0\) such that

\[\sup_{\lambda\in\Lambda}\int_{A}|f_{\lambda}|d\mu<\epsilon\,\,\,\mbox{if}\,\, \mu(A)<\delta_{\epsilon}. \tag{5.13}\]

Also, for any nonnegative \(f\) in \(L_{1}(\Omega,{\cal F},\mu)\) and \(t>0\), \(\int fd\mu\geq\int_{\{f>t\}}fd\mu\geq t\mu\big{(}\{f\geq t\}\big{)}\), which implies that

\[\mu\big{(}\{f\geq t\}\big{)}\leq\frac{\int fd\mu}{t}.\]

(This is known as _Markov's inequality_ \(-\) see Chapter 3). Hence, it follows that

\[\sup_{\lambda\in\Lambda}\mu(\{|f_{\lambda}|\geq t\})\leq\Big{[}\sup_{\lambda \in\Lambda}\int|f_{\lambda}|d\mu\Big{]}/\,t. \tag{5.14}\]Now given \(\epsilon>0\), choose \(T_{\epsilon}\) such that \(\Bigl{[}\sup_{\lambda\in\Lambda}\int|f_{\lambda}|d\mu\Bigr{]}/T_{\epsilon}<\delta_ {\epsilon}\) where \(\delta_{\epsilon}\) is as in (5.13). Then, by (5.14), it follows that

\[\sup_{\lambda\in\Lambda}\int_{\{|f_{\lambda}|\geq T_{\epsilon}\}}|f_{\lambda}|d \mu\,<\,\epsilon,\]

i.e., \(\{f_{\lambda}:\lambda\in\Lambda\}\) is UI. \(\Box\)

**Theorem 2.5.10:**_Let \((\Omega,{\cal F},\mu)\) be a measure space with \(\mu(\Omega)<\infty\), and let \(\{f_{n}:n\geq 1\}\subset L^{1}(\Omega,{\cal F},\mu)\) be such that \(f_{n}\to f\) a.e. \((\mu)\) and \(f\) is \(\langle{\cal F},{\cal B}({\mathbb{R}})\rangle\)-measurable. If \(\{f_{n}:n\geq 1\}\) is UI, then \(f\) is integrable and_

\[\lim_{n\to\infty}\int|f_{n}-f|d\mu=0.\]

**Remark 2.5.4:** In view of Proposition 2.5.7 (iii), Theorem 2.5.10 yields convergence of \(\int fd\mu\) to \(\int fd\mu\) under _weaker_ conditions than the DCT, provided \(\mu(\Omega)<\infty\). However, even under the restriction \(\mu(\Omega)<\infty\), UI of \(\{f_{n}\}_{n\geq 1}\) is a sufficient, but _not_ a necessary condition for convergence of \(\int f_{n}d\mu\) to \(\int fd\mu\) (Problem 2.28). In the special case where \(f_{n}\)'s are non-negative (and \(\mu(\Omega)<\infty\)), \(\int f_{n}d\mu\to\int fd\mu<\infty\) if and only if \(\{f_{n}\}_{n\geq 1}\) are UI (Problem 2.29). On the other hand, when \(\mu(\Omega)=+\infty\), UI is no longer sufficient to guarantee the convergence of \(\int f_{n}d\mu\) to \(\int fd\mu\) (Problem 2.30). Thus, the notion of UI is useful mainly for _finite_ measures and, in particular, probability measures.

**Proof:** By Proposition 2.5.7 and Fatou's lemma,

\[\int|f|d\mu\leq\liminf_{n\to\infty}\int|f_{n}|d\mu\leq\sup_{n\geq 1}\int|f_{n} |d\mu<\infty\]

and hence \(f\) is integrable.

Next for \(n\in{\mathbb{N}},t\in(0,\infty)\), define the functions \(g_{n}=|f_{n}-f|\), \(\underline{g}_{n,t}=g_{n}I(|g_{n}|\leq t)\) and \(\bar{g}_{n,t}=g_{n}I(|g_{n}|>t)\). Since \(\{f_{n}\}_{n\geq 1}\) is UI and \(f\) is integrable, by Proposition 2.5.7 (iv), \(\{g_{n}\}_{n\geq 1}\) is UI. Hence, for any \(\epsilon>0\), there exists \(t_{\epsilon}>0\) such that

\[\sup_{n\geq 1}\int\bar{g}_{n,t}d\mu<\epsilon\quad\mbox{for all}\quad t\geq t_{ \epsilon}. \tag{5.15}\]

Next note that for any \(t>0\), since \(f_{n}\to f\) a.e. \((\mu)\), \(\underline{g}_{n,t}\to 0\) a.e. \((\mu)\), and \(|\underline{g}_{n,t}|\leq t\) and \(\int(t)d\mu=t\mu(\Omega)<\infty\). Hence, by the DCT,

\[\lim_{n\to\infty}\int\underline{g}_{n,t}d\mu=0\quad\mbox{for all}\quad t>0. \tag{5.16}\]

[MISSING_PAGE_FAIL:85]

Let \(f_{K}(x)=f(x)I_{[a,b]}(x)I_{\{|f|\leq K\}}(x).\) Then \(f_{K}:\mathbb{R}\to\mathbb{R}\) is bounded, \(\langle\mathcal{M}_{\mu_{F}^{*}},\mathcal{B}(\mathbb{R})\rangle\)-measurable and zero for \(|x|>K.\) Consider now the following claim: _For every \(\epsilon>0\), there exists a continuous function \(g:[a,b]\to\mathbb{R}\) such that_

\[\mu\big{(}\{x:f_{K}(x)\neq g(x),a\leq x\leq b\}\big{)}<\frac{\epsilon}{2}. \tag{5.17}\]

Clearly this implies that

\[\mu\big{(}\{x:a\leq x\leq b,f(x)\neq g(x)\}\big{)}<\epsilon. \tag{5.18}\]

Fix \(\delta>0.\) Now for each \(n\in\mathbb{Z},\) take \([a,b]=[n,n+1],\)\(\epsilon=\frac{\delta}{2^{|n|+2}},\) apply (5.18) and call the resulting continuous function \(g_{n}.\) Let \(\tilde{g}\) be a continuous function from \(\mathbb{R}\to\mathbb{R}\) such that \(\mu(\{x:n\leq x\leq n+1,\tilde{g}(x)\neq g_{n}(x)\})<\frac{\delta}{2^{|n|+2}}.\) This can be done by setting \(\tilde{g}(x)=g_{n}(x)\) for \(n\leq x\leq(n+1)-\delta_{n}\) and linear on \(\big{[}(n+1)-\delta_{n},n+1\big{]}\) for some \(0<\delta_{n}<\frac{\delta}{2^{|n|+3}}.\) Then

\[\mu\big{(}\{x:f(x)\neq\tilde{g}(x)\}\big{)} \leq \sum_{n=-\infty}^{\infty}\mu\big{(}\{x:n\leq x\leq n+1,\,f(x)\neq \tilde{g}(x)\}\big{)}\] \[\leq \sum_{n=-\infty}^{\infty}\mu\big{(}\{x:n\leq x\leq n+1,f(x)\neq g _{n}(x)\}\big{)}\] \[+\sum_{n=-\infty}^{\infty}\mu\big{(}\{x:n\leq x\leq n+1,\,g_{n}(x )\neq\tilde{g}(x)\}\big{)}\] \[< 2\sum_{n=-\infty}^{\infty}\frac{\delta}{2^{|n|+2}}\leq 2\delta.\]

So it suffices to establish (5.17). Since \(f_{K}:\mathbb{R}\to\mathbb{R}\) is bounded and \(\langle\mathcal{M}_{\mu_{F}^{*}},\mathcal{B}(\mathbb{R})\rangle\)-measurable, for each \(\epsilon>0,\) there exists a simple function \(s(x)\equiv\sum_{i=1}^{k}c_{i}I_{A_{i}}(x),\) with \(A_{i}\subset[a,b],\)\(A_{i}\in\mathcal{M}_{\mu_{F}^{*}},\)\(\{A_{i}:1\leq i\leq k\}\) are disjoint, \(\mu(A_{i})<\infty,\) and \(c_{i}\in\mathbb{R}\) for \(i=1,\ldots,k,\) such that \(|f(x)-s(x)|<\frac{\epsilon}{4}\) for all \(a\leq x\leq b.\) By Theorem 1.3.4, for each \(A_{i}\) and \(\eta>0,\) there exist a finite number of open disjoint intervals \(I_{ij}=(a_{ij},b_{ij}),j=1,\ldots n_{i}\) such that

\[\mu\bigg{(}A_{i}\bigtriangleup\bigcup_{j=1}^{n_{i}}I_{ij}\bigg{)}<\frac{\eta }{2k}.\]

Now as in Problem 1.32 (g), there exists a continuous function \(g_{ij}\) such that

\[\mu\Big{(}g_{ij}^{-1}\{1\}\bigtriangleup I_{ij}\Big{)}<\frac{\eta}{kn_{i}},\ j=1,2,\ldots,n_{i},\ i=1,2,\ldots,k.\]

Let

\[g_{i}\equiv\sum_{j=1}^{n_{i}}g_{ij},\ 1\leq i\leq k.\]Then \(\mu(A_{i}\bigtriangleup g_{i}^{-1}\{1\})<\frac{\eta}{k}.\) Let \(g=\sum_{i=1}^{k}c_{i}g_{i}.\) Then \(\mu(\{s\neq g\})<\eta.\) Hence for every \(\epsilon>0,\)\(\eta>0,\) there is a continuous function \(g_{\epsilon,\eta}:[a,b]\to\mathbb{R}\) such that

\[\mu\big{(}\big{\{}x:a\leq x\leq b,|f_{K}(x)-g_{\epsilon,\eta}(x)|>\epsilon\big{\}} \big{)}<\eta.\]

Now for each \(n\geq 1,\) let

\[h_{n}(\cdot)\equiv g_{\frac{1}{2^{n}},\frac{1}{2^{n}}}(\cdot)\quad\mbox{and} \quad A_{n}=\big{\{}x:a\leq x\leq b,|f_{K}(x)-h_{n}(x)|>\frac{1}{2^{n}}\big{\}}.\]

Then, \(\mu(A_{n})<\frac{1}{2^{n}}\) and hence

\[\sum_{n=1}^{\infty}\mu(A_{n})<\infty.\]

By the MCT, this implies that \(\int_{[a,b]}\big{(}\sum_{n=1}^{\infty}I_{A_{n}}\big{)}d\mu<\infty\) and hence

\[\sum_{n=1}^{\infty}I_{A_{n}}<\infty\quad\mbox{a.e.}\quad\mu.\]

Thus \(h_{n}\to f_{K}\) a.e. \(\mu\) on \([a,b].\) By Egorov's theorem for any \(\epsilon>0,\) there is a set \(A_{\epsilon}\in\mathcal{B}([a,b])\) such that

\[\mu(A_{\epsilon}^{c})<\epsilon/2\quad\mbox{and}\quad h_{n}\to f_{K}\quad\mbox{ uniformly on}\quad A_{\epsilon}.\]

By the inner regularity (Corollary 1.3.5) of \(\mu,\) there is a compact set \(D\subset A_{\epsilon}\) such that

\[\mu(A_{\epsilon}\backslash D)<\epsilon/2.\]

Since \(h_{n}\to f_{K}\) uniformly on \(A_{\epsilon},\)\(f_{K}\) is continuous on \(A_{\epsilon}\) and hence on \(D.\) It can be shown that there exists a continuous function \(g:[a,b]\to\mathbb{R}\) such that \(g=f_{K}\) on \(D\) (Problem 2.8 (e)). A more general result extending a continuous function defined on a closed set to the whole space is known as Tietze's extension theorem (see Munkres (1975)). Thus \(\mu(\{x:a\leq x\leq b,f_{K}(x)\neq g(x)\})<\epsilon.\) This completes the proof of (5.17) and hence that of the proposition. \(\Box\)

**Remark 2.5.5:** (_Littlewood's principles_). As pointed out in Section 1.3, Theorems 1.3.4, 2.5.11, and 2.5.12 constitute J. E. Littlewood's three principles: _every \(\mathcal{M}_{\mu_{F}^{*}}\) measurable set is nearly a finite union of intervals; every a.e. convergent sequence is nearly uniformly convergent; and every \(\mathcal{M}_{\mu_{F}^{*}}\)-measurable function is nearly continuous_.

### 2.6 Problems

* Let \(\Omega_{i},\ i=1,2\) be two nonempty sets and \(T:\Omega_{1}\to\Omega_{2}\) be a map. Then for any collection \(\{A_{\alpha}:\alpha\in I\}\) of subsets of \(\Omega_{2},\) show that \[T^{-1}\Big{(}\bigcup_{\alpha\in I}A_{\alpha}\Big{)} = \bigcup_{\alpha\in I}T^{-1}\big{(}A_{\alpha}\big{)}\]Integration \[\mbox{and}\ \ T^{-1}\Big{(}\bigcap_{\alpha\in I}A_{\alpha}\Big{)} = \bigcap_{\alpha\in I}T^{-1}\big{(}A_{\alpha}\big{)}.\] Further, \(\big{(}T^{-1}(A)\big{)}^{c}=T^{-1}(A^{c})\) for all \(A\subset\Omega_{2}.\) (These are known as _de Morgan's_ laws.)
2. Let \(\{A_{i}\}_{i\geq 1}\) be a collection of disjoint sets in a measurable space \((\Omega,{\cal F}).\) 1. Let \(\{g_{i}\}_{i\geq 1}\) be a collection of \(\langle{\cal F},{\cal B}({\mathbb{R}})\rangle\)-measurable functions from \(\Omega\) to \({\mathbb{R}}.\) Show that \(\sum_{i=1}^{\infty}I_{A_{i}}g_{i}\) converges on \({\mathbb{R}}\) and is \(\langle{\cal F},{\cal B}({\mathbb{R}})\rangle\)-measurable. 2. Let \({\cal G}\equiv\sigma\langle\{A_{i}:i\geq 1\}\rangle.\) Show that \(h:\Omega\to{\mathbb{R}}\) is \(\langle{\cal G},{\cal B}({\mathbb{R}})\rangle\)-measurable iff \(g(\cdot)\) is constant on each \(A_{i}.\)
2. Let \(f,g:\Omega\to{\mathbb{R}}\) be \(\langle{\cal F},{\cal B}({\mathbb{R}})\rangle\)-measurable. Set \[h(\omega)=\frac{f(\omega)}{g(\omega)}I(g(\omega)\neq 0),\quad\omega\in\Omega.\] Verify that \(h\) is \(\langle{\cal F},{\cal B}({\mathbb{R}})\rangle\)-measurable.
2. Let \(g:\Omega\to{\mathbb{R}}\) be such that for every \(r\in{\mathbb{R}},\)\(g^{-1}((-\infty,r])\in{\cal F}.\) Show that \(g\) is \(\langle{\cal F},{\cal B}({\mathbb{R}})\rangle\)-measurable.
2. Prove Proposition 2.1.6. (**Hint:** Show that \[\sigma\langle\{f_{\lambda}:\lambda\in\Lambda\}\rangle=\bigcup_{L\in{\cal C}} \ \sigma\langle\{f_{\lambda}:\lambda\in L\}\rangle\] where \({\cal C}\) is the collection of all countable subsets of \(\Lambda.\))
2. Let \(X_{i},\,i=1,2,3\) be random variables on a probability space \((\Omega,{\cal F},P).\) Consider the random equation (in \(t\in{\mathbb{R}}\)): \[X_{1}(\omega)t^{2}+X_{2}(\omega)t+X_{3}(\omega)=0.\] (6.1) 1. Show that \(A\equiv\{\omega\in\Omega:\) Equation (6.1) has two distinct real roots\(\}\in{\cal F}.\) 2. Let \(T_{1}(\omega)\) and \(T_{2}(\omega)\) denote the two roots of (6.1) on \(A.\) Let \[f_{i}(w)=\left\{\begin{array}{ll}T_{i}(w)&\mbox{on $A$}\\ 0&\mbox{on $A^{c}$}\end{array}\right.,\] \(i=1,2.\) Show that \((f_{1},f_{2})\) is \(\langle{\cal F},{\cal B}({\mathbb{R}}^{2})\rangle\)-measurable.

2.7 Let \(M\equiv\big{(}\big{(}X_{ij}\big{)}\big{)},1\leq i,j\leq k,\) be a (random) matrix of random variables \(X_{ij}\) defined on a probability space \((\Omega,\mathcal{F},P).\) 1. Show that \(Y_{1}\equiv det(M)\) (the determinant of \(M\)) and \(Y_{2}\equiv tr(M)\)(the trace of \(M\)) are both \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable. 2. Show also that \(Y_{3}\equiv\) the largest eigenvalue of \(M^{\prime}M\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable, where \(M^{\prime}\) is the transpose of \(M\). (**Hint:** Use the result that \(Y_{3}=\sup_{x\neq 0}\frac{(x^{\prime}M^{\prime}Mx)}{(x^{\prime}x)}.\))
2.8 Let \(f:\mathbb{R}\to\mathbb{R}.\) Let \(\bar{f}(x)=\inf_{\delta>0}\sup_{|y-x|<\delta}f(y)\) and \(\underline{f}(x)=\sup_{\delta>0}\inf_{|y-x|<\delta}f(y),\)\(x\in\mathbb{R}.\) 1. Show that for any \(t\in\mathbb{R},\) \[\{x:\bar{f}(x)<t\}\] is open and hence, \(\bar{f}\) is \(\langle\mathcal{B}(\mathbb{R}),\mathcal{B}(\mathbb{R})\rangle\)-measurable. 2. Show that for any \(t>0,\) \[\{x:\bar{f}(x)-\underline{f}(x)<t\}\equiv\bigcup_{r\in\mathbb{Q}}\{x:\bar{f}(x )<t+r,\underline{f}(x)>r\}\] and hence is open. 3. Show that \(f\) is continuous at some \(x_{0}\) in \(\mathbb{R}\) iff \(\bar{f}(x_{0})=\underline{f}(x_{0}).\) 4. Show that the set \(C_{f}\equiv\{x:f(\cdot)\) is continuous at \(x\}\) is a \(G_{\delta}\) set, i.e., an intersection of a countable number of open sets, and hence, \(C_{f}\) is a Borel set. 5. Let \(D\) be a closed set in \(\mathbb{R}.\) Let \(f:D\to\mathbb{R}\) be continuous on \(D.\) Show that there exists a \(g:\mathbb{R}\to\mathbb{R}\) continuous such that \(g=f\) on \(D.\) (**Hint:** Note that \(D^{c}\) is open in \(\mathbb{R}\) and hence it can be expressed as a countable union of disjoint open intervals \(\{I_{j}=(a_{j},b_{j}):1\leq j\leq k\leq\infty\}.\) Note that \(a_{j},b_{j}\in D\) for all \(j\) except for possibly the \(j\)'s for which \(a_{j}=-\infty\) or \(b_{j}=+\infty.\) Let \[g(x)\equiv\left\{\begin{array}{ll}f(x)&\mbox{if}\quad x\in D\\ f(a_{j})+\frac{(x-a_{j})}{(b_{j}-a_{j})}(f(b_{j})-f(a_{j}))&\mbox{if}\quad x\in (a_{j},b_{j}),\\ &\mbox{}\quad a_{j},b_{j}\in D\\ f(b_{j})&\mbox{if}\quad a_{j}=-\infty,\\ &\mbox{}\quad x\in(a_{j},b_{j})\\ f(a_{j})&\mbox{if}\quad b_{j}=\infty,\\ &\mbox{}\quad x\in(a_{j},b_{j}).\end{array}\right.\] Now verify that \(g\) has the required properties.)2.9 Prove Proposition 2.2.1 using Problem 2.1. * Show that for any \(x\in\mathbb{R}\) and any random variable \(X\) with cdf \(F_{X}(\cdot)\), \(P(X<x)=F_{X}(x-)\equiv\lim_{y\uparrow x}F_{X}(y)\). * Show that \(F_{c}(\cdot)\) in (2.5) is continuous.
2.11 Let \(F:\mathbb{R}\to\mathbb{R}\) be nondecreasing. * Show that for \(x\in\mathbb{R}\), \(F(x-)\equiv\lim_{y\uparrow x}F(y)\) and \(F(x+)=\lim_{y\downarrow x}F(y)\) exist and satisfy \(F(x-)\leq F(x)\leq F(x+)\). * Let \(D\equiv\{x:F(x+)-F(x-)>0\}\). Show that \(D\) is at most countable. * Show that \[D=\bigcup_{n\geq 1}\bigcup_{r\geq 1}D_{n,r},\] where \(D_{n,r}=\{x:|x|\leq n,F(x+)-F(x-)>\frac{1}{r}\}\) and that each \(D_{n,r}\) is finite.)
2.12 Suppose that (i) and (iii) of Proposition 2.2.3 hold. Show that for any \(a_{1}\) in \(\mathbb{R}\) and \(-\infty<a_{2}\leq b_{2}<\infty\), \(F(a_{2},a_{1})\leq F(b_{2},a_{1})\) and \(F(a_{1},a_{2})\leq F(a_{1},b_{2})\), (i.e., \(F\) is monotone coordinatewise).
2.13 Let \(F:\mathbb{R}^{k}\to\mathbb{R}\) be such that: * for \(x_{1}=(x_{11},x_{12},\ldots,x_{1k})\) and \(x_{2}=(x_{21},x_{22},\ldots,x_{2k})\) with \(x_{1i}\leq x_{2i}\) for \(i=1,2,\ldots k\), \[\Delta F(x_{1},x_{2})\equiv\sum_{a\in A}(-1)^{s(a)}F(a)\geq 0,\] where \(A\equiv\{a=(a_{1},a_{2},\ldots,a_{k}):a_{i}\in\{x_{1i},x_{2i}\},i=1,2,\ldots,k\}\) and for \(a\) in \(A\), \(s(a)=|\{i:a_{i}=x_{1i},i=1,2,\ldots,k\}|\) is the number of indices \(i\) for which \(a_{i}=x_{1i}\). * For each \(i=1,2,\ldots,k\), \(\lim_{x_{i1}\downarrow-\infty}F(x_{i})=0\). Let \(\mathcal{C}_{k}\) be the semialgebra of sets of the form \(\{A:A=A_{1}\times\ldots\times A_{k}\), \(A_{i}\in\mathcal{C}\) for all \(1\leq i\leq k\}\) where \(\mathcal{C}\) is the semialgebra in \(\mathbb{R}\) defined in (3.7). Set \(\mu_{F}(A)\equiv\Delta F(x_{1},x_{2})\) if \(A=(x_{11},x_{21}]\times(x_{12},x_{22}]\times\ldots\times(x_{1k},x_{2k}]\) is bounded and set \(\mu_{F}(A)=\lim_{n\to\infty}\mu(A\cap J_{n})\), where \(J_{n}=(-n,n]^{k}\). * Show that \(F\) is coordinatewise monotone, i.e., if \(x=(x_{1},\ldots,x_{k}),\ y=(y_{1},\ldots,y_{k})\) and \(x_{i}\leq y_{i}\) for every \(i=1,\ldots,k\), then \[F(y)\geq F(x).\]2. Show that \(\mathcal{C}\) is a semialgebra and \(\mu_{F}\) is a measure on \(\mathcal{C}\) by using the Heine-Borel theorem as in Problem 1.22 and 1.23.
2.14 Let \(m(\cdot)\) denote the Lebesgue measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Let \(T:\mathbb{R}\rightarrow\mathbb{R}\) be the map \(T(x)=x^{2}\). Evaluate the induced measure \(mT^{-1}(A)\) of the set \(A\), where 1. \(A=[0,t],t>0\). 2. \(A=(-\infty,0)\). 3. \(A=\{1,2,3,\ldots\}\). 4. \(A=\bigcup_{i=1}^{\infty}(i^{2},(i+\frac{1}{i^{2}})^{2})\). 5. \(A=\bigcup_{i=1}^{\infty}(i^{2},(i+\frac{1}{i})^{2})\).
2.15 Consider the probability space \(\big{(}(0,1),\mathcal{B}((0,1)),m\big{)}\), where \(m(\cdot)\) is the Lebesgue measure. 1. Let \(Y_{1}\) be the random variable \(Y_{1}(x)\equiv\sin 2\pi x\) for \(x\in(0,1)\). Find the cdf of \(Y_{1}\). 2. Let \(Y_{2}\) be the random variable \(Y_{2}(x)\equiv\log x\) for \(x\in(0,1)\). Find the cdf of \(Y_{2}\). 3. Let \(F:\mathbb{R}\rightarrow\mathbb{R}\) be a cdf. For \(0<x<1\), let \[F_{1}^{-1}(x) = \inf\{y:y\in\mathbb{R},F(y)\geq x\}\] \[F_{2}^{-1}(x) = \sup\{y:y\in\mathbb{R},F(y)\leq x\}.\] Let \(Z_{i}\) be the random variable defined by \[Z_{i}=F_{i}^{-1}(x)\ \ 0<x<1,\ \ \ i=1,2.\] 1. Find the cdf of \(Z_{i},i=1,2\). (**Hint:** Verify using the right continuity of \(F\) that for any \(0<x<1\), \(t\in\mathbb{R}\), \(F(t)\geq x\Leftrightarrow F_{1}^{-1}(x)\leq t\).) 2. Show also that \(F_{1}^{-1}(\cdot)\) is left continuous and \(F_{2}^{-1}(\cdot)\) is right continuous.
2.16 1. Let \((\Omega,\mathcal{F}_{1},\mu)\) be a \(\sigma\)-finite measure space. Let \(T:\Omega\rightarrow\mathbb{R}\) be \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable. Show, by an example, that the induced measure \(\mu T^{-1}\) need not be \(\sigma\)-finite. 2. Let \((\Omega_{i},\mathcal{F}_{i})\) be measurable spaces for \(i=1,2\) and let \(T:\Omega_{1}\rightarrow\Omega_{2}\) be \(\langle\mathcal{F}_{1},\mathcal{F}_{2}\rangle\)-measurable. Show that any measure \(\mu\) on \((\Omega_{1},\mathcal{F}_{1})\) is \(\sigma\)-finite if \(\mu T^{-1}\) is \(\sigma\)-finite on \((\Omega_{2},\mathcal{F}_{2})\).

2.17 Let \((\Omega,\mathcal{F},\mu)\) be a measure space and let \(f:\Omega\to[0,\infty]\) be such that it admits two representations \[f=\sum_{i=1}^{k}c_{i}I_{A_{i}}\ \ \text{and}\ \ f=\sum_{j=1}^{\ell}d_{j}I_{B_{j}},\] where \(c_{i},d_{j}\in[0,\infty]\),and \(A_{i}\) and \(B_{j}\in\mathcal{F}\) for all \(i,j\). Show that \[\sum_{i=1}^{k}c_{i}\mu(A_{i})=\sum_{j=1}^{\ell}d_{j}\mu(B_{j}).\] (**Hint:** Express \(A_{i}\) and \(B_{j}\) as finite unions of a common collection of disjoint sets in \(\mathcal{F}\).) 2.18 1. Prove Proposition 2.3.1. 2. In the proof of Proposition 2.3.2, verify that \(D_{n}\uparrow D\). 3. Verify Remark 2.3.1. (**Hint:** Let \[A_{n} = \{\omega:f_{n-1}(\omega)\geq f_{n}(\omega),\ g_{n+1}(\omega)\geq g _{n}(\omega)\}\] \[A = \Big{(}\bigcap_{n\geq 1}A_{n}\Big{)}\bigcap\Big{\{}\omega: \lim_{n\to\infty}g_{n}(\omega)=g(\omega),\] \[\lim_{n\to\infty}f_{n}(\omega)=f(\omega)\Big{\}}\] and \(\tilde{f}_{n}=f_{n}I_{A}\), \(\tilde{g}_{n}=g_{n}I_{A}\). Verify that \(\mu(A^{c})=0\) and apply Proposition 2.3.2 to \(\{\tilde{f}_{n}\}_{n\geq 1}\) and \(\{\tilde{g}_{n}\}_{n\geq 1}\).) 2.19 1. Verify that \(f_{n}(\cdot)\) defined in (3.7) satisfies \(f_{n}(\omega)\uparrow f(\omega)\) for all \(\omega\) in \(\Omega\). 2. Verify that the sequence \(\{h_{n}\}_{n\geq 1}\) of Remark 2.3.3 satisfies \(\lim_{n\to\infty}h_{n}=f\) a.e. \((\mu)\), and \(\lim_{n\to\infty}\int h_{n}d\mu=M\).
2.20 Apply Corollary 2.3.5 to show that for any collection \(\{a_{ij}:i,j\in\mathbb{N}\}\) of nonnegative numbers, \[\sum_{i=1}^{\infty}\left(\sum_{j=1}^{\infty}a_{ij}\right)=\sum_{j=1}^{\infty} \left(\sum_{i=1}^{\infty}a_{ij}\right).\]
2.21 Let \(g:\mathbb{R}\to\mathbb{R}\). 1. Recall that \(\lim_{t\to\infty}g(t)=L\) for some \(L\) in \(\mathbb{R}\) if for every \(\epsilon>0\), there exists a \(T_{\epsilon}<\infty\) such that \(t\geq T_{\epsilon}\Rightarrow\ |g(t)-L|<\epsilon\). Show that \(\lim_{t\to\infty}g(t)=L\) for some \(L\) in \(\mathbb{R}\) iff \(\lim_{n\to\infty}g(t_{n})=L\) for every sequence \(\{t_{n}\}_{n\geq 1}\) with \(\lim_{n\to\infty}t_{n}=\infty\).

2. Formulate and prove a similar result when \(\lim_{t\to a}g(t)=L\) for some \(a,L\in\mathbb{R}\).
2.22 Let \(\{f_{t}:t\in\mathbb{R}\}\subset L^{1}(\Omega,\mathcal{F},\mu)\). 1. (_The continuous version of the MCT_). Suppose that \(f_{t}\uparrow f\) as \(t\uparrow\infty\) a.e. \((\mu)\) and for each \(t\), \(f_{t}\geq 0\) a.e. \((\mu)\). Show that \[\int f_{t}d\mu\uparrow\int fd\mu.\] 2. (_The continuous version of the DCT_). Suppose there exists a nonnegative \(g\in L^{1}(\Omega,\mathcal{F},\mu)\) such that for each \(t\), \(|f_{t}|\leq g\) a.e. \((\mu)\) and as \(t\to\infty\), \(f_{t}\to f\) a.e. \((\mu)\). Then \(f\in L^{1}(\Omega,\mathcal{F},\mu)\) and \(\int|f_{t}-f|d\mu\to 0\) and hence, \(\int f_{t}d\mu\to\int fd\mu\), as \(t\to\infty\).
2.23 Let \(f:[a,b]\to\mathbb{R}\) be bounded where \(-\infty<a<b<\infty\). Let \(\{P_{n}\}_{n\geq 1}\) be a sequence of partitions such that \(\Delta(P_{n})\to 0\). Show that as \(n\to\infty\), \[U(P_{n},f)\to\overline{\int}f\quad\text{and}\quad L(P_{n},f)\to\underline{\int }f,\] where \(\overline{\int}f\) and \(\underline{\int}f\) are as defined in (4.3) and (4.4), respectively. (**Hint:** Given \(\epsilon>0\), fix a partition \(P=\{x_{0}=a<x_{1}<\ldots<x_{k}=b\}\) such that \(\overline{\int}f<U(P,f)<\overline{f}+\epsilon\). Let \(\delta=\min_{0\leq i\leq k-1}(x_{i+1}-x_{i})\). Choose \(n\) large such that the diameter \(\Delta(P_{n})<\delta\). Verify that \[U(P_{n},f)<U(P,f)+kB\Delta(P_{n})\] where \(B=\sup\{|f(x)|:a\leq x\leq b\}\) and conclude that \(\overline{\lim}_{n}U(P_{n},f)\leq\overline{\int}f+\epsilon\).)
2.24 Establish (4.6) and (4.7). (**Hint:** Show that for every \(x\) and any \(\epsilon>0\), \(\phi_{n}(x)\leq\phi(x)+\epsilon\) for all \(n\) large and that for \(x\not\in\bigcup_{n\geq 1}P_{n}\), \(\phi_{n}(x)\geq\phi(x)\) for all \(n\).)
2.25 If \(f(x)=I_{\mathbb{Q}_{1}}(x)\) where \(\mathbb{Q}_{1}=\mathbb{Q}\cap[0,1]\), \(\mathbb{Q}\) being the set of all rationals, then show that for any partition \(P\), \(U(P,f)=1\) and \(L(P,f)=0\).
2.26 Establish Theorem 2.5.1. (**Hint:** Verify that \[D \equiv \{\omega:f_{j}(\omega)\not\to f(\omega)\}\] \[= \bigcup_{r\geq 1}\bigcap_{n\geq 1}\bigcup_{j\geq n}A_{jr},\] where \(A_{jr}=\{|f_{j}-f|>\frac{1}{r}\}\). Show that since \(\mu(D)=0\) and \(\mu(\Omega)<\infty\), \(\mu(D_{rn})\to 0\) as \(n\to\infty\) for each \(r\in\mathbb{N}\), where \(D_{rn}=\bigcup_{j\geq n}A_{jr}\).)2.27 Let \(\phi:\mathbb{R}_{+}\to\mathbb{R}_{+}\) be nondecreasing and \(\frac{\phi(x)}{x}\uparrow\infty\) as \(x\uparrow\infty\). Also, let \(\{f_{\lambda}:\lambda\in\Lambda\}\) be a subset of \(L^{1}(\Omega,\mathcal{F},\mu)\). Show that if \(\sup_{\lambda\in\Lambda}\int\phi(|f_{\lambda}|)d\mu<\infty\), then \(\{f_{\lambda}:\lambda\in\Lambda\}\) is UI.
2.28 Let \(\mu\) be the Lebesgue measure on \(([-1,1],\mathcal{B}([-1,1]))\). For \(n\geq 1\), define \(f_{n}(x)=nI_{(0,n^{-1})}(x)-nI_{(-n^{-1},0)}(x)\) and \(f(x)\equiv 0\) for \(x\in[-1,1]\). Show that \(f_{n}\to f\) a.e. \((\mu)\) and \(\int f_{n}d\mu\to\int fd\mu\) but \(\{f_{n}\}_{n\geq 1}\) is not UI.
2.29 Let \(\{f_{n}:n\geq 1\}\cup\{f\}\subset L^{1}(\Omega,\mathcal{F},\mu)\). 1. Show that \(\int|f_{n}-f|(d\mu)\to 0\) iff \(f_{n}\longrightarrow^{m}f\) and \(\int|f_{n}|d\mu\to\int|f|d\mu\). 2. Show further that if \(\mu(\Omega)<\infty\) then the above two are equivalent to \(f_{n}\longrightarrow^{m}f\) and \(\{f_{n}\}\) UI.
2.30 For \(n\geq 1\), let \(f_{n}(x)=n^{-1/2}I_{(0,n)}(x),\;x\in\mathbb{R}\), and let \(f(x)=0,\;x\in\mathbb{R}\). Let \(m\) denote the Lebesgue measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Show that \(f_{n}\to f\) a.e. \((m)\) and \(\{f_{n}\}_{n\geq 1}\) is UI, but \(\int f_{n}dm\not\to\int fd\mu\).
2.31 (_Computing integrals w.r.t. the Lebesgue measure_). Let \(f\in L^{1}(\mathbb{R},\mathcal{M}_{m},m)\) where \((\mathbb{R},\mathcal{M}_{m},m)\) is the real line with Lebesgue \(\sigma\)-algebra, and Lebesgue measure, i.e., \(m=\mu_{F}^{*}\) where \(F(x)\equiv x\). The definition of \(\int fdm\) as \(\int f^{+}dm-\int f^{-}dm\) involves computing \(\int f^{+}dm\) and \(\int f^{-}dm\) which in turn is given in terms of approximating by integrals of simple nonnegative functions. This is not a very practical procedure. For \(f\) that happens to be continuous a.e. and bounded on finite intervals, one can compute the Riemann integral of \(f\) over finite intervals and pass to the limit. Justify the following steps: 1. Let \(f\) be continuous a.e. and bounded on finite intervals and \(f\in L^{1}(\mathbb{R},\mathcal{M}_{m},m)\). Show that for \(-\infty<a<b<\infty\), \(f\in L^{1}([a,b],\mathcal{M}_{m},m)\) and \[\int_{[a,b]}fdm=\oint_{[a,b]}f(x)dx,\] where the right side denotes the Riemann integral of \(f\) on \([a,b]\). 2. If, in addition, \(f\in L^{1}(\mathbb{R},\mathcal{M}_{m},m)\), then \[\int_{\mathbb{R}}fdm=\lim_{a\to-\infty\atop b\to+\infty}\int_{[a,b]}fdm.\] 3. If \(f\) is continuous a.e. and \(\in L^{1}(\mathbb{R},\mathcal{M}_{m},m)\), then \[\int_{\mathbb{R}}fdm=\lim_{a\to-\infty\atop b\to+\infty}\;\int_{[a,b]}\phi_{c }(f)dm\] where \(\phi_{c}(f)=f(x)I(|f(x)|\leq c)+cI(f(x)>c)-cI(f(x)<-c)\).

* Apply the above procedure to compute \(\int_{\mathbb{R}}fdm\) for
* Let \(a\in\mathbb{R}\). Show that if \(a_{1},a_{2}\) are nonnegative such that \(a=a_{1}-a_{2}\) then \(a_{1}\geq a^{+}\), \(a_{2}\geq a^{-}\) and \(a_{1}-a^{+}=a_{2}-a^{-}\).
* Let \(f=f_{1}-f_{2}\) where \(f_{1},f_{2}\) are nonnegative and are in \(L^{1}(\Omega,\mathcal{F},\mu)\). Show that \(f\in L^{1}(\Omega,\mathcal{F},\mu)\) and \(\int fd\mu=\int f_{1}d\mu-\int f_{2}d\mu\).
* Let \((\Omega,\mathcal{F},\mu)\) be a measure space. Let \(f:\Omega\times(a,b)\to\mathbb{R}\) be such that for each \(a<t<b\), \(f(\cdot,t)\in L^{1}(\Omega,\mathcal{F},\mu)\).
* Suppose for each \(a<t<b\),
* \(\lim_{h\to 0}f(\omega,t+h)=f(\omega,t)\) a.e. \((\mu)\).
* \(\sup_{|h|\leq 1}|f(\omega,t+h)|\leq g_{1}(\omega,t)\), where \(g_{1}(\cdot,t)\in L^{1}(\Omega,\mathcal{F},\mu)\). Show that \(\phi(t)\equiv\int_{\Omega}f(\omega,t)d\mu\) is continuous on \((a,b)\).
* Suppose for each \(a<t<b\).
* \(\lim_{h\to 0}\frac{f(\omega,t+h)-f(\omega,t)}{h}=g_{2}(\omega,t)\) exists a.e. \((\mu)\),
* \(\sup_{0\leq|h|\leq 1}\Big{|}\frac{f(\omega,t+h)-f(\omega,t)}{h}\Big{|}\leq G (\omega,t)\) a.e. \((\mu)\),
* \(G(\omega,t)\in L^{1}(\Omega,\mathcal{F},\mu)\). Show that \[\phi(t)\equiv\int_{\Omega}f(\omega,t)d\mu\] is differentiable on \((a,b)\). (**Hint:*
* Use the continuous version of DCT (cf. Problem 2.22).)
* Let \(A\equiv\big{(}(a_{ij})\big{)}\) be an infinite matrix of real numbers. Suppose that for each \(j\), \(\lim_{i\to\infty}a_{ij}=a_{j}\) exists in \(\mathbb{R}\) and \(\sup_{i}|a_{ij}|\leq b_{j}\), where \(\sum_{j=1}^{\infty}b_{j}<\infty\).
* Show by an application of the DCT that \[\lim_{i\to\infty}\sum_{j=1}^{\infty}|a_{ij}-a_{j}|=0.\]
* Show the same directly, i.e., without using the DCT.

2.35 Using the above problem or otherwise, show that for any sequence \(\{x_{n}\}_{n\geq 1}\) with \(\lim_{n\to\infty}x_{n}=x\) in \(\mathbb{R}\), \[\lim_{n\to\infty}\Big{(}1+\frac{x_{n}}{n}\Big{)}^{n}=\sum_{j=0}^{\infty}\frac{x^ {j}}{j!}\equiv\exp(x).\]
2.36 1. Let \(\{f_{n}\}_{n\geq 1}\subset L^{1}(\Omega,\mathcal{F},\mu)\) such that \(f_{n}\to 0\) in \(L^{1}(\mu)\). Show that \(\{f_{n}\}_{n\geq 1}\) is UI. 2. Let \(\{f_{n}\}_{n\geq 1}\subset L^{p}(\Omega,\mathcal{F},\mu)\), \(0<p<\infty\), such that \(\mu(\Omega)<\infty\), \(\{|f_{n}|^{p}\}_{n\geq 1}\) is UI and \(f_{n}\longrightarrow^{m}f\). Show that \(f\in L^{p}(\mu)\) and \(f_{n}\to f\) in \(L^{p}(\mu)\).
2.37 1. Show that for a sequence of real numbers \(\{x_{n}\}_{n\geq 1}\), \(\lim_{n\to\infty}x_{n}=x\in\bar{\mathbb{R}}\) holds iff every subsequence \(\{x_{n_{j}}\}_{j\geq 1}\) of \(\{x_{n}\}_{n\geq 1}\) has a further subsequence \(\{x_{n_{jk}}\}_{k\geq 1}\) such that \(\lim_{k\to\infty}x_{n_{jk}}=x\). 2. Use (a) and Theorem 2.5.2 to show that the extended DCT (Theorem 2.3.11) is valid if the a.e. convergence of \(\{f_{n}\}_{n\geq 1}\) and \(\{g_{n}\}_{n\geq 1}\) is replaced by convergence in measure.
2.38 Let \((\mathbb{R},\mathcal{M}_{\mu^{*}_{F}},\mu^{*}_{F})\) be a Lebesgue-Stieltjes measure space generated by a \(F:\mathbb{R}\to\mathbb{R}\) nondecreasing. Let \(f:\mathbb{R}\to\bar{\mathbb{R}}\) be \(\mathcal{M}_{\mu^{*}_{F}}\)-measurable such that \(|f|<\infty\) a.e. \((\mu^{*}_{F})\). Show that for every \(k\in\mathbb{N}\) and for every \(\epsilon,\eta\in(0,\infty)\), there exists a continuous function \(g:\mathbb{R}\to\mathbb{R}\) such that \(g(x)=0\) for \(|x|>k\) and \(\mu^{*}_{F}(\{x:|x|\leq k,|f(x)-g(x)|>\eta\}<\epsilon\). (**Hint:** Complete the following: Step I: For all \(\epsilon>0\), there exists \(M_{k,\epsilon}\in(0,\infty)\) such that \(\mu^{*}_{F}(\{x:|x|\leq k,|f(x)|>M_{k,\epsilon}\})<\epsilon\). Step II: For \(\eta>0\), there exists a simple function \(s(\cdot)\) such that \(\mu^{*}_{F}(\{x:|x|\leq k,|f(x)|\leq M_{k,\epsilon},|f(x)-s(x)|>\eta\})=0\). Step III: For \(\delta>0\), there exists a continuous function \(g(\cdot)\) such that \(g\equiv 0\) for \(|x|>k\) and \(\mu^{*}_{F}(\{x:|x|\leq k,s(x)\neq g(x)\})<\delta\).)
2.39 Recall from Corollary 2.3.6 that for \(g\in L^{1}(\Omega,\mathcal{F},\mu)\) and nonnegative \(\nu_{g}(A)\equiv\int_{A}gd\mu\) is a measure. Next for any \(\mathcal{F}\)-measurable function \(h\), show that \(h\in L^{1}(\nu_{g})\) iff \(h\cdot g\in L^{1}(\mu)\) and \(\int hd\nu_{g}=\int hgd\mu\). (**Hint:** Verify first for \(h\) simple and nonnegative, next for \(h\) nonnegative, and finally for any \(h\).)
2.40 Prove the BCT, Corollary 2.3.13, using Egorov's theorem (Theorem 2.5.11).
2.41 Deduce the DCT from the BCT with the notation as in Corollary 2.3.12. (**Hint:** Apply the BCT to the measure space \((\Omega,\mathcal{F},\nu_{g})\) and functions \(h_{n}=\frac{f_{n}}{g}I(g>0)\), \(h=\frac{f}{g}I(g>0)\) where \(\nu_{g}\) is as in Problem 2.39.)2.42 (_Change of variables formula_). Let \((\Omega_{i},\mathcal{F}_{i})\), \(i=1,2\) be two measurable spaces. Let \(f:\Omega_{1}\to\Omega_{2}\) be \(\langle\mathcal{F}_{1},\mathcal{F}_{2}\rangle\)-measurable, \(h:\Omega_{2}\to\mathbb{R}\) be \(\langle\mathcal{F}_{2},\mathcal{B}(\mathbb{R})\rangle\)-measurable, and \(\mu_{1}\) be a measure on \((\Omega_{1},\mathcal{F}_{1})\). Show that \(g\equiv h\circ f\), i.e., \(g(\omega)\equiv h(f(\omega))\) for \(\omega\in\Omega_{1}\) is in \(L^{1}(\mu)\) iff \(h(\cdot)\in L^{1}(\Omega_{2},\mathcal{F}_{2},\mu_{2})\) where \(\mu_{2}=\mu_{1}f^{-1}\) iff \(I(\cdot)\in L^{1}\big{(}\mathbb{R},\mathcal{B}(\mathbb{R}),\mu_{3}\equiv\mu_{ 2}h^{-1}\big{)}\) where \(I(\cdot)\) is the identity function in \(\mathbb{R}\), i.e., \(I(x)\equiv x\) for all \(x\in\mathbb{R}\) and also that \[\int_{\Omega_{1}}gd\mu_{1}=\int_{\Omega_{2}}hd\mu_{2}=\int_{\mathbb{R}}xd\mu_{ 3}.\]
2.43 Let \(\phi(x)\equiv\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}\) be the standard \(N(0,1)\) pdf on \(\mathbb{R}\). Let \(\{(\mu_{n},\sigma_{n})\}_{n\geq 1}\) be a sequence in \(\mathbb{R}\times\mathbb{R}_{+}\). Suppose \(\mu_{n}\to\mu\) and \(\sigma_{n}\to\sigma\) as \(n\to\infty\). Let \(f_{n}(x)=\frac{1}{\sigma_{n}}\phi\big{(}\frac{x-\mu_{n}}{\sigma_{n}}\big{)}\), \(f(x)=\frac{1}{\sigma}\phi\big{(}\frac{x-\mu}{\sigma}\big{)}\) and \(\nu_{n}(A)=\int_{A}f_{n}dm\), \(\nu(A)=\int_{A}fdm\) for any Borel set \(A\) in \(\mathbb{R}\), where \(m(\cdot)\) is the Lebesgue measure on \(\mathbb{R}\). Using Scheffe's theorem, verify that, as \(n\to\infty\), \(\nu_{n}(\cdot)\to\nu(\cdot)\) uniformly on \(\mathcal{B}(\mathbb{R})\) and that for any \(h:\mathbb{R}\to\mathbb{R}\), bounded and Borel measurable, \[\int hd\nu_{n}\to\int hd\nu.\]
2.44 Let \(f_{n}(x)=c_{n}\big{(}1-\frac{x}{n}\big{)}^{n}I_{[0,n]}(x)\), \(x\in\mathbb{R}\), \(n\geq 1\). 1. Find \(c_{n}\) such that \(\int f_{n}dm=1\). 2. Show that \(\lim_{n\to\infty}f_{n}(x)\equiv f(x)\) exists for all \(x\) in \(\mathbb{R}\) and that \(f\) is a pdf 3. For \(A\in\mathcal{B}(\mathbb{R})\), let \[\nu_{n}(A)\equiv\int_{A}f_{n}dm\quad\text{and}\quad\nu(A)=\int_{A}fdm.\] Show that \(\nu_{n}\to\nu\) uniformly on \(\mathcal{B}(\mathbb{R})\).
2.45 Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(f:\Omega\to\mathbb{R}\) be \(\mathcal{F}\)-measurable. Suppose that \(\int_{\Omega}|f|d\mu<\infty\) and \(D\) is a closed set in \(\mathbb{R}\) such that for all \(B\in\mathcal{F}\) with \(\mu(B)>0\), \[\frac{1}{\mu(B)}\int_{B}fd\mu\in D.\] Show that \(f(\omega)\in D\) a.e. \(\mu\). (**Hint:** Show that for \(x\not\in D\), there exists \(r>0\) such that \(\mu\{\omega:|f(\omega)-x|<r\}=0\).)2.46 Find a sequence of nonnegative continuous functions \(\{f_{n}\}_{n\geq 1}\) on [0,1] such that \(\int_{[0,1]}f_{n}dm\to 0\) but \(\{f_{n}(x)\}_{n\geq 1}\) does not converge for any \(x\). (**Hint:** Let for \(m\geq 1\), \(1\leq k\leq m\), \(g_{m,k}=I_{\left[\frac{k-1}{m},\frac{k}{m}\right]}\) and \(\{f_{n}\}_{n\geq 1}\) be a reordering of \(\{g_{n,k}:1\leq k\leq n,n\geq 1\}\).)
2.47 Let \(\{f_{n}\}_{n\geq 1}\) be a sequence of continuous functions from [0,1] to [0,1] such that \(f_{n}(x)\to 0\) as \(n\to\infty\) for all \(0\leq x\leq 1\). Show that \(\int_{0}^{1}f_{n}(x)dx\to 0\) as \(n\to\infty\) (where the integral is the Riemann integral) by two methods: one by using BCT and one without using BCT. Show also that if \(\mu\) is a finite measure on \(([0,1],\mathcal{B}([0,1]))\), then \(\int_{[0,1]}f_{n}d\mu\to 0\) as \(n\to\infty\).
2.48 (_Invariance of Lebesgue measure under translation and reflection._) Let \(m(\cdot)\) be Lebesgue measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). 1. For any \(E\in\mathcal{B}(\mathbb{R})\) and \(c\in\mathbb{R}\), define \(-E\equiv\{x:-x\in E\}\) and \(E+c\equiv\{y:y=x+c,x\in E\}\). Show that \[\begin{array}{rcl}m(-E)&=&m(E)\quad\mbox{and}\\ m(E+c)&=&m(E)\end{array}\] for all \(E\in\mathcal{B}(\mathbb{R})\) and \(c\in\mathbb{R}\). 2. For any \(f\in L^{1}(\mathbb{R},\mathcal{B}(\mathbb{R}),m)\) and \(c\in\mathbb{R}\), let \(\tilde{f}(x)\equiv f(-x)\) and \(f_{c}(x)\equiv f(x+c)\). Show that \[\int\tilde{f}dm=\int f_{c}dm=\int fdm.\]\(L^{p}\)-Spaces

### 3.1 Inequalities

This section contains a number of useful inequalities.

**Theorem 3.1.1:** (_Markov's inequality_). _Let \(f\) be a nonnegative measurable function on a measure space \((\Omega,{\cal F},\mu).\) Then for any \(0<t<\infty\),_

\[\mu(\{f\geq t\})\leq\frac{\int fd\mu}{t}. \tag{1.1}\]

**Proof:** Since \(f\) is nonnegative, \(\int fd\mu\geq\int_{(\{f\geq t\})}fd\mu\geq t\mu(\{f\geq t\})\). \(\Box\)

**Corollary 3.1.2:**_Let \(X\) be a random variable on a probability space \((\Omega,{\cal F},P).\) Then, for \(r>0\), \(t>0\),_

\[P(|X|\geq t)\leq\frac{E|X|^{r}}{t^{r}}.\]

**Proof:** Since \(\{|X|\geq t\}=\{|X|^{r}\geq t^{r}\}\) for all \(t>0\), \(r>0\), this follows from (1.1). \(\Box\)

**Corollary 3.1.3:** (_Chebychev's inequality_). _Let \(X\) be a random variable with \(EX^{2}<\infty\), \(E(X)=\mu\) and Var\((X)=\sigma^{2}.\) Then for any \(0<k<\infty\),_

\[P(|X-\mu|\geq k\sigma)\leq\frac{1}{k^{2}}.\]

**Proof:** Follows from Corollary 3.1.2 with \(X\) replaced by \(X-\mu\) and with \(r=2\), \(t=k\sigma\). \(\Box\)

**Corollary 3.1.4:**_Let \(\phi:\mathbb{R}_{+}\rightarrow\mathbb{R}_{+}\) be nondecreasing. Then for any random variable \(X\) and \(0<t<\infty\),_

\[P(|X|\geq t)\leq\frac{E\phi(|X|)}{\phi(t)}.\]

**Proof:** Use (1.1) and the fact that \(|X|\geq t\Rightarrow\phi(|X|)\geq\phi(t)\). \(\Box\)

**Corollary 3.1.5:** (_Cramer's inequality_). _For any random variable \(X\) and \(t>0\),_

\[P(X\geq t)\leq\inf_{\theta>0}\frac{E(e^{\theta X})}{e^{\theta t}}\.\]

**Proof:** For \(t>0\), \(\theta>0\), \(P(X\geq t)=P\big{(}e^{\theta X}\geq e^{\theta t}\big{)}\leq\frac{E(e^{\theta X })}{e^{\theta t}}\), by (1.1). \(\Box\)

**Definition 3.1.1:** A function \(\phi:(a,b)\rightarrow\mathbb{R}\) is called _convex_ if for all \(0\leq\lambda\leq 1\), \(a<x\leq y<b\),

\[\phi(\lambda x+(1-\lambda)y)\leq\lambda\phi(x)+(1-\lambda)\phi(y). \tag{1.2}\]

Geometrically, this means that for the graph of \(y=\phi(x)\) on \((a,b)\), for each fixed \(t\in(0,\infty)\), the chord over the interval \((x,x+t)\) turns in the counterclockwise direction as \(x\) increases.

More precisely, the following result holds.

**Proposition 3.1.6:**_Let \(\phi:(a,b)\rightarrow\mathbb{R}\). Then \(\phi\) is convex iff for all \(a<x_{1}<x_{2}<x_{3}<b\),_

\[\frac{\phi(x_{2})-\phi(x_{1})}{x_{2}-x_{1}}\leq\frac{\phi(x_{3})-\phi(x_{2})}{ x_{3}-x_{2}}, \tag{1.3}\]

_which is equivalent to_

\[\frac{\phi(x_{2})-\phi(x_{1})}{x_{2}-x_{1}}\leq\frac{\phi(x_{3})-\phi(x_{1})}{ (x_{3}-x_{1})}\leq\frac{\phi(x_{3})-\phi(x_{2})}{x_{3}-x_{2}}. \tag{1.4}\]

**Proof:** Let \(\phi\) be convex and \(a<x_{1}<x_{2}<x_{3}<b\). Then one can write \(x_{2}=\lambda x_{1}+(1-\lambda)x_{3}\) with \(\lambda=\frac{(x_{3}-x_{2})}{(x_{3}-x_{1})}\). So by (1.2),

\[\phi(x_{2}) = \phi(\lambda x_{1}+(1-\lambda)x_{3})\] \[\leq \lambda\phi(x_{1})+(1-\lambda)\phi(x_{3})\] \[= \frac{(x_{3}-x_{2})}{(x_{3}-x_{1})}\phi(x_{1})+\frac{(x_{2}-x_{1 })}{(x_{3}-x_{1})}\phi(x_{3})\]which is equivalent to (1.3). Also, since

\[\frac{\phi(x_{3})-\phi(x_{1})}{(x_{3}-x_{1})}=\lambda\frac{\phi(x_{3})-\phi(x_{2}) }{(x_{3}-x_{2})}+(1-\lambda)\frac{\phi(x_{2})-\phi(x_{1})}{(x_{2}-x_{1})},\]

(1.4) follows from (1.3).

Conversely, suppose (1.4) holds for all \(a<x_{1}<x_{2}<x_{3}<b\). Then given \(a<x<y<b\) and \(0<\lambda<1\), set \(x_{1}=x\), \(x_{2}=\lambda x+(1-\lambda)y\), \(x_{3}=y\) and apply (1.4) to verify (1.2). \(\Box\)

The following properties of a convex function are direct consequences of (1.3). The proof is left as an exercise (Problem 3.1).

**Proposition 3.1.7:**_Let \(\phi:(a,b)\to\mathbb{R}\) be convex. Then,_

* _For each_ \(x\in(a,b)\)_,_ \[\phi^{\prime}_{+}(x)\equiv\lim_{y\downarrow x}\frac{\phi(y)-\phi(x)}{(y-x)}, \quad\phi^{\prime}_{-}(x)\equiv\lim_{y\uparrow x}\frac{\phi(y)-\phi(x)}{(y-x)}\] _exist and are finite._
* _Further,_ \(\phi^{\prime}_{-}(\cdot)\leq\phi^{\prime}_{+}(\cdot)\) _and both are nondecreasing on_ \((a,b)\)_._
* \(\phi^{\prime}(\cdot)\) _exists except on the countable set of discontinuity points of_ \(\phi^{\prime}_{+}\) _and_ \(\phi^{\prime}_{-}\)_._
* _For any_ \(a<c<d<b\)_,_ \(\phi\) _is Lipschitz on_ \([c,d]\)_, i.e., there exists a constant_ \(K<\infty\) _such that_ \[|\phi(x)-\phi(y)|\leq K|x-y|\quad\mbox{for all}\quad c\leq x,\ y\leq d.\]
* _For any_ \(a<c\)_,_ \(x<b\)_,_ \[\phi(x)-\phi(c)\geq\phi^{\prime}_{+}(c)(x-c)\quad\mbox{and}\quad\phi(x)-\phi(c )\geq\phi^{\prime}_{-}(c)(x-c).\] (1.5)

By the mean value theorem, a sufficient condition for (1.3) and hence, for the convexity of \(\phi\) is that \(\phi\) be differentiable on \((a,b)\) and \(\phi^{\prime}\) be nondecreasing. A further sufficient condition for this is that \(\phi\) be twice differentiable on \((a,b)\) and \(\phi^{\prime\prime}\) be nonnegative. This is stated as

**Proposition 3.1.8:**_Let \(\phi\) be twice differentiable on \((a,b)\) and \(\phi^{\prime\prime}\) be nonnegative on \((a,b)\). Then \(\phi\) is convex on \((a,b)\)._

**Example 3.1.1:** The following functions are convex in the given intervals:

* \(\phi(x)=|x|^{p}\), \(p\geq 1,\quad(-\infty,\infty)\).
* \(\phi(x)=e^{x},\quad(-\infty,\infty)\).

* \(\phi(x)=-\log x,\quad(0,\infty)\).
* \(\phi(x)=x\log x,\quad(0,\infty)\).

**Remark 3.1.1:** By Proposition 3.1.7 (iii), the convexity of \(\phi\) implies that \(\phi^{\prime}\) exists except on a countable set. For example, the function \(\phi(x)=|x|\) is convex on \(\mathbb{R}\); it is differentiable at all \(x\neq 0\). Similarly, it is easy to construct a piecewise linear convex function \(\phi\) with a countable number of points where \(\phi\) is not differentiable.

The following is an important inequality for convex functions.

**Theorem 3.1.9:** (_Jensen's inequality_). _Let \(f\) be a measurable function on a probability space \((\Omega,\mathcal{F},P)\) with \(P(f\in(a,b))=1\) for some interval \((a,b)\), \(-\infty\leq a<b\leq\infty\) and let \(\phi:(a,b)\to\mathbb{R}\) be convex. Then_

\[\phi\Big{(}\int fdP\Big{)}\leq\int\phi(f)dP, \tag{1.6}\]

_provided \(\int|f|dP<\infty\) and \(\int|\phi(f)|dP<\infty\)._

**Remark 3.1.2:** In terms of random variables, this says that for any random variable \(X\) on a probability space \((\Omega,\mathcal{F},P)\) with \(P(X\in(a,b))=1\) and for any function \(\phi\) that is convex on \((a,b)\),

\[\phi(EX)\leq E\phi(X), \tag{1.7}\]

provided \(E|X|<\infty\) and \(E|\phi(X)|<\infty\), where for any Borel measurable function \(h\), \(Eh(X)\equiv\int h(X)dP\).

**Proof of Theorem 3.1.9:** Let \(c=\int fdP\). Applying (1.5), one gets

\[Y(\omega)\equiv\phi(f(\omega))-\phi(c)-\phi^{{}^{\prime}}_{+}(c)(f(\omega)-c) \geq 0\quad\mbox{a.e. }(P), \tag{1.8}\]

which, when integrated, yields \(\int Y(\omega)P(d\omega)\geq 0\). Since \(\int(f(\omega)-c)\)\(P(d\omega)=0\), (1.6) follows. \(\Box\)

**Remark 3.1.3:** Suppose that equality holds in (1.6). Then, it follows that \(\int Y(\omega)P(d\omega)=0\). By (1.8), this implies

\[\phi(f(\omega))-\phi(c)=\phi^{\prime}_{+}(c)(f(\omega)-c)\quad\mbox{a.e. }(P).\]

Thus, if \(\phi\) is a strictly convex function (i.e., strict inequality holds in (1.2) for all \(x\), \(y\in(a,b)\) and \(0<\lambda<1\)), then equality holds in (1.6) iff \(f(\omega)=c\) a.e. \((P)\).

The following are easy consequences of Jensen's inequality (Problem 3.3).

**Proposition 3.1.10:** _Let \(k\geq 1\) be an integer._1. _Let_ \(a_{1},a_{2},,\ldots,a_{k}\) _be real and_ \(p_{1},p_{2},,\ldots,p_{k}\) _be positive numbers such that_ \(\sum_{i=1}^{k}p_{i}=1.\) _Then_ \[\sum_{i=1}^{k}p_{i}\exp(a_{i})\geq\exp\bigg{(}\sum_{i=1}^{k}p_{i}a_{i}\bigg{)}.\] (1.9)
2. _Let_ \(b_{1},b_{2},\ldots,b_{k}\) _be nonnegative numbers and_ \(p_{1},p_{2},\ldots,p_{k}\) _be as in (i). Then_ \[\sum_{i=1}^{k}p_{i}b_{i}\geq\prod_{i=1}^{k}b_{i}^{p_{i}},\] (1.10) _and in particular,_ \[\frac{1}{k}\sum_{i=1}^{k}b_{i}\geq\bigg{(}\prod_{i=1}^{k}b_{i}\bigg{)}^{\frac {1}{k}},\] (1.11) _i.e., the arithmetic mean of_ \(b_{1},\ldots,b_{k}\) _is greater than or equal to the geometric mean of the_ \(b_{i}\)_'s. Further, equality holds in (_1.10_) iff_ \(b_{1}=b_{2}=\ldots=b_{k}\)_._
3. _For any_ \(a,b\) _real and_ \(1\leq p<\infty\)_,_ \[|a+b|^{p}\leq 2^{p-1}(|a|^{p}+|b|^{p}).\] (1.12)

Inequality (1.10) is useful in establishing the following:

**Theorem 3.1.11:** (_Holder's inequality_). _Let_ \((\Omega,\mathcal{F},\mu)\) _be a measure space. Let_ \(1<p<\infty,\ f\in L^{p}(\Omega,\mathcal{F},\mu)\) _and_ \(g\in L^{q}(\Omega,\mathcal{F},\mu)\) _where_ \(q=\frac{p}{(p-1)}\)_. Then_

\[\int|fg|d\mu \leq \bigg{(}\int|f|^{p}d\mu\bigg{)}^{\frac{1}{p}}\bigg{(}\int|g|^{q} d\mu\bigg{)}^{\frac{1}{q}}, \tag{1.13}\] \[i.e.,\quad\|fg\|_{1} \leq \|f\|_{p}\|g\|_{q}.\]

_If_ \(\|fg\|_{1}\neq 0\)_, then equality holds in (_1.13_) iff_ \(|f|^{p}=c|g|^{q}\) _a.e._ \((\mu)\) _for some constant_ \(c\in(0,\infty)\)_._

**Proof:** W.l.o.g. assume that \(\int|f|^{p}d\mu>0\) and \(\int|g|^{q}d\mu>0.\) Fix \(\omega\in\Omega.\) Let \(p_{1}=\frac{1}{p},\)\(p_{2}=\frac{1}{q},\)\(b_{1}=c_{1}|f(\omega)|^{p},\) and \(b_{2}=c_{2}|g(\omega)|^{q},\) where \(c_{1}=(\int|f|^{p}d\mu)^{-1}\) and \(c_{2}=(\int|g|^{q}d\mu)^{-1}.\) Then applying (1.10) with \(k=2\) yields

\[\frac{c_{1}}{p}|f(\omega)|^{p}+\frac{c_{2}}{q}|g(\omega)|^{q}\geq c_{1}^{\frac {1}{p}}c_{2}^{\frac{1}{q}}|f(\omega)g(\omega)|. \tag{1.14}\]

Integrating w.r.t. \(\mu\) yields

\[1\geq c_{1}^{\frac{1}{p}}c_{2}^{\frac{1}{q}}\int|f(\omega)g(\omega)|d\mu(\omega)\]which is equivalent to (1.13).

Next, equality in (1.13) implies equality in (1.14) a.e. \((\mu).\) Since \(1<p<\infty,\) by the last part of Proposition 3.1.10 (ii), this implies that \(b_{1}=b_{2}\) a.e. \((\mu),\) i.e., \(|f(\omega)|^{p}=c_{2}c_{1}^{-1}|g(\omega)|^{q}\) a.e. \((\mu).\)\(\Box\)

**Remark 3.1.4:** (_Holder's inequality for \(p=1,q=\infty\)_). Let \(f\in L^{1}(\Omega,\mathcal{F},\mu)\) and \(g\in L^{\infty}(\Omega,\mathcal{F},\mu).\) Then \(|fg|\leq|f|\,\|g\|_{\infty}\) a.e. \((\mu)\) and hence,

\[\|fg\|_{1}\equiv\int|fg|d\mu\leq\|f\|_{1}\|g\|_{\infty}.\]

If equality holds in the above inequality, then \(|f|(\|g\|_{\infty}-|g|)=0\) a.e. \((\mu)\) and hence, \(|g|=\|g\|_{\infty}\) on the set \(\{|f|>0\}\) a.e. \((\mu).\)

The next two corollaries follow directly from Theorem 3.1.11. The proof is left as an exercise (Problem 3.9).

**Corollary 3.1.12:** (_Cauchy-Schwarz inequality_). _Let \(f\), \(g\in L^{2}(\Omega,\mathcal{F},\mu).\) Then_

\[\int|fg|d\mu \leq \left(\int|f|^{2}d\mu\right)^{\frac{1}{2}}\left(\int|g|^{2}d\mu \right)^{\frac{1}{2}}, \tag{1.15}\] \[i.e.,\quad\|fg\|_{1} \leq \|f\|_{2}\|g\|_{2}.\]

**Corollary 3.1.13:** _Let \(k\in\mathbb{N}.\) Let \(a_{1},a_{2},\ldots,a_{k}\), \(b_{1},b_{2},\ldots,b_{k}\) be real numbers and \(c_{1},c_{2},\ldots,c_{k}\) be positive real numbers._

* _Then, for any_ \(1<p<\infty\)_,_ \[\sum_{i=1}^{k}|a_{i}b_{i}|c_{i}\leq\left(\sum_{i=1}^{k}|a_{i}|^{p}c_{i}\right) ^{\frac{1}{p}}\left(\sum_{i=1}^{k}|b_{i}|^{q}c_{i}\right)^{\frac{1}{q}},\] (1.16) _where_ \(q=\frac{p-1}{p}\)_._
* \[\sum_{i=1}^{k}|a_{i}b_{i}|c_{i}\leq\left(\sum_{i=1}^{k}|a_{i}|^{2}c_{i}\right) ^{\frac{1}{2}}\left(\sum_{i=1}^{k}|b_{i}|^{2}c_{i}\right)^{\frac{1}{2}}.\] (1.17)

Next, as an application of Holder's inequality, one gets

**Theorem 3.1.14:** (_Minkowski's inequality_). _Let \(1<p<\infty\) and \(f,g\in L^{p}(\Omega,\mathcal{F},\mu).\) Then_

\[\left(\,\int|f+g|^{p}d\mu\right)^{\frac{1}{p}} \leq \left(\,\int|f|^{p}d\mu\right)^{\frac{1}{p}}+\left(\,\int|g|^{p}d \mu\right)^{\frac{1}{p}},\] \[i.e.,\quad\|f+g\|_{p} \leq \|f\|_{p}+\|g\|_{q}. \tag{1.18}\]

**Proof:** Let \(h_{1}=|f+g|\), \(h_{2}=|f+g|^{p-1}\). Then by (1.12),

\[|f+g|^{p}\leq 2^{p-1}(|f|^{p}+|g|^{p}),\]

implying that \(h_{1}\in L^{p}(\Omega,{\cal F},\mu)\) and \(h_{2}\in L^{q}(\Omega,{\cal F},\mu)\), where \(q=\frac{p}{(p-1)}\). Since \(|f+g|^{p}=h_{1}h_{2}\leq|f|h_{2}+|g|h_{2}\), by Holder's inequality,

\[\int|f+g|^{p}d\mu\leq\left(\int|f|^{p}d\mu\right)^{\frac{1}{p}}\left(\int h_{2} ^{q}\right)^{\frac{1}{q}}+\left(\int|g|^{p}d\mu\right)^{\frac{1}{p}}\left(\int h _{2}^{q}\right)^{\frac{1}{q}}. \tag{1.19}\]

But \(\int h_{2}^{q}=\int|f+g|^{p}\) and so (1.19) yields (1.18). \(\Box\)

**Remark 3.1.5:** Inequality (1.18) holds for both \(p=1\) and \(p=\infty\).

### \(L^{p}\)-Spaces

#### Basic properties

Let \((\Omega,{\cal F},\mu)\) be a measure space. Recall the definition of \(L^{p}(\Omega,{\cal F},\mu)\), \(0<p\leq\infty\), from Section 2.5 as the set of all measurable functions \(f\) on \((\Omega,{\cal F},\mu)\) such that \(\|f\|_{p}<\infty\), where for \(0<p<\infty\),

\[\|f\|_{p}=\Big{(}\int|f|^{p}d\mu\Big{)}^{\min\{\frac{1}{p},1\}}\]

and for \(p=\infty\),

\[\|f\|_{\infty}\equiv\sup\{k:\mu(\{|f|>k\})>0\}\]

(called the _essential supremum_ of \(f\)). In this section and elsewhere, \(L^{p}(\mu)\) denotes \(L^{p}(\Omega,{\cal F},\mu)\). The following proposition shows that \(L^{p}(\mu)\) is a vector space over \(\mathbb{R}\).

**Proposition 3.2.1:**_For \(0<p\leq\infty\),_

\[f,g\in L^{p}(\mu),\ \ a,b\in\mathbb{R}\quad\Rightarrow\quad af+bg\in L^{p}(\mu). \tag{2.1}\]

**Proof:**

Case 1: \(0<p\leq 1\). For any two positive numbers \(x,y\),

\[\left(\frac{x}{x+y}\right)^{p}+\left(\frac{y}{x+y}\right)^{p}\geq\frac{x}{x+y} +\frac{y}{x+y}=1.\]

Hence, for all \(x,y\in(0,\infty)\)

\[(x+y)^{p}\leq x^{p}+y^{p}. \tag{2.2}\]It is easy to check that (2.2) continues to hold if \(x,y\in[0,\infty).\) This yields \(|af+bg|^{p}\leq|a|^{p}|f|^{p}+|b|^{p}|g|^{p},\) which, in turn, yields (2.1) by integration.

Case 2: \(1<p<\infty.\) By (1.12),

\[|af+bg|^{p}\leq 2^{p-1}(|af|^{p}+|bg|^{p}).\]

Integrating both sides of the above inequality yields (2.1).

Case 3: \(p=\infty.\) By definition, there exist constants \(K_{1}<\infty\) and \(K_{2}<\infty\) such that

\[\mu(\{|f|>K_{1}\})=0=\mu(\{|g|>K_{2}\}).\]

This implies that \(\mu(\{|af+bg|>K\})=0\) for any \(K>|a|K_{1}+|b|K_{2}.\) Hence, \(af+bg\in L^{\infty}(\mu)\) and

\[\|af+bg\|_{\infty}\leq|a|\,\|f\|_{\infty}+|b|\,\|g\|_{\infty}\.\]

\(\Box\)

Recall that a set \(\mathbb{S}\) with a function \(d:\mathbb{S}\times\mathbb{S}\to[0,\infty]\) is called a _metric space_ if for all \(x,\)\(y,\)\(z\in\mathbb{S},\)

* \(d(x,y)=d(y,x)\)  (_symmetry_)
* \(d(x,y)\leq d(x,z)+d(y,z)\)  (_triangle inequality_)
* \(d(x,y)=0\) iff \(x=y\)

and the function \(d(\cdot,\cdot)\) is called a _metric_ on \(\mathbb{S}.\) Some examples are

* \(\mathbb{S}=\mathbb{R}^{k}\) with \(d(x,y)=\Bigl{(}\sum_{i=1}^{k}|x_{i}-y_{i}|^{2}\Bigr{)}^{\frac{1}{2}};\)
* \(\mathbb{S}=C[0,1],\) the space of all continuous functions on \([0,1]\) with \(d(f,g)=\sup\{|f(x)-g(x)|:0\leq x\leq 1\};\)
* \(\mathbb{S}=\) a nonempty set, and \(d(x,y)=1\) if \(x\neq y\) and \(0\) if \(x=y.\) (This \(d(\cdot,\cdot)\) is called the _discrete metric_ on \(\mathbb{S}.\))

The \(L^{p}\)-norm \(\|\cdot\|_{p}\) can be used to introduce a distance notion in \(L^{p}(\mu)\) for \(0<p\leq\infty.\)

**Definition 3.2.1:** For \(f,\)\(g\in L^{p}(\mu),\)\(0<p\leq\infty,\) let

\[d_{p}(f,g)\equiv\|f-g\|_{p}. \tag{2.3}\]

Note that, for any \(f,\)\(g,\)\(h\in L^{p}(\mu)\) and \(1\leq p\leq\infty,\)

* \(d_{p}(f,g)=d_{p}(g,f)\geq 0\)  (_nonnegativity and symmetry_), and
* \(d_{p}(f,h)\leq d_{p}(f,g)+d_{p}(g,h)\)  (_triangle inequality_),which follows by Minkowski's inequality (Theorem 3.1.14) for \(1\leq p<\infty\), and by Proposition 3.2.1 for \(p=\infty\). However, \(d_{p}(f,g)=0\) implies only that \(f=g\) a.e. \((\mu)\). Thus, \(d_{p}(\cdot,\cdot)\) of (2.3) satisfies conditions (i) and (ii) of being a metric and it satisfies condition (iii) as well, provided any two functions \(f\) and \(g\) that agree a.e. \((\mu)\) are regarded as the same element of \(L^{p}(\mu)\). This leads to the following:

**Definition 3.2.2:** For \(f\), \(g\) in \(L^{p}(\mu)\), \(f\) is called _equivalent_ to \(g\) and is written as \(f\sim g\), if \(f=g\) a.e. \((\mu)\).

It is easy to verify that the relation \(\sim\) of Definition 3.2.2 is an _equivalence relation_, i.e., it satisfies

* \(f\sim f\) for all \(f\) in \(L^{p}(\mu)\)  (_reflexive_)
* \(f\sim g\Rightarrow g\sim f\)  (_symmetry_)
* \(f\sim g,g\sim h\Rightarrow f\sim h\)  (_transitive_).

This equivalence relation \(\sim\) divides \(L^{p}(\mu)\) into disjoint equivalence classes such that in each class all elements are equivalent. The notion of distance between these classes may be defined as follows:

\[d_{p}([f],[g])\equiv d_{p}(f,g)\]

where \([f]\) and \([g]\) denote, respectively, the equivalence classes of functions containing \(f\) and \(g\). It can be verified that this is a metric on the set of equivalence classes. In what follows, the equivalence class \([f]\) is identified with the element \(f\). With this identification, \((L^{p}(\mu),d_{p}(\cdot,\cdot))\) becomes a metric space for \(1\leq p\leq\infty\).

**Remark 3.2.1:** For \(0<p<1\), if one defines

\[d_{p}(f,g)\equiv\int|f-g|^{p}d\mu, \tag{2.4}\]

then \((L^{p}(\mu),d_{p})\) becomes a metric space (with the same identification as above of functions with their equivalence classes). The triangle inequality follows from (2.2).

Recall that a metric space \((\mathbb{S},d)\) is called _complete_ if every _Cauchy sequence_ in \((\mathbb{S},d)\) converges to an element in \(\mathbb{S}\), i.e., if \(\{x_{n}\}_{n\geq 1}\) is a sequence in \(\mathbb{S}\) such that for every \(\epsilon>0\), there exists a \(N_{\epsilon}\) such that \(n,m\geq N_{\epsilon}\Rightarrow d(x_{n},x_{m})\leq\epsilon\), then there exists an element \(x\) in \((\mathbb{S},d)\) such that \(\lim_{n\to\infty}d(x_{n},x)=0\). The next step is to establish the _completeness of \(L^{p}(\mu)\)_.

**Theorem 3.2.2:** _For \(0<p\leq\infty\), \((L^{p}(\mu)\), \(d_{p}(\cdot,\cdot))\) is complete_, _where \(d_{p}\) is as in (2.3)._

**Proof:** Let \(\{f_{n}\}_{n\geq 1}\) be a Cauchy sequence in \(L^{p}(\mu)\) for \(0<p<\infty\). The main steps in the proof are as follows:

* there exists a subsequence \(\{n_{k}\}_{k\geq 1}\) such that \(\{f_{n_{k}}\}_{k\geq 1}\) converges a.e. \((\mu)\) to a limit function \(f\);
* \(\lim_{k\to\infty}d_{p}(f_{n_{k}},f)=0\);
* \(\lim_{n\to\infty}d_{p}(f_{n},f)=0\).

Step (I): Let \(\{\epsilon_{k}\}_{k\geq 1}\) and \(\{\delta_{k}\}_{k\geq 1}\) be sequences of positive numbers decreasing to zero. Since \(\{f_{n}\}_{n\geq 1}\) is Cauchy, for each \(k\geq 1\), there exists an integer \(n_{k}\) such that

\[\int|f_{n}-f_{m}|^{p}d\mu\leq\epsilon_{k}\quad\mbox{for all}\quad n,m\geq n_{ k}. \tag{2.5}\]

W.l.o.g., let \(n_{k+1}>n_{k}\) for each \(k\geq 1\). Then, by Markov's inequality (Theorem 3.1.1),

\[\mu(\{|f_{n_{k+1}}-f_{n_{k}}|\geq\delta_{k}\})\leq\delta_{k}^{-p}\int|f_{n_{k+ 1}}-f_{n_{k}}|^{p}d\mu\leq\delta_{k}^{-p}\epsilon_{k}. \tag{2.6}\]

Let \(A_{k}=\{|f_{n_{k+1}}-f_{n_{k}}|\geq\delta_{k}\}\), \(k=1,2,\ldots\) and \(A=\limsup_{k\to\infty}A_{k}\equiv\bigcap_{j=1}^{\infty}\bigcup_{k\geq j}A_{k}\). If \(\{\epsilon_{k}\}_{k\geq 1}\) and \(\{\delta_{k}\}_{k\geq 1}\) satisfy

\[\sum_{k=1}^{\infty}\delta_{k}^{-p}\epsilon_{k}<\infty, \tag{2.7}\]

then by (2.6), \(\sum_{k=1}^{\infty}\mu(A_{k})<\infty\) and hence, as in the proof of Theorem 2.5.2, \(\mu(A)=0\).

Note that for \(\omega\) in \(A^{c}\), \(|f_{n_{k+1}}(\omega)-f_{n_{k}}(\omega)|<\delta_{k}\) for all \(k\) large. Thus, if \(\sum_{k=1}^{\infty}\delta_{k}<\infty\), then for \(\omega\) in \(A^{c}\), \(\{f_{n_{k}}(\omega)\}_{k\geq 1}\) is a Cauchy sequence in \(\mathbb{R}\) and hence, it converges to some \(f(\omega)\) in \(\mathbb{R}\). Setting \(f(\omega)=0\) for \(\omega\in A\), one gets

\[\lim_{k\to\infty}f_{n_{k}}=f\quad\mbox{a.e.}\ (\mu).\]

A choice of \(\{\epsilon_{k}\}_{k\geq 1}\) and \(\{\delta_{k}\}_{k\geq 1}\) such that \(\sum_{k=1}^{\infty}\delta_{k}<\infty\) and (2.7) holds is given by \(\epsilon_{k}=2^{-(p+1)k}\) and \(\delta_{k}=2^{-k}\). This completes Step (I).

Step (II): By Fatou's lemma, part (I), and (2.5), for any \(k\geq 1\) fixed,

\[\epsilon_{k}\geq\liminf_{j\to\infty}\int|f_{n_{k}}-f_{n_{k+j}}|^{p}d\mu\geq \int|f_{n_{k}}-f|^{p}d\mu\.\]

Since \(f_{n_{k}}\in L^{p}(\mu)\), this shows that \(f\in L^{p}(\mu)\). Now, on letting \(k\to\infty\), (II) follows.

Step (III): By triangle inequality, for any \(k\geq 1\) fixed,

\[d_{p}(f_{n},f)\leq d_{p}(f_{n},f_{n_{k}})+d_{p}(f_{n_{k}},f).\]

By (2.5) and (II), for \(n\geq n_{k}\), the right side above is \(\leq 2\tilde{\epsilon}_{k}\), where \(\tilde{\epsilon}_{k}=\epsilon_{k}\) if \(0<p<1\) and \(\tilde{\epsilon}_{k}=\epsilon_{k}^{1/p}\) if \(1\leq p<\infty\). Now letting \(k\to\infty\), (III) follows.

The proof of Theorem 3.2.2 is complete for \(0<p<\infty\). The case \(p=\infty\) is left as an exercise (Problem 3.14). \(\Box\)

#### Dual spaces

Let \(1\leq p<\infty\). Let \(g\in L^{q}(\mu)\), where \(q=\frac{p}{(p-1)}\) if \(1<p<\infty\) and \(q=\infty\) if \(p=1\). Let

\[T_{g}(f)=\int fgd\mu,\quad f\in L^{p}(\mu). \tag{2.8}\]

By Holders inequality, \(\int|fg|d\mu<\infty\) and so \(T_{g}(\cdot)\) is well defined. Clearly \(T_{g}\) is _linear_, i.e.,

\[T_{g}(\alpha_{1}f_{1}+\alpha_{2}f_{2})=\alpha_{1}T_{g}(f_{1})+\alpha_{2}T_{g}( f_{2}) \tag{2.9}\]

for all \(\alpha_{1},\alpha_{2}\in\mathbb{R}\) and \(f_{1},f_{2}\in L^{p}(\mu)\).

**Definition 3.2.3:**

* A function \(T:L^{p}(\mu)\to\mathbb{R}\) that satisfies (2.9) is called a _linear functional_.
* A linear functional \(T\) on \(L^{p}(\mu)\) is called _bounded_ if there is a constant \(c\in(0,\infty)\) such that \[|T(f)|\leq c\|f\|_{p}\quad\mbox{for all}\quad f\in L^{p}(\mu).\]
* The norm of a bounded linear functional \(T\) on \(L^{p}(\mu)\) is defined as \[\|T\|=\sup\big{\{}|Tf|:f\in L^{p}(\mu),\|f\|_{p}=1\big{\}}.\]

By Holder's inequality (cf. Theorem 3.1.11 and Remark 3.1.4),

\[|T_{g}(f)|\leq\|g\|_{q}\|f\|_{p}\quad\mbox{for all}\quad f\in L^{p}(\mu),\]

and hence, \(T_{g}\) is a bounded linear functional on \(L^{p}(\mu)\). This implies that if \(d_{p}(f_{n},f)\to 0\), then

\[|T_{g}(f_{n})-T_{g}(f)|\leq\|g\|_{q}d_{p}(f_{n},f)\to 0,\]

i.e., \(T_{g}\) is _continuous_ on the metric space \((L^{p}(\mu),d_{p})\).

**Definition 3.2.4:** The set of all continuous linear functionals on \(L^{p}(\mu)\) is called the _dual space_ of \(L^{p}(\mu)\) and is denoted by \((L^{p}(\mu))^{*}\).

In the next section, it will be shown that continuity of a linear functional on \(L^{p}(\mu)\) implies boundedness. A natural question is whether every continuous linear functional \(T\) on \(L^{p}(\mu)\) coincides with \(T_{g}\) for some \(g\) in \(L^{q}(\mu)\). The answer is "yes" for \(1\leq p<\infty\), as shown by the following result.

**Theorem 3.2.3:** (_Riesz representation theorem_)_. Let \(1\leq p<\infty\). Let \(T:L^{p}(\mu)\to\mathbb{R}\) be linear and continuous. Then, there exists a \(g\) in \(L^{q}(\mu)\) such that \(T=T_{g}\), i.e.,_

\[T(f)=T_{g}(f)\equiv\int fgd\mu\quad\mbox{for all}\quad f\in L^{p}(\mu), \tag{2.10}\]

_where \(q=\frac{p}{p-1}\) for \(1<p<\infty\) and \(q=\infty\) if \(p=1\)._

**Remark 3.2.2:** Such a representation is not valid for \(p=\infty\). That is, there exists continuous linear functionals \(T\) on \(L^{\infty}(\mu)\) for which there is no \(g\in L^{1}(\mu)\) such that \(T(f)=\int fgd\mu\) for all \(f\in L^{\infty}(\mu)\), provided \(\mu\) is not concentrated on a finite set \(\{\omega_{1},\omega_{2},\ldots,\omega_{k}\}\subset\Omega\).

For a proof of Theorem 3.2.3 and the above remark, see Royden (1988) or Rudin (1987).

Next consider the mapping from \((L^{p}(\mu))^{*}\) and \(L^{q}(\mu)\) defined by

\[\phi(T_{g})=g,\]

where \(T_{g}\) is as defined in (2.10). Then, \(\phi\) is linear, i.e.,

\[\phi(\alpha_{1}T_{1}+\alpha_{2}T_{2})=\alpha_{1}\phi(T_{1})+\alpha_{2}\phi(T_{ 2})\]

for all \(\alpha_{1},\alpha_{2}\in\mathbb{R}\) and \(T_{1},T_{2}\in(L^{p}(\mu))^{*}\). Further,

\[\|\phi(T)\|_{q}=\|T\|\quad\mbox{for all}\quad T\in(L^{p}(\mu))^{*}.\]

Thus, \(\phi\) preserves the vector space structure of \((L^{p}(\mu))^{*}\) and the norm. For this reason, it is called an _isometry_ between \((L^{p}(\mu))^{*}\) and \(L^{q}(\mu)\).

### 3.3 Banach and Hilbert spaces

#### Banach spaces

If \((\Omega,\mathcal{F},\mu)\) is a measure space it was seen in the previous section that the space \(L^{p}(\Omega,\mathcal{F},\mu)\) of equivalence classes of functions \(f\) with \(\int_{\Omega}|f|^{p}d\mu<\infty\) is a vector space over \(\mathbb{R}\) for all \(1\leq p<\infty\) and for \(p\geq 1\), \(\|\cdot\|_{p}\equiv(\int|f|^{p}d\mu)^{1/p}\) satisfies1. \(\|f+g\|\leq\|f\|+\|g\|\),
2. \(\|\alpha f\|=|a|\,\|f\|\) for every \(\alpha\in\mathbb{R}\),
3. \(\|f\|=0\) iff \(f=0\) a.e. \((\mu)\).

The Euclidean spaces \(\mathbb{R}^{k}\) for any \(k\in\mathbb{N}\) is also a vector space. Note that for \(p\geq 1\), setting \(\|x\|_{p}\equiv(\sum_{i=1}^{k}|x_{i}|^{p})^{1/p}\) if \(x=(x_{1},x_{2},\ldots,x_{k})\), \((\mathbb{R}^{k},\|x\|_{p})\) may be identified with a special case of \(L^{p}(\Omega,\mathcal{F},\mu)\), where \(\Omega\equiv\{1,2,\ldots,k\}\), \(\mathcal{F}=\mathcal{P}(\Omega)\) and \(\mu\) is the counting measure.

Generalizing the above examples leads to the notion of a _normed vector space_ (also called _normed linear space_). Recall that a _vector space \(V\) over \(\mathbb{R}\)_ is a nonempty set with a binary operation \(+\), a function from \(V\times V\) to \(V\) (called _addition_), and scalar multiplication by the real numbers, i.e., a function from \(\mathbb{R}\times V\to V\), \(\big{(}(\alpha,v)\to\alpha v\big{)}\) satisfying

1. \(v_{1},v_{2}\in V\Rightarrow v_{1}+v_{2}=v_{2}+v_{1}\in V\).
2. \(v_{1},v_{2},v_{3}\in V\Rightarrow(v_{1}+v_{2})+v_{3}=v_{1}+(v_{2}+v_{3})\).
3. There exists an element \(\theta\), called the _zero vector_, in \(V\) such that \(v+\theta=v\) for all \(v\) in \(V\).
4. \(\alpha\in\mathbb{R}\), \(v\in V\Rightarrow\alpha v\in V\).
5. \(\alpha\in\mathbb{R}\), \(v_{1},v_{2}\in V\Rightarrow\alpha(v_{1}+v_{2})=\alpha v_{1}+\alpha v_{2}\).
6. \(\alpha_{1}\), \(\alpha_{2}\in\mathbb{R}\), \(v\in V\Rightarrow(\alpha_{1}+\alpha_{2})v=\alpha_{1}v+\alpha_{2}v\) and \(\alpha_{1}(\alpha_{2}v)=(\alpha_{1}\alpha_{2})v\).
7. \(v\in V\Rightarrow 0v=\theta\) and \(1v=v\).

Note that from conditions (vi) and (vii) above, it follows that for any \(v\) in \(V\), \(v+(-1)v=0\cdot v=\theta\). Thus for any \(v\) in \(V\), \((-1)v\) is the additive inverse and is denoted by \(-v\). Conditions (i), (ii), and (iii) are called respectively _commutativity_, _associativity_, and the _existence of an additive identity_. Thus \(V\) under the operation \(+\) is an Abelian (i.e., commutative) group.

**Definition 3.3.1:** A function \(f\) from \(V\) to \(\mathbb{R}_{+}\) denoted by \(f(v)\equiv\|v\|\) is called a _norm_ if

1. \(v_{1},v_{2}\in V\Rightarrow\|v_{1}+v_{2}\|\leq\|v_{1}\|+\|v_{2}\|\)  (_triangle inequality_)
2. \(\alpha\in\mathbb{R}\), \(v\in V\Rightarrow\|\alpha v\|=|\alpha|\,\|v\|\)  (_scalar homogeneity_)
3. \(\|v\|=0\) iff \(v=\theta\).

A vector space \(V\) with a norm \(\|\cdot\|\) defined on it is called a _normed vector space_ or _normed linear space_ and is denoted as \((V,\|\cdot\|)\). Let \(d(v_{1},v_{2})\equiv\|v_{1}-v_{2}\|\), \(v_{1},v_{2}\in V\). Then from the definition of \(\|\cdot\|\), it follows that \(d\) is a metric on \(V\), i.e., \((V,d)\) is a metric space. Recall that a metric space \((\mathbb{S},d)\)is called _complete_ if every Cauchy sequence \(\{x_{n}\}_{n\geq 1}\) in \(\mathbb{S}\) converges to an element \(x\) in \(\mathbb{S}\).

**Definition 3.3.2:** A _Banach space_ is a complete normed linear space \((V,\|\cdot\|)\).

It was shown by S. Banach of Poland that all \(L^{p}(\Omega,\mathcal{B},\mu)\) spaces are Banach spaces, provided \(p\geq 1\) and in particular, all Euclidean spaces are Banach spaces. An example of a different kind is the space \(C[0,1]\) of all real valued continuous functions on \([0,1]\) with the usual operation of pointwise addition and scalar multiplication, i.e., \((f+g)(x)=f(x)+g(x)\) and \((\alpha f)(x)=\alpha\cdot f(x)\) for all \(\alpha\in\mathbb{R}\), \(0\leq x\leq 1\), \(f\), \(g\in C[0,1]\) where the norm (called the _supnorm_) is defined by \(\|f\|=\sup\{|f(x)|=0\leq x\leq 1\}\). The verification of the fact that \(C[0,1]\) with the supnorm is a Banach space is left as an exercise (Problem 3.22). The space \(\mathcal{P}\) of all polynomials on \([0,1]\) is also a normed linear space under the above norm but \((\mathcal{P},\|\cdot\|)\) is _not complete_ (Problem 3.23). However for each \(n\in\mathbb{N}\), the space \(\mathcal{P}_{n}\) of all polynomials on \([0,1]\) of degree \(\leq n\) is a Banach space under the supnorm (Problem 3.26).

**Definition 3.3.3:** Let \(V\) be a vector space. A subset \(W\subset V\) is called a _subspace_ of \(V\) if \(v_{1},v_{2}\in W,\alpha_{1},\alpha_{2}\in\mathbb{R}\Rightarrow\alpha_{1}v_{1} +\alpha_{2}v_{2}\in W\). If \((V,\|\cdot\|)\) is a normed vector space and \(W\) is a subspace of \(V\), then \((W,\|\cdot\|)\) is also a normed vector space. If \(W\) is closed in \((V,\|\cdot\|)\), then \(W\) is called a _closed subspace_ of \(V\).

**Remark 3.3.1:** If \((V,\|\cdot\|)\) is a Banach space and \(W\) is a closed subspace of \(V\), then \((W,\|\cdot\|)\) is also a Banach space.

#### 3.3.2 Linear transformations

Let \((V_{i},\|\cdot\|_{i})\), \(i=1,2\) be two normed linear spaces over \(\mathbb{R}\).

**Definition 3.3.4:** A function \(T\) from \(V_{1}\) to \(V_{2}\) is called a _linear transformation_ or _linear operator_ if \(\alpha_{1}\), \(\alpha_{2}\in\mathbb{R}\), \(x\), \(y\in V_{1}\Rightarrow T(\alpha_{1}x+\alpha_{2}y)=\alpha_{1}T(x)+\alpha_{2}T(y)\).

**Definition 3.3.5:** A linear operator \(T\) from \((V_{1},\|\cdot\|_{1})\) to \((V_{2},\|\cdot\|_{2})\) is called _bounded_ if \(\|T\|\equiv\ \sup\{\|Tx\|_{2}:\|x\|_{1}<1\}<\infty\), i.e., the image of the _unit ball_ in \((V_{1},\|\cdot\|_{1})\) is contained in a ball of finite radius centered at the zero in \(V_{2}\).

Here is a summary of some important results on this topic. By linearity of \(T\), \(T(\frac{x}{\|x\|})=\frac{1}{\|x\|}T(x)\) for any \(x\neq 0\). It follows that \(T\) is _bounded_ iff there exists \(k<\infty\) such that for any \(x\in V_{1}\), \(\|Tx\|_{2}\leq k\|x\|_{1}\). Clearly, then \(k\) can be taken to be \(\|T\|\). Also by linearity, if \(T\) is bounded, then \(\|Tx_{1}-Tx_{2}\|=\|T(x_{1}-x_{2})\|\leq\|T\|\ \|x_{1}-x_{2}\|\) and so the map \(T\) is _continuous_(indeed, uniformly so). It turns out that if a linear operator \(T\) is continuous at some \(x_{0}\) in \(V_{1}\), then \(T\) is continuous on all of \(V_{1}\) and is bounded (Problem 3.28 (a)).

Now let \(B(V_{1},V_{2})\) be the space of all bounded linear operators from \((V_{1},\|\cdot\|_{1})\) to \((V_{2},\|\cdot\|_{2})\). For \(T_{1}\), \(T_{2}\in B(V_{1},V_{2})\), \(\alpha_{1}\), \(\alpha_{2}\) in \(\mathbb{R}\), let \((\alpha_{1}T_{1}+\alpha_{2}T_{2})\) be defined by \((\alpha_{1}T_{1}+\alpha_{2}T_{2})(x)\equiv\alpha_{1}T_{1}(x)+\alpha_{2}T_{1}(x)\) for all \(x\) in \(V_{1}\). Then it can be verified that \((\alpha_{1}T_{1}+\alpha_{2}T_{2})\) also belongs to \(B(V_{1},V_{2})\) and

\[\|T\|\equiv\sup\{\|Tx\|_{2}:\|x\|_{1}\leq 1\} \tag{3.1}\]

is a norm on \(B(V_{1},V_{2})\). Thus \((B(V_{1},V_{2}),\|\cdot\|)\) is also a normed linear space.

If \((V_{2},\|\cdot\|_{2})\) is complete, then it can be shown that \((B(V_{1},V_{2}),\|\cdot\|)\) is also a Banach space (Problem 3.28 (b)). In particular, if \((V_{2},\|\cdot\|_{2})\) is the real line, the space \((B(V_{1},\mathbb{R}),\|\cdot\|)\) is a Banach space.

#### Dual spaces

**Definition 3.3.6:** The space of all bounded linear functions from \((V_{1},\|\cdot\|)\) to \(\mathbb{R}\) (also called _bounded linear functionals_), denoted by \(V_{1}^{*}\), is called the _dual space_ of \(V_{1}\).

Thus, for any normed linear space \((V_{1},\|\cdot\|_{1})\) (that need not be complete), the dual space \((V_{1}^{*},\|\cdot\|)\) is always a Banach space, where \(\|T\|\equiv\sup\{|Tx|:\|x\|_{1}<1\}\) for \(T\in V_{1}^{*}\). If \((V_{1},\|\cdot\|_{1})=L^{p}(\Omega,\mathcal{F},\mu)\) for some measure space \((\Omega,\mathcal{F},\mu)\) and \(1\leq p<\infty\), by the _Riesz representation theorem_ (see Theorem 3.2.3), the dual space may be identified with \(L^{q}(\Omega,\mathcal{F},\mu)\) where \(q\) is the conjugate of \(p\), i.e., \(\frac{1}{p}+\frac{1}{q}=1\). However, as pointed out earlier in Section 3.2, this is not true for \(p=\infty\). That is, the dual of \(L^{\infty}(\Omega,\mathcal{F},\mu)\) is not \(L^{1}(\Omega,\mathcal{F},\mu)\) unless \((\Omega,\mathcal{F},\mu)\) is a measure space where \(\Omega\) is a finite set \(\{w_{1},w_{2},\ldots,w_{k}\}\) and \(\mathcal{F}=\mathcal{P}(\Omega)\). An example for the \(p=\infty\) case can be constructed for the space \(\ell_{\infty}\) of all bounded sequences of real numbers (cf. Royden (1988)).

The representation of the dual space of the Banach space \(C[0,1]\) with supnorm is in terms of finite signed measures (cf. Section 4.2).

**Theorem 3.3.1:** (_Riesz_). _Let \(T:C[0,1]\to\mathbb{R}\) be linear and bounded. Then there exists two finite measures \(\mu_{1}\) and \(\mu_{2}\) on \([0,1]\) such that for any \(f\in C[0,1]\)_

\[T(f)=\int fd\mu_{1}-\int fd\mu_{2}.\]

For a proof see Royden (1988) or Rudin (1987) (see also Problem 3.27).

#### 3.3.4 Hilbert space

A vector space \(V\) over \(\mathbb{R}\) is called a _real innerproduct_ space if there exists a function \(f:V\times V\to\mathbb{R}\), denoted by \(f(x,y)\equiv\langle x,y\rangle\) (and called the _innerproduct_) that satisfies

1. \(\langle x,y\rangle=\langle y,x\rangle\) for all \(x,y\in V\),
2. \((\)_linearity_\()\)\(\langle\alpha_{1}x_{1}+\alpha_{2}x_{2},y\rangle=\alpha_{1}\langle x,y\rangle+ \alpha_{2}\langle x_{2},y\rangle\) for all \(\alpha_{1}\), \(\alpha_{2}\in\mathbb{R}\), \(x_{1}\), \(x_{2}\), \(y\in V\),
3. \(\langle x,x\rangle\geq 0\) for all \(x\in V\) and \(\langle x,x\rangle=0\) iff \(x=\theta\), the zero vector of \(V\).

Using the fact that the quadratic function \(\varphi(t)=\langle x+ty,x+ty\rangle=\langle x,x\rangle+2t\langle x,y\rangle+t ^{2}\langle y,y\rangle\) is nonnegative for all \(t\in\mathbb{R}\), one gets the _Cauchy-Schwarz_ inequality

\[|\langle x,y\rangle|\leq\sqrt{\langle x,x\rangle\langle y,y\rangle}\quad\text {for all}\quad x,y\in V.\]

Now setting \(\|x\|=\sqrt{\langle x,x\rangle}\) and using the Cauchy-Schwarz inequality, one verifies that \(\|x\|\) is a norm on \(V\) and thus \((V,\|\cdot\|)\) is a normed linear space. Further, the function \(\langle x,y\rangle\) from \(V\times V\) to \(\mathbb{R}\) is continuous (Problem 3.29) under the norm \(\|(x_{1},x_{2})\|=\|x_{1}\|+\|x_{2}\|\), \((x_{1},x_{2})\in V\times V\).

**Definition 3.3.7:** Let \((V,\langle\cdot,\cdot\rangle)\) be a real innerproduct space. It is called a _Hilbert space_ if \((V,\|\cdot\|)\) is a Banach space, i.e., if it is complete.

It was seen in Section 3.2 that for any measure space \((\Omega,\mathcal{F},\mu)\), the space \(L^{2}(\Omega,\mathcal{F},\mu)\) of all equivalence classes of functions \(f:\Omega\to\mathbb{R}\) satisfying \(\int|f|^{2}d\mu<\infty\) is a complete innerproduct space with the innerproduct \(\langle f,g\rangle=\int fgd\mu\) and hence a Hilbert space. It turns out that every Hilbert space \(H\) is an \(L^{2}(\Omega,\mathcal{F},\mu)\) for some \((\Omega,\mathcal{F},\mu)\). (The axiom of choice or its equivalent, the Hausdorff's maximality principle, is required for a proof of this. See Rudin (1987).) This is in contrast to the Banach space case where every \(L^{p}(\Omega,\mathcal{F},\mu)\) with \(p\geq 1\) is a Banach space but not conversely, i.e., every Banach space need not be an \(L^{p}(\Omega,\mathcal{F},\mu)\).

Next for each \(x\) in a Hilbert space \(H\), let \(T_{x}:H\to\mathbb{R}\) be defined by \(T_{x}(y)=\langle x,y\rangle\). By the defining properties of \(\langle x,y\rangle\) and the Cauchy-Schwarz inequality, it is easy to verify that \(T_{x}\) is a bounded linear function on \(H\), i.e.,

\[T_{x}(\alpha_{1}y_{1}+\alpha_{2}y_{2})=\alpha_{1}T_{x}(y_{1})+\alpha_{2}T_{x}( y_{2})\quad\text{for all}\quad\alpha_{1},\alpha_{2}\in\mathbb{R},y_{1},y_{2}\in H \tag{3.2}\]

and

\[|T_{x}(y)|\leq\|x\|\,\|y\|\quad\text{for all}\quad y\in H. \tag{3.3}\]

Thus \(T_{x}\in H^{*}\), the dual space. It is an important result (see Theorem 3.3.3 below) that every \(T\in H^{*}\) is equal to \(T_{x}\) for some \(x\) in \(H\) and \(\|T\|=\|x\|\). Thus \(H^{*}\) can be identified with \(H\).

**Definition 3.3.8:** Let \((V,\langle\cdot,\cdot\rangle)\) be an inner product space. Two vectors \(x,y\) in \(V\) are said to be _orthogonal_ and written as \(x\perp y\) if \(\langle x,y\rangle=0\).

A collection \(B\subset V\) is called _orthogonal_ if \(x,y\in B\), \(x\neq y\Rightarrow\langle x,y\rangle=0\). The collection \(B\) is called _orthonormal_ if it is orthogonal and in addition for all \(x\) in \(B\), \(\|x\|=1\). Note that if \(x\perp y\), then \(\|x-y\|^{2}=\langle x-y,x-y\rangle=\langle x,x\rangle+\langle y,y\rangle=\|x \|^{2}+\|y\|^{2}\) and so if \(B\) is an orthonormal set, then \(x,y\in B\Rightarrow\) either \(x=y\) or \(\|x-y\|=\sqrt{2}\). Thus, if \(V\) is separable under the metric \(d(x,y)=\|x-y\|\) (i.e., there exists a countable set \(D\subset V\) such that for every \(x\) in \(V\) and \(\epsilon>0\), there exists a \(d\in D\) such that \(\|x-d\|<\epsilon\)) and if \(B\subset V\) is an orthonormal system, then the open ball \(S_{b}\) of radius \(\frac{1}{2\sqrt{2}}\) around each \(b\in B\) satisfies \(\{S_{b}\cap D:b\in B\}\) are disjoint and nonempty. Thus \(B\) is countable.

Now let \((V,\langle\cdot,\cdot\rangle)\) be a separable innerproduct space and \(B\subset V\) be an orthonormal system.

**Definition 3.3.9:** The _Fourier coefficients_ of a vector \(x\) in \(V\) with respect on _orthonormal set_\(B\) is the set \(\{\langle x,b\rangle:b\in B\}\).

Since \(V\) is separable, \(B\) is countable. Let \(B=\{b_{i}:i\in\mathbb{N}\}\). For a given \(x\in V\), let \(c_{i}=\langle x,b_{i}\rangle\), \(i\geq 1\). Let \(x_{n}\equiv\sum_{i=1}^{n}c_{i}b_{i}\), \(n\in\mathbb{N}\). The sequence \(\{x_{n}\}_{n\geq 1}\) is called the _partial sum sequence of the Fourier expansion of the vector \(x\) w.r.t. the orthonormal set \(B\)_.

A natural question is: _when does \(\{x_{n}\}_{n\geq 1}\) converge to \(x\)?_ By the linearity property in the definition of the innerproduct \(\langle\cdot,\cdot\rangle\), it follows that

\[0\leq\|x-x_{n}\|^{2}=\langle x-x_{n},x-x_{n}\rangle=\langle x,x\rangle-2 \langle x,x_{n}\rangle+\langle x_{n},x_{n}\rangle\]

and

\[\langle x,x_{n}\rangle=\sum_{i=1}^{n}c_{i}\langle x,b_{i}\rangle=\sum_{i=1}^{ n}c_{i}^{2}.\]

Since \(\{b_{i}\}_{i\geq 1}\) are orthonormal,

\[\|x_{n}\|^{2}=\langle x_{n},x_{n}\rangle=\sum_{i=1}^{n}c_{i}^{2}=\langle x,x_ {n}\rangle.\]

Thus,

\[0\leq\|x-x_{n}\|^{2}=\|x\|^{2}-\|x_{n}\|^{2}=\|x\|^{2}-\sum_{i=1}^{n}c_{i}^{2},\]

leading to

**Proposition 3.3.2:** (_Bessel's inequality_)_. Let \(\{b_{i}\}_{i\geq 1}\) be orthonormal in an innerproduct space \((V,\langle\cdot,\cdot\rangle)\). Then, for any \(x\) in \(V\),_

\[\sum_{i=1}^{\infty}\langle x,b_{i}\rangle^{2}\leq\|x\|^{2}. \tag{3.4}\]Now let \((V,\langle\cdot,\cdot\rangle)\) be a _Hilbert space_. Since for \(m>n\),

\[\|x_{n}-x_{m}\|^{2}=\sum_{i=n+1}^{m}\langle x,b_{i}\rangle^{2},\]

it follows from Bessel's inequality that \(\{x_{n}\}_{n\geq 1}\) is a Cauchy sequence, and since \(V\) is complete, there is a \(y\) in \(V\) such that \(x_{n}\to y\). This implies (by the continuity of \(\langle x,y\rangle\)) that \(\langle x,b_{i}\rangle=\lim_{n\to\infty}\langle x_{n},b_{i}\rangle=\langle y, b_{i}\rangle\Rightarrow\langle x-y,b_{i}\rangle=0\) for all \(i\geq 1\). Thus, it follows that \(\langle x,b_{i}\rangle=\langle y,b_{i}\rangle\) for all \(i\geq 1\). The last relation implies \(y=x\) iff the set \(\{b_{i}\}_{i\geq 1}\) satisfies the property that

\[\langle z,b_{i}\rangle=0\quad\text{ for all }\quad i\geq 1\Rightarrow\|z\|=0. \tag{3.5}\]

This property is called the _completeness of \(B\)_. Thus \(B\equiv\{b_{i}\}_{i\geq 1}\) is a _complete orthonormal set_ for a Hilbert space \(H\equiv(V,\langle\cdot,\cdot\rangle)\), iff for every vector \(x\),

\[\sum_{i=1}^{\infty}c_{i}^{2}=\|x\|^{2}, \tag{3.6}\]

where \(c_{i}=\langle x,b_{i}\rangle\), \(i\geq 1\), which in turn holds iff

\[\left\|x-\sum_{i=1}^{n}c_{i}b_{i}\right\|\to 0\quad\text{as}\quad n\to\infty. \tag{3.7}\]

Conversely, if \(\{c_{i}\}_{i\geq 1}\) is a sequence of real numbers such that \(\sum_{i=1}^{\infty}c_{i}^{2}<\infty\), then the sequence \(\{x_{n}\equiv\sum_{i=1}^{n}c_{i}b_{i}\}_{n\geq 1}\) is Cauchy and hence converges to an \(x\) in \(V\). Thus the Hilbert space \(H\) can be identified with the space \(\ell_{2}\) of all _square summable_ sequences \(\big{\{}\{c_{i}\}_{i\geq 1}:\sum_{i=1}^{\infty}c_{i}^{2}<\infty\big{\}}\), in the sense that the map \(\varphi:x\to\{c_{i}\}_{i\geq 1}\), where \(c_{i}=\langle x,b_{i}\rangle\), \(i\geq 1\), preserves the algebraic structure as well as the innerproduct, i.e., \(\varphi\) is a linear operator from \(H\) to \(\ell_{2}\) and \(\langle\varphi(x),\varphi(y)\rangle=\langle x,y\rangle\) for all \(x,y\in H\). Such a \(\varphi\) is called an _isometric isomorphism_ between \(H\) to \(\ell_{2}\). Note also that \(\ell_{2}\) is simply \(L_{2}(\Omega,\mathcal{F},\mu)\) where \(\Omega\equiv\mathbb{N}\), \(\mathcal{F}=\mathcal{P}(\mathbb{N})\), and \(\mu\), the counting measure. It can be shown (using the axiom of choice) that every separable Hilbert space does possess a complete orthonormal system, i.e., an _orthonormal basis_.

Next some examples are given. Here, unless otherwise indicated, \(H\) denotes the Hilbert space and \(B\) denotes an orthonormal basis of \(H\).

**Example 3.3.1**:
1. \(H\equiv\ell_{2}=\{(x_{1},x_{2},\ldots):x_{i}\in\mathbb{R},\sum_{i=1}^{\infty }x_{i}^{2}<\infty\}\). \(B\equiv\{e_{i}:i\geq 1\}\quad\text{where}\quad e_{i}\equiv(0,0,\ldots,1,0,\ldots)\) with \(1\) in the \(i\)th position and \(0\) elsewhere.
2. \(H\equiv L^{2}\left([0,2\pi],\mathcal{B}\big{(}[0,2\pi]\big{)},\mu\right)\) where \(\mu(A)=\frac{1}{2\pi}m(A)\), \(m(\cdot)\) being Lebesgue measure. \(B\equiv\{\cos nx:n=0,1,2,\ldots,\}\cup\{\sin nx:n=1,2,\ldots\}\). (For a proof, see Chapter 5.)3. Let \(H\equiv L^{2}(\mathbb{R},\mathcal{B}(\mathbb{R}),\mu)\) where \(\mu\) is a finite measure such that \(\int|x|^{k}d\mu<\infty\) for all \(k=1,2,\ldots\). Let \(B_{1}\equiv\{1,x,x^{2},\ldots\}\) and \(B\) be the orthonormal set generated by applying the Gram-Schmidt procedure to \(B_{1}\) (see Problem 3.31). It can be shown that \(B\) is a basis for \(H\) (Problem 3.39). When \(\mu\) is the standard normal distribution, the elements of \(B\) are called _Hermite_ polynomials.

For one more example, i.e., Haar functions, see Problem 3.40.

**Theorem 3.3.3:** (_Riesz representation_)_. Let \(H\) be a separable Hilbert space. Then every bounded linear functional \(T\) on \(H\to\mathbb{R}\) can be represented as \(T\equiv T_{x_{0}}\) for some \(x_{0}\in V\), where \(T_{x_{0}}(y)\equiv\langle y,x_{0}\rangle\)._

**Proof:** Let \(B=\{b_{i}\}_{i\geq 1}\) be an orthonormal basis for \(H\). Let \(c_{i}\equiv T(b_{i})\), \(i\geq 1\). Then, for \(n\geq 1\),

\[\sum_{i=1}^{n}c_{i}^{2} = \sum_{i=1}^{n}c_{i}T(b_{i})\] \[= T\bigg{(}\sum_{i=1}^{n}c_{i}b_{i}\bigg{)}\quad\text{(by the linearity of $T$)}\] \[\Rightarrow\bigg{|}\sum_{i=1}^{n}c_{i}^{2}\bigg{|} \leq \|T\|\bigg{\|}\sum_{i=1}^{n}c_{i}b_{i}\bigg{\|}=\|T\|\bigg{(} \sum_{i=1}^{n}c_{i}^{2}\bigg{)}^{1/2}\] \[\Rightarrow\sum_{i=1}^{n}c_{i}^{2} \leq \|T\|^{2}\] \[\Rightarrow\sum_{i=1}^{\infty}c_{i}^{2} < \infty.\]

Thus \(\{x_{n}\equiv\sum_{i=1}^{n}c_{i}b_{i}\}_{n\geq 1}\) is Cauchy in \(H\) and hence converges to an \(x_{0}\) in \(H\). By the continuity of \(T\), for any \(y\), \(Ty=\lim_{n\to\infty}Ty_{n}\), where \(y_{n}\equiv\sum_{i=1}^{n}\langle y,b_{i}\rangle b_{i}\), \(n\geq 1\). But

\[Ty_{n} = \sum_{i=1}^{n}\langle y,b_{i}\rangle c_{i}=\bigg{\langle}y,\sum_{ i=1}^{n}b_{i}c_{i}\bigg{\rangle}\] \[= \langle y,x_{n}\rangle,\ \ \text{by the linearity of $T$}\]

Again by continuity of \(\langle y,x\rangle\), it follows that \(Ty=\langle y,x_{0}\rangle\). \(\Box\)

A sufficient condition for an \(L^{2}(\Omega,\mathcal{F},\mu)\) to be separable is that there exists an at most countable family \(\mathcal{A}\equiv\{A_{j}\}_{j\geq 1}\) of sets in \(\mathcal{F}\) such that \(\mathcal{F}=\sigma\langle\mathcal{A}\rangle\) and \(\mu(A_{j})>0\) for each \(j\). This holds for any \(\sigma\)-finite measure \(\mu\) on \(\big{(}\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k})\big{)}\) (Problem 3.38).

**Remark 3.3.2:** Assuming the axiom of choice, the Riesz representation theorem remains valid for any Hilbert space, separable or not (Problem 3.43).

### 3.4 Problems

1. Prove Proposition 3.1.7. (**Hint:** Use (1.4) repeatedly.)
2. Let \((\Omega,\mathcal{F},\mu)\) be a measure space with \(\mu(\Omega)\leq 1\) and \(f:\Omega\to(a,b)\subset\mathbb{R}\) be in \(L^{1}(\Omega,\mathcal{F},\mu)\). Let \(\phi:(a,b)\to\mathbb{R}\) be convex. Show that if \(c\equiv\int fd\mu\in(a,b)\) and \(\phi(f)\in L^{1}(\Omega,\mathcal{F},\mu)\) and \(c\phi^{\prime}_{+}(c)\geq 0\), then \[\mu(\Omega)\,\phi\bigg{(}\int fd\mu\bigg{)}\leq\int\phi(f)d\mu.\]
3.3 Prove Proposition 3.1.10. (**Hint:** Apply Jensen's inequality with \(\Omega\equiv\{1,2,\ldots,k\}\), \(\mathcal{F}=\mathcal{P}(\Omega)\), \(P(\{i\})=p_{i}\), \(f(i)=a_{i}\), \(i=1,2,\ldots,k\), and \(\phi(x)=e^{x}\) to get (i). Deduce (ii) from (i) and Remark 3.1.3. For (iii), consider \(\phi(x)=|x|^{p}\).)
4. Give an example of a convex function \(\phi\) on \((0,1)\) with a finite number of points where it is not differentiable. Can this be extended to the countable case? Uncountable case? (**Hint:** Note that \(\phi^{\prime}_{+}(\cdot)\) and \(\phi^{\prime}_{-}(\cdot)\) are both monotone and hence have at most a countable number of discontinuity points.)
5. Let \(\phi:(a,b)\to\mathbb{R}\) be convex. 1. Using the definition and induction, show that \[\phi\bigg{(}\sum_{i=1}^{n}p_{i}x_{i}\bigg{)}\leq\sum_{i=1}^{n}p_{i}\phi(x_{i})\] for any \(n\geq 2\), \(x_{1},x_{2}\ldots,x_{n}\) in \((a,b)\) and \(\{p_{1},p_{2},\ldots,p_{n}\}\), a probability distribution. 2. Use (a) to prove Jensen's inequality for any bounded \(\phi\).
6. Show that a function \(\phi:\mathbb{R}\to\mathbb{R}\) is convex iff \[\phi\bigg{(}\int_{[0,1]}fdm\bigg{)}\leq\int_{[0,1]}\phi(f)dm\] for every bounded Borel measurable function \(f:[0,1]\to\mathbb{R}\), where \(m(\cdot)\) is the Lebesgue measure.

3.7 Let \(\phi\) be convex on \((a,b)\) and \(\psi:\mathbb{R}\to\mathbb{R}\) be convex and nondecreasing. Show that \(\psi\circ\phi\) is convex on \((a,b)\).
3.8 Let \(X\) be a nonnegative random variable on some probability space. 1. Show that \((EX)(E\frac{1}{X})\geq 1\). What does this say about the correlation between \(X\) and \(\frac{1}{X}\)? 2. Let \(f,g:\mathbb{R}_{+}\to\mathbb{R}_{+}\) be Borel measurable and such that \(f(x)g(x)\geq 1\) for all \(x\) in \(\mathbb{R}_{+}\). Show that \(Ef(X)Eg(X)\geq 1\).
3.9 Prove Corollary 3.1.13 using Holder's inequality applied to an appropriate measure space.
3.10 Extend Holder's inequality as follows. Let \(1<p_{i}<\infty\), and \(f_{i}\in L^{p_{i}}(\Omega,\mathcal{F},\mu)\), \(i=1,2,\ldots,k\). Suppose \(\sum_{i=1}^{k}\frac{1}{p_{i}}=1\). Show that \(\int\big{(}\prod_{i=1}^{k}f_{i}\big{)}d\mu\leq\prod_{i=1}^{k}\|f_{i}\|_{p_{i}}\). (**Hint:** Use Proposition 3.1.10 (ii).)
3.11 Verify Minkowski's inequality for \(p=1\) and \(p=\infty\).
3.12 1. Find \((\Omega,\mathcal{F},\mu)\), \(0<p<1\), \(f\), \(g\in L^{p}(\Omega,\mathcal{F},\mu)\) such that \[\Big{(}\int|f+g|^{p}d\mu\Big{)}^{1/p}>\Big{(}\int|f|^{p}d\mu\Big{)}^{1/p}+ \Big{(}\int|g|^{p}d\mu\Big{)}^{1/p}.\] 2. Prove (1.18) for \(0<p<1\) with \(\|f\|_{p}=\int|f|^{p}d\mu\).
3.13 Let \((\Omega,\mathcal{F},\mu)\) be a measure space. Let \(\{A_{k}\}_{k\geq 1}\subset\mathcal{F}\) and \(\sum_{k=1}^{\infty}\mu(A_{k})<\infty\). Show that \(\mu\Big{(}\underset{k\to\infty}{\overline{\lim}}A_{k}\Big{)}=0\), where \(\underset{k\to\infty}{\overline{\lim}}A_{k}=\bigcap_{n=1}^{\infty}\bigcup_{j \geq n}A_{j}=\{\omega:\omega\in A_{j}\) for infinitely many \(j\geq 1\}\).
3.14 Establish Theorem 3.2.2 for \(p=\infty\). (**Hint:** For each \(k\geq 1\), choose \(n_{k}\uparrow\) such that \(\|f_{n_{k+1}}-f_{n_{k}}\|_{\infty}<2^{-k}\). Show that there exists a set A with \(\mu(A^{c})=0\) and for \(\omega\) in \(A\), \(|f_{n_{k+1}}(\omega)-f_{n_{k}}(\omega)|<2^{-k}\) for all \(k\geq 1\) and now proceed as in the proof for the case \(0<p<\infty\).)
3.15 Let \(f\), \(g\in L^{p}(\Omega,\mathcal{F},\mu)\), \(0<p<1\). Show that \(d(f,g)=\int|f-g|^{p}d\mu\) is a metric and \((L^{p}(\Omega,\mathcal{F},\mu),d)\) is complete.
3.16 Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(f:\Omega\to\mathbb{R}\) be \(\mathcal{F}\)-measurable. Let \(A_{f}=\{p:0<p<\infty,\int|f|^{p}d\mu<\infty\}\). 1. Show that \(p_{1}\), \(p_{2}\in A_{f}\), \(p_{1}<p_{2}\) implies \([p_{1},p_{2}]\subset A_{f}\). (**Hint:** Use \(\int_{|f|\geq 1}|f|^{p}d\mu\leq\int_{|f|\geq 1}|f|^{p_{2}}d\mu\) and \(\int_{|f|\leq 1}|f|^{p}d\mu\leq\int_{|f|\leq 1}|f|^{p_{1}}d\mu\) for any \(p_{1}<p<p_{2}\).)2. Let \(\psi(p)=\log\int|f|^{p}d\mu\) for \(p\in A_{f}\). By (a), it is known that \(A_{f}\) is connected, i.e., it is an interval. Show that \(\psi\) is convex in the open interior of \(A_{f}\). (**Hint:** Use Holder's inequality.) 3. Give examples to show that \(A_{f}\) could be a closed interval, an open interval, and semi-open intervals. 4. If \(0<p_{1}<p<p_{2}\), show that \[\|f\|_{p}\leq\max\{\|f\|_{p_{1}},\|f\|_{p_{2}}\}\] (**Hint:** Use (b).) 5. Show that if \(\int|f|^{r}d\mu<\infty\) for some \(0<r<\infty\), then \(\|f\|_{p}\to\|f\|_{\infty}\) as \(p\to\infty\). (**Hint:** Show first that for any \(K>0\), \(\mu(|f|>K)>0\Rightarrow\varliminf_{p\to\infty}\|f\|_{p}\geq K\). If \(\|f\|_{\infty}<\infty\) and \(\mu(\Omega)<\infty\), use the fact that \(\|f\|_{p}\leq\|f\|_{\infty}(\mu(\Omega))^{1/p}\) and reduce the general case under the hypothesis that \(\int|f|^{p}d\mu<\infty\) for some \(p\) to this case.)
3. Let \(X\) be a random variable on a probability space \((\Omega,\mathcal{F},\mu)\). Recall that \(Eh(X)=\int h(X)d\mu\) if \(h(X)\in L^{1}(\Omega,\mathcal{F},\mu)\). 1. Show that \((E|X|^{p_{1}})\leq(E|X|^{p_{2}})^{\frac{p_{1}}{p_{2}}}\) for any \(0<p_{1}<p_{2}<\infty\). 2. Show that '=' holds in (a) iff \(|X|\) is a constant a.e. \((\mu)\). 3. Show that if \(E|X|<\infty\), then \(E|\log|X||<\infty\) and \(E|X|^{r}<\infty\) for all \(0<r<1\), and \(\frac{1}{r}\log(E|X|^{r})\to E\log|X|\) as \(r\to 0\).
3. Let \(X\) be a nonnegative random variable. 1. Show that \(EX\log X\geq(EX)(E\log X)\). 2. Show that \(\sqrt{1+(EX)^{2}}\leq E(\sqrt{1+X^{2}})\leq 1+EX\).
3. Let \(\Omega=\mathbb{N}\), \(\mathcal{F}=\mathcal{P}(\mathbb{N})\), and let \(\mu\) be the counting measure. Denote \(L^{p}(\Omega,\mathcal{F},\mu)\) for this case by \(\ell_{p}\). 1. Show that \(\ell_{p}\) is the set of all sequences \(\{x_{n}\}_{n\geq 1}\) such that \(\sum_{n=1}^{\infty}|x_{n}|^{p}<\infty\). 2. For the following sequences, find all \(p>0\) such that they belong to \(\ell_{p}\): 1. \(x_{n}\equiv\frac{1}{n}\), \(n\geq 1\). 2. \(x_{n}=\frac{1}{n(\log(n+1))^{2}}\), \(n\geq 1\).

3.20 For \(1\leq p<\infty\), prove the Riesz representation theorem for \(\ell_{p}\). That is, show that if \(T\) is a bounded linear functional from \(\ell_{p}\to\mathbb{R}\), then there exists a \(y=\{y_{i}\}_{i\geq 1}\in\ell_{q}\) such that for any \(x=\{x_{i}\}_{i\geq 1}\) in \(\ell_{p}\), \(T(x)=\sum_{i=1}^{\infty}x_{i}y_{i}\). (**Hint:** Let \(y_{i}=T(e_{i})\) where \(e_{i}=\{e_{i}(j)\}_{j\geq 1}\), \(e_{i}(j)=1\) if \(i=j\), \(0\) if \(i\neq j\). Use the fact \(|T(x)|\leq\|T\|\,\|x\|_{p}\) to show that for each \(n\in\mathbb{N}\), \((\sum_{i=1}^{n}|y_{i}|^{q})\leq\|T\|^{q}<\infty\).)
3.21 Let \(\Omega=\mathbb{R}\), \(\mathcal{F}=\mathcal{B}(\mathbb{R})\), \(\mu=\mu_{F}\) where \(F\) is a cdf on \(\mathbb{R}\). If \(f(x)\equiv x^{2}\), find \(A_{f}=\{p:0<p<\infty\), \(f\in L^{p}(\mathbb{R},\mathcal{B}(\mathbb{R}),\,\mu_{F})\}\) for the following cases: 1. \(F(x)=\Phi(x)\), the \(N(0,1)\) cdf, i.e., \(\Phi(x)\equiv\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-u^{2}/2}du\), \(x\in\mathbb{R}\). 2. \(F(x)=\frac{1}{\pi}\int_{-\infty}^{x}\frac{1}{1+u^{2}}du\), \(x\in\mathbb{R}\).
3.22 Show that \(C[0,1]\) with the supnorm (i.e., with \(\|f\|=\sup\{|f(x)|:0\leq x\leq 1\}\)) is a Banach space. (**Hint:** To verify completeness, let \(\{f_{n}\}_{n\geq 1}\) be a Cauchy sequence in \(C[0,1]\). Show that for each \(0\leq x\leq 1\), \(\{f_{n}(x)\}_{n\geq 1}\) is a Cauchy sequence in \(\mathbb{R}\). Let \(f(x)=\lim\limits_{n\to\infty}f_{n}(x)\). Now show that \(\sup\{|f_{n}(x)-f(x)|:0\leq s\leq 1\}\leq\lim\limits_{m\to\infty}\|f_{n}-f_{m}\|\). Conclude that \(f_{n}\) converges to \(f\) uniformly on \([0,1]\) and that \(f\in C[0,1]\).)
3.23 Show that the space \((\mathcal{P},\|\cdot\|)\) of all polynomials on \([0,1]\) with the supnorm is a normed linear space that is not complete. (**Hint:** Let \(g(t)=\frac{1}{1-t/2}\) for \(0\leq t\leq 1\). Find a sequence of polynomials \(\{f_{n}\}_{n\geq 1}\) in \(\mathcal{P}\) that converge to \(g\) in supnorm.)
3.24 Show that the function \(f(v)\equiv\|v\|\) from a normed linear space \((V,\|\cdot\|)\) to \(\mathbb{R}_{+}\) is continuous.
3.25 Let \((V,\|\cdot\|)\) be a normed linear space. Let \(S=\{v:\|v\|<1\}\). Show that \(S\) is an open set in \(V\).
3.26 Show that the space \(\mathcal{P}_{k}\) of all polynomials on \([0,1]\) of degree \(\leq k\) is a Banach space under the supnorm, i.e., under \(\|f\|=\sup\{|f(x)|\), \(0\leq s\leq 1\}\). (**Hint:** Let \(p_{n}(x)=\sum_{j=0}^{k}a_{nj}x^{j}\) be a sequence of elements in \(\mathcal{P}_{k}\) that converge in supnorm to some \(f(\cdot)\). Show that \(\{a_{n1}\}_{n\geq 1}\) converges and recursively, \(\{a_{ni}\}_{n\geq 1}\) converges for each \(i\).)
3.27 Let \(\mu\) be a finite measure on [0,1]. Verify that \(T_{\mu}(f)\equiv\int fd\mu\) is a bounded linear functional on \(C[0,1]\) and that \(\|T_{\mu}\|=\mu([0,1])\).

3. \(L^{p}\)-Spaces

Let \((V_{i},\|\cdot\|_{i})\), \(i=1,2\), be two normed linear spaces over \(\mathbb{R}\).

1. Let \(T:V_{1}\to V_{2}\) be a linear operator. Show that if for some \(x_{0}\), \(\|Tx-Tx_{0}\|\to 0\) as \(x\to x_{0}\), then \(T\) is continuous on \(V_{1}\) and hence bounded.
2. Show that if \((V_{2},\|\cdot\|_{2})\) is complete, then \(B(V_{1},V_{2})\equiv\{T\mid T:V_{1}\to V_{2}\), \(T\) linear and bounded} is complete under the operator norm defined in (3.1).

_In the following, \(H\) will denote a real Hilbert space_.

1. Use the Cauchy-Schwarz inequality to show that the function \(f(x,y)=\langle x,y\rangle\) is continuous from \(H\times H\to\mathbb{R}\). 2. (_Parallelogram law_). Show that in an innerproduct space \((V,\langle\cdot,\cdot\rangle)\), for any \(x\), \(y\in V\) \[\|x+y\|^{2}+\|x-y\|^{2}=2(\|x\|^{2}+\|y\|^{2})\] where \(\|x\|^{2}=\langle x,x\rangle\).
2. Let \(\{Q_{n}(x)\}_{n\geq 0}\) be defined on \([0,2\pi]\) by \[Q_{n}(x)=c_{n}\Big{(}\frac{1+\cos x}{2}\Big{)}^{n}\] where \(c_{n}\) is such that \[\frac{1}{2\pi}\int_{0}^{2\pi}Q_{n}(x)dx=1.\] Clearly, \(Q_{n}(\cdot)\geq 0\). 1. Verify that for each \(\delta>0\), \[\sup\{Q_{n}(x):\delta\leq x\leq 2\pi-\delta\}\to 0\quad\mbox{as}\quad n\to\infty.\] 2. Use this to show that if \(f\in C[0,2\pi]\) and if \[P_{n}(t)\equiv\frac{1}{2\pi}\int_{0}^{2\pi}Q_{n}(t-s)f(s)ds,\ \ n\geq 0,\] (4.1) then \(P_{n}\to f\) uniformly on \([0,2\pi]\). 2. Use this to give a proof of the completeness of the class \(C\) of trigonometric functions. 3. Show that if \(f\in L^{1}([0,2\pi])\), then \(P_{n}(\cdot)\) converges to \(f\) in \(L^{1}([0,2\pi])\).

4. Let \(\{\mu_{n}(\cdot)\}_{n\geq 1}\) be a sequence of probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that for each \(\delta>0\), \(\mu_{n}(\{x:|x|\geq\delta\})\to 0\) as \(n\to\infty\). Let \(f:\mathbb{R}\to\mathbb{R}\) be Borel measurable. Let \(f_{n}(x)\equiv\int f(x-y)\mu_{n}(dy)\), \(n\geq 1\). Assuming that \(f_{n}(\cdot)\) is well defined and Borel measurable, show that 1. \(f(\cdot)\) continuous at \(x_{0}\) and bounded \(\Rightarrow f_{n}(x_{0})\to f(x_{0})\). 2. \(f(\cdot)\) uniformly continuous and bounded on \(\mathbb{R}\Rightarrow f_{n}\to f\) uniformly on \(\mathbb{R}\). 3. \(f\in L^{p}(\mathbb{R},\mathcal{B}(\mathbb{R}),m)\), \(0<p<\infty\), \(m(\cdot)=\) Lebesgue measure \(\Rightarrow\int|f_{n}-f|^{p}dm\to 0\). 4. Show that (iii) \(\Rightarrow(c)\).
3.31 (_Gram-Schmidt procedure_). Let \(B\equiv\{b_{n}:n\in\mathbb{N}\}\) be a collection of nonzero vector in \(H\). Set \[e_{1} = \frac{b_{1}}{\|b_{1}\|}\] \[\tilde{e}_{2} = b_{2}-\langle b_{2},e_{1}\rangle e_{1},\] \[e_{2} = \frac{\tilde{e}_{2}}{\|\tilde{e}_{2}\|}\quad\mbox{(provided $\| \tilde{e}_{2}\|\neq 0$), and so on.}\] If \(\|\tilde{e}_{n}\|=0\) for some \(n\in\mathbb{N}\), then delete \(b_{n}\). Let \(E\equiv\{e_{j}:1\leq j<k\}\), \(k\leq\infty\), be the collection of vectors reached this way. 1. Show that \(E\) is an orthonormal system. 2. Let \(H_{B}\) denote the closed linear subspace generated by \(B\), i.e., \[H_{B} \equiv \bigg{\{}x:x\in H,\mbox{ there exists $x_{n}$ of the form }\sum_{j=1}^{n}a_{j}b_{j},\] \[a_{j}\in\mathbb{R},\mbox{ such that $\|x_{n}-x\|\to 0$}\bigg{\}}.\] Show that \(H_{B}\) is a Hilbert space and \(E\) is a basis for \(H_{B}\).
3.32 Let \(H=L^{2}(\mathbb{R},B(\mathbb{R}),\mu)\), where \(\mu\) is a probability measure. Let \(B\equiv\{1,x,x^{2},\ldots\}\). Assume that \(\int|x|^{k}d\mu<\infty\) for all \(k\in\mathbb{N}\). Apply the Gram-Schmidt procedure in Problem 3.31 to the set \(B\) for the following cases and evaluate \(e_{1},e_{2},e_{3}\). 1. \(\mu=\) Uniform \([0,1]\) distribution. 2. \(\mu=\) standard normal distribution. 3. \(\mu=\) Exponential (1) distribution. The orthonormal basis \(E\) obtained this way is called _Orthogonal Polynomials_ w.r.t. the _given measure_. (See Szego (1939).)3. \(L^{p}\)-Spaces
3.33 Let \(B\subset H\) be an orthonormal system. Show that for any \(x\) in \(H\), \(\{b:\langle x,b\rangle\neq 0\}\) is at most countable. (**Hint:** Show first that if \(\{y_{\alpha}:\alpha\in I\}\) is a collection of nonnegative real numbers such that for some \(C<\infty\), \(\sum_{\alpha\in F}y_{\alpha}\leq C\) for all \(F\subset I\), \(F\) finite, then \(\{\alpha:y_{\alpha}>0\}\) is at most countable and apply this to the Bessel inequality.)
3.34 Let \(B\subset H\). Define \(B^{\perp}\equiv\{x:x\in H,\ \langle x,b\rangle=0,\ \mbox{for all}\ b\in B\}\). Show that \(B^{\perp}\) is a closed subspace of \(H\).
3.35 Let \(B\subset H\) be a closed subspace of \(H\). 1. Using the fact that every Hilbert space admits an orthonormal basis, show that every \(x\) in \(H\) can be uniquely decomposed as \[x=y+z\] (4.2) where \(y\in B\) and \(z\in B^{\perp}\) and \(\|x\|^{2}=\|y\|^{2}+\|z\|^{2}\). 2. Let \(P_{B}:H\to B\) be defined by \(P_{B}x=y\) where \(x\) admits the decomposition in (4.2) above. Verify that \(P_{B}\) is a bounded linear operator from \(H\) to \(B\) and is of norm \(1\) if \(B\) has at least one nonzero vector. (The operator \(P_{B}\) is called the _projection_ onto \(B\).) 3. Verify that \(P_{B}(P_{B}x)=P_{B}x\) for all \(x\) in \(H\).
3.36 Let \(H\) be separable and \(\{x_{n}\}_{n\geq 1}\subset H\) be such that \(\{\|x_{n}\|_{n\geq 1}\}\) is bounded by some \(C<\infty\). Show that there exist a subsequence \(\{x_{n_{j}}\}_{j\geq 1}\subset\{x_{n}\}_{n\geq 1}\) and an \(x_{0}\) in \(H\), such that for every \(y\) in \(H\), \[\langle x_{n_{j}},y\rangle\rightarrow\langle x_{0},y\rangle.\] (**Hint:** Fix an orthonormal basis \(B\equiv\{b_{n}\}_{n\geq 1}\subset H\). Let \(a_{ni}=\langle x_{n},b_{i}\rangle\), \(n\geq 1\), \(i\geq 1\). Using \(\sum_{i=1}^{\infty}a_{ni}^{2}\leq C\) for all \(n\) and the Bolzano-Wierstrass property, show that 1. there exists \(\{n_{j}\}_{j\geq 1}\) such that \(\lim_{j\rightarrow\infty}a_{n_{j}i}=a_{i}\) exists for all \(i\geq 1\), \(\sum_{i=1}^{\infty}a_{i}^{2}<\infty\), 2. \(\lim_{n\rightarrow\infty}\sum_{i=1}^{n}a_{i}b_{i}\equiv x_{0}\) exists in \(H\), 3. \(\langle x_{n_{j}},y\rangle\rightarrow\langle x_{0},y\rangle\) for all \(y\) in \(H\).)
3.37 Let \((V,\langle\cdot,\cdot\rangle)\) be an innerproduct space. Verify that \(\langle\cdot,\cdot\rangle\) is _bilinear_, i.e., for \(\alpha_{1}\), \(\alpha_{2}\), \(\beta_{1}\), \(\beta_{2}\in\mathbb{R}\), \(x_{1}\), \(x_{2}\), \(y_{1}\), \(y_{2}\) in \(V\), \[\langle\alpha_{1}x_{1}+\alpha_{2}x_{2},\beta_{1}y_{1}+\beta_{2}y_{2}\rangle = \alpha_{1}\beta_{1}\langle x_{1},y_{1}\rangle+\alpha_{1}\beta_{2} \langle x_{1},y_{2}\rangle\] \[+\alpha_{2}\beta_{1}\langle x_{2},y_{1}\rangle+\alpha_{2}\beta_{ 2}\langle x_{2},y_{2}\rangle.\] State and prove an extension to more than two vectors.

3.38 Let \((\Omega,{\cal F},\mu)\) be a measure space. Suppose that there exists an at most countable family \({\cal A}\equiv\{A_{j}\}_{j\geq 1}\subset{\cal F}\) such that \({\cal F}=\sigma\langle{\cal A}\rangle\) and \(\mu(A_{j})>0\) for each \(j\geq 1\). Then show that for \(0<p<\infty\), \(L^{p}(\Omega,{\cal F},\mu)\) is separable. (**Hint:** Show first that for any \(A\in{\cal F}\) with \(\mu(A)<\infty\), and \(\epsilon>0\), there exists a countable subcollection \({\cal A}_{1}\) of \({\cal A}\) such that \(\mu(A\triangle B)<\epsilon\) where \(B=\{\cup A_{j}:A_{j}\in{\cal A}_{1}\}\). Now consider the class of functions \(\{\sum_{i=1}^{n}c_{i}I_{A_{i}}\), \(n\geq 1\), \(A_{i}\in{\cal A}\), \(c_{i}\in{\mathbb{Q}}\}\).)
3.39 Show that \(B\) in Example 3.3.1 (c) is a basis for \(H\). (**Hint:** Using Theorem 2.3.14 prove that the set of all polynomials are dense in H.)
3.40 (_Haar functions_). For \(x\) in \({\mathbb{R}}\) let \(h(x)=I_{[0,1/2)}(x)-I_{[1/2,1)}(x)\). Let \(h_{00}(x)\equiv I_{[0,1)}(x)\) and for \(k\geq 1\), \(0\leq j<2^{k-1}\), let \(h_{kj}(x)\equiv 2^{\frac{k-1}{2}}h(2^{k-1}x-j)\), \(0\leq x<1\). 1. 1. Verify that the family \(\{h_{kj}(\cdot),\ k\geq 0,\ 0\leq j<2^{k-1}\}\) is an orthonormal set in \(L^{2}([0,1],{\cal B}([0,1]),m)\), where \(m(\cdot)\) is Lebesgue measure. 2. Verify that this family is complete by completing the following two proofs: 1. Show that for indicator function \(f\) of dyadic interval of the form \(\big{[}\frac{k}{2^{n}},\frac{\ell}{2^{n}}\big{)}\), \(k<\ell\), the following identity holds: \[\int f^{2}dm=\frac{\ell-k}{2^{n}}=\sum_{k,j}\Big{(}\int fh_{kj}dm\Big{)}^{2}.\] Now use the fact the linear combinations of such \(f\)'s is dense in \(L^{2}[0,1]\). 2. For each \(f\in L^{2}([0,1],{\cal B}([0,1]),m)\) such that \(f\) is orthogonal to the Haar functions, \(F(t)\equiv\int_{[0,t]}fdm\), \(0\leq t\leq 1\) is continuous and satisfies \(F(\frac{j}{2^{n}})=0\) for all \(0\leq j\leq 2^{n}\), \(n=1,2,\ldots\) and hence \(F\equiv 0\) implying \(f=0\) a.e.
3.41 Let \(H\) be a Hilbert space over \({\mathbb{R}}\) and \(M\) be a closed subspace of \(H\). Let \(v_{0}\in H\). Show that \[\min\{\|v-v_{0}\|:v\in M\}=\max\{\langle v_{0},u\rangle,u\in M^{\perp},\ \|u\|=1\},\] where \(M^{\perp}\) is the orthogonal complement of \(M\), i.e., \(M^{\perp}\equiv\{u:\langle v,u\rangle=0\) for all \(v\in M\}\). (**Hint:** Use Problem 3.35 (a).)
3. \(L^{p}\)-Spaces

Let \(B\) be an orthonormal set in a Hilbert space \(H\).

1. 1. Show that for any \(x\) in \(H\) and any finite set \(\{b_{i}:1\leq i\leq k\}\subset B\), \(k<\infty\), \[\sum_{i=1}^{k}\langle x,b_{i}\rangle^{2}\leq\|x\|^{2}.\] 2. Conclude that for all \(x\) in \(H\), \(B_{x}\equiv\{b:\langle x,b\rangle\neq 0,\ b\in B\}\) is at most countable. 2. Show that the following are equivalent: 1. \(B\) is complete, i.e., \(x\in H\), \(\langle x,b\rangle=0\) for all \(b\in B\Rightarrow x=0\). 2. For all \(x\in B\), there exists an at most countable set \(B_{x}\equiv\{b_{i}:i\geq 1\}\) such that \(\|x\|^{2}=\sum_{i=1}^{\infty}\langle x,b_{i}\rangle^{2}\). 3. For all \(x\in B\), \(\epsilon>0\), there exists a finite set \(\{b_{1},b_{2},\ldots,b_{k}\}\subset B\) such that \[\left\|x-\sum_{i=1}^{k}\langle x,b_{i}\rangle b_{i}\right\|<\epsilon.\] 4. If \(B\subset B^{1}\), \(B^{1}\) an orthonormal set in \(H\Rightarrow B=B^{1}\).
3. Extend Theorem 3.3.3 to any Hilbert space assuming that the axiom of choice holds. (**Hint:** Using the axiom of choice or its equivalent, the Hausdorff maximality principle, it can be shown that every Hilbert space \(H\) admits an orthonormal basis \(B\) (see Rudin (1987)). Now let \(T\) be a bounded linear functional from \(H\) to \(\mathbb{R}\). Let \(f(b)\equiv T(b)\) for \(b\) in \(B\). Verify that \(\sum_{i=1}^{k}|f(b_{i})|^{2}\leq\|T\|^{2}\) for all finite collection \(\{b_{i}:1\leq i\leq k\}\subset B\). Conclude that \(D\equiv\{b:f(b)\neq 0\}\) is countable. Let \(x_{0}\equiv\sum_{b\in D}f(b)b\). Now use the proof of Theorem 3.3.3 to show that \(T(x)\equiv\langle x,x_{0}\rangle\) for all \(x\) in \(H\).)
3. Let \((V,\|\cdot\|)\) be a normed linear space. Let \(\{T_{n}\}_{n\geq 1}\) and \(T\) be bounded linear operators from \(V\) to \(V\). The sequence \(\{T_{n}\}_{n\geq 1}\) is said to converge 1. _weakly_ to \(T\) if for each \(w\) in \(V^{*}\), the dual of \(V\), and each \(v\) in \(V\), \[w(T_{n}(v))\to w(T(v)),\] 2. _strongly_ if for each \(v\) in \(V\), \(\|T_{n}v-Tv\|\to 0\), 3. _uniformly_ if \(\sup\{\|T_{n}v-Tv\|:\|v\|\leq 1\}\to 0\).

Let \(V_{p}=L^{p}(\mathbb{R},\mathcal{B}(\mathbb{R}),\mu_{L})\), \(1\leq p\leq\infty\). Let \(\{h_{n}\}_{n\geq 1}\subset\mathbb{R}\) be such that \(h_{n}\neq 0\), \(h_{n}\to 0\), as \(n\to\infty\). Let \((T_{n}f)(\cdot)\equiv f(\cdot+h_{n})\), \(Tf(\cdot)\equiv f(\cdot)\). Verify that

1. \(\{T_{n}\}_{n\geq 1}\) and \(T\) are bounded linear operators on \(V_{p}\), \(1\leq p\leq\infty\),
2. for \(1\leq p<\infty\), \(\{T_{n}\}_{n\geq 1}\) converges to \(T\) weakly,
3. for \(1\leq p<\infty\), \(\{T_{n}\}\) converges to \(T\) strongly,
4. for \(1\leq p<\infty\), \(\{T_{n}\}\) does not converge to \(T\) uniformly by showing that for all \(n\), \(\|T_{n}-T\|=1\),
5. for \(p=\infty\), show that \(T_{n}\) does not converge weakly to \(T\).

Differentiation

### 4.1 The Lebesgue-Radon-Nikodym theorem

**Definition 4.1.1:** Let \((\Omega,\mathcal{F})\) be a measurable space and let \(\mu\) and \(\nu\) be two measures on \((\Omega,\mathcal{F})\). The measure \(\mu\) is said to be _dominated by \(\nu\)_ or _absolutely continuous w.r.t. \(\nu\)_ and written as \(\mu\ll\nu\) if

\[\nu(A)=0\Rightarrow\mu(A)=0\quad\text{for all}\quad A\in\mathcal{F}. \tag{1.1}\]

**Example 4.1.1:** Let \(m\) be the Legesgue measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) and let \(\mu\) be the standard normal distribution, i.e.,

\[\mu(A)\equiv\int_{A}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}m(dx),\quad A\in\mathcal{ B}(\mathbb{R}).\]

Then \(m(A)=0\Rightarrow\mu(A)=0\) and hence \(\mu\ll m\).

**Example 4.1.2:** Let \(\mathbb{Z}_{+}\equiv\{0,1,2,\ldots\}\) denote the set of all nonnegative integers. Let \(\nu\) be the counting measure on \(\Omega=\mathbb{Z}_{+}\) and \(\mu\) be the Poisson \((\lambda)\) distribution for \(0<\lambda<\infty\), i.e.,

\[\nu(A)=\text{number of elements in }A\]

and

\[\mu(A)=\sum_{j\in A}\frac{e^{-\lambda}\lambda^{j}}{j!}\]

for all \(A\in\mathcal{P}(\Omega)\), the power set of \(\Omega\). Since \(\nu(A)=0\Leftrightarrow A=\emptyset\Leftrightarrow\mu(A)=0\), it follows that \(\mu\ll\nu\) and \(\nu\ll\mu\).

**Example 4.1.3:** Let \(f\) be a nonnegative measurable function on a measure space \((\Omega,\mathcal{F},\nu)\). Let

\[\mu(A)\equiv\int_{A}fd\nu\quad\text{for all}\quad A\in\mathcal{F}. \tag{1.2}\]

Then, \(\mu\) is a measure on \((\Omega,\mathcal{F})\) and \(\nu(A)=0\Rightarrow\mu(A)=0\) for all \(A\in\mathcal{F}\) and hence \(\mu\ll\nu\).

The Radon-Nikodym theorem is a sort of converse to Example 4.1.3. It says that if \(\mu\) and \(\nu\) are \(\sigma\)-finite measures (see Section 1.2) on a measurable space \((\Omega,\mathcal{F})\) and if \(\mu\ll\nu\), then there is a nonnegative measurable function \(f\) on \((\Omega,\mathcal{F})\) such that (1.2) holds.

**Definition 4.1.2:** Let \((\Omega,\mathcal{F})\) be a measurable space and let \(\mu\) and \(\nu\) be two measures on \((\Omega,\mathcal{F})\). Then, \(\mu\) is called _singular_ w.r.t. \(\nu\) and written as \(\mu\perp\nu\) if there exists a set \(B\in\mathcal{F}\) such that

\[\mu(B)=0\quad\text{and}\quad\nu(B^{c})=0. \tag{1.3}\]

Note that \(\mu\) is singular w.r.t. \(\nu\) implies that \(\nu\) is singular w.r.t. \(\mu\). Thus, the notion of singularity between two measures \(\mu\) and \(\nu\) is symmetric but that of absolutely continuity is not. Note also that if \(\mu\) and \(\nu\) are mutually singular and \(B\) satisfies (1.3), then for all \(A\in\mathcal{F}\),

\[\mu(A)=\mu(A\cap B^{c})\quad\text{and}\quad\nu(A)=\nu(A\cap B). \tag{1.4}\]

**Example 4.1.4:** Let \(\mu\) be the Lebesgue measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) and \(\nu\) be defined as \(\nu(A)=\#\) elements in \(A\cap\mathbb{Z}\) where \(\mathbb{Z}\) is the set of integers. Then

\[\nu(\mathbb{Z}^{c})=0\quad\text{and}\quad\mu(\mathbb{Z})=0\]

and hence (1.3) holds with \(B=\mathbb{Z}\). Thus \(\mu\) and \(\nu\) are _mutually singular_.

Another example is the pair \(m\) and \(\mu_{c}\) on [0,1] where \(\mu_{c}\) is the Lebesgue-Stieltjes measure generated by the Cantor function (cf. Section 4.5) and \(m\) is the Lebesgue measure.

**Example 4.1.5:** Let \(\mu\) be the Lebesgue measure restricted to \((-\infty,0]\) and \(\nu\) be the Exponential(1) distribution. That is, for any \(A\in\mathcal{B}(\mathbb{R})\),

\[\mu(A) = \text{the Lebesgue measure of}\quad A\cap(-\infty,0];\] \[\nu(A) = \int_{A\cap(0,\infty)}e^{-x}dx.\]

Then, \(\mu((0,\infty))=0\) and \(\nu((-\infty,0])=0\), and (1.3) holds with \(B=(-\infty,0]\).

Suppose that \(\mu\) and \(\nu\) are two finite measures on a measurable space \((\Omega,\mathcal{F})\). H. Lebesgue showed that \(\mu_{1}\) can be decomposed as a sum of two measures, i.e.,

\[\mu=\mu_{a}+\mu_{s}\]

where \(\mu_{a}\ll\nu\) and \(\mu_{s}\perp\nu\). The next theorem is the main result of this section and it combines the above decomposition result of Lebesgue with the Radon-Nikodym theorem mentioned earlier.

**Theorem 4.1.1:**_Let \((\Omega,\mathcal{F})\) be a measurable space and let \(\mu_{1}\) and \(\mu_{2}\) be two \(\sigma\)-finite measures on \((\Omega,\mathcal{F})\)._

1. \((\)_The Lebesgue decomposition theorem\()\). The measure_ \(\mu_{1}\) _can be uniquely decomposed as_ \[\mu_{1}=\mu_{1a}+\mu_{1s}\] (1.5) _where_ \(\mu_{1a}\) _and_ \(\mu_{1s}\) _are_ \(\sigma\)_-finite measures on_ \((\Omega,\mathcal{F})\) _such that_ \(\mu_{1a}\ll\mu_{2}\) _and_ \(\mu_{1s}\perp\mu_{2}\)_._
2. \((\)_The Radon-Nikodym theorem\()\). There exists a nonnegative measurable function_ \(h\) _on_ \((\Omega,\mathcal{F})\) _such that_ \[\mu_{1a}(A)=\int_{A}hd\mu_{2}\quad\mbox{for all}\quad A\in\mathcal{F}.\] (1.6)

**Proof:** Case 1: Suppose that \(\mu_{1}\) and \(\mu_{2}\) are finite measures. Let \(\mu\) be the measure \(\mu=\mu_{1}+\mu_{2}\) and let \(H=L^{2}(\mu)\). Define a linear function \(T\) on \(H\) by

\[T(f)=\int fd\mu_{1}. \tag{1.7}\]

Then, by the Cauchy-Schwarz inequality applied to the functions \(f\) and \(g\equiv 1\),

\[|T(f)| \leq \left(\int f^{2}d\mu_{1}\right)^{\frac{1}{2}}\left(\mu_{1}(\Omega )\right)^{\frac{1}{2}}\] \[\leq \left(\int f^{2}d\mu\right)^{\frac{1}{2}}\left(\mu_{1}(\Omega) \right)^{\frac{1}{2}}.\]

This shows that \(T\) is a bounded linear functional on \(H\) with \(\|T\|\leq M\equiv(\mu_{1}(\Omega))^{\frac{1}{2}}\). By the Riesz representation theorem (cf. Theorem 3.3.3 and Remark 3.3.2), there exists a \(g\in L^{2}(\mu)\) such that

\[T(f)=\int fgd\mu \tag{1.8}\]for all \(f\in L^{2}(\mu)\). Let \(f=I_{A}\) for \(A\) in \(\Cal{F}\). Then, (1.7) and (1.8) yield

\[\mu_{1}(A)=T(I_{A})=\int_{A}gd\mu.\]

But \(0\leq\mu_{1}(A)\leq\mu(A)\) for all \(A\in\Cal{F}\). Hence the function \(g\) in \(L^{2}(\mu)\) satisfies

\[0\leq\int_{A}gd\mu\leq\mu(A)\quad\text{for all}\quad A\in\Cal{F}.\]

Let \(A_{1}=\{0\leq g<1\},\ A_{2}=\{g=1\},\ A_{3}=\{g\not\in[0,1]\}\). Then (1.9) implies that \(\mu(A_{3})=0\) (see Problem 4.1). Now define the measures \(\mu_{1a}(\cdot)\) and \(\mu_{1s}(\cdot)\) by

\[\mu_{1a}(A)\equiv\mu_{1}(A\cap A_{1}),\quad\mu_{1s}(A)\equiv\mu_{1}(A\cap A_{2 }),\ \ A\in\Cal{F}.\]

Next it will be shown that \(\mu_{1a}\ll\mu_{2}\) and \(\mu_{1s}\perp\mu_{2}\), thus establishing (1.5). By (1.7) and (1.8), for all \(f\in H\),

\[\int fd\mu_{1}=\int fgd\mu=\int fgd\mu_{1}+\int fgd\mu_{2}\]

\[\Rightarrow\int f(1-g)d\mu_{1}=\int fgd\mu_{2}.\]

Setting \(f=I_{A_{2}}\) yields

\[0=\mu_{2}(A_{2}).\]

From (1.10), since \(\mu_{1s}(A_{2}^{c})=0\), it follows that \(\mu_{1s}\perp\mu_{2}\). Now fix \(n\geq 1\) and \(A\in\Cal{F}\). Let \(f=I_{A\cap A_{1}}(1+g+\ldots+g^{n-1})\). Then (1.11) implies that

\[\int_{A\cap A_{1}}(1-g^{n})d\mu_{1}=\int_{A\cap A_{1}}g(1+g+\ldots+g^{n-1})d \mu_{2}.\]

Now letting \(n\to\infty\), and using the MCT on both sides, yields

\[\mu_{1a}(A)=\int_{A}I_{A_{1}}\frac{g}{(1-g)}d\mu_{2}.\]

Setting \(h\equiv\frac{g}{1-g}I_{A_{1}}\) completes the proof of (1.5) and (1.6).

Case 2: Now suppose that \(\mu_{1}\) and \(\mu_{2}\) are \(\sigma\)-finite. Then there exists a countable partition \(\{D_{n}\}_{\geq 1}\subset\Cal{F}\) of \(\Omega\) such that \(\mu_{1}(D_{n})\) and \(\mu_{2}(D_{n})\) are both finite for all \(n\geq 1\). Let \(\mu_{1}^{(n)}(\cdot)\equiv\mu_{1}(\cdot\cap D_{n})\) and \(\mu_{2}^{(n)}\equiv\mu_{2}(\cdot\cap D_{n})\). Then applying 'Case 1' to \(\mu_{1}^{(n)}\) and \(\mu_{2}^{(n)}\) for each \(n\geq 1\), one gets measures \(\mu_{1a}^{(n)},\mu_{1s}^{(n)}\) and a function \(h_{n}\) such that

\[\mu_{1}^{(n)}(\cdot)\equiv\mu_{1a}^{(n)}(\cdot)+\mu_{1s}^{(n)}(\cdot)\]where, for \(A\) in \({\cal F}\), \(\mu_{1a}^{(n)}(A)=\int_{A}h_{n}d\mu_{2}^{(n)}=\int_{A}h_{n}I_{D_{n}}d\mu_{2}\) and \(\mu_{1s}^{(n)}(\cdot)\perp\mu_{2}^{(n)}\). Since \(\mu_{1}(\cdot)=\sum_{n=1}^{\infty}\mu_{1}^{(n)}(\cdot)\), it follows from (1.13) that

\[\mu_{1}(\cdot)=\mu_{1a}(\cdot)+\mu_{1s}(\cdot), \tag{1.14}\]

where \(\mu_{1a}(A)\equiv\sum_{n=1}^{\infty}\mu_{1a}^{(n)}(A)\) and \(\mu_{1s}(\cdot)=\sum_{n=1}^{\infty}\mu_{1s}^{(n)}(\cdot)\). By the MCT,

\[\mu_{1a}(A)=\int_{A}hd\mu_{2},\quad A\in{\cal F},\]

where \(h\equiv\sum_{n=1}^{\infty}h_{n}I_{D_{n}}\).

Clearly, \(\mu_{1a}\ll\mu_{2}\). The verification of the singularity of \(\mu_{1s}\) and \(\mu_{2}\) is left as an exercise (Problem 4.2).

It remains to prove the uniqueness of the decomposition. Let \(\mu_{1}=\mu_{a}+\mu_{s}\) and \(\mu_{1}=\mu_{a}^{\prime}+\mu_{s}^{\prime}\) be two decompositions of \(\mu_{1}\) where \(\mu_{a}\) and \(\mu_{a}^{\prime}\) are absolutely continuous w.r.t. \(\mu_{2}\) and \(\mu_{s}\) and \(\mu_{s}^{\prime}\) are singular w.r.t. \(\mu_{2}\). By definition, there exist sets \(B\) and \(B^{\prime}\) in \({\cal F}\) such that

\[\mu_{2}(B)=0,\mu_{2}(B^{\prime})=0,\quad\mbox{and}\quad\mu_{s}(B^{c})=0,\mu_{s }^{\prime}(B^{\prime c})=0.\]

Let \(D=B\cup B^{\prime}\). Then \(\mu_{2}(D)=0\) and \(\mu_{s}(D^{c})\leq\mu_{s}(B^{c})=0\). Similarly, \(\mu_{s}^{\prime}(D^{c})\leq\mu_{s}^{\prime}(B^{\prime c})=0\). Also \(\mu_{2}(D)=0\) implies \(\mu_{a}(D)=0=\mu_{a}^{\prime}(D)\). Thus for any \(A\in{\cal F}\),

\[\mu_{a}(A)=\mu_{a}(A\cap D^{c})\quad\mbox{and}\quad\mu_{a}^{\prime}(A)=\mu_{a} ^{\prime}(A\cap D^{c}).\]

Also

\[\mu_{s}(A\cap D^{c}) \leq \mu_{s}(A\cap B^{c})=0\] \[\mu_{s}^{\prime}(A\cap D^{c}) \leq \mu_{s}^{\prime}(A\cap B^{\prime c})=0.\]

Thus, \(\mu(A\cap D^{c})=\mu_{a}(A\cap D^{c})+\mu_{s}(A\cap D^{c})=\mu_{a}(A\cap D^{c} )=\mu_{a}(A)\) and \(\mu(A\cap D^{c})=\mu_{a}^{\prime}(A\cap D^{c})+\mu_{s}^{\prime}(A\cap D^{c})= \mu_{a}^{\prime}(A\cap D^{c})=\mu_{a}^{\prime}(A)\). Hence, \(\mu_{a}(A)=\mu(A\cap D^{c})=\mu_{a}^{\prime}(A)\) for every \(A\in{\cal F}\). That is, \(\mu_{a}=\mu_{a}^{\prime}\) and hence, \(\mu_{s}=\mu_{s}^{\prime}\). \(\Box\)

**Remark 4.1.1:** In Theorem 4.1.1, the hypothesis of \(\sigma\) finiteness cannot be dropped. For example, let \(\mu\) be the Lebesgue measure and \(\nu\) be the counting measure on \([0,1]\). Then \(\mu\ll\nu\) but there does not exist a nonnegative \({\cal F}\)-measurable function \(h\) such that \(\mu(A)=\int_{A}hd\nu\). To see this, if possible, suppose that for some \(h\in L^{1}(\nu)\), \(\mu(A)=\int_{A}hd\nu\) for all \(A\in{\cal F}\). Note that \(\mu([0,1])=1\) implies that \(\int_{[0,1]}hd\nu<\infty\) and hence, that \(B\equiv\{x:h(x)>0\}\) is countable (Problem 4.3). But \(\mu\) being the Lebesgue measure, \(\mu(B)=0\) and \(\mu(B^{c})=1\). Since by definition, \(h\equiv 0\) on \(B^{c}\), this implies \(1=\mu(B^{c})=\int_{B^{c}}hd\nu=0\), leading to a contradiction. However, if \(\nu\) is \(\sigma\)-finite and \(\mu\ll\nu\) (\(\mu\) not necessarily \(\sigma\)-finite), then the Radon-Nikodym theorem holds, i.e., _there exists a nonnegative \({\cal F}\)-measurable function \(h\) such that_

\[\mu(A)=\int_{A}hd\nu\quad\mbox{for all}\quad A\in{\cal F}.\]For a proof, see Royden (1988), Chapter 11.

**Definition 4.1.3:** Let \(\mu\) and \(\nu\) be measures on a measurable space \((\Omega,\mathcal{F})\) and let \(h\) be a nonnegative measurable function such that

\[\mu(A)=\int_{A}hd\nu\quad\text{for all}\quad A\in\mathcal{F}.\]

Then \(h\) is called the _Radon-Nikodym derivative_ of \(\mu\) w.r.t. \(\nu\) and is written as

\[\frac{d\mu}{d\nu}=h.\]

If \(\mu(\Omega)<\infty\) and there exist two nonnegative \(\mathcal{F}\)-measurable functions \(h_{1}\) and \(h_{2}\) such that

\[\mu(A)=\int_{A}h_{1}d\nu=\int_{A}h_{2}d\nu\]

for all \(A\in\mathcal{F}\), then \(h_{1}=h_{2}\) a.e. \((\nu)\) and thus the Radon-Nikodym derivative \(\frac{d\mu}{d\nu}\) is unique up to equivalence a.e. \((\nu)\). This also extends to the case when \(\mu\) is \(\sigma\)-finite.

The following proposition is easy to verify (cf. Problem 4.4).

**Proposition 4.1.2:** _Let \(\nu,\mu,\mu_{1},\mu_{2},\ldots\) be \(\sigma\)-finite measures on a measurable space \((\Omega,\mathcal{F})\)._

* _If_ \(\mu_{1}\ll\mu_{2}\) _and_ \(\mu_{2}\ll\mu_{3}\)_, then_ \(\mu_{1}\ll\mu_{3}\) _and_ \[\frac{d\mu_{1}}{d\mu_{3}}=\frac{d\mu_{1}}{d\mu_{2}}\frac{d\mu_{2}}{d\mu_{3}} \quad\text{a.e. }(\mu_{3}).\]
* _Suppose that_ \(\mu_{1}\) _and_ \(\mu_{2}\) _are dominated by_ \(\mu_{3}\)_. Then for any_ \(\alpha,\beta\geq 0\)_,_ \(\alpha\mu_{1}+\beta\mu_{2}\) _is dominated by_ \(\mu_{3}\) _and_ \[\frac{d(\alpha\mu_{1}+\beta\mu_{2})}{d\mu_{3}}=\alpha\frac{d\mu_{1}}{d\mu_{3}}+ \beta\frac{d\mu_{2}}{d\mu_{3}}\quad\text{a.e. }(\mu_{3}).\]
* _If_ \(\mu\ll\nu\) _and_ \(\frac{d\mu}{d\nu}>0\) _a.e._ \((\nu)\)_, then_ \(\nu\ll\mu\) _and_ \[\frac{d\nu}{d\mu}=\left(\frac{d\mu}{d\nu}\right)^{-1}\quad\text{a.e. }(\mu).\]
* _Let_ \(\{\mu_{n}\}_{n\geq 1}\) _be a sequence of measures and_ \(\{\alpha_{n}\}_{n\geq 1}\) _be a sequence of positive real numbers, i.e.,_ \(\alpha_{n}>0\) _for all_ \(n\geq 1\)_. Define_ \(\mu=\sum_{n=1}^{\infty}\alpha_{n}\mu_{n}\)1. _Then,_ \(\mu\ll\nu\) _iff_ \(\mu_{n}\ll\nu\) _for each_ \(n\geq 1\) _and in this case,_ \[\frac{d\mu}{d\nu}=\sum_{n=1}^{\infty}\alpha_{n}\frac{d\mu_{n}}{d\nu}\quad\mbox{ a.e. }(\nu).\] 2. \(\mu\perp\nu\) _iff_ \(\mu_{n}\perp\nu\) _for all_ \(n\geq 1\)_._

### 4.2 Signed measures

Let \(\mu_{1}\) and \(\mu_{2}\) be two finite measures on a measurable space \((\Omega,\mathcal{F})\). Let

\[\nu(A)\equiv\mu_{1}(A)-\mu_{2}(A),\quad\mbox{ for all }\quad A\in\mathcal{F}. \tag{2.1}\]

Then \(\nu:F\to\mathbb{R}\) satisfies the following:

1. \(\nu(\emptyset)=0\).
2. For any \(\{A_{n}\}_{n\geq 1}\subset\mathcal{F}\) with \(A_{i}\cap A_{j}=\emptyset\) for \(i\neq j\), and with \(\sum_{i=1}^{\infty}|\nu(A_{i})|<\infty\), \[\nu(A)=\sum_{i=1}^{\infty}\nu(A_{i}).\] (2.2)
3. Let \[\|\nu\| \equiv \sup\bigg{\{}\sum_{i=1}^{\infty}|\nu(A_{i})|:\{A_{n}\}_{n\geq 1} \subset\mathcal{F},\ A_{i}\cap A_{j}=\emptyset\quad\mbox{for}\] (2.3) \[i\neq j,\ \bigcup_{n\geq 1}A_{n}=\Omega\bigg{\}}.\] Then, \(\|\nu\|\) is finite.

Note that (iii) holds because \(\|\nu\|\leq\mu_{1}(\Omega)+\mu_{2}(\Omega)<\infty\).

**Definition 4.2.1:** A set function \(\nu:\mathcal{F}\to\mathbb{R}\) satisfying (i), (ii), and (iii) above is called a _finite signed measure_.

The above example shows that the difference of two finite measures is a finite signed measure. It will be shown below that every finite signed measure can be expressed as the difference of two finite measures.

**Proposition 4.2.1:**_Let \(\nu\) be a finite signed measure on \((\Omega,\mathcal{F})\). Let_

\[|\nu|(A) \equiv \sup\bigg{\{}\sum_{n=1}^{\infty}|\nu(A_{n})|:\{A_{n}\}_{n\geq 1} \subset\mathcal{F},A_{i}\cap A_{j}=\emptyset\quad\mbox{for}\quad i\neq j, \tag{2.4}\] \[\bigcup_{n\geq 1}A_{n}=A\bigg{\}}.\]

[MISSING_PAGE_FAIL:135]

and with (2.5), this completes the proof. \(\Box\)

**Definition 4.2.2:** The measure \(|\nu|\) defined by (2.4) is called the _total variation measure_ of the signed measure \(\nu\).

Next, define the set functions

\[\nu^{+}\equiv\frac{|\nu|+\nu}{2},\quad\nu^{-}\equiv\frac{|\nu|-\nu}{2}. \tag{2.8}\]

It can be verified that \(\nu^{+}\) and \(\nu^{-}\) are both finite measures on \((\Omega,\mathcal{F})\).

**Definition 4.2.3:** The measures \(\nu^{+}\) and \(\nu^{-}\) are called the _positive_ and _negative variation measures_ of the signed measure \(\nu\), respectively.

It follows from (2.8) that

\[\nu=\nu^{+}-\nu^{-}. \tag{2.9}\]

Thus every finite signed measure \(\nu\) on \((\Omega,\mathcal{F})\) is the difference of two finite measures, as claimed earlier.

Note that both \(\nu^{+}\) and \(\nu^{-}\) are dominated by \(|\nu|\) and all three measures are finite. By the Radon-Nikodym theorem (Theorem 4.1.1), there exist functions \(h_{1}\) and \(h_{2}\) in \(L^{1}(\Omega,\mathcal{F},|\nu|)\) such that

\[\frac{d\nu^{+}}{d|\nu|}=h_{1}\quad\text{and}\quad\frac{d\nu^{-}}{d|\nu|}=h_{2}. \tag{2.10}\]

This and (2.9) imply that for any \(A\) in \(\mathcal{F}\),

\[\nu(A)=\int_{A}h_{1}d|\nu|-\int_{A}h_{2}d|\nu|=\int_{A}hd|\nu|, \tag{2.11}\]

where \(h=h_{1}-h_{2}\). Thus every finite signed measure \(\nu\) on \((\Omega,\mathcal{F})\) can be expressed as

\[\nu(A)=\int_{A}fd\mu,\ A\in\mathcal{F} \tag{2.12}\]

for some finite measure \(\mu\) on \((\Omega,\mathcal{F})\) and some \(f\in L^{1}(\Omega,\mathcal{F},\mu)\).

Conversely, it is easy to verify that a set function \(\nu\) defined by (2.12) for some finite measure \(\mu\) on \((\Omega,\mathcal{F})\) and some \(f\in L^{1}(\Omega,\mathcal{F},\mu)\) is a finite signed measure (cf. Problem 4.6). This leads to the following:

**Theorem 4.2.2:**

1. _A set function_ \(\nu\) _on a measurable space_ \((\Omega,\mathcal{F})\) _is a finite signed measure iff there exist two finite measures_ \(\mu_{1}\) _and_ \(\mu_{2}\) _on_ \((\Omega,\mathcal{F})\) _such that_ \(\nu=\mu_{1}-\mu_{2}\)_._
* _A set function_ \(\nu\) _on a measurable space_ \((\Omega,\mathcal{F})\) _is a finite signed measure iff there exist a finite measure_ \(\mu\) _on_ \((\Omega,\mathcal{F})\) _and an_ \(f\in L^{1}(\Omega,\mathcal{F},\mu)\) _such that for all_ \(A\) _in_ \(\mathcal{F}\)_,_ \[\nu(A)=\int_{A}f\ d\mu.\]

**Definition 4.2.4:** Let \(\nu\) be a finite signed measure on a measurable space on \((\Omega,\mathcal{F})\). A set \(A\in\mathcal{F}\) is called a _positive set_ for \(\nu\) if for any \(B\subset A,B\in\mathcal{F}\), \(\nu(B)\geq 0\). A set \(A\in\mathcal{F}\) is called a _negative set_ for \(\nu\) if for any \(B\subset A\) with \(B\in\mathcal{F}\), \(\nu(B)\leq 0\).

Let \(h\) be as in (2.11). Let

\[\Omega^{+}=\{\omega:h(\omega)\geq 0\}\quad\text{and}\quad\Omega^{-}=\{\omega:h (\omega)<0\}. \tag{2.13}\]

From (2.11), it follows that for all \(A\) in \(\mathcal{F}\), \(\nu(A\cap\Omega^{+})\geq 0\) and \(\nu(A\cap\Omega^{-})\leq 0\). Thus \(\Omega^{+}\) is a positive set and \(\Omega^{-}\) is a negative set for \(\nu\). Furthermore, \(\Omega^{+}\cup\Omega^{-}=\Omega\) and \(\Omega^{+}\cap\Omega^{-}=\emptyset\). Summarizing this discussion, one gets the following theorem.

**Theorem 4.2.3:** (_Hahn decomposition theorem_). _Let \(\nu\) be a finite signed measure on a measurable space \((\Omega,\mathcal{F})\). Then there exist a positive set \(\Omega^{+}\) and a negative set \(\Omega^{-}\) for \(\nu\) such that \(\Omega=\Omega^{+}\cup\Omega^{-}\) and \(\Omega^{+}\cap\Omega^{-}=\emptyset\)._

Let \(\Omega^{+}\) and \(\Omega^{-}\) be as in (2.13). It can be verified (Problem 4.8) that for any \(B\in\mathcal{F}\), if \(B\subset\Omega^{+}\), then \(\nu(B)=|\nu|(B)\). By (2.11), this implies that for all \(A\) in \(\mathcal{F}\),

\[\int_{A\cap\Omega^{+}}hd|\nu|=|\nu|(A\cap\Omega^{+}).\]

It follows that \(h=1\) a.e. \((|\nu|)\) on \(\Omega^{+}\). Similarly, \(h=-1\) a.e. \((|\nu|)\) on \(\Omega^{-}\). Thus, the measures \(\nu^{+}\) and \(\nu^{-}\), defined in (2.8), reduce to

\[\nu^{+}(A) = \int_{A}\frac{(1+h)}{2}d|\nu|\] \[= \int_{A\cap\Omega^{+}}\frac{(1+h)}{2}d|\nu|+\int_{A\cap\Omega^{- }}\frac{(1+h)}{d}|\nu|\] \[= |\nu|(A\cap\Omega^{+}),\]

and similarly

\[\nu^{-}(A)=|\nu|(A\cap\Omega^{-}).\]

Note that \(\nu^{+}\) and \(\nu^{-}\) are both finite measures that are mutually singular. This particular decomposition of \(\nu\) as

\[\nu=\nu^{+}-\nu^{-}\]is known as the _Jordan decomposition_ of \(\nu\). It will now be shown that this decomposition is minimal and that it is unique in the class of signed measures with mutually singular components. Suppose there exist two finite measures \(\mu_{1}\) and \(\mu_{2}\) on \((\Omega,\mathcal{F})\) such that \(\nu=\mu_{1}-\mu_{2}\). For any \(A\in\mathcal{F}\), \(\nu^{+}(A)=\nu(A\cap\Omega^{+})=\mu_{1}(A\cap\Omega^{+})-\mu_{2}(A\cap\Omega^{ +})\leq\mu_{1}(A\cap\Omega^{+})\leq\mu_{1}(A)\) and \(\nu^{-}(A)=-\nu(A\cap\Omega^{-})=\mu_{2}(A\cap\Omega^{-})-\mu_{1}(A\cap\Omega^{ -})\leq\mu_{2}(A\cap\Omega^{-})\leq\mu_{2}(A)\). Thus, \(\nu^{+}\leq\mu_{1}\) and \(\nu^{-}\leq\mu_{2}\). Clearly, since both \(\mu_{1}\) and \(\nu^{+}\) are finite measures on \((\Omega,\mathcal{F}),\ \mu_{1}-\nu^{+}\) is also a finite measure. Similarly, \(\mu_{2}-\nu^{-}\) is also a finite measure. Also, since \(\mu_{1}-\mu_{2}=\nu=\nu^{+}-\nu^{-}\), it follows that \(\mu_{1}-\nu^{+}=\mu_{2}-\nu^{-}=\lambda\), say. Thus, for any decomposition of \(\nu\) as \(\mu_{1}-\mu_{2}\) with \(\mu_{1}\), \(\mu_{2}\) finite measures, it holds that \(\mu_{1}=\nu^{+}+\lambda\) and \(\mu_{2}=\nu^{-}+\lambda\), where \(\lambda\) is a measure on \((\Omega,\mathcal{F})\). Thus, \(\nu=\nu^{+}-\nu^{-}\) is a _minimal_ decomposition in the sense that in this case \(\lambda=0\). Now suppose \(\mu_{1}\) and \(\mu_{2}\) are mutually singular, i.e., there exist \(\Omega_{1},\Omega_{2}\in\mathcal{F}\) such that \(\Omega_{1}\cap\Omega_{2}=\emptyset\), \(\Omega_{1}\cup\Omega_{2}=\Omega\), and \(\mu_{1}(\Omega_{2})=0=\mu_{2}(\Omega_{1})\). Since \(\mu_{1}\geq\lambda\) and \(\mu_{2}\geq\lambda\), it follows that \(\lambda(\Omega_{2})=0=\lambda(\Omega_{1})\). Thus \(\lambda=0\) and \(\mu_{1}=\nu^{+}\) and \(\mu_{2}=\nu^{-}\).

Summarizing the above discussion yields:

**Theorem 4.2.4:** _Let \(\nu\) be a finite signed measure on a measurable space \((\Omega,\mathcal{F})\) and let \(\mu_{1}\) and \(\mu_{2}\) be two finite measures on \((\Omega,\mathcal{F})\) such that \(\nu=\mu_{1}-\mu_{2}\). Then there exists a finite measure \(\lambda\) such that \(\mu_{1}=\nu^{+}+\lambda\) and \(\mu_{2}=\nu^{-}+\lambda\) with \(\lambda=0\) iff \(\mu_{1}\) and \(\mu_{2}\) are mutually singular._

Let

\[\mathbb{S}\equiv\{\nu:\nu\quad\mbox{is a finite signed measure on}\quad(\Omega, \mathcal{F})\}.\]

Also, for any \(\alpha\in\mathbb{R}\), let \(\alpha^{+}=\max(\alpha,0)\) and \(\alpha^{-}=\max(-\alpha,0)\). Note that for \(\nu_{1},\nu_{2}\) in \(\mathbb{S}\) and \(\alpha_{1},\alpha_{2}\in\mathbb{R}\),

\[\alpha_{1}\nu_{1}+\alpha_{2}\nu_{2} = (\alpha_{1}^{+}-\alpha_{1}^{-})(\nu_{1}^{+}-\nu_{1}^{-})+(\alpha_ {2}^{+}-\alpha_{2}^{-})(\nu_{2}^{+}-\nu_{2}^{-})\] \[= (\alpha_{1}^{+}\nu_{1}^{+}+\alpha_{1}^{-}\nu_{1}^{-}+\alpha_{2}^ {+}\nu_{2}^{+}+\alpha_{2}^{-}\nu_{2}^{-})\] \[-\,(\alpha_{1}^{+}\nu_{1}^{-}+\alpha_{1}^{-}\nu_{1}^{+}+\alpha_{2 }^{+}\nu_{2}^{-}+\alpha_{2}^{-}\nu_{2}^{+})\] \[= \lambda_{1}-\lambda_{2},\quad\mbox{say,}\]

where \(\lambda_{1}\) and \(\lambda_{2}\) are both finite measures. It now follows from Theorem 4.2.2 that \(\alpha_{1}\nu_{1}+\alpha_{2}\nu_{2}\in\mathbb{S}\). Thus, \(\mathbb{S}\) is a vector space over \(\mathbb{R}\).

Now it will be shown that \(\|\nu\|\equiv|\nu|(\Omega)\) is a norm on \(\mathbb{S}\) and that \((\mathbb{S},\|\cdot\|)\) is a Banach space.

**Definition 4.2.5:** For a finite signed measure \(\nu\) on a measurable space \((\Omega,\mathcal{F})\), the _total variation norm_\(\nu\) is defined by \(\|\nu\|\equiv|\nu|(\Omega)\).

**Proposition 4.2.5:** _Let \(\mathbb{S}\equiv\{\nu:\nu\) a finite signed measure on \((\Omega,\mathcal{F})\}\). Then, \(\|\nu\|\equiv|\nu|(\Omega)\), \(\nu\in\mathbb{S}\) is a norm on \(\mathbb{S}\)._

[MISSING_PAGE_FAIL:139]

\(\nu(A)=\sum_{i=1}^{\infty}\nu(A_{i})\). Also for any countable partition \(\{A_{i}\}_{i\geq 1}\subset{\cal F}\) of \(\Omega\),

\[\sum_{i=1}^{\infty}|\nu(A_{i})|=\lim_{n\to\infty}\sum_{i=1}^{\infty}|\nu_{n}(A_{ i})|\leq\lim_{n\to\infty}\|\nu_{n}\|<\infty.\]

Thus \(|\nu|(\Omega)<\infty\) and hence, \(\nu\in\mathbb{S}\). Finally,

\[\|\nu_{n}-\nu\| = \sup\bigg{\{}\sum_{i=1}^{\infty}|\nu_{n}(A_{i})-\nu(A_{i})|:\{A_{ i}\}_{i\geq 1}\subset{\cal F}\] \[\mbox{ is a disjoint partition of }\quad\Omega\bigg{\}}.\]

But for every countable partition \(\{A_{i}\}_{i\geq 1}\subset{\cal F}\),

\[\sum_{i=1}^{\infty}|\nu_{n}(A_{i})-\nu(A_{i})|=\lim_{m\to\infty}\sum_{i=1}^{ \infty}|\nu_{n}(A_{i})-\nu_{m}(A_{i})|\leq\lim_{m\to\infty}\|\nu_{n}-\nu_{m}\|.\]

Thus, \(\|\nu_{n}-\nu\|\leq\lim_{m\to\infty}\|\nu_{n}-\nu_{m}\|\) and hence, \(\lim_{n\to\infty}\|\nu_{n}-\nu\|\leq\lim_{n\to\infty}\lim_{m\to\infty}\|\nu_{ n}-\nu_{m}\|=0\). Hence, \(\nu_{n}\to\nu\) in \(\mathbb{S}\). \(\Box\)

**Definition 4.2.6:** (_Integration w.r.t. signed measures_). Let \(\mu\) be a finite signed measure on a measurable space \((\Omega,{\cal F})\) and \(|\mu|\) be its total variation measure as in Definition 4.2.2. Then, for any \(f\in L^{1}(\Omega,{\cal F},|\mu|)\), \(\int fd\mu\) is defined as

\[\int fd\mu=\int fd\mu^{+}-\int fd\mu^{-}\,,\]

where \(\mu^{+}\) and \(\mu^{-}\) are the positive and negative variations of \(\mu\) as defined in (2.8).

**Proposition 4.2.7:**_Let \(\mu\) be a signed measure on a measurable space \((\Omega,{\cal F},P)\). Let \(\mu=\lambda_{1}-\lambda_{2}\) where \(\lambda_{1}\) and \(\lambda_{2}\) are finite measures. Let \(f\in L^{1}(\Omega,{\cal F},\lambda_{1}+\lambda_{2})\). Then \(f\in L^{1}(\Omega,{\cal F},|\mu|)\) and_

\[\int fd\mu=\int fd\lambda_{1}-\int fd\lambda_{2}. \tag{2.14}\]

**Proof:** Left as an exercise (Problem 4.13).

### 4.3 Functions of bounded variation

From the construction of the Lebesgue-Stieltjes measures on \((\mathbb{R},{\cal B}(\mathbb{R}))\) discussed in Chapter 1, it is seen that to every nondecreasing right continuous function \(F:\mathbb{R}\to\mathbb{R}\), there is a (Radon) measure \(\mu_{F}\) on \((\mathbb{R},{\cal B}(\mathbb{R}))\) such that \(\mu_{F}((a,b])=F(b)-F(a)\) for all \(a<b\) and conversely. If \(\mu_{1}\) and \(\mu_{2}\) are two Radon measures and \(\mu=\mu_{1}-\mu_{2}\), let

\[G(x) \equiv \left\{\begin{array}{ll}\mu((0,x])&\mbox{for}\quad x>0,\\ -\mu((x,0])&\mbox{for}\quad x<0,\\ 0&\mbox{for}\quad x=0.\end{array}\right.\] \[= \left\{\begin{array}{ll}F_{1}(x)-F_{2}(x)-(F_{1}(0)-F_{2}(0))& \mbox{for}\quad x>0,\\ (F_{1}(0)-F_{2}(0))-(F_{1}(x)-F_{2}(x))&\mbox{for}\quad x<0,\\ 0&\mbox{for}\quad x=0.\end{array}\right.\]

Thus to every finite signed measure \(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\), there corresponds a function \(G(\cdot)\) that is the difference of two right continuous nondecreasing and bounded functions. The converse is also easy to establish. A characterization of such a function \(G(\cdot)\) without any reference to measures is given below.

**Definition 4.3.1:** Let \(f:[a,b]\rightarrow\mathbb{R}\), where \(-\infty<a<b<\infty\). Then for any partition \(Q=\{a=x_{0}<x_{1}<x_{2}<\ldots<x_{n}=b\}\), \(n\in\mathbb{N}\), the _positive, negative and total variations of \(f\) with respect to \(Q\)_ are respectively defined as

\[P(f,Q) \equiv \sum_{i=1}^{n}(f(x_{i})-f(x_{i-1}))^{+}\] \[N(f,Q) \equiv \sum_{i=1}^{n}(f(x_{i})-f(x_{i-1}))^{-}\] \[T(f,Q) \equiv \sum_{i=1}^{n}|f(x_{i})-f(x_{i-1})|.\]

It is easy to verify that (i) if \(f\) is nondecreasing, then

\[P(f,Q)=T(f,Q)=f(b)-f(a)\quad\mbox{and}\quad N(f,Q)=0\]

and that (ii) for any \(f\),

\[P(f,Q)+N(f,Q)=T(f,Q).\]

**Definition 4.3.2:** Let \(f=[a,b]\rightarrow\mathbb{R}\), where \(-\infty<a<b<\infty\). The _positive, negative and total variations of \(f\) over \([a,b]\)_ are respectively defined as

\[P(f,[a,b]) \equiv \sup_{Q}P(f,Q)\] \[N(f,[a,b]) \equiv \sup_{Q}N(f,Q)\] \[T(f,[a,b]) \equiv \sup_{Q}T(f,Q),\]where the supremum in each case is taken over all finite partitions \(Q\) of \([a,b]\).

**Definition 4.3.3:** Let \(f:[a,b]\rightarrow\mathbb{R}\), where \(-\infty<a<b<\infty\). Then, \(f\) is said to be of _bounded variation on \([a,b]\)_ if \(T(f,[a,b])<\infty\). The set of all such functions is denoted by \(BV[a,b]\).

As remarked earlier, if \(f\) is nondecreasing, then \(T(f,Q)=f(b)-f(a)\) for each \(Q\) and hence \(T(f,[a,b])=f(b)-f(a)\). It follows that if \(f=f_{1}-f_{2}\), where both \(f_{1}\) and \(f_{2}\) are nondecreasing, then \(f\in BV[a,b]\). A natural question is whether the converse is true. The answer is yes, as shown by the following result.

**Theorem 4.3.1:** _Let \(f\in BV[a,b]\). Let \(f_{1}(x)\equiv P(f,[a,x])\) and \(f_{2}(x)\equiv N(f,[a,x])\). Then \(f_{1}\) and \(f_{2}\) are nondecreasing in \([a,b]\) and for all \(a\leq x\leq b\),_

\[f(x)=f_{1}(x)-f_{2}(x)\]

**Proof:** That \(f_{1}\) and \(f_{2}\) are nondecreasing follows from the definition. It is enough to verify that if \(f\in BV[a,b]\), then

\[f(b)-f(a)=P(f,[a,b])-N(f,[a,b]),\]

as this can be applied to \([a,x]\) for \(a\leq x<b\). For each finite partition \(Q\) of \([a,b]\),

\[f(b)-f(a) = \sum_{i=1}^{n}(f(x_{i})-f(x_{i-1}))\] \[= P(f,Q)-N(f,Q).\]

Thus \(P(f,Q)=f(b)-f(a)+N(f,Q)\). By taking supremum over all finite partitions \(Q\), it follows that

\[P(f,[a,b])=f(b)-f(a)+N(f,[a,b]).\]

If \(f\in BV[a,b]\), this yields \(f(b)-f(a)=P(f,[a,b])-N(f,[a,b])\). \(\Box\)

**Remark 4.3.1:** Since \(T(f,Q)=P(f,Q)+N(f,Q)=2P(f,Q)-(f(b)-f(a))\), it follows that if \(f\in BV[a,b]\), then

\[T(f,[a,b]) =2P(f,[a,b])-(f(b)-f(a))\] \[=P(f,[a,b])+N(f,[a,b]).\]

**Corollary 4.3.2:** _A function \(f\in BV[a,b]\) iff there exists a finite signed measure \(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \(f(x)=\mu([a,x])\), \(a\leq x\leq b\)._

The proof of this corollary is left as an exercise.

**Remark 4.3.2:** Some observations on functions of bounded variations are listed below.

* Let \(f=I_{\mathbb{Q}}\) where \(\mathbb{Q}\) is the set of rationals. Then for any \([a,b],a<b,P(f,[a,b])=N(f,[a,b])=\infty\) and so \(f\notin BV[a,b]\). This holds for \(f=I_{D}\) for any set \(D\) such that both \(D\) and \(D^{c}\) are dense in \(\mathbb{R}\).
* Let \(f\) be _Lipschitz_ on \([a,b]\). That is, \(|f(x)-f(y)|\leq K|x-y|\) for all \(x,y\) in \([a,b]\) where \(K\in(0,\infty)\) is a constant. Then, \(f\in BV[a,b]\).
* Let \(f\) be differentiable in \((a,b)\) and continuous on \([a,b]\) and \(f^{\prime}(\cdot)\) be bounded in \((a,b)\). Then by the mean value theorem, \(f\) is Lipschitz and hence, \(f\) is in \(BV[a,b]\).
* Let \(f(x)=x^{2}\sin\frac{1}{x},\ 0<x\leq 1\), and let \(f(0)=0\). Then \(f\) is continuous on \([0,1]\), differentiable on \((0,1)\) with \(f^{\prime}\) bounded on \((0,1)\), and hence \(f\in BV[0,1]\).
* Let \(g(x)=x^{2}\sin\frac{1}{x^{2}},\ 0<x\leq 1,\ g(0)=0\). Then \(g\) is continuous on \([0,1]\), differentiable on \((0,1)\) but \(g^{\prime}\) is not bounded on \((0,1)\). This by itself does not imply that \(g\notin BV[0,1]\), since being Lipschitz is only a sufficient condition. But it turns out that \(g\notin BV[0,1]\). To see this, let \(x_{n}=\sqrt{\frac{1}{(2n+1)\frac{n}{2}}},\ n=0,1,2\ldots\). Then \(\sum\limits_{i=1}^{n}|g(x_{i})-g(x_{i-1})|\geq\sum\limits_{i=1}^{n}\frac{1}{(2 i+1)\frac{n}{2}}\) and hence \(T(g,[0,1])=\infty\).
* It is known (see Royden (1988), Chapter 4) that if \(f:[a,b]\rightarrow\mathbb{R}\) is nondecreasing, then \(f\) is differentiable a.e. \((m)\) on \((a,b)\) and \(\int_{[a,b]}f^{\prime}dm\leq f(b)-f(c)\), where \((m)\) denotes the Lebesgue measure. This implies that if \(f\in BV[a,b]\), then \(f\) is differentiable a.e. \((m)\) on \((a,b)\) and so, \(\int_{[a,b]}|f^{\prime}|dm\leq T(f,[a,b])\).

### 4.4 Absolutely continuous functions on \(\mathbb{R}\)

**Definition 4.4.1:** A function \(F:\mathbb{R}\rightarrow\mathbb{R}\) is _absolutely continuous_ (a.c.) if for all \(\epsilon>0\), there exists \(\delta>0\) such that if \(I_{j}=[a_{j},b_{j}],\ j=1,2,\ldots,k\) (\(k\in\mathbb{N}\)) are disjoint and \(\sum_{j=1}^{k}(b_{j}-a_{j})<\delta\), then \(\sum_{j=1}^{k}|F(b_{j})-F(a_{j})|<\epsilon\).

By the mean value theorem, it follows that if \(F\) is differentiable and \(F^{\prime}(\cdot)\) is bounded, then \(F\) is a.c. Also note that \(F\) is a.c. implies \(F\) is uniformly continuous.

**Definition 4.4.2:** A function \(F:[a,b]\to\mathbb{R}\) is _absolutely continuous_ if the function \(\tilde{F}\), defined by

\[\tilde{F}(x)=\left\{\begin{array}{lll}F(x)&\mbox{ if }&a\leq x\leq b,\\ F(a)&\mbox{ if }&x<a,\\ F(b)&\mbox{ if }&x>b,\end{array}\right.\]

is absolutely continuous.

Thus \(F(x)=x\) is a.c. on \(\mathbb{R}\). Any polynomial is a.c. on any bounded interval but not necessarily on all of \(\mathbb{R}\). For example, \(F(x)=x^{2}\) is a.c. on any bounded interval but not a.c. on \(\mathbb{R}\), since it is not uniformly continuous on \(\mathbb{R}\).

The main result of this section is the following result due to H. Lebesgue, known as the _fundamental theorem of Lebesgue integral calculus_.

**Theorem 4.4.1:**_A function \(F:[a,b]\to\mathbb{R}\) is absolutely continuous iff there is a function \(f:[a,b]\to\mathbb{R}\) such that \(f\) is Lebesgue measurable and integrable w.r.t. \(m\) and such that_

\[F(x)=F(a)+\int_{[a,x]}f\ dm,\quad\mbox{for all}\quad a\leq x\leq b \tag{4.1}\]

_where \(m\) is the Lebesgue measure._

**Proof:** First consider the "if part." Suppose that (4.1) holds. Since \(\int_{[a,b]}|f|dm<\infty\), for any \(\epsilon>0\), there exists a \(\delta>0\) such that (cf. Proposition 2.5.8).

\[m(A)<\delta\Rightarrow\int_{A}|f|dm<\epsilon. \tag{4.2}\]

Thus, if \(I_{j}=(a_{j},b_{j}),\subset[a,b],\;j=1,2,\ldots,k\) are such that \(\sum_{j=1}^{k}(b_{j}-a_{j})<\delta\), then

\[\sum_{j=1}^{k}|F(b_{j})-F(a_{j})|\leq\int_{\bigcup_{j=1}^{k}I_{j}}|f|dm<\epsilon,\]

since \(m(\bigcup_{j=1}^{k}I_{j})\leq\sum_{j=1}^{k}(b_{j}-a_{j})<\delta\) and (4.2) holds. Thus, \(F\) is a.c.

Next consider the "only if part." It is not difficult to verify (Problem 4.18) that \(F\) a.c. implies \(F\) is of bounded variation on any finite interval \([a,b]\) and both the positive and the negative variations of \(F\) on \([a,b]\) are a.c. as well. Hence, it suffices to establish (4.1) assuming that \(F\) is a.c. and nondecreasing. Let \(\mu_{F}\) be the Lebesgue-Stieltjes measure generated by \(\tilde{F}\) as in Definition 4.4.2. It will now be shown that \(\mu_{F}\) is absolutely continuous w.r.t. the Lebesgue measure. Fix \(\epsilon>0\). Let \(\delta>0\) be chosen so that

\[(a_{j},b_{j})\subset[a,b],j=1,2,\ldots,k,\sum_{j=1}^{k}(b_{j}-a_{j})<\delta \Rightarrow\sum_{j=1}^{k}|F(b_{j})-F(a_{j})|<\epsilon.\]Let \(A\in{\cal M}_{m}\), \(A\subset(a,b)\), and \(m(A)=0\). Then, there exist a countable collection of disjoint open intervals \(\{I_{j}=(a_{j},b_{j}):I_{j}\subset[a,b]\}_{j\geq 1}\) such that

\[A\subset\bigcup_{j\geq 1}I_{j}\quad\mbox{and}\quad\sum_{j\geq 1}(b_{j}-a_{j})<\delta.\]

Thus

\[\mu_{F}\bigg{(}A\cap\bigcup_{j=1}^{k}I_{j}\bigg{)} \leq \mu_{F}\bigg{(}\bigcup_{i=1}^{k}I_{j}\bigg{)}\] \[\leq \sum_{j=1}^{k}\mu_{F}(I_{j})=\sum_{j=1}^{k}(F(b_{j})-F(a_{j}))<\epsilon\]

for all \(k\in\mathbb{N}\).

Since \(A\subset\bigcup_{j\geq 1}I_{j}\), by the m.c.f.b. property of \(\mu_{F}(\cdot)\), \(\mu_{F}(A)=\lim_{k\to\infty}\mu_{F}(A\cap\bigcup_{j=1}^{k}I_{j})\leq\epsilon\). This being true for any \(\epsilon>0\), it follows that \(\mu_{F}(A)=0\). Since \(F\) is continuous, \(\mu_{F}(\{a,b\})=0\) and hence \(\mu_{F}\big{(}(a,b)^{c}\big{)}=0\). Thus, \(\mu_{F}\ll m\), i.e., \(\mu_{F}\) is dominated by \(m\). Now, by the Radon-Nikodym theorem (cf. Theorem 4.1.1 (ii)), there exists a nonnegative measurable function \(f\) such that \(A\in{\cal M}_{m}\) implies that \(\mu_{F}(A)=\int_{A\cap[a,b]}f\,dm\) and, in particular, for \(a\leq x\leq b\),

\[\mu_{F}([a,x])=F(x)-F(a)=\int_{[a,x]}f\,dm,\]

i.e., (4.1) holds. \(\Box\)

The representation (4.1) of an absolutely continuous \(F\) can be strengthened as follows:

**Theorem 4.4.2:**_Let \(F:\mathbb{R}\to\mathbb{R}\) satisfy (4.1). Then \(F\) is differentiable a.e. (\(m\)) and \(F^{\prime}(\cdot)=f(\cdot)\) a.e. (\(m\))._

For a proof of this result, see Royden (1988), Chapter 4.

The relation between the notion of absolute continuity of a distribution function \(F:\mathbb{R}\to\mathbb{R}\) and that of the associated Lebesgue-Stieltjes measure \(\mu_{F}\) w.r.t. Lebesgue measure \(m\) will be discussed now.

Let \(F:\mathbb{R}\to\mathbb{R}\) be a distribution function, i.e., \(F\) is nondecreasing and right continuous. Let \(\mu_{F}\) be the associated Lebesgue-Stieltjes measure such that \(\mu_{F}((a,b])=F(b)-F(a)\) for all \(-\infty<a<b<\infty\). Recall that \(F\) is said to be _absolutely continuous on an interval_\([a,b]\) if for each \(\epsilon>0\), there exists a \(\delta>0\) such that for any finite collection of intervals \(I_{j}=(a_{j},b_{j}),j=1,2,\ldots,n\), contained in \([a,b]\),

\[\sum_{j=1}^{n}(b_{j}-a_{j})<\delta\quad\Rightarrow\quad\sum_{j=1}^{n}(F(b_{j} )-F(a_{j}))<\epsilon.\]Recall also that \(\mu_{F}\) is _absolutely continuous w.r.t. the Lebesgue measure \(m\)_ if for \(A\in{\cal B}({\mathbb{R}})\), \(m(A)=0\Rightarrow\mu_{F}(A)=0\). A natural question is that if \(F\) is absolutely continuous on every interval \([a,b]\subset{\mathbb{R}}\), is \(\mu_{F}\) absolutely continuous w.r.t. \(m\) and conversely? The answer is yes.

**Theorem 4.4.3:**_Let \(F:{\mathbb{R}}\to{\mathbb{R}}\) be a nondecreasing function and let \(\mu_{F}\) be the associated Lebesgue-Stieltjes measure. Then \(F\) is absolutely continuous on \([a,b]\) for all \(-\infty<a<b<\infty\) iff \(\mu_{F}\ll m\) where \(m\) is the Lebesgue measure on \(({\mathbb{R}},{\cal B}({\mathbb{R}}))\)._

**Proof:** Suppose that \(\mu_{F}\ll m\). Then by Theorem 4.1.1, there exists a nonnegative measurable function \(h\) such that

\[\mu_{F}(A)=\int_{A}hdm\quad\mbox{for all}\quad A\quad\mbox{in}\quad{\cal B}({ \mathbb{R}}).\]

Hence, for any \(a<b\) in \({\mathbb{R}}\) and \(a<x<b\),

\[F(x)-F(a)\equiv\mu_{F}((a,x])=\int_{(a,x]}hdm.\]

This implies the absolute continuity of \(F\) on \([a,b]\) as shown in Theorem 4.4.1.

Conversely, if \(F\) is absolutely continuous on \([a,b]\) for all \(-\infty<a<b<\infty\), then as shown in the proof of the "only if" part of Theorem 4.4.1, for all \(-\infty<a<b<\infty\), then \(\mu_{F}(A\cap[a,b])=0\) if \(m(A\cap[a,b])=0\). Thus, if \(m(A)=0\), then for all \(-\infty<a<b<\infty\), \(m(A\cap[a,b])=0\) and hence \(\mu_{F}(A\cap[a,b])=0\) and hence \(\mu_{F}(A)=0\), i.e., \(\mu_{F}\ll m\). \(\Box\)

Recall that a measure \(\mu\) on \(({\mathbb{R}}^{k},{\cal B}({\mathbb{R}}^{k}))\) is a _Radon measure_ if \(\mu(A)<\infty\) for every bounded Borel set \(A\). In the following, let \(m(\cdot)\) denote the Lebesgue measure on \({\mathbb{R}}^{k}\).

**Definition 4.4.3:** A Radon measure \(\mu\) on \(({\mathbb{R}}^{k},{\cal B}({\mathbb{R}}^{k}))\) is _differentiable_ at \(x\in{\mathbb{R}}^{k}\) with _derivative_\((D\mu)(x)\) if for any \(\epsilon>0\), there is a \(\delta>0\) such that

\[\Big{|}\frac{\mu(A)}{m(A)}-(D\mu)(x)\Big{|}<\epsilon\]

for every open ball \(A\) such that \(x\in A\) and diam. \((A)\equiv\sup\{\|x-y\|:x,y\in A\}\), the diameter of \(A\), is less than \(\delta\).

**Theorem 4.4.4:**_Let \(\mu\) be a Radon measure on \(({\mathbb{R}}^{k},{\cal B}({\mathbb{R}}^{k}))\). Then_

* \(\mu\) _is differentiable a.e._ \((m)\)_,_ \(D\mu(\cdot)\) _is Lebesgue measurable, and_ \(\geq 0\) _a.e._ \((m)\) _and for all bounded Borel sets_ \(A\in{\cal B}({\mathbb{R}}^{k})\), \[\int_{A}D\mu(\cdot)dm\leq\mu(A).\]_._
* _Let_ \(\mu_{a}(A)\equiv\int_{A}D\mu(\cdot)dm\)_,_ \(A\in{\cal B}({\mathbb{R}}^{k})\)_. Let_ \(\mu_{s}(\cdot)\) _be the unique measure on_ \({\cal B}({\mathbb{R}}^{k})\) _such that for all bounded Borel sets_ \(A\)__ \[\mu_{s}(A)=\mu(A)-\mu_{a}(A).\] _Then_ \[\mu_{s}\perp m\quad\mbox{and}\quad D\mu_{s}(\cdot)=0\quad\mbox{a.e.}\;(m).\]

For a proof, see Rudin (1987).

**Remark 4.4.1:** By the uniqueness of the Lebesgue decomposition, it follows that a Radon measure \(\mu\) on \({\cal B}({\mathbb{R}}^{k})\) is \(\perp m\) iff \(D\mu(\cdot)=0\) a.e. \((m)\) and is \(\ll m\) iff \(\mu(A)=\int_{A}D\mu(\cdot)dm\) for all \(A\in{\cal B}({\mathbb{R}}^{k})\).

Let \(f:{\mathbb{R}}^{k}\to{\mathbb{R}}_{+}\) be integrable w.r.t. \(m\) on bounded sets. Let \(\mu(A)\equiv\int_{A}fdm\) for \(A\in{\cal B}({\mathbb{R}}^{k})\). Then \(\mu(\cdot)\) is a Radon measure and that is \(\ll m\) and hence by Theorem 4.4.4

\[D\mu(x)=f(x)\quad\mbox{for almost all}\quad x(m).\]

That is, for almost all \(x(m)\), for each \(\epsilon>0\), there is a \(\delta>0\) such that

\[\Big{|}\frac{1}{m(A)}\int_{A}fdm-f(x)\Big{|}<\epsilon\]

for all open balls \(A\) such that \(x\in A\) and \(\mbox{diam.}\,(A)<\delta\).

It turns out that a stronger result holds.

**Theorem 4.4.5:** _For almost all \(x(m)\), for each \(\epsilon>0\), there is a \(\delta>0\) such that_

\[\frac{1}{m(A)}\int_{A}|f-f(x)|dm<\epsilon\]

_for all open balls \(A\) such that \(x\in A\) and \(\mbox{diam.}\,(A)<\delta\) (see Problems 4.23, 4.24)._

**Theorem 4.4.6:** (_Change of variables formula in \({\mathbb{R}}^{k}\), \(k>1\)_). Let \(V\) be an open set in \({\mathbb{R}}^{k}\). Let \(T\equiv(T_{1},T_{2},\ldots,T_{k})\) be a map from \({\mathbb{R}}^{k}\to{\mathbb{R}}^{k}\) such that for each \(i\), \(T_{i}:{\mathbb{R}}^{k}\to{\mathbb{R}}\) and \(\frac{\partial T_{i}(\cdot)}{\partial x_{j}}\) exists on \(V\) for all \(1\leq i\), \(j\leq k\). Suppose that the Jacobian \(J_{T}(\cdot)\equiv det\big{(}\big{(}\frac{\partial T_{i}(\cdot)}{\partial x_{j }}\big{)}\big{)}\) is continuous and positive on \(V\). Suppose further that \(T(V)\) is a bounded open set \(W\) in \({\mathbb{R}}^{k}\) and that \(T\) is \((1-1)\) and \(T^{-1}\) is continuous. Then_

* _For all Borel set_ \(E\subset V\)_,_ \(T(E)\) _is a Borel set_ \(\subset W\)_._
* \(\nu(\cdot)\equiv m(T(\cdot))\) _is a measure on_ \({\cal B}(W)\) _and_ \(\nu\ll m\) _with_ \[\frac{d\nu(\cdot)}{dm}=J_{T}(\cdot).\]_._
* _For any_ \(h\in L^{1}(W,m)\)__ \[\int_{W}hdm=\int_{V}h\bigl{(}T(\cdot)\bigr{)}J_{T}(\cdot)dm.\]
* \(\lambda(\cdot)\equiv mT^{-1}(\cdot)\) _is a measure on_ \({\cal B}(W)\) _and_ \(\lambda\ll m\) _with_ \[\frac{d\lambda}{dm}=|J\bigl{(}T^{-1}(\cdot)\bigr{)}|^{-1}.\]
* _For any_ \(\mu\ll m\) _on_ \({\cal B}(V)\) _the measure_ \(\psi(\cdot)\equiv\mu T^{-1}(\cdot)\) _is dominated by_ \(m\) _with_ \[\frac{d\psi}{dm}(\cdot)=\Bigl{(}\frac{d\mu}{dm}\Bigr{)}\bigl{(}T^{-1}(\cdot) \bigr{)}\Bigl{(}J_{T}\bigl{(}T^{-1}(\cdot)\bigr{)}\Bigr{)}^{-1}\quad on\quad W.\]

For a proof see Rudin (1987), Chapter 7.

### 4.5 Singular distributions

#### Decomposition of a cdf

Recall that a _cumulative distribution function_ (cdf) on \({\mathbb{R}}\) is a function \(F:{\mathbb{R}}\to[0,1]\) such that it is nondecreasing, right continuous, \(F(-\infty)=0\), \(F(\infty)=1\). In Section 2.2, it was shown that any cdf \(F\) on \({\mathbb{R}}\) can be written as

\[F=\alpha F_{d}+(1-\alpha)F_{c}, \tag{5.1}\]

where \(F_{d}\) and \(F_{c}\) are discrete and continuous cdfs respectively. In this section, the cdf \(F_{c}\) will be further decomposed into a singular continuous and absolutely continuous cdfs.

**Definition 4.5.1:** A cdf \(F\) is _singular_ if \(F^{\prime}\equiv 0\) almost everywhere w.r.t. the Lebesgue measure on \({\mathbb{R}}\).

**Example 4.5.1:** The cdfs of Binomial, Poisson, or any integer valued random variables are singular.

It is known (cf. Royden (1988), Chapter 5) that a monotone function \(F:{\mathbb{R}}\to{\mathbb{R}}\) is differentiable almost everywhere w.r.t. the Lebesgue measure and its derivative \(F^{\prime}\) satisfies

\[\int_{a}^{b}F^{\prime}(x)dx\leq F(b)-F(a), \tag{5.2}\]

for any \(-\infty<a<b<\infty\).

For \(x\in\mathbb{R}\), let \(\tilde{F}_{ac}(x)\equiv\int_{-\infty}^{x}F^{\prime}_{c}(t)dt\) and \(\tilde{F}_{sc}(x)\equiv F_{c}(x)-\tilde{F}_{ac}(x)\). If \(\tilde{\beta}\equiv\int_{-\infty}^{\infty}F^{\prime}_{c}(t)dt=\tilde{F}_{ac}( \infty)=0\), then \(F^{\prime}_{c}(t)=0\) a.e. and so \(F_{c}\) is singular continuous. If \(\tilde{\beta}=1\), then \(F_{c}=\tilde{F}_{ac}\) and so, \(F_{c}\) is absolutely continuous. If \(0<\alpha<1\) and \(0<\tilde{\beta}<1\), then \(F\) can be written as

\[F=\alpha F_{d}+\beta F_{ac}+\gamma F_{sc}, \tag{5.3}\]

where \(\beta=(1-\alpha)\tilde{\beta}\), \(\gamma=(1-\alpha)(1-\tilde{\beta})\), \(F_{ac}=\tilde{\beta}^{-1}\tilde{F}_{ac}\), \(F_{sc}=(1-\tilde{\beta})^{-1}\tilde{F}_{sc}\), and \(F_{d}\) is as in (5.1). Note that \(F_{d}\), \(F_{ac}\), \(F_{sc}\) are all cdfs and \(\alpha\), \(\beta\), \(\gamma\) are nonnegative numbers adding up to 1. Summarizing the above discussions, one has the following:

**Proposition 4.5.1:**_Given any cdf \(F\), there exist nonnegative constants \(\alpha\), \(\beta\), \(\gamma\) and cdfs \(F_{d}\), \(F_{ac}\), \(F_{sc}\) satisfying (a) \(\alpha+\beta+\gamma=1\), and (b) \(F_{d}\) is discrete, \(F_{ac}\) is absolutely continuous, \(F_{sc}\) is singular continuous, such that the decomposition (5.3) holds._

It can be shown that the constants \(\alpha\), \(\beta\), and \(\gamma\) are uniquely determined, and that when \(0<\alpha<1\), the decomposition (5.1) is unique, and that when \(0<\alpha,\beta,\gamma<1\), the decomposition (5.3) is unique. The decomposition (5.3) also has a probabilistic interpretation. Any random variable \(X\) can be realized as a randomized choice over three random variables \(X_{d}\), \(X_{ac}\), and \(X_{sc}\) having cdfs \(F_{d}\), \(F_{ac}\), and \(F_{sc}\), respectively, and with corresponding randomization probabilities \(\alpha\), \(\beta\), and \(\gamma\). For more details see Problem 6.15 in Chapter 6.

#### 4.5.2 Cantor ternary set

Recall the construction of the Cantor set from Section 1.3.

Let \(I_{0}=[0,1]\) denote the unit interval. If one deletes the open middle third of \(I_{0}\), then one gets two disjoint closed intervals \(I_{11}=\left[0,\frac{1}{3}\right]\) and \(I_{12}=\left[\frac{2}{3},1\right]\). Proceeding similarly with the closed intervals \(I_{11}\) and \(I_{12}\), one gets four disjoint intervals \(I_{21}=\left[0,\frac{1}{9}\right]\), \(I_{22}=\left[\frac{2}{9},\frac{1}{3}\right]\), \(I_{23}=\left[\frac{2}{3},\frac{7}{9}\right]\), \(I_{24}=\left[\frac{8}{9},1\right]\), and so on. Thus, at each step, deleting the open middle third of the closed intervals constructed in the previous step, one is left with \(2^{n}\) disjoint closed intervals each of length \(3^{-n}\) after \(n\) steps. Let \(C_{n}=\bigcup_{j=1}^{2^{n}}I_{nj}\) and \(C=\bigcap_{n=1}^{\infty}C_{n}\). By construction \(C_{n+1}\subset C_{n}\) for each \(n\) and \(C_{n}\)'s are closed sets. With \(m(\cdot)\) denoting Lebesgue measure, one has \(m(C_{0})=1\) and \(m(C_{n})=2^{n}3^{-n}=\left(\frac{2}{3}\right)^{n}\).

**Definition 4.5.2:** The set \(C\equiv\bigcap_{n=1}^{\infty}C_{n}\) is called the _Cantor ternary set_ or simply the _Cantor set_.

Since \(m(C_{0})=1\), by m.c.f.a. \(m(C)=\lim_{n\to\infty}m(C_{n})=\lim_{n\to\infty}\left(\frac{2}{3}\right)^{n}=0\). Thus, the Cantor set \(C\) has zero Lebesgue measure. Next, let \(U_{1}=U_{11}=\left(\frac{1}{3},\frac{2}{3}\right)\) be the deleted interval at the first stage,

[MISSING_PAGE_FAIL:150]

where \(\delta_{i}(x)\in\{0,1\}\) for all \(i\). Thus, the interval [0,1) is in one-to-one correspondence with the set of all sequences of 0's and 1's.

It is not difficult to prove the following (Problem 4.31).

Theorem 4.5.2: A number \(x\) belongs to the Cantor set \(C\) iff in (5.4), for all \(i\geq 1\), \(a_{i}(x)\) is either 0 or 2.

Corollary 4.5.3: The Cantor set \(C\) is in one-to-one correspondence with the set of all sequences of 0's and 1's and hence is in one-to-one correspondence with the unit interval [0,1].

Remark 4.5.1: Thus the Cantor ternary set \(C\) is a closed subset of [0,1], its Lebesgue measure \(m(C)=0\), and its cardinality is the same as that of [0,1]. Further, it is _nowhere dense_, i.e., its complement \(U\) is dense in the sense that for every open interval \((a,b)\subset[0,1]\), \(U\cap(a,b)\) is nonempty. It is also possible to get a Cantor like set \(C_{\alpha}\) with (Lebesgue) measure \(\alpha\), \(0<\alpha<1\), by following the above iterative procedure of deleting at each stage intervals of length that is a fraction \(\frac{(1-\alpha)}{3}\) of the full interval (Problem 4.32).

#### Cantor ternary function

The _Cantor ternary function_\(F:[0,1]\rightarrow[0,1]\) is defined as follows: For \(n\geq 1\), let \(\{U_{nj}:j=1,\ldots,2^{n-1}\}\) denote the set of "deleted" intervals at step \(n\) in the definition of the Cantor set \(C\). Define \(F\) on \(C^{c}=U\) by

\[F(x) = \frac{1}{2}\quad\mbox{on}\quad U_{11}=\left(\frac{1}{3},\frac{2} {3}\right)\quad\mbox{and}\] \[= \frac{1}{4}\quad\mbox{on}\quad U_{21}=\left(\frac{1}{9},\frac{2} {9}\right)\] \[= \frac{3}{4}\quad\mbox{on}\quad U_{22}=\left(\frac{7}{9},\frac{8} {9}\right)\]

and so on. It can be checked that \(F\) is uniformly continuous on \(U\) and has a continuous extension to \(I_{0}=[0,1]\). The extension of the function \(F\) (also denoted by \(F\)) maps [0,1] onto [0,1] and is continuous and nondecreasing. Further, on \(U\), it is differentiable with derivative \(F^{\prime}\equiv 0\). Since \(m(U^{c})=0\), \(F\) is a singular cdf (cf. see Definition 4.5.1). It can be shown that if \(\sum_{i=1}^{\infty}\frac{a_{i}(x)}{3^{i}}\) is the ternary expansion of \(x\in(0,1)\), then

\[F(x)=\sum_{i=1}^{N(x)-1}\frac{a_{i}(x)}{2^{i+1}}+\frac{1}{2^{N(x)}} \tag{5.5}\]

Where \(N(x)=\inf\{i:i\geq 1,a_{i}(x)=1\}\). For example, \(x=\frac{1}{3}=\sum_{i=2}^{\infty}\frac{2}{3^{i}}\Rightarrow N(x)=\infty\), \(a_{1}(x)=0\), \(a_{i}(x)=2\) for \(i\geq 2\Rightarrow F(x)=\sum_{i=2}^{\infty}\frac{1}{2^{i}}=\frac{1}{2}\) while \(x=\frac{4}{9}=\frac{1}{3}+\frac{0}{3^{2}}+\sum_{i=3}^{\infty}\frac{2}{3^{i}} \Rightarrow N(x)=1\Rightarrow F(x)=\frac{1}{2}\).

It can be shown that if \(\{\delta_{n}\}_{n\geq 1}\) is a sequence of independent \(\{0,1\}\) valued random variables with \(P(\delta_{1}=0)=\frac{1}{2}=P(\delta_{1}=1)\), then the above \(F\) is the cdf of the random variable \(X=\sum_{i=1}^{\infty}\frac{2\delta_{i}}{3^{i}}\) which lies in the Cantor set w.p. 1.

### Problems

* Show that in the proof of Theorem 4.1.1, \(\mu(A_{3})=0\) where \(A_{3}=\{g\not\in[0,1]\}\) and \(g\) satisfies (1.9). (**Hint:** Apply (1.9) separately to \(A_{31}=\{g>1\}\) and \(A_{32}=\{g<0\}\).)
* Verify that \(\mu_{1s}\), defined in (1.14) and \(\mu_{2}\) are singular. (**Hint:** For each \(n\), by Case 1, there exists a \(g_{n}\) in \(L^{2}(D_{n},\mathcal{F}_{n},\mu^{(n)})\) where \(\mu^{(n)}=\mu_{1}^{(n)}+\mu_{2}^{(n)}\) and \(\mathcal{F}_{n}\equiv\{A\cap D_{n}:A\in\mathcal{F}\}\), such that \(0\leq\int_{A}g_{n}d\mu^{(n)}\leq\mu^{(n)}(A)\) for all \(A\) in \(\mathcal{F}_{n}\). Let \(A_{1n}=\{w:w\in D_{n},0\leq g_{n}(w)<1\}\), \(A_{2n}=\{w:w\in D_{n},g_{n}(w)=1\}\), and \(A_{2}=\bigcup_{n\geq 1}A_{2n}\). Show that \(\mu_{2}(A_{2})=0\) and \(\mu_{1s}(A)=\sum_{n=1}^{\infty}\mu_{1n}(A\cap A_{2n})\) and hence \(\mu_{1s}(A_{2}^{c})=0\).)
* Let \(\nu\) be the counting measure on \([0,1]\) and \(\int_{[0,1]}hd\nu<\infty\) for some nonnegative function \(h\). Show that \(B=\{x:h(x)>0\}\) is countable. (**Hint:** Let \(B_{n}=\{x:h(x)>\frac{1}{n}\}\). Show that \(B_{n}\) is a finite set for each \(n\in\mathbb{N}\).)
* Prove Proposition 4.1.2.
* Find the Lebesgue decomposition of \(\mu\) w.r.t. \(\nu\) and the Radon-Nikodym derivative \(\frac{d\mu_{a}}{d\nu}\) in the following cases where \(\mu_{a}\) is the absolutely continuous component of \(\mu\) w.r.t. \(\nu\).
* \(\mu=N(0,1)\), \(\nu=\mbox{Exponential}(1)\)
* \(\mu=\mbox{Exponential}(1)\), \(\nu=N(0,1)\)
* \(\mu=\mu_{1}+\mu_{2}\), where \(\mu_{1}=N(0,1)\), \(\mu_{2}=\mbox{Poisson}(1)\) and \(\nu=\mbox{Cauchy}(0,1)\).
* \(\mu=\mu_{1}+\mu_{2}\), \(\nu=\mbox{Geometric}(p)\), \(0<p<1\), where \(\mu_{1}=N(0,1)\) and \(\mu_{2}=\mbox{Poisson}(1)\).
* \(\mu=\mu_{1}+\mu_{2}\), \(\nu=\nu_{1}+\nu_{2}\) where \(\mu_{1}=N(0,1)\), \(\mu_{2}=\mbox{Poisson}(1)\), \(\nu_{1}=\mbox{Cauchy}(0,1)\) and \(\nu_{2}=\mbox{Geometric}(p)\), \(0<p<1\).
* \(\mu=\mbox{Binomial}\) (10, 1/2), \(\nu=\mbox{Poisson}\) (1). The measures referred to above are defined in Tables 4.6.1 and 4.6.2, given at the end of this section.
4.6 Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(f\in L^{1}(\Omega,\mathcal{F},\mu).\) Let \[\nu_{f}(A)\equiv\int_{A}fd\mu\quad\text{for all}\quad A\in\mathcal{F}.\] 1. Show that \(\nu_{f}\) is a finite signed measure. 2. Show that \(\|\nu\|=\int_{\Omega}|f|d\mu\) and for \(A\in\mathcal{F}\), \(\nu_{f}^{+}(A)=\int_{A}f^{+}d\mu\), \(\nu_{f}^{-}(A)=\int_{A}fd\mu\), and \(|\nu_{f}|(A)=\int_{A}|f|(d\mu)\).
4.7 1. Let \(\mu_{1}\) and \(\mu_{2}\) be two finite measures such that both are dominated by a \(\sigma\)-finite measure \(\nu.\) Show that the total variation measure of the signed measure \(\mu\equiv\mu_{1}-\mu_{2}\) is given by \(|\mu|(A)=\int_{A}|h_{1}-h_{2}|d\nu\) where for \(i=1,2\), \(h_{i}=\frac{d\mu_{i}}{d\nu}\). 2. Conclude that if \(\mu_{1}\) and \(\mu_{2}\) are two measures on a countable set \(\Omega\equiv\{\omega_{i}\}_{i\geq 1}\) with \(\mathcal{F}\equiv\mathcal{P}(\Omega)\), then \(|\mu|(A)=\sum_{i\in A}|\mu_{1}(\omega_{i})-\mu_{2}(\omega_{i})|\). 3. Show that if \(\mu_{n}\) is the Binomial \((n,p_{n})\) measure and \(\mu\) is the Poisson \((\lambda)\) measure, \(0<\lambda<\infty\), then as \(n\to\infty\), \(|\mu_{n}-\mu|(\cdot)\to 0\) uniformly on \(\mathcal{P}(\mathbb{Z}_{+})\) iff \(np_{n}\to\lambda\). (**Hint:** Show that for each \(i\in\mathbb{Z}_{+}\equiv\{0,1,2,\ldots\}\), \(\mu_{n}(\{i\})\to\mu(\{i\})\) and use Scheffe's theorem.)
4.8 Let \(\nu\) be a finite signed measure on a measurable space \((\Omega,\mathcal{F})\) and let \(|\nu|\) be the total variation measure corresponding to \(\nu.\) Show that for any \(B\in\Omega^{+},\ B\subset\mathcal{F}\), \[|\nu|(B)=\nu(B),\] where \(\Omega^{+}\) is as defined in (2.13). (**Hint:** For any set \(A\subset\Omega^{+}\), \[\nu(A)=\int_{A}hd|\nu|=\int_{A\cap\Omega^{+}}hd|\nu|\geq 0.)\] 4.9 Show that the Banach space \(\mathbb{S}\) of finite signed measures on \((\mathbb{N},\mathcal{P}(\mathbb{N}))\) is isomorphic to \(\ell_{1}\), the Banach space of absolutely convergent sequences \(\{x_{n}\}_{n\geq 1}\) in \(\mathbb{R}\).
4.10 Let \(\mu_{1}\) and \(\mu_{2}\) be two probability measures on \((\Omega,\mathcal{F})\). 1. Show that \[\|\mu_{1}-\mu_{2}\|=2\sup\{|\mu_{1}(A)-\mu_{2}(A)|:A\in\mathcal{F}\}.\] (**Hint:** For any \(A\in\mathcal{F}\), \(\{A,A^{c}\}\) is a partition of \(\Omega\) and so \(\|\mu_{1}-\mu_{2}\|\geq|\mu_{1}(A)-\mu_{2}(A)|+|\mu_{1}(A^{c})-\mu_{2}(A^{c})|= 2|\mu_{1}(A)-\mu_{2}(A)|\)since \(\mu_{1}\) and \(\mu_{2}\) are probability measures. For the opposite inequality, use the Hahn decomposition of \(\Omega\) w.r.t. \(\mu_{1}-\mu_{2}\) and the fact \(\|\mu_{1}-\mu_{2}\|=|(\mu_{1}-\mu_{2})(\Omega^{+})|+|(\mu_{1}-\mu_{2})(\Omega^{- })|\).) 2. Show that \(\|\mu_{1}-\mu_{2}\|\) is also equal to \[\sup\Big{\{}\Big{|}\int fd\mu_{1}-\int fd\mu_{2}\Big{|}:f\in B(\Omega,\mathbb{R })\Big{\}}\] where \(B(\Omega,\mathbb{R})\) is the collection of all \(\mathcal{F}\)-measurable functions from \(\Omega\) to \(\mathbb{R}\) such that \(\sup\{|f(\omega)|:\omega\in\Omega\}\leq 1\).
4.11 Let \((\Omega,\mathcal{F})\) be a measurable space. 1. Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of finite measures on \((\Omega,\mathcal{F})\). Show that there exists a probability measure \(\lambda\) such that \(\mu_{n}\ll\lambda\). (**Hint:** Consider \(\lambda(\cdot)=\sum_{n=1}^{\infty}\frac{1}{2^{n}}\frac{\mu_{n}(\cdot)}{\mu_{n }(\Omega)}\).) 2. Extend (a) to the case where \(\{\mu_{n}\}_{n\geq 1}\) are \(\sigma\)-finite. 3. Conclude that for any sequence \(\{\nu_{n}\}_{n\geq 1}\) of finite signed measures on \((\Omega,\mathcal{F})\), there exists a probability measure \(\lambda\) such that \(|\nu_{n}|\ll\lambda\) for all \(n\geq 1\).
4.12 Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of finite measures on a measurable space \((\Omega,\mathcal{F})\). Show that there exists a finite measure \(\mu\) on \((\Omega,\mathcal{F})\) such that \(\|\mu_{n}-\mu\|\to 0\) iff there is a finite measure \(\lambda\) dominating \(\mu\) and \(\mu_{n}\), \(n\geq 1\) such that the Radon-Nikodym derivatives \(f_{n}\equiv\frac{d\mu_{n}}{d\lambda}\to f\equiv\frac{d\mu}{d\lambda}\) in measure on \((\Omega,\mathcal{F},\lambda)\) and \(\mu_{n}(\Omega)\to\mu(\Omega)\).
4.13 1. Let \(\mu_{1}\) and \(\mu_{2}\) be two finite measures on \((\Omega,\mathcal{F})\). Let \(\mu_{1}=\mu_{1a}+\mu_{1s}\) be the Lebesgue-Radon-Nikodym decomposition of \(\mu_{1}\) w.r.t. \(\mu_{2}\) as in Theorem 4.1.1. Show that if \(\mu=\mu_{1}-\mu_{2}\), then for all \(A\in\mathcal{F}\), \[|\mu|(A)=\int_{A}|h-1|d\mu_{2}+\mu_{1s}(A)\quad\mbox{where}\quad h=\frac{d\mu _{1a}}{d\mu_{2}}\] is the Radon-Nikodym derivative of \(\mu_{1a}\) w.r.t. \(\mu_{2}\). Conclude that if \(\mu_{1}\perp\mu_{2}\), then \(|\mu|(\cdot)=\mu_{1}(\cdot)+\mu_{2}(\cdot)\) and if \(\mu_{1}\ll\mu_{2}\), then \(|\mu|(A)=\int_{A}|\frac{d\mu_{1}}{d\mu_{2}}-1\big{|}d\mu_{2}\). 2. Compute \(|\mu|(\cdot)\), \(\|\mu\|\) if \(\mu=\mu_{1}-\mu_{2}\) for the following cases 1. \(\mu_{1}=N(0,1)\), \(\mu_{2}=N(1,1)\) 2. \(\mu_{1}=\) Cauchy (0,1), \(\mu_{2}=N(0,1)\) 3. \(\mu_{1}=N(0,1)\), \(\mu_{2}=\) Poisson \((\lambda)\). 3. Establish Proposition 4.2.7.

4. Differentiation

**4.14**: Give another proof of the completeness of \((\mathbb{S},\|\cdot\|))\) by verifying the following steps.

1. For any sequence \(\{\nu_{n}\}_{n\geq 1}\) in \(\mathbb{S}\), there is a finite measure \(\lambda\) and \(\{f_{n}\}_{n\geq 1}\subset L^{1}(\Omega,\mathcal{F},\lambda)\) such that \[\nu_{n}(A)=\int_{A}f_{n}d\lambda\quad\text{for all}\quad A\in\mathcal{F}, \quad\text{for all}\quad n\geq 1.\] 2. \(\{\nu_{n}\}_{n\geq 1}\) Cauchy in \(\mathbb{S}\) is the same as \(\{f_{n}\}_{n\geq 1}\) Cauchy in \(L^{1}(\Omega,\mathcal{F},\lambda)\) and hence, the completeness of \((\mathbb{S},\|\cdot\|))\) follows from the completeness of \(L^{1}(\Omega,\mathcal{F},\lambda)\).

**4.15**: Let \(f,g\in BV[a,b]\).

1. Show that \(P(f+g;[a,b])\leq P(f;[a,b])+P(g;[a,b])\) and that the same is true for \(N(\cdot;\cdot)\) and \(T(\cdot;\cdot)\).
2. Show that for any \(c\in\mathbb{R}\), \[P(cf;[a,b])=|c|P(f;[a,b])\] and do the same for \(N(\cdot;\cdot)\) and \(T(\cdot;\cdot)\).
3. For any \(a<c<b\), \(P(f;[a,b])=P(f;[a,c])+P(f;[c,b])\).

**4.16**: Let \(\{f_{n}\}_{n\geq 1}\subset BV[a,b]\) and let \(\lim_{n}f_{n}(x)=f(x)\quad\text{for all}\quad x\) in \([a,b]\). Show that \(P(f;[a,b])\leq\lim_{n\to\infty}P(f_{n};[a,b])\) and do the same for \(N(\cdot;\cdot)\) and \(T(\cdot;\cdot)\).

**4.17**: Let \(f\in BV[a,b]\). Show that \(f\) is continuous except on an at most countable set.

**4.18**: Let \(F:[a,b]\to\mathbb{R}\) be a.c. Show that it is of bounded variation.

1. **(Hint:** By the definition of a.c., for \(\epsilon=1\), there is a \(\delta_{1}>0\) such that \(\sum_{j=1}^{k}|a_{j}-b_{j}|<\delta_{1}\Rightarrow\sum_{j=1}^{k}|F(a_{j})-F(b_ {j})|<1\). Let \(M\) be an integer \(>\frac{b-a}{\delta}+1\). Show that \(T(F,[a,b])\leq M\).)

**4.19**: Let \(F\) be an absolutely continuous nondecreasing function on \(\mathbb{R}\). Let \(\mu_{F}\) be the Lebesgue-Stieltjes measure corresponding to \(F\). Show that for any \(h\in L^{1}(\mathbb{R},\mathcal{M}_{\mu_{F}},\mu_{F})\),

\[\int_{\mathbb{R}}hd\mu_{F}=\int hfdm\]

where \(f\) is a nonnegative measurable function such that \(F(b)-F(a)=\int_{[a,b]}fdm\) for any \(a<b\).

[MISSING_PAGE_FAIL:156]

4. Differentiation

4.21 Let \(F:\mathbb{R}\to\mathbb{R}\) be absolutely continuous on every finite interval. * Show that the \(f\) in (4.1) can be chosen independently of the interval \([a,b]\). * Further, if \(f\) is integrable over \(\mathbb{R}\), then \(\lim_{x\to-\infty}F(x)\equiv F(-\infty)\) and \(\lim_{x\to\infty}F(x)\equiv F(\infty)\) exist and \(F(x)=F(-\infty)+\int_{(-\infty,x)}fd\mu_{L}\) for all \(x\) in \(\mathbb{R}\). * Give an example where \(F:\mathbb{R}\to\mathbb{R}\) is a.c., but \(f\) is not integrable over \(\mathbb{R}\).
4.22 Let \(F:\mathbb{R}\to\mathbb{R}\) be absolutely continuous on bounded intervals. Let \(\{I_{j}=1\leq j\leq k\leq\infty\}\) be a collection of disjoint intervals such that \(\bigcup_{j=1}^{k}I_{j}\equiv\mathbb{R}\) and on each \(I_{j}\), \(F^{\prime}(\cdot)>0\) a.e. or \(F^{\prime}(\cdot)<0\) a.e. w.r.t. \(m\). * Show that for any \(h\in L^{1}(\mathbb{R},m)\), \[\int_{\mathbb{R}}hdm=\int_{\mathbb{R}}h\big{(}F(\cdot)\big{)}|F^{\prime}(\cdot )|dm.\] * Show that if \(\mu\) is a measure on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\) dominated by \(m\) then the measure \(\mu F^{-1}(\cdot)\) is also dominated by \(m\) and \[\frac{d\mu F^{-1}}{dm}(y)=\sum_{x_{j}\in D(y)}\frac{f(x_{j})}{|F^{\prime}(x_{j })|}\] where \(f(\cdot)=\frac{d\mu}{dm}\) and \(D(y)=\{x_{j}:x_{j}\in I_{j},F(x_{j})=y\}\). * Let \(\mu\) be the \(N(0,1)\) measure on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\), i.e., \[\frac{d\mu}{dm}(x)=\frac{1}{\sqrt{2n}}\ e^{-\frac{x^{2}}{2}},\ -\infty<x<\infty.\] Let \(F(x)=x^{2}\). Find \(\frac{d\mu F^{-1}}{dm}\).
4.23 Let \(f:\mathbb{R}\to\mathbb{R}\) be integrable w.r.t. \(m\) on bounded intervals. Show that for almost all \(x_{0}\) in \(\mathbb{R}\) (w.r.t. \(m\)), \[\lim_{\stackrel{{ a\uparrow x_{0}}}{{b\downarrow x_{0}}}}\frac{1} {(b-a)}\int_{a}^{b}|f(x)-f(x_{0})|dx=0.\] (**Hint:** For each rational \(r\), by Theorem 4.4.4 \[\lim_{\stackrel{{ a\uparrow x_{0}}}{{b\downarrow x_{0}}}}\frac{1} {(b-a)}\int_{a}^{b}|f(x)-r|dx=|f(x_{0})-r|.\]a.e. \((m)\). Let \(A_{r}\) denote the set of \(x_{0}\) for which this fails to hold. Let \(A=\bigcup_{r\in\mathbb{Q}}A_{r}\). Then \(m(A)=0\). For any \(x_{0}\not\in A\) and any \(\epsilon>0\), choose a rational \(r\) such that \(|f(x_{0})-r|<\epsilon\) and now show that \[\lim_{{a\neq\mathbb{T}_{0}}\atop{b\downarrow x_{0}}}{1\over(b-a)}\int_{a}^{b}| f(x)-f(x_{0})|dx<\epsilon.\ )\]
4.24 Use the hint to the above problem to establish Theorem 4.4.5.
4.25 Let \((\Omega,\mathcal{B})\) be a measurable space. Let \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) be \(\sigma\)-finite measures on \((\Omega,\mathcal{B})\). Let for each \(n\geq 1\), \(\mu_{n}=\mu_{na}+\mu_{ns}\) be the Lebesgue decomposition of \(\mu_{n}\) w.r.t. \(\mu\) with \(\mu_{na}\ll\mu\) and \(\mu_{ns}\perp\mu\). Let \(\lambda=\sum_{n\geq 1}\mu_{n}\), \(\lambda_{a}=\sum_{n\geq 1}\mu_{na}\), \(\lambda_{s}=\sum_{n\geq 1}\mu_{ns}\). Show that \(\lambda_{a}\ll\mu\) and \(\lambda_{s}\perp\mu\) and that \(\lambda=\lambda_{a}+\lambda_{s}\) is the Lebesgue decomposition of \(\lambda\) w.r.t. \(\mu\).
4.26 Let \(\{\mu_{n}\}_{n\geq 1}\) be Radon measures on \(\bigl{(}\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k})\bigr{)}\) and \(m\) be the Lebesgue measure on \(\mathbb{R}^{k}\). Show that if \(\lambda=\sum_{n=1}^{\infty}\mu_{n}\) is also a Radon measure, then \[D\lambda=\sum_{n=1}^{\infty}D\mu_{n}\quad\mbox{a.e.}\quad(m).\] (**Hint:** Use Theorem 4.4.4 and the uniqueness of the Lebesgue decomposition.)
4.27 Let \(F_{n}\), \(n\geq 1\) be a sequence of nondecreasing functions from \(\mathbb{R}\to\mathbb{R}\). Let \(F(x)=\sum_{n\geq 1}F_{n}(x)<\infty\) for all \(x\in\mathbb{R}\). Show that \(F(\cdot)\) is nondecreasing and \[F^{\prime}(\cdot)=\sum_{n\geq 1}F^{\prime}_{n}(\cdot)\quad\mbox{a.e.}\quad(m).\]
4.28 Let \(E\) be a Lebesgue measurable set in \(\mathbb{R}\). The _metric density_ of \(E\) at \(x\) is defined as \[D_{E}(x)\equiv\lim_{\delta\downarrow 0}{m\bigl{(}E\cap(x-\delta,x+\delta) \bigr{)}\over 2\delta}\] if it exists. Show that \(D_{E}(\cdot)=I_{E}(\cdot)\) a.e. \(m\). (**Hint:** Consider the measure \(\lambda_{E}(\cdot)\equiv m(E\cap\cdot)\) on the Lebesgue \(\sigma\)-algebra. Show that \(\lambda_{E}\ll m\) and find \(\lambda^{\prime}_{E}(\cdot)\) (cf. Definition 4.4.2).)
4.29 Let \(F\), \(G:[a,b]\to\mathbb{R}\) be both absolutely continuous. Show that \(H=FG\) is also absolutely continuous on \([a,b]\) and that \[\int_{[a,b]}FdG+\int_{[a,b]}GdF=F(b)G(b)-F(a)G(a).\]4.30 Let \((\Omega,\mathcal{F},\mu)\) be a finite measure space. Fix \(1\leq p<\infty\). Let \(T:L^{p}(\mu)\to\mathbb{R}\) be a bounded linear functional as defined in (3.2.10) (cf. Section 3.2). Complete the following outline of a proof of Theorem 3.2.3 (Riesz representation theorem). 1. Let \(\nu(A)\equiv T(I_{A})\), \(A\in\mathcal{F}\). Verify that \(\nu(\cdot)\) is a signed measure on \((\Omega,\mathcal{F})\). 2. Verify that \(|\nu|\ll\mu\). 3. Let \(g\equiv\frac{d\nu}{d\mu}\). Show that \(g\in L^{q}(\mu)\) where \(q=\frac{p}{p-1}\), \(1<p<\infty\) and \(q=\infty\) if \(p=1\). 4. Show that \(T=T_{g}\).
4.31 Prove Theorem 4.5.2 and Corollary 4.5.3.
4.32 For \(0<\alpha<1\), construct a Cantor like set \(C_{\alpha}\) as described in Remark 4.5.1.
4.33 Show that the Cantor ternary function can be expressed as in (5.5).
4.34 Let \((\Omega,\mathcal{F},\mu)\) be a \(\sigma\)-finite measure space. 1. Let \(\mathcal{G}\) be a \(\sigma\)-algebra \(\subset\mathcal{F}\). Let \(f:\Omega\to\mathbb{R}_{+}\) be \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable. Show that there exists a \(g:\Omega\to\mathbb{R}_{+}\) that is \(\langle\mathcal{G},\mathcal{B}(\mathbb{R})\rangle\)-measurable and \(\nu(A)\equiv\int_{A}fd\mu=\int_{A}gd\mu\) for all \(A\) in \(\mathcal{G}\). 2. (**Hint:** Apply Theorem 4.1.1 (b) to the measures \(\nu\) and \(\mu\) restricted to \(\mathcal{G}\). When \(\mu\) is a probability measure, \(g\) is called the conditional expectation of \(f\) given \(\mathcal{G}\) (cf. Chapter 12).) 3. Now suppose \(\mathcal{G}=\sigma\langle\{A_{i}\}_{i\geq 1}\rangle\) where \(\{A_{i}\}_{i\geq 1}\) is a partition of \(\Omega\subset\mathcal{F}\). Determine \(g(\cdot)\) explicitly on each \(A_{i}\) such that \(0<\mu(A_{i})<\infty\).

\begin{table}
\begin{tabular}{l|c} \hline Mean \(\mu\) & \(\mu(A),\ A\in\mathcal{B}(\mathbb{R})\) \\ \hline Bernoulli (\(p\)), & \(\sum_{i=0}^{1}p^{i}(1-p)^{1-i}I_{A}(i)\) \\ \(0<p<1\) & \\ Binomial (\(n,p\)), & \(\sum_{i=0}^{n}\binom{n}{i}p^{i}(1-p)^{n-i}I_{A}(i)\) \\ \(0<p<1\), \(n\in\mathbb{N}\) & \\ Geometric (\(p\)), & \(\sum_{i=0}^{\infty}p(1-p)^{i-1}I_{A}(i)\) \\ \(0<p<1\) & \\ Poisson (\(\lambda\)), & \(\sum_{i=0}^{\infty}e^{-\lambda}\frac{\lambda^{i}}{i!}\cdot I_{A}(i)\) \\ \(0<\lambda<\infty\) & \\ \hline \end{tabular}
\end{table}
Table 4.6.1: Some discrete univariate distributions.

\begin{table}
\begin{tabular}{l|c} \hline Measure \(\mu\) & \(\mu(A)\), \(A\in\mathcal{B}(\mathbb{R})\) \\ \hline Uniform \((a,b)\), & \(m(A\cap[a,b])/(b-a)\) \\ \(-\infty<a<b<\infty\) & \\ Exponential \((\beta)\), & \(\int_{A}\frac{1}{\beta}\exp(-x/\beta)I_{(0,\infty)}(x)m(dx)\) \\ \(\beta\in(0,\infty)\) & \(\int_{A}\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1}\exp(-x/\beta)I_{(0,\infty)}(x)m(dx)\) \\ Gamma \((\alpha,\beta)\), & where \(\Gamma(a)=\int_{0}^{\infty}x^{a-1}e^{-x}dx\), \(a\in(0,\infty)\) \\ Beta \((\alpha,\beta)\), & \(\frac{1}{B(\alpha,\beta)}\int_{A}x^{\alpha-1}(1-x)^{\beta-1}I_{(0,1)}(x)m(dx)\), \\ \(\alpha,\beta\in(0,\infty)\) & where \(B(a,b)=\Gamma(a)\Gamma(b)/\Gamma(a+b)\), \(a,b\in(0,\infty)\) \\ Cauchy \((\gamma,\sigma)\) & \(\int_{A}\frac{1}{\pi\sigma}\ \frac{\sigma^{2}}{\sigma^{2}+(x-\gamma)^{2}}\ m(dx)\) \\ \(\gamma\in\mathbb{R}\), \(\sigma\in(0,\infty)\) & \(\int_{A}\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x-\gamma)^{2}/\sigma^{2})m(dx)\) \\ Lognormal \((\gamma,\sigma^{2})\), & \(\int_{A}\frac{1}{\sqrt{2\pi}\sigma}\ \frac{e^{-(\log x-\gamma)^{2}/2\sigma^{2}}}{x}\ I_{(0, \infty)}(x)m(dx)\) \\ \end{tabular}
\end{table}
Table 4.6.2: Some standard absolutely continuous distributions. Here \(m\) denotes the Lebesgue measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\).

Product Measures, Convolutions, and Transforms

### 5.1 Product spaces and product measures

Given two measure spaces \((\Omega_{i},{\cal F}_{i},\mu_{i})\), \(i=1,2\), is it possible to construct a measure \(\mu\equiv\mu_{1}\times\mu_{2}\) on a \(\sigma\)-algebra on the product space \(\Omega_{1}\times\Omega_{2}\) such that \(\mu(A\times B)=\mu_{1}(A)\mu_{2}(B)\) for \(A\in{\cal F}_{1}\) and \(B\in{\cal F}_{2}\)? This section is devoted to studying this question.

**Definition 5.1.1:** Let \((\Omega_{i},{\cal F}_{i})\), \(i=1,2\) be measurable spaces.

* \(\Omega_{1}\times\Omega_{2}\equiv\{(\omega_{1},\omega_{2}):\omega_{1}\in\Omega_ {1},\omega_{2}\in\Omega_{2}\}\), the set of all ordered pairs, is called the _(Cartesian) product of \(\Omega_{1}\) and \(\Omega_{2}\)_.
* The set \(A_{1}\times A_{2}\) with \(A_{1}\in{\cal F}_{1},A_{2}\in{\cal F}_{2}\) is called a _measurable rectangle_. The collection of measurable rectangles will be denoted by \({\cal C}\).
* The _product \(\sigma\)-algebra of \({\cal F}_{1}\) and \({\cal F}_{2}\)_ on \(\Omega_{1}\times\Omega_{2}\), denoted by \({\cal F}_{1},\times{\cal F}_{2}\), is the smallest \(\sigma\)-algebra generated by \({\cal C}\), i.e., \[{\cal F}_{1}\times{\cal F}_{2}\equiv\sigma\langle\{A_{1}\times A_{2}:A_{1}\in {\cal F}_{1},A_{2}\in{\cal F}_{2}\}\rangle.\]
* \((\Omega_{1}\times\Omega_{2},{\cal F}_{1}\times{\cal F}_{2})\) is called the _product measurable space_.

Starting with the definition of \(\mu\) on the class \({\cal C}\) of measurable rectangles by

\[\mu(A_{1}\times A_{2})=\mu_{1}(A_{1})\mu_{2}(A_{2}) \tag{1.1}\]for all \(A_{1}\in{\cal F}_{1},A_{2}\in{\cal F}_{2}\), one can extend it to a measure on the algebra \({\cal A}\) of all finite unions of disjoint measurable rectangles in a natural way. Indeed, the extension to \({\cal A}\) is obtained simply by assigning the \(\mu\)-measure of a finite union of disjoint measurable rectangles as the sum of the \(\mu\)-measures of the corresponding individual measurable rectangles. Then, by the extension theorem (cf. Theorem 1.3.3), it can be further extended to a complete measure \(\mu\) on a \(\sigma\)-algebra containing \({\cal F}_{1}\times{\cal F}_{2}\), defined in (c) above. However, to evaluate \(\mu(A)\) for an arbitrary \(A\) in \({\cal F}_{1}\times{\cal F}_{2}\) that is not a measurable rectangle and to evaluate \(\int hd\mu\) for arbitrary measurable functions \(h\), some further work is needed. For the case of the product of two measure spaces, here an alternate approach that yields a direct way of computing these quantities is presented. The extension theorem approach will be used for the more general case of products of finitely many measure spaces in Section 5.3. The case of products of infinitely many probability spaces will be discussed in Chapter 6.

**Definition 5.1.2:**

* Let \(A\in{\cal F}_{1}\times{\cal F}_{2}\). Then, for any \(\omega_{1}\in\Omega_{1}\), the set \[A_{1\omega_{1}}\equiv\{\omega_{2}\in\Omega_{2}:(\omega_{1},\omega_{2})\in A\}\] (1.2) is called the \(\omega_{1}\)_-section of_ \(A\) and for any \(\omega_{2}\in\Omega_{2}\), the set \(A_{2\omega_{2}}\equiv\{\omega_{1}\in\Omega_{1}:(\omega_{1},\omega_{2})\in A\}\) is called the \(\omega_{2}\)-_section_ of \(A\).
* If \(f:(\Omega_{1}\times\Omega_{2})\to\Omega_{3}\) is a \(\langle{\cal F}_{1}\times{\cal F}_{2},{\cal F}_{3}\rangle\) measurable mapping from \(\Omega_{1}\times\Omega_{2}\) into some measurable space \((\Omega_{3},{\cal F}_{3})\), then the \(\omega_{1}\)_-section of_ \(f\) is the function \(f_{1\omega_{1}}:\Omega_{2}\to\Omega_{3}\), given by \[f_{1\omega_{1}}(\omega_{2})=f(\omega_{1},\omega_{2}),\ \omega_{2}\in\Omega_{2}.\] (1.3)

Similarly, one may define the \(\omega_{2}\)_-sections_ of \(f\) by \(f_{2\omega_{2}}(\omega_{1})=f(\omega_{1},\omega_{2})\), \(\omega_{1}\in\Omega_{1}\). The following result shows that the \(\omega_{1}\)-sections of a \({\cal F}_{1}\times{\cal F}_{2}\)-measurable function is \({\cal F}_{2}\)-measurable when considered as a function of \(\omega_{2}\in\Omega_{2}\).

**Proposition 5.1.1:**_Let \((\Omega_{1}\times\Omega_{2},{\cal F}_{1}\times{\cal F}_{2})\) be a product space, \(A\in{\cal F}_{1}\times{\cal F}_{2}\) and let \(f:\Omega_{1}\times\Omega_{2}\to\Omega_{3}\) be a \(\langle{\cal F}_{1}\times{\cal F}_{2},{\cal F}_{3}\rangle\)-measurable function._

* _For every_ \(\omega_{1}\in\Omega_{1}\)_,_ \(A_{1\omega_{1}}\in{\cal F}_{2}\) _and for every_ \(\omega_{2}\in\Omega_{2}\)_,_ \(A_{2\omega_{2}}\in{\cal F}_{1}\)_._
* _For every_ \(\omega_{1}\in\Omega_{1}\)_,_ \(f_{1\omega_{1}}\) _is_ \(\langle{\cal F}_{2},{\cal F}_{3}\rangle\)_-measurable and for every_ \(\omega_{2}\in\Omega_{2}\)_,_ \(f_{2\omega_{2}}\) _is_ \(\langle{\cal F}_{1},{\cal F}_{3}\rangle\)_-measurable._

**Proof:** Fix \(\omega_{1}\in\Omega_{1}\). Define the function \(g:\Omega_{2}\to\Omega_{1}\times\Omega_{2}\) by

\[g(\omega_{2})=(\omega_{1},\omega_{2}),\ \omega_{2}\in\Omega_{2}.\]Note that for any measurable rectangle \(A\equiv A_{1}\times A_{2}\in{\cal F}_{1}\times{\cal F}_{2}\),

\[A_{1\omega_{1}}=\left\{\begin{array}{lll}A_{2}&\mbox{if}&\omega_{1}\in A_{1}\\ \emptyset&\mbox{if}&\omega_{1}\not\in A_{1}\end{array}\right.\]

and hence \(g^{-1}(A_{1}\times A_{2})\in{\cal F}_{2}\). Since the class of all measurable rectangles generates \({\cal F}_{1}\times{\cal F}_{2}\), for fixed \(\omega_{1}\) in \(\Omega_{1}\), \(g\) is \(\langle{\cal F}_{2},{\cal F}_{1}\times{\cal F}_{2}\rangle\)-measurable. Therefore, for \(A\) in \({\cal F}_{1}\times{\cal F}_{2}\), \(A_{1\omega_{1}}=g^{-1}(A)\in{\cal F}_{2}\) and for \(f\) as given, \(f_{1\omega_{1}}=f\circ g\) is \(\langle{\cal F}_{2},{\cal F}_{3}\rangle\)-measurable. This proves (i) and (ii) for the \(\omega_{1}\)-sections. The proof for the \(\omega_{2}\)-sections are similar. \(\Box\)

Now suppose \(\mu_{1}\) and \(\mu_{2}\) are measures on \((\Omega_{1},{\cal F}_{1})\) and \((\Omega_{2},{\cal F}_{2})\), respectively. Then, for any set \(A\in{\cal F}_{1}\times{\cal F}_{2}\), for all \(\omega_{1}\in\Omega_{1}\), \(A_{1\omega_{1}}\in{\cal F}_{2}\) and hence \(\mu_{2}(A_{1\omega_{1}})\) is well defined. If this were an \({\cal F}_{1}\)-measurable function, then one might define a set function on \({\cal F}_{1}\times{\cal F}_{2}\) by

\[\mu_{12}(A)=\int_{\Omega_{1}}\mu_{2}(A_{1\omega_{1}})\mu_{1}(d\omega_{1}). \tag{1.4}\]

And similarly, reversing the order of \(\mu_{1}\) and \(\mu_{2}\), one might define a second set function

\[\mu_{21}(A)=\int_{\Omega_{2}}\mu_{1}(A_{2\omega_{2}})\mu_{2}(d\omega_{2}), \tag{1.5}\]

provided that \(\mu_{1}(A_{2\omega_{2}})\) is \({\cal F}_{2}\)-measurable. Note that for the measurable rectangles \(A=A_{1}\times A_{2}\), \(\mu_{12}(A)=\mu_{1}(A_{1})\mu_{2}(A_{2})=\mu_{21}(A)\) and thus both \(\mu_{12}\) and \(\mu_{21}\) coincide (with the _product_ measure \(\mu\)) on the class \({\cal C}\) of all measurable rectangles. This implies that if the product measure \(\mu\) is _unique_ on \({\cal F}_{1}\times{\cal F}_{2}\), and \(\mu_{12}\) and \(\mu_{21}\) are measures on \({\cal F}_{1}\times{\cal F}_{2}\), then \(\mu_{12},\mu_{21}\) and \(\mu\) must coincide on the \(\sigma\)-algebra \({\cal F}_{1}\times{\cal F}_{2}\). Then, one can evaluate \(\mu(A)\) for any set \(A\in{\cal F}_{1}\times{\cal F}_{2}\) using either of the relations (1.4) or (1.5). The following result makes this heuristic discussion rigorous.

**Theorem 5.1.2:**_Let \((\Omega_{i},{\cal F}_{i},\mu_{i})\), \(i=1,2\) be \(\sigma\)-finite measure spaces. Then,_

* _for all_ \(A\in{\cal F}_{1}\times{\cal F}_{2}\)_, the functions_ \(\mu_{2}(A_{1\omega_{1}})\) _and_ \(\mu_{1}(A_{2\omega_{2}})\) _are_ \({\cal F}_{1}\)_- and_ \({\cal F}_{2}\)_-measurable, respectively._
* _The set functions_ \(\mu_{12}\) _and_ \(\mu_{21}\)_, given by (_1.4_) and (_1.5_) respectively, are measures on_ \({\cal F}_{1}\times{\cal F}_{2}\)_, satisfying_ \(\mu_{12}(A)=\mu_{21}(A)\) _for all_ \(A\in{\cal F}_{1}\times{\cal F}_{2}\)_._
* _Further,_ \(\mu_{12}=\mu_{21}\equiv\mu\) _is_ \(\sigma\)_-finite and it is the only measure satisfying_ \[\mu(A_{1}\times A_{2})=\mu_{1}(A_{1})\mu_{2}(A_{2})\quad\mbox{for all}\quad A _{1}\times A_{2}\in{\cal C}.\]

**Proof:** First suppose that \(\mu_{1}\) and \(\mu_{2}\) are finite measures. Define

\[{\cal L}=\big{\{}A\in{\cal F}_{1}\times{\cal F}_{2}:\mu_{2}(A_{1\omega_{1}}) \quad\mbox{is a}\quad\langle{\cal F}_{1},{\cal B}({\mathbb{R}})\rangle\mbox{- measurable function}\big{\}}.\]For \(A=\Omega_{1}\times\Omega_{2}\), \(\mu_{2}(A_{1\omega_{1}})\equiv\mu_{2}(\Omega_{2})\) for all \(\omega_{1}\in\Omega_{1}\) and hence \(\Omega_{1}\times\Omega_{2}\in{\cal L}\). Next, let \(A,B\in{\cal L}\) with \(A\subset B\). Then, it is easy to check that \((A\setminus B)_{1\omega_{1}}=A_{1\omega_{1}}\setminus B_{1\omega_{1}}\). Since \(\mu_{2}\) is a finite measure and \(A,B\in{\cal L}\), it follows that \(\mu_{2}((A\setminus B)_{1\omega_{1}})=\mu_{2}(A_{1\omega_{1}}\setminus B_{1 \omega_{1}})=\mu_{2}(A_{1\omega_{1}})-\mu_{2}(B_{1\omega_{1}})\) is \(\langle{\cal F}_{1},{\cal B}({\mathbb{R}})\rangle\)-measurable. Thus, \(A\setminus B\in{\cal L}\). Finally, let \(\{B_{n}\}_{n\geq 1}\subset{\cal L}\) be such that \(B_{n}\subset B_{n+1}\) for all \(n\geq 1\). Then for any \(\omega_{1}\in\Omega_{1}\), \((B_{n})_{1\omega_{1}}\subset(B_{n+1})_{1\omega_{1}}\) for all \(n\geq 1\). Hence by the MCT,

\[\infty>\mu_{2}\Big{(}\Big{(}\bigcup_{n\geq 1}B_{n}\Big{)}_{1\omega_{1}}\Big{)}= \mu_{2}\Big{(}\bigcup_{n\geq 1}(B_{n})_{1\omega_{1}}\Big{)}=\lim_{n\to\infty} \mu_{2}\big{(}(B_{n})_{1\omega_{1}}\big{)}\]

for all \(\omega_{1}\in\Omega_{1}\). By Proposition 2.1.5, this implies that \(\mu_{2}\big{(}\big{(}\bigcup_{n\geq 1}B_{n})_{1\omega_{1}}\big{)}\) is \(\langle{\cal F}_{1},{\cal B}({\mathbb{R}})\rangle\)-measurable, and hence \(\bigcup_{n\geq 1}B_{n}\in{\cal L}\). Thus, \({\cal L}\) is a \(\lambda\)-system. For \(A=A_{1}\times A_{2}\in{\cal C}\), \(\mu_{2}(A_{1\omega_{1}})=\mu_{2}(A_{2})I_{A_{1}}(\omega_{1})\) and hence \({\cal C}\subset{\cal L}\). Since \({\cal C}\) is a \(\pi\)-system, by Corollary 1.1.3, it follows that \({\cal L}={\cal F}_{1}\times{\cal F}_{2}\). Thus, \(\mu_{2}(A_{1\omega_{1}})\), considered as a function of \(\omega_{1}\), is \(\langle{\cal F}_{1},{\cal B}({\mathbb{R}})\rangle\)-measurable for all \(A\in{\cal F}_{1}\times{\cal F}_{2}\), proving (i).

Next, consider part (ii). By part (i), \(\mu_{12}\) is a well-defined set function on \({\cal F}_{1}\times{\cal F}_{2}\). It is easy to check that \(\mu_{12}\) is a measure on \({\cal F}_{1}\times{\cal F}_{2}\) (Problem 5.1). Similarly, \(\mu_{21}\) is a well-defined measure on \({\cal F}_{1}\times{\cal F}_{2}\). Since \(\mu_{12}(A)=\mu_{21}(A)=\mu_{1}(A_{1})\mu_{2}(A_{2})\) for all \(A=A_{1}\times A_{2}\in{\cal C}\) and \({\cal C}\) is a \(\pi\)-system generating \({\cal F}_{1}\times{\cal F}_{2}\), it follows from Theorem 1.2.4 that \(\mu_{12}(A)=\mu_{21}(A)\) for all \(A\in{\cal F}_{1}\times{\cal F}_{2}\). This proves (ii) for the case where \(\mu_{i}(\Omega_{i})<\infty,\ i=1,2\).

Next, suppose that \(\mu_{i}\)'s are \(\sigma\)-finite. Then, there exist disjoint sets \(\{B_{in}\}_{n\geq 1}\subset{\cal F}_{i}\), such that \(\bigcup_{n\geq 1}B_{in}=\Omega_{i}\) and \(\mu_{i}(B_{in})<\infty\) for all \(n\geq 1\), \(i=1,2\). Define the finite measures

\[\mu_{in}(D)=\mu_{i}(D\cap B_{in}),\quad D\in{\cal F}_{i},\]

for \(n\geq 1,\,i=1,2\). The arguments above with \(\mu_{i}\) replaced by \(\mu_{in}\) imply that for any \(A\in{\cal F}_{1}\times{\cal F}_{2}\), \(\mu_{2n}(A_{1\omega_{1}})\) is \(\langle{\cal F}_{1},{\cal B}({\mathbb{R}})\rangle\)-measurable for all \(n\geq 1\). Since \(\mu_{2}\) is a measure on \({\cal F}_{2}\), \(\mu_{2}(A_{1\omega_{1}})=\sum_{n=1}^{\infty}\mu_{2n}(A_{\omega_{1}})\) and hence, considered as a function of \(\omega_{1}\), it is \(\langle{\cal F}_{1},{\cal B}({\mathbb{R}})\rangle\)-measurable for all \(A\in{\cal F}_{1}\times{\cal F}_{2}\). Thus, the set function \(\mu_{12}\) of (1.4) is well defined in the \(\sigma\)-finite case as well. Similarly, \(\mu_{21}\) of (1.5) is a well-defined set function on \({\cal F}_{1}\times{\cal F}_{2}\). Let \(\mu_{12}^{(m,n)}\) and \(\mu_{21}^{(m,n)}\), respectively, denote the set functions defined by (1.4) and (1.5) with \(\mu_{1}\) replaced by \(\mu_{1m}\) and \(\mu_{2}\) replaced by \(\mu_{2n}\), \(m\geq 1\), \(n\geq 1\). Then, by repeated use of the MCT,

\[\mu_{12}(A) = \int_{\Omega_{1}}\mu_{2}(A_{1\omega_{1}})\mu_{1}(d\omega_{1})\] \[= \sum_{m=1}^{\infty}\bigg{(}\int_{B_{1m}}\sum_{n=1}^{\infty}\mu_ {2}(A_{1\omega_{1}}\cap B_{2n})\bigg{)}\mu_{1}(d\omega_{1})\] \[= \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}\int_{B_{1m}}\mu_{2}(A_{1 \omega_{1}}\cap B_{2n})\mu_{1}(d\omega_{1})\]\[= \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}\mu_{12}^{(m,n)}(A),\quad A\in \mathcal{F}_{1}\times\mathcal{F}_{2}\]

and similarly,

\[\mu_{21}(A)=\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}\mu_{21}^{(m,n)}(A),\quad A\in \mathcal{F}_{1}\times\mathcal{F}_{2}.\]

Since \(\mu_{12}^{(m,n)}\) and \(\mu_{21}^{(m,n)}\) are (finite) measures, it is easy to check that \(\mu_{12}\) and \(\mu_{21}\) are measures on \(\mathcal{F}_{1}\times\mathcal{F}_{2}\). Also, by the finite case, \(\mu_{12}^{(m,n)}(A_{1}\times A_{2})=\mu_{21}^{(m,n)}(A_{1}\times A_{2})\) for all \(n\geq 1\), \(m\geq 1\) and hence

\[\mu_{12}(A_{1}\times A_{2})=\mu_{21}(A_{1}\times A_{2})\quad\mbox{for all} \quad A_{1}\times A_{2}\in\mathcal{C}.\]

Next note that \(\{B_{1m}\times B_{2n}:m\geq 1,n\geq 1\}\) is a partition of \(\Omega_{1}\times\Omega_{2}\) by \(\mathcal{F}_{1}\times\mathcal{F}_{2}\) sets and by (1.6) and (1.7), for all \(m\geq 1\), \(n\geq 1\),

\[\mu_{12}(B_{1m}\times B_{2n})=\mu_{1}(B_{1m})\mu_{2}(B_{2n})=\mu_{21}(B_{1m} \times B_{2n})<\infty.\]

Hence, \(\mu_{12}\) and \(\mu_{21}\) are \(\sigma\)-finite on \(\mathcal{F}_{1}\times\mathcal{F}_{2}\). Since \(\mu_{12}\) and \(\mu_{21}\) agree on \(\mathcal{C}\) and \(\mathcal{C}\) is a \(\pi\)-system generating the product \(\sigma\)-algebra, it follows that \(\mu_{12}(A)=\mu_{21}(A)\) for all \(A\in\mathcal{F}_{1}\times\mathcal{F}_{2}\) and it is the unique measure satisfying \(\mu(A_{1}\times A_{2})=\mu_{1}(A_{1})\mu_{2}(A_{2})\) for all \(A_{1}\times A_{2}\in\mathcal{C}\). This completes the proof of the theorem. \(\Box\)

**Remark 5.1.1:** In the above theorem, the \(\sigma\)-finiteness condition on the measures \(\mu_{1}\) and \(\mu_{2}\) cannot be dropped. For example, let \(\Omega_{1}=\Omega_{2}=[0,1]\), \(\mathcal{F}_{1}=\mathcal{F}_{2}=\mathcal{B}([0,1])\), \(\mu_{1}=\) the Lebesgue measure and \(\mu_{2}=\) the counting measure. Clearly \(\mu_{2}\) is not \(\sigma\)-finite since \([0,1]\) is uncountable. Let \(A\) be the diagonal set in the product space \([0,1]\times[0,1]\), i.e., \(A=\{(\omega_{1},\omega_{2})\in\Omega_{1}\times\Omega_{2}:\omega_{1}=\omega_{2}\}\). Then \(A\in\mathcal{F}_{1}\times\mathcal{F}_{2}\), \(A_{1\omega_{1}}=\{\omega_{1}\}\) and \(A_{2\omega_{2}}=\{\omega_{2}\}\). Further, \(\mu_{2}(A_{1\omega_{1}})=1\) for all \(\omega_{1}\) and \(\mu_{1}(A_{2\omega_{2}})=0\) for all \(\omega_{2}\). Thus \(\mu_{12}(A)=\int_{\Omega_{1}}\mu_{2}(A_{1\omega_{1}})\mu_{1}(d\omega_{1})=1\), but \(\mu_{21}(A)=\int_{\Omega_{2}}\mu_{1}(A_{2\omega_{2}})\mu_{2}(d\omega_{2})=0\), and hence, \(\mu_{12}(A)\neq\mu_{21}(A)\).

**Remark 5.1.2:** Although this approach allows one to compute the product measure \(\mu_{1}\times\mu_{2}\) of a set \(A\in\mathcal{F}_{1}\times\mathcal{F}_{2}\), the measure space \((\Omega_{1}\times\Omega_{2},\mathcal{F}_{1}\times\mathcal{F}_{2},\mu_{1}\times \mu_{2})\) may not be complete even if both \((\Omega_{i},\mathcal{F}_{i},\mu_{i})\), \(i=1,2\) are complete (Problem 5.2). However, the approach based on the extension theorem yields a product measure space that is complete. See Remark 5.2.2 for further discussion on the topic.

**Definition 5.1.3:**

* The unique measure \(\mu\) on \(\mathcal{F}_{1}\times\mathcal{F}_{2}\) defined in Theorem 5.1.2 (iii) is called the _product measure_ and is denoted by \(\mu_{1}\times\mu_{2}\).

2. The measure space \((\Omega_{1}\times\Omega_{2},\mathcal{F}_{1}\times\mathcal{F}_{2},\mu_{1}\times\mu_ {2})\) is called the _product measure space_.

Formulas (1.4) and (1.5) give two different ways of evaluating \(\mu_{1}\times\mu_{2}(A)\) for an \(A\in\mathcal{F}_{1}\times\mathcal{F}_{2}\). In the next section, integration of a measurable function \(f:\Omega_{1}\times\Omega_{2}\to\mathbb{R}\) w.r.t. the product measure \(\mu_{1}\times\mu_{2}\) is considered and the above approach is extended to justify evaluation of the integral iteratively.

### 5.2 Fubini-Tonelli theorems

Let \(f:\Omega_{1}\times\Omega_{2}\to\mathbb{R}\) be a \(\langle\mathcal{F}_{1}\times\mathcal{F}_{2},\mathcal{B}(\mathbb{R})\rangle\)-measurable function. Relations (1.4) and (1.5) suggest that the integral of \(f\) w.r.t. \(\mu_{1}\times\mu_{2}\) may be evaluated as iterated integrals, using the formulas

\[\int_{\Omega_{1}\times\Omega_{2}}f(\omega_{1},\omega_{2})\mu_{1}\times\mu_{2}( d(\omega_{1},\omega_{2}))=\int_{\Omega_{2}}\left[\int_{\Omega_{1}}f(\omega_{1}, \omega_{2})\mu_{1}(d\omega_{1})\right]\mu_{2}(d\omega_{2}) \tag{2.1}\]

and

\[\int_{\Omega_{1}\times\Omega_{2}}f(\omega_{1},\omega_{2})\mu_{1}\times\mu_{2}( d(\omega_{1},\omega_{2}))=\int_{\Omega_{1}}\left[\int_{\Omega_{2}}f(\omega_{1}, \omega_{2})\mu_{2}(d\omega_{2})\right]\mu_{1}(d\omega_{1}). \tag{2.2}\]

Here, the left sides of both (2.1) and (2.2) are simply the integral of \(f\) on the space \(\Omega=\Omega_{1}\times\Omega_{2}\) w.r.t. the measure \(\mu=\mu_{1}\times\mu_{2}\). The expressions on the right sides of (2.1) and (2.2) are, however, _iterated integrals_, where integrals of sections of \(f\) are evaluated first and then the resulting _sectional integrals_ are integrated again to get the final expression. Conditions for the validity of (2.1) and (2.2) are provided by the Fubini-Tonelli theorems stated below.

**Theorem 5.2.1:** (_Tonelli's theorem_)_. Let \((\Omega_{i},\mathcal{F}_{i},\mu_{i})\), \(i=1,2\) be \(\sigma\)-finite measure spaces and let \(f:\Omega_{1}\times\Omega_{2}\to\mathbb{R}_{+}\) be a nonnegative \(\mathcal{F}_{1}\times\mathcal{F}_{2}\)-measurable function. Then_

\[g_{1}(\omega_{1})\equiv\int_{\Omega_{2}}f(\omega_{1},\omega_{2})\mu_{2}(d \omega_{2}):\Omega_{1}\to\bar{\mathbb{R}}\quad\text{is}\quad\langle\mathcal{F }_{1},\mathcal{B}(\bar{\mathbb{R}})\rangle\text{-measurable} \tag{2.3}\]

_and_

\[g_{2}(\omega_{2})\equiv\int_{\Omega_{1}}f(\omega_{1},\omega_{2})\mu_{1}(d \omega_{1}):\Omega_{2}\to\bar{\mathbb{R}}\quad\text{is}\quad\langle\mathcal{ F}_{2},\mathcal{B}(\bar{\mathbb{R}})\rangle\text{-measurable.} \tag{2.4}\]

_Further_

\[\int_{\Omega_{1}\times\Omega_{2}}fd\mu=\int_{\Omega_{1}}g_{1}d\mu_{1}=\int_{ \Omega_{2}}g_{2}d\mu_{2}, \tag{2.5}\]_where \(\mu=\mu_{1}\times\mu_{2}\)._

**Proof:** If \(f=I_{A}\) for some \(A\) in \({\cal F}_{1}\times{\cal F}_{2}\), the result follows from Theorem 5.1.2. By the linearity of integrals, the result now holds for all simple nonnegative functions \(f\). For a general nonnegative function \(f\), there exist a sequence \(\{f_{n}\}_{n\geq 1}\) of nonnegative simple functions such that \(f_{n}(\omega_{1},\omega_{2})\uparrow f(\omega_{1},\omega_{2})\) for all \((\omega_{1},\omega_{2})\in\Omega_{1}\times\Omega_{2}\). Write \(g_{1n}(\omega_{1})=\int_{\Omega_{1}}f_{n}(\omega_{1},\omega_{2})\mu_{2}(d \omega_{2})\). Then, \(g_{1n}\) is \({\cal F}_{1}\)-measurable for all \(n\geq 1\), \(g_{1n}\)'s are nondecreasing, and by the MCT,

\[g_{1}(\omega_{1}) \equiv \int_{\Omega_{1}}f(\omega_{1},\omega_{2})\mu_{2}(d\omega_{2}) \tag{2.6}\] \[= \lim_{n\to\infty}\int f_{n}(\omega_{1},\omega_{2})\mu_{2}(d \omega_{2})\] \[= \lim_{n\to\infty}g_{1n}(\omega_{1})\]

for all \(\omega_{1}\in\Omega_{1}\). Thus, by Proposition 2.1.5, \(g_{1}\) is \(\langle{\cal F}_{1},{\cal B}(\bar{\mathbb{R}})\rangle\)-measurable. Since (2.5) holds for simple functions, \(\int f_{n}d\mu=\int g_{1n}d\mu_{1}\) for all \(n\geq 1\). Hence, by repeated applications of the MCT, it follows that

\[\int fd\mu = \lim_{n\to\infty}\int f_{n}d\mu=\lim_{n\to\infty}\int g_{1n}d\mu_ {1}\] \[= \int(\lim_{n\to\infty}g_{1n})d\mu_{1}=\int g_{1}d\mu_{1}.\]

The proofs of (2.4) and the second equality in (2.5) are similar. \(\Box\)

**Theorem 5.2.2:** (_Fubini's theorem_). _Let \((\Omega_{i},{\cal F}_{i},\mu_{i}),\ i=1,2\) be \(\sigma\)-finite measure spaces and let \(f\in L^{1}(\Omega_{1}\times\Omega_{2},{\cal F}_{1}\times{\cal F}_{2},\mu_{1} \times\mu_{2})\). Then there exist sets \(B_{i}\in{\cal F}_{i}\), \(i=1,2\) such that_

* \(\mu_{i}(\Omega_{i}\setminus B_{i})=0\) _for_ \(i=1,2\)_,_
* _for_ \(\omega_{1}\in B_{1}\)_,_ \(f(\omega_{1},\cdot)\in L^{1}(\Omega_{2},{\cal F}_{2},\mu_{2})\)_,_
* _the function_ \[g_{1}(\omega_{1})\equiv\left\{\begin{array}{ll}\int_{\Omega_{2}}f(\omega_{1},\omega_{2})\mu_{2}(d\omega_{2})&\mbox{for}\quad\omega_{1}\mbox{ in }B_{1}\\ 0&\mbox{for}\quad\omega_{1}\mbox{ in }B_{1}^{c}\end{array}\right.\] _is_ \({\cal F}_{1}\)_-measurable and_ \[\int_{\Omega_{1}}g_{1}d\mu_{1}=\int_{\Omega_{1}\times\Omega_{2}}fd(\mu_{1} \times\mu_{2}),\] (2.7)
* _for_ \(\omega_{2}\in B_{2}\)_,_ \(f(\cdot,\omega_{2})\in L^{1}(\Omega_{1},{\cal F}_{1},\mu_{1})\)_,_* _the function_ \[g_{2}(\omega_{2})\equiv\left\{\begin{array}{ll}\int_{\Omega_{1}}f(\omega_{1}, \omega_{2})\mu_{1}(d\omega_{1})&\mbox{for}\quad\omega_{2}\mbox{ in }B_{2}\\ 0&\mbox{for}\quad\omega_{2}\mbox{ in }B_{2}^{c}\end{array}\right.\] _is_ \({\cal F}_{2}\)_-measurable and_ \[\int_{\Omega_{2}}g_{2}d\mu_{2}=\int_{\Omega_{1}\times\Omega_{2}}fd(\mu_{1} \times\mu_{2}).\] (2.8)

**Remark 5.2.1:** An informal statement of the above theorem is as follows. If \(f\) is integrable on the product space, then the sectional integrals \(\int_{\Omega_{2}}f(\omega_{1},\cdot)d\mu_{2}\) and \(\int_{\Omega_{1}}f(\cdot,\omega_{2})d\mu_{1}\) are well defined a.e., and their integrals w.r.t. \(\mu_{1}\) and \(\mu_{2}\), respectively, are equal to the integral of \(f\) w.r.t. the product measure \(\mu_{1}\times\mu_{2}\).

**Proof:** By Tonelli's theorem

\[\int_{\Omega_{1}\times\Omega_{2}}|f|d(\mu_{1}\times\mu_{2})=\int_{\Omega_{1}} \left(\int_{\Omega_{2}}|f(\omega_{1},\omega_{2})|\mu_{2}(d\omega_{2})\right) \mu_{1}(d\omega_{1}).\]

So \(\int_{\Omega_{1}\times\Omega_{2}}|f|d(\mu_{1}\times\mu_{2})<\infty\) implies that \(\mu_{1}(B_{1}^{c})=0\) where \(B_{1}=\{\omega_{1}:\int|f(\omega_{1},\cdot)|d\mu_{2}<\infty\}\). Also, by Tonelli's theorem

\[g_{11}(\omega_{1})\equiv\int_{\Omega_{2}}f^{+}(\omega_{1},\cdot)d\mu_{2}\quad \mbox{ and }\quad g_{12}(\omega_{1})\equiv\int_{\Omega_{2}}f^{-}(\omega_{1}, \cdot)d\mu_{2}\]

are both \({\cal F}_{1}\)-measurable and

\[\int_{\Omega_{1}}g_{11}d\mu_{1}=\int_{\Omega_{1}\times\Omega_{2}}f^{+}d(\mu_{1 }\times\mu_{2}),\quad\int_{\Omega_{1}}g_{12}d\mu_{1}=\int_{\Omega_{1}\times \Omega_{2}}f^{-}d(\mu_{1}\times\mu_{2}). \tag{2.9}\]

Since \(g_{1}\) defined in (iii) can be written as \(g_{1}=(g_{11}-g_{12})I_{B_{1}}\), \(g_{1}\) is \({\cal F}_{1}\)-measurable. Also,

\[\int_{\Omega_{1}}|g_{1}|d\mu_{1} \leq \int_{\Omega_{1}}g_{11}d\mu_{1}+\int_{\Omega_{1}}g_{12}d\mu_{1}\] \[= \int_{\Omega_{1}\times\Omega_{2}}f^{+}d(\mu_{1}\times\mu_{2})+ \int_{\Omega_{1}\times\Omega_{2}}f^{-}d(\mu_{1}\times\mu_{2})\] \[< \infty.\]

Further, as \(\int_{\Omega_{1}\times\Omega_{2}}|f|d\mu_{1}\times\mu_{2}<\infty\), by (2.9), \(g_{11}\) and \(g_{12}\in L^{1}(\Omega_{1},{\cal F}_{1},\mu_{1})\). Noting that \(\mu_{1}(B_{1}^{c})=0\), one gets

\[\int_{\Omega_{1}}g_{1}d\mu_{1} = \int_{\Omega_{1}}(g_{11}-g_{12})I_{B_{1}}d\mu_{1}\] \[= \int_{\Omega_{1}}g_{11}I_{B_{1}}d\mu_{1}-\int_{\Omega_{1}}g_{12} I_{B_{1}}d\mu_{1}\] \[= \int_{\Omega_{1}}g_{11}d\mu_{1}-\int_{\Omega_{1}}g_{12}d\mu_{1}\]which, by (2.9), equals \(\int_{\Omega_{1}\times\Omega_{2}}f^{+}d(\mu_{1}\times\mu_{2})-\int_{\Omega_{1} \times\Omega_{2}}f^{-}d(\mu_{1}\times\mu_{2})=\int_{\Omega_{1}\times\Omega_{2}} fd(\mu_{1}\times\mu_{2})\). Thus, (ii) and (iii) of the theorem have been established as well as (i) for \(i=1\). The proofs of (iv) and (v) and that of (i) for \(i=2\) are similar. \(\Box\)

An application of the Fubini-Tonelli theorems gives an _integration by parts_ formula. Let \(F_{1}\) and \(F_{2}\) be two nondecreasing right continuous functions on an interval \([a,b]\). Let \(\mu_{i}\) be the Lebesgue-Stieltjes measure on \({\cal B}([a,b])\) corresponding to \(F_{i}\), \(i=1,2\). The 'integration by parts' formula allows one to write \(\int_{(a,b]}F_{1}(x)dF_{2}(x)\equiv\int_{(a,b]}F_{1}d\mu_{2}\) in terms of \(\int_{(a,b]}F_{2}(x)dF_{1}(x)\equiv\int_{(a,b]}F_{2}d\mu_{1}\).

**Theorem 5.2.3:**_Let \(F_{1},F_{2}\) be two nondecreasing right continuous functions on \([a,b]\) with no common points of discontinuity in \((a,b]\). Then_

\[\int_{(a,b]}F_{1}(x)dF_{2}(x)=F_{1}(b)F_{2}(b)-F_{1}(a)F_{2}(a)-\int_{(a,b]}F_{ 2}(x)dF_{1}(x). \tag{2.10}\]

**Proof:** Note that \(\bigl{(}(a,b],{\cal B}(a,b],\mu_{i}\bigr{)}\), \(i=1,2\) are finite measure spaces. Consider the product space \(\bigl{(}(a,b]\times(a,b],{\cal B}((a,b]\times(a,b]),\mu_{1}\times\mu_{2}\bigr{)}\). Define the sets

\[A = \{(x,y):a<x\leq y\leq b\}\] \[B = \{(x,y):a<y\leq x\leq b\}\] \[\mbox{and}\] \[C = \{(x,y):a<x=y\leq b\}.\]

For notational simplicity, write \(A_{x}\) and \(A_{y}\) for the \(x\)-section and the \(y\)-section of \(A\), respectively, and similarly, for the sets \(B\) and \(C\). Since \(F_{1}\) and \(F_{2}\) have no common points of discontinuity, by Theorem 5.1.2

\[\mu_{1}\times\mu_{2}(C) = \int_{(a,b]}\mu_{2}(C_{x})\mu_{1}(dx) \tag{2.11}\] \[= \int_{(a,b]}\mu_{2}(\{x\})\mu_{1}(dx)\] \[= 0.\]

(see Problem 5.3). And by Theorem 5.1.2,

\[\mu_{1}\times\mu_{2}(A) = \int_{(a,b]}\mu_{1}(A_{y})\mu_{2}(dy) \tag{2.12}\] \[= \int_{(a,b]}\mu_{1}((a,y])\mu_{2}(dy)\] \[= \int_{(a,b]}[F_{1}(y)-F_{1}(a)]dF_{2}(y)\]and similarly,

\[\mu_{1}\times\mu_{2}(B) = \int_{(a,b]}\mu_{2}(B_{x})\mu_{1}(dx) \tag{2.13}\] \[= \int_{(a,b]}[F_{2}(x)-F_{2}(a)]dF_{1}(x).\]

Next note that \((\mu_{1}\times\mu_{2})((a,b]\times(a,b])=\mu_{1}((a,b])\cdot\mu_{2}((a,b])=[F_{1}(b)-F_{1}(a )][F_{2}(b)-F_{2}(a)].\) Hence, by (2.11)-(2.13),

\[[F_{1}(b)-F_{1}(a)][F_{2}(b)-F_{2}(a)]\] \[= (\mu_{1}\times\mu_{2})((a,b]\times(a,b])\] \[= \mu_{1}\times\mu_{2}(A)+\mu_{1}\times\mu_{2}(B)-\mu_{1}\times\mu_ {2}(C)\] \[= \int_{(a,b]}[F_{1}(y)-F_{1}(a)]dF_{2}(y)+\int_{(a,b]}[F_{2}(x)-F_ {2}(a)]dF_{1}(x),\]

which yields (2.10), thereby completing the proof of Theorem 5.2.3. \(\Box\)

If \(F_{1}\) and \(F_{2}\) are absolutely continuous with nonnegative densities \(f_{1}\) and \(f_{2}\) w.r.t. the Lebesgue measure on \((a,b],\) then (2.10) yields

\[\int_{a}^{b}F_{1}(x)f_{2}(x)dx=F_{1}(b)F_{2}(b)-F_{1}(a)F_{2}(a)-\int_{a}^{b}F _{2}(x)f_{1}(x)dx. \tag{2.14}\]

If \(f_{1}\) and \(f_{2}\) are any two Lebesgue integrable function on \((a,b]\) that are not necessarily nonnegative, one can decompose \(f_{i}\) as \(f_{i}=f_{i}^{+}-f_{i}^{-}\) and apply (2.14) to \(f_{i}^{+}\)'s and \(f_{i}^{-}\)'s separately. Then, by linearity, it follows that the relation (2.14) also holds for the given \(f_{1}\) and \(f_{2}.\) Thus, the standard 'integration by parts' formula is a special case of Theorem 5.2.3. Relations (2.10) and (2.14) can be extended to unbounded intervals under suitable conditions on \(F_{1}\) and \(F_{2}\) (Problem 5.5).

**Remark 5.2.2:** The measure space \((\Omega_{1}\times\Omega_{2},{\cal F}_{1}\times{\cal F}_{2},\mu_{1}\times\mu_{ 2})\) constructed using the integrals in (1.4) and (1.5) is not necessarily complete. As mentioned at the beginning of this section, the approach based on the extension theorem (applied to the set function defined in (1.1) on the algebra \({\cal A}\) of finite disjoint unions of measurable rectangles) does yield a complete measure space \((\Omega_{1}\times\Omega_{2},{\cal M},\lambda)\) such that \({\cal F}_{1}\times{\cal F}_{2}\subset{\cal M}\) and \(\lambda(A)=(\mu_{1}\times\mu_{2})(A)\) for all \(A\) in \({\cal F}_{1}\times{\cal F}_{2}.\) To compute the integral \(\int fd\lambda\) of an \({\cal M}\)-measurable function \(f\) w.r.t. \(\lambda,\) formula (2.5) in Tonelli's theorem and formulas (2.7) and (2.8) in Fubini's theorem continue to be valid with some modifications. In the following, a modified statement of Fubini's theorem is presented. Similar modification also holds for Tonelli's theorem (cf. Royden (1988), Chapter 12, Section 4).

**Theorem 5.2.4:**_Let \((\Omega_{i},{\cal F}_{i},\mu_{i}),\ i=1,2\) be \(\sigma\)-finite measure spaces and let \(f\in L^{1}(\Omega_{1}\times\Omega_{2},{\cal M},\mu_{1}\times\mu_{2}).\) Let \((\Omega_{i},\bar{{\cal F}}_{i},\bar{\mu}_{i})\) be the completion of \((\Omega_{i},{\cal F}_{i},\mu_{i})\), \(i=1,2\). Then there exist sets \(B_{i}\) in \(\bar{{\cal F}}_{i}\), \(i=1,2\) such that_1. \(\bar{\mu}_{i}(B_{i}^{c})=0\), \(i=1,2\),
2. for \(\omega_{1}\in B_{1}\), \(f(\omega_{1},\cdot)\) is \(\bar{\mathcal{F}}_{2}\)-measurable,
3. \(\int_{\Omega_{2}}|f(\omega_{1},\cdot)|d\bar{\mu}_{2}<\infty\) for all \(\omega_{1}\in B_{1}\),
4. \(g_{1}(\omega_{1})\equiv\left\{\begin{array}{ll}\int_{\Omega_{2}}f(\omega_{1}, \cdot)d\bar{\mu}_{2}&\mbox{for}\quad\omega_{1}\mbox{ in }B,\\ 0&\mbox{for}\quad\omega_{1}\mbox{ in }B_{1}^{c}\end{array}\right.\) _is_ \(\bar{\mathcal{F}}_{1}\)_-measurable,_
5. \(\int_{\Omega_{1}}g_{1}d\bar{\mu}_{1}=\int_{\Omega_{1}\times\Omega_{2}}fd\lambda\)_._

_Further, a similar statement holds for \(i=2\)._

### 5.3 Extensions to products of higher orders

**Definition 5.3.1:** Let \((\Omega_{i},\mathcal{F}_{i})\), \(i=1,\ldots,k\) be measurable spaces, \(2\leq k<\infty\). Then

1. \(\times_{i=1}^{k}\Omega_{i}=\Omega_{1}\times\ldots\times\Omega_{k}=\{(\omega_{ 1},\ldots,\omega_{i}):\omega:\in\Omega_{i}\mbox{ for }i=1,\ldots,k\}\) is called the _product set_ of \(\Omega_{1},\ldots,\Omega_{k}\).
2. A set of the form \(A_{1}\times\ldots\times A_{k}\) with \(A_{i}\in\mathcal{F}_{i},1\leq i\leq k\) is called a _measurable rectangle_. The _product \(\sigma\)-algebra_ on \(\times_{i=1}^{k}\Omega_{i}\), denoted by \(\times_{i=1}^{k}\mathcal{F}_{i}\), is the \(\sigma\)-algebra generated by the collection of all measurable rectangles, i.e., \[\times_{i=1}^{k}\mathcal{F}_{i}=\sigma\langle\{A_{1}\times\ldots\times A_{k}: A_{i}\in\mathcal{F}_{i},1\leq i\leq k\}\rangle.\]
3. \((\times_{i=1}^{k}\Omega_{i},\times_{i=1}^{k}\mathcal{F}_{i})\) is called the _product space_ or the _product measurable space_. When \((\Omega_{i},\mathcal{F}_{i})=(\Omega,\mathcal{F})\) for all \(1\leq i\leq k\), the product space will be denoted by \((\Omega^{k},\mathcal{F}^{k})\).

To define the product measure, one starts with a natural set function on \(\mathcal{C}\), the class of all measurable rectangles, extends it to the algebra \(\mathcal{A}\) of finite disjoint unions of measurable rectangles by linearity, and then uses the extension theorem (Theorem 1.3.3) to obtain a measure on the product space. The details of this construction are now described below.

Define a set function \(\mu\) on \(\mathcal{C}\) by

\[\mu(A)=\prod_{i=1}^{k}\mu(A_{i}) \tag{3.1}\]

where \(A=A_{1}\times A_{2}\times\quad\times A_{k}\) with \(A_{i}\in\mathcal{F}_{i},\ i=1,2,\ldots,k\). Next extend it to the algebra \(\mathcal{A}\) by linearity. If \(B\in\mathcal{A}\) is of the form \(B=\bigcup_{j=1}^{m}B_{j}\) where \(\{B_{j}:1,\ldots,m\}\subset{\cal C}\) are disjoint, then define \(\mu(B)\) by

\[\mu(B)=\sum_{j=1}^{m}\mu(B_{j}). \tag{3.2}\]

If the set \(B\in{\cal A}\) admits two different representations as finite unions of measurable rectangles then it is not difficult to verify that the value of \(\mu(B)\) remains unchanged. Next it is shown that \(\mu\) is a measure on \({\cal A}\).

**Proposition 5.3.1:** _Let \(\mu\) be as defined by (3.1) and (3.2) above on the algebra \({\cal A}\). Then, it is countably additive on \({\cal A}\)._

**Proof:** Let \(\{B_{n}\}_{n=1}^{\infty}\subset{\cal A}\) be disjoint such that \(B=\bigcup_{n\geq 1}B_{n}\) also belongs to \({\cal A}\). It is enough to show that

\[\mu(B)=\sum_{n=1}^{\infty}\mu(B_{n}). \tag{3.3}\]

Let \(B\) and \(\{B_{n}\}_{n=1}^{\infty}\) admit the representations \(B=\bigcup_{i=1}^{\ell}A_{i}\) and \(B_{j}=\bigcup_{r=1}^{\ell_{j}}A_{jr}\) where \(\ell\) and \(\ell_{j}\) are (finite) integers, \(A_{i}\), \(A_{jr}\in{\cal C}\) and each of the collections \(\{A_{i}\}_{i=1}^{\ell}\) and \(\{A_{jr}\}_{r=1}^{\ell_{j}}\), \(j\geq 1\) is disjoint. Then each \(A_{i}\) can be written as

\[A_{i}=A_{i}\cap\Big{[}\bigcup_{n\geq 1}B_{n}\Big{]}=\bigcup_{n\geq 1}\bigcup_{ r=1}^{\ell_{n}}(A_{i}\cap A_{nr}).\]

Suppose it is shown that for each \(i\geq 1\),

\[\mu(A_{i})=\sum_{j=1}^{\infty}\sum_{r=1}^{\ell_{j}}\mu(A_{i}\cap A_{jr}). \tag{3.4}\]

Then, by (3.2),

\[\mu(B) = \sum_{i=1}^{\ell}\mu(A_{i})\] \[= \sum_{i=1}^{\ell}\sum_{j=1}^{\infty}\sum_{r=1}^{\ell_{j}}\mu(A_{i }\cap A_{jr})\] \[= \sum_{j=1}^{\infty}\sum_{r=1}^{\ell_{j}}\sum_{i=1}^{\ell}\mu(A_{ i}\cap A_{jr})\] \[= \sum_{j=1}^{\infty}\sum_{r=1}^{\ell_{j}}\mu(B\cap A_{jr}),\]where the last equality follows from the representation of \(B\cap A_{jr}\) as \(\bigcup_{i=1}^{\ell}(A_{i}\cap A_{jr})\) and the fact that \(A_{i}\cap A_{jr}\in{\cal C}\) for all \(i,j,r\). Since \(A_{jr}\subset B_{j}\subset B\), the above yields

\[\sum_{j=1}^{\infty}\sum_{r=1}^{\ell_{j}}\mu(A_{jr})=\sum_{j=1}^{\infty}\mu(B_{j })\qquad\mbox{(by (\ref{eq:Ajr}))}\]

which establishes (3.3).

Thus, it remains to show (3.4). This is implied by the following:

_Let \(C=\bigcup_{n\geq 1}C_{n}\) where \(\{C_{n}\}_{n=1}^{\infty}\) is a collection of disjoint measurable rectangles and \(C\) is also a measurable rectangle. Then_

\[\mu(C)=\sum_{i=1}^{\infty}\mu(C_{i}). \tag{3.5}\]

Let \(C=A_{1}\times A_{2}\times\quad\times A_{k}\) and \(C_{i}=A_{i1}\times A_{i2}\times\quad\times A_{ik}\), \(i=1,2,\ldots\). Since \(C=\bigcup_{n\geq 1}C_{n}\) and \(\{C_{n}\}_{n=1}^{\infty}\) are disjoint, this implies \(I_{C}(\omega_{1},\ldots,\omega_{k})=\sum_{i=1}^{\infty}I_{C_{i}}(\omega_{1}, \ldots,\omega_{k})\), for all \((\omega_{1},\ldots,\omega_{k})\in\Omega_{1}\times\ldots\times\Omega_{k}\). That is,

\[\prod_{j=1}^{k}I_{A_{j}}(\omega_{j})=\sum_{i=1}^{\infty}\prod_{j=1}^{k}I_{A_{ ij}}(\omega_{j}) \tag{3.6}\]

for all \((\omega_{1},\ldots,\omega_{k})\in\Omega_{1}\times\ldots\times\Omega_{k}\). Integration of both sides of (3.6) w.r.t. \(\mu_{1}\) over \(\Omega_{1}\) yields

\[\mu_{1}(A_{1})\bigg{(}\prod_{j=2}^{k}I_{A_{j}}(\omega_{j})\bigg{)}=\sum_{i=1}^{ \infty}\mu_{1}(A_{i1})\prod_{j=2}^{k}I_{A_{ij}}(\omega_{j}),\]

and by iteration

\[\prod_{j=1}^{k}\mu_{j}(A_{j})=\sum_{i=1}^{\infty}\prod_{j=1}^{k}\mu_{j}(A_{ij}).\]

Hence, (3.5) follows. \(\Box\)

By the extension theorem (Theorem 1.3.3), there exists a \(\sigma\)-algebra \({\cal M}\) and a measure \(\bar{\mu}\) such that

* \((\times_{i=1}^{k}\Omega_{i},{\cal M},\bar{\mu})\) is complete,
* \(\times_{i=1}^{k}{\cal F}_{i}\subset{\cal M}\), and
* \(\bar{\mu}(A)=\mu(A)\quad\mbox{for all}\quad A\in{\cal A}\).

Thus, the above procedure yields an extension \(\bar{\mu}\) of \(\mu\) on \({\cal A}\), and this extension is unique when all the \(\mu_{i}\)'s are \(\sigma\)-finite. Further, under the hypothesis that \(\mu_{1},\ldots,\mu_{k}\) are \(\sigma\)-finite, the analogs of formulas (1.4) and (1.5) for computing the product measure of a set via the iterated integrals are valid. More generally, the Tonelli-Fubini theorems extend to the \(k\)-fold product spaces in an obvious manner. For example, if \((\Omega_{i},{\cal F}_{i},\mu_{i})\) are \(\sigma\)-finite measure spaces for \(i=1,\ldots,k\) and \(f\) is a nonnegative measurable function on \((\times_{i=1}^{k}\Omega_{i},\times_{i=1}^{k}{\cal F}_{i},\bar{\mu})\), then

\[\int fd\bar{\mu}=\int_{\Omega_{i_{1}}}\int_{\Omega_{i_{2}}}\ldots\int_{\Omega_{ i_{k}}}f(\omega_{1},\omega_{2},\quad\omega_{k})\mu_{i_{1}}(d\omega_{i_{1}}) \ldots\mu_{i_{k}}(d\omega_{i_{k}})\]

for any permutation \((i_{1},i_{2},\ldots,i_{k})\) of \((1,2,\ldots,k)\).

**Definition 5.3.2:** The measure space \((\times_{i=1}^{k}\Omega_{i},{\cal M},\bar{\mu})\) is called the _complete product measure space_ and \(\bar{\mu}\) the _complete product measure_.

**Remark 5.3.1:** If \((\Omega_{i},{\cal F}_{i},\mu_{i})\equiv(\mathbb{R},{\cal L},m)\) where \({\cal L}\) is the Lebesgue \(\sigma\)-algebra and \(m\) is the Lebesgue measure, then the above extension coincides with the \((\mathbb{R}^{k},{\cal L}^{k},m^{k})\), where \({\cal L}^{k}\) is the Lebesgue \(\sigma\)-algebra in \(\mathbb{R}^{k}\) and \(m^{k}\) is the \(k\) dimensional Lebesgue measure.

### 5.4 Convolutions

#### Convolution of measures on \((\mathbb{R},{\cal B}(\mathbb{R}))\)

In this section, convolution of measures on \(\big{(}\mathbb{R},{\cal B}(\mathbb{R})\big{)}\) is discussed. From this, one can easily obtain convolution of sequences, convolution of functions in \(L^{1}(\mathbb{R})\) and convolution of functions with measures.

**Proposition 5.4.1:** _Let \(\mu\) and \(\lambda\) be two \(\sigma\)-finite measures on \(\big{(}\mathbb{R},{\cal B}(\mathbb{R})\big{)}\). For any Borel set \(A\) in \({\cal B}(\mathbb{R})\), let_

\[(\mu*\lambda)(A)\equiv\int\int I_{A}(x+y)\mu(dx)\lambda(dy). \tag{4.1}\]

_Then \((\mu*\lambda)(\cdot)\) is a measure on \(\big{(}\mathbb{R},{\cal B}(\mathbb{R})\big{)}\)._

**Proof:** Let \(h(x,y)\equiv x+y\) for \((x,y)\in\mathbb{R}\times\mathbb{R}\). Then \(h:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}\) is continuous and hence is \(\langle{\cal B}(\mathbb{R})\times{\cal B}(\mathbb{R}),{\cal B}(\mathbb{R})\rangle\)-measurable. Consider the measure \((\mu\times\lambda)h^{-1}(\cdot)\) on \((\mathbb{R},{\cal B}(\mathbb{R}))\) induced by the map \(h\) and the measure \(\mu\times\lambda\) on \(\langle\mathbb{R}\times\mathbb{R},{\cal B}(\mathbb{R})\times{\cal B}(\mathbb{R })\rangle\). Clearly

\[(\mu*\lambda)(\cdot)=(\mu\times\lambda)h^{-1}(\cdot)\]

and hence the proposition is proved. \(\Box\)

**Definition 5.4.1:** For any two \(\sigma\)-finite measures \(\mu\) and \(\lambda\) on \(\big{(}\mathbb{R},{\cal B}(\mathbb{R})\big{)}\), the measure \((\mu*\lambda)(A)\) defined in (4.1) is called the _convolution of \(\mu\) and \(\lambda\)._The following proposition is easy to verify.

**Proposition 5.4.2:** _Let \(\mu_{1}\), \(\mu_{2}\), \(\mu_{3}\) be \(\sigma\)-finite measures on \(\bigl{(}\mathbb{R},\mathcal{B}(\mathbb{R})\bigr{)}\). Then_

1. \((\)_Commutativity_\()\)__\(\mu_{1}*\mu_{2}=\mu_{2}*\mu_{1},\)__
2. \((\)_Associativity_\()\)__\(\mu_{1}*(\mu_{2}*\mu_{3})=(\mu_{1}*\mu_{2})*\mu_{3},\)__
3. \((\)_Distributive_\()\)__\(\mu_{1}*(\mu_{2}+\mu_{3})=\mu_{1}*\mu_{2}+\mu_{1}*\mu_{3},\)__
4. \((\)_Identity element_\()\)__\(\mu_{1}*\delta_{\{0\}}=\mu_{1}\)__

_where \(\delta_{\{0\}}(\cdot)\) is the delta measure at 0, i.e.,_

\[\delta_{\{0\}}(A)=\left\{\begin{array}{ll}1&\mbox{if}\quad 0\in A,\\ 0&\mbox{if}\quad 0\not\in A\end{array}\right.\]

_for all \(A\in\mathcal{B}(\mathbb{R})\)._

**Remark 5.4.1:** (_Extension to \(\mathbb{R}^{k}\)_). Definition 5.4.1 extends to measures on \(\bigl{(}\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k})\bigr{)}\) for any integer \(k\geq 1\) as well as to any space that is a commutative group under addition such as the sequence space \(\mathbb{R}^{\infty}\), the function spaces \(\mathcal{C}[0,1]\) and \(\mathcal{C}[0,\infty)\). These are relevant in the study of stochastic processes.

**Remark 5.4.2:** (_Sums of independent random variables_). It will be seen in Chapter 7 that if \(X\) and \(Y\) are two independent random variables on a probability space \((\Omega,\mathcal{F},P)\), then \(P_{X}*P_{Y}=P_{X+Y}\) where for any random variable \(Z\) on \((\Omega,\mathcal{F},P)\), \(P_{Z}\) is the distribution of \(Z\), i.e., the probability measure on \(\bigl{(}\mathbb{R},\mathcal{B}(\mathbb{R})\bigr{)}\) induced by \(P\) and \(Z\), i.e., \(P_{Z}(\cdot)=PZ^{-1}(\cdot)\) on \(\mathcal{B}(\mathbb{R})\).

**Remark 5.4.3:** (_Extension to signed measures_). Let \(\mu\) and \(\lambda\) be two finite signed measures on \(\bigl{(}\mathbb{R},\mathcal{B}(\mathbb{R})\bigr{)}\) as defined in Section 4.2. Let \(\mu=\mu^{+}-\mu^{-}\), \(\lambda=\lambda^{+}-\lambda^{-}\) be the Jordan decomposition of \(\mu\) and \(\lambda\), respectively. Then the _product measure_\(\mu\times\lambda\) can be defined as the signed measure

\[\mu\times\lambda \equiv \bigl{[}(\mu^{+}\times\lambda^{+})+(\mu^{-}\times\lambda^{-}) \bigr{]}-\bigl{[}(\mu^{+}\times\lambda^{-})+(\mu^{-}+\lambda^{+})\bigr{]} \tag{4.2}\] \[\equiv \gamma_{1}-\gamma_{2},\quad\mbox{say}.\]

This is well defined since the measures \((\mu^{+}\times\lambda^{+})+(\mu^{-}\times\lambda^{-})\) and \((\mu^{+}\times\lambda^{-})+(\mu^{-}\times\lambda^{+})\) are both finite measures. Now the definition of convolution of measures in (4.1) carries over to signed measures using the definition of integration w.r.t. signed measures discussed in Section 4.2.

**Definition 5.4.2:** Let \(\mu\) and \(\nu\) be (finite) signed measures on a measurable space \(\bigl{(}\mathbb{R},\mathcal{B}(\mathbb{R})\bigr{)}\). The _convolution of \(\mu\) and \(\nu\)_, denoted by \(\mu*\nu\) is the signed measure defined by

\[(\mu*\nu)(A) = \int\int I_{A}(x+y)d(\mu\times\nu)\]\[= \int\int I_{A}(x+y)d\gamma_{1}-\int\int I_{A}(x+y)d\gamma_{2} \tag{4.3}\]

where \(\gamma_{1}\) and \(\gamma_{2}\) are as in (4.2).

#### Convolution of sequences

**Definition 5.4.3:** Let \(\tilde{a}\equiv\{a_{n}\}_{n\geq 0}\) and \(\tilde{b}\equiv\{b_{n}\}_{n\geq 0}\) be two sequences of real numbers. Then the _convolution of \(\tilde{a}\) and \(\tilde{b}\)_ denoted by \(\tilde{a}*\tilde{b}\) is the sequence

\[(\tilde{a}*\tilde{b})(n)=\sum_{j=0}^{n}a_{j}b_{n-j},\ \ n\geq 0. \tag{4.4}\]

If \(\sum_{j=0}^{\infty}|a_{j}|<\infty\) and \(\sum_{j=0}^{\infty}|b_{j}|<\infty\), then \(\tilde{a}*\tilde{b}\) corresponds to the convolution of the signed measures \(\tilde{a}\) and \(\tilde{b}\) on \(\mathbb{Z}_{+}\), defined by \(\tilde{a}(i)=a_{i}\), \(\tilde{b}(i)=b_{i}\), \(i\in\mathbb{Z}_{+}\).

**Example 5.4.1:**

1. Let \(b_{n}\equiv 1\) for \(n\geq 0\). Then for any \(\{a_{n}\}_{n\geq 0}\), \((\tilde{a}*\tilde{b})(n)=\sum_{j=0}^{n}a_{j}\).
2. Fix \(0<p<1\), \(k_{1},k_{2}\) positive integers. For \(n\in\mathbb{Z}_{+}\), let \[a_{n} \equiv \binom{k_{1}}{n}p^{n}(1-p)^{k_{1}-n}I_{[0,k_{1}]}(n)\] \[b_{n} \equiv \binom{k_{2}}{n}p^{n}(1-p)^{k_{2}-n}I_{[0,k_{2}]}(n).\] Then \((\tilde{a}*\tilde{b})(n)=\binom{k_{1}+k_{2}}{n}p^{n}(1-p)^{k_{2}-n}I_{[0,k_{1 }+k_{2}]}(n)\).
3. Fix \(0<\lambda_{1},\lambda_{2}<\infty\). Let \[a_{n} \equiv e^{-\lambda_{1}}\frac{\lambda_{1}^{n}}{n!},\ n=0,1,2,\ldots\] \[b_{n} \equiv e^{-\lambda_{2}}\frac{\lambda_{2}^{n}}{n!},\ n=0,1,2,\ldots\.\] Then \((\tilde{a}*\tilde{b})(n)=e^{-(\lambda_{1}+\lambda_{2})}\,\frac{(\lambda_{1}+ \lambda_{2})^{n}}{n!},\ n=0,1,2,\ldots\).

The verification of these claims is left as an exercise (Problem 5.20).

A useful technique to determine \(\tilde{a}*\tilde{b}\) is the use of generating functions. This will be discussed in Section 5.5.

#### Convolution of functions in \(L^{1}(\mathbb{R})\)

Let \(L^{1}(\mathbb{R})\equiv L^{1}(\mathbb{R},\mathcal{B}(\mathbb{R}),m)\) where \(m(\cdot)\) is the Lebesgue measure. In the following, for \(f\in L^{1}(\mathbb{R})\), \(\int fdm\) will also be written as \(\int f(x)dx\).

**Proposition 5.4.3:** _Let \(f\), \(g\in L^{1}(\mathbb{R})\). Then for almost all \(x\) (w.r.t. \(m\)),_

\[\int|f(x-u)|\,|g(u)|du<\infty. \tag{4.5}\]

**Proof:** Let \(k(x,u)=f(x-u)g(u)\). Since \(\pi(x,u)\equiv x-u\) is a continuous map from \(\mathbb{R}\times\mathbb{R}\to\mathbb{R}\), it is \(\langle\mathcal{B}(\mathbb{R})\times\mathcal{B}(\mathbb{R}),\mathcal{B}( \mathbb{R})\rangle\)-measurable. Also, since \(f\) and \(g\) are Borel measurable, it follows that \(k:\mathbb{R}\times\mathbb{R}\to\mathbb{R}\) is \(\langle\mathcal{B}(\mathbb{R})\times\mathcal{B}(\mathbb{R}),\mathcal{B}( \mathbb{R})\rangle\)-measurable. Also note that by the translation invariance of \(m\), \(\int|f(x-u)|dx=\int|f(x)|dx\) for any Borel measurable \(f:\mathbb{R}\to\mathbb{R}\). Hence, by Tonelli's theorem,

\[\int\int|k(x,u)|dxdu = \int\Big{(}\int|k(x,u)|dx\Big{)}du \tag{4.6}\] \[= \int|g(u)|\Big{(}\int|f(x)|dx\Big{)}du\] \[= \|g\|_{1}\,\|f\|_{1}<\infty.\]

Also by Tonelli's theorem

\[\int\int|k(x,u)|dxdu=\int\Big{(}\int|k(x,u)|du\Big{)}dx.\]

By (4.6), this yields

\[\int|k(x,u)|du<\infty\quad\mbox{a.e.}\quad(m)\]

which is the same as (4.5). \(\Box\)

**Definition 5.4.4:** Let \(f\), \(g\in L^{1}(\mathbb{R})\). Then the _convolution of \(f\) and \(g\)_, denoted by \(f*g\) is the function defined a.e. \((m)\) by

\[(f*g)(x)\equiv\int f(x-u)g(u)du. \tag{4.7}\]

Note that by (4.5) this is well defined.

**Proposition 5.4.4:** _Let \(f\), \(g\in L^{1}(\mathbb{R})\). Then_

* \(f*g=g*f\)__
* \(f*g\in L^{1}(\mathbb{R})\) _and_ \(\|f*g\|_{1}\leq\|f\|_{1}\,\|g\|_{1}\)__
* \(\int f*g\,dm=\Big{(}\int fdm\Big{)}\Big{(}\int gdm\Big{)}\)_._

**Proof:** For (i), use the change of variable \(u\to x-u\) and the translation invariance of \(m\). For (ii) use (4.6). For (iii), use Fubini's theorem.

It is easy to see that if \(\mu\) and \(\lambda\) denote the signed measures with Radon-Nikodym derivatives \(f\) and \(g\), respectively, w.r.t. \(m\), then \(\mu*\lambda\) is the signed measure with Radon-Nikodym derivative \(f*g\) w.r.t. \(m\).

**Example 5.4.2:** Here are two examples from probability theory.

* (_Convolutions of Uniform \([0,1]\) densities_). Let \(f=g=I_{[0,1]}\). Then \[(f*g)(x)=\left\{\begin{array}{cc}x&0\leq x\leq 1\\ 2-x&1\leq x\leq 2.\end{array}\right.\]
* (_Convolutions of \(N(0,1)\) densities_). Let \(f(x)\equiv g(x)\equiv\frac{1}{\sqrt{2\pi}}\,e^{-\frac{x^{2}}{2}}\), \(-\infty<x<\infty\). Then \((f*g)(x)=\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{2}}\,e^{-\frac{x^{2}}{4}}\), \(-\infty<x<\infty\).

#### Convolution of functions and measures

Let \(f\in L^{1}(\mathbb{R})\) and \(\lambda\) be a signed measure. Let \(\mu(A)\equiv\int_{A}fdm\), \(A\in\mathcal{B}(\mathbb{R})\). Then it can be shown that \(\mu*\lambda\) is dominated by \(m\) and its Radon-Nikodym derivative is given by

\[f*\lambda(x)\equiv\int f(x-u)\lambda(du),\ \ x\in\mathbb{R}. \tag{4.8}\]

Note that \(f*\lambda\) in (4.8) is well defined for any nonnegative Borel measurable \(f\) and any measure \(\lambda\) on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\).

### 5.5 Generating functions and Laplace transforms

**Definition 5.5.1:** Let \(\{a_{n}\}_{n\geq 0}\) be a sequence in \(\mathbb{R}\) and let \(\rho=\big{(}\,\varlimsup\limits_{n\to\infty}|a_{n}|^{1/n}\big{)}^{-1}\). The power series

\[A(s)\equiv\sum_{n=0}^{\infty}a_{n}s^{n}, \tag{5.1}\]

defined for all \(s\) in \((-\rho,\rho)\), is called the _generating function of \(\{a_{n}\}_{n\geq 0}\)_.

Note that the power series in (5.1) converges absolutely for \(|s|<\rho\) and \(\rho\) is called the _radius of convergence_ of \(A(s)\) (cf. Appendix A.2).

The usefulness of generating functions is given by the following:

**Proposition 5.5.1:** _Let \(\{a_{n}\}_{n\geq 0}\) and \(\{b_{n}\}_{n\geq 0}\) be two sequences in \(\mathbb{R}\). Let \(\{c_{n}\}_{n\geq 0}\) be the convolution of \(\{a_{n}\}\) and \(\{b_{n}\}\). That is,_

\[c_{n}=\sum_{j=0}^{n}a_{j}b_{n-j},\ n\geq 0.\]_Then_

\[C(s)=A(s)B(s) \tag{5.2}\]

_for all \(s\) in \((-\rho,\rho)\), where \(A(\cdot)\), \(B(\cdot)\), and \(C(\cdot)\) are, respectively, the generating function of the sequences \(\{a_{n}\}_{n\geq 0}\), \(\{b_{n}\}_{n\geq 0}\) and \(\{c_{n}\}_{n\geq 0}\) and \(\rho=\min(\rho_{a},\rho_{b})\) with \(\rho_{a}=\big{(}\mathop{\varlimsup}\limits_{n\to\infty}|a_{n}|^{1/n}\big{)}^{-1}\) and \(\rho_{b}=\big{(}\mathop{\varlimsup}\limits_{n\to\infty}|b_{n}|^{1/n}\big{)}^{-1}\)._

**Proof:** By Tonelli's theorem applied to the counting measure on the product space \((\mathbb{Z}_{+}\times\mathbb{Z}_{+})\),

\[\sum_{n=0}^{\infty}|s|^{n}\bigg{(}\sum_{j=0}^{n}|a_{j}|\,|b_{n-j} |\bigg{)}\] \[= \sum_{j=0}^{\infty}|a_{j}|\,|s|^{j}\bigg{(}\sum_{n=j}^{\infty}|s |^{n-j}|b_{n-j}|\bigg{)}\] \[= A(|s|)B(|s|)<\infty\quad\mbox{if}\quad|s|<\rho.\]

Now by Fubini's theorem, (5.2) follows. \(\Box\)

It is easy to verify the claims in Example 5.4.1 by using the above proposition and the uniqueness of the power series coefficients (Problem 5.20).

**Proposition 5.5.2:** (_Renewal sequences_). _Let \(\{a_{n}\}_{n\geq 0}\), \(\{b_{n}\}_{n\geq 0}\), \(\{p_{n}\}_{n\geq 0}\) be sequences in \(\mathbb{R}\) such that_

\[a_{n}=b_{n}+\sum_{j=0}^{n}a_{n-j}p_{j},\ n\geq 0 \tag{5.3}\]

_and \(p_{0}=0\). Then_

\[A(s)=\frac{B(s)}{1-P(s)}\]

_for all \(s\) such that \(|s|<\rho\) and \(P(s)\neq 1\), where \(\rho=\min(\rho_{b},\rho_{p})\), \(\rho_{b}=\big{(}\mathop{\varlimsup}\limits_{n\to\infty}|b_{n}|^{1/n}\big{)}^{-1}\), \(\rho_{p}=\big{(}\mathop{\varlimsup}\limits_{n\to\infty}|p_{n}|^{1/n}\big{)}^{-1}\)._

For applications of this to renewal theory, see Chapter 8.

**Definition 5.5.2:** (_Laplace transform_). Let \(f:[0,\infty)\to\mathbb{R}\) be Borel measurable. The function

\[(Lf)(s)\equiv\int_{[0,\infty)}e^{sx}f(x)dx, \tag{5.4}\]

defined for all \(s\) in \(\mathbb{R}\) such that

\[\int_{[0,\infty)}e^{sx}|f(x)|dx<\infty, \tag{5.5}\]

is called the _Laplace transform of \(f\)_.

It is easily seen that if (5.5) holds for some \(s=s_{0}\), then it does for all \(s<s_{0}\). The analog of Proposition 5.5.1 is the following.

**Proposition 5.5.3:** _Let \(f\), \(g\in L^{1}(\mathbb{R})\). Then_

\[L(f*g)(s)=Lf(s)Lg(s) \tag{5.6}\]

_for all \(s\) such that (5.5) holds for both \(f\) and \(g\)._

**Definition 5.5.3:** (_Laplace-Stieltjes transform_). Let \(\mu\) be a measure on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\) such that \(\mu\big{(}(-\infty,0)\big{)}=0\). The function

\[\mu^{*}(s)\equiv L\mu(s)\equiv\int_{[0,\infty)}e^{sx}\mu(dx)\]

is called the _Laplace-Stieltjes transform_ of \(\mu\). Clearly, \(\mu^{*}(s)\) is well defined for all \(s\) in \(\mathbb{R}\). However, \(\mu^{*}(s_{0})=\infty\) implies \(\mu^{*}(s)=\infty\) for \(s\geq s_{0}\).

**Proposition 5.5.4:** _Let \(\mu\) and \(\lambda\) be measures on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\) such that \(\mu\big{(}(-\infty,0)\big{)}=0=\lambda\big{(}(-\infty,0)\big{)}\). Then_

\[L(\mu*\lambda)(s)=L\mu(s)L\lambda(s)\ \ \mbox{for all}\ \ s\ \mbox{in}\ \mathbb{R}.\]

For an inversion formula to obtain a probability measure \(\mu\) from \(L\mu(\cdot)\), see Feller (1966), Section 13.4.

### Fourier series

In this section, \(L^{p}[0,2\pi]\) stands for \(L^{p}\big{(}[0,2\pi],\mathcal{B}([0,2\pi]),m\big{)}\) where \(m(\cdot)\) is Lebesgue measure and \(0<p<\infty\).

**Definition 5.6.1:** For \(f\in L^{1}[0,2\pi]\), \(n\geq 0\), let

\[a_{n} \equiv \frac{1}{2\pi}\int_{0}^{2\pi}f(x)\cos nx\,dx\] \[b_{n} \equiv \frac{1}{2\pi}\int_{0}^{2\pi}f(x)\sin nx\,dx, \tag{6.1}\] \[s_{n}(f,x)\equiv a_{0}+\sum_{j=1}^{n}(a_{j}\cos jx+b_{j}\sin jx). \tag{6.2}\]

Then \(\{a_{n},b_{n},n=0,1,2,\ldots\}\) are called the _Fourier coefficients of \(f\)_ and the sequence \(\{s_{n}(f,x):n=0,1,2,\ldots\}\) is called the _partial sum sequence_ of the _Fourier series expansion of \(f\)_.

Since \(\mathcal{C}[0,2\pi]\subset L^{2}[0,2\pi]\subset L^{1}[0,2\pi]\), the Fourier coefficients and the partial sum sequence of the Fourier series are well defined for \(f\) in \(\mathcal{C}[0,2\pi]\)and \(f\) in \(L^{2}[0,2\pi].\) J. Fourier introduced these series in the early 19th century to approximate certain functions exhibiting a periodic behavior that arose in the study of physics and mechanics and in particular in the theory of heat conduction (see Korner (1989) and Bhatia (2003)).

A natural question is: _In what sense does \(s_{n}(f,x)\) approximate \(f(x)\)?_ It turns out that one can prove the strongest results for \(f\) in \({\cal C}[0,2\pi],\) less stronger results for \(f\) in \(L^{2}[0,2\pi],\) and finally, for \(f\) in \(L^{1}[0,2\pi].\)

In the early 19th century it was believed that for any \(f\) in \({\cal C}[0,2\pi],\)\(s_{n}(f,x)\) converged to \(f(x)\) for all \(x\) in \([0,2\pi].\) But in 1876, D. Raymond constructed a continuous function \(f\) for which \(\overline{\lim}_{n\to\infty}|s_{n}(f;x_{0})|=\infty\) for some \(x_{0}.\) However, Fejer showed in 1903 that for \(f\) in \({\cal C}[0,2\pi],\)\(s_{n}(f,\cdot)\) does converge to \(f\) uniformly in the _Cesaro sense_.

**Theorem 5.6.1:** (_Fejer_). _Let \(f\in{\cal C}[0,2\pi]\) and \(s_{n}(f,\cdot)\), \(n\geq 0\) be as in Definition 3.4.1. Let_

\[{\cal D}_{n}(f,\cdot)=\frac{1}{n}\sum_{j=0}^{n-1}s_{j}(f,\cdot),\ n\geq 1. \tag{6.3}\]

_Then_

\[{\cal D}_{n}(f,\cdot)\to f(\cdot)\quad\mbox{uniformly on}\quad[0,2\pi]\quad \mbox{as}\quad n\to\infty. \tag{6.4}\]

For the proof of this result, the following result of some independent interest is needed.

**Lemma 5.6.2:** (_Fejer_). _Let for \(m\geq 1\)_

\[K_{m}(x)\equiv 1+\frac{2}{m}\sum_{j=1}^{m-1}(m-j)\cos jx,\ x\in\mathbb{R}. \tag{6.5}\]

_Then_

1. \[K_{m}(x)=\left\{\begin{array}{ccc}\frac{1}{m}\biggl{(}\frac{\sin\frac{mx}{2} }{\sin\frac{x}{2}}\biggr{)}^{2}&\mbox{if}&x\neq 0\\ m&\mbox{if}&x=0\.\end{array}\right.\] (6.6)
2. For \(\delta>0,\) \[\sup\{K_{m}(x):\delta\leq|x|\leq 2\pi-\delta\}\to 0\quad\mbox{as}\quad m\to\infty.\] (6.7)
3. \[\frac{1}{2\pi}\int_{0}^{2\pi}K_{m}(x)dx=1.\] (6.8)

**Proof:** Clearly, (iii) follows from (6.5) on integration. Also, (ii) follows from (i) since for \(\delta\leq x\leq 2\pi-\delta\),

\[K_{m}(x)\leq\frac{1}{m}\frac{1}{(\sin\frac{\delta}{2})^{2}}\to 0\quad\mbox{as}\quad m\to\infty.\]

To establish (i) note that using Euler's formula (cf. Section A.3, Appendix) \(e^{\iota x}=\cos x+\iota\sin x\), \(K_{m}(x)\) can be written as

\[K_{m}(x) = \frac{1}{m}\sum_{j=-(m-1)}^{(m-1)}(m-|j|)e^{\iota jx}\] \[= \frac{1}{m}\sum_{j=0}^{2(m-1)}(m-|j-(m-1)|)e^{\iota(j-(m-1))x}\] \[= \frac{1}{m}\bigg{(}\sum_{k=0}^{m-1}e^{\iota(k-\frac{m-1}{2})x} \bigg{)}^{2}.\]

For \(x\neq 0\),

\[K_{m}(x) = \frac{1}{m}\Big{(}e^{-\frac{\iota(m-1)x}{2}}\frac{1-e^{\iota mx}} {1-e^{\iota x}}\Big{)}^{2}\,\] \[= \frac{1}{m}\bigg{(}\frac{e^{-\frac{\iota mx}{2}}-e^{\frac{\iota mx }{2}}}{e^{-\frac{4k}{2}}-e^{\frac{4k}{2}}}\bigg{)}^{2}\,\] \[= \frac{1}{m}\Big{(}\frac{\sin\frac{mx}{2}}{\sin\frac{x}{2}}\Big{)} ^{2}.\]

For \(x=0\), (6.5) yields

\[K_{m}(0) = 1+\frac{2}{m}\sum_{j=1}^{m-1}(m-j)\] \[= 1+\frac{2}{m}\frac{(m-1)m}{2}=m.\]

\(\Box\)

**Proof of Theorem 5.6.1:** Let \(f\in{\cal C}[0,2\pi]\). Let \(\{a_{n},b_{n}\}_{n\geq 0}\), \(s_{n}(f,\cdot)\), \({\cal D}_{m}(f,\cdot)\) be as in (6.1), (6.2), and (6.3). Then from the definition of \(a_{n},b_{n},n\geq 0\), it follows that

\[{\cal D}_{m}(f,\cdot) = \frac{1}{2\pi}\int_{0}^{2\pi}K_{m}(x-u)f(u)du \tag{6.9}\] \[= \frac{1}{2\pi}\int_{0}^{2\pi}f(x-u)K_{m}(u)du\]where \(K_{m}(\cdot)\) is defined in (6.5) and \(f(\cdot)\) is extended to all of \(\mathbb{R}\) periodically with period \(2\pi.\) Since \(f\in\mathcal{C}[0,2\pi],\) given \(\epsilon>0,\) there exists a \(\delta>0\) such that

\[x,y\in[0,2\pi],\ |x-y|<\delta\Rightarrow|f(x)-f(y)|<\epsilon.\]

Also from (6.7), for this \(\delta>0,\) there exist an \(m_{0}\) such that \(m\geq m_{0}\Rightarrow\sup\{K_{m}(x):\delta\leq x\leq 2\pi-\delta\}<\epsilon.\) Now (6.9) yields, for \(x\in[0,2\pi],\)\(m\geq m_{0}\)

\[|\mathcal{D}_{m}(f,x)-f(x)| \leq \frac{1}{2\pi}\int_{0}^{2\pi}|f(x-u)-f(x)|K_{m}(u)du\] \[= \frac{1}{2\pi}\int\limits_{(\delta\leq u\leq 2\pi-\delta)^{c}}|f (x-u)-f(x)|K_{m}(u)du\] \[\mbox{}+\frac{1}{2\pi}\int\limits_{(\delta\leq u\leq 2\pi-\delta)} |f(x-u)-f(x)|K_{m}(u)du\] \[\leq \frac{1}{2\pi}(\epsilon+2\|f\|\epsilon 2\pi)\]

where \(\|f\|=\sup\{|f(x)|:x\in[0,2\pi]\}.\) Thus, \(m\geq m_{0}\Rightarrow\sup\{|\mathcal{D}_{m}(f,x)-f(x)|:x\in[0,2\pi]\}\leq \epsilon\frac{(1+4\pi\|f\|)}{2\pi}.\) Since \(\epsilon>0\) is arbitrary, the proof of Theorem 5.6.1 is complete. \(\Box\)

An immediate consequence of Theorem 5.6.1 is the completeness of the trigonometric functions.

**Theorem 5.6.3:**_The collection \(\mathcal{T}_{0}\equiv\{\cos nx:n=0,1,2,\ldots\}\cup\{\sin nx:n=1,2,\ldots\}\) is a complete orthogonal system for \(L^{2}[0,2\pi].\)_

**Proof:** By Theorem 5.6.1 for each \(f\in\mathcal{C}[0,2\pi]\) and \(\epsilon>0,\) there exists a finite linear combination \(\mathcal{D}_{m}(f,\cdot)\) of \(\{\cos nx:n=0,1,2,\ldots\}\) and \(\{\sin nx:n=1,2,\ldots\}\) such that

\[\int_{0}^{2\pi}|f-\mathcal{D}_{m}(f,\cdot)|^{2}dx\] \[\leq 2\pi(\sup\{|f(x)-\mathcal{D}_{m}(f,x)|:x\in[0,2\pi]\})^{2}< \epsilon^{2}.\]

Also, from Theorem 2.3.14 it is known that given any \(g\in L^{2}[0,2\pi],\) and any \(\epsilon>0,\) there is a \(f\in\mathcal{C}[0,2\pi]\) such that \(\int_{0}^{2\pi}|f-g|^{2}dx<\epsilon^{2}.\) Thus, for any \(g\in L^{2}[0,2\pi],\)\(\epsilon>0,\) there is a \(f\in\mathcal{C}[0,2\pi]\) and an \(m\geq 1\) such that

\[\|g-\mathcal{D}_{m}(f,\cdot)\|_{2}<2\epsilon.\]

That is, the set \(\mathcal{T}\) of all finite linear combinations of the functions in the class \(\mathcal{T}_{0}\) is dense in \(L^{2}[0,2\pi].\) Further, it is easy to verify that \(h_{1},h_{2}\in\mathcal{T}_{0},\)\(h_{1}\neq h_{2}\) implies

\[\int_{0}^{2\pi}h_{1}(x)h_{2}(x)dx=0,\]i.e., \({\cal T}_{0}\) is an orthogonal family. Since \({\cal T}_{0}\) is orthogonal and \({\cal T}\) is dense in \(L^{2}[0,2\pi]\), \({\cal T}_{0}\) is complete. \(\Box\)

**Definition 5.6.2:** A function in \({\cal T}\) is called a _trigonometric polynomial_.

Thus, the above theorem says that trigonometric polynomials are dense in \(L^{2}[0,2\pi]\). Completeness of \({\cal T}\) in \(L^{2}[0,2\pi]\) and the results of Section 3.3 lead to

**Theorem 5.6.4:**_Let \(f\in L^{2}[0,2\pi]\). Let \(\{(a_{n},b_{n}),n=0,1,2,\ldots\}\) and \(\{s_{n}(f,\cdot)\}_{n\geq 0}\) be the associated Fourier coefficient sequences and partial sum sequence of the Fourier series for \(f\) as in Definition 5.6.1. Then_

* \(s_{n}(f,\cdot)\to f\) _in_ \(L^{2}[0,2\pi]\)_,_
* \(\sum_{n=0}^{\infty}(\tilde{a}_{n}^{2}+\tilde{b}_{n}^{2})=\frac{1}{2\pi}\int_{ 0}^{2\pi}|f|^{2}dx\) _where for_ \(n\geq 0\)_,_ \(\tilde{a}_{n}=a_{n}/c_{n}\)_, with_ \(c_{n}^{2}=\frac{1}{2\pi}\int_{0}^{2\pi}(\cos nx)^{2}dx=\frac{1}{2}\)_, and for_ \(n\geq 1\)_,_ \(\tilde{b}_{n}=\frac{b_{n}}{d_{n}}\) _with_ \(d_{n}^{2}=\frac{1}{2\pi}\int_{0}^{2\pi}(\sin nx)^{2}dx=\frac{1}{2}\)_._
* _Further, if_ \(f\)_,_ \(g\in L^{2}[0,2\pi]\)_, then_ \(\frac{1}{2\pi}\int_{0}^{2\pi}fg\,dx=a_{0}\alpha_{0}+\sum_{n=1}^{\infty}\left( \frac{a_{n}\alpha_{n}}{c_{n}^{2}}\,+\,\frac{b_{n}\beta_{n}}{d_{n}^{2}}\right)\)_, where_ \(\{(a_{n},b_{n}):n=0,1,2,\ldots\}\) _and_ \(\{(\alpha_{n},\beta_{n}):n=0,1,2,\ldots\}\) _are, respectively, the Fourier coefficients of_ \(f\) _and_ \(g\)_._

Clearly (ii) above is a restatement of Bessel's equality. Assertion (iii) is known as the _Parseval identity_. As for convergence pointwise or almost everywhere, A. N. Kolmogorov showed in 1926 (see Korner (1989)) that there exists an \(f\in L^{1}[0,2\pi]\) such that \(\overline{\lim}_{n\to\infty}|s_{n}(f,x)|=\infty\) everywhere on \([0,2\pi]\). This led to the belief that for \(f\not\in{\cal C}[0,2\pi]\), the mean square convergence of (i) in Theorem 5.6.4 cannot be improved upon. But L. Carleson showed in 1964 (see Korner (1989)) that for \(f\) in \(L^{2}[0,2\pi]\), \(s_{n}(f,\cdot)\to f(\cdot)\) almost everywhere. Finally, turning to \(L^{1}[0,2\pi]\), one has the following:

**Theorem 5.6.5:**_Let \(f\in L^{1}[0,2\pi]\). Let \(\{(a_{n},b_{n}):n\geq 0\}\) be as in (6.1) and satisfy \(\sum_{n=0}^{\infty}(|a_{n}|+|b_{n}|)<\infty\). Let \(s_{n}(f,\cdot)\) be as in (6.2). Then \(s_{n}(f,\cdot)\) converges uniformly on \([0,2\pi]\) and the limit coincides with \(f\) almost everywhere._

**Proof:** Note that \(\sum_{n=0}^{\infty}(|a_{n}|+|b_{n}|)<\infty\) implies that the sequence \(\{s_{n}(f,\cdot)\}_{n\geq 0}\) is a Cauchy sequence in the Banach space \({\cal C}[0,2\pi]\) with the sup-norm. Thus, there exists a \(g\) in \({\cal C}[0,2\pi]\) such that \(s_{n}(f,\cdot)\to g\) uniformly on \([0,2\pi]\). It is easy to check that this implies that \(g\) and \(f\) have the same Fourier coefficients. Set \(h=g-f\). Then \(h\in L^{1}[0,2\pi]\) and the Fourier coefficients of \(h\) are all zero. This implies that \(h\) is orthogonal to the members of the class \({\cal T}\), which in turn yields that \(h\) is orthogonal to all continuous functions in \({\cal C}[0,2\pi]\), i.e.,

\[\int_{0}^{2\pi}h(x)k(x)dx=0\]

for all \(k\in{\cal C}[0,2\pi]\). Since \(h\in L^{1}[0,2\pi]\) and for any interval \(A\subset[0,2\pi]\), there exists a sequence \(\{k_{n}\}_{n\geq 1}\) of uniformly bounded continuous functions, such that \(k_{n}\to I_{A}\) a.e. \((m)\), by the DCT,

\[\int h(x)I_{A}(x)dx=\lim_{n\to\infty}\int h(x)k_{n}(x)dx=0.\]

This in turn implies that \(h=0\) a.e., i.e., \(g=f\) a.e. \(\Box\)

**Remark 5.6.1:** If \(f\in L^{2}[0,2\pi]\), the Fourier coefficients \(\{a_{n},b_{n}\}\) are square summable and hence go to zero as \(n\to\infty\). What if \(f\in L^{1}[0,2\pi]\)?

If \(f\in L^{1}[0,2\pi]\), one can assert the following:

**Theorem 5.6.6:** (_Riemann-Lebesgue lemma_). _Let \(f\in L^{1}[0,2\pi]\). Then_

\[\lim_{n\to\infty}\int_{0}^{2\pi}f(x)\cos nx\,dx=0=\lim_{n\to\infty}\int_{0}^{2 \pi}f(x)\sin nx\,dx.\]

**Proof:** The lemma holds if \(f=I_{A}\) for any interval \(A\subset[0,2\pi]\) and since step functions (i.e., linear combinations of indicator functions of intervals) are dense in \(L^{1}[0,2\pi]\), the lemma is proved. \(\Box\)

It can be shown that the mapping \(f\to\{(a_{n},b_{n})\}_{n\geq 0}\) from \(L^{1}[0,2\pi]\) to bivariate sequences that go to zero as \(n\to\infty\) is one-to-one but not onto (Rudin (1987), Chapter 5).

**Remark 5.6.2:** (_The complex case_). Let \(T\equiv\{z:z=e^{\iota\theta},0\leq\theta\leq 2\pi\}\) be the unit circle in the complex plane \({\mathbb{C}}\). Every function \(g:T\to{\mathbb{C}}\) can be identified with a function \(f\) on \({\mathbb{R}}\) by \(f(t)=g(e^{\iota t})\). Clearly, \(f(\cdot)\) is periodic on \({\mathbb{R}}\) with period \(2\pi\). In the rest of this section, for \(0<p<\infty\), \(L^{p}(T)\) will stand for the collection of all Borel measurable functions \(f:[0,2\pi]\) to \({\mathbb{C}}\) such that \(\int_{[0,2\pi]}|f|^{p}dm<\infty\) where \(m(\cdot)\) is the Lebesgue measure. A _trigonometric polynomial_ is a function of form

\[f(\cdot)\equiv\sum_{n=-k}^{k}\alpha_{n}e^{\iota nx}\equiv a_{0}+\sum_{n=1}^{k }(a_{n}\cos nx+b_{n}\sin nx),\]

\(k<\infty\), where \(\{\alpha_{n}\}\), \(\{a_{n}\}_{n\geq 0}\), and \(\{b_{n}\}_{n\geq 0}\) are sequences of complex numbers.

The completeness of the trigonometric polynomials proved in Theorem 5.6.3 implies that the family \(\{e^{\iota nx}:n=0,\pm 1,\pm 2,\ldots\}\) is a complete orthonormal basis for \(L^{2}(T)\), which is a complex Hilbert space.

Thus Theorem 5.6.4 carries over to this case.

**Theorem 5.6.7:**

1. _Let_ \(f\in L^{2}(T)\)_. Then,_ \[\sum_{n=-k}^{k}\alpha_{n}e^{\iota nx}\to f\quad\text{in}\quad L^{2}(T)\] _where_ \[\alpha_{n}\equiv\frac{1}{2\pi}\int_{0}^{2\pi}f(x)e^{-\iota ns}dx,\ n\in\mathbb{ Z}.\] _Further,_ \[\sum_{n=-\infty}^{\infty}|\alpha_{n}|^{2}=\frac{1}{2\pi}\int_{0}^{2\pi}|f|^{2} dm.\]
2. _For any sequence_ \(\{\alpha_{n}\}_{n\in\mathbb{Z}}\) _of complex numbers such that_ \(\sum_{n=-\infty}^{\infty}|\alpha_{n}|^{2}<\infty\)_, the sequence_ \(\left\{f_{k}(x)\equiv\sum_{n=-k}^{k}\alpha_{n}e^{\iota nx}\right\}_{k\geq 1}\) _converges in_ \(L^{2}(T)\) _to a unique_ \(f\) _such that_ \[\alpha_{n}=\frac{1}{2\pi}\int_{0}^{2\pi}f(x)e^{-\iota nx}dx.\]
3. _For any_ \(f\)_,_ \(g\in L^{2}(T)\)_,_ \[\sum_{n=-\infty}^{\infty}\alpha_{n}\bar{\beta}_{n}=\frac{1}{2\pi}\int_{0}^{2 \pi}f(x)\overline{g(x)}dx\] _where_ \[\alpha_{n} = \hat{f}(n)=\frac{1}{2\pi}\int_{0}^{2\pi}f(x)e^{-\iota nx}dx,\] \[\beta_{n} = \hat{g}(n)=\frac{1}{2\pi}\int_{0}^{2\pi}g(x)e^{-\iota nx}dx,\ n\in \mathbb{Z}.\] _Further,_ \[\sum_{n=-\infty}^{\infty}|\alpha_{n}\beta_{n}|<\infty.\]
4. \(L^{2}(T)\) _is isomorphic to_ \(\ell_{2}(\mathbb{Z})\)_, the Hilbert space of all square summable sequences of complex numbers on_ \(\mathbb{Z}\)_._

Similarly, Theorem 5.6.5 carries over to the complex case.

**Theorem 5.6.8:**_Let \(f\in L^{1}(T)\). Suppose_

\[\sum_{n=-\infty}^{\infty}|\hat{f}(n)|<\infty\]

_where_

\[\hat{f}(n)=\frac{1}{2\pi}\int_{0}^{2\pi}f(x)e^{-\iota nx}dx,\ n\in\mathbb{Z}.\]

_Then_

\[s_{n}(f,x)\equiv\sum_{j=-n}^{n}\hat{f}(j)e^{-\iota jx}\]

_converges uniformly on \([0,2\pi]\) and the limit coincides with \(f\) a.e. and hence \(f\) is continuous a.e._

### 5.7 Fourier transforms on \(\mathbb{R}\)

In this section and in Section 5.8, let \(L^{p}(\mathbb{R})\) stand for

\[L^{p}(\mathbb{R})\equiv\{f:f:\mathbb{R}\to\mathbb{C},\ \mbox{\rm Borel measurable},\ \int_{\mathbb{R}}|f|^{p}dm<\infty\} \tag{7.1}\]

where \(m(\cdot)\) is Lebesgue measure. Also, for \(f\in L^{1}(\mathbb{R})\), \(\int_{\mathbb{R}}fdm\) will often be written as \(\int f(x)dx\). Let

\[{\cal C}_{0}\equiv\{f:f:\mathbb{R}\to\mathbb{C},\ \mbox{\rm continuous and}\ \lim_{|x|\to\infty}f(x)=0\}. \tag{7.2}\]

**Definition 5.7.1:** For \(f\in L^{1}(\mathbb{R})\), \(t\in\mathbb{R}\),

\[\hat{f}(t)\equiv\int f(x)e^{-\iota tx}dx \tag{7.3}\]

is called the _Fourier transform of \(f\)_.

**Proposition 5.7.1:**_Let \(f\in L^{1}(\mathbb{R})\) and \(\hat{f}(\cdot)\) be as in (7.3). Then_

* \(\hat{f}(\cdot)\in{\cal C}_{0}\)_._
* _If_ \(f_{a}(x)\equiv f(x-a)\)_,_ \(a\in\mathbb{R}\)_, then_ \(\hat{f}_{a}(t)=e^{\iota ta}\hat{f}(t)\)_,_ \(t\in\mathbb{R}\)_._

**Proof:**

* For any \(t\in\mathbb{R}\), \(t_{n}\to t\Rightarrow e^{\iota t_{n}x}f(x)\to e^{\iota tx}f(x)\) for all \(x\in\mathbb{R}\) and since \(|e^{\iota t_{n}x}f(x)|\leq|f(x)|\) for all \(n\) and \(x\), by the DCT \(\hat{f}(t_{n})\to\hat{f}(t)\). To show that \(\hat{f}(t)\to 0\) as \(|t|\to\infty\), the same proof as that of Theorem 5.6.6 works. Thus, it holds if \(f=I_{[a,b]}\), for \(a\), \(b\in\mathbb{R}\), \(a<b\) and since the step functions are dense in \(L^{1}(\mathbb{R})\), it holds for all \(f\in L^{1}(\mathbb{R})\).

* This is a consequence of the translation invariance of \(m(\cdot)\), i.e., \(m(A+a)=m(A)\) for all \(A\in{\cal B}({\mathbb{R}})\), \(a\in{\mathbb{R}}\). \(\Box\)

The continuity of \(\hat{f}(\cdot)\) can be strengthened to differentiability if, in addition to \(f\in L^{1}({\mathbb{R}})\), \(xf(x)\in L^{1}({\mathbb{R}})\). More generally, if \(f\in L^{1}({\mathbb{R}})\) and \(x^{k}f(x)\in L^{1}({\mathbb{R}})\) for some \(k\geq 1\), then \(\hat{f}(\cdot)\) is differentiable \(k\)-times with all derivatives \(\hat{f}^{(r)}(t)\to 0\) as \(|t|\to\infty\) for \(r\leq k\) (Problem 5.22).

**Proposition 5.7.2:** _Let \(f\), \(g\in L^{1}({\mathbb{R}})\) and \(f*g\) be their convolution as defined in (4.7). Then_

\[\widehat{f*g}=\hat{f}\hat{g}. \tag{7.4}\]

**Proof:**

\[\widehat{f*g}(t) = \int_{\mathbb{R}}e^{-\iota tx}\Big{(}\int_{\mathbb{R}}f(x-u)g(u) du\Big{)}dx \tag{7.5}\] \[= \int_{\mathbb{R}}\Big{(}\int_{\mathbb{R}}e^{-\iota t(x-u)}f(x-u) e^{-\iota tu}g(u)du\Big{)}dx\] \[= \int_{\mathbb{R}}(f_{t}*g_{t})(x)dx\]

where \(f_{t}(x)=e^{-\iota tx}f(x)\), \(g_{t}(x)=e^{-\iota tx}g(x)\). Thus, by Proposition 5.4.4

\[\widehat{f*g}(t) = \Big{(}\int_{\mathbb{R}}f_{t}(x)dx\Big{)}\Big{(}\int_{\mathbb{R} }g_{t}(x)dx\Big{)}\] \[= \hat{f}(t)\hat{g}(t).\]

The process of recovering \(f\) from \(\hat{f}\) (i.e., that of finding an inversion formula) can be developed along the lines of Fejer's theorem (Theorem 5.6.1).

**Theorem 5.7.3:** (_Fejer's theorem_). _Let \(f\in L^{1}({\mathbb{R}})\), \(\hat{f}(\cdot)\) be as in (7.3) and_

\[S_{T}(f,x) \equiv \frac{1}{2\pi}\int_{-T}^{T}\hat{f}(t)e^{\iota tx}dt,\ T\geq 0, \tag{7.6}\] \[D_{R}(f,x) \equiv \frac{1}{R}\int_{0}^{R}S_{T}(f,x)dT,\ R\geq 0. \tag{7.7}\]

* _If_ \(f\) _is continuous at_ \(x_{0}\) _and_ \(f\) _is bounded on_ \({\mathbb{R}}\)_, then_ \[\lim_{R\to\infty}D_{R}(f,x_{0})=f(x_{0}).\] (7.8)
* _If_ \(f\) _is uniformly continuous and bounded on_ \({\mathbb{R}}\)_, then_ \[\lim_{R\to\infty}D_{R}(f,\cdot)=f(\cdot)\ \mbox{uniformly on}\ {\mathbb{R}}.\] (7.9)_._
3. _As_ \(R\to\infty\)_,_ \[D_{R}(f,\cdot)\to f(\cdot)\quad\mbox{in}\quad L^{1}(\mathbb{R}).\] (7.10)
4. _If_ \(f\in L^{p}(\mathbb{R})\)_,_ \(1\leq p<\infty\)_, then as_ \(R\to\infty\)_,_ \[D_{R}(f,\cdot)\to f(\cdot)\quad\mbox{in}\quad L^{p}(\mathbb{R}).\] (7.11)

**Corollary 5.7.4:** (_Uniqueness theorem_). _If_ \(f\) _and_ \(g\in L^{1}(\mathbb{R})\) _and_ \(\hat{f}(\cdot)=\hat{g}(\cdot)\)_, then_ \(f=g\) _a.e._ \((m)\)_._

**Proof:** Let \(h=f-g.\) Then \(h\in L^{1}(\mathbb{R})\) and \(\hat{h}(\cdot)\equiv 0.\) Thus, \(S_{T}(h,\cdot)\equiv 0\) and \(D_{R}(h,\cdot)\equiv 0\) where \(S_{T}(h,\cdot)\) and \(D_{R}(h,\cdot)\) are as in (7.6) and (7.7). Hence by Theorem 5.7.3 (iii), \(h=0\) a.e. (m), i.e., \(f=g\) a.e. (m). \(\Box\)

**Corollary 5.7.5:** (_Inversion formula_). _Let \(f\in L^{1}(\mathbb{R})\) and \(\hat{f}\in L^{1}(\mathbb{R})\). Then_

\[f(x)=\frac{1}{2\pi}\int_{\mathbb{R}}\hat{f}(t)e^{\iota tx}dx\quad\mbox{a.e.} \quad(m). \tag{7.12}\]

**Proof:** Since \(\hat{f}\in L^{1}(\mathbb{R})\), by the DCT,

\[S_{T}(f,x)\to\frac{1}{2\pi}\int_{\mathbb{R}}\hat{f}(t)e^{\iota tx}dt\quad\mbox {as}\quad T\to\infty\]

for all \(x\) in \(\mathbb{R}\) and hence \(D_{R}(f,x)\) has the same limit as \(R\to\infty.\) Now (7.12) follows from (7.10). \(\Box\)

The following results, i.e., Lemma 5.7.6 and Lemma 5.7.7, are needed for the proof of Theorem 5.7.3. The first one is an analog of Lemma 5.6.2.

**Lemma 5.7.6:** (_Fejer_). _For \(R>0\), let_

\[K_{R}(x)\equiv\frac{1}{2\pi}\frac{1}{R}\int_{0}^{R}\Big{(}\int_{-T}^{T}e^{ \iota tx}dt\Big{)}dT. \tag{7.13}\]

_Then_

1. \[K_{R}(x)=\left\{\begin{array}{ll}\frac{1}{\pi}\frac{(1-\cos Rx)}{x^{2}},&x \neq 0\\ \frac{R}{2\pi}&x=0,\end{array}\right.\] (7.14) _and hence_ \(K_{R}(\cdot)\geq 0\)_._
2. _For_ \(\delta>0\)_,_ \[\int_{|x|\geq\delta}K_{R}(x)dx\to 0\quad\mbox{as}\quad R\to\infty.\] (7.15)_(iii)_

\[\int_{-\infty}^{\infty}K_{R}(x)dx=1. \tag{7.16}\]

**Proof:**

* \(K_{R}(0)=\frac{1}{2\pi R}\int_{0}^{R}(2T)dT=\frac{1}{2\pi}R.\) For \(x\neq 0\) and \(R>0\), \[K_{R}(x) = \frac{1}{2\pi R}\int_{0}^{R}\frac{2\sin Tx}{x}dT\] \[= \frac{1}{2\pi R}\;\frac{2(1-\cos Rx)}{x^{2}}.\]
* For \(\delta>0\), \[0\leq\int_{|x|\geq\delta}K_{R}(x)dx \leq \frac{2}{2\pi R}\int_{|x|\geq\delta}\frac{1}{x^{2}}\,dx\] \[= \frac{2}{\pi R}\,\frac{1}{\delta}\to 0\quad\mbox{as}\quad R \rightarrow\infty.\]
* \[\int_{-\infty}^{\infty}K_{R}(x)dx = \frac{2}{\pi}\,\frac{1}{R}\int_{0}^{\infty}\Big{(}\frac{1-\cos Rx }{x^{2}}\Big{)}dx\] \[= \frac{2}{\pi}\int_{0}^{\infty}\Big{(}\frac{1-\cos u}{u^{2}}\Big{)} du.\]

Now

\[\int_{0}^{\infty}\frac{1-\cos u}{u^{2}}\,du\] \[= \lim_{L\rightarrow\infty}\int_{0}^{L}\Big{(}\frac{1-\cos u}{u^{2 }}\Big{)}du\quad\mbox{(by the MCT)}\] \[= \lim_{L\rightarrow\infty}\int_{0}^{L}\Big{(}\int_{0}^{u}\sin x\, dx\Big{)}\frac{1}{u^{2}}\,du\] \[= \lim_{L\rightarrow\infty}\int_{0}^{L}\sin x\Big{(}\int_{x}^{L} \frac{1}{u^{2}}\,du\Big{)}dx\quad\mbox{(by Fubini's theorem)}\] \[= \lim_{L\rightarrow\infty}\Big{(}\int_{0}^{L}\frac{\sin x}{x}\,dx -\frac{1}{L}\int_{0}^{L}\sin x\,dx\Big{)}\] \[= \lim_{L\rightarrow\infty}\int_{0}^{L}\frac{\sin x}{x}\,dx\quad \mbox{since}\quad\Big{|}\int_{0}^{L}\sin x\,dx\Big{|}\leq 1.\]Thus,

\[\int_{0}^{\infty}\frac{1-\cos u}{u^{2}}du=\int_{0}^{\infty}\frac{\sin x}{x}dx= \frac{\pi}{2} \tag{7.17}\]

(cf. Problem 5.9). Hence (iii) follows. \(\Box\)

**Lemma 5.7.7:** _Let \(f\in L^{p}(\mathbb{R})\), \(0<p<\infty\). Then_

\[\int_{-\infty}^{\infty}|f(x-u)-f(x)|^{p}dx\to 0\quad\mbox{as}\quad|u|\to 0. \tag{7.18}\]

**Proof:** The lemma holds if \(f\in\mathcal{C}_{K}\), i.e., if \(f\) is continuous on \(\mathbb{R}\) (with values in \(\mathbb{C}\)) and vanishes outside a bounded interval. By Theorem 2.3.14, such functions are dense in \(L^{p}(\mathbb{R})\). So given \(f\in L^{p}(\mathbb{R})\), \(0<p<\infty\) and \(\epsilon>0\), let \(g\in\mathcal{C}_{K}\) be such that

\[\int|f-g|^{p}dm<\epsilon.\]

For any \(0<p<\infty\), there is a \(0<c_{p}<\infty\) such that for all \(x,y,z\in(0,\infty)\), \(|x+y+z|^{p}\leq c_{p}(|x|^{p}+|y|^{p}+|z|^{p})\). Then,

\[\int|f(x-u)-f(x)|^{p}dx\] \[\leq c_{p}\int\big{(}|f(x-u)-g(x-u)|^{p}+|g(x-u)-g(x)|^{p}\] \[\quad+\int|g(x)-f(x)|^{p}\big{)}dx\] \[= c_{p}\Big{(}2\epsilon+\int|g(x-u)-g(x)|^{p}\Big{)}du.\]

So

\[\varlimsup_{|u|\to 0}\int|f(x-u)-f(x)|^{p}dx\leq c_{p}\,2\epsilon.\]

Since \(\epsilon>0\) is arbitrary, the lemma is proved. \(\Box\)

**Proof of Theorem 5.7.3:** From (7.7)

\[D_{R}(f,x) \equiv \frac{1}{2\pi R}\int_{0}^{R}\Big{(}\int_{-T}^{T}e^{\iota tx} \Big{(}\int_{-\infty}^{\infty}e^{-\iota ty}f(y)dy\Big{)}dt\Big{)}dT\] \[= \frac{1}{2\pi}\frac{1}{R}\int_{0}^{R}\Big{(}\int_{-T}^{T}\Big{(} \int_{-\infty}^{\infty}e^{\iota tu}f(x-u)du\Big{)}dt\Big{)}dT.\]

Now Fubini's theorem yields

\[D_{R}(f,x) = \int_{-\infty}^{\infty}f(x-u)\Big{(}\frac{1}{2\pi}\frac{1}{R}\int _{0}^{R}\Big{(}\int_{-T}^{T}e^{\iota tu}dt\Big{)}dR\Big{)}du \tag{7.19}\] \[= \int_{-\infty}^{\infty}f(x-u)K_{R}(u)du\]where \(K_{R}(\cdot)\) is as in (7.13).

Now let \(f\) be continuous at \(x_{0}\) and bounded on \(\mathbb{R}\) by \(M_{f}\). Fix \(\epsilon>0\) and choose \(\delta>0\) such that \(|x-x_{0}|<\delta\Rightarrow|f(x)-f(x_{0})|<\epsilon\). From (7.16) and (7.19),

\[D_{R}(f,x_{0})-f(x_{0})=\int\big{(}f(x_{0}-u)-f(x_{0})\big{)}K_{R}(u)du\]

implying

\[|D_{R}(f,x_{0})-f(x_{0})| \leq \int_{|u|<\delta}|f(x_{0}-u)-f(x_{0})|K_{R}(u)du\] \[+\int_{|u|\geq\delta}|f(x_{0}-u)-f(x_{0})|K_{R}(u)du\] \[<\epsilon+2M_{f}\int_{|u|\geq\delta}K_{R}(u)du.\]

Now from (7.15), it follows that

\[\varlimsup_{R\to\infty}|D_{R}(f,x_{0})-f(x_{0})|\leq\epsilon,\]

proving (i).

The proof of (ii) is similar to this and is omitted.

Clearly (iv) implies (iii). To establish (iv), note that for \(1\leq p<\infty\), by Jensen's inequality (which applies since \(K_{R}(u)\geq 0\) and \(\int K_{R}(u)du=1\)), for \(x\) in \(\mathbb{R}\),

\[|D_{R}(f,x)-f(x)|^{p}\leq\int\big{|}\big{(}f(x-u)-f(x)\big{)}\big{|}^{p}K_{R}(u )du\]

and hence

\[\int|D_{R}(f,x)-f(x)|^{p}dx\leq\int\Big{(}\int|f(x-u)-f(x)|^{p}dx\Big{)}K_{R}(u )du.\]

Now (7.18) and the arguments in the proof of (i) yield (iv). \(\Box\)

### 5.8 Plancherel transform

If \(f\in L^{2}(\mathbb{R})\), it need not be in \(L^{1}(\mathbb{R})\) and so the Fourier transform is not defined. However, it is possible to extend the definition using an approximation procedure due to Plancherel.

**Proposition 5.8.1:**_Let \(f\in L^{1}(\mathbb{R})\cap L^{2}(\mathbb{R})\) and let \(\hat{f}(t)\equiv\int f(x)e^{-\iota tx}dx\), \(t\in\mathbb{R}\). Then, \(\hat{f}\in L^{2}(\mathbb{R})\) and_

\[\int|f|^{2}dm=\frac{1}{2\pi}\int|\hat{f}|^{2}dm. \tag{8.1}\]

**Proof:** Let \(\tilde{f}(x)=\overline{f(-x)}\) and \(g=f*\tilde{f}\). Since \(f\) and \(\tilde{f}\) are in \(L^{1}(\mathbb{R})\), \(g\) is well defined and is in \(L^{1}(\mathbb{R})\). Further by Cauchy-Schwarz inequality,

\[|g(x_{1})-g(x_{2})| \leq \int|f(x_{1}-u)-f(x_{2}-u)|\,|\tilde{f}(u)|du\] \[\leq \Big{(}\int|f(x_{1}-u)-f(x_{2}-u)|^{2}du\Big{)}^{1/2}\Big{(}\int| \tilde{f}(u)|^{2}du\Big{)}.\]

By Lemma 5.7.7, the right side goes to zero uniformly as \(|x_{1}-x_{2}|\to 0\). Thus \(g(\cdot)\) is uniformly continuous. Also, by Cauchy-Schwarz inequality,

\[|g(x)| = \Big{|}\int f(x-u)\tilde{f}(u)du\Big{|}\] \[\leq \Big{(}\int|f(u)|^{2}du\Big{)}^{1/2}\Big{(}\int|\tilde{f}(u)|^{2} du\Big{)}^{1/2}\] \[= \int|f(u)|^{2}du,\]

and hence \(g(\cdot)\) is bounded. Thus, \(g\) is continuous, bounded, and integrable on \(\mathbb{R}\). By Theorem 5.7.3 (Fejer's theorem)

\[D_{R}(g,0)\to g(0)\quad\mbox{as}\quad R\to\infty. \tag{8.2}\]

But

\[g(0)=\int f(u)\overline{f(-u)}du=\int|f|^{2}dm. \tag{8.3}\]

By Proposition 5.7.2, \(\hat{g}(t)=\hat{f}(t)\hat{\tilde{f}}(t)\). Also, note that

\[\hat{\tilde{f}}(t) = \int\tilde{f}(x)e^{-\iota tx}dx\] \[= \int\overline{f(-x)}e^{-\iota tx}dx\] \[= \overline{\hat{f}(t)}\]

and hence \(\hat{g}(t)=|\hat{f}(t)|^{2}\geq 0\). But

\[D_{R}(g,0) = \frac{1}{2\pi}\frac{1}{R}\int_{0}^{R}\Big{(}\int_{-T}^{T}\hat{g} (t)dt\Big{)}dT\] \[= \frac{1}{\pi}\int_{0}^{R}\hat{g}(t)\Big{(}1-\frac{|t|}{R}\Big{)}dt.\]

Since \(\hat{g}(\cdot)\geq 0\), by the MCT,

\[\int_{0}^{R}\hat{g}(t)\Big{(}1-\frac{|t|}{R}\Big{)}dt\uparrow\int_{0}^{\infty} \hat{g}(t)dt\quad\mbox{as}\quad R\uparrow\infty.\]Thus, \(\lim_{R\to\infty}D_{R}(g,0)=\frac{1}{\pi}\int_{0}^{\infty}\hat{g}(t)dt.\) Since \(\hat{g}(-t)=\hat{g}(t)\) for all \(t\) in \(\mathbb{R}\),

\[\lim_{R\to\infty}D_{R}(g,0)=\frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{g}(t)dt= \frac{1}{2\pi}\int|\hat{f}(t)|^{2}dt. \tag{8.4}\]

Clearly, (8.2)-(8.4) imply (8.1). \(\Box\)

**Proposition 5.8.2:** _Let \(f\in L^{2}(\mathbb{R})\) and \(f_{n}(\cdot)\equiv fI_{[-n,n]}(\cdot)\), \(n\geq 1\). Then, \(\{f_{n}\}_{n\geq 1}\), \(\{\hat{f}_{n}\}_{n\geq 1}\) are both Cauchy in \(L^{2}(\mathbb{R})\) and hence, convergent in \(L^{2}(\mathbb{R})\)._

**Proof:** Since for each \(n\geq 1\), \(f_{n}\in L^{2}(\mathbb{R})\cap L^{1}(\mathbb{R})\), by Proposition 5.8.1,

\[\|f_{n_{1}}-f_{n_{2}}\|_{2}^{2}=\frac{1}{2\pi}\int|\hat{f}_{n_{1}}-\hat{f}_{n_ {2}}|^{2}dm. \tag{8.5}\]

Since \(f\in L^{2}(\mathbb{R})\), \(f_{n}\to f\) in \(L^{2}(\mathbb{R})\), \(\{f_{n}\}_{n\geq 1}\) is Cauchy in \(L^{2}(\mathbb{R})\). By (8.5) \(\{\hat{f}_{n}\}_{n\geq 1}\) is also Cauchy in \(L^{2}(\mathbb{R})\). \(\Box\)

**Definition 5.8.1:** Let \(f\in L^{2}(\mathbb{R})\). The _Plancherel transform of \(f\)_, denoted by \(\hat{f}\), is defined as \(\lim_{n\to\infty}\hat{f}_{n}\), where

\[\hat{f}_{n}(t)=\int_{-n}^{n}e^{-\iota tx}f(x)dx \tag{8.6}\]

and the limit is taken in \(L^{2}(\mathbb{R})\).

**Theorem 5.8.3:** _Let \(f\in L^{2}(\mathbb{R})\) and \(\hat{f}\) be its Plancherel transform. Then_

* \[\int|f|^{2}dm=\frac{1}{2\pi}\int|\hat{f}|^{2}dm.\] (8.7)
* _For_ \(f\in L^{1}(\mathbb{R})\cap L^{2}(\mathbb{R})\)_, the Plancherel transform coincides with the Fourier transform._

**Proof:** From Propositions 5.8.1 and 5.8.2 and the definition of \(\hat{f}\),

\[\frac{1}{2\pi}\int|\hat{f}|^{2}dm=\lim_{n\to\infty}\frac{1}{2\pi}\int|\hat{f} _{n}|^{2}dm=\lim_{n\to\infty}\int|f_{n}|^{2}dm=\int|f|^{2}dm\]

proving (i). If \(f\in L^{1}(\mathbb{R})\cap L^{2}(\mathbb{R})\), \(\hat{f}_{n}\) defined in (8.6) converges as \(n\to\infty\) pointwise to the Fourier transform of \(f\) (by the DCT). But by Definition 5.7.1, \(\hat{f}_{n}\) converges in \(L^{2}(\mathbb{R})\) to the Plancherel transform of \(f\). So (ii) follows. \(\Box\)

It can also be shown that for \(f\), \(\hat{f}\) as in the above theorem, the following _inversion formula_ holds:

\[\frac{1}{2\pi}\int_{-n}^{n}\hat{f}(t)e^{\iota tx}dt\to f(x)\quad\mbox{in}\quad L ^{2}(\mathbb{R}) \tag{8.8}\]and that the map \(f\to\hat{f}\) is a Hilbert space isomorphism of \(L^{2}(\mathbb{R})\) onto \(L^{2}(\mathbb{R})\). For a proof, see Rudin (1987), Chapter 9. This in turn implies that if \(f\), \(g\in L^{2}(\mathbb{R})\) with Plancherel transforms \(\hat{f}\) and \(\hat{g}\), respectively, then

\[\int f\,\bar{g}\,dm=\frac{1}{2\pi}\int\hat{f}\,\bar{\hat{g}}\,dm. \tag{8.9}\]

This is known as the _Parseval identity_.

### 8.9 Problems

1. Verify that if \(\mu_{1}\) and \(\mu_{2}\) are finite measures on \((\Omega_{1},\mathcal{F}_{1})\) and \((\Omega_{2},\mathcal{F}_{2})\), respectively, then \(\mu_{12}(\cdot)\) and \(\mu_{21}(\cdot)\), defined by (1.4) and (1.5), respectively, are measures on \((\Omega_{1}\times\Omega_{2},\,\mathcal{F}_{1}\times\mathcal{F}_{2})\). (**Hint:** Use the MCT.)
2. Let \((\Omega,\mathcal{F},\mu)\) be a complete measure space such that \(\mathcal{F}\neq\mathcal{P}(\Omega)\) and for some \(A_{0}\in\mathcal{F}\) with \(A_{0}\neq\emptyset\), \(\mu(A_{0})=0\). Let \(B\in\mathcal{P}(\Omega)\setminus\mathcal{F}\). Then show that \((\mu\times\mu)(\Omega\times A_{0})=0\) but \(B\times A_{0}\not\in\mathcal{F}\times\mathcal{F}\). Conclude that \((\Omega\times\Omega,\mathcal{F}\times\mathcal{F},\mu\times\mu)\) is not complete. (An example of such a measure space \((\Omega,\mathcal{F},\mu)\) is the space \(([0,1],\mathcal{M}_{L},m)\) where \(\mathcal{M}_{L}\) is the Lebesgue \(\sigma\)-algebra on \([0,1]\) and \(m\) is the Lebesgue measure.)
3. Let \(\mu_{i}\), \(i=1,2\) be two \(\sigma\)-finite measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Let \(D_{i}=\{x:\mu_{i}(\{x\})>0\},i=1,2\). 1. Show that \(D_{1}\cup D_{2}\) is countable. 2. Let \(\phi_{i}(x)=\mu_{i}(\{x\})\)\(x\in\mathbb{R}\), \(i=1,2\). Show that \(\phi_{i}\) is Borel measurable for \(i=1,2\). 3. Show that \(\int\phi_{1}d\mu_{2}=\sum_{z\in D_{1}\cap D_{2}}\phi_{1}(z)\phi_{2}(z)\). 4. Deduce (2.11) from (c).
4. Extend Theorem 5.2.3 as follows. Let \(F_{1},F_{2}\) be two nondecreasing right continuous functions on \([a,b]\). Then \[\int_{(a,b]}F_{1}dF_{2}+\int_{(a,b]}F_{2}dF_{1}=F_{1}(b)F_{2}(b)-F_{1}(a)F_{2}( a)+\int_{(a,b]}\phi_{1}d\mu_{2},\] where \(\phi_{1}\) is as in Problem 5.3.
5. Let \(F_{i}:\mathbb{R}\to\mathbb{R}\) be nondecreasing and right continuous, \(i=1,2\). Show that if \(\lim_{b\uparrow\infty}F_{1}(b)F_{2}(b)=\lambda_{1}\) and \(\lim_{a\downarrow-\infty}F_{1}(a)F_{2}(a)=\lambda_{2}\) exist and are finite, then \[\int_{\mathbb{R}}F_{1}dF_{2}+\int_{\mathbb{R}}F_{2}dF_{1}=\lambda_{1}-\lambda _{2}+\int_{\mathbb{R}}\phi_{1}d\mu_{2}\] where \(\phi_{1}\) is as in Problem 5.3.

* Let \((\Omega,\mathcal{F},\mu)\) be a \(\sigma\)-finite measure space and \(f\) be a nonnegative measurable function. Then, \(\int_{\Omega}fd\mu=\int_{[0,\infty)}\mu(\{f\geq t\})dt\). (**Hint:** Consider the product space of \((\Omega,\mathcal{F},\mu)\) with \((\mathbb{R}_{+},\mathcal{B}(\mathbb{R}_{+}),m)\) and apply Tonelli's theorem to the function \(g(\omega,t)=I(f(\omega)\geq t)\), after showing that \(g\) is \(\mathcal{F}\times\mathcal{B}(\mathbb{R}_{+})\))-measurable.)
* Let \((\Omega,\mathcal{F},P)\) be a probability space and \(X:\Omega\to\mathbb{R}_{+}\) be a random variable. 1. Show that for any \(h:\mathbb{R}_{+}\to\mathbb{R}_{+}\) that is absolutely continuous, \[\int_{\Omega}h(X)dP = h(0)+\int_{[0,\infty)}h^{\prime}(t)P(X\geq t)dt\] \[= h(0)+\int_{(0,\infty)}h^{\prime}(t)P(X>t)dt.\] 2. Show that for any \(0<p<\infty\), \[\int_{\Omega}X^{p}dP=\int_{[0,\infty)}pt^{p-1}P(X\geq t)dt.\] 3. Show that for any \(0<p<\infty\), \[\int_{\Omega}X^{-p}dP=\frac{1}{\Gamma(p)}\int_{[0,\infty)}\psi_{X}(t)t^{p-1}dt,\] where \(\Gamma(p)=\int_{[0,\infty)}e^{-t}t^{p-1}dt,p>0\), and \(\psi_{X}(t)=\int_{\Omega}e^{-tX}dP\), \(t\in\mathbb{R}_{+}\). (**Hint:** (a) Apply Tonelli's theorem to the function \(f(t,\omega)\equiv h^{\prime}(t)I(X(w)\geq t)\) on the product measure space \(([0,\infty)\times\Omega,\mathcal{B}([0,\infty))\times\mathcal{F},m\times P)\), where \(m\) is Lebesgue measure on \((\mathbb{R}_{+},\mathcal{B}(\mathbb{R}_{+}))\).)
* Let \(g:\mathbb{R}_{+}\to\mathbb{R}_{+}\) and \(f:\mathbb{R}^{2}\to\mathbb{R}_{+}\) be Borel measurable. Let \(A=\{(x,y):x\geq 0,0\leq y\leq g(x)\}\). 1. Show that \(A\in\mathcal{B}(\mathbb{R}^{2})\). 2. Show that \[\int_{\mathbb{R}_{+}}\Big{(}\int_{[0,g(x)]}f(x,y)m(dy)\Big{)}m(dx)=\int fI_{A} dm^{(2)},\] where \(m^{(2)}\) is Lebesgue measure on \((\mathbb{R}^{2},\mathcal{B}(\mathbb{R}^{2}))\) and \(m(\cdot)\) is Lebesgue measure on \(\mathbb{R}\).

3. If \(g\) is continuous and strictly increasing show that the two integrals in (b) equal \[\int_{\mathbb{R}_{+}}\Big{(}\int_{[g^{-1}(y),\infty)}f(x,y)m(dx)\Big{)}m(dy).\]

5.9. 1. For \(1<A<\infty\), let \[h_{A}(t)=\int_{0}^{A}e^{-xt}\sin x\,dx,\ t\geq 0.\] Use integration by parts to show that \[|h_{A}(t)|\leq\frac{1}{1+t^{2}}+e^{-t}\] and \[h_{A}(t)\to\frac{1}{1+t^{2}}\quad\mbox{as}\quad A\to\infty.\] 2. Show using Fubini's theorem that for \(0<A<\infty\), \[\int_{0}^{\infty}h_{A}(t)dt=\int_{0}^{A}\frac{\sin x}{x}\,dx.\] 3. Conclude using the DCT that \[\lim_{A\to\infty}\int_{0}^{A}\frac{\sin x}{x}\,dx=\int_{0}^{\infty}\frac{1}{1 +t^{2}}\,dt.\] 4. Using Theorem 4.4.1 and the fact that \(\phi(x)\equiv\tan x\) is a (1-1) strictly monotone map from \((0,\frac{\pi}{2})\) to \((0,\infty)\) having the inverse map \(\psi(\cdot)\) with derivative \(\psi^{\prime}(t)\equiv\frac{1}{1+t^{2}}\), \(0<t<\infty\), conclude that \[\int_{0}^{\infty}\frac{1}{1+t^{2}}dt=\frac{\pi}{2}\.\] 5.10. Show that \(I\equiv\int_{0}^{\infty}e^{-x^{2}/2}dx=\sqrt{\frac{\pi}{2}}\). (**Hint:** By Tonelli's theorem, \(I^{2}=\int_{0}^{\infty}\int_{0}^{\infty}e^{-\frac{(x^{2}+y^{2})}{2}}dxdy.\) Now use the change of variables \(x=r\cos\theta\), \(y=r\sin\theta\), \(0<r<\infty\), \(0<\theta<\frac{\pi}{2}\).)
5.11. Let \(\mu\) be a finite measure on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\). Let \(f\), \(g:\mathbb{R}\to\mathbb{R}_{+}\) be nondecreasing. Show that \[\mu(\mathbb{R})\int fg\,d\mu\geq\Big{(}\int fd\mu\Big{)}\Big{(}\int gd\mu\Big{)}.\] (**Hint:** Consider \(h(x_{1},x_{2})=\big{(}f(x_{1})-f(x_{2})\big{)}\big{(}g(x_{1})-g(x_{2})\big{)}\) on \(\mathbb{R}^{2}\) and integrate w.r.t. \(\mu\times\mu\).)
* Let \(\mu\) and \(\lambda\) be \(\sigma\)-finite measures on \(\bigl{(}\mathbb{R},\mathcal{B}(\mathbb{R})\bigr{)}\). Recall that \[\nu(A)\equiv(\mu*\lambda)(A)\equiv\int\int I_{A}(x+y)d\mu d\lambda,\ A\in \mathcal{B}(\mathbb{R}).\]
* Show that for any Borel measurable \(f:\mathbb{R}\to\mathbb{R}_{+}\), \(f(x+y)\) is \(\langle\mathcal{B}(\mathbb{R})\times\mathcal{B}(\mathbb{R}),\mathcal{B}( \mathbb{R})\rangle\)-measurable from \(\mathbb{R}\times\mathbb{R}\to\mathbb{R}\) and \[\int fd\nu=\int\int f(x+y)d\mu d\lambda.\]
* Show that \[\nu(A)=\int_{\mathbb{R}}\mu(A-t)\lambda(dt),\ A\in\mathcal{B}(\mathbb{R}).\]
* Suppose there exist countable sets \(B_{\lambda}\), \(B_{\mu}\) such that \(\mu(B_{\mu}^{c})=0=\lambda(B_{\lambda}^{c})\). Show that there exists a countable set \(B_{\nu}\) such that \(\nu(B_{\nu}^{c})=0\).
* Suppose that \(\mu(\{x\})=0\) for all \(x\) in \(\mathbb{R}\). Show that \(\nu(\{x\})=0\) for all \(x\) in \(\mathbb{R}\).
* Suppose that \(\mu\ll m\) with \(\frac{d\mu}{dm}=h\). Show that \(\nu\ll m\) and find \(\frac{d\nu}{dm}\) in terms of \(h\), \(\mu\) and \(\lambda\).
* Suppose that \(\mu\ll m\) and \(\lambda\ll m\). Show that \[\frac{d\nu}{dm}=\frac{d\mu}{dm}*\frac{d\lambda}{dm}.\]
* (_Convolution of cdfs_). Let \(F_{i}\), \(i=1,2\) be cdfs on \(\mathbb{R}\). Recall that a cdf \(F\) on \(\mathbb{R}\) is a function from \(\mathbb{R}\to\mathbb{R}_{+}\) such that it is nondecreasing, right continuous with \(F(x)\to 0\) as \(x\to-\infty\) and \(F(x)\to 1\) as \(x\to\infty\).
* Show that \((F_{1}*F_{2})(x)\equiv\int_{\mathbb{R}}F_{1}(x-u)dF_{2}(u)\) is well defined and is a cdf on \(\mathbb{R}\).
* Show also that \((F_{1}*F_{2})(\cdot)=(F_{2}*F_{1})(\cdot)\).
* Suppose \(t\in\mathbb{R}\) is such that \(\int e^{tx}dF_{i}(x)<\infty\) for \(i=1,2\). Show that \(\int e^{tx}d(F_{1}*F_{2})(x)=\Big{(}\int e^{tx}dF_{1}(x)\Big{)}\Big{(}\int e^{ tx}dF_{2}(x)\Big{)}\).
* Let \(f\), \(g\in L^{1}(\mathbb{R},\mathcal{B}(\mathbb{R}),m)\).
* Show that if \(f\) is continuous and bounded on \(\mathbb{R}\), then so is \(f*g\).
* Show that if \(f\) is differentiable with a bounded derivative on \(\mathbb{R}\), then so is \(f*g\). (**Hint:*
* Use the DCT.)
5.15: Let \(f\in L^{1}(\mathbb{R})\), \(g\in L^{p}(\mathbb{R})\), \(1\leq p\leq\infty\). 1. Show that if \(1\leq p<\infty\), then for all \(x\) in \(\mathbb{R}\) \[\Big{|}\int|f(u)g(x\!-\!u)|du\Big{|}^{p}\leq\Big{(}\int|f|dm\Big{)}^{p-1}\Big{(} \int|g(x\!-\!u)|^{p}|f(u)|du\Big{)}\] and hence that \[(f\ast g)(x)\equiv\int f(u)g(x-u)du\] is well defined a.e. \((m)\) and \[\|f\ast g\|_{p}\leq\|f\|_{1}\,\|g\|_{p}\] with "\(=\)" holding iff either \(f=0\) a.e. or \(g=0\) a.e. 2. Show that if \(p=\infty\) then \[\|f\ast g\|_{\infty}\leq\|f\|_{1}\,\|g\|_{\infty}\] and "\(=\)" can hold for some nonzero \(f\) and \(g\). (**Hint for (a):** Use Jensen's inequality with probability measure \(d\mu=\frac{|f|dm}{\|f\|_{1}}\) if \(\|f\|_{1}>0\).) 5.16: Let \(1\leq p\leq\infty\) and \(q=1-\frac{1}{p}\). Let \(f\in L^{p}(\mathbb{R})\), \(g\in L^{q}(\mathbb{R})\). 1. Show that \(f\ast g\) is well defined and uniformly continuous. 2. Show that if \(1<p<\infty\), \[\lim_{|x|\to\infty}(f\ast g)(x)=0.\] (**Hint:** For (a) use Holder's inequality and Lemma 5.7.7. For (b) approximate \(g\) by simple functions.)
5.17: Let \(g:\mathbb{R}\to\mathbb{R}\) be infinitely differentiable and be zero outside a bounded interval. 1. Let \(f:\mathbb{R}\to\mathbb{R}\) be Borel measurable and \(\int_{A}|f|dm<\infty\) for all bounded intervals \(A\) in \(\mathbb{R}\). Show that \(f\ast g\) is well defined and infinitely differentiable. 2. Show that for any \(f\in L^{1}(\mathbb{R})\), there exist a sequence \(\{g_{n}\}_{n\geq 1}\) of such functions such that \(f\ast g_{n}\to f\) in \(L^{1}(\mathbb{R})\).
5.18: For \(f\in L^{1}(\mathbb{R})\), let \(f_{\sigma}=f\ast\phi_{\sigma}\) where \(\phi_{\sigma}(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^{2}}{2\sigma^{2}}}\), \(0<\sigma<\infty\), \(x\in\mathbb{R}\).

* Show that \(f_{\sigma}\) is infinitely differentiable.
* Show that if \(f\) is continuous and zero outside a bounded interval, then \(f_{\sigma}\) converges to \(g\) uniformly on \(\mathbb{R}\) as \(\sigma\to 0\).
* Show that if \(f\in L^{p}(\mathbb{R})\), \(1\leq p<\infty\), then \(f_{\sigma}\to f\) in \(L^{p}(\mathbb{R})\) as \(\sigma\to 0\).
* Let \(f\in L^{p}(\mathbb{R})\), \(1\leq p<\infty\) and \(h(x)=\int_{A+x}f(u)du\) where \(A\) is a bounded Borel set and \[A+x\equiv\{y:y=a+x,a\in A\}.\]
* Show that \(h=f*g\) for some \(g\) bounded and with bounded support.
* Show that \(h(\cdot)\) is continuous and that \(\lim_{|x|\to\infty}h(x)=0\).
* For \(1<p<\infty\), use Holder's inequality and show that \(m\big{(}(A+x_{1})\bigtriangleup(A+x_{2})\big{)}\to 0\) as \(|x_{1}-x_{2}|\to 0\).)
* Verify the claims in Examples 5.4.1 directly.
* Verify the same using generating functions.
* Let \(f\) be a probability density on \(\mathbb{R}\), i.e., \(f\) is nonnegative, Borel measurable and \(\int fdm=1\). Show that \(|\hat{f}(t)|<1\) for all \(t\neq 0\).
* If \(|\hat{f}(t_{0})|=1\) for some \(t_{0}\neq 0\), show that \(\int\big{(}1-\cos t_{0}(x-\theta)\big{)}\)\(f(x)dx=0\) for some \(\theta\).)
* Let \(f\in L^{1}(\mathbb{R})\) and \(x^{k}f(x)\in L^{1}(\mathbb{R})\) for some \(k\geq 1\). Show that \(\hat{f}(\cdot)\) is \(k\)-times differentiable on \(\mathbb{R}\) with all derivatives \(\hat{f}^{(r)}(t)\to 0\) as \(|t|\to\infty\) for \(r\leq k\).
* Let \(f\in L^{1}(\mathbb{R})\). Suppose \(\int|t\hat{f}(t)|dt<\infty\). Show that there exists a function \(g:\mathbb{R}\to\mathbb{R}\) such that it is differentiable, \(\lim_{|x|\to\infty}(|g(x)|+|g^{\prime}(x)|)=0\) and \(g=f\) a.e. \((m)\). Extend this to the case where \(\int|t^{k}\hat{f}(t)|dt<\infty\) for some \(k>1\).
* Consider \(g(x)=\frac{1}{2\pi}\int e^{-ttx}\hat{f}(t)dt\).)
* Let \(f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\), \(x\in\mathbb{R}\).
* Show that \(\hat{f}(\cdot)\) is real valued, differentiable and satisfies the ordinary differential equation \(\hat{f}^{\prime}(t)+t\hat{f}(t)=0\), \(t\in\mathbb{R}\) and \(\hat{f}(0)=1\). Find \(\hat{f}(t)\).
2. For \(\mu\) in \(\mathbb{R}\), \(\sigma>0\), let \[f_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi}\sigma}\,e^{-\frac{1}{2}(\frac{\pi-u}{ \sigma})^{2}}.\] Find \(\hat{f}_{\mu,\sigma}\) and verify that for any \((\mu_{i},\sigma_{i})\), \(i=1,2\), \[f_{\mu_{1},\sigma_{1}}*f_{\mu_{2},\sigma_{2}}=f_{\mu_{1}+\mu_{2},\sigma_{1}^{2 }+\sigma_{2}^{2}}.\] (**Hint for (b):** Use Fourier transforms and uniqueness.)
5.24 (_Rate of convergence of Fourier series_). Consider the function \(h(x)=\frac{\pi}{2}-|x|\) in \(-\pi\leq x\leq\pi\). 1. Find \(\hat{h}(n)\equiv\frac{1}{2\pi}\int_{-\pi}^{\pi}e^{-\iota nx}h(x)dx\), \(n=0,\pm 1,\pm 2\). 2. Show that \(\sum_{n=-\infty}^{\infty}|\hat{h}(n)|<\infty\). 3. Show that \(S_{n}(h,x)\equiv\sum_{j=-n}^{+n}\hat{h}(j)e^{\iota jx}\) converges to \(h(x)\) uniformly on \([-\pi,\pi]\). 4. Verify that \[\sup\{|S_{n}(h,x)-h(x)|:-\pi\leq x\leq\pi\}\leq\frac{2}{\pi}\frac{1}{(n-1)},\ n\geq 2\] and \[|S_{n}(h,0)-h(0)|\geq\frac{2}{\pi}\frac{1}{(n+2)}\.\] (**Remark:** This example shows that the Fourier series of a function can converge very slowly such as in this example where the rate of decay is \(\frac{1}{n}\).)
5.25 Using Fejer's theorem (Theorem 5.6.1) prove Wierstrass' theorem on uniform approximation of a continuous function on a bounded closed interval by a polynomial. (**Hint:** Show that on bounded intervals a trigonometric polynomial can be approximated uniformly by a polynomial using the power series representation of sine and cosine functions (see Section A.3).
5.26 Evaluate \[\lim_{n\to\infty}\int_{-n}^{n}\frac{\sin\lambda x}{x}e^{\iota tx}dx,\ 0< \lambda<\infty,\ 0<x<\infty.\] (**Hint:** For \(0<\lambda<\infty\), \(\frac{\sin\lambda x}{x}\in L^{2}(\mathbb{R})\) and it is the Fourier transform of \(f(t)=I_{[-\lambda,\lambda]}(\cdot)\). Now apply Plancherel theorem. Alternatively use Fubini's theorem and the fact \(\lim_{n\to\infty}\int_{-n}^{n}\frac{\sin y}{y}dy\) exists in \(\mathbb{R}\).)* 5.27 Find an example of a function \(f\in L^{2}(\mathbb{R})\cap\left(L^{1}(\mathbb{R})\right)^{c}\) such that its Plancherel transform \(\hat{f}\in L^{1}(\mathbb{R})\). (**Hint:** Examine Problem 5.26.)Probability Spaces

### Kolmogorov's probability model

Probability theory provides a mathematical model for random phenomena, i.e., those involving uncertainty. First one identifies the set \(\Omega\) of possible outcomes of (random) experiment associated with the phenomenon. This set \(\Omega\) is called the _sample space_, and an individual element \(\omega\) of \(\Omega\) is called a _sample point_. Even though the outcome is not predictable ahead of time, one is interested in the "chances" of some particular statement to be valid for the resulting outcome. The set of \(\omega\)'s for which a given statement is valid is called an _event_. Thus, an event is a subset of \(\Omega\). One then identifies a class \(\mathcal{F}\) of events, i.e., a class \(\mathcal{F}\) of subsets of \(\Omega\) (not necessarily all of \(\mathcal{P}(\Omega)\), the power set of \(\Omega\)), and then a set function \(P\) on \(\mathcal{F}\) such that for \(A\) in \(\mathcal{F}\), \(P(A)\) represents the "chance" of the event \(A\) happening. Thus, it is reasonable to impose the following conditions on \(\mathcal{F}\) and \(P\):

1. \(A\in\mathcal{F}\Rightarrow A^{c}\in\mathcal{F}\) (i.e., if one can define the probability of an event \(A\), then the probability of \(A\) not happening is also well defined).
2. \(A_{1}\), \(A_{2}\in\mathcal{F}\Rightarrow A_{1}\cup A_{2}\in\mathcal{F}\) (i.e., if one can define the probabilities of \(A_{1}\) and \(A_{2}\), then the probability of at least one of \(A_{1}\) or \(A_{2}\) happening is also well defined).
3. for all \(A\) in \(\mathcal{F}\), \(0\leq P(A)\leq 1\), \(P(\emptyset)=0\), and \(P(\Omega)=1\).

* \(A_{1}\), \(A_{2}\in{\cal F}\), \(A_{1}\cap A_{2}=\emptyset\Rightarrow P(A_{1}\cup A_{2})=P(A_{1})+P(A_{2})\) (i.e., if \(A_{1}\) and \(A_{2}\) are mutually exclusive events, then the probability of at least one of the two happening is simply the sum of the probabilities). The above conditions imply that \({\cal F}\) is an _algebra_ and \(P\) is a _finitely additive_ set function. Next, as explained in Section 1.2, it is natural to require that \({\cal F}\) be closed under monotone increasing unions and \(P\) be monotone continuous from below. That is, if \(\{A_{n}\}_{n\geq 1}\) is a sequence of events in \({\cal F}\) such that \(A_{n}\) implies \(A_{n+1}\) (i.e., \(A_{n}\subset A_{n+1}\)) for all \(n\geq 1\), then the probability of at least one of the \(A_{n}\)'s happening is well defined and is the limit of the corresponding probabilities. In other words, the following conditions on \({\cal F}\) and \(P\) must hold in addition to (i)-(iv) above:
* \(A_{n}\in{\cal F}\), \(A_{n}\subset A_{n+1}\) for all \(n=1,2,\ldots\quad\Rightarrow\bigcup_{n\geq 1}A_{n},\in{\cal F}\) and \(P(A_{n})\uparrow P(\bigcup_{n\geq 1}A_{n})\).

As noted in Section 1.2, conditions (i)-(v) imply that \((\Omega,{\cal F},P)\) is a _measure space_, i.e., \({\cal F}\) is a \(\sigma\)-algebra and \(P\) is a measure on \({\cal F}\) with \(P(\Omega)=1\). That is, \((\Omega,{\cal F},P)\) is a _probability space_. This is known as _Kolmogorov's probability model_ for random phenomena (see Kolmogorov (1956), Parthasarathy (2005)). Here are some examples.

**Example 6.1.1:** (_Finite sample spaces_). Let \(\Omega\equiv\{\omega_{1},\omega_{2},\ldots,\omega_{k}\},1\leq k<\infty\), \({\cal F}\equiv{\cal P}(\Omega)\), the power set of \(\Omega\) and \(P(A)=\sum_{i=1}^{k}p_{i}I_{A}(\omega_{i})\) where \(\{p_{i}\}_{i=1}^{k}\) are such that \(p_{i}\geq 0\) and \(\sum_{i=1}^{k}p_{i}=1\). This is a probability model for random experiments with finitely many possible outcomes.

An important application of this probability model is finite population sampling. Let \(\{U_{1},U_{2},\ldots,U_{N}\}\) be a finite population of \(N\) units or objects. These could be individuals in a city, counties in a state, etc. In a typical sample survey procedure, one chooses a subset of size \(n\) (\(1\leq n\leq N\)) from this population. Let \(\Omega\) denote the collection of all possible subsets of size \(n\). Here \(k={N\choose n}\), each \(\omega_{i}\) is a sample of size \(n\) and \(p_{i}\) is the selection probability of \(\omega_{i}\). The assignment of \(\{p_{i}\}_{i=1}^{k}\) is determined by a given _sampling scheme_. For example, in simple random sampling without replacement, \(p_{i}=\frac{1}{k}\) for \(i=1,2,\ldots,k\).

Other examples include coin tossing, rolling of dice, bridge hands, and acceptance sampling in statistical quality control (Feller (1968)).

**Example 6.1.2:** (_Countably infinite sample spaces_). Let \(\Omega\equiv\{\omega_{1},\omega_{2},\ldots\}\) be a countable set, \({\cal F}={\cal P}(\Omega)\), and \(P(A)\equiv\sum_{i=1}^{\infty}p_{i}I_{A}(\omega_{i})\) where \(\{p_{i}\}_{i=1}^{\infty}\) satisfy \(p_{i}\geq 0\) and \(\sum_{i=1}^{\infty}p_{i}=1\). It is easy to verify that \((\Omega,{\cal F},P)\) is a probability space. This is a probability model for random experiments with countably infinite number of outcomes. For example, the experiment of tossing a coin until a "head" is produced leads to such a probability space.

**Example 6.1.3:** (_Uncountable sample spaces_).

* (_Random variables_). Let \(\Omega=\mathbb{R}\), \(\mathcal{F}=\mathcal{B}(\mathbb{R})\), \(P=\mu_{F}\), the Lebesgue-Stieltjes measure corresponding to a cdf \(F\), i.e., corresponding to a function \(F:\mathbb{R}\to\mathbb{R}\) that is nondecreasing, right continuous, and satisfies \(F(-\infty)=0,\ F(+\infty)=1.\) See Section 1.3. This serves as a model for a single random variable \(X\).
* (_Random vectors_). Let \(\Omega=\mathbb{R}^{k}\), \(\mathcal{F}=\mathcal{B}(\mathbb{R}^{k})\), \(P=\mu_{F}\), the Lebesgue-Stieltjes measure corresponding to a (multidimensional) cdf \(F\) on \(\mathbb{R}^{k}\) where \(k\in\mathbb{N}.\) See Section 1.3. This is a model for a random vector \((X_{1},X_{2},\ldots,X_{k})\).
* (_Random sequences_). Let \(\Omega=\mathbb{R}^{\infty}\equiv\mathbb{R}\times\mathbb{R}\times\ldots\) be the set of all sequences \(\{x_{n}\}_{n\geq 1}\) of real numbers. Let \(\mathcal{C}\) be the class of all finite dimensional sets of the form \(A\times\mathbb{R}\times\mathbb{R}\times\ldots\), where \(A\in\mathcal{B}(\mathbb{R}^{k})\) for some \(1\leq k<\infty.\) Let \(\mathcal{F}\) be the \(\sigma\)-algebra generated by \(\mathcal{C}.\) For each \(1\leq k<\infty\), let \(\mu_{k}\) be a probability measure on \(\mathcal{B}(\mathbb{R}^{k})\) such that \(\mu_{k+1}(A\times\mathbb{R})=\mu_{k}(A)\) for all \(A\in\mathcal{B}(\mathbb{R}^{k}).\) Then there exists a probability measure \(\mu\) on \(\mathcal{F}\) such that \(\mu(A\times\mathbb{R}\times\mathbb{R}\times\ldots)=\mu_{k}(A)\) if \(A\in\mathcal{B}(\mathbb{R}^{k}).\) (This will be shown later as a special case of the Kolmogorov's consistency theorem in Section 6.3.) This will be a model for a sequence \(\{X_{n}\}_{n\geq 1}\) of random variables such that for each \(k\), \(1\leq k<\infty\), the distribution of \((X_{1},X_{2},\ldots,X_{k})\) is \(\mu_{k}\).

### 6.2 Random variables and random vectors

Recall the following definitions introduced earlier in Sections 2.1 and 2.2.

**Definition 6.2.1:** Let \((\Omega,\mathcal{F},P)\) be a probability space and \(X:\Omega\to\mathbb{R}\) be \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable, that is, \(X^{-1}(A)\in\mathcal{F}\) for all \(A\in\mathcal{B}(\mathbb{R}).\) Then, \(X\) is called a _random variable_ on \((\Omega,\mathcal{F},P)\).

Recall that \(X:\Omega\to\mathbb{R}\) is \(\langle\mathcal{F},\mathcal{B}(\mathbb{R})\rangle\)-measurable iff for all \(x\in\mathbb{R}\), \(\{\omega:X(\omega)\leq x\}\in\mathcal{F}\).

**Definition 6.2.2:** Let \(X\) be a random variable on \((\Omega,\mathcal{F},P).\) Let

\[F_{X}(x)\equiv P(\{\omega:X(\omega)\leq x\}),\ \ x\in\mathbb{R}. \tag{2.1}\]

Then \(F_{X}(\cdot)\) is called the _cumulative distribution function_ (cdf) of \(X\).

**Definition 6.2.3:** Let \(X\) be a random variable on \((\Omega,\mathcal{F},P).\) Let

\[P_{X}(A)\equiv P(X^{-1}(A))\ \ \ \mbox{for all}\ \ \ A\in\mathcal{B}( \mathbb{R}). \tag{2.2}\]

Then the probability measure \(P_{X}\) is called the _probability distribution_ of \(X\).

Note that \(P_{X}\) is the measure induced by \(X\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) under \(P\) and that the Lebesgue-Stieltjes measure \(\mu_{F_{X}}\) on \(\mathcal{B}(\mathbb{R})\) corresponding with the cdf \(F_{X}\) of \(X\) is the same as \(P_{X}\).

**Definition 6.2.4:** Let \((\Omega,\mathcal{F},P)\) be a probability space, \(k\in\mathbb{N}\) and \(X:\Omega\to\mathbb{R}^{k}\) be \(\langle\mathcal{F},\mathcal{B}(\mathbb{R}^{k})\rangle\)-measurable, i.e., \(X^{-1}(A)\in\mathcal{F}\) for all \(A\in\mathcal{B}(\mathbb{R}^{k})\). Then \(X\) is called a (\(k\)-dimensional) _random vector_ on \((\Omega,\mathcal{F},P)\).

Let \(X=(X_{1},X_{2},\ldots,X_{k})\) be a random vector with components \(X_{i}\), \(i=1,2,\ldots,k\). Then each \(X_{i}\) is a random variable on \((\Omega,\mathcal{F},P)\). This follows from the fact that the coordinate projection maps from \(\mathbb{R}^{k}\) to \(\mathbb{R}\), given by

\[\pi_{i}(x_{1},x_{2},\ldots,x_{k})\equiv x_{i},\ 1\leq i\leq k\]

are continuous and hence, are Borel measurable. Conversely, if for \(1\leq i\leq k\), \(X_{i}\) is a random variable on \((\Omega,\mathcal{F},P)\), then \(X=(X_{1},X_{2},\ldots,X_{k})\) is a random vector (cf. Proposition 2.1.3).

**Definition 6.2.5:** Let \(X\) be a \(k\)-dimensional random vector on \((\Omega,\mathcal{F},P)\) for some \(k\in\mathbb{N}\). Let

\[F_{X}(x)\equiv P(\{\omega:X_{1}(\omega)\leq x_{1},X_{2}(\omega)\leq x_{2}, \ldots,X_{k}(\omega)\leq x_{k}\})\]

for \(x=(x_{1},x_{2},\ldots,x_{k})\in\mathbb{R}^{k}\). Then \(F_{X}(\cdot)\) is called the _joint cumulative distribution function_ (joint cdf) of the random vector \(X\).

**Definition 6.2.6:** Let \(X\) be a \(k\)-dimensional random vector on \((\Omega,\mathcal{F},P)\) for some \(k\in\mathbb{N}\). Let

\[P_{X}(A)=P(X^{-1}(A))\quad\text{for all}\quad A\in\mathcal{B}(\mathbb{R}^{k}).\]

The probability measure \(P_{X}\) is called the _(joint) probability distribution_ of \(X\).

As in the case \(k=1\), the Lebesgue-Stieltjes measure \(\mu_{F_{X}}\) on \(\mathcal{B}(\mathbb{R}^{k})\) corresponding to the joint cdf \(F_{X}\) is the same as \(P_{X}\).

Next, let \(X=(X_{1},X_{2},\ldots,X_{k})\) be a random vector. Let \(Y=(X_{i_{1}},X_{i_{2}},\ldots,X_{i_{r}})\) for some \(1\leq i_{1}<i_{2}<\ldots<i_{r}\leq k\) and some \(1\leq r\leq k\). Then, \(Y\) is also a random vector. Further, the joint cdf of \(Y\) can be obtained from \(F_{X}\) by setting the components \(x_{j}\), \(j\not\in\{i_{1},i_{2},\ldots,i_{r}\}\) equal to \(\infty\). Similarly, the probability distribution \(P_{Y}\) can be obtained from \(P_{X}\) as an induced measure from the projection map \(\pi(x)=(x_{i_{1}},x_{i_{2}},\ldots,x_{i_{r}})\), \(x\in\mathbb{R}^{k}\). For example, if \((i_{1},i_{2},\ldots,i_{r})=(1,2,\ldots,r)\), \(r\leq k\), then

\[F_{Y}(y_{1},\ldots,y_{r})=F_{X}(y_{1},\ldots,y_{r},\infty,\ldots,\infty),\ (y_{1},\ldots,y_{r})\in\mathbb{R}^{r}\]

and

\[P_{Y}(A)=P_{X}(A\times\mathbb{R}^{(k-r)}),\ A\in\mathcal{B}(\mathbb{R}^{r}).\]Definition 6.2.7:Let \(X=(X_{1},X_{2},\ldots,X_{k})\) be a random vector on \((\Omega,{\cal F},P)\). Then, for each \(i=1,\ldots,k\), the cdf \(F_{X_{i}}\) and the probability distribution \(P_{X_{i}}\) of the random variable \(X_{i}\) are called the _marginal_ cdf and the _marginal probability distribution_ of \(X_{i}\), respectively.

It is clear that the distribution of \(X\) determines the marginal distribution \(P_{X_{i}}\) of \(X_{i}\) for all \(i=1,2,\ldots,k\). However, the marginal distributions \(\{P_{X_{i}}:i=1,2,\ldots,k\}\) do not uniquely determine the joint distribution \(P_{X}\), without additional conditions, such as independence (see Problem 6.1).

Definition 6.2.8:Let \(X\) be a random variable on \((\Omega,{\cal F},P)\). The _expected value_ of \(X\), denoted by \(EX\) or \(E(X)\), is defined as

\[EX=\int_{\Omega}XdP, \tag{2.5}\]

provided the integral is well defined. That is, at least one of the two quantities \(\int X^{+}dP\) and \(\int X^{-}dP\) is finite.

If \(X\) is a random variable on \((\Omega,{\cal F},P)\) and \(h:{\mathbb{R}}\to{\mathbb{R}}\) is Borel measurable, then \(Y=h(X)\) is also a random variable on \((\Omega,{\cal F},P)\). The expected value of \(Y\) may be computed as follows.

Proposition 6.2.1: (_The change of variable formula_). _Let \(X\) be a random variable on \((\Omega,{\cal F},P)\) and \(h:{\mathbb{R}}\to{\mathbb{R}}\) be Borel measurable. Let \(Y=h(X)\). Then_

* \(\int_{\Omega}|Y|dP=\int_{\mathbb{R}}|h(x)|P_{X}(dx)=\int_{\mathbb{R}}|y|P_{Y}( dy)\)_._
* _If_ \(\int_{\Omega}|Y|dP<\infty\)_, then_ \[\int_{\Omega}YdP=\int_{\mathbb{R}}h(x)P_{X}(dx)=\int_{\mathbb{R}}yP_{Y}(dy).\] (2.6)

Proof:If \(h=I_{A}\) for \(A\) in \({\cal B}({\mathbb{R}})\), the proposition follows from the definition of \(P_{X}\). By linearity, this extends to a nonnegative and simple function \(h\) and by the MCT, to any nonnegative measurable \(h\), and hence to any measurable \(h\). \(\Box\)

Remark 6.2.1:Proposition 6.2.1 shows that the expectation of \(Y\) can be computed in three different ways, i.e., by integrating \(Y\) with respect to \(P\) on \(\Omega\) or by integrating \(h(x)\) on \({\mathbb{R}}\) with respect to the probability distribution \(P_{X}\) of the random variable \(X\) or by integrating \(y\) on \({\mathbb{R}}\) with respect to the probability distribution \(P_{Y}\) of the random variable \(Y\).

Remark 6.2.2:If the function \(h\) is nonnegative, then the relation \(EY=\int_{\mathbb{R}}h(x)P_{X}(dx)\) is valid even if \(EY=\infty\).

**Definition 6.2.9:** For any positive integer \(n\), the \(n\)th _moment_\(\mu_{n}\) of a random variable \(X\) is defined by

\[\mu_{n}\equiv EX^{n}, \tag{2.7}\]

provided the expectation is well defined.

**Definition 6.2.10:** The variance of a random variable \(X\) is defined as \(\mbox{Var}(X)=E(X-EX)^{2}\), provided \(EX^{2}<\infty\).

**Definition 6.2.11:** The _moment generating function (mgf)_ of a random variable \(X\) is defined by

\[M_{X}(t)\equiv E(e^{tX})\quad\mbox{for all}\quad t\in\mathbb{R}. \tag{2.8}\]

Since \(e^{tX}\) is always nonnegative, \(E(e^{tX})\) is well defined but could be infinity. Proposition 6.2.1 gives a way of computing the moments and the mgf of \(X\) without explicitly computing the distribution of \(X^{k}\) or \(e^{tX}\). As an illustration, consider the case of a random variable \(X\) defined on the probability space \((\Omega,\mathcal{F},P)\) with \(\Omega=\{H,T\}^{n}\), \(n\in\mathbb{N}\), \(\mathcal{F}=\) the power set of \(\Omega\) and \(P=\) the probability distribution defined by

\[P(\{\omega\})=p^{X(\omega)}q^{n-X(\omega)}\]

where \(0<p<1\), \(q=1-p\), and \(X(\omega)=\) the number of \(H\)'s in \(\omega\). By the change of variable formula, the mgf of \(X\) is given by

\[M_{X}(t) \equiv \int e^{tx}P_{X}(dx)\] \[= \sum_{r=0}^{n}e^{tr}{n\choose r}p^{r}q^{n-r}=(pe^{t}+q)^{n},\]

since \(P_{X}\), the probability distribution of \(X\), is supported on \(\{0,1,2,\ldots,n\}\) with \(P_{X}(\{r\})={n\choose r}p^{r}q^{n-r}\). Note that \(P_{X}\) is the Binomial \((n,p)\) distribution. Here, \(M_{X}(t)\) is computed using the distribution of \(X\), i.e., using the middle term in (2.6) only.

The connection between the mgf \(M_{X}(\cdot)\) and the moments of a random variable \(X\) is given in the following propositions.

**Proposition 6.2.2:** _Let \(X\) be a nonnegative random variable and \(t\geq 0\). Then_

\[M_{X}(t)\equiv E(e^{tX})=\sum_{n=0}^{\infty}\frac{t^{n}\mu_{n}}{n!} \tag{2.9}\]

_where \(\mu_{n}\) is as in (2.7)._

**Proof:** Since \(e^{tX}=\sum_{n=0}^{\infty}t^{n}\frac{X^{n}}{n!}\) and \(X\) is nonnegative, (2.9) follows from the MCT. \(\Box\)

**Proposition 6.2.3:** _Let \(X\) be a random variable and let \(M_{X}(t)\) be finite for all \(|t|<\epsilon\), for some \(\epsilon>0.\) Then_

* \(E|X|^{n}<\infty\quad\mbox{for all}\quad n\geq 1\)_,_
* \(M_{X}(t)=\sum_{n=0}^{\infty}t^{n}\frac{\mu_{n}}{n!}\quad \mbox{for all}\quad|t|<\epsilon\)_,_
* \(M_{X}(\cdot)\) _is infinitely differentiable on_ \((-\epsilon,+\epsilon)\) _and for_ \(r\in\mathbb{N}\)_, the_ \(r\)_th derivative of_ \(M_{X}(\cdot)\) _is_ \[M_{X}^{(r)}(t)=\sum_{n=0}^{\infty}\frac{t^{n}}{n!}\mu_{n+r}=E(e^{tX}X^{r})\quad \mbox{for}\quad|t|<\epsilon.\] (2.10) _In particular,_ \[M_{X}^{(r)}(0)=\mu_{r}=EX^{r}.\] (2.11)

**Proof:** Since \(M_{X}(t)<\infty\) for all \(|t|<\epsilon\),

\[E(e^{|tX|})\leq E(e^{tX})+E(e^{-tX})<\infty\quad\mbox{for}\quad|t|<\epsilon. \tag{2.12}\]

Also, \(e^{|tX|}\geq\frac{|t|^{n}|X|^{n}}{n!}\) for all \(n\in\mathbb{N}\) and hence, (i) follows by choosing a \(t\) in \((0,\epsilon).\) Next note that \(\left|\sum_{j=0}^{n}\frac{(tx)^{j}}{j!}\right|\leq e^{|tx|}\) for all \(x\) in \(\mathbb{R}\) and all \(n\in\mathbb{N}\). Hence, by (2.12) and the DCT, (ii) follows.

Turning to (iii), since \(M_{X}(\cdot)\) admits a power series expansion convergent in \(|t|<\epsilon\), it is infinitely differentiable in \(|t|<\epsilon\) and the derivatives of \(M_{X}(\cdot)\) can be found by term-by-term differentiation of the power series (see Rudin (1976), Chapter 9). Hence,

\[M_{X}^{(r)}(t) = \frac{d^{r}}{dt^{r}}\left(\sum_{n=0}^{\infty}\frac{t^{n}\mu_{n}}{ n!}\right)\] \[= \sum_{n=0}^{\infty}\frac{\mu_{n}}{n!}\frac{d^{r}(t^{n})}{dt^{r}}\] \[= \sum_{n=r}^{\infty}\mu_{n}\frac{t^{n-r}}{(n-r)!}\] \[= \sum_{n=0}^{\infty}\frac{t^{n}}{n!}\mu_{n+r}\.\]

The verification of the second equality in (2.10) is left in an exercise (see Problem 6.4). \(\Box\)

**Remark 6.2.3:** If the mgf \(M_{X}(\cdot)\) is finite for \(|t|<\epsilon\) for some \(\epsilon>0\), then by part (ii) of the above proposition, \(M_{X}(t)\) has a power series expansionin \(t\) around \(0\) and \(\frac{\mu_{n}}{n!}\) is simply the coefficient of \(t^{n}\). For example, if \(X\) has a \(N(0,1)\) distribution, then for all \(t\in\mathbb{R}\),

\[M_{X}(t) = \int_{-\infty}^{\infty}e^{tx}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}dx= e^{t^{2}/2} \tag{2.13}\] \[= \sum_{k=0}^{\infty}\frac{(t^{2})^{k}}{k!}\frac{1}{2^{k}}\.\]

Thus, \(\mu_{n}=\left\{\begin{array}{ll}0&\mbox{if $n$ is odd}\\ \frac{(2k)!}{k!2^{k}}&\mbox{if $n=2k,k=1,2,\ldots$}\end{array}\right.\)

**Remark 6.2.4:** If \(M_{X}(t)\) is finite for \(|t|<\epsilon\) for some \(\epsilon>0\), then all the moments \(\{\mu_{n}\}_{n\geq 1}\) of \(X\) are determined and also its probability distribution. However, in general, the sequence \(\{\mu_{n}\}_{n\geq 1}\) of moments of \(X\) need not determine the distribution of \(X\) uniquely.

Table 6.2.1 gives the mean, variance, and the mgf of a number of standard probability distributions on the real line.

For future reference, some of the inequalities established in Section 3.1 are specialized for random variables and collected below without proofs.

**Proposition 6.2.4:** (_Markov's inequality_). _Let \(X\) be a random variable on \((\Omega,\mathcal{F},P)\). Then for any \(\phi:\mathbb{R}_{+}\to\mathbb{R}_{+}\) nondecreasing and any \(t>0\) with \(\phi(t)>0\),_

\[P(|X|\geq t)\leq\frac{E\bigl{(}\phi(|X|)\bigr{)}}{\phi(t)}. \tag{2.14}\]

_In particular,_

* _for_ \(r>0\)_,_ \(t>0\)_,_ \[P(X\geq t)\leq P(|X|\geq t)\leq\frac{E|X|^{r}}{t^{r}},\] (2.15)
* _for any_ \(t\geq 0\)_,_ \[P(|X|\geq t)\leq\frac{E(e^{\theta|X|})}{e^{\theta t}}\,\] _for any_ \(\theta>0\) _and hence_ \[P(|X|\geq t)\leq\inf_{\theta>0}\frac{E(e^{\theta|X|})}{e^{\theta t}}\.\] (2.16)

**Proposition 6.2.5:** (_Chebychev's inequality_). _Let \(X\) be a random variable with \(EX^{2}<\infty\), \(EX=\mu\), \(\mbox{Var}(X)=\sigma^{2}\). Then for any \(k>0\),_

\[P(|X-\mu|\geq k\sigma)\leq\frac{1}{k^{2}}. \tag{2.17}\]

**P

**Proposition 6.2.6:** (_Jensen's inequality_). _Let \(X\) be a random variable with \(P(a<X<b)=1\) for \(-\infty\leq a<b\leq\infty.\) Let \(\phi:(a,b)\to\mathbb{R}\) be convex on \((a,b).\) Then_

\[E\phi(X)\geq\phi(EX), \tag{2.18}\]

_provided \(E|X|<\infty\) and \(E|\phi(X)|<\infty.\)_

\begin{table}
\begin{tabular}{l|c c c} \hline Distribution & Mean & Variance & mgf M(t) \\ \hline Bernoulli (\(p\)), \(0<p<1\) & \(p\) & \(p(1-p)\) & \((1-p)+pe^{t},\) \(t\in\mathbb{R}\) \\ Binomial (\(n,p\)), \(p\in(0,1),\ n\in\mathbb{N}\) & \(np\) & \(np(1-p)\) & \(\big{(}(1-p)+pe^{t}\big{)}^{n},\) \(t\in\mathbb{R}\) \\ Geometric (\(p\)), \(p\in(0,1)\) & \(\frac{1}{p}\) & \(\frac{1-p}{p}\) & \(\frac{pe^{t}}{1-(1-p)e^{t}},\)\(t\in\) \\ Poisson (\(\lambda\)) \(\lambda\) \(\lambda\) \(t\in\mathbb{R}\) \\ Uniform (\(a,b\)), \(-\infty<a<b<\infty\) & \(\frac{(b-a)^{2}}{(1-p)}\) & \(\frac{pe^{t}}{1-(1-p)e^{t}},\)\(t\in\mathbb{R}\setminus\{0\};\) \\ Exponential (\(\beta\)), \(\beta\) \(\beta^{2}\) \(\beta\in(0,\infty)\) & \(\beta^{2}\) & \(\frac{1}{1-\beta t},\) \(t\in(-\infty,\frac{1}{\beta})\) \\ Gamma (\(\alpha,\beta\)), \(\alpha,\beta\in(0,\infty)\) & \(\alpha\beta\) & \(\alpha\beta^{2}\) & \((1-\beta t)^{-\alpha},\) \(t\in(-\infty,\frac{1}{\beta})\) \\ Beta (\(\alpha,\beta\)), \(\alpha,\beta\in(0,\infty)\) & \(\frac{\alpha}{\alpha+\beta}\) & \(\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}\) & \(\Big{[}1+\sum_{k=1}^{\infty}\) \(\Big{(}\prod_{r=0}^{k-1}\frac{\alpha+r}{\alpha+\beta+r}\Big{)}\frac{t^{k}}{k! }\Big{]},\) \(t\in\mathbb{R}\) \\ Cauchy (\(\gamma,\sigma\)), \(\gamma\in\mathbb{R},\ \sigma\in(0,\infty)\) & not defined & not defined & \(\infty\) for all \(t\neq 0\) \\ since & since & & \\ \(E|X|=\infty\) & \(E|X|^{2}=\infty\) & & \\ Normal (\(\gamma,\sigma^{2}\)), \(\gamma\in\mathbb{R},\ \sigma\in(0,\infty)\) & \(\gamma\) & \(\sigma^{2}\) & \(\exp(\gamma t+\frac{t^{2}\sigma^{2}}{2}),\) \(t\in\mathbb{R}\) \\ Lognormal (\(\gamma,\sigma^{2}\)), \(\gamma\in\mathbb{R},\ \sigma\in(0,\infty)\) & \(e^{\gamma+\frac{\sigma^{2}}{2}}\) & \([e^{2(\gamma+\sigma^{2})}\) & \(\infty\) for all \(t>0\) \\ \hline \end{tabular}
\end{table}
Table 6.2.1: Mean, variance, mgf of the distributions listed in Tables 4.6.1 and 4.6.2.

[MISSING_PAGE_FAIL:212]

Then, \[M_{X}(t_{1},\ldots,t_{k})=\sum_{r\in\mathbb{Z}_{+}^{k}}\frac{t^{r}}{r!}\,\mu_{r}\] (2.25) for all \(t=(t_{1},t_{2},\ldots,t_{k})\in(-\epsilon,+\epsilon)^{k}\).
3. For any \(r=(r_{1},\ldots,r_{k})\in\mathbb{Z}_{+}^{k}\), \[\frac{d^{r}}{dt^{r}}M_{X}(t)\Big{|}_{t=0}=\mu_{r},\] (2.26) where \(\frac{d^{r}}{dt^{r}}=\frac{\partial^{r_{1}}}{\partial t_{1}^{r_{1}}}\frac{ \partial^{r_{2}}}{\partial t_{2}^{r_{2}}}\ldots\frac{\partial^{r_{k}}}{ \partial t_{k}^{r_{k}}}\).

### Kolmogorov's consistency theorem

In the previous section, the case of a single random variable and that of a finite dimensional random vector were discussed. The goal of this section is to discuss infinite families of random variables such as a random sequence \(\{X_{n}\}_{n\geq 1}\) or a random function \(\{X(t):0\leq t<T\},0\leq T\leq\infty\). For example, \(X_{n}\) could be the population size of the \(n\)th generation of a randomly evolving biological population, and \(X(t)\) could be the temperature at time \(t\) in a chemical reaction over a period \([0,T]\). An example from modeling of spatial random phenomenon is a collection \(\{X(s):s\in S\}\) of random variables \(X(s)\) where \(S\) is a specified region such as the U.S., and \(X(s)\) is the amount of rainfall at location \(s\in S\) during a specified month.

Let \((\Omega,\mathcal{F},P)\) be a probability space and \(\{X_{\alpha}:\alpha\in A\}\) be a collection of random variables defined on \((\Omega,\mathcal{F},P)\), where \(A\) is a nonempty set. Then for any \((\alpha_{1},\alpha_{2},\ldots,\alpha_{k})\in A^{k},1\leq k<\infty\), the random vector \((X_{\alpha_{1}},X_{\alpha_{2}},\ldots,X_{\alpha_{k}})\) has a joint probability distribution \(\mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}\) over \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\).

**Definition 6.3.1:** A (real valued) _stochastic process_ with index set \(A\) is a family \(\{X_{\alpha}:\alpha\in A\}\) of random variables defined on a probability space \((\Omega,\mathcal{F},P)\).

**Example 6.3.1:** (_Examples of stochastic processes_). Let \(\Omega=[0,1]\), \(\mathcal{F}=\mathcal{B}([0,1])\), \(P=\) the Lebesgue measure on \([0,1]\). Let \(A_{1}=\{1,2,3,\ldots\}\), \(A_{2}=[0,T]\), \(0<T<\infty\). For \(\omega\in\Omega\), \(n\in A_{1}\), \(t\in A_{2}\), let

\[X_{n}(\omega) = \sin 2\pi n\omega\] \[Y_{t}(\omega) = \sin 2\pi t\omega\] \[Z_{n}(\omega) = n\mbox{th digit in the decimal expansion of }\omega\] \[V_{n,t}(\omega) = X_{n}^{2}(\omega)+Y_{t}^{2}(\omega).\]

Then \(\{X_{n}:n\in A_{1}\}\), \(\{Z_{n}:n\in A_{1}\}\), \(\{V_{n,t}:(n,t)\in A_{1}\times A_{2}\}\), \(\{Y_{t}:t\in A_{2}\}\) are all stochastic processes.

Note that a real valued stochastic process \(\{X_{\alpha}:\alpha\in A\}\) may also be viewed as a random real valued function on the set \(A\) by the identification \(\omega\to f(\omega,\cdot)\), where \(f(\omega,\alpha)=X_{\alpha}(\omega)\) for \(\alpha\) in \(A\).

**Definition 6.3.2:** The family \(\{\mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(\cdot)\equiv P((X_{\alpha_{1} },\ldots,X_{\alpha_{k}})\in\cdot)\): \((\alpha_{1},\alpha_{2},\ldots,\alpha_{k})\in A^{k},1\leq k<\infty\}\) of probability distributions is called the _family of finite dimensional distributions_ (_fdds_) associated with the stochastic process \(\{X_{\alpha}:\alpha\in A\}\).

This family of finite dimensional distributions satisfies the following _consistency_ conditions: For any \((\alpha_{1},\alpha_{2},\ldots,\alpha_{k})\in A^{k}\), \(2\leq k<\infty\), and any \(B_{1},B_{2},\ldots,B_{k}\) in \({\cal B}(\mathbb{R})\),

1. \(\mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B_{1}\times\cdots\times B_{k-1 }\times\mathbb{R})=\mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k-1})}(B_{1} \times\cdots\times B_{k-1})\);
2. For any permutation \((i_{1},i_{2},\ldots,i_{k})\) of \((1,2,\ldots,k)\), \[\mu_{(\alpha_{i_{1}},\alpha_{i_{2}},\ldots,\alpha_{i_{k}})}(B_{i_{1}}\times B _{i_{2}}\times\cdots\times B_{i_{k}})=\mu_{(\alpha_{1},\ldots,\alpha_{k})}(B_{1 }\times B_{2}\times\cdots\times B_{k})\;.\]

To verify C1, note that

\[\mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B_{1}\times B_{2} \times\cdots\times B_{k-1}\times\mathbb{R})\] \[= P(X_{\alpha_{1}}\in B_{1},X_{\alpha_{2}}\in B_{2},\ldots,X_{ \alpha_{k-1}}\in B_{k-1},X_{\alpha_{k}}\in\mathbb{R})\] \[= P(X_{\alpha_{1}}\in B_{1},X_{\alpha_{2}}\in B_{2},\ldots,X_{ \alpha_{k-1}}\in B_{k-1})\] \[= \mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k-1})}(B_{1}\times B_{ 2}\times\cdots\times B_{k-1}).\]

Similarly, to verify C2, note that

\[\mu_{(\alpha_{i_{1}},\alpha_{i_{2}},\ldots,\alpha_{i_{k}})}(B_{i _{1}}\times B_{i_{2}}\cdots\times B_{i_{k}})\] \[= P(X_{\alpha_{i_{1}}}\in B_{i_{1}},X_{\alpha_{i_{2}}}\in B_{i_{2} },\ldots,X_{\alpha_{i_{k}}}\in B_{i_{k}})\] \[= P(X_{\alpha_{1}}\in B_{1},X_{\alpha_{2}}\in B_{2},\ldots,X_{ \alpha_{k}}\in B_{k})\] \[= \mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B_{1}\times B_{2} \times\cdots\times B_{k}).\]

A natural question is that given a family of probability distributions \(Q_{A}\equiv\{\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}:(\alpha_{1}, \alpha_{2},\ldots,\alpha_{k})\in A^{k},\,1\leq k<\infty\}\) on finite dimensional Euclidean spaces, does there exist a real valued stochastic process \(\{X_{\alpha}:\alpha\in A\}\) such that its family of finite dimensional distributions coincides with \(Q_{A}\)?

Kolmogorov (1956) showed that if \(Q_{A}\) satisfies C1 and C2, then such a stochastic process does exist. This is known as Kolmogorov's consistency theorem (also known as Kolmogorov's existence theorem).

**Theorem 6.3.1:** (_Kolmogorov's consistency theorem_). _Let \(A\) be a nonempty set. Let \(Q_{A}\equiv\{\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}:(\alpha_{1}, \alpha_{2},\ldots,\alpha_{k})\in A^{k}\), \(1\leq k<\infty\}\) be a family of probability distributions such that for each \((\alpha_{1},\alpha_{2},\ldots,\alpha_{k})\in A^{k}\), \(1\leq k<\infty\),_1. \(\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}\) _is a probability distribution on_ \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\)_,_
2. _C1 and C2 hold, i.e., for all_ \(B_{1},B_{2},\ldots,B_{k}\in\mathcal{B}(\mathbb{R})\)_,_ \(2\leq k<\infty\)_,_ \[\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B_{1}\times B_{2} \times\cdots\times B_{k-1}\times\mathbb{R})\] \[\qquad=\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k-1})}(B_{1} \times B_{2}\times\cdots\times B_{k-1})\] (3.1) _and for any permutation_ \((i_{1},i_{2},\ldots,i_{k})\) _of_ \((1,2,\ldots,k)\)_,_ \[\mu_{(\alpha_{i_{1}},\alpha_{i_{2}},\ldots,\alpha_{i_{k}})}(B_{i_ {1}}\times B_{i_{2}}\times\cdots\times B_{i_{k}})\] \[\qquad=\mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B_{1} \times B_{2}\times\cdots\times B_{k}).\] (3.2)

_Then, there exists a probability space \((\Omega,\mathcal{F},P)\) and a stochastic process \(X_{A}\equiv\{X_{\alpha}:\alpha\in A\}\) on \((\Omega,\mathcal{F},P)\) such that \(Q_{A}\) is the family of finite dimensional distributions associated with \(X_{A}\)._

**Remark 6.3.1:** Thus the above theorem says that given the family \(Q_{A}\) satisfying conditions (i) and (ii), there exists a real valued function on \(A\times\Omega\) such that for each \(\omega\), \(f(\cdot,\omega)\) is a function on \(A\) and for each \((\alpha_{1},\alpha_{2},\ldots,\alpha_{k})\in A^{k}\), the vector \(\big{(}f(\alpha_{1},\omega),f(\alpha_{2},\omega),\ldots,f(\alpha_{k},\omega) \big{)}\) is a random vector with probability distribution \(\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}\). This random function point of view is useful in dealing with functionals of the form \(M(\omega)\equiv\{\sup f(\alpha,\omega):\alpha\in A\}\). For example, if \(A_{1}=\{1,2,\ldots\}\), then one might consider functionals such as \(\lim_{n\to\infty}f(n,\omega)\), \(\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^{n}f(j,\omega)\), \(\sum_{j=1}^{\infty}f(j,\omega)\), etc. Since the random functionals are not fully determined by \(f(\alpha,\omega)\) for finitely many \(\alpha\)'s, it is not possible to compute probabilities of events defined in terms of these functionals from the knowledge of the finite dimensional distribution of \((f(\alpha_{1},\omega),\ldots,f(\alpha_{k},\omega))\) for a given \((\alpha_{1},\ldots,\alpha_{k})\), no matter how large \(k\) is. Kolmogorov's consistency theorem allows one to compute these probabilities given _all_ finite dimensional distributions (provided that the functionals satisfy appropriate measurability conditions).

Given a probability measure \(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\), now consider the problem of constructing a probability space \((\Omega,\mathcal{F},P)\) and a random variable \(X\) on it with distribution \(\mu\). A natural solution is to set the sample space \(\Omega\) to be \(\mathbb{R}\), the \(\sigma\)-algebra \(\mathcal{F}\) to be \(\mathcal{B}(\mathbb{R})\), and the probability measure \(P\) to be \(\mu\) and the random variable \(X\) to be the identity map \(X(\omega)\equiv\omega\). Similarly, given a probability measure \(\mu\) on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R})^{k})\), one can set the sample space \(\Omega\) to be \(\mathbb{R}^{k}\) and the \(\sigma\)-algebra \(\mathcal{F}\) to be \(\mathcal{B}(\mathbb{R}^{k})\) and the probability measure \(P\) to be \(\mu\) and the random vector \(X\) to be the identity map.

Arguing in the same fashion, given a family \(Q_{A}\) of finite dimensional distributions with index set \(A\), to construct a stochastic process \(\{X_{\alpha}:\alpha\in A\}\) with index set \(A\) on some probability space \((\Omega,\mathcal{F},P)\), it is natural to set the sample space \(\Omega\) to be \(\mathbb{R}^{A}\), the collection of all real valued functions on \(A\), \(\mathcal{F}\) to be a suitable \(\sigma\)-algebra that includes all finite dimensional events,\(P\) to be an appropriate probability measure that yields \(Q_{A}\), and \(X\) to be the identity map.

These considerations lead to the following definitions.

**Definition 6.3.3:** Let \(A\) be a nonempty set. Then \(\mathbb{R}^{A}\equiv\{f\mid f:A\to\mathbb{R}\}\), the collection of all real valued functions on \(A\).

If \(A\) is a finite set \(\{a_{1},a_{2},\ldots,a_{k}\}\), then \(\mathbb{R}^{A}\) can be identified with \(\mathbb{R}^{k}\) by associating each \(f\in\mathbb{R}^{A}\) with the vector \((f(a_{1}),f(a_{2}),\ldots,f(a_{k}))\) in \(\mathbb{R}^{k}\). If \(A\) is a countably infinite set \(\{a_{1},a_{2},a_{3},\ldots\}\), then \(\mathbb{R}^{A}\) can be similarly identified with \(\mathbb{R}^{\infty}\), the set of all sequences \(\{x_{1},x_{2},x_{3},\ldots\}\) of real numbers. If \(A\) is the interval \([0,1]\), then \(\mathbb{R}^{A}\) is the collection of all real valued functions on \([0,1]\).

**Definition 6.3.4:** Let \(A\) be a nonempty set. \(A\) subset \(C\subset\mathbb{R}^{A}\) is called a _finite dimensional cylinder set_ (_fdcs_) if there exists a finite subset \(A_{1}\subset A\), say, \(A_{1}\equiv\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\},1\leq k<\infty\) and a Borel set \(B\) in \(\mathcal{B}(\mathbb{R}^{k})\) such that \(C=\{f:f\in\mathbb{R}^{A}\) and \((f(\alpha_{1}),f(\alpha_{2}),\ldots,f(\alpha_{k}))\in B\}\). The set \(B\) is called a _base_ for \(C\).

The collection of all finite dimensional cylinder sets will be denoted by \(\mathcal{C}\).

The name _cylinder_ is motivated by the following example:

**Example 6.3.2:** Let \(A=\{1,2,3\}\) and \(C=\{(x_{1},x_{2},x_{3}):x_{1}^{2}+x_{2}^{2}\leq 1\}\). Then \(C\) is a cylinder (in the usual sense of the English word), but with infinite height and depth. According to Definition 6.3.4, \(C\) is also a cylinder in \(\mathbb{R}^{3}\) with the unit circle in \(\mathbb{R}^{2}\) as its base.

Examples 6.3.3 and 6.3.4 below are examples of fdcs, whereas Example 6.3.5 is an example of a set that is not a fdcs.

**Example 6.3.3:** Let \(A=\{1,2\}\) and \(C=\{(x_{1},x_{2}):|\sin 2\pi x_{1}|\leq\frac{1}{\sqrt{2}}\}\).

**Example 6.3.4:** Let \(A=\{1,2,3,\ldots\}\) and \(C=\{(x_{1},x_{2},x_{3},\ldots):\frac{x_{17}^{2}}{4}+\frac{x_{30}^{2}}{10}- \frac{x_{42}^{2}}{5}\leq 10\}\).

**Example 6.3.5:** Let \(A=\{1,2,3,\ldots\}\) and \(D=\{(x_{1},x_{2},x_{3},\ldots):x_{j}\in\mathbb{R}\) for all \(j\geq 1\) and \(\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^{n}x_{j}\) exists\(\}\) is _not_ a finite dimensional cylinder set (Problem 6.8).

**Proposition 6.3.2:**_Let \(A\) be a nonempty set and \(\mathcal{C}\) be the collection of all finite dimensional cylinder sets in \(\mathbb{R}^{A}\). Then \(\mathcal{C}\) is an algebra._

**Proof:** Let \(C_{1},C_{2}\in\mathcal{C}\) and let

\[C_{1} = \{f:f\in\mathbb{R}^{A}\mbox{ and }\big{(}f(\alpha_{1}),f( \alpha_{2}),\ldots,f(\alpha_{k})\big{)}\in B_{1}\}\] \[C_{2} = \{f:f\in\mathbb{R}^{A}\mbox{ and }\big{(}f(\beta_{1}),f( \beta_{2}),\ldots,f(\beta_{j})\big{)}\in B_{2}\}\]for some \(A_{1}=\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A,A_{2}=\{\beta_{1},\beta_ {2},\ldots,\beta_{j}\}\subset A,B_{1}\in{\cal B}(\mathbb{R}^{k}),B_{2}\in{\cal B }(\mathbb{R}^{j}),1\leq k<\infty,1\leq j<\infty\). Let \(A_{3}=A_{1}\cup A_{2}=\{\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell}\}\), where without loss of generality \((\gamma_{1},\gamma_{2},\ldots,\gamma_{k})=(\alpha_{1},\alpha_{2},\ldots,\alpha _{k})\) and \((\gamma_{\ell-j+1},\ldots,\gamma_{\ell-1},\gamma_{\ell})=(\beta_{1},\beta_{2}, \ldots,\beta_{j})\). Then \(C_{1}\) and \(C_{2}\) may be expressed as

\[C_{1} = \{f:f\in\mathbb{R}^{A}\mbox{ and }\big{(}f(\gamma_{1}),f(\gamma_{2}),\ldots,f(\gamma_{\ell})\big{)}\in\tilde{B}_{1}\}\] \[C_{2} = \{f:f\in\mathbb{R}^{A}\mbox{ and }\big{(}f(\gamma_{1}),\ldots,f( \gamma_{\ell})\big{)}\in\tilde{B}_{2}\}\]

where \(\tilde{B}_{1}=B_{1}\times\mathbb{R}^{\ell-k}\) and \(\tilde{B}_{2}=\mathbb{R}^{\ell-j}\times B_{2}\). Thus, \(C_{1}\cup C_{2}=\{f:f\in\mathbb{R}^{A}\) and \((f(\gamma_{1}),\ldots,f(\gamma_{\ell}))\in\tilde{B}_{1}\cup\tilde{B}_{2}\}\). Since both \(\tilde{B}_{1}\) and \(\tilde{B}_{2}\) lie in \({\cal B}(\mathbb{R}^{\ell})\), \(C_{1}\cup C_{2}\in{\cal C}\).

Next note that, \(C_{1}^{c}=\{f:f\in\mathbb{R}^{A}\mbox{ and }\big{(}f(\alpha_{1}),\ldots,f( \alpha_{k})\big{)}\in B_{1}^{c}\}\). Since \(B_{1}^{c}\in{\cal B}(\mathbb{R}^{k})\), it follows that \(C_{1}^{c}\in{\cal C}\). Thus, \({\cal C}\) is an algebra. \(\Box\)

**Remark 6.3.2:** If \(A\) is a _finite_ nonempty set, the collection \({\cal C}\) is also a \(\sigma\)-algebra.

**Definition 6.3.5:** Let \(A\) be a nonempty set. Let \({\cal R}^{A}\) be the \(\sigma\)-algebra generated by the collection \({\cal C}\). Then \({\cal R}^{A}\) is called the _product \(\sigma\)-algebra_ on \(\mathbb{R}^{A}\).

**Remark 6.3.3:** If \(A=\{1,2,3,\ldots\}\equiv\mathbb{N}\) and \(\mathbb{R}^{\mathbb{N}}\) is identified with the set \(\mathbb{R}^{\infty}\) of all sequences of real numbers, then the product \(\sigma\)-algebra \(\mathbb{R}^{\mathbb{N}}\) coincides with the Borel \(\sigma\)-algebra \({\cal B}(\mathbb{R}^{\infty})\) on \(\mathbb{R}^{\infty}\) under the metric

\[d(x,y)=\sum_{j=1}^{\infty}\frac{1}{2^{j}}\left(\frac{|x_{j}-y_{j}|}{1+|x_{j}-y _{j}|}\right) \tag{3.3}\]

for \(x=(x_{1},x_{2},\ldots)\), \(y=(y_{1},y_{2},\ldots)\) in \(\mathbb{R}^{\infty}\) (Problem 6.9).

**Definition 6.3.6:** Let \(A\) be a nonempty set. For any \((\alpha_{1},\alpha_{2},\ldots,\alpha_{k})\in A^{k}\), \(1\leq k<\infty\), the projection map \(\pi_{(\alpha_{1},\ldots,\alpha_{k})}\) from \(\mathbb{R}^{A}\) to \(\mathbb{R}^{k}\) is defined by

\[\pi_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(f)=(f(\alpha_{1}),f(\alpha_{2} ),\ldots,f(\alpha_{k})). \tag{3.4}\]

In particular, for \(\alpha\in A\),

\[\pi_{\alpha}(f)=f(\alpha) \tag{3.5}\]

is called a _co-ordinate_ map.

The projection map \(\pi_{A_{1}}\) for any arbitrary subset \(A_{1}\subset A\) may be similarly defined. The next proposition follows from the definition of \({\cal R}^{A}\).

**Proposition 6.3.3:**

1. _For each_ \(\alpha\in A\)_, the map_ \(\pi_{\alpha}\) _from_ \(\mathbb{R}^{A}\) _to_ \(\mathbb{R}\) _is_ \(\langle{\cal R}^{A},{\cal B}(\mathbb{R})\rangle\)_-measurable._
2. _For any_ \((\alpha_{1},\alpha_{2},\ldots,\alpha_{k})\in A^{k}\)_,_ \(1\leq k<\infty\)_, the map_ \(\pi_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}\) _from_ \(\mathbb{R}^{A}\) _to_ \(\mathbb{R}^{k}\) _is_ \(\langle{\cal R}^{A},{\cal B}(\mathbb{R}^{k})\rangle\)_-measurable._

**Proof of Theorem 6.3.1:** Let \(\Omega=\mathbb{R}^{A}\) and \(\mathcal{F}\equiv\mathcal{R}^{A}\). Define a set function \(P\) on \(\mathcal{C}\) by

\[P(C)=\mu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B) \tag{3.6}\]

for a \(C\) in \(\mathcal{C}\) with representation

\[C=\{\omega:\omega\in\mathbb{R}^{A},\ \big{(}\omega(\alpha_{1}),\omega(\alpha_{2} ),\ldots,\omega(\alpha_{k})\big{)}\in B\}. \tag{3.7}\]

The main steps in the proof are

1. To show that \(P(C)\) as defined in (3.6) is independent of the representation (3.7) of \(C\), and
2. \(P(\cdot)\) is countably additive on \(\mathcal{C}\).

Next, by the Caratheodory extension theorem (Theorem 1.3.3), there exists a unique extension of \(P\) (also denoted by \(P\)) to \(\mathcal{F}\) such that \((\Omega,\mathcal{F},P)\) is a probability space. Defining \(X_{\alpha}(\omega)\equiv\pi_{\alpha}(\omega)=\omega(\alpha)\) for \(\alpha\) in \(A\) yields a stochastic process \(\{X_{\alpha}:\alpha\in A\}\) on the probability space

\[(\mathbb{R}^{A},\mathcal{R}^{A},P)\equiv(\Omega,\mathcal{F},P)\]

with the family \(Q_{A}\) as its set of finite dimensional distributions. Hence, it remains to establish (i) and (ii). Let \(C\in\mathcal{C}\) admit two representations:

\[C \equiv \{\omega:\big{(}\omega(\alpha_{1}),\omega(\alpha_{2}),\ldots, \omega(\alpha_{k})\big{)}\in B_{1}\}\] \[\equiv \pi_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B_{1})\]

and

\[C \equiv \{\omega:\big{(}\omega(\beta_{1}),\omega(\beta_{2}),\ldots, \omega(\beta_{j})\big{)}\in B_{2}\}\] \[\equiv \pi_{(\beta_{1},\beta_{2},\ldots,\beta_{j})}^{-1}(B_{2})\]

for some \(A_{1}=\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A\), \(1\leq k<\infty\), and some \(A_{2}=\{\beta_{1},\beta_{2},\ldots,\beta_{j}\}\subset A\), \(1\leq j<\infty\), \(B_{1}\in\mathcal{B}(\mathbb{R}^{k})\) and \(B_{2}\in\mathcal{B}(\mathbb{R}^{j})\). Let \(A_{3}=A_{1}\cup A_{2}=\{\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell}\}\) and w.l.o.g., let \((\gamma_{1},\gamma_{2},\ldots,\gamma_{k})=(\alpha_{1},\alpha_{2},\ldots, \alpha_{k})\) and \((\gamma_{\ell-j+1},\gamma_{\ell-j+2},\gamma_{\ell-1},\gamma_{\ell})=(\beta_{1},\beta_{2},\ldots,\beta_{j})\). Then \(C\) may be represented as

\[C = \pi_{\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell}}^{-1}(\tilde{B}_{ 1})\] \[= \pi_{\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell}}^{-1}(\tilde{B}_{ 2})\]

where \(\tilde{B}_{1}=B_{1}\times\mathbb{R}^{\ell-k}\) and \(\tilde{B}_{2}=\mathbb{R}^{\ell-j}\times B_{2}\). Note that \((\omega(\gamma_{1}),\ldots,\omega(\gamma_{\ell}))\in\tilde{B}_{1}\) iff \(\omega\in C\) iff \((\omega(\gamma_{1}),\ldots,\omega(\gamma_{\ell}))\in\tilde{B}_{2}\) and thus

\[\tilde{B}_{1}=\tilde{B}_{2}. \tag{3.8}\]Next by the first consistency condition (3.1) and induction,

\[\nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\tilde{B}_{1})=\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(B_{1}). \tag{3.9}\]

Also by (3.2), for \(B_{2}\) of the form \(B_{21}\times B_{22}\times\cdots\times B_{2j}\) with \(B_{2i}\in\mathcal{B}(\mathbb{R})\) for all \(1\leq i\leq j\),

\[\nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\mathbb{R}^{\ell-j}\times B_ {2})=\nu_{(\gamma_{\ell-j+1},\ldots,\gamma_{\ell},\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell-j})}(B_{2}\times\mathbb{R}^{\ell-j}).\]

Now note that

* \(\nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\mathbb{R}^{\ell-j}\times B)\) and \(\nu_{(\gamma_{\ell-j+1},\ldots,\gamma_{\ell},\gamma_{1},\gamma_{2},\ldots, \gamma_{\ell-j})}(B\times\mathbb{R}^{\ell-j})\), considered as set functions defined for \(B\in\mathcal{B}(\mathbb{R}^{j})\), are probability measures on \(\mathcal{B}(\mathbb{R}^{j})\),
* they coincide on the class \(\Gamma\) of sets of the form \(B=B_{21}\times B_{22}\times\cdots\times B_{2j}\) with \(B_{2i}\in\mathcal{B}(\mathbb{R})\) for all \(i\), and
* the class \(\Gamma\) is a \(\pi\)-class and it generates \(\mathcal{B}(\mathbb{R}^{j})\).

Hence, by the uniqueness theorem (Theorem 1.3.6),

\[\nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\mathbb{R}^{\ell-j}\times B )=\nu_{(\gamma_{\ell-j+1},\ldots,\gamma_{\ell},\gamma_{1},\gamma_{2},\ldots, \gamma_{\ell-j})}(B\times\mathbb{R}^{\ell-j}) \tag{3.10}\]

for all \(B\in\mathcal{B}(\mathbb{R}^{j})\).

Again by (3.1) and induction

\[\nu_{(\gamma_{\ell-j+1},\ldots,\gamma_{\ell},\gamma_{1},\gamma_{2 },\ldots,\gamma_{\ell-j})}(B_{2}\times\mathbb{R}^{\ell-j})\] \[=\nu_{(\gamma_{\ell-j+1},\ldots,\gamma_{\ell})}(B_{2})=\nu_{( \beta_{1},\beta_{2},\ldots,\beta_{j})}(B_{2}). \tag{3.11}\]

Since \(\tilde{B}_{2}=\mathbb{R}^{\ell-j}\times B_{2}\), by (3.10) and (3.11)

\[\nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\tilde{B}_{2})=\nu_{(\beta_ {1},\beta_{2},\ldots,\beta_{j})}(B_{2}).\]

Now from (3.8) and (3.9) it follows that

\[\nu_{(\alpha_{1},\ldots,\alpha_{k})}(B_{1}) = \nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\tilde{B}_{1})\] \[= \nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\tilde{B}_{2})\] \[= \nu_{(\beta_{1},\beta_{2},\ldots,\beta_{j})}(B_{2}),\]

thus establishing (i).

To establish (ii), it needs to be shown that

* \(P(C_{1}\cup C_{2})=P(C_{1})+P(C_{2})\) if \(C_{1},C_{2}\in\mathcal{C}\) and \(C_{1}\cap C_{2}=\emptyset\).
* \(C_{n}\in\mathcal{C},C_{n}\supset C_{n+1}\) for all \(n\), \(\bigcap_{n\geq 1}C_{n}=\emptyset\Rightarrow P(C_{n})\downarrow 0\).

Let \(C_{1}=\pi^{-1}_{(\alpha_{1},\ldots,\alpha_{k})}(B_{1})\) and \(C_{2}=\pi^{-1}_{(\beta_{1},\ldots,\beta_{j})}(B_{2})\) for \(B_{1}\in\mathcal{B}(\mathbb{R}^{k})\), \(B_{2}\in\mathcal{B}(\mathbb{R}^{j})\), \(\{\alpha_{1},\ldots,\alpha_{k}\}\subset A\) and \(\{\beta_{1},\ldots,\beta_{j}\}\subset A\), \(1\leq j\), \(k<\infty\).

As in the proof of Proposition 6.3.2, \(C_{1}\) and \(C_{2}\) may be represented as

\[C_{i}=\pi^{-1}_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\tilde{B}_{i}), \ i=1,2,\]

where \(\tilde{B}_{i}\in\mathcal{B}(\mathbb{R}^{\ell})\). Since \(C_{1}\) and \(C_{2}\) are disjoint by hypothesis, it follows that \(\tilde{B}_{1}\) and \(\tilde{B}_{2}\) are disjoint. Also, since

\[P(C_{i})=\nu_{(\gamma_{1},\gamma_{2},\ldots,\gamma_{\ell})}(\tilde{B}_{i}),\ i=1,2,\]

and \(\nu_{(\gamma_{1},\ldots,\gamma_{\ell})}(\cdot)\) is a measure on \(\mathcal{B}(\mathbb{R}^{\ell})\), it follows that

\[P(C_{1}\cup C_{2}) = \nu_{(\gamma_{1},\ldots,\gamma_{\ell})}(\tilde{B}_{1}\cup\tilde{B }_{2})\] \[= \nu_{(\gamma_{1},\ldots,\gamma_{\ell})}(\tilde{B}_{1})+\nu_{( \gamma_{1},\ldots,\gamma_{\ell})}(\tilde{B}_{2})\] \[= P(C_{1})+P(C_{2}),\]

thus proving (ii)\({}^{a}\).

To prove (ii)\({}^{b}\), note that for any sequence \(\{C_{n}\}_{n\geq 1}\subset\mathcal{C}\), there exists a countable set \(A_{1}=\{\alpha_{1},\alpha_{2},\ldots,\alpha_{n},\ldots\}\), an increasing sequence \(\{k_{n}\}_{n\geq 1}\) of positive integers and a sequence of Borel sets \(\{B_{n}\}_{n\geq 1}\) such that \(B_{n}\in\mathcal{B}(\mathbb{R}^{k_{n}})\) and \(C_{n}=\pi^{-1}_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k_{n}})}(B_{n})\) for all \(n\in\mathbb{N}\). Now suppose that \(\{C_{n}\}_{n\geq 1}\) is decreasing. It will be shown that if \(\lim_{n\to\infty}P(C_{n})=\delta>0\), then \(\bigcap_{n\geq 1}C_{n}\neq\emptyset\). For each \(n\), by the regularity of measures (Corollary 1.3.5), there exists a compact set \(G_{n}\subset B_{n}\) such that

\[\nu_{(\alpha_{1},\ldots,\alpha_{k_{n}})}(B_{n}\setminus G_{n})<\frac{\delta}{2 ^{n+1}}\.\]

Let \(D_{n}=\pi^{-1}_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k_{n}})}(G_{n})\). Then \(P(C_{n}\setminus D_{n})<\frac{\delta}{2^{n+1}}\). Let \(H_{n}=\bigcap_{j=1}^{n}D_{j}\). Then \(\{H_{n}\}_{n\geq 1}\) is decreasing and

\[P(C_{n}\setminus H_{n}) = P\big{(}C_{n}\cap H_{n}^{c}\big{)}=P\bigg{(}\bigcup_{j=1}^{n}(C _{n}\cap D_{j}^{c})\bigg{)}\] \[\leq \sum_{j=1}^{n}P(C_{n}\setminus D_{j})\leq\sum_{j=1}^{n}P(C_{j} \setminus D_{j})\] \[\qquad\qquad\mbox{(since $\{C_{n}\}_{n\geq 1}$ is decreasing)}\] \[\leq \sum_{j=1}^{n}\frac{\delta}{2^{j+1}}<\frac{\delta}{2}\.\]

Since \(P(C_{n})\downarrow\delta>0\), \(H_{n}\subset C_{n}\), and \(P(C_{n}\setminus H_{n})<\frac{\delta}{2}\), it follows that \(P(H_{n})>\frac{\delta}{2}\) for all \(n\geq 1\). This implies \(H_{n}\neq\emptyset\) for each \(n\). It will now be shown that \(\bigcap_{n\geq 1}H_{n}\neq\emptyset\). Let \(\{\omega_{n}\}_{n\geq 1}\) be a sequence of elementsfrom \(\Omega=\mathbb{R}^{A}\) such that for each \(n\), \(\omega_{n}\in H_{n}\). Then, since \(\{H_{n}\}_{n\geq 1}\) is a decreasing sequence, for each \(1\leq j<\infty\), \(\omega_{n}\in H_{j}\) for \(n\geq j\). This implies that the vector \((\omega_{n}(\alpha_{1}),\omega_{n}(\alpha_{2}),\ldots,\omega_{n}(\alpha_{k_{j} }))\in G_{j}\) for all \(n\geq j\). Since \(G_{1}\) is compact, there exists a subsequence \(\{n_{1i}\}_{i\geq 1}\) such that \(\lim_{i\to\infty}\omega_{n_{1i}}(\alpha_{1})=\omega(\alpha_{1})\) exists. Next, since \(G_{2}\) is compact, there exists a further sequence \(\{n_{2i}\}_{i\geq 1}\) of \(\{n_{1i}\}_{i\geq 1}\) such that \(\lim_{i\to\infty}\omega_{n_{2i}}(\alpha_{2})=\omega(\alpha_{2})\) exists. Proceeding this way and applying the usual 'diagonal method,' a subsequence \(\{n_{i}\}_{i\geq 1}\) is obtained such that \(\lim_{i\to\infty}\omega_{n_{i}}(\alpha_{j})=\omega(\alpha_{j})\) for all \(1\leq j<\infty\). Let \(\omega(\alpha)=0\) for \(\alpha\not\in\{\alpha_{1},\alpha_{2},\ldots\}\). Since for each \(j\), \(G_{j}\) is compact, \((\omega(\alpha_{1}),\omega(\alpha_{2}),\ldots,\omega(\alpha_{k_{j}}))\in G_{j}\) and hence \(\omega\in H_{j}\). Thus, \(\omega\in\bigcap_{j\geq 1}H_{j}\subset\bigcap_{j\geq 1}C_{j}\) implying \(\bigcap_{j\geq 1}C_{j}\neq\emptyset\). The proof of the theorem is now complete. \(\Box\)

When the index set \(A\) is countable and identified with the set \(\mathbb{N}\equiv\{1,2,3,\ldots\}\), it is possible to give a simpler formulation of the consistency conditions.

Theorem 6.3.4: Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of probability measures such that

* for each \(n\in\mathbb{N}\), \(\mu_{n}\) is a probability measure on \((\mathbb{R}^{n},\mathcal{B}(\mathbb{R}^{n}))\),
* for each \(n\in\mathbb{N}\), \(\mu_{n+1}(B\times\mathbb{R})=\mu_{n}(B)\) for all \(B\in\mathcal{B}(\mathbb{R}^{n})\).

Then there exists a stochastic process \(\{X_{n}:n\geq 1\}\) on a probability space \((\Omega,\mathcal{F},P)\) with \(\Omega=\mathbb{R}^{\infty},\mathcal{F}=\mathcal{B}(\mathbb{R}^{\infty})\) such that for each \(n\geq 1\), the probability distribution \(P_{(X_{1},X_{2},\ldots,X_{n})}\) of the random vector \((X_{1},X_{2},\ldots,X_{n})\) is \(\mu_{n}\).

**Proof:** For any \(\{i_{1},i_{2},\ldots,i_{k}\}\subset\mathbb{N}\), let \(j_{1}<j_{2}<\cdots<j_{k}\) be the increasing rearrangement of \(i_{1},i_{2},\ldots,i_{k}\). Then there exists a permutation \((r_{1},r_{2},\ldots,r_{k})\) of \((1,2,\ldots,k)\) such that \(j_{1}=i_{r_{1}},j_{2}=i_{r_{2}},\ldots,j_{k}=i_{r_{k}}\).

Now define

\[\nu_{(j_{1},j_{2},\ldots,j_{k})}(\cdot)\equiv\mu_{j_{k}}\pi_{j_{1},j_{2}, \ldots,j_{k}}^{-1}(\cdot)\]

where \(\pi_{j_{1},j_{2},\ldots,j_{k}}(x_{1},\ldots,x_{j_{k}})=(x_{j_{1}},x_{j_{2}}, \ldots,x_{j_{k}})\) for all \((x_{1},x_{2},\ldots,x_{j_{k}})\in\mathbb{R}^{j_{k}}\).

Next define

\[\nu_{(i_{1},i_{2},\ldots,i_{k})}(B_{1}\times B_{2}\times\ldots\times B_{k}) \equiv\nu_{(j_{1},j_{2},\ldots,j_{k})}(B_{r_{1}}\times B_{r_{2}}\times\ldots \times B_{r_{k}})\]

where \(B_{i}\in\mathcal{B}(\mathbb{R})\) for all \(i\), \(1\leq i\leq k\). It can be verified that this family of finite dimensional distributions

\[Q_{\mathbb{N}}\equiv\{\nu_{(i_{1},i_{2},\ldots,i_{k})}(\cdot):\{i_{1},i_{2}, \ldots,i_{k}\}\subset\mathbb{N},\ 1\leq k<\infty\} \tag{3.12}\]

satisfies the consistency conditions (3.1) and (3.2) of Theorem 6.3.1 and hence the assertion follows. \(\Box\)

**Example 6.3.6:** (_Sequence of independent random variables_). Let \(\{F_{n}\}_{n\geq 1}\) be a sequence of cdfs on \(\mathbb{R}.\) Consider the problem of constructing a sequence \(\{X_{n}\}_{n\geq 1}\) of random variables on a probability space \((\Omega,\mathcal{F},P)\) such that (i) for each \(n\in\mathbb{N},\)\(X_{n}\) has cdf \(F_{n}\) and (ii) for any \(n\in\mathbb{N}\) and any \(\{i_{1},i_{2},\ldots,i_{n}\}\subset\mathbb{N},\) the random variables \(\{X_{i_{1}},X_{i_{2}},\ldots,X_{i_{n}}\}\) are _independent_, i.e.,

\[P(X_{i_{1}}\leq x_{1},X_{i_{2}}\leq x_{2},\ldots,X_{i_{n}}\leq x_{n})=\prod_{j =1}^{n}F_{i_{j}}(x_{j}) \tag{3.13}\]

for all \(x_{1},x_{2},\ldots,x_{n}\) in \(\mathbb{R}.\)

This problem can be solved by using Theorem 6.3.4. Let \(\mu_{n}\) be the Lebesgue-Stieltjes probability measure on \((\mathbb{R}^{n},\mathcal{B}(\mathbb{R}^{n}))\) corresponding to the distribution function

\[F_{1,2,\ldots,n}(x_{1},x_{2},\ldots,x_{n})\equiv\prod_{j=1}^{n}F(x_{j}),\quad x _{1},\ldots,x_{n}\in\mathbb{R}.\]

It is easy to verify that the family \(\{\mu_{n}:n\geq 1\}\) satisfies (i) and (ii) of Theorem 6.3.4. Hence, there exist a probability measure \(P\) on the sequence space \(\Omega\equiv\mathbb{R}^{\infty}\) equipped with \(\sigma\)-algebra \(\mathcal{F}\equiv\mathcal{B}(\mathbb{R}^{\infty})\) and random variables \(X_{n}(\omega)\equiv\pi_{n}(\omega)\equiv\omega(n),\) for \(\omega=(\omega(1),\omega(2),\ldots)\) in \(\mathbb{R}^{\infty},\)\(n\geq 1,\) such that (3.13) holds.

**Example 6.3.7:** (_Family of independent random variables_). Given a family \(\{F_{\alpha}:\alpha\in A\}\) of cdfs on \(\mathbb{R}\) for some index set \(A,\) a construction similar to Example 6.3.6, but using Theorem 6.3.1 yields the existence of a real valued stochastic process \(\{X_{\alpha}:\alpha\in A\}\) such that for any \(\{\alpha_{1},\alpha_{2},\ldots,\alpha_{n}\}\subset A,\)\(1\leq n<\infty,\) the random variables \(\{X_{\alpha_{1}},X_{\alpha_{2}},\ldots,X_{\alpha_{n}}\}\) are independent, i.e., (3.13) holds.

**Example 6.3.8:** (_Markov chains_). Let \(Q=((q_{ij}))\) be a \(k\times k\)_stochastic matrix_ for some \(1<k<\infty.\) That is,

* for all \(1\leq i,\)\(j\leq k,\)\(q_{ij}\geq 0\) and
* for each \(1\leq i\leq k,\)\(\sum_{j=1}^{k}q_{ij}=1.\)

Let \(p=(p_{1},p_{2},\ldots,p_{k})\) be a probability vector, i.e., for all \(i,\)\(p_{i}\geq 0,\) and \(\sum_{i=1}^{k}p_{i}=1.\) Consider the problem of constructing a sequence \(\{X_{n}\}_{n\geq 1}\) of random variables such that for each \(n\in\mathbb{N},\)

\[P(X_{1}=j_{1},X_{2}=j_{2},\ldots,X_{n}=j_{n})=p_{j_{1}}q_{j_{1}j_{2}}\ldots q_ {j_{n-1}j_{n}} \tag{3.14}\]

for \(1\leq j_{i}\leq k,\)\(i=1,2,\ldots,n.\)

[MISSING_PAGE_FAIL:223]

\(\operatorname{Cov}(X_{\alpha},X_{\beta})\) are called the _mean and covariance functions_, respectively. Since \(\operatorname{Var}(\sum_{i=1}^{k}t_{i}X_{\alpha_{i}})\geq 0\), it follows that for any \(t_{1},t_{2},\ldots,t_{k}\),

\[\sum_{i=1}^{k}\sum_{j=1}^{k}t_{i}t_{j}\sigma(\alpha_{i},\alpha_{j})\geq 0. \tag{3.17}\]

This property of the covariance function \(\sigma(\cdot,\cdot)\) is called _nonnegative definiteness_.

A natural question is: Given functions \(\mu:A\to\mathbb{R}\) and \(\sigma:A\times A\to\mathbb{R}\) such that \(\sigma\) is symmetric and satisfies (3.17), does there exist a Gaussian process \(\{X_{\alpha}:\alpha\in A\}\) with \(\mu(\cdot)\) and \(\sigma(\cdot;)\) as its mean and covariance functions, respectively? The answer is yes and it follows from Theorem 6.3.1 by defining the family \(Q_{A}\) of finite dimensional distributions as follows. Let \(\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}\) be the unique probability distribution on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\) with the moment generating function

\[M_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(s_{1},s_{2},\ldots,s_{k}) \tag{3.18}\] \[= \exp\bigg{(}\sum_{i=1}^{k}s_{i}\mu(\alpha_{i})+\frac{1}{2}\sum_{i =1}^{k}\sum_{j=1}^{k}s_{i}s_{j}\sigma(\alpha_{i},\alpha_{j})\bigg{)}\]

for \(s_{1},s_{2},\ldots,s_{k}\) in \(\mathbb{R}\). If the matrix \(\Sigma\equiv\big{(}(\sigma(\alpha_{i},\alpha_{j}))\big{)}\), \(1\leq i\), \(j\leq k\) is _positive definite_, i.e., it is such that in (3.17) equality holds iff \(t_{i}=0\) for all \(i\), then \(\nu_{(\alpha_{1},\ldots,\alpha_{k})}(\cdot)\) can be shown to be a probability measure that is absolutely continuous w.r.t. \(m^{k}\), the Lebesgue measure on \(\mathbb{R}^{k}\) with density \(\frac{1}{(2\pi)^{k/2}}|\Sigma|^{-\frac{1}{2}}\,e^{-\sum_{i=1}^{k}\sum_{j=1}^{k} \big{(}x_{i}-\mu(\alpha_{i})\big{)}\tilde{\sigma}_{ij}\big{(}x_{j}-\mu(\alpha_ {j})\big{)}/2}\) where \(\tilde{\Sigma}\equiv\big{(}(\tilde{\sigma}_{ij})\big{)}=\Sigma^{-1}\), the inverse of \(\Sigma\) and \(|\Sigma|=\) the determinant of \(\Sigma\).

The verification of conditions (3.1) and (3.2) for this family is left as an exercise (Problem 6.12).

**Remark 6.3.4:** Kolmogorov's consistency theorem (Theorem 6.3.1) remains valid when the real line \(\mathbb{R}\) is replaced by a complete separable metric space \(\mathbb{S}\). More specifically, let \(A\) be a nonempty set and for \(\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A\), \(1\leq k<\infty\), let \(\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(\cdot)\) be a probability measure on \((\mathbb{S}^{k},\mathcal{B}(\mathbb{S}^{k}))\). If the family \(Q_{A}\equiv\{\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}:\{\alpha_{1}, \alpha_{2},\ldots,\alpha_{k}\}\subset A\), \(1\leq k<\infty\}\) satisfies the natural analogs of (3.1) and (3.2), then there exists a probability measure \(P\) on \((\Omega\equiv\mathbb{S}^{A},\mathcal{F}\equiv(\mathcal{B}(\mathbb{S}))^{A})\) and an \(\mathbb{S}\)-valued stochastic process \(\{X_{\alpha}:\alpha\in A\}\) on \((\Omega,\mathcal{F},P)\) such that \(\nu_{(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})}(\cdot)=P(X_{\alpha_{1}},X_{ \alpha_{2}},\ldots X_{\alpha_{K}})^{-1}(\cdot)\). Here \(\mathbb{S}^{A}\) is the set of all \(\mathbb{S}\) valued functions on \(A\), \((\mathcal{B}(\mathbb{S}))^{A}\) is the \(\sigma\)-algebra generated by the cylinder sets of the form

\[C=\{f:f:A\to\mathbb{S},\ f(\alpha_{i})\in B_{i},\ i=1,2,\ldots,k\}\]

where \(\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A\), \(B_{i}\in\mathcal{B}(\mathbb{S})\), \(1\leq i\leq k\), \(1\leq k<\infty\), and also \(X_{\alpha}(\omega)\) is the projection map \(X_{\alpha}(\omega)\equiv\omega(\alpha)\). The main step in the 

[MISSING_PAGE_FAIL:225]

\({\cal F}\equiv\times_{\alpha\in A}{\cal F}_{\alpha}\) is the \(\sigma\)-algebra generated by finite dimensional cylinder sets of the form

\[C=\{\omega:\omega(\alpha_{i})\in B_{\alpha_{i}},\ i=1,2,\ldots,k\}, \tag{3.21}\]

\(1\leq k<\infty,\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A\), \(B_{\alpha_{i}}\in{\cal F}_{\alpha_{i}}\) and \(P\equiv\times_{\alpha\in A}P_{\alpha}\) is the probability measure on \({\cal F}\) such that for \(C\) of (3.21),

\[P(C)=\prod_{i=1}^{k}P_{\alpha_{i}}(B_{\alpha_{i}}). \tag{3.22}\]

The proof of the existence of such a \(P\) on \({\cal F}\) is an application of the extension theorem (Theorem 1.3.3). The verification of countable additivity on the class \({\cal C}\) of cylinder sets is not difficult. See Kolmogorov (1956).

### Problems

1. Let \(\mu_{1}=\mu_{2}\) be the probability distribution on \(\Omega=\{1,2\}\) with \(\mu_{1}(\{1\})=1/2\). Find two distinct probability distributions on \(\Omega\times\Omega\) with \(\mu_{1}\) and \(\mu_{2}\) as the set of marginals.
2. Let \(\Omega=(0,1)\), \({\cal F}={\cal B}((0,1))\) and \(P\) be the Lebesgue measure on (0,1). Let \(X(\omega)=-\log\omega\), \(h(x)=x^{2}\) and \(Y=h(X)\). Find \(P_{X}\) and \(P_{Y}\) and evaluate \(EY\) by applying the change of variables formula (Proposition 6.2.1).
3. In the change of variables formula, one of the three integrals is usually easier to evaluate than the other two. In this problem, in part (a), the first integral is easier to evaluate than the other two while in part (b), the second one is easier. 1. Let \(Z\sim N(0,1)\), \(X=Z^{2}\), and \(Y=e^{-X}\). 1. Find the distributions \(P_{X}\) and \(P_{Y}\) on \((\mathbb{R},{\cal B}(\mathbb{R}))\). 2. Compute the integrals \[\int_{\mathbb{R}}e^{-z^{2}}\phi(z)dz,\ \ \int_{\mathbb{R}}e^{-x}P_{X}(dx)\ \ \ \mbox{and}\ \ \ \int_{\mathbb{R}}yP_{Y}(dy),\] where \(\phi(z)=\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\), \(-\infty<z<\infty\). Verify that all three integrals agree. 2. Let \(X_{1},X_{2},\ldots,X_{n}\) be iid \(N(0,1)\) random variables. Let \(Y=(X_{1}+\cdots+X_{k})\) and \(Z=Y^{2}\). 1. Find the distributions of \(Y\) and \(Z\). 2. Evaluate \(\int_{\mathbb{R}^{k}}(x_{1}\ +\ \cdots\ +\ x_{k})^{2}dP_{X_{1},\ldots,X_{k}}(x_{1},\ldots,x_{k})\), \(\int_{\mathbb{R}}y^{2}P_{Y}(dy)\), and \(\int_{\mathbb{R}_{+}}zP_{Z}(dz)\).

3. Let \(X_{1},X_{2},\ldots,X_{k}\) be independent Binomial \((n_{i},p)\), \(i=1,2,\ldots,k\) random variables. Let \(Y=(X_{1}+\cdots+X_{k})\). 1. Find the distribution \(P_{Y}\) of \(Y\). 2. Evaluate \(\int_{\mathbb{R}^{k}}(x_{1}\;+\;\cdots\;+\;x_{k})dP_{X_{1},\ldots,X_{k}}(x_{1}, \ldots,x_{k})\) and \(\int_{\mathbb{R}}yP_{Y}(dy)\).
6.4 Let \(X\) be a random variable such that \(M_{X}(t)\equiv E(e^{tX})<\infty\) for \(|t|<\epsilon\) for some \(\epsilon>0\). 1. Show that \(E(e^{tX}|X|^{r})<\infty\) for all \(r>0\) and \(|t|<\epsilon\). 2. Show that \(M_{X}^{(r)}(t)\), the \(r\)th derivative of \(M_{X}(t)\) for \(r\in\mathbb{N}\), satisfies \[M_{X}^{(r)}(t)=E(e^{tX}X^{r})\quad\mbox{for}\quad|t|<\epsilon.\] 3. Verify (2.25). (**Hint:** (a) First show that for \(t_{1}\in(-\epsilon,\epsilon)\), there exist a \(t_{2}\in(-\epsilon,\epsilon)\) such that \(|t_{1}|<|t_{2}|<\epsilon\) and for some \(C<\infty\), \(e^{t_{1}x}|x|^{r}\leq Ce^{|t_{2}x|}\) for all \(x\) in \(\mathbb{R}\). (b) Verify that for all \(x\in\mathbb{R}\), \(|e^{x}-1|\leq|x|e^{|x|}\). Now use (a) and the DCT to show that \(M_{X}(t)\) is differentiable and \(M_{X}^{(1)}(t)=E(e^{tX}X)\) for all \(|t|<\epsilon\). Now complete the proof by induction.)
6.5 Let \(X\) be a random variable. 1. Show that \(\phi(r)\equiv(E|X|^{r})^{1/r}\) is nondecreasing on \((0,\infty)\). 2. Show that \(\phi(r)\equiv\log E|X|^{r}\) is convex in \((0,r_{0})\) if \(E|X|^{r_{0}}<\infty\). 3. Let \(M=\sup\{x:P(|X|>x)>0\}\). Show that 1. \(\lim_{r\uparrow\infty}\phi(r)=M\). 2. \(\lim_{n\to\infty}\frac{E|X|^{n+1}}{E|X|^{n}}=M\). (**Hint:** For \(M<\infty\), note that \(E|X|^{r}\geq(M-\epsilon)^{r}P(|X|>M-\epsilon)\) for any \(\epsilon>0\).)
6.6 Show that if equality holds in (2.20), then there exist constants \(a\) and \(b\) such that \(P(Y=aX+b)=1\). (**Hint:** Show that there exist a constant \(a\) such that \(\mbox{Var}(Y-aX)=0\).)
6.7 Determine \(C\) and its base \(B\) explicitly in Examples 6.3.3 and 6.3.4.
6.8 1. Show that \(D\) in Example 6.3.5 is not a finite dimensional cylinder set. (**Hint:** Note that \(\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^{n}x_{j}\) is not determined by the values of finitely many \(x_{i}\)'s.)2. Find three other such examples of sets \(D\) in \(\mathbb{R}^{\infty}\) that are not finite dimensional cylinder sets.
6.9 Establish the assertion in Remark 6.3.3 by completing the following steps: 1. Show that the coordinate map \(f_{n}(x)\equiv x_{n}\) from \(\mathbb{R}^{\infty}\) to \(\mathbb{R}\) is continuous under the metric \(d\) of (3.3). (Conclude, using Example 1.1.6, that \(\mathcal{R}^{\mathbb{N}}\subset\mathcal{B}(\mathbb{R}^{\infty})\)). 2. Let \(\mathcal{C}_{1}\equiv\{A:A=(a_{1},b_{1})\times\cdots\times(a_{k},b_{k})\times \mathbb{R}^{\infty},\,-\infty\leq a_{i}<b_{i}\leq\infty,\,1\leq i\leq k,\) for some \(k<\infty\}\) and \(\mathcal{C}_{2}\equiv\{A:A\) is an open ball in \((\mathbb{R}^{\infty},d)\}\). Show that \(\sigma\langle\mathcal{C}_{2}\rangle\subset\sigma\langle\mathcal{C}_{1}\rangle\). 3. Show that \(\sigma\langle\mathcal{C}_{2}\rangle=\mathcal{B}(\mathbb{R}^{\infty})\) by showing that every open set in \((\mathbb{R}^{\infty},d)\) is a countable union of open balls.
6.10 Show that the family \(Q_{\mathbb{N}}\) defined in (3.12) satisfies the consistency conditions (3.1) and (3.2) of Theorem 6.3.1.
6.11 Verify that the family of finite dimensional distributions defined by the right side of (3.14) satisfies the conditions of Theorem 6.3.4.
6.12 Verify that the family of distributions defined in (3.18) satisfies conditions (3.1) and (3.2) of Theorem 6.3.1. (**Hint:** Use the fact that for any \(k\geq 1\), any \(\mu=(\mu_{1},\mu_{2},\ldots,\mu_{k})\in\mathbb{R}^{k}\), and any nonnegative definite \(k\times k\) matrix \(\Sigma\equiv((\sigma_{ij}))_{k\times k}\), there is a unique probability distribution \(\nu\) such that for any \(s=(s_{1},s_{2},\ldots,s_{k})\) in \(\mathbb{R}^{k}\), \[\int_{\mathbb{R}^{k}}\exp\bigg{(}\sum_{i=1}^{k}s_{i}x_{k}\bigg{)} \nu(dx)\] \[= \exp\bigg{(}\sum_{i=1}^{k}s_{i}\mu_{i}+\frac{1}{2}\sum_{i=1}^{k} \sum_{j=1}^{k}s_{i}s_{j}\sigma_{ij}\bigg{)}.\] Observe that this implies that for \(s=(s_{1},s_{2},\ldots,s_{k})\) in \(\mathbb{R}^{k}\), the induced distribution (under \(\nu\)) on \(\mathbb{R}\) by the map \(g(x)=\sum_{i=1}^{k}s_{i}x_{i}\) from \(\mathbb{R}^{k}\to\mathbb{R}\) is univariate normal with mean \(\sum_{i=1}^{k}s_{i}\mu_{i}\) and variance \(\sum_{i=1}^{k}\sum_{j=1}^{k}s_{i}s_{j}\sigma_{ij}\).)
6.13 Show that the set \(D\equiv C[0,1]\) of continuous functions from \([0,1]\) to \(\mathbb{R}\) is not a member of the \(\sigma\)-algebra \(\mathcal{F}\equiv(\mathcal{B}(\mathbb{R}))^{[0,1]}\). (**Hint:** If \(D\in\mathcal{F}\), then by Proposition 6.3.5, \(D\) is of the form \(\pi_{A_{1}}^{-1}(B)\) for some \(B\) in \(\mathcal{B}(\mathbb{R}^{\infty})\), where \(A_{1}\subset[0,1]\) is countable. Show that for any such \(A_{1}\) and \(B\), there exist functions \(f:[0,1]\to\mathbb{R}\) such that \(f\in\pi_{A_{1}}^{-1}(B)\) but \(f\) is not continuous on \([0,1]\).)* Show that \(K\equiv\big{\{}\omega:\omega\in\mathbb{R}^{[0,1]},\)\(\sup_{0\leq\alpha\leq 1}|\omega(\alpha)|<1\big{\}}\) is not in \(\mathcal{F}\equiv(\mathcal{B}(\mathbb{R}))^{[0,1]}\). (**Hint:** Observe that \(\sup_{0\leq\alpha\leq 1}|\omega(\alpha)|\) is not determined by the values of \(\omega(\alpha)\) for countably many \(\alpha\)'s.)
* Let \(\{\mu_{i}\}_{i\geq 1}\) be a sequence of probability distributions on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\) and let \(\mu\) be a probability distribution on \(\mathbb{N}\) with \(p_{i}\equiv\mu(\{i\}),\)\(i\geq 1\).
* Verify that \(\nu(\cdot)\equiv\sum_{i\geq 1}p_{i}\mu_{i}(\cdot)\) is a probability distribution on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\).
* Show that there exists a probability space \((\Omega,\mathcal{F},P)\) and a collection of independent random variables \(\{J,X_{1},X_{2},\ldots\}\) on \((\Omega,\mathcal{F},P)\) such that for each \(i\geq 1\), \(X_{i}\) has distribution \(\mu_{i}\) and \(J\sim\mu\).
* Let \(Y=X_{J}\), i.e., \(Y(\omega)\equiv X_{J(\omega)}(\omega)\). Show that \(Y\) is a random variable on \((\Omega,\mathcal{F},P)\) and \(Y\sim\nu\).
* Let \(F\) be a cdf on \(\mathbb{R}\) and let \(F\) be decomposed as \[F=\alpha F_{d}+\beta F_{ac}+\gamma F_{sc}\] where \(\alpha,\beta,\gamma\in[0,1]\) and \(\alpha+\beta+\gamma=1\) and \(F_{d}\), \(F_{ac}\), \(F_{sc}\) are discrete, absolutely continuous, and singular continuous cdfs on \(\mathbb{R}\) (cf. (4.5.3)). Show that there exist independent random variables \(X_{1},X_{2},X_{3}\) and \(J\) on some probability space such that \(X_{1}\sim F_{d}\), \(X_{2}\sim F_{ac}\), \(X_{3}\sim F_{sc}\), \(P(J=1)=\alpha\), \(P(J=2)=\beta\), \(P(J=3)=\gamma\) and \(X_{J}\sim F\), where \(\sim\) means "has cdf".
* Let \(\mu\) be a probability measure on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\). Let for each \(x\) in \(\mathbb{R}\), \(F(x,\cdot)\) be a cdf on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\). Let \(\psi(x,t)\equiv\inf\{y:F(x,y)\geq t\}\), for \(x\) in \(\mathbb{R}\), \(0<t<1\). Assume that \(\psi(\cdot,\cdot):\mathbb{R}\times(0,1)\to\mathbb{R}\) is measurable. Let \(X\) and \(U\) be independent random variables on some probability space \((\Omega,\mathcal{F},P)\) such that \(X\sim\mu\) and \(U\sim\) uniform (0,1).
* Show that \(Y=\psi(X,U)\) is a random variable.
* Show that \(P(Y\leq y)=\int_{\mathbb{R}}F(x,y)\mu(dx)\). (The distribution of \(Y\) is called a mixture of distributions with \(\mu\) as the mixing distribution. This is of relevance in Bayesian statistical inference.)
* Let \((\mathbb{S}_{i},\mathcal{S}_{i})\), \(i=1,2\) be two measurable spaces. Let \(\mu\) be a probability measure on \((\mathbb{S}_{1},\mathcal{S}_{1})\) and let \(Q:\mathbb{S}_{1}\times\mathcal{S}_{2}\to[0,1]\) be such that for each \(x\) in \(\mathbb{S}_{1}\), \(Q(x,\cdot)\) is a probability measure on \((\mathbb{S}_{2},\mathcal{S}_{2})\) and for each \(B\) in \(\mathcal{S}_{2}\), \(Q(\cdot,B)\) is \(\mathcal{S}_{1}\)-measurable. Define \[\nu(B_{1}\times B_{2})\equiv\int_{B_{1}}Q(x,B_{2})\mu(dx)\]

[MISSING_PAGE_FAIL:230]

4. _singular_ if \(PX^{-1}\bot m\) or equivalently \(F_{X}(\cdot)=0\) a.e. \(m\), 5. _singular continuous_ if it is singular and continuous.

Let \(g:\mathbb{R}\to\mathbb{R}\) be Borel measurable and \(Y=g(X)\).

1. Show that if \(X\) is discrete then so is \(Y\) but not conversely.
2. Show that if \(X\) is continuous and \(g\) is (1-1) on the range of \(X\), then \(Y\) is continuous.
3. Show if \(X\) is absolutely continuous with pdf \(f_{X}(\cdot)\) and \(g\) is absolutely continuous on bounded intervals such that \(g^{\prime}(\cdot)>0\) a.e. \((m)\), then \(Y\) is also absolutely continuous with pdf \[f_{Y}(y)=\frac{f_{X}\big{(}g^{-1}(y)\big{)}}{g^{\prime}\big{(}g^{-1}(y)\big{)}}.\]
4. Let \(X\) be as in (c) above. Suppose \(g\) is absolutely continuous on bounded intervals and there exist disjoint intervals \(\{I_{j}\}_{1\leq j\leq k}\), \(1\leq k\leq\infty\), such that \(\bigcup_{1\leq j\leq k}I_{j}=\mathbb{R}\) and for each \(j\), either \(g^{\prime}(\cdot)>0\) a.e. \((m)\) on \(I_{j}\) or \(g^{\prime}(\cdot)<0\) a.e. \((m)\) on \(I_{j}\). Show that \(Y\) is also absolutely continuous with pdf \[f_{Y}(y)=\sum_{x_{j}\in D(y)}\frac{f_{X}(x_{j})}{|g^{\prime}(x_{j})|}\] where \(D\{y\}\equiv\{x_{j}:x_{j}\in I_{j}\), \(g(x_{j})=y\}\).
5. Use (c) to compute the pdf of \(Y\) when 1. \(X\sim N(0,1)\), \(g(x)=e^{x}\). 2. \(X\sim N(0,1)\), \(g(x)=x^{2}\). 3. \(X\sim N(0,1)\), \(g(x)=\sin 2\pi x\). 4. \(X\sim\exp(1)\), \(g(x)=e^{-x}\).
6.21 (_Simple random sampling without replacement_). Let \(S\equiv\{1,2,\ldots,m\}\), \(1<m<\infty\). Fix \(1\leq n\leq m\). Choose an element \(X_{1}\) from \(S\) such that the probability that \(X_{1}=j\) is \(\frac{1}{m}\) for all \(j\in S\). Next, choose an element \(X_{2}\) from \(S-\{X_{1}\}\) such that the probability that \(X_{2}=j\) is \(\frac{1}{(m-1)}\) for \(j\in S-\{X_{1}\}\). Continue this procedure for \(n\) steps. Write the outcome as the ordered vector \(\omega\equiv(X_{1},X_{2},\ldots,X_{n})\). 1. Identify the sample space \(\Omega\), the \(\sigma\)-algebra \(\mathcal{F}\) and the probability measure \(P\) for this experiment. 2. Show that for any permutation \(\sigma\) of \(\{1,2,\ldots,n\}\), the random vector \(Y_{\sigma}=(X_{\sigma(1)},X_{\sigma(2)},\ldots,X_{\sigma(n)})\) has the same distribution as \((X_{1},X_{2},\ldots,X_{n})\).

3. Conclude that \(\{X_{i}\}_{1\leq i\leq n}\) are identically distributed and that \(EX_{i}\), \(\operatorname{Cov}(X_{i},X_{j})\), \(i\neq j\) are independent of \(i\) and \(j\) and compute them.
4. Answer the same questions (a)-(c) if the sampling is changed to _with replacement_, i.e., at each stage \(i\), the probability that \(P(X_{i}=j)=\frac{1}{m}\) for all \(j\in S\). 5. In (d), let \(D\) be the number of _distinct units_ in the sample. Find \(E(D)\) and \(\operatorname{Var}(D)\).
6.22 Let \(X\) be a nonnegative random variable. Show that \[\sqrt{1+(EX)^{2}}\leq E\sqrt{1+X^{2}}\leq 1+EX.\] (Note that \(f(x)\equiv\sqrt{1+x^{2}}\) is convex on \([0,\infty)\) and bounded by \(1+x\).)
6.23 Let \(X\) and \(Y\) be nonnegative random variables defined on a probability space \((\Omega,\mathcal{F},P)\). Suppose \(X\cdot Y\geq 1\) w.p. 1. Show that \[EX\cdot EY\geq 1.\] (**Hint:** Use Cauchy-Schwarz on \(\sqrt{X}\sqrt{Y}\).)
6.24 Let \(\mu\) be a probability measure on \(\bigl{(}\mathbb{R},\mathcal{B}(\mathbb{R})\bigr{)}\). Show that there is a random variable \(X\) on the Lebesgue space \(([0,1],\mathcal{B}([0,1]),m)\) such that \(m\,X^{-1}\equiv\mu\) where \(m\) is the Lebesgue measure. Extend this to \(\bigl{(}\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k})\bigr{)}\), where \(k\) is an integer \(>1\). (Note: This is true for any Polish space, i.e., a complete separable metric space, see Billingsley (1968).)

## Chapter 7 Independence

### 7.1 Independent events and random variables

Although a probability space is nothing more than a measure space with the measure of the whole space equal to one, probability theory is not merely a subset of measure theory. A distinguishing and fundamental feature of probability theory is the notion of _independence_.

**Definition 7.1.1:** Let \((\Omega,\mathcal{F},P)\) be a probability space and \(\{B_{1},B_{2},\ldots,B_{n}\}\subset\mathcal{F}\) be a finite collection of events.

1. \(B_{1},B_{2},\ldots,B_{n}\) are called _independent_ w.r.t. \(P\), if \[P\bigg{(}\bigcap_{j=1}^{k}B_{i_{j}}\bigg{)}=\prod_{j=1}^{k}P(B_{i_{j}})\] (1.1) for all \(\{i_{1},i_{2},\ldots,i_{k}\}\subset\{1,2,\ldots,n\},1\leq k\leq n\).
2. \(B_{1},B_{2},\ldots,B_{n}\) are called _pairwise independent_ w.r.t. \(P\) if \(P(B_{i}\cap B_{j})=P(B_{i})P(B_{j})\) for all \(i,j,i\neq j\).

Note that a collection \(B_{1},B_{2},\ldots,B_{n}\) of events may be independent with respect to one probability measure \(P\) but not with respect to another measure \(P^{\prime}\). Note also that pairwise independence does not imply independence (Problem 7.1).

**Definition 7.1.2:** Let \((\Omega,\mathcal{F},P)\) be a probability space. A collection of events \(\{B_{\alpha},\alpha\in A\}\subset\mathcal{F}\) is called _independent_ w.r.t. \(P\) if for every finitesubcollection \(\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A,1\leq k<\infty,\)

\[P\biggl{(}\bigcap_{i=1}^{k}B_{\alpha_{i}}\biggr{)}=\prod_{i=1}^{k}P(B_{\alpha_{i} }). \tag{1.2}\]

**Definition 7.1.3:** Let \((\Omega,{\cal F},P)\) be a probability space. Let \(A\) be a nonempty set. For each \(\alpha\) in \(A\), let \({\cal G}_{\alpha}\subset{\cal F}\) be a collection of events. Then the family \(\{{\cal G}_{\alpha}:\alpha\in A\}\) is called _independent_ w.r.t \(P\) if for every choice of \(B_{\alpha}\) in \({\cal G}_{\alpha}\) for \(\alpha\) in \(A\), the collection of events \(\{B_{\alpha}:\alpha\in A\}\) is independent w.r.t. \(P\) as in Definition 7.1.2.

**Definition 7.1.4:** Let \((\Omega,{\cal F},P)\) be a probability space and let \(\{X_{\alpha}:\alpha\in A\}\) be a collection of random variables on \((\Omega,{\cal F},P)\). Then the collection \(\{X_{\alpha}:\alpha\in A\}\) is called _independent_ w.r.t. \(P\) if the family of \(\sigma\)-algebras \(\{\sigma\langle X_{\alpha}\rangle:\alpha\in A\}\) is independent w.r.t. \(P\), where \(\sigma\langle X_{\alpha}\rangle\) is the \(\sigma\)-algebra generated by \(X_{\alpha}\), i.e.,

\[\sigma\langle X_{\alpha}\rangle\equiv\{X_{\alpha}^{-1}(B):B\in{\cal B}({\mathbb{ R}})\}. \tag{1.3}\]

Note that the collection \(\{X_{\alpha}:\alpha\in A\}\) is independent iff for any \(\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A\), and \(B_{i}\in{\cal B}({\mathbb{R}})\), for \(i=1,2,\ldots,k\), \(1\leq k<\infty\),

\[P(X_{\alpha_{i}}\in B_{i},\,i=1,2,\ldots,k)=\prod_{i=1}^{k}P(X_{\alpha_{i}}\in B _{i}). \tag{1.4}\]

It turns out that if (1.4) holds for all \(B_{i}\) of the form \(B_{i}=(-\infty,x_{i}],x_{i}\in{\mathbb{R}}\), then it holds for all \(B_{i}\in{\cal B}({\mathbb{R}})\), \(i=1,2,\ldots,k\). This follows from the proposition below.

**Proposition 7.1.1:** _Let \((\Omega,{\cal F},P)\) be a probability space. Let \(A\) be a nonempty set. Let \({\cal G}_{\alpha}\subset{\cal F}\) be a \(\pi\)-system for each \(\alpha\) in \(A\). Let \(\{{\cal G}_{\alpha}:\alpha\in A\}\) be independent w.r.t. \(P\). Then the family of \(\sigma\)-algebras \(\{\sigma\langle{\cal G}_{\alpha}\rangle:\alpha\in A\}\) is also independent w.r.t. \(P\)._

**Proof:** Fix \(2\leq k<\infty\), \(\{\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\}\subset A\), \(B_{i}\in{\cal G}_{\alpha_{i}}\), \(i=1,2,\ldots,k-1\). Let

\[{\cal L}\equiv\biggl{\{}B:B\in\sigma\langle{\cal G}_{\alpha_{k}}\rangle,\ P(B _{1}\cap\cdots\cap B_{k-1}\cap B)=\biggl{(}\prod_{i=1}^{k-1}P(B_{i})\biggr{)}P (B)\biggr{\}}. \tag{1.5}\]

[MISSING_PAGE_FAIL:235]

if \(\{X_{1},X_{2},\ldots,X_{k}\}\) are independent, then (1.8) holds for \(h_{i}=I_{B_{i}}\) for \(B_{i}\in{\cal B}(\mathbb{R})\), \(i=1,2,\ldots,k\), and hence for simple functions \(\{h_{1},h_{2},\ldots,h_{k}\}\). Now (1.8) follows from the BCT.
2. Note that by the change of variable formula (Proposition 6.2.1) \[E|X_{1}X_{2}| = \int_{\mathbb{R}^{2}}|x_{1}x_{2}|dP_{X_{1},X_{2}}(x_{1},x_{2}),\] \[E|X_{i}| = \int_{\mathbb{R}}|x_{i}|dP_{X_{i}}(x_{i}),\quad i=1,2,\] where \(P_{X_{1},X_{2}}\) is the joint distribution of \((X_{1},X_{2})\) and \(P_{X_{i}}\) is the marginal distribution of \(X_{i}\), \(i=1,2\). Also, by the independence of \(X_{1}\) and \(X_{2}\), \(P_{X_{1},X_{2}}\) is equal to the product measure \(P_{X_{1}}\times P_{X_{2}}\). Hence, by Tonelli's theorem, \[E|X_{1}X_{2}| = \int_{\mathbb{R}^{2}}|x_{1}x_{2}|dP_{X_{1},X_{2}}(x_{1},x_{2})\] \[= \int_{\mathbb{R}^{2}}|x_{1}x_{2}|dP_{X_{1}}(x_{1})dP_{X_{2}}(x_{ 2})\] \[= \bigg{(}\int_{\mathbb{R}}|x_{1}|dP_{X_{1}}(x_{1})\bigg{)}\bigg{(} \int_{\mathbb{R}}|x_{2}|dP_{X_{2}}(x_{2})\bigg{)}\] \[= E|X_{1}|E|X_{2}|<\infty.\] Now using Fubini's theorem, one gets (1.9). \(\Box\)

**Remark 7.1.2:** Note that the converse to (ii) above need not hold. That is, if \(X_{1}\) and \(X_{2}\) are two random variables such that \(E|X_{1}|<\infty\), \(E|X_{2}|<\infty\), \(E|X_{1}X_{2}|<\infty\), and \(EX_{1}X_{2}=EX_{1}EX_{2}\), then \(X_{1}\) and \(X_{2}\) need not be independent.

### Borel-Cantelli lemmas, tail \(\sigma\)-algebras, and Kolmogorov's zero-one law

In this section some basic results on classes of independent events are established. These will play an important role in proving laws of large numbers in Chapter 8.

**Definition 7.2.1:** Let \((\Omega,{\cal F})\) be a measurable space and \(\{A_{n}\}_{n\geq 1}\) be a sequence of sets in \({\cal F}\). Then

\[\limsup_{n\to\infty}A_{n} \equiv \overline{\lim}\,A_{n}\equiv\bigcap_{k=1}^{\infty}\bigg{(}\bigcup _{n\geq k}A_{n}\bigg{)} \tag{2.1}\]\[\lim_{n\to\infty}A_{n} \equiv \lim_{n\to\infty}A_{n}\equiv\bigcup_{k=1}^{\infty}\bigcap_{n\geq k}A _{n}. \tag{2.2}\]

**Proposition 7.2.1:** _Both \(\overline{\lim}\,A_{n}\) and \(\lim A_{n}\in{\cal F}\) and_

\[\overline{\lim}\,A_{n} = \{\omega:\omega\in A_{n}\mbox{ for infinitely many }n\}\] \[\underline{\lim}\,A_{n} = \{\omega:\omega\in A_{n}\mbox{ for all but a finite number of }n\}.\]

**Proof:** Since \(\{A_{n}\}_{n\geq 1}\subset{\cal F}\) and \({\cal F}\) is a \(\sigma\)-algebra, \(B_{k}=\bigcup_{n\geq k}A_{n}\in{\cal F}\) for each \(k\in\mathbb{N}\) and hence \(\overline{\lim}\,A_{n}\equiv\bigcap_{k=1}^{\infty}B_{k}\in{\cal F}.\) Next,

\[\omega\in\overline{\lim}\,A_{n}\] \[\Longleftrightarrow\omega\in B_{k}\quad\mbox{for all}\quad k=1, 2,...\] \[\Longleftrightarrow\mbox{for each}\quad k,\quad\mbox{there exists}\quad n_{k}\geq k\quad\mbox{such that}\quad\omega\in A_{n_{k}}\] \[\Longleftrightarrow\omega\in A_{n}\quad\mbox{for infinitely many}\quad n.\]

The proof for \(\underline{\lim}\,A_{n}\) is similar. \(\Box\)

In probability theory, \(\overline{\lim}\,A_{n}\) is referred to as the event that "\(A_{n}\) happens infinitely often (i.o.)" and \(\underline{\lim}\,A_{n}\) as the event that "all but a finitely many \(A_{n}\)'s happen."

**Example 7.2.1:** Let \(\Omega=\mathbb{R}\), \({\cal F}={\cal B}(\mathbb{R})\), and let

\[A_{n}=\left\{\begin{array}{ll}\left[0,\frac{1}{n}\right]\ \ \mbox{for}\ \ n\ \ \mbox{odd}\\ \left[1-\frac{1}{n},1\right]\ \ \mbox{for}\ \ n\ \ \mbox{even}.\end{array}\right.\]

Then \(\overline{\lim}\,A_{n}=\{0,1\}\), \(\underline{\lim}\,A_{n}=\emptyset\).

The following result on the probabilities of \(\overline{\lim}\,A_{n}\) and \(\underline{\lim}\,A_{n}\) is very useful in probability theory.

**Theorem 7.2.2:** _Let \((\Omega,{\cal F},P)\) be a probability space and \(\{A_{n}\}_{n\geq 1}\) be a sequence of events in \({\cal F}\). Then_

* \((\)_The first Borel-Cantelli lemma_\()\)_. If_ \(\sum\limits_{n=1}^{\infty}P(A_{n})<\infty\)_, then_ \(P(\overline{\lim}\,A_{n})=0\)_._
* \((\)_The second Borel-Cantelli lemma_\()\)_. If_ \(\sum\limits_{n=1}^{\infty}P(A_{n})=\infty\) _and_ \(\{A_{n}\}_{n\geq 1}\) _are pairwise independent, then_ \(P(\overline{\lim}\,A_{n})=1\)_._

**Remark 7.2.1:** This result is also called a _zero-one law_ as it asserts that for pairwise independent events \(\{A_{n}\}_{n\geq 1}\), \(P(\overline{\lim}\,A_{n})=0\) or \(1\) according to \(\sum_{n=1}^{\infty}P(A_{n})<\infty\) or equal to \(\infty\).

**Proof:**

* Let \(Z_{n}\equiv\sum_{j=1}^{n}I_{A_{j}}\). Then \(Z_{n}\uparrow Z\equiv\sum_{j=1}^{\infty}I_{A_{j}}\) and by the MCT, \(EZ_{n}\equiv\sum_{j=1}^{n}P(A_{j})\uparrow EZ\). Thus, \(\sum_{j=1}^{\infty}P(A_{j})<\infty\Rightarrow EZ<\infty\Rightarrow Z<\infty\) w.p. \(1\Rightarrow P(Z=\infty)=0\). But the event \(\overline{\lim}\,A_{n}=\{Z=\infty\}\) and so (a) follows.
* Without loss of generality, assume \(P(A_{j})>0\) for some \(j\). Let \(J_{n}=\frac{Z_{n}}{EZ_{n}}\) for \(n\geq j\) where \(Z_{n}\) is as above. Then, \(EJ_{n}=1\) and by the pairwise independence of \(\{A_{n}\}_{n\geq 1}\), the variance of \(J_{n}\) is \[\mbox{Var}(J_{n})=\frac{\sum_{j=1}^{n}P(A_{j})(1-P(A_{j}))}{(EZ_{n})^{2}}\leq \frac{1}{(EZ_{n})}.\] If \(\sum_{j=1}^{\infty}P(A_{j})=\infty\), then \(EZ_{n}=\sum_{j=1}^{n}P(A_{j})\uparrow\infty\), by the MCT. Thus \(EJ_{n}\equiv 1\), \(\mbox{Var}(J_{n})\to 0\) as \(n\to\infty\). By Chebychev's inequality, for all \(\epsilon>0\), \[P(|J_{n}-1|>\epsilon)\leq\frac{\mbox{Var}(J_{n})}{\epsilon^{2}}\to 0\quad\mbox{as}\quad n\to\infty.\] Thus, \(J_{n}\to 1\) in probability and hence there exists a subsequence \(\{n_{k}\}_{k\geq 1}\) such that \(J_{n_{k}}\to 1\) w.p. \(1\) (cf. Theorem 2.5.2). Since \(EZ_{n_{k}}\uparrow\infty\), this implies that \(Z_{n_{k}}\to\infty\) w.p. \(1\). But \(\{Z_{n}\}_{n\geq 1}\) is nondecreasing in \(n\) and hence \(Z_{n}\uparrow\infty\) w.p. \(1\). Now since \(\overline{\lim}\,A_{n}=\{Z=\infty\}\), it follows that \(P(\overline{\lim}\,A_{n})=P(Z=\infty)=1\). \(\Box\)

**Proposition 7.2.3:** _Let \(\{X_{n}\}_{n\geq 1}\), be a sequence of random variables on some probability space \((\Omega,{\cal F},P)\)._

* _If_ \(\sum_{n=1}^{\infty}P(|X_{n}|>\epsilon)<\infty\) _for each_ \(\epsilon>0\)_, then_ \[P(\lim_{n\to\infty}X_{n}=0)=1.\]
* _If_ \(\{X_{n}\}_{n\geq}\) _are pairwise independent and_ \(P(\lim_{n\to\infty}X_{n}=0)=1\)_, then_ \(\sum_{n=1}^{\infty}P(|X_{n}|>\epsilon)<\infty\) _for each_ \(\epsilon>0\)_._

**Proof:**

* Fix \(\epsilon>0\). Let \(A_{n}=\{|X_{n}|>\epsilon\},n\geq 1\). Then \(\sum_{n=1}^{\infty}P(A_{n})<\infty\Rightarrow P(\overline{\lim}\,A_{n})=0\), by the first Borel-Cantelli lemma (Theorem 7.2.2 (a)). But \[(\overline{\lim}\,A_{n})^{c} = \{\omega:\mbox{ there exists }\ n(\omega)<\infty\ \mbox{ such that for all}\] \[\qquad\qquad n\geq n(\omega),\ w\not\in A_{n}\}\] \[= \{\omega:\mbox{ there exists }\ n(\omega)<\infty\ \mbox{ such that }\ |X_{n}(\omega)|\leq\epsilon\] \[\qquad\qquad\mbox{ for all }\ n\geq n(\omega)\}\] \[= B_{\epsilon},\ \ \mbox{say}.\]Thus, \(\sum_{n=1}^{\infty}P(A_{n})<\infty\Rightarrow P(B_{\epsilon})=1\). Let \(B=\bigcap_{r=1}^{\infty}B_{\frac{1}{r}}\). Now note that \[\left\{\omega:\lim_{n\to\infty}|X_{n}(\omega)|=0\right\}=\bigcap_{r=1}^{\infty }B_{\frac{1}{r}}.\] Since \(P(B^{c})\leq\sum_{r=1}^{\infty}P(B_{\frac{1}{r}}^{c})=0\), \(P(B)=1\).
2. Let \(\{X_{n}\}_{n\geq 1}\) be pairwise independent and \(\sum_{n=1}^{\infty}P(|X_{n}|>\epsilon_{0})=\infty\) for some \(\epsilon_{0}>0\). Let \(A_{n}=\{|X_{n}|>\epsilon_{0}\}\). Since \(\{X_{n}\}_{n\geq 1}\) are pairwise independent, so are \(\{A_{n}\}_{n\geq 1}\). By the second Borel-Cantelli lemma \[P(\overline{\lim}\,A_{n})=1.\] But \(\omega\in\overline{\lim}\,A_{n}\Rightarrow\limsup_{n\to\infty}|X_{n}|\geq \epsilon_{0}\) and hence \(P(\lim_{n\to\infty}|X_{n}|=0)=0\). This contradicts the hypothesis that \(P(\limsup_{n\to\infty}|X_{n}|=0)=1\). \(\Box\)

**Definition 7.2.2:** The _tail \(\sigma\)-algebra_ of a sequence of random variables \(\{X_{n}\}_{n\geq 1}\) on a probability space \((\Omega,\mathcal{F},P)\) is

\[\mathcal{T}=\bigcap_{n=1}^{\infty}\ \sigma\langle\{X_{j}:j\geq n\}\rangle\]

and any \(A\in\mathcal{T}\) is called a _tail event_. Further, any \(\mathcal{T}\)-measurable random variable is called a _tail random variable_ (w.r.t. \(\{X_{n}\}_{n\geq 1}\)).

Tail events are determined by the behavior of the sequence \(\{X_{n}\}_{n\geq 1}\) for large \(n\) and they remain unchanged if any finite subcollection of the \(X_{n}\)'s are dropped or replaced by another finite set of random variables. Events such as \(\{\limsup_{n\to\infty}X_{n}<x\}\) or \(\{\lim_{n\to\infty}X_{n}=x\}\), \(x\in\mathbb{R}\), belong to \(\mathcal{T}\). A remarkable result of Kolmogorov is that for any sequence of independent random variables, any tail event has probability zero or one.

**Theorem 7.2.4:** (_Kolmogorov's 0-1 law_)_. Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables on a probability space \((\Omega,\mathcal{F},P)\) and let \(\mathcal{T}\) be the tail \(\sigma\)-algebra of \(\{X_{n}\}_{n\geq 1}\). Then \(P(A)=0\) or 1 for all \(A\in\mathcal{T}\)._

**Remark 7.2.2:** Note that in Proposition 7.2.3, the event \(A\equiv\{\lim_{n\to\infty}X_{n}=0\}\) belongs to \(\mathcal{T}\), and hence, by the above theorem, \(P(A)=0\) or 1. Thus, proving that \(P(A)\neq 1\) is equivalent to proving \(P(A)=0\). Kolmogorov's 0-1 law only restricts the possible values of tail events like \(A\) to 0 or 1, while the Borel-Cantelli lemmas (Theorem 7.2.2) provide a tool for ascertaining whether the value is either 0 or 1. On the other hand, note that Theorem 7.2.2 requires only _pairwise independence_ of \(\{A_{n}\}_{n\geq 1}\) but Kolmogorov's 0-1 law requires the _full independence_ of the sequence \(\{X_{n}\}_{n\geq 1}\).

**Proof:** For \(n\geq 1\), define the \(\sigma\)-algebras \({\cal F}_{n}\) and \({\cal T}_{n}\) by \({\cal F}_{n}=\sigma\langle\{X_{1},\ldots,X_{n}\}\rangle\) and \({\cal T}_{n}=\sigma\langle\{X_{n+1},X_{n+2},\ldots\}\rangle\). Since \(X_{n}\), \(n\geq 1\) are independent, \({\cal F}_{n}\) is independent of \({\cal T}_{n}\) for all \(n\geq 1\). Since, for each \(n\), \({\cal T}=\bigcap_{m=n}^{\infty}{\cal F}_{m}\) is a sub \(\sigma\)-algebra of \({\cal T}_{n}\), this implies \({\cal F}_{n}\) is independent of \({\cal T}\) for all \(n\geq 1\) and hence \({\cal A}\equiv\bigcup_{n=1}^{\infty}{\cal F}_{n}\) is independent of \({\cal T}\). It is easy to check that \({\cal A}\) is an algebra (and hence, is a \(\pi\)-system). Hence, by Proposition 7.1.1, \(\sigma\langle{\cal A}\rangle\) is independent of \({\cal T}\). Since \({\cal T}\) is also a sub-\(\sigma\)-algebra of \(\sigma\langle{\cal A}\rangle=\sigma\langle\{X_{n}:n\geq 1\}\rangle\), this implies \({\cal T}\) is independent of itself. Hence for any \(B\in{\cal T}\),

\[P(B\cap B)=P(B)\cdot P(B),\]

which implies \(P(B)=0\) or \(1\). \(\Box\)

**Definition 7.2.3:** Let \((\Omega,{\cal F},P)\) be a probability space and let \(X:\Omega\to\bar{\mathbb{R}}\) be a \(\langle{\cal F}\), \({\cal B}(\bar{\mathbb{R}})\rangle\)-measurable mapping. (Recall the definition of \({\cal B}(\bar{\mathbb{R}})\) from (2.1.4)). Then \(X\) is called an _extended real-valued random variable_ or an \(\bar{\mathbb{R}}\)_-valued random variable_.

**Corollary 7.2.5:**_Let \({\cal T}\) be the tail \(\sigma\)-algebra of a sequence of independent random variables \(\{X_{n}\}_{n\geq 1}\) on \((\Omega,{\cal F},P)\) and let \(X\) be a \(\langle{\cal T},{\cal B}(\bar{\mathbb{R}})\rangle\)-measurable \(\bar{\mathbb{R}}\)-valued random variable from \(\Omega\) to \(\bar{\mathbb{R}}\). Then, there exists \(c\in\bar{\mathbb{R}}\) such that_

\[P(X=c)=1.\]

**Proof:** If \(P(X\leq x)=0\) for all \(x\in\mathbb{R}\), then \(P(X=+\infty)=1\). Hence, suppose that \(B\equiv\{x\in\mathbb{R}:P(X\leq x)\neq 0\}\neq\emptyset\). Since \(\{X\leq x\}\in{\cal T}\) for all \(x\in\mathbb{R}\), \(P(X\leq x)=1\) for all \(x\in B\). Define \(c=\inf\{x:x\in B\}\). Check that \(P(X=c)=1\). \(\Box\)

An immediate implication of Corollary 7.2.5 is that for any sequence of _independent_ random variables \(\{X_{n}\}_{n\geq 1}\), the \(\bar{\mathbb{R}}\)-valued random variables \(\limsup_{n\to\infty}X_{n}\) and \(\liminf_{n\to\infty}X_{n}\) are degenerate, i.e., they are constants w.p. 1.

**Example 7.2.2:** Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables on \((\Omega,{\cal F},P)\) with \(EX_{n}=0\), \(EX_{n}^{2}=1\) for all \(n\geq 1\). Let \(S_{n}=X_{1}+\ldots+X_{n},n\geq 1\) and \(\Phi(x)=\int_{-\infty}^{x}(\sqrt{2\pi})^{-1}\exp(-y^{2}/2)dy,x\in\mathbb{R}\). If \(P(S_{n}\leq\sqrt{n}x)\to\Phi(x)\) for all \(x\in\mathbb{R}\), then

\[\limsup_{n\to\infty}\frac{S_{n}}{\sqrt{n}}=+\infty\quad\mbox{a.s.} \tag{2.3}\]

To show this, let \(S=\limsup_{n\to\infty}S_{n}/\sqrt{n}\). First it will be shown that \(S\) is \(\langle{\cal T},{\cal B}(\bar{\mathbb{R}})\rangle\)-measurable. For any \(m\geq 1\), define the variables \(T_{m,n}=(X_{m+1}+\ldots+X_{n})/\sqrt{n}\) and \(S_{m,n}=(X_{1}+\ldots+X_{m})/\sqrt{n},n>m\). Note that for any fixed \(m\geq 1\), \(T_{m,n}\) is \(\sigma\langle X_{m+1},\ldots\rangle\)-measurable and \(S_{m,n}(\omega)\to 0\) as\(n\to\infty\) for all \(\omega\in\Omega.\) Hence, for any \(m\geq 1,\)

\[S = \limsup_{n\to\infty}\,\left(S_{m,n}+T_{m,n}\right)\] \[= \limsup_{n\to\infty}\,\,T_{m,n}\]

is \(\sigma\langle X_{m+1},X_{m+2},\ldots\rangle\)-measurable. Thus, \(S\) is measurable with respect to \({\cal T}=\bigcap_{m=1}^{\infty}\sigma\langle X_{m+1},X_{m+2},\ldots\rangle.\) Hence, by Theorem 7.2.4, \(P(S=+\infty)\in\{0,1\}.\)

If possible, now suppose that \(P(S=+\infty)=0.\) Then, by Corollary 7.2.5, there exists \(c\in[-\infty,\infty)\) such that \(P(S=c)=1.\) Let \(A_{n}=\{S_{n}>\sqrt{n}x\},\)\(n\geq 1,\) with \(x=c+1.\) Then,

\[0<1-\Phi(x) = \lim_{n\to\infty}P(A_{n})\] \[\leq \lim_{n\to\infty}P\bigg{(}\bigcup_{m\geq n}A_{m}\bigg{)}\] \[= P\bigg{(}\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}A_{m}\bigg{)}\] \[= P\bigg{(}\frac{S_{n}}{\sqrt{n}}>x\mbox{ i.o.}\bigg{)}\] \[\leq P(S\geq c+1)=0.\]

This shows that \(P(S=+\infty)\) must be \(1.\) Also see Problem 7.16.

**Remark 7.2.3:** It will be shown in Chapter 11 that if \(\{X_{i}\}_{i\geq 1}\) are independent and identically distributed (iid) random variables with \(EX_{1}=0\) and \(EX_{1}^{2}=1,\) then

\[P\Big{(}\frac{S_{n}}{\sqrt{n}}\leq x\Big{)}\to\Phi(x)\quad\mbox{for all}\quad x \quad\mbox{in}\quad\mathbb{R}.\]

(This is known as the central limit theorem.) Indeed, a stronger result known as the law of the iterated logarithm holds, which says that for such \(\{X_{i}\}_{i\geq 1},\)

\[\limsup_{n\to\infty}\frac{S_{n}}{\sqrt{2n\log\log n}}=+1,\quad\mbox{w.p. }1.\]

### 7.3 Problems

* Give an example of three events \(A_{1},A_{2},A_{3}\) on some probability space such that they are pairwise independent but not independent. (**Hint:** Consider iid random variables \(X_{1},X_{2},X_{3}\) with \(\frac{1}{2}=P(X_{1}=0)\) and the events \(A_{1}=\{X_{1}=X_{2}\}\), \(A_{2}=\{X_{1}=X_{3}\}\), \(A_{3}=\{X_{3}=X_{1}\}\).)
7. Let \(\{X_{\alpha}:\alpha\in A\}\) be a collection of independent random variables on some probability space \((\Omega,\mathcal{F},P)\). For any subset \(B\subset A\), let \(\mathcal{X}_{B}\equiv\{X_{\alpha}:\alpha\in B\}\). 1. Let \(B\) be a nonempty proper subset of \(A\). Show that the collections \(\mathcal{X}_{B}\) and \(\mathcal{X}_{B^{c}}\) are independent, i.e., the \(\sigma\)-algebras \(\sigma\langle\mathcal{X}_{B}\rangle\) and \(\sigma\langle\mathcal{X}_{B^{c}}\rangle\) are independent w.r.t. \(P\). 2. Let \(\{B_{\gamma}:\gamma\in\Gamma\}\) be a partition of \(A\) by nonempty proper subsets \(B_{\gamma}\). Show that the family of \(\sigma\)-algebras \(\{\sigma\langle\mathcal{X}_{B_{\gamma}}\rangle:\gamma\in\Gamma\}\) are independent w.r.t. \(P\).
7. Let \(X_{1},X_{2}\) be iid standard exponential random variables, i.e., \[P(X_{1}\in A)=\int\limits_{A\cap(0,\infty)}e^{-x}dx,\quad A\in\mathcal{B}( \mathbb{R}).\] Let \(Y_{1}=\min(X_{1},X_{2})\) and \(Y_{2}=\max(X_{1},X_{2})-Y_{1}\). Show that \(Y_{1}\) and \(Y_{2}\) are independent. Generalize this to the case of three iid standard exponential random variables.
7. Let \(\Omega=(0,1)\), \(\mathcal{F}=\mathcal{B}((0,1))\), the Borel \(\sigma\)-algebra on (0,1) and \(P\) be the Lebesgue measure on \((0,1)\). For each \(\omega\in(0,1)\), let \(\omega=\sum_{i=1}^{\infty}\frac{X_{i}(\omega)}{2^{i}}\) be the nonterminating binary expansion of \(\omega\). 1. Show that \(\{X_{i}\}_{i\geq 1}\) are iid Bernouilli (\(\frac{1}{2}\)) random variables, i.e., is \(P(X_{1}=0)=\frac{1}{2}=P(X_{1}=1)\). (**Hint:** Let \(s_{i}\in\{0,1\}\), \(i=1,2,\ldots,k\), \(k\in\mathbb{N}\). Show that the set \(\{\omega:0<\omega<1\), \(X_{i}(\omega)=s_{i}\), \(1\leq i\leq k\}\) is an interval of length \(2^{-k}\).) 2. Show that \[Y_{1} = \sum_{i=1}^{\infty}\frac{X_{2i-1}}{2^{i}}\] (3.1) \[Y_{2} = \sum_{i=1}^{\infty}\frac{X_{2i}}{2^{i}}\] (3.2) are independent Uniform (0,1) random variables. 3. Using the fact that the set \(\mathbb{N}\times\mathbb{N}\) of lattice points \((m,n)\) is in one to one correspondence with \(\mathbb{N}\) itself, construct a sequence \(\{Y_{i}\}_{i\geq 1}\) of iid Uniform (0,1) random variables such that for each \(j\), \(Y_{j}\) is a function of \(\{X_{i}\}_{i\geq 1}\).

4. For any cdf \(F\), show that the random variable \(X(\omega)\equiv F^{-1}(\omega)\) has cdf \(F\), where \[F^{-1}(u)=\inf\{x:F(x)\geq u\}\quad\mbox{for}\quad 0<u<1.\] (3.3) (e) Let \(\{F_{i}\}_{i\geq 1}\) be a sequence of cdfs on \(\mathbb{R}\). Using (c), construct a sequence \(\{Z_{i}\}_{i\geq 1}\) of independent random variables on \((\Omega,\mathcal{F},P)\) such that \(Z_{i}\) has cdf \(F_{i}\), \(i\geq 1\). (f) Show that the cdf of the random variable \(W\equiv\sum_{i=1}^{\infty}\frac{2X_{i}}{3^{i}}\) is the Cantor function (cf. Section 4.5). (g) Let \(p>1\) be a positive integer. For each \(\omega\in(0,1)\) let \(\omega\equiv\sum_{i=1}^{\infty}\frac{V_{i}(\omega)}{p^{i}}\) be the nonterminating \(p\)-nary expansion of \(\omega\). Show that \(\{V_{i}\}_{i\geq 1}\) are iid and determine the distribution of \(V_{1}\).
7. Let \(\{X_{i}\}_{i\geq 1}\) be a Markov chain with state space \(\mathbb{S}=\{0,1\}\) and transition probability matrix \[Q=\left(\begin{array}{cc}q_{0}&p_{0}\\ p_{1}&q_{1}\end{array}\right)\quad\mbox{where}\quad p_{i}=1-q_{i},\;0<q_{i}<1,\; i=0,1\.\] Let \(\tau_{1}=\min\{j:X_{j}=0\}\) and \(\tau_{k+1}=\min\{j:j>\tau_{k},X_{j}=0\}\), \(k=1,2,\ldots\). Note that \(\tau_{k}\) is the time of \(k\)th visit to the state \(0\). 1. Show that \(\{\tau_{k+1}-\tau_{k}:k\geq 1\}\) are iid random variables and independent of \(\tau_{1}\). 2. Show that \[P_{i}(\tau_{1}<\infty)=1\quad\mbox{and hence}\quad P_{i}(\tau_{k}<\infty)=1\] for all \(k\geq 2\), \(i=0,1\) where \(P_{i}\) denotes the probability distribution with \(X_{1}=i\) w.p. 1. (**Hint:** Show that \(\sum_{k=1}^{\infty}P(\tau_{1}>k\mid X_{1}=i)<\infty\) for \(i=0,1\) and use the Borel-Cantelli lemma.) 3. Show also that \(E_{i}(e^{\theta_{0}\tau_{1}})<\infty\) for some \(\theta_{0}>0\), \(i=0,1\), where \(E_{i}\) denotes the expectation under \(P_{i}\).
8. Let \(X_{1}\) and \(X_{2}\) be independent random variables. 1. Show that for any \(p>0\), \[E|X_{1}+X_{2}|^{p}<\infty\quad\mbox{iff}\quad E|X_{1}|^{p}<\infty,\;E|X_{2}|^{ p}<\infty.\] Show that this is false if \(X_{1}\) and \(X_{2}\) are not independent. (**Hint:** Use Fubini's theorem to conclude that \(E|X_{1}+X_{2}|^{p}<\infty\) implies that \(E|X_{1}+x_{2}|^{p}<\infty\) for some \(x_{2}\) and hence \(E|X_{1}|^{p}<\infty\).)2. Show that if \(E(X_{1}^{2}+X_{2}^{2})<\infty\), then \[\mbox{Var}(X_{1}+X_{2})=\mbox{Var}(X_{1})+\mbox{Var}(X_{2}).\] Show by an example that (3.4) need not imply the independence of \(X_{1}\) and \(X_{2}\). Show also that if \(X_{1}\) and \(X_{2}\) take only two values each and (3.4) holds, then \(X_{1}\) and \(X_{2}\) are independent.
7. Let \(X_{1}\) and \(X_{2}\) be two random variables on a probability space \((\Omega,\mathcal{F},P)\). 1. Show that, if \[P\big{(}X_{1}\in(a_{1},b_{1}),\ X_{2}\in(a_{2},b_{2})\big{)}\] \[=P\big{(}X_{1}\in(a_{1},b_{1})\big{)}P\big{(}X_{2}\in(a_{2},b_{2} )\big{)}\] (3.5) for all \(a_{1}\), \(b_{1}\), \(a_{2}\), \(b_{2}\) in a dense set \(D\) in \(\mathbb{R}\), then \(X_{1}\) and \(X_{2}\) are independent. (**Hint:** Show that (3.5) implies that the joint cdf of \((X_{1},X_{2})\) is the product of the marginal cdfs of \(X_{1}\) and \(X_{2}\) and use Corollary 7.1.2.) 2. Let \(f_{i}:\mathbb{R}\to\mathbb{R}\), \(i=1,2\) be two one-one functions such that both \(f_{i}\) and \(f_{i}^{-1}\) are Borel measurable, \(i=1,2\). Show that \(X_{1}\) and \(X_{2}\) are independent iff \(f_{1}(X_{1})\) and \(f_{2}(X_{2})\) are independent. Conclude that \(X_{1}\) and \(X_{2}\) are independent iff \(e^{X_{1}}\) and \(e^{X_{2}}\) are independent.
7.8 1. Let \(X_{1}\) and \(X_{2}\) be two independent bounded random variables. Show that \[E(p_{1}(X_{1})p_{2}(X_{2}))=(Ep_{1}(X_{1}))(Ep_{2}(X_{2}))\] (3.6) where \(p_{1}(\cdot)\) and \(p_{2}(\cdot)\) are polynomials. 2. Show that if \(X_{1}\) and \(X_{2}\) are bounded random variables and (3.6) holds for all polynomials \(p_{1}(\cdot)\) and \(p_{2}(\cdot)\), then \(X_{1}\) and \(X_{2}\) are independent. (**Hint:** Use the facts that (i) continuous functions on a bounded closed interval \([a,b]\) can be approximated uniformly by polynomials, and (ii) for any interval \((c,d)\subset[a,b]\), any random variable \(X\) and \(\epsilon>0\), there exists a continuous function \(f\) on \([a,b]\) such that \(E|f(X)-I_{(c,d)}(X)|<\epsilon\), provided \(P(X=c\) or \(d)=0\).)
7.9 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables on a probability space \((\Omega,\mathcal{F},P)\). Let \(R=R(\omega)\) be the radius of convergence of the power series \(\sum_{n=1}^{\infty}X_{n}r^{n}\).

* Show that \(R\) is a tail random variable. (**Hint**: Note that \[R=\frac{1}{\limsup_{n\to\infty}|X_{n}|^{1/n}}\.)\]
* Show that if \(E(\log|X_{1}|)^{+}=\infty\), then \(R=0\) w.p. 1. and if \(E(\log|X_{1}|)^{+}<\infty\), then \(R\geq 1\) w.p. 1. (**Hint:*
* Apply the Borel-Cantelli lemmas to \(A_{n}=\{|X_{n}|>\lambda^{n}\}\) for each \(\lambda>1\).)
* Let \(\{A_{n}\}_{n\geq 1}\) be a sequence of events in \((\Omega,{\cal F},P)\) such that \[\sum_{n=1}^{\infty}P(A_{n}\cap A_{n+1}^{c})<\infty\] and \(\lim_{n\to\infty}P(A_{n})=0\). Show that \[P(\limsup_{n\to\infty}A_{n})=0.\] Show also that \(\lim_{n\to\infty}P(A_{n})=0\) can be replaced by \(\lim_{n\to\infty}P(\bigcap_{j\geq n}A_{j})=0\). (**Hint:** Let \(B_{n}=A_{n}\cap A_{n+1}^{c}\), \(n\geq 1\), \(B=\overline{\lim}\,B_{n}\), \(A=\overline{\lim}\,A_{n}\). Show that \(A\cap B^{c}\subset\underline{\lim}\,A_{n}\).)
* For any nonnegative random variable \(X\), show that \(E|X|<\infty\) iff \(\sum_{n=1}^{\infty}P(|X|>\epsilon n)<\infty\) for every \(\epsilon>0\).
* Let \(\{X_{i}\}_{i\geq 1}\) be a sequence of pairwise independent and identically distribution random variables.
* Show that \(\lim_{n\to\infty}\frac{X_{n}}{n}=0\) w.p. 1 iff \(E|X_{1}|<\infty\). (**Hint:**\(E|X_{1}|<\infty\Longleftrightarrow\sum\limits_{n=1}^{\infty}P(|X_{n}|> \epsilon n)<\infty\) for all \(\epsilon>0\).)
* Show that \(E(\log|X_{1}|)^{+}<\infty\) iff \[\left(|X_{n}|\right)^{1/n}\to 1\quad\mbox{w.p. 1}.\]
* Let \(\{X_{i}\}_{i\geq 1}\) be a sequence of identically distributed random variables and let \(M_{n}=\max\{|X_{j}|:1\leq j\leq n\}\).

1. If \(E|X_{1}|^{\alpha}<\infty\) for some \(\alpha\in(0,\infty)\), then show that \[\frac{M_{n}}{n^{1/\alpha}}\to 0\mbox{ w.p.\ 1}.\] (3.7) (**Hint:** Fix \(\epsilon>0\). Let \(A_{n}=\{|X_{n}|>\epsilon n^{1/\alpha}\}\). Apply the first Borel-Cantelli lemma.) 2. Show that if \(\{X_{i}\}_{i\geq 1}\) are iid satisfying (3.7) for some \(\alpha>0\), then \(E|X_{1}|^{\alpha}<\infty\). (**Hint:** Apply the second Borel-Cantelli lemma.)
7.14 Let \(X_{1}\) and \(X_{2}\) be independent random variables with distributions \(\mu_{1}\) and \(\mu_{2}\). Let \(Y=(X_{1}+X_{2})\). 1. Show that the distribution \(\mu\) of \(Y\) is the convolution \(\mu_{1}*\mu_{2}\) as defined by \[(\mu_{1}*\mu_{2})(A)=\int_{\mathbb{R}}\mu_{1}(A-x)\mu_{2}(dx)\] (cf. Problem 5.12). 2. Show that if \(X_{1}\) has a continuous distribution then so does \(Y\). 3. Show that if \(X_{1}\) has an absolutely continuous distribution then so does \(Y\) and that the density function of \(Y\) is given by \[\Big{(}\frac{d\mu}{dm}\Big{)}(x)\equiv f_{Y}(x)=\int f_{X_{1}}(x-u)\mu_{2}(du)\] where \(f_{X_{1}}(x)\equiv\big{(}\frac{d\mu_{1}}{dm}\big{)}(x)\), the probability density of \(X_{1}\). 4. \(Y\) has a discrete distribution iff both \(X_{1}\) and \(X_{2}\) are discrete.
7.15 (_AR(1) series_). Let \(\{X_{n}\}_{n\geq 0}\) be a sequence of random variables such that for some \(\rho\in\mathbb{R}\), \[X_{n+1}=\rho X_{n}+\epsilon_{n+1},\ \ n\geq 0\] where \(\{\epsilon_{n}\}_{n\geq 1}\) are independent and independent of \(X_{0}\). 1. Show that if \(|\rho|<1\) and \(E(\log|\epsilon_{1}|)^{+}<\infty\), then \[\hat{X}_{n}\equiv\sum_{j=0}^{n}\rho^{j}\epsilon_{j+1}\ \ \mbox{ converges w.p. 1}\] to a limit \(\hat{X}_{\infty}\), say.

[MISSING_PAGE_FAIL:247]

3. Show that there exists a probability space \((\Omega,\mathcal{F},P)\) such that \(|\Omega|=2^{k}\) and \(k\) independent events \(A_{1},A_{2},\ldots,A_{k}\) in \(\mathcal{F}\) such that \(0<P(A_{i})<1\), \(i=1,2,\ldots,k\).
4. Let \(\Omega\equiv\{(x_{1},x_{2}):x_{1},x_{2}\in\mathbb{R},x_{1}^{2}+x_{2}^{2}\leq 1\}\) be the unit disc in \(\mathbb{R}^{2}\). Let \(\mathcal{F}\equiv\mathcal{B}(\Omega)\), the Borel \(\sigma\)-algebra in \(\Omega\) and \(P=\) normalized Lebesgue measure, i.e., \(P(A)\equiv\frac{m(A)}{\pi}\), \(A\in\mathcal{F}\). For \(\omega=(x_{1},x_{2})\) let \[X_{1}(\omega)=x_{1},\ X_{2}(\omega)=x_{2},\] and \(\big{(}R(\omega),\theta(\omega)\big{)}\) be the polar representation of \(\omega\). Show that the random variables \(R\) and \(\theta\) are independent but \(X_{1}\) and \(X_{2}\) are not. 2. Formulate and establish an extension of the above to the unit sphere in \(\mathbb{R}^{3}\).
5. Let \(X_{1},X_{2},X_{3}\) be iid random variables such that \(P(X_{1}=x)=0\) for all \(x\in R\). 1. Show that for any permutation \(\sigma\) of (1,2,3) \[P\Big{(}X_{\sigma(1)}>X_{\sigma(2)}>X_{\sigma(3)}\Big{)}=\frac{1}{3!}.\] 2. Show that for any \(i=1,2,3\) \[P\Big{(}X_{i}=\max_{1\leq j\leq 3}X_{j}\Big{)}=\frac{1}{3}.\] 3. State and prove a generalization of (a) and (b) to random variables \(\{X_{i}:1\leq i\leq n\}\) such that the joint distribution of \((X_{1},X_{2},\ldots,X_{n})\) is the same as that of \((X_{\sigma(1)},X_{\sigma(2)},\ldots,X_{\sigma(n)})\) for any permutation \(\sigma\) of \(\{1,2,\ldots,n\}\) and \(P(X_{1}=x)=0\) for all \(x\in\mathbb{R}\).
6. Let \(f\), \(g:\mathbb{R}\rightarrow\mathbb{R}\) be monotone nondecreasing. Show that for any random variable \(X\) \[Ef(X)g(X)\geq Ef(X)Eg(X)\] provided all the expectations exist. (**Hint:** Let \(Y\) be independent of \(X\) with same distribution. Note that \(Z=\big{(}f(X)-f(Y)\big{)}\big{(}g(X)-g(Y)\big{)}\geq 0\) w.p. 1.)
7. Let \(X_{1},X_{2},\ldots,X_{n}\) be random variables on some probability space \((\Omega,\mathcal{F},P)\). Show that if \(P(X_{1},X_{2},\ldots,X_{n})^{-1}(\cdot)\ll m_{n}\), the Lebesgue measure on \(\mathbb{R}^{n}\) then for each \(i\), \(PX_{i}^{-1}(\cdot)\ll m\). Give an example to show that the converse is not true. Show also that if \(P(X_{1},X_{2},\ldots,X_{n})^{-1}(\cdot)\ll m_{n}\) then \(\{X_{1},X_{2},\ldots,X_{n}\}\) are independent iff \(f_{(X_{1},X_{2},\ldots,X_{n})}(x_{1},x_{2},\ldots,x_{n})=\prod_{i=1}^{n}f_{X_{i} }(x_{i})\) where the \(f\)'s are the respective pdfs.

Laws of Large Numbers

When measuring a physical quantity such as the mass of an object, it is commonly believed that the average of several measurements is more reliable than a single one. Similarly, in applications of statistical inference when estimating a population mean \(\mu\), a random sample \(\{X_{1},X_{2},\ldots,X_{n}\}\) of size \(n\) is drawn from the population, and the sample average \(\bar{X}_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}X_{i}\) is used as an estimator for the parameter \(\mu\). This is based on the idea that as \(n\) gets large, \(\bar{X}_{n}\) will be close to \(\mu\) in some suitable sense. In many time-evolving physical systems \(\{f(t):0\leq t<\infty\}\), where \(f(t)\) is an element in the phase space \(\mathbb{S}\), "time averages" of the form \(\frac{1}{T}\int_{0}^{T}h(f(t))dt\) (where \(h\) is a bounded function on \(\mathbb{S}\)) converge, as \(T\) gets large, to the "space average" of the form \(\int_{\mathbb{S}}h(x)\pi(dx)\) for some appropriate measure \(\pi\) on \(\mathbb{S}\). The above three are examples of a general phenomenon known as the _law of large numbers_. This chapter is devoted to a systematic development of this topic for sequences of independent random variables and also to some important refinements of the law of large numbers.

### 8.1 Weak laws of large numbers

Let \(\{Z_{n}\}_{n\geq 1}\) be a sequence of random variables on a probability space \((\Omega,\mathcal{F},P)\). Recall that the sequence \(\{Z_{n}\}_{n\geq 1}\) is said to _converge in probability_ to a random variable \(Z\) if for each \(\epsilon>0\),

\[\lim_{n\to\infty}P(|Z_{n}-Z|\geq\epsilon)=0. \tag{1.1}\]This is written as \(Z_{n}\longrightarrow^{p}Z\). The sequence \(\{Z_{n}\}_{n\geq 1}\) is said to _converge with probability one_ or _almost surely_ (a.s.) to \(Z\) if there exists a set \(A\) in \({\cal F}\) such that

\[P(A)=1\ \ {\rm and\ for\ all}\ \ \omega\ \ {\rm in}\ \ A,\ \ \lim_{n\to\infty}Z_{n}( \omega)=Z(\omega).\]

This is written as \(Z_{n}\to Z\) w.p. 1 or \(Z_{n}\to Z\) a.s.

**Definition 8.1.1:** A sequence \(\{X_{n}\}_{n\geq 1}\) of random variables on a probability space \((\Omega,{\cal F},P)\) is said to obey the _weak law of large numbers_ (WLLN) with normalizing sequences of real numbers \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) if

\[{S_{n}-a_{n}\over b_{n}}\longrightarrow^{p}0\ \ \ {\rm as}\ \ \ n\to\infty\]

where \(S_{n}=\sum_{i=1}^{n}X_{i}\) for \(n\geq 1\).

The following theorem says that if \(\{X_{n}\}_{n\geq 1}\) is a sequence of iid random variables with \(EX_{1}^{2}<\infty\), then it obeys the weak law of large numbers with \(a_{n}=nEX_{1}\) and \(b_{n}=n\).

**Theorem 8.1.1:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables such that \(EX_{1}^{2}<\infty\). Then_

\[\bar{X}_{n}\equiv{X_{1}+\ldots+X_{n}\over n}\longrightarrow^{p}EX_{1}.\]

**Proof:** By Chebychev's inequality, for any \(\epsilon>0\),

\[P(|\bar{X}_{n}-EX_{1}|>\epsilon)\leq{{\rm Var}(\bar{X}_{n})\over\epsilon^{2}}= {1\over\epsilon^{2}}\cdot{\sigma^{2}\over n},\]

where \(\sigma^{2}={\rm Var}(X_{1})\). Since \({\sigma^{2}\over n\epsilon^{2}}\to 0\) as \(n\to\infty\), (1.4) follows. \(\Box\)

**Corollary 8.1.2:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid Bernoulli \((p)\) random variables, i.e., \(P(X_{1}=1)=p=1-P(X_{1}=0)\). Let_

\[\hat{p}_{n}={\#\{i:1\leq i\leq n,X_{i}=1\}\over n},\ \ n\geq 1,\]

_where for a finite set \(A\), \(\#A\) denotes the number of elements in \(A\). Then \(\hat{p}_{n}\longrightarrow^{p}p\)._

**Proof:** Check that \(EX_{1}=p\) and \(\hat{p}_{n}=\bar{X}_{n}\). \(\Box\)

This says that one can estimate the probability \(p\) of getting a "head" of a coin by tossing it \(n\) times and calculating the proportion of "heads." This is also the basis of public opinion polls. Since the proof of Theorem 8.1.1 depended only on Chebychev's inequality, the following generalization is immediate (Problem 8.1).

**Theorem 8.1.3:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables on a probability space such that_

* \(EX_{n}^{2}<\infty\quad\mbox{for all}\quad n\geq 1\)_,_
* \(EX_{i}X_{j}=(EX_{i})(EX_{j})\quad\mbox{for all}\quad i\neq j\) _(i.e.,_ \(\{X_{n}\}_{n\geq 1}\) _are uncorrelated),_
* \(\frac{1}{n^{2}}\sum_{i=1}^{n}\sigma_{i}^{2}\to 0\) _as_ \(n\to\infty\)_, where_ \(\sigma_{i}^{2}=\mbox{Var}(X_{i})\)_,_ \(i\geq 1\)_._

_Then_

\[\bar{X}_{n}-\bar{\mu}_{n}\longrightarrow^{p}0 \tag{1.7}\]

_where \(\bar{\mu}_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}EX_{i}\)._

**Corollary 8.1.4:** _Let \(\{X_{n}\}_{n\geq 1}\) satisfy (i) and (ii) of the above theorem and let the sequence \(\{\sigma_{n}^{2}\}_{n\geq 1}\) be bounded. Let \(\bar{\mu}_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}EX_{i}\to\mu\) as \(n\to\infty\). Then \(\bar{X}_{n}\longrightarrow^{p}\mu\)._

**An Application to Real Analysis**

Let \(f:[0,1]\to\mathbb{R}\) be a continuous function. K. Weierstrass showed that \(f\) can be approximated uniformly over \([0,1]\) by polynomials. S.N. Bernstein constructed a special class of such polynomials. A proof of Bernstein's result using the WLLN (Theorem 8.1.1) is given below.

**Theorem 8.1.5:** _Let \(f:[0,1]\to\mathbb{R}\) be a continuous function. Let_

\[B_{n,f}(x)\equiv\sum_{r=0}^{n}f\Big{(}\frac{r}{n}\Big{)}\binom{n}{r}x^{r}(1-x) ^{n-r},\ \ 0\leq x\leq 1 \tag{1.8}\]

_be the Bernstein polynomial of order n for the function \(f\). Then_

\[\lim_{n\to\infty}\ \sup\Big{\{}|f(x)-B_{n,f}(x)|:0\leq x\leq 1\Big{\}}=0.\]

**Proof:** Since \(f\) is continuous on the closed and bounded interval \([0,1]\), it is uniformly continuous and hence for any \(\epsilon>0\), there exists a \(\delta_{\epsilon}>0\) such that

\[|x-y|<\delta_{\epsilon}\Rightarrow|f(x)-f(y)|<\epsilon. \tag{1.9}\]

Fix \(x\) in \([0,1]\). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid Bernoulli \((x)\) random variables. Let \(\hat{p}_{n}\) be as in (1.6). Then \(B_{n,f}(x)=Ef(\hat{p}_{n})\). Hence,

\[|f(x)-B_{n,f}(x)| \leq E|f(\hat{p}_{n})-f(x)|\] \[= E\Big{\{}|f(\hat{p}_{n})-f(x)|I(|\hat{p}_{n}-x|<\delta_{\epsilon })\Big{\}}\] \[\ \ \ +E\Big{\{}|f(\hat{p}_{n})-f(x)|I(|\hat{p}_{n}-x|\geq\delta_{ \epsilon})\Big{\}}\] \[\leq \epsilon+2\|f\|P(|\hat{p}_{n}-x|\geq\delta_{\epsilon})\]where \(\|f\|=\sup\{|f(x)|:0\leq x\leq 1\}\). But by Chebychev's inequality,

\[P(|\hat{p}_{n}-x|\geq\delta_{\epsilon}) \leq \frac{1}{\delta_{\epsilon}^{2}}\mbox{Var}(\hat{p}_{n})\] \[= \frac{x(1-x)}{n\delta_{\epsilon}^{2}}\leq\frac{1}{4n\delta_{ \epsilon}^{2}}\quad\mbox{for all}\quad 0\leq x\leq 1.\]

Thus, \(\sup\left\{|f(x)-B_{n,f}(x)|:0\leq x\leq 1\right\}\leq\epsilon+2\|f\|\frac{1}{4n \delta_{\epsilon}^{2}}\). Letting \(n\to\infty\) first and then \(\epsilon\downarrow 0\) completes the proof. \(\Box\)

### 8.2 Strong laws of large numbers

**Definition 8.2.1:** A sequence \(\{X_{n}\}_{n\geq 1}\) of random variables on a probability space \((\Omega,\mathcal{F},P)\) is said to obey the _strong law of large numbers_ (SLLN) with normalizing sequences of real numbers \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) if

\[\frac{S_{n}-a_{n}}{b_{n}}\to 0\quad\mbox{as}\quad n\to\infty\quad\mbox{w.p. 1}, \tag{2.1}\]

where \(S_{n}=\sum_{i=1}^{n}X_{i}\) for \(n\geq 1\).

The following theorem says that if \(\{X_{n}\}_{n\geq 1}\) is a sequence of iid random variables with \(EX_{1}^{4}<\infty\), then the strong law of large numbers holds with \(a_{n}=nEX_{1}\) and \(b_{n}=n\). This result is referred to as Borel's SLLN.

**Theorem 8.2.1:** (_Borel's SLLN_). _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables such that \(EX_{1}^{4}<\infty\). Then_

\[\bar{X}_{n}\equiv\frac{X_{1}+X_{2}+\ldots+X_{n}}{n}\to EX_{1}\quad\mbox{w.p. 1}. \tag{2.2}\]

**Proof:** Fix \(\epsilon>0\) and let \(A_{n}\equiv\{|\bar{X}_{n}-EX_{1}|\geq\epsilon\}\), \(n\geq 1\). To establish (2.2), by Proposition 7.2.3 (a), it suffices to show that

\[\sum_{n=1}^{\infty}P(A_{n})<\infty. \tag{2.3}\]

By Markov's inequality

\[P(A_{n})\leq\frac{E|\bar{X}_{n}-EX_{1}|^{4}}{\epsilon^{4}}. \tag{2.4}\]

Let \(Y_{i}=X_{i}-EX_{1}\) for \(i\geq 1\). Since the \(X_{i}\)'s are independent, it is easy to check that

\[E|\bar{X}_{n}-EX_{1}|^{4} = \frac{1}{n^{4}}E\Bigg{(}\bigg{(}\sum_{i=1}^{n}Y_{i}\bigg{)}^{4} \Bigg{)}\]\[= \frac{1}{n^{4}}\Big{(}nEY_{1}^{4}+3n(n-1)(EY_{1}^{2})^{2}\Big{)}\] \[= O(n^{-2}).\]

By (2.4) this implies (2.3). \(\Box\)

The following two results are easy consequences of the above theorem.

Corollary 8.2.2: Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables that are bounded, i.e., there exists a \(C<\infty\) such that \(P(|X_{1}|\leq C)=1\). Then

\[\bar{X}_{n}\to EX_{1}\quad\text{w.p. 1}.\]

Corollary 8.2.3: Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid Bernoulli\((p)\) random variables. Then

\[\hat{p}_{n}\equiv\frac{\#\{i:1\leq i\leq n,X_{i}=1\}}{n}\to p\quad\text{w.p. 1}.\]

An application of the above result yields the following theorem on the uniform convergence of the empirical cdf to the true cdf.

Theorem 8.2.4: \((\)_Glivenko-Cantelli\()\). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with a common cdf \(F(\cdot)\). Let \(F_{n}(\cdot)\), the empirical cdf based on \(\{X_{1},X_{2},\ldots,X_{n}\}\), be defined by_

\[F_{n}(x)\equiv\frac{1}{n}\sum_{j=1}^{n}I(X_{j}\leq x),\quad x\in\mathbb{R}.\]

_Then,_

\[\bar{\Delta}_{n}\equiv\sup_{x}|F_{n}(x)-F(x)|\to 0\quad\text{w.p. 1}.\]

Remark 8.2.1: Note that by applying Corollary 8.2.3 to the sequence of Bernoulli random variables \(\{Y_{n}\equiv I(X_{n}\leq x)\}_{n\geq 1}\), one may conclude that \(F_{n}(x)\to F(x)\) w.p. 1 for each _fixed_\(x\). So the main thrust of this theorem is the _uniform_ convergence on \(\mathbb{R}\) of \(F_{n}\) to \(F\) w.p. 1. It can be shown that (2.7) holds for sequences \(\{X_{n}\}_{n\geq 1}\) that are identically distributed and only pairwise independent. The proof is based on Etemadi's SLLN (Theorem 8.2.7) below.

The proof of Theorem 8.2.4 makes use of the following two lemmas.

Lemma 8.2.5: \((\)_Scheffe's theorem: A generalized version\()\). Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(\{f_{n}\}_{n\geq 1}\) and \(f\) be nonnegative \(\mu\)-integrable functions such that as \(n\to\infty\), (i) \(f_{n}\to f\) a.e. \((\mu)\) and (ii) \(\int f_{n}d\mu\to\int fd\mu\). Then \(\int|f-f_{n}|d\mu\to 0\) as \(n\to\infty\)._

**Proof:** See Theorem 2.5.4. \(\Box\)

For any bounded monotone function \(H\): \(\mathbb{R}\to\mathbb{R}\), define

\[H(\infty)\equiv\lim_{x\uparrow\infty}H(x),\quad H(-\infty)\equiv\lim_{x\downarrow -\infty}H(x).\]

**Lemma 8.2.6:** (_Polya's theorem_). _Let \(\{G_{n}\}_{n\geq 1}\) and \(G\) be a collection of bounded nondecreasing functions on \(\mathbb{R}\to\mathbb{R}\) such that \(G(\cdot)\) is continuous on \(\mathbb{R}\) and_

\[G_{n}(x)\to G(x)\mbox{ for all }x\mbox{ in }D\cup\{-\infty,+\infty\},\]

_where \(D\) is dense in \(\mathbb{R}.\) Then \(\Delta_{n}\equiv\sup\{|G_{n}(x)-G(x)|:x\in\mathbb{R}\}\to 0.\) That is, \(G_{n}\to G\) uniformly on \(\mathbb{R}.\)_

**Proof:** Fix \(\epsilon>0.\) By the definitions of \(G(\infty)\) and \(G(-\infty)\), there exist \(C_{1}\) and \(C_{2}\) in \(D\) such that

\[G(C_{1})-G(-\infty)<\epsilon,\quad\mbox{and}\quad G(\infty)-G(C_{2})<\epsilon. \tag{2.8}\]

Since \(G(\cdot)\) is continuous, it is uniformly continuous on \([C_{1},C_{2}]\) and so there exists a \(\delta>0\) such that

\[x,y\in[C_{1},C_{2}],|x-y|<\delta\Rightarrow|G(x)-G(y)|<\epsilon. \tag{2.9}\]

Also, there exist points \(a_{1}=C_{1}<a_{2}<\ldots<a_{k}=C_{2}\), \(1<k<\infty\), in \(D\) such that

\[\max\{(a_{i+1}-a_{i}):1\leq i\leq k-1\}<\delta.\]

Let \(a_{0}=-\infty\), \(a_{k+1}=\infty.\) By the convergence of \(G_{n}(\cdot)\) to \(G(\cdot)\), on \(D\cup\{-\infty,\infty\},\)

\[\Delta_{n1} \equiv \max\{|G_{n}(a_{i})-G(a_{i})|:0\leq i\leq k+1\}\to 0 \tag{2.10}\]

as \(n\to\infty.\) Now note that for any \(x\) in \([a_{i},a_{i+1}]\), \(1\leq i\leq k-1\), by the monotonicity of \(G_{n}(\cdot)\) and \(G(\cdot)\), and by (2.9) and (2.10),

\[G_{n}(x)-G(x) \leq G_{n}(a_{i+1})-G(a_{i})\] \[\leq G_{n}(a_{i+1})-G(a_{i+1})+G(a_{i+1})-G(a_{i})\] \[\leq \Delta_{n1}+\epsilon,\]

and similarly,

\[G_{n}(x)-G(x)\geq-\Delta_{n1}-\epsilon.\]

Thus

\[\sup\{|G_{n}(x)-G(x)|:a_{1}\leq x\leq a_{k}\}\leq\Delta_{n1}+\epsilon. \tag{2.11}\]For \(x<a_{1}\), by (2.8) and (2.10),

\[|G_{n}(x)-G(x)| \leq |G_{n}(x)-G_{n}(-\infty)|+|G_{n}(-\infty)-G(-\infty)|\] \[+\,|G(-\infty)-G(x)|\] \[\leq (G_{n}(a_{1})-G_{n}(-\infty))+|G_{n}(-\infty)-G(-\infty)|+\epsilon\] \[\leq |G_{n}(a_{1})-G(a_{1})|+|G(a_{1})-G(-\infty)|\] \[+\,2|G_{n}(-\infty)-G(-\infty)|+\epsilon\] \[\leq 3\Delta_{n1}+2\epsilon.\]

Similarly, for \(x>a_{k}\),

\[|G_{n}(x)-G(x)|\leq 3\Delta_{n1}+2\epsilon.\]

Combining the above with (2.11) yields

\[\Delta_{n}\leq 3\Delta_{n1}+2\epsilon.\]

By (2.10),

\[\limsup_{n\to\infty}\ \Delta_{n}\leq 2\epsilon,\]

and \(\epsilon>0\) being arbitrary, the proof is complete. \(\Box\)

**Proof of Theorem 8.2.4:** First note that \(\tilde{\Delta}_{n}=\sup_{x\in{\mathbb{Q}}}|F_{n}(x)-F(x)|\) and hence, it is a random variable. Let \(B\equiv\{b_{j}:j\in J\}\) be the set of jump discontinuity points of \(F\) with the corresponding jump sizes \(\{p_{j}:j\in J\}\), where \(J\) is a subset of \({\mathbb{N}}\). Let \(p=\sum_{j\in J}p_{j}\).

Note that

\[F_{n}(x) = \frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x) \tag{2.12}\] \[= \frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x,X_{i}\in B)+\frac{1}{n} \sum_{i=1}^{n}I(X_{i}\leq x,X_{i}\not\in B)\] \[= F_{nd}(x)+F_{nc}(x),\ \mbox{say}.\]

Then, \(F_{nd}(x)=\sum_{j\in J}\hat{p}_{nj}I(b_{j}\leq x)\), where

\[\hat{p}_{nj}=\frac{\#\{i:1\leq i\leq n,X_{i}=b_{j}\}}{n}.\]

Let \(\hat{p}_{n}=\sum_{j\in J}\hat{p}_{nj}=\frac{1}{n}\cdot\#\{i:1\leq i\leq n,X_{i }\in B\}\). By Corollary 8.2.3, for each \(j\in J\),

\[\hat{p}_{nj}\to p_{j}\quad\mbox{w.p. }1\ \ \mbox{and}\ \ \hat{p}_{n}\to p\quad\mbox{w.p. }1.\]

Since \(B\) is countable, there exists a set \(A_{0}\) in \({\cal F}\) such that \(P(A_{0})=1\) and for all \(\omega\) in \(A_{0}\), \(\hat{p}_{nj}\to p_{j}\) for all \(j\in J\) and \(\sum_{j\in J}\hat{p}_{nj}=\hat{p}_{n}\to p=\sum_{j\in J}p_{j}\).

By Lemma 8.2.5 (applied with \(\mu\) being the counting measure on the set \(J\)), it follows that for \(\omega\) in \(A_{0}\),

\[\sum_{j\in J}|\hat{p}_{nj}-p_{j}|\to 0. \tag{2.13}\]

Let \(F_{d}(x)\equiv\sum_{j\in J}p_{j}I(b_{j}\leq x),\ x\in\mathbb{R}.\) Then,

\[\sup_{x\in\mathbb{R}}|F_{nd}(x)-F_{d}(x)|\leq\sum_{j\in J}|\hat{p}_{nj}-p_{j}|, \tag{2.14}\]

which \(\to 0\) as \(n\to\infty\) for all \(\omega\) in \(A_{0}\), by (2.13).

Next let,

\[F_{c}(x)\equiv F(x)-F_{d}(x),\ \ x\in\mathbb{R}.\]

Then, it is easy to check that, \(F_{c}(\cdot)\) is continuous and nondecreasing on \(\mathbb{R}\), \(F_{c}(-\infty)=0\) and \(F_{c}(\infty)=1-p\).

Again, by Corollary 8.2.3, there exists a set \(A_{1}\) in \(\mathcal{F}\) such that \(P(A_{1})=1\) and for all \(\omega\) in \(A_{1}\),

\[F_{nc}(x)\to F_{c}(x)\]

for all rational \(x\) in \(\mathbb{R}\) and

\[F_{nc}(\infty)\equiv 1-\hat{p}_{n}\to 1-p=F_{c}(\infty).\]

Also, \(F_{nc}(-\infty)=0=F_{c}(-\infty).\) So by Lemma 8.2.6, with \(D=\mathbb{Q}\), for \(\omega\) in \(A_{1}\),

\[\sup_{x\in\mathbb{R}}|F_{nc}(x)-F_{c}(x)|\to 0\ \ \ {\rm as}\ \ \ n\to\infty. \tag{2.15}\]

Since \(P(A_{0}\cap A_{1})=1\), the theorem follows from (2.12)-(2.15). \(\Box\)

Borel's SLLN for iid random variables requires that \(E|X_{1}|^{4}<\infty.\) Kolmogorov (1956) improved on this significantly by using his "3-series" theorem and reduced the moment condition to \(E|X_{1}|<\infty.\) More recently, Etemadi (1981) N. improved this further by assuming only that the \(\{X_{n}\}_{n\geq 1}\) are _pairwise independent_ and identically distributed with \(E|X_{1}|<\infty.\) More precisely, he proved the following.

**Theorem 8.2.7:**_(Etemadi's SLLN). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of pairwise independent and identically distributed random variables with \(E|X_{1}|<\infty.\) Then_

\[\bar{X}_{n}\to EX_{1}\ \ {\rm w.p.\ 1}. \tag{2.16}\]

**Proof:** The main steps in the proof are

* reduction to the nonnegative case,* proof of convergence of \(\bar{Y}_{n}\) along a geometrically growing subsequence using the Borel-Cantelli lemma and Chebychev's inequality, where \(\bar{Y}_{n}\) is the average of certain truncated versions of \(X_{1},\ldots,X_{n}\), and extending the convergence from the geometric subsequence to the full sequence.

Step I: Since the \(\{X_{n}\}_{n\geq 1}\) are pairwise independent and identically distributed with \(E|X_{1}|<\infty\), it follows that \(\{X_{n}^{+}\}_{n\geq 1}\) and \(\{X_{n}^{-}\}_{n\geq 1}\) are both sequences of pairwise independent and identically distributed nonnegative random variables with \(EX_{1}^{+}<\infty\) and \(EX_{1}^{-}<\infty\). Also, since

\[\bar{X}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}=\bigg{(}\frac{1}{n}\sum_{i=1}^{n}X_ {i}^{+}\bigg{)}-\bigg{(}\frac{1}{n}\sum_{i=1}^{n}X_{i}^{-}\bigg{)},\]

it is enough to prove the theorem under the assumption that the \(X_{i}\)'s are nonnegative.

Step II: Now let \(X_{i}\)'s be nonnegative and let

\[Y_{i}=X_{i}I(X_{i}\leq i),\ \ i\geq 1.\]

Then,

\[\sum_{i=1}^{\infty}P(X_{i}\neq Y_{i}) = \sum_{i=1}^{\infty}P(X_{i}>i)\] \[= \sum_{i=1}^{\infty}P(X_{1}>i)\leq\sum_{i=1}^{\infty}\int_{i-1}^{i }P(X_{1}>t)dt\] \[= \int_{0}^{\infty}P(X_{1}>t)dt\] \[= EX_{1}<\infty.\]

Hence, by the Borel-Cantelli lemma,

\[P(X_{i}\neq Y_{i},\ \mbox{infinitely often})=0.\]

This implies that w.p. 1, \(X_{i}=Y_{i}\) for all but finitely many \(i\)'s and hence, it suffices to show that

\[\bar{Y}_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}Y_{i}\to EX_{1}\ \ \mbox{w.p. 1}. \tag{2.17}\]

Next, \(EY_{i}=E(X_{i}I(X_{i}\leq i)=E(X_{1}I(X_{1}\leq i))\to EX_{1}\) (by the MCT) and hence

\[E\bar{Y}_{n}=\frac{1}{n}\sum_{i=1}^{n}EY_{i}\to EX_{1}\ \ \mbox{ as}\ \ \ n\to\infty. \tag{2.18}\]Suppose for the moment that for each fixed \(1<\rho<\infty\), it is shown that

\[\bar{Y}_{n_{k}}\to EX_{1}\mbox{ as }k\to\infty\ \ \mbox{w.p. }1 \tag{2.19}\]

where \(n_{k}=\lfloor\rho^{k}\rfloor=\) the greatest integer less than or equal to \(\rho^{k}\), \(k\in\mathbb{N}\). Then, since the \(Y_{i}\)'s are nonnegative, for any \(n\) and \(k\) satisfying \(\rho^{k}\leq n<\rho^{k+1}\), one gets

\[\frac{1}{n}\sum_{i=1}^{n_{k}}Y_{i}\leq\bar{Y}_{n}=\frac{1}{n}\sum _{j=1}^{n}Y_{j}\leq\frac{1}{n}\sum_{i=1}^{n_{k+1}}Y_{i}\] \[\Longrightarrow \frac{n_{k}}{n}\bar{Y}_{n_{k}}\leq\bar{Y}_{n}\leq\frac{n_{k+1}}{n }\bar{Y}_{n_{k+1}}\] \[\Longrightarrow \frac{1}{\rho}\,\bar{Y}_{n_{k}}\leq\bar{Y}_{n}\leq\rho\bar{Y}_{n_ {k+1}}.\]

From (2.19), it follows that

\[\frac{1}{\rho}\,EX_{1}\leq\liminf_{n\to\infty}\bar{Y}_{n}\leq\limsup_{n\to \infty}\bar{Y}_{n}\leq\rho EX\ \ \mbox{w.p. }1.\]

Since this is true for each \(1<\rho<\infty\), by taking \(\rho=1+\frac{1}{r}\) for \(r=1,2,\ldots\), it follows that

\[EX_{1}\leq\liminf_{n\to\infty}\bar{Y}_{n}\leq\limsup_{n\to\infty}\bar{Y}_{n} \leq EX_{1}\ \ \mbox{w.p. }1,\]

establishing (2.17).

It now remains to prove (2.19). By (2.18), it is enough to show that

\[\bar{Y}_{n_{k}}-E\bar{Y}_{n_{k}}\to 0\ \ \mbox{ as }\ \ k\to\infty,\ \ \mbox{w.p. }1. \tag{2.20}\]

By Chebychev's inequality and the pairwise independence of the variables \(\{Y_{n}\}_{n\geq 1}\), for any \(\epsilon>0\),

\[P(|\bar{Y}_{n_{k}}-E\bar{Y}_{n_{k}}|>\epsilon) \leq \frac{1}{\epsilon^{2}}\,\mbox{Var}(\bar{Y}_{n_{k}})=\frac{1}{ \epsilon^{2}}\frac{1}{n_{k}^{2}}\sum_{i=1}^{n_{k}}\mbox{Var}(Y_{i})\] \[\leq \frac{1}{\epsilon^{2}}\,\frac{1}{n_{k}^{2}}\sum_{i=1}^{n_{k}}EY_{ i}^{2}.\]

Thus,

\[\sum_{k=1}^{\infty}P(|\bar{Y}_{n_{k}}-E\bar{Y}_{n_{k}}|>\epsilon) \leq \frac{1}{\epsilon^{2}}\sum_{k=1}^{\infty}\frac{1}{n_{k}^{2}}\sum_ {i=1}^{n_{k}}EY_{i}^{2} \tag{2.21}\] \[= \frac{1}{\epsilon^{2}}\sum_{i=1}^{\infty}EY_{i}^{2}\bigg{(}\sum_{ k:n_{k}\geq i}\frac{1}{n_{k}^{2}}\bigg{)}.\]Since \(n_{k}=\lfloor\rho^{k}\rfloor>\rho^{k-1}\) for \(1<\rho<\infty\),

\[\sum_{k:n_{k}\geq i}\frac{1}{n_{k}^{2}}\leq\sum_{k:\rho^{k-1}\geq i}\frac{1}{ \rho^{(k-1)2}}\leq\frac{C_{1}}{i^{2}} \tag{2.22}\]

for some constant \(C_{1}\), \(0<C_{1}<\infty\).

Next, since the \(X_{i}\)'s are identically distributed,

\[\sum_{i=1}^{\infty}\frac{EY_{i}^{2}}{i^{2}} = \sum_{i=1}^{\infty}\frac{EX_{1}^{2}I(0\leq X_{1}\leq i)}{i^{2}} \tag{2.23}\] \[= \sum_{i=1}^{\infty}\sum_{j=1}^{i}\frac{EX_{1}^{2}I(j-1<X_{1}\leq j )}{i^{2}}\] \[= \sum_{j=1}^{\infty}\big{(}EX_{1}^{2}I(j-1<X_{1}\leq j)\big{)} \sum_{i=j}^{\infty}i^{-2}\] \[\leq \sum_{j=1}^{\infty}\big{(}jEX_{1}I(j-1<X_{1}\leq j)\big{)}\cdot C _{2}j^{-1}\] \[= C_{2}EX_{1}<\infty,\]

for some constant \(C_{2}\), \(0<C_{2}<\infty\).

Now (2.21)-(2.23) imply that

\[\sum_{k=1}^{\infty}P(|\bar{Y}_{n_{k}}-E\bar{Y}_{n_{k}}|>\epsilon)<\infty\]

for each \(\epsilon>0\). By the Borel-Cantelli lemma and Proposition 7.2.3 (a), (2.20) follows and the proof is complete. \(\Box\)

The following corollary is immediate from the above theorem.

**Corollary 8.2.8:** (_Extension to the vector case_)_. Let \(\{X_{n}=(X_{n1},\ldots,X_{nk})\}_{n\geq 1}\) be a sequence of \(k\)-dimensional random vectors defined on a probability space \((\Omega,\mathcal{F},P)\) such that for each \(i\), \(1\leq i\leq k\), the sequence \(\{X_{ni}\}_{n\geq 1}\) are pairwise independent and identically distributed with \(E|X_{1i}|<\infty\). Let \(\mu=(EX_{11},EX_{12},\ldots,EX_{1k})\) and \(f:\mathbb{R}^{k}\rightarrow\mathbb{R}\) be continuous at \(\mu\). Then_

* \(\bar{X}_{n}\equiv(\bar{X}_{n1},\bar{X}_{n2},\ldots,\bar{X}_{nk})\rightarrow\mu\) _w.p. 1, where_ \(\bar{X}_{ni}=\frac{1}{n}\sum_{j=1}^{n}X_{ji}\) _for_ \(1\leq i\leq k\)_._
* \(f(\bar{X}_{n})\to f(\mu)\) _w.p. 1._

**Example 8.2.1:** Let \((X_{n},Y_{n})\), \(n=1,2,\ldots\) be a sequence of bivariate iid random vectors with \(EX_{1}^{2}<\infty\), \(EY_{1}^{2}<\infty\). Then the _sample correlation coefficient \(\hat{\rho}_{n}\)_, defined by,

\[\hat{\rho}_{n}\equiv\frac{\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}Y_{i}-\bar{X}_{n} \bar{Y}_{n}\right)}{\sqrt{\left(\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2 }\right)\left(\frac{1}{n}\sum_{i=1}^{n}(Y_{i}-\bar{Y}_{n})^{2}\right)}}\]

is a _strongly consistent_ estimator of the _population correlation coefficient_\(\rho\), defined by,

\[\rho=\frac{\mathrm{Cov}(X_{1},Y_{1})}{\sqrt{\mathrm{Var}(X_{1})\mathrm{Var}(Y_ {1})}},\]

i.e., \(\hat{\rho}_{n}\rightarrow\rho\) w.p. 1. This follows from the above corollary by taking \(f:\mathbb{R}^{5}\rightarrow\mathbb{R}\) to be

\[f(t_{1},t_{2},t_{3},t_{4},t_{5})=\left\{\begin{array}{cl}\frac{t_{5}-t_{1}t _{2}}{\sqrt{(t_{3}-t_{1}^{2})(t_{4}-t_{2}^{2})}}\,&\mathrm{for}\ \ t_{3}>t_{1}^{2},\ t_{4}>t_{2}^{2}\\ 0,&\mathrm{otherwise},\end{array}\right.\]

and the vector \((X_{n1},X_{n2},\ldots,X_{n5})\) to be

\[X_{n1}=X_{n},\ X_{n2}=Y_{n},\ X_{n3}=X_{n}^{2},\ X_{n4}=Y_{n}^{2},\ X_{n5}=X_{n }Y_{n}.\]

**Corollary 8.2.9:** (_Extension to the pairwise \(m\)-dependent case_). _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables on a probability space \((\Omega,\mathcal{F},P)\) such that for an integer \(m\), \(1\leq m<\infty\), and for each \(i\), \(1\leq i\leq m\), the random variables \(\{X_{i},X_{i+m},X_{i+2m},\ldots\}\) are identically distributed and pairwise independent with \(E|X_{i}|<\infty\). Then_

\[\bar{X}_{n}\rightarrow\frac{1}{m}\sum_{i=1}^{m}EX_{i}\ \ \mbox{w.p. }1.\]

The proof is left as an exercise (Problem 8.2). For an application of the above result to a discussion on _normal numbers_, see Problem 8.15.

**Example 8.2.2:** (_IID Monte Carlo_).  Let \((\mathbb{S},\mathcal{S},\pi)\) be a probability space, \(f\in L^{1}(\mathbb{S},\mathcal{S},\pi)\) and \(\lambda=\int_{\mathbb{S}}fd\pi\). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid \(\mathbb{S}\)-valued random variables with distribution \(\pi\). Then, the _IID Monte Carlo approximation_ to \(\lambda\) is defined as

\[\hat{\lambda}_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}f(X_{i}).\]

Note that by the SLLN, \(\hat{\lambda}_{n}\rightarrow\lambda\) w.p. 1.

An extension of this to the case where \(\{X_{i}\}_{i\geq 1}\) is a Markov chain, known as Markov chain Monte Carlo (MCMC), is discussed in Chapter 14.

### Series of independent random variables

Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables on a probability space \((\Omega,\mathcal{F},P)\). The goal of this section is to investigate the convergence of the infinite series \(\sum_{n=1}^{\infty}X_{n}\), i.e., that of the partial sum sequence, \(S_{n}=\sum_{i=1}^{n}X_{i}\), \(n\geq 1\).

The main result of this section is Kolmogorov's 3-series theorem (Theorem 8.3.5). The following two inequalities play a fundamental role in the proof of this theorem and also have other important applications.

**Theorem 8.3.1:** _Let \(\{X_{j}:1\leq j\leq n\}\) be a collection of independent random variables. Let \(S_{i}=\sum_{j=1}^{i}X_{j}\) for \(1\leq i\leq n\)._

* \((\)_Kolmogorov's first inequality_\()\)_. Suppose that_ \(EX_{j}=0\)_v and_ \(EX_{j}^{2}<\infty\)_,_ \(1\leq j\leq n\)_. Then, for_ \(0<\lambda<\infty\)_,_ \[P\Big{(}\max_{1\leq i\leq n}|S_{i}|\geq\lambda\Big{)}\leq\frac{\mathrm{Var}(S_ {n})}{\lambda^{2}}.\] (3.1)
* \((\)_Kolmogorov's second inequality_\()\)_. Suppose that there exists a constant_ \(C\in(0,\infty)\) _such that_ \(P(|X_{j}-EX_{j}|\leq C)=1\) _for_ \(1\leq j\leq n\)_. Then, for any_ \(0<\lambda<\infty\)_,_ \[P\Big{(}\max_{1\leq i\leq n}|S_{i}|\leq\lambda\Big{)}\leq\frac{(2C+4\lambda)^{2 }}{\mathrm{Var}(S_{n})}.\]

**Proof:** Let \(A=\{\max_{1\leq i\leq n}|S_{i}|\geq\lambda\}\) and let

\[A_{1} = \{|S_{1}|\geq\lambda\},\] \[A_{j} = \{|S_{1}|<\lambda,|S_{2}|<\lambda,\ldots,|S_{j-1}|<\lambda,|S_{j} |\geq\lambda\}\]

for \(j=2,\ldots,n\). Then \(A_{1},\ldots,A_{n}\) are disjoint, \(\bigcup_{j=1}^{n}A_{j}=A\) and \(P(A)=\sum_{j=1}^{n}P(A_{j})\). Since \(EX_{j}=0\) for all \(j\),

\[\mathrm{Var}(S_{n})=ES_{n}^{2} \geq E(S_{n}^{2}I_{A})=\sum_{j=1}^{n}E(S_{n}^{2}I_{A_{j}}) \tag{3.2}\] \[= \sum_{j=1}^{n}E\Big{[}\Big{(}(S_{n}-S_{j})^{2}+S_{j}^{2}+2(S_{n}- S_{j})S_{j}\Big{)}I_{A_{j}}\Big{]}\] \[\geq \sum_{j=1}^{n}E(S_{j}^{2}I_{A_{j}})+2\sum_{j=1}^{n-1}E\Big{(}(S_{ n}-S_{j})S_{j}I_{A_{j}}\Big{)}.\]

Note that since \(\{X_{1},\ldots,X_{n}\}\) are independent, \((S_{n}-S_{j})\equiv\sum_{i=j+1}^{n}X_{i}\) and \(S_{j}I_{A_{j}}\) are independent for \(1\leq j\leq n-1\). Hence,

\[E\Big{(}(S_{n}-S_{j})S_{j}I_{A_{j}}\Big{)}=E(S_{n}-S_{j})E(S_{j}I_{A_{j}})=0.\]

[MISSING_PAGE_FAIL:263]

\[= \sum_{j=1}^{n}P(A_{j})P(B_{j})\] \[\geq \frac{1}{2}\,P\bigg{(}\bigcup_{j=1}^{n}B_{j}\bigg{)}\] \[= \frac{1}{2}\,P(B),\]

proving part (i). Now part (ii) follows by applying part (i) to both \(\{X_{i}\}_{i=1}^{n}\) and \(\{-X_{i}\}_{i=1}^{n}\). \(\Box\)

Recall that if \(\{Y_{n}\}_{n\geq 1}\) is a sequence of random variables, then \(\{Y_{n}\}_{n\geq 1}\) converges w.p. 1 implies that \(\{Y_{n}\}_{n\geq 1}\) converges in probability as well. A remarkable result of P. Levy is that if \(\{S_{n}\}_{n\geq 1}\) is the sequence of partial sums of _independent_ random variables and \(\{S_{n}\}_{n\geq 1}\) converges in probability, then \(\{S_{n}\}_{n\geq 1}\) must converge w.p. 1 as well. The proof of this uses Levy's inequality proved above.

**Theorem 8.3.3:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables. Let \(S_{n}=\sum_{j=1}^{n}X_{j}\) for \(1\leq n<\infty\) and let \(\{S_{n}\}_{n\geq 1}\) converge in probability to a random variable \(S\). Then \(S_{n}\to S\) w.p. 1._

**Proof:** Recall that a sequence \(\{x_{n}\}_{n\geq 1}\) of real numbers converges iff it is Cauchy iff \(\delta_{n}\equiv\sup\{|x_{k}-x_{\ell}|:k,\ell\geq n\}\to 0\) as \(n\to\infty\). Let

\[\tilde{\Delta}_{n} \equiv \sup\{|S_{k}-S_{\ell}|:k,\ell\geq n\}\mbox{ and }\] \[\Delta_{n} \equiv \sup\{|S_{k}-S_{n}|:k\geq n\}.\]

Then, \(\tilde{\Delta}_{n}\leq 2\Delta_{n}\) and \(\tilde{\Delta}_{n}\) is decreasing in \(n\). Suppose it is shown that

\[\Delta_{n}\longrightarrow^{p}0. \tag{3.4}\]

Then, \(\tilde{\Delta}_{n}\longrightarrow^{p}0\) and hence there is a subsequence \(\{n_{k}\}_{k\geq 1}\) such that \(\tilde{\Delta}_{n_{k}}\to 0\) as \(k\to\infty\) w.p. 1. Since \(\tilde{\Delta}_{n}\) is decreasing in \(n\), this implies that \(\tilde{\Delta}_{n}\to 0\) w.p. 1. Thus it suffices to establish (3.4). Fix \(0<\epsilon<1\). Let

\[S_{n,\ell} = S_{n+\ell}-S_{n}\quad\mbox{for}\quad\ell\geq 1,\] \[\Delta_{n,k} = \max\{|S_{n,\ell}|:1\leq\ell\leq k\},\ k\geq 1.\]

Note that for each \(n\geq 1\), \(\{\Delta_{n,k}\}_{k\geq 1}\) is a nondecreasing sequence, \(\lim_{k\to\infty}\Delta_{n,k}=\Delta_{n}\) and hence, for any \(n\geq 1\),

\[P(\Delta_{n}>\epsilon)=\lim_{k\to\infty}P(\Delta_{n,k}>\epsilon). \tag{3.5}\]

Levy's inequality (Theorem 8.3.2) will now be used to bound \(P(\Delta_{n,k}>\epsilon)\) uniformly in \(k\). Since \(S_{n}\longrightarrow^{p}S\), for any \(\eta>0\), there exists an \(n_{0}\geq 1\) such that for all \(n\geq n_{0}\),

\[P(|S_{n}-S|>\eta)<\eta.\]This implies that for all \(k\geq\ell\geq n_{0}\),

\[P(|S_{k}-S_{\ell}|>2\eta)<2\eta. \tag{3.6}\]

If \(0<\eta<\frac{1}{4}\), then the medians of \(S_{k}-S_{\ell}\) for \(k\geq\ell\geq n_{0}\) are bounded by \(2\eta\). Hence, for \(n\geq n_{0}\) and \(k\geq 1\), applying Levy's inequality (i.e., the above theorem) to \(\{X_{i}:n+1\leq i\leq n+k\}\),

\[P(\Delta_{n,k}>\epsilon) = P\Big{(}\max_{1\leq j\leq k}|S_{n,j}|>\epsilon\Big{)}\] \[\leq P\Big{(}\max_{1\leq j\leq k}|S_{n,j}-c_{n+j,n+k}|\geq\epsilon-2 \eta\Big{)}\] \[\leq 2P(|S_{n,k}|\geq\epsilon-2\eta).\]

Now, choosing \(0<\eta<\frac{\epsilon}{4}\), (3.6) yields \(P(\Delta_{n,k}>\epsilon)<4\eta<\epsilon\) for all \(n\geq n_{0}\), \(k\geq 1\). Then, by (3.5), \(P(\Delta_{n}>\epsilon)\leq\epsilon\) for all \(n\geq n_{0}\). Hence, (3.4) holds. \(\Box\)

The following result on convergence of infinite series of independent random variables is an immediate consequence of the above theorem.

**Theorem 8.3.4:** (_Khinchine-Kolmogorov's 1-series theorem_). _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables on a probability space \((\Omega,{\cal F},P)\) such that \(EX_{n}=0\) for all \(n\geq 1\) and \(\sum_{n=1}^{\infty}EX_{n}^{2}<\infty\). Then \(S_{n}\equiv\sum_{j=1}^{n}X_{j}\) converges in mean square and almost surely, as \(n\to\infty\)._

**Proof:** For any \(n\), \(k\in\mathbb{N}\),

\[E(S_{n}-S_{n+k})^{2}=\mbox{Var}(S_{n}-S_{n+k})=\sum_{j=n+1}^{n+k}\mbox{Var}(X_{ j})=\sum_{j=k+1}^{n+k}EX_{j}^{2},\]

by independence. Since \(\sum_{n=1}^{\infty}EX_{n}^{2}<\infty\), \(\{S_{n}\}_{n\geq 1}\) is a Cauchy sequence in \(L^{2}(\Omega,{\cal F},P)\) and hence converges in mean square to some \(S\) in \(L^{2}(\Omega,{\cal F},P)\). This implies that \(S_{n}\longrightarrow^{p}S\), and by the above theorem \(S_{n}\to S\) w.p. 1. \(\Box\)

**Remark 8.3.2:** It is possible to give another proof of the above theorem using Kolmogorov's inequality. See Problem 8.5.

**Theorem 8.3.5:** (_Kolmogorov's 3-series theorem_). _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables on a probability space \((\Omega,{\cal F},P)\) and let \(S_{n}=\sum_{i=1}^{n}X_{i}\), \(n\geq 1\). Then the sequence \(\{S_{n}\}_{n\geq 1}\) converges w.p. 1 iff the following 3-series converge for some \(0<c<\infty\):_

* \(\sum_{i=1}^{\infty}P(|X_{i}|>c)<\infty\)_,_
* \(\sum_{i=1}^{\infty}E(Y_{i})\) _converges,_
* \(\sum_{i=1}^{\infty}\mbox{Var}(Y_{i})<\infty\)_,__where \(Y_{i}=X_{i}I(|X_{i}|\leq c)\), \(i\geq 1\)._

**Proof:** (_Sufficiency_). By (i) and the Borel-Cantelli lemma, \(P(X_{i}\neq Y_{i}\) i.o.) \(=P(|X_{i}|>c\;\) i.o.) \(=0\). Hence \(\{S_{n}\}_{n\geq 1}\) converges w.p. 1 iff \(\{T_{n}\}_{n\geq 1}\) converges w.p. 1, where \(T_{n}=\sum_{i=1}^{n}Y_{i}\), \(n\geq 1\). By (iii) and the 1-series theorem, the sequence \(\{\sum_{i=1}^{n}(Y_{i}-EY_{i})\}_{n\geq 1}\) converges w.p. 1. Hence, by (ii), \(\{T_{n}\}_{n\geq 1}\) converges w.p. 1 and hence \(\{S_{n}\}_{n\geq 1}\) converges w.p. 1.

(_Necessity_). Suppose \(\{S_{n}\}_{n\geq 1}\) converges w.p. 1. Fix \(0<c<\infty\) and let \(Y_{i}=X_{i}I(|X_{i}|\leq c)\), \(i\geq 1\). Since \(\{S_{n}\}_{n\geq 1}\) converges w.p. 1, \(X_{n}\to 0\) w.p. 1. Hence, w.p. 1, \(|X_{i}|\leq c\) for all but a finite number of \(i\)'s. If \(A_{i}\equiv\{X_{i}\neq Y_{i}\}=\{|X_{i}|>c\}\), then by the second Borel-Cantelli lemma,

\[\sum_{i=1}^{\infty}P(A_{i})<\infty,\mbox{ establishing (i)}.\]

To establish (ii) and (iii), the following construction and the second inequality of Kolmogorov will be used. Without loss of generality, assume that there is another sequence \(\{\tilde{X}_{n}\}_{n\geq 1}\) of random variables on the same probability space \((\Omega,\mathcal{F},P)\) such that (a) \(\{\tilde{X}_{n}\}_{n\geq 1}\) are independent, (b) \(\{\tilde{X}_{n}\}_{n\geq 1}\) is independent of \(\{X_{n}\}_{n\geq 1}\), and (c) for each \(n\geq 1\), \(X_{n}=^{d}\tilde{X}_{n}\), i.e., \(X_{n}\) and \(\tilde{X}_{n}\) have the same distribution. Let

\[\tilde{Y}_{i} = \tilde{X}_{i}I(|\tilde{X}_{i}|\leq c),\] \[Z_{i} = Y_{i}-\tilde{Y}_{i},\ i\geq 1,\] \[T_{n} \equiv \sum_{i=1}^{n}Y_{i},\] \[\tilde{T}_{n} \equiv \sum_{i=1}^{n}\tilde{Y}_{i},\]

and

\[R_{n}\equiv\sum_{i=1}^{n}Z_{i},\ n\geq 1.\]

Since \(\{S_{n}\equiv\sum_{i=1}^{n}X_{i}\}_{n\geq 1}\) converges w.p. 1, and \(X_{i}=Y_{i}\) for all but a finite number of \(i\), \(\{T_{n}\}_{n\geq 1}\) converges w.p. 1. Since \(\{Y_{i}\}_{n\geq 1}\) and \(\{\tilde{Y}_{i}\}_{n\geq 1}\) have the same distribution on \(\mathbb{R}^{\infty}\), \(\{\tilde{T}_{n}\}_{n\geq 1}\) converges w.p. 1. Thus the difference sequence \(\{R_{n}\}_{n\geq 1}\) converges w.p. 1.

Next, note that \(\{Z_{n}\}_{n\geq 1}\) are independent random variables with mean 0 and \(\{Z_{n}\}_{n\geq 1}\) are uniformly bounded by \(2c\). Applying Kolmogorov's second inequality (Theorem 8.3.1 (b)) to \(\{Z_{j}:m<j\leq m+n\}\) yields

\[P\bigg{(}\max_{m<j\leq m+n}|R_{j}-R_{m}|\leq\epsilon\bigg{)}\leq\frac{(2c+4 \epsilon)^{2}}{\sum_{i=m+1}^{m+n}\mbox{Var}(Z_{i})} \tag{3.7}\]

for all \(m\geq 1\), \(n\geq 1\), \(0<\epsilon<\infty\).

Let \(\Delta_{m}\equiv\max_{m<j}|R_{j}-R_{m}|\). Let \(n\to\infty\) in (3.7) to conclude that

\[P(\Delta_{m}\leq\epsilon)\leq\frac{(2c+4\epsilon)^{2}}{\sum_{i=m+1}^{\infty} \mbox{Var}(Z_{i})}.\]

Now suppose (iii) does not hold. Then, since \(Y_{i}\) and \(\tilde{Y}_{i}\) are iid, \(\mbox{Var}(Z_{i})=2\mbox{Var}(Y_{i})\) for all \(i\geq 1\), and thus \(\sum_{i=m+1}^{\infty}\mbox{Var}(Z_{i})=\infty\) and hence \(P(\Delta_{m}\leq\epsilon)=0\) for each \(m\geq 1\), \(0<\epsilon<\infty\). This implies that \(P(\Delta_{m}>\epsilon)=1\) for each \(\epsilon>0\) and hence that \(\Delta_{m}=\infty\) w.p. 1 for all \(m\geq 1\). This contradicts the convergence w.p. 1 of the sequence \(\{R_{n}\}_{n\geq 1}\). Hence (iii) holds.

By the 1-series theorem, \(\{\sum_{i=1}^{n}(Y_{i}-EY_{i})\}_{n\geq 1}\) converges w.p. 1. Since \(\{\sum_{i=1}^{n}Y_{i}\}_{n\geq 1}\) converges w.p. 1, \(\sum_{i=1}^{\infty}EY_{i}\) converges, establishing (ii). This completes the proof of necessity part and of the theorem. \(\Box\)

**Remark 8.3.3:** To go from the convergence w.p. 1 of \(\{R_{n}\}_{n\geq 1}\) to (iii), it suffices to show that if (iii) fails, then for each \(0<A<\infty\), \(P(|R_{n}|\leq A)\to 0\) as \(n\to\infty\). This can be established without the use of (3.7) but using the central limit theorem (to be proved later in Chapter 11), which shows that if \(\mbox{Var}(R_{n})\to\infty\), then

\[P\biggl{(}\frac{R_{n}}{\sqrt{\mbox{Var}(R_{n})}}\leq x\biggr{)}\to\Phi(x)\equiv \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-t^{2}/2}dt,\]

for all \(x\) in \(\mathbb{R}\). (Also see Billingsley (1995), p. 290.)

### 8.4 Kolmogorov and Marcinkiewz-Zygmund SLLNs

For a sequence of independent and identically distributed random variables \(\{X_{n}\}_{n\geq 1}\), Kolmogorov showed that \(\{X_{n}\}_{n\geq 1}\) obeys the SLLN with \(b_{n}=n\) iff \(E|X_{1}|<\infty\). Marcinkiewz and Zygmund generalized this result and proved a class of SLLNs for \(\{X_{n}\}_{n\geq 1}\) when \(E|X|^{p}<\infty\) for some \(p\in(0,2)\). The proof uses Kolmogorov's 3-series theorem and some results from real analysis. This approach is to be contrasted with Etemadi's proof of the SLLN, which uses a decomposition of the random variables \(\{X_{n}\}_{n\geq 1}\) into positive and negative parts and uses monotonicity of the sum to establish almost sure convergence along a subsequence by an application of the Borel-Cantelli lemma. The alternative approach presented in this section is also useful for proving SLLNs for sums of independent random variables that are not necessarily identically distributed.

The next three are preparatory results for Theorem 8.4.4.

**Lemma 8.4.1:** (_Abel's summation formula_). _Let \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) be two sequences of real numbers. Then, for all \(n\geq 2\),_

\[\sum_{j=1}^{n}a_{j}b_{j}=A_{n}b_{n}-\sum_{j=1}^{n-1}A_{j}(b_{j+1}-b_{j}) \tag{4.1}\]_where \(A_{k}=\sum_{j=1}^{k}a_{j}\), \(k\geq 1\)._

**Proof:** Let \(A_{0}=0\). Then, \(a_{j}=A_{j}-A_{j-1}\), \(j\geq 1\). Hence,

\[\sum_{j=1}^{n}a_{j}b_{j} = \sum_{j=1}^{n}(A_{j}-A_{j-1})b_{j}=\sum_{j=1}^{n}A_{j}b_{j}-\sum_{ j=1}^{n}A_{j-1}b_{j}\] \[= \sum_{j=1}^{n}A_{j}b_{j}-\sum_{j=1}^{n-1}A_{j}b_{j+1},\]

yielding (4.1). \(\Box\)

**Lemma 8.4.2:** (_Kronecker's lemma_). _Let \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) be sequences of real numbers such that \(0<b_{n}\uparrow\infty\) as \(n\to\infty\) and \(\sum_{j=1}^{\infty}a_{j}\) converges. Then,_

\[\frac{1}{b_{n}}\sum_{j=1}^{n}a_{j}b_{j}\longrightarrow 0\quad\mbox{as} \quad n\to\infty. \tag{4.2}\]

**Proof:** Let \(A_{k}=\sum_{j=1}^{k}a_{j}\), \(A\equiv\sum_{j=1}^{\infty}a_{j}=\lim_{k\to\infty}A_{k}\) and \(R_{k}=A-A_{k}\), \(k\geq 1\). Then, by Lemma 8.4.1 for \(n\geq 2\),

\[\sum_{j=1}^{n}a_{j}b_{j} = A_{n}b_{n}-\sum_{j=1}^{n-1}A_{j}(b_{j+1}-b_{j}) \tag{4.3}\] \[= A_{n}b_{n}-\sum_{j=1}^{n-1}(A-R_{j})(b_{j+1}-b_{j})\] \[= A_{n}b_{n}-A\sum_{j=1}^{n-1}(b_{j+1}-b_{j})+\sum_{j=1}^{n-1}R_{j }(b_{j+1}-b_{j})\] \[= A_{n}b_{n}-Ab_{n}+Ab_{1}+\sum_{j=1}^{n-1}R_{j}(b_{j+1}-b_{j})\] \[= -R_{n}b_{n}+Ab_{1}+\sum_{j=1}^{n-1}R_{j}(b_{j+1}-b_{j}).\]

Since \(\sum_{n=1}^{\infty}a_{n}\) converges, \(R_{n}\to 0\) as \(n\to\infty\). Hence, given any \(\epsilon>0\), there exists \(N=N_{\epsilon}>1\) such that \(|R_{n}|\leq\epsilon\) for all \(n\geq N\). Since \(0<b_{n}\uparrow\infty\), for all \(n>N\),

\[\left|b_{n}^{-1}\sum_{j=1}^{n-1}R_{j}(b_{j+1}-b_{j})\right|\]\[\leq b_{n}^{-1}\sum_{j=1}^{N-1}|R_{j}|\,|b_{j+1}-b_{j}|+\epsilon\,b_{n}^{-1} \sum_{j=N}^{n-1}(b_{j+1}-b_{j})\] \[= b_{n}^{-1}\sum_{j=1}^{N-1}|R_{j}|\,|b_{j+1}-b_{j}|+\epsilon.\]

Now letting \(n\rightarrow\infty\) and then letting \(\epsilon\downarrow 0\), yields

\[\limsup_{n\rightarrow\infty}\left|b_{n}^{-1}\sum_{j=1}^{n-1}R_{j}(b_{j+1}-b_{ j})\right|=0.\]

Hence, (4.2) follows from (4.3). \(\Box\)

**Lemma 8.4.3:**_For any random variable \(X\),_

\[\sum_{n=1}^{\infty}P(|X|>n)\leq E|X|\leq\sum_{n=0}^{\infty}P(|X|>n). \tag{4.4}\]

**Proof:** For \(n\geq 1\), let \(A_{n}=\{n-1<|X|\leq n\}.\) Define the random variables

\[Y=\sum_{n=1}^{\infty}(n-1)\ I_{A_{n}}\quad\mbox{and}\quad Z=\sum_{n=1}^{\infty }n\ I_{A_{n}}.\]

Then, it is clear that \(Y\leq|X|\leq Z\), so that

\[EY\leq E|X|\leq EZ. \tag{4.5}\]

Note that

\[EY = \sum_{n=1}^{\infty}(n-1)P(A_{n})\] \[= \sum_{n=2}^{\infty}\sum_{j=1}^{n-1}P(A_{n})\] \[= \sum_{j=1}^{\infty}\sum_{n=j+1}^{\infty}P(n-1<|X|\leq n)\] \[= \sum_{j=1}^{\infty}P(|X|>j).\]

Similarly, one can show that \(EZ=\sum_{j=0}^{\infty}P(|X|>j).\) Hence, (4.4) follows. \(\Box\)

**Theorem 8.4.4:**_(Marcinkiew-Zygmund SLLNs). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of identically distributed random variables and let \(p\in(0,2)\). Write \(S_{n}=\sum_{j=1}^{n}X_{i}\), \(n\geq 1\)._1. _If_ \(\{X_{n}\}_{n\geq 1}\) _are pairwise independent and_ \[\frac{S_{n}-nc}{n^{1/p}}\quad\text{converges w.p. 1}\] (4.6) _for some_ \(c\in\mathbb{R}\)_, then_ \(E|X_{1}|^{p}<\infty\)_._
2. _Conversely, if_ \(E|X_{1}|^{p}<\infty\) _and_ \(\{X_{n}\}_{n\geq 1}\) _are independent, then (_4.6_) holds with_ \(c=EX_{1}\) _for_ \(p\in[1,2)\) _and with any_ \(c\in\mathbb{R}\) _for_ \(p\in(0,1)\)_._

**Corollary 8.4.5:** (_Kolmogorov's SLLN_). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables. Then,_

\[\frac{S_{n}-nc}{n}\to 0\quad\text{w.p. 1}\]

_for some \(c\in\mathbb{R}\) iff \(E|X_{1}|<\infty\), in which case, \(c=EX_{1}\)._

Thus, Kolmogorov's SLLN corresponds with the special case \(p=1\) of Theorem 8.4.4. Note that compared with the WLLN and Borel's SLLN of Sections 8.1 and 8.2, Kolmogorov's SLLN presents a significant improvement in the moment condition, i.e., it assumes the finiteness of only the first absolute moment. Further, both the Kolmogorov's SLLN and the Marcinkiewz-Zygmund SLLN are proved under minimal moment conditions, since the corresponding moment conditions are shown to be necessary.

**Proof of Theorem 8.4.4:** (i) Suppose that (4.6) holds for some \(c\in\mathbb{R}\). Then,

\[\frac{X_{n}}{n^{1/p}} = \frac{S_{n}-S_{n-1}}{n^{1/p}}\] \[= \frac{S_{n}-nc}{n^{1/p}}-\frac{S_{n-1}-(n-1)c}{n^{1/p}}+\frac{c}{n ^{1/p}}\] \[\to 0\quad\text{as}\quad n\to\infty,\quad\text{a.s.}\]

Hence, \(P(|X_{n}/n^{1/p}|>1\ \ \text{i.o.})=0\). By the second Borel-Cantelli lemma and by the pairwise independence of \(\{X_{n}\}_{n\geq 1}\), this implies

\[\sum_{n=1}^{\infty}P\biggl{(}\frac{|X_{n}|}{n^{1/p}}>1\biggr{)}<\infty,\]

i.e.,

\[\sum_{n=1}^{\infty}P\bigl{(}|X_{1}|^{p}>n\bigr{)}<\infty.\]

Hence, by Lemma 8.4.3, \(E|X_{1}|^{p}<\infty\).

To prove (ii), suppose that \(E|X_{1}|^{p}<\infty\) for some \(p\in(0,2)\). For \(1\leq p<2\), w.l.o.g. assume that \(EX_{1}=0\). Next, define the variables \(Z_{n}=X_{n}I(|X_{n}|^{p}\leq n)\), \(n\geq 1\). Then, by Lemma 8.4.3,

\[\sum_{n=1}^{\infty}P(X_{n}\neq Z_{n})\] \[= \sum_{n=1}^{\infty}P(|X_{n}|^{p}>n)=\sum_{n=1}^{\infty}P(|X_{1}|^ {p}>n)\leq E|X_{1}|^{p}<\infty.\]

Hence, by the Borel-Cantelli lemma,

\[P(X_{n}\neq Z_{n}\ \ {\rm i.o.})=0. \tag{4.7}\]

Note that, in view of (4.7), (4.6) holds with \(c=0\) if and only if

\[n^{1/p}\sum_{i=1}^{n}Z_{i}\to 0\quad{\rm as}\quad n\to\infty,\quad{\rm w.p. }\ 1. \tag{4.8}\]

Note that for any \(j\in\mathbb{N}\), \(\theta>1\) and \(\beta\in(-\infty,0)\backslash\{-1\}\),

\[\sum_{n=j}^{\infty}n^{-\theta} \leq j^{-\theta}+\sum_{n=j+1}^{\infty}\int_{n-1}^{n}x^{-\theta}dx \tag{4.9}\] \[= j^{-\theta}+\frac{1}{\theta-1}\cdot j^{-(\theta-1)}\] \[\leq \frac{\theta}{\theta-1}\cdot j^{-(\theta-1)}\]

and similarly,

\[\sum_{n=1}^{j}n^{\beta} \leq \big{[}\beta+j^{(\beta+1)}\big{]}/(\beta+1) \tag{4.10}\] \[\leq \frac{\beta}{\beta+1}I(\beta<-1)+\frac{j^{\beta+1}}{\beta+1}I(-1< \beta<0).\]

Now,

\[\sum_{n=1}^{\infty}{\rm Var}(Z_{n}/n^{1/p})\] \[\leq \sum_{n=1}^{\infty}EX_{1}^{2}I(|X_{1}|^{p}\leq n)\cdot n^{-2/p}\] \[= \sum_{n=1}^{\infty}\sum_{j=1}^{n}EX_{1}^{2}I(j-1<|X_{1}|^{p}\leq j )\cdot n^{-2/p}\]\[= \sum_{j=1}^{\infty}\bigg{(}\sum_{n=j}^{\infty}n^{-2/p}\bigg{)}\cdot EX _{1}^{2}I(j-1<|X_{1}|^{p}\leq j)\] \[\leq \frac{2}{2-p}\sum_{j=1}^{\infty}j^{-(\frac{2}{p}-1)}\cdot EX_{1}^{ 2}I\big{(}(j-1)<|X_{1}|^{p}\leq j\big{)}\quad\text{(by (\ref{eq:2.1}))}\] \[\leq \frac{2}{2-p}\sum_{j=1}^{\infty}j^{-(\frac{2}{p}-1)}\cdot E|X_{1} |^{p}I(j-1<|X_{1}|^{p}\leq j)\cdot(j^{1/p})^{2-p}\] \[= \frac{2}{2-p}E|X_{1}|^{p}<\infty.\]

Hence, by Theorem 8.3.4, \(\sum_{n=1}^{\infty}(Z_{n}-EZ_{n})/n^{1/p}\) converges w.p. 1. By Kronecker's lemma (viz. Lemma 8.4.2),

\[n^{-1/p}\sum_{j=1}^{n}(Z_{j}-EZ_{j})\to 0\quad\text{as}\quad n\to\infty, \quad\text{w.p. 1}. \tag{4.11}\]

Now consider the case \(p=1\). In this case, \(E|X_{1}|<\infty\) and by the DCT, \(EZ_{n}=EX_{1}I(|X_{1}|\leq n)\to EX_{1}=0\) as \(n\to\infty\). Hence, \(n^{-1}\sum_{i=1}^{n}EZ_{i}\to 0\). Part (ii) of the theorem now follows from (4.8) and (4.11) for \(p=1\).

Next consider the case \(p\in(0,2)\), \(p\neq 1\). Using (4.9) and (4.10), one can show (cf. Problem 8.12) that

\[n^{-1/p}\sum_{j=1}^{n}EZ_{j}\to 0\quad\text{as}\quad n\to\infty. \tag{4.12}\]

Hence, by (4.8), (4.11), and (4.12), one gets (4.6) with \(c=0\) for \(p\in(0,2)\backslash\{1\}\). Finally, note that for \(p\in(0,1)\), and for any \(c\in\mathbb{R}\),

\[\begin{array}{rcl}\frac{S_{n}-nc}{n^{1/p}}&=&\frac{S_{n}}{n^{1/ p}}-\frac{nc}{n^{1/p}}\\ &&\to 0\quad\text{as}\quad n\to\infty,\quad\text{a.s.},\end{array}\]

whenever \(S_{n}/n^{1/p}\to 0\) as \(n\to\infty\), w.p. 1. Hence, (4.6) holds with an arbitrary \(c\in\mathbb{R}\) for \(p\in(0,1)\). This completes the proof of part (ii) for \(p\in(0,2)\backslash\{1\}\) and hence of the theorem. \(\Box\)

The next result gives a SLLN for independent random variables that are not necessarily identically distributed.

**Theorem 8.4.6:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables. If \(\sum_{n=1}^{\infty}E|X_{n}|^{\alpha_{n}}/n^{\alpha_{n}}<\infty\) for some \(\alpha_{n}\in[1,2]\), \(n\geq 1\), then_

\[n^{-1}\sum_{j=1}^{n}(X_{j}-EX_{j})\to 0\quad\text{as}\quad n\to\infty,\quad \text{w.p. 1}. \tag{4.13}\]

**Proof:** W.l.o.g. suppose that \(EX_{n}=0\) for all \(n\geq 1\). Let \(Y_{n}=X_{n}I(|X_{n}|\leq n)/n\). Note that \(|EY_{n}|=|n^{-1}(EX_{n}-EX_{n}I(|X_{n}|>n))|=n^{-1}|EX_{n}I(|X_{n}|>n)|\), \(n\geq 1\). Since \(1\leq\alpha_{n}\leq 2\),

\[\sum_{n=1}^{\infty}\{P(|X_{n}|>n)+|EY_{n}|\}\] \[\leq 2\sum_{n=1}^{\infty}n^{-1}E|X_{n}|I(|X_{n}|>n)\] \[\leq 2\sum_{n=1}^{\infty}E|X_{n}|^{\alpha_{n}}/n^{\alpha_{n}}<\infty\]

and

\[\sum_{n=1}^{\infty}\mbox{Var}(Y_{n}) \leq \sum_{n=1}^{\infty}n^{-2}EX_{n}^{2}I(|X_{n}|\leq n)\] \[\leq \sum_{n=1}^{\infty}n^{-\alpha_{n}}EX_{n}^{\alpha_{n}}<\infty.\]

Hence, by Kolmogorov's 3-series theorem, \(\sum_{n=1}^{\infty}(X_{n}/n)\) converges w.p. 1. Now the theorem follows from Lemma 8.4.2. \(\Box\)

**Corollary 8.4.7:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables such that for some \(\alpha\in[1,2]\), \(\sum_{n=1}^{\infty}(n^{-\alpha}E|X_{n}|^{\alpha})<\infty\). Then (4.13) holds._

### Renewal theory

#### Definitions and basic properties

Let \(\{X_{n}\}_{n\geq 0}\) be a sequence of nonnegative random variables that are independent and, for \(i\geq 1\), identically distributed with cdf \(F\). Let \(S_{n}=\sum_{i=0}^{n}X_{i}\) for \(n\geq 0\). Imagine a system where a component in operation at time \(t=0\) lasts \(X_{0}\) units of time and then is replaced by a new one that lasts \(X_{1}\) units of time, which, at failure, is replaced by yet another new one that lasts \(X_{2}\) units of time and so on. The sequence \(\{S_{n}\}_{n\geq 0}\) represents the sequence of epochs when'renewal' takes place and is called a _renewal sequence_. Assume that \(P(X_{1}=0)<1\). Then, since \(P(X_{1}<\infty)=1\), it follows that for each \(n\), \(P(S_{n}<\infty)=1\) and \(\lim_{n\to\infty}S_{n}=\infty\) w.p. 1 (Problem 8.16). Now define the counting process \(\{N(t):t\geq 0\}\) by the relation

\[N(t)=k\quad\mbox{if}\quad S_{k-1}\leq t<S_{k}\quad\mbox{for}\quad k=0,1,2,\ldots \tag{5.1}\]

where \(S_{-1}=0\). Thus \(N(t)\) counts the number of renewals up to time \(t\).

**Definition 8.5.1:** The stochastic process \(\{N(t):t\geq 0\}\) is called a _renewal process_ with lifetime distribution \(F\). The renewal sequence \(\{S_{n}\}_{n\geq 0}\) and the renewal process \(\{N(t):t\geq 0\}\) are called _nondelayed_ or _standard_ if \(X_{0}\) has the same distribution as \(X_{1}\) and are called _delayed_ otherwise.

Since \(P(X_{1}\geq 0)=1\), \(\{S_{n}\}_{n\geq 0}\) is nondecreasing in \(n\) and for each \(t\geq 0\), the event \(\{N(t)=k\}=\{S_{k-1}\leq t<S_{k}\}\) belongs to the \(\sigma\)-algebra \(\sigma\langle\{X_{j}:0\leq j\leq k\}\rangle\) and hence \(N(t)\) is a random variable. Using the nontriviality hypothesis that \(P(X_{1}=0)<1\), it is shown below that for each \(t>0\), the random variable \(N(t)\) has finite moments of all order.

**Proposition 8.5.1:** _Let \(P(X_{1}=0)<1\). Then there exists \(0<\lambda<1\) (not depending on \(t\)) and a constant \(C(t)\in(0,\infty)\) such that_

\[P(N(t)>k)\leq C(t)\lambda^{k}\quad\mbox{for all}\quad k>0. \tag{5.2}\]

**Proof:** For \(t>0\), \(k\in\mathbb{N}\),

\[P(N(t)>k) = P(S_{k}\leq t)\] \[= P\bigl{(}e^{-\theta S_{k}}\geq e^{-\theta t}\bigr{)}\quad\mbox{ for}\quad\theta>0\] \[\leq e^{\theta t}E\bigl{(}e^{-\theta S_{k}}\bigr{)}\quad\mbox{(by Markov's inequality)}\] \[= e^{\theta t}E\bigl{(}e^{-\theta X_{0}}\bigr{)}\Bigl{(}E\bigl{(}e ^{-\theta X_{1}}\bigr{)}\Bigr{)}^{k}.\]

By BCT, \(\lim_{\theta\uparrow\infty}E(e^{-\theta X_{1}})=P(X_{1}=0)<1\). Hence, there exists a \(\theta\) large such that \(\lambda\equiv E(e^{-\theta X_{1}})\) is less than one, thus, completing the proof. \(\Box\)

**Corollary 8.5.2:** _There exists an \(s_{0}>0\) such that the moment generating function (m.g.f.) \(E(e^{sN(t)})<\infty\) for all \(s<s_{0}\) and \(t\geq 0\)._

**Proof:** From (5.2), for any \(t>0\), it follows that \(P\bigl{(}N(t)=k\bigr{)}=O(\lambda^{k})\) as \(k\to\infty\) for some \(0<\lambda<1\) and hence \(E\bigl{(}e^{sN(t)}\bigr{)}=\sum_{k=0}^{\infty}(e^{s})^{k}P\bigl{(}N(t)=k \bigr{)}<\infty\) for any \(s\) such that \(e^{s}\lambda<1\), i.e., for all \(s<s_{0}\equiv-\log\lambda\). \(\Box\)

From (5.1), it follows that for \(t>0\),

\[S_{N(t)-1}\leq t<S_{N(t)}\] \[\Rightarrow \Bigl{(}\frac{N(t)-1}{N(t)}\Bigr{)}\frac{S_{N(t)-1}}{(N(t)-1)} \leq\frac{t}{N(t)}\leq\Bigl{(}\frac{S_{N(t)}}{N(t)}\Bigr{)}. \tag{5.3}\]

Let \(A\) be the event that \(\frac{S_{n}}{n}\to EX_{1}\) as \(n\to\infty\) and let \(B\) be the event that \(N(t)\to\infty\) as \(t\to\infty\). Since \(S_{n}\to\infty\) w.p. 1, it follows that \(P(B)=1\). Also, by the SLLN, \(P(A)=1\). On the event \(C=A\cap B\), it holds that

\[\frac{S_{N(t)}}{N(t)}\to EX_{1}\quad\mbox{as}\quad t\to\infty.\]This together with (5.3) yields the following result.

Proposition 8.5.3: Suppose that \(P(X_{1}=0)<1\). Then,

\[\lim_{t\to\infty}\frac{N(t)}{t}=\frac{1}{EX_{1}}\quad\text{w.p. 1}.\]

Definition 8.5.2: The function \(U(t)\equiv EN(t)\) for the nondelayed process is called the renewal function.

An explicit expression for \(U(\cdot)\) is given by (5.13) below.

Next consider the convergence of \(EN(t)/t\). By (5.4) and Fatou's lemma, one gets

\[\liminf_{t\to\infty}\frac{EN(t)}{t}\geq\frac{1}{EX_{1}}.\]

It turns out that the \(\liminf_{t\to\infty}\) in (5.5) can be replaced by \(\lim_{t\to\infty}\) and \(\geq\) by equality. To do this it suffices to show that the family \(\{\frac{N(t)}{t}:t\geq k\}\) is uniformly integrable for some \(k<\infty\). This can be done by showing \(E(\frac{N(t)}{t})^{2}\) is bounded in \(t\) (see Chung (1974), Chapter 5). An alternate approach is to bound the \(\limsup\). For this one can use an identity known as Wald's equation (see also Chapter 13).

#### 8.5.2 Wald's equation

Let \(\{X_{j}\}_{j\geq 1}\) be independent random variables with \(EX_{j}=0\) for all \(j\geq 1\). Also, let \(S_{0}=0\), \(S_{n}=\sum_{j=1}^{n}X_{j}\), \(n\geq 1\).

Definition 8.5.3: A positive integer valued random variable \(N\) is called a _stopping time_ with respect to \(\{X_{j}\}_{j\geq 1}\) if for every \(j\geq 1\), the event \(\{N=j\}\in\sigma\langle\{X_{1},\ldots,X_{j}\}\rangle\). A stopping time \(N\) is called _bounded_ if there exists a \(K<\infty\) such that \(P(N\leq K)=1\).

Example 8.5.1: \(N\equiv\min\{n:\sum_{j=1}^{n}X_{j}\geq 25\}\) is a stopping time w.r.t. \(\{X_{j}\}_{j\geq 1}\), but \(M\equiv\max\{n:\sum_{j=1}^{n}X_{j}\geq 25\}\) is not.

Proposition 8.5.4: Let \(\{X_{j}\}_{j\geq 1}\) be independent random variables with \(EX_{j}=0\). Let \(N\) be a bounded stopping time w.r.t. \(\{X_{j}\}_{j\geq 1}\). Then

\[E(|S_{N}|)<\infty\quad\text{and}\quad ES_{N}=0.\]

**Proof:** Let \(K\in\mathbb{N}\) be such that \(P(N\leq K)=1\). Then \(|S_{N}|\leq\sum_{j=1}^{K}|X_{i}|\) and hence \(E|S_{N}|<\infty\). Next, \(S_{N}=\sum_{j=1}^{K}X_{j}I(N\geq j)\) and hence

\[ES_{N}=\sum_{j=1}^{K}E\bigl{(}X_{j}I(N\geq j)\bigr{)}.\]But the event \(\{N\geq j\}=\{N\leq j-1\}^{c}\in\sigma\langle\{X_{1},X_{2},\ldots,X_{j-1}\}\rangle\). Since \(X_{j}\) is independent of \(\sigma\langle X_{1},X_{2},\ldots,X_{j-1}\rangle\),

\[E\bigl{(}X_{j}I(N\geq j)\bigr{)}=0\quad\mbox{for}\quad 1\leq j\leq K.\]

Thus \(ES_{N}=0\). \(\Box\)

**Corollary 8.5.5:** _Let \(\{X_{j}\}_{j\geq 1}\) be iid random variables with \(E|X_{1}|<\infty\). Let \(N\) be a bounded stopping time w.r.t. \(\{X_{j}\}_{j\geq 1}\). Then_

\[ES_{N}=(EN)EX_{1}.\]

**Corollary 8.5.6:** _Let \(\{X_{j}\}_{j\geq 1}\) be iid nonnegative random variable with \(E|X_{1}|<\infty\). Let \(N\) be a stopping time w.r.t. \(\{X_{j}\}_{j\geq 1}\). Then_

\[ES_{N}=(EN)EX_{1}.\]

**Proof:** Let \(N_{k}=N\wedge k\), \(k=1,2,\ldots\). Then \(N_{k}\) is a bounded stopping time. By Corollary 8.5.5,

\[E(S_{N_{k}})=(EN_{k})EX_{1}.\]

Let \(k\uparrow\infty\). Then \(0\leq S_{N_{k}}\uparrow S_{N}\) and \(N_{k}\uparrow N\). By the MCT, \(ES_{N_{k}}\uparrow ES_{N}\) and \(EN_{k}\uparrow EN\). \(\Box\)

**Theorem 8.5.7:** (_Wald's equation_). _Let \(\{X_{j}\}_{j\geq 1}\) be iid random variables with \(E|X_{1}|<\infty\). Let \(N\) be a stopping time w.r.t. \(\{X_{j}\}_{j\geq 1}\) such that \(EN<\infty\). Then_

\[ES_{N}=(EN)EX_{1}.\]

**Proof:** Let \(T_{n}=\sum_{j=1}^{n}|X_{j}|\), \(n\geq 1\). Let \(N_{k}=N\wedge k\), \(k=1,2,\ldots\). Then by Corollary 8.5.5,

\[E(S_{N_{k}})=(EN_{k})EX_{1}.\]

Also, \(|S_{N_{k}}|\leq T_{N_{k}}\) and

\[ET_{N_{k}}=(EN_{k})E|X_{1}|.\]

Further, as \(k\to\infty\), \(N_{k}\to N\), \(S_{N_{k}}\to S_{N}\), \(T_{N_{k}}\to T_{N}\), and

\[ET_{N_{k}}\to ET_{N}=(EN)E|X_{1}|<\infty.\]

So, by the extended \(DCT\) (Theorem 2.3.11)

\[ES_{N_{k}}\to ES_{N}\]

\[\mbox{i.e.,}\quad(EN_{k})EX_{1}\to ES_{N}\]

\[\mbox{i.e.,}\quad ES_{N}=(EN)EX_{1}.\]

#### The renewal theorems

In this section, two versions of the renewal theorem will be proved. For this, the notation and concepts introduced in Sections 8.5.1 and 8.5.2 will be used without further explanation. Note that for each \(t>0\) and \(j=0,1,2,\ldots\), the event \(\{N(t)=j\}=\{S_{j-1}\leq t<S_{j}\}\) belongs to \(\sigma\langle\{X_{0},\ldots,X_{j}\}\rangle\). Thus, by Wald's equation (Theorem 8.5.7 above)

\[E(S_{N(t)})=\bigl{(}EN(t)\bigr{)}EX_{1}+EX_{0}.\]

Let \(m\in(0,\infty)\) and \(\tilde{X}_{i}=\min\{X_{i},m\}\), \(i\geq 0\). Let \(\{\tilde{S}_{n}\}_{n\geq 0}\) and \(\{\tilde{N}(t)\}_{t\geq 0}\) be the associated renewal sequence and renewal process, respectively. Again, by Wald's equation,

\[E\bigl{(}\tilde{S}_{\tilde{N}(t)}\bigr{)}=\bigl{(}E\tilde{N}(t)\bigr{)}E \tilde{X}_{1}+E\tilde{X}_{0}.\]

But since \(\tilde{S}_{\tilde{N}(t)-1}\leq t<\tilde{S}_{\tilde{N}(t)}\), it follows that \(\tilde{S}_{\tilde{N}(t)}\leq t+m\) and hence

\[(E\tilde{N}(t))E\tilde{X}_{1}+E\tilde{X}_{0}\leq t+m.\]

This yields

\[\limsup_{t\to\infty}\frac{E\tilde{N}(t)}{t}\leq\frac{1}{E\tilde{X}_{1}}.\]

Clearly, for all \(t>0\), \(\tilde{N}(t)\geq N(t)\) and hence

\[\limsup_{t\to\infty}E\frac{N(t)}{t}\leq\frac{1}{E\tilde{X}_{1}}. \tag{5.6}\]

Since this is true for each \(m\in(0,\infty)\) and by the MCT, \(E\tilde{X}_{1}\to EX_{1}\) as \(m\to\infty\), it follows that

\[\limsup_{t\to\infty}\frac{EN(t)}{t}\leq\frac{1}{EX_{1}}.\]

Combining this with (5.5) leads to the following result.

**Theorem 8.5.8:** (_The weak renewal theorem_). _Let \(\{N(t):t\geq 0\}\) be a renewal process with distribution \(F\). Let \(\mu=\int_{[0,\infty)}xdF(x)\in(0,\infty)\). Then,_

\[\lim_{t\to\infty}\frac{EN(t)}{t}=\frac{1}{\mu}. \tag{5.7}\]

The above result is also valid when \(\mu=\infty\) when \(\frac{1}{\mu}\) is interpreted as zero.

**Definition 8.5.4:** A random variable \(X\) (and its probability distribution) is called _arithmetic_ (or _lattice_) if there exists \(a\in\mathbb{R}\) and \(d>0\) such thatis integer valued. The largest such \(d\) is called the _span_ of (the distribution of) \(X\).

**Definition 8.5.5:** A random variable \(X\) (and its distribution distribution) is called _nonarithmetic_ (or _nonlattice_) if it is not arithmetic.

The weak renewal theorem (Theorem 8.5.8) implies that \(EN(t)=t/\mu+o(t)\) as \(t\to\infty\). This suggests that \(E\bigl{(}N(t+h)-N(t)\bigr{)}=(t+h)/\mu-t/\mu+o(t)=h/\mu+o(t)\). A strengthening of the above result is as follows.

**Theorem 8.5.9:** (_The strong renewal theorem_). _Let \(\{N(t):t\geq 0\}\) be a renewal process with a nonarithmetic distribution \(F\) with a finite positive mean \(\mu\). Then, for each \(h>0\),_

\[\lim_{t\to\infty}E\bigl{(}N(t+h)-N(t)\bigr{)}=\frac{h}{\mu}. \tag{5.8}\]

**Remark 8.5.1:** Since

\[N(t)=\sum_{j=0}^{k-1}\bigl{(}N(t-j)-N(t-j-1)\bigr{)}+N(t-k)\]

where \(k\leq t<k+1\), the weak renewal theorem follows from the strong renewal theorem.

The following are the "arithmetic versions" of Theorems 8.5.8 and 8.5.9. Let \(\{X_{i}\}_{i\geq 0}\) be independent positive integer valued random variables such that \(\{X_{i}\}_{i\geq 1}\) are iid with distribution \(\{p_{j}\}_{j\geq 1}\). Let \(S_{n}=\sum_{j=0}^{n}X_{j}\), \(n\geq 0\), \(S_{-1}=0\). Let \(N_{n}=k\) if \(S_{k-1}\leq n<S_{k}\), \(k=0,1,2,\ldots\). Let

\[u_{n} = P(\mbox{there is a renewal at time $n$})\] \[= P(S_{k}=n\mbox{ for some $k\geq 0$}).\]

**Theorem 8.5.10:** _Let \(\mu=\sum_{j=1}^{\infty}jp_{j}\in(0,\infty)\). Then_

\[\frac{1}{n}\sum_{j=0}^{n}u_{j}\to\frac{1}{\mu}\quad\mbox{as}\quad n\to\infty. \tag{5.9}\]

**Theorem 8.5.11:** _Let \(\mu=\sum_{j=1}^{\infty}jp_{j}\in(0,\infty)\) and g.c.d. \(\{k:p_{k}>0\}=1\). Then_

\[u_{n}\to\frac{1}{\mu}\quad\mbox{as}\quad n\to\infty. \tag{5.10}\]

For proofs of Theorems 8.5.9 and 8.5.11, see Feller (1966) for an analytic proof or Lindvall (1992) for a proof using the coupling method. The proof of Theorem 8.5.10 is similar to that of Theorem 8.5.8.

#### Renewal equations

The above strong renewal theorems have many applications. These are via what are known as _renewal equations_.

Let \(F(\cdot)\) be a cdf such that \(F(0)=0\). Let \(\mathbf{B}_{0}\equiv\{f\mid f:[0,\infty)\to\mathbb{R},\,f\) is Borel measurable and bounded on bounded intervals\(\}\).

**Definition 8.5.6:** A function \(a(\cdot)\) is said to satisfy the _renewal equation_ with _distribution_\(F(\cdot)\) and _forcing function_\(b(\cdot)\in\mathbf{B}_{0}\) if \(a\in\mathbf{B}_{0}\) and

\[a(t)=b(t)+\int_{(0,t]}a(t-u)dF(u)\quad\mbox{for}\quad t\geq 0. \tag{5.11}\]

**Theorem 8.5.12:**_Let \(F\) be a cdf such that \(F(0)=0\) and let \(b(\cdot)\in\mathbf{B}_{0}\). Then there is a unique solution \(a_{0}(\cdot)\in\mathbf{B}_{0}\) to (5.11) given by_

\[a_{0}(t)=\int_{[0,t]}b(t-u)U(du) \tag{5.12}\]

_where \(U(\cdot)\) is the Lebesgue-Stieltjes measure induced by the nondecreasing function_

\[U(t)\equiv\sum_{n=0}^{\infty}F^{(n)}(t), \tag{5.13}\]

_with \(F^{(n)}(\cdot)\), \(n\geq 0\) being defined by the relations_

\[F^{(n)}(t) = \int_{(0,t]}F^{(n-1)}(t-u)dF(u),\ t\in\mathbb{R},\ n\geq 1,\] \[F^{(0)}(t) = \left\{\begin{array}{ll}1&\mbox{if}\quad t\geq 0\\ 0&t<0.\end{array}\right.\]

It will be shown below that the function \(U(\cdot)\) defined in (5.13) is the _renewal function_\(EN(t)\) as in Definition 8.5.2.

**Proof:** For any function \(b\in\mathbf{B}_{0}\) and any nondecreasing right continuous function \(G:[0,\infty)\to\mathbb{R}\), let

\[(b*G)(t)\equiv\int_{[0,t]}b(t-u)dG(u).\]

Then since \(F(0)=0\), the equation (5.11) can be rewritten as

\[a=b+a*F. \tag{5.14}\]

Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with cdf \(F\). Then it is easy to verify that \(F^{(n)}(t)=P(S_{n}\leq t)\), where \(S_{0}=0\), and \(S_{n}=\sum_{i=1}^{n}X_{i}\) for \(n\geq 1\). Let\(\{N(t):t\geq 0\}\) be as defined by (5.1). Then, for \(t\in(0,\infty)\),

\[EN(t)=\sum_{j=1}^{\infty}P\big{(}N(t)\geq j\big{)}=\sum_{j=1}^{\infty}P(S_{j-1} \leq t)=\sum_{n=0}^{\infty}F^{(n)}(t)=U(t).\]

By Proposition 8.5.1, \(U(t)<\infty\) for all \(t>0\) and is nondecreasing. Since \(b\in\mathbf{B}_{0}\) for each \(0<t<\infty\), \(a_{0}\) defined by (5.12) is well-defined. By definition \(a_{0}=b*U\) and by (5.13), \(a_{0}\) satisfies (5.14) and hence (5.11). If \(a_{1}\) and \(a_{2}\) from \(\mathbf{B}_{0}\) are two solutions to (5.14) then \(\tilde{a}\equiv a_{1}-a_{2}\) satisfies

\[\tilde{a}=\tilde{a}*F\]

and hence

\[\tilde{a}=\tilde{a}*F^{(n)}\quad\mbox{for all}\quad n\geq 1.\]

This implies

\[M(t)\equiv\sup\{|\tilde{a}(u)|:0\leq u\leq t\}\leq M(t)F^{(n)}(t).\]

But \(F^{(n)}(t)\to 0\) as \(n\to\infty\). Hence \(|\tilde{a}|=0\) on \((0,t]\) for each \(t\). Thus \(a_{0}=b*U\) is the unique solution to (5.11). \(\Box\)

The discrete or arithmetic analog of the renewal equation (5.11) is as follows. Let \(\{X_{i}\}_{i\geq 1}\) be iid positive integer valued random variables with distribution \(\{p_{j}\}_{j\geq 1}\). Let \(S_{0}=0\), and \(S_{n}=\sum_{i=1}^{n}X_{i}\) for \(n\geq 1\). Let \(u_{n}=P(S_{j}=n\) for some \(j\geq 0)\). Then, \(u_{0}=1\) and \(u_{n}\) satisfies \(u_{n}=\sum_{j=1}^{n}p_{j}u_{n-j}\) for \(n\geq 1\). For any sequence \(\{b_{j}\}_{j\geq 0}\), the equation

\[a_{n}=b_{n}+\sum_{j=1}^{n}a_{n-j}p_{j},\ n=0,1,2,\ldots \tag{5.15}\]

is called the _discrete renewal equation_. As in the general case, it can be shown (Problem 8.17 (a)) that the unique solution to (5.15) is given by

\[a_{n}=\sum_{j=0}^{n}b_{n-j}u_{j}. \tag{5.16}\]

The following convergence results are easy to establish from Theorem 8.5.11 (Problem 8.17 (b)).

**Theorem 8.5.13:** (_The key renewal theorem, discrete case_). _Let \(\{p_{j}\}_{j\geq 1}\) be aperiodic, i.e., g.c.d. \(\{k:p_{k}>0\}=1\) and \(\mu\equiv\sum_{j=1}^{\infty}jp_{j}\in(0,\infty)\). Let \(\{u_{n}\}_{n\geq 0}\) be the renewal sequence associated with \(\{p_{j}\}_{j\geq 1}\). That is, \(u_{0}=1\) and \(u_{n}=\sum_{j=1}^{n}p_{j}u_{n-j}\) for \(n\geq 1\). Let \(\{b_{j}\}_{j\geq 0}\) be such that \(\sum_{j=1}^{\infty}|b_{j}|<\infty\). Let \(\{a_{n}\}_{n\geq 0}\) satisfy \(a_{0}=b_{0}\) and_

\[a_{n}=b_{n}+\sum_{j=1}^{\infty}a_{n-j}p_{j}\ \ n\geq 1. \tag{5.17}\]_Then \(a_{n}=\sum_{j=0}^{\infty}b_{j}u_{n-j}\), \(n\geq 0\) and \(\lim_{n\to\infty}a_{n}=\frac{1}{\mu}\sum_{j=0}^{\infty}b_{j}\)._

The nonarithmetic analog of the above is as follows.

**Definition 8.5.7:** A function \(b(\cdot)\in\mathbf{B}_{0}\) is _directly Riemann integrable_ (_dri_) on \([0,\infty)\) iff (i) for all \(h>0\), \(\sum_{n=0}^{\infty}\sup\{|b(u)|:nh\leq u\leq(n+1)h\}<\infty\), and (ii) \(\lim_{h\to 0}\sum_{n=0}^{\infty}h(\overline{m}_{n}(h)-\underline{m}_{n}(h) \big{)}=0\) where

\[\overline{m}_{n}(h) = \sup\{b(u):nh\leq u\leq(n+1)h\}\] \[\underline{m}_{n}(h) = \inf\{b(u):nh\leq u\leq(n+1)h\}.\]

**Theorem 8.5.14:** (_The key renewal theorem, nonarithmetic case_). _Let \(F(\cdot)\) be a nonarithmetic distribution with \(F(0)=0\) and \(\mu=\int_{[0,\infty)}udF(u)<\infty\). Let \(U(\cdot)=\sum_{n=0}^{\infty}F^{(n)}(\cdot)\) be the renewal function associated with \(F\). Let \(b(\cdot)\in\mathbf{B}_{0}\) be directly Riemann integrable._

_Then the unique solution to the renewal equation_

\[a=b+a*F \tag{5.18}\]

_is given by \(a=b*U\) and_

\[\lim_{t\to\infty}a(t)=\frac{c(b)}{\mu} \tag{5.19}\]

_where \(c(b)\equiv\lim_{h\to 0}\sum_{n=0}^{\infty}h\overline{m}_{n}(h)\)._

**Remark 8.5.2:** A sufficient condition for \(b(\cdot)\) to be \(dri\) is that it is Riemann integrable on bounded intervals and that there exists a nonincreasing integrable function \(h(\cdot)\) on \([0,\infty)\) and a constant \(C\) such that \(|b(\cdot)|\leq Ch(\cdot)\) (Problem 8.18 (b)).

#### Applications

Here are two important applications of the above two theorems to a class of stochastic processes known as _regenerative processes_.

**Definition 8.5.8:**

* A sequence of random variables \(\{Y_{n}\}_{n\geq 0}\) is called _regenerative_ if there exists a renewal sequence \(\{T_{j}\}_{j\geq 0}\) such that the random cycles and cycle length variables \(\eta_{j}=\big{(}\{Y_{i}:T_{j}\leq i<T_{j+1}\},T_{j+1}-T_{j}\big{)}\) for \(j=0,1,2,\ldots\) are iid.
* A stochastic process \(\{Y(t):t\geq 0\}\) is called _regenerative_ if there exists a renewal sequence \(\{T_{j}\}_{j\geq 0}\) such that the random cycles and cycle length variables \(\eta_{j}\equiv\{Y(t):T_{j}\leq t<T_{j+1},T_{j+1}-T_{j}\}\) for \(j=0,1,2,\ldots\) are iid.
3. In both (a) and (b), the sequence \(\{T_{j}\}_{j\geq 0}\) are called the _regeneration times_.

**Example 8.5.2:** Let \(\{Y_{n}\}_{n\geq 0}\) be a countable state space Markov chain (see Chapter 14) that is irreducible and recurrent. Fix a state \(\Delta\). Let

\[T_{0} = \min\{n:n>0,\ Y_{n}=\Delta\}\] \[T_{j+1} = \min\{n:n>T_{j},\ Y_{n}=\Delta\},\ n\geq 0.\]

Then \(\{Y_{n}\}_{n\geq 0}\) is regenerative (Problem 8.19).

**Example 8.5.3:** Let \(\{Y(t):t\geq 0\}\) be a continuous time Markov chain (see Chapter 14) with a countable state space that is irreducible and recurrent. Fix a state \(\Delta\). Let

\[T_{0} = \inf\{t:t>0,\ Y(t)=\Delta\}\] \[T_{j+1} = \inf\{t:t>T_{j},\ Y(t)=\Delta\}.\]

Then \(\{Y(t):t\geq 0\}\) is regenerative (Problem 8.19).

**Theorem 8.5.15:**_Let \(\{Y_{n}\}_{n\geq 0}\) be a regenerative sequence of random variables with some state space \((\mathbb{S},\mathcal{S})\) where \(\mathcal{S}\) is a \(\sigma\)-algebra on \(\mathbb{S}\) with regeneration times \(\{T_{j}\}_{j\geq 0}\). Let \(f:\mathbb{S}\to\mathbb{R}\) be bounded and \(\langle\mathcal{S},\mathcal{B}(\mathbb{R})\rangle\)-measurable. Let_

\[a_{n} \equiv Ef(Y_{n+T_{0}}),\] \[b_{n} \equiv Ef(Y_{T_{0}+n})I(T_{1}>T_{0}+n). \tag{5.20}\]

_Let \(\mu=E(T_{1}-T_{0})\in(0,\infty)\) and g.c.d. \(\{j:p_{j}\equiv P(T_{1}-T_{0}=j)>0\}=1\). Then_

1. \[a_{n}\to\int_{\mathbb{S}}f(y)\pi(dy)\] _where_ \(\pi(A)\equiv\frac{1}{\mu}\,E\Big{(}\sum_{j=T_{0}}^{T_{1}-1}I_{A}(Y_{j})\Big{)}\)_,_ \(A\in\mathcal{S}\)_._
2. _In particular,_ \[\|P(Y_{n}\in\cdot)-\pi(\cdot)\|\to 0\quad\mbox{as}\quad n\to\infty,\] (5.21) _where_ \(\|\cdot\|\) _denotes the total variation norm._

**Proof:** By the regenerative property, \(\{a_{n}\}_{n\geq 1}\) satisfies the renewal equation

\[a_{n}=b_{n}+\sum_{j=0}^{n}a_{n-j}p_{j}\]and hence, part (i) of the theorem follows from Theorem 8.5.13 and the fact \(\sum_{n=0}^{\infty}b_{n}=\mu\pi(A)\).

To prove (ii) note that \(\tilde{a}_{n}\equiv Ef(Y_{n})=E(f(Y_{n})I(T_{0}>n))+\sum_{j=0}^{n}a_{n-j}P(T_{0} =j)\) and by \(DCT\,\lim_{n\to\infty}\tilde{a}_{n}=\lim_{n\to\infty}a_{n}\).

It is not difficult to show that for any two probability measures \(\mu\) and \(\nu\) on \((\mathbb{S},\mathcal{S})\), the total variation norm

\[\|\mu-\nu\|=\sup\Big{\{}\Big{|}\int fd\mu-\int fd\nu\Big{|}:f\in\mathbf{ B}(\mathbb{S},\mathbb{R})\Big{\}}\]

where \(\mathbf{B}(\mathbb{S},\mathbb{R})=\{f:f:\mathbb{S}\to\mathbb{R},\, \mathcal{F}\) measurable, \(\sup\{|f(s)|:s\in\mathbb{S}\}\leq 1\}\) (Problem 4.10 (b)). Thus,

\[\|P(Y_{n+T_{0}}\in\cdot)-\pi(\cdot)\| \tag{5.22}\] \[\leq \sup\Big{\{}\Big{|}Ef(Y_{n_{0}+T})-\int fd\pi\Big{|}:f\in\mathbf{B}(\mathbb{S},\mathbb{R})\Big{\}}.\]

Now, for any \(f\in\mathbf{B}(\mathbb{S},\mathbb{R})\) and any integer \(K\geq 1\), from Theorem 8.5.13,

\[\Big{|}Ef(Y_{n_{0}+T})-\int fd\pi\Big{|} \tag{5.23}\] \[\leq \sum_{j=0}^{K}b_{j}\Big{|}u_{n-j}-\frac{1}{\mu}\Big{|}+2\sum_{j=( K+1)}^{\infty}P(T_{1}-T_{0}>j)\equiv\delta_{n},\mbox{ say }\]

where \(\{b_{j}\}\) is defined in (5.20). Since \(E(T_{1}-T_{0})<\infty\), given \(\epsilon>0\), there exists a \(K\) such that

\[\sum_{j=(K+1)}^{\infty}P(T_{1}-T_{0}>j)<\epsilon/2.\]

By Theorem (8.5.11), \(u_{n}\to\frac{1}{\mu}\). Thus, in (5.23), \(\overline{\lim}\,\delta_{n}\leq\epsilon\) and so from (5.22), (ii) follows. \(\Box\)

**Theorem 8.5.16:** _Let \(\{Y(t):t\geq 0\}\) be a regenerative stochastic process with state space \((\mathbb{S},\mathcal{S})\) where \(\mathcal{S}\) is a \(\sigma\)-algebra on \(\mathbb{S}\). Let \(f:\mathbb{S}\to\mathbb{R}\) be bounded and \(\langle\mathcal{S},\mathcal{B}(\mathbb{R})\rangle\)-measurable. Let_

\[a(t) = Ef(Y_{T_{0}+t}),\ t\geq 0,\] \[b(t) \equiv Ef(Y_{T_{0}+t})I(T_{1}>T_{0}+t),\ t\geq 0.\]

_Let \(\mu=E(T_{1}-T_{0})\in(0,\infty)\) and the distribution of \(T_{1}-T_{0}\) be nonarithmetic. Then_

1. \[a(t)\to\int_{\mathbb{S}}f(y)\pi(dy)\] _where_ \(\pi(A)=\frac{1}{\mu}\,E\Big{(}\int_{T_{0}}^{T}I_{A}(Y(u))du\Big{)}\)_,_ \(A\in\mathcal{S}\)_(ii) In particular,_

\[\|P(Y_{t}\in\cdot)-\pi(\cdot)\|\to 0\quad\mbox{as}\quad t\to\infty \tag{5.24}\]

_where \(\|\cdot\|\) is the total variation norm._

The proof of this is similar to that of the previous theorem but uses Theorem 8.5.14. \(\Box\)

### 8.6 Ergodic theorems

#### Basic definitions and examples

The law of large numbers proved in Section 8.2 states that if \(\{X_{i}\}_{i\geq 1}\) are pairwise independent and identically distributed and if \(h(\cdot)\) is a Borel measurable function, then

\[\mbox{the time average, i.e., }\frac{1}{n}\,\sum_{i=1}^{n}h(X_{i})\] \[\to Eh(X_{1}),\mbox{ i.e., space average w.p. }1 \tag{6.1}\]

as \(n\to\infty\), provided \(E|h(X_{1})|<\infty\).

The goal of this section is to investigate how far the independence assumption can be relaxed.

**Definition 8.6.1:** (_Stationary sequences_). A sequence of random variables \(\{X_{i}\}_{i\geq 1}\) on a probability space \((\Omega,{\cal F},P)\) is called _strictly stationary_ if for each \(k\geq 1\) the joint distribution of \((X_{i+j}:j=1,2,\ldots,k)\) is the same for all \(i\geq 0\).

**Example 8.6.1:** \(\{X_{i}\}_{i\geq 1}\) iid.

**Example 8.6.2:** Let \(\{X_{i}\}_{i\geq 1}\) be iid. Fix \(1\leq\ell<\infty\). Let \(h:\mathbb{R}^{\ell}\to\mathbb{R}\) be a Borel function and \(Y_{i}=h(X_{i},X_{i+1},\ldots,X_{i+\ell-1})\), \(i\geq 1\). Then \(\{Y_{i}\}_{i\geq 1}\) is strictly stationary.

**Example 8.6.3:** Let \(\{X_{i}\}_{i\geq 1}\) be a Markov chain with a stationary distribution \(\pi\). If \(X_{1}\sim\pi\) then \(\{X_{i}\}_{i\geq 1}\) is strictly stationary (see Chapter 14).

It will be shown that if \(\{X_{i}\}_{i\geq 1}\) is a strictly stationary sequence that is not a mixture of two other strictly stationary sequences, then (6.1) holds. This is known as _the ergodic theorem_ (Theorem 8.6.1 below).

**Definition 8.6.2:** (_Measure preserving transformations_). Let \((\Omega,{\cal F},P)\) be a probability space and \(T:\Omega\to\Omega\) be \(\langle{\cal F},{\cal F}\rangle\) measurable. Then, \(T\) is called _\(P\)-preserving_ (or simply _measure preserving_ on \((\Omega,{\cal F},P)\)) if for all \(A\in{\cal F}\), \(P(T^{-1}(A))=P(A)\). That is, the random point \(T(\omega)\) has the same distribution as \(\omega\).

Let \(X\) be a real valued random variable on \((\Omega,{\cal F},P)\). Let \(X_{i}\equiv X(T^{(i-1)}(\omega))\) where \(T^{(0)}(\omega)=\omega\), \(T^{(i)}(\omega)=T(T^{(i-1)}(\omega))\), \(i\geq 1\). Then \(\{X_{i}\}_{i\geq 1}\) is a strictly stationary sequence.

It turns out that every strictly stationary sequence arises this way. Let \(\{X_{i}\}_{i\geq 1}\) be a strictly stationary sequence defined on some probability space \((\Omega,{\cal F},P)\). Let \(\tilde{P}\) be the probability measure induced by \(\tilde{X}\equiv\{X_{i}(\omega)\}_{i\geq 1}\) on \(\bigl{(}\tilde{\Omega}\equiv{\mathbb{R}}^{\infty},\tilde{\cal F}\equiv{\cal B} ({\mathbb{R}}^{\infty})\bigr{)}\) where \({\mathbb{R}}^{\infty}\) is the space of all sequences of real numbers and \({\cal B}({\mathbb{R}}^{\infty})\) is the \(\sigma\)-algebra generated by finite dimensional cylinder sets of the form \(\{x:(x_{j}:j=1,2,\ldots,k)\in A_{k}\}\), \(1\leq k<\infty\), \(A_{k}\in{\cal B}({\mathbb{R}}^{k})\). Let \(T:{\mathbb{R}}^{\infty}\to{\mathbb{R}}^{\infty}\) be the _unilateral (one sided) shift to the right_, i.e., \(T\bigl{(}(x_{i})_{i\geq 1}\bigr{)}=(x_{i})_{i\geq 2}\). Then \(T\) is measure preserving on \((\tilde{\Omega},\tilde{\cal F},\tilde{P})\). Let \(Y_{1}(\tilde{\omega})=x_{1}\), and \(Y_{i}(\tilde{\omega})=Y_{1}(T^{i-1}\tilde{\omega})=x_{i}\) for \(i\geq 2\) if \(\tilde{\omega}=(x_{1},x_{2},x_{3},\ldots)\). Then \(\{Y_{i}\}_{i\geq 1}\) is a strictly stationary sequence on \((\tilde{\Omega},\tilde{\cal F},\tilde{P})\) and has the same distribution as \(\{X_{i}\}_{i\geq 1}\).

**Example 8.6.4:** Let \(\Omega=[0,1]\), \({\cal F}={\cal B}([0,1])\), \(P=\) Lebesgue measure. Let \(T\omega\equiv 2\omega\) mod 1, i.e.,

\[T\omega=\left\{\begin{array}{ccc}2\omega&\mbox{if}&0\leq\omega<\frac{1}{2} \\ 2\omega-1&\mbox{if}&\frac{1}{2}\leq\omega<1\\ 0&\omega=1.\end{array}\right.\]

Then \(T\) is measure preserving since \(P(\{\omega:a<T\omega<b\})=(b-a)\) for all \(0<a<b<1\) (Problem 8.20).

This example is an equivalent version of the iid sequence \(\{\delta_{i}\}_{i\geq 1}\) of Bernoulli \((1/2)\) random variables. To see this, let \(\omega=\sum_{i=1}^{\infty}\frac{\delta_{i}(\omega)}{2^{i}}\) be the binary expansion of \(\omega\). Then \(\{\delta_{i}\}_{i\geq 1}\) is iid Bernoulli \((1/2)\) and \(T\omega=2\omega\) mod \(1=\sum_{i=2}^{\infty}\frac{\delta_{i}(\omega)}{2^{i-1}}\) (cf. Problem 7.4). Thus \(T\) corresponds with the unilateral shift to right on the iid sequence \(\{\delta_{i}\}_{i\geq 1}\). For this reason, \(T\) is called the _Bernoulli shift_.

**Example 8.6.5:** (_Rotation_). Let \(\Omega=\{(x,y):x^{2}+y^{2}=1\}\) be the unit circle. Fix \(\theta_{0}\) in \([0,2\pi)\). If \(\omega=(\cos\theta,\sin\theta)\), \(\theta\) in \([0,2\pi)\) set \(T\omega=\bigl{(}\cos(\theta+\theta_{0}),\sin(\theta+\theta_{0})\bigr{)}\). That is, \(T\) rotates any point \(\omega\) on \(\Omega\) counterclockwise through an angle \(\theta_{0}\). Then \(T\) is measure preserving w.r.t. the Uniform distribution on \([0,2\pi]\).

**Definition 8.6.3:** Let \((\Omega,{\cal F},P)\) be a probability space and \(T:\Omega\to\Omega\) be a \(\langle{\cal F},{\cal F}\rangle\) measurable map. A set \(A\in{\cal F}\) is _T-invariant_ if \(A=T^{-1}A\). A set \(A\in{\cal F}\) is _almost \(T\)-invariant_ w.r.t. \(P\) if \(P(A\bigtriangleup T^{-1}A)=0\) where \(A_{1}\bigtriangleup A_{2}=(A_{1}\cap A_{2}^{c})\cup(A_{1}^{c}\cap A_{2})\) is the symmetric difference of \(A_{1}\) and \(A_{2}\)It can be shown that \(A\) is almost \(T\)-invariant w.r.t. \(P\) iff there exists a set \(A^{\prime}\) that is \(T\)-invariant and \(P(A\bigtriangleup A^{\prime})=0\) (Problem 8.21).

Examples of \(T\)-invariant sets are \(A_{1}=\{\omega:T^{j}\omega\in A_{0}\) for infinitely many \(i\geq 1\}\) where \(A_{0}\in{\cal F}\); \(A_{2}=\big{\{}\omega:\frac{1}{n}\sum_{j=1}^{n}h(T^{j}\omega)\) converges as \(n\to\infty\big{\}}\) where \(h:\Omega\to{\mathbb{R}}\) is a \({\cal F}\) measurable function. On the other hand, the event \(\{x:x_{1}\leq 0\}\) is not shift invariant in \(\big{(}{\mathbb{R}}^{\infty},{\cal B}({\mathbb{R}}^{\infty})\big{)}\) nor is it almost shift invariant if \(\tilde{P}\) corresponds to the iid case with a nondegenerate distribution.

The collection \({\cal I}\) of \(T\)-invariant sets is a \(\sigma\)-algebra and is called the _invariant \(\sigma\)-algebra_. A function \(h:\Omega\to{\mathbb{R}}\) is \({\cal I}\)-measurable iff \(h(\omega)=h(T\omega)\) for all \(\omega\) (Problem 8.22).

**Definition 8.6.4:** A measure preserving transformation \(T\) on a probability space \((\Omega,{\cal F},P)\) is _ergodic_ or _irreducible_ (w.r.t. \(P\)) if A is \(T\)-invariant implies \(P(A)=0\) or \(1\).

**Definition 8.6.5:** A stationary sequence of random variables \(\{X_{i}\}_{i\geq 1}\) is _ergodic_ if the unilateral shift \(T\) is _ergodic_ on the sequence space \(({\mathbb{R}}^{\infty},{\cal B}({\mathbb{R}}^{\infty}),\tilde{P})\) where \(\tilde{P}\) is the measure on \({\mathbb{R}}^{\infty}\) induced by \(\{X_{i}\}_{i\geq 1}\).

**Example 8.6.6:** Consider the above sequence space. Then \(A\in\tilde{\cal F}\) is invariant with respect to the unilateral shift implies that \(A\) is in the tail \(\sigma\)-algebra \({\cal T}\equiv\bigcap_{n=1}^{\infty}\sigma(\tilde{X}_{j}(\omega),j\geq n)\) (Problem 8.23). If \(\{X_{i}\}_{i\geq 1}\) are independent then by the Kolmogorov's zero-one law, \(A\in{\cal T}\) implies \(P(A)=0\) or \(1\). Thus, if \(\{X_{i}\}_{i\geq 1}\) are iid then it is _ergodic_.

On the other hand, mixtures of iid sequences are not ergodic as seen below.

**Example 8.6.7:** Let \(\{X_{i}\}_{i\geq 1}\) and \(\{Y_{i}\}_{i\geq 1}\) be two iid sequences with different distributions. Let \(\delta\) be Bernoulli \((p)\), \(0<p<1\) and independent of both \(\{X_{i}\}_{i\geq 1}\) and \(\{Y_{i}\}_{i\geq 1}\). Let \(Z_{i}\equiv\delta X_{i}+(1-\delta)Y_{i}\), \(i\geq 1\). Then \(\{Z_{i}\}_{i\geq 1}\) is a stationary sequence and is not _ergodic_ (Problem 8.24).

The above example can be extended to mixtures of irreducible positive recurrent discrete state space Markov chains (Problem 8.25 (a)). Another example is Example 8.6.5, i.e., rotation of the circle when \(\theta\) is rational (Problem 8.25 (b)).

**Remark 8.6.1:** There is a simple example of a measure preserving transformation \(T\) that is ergodic but \(T^{2}\) is not. Let \(\Omega=\{\omega_{1},\omega_{2}\}\), \(\omega_{1}\neq\omega_{2}\). Let \(T\omega_{1}=\omega_{2}\), \(T\omega_{2}=\omega_{1}\), \(P\) be the distribution \(P(\{\omega_{1}\})=P(\{\omega_{2}\})=\frac{1}{2}\). Then \(T\) is ergodic but \(T^{2}\) is not (Problem 8.26).

#### Birkhoff's ergodic theorem

**Theorem 8.6.1:** _Let \((\Omega,\mathcal{F},P)\) be a probability space, \(T:\Omega\to\Omega\) be a measure preserving ergodic map on \((\Omega,\mathcal{F},P)\) and \(X\in L^{1}(\Omega,\mathcal{F},P)\). Then_

\[\frac{1}{n}\sum_{j=0}^{n-1}X(T^{j}\omega)\to EX\equiv\int_{\Omega}XdP \tag{6.2}\]

_w.p. 1 and in \(L^{1}\) as \(n\to\infty\)._

**Remark 8.6.2:** A more general version is without the assumption of \(T\) being ergodic. In this case, the right side of (6.2) is a random variable \(Y(\omega)\) that is \(T\)-invariant, i.e., \(Y(\omega)=Y(T(\omega))\) w.p. 1 and satisfies \(\int_{A}XdP=\int_{A}YdP\) for all \(T\)-invariant sets \(A\). This \(Y\) is called the conditional expectation of \(X\) given \(\mathcal{I}\), the \(\sigma\)-algebra of invariant sets (Chapter 13).

For a proof of this version, see Durrett (2004).

The proof of Theorem 8.6.1 depends on the following inequality.

**Lemma 8.6.2:** (_Maximal ergodic inequality_). Let \(T\) be measure preserving on a probability space \((\Omega,\mathcal{F},P)\) and \(X\in L^{1}(\Omega,\mathcal{F},P)\). Let \(S_{0}(\omega)=0\), \(S_{n}(\omega)=\sum_{j=0}^{n-1}X(T^{j}\omega)\), \(n\geq 1\), \(M_{n}(\omega)=\max\{S_{j}(\omega):0\leq j\leq n\}\). Then_

\[E\big{(}X(\omega)I\big{(}M_{n}(\omega)>0\big{)}\big{)}\geq 0.\]

**Proof:** By definition of \(M_{n}(\omega)\), \(S_{j}(\omega)\leq M_{n}(\omega)\) for \(1\leq j\leq n\). Thus

\[X(\omega)+M_{n}(T\omega)\geq X(\omega)+S_{j}(T\omega)=S_{j+1}(\omega).\]

Also, since \(M_{n}(T\omega)\geq 0\),

\[X(\omega)\geq X(\omega)-M_{n}(T\omega)=S_{1}(\omega)-M_{n}(T\omega).\]

Thus \(X(\omega)\geq\max\big{\{}S_{j}(\omega):1\leq j\leq n\big{\}}-M_{n}(T\omega)\). For \(\omega\) such that \(M_{n}(\omega)>0\), \(M_{n}(\omega)=\max\big{\{}S_{j}(\omega):1\leq j\leq n\big{\}}\) and hence \(X(\omega)\geq M_{n}(\omega)-M_{n}(T\omega)\). Also, since \(X\in L^{1}(\Omega,\mathcal{F},P)\) it follows that \(M_{n}\in L^{1}(\Omega,\mathcal{F},P)\) for all \(n\geq 1\). Taking expectations yields

\[E\big{(}X(\omega)I\big{(}M_{n}(\omega)>0\big{)}\big{)}\] \[\geq E\big{(}M_{n}(\omega)-M_{n}(T\omega)I\big{(}M_{n}(\omega)>0\big{)} \big{)}\] \[\geq E\big{(}M_{n}(\omega)-M_{n}(T\omega)I\big{(}M_{n}(\omega)\geq 0 \big{)}\big{)}\ (\text{since }M_{n}(T\omega)\geq 0)\] \[= E\big{(}M_{n}(\omega)-M_{n}(T\omega)\big{)}=0,\]

since \(T\) is measure preserving. \(\Box\)

**Remark 8.6.3:** Note that the measure preserving property of \(T\) is used only at the last step.

**Proof of Theorem 8.6.1:** W.l.o.g. assume that \(EX=0\). Let \(Z(\omega)\equiv\limsup_{n\to\infty}\frac{S_{n}(\omega)}{n}\). Fix \(\epsilon>0\) and set \(A_{\epsilon}\equiv\{\omega:Z(\omega)>\epsilon\}\). It will be shown that \(P(A_{\epsilon})=0\). Clearly, \(A_{\epsilon}\) is \(T\) invariant. Since \(T\) is ergodic, \(P(A_{\epsilon})=0\) or \(1\). Suppose \(P(A_{\epsilon})=1\). Let \(Y(\omega)=X(\omega)-\epsilon\). Let \(M_{n,Y}(\omega)\equiv\max\{S_{j,Y}(\omega):0\leq j\leq n\}\) where \(S_{0,Y}(\omega)\equiv 0\), \(S_{j,Y}(\omega)\equiv\sum_{k=0}^{j-1}Y(T^{k}\omega)\), \(j\geq 1\). Then by Lemma 8.6.2 applied to \(Y(\omega)\)

\[E\big{(}Y(\omega)I\big{(}M_{n,Y}(\omega)>0\big{)}\big{)}\geq 0.\]

But \(B_{n}\equiv\{\omega:M_{n,Y}(\omega)>0\}=\{\omega:\sup_{1\leq j\leq n}\frac{1}{ j}S_{j,Y}(\omega)>0\}\). Clearly, \(B_{n}\uparrow B\equiv\{\omega:\sup_{1\leq j<\infty}\frac{1}{j}S_{j,Y}(\omega)>0\}\). Since \(\frac{1}{j}S_{j,Y}(\omega)=\frac{1}{j}S_{j}(\omega)-\epsilon\) for \(j\geq 1\), \(B\supset A_{\epsilon}\) and since \(P(A_{\epsilon})=1\), it follows that \(P(B)=1\). Also \(|Y|\leq|X|+\epsilon\in L^{1}(\Omega,\mathcal{F},P)\). So by the bounded convergence theorem, \(0\leq E(YI_{B_{n}})\to E(YI_{B})=EY=0-\epsilon<0\), which is a contradiction. Thus \(P(A_{\epsilon})=0\). This being true for every \(\epsilon>0\) it follows that \(P(\varlimsup_{n\to\infty}\frac{S_{n}(\omega)}{n}\leq 0)=1\). Applying this to \(-X(\omega)\) yields

\[P\Big{(}\varliminf_{n\to\infty}\frac{S_{n}(\omega)}{n}\geq 0\Big{)}=1\]

and hence \(P\big{(}\lim_{n\to\infty}\frac{S_{n}(\omega)}{n}=0\big{)}=1\).

To prove \(L^{1}\)-convergence, note that applying the above to \(X^{+}\) and \(X^{-}\) yields

\[f_{n}(\omega)\equiv\frac{1}{n}\sum_{i=1}^{n}X^{+}(T^{i}\omega)\to EX^{+}( \omega)\quad\mbox{w.p. 1.}\]

Since \(T\) is measure preserving \(\int f_{n}(\omega)dP=EX^{+}(\omega)\) for all \(n\). So by Scheffe's theorem (Lemma 8.2.5), \(\int|f_{n}(\omega)-EX^{+}(\omega)|dP\to 0\), i.e., \(E\left|\frac{1}{n}\sum_{i=1}^{n}X^{+}(T^{i}\omega)-EX^{+}\right|\to 0\). Similarly, \(E\left|\frac{1}{n}\sum_{i=1}^{n}X^{-}(T^{i}\omega)-EX^{-}\right|\to 0\). This yields \(L^{1}\) convergence. \(\Box\)

**Corollary 8.6.3:** _Let \(\{X_{i}\}_{i\geq 1}\) be a stationary ergodic sequence of \(\mathbb{R}^{k}\) valued random variables on some probability space \((\Omega,\mathcal{F},P)\). Let \(h:\mathbb{R}^{k}\to\mathbb{R}\) be Borel measurable and let \(E|h(X_{1},X_{2},\ldots,X_{k})|<\infty\). Then_

\[\frac{1}{n}\sum_{i=1}^{n}h(X_{i},X_{i+1},\ldots,X_{i+k-1})\to Eh(X_{1},X_{2}, \ldots,X_{k})\quad\mbox{w.p. 1.}\]

**Proof:** Consider the probability space \(\tilde{\Omega}=(\mathbb{R}^{k})^{\infty}\), \(\tilde{\mathcal{F}}\equiv\mathcal{B}\big{(}(\mathbb{R}^{k})^{\infty}\big{)}\) and \(\tilde{P}\) the probability measure induced by the map \(\omega\to(X_{i}(\omega))_{i\geq 1}\) and the unilateral shift map \(\tilde{T}\) on \(\tilde{\Omega}\) defined by \(\tilde{T}(x_{i})_{i\geq 1}=(x_{i})_{i\geq 2}\). Then \(\tilde{T}\) is

[MISSING_PAGE_FAIL:289]

\(P(X_{1}=a_{j})\), \(1\leq j\leq k\). This is called the Kolmogorov-Shannon entropy of the distribution \(\{p_{j}:1\leq j\leq k\}\).

If \(\{X_{i}\}_{i\geq 1}\) is a stationary ergodic Markov chain, then again it is a consequence of the strong law of large numbers, and \(H\) can be identified with

\[E\big{(}-\log p(X_{2}\mid X_{1})\big{)}=\sum_{i=1}^{k}\pi_{i}\sum_{j=1}^{k}(- \log p_{ij})p_{ij}\]

where \(\pi\equiv\{\pi_{i}:1\leq i\leq k\}\) is the stationary distribution and \(P\equiv\big{(}(p_{ij})\big{)}\) is the transition probability matrix of the Markov chain \(\{X_{i}\}_{i\geq 1}\). See Problem 8.27.

A more general version of the ergodic Theorem 8.6.1 is the following.

**Theorem 8.6.5:** (_Kingman's subadditive ergodic theorem_). _Let \(\{X_{m,n}:0\leq m<n\}_{n\geq 1}\) be a collection of random variables such that_

* \(X_{0,m}+X_{m,n}\geq X_{0,n}\) _for all_ \(0\leq m<n\)_,_ \(n\geq 1\)_._
* _For all_ \(k\geq 1\)_,_ \(\{X_{nk,(n+1)k}\}_{n\geq 1}\) _is a stationary sequence._
* _The sequence_ \(\{X_{m,m+k},k\geq 1\}\) _has a distribution that does not depend on_ \(m\geq 0\)_._
* \(EX_{0,1}^{+}<\infty\) _and for all_ \(n\)_,_ \(\frac{EX_{0,n}}{n}\geq\gamma_{0}\)_, where_ \(\gamma_{0}>-\infty\)_._

_Then_

* \(\lim_{n\to\infty}\frac{EX_{0,n}}{n}=\inf_{n\geq 1}\frac{EX_{0,n}}{n}\equiv\gamma\)_._
* \(\lim_{n\to\infty}\frac{X_{0,n}}{n}\equiv X\) _exists w.p. 1 and in_ \(L^{1}\)_, and_ \(EX=\gamma\)_._
* _If_ \(\{X_{nk,(n+1)k}\}_{n\geq 1}\) _is ergodic for each_ \(k\geq 1\)_, then_ \(X\equiv\gamma\) _w.p. 1._

A nice application of this is a result on products of random matrices.

**Theorem 8.6.6:** _Let \(\{A_{i}\}_{i\geq 1}\) be a stationary sequence of \(k\times k\) random matrices with nonnegative entries. Let \(\alpha_{m,n}(i,j)\) be the \((i,j)\)th entry in \(A_{m+1},\cdots,A_{n}\). Suppose \(E|\log\alpha_{1,2}(i,j)|<\infty\) for all \(i,j\). Then_

* \(\lim_{n\to\infty}\frac{1}{n}\log\alpha_{0,n}(i,j)=\eta\) _exists w.p. 1._
* _For any_ \(m\)_,_ \(\lim_{n\to\infty}\frac{1}{n}\log\|A_{m+1}\cdots,A_{n}\|=\eta\) _w.p. 1, where for any_ \(k\times k\) _matrix_ \(B\equiv((b_{ij})),\|B\|=\max\big{\{}\sum_{j=1}^{k}|b_{ij}|:1\leq i\leq k\big{\}}\)

**Remark 8.6.6:** A concept related to ergodicity is that of mixing. A measure preserving transformation \(T\) on a probability space \((\Omega,\mathcal{F},P)\) is _mixing_ if for all \(A\), \(B\in\mathcal{B}\)

\[\lim_{n\to\infty}\big{|}P(A\cap T^{-n}B)-P(A)P(T^{-n}B)\big{|}=0.\]

A stationary sequence of random variables \(\{X_{i}\}_{i\geq 1}\) is _mixing_ if the unilateral shift on the sequence space \(\mathbb{R}^{\infty}\) induced by \(\{X_{i}\}_{i\geq 1}\) is mixing. If \(T\) is mixing and \(A\) is \(T\)-invariant, then taking \(B=A\) in the above yields

\[P(A)=P^{2}(A)\]

i.e., \(P(A)=0\) or \(1\). Thus, if \(T\) is mixing, then \(T\) is ergodic. Conversely, if \(T\) is ergodic, then by Theorem 8.6.1, for any \(B\) in \(\mathcal{B}\)

\[\frac{1}{n}\sum_{j=1}^{n}I_{B}(T^{j}\omega)\to P(B)\ \ \mbox{w.p.\ 1}.\]

Integrating both sides over \(A\) w.r.t. \(P\) yields \(\frac{1}{n}\sum_{j=1}^{n}P(A\cap T^{-j}B)\to P(A)P(B)\), i.e., \(T\) is mixing in an average sense, i.e., the Cesaro sense. A sufficient condition for a stationary sequence to be mixing is that the tail \(\sigma\)-algebra be trivial. If \(\{X_{i}\}_{i\geq 1}\) is a stationary irreducible Markov chain with a countable state space, then it is _mixing_ iff it is aperiodic.

For proofs of the above results, see Durrett (2004).

### 8.7 Law of the iterated logarithm

Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=0\), \(EX_{1}^{2}=1\). The SLLN asserts that the sample mean \(\bar{X}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\to 0\) w.p. \(1\). The central limit theorem (to be proved later) asserts that for all \(-\infty<a<b<\infty\), \(P(a\leq\sqrt{n}\bar{X}_{n}\leq b)\to\Phi(b)-\Phi(a)\) where \(\Phi(\cdot)\) is the standard Normal cdf. This suggests that \(S_{n}=\sum_{i=1}^{n}X_{i}\) is of the order magnitude \(\sqrt{n}\) for large \(n\). This raises the question of how large does \(\frac{S_{n}}{\sqrt{n}}\) get as a function of \(n\). It turns out that it is of the order \(\sqrt{2n\log\log n}\). More precisely, the following holds:

**Theorem 8.7.1:** (_Law of the iterated logarithm_). _Let \(\{X_{i}(\omega)\}_{i\geq 1}\) be iid random variables on a probability space \((\Omega,\mathcal{F},P)\) with mean zero and variance one. Let \(S_{0}(\omega)=0\), \(S_{n}(\omega)=\sum_{i=1}^{n}X_{i}(\omega)\), \(n\geq 1\). For each \(\omega\), let \(A(\omega)\) be the set of limit points of \(\left\{\frac{S_{n}(\omega)}{\sqrt{2n\log\log n}}\right\}_{n\geq 1}\). Then \(P\{\omega:A(\omega)=[-1,+1]\}=1\)._

For a proof, see Durrett (2004).

A deep generalization of the above was obtained by Strassen (1964).

**Theorem 8.7.2:**_Under the setup of Theorem 8.7.1, the following holds: Let \(Y_{n}(\frac{j}{n};\omega)=\frac{S_{j}(\omega)}{\sqrt{2n\log\log n}}\), \(j=0,1,2,\ldots,n\) and \(Y_{n}(t,\omega)\) be the function obtained by linearly interpolating the above values on \([0,1]\). For each \(\omega\), let \(B(\omega)\) be the set of limit points of \(\{Y_{n}(\cdot,\omega)\}_{n\geq 1}\) in the function space \(C[0,1]\) of all continuous functions on \([0,1]\) with the supnorm. Then_

\[P\{\omega:B(\omega)=K\}=1\]

_where \(K\equiv\{f:f:[0,1]\to\mathbb{R}\), \(f\) is continuously differentiable, \(f(0)=0\) and \(\frac{1}{2}\int_{0}^{1}(f^{\prime}(t))^{2}dt\leq 1\}\)._

### Problems

* Prove Theorem 8.1.3 and Corollary 8.1.4. (**Hint:** Use Chebychev's inequality.)
* Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables on a probability space \((\Omega,\mathcal{F},P)\) such that for some \(m\in\mathbb{N}\) and for each \(i=1,\ldots,m\), \(\{X_{i},X_{i+m},X_{i+2m},\ldots\}\) are identically distributed and pairwise independent. Furthermore, suppose that \(E(|X_{1}|+\cdots+|X_{m}|)<\infty\). Show that \[\overline{X}_{n}\longrightarrow\frac{1}{m}\sum_{i=1}^{m}EX_{i},\quad\mbox{w.p. 1}.\] (**Hint:** Reduce the problem to nonnegative \(X_{n}\)'s and apply Theorem 8.2.7 for each \(i=1,\ldots,m\).)
* Let \(f\) be a bounded measurable function on [0,1] that is continuous at \(\frac{1}{2}\). Evaluate \(\lim\limits_{n\to\infty}\int_{0}^{1}\int_{0}^{1}\cdots\int_{0}^{1}f\Big{(} \frac{x_{1}+x_{2}+\cdots+x_{n}}{n}\Big{)}dx_{1}dx_{2}\ldots dx_{n}\).
* Show that if \(P(|X|>\alpha)<\frac{1}{2}\) for some real number \(\alpha\), then any median of \(X\) must lie in the interval \([-\alpha,\alpha]\).
* Prove Theorem 8.3.4 using Kolmogorov's first inequality (Theorem 8.3.1 (a)). (**Hint:** Apply Theorem 8.3.1 to \(\Delta_{n,k}\) defined in the proof of Theorem 8.3.3 to establish (3.4).)
* Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(E|X_{1}|^{\alpha}<\infty\) for some \(\alpha>0\). Derive a necessary and sufficient condition on \(\alpha\) for almost sure convergence of the series \(\sum_{n=1}^{\infty}X_{n}\sin 2\pi nt\) for all \(t\in(0,1)\).

* Show that for any given sequence of random variables \(\{X_{n}\}_{n\geq 1}\), there exists a sequence of real numbers \(\{a_{n}\}_{n\geq 1}\subset(0,\infty)\) such that \(\frac{X_{n}}{a_{n}}\to 0\) w.p. 1.
* Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables with \[P(X_{n}=2)=P(X_{n}=n^{\beta})=a_{n},\ P(X_{n}=a_{n})=1-2a_{n}\] for some \(a_{n}\in(0,\frac{1}{3})\) and \(\beta\in\mathbb{R}\). Show that \(\sum_{n=1}^{\infty}X_{n}\) converges if and only if \(\sum_{n=1}^{\infty}a_{n}<\infty\).
* Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(E|X_{1}|^{p}=\infty\) for some \(p\in(0,2)\). Then \(P(\limsup_{n\to\infty}|n^{-1/p}\sum_{i=1}^{n}X_{i}|=\infty)=1\).
* For any random variable \(X\) and any \(r\in(0,\infty)\), \(E|X|^{r}<\infty\) iff \(\sum_{n=1}^{\infty}n^{r-1}(\log n)^{r}P(|X|>n\log n)<\infty\). (**Hint:** Check that \(\sum_{n=1}^{m}n^{r-1}(\log n)^{r}\sim r^{-1}m^{r}(\log m)^{r}\) as \(m\to\infty\).)
* Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables with \(EX_{n}=0\), \(EX_{n}^{2}=\sigma_{n}^{2}\), \(s_{n}^{2}=\sum_{j=1}^{n}\sigma_{j}^{2}\to\infty\). Then, show that for any \(a>\frac{1}{2}\), \[s_{n}^{-2}(\log s_{n}^{2})^{-a}\sum_{i=1}^{n}X_{i}\to 0\quad\mbox{w.p. 1}.\]
* Show that for \(p\in(0,2)\), \(p\neq 1\), (4.12) holds. (**Hint:** For \(p\in(1,2)\), \(\sum_{n=1}^{\infty}|EZ_{n}/n^{1/p}|\leq\sum_{n=1}^{\infty}E|X_{1}|I(|X_{1}|>n) n^{-1/p}=\sum_{j=1}^{\infty}\sum_{n=1}^{j}n^{-1/p}\cdot E|X_{1}|I(j<|X_{1}|^{p} \leq j+1)\leq\frac{p}{p-1}E|X_{1}|^{p}<\infty\), by (4.10). For \(p\in(0,1)\), \(\sum_{n=1}^{\infty}|EZ_{n}/n^{1/p}|\leq\sum_{j=1}^{\infty}(\sum_{n=j}^{\infty} n^{-1/p})E|X_{1}|I(j-1<|X_{1}|^{p}\leq j)\leq\frac{1}{1-p}E|X_{1}|^{p}\), by (4.9).)
* Let \(Y_{i}=x_{i}\beta+\epsilon_{i}\), \(i\geq 1\) where \(\{\epsilon_{n}\}_{n\geq 1}\) is a sequence of iid random vectors, \(\{x_{n}\}_{n\geq 1}\) is a sequence of constants, and \(\beta\in\mathbb{R}\) is a constant (the regression parameter). Let \(\hat{\beta}_{n}=\sum_{i=1}^{n}x_{i}Y_{i}/\sum_{i=1}^{n}x_{i}^{2}\) denote (the least squares) estimator of \(\beta\). Let \(n^{-1}\sum_{i=1}^{n}x_{i}^{2}\to c\in(0,\infty)\) and \(E\epsilon_{1}=0\).
* If \(E|\epsilon_{1}|^{1+\delta}<\infty\) for some \(\delta\in(0,\infty)\), then show that \[\hat{\beta}_{n}\longrightarrow\beta\quad\mbox{as}\quad n\to\infty,\quad\mbox {w.p. 1}.\] (8.1)
* Suppose \(\sup\{|x_{i}|:i\geq 1\}<\infty\) and \(E|\epsilon_{1}|<\infty\). Show that (8.1) holds.
* (_Strongly consistent estimation_.) Let \(\{X_{i}\}_{i\geq 1}\) be random variables on some probability space \((\Omega,{\cal F},P)\) such that (i) for some integer \(m\geq 1\) the collections \(\{X_{i}:i\leq n\}\) and \(\{X_{i}:i\geq n+m\}\) are independent for each \(n\geq 1\), and (ii) the distribution of \(\{X_{i+j};0\leq j\leq k\}\) is independent of \(i\), for all \(k\geq 0\).
* Show that for every \(\ell\geq 1\) and \(h:{\mathbb{R}}^{\ell}\to{\mathbb{R}}\) with \(E|h(X_{1},X_{2},\ldots,X_{\ell})|<\infty\), there are functions \(\{f_{n}:{\mathbb{R}}^{n}\to{\mathbb{R}}\}_{n\geq 1}\) such that \(f_{n}(X_{1},X_{2},\ldots,X_{n})\to\lambda\equiv Eh(X_{1},X_{2},\ldots,X_{\ell})\) w.p. 1. In this case, one says \(\lambda\) is estimable from \(\{X_{i}\}_{i\geq 1}\) in a strongly consistent manner.
* Now suppose the distribution \(\mu(\cdot)\) of \(X_{1}\) is a mixture of the form \(\mu=\sum_{i=1}^{k}\alpha_{i}\mu_{i}\). Suppose there exist disjoint Borel sets \(\{A_{i}\}_{1\leq i\leq k}\) in \({\mathbb{R}}\) such that \(\mu_{i}(A_{i})=1\) for each \(i\). Show that all the \(\alpha_{i}\)'s as well as \(\lambda_{i}\equiv\int h_{i}(x)d\mu_{i}\) where \(h_{i}\in L_{1}(\mu_{i})\) are estimable from \(\{X_{i}\}_{i\geq 1}\) in a strongly consistent manner.
* (_Normal numbers_). Recall that in Section 4.5 it was shown that for any positive integer \(p>1\) and for any \(0\leq\omega\leq 1\), it is possible to write \(\omega\) as \[\omega=\sum_{i=1}^{\infty}\frac{X_{i}(\omega)}{p^{i}}\] (8.2) where for each \(i\), \(X_{i}(\omega)\in\{0,1,2,\ldots,p-1\}\). Recall also that such an expansion is unique except for \(\omega\) of the form \(q/p^{n}\), \(q=1,2,\ldots,p^{n}-1\), \(n\geq 1\) in which case there are exactly two expansions, one of which is recurring. In what follows, for such \(\omega\)'s the recurrent expansion will be the one used in (8.2). A number \(\omega\) in [0,1] is called _normal_ w.r.t. the integer \(p\) if for every finite pattern \(a_{1}a_{2}\ldots a_{k}\) where \(k\geq 1\) is a positive integer and \(a_{i}\in\{0,1,2,\ldots,p-1\}\) for \(1\leq i\leq k\) the relative frequency \(\frac{1}{n}\sum_{i=1}^{n}\delta_{i}(\omega)\) where \[\delta_{i}(\omega)=\left\{\begin{array}{ll}1&\mbox{if}\quad X_{i+j}(\omega)= a_{j+1},\ j=0,1,2,\ldots,k-1\\ 0&\mbox{otherwise}\end{array}\right.\] converges to \(p^{-k}\) as \(n\to\infty\). A number \(\omega\) in [0,1] is called _absolutely normal_ if it is normal w.r.t. \(p\) for every integer \(p>1\). Show that the set \(A\) of all numbers \(\omega\) in [0,1] that are absolutely normal has Lebesgue measure one. (**Hint:** Note that in (8.2), the function \(\{X_{i}(\omega)\}_{i\geq 1}\) are iid random variables. Now use Problem 8.14 repeatedly.)
* Show that for the renewal sequence \(\{S_{n}\}_{n=0}^{\infty}\), if \(P(X_{1}>0)>0\), then \(\lim_{n\to\infty}S_{n}=\infty\) w.p. 1.

* 8.17 (a) Show that \(\{a_{n}\}_{n\geq 0}\) of (5.16) is the unique solution to (5.15) by using generating functions (cf. Section 5.5). (b) Deduce Theorems 8.5.13 and 8.5.14 from Theorems 8.5.11 and 8.5.12, respectively. (**Hint:** For Theorems 8.5.13 use the \(DCT\), and for Theorem 8.5.14, show first that \[\sum_{n=0}^{k}\underline{m}_{n}(h)\bigl{(}U((n+1)h)-U(nh)\bigr{)}\] \[\leq a(kh)\] \[\leq \sum_{n=0}^{k}\overline{m}_{n}(h)\bigl{(}U((n+1)h)-U(nh)\bigr{)}.)\]
* 8.18 (a) Let \(b(\cdot):[0,\infty)\to\mathbb{R}\) be \(dri\). Show that \(b(\cdot)\) is Riemann integrable on every bounded interval. Conclude that if \(b(\cdot)\) is \(dri\) it must be continuous almost everywhere w.r.t. Lebesgue measure. (b) Let \(b(\cdot):[0,\infty)\to\mathbb{R}\) be Riemann integrable on \([0,K]\) for each \(K<\infty\). Let \(h(\cdot):[0,\infty)\to\mathbb{R}^{+}\) be nonincreasing and integrable w.r.t. Lebesgue measure and \(|b(\cdot)|\leq h(\cdot)\) on \([0,\infty)\). Show that \(b(\cdot)\) is \(dri\).
* 8.19 Verify that the sequence \(\{Y_{n}\}_{n\geq 0}\) in Example 8.5.2 and the process \(\{Y(t):t\geq 0\}\) in Example 8.5.3 are both regenerative.
* 8.20 Show that the map \(T\) in Example 8.6.4 in Section 8.6 is measure preserving. (**Hint:** Show that for \(0<a<b<1\), \(P\bigl{(}\omega:T\omega\in(a,b)\bigr{)}=(b-a)\).)
* 8.21 Let \(T\) be a measure preserving map on a probability space \((\Omega,\mathcal{F},P)\). Show that \(A\) is almost \(T\)-invariant w.r.t. \(P\) iff there exists a set \(A_{1}\) such that \(A_{1}=T^{-1}A_{1}\) and \(P(A\bigtriangleup A_{1})=0\). (**Hint:** Consider \(A_{1}=\bigcup_{n=0}^{\infty}T^{-n}A\). )
* 8.22 Show that a function \(h:\Omega\to\mathbb{R}\) is \(\mathcal{I}\)-measurable iff \(h(\omega)=h(T\omega)\) for all \(\omega\) where \(\mathcal{I}\) is the \(\sigma\)-algebra of \(T\)-invariant sets.
* 8.23 Consider the sequence space \(\bigl{(}\mathbb{R}^{\infty},\mathcal{B}(\mathbb{R}^{\infty})\bigr{)}\). Show that \(A\in\mathcal{B}(\mathbb{R}^{\infty})\) is invariant w.r.t. the unilateral shift \(T\) implies that \(A\) is in the tail \(\sigma\)-algebra.
* 8.24 In Example 8.6.7 of Section 8.6, show that \(\{Z_{i}\}_{i\geq 1}\) is a stationary sequence that is not ergodic. (**Hint:** Assuming it is ergodic, derive a contradiction using the ergodic Theorem 8.6.1.)* 1. Extend Example 8.6.7 to the Markov chain case with two disjoint irreducible positive recurrent subsets. 2. Show that in Example 8.6.5, if \(\theta_{0}\) is rational, then \(T\) is not ergodic.
* 2. Verify that in Remark 8.6.1, \(T\) is ergodic but \(T^{2}\) is not. 3. Construct a Markov chain with four states for which \(T\) is ergodic but \(T^{2}\) is not.
* 2. In Remark 8.6.5, prove the Shannon-McMillan-Breiman theorem directly for the Markov chain case. (**Hint:** Express \(p(X_{1},X_{2},\ldots,X_{n})\) as \(\bigg{(}\prod\limits_{i=1}^{n-1}p_{X_{i}X_{i+1}}\bigg{)}p(X_{1})\).)
* 2. Let \(\{X_{i}\}_{i\geq 1}\) be iid Bernoulli (\(1/2\)) random variables. Let \[W_{1} = \sum\limits_{i=1}^{\infty}\frac{2X_{2i}}{4^{i}}\] \[W_{2} = \sum\limits_{i=1}^{\infty}\frac{X_{2i-1}}{4^{i}}.\] 1. Show that \(W_{1}\) and \(W_{2}\) are independent. 2. Let \(A_{1}=\{\omega:\omega\in(0,1)\) such that in the expansion of \(\omega\) in base 4 only the digits 0 and 2 appear\(\}\) and \(A_{2}=\{\omega:\omega\in(0,1)\) such that in the expansion of \(\omega\) in base 4 only the digits 0 and 1 appear\(\}\). Show that \(m(A_{1})=m(A_{2})=0\) where \(m(\cdot)\) is Lebesgue measure and hence that the distribution of \(W_{1}\) and \(W_{2}\) are singular w.r.t. \(m(\cdot)\). 3. Let \(W\equiv W_{1}+W_{2}\). Then show that \(W\) has uniform (0,1) distribution. (**Hint:** For (b) use the SLLN.) Remark: This example shows that the convolution of two singular probability measures can be absolutely continuous w.r.t. Lebesgue measure.
* 2. Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of pairwise independent and identically distributed random variables with \(P(X_{1}\leq x)=F(x)\), \(x\in\mathbb{R}\). Fix \(0<p<1\). Suppose that \(F(\zeta_{p}+\epsilon)>p\) for all \(\epsilon>0\) where \[\zeta_{p}=F^{-1}(p)\equiv\inf\{x:F(x)\geq p\}.\] Show that \(\hat{\zeta}_{n}\equiv F_{n}^{-1}(p)\equiv\inf\{x:F_{n}(x)\geq p\}\) converges to \(\zeta_{p}\) w.p. 1 where \(F_{n}(x)\equiv n^{-1}\sum_{i=1}^{n}I(X_{i}\leq x)\), \(x\in\mathbb{R}\) is the empirical distribution function of \(X_{1},\ldots,X_{n}\).

* Let \(\{X_{i}\}_{i\geq 1}\) be random variables such that \(EX_{i}^{2}<\infty\) for all \(i\geq 1\). Suppose \(\frac{1}{n}\sum_{i=1}^{n}EX_{i}\to 0\) and \(a_{n}\equiv\frac{1}{n^{2}}\sum_{j=0}^{n}(n-j)v(j)\to 0\) as \(n\to\infty\) where \(v(j)=\sup_{i}\big{|}{\rm Cov}(X_{i},X_{i+j})\big{|}\).
* Show that \(\bar{X}_{n}\longrightarrow^{p}0\).
* Suppose further that \(\sum_{n=1}^{\infty}a_{n}<\infty\). Show that \(\bar{X}_{n}\to 0\) w.p. 1.
* Show that as \(n\to\infty\), \(v(n)\to 0\) implies \(a_{n}\to 0\) but the converse need not hold.
* Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with cdf \(F(\cdot)\). Let \(F_{n}(x)\equiv\frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x)\) be the empirical cdf. Suppose \(x_{n}\to x_{0}\) and \(F(\cdot)\) is continuous at \(x_{0}\). Show that \(F_{n}(x_{n})\to F(x_{0})\) w.p. 1.
* Let \(p\) be a positive integer \(>1\). Let \(\{\delta_{i}\}_{i\geq 1}\) be iid random variable with distribution \(P(\delta_{1}=j)=p_{j}\), \(0\leq j\leq p-1\), \(p_{j}\geq 0\), \(\sum_{0}^{p-1}p_{j}=1\). Let \(X=\sum_{i=1}^{\infty}\frac{\delta_{i}}{p^{i}}\). Show that
* \(F_{X}(x)\equiv P(X\leq x)\) is continuous and strictly increasing in (0,1) if \(0<p_{j}<1\) for any \(0\leq j\leq p-1\).
* \(F_{X}(\cdot)\) is absolutely continuous iff \(p_{j}=\frac{1}{j}\) for all \(0\leq j\leq p-1\) in which case \(F_{X}(x)\equiv x\), \(0\leq x\leq 1\).
* Let \(\{X_{n}\}_{n\geq 0}\) be a sequence of random variables such that \[X_{n+1}=\rho_{n+1}X_{n}+\epsilon_{n+1},\ \ n\geq 0\] where the sequence \(\{(\rho_{n},\epsilon_{n})\}_{n\geq 1}\) are iid and independent of \(X_{0}\).
* Show that if \(E(\log|\rho_{1}|)<0\) and \(E(\log|\epsilon_{1}|)^{+}<\infty\) then \[\hat{X}_{n}\equiv\sum_{j=0}^{n}\rho_{1}\rho_{2}\ldots\rho_{j},\epsilon_{j+1} \quad\mbox{converges w.p.\ 1}.\]
* Show that under the hypothesis of (a), for any bounded continuous function \(h:\mathbb{R}\to\mathbb{R}\) and for any distribution of \(X_{0}\) \[Eh(X_{n})\to Eh(\hat{X}_{\infty}).\] (**Hint:*
* Show by SLLN that there is a \(0<\lambda<1\) such that \(\rho_{1},\rho_{2},\ldots,\rho_{j}=0(\lambda^{j})\) w.p. 1 as \(j\to\infty\) and by Borel-Cantelli \(|\epsilon_{j}|=0(\lambda^{j})\) for some \(\lambda^{\prime}>0\ni\lambda^{\prime}\lambda<1\).)
* Let \((\mathbb{S},\rho)\) be a complete separable metric space. Let \((G,\mathcal{G})\) be a measurable space. Let \(f:G\times\mathbb{S}\to\mathbb{S}\) be \(\langle{\cal G}\times{\cal B}({\mathbb{S}}),{\cal B}({\mathbb{S}})\rangle\) measurable function. Let \((\Omega,{\cal F},P)\) be a probability space and \(\{\theta_{i}\}_{i\geq 1}\) be iid \(G\)-valued random variables on \((\Omega,{\cal F},P)\). Let \(X_{0}\) be an \({\mathbb{S}}\)-valued random variable on \((\Omega,{\cal F},P)\) independent of \(\{\theta_{i}\}_{i\geq 1}\). Define \(\{X_{n}\}_{n\geq 0}\) by the random iteration scheme, \[X_{0}(x,\omega)\equiv x\] \[X_{n+1}(x,\omega)=f\bigl{(}\theta_{n+1}(\omega),X_{n}(x,\omega)\bigr{)}\ n\geq 0.\] (a) Show that for each \(n\geq 0\), the map \(X_{n}={\mathbb{S}}\times\Omega\to{\mathbb{S}}\) is \(\langle{\cal B}({\mathbb{S}})\times{\cal F},{\cal B}({\mathbb{S}})\rangle\) measurable. (b) Let \(f_{n}(x)\equiv f_{n}(x,\omega)\equiv f(\theta_{n}(\omega),x)\). Let \(\hat{X}_{n}(x,\omega)=f_{1}(f_{2},\ldots,f_{n}(x))\). Show that for each \(x\) and \(n\), \(\hat{X}_{n}(x,\omega)\) and \(X_{n}(x,\omega)\) have the same distribution. (c) Now assume that for all \(\omega\), \(f(\theta_{1}(\omega),x)\) is Lipschitz from \({\mathbb{S}}\) to \({\mathbb{S}}\), i.e., \[\ell_{i}(\omega)\equiv\sup_{x\neq y}\frac{d(f(\theta_{i}(\omega),x),f(\theta_{i }(\omega),y))}{d(x,y)}<\infty.\] Show that \(\ell_{i}(\omega)\) is a random variable on \((\Omega,{\cal F},P)\), i.e. that \(\ell_{i}(\cdot):\Omega\to{\mathbb{R}}^{+}\) is \(\langle{\cal F},{\cal B}({\mathbb{R}})\rangle\) measurable. (d) Suppose that \(E|\log\ell_{1}(\omega)|<\infty\) and \(E\log\ell_{1}(\omega)<0\), \(E|\log d(f(\theta_{1},x),x)|<\infty\) for all \(x\). Show that \(\lim_{n}\hat{X}_{n}(x,\omega)=\hat{X}_{\infty}(\omega)\) exists w.p. 1 and is independent of \(x\) w.p. 1. (**Hint:** Use Borel-Cantelli to show that for each \(x\), \(\{\hat{X}_{n}(x,\omega)\}_{n\geq 1}\) is Cauchy in \(({\mathbb{S}},\rho)\).) (e) Under the hypothesis in (d) show that for any bounded continuous \(h:{\mathbb{S}}\to{\mathbb{R}}\) and for any \(x\in{\mathbb{S}}\), \(\lim_{n\to\infty}Eh(X_{n}(x,\omega))=Eh(\hat{X}_{\infty}(\omega))\). (f) Deduce the results in Problems 7.15 and 8.33 as special cases.
8.35 (_Extension of Gilvenko-Cantelli (Theorem 8.2.4) to the multivariate case_). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of pairwise independent and identically distributed random vectors taking values in \({\mathbb{R}}^{k}\) with cdf \(F(x)\equiv P\bigl{(}X_{11}\leq x_{1},X_{12}\leq x_{2},\ldots,X_{1k}\leq x_{k} \bigr{)}\) where \(X_{1}=(X_{11},X_{12},\ldots,X_{1k})\) and \(x=(x_{1},x_{2},\ldots,x_{k})\in{\mathbb{R}}\). Let \(F_{n}(x)\equiv\frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x)\) be the _empirical cdf_ based on \(\{X_{i}\}_{1\leq i\leq n}\). Show that \(\sup\{|F_{n}(x)-F(x)|:x\in{\mathbb{R}}\}\to 0\) w.p. 1. (**Hint:** First prove an extension of Polya's theorem (Lemma 8.2.6) to the multivariate case.)Convergence in Distribution

### 9.1 Definitions and basic properties

In this section, the notion of 'convergence in distribution' of a sequence of random variables is discussed. The importance and usefulness of this notion lie in the following observation: if a sequence of random variables \(X_{n}\) converges in distribution to a random variable \(X\), then one may approximate the probabilities \(P(X_{n}\in A)\) by \(P(X\in A)\) for large \(n\) for a large class of sets \(A\in\mathcal{B}(\mathbb{R})\). In many situations, exact evaluation of \(P(X_{n}\in A)\) is a more difficult task than the evaluation of \(P(X\in A)\). As a result, one may work with the limiting value \(P(X\in A)\) instead of \(P(X_{n}\in A)\), when \(n\) is large. As an example, consider the following problem from statistical inference. Let \(Y_{1},Y_{2},\ldots\) be a collection of iid random variables with a finite second moment. Suppose that one is interested in finding the observed level of significance or the \(p\)-value for a statistical test of the hypotheses \(H_{0}:\mu=0\) against an alternative \(H_{1}:\mu\neq 0\) about the population mean \(\mu\). If the test statistic \(\bar{Y}_{n}=n^{-1}\sum_{i=1}^{n}Y_{i}\) is used and the test rejects \(H_{0}\) for large values of \(|\sqrt{n}\bar{Y}_{n}|\), then the \(p\)-value of the test can be found using the function \(\psi_{n}(a)\equiv P_{0}(|\sqrt{n}\bar{Y}_{n}|>a)\), \(a\in[0,\infty)\), where \(P_{0}\) denotes the joint distribution of \(\{Y_{n}\}_{n\geq 1}\) under \(\mu=0\). Note that here, finding \(\psi_{n}(\cdot)\) is difficult, as it depends on the joint distribution of \(Y_{1},\ldots,Y_{n}\). If, however, one knows that under \(\mu=0\), \(\sqrt{n}\bar{Y}_{n}\) converges in distribution to a normal random variable \(Z\) (which is in fact guaranteed by the central limit theorem, see Chapter 11), then one may approximate \(\psi_{n}(a)\) by \(P(|Z|>a)\), which can be found, e.g., by using a table of normal probabilities.

The formal definition of 'convergence in distribution' is given below.

**Definition 9.1.1:** Let \(X_{n}\), \(n\geq 0\) be a collection of random variables and let \(F_{n}\) denote the cdf of \(X_{n}\), \(n\geq 0\). Then, \(\{X_{n}\}_{n\geq 1}\) is said to _converge in distribution to \(X_{0}\)_, written as \(X_{n}\longrightarrow^{d}X_{0}\), if

\[\lim_{n\to\infty}F_{n}(x)=F_{0}(x)\quad\mbox{for every}\quad x\in C(F_{0}) \tag{1.1}\]

where \(C(F_{0})=\{x\in\mathbb{R}:F_{0}\mbox{ is continuous at }x\}\).

**Definition 9.1.2:** Let \(\{\mu_{n}\}_{n\geq 0}\) be probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Then \(\{\mu_{n}\}_{n\geq 1}\) is said to _converge_ to \(\mu_{0}\)_weakly_ or _in distribution_, denoted by \(\mu_{n}\longrightarrow^{d}\mu_{0}\), if (1.1) holds with \(F_{n}(x)\equiv\mu_{n}\big{(}(-\infty,x]\big{)}\), \(x\in\mathbb{R}\), \(n\geq 0\).

Unlike the notions of convergence in probability and convergence almost surely, the notion of convergence in distribution does not require that the random variables \(X_{n}\), \(n\geq 0\) be defined on a common probability space. Indeed, for each \(n\geq 0\), \(X_{n}\) may be defined on a different probability space \((\Omega_{n},\mathcal{F}_{n},P_{n})\) and \(\{X_{n}\}_{n\geq 1}\) may converge in distribution to \(X_{0}\). In such a context, the notions of convergence of \(\{X_{n}\}_{n\geq 1}\) to \(X_{0}\) in probability or almost surely are not well defined. Definition 9.1.1 requires only the cdfs of \(X_{n}\)'s to converge to that of \(X_{0}\) at each \(x\in C(F_{0})\subset\mathbb{R}\), but does not require the (almost sure or in probability) convergence of the random variables \(X_{n}\)'s themselves.

**Example 9.1.1:** For \(n\geq 1\), let \(X_{n}\sim\mbox{Uniform }(0,\frac{1}{n})\), i.e., \(X_{n}\) has the cdf

\[F_{n}(x)=\left\{\begin{array}{ccc}0&\mbox{if}&x\leq 0\\ nx&\mbox{if}&0<x<\frac{1}{n}\\ 1&\mbox{if}&x\geq\frac{1}{n}\end{array}\right.\]

and let \(X_{0}\) be the degenerate random variable taking the value \(0\) with probability \(1\), i.e., the cdf of \(X_{0}\) is

\[F_{0}(x)=\left\{\begin{array}{ccc}0&\mbox{if}&x<0\\ 1&\mbox{if}&x\geq 0.\end{array}\right.\]

Note that the function \(F_{0}(x)\) is discontinuous only at \(x=0\). Hence, \(C(F_{0})=\mathbb{R}\backslash\{0\}\). It is easy to verify that for every \(x\neq 0\),

\[F_{n}(x)\to F_{0}(x)\quad\mbox{as}\quad n\to\infty.\]

Hence, \(X_{n}\longrightarrow^{d}X_{0}\).

**Example 9.1.2:** Let \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) be sequences of real numbers such that \(0<b_{n}<\infty\) for all \(n\geq 1\). Let \(X_{n}\sim N(a_{n},b_{n})\), \(n\geq 1\). Then, the cdf of \(X_{n}\) is given by

\[F_{n}(x)=\Phi\Big{(}\frac{x-a_{n}}{b_{n}}\Big{)},\ x\in\mathbb{R} \tag{1.2}\]where \(\Phi(x)=\int_{-\infty}^{x}\phi(t)dt\) and \(\phi(x)=(2\pi)^{-1/2}\exp(-x^{2}/2)\), \(x\in\mathbb{R}\). If \(X_{0}\sim N(a_{0},b_{0})\) for some \(a_{0}\in\mathbb{R}\), \(b_{0}\in[0,\infty)\), then using (1.2), one can show that \(X_{n}\longrightarrow^{d}X_{0}\) if and only if \(a_{n}\to a_{0}\) and \(b_{n}\to b_{0}\) as \(n\to\infty\) (Problem 9.8).

Next some simple implications of Definition 9.1.1 are considered.

**Proposition 9.1.1:** _If \(X_{n}\longrightarrow_{p}X_{0}\), then \(X_{n}\longrightarrow^{d}X_{0}\)._

**Proof:** Let \(F_{n}\) denote the cdf of \(X_{n}\), \(n\geq 0\). Fix \(x\in C(F_{0})\). Then, for any \(\epsilon>0\),

\[P(X_{n}\leq x) \leq P(X_{0}\leq x+\epsilon)+P(X_{n}\leq x,X_{0}>x+\epsilon) \tag{1.3}\] \[\leq P(X_{0}\leq x+\epsilon)+P(|X_{n}-X_{0}|>\epsilon)\]

and similarly,

\[P(X_{n}\leq x)\geq P(X_{0}\leq x-\epsilon)-P(|X_{n}-X_{0}|>\epsilon). \tag{1.4}\]

Hence, by (1.3) and (1.4),

\[F_{0}(x-\epsilon)-P(|X_{n}-X_{0}|>\epsilon)\leq F_{n}(x)\leq F_{0}(x+\epsilon) +P(|X_{n}-X_{0}|>\epsilon).\]

Since \(X_{n}\longrightarrow_{p}X_{0}\), letting \(n\to\infty\), one gets

\[F_{0}(x-\epsilon)\leq\liminf_{n\to\infty}F_{n}(x)\leq\limsup_{n\to\infty}F_{n}( x)\leq F_{0}(x+\epsilon) \tag{1.5}\]

for all \(\epsilon\in(0,\infty)\). Note that as \(x\in C(F_{0})\), \(F_{0}(x-)=F_{0}(x)\). Hence, letting \(\epsilon\downarrow 0\) in (1.5), one has \(\lim_{n\to\infty}F_{n}(x)=F_{0}(x)\). This proves the result. \(\Box\)

As pointed out before, the converse of Proposition 9.1.1 is false in general. The following is a partial converse. The proof follows from the definitions of convergence in probability and convergence in distribution and is left as an exercise (Problem 9.1).

**Proposition 9.1.2:** _If \(X_{n}\longrightarrow^{d}X_{0}\) and \(P(X_{0}=c)=1\) for some \(c\in\mathbb{R}\), then \(X_{n}\longrightarrow_{p}c\)._

**Theorem 9.1.3:** _Let \(X_{n}\), \(n\geq 0\) be a collection of random variables with respective cdfs \(F_{n}\), \(n\geq 0\). Then, \(X_{n}\longrightarrow^{d}X_{0}\) if and only if there exists a dense set \(D\) in \(\mathbb{R}\) such that_

\[\lim_{n\to\infty}F_{n}(x)=F_{0}(x)\quad\mbox{for all}\quad x\in D. \tag{1.6}\]

**Proof:** Since \(C(F_{0})^{c}\) has at most countably many points, the 'only if' part follows. To prove the 'if' part, suppose that (1.6) holds. Fix \(x\in C(F_{0})\). Then, there exist sequences \(\{x_{n}\}_{n\geq 1}\), \(\{y_{n}\}_{n\geq 1}\) in \(D\) such that \(x_{n}\uparrow x\) and \(y_{n}\downarrow x\) as \(n\to\infty\). Hence, for any \(k\), \(n\in\mathbb{N}\),

\[F_{n}(x_{k})\leq F_{n}(x)\leq F_{n}(y_{k}).\]By (1.6), for every \(k\in\mathbb{N}\),

\[F_{0}(x_{k}) = \lim_{n\to\infty}F_{n}(x_{k})\leq\liminf_{n\to\infty}F_{n}(x) \tag{1.7}\] \[\leq \limsup_{n\to\infty}F_{n}(x)\leq\lim_{n\to\infty}F_{n}(y_{k})=F_{0 }(y_{k}).\]

Since \(x\in C(F_{0})\), \(\lim_{k\to\infty}F_{0}(x_{k})=F_{0}(x)=\lim_{k\to\infty}F_{0}(y)\). Hence, by (1.7), \(\lim_{n\to\infty}F_{n}(x)\) exists and equals \(F_{0}(x)\). This completes the proof of Theorem 9.1.3. \(\Box\)

**Theorem 9.1.4:** (_Polya's theorem_). _Let \(X_{n}\), \(n\geq 0\) be random variables with respective cdfs \(F_{n}\), \(n\geq 0\). If \(F_{0}\) is continuous on \(\mathbb{R}\), then_

\[\sup_{x\in\mathbb{R}}\bigl{|}F_{n}(x)-F_{0}(x)\bigr{|}\to 0\quad\mbox{as} \quad n\to\infty.\]

**Proof:** This is a special case of Lemma 8.2.6 and uses the following proposition. \(\Box\)

**Proposition 9.1.5:** _If a cdf \(F\) is continuous on \(\mathbb{R}\), then it is uniformly continuous on \(\mathbb{R}\)._

The proof of Proposition 9.1.5 is left as an exercise (Problem 9.2).

**Theorem 9.1.6:** (_Slutsky's theorem_). _Let \(\{X_{n}\}_{n\geq 1}\) and \(\{Y_{n}\}_{n\geq 1}\) be two sequences of random variables such that for each \(n\geq 1\), \((X_{n},Y_{n})\) is defined on a probability space \((\Omega_{n},{\cal F}_{n},P_{n})\). If \(X_{n}\longrightarrow^{d}X\) and \(Y_{n}\longrightarrow_{p}a\) for some \(a\in\mathbb{R}\), then_

* \(X_{n}+Y_{n}\longrightarrow^{d}X+a\)_,_
* \(X_{n}Y_{n}\longrightarrow^{d}aX\)_, and_
* \(X_{n}/Y_{n}\longrightarrow^{d}X/a\)_, provided_ \(a\neq 0\)_._

**Proof:** Only a proof of part (i) is given here. The other parts may be proved similarly. Let \(F_{0}\) denote the cdf of \(X\). Then, the cdf of \(X+a\) is given by \(F(x)=F_{0}(x-a)\), \(x\in\mathbb{R}\). Fix \(x\in C(F)\). Then, \(x-a\in C(F_{0})\). For any \(\epsilon>0\) (as in the derivations of (1.3) and (1.4)),

\[P(X_{n}+Y_{n}\leq x)\leq P(|Y_{n}-a|>\epsilon)+P(X_{n}+a-\epsilon\leq x)\]

and

\[P(X_{n}+Y_{n}\leq x)\geq P(X_{n}+a+\epsilon\leq x)-P(|Y-a|>\epsilon).\]

Now fix \(\epsilon>0\) such that \(x-a-\epsilon\), \(x-a+\epsilon\in C(F_{0})\). This is possible since \(\mathbb{R}\backslash C(F_{0})\) is countable. Then, from (1.8) and (1.9), it follows that

\[\limsup_{n\to\infty}P(X_{n}+Y_{n}\leq x)\]\[\leq \lim_{n\to\infty}\Big{[}P((Y_{n}-a)>\epsilon)+P(X_{n}\leq x-a+ \epsilon)\Big{]}\] \[= F_{0}(x-a+\epsilon) \tag{1.10}\]

and similarly,

\[\liminf_{n\to\infty}P(X_{n}+Y_{n}\leq x)\geq F_{0}(x-a-\epsilon). \tag{1.11}\]

Now letting \(\epsilon\to 0+\) in such a way that \(x-a\pm\epsilon\in C(F_{0})\), from (1.10) and (1.11), it follows that

\[F_{0}((x-a)-) \leq \liminf_{n\to\infty}P(X_{n}+Y_{n}\leq x)\] \[\leq \limsup_{n\to\infty}P(X_{n}+Y_{n}\leq x)\] \[\leq F_{0}(x-a).\]

Since \(x-a\in C(F_{0})\), (i) is proved. \(\Box\)

### 9.2 Vague convergence, Helly-Bray theorems, and tightness

One version of the Bolzano-Weirstrass theorem from real analysis states that if \(A\subset[0,1]\) is an infinite set, then there exists a sequence \(\{x_{n}\}_{n\geq 1}\subset A\) such that \(\lim_{n\to\infty}x_{n}\equiv x\) exists in \([0,1]\). Note that \(x\) need not be in \(A\) unless \(A\) is closed. There is an analog of this for sub-probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\), i.e., for measures \(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \(\mu(\mathbb{R})\leq 1\). First, one needs a definition of convergence of sub-probability measures.

**Definition 9.2.1:** Let \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be sub-probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Then \(\{\mu_{n}\}_{n\geq 1}\) is said to converge to \(\mu\)_vaguely_, denoted by \(\mu_{n}\longrightarrow^{v}\mu\), if there exists a set \(D\subset\mathbb{R}\) such that \(D\) is dense in \(\mathbb{R}\) and

\[\mu_{n}((a,b])\to\mu((a,b])\quad\mbox{as}\quad n\to\infty\quad\mbox{for all} \quad a,b\in D. \tag{2.1}\]

**Example 9.2.1:** Let \(\{X_{n}\}_{n\geq 1}\), \(X\) be random variables such that \(X_{n}\) converges to \(X\) in distribution, i.e.,

\[F_{n}(x)\equiv P(X_{n}\leq x)\to F(x)\equiv P(X\leq x) \tag{2.2}\]

for all \(x\in C(F)\), the set of continuity points of \(F\). Since the complement of \(C(F)\) is at most countable, (2.2) implies that \(\mu_{n}\longrightarrow^{v}\mu\) where \(\mu_{n}(\cdot)\equiv P(X_{n}\in\cdot)\) and \(\mu(\cdot)\equiv P(X\in\cdot)\).

**Remark 9.2.1:** It follows from above that if \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) are probability measures, then

\[\mu_{n}\longrightarrow^{d}\mu\Rightarrow\mu_{n}\longrightarrow^{v}\mu. \tag{2.3}\]Conversely, it is not difficult to show that (Problem 9.4) if \(\mu_{n}\longrightarrow^{v}\mu\) and \(\mu_{n}\) and \(\mu\) are probability measures, then \(\mu_{n}\longrightarrow^{d}\mu\).

**Example 9.2.2:** Let \(\mu_{n}\) be the probability measure corresponding to the Uniform distribution on \([-n,n]\), \(n\geq 1\). It is easy to show that \(\mu_{n}\longrightarrow^{v}\mu_{0}\), where \(\mu_{0}\) is the measure that assigns zero mass to all Borel sets. This shows that if \(\mu_{n}\longrightarrow^{v}\mu\), then \(\mu_{n}(\mathbb{R})\) need not converge to \(\mu(\mathbb{R})\). But if \(\mu_{n}(\mathbb{R})\) does converge to \(\mu(\mathbb{R})\) and \(\mu(\mathbb{R})>0\) and if \(\mu_{n}\longrightarrow^{v}\mu\), then it can be shown that \(\mu^{\prime}_{n}\longrightarrow^{d}\mu^{\prime}\) where \(\mu^{\prime}_{n}=\frac{\mu_{n}}{\mu_{n}(\mathbb{R})}\) and \(\mu^{\prime}=\frac{\mu}{\mu(\mathbb{R})}\).

**Theorem 9.2.1:** (_Helly's selection theorem_). _Let \(A\) be an infinite collection of sub-probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Then, there exist a sequence \(\{\mu_{n}\}_{n\geq 1}\subset A\) and a sub-probability measure \(\mu\) such that \(\mu_{n}\longrightarrow^{v}\mu\)._

**Proof:** Let \(D\equiv\{r_{n}\}_{n\geq 1}\) be a countable dense set in \(\mathbb{R}\) (for example, one may take \(D=\mathbb{Q}\), the set of rationals or \(D=D_{d}\), the set of all dyadic rationals of the form \(\{j/2^{n}:j\) an integer, \(n\) a positive integer\(\}\)). Let for each \(x\), \(A(x)\equiv\{\mu((-\infty,x]):\mu\in A\}\). Then \(A(x)\subset[0,1]\) and so by the Bolzano-Weirstrass theorem applied to the set \(A(r_{1})\), one gets a sequence \(\{\mu_{1n}\}_{n\geq 1}\subset A\) such that \(\lim_{n\to\infty}F_{1n}(r_{1})\equiv F(r_{1})\) exists, where \(F_{1i}(x)\equiv\mu_{1i}((-\infty,x])\), \(x\in\mathbb{R}\). Next, applying the Bolzano-Weirstrass theorem to \(\{F_{1n}(r_{2})\}_{n\geq 1}\) yields a further subsequence \(\{\mu_{2n}\}_{n\geq 1}\subset\{\mu_{1n}\}_{n\geq 1}\subset A\) such that \(\lim_{n\to\infty}F_{2n}(r_{2})\equiv F(r_{2})\) exists, where \(F_{2i}(x)\equiv\mu_{2i}((-\infty,x])\), \(i\geq 1\). Continuing this way, one obtains a sequence of nested subsequences \(\{\mu_{jn}\}_{n\geq 1}\), \(j=1,2,\ldots\) such that for each \(j\), \(\lim_{n\to\infty}F_{jn}(r_{j})\equiv F(r_{j})\) exists. In particular, for the subsequence \(\{\mu_{nn}\}_{n\geq 1}\),

\[\lim_{n\to\infty}F_{nn}(r_{j})=F(r_{j}) \tag{2.4}\]

exists for all \(j\). Now set

\[\tilde{F}(x)\equiv\inf\{F(r):r>x,r\in D\}. \tag{2.5}\]

Then, \(\tilde{F}(\cdot)\) is a nondecreasing right continuous function on \(\mathbb{R}\) (Problem 9.5) and it equals \(F(\cdot)\) on \(D\). Let \(\mu\) be the Lebesgue-Stieltjes measure generated by \(\tilde{F}\). Since \(F_{nn}(x)\leq 1\) for all \(n\) and \(x\), it follows that \(\tilde{F}(x)\leq 1\) for all \(x\) and hence that \(\mu\) is a sub-probability measure. Suppose it is shown that (2.4) also implies that

\[\lim_{n\to\infty}F_{nn}(x)=\tilde{F}(x) \tag{2.6}\]

for all \(x\in C_{\tilde{F}}\), the set of continuity points of \(\tilde{F}\). Then, all \(a\), \(b\in C_{\tilde{F}}\), \(\mu_{nn}((a,b])\equiv F_{nn}(b)-F_{nn}(a)\to\tilde{F}(b)-\tilde{F}(a)=\mu((a,b])\) and hence that \(\mu_{n}\longrightarrow^{v}\mu\). To establish (2.6), fix \(x\in C_{\tilde{F}}\) and \(\epsilon>0\). Then, there is a \(\delta>0\) such that for all \(x-\delta<y<x+\delta\), \(\tilde{F}(x)-\epsilon<\tilde{F}(y)<\tilde{F}(x)+\epsilon\). This implies that there exist \(x-\delta<r<x<r^{\prime}<x+\delta\), \(r,r^{\prime}\in D\) and \(\tilde{F}(x)-\epsilon<F(r)\leq\tilde{F}(x)\leq F(r^{\prime})<\tilde{F}(x)+\epsilon\). Since \(F_{nn}(r)\leq F_{nn}(x)\leq F_{nn}(r^{\prime})\), it follows that

\[\tilde{F}(x)-\epsilon\leq\varliminf_{n\to\infty}F_{nn}(x)\leq\varlimsup_{n\to \infty}F_{nn}(x)\leq\tilde{F}(x)+\epsilon,\]

establishing (2.6). \(\Box\)

Next, some characterization results on vague convergence and convergence in distribution will be established. These can then be used to define the notions of convergence of sub-probability measures on more general metric spaces.

**Theorem 9.2.2:** (_The first Helly-Bray theorem or the Helly-Bray theorem for vague convergence_)_. Let \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) be sub-probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Then \(\mu_{n}\longrightarrow^{v}\mu\) iff_

\[\int fd\mu_{n}\to\int fd\mu \tag{2.7}\]

_for all \(f\in\mathcal{C}_{0}(\mathbb{R})\equiv\{g\mid g:\mathbb{R}\to\mathbb{R}\) is continuous and \(\lim_{|x|\to\infty}g(x)=0\}\)._

**Proof:** Let \(\mu_{n}\longrightarrow^{v}\mu\) and let \(f\in\mathcal{C}_{0}(\mathbb{R})\). Given \(\epsilon>0\), choose \(K\) large such that \(|f(x)|<\epsilon\) for \(|x|>K\). Since \(\mu_{n}\longrightarrow^{v}\mu\), there exists a dense set \(D\subset\mathbb{R}\) such that \(\mu_{n}((a,b])\to\mu((a,b])\) for all \(a\), \(b\in D\). Now choose \(a\), \(b\in D\) such that \(a<-K\) and \(b>K\). Since \(f\) is uniformly continuous on \([a,b]\) and \(D\) is dense in \(\mathbb{R}\), there exist points \(x_{0}=a<x_{1}<x_{2}<\cdots<x_{m}=b\) in \(D\) such that \(\sup_{x_{i}\leq x\leq x_{i+1}}|f(x)-f(x_{i})|<\epsilon\) for all \(0\leq i<m\). Now

\[\int fd\mu_{n}=\int_{(-\infty,a]}fd\mu_{n}+\sum_{i=0}^{m-1}\int_{(x_{i},x_{i+1 }]}fd\mu_{n}+\int_{(b,\infty)}fd\mu_{n}\]

and so

\[\bigg{|}\int fd\mu_{n}-\sum_{i=0}^{m-1}f(x_{i})\mu_{n}((x_{i},x_{i+1}])\bigg{|} <2\epsilon+\epsilon\cdot\mu_{n}((a,b])<3\epsilon.\]

A similar approximation holds for \(\int fd\mu\). Since \(\mu_{n}\), \(\mu\) are sub-probability measures, it follows that

\[\bigg{|}\int fd\mu_{n}-\int fd\mu\bigg{|}<6\epsilon+\|f\|\sum_{i=0}^{m}\big{|} \mu_{n}((x_{i},x_{i+1}])-\mu((x_{i},x_{i+1}])\big{|},\]

where \(\|f\|=\sup\{|f(x)|:x\in\mathbb{R}\}\). Letting \(n\to\infty\) and noting that \(\mu_{n}\longrightarrow^{v}\mu\) and \(\{x_{i}\}_{i=0}^{m}\subset D\), one gets

\[\limsup_{n\to\infty}\bigg{|}\int fd\mu_{n}-\int fd\mu\bigg{|}\leq 6\epsilon.\]Since \(\epsilon>0\) is arbitrary, (2.7) follows and the proof of the "only if" part is complete.

To prove the converse, let \(D\) be the set of points \(\{x:\mu(\{x\})=0\}\). Fix \(a\), \(b\in D\), \(a<b\). Let \(\epsilon>0\). Let \(f_{1}\) be the function defined by

\[f_{1}(x)=\left\{\begin{array}{ll}1&\mbox{if}\quad a\leq x\leq b\\ 0&\mbox{if}\quad x<a-\epsilon\quad\mbox{or}\quad x>b+\epsilon\\ \mbox{linear on}\quad a-\epsilon\leq x<a,\ b\leq x\leq b+\epsilon.\end{array}\right.\]

Then, \(f_{1}\in{\cal C}_{0}({\mathbb{R}})\) and by (2.7),

\[\int f_{1}d\mu_{n}\rightarrow\int f_{1}d\mu.\]

But \(\mu_{n}((a,b])\leq\int f_{1}d\mu_{n}\) and \(\int f_{1}d\mu\leq\mu((a-\epsilon,b+\epsilon])\). Thus, \(\limsup_{n\rightarrow\infty}\mu_{n}((a,b])\leq\mu((a-\epsilon,b+\epsilon])\). Letting \(\epsilon\downarrow 0\) and noting that \(a\), \(b\in D\), one gets

\[\limsup_{n\rightarrow\infty}\mu_{n}((a,b])\leq\mu((a,b]). \tag{2.8}\]

A similar argument with \(f_{2}=1\) on \([a+\epsilon,b-\epsilon]\) and \(0\) for \(x\leq a\) and \(\geq b\) and linear in between, yields

\[\liminf_{n\rightarrow\infty}\mu_{n}((a,b])\geq\mu((a,b]).\]

This with (2.8) completes the proof of the "if" part. \(\Box\)

**Theorem 9.2.3:** (_The second Helly-Bray theorem or the Helly-Bray theorem for weak convergence_)_. Let \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be probability measures on \(({\mathbb{R}},{\cal B}({\mathbb{R}}))\). Then, \(\mu_{n}\longrightarrow^{d}\mu\) iff_

\[\int fd\mu_{n}\rightarrow\int fd\mu \tag{2.9}\]

_for all \(f\in{\cal C}_{B}({\mathbb{R}})\equiv\{g\mid g:{\mathbb{R}}\rightarrow{\mathbb{ R}}\), \(g\) is continuous and bounded_}_._

**Proof:** Let \(\mu_{n}\longrightarrow^{d}\mu\). Let \(\epsilon>0\) and \(f\in{\cal C}_{B}({\mathbb{R}})\) be given. Choose \(K\) large such that \(\mu((-K,K])>1-\epsilon\). Also, choose \(a<-K\) and \(b>K\) such that \(\mu(\{a\})=\mu(\{b\})=0\), \(a,b\in D\). Let \(a=x_{0}<x_{1}<\ <x_{m}=b\) be chosen so that \(x_{0},\ldots,x_{m}\in D\) and

\[\sup_{x_{i}\leq x\leq x_{i+1}}|f(x)-f(x_{i})|<\epsilon\]

for all \(i=1,\ldots,m-1\). Since \(\int fd\mu_{n}-\int fd\mu=\int_{(-\infty,a]}fd\mu_{n}-\int_{(-\infty,a]}fd\mu+\sum _{i=1}^{m-1}(\int_{(x_{i},x_{i+1}]}fd\mu_{n}-\int_{(x_{i},x_{i+1}]}fd\mu)+\int_ {(b,\infty)}fd\mu_{n}-\int_{(b,\infty)}fd\mu\), it follows that

\[\left|\int fd\mu_{n}-\int fd\mu\right| < \|f\|\Big{[}\big{(}\mu_{n}((-\infty,a])+\mu((-\infty,a])\big{)}\]9.2 Vague convergence, Helly-Bray theorems, and tightness

\[\eqalign{&+\sum_{i=0}^{m-1}\big{|}\mu_{n}((x_{i},x_{i+1}])-\mu((x_{i},x_{i+1}]) \big{|}\cr&+\mu_{n}((b,\infty))+\mu((b,\infty))\Big{]}.\cr}\]

Since, \(a,\,b,\,x_{0},x_{1},\ldots,x_{m}\in D,\)

\[\limsup_{n\to\infty}\bigg{|}\int fd\mu_{n}-\int fd\mu\bigg{|}\leq\|f\|2(1-\mu( (a,b]))\leq\|f\|2\epsilon.\]

Since \(\epsilon>0\) is arbitrary, the "only if" part is proved.

Next consider the "if" part. Since \({\cal C}_{0}({\mathbb{R}})\subset{\cal C}_{B}({\mathbb{R}}),\) (2.9) and Theorem 9.2.2 imply that \(\mu_{n}\longrightarrow^{v}\mu.\) As noted in Remark 9.2.1, if \(\{\mu_{n}\}_{n\geq 1},\,\mu\) are probability measures then \(\mu_{n}\longrightarrow^{v}\mu\) iff \(\mu_{n}\longrightarrow^{d}\mu.\) So the proof is complete. \(\Box\)

**Definition 9.2.2:**

* A sequence of probability measures \(\{\mu_{n}\}_{n\geq 1}\) on \(({\mathbb{R}},{\cal B}({\mathbb{R}}))\) is called _tight_ if for any \(\epsilon>0,\) there exists \(M=M_{\epsilon}\in(0,\infty)\) such that \[\sup_{n\geq 1}\mu_{n}\big{(}[-M,M]^{c}\big{)}<\epsilon.\] (2.10)
* A sequence of random variables \(\{X_{n}\}_{n\geq 1}\) is called _tight_ or _stochastically bounded_ if the sequence of probability distributions \(\{\mu_{n}\}_{n\geq 1}\) of \(\{X_{n}\}_{n\geq 1}\) is tight, i.e., given any \(\epsilon>0,\) there exists \(M=M_{\epsilon}\in(0,\infty)\) such that \[\sup_{n\geq 1}P(|X_{n}|>M)<\epsilon.\] (2.11)

**Remark 9.2.3:** In Definition 9.2.2 (b), the random variables \(X_{n},\,n\geq 1\) need not be defined on a common probability space. If \(X_{n}\) is defined on a probability space \((\Omega_{n},{\cal F}_{n},P_{n}),\,n\geq 1,\) then (2.11) needs to be replaced by

\[\sup_{n\geq 1}P_{n}(|X_{n}|>M)<\epsilon.\]

**Example 9.2.3:** Let \(X_{n}\sim\hbox{Uniform}(n,n+1).\) Then, for any given \(M\in(0,\infty),\)

\[P(|X_{n}|>M)\geq P(X_{n}>M)=1\quad\hbox{for all}\quad n>M.\]

Consequently, for any \(M\in(0,\infty),\)

\[\sup_{n\geq 1}P(|X_{n}|>M)=1\]

and the sequence \(\{X_{n}\}_{n\geq 1}\) cannot be stochastically bounded.

**Example 9.2.4:** For \(n\geq 1\), let

\[X_{n}\sim\mbox{Uniform}(a_{n},2+a_{n}), \tag{2.12}\]

where \(a_{n}=(-1)^{n}\). Then, \(\{X_{n}\}_{n\geq 1}\) is stochastically bounded. Indeed, \(|X_{n}|\leq 3\) for all \(n\geq 1\) and therefore, for any \(\epsilon>0\), (2.11) holds with \(M=3\). Note that in this example, the sequence \(\{X_{n}\}_{n\geq 1}\) does not converge in distribution to a random variable \(X\). From (2.12), it follows that as \(k\to\infty\),

\[X_{2k}\longrightarrow^{d}\mbox{Uniform}(1,3),\ X_{2k-1}\longrightarrow^{d} \mbox{Uniform}(-1,1). \tag{2.13}\]

Examples 9.2.3 and 9.2.4 highlight two important characteristics of a tight sequence of random variables or probability measures. First, the notion of tightness of probability measures or random variables is analogous to the notion of boundedness of a sequence of real numbers. For a sequence of bounded real numbers \(\{x_{n}\}_{n\geq 1}\), all the \(x_{n}\)'s must lie in a bounded interval \([-M,M]\), \(M\in(0,\infty)\). For a sequence of random variables \(\{X_{n}\}_{n\geq 1}\), the condition of tightness requires that given \(\epsilon>0\) arbitrarily small, there exists an \(M=M_{\epsilon}\) in \((0,\infty)\) such that for each \(n\), \(X_{n}\) lies in \([-M,M]\) with probability at least \(1-\epsilon\). Thus, for a tight sequence of random variables, no positive mass can escape to \(\pm\infty\), which is contrary to what happens with the random variables \(\{X_{n}\}_{n\geq 1}\) of Example 9.2.3.

The second property illustrated by Example 9.2.4 is that like a bounded sequence of real numbers, a tight or stochastically bounded sequence of random variables may not converge in distribution, but has one or more convergent subsequences (cf. (2.13)). Indeed, the notion of tightness can be characterized by this property, as shown by the following result. For consistency with the other results in this section, it is stated in terms of probability measures instead of random variables.

**Theorem 9.2.4:** _Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of probability measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). The sequence \(\{\mu_{n}\}_{n\geq 1}\) is tight iff given any subsequence \(\{\mu_{n_{i}}\}_{i\geq 1}\) of \(\{\mu_{n}\}_{n\geq 1}\), there exists a further subsequence \(\{\mu_{m_{i}}\}_{i\geq 1}\) of \(\{\mu_{n_{i}}\}_{i\geq 1}\) and a probability measure \(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that_

\[\mu_{m_{i}}\longrightarrow^{d}\mu\quad\mbox{as}\quad i\to\infty. \tag{2.14}\]

**Proof:** Suppose that \(\{\mu_{n}\}_{n\geq 1}\) is tight. Given any subsequence \(\{\mu_{n_{i}}\}_{i\geq 1}\) of \(\{\mu_{n}\}_{n\geq 1}\), by Helly's selection theorem (Theorem 9.2.1), there exists a sub-probability measure \(\mu\) and a further subsequence \(\{\mu_{m_{i}}\}_{i\geq 1}\) of \(\{\mu_{n_{i}}\}_{i\geq 1}\) such that

\[\mu_{m_{i}}\longrightarrow^{v}\mu. \tag{2.15}\]

Next, fix \(\epsilon\in(0,1)\). Since \(\{\mu_{n}\}_{n\geq 1}\) is tight, there exists \(M\in(0,\infty)\) such that

\[\sup_{n\geq 1}\mu_{n}\big{(}[-M,M]^{c}\big{)}<\epsilon. \tag{2.16}\]

[MISSING_PAGE_EMPTY:309]

Let \(M=\max\{M_{i}:0\leq i\leq n_{0}\}\), where \(M_{0}=\max\{|a|,|b|\}\). Then by (2.19) and (2.20),

\[\sup_{n\geq 1}\mu_{n}\bigl{(}[-M,M]^{c}\bigr{)}<\epsilon.\]

Thus, \(\{\mu_{n}\}_{n\geq 1}\) is tight. \(\Box\)

An easy consequence of Theorems 9.2.4 and 9.2.5 is the following characterization of weak convergence.

**Theorem 9.2.6:**_Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of probability measures on \(({\mathbb{R}},{\cal B}({\mathbb{R}}))\). Then \(\mu_{n}\longrightarrow^{d}\mu\) iff \(\{\mu_{n}\}_{n\geq 1}\) is tight and all weakly convergent subsequences of \(\{\mu_{n}\}_{n\geq 1}\) converge to the same limiting probability measure \(\mu\)._

**Proof:** If \(\mu_{n}\longrightarrow^{d}\mu\), then any weakly convergent subsequence of \(\{\mu_{n}\}_{n\geq 1}\) converges to \(\mu\) and by Theorem 9.2.5, \(\{\mu_{n}\}_{n\geq 1}\) is tight. Hence, the 'only if' part follows. To prove the 'if' part, suppose that \(\{\mu_{n}\}_{n\geq 1}\) is tight and that all weakly convergent subsequences of \(\{\mu_{n}\}_{n\geq 1}\) converges to \(\mu\). Let \(\{F_{n}\}_{n\geq 1}\) and \(F\) denote the cdfs corresponding to \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\), respectively. If possible, suppose that \(\{\mu_{n}\}_{n\geq 1}\) does not converge in distribution to \(\mu\). Then, by definition, there exists \(x_{0}\in{\mathbb{R}}\) with \(\mu\bigl{(}\{x_{0}\}\bigr{)}=0\) such that \(F_{n}(x_{0})\) does not converge to \(F(x_{0})\) as \(n\to\infty\). Then, there exist \(\epsilon_{0}\in(0,1)\) and a subsequence \(\{n_{i}\}_{i\geq 1}\) such that

\[\bigl{|}F_{n_{i}}(x_{0})-F(x_{0})\bigr{|}\geq\epsilon_{0}\quad\mbox{for all} \quad i\geq 1.\]

Since \(\{\mu_{n}\}_{n\geq 1}\) is tight, there exists a subsequence \(\{m_{i}\}_{i\geq 1}\subset\{n_{i}\}_{i\geq 1}\) and a probability measure \(\mu_{0}\) such that

\[\mu_{m_{i}}\longrightarrow^{d}\mu_{0}\quad\mbox{as}\quad i\to\infty.\]

By hypothesis, \(\mu_{0}=\mu\). Hence \(\mu_{0}(\{x_{0}\})=\mu\bigl{(}\{x_{0}\}\bigr{)}=0\) and by (2.22),

\[F_{m_{i}}(x_{0})\to F(x_{0})\quad\mbox{as}\quad i\to\infty,\]

contradicting (2.21). Therefore, \(\mu_{n}\longrightarrow^{d}\mu\). \(\Box\)

For another proof of the 'if' part, see Problem 9.6.

Note that by Slutsky's theorem, if \(X_{n}\longrightarrow^{d}X\) and \(Y_{n}\longrightarrow_{p}0\), then \(X_{n}Y_{n}\longrightarrow_{p}0\). The following result gives a refinement of this.

**Proposition 9.2.7:**_If \(\{X_{n}\}_{n\geq 1}\) is stochastically bounded and \(Y_{n}\longrightarrow_{p}0\), then \(X_{n}Y_{n}\longrightarrow_{p}0\)._

The proof is left as an exercise (Problem 9.7).

### 9.3 Weak convergence on metric spaces

The Helly-Bray theorems proved above suggest the following definitions of vague convergence and convergence in distribution for measures on metric spaces. Recall that \((\mathbb{S},d)\) is called a _metric space_, if \(\mathbb{S}\) is a nonempty set and \(d\) is a function from \(\mathbb{S}\times\mathbb{S}\to[0,\infty)\) such that

* \(d(x,y)=d(y,x)\) for all \(x,y\in\mathbb{S}\),
* \(d(x,y)=0\) iff \(x=y\) for all \(x,y\in\mathbb{S}\),
* \(d(x,z)\leq d(x,y)+d(y,z)\) for all \(x,y,z\in\mathbb{S}\).

A common example of a metric space is given by \(\mathbb{S}=\mathbb{R}^{k}\) and \(d(x,y)\), the Euclidean distance. A set \(G\subset\mathbb{S}\) is _open_ if for all \(x\in G\), there exists an \(\epsilon>0\) such that for any \(y\) in \(\mathbb{S}\), \(d(x,y)<\epsilon\Rightarrow y\in G\). The set

\[B(x,\epsilon)=\{y:d(x,y)<\epsilon\}\]

is called the _open ball_ of radius \(\epsilon\) with center at \(x\), \(x\in\mathbb{S}\), \(\epsilon>0\). Recall that \(f:\mathbb{S}\to\mathbb{R}\) is _continuous_ if \(f^{-1}((a,b))\) is open for every \(-\infty<a<b<\infty\). A family \(\mathcal{G}\) of open sets in \(\mathbb{S}\) is called an _open cover_ for a set \(B\subset\mathbb{S}\) if for each \(x\in B\), there exists a \(G\in\mathcal{G}\) such that \(x\in G\). A set \(K\subset\mathbb{S}\) is called _compact_ if given any open cover \(\mathcal{G}\) for \(K\), there is a finite subfamily \(\mathcal{G}_{1}\subset\mathcal{G}\) such that \(\mathcal{G}_{1}\) is an open cover for \(K\).

Let \(\mathcal{S}\) be the _Borel \(\sigma\)-algebra_ on \(\mathbb{S}\), i.e., let \(\mathcal{S}\) be the \(\sigma\)-algebra generated by the open sets in \(\mathbb{S}\). A measure on the measurable space \((\mathbb{S},\mathcal{S})\) is often simply referred to as a measure on \((\mathbb{S},d)\).

**Definition 9.3.1:** Let \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) be sub-probability measures on a metric space \((\mathbb{S},d)\), i.e., \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) are measures on \((\mathbb{S},\mathcal{S})\) such that \(\mu_{n}(\mathbb{S})\leq 1\) for all \(n\geq 1\) and \(\mu(\mathbb{S})\leq 1\). Then \(\{\mu_{n}\}_{n\geq 1}\)_converges vaguely to_\(\mu\) (written as \(\mu_{n}\longrightarrow^{v}\mu\)) if

\[\int fd\mu_{n}\to\int fd\mu \tag{3.1}\]

for all \(f\in\mathcal{C}_{0}(\mathbb{S})\), where \(\mathcal{C}_{0}(\mathbb{S})\equiv\{f\mid f:\mathbb{S}\to\mathbb{R}\), \(f\) is continuous and for every \(\epsilon>0\), there exists a compact set \(K\) such that \(|f(x)|<\epsilon\) for all \(x\not\in K\}\).

**Definition 9.3.2:** Let \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) be probability measures on a metric space \((\mathbb{S},d)\). Then \(\{\mu_{n}\}_{n\geq 1}\)_converges in distribution_ or _converges weakly_ to \(\mu\) (written as \(\mu_{n}\longrightarrow^{d}\mu\)) if

\[\int fd\mu_{n}\to\int fd\mu \tag{3.2}\]

for all \(f\in C_{B}(\mathbb{S})\equiv\{f\mid f:\mathbb{S}\to\mathbb{R}\), \(f\) is continuous and bounded\(\}\).

Recall that a sequence \(\{x_{n}\}_{n\geq 1}\) in a metric space \((\mathbb{S},d)\) is called _Cauchy_ if for every \(\epsilon>0\), there exists \(N_{\epsilon}\) such that \(n\), \(m>N_{\epsilon}\Rightarrow d(x_{n},x_{m})<\epsilon\). A metric space \((\mathbb{S},d)\) is _complete_ if every Cauchy sequence \(\{x_{n}\}_{n\geq 1}\) in \(\mathbb{S}\) converges in \(\mathbb{S}\), i.e., given a Cauchy sequence \(\{x_{n}\}_{n\geq 1}\), there exists an \(x\) in \(\mathbb{S}\) such that \(d(x_{n},x)\to 0\) as \(n\to\infty\).

**Example 9.3.1:** For any \(k\in\mathbb{N}\), \(\mathbb{R}^{k}\) with the Euclidean metric is complete but the set of all rational vectors \(\mathbb{Q}^{k}\) with the Euclidean metric \(d(x,y)\equiv\|x-y\|\) is not _complete_. The set \(C[0,1]\) of all continuous functions on \([0,1]\) is complete with the _supremum metric_\(d(f,g)=\sup\{|f(u)-g(u)|:0\leq u\leq 1\}\) but the set of all polynomials on \([0,1]\) is not complete under the same metric.

Recall that a set \(D\) is called _dense_ in \((\mathbb{S},d)\) if \(B(x,\epsilon)\cap D\neq\emptyset\) for all \(x\in\mathbb{S}\) and for all \(\epsilon>0\), where \(B(x,\epsilon)\) is the open ball with center at \(x\) and radius \(\epsilon\). Also, \((\mathbb{S},d)\) is called _separable_ if there exists a countable dense set \(D\subset\mathbb{S}\).

**Definition 9.3.3:** A metric space \((\mathbb{S},d)\) is called _Polish_ if it is complete and separable.

**Example 9.3.2:** All Euclidean spaces with the Euclidean metric as well as with the \(L^{p}\) metric for \(1\leq p\leq\infty\), are complete. The space \(\mathcal{C}[0,1]\) of continuous functions on [0,1] with the supremum metric is complete. All \(L^{p}\)-spaces over measure spaces with a \(\sigma\)-finite measure and a countably generated \(\sigma\)-algebra, \(1\leq p\leq\infty\), are complete (cf. Chapter 3).

The following theorem gives several equivalent conditions for weak convergence of probability measures on a Polish space.

**Theorem 9.3.1:** _Let \((\mathbb{S},d)\) be Polish and \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be probability measures. Then the following are equivalent:_

* \(\mu_{n}\longrightarrow^{d}\mu\)_._
* _For any open set_ \(G\)_,_ \(\liminf_{n\to\infty}\mu_{n}(G)\geq\mu(G)\)_._
* _For any closed set_ \(C\)_,_ \(\limsup_{n\to\infty}\mu_{n}(C)\leq\mu(C)\)_._
* _For all_ \(B\in\mathcal{S}\) _such that_ \(\mu(\partial B)=0\)_,_ \[\lim_{n\to\infty}\mu_{n}(B)=\mu(B),\] _where_ \(\partial B\) _is the_ boundary _of_ \(B\)_, i.e.,_ \(\partial B=\{x:\) _for all_ \(\epsilon>0\)_,_ \(B(x,\epsilon)\cap B\neq\emptyset\)_,_ \(B(x,\epsilon)\cap B^{c}\neq\emptyset\}\)_._
* _For every uniformly continuous and bounded function_ \(f:\mathbb{S}\to\mathbb{R}\)_,_ \(\int fd\mu_{n}\to\int fd\mu\)The proof uses the following fact.

**Proposition 9.3.2:**_For every open set \(G\) in a metric space \((\mathbb{S},d)\), there exists a sequence \(\{f_{n}\}_{n\geq 1}\) of bounded continuous functions from \(\mathbb{S}\) to [0,1] such that as \(n\uparrow\infty\), \(f_{n}(x)\uparrow I_{G}(x)\) for all \(x\in\mathbb{S}\)._

**Proof:** Let \(G_{n}\equiv\{x:d(x,G^{c})>\frac{1}{n}\}\) where for any set \(A\) in \((\mathbb{S},d)\), \(d(x,A)\equiv\inf\{d(x,y):y\in A\}\). Then since \(G\) is open, \(d(x,G^{c})>0\) for all \(x\) in \(G\). Thus \(G_{n}\uparrow G\). Let for each \(n\geq 1\),

\[f_{n}(x)\equiv\frac{d(x,G^{c})}{d(x,G^{c})+d(x,G_{n})},\ x\in\mathbb{S}. \tag{3.3}\]

Check that (Problem 9.10) for each \(n\), \(f_{n}(x)\) is continuous on \(\mathbb{S}\), \(f_{n}(x)=1\) on \(G_{n}\) and \(0\) on \(G^{c}\), \(0\leq f_{n}(x)\leq 1\) for all \(x\) in \(\mathbb{S}\). Further \(f_{n}(\cdot)\uparrow I_{G}(\cdot)\). \(\Box\)

**Proof of Theorem 9.3.1**: _(i) \(\Rightarrow\) (ii)_: Let \(G\) be open. Choose \(\{f_{n}\}_{n\geq 1}\) as in Proposition 9.3.2. Then for \(j\in\mathbb{N}\),

\[\mu_{n}(G)\geq\int f_{j}d\mu_{n}\Rightarrow\liminf_{n\to\infty}\ \mu_{n}(G) \geq\liminf_{n\to\infty}\int f_{j}d\mu_{n}=\int f_{j}d\mu\]

(by (i)). But \(\lim_{j\to\infty}\int f_{j}d\mu=\mu(G)\), by the bounded convergence theorem. Hence (ii) holds.

_(ii) \(\Leftrightarrow\) (iii)_: Suppose (ii) holds. Let \(C\) be closed. Then \(G=C^{c}\) is open. So by (ii),

\[\liminf_{n\to\infty}\ \mu_{n}(C^{c})\geq\mu(C^{c})\Rightarrow\limsup_{n\to \infty}\ \mu_{n}(C)\leq\mu(C),\]

since \(\mu_{n}\) and \(\mu\) are probability measures. Thus, (iii) holds. Similarly, (iii) \(\Rightarrow\) (ii).

_(iii) \(\Rightarrow\) (iv)_: For any \(B\in\mathbb{S}\), let \(B^{0}\) and \(\bar{B}\) denote, respectively, the interior and the closure of \(B\). That is, \(B^{0}=\{y:B(y,\epsilon)\subset B\) for some \(\epsilon>0\}\) and \(\bar{B}=\{y:\) for some \(\{x_{n}\}_{n\geq 1}\subset B\), \(\lim_{n\to\infty}x_{n}=y\}\). Then, for any \(n\geq 1\),

\[\mu_{n}(B^{0})\leq\mu_{n}(B)\leq\mu_{n}(\bar{B})\]

and by (ii) and (iii),

\[\mu(B^{0})\leq\liminf_{n\to\infty}\ \mu_{n}(B)\leq\limsup_{n\to\infty}\ \mu_{n}(B)\leq\mu(\bar{B}).\]

But \(\partial B=\bar{B}\setminus B^{0}\) and so \(\mu(\partial B)=0\) implies \(\mu(B^{0})=\mu(\bar{B})\). Thus, \(\lim_{n\to\infty}\mu_{n}(B)=\mu(B)\).

_(iv) \(\Rightarrow\) (v) \(\Rightarrow\) (i)_: This will be proved for the case where \(\mathbb{S}\) is the real line. For the general Polish case, see Billingsley (1968). Let \(F(x)\equiv\mu((-\infty,x])\) and \(F_{n}(x)\equiv\mu_{n}((-\infty,x])\), \(x\in\mathbb{R}\), \(n\geq 1\). Let \(x\) be a continuity point of \(F\). Then \(\mu(\{x\})=0\). Since if \(B=(-\infty,x]\), then \(\partial B=\{x\}\), by (iv),

\[F_{n}(x)=\mu_{n}((-\infty,x])\to\mu((-\infty,x])=F(x).\]Thus, \(\mu_{n}\longrightarrow^{d}\mu\). By Theorem 9.2.3, (i) holds and hence (v) holds.

_(v) \(\Rightarrow\) (i)_: Note that in the proof of Theorem 9.2.2, the approximating functions \(f_{1}\) and \(f_{2}\) were both uniformly continuous. Hence, the assertion follows from Theorem 9.2.2 and Remark 9.2.1. This completes the proof of Theorem 9.3.1. \(\Box\)

The following example shows that the inequality can be strict in (ii) and (iii) of the above theorem.

**Example 9.3.3:** Let \(X\) be a random variable. Set \(X_{n}=X+\frac{1}{n}\), \(Y_{n}=X-\frac{1}{n}\), \(n\geq 1\). Since \(X_{n}\) and \(Y_{n}\) both converge to \(X\) w.p. 1, the distributions of \(X_{n}\) and \(Y_{n}\) converge to that of \(X\).

Now suppose that there is a value \(x_{0}\) such that \(P(X=x_{0})>0\). Then,

\[\mu_{n}\big{(}(-\infty,x_{0})\big{)} \equiv P(X_{n}<x_{0})\] \[= P\Big{(}X<x_{0}-\frac{1}{n}\Big{)}\] \[\rightarrow P(X<x_{0})=\mu\big{(}(-\infty,x_{0})\big{)},\]

\[\mu_{n}\big{(}(-\infty,x_{0}]\big{)} = P(X_{n}\leq x_{0})\] \[= P\Big{(}X\leq x_{0}-\frac{1}{n}\Big{)}\to P(X<x_{0})<\mu \big{(}(-\infty,x_{0}]\big{)},\]

and

\[\nu_{n}\big{(}(-\infty,x_{0})\big{)} \equiv P(Y_{n}<x_{0})\] \[= P\Big{(}X<x_{0}+\frac{1}{n}\Big{)}\to P(X\leq x_{0})>P(X<x_{0}).\]

Note that \(\mu_{n}\) and \(\nu_{n}\) both converge in distribution to \(\mu\). However, for the closed set \((-\infty,x_{0}]\),

\[\limsup_{n\rightarrow\infty}\ \mu_{n}\big{(}(-\infty,x_{0}]\big{)}<\mu\big{(}(- \infty,x_{0}]\big{)}\]

and for the open set \((-\infty,x_{0})\),

\[\liminf_{n\rightarrow\infty}\ \nu_{n}\big{(}(-\infty,x_{0})\big{)}>\mu\big{(}(- \infty,x_{0})\big{)}.\]

**Remark 9.3.1:** Convergent sequences of probability distributions arise in a natural way in parametric families in mathematical statistics. For example, let \(\mu(\cdot;\theta)\) denote the normal distribution with mean \(\theta\) and variance 1. Then, \(\theta_{n}\rightarrow\theta\Rightarrow\mu_{n}(\cdot)\equiv N(\theta_{n},1) \longrightarrow^{d}N(\theta,1)\equiv\mu(\cdot)\). Similarly, let \(\theta=(\lambda,\Sigma)\), where \(\lambda\in\mathbb{R}^{k}\) and \(\Sigma\) is a \(k\times k\) positive definite matrix. Let \(\mu(\cdot;\theta)\) be the \(k\)-variate normal distribution with mean \(\lambda\) and variance covariance matrix \(\Sigma\). Then, \(\mu(\cdot;\theta)\) is continuous in \(\theta\) in the sense that if \(\theta_{n}\to\theta\) in the Euclidean metric, then \(\mu(\cdot;\theta_{n})\longrightarrow^{d}\mu(\cdot;\theta)\). Most parametric families in mathematical statistics possess this continuity property.

**Definition 9.3.4:** Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of probability measures on \((\mathbb{S},\mathcal{S})\), where \(\mathbb{S}\) is a Polish space and \(\mathcal{S}\) is the Borel \(\sigma\)-algebra on \(\mathbb{S}\). Then \(\{\mu_{n}\}_{n\geq 1}\) is called _tight_ if for any \(\epsilon>0\), there exists a compact set \(K\) such that

\[\sup_{n\geq 1}\mu_{n}(K^{c})<\epsilon. \tag{3.4}\]

A sequence of \(\mathbb{S}\)-valued random variables \(\{X_{n}\}_{n\geq 1}\) is called _tight_ or _stochastically bounded_ if the sequence \(\{\mu_{X_{n}}\}_{n\geq 1}\) is tight, where \(\mu_{X_{n}}\) is the probability distribution of \(X_{n}\) on \((\mathbb{S},\mathcal{S})\).

If \(\mathbb{S}=\mathbb{R}^{k}\), \(k\in\mathbb{N}\), and \(\{X_{n}\}_{n\geq 1}\) is a sequence of \(k\)-dimensional random vectors, then, by Definition 9.3.4, \(\{X_{n}\}_{n\geq 1}\) is tight if and only if for every \(\epsilon>0\), there exists \(M\in(0,\infty)\) such that

\[\sup_{n\geq 1}P(\|X_{n}\|>M)<\epsilon, \tag{3.5}\]

where \(\|\cdot\|\) denotes the usual Euclidean norm on \(\mathbb{R}^{k}\). Furthermore, if \(X_{n}=(X_{n1},\ldots,X_{nk})\), \(n\geq 1\), then the tightness of \(\{X_{n}\}_{n\geq 1}\) is equivalent to the tightness of the \(k\) sequences of random variables \(\{X_{nj}\}_{n\geq 1}\), \(j=1,\ldots,k\) (Problem 9.9).

An analog of Theorem 9.2.4 holds for probability measures on \((\mathbb{S},\mathcal{S})\) when \(\mathbb{S}\) is Polish.

**Theorem 9.3.3:** (_Prohorov-Varadarajan theorem_). _Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of probability measures on \((\mathbb{S},\mathcal{S})\) where \(\mathbb{S}\) is a Polish space and \(\mathcal{S}\) is the Borel \(\sigma\)-algebra on \(\mathbb{S}\). Then, \(\{\mu_{n}\}_{n\geq 1}\) is tight iff given any subsequence \(\{\mu_{n_{i}}\}_{i\geq 1}\subset\{\mu_{n}\}_{n\geq 1}\), there exist a further subsequence \(\{\mu_{m_{i}}\}_{i\geq 1}\) of \(\{\mu_{n_{i}}\}_{i\geq 1}\) and a probability measure \(\mu\) on \((\mathbb{S},\mathcal{S})\) such that_

\[\mu_{m_{i}}\longrightarrow^{d}\mu\quad\text{as}\quad i\to\infty. \tag{3.6}\]

For a proof of this result, see Section 1.6 of Billingsley (1968). This result is useful for proving weak convergence in function spaces (e.g., see Chapter 11 where a functional central limit theorem is stated).

### 9.4 Skorohod's theorem and the continuous mapping theorem

If \(\{X_{n}\}_{n\geq 1}\) is a sequence of random variables that converge to a random variable \(X\) in probability, then \(X_{n}\) does converge in distribution to \(X\) (cf.

Proposition 9.1.1). Here is another proof of this fact using Theorem 9.2.3. Let \(f:\mathbb{R}\to\mathbb{R}\) be bounded and continuous. Then \(X_{n}\to X\) in probability implies that \(f(X_{n})\to f(X)\) in probability (Problem 9.13) and by the BCT,

\[\int fd\mu_{n}=Ef(X_{n})\to Ef(X)=\int fd\mu,\]

where \(\mu_{n}(\cdot)=P(X_{n}\in\cdot)\), \(n\geq 1\) and \(\mu(\cdot)=P(X\in\cdot)\). Hence, \(\mu_{n}\longrightarrow^{d}\mu\). In particular, it follows that if \(X_{n}\to X\) w.p. 1, then \(X_{n}\longrightarrow^{d}X\). Skorohod's theorem is a sort of converse to this. If \(\mu_{n}\longrightarrow^{d}\mu\), then there exist random variables \(X_{n}\), \(n\geq 1\) and \(X\) such that \(X_{n}\) has distribution \(\mu_{n}\), \(n\geq 1\) and \(X\) has distribution \(\mu\) and \(X_{n}\to X\) w.p. 1.

**Theorem 9.4.1:** (_Skorohod's theorem_). _Let \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \(\mu_{n}\longrightarrow^{d}\mu\). Let_

\[X_{n}(\omega) \equiv \sup\{t:\mu_{n}((-\infty,t])<\omega\}\] \[X(\omega) \equiv \sup\{t:\mu((-\infty,t])<\omega\}\]

_for \(0<\omega<1\). Then, \(X_{n}\) and \(X\) are random variables on \(((0,1),\mathcal{B}\big{(}(0,1)\big{)},m)\) where \(m\) is the Lebesgue measure. Furthermore, \(X_{n}\) has distribution \(\mu_{n}\), \(n\geq 1\), \(X\) has distribution \(\mu\) and \(X_{n}\to X\) w.p. 1._

**Proof:** For any cdf \(F(\cdot)\), let \(F^{-1}(u)\equiv\sup\{t:F(t)<u\}\). Then for any \(u\in(0,1)\) and \(t\in\mathbb{R}\), it can be verified that \(F^{-1}(u)\leq t\Rightarrow F(t)\geq u\Rightarrow F^{-1}(u)\leq t\) and hence, if \(U\) is a Uniform (0,1) random variable (Problem 9.11),

\[P(F^{-1}(U)\leq t)=P(U\leq F(t))=F(t),\]

implying that

\[F^{-1}(U)\quad\mbox{has cdf}\quad F(\cdot).\]

This shows that \(X_{n}\), \(n\geq 1\) and \(X\) have the asserted distributions. It remains to show that

\[X_{n}(\omega)\to X(\omega)\quad\mbox{w.p. 1}\]

Fix \(\omega\in(0,1)\) and let \(y<X(\omega)\) be such that \(\mu(\{y\})=0\). Now \(y<X(\omega)\Rightarrow\mu((-\infty,y])<\omega\). Since \(\mu_{n}\longrightarrow^{d}\mu\) and \(\mu(\{y\})=0\), \(\mu_{n}((-\infty,y])\to\mu((-\infty,y])\) and so \(\mu_{n}((-\infty,y])<\omega\) for large \(n\). This implies that \(X_{n}(\omega)\geq y\) for large \(n\) and hence \(\liminf_{n\to\infty}X_{n}(\omega)\geq y\). Since this is true for all \(y<X(\omega)\) with \(\mu(\{y\})=0\), and since the set of all such \(y\)'s is dense in \(\mathbb{R}\), it follows that

\[\liminf_{n\to\infty}X_{n}(\omega)\geq X(\omega)\quad\mbox{for all}\quad\omega \quad\mbox{in }(0,1).\]

Next fix \(\epsilon>0\) and \(y>X(\omega+\epsilon)\), and \(\mu(\{y\})=0\). Then \(\mu((-\infty,y])\geq\omega+\epsilon\). Since \(\mu(\{y\})=0\), \(\mu_{n}((-\infty,y])\to\mu((-\infty,y])\). Thus, for large \(n\)\(\mu_{n}(-\infty,y]\geq\omega.\) This implies that \(X_{n}(\omega)\leq y\) for large \(n\) and hence that \(\limsup_{n\to\infty}X_{n}(\omega)\leq y.\) Since this is true for all \(y>X(\omega+\epsilon),\)\(\mu(\{y\})=0,\) it follows that

\[\limsup_{n\to\infty}X_{n}(\omega)\leq X(\omega+\epsilon)\quad\mbox{for every} \quad\epsilon>0\]

and hence that

\[\limsup_{n\to\infty}X_{n}(\omega)\leq X(\omega+).\]

Thus it has been shown that for all \(0<\omega<1,\)

\[X(\omega)\leq\liminf_{n\to\infty}X_{n}(\omega)\leq\limsup_{n\to\infty}X_{n}( \omega)\leq X(\omega+).\]

Since \(X(\omega)\) is a nondecreasing function on \((0,1),\) it has at most a countable number of discontinuities and so

\[\lim_{n\to\infty}X_{n}(\omega)=X(\omega)\quad\mbox{w.p.\ 1}.\]

An immediate consequence of the above theorem is the continuity of convergence in distribution under continuous transformations.

**Theorem 9.4.2:** (_The continuous mapping theorem_). _Let \(\{X_{n}\}_{n\geq 1}\), \(X\) be random variables such that \(X_{n}\longrightarrow^{d}X.\) Let \(f:\mathbb{R}\to\mathbb{R}\) be Borel measurable such that \(P(X\in D_{f})=0\), where \(D_{f}\) is the set of discontinuities of \(f.\) Then \(f(X_{n})\longrightarrow^{d}f(X).\) In particular, this holds if \(f:\mathbb{R}\to\mathbb{R}\) is continuous._

**Remark 9.4.1:** It can be shown that for any \(f:\mathbb{R}\to\mathbb{R},\) the set \(D_{f}=\{x:f\mbox{ is discontinuous at }x\}\in\mathcal{B}(\mathbb{R})\) (Problem 9.12). Thus, \(\{X\in D_{f}\}\in\mathcal{F},\) and \(P(X\in D_{f})\) is well defined.

**Proof:** By Skorohod's theorem, there exist random variables \(\{\tilde{X}_{n}\}_{n\geq 1},\)\(\tilde{X}\) defined on the Lebesgue space (\(\Omega=(0,1),\)\(\mathcal{B}((0,1)),\)\(m=\) Lebesgue measure) such that \(\tilde{X}_{n}=^{d}X_{n}\) for \(n\geq 1,\)\(\tilde{X}=^{d}X,\) and

\[\tilde{X}_{n}\to\tilde{X}\quad\mbox{w.p.\ 1}.\]

Let \(A=\{\omega:\tilde{X}_{n}(\omega)\to\tilde{X}(\omega)\}\) and \(B=\{\omega:\tilde{X}(\omega)\not\in D_{f}\}.\) Then, \(P(A)=1=P(B)\) and so, for \(\omega\in A\cap B,\)

\[f(\tilde{X}_{n}(\omega))\to f(\tilde{X}(\omega)).\]

Thus, \(f(\tilde{X}_{n})\to f(\tilde{X})\) w.p. 1 and hence \(f(X_{n})\longrightarrow^{d}f(X).\)\(\Box\)

Another easy consequence of Skorohod's theorem is the Helly-Bray Theorem 9.2.3. Since \(\tilde{X}_{n}\to\tilde{X}\) w.p. 1 and \(f\) is a bounded continuous function, then \(f(\tilde{X}_{n})\to f(\tilde{X})\) w.p. 1 and so by the bounded convergence theorem

\[Ef(\tilde{X}_{n})\to Ef(\tilde{X}).\]Since \(\tilde{X}_{n}=^{d}X_{n}\) for \(n\geq 1\) and \(\tilde{X}=^{d}X\), this is the same as saying that

\[Ef(X_{n})\to Ef(X).\]

That is, \(\int fd\mu_{n}\to\int fd\mu\), where \(\mu_{n}(\cdot)=P(X_{n}\in\cdot)\), \(n\geq 1\) and \(\mu(\cdot)=P(X\in\cdot)\).

**Remark 9.4.2:** Skorohod's theorem is valid for any Polish space. Suppose that \(\mathbb{S}\) is a Polish space and \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) are probability measures on \((\mathbb{S},\mathcal{S})\), where \(\mathcal{S}\) is the Borel \(\sigma\)-algebra on \(\mathbb{S}\), such that \(\mu_{n}\longrightarrow^{d}\mu\). Then there exist random variables \(X_{n}\) and \(X\) defined on the Lebesgue space \(\big{(}(0,1)\), \(\mathcal{B}((0,1))\), \(m=\) the Lebesgue measure\(\big{)}\) such that for all \(n\geq 1\), \(X_{n}\) has distribution \(\mu_{n}\), \(X\) has distribution \(\mu\) and \(X_{n}\to X\) w.p. 1. For a proof, see Billingsley (1968).

### The method of moments and the moment problem

#### Convergence of moments

Let \(\{X_{n}\}_{n\geq 1}\) and \(X\) be random variables such that \(X_{n}\) converges to \(X\) in distribution. Suppose for some \(k>0\), \(E|X_{n}|^{k}<\infty\) for each \(n\geq 1\). A natural question is: When does this imply \(E|X|^{k}<\infty\) and \(\lim_{n\to\infty}E|X_{n}|^{k}=EX^{k}\)?

By Skorohod's theorem, one can assume w.l.o.g. that \(X_{n}\to X\) w.p. 1. Then the results from Section 2.5 yield the following.

**Theorem 9.5.1:** _Let \(\{X_{n}\}_{n\geq 1}\) and \(X\) be a collection of random variables such that \(X_{n}\longrightarrow^{d}X\). Then, for each \(0<k<\infty\), the following are equivalent:_

* \(E|X_{n}|^{k}<\infty\) _for each_ \(n\geq 1\)_,_ \(E|X|^{k}<\infty\) _and_ \(E|X_{n}|^{k}\to E|X|^{k}\)_._
* \(\{|X_{n}|^{k}\}_{n\geq 1}\) _are uniformly integrable, i.e., for every_ \(\epsilon>0\)_, there exists an_ \(M_{\epsilon}\in(0,\infty)\) _such that_ \[\sup_{n\geq 1}E(|X_{n}|^{k}I(|X_{n}|>M_{\epsilon}))<\epsilon.\]

**Remark 9.5.1:** Recall that a sufficient condition for the uniform integrability of \(\{|X_{n}|^{k}\}_{n\geq 1}\) is that

\[\sup_{n\geq 1}E|X_{n}|^{\ell}<\infty\quad\mbox{for some}\quad\ell\in(k, \infty).\]

**Example 9.5.1:** Let \(X_{n}\) have the distribution \(P(X_{n}=0)=1-\frac{1}{n}\), \(P(X_{n}=n)=\frac{1}{n}\) for \(n=1,2,\ldots\). Then \(X_{n}\longrightarrow^{d}0\) but \(EX_{n}=1\) does not go to 0. Note that \(\{X_{n}\}_{n\geq 1}\) is not uniformly integrable here.

**Remark 9.5.2:** In Theorem 9.5.1, under hypothesis (ii), it follows that

\[E|X_{n}|^{\ell}\to E|X|^{\ell}\quad\mbox{for all real numbers}\quad\ell\in(0,k)\]

and

\[EX_{n}^{p}\to EX^{p}\quad\mbox{for all positive integers}\quad p,\ 0<p\leq k.\]

#### The method of moments

Suppose that \(\{X_{n}\}_{n\geq 1}\) are random variables such that \(\lim_{n\to\infty}EX_{n}^{k}=m_{k}<\infty\) exists for all integers \(k=0,1,2,\ldots\). Does there exist a random variable \(X\) such that \(X_{n}\longrightarrow^{d}X\)? The answer is 'yes' provided that the moments \(\{m_{k}\}_{k\geq 1}\) determine the distribution of the random variable \(X\) uniquely.

**Theorem 9.5.2:** (_Frechet-Shohat theorem_). _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables such that for each \(k\in\mathbb{N}\), \(\lim_{n\to\infty}EX_{n}^{k}\equiv m_{k}\) exists and is finite. If the sequence \(\{m_{k}\}_{k\geq 1}\) uniquely determines the distribution of a random variable \(X\), then \(X_{n}\longrightarrow^{d}X\)._

**Proof:** Suppose that for some subsequence \(\{n_{j}\}_{j\geq 1}\), the probability distributions \(\{\mu_{n_{j}}\}_{j\geq 1}\) of \(\{X_{n_{j}}\}_{j\geq 1}\) converge vaguely to some \(\mu\). Since \(\{EX_{n_{j}}^{2}\}_{j\geq 1}\) is a bounded sequence, \(\{\mu_{n_{j}}\}_{j\geq 1}\) is _tight_. Hence \(\mu\) must be a probability distribution and by Theorem 9.5.1, the moments of \(\mu\) must coincide with \(\{m_{k}\}_{k\geq 1}\). Since the sequence \(\{m_{k}\}_{k\geq 1}\) determines the distribution uniquely, \(\mu\) is unique and is the unique vague limit point of \(\{\mu_{n}\}_{n\geq 1}\) and by Theorem 9.2.6, \(\mu_{n}\longrightarrow^{d}\mu\). So if \(X\) is a random variable with distribution \(\mu\), then \(X_{n}\longrightarrow^{d}X\). \(\Box\)

The above "method of moments" used to be a tool for proving convergence in distribution, e.g., for proving asymptotic normality of the Binomial \((n,p)\) distribution. Since it requires existence of all moments, this method is too restrictive and is of limited use. However, the question of when do the moments determine a distribution is an interesting one and is discussed next.

#### The moment problem

Suppose \(\{m_{k}\}_{k\geq 1}\) is a sequence of real numbers such that there is at least one probability measure \(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that for all \(k\in\mathbb{N}\)

\[m_{k}=\int x^{k}\mu(dx).\]

Does the sequence \(\{m_{k}\}_{k\geq 1}\) determine \(\mu\) uniquely? This is a part of the _Hamburger-moment problem_, which includes seeking conditions underwhich a given sequence \(\{m_{k}\}_{k\geq 1}\) is the moment sequence of a probability distribution.

The answer to the uniqueness question posed above is 'no,' as the following example shows.

**Example 9.5.2:** Let \(Y\) be a standard normal random variable and let \(X=\exp(Y)\). Then \(X\) is said to have the _log-normal distribution_ (which is a misnomer as a more appropriate name would be something like _exponormal_). Then \(X\) has the probability density function

\[f(x)=\left\{\begin{array}{cc}\frac{1}{\sqrt{2\pi}}\frac{1}{x}\exp(-[\log x]^ {2}/2)&x>0\\ 0&\mbox{otherwise.}\end{array}\right.\]

Consider now the family of functions

\[f_{\alpha}(x)=f(x)(1+\alpha\sin(2\pi\log x))\]

with \(|\alpha|\leq 1\). It is clear that \(f_{\alpha}(x)\geq 0\). Further, it is not difficult to check that for any \(\alpha\in[-1,1]\),

\[\int x^{r}f_{\alpha}(x)dx=\int x^{r}f(x)dx\]

for all \(r=0,1,2,\ldots\). Thus, the sequence of moments \(m_{k}\equiv\int x^{k}f(x)dx\) does not determine the log-normal distribution (5.1).

A sufficient condition for uniqueness is _Carleman's condition_:

\[\sum_{k=1}^{\infty}m_{2k}^{-1/2k}=\infty.\]

For a proof, see Feller (1966) or Shohat and Tamarkin (1943).

**Remark 9.5.3:** A special case of the above is when

\[\limsup_{k\to\infty}\ m_{2k}^{1/2k}=r\in[0,\infty).\]

In particular, if \(\{m_{k}\}_{k\geq 1}\) is a moment sequence, then within the class of probability distributions \(\mu\) that have bounded support and have \(\{m_{k}\}_{k\geq 1}\) as their moment sequence, \(\mu\) is uniquely determined. This is so since if \(M\equiv\sup\{x:\mu([-x,x])<1\}\), then (Problem 9.27)

\[m_{2k}^{1/2k}\to M\quad\mbox{as}\quad k\to\infty.\]

More generally, if \(\mu\) is a probability distribution on \(\mathbb{R}\) such that \(\int e^{tx}d\mu(x)<\infty\) for all \(|t|<\delta\) for some \(\delta>0\), then all its moments are finite and (5.2) holds and hence \(\mu\) is uniquely determined by its moments(Problem 9.28). In particular, the normal and Gamma distributions are determined by their moments.

**Remark 9.5.4:** If \(\{m_{k}\}_{k\geq 1}\) is a moment sequence of a distribution \(\mu\) concentrated on \([0,\infty)\), the problem of determining \(\mu\) uniquely is known as the _Stieltjes moment problem_. If \(X\) is a random variable with distribution \(\mu\), let \(Y=\delta\sqrt{X}\), where \(\delta\) is independent of \(X\) and takes two values \(\{-1,+1\}\) with equal probability. Then \(Y\) has a symmetric distribution and for all \(k\geq 1\),

\[E|Y|^{2k}=E|X|^{k}.\]

The distribution of \(Y\) is uniquely determined (and hence that of \(\sqrt{X}\) and hence that of \(X\)) if

\[\limsup_{k\to\infty}\frac{(EY^{2k})^{1/2k}}{2k}<\infty\]

i.e.,

\[\limsup_{k\to\infty}\frac{m_{k}^{1/2k}}{2k}<\infty.\]

### Problems

1. If \(X_{n}\longrightarrow^{d}X_{0}\) and \(P(X_{0}=c)=1\) for some \(c\in\mathbb{R}\), then \(X_{n}\longrightarrow_{p}c\).
2. If a cdf is continuous on \(\mathbb{R}\), then it is uniformly continuous on \(\mathbb{R}\). (**Hint:** Use the facts that (i) given any \(\epsilon>0\), there exists \(M\in(0,\infty)\) such that \(F(-x)+1-F(x)<\epsilon\) for all \(x>M\), and (ii) \(F\) is uniformly continuous on \([-M,M]\).)
3. Prove parts (ii) and (iii) of Theorem 9.1.6.
4. Let \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \(\mu_{n}\longrightarrow^{v}\mu\). Show that \(\mu_{n}\longrightarrow^{d}\mu\).
5. Show that the function \(\tilde{F}(\cdot)\), defined in (2.5), is nondecreasing and right continuous and that the function \(F(x)\equiv\inf\{F(r):r\geq x,r\in D\}\) is nondecreasing and left continuous.
6. Give another proof of the 'if' part of Theorem 9.2.6 by using Theorem 9.2.1 and showing that for any \(f:\mathbb{R}\to\mathbb{R}\) continuous and bounded and any subsequence \(\{n_{i}\}_{i\geq 1}\), there exist a further subsequence \(\{m_{j}\}_{j\geq 1}\) such that \(a_{m_{j}}=\int fd\mu_{m_{j}}\to a=\int fd\mu\) and hence, \(a_{n}\equiv\int fd\mu_{n}\to a\).
7. If \(\{X_{n}\}_{n\geq 1}\) is stochastically bounded and \(Y_{n}\longrightarrow_{p}0\), then show that \(X_{n}Y_{n}\longrightarrow_{p}0\).

* Let \(X_{n}\sim N(a_{n},b_{n})\) for \(n\geq 0\), where \(b_{n}>0\) for \(n\geq 1\), \(b_{0}\in[0,\infty)\) and \(a_{n}\in\mathbb{R}\) for all \(n\geq 0\).
* Show that if \(a_{n}\to a_{0}\), \(b_{n}\to b_{0}\) as \(n\to\infty\), then \(X_{n}\longrightarrow^{d}X_{0}\).
* Show that if \(X_{n}\longrightarrow^{d}X_{0}\) as \(n\to\infty\), then \(a_{n}\to a_{0}\) and \(b_{n}\to b_{0}\).
* First show that \(\{b_{n}\}_{n\geq 1}\) is bounded and then that \(\{a_{n}\}_{n\geq 1}\) is bounded and finally, that \(a_{0}\) and \(b_{0}\) are the only limit points of \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\), respectively.)
* For \(n\geq 1\), let \(X_{n}\sim N(a_{n},\Sigma_{n})\) where \(a_{n}\in\mathbb{R}^{k}\) and \(\Sigma_{n}\) is a \(k\times k\) positive definite matrix, \(k\in\mathbb{N}\). Then, \(\{X_{n}\}_{n\geq 1}\) is stochastically bounded if and only if \(\{\|a_{n}\|\}_{n\geq 1}\) and \(\{\|\Sigma_{n}\|\}_{n\geq 1}\) are bounded.
* Let \(\{X_{jn}\}_{n\geq 1}\), \(j=1,\ldots,k\), \(k\in\mathbb{N}\) be sequences of random variables. Let \(X_{n}=(X_{1n},\ldots,X_{kn})\), \(n\geq 1\). Show that the sequence of random vectors \(\{X_{n}\}_{n\geq 1}\) is tight in \(\mathbb{R}^{k}\) iff for each \(1\leq j\leq k\), the sequence of random variables \(\{X_{jn}\}_{n\geq 1}\) is tight in \(\mathbb{R}\).
* Let \((\mathbb{S},d)\) be a metric space.
* For any set \(A\subset\mathbb{S}\), let \[d(x,A)\equiv\inf\{d(x,y):y\in A\}.\] Show that for each \(A\), \(d(\cdot,A)\) is continuous on \(\mathbb{S}\).
* Let \(f_{n}(\cdot)\) be as in (3.3). Show that \(f_{n}(\cdot)\) is continuous on \(\mathbb{S}\) and \(f_{n}(\cdot)\uparrow I_{G}(\cdot)\).
* Note that \(d(x,G^{c})+d(x,G_{n})>0\) for all \(x\) in \(\mathbb{S}\). )
* For any cdf \(F\), let \(F^{-1}(u)\equiv\sup\{t:F(t)<u\}\), \(0<u<1\). Show that for any \(0<u_{0}<1\) and \(t_{0}\) in \(\mathbb{R}\), \[F^{-1}(u_{0})\leq t_{0}\Leftrightarrow F(t_{0})\geq u_{0}.\]
* For \(\Rightarrow\), use the right continuity of \(F\) and for \(\Leftarrow\), use the definition of sup.)
* For a function \(f:\mathbb{R}^{k}\to\mathbb{R}\) (\(k\in\mathbb{N}\)), define \(D_{f}=\{x\in\mathbb{R}^{k}:f\) is discontinuous at \(x\}\). Show that \(D_{f}\in\mathcal{B}(\mathbb{R}^{k})\).
* If \(X_{n}\longrightarrow_{p}X\) and \(f:\mathbb{R}\to\mathbb{R}\) is continuous, then \(f(X_{n})\longrightarrow_{p}f(X)\).
* (_The Delta method_). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables and \(\{a_{n}\}_{n\geq 1}\subset(0,\infty)\) be a sequence of constants such that \(a_{n}\to\infty\) as \(n\to\infty\) and \[a_{n}(X_{n}-\theta)\longrightarrow^{d}Z\]for some random variable \(Z\) and for some \(\theta\in\mathbb{R}\). Let \(H:\mathbb{R}\to\mathbb{R}\) be a function that is differentiable at \(\theta\) with derivative \(c\). Show that \[a_{n}\big{(}H(X_{n})-H(\theta)\big{)}\longrightarrow^{d}cZ.\] (**Hint:** By Taylor's expansion, for any \(x\in\mathbb{R}\), \[H(x)=H(\theta)+c(x-\theta)+R(x)(x-\theta)\] where \(R(x)\to 0\) as \(x\to\theta\). Now use Problem 9.7 and Slutsky's theorem.)
9.15 Let \(X\) be a random variable with \(P(X=c)>0\) for some \(c\in\mathbb{R}\). Give examples of two sequences \(\{X_{n}\}_{n\geq 1}\) and \(\{Y_{n}\}_{n\geq 1}\) satisfying \(X_{n}\longrightarrow^{d}X\) and \(Y_{n}\longrightarrow^{d}X\) such that \[\lim_{n\to\infty}P(X_{n}\leq c)=P(X\leq c)\] but \[\lim_{n\to\infty}P(Y_{n}\leq c)\neq P(X\leq c).\] (**Hint:** Take \(X_{n}=^{d}X\), \(n\geq 1\) and \(Y_{n}=^{d}X+\frac{1}{n}\), \(n\geq 1\), say.)
9.16 Let \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \[\int fd\mu_{n}\to\int fd\mu\quad\mbox{for all}\quad f\in\mathcal{F}\] for some collection \(\mathcal{F}\) of functions from \(\mathbb{R}\) to \(\mathbb{R}\) specified below. Does \(\mu_{n}\longrightarrow^{d}\mu\) if 1. \(\mathcal{F}=\{f\ |\ f:\mathbb{R}\to\mathbb{R}\), \(f\) is bounded and continuously differentiable on \(\mathbb{R}\) with a bounded derivative\(\}\)? 2. \(\mathcal{F}=\{f\ |\ f:\mathbb{R}\to\mathbb{R}\), \(f\) is bounded and infinitely differentiable on \(\mathbb{R}\}\)? 3. \(\mathcal{F}\equiv\{f\ |\ f\) is a polynomial with real coefficients\(\}\) and \(\int|x|^{k}\mu(dx)+\int|x|^{k}d\mu_{n}(dx)<\infty\) for all \(n\), \(k\in\mathbb{N}\)?
9.17 For any two cdfs \(F\), \(G\) on \(\mathbb{R}\), define \[d_{L}(F,G) = \inf\{\epsilon>0:G(x-\epsilon)-\epsilon<F(x)\] (6.1) Verify that \(d_{L}\) defines a metric on the collection of all probability distributions on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). The metric \(d_{L}\) is called the _Levy_ metric.
9.18 Let \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\), with the corresponding cdfs \(\{F_{n}\}_{n\geq 1}\) and \(F\). Show that \(\mu_{n}\longrightarrow^{d}\mu\) iff \[d_{L}(F_{n},F)\to 0\quad\mbox{as}\quad n\to\infty.\]
* Show that for any two cdfs \(F\), \(G\) on \(\mathbb{R}\), \[d_{L}(F,G)\leq d_{K}(F,G),\] (6.2) where \[d_{K}(F,G)=\sup_{x\in\mathbb{R}}|F(x)-G(x)|\] (6.3) (\(d_{K}\) is called the _Kolmogorov distance or metric_ between \(F\) and \(G\)).
* Give examples where (i) equality holds in (6.2), and (ii) where strict inequality holds in (6.2).
* Let \(\{\mu_{n}\}_{n\geq 1}\), \(\mu\) be probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \(\mu_{n}\longrightarrow^{d}\mu\). Let \(\{f_{a}:a\in\mathbb{R}\}\) be a collection of bounded functions from \(\mathbb{R}\to\mathbb{R}\) such that \(\mu(D_{f_{a}})=0\) for all \(a\in\mathbb{R}\) and \(|f_{a}(x)-f_{b}(x)|\leq h(x)|b-a|\) for all \(a,b\in\mathbb{R}\) and for some \(h:\mathbb{R}\to(0,\infty)\) with \(\mu(D_{h})=0\) and \(\int|h|d\mu<\infty\). Show that \[\sup_{a\in\mathbb{R}}\Big{|}\int f_{a}d\mu_{n}-\int f_{a}d\mu\Big{|}\to 0 \quad\text{as}\quad n\to\infty.\]
* Let \(\{X_{n}\}_{n\geq 1}\), \(X\) be \(k\)-dimensional random vectors such that \(X_{n}\longrightarrow^{d}X\). Let \(\{A_{n}\}_{n\geq 1}\) be a sequence of \(r\times k\)-matrices of real numbers and \(\{b_{n}\}_{n\geq 1}\subset\mathbb{R}^{r}\), \(r\in\mathbb{N}\). Define \(Y_{n}=A_{n}X_{n}+b_{n}\) and \(Z_{n}=A_{n}X_{n}X_{n}^{T}\) where \(X_{n}^{T}\) denotes the transpose of \(X\). Suppose that \(A_{n}\to A\) and \(b_{n}\to b\). Show that
* \(Y_{n}\longrightarrow^{d}Y\), where \(Y=^{d}AX+b\),
* \(Z_{n}\longrightarrow^{d}Z\), where \(Z=^{d}AXX^{T}\). (**Note:*
* Here convergence in distribution of a sequence of \(r\times k\) matrix-valued random variables may be interpreted by considering the corresponding \(rk\)-dimensional random vectors obtained by concatenating the rows of the \(r\times k\) matrix side-by-side and using the definition of convergence in distribution for random vectors.)
* Let \(\mu_{n}\), \(\mu\) be probability measures on a countable set \(D\equiv\{a_{j}\}_{j\geq 1}\subset\mathbb{R}\). Let \(p_{nj}=\mu_{n}(\{a_{j}\})\), \(j\geq 1\), \(n\geq 1\) and \(p_{j}=\mu(\{a_{j}\})\). Show that, as \(n\to\infty\), \(\mu_{n}\longrightarrow^{d}\mu\) iff for all \(j\), \(p_{nj}\to p_{j}\) iff \(\sum_{j}|p_{nj}-p_{j}|\to 0\).
* Let \(X_{n}\sim\text{Binomial}(n,p_{n})\), \(n\geq 1\). Suppose \(np_{n}\to\lambda\), \(0<\lambda<\infty\). Show that \(X_{n}\to X\), where \(X\sim\text{Poisson}(\lambda)\).
* Let \(X_{n}\sim\text{Geo}(p_{n})\), i.e. \(P(X_{n}=r)=q_{n}^{r-1}p_{n}\), \(r\geq 1\), where \(0<p_{n}<1\) and \(q_{n}=1-p_{n}\). Show that as \(n\to\infty\) if \(p_{n}\to 0\) then \[p_{n}X_{n}\longrightarrow^{d}X,\] (6.4) where \(X\sim\text{Exponential}\) (1).

2. Fix a positive integer \(k\). Let for \(n\geq 1\), \[p_{nr}={r-1\choose k-1}p_{n}^{r-1}q_{n}^{r-k},\ r\geq k\] where \(0<p_{n}<1\), \(q_{n}=1-p_{n}\). 1. Verify that for each \(n\), \(\{p_{nr}\}_{r\geq k}\) is a probability distribution, i.e., \(\sum_{r=k}^{\infty}p_{nr}=1\). 2. Let \(Y_{n}\) be a random variable with distribution \(P(Y_{n}=r)=p_{nr}\), \(r\geq k\). Show that as \(n\to\infty\) if \(p_{n}\to 0\) then \(\{p_{n}Y_{n}\}_{n\geq 1}\) converges in distribution and identify the limit.
9.25 Let \(\{F_{n}\}_{n\geq 1}\) and \(\{G_{n}\}_{n\geq 1}\) be two sequences of cdfs on \(\mathbb{R}\) such that, as \(n\to\infty\), \(F_{n}\longrightarrow^{d}F\), \(G_{n}\longrightarrow^{d}G\) where \(F\) and \(G\) are cdfs on \(\mathbb{R}\). 1. Show that for each \(n\geq 1\), \[H_{n}(x)\equiv(F_{n}*G_{n})(x)\equiv\int_{\mathbb{R}}F_{n}(x-y)dG(y)\] is a cdf on \(\mathbb{R}\). 2. Show that, as \(n\to\infty\), \(H_{n}\longrightarrow^{d}H\) where \(H=F*G\), by direct calculation and by Skorohod's theorem (i.e., Theorem 9.4.1) and Problem 7.14.
9.26 Let \(Y_{n}\) have discrete uniform distribution on the integers \(\{1,2,\ldots,n\}\). Show that \(X_{n}\equiv\frac{Y_{n}}{n}\) and let \(X\sim\) Uniform (0,1) random variable. Show that \(X_{n}\longrightarrow^{d}X\) using three different methods as follows: 1. Helly-Bray theorem, 2. the method of moments, 3. using the cdfs.
9.27 Establish (5.4) in Remark 9.5.3. (**Hint:** Show that for any \(\epsilon>0\), \(m_{2k}^{1/2k}\geq(M-\epsilon)\bigl{(}\mu(\{x:|x|>M-\epsilon\})\bigr{)}^{1/2k}\).)
9.28 Let \(\mu\) be a probability distribution on \(\mathbb{R}\) such that \(\phi(t_{0})\equiv\int e^{t|x|}d\mu(x)<\infty\) for some \(t_{0}>0\). Show that Carleman's condition (5.2) is satisfied. (**Hint:** Show that by Cramer's inequality (Corollary 3.1.5) \[m_{2k} \leq 2k\,\phi(t_{0})\int_{0}^{\infty}x^{2k-1}e^{-t_{0}x}dx\] \[= \phi(t_{0})2k\,t_{0}^{-2k}(2k-1)!\] and then use _Stirling's approximation_: '\(n!\sim\sqrt{2\pi}\ n^{n+1/2}e^{-n}\) as \(n\to\infty\)' (Feller (1968)).)* 9.29 (_Continuity theorem for mgfs_). Let \(\{X_{n}\}_{n\geq 1}\) and \(X\) be random variables such that for some \(\delta>0\), the mgf \(M_{X_{n}}(t)\equiv E(e^{tX_{n}})\) and \(M_{X}(t)\equiv E(e^{tX})\) are finite for all \(|t|<\delta\). Further, let \(M_{X_{n}}(t)\to M_{X}(t)\) for all \(|t|<\delta\). Show that \(X_{n}\longrightarrow^{d}X\). (**Hint:** Show first that \(\{X_{n}\}_{n\geq 1}\) is tight and the fact that by Remark 9.5.3, the distribution of \(X\) is determined by \(M_{X}(\cdot)\).)
* 9.30 Let \(X_{n}\sim\mbox{Binomial}(n,p_{n})\). Suppose \(np_{n}\to\infty\). Let \(Y_{n}=\frac{X_{n}-np_{n}}{\sqrt{np_{n}(1-p_{n})}}\), \(n\geq 1\). Show that \(Y_{n}\longrightarrow^{d}Y\), where \(Y\sim N(0,1)\). (**Hint:** Use Problem 9.29.)
* 9.31 Use the continuity theorem for mgfs to establish (6.4) and the convergence in distribution of \(\{p_{n}Y_{n}\}_{n\geq 1}\) in Problem 9.24 (b)(ii).
* 9.32 Let \(\{X_{j},V_{j}:j\geq 1\}\) be a collection of random variables on some probability space \((\Omega,\mathcal{F},P)\) such that \(P(V_{j}\in\mathbb{N})=1\) for all \(j\), \(V_{j}\to\infty\) w.p. \(1\) and \(X_{j}\longrightarrow^{d}X\). Suppose that for each \(j\), the random variable \(V_{j}\) is independent of the sequence \(\{X_{n}\}_{n\geq 1}\). Show that \(X_{V_{j}}\longrightarrow^{d}X\). (**Hint:** Verify that for any bounded continuous function \(h:\mathbb{R}\to\mathbb{R}\), \[\bigl{|}Eh(X_{V_{j}})-Eh(X)\bigr{|}\leq 2\|h\|P(V_{j}\leq N)+\Delta_{N}P(V_{j}>N)\] where \(\Delta_{N}=\sup_{k>N}\bigl{|}Eh(X_{k})-Eh(X)\bigr{|}\) and \(\|h\|=\sup\{|h(x)|:x\in\mathbb{R}\}\).)
* 9.33 Let \(X_{n}\longrightarrow^{d}X\) and \(x_{n}\to x\) as \(n\to\infty\). If \(P(X=x)=0\), then show that \(P(X_{n}\leq x_{n})\to P(X\leq x)\).
* 9.34 (_Weyl's equi-distribution property_). Let \(0<\alpha<1\) be an irrational number. Let \(\mu_{n}(\cdot)\) be the measure defined by \(\mu_{n}(A)\equiv\frac{1}{n}\sum_{j=0}^{n-1}I_{A}(j\alpha\,\mbox{mod}\,1)\), \(A\in\mathcal{B}([0,1])\). Show that \(\mu_{n}\longrightarrow^{d}\) Uniform (0,1). (**Hint:** Verify that \(\int fd\mu_{n}\to\int_{0}^{1}f(x)dx\) for all \(f\) of the form \(f(x)=e^{t2\pi kx}\), \(k\in\mathbb{Z}\) and then approximate a bounded continuous function \(f\) by trigonometric polynomials (cf. Section 5.6).)
* 9.35 1. Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with Uniform (0,1) distribution. Let \(M_{n}=\max_{1\leq i\leq n}X_{i}\). Show that \(n(1-M_{n})\longrightarrow^{d}\) Exponential (1). 2. Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables such that \(\lambda\equiv\sup\{x:P(X_{1}\leq x)<1\}<\infty\), \(P(X_{1}=\lambda)=0\), and \(P(\lambda-x<X_{1}<\lambda)\sim cx^{\alpha}L(x)\) as \(x\downarrow 0\) where \(\alpha>0\), \(c>0\), and \(L(\cdot)\) is slowly varying at 0, i.e., \(\lim_{x\downarrow 0}\frac{L(cx)}{L(x)}=1\) for all \(0<c<\infty\). Let \(M_{n}=\max_{1\leq i\leq n}X_{i}\). Show that \(Y_{n}\equiv n^{1/\alpha}(\lambda-M_{n})\) converges in distribution as \(n\to\infty\) and identify the limit.

* 9.36 Let \(\{X_{i}\}_{i\geq 1}\) be iid positive random variables such that \(P(X_{1}<x)\sim cx^{\alpha}L(x)\) as \(x\downarrow 0\), where \(c\), \(\alpha\) and \(L(\cdot)\) are as in Problem 9.35. Let \(X_{1n}\equiv\min_{1\leq i\leq n}X_{i}\). Find \(\{a_{n}\}_{n\geq 1}\subset\mathbb{R}^{+}\) such that \(Z_{n}\equiv a_{n}X_{1n}\) converges in distribution to a nondegenerate limit and identify the distribution. Specialize this to the cases where \(X_{1}\) has a pdf \(f_{X}(\cdot)\) such that
* \(\lim_{x\downarrow 0}f_{X}(x)=f_{X}(0+)\) exists and is positive,
* \(X_{1}\) has a Beta \((a,b)\) distribution.
Characteristic Functions

### 10.1 Definition and examples

Characteristic functions play an important role in studying (asymptotic) distributional properties of random variables, particularly for sums of independent random variables. The main uses of characteristic functions are (1) to characterize the probability distribution of a given random variable, and (2) to establish convergence in distribution of a sequence of random variables and to identify the limit distribution.

**Definition 10.1.1**::
1. The _characteristic function_ of a random variable \(X\) is defined as \[\phi_{X}(t)=E\exp(\iota tx),\quad t\in\mathbb{R},\] (1.1) where \(\iota=\sqrt{-1}\).
2. The characteristic function of a probability measure \(\mu\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) is defined as \[\hat{\mu}(t)=\int\exp(\iota tx)\mu(dx),\quad t\in\mathbb{R}.\] (1.2)
3. Let \(F\) be cdf on \(\mathbb{R}\). Then, the characteristic function of \(F\) is defined as \(\hat{\mu}_{F}(\cdot)\), where \(\mu_{F}\) is the Lebesgue-Stieltjes measure corresponding to \(F\).

Note that the integrands in (1.1) and (1.2) are complex valued. Here and elsewhere, for any \(f_{1},f_{2}\in L^{1}(\Omega,\mathcal{F},\mu)\), the integral of \((f_{1}+\iota f_{2})\) with respect to \(\mu\) is defined as

\[\int(f_{1}+\iota f_{2})d\mu=\int f_{1}d\mu+\iota\int f_{2}d\mu. \tag{1.3}\]

Thus, the characteristic function of \(X\) is given by \(\phi_{X}(t)=(E\cos tX)+\iota(E\sin tX)\), \(t\in\mathbb{R}\). Since the functions \(\cos tx\) and \(\sin tx\) are bounded for every \(t\in\mathbb{R}\), \(\phi_{X}(t)\) is well defined for all \(t\in\mathbb{R}\). Furthermore, \(\phi_{X}(0)=1\) and for any \(t\in\mathbb{R}\),

\[|\phi_{X}(t)| = \big{\{}(E\cos\,tX)^{2}+(E\sin\,tX)^{2}\big{\}}^{1/2} \tag{1.4}\] \[\leq \big{\{}E(\cos\,tX)^{2}+E(\sin\,tX)^{2}\big{\}}^{1/2}\leq 1.\]

If equality holds in (1.4), i.e., if \(|\phi_{X}(t_{0})|=1\) for some \(t_{0}\neq 0\), then the random variable is necessarily discrete, as shown by the following proposition.

**Proposition 10.1.1:** _Let \(X\) be a random variable with characteristic function \(\phi_{X}(\cdot)\). Then the following are equivalent:_

* \(|\phi_{X}(t_{0})|=1\) _for some_ \(t_{0}\neq 0\)_._
* _There exist_ \(a\in\mathbb{R}\)_,_ \(h\neq 0\) _such that_ \[P\big{(}X\in\{a+jh:j\in\mathbb{Z}\}\big{)}=1.\] (1.5)

**Proof:** Suppose that (i) holds. Since \(|\phi_{X}(t_{0})|=1\), there exists \(a_{0}\in\mathbb{R}\) such that

\[\phi_{X}(t_{0})=e^{\iota a_{0}},\quad\mbox{i.e.,}\quad e^{-\iota a_{0}}\phi_{X }(t_{0})=1.\]

Let \(a=a_{0}/t_{0}\). Since the characteristic function of \((X-a)\) is given by \(e^{-\iota at}\phi_{X}(t)\), it follows that \(E\exp(\iota t_{0}(X-a))=1\). Equating the real parts, one gets

\[E\cos t_{0}(X-a)=1. \tag{1.6}\]

Since \(|\cos\theta|\leq 1\) for all \(\theta\) and \(\cos\theta=1\) if and only if \(\theta=2\pi n\) for some \(n\in\mathbb{Z}\), (1.6) implies that

\[P\big{(}t_{0}(X-a)\in\{2\pi j:j\in\mathbb{Z}\}\big{)}=1. \tag{1.7}\]

Therefore, (ii) holds with \(h=\frac{2\pi}{|t_{0}|}\) and with \(a=a_{0}/t_{0}\).

For the converse, note that with \(p_{j}=P(X=a+jh)\), \(j\in\mathbb{Z}\),

\[\phi_{X}(t)=\sum_{j\in\mathbb{Z}}\exp\big{(}\iota t(a+jh)\big{)}p_{j},\ \ t\in \mathbb{R},\]and hence \(\big{|}\phi_{X}\big{(}\frac{2\pi}{h}\big{)}\big{|}=1\). \(\Box\)

**Definition 10.1.2:** A random variable \(X\) satisfying (1.5) for some \(a\in\mathbb{R}\) and \(h>0\) is called a _lattice_ random variable. In this case, the distribution of \(X\) is also called _lattice_ or _arithmetic_. If \(X\) is a nondegenerate lattice random variable, then the largest \(h>0\) for which (1.5) holds is called the _span_ (of the probability distribution or of the characteristic function) of \(X\).

An inspection of the proof of Proposition 10.1.1 shows that for a lattice random variable \(X\) with span \(h>0\), its characteristic function satisfies the relation

\[\big{|}\phi_{X}(2\pi j/h)\big{|}=1\quad\mbox{for all}\quad j\in\mathbb{Z}. \tag{1.8}\]

In particular, this implies that \(\limsup_{|t|\to\infty}|\phi_{X}(t)|=1\). The next result shows that characteristic functions of random variables with absolutely continuous cdfs exhibit a very different limit behavior.

**Proposition 10.1.2:** _Let \(X\) be a random variable with cdf \(F\) and characteristic function \(\phi_{X}\). If \(F\) is absolutely continuous, then_

\[\lim_{|t|\to\infty}|\phi_{X}(t)|=0. \tag{1.9}\]

**Proof:** Since \(F\) is absolutely continuous, the probability distribution \(\mu_{X}\) of \(X\) has a density, say \(f\), w.r.t. the Lebesgue measure \(m\) on \(\mathbb{R}\), and

\[\phi_{X}(t)=\int\exp(\iota tx)f(x)dx,\quad t\in\mathbb{R}.\]

Fix \(\epsilon\in(0,\infty)\). Since \(f\in L^{1}(\mathbb{R},\mathcal{B}(\mathbb{R}),m)\), by Theorem 2.3.14, there exists a step function \(f_{\epsilon}=\sum_{j=1}^{k}c_{j}I_{(a_{j}b_{j})}\) with \(1\leq k<\infty\) and \(a_{j},b_{j},c_{j}\in\mathbb{R}\) for \(j=1,\ldots,k\), such that

\[\int|f-f_{\epsilon}|dm<\epsilon/2. \tag{1.10}\]

Next note that for any \(t\neq 0\),

\[\Big{|}\int\exp(\iota tx)f_{\epsilon}(x)dx\Big{|} \tag{1.11}\] \[= \Big{|}\sum_{j=1}^{k}c_{j}\int_{a_{j}}^{b_{j}}\exp(\iota tx)dx \Big{|}\] \[\leq \sum_{j=1}^{k}|c_{j}|\frac{2}{|t|}.\]

Hence, by (1.10) and (1.11), it follows that

\[|\phi_{X}(t)| = \Big{|}\int\exp(\iota tx)f(x)dx\Big{|}\]\[\leq \int|f-f_{\epsilon}|dx+\Big{|}\int\exp(\iota tx)f_{\epsilon}(x)dx\Big{|}\] \[< \epsilon/2+\epsilon/2\]

for all \(|t|>t_{\epsilon},\) where \(t_{\epsilon}=4\sum_{j=1}^{k}|c_{j}|/\epsilon.\) Thus (1.9) holds. \(\Box\)

Note that the above proof shows that for any \(f\in L^{1}(m),\) the _Fourier transforms_

\[\hat{f}(t)\equiv\int e^{\iota tx}f(x)dx,\ t\in\mathbb{R}\]

satisfies \(\lim_{|t|\to\infty}\hat{f}(t)=0.\) This is known as the _Riemann-Lebesgue lemma_ (cf. Proposition 5.7.1).

Next, some basic results on smoothness properties of the characteristic function are presented.

**Proposition 10.1.3:** _Let \(X\) be a random variable with characteristic function \(\phi_{X}(\cdot).\) Then, \(\phi_{X}(\cdot)\) is uniformly continuous on \(\mathbb{R}.\)_

**Proof:** For \(t,h\in\mathbb{R},\)

\[\Big{|}\phi_{X}(t+h)-\phi_{X}(t)\big{|}\] \[= \Big{|}E\big{\{}\exp(\iota(t+h)X)-\exp(\iota tX)\big{\}}\Big{|}\] \[= \Big{|}E\exp(\iota tX)\cdot(e^{\iota hX}-1)\Big{|}\] \[\leq E\big{|}e^{\iota hX}-1\big{|}\equiv E\Delta(h),\quad\mbox{say},\]

where \(\Delta(h)\equiv|\exp(\iota hX)-1|.\) It is easy to check that \(|\Delta(h)|\leq 2\) and \(\lim_{h\to 0}\Delta(h)=0\) w.p. 1 (infact, everywhere). Hence, by the BCT, \(E\Delta(h)\to 0\) as \(h\to 0.\) Therefore,

\[\lim_{h\to 0}\ \sup_{t\in\mathbb{R}}\big{|}\phi_{X}(t+h)-\phi_{X}(t)\big{|} \leq\lim_{h\to 0}E\Delta(h)=0 \tag{1.12}\]

and hence, \(\phi_{X}(\cdot)\) is uniformly continuous on \(\mathbb{R}.\)\(\Box\)

**Theorem 10.1.4:** _Let \(X\) be a random variable with characteristic function \(\phi_{X}(\cdot).\) If \(E|X|^{r}<\infty\) for some \(r\in\mathbb{N}\), then \(\phi_{X}(\cdot)\) is \(r\)-times continuously differentiable and_

\[\phi_{X}^{(r)}(t)=E(\iota X)^{r}\exp(\iota tX),\ \ t\in\mathbb{R}. \tag{1.13}\]

For proving the theorem, the following bound on the function \(\exp(\iota x)\) is useful.

**Lemma 10.1.5:** _For any \(x\in\mathbb{R}\), \(r\in\mathbb{N}\),_

\[\Big{|}\exp(\iota x)-\sum_{k=0}^{r-1}\frac{(\iota x)^{k}}{k!}\Big{|}\leq\min \bigg{\{}\frac{|x|^{r}}{r!},\frac{2|x|^{r-1}}{(r-1)!}\bigg{\}}. \tag{1.14}\]

**Proof:** Note that for any \(x\in\mathbb{R}\) and for any \(r\in\mathbb{N}\),

\[\frac{d^{r}}{dx^{r}}[\exp(\iota x)] = \Big{[}\frac{d^{r}}{dx^{r}}\cos x\Big{]}+\iota\Big{[}\frac{d^{r}}{ dx^{r}}\sin x\Big{]} \tag{1.15}\] \[= \iota^{r}\exp(\iota x).\]

Hence, by (1.15) and Taylor's expansion (applied to the functions \(\sin x\) and \(\cos x\) of a real variable \(x\)), for any \(x\in\mathbb{R}\) and \(r\in\mathbb{N}\),

\[\exp(\iota x)=\sum_{k=0}^{r-1}\frac{(\iota x)^{k}}{k!}+\frac{(\iota x)^{r}}{(r- 1)!}\int_{0}^{1}(1-u)^{r}\exp(\iota ux)du. \tag{1.16}\]

Hence, for any \(x\in\mathbb{R}\) and any \(r\in\mathbb{N}\),

\[\bigg{|}\exp(\iota x)-\sum_{k=0}^{r-1}\frac{(\iota x)^{k}}{k!}\bigg{|}\leq \frac{|x|^{r}}{r!}. \tag{1.17}\]

Also, for \(r\geq 2\), using (1.17) with \(r\) replaced by \(r-1\), one gets

\[\bigg{|}\exp(\iota x)-\sum_{k=0}^{r-1}\frac{(\iota x)^{k}}{k!} \bigg{|} \tag{1.18}\] \[\leq \bigg{|}\exp(\iota x)-\sum_{k=0}^{r-2}\frac{(\iota x)^{k}}{k!} \bigg{|}+\frac{|x|^{r-1}}{(r-1)!}\] \[\leq \frac{2|x|^{r-1}}{(r-1)!}.\]

Hence, by (1.17) and (1.18), (1.14) holds for all \(x\in\mathbb{R}\), \(r\in\mathbb{N}\) with \(r\geq 2\). For \(r=1\), (1.14) follows from (1.17) and the bound '\(\sup_{x}|\exp(\iota x)-1|\leq 2\).' \(\Box\)

Lemma 10.1.5 gives two upper bounds on the difference between the function \(\exp(\iota x)\) and its \((r-1)\)th order Taylor's expansion around \(x=0\). For small values of \(|x|\), the first bound (i.e., \(\frac{|x|^{r}}{r!}\)) is more accurate, whereas for large values of \(|x|\), the other bound (i.e., \(\frac{2|x|^{r-1}}{(r-1)!}\)) is more accurate.

**Proof of Theorem 10.1.4:** Let \(\mu\) denote the probability distribution of \(X\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\). Suppose that \(E|X|<\infty\). First it will be shown that \(\phi_{X}(\cdot)\) is differentiable with \(\phi_{X}^{(1)}(t)=E\{\iota X\exp(\iota tX)\}\), \(t\in\mathbb{R}\). Fix \(t\in\mathbb{R}\). For any \(h\in\mathbb{R}\), \(h\neq 0\),

\[h^{-1}[\phi_{X}(t+h)-\phi_{X}(t)]\] \[= \int_{\mathbb{R}}\exp(\iota tx)\Big{[}\frac{\exp(\iota hx)-1}{h} \Big{]}\mu(dx)\] \[= \int_{\mathbb{R}}\exp(\iota tx)\Big{[}\frac{\exp(\iota hx)-1}{h} -\iota x\Big{]}\mu(dx)+\int_{\mathbb{R}}\iota x\exp(\iota tx)\mu(dx)\]

[MISSING_PAGE_FAIL:333]

Finally, \(\mbox{Re}(\phi_{X})(t)=\frac{1}{2}(\phi_{X}(t)+\bar{\phi}_{X}(t))=\int\exp(ttx)\mu( dx)\), \(t\in\mathbb{R}\), where \(\mu(A)=2^{-1}\big{[}P(X\in A)+P(-X\in A)\big{]}\), \(A\in\mathcal{B}(\mathbb{R})\). \(\Box\)

**Definition 10.1.3:** A function \(\phi:\mathbb{R}\to\mathbb{C}\), the set of complex numbers is said to be _nonnegative definite_ if for any \(k\in\mathbb{N}\), \(t_{1},t_{2},\ldots,t_{k}\in\mathbb{R}\), \(\alpha_{1},\alpha_{2},\ldots,\alpha_{k}\in\mathbb{C}\)

\[\sum_{i=1}^{k}\sum_{j=1}^{k}\alpha_{i}\bar{\alpha}_{j}\,\phi(t_{i}-t_{j})\geq 0. \tag{1.24}\]

**Proposition 10.1.7:** _Let \(\phi(\cdot)\) be the characteristic function of a random variable \(X\). Then \(\phi\) is nonnegative definite._

**Proof:** Check that for \(k\), \(\{t_{i}\}\), \(\{\alpha_{i}\}\) as in Definition 10.1.3,

\[\sum_{i=1}^{k}\sum_{j=1}^{k}\alpha_{i}\bar{\alpha}_{j}\,\phi(t_{i}-t_{j})=E \bigg{(}\Big{|}\sum_{j=1}^{k}\alpha_{j}e^{\iota t_{j}X}\Big{|}^{2}\bigg{)}.\]

\(\Box\)

A converse to the above is known as the Bochner-Khinchine theorem, which states that if \(\phi:\mathbb{R}\to\mathbb{C}\) is nonnegative definite, continuous, and \(\phi(0)=1\), then \(\phi\) is the characteristic function of a random variable \(X\). For a proof, see Chung (1974).

Another criterion for a function \(\phi:\mathbb{R}\to\mathbb{C}\) to be a characteristic function is due to Polya. For a proof, see Chung (1974).

**Proposition 10.1.8:** (_Polya's criterion_). _Let \(\phi:\mathbb{R}\to\mathbb{C}\) satisfy \(\phi(0)=1\), \(\phi(t)\geq 0\), \(\phi(t)=\phi(-t)\) for all \(t\in\mathbb{R}\) and \(\phi(\cdot)\) is nonincreasing and convex on \([0,\infty)\). Then \(\phi\) is a characteristic function._

### Inversion formulas

Let \(F\) be a cdf and \(\phi\) be its characteristic function. In this section, two inversion formulas to get the cdf \(F\) from \(\phi\) are presented. The first one is from Feller (1966), and the second one is more standard.

_Unless otherwise mentioned, for the rest of this section, \(X\) will be a random variable with cdf \(F\) and characteristic function \(\phi_{X}\) and \(N\) a standard normal random variable independent of \(X\)._

**Lemma 10.2.1:** _Let \(g:\mathbb{R}\to\mathbb{R}\) be a Borel measurable bounded function vanishing outside a bounded set and let \(\epsilon\in(0,\infty)\). Then_

\[Eg(X+\epsilon N)=\frac{1}{2\pi}\int\int g(x)\phi_{X}(t)e^{-\iota tx}e^{-\frac{ \epsilon^{2}t^{2}}{2}}dtdx. \tag{2.1}\]

**Proof:** The integrand on the right is bounded by \(e^{-\frac{e^{2}t^{2}}{2}}|g(x)|\) and so is integrable on \(\mathbb{R}\times\mathbb{R}\) with respect to the Lebesgue measure on \((\mathbb{R}^{2},\mathcal{B}(\mathbb{R}^{2}))\). Further, \(\phi_{X}(t)=\int e^{tt}ydF(y)\) and \(e^{-\frac{t^{2}}{2}}=\frac{1}{\sqrt{2\pi}}\int e^{ttx}e^{-\frac{x^{2}}{2}}dx\), \(t\in\mathbb{R}.\) By repeated applications of Fubini's theorem and the above two identities, the right side of (2.1) becomes

\[\frac{1}{\sqrt{2\pi}\epsilon}\int g(x)\int\bigg{(}\int\frac{ \epsilon}{\sqrt{2\pi}}e^{tt(y-x)}e^{\frac{-\epsilon^{2}t^{2}}{2}}dt\bigg{)}dF( y)dx\] \[\mbox{[ set }s=\epsilon t]\] \[= \frac{1}{\sqrt{2\pi}\epsilon}\int g(x)\int\bigg{(}\int\frac{1}{ \sqrt{2\pi}}e^{\epsilon s(y-x)/\epsilon}e^{-s^{2}/2}ds\bigg{)}dF(y)dx\] \[= \int g(x)\bigg{(}\frac{1}{\sqrt{2\pi}\epsilon}\int e^{-\frac{(y- x)^{2}}{2\epsilon^{2}}}dF(y)\bigg{)}dx.\]

Since \(X\) and \(N\) are independent and \(N\) has an absolutely continuous distribution w.r.t. the Lebesgue measure, \(X+\epsilon N\) also has an absolutely continuous distribution with density

\[f_{X+\epsilon N}(x)=\frac{1}{\sqrt{2\pi}\epsilon}\int e^{-\frac{(y-x)^{2}}{2 \epsilon^{2}}}dF(y),\ \ x\in\mathbb{R}.\]

Thus, the right side of (2.1) reduces to

\[\int g(x)f_{X+\epsilon N}(x)dx=Eg(X+\epsilon N).\]

\(\Box\)

**Corollary 10.2.2:** _Let \(g:\mathbb{R}\rightarrow\mathbb{R}\) be continuous and let \(g(x)=0\) for all \(|x|>K\), for some \(K\), \(0<K<\infty.\) Then_

\[Eg(X) = \int g(x)dF(x) \tag{2.2}\] \[= \lim_{\epsilon\to 0+}\int\int\frac{1}{2\pi}g(x)e^{-ttx} \phi_{X}(t)e^{-\frac{\epsilon^{2}t^{2}}{2}}dtdx.\]

**Proof:** This follows from Lemma 10.2.1, the fact that \(X+\epsilon N\to X\) w.p. \(1\) as \(\epsilon\to 0\), and the BCT. \(\Box\)

**Corollary 10.2.3:** (_Feller's inversion formula_). _Let \(a\) and \(b\), \(-\infty<a<b<\infty\), be two continuity points of \(F.\) Then_

\[F(b)-F(a)=\lim_{\epsilon\to 0+}\int_{a}^{b}\bigg{(}\frac{1}{2\pi}\int e^{- \iota tx}\phi_{X}(t)e^{-\frac{\epsilon^{2}t^{2}}{2}}dt\bigg{)}dx. \tag{2.3}\]

**Proof:** This follows from Lemma 10.2.1 and Theorem 9.4.2, since the function \(g(x)=1\) for \(a\leq x\leq b\) and \(0\) otherwise is continuous except at \(a\) and \(b\), which are continuity points of \(F\). \(\Box\)

**Corollary 10.2.4:** _If \(\phi_{X}(t)\) is integrable w.r.t. the Lebesgue measure \(m\) on \(\mathbb{R}\), then \(F\) is absolutely continuous with density w.r.t. \(m\), given by_

\[f(x)=\frac{1}{2\pi}\int e^{-ttx}\phi_{X}(t)dt,\ \ x\in\mathbb{R}. \tag{2.4}\]

**Proof:** If \(\phi_{X}\) is integrable, then

\[\frac{1}{2\pi}\int\phi_{X}(t)e^{-\iota tx}e^{-\frac{a^{2}t^{2}}{2}}dt\]

is bounded by \((2\pi)^{-1}\int|\phi_{X}(t)|dt\) for all \(x\in\mathbb{R}\), and it converges to \((2\pi)^{-1}\int e^{-ttx}\phi_{X}(t)dt\) as \(\epsilon\to 0+\) for each \(x\in\mathbb{R}\). Hence, by the BCT and Corollary 10.2.3, for any \(a,b,-\infty<a<b<\infty\), that are continuity points of \(F\)

\[F(b)-F(a)=\int_{a}^{b}\Big{[}\frac{1}{2\pi}\int\phi_{X}(t)e^{-\iota tx}dt\Big{]} dx.\]

Since \(F\) has at most countably many discontinuity points and \(F\) is right continuous, the above relation holds for all \(-\infty<a<b<\infty\). \(\Box\)

**Remark 10.2.1:** The integrability of \(\phi_{X}\) in Corollary 10.2.4 is only a sufficient condition. The standard exponential distribution has characteristic function \((1-\iota t)^{-1}\) which is not integrable but the distribution is absolutely continuous.

**Corollary 10.2.5:** (_Uniqueness_). _The characteristic function \(\phi_{X}\) determines \(F\) uniquely._

**Proof:** Since a cdf \(F\) is uniquely determined by its values on the set of its continuity points, this corollary follows from Corollary 10.2.3. \(\Box\)

A more standard inversion formula is the following.

**Theorem 10.2.6:** _Let \(F\) be a cdf on \(\mathbb{R}\) and \(\phi(t)\equiv\int e^{\iota tx}dF(x)\), \(t\in\mathbb{R}\) be its characteristic function._

* _For any_ \(a<b\)_,_ \(a,b\in\mathbb{R}\)_, that are continuity points of_ \(F\)_,_ \[\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^{T}\frac{e^{-\iota ta}-e^{-\iota tb}} {\iota t}\phi(t)dt=\mu_{F}((a,b)),\] (2.5) _where_ \(\mu_{F}\) _is the Lebesgue-Stieltjes measure generated by_ \(F\)_._
* _For any_ \(a\in\mathbb{R}\)_,_ \[\mu_{F}(\{a\})=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}e^{-\iota ta}\phi(t)dt.\] (2.6)A multivariate extension of part (i) and its proof are given in Section 10.4. See also Problem 10.4. For a proof of part (ii), see Problem 10.5 or see Chung (1974) or Durrett (2004).

**Remark 10.2.2:** (_Inversion formula for integer valued random variables_). If \(X\) is _integer valued_ with \(p_{k}=P(X=k)\), \(k\in\mathbb{Z}\), then its characteristic function is the _Fourier series_

\[\phi(t)=\sum_{k\in\mathbb{Z}}p_{k}e^{\iota tk},\ \ t\in\mathbb{R}. \tag{2.7}\]

Since \(\int_{-\pi}^{\pi}e^{\iota tj}dt=2\pi\) if \(j=0\) and \(=0\) otherwise, multiplying both sides of (2.7) by \(e^{-\iota tk}\) and integrating over \(t\in(-\pi,\pi)\) and using DCT, one gets

\[p_{k}=\frac{1}{2\pi}\int_{-\pi}^{\pi}\phi(t)e^{-\iota tk}dt,\ \ k\in\mathbb{Z}. \tag{2.8}\]

As a corollary to part (ii) of Theorem 10.2.6, one can deduce a criterion for a distribution to be continuous. Let \(\mu\) be a probability distribution and let \(\{p_{j}\}\) be its atoms, if any. Let \(\alpha=\sum_{j}p_{j}^{2}\). Let \(X\) and \(Y\) be two independent random variables with distribution \(\mu\) and characteristic function \(\phi(\cdot)\). Then \(Z=X-Y\) has characteristic function \(|\phi(\cdot)|^{2}\) and by Theorem 10.2.6, part (ii),

\[P(Z=0)=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}|\phi(t)|^{2}dt.\]

But \(P(Z=0)=\alpha\). Hence, it follows that

\[\sum_{j\in Z}p_{j}^{2}=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}|\phi(t)|^{2}dt. \tag{2.9}\]

**Corollary 10.2.7:** _A distribution is continuous iff_

\[\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}|\phi(t)|^{2}dt=0. \tag{2.10}\]

Some consequences of the uniqueness result (cf. Corollary 10.2.5) are the following.

**Corollary 10.2.8:** _For a random variable \(X\), \(X\) and \(-X\) have the same distribution iff the characteristic function \(\phi_{X}(t)\) of \(X\) is real valued for all \(t\in\mathbb{R}\)._

**Proof:** If \(\phi_{X}(t)\) is real, then

\[\phi_{X}(t)=\int(\cos tx)dF(x)\ \ \ \mbox{for all}\ \ \ t\in\mathbb{R},\]where \(F\) is the cdf of \(X\). So

\[\phi_{X}(t)=\phi_{X}(-t)=E(e^{-\iota tX})=E(e^{\iota t(-X)}). \tag{2.11}\]

Since the characteristic function of \(-X\) coincides with \(\phi_{X}(t)\), the 'if part' follows.

To prove the 'only if' part, suppose that \(X\) and \(-X\) have the same distribution. Then as in (2.11),

\[\phi_{X}(t)=\phi_{-X}(t)=\phi_{X}(-t)=\overline{\phi_{X}(t)},\]

where for any complex number \(z=a+\iota b\), \(a,b\in\mathbb{R}\), \(\bar{z}\equiv a-\iota b\) denotes its conjugate. Hence, \(\phi_{X}(t)\) is real for all \(t\in\mathbb{R}\). \(\Box\)

**Example 10.2.1:** The standard Cauchy distribution has density

\[f(x)=\frac{1}{\pi}\frac{1}{1+x^{2}},\ \ -\infty<x<\infty. \tag{2.12}\]

Its characteristic function is given by

\[\phi(t)=\frac{1}{\pi}\int\frac{e^{\iota tx}}{1+x^{2}}dx=e^{-|t|},\ \ t\in \mathbb{R}. \tag{2.13}\]

To see this, let \(X_{1}\) and \(X_{2}\) be two independent copies of the standard exponential distribution. Since \(\phi_{X_{1}}(t)=(1-\iota t)^{-1}\), \(t\in\mathbb{R}\), \(Y\equiv X_{1}-X_{2}\) has characteristic function

\[\phi_{Y}(t)=|\phi_{X_{1}}(t)|^{2}=(1+t^{2})^{-1},\ \ \ t\in\mathbb{R}.\]

Since \(\phi_{Y}\) is integrable, the density of \(Y\) is

\[f_{Y}(y)=\frac{1}{2\pi}\int\frac{1}{1+u^{2}}e^{-\iota uy}du,\ \ y\in\mathbb{R}.\]

But by the convolution formula, \(f_{Y}(y)=\int_{x>-y}e^{-x}e^{-(y+x)}dx=\int_{0}^{\infty}e^{-x}e^{-(y+x)}1\!\!1_ {(0,\infty)}(y+x)dx=\frac{1}{2}e^{-|y|}\), \(y\in\mathbb{R}\). So

\[\frac{1}{\pi}\int\frac{1}{1+u^{2}}e^{\iota uy}dt=e^{-|y|},\ \ y\in\mathbb{R},\]

proving (2.13).

### Levy-Cramer continuity theorem

Characteristic functions are very useful in determining distributions, moments, and establishing various identities involving distributions. But by far their most important use is in establishing convergence in distribution. This is the content of a continuity theorem established by Paul Levy and H. Cramer. It says that the map \(\psi\) taking a distribution \(F\) to its characteristic function \(\phi\) is a homeomorphism. That is, if \(F_{n}\longrightarrow^{d}F\), then \(\phi_{n}\rightarrow\phi\) and conversely. Here, the notion of convergence of \(\phi_{n}\) to \(\phi\) is that of uniform convergence on bounded intervals. The following result deals with the 'if' part.

**Theorem 10.3.1:**_Let \(F_{n}\), \(n\geq 1\) and \(F\) be cdfs with characteristic functions \(\phi_{n}\), \(n\geq 1\) and \(\phi\), respectively. Let \(F_{n}\longrightarrow^{d}F\). Then, for each \(0<K<\infty\),_

\[\sup_{|t|\leq K}|\phi_{n}(t)-\phi(t)|\to 0\quad\mbox{as}\quad n \rightarrow\infty.\]

_That is, \(\phi_{n}\) converges to \(\phi\) uniformly on bounded intervals._

**Proof:** By Skorohod's theorem, there exist random variables \(X_{n}\), \(X\) defined on the Lebesgue space \(\left([0,1],{\cal B}([0,1]),m\right)\) where \(m(\cdot)\) is the Lebesgue measure such that \(X_{n}\sim F_{n}\), \(X\sim F\) and \(X_{n}\to X\) w.p. 1. Now, for any \(t\in\mathbb{R}\),

\[|\phi_{n}(t)-\phi(t)| = \left|E\Big{(}e^{\iota tX_{n}}-e^{\iota tX}\Big{)}\right|\] \[\leq E\Big{(}\Big{|}1-e^{\iota t(X-X_{n})}\Big{|}\Big{)}\] \[\leq E\Big{(}\Big{|}1-e^{\iota t(X-X_{n})}\Big{|}1\!\!1(|X-X_{n}|\leq \epsilon)\Big{)}\] \[+P(|X_{n}-X|>\epsilon).\]

Hence,

\[\sup_{|t|\leq K}|\phi_{n}(t)-\phi(t)|\leq\bigg{(}\sup_{|u|\leq K\epsilon}|1-e^ {\iota u}|\bigg{)}+P(|X_{n}-X|>\epsilon).\]

Given \(K\) and \(\delta>0\), choose \(\epsilon\in(0,\infty)\) small such that

\[\sup_{|u|\leq K\epsilon}|1-e^{\iota u}|<\delta.\]

Since for all \(\epsilon>0\), \(P(|X_{n}-X|>\epsilon)\to 0\) as \(n\rightarrow\infty\), it follows that

\[\lim_{n\rightarrow\infty}\sup_{|t|\leq K}|\phi_{n}(t)-\phi(t)|=0.\]

\(\Box\)

The Levy-Cramer theorem is a converse to the above theorem. That is, if \(\phi_{n}\rightarrow\phi\) uniformly on bounded intervals, then \(F_{n}\longrightarrow^{d}F\). Actually, it is a stronger result than this converse. It says that it is enough to know that \(\phi_{n}\) converges pointwise to a limit \(\phi\) that is continuous at 0. Then \(\phi\) is the characteristic function of some distribution \(F\) and \(F_{n}\longrightarrow^{d}F\). The key to this is that under the given hypotheses, the family \(\{F_{n}\}_{n\geq 1}\) is _tight_.

The next result relates the tail behavior of a probability measure to the behavior of its characteristic function near the origin, which in turn will be used to establish the tightness of \(\{F_{n}\}_{n\geq 1}\).

**Lemma 10.3.2:**_Let \(\mu\) be a probability measure on \(\mathbb{R}\) with characteristic function \(\phi\). Then, for each \(\delta>0\),_

\[\mu\big{(}\{x:|x|\delta\geq 2\}\big{)}\leq\frac{1}{\delta}\int_{-\delta}^{ \delta}(1-\phi(u))du.\]

**Proof:** Fix \(\delta\in(0,\infty)\). Then, using Fubini's theorem and the fact that \(1-\frac{\sin x}{x}\geq 0\) for all \(x\), one gets

\[\int_{-\delta}^{\delta}(1-\phi(u))du = \int\bigg{(}\int_{-\delta}^{\delta}(1-e^{\iota ux})du\bigg{)} \mu(dx)\] \[= \int\Big{[}2\delta-\frac{2\sin\delta x}{x}\Big{]}\mu(dx)\] \[= 2\delta\int\Big{[}1-\frac{\sin\delta x}{x\delta}\Big{]}\mu(dx)\] \[\geq 2\delta\int_{\{x:|x\delta|\geq 2\}}\Big{(}1-\frac{1}{|x\delta|} \Big{)}\mu(dx)\] \[\geq \delta\mu\big{(}\{x:|x|\delta\geq 2\}\big{)}.\]

\(\Box\)

**Lemma 10.3.3:**_Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of probability measures with characteristic functions \(\{\phi_{n}\}_{n\geq 1}\). Let \(\lim_{n\to\infty}\phi_{n}(t)\equiv\phi(t)\) exist for \(|t|\leq\delta_{0}\) for some \(\delta_{0}>0\). Let \(\phi(\cdot)\) be continuous at 0. Then \(\{\mu_{n}\}_{n\geq 1}\) is tight._

**Proof:** For any \(0<\delta<\delta_{0}\), by the BCT,

\[\frac{1}{\delta}\int_{-\delta}^{\delta}(1-\phi_{n}(t))dt\to\frac{1}{\delta}\int _{-\delta}^{\delta}[1-\phi(t)]dt.\]

Also, by continuity of \(\phi\) at 0,

\[\frac{1}{\delta}\int_{-\delta}^{\delta}[1-\phi(t)]dt\to 0\quad\mbox{as}\quad \delta\to 0.\]

Thus, given \(\epsilon>0\), there exists a \(\delta_{\epsilon}\in(0,\delta_{0})\) and an \(M_{\epsilon}\in(0,\infty)\) such that for all \(n\geq M_{\epsilon}\),

\[\frac{1}{\delta_{\epsilon}}\int_{-\delta_{\epsilon}}^{\delta_{\epsilon}}(1- \phi_{n}(t))dt<\epsilon.\]

By Lemma 10.3.2, this implies that for all \(n\geq M_{\epsilon}\),

\[\mu_{n}\Big{(}\Big{\{}x:|x|\geq\frac{2}{\delta_{\epsilon}}\Big{\}}\Big{)}<\epsilon.\]Now choose \(K_{\epsilon}>\frac{2}{\delta_{\epsilon}}\) such that

\[\mu_{j}\big{(}\{x:|x|\geq K_{\epsilon}\}\big{)}<\epsilon\quad\mbox{for}\quad 1 \leq j\leq M_{\epsilon}.\]

Then,

\[\sup_{n\geq 1}\mu_{n}\big{(}\{x:|x|\geq K_{\epsilon}\}\big{)}<\epsilon\]

and hence, \(\{\mu_{n}\}_{n\geq 1}\) is tight. \(\Box\)

**Theorem 10.3.4:** (_Levy-Cramer continuity theorem_). _Let \(\{\mu_{n}\}_{n\geq 1}\) be a sequence of probability measures on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) with characteristic functions \(\{\phi_{n}\}_{n\geq 1}\). Let \(\lim_{n\to\infty}\phi_{n}(t)\equiv\phi(t)\) exist for all \(\in\mathbb{R}\) and let \(\phi\) be continuous at 0. Then \(\phi\) is the characteristic function of a probability measure \(\mu\) and \(\mu_{n}\longrightarrow^{d}\mu\)._

**Proof:** By Lemma 10.3.3, \(\{\mu_{n}\}_{n\geq 1}\) is tight. Let \(\{\mu_{n_{j}}\}_{j\geq 1}\) be any subsequence of \(\{\mu_{n}\}_{n\geq 1}\) that converges vaguely to a limit \(\mu\). By tightness, \(\mu\) is a probability measure and by Theorem 10.3.1, \(\lim_{j\to\infty}\phi_{n_{j}}(t)\) is the characteristic function of \(\mu\). That is, \(\phi\) is the characteristic function of \(\mu\). Since \(\phi\) determines \(\mu\) uniquely, all vague limit points of \(\{\mu_{n}\}_{n\geq 1}\) coincide with \(\mu\) and hence by Theorem 9.2.6, \(\mu_{n}\longrightarrow^{d}\mu\). \(\Box\)

This theorem will be used extensively in the next chapter on central limit theorems. For the moment, some easy applications are given.

**Example 10.3.1:** (_Convergence of Binomial to Poisson_). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables such that \(X_{n}\sim\mbox{Binomial}(N_{n},p_{n})\) for all \(n\geq 1\). Suppose that as \(n\to\infty\), \(N_{n}\to\infty\), \(p_{n}\to 0\) and \(N_{n}p_{n}\to\lambda\), \(\lambda\in(0,\infty)\). Then

\[X_{n}\longrightarrow^{d}X\quad\mbox{where}\quad X\sim Poisson(\lambda). \tag{3.1}\]

To prove (3.1), note that the characteristic function \(\phi_{n}\) of \(X_{n}\) is

\[\phi_{n}(t) = (p_{n}e^{\iota t}+1-p_{n})^{N_{n}}\] \[= \big{(}1+p_{n}(e^{\iota t}-1)\big{)}^{N_{n}}\] \[= \Big{(}1+\frac{N_{n}p_{n}}{N_{n}}(e^{\iota t}-1)\Big{)}^{N_{n}}, \ t\in\mathbb{R}.\]

Next recall the fact that if \(\{z_{n}\}_{n\geq 1}\) is a sequence of complex numbers such that \(\lim_{n\to\infty}z_{n}=z\) exists, then

\[(1+n^{-1}z_{n})^{n}\to z\quad\mbox{as}\quad n\to\infty. \tag{3.2}\]

So \(\phi_{n}(t)\to e^{\lambda(e^{\iota t}-1)}\) for all \(t\in\mathbb{R}\). Since \(\phi(t)\equiv e^{\lambda(e^{\iota t}-1)}\), \(t\in\mathbb{R}\) is the characteristic function of a Poisson (\(\lambda\)) random variable, (3.1) follows.

A direct proof of (3.1) consists of showing that for each \(j=0,1,2,\dots\)

\[P(X_{n}=j)\equiv{N_{n}\choose j}p_{n}^{j}(1-p_{n})^{N_{n}-j}\to P(X=j)=\frac{e^{- \lambda}\lambda^{j}}{j!}.\]

**Example 10.3.2:** (_Convergence of Binomial to Normal_). Let \(X_{n}\sim\mbox{Binomial}(N_{n},p_{n})\) for all \(n\geq 1.\) Suppose that as \(n\to\infty,\)\(N_{n}\to\infty\) and \(s_{n}^{2}\equiv N_{n}p_{n}(1-p_{n})\to\infty.\) Then

\[Z_{n}\equiv\frac{X_{n}-N_{n}p_{n}}{s_{n}}\longrightarrow^{d}N(0,1). \tag{3.3}\]

To prove (3.3), note that the characteristic function \(\phi_{n}\) of \(Z_{n}\) is

\[\phi_{n}(t) = \big{[}p_{n}\exp(\iota t(1-p_{n})/s_{n})+(1-p_{n})\exp(-\iota tp_ {n}/s_{n})\big{]}^{N_{n}}\] \[\equiv \Big{[}1+\frac{z_{n}(t)}{N_{n}}\Big{]}^{N_{n}},\quad\mbox{say},\]

where \(z_{n}(t)=N_{n}\big{[}\big{(}p_{n}e^{\frac{\iota t}{s_{n}}(1-p_{n})}+(1-p_{n})e ^{\frac{-\iota tp_{n}}{s_{n}}}\big{)}-1\big{]}.\) By (3.2), it suffices to show that for all \(t\in\mathbb{R},\)

\[z_{n}(t)\to-\frac{t^{2}}{2}\quad\mbox{as}\quad n\to\infty.\]

By Lemma 10.1.5, for any \(x\) real,

\[\Big{|}e^{\iota x}-1-\iota x-\frac{(\iota x)^{2}}{2}\Big{|}\leq\frac{|x|^{3}}{ 3!}. \tag{3.4}\]

Since \(s_{n}\to\infty,\) for any \(t\in\mathbb{R},\) with \(p_{n}(t)\equiv tp_{n}/s_{n}\) and \(q_{n}(t)\equiv t(1-p_{n})/s_{n},\) one has

\[z_{n}(t) = N_{n}\Big{[}\big{\{}p_{n}\exp(\iota t(1-p_{n})/s_{n})+(1-p_{n}) \exp(-\iota tp_{n}/s_{n})\big{\}}-1\Big{]}\] \[= N_{n}\Big{[}p_{n}\big{\{}e^{\iota q_{n}(t)}-1-\iota q_{n}(t) \big{\}}+(1-p_{n})\big{\{}e^{\iota p_{n}(t)}-1-\iota p_{n}(t)\big{\}}\Big{]}\] \[= N_{n}\Big{[}\frac{p_{n}}{2}(\iota q_{n}(t))^{2}+\frac{1-p_{n}}{2 }(\iota p_{n}(t))^{2}\Big{]}\] \[\quad+\;N_{n}O\Big{(}\frac{p_{n}(1-p_{n})|t|^{3}}{s_{n}^{3}}\Big{)}\] \[= \frac{-t^{2}}{2}+o(1)\quad\mbox{as}\quad n\to\infty.\]

This is known as the DeMovire-Laplace CLT in the case \(N_{n}=n,\)\(p_{n}=p,\)\(0<p<1.\) The original proof was based on Stirling's approximation.

**Example 10.3.3:** (_Convergence of Poisson to Normal_). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables such that for \(n\geq 1\), \(X_{n}\sim\mbox{Poisson}(\lambda_{n})\), \(\lambda_{n}\in(0,\infty)\). Let \(Y_{n}=\frac{X_{n}-\lambda_{n}}{\sqrt{\lambda_{n}}}\), \(n\geq 1\). If \(\lambda_{n}\to\infty\) as \(n\to\infty\), then

\[Y_{n}\longrightarrow^{d}N(0,1). \tag{3.5}\]

To prove (3.5), note that the characteristic function \(\phi_{n}\) of \(Y_{n}\) is

\[\phi_{n}(t) = \exp\Big{(}-\iota t\sqrt{\lambda_{n}}\Big{)}\exp\Big{(}\lambda_{n }\Big{[}\exp\Big{(}\iota t/\sqrt{\lambda_{n}}\Big{)}-1\Big{]}\Big{)}\] \[= \exp\Big{(}\lambda_{n}\Big{[}\exp\Big{(}\iota t/\sqrt{\lambda_{n} }\Big{)}-1-\Big{(}\iota t/\sqrt{\lambda_{n}}\Big{)}\Big{]}\Big{)},\]

\(t\in\mathbb{R}\). Now using (3.4) again it is easy to show that for each \(t\in\mathbb{R}\),

\[\lambda_{n}\bigg{(}\exp\Big{(}\frac{\iota t}{\sqrt{\lambda_{n}}}\Big{)}-1- \frac{\iota t}{\sqrt{\lambda_{n}}}\bigg{)}\to\frac{-t^{2}}{2}\quad\mbox{as} \quad n\to\infty.\]

Hence, (3.5) follows.

### Extension to \(\mathbb{R}^{k}\)

**Definition 10.4.1:**

1. Let \(X=(X_{1},\ldots,X_{k})\) be a \(k\)-dimensional random vector (\(k\in\mathbb{N}\)). The characteristic function of \(X\) is defined as \[\phi_{X}(t) = E\exp(\iota t\cdot X)\] (4.1) \[= E\exp\bigg{(}\iota\sum_{j=1}^{k}t_{j}X_{j}\bigg{)},\] \(t=(t_{1},\ldots,t_{k})\in\mathbb{R}^{k}\), where \(t\cdot x=\sum_{j=1}^{k}t_{j}x_{j}\) denotes the inner product of the two vectors \(t=(t_{1},\ldots,t_{k})\), \(x=(x_{1},\ldots,x_{k})\in\mathbb{R}^{k}\).
2. For a probability measure \(\mu\) on \(\big{(}\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k})\big{)}\), its characteristic function is defined as \[\phi(t)=\int_{\mathbb{R}^{k}}\exp(\iota t\cdot x)\mu(dx).\] (4.2)

Note that for a linear combination \(L\equiv a_{1}X_{1}+\cdots+a_{k}X_{k}\), \(a_{1},\ldots,a_{k}\in\mathbb{R}\), of a set of random variables \(X_{1},\ldots,X_{k}\), all defined on a common probability space, the characteristic functions of \(L\) and \(X=(X_{1},\ldots,X_{k})\) arerelated by the identity

\[\phi_{L}(\lambda) = E\exp\left(\iota\lambda\sum_{j=1}^{k}a_{j}X_{j}\right) \tag{4.3}\] \[= \phi_{X}(\lambda a),\quad\lambda\in\mathbb{R},\]

where \(a=(a_{1},\ldots,a_{k})\). Thus, the characteristic function of a random vector \(X=(X_{1},\ldots,X_{k})\) is determined by the characteristic functions of all its linear combinations and vice versa. It will now be shown that as in the one-dimensional case, the characteristic function of a random vector \(X\) uniquely determines its probability distribution. The following is a multivariate version of Theorem 10.2.6.

**Theorem 10.4.1:** _Let \(X=(X_{1},\ldots,X_{k})\) be a random vector with characteristic function \(\phi_{X}(\cdot)\) and let \(A=(a_{1},b_{1}]\times\cdots\times(a_{k},b_{k}]\) be a rectangle in \(\mathbb{R}^{k}\) with \(-\infty<a_{i}<b_{i}<\infty\) for all \(i=1,\ldots,k\). If \(P(X\in\partial A)=0\), then_

\[P(X\in A) = \lim_{T\to\infty}\frac{1}{(2\pi)^{k}}\int_{-T}^{T}\cdots\int_{-T} ^{T}\prod_{j=1}^{k}h_{j}(t_{j}) \tag{4.4}\] \[\times\,\phi_{X}(t_{1},\ldots,t_{k})dt_{1}\ldots dt_{k},\]

_where \(\partial A\) denotes the boundary of \(A\) and where \(h_{j}(t_{j})\equiv\big{(}\exp(-\iota t_{j}a_{j})-\exp(-\iota t_{j}b_{j})\big{)} (\iota t_{j})^{-1}\) for \(t_{j}\neq 0\) and \(h_{j}(0)=(b_{j}-a_{j})\), \(1\leq j\leq k\)._

**Proof:** Consider the product space \(\Omega=[-T,T]^{k}\times\mathbb{R}^{k}\) with the corresponding Borel-\(\sigma\)-algebra \(\mathcal{F}=\mathcal{B}([-T,T]^{k})\times\mathcal{B}(\mathbb{R}^{k})\) and the product measure \(\mu=\mu_{1}\times\mu_{2}\), where \(\mu_{1}\) is the Lebesgue's measure on \(\big{(}[-T,T]^{k},\mathcal{B}([-T,T]^{k})\big{)}\) and \(\mu_{2}\) is the probability distribution of \(X\) on \(\big{(}\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k})\big{)}\). Since the function

\[f(t,x)\equiv\prod_{j=1}^{k}h_{j}(t_{j})\exp(\iota t\cdot x),\]

\((t,x)\in\Omega\) is integrable w.r.t. the product measure \(\mu\), by Fubini's theorem,

\[I_{T} \equiv \int_{-T}^{T}\cdots\int_{-T}^{T}\bigg{\{}\prod_{j=1}^{k}h_{j}(t_ {j})\bigg{\}}\phi_{X}(t_{1},\ldots,t_{k})dt_{1}\ldots dt_{k}\] \[= \int_{\mathbb{R}^{k}}\int_{-T}^{T}\cdots\int_{-T}^{T}\prod_{j=1}^ {k}\{h_{j}(t_{j})\exp(\iota t_{j}x_{j})\}dt_{1}\ldots dt_{k}\,\mu_{2}(dx)\] \[= \int_{\mathbb{R}^{k}}\prod_{j=1}^{k}\bigg{[}\int_{-T}^{T}\frac{ \exp(\iota t_{j}(x_{j}-a_{j}))-\exp(\iota t_{j}(x_{j}-b_{j}))}{\iota t_{j}}dt_ {j}\bigg{]}\mu_{2}(dx)\]\[= \int_{\mathbb{R}^{k}}\prod_{j=1}^{k}\bigg{[}2\int_{0}^{T}\frac{\sin t_{ j}(x_{j}-a_{j})}{t_{j}}dt_{j} \tag{4.5}\] \[\qquad\qquad-2\int_{0}^{T}\frac{\sin t_{j}(x_{j}-b_{j})}{t_{j}}dt _{j}\bigg{]}\mu_{2}(dx),\]

using (1.3) and the fact that \(\frac{\sin\theta}{\theta}\) and \(\frac{\cos\theta}{\theta}\) are respectively even and odd functions of \(\theta.\) It can be shown that (Problem 10.8)

\[\lim_{T\to\infty}\int_{0}^{T}\frac{\sin t}{t}dt=\pi/2. \tag{4.6}\]

Hence, by the change of variables theorem, it follows that, for any \(c\in\mathbb{R},\)

\[\lim_{T\to\infty}\int_{0}^{T}\frac{\sin tc}{t}dt=\left\{\begin{array}{ccc}0& \mbox{if}&c=0\\ \pi/2&\mbox{if}&c>0\\ -\pi/2&\mbox{if}&c<0\end{array}\right. \tag{4.7}\]

and

\[\sup_{T>0,c\in\mathbb{R}}\bigg{|}\int_{0}^{T}\frac{\sin tc}{t}\,dt\bigg{|}= \sup_{T>0}\bigg{|}\int_{0}^{T}\frac{\sin u}{u}\,du\bigg{|}\equiv K<\infty. \tag{4.8}\]

This implies that as \(T\to\infty,\) the integrand in (4.5) converges to the function \(\prod_{j=1}^{k}g_{j}(x_{j})\) for each \(x\in\mathbb{R}^{k},\) where

\[g_{j}(y)=\left\{\begin{array}{ccc}\pi&\mbox{if}&y\in\{a_{j},b_{j}\}\\ 2\pi&\mbox{if}&y\in(a_{j},b_{j})\\ 0&\mbox{if}&y\in(-\infty,a_{j})\cup(b_{j},\infty).\end{array}\right. \tag{4.9}\]

Hence, by (4.5), (4.8), (4.9), and the BCT,

\[\lim_{T\to\infty}I_{T}=\int_{\mathbb{R}^{k}}\prod_{j=1}^{k}g_{j}(x_{j})\mu_{2} (dx).\]

By the boundary condition \(P(X\in\partial A)=0,\) the right side above equals \((2\pi)^{k}P(X\in(a_{1},b_{1})\times\cdots\times(a_{k},b_{k})),\) proving the theorem. \(\Box\)

**Remark 10.4.1:** The inversion formula (2.3) can also be extended to the multivariate case.

**Corollary 10.4.2:**_A probability measure on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\) is uniquely determined by its characteristic function._

**Proof:** Let \(\mu\) and \(\nu\) be probability measures on \((\mathbb{R}^{k},\mathcal{B}(\mathbb{R}^{k}))\) with the same characteristic function \(\phi(\cdot),\) i.e.,

\[\phi(t)=\int\exp(\iota t\cdot x)\mu(dx)=\int\exp(\iota t\cdot x)\nu(dx),\]

[MISSING_PAGE_EMPTY:346]

for all \(t\in\mathbb{R}^{k}\). Thus, \(\phi_{X_{0}}(\cdot)=\phi_{X}(\cdot)\) and by the uniqueness of characteristic functions, \(X_{0}=^{d}X\). Thus, all convergent subsequences of \(\{X_{n}\}_{n\geq 1}\) have the same limit. By arguments similar to the proof of Theorem 9.2.6, \(X_{n}\longrightarrow^{d}X\). This completes the proof of the theorem. \(\Box\)

Theorem 10.4.4 shows that as in the one-dimensional case, the (pointwise) convergence of the characteristic functions of a sequence of \(k\)-dimensional random vectors \(\{X_{n}\}_{n\geq 1}\) to a given characteristic function is equivalent to convergence in distribution of the sequence \(\{X_{n}\}_{n\geq 1}\). Since the characteristic function of a random vector is determined by the characteristic functions of all its linear combinations, this suggests that one may also be able to establish convergence in distribution of a sequence of random vectors by considering the convergence of the sequences of linear combinations that are one-dimensional random variables. This is indeed true as shown by the following result.

**Theorem 10.4.5:** (_Cramer-Wold device_). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of \(k\)-dimensional random vectors and let \(X\) be a \(k\)-dimensional random vector. Then, \(X_{n}\longrightarrow^{d}X\) iff for all \(a\in\mathbb{R}^{k}\),

\[a\cdot X_{n}\longrightarrow^{d}a\cdot X. \tag{4.12}\]

**Proof:** Suppose \(X_{n}\longrightarrow^{d}X\). Then, for any \(a\in\mathbb{R}^{k}\), the function \(h(x)=a\cdot x\), \(x\in\mathbb{R}^{k}\) is continuous on \(\mathbb{R}^{k}\). Hence, (4.12) follows from Theorem 9.4.2.

Conversely, suppose that (4.12) holds for all \(a\in\mathbb{R}^{k}\). By (4.3) and Theorem 10.3.1, this implies that as \(n\to\infty\)

\[\phi_{X_{n}}(a) = \phi_{a\cdot X_{n}}(1)\] \[\to \phi_{a\cdot X}(1)=\phi_{X}(a),\]

for all \(a\in\mathbb{R}^{k}\). Hence, by Theorem 10.4.4, \(X_{n}\longrightarrow^{d}X\). \(\Box\)

Recall that a set of random variables \(X_{1},\ldots,X_{k}\) defined on a common probability space are independent iff the joint cdf of \(X_{1},\ldots,X_{k}\) is the product of the marginal cdfs of the \(X_{i}\)'s. A similar characterization of independence can be given in terms of the characteristic functions, as shown by the following result. The proof is left as an exercise (Problem 10.16).

**Proposition 10.4.6:** _Let \(X_{1},\ldots,X_{k}\), \((k\in\mathbb{N})\) be a collection of random variables defined on a common probability space. Then, \(X_{1},\ldots,X_{k}\) are independent iff_

\[\phi_{(X_{1},\ldots,X_{k})}(t_{1},\ldots,t_{k})=\prod_{j=1}^{k}\phi_{X_{j}}(t_ {j})\]

_for all \(t_{1},\ldots,t_{k}\in\mathbb{R}\)._

### Problems

10.1 Let \(\{X_{n}\}_{n\geq 1}\) and \(\{Y_{n}\}_{n\geq 1}\) be two sequences of random variables such that for each \(n\geq 1\), \(X_{n}\) and \(Y_{n}\) are defined on a common probability space and \(X_{n}\) and \(Y_{n}\) are independent. If \(X_{n}\longrightarrow^{d}X\) and \(Y_{n}\longrightarrow^{d}Y\), then show that \[X_{n}+Y_{n}\longrightarrow^{d}X_{0}+Y_{0}\] (5.1) where \(X_{0}=^{d}X\), \(Y_{0}=^{d}Y\) (cf. Section 2.2) and \(X_{0}\) and \(Y_{0}\) are independent. Show by an example that (5.1) is false without the independence hypothesis.
10.2 Give an example of a nonlattice discrete distribution \(F\) on \(\mathbb{R}\) supported by only a three point set.
10.3 Let \(F\) be an absolutely continuous cdf on \(\mathbb{R}\) with density \(f\) and with characteristic function \(\phi\). Show that if \(f\) has a derivative \(f^{(1)}\in L^{1}(\mathbb{R})\), then \[\lim_{|t|\to\infty}|t\phi(t)|=0.\] Generalize this result when \(f\) is \(r\)-times differentiable and the \(j\)th derivative \(f^{(j)}\) lie in \(L^{1}(\mathbb{R})\) for \(j=1,\ldots,r\).
10.4 Let \(F\) be a cdf on \(\mathbb{R}\) with characteristic function \(\phi\). Show that for any \(a<b\), \(a,b\in\mathbb{R}\), \[\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^{T}\big{[}\exp(-\iota ta )-\exp(-\iota tb)\big{]}(\iota t)^{-1}\phi(t)dt\] \[=\mu_{F}((a,b))+\frac{1}{2}\mu_{F}(\{a,b\}),\] (5.2) where \(\mu_{F}\) denotes the Lebesgue-Stieltjes measure corresponding to \(F\). (**Hint:** Use (4.7) and the arguments in the proof of Theorem 10.4.1.)
10.5 Let \(\phi\) be a characteristic function of a cdf \(F\) and let \(\mu_{F}\) denote the Lebesgue-Stieltjes measure corresponding to \(F\). 1. Show that for any \(a\in\mathbb{R}\) and \(T\in(0,\infty)\), \[\int_{-T}^{T}\exp(-\iota ta)\phi(t)dt\] \[= 2T\mu_{F}(\{a\})\] \[\quad+\int_{\{x\neq a\}}\frac{\exp(\iota T(x-a))-\exp(-\iota T(x- a))}{T(x-a)}\mu_{F}(dx).\]2. Conclude from (5.3) that for any \(a\in\mathbb{R}\), \[F(a)-F(a-)=\lim_{T\to\infty}\frac{1}{2T}\int_{-T}^{T}\exp(-\iota ta)\phi(t)dt.\] (5.4)
10.6 Let \(F\) be a cdf on \(\mathbb{R}\) with characteristic function \(\phi\). If \(|\phi|\in L^{2}(\mathbb{R})\), then show that \(F\) is continuous. (**Hint:** Use Corollary 10.1.7.)
10.7 Let \(\{F_{n}\}_{n\geq 1}\), \(F\) be cdfs with characteristic functions \(\{\phi_{n}\}_{n\geq 1}\), \(\phi\), respectively. Suppose that \(F_{n}\longrightarrow^{d}F\). 1. Give an example to show that \(\phi_{n}\) may not converge to \(\phi\) uniformly over all of \(\mathbb{R}\). (**Hint:** Try \(\phi_{n}(t)\equiv e^{-\frac{t^{2}}{n}}\).) 2. Let \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) denote the Lebesgue-Stieltjes measures corresponding to \(\{F_{n}\}_{n\geq 1}\) and \(F\), respectively. Suppose that \(\{\mu_{n}\}_{n\geq 1}\) and \(\mu\) are dominated by a \(\sigma\)-finite measure \(\lambda\) on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) with Radon-Nikodym derivatives \(f_{n}=\frac{d\mu_{n}}{d\lambda}\), \(n\geq 1\) and \(f=\frac{d\mu}{d\lambda}\). If \(f_{n}\longrightarrow f\) a.e. \((\lambda)\), then show that \[\sup_{t\in\mathbb{R}}|\phi_{n}(t)-\phi(t)|\to 0\quad\text{as}\quad n\to\infty.\] (5.5)
10.8 Let \(G(x,a)=(1+a^{2})^{-1}(1-e^{-ax}\{a\sin x+\cos x\})\), \(x\in\mathbb{R}\), \(a\in\mathbb{R}\). 1. Show that for any \(a>0\), \(x_{0}\geq 0\), \[\int_{0}^{x_{0}}(\sin x)\,e^{-ax}dx=G(a,x_{0}).\] (5.6) (**Hint:** Consider the derivatives of the left and the right sides of (5.6) w.r.t. \(x_{0}\).) 2. Use Fubini's theorem to justify that for all \(T>0\), \[\int_{0}^{T}\int_{0}^{\infty}(\sin x)\,e^{-ax}dadx=\int_{0}^{\infty}\int_{0}^{T }(\sin x)\,e^{-ax}dxda.\] (5.7) 3. Use (5.6), (5.7) and the identity that for \(x>0\), \(\int_{0}^{\infty}e^{-ax}da=\frac{1}{x}\) to conclude that for any \(T>0\) \[\int_{0}^{T}\frac{\sin x}{x}dx=\int_{0}^{\infty}G(a,T)da.\] (5.8) 4. Use the DCT and the fact that \(\int_{0}^{\infty}(1+a^{2})^{-1}da=\frac{\pi}{2}\) to conclude that the limit of the right side of (5.8) exists and equals \(\frac{\pi}{2}\).

10.9 Let \(F_{1}\), \(F_{2}\), and \(F_{3}\) be three cdfs on \(\mathbb{R}\). Then show by an example that \(F_{1}*F_{2}=F_{1}*F_{3}\) does not imply that \(F_{1}=F_{2}\). Here \(F_{i}*F_{j}\) denotes the convolution of \(F_{i}\) and \(F_{j}\), \(1\leq i,j\leq 3\). (**Hint:** For \(F_{1}\), consider a cdf whose characteristic function \(\phi\) has a bounded support.)
10.10 Let \(\mu\) be a probability measure on \(\mathbb{R}\) with characteristic function \(\phi\). Prove that \[\frac{2}{\pi}\int_{-\infty}^{\infty}[1-\operatorname{Re}(\phi(t))]t^{-2}dt= \int|x|\mu(dx).\]
10.11 Let \(\phi\) be the characteristic function of a random variable \(X\). If \(|\phi(t)|=1=|\phi(\alpha t)|\) for some \(t\neq 0\) and \(\alpha\in\mathbb{R}\) irrational, then there exists \(x_{0}\in\mathbb{R}\) such that \(P(X=x_{0})=1\). (**Hint:** Use Proposition 10.1.1.)
10.12 Show that for any characteristic function \(\phi\), \(\{t\in\mathbb{R}:|\phi(t)|=1\}\) is either \(\{0\}\) or countably infinite or all of \(\mathbb{R}\).
10.13 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with a nondegenerate distribution \(F\). Suppose that there exist \(a_{n}\in(0,\infty)\) and \(b_{n}\in\mathbb{R}\) such that \[a_{n}^{-1}\bigg{(}\sum_{j=1}^{n}X_{j}-b_{n}\bigg{)}\longrightarrow^{d}Z\] (5.9) for some nondegenerate random variable \(Z\). 1. Show that \[a_{n}\to\infty\quad\text{as}\quad n\to\infty.\] (**Hint:** If \(a_{n}\to a\in\mathbb{R}\), then \(E\exp(\iota taZ)=\lim_{n\to\infty}E\exp\bigg{(}\iota t\bigg{[}\sum_{j=1}^{n}X_ {j}-b_{n}\bigg{]}\bigg{)}=0\) for all except countably many \(t\in\mathbb{R}\), which leads to a contradiction.) 2. Show that as \(n\to\infty\) \[b_{n}-b_{n-1}=o(a_{n})\quad\text{and}\quad\frac{a_{n}}{a_{n-1}}\to 1.\] (**Hint:** Use (a) to show that \(\Big{(}\sum_{j=1}^{n-1}X_{j}-b_{n}\Big{)}/a_{n}\longrightarrow^{d}Z\) and by (5.9), \(\Big{(}\sum_{j=1}^{n-1}X_{j}-b_{n-1}\Big{)}/a_{n-1}\longrightarrow^{d}Z\).)* Show that for every \(T\in(0,\infty)\), there exist two distinct characteristic functions \(\phi_{1T}\) and \(\phi_{2T}\) satisfying \[\phi_{1T}(t)=\phi_{2T}(t)\quad\text{for all}\quad|t|\leq T.\] (**Hint:** Let \(\phi_{1}(t)=e^{-|t|}\), \(t\in\mathbb{R}\) and for any \(T\in(0,\infty)\), define an even function \(\phi_{2T}(\cdot)\) by \[\phi_{2T}(t)=\left\{\begin{array}{ll}\phi_{1}(t)&\text{for }0\leq t\leq T\\ \phi_{1}(T)+(t-T)(-\phi_{1}(T))&T\leq t<T+1\\ 0&t>T.\end{array}\right.\] Now use Polya's criterion.)
* Show that \(\phi_{\alpha}(t)=\exp(-|t|^{\alpha})\), \(t\in\mathbb{R}\), \(\alpha\in(0,\infty)\) is a characteristic function for \(0\leq\alpha\leq 2\).
* Prove Proposition 10.4.6. (**Hint:** The 'only if' part follows from (4.2) and Proposition 7.1.3. The 'if part' follows by using the inversion formulas of Theorems 10.4.1 and 10.2.6 and the characterization of independence in terms of cdfs (Corollary 7.1.2).)
* Let \(\{X_{n}\}_{n\geq 0}\) be a collection of random variables with characteristic functions \(\{\phi_{n}\}_{n\geq 0}\). Suppose that \(\int|\phi_{n}(t)|dt<\infty\) for all \(n\geq 0\) and that \(\phi_{n}(\cdot)\rightarrow\phi_{0}(\cdot)\) in \(L^{1}(\mathbb{R})\) as \(n\rightarrow\infty\). Show that \[\sup_{B\in\mathcal{B}(\mathbb{R})}\big{|}P(X_{n}\in B)-P(X_{0}\in B)\big{|} \to 0\] as \(n\rightarrow\infty\).
* Let \(\phi(\cdot)\) be a characteristic function on \(\mathbb{R}\) such that \(\phi(t)\to 0\) as \(|t|\rightarrow\infty\). Let \(X\) be a random variable with \(\phi\) as its characteristic function. For each \(n\geq 1\), let \(X_{n}=\frac{k}{n}\) if \(\frac{k}{n}\leq X<\frac{k+1}{n}\), \(k=0\), \(\pm 1,\pm 2,\ldots\). Show that if \(\phi_{n}(t)\equiv E(e^{itX_{n}})\), then \(\phi_{n}(t)\rightarrow\phi(t)\) for each \(t\in\mathbb{R}\) but for each \(n\geq 1\), \[\sup\big{\{}|\phi_{n}(t)-\phi(t)|:t\in\mathbb{R}\big{\}}=1.\]
* Let \(\{\delta_{i}\}_{i\geq 1}\) be iid random variables with distribution \[P(\delta_{1}=1)=P(\delta_{1}=-1)=1/2.\] Let \(X_{n}=\sum_{i=1}^{n}\frac{\delta_{i}}{2^{i}}\) and \(X=\lim_{n\rightarrow\infty}X_{n}\).
* Find the characteristic function of \(X_{n}\).
* 2. Show that the characteristic function of \(X\) is \(\phi_{X}(t)\equiv\frac{\sin t}{t}\).
* 3. Let \(\{X_{k}\}_{k\geq 1}\) be iid random variables with pdf \(f(x)=\frac{1}{2}\,e^{-|x|}\), \(x\in\mathbb{R}\). Show that \(\sum_{k=1}^{\infty}\frac{1}{k}\,X_{k}\) converges w.p. 1 and compute its characteristic function. (**Hint:** Note that the characteristic function of the standard Cauchy (0,1) distribution is \(e^{-|t|}\).)
* 4. Establish an extension of formula (2.3) to the multivariate case.

Central Limit Theorems

### 11.1 Lindeberg-Feller theorems

The central limit theorem (CLT) is one of the oldest and most useful results in probability theory. Empirical findings in applied sciences, dating back to the 17th century, showed that the averages of laboratory measurements on various physical quantities tended to have a bell-shaped distribution. The CLT provides a theoretical justification for this observation. Roughly speaking, it says that under some mild conditions, the average of a large number of iid random variables is approximately normally distributed. A version of this result for 0-1 valued random variables was proved by DeMoivre and Laplace in the early 18th century. An extension of this result to the averages of iid random variables with a finite second moment was done in the early 20th century. In this section, a more general set up is considered, namely, that of the limit behavior of the row sums of a triangular array of independent random variables.

**Definition 11.1.1:** For each \(n\geq 1\), let \(\{X_{n1},\ldots,X_{nr_{n}}\}\) be a collection of random variables defined on a probability space \((\Omega_{n},\mathcal{F}_{n},P_{n})\) such that \(X_{n1},\ldots,X_{nr_{n}}\) are independent. Then, \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) is called a _triangular array_ of independent random variables.

Let \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) be a triangular array of independent random variables. Define the row sums\[S_{n}=\sum_{j=1}^{r_{n}}X_{nj},\ n\geq 1. \tag{1.1}\]

Suppose that \(EX_{nj}^{2}<\infty\) for all \(j,n\). Write \(s_{n}^{2}=\mbox{Var}(S_{n})=\sum_{j=1}^{r_{n}}\mbox{Var}(X_{nj})\), \(n\geq 1\). The following condition, introduced by Lindeberg, plays an important role in establishing convergence of \(\big{(}\frac{S_{n}-ES_{n}}{s_{n}}\big{)}\) to a standard normal random variable in distribution.

**Definition 11.1.2:** Let \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) be a triangular array of independent random variables such that

\[EX_{nj}=0,\ 0<EX_{nj}^{2}\equiv\sigma_{nj}^{2}<\infty\quad\mbox{for all}\quad 1\leq j\leq r_{n},\ n\geq 1. \tag{1.2}\]

Then, \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) is said to satisfy the _Lindeberg condition_ if for every \(\epsilon>0\),

\[\lim_{n\to\infty}s_{n}^{-2}\sum_{j=1}^{r_{n}}EX_{nj}^{2}I(|X_{nj}|>\epsilon s_ {n})=0, \tag{1.3}\]

where \(s_{n}^{2}=\sum_{j=1}^{r_{n}}\sigma_{nj}^{2}\), \(n\geq 1\).

**Example 11.1.1:** Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=\mu\) and \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\). Consider the centered and scaled sample mean

\[T_{n}=\frac{\sqrt{n}(\bar{X}_{n}-\mu)}{\sigma},\ n\geq 1, \tag{1.4}\]

where \(\bar{X}_{n}=n^{-1}\sum_{j=1}^{n}X_{j}\). Note that \(T_{n}\) can be written as the row sum of a triangular array of independent random variables:

\[T_{n}=\sum_{j=1}^{n}X_{nj}, \tag{1.5}\]

where \(X_{nj}=(X_{j}-\mu)/\{\sigma\sqrt{n}\}\), \(1\leq j\leq n\), \(n\geq 1\). Clearly, \(\{X_{nj}:1\leq j\leq n\}_{n\geq 1}\) satisfies (1.2) with \(\sigma_{nj}^{2}=EX_{nj}^{2}=\frac{1}{n\sigma^{2}}\mbox{Var}(X_{1})=1/n\) for all \(1\leq j\leq n\), and hence, \(s_{n}^{2}=\sum_{j=1}^{n}\sigma_{nj}^{2}=1\) for all \(n\geq 1\). Now, for any \(\epsilon>0\),

\[s_{n}^{-2}\sum_{j=1}^{n}EX_{nj}^{2}I(|X_{nj}|>\epsilon s_{n})\] \[= \sum_{j=1}^{n}E\Big{(}\frac{X_{j}-\mu}{\sigma\sqrt{n}}\Big{)}^{2} I\Big{(}\Big{|}\frac{X_{j}-\mu}{\sigma\sqrt{n}}\Big{|}>\epsilon\Big{)}\] \[= n\Big{[}\frac{1}{\sigma^{2}n}E\big{\{}(X_{1}-\mu)^{2}I\big{(}|X_ {1}-\mu|>\epsilon\sigma\sqrt{n}\big{)}\big{\}}\Big{]}\] \[= \sigma^{-2}E\big{\{}(X_{1}-\mu)^{2}I\big{(}|X_{1}-\mu|>\epsilon \sigma\sqrt{n}\big{)}\big{\}}\to 0\quad\mbox{as}\quad n\to\infty,\]by the DCT, since \(E(X_{1}-\mu)^{2}<\infty\). Thus, the triangular array \(\{X_{nj}:1\leq j\leq n\}\) of (1.5) satisfies the Lindeberg condition (1.3).

The main result of this section is the following CLT for scaled row sums of a triangular array of independent random variables.

**Theorem 11.1.1:** (_Lindeberg CLT_). _Let \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) be a triangular array of independent random variables satisfying (1.2) and the Lindeberg condition (1.3). Then,_

\[\frac{S_{n}}{s_{n}}\longrightarrow^{d}N(0,1) \tag{1.6}\]

_where \(S_{n}=\sum_{j=1}^{r_{n}}X_{nj}\) and \(s_{n}^{2}=\mbox{Var}(S_{n})=\sum_{j=1}^{r_{n}}\sigma_{nj}^{2}\)._

As a direct consequence of Theorem 11.1.1 and Example 11.1.1, one gets the more familiar version of the CLT for the sample mean of iid random variables.

**Corollary 11.1.2:** (_CLT for iid random variables_). _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=\mu\) and \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\). Then,_

\[\sqrt{n}(\bar{X}_{n}-\mu)\longrightarrow^{d}N(0,\sigma^{2}), \tag{1.7}\]

_where \(\bar{X}_{n}=n^{-1}\sum_{j=1}^{n}X_{nj}\), \(n\geq 1\)._

For proving the theorem, the following simple inequality will be used.

**Lemma 11.1.3:** _For any \(m\in\mathbb{N}\) and for any complex numbers \(z_{1},\ldots,z_{m}\), \(\omega_{1},\ldots,\omega_{m}\), with \(|z_{j}|\leq 1\), \(|\omega_{j}|\leq 1\) for all \(j=1,\ldots,m\),_

\[\bigg{|}\prod_{j=1}^{m}z_{j}-\prod_{j=1}^{m}\omega_{j}\bigg{|}\leq\sum_{j=1}^{m }|z_{j}-\omega_{j}|. \tag{1.8}\]

**Proof:** Inequality (1.8) follows from the identity

\[\prod_{j=1}^{m}z_{j}-\prod_{j=1}^{m}\omega_{j} = \prod_{j=1}^{m}z_{j}-\bigg{(}\prod_{j=1}^{m-1}z_{j}\bigg{)}\omega _{m}\] \[+\bigg{(}\prod_{j=1}^{m-1}z_{j}\bigg{)}\omega_{m}-\bigg{(}\prod_{ j=1}^{m-2}z_{j}\bigg{)}\omega_{m-1}\omega_{m}\] \[+\cdots+z_{1}\prod_{j=2}^{m}\omega_{j}-\prod_{j=1}^{m}\omega_{j}.\]

\(\Box\)

**Proof of Theorem 11.1.1:** W.l.o.g., suppose that \(s_{n}^{2}=1\) for all \(n\geq 1\). (Otherwise, setting \(\tilde{X}_{nj}\equiv X_{nj}/s_{n}\), \(1\leq j\leq r_{n}\), \(n\geq 1\), it is easy to check that for the triangular array \(\{\tilde{X}_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\), the variance of the \(n\)th row sum \(\tilde{s}_{n}^{2}\equiv\sum_{j=1}^{r_{n}}\mbox{Var}(\tilde{X}_{nj})=1\) for all \(n\geq 1\), the Lindeberg condition holds, and \(\tilde{s}_{n}^{-1}\sum_{j=1}^{r_{n}}\tilde{X}_{nj}\longrightarrow^{d}N(0,1)\) iff (1.6) holds.) Then, by Theorem 10.3.4, it is enough to show that

\[\lim_{n\to\infty}E\exp(\iota tS_{n})=e^{-t^{2}/2}\quad\mbox{for all}\quad t\in \mathbb{R}. \tag{1.9}\]

For any \(\epsilon>0\),

\[\Delta_{n} \equiv \max\left\{EX_{nj}^{2}:1\leq j\leq r_{n}\right\}\] \[\leq \max\left\{EX_{nj}^{2}I(|X_{nj}|>\epsilon)+EX_{nj}^{2}I(|X_{nj}| \leq\epsilon):1\leq j\leq r_{n}\right\}\] \[\leq \sum_{j=1}^{r_{n}}EX_{nj}^{2}I(|X_{nj}|>\epsilon)+\epsilon^{2}\] \[= o(1)+\epsilon^{2}\quad\mbox{as}\quad n\to\infty,\ \ \mbox{by the Lindeberg condition (\ref{L1})}.\]

Hence,

\[\Delta_{n}\to 0\quad\mbox{as}\quad n\to\infty. \tag{1.10}\]

Fix \(t\in\mathbb{R}\). Let \(\phi_{nj}(\cdot)\) denote the characteristic function of \(X_{nj}\), \(1\leq j\leq r_{n}\), \(n\geq 1\). Note that by (1.10), there exists \(n_{0}\in\mathbb{N}\) such that for all \(n\geq n_{0}\), \(I_{1n}\equiv\max\{|1-t^{2}\sigma_{nj}^{2}/2|:1\leq j\leq r_{n}\}\leq 1\). Next, noting that \(s_{n}^{2}=\sum_{j=1}^{r_{n}}\sigma_{nj}^{2}=1\), by Lemma 11.3, for all \(n\geq n_{0}\),

\[\left|E\exp(\iota tS_{n})-e^{-t^{2}/2}\right| \tag{1.11}\] \[\leq \left|\prod_{j=1}^{r_{n}}\phi_{nj}(t)-\prod_{j=1}^{r_{n}}\left(1 -\frac{t^{2}\sigma_{nj}^{2}}{2}\right)\right|\] \[+\left|\prod_{j=1}^{r_{n}}\left(1-\frac{t^{2}\sigma_{nj}^{2}}{2} \right)-\prod_{j=1}^{r_{n}}\exp(-t^{2}\sigma_{nj}^{2}/2)\right|\] \[\leq \sum_{j=1}^{r_{n}}\left|\phi_{nj}(t)-\left[1-\frac{t^{2}\sigma_{nj }^{2}}{2}\right]\right|\] \[+\sum_{j=1}^{r_{n}}\left|\exp(-t^{2}\sigma_{nj}^{2}/2)-\left[1- \frac{t^{2}\sigma_{nj}^{2}}{2}\right]\right|\] \[\equiv I_{2n}+I_{3n},\quad\mbox{say}.\]

It will now be shown that

\[\lim_{n\to\infty}I_{kn}=0\quad\mbox{for}\quad k=2,3.\]

First consider \(I_{2n}\). Since \(\left|\exp(\iota x)-[1+\iota x+(\iota x)^{2}/2]\right|\leq\min\{|x|^{3}/3!,|x|^ {2}\}\) for all \(x\in\mathbb{R}\) (cf. Lemma 10.1.5) and \(EX_{nj}=0\) for all \(1\leq j\leq r_{n}\), for any \(\epsilon>0\), by the Lindeberg condition, one gets

\[I_{2n} \equiv \sum_{j=1}^{r_{n}}\Big{|}\phi_{nj}(t)-\Big{[}1-\frac{t^{2}\sigma_{nj} ^{2}}{2}\Big{]}\Big{|} \tag{1.12}\] \[= \sum_{j=1}^{n}\Big{|}E\exp(\iota tX_{nj})-\Big{[}1+\iota tEX_{nj} +\frac{(\iota t)^{2}}{2!}EX_{nj}^{2}\Big{]}\Big{|}\] \[\leq \sum_{j=1}^{r_{n}}E\min\Big{\{}\frac{|tX_{nj}|^{3}}{3!},|tX_{nj}|^ {2}\Big{\}}\] \[\leq \sum_{j=1}^{r_{n}}E|tX_{nj}|^{3}I(|X_{nj}|\leq\epsilon)+\sum_{j=1} ^{r_{n}}E(tX_{nj})^{2}I(|X_{nj}|>\epsilon)\] \[\leq |t|^{3}\epsilon\sum_{j=1}^{r_{n}}EX_{nj}^{2}+t^{2}\sum_{j=1}^{r_{ n}}EX_{nj}^{2}I(|X_{nj}|>\epsilon)\] \[\leq |t|^{3}\epsilon+t^{2}\cdot o(1)\quad\mbox{as}\quad n\to\infty.\]

Since \(\epsilon\in(0,\infty)\) is arbitrary, \(I_{2n}\to 0\) as \(n\to\infty\).

Next, consider \(I_{3n}\). Note that for any \(x\in\mathbb{R}\),

\[|e^{x}-1-x|=\Bigg{|}\sum_{k=2}^{\infty}x^{k}/k!\Bigg{|}\leq x^{2}\sum_{k=2}^{ \infty}\frac{|x|^{k-2}}{k!}\leq x^{2}e^{|x|}.\]

Hence, using (1.10) and the fact that \(s_{n}^{2}=1\), one gets

\[I_{3n} \leq \sum_{j=1}^{r_{n}}\Big{(}\frac{t^{2}\sigma_{nj}^{2}}{2}\Big{)}^{ 2}\exp(t^{2}\sigma_{nj}^{2}/2) \tag{1.13}\] \[\leq t^{4}\exp(t^{2}\Delta_{n}/2)\Bigg{[}\sum_{j=1}^{r_{n}}\sigma_{nj }^{2}\cdot\Delta_{n}\Bigg{]}\] \[= t^{4}\exp(t^{2}\Delta_{n}/2)\Delta_{n}\] \[\to 0\quad\mbox{as}\quad n\to\infty.\]

Now (1.9) follows from (1.11), (1.12), and (1.13). This completes the proof of the theorem. \(\Box\)

Oftentimes, verification of the Lindeberg condition (1.3) becomes difficult as one has to find the truncated second moments of \(X_{nj}\)'s. A simpler sufficient condition for the CLT is provided by Lyapounov's condition.

**Definition 11.1.3:** A triangular array \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) of independent random variables satisfying (1.2) is said to satisfy _Lyapounov's condition_ if there exists a \(\delta\in(0,\infty)\) such that

\[\lim_{n\to\infty}s_{n}^{-(2+\delta)}\sum_{j=1}^{r_{n}}E|X_{nj}|^{2+\delta}=0, \tag{1.14}\]

[MISSING_PAGE_FAIL:358]

\(1-x^{2}/2\) for all \(x\in\mathbb{R}\), one gets

\[\sum_{j=1}^{r_{n}}\big{(}E\cos\,t_{0}X_{nj}-1\big{)}+\frac{t_{0}^{2 }}{2}\] \[= \sum_{j=1}^{r_{n}}E\Big{(}\frac{t_{0}^{2}X_{nj}^{2}}{2}-1+\cos\,t_ {0}X_{nj}\Big{)}\] \[\geq \sum_{j=1}^{r_{n}}E\Big{(}\frac{t_{0}^{2}X_{nj}^{2}}{2}-1+\cos\,t_ {0}X_{nj}\Big{)}I\big{(}|X_{nj}|>\epsilon\big{)}\] \[\geq \sum_{j=1}^{r_{n}}E\Big{(}\frac{t_{0}^{2}X_{nj}^{2}}{2}-2\Big{)}I \big{(}|X_{nj}|>\epsilon\big{)}\] \[\geq \Big{(}\frac{t_{0}^{2}}{2}-\frac{2}{\epsilon^{2}}\Big{)}\sum_{j=1 }^{r_{n}}EX_{nj}^{2}I\big{(}|X_{nj}|>\epsilon\big{)}\] \[= \frac{6}{\epsilon^{2}}\sum_{j=1}^{r_{n}}EX_{nj}^{2}I\big{(}|X_{nj }|>\epsilon\big{)}.\]

Hence, the Lindeberg condition would hold if it is shown that for all \(t\in\mathbb{R}\),

\[\sum_{j=1}^{r_{n}}\big{(}E\cos tX_{nj}-1\big{)}+\frac{t^{2}}{2} \to 0\quad\mbox{as}\quad n\to\infty\] \[\Leftrightarrow \exp\bigg{(}\sum_{j=1}^{r_{n}}\big{(}E\cos tX_{nj}-1\big{)}\bigg{)} \to e^{-t^{2}/2}\quad\mbox{as}\quad n\to\infty. \tag{1.17}\]

Let \(\phi_{nj}(t)=E\exp(\iota tX_{nj})\), \(t\in\mathbb{R}\) denote the characteristic function of \(X_{nj}\), \(1\leq j\leq r_{n}\), \(n\geq 1\). Note that \(E\cos tX_{nj}=\mbox{Re}(\phi_{nj}(t))\), where recall that for any complex number \(z\), \(\mbox{Re}(z)\) denotes the real part of \(z\), i.e., \(\mbox{Re}(z)=a\) if \(z=a+\iota b\), \(a,b\in\mathbb{R}\). Since the function \(h(z)=|z|\) is continuous on \(\mathbb{C}\) and \(|\exp(\phi_{nj}(t))|=\exp(E\cos tX_{nj})\), it follows that (1.17) holds if, for all \(t\in\mathbb{R}\),

\[\exp\bigg{(}\sum_{j=1}^{r_{n}}(\phi_{nj}(t)-1)\bigg{)}\to e^{-t^{2}/2}\quad \mbox{as}\quad n\to\infty.\]

However, by (1.16), \(E\exp(\iota tS_{n})=\prod_{j=1}^{r_{n}}\phi_{nj}(t)\to e^{-t^{2}/2}\) for all \(t\in\mathbb{R}\). Hence, it is enough to show that

\[I_{1n}(t) \equiv \bigg{[}\exp\bigg{(}\sum_{j=1}^{r_{n}}[\phi_{nj}(t)-1]\bigg{)}- \prod_{j=1}^{r_{n}}\phi_{nj}(t)\bigg{]} \tag{1.18}\] \[\to 0\quad\mbox{as}\quad n\to\infty\quad\mbox{for all}\quad t\in \mathbb{R}.\]Note that for any \(\epsilon\in(0,\infty)\), by (1.15) and the inequality \(|e^{\iota x}-1|\leq\min\{2,|x|\}\) for all \(x\in\mathbb{R}\), one has

\[|\phi_{nj}(t)-1| = \big{|}E\big{(}\exp(\iota tX_{nj})-1\big{)}\big{|}\] \[\leq E\min\{|tX_{nj}|,2\}\] \[\leq 2P(|X_{nj}|>\epsilon)+|t|\epsilon\]

uniformly in \(j=1,\ldots,r_{n}\). Hence, letting \(n\to\infty\) and then \(\epsilon\downarrow 0\), by (1.15), one gets

\[I_{2n}(t)\equiv\max_{1\leq j\leq r_{n}}|\phi_{nj}(t)-1|=o(1)\quad\mbox{as}\quad n \to\infty \tag{1.19}\]

for all \(t\in\mathbb{R}\). Further, by the inequality \(|e^{\iota x}-1-\iota x|\leq|x|^{2}/2\), \(x\in\mathbb{R}\),

\[\sum_{j=1}^{r_{n}}|\phi_{nj}(t)-1| = \sum_{j=1}^{r_{n}}\big{|}E\exp(\iota tX_{nj})-1-E(\iota tX_{nj}) \big{|} \tag{1.20}\] \[\leq \frac{t^{2}}{2}\sum_{j=1}^{r_{n}}EX_{nj}^{2}=\frac{t^{2}}{2}s_{n }^{2}=\frac{t^{2}}{2}\]

uniformly in \(t\in\mathbb{R}\), \(n\geq 1\). Now fix \(t\in\mathbb{R}\). Then, by (1.19), there exists \(n_{0}\in\mathbb{N}\) such that for all \(n\geq n_{0}\), \(\max_{1\leq j\leq r_{n}}|\phi_{n}(t)-1|\leq 1\). Hence, by the arguments in the proof of Lemma 11.1.3, and by the inequalities \(|e^{z}|\leq\sum_{k=0}^{\infty}|z|^{k}/k!=e^{|z|}\), and \(|e^{z}-1-z|=\big{|}\sum_{k=2}^{\infty}z^{k}/k!\big{|}\leq|z|^{2}\exp(|z|)\), \(z\in\mathbb{C}\), for all \(n\geq n_{0}\), one has

\[I_{1n}(t) = \bigg{|}\prod_{j=1}^{r_{n}}\exp\big{(}[\phi_{nj}(t)-1]\big{)}- \prod_{j=1}^{r_{n}}\phi_{nj}(t)\bigg{|}\] \[\leq \sum_{j=1}^{r_{n}}\big{|}\exp\big{(}[\phi_{nj}(t)-1]\big{)}-\phi _{nj}(t)\big{|}\cdot\prod_{k=1}^{r_{n}-j}\big{|}\exp\big{(}[\phi_{nj}(t)-1] \big{)}\big{|}\] \[\leq \sum_{j=1}^{r_{n}}\big{|}\exp\big{(}[\phi_{nj}(t)-1]\big{)}-\phi _{nj}(t)\big{|}\cdot\exp\bigg{(}\sum_{j=1}^{r_{n}}\big{|}\phi_{nj}(t)-1\big{|} \bigg{)}\] \[\leq \sum_{j=1}^{r_{n}}\big{|}\exp\big{(}[\phi_{nj}(t)-1]\big{)}-\phi _{nj}(t)\big{|}\cdot\exp\bigg{(}\frac{t^{2}}{2}\bigg{)}\] \[= \sum_{j=1}^{r_{n}}\big{|}\exp\big{(}[\phi_{nj}(t)-1]\big{)}-1-[ \phi_{nj}(t)-1]\big{|}\cdot\exp\bigg{(}\frac{t^{2}}{2}\bigg{)}\] \[\leq \sum_{j=1}^{r_{n}}\big{|}\phi_{nj}(t)-1\big{|}^{2}\cdot\exp\Big{(} 1+\frac{t^{2}}{2}\Big{)}\] \[\leq \max_{1\leq j\leq r_{n}}|\phi_{nj}(t)-1|\bigg{(}\sum_{j=1}^{r_{n}} |\phi_{nj}(t)-1|\bigg{)}\exp\Big{(}1+\frac{t^{2}}{2}\Big{)}\]\[\leq I_{2n}(t)\cdot\Big{[}t^{2}\cdot\exp\Big{(}1+\frac{t^{2}}{2}\Big{)}/2 \Big{]}\] \[\to 0\quad\mbox{as}\quad n\to\infty,\]

by (1.19) and (1.20). Hence, (1.18) holds. This completes the proof of the theorem. \(\Box\)

The following example is an application of the Lindeberg CLT for proving asymptotic normality of the least squares estimator of a regression parameter.

**Example 11.1.2:** Let

\[Y_{j}=x_{j}\beta+\epsilon_{j},\quad j=1,2,\ldots \tag{1.21}\]

be a simple linear regression model, where \(\{x_{n}\}_{n\geq 1}\) is a given sequence of real numbers, \(\beta\in\mathbb{R}\) is the regression parameter and \(\{\epsilon_{n}\}_{n\geq 1}\) is a sequence of iid random variables with \(E\epsilon_{1}=0\) and \(E\epsilon_{1}^{2}\equiv\sigma^{2}\in(0,\infty)\). The least squares estimator of \(\beta\) based on \(Y_{1},\ldots,Y_{n}\) is given by

\[\hat{\beta}_{n}=\sum_{j=1}^{n}x_{j}Y_{j}/a_{n}^{2},\quad n\geq 1,\]

where \(a_{n}^{2}=\sum_{j=1}^{n}x_{j}^{2}\). Suppose that the sequence \(\{x_{n}\}_{n\geq 1}\) satisfies

\[\max_{1\leq j\leq n}\{|x_{j}|/a_{n}\}\to 0\quad\mbox{as}\quad n\to\infty. \tag{1.22}\]

Then,

\[a_{n}(\hat{\beta}_{n}-\beta)\longrightarrow^{d}N(0,\sigma^{2}). \tag{1.23}\]

To prove (1.23), note that by (1.21),

\[a_{n}(\hat{\beta}_{n}-\beta) = a_{n}\bigg{[}\sum_{j=1}^{n}x_{j}Y_{j}-a_{n}^{2}\beta\bigg{]} \bigg{/}a_{n}^{2} \tag{1.24}\] \[= a_{n}^{-1}\sum_{j=1}^{n}x_{j}\epsilon_{j}\equiv\sum_{j=1}^{n}X_{ nj},\quad\mbox{say}\]

where \(X_{nj}=x_{j}\epsilon_{j}/a_{n}\), \(1\leq j\leq n\), \(n\geq 1\). Note that \(EX_{nj}=0\), \(EX_{nj}^{2}<\infty\) and \(s_{n}^{2}\equiv\sum_{j=1}^{n}EX_{nj}^{2}=\sum_{j=1}^{n}x_{j}^{2}E\epsilon_{j}^ {2}/a_{n}^{2}=\sigma^{2}\). Thus, \(\{X_{nj}:1\leq j\leq n\}_{n\geq 1}\) is a triangular array of independent random variables satisfying (1.2). Next, let \(m_{n}=\max\{|x_{j}|/a_{n}:1\leq j\leq n\}\), \(n\geq 1\). Then, by (1.22), for any \(\delta\in(0,\infty)\),

\[s_{n}^{-2}\sum_{j=1}^{n}EX_{nj}^{2}I\big{(}|X_{nj}|>\delta s_{n}\big{)}\]\[= \sigma^{-2}a_{n}^{-2}\sum_{j=1}^{n}x_{j}^{2}E\epsilon_{j}^{2}I(|x_{j} \epsilon_{j}/a_{n}|>\delta\sigma)\] \[\leq \sigma^{-2}a_{n}^{-2}\sum_{j=1}^{n}x_{j}^{2}\cdot E\epsilon_{1}^{2 }I(m_{n}\cdot|\epsilon_{1}|>\delta\sigma)\] \[= \sigma^{-2}E\epsilon_{1}^{2}I\big{(}|\epsilon_{1}|>\delta\sigma \cdot m_{n}^{-1}\big{)}\] \[\to 0\quad\mbox{as}\quad n\to\infty\ \ \mbox{by the DCT}.\]

Thus, \(\{X_{nj}:1\leq j\leq n\}_{n\geq 1}\) satisfies the Lindeberg condition (1.3) and hence, by Theorem 11.1.1,

\[\frac{\sum_{j=1}^{n}X_{nj}}{\sigma}\longrightarrow^{d}N(0,1),\]

which, in view of (1.24), implies (1.23).

The next result gives a multivariate generalization of Theorem 11.1.1.

**Theorem 11.1.6:** (_A multivariate version of the Lindeberg CLT_). _For each \(n\geq 1\), let \(\{X_{nj}:1\leq j\leq r_{n}\}\) be a collection of independent \(k\)-dimensional random vectors satisfying_

\[EX_{nj}=0,\ 1\leq j\leq r_{n}\quad\mbox{and}\quad\sum_{j=1}^{r_{n}}EX^{\prime}_ {nj}X_{nj}=\mathbb{I}_{k},\]

_where \(\mathbb{I}_{k}\) denotes the identity matrix of order \(k\) and for any vector \(x\), \(x^{\prime}\) denotes its transpose. Suppose that for every \(\epsilon\in(0,\infty)\),_

\[\lim_{n\to\infty}\sum_{j=1}^{r_{n}}E\|X_{nj}\|^{2}I(\|X_{nj}\|\ >\epsilon)=0.\]

_Then,_

\[\sum_{j=1}^{r_{n}}X_{nj}\longrightarrow^{d}N(0,\mathbb{I}_{k}).\]

The proof is a consequence of Theorem 11.1.1 and the Cramer-Wold device (cf. Theorem 10.4.5) and is left as an exercise (Problem 11.17).

### Stable distributions

If \(\{X_{n}\}_{n\geq 1}\) is a sequence of iid \(N(\mu,\sigma^{2})\) random variables, then for each \(k\geq 1\), \(S_{k}\equiv\sum_{i=1}^{k}X_{i}\) has a \(N(k\mu,k\sigma^{2})\) distribution. Similarly, if is a sequence of iid Cauchy \((\mu,\sigma)\) random variables, then for each \(k\geq 1\), \(S_{k}\equiv\sum_{i=1}^{k}X_{i}\) has a Cauchy \((k\mu,k\sigma)\) distribution. Thus, in both cases, for each \(k\geq 1\), there exist constants \(a_{k}\) and \(b_{k}\) such that \(S_{k}\) has the same distribution as \(a_{k}X_{1}+b_{k}\) (Problem 11.21).

**Definition 11.2.1:** A nondegenerate random variable \(X\) is called _stable_ if the above property holds, i.e., for each \(k\in\mathbb{N}\), there exist constants \(a_{k}\) and \(b_{k}\) such that

\[S_{k}=^{d}a_{k}X_{1}+b_{k}, \tag{2.1}\]

where \(X_{1},X_{2},\ldots\) are iid random variables with the same distribution as \(X\), and \(S_{k}=\sum_{i=1}^{k}X_{i}\). In this case, the distribution \(F_{X}\) of \(X\) is called a _stable distribution_.

There are two characterizations of stable distributions.

**Theorem 11.2.1:**_A nondegenerate distribution \(F\) is stable iff there exists a sequence of iid random variable \(\{Y_{n}\}_{n\geq 1}\) and constants \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that \(\big{(}\sum_{i=1}^{n}Y_{i}-b_{k}\big{)}\big{/}a_{k}\) converges in distribution to \(F\)._

**Theorem 11.2.2:**_A nondegenerate distribution \(F\) is stable iff its characteristic function \(\phi(t)\) admits the representation_

\[\phi(t)=\exp\big{(}\iota tc-b|t|^{\alpha}(1+\iota\lambda sgn(t)\omega_{\alpha} (t))\big{)} \tag{2.2}\]

_where \(\iota=\sqrt{-1}\), \(-1\leq\lambda\leq 1\), \(0<\alpha\leq 2\), \(0\leq b<\infty\), and the functions \(\omega_{\alpha}(t)\) and \(sgn(\cdot)\) are defined as_

\[\omega_{\alpha}(t)=\left\{\begin{array}{ll}\tan\frac{\pi\alpha}{2}&\mbox{if} \quad\alpha\neq 1\\ \frac{2}{\pi}\log|t|&\mbox{if}\quad\alpha=1\end{array}\right. \tag{2.3}\]

_and_

\[sgn(t)=\left\{\begin{array}{ll}1&\mbox{if}\quad t>0\\ -1&\mbox{if}\quad t<0\\ 0&\mbox{if}\quad t=0\.\end{array}\right.\]

**Remark 11.2.1:** When \(\alpha=2\), \(\phi(t)\) in (2.2) reduces to \(\exp(\iota tc-bt^{2})\), which is the characteristic function of a normal random variable with mean \(c\) and variance \(2b\).

**Remark 11.2.2:** When \(\alpha=1\), \(\lambda=0\), \(\phi(t)\) reduces to \(\exp(\iota tc-b|t|)\), which is the characteristic function of a Cauchy \((c,b)\) distribution.

**Remark 11.2.3:** Since \(|\phi(t)|\) is integrable, the distribution \(F\) must be absolutely continuous. Apart from the normal and Cauchy distributions, for \(\alpha=1/2\), \(\lambda=1\), the density is given by

\[f(x)=\frac{1}{\sqrt{2\pi}}\frac{1}{x^{3/2}}\ \exp\Big{(}-\frac{1}{2x}\Big{)},\ x>0. \tag{2.4}\]For an explicit expression for the density of \(F\), in other cases, see Feller (1966), Section 17.6.

**Definition 11.2.2:** The parameter \(\alpha\) in (2.2) is called the _index_ of the stable distribution.

**Remark 11.2.4:** The parameter \(\lambda\) is related to the behavior of the ratio of the right tail of the distribution to the left tail through the relation

\[\lim_{x\to\infty}\frac{1-F(x)}{F(-x)}=\frac{1+\lambda}{(1-\lambda)}, \tag{2.5}\]

where for \(\lambda=1\), the ratio on the right side of (2.5) is defined to be \(+\infty\).

**Definition 11.2.3:** A function \(L:(0,\infty)\to(0,\infty)\) is called _slowly varying_ at \(\infty\) if

\[\lim_{x\to\infty}\frac{L(cx)}{L(x)}=1\quad\text{for all}\quad 0<c<\infty. \tag{2.6}\]

A function \(f:(0,\infty)\to(0,\infty)\) is called _regularly varying_ at \(\infty\) with index \(\alpha\in\mathbb{R}\), \(\alpha\neq 0\) if \(f(x)=x^{\alpha}L(x)\) for all \(x\in(0,\infty)\) where \(L(\cdot)\) is slowly varying at \(\infty\). The functions \(L_{1}(t)=\log t\), \(L_{2}(t)=\log(\log t)\), \(L_{3}(t)=(\log t)^{2}\) are slowly varying at \(\infty\) but the function \(L_{4}(t)=t^{p}\) is not so for \(p\neq 0\).

There is a companion result to Theorem 11.2.1 giving necessary and sufficient conditions for convergence of normalized sums of iid random variables to a stable distribution.

**Theorem 11.2.3:**_Let \(F\) be a nondegenerate stable distribution with index \(\alpha\), \(0<\alpha<2\). Then in order that a sequence \(\{Y_{n}\}_{n\geq 1}\) of iid random variables admits a sequence of constants \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that_

\[\frac{\sum_{i=1}^{n}Y_{i}-b_{n}}{a_{n}}\longrightarrow^{d}F, \tag{2.7}\]

_it is necessary and sufficient that_

\[\lim_{x\to\infty}\frac{P(Y_{1}>x)}{P(|Y_{1}|>x)}\equiv\theta\in[0,1] \tag{2.8}\]

_exists and_

\[P(|Y_{1}|>x)=x^{-\alpha}L(x), \tag{2.9}\]

_where \(L(\cdot)\) is a slowly varying function at \(\infty\). If (2.8) and (2.9) hold, then the normalizing constants \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) may be chosen to satisfy_

\[na_{n}^{-\alpha}L(a_{n})\to 1\quad\text{and}\quad b_{n}=nEY_{1}I(|Y_{1}|\leq a_{ n}). \tag{2.10}\]

**Remark 11.2.5:** The analog of Theorem 11.2.3 for the case \(\alpha=2\), i.e., for the normal distribution is the following.

**Theorem 11.2.4:** _Let \(\{Y_{n}\}_{n\geq 1}\) be iid random variables. In order that there exist constants \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that_

\[\frac{\sum_{i=1}^{n}Y_{i}-b_{n}}{a_{n}}\longrightarrow^{d}N(0,1), \tag{2.11}\]

_it is necessary and sufficient that_

\[\frac{x^{2}P(|Y_{1}|>x)}{EY_{1}^{2}I(|Y_{1}|\leq x)}\to 0\quad\mbox{as}\quad x\to\infty. \tag{2.12}\]

**Remark 11.2.6:** Note that condition (2.12) holds if \(EY_{1}^{2}<\infty\). However, if \(P(|Y_{1}|>x)\sim\frac{C}{x^{2}}\) as \(x\to\infty\), then \(EY_{1}^{2}=\infty\) and the classical CLT (cf. Corollary 11.1.2) fails. However, in this case, (2.12) holds and \(\sum_{i=1}^{n}Y_{i}\) is asymptotically normal with a suitable centering and scaling (different from \(\sqrt{n}\)) (Problem 11.20).

Here, only the proof of Theorem 11.2.1 will be given. Further, a proof of Theorem 11.2.3, sufficiency part, is also outlined. For the rest, see Feller (1966) or Gnedenko and Kolmogorov (1968). For proving Theorem 11.2.1, the following result is needed.

**Theorem 11.2.5:** (_Khinchine's theorem on convergence of types_). _Let \(\{W_{n}\}_{n\geq 1}\) be a sequence of random variables such that for some sequences \(\{\alpha_{n}\}_{n\geq 1}\subset[0,\infty)\) and \(\{\beta_{n}\}_{n\geq 1}\subset\mathbb{R}\), both \(W_{n}\) and \(\alpha_{n}W_{n}+\beta_{n}\) converge in distribution to nondegenerate distributions \(G\) and \(H\) on \(\mathbb{R}\), respectively. Then \(\lim_{n\to\infty}\alpha_{n}=\alpha\) and \(\lim_{n\to\infty}\beta_{n}=\beta\) exist with \(0<\alpha<\infty\) and \(-\infty<\beta<\infty\)._

**Proof:** Let \(\{W^{\prime}_{n}\}_{n\geq 1}\) be a sequence of random variables such that for each \(n\geq 1\), \(W_{n}\) and \(W^{\prime}_{n}\) have the same distribution and \(W_{n}\) and \(W^{\prime}_{n}\) are independent. Then \(Y_{n}\equiv W_{n}-W^{\prime}_{n}\) and \(Z_{n}\equiv\alpha_{n}(W_{n}-W^{\prime}_{n})\) both convergence in distribution to nondegenerate limits, say \(\tilde{G}\) and \(\tilde{H}\). Indeed \(\tilde{G}=G*G\) and \(\tilde{H}=H*H\), where \(*\) denotes convolution. This implies that \(\{\alpha_{n}\}_{n\geq 1}\) cannot have \(0\) or \(\infty\) as limit points. Also if \(0<\alpha\leq\alpha^{\prime}<\infty\) are two limit points of \(\{\alpha_{n}\}_{n\geq 1}\), then \(\tilde{H}(x)=\tilde{G}(\frac{x}{\alpha})=\tilde{G}(\frac{x}{\alpha^{\prime}})\) for all \(x\). Since \(\tilde{G}(\cdot)\) is nondegenerate, \(\alpha\) must equal \(\alpha^{\prime}\) and so \(\lim_{n\to\infty}\alpha_{n}\) exists in \((0,\infty)\). This implies that \(\lim_{n\to\infty}\beta_{n}\) exists in \(\mathbb{R}\). \(\Box\)

**Proof of Theorem 11.2.1:** The 'only if' part follows from the definition of \(F\) being stable, since one can take \(\{Y_{n}\}_{n\geq 1}\) to be iid with distributionFor the 'if part,' let \(\{Y_{n}\}_{n\geq 1}\) be iid random variables such that there exists constants \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that as \(n\to\infty\)

\[\frac{\sum_{i=1}^{n}Y_{i}-b_{n}}{a_{n}}\longrightarrow^{d}F.\]

To show that \(F\) is stable, fix an integer \(r\geq 1\). Let \(\{X_{n}\}_{n\geq 1}\) be iid random variables with distribution \(F\). Then as \(k\to\infty\),

\[\frac{\sum_{i=1}^{kr}Y_{i}-b_{kr}}{a_{kr}}\longrightarrow^{d}X_{1}.\]

Also, the left side above equals

\[\sum_{j=0}^{r-1}\bigg{(}\frac{\sum_{i=jk+1}^{(j+1)k}Y_{i}-b_{k}}{a_{k}}\bigg{)} \frac{a_{k}}{a_{kr}}+\frac{rb_{k}-b_{kr}}{a_{kr}}=\alpha_{kr}\bigg{(}\sum_{j=0} ^{r-1}\eta_{jk}\bigg{)}+\beta_{kr},\quad\text{say}.\]

where \(\alpha_{kr}=\frac{a_{k}}{a_{kr}}\), \(\eta_{jk}=\Big{(}\frac{\sum_{i=jk+1}^{(j+1)k}Y_{i}-b_{k}}{a_{k}}\Big{)}\) and \(\beta_{kr}=\frac{rb_{k}-b_{kr}}{a_{kr}}\). Since \(\{\eta_{jk}:j=0,1,\ldots,r-1\}\) are independent and for each \(j\), \(\eta_{jk}\longrightarrow^{d}X_{j+1}\) as \(k\to\infty\), it follows that as \(k\to\infty\),

\[W_{k}=\sum_{j=0}^{r-1}\eta_{jk}\longrightarrow^{d}\sum_{j=0}^{r-1}X_{j+1}=\sum_ {j=1}^{r}X_{j}.\]

Also, as \(k\to\infty\),

\[\alpha_{kr}W_{k}+\beta_{kr}\longrightarrow^{d}X_{1}.\]

Since \(F\) is nondegenerate, both \(X_{1}\) and \(\sum_{j=1}^{r}X_{j}\) are nondegenerate random variables. Thus, as \(k\to\infty\), \(W_{k}\) and \(\alpha_{kr}W_{k}+\beta_{kr}\) converge in distribution to nondegenerate random variables. Thus, by Khinchine's theorem on convergence types proved above, it follows that \(\alpha_{kr}\to\alpha^{\prime}_{r}\) and \(\beta_{kr}\to\beta^{\prime}_{r}\), \(0<\alpha^{\prime}_{r}<\infty\) and \(-\infty<\beta^{\prime}_{r}<\infty\). This yields that for each \(r\in\mathbb{N}\) that \(\sum_{j=1}^{r}X_{j}\) has the same distribution as \(\frac{1}{\alpha^{\prime}_{r}}(X_{1}-\beta^{\prime}_{r})\), i.e., \(X_{1}\) is stable. \(\Box\)

**Proof of the sufficiency part of Theorem 11.2.3:** (_Outline_). The proof is based on the continuity theorem. The characteristic function of \(T_{n}\equiv\frac{S_{n}-b_{n}}{a_{n}}\) is

\[\phi_{n}(t)=E\Big{(}e^{t\,t\frac{\beta_{n}-b_{n}}{a_{n}}}\Big{)}=\Big{(}\phi \Big{(}\frac{t}{a_{n}}\Big{)}e^{-\iota tb^{\prime}_{n}/a_{n}}\Big{)}^{n}\equiv \Big{(}1+\frac{1}{n}h_{n}(t)\Big{)}^{n}\]

where \(b^{\prime}_{n}=b_{n}/n\) and \(h_{n}(t)=n\Big{(}\phi(\frac{t}{a_{n}})e^{-\iota tb^{\prime}_{n}/a_{n}}-1\Big{)}\). Let \(G(\cdot)\) be the cdf of \(Y_{1}\). Then

\[h_{n}(t)=n\int\Big{(}e^{\iota t(y-b^{\prime}_{n})/a_{n}}-1\Big{)}dG(y)=\int \big{(}e^{\iota tx}-1\big{)}\mu_{n}(dx)\]where \(\mu_{n}(A)=nP(Y_{1}\in b^{\prime}_{n}+a_{n}A)\), \(A\in{\cal B}({\mathbb{R}})\). If \(A=(u,v]\), \(0<u<v<\infty\), then

\[nP\bigl{(}Y_{1}\in b^{\prime}_{n}+a_{n}A\bigr{)} = nP\bigl{(}a_{n}u+b^{\prime}_{n}<Y_{1}\leq a_{n}v+b^{\prime}_{n} \bigr{)}\] \[= nP\bigl{(}Y_{1}>a_{n}u+b^{\prime}_{n}\bigr{)}-nP\bigl{(}Y_{1}>a_ {n}v+b^{\prime}_{n}\bigr{)}.\]

By (2.8)-(2.10),

\[nP(Y_{1}>a_{n}x)=\left(\frac{P(Y_{1}>a_{n}x)}{P(Y_{1}>a_{n})}\right)nP(Y_{1}>a_ {n})\to\theta x^{-\alpha}\mbox{ for }x>0.\]

By using (2.10), it can be show that

\[\frac{b^{\prime}_{n}}{a_{n}} = \frac{EY_{1}I(|Y_{1}|\leq a_{n})}{a_{n}}\] \[= O\Bigl{(}\frac{a_{n}^{1-\alpha}L(a_{n})}{a_{n}}\Bigr{)}\] \[= o(1)\quad\mbox{as}\quad n\to\infty.\]

Hence, it follows that

\[nP(Y_{1}>a_{n}u+b^{\prime}_{n})-nP(Y_{1}>a_{n}v+b^{\prime}_{n})\to\theta(u^{- \alpha}-v^{-\alpha}).\]

Similarly, for \(A=(-v,-u]\),

\[nP(Y_{1}\in b^{\prime}_{n}+a_{n}A)\to(1-\theta)(u^{-\alpha}-v^{-\alpha}).\]

This suggests that \(h_{n}(t)\) should approach

\[\theta\alpha\int_{0}^{\infty}(e^{\iota tx}-1)x^{-(\alpha+1)}dx+(1-\theta)\alpha \int_{-\infty}^{0}(e^{\iota tx}-1)|x|^{-(\alpha+1)}dx.\]

But there are integrability problems for \(|x|^{-(\alpha+1)}\) near \(0\) and so a more careful analysis is needed. It can be shown that

\[\lim_{n\to\infty}h_{n}(t) = \iota tc+\theta\alpha\int_{0}^{\infty}\Big{(}e^{\iota tx}-1-\frac {\iota tx}{1+x^{2}}\Big{)}x^{(\alpha+1)}dx\] \[\mbox{}+(1-\theta)\alpha\int_{-\infty}^{0}\Big{(}e^{\iota tx}-1- \frac{\iota tx}{1+x^{2}}\Big{)}|x|^{-(\alpha+1)}dx\]

where \(c\) is a constant. The right side is continuous at \(t=0\) and so, the result follows by the continuity theorem. For details, see Feller (1966). \(\Box\)

**Remark 11.2.7:** By the necessity part of Theorem 11.2.3, every stable distribution \(F\) must satisfy

\[1-F(x) = \theta x^{-\alpha}L(x)\] \[F(-x) = (1-\theta)x^{-\alpha}L(x)\]for large \(x\) where \(0\leq\theta\leq 1\) and \(L(\cdot)\) is slowing varying at \(\infty\) and \(0<\alpha<2\). This implies that \(F\) has moments of order \(p\) such that \(\alpha>p\). Distributions satisfying the above tail condition are called _heavy tailed_ and arise in many applications. The _Pareto distribution_ in economics is an example of a heavy tail distribution.

**Remark 11.2.8:** One way to generate heavy tailed distributions is as follows. If \(Y\) is a positive random variable such that there exist \(0<c<\infty\) and \(0<p<\infty\) satisfying

\[P(Y<y)\sim c\hskip 1.0pt\mbox{\rm{y}}^{p}\quad\mbox{as}\quad y\downarrow 0,\]

then the random variable \(X=Y^{-q}\) has the property

\[P(X>x)=P(Y<x^{-1/q})\sim c\hskip 1.0pt\mbox{\rm{x}}^{-p/q}\quad\mbox{as}\quad x \rightarrow\infty.\]

If \(p<2q\), then \(X\) has heavy tails. Thus if \(\{Y_{n}\}_{n\geq 1}\) are iid Gamma(1,2), then \(n^{-1}\sum_{i=1}^{n}Y_{i}^{-1}\) converges in distribution to a one sided Cauchy distribution (Problem 11.15).

**Definition 11.2.4:** Let \(F\) and \(G\) be two probability distributions on \(\mathbb{R}\). Then \(G\) is said to belong to the _domain of attraction_ of \(F\) if there exist a sequence of iid random variables \(\{Y_{n}\}_{n\geq 1}\) with distribution \(G\) and constants \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that

\[\underline{\sum_{i=1}^{n}Y_{i}-b_{n}}\longrightarrow^{d}F.\]

Theorem 11.2.1 says that the only nondegenerate distributions \(F\) that admit a nonempty domain of attraction are the stable distributions.

### 11.3 Infinitely divisible distributions

**Definition 11.3.1:** A random variable \(X\) (and its distribution) is called _infinitely divisible_ if for each integer \(k\in\mathbb{N}\), there exist iid random variables \(X_{k1},X_{k2},\ldots,X_{kk}\) such that \(\sum_{j=1}^{k}X_{kj}\) has the same distribution as \(X\).

Examples include constants (degenerate distributions), normal, Poisson, Cauchy, and Gamma distributions. But distributions with bounded support cannot be infinitely divisible unless they are degenerate. In fact, if \(X\) is infinitely divisible satisfying \(P(|X|\leq M)=1\) for some \(M<\infty\), then the \(X_{ki}\)'s in the above definition must satisfy \(P\big{(}|X_{k1}|<\frac{M}{k}\big{)}=1\) and so \(\mbox{Var}(X_{k1})\leq EX_{k1}^{2}\leq\frac{M^{2}}{k^{2}}\) implying \(\mbox{Var}(X)=k\mbox{Var}(X_{k1})\leq\frac{M^{2}}{k}\) for each \(k\geq 1\). Hence \(\mbox{Var}(X)\) must be zero, and the random variable \(X\) is a constant w.p. 1.

The following results are easy to establish.

**Theorem 11.3.1:**_(a) If \(X\) and \(Y\) are independent and infinitely divisible, then \(X+Y\) is also infinitely divisible. (b) If \(X_{n}\) is infinitely divisible for each \(n\in\mathbb{N}\) and \(X_{n}\longrightarrow^{d}X\), then \(X\) is infinitely divisible._

**Proof:** (a) Follows from the definition. (b) For each \(k\geq 1\) and \(n\geq 1\), there exist iid random variables \(X_{nk1},X_{nk2},\ldots,X_{nkk}\) such that \(X_{n}\) and \(\sum_{j=1}^{k}X_{nkj}\) have the same distribution. Now fix \(k\geq 1\). Then for any \(y>0\),

\[\big{(}P(X_{nk1}>y)\big{)}^{k}=P(X_{nkj}>y\quad\mbox{for all}\quad j=1,...,k) \leq P(X_{n}>ky)\]

and similarly,

\[\big{(}P(X_{nk1}<-y)\big{)}^{k}\leq P(X_{n}\leq ky).\]

Since \(X_{n}\longrightarrow^{d}X\), the distributions of \(\{X_{n}\}_{n\geq 1}\) are tight and so are \(\{X_{nk1}\}_{n=1}^{\infty}\). So if \(F_{k}\) is a weak limit point of \(\{X_{nk1}\}_{n=1}^{\infty}\) and if \(\{Y_{kj}\}_{j=1}^{k}\) are iid with distribution \(F_{k}\), then \(X\) and \(\sum_{j=1}^{k}Y_{kj}\) have the same distribution and so \(X\) is infinitely divisible. \(\Box\)

A large class of infinitely divisible distributions are generated by the compound Poisson family.

**Definition 11.3.2:** Let \(\{Y_{n}\}_{n\geq 1}\) be iid random variables and let \(N\) be a Poisson (\(\lambda\)) random variable, independent of the \(\{Y_{n}\}_{n\geq 1}\). The random variable \(X\equiv\sum_{i=1}^{N}Y_{i}\) is said to have a _compound Poisson distribution_.

**Theorem 11.3.2:**_A compound Poisson distribution is infinitely divisible._

**Proof:** Let \(X\) be a random variable as in Definition 11.3.2. For each \(k\geq 1\), let \(\{N_{i}\}_{i=1}^{k}\) be iid Poisson random variables with mean \(\frac{\lambda}{k}\) that are independent of \(\{Y_{n}\}_{n\geq 1}\). Let

\[X_{kj}=\sum_{i=T_{j}+1}^{T_{j+1}}Y_{i},\ 1\leq j\leq k\]

where \(T_{1}=0\), \(T_{j}=\sum_{i=1}^{j-1}N_{i}\), \(2\leq j\leq k\). Then \(\{X_{kj}\}_{j=1}^{k}\) are iid and \(\sum_{j=1}^{k}X_{kj}\) and \(X\) are identically distributed and so \(X\) is infinitely divisible. \(\Box\)

Although the converse to the above is not valid, it is known that every infinitely divisible distribution is the limit of a sequence of centered and scaled compound Poisson distributions. This is a consequence of a deep result giving an explicit formula for the characteristic function of an infinitely divisible distribution which is stated below. For a proof of this result (stated below), see Feller (1966) and Chung (1974) or Gnedenko and Kolmogorov (1968).

**Theorem 11.3.3:** (_Levy-Khinchine representation theorem_). _Let \(X\) be an infinitely divisible random variable. Then its characteristic function\(E(e^{\iota tx})\) is of the form_

\[\phi(t)=\exp\bigg{(}\iota tc-\beta\frac{t^{2}}{2}+\int_{\mathbb{R}}\Big{(}e^{ \iota tx}-1-\frac{\iota tx}{1+x^{2}}\Big{)}\mu(dx)\bigg{)}\,\]

_where \(c\in\mathbb{R}\), \(\beta>0\) and \(\mu\) is a measure on \((\mathbb{R},\mathcal{B}(\mathbb{R}))\) such that \(\mu(\{0\})=0\) and \(\int_{|x|\leq 1}x^{2}\mu(dx)<\infty\) and \(\mu(\{x:|x|>1\})<\infty\)._

**Corollary 11.3.4:** _Stable distributions are infinitely divisible._

**Proof:** The normal distribution corresponds to the case \(\mu(\cdot)\equiv 0\) and \(\beta>0\). For nonnormal stable laws with index \(\alpha<2\), set \(\beta=0\) and \(\mu(dx)=\theta x^{-(\alpha+1)}dx\) for \(x>0\) and \((1-\theta)|x|^{-(\alpha+1)}dx\) for \(x<0\). \(\Box\)

**Corollary 11.3.5:** _Every infinitely divisible distribution is the limit of centered and scaled compound Poisson distributions._

**Proof:** Since the normal distribution can be obtained as a (weak) limit of centered and scaled Poisson distributions, it is enough to consider the case when \(\beta=0\), \(c=0\). Let \(\mu_{n}(A)=\mu(A\cap\{x:|x|>n^{-1}\})\), \(A\in\mathcal{B}(\mathbb{R})\) and let

\[\phi_{n}(t) = \exp\bigg{(}\int\Big{(}e^{\iota tx}-1-\frac{\iota tx}{1+x^{2}} \Big{)}\mu_{n}(dx)\bigg{)}\] \[= \exp\bigg{(}\lambda_{n}\Big{[}\int(e^{\iota tx}-1)\tilde{\mu}_{n} (dx)-\iota tc_{n}\Big{]}\bigg{)}\]

where

\[\tilde{\mu}_{n}(A) = \mu_{n}(A)/\mu_{n}(\mathbb{R}),\ A\in\mathcal{B}(\mathbb{R}),\ \lambda_{n}=\mu_{n}(\mathbb{R}),\quad\mbox{and}\] \[c_{n} = \int\frac{x}{1+x^{2}}\tilde{\mu}_{n}(dx).\]

Thus, \(\phi_{n}(\cdot)\) is a compound Poisson characteristic function centered at \(c_{n}\), with Poisson parameter \(\lambda_{n}\) and with the compounding distribution \(\tilde{\mu}_{n}\). By the DCT, \(\phi_{n}(t)\rightarrow\phi(t)\) for each \(t\in\mathbb{R}\). Hence by the Levy-Cramer continuity theorem, the result follows. \(\Box\)

Another characterization of infinitely divisible distributions is similar to that of stable distributions. Recall that a stable distribution is one that is the limit of normalized sums of iid random variables and conversely.

**Theorem 11.3.6:** _A random variable \(X\) is infinitely divisible iff it is the limit in distribution of a sequence \(\{X_{n}\}_{n\geq 1}\) where for each \(n\), \(X_{n}\) is the sum of \(n\) iid random variables \(\{X_{nj}\}_{j=1}^{n}\)._

Thus \(X\) is infinitely divisible iff it is the limit in distribution of the row sums of a triangular array of random variables where in each row, all the random variables are iid.

**Proof:** The 'only if' part follows from the definition. For the 'if' part, fix \(k\geq 1\). Then \(X_{k\cdot n}\) can be written as

\[X_{k\cdot n}=\sum_{j=1}^{k}Y_{jn},\]

where \(Y_{jn}=\sum_{r=(j-1)n+1}^{jn}X_{k\cdot n,r}\)\(j=1,2,\ldots,k\). By hypothesis, \(X_{k\cdot n}\longrightarrow^{d}X\). Now, \(\left\{Y_{jn}\right\}_{j=1}^{k}\) are iid and it can be shown, as in the proof of Theorem 11.3.1, that for each \(i=1,\ldots,k\), \(\left\{Y_{in}\right\}_{n=1}^{\infty}\) are tight and hence, converges in distribution to a limit \(Y_{i}\) through a subsequence, and that \(X\) and \(\sum_{i=1}^{k}Y_{i}\) have the same distribution. Thus, \(X\) is infinitely divisible. \(\Box\)

### 11.4 Refinements and extensions of the CLT

This section is devoted to studying some refinements and generalizations of the basic CLT results, such as the rate of convergence in the CLT, Edgeworth expansions and large deviations for sums of iid random variables, and also a generalization of the basic CLT to a functional version.

#### 11.4.1 The Berry-Esseen theorem

Let \(X_{1},X_{2},\ldots\) be a sequence of iid random variables with \(EX_{1}=\mu\) and \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\). Then, Corollary 11.1.2 and Polya's theorem imply that

\[\Delta_{n}\equiv\sup_{x\in\mathbb{R}}\left|P\Big{(}\frac{S_{n}-n\mu}{\sigma \sqrt{n}}\leq x\Big{)}-\Phi(x)\right|\to 0\quad\mbox{as}\quad n\to\infty, \tag{4.1}\]

where \(S_{n}=X_{1}+\cdots+X_{n}\), \(n\geq 1\), and \(\Phi(\cdot)\) is the cdf of the \(N(0,1)\) distribution. A natural question that arises in this context is "how fast does \(\Delta_{n}\) go to zero?" Berry (1941) and Esseen (1942) independently proved that \(\Delta_{n}=O(n^{-1/2})\) as \(n\to\infty\), provided \(E|X_{1}|^{3}<\infty\). This result is referred to as the Berry-Esseen theorem.

**Theorem 11.4.1:** (_The Berry-Esseen theorem_). _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=\mu\), \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\) and \(E|X_{1}|^{3}<\infty\). Then, for all \(n\geq 1\),_

\[\Delta_{n}\equiv\sup_{x\in\mathbb{R}}\left|P\Big{(}\frac{S_{n}-n\mu}{\sigma \sqrt{n}}\leq x\Big{)}-\Phi(x)\right|\leq C\cdot\frac{E|X_{1}-\mu|^{3}}{\sigma ^{3}\sqrt{n}} \tag{4.2}\]

_where \(C\in(0,\infty)\) is a constant._

The value of the constant \(C\in(0,\infty)\) does not depend on \(n\) and on any characteristics of the distribution of \(X_{1}\). Indeed, the proof of Theorem 11.4.1 below shows that \(C\leq\sqrt{\frac{2}{\pi}}\cdot\left[\frac{5}{2}+\frac{12}{\pi}\right]<5.05\).

The following result plays an important role in the proof of Theorem 11.4.1.

**Lemma 11.4.2:** (_A smoothing inequality_). Let \(F\) be a cdf on \(\mathbb{R}\) with \(\int xdF(x)=0\) and characteristic function \(\zeta(t)=\int\exp(\iota tx)dF(x)\), \(t\in\mathbb{R}\). Let \(G:\mathbb{R}\to\mathbb{R}\) be a differentiable function with derivative \(g\) such that \(\lim_{|x|\to\infty}\big{(}F(x)-G(x)\big{)}=0\). Suppose that \(\int(1+|x|)|g(x)|dx<\infty\), \(\int_{-\infty}^{\infty}x^{r}g(x)dx=0\) for \(r=0,1\) and \(|g(x)|\leq C_{0}\) for all \(x\in\mathbb{R}\), for some \(C_{0}\in(0,\infty)\). Then, for any \(T\in(0,\infty)\),_

\[\sup_{x\in\mathbb{R}}\Big{|}F(x)-G(x)\Big{|}\leq\frac{1}{\pi}\int_{-T}^{T}\frac {|\zeta(t)-\xi(t)|}{|t|}\,dt+\frac{24C_{0}}{\pi T} \tag{4.3}\]

_where \(\xi(t)=\int_{-\infty}^{\infty}\exp(\iota tx)g(x)dx\), \(t\in\mathbb{R}\)._

For a proof of Lemma 11.4.2, see Feller (1966).

The next lemma deals with an expansion of the logarithm of the characteristic function of \(X\), in a neighborhood of zero. Let \(z=re^{i\theta}\), \(r\in(0,\infty)\), \(\theta\in[0,2\pi)\) be the polar representation of a nonzero complex number \(z\). Then, the (principal branch of the) complex logarithm of \(z\) is defined as

\[\log z=\log r+i\theta. \tag{4.4}\]

The function \(\log z\) is infinitely differentiable on the set \(\{z\in\mathbb{C}:z=re^{i\theta},r\in(0,\infty),0\leq\theta<2\pi\}\) and has a convergent Taylor's series expansion around \(1\) on the unit disc:

\[\log(1+z)=\sum_{k=1}^{\infty}z^{k}/k\quad\mbox{for}\quad|z|<1. \tag{4.5}\]

**Lemma 11.4.3:** _Let \(Y\) be a random variable with \(EY=0\), \(\tilde{\sigma}^{2}=EY^{2}\in(0,\infty)\), \(\tilde{\rho}=E|Y|^{3}<\infty\) and characteristic function \(\phi_{Y}(t)=E\exp(\iota tY)\), \(t\in\mathbb{R}\). Then, for all \(t\in\big{[}-\frac{1}{\tilde{\sigma}},\frac{1}{\tilde{\sigma}}\big{]}\),_

\[\bigg{|}\log\phi_{Y}(t)+\frac{t^{2}\tilde{\sigma}^{2}}{2}\bigg{|}\leq\frac{5}{ 12}\,|t|^{3}\tilde{\rho} \tag{4.6}\]

_and_

\[\bigg{|}\log\phi_{Y}(t)-\Big{[}\frac{(\iota t)^{2}}{2!}\tilde{ \sigma}^{2}+\frac{(\iota t)^{3}}{3!}EY^{3}\Big{]}\bigg{|} \tag{4.7}\] \[\leq E\bigg{(}\min\bigg{\{}\frac{|tY|^{3}}{3},\frac{(tY)^{4}}{24} \bigg{\}}\bigg{)}+\frac{t^{4}\tilde{\sigma}^{4}}{4}.\]

**Proof:** Note that by Lemma 10.1.5,

\[\big{|}\phi_{Y}(t)-1\big{|}=\big{|}E\big{(}\exp(\iota tY)-1-\iota tY\big{)}\big{|} \leq\frac{t^{2}EY^{2}}{2}\leq\frac{1}{2} \tag{4.8}\]

whenever \(|t|\leq\tilde{\sigma}^{-1}\). In particular, \(\log\phi_{Y}(t)\) is well defined for all \(t\in\big{[}-\tilde{\sigma}^{-1},\tilde{\sigma}^{-1}\big{]}\).

By (4.5), (4.8), and Lemma 10.1.5, for \(|t|\leq\tilde{\sigma}^{-1}\),

\[\bigg{|}\log\phi_{Y}(t)+\frac{t^{2}\tilde{\sigma}^{2}}{2}\bigg{|} \tag{4.10}\] \[= \bigg{|}\log\Big{[}1+\big{(}\phi_{Y}(t)-1\big{)}\Big{]}+\frac{t^{ 2}\tilde{\sigma}^{2}}{2}\bigg{|}\] \[\leq \bigg{|}\phi_{Y}(t)-\Big{[}1-\frac{t^{2}\tilde{\sigma}^{2}}{2} \Big{]}\bigg{|}+\sum_{k=2}^{\infty}\big{|}\phi_{Y}(t)-1\big{|}^{k}/k\] \[\leq E\bigg{|}(tY)^{2}\wedge\frac{|tY|^{3}}{3!}\bigg{|}+\frac{1}{2} \Big{(}\frac{t^{2}\tilde{\sigma}^{2}}{2}\Big{)}^{2}\sum_{k=2}^{\infty}\Big{(} \frac{1}{2}\Big{)}^{k-2}\] \[\leq \frac{|t|^{3}\rho}{6}+\frac{t^{4}\tilde{\sigma}^{4}}{4}.\]

Now using the bounds \(|t\tilde{\sigma}|\leq 1\) and \(\tilde{\sigma}^{3}=(EY^{2})^{3/2}\leq E|Y|^{3}=\tilde{\rho}\), one gets (4.6). The proof of (4.7) is similar and hence, it is left as an exercise (Problem 11.27). \(\Box\)

**Proof of Theorem 11.4.1:** W.l.o.g., set \(\mu=0\) and \(\sigma=1\). Then, \(X_{1},X_{2},\ldots\) are iid zero mean, unit variance random variables. Let \(X=^{d}X_{1}\), \(\rho=E|X|^{3}\) and \(\phi_{X}(\cdot)\) denote the characteristic function of \(X\). It is easy to check that the conditions of Lemma 11.4.2 hold with \(F(x)=P\big{(}\frac{S_{n}}{\sqrt{n}}\leq x\big{)}\), \(G(x)=\Phi(x)\), \(x\in\mathbb{R}\), and \(C_{0}=\frac{1}{\sqrt{2\pi}}\). Hence, by Lemma 11.4.2, with \(T=\sqrt{n}/\rho\),

\[\Delta_{n}\leq\frac{1}{\pi}\int_{-T}^{T}\frac{\big{|}\phi_{X}^{n}(\frac{t}{ \sqrt{n}})-e^{-t^{2}/2}\big{|}}{|t|}\,dt+\frac{24\rho}{\pi\sqrt{2\pi n}}. \tag{4.11}\]

By Lemma 11.4.3 (with \(Y=\frac{X_{1}-\mu}{\sigma}\) and \(t\) replaced by \(\frac{t}{\sqrt{n}}\)),

\[r_{n}(t) \equiv \Big{|}n\log\phi_{X}\Big{(}\frac{t}{\sqrt{n}}\Big{)}+\frac{t^{2}} {2}\Big{|} \tag{4.12}\] \[= n\Big{|}\log\phi_{X}\Big{(}\frac{t}{\sqrt{n}}\Big{)}+\Big{(} \frac{t}{\sqrt{n}}\Big{)}^{2}\,\frac{\sigma^{2}}{2}\Big{|}\] \[\leq \frac{5}{12}\cdot\frac{\rho|t|^{3}}{\sqrt{n}}\]

for all \(|t|\leq\sqrt{n}\), \(n\geq 1\).

Since \(\rho=E|X_{1}|^{3}\geq(EX_{1}^{2})^{3/2}=\sigma^{3}=1\), \(|T|\leq\sqrt{n}\). Hence, using the inequality \(|e^{z}-1|\leq|z|e^{|z|}\) for all \(z\in\mathbb{C}\) and (4.10), one gets

\[\Big{|}\phi_{X}^{n}\Big{(}\frac{t}{\sqrt{n}}\Big{)}-e^{-t^{2}/2} \Big{|} \tag{4.11}\] \[= \Big{|}\exp\Big{(}n\cdot\log\phi_{X}\Big{(}\frac{t}{\sqrt{n}} \Big{)}+\frac{t^{2}}{2}\Big{)}-1\Big{|}\cdot\exp\Big{(}-\frac{t^{2}}{2}\Big{)}\] \[\leq |r_{n}(t)|\exp\big{(}|r_{n}(t)|\big{)}\cdot\exp\Big{(}-\frac{t^{2 }}{2}\Big{)}\] \[\leq \frac{5\rho}{12\sqrt{n}}\,|t|^{3}\,\exp\Big{(}-\frac{t^{2}}{2} \Big{[}1-\frac{5\rho|t|}{6\sqrt{n}}\Big{]}\Big{)}\] \[\leq \frac{5\rho}{12\sqrt{n}}\,|t|^{3}\,\exp\Big{(}-\frac{t^{2}}{12} \Big{)}\]

for all \(\frac{\rho|t|}{\sqrt{n}}\leq 1\), i.e., for all \(|t|\leq T\), \(n\geq 1\). Since \(\int_{-\infty}^{\infty}t^{2}\exp\big{(}-\frac{t^{2}}{12}\big{)}dt=6\sqrt{2\pi}\), the theorem follows from (4.9) and (4.11) with \(C=\sqrt{\frac{2}{\pi}}\big{[}\frac{5}{2}+\frac{12}{\pi}\big{]}\). \(\Box\)

A striking feature of Theorem 11.4.1 is that the upper bound on \(\Delta_{n}\) in (4.2) is valid for all \(n\geq 1\). Also, under the conditions of Theorem 11.4.1, the rate \(O(\frac{1}{\sqrt{n}})\) in (4.2) is the best possible in the sense that there exist random variables for which \(\Delta_{n}\) is bounded below by a constant multiple of \(\frac{1}{\sqrt{n}}\) (cf. Problem 11.29). Edgeworth expansions of the cdf of \(\frac{S_{n}-n\mu}{\sigma\sqrt{n}}\), to be developed in the next section, can be used to show that for certain random variables \(X_{1}\) satisfying additional moment and symmetry conditions, \(\Delta_{n}\) may go to zero at a faster rate. (For example, consider \(X_{1}\sim N(\mu,\sigma^{2})\).)

For iid sequences \(\{X_{n}\}_{n\geq 1}\) with \(E|X_{1}|^{2+\delta}<\infty\) for some \(\delta\in(0,1]\), Theorem 11.4.1 can be strengthened to show that \(\Delta_{n}\) decreases at the rate \(O(n^{-\delta/2})\) as \(n\to\infty\) (cf. Chow and Teicher (1997), Chapter 9).

#### Edgeworth expansions

Recall from Chapter 10 that a random variable \(X_{1}\) is called _lattice_ if there exist \(a\in\mathbb{R}\) and \(h\in(0,\infty)\) such that

\[P\big{(}X_{1}\in\{a+ih:i\in\mathbb{Z}\}\big{)}=1. \tag{4.12}\]

The largest \(h\) satisfying (4.12) is called the _span_ of (the distribution of) \(X_{1}\). A random variable \(X_{1}\) is called _nonlattice_ if it is not a lattice random variable. From Proposition 10.1.1, it follows that \(X_{1}\) is nonlattice iff

\[\big{|}E\exp(tX_{1})\big{|}<1\quad\mbox{for all}\quad t\neq 0. \tag{4.13}\]

The next result gives an Edgeworth expansion for the cdf of \(\frac{S_{n}-n\mu}{\sigma\sqrt{n}}\) with an error of order \(o(n^{-1/2})\) for nonlattice random variables.

**Theorem 11.4.4:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=\mu\), \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\) and \(E|X_{1}|^{3}<\infty\). Suppose, in addition, that \(X_{1}\) is nonlattice, i.e., it satisfies (4.13). Then,_

\[\sup_{x\in\mathbb{R}}\left|P\Big{(}\frac{S_{n}-n\mu}{\sigma\sqrt{n }}\leq x\Big{)}-\Big{[}\Phi(x)-\frac{1}{\sqrt{n}}\cdot\frac{\mu_{3}}{6\sigma^{ 3}}(x^{2}-1)\phi(x)\Big{]}\right|\] \[=o(n^{-1/2})\quad\mbox{as}\quad n\to\infty, \tag{4.14}\]

_where \(\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}\), \(x\in\mathbb{R}\) and \(\mu_{3}=E(X_{1}-\mu)^{3}\)._

The function

\[e_{n,2}(x)\equiv\Phi(x)-\frac{1}{\sqrt{n}}\cdot\frac{\mu_{3}}{6\sigma^{3}}(x^{ 2}-1),\ x\in\mathbb{R} \tag{4.15}\]

is called a _second order Edgeworth expansion_ for \(T_{n}\equiv\frac{S_{n}-n\mu}{\sigma\sqrt{n}}\). The above theorem shows that the cdf of the _normalized sum_\(T_{n}\) can be approximated by the second order Edgeworth expansion with accuracy \(o(n^{-1/2})\). It can be shown that if \(E|X_{1}|^{4}<\infty\) and \(X_{1}\) satisfies _Cramer's condition_:

\[\limsup_{|t|\to\infty}\big{|}E\exp(\iota tX_{1})\big{|}<1, \tag{4.16}\]

then the bound on the right side of (4.14) can be improved to \(O(n^{-1})\). Note that for a symmetric random variable \(X_{1}\), having a finite fourth moment and satisfying (4.16), the second term in \(e_{n,2}(x)\) is zero and the rate of normal approximation becomes \(O(n^{-1})\). Higher order Edgeworth expansions for \(T_{n}\) can be derived using (4.16) and arguments similar to those in the proof of Theorem 11.4.4, but the form of the expansion becomes more complicated. See Petrov (1975), Bhattacharya and Rao (1986), and Hall (1992) for detailed accounts of the Edgeworth expansion theory.

**Proof of Theorem 11.4.4:** W.l.o.g., let \(\mu=0\) and \(\sigma=1\). In Lemma 11.4.2, take \(F(x)=P(T_{n}\leq x)\), and \(G(x)=e_{n,2}(x)\), \(x\in\mathbb{R}\). Then, it is easy to verify that the conditions of Lemma 11.4.2 hold with \(g(x)=g_{n}(x)\equiv\phi(x)+\frac{\mu_{3}}{6\sqrt{n}}(x^{3}-3x)\phi(x)\), \(x\in\mathbb{R}\). Using repeated differentiation on both sides of the identity (inversion formula):

\[\frac{e^{-x^{2}/2}}{\sqrt{2\pi}}=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{- \iota tx}\cdot e^{-\iota^{2}/2}dt,\ x\in\mathbb{R},\]

one can show that

\[-(x^{3}-3x)\frac{e^{-x^{2}/2}}{\sqrt{2\pi}}=\frac{d^{3}}{dx^{3}}\Big{(}\frac{e ^{-x^{2}/2}}{\sqrt{2\pi}}\Big{)}=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{- \iota tx}(-\iota t)^{3}e^{-\iota^{2}/2}dt,\]

\(x\in\mathbb{R}\). Hence,

\[\xi_{n}(t)\equiv\xi(t)=\int e^{\iota tx}g_{n}(x)dx=e^{-t^{2}/2}\Big{[}1+\frac {\mu_{3}}{6\sqrt{n}}(\iota t)^{3}\Big{]},\ t\in\mathbb{R}. \tag{4.17}\]

[MISSING_PAGE_EMPTY:376]

Since \(r_{n}(t)\vee\frac{|\mu_{3}t|^{3}}{6\sqrt{n}}\leq\frac{5}{12}|t|^{2}\) for all \(|t|\leq\delta\sqrt{n}\),

\[\int_{-\delta\sqrt{n}}^{\delta\sqrt{n}}\frac{\big{|}\phi_{X}^{n}( \frac{t}{\sqrt{n}})-\xi_{n}(t)\big{|}}{|t|}dt \tag{4.21}\] \[\leq \int_{-\delta\sqrt{n}}^{\delta\sqrt{n}}\Big{[}\frac{\epsilon}{ \sqrt{n}}\cdot t^{2}+\frac{\mu_{3}^{2}}{72n}\cdot|t|^{5}\Big{]}\exp\Big{(}- \frac{t^{2}}{12}\Big{)}dt\] \[\leq C_{1}\cdot\frac{\epsilon}{\sqrt{n}}\]

for some \(C_{1}\in(0,\infty)\). By (4.13),

\[\sup\Big{\{}\Big{|}\phi_{X}\Big{(}\frac{t}{\sqrt{n}}\Big{)}\Big{|}^{n}:\delta \sqrt{n}<|t|<c\sqrt{n}\Big{\}}\leq\theta^{n}\]

for some \(\theta\in(0,1)\). Hence,

\[\int_{\delta\sqrt{n}<|t|<c\sqrt{n}}\frac{\big{|}\phi_{X}^{n}\big{(} \frac{t}{\sqrt{n}}\big{)}-\xi_{n}(t)\big{|}}{|t|}dt \tag{4.22}\] \[\leq 2\theta^{n}\log(c/\delta)+\frac{1}{\delta\sqrt{n}}\int_{|t|> \delta\sqrt{n}}e^{-t^{2}/2}\Big{(}1+\frac{\rho}{\sqrt{n}}|t|^{3}\Big{)}dt,\] \[= O(\theta_{1}^{n})\quad\mbox{as}\quad n\to\infty\]

for some \(\theta_{1}=\theta_{1}(\delta)\in(0,1)\). Since \(\epsilon>0\) is arbitrary, the result follows from (4.18), (4.21), and (4.22). \(\Box\)

This section concludes with an analog of Theorem 11.4.4 for lattice random variables. Note that for \(X_{1}\) satisfying \(P(X_{1}\in\{a+jh:j\in\mathbb{Z}\})=1\), the normalized sample sum \(T_{n}\) takes values in the lattice \(\big{\{}\frac{na-n\mu}{\sigma\sqrt{n}}+jh/\sigma\sqrt{n}:j\in\mathbb{Z}\big{\}}\). Hence, the cdf of \(T_{n}\) is a step function. The second order Edgeworth expansion for \(T_{n}\) is no longer a smooth function -- the effect of the jumps of the cdf of \(T_{n}\) is now accounted for by adding a discontinuous function to the expansion \(e_{n,2}\). Let

\[Q(x)=x-\lfloor x\rfloor-\frac{1}{2},\ x\in\mathbb{R} \tag{4.23}\]

where \(\lfloor x\rfloor\) denotes the largest integer not exceeding \(x\), \(x\in\mathbb{R}\). It is easy to check that \(Q(x)\) is a periodic function of period \(1\) with values in \(\big{[}-\frac{1}{2},\frac{1}{2}\big{]}\), \(Q(x)\) is right continuous, and it has jumps of size \(1\) at the integer values. The second order Edgeworth expansions for \(T_{n}\) in the lattice case involves the function \(Q\), as shown below.

**Theorem 11.4.5:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of lattice random variables with span \(h>0\). Suppose that \(E|X_{1}|^{3}<\infty\) and \(\sigma^{2}=\mbox{Var}(X_{1})\in(0,\infty)\). Then,_

\[\sup_{x\in\mathbb{R}}\left|P\Big{(}\frac{S_{n}-n\mu}{\sigma\sqrt{n}}\leq x \Big{)}-\tilde{e}_{n,2}(x)\right|=o(n^{-1/2})\quad\mbox{as}\quad n\to\infty \tag{4.24}\]_where \(\mu=EX_{1}\), \(S_{n}=X_{1}+\cdots+X_{n}\), \(n\geq 1\),_

\[\tilde{e}_{n,2}(x)=\Phi(x)-\frac{1}{\sqrt{n}}\biggl{[}\frac{\mu_{3}}{6\sigma^{3} }(x^{2}-1)+\frac{h}{\sigma}\,Q\Bigl{(}\frac{\sqrt{n}[nx\sigma-nx_{0}]}{h} \Bigr{)}\biggr{]}\phi(x), \tag{4.25}\]

\(x\in\mathbb{R}\)_, \(Q(x)\) is as in (4.23), \(\phi(x)=\frac{1}{\sqrt{2\pi}}\,\exp(-x^{2}/2)\), \(x\in\mathbb{R}\) and \(x_{0}\) is a real number satisfying \(P(X_{1}=\mu+x_{0})>0\)._

For a proof of Theorem 11.4.5 and for related results, see Esseen (1945) and Bhattacharya and Rao (1986).

#### Large deviations

Let \(X_{1},X_{2},\ldots\) be iid random variables with \(EX_{1}=\mu\). The SLLN implies that for any \(x>\mu\),

\[P(\bar{X}_{n}>x)\to 0\quad\mbox{as}\quad n\to\infty, \tag{4.26}\]

where \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\), \(n\geq 1\). If \(X_{i}\)'s were distributed as \(N(\mu,\sigma^{2})\) for some \(\mu\in\mathbb{R}\) and \(\sigma^{2}\in(0,\infty)\), then the left side of (4.26) equals \(\Phi\bigl{(}\frac{\sqrt{n}(x-\mu)}{\sigma}\bigr{)}\). Now note that

\[-\log\Phi\Bigl{(}\frac{\sqrt{n}(x-\mu)}{\sigma}\Bigr{)}\sim-\log\big{[}\exp(-nc _{1}^{2})/(\sqrt{n}\,c_{1}\sqrt{2n})\big{]}\sim nc_{1}^{2},\]

where \(c_{1}=(x-\mu)/\sigma\in(0,\infty)\). Large deviation bounds on the probability \(P(\bar{X}_{n}>x)\) assert that a similar behavior holds for many distributions other than the normal distribution on the (negative) logarithmic scale.

The main result of this section is the following.

**Theorem 11.4.6:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid nondegenerate random variables with_

\[\phi(t)\equiv Ee^{tX_{1}}<\infty\quad\mbox{for all}\quad t>0. \tag{4.27}\]

_Let \(\mu=EX_{1}\). Then, for all \(x\in(\mu,\theta)\)_

\[\lim_{n\to\infty}n^{-1}\log P(\bar{X}_{n}\geq x)=-\gamma(x), \tag{4.28}\]

_where_

\[\gamma(x)=\sup_{t>0}\{tx-\log\phi(t)\}\mbox{ and }\theta=\sup\{x\in\mathbb{R}: P(X_{1}\leq x)<1\}. \tag{4.29}\]

Note that under (4.27), \(EX_{1}^{+}<\infty\) and hence, \(\mu\equiv EX_{1}\) is well defined, and \(\mu\in[-\infty,\infty)\).

For proving the theorem, the following results are needed.

**Lemma 11.4.7:**_Let \(X_{1}\) be a nondegenerate random variable satisfying (4.27). Let \(\mu=EX_{1}\) and let \(\gamma(x)\), \(\theta\) be as in (4.29). Then,_* _the function_ \(\phi(t)\) _is infinitely differentiable on_ \((0,\infty)\) _with_ \(\phi^{(r)}(t)\)_, the_ \(r\)_th derivative of_ \(\phi(t)\) _being given by_ \[\phi^{(r)}(t)=E\Big{(}X_{1}^{r}e^{tX_{1}}\Big{)},\ t\in(0,\infty),\ r\in\mathbb{ N},\] (4.30)
* \(\lim_{t\downarrow 0}\phi(t)=1\)_,_ \(\lim_{t\downarrow 0}\phi^{(1)}(t)=\mu\)_, and_ \(\lim_{t\to\infty}\frac{\phi^{(1)}(t)}{\phi(t)}=\theta\)_,_
* _for every_ \(x\in(\mu,\theta)\)_, there exists a unique solution_ \(a_{x}\in\mathbb{R}\) _to the equation_ \[x=\phi^{\prime}(a_{x})/\phi(a_{x})\] (4.31) _such that_ \(\gamma(x)=xa_{x}-\log\phi(a_{x})\)_._

**Proof:** Let \(F\) denote the cdf of \(X_{1}\). (i) Note that for any \(h\neq 0\),

\[h^{-1}\big{[}\phi(t+h)-\phi(t)\big{]}=\int_{-\infty}^{\infty}\frac{e^{hx}-1}{h }\cdot e^{tx}dF(x).\]

As \(h\to 0\), the integrand converges \(xe^{tx}\) for all \(x,t\). Also, \(\big{|}\frac{e^{hx}-1}{h}\big{|}\leq\sum_{k=1}^{\infty}\big{|}h^{k-1}x^{k} \big{|}/k!\leq|x|e^{|hx|}\) for all \(h,x\). Hence, for any \(x\in\mathbb{R}\), \(t\in(0,\infty)\), and \(0<|h|<t/2\), the integrand is bounded above by

\[|x|e^{|hx|}e^{tx} = |x|e^{(t-|h|)x}I_{(-\infty,0)}(x)+|x|e^{(t+|h|)x}I(x>0) \tag{4.32}\] \[\leq |x|e^{-t|x|/2}I_{(-\infty,0)}(x)+|x|e^{3tx}I_{(0,\infty)}(x)\] \[\equiv g(x),\ \mbox{say}.\]

Since \(\int g(x)dF(x)<\infty\), by the DCT, it follows that

\[\lim_{h\to 0}\frac{\phi(t+h)-\phi(t)}{h}\quad\mbox{exists and equals}\quad\int x e ^{tx}dF(x)\]

for all \(t\in(0,\infty)\). Thus, \(\phi(t)\) is differentiable on \((0,\infty)\) with \(\phi^{(1)}(t)=EX_{1}e^{tX_{1}}\), \(t\in(0,\infty)\). Now, using induction and similar arguments, one can complete the proof of part (i) (Problem 11.34).

Next consider (ii). Since \(e^{tx}\leq I_{(-\infty,0]}(x)+e^{x}I_{(0,\infty)}(x)\) for all \(x\in\mathbb{R}\), \(t\in(0,1)\), by the DCT, the first relation follows. For the second, note that

\[|x|e^{tx}I_{(-\infty,0)}(x)\uparrow|x|I_{(-\infty,0]}(x)\quad\mbox{as}\quad t\downarrow 0 \tag{4.33}\]

and \(|x|e^{tx}\leq|x|e^{x}\) for all \(0<t\leq 1\), \(x>0\). Hence, applying the MCT for \(x\in(-\infty,0]\) and the DCT for \(x\in(0,\infty)\), one obtains the second limit. Derivation of the third limit is left as an exercise (Problem 11.35).

To prove part (iii), fix \(x\in(\mu,\theta)\) and let \(\gamma(t)=tx-\log\phi(t)\), \(t\geq 0\). Then, for \(t\in(0,\infty)\),

\[\gamma^{(1)}(t) = x-\frac{\phi^{(1)}(t)}{\phi(t)}\] \[\gamma^{(2)}(t) = \frac{\phi^{(2)}(t)}{\phi(t)}-\Big{(}\frac{\phi^{(1)}(t)}{\phi(t) }\Big{)}^{2}=\mbox{Var}(Y_{t}), \tag{4.34}\]

where \(Y_{t}\) is a random variable with cdf \(P(Y_{t}\leq y)=\int_{-\infty}^{y}e^{tu}dF(x)/\phi(t)\), \(y\in\mathbb{R}\). Since \(X_{1}\) is nondegenerate, so is \(Y_{t}\) (for any \(t\geq 0\)) and hence, \(\mbox{Var}(Y_{t})>0\). As a consequence, the second derivative of the function \(\gamma(t)\) is positive. And the minimum of \(\gamma(t)\) over \((0,\infty)\) is attained by a solution to the equation \(\gamma^{(1)}(t)=0\), i.e., by \(t=a_{x}\) satisfying (4.30). That such a solution exists and is unique follows from part (ii) and the facts that \(x>\mu\), \(\frac{\phi^{(1)}(0+)}{\phi(0)}=\mu\) (by (ii)), and that \(\frac{\phi^{(1)}(t)}{\phi(t)}\) is continuous and strictly increasing on \((0,\infty)\) (as for any \(t\in(0,\infty)\), the derivative of \(\frac{\phi^{(1)}(t)}{\phi(t)}\) coincides with \(\gamma^{(2)}(t)\), which is positive by (4.34)). This proves part (iii). \(\Box\)

**Lemma 11.4.8:** _Let \(\{X_{n}\}_{n\geq 1}\) be as in Theorem 11.4.6. For \(t\in(0,\infty)\), let \(\{Y_{t,n}\}_{n\geq 1}\) be a sequence of iid random variables with cdf_

\[P(Y_{t,1}\leq y)=\int_{-\infty}^{y}e^{tu}dF(u)/\phi(t),\ y\in\mathbb{R},\]

_where \(F\) is the cdf of \(X_{1}\). Let \(\nu_{n}\) and \(\lambda_{n}\) denote the probability distributions of \(S_{n}\equiv X_{1}+\cdots+X_{n}\) and \(T_{n,t}=Y_{t,1}+\cdots+Y_{t,n}\), \(n\geq 1\). Then, for each \(n\geq 1\),_

\[\nu_{n}\ll\lambda_{n}\quad\mbox{and}\quad\frac{d\nu_{n}}{d\lambda_{n}}(x)=e^{- tx}\phi(t)^{n},\ x\in\mathbb{R}. \tag{4.35}\]

**Proof:** The proof is by induction. Clearly, the assertion holds for \(n=1\). Next, suppose that (4.35) is true for some \(r\in\mathbb{N}\) and let \(n=r+1\). Then, for any \(A\in\mathcal{B}(\mathbb{R})\),

\[\nu_{n}(A) = P\big{(}X_{1}+\cdots+X_{n}\in A\big{)}\] \[= \int_{-\infty}^{\infty}P\big{(}X_{1}+\cdots+X_{n-1}\in A-x\big{)} dF(x)\] \[= \int_{-\infty}^{\infty}\int_{A-x}\Big{[}\frac{d\nu_{n-1}}{d \lambda_{n-1}}(u)\Big{]}d\lambda_{n-1}(u)dF(x)\] \[= \int_{-\infty}^{\infty}\int_{A-x}e^{-tu}\phi(t)^{n-1}d\lambda_{n -1}(u)dF(x)\] \[= [\phi(t)]^{n}\int_{-\infty}^{\infty}\int_{A-x}e^{-t(u+x)}d \lambda_{n-1}(u)d\lambda_{1}(x)\] \[= [\phi(t)]^{n}\int_{A}e^{-tu}\big{(}\lambda_{n-1}*\lambda_{1} \big{)}(d\nu),\]where \(*\) denotes convolution. Since \(\lambda_{n-1}*\lambda_{1}=\lambda_{n}\), the result follows. \(\Box\)

**Proof of Theorem 11.4.6:** Fix \(x\in(\mu,\theta)\). Note that by Markov's inequality, for any \(t>0\), \(n\geq 1\),

\[P(\bar{X}_{n}\geq x) = P\Big{(}e^{t\bar{X}_{n}}\geq e^{tx}\Big{)}\] \[\leq e^{-tx}E\Big{(}e^{t\bar{X}_{n}}\Big{)}\] \[= \exp\big{(}-tx+n\log\phi(t/n)\big{)}.\]

Hence,

\[n^{-1}\log P\big{(}\bar{X}_{n}\geq x\big{)}\leq-x\cdot\frac{t}{n}+ \log\phi\big{(}\frac{t}{n}\big{)}\quad\mbox{for all}\quad t>0,\ n\geq 1\] \[\Rightarrow\limsup_{n\to\infty}n^{-1}\log P\big{(}\bar{X}_{n} \geq x\big{)}\leq\inf_{t>0}\{-xt+\log\phi(t)\}=-\gamma(x). \tag{4.36}\]

This yields the upper bound. Next it will be shown that

\[\liminf_{n\to\infty}n^{-1}\log P\big{(}\bar{X}_{n}\geq x\big{)}\geq-\gamma(x). \tag{4.37}\]

To that end, let \(\{Y_{t,n}\}_{n\geq 1}\), \(\nu_{n}\), and \(\lambda_{n}\) be as in Lemma 11.4.8. Also, let \(a_{x}\) be as in (4.30). Then, for any \(y>x\), \(t\in(a_{x},\infty)\), and \(n\geq 1\), by Lemma 11.4.8,

\[P\big{(}\bar{X}_{n}\geq x\big{)} = \nu_{n}\big{(}[nx,\infty)\big{)} \tag{4.38}\] \[= \int_{[nx,\infty)}e^{-tu}\phi(t)^{n}\,du\] \[\geq \int_{[nx,ny]}e^{-tu}\phi(t)^{n}\,du\] \[\geq \phi(t)^{n}e^{-tny}\lambda_{n}\big{(}[nx,ny]\big{)}.\]

Note that \(EY_{t,1}=\int ud\lambda_{1}(u)=\phi^{(1)}(t)/\phi(t)\). Since \(\phi^{(1)}(\cdot)/\phi(\cdot)\) is strictly increasing and continuous on \((0,\infty)\), given \(y>x\), there exists a \(t=t_{y}\in(a_{x},\infty)\) such that

\[y>\frac{\phi^{(1)}(t)}{\phi(t)}>\frac{\phi^{(1)}(a_{x})}{\phi(a_{x})}=x. \tag{4.39}\]

By the WLLN, for any \(y>x\) and \(t\) satisfying (4.39),

\[\lambda_{n}\big{(}[nx,ny]\big{)} = P\bigg{(}x\leq\frac{Y_{t,1}+\cdots+Y_{t,n}}{n}\leq y\bigg{)}\] \[\to 1\quad\mbox{as}\quad n\to\infty.\]

Hence, from (4.38), it follows that

\[\liminf_{n\to\infty}n^{-1}\log P\big{(}\bar{X}_{n}\geq x\big{)}\geq-ty+\log \phi(t)\]for all \(y>x\) and all \(t\in(a_{x},\infty)\) satisfying (4.39). Now, letting \(t\downarrow a_{x}\) first and then \(y\downarrow x\), one gets (4.37). This completes the proof of Theorem 11.4.6. \(\Box\)

**Remark 11.4.1:** If (4.27) holds and \(\theta<\infty\), then

\[P\bigl{(}\bar{X}_{n}\geq\theta\bigr{)}=\bigl{[}P(X_{1}=\theta)\bigr{]}^{n}\]

so that

\[\lim_{n\to\infty}n^{-1}\log P\bigl{(}\bar{X}_{n}\geq\theta\bigr{)}=\log P(X_{1 }=\theta).\]

In this case, (4.28) holds for \(x=\theta\) with \(\gamma(\theta)=-\log P(X_{1}=\theta)\). For \(x>\theta\), (4.28) holds with \(\gamma(x)=+\infty\).

**Remark 11.4.2:** Suppose that there exists a \(t_{0}\in(0,\infty)\) such that, instead of (4.27), the following condition holds:

\[\phi(t) = +\infty\quad\mbox{for all}\quad t>t_{0}\] \[< \infty\quad\mbox{for all}\quad t\in(0,t_{0}),\]

and \(\phi^{\prime}(t)/\phi(t)\) increases to a finite limit \(\theta_{0}\) as \(t\uparrow t_{0}\). Then, \(\theta\) must be \(+\infty\). In this case, it can be shown that (4.28) holds for all \(x\in(\mu,\theta_{0})\) (with the given definition of \(\gamma(x)\)) and that (4.28) holds for all \(x\in[\theta_{0},\infty)\), with \(\gamma(x)\equiv t_{0}x-\log\phi(t_{0})\). See Theorem 9.6, Chapter 1, Durrett (2004).

#### The functional central limit theorem

Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with \(EX_{1}=0\), \(EX_{1}^{2}=1\). Let \(S_{0}=0\), \(S_{n}=\sum_{i=1}^{n}X_{i}\), \(n\geq 1\). The central limit theorem says that as \(n\to\infty\),

\[W_{n}\equiv\frac{S_{n}}{\sqrt{n}}\longrightarrow^{d}N(0,1).\]

Now let

\[W_{n}\Bigl{(}\frac{j}{n}\Bigr{)}=\frac{1}{\sqrt{n}}\,S_{j},\ j=0,1,2,\ldots,n \tag{4.40}\]

and for any \(\frac{j}{n}\leq t<\frac{j+1}{n}\), \(j=0,1,2,\ldots,n\), let

\[W_{n}(t)=W_{n}\Bigl{(}\frac{j}{n}\Bigr{)}+\Bigl{(}t-\frac{j}{n}\Bigr{)}\frac{ \Bigl{(}W_{n}(\frac{j+1}{n})-W_{n}(\frac{j}{n})\Bigr{)}}{\frac{1}{n}} \tag{4.41}\]

be the function obtained by linear interpolation of \(\{W_{n}\bigl{(}\frac{j}{n}\bigr{)}:0\leq j\leq n\}\) on \([0,1]\). Then, \(W_{n}(\cdot)\) is a random element of the metric space \(\mathbb{S}\equiv\mathcal{C}[0,1]\) of all real valued continuous functions on \([0,1]\) with the supremum metric \(\rho(f,g)\equiv\sup\{|f(t)-g(t)|:0\leq t\leq 1\}\). Let \(\mu_{n}(\cdot)\) be the probability distribution induced on \(\mathcal{C}[0,1]\) by \(W_{n}(\cdot)\).

By an application of the multivariate CLT it can be shown that for any \(k\in\mathbb{N}\) and any \(0\leq t_{1}\leq t_{2}\leq\cdots\leq t_{k}\leq 1\), the joint distribution of

\[\Big{(}W_{n}(t_{1}),W_{n}(t_{2}),\ldots,W_{n}(t_{k})\Big{)}\]

will converge to a \(k\)-variate normal distribution with mean vector \((0,0,\ldots,0)\) and covariance matrix \(\Sigma\equiv\big{(}(\sigma_{ij})\big{)}\), where \(\sigma_{ij}=t_{i}\wedge t_{j}\). It turns out that a \(\mathcal{C}[0,1]\) valued random variable \(W(\cdot)\), called the _standard Brownian motion on \([0,1]\)_ (SBM \([0,1]\)) can be defined such that for any \(k\geq 1\) and any \(0\leq t_{1}\leq t_{2}\leq\cdots\leq t_{k}\leq 1\), \((W(t_{1}),\ldots,W(t_{k}))\) has a \(k\)-variate normal distribution with mean vector \((0,0,\ldots,0)\) and covariance matrix \(\Sigma\) as above (see Chapter 15). Thus,

\[\Big{(}W_{n}(t_{1}),\ldots,W_{n}(t_{k})\Big{)}\longrightarrow^{d}\Big{(}W(t_{ 1}),\ldots,W(t_{k})\Big{)}.\]

If \(\mu(\cdot)\) is the probability distribution of \(W(\cdot)\) on \(\mathcal{C}[0,1]\), then the above suggests that \(\mu_{n}\longrightarrow^{d}\mu\). This is indeed true and is known as a _functional central limit theorem_. See Billingsley (1968, 1995) for more details.

**Theorem 11.4.9:** (_Functional central limit theorem_). _Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with \(EX_{1}=0\), \(EX_{1}^{2}=1\). Let \(S_{0}=0\), \(S_{n}=\sum_{i=1}^{n}X_{i}\), \(n\geq 1\) and for all \(n\geq 1\), let \(W_{n}(\cdot)\) be the \(\mathcal{C}[0,1]\) random elements obtained by the linear interpolation of \(W_{n}(\frac{j}{n})\equiv\frac{S_{j}}{\sqrt{n}}\), \(j=0,1,2,\ldots,n\) as defined in (4.41). Then_

* _there exists a_ \(\mathcal{C}[0,1]\)_-valued random variable_ \(W(\cdot)\) _such that_ \[W_{n}(\cdot)\longrightarrow^{d}W(\cdot)\quad\mbox{in}\quad\mathcal{C}[0,1],\]
* \(W(\cdot)\) _is a Gaussian process with zero as its mean function and the covariance function_ \(C(s,t)=s\wedge t\)_,_ \(0\leq s\)_,_ \(t\leq 1\)_._

**Proof:** (_An outline_). There are three steps.

Step I: The sequence of probability distributions \(\{\mu_{n}(\cdot)\}_{n\geq 1}\) on \(\mathcal{C}[0,1]\) induced by \(\{W_{n}(\cdot)\}_{n\geq 1}\) is tight (as defined in Chapter 9).

Step II: For any random element \(\tilde{W}(\cdot)\) of \(\mathcal{C}[0,1]\), the family of finite dimensional joint distributions of \(\{\tilde{W}(t_{1}),\ldots,\tilde{W}(t_{k})\}\), with \(k\) ranging over \(\mathbb{N}\) and \(0\leq t_{1}\leq t_{2}\leq\ldots\leq t_{k}\leq 1\) determines the probability distribution of \(\tilde{W}(\cdot)\) in \(\mathcal{C}[0,1]\).

Step III: By Step I, every subsequence \(\{\mu_{nj}(\cdot)\}\) of \(\{\mu_{n}(\cdot)\}\) has a further subsequence \(\{\mu_{njk}(\cdot)\}\) that converges to some probability distribution \(\mu\) on \(\mathcal{C}[0,1]\). (This is a generalization of the Helly's selection theorem. It is known as the Prohorov-Varadarajan theorem.) But then \(\mu\) has the same finite dimensional distribution as SBM [0,1] and hence, by Step II, the measure \(\mu\) is independent of the subsequence. Thus \(\mu_{n}\longrightarrow^{d}\mu\). For a full proof, see Billingsley (1968, 1995).

**Corollary 11.4.10:** _For \(k\in\mathbb{N}\), let \(T:\mathcal{C}[0,1]\rightarrow\mathbb{R}^{k}\) be a continuous function from the metric space \((\mathcal{C}[0,1],\rho)\) to \(\mathbb{R}^{k}\). Then_

\[T\big{(}W_{n}(\cdot)\big{)}\longrightarrow^{d}T\big{(}W(\cdot)\big{)}.\]

Some examples of such continuous functions on the metric space \(\mathcal{C}[0,1]\) are

\[T_{1}(f) \equiv \sup\{f(x):0\leq x\leq 1\},\] \[T_{3}(f) \equiv \sup\{|f(x)|:0\leq x\leq 1\},\quad\mbox{and} \tag{4.42}\] \[T_{2}(f) \equiv \inf\{f(x):0\leq x\leq 1\}.\]

As an application of Corollary 11.4.10 to the above choices of \(T\) yields

**Corollary 11.4.11:** _Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with \(EX_{1}=0\), \(EX_{1}^{2}=1\), let_

\[M_{n1} = \max_{0\leq j\leq n}S_{j},\] \[M_{n2} = \min_{0\leq j\leq n}S_{j},\quad\mbox{and}\] \[M_{n3} = \max_{0\leq j\leq n}|S_{j}|.\]

_Then,_

\[\frac{1}{\sqrt{n}}(M_{n1},M_{n2},M_{n3})\longrightarrow^{d}(M_{1},M_{2},M_{3})\]

_where_

\[M_{1} = \sup\{W(x):0\leq x\leq 1\}\] \[M_{2} = \inf\{W(x):0\leq x\leq 1\}\] \[M_{3} = \sup\{|W(x)|:0\leq x\leq 1\}\]

_and where \(W(\cdot)\) is the Standard Brownian motion on \([0,1]\), as defined in Theorem 11.4.9._

Corollary 11.4.11 is useful in statistical inference in obtaining approximations to the sampling distributions of the statistics \((M_{n1},M_{n2},M_{n3})\) for large \(n\). The exact distribution of \((M_{1},M_{2},M_{3})\) can be obtained by using the _reflection principle_ as discussed in Chapter 15.

#### Empirical process and Brownian bridge

Let \(\{U_{i}\}_{i\geq 1}\) be iid Uniform \([0,1]\) random variable. Let \(F_{n}(t)=\frac{1}{n}\sum_{i=1}^{n}I(U_{i}\leq t)\), \(0\leq t\leq 1\) be the empirical cdf of \(\{U_{1},U_{2},\ldots,U_{n}\}\)Clearly, \(F_{n}(\cdot)\) is a step function on \([0,1]\) with jumps of size \(\frac{1}{n}\) at \(U_{n1}<U_{n2}<\cdots<U_{nn}\), where \((U_{n1},U_{n2},\ldots,U_{nn})\) is the increasing rearrangement of \((U_{1},U_{2},\ldots,U_{n})\), i.e., the \(n\)_order statistics_ based on \((U_{1},U_{2},\ldots,U_{n})\).

Let \(Y_{n}(t)\) be the function obtained by linearly interpolating \(\sqrt{n}(F_{n}(t)-t)\), \(0\leq t\leq 1\) from the values at the jump points \((U_{n1},U_{n2},\ldots,U_{nn})\). Then \(Y_{n}(\cdot)\) is a random element of the metric space \(({\cal C}[0,1],\rho)\), the space of all real valued continuous functions on \([0,1]\) with the supremum metric \(\rho\), where \(\rho(f,g)=\sup\{|f(t)-g(t)|:0\leq t\leq 1\}\). Let \(\{B(t):0\leq t\leq 1\}\) be the Standard Brownian motion, i.e., a \({\cal C}[0,1]\)-valued random variable that is also a Gaussian process with mean zero and covariance function \(c(s,t)=s\wedge t\), \(0\leq s,t\leq 1\). Let \(B_{0}(t)\equiv B(t)-tB(1)\), \(0\leq t\leq 1\). Then \(B_{0}(\cdot)\) is a random element of \({\cal C}[0,1]\). It is also a Gaussian process with mean zero and covariance function \(c_{0}(s,t)=s\wedge t-st-st+st=s\wedge t-st\).

Theorem 11.4.12: \(Y_{n}(\cdot)\longrightarrow^{d}B_{0}(\cdot)\) in \({\cal C}[0,1]\).

The proof is similar to that of Theorem 11.4.9. See Billingsley (1968, 1995) for details.

Definition 11.4.1: The process \(\{B_{0}(t):0\leq t\leq 1\}\) is called the _Brownian bridge_ and the process \(\{Y_{n}(t):0\leq t\leq 1\}\) is called the _empirical process_ based on \(\{U_{1},U_{2},\ldots,U_{n}\}\).

Now recall that by the Glivenko-Cantelli theorem (cf. Chapter 8),

\[\sup\{|F_{n}(t)-t|:0\leq t\leq 1\}\to 0.\]

Since \(T(f)\equiv\sup\{|f(x)|:0\leq x\leq 1\}\) is a continuous map form \({\cal C}[0,1]\) to \({\mathbb{R}}_{+}\), by Theorem 11.4.12 this leads to

Corollary 11.4.13: \(\sqrt{n}\sup\{|F_{n}(t)-t|:0\leq t\leq 1\}\longrightarrow^{d}\sup\{|B_{0}(t)|:0 \leq t\leq 1\}\).

This, in turn, can be used to find the asymptotic distribution of the _Kolmogorov-Smirnov statistic_ (see (4.43) below).

Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with a continuous distribution function \(F(\cdot)\). Let \(F_{n}(x)\equiv\frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x)\) be the empirical distribution based on \((X_{1},X_{2},\ldots,X_{n})\). Let \(U_{i}=F^{-1}(X_{i})\), \(i=1,2,\ldots,n\) where \(F^{-1}(t)=\inf\{x:F(x)\geq t\}\), \(0\leq t\leq 1\). Then \(\{U_{i}\}_{i\geq 1}\) are iid Uniform (0,1) random variables. It can be shown that the Kolmogorov-Smirnov statistic

\[KS(F_{n})\equiv\sqrt{n}\sup\{|F_{n}(x)-F(x)|:0\leq x\leq 1\} \tag{4.43}\]

has the same distribution as \(\sup\{|Y_{n}(t)|:0\leq t\leq 1\}\) and hence

\[KS(F_{n})\longrightarrow^{d}M_{0}\equiv\sup\{|B_{0}(t)|:0\leq t\leq 1\}.\]This can be used to test the hypothesis that \(F(\cdot)\) is the cdf of \(\{X_{i}\}_{i\geq 1}\) by rejecting it if \(KS(F_{n})\) is large. To decide on what is large, the distribution of \(KS(F_{n})\) can be approximated by that of \(M_{0}\). Thus, if the significance level is \(\alpha\), \(0<\alpha<1\), then one determines a value \(C_{0}\) such that \(P(M_{0}>C_{0})=\alpha\) and rejects the hypothesis that \(F\) is the distribution of \(\{X_{i}\}_{i\geq 1}\) if \(KS(F_{n})>C_{0}\) and accepts it, otherwise.

### Problems

11.1 Show that the triangular array \(\{X_{nj}:1\leq j\leq n\}_{n\geq 1}\), with \(X_{nj}\) as in (1.24), is a null array, i.e., satisfies (1.15) iff (1.22) holds.
11.2 Construct an example of a triangular array \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) of independent random variables such that for any \(1\leq j\leq r_{n}\), \(n\geq 1\), \(E|X_{nj}|^{\alpha}=\infty\) for all \(\alpha\in(0,\infty)\), but there exist sequences \(\{a_{n}\}_{n\geq 1}\subset(0,\infty)\) and \(\{b_{n}\}_{n\geq 1}\subset\mathbb{R}\) such that \[\frac{S_{n}-b_{n}}{a_{n}}\longrightarrow^{d}N(0,1).\]
11.3 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables with \[P(X_{n}=\pm 1)=\frac{1}{2}-\frac{1}{2\sqrt{n}},\ P(X_{n}=\pm n^{2})=\frac{1}{2 \sqrt{n}},\ n\geq 1.\] Find constants \(\{a_{n}\}_{n\geq 1}\subset(0,\infty)\) and \(\{b_{n}\}_{n\geq 1}\subset\mathbb{R}\) such that \[\frac{\sum_{j=1}^{n}X_{j}-b_{n}}{a_{n}}\longrightarrow^{d}N(0,1).\]
11.4 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables such that for some \(\alpha\geq\frac{1}{2}\), \[P(X_{n}=\pm n^{\alpha})=\frac{n^{1-2\alpha}}{2}\quad\mbox{and}\quad P(X_{n}=0) =1-n^{1-2\alpha},\ n\geq 1.\] Let \(S_{n}=\sum_{j=1}^{n}X_{j}\) and \(s_{n}^{2}=\mbox{Var}(S_{n})\). 1. Show that for all \(\alpha\in[\frac{1}{2},1)\), \[\frac{S_{n}}{s_{n}}\longrightarrow^{d}N(0,1).\] 2. Show that (5.1) fails for \(\alpha\in[1,\infty)\). 3. Show that for \(\alpha>1\), \(\{S_{n}\}_{n\geq 1}\) converges to a random variable \(S\) w.p. 1 and that \(s_{n}\to\infty\).

11.5 Let \(\{X_{nj}:1\leq j\leq r_{n}\}_{n\geq 1}\) be a triangular array of independent zero mean random variables satisfying the Lindeberg condition. Show that \[E\bigg{[}\max_{1\leq j\leq r_{n}}\frac{X_{nj}^{2}}{s_{n}^{2}}\bigg{]}\to 0 \quad\mbox{as}\quad n\to\infty,\] where \(s_{n}^{2}=\sum_{j=1}^{r_{n}}\mbox{Var}(X_{nj})\), \(n\geq 1\).
11.6 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables. Let \(S_{n}=\sum_{j=1}^{n}X_{j}\) and \(s_{n}^{2}=\sum_{j=1}^{n}EX_{j}^{2}<\infty\), \(n\geq 1\). If \(s_{n}^{2}\to\infty\) then show that \[\lim_{n\to\infty}s_{n}^{-2}\sum_{j=1}^{n}EX_{j}^{2}I(|X_{j}|> \epsilon s_{n})=0\quad\mbox{for all}\quad\epsilon>0\] \[\Longleftrightarrow\quad\lim_{n\to\infty}s_{n}^{-2}\sum_{j=1}^{n}EX _{j}^{2}I(|X_{j}|>\epsilon s_{j})=0\quad\mbox{for all}\quad\epsilon>0.\] (**Hint:** Verify that for any \(\delta>0\), \(\sum_{j:s_{j}<\delta s_{n}}EX_{j}^{2}\leq\delta^{2}s_{n}^{2}\).)
11.7 For a sequence of random variables \(\{X_{n}\}_{n\geq 1}\) and for a real number \(r>2\), show that \[\lim_{n\to\infty}s_{n}^{-r}\sum_{j=1}^{n}E|X_{j}|^{r}I(|X_{j}|> \epsilon s_{n})=0\quad\mbox{for all}\quad\epsilon\in(0,\infty),\] \[\Longleftrightarrow\quad\lim_{n\to\infty}s_{n}^{-r}\sum_{j=1}^{n}E|X_{j}|^{r}=0,\] (5.2) where \(s_{n}^{2}=\sum_{j=1}^{n}EX_{j}^{2}\).
11.8 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of zero mean independent random variables satisfying (5.2) for \(r=4\). 1. Show that \[\lim_{n\to\infty}E(s_{n}^{-1}S_{n})^{k}=EZ^{k}\] for all \(k=2,3,4\), where \(Z\sim N(0,1)\). 2. Show that \(\big{\{}\big{(}\frac{S_{n}}{s_{n}}\big{)}^{4}\big{\}}_{n\geq 1}\) is uniformly integrable. 3. Show that \(\lim_{n\to\infty}Eh\big{(}\frac{S_{n}}{s_{n}}\big{)}=Eh(Z)\) where \(h(\cdot):\mathbb{R}\to\mathbb{R}\) is continuous and \(h(x)=O(|x|^{4})\) as \(|x|\to\infty\).
11.9 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of independent random variables such that \[P(X_{n}=\pm 1)=\frac{1}{4},\ P(X_{n}=\pm n)=\frac{1}{4n^{2}}\] and \[P(X_{n}=0)=\frac{1}{2}(1-\frac{1}{n^{2}}),\ n\geq 1.\]1. Show that the triangular array \(\{X_{nj}:1\leq j\leq n\}_{n\geq 1}\) with \(X_{nj}=X_{j}/\sqrt{n}\), \(1\leq j\leq n\), \(n\geq 1\) does not satisfy the Lindeberg condition. 2. Show that there exists \(\sigma\in(0,\infty)\) such that \[\frac{S_{n}}{\sqrt{n}}\longrightarrow^{d}N(0,\sigma^{2}).\] Find \(\sigma^{2}\).
11. Let \(\{X_{j}\}_{j\geq 1}\) be independent random variables such that \(X_{j}\) has Uniform \([-j,j]\) distribution. Show that the Lindeberg-Feller condition holds for the triangular array \(X_{nj}=X_{j}/n^{3/2}\), \(1\leq j\leq n\), \(n\geq 1\).
11.11 (_CLT for random sums_). Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with \(EX_{1}=0\), \(EX_{1}^{2}=1\). Let \(\{N_{n}\}_{n\geq 1}\) be a sequence of positive integer valued random variables such that \(\frac{N_{n}}{n}\longrightarrow^{p}c\), \(0<c<\infty\). Show that \(\frac{S_{N_{n}}}{\sqrt{N_{n}}}\longrightarrow^{d}N(0,1)\). (**Hint:** Use Kolmogorov's first inequality (cf. 8.3.1) to show that \(P\Big{(}\frac{|S_{N_{n}}-S_{nc}|}{\sqrt{n}}>\lambda,|N_{n}-nc|<n\epsilon\Big{)} \leq\frac{\epsilon}{\lambda^{2}}\) for any \(\epsilon>0\), \(\lambda>0\).)
11.12 Let \(\{N(t):t\leq 0\}\) be the renewal process as defined in (5.1) of Section 8.5. Assume \(EX_{1}=\mu\in(0,\infty)\), \(EX_{1}^{2}<\infty\). Show that \[\frac{N(t)-t/\mu}{\sqrt{t}}\longrightarrow^{d}N(0,\sigma^{2})\] (5.3) for some \(0<\sigma^{2}<\infty\). Find \(\sigma^{2}\). (**Hint:** Use \[\frac{S_{N(t)}-N(t)\mu}{\sqrt{N(t)}}\leq\frac{t-N(t)\mu}{\sqrt{N(t)}}\leq\frac {S_{N(t)+1}-(N(t)+1)\mu}{\sqrt{N(t)}}+\frac{\mu}{\sqrt{N(t)}}\] and the fact that \(\frac{N(t)}{t}\rightarrow\frac{1}{\mu}\) w.p. 1.)
11.13 Let \(\{N(t):t\geq 0\}\) be as in the above problem. Give another proof of (5.3) by using the CLT for \(\{S_{n}\}_{n\geq 1}\) and the relation \(P(N(t)>n)=P(S_{n}<t)\) for all \(t,n\).
11.14 Let \(\{X_{j}\}_{j\geq 1}\) be iid random variables with distribution \(P(X_{1}=1)=1/2=P(X_{1}=-1)\). Show that there exist positive integer valued random variables \(\{r_{k}\}_{k\geq 1}\) such that \(r_{k}\rightarrow\infty\) w.p. 1, but \(\frac{S_{r_{k}}}{\sqrt{r_{k}}}\) does not converge in distribution. (**Hint:** Let \(r_{1}=\min\{n:\frac{S_{n}}{\sqrt{n}}>1\}\) and for \(k\geq 1\), define recursively \(r_{k+1}=\min\{n:n>r_{k},\frac{S_{n}}{\sqrt{n}}>k+1\}\).)11.15 (_CLT for sample quantiles_). Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables. Let \(0<p<1\) and let \(Y_{n}\equiv F_{n}^{-1}(p)=\inf\{x:F_{n}(x)\geq p\},\) where \(F_{n}(x)\equiv\frac{1}{n}\sum_{i=1}^{n}I(X_{i}\leq x)\) is the empirical cdf based on \(X_{1},X_{2},\ldots,X_{n}.\) Assume that the cdf \(F(x)\equiv P(X_{1}\leq x)\) is differentiable at \(F^{-1}(p)\equiv\inf\{x:F(x)\geq p\}\) and that \(\lambda_{p}\equiv F^{\prime}\bigl{(}F^{-1}(p)\bigr{)}>0.\) Then show that \(\sqrt{n}\bigl{(}Y_{n}-F^{-1}(p)\bigr{)}\longrightarrow^{d}N(0,\sigma^{2}),\) where \(\sigma^{2}=p(1-p)/\lambda_{p}^{2}.\) (**Hint:** Use the identity \(P(Y_{n}\leq x)=P(F_{n}(x)\geq p)\) for all \(x\) and \(p.\))
11.16 (_A coupon collector's problem_). For each \(n\in\mathbb{N},\) let \(\{X_{ni}\}_{i\geq 1}\) be iid random variables such that \(P(X_{n1}=j)=\frac{1}{n},\)\(1\leq j\leq n.\) Let \(T_{n0}=1,\)\(T_{n1}=\min\{j:j>1,X_{j}\neq X_{1}\},\) and \(T_{n(i+1)}=\min\big{\{}j:j>T_{ni},\;X_{j}\notin\{X_{T_{nk}}:0\leq k\leq i\}\big{\}}.\) That is, \(T_{ni}\) is the first time the sample has \((i+1)\) distinct elements. Suppose \(k_{n}\uparrow\infty\) such that \(\frac{k_{n}}{n}\to\theta,\)\(0<\theta<1.\) Show that for some \(a_{n},\)\(b_{n}\) \[\frac{T_{n,k_{n}}-a_{n}}{b_{n}}\longrightarrow^{d}N(0,1).\] (**Hint:** Let \(Y_{nj}=T_{nj}-T_{n(j-1)},\)\(j=1,2,\ldots,(n-1).\) Show that for each \(n,\)\(\{Y_{nj}\}_{j=1,2,\ldots}\) are independent with \(Y_{nj}\) having a geometric distribution with parameter \(\bigl{(}1-\frac{j}{n}\bigr{)}.\) Now apply Lyapounov's criterion to the triangular array \(\{Y_{nj}:1\leq j\leq k_{n}\}.\))
11.17 Prove Theorem 11.1.6.
11.18 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{n}=0\) and \(EX_{n}^{2}=\sigma^{2}\in(0,\infty).\) Let \(S_{n}=\sum_{j=1}^{n}X_{j},\)\(n\geq 1.\) For each \(k\in\mathbb{N},\) find the limit distribution of the \(k\)-dimensional vector(s). 1. \(\Bigl{(}\frac{S_{n}}{\sqrt{n}},\frac{S_{2n}-S_{n}}{\sqrt{n}},\ldots,\frac{S_{nk }-S_{n(k-1)}}{\sqrt{n}}\Bigr{)},\) 2. \(\Bigl{(}\frac{S_{n_{1}}}{\sqrt{n}},\frac{S_{n_{2}}}{\sqrt{n}},\ldots,\frac{S_{ n_{nk}}}{\sqrt{n}}\Bigr{)},\) where \(0<a_{1}<a_{2}<\cdots<a_{k}<\infty\) are given real numbers, 3. \(\Bigl{(}\frac{S_{2n}}{\sqrt{n}},\frac{S_{3n}-S_{n}}{\sqrt{n}},\ldots,\frac{S_{ (k+1)n}-S_{(k-1)n}}{\sqrt{n}}\Bigr{)}.\)
11.19 For any random variable \(X\), show that \(EX^{2}<\infty\) implies \[\frac{y^{2}P(|x|>y)}{E(X^{2}I(|x|\leq y))}\to 0\] as \(y\to\infty.\) Give an example to show that the converse is false. (**Hint:** Consider a random variable \(X\) with pdf \(f(x)=c_{1}|x|^{-3}\) for \(|x|>2.\))11. Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with common distribution \[P(X_{1}\in A)=\int_{A}|x|^{-3}I(|x|>1)dx,\ A\in{\cal B}({\mathbb{R}}).\] Find sequences \(\{a_{n}\}_{n\geq 1}\subset(0,\infty)\) and \(\{b_{n}\}_{n\geq 1}\subset{\mathbb{R}}\) such that \[\frac{S_{n}-b_{n}}{a_{n}}\longrightarrow^{d}N(0,1)\] where \(S_{n}=\sum_{j=1}^{n}X_{j}\), \(n\geq 1\).
11. Show using characteristic functions that if \(X_{1},X_{2},\ldots,X_{k}\) are iid Cauchy \((\mu,\sigma^{2})\) random variables, then \(S_{k}\equiv\sum_{i=1}^{k}X_{i}\) has a Cauchy \((k\mu,k\sigma)\) distribution.
11. Show that if a random variable \(Y_{1}\) has pdf \(f\) as in (2.4), then (2.9) holds with \(\alpha=1/2\).
11. If \(\{Y_{n}\}_{n\geq 1}\) are iid Gamma (1,2), then \(n^{-1}\sum_{i=1}^{n}Y_{i}^{-1}\longrightarrow^{d}W\), where \(W\) has pdf \(f_{W}(w)\equiv\left(\frac{2}{\pi}\frac{1}{1+w^{2}}\right)\cdot I_{(0,\infty)}(w)\).
11. Let \(X\) be a nonnegative random variable such that \(P(X\leq x)\sim x^{\alpha}L(x)\) as \(x\downarrow 0\) for some \(\alpha>0\) and \(L(\cdot)\) slowly varying at \(0\). Let \(Y=X^{-\beta}\), \(\beta>0\). Show that \[P(Y>y)\sim y^{-\gamma}\tilde{L}(y)\quad\mbox{as}\quad y\uparrow\infty\] for some \(\gamma>0\) and \(\tilde{L}(\cdot)\) slowly varying at \(\infty\).
11. Let \(\{X_{i}\}_{i\geq 1}\) be iid Beta \((m,n)\) random variables. Let \(Y_{i}=X_{i}^{-\beta}\), \(\beta>0\), \(i\geq 1\). Show that there exist sequences \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that \(\frac{\sum_{i=1}^{n}Y_{i}-a_{n}}{b_{n}}\longrightarrow^{d}\) a stable law of order \(\gamma\) for some \(\gamma\) in \((0,2]\).
11. Let \(\{X_{i}\}_{i\geq 1}\) be iid Uniform \([0,1]\) random variables. 1. Show that for each \(0<\beta<\frac{1}{2}\), there exist constants \(\mu\) and \(\sigma^{2}\) such that \[\frac{1}{\sigma\sqrt{n}}\bigg{(}\sum_{i=1}^{n}X_{i}^{-\beta}-n\mu\bigg{)} \longrightarrow^{d}N(0,1).\] 2. Show that for each \(\frac{1}{2}<\beta<1\), there exist a constant \(0<\gamma<2\) and sequences \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that \[\frac{1}{b_{n}}\bigg{(}\sum_{i=1}^{n}X_{i}^{-\beta}-a_{n}\bigg{)} \longrightarrow^{d}\quad\mbox{a stable law of order}\quad\gamma.\]11.27 Prove (4.7). (**Hint:** Use (4.5) and Lemma 10.1.5.) 11.28 1. Show that the Gamma \((\alpha,\beta)\) distribution is infinitely divisible, \(0<\alpha\), \(\beta<\infty\). 2. Let \(\mu\) be a finite measure on \(\bigl{(}\mathbb{R},\beta(\mathbb{R})\bigr{)}\). Show that \(\phi(t)\equiv\exp\big{\{}\int(e^{\iota tu}-1)\mu(du)\big{\}}\) is the characteristic function of an infinitely divisible distribution.
11.29 Let \(\{X_{n}\}_{n\geq 1}\) be iid random variables with \(P(X_{1}=0)=P(X_{1}=1)=\frac{1}{2}\). Show that there exists a constant \(C_{1}\in(0,\infty)\) such that \[\Delta_{n}\geq C_{1}n^{-1/2}\quad\text{for all}\quad n\geq 1,\] where \(\Delta_{n}\) is as in (4.1).
11.30 Let \(X_{1}\) be a random variable such that the absolutely continuous component \(\beta F_{ac}(\cdot)\) in the decomposition (4.5.3) of the cdf \(F\) of \(X_{1}\) is nonzero. Show that \(X_{1}\) satisfies Cramer's condition (4.13). (**Hint:** Use the Riemann-Lebesgue lemma.) 11.31 (_Berry-Esseen theorem for sample quantile_). Let \(\{X_{n}\}_{n\geq 1}\) be a collection of iid random variables with cdf \(F(\cdot)\). Let \(0<p<1\) and \(Y_{n}=F_{n}^{-1}(p)\), where \(F_{n}(x)=n^{-1}\sum_{i=1}^{n}I(X_{1}\leq x)\), \(x\in\mathbb{R}\). Suppose that \(F(\cdot)\) is twice differentiable in a neighborhood of \(\xi_{p}\equiv F^{-1}(p)\) and \(F^{\prime}(\xi_{p})\in(0,\infty)\). Show that \[\sup_{x\in\mathbb{R}}\Big{|}P\Bigl{(}\sqrt{n}(Y_{n}-\xi_{p})/\sigma_{p}\leq x \Bigr{)}-\Phi(x)\Big{|}=O(n^{-1/2})\quad\text{as}\quad n\to\infty\] where \(\sigma_{p}=p(1-p)/\bigl{(}F^{\prime}(\xi_{p})\bigr{)}^{2}\). (**Hint:** Use the identity \(P(Y_{n}\leq x)=P(F_{n}(x)\geq p)\) for all \(x\) and \(p\), apply Theorem 11.4.1 to \(F_{n}(x)\) for \(\sqrt{n}|x-\xi_{p}|\leq\log n\), and use monotonicity of cdfs for \(\sqrt{n}|x-\xi_{p}|>\log n\). See Lahiri (1992) for more details. Also, see Reiss (1974) for a different proof.) 11.32 (_A moderate deviation bound_). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=\mu\), \(\operatorname{Var}(X)=\sigma^{2}\in(0,\infty)\) and \(E|X_{1}|^{3}<\infty\). Show that \[P\Bigl{(}\sqrt{n}\big{|}\bar{X}_{n}-\mu\big{|}>\sigma\sqrt{\log n}\Bigr{)}=O(n ^{-1/2})\quad\text{as}\quad n\to\infty.\] (**Hint:** Apply Theorem 11.4.1.) _It can be shown that the bound on the right side is indeed \(o(n^{-1/2})\) as \(n\to\infty\). For a more general version of this result, see Gotze and Hipp (1978)._11.33 Show that the values of the functions \(e_{n,2}(x)\) of (4.14) and \(\tilde{e}_{n,2}(x)\) of (4.24) are not necessarily nonnegative for all \(x\in\mathbb{R}\).
11.34 Complete the proof of Lemma 11.4.7 (i).

(**Hint:** Suppose that for some \(r\in\mathbb{N}\), \(\phi\) is \(r\)-times differentiable with its \(r\)th derivative given by (4.30). Then, for \(t\in(0,\infty)\),

\[h^{-1}\big{[}\phi^{(r)}(t+h)-\phi^{(r)}(t)\big{]}=\int_{-\infty}^{\infty}\frac{ e^{hx}-1}{h}\cdot x^{r}e^{tx}F(dx)\]

and the integrand is bounded by the integrable function \(|x|^{r}g(x)\) for all \(x\in\mathbb{R}\), \(0<|h|<t/2\), where \(g(\cdot)\) is as in (4.32). Now apply the DCT.)
11.35 Under the conditions of Lemma 11.4.7, show that

\[\lim_{t\to\infty}\frac{\phi^{(1)}(t)}{\phi(t)}=\theta.\]

(**Hint:** Consider the cases '\(\theta\in\mathbb{R}\)' and '\(\theta=\infty\)' separately.)
11.36 Find the function \(\gamma(x)\) of (4.28) in each of the following cases:

1. \(X_{1}\sim N(\mu,\sigma^{2})\),
2. \(X_{1}\sim\) Gamma \((\alpha,\beta)\),
3. \(X_{1}\sim\) Uniform \((0,1)\).
11.37 Verify that the functions \(T_{i}\), \(i=1,2,3\) defined by (4.42) are continuous on \(\mathcal{C}[0,1]\).

Conditional Expectation and Conditional Probability

### 12.1 Conditional expectation: Definitions and examples

This section motivates the definition of conditional expectation for random variables with a finite variance through a mean square error prediction problem. The definition is then extended to integrable random variables by an approximation argument (cf. Definition 12.1.3). The more standard approach of proving the existence of conditional expectation by the use of Radon-Nikodym theorem is also outlined.

Let \((X,Y)\) be a bivariate random vector. A standard problem in regression analysis is to predict \(Y\) having observed \(X\). That is, to find a function \(h(X)\) that predicts \(Y\). A common criterion for measuring the accuracy of such a predictor is the mean squared error \(E(Y-h(X))^{2}\). Under the assumption that \(E|Y|^{2}<\infty\), it can be shown that there exists a unique \(h_{0}(X)\) that minimizes the mean squared error.

**Theorem 12.1.1:**_Let \((X,Y)\) be a bivariate random vector. Let \(EY^{2}<\infty\). Then there exists a Borel measurable function \(h_{0}:\mathbb{R}\rightarrow\mathbb{R}\) with \(E\big{(}h_{0}(X)\big{)}^{2}<\infty\), such that_

\[E\big{(}Y-h_{0}(X)\big{)}^{2}=\inf\big{\{}E\big{(}Y-h(X)\big{)}^{2}:h(X)\in \mathbb{H}_{0}\big{\}}, \tag{1.1}\]

_where \(\mathbb{H}_{0}=\big{\{}h(X)\mid h:\mathbb{R}\rightarrow\mathbb{R}\) is Borel measurable and \(E(h(X))^{2}<\infty\big{\}}\)._

**Proof:** Let \(\mathbb{H}\) be the space of all Borel measurable functions of \((X,Y)\) that have a finite second moment. Let \(\mathbb{H}_{0}\) be the subspace of all Borel measurable functions of \(X\) that have a finite second moment. It is known that \(\mathbb{H}_{0}\) is a closed subspace of \(\mathbb{H}\) (Problem 12.1) and that for any \(Z\) in \(\mathbb{H}\), there exists a unique \(Z_{0}\) in \(\mathbb{H}_{0}\) such that

\[E(Z-Z_{0})^{2}=\min\big{\{}E(Z-Z_{1})^{2}:Z_{1}\in\mathbb{H}_{0}\big{\}}.\]

Further, \(Z_{0}\) is the unique random variable (up to equivalence w.p. 1) such that

\[E(Z-Z_{0})Z_{1}=0\quad\text{for all}\quad Z_{1}\in\mathbb{H}_{0}. \tag{1.2}\]

A proof of this fact is given at the end of this section in Theorem 12.1.6. If one takes \(Z\) to be \(Y\), then this \(Z_{0}\) is the desired \(h_{0}(X)\). \(\Box\)

**Remark 12.1.1:** The random variable \(Z_{0}\) in (1.2) is known as the _projection of Y onto \(\mathbb{H}_{0}\)_.

It is known that for any random variable \(Y\) with \(EY^{2}<\infty\), the constant \(c\) that minimizes \(E(Y-c)^{2}\) over all \(c\in\mathbb{R}\) is \(c=EY\), the expected value of \(Y\). By analogy with this, one is led to the following definition.

**Definition 12.1.1:** For any bivariate random vector \((X,Y)\) with \(EY^{2}<\infty\), the _conditional expectation of Y given X_, denoted as \(E(Y|X)\), is the function \(h_{0}(X)\) of Theorem 12.1.1. Note that \(h_{0}(X)\) is determined up to equivalence w.p. 1. Any such \(h_{0}(X)\) is called a _version_ of \(E(Y|X)\).

From (1.2) in the proof of Theorem 12.1.1, by taking \(Z=Y\), \(Z_{1}=I_{B}(X)\), one finds that \(Z_{0}=h_{0}(X)\) satisfies

\[EYI_{A}=Eh_{0}(X)I_{A} \tag{1.3}\]

for every event \(A\) of the form \(X^{-1}(B)\) where \(B\in\mathcal{B}(\mathbb{R})\). Conversely, it can be shown that (1.3) implies (1.2), by the usual approximation procedure (Problem 12.1). From (1.3), the function \(h_{0}(X)\) is determined w.p. 1. So one can take (1.3) to be the definition of \(h_{0}(X)\). In statistics, the function \(E(Y|X)\) is called the _regression of \(Y\) on \(X\)_.

The function \(h_{0}(x)\) can be determined explicitly in the following two special cases.

If \(X\) is a _discrete_ random variable with values \(x_{1},x_{2},\ldots\), then (1.3) implies, by taking \(A=\{X=x_{i}\}\), that

\[h_{0}(x_{i})=\frac{E\big{(}YI(X=x_{i})\big{)}}{P(X=x_{i})},\ i=1,2,\ldots. \tag{1.4}\]

Similarly, if \((X,Y)\) has an absolutely continuous distribution with joint probability density \(f(x,y)\), it can be shown that w.p. 1, \(E(Y|X)=h_{0}(X)\), where

\[h_{0}(x)=\left(\frac{\int yf(x,y)dy}{f_{X}(x)}\right) \tag{1.5}\]if \(f_{X}(x)>0\) and \(0\) otherwise, where \(f_{X}(x)=\int f(x,y)dy\) is the probability density function of \(X\) (Problem 12.2).

The definition of \(E(Y|X)\) can be generalized to the case when \(X\) is a vector and more generally, as follows.

Theorem 12.1.2: Let \((\Omega,{\cal F},P)\) be a probability space and \({\cal G}\subset{\cal F}\) be a \(\sigma\)-algebra. Let \(\mathbb{H}\equiv L^{2}(\Omega,{\cal F},P)\) and \(\mathbb{H}_{0}=L^{2}(\Omega,{\cal G},P)\). Then for any \(Y\in\mathbb{H}\), there exist a \(Z_{0}\in\mathbb{H}_{0}\) such that

\[E(Y-Z_{0})^{2}=\inf\{E(Y-Z)^{2}:Z\in\mathbb{H}_{0}\}\]

and this \(Z_{0}\) is determined w.p. 1 by the condition

\[E(YI_{A})=E(Z_{0}I_{A})\quad\mbox{for all}\quad A\in{\cal G}.\]

The proof is similar to that of Theorem 12.1.1.

Definition Definition 12.1.2: The random variable \(Z_{0}\) in (1.7) is called the conditional expectation of \(Y\) given \({\cal G}\) and is written as \(E(Y|{\cal G})\).

When \({\cal G}=\sigma\langle X\rangle\), the \(\sigma\)-algebra generated by a random variable \(X\), \(E(Y|{\cal G})\) reduces to \(E(Y|X)\) in Definition 12.1.1. The following properties of \(E(Y|{\cal G})\) are easy to verify by using the defining equation (1.7) (Problem 12.3).

Proposition 12.1.3: Let \(Y\) and \({\cal G}\) be as in Theorem 12.1.2.

* \(Y\geq 0\) w.p. 1 \(\Rightarrow E(Y|{\cal G})\geq 0\) w.p. 1
* \(Y_{1},Y_{2}\in\mathbb{H}\Rightarrow E\bigl{(}(\alpha Y_{1}+\beta Y_{2})|{\cal G }\bigr{)}=\alpha E(Y_{1}|{\cal G})+\beta E(Y_{2}|{\cal G})\) for any \(\alpha,\beta\in\mathbb{R}\).
* \(Y_{1}\geq Y_{2}\) w.p. 1 \(\Rightarrow E(Y_{1}|{\cal G})\geq E(Y_{2}|{\cal G})\) w.p. 1.

Using a natural approximation procedure it is possible to extend the definition of \(E(Y|{\cal G})\) to all random variables with just the first moment, i.e., \(E|Y|<\infty\). This is done in the following result.

Theorem 12.1.4: Let \((\Omega,{\cal F},P)\) be a probability space and \({\cal G}\subset{\cal F}\) be a sub-\(\sigma\)-algebra. Let \(Y:\Omega\to\mathbb{R}\) be a \({\cal F}\)-measurable random variable with \(E|Y|<\infty\). Then there exists a random variable \(Z_{0}:\Omega\to\mathbb{R}\) that is \({\cal G}\)-measurable, \(E|Z_{0}|<\infty\), and is uniquely determined (up to equivalence w.p. 1) by

\[E(YI_{A})=E(Z_{0}I_{A})\quad\mbox{for all}\quad A\in{\cal G}.\]

**Proof:** Since \(Y\) can be written as \(Y=Y^{+}-Y^{-}\), it is enough to consider the case \(Y\geq 0\) w.p. 1. Let \(Y_{n}=\min\{Y,n\}\) for \(n=1,2,\ldots\). Then \(EY_{n}^{2}<\infty\)

[MISSING_PAGE_FAIL:396]

* By taking \(A\) to be \(\Omega\) in (1.8), \[EY=E\big{(}E(Y|{\cal G})\big{)}.\] Furthermore, Proposition 12.1.3 extends to this case. When \({\cal G}=\sigma\langle X\rangle\) with \(X\) discrete, (1.4) holds provided \(E|Y|<\infty\). Part (iv) is useful in computing \(EY\) without explicitly determining the distribution of \(Y\). Suppose \(E(Y|X)=m(X)\) and \(Em(X)\) is easy to compute but finding the distribution of \(Y\) is not so easy. Then \(EY\) can still be computed as \(Em(X)\). For example, let \((X,Y)\) have a bivariate distribution with pdf \[f_{X,Y}(x,y)=\left\{\begin{array}{ccc}\frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{2 \pi}|x|}\,e^{-\frac{(y-x)^{2}}{2x^{2}}}\,e^{-\frac{(x-1)^{2}}{2}}&\mbox{if}&x \neq 0,\\ 0&\mbox{if}&x=0,\end{array}\right.\] \(x,y\in\mathbb{R}^{2}\). In this case, evaluating \(f_{Y}(y)\) is not easy. On the other hand, it can be verified that for each \(x\), \[m(x)\equiv\int y\frac{f_{X,Y}(x,y)dy}{f_{X}(x)}=x,\] and that \(f_{X}(x)=\frac{1}{\sqrt{2\pi}}\,e^{-\frac{(x-1)^{2}}{2}}\). Thus, \(EY=EX=1\). For more examples of this type, see Problem 12.29. The next proposition lists some useful properties of the conditional expectation. Proposition Proposition 12.1.5: _Let \((\Omega,{\cal F},P)\) be a probability space and let \(Y\) be a \({\cal F}\)-measurable random variable with \(E|Y|<\infty\). Let \({\cal G}_{1}\subset{\cal G}_{2}\subset{\cal F}\) be two sub-\(\sigma\)-algebras contained in \({\cal F}\)._

* _Then_ \[E(Y|{\cal G}_{1})=E\big{(}E(Y|{\cal G}_{2})|{\cal G}_{1}\big{)}.\] (1.10)
* _For any bounded_ \({\cal G}_{1}\)_-measurable random variable_ \(U\)_,_ \[E(YU|{\cal G}_{1})=UE(Y|{\cal G}_{1}).\] (1.11)

**Proof:** (i) Let \(A\in{\cal G}_{1},Z_{1}=E(Y|{\cal G}_{1})\), and \(Z_{2}=E(Y|{\cal G}_{2})\). Then \(E(YI_{A})=E(Z_{1}I_{A})\) by the definition of \(Z_{1}\). Since \({\cal G}_{1}\subset{\cal G}_{2}\), \(A\in{\cal G}_{2}\) and again by the definition of \(Z_{2}\),

\[E(YI_{A})=E(Z_{2}I_{A}).\]

Thus,

\[E(Z_{2}I_{A})=E(Z_{1}I_{A})\quad\mbox{for all}\quad A\in{\cal G}_{1}\]

and by the definition of \(E(Z_{2}|{\cal G}_{1})\), it follows that \(Z_{1}=E(Z_{2}|{\cal G}_{1})\), proving (i).

(ii) Let \(Z_{1}=E(Y|{\cal G}_{1})\). If \(U=I_{B}\) some \(B\in{\cal G}_{1}\), then for any \(A\in{\cal G}_{1}\), \(A\cap B\in{\cal G}_{1}\) and by (1.8),

\[EYI_{B}I_{A}=EYI_{A\cap B}=E(Z_{1}I_{A\cap B})=E(Z_{1}I_{B}\cdot I_{A}).\]

So in this case \(E(YU|{\cal G}_{1})=Z_{1}U\). By linearity (Proposition 12.1.3 (ii)), it extends to all \(U\) that are simple and \({\cal G}_{1}\)-measurable. For any bounded \({\cal G}_{1}\)-measurable \(U\), there exists a sequence of bounded, \({\cal G}_{1}\)-measurable, and simple random variables \(\{U_{n}\}_{n\geq 1}\) that converge to \(U\) uniformly. Hence, for any \(A\in{\cal G}_{1}\) and for \(n\geq 1\),

\[EYU_{n}I_{A}=EZ_{1}U_{n}I_{A}.\]

The bounded convergence theorem applied to both sides yields

\[EYUI_{A}=EZ_{1}UI_{A}.\]

Since \(Z_{1}\) and \(U\) are both \({\cal G}_{1}\)-measurable, (ii) follows. \(\Box\)

**Remark 12.1.4:** If the random variable \(U\) in Proposition 12.1.5 is \({\cal G}_{1}\)-measurable and \(E|YU|<\infty\), then part (ii) of the proposition holds. The proof needs a more careful approximation (see Billingsley (1995), pp. 447).

**An Approximation Theorem**

**Theorem 12.1.6:** _Let \({\mathbb{H}}\) be a real Hilbert space and \({\mathbb{M}}\) be a nonempty closed convex subset of \({\mathbb{H}}\). Then for every \(v\in{\mathbb{H}}\), there is a unique \(u_{0}\in{\mathbb{M}}\) such that_

\[\|v-u_{0}\|=\inf\{\|v-u\|:u\in{\mathbb{M}}\} \tag{1.12}\]

_where \(\|x\|^{2}=\langle x,x\rangle\), with \(\langle x,y\rangle\) denoting the inner-product in \({\mathbb{H}}\)._

**Proof:** Let \(\delta=\inf\{\|v-u\|:u\in{\mathbb{M}}\}\). Then, \(\delta\in[0,\infty)\). By definition, there exists a sequence \(\{u_{n}\}_{n\geq 1}\subset{\mathbb{M}}\) such that

\[\|v-u_{n}\|\to\delta.\]

Also note that in any inner-product space, the parallelogram law holds, i.e., for any \(x,y\in{\mathbb{H}}\),

\[\|x+y\|^{2}+\|x-y\|^{2}=2(\|x\|^{2}+\|y\|^{2}).\]

Thus

\[\|2v-(u_{n}+u_{m})\|^{2}+\|u_{n}-u_{m}\|^{2} \tag{1.13}\] \[= 2(\|v-u_{n}\|^{2}+\|v-u_{m}\|^{2}).\]

Since \({\mathbb{M}}\) is convex, \(\frac{u_{n}+u_{m}}{2}\in{\mathbb{M}}\) implying that \(\left\|v-\frac{u_{n}+u_{m}}{2}\right\|^{2}\geq\delta^{2}\). This, with (1.13), implies that

\[\limsup_{m,n\to\infty}\|u_{n}-u_{m}\|^{2}=0,\]making \(\{u_{n}\}_{n\geq 1}\) a Cauchy sequence. Since \(\mathbb{H}\) is a Hilbert space, there exists a \(u_{0}\in\mathbb{H}\) such that \(\{u_{n}\}_{n\geq 1}\) converges to \(u_{0}\). Also, since \(\mathbb{M}\) is closed, \(u_{0}\in\mathbb{M}\). Since \(\|v-u_{n}\|\to\delta\), it follows that \(\|v-u_{0}\|=\delta\).

To show the uniqueness, let \(u^{\prime}_{0}\in\mathbb{M}\) also satisfies \(\|v-u^{\prime}_{0}\|=\delta\). Then as in (1.13),

\[\left\|v-\frac{u_{0}+u^{\prime}_{0}}{2}\right\|^{2}+\left\|\frac{u_{0}-u^{ \prime}_{0}}{2}\right\|^{2}=\delta^{2},\]

implying \(\|u_{0}-u^{\prime}_{0}\|=0\). \(\Box\)

**Remark 12.1.5:** The above theorem holds if \(\mathbb{M}\) is a closed subspace of \(\mathbb{H}\).

### 12.2 Convergence theorems

From Proposition 12.1.3, it is seen that \(E(Y|{\cal G})\) is monotone and linear in \(Y\), suggesting that it behaves like an ordinary expectation. A natural question is whether under appropriate conditions, the basic convergence results extend to conditional expectations (CE). The answer is 'yes,' as shown by the following results.

**Theorem 12.2.1:** (_Monotone convergence theorem for CE_). _Let \((\Omega,{\cal F},P)\) be a probability space and \({\cal G}\subset{\cal F}\) be a sub-\(\sigma\)-algebra of \({\cal F}\). Let \(\{Y_{n}\}_{n\geq 1}\) be a sequence of nonnegative \({\cal F}\)-measurable random variables such that \(0\leq Y_{n}\leq Y_{n+1}\) w.p. 1. Let \(Y\equiv\lim\limits_{n\to\infty}Y_{n}\) w.p. 1. Then_

\[\lim\limits_{n\to\infty}E(Y_{n}|{\cal G})=E(Y|{\cal G})\ \ \mbox{w.p. 1.} \tag{2.1}\]

**Proof:** By Proposition 12.1.3 (i), \(Z_{n}\equiv E(Y_{n}|{\cal G})\) is monotone nondecreasing in \(n\), w.p. 1, and so \(Z\equiv\lim_{n\to\infty}Z_{n}\) exists w.p. 1. By the MCT, for all \(A\in{\cal G}\),

\[E(YI_{A})=\lim\limits_{n\to\infty}EY_{n}I_{A}=\lim\limits_{n\to\infty}E(Z_{n}I _{A})=E(ZI_{A}).\]

Thus, \(Z=E(Y|{\cal G})\) w.p. 1, proving (2.1). \(\Box\)

**Theorem 12.2.2:** (_Fatou's lemma for CE_). _Let \(\{Y_{n}\}_{n\geq 1}\) be a sequence of nonnegative random variables on a probability space \((\Omega,{\cal F},P)\) and let \({\cal G}\) be a sub-\(\sigma\)-algebra of \({\cal F}\). Then_

\[\liminf\limits_{n\to\infty}E(Y_{n}|{\cal G})\geq E(\liminf\limits_{n\to\infty} Y_{n}|{\cal G}). \tag{2.2}\]

**Proof:** Let \(\tilde{Y}_{n}=\inf_{j\geq n}Y_{j}\). Then \(\{\tilde{Y}_{n}\}_{n\geq 1}\) is a sequence of nonnegative nondecreasing random variables and \(\lim_{n\to\infty}\tilde{Y}_{n}=\liminf_{n\to\infty}Y_{n}\). By the previous theorem,

\[\lim\limits_{n\to\infty}E(\tilde{Y}_{n}|{\cal G})=E(\liminf\limits_{n\to\infty }Y_{n}|{\cal G}). \tag{2.3}\]

[MISSING_PAGE_FAIL:400]

12.2 Convergence theorems

_(i) If \(EY^{2}<\infty\), then \(E(Y^{2}|{\cal G})\geq\big{(}E(Y|{\cal G})\big{)}^{2}\)._ (ii) If \(E|Y|^{p}<\infty\) for some \(p\in[1,\infty)\), then \(E(|Y|^{p}|{\cal G})\geq|(EY|{\cal G})|^{p}\)._

**Definition 12.2.1:** Let \(EY^{2}<\infty\). The _conditional variance of \(Y\) given \({\cal G}\)_, denoted by \({\rm Var}(Y|{\cal G})\), is defined as

\[{\rm Var}(Y|{\cal G})=E(Y^{2}|{\cal G})-(E(Y|{\cal G}))^{2}. \tag{2.8}\]

This leads to the following formula for a decomposition of the variance of \(Y\), known as the _Analysis of Variance_ formula.

**Theorem 12.2.6:** Let \(EY^{2}<\infty\). Then

\[{\it Var}(Y)={\it Var}(E(Y|{\cal G}))+E\big{(}{\it Var}(Y|{\cal G})\big{)}. \tag{2.9}\]

**Proof:**\({\rm Var}(Y)=E(Y-EY)^{2}\). But \(Y-EY=Y-E(Y|{\cal G})+E(Y|{\cal G})-EY\). Also by (1.11),

\[E\big{(}\big{[}Y-E(Y|{\cal G})\big{]}\big{[}E(Y|{\cal G})-EY\big{]} \big{|}{\cal G}\big{)}\] \[= \big{[}E(Y|{\cal G})-EY\big{]}E\big{(}\big{[}Y-E(Y|{\cal G})\big{]} \big{|}{\cal G}\big{)}\] \[= \big{[}E(Y|{\cal G})-EY\big{]}0=0.\]

Thus, \(E\big{(}\big{[}Y-E(Y|{\cal G})\big{]}\big{[}E(Y|{\cal G})-EY\big{]}\big{)}=0\) and so

\[E(Y-EY)^{2}=E\big{(}Y-E(Y|{\cal G})\big{)}^{2}+E\big{(}E(Y|{\cal G})-EY\big{)}^ {2}. \tag{2.10}\]

Now, noting that \(E\big{[}YE(Y|{\cal G})\big{]}=E\big{[}E\big{\{}Y(EY|{\cal G})|{\cal G}\big{\}} \big{]}=E\big{[}E(Y|{\cal G})\big{]}^{2}\), one gets

\[E(Y-E(Y|{\cal G}))^{2} = EY^{2}-2E\big{[}YE(Y|{\cal G})\big{]}+E\big{(}E(Y|{\cal G})\big{)} ^{2}\] \[= EY^{2}-E\big{[}\big{(}E(Y|{\cal G})\big{)}^{2}\big{]}\] \[= E\big{[}E(Y^{2}|{\cal G})\big{]}-E\big{[}\big{(}E(Y|{\cal G}) \big{)}^{2}\big{]}\] \[= E\big{(}{\rm Var}(Y|{\cal G})\big{)}.\]

Also, since \(E\big{[}E(Y|{\cal G})\big{]}=EY\),

\[E\big{(}E(Y|{\cal G})-EY\big{)}^{2}={\rm Var}\big{(}E(Y|{\cal G})\big{)}.\]

Hence, (2.9) follows from (2.10). \(\Box\)

**Remark 12.2.1:**\(E\big{(}{\rm Var}(Y|{\cal G})\big{)}\) is called the _variance within_ and \({\rm Var}\big{(}E(Y|{\cal G})\big{)}\) is the _variance between_. The above proof also shows that

\[E(Y-Z)^{2}=E\big{(}Y-E(Y|{\cal G})\big{)}^{2}+E\big{(}E(Y|{\cal G})-Z\big{)}^ {2} \tag{2.11}\]

for any random variable \(Z\) that is \({\cal G}\)-measurable. This is used to prove the Rao-Blackwell theorem in mathematical statistics (Lehmann and Casella (1998)) (Problem 12.27).

### Conditional probability

Let \((\Omega,{\cal F},P)\) be a probability space and let \({\cal G}\subset{\cal F}\) be a sub-\(\sigma\)-algebra.

**Definition 12.3.1:** For \(B\in{\cal F}\), the _conditional probability_ of \(B\) given \({\cal G}\), denoted by \(P(B|{\cal G})\), is defined as

\[P(B|{\cal G})=E(I_{B}|{\cal G}). \tag{3.1}\]

Thus \(Z\equiv P(B|{\cal G})\) is a \({\cal G}\)-measurable function such that

\[P(A\cap B)=E(ZI_{A})\quad\mbox{for all}\quad A\in{\cal G}. \tag{3.2}\]

Since \(0\leq P(A\cap B)\leq P(A)\) for all \(A\in{\cal G}\), it follows that \(0\leq P(B|{\cal G})\leq 1\) w.p. 1. It is easy to check that w.p. 1

\[P(\Omega|{\cal G})=1\quad\mbox{and}\quad P(\emptyset|{\cal G})=0.\]

Also, by linearity, if \(B_{1},B_{2}\in{\cal F},B_{1}\cap B_{2}=\emptyset\), then

\[P(B_{1}\cup B_{2}|{\cal G})=P(B_{1}|{\cal G})+P(B_{2}|{\cal G})\quad\mbox{w.p. 1}.\]

This suggests that w.p. 1, \(P(B|{\cal G})\) is countably additive as a set function in \(B\). That is, there exists a set \(A_{0}\in{\cal G}\) such that \(P(A_{0})=0\) and for all \(\omega\not\in A_{0}\), the map \(B\to P(B|{\cal G})(\omega)\) is countably additive. However, this is not true. Although for a _given_ collection \(\{B_{n}\}_{n\geq 1}\) of disjoint sets in \({\cal F}\), there is an exceptional set \(A_{0}\) such that \(P(A_{0})=0\) and for \(\omega\not\in A_{0}\),

\[P(\bigcup_{n\geq 1}B_{n}|{\cal G})(\omega)=\sum_{n=1}^{\infty}P(B_{n}|{\cal G})( \omega).\]

However, this \(A_{0}\) depends on \(\{B_{n}\}_{n\geq 1}\) and as the collection varies, these exceptional sets can be an uncountable collection whose union may not be contained in a set of probability zero.

**Definition 12.3.2:** Let \((\Omega,{\cal F},P)\) be a probability space and \({\cal G}\) be a sub-\(\sigma\)-algebra of \({\cal F}\). A function \(\mu:{\cal F}\times\Omega\to[0,1]\) is called a _regular conditional probability_ on \({\cal F}\) given \({\cal G}\) if

* for all \(B\in{\cal F}\), \(\mu(B,\omega)=P(B|{\cal G})\) w.p. 1, and
* for all \(\omega\in\Omega\), \(\mu(B,\omega)\) is a probability measure on \((\Omega,{\cal F})\).

If a regular conditional probability (r.c.p.) \(\mu(\cdot,\cdot)\) exists on \({\cal F}\) given \({\cal G}\), then conditional expectation of \(Y\) given \({\cal G}\) can be computed as

\[E(Y|{\cal G})(\omega)=\int Y(\omega^{\prime})\mu(d\omega^{\prime},\omega)\quad \mbox{w.p. 1}\]for all \(Y\) such that \(E|Y|<\infty\). The proof of this is via standard approximation using simple random variables (Problem 12.15).

A sufficient condition for the existence of r.c.p. is provided by the following result.

Theorem 12.3.1: Let \((\Omega,\Cal{F},P)\) be a probability space. Let \(\mathbb{S}\) be a Polish space and \(\Cal{S}\) be its Borel \(\sigma\)-algebra. Let \(X\) be an \(\mathbb{S}\)-valued random variable on \((\Omega,\Cal{F})\). Then for any \(\sigma\)-algebra \(\Cal{G}\subset\Cal{F}\), there is a regular conditional probability on \(\sigma\langle X\rangle\) given \(\Cal{G}\), where \(\sigma\langle X\rangle=\{X^{-1}(D):D\in\Cal{S}\}\).

**Proof:** (for \(\mathbb{S}=\mathbb{R}\)). Let \(\mathbb{Q}=\{r_{j}\}\) be the set of rationals. Let \(F(r_{j},\omega)=P(X\leq r_{j}|\Cal{G})(\omega)\) w.p. 1. Then, there is a set \(A_{0}\in\Cal{G}\) such that \(P(A_{0})=0\) and for \(\omega\not\in A_{0}\), \(F(r,\omega)\) is monotone nondecreasing on \(\mathbb{Q}\). For \(x\in\mathbb{R}\), set

\[F(x,\omega)\equiv\left\{\begin{array}{ll}\sup\{F(r,\omega):r\leq x\}&\text{ if}\quad\omega\not\in A_{0}\\ F_{0}(x)&\text{ if}\quad\omega\in A_{0},\end{array}\right.\]

where \(F_{0}(x)\) is a fixed cdf (say, \(F_{0}=\Phi\), the standard normal cdf). Then, \(F(x,\omega)\) is a cdf in \(x\) for each \(\omega\) and for each \(x\), \(F(x,\cdot)\) is \(\Cal{G}\)-measurable.

Let \(\mu(B,\omega)\) be the Lebesgue-Stieltjes measure induced by \(F(\cdot,\omega)\). Then it can be checked using the \(\pi-\lambda\) theorem (Theorem 1.1.2) that \(\mu(\cdot,\cdot)\) is a regular conditional probability on \(\sigma\langle X\rangle\) given \(\Cal{G}\) (Problem 12.16). \(\Box\)

Remark 12.3.1: When \(\Cal{F}=\sigma\langle X\rangle\), the regular conditional probability on \(\Cal{F}\) given \(\Cal{G}\) is also called the _regular conditional probability distribution_ of \(X\) given \(\Cal{G}\).

Remark 12.3.2: For a proof for the general Polish case, see Durrett (2004) and Parthasarathy (1967).

### Problems

12.1 Let \((X,Y)\) be a bivariate random vector with \(EY^{2}<\infty\). Let \(\mathbb{H}=L^{2}(\mathbb{R}^{2},\Cal{B}(\mathbb{R}),P_{X,Y})\) and \(\mathbb{H}_{0}=\{h(X)\mid h:\mathbb{R}\to\mathbb{R}\) is Borel measurable and \(Eh(X)^{2}<\infty\}\). Suppose that for some \(h(X)\in\mathbb{H}_{0}\),

\[EYI_{A}=Eh(X)I_{A}\quad\text{for all}\quad A\in\sigma\langle X\rangle.\]

Show that \(E(Y-h(X))Z_{1}=0\) for all \(Z_{1}\in\mathbb{H}_{0}\). Show also that \(\mathbb{H}_{0}\) is a closed subspace of \(\mathbb{H}\).

(**Hint:** For any \(Z_{1}\in\mathbb{H}_{0}\), there exists a sequence of simple random variables \(\{W_{n}\}_{n\geq 1}\subset\mathbb{H}_{0}\) such that \(|W_{n}|\leq|Z_{1}|\) and \(W_{n}\to Z_{1}\) a.s. Now, apply the DCT. For the second part, use the fact that \(f:\Omega\to\mathbb{R}\) is \(\sigma\langle X\rangle\)-measurable iff there is a Borel measurable function \(h:\mathbb{R}\to\mathbb{R}\) such that \(f=h(X)\).)12. Let \((X,Y)\) be a bivariate random vector that has an absolutely continuous distribution on \((\mathbb{R}^{2},\mathcal{B}(\mathbb{R}^{2}))\) w.r.t. the Lebesgue measure with density \(f(x,y)\). Suppose that \(E|Y|<\infty\). Show that a version of \(E(Y|X)\) is given by \(h_{0}(X)\), where, with \(f_{X}(x)=\int f(x,y)dy\), \[h_{0}(x)=\left\{\begin{array}{ccc}\frac{\int yf(x,y)dy}{f_{X}(x)}&\mbox{if}&f _{X}(x)>0\\ 0&\mbox{otherwise.}\end{array}\right.\] (**Hint:** Verify (1.8) for all \(A\in\sigma\langle X\rangle\).)
12. Let \(Z_{1}\) and \(Z_{2}\) be two random variables on a probability space \((\Omega,\mathcal{G},P)\). 1. Suppose that \(E|Z_{1}|<\infty\), \(E|Z_{2}|<\infty\) and \[EZ_{1}I_{A}=EZ_{2}I_{A}\quad\mbox{for all}\quad A\in\mathcal{G}.\] (4.1) Show that \(P(Z_{1}=Z_{2})=1\). 2. Suppose that \(Z_{1}\) and \(Z_{2}\) are nonnegative and (4.1) holds. Show that \(P(Z_{1}=Z_{2})=1\). 3. Prove Proposition 12.1.3. (**Hint:** (a) Consider (4.1) with \(A_{1}=\{Z_{1}-Z_{2}>0\}\) and \(A_{2}=\{Z_{1}-Z_{2}<0\}\) and conclude that \(P(A_{1})=0=P(A_{2})\). (b) Let \(A_{1n}=\{Z_{1}\leq n,Z_{2}\leq n,Z_{1}-Z_{2}>0\}\) and \(A_{2n}=\{Z_{1}\leq n,Z_{2}\leq n,Z_{1}-Z_{2}<0\}\), \(n\geq 1\). Then, by (4.1), \(P(A_{1n})=0=P(A_{2n})\) for all \(n\geq 1\). But \(A_{1}=\bigcup_{n\geq 1}A_{in}\), \(i=1,2,\dots\), where \(A_{i}\)'s are as above.)
12.4 Prove Theorem 12.2.3.
12. Let \(X_{i}\) be a \(k_{i}\)-dimensional random vector, \(k_{i}\in\mathbb{N}\), \(i=1,2\) such that \(X_{1}\) and \(X_{2}\) are independent. Let \(h:\mathbb{R}^{k_{1}+k_{2}}\to[0,\infty)\) be a Borel measurable function. Show that \[E\big{(}h(X_{1},X_{2})\mid X_{1}\big{)}=g(X_{1})\] (4.2) where \(g(x)=Eh(x,X_{2})\), \(x\in\mathbb{R}^{k_{1}}\). Show that (4.2) is also valid for a real valued \(h\) with \(E\big{|}h(X_{1},X_{2})\big{|}<\infty\). (**Hint:** Let \(k=k_{1}+k_{2}\), \(\Omega=\mathbb{R}^{k}\), \(\mathcal{F}=\mathcal{B}(\mathbb{R}^{k})\), \(P=P_{X_{1}}\times P_{X_{2}}\). Verify (1.8) for all \(A\in\{A_{1}\times\mathbb{R}^{k_{2}}:A_{1}\in\mathcal{B}(\mathbb{R}^{k_{1}})\} \equiv\sigma\langle X_{1}\rangle\).)
12.6 Let \(X\) be a random variable on a probability space \((\Omega,\mathcal{F},P)\) with \(EX^{2}<\infty\) and let \(\mathcal{G}\subset\mathcal{F}\) be a sub-\(\sigma\)-field. 1. Show that for any \(A\in\mathcal{G}\), \[\Big{|}\int_{A}E(X|\mathcal{G})dP\Big{|}\leq\Big{(}\int_{A}E(X^{2}|\mathcal{G} )dP\Big{)}^{1/2}.\] (4.3)2. Show that (4.3) is valid for all \(A\in{\cal F}\).
12.7 Let \(f:(\mathbb{R}^{k},{\cal B}(\mathbb{R}^{k}),P)\to(\mathbb{R},{\cal B}(\mathbb{R}))\) be an integrable function where \[P(A)=2^{-k}\int_{A}\exp\bigg{(}-\sum_{i=1}^{k}|x_{i}|\bigg{)}dx_{1},\ldots,dx_ {k},\ A\in{\cal B}(\mathbb{R}^{k}).\] For each of the following cases, find a version of \(E(f|{\cal G})\) and justify your answer: 1. \({\cal G}=\sigma\langle\{A\in{\cal B}(\mathbb{R}^{k}):A=-A\}\rangle,\) 2. \({\cal G}=\sigma\langle\{(j_{1},j_{1}+1]\times\cdots\times(j_{k},j_{k}+1]:j_{1}, \ldots,j_{k}\in\mathbb{Z}\}\rangle,\) 3. \({\cal G}=\sigma\langle\{B\times\{0\}:B\in{\cal B}(\mathbb{R}^{k-1})\}\rangle.\)
12.8 Let \((\Omega,{\cal F},P)\) be a probability space and \({\cal G}=\{\emptyset,B,B^{c},\Omega\}\) for some \(B\in{\cal F}\) with \(P(B)\in(0,1).\) Determine \(P(A|{\cal G})\) for \(A\in{\cal F}.\)
12.9 Let \(\{X_{n}:n\in\mathbb{Z}\}\) be a collection of independent random variables with \(E|X_{0}|<\infty.\) Show that 1. \(E(X_{0}\mid X_{1},\ldots,X_{n})=EX_{0}\) for any \(n\in\mathbb{N},\) 2. \(E(X_{0}\mid X_{-n},\ldots,X_{-1})=EX_{0}\) for any \(n\in\mathbb{N},\) 3. \(E(X_{0}\mid X_{1},X_{2},\ldots)=EX_{0}=E(X_{0}\mid\ldots,X_{-2},X_{-1}).\)
12.10 Let \(X\) be a random variable on a probability space \((\Omega,{\cal F},P)\) with \(E|X|<\infty\) and let \({\cal C}\) be a \(\pi\)-system such that \(\sigma\langle{\cal C}\rangle={\cal G}\subset{\cal F}.\) Suppose that there exists a \({\cal G}\)-measurable function \(f:\Omega\to\mathbb{R}\) such that \[\int_{A}fdP=\int_{A}XdP\quad\mbox{for all}\quad A\in{\cal C}.\] Show that \(f=E(X|{\cal G}).\)
12.11 Let \(X\) and \(Y\) be integrable random variables on \((\Omega,{\cal F},P)\) and let \({\cal C}\) be a semi-algebra, \({\cal C}\subset{\cal F}.\) Suppose that \(\int_{A}XdP\leq\int_{A}YdP\) for all \(A\in{\cal C}.\) Show that \[E(X|{\cal G})\leq E(Y|{\cal G})\] where \({\cal G}=\sigma\langle{\cal C}\rangle.\)
12.12 Let \(X,Y\in L^{2}(\Omega,{\cal F},P).\) If \(E(X|Y)=Y\) and \(E(Y|X)=X,\) then \(P(X=Y)=1.\) (**Hint:** Show that \(E(X-Y)^{2}=EX^{2}-EY^{2}.\))
12.13 Let \(\{X_{n}\}_{n\geq 1},\)\(X\) be a collection of random variables on \((\Omega,{\cal F},P)\) and let \({\cal G}\) be a sub-\(\sigma\)-algebra of \({\cal F}.\) If \(\lim_{n\to\infty}E|X_{n}-X|^{r}=0\) for some \(r\geq 1,\) then \[\lim_{n\to\infty}E\big{|}E(X_{n}|{\cal G})-E(X|{\cal G})\big{|}^{r}=0.\]12.14 Let \(X,Y\in L^{2}(\Omega,\mathcal{F},P)\) and let \(\mathcal{G}\) be a sub-\(\sigma\)-algebra of \(\mathcal{F}\). Show that \[E\{YE(X|\mathcal{G})\}=E\{XE(Y|\mathcal{G})\}.\]
12.15 Let \(Y\) be an integrable random variable on \((\Omega,\mathcal{F},P)\) and let \(\mu\) be a r.c.p. on \(\mathcal{F}\) given \(\mathcal{G}\). Show that \(h(\omega)\equiv\int Y(\omega_{1})\mu(d\omega_{1},\omega)\), \(\omega\in\Omega\) is a version of \(E(Y|\mathcal{G})\). (**Hint:** Prove this first for \(Y=I_{A}\), \(A\in\mathcal{F}\). Extend to simple functions by linearity. Use the DCT for CE for the general case.)
12.16 Complete the proof of Theorem 12.3.1 for \(\mathbb{S}=\mathbb{R}\).
12.17 Let \((\Omega,\mathcal{F},P)\) be a probability space, \(\mathcal{G}\) be a sub-\(\sigma\)-algebra of \(\mathcal{F}\), and let \(\{A_{n}\}_{n\geq 1}\subset\mathcal{F}\) be a collection of disjoint sets. Show that \[P\bigg{(}\bigcup_{n\geq 1}A_{n}|\mathcal{G}\bigg{)}=\sum_{n=1}^{\infty}P(A_{n} |\mathcal{G}).\]

**Definition 12.4.1:** Let \(\mathcal{G}\) be a \(\sigma\)-algebra and let \(\{\mathcal{G}_{\lambda}:\lambda\in\Lambda\}\) be a collection of subsets of \(\mathcal{F}\) in a probability space \((\Omega,\mathcal{F},P)\). Then, \(\{\mathcal{G}_{\lambda}:\lambda\in\Lambda\}\) is called _conditionally independent given \(\mathcal{G}\)_ if for any \(\lambda_{1},\ldots,\lambda_{k}\in\Lambda\), \(k\in\mathbb{N}\),

\[P\big{(}A_{1}\cap\cdots\cap A_{k}|\mathcal{G}\big{)}=\prod_{i=1}^{k}P(A_{i}| \mathcal{G})\]

for all \(A_{1}\in\mathcal{G}_{1},\ldots,A_{k}\in\mathcal{G}_{k}\). A collection of random variables \(\{X_{\lambda}:\lambda\in\Lambda\}\) on \((\Omega,\mathcal{F},P)\) is called _conditionally independent given \(\mathcal{G}\)_ if \(\{\sigma\langle X_{\lambda}\rangle:\lambda\in\Lambda\}\) is conditionally independent given \(\mathcal{G}\).
12.18 Let \(\mathcal{G}_{1},\mathcal{G}_{2},\mathcal{G}_{3}\) be three sub-\(\sigma\)-algebras of \(\mathcal{F}\). Recall that \(\mathcal{G}_{i}\vee\mathcal{G}_{j}=\sigma\langle\mathcal{G}_{i}\cup\mathcal{G }_{j}\rangle\), \(1\leq i\neq j\leq 3\). Show that \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are conditionally independent given \(\mathcal{G}_{3}\) iff \[P(A_{1}|\mathcal{G}_{2}\vee\mathcal{G}_{3})=P(A_{1}|\mathcal{G}_{3})\quad\text {for all}\quad A_{1}\in\mathcal{G},\] \[\text{iff}\ \ E(X|\mathcal{G}_{2}\vee\mathcal{G}_{3})=E(X|\mathcal{G}_{ 3})\] for every \(X\in L^{1}(\Omega,\mathcal{G}_{1}\vee\mathcal{G}_{3},P)\).
12.19 Let \(\mathcal{G}_{1},\mathcal{G}_{2},\mathcal{G}_{3}\) be sub-\(\sigma\)-algebra of \(\mathcal{F}\). Show that if \(\mathcal{G}_{1}\vee\mathcal{G}_{3}\) is independent of \(\mathcal{G}_{2}\), then \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are conditionally independent given \(\mathcal{G}_{3}\).
12.20 Give an example where 12.21 Let \(X\) be an Exponential (1) random variable. For \(t>0\), let \(Y_{1}=\min\{X,t\}\) and \(Y_{2}=\max\{X,t\}\). Find \(E(X|Y_{i})\,i=1,2\). (**Hint:** Verify that \(\sigma\langle Y_{1}\rangle\) is the \(\sigma\)-algebra generated by the collection \(\{X^{-1}(A):A\in{\cal B}({\mathbb{R}})\), \(A\subset[0,t)\}\cup\{X^{-1}[t,\infty)\}\).)
12.22 Let \((X,Y)\) be a bivariate random vector with a joint pdf w.r.t. the Lebesgue measure \(f(x,y)\). Show that \(E(X|X+Y)=h(X+Y)\) where \(h(z)=\biggl{(}\frac{\int xf(x,z-x)dx}{\int f(x,z-x)dx}\biggr{)}I_{(0,\infty)} \Bigl{(}\int f(x,z-x)dx\Bigr{)}\).
12.23 Let \(\{X_{i}\}_{i\geq 1}\) be iid random variables with \(E|X_{1}|<\infty\). Show that for any \(n\geq 1\), \(E\bigl{(}X_{1}\mid(X_{1}+X_{2}+\cdots+X_{n})\bigr{)}=\frac{X_{1}+X_{2}+\cdots +X_{n}}{n}\). (**Hint:** Show that \(E\bigl{(}X_{i}\mid(X_{1}+\cdots+X_{n})\bigr{)}\) is the same for all \(1\leq i\leq n\).)

**Definition 12.4.2:** A finite collection of random variables \(\{X_{i}:1\leq i\leq n\}\) on a probability space \((\Omega,{\cal F},P)\) is said to be _exchangeable_ if for any permutation \((i_{1},i_{2},\ldots,i_{n})\) of \((1,2,\ldots,n)\), the joint distribution of \((X_{i_{1}},X_{i_{2}},\ldots,X_{i_{n}})\) is the same as that of \((X_{1},X_{2},\ldots,X_{n})\). A sequence of random variables \(\{X_{i}\}_{i\geq 1}\) on a probability space \((\Omega,{\cal F},P)\) is said to be _exchangeable_ if for any finite \(n\), the collection \(\{X_{i}:1\leq i\leq n\}\) is exchangeable.
12.24 Let \(\{X_{i}:1\leq i\leq n+1\}\) be a finite collection of random variables such that conditional \(X_{n+1}\), \(\{X_{1},X_{2},\ldots,X_{n}\}\) are iid. Show that \(\{X_{i}:1\leq i\leq n\}\) is exchangeable.
12.25 Let \(\{X_{i}:1\leq i\leq n\}\) be exchangeable. Suppose \(E|X_{1}|<\infty\). Show that \(E\bigl{(}X_{1}\mid(X_{1}+\cdots+X_{n})\bigr{)}=\frac{X_{1}+X_{2}+\cdots+X_{n}} {n}\).
12.26 Let \((X_{1},X_{2},X_{3})\) be random variables such that \(P(X_{2}\in\cdot\mid X_{1}) = p_{1}(X_{1},\cdot)\quad\mbox{and}\) \(P(X_{3}\in\cdot\mid X_{1},X_{2}) = p_{2}(X_{2},\cdot)\) where for each \(i=1,2\), \(p_{i}(x,\cdot)\) is a probability transition function on \({\mathbb{R}}\) as defined in Example 6.3.8. Suppose \(p_{i}(x,\cdot)\) admits a pdf \(f_{i}(x,\cdot)\)\(i=1,2,\ldots\). Show that \(P(X_{1}\in\cdot\mid X_{2},X_{3})=P(X_{1}\in\cdot\mid X_{2})\). (_This says that if \(\{X_{1},X_{2},X_{3}\}\) has the Markov property, then so does \(\{X_{3},X_{2},X_{1}\}\)._)12.27 (_Rao-Blackwell theorem_). Let \(Y\in L^{2}(\Omega,\mathcal{F},P)\) and \(\mathcal{G}\subset\mathcal{F}\) be a sub-\(\sigma\)-algebra. Show that there exists \(Z\in L^{2}(\Omega,\mathcal{G},P)\) such that \(EZ=EY\) and \(\operatorname{Var}(Z)\leq\operatorname{Var}(Y)\). (**Hint:** Consider \(Z=E(Y|\mathcal{G})\).)
12.28 Let \((X,Y)\) have an absolutely continuous bivariate distribution with density \(f_{X,Y}(x,y)\). Show that there is a regular conditional probability on \(\sigma\langle Y\rangle\) given \(\sigma\langle X\rangle\) and that this probability measure induces an absolutely continuous distribution on \(\mathbb{R}\). Find its density.
12.29 Suppose, in the above problem, \[f_{X,Y}(x,y)=\frac{1}{\sigma(x)}\phi\Big{(}\frac{y-m(x)}{\sigma(x)}\Big{)}g(x)\] where \(m(\cdot)\), \(\sigma(\cdot)\), \(\phi(\cdot)\), and \(g(\cdot)\) are all Borel measurable functions on \(\mathbb{R}\) to \(\mathbb{R}\) with \(\sigma\), \(\phi\), and \(g\) being nonnegative and \(\phi\) and \(g\) being probability densities. 1. Find the marginal probability densities \(f_{X}(\cdot)\) and \(f_{Y}(\cdot)\) of \(X\) and \(Y\), respectively. Set up the integrals for \(EX\) and \(EY\). 2. Using the conditioning argument in Proposition 12.1.5, show that \[EY=\int m(x)g(x)dx+\Big{(}\int u\phi(u)du\Big{)}\Big{(}\int\sigma(x)g(x)dx\Big{)}\] (assuming that all the integrals are well defined). 3. Find similar expressions for \(EY^{2}\) and \(E(e^{tY})\).
12.30 Let \(X\), \(Y\), \(Z\in L^{1}(\Omega,\mathcal{F},P)\). Suppose that \[E(X|Y)=Z,\ E(Y|Z)=X,\ E(Z|X)=Y.\] Show that \(X=Y=Z\) w.p. 1.
12.31 Let \(X,Y\in L^{2}(\Omega,\mathcal{F},P)\). Suppose \(E|Y|^{4}<\infty\). Show that \[\min\big{\{}E|X-(a+bY+cY^{2})|^{2}:a,b,c\in\mathbb{R}\big{\}}\] \[= \max\big{\{}E(XZ):Z\in L^{2}(\Omega,\mathcal{F},P),\ EZ=0,\ EZY=0,\] \[EZY^{2}=0,\ EZ^{2}=1\big{\}}.\]
12.32 Let \(X\in L^{2}(\Omega,\mathcal{F},P)\) and \(\mathcal{G}\) be a sub-\(\sigma\)-algebra of \(\mathcal{F}\) 12.33.1 Show that \[\min\big{\{}E(X-Y)^{2}:Y\in L^{2}(\Omega,\mathcal{G},P)\big{\}}\] \[= \max\big{\{}(EXZ)^{2}:EZ^{2}=1,\ E(Z|\mathcal{G})=0\big{\}}.\] 12.34 Find a random variable \(Z\) such that \(E(Z|\mathcal{G})=0\) w.p. 1 and \(\rho\equiv\operatorname{corr}(X,Z)\) is maximized.

## Chapter 13 Discrete Parameter Martingales

### 13.1 Definitions and examples

This section deals with a class of stochastic processes called _martingales_. Martingales arise in a natural way in many problems in probability and statistics. It provides a more general framework than the case of independent random variables where results, like the SLLN, the CLT, and other convergence theorems, can be established. Much of the discrete parameter martingale theory was developed by the great American mathematician J. L. Doob, whose book (Doob (1953)) has been very influential.

**Definition 13.1.1:** Let \((\Omega,\mathcal{F},P)\) be a probability space and let \(N=\{1,\ldots,n_{0}\}\) be a nonempty subset of \(\mathbb{N}=\{1,2,\ldots\}\), \(n_{0}\leq\infty\).

* A collection \(\{\mathcal{F}_{n}:n\in N\}\) of sub-\(\sigma\)-algebras of \(\mathcal{F}\) is called a _filtration_ if \(\mathcal{F}_{n}\subset\mathcal{F}_{n+1}\) for all \(1\leq n<n_{0}\).
* A collection of random variables \(\{X_{n}:n\in N\}\) is said to be _adapted_ to the filtration \(\{\mathcal{F}_{n}:n\in N\}\) if \(X_{n}\) is \(\mathcal{F}_{n}\)-measurable for all \(n\in N\).
* Given a filtration \(\{\mathcal{F}_{n}:n\in N\}\) and random variables \(\{X_{n}:n\in N\}\), the collection \(\{(X_{n},\mathcal{F}_{n}):n\in N\}\) is called a _martingale_ if
* \(\{X_{n}:n\in N\}\) is adapted to \(\{\mathcal{F}_{n}:n\in N\}\),
* \(E|X_{n}|<\infty\) for all \(n\in N\), and
* for all \(1\leq n<n_{0}\), \[E(X_{n+1}|\mathcal{F}_{n})=X_{n}.\] (1.1)
When \(N=\mathbb{N}\), there is no maximum element in \(N\). In this case, Definition 13.1.1 is to be interpreted by setting \(n_{0}=+\infty\) in parts (a) and (c) (iii). A similar convention applies to Definition 13.1.2 below. Also, recall that equalities and inequalities involving conditional expectations are interpreted as being valid events w.p. 1.

If \(\{(X_{n},\mathcal{F}_{n}):n\in N\}\) is a martingale, then \(\{X_{n}:n\in N\}\) is also said to be a martingale w.r.t. (the filtration) \(\{\mathcal{F}_{n}:n\in N\}\). Also \(\{X_{n}:n\in N\}\) is called a martingale if it is a martingale w.r.t. some filtration. Observe that if \(\{X_{n}:n\in N\}\) is a martingale w.r.t. any given filtration \(\{\mathcal{F}_{n}:n\in N\}\), it is also a martingale w.r.t. the natural filtration \(\{\mathcal{X}_{n}:n\in N\}\), where \(\mathcal{X}_{n}=\sigma\langle\{X_{1},\ldots,X_{n}\}\rangle,n\in N\). Clearly, \(\{X_{n}:n\in N\}\) is adapted to \(\{\mathcal{X}_{n}:n\in N\}\). To see that \(E(X_{n+1}|\mathcal{X}_{n})=X_{n}\) for all \(1\leq n<n_{0}\), note that \(\mathcal{X}_{n}\subset\mathcal{F}_{n}\) for all \(n\in N\) and hence,

\[E(X_{n+1}|\mathcal{X}_{n}) = E(E(X_{n+1}|\mathcal{F}_{n})\mid\mathcal{X}_{n}) \tag{1.2}\] \[= E(X_{n}|\mathcal{X}_{n})=X_{n}.\]

Thus, \(\{(X_{n},\mathcal{X}_{n}):n\in N\}\) is a martingale.

A classic interpretation of martingales in the context of gambling is given as follows. Let \(X_{n}\) represent the fortune of a gambler at the end of the \(n\)th play and let \(\mathcal{F}_{n}\) be the information available to the gambler up to and including the \(n\)th play. Then, \(\mathcal{F}_{n}\) contains the knowledge of all events like \(\{X_{j}\leq r\}\) for \(r\in\mathbb{R}\), \(j\leq n\), making \(X_{n}\) measurable w.r.t. \(\mathcal{F}_{n}\). And Condition (iii) in Definition 13.1.1 (c) says that given all the information up until the end of the \(n\)th play, the expected fortune of the gambler at the end of the \((n+1)\)th play remains unchanged. Thus a martingale represents a _fair game_. In situations where the game puts the gambler in a favorable or unfavorable position, one may express that by suitably modifying condition (iii), yielding what are known as sub- and super-martingales, respectively.

**Definition 13.1.2:** Let \(\{\mathcal{F}_{n}:n\in N\}\) be a filtration and \(\{X_{n}:n\in N\}\) be a collection of random variables in \(L^{1}(\Omega,\mathcal{F},P)\) adapted to \(\{\mathcal{F}_{n}:n\in N\}\). Then \(\{(X_{n},\mathcal{F}_{n}):n\in N\}\) is called a _sub-martingale_ if

\[E(X_{n+1}|\mathcal{F}_{n})\geq X_{n}\quad\mbox{for all}\quad 1\leq n<n_{0}, \tag{1.3}\]

and a _super-martingale_

\[E(X_{n+1}|\mathcal{F}_{n})\leq X_{n}\quad\mbox{for all}\quad 1\leq n<n_{0}. \tag{1.4}\]

Suppose that \(\{(X_{n},\mathcal{F}_{n}):n\in N\}\) is a sub-martingale. Then \(A\in\mathcal{F}_{n}\) implies that \(A\in\mathcal{F}_{n+1}\subset\ldots\subset\mathcal{F}_{n+k}\) for every \(k\geq 1\), \(n+k\in N\) and hence, by (1.3),

\[\int_{A}X_{n}dP \leq \int_{A}E(X_{n+1}|\mathcal{F}_{n})dP=\int_{A}X_{n+1}dP\]\[\vdots\] \[\leq \int_{A}X_{n+k}dP.\]

Therefore, \(E(X_{n+k}|{\cal F}_{n})\geq X_{n}\) and, by taking \(A=\Omega\) in (1.5), \(EX_{n+k}\geq EX_{n}\). Thus, the expected values of a sub-martingale is nondecreasing. For a martingale, by (1.2), equality holds at every step of (1.5) and hence,

\[E(X_{n+k}|{\cal F}_{n})=X_{n},\ \ EX_{n+k}=EX_{n} \tag{1.6}\]

for all \(k\geq 1\), \(n\), \(n+k\in N\). Thus, in a fair game, the expected fortune of the gambler remains constant over time.

Here are some examples.

**Example 13.1.1:** (_Random walk_). Let \(Z_{1},Z_{2},\ldots\) be a sequence of iid random variables on a probability space \((\Omega,{\cal F},P)\) with finite mean \(\mu=EZ_{1}\) and let \({\cal F}_{n}=\sigma\langle Z_{1},\ldots,Z_{n}\rangle,n\geq 1\). Let \(X_{n}=Z_{1}+\ldots Z_{n},n\geq 1\). Then, for all \(n\geq 1\), \(\sigma\langle X_{n}\rangle\subset{\cal F}_{n}\) and \(E|X_{n}|<\infty\) for all \(n\geq 1\). Also,

\[E(X_{n+1}|{\cal F}_{n}) = E\big{(}(Z_{1}+\ldots+Z_{n+1})\mid Z_{1},\ldots,Z_{n}\big{)}\] \[= Z_{1}+\ldots+Z_{n}+EZ_{n+1}\quad\mbox{(by independence)}\] \[= X_{n}+\mu,\]

so that

\[E(X_{n+1}|{\cal F}_{n}) = X_{n}\quad\mbox{if}\quad\mu=0\] \[> X_{n}\quad\mbox{if}\quad\mu>0\] \[< X_{n}\quad\mbox{if}\quad\mu<0.\]

Thus, \(\{(X_{n},{\cal F}_{n}):n\geq 1\}\) is a martingale if \(\mu=0\), a sub-martingale if \(\mu\geq 0\) and a super-martingale if \(\mu\leq 0\).

**Example 13.1.2:** (_Random walk continued_). Let \(\{Z_{n}\}_{n\geq 1}\) and \(\{{\cal F}_{n}\}_{n\geq 1}\) be as in Example 13.1.1 and let \(EZ_{1}^{2}<\infty\). Let \(Y_{n}=\sum_{i=1}^{n}(Z_{i}-\mu)^{2}\) and \(\tilde{Y}_{n}=Y_{n}-n\sigma^{2}\), where \(\sigma^{2}=Var(Z_{1})\). Then, check that \(\{(Y_{n},{\cal F}_{n}):n\geq 1\}\) is a sub-martingale and \(\{(\tilde{Y}_{n},{\cal F}_{n}):n\geq 1\}\) is a martingale (Problem 13.3).

**Example 13.1.3:** (_Doob's martingale_). Let \(Z\) be an integrable random variable and let \(\{{\cal F}_{n}:n\geq 1\}\) be a filtration both defined on a probability space \((\Omega,{\cal F},P)\). Define

\[X_{n}=E(Z|{\cal F}_{n}),\ \ n\geq 1. \tag{1.7}\]

Then, clearly, \(X_{n}\) is integrable and \({\cal F}_{n}\)-measurable for all \(n\geq 1\). Also,

\[E(X_{n+1}|{\cal F}_{n}) = E(E(Z|{\cal F}_{n+1})\mid{\cal F}_{n})\] \[= E(Z|{\cal F}_{n})=X_{n}.\]Thus, \(\{(X_{n},\mathcal{F}_{n}):n\geq 1\}\) is a martingale.

**Example 13.1.4:** (_Generation of a martingale from a given sequence of random variables_). Let \(\{Y_{n}\}_{n\geq 1}\subset L^{1}(\Omega,\mathcal{F},P)\) be an arbitrary collection of integrable random variables and let \(\mathcal{F}_{n}=\sigma\langle Y_{1},\ldots,Y_{n}\rangle,n\geq 1\). For \(n\geq 1\), define

\[X_{n}=\sum_{j=1}^{n}\{Y_{j}-E(Y_{j}|\mathcal{F}_{j-1})\} \tag{1.8}\]

where \(\mathcal{F}_{0}\equiv\{\emptyset,\Omega\}\).

Then, for each \(n\geq 1\), \(X_{n}\) is integrable and \(\mathcal{F}_{n}\)-measurable. Also, for \(n\geq 1\),

\[E(X_{n+1}|\mathcal{F}_{n}) = \sum_{j=1}^{n+1}E([Y_{j}-E(Y_{j}|\mathcal{F}_{j-1})]\mid\mathcal{ F}_{n})\] \[= \sum_{j=1}^{n}[Y_{j}-E(Y_{j}|\mathcal{F}_{j-1})]+[E(Y_{n+1}| \mathcal{F}_{n})-E(Y_{n+1}|\mathcal{F}_{n})]\] \[= X_{n}.\]

Hence \(\{(X_{n},\mathcal{F}_{n}):n\geq 1\}\) is a martingale. Thus, one can construct a martingale sequence starting from any arbitrary sequence of integrable random variables. When \(\{Y_{n}\}_{n\geq 1}\) are iid with \(EY_{1}=0\), (1.8) yields \(X_{n}=\sum_{j=1}^{n}Y_{j}\) and one gets the martingale sequence of Example 13.1.1.

**Example 13.1.5:** (_Branching process_). Let \(\{\xi_{nk}:n\geq 1,k\geq 1\}\) be a double array of iid nonnegative integer valued random variables with \(E\xi_{nk}=\mu\in(0,\infty)\). One may think of \(\xi_{nk}\) as the number of offspring of the \(k\)th individual at time \(n\) in an evolving population. Let \(Z_{n}\) denote the size of the population at time \(n\), \(n\geq 0\). If one considers the evolution of the population starting with a single individual at time \(n=0\), then

\[Z_{0}=1\quad\mbox{and}\quad Z_{n}=\sum_{k=1}^{Z_{n-1}}\xi_{nk},\ \ n\geq 1.\]

The sequence \(Z_{0},Z_{1},\ldots\) is called a _branching process_ (cf. Chapter 18).

Let \(\mathcal{F}_{n}=\sigma\langle Z_{1},\ldots,Z_{n}\rangle,\ \ n\geq 1\). Then, for \(n\geq 1\),

\[E(Z_{n+1}|\mathcal{F}_{n})=E\bigg{(}\sum_{k=1}^{Z_{n}}\xi_{n+1,k}\mid Z_{n} \bigg{)}=\mu Z_{n} \tag{1.9}\]

and therefore, \(\{(Z_{n},\mathcal{F}_{n}):n\geq 1\}\) is a martingale, sub-martingale or super-martingale according as \(\mu=1\), \(\mu\geq 1\) or \(\mu\leq 1\). One can define a new sequence \(\{X_{n}\}_{n\geq 1}\) from \(\{Z_{n}\}_{n\geq 1}\) such that \(\{X_{n}\}_{n\geq 1}\) is a martingale w.r.t. \({\cal F}_{n}\) for all values of \(\mu\in(0,\infty)\). Let

\[X_{n}=\mu^{-n}Z_{n},\ n\geq 1. \tag{1.10}\]

Then, using (1.9), it is easy to check that \(\{(X_{n},{\cal F}_{n}):n\geq 1\}\) is a martingale.

**Example 13.1.6:** (_Likelihood ratio_). Let \(Y_{1},Y_{2},\ldots\) be a collection of random variables on a probability space \((\Omega,{\cal F},P)\) and let \({\cal F}_{n}=\sigma\langle Y_{1},\ldots,Y_{n}\rangle\), \(n\geq 1\). Let \(Q\) be another probability measure on \({\cal F}\). Suppose that under both \(P\) and \(Q\), the joint distributions of \((Y_{1},\ldots,Y_{n})\) are absolutely continuous w.r.t. the Lebesgue measure \(\lambda_{n}\) on \({\mathbb{R}}^{n},n\geq 1\). Denote the corresponding densities by \(p_{n}(y_{1},\ldots,y_{n})\) and \(q_{n}(y_{1},\ldots,y_{n}),n\geq 1\) and for simplicity, suppose that \(p_{n}(y_{1},\ldots,y_{n})\) is everywhere positive. Then, a _likelihood ratio_ for discriminating between the probability measures \(P\) and \(Q\) on the basis of the observations \(Y_{1},\ldots,Y_{n}\), is given by

\[X_{n}=q_{n}(Y_{1},\ldots,Y_{n})/p_{n}(Y_{1},\ldots,Y_{n}),\ n\geq 1.\]

A higher value of \(X_{n}\) is supposed to provide "evidence" in favor of \(Q\) (over \(P\)) as the "true" probability measure determining the distribution of \((Y_{1},\ldots,Y_{n})\). It will now be shown that \(\{X_{n}\}_{n\geq 1}\) is a martingale w.r.t. \({\cal F}_{n}\), \(n\geq 1\) under \(P\). Clearly, \(X_{n}\) is \({\cal F}_{n}\)-measurable for all \(n\geq 1\) and

\[\int|X_{n}|dP = \int_{\Omega}\frac{q_{n}(Y_{1},\ldots,Y_{n})}{p_{n}(Y_{1},\ldots, Y_{n})}dP\] \[= \int_{{\mathbb{R}}^{n}}\frac{q_{n}(y_{1},\ldots,y_{n})}{p_{n}(y_ {1},\ldots,y_{n})}p_{n}(y_{1},\ldots,y_{n})d\lambda_{n}\] \[= \int_{{\mathbb{R}}^{n}}q_{n}(y_{1},\ldots,y_{n})d\lambda_{n}=Q( \Omega)=1<\infty,\]

so that \(X_{n}\) is integrable (w.r.t. \(P\)) for all \(n\geq 1\). Noting that the sets in the \(\sigma\)-algebra \({\cal F}_{n}\) are given by \(\{(Y_{1},\ldots,Y_{n})\in B\}\) for \(B\in{\cal B}({\mathbb{R}}^{n})\), one has, for any \(n\geq 1\),

\[\int_{\{(Y_{1},\ldots,Y_{n})\in B\}}X_{n+1}dP = \int_{\{(Y_{1},\ldots,Y_{n+1})\in B\times{\mathbb{R}}\}}\frac{q_{ n+1}(Y_{1},\ldots,Y_{n+1})}{p_{n+1}(Y_{1},\ldots,Y_{n+1})}dP\] \[= \int_{B\times{\mathbb{R}}}\frac{q_{n+1}(y_{1},\ldots,y_{n+1})}{p_ {n+1}(y_{1},\ldots,y_{n+1})}\] \[\qquad\qquad\qquad p_{n+1}(y_{1},\ldots,y_{n+1})d\lambda_{n+1}\] \[= \int_{B}q_{n}(y_{1},\ldots,y_{n})d\lambda_{n}\] \[= \int_{B}\frac{q_{n}(y_{1},\ldots,y_{n})}{p_{n}(y_{1},\ldots,y_{n} )}p_{n}(y_{1},\ldots,y_{n})d\lambda_{n}\]\[= \int_{\{(Y_{1},\ldots,Y_{n})\in B\}}X_{n}dP,\]

implying that \(E(X_{n+1}|{\cal F}_{n})=X_{n},n\geq 1\). This shows that \(\{(X_{n},{\cal F}_{n}):n\geq 1\}\) is a martingale for any arbitrary sequence of random variables \(\{Y_{n}\}_{n\geq 1}\) under \(P\). However, \(\{(X_{n},{\cal F}_{n}):n\geq 1\}\) may _not_ be a martingale under \(Q\).

**Example 13.1.7:** (_Radon-Nikodym derivatives_). Let \(\Omega=(0,1]\), \({\cal F}={\cal B}(0,1]\), the Borel \(\sigma\)-algebra  on \((0,1]\) and let \(P\) denote the Lebesgue measure on \((0,1]\). For \(n\geq 1\), let \({\cal F}_{n}\) be the \(\sigma\)-algebra generated by the partition \(\{((k-1)2^{-n},k2^{n}]\), \(k=1,\ldots,2^{n}\}\) of \((0,1]\) by dyadic intervals. Let \(\nu\) be a finite measure on \((\Omega,{\cal F})\). Let \(\nu_{n}\) be the restriction of \(\nu\) to \({\cal F}_{n}\) and \(P_{n}\) be the restriction of \(P\) to \({\cal F}_{n}\), for each \(n\geq 1\). As \({\cal F}_{n}\) consists of all disjoint unions of the intervals \(((k-1)2^{-n},k2^{-n}],1\leq k\leq 2^{n}\), \(P_{n}(A)=0\) for some \(A\in{\cal F}_{n}\) iff \(A=\emptyset\). Consequently, \(\nu_{n}(A)=0\) whenever \(P_{n}(A)=0\), \(A\in{\cal F}_{n}\), i.e., \(\nu_{n}\ll P_{n}\). Let \(X_{n}\) denote the Radon-Nikodym derivative of \(\nu_{n}\) w.r.t. \(P_{n}\), given by

\[X_{n}=\sum_{k=1}^{2^{n}}\Big{[}\nu\Big{(}((k-1)2^{-n},k2^{-n}]\Big{)}2^{n}\Big{]} I_{((k-1)2^{-n},k2^{-n}]}. \tag{1.11}\]

Clearly, \(X_{n}\) is \({\cal F}_{n}\)-measurable and \(P\)-integrable. It is easy to check that \(E(X_{n+1}|{\cal F}_{n})=X_{n}\) for all \(n\geq 1\). Hence \(\{(X_{n},{\cal F}_{n}):n\geq 1\}\) is a martingale on \((\Omega,{\cal F},P)\). Note that the absolute continuity of \(\nu_{n}\) w.r.t. \(P_{n}\) (on \({\cal F}_{n}\)) holds for each \(1\leq n<\infty\) even though the measure \(\nu\) may not be absolutely continuous w.r.t. \(P\) on \({\cal F}={\cal B}((0,1])\).

**Proposition 13.1.1:** (_Convex functions of martingales and sub-martingales_). _Let \(\phi:{\mathbb{R}}\to{\mathbb{R}}\) be a convex function and let \(N=\{1,2,\ldots n_{0}\}\subset{\mathbb{N}}\) be a nonempty subset._

* _If_ \(\{(X_{n},{\cal F}_{n}):n\in N\}\) _is a martingale with_ \(E|\phi(X_{n})|<\infty\) _for all_ \(n\in N\)_, then_ \(\{(\phi(X_{n}),{\cal F}_{n}):n\in N\}\) _is a sub-martingale._
* _If_ \(\{(X_{n},{\cal F}_{n}):n\in N\}\) _is a sub-martingale,_ \(E|\phi(X_{n})|<\infty\) _for all_ \(n\in N\)_, and in addition,_ \(\phi\) _is nondecreasing, then_ \(\{(\phi(X_{n}),{\cal F}_{n}):n\in N\}\) _is a sub-martingale._

**Proof:** By the conditional Jensen's inequality (Theorem 12.2.4) for all \(1\leq n<n_{0}\),

\[E(\phi(X_{n+1})|{\cal F}_{n})\geq\phi(E(X_{n+1}|{\cal F}_{n})). \tag{1.12}\]

Parts (i) and (ii) follow from (1.12) on using the martingale and sub-martingale properties of \(\{X_{n}\}_{n\in N}\), respectively. \(\Box\)

**Proposition 13.1.2:** (_Doob's decomposition of a sub-martingale_). _Let \(\{(X_{n},{\cal F}_{n}):n\in N\}\) be a sub-martingale for some \(N=\{1,\ldots,n_{0}\}\subset{\mathbb{N}}\). Then, there exist two sets of random variables \(\{Y_{n}:n\in N\}\) and \(\{Z_{n}:n\in N\}\) satisfying \(X_{n}=Y_{n}+Z_{n}\), \(n\in N\) such that_13.2 Stopping times and optional stopping theorems

**Proof:** Define the difference variables \(\Delta_{n}\)'s by

\[\Delta_{1}=X_{1},\quad\mbox{and}\quad\Delta_{n}=X_{n}-X_{n-1},\ n\geq 2,\ n\in N.\]

Note that \(X_{n}=\sum_{j=1}^{n}\Delta_{j}\), \(n\in N\), and \(E(\Delta_{n}|{\cal F}_{n-1})\geq 0\) for all \(n\geq 2\), \(n\in N\). Now, set

\[Y_{1} = \Delta_{1},\ \ Y_{n}=X_{n}-\sum_{j=2}^{n}E(\Delta_{j}|{\cal F}_{j-1}), \ \ n\geq 2,n\in N\]

and

\[Z_{1} = 0,\ \ Z_{n}=\sum_{j=2}^{n}E(\Delta_{j}|{\cal F}_{j-1}),\ \ n\geq 2,\ n\in N.\]

Check that the requirements (i) and (ii) above hold. To prove the \(L^{1}\)-boundedness, notice that by (ii), for any \(n\geq 2\), \(n\in N\),

\[E|Z_{n}| = EZ_{n}=E\bigg{[}\sum_{j=2}^{n}E(\Delta_{j}|{\cal F}_{j-1})\bigg{]} =\sum_{j=2}^{n}E(\Delta_{j}) \tag{1.13}\] \[= EX_{n}-EX_{1}\leq 2M.\]

Also, \(X_{n}=Y_{n}+Z_{n}\) for all \(n\in N\) implies that

\[|Y_{n}|\leq|X_{n}|+|Z_{n}|,\ n\in N. \tag{1.14}\]

Hence, (iii) follows. \(\Box\)

### Stopping times and optional stopping theorems

In the following (and elsewhere), '\(n\geq 1\)' is used as an alternative for the statement '\(n\in\mathbb{N}\)' or equivalently for \(1\leq n<\infty\).

**Definition 13.2.1:** Let \((\Omega,{\cal F},P)\) be a probability space, \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration and \(T\) be a \({\cal F}\)-measurable random variable taking values in the set \(\bar{\mathbb{N}}\equiv\mathbb{N}\cup\{\infty\}=\{1,2,\dots,\}\cup\{\infty\}\).

1. \(T\) is called a _stopping time_ w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) if \[\{T=n\}\in{\cal F}_{n}\quad\mbox{for all}\quad n\geq 1.\] (2.1)
2. \(T\) is called a _finite_ or _proper_ stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) (under \(P\)) if \[P(T<\infty)=1.\] (2.2)

Given a filtration \(\{{\cal F}_{n}\}_{n\geq 1}\), define the \(\sigma\)-algebra

\[{\cal F}_{\infty}=\sigma\langle\bigcup_{n\geq 1}{\cal F}_{n}\rangle. \tag{2.3}\]

Since \(\{T=+\infty\}^{c}=\bigcup_{n\geq 1}\{T=n\}\in{\cal F}_{\infty}\), (2.1) is equivalent to '\(\{T=n\}\in{\cal F}_{n}\) for all \(1\leq n\leq\infty\)'. It is also easy to check (Problem 13.7) that \(T\) is a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) iff

\[\{T\leq n\}\in{\cal F}_{n}\quad\mbox{for all}\quad n\geq 1 \tag{2.4}\]

iff \(\{T>n\}\in{\cal F}_{n}\) for all \(n\geq 1\). However, the condition

\[`\{T\geq n\}\in{\cal F}_{n}\quad\mbox{for all}\quad n\geq 1\mbox{'} \tag{2.5}\]

does not always imply that \(T\) is a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) (cf. Problem 13.7). Note that for a stopping time \(T\) w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\),

\[\{T\geq n\}=\{T\leq n-1\}^{c}\in{\cal F}_{n-1}\quad\mbox{for}\quad n\geq 2,\]

and \(\{T\geq 1\}=\Omega\). Since \({\cal F}_{n-1}\subset{\cal F}_{n}\) for all \(n\geq 2\), (2.5) is a weaker requirement than \(T\) being a stopping time w.r.t \(\{{\cal F}_{n}\}_{n\geq 1}\).

**Proposition 13.2.1:** _Let \(T\) be a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) and let \({\cal F}_{\infty}\) be as in (2.3). Define_

\[{\cal F}_{T}=\{A\in{\cal F}_{\infty}:A\cap\{T=n\}\in{\cal F}_{n}\quad\mbox{ for all}\quad n\geq 1\}. \tag{2.6}\]

_Then, \({\cal F}_{T}\) is a \(\sigma\)-algebra._

**Proof:** Left as an exercise (Problem 13.8). \(\Box\)

If \(T\) is a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\), then for any \(m\in{\mathbb{N}}\), \(\{T=m\}\cap\{T=n\}=\emptyset\in{\cal F}_{n}\) for all \(n\neq m\) and \(\{T=m\}\cap\{T=n\}=\{T=m\}\in{\cal F}_{m}\) for \(n=m\). Thus, \(\{T=m\}\in{\cal F}_{T}\) for all \(m\geq 1\) and hence, \(\sigma\langle T\rangle\subset{\cal F}_{T}\). But the reverse inclusion may not hold as shown below.

**Example 13.2.1:** Let \(T\equiv m\) for some given integer \(m\geq 1\). Then, \(T\) is a stopping time w.r.t. any filtration \(\{{\cal F}_{n}\}_{n\geq 1}\). For this \(T\),

\[A\in{\cal F}_{T}\Rightarrow A\cap\{T=m\}\in{\cal F}_{m}\Rightarrow A\in{\cal F }_{m},\]so that \({\cal F}_{T}\subset{\cal F}_{m}\). Conversely, suppose \(A\in{\cal F}_{m}\). Then, \(A\cap\{T=n\}=\emptyset\in{\cal F}_{n}\) for all \(n\neq m\), and \(A\cap\{T=m\}=A\in{\cal F}_{m}\) for \(n=m\). Thus, \({\cal F}_{m}={\cal F}_{T}\). But \(\sigma\langle T\rangle=\{\Omega,\emptyset\}\).

**Example 13.2.2:** Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables adapted to a filtration \(\{{\cal F}_{n}\}_{n\geq 1}\) and let \(\{B_{n}\}_{n\geq 1}\) be a sequence of Borel sets in \({\mathbb{R}}\). Define the random variable

\[T=\inf\big{\{}n\geq 1:X_{n}\in B_{n}\big{\}}.\]

Then, \(T(\omega)<\infty\) if \(X_{n}(\omega)\in B_{n}\) for some \(n\in{\mathbb{N}}\) and \(T(\omega)=+\infty\) if \(X_{n}(\omega)\not\in B_{n}\) for all \(n\in{\mathbb{N}}\). Since, for any \(n\geq 1\),

\[\{T=n\}=\big{\{}X_{1}\not\in B_{1},\ldots,X_{n-1}\not\in B_{n-1},X_{n}\in B_{n }\big{\}}\in{\cal F}_{n},\]

\(T\) is a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\). Now, define a new random variable \(X_{T}\) by

\[X_{T}=\left\{\begin{array}{lll}X_{m}&\mbox{if}&T=m\\ \limsup\limits_{n\to\infty}X_{n}&\mbox{if}&T=\infty.\end{array}\right.\]

The, \(X_{T}\in{\bar{\mathbb{R}}}\) and for any \(n\geq 1\) and \(r\in{\mathbb{R}}\),

\[\{X_{T}\leq r\}\cap\{T=n\}=\{X_{n}\leq r\}\cap\{T=n\}\in{\cal F}_{n}.\]

Also, \(\{X_{T}=\pm\infty\}\cap\{T=n\}=\{X_{n}=\pm\infty\}\cap\{T=n\}=\emptyset\) for all \(n\geq 1\). Hence, it follows that \(X_{T}\) is \(\langle{\cal F}_{T},{\cal B}({\bar{\mathbb{R}}})\rangle\)-measurable.

**Example 13.2.3:** Let \(\{Y_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EY_{1}=\mu\). Let \(X_{n}=(Y_{1}+\ldots+Y_{n})\), \(n\geq 1\) denote the random walk corresponding to \(\{Y_{n}\}_{n\geq 1}\). For \(x>0\), let

\[T_{x}=\inf\big{\{}n\geq 1:X_{n}>n\mu+x\sqrt{n}\big{\}}.\]

Then, \(T_{x}\) is the first time the sequence of partial sums \(\{X_{n}\}_{n\geq 1}\) exceeds the level \(n\mu+x\sqrt{n}\) and is a special case of (2.7) with \(B_{n}=(n\mu+x\sqrt{n},\infty)\), \(n\geq 1\). Consequently, \(T_{x}\) is a stopping time w.r.t. \({\cal F}_{n}=\sigma\langle Y_{1},Y_{2},\ldots,Y_{n}\rangle\), \(n\geq 1\). Note that if \(EY_{1}^{2}<\infty\), by the law of iterated logarithm (cf. 8.7),

\[\limsup\limits_{n\to\infty}{X_{n}-n\mu\over\sqrt{2\sigma^{2}n\log\log n}}=1 \quad\mbox{w.p. 1},\]

\[\mbox{i.e.,}\quad X_{n}>n\mu+C\sqrt{n\log\log n}\quad\mbox{infinitely often w.p. 1}\]

for some constant \(C>0\). Thus, \(P(T_{x}<\infty)=1\) and hence, \(T_{x}\) is a finite stopping time. This random variable \(T_{x}\) arises in sequential probability ratio tests (SPRT) for testing hypotheses on the mean of a (normal) population. See Woodroofe (1982), Chapter 3.

**Definition 13.2.2:** Let \(\{{\cal F}_{n}\}_{n\geq 0}\) be a filtration in a probability space \((\Omega,{\cal F},P)\). A _betting sequence_ w.r.t. \(\{{\cal F}_{n}\}_{n\geq 0}\) is a sequence \(\{H_{n}\}_{n\geq 1}\) of nonnegative random variables such that for each \(n\geq 1\), \(H_{n}\) is \({\cal F}_{n-1}\) measurable. The following result says that there is no betting scheme that can beat a gambling system, i.e., convert a fair one into a favorable one or the other way around.

**Theorem 13.2.2:** (_Betting theorem_). Let \(\{{\cal F}_{n}\}_{n\geq 0}\) be a filtration in a probability space. Let \(\{H_{n}\}_{n\geq 0}\) be a betting sequence w.r.t. \(\{{\cal F}_{n}\}_{n\geq 0}\). For an adapted sequence \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) let \(\{Y_{n}\}_{n\geq 0}\) be defined by \(Y_{0}=X_{0}\), \(Y_{n}=Y_{0}+\sum_{j=1}^{n}(X_{j}-X_{j-1})H_{j}\), \(n\geq 1\). Let \(E|(X_{j}-X_{j-1})H_{j}|<\infty\) for \(j\geq 1\). Then,_

* \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) _a martingale_ \(\Rightarrow\{Y_{n},{\cal F}_{n}\}_{n\geq 0}\) _is also a martingale,_
* \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) _a sub-martingale_ \(\Rightarrow\)__\(\{Y_{n},{\cal F}_{n}\}_{n\geq 0}\) _is also a sub-martingale,_

**Proof:** Clearly, for all \(n\geq 1\), \(E|Y_{n}|<\infty\) and \(Y_{n}\) is \({\cal F}_{n}\)-measurable. Further,

\[E\big{(}Y_{n+1}\big{|}{\cal F}_{n}\big{)} = Y_{n}+E\big{(}(X_{n+1}-X_{n})H_{n+1}\mid{\cal F}_{n}\big{)}\] \[= Y_{n}+H_{n+1}\ E\big{(}(X_{n+1}-X_{n})\mid{\cal F}_{n}\big{)}\]

since \(H_{n+1}\) is \({\cal F}_{n+1}\)-measurable. Now the theorem follows from the defining properties of \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\). \(\Box\)

The above theorem leads to the following results known as Doob's optional stopping theorems.

**Theorem 13.2.3:** (_Doob's optional stopping theorem_ 1). Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) be a sub-martingale. Let \(T\) be a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 0}\). Let \(X_{n}\equiv X_{T\wedge n}\), \(n\geq 0\). Then \(\{\tilde{X}_{n},{\cal F}_{n}\}_{n\geq 0}\) is also a sub-martingale and hence \(E\tilde{X}_{n}\geq EX_{0}\) for all \(n\geq 1\).

**Proof:** For any \(A\in{\cal B}(\mathbb{R})\) and \(n\geq 0\),

\[\tilde{X}_{n}^{-1}(A) = \{\omega:\tilde{X}_{n}\in A\}\] \[= \bigg{(}\bigcup_{j=1}^{n}\{\omega:X_{j}\in A,T=j\}\bigg{)}\cup\{ \omega:X_{n}\in A,T>n\}.\]

Since \(T\) is a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 0}\) the right side above belongs to \({\cal F}_{n}\) for each \(n\geq 0\). Next, \(|\tilde{X}_{n}|\leq\sum_{j=1}^{n}|X_{j}|\) and hence \(E|\tilde{X}_{n}|<\infty\).

Finally, let \(H_{j}=1\) if \(j\leq T\) and \(0\) if \(j>T\). Since for all \(j\geq 1\), \(\{\omega:H_{j}=1\}=\{\omega:T\leq j-1\}^{c}\in{\cal F}_{j-1}\), \(\{H_{j}\}_{j\geq 1}\) is a betting sequence w.r.t. \(\{{\cal F}_{n}\}_{n\geq 0}\). Also, \(\tilde{X}_{n}=X_{0}+\sum_{j=1}^{n}(X_{j}-X_{j-1})H_{j}\). Now the betting theorem (Theorem 13.2.2) implies the present theorem.

**Remark 13.2.1:** If \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) is a martingale, then both \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) and \(\{-X_{n},{\cal F}_{n}\}_{n\geq 0}\) are sub-martingales, and hence the above theorem implies that if \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) is a martingale, then so is \(\{\tilde{X}_{n},{\cal F}_{n}\}_{n\geq 0}\), and hence \(E\tilde{X}_{n}=EX_{T\wedge n}=EX_{0}=EX_{n}\) for all \(n\geq 1\).

This suggests the question that if \(P(T<\infty)=1\), then on letting \(n\to\infty\), does \(E\tilde{X}_{n}\to EX_{T}\)? Consider the following example. Let \(\{X_{n}\}_{n\geq 0}\) denote the symmetric simple random walk on the integers with \(X_{0}=0\). Let \(T=\inf\{n:n\geq 1,X_{n}=1\}\). Then \(P(T<\infty)=1\) and \(E\tilde{X}_{n}=EX_{T\wedge n}=EX_{0}=0\) but \(X_{T}=1\) w.p. 1 and hence \(E\tilde{X}_{n}\not\to EX_{T}=1\). So, clearly some additional hypothesis is needed.

**Theorem 13.2.4:** (_Doob's optional stopping theorem II_)_. Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 0}\) be a martingale. Let \(T\) be a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 0}\). Suppose \(P(T<\infty)=1\) and there is a \(0<K<\infty\) such that for all \(n\geq 0\)_

\[|X_{T\wedge n}|\leq K\ \ \mbox{w.p. 1}.\]

_Then \(EX_{T}=EX_{0}\)._

**Proof:** Since \(P(T<\infty)=1\), \(X_{T\wedge n}\to X_{T}\) w.p. 1 and \(|X_{T}|\leq K<\infty\) and hence \(E|X_{T}|<\infty\). Thus, \(E|X_{T}-X_{T\wedge n}|\leq 2KP(T>n)\to 0\). \(\Box\)

**Remark 13.2.2:** Since

\[X_{T}=X_{T}I_{(T\leq n)}+X_{n}I_{(T>n)}\]

and

\[EX_{T}I_{(T\leq n)} = \sum_{j=0}^{n}E(X_{j}:T=j)\] \[= \sum_{j=0}^{n}E(X_{0}:T=j)=E(X_{0}:T\leq n)\]

it follows that if \(E\big{(}|X_{n}|I_{(T>n)}\big{)}\to 0\) as \(n\to\infty\) and \(P(T<\infty)=1\) then \(EX_{T}=EX_{0}\).

A stronger version of Doob's optional stopping theorem is given below in Theorem 13.2.6.

**Proposition 13.2.5:** _Let \(S\) and \(T\) be two stopping times w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) with \(S\leq T\). Then, \({\cal F}_{S}\subset{\cal F}_{T}\)._

**Proof:** For any \(A\in{\cal F}_{S}\) and \(n\geq 1\),

\[A\cap\{T=n\} = A\cap\{T=n\}\cap\{S\leq n\}\] \[= \bigg{[}\bigcup_{k=1}^{n}A\cap\{S=k\}\bigg{]}\cap\{T=n\}\] \[\in {\cal F}_{n},\]since \(A\cap\{S=k\}\in{\cal F}_{k}\) for all \(1\leq k\leq n\). Thus, \(A\in{\cal F}_{T}\), proving the result. \(\Box\)

**Theorem 13.2.6:** (_Doob's optional stopping theorem III_). _Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a sub-martingale and \(S\) and \(T\) be two finite stopping times w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) such that \(S\leq T\). If \(X_{S}\) and \(X_{T}\) are integrable and if_

\[\liminf_{n\to\infty}E|X_{n}|I(|X_{n}|>T)=0, \tag{2.10}\]

_then_

\[E(X_{T}|{\cal F}_{S})\geq X_{S}\quad\mbox{a.s.} \tag{2.11}\]

_If, in addition, \(\{X_{n}\}_{n\geq 1}\) is a martingale, then equality holds in (2.11)._

Thus, Theorem 13.2.6 shows that if a martingale (or a sub-martingale) is stopped at random time points \(S\) and \(T\) with \(S\leq T\), then under very mild conditions, \(\big{\{}(X_{S},{\cal F}_{S}),(X_{T},{\cal F}_{T})\big{\}}\) continues to have the martingale (sub-martingale, respectively) property.

**Proof:** To show (2.11), it is enough to show that

\[\int_{A}(X_{T}-X_{S})dP\geq 0\quad\mbox{for all}\quad A\in{\cal F}_{S}. \tag{2.12}\]

Fix \(A\in{\cal F}_{S}\). Let \(\{n_{k}\}_{k\geq 1}\) be a subsequence along which the "\(\liminf\)" is attained in (2.10). Let \(T_{k}=\min\{T,n_{k}\}\) and \(S_{k}=\min\{S,n_{k}\}\), \(k\geq 1\). The proof of (2.12) involves showing that

\[\int_{A}(X_{T_{k}}-X_{S_{k}})dP\geq 0\quad\mbox{for all}\quad k\geq 1 \tag{2.13}\]

and

\[\lim_{k\to\infty}\int_{A}\big{[}(X_{T}-X_{S})-(X_{T_{k}}-X_{S_{k}})\big{]}dP=0. \tag{2.14}\]

Consider (2.13). Since \(S_{k}\leq T_{k}\leq n_{k}\),

\[X_{T_{k}}-X_{S_{k}} = \sum_{n=S_{k}+1}^{T_{k}}(X_{n}-X_{n-1}) \tag{2.15}\] \[= \sum_{n=2}^{n_{k}}(X_{n}-X_{n-1})I(S_{k}+1\leq n\leq T_{k}).\]

Note that for all \(2\leq n\leq n_{k}\), \(\{T_{k}\geq n\}=\{T_{k}\leq n-1\}^{c}=\{T\leq n-1\}^{c}\in{\cal F}_{n-1}\) and \(\{S_{k}+1\leq n\}=\{S_{k}\leq n-1\}=\{S\leq n-1\}\in{\cal F}_{n-1}\). Also, since \(A\in{\cal F}_{S},B_{n}\equiv A\cap\{S_{k}+1\leq n\leq T_{k}\}=(A\cap\{S_{k}+1 \leq n\})\cap\{T_{k}\geq n\}\in{\cal F}_{n-1}\) for all \(2\leq n\leq n_{k}\). Hence, by the sub-martingale property of \(\{X_{n}\}_{n\geq 1}\), from (2.15),

\[\int_{A}(X_{T_{k}}-X_{S_{k}})dP = \sum_{n=2}^{n_{k}}\int_{A\cap\{S_{k}+1\leq n\leq T_{k}\}}(X_{n}-X_{n -1})dP \tag{2.16}\] \[= \sum_{n=1}^{n_{k}}\int_{B_{n}}\big{[}E(X_{n}|{\cal F}_{n-1})-X_{n- 1}\big{]}dP\] \[\geq 0.\]

This proves (2.13). To prove (2.14), note that by (2.10) and the integrability of \(X_{S}\) and \(X_{T}\) and the DCT,

\[\lim_{k\to\infty}\Big{|}\int_{A}\big{[}(X_{T}-X_{S})-(X_{T_{k}}-X_ {S_{k}})\big{]}dP\Big{|}\] \[\leq\lim_{k\to\infty}\int\big{[}|X_{T}-X_{T_{k}}|+|X_{S}-X_{S_{k} }|\big{]}dP\] \[\leq\lim_{k\to\infty}\Big{[}\int_{\{T>n_{k}\}}(|X_{T}|+|X_{n_{k}}| )dP+\int_{\{S>n_{k}\}}(|X_{S}|+|X_{n_{k}}|)dP\Big{]}\] \[\leq\lim_{k\to\infty}\Big{[}\int_{\{T>n_{k}\}}|X_{T}|dP+2\int_{\{ T>n_{k}\}}|X_{n_{k}}|dP+\int_{\{S>n_{k}\}}|X_{S}|dP\Big{]}\] \[=0,\]

since \(\{S>n_{k}\}\subset\{T>n_{k}\}\) and \(\{T>n_{k}\}\downarrow\emptyset\) as \(k\to\infty\). This proves the theorem for the case when \(\{X_{n}\}_{n\geq 1}\) is a sub-martingale. When \(\{X_{n}\}_{n\geq 1}\) is a martingale, equality holds in the last line of (2.16), which implies equality in (2.13) and hence, in (2.12). This completes the proof. \(\Box\)

**Remark 13.2.3:** If there exists a \(t_{0}<\infty\) such that \(P(T\leq t_{0})=1\), then (2.10) holds.

**Corollary 13.2.7:** _Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a sub-martingale and let \(T\) be a finite stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) such that \(E|X_{T}|<\infty\) and (2.10) holds. Then,_

\[EX_{T}\geq EX_{1}. \tag{2.17}\]

_If, in addition, \(\{X_{n}\}_{n\geq 1}\) is a martingale, then equality holds in (2.17)._

**Proof:** Follows from Theorem 13.2.6 by setting \(S\equiv 1\). \(\Box\)

**Corollary 13.2.8:** _Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a sub-martingale. Let \(\{T_{n}\}_{n\geq 1}\) be a sequence of stopping times such that_

* _for all_ \(n\geq 1\)_,_ \(T_{n}\leq T_{n+1}\) _w.p._ 1_,_
* _for all_ \(n\geq 1\)_, there exist a nonrandom_ \(t_{n}\in(0,\infty)\) _such that_ \(P(T_{n}\leq t_{n})=1\)

[MISSING_PAGE_FAIL:422]

By the sub-martingale property of \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\),

\[E(X_{k}I_{A_{k}})\leq E(X_{m}I_{A_{k}})\quad\mbox{for}\quad k\leq m.\]

Thus

\[P(M_{m}>x) \leq \frac{1}{x}\ E\biggl{(}X_{m}\sum_{k=1}^{m}I_{A_{k}}\biggr{)}\] \[\leq \frac{1}{x}\ E(X_{m}I_{A})\] \[\leq \frac{1}{x}\ E\bigl{(}X_{m}^{+}I_{A}\bigr{)}\leq\frac{1}{x}\ E \bigl{(}X_{m}^{+}\bigr{)}.\]

**Theorem 13.2.11:** (_Doob's \(L^{p}\)-maximal inequality for sub-martingales_)_._

_Let_ \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) _be a sub-martingale and let_ \(M_{n}=\max\{X_{j}:1\leq j\leq n\}\)_. Then, for any_ \(p\in(1,\infty)\)_,_

\[E(M_{n}^{+})^{p}\leq\biggl{(}\frac{p}{p-1}\biggr{)}^{p}E(X_{n}^{+})^{p}\leq\infty. \tag{2.19}\]

**Proof:** If \(E(X_{n}^{+})^{p}=\infty\), then (2.19) holds trivially. Let \(E(X_{n}^{+})^{p}<\infty\). Since for \(p>1\), \(\phi(x)=(x^{+})^{p}\) is a convex nondecreasing function on \(\mathbb{R}\). Hence, \(\{(X_{n}^{+})^{p},{\cal F}_{n}\}_{n\geq 1}\) is a sub-martingale and \(E(X_{j}^{+})^{p}\leq E(X_{n}^{+})^{p}<\infty\) for all \(j\leq n\). Since \((M_{n}^{+})^{p}\leq\sum_{j=1}^{n}(X_{j}^{+})^{p}\), this implies that \(E(M_{n}^{+})^{p}<\infty\).

For any nonnegative random variable \(Y\) and \(p>0\), by Tonelli's theorem,

\[EY^{p} = pE\biggl{(}\int_{0}^{Y}x^{p-1}dx\biggr{)}\] \[= pE\biggl{(}\int_{0}^{\infty}x^{p-1}I(Y>x)dx\biggr{)}\] \[= \int_{0}^{\infty}px^{p-1}P(Y>x)dx.\]

Thus,

\[E(M_{n}^{+})^{p} = \int_{0}^{\infty}px^{p-1}P(M_{n}^{+}>x)dx\] \[= \int_{0}^{\infty}px^{p-1}P(M_{n}>x)dx.\]

By Theorem 13.2.10, for \(x>0\)

\[P(M_{n}>x)\leq\frac{1}{x}\ E(X_{n}^{+}I(M_{n}>x)),\]and hence

\[E(M_{n}^{+})^{p} \leq \int_{0}^{\infty}px^{p-2}E(X_{n}^{+}I(M_{n}>x))dx\] \[= \frac{p}{(p-1)}\,E(X_{n}^{+}M_{n}^{p-1})\] \[\leq \biggl{(}\frac{p}{p-1}\biggr{)}(E(X_{n}^{+})^{p})^{1/p}(E(M_{n}^{ (p-1)q})^{1/q}\]

(by Holder's inequality) where \(q\) is the conjugate of \(p\), i.e. \(q=\frac{p}{(p-1)}\).

Thus,

\[\biggl{(}E(M_{n}^{+})^{p}\biggr{)}^{1/p}\leq\biggl{(}\frac{p}{p-1}\biggr{)} \biggl{(}E(X_{n}^{+})^{p}\biggr{)}^{1/p},\]

\(\Box\)

**Corollary 13.2.12:** _Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a martingale and let \(\tilde{M}_{n}=\sup\{|X_{j}|:1\leq j\leq n\}\). Then, for \(p\in(1,\infty)\),_

\[E\tilde{M}_{n}^{p}\leq\biggl{(}\frac{p}{p-1}\biggr{)}^{p}\,E\biggl{(}|X_{n}|^{ p}\biggr{)}.\]

**Proof:** Since \(\{|X_{n}|,{\cal F}_{n}\}_{n\geq 1}\) is a sub-martingale, this follows from Theorem 13.2.11. \(\Box\)

**Theorem 13.2.13:** (_Doob's \(L\log L\) maximal inequality for sub-martingales_). _Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a sub-martingale and \(M_{n}=\max\{X_{j}:1\leq j\leq n\}\). Then_

\[EM_{n}^{+}\leq\biggl{(}\frac{e}{e-1}\biggr{)}\,\biggl{(}1+E(X_{n}^{+}\log X_{n }^{+})\biggr{)}, \tag{2.20}\]

_where \(0\log 0\) is interpreted as 0._

**Proof:** As in the proof of Theorem 13.2.11,

\[EM_{n}^{+} = \int_{0}^{\infty}P(M_{n}^{+}>x)dx \tag{2.21}\] \[\leq 1+\int_{1}^{\infty}\frac{1}{x}E(X_{n}^{+}I(M_{n}^{+}>x))dx\] \[= 1+E(X_{n}^{+}\log M_{n}^{+})\.\]

For \(x>0\), \(y>0\),

\[x\log y=x\log x+x\log\frac{y}{x}\.\]

Now \(x\log\frac{y}{x}=y\phi(\frac{x}{y})\) where \(\phi(t)\equiv-t\log t\), \(t>0\). It can be verified \(\phi(t)\) attains its maximum \(\frac{1}{e}\) at \(t=\frac{1}{e}\). Thus

\[x\log y\leq x\log x+\frac{y}{e}.\]So

\[EX_{n}^{+}\log M_{n}^{+}\leq EX_{n}^{+}\log X_{n}^{+}+\frac{EM_{n}^{+}}{e}. \tag{2.22}\]

If \(EX_{n}^{+}\log X_{n}^{+}=\infty\), (2.20) is trivially true. If \(EX_{n}^{+}\log X_{n}^{+}<\infty\), then as in the proof of Theorem 13.2.11, it can be shown that \(EM_{n}^{+}<\infty\). Hence, the theorem follows from (2.21) and (2.22). \(\Box\)

A special case of Theorem 13.2.10 is the maximal inequality of Kolmogorov (cf. Section 8.3) as shown by the following example.

**Example 13.2.4:** Let \(\{Y_{n}\}_{n\geq 1}\) be a sequence of independent random variables with \(EY_{n}=0\) and \(E|Y_{n}|^{\alpha}<\infty\) for all \(n\geq 1\) for some \(\alpha\in(1,\infty)\). Let \(S_{n}=Y_{1}+\ldots+Y_{n},n\geq 1\). Then, \(\phi(x)\equiv|x|^{\alpha}\), \(x>0\) is a convex function, and hence, by Proposition 13.1.1, \(X_{n}\equiv\phi(|S_{n}|)\), \(n\geq 1\) is a sub-martingale w.r.t. \({\cal F}_{n}=\sigma\langle\{Y_{1},\ldots,Y_{n}\}\rangle\), \(n\geq 1\). Now, by Theorem 13.2.10, for any \(x>0\) and \(m\geq 1\),

\[P\Big{(}\max_{1\leq n\leq m}|S_{n}|>x\Big{)} = P\Big{(}\max_{1\leq n\leq m}X_{n}>x^{\alpha}\Big{)} \tag{2.23}\] \[\leq x^{-\alpha}EX_{m}^{+}\] \[\leq x^{-\alpha}E|S_{m}|^{\alpha}.\]

Kolmogorov's inequality corresponds to the case where \(\alpha=2\).

Another application of the optimal stopping theorem yields the following useful result.

**Theorem 13.2.14:** (_Wald's lemmas_). _Let \(\{Y_{n}\}_{n\geq 1}\) be a sequence of iid random variables and let \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration such that_

_(i)_ \(Y_{n}\) _is_ \(F_{n}\)_-measurable and (ii)_ \(F_{n}\) _and_ \(\sigma\langle\{Y_{k}:k\geq n+1\}\rangle\) _are independent for all_ \(n\geq 1\)_._ (2.24)

_Also, let \(T\) be a finite stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) and \(E|T|<\infty\). Let \(S_{n}=Y_{1}+\ldots+Y_{n},n\geq 1\). Then,_

* \(E|Y_{1}|<\infty\) _implies_ \[ES_{T}=(EY_{1})(ET).\] (2.25)
* \(EY_{1}^{2}<\infty\) _implies_ \[E(S_{T}-TEY_{1})^{2}=\mbox{Var}(Y_{1})E(T).\] (2.26)

**Proof:** W.l.o.g., suppose that \(EY_{1}=0\). Then, \(\{S_{n},{\cal F}_{n}\}_{n\geq 1}\) is a martingale. By Corollary 13.2.7, (2.25) would follow if one showed that (2.10) holds with \(X_{n}=S_{n}\) and that \(E|S_{T}|<\infty\). Since \(|S_{n}|\leq\sum_{i=1}^{n}|Y_{i}|\leq\sum_{i=1}^{T}|Y_{i}|\) on the set \(\{T\geq n\}\), both these conditions would hold if \(E(\sum_{i=1}^{T}|Y_{i}|)<\infty\). Now,by the MCT and the independence of \(Y_{i}\) and \(\{T\geq i\}=\{T\leq i-1\}^{c}\in{\cal F}_{i-1}\) for \(i\geq 2\) and the fact that \(\{T\geq 1\}=\Omega\), it follows that

\[E\sum_{i=1}^{T}|Y_{i}| = E\bigg{(}\sum_{i=1}^{\infty}|Y_{i}|I(i\leq T)\bigg{)} \tag{2.27}\] \[= \sum_{i=1}^{\infty}E|Y_{i}|I(i\leq T)\] \[= E|Y_{1}|\sum_{i=1}^{\infty}P(T\geq i)\] \[= E|Y_{1}|E|T|<\infty.\]

This proves (a).

To prove (b), set \(\sigma^{2}={\rm Var}(Y_{1})\) and note that \(EY_{1}=0\Rightarrow\{S_{n}^{2}-n\sigma^{2},{\cal F}_{n}\}_{n\geq 1}\) is a martingale. Let \(T_{n}=T\wedge n,n\geq 1\). Then, \(T_{n}\) is a bounded stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) and hence, by Theorem 13.2.6,

\[E[S_{T_{n}}^{2}-(ET_{n})\sigma^{2}]=E(S_{1}^{2}-\sigma^{2})=0\mbox{ for all }n\geq 1. \tag{2.28}\]

Thus, (2.26) holds with \(T\) replaced by \(T_{n}\). Since \(T\) is a finite stopping time, \(T_{n}\uparrow T<\infty\) w.p. 1 and therefore, \(S_{T_{n}}\to S_{T}\) as \(n\to\infty\), w.p. 1. Now applying Fatou's lemma and the MCT, from (2.28), one gets

\[ES_{T}^{2}\leq\liminf_{n\to\infty}ES_{T_{n}}^{2}=\liminf_{n\to\infty}\,(ET_{n} )\sigma^{2}=(ET)\sigma^{2}. \tag{2.29}\]

Also, note that for any \(n\geq 1\)

\[E(S_{T}^{2}-S_{T_{n}}^{2}) = E(S_{T}^{2}-S_{n}^{2})I(T>n) \tag{2.30}\] \[= E[(S_{T}-S_{n})^{2}+2S_{n}(S_{T}-S_{n})]I(T>n)\] \[\geq 2ES_{n}(S_{T}-S_{n})I(T>n)\] \[= 2ES_{n}(S_{T_{1n}}-S_{n})\] \[= 2E[S_{n}\cdot E\{(S_{T_{1n}}-S_{n})|{\cal F}_{n}\}],\]

where \(T_{1n}=\max\{T,n\}\). Since \(ET_{1n}\leq ET+n<\infty\), and \(\{T_{1n}>k\}=\{T>k\}\) for all \(k>n\), the conditions of Theorem 13.2.6 hold with \(X_{n}=S_{n},S=n\) and \(T=T_{1n}\). Hence, \(E(S_{T_{1n}}-S_{n}|F_{n})=0\) a.s. and by (2.30), \(ES_{T}^{2}\geq ES_{T_{n}}^{2}\) for all \(n\geq 1\). Now letting \(n\to\infty\) and using (2.28), one gets \(ES_{T}^{2}\geq(ET)\sigma^{2}\), as in (2.29). This completes the proof of (b). \(\Box\)

This section is concluded with the statement of an inequality relating the \(p\)th moment of a martingale to the \((p/2)\)th moment of its squared variation.

**Theorem 13.2.15:** (_Burkholder's inequality_)_. Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a martingale sequence. Let \(\xi_{j}=X_{j}-X_{j-1}\), \(\alpha\geq 1\), with \(X_{0}=0\). Then for any \(p\in[2,\infty)\), there exist positive constants \(A_{p}\) and \(B_{p}\) such that_

\[E|X_{n}|^{p}\leq A_{p}\,E\bigg{(}\sum_{i=1}^{n}\xi_{i}^{2}\bigg{)}^{p/2}\]

_and_

\[E|X_{n}|^{p}\leq B_{p}\bigg{\{}E\bigg{(}\sum_{i=1}^{n}E\big{(}\xi_{i}^{2}| \mathcal{F}_{i-1}\big{)}\bigg{)}^{p/2}+\sum_{i=1}^{n}E|\xi_{i}|^{p}\bigg{\}}.\]

For a proof, see Chow and Teicher (1997).

### 13.3 Martingale convergence theorems

The martingale (or sub- or super-martingale) property of a sequence of random variables \(\{X_{n}\}_{n\geq 1}\) implies, under some mild additional conditions, a remarkable regularity, namely, that \(\{X_{n}\}_{n\geq 1}\) converges w.p. 1 as \(n\to\infty\). For example, any nonnegative super-martingale converges w.p. 1. Also any sub-martingale \(\{X_{n}\}_{n\geq 1}\) for which \(\{E|X_{n}|\}_{n\geq 1}\) is bounded converges w.p. 1. Further, if \(\{E|X_{n}|^{p}\}_{n\geq 1}\) is bounded for some \(p\in(1,\infty)\), then \(X_{n}\) converges w.p. 1 and in \(L^{p}\) as well.

The proof of these assertions depend crucially on an ingenious inequality due to Doob. Recall that one way to prove that a sequence of real numbers \(\{x_{n}\}_{n\geq 1}\) converges as \(n\to\infty\) is to show that it does not oscillate too much as \(n\to\infty\). That is, for all \(a<b\), the number of times the sequence goes from below \(a\) to above \(b\) is finite. This number is referred to as the number of upcrossings from \(a\) to \(b\). Doob's upcrossing lemma (see Theorem 13.3.1 below) shows that for a sub-martingale, the mean of the upcrossings can be bounded above. First, a formal definition of upcrossings of a given sequence \(\{x_{j}:1\leq j\leq n\}\) of real numbers from level \(a\) to level \(b\) with \(a<b\) is given.

Let

\[N_{1} = \min\{j:1\leq j\leq n,x_{j}\leq a\}\] \[N_{2} = \min\{j:N_{1}<j\leq n,x_{j}\geq b\}\]

and, define recursively,

\[N_{2k-1} = \min\{j:N_{2k-2}<j\leq n,x_{j}\leq a\}\] \[N_{2k} = \min\{j:N_{2k-1}<j\leq n,x_{j}\geq b\}.\]

If any of these sets on the right side is empty, all subsequent ones will be empty as well and the corresponding \(N_{k}\)'s will not be well defined. If \(N_{1}\) or \(N_{2}\) is not well defined, then set \(U\big{\{}\{x_{j}\}_{j=1}^{n};a,b\big{\}}\), the _number of upcrossings of the interval \((a,b)\) by \(\{x_{j}\}_{j=1}\)_ equal to zero. Otherwise let \(N_{\ell}\) be the last one that is well defined. Set \(U\big{\{}\{x_{j}\}_{j=1}^{n};\,a,b\big{\}}=\frac{\ell}{2}\) if \(\ell\) is even and \(\frac{\ell-1}{2}\) if \(\ell\) is odd.

**Theorem 13.3.1:** (_Doob's upcrossing lemma_). _Let \(\{X_{j},{\cal F}_{j}\}_{j=1}^{n}\) be a submartingale and let \(a<b\) be real numbers. Let \(U_{n}\equiv U\big{\{}\{X_{j}\}_{j=1}^{n};\,a,b\big{\}}\). Then_

\[EU_{n}\leq\frac{E(X_{n}-a)^{+}-E(X_{1}-a)^{+}}{(b-a)}\leq\frac{EX_{n}^{+}+|a|}{ (b-a)}. \tag{3.1}\]

**Proof:** Consider first the special case when \(X_{j}\geq 0\) w.p. \(1\) for all \(j\geq 1\) and \(a=0\). Let \(\tilde{N}_{0}=1\). Let

\[\tilde{N}_{j}=\left\{\begin{array}{ll}N_{j}&\mbox{if}\ \ j=2k,\ k\leq U_{n}\ \ \mbox{or if}\\ &j=2k-1,\ k\leq U_{n},\\ n&\mbox{otherwise}.\end{array}\right.\]

If \(j\) is odd and \(j+1\leq 2U_{n}\), then

\[X_{\tilde{N}_{j+1}}\geq b>0.\]

If \(j\) is odd and \(j+1\geq 2U_{n}+2\), then

\[X_{\tilde{N}_{j+1}}=X_{n}=X_{\tilde{N}_{j}}.\]

Thus \(\sum_{j}\mbox{odd}(X_{\tilde{N}_{j+1}}-X_{\tilde{N}_{j}})\geq bU_{n}\). It is easy to check that \(\{\tilde{N}_{j}\}_{j=1}^{n}\) are stopping times. By Theorem 13.2.6,

\[E(X_{\tilde{N}_{j+1}}-X_{\tilde{N}_{j}})\geq 0\quad\mbox{for}\quad j=1,2,\ldots,n.\]

Thus,

\[E(X_{n}-X_{1}) = E\bigg{(}\sum_{j=0}^{n-1}(X_{\tilde{N}_{j+1}}-X_{\tilde{N}_{j}}) \bigg{)} \tag{3.2}\] \[\geq bEU_{n}+E\bigg{(}\sum_{j}\sum_{\mbox{even}}(X_{\tilde{N}_{j+1}}-X _{\tilde{N}_{j}})\bigg{)}\] \[\geq bEU_{n}.\]

Hence, both inequalities of (3.1) hold for the special case.

Now for the general case, let \(Y_{j}\equiv(X_{j}-a)^{+}\), \(1\leq j\leq n\). Then \(\{Y_{j},{\cal F}_{j}\}_{j=1}^{n}\) is a nonnegative sub-martingale and \(U_{n}\big{\{}\{Y_{j}\}_{j=1}^{n},0,b-a\big{\}}\equiv U_{n}\big{\{}\{X_{j}\}_{j= 1}^{n},a,b\big{\}}\). Thus, from (3.2)

\[EU_{n} \leq \frac{E(Y_{n}-Y_{1})}{(b-a)}\] \[= \frac{E((X_{n}-a)^{+})-E((X_{1}-a)^{+})}{(b-a)}\,\]proving the first inequality of (3.1). The second inequality follows by noting that \((x-a)^{+}\leq x^{+}+|a|\) for any \(x,a\in\mathbb{R}\). \(\Box\)

The first convergence theorem is an easy consequence of the above theorem.

**Theorem 13.3.2:**_Let \(\{X_{n},\mathcal{F}_{n}\}_{n\geq 1}\) be a sub-martingale such that_

\[\sup_{n\geq 1}EX_{n}^{+}<\infty.\]

_Then \(\{X_{n}\}_{n\geq 1}\) converges to a finite limit \(X_{\infty}\) w.p. 1 and \(E|X_{\infty}|<\infty\)._

**Proof:** Let

\[A=\{\omega:\liminf_{n\to\infty}X_{n}<\limsup_{n\to\infty}X_{n}\},\]

and for \(a<b\), let

\[A(a,b)=\{\omega:\liminf_{n\to\infty}X_{n}<a<b<\limsup_{n\to\infty}X_{n}\}.\]

Then, \(A=\cup A(a,b)\) where the union is taken over all rationals \(a,b\) such that \(a<b\). To establish convergence of \(\{X_{n}\}_{n\geq 1}\) it suffices to show that \(P(A(a,b))=0\) for each \(a<b\), as this implies \(P(A)=0\). Fix \(a<b\) and let \(U_{n}=U\big{\{}\{X_{j}\}_{j=1}^{n};\ a,b\big{\}}\). For \(\omega\in A(a,b)\), \(U_{n}\to\infty\) as \(n\to\infty\). On the other hand, by the upcrossing lemma

\[EU_{n}\leq\frac{EX_{n}^{+}+|a|}{(b-a)}\]

and by hypothesis, \(\sup_{n\geq 1}EX_{n}^{+}<\infty\), implying that

\[\sup_{n\geq 1}EU_{n}<\infty.\]

By the MCT, \(E\big{[}\lim_{n\to\infty}U_{n}\big{]}=\lim_{n\to\infty}EU_{n}\), and hence

\[\lim_{n\to\infty}U_{n}<\infty\quad\mbox{w.p. 1}.\]

Thus, \(P(A(a,b))=0\) for all \(a<b\), and hence \(\lim_{n\to\infty}X_{n}=X_{\infty}\) exists w.p. 1. By Fatou's lemma

\[E|X_{\infty}|\leq\lim_{n\to\infty}E|X_{n}|\leq\sup_{n\geq 1}E|X_{n}|\.\]

But \(E|X_{n}|=2E(X_{n}^{+})-EX_{n}\leq 2EX_{n}^{+}-EX_{1}\), as \(\{X_{n},\mathcal{F}_{n}\}_{n\geq 1}\) a sub-martingale implies \(EX_{n}\geq EX_{1}\). Thus, \(\sup_{n\geq 1}EX_{n}^{+}<\infty\) implies \(\sup_{n\geq 1}E|X_{n}|<\infty\). So, \(E|X_{\infty}|<\infty\) and hence \(|X_{\infty}|<\infty\) w.p. 1. \(\Box\)

**Corollary 13.3.3:**_Let \(\{X_{n},\mathcal{F}_{n}\}_{n\geq 1}\) be a nonnegative super-martingale. Then \(\{X_{n}\}_{n\geq 1}\) converges to a finite limit w.p. 1._

**Proof:** Since \(\{-X_{n},\mathcal{F}_{n}\}_{n\geq 1}\) is a nonpositive sub-martingale, \(\sup_{n\geq 1}E(-X_{n})^{+}=0<\infty\). By Theorem 13.3.2, \(\{-X_{n}\}_{n\geq 1}\) converges to a finite limit w.p. 1. \(\Box\)

**Corollary 13.3.4:** _Every nonnegative martingale converges w.p. 1._

A natural question is that if a sub-martingale converges w.p. 1 to a finite limit, does it do so in \(L^{1}\) or in \(L^{p}\) for \(p>1\). It turns out that if a sub-martingale is \(L^{p}\) bounded for some \(p>1\), then it converges in \(L^{p}\). But this is false for \(p=1\) as the following examples show.

**Example 13.3.1:** (_Gambler's ruin problem_). Let \(\{S_{n}\}_{n\geq 1}\) be the simple symmetric random walk, i.e., \(S_{n}=\sum_{i=1}^{n}\xi_{i}\), \(n\geq 1\), where \(\{\xi_{n}\}_{n\geq 1}\) is a sequence of iid random variables with \(P(\xi_{1}=1)=\frac{1}{2}=P(\xi_{1}=-1)\). Let

\[N=\inf\{n:n\geq 1,S_{n}=1\}.\]

As noted earlier, \(N\) is a finite stopping time and that \(\{S_{n}\}_{n\geq 1}\) is a martingale. Let \(X_{n}=S_{N\wedge n}\), \(n\geq 1\). Then by the optional sampling theorem, \(\{X_{n}\}_{n\geq 1}\) is a martingale. Clearly, \(\lim_{n\to\infty}X_{n}\equiv X_{\infty}=S_{N}=1\) exists w.p. 1. But \(EX_{n}\equiv 0\) while \(EX_{\infty}=1\) and so \(X_{n}\) does not converge to \(X_{\infty}\) in \(L^{1}\).

**Example 13.3.2:** Suppose that \(\{\xi_{n}\}_{n\geq 1}\) is a sequence of iid nonnegative random variables with \(E\xi_{1}=1\). Let \(X_{n}=\Pi_{i=1}^{n}\xi_{i}\), \(n\geq 1\). Then \(\{X_{n}\}_{n\geq 1}\) is a nonnegative martingale and hence converges w.p. 1 to \(X_{\infty}\), say. If \(P(\xi_{1}=1)<1\), it can be shown that \(X_{\infty}=0\) w.p. 1. Thus, \(X_{n}\nrightarrow X_{\infty}\) in \(L^{1}\). In particular, \(\{X_{n}\}_{n\geq 1}\) is not UI (Problem 13.19).

**Example 13.3.3:** If \(\{Z_{n}\}_{n\geq 0}\) is a branching process with offspring distribution \(\{p_{j}\}_{j\geq 0}\) and mean \(m=\sum_{j=1}^{\infty}jp_{j}\) then \(X_{n}\equiv Z_{n}/m^{n}\) (cf. 1.9) is a nonnegative martingale and hence \(\lim_{n}X_{n}=X_{\infty}\) exists w.p. 1. It is known that \(X_{n}\) converges to \(X_{\infty}\) in \(L^{1}\) iff \(m>1\) and \(\sum_{j=1}^{\infty}j\log p_{j}<\infty\) (cf. Chapter 18). See also Athreya and Ney (2004).

**Theorem 13.3.5:** _Let \(\{X_{n},\mathcal{F}_{n}\}_{n\geq 1}\) be a sub-martingale. Then the following are equivalent:_

* _There exists a random variable_ \(X_{\infty}\) _in_ \(L^{1}\) _such that_ \(X_{n}\to X_{\infty}\) _in_ \(L^{1}\)_._
* \(\{X_{n}\}_{n\geq 1}\) _is uniformly integrable._

**Proof:** Clearly, (i) \(\Rightarrow\) (ii) for any sequence of integrable random variables \(\{X_{n}\}_{n\geq 1}\). Conversely, if (ii) holds, then \(\{E|X_{n}|\}_{n\geq 1}\) is bounded and hence by Theorem 13.3.2, \(X_{n}\to X_{\infty}\) w.p. 1 and by uniform integrability, \(X_{n}\to X_{\infty}\) in \(L^{1}\), i.e., (i) holds.

**Remark 13.3.1:** Let (ii) of Theorem 13.3.5 hold. For any \(A\in{\cal F}_{n}\) and \(m>n\), by the sub-martingale property

\[E(X_{n}I_{A})\leq E(X_{m}I_{A}).\]

By uniform integrability, for any \(A\in{\cal F}\),

\[EX_{n}I_{A}\to EX_{\infty}I_{A}\quad\mbox{as}\quad n\to\infty.\]

This implies that \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\ \cup\{X_{\infty},{\cal F}_{\infty}\}\) is a sub-martingale, where \({\cal F}_{\infty}=\sigma(\bigcup_{n\geq 1}{\cal F}_{n})\). That is, the sub-martingale is _closable_ at right. Further, \(EX_{n}\to EX_{\infty}\). Conversely, it can be shown that if there exists a random variable \(X_{\infty}\), measurable w.r.t. \({\cal F}_{\infty}\), such that

* \(E|X_{\infty}|<\infty\),
* \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\ \cup\{X_{\infty},{\cal F}_{\infty}\}\) is a sub-martingale, and
* \(EX_{n}\to EX_{\infty}\),

then by (a) and (b), \(\{X_{n}\}_{n\geq 1}\) is uniformly integrable and (i) of Theorem 13.3.5 holds.

**Corollary 13.3.6:**_If \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) is a martingale, then it is closable at right iff \(\{X_{n}\}_{n\geq 1}\) is uniformly integrable iff \(X_{n}\) converges in \(L^{1}\)._

This follows from the previous remark since for a martingale, \(EX_{n}\) is constant for \(1\leq n\leq\infty\).

**Remark 13.3.2:** A sufficient condition for a sequence \(\{X_{n}\}_{n\geq 1}^{\infty}\) of random variables to be uniformly integrable is that there exists a random variable \(M\) such that \(EM<\infty\) and \(|X_{n}|\leq M\) w.p. 1 for all \(n\geq 1\). Suppose that \(\{X_{n}\}_{n\geq 1}\) is a nonnegative sub-martingale and \(M=\sup_{n\geq 1}X_{n}=\lim_{n\to\infty}M_{n}\) where \(M_{n}=\sup_{1\leq j\leq n}X_{j}\). By the MCT, \(EM=\lim_{n\to\infty}EM_{n}\). But by Doob's \(L\log L\) maximal inequality (Theorem 13.2.13),

\[EM_{n}\leq\frac{e}{e-1}\Big{[}1+E\big{(}X_{n}(\log X_{n})^{+}\big{)}\Big{]},\]

for all \(n\geq 1\). Thus, if \(\{X_{n}\}_{n\geq 1}\) is a nonnegative sub-martingale and \(\sup_{n\geq 1}E(X_{n}(\log X_{n})^{+})<\infty\), then \(EM<\infty\) and hence \(\{X_{n}\}_{n\geq 1}\) is uniformly integrable and converges w.p. 1 and in \(L^{1}\). Similarly, if \(\{X_{n}\}_{n\geq 1}\) is a martingale such that \(\sup_{n\geq 1}E(|X_{n}|(\log|X_{n}|)^{+})<\infty\), then \(\{X_{n}\}_{n\geq 1}\) is uniformly integrable.

\(L^{1}\) Convergence of the Doob Martingale

**Definition 13.3.1:** Let \(X\) be a random variable on a probability space \((\Omega,{\cal F},P)\) and \(\{{\cal F}_{n}\}_{n\geq 1}\) a filtration in \({\cal F}\). Let \(E|X|<\infty\) and\(E(X|{\cal F}_{n}),n\geq 1\). Then \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) is called a _Doob martingale_ (cf. Example 13.1.3).

For a Doob martingale, \(E|X_{n}|\leq E|X|\) and it can be shown that \(\{X_{n}\}_{n\geq 1}\) is uniformly integrable (Problem 13.20). Hence, \(\lim_{n\to\infty}X_{n}\) exists w.p. 1 and in \(L^{1}\), and equals \(E(X|{\cal F}_{\infty})\), where \({\cal F}_{\infty}=\sigma(\bigcup_{n\geq 1}{\cal F}_{n})\). This may be summarized as:

**Theorem 13.3.7:** _Let \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration and let \(X\) be an \({\cal F}_{\infty}\)-measurable with \(E|X|<\infty\). Then_

\[E(X|{\cal F}_{n})\to X\quad\mbox{w.p. 1 and in}\quad L^{1}.\]

**Corollary 13.3.8:** _Let \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration and \({\cal F}_{\infty}=\sigma(\bigcup_{n\geq 1}{\cal F}_{n})\)._

* _For any_ \(A\in{\cal F}_{\infty}\)_, one has_ \[P(A|{\cal F}_{n})\to I_{A}\quad\mbox{w.p. 1.}\]
* _For any random variable_ \(X\) _with_ \(E|X|<\infty\)_,_ \[E(X|{\cal F}_{n})\to E(X|{\cal F}_{\infty})\quad\mbox{w.p. 1.}\]

**Proof:** Take \(X=I_{A}\) for (i) and in Theorem 13.3.7, replace \(X\) by \(E(X|{\cal F}_{\infty})\) for (ii). \(\Box\)

Kolmogorov's zero-one law (Theorem 7.2.4) is an easy consequence of this. If \(\{\xi_{n}\}_{n\geq 1}\) are independent random variables and \(A\) is a tail event and \({\cal F}_{n}\equiv\sigma(\xi_{j}:1\leq j\leq n)\), then \(P(A|{\cal F}_{n})=P(A)\) for each \(n\) and hence \(P(A)=I_{A}\) w.p. 1, i.e., \(P(A)=0\) or 1.

**Theorem 13.3.9:** (\(L^{p}\) _convergence of sub-martingales, \(p>1\)_). Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a nonnegative sub-martingale. Let \(1<p<\infty\) and \(\sup_{n\geq 1}E|X_{n}|^{p}<\infty\). Then \(\lim_{n\to\infty}X_{n}=X_{\infty}\) exists w.p. 1 and in \(L^{p}\), and \(\{(X_{n},{\cal F}_{n})\}_{n\geq 1}\ \cup\{X_{\infty},{\cal F}_{\infty}\}\) is a \(L^{p}\)-bounded sub-martingale._

**Proof:** By Doob's maximal \(L^{p}\) inequality (Theorem 13.2.11), for any \(n\geq 1\),

\[EM^{p}_{n}\leq\left(\frac{p}{p-1}\right)^{p}EX^{p}_{n}\leq\left(\frac{p}{p-1} \right)^{p}\sup_{m\geq 1}EX^{p}_{m}, \tag{3.3}\]

where \(M_{n}=\max\{X_{j}:1\leq j\leq n\}\). Let \(M=\lim_{n\to\infty}M_{n}\). Then (3.3) yields

\[EM^{p}<\infty\.\]

This makes \(\{|X_{n}|^{p}\}_{n\geq 1}\) uniformly integrable. Also \(\sup_{n\geq 1}E|X_{n}|^{p}<\infty\) and \(p>1\Rightarrow\sup_{n}E|X_{n}|<\infty\) and hence, \(\lim_{n\to\infty}X_{n}=X_{\infty}\) exists w.p. 1 as a finite limit. The uniform integrability of \(\{|X_{n}|^{p}\}_{n\geq p}\) implies \(L^{p}\) convergence (cf. Problem 2.36). The closability also follows as in Remark 13.3.1. \(\Box\)

**Corollary 13.3.10:** _Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a martingale. Let \(1<p<\infty\) and \(\sup_{n\geq 1}E|X_{n}|^{p}<\infty\). Then the conclusions of Theorem 13.3.9 hold._

**Proof:** Since \(\{Y_{n}\equiv|X_{n}|,{\cal F}_{n}\}_{n\geq 1}\) is a nonnegative sub-martingale, Theorem 13.3.9 applies. \(\Box\)

**Reversed Martingales**

**Definition 13.3.2:** Let \(\{X_{n},{\cal F}_{n}\}_{n\leq-1}\) be an adapted family with \((\Omega,{\cal F},P)\) as the underlying probability space, i.e., for \(n<m,{\cal F}_{n}\subset{\cal F}_{m}\subset{\cal F}\) and \(X_{n}\) is \({\cal F}_{n}\)-measurable for each \(n\leq-1\). Such a sequence is called a _reversed martingale_ if

* \(E|X_{n}|<\infty\) for all \(n\leq-1\),
* \(E(X_{n+1}|{\cal F}_{n})=X_{n}\) for all \(n\leq-1\).

The definitions of _reversed sub-_ and _super-martingales_ are similar.

Reversed martingales are well behaved since they are closed at right.

**Theorem 13.3.11:** _Let \(\{X_{n},{\cal F}_{n}\}_{n\leq-1}\) be a reversed martingale. Then_

* \(\lim_{n\to-\infty}X_{n}=X_{-\infty}\) _exists w.p. 1 and in_ \(L^{1}\)_,_
* \(X_{-\infty}=E(X_{-1}|{\cal F}_{-\infty})\)_, where_ \({\cal F}_{-\infty}\equiv\bigcap_{n\leq-1}{\cal F}_{n}\)_._

**Proof:** Fix \(a<b\). For \(n\leq-1\), let \(U_{n}\) be the number of \((a,b)\) upcrossings of \(\{X_{j}:n\leq j\leq-1\}\). Then by Doob's upcrossing lemma (Theorem 13.3.1),

\[EU_{n}\leq\frac{E(X_{1}-a)^{+}}{(b-a)}\.\]

Let \(U=\lim_{n\to-\infty}U_{n}\). Letting \(n\to-\infty\), by the MCT, it follows that

\[EU<\infty.\]

Thus, \(U<\infty\) w.p. 1. This being true for every \(a<b\), one may conclude as in Theorem 13.3.2 that \(P(\overline{\lim}_{n\to-\infty}X_{n}>\underline{\lim}_{n\to-\infty}X_{n})=0\). So \(\lim_{n\to-\infty}X_{n}=X_{-\infty}\) exists w.p. 1. Also, by Jensen's inequality, \(\{X_{n}\}_{n\leq-1}\) is uniformly integrable. So \(X_{n}\to X_{-\infty}\) in \(L^{1}\), proving (a).

To prove (b), note that for any \(A\in{\cal F}_{-\infty}\), by uniform integrability,

\[\int_{A}X_{-\infty}dP = \lim_{n\to-\infty}\int_{A}X_{-n}dP\] \[= \int_{A}X_{-1}dP,\quad\mbox{by the martingale property.}\]

**Corollary 13.3.12:** (_The Strong Law of Large Numbers for iid random variables_). _Let \(\{\xi_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(E|\xi_{1}|<\infty.\) Then, \(n^{-1}\sum_{i=1}^{n}\xi_{i}\to E\xi_{1}\) as \(n\to\infty\), w.p. 1._

**Proof:** For \(k\geq 1,\) let \(S_{k}=\xi_{1}+\cdots+\xi_{k}\) and

\[{\cal F}_{-k}=\sigma\langle\{S_{k},\xi_{k+1},\xi_{k+2},\ldots\}\rangle.\]

Let \(X_{n}\equiv E(\xi_{1}|{\cal F}_{n})_{n\leq-1}.\) By the independence of \(\{\xi_{i}\}_{i\geq 1},\) for any \(n\leq-1,\) with \(k=-n,\)

\[X_{n}=E(\xi_{1}|\sigma\langle S_{k}\rangle).\]

Also, by symmetry, for any \(k\geq 1,\)

\[E(\xi_{1}|\sigma\langle S_{k}\rangle)=E(\xi_{j}|\sigma\langle S_{k}\rangle)\mbox { for }1\leq j\leq k.\]

Thus, \(X_{n}=\frac{1}{k}\sum_{j=1}^{k}E(\xi_{j}|\sigma\langle S_{k}\rangle)=\frac{S_{ k}}{k},\) for all \(k=-n\geq 1.\) It is easy to check that \(\{X_{n},{\cal F}_{n}\}_{n\leq-1}\) is a reversed martingale and so by Theorem 13.3.11,

\[\lim_{n\to-\infty}X_{n}=\lim_{k\to\infty}\frac{S_{k}}{k}\quad\mbox{exists w.p. 1 and in}\quad L^{1}.\]

By Kolmogorov's zero-one law, \(\lim_{k\to\infty}\frac{S_{k}}{k}\) is a tail random variable, and so a constant, which by \(L^{1}\) convergence must equal \(E\xi_{1}.\)\(\Box\)

### Applications of martingale methods

#### 13.4.1 Supercritical branching processes

Recall Example 13.1.5 on branching processes. Assume that it is supercritical, i.e., \(\mu=E\xi_{11}>1\) and that \(\sigma^{2}=\mbox{Var}(\xi_{11})<\infty.\)

**Proposition 13.4.1:**_Let \(X_{n}=\mu^{-n}Z_{n}\) be the martingale defined in (1.9). Then, \(\{X_{n}\}_{n\geq 1}\) is an \(L^{2}\)-bounded martingale._

**Proof:** Let \(v_{n}=\mbox{Var}(X_{n}),\)\(n\geq 1.\) Then

\[v_{n+1} = \mbox{Var}(E(X_{n+1}|{\cal F}_{n}))+E(\mbox{Var}(X_{n+1}|{\cal F} _{n}))\] \[= \mbox{Var}(X_{n})+\frac{E(Z_{n}\sigma^{2})}{\mu^{2(n+1)}}\] \[= v_{n}+\sigma^{2}\mu^{-2}\mu^{-n},\ n\geq 1.\]

Thus, \(v_{n+1}=\sigma^{2}\mu^{-2}\sum_{j=1}^{n}\mu^{-j}.\) Since \(\mu>1,\)\(\{v_{n}\}_{n\geq 1}\) is bounded. Now since \(EX_{n}\equiv 1,\{X_{n}\}_{n\geq 1}\) is \(L_{2}\)-bounded. \(\Box\)

A direct consequence of Proposition 13.4.1 and Theorem 13.3.8 is that \(\lim_{n\to\infty}X_{n}=X_{\infty}\) exists w.p. 1 and in mean-square.

[MISSING_PAGE_FAIL:435]

If \(s_{n}\to\infty\), then by Kronecker's lemma (cf. Lemma 8.4.2).

\[\frac{1}{s_{n}}\sum_{j=1}^{n}(\delta_{j}-p_{j})\to 0\] \[\Rightarrow\biggl{(}\frac{\sum_{j=1}^{n}\delta_{j}}{\sum_{j=1}^{n} p_{j}}-1\biggr{)}\frac{\sum_{j=1}^{n}p_{j}}{s_{n}}\to 0\.\]

But \(\sum_{j=1}^{n}p_{j}\geq s_{n}\) and hence

\[\frac{\sum_{j=1}^{n}\delta_{j}}{\sum_{j=1}^{n}p_{j}}\to 1\quad\text{w.p. 1 on the event}\quad B\equiv\{s_{n}\to\infty\}. \tag{4.1}\]

Next it is claimed that _on_\(B^{c}\equiv\{\lim_{n\to\infty}s_{n}<\infty\}\)_,_\(\lim_{n\to\infty}X_{n}=X\)_exists and is finite w.p. 1_. To prove the claim, fix \(0<t<\infty\). Let \(N_{t}=\inf\{n:s_{n+1}>t\}\). Since \(s_{n+1}\) is \(\mathcal{F}_{n}\)-measurable, \(N_{t}\) is a stopping time and by the optional stopping theorem I (Theorem 13.2.3), \(\{Z_{n}\equiv X_{N_{t}\wedge n}\}_{n\geq 1}\) is a martingale. By Doob's \(L^{2}\)-maximal inequality,

\[E\Bigl{(}\sup_{1\leq j\leq n}Z_{j}^{2}\Bigr{)}\leq 4E(Z_{n}^{2}).\]

Also it is easy to verify that \(\{X_{n}^{2}-s_{n}^{2}\}_{n\geq 1}\) is a martingale and by the optional sampling theorem (Theorem 13.2.4),

\[E(X_{n\wedge N_{t}}^{2}-s_{n\wedge N_{t}}^{2})=0.\]

Thus, \(EZ_{n}^{2}=Es_{n\wedge N_{t}}^{2}\leq t\) and hence for each \(t\), \(\lim_{n\to\infty}Z_{n}\) exists w.p. 1 and in \(L_{2}\). Thus, \(\lim_{n\to\infty}X_{N_{t}\wedge n}\) exists w.p. 1 for each t. But, on \(B^{c}\), \(N_{t}=\infty\) for all large \(t\). So \(\lim_{n\to\infty}X_{n}=X\) exists w.p. 1 on \(B^{c}\). This proves the claim.

It follows that on \(B^{c}\cap\{\sum_{j=1}^{\infty}p_{j}=\infty\}\),

\[\frac{\sum_{j=1}^{n}\delta_{j}}{\sum_{j=1}^{n}p_{j}}-1=\frac{X_{n}}{\sum_{j=1} ^{n}p_{j}}\to 0.\]

Also, since \(B\equiv\{s_{n}\to\infty\}\) is a subset of \(\{\sum_{j=1}^{\infty}p_{j}=\infty\}\) and it has been shown in (4.1) that

\[\frac{\sum_{j=1}^{n}\delta_{j}}{\sum_{j=1}^{n}p_{j}}\to 1\quad\text{w.p. 1 on}\quad B,\]

it follows that

\[\frac{\sum_{j=1}^{n}\delta_{j}}{\sum_{j=1}^{n}p_{j}}\to 1\quad\text{w.p. 1 on}\quad\biggl{\{}\sum_{j=1}^{\infty}p_{j}=\infty\biggr{\}}.\]Summarizing the above, one gets the following result.

**Theorem 13.4.2:** (_A conditional Borel-Cantelli lemma_). _Let \(\{A_{n}\}_{n\geq 1}\) be a sequence of events in a probability space \((\Omega,\mathcal{F},P)\) and \(\{\mathcal{F}_{n}\}_{n\geq 1}\) be a filtration such that \(A_{n}\in\mathcal{F}_{n}\), for all \(n\geq 1\). Let \(p_{n}=P(A_{n}|\mathcal{F}_{n-1})\) for \(n\geq 2\), \(p_{1}=P(A_{1})\). Then on the event \(B_{0}\equiv\{\sum_{j=1}^{\infty}p_{j}=\infty\}\),_

\[\frac{\sum_{j=1}^{n}I_{A_{j}}}{\sum_{j=1}^{n}p_{j}}\to 1\quad\mbox{w.p. 1},\]

_and in particular, infinitely many \(A_{n}\)'s happen w.p. 1 on \(B_{0}\)._

#### Decomposition of probability measures

The almost sure convergence of a nonnegative martingale yields the following theorem on the Lebesgue decomposition of two probability measures on a given measurable space.

**Theorem 13.4.3:** _Let \((\Omega,\mathcal{F})\) be a measurable space and \(\{\mathcal{F}_{n}\}_{n\geq 1}\) be a filtration with \(\mathcal{F}_{n}\subset\mathcal{F}\) for all \(n\geq 1\). Let \(P\) and \(Q\) be two probability measures on \((\Omega,\mathcal{F})\) such that for each \(n\geq 1\), \(P_{n}\equiv\) the restriction of \(P\) to \(\mathcal{F}_{n}\) is absolutely continuous w.r.t. \(Q_{n}\equiv\) on the restriction of \(Q\) to \(\mathcal{F}_{n}\), with the Radon-Nikodym derivative \(X_{n}=\frac{dP_{n}}{dQ_{n}}\). Let \(\mathcal{F}_{\infty}\equiv\sigma\langle\bigcup_{n\geq 1}\mathcal{F}_{n}\rangle\) and \(X\equiv\overline{\lim}_{n\to\infty}X_{n}\). Then for any \(A\in\mathcal{F}_{\infty}\),_

\[P(A) = \int_{A}XdQ+P(A\cap(X=\infty)) \tag{4.2}\] \[\equiv P_{a}(A)+P_{s}(A),\mbox{ say},\]

_and \(P_{a}\ll Q\) and \(P_{s}\perp Q\)._

**Proof:** For \(1\leq k\leq n\), let \(M_{k,n}=\max_{k\leq j\leq n}X_{j}\). Let \(M_{k}=\sup_{n\geq k}M_{k,n}=\lim_{n\to\infty}M_{k,n}\). Then \(X\equiv\overline{\lim}_{n\to\infty}X_{n}=\lim_{k\to\infty}M_{k}\). Fix \(1\leq k_{0}\), \(N<\infty\) and \(A\in\mathcal{F}_{k_{0}}\). Then for \(n\geq k\geq k_{0}\), \(B_{k,n}\equiv A\cap\{M_{k,n}\leq N\}\in\mathcal{F}_{n}\) and hence

\[P(B_{k,n})=\int X_{n}I_{B_{k,n}}dQ. \tag{4.3}\]

As \(n\to\infty\), \(M_{k,n}\uparrow M_{k}\) and so \(I_{B_{k,n}}\downarrow I_{B_{k}}\), where \(B_{k}\equiv A\cap\{M_{k}\leq N\}=\bigcap_{n\geq k}B_{k,n}\). Also, since \(\{X_{n},\mathcal{F}_{n}\}_{n\geq 1}\) is a nonnegative martingale under the probability measure \(Q\), \(\lim_{n\to\infty}X_{n}\) exists w.p. 1 and hence coincides with \(X\). Thus, by the bounded convergence theorem applied to (4.3),

\[P(B_{k})=\int XI_{B_{k}}dQ.\]Now let \(N\to\infty\) and use the MCT to conclude that

\[P(A\cap(M_{k}<\infty))=\int XI_{\{M_{k}<\infty\}}dQ.\]

Since \(\{M_{k}<\infty\}\uparrow\{X<\infty\}\), another application of the MCT yields

\[P(A\cap\{X<\infty\})=\int XI_{\{X<\infty\}}dQ.\]

Since \(Q(X<\infty)=1\) and

\[P(A)=P(A\cap(X<\infty))+P(A\cap(X=\infty)),\]

(4.2) is proved for all \(A\in{\Cal{F}}_{k_{0}}\) and hence, also for all \(A\in\bigcup_{k\geq 1}{\Cal{F}}_{k}\). Since \(\bigcup_{k\geq 1}{\Cal{F}}_{k}\) is an algebra, (4.2) holds for all \(A\in{\Cal{F}}_{\infty}\equiv\sigma(\bigcup_{k\geq 1}{\Cal{F}}_{k})\).

To prove the second part, simply note that \(Q(A)=0\) implies \(P_{a}(A)=0\) and \(Q(X=\infty)=0\). \(\Box\)

**Remark 13.4.1:** The right side of (4.2) provides the Lebesgue decomposition of \(P\) w.r.t. \(Q\).

**Corollary 13.4.4:**

* \(P\ll Q\) _iff_ \(E_{Q}X=1\) _iff_ \(P(X=\infty)=0\)_._
* \(P\perp Q\) _iff_ \(Q(X=0)=1\) _iff_ \(P(X=\infty)=1\)_._

**Proof:** Follows easily from (4.2). \(\Box\)

For some applications of Corollary 13.4.4 to branching processes see Athreya (2000).

**Corollary 13.4.5:** _Let \(\{X_{n},{\Cal{F}}_{n}\}_{n\geq 1}\) be a nonnegative martingale on a probability space \((\Omega,{\Cal{F}},Q)\), with \(E_{Q}X_{1}=1\). Let \(P\) be defined on \(\bigcup_{n\geq 1}{\Cal{F}}_{n}\) by \(P(A)=E_{Q}X_{n}I_{A}\) if \(A\in{\Cal{F}}_{n}\). Then \(P\) is well defined. Suppose further that \(P\) admits an extension to a probability measure on \((\Omega,{\Cal{F}}_{\infty})\) where \({\Cal{F}}_{\infty}\equiv\sigma(\bigcup_{n\geq 1}{\Cal{F}}_{n})\). Then (4.2) holds._

**Proof:** If \(A\in{\Cal{F}}_{n}\), then \(A\in{\Cal{F}}_{m}\) for any \(m>n\). But \(E_{Q}X_{m}I_{A}=E_{Q}X_{n}I_{A}\) by the martingale property, and so \(P\) is well defined on \(\bigcup_{n\geq 1}{\Cal{F}}_{n}\). The rest of the corollary follows from the theorem. \(\Box\)

**Remark 13.4.2:** It can be shown that if \({\Cal{F}}_{n}\equiv\sigma\langle X_{j}:1\leq j\leq n\rangle\), then \(P\) does admit an extension to \({\Cal{F}}_{\infty}\) (cf. see Athreya (2000)).

**Corollary 13.4.6:** _Let \((\Omega,{\Cal{F}})\) be a measurable space. Let for each \(n\geq 1\), \({\Cal{A}}_{n}\equiv\{A_{n1},A_{n2},\ldots,A_{nk_{n}}\}\subset{\Cal{F}}\) be a partition of \(\Omega\). Let \({\Cal{A}}_{n}\subset\sigma\langle{\Cal{A}}_{n+1}\rangle\) for all \(n\geq 1\). Let \(P\) and \(Q\) be two probability measures on \((\Omega,{\Cal{F}})\). Let \(Q\) be such that \(Q(A_{ni})>0\) for all \(n\) and \(i\). Let \({\Cal{F}}=\sigma\langle\bigcup_{n\geq 1}{\Cal{A}}_{n}\rangle\). Let\(X_{n}\equiv\sum_{i=1}^{k_{n}}\frac{P(A_{ni})}{Q(A_{ni})}I_{A_{ni}}\). Then \(\{X_{n},\mathcal{F}_{n}\}_{n\geq 1}\) is a martingale on \((\Omega,\mathcal{F},Q)\) and \(P\) satisfies the decomposition (4.2)._

The proof of Corollary 13.4.6 is left as an exercise (Problem 13.22).

**Remark 13.4.3:** This yields the Lebesgue decomposition of \(P\) w.r.t. \(Q\) when \(\mathcal{F}\) is countably generated, i.e., when there exists a countable collection \(\mathcal{A}\) of subsets of \(\Omega\) such that \(\mathcal{F}=\sigma\langle\mathcal{A}\rangle\). In particular, this holds if \(\Omega=\mathbb{R}^{k}\) and \(\mathcal{F}\equiv\mathcal{B}\big{(}(\mathbb{R}^{k})\big{)}\) for \(k\in\mathbb{N}\).

#### 13.4.5 Kakutani's theorem

**Theorem 13.4.7:** (_Kakutani's theorem_). Let \(P\) and \(Q\) be the probability distributions on \(\big{(}\mathbb{R}^{\infty},\mathcal{B}(\mathbb{R}^{\infty})\big{)}\) of the sequences of independent random variables \(\{X_{j}\}_{j\geq 1}\) and \(\{Y_{j}\}_{j\geq 1}\), respectively. Let for each \(j\geq 1\), the distribution of \(X_{j}\) be dominated by that of \(Y_{j}\). Then_

\[\mbox{either}\quad P\ll Q\quad\mbox{or}\quad P\perp Q. \tag{4.5}\]

**Proof:** Let \(f_{j}\) be the density of \(\lambda_{j}\) w.r.t. \(\mu_{j}\) where \(\lambda_{j}(\cdot)=P(X_{j}\in\cdot)\) and \(\mu_{j}(\cdot)=Q(Y_{j}\in\cdot)\). Let \(\Omega=\mathbb{R}^{\infty}\), \(\mathcal{F}=(\mathcal{B}(\mathbb{R}))^{\infty}\). Then \(P=\Pi_{j\geq 1}\lambda_{j}\), \(Q=\Pi_{j\geq 1}\mu_{j}\). Let \(\xi_{n}(\omega)\equiv\omega(n)\), the \(n\)th co-ordinate of \(\omega=(\omega_{1},\omega_{2},\ldots)\in\Omega\), and \(\mathcal{F}_{n}\equiv\sigma\langle\xi_{j}:1\leq j\leq n\rangle\). Also let \(P_{n}\) be the restriction of \(P\) to \(\mathcal{F}_{n}\) and \(Q_{n}\) be that of \(Q\) to \(\mathcal{F}_{n}\). Then \(P_{n}\ll Q_{n}\) with probability density

\[L_{n}=\frac{dP_{n}}{dQ_{n}}=\prod_{j=1}^{n}f_{j}(\xi_{j}).\]

Since \(\{\overline{\lim}_{n\to\infty}L_{n}<\infty\}\) is a tail event, by the independence of \(\{\xi_{j}\}_{j\geq 1}\) under \(P\) and the Kolmogorov's zero-one law, \(P(\overline{\lim}_{n\to\infty}L_{n}<\infty)=0\) or \(1\). Now, by Corollary 13.4.4, (4.5) follows. \(\Box\)

**Remark 13.4.4:** It can be shown that \(P\ll Q\) or \(P\perp Q\) according as \(\prod_{j=1}^{\infty}E\sqrt{f_{j}(Y_{j})}>0\) or \(=0\). For a proof, see Durrett (2004).

**Remark 13.4.5:** If \(\{X_{j}\}_{j\geq 1}\) are iid and \(\{Y_{j}\}_{j\geq 1}\) are also iid, then \(P=Q\) or \(P\perp Q\). This is because \(f_{j}=f_{1}\) for all \(j\) and \(E_{Q}\sqrt{f_{1}}\leq(E_{Q}f_{1})^{1/2}\leq 1\) and so either \(E_{Q}\sqrt{f_{1}}<1\) or \(E_{Q}\sqrt{f_{1}}=1\). In the latter case \(f_{1}\equiv 1\), since \(E_{Q}(\sqrt{f_{1}})^{2}=1=E_{Q}(\sqrt{f_{1}})\Rightarrow\mbox{Var}_{Q}(\sqrt{f _{1}})=0\).

**Remark 13.4.6:** The above result can be extended to Markov chains. Let \(P\) and \(Q\) be two irreducible stochastic matrices on a countable set and let \(Q\) be positive recurrent. Also, let \(P_{x_{0}}\) denote the distribution of a Markov chain \(\{X_{n}\}_{n\geq 1}\) starting at \(x_{0}\) and with transition probability \(P\), and similarly, let \(Q_{y_{0}}\) denote the distribution of a Markov chain starting at \(y_{0}\) and with transition probability \(Q\). Then

\[\mbox{either}\quad P_{x_{0}}\perp Q_{y_{0}}\quad\mbox{or}\quad P=Q. \tag{4.6}\]

The proof of this is left as an exercise (Problem 13.23).

#### de Finetti's theorem

Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of exchangeable random variables on a probability space \((\Omega,\mathcal{F},P)\), i.e., for each \(n\geq 1\), the distribution of \((X_{\sigma(1)},X_{\sigma(2)},\ldots,X_{\sigma(n)})\) is the same as that of \((X_{1},X_{2},\ldots,X_{n})\) where \((\sigma(1),\sigma(2),\ldots,\sigma(n))\) is a permutation of \((1,2,\ldots,n)\). Then there is a \(\sigma\)-algebra \(\mathcal{G}\subset\mathcal{F}\) such that for each \(n\geq 1\),

\[P(X_{i}\in B_{i},\,i=1,2,\ldots,n\mid\mathcal{G})=\prod_{i=1}^{n}P(X_{i}\in B_{ i}\mid\mathcal{G}) \tag{4.7}\]

for all \(B_{1},\ldots,B_{n}\in\mathcal{B}(\mathbb{R})\).

This is known as de Finetti's theorem. For a proof, see Durrett (2004) and Chow and Teicher (1997). This theorem says that conditioned on \(\mathcal{G}\) the \(\{X_{i}\}_{i\geq 1}\) are iid random variables with distribution \(P(X_{1}\in\cdot\mid\mathcal{G})\). The converse to this result, i.e., if for some \(\sigma\)-algebra \(\mathcal{G}\subset\mathcal{F}\) (4.7) holds, then the sequence \(\{X_{i}\}_{i\geq 1}\) is exchangeable is not difficult to verify (Problem 13.26).

### Problems

13.1. Let \(\Omega\) be a nonempty set and let \(\{A_{j}\}_{j\geq 1}\) be a countable partition of \(\Omega\). For \(n\geq 1\), let \(\mathcal{F}_{n}=\sigma\)-algebra generated by \(\{A_{j}\}_{j=1}^{n}\). 1. Show that \(\{F_{n}\}_{n\geq 1}\) is a filtration. 2. Find \(\mathcal{F}_{\infty}=\sigma\langle\bigcup_{n\geq 1}\mathcal{F}_{n}\rangle\).
13.2. Let \(\Omega\) be a nonempty set. For each \(n\geq 1\), let \(\pi_{n}\equiv\{A_{nj}:j=1,2,\ldots,k_{n}\}\) be a partition of \(\Omega\). Suppose that each \(n\) and \(j\), \(A_{nj}\) is a union of sets of \(\pi_{n+1}\). Let \(\mathcal{F}_{n}\equiv\sigma\langle\pi_{n}\rangle\) for \(n\geq 1\). 1. Show that \(\{\mathcal{F}_{n}\}_{n\geq 1}\) is a filtration. 2. Suppose \(\Delta=[0,1)\) and \(\pi_{n}\equiv\{\left[\frac{j-1}{2^{n}},\frac{j}{2^{n}}\right):j=1,2,\ldots,2^ {n}\}\). Show that \(\mathcal{F}_{\infty}=\sigma\langle\bigcup_{n\geq 1}\mathcal{F}_{n}\rangle\) is the Borel \(\sigma\)-algebra \(\mathcal{B}([0,1))\).
13.3. Let \(\{(Y_{n},\mathcal{F}_{n}):n\geq 1\}\) and \(\{(\tilde{Y}_{n},\mathcal{F}_{n}):n\geq 1\}\) be as in Example 13.1.2. Verify that \(\{(Y_{n},\mathcal{F}_{n}):n\geq 1\}\) is a sub-martingale and \(\{(\tilde{Y}_{n},\tilde{\mathcal{F}}_{n}):n\geq 1\}\) is a martingale.

13.4: Give an example of a random variable \(T\) and two filtrations \(\{{\cal F}_{n}\}_{n\geq 1}\) and \(\{{\cal G}_{n}\}_{n\geq 1}\) such that \(T\) is a stopping time w.r.t. the filtration \(\{{\cal F}_{n}\}_{n\geq 1}\) but not w.r.t. \(\{{\cal G}_{n}\}_{n\geq 1}\).
13.5: Let \(T_{1}\) and \(T_{2}\) be stopping times w.r.t. a filtration \(\{{\cal F}_{n}\}_{n\geq 1}\). Verify that \(\min(T_{1},T_{2})\), \(\max(T_{1},T_{2})\), \(T_{1}+T_{2}\), and \(T_{1}^{2}\) are stopping times w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\). Give an example to show that \(\sqrt{T_{1}}\) and \(T_{1}-1\) need not be stopping times w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\).
13.6: Let \(T\) be a random variable taking values in \(\{1,2,3,\ldots\}\). Show that there is a filtration \(\{{\cal F}_{n}\}_{n\geq 1}\) w.r.t. which \(T\) is a stopping time.
13.7: Let \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration. 1. Show that \(T\) is a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\) iff \[\{T\leq n\}\in{\cal F}_{n}\quad\mbox{for all}\quad n\geq 1\.\] 2. Show by an example that if a random variable \(T\) satisfies \(\{T\geq n\}\in{\cal F}_{n}\) for all \(n\geq 1\), it need not be a stopping time w.r.t. \(\{{\cal F}_{n}\}_{n\geq 1}\). (**Hint:** Consider a \(T\) of the form \[T=\inf\{k:k\geq 1,X_{k+1}\in A\}\quad\mbox{and}\quad{\cal F}_{n}=\sigma \langle X_{j}:j\leq n\rangle.)\]
13.8: Show that \({\cal F}_{T}\) defined in (2.6) is a \(\sigma\)-algebra.
13.9: Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables. Let \({\cal G}_{n}=\sigma\langle\{X_{j}:1\leq j\leq n\}\rangle\). Let \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration such that \({\cal G}_{n}\subset{\cal F}_{n}\) for each \(n\geq 1\). 1. Show that if \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) is a martingale, then so is \(\{X_{n},{\cal G}_{n}\}_{n\geq 1}\). 2. Show by an example that the converse need not be true. 3. Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a martingale. Let \(1\leq k_{1}<k_{2}<k_{3}\cdots\) be a sequence of integers. Let \(Y_{n}\equiv X_{k_{n}}\), \({\cal H}_{n}\equiv{\cal F}_{k_{n}}\), \(n\geq 1\). Show that \(\{Y_{n},{\cal H}_{n}\}_{n\geq 1}\) is also a martingale.
13.10: A branching random walk is a branching process and a random walk associated with it. Individuals reproduce according to a branching process and the offspring move away from the parent a random distance. If \(X_{n}\equiv\{x_{n1},x_{n2},\ldots x_{nZ_{n}}\}\) denotes the position vector of the \(Z_{n}\) individuals in the \(n\)th generation and the individual at location \(x_{ni}\) produces \(\rho_{ni}\) offspring, then each of them chooses a new position by moving a random distance from \(x_{ni}\) and these are assumed to be iid. Let \(\eta_{nij}\) be the random distance moved by the \(j\)th offspring of the individual at \(x_{ni}\). Then the position vector of the \((n+1)\)st generation is given by \[X_{n+1} = \Big{\{}\{x_{ni}+\eta_{nij}\}_{j=1}^{\rho_{ni}},\quad i=1,2,\ldots,n \Big{\}}\] \[\equiv \big{\{}x_{n+1,k}:k=1,2,\ldots Z_{n+1}\big{\}},\quad\mbox{say},\] where \[Z_{n+1} = \mbox{population size of the (n+1)st generation}\] \[= \sum_{i=1}^{Z_{n}}\rho_{ni}.\] Let the offspring distribution be \(\{p_{k}\}_{k\geq 0}\) and the jump size distribution be denoted by \(F(\cdot)\). Assume that the \(\eta\)'s are real valued and also that the collection \(\{\rho_{ni}\}_{i\geq 1,n\geq 0}\), \(\{\eta_{nij}\}_{i\geq 1,j\geq 1,n\geq 0}\) are all independent with the \(\rho\)'s being iid with distribution \(\{p_{k}\}_{k\geq 0}\) and the \(\eta\)'s iid with distribution \(F\). Fix \(\theta\in\mathbb{R}\). For \(n\geq 0\), let \[Z_{n}(\theta)\equiv\bigg{(}\sum_{i=1}^{Z_{n}}e^{\theta x_{ni}}\bigg{)}\quad \mbox{and}\quad Y_{n}(\theta)=\big{(}Z_{n}(\theta)\big{)}\big{(}\rho\phi( \theta)\big{)}^{-n}\] where \(\rho=\sum_{k=0}^{\infty}kp_{k}\), \(\phi(\theta)=E(e^{\theta\eta_{111}})=\int e^{\theta x}dF(x)\). Assume \(0<\phi(\theta)<\infty\), \(0<\rho<\infty\). 1. Verify that \(\{Y_{n}(\theta)\}_{n\geq 0}\) is a martingale w.r.t. an appropriate filtration \(\{\mathcal{F}_{n}\}_{n\geq 0}\). 2. Show that \[\mbox{Var}\big{(}Z_{n+1}(\theta)\big{)} = \mbox{Var}\big{(}Z_{n}(\theta)\rho\phi(\theta)\big{)}\] \[+\ \big{(}EZ_{n}(2\theta)\big{)}\big{(}\rho\psi(\theta)+(\phi( \theta))^{2}\sigma^{2}\big{)},\] where \(\psi(\theta)=\phi(2\theta)-\big{(}\phi(\theta)\big{)}^{2}\) and \(\sigma^{2}\) is the variance of the distribution \(\{p_{k}\}_{k\geq 0}\). 3. State a sufficient condition on \(\rho\), \(\sigma^{2}\), \(\psi(\cdot)\) and \(\phi(\cdot)\) and \(\theta\) for \(\{Y_{n}(\theta)\}\) to be \(L_{2}\)-bounded.
13.11 Let \(\{\eta_{j}\}_{j\geq 1}\) be adapted to a filtration \(\{\mathcal{F}_{j}\}_{j\geq 1}\). Let \(E(\eta_{j}|\mathcal{F}_{j-1})=0\) and \(V_{j}=E(\eta_{j}^{2}|\mathcal{F}_{j-1})\) for \(j\geq 2\). Let \(s_{n}^{2}=\sum_{j=1}^{n}V_{j}\), \(n\geq 2\). 1. Verify that \(\{Y_{n}\equiv\sum_{j=2}^{n}\frac{\eta_{j}}{\tilde{s}_{j}},\mathcal{F}_{n}\}_{n \geq 1}\) is a martingale, where \(\tilde{s}_{j}=\max(s_{j},1)\). 2. Show that \(\mbox{Var}(Y_{n})=E\Big{(}\sum_{j=2}^{n}\frac{V_{j}}{\tilde{s}_{j}^{2}}\Big{)}\). 3. Show that \(\sum_{j=2}^{\infty}\frac{V_{j}}{\tilde{s}_{j}^{2}}\leq\int_{1}^{\infty}\frac{ 1}{t^{2}}dt+1\). 2. 3. The \(\eta_{j}\)'s are real valued and \(\{\eta_{j}\}_{j\geq 1}\). 4. Show that \(\mbox{Var}(Y_{n})=E\Big{(}\sum_{j=2}^{n}\frac{V_{j}}{\tilde{s}_{j}^{2}}\Big{)}\). 5. Show that \(\sum_{j=2}^{\infty}\frac{V_{j}}{\tilde{s}_{j}^{2}}\leq\int_{1}^{\infty}\frac{ 1}{t^{2}}dt+1\). 6. Show that \(\mbox{Var}(Y_{n})=E\Big{(}

* Conclude that \(Y_{n}\) converges w.p. 1 and in \(L^{2}\).
* Now suppose that \(s_{n}\to\infty\) w.p. 1. Show that \(\frac{1}{s_{n}}\sum_{j=1}^{n}\eta_{j}\to 0\) w.p. 1. (**Hint:*
* Use Kronecker's lemma (cf. Chapter 8).)
* Let \(\{\xi_{i}\}_{i\geq 1}\) be iid random variable with distribution \(P(\xi_{1}=1)=\frac{1}{2}=P(\xi_{1}=-1)\). Let \(S_{0}=0\), \(S_{n}=\sum_{i=1}^{n}\xi_{i}\), \(n\geq 1\). Let \(-a<0<b\) be integers and \(T=T_{-a,b}=\inf\{n:n\geq 1,S_{n}=-a\) or \(b\}\). Show, using Wald's lemmas (Theorem 13.2.14), that
* Extend the above arguments to find \(\mbox{Var}(T)\). (**Hint:*
* Consider \(T\wedge n\) first and then let \(n\uparrow\infty\).)
* Use Problem 13.12 to conclude that for any positive integer \(b\) \[P(T_{b}<\infty)=1,\quad\mbox{but}\quad ET_{b}=\infty,\] where for any integer \(i\), \[T_{i}=\inf\{n:n\geq 1,S_{n}=i\}.\] (**Hint:** Use the relation \(T_{b}=\lim_{i\to\infty}T_{-i,b}\).)
* Let \(\{\xi_{i}\}_{i\geq 1}\) be iid random variables with distribution \[P(\xi_{i}=1)=p=1-P(\xi_{i}=-1),\ 0<p\neq\frac{1}{2}<1\.\] Let \(S_{0}=0\), \(S_{n}=\sum_{i=1}^{n}\xi_{i}\), \(n\geq 1\). Let \(\psi(x)=\left(\frac{q}{p}\right)^{x}\), \(x\in\mathbb{R}\) where \(q=1-p\).
* Show that \(X_{n}=\psi(S_{n})\), \(n\geq 0\) is a martingale w.r.t. the filtration \(\mathcal{F}_{n}=\sigma\langle\xi_{1},\ldots,\xi_{n}\rangle\), \(n\geq 1\), and \(\mathcal{F}_{0}=\{\Omega,\emptyset\}\).
* Let \(T_{a,b}=\inf\{n:n\geq 1,S_{n}=-a\) or \(b\}\), for positive integers \(a\) and \(b\). Show that \(P(T_{-a,b}<\infty)=1\). (**Hint:*
* Use the strong law of large numbers.)
* Use (a) to show that for positive integers \(a,b\), \[\theta\equiv P(T_{-a}<T_{b})=\frac{\psi(b)-1}{\psi(b)-\psi(-a)},\] where for any integer \(i\), \(T_{i}=\inf\{n:n\geq 1,S_{n}=i\}\).
* Show that \(ET_{-a,b}=\frac{b-\theta(b-a)}{(p-q)}\)
* Show that if \(p>q\) then \(ET_{-a}=\infty\) and \(ET_{b}=\frac{b}{(p-q)}\).
* Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain with state space \(\mathbb{S}=\{1,2,3,\ldots,\}\) and transition probability matrix \(P=((p_{ij}))\). That is, for each \(n\geq 1\), \[P(X_{0}=i_{0},X_{1}=i,\ldots,X_{n}=i_{n})\] \[=P(X_{0}=i_{0})p_{i_{0}i_{1}}\ldots p_{i_{n-1}i_{n}}\] for all \(i_{0},i_{1},i_{2},\ldots,i_{n}\in\mathbb{S}\). Let \(h:\mathbb{S}\rightarrow\mathbb{R}\) and \(\rho\in\mathbb{R}\) be such that \[\sum_{j=1}^{\infty}|h(j)|p_{ij}<\infty\quad\text{for all}\quad i\] and \[\sum_{j=1}^{\infty}h(j)p_{ij}=\rho h(i)\quad\text{for all}\quad i.\]
* Verify that \(\{X_{n}\}_{n\geq 1}\) has the Markov property, namely, for all \(n\geq 0\), \[P(X_{n+1}=i_{n+1}\mid X_{n}=i_{n},X_{n-1}=i_{n-1},X_{0}=i_{0})\] \[=P(X_{n+1}=i_{n+1}\mid X_{n}=i_{n})=p_{i_{n}i_{n+1}}.\]
* Verify that \(\{Y_{n}\equiv h(X_{n})\rho^{-n}\}_{n\geq 0}\) is a martingale w.r.t. the filtration \(\mathcal{F}_{n}\equiv\sigma\langle X_{0},X_{1},\ldots,X_{n}\rangle\).
* Suppose \(\rho=1\) and \(h\) is bounded below and attains its lower bound. Suppose also that \(\{X_{n}\}_{n\geq 0}\) is irreducible and _recurrent_. That is, \(P(X_{n}=j\) for some \(n\geq 1\mid X_{0}=i)=1\) for all \(i,j\). Show that \(h\) is a constant function.
* Use the optional stopping theorem II.)
* Let \(\{Y_{j}\}_{j\geq 1}\) be a sequence of random variables such that \(P(|Y_{j}|\leq 1)=1\) for all \(j\geq 1\) and \(E(Y_{j}|\mathcal{F}_{j-1})=0\) for \(j\geq 2\) where \(\mathcal{F}_{j}=\sigma\langle Y_{1},Y_{2},\ldots,Y_{j}\rangle\). Let \(X_{n}=\sum_{j=1}^{n}Y_{j},n\geq 1\) and let \(\tau\) be a stopping time w.r.t. \(\{\mathcal{F}_{n}\}_{n\geq 1}\) and \(E\tau<\infty\). Show that \(E|X_{\tau}|<\infty\) and \(EX_{\tau}=EX_{1}\).
* Let \(\theta\), \(\{\xi_{j}\}_{j\geq 1}\) be a sequence of random variables such that \(E|\theta|<\infty\) and \(\{\xi_{j}\}_{j\geq 1}\) are iid with mean zero. For \(j\geq 1\) let \(X_{j}=\theta+\xi_{j}\) and \(\mathcal{F}_{j}=\sigma\langle X_{1},X_{2},\ldots,X_{j}\rangle\). Show that \(Y_{j}\equiv E(\theta|\mathcal{F}_{j})\rightarrow\theta\) w.p. 1.
* Use the convergence result for a Doob martingale and the SLLN to \(\bar{X}_{n}=\theta+n^{-1}\sum_{j=1}^{n}\xi_{j}\) to show that \(\theta\) is \(\mathcal{F}_{\infty}\)-measurable.)
13.18 Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a martingale sequence such that \(EX_{n}^{2}<\infty\) for all \(n\geq 1\). Let \(Y_{1}=X_{1}\), \(Y_{j}=X_{j}-X_{j-1}\), \(j\geq 2\). Let \(V_{j}=E(Y_{j}^{2}|{\cal F}_{j-1})\) for \(j\geq 2\) and \(A_{n}=\sum_{j=2}^{n}V_{j}\), \(n\geq 2\). Verify that 1. \(A_{n}\) is \({\cal F}_{n-1}\) measurable and nondecreasing in \(n\). 2. \(\{X_{n}^{2}-A_{n},{\cal F}_{n}\}_{n\geq 2}\) is a martingale. 3. Verify that \(X_{n}^{2}=X_{n}^{2}-A_{n}+A_{n}\) is the Doob decomposition of the sub-martingale \(\{X_{n}^{2},{\cal F}_{n}\}\) (Proposition 13.1.2).
13.19 Show that the random variable \(X_{\infty}\) defined in Example 13.3.2 is zero w.p. 1. (**Hint:** Show that \(E\log\xi_{1}<0\) and use the SLLN.)
13.20 Show that the Doob martingale in Definition 13.3.1 is uniformly integrable. (**Hint:** Show that for any \(\lambda>0\), \(\lambda_{0}>0\) \[E\bigl{(}|X_{n}|I(|X_{n}|>\lambda) \leq E\bigl{(}|X|I|X_{n}|>\lambda\bigr{)}\] \[\leq E\bigl{(}|X|I(|X|>\lambda_{0})\bigr{)}+\lambda_{0}P(|X_{n}|> \lambda)\bigr{)}.\ \ )
13.21 Consider the following urn scheme due to Polya. Let an urn contain \(w_{0}\) white and \(b_{0}\) black balls at time \(n=0\). A ball is drawn from the urn at random. It is returned to the urn with one more ball of the color drawn. Repeat this procedure for all \(n\geq 1\). Let \(W_{n}\) and \(B_{n}\) denote the number of white and black balls in the urn after \(n\) draws. Let \(Z_{n}=\frac{W_{n}}{W_{n}+B_{n}},n\geq 0\). Let \({\cal F}_{n}=\sigma\langle Z_{0},Z_{1},\ldots,Z_{n}\rangle\). 1. Show that \(\{(Z_{n},{\cal F}_{n})\}_{n\geq 0}\) is a martingale. 2. Conclude that \(Z_{n}\) converges w.p. 1 and in \(L_{1}\) to a random variable \(Z\). 3. Show that for any \(k\in{\mathbb{N}}\), \(\lim_{n\to\infty}EZ_{n}^{k}\) converges and evaluate the limit. Deduce that \(Z\) has Beta \((w_{0},b_{0})\) distribution, i.e., its pdf \(f_{Z}(z)\equiv\frac{(w_{0}+b_{0}-1)!}{(w_{0}-1)!(b_{0}-1)!}z^{w_{0}-1}(1-z)^{ b_{0}-1}I_{[0,1]}(z)\). 4. Generalize (a) to the case when at the \(n\)th stage a random number \(\alpha_{n}\) of balls of the color drawn are added where \(\{\alpha_{n}\}_{n\geq 1}\) is any sequence of nonnegative integer valued random variables.
13.22 Prove Corollary 13.4.6. (**Hint:** Argue as in Example 13.1.7.)
13.23 Prove the last equation (4.6) of Section 13.4. (**Hint:** Show using the strong law for the \(Q\) chain that under \(Q\), the martingale \(X_{n}\) converges to 0 w.p. 1, where \(X_{n}\) is the Radon-Nikodym derivative of \(P_{x_{0}}\bigl{(}(X_{0},\ldots,X_{n})\in\cdot\bigr{)}\) w.r.t. \(Q_{x_{0}}\bigl{(}(X_{0},\ldots,X_{n})\in\cdot\bigr{)}\).)13.24 Let \(\{{\cal F}_{n}\}_{n\geq 0}\) be a filtration \(\subset{\cal F}\) where \((\Omega,{\cal F},P)\) is a probability space. Let \(\{Y_{n}\}_{n\geq 0}\subset L^{1}(\Omega,{\cal F},P)\). Suppose \[Z\equiv\sup_{n\geq 1}|Y_{n}|\in L^{1}(\Omega,{\cal F},P)\quad\mbox{and}\quad \lim_{n\to\infty}Y_{n}\equiv Y\quad\mbox{exists w.p. 1}.\] Show that \(E(Y_{n}|{\cal F}_{n})\to E(Y|{\cal F}_{\infty})\) w.p. 1. (**Hint:** Fix \(m\geq 1\) and let \(V_{m}=\sup_{n\geq m}|Y_{n}-Y|\). Show that \[\overline{\lim}_{n}E(|Y_{n}-Y||{\cal F}_{n})\leq\lim_{n\to\infty}E(V_{m}|{ \cal F}_{n})=E(V_{m}|{\cal F}_{\infty}).\] Now show that \(E(V_{m}|{\cal F}_{\infty})\to 0\) as \(m\to\infty\).)
13.25 Let \(\{X_{t},{\cal F}_{t}:t\in I\equiv{\cal Q}\cap(0,1)\}\) be a martingale, i.e., for all \(t_{1}<t_{2}\) in \(I\), \[E(X_{t_{2}}|{\cal F}_{t_{1}})=X_{t_{1}}.\] Show that for each \(t\) in \(I\) \[\lim_{s\uparrow t,s\in I}X_{s}\quad\mbox{and}\quad\lim_{s\downarrow t,s\in I}X _{s}\] both exist w.p. 1 and in \(L^{1}\) and equal \(X_{t}\) w.p. 1.
13.26 Let \(\{X_{i}\}_{i\geq 1}\) be random variables on a probability space \((\Omega,{\cal F},P)\). Suppose for some \(\sigma\)-algebra \({\cal G}\subset{\cal F}\) (4.7) holds. Show that \(\{X_{i}\}_{i\geq 1}\) are exchangeable.
13.27 Let \(\{X_{n}\}_{n\geq 0}\), \(\{Y_{n}\}_{n\geq 0}\) be martingales in \(L^{2}(\Omega,{\cal F},P)\) w.r.t. the same filtration \(\{{\cal F}_{n}\}_{n\geq 1}\). Let \(X_{0}=Y_{0}=0\). Show that \[E(X_{n}Y_{n})=\sum_{k=1}^{n}E(X_{k}-X_{k-1})(Y_{k}-Y_{k-1}),\ n\geq 1\] and, in particular, \[E(X_{n}^{2})=\sum_{k=1}^{n}E(X_{k}-X_{k-1})^{2}.\]
13.28 Let \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) be a martingale in \(L^{2}(\Omega,{\cal F},P)\). Suppose \(0\leq b_{n}\uparrow\infty\) such that \(\sum_{j=2}^{n}\frac{E(X_{j}-X_{j-1})^{2}}{b_{j}^{2}}<\infty\). Show that \(\frac{X_{n}}{b_{n}}\to 0\) w.p. 1. (**Hint:** Consider the sequence \(Y_{n}\equiv\sum_{j=2}^{n}\frac{(X_{j}-X_{j-1})}{b_{j}}\), \(n\geq 2\). Verify that \(\{Y_{n},{\cal F}_{n}\}_{n\geq 2}\) is a \(L^{2}\) bounded martingale and use Kronecker's lemma (cf. Chapter 8).)13.5 Problems 437

13.29 Let \(f\in L^{1}\big{(}[0,1],{\cal B}([0,1]),m\big{)}\) where \(m(\cdot)\) is Lebesgue measure on \([0,1].\) Let \(\{H_{k}(\cdot)\}_{k\geq 1}\) be the Haar functions defined by

\[H_{1}(t) \equiv 1,\] \[H_{2}(t) \equiv \left\{\begin{array}{ll}1&0\leq t<\frac{1}{2}\\ -1&\frac{1}{2}\leq t<1,\end{array}\right.\] \[H_{2^{n}+1}(t) = \left\{\begin{array}{ll}2^{n/2}&0\leq t<2^{-(n+1)}\\ -2^{n/2}&2^{-(n+1)}\leq t<2^{-n},\ n=1,2,\ldots\\ 0&\mbox{otherwise,}\end{array}\right.\] \[H_{2^{n}+j}(t) = H_{2^{n}+1}\Big{(}t-\frac{j-1}{2^{n}}\Big{)},\ j=1,2,\ldots,2^{n}.\]

Let \(a_{k}\equiv\int_{0}^{1}f(t)H_{k}(t)dt,\)\(k=1,2,\ldots\)

1. Verify that \(\{X_{n}(t)\equiv\sum_{k=1}^{n}a_{k}H_{k}(t)\}_{n\geq 1}\) is a martingale w.r.t. the natural filtration.
2. Show that \(X_{n}\) converges w.p. 1 and in \(L^{1}\) to \(f\).

13.30 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of nonnegative random variables on some probability space \((\Omega,{\cal F},P)\) such that \(E(X_{n+1}|{\cal F}_{n})\leq X_{n}+Y_{n}\) where \({\cal F}_{n}\equiv\sigma\langle X_{1},\ldots,X_{n}\rangle\) where \(\{Y_{n}\}_{n\geq 1}\) is a sequence of nonnegative constants such that \(\sum_{n=1}^{\infty}Y_{n}<\infty.\) Show that \(\{X_{n}\}_{n\geq 1}\) converges w.p. 1.

13.31 Let \(\{\tau_{j}\}_{j\geq 1}\) be independent exponential random variables with \(\lambda_{j}=E\tau_{j},\)\(j\geq 1\) such that \(\sum_{j=1}^{\infty}\frac{1}{\lambda_{j}^{2}}<\infty.\) Let \(T_{0}=0,\)\(T_{n}=\sum_{j=1}^{n}\tau_{j},\)\(n\geq 1,\)\(s_{n}=\sum_{j=1}^{n}\lambda_{j}.\) Show that \(\{X_{n}\equiv T_{n}-s_{n}\}_{n\geq 1}\) converges w.p. 1 and in mean square.

(**Hint:** Show that \(\{X_{n}\}_{n\geq 1}\) is an \(L^{2}\)-bounded martingale.)

### 14 Markov Chains and MCMC

#### 14.1.1 Definition

Let \(\mathbb{S}=\{a_{j}:j=1,2,\ldots,K\}\), \(K\leq\infty\) be a finite or countable set. Let \(\boldsymbol{P}=((p_{ij}))_{K\times K}\) be a _stochastic matrix_, i.e., \(p_{ij}\geq 0\), for every \(i\), \(\sum_{j=1}^{K}p_{ij}=1\) and \(\mu=\{\mu_{j}:1\leq j\leq K\}\) be a probability distribution, i.e., \(\mu_{j}\geq 0\) for all \(j\) and \(\sum_{j=1}^{K}\mu_{j}=1\).

**Definition 14.1.1:** A sequence \(\{X_{n}\}_{n=0}^{\infty}\) of \(\mathbb{S}\)-valued random variables on some probability space \((\Omega,\mathcal{F},P)\) is called a _Markov chain_ with _stationary transition probabilities_\(\boldsymbol{P}=((p_{ij}))\), _initial distribution_\(\mu\), and state space \(\mathbb{S}\) if

1. \(X_{0}\sim\mu\), i.e., \(P(X_{0}=a_{j})=\mu_{j}\) for all \(j\), and
2. \(P\big{(}X_{n+1}=a_{j}\mid X_{n}=a_{i}\), \(X_{n-1}=a_{i_{n-1}},\ldots,X_{0}=a_{i_{0}}\big{)}=p_{ij}\) for all \(a_{i},a_{j},a_{i_{n-1}},\ldots,a_{i_{0}}\in\mathbb{S}\) and \(n=0,1,2,\ldots\),

i.e., the sequence is _memoryless_. Given \(X_{n}\), \(X_{n+1}\) is independent of \(\{X_{j}:j\leq n-1\}\). More generally, given the _present_\((X_{n})\), the _past_\((\{X_{j}:j\leq n-1\})\) and the _future_\((\{X_{j}:j>n\})\) are stochastically independent (Problem 14.1).

A few questions arise:

**Question 1:** Does such a sequence \(\{X_{n}\}_{n=0}^{\infty}\) exist for every \(\mu\) and \(\boldsymbol{P}\), andif so, how does one generate them?

The answer is yes. There are two approaches, namely, (i) using Kolmogorov's consistency theorem and (ii) an iid random iteration scheme.

**Question 2:** How does one describe the finite time behavior, i.e., the joint distribution of \((X_{0},X_{1},\ldots,X_{n})\) for any \(n\in\mathbb{N}\)?

One may use the Markov property repeatedly to obtain the joint distribution.

**Question 3:** What can one say about the long-term behavior? One can ask questions like:

1. Does the trajectory \(n\to X_{n}\) converge as \(n\to\infty\)?
2. Does the distribution of \(X_{n}\) converge as \(n\to\infty\)?
3. Do the laws of large numbers hold for a suitable class of functions \(f\)'s, i.e., do the limits \(\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^{n}f(X_{j})\) exist w.p. 1?
4. Do _stationary_ distributions exist? (A probability distribution \(\pi=\{\pi_{i}\}_{i\in\mathbb{S}}\) is called a stationary distribution for a Markov chain \(\{X_{n}\}_{n\geq 0}\) if \(X_{0}\) has distribution \(\pi\), then \(X_{n}\) also has distribution \(\pi\) for all \(n\geq 1\).)

The key to answering these questions are the concepts of communication, irreducibility, aperiodicity, and most importantly _recurrence_. The main tools are the laws of large numbers, renewal theory, and coupling.

#### Examples

**Example 14.1.1:** (_IID sequence_). Let \(\{X_{n}\}_{n=0}^{\infty}\) be a sequence of iid \(\mathbb{S}\)-valued random variables with distribution \(\mu=\{\mu_{j}\}\). Then \(\{X_{n}\}_{n=0}^{\infty}\) is a Markov chain with initial distribution \(\mu\) and transition probabilities given by \(p_{ij}=\mu_{j}\) for all \(i\), i.e., all rows of \(\boldsymbol{P}\) are identical. It is also easy to prove the converse, i.e., if all rows of \(\boldsymbol{P}\) are identical, then \(\{X_{n}\}_{n=1}^{\infty}\) are iid and independent of \(X_{0}\).

To answer Question 3 in this case, note that \(P[X_{n}=j]=\mu_{j}\) for all \(n\) and thus \(X_{n}\) converges in distribution. But the trajectories do not converge. However, the law of large numbers holds and \(\mu\) is the unique stationary distribution.

**Example 14.1.2:** (_Random walks_). Let \(\mathbb{S}=\mathbb{Z}\), the set of integers. Let \(\{\epsilon_{n}\}_{n\geq 1}\) be iid with distribution \(\{p_{j}\}_{j\in\mathbb{Z}}\), i.e., \(P[\epsilon_{1}=j]=p_{j}\) for \(j\in\mathbb{Z}\) and \(\{\epsilon_{n}\}_{n\geq 1}\) are independent. Let \(X_{0}\) be a \(\mathbb{Z}\)-valued random variable independent of \(\{\epsilon_{n}\}_{n\geq 1}\). Then, define for \(n\geq 0\),

\[X_{n+1}=X_{n}+\epsilon_{n+1}=X_{n-1}+\epsilon_{n}+\epsilon_{n-1}=\cdots=X_{0} +\sum_{j=1}^{n+1}\epsilon_{j}\.\]In this case, with probability one, the trajectories of \(X_{n}\) go to \(+\infty\) (respectively, \(-\infty\)), if \(E(\epsilon_{1})>0\) (respectively \(<0\)). If \(E(\epsilon_{1})=0\), then the trajectories fluctuate infinitely often provided \(p_{0}\neq 1\).

**Example 14.1.3:** (_Branching processes_). Let \(\mathbb{S}=\mathbb{Z}_{+}=\{0,1,2,\ldots\}\). Let \(\{p_{j}\}_{j=0}^{\infty}\) be a probability distribution. Let \(\{\xi_{ni}\}_{i\in\mathbb{N},n\in\mathbb{Z}_{+}}\) be iid random variables with distribution \(\{p_{j}\}_{j=0}^{\infty}\). Let \(Z_{0}\) be a \(\mathbb{Z}_{+}\)-valued random variable independent of \(\{\xi_{ni}\}\). Let

\[Z_{n+1}=\sum_{i=1}^{Z_{n}}\xi_{ni}\quad\mbox{for}\quad n\geq 0.\]

If \(p_{0}=0\) and \(p_{1}<1\), then \(Z_{n}\to\infty\) w.p. 1. If \(p_{0}>0\), then \(P[Z_{n}\to\infty]+P[Z_{n}\to 0]=1\). Also, \(P[Z_{n}\to 0\mid Z_{0}=1]=q\) is the smallest solution in [0,1] to the equation

\[q=f(q)=\sum_{j=0}^{\infty}p_{j}q^{j}.\]

So \(q=1\) iff \(m\equiv\sum_{j=1}^{\infty}jp_{j}(1)\leq 1\) (see Chapter 18 also).

**Example 14.1.4:** (_Birth and death chains_). Again take \(\mathbb{S}=\mathbb{Z}_{+}\). Define \(P\) by

\[p_{i,i+1}=\alpha_{i}, p_{i,i-1}=\beta_{i}=1-\alpha_{i},\mbox{ for }i\geq 1,\] \[p_{0,1}=\alpha_{0}, p_{0,0}=\beta_{0}=1-\alpha_{0}.\]

The population increases at rate \(\alpha_{i}\) and decreases at rate \(1-\alpha_{i}\).

**Example 14.1.5:** (_Iterated function systems_). Let \(G:=\{h_{i}:h_{i}:\mathbb{S}\to\mathbb{S},\ i=1,2,\ldots,L\}\), \(L\leq\infty\). Let \(\mu=\{p_{i}\}_{i=1}^{L}\) be a probability distribution. Let \(\{f_{n}\}_{n=1}^{\infty}\) be iid, such that \(P(f_{n}=h_{i})=p_{i}\), \(1\leq i\leq L\). Let \(X_{0}\) be a \(\mathbb{S}\)-valued random variable independent of \(\{f_{n}\}_{n=1}^{\infty}\). Then, the iid random iteration scheme

\[X_{1} = f_{1}(X_{0})\] \[X_{2} = f_{2}(X_{1})\] \[\vdots\] \[X_{n+1} = f_{n+1}(X_{n})=f_{n+1}\big{(}f_{n}(\cdots(f_{1}(X_{0}))\cdots) \big{)}\]

is a Markov chain with transition probability matrix

\[p_{ij}=P(f_{1}(i)=j)=\sum_{r=1}^{L}p_{r}I\big{(}h_{r}(i)=j\big{)}.\]

**Remark 14.1.1:** Any discrete state space Markov chain can be generated in this way (see II in 14.1.3 below).

#### Existence of a Markov chain

1. _Kolmogorov's approach_. Let \(\Omega=\mathbb{S}^{\mathbb{Z}_{+}}=\{\omega:\omega\equiv\{x_{n}\}_{n=0}^{\infty},\)\(x_{n}\in\mathbb{S}\) for all \(n\}\) be the set of all sequences \(\{x_{n}\}_{n=0}^{\infty}\) with values in \(\mathbb{S}.\) Let \(\mathcal{F}_{0}\) consist of all _finite dimensional_ subsets of \(\Omega\) of the form \[A=\big{\{}\omega:\omega=\{x_{n}\}_{n=0}^{\infty},\ x_{j}=a_{j},\ 0\leq j\leq m \big{\}},\] where \(m<\infty\) and \(a_{j}\in\mathbb{S}\) for all \(j=0,1,\ldots,m.\) Let \(\mathcal{F}\) be the \(\sigma\)-algebra generated by \(\mathcal{F}_{0}.\) Fix \(\mu\) and \(\boldsymbol{P}.\) For \(A\) as above let \[\lambda_{\mu,\boldsymbol{P}}(A):=\mu_{a_{0}}p_{a_{0}a_{1}}p_{a_{1}a_{2}} \cdots p_{a_{m-1}a_{m}}.\] Then it can be shown, using the extension theorem from Chapter 2 or Kolmogorov's consistency theorem of Chapter 6, that \(\lambda_{\mu,\boldsymbol{P}}\) can be extended to be a probability measure on \(\mathcal{F}.\) Let \(X_{n}(\omega)=x_{n},\) if \(\omega=\{x_{n}\}_{n=0}^{\infty},\) be the coordinate projection. Then \(\{X_{n}\}_{n=0}^{\infty}\) will be a sequence of \(\mathbb{S}\)-valued random variables on \((\Omega,\mathcal{F},\lambda_{\mu,\boldsymbol{P}}),\) such that it is a Markov chain with initial distribution \(\mu\) and transition probability \(\boldsymbol{P}.\) A typical element \(\omega=\{x_{n}\}_{n=0}^{\infty}\) of \(\Omega\) is called a _sample path_ or a _sample trajectory_. The following are examples of events (sets) in \(\mathcal{F},\) which are not finite-dimensional: \[A_{1} = \Big{\{}\omega:\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^{n}h(x_{j}) \text{ exists}\Big{\}}\text{ for a given }h:\mathbb{S}\to\mathbb{R},\] \[A_{2} = \big{\{}\omega:\text{the set of limit points of }\{x_{n}\}_{n=0}^{\infty}=\{a,b\}\big{\}}\.\] Thus, it is _essential_ to go to \((\Omega,\mathcal{F},\lambda_{\mu,\boldsymbol{P}})\) to discuss the events involving asymptotic (long term) behavior, i.e., as \(n\to\infty.\)
2. _IIDRM approach (iteration of iid random maps)_. Let \(\boldsymbol{P}=((p_{ij}))_{K\times K}\) be a stochastic matrix. Let \(f:\mathbb{S}\times[0,1]\to\mathbb{S}\) be \[f(a_{i},u)=\left\{\begin{array}{ccc}a_{1}&\text{if}&0\leq u<p_{i1}\\ a_{2}&\text{if}&p_{i1}\leq u<p_{i1}+p_{i2}\\ \vdots\\ a_{j}&\text{if}&p_{i1}+p_{i2}+\cdots+p_{i(j-1)}\leq u<p_{i1}+p_{i2}\\ &\hskip 142.26378pt+\cdots+p_{ij}\\ \vdots\\ a_{K}&\text{if}&p_{i1}+p_{i2}+\cdots+p_{i(K-1)}\leq u<1\.\end{array}\right.\] Let \(U_{1},U_{2},\ldots\) be iid Uniform \([0,1]\) random variables. Let \(f_{n}(\cdot):=f(\cdot,U_{n}).\) Then for each \(n,\)\(f_{n}\) maps \(\mathbb{S}\) to \(\mathbb{S}.\) Also \(\{f_{n}\}_{n=1}^{\infty}\) are iid.

Let \(X_{0}\) be independent of \(\{U_{i}\}_{i=1}^{\infty}\) and \(X_{0}\sim\mu.\) Then the sequence \(\{X_{n}\}_{n=0}^{\infty}\) defined by

\[X_{n+1}=f_{n+1}(X_{n})=f(X_{n},U_{n+1})\]

is a Markov chain with initial distribution \(\mu\) and transition probability \(\boldsymbol{P}.\) The underlying probability space on which \(X_{0}\) and \(\{U_{i}\}_{i=1}^{\infty}\) are defined can be taken to be the Lebesgue space \(([0,1],\mathcal{B}([0,1]),m),\) where \(m\) is the Lebesgue measure.

**Finite Time Behavior of \(\{X_{n}\}\)**

For each \(n\in\mathbb{N},\)

\[P\big{(}X_{0}=a_{0},X_{1}=a_{1},\ldots,X_{n}=a_{n}\big{)} \tag{1.2}\] \[= \bigg{(}\prod_{j=1}^{n}P\big{(}X_{j}=a_{j}\mid X_{j-1}=a_{j-1}, \ldots,X_{0}=a_{0}\big{)}\bigg{)}P\big{(}X_{0}=a_{0}\big{)}\] \[= \bigg{(}\prod_{j=1}^{n}p_{a_{j-1}a_{j}}\bigg{)}\mu_{a_{0}}.\]

Thus, the joint distribution for any finite \(n\) is determined by \(\mu\) and \(\boldsymbol{P}.\) In particular,

\[P\big{(}X_{n}=a_{n}\mid X_{0}=a_{0}\big{)}=\sum\prod_{j=1}^{n}p_{a_{j-1}a_{j}} =(\boldsymbol{P}^{n})_{a_{0}a_{n}}, \tag{1.3}\]

where the sum in the middle term runs over all \(a_{1},a_{2},\ldots,a_{j-1}\) and \(\boldsymbol{P}^{n}\) is the \(n\)th power of \(\boldsymbol{P}.\) So the behavior of the distribution of \(X_{n}\) can be studied via that of \(\boldsymbol{P}^{n}\) for large \(n.\) But this analytic approach is not as comprehensive as the probabilistic one, via the concept of recurrence, which will be described next.

#### Limit theory

Let \(\{X_{n}\}_{n=0}^{\infty}\) be a Markov chain with state space \(\mathbb{S}=\{1,2,\ldots,K\},\)\(K\leq\infty,\) and transition probability matrix \(\boldsymbol{P}=\big{(}(p_{ij})\big{)}_{K\times K}.\)

**Definition 14.1.2:** (_Hitting times_). For any set \(A\subset\mathbb{S},\) the _hitting time_\(T_{A}\) is defined as

\[T_{A}=\inf\{n:X_{n}\in A,n\geq 1\}\]

i.e., it is the first time after \(0\) that the chain enters \(A.\) The random variable \(T_{A}\) is also called the _first passage time_ for \(A\) or the _first entrance time_ for \(A.\) Note that \(T_{A}\) is a _stopping time_ (cf. Chapter 13) w.r.t. the _filtration_\(\{\mathcal{F}_{n}\equiv\sigma\langle\{X_{j}:0\leq j\leq n\}\rangle\}_{n\geq 0}.\)

[MISSING_PAGE_FAIL:453]

\[\cdot P_{\mu}(X_{n}=i,\ X_{r}\neq i,\ 1\leq r\leq n-1)\] \[= P_{i}(X_{j}=a_{j},\ 1\leq j\leq k)P_{\mu}(X_{n}=i,\ X_{r}\neq i,\ 1\leq r\leq n-1)\] \[= P_{i}(X_{j}=a_{j},\ 1\leq j\leq k)P_{\mu}(T_{i}=n).\]

Adding both sides over \(n\) yields the result. \(\Box\)

The strong Markov property leads to the important useful technique of breaking up the time evolution of a Markov chain into iid cycles. This combined with the law of large numbers yield the basic convergence results.

**Definition 14.1.5:** (_IID cycles_). Let \(i\) be a state. Let \(T_{i}^{(0)}=0\) and

\[T_{i}^{(k+1)}=\left\{\begin{array}{ll}\inf\{n:n>T_{i}^{(k)},X_{n}=i\},&\mbox {if}\quad T_{i}^{(k)}<\infty,\\ \infty,&\mbox{if}\quad T_{i}^{(k)}=\infty,\end{array}\right. \tag{1.5}\]

i.e., for \(k=0,1,2,\ldots\), \(T_{i}^{(k)}\) is the successive return times to state \(i\).

**Proposition 14.1.2:** _Let \(i\) be a recurrent state. Then \(P_{i}\bigl{(}T_{i}^{(k)}<\infty\bigr{)}=1\) for all \(k\geq 1\)._

**Proof:** By definition of recurrence, the claim is true for \(k=1\). If it is true for \(k>1\), then

\[P_{i}\Bigl{(}T_{i}^{(k+1)}<\infty\Bigr{)}\] \[= \sum_{j=k}^{\infty}P\Bigl{(}T_{i}^{(k+1)}<\infty,T_{i}^{(k)}=j \Bigr{)}\] \[= \sum_{j=k}^{\infty}P_{i}\Bigl{(}T_{i}^{(1)}<\infty\Bigr{)}P_{i} \Bigl{(}T_{i}^{(k)}=j\Bigr{)}\quad\mbox{(by Markov property)}\] \[= P_{i}\Bigl{(}T_{i}^{(1)}<\infty\Bigr{)}P_{i}\Bigl{(}T_{i}^{(k)}< \infty\Bigr{)}=1.\]

\(\Box\)

Let \(\eta_{r}=\{X_{j},T_{i}^{(r)}\leq j\leq T_{i}^{(r+1)}-1;\,T_{i}^{(r+1)}-T_{i}^{( r)}\}\) for \(r=0,1,2,\ldots\). The \(\eta_{r}\)'s are called _cycles_ or _excursions_.

**Theorem 14.1.3:** _Let \(i\) be a recurrent state. Under \(P_{i}\), the sequence \(\{\eta_{r}\}_{r=0}^{\infty}\) are iid as random vectors with a random number of components. More precisely, for any \(k\in\mathbb{N}\),_

\[P_{i}\Bigl{(}\eta_{r}=(x_{r0},x_{r1},\ldots,x_{rj_{r}}),T_{i}^{(r +1)}-T_{i}^{(r)}=j_{r},r=0,1,\ldots,k\Bigr{)}\] \[=\prod_{r=0}^{k}P_{i}\Bigl{(}\eta_{1}=(x_{r0},x_{r1},\ldots,x_{rj_ {r}}),T_{i}^{(1)}=j_{r}\Bigr{)} \tag{1.6}\]

_for any \(\{x_{r0},x_{r1},\ldots,x_{rj_{r}},j_{r}\}\), \(r=0,1,\ldots,k\)._

**Proof:** Use the strong Markov property repeatedly (Problem 14.7).

**Proposition 14.1.4:**_For any state \(i\), let \(N_{i}\equiv\sum_{n=1}^{\infty}I_{\{i\}}(X_{n})\) be the total number of visits to \(i\). Then,_

* \(i\) _recurrent_ \(\Rightarrow P(N_{i}=\infty\mid X_{0}=i)=1\)_._
* \(i\) _transient_ \(\Rightarrow P(N_{i}=j\mid X_{0}=i)=f_{ii}^{j}(1-f_{ii})\) _for_ \(j=0,1,2,\ldots\) _where_ \(f_{ii}=P(T_{i}<\infty\mid X_{0}=i)\) _is the probability of returning to_ \(i\) _starting from_ \(i\)_, i.e., under_ \(P_{i}\)_,_ \(N_{i}\) _has a geometric distribution with parameter_ \((1-f_{ii})\)_._

**Proof:** Follows from the strong Markov property (Proposition 14.1.1).

**Corollary 14.1.5:**_A state \(i\) is recurrent iff_

\[E_{i}N_{i}\equiv E(N_{i}\mid X_{0}=i)=\sum_{n=1}^{\infty}p_{ii}^{(n)}=\infty. \tag{1.7}\]

**Proof:** If \(i\) is recurrent then \(P_{i}(N_{i}=\infty)=1\) and so \(E_{i}N_{i}=\infty\).

If \(i\) is transient then \(E_{i}N_{i}=\sum_{j=0}^{\infty}jf_{ii}^{j}(1-f_{ii})=\frac{f_{ii}}{1-f_{ii}}<\infty\). Also by the monotone convergence theorem,

\[E(N_{i}\mid X_{0}=i)=\sum_{n=1}^{\infty}E(\delta_{X_{n}i}\mid X_{0}=i)=\sum_{n =1}^{\infty}p_{ii}^{(n)}.\]

\(\Box\)

**An Application**

For the simple symmetric random walk (SSRW) in \(\mathbb{Z}\), \(0\) is recurrent. Indeed,

\[p_{00}^{(n)}=\left\{\begin{array}{lcl}0&\mbox{if}&n\mbox{ is odd}\\ {2k\choose k}{\left(\frac{1}{2}\right)}^{2k}&\mbox{if}&n=2k.\end{array}\right. \tag{1.8}\]

By Stirling's formula,

\[{2k\choose k}=\frac{(2k)!}{k!k!}\sim\frac{e^{-2k}(2k)^{2k+\frac{1}{2}}\sqrt{2 \pi}}{(e^{-k}k^{k+\frac{1}{2}}\sqrt{2\pi})^{2}}\sim\frac{4^{k}}{\sqrt{k}}\frac {1}{\sqrt{\pi}}\]

and hence

\[p_{00}^{(2k)}\sim\frac{1}{\sqrt{\pi}}\frac{1}{\sqrt{k}}.\]

Thus \(\sum_{k=1}^{\infty}p_{00}^{(2k)}=\infty\), implying that \(0\) is recurrent.

By a similar argument, it can be shown that the simple symmetric random walk in \(Z^{(2)}\), the integer lattice in \(\mathbb{R}^{2}\), the origin is _recurrent_, but in \(Z^{(d)}\) for \(d\geq 3\), the origin is transient (Problem 14.3).

**Corollary 14.1.6:**_If the state space \(\mathbb{S}\) is finite, then at least one state must be recurrent._

**Proof:** Let \(\mathbb{S}\equiv\{1,2,\ldots,K\}\), \(K<\infty\). Since \(n=\sum_{i=1}^{K}\sum_{j=1}^{n}\delta_{X_{j}i}\), there exists an \(i_{0}\) such that as \(n\rightarrow\infty\), \(\sum_{j=1}^{n}\delta_{X_{j}i_{0}}\rightarrow\infty\) with positive probability. This implies that \(i_{0}\) must be recurrent. \(\Box\)

**Definition 14.1.6:** (_Communication_). A state \(i\)_leads_ to \(j\) (write \(i\to j\)) if for some \(n\geq 1\), \(p_{ij}^{(n)}>0\). A pair of states \(i,j\) are said to _communicate_ if \(i\to j\) and \(j\to i\), i.e., if there exist \(n\geq 1\), \(m\geq 1\) such that \(p_{ij}^{(n)}>0\), \(p_{ji}^{(m)}>0\).

**Definition 14.1.7:** (_Irreducibility_). A Markov chain with state space \(\mathbb{S}\equiv\{1,2,\ldots,K\}\), \(K\leq\infty\) and transition probability matrix \(\mathbf{P}\equiv((p_{ij}))\) is _irreducible_ if for each \(i\), \(j\) in \(\mathbb{S}\), \(i\) and \(j\) communicate.

**Definition 14.1.8:** A state \(i\) is _absorbing_ if \(p_{ii}=1\).

It is easy to show that if \(i\) is absorbing and \(j\to i\), then \(j\) is _transient_ (Problem 14.4).

**Proposition 14.1.7:** (_Solidarity property_). _Let \(i\) be recurrent and \(i\to j\). Then \(f_{ji}=1\) and \(j\) is recurrent, where \(f_{ji}\equiv P(T_{i}<\infty\mid X_{0}=j)\)._

**Proof:** By the (strong) Markov property, \(1-f_{ii}=P(T_{i}=\infty\mid X_{0}=i)\geq P(T_{j}<\infty,T_{i}=\infty\mid X_{0}=i) =P(T_{i}=\infty\mid X_{0}=j)P(T_{j}<T_{i}\mid X_{0}=i)\) (intuitively speaking, one possibility of not returning to \(i\) (starting from \(i\)) is to visit \(j\) and then not returning to \(i)=(1-f_{ji})f_{ij}^{*}\), where \(f_{ij}^{*}=P(\)visiting \(j\) before visiting \(i\mid X_{0}=i)\). Now \(i\) recurrent and \(i\to j\) yield \(1-f_{ii}=0\) and \(f_{ij}^{*}>0\) (Problem 14.4) and so \(1-f_{ji}=0\), i.e., \(f_{ji}=1\). Thus, starting from \(j\), the chain visits \(i\) w.p. 1. From \(i\), it keeps returning to \(i\) infinitely often. In each of these excursions, the probability \(f_{ij}^{*}\) of visiting \(j\) is positive and since there are infinite number of such excursions and they are iid, the chain does visit \(j\) in one of these excursions w.p. 1. That is \(j\) is recurrent. \(\Box\)

Also an alternate proof using the Corollary 14.1.5 is possible (Problem 14.5).

**Proposition 14.1.8:**_In a finite state space irreducible Markov chain all states are recurrent._

**Proof:** By Corollary 14.1.6, there is at least one state \(i_{0}\) that is recurrent. By irreducibility and solidarity, this implies all states are recurrent.

**Remark 14.1.2:** A stronger result holds, namely, that for a finite state space irreducible Markov chain, all states are _positive recurrent_ (Problem 14.6).

**Theorem 14.1.9:** (_A law of large numbers_). _Let \(i\) be positive recurrent. Let_

\[N_{n}(j)=\#\{k:0\leq k\leq n,X_{k}=j\},\ j\in\mathbb{S}\]

_be the number of visits to \(j\) during \(\{0,1,\ldots,n\}\). Let \(\bigl{\{}L_{n}(j)\equiv\frac{N_{n}(j)}{n+1}\bigr{\}}\) be the empirical distribution at time \(n\). Let \(X_{0}=i\), with probability 1. Then_

\[L_{n}(j)\to\frac{V_{ij}}{E_{i}(T_{i})},\ \ \mbox{w.p. 1}\]

_where \(V_{ij}=E_{i}\bigl{(}\sum_{k=0}^{T_{i}-1}\delta_{X_{k},j}\bigr{)}\) is the mean number of visits to \(j\) during \(\{0,1,\ldots,T_{i}-1\}\) starting from \(i\). In particular, \(L_{n}(i)\to\frac{1}{E_{i}T_{i}}\), w.p. 1._

**Proof:** For each \(n\), let \(k\equiv k(n)\) be such that \(T_{i}^{(k)}\leq n<T_{i}^{(k+1)}\). Then,

\[N_{T_{i}^{(k)}}(j)\leq N_{n}(j)\leq N_{T_{i}^{(k+1)}}(j),\]

i.e.,

\[\sum_{r=0}^{k}\xi_{r}\leq N_{n}(j)\leq\sum_{r=0}^{k+1}\xi_{r},\]

where \(\xi_{r}=\#\{l:T_{i}^{(r)}\leq l<T_{i}^{(r+1)},X_{l}=j\}\) is the number of visits to \(j\) during the \(r\)th excursion. Since \(V_{ij}\equiv E_{i}\xi_{1}\leq E_{i}T_{i}<\infty\), by the SLLN, with probability 1,

\[\frac{1}{k(n)}\sum_{r=0}^{k(n)}\xi_{r}\to E_{i}(\xi_{1})=V_{ij}\]

and \(\frac{1}{k}T_{i}^{(k)}\to E_{i}(T_{i})\). Note that since \(n\) is between \(T_{i}^{(k(n))}\) and \(T_{i}^{(k(n)+1)}\), so \(\frac{k}{k(n)}\to E_{i}T_{i}\). Since

\[\frac{k}{n}\frac{1}{k}\sum_{r=0}^{k}\xi_{r}\leq\frac{N_{n}(j)}{n}\leq\frac{k+1 }{n}\frac{1}{k+1}\sum_{r=0}^{k+1}\xi_{r},\]

it follows that \(L_{n}(j)=\frac{n}{n+1}\frac{N_{n}(j)}{n}\to\frac{E_{i}(\xi_{1})}{E_{i}(T_{i})}\). \(\Box\)

Note that the above proof works for any initial distribution \(\mu\) such that \(P_{\mu}(T_{i}<\infty)=1\) and further, the limit of \(L_{n}(j)\) is independent of any such \(\mu\). Thus, if \((\mathbb{S},\mathbf{P})\) is irreducible and one state \(i\) is positive recurrent, then for any initial distribution \(\mu\), \(P_{\mu}(T_{i}<\infty)=1\).

Note finally that the proof in Theorem 14.1.9 can be adapted to yield a criterion for transience, null recurrence, and positive recurrence of a state. Thus, the following holds.

**Corollary 14.1.10:** _Fix a state \(i\). Then_1. \(i\) _is transient iff_ \(\lim_{n\to\infty}N_{n}(i)\) _exists and is finite w.p. 1 for any initial distribution iff_ \[\lim_{n\to\infty}E_{i}N_{n}(i)=\sum_{k=0}^{\infty}p_{ii}^{(k)}<\infty.\]
2. \(i\) _is null recurrent iff_ \(\sum_{k=0}^{\infty}p_{ii}^{(k)}=\infty\) _and_ \[\lim_{n\to\infty}E_{i}L_{n}(i)=\lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n}p_{ii }^{(k)}=0.\]
3. \(i\) _is positive recurrent iff_ \[\lim_{n\to\infty}E_{i}L_{n}(i)=\lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n}p_{ii }^{(k)}>0.\]
4. _Let_ \((\mathbb{S},\boldsymbol{P})\) _be irreducible and let one state_ \(i\) _be positive recurrent. Then, for any_ \(j\) _and any initial distribution_ \(\mu\)_,_ \[L_{n}(j)\to\frac{1}{E_{j}T_{j}}\in(0,\infty)\quad\mbox{w.p. 1.}\]

Thus, for the symmetric simple random walk on the integers

\[p_{00}^{(2k)}\sim\frac{c}{\sqrt{k}}\mbox{ as }k\to\infty,\ \ 0<c<\infty\]

and hence

\[\sum_{n=0}^{\infty}p_{00}^{(n)}=\infty\quad\mbox{and}\quad\frac{1}{n}\sum_{k=0 }^{n}p_{00}^{(k)}\to 0.\]

Thus \(0\) is null recurrent.

It is not difficult to show that if \(j\) leads to \(i\), i.e., if \(f_{ji}\equiv P_{j}(T_{i}<\infty)\) is positive then the number \(\xi_{1}\) of visits to \(j\) between consecutive visits to \(i\) has all moments (Problem 14.9).

Using the SLLN, Theorem 14.1.9 can be extended as follows to cover both null and positive recurrent cases.

**Theorem 14.1.11:** _Let \(i\) be a recurrent state. Then, for any \(j\) and any initial distribution \(\mu\) such that \(P_{\mu}(T_{i}<\infty)=1\),_

\[L_{n}(j)\equiv\frac{1}{n+1}\sum_{k=0}^{n}\delta_{X_{kj}}\to\pi_{ij}\equiv\frac{ V_{ij}}{E_{i}T_{i}}\,,\quad\mbox{w.p. 1} \tag{1.11}\]

_as \(n\to\infty\) where \(V_{ij}<\infty\). (If \(E_{i}T_{i}=\infty\), then \(\pi_{ij}=0\) for all \(j\).)_

**Corollary 14.1.12:**_Let \((\mathbb{S},\boldsymbol{P})\) be irreducible and let one state be recurrent. Then, for any \(j\) and any initial distribution,_

\[L_{n}(j)\to c_{j}\quad\text{as}\quad n\to\infty,\quad\text{w.p. 1}, \tag{1.12}\]

_where \(c_{j}=1/E_{j}T_{j}\) if \(E_{j}T_{j}<\infty\) and \(c_{j}=0\) otherwise._

**The Basic Ergodic Theorem**

Taking expectation in Theorem 14.1.11 and using the bounded convergence theorem leads to

**Corollary 14.1.13:**_Let \(i\) be recurrent. Then, for any initial distribution \(\mu\) with \(P_{\mu}(T_{i}<\infty)=1\),_

\[E_{\mu}(L_{n}(j))=\frac{1}{n+1}\sum_{k=0}^{n}P_{\mu}(X_{k}=j)\to\frac{V_{ij}}{ E_{i}(T_{i})}:=\pi_{ij}\quad\text{as}\quad n\to\infty. \tag{1.13}\]

**Theorem 14.1.14:**_Let \(i\) be positive recurrent. Let \(\pi_{j}:=\pi_{ij}\) for \(j\in\mathbb{S}\). Then \(\{\pi_{j}\}_{j\in\mathbb{S}}\) is a stationary distribution for \(\boldsymbol{P}\), i.e., \(\sum_{j}\pi_{j}=1\) and \(\sum_{l\in\mathbb{S}}\pi_{l}p_{lj}=\pi_{j}\), for all \(j\)._

**Proof:**

\[\frac{1}{n+1}\sum_{k=0}^{n}p_{ij}^{(k)} = \frac{1}{n+1}\delta_{ij}+\frac{1}{n+1}\sum_{k=1}^{n}p_{ij}^{(k)}\] \[= \frac{1}{n+1}\delta_{ij}+\frac{1}{n+1}\sum_{k=1}^{n}\sum_{l}p_{il }^{(k-1)}p_{lj}\] \[= \frac{1}{(n+1)}\delta_{ij}+\sum_{l}\bigg{(}\frac{1}{n+1}\sum_{k=1 }^{n}p_{il}^{(k-1)}\bigg{)}p_{lj}.\]

Taking limit as \(n\to\infty\) and using Fatou's lemma yields

\[\pi_{j}\geq\sum_{l}\pi_{l}p_{lj}. \tag{1.14}\]

If strict inequality were to hold for some \(j_{0}\), then adding over \(j\) would yield

\[\sum_{j}\pi_{j}>\sum_{j}\sum_{l}\pi_{l}p_{lj}=\sum_{l}\pi_{l}\Big{(}\sum_{j}p_ {lj}\Big{)}=\sum_{l}\pi_{l}.\]

Since \(\sum_{j\in\mathbb{S}}V_{ij}=E_{i}T_{i}\), \(\sum_{j}\pi_{j}=1\), so there cannot be a strict inequality in (1.14) for any \(j\). \(\Box\)

Therefore, the following has been established.

**Theorem 14.1.15:**_Let \(i\) be a positive recurrent state. Let_

\[\pi_{j}:=\frac{E_{i}(\sum_{k=0}^{T_{i}-1}\delta_{X_{k},j})}{E_{i}(T_{i})}.\]

_Then_

* \(\{\pi_{j}\}_{j\in\mathbb{S}}\) _is a stationary distribution._
* _For any_ \(j\) _and any initial distribution_ \(\mu\) _with_ \(P_{\mu}(T_{i}<\infty)=1\)_,_
* \(L_{n}(j)\equiv\frac{1}{n+1}\#\{k:X_{k}=j,0\leq j\leq n\}\to\pi_{j}\) _w.p. 1_ \((P_{\mu})\)__

_In particular, if \(j=i\), we have_

\[\frac{1}{n+1}\sum_{k=0}^{n}p_{ii}^{(k)}\to\pi_{i}=\frac{1}{E_{i}(T_{i})}. \tag{1.15}\]

Now let \(i\) be a positive recurrent state and \(j\) be such that \(i\to j\). Then \(V_{ij}>0\) and by the solidarity property, \(f_{ji}=1\) and \(j\) is recurrent. Now taking \(\mu=\delta_{j}\) in (ii) above and using Corollary 14.1.13, leads to the conclusions that

\[\frac{1}{n+1}\sum_{k=0}^{n}p_{jj}^{(k)}\to\pi_{j}>0 \tag{1.16}\]

and

\[\pi_{j}=\frac{1}{E_{j}T_{j}}. \tag{1.17}\]

Thus, \(j\) is positive recurrent.

Now Theorem 14.1.15 leads to the _basic ergodic theorem for Markov chains_.

**Theorem 14.1.16:**_Let \((\mathbb{S},\boldsymbol{P})\) be irreducible and let one state be positive recurrent. Then_

* _all states are positive recurrent,_
* \(\pi\equiv\{\pi_{j}\equiv(E_{j}T_{j})^{-1}:j\in\mathbb{S}\}\) _is a stationary distribution for_ \((\mathbb{S},\boldsymbol{P})\)_,_
* _for any initial distribution_ \(\mu\) _and any_ \(j\in\mathbb{S}\)__
* \(\frac{1}{n+1}\sum_{k=0}^{n}P_{\mu}(X_{k}=j)\to\pi_{j}\)_, i.e.,_ \(\pi\) _is the unique limiting distribution (in the average sense) and hence the unique stationary distribution,_
_._
2. \(L_{n}(j)\equiv\frac{1}{n+1}\sum_{k=0}^{n}\delta_{X_{k}j}\to\pi_{j}\) _w.p. 1_ \((P_{\mu})\)_._

There is a converse to the above result. To develop this, note first that if \(j\) is a transient state, then the total number \(N_{j}\) of visits to \(j\) is finite w.p. 1 (for any initial distribution \(\mu\)) and hence \(L_{n}(j)\to 0\) w.p. 1 and taking expectations, for any \(i\)

\[\frac{1}{n+1}\sum_{k=0}^{n}p_{ij}^{(k)}\to 0\quad\mbox{as}\quad n\to\infty.\]

Now, suppose that \(\pi\equiv\{\pi_{j}:j\in\mathbb{S}\}\) is a stationary distribution for \((\mathbb{S},\mathbf{P})\). Then, for all \(j\), \(\pi_{j}=\sum_{i\in\mathbb{S}}\pi_{i}p_{ij}\) and hence

\[\pi_{j} = \sum_{i\in\mathbb{S}}\sum_{r\in\mathbb{S}}\pi_{r}p_{ri}p_{ij}\] \[= \sum_{r\in\mathbb{S}}\pi_{r}p_{rj}^{(2)}\]

and by induction,

\[\pi_{j}=\sum_{i\in\mathbb{S}}\pi_{i}p_{ij}^{(k)}\quad\mbox{for all}\quad k\geq 0\]

implying

\[\pi_{j}=\sum_{i\in\mathbb{S}}\pi_{i}\biggl{(}\frac{1}{n+1}\sum_{k=0}^{n}p_{ij} ^{(k)}\biggr{)}.\]

Now if \(j\) is transient then for any \(i\)

\[\frac{1}{n+1}\sum_{k=0}^{n}p_{ij}^{(k)}\to 0\quad\mbox{as}\quad n\to\infty\]

and so by the bounded convergence theorem

\[\pi_{j}=\lim_{n\to\infty}\sum_{i\in\mathbb{S}}\pi_{i}\biggl{(}\frac{1}{n+1} \sum_{k=0}^{n}p_{ij}^{(k)}\biggr{)}=\sum_{i\in\mathbb{S}}\pi_{i}\cdot 0=0.\]

Thus, \(\pi_{j}>0\) implies \(j\) is recurrent. For \(j\) recurrent, it follows from arguments similar to those used to establish Theorem 14.1.15 that

\[\frac{1}{n}\sum_{k=0}^{n}p_{ij}^{(k)}\to f_{ij}\frac{1}{E_{j}T_{j}}.\]

Thus, \(\pi_{j}=\Big{(}\sum_{i\in\mathbb{S}}\pi_{i}f_{ij}\Big{)}\frac{1}{E_{j}T_{j}}\). But \(\sum_{i\in\mathbb{S}}\pi_{i}f_{ij}\geq\pi_{j}f_{jj}=\pi_{j}>0\). So \(E_{j}T_{j}<\infty\), i.e., \(j\) is positive recurrent. Summarizing the above discussion leads to 

**Proposition 14.1.17:**_Let \(\pi\equiv\{\pi_{j}:j\in\mathbb{S}\}\) be a stationary distribution for \((\mathbb{S},\boldsymbol{P})\). Then, \(\pi_{j}>0\) implies that \(j\) is positive recurrent._

It is now possible to state a converse to Theorem 14.1.16.

**Theorem 14.1.18:**_Let \((\mathbb{S},\boldsymbol{P})\) be irreducible and admit a stationary distribution \(\pi\equiv\{\pi_{j}:j\in\mathbb{S}\}\). Then,_

* _all states are positive recurrent,_
* \(\pi\equiv\{\pi_{j}:\pi_{j}=\frac{1}{E_{j}T_{j}}\)_,_ \(j\in\mathbb{S}\}\) _is the unique stationary distribution,_
* _for any initial distribution_ \(\mu\) _and for all_ \(j\in\mathbb{S}\)_,_
* \(\frac{1}{n+1}\sum_{k=0}^{n}\delta_{X_{k}j}\to\pi_{j}\) _w.p. 1_ \((P_{\mu})\)_._

In summary, for an irreducible Markov chain \((\mathbb{S},\boldsymbol{P})\) with a countable state space, a stationary distribution \(\pi\) exists iff all states are positive recurrent. In which case, \(\pi\) is unique and for any initial distribution \(\mu\), the distribution at time \(n\) converges to \(\pi\) in the Cesaro sense (i.e., average) and the (LLN) law of large numbers holds. For the finite state space case, irreducibility suffices (Problem 14.6).

If \(h:\mathbb{S}\to\mathbb{R}\) is a function such that \(\sum_{j\in\mathbb{S}}|h(j)|\pi_{j}<\infty\), then the LLN can be strengthened to

\[\frac{1}{n+1}\sum_{k=0}^{n}h(X_{k})\to\sum_{j=0}^{\infty}h(j)\pi_{j}\mbox{ w.p. 1 }(P_{\mu})\]

for any \(\mu\) (Problem 14.10). In particular, if \(A\) is any subset of \(\mathbb{S}\), then

\[L_{n}(A)\equiv\frac{1}{n+1}\sum_{k=0}^{n}I_{A}(X_{k})\to\pi(A)\equiv\sum_{j\in A }\pi_{j}\]

w.p. 1 \((P_{\mu})\) for any \(\mu\).

An important question that remains is whether the convergence of \(P_{\mu}(X_{n}=j)\) to \(\pi_{j}\) can be strengthened from the average sense to full, i.e., from the convergence to \(\pi_{j}\) of \(\frac{1}{(n+1)}\sum_{k=0}^{n}P_{\mu}(X_{k}=j)\) to the convergence to \(\pi_{j}\) of \(P_{\mu}(X_{n}=j)\) as \(n\to\infty\). For this, the additional hypothesis needed is _aperiodicity_.

**Definition 14.1.9:** For any state \(i\), the _period_\(d_{i}\) of the state \(i\) is the

\[g.c.d.\big{\{}n:n\geq 1,p_{ii}^{(n)}>0\big{\}}.\]Further, \(i\) is called _aperiodic_ if \(d_{i}=1\).

**Example 14.1.8:** Let \(\mathbb{S}=\{0,1,2\}\) and

\[P=\left(\begin{array}{ccc}0&1&0\\ \frac{1}{2}&0&\frac{1}{2}\\ 0&1&0\end{array}\right).\]

Then \(d_{i}=2\) for all \(i\).

Note that in this example, since \((\mathbb{S},\boldsymbol{P})\) is finite and irreducible, it has a unique stationary distribution, given by \(\pi=(\frac{1}{4},\frac{1}{2},\frac{1}{4})\) and

\[\frac{1}{n+1}\sum_{k=0}^{n}p_{00}^{(k)}\rightarrow\frac{1}{4}\quad\mbox{as} \quad n\rightarrow\infty\]

but \(p_{00}^{(2n+1)}=0\) for each \(n\) and \(p_{00}^{(2n)}\rightarrow\frac{1}{4}\) as \(n\rightarrow\infty\). This suggests that aperiodicity will be needed. It turns out that if \((\mathbb{S},\boldsymbol{P})\) is irreducible, the period \(d_{i}\) is the same for all \(i\) (Problem 14.5).

**Theorem 14.1.19:** _Let \((\mathbb{S},\boldsymbol{P})\) be irreducible, positive recurrent and aperiodic (i.e., \(d_{i}=1\) for all \(i\)). Let \(\{X_{n}\}_{n\geq 0}\) be a \((\mathbb{S},\boldsymbol{P})\) Markov chain. Then, for any initial distribution \(\mu\), \(\lim_{n\rightarrow\infty}P_{\mu}(X_{n}=i)\equiv\pi_{i}\) exists for all \(i\), where \(\pi\equiv\{\pi_{j}:j\in\mathbb{S}\}\) is the unique stationary distribution._

There are many proofs known for this, and two of them are outlined below. The first uses the discrete renewal theorem and the second uses a coupling argument.

**Proof 1:** (_via the discrete renewal theorem_). Fix a state \(i\). Recall that for \(n\geq 1\), \(p_{ii}^{(n)}=P(X_{n}=i\mid X_{0}=i)\), \(n\geq 0\) and \(f_{ii}^{(n)}=P(T_{i}=n\mid X_{0}=i)\), \(n\geq 1\). Using the Markov property, for \(n\geq 1\),

\[p_{ii}^{(n)}=P(X_{n}=i\mid X_{0}=i) = \sum_{k=1}^{n}P(X_{n}=i,T_{i}=k\mid X_{0}=i)\] \[= \sum_{k=1}^{n}P(T_{i}=k\mid X_{0}=i)P(X_{n}=i\mid X_{k}=i)\] \[= \sum_{k=1}^{n}f_{ii}^{(k)}p_{ii}^{(n-k)}.\]

Let \(a_{n}\equiv p_{ii}^{(n)}\), \(n\geq 0\), \(p_{n}\equiv f_{ii}^{(n)}\), \(n\geq 1\). Then \(\{p_{n}\}_{n\geq 1}\) is a probability distribution and \(\{a_{n}\}_{n\geq 0}\) satisfies the discrete renewal equation

\[a_{n}=b_{n}+\sum_{k=0}^{n}a_{n-k}p_{k},\ n\geq 0\]where \(b_{n}=\delta_{n0}\) and \(p_{0}=0\). It can be shown that \(d_{i}=1\) iff

\[g.c.d.\big{\{}k:k\geq 1,p_{k}>0\big{\}}=1.\]

Further, \(E_{i}T_{i}=\sum_{k=1}^{\infty}kp_{k}<\infty\), by the assumption of positive recurrence. Now it follows from the discrete renewal theorem (see Section 8.5) that

\[\lim_{n\to\infty}a_{n}\ \ \mbox{exists and equals}\ \ \frac{\sum_{j=0}^{\infty}b_{j}}{\sum_{k=1}^{\infty}kp_{k}}=\frac{1}{E_{i}T_{i}}=\pi_{i}.\]

\(\Box\)

**Proof 2:** (_Using coupling arguments_). Let \(\{X_{n}\}_{n\geq 0}\) and \(\{Y_{n}\}_{n\geq 0}\) be independent \((\mathbb{S},\mathbf{P})\) Markov chains such that \(Y_{0}\) has distribution \(\pi\) and \(X_{0}\) has distribution \(\mu\). Then \(\{Z_{n}=(X_{n},Y_{n})\}_{n\geq 0}\) is a Markov chain with state space \(\mathbb{S}\times\mathbb{S}\) and transition probability \(\mathbf{P}\times\mathbf{P}\equiv\big{(}(p_{(i,j),(k,\ell)}=p_{ ik}p_{j\ell})\big{)}\). Further, it can be shown that (see Hoel, Port and Stone (1972))

* \(\{\pi_{i,j}\equiv\pi_{i}\pi_{j}:(i,j)\in\mathbb{S}\times\mathbb{S}\}\) is a stationary distribution for \(\{Z_{n}\}\),
* since \((\mathbb{S},\mathbf{P})\) is irreducible and aperiodic, the pair \((\mathbb{S}\times\mathbb{S},\mathbf{P}\times\mathbf{P})\) is irreducible.

Since \((\mathbb{S}\times\mathbb{S},\mathbf{P}\times\mathbf{P})\) is irreducible and admits a stationary distribution, it is necessarily recurrent and so from any initial distribution the first passage time \(T_{D}\) for the diagonal \(D\equiv\{(i,i):i\in\mathbb{S}\}\) is finite with probability one. Thus, \(T_{c}\equiv\min\{n:n\geq 1,X_{n}=Y_{n}\}\) is finite w.p. 1. The random variable \(T_{c}\) is called the _coupling time_. Let

\[\tilde{X}_{n}=\left\{\begin{array}{ccc}X_{n}&,&n\leq T_{c}\\ Y_{n}&,&n>T_{c}.\end{array}\right.\]

Then, it can be verified that \(\{X_{n}\}_{n\geq 0}\) and \(\{\tilde{X}_{n}\}_{n\geq 0}\) are identically distributed Markov chains. Thus

\[P(X_{n}=j) = P(\tilde{X}_{n}=j)\] \[= P(\tilde{X}_{n}=j,T_{c}<n)+P(\tilde{X}_{n}=j,T_{c}\geq n)\]

and

\[P(Y_{n}=j)=P(Y_{n}=j,T_{c}<n)+P(Y_{n}=j,T_{c}\geq n)\]

implying that

\[\big{|}P(X_{n}=j)-P(Y_{n}=j)\big{|}\leq 2P(T_{c}\geq n).\]

Since \(P(T_{c}\geq n)\to 0\) as \(n\to\infty\) and by the stationarity of \(\pi\), \(P(Y_{n}=j)=\pi_{j}\) for all \(n\) and \(j\) it follows that for any \(j\)

\[\lim_{n\to\infty}P(X_{n}=j)\quad\mbox{exists and}\quad=\pi_{j}.\]In order to obtain results on rates of convergence for \(|P(X_{n}=j)-\pi_{j}|\), one needs more assumptions on the distribution of return time \(T_{i}\) or the coupling time \(T_{c}\). For results on this, the books of Hoel et al. (1972), Meyn and Tweedie (1993), and Lindvall (1992) are good sources. It can be shown that in the irreducible case if for some \(i\), \(P_{i}(T_{i}>n)=O(\lambda_{1}^{n})\) for some \(0<\lambda_{1}<1\), then \(\sum_{j\in\mathbb{S}}|P_{i}(X_{n}=j)-\pi_{j}|=O(\lambda_{2}^{n})\) for some \(\lambda_{1}<\lambda_{2}<1\). In particular, this geometric convergence holds for the finite state space irreducible case.

The main results of this section are summarized below.

**Theorem 14.1.20:**_Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain with a countable state space \(\mathbb{S}=\{0,1,2,\ldots,K\}\), \(K\leq\infty\) and transition probability matrix \(\boldsymbol{P}\equiv\big{(}(p_{ij})\big{)}\). Let \((\mathbb{S},\boldsymbol{P})\) be irreducible. Then,_

* _All states are recurrent iff for some_ \(i\) _in_ \(\mathbb{S}\)_,_ \[\sum_{n=1}^{\infty}p_{ii}^{(n)}=\infty.\]
* _All states are positive recurrent iff for some_ \(i\) _in_ \(\mathbb{S}\)_,_ \[\lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n}p_{ii}^{(k)}\quad\text{exists and is strictly positive}.\]
* _There exists a stationary probability distribution_ \(\pi\) _iff there exists a positive recurrent state._
* _If there exists a stationary distribution_ \(\pi\equiv\{\pi_{j}:j\in\mathbb{S}\}\)_, then_
* _it is unique, all states are positive recurrent and for all_ \(j\in\mathbb{S}\)_,_ \(\pi_{j}=(E_{j}T_{j})^{-1}\)_,_
* _for all_ \(i,j\in\mathbb{S}\)_,_ \[\frac{1}{n+1}\sum_{k=0}^{n}p_{ij}^{(k)}\to\pi_{j}\quad\text{as}\quad n\to\infty,\]
* _for any initial distribution and any_ \(j\in\mathbb{S}\)_,_ \[\frac{1}{n+1}\sum_{k=0}^{n}\delta_{X_{k}j}\to\pi_{j}\quad w.p.\ 1,\]
* _if_ \(\sum_{j\in\mathbb{S}}|h(j)|\pi_{j}<\infty\)_, then_ \[\frac{1}{n+1}\sum_{k=0}^{n}h(X_{k})\to\sum_{j\in\mathbb{S}}h(j)\pi_{j}\quad w. p.\ 1,\]_
* _if, in addition,_ \(d_{i}=1\) _for some_ \(i\in\mathbb{S}\)_, then_ \(d_{j}=1\) _for all_ \(j\in\mathbb{S}\) _and for all_ \(i,j\)_,_ \[p_{ij}^{(n)}\to\pi_{j}\quad\mbox{as}\quad n\to\infty.\]

### 14.2 Markov chains on a general state space

#### Basic definitions

Let \(\{X_{n}\}_{n\geq 0}\) be a sequence of random variables with values in some space \(\mathbb{S}\) that is not necessarily finite or countable. The Markov property says that conditioned on \(X_{n},X_{n-1},\ldots,X_{0}\), the distribution of \(X_{n+1}\) depends only on \(X_{n}\) and not on the past, i.e., \(X_{j}:j\leq n-1\). When \(\mathbb{S}\) is not countable, to make this notion of Markov property precise, one needs the following set up.

Let \((\mathbb{S},\mathcal{S})\) be a measurable space. Let \((\Omega,\mathcal{F},P)\) be a probability space and \(\{X_{n}(\omega)\}_{n\geq 0}\) be a sequence of maps from \(\Omega\) to \(\mathbb{S}\) such that for each \(n\), \(X_{n}\) is \((\mathcal{F},\mathbb{S})\) measurable. Let \(\mathcal{F}_{n}\equiv\sigma\langle\{X_{j}:0\leq j\leq n\}\rangle\) be the sub-\(\sigma\)-algebra of \(\mathcal{F}\) generated by \(\{X_{j}:0\leq j\leq n\}\). In what follows, for any sub-\(\sigma\)-algebra \(\mathcal{Y}\) of \(\mathcal{F}\), let \(P(\cdot\mid\mathcal{Y})\) denote the conditional probability given, \(\mathcal{Y}\) as defined in Chapter 12.

**Definition 14.2.1:** The sequence \(\{X_{n}\}_{n\geq 0}\) is a _Markov chain_ if for all \(A\in\mathcal{S}\),

\[P\big{(}(X_{n+1}\in A)\mid\mathcal{F}_{n}\big{)}=P\big{(}(X_{n+1}\in A)\mid \sigma\langle X_{n}\rangle\big{)}\quad\mbox{w.p. 1, for all}\quad n\geq 0, \tag{2.1}\]

for any initial distribution of \(X_{0}\), where \(\sigma\langle X_{n}\rangle\) is the sub-\(\sigma\)-algebra of \(\mathcal{F}\) generated by \(X_{n}\).

It is easy to verify that (2.1) holds for all \(A\in\mathcal{S}\) iff for any bounded measurable \(h\) from \((\mathbb{S},\mathcal{S})\) to \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\),

\[E\big{(}h(X_{n+1})\mid\mathcal{F}_{n}\big{)}=E\big{(}h(X_{n+1})\mid\sigma \langle X_{n}\rangle\big{)}\quad\mbox{w.p. 1 for all}\quad n\geq 0 \tag{2.2}\]

for any initial distribution of \(X_{0}\).

Another equivalent formulation that makes the Markov property symmetric with respect to time is the following that says that given the _present_, the _past_ and _future_ are independent.

**Proposition 14.2.1:**_A sequence of random variables \(\{X_{n}\}_{n\geq 0}\) satisfies (2.1) iff for any \(\{A_{j}\}_{0}^{n+k}\subset\mathcal{S}\),_

\[P\big{(}X_{j}\in A_{j},j=0,1,2,\ldots,n-1,n+1,\ldots,n+k\mid \sigma\langle X_{n}\rangle\big{)}\] \[= P\big{(}X_{j}\in A_{j},j=0,1,2,\ldots,n-1\mid\sigma\langle X_{n} \rangle\big{)}\times\] \[P\big{(}X_{j}\in A_{j},j=n+1,\ldots,n+k\mid\sigma\langle X_{n} \rangle\big{)}\]

### w.p. 1

The proof is somewhat involved but not difficult. The countable state space case is easier (Problem 14.1).

An important tool for studying Markov chains is the notion of a _transition probability function_.

**Definition 14.2.2:** A function \(P:\mathbb{S}\times\mathcal{S}\to[0,1]\) is called a _transition probability function_ on \(\mathbb{S}\) if

1. for all \(x\) in \(\mathbb{S}\), \(P(x,\cdot)\) is a probability measure on \((\mathbb{S},\mathcal{S})\),
2. for all \(A\in\mathcal{S}\), \(P(\cdot,A)\) is an \(\mathcal{S}\)-measurable function from \((\mathbb{S},\mathcal{S})\to[0,1]\).

Under some general conditions guaranteeing the existence of regular conditional probabilities, the right side of (2.1) can be expressed as \(P_{n}(X_{n},A)\), where \(P_{n}(\cdot,\cdot)\) is a transition probability function on \(\mathbb{S}\). In such a case, yet another formulation of Markov property is in terms of the joint distributions of \(\{X_{0},X_{1},\ldots,X_{n}\}\) for any finite \(n\).

**Proposition 14.2.2:**_A sequence \(\{X_{n}\}_{n\geq 0}\) satisfies (2.1) iff for any \(n\in\mathbb{N}\) and \(A_{0},A_{1},\ldots,A_{n}\in\mathcal{S}\),_

\[P(X_{j}\in A_{j},j=0,1,2,\ldots,n)\] \[= \int_{A_{0}}\int_{A_{n-2}}\int_{A_{n-1}}P_{n-1}(x_{n-1},A_{n})P_{ n-2}(x_{n-2},dx_{n-1})\] \[\ldots P_{1}(x_{0},dx_{1})\mu_{0}(dx_{0}),\]

where \(\mu_{0}(A)=P(x_{0}\in A)\), \(A\in\mathcal{S}\).

The proof is by induction and left as an exercise (Problem 14.16). In what follows, it will be assumed that such transition functions exist.

**Definition 14.2.3:** A sequence of \(\mathbb{S}\)-valued random variables \(\{X_{n}\}_{n\geq 0}\) is called a _Markov chain with transition function \(P(\cdot,\cdot)\)_ if (2.1) holds and the right side equals \(P(X_{n},A)\) for all \(n\in\mathbb{N}\).

#### 14.2.2 Examples

**Example 14.2.1:** (_IID sequence_). Let \(\{X_{n}\}_{n\geq 0}\) be iid \(\mathbb{S}\)-valued random variables with distribution \(\mu\). Then \(\{X_{n}\}_{n\geq 0}\) is a Markov chain with transition function \(P(x,A)\equiv\mu(A)\) and initial distribution \(\mu\).

**Example 14.2.2:** (_(Additive) random walk in \(\mathbb{R}^{k}\)_). Let \(\{\eta_{j}\}_{j\geq 1}\) be iid \(\mathbb{R}^{k}\)-valued random variables with distribution \(\nu\). Let \(X_{0}\) be an \(\mathbb{R}^{k}\)-valued random variable independent of \(\{\eta_{j}\}_{j\geq 1}\) and with distribution \(\mu\). Let

\[X_{n+1} = X_{n}+\eta_{n+1}\]\[X_{n+1}=\max\big{\{}X_{n}+\eta_{n+1},0\big{\}}.\]

Then \(\{X_{n}\}_{n\geq 0}\) is a nonnegative valued Markov chain with transition function \(P(x,A)\equiv P\big{(}\max\{x+\eta_{1},0\}\in A\big{)}\) and initial distribution \(\mu\). In the queuing theory context, if \(\eta_{n}\) represents the difference between the \(n\)th interarrival time and service time, then \(X_{n}\) represents the waiting time at the \(n\)th arrival.

All the above are special cases of the following:

**Example 14.2.7:** (_Iterated function system (IFS)_). Let \((\mathbb{S},\mathcal{S})\) be a measurable space. Let \((\Omega,\mathcal{F},P)\) be a probability space. Let \(\{f_{i}(x,\omega)\}_{i\geq 1}\) be such that for each \(i\), \(f_{i}:\mathbb{S}\times\Omega\rightarrow\mathbb{S}\) is \((\mathcal{S}\times\mathcal{F},\mathcal{S})\)-measurable and the stochastic processes \(\{f_{i}(\cdot,\omega)\}_{i\geq 1}\) are iid. Let \(X_{0}\) be a \(\mathbb{S}\)-valued random variable on \((\Omega,\mathcal{F},P)\) with distribution \(\mu\) and independent of \(\{f_{i}(\cdot,\cdot)\}_{i\geq 1}\). Let

\[X_{n+1}(\omega)=f_{n+1}(X_{n}(\omega),\omega),\ n\geq 0.\]

Then \(\{X_{n}\}_{n\geq 0}\) is an \(\mathbb{S}\)-valued Markov chain with transition function \(P(x,A)\equiv P(f_{1}(x,\omega)\in A)\) and initial distribution \(\mu\).

It turns out that when \(\mathbb{S}\) is a Polish space with \(\mathcal{S}\) as the Borel \(\sigma\)-algebra and \(P(\cdot,\cdot)\) is a transition function on \(\mathbb{S}\), any \(\mathbb{S}\)-valued Markov chain \(\{X_{n}\}_{n\geq 0}\) with transition function \(P(\cdot,\cdot)\) can be generated by an IFS as in Example 14.2.7. For a proof, see Kifer (1988) and Athreya and Stenflo (2003). When \(\{f_{i}\}_{i\geq 1}\) are iid such that \(f_{1}\) has only finite many choices \(\{h_{j}\}_{j=1}^{k}\), where each \(h_{j}\) is an affine contraction on \(\mathbb{R}^{p}\), then the Markov chain \(\{X_{n}\}\) converges in distribution to some \(\pi(\cdot)\). Further, the limit point set of \(\{X_{n}\}\) coincides w.p. 1 with the support \(M\) of the limit distribution \(\pi(\cdot)\). This has been exploited by Barnsley and others to solve the inverse problem: given a compact set \(M\) in \(\mathbb{R}^{p}\), find an IFS \(\{h_{j}\}_{j=1}^{k}\), of affine contractions so that by generating the Markov chain \(\{X_{n}\}\), one can get an approximate picture of \(M\). This is called data compression or image generation by Markov chain Monte Carlo. See Barnsley (1992) for details on this. More generally, when \(\{f_{i}\}\) are Lipschitz maps, the following holds.

**Theorem 14.2.3:** _Let \(\{f_{i}(\cdot,\cdot)\}_{i\geq 1}\) be iid Lipschitz maps on \(\mathbb{S}\). Assume_

* \(E|\log s(f_{1})|<\infty\) _and_ \(E\log s(f_{1})<0\)_, where_ \(s(f_{1})=\sup\limits_{x\neq y}\dfrac{d\big{(}f_{1}(x,\omega),f_{1}(y,\omega) \big{)}}{d(x,y)}\) _and_ \(d(\cdot,\cdot)\) _is the metric on_ \(\mathbb{S}\)_, and_
* _for some_ \(x_{0}\)_,_ \(E\big{(}\log d(f_{1}(x_{0},\omega),x_{0})\big{)}^{+}<\infty\)_._

_Then,_

* \(\hat{X}_{n}(x,\omega)\equiv f_{1}\big{(}f_{2}(\ldots f_{n-1}(f_{n}(x,\omega), \omega))\ldots\big{)}\) _converges w.p. 1 to a random variable_ \(\hat{X}(\omega)\) _that is independent of_ \(x\)_,_
* _for all_ \(x\)_,_ \(X_{n}(x,\omega)\equiv f_{n}\big{(}f_{n-1}\ldots(f_{1}(x,\omega),\omega)\ldots \big{)}\) _converges in distribution to_ \(\hat{X}(\omega)\)That (ii) is a consequence of (i) is clear since for each \(n\), \(x\), \(X_{n}(x,\omega)\) and \(\hat{X}_{n}(x,\omega)\) are identically distributed.

The proof of (i) involves showing that \(\{\hat{X}_{n}\}\) is a Cauchy sequence in \(\mathbb{S}\) w.p. 1 (Problem 14.17).

#### Chapman-Kolmogorov equations

Let \(P(\cdot,\cdot)\) be a transition function on \((\mathbb{S},\mathcal{S})\). For each \(n\geq 0\), define a sequence of functions \(\{P^{(n)}(\cdot,\cdot)\}_{n\geq 0}\) by the iteration scheme

\[P^{(n+1)}(x,A)=\int_{\mathbb{S}}P^{(n)}(y,A)P(x,dy),\ n\geq 0, \tag{2.3}\]

where \(P^{(0)}(x,A)\equiv I_{A}(x)\). It can be verified by induction that for each \(n\), \(P^{(n)}(\cdot,\cdot)\) is a transition probability function.

**Definition 14.2.4:**_\(P^{(n)}(\cdot,\cdot)\) defined by (2.3) is called the \(n\)-step transition function generated by \(P(\cdot,\cdot)\)._

It is easy to show by induction that if \(X_{0}=x\) w.p. 1, then

\[P(X_{n}\in A)=P^{(n)}(x,A)\quad\mbox{for all}\quad n\geq 0 \tag{2.4}\]

(Problem 14.18). This leads to the _Chapman-Kolmogorov equations_.

**Proposition 14.2.4:**_Let \(P(\cdot,\cdot)\) be a transition probability function on \((\mathbb{S},\mathcal{S})\) and let \(P^{(n)}(\cdot,\cdot)\) be defined by (2.3). Then for any \(n\), \(m\geq 0,\)_

\[P^{(n+m)}(x,A)=\int P^{(n)}(y,A)P^{(m)}(x,dy). \tag{2.5}\]

**Proof:** The analytic verification is straightforward by induction. One can verify this probabilistically using the Markov property. Indeed, from (2.4) the left side of (2.5) is

\[P_{x}(X_{n+m}\in A) = E_{x}\big{(}P(X_{n+m}\in A\mid\mathcal{F}_{m})\big{)}\] \[= E_{x}\big{(}P^{(n)}(X_{m},A)\big{)}\quad\mbox{(by Markov property)}\] \[= \mbox{right-hand side of (\ref{eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq: eq:eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq: eq:the operator \(\boldsymbol{P}\) on the Banach space \(\mathcal{B}(\mathbb{S},\mathbb{R})\) of bounded measurable real valued functions from \(\mathbb{S}\) to \(\mathbb{R}\) (with sup norm), defined by

\[(\boldsymbol{P}h)(x)\equiv E_{x}h(X_{1})\equiv\int h(y)P(x,dy). \tag{2.6}\]

It is easy to verify that \(\boldsymbol{P}\) is a positive bounded linear operator on \(\mathcal{B}(\mathbb{S},\mathbb{R})\) of norm one. The Chapman-Kolmogorov equation (2.4) is equivalent to saying that \(E_{x}h(X_{n})=(\boldsymbol{P}^{n}h)(x)\). Thus, analytically the study of the limit distribution of \(\{X_{n}\}_{n\geq 0}\) can be reduced to that of the sequence \(\{\boldsymbol{P}^{n}\}_{n\geq 0}\) of the operator \(\boldsymbol{P}\). However, probabilistic approaches via the notion of Harris irreducibility and recurrence when applicable and via the notion of Feller continuity when \(\mathbb{S}\) is a Polish space are more fruitful and will be developed below.

#### Harris irreducibility, recurrence, and minorization

##### Definition of irreducibility

Recall that a Markov chain \(\{X_{n}\}_{n\geq 0}\) with a discrete state space \(\mathbb{S}\) and transition probability matrix \(P\equiv((p_{ij}))\) is _irreducible_ if for every \(i\), \(j\) in \(\mathbb{S}\), \(i\) leads to \(j\), i.e.,

\[P(X_{n}=j\quad\text{for some}\quad n\geq 1\mid X_{0}=i)\] \[\equiv P_{i}(T_{j}<\infty)\equiv f_{ij}>0,\]

where \(T_{j}=\min\{n:n\geq 1,X_{n}=j\}\) is the time of first visit to \(j\).

To generalize this to the case of general state spaces, one starts with the notion of _first entrance time or hitting time_ (also called the _first passage time_).

**Definition 14.2.5:** For any \(A\in\mathcal{S}\), the _first entrance time_ to \(A\) is defined as

\[T_{A}\equiv\left\{\begin{array}{l}\min\{n:n\geq 1,X_{n}\in A\}\\ \infty\quad\text{if}\quad X_{n}\not\in A\quad\text{for any}\quad n\geq 1.\end{array}\right.\]

Since the event \(\{T_{A}=1\}=\{X_{1}\in A\}\) and for \(k\geq 2\), \(\{T_{A}=k\}=\{X_{1}\not\in A,X_{2}\not\in A,\ldots,X_{k-1}\not\in A,X_{k}\in A\}\) is an element of \(\mathcal{F}_{k}=\sigma\langle\{X_{j}:j\leq k\}\rangle\), \(T_{A}\) is a _stopping time_ w.r.t. the filtration \(\{\mathcal{F}_{n}\}_{n\geq 1}\) (cf. Chapter 13).

**Definition 14.2.6:** Let \(\phi\) be a nonzero \(\sigma\)-finite measure on \((\mathbb{S},\mathcal{S})\). The Markov chain \(\{X_{n}\}_{n\geq 0}\) (or equivalently, its transition function \(P(\cdot,\cdot)\)) is said to be \(\phi\)-_irreducible_ (or _irreducible in the sense of Harris with reference measure_\(\phi\)) if for any \(A\) in \(\mathcal{S}\),

\[\phi(A)>0\Rightarrow L(x,A)\equiv P_{x}(T_{A}<\infty)>0 \tag{2.7}\]

for all \(x\) in \(\mathbb{S}\). This says that if a set \(A\) in \(\mathcal{S}\) is considered important by the measure \(\phi\) (i.e., \(\phi(A)>0\)), then so does the chain \(\{X_{n}\}_{n\geq 0}\) starting from any \(x\) in \(\mathbb{S}\). If \(G(x,A)\equiv\sum_{n=1}^{\infty}P^{n}(x,A)\) is the _Greens function_ associated with \(\boldsymbol{P}\), then (2.7) is equivalent to

\[\phi(A)>0\Rightarrow G(x,A)>0\quad\text{for all}\quad x\in\mathbb{S}, \tag{2.8}\]

i.e., \(\phi(\cdot)\) is dominated by \(G(x,\cdot)\) for all \(x\) in \(\mathbb{S}\).

#### 14.2.4 Examples

**Example 14.2.7:** If \(\mathbb{S}\) is countable and \(\phi\) is the counting measure on \(\mathbb{S}\), then the irreducibility of a Markov chain \(\{X_{n}\}_{n\geq 0}\) with state space \(\mathbb{S}\) is the same as \(\phi\)-irreducibility.

**Example 14.2.8:** If \(\{X_{n}\}_{n\geq 0}\) are iid with distribution \(\nu\), then it is \(\nu\)-irreducible.

**Example 14.2.9:** It can be verified that the random walk with jump distribution \(\nu\) (Example 14.2.2) is \(\phi\)-irreducible with the Lebesgue measure as \(\phi\) if \(\nu\) has a nonzero absolutely continuous component with a positive density on some open interval (Problem 14.19 (a)).

**Example 14.2.10:** The AR(1) with \(\eta_{1}\) having a nontrivial absolutely continuous component can be shown to be \(\phi\)-irreducible for some \(\phi\). On the other hand the AR(1) chain

\[X_{n+1}=\frac{X_{n}}{2}+\frac{1}{2}\eta_{n},\ n\geq 0,\]

where \(\{\eta_{n}\}_{n\geq 1}\) are iid Bernoulli \((\frac{1}{2})\) random variables, is not \(\phi\)-irreducible for any \(\phi\). In general, if \(\{X_{n}\}_{n\geq 0}\) is a Markov chain that has a discrete distribution for each \(n\) and has a limit distribution that is nonatomic, then \(\{X_{n}\}_{n\geq 0}\) cannot be \(\phi\)-irreducible for any \(\phi\) (Problem 14.19 (b)).

The waiting time chain (Example 14.2.6) is irreducible w.r.t. \(\phi\equiv\delta_{0}\), the delta measure at \(0\) if \(P(\eta_{1}<0)>0\) (Problem 14.20).

It can be shown that if \(\{X_{n}\}_{n\geq 0}\) is \(\phi\)-irreducible for some \(\sigma\)-finite \(\phi\), then there exists a probability measure \(\psi\) such that \(\{X_{n}\}_{n\geq 0}\) is \(\psi\)-irreducible and it is _maximal_ in the sense that if \(\{X_{n}\}_{n\geq 0}\) is \(\tilde{\phi}\)-irreducible for some \(\tilde{\phi}\), then \(\tilde{\phi}\) is dominated by \(\psi\). See Nummelin (1984).

#### 14.2.4 Harris recurrence

A Markov chain \(\{X_{n}\}_{n\geq 0}\) that is Harris irreducible with reference measure \(\phi\) is said to be _Harris recurrent_ if

\[A\in\mathcal{S},\ \phi(A)>0\Rightarrow P_{x}(T_{A}<\infty)=1\quad\text{for all} \quad x\quad\text{in}\quad\mathbb{S}. \tag{2.9}\]

Recall that irreducibility requires only that \(P_{x}(T_{A}<\infty)\) be \(>0\).

When \(\mathbb{S}\) is countable and \(\phi\) is the counting measure, this reduces to the usual notion of irreducibility and recurrence. If \(\mathbb{S}\) is not countable but hasa singleton \(\Delta\) such that \(P_{x}(T_{\Delta}<\infty)=1\) for all \(x\) in \(\mathbb{S}\), then the chain \(\{X_{n}\}_{n\geq 0}\) is Harris recurrent with respect to the measure \(\phi(\cdot)\equiv\delta_{\Delta}(\cdot)\), the delta measure at \(\Delta\). The waiting time chain (Example 14.2.6) has such a \(\Delta\) in \(0\) if \(E\eta_{1}<0\) (Problem 14.20). If such a recurrent singleton \(\Delta\) exists, then the sample paths of \(\{X_{n}\}_{n\geq 0}\) can be broken into iid excursions by looking at the chain between consecutive returns to \(\Delta\). This in turn will allow a complete extension of the basic limit theory from the countable case to this special case. In general, such a singleton may not exist. For example, for the AR(1) sequence with \(\{\eta_{i}\}_{i\geq 1}\) having a continuous distribution, \(P_{x}(X_{n}=x\) for some \(n\geq 1)=0\) for all \(x\). However, it turns out that _for Harris recurrent chains, such a singleton can be constructed via the regeneration theorem below_ established independently by Athreya and Ney (1978) and Nummelin (1978).

#### The minorization theorem

A remarkable result of the subject is that when \(\mathcal{S}\) is countably generated, a Harris recurrent chain can be embedded in a chain that has a recurrent singleton. This is achieved via the minorization theorem and the fundamental regeneration theorem below.

**Theorem 14.2.5:** (_The minorization theorem_). _Let \((\mathbb{S},\mathcal{S})\) be such that \(\mathcal{S}\) is countably generated. Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain with state space \((\mathbb{S},\mathcal{S})\) and transition function \(P(\cdot,\cdot)\) such that it is Harris irreducible with reference measure \(\phi(\cdot)\). Then the following minorization hypothesis holds._

* _(Hypothesis M)._ _For every_ \(B_{0}\in\mathcal{S}\) _such that_ \(\phi(B_{0})>0\)_, there exists a set_ \(A_{0}\subset B_{0}\)_, an integer_ \(n_{0}\geq 1\)_, a constant_ \(0<\alpha<1\)_, and a probability measure_ \(\nu\) _on_ \((\mathbb{S},\mathcal{S})\) _such that_ \(\phi(A_{0})>0\) _and for all_ \(x\) _in_ \(A_{0}\)_,_ \[P^{n_{0}}(x,A)\geq\alpha\nu(A)\quad\text{for all}\quad A\in\mathcal{S}.\]
* _(The_ \(\boldsymbol{C}\)_-set lemma)._ _For any set_ \(B_{0}\in\mathcal{S}\) _such that_ \(\phi(B_{0})>0\)_, there exists a set_ \(A_{0}\subset B_{0}\)_, an_ \(n_{0}\geq 1\)_, a constant_ \(0<\alpha^{\prime}<1\) _such that for_ \(x,y\) _in_ \(A_{0}\)_,_ \[p^{n_{0}}(x,y)\geq\alpha^{\prime},\] _where_ \(p^{n_{0}}(x,\cdot)\) _is the Radon-Nikodym derivative of the absolutely continuous component of_ \[P^{n_{0}}(x,\cdot)\quad\text{w.r.t.}\quad\phi(\cdot).\]

The proof of the \(C\)-set lemma is a nice application of the martingale convergence theorem (see Orey (1971)). The proof of Theorem 14.2.5 (i) using the \(C\)-set lemma is easy and is left as an exercise (Problem 14.21).

[MISSING_PAGE_FAIL:474]

it follows that for any initial distribution \(\mu\), there is a random time \(T\) such that \(X_{T}\) is distributed as \(\nu(\cdot)\) and is independent of all history up to \(T-1\). More precisely, for any \(\mu\), \(P_{\mu}(T<\infty)=1\) and

\[P_{\mu}\left(X_{j}\in A_{j},j=0,1,2,\ldots,n+k,T=n+1\right)\] \[= P_{\mu}\left(X_{j}\in A_{j},j=0,1,2,\ldots,n,T=n+1\right)\times\] \[P_{\nu}\left(X_{0}\in A_{n+1},X_{1}\in A_{n+2},\ldots,X_{k-1}\in A _{n+k}\right).\]

Since this is true for any \(\mu\), it is true for \(\mu=\nu\) and hence the theorem follows. \(\Box\)

A consequence of the above theorem is following.

**Theorem 14.2.7:**_Suppose in Theorem 14.2.6, instead of (2.10) and (2.11), the following holds._

_There exists an \(n_{0}\geq 1\) such that for all \(x\) in \(A_{0}\), \(A\) in \({\cal S}\),_

\[P^{n_{0}}(x,A)\geq\alpha\nu(A) \tag{2.14}\]

_and for all \(x\) in \(A_{0}\),_

\[P_{x}(X_{nn_{0}}\in A_{0}\quad\mbox{for some}\quad n\geq 1)=1 \tag{2.15}\]

_and_

\[P_{x}(T_{A_{0}}<\infty)=1\quad\mbox{for all}\quad x\ \ \mbox{in}\ \ {\mathbb{S}}. \tag{2.16}\]

_Let \(Y_{n}\equiv X_{nn_{0}}\), \(n\geq 0\) (where \(nn_{0}\) stands for the product of \(n\) and \(n_{0}\)). Then, for any initial distribution \(\mu\), there exist random times \(\{T_{i}\}_{i\geq 1}\) such that under \(P_{\mu}\), the sequence \(\eta_{j}\equiv\{Y_{j}:T_{j+r},0\leq r<T_{j+1}-T_{j},T_{j+1}-T_{j}\}\) for \(j=1,2,\ldots\) are iid with \(Y_{T_{j}}\sim\nu(\cdot)\)._

**Proof:** For any initial distribution \(\mu\) such that \(\mu(A_{0})=1\), the theorem follows from Theorem 14.2.6 since (2.14) and (2.15) are the same as (2.10) and (2.11) for the transition function \(P^{n_{0}}(\cdot,\cdot)\) and the chain \(\{Y_{n}\}_{n\geq 0}\). By (2.16) for any other \(\mu\), \(P_{\mu}(T_{A_{0}}<\infty)=1\). \(\Box\)

Given a realization of the Markov chain \(\{Y_{n}\equiv X_{nn_{0}}\}_{n\geq 0}\), it is possible to construct a realization of the full Markov chain \(\{X_{n}\}_{n\geq 0}\) by "filling the gaps" \(\{X_{j}:kn_{0}+1\leq j\leq(k+1)n_{0}-1\}\) as follows: Given \(X_{kn_{0}}=x\), \(X_{(k+1)n_{0}}=y\), generate an observation from the conditional distribution of \((X_{1},X_{2},\ldots,X_{n_{0}-1})\), given \(X_{0}=x\), \(X_{n_{0}}=y\). This leads to the following.

**Theorem 14.2.8:**_Under the set up of Theorem 14.2.7, the "excursions"_

\[\tilde{\eta}_{j}\equiv\left\{X_{n_{0}T_{j}+k}:0\leq k<n_{0}(T_{j+1}-T_{j}),T_{ j+1}-T_{j}\right\}_{j=1}^{\infty},\]

_are identically distributed and are one dependent, i.e., for each \(r\geq 1\), the collections \(\{\tilde{\eta}_{1},\tilde{\eta}_{2},\ldots,\tilde{\eta}_{r}\}\) and \(\{\tilde{\eta}_{r+2},\tilde{\eta}_{r+3},\ldots\}\) are independent._

**Proof:** Note that in applying the regeneration method to the sequence \(\{Y_{n}\}_{n\geq 0}\) and then doing the "filling the gaps" lead to the common portion

\[X_{(T_{j}-1)n_{0}+r}\ 0\leq r\leq n_{0}\]

with given the values \(X_{(T_{j}-1)n_{0}}\) and \(X_{T_{j}n_{0}}\). This makes two successive \(\tilde{\eta}_{j-1}\) and \(\tilde{\eta}_{j}\) dependent. But Markov property renders \(\tilde{\eta}_{j}\) and \(\tilde{\eta}_{j+2}\) independent. This yields the one-dependence of \(\{\tilde{\eta}_{j}\}_{j\geq 1}\). \(\Box\)

By the \(C\)-set lemma and the minorization Theorem 14.2.5, \(\phi\)-recurrence yields the hypothesis of Theorem 14.2.7.

**Theorem 14.2.9:**_Let \(\{X_{n}\}_{n\geq 0}\) be a \(\phi\)-recurrent Markov chain with state space \((\mathbb{S},\mathcal{S})\), where \(\mathcal{S}\) is countably generated. Then there exist an \(A_{0}\) in \(\mathcal{S}\), \(n_{0}\geq 1\), \(0<\alpha<1\) and a probability measure \(\nu\) such that (2.14), (2.15), and (2.16) hold and hence, the conclusions of Theorem 14.2.8 hold._

Thus, \(\phi\)-recurrence implies that the Markov chain \(\{X_{n}\}_{n\geq 0}\) is regenerative (defined fully below). This makes the law of large numbers for iid random variables available to such chains. The limit theory of regenerative sequences developed in Section 8.5 is reviewed below and by the above results, such a theory will hold for \(\phi\)-recurrent chains.

#### Limit theory for regenerative sequences

**Definition 14.2.7:** Let \((\Omega,\mathcal{F},P)\) be a probability space and \((\mathbb{S},\mathcal{S})\) be a measurable space. A sequence of random variables \(\{X_{n}\}_{n\geq 0}\) defined on \((\Omega,\mathcal{F},P)\) with values in \((\mathbb{S},\mathcal{S})\) is called _regenerative_ if there exists a sequence of random times \(0<T_{1}<T_{2}<T_{3}<\cdots\) such that the excursions \(\eta_{j}\equiv\{X_{n}:T_{j}\leq n<T_{j+1},T_{j+1}-T_{j}\}_{j\geq 1}\) are iid, i.e.,

\[P\bigl{(}T_{j+1}-T_{j}=k_{j},X_{T_{j}+\ell}\in A_{\ell,j},0\leq \ell<k_{j},j=1,2,\ldots,r\bigr{)}\] \[= \prod_{j=1}^{r}P\bigl{(}T_{2}-T_{1}=k_{j},X_{T_{1}+\ell}\in A_{ \ell,j},0\leq\ell<k_{j}\bigr{)} \tag{2.17}\]

for all \(k_{1},k_{2},\ldots,k_{r}\in\mathbb{N}\) and \(A_{\ell,j}\in\mathcal{S}\), \(1\leq\ell\leq k_{j}\), \(j=1,\ldots,r\).

**Example 14.2.11:** Any Markov chain \(\{X_{n}\}_{n\geq 0}\) with a countable state space \(\mathbb{S}\) that is irreducible and recurrent is regenerative with \(\{T_{i}\}_{i\geq 1}\) being the times of successive returns to a given state \(\Delta\).

**Example 14.2.12:** Any Harris recurrent chain satisfying the minorization condition (2.10) is regenerative by Theorem 14.2.6.

**Example 14.2.13:** The waiting time chain (Example 14.2.6) with \(E\eta_{1}<0\) is regenerative with \(\{T_{i}\}_{i\geq 1}\) being the times of successive returns of \(\{X_{n}\}_{n\geq 0}\) to zero.

**Example 14.2.14:** An example of a non-Markov sequence \(\{X_{n}\}_{n\geq 0}\) that is regenerative is a _semi-Markov chain_. Let \(\{y_{n}\}_{n\geq 0}\) be a Harris recurrent Markov chain satisfying (2.10). Given \(\{y_{n}=a_{n}\}_{n\geq 0}\), let \(\{L_{n}\}_{n\geq 0}\) be independent positive integer valued random variables. Set

\[X_{j}=\left\{\begin{array}{ll}y_{0}&0\leq j<L_{0}\\ y_{1}&L_{0}\leq j<L_{0}+L_{1}\\ y_{2}&L_{0}+L_{1}\leq j<L_{0}+L_{1}+L_{2}\\ \vdots\end{array}\right.\]

Then \(\{X_{n}\}_{n\geq 0}\) is called a _semi-Markov chain_ with embedded Markov chain \(\{y_{n}\}_{n\geq 0}\) and sojourn times \(\{L_{n}\}_{n\geq 0}\). It is regenerative if \(\{T_{i}\}_{i\geq 1}\) are defined by \(T_{i}=\sum_{j=0}^{N_{i}-1}L_{j}\), where \(\{N_{i}\}_{i\geq 1}\) are the successive regeneration times for \(\{y_{n}\}\) as in Theorem 14.2.7.

**Theorem 14.2.10:** _Let \(\{X_{n}\}_{n\geq 0}\) be a regenerative sequence with regeneration times \(\{T_{i}\}_{i\geq 1}\). Let \(\tilde{\pi}(A)\equiv E\big{(}\sum_{j=T_{1}}^{T_{2}-1}I_{A}(X_{j})\big{)}\) for \(A\in{\cal S}\). Suppose \(\tilde{\pi}({\mathbb{S}})\equiv E(T_{2}-T_{1})<\infty\). Let \(\pi(\cdot)=\tilde{\pi}(\cdot)\big{/}\tilde{\pi}({\mathbb{S}})\). Then_

* \(\frac{1}{n}\sum_{j=0}^{n}f(X_{j})\to\int fd\pi\) _w.p. 1 for any_ \(f\in L^{1}({\mathbb{S}},{\cal S},\pi)\)_._
* \(\mu_{n}(\cdot)\equiv\frac{1}{n}\sum_{j=0}^{n}P(X_{j}\in\cdot) \to\pi(\cdot)\) _in total variation._
* _If the distribution of_ \(T_{2}-T_{1}\) _is aperiodic, then_ \(P(X_{n}\in\cdot)\to\pi(\cdot)\) _in total variation._

**Proof:** To prove (i) it suffices to consider nonnegative \(f\). For each \(n\), let \(N_{n}=k\) if \(T_{k}\leq n<T_{k+1}\). Let

\[Y_{i}=\sum_{j=T_{i}}^{T_{i+1}-1}f(X_{j}),\ i\geq 1\quad\mbox{and}\quad Y_{0}= \sum_{j=0}^{T_{1}-1}f(X_{j}).\]

Then

\[Y_{0}+\sum_{i=1}^{N_{n}-1}Y_{i}\leq\sum_{i=0}^{n}f(X_{i})\leq Y_{0}+\sum_{i=1} ^{N_{n}}Y_{i}. \tag{2.18}\]

By the SLLN, \(\frac{1}{N_{n}}\sum_{i=1}^{N_{n}-1}Y_{i}\) and \(\frac{1}{N_{n}}\sum_{i=1}^{N_{n}}Y_{i}\) converge to \(EY_{1}\) w.p. 1 and \(\frac{N_{n}}{n}\to\big{(}E(T_{2}-T_{1})\big{)}^{-1}\). It follows from (2.18) that

\[\lim_{n\to\infty}\frac{1}{n}\sum_{i=0}^{n}f(X_{i})=\frac{EY_{1}}{E(T_{2}-T_{1} )}=\frac{\int fd\tilde{\pi}}{\tilde{\pi}({\mathbb{S}})}=\int fd\pi,\]establishing (i).

By taking \(f=I_{A}\) and using the BCT, one concludes from (i) that \(\mu_{n}(A)\to\pi(A)\) for every \(A\) in \(\mathcal{S}.\) Since \(\mu_{n}\) and \(\pi\) are probability measures, this implies that \(\mu_{n}\to\pi\) in total variation, proving (ii).

To prove (iii), note that for any bounded measurable \(f,\)\(a_{n}\equiv Ef(X_{T_{1}+n})\) satisfies

\[a_{n} = E\bigl{(}f(X_{T_{1}+n})I(T_{2}-T_{1}>n)\bigr{)}+\sum_{r=1}^{n}E \bigl{(}f(X_{T_{1}+n})I(T_{2}-T_{1}=r)\bigr{)}\] \[= b_{n}+\sum_{r=1}^{n}E\bigl{(}f(X_{T_{2}+n-r})\bigr{)}P(T_{2}-T_{ 1}=r)\] \[= b_{n}+\sum_{r=1}^{n}a_{n-r}p_{r},\]

where \(p_{r}=P(T_{2}-T_{1}=r).\) Now by the discrete renewal theorems from Section 8.5 (which applies since \(ET_{2}-T_{1}<\infty\) and \(T_{2}-T_{1}\) has an aperiodic distribution), (iii) follows. \(\Box\)

**Remark 14.2.1:** Since the strong law is valid for any \(m\)-dependent (\(m<\infty\)) and stationary sequence of random variables, Theorem 14.2.10 is valid even if the excursions \(\{\eta_{j}\}_{j\geq 1}\) are \(m\)-dependent.

#### Limit theory of Harris recurrent Markov chains

The minorization theorem, the fundamental regeneration theorem, and the limit theorem for regenerative sequences, i.e., Theorems 14.2.5, 14.2.6, 14.2.7, and 14.2.10, are the essential components of a limit theory for Harris recurrent Markov chains that parallels the limit theory for discrete state space irreducible recurrent Markov chains.

**Definition 14.2.8:** A probability measure \(\pi\) on \((\mathbb{S},\mathcal{S})\) is called _stationary_ for a transition function \(P(\cdot,\cdot)\) if

\[\pi(A)=\int_{\mathbb{S}}P(x,A)\pi(dx)\quad\text{for all}\quad A\in\mathcal{S}.\]

Note that if \(X_{0}\sim\pi,\) then \(X_{n}\sim\pi\) for all \(n\geq 1,\) justifying the term "stationary."

**Theorem 14.2.11:**_Let \(\{X_{n}\}_{n\geq 0}\) be a Harris recurrent Markov chain with state space \((\mathbb{S},\mathcal{S})\) and transition function \(P(\cdot,\cdot).\) Let \(\mathcal{S}\) be countably generated. Suppose there exists a stationary probability measure \(\pi.\) Then,_

1. \(\pi\) _is unique.__(The law of large numbers). For all \(f\in L^{1}(\mathbb{S},\mathcal{S},\pi)\), for all \(x\in\mathbb{S}\),_

\[\frac{1}{n}\sum_{j=0}^{n-1}f(X_{j})\to\int fd\pi\mbox{ w.p. }1\mbox{ }(P_{x}).\]

_(Convergence of \(n\)-step probabilities). For all \(x\in\mathbb{S}\)_

\[\mu_{n,x}(\cdot)\equiv\frac{1}{n}\sum_{j=0}^{n-1}P_{x}(X_{j}\in\cdot)\to\pi( \cdot)\quad\mbox{in total variation}.\]

**Proof:** By Harris recurrence and the minorization Theorem 14.2.5, there exist a set \(A_{0}\in\mathcal{S}\), a constant \(0<\alpha<1\), an integer \(n_{0}\geq 1\), a probability measure \(\nu\) such that

\[\mbox{for all }x\in A_{0},\ A\in\mathcal{S},\ P^{n_{0}}(x,A)\geq\alpha\nu(A) \tag{2.19}\]

and

\[\mbox{for all }x\mbox{ in }\mathbb{S},\ P_{x}(T_{A_{0}}<\infty)=1. \tag{2.20}\]

For simplicity of exposition, assume that \(n_{0}=1\). (The general case \(n_{0}>1\) can be reduced to this by considering the transition function \(P^{n_{0}}\).)

Let the sequence \(\{\eta_{n},\delta_{n},Y_{n,x}\}_{n\geq 1}\) and the regeneration times \(\{T_{i}\}_{i\geq 1}\) be as in Theorem 14.2.6. Recall that the first regeneration time \(T_{1}\) can be defined as

\[T_{1}=\min\{n:n>0,\ X_{n-1}\in A_{0},\ \delta_{n}=1\} \tag{2.21}\]

and the succeeding ones by

\[T_{i+1}=\min\{n:n>T_{i},\ X_{n-1}\in A_{0},\ \delta_{n}=1\}, \tag{2.22}\]

and that \(X_{T_{i}}\) are distributed as \(\nu\) independent of the past. Let for \(n\geq 1\), \(N_{n}=k\) if \(T_{k}\leq n<T_{k+1}\). By Harris recurrence, for all \(x\) in \(\mathbb{S}\), \(N_{n}\to\infty\) w.p. \(1\) (\(P_{x}\)) and by the SLLN, for all \(x\) in \(\mathbb{S}\),

\[\frac{N_{n}}{n}\to\frac{1}{E_{\nu}T_{1}}\ \ \mbox{ w.p. }1\mbox{ }(P_{x}).\]

and hence, by the BCT,

\[E_{x}\frac{N_{n}}{n}\to\frac{1}{E_{\nu}T_{1}}.\]

On the other hand, for any \(k\geq 1\), \(x\in\mathbb{S}\),

\[P_{x}(\mbox{a regeneration occurs at }k) = P_{x}(X_{k-1}\in A_{0},\ \delta_{k}=1)\] \[= P_{x}(X_{k-1}\in A_{0})\alpha.\]

Thus

\[E_{x}\frac{N_{n}}{n}=\frac{1}{n}\sum_{k=1}^{n}P_{x}(X_{k-1}\in A_{0})\alpha\]and hence, for all \(x\) in \(\mathbb{S}\),

\[\frac{1}{n}\sum_{j=0}^{n-1}P_{x}(X_{j}\in A_{0})\to\frac{1}{\alpha E_{\nu}T_{1}}.\]

Now let \(\pi\) be a stationary measure for \(P(\cdot,\cdot)\). Then

\[\pi(A_{0})=\int P_{x}(X_{j}\in A_{0})\pi(dx)\quad\text{for all}\quad j=0,1,2,\ldots\]

and hence

\[n\pi(A_{0})=\int\sum_{j=0}^{n-1}P_{x}(X_{j}\in A_{0})\pi(dx). \tag{2.23}\]

Since \(G(x,A_{0})\equiv\sum_{j=0}^{\infty}P_{x}(X_{j}\in A_{0})>0\) for all \(x\) in \(\mathbb{S}\), by Harris recurrence (Harris irreducibility will do for this), it follows that \(\pi(A_{0})>0\). Dividing both sides of (2.23) by \(n\) and letting \(n\to\infty\) yield

\[\pi(A_{0})=\frac{1}{\alpha E_{\nu}T_{1}}\]

and hence that \(E_{\nu}T_{1}<\infty\). Since \(E_{\nu}T_{1}\equiv E(T_{2}-T_{1})<\infty\), by Theorem 14.2.10, for all \(x\) in \(\mathbb{S}\), \(A\in\mathcal{S}\),

\[\frac{1}{n}\sum_{j=0}^{n-1}P_{x}(X_{j}\in A)\to\frac{E_{\nu}\big{(}\sum_{j=0}^{ T-1}I_{A}(X_{j})\big{)}}{E_{\nu}(T_{1})}.\]

Integrating the left side with respect to \(\pi\) yields that for any \(A\in\mathcal{S}\),

\[\pi(A)=\frac{E_{\nu}\big{(}\sum_{j=0}^{T-1}I_{A}(X_{j})\big{)}}{E_{\nu}(T_{1}) }\,\]

thus establishing the uniqueness of \(\pi\), i.e., establishing (i) of Theorem 14.2.11. The other two parts follow from the regeneration Theorem 14.2.6 and the limit Theorem 14.2.10. \(\Box\)

**Remark 14.2.2:** Under the assumption \(n_{0}=1\) that was made at the beginning of the proof, it also follows that

\[P_{x}(X_{j}\in\cdot)\to\pi(\cdot) \tag{2.24}\]

in total variation.

This also holds if the g.c.d. of the \(n_{0}\)'s for which there exist \(A_{0}\), \(\alpha\), \(\nu\) satisfying (2.19) is one.

**Remark 14.2.3:** A necessary and sufficient condition for the existence of a stationary distribution for a Harris recurrent chain is that there exists a set \(\{A_{0},\alpha,\nu,n_{0}\}\) satisfying (2.19) and (2.20) and

\[E_{\nu}T_{A_{0}}<\infty.\]A more general result than Theorem 14.2.11 is the following that was motivated by applications to Markov chain Monte Carlo methods.

Theorem 14.2.12: Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain with state space \((\mathbb{S},\mathcal{S})\) and transition function \(P(\cdot,\cdot)\). Suppose (2.19) holds for some \((A_{0},\alpha,\nu,n_{0})\). Suppose \(\pi\) is a stationary probability measure for \(P(\cdot,\cdot)\) such that

\[\pi\big{(}\{x:P_{x}(T_{A_{0}}<\infty)>0\}\big{)}=1.\]

Then, for \(\pi\)-almost all \(x\),

* \(P_{x}(T_{A_{0}}<\infty)=1\).
* \(\mu_{n,x}(\cdot)=\frac{1}{n}\sum_{j=0}^{n-1}P_{x}(X_{j}\in\cdot)\to\pi(\cdot)\) in total variation.
* For any \(f\in L^{1}(\mathbb{S},\mathcal{S},\pi)\), \[\frac{1}{n}\sum_{j=0}^{n}f(X_{j})\to\int fd\pi\ \ \text{w.p. }1\ \ (P_{x}).\]
* \(\frac{1}{n}\sum_{j=0}^{n}E_{x}f(X_{j})\to\int fd\pi\).

If, further the g.c.d. \(\{m\): there exists \(\alpha_{m}>0\) such that for all \(x\) in \(A_{0}\), \(P^{m}(x,\cdot)\geq\alpha\nu(\cdot)\}=1\), then (ii) can be strengthened to

\[P_{x}(X_{n}\in\cdot)\to\pi(\cdot)\ \ \text{in total variation}.\]

The key difference between Theorems 14.2.11 and 14.2.12 is that the latter does not require Harris recurrence which is often difficult to verify. On the other hand, the conclusions of Theorem 14.2.12 are valid only for \(\pi\)-almost all \(x\) unlike for all \(x\) in Theorem 14.2.11. In MCMC applications, the existence of a stationary measure is given (as it is the 'target distribution') and the minorization condition is more easy to verify as is the milder form of irreducibility condition (2.25). (Harris irreducibility will require \(P_{x}(T_{A_{0}}<\infty)>0\) for all \(x\) in \(\mathbb{S}\).) For a proof of Theorem 14.2.12 and applications to MCMC, see Athreya, Doss and Sethuraman (1996).

Example 14.2.15: (AR(1) time series) (Example 14.2.4). Suppose \(\eta_{1}\) has an absolutely continuous component and that \(|\rho|<1\). Then the chain is Harris recurrent and admits a stationary probability distribution \(\pi(\cdot)\) of \(\sum_{j=0}^{\infty}\rho^{j}\eta_{j}\) and the \(P_{x}(X_{n}\in\cdot)\to\pi(\cdot)\) in total variation for any \(x\).

Example 14.2.16: (Waiting time chain) (Example 14.2.6). If \(E\eta_{1}<0\), the state \(0\) is recurrent and hence the Markov chain is Harris recurrent. Also a stationary distribution \(\pi\) does exist. It is known that \(\pi\) is the same as the distribution of \(M_{\infty}\equiv\sup_{j\geq 0}S_{j}\), where \(S_{0}=0\), \(S_{j}=\sum_{i=1}^{j}\eta_{i}\), \(j\geq 1\), \(\{\eta_{i}\}_{i\geq 1}\) being iid.

#### 14.2.9 Markov chains on metric spaces

##### 14.2.9.1 Feller continuity

Let \((\mathbb{S},d)\) be a metric space and \(\mathcal{S}\) be the Borel \(\sigma\)-algebra in \(\mathbb{S}\). Let \(P(\cdot,\cdot)\) be a transition function. Let \(\{X_{n}\}_{n\geq 0}\) be Markov chain with state space \((\mathbb{S},\mathcal{S})\) and transition function \(P(\cdot,\cdot)\).

**Definition 14.2.9:** The transition function \(P(\cdot,\cdot)\) is called _Feller continuous_ (or simply _Feller_) if \(x_{n}\to x\) in \(\mathbb{S}\Rightarrow P(x_{n},\cdot)\longrightarrow^{d}P(x,\cdot)\) i.e. \((Pf)(x_{n})\equiv\int f(y)P(x_{n},dy)\rightarrow(Pf)(x)\equiv\int f(y)P(x,dy)\) for all bounded continuous \(f:\mathbb{S}\rightarrow\mathbb{R}\). In terms of the Markov chain, this says

\[E\big{(}f(X_{1})\mid X_{0}=x_{n}\big{)}\to E\big{(}f(X_{1})\mid X_{0}=x \big{)}\quad\text{if}\quad x_{n}\to x.\]

**Example 14.2.17:** Let \((\Omega,\mathcal{F},P)\) be a probability space and \(h:\mathbb{S}\times\Omega\rightarrow\mathbb{S}\) be jointly measurable and \(h(\cdot,\omega)\) be continuous w.p. 1. Let \(P(x,A)\equiv P(h(x,\omega)\in A)\) for \(x\in\mathbb{S}\), \(A\in\mathcal{S}\). Then \(P(\cdot,\cdot)\) is a Feller continuous transition function. Indeed, for any bounded continuous \(f:\mathbb{S}\rightarrow\mathbb{R}\)

\[(Pf)(x)\equiv\int f(y)P(x,dy)=Ef\big{(}h(x,\omega)\big{)}.\]

Now, \(x_{n}\to x\)

\[\Rightarrow h(x_{n},\omega)\to h(x,\omega)\text{ w.p. 1 }\] \[\Rightarrow f\big{(}h(x_{n},\omega)\big{)}\to f\big{(}h(x,\omega) \big{)}\text{ w.p. 1 (by continuity of }f\big{)}\] \[\Rightarrow Ef\big{(}h(x_{n},\omega)\big{)}\to Efh(x,\omega)\text{ (by bounded convergence theorem).}\]

The first five examples of Section 14.2.4 fall in this category. If \(h\) is discontinuous, then \(P(\cdot,\cdot)\) need not be Feller (Problem 14.22). That \(P(\cdot,\cdot)\) is a transition function requires only that \(h(\cdot,\cdot)\) be jointly measurable (Problem 14.23).

##### 14.2.9.2 Stationary measures

A general method of finding a stationary measure for a Feller transition function \(P(\cdot,\cdot)\) is to consider weak or vague limits of the occupation measures \(\mu_{n,\lambda}(A)\equiv\frac{1}{n}\sum_{j=0}^{n-1}P_{\lambda}(X_{j}\in A)\), where \(\lambda\) is the initial distribution.

**Theorem 14.2.13:** _Fix an initial distribution \(\lambda\). Suppose a probability measure \(\mu\) is a weak limit point of \(\{\mu_{n,\lambda}\}_{n\geq 1}\). That is, for some \(n_{1}<n_{2}<n_{3}<\cdots\), \(\mu_{n_{k},\lambda}\longrightarrow^{d}\mu\). Assume \(P(\cdot,\cdot)\) is Feller. Then \(\mu\) is a stationary probability measure for \(P(\cdot,\cdot)\)._

**Proof:** Let \(f:\mathbb{S}\rightarrow\mathbb{R}\) be continuous and bounded. Then

\[\int f(y)\mu_{n_{k},\lambda}(dy)\rightarrow\int f(y)\mu(dy).\]But the left side equals

\[\frac{1}{n_{k}}\sum_{j=0}^{n_{k}-1}E_{\lambda}f(X_{j})\] \[= \frac{1}{n_{k}}E_{\lambda}f(X_{0})+\frac{1}{n_{k}}\sum_{j=1}^{n_{k} -1}E_{\lambda}f(X_{j})\] \[= \frac{1}{n_{k}}E_{\lambda}f(X_{0})+\frac{1}{n_{k}}\sum_{j=1}^{n_{k }-1}E_{\lambda}(Pf)(X_{j-1})\] \[\big{(}\mbox{since by Markov property, for }j\geq 1,\,E_{\lambda}f(X_{j})=E_{\lambda}( Pf)(X_{j-1})\big{)}\] \[= \frac{1}{n_{k}}E_{\lambda}f(X_{0})+\frac{1}{n_{k}}\sum_{j=0}^{n_{ k}-1}E_{\lambda}(Pf)(X_{j})-\frac{1}{n_{k}}E_{\lambda}(Pf)(X_{n_{k}-1}).\]

The first and third term on the right side go to zero since \(f\) is bounded and \(n_{k}\to\infty\). The second term goes to \(\int(Pf)(y)\mu(dy)\) since by the Feller property \(Pf\) is a bounded continuous function. Thus,

\[\int_{\mathbb{S}}f(y)\mu(dy) = \int_{\mathbb{S}}(Pf)(y)\mu(dy)\] \[= \int_{\mathbb{S}}\Big{(}\int_{\mathbb{S}}f(z)P(y,dz)\Big{)}\mu(dy)\] \[= \int_{\mathbb{S}}f(z)(\mu P)(dz)\]

where

\[\mu P(A)\equiv\int_{\mathbb{S}}P(y,A)\mu(dy).\]

This being true for all bounded continuous \(f\), it follows that \(\mu=\mu P\), i.e., \(\mu\) is stationary for \(P(\cdot,\cdot)\). \(\Box\)

A more general result is the following.

**Theorem 14.2.14:**_Let \(\lambda\) be an initial distribution. Let \(\mu\) be a subprobability measure (i.e., \(\mu(\mathbb{S})\leq 1\)) such that for some \(n_{1}<n_{2}<n_{3}<\cdots\), \(\{\mu_{n_{k},\lambda}\}\) converges vaguely to \(\mu\), i.e., \(\int fd\mu_{n_{k},\lambda}\to\int fd\mu\) for all \(f:\mathbb{S}\to\mathbb{R}\) continuous with compact support. Suppose there exists an approximate identity \(\{g_{n}\}_{n\geq 1}\) for \(\mathbb{S}\), i.e., for all \(n\), \(g_{n}\) is a continuous function from \(\mathbb{S}\) to [0,1] with compact support and for every \(x\) in \(\mathbb{S}\), \(g_{n}(x)\uparrow 1\) as \(n\to\infty\). Then \(\mu\) is stationary for \(P(\cdot,\cdot)\), i.e.,_

\[\mu(A)=\int_{\mathbb{S}}P(x,A)\mu(dx)\quad\mbox{for all}\quad A\in\mathcal{S}.\]For a proof, see Athreya (2004).

If \(\mathbb{S}=\mathbb{R}^{k}\) for some \(k<\infty\), \(\mathbb{S}\) admits an approximate identity. Conditions to ensure that there is a vague limit point \(\mu\) such that \(\mu(\mathbb{S})>0\) is provided by the following.

Theorem 14.2.15: Suppose there exists a set \(A_{0}\in\mathcal{S}\), a function \(V:\mathbb{S}\to[0,\infty)\) and numbers \(0<\alpha\), \(M<\infty\) such that

\[(PV)(x)\equiv E_{x}V(X_{1})\leq V(x)-\alpha\quad\mbox{for}\quad x\notin A_{0} \tag{2.26}\]

and

\[\sup_{x\in A_{0}}\big{(}E_{x}V(X_{1})-V(x)\big{)}\equiv M<\infty. \tag{2.27}\]

Then, for any initial distribution \(\lambda\),

\[\varliminf_{n\to\infty}\mu_{n,\lambda}(A_{0})\geq\frac{\alpha}{\alpha+M}. \tag{2.28}\]

**Proof:** For \(j\geq 1\),

\[E_{\lambda}V(X_{j})-E_{\lambda}V(X_{j-1}) = E_{\lambda}\big{(}PV(X_{j-1})-V(X_{j-1})\big{)}\] \[= E_{\lambda}\Big{(}\big{(}PV(X_{j-1})-V(X_{j-1})\big{)}I_{A_{0}} (X_{j-1})\Big{)}\] \[\quad+E_{\lambda}\Big{(}\big{(}PV(X_{j-1})-V(X_{j-1})\big{)}I_{A _{0}^{c}}(X_{j-1})\Big{)}\] \[\leq MP_{\lambda}(X_{j-1}\in A_{0})-\alpha P_{\lambda}(X_{j-1}\notin A _{0}).\]

Adding over \(j=1,2,\ldots,n\) yields

\[\frac{1}{n}\big{(}E_{\lambda}V(X_{n})-V(x)\big{)}\leq-\alpha+(\alpha+M)\mu_{n, \lambda}(A_{0}).\]

Since \(V(\cdot)\geq 0\), letting \(n\to\infty\) yields (2.28). \(\Box\)

Definition 14.2.10:A metric space \((\mathbb{S},d)\) has the _vague compactness property_ if given any collection \(\{\mu_{\alpha}:\alpha\in I\}\) of subprobability measures, there is a sequence \(\{\alpha_{j}\}\subset I\) such that \(\mu_{\alpha_{j}}\) converges vaguely to a subprobability measure \(\mu\). It is known by the Helly's selection theorem that all Euclidean spaces have this property. It is also known that any Polish space, i.e., a complete, separable, metric space has this property (see Billingsley (1968)).

Combining the above two results yields the following:

Theorem 14.2.16: Let \(P(\cdot,\cdot)\) be a Feller transition function on a metric space \((\mathbb{S},d)\) that admits an approximate identity and has the vague compactness property. Suppose there exists a closed set \(A_{0}\), a function \(V:\mathbb{S}\to[0,\infty)\), numbers \(0<\alpha\), \(M<\infty\) such that (2.26) and (2.27) hold. Then there exists a stationary probability measure \(\mu\) for \(P(\cdot,\cdot)\)._

**Proof:** Fix an initial distribution \(\lambda\). Then the family \(\{\mu_{n,\lambda}\}_{n\geq 1}\) has a subsequence \(\{\mu_{n_{k},\lambda}\}_{k\geq 1}\) and a subprobability measure \(\mu\) such that \(\mu_{n_{k},\lambda}\to\mu\) vaguely. Since \(A_{0}\) is closed, this implies

\[\varlimsup_{k\to\infty}\mu_{n_{k},\lambda}(A_{0})\leq\mu(A_{0}).\]

Thus \(\mu(A_{0})\geq\varlimsup\mu_{n_{k},\lambda}(A_{0})\geq\varliminf\mu_{n_{k}, \lambda}(A_{0})\geq\frac{\alpha}{\alpha+M}\), by Theorem 14.2.15. This yields that \(\mu(\mathbb{S})>0\). By Theorem 14.2.14, \(\mu\) is stationary for \(P\). So \(\tilde{\mu}(\cdot)\equiv\frac{\mu(\cdot)}{\mu(\mathbb{S})}\) is a probability measure that is stationary for \(P\). \(\Box\)

**Example 14.2.18:** Consider a Markov chain generated by the iteration of iid random logistic maps

\[X_{n+1}=C_{n+1}X_{n}(1-X_{n}),\ n\geq 0\]

with \(\lx@sectionsign=[0,1]\), \(\{C_{n}\}_{n\geq 1}\) iid with values in [0,4] and \(X_{0}\) is independent of \(\{C_{n}\}_{n\geq 1}\). Assume \(E\log C_{1}>0\) and \(E|\log(4-C_{1})|<\infty\). Then there exists a stationary probability measure \(\pi\) such that \(\pi\big{(}(0,1)\big{)}=1\). This follows from Theorem 14.2.16 by showing that if \(V(x)=|\log x|\), then there exists \(A_{0}=[a,b]\subset(0,1)\) and constants \(0<\alpha\), \(M<\infty\) such that (2.26) and (2.27) hold. For details, see Athreya (2004).

#### 14.2.9.3 Convergence questions

If \(\{X_{n}\}_{n\geq 0}\) is a Feller Markov chain (i.e., its transition function \(P(\cdot,\cdot)\) is Feller continuous), what can one say about the convergence of the distribution of \(X_{n}\) as \(n\to\infty\)?

If \(P(\cdot,\cdot)\) admits a unique stationary probability measure \(\pi\) and the family \(\{\mu_{n,\lambda}:n\geq 1\}\) is tight for a given \(\lambda\), then one can conclude from Theorem 14.2.13 that every weak limit point of this family has to be \(\pi\) and hence \(\pi\) is the only weak limit point and that for this \(\lambda\), \(\mu_{n,\lambda}\longrightarrow^{d}\pi\). To go from this to the convergence of \(P_{\lambda}(X_{n}\in\cdot)\) to \(\pi(\cdot)\), one needs extra conditions to rule out periodic behavior.

Since the _occupation measure_\(\mu_{n,\lambda}(A)\) is the mean of the _empirical measure_

\[L_{n}(A)\equiv\frac{1}{n}\sum_{j=0}^{n-1}I_{A}(X_{j}),\]

a natural question is what can one say about the convergence of the empirical measure? This is important for the statistical estimation of \(\pi\).

When the chain is Harris recurrent, it was shown in the previous section that for each \(x\) and for each \(A\in\mathcal{S}\)

\[L_{n}(A)\to\pi(A)\ \ w.p.\ 1\ \ (P_{x}).\]For a Feller chain admitting a unique stationary measure \(\pi\), one can appeal to the ergodic theorem to conclude that for each \(A\) in \(\mathcal{S}\), \(L_{n}(A)\to\pi(A)\) w.p. 1 (\(P_{x}\)) for \(\pi\)-almost all \(x\) in \(\mathbb{S}\). Further, if \(\mathbb{S}\) is Polish, then one can show that for \(\pi\)-almost all \(x\), \(L_{n}(\cdot)\longrightarrow^{d}\pi(\cdot)\) w.p. 1 (\(P_{x}\)).

### Markov chain Monte Carlo (MCMC)

#### 14.3.1 Introduction

Let \(\pi\) be a probability measure on a measurable space \((\mathbb{S},\mathcal{S})\). Let \(h(\cdot):\mathbb{S}\to\mathbb{R}\) be \(\mathcal{S}\)-measurable and \(\int|h|d\pi<\infty\) and \(\lambda=\int hd\pi\). The effort in the computation of \(\lambda\) depends on the complexity of the function \(h(\cdot)\) as well as that of the measure \(\pi\). Clearly, a first approach is to go back to the definition of \(\int hd\pi\) and use numerical approximation such as approximating \(h(\cdot)\) by a sequence of simple functions and evaluating \(\pi(\cdot)\) on the sets involved in these simple functions. However, in many situations this may not be feasible especially if the measure \(\pi(\cdot)\) is specified only up to a constant that is not easy to evaluate. Such is often the case in Bayesian statistics where \(\pi\) is the posterior distribution \(\pi_{\theta|X}\) of the parameter \(\theta\) given the data \(X\) whose density is proportional to \(f(X|\theta)\nu(d\theta)\), \(f(X|\theta)\) being the density of \(X\) given \(\theta\) and \(\nu(d\theta)\) the prior distribution of \(\theta\). In such situations, objects of interest are the posterior mean, variance, and other moments as well as posterior probability of \(\theta\) being in some set of interest. In these problems, the main difficulty lies in the evaluation of the normalizing constant \(C(X)=\int f(X|\theta)\nu(d\theta)\). However, it may be possible to generate a sequence of random variables \(\{X_{n}\}_{n\geq 1}\) such that the distribution of \(X_{n}\) gets close to \(\pi\) in a suitable sense and a law of large numbers asserting that

\[\frac{1}{n}\sum_{i=1}^{n}h(X_{i})\to\lambda=\int hd\pi\]

holds for a large class of \(h\) such that \(\int|h|d\pi<\infty\).

A method that has become very useful in Bayesian statistics in the past twenty years or so (with the advent of high speed computing) is that of generating a Markov chain \(\{X_{n}\}_{n\geq 1}\) with stationary distribution \(\pi\). This method has its origins in the important paper of Metropolis, Rosenbluth, Rosenbluth, Teller and Teller (1953). For the adaptation of this method to image processing problems, see Geman and Geman (1984).

This method is now known as the Markov chain Monte Carlo, or MCMC for short. For the basic limit theory of Markov chains, see Section 14.2. For proofs of the claims in the rest of this section and further details on MCMC, see the recent book of Robert and Casella (1999). In the rest of this section two of the widely used MCMC algorithms are discussed. These are the Metropolis-Hastings algorithm and the Gibbs sampler.

#### Metropolis-Hastings algorithm

Let \(\pi\) be a probability measure on a measurable space \((\mathbb{S},\mathcal{S})\). Let \(\pi\) be dominated by a \(\sigma\)-finite measure \(\mu\) with density \(f(\cdot)\). Let for each \(x\), \(q(y|x)\) be a probability density in \(y\) w.r.t. \(\mu\). That is, \(q(y|x)\) is jointly measurable as a function from \((\mathbb{S}\times\mathbb{S},\mathcal{S}\times\mathcal{S})\to\mathbb{R}^{+}\) and for each \(x\), \(\int_{\mathbb{S}}q(y|x)\mu(dy)=1\). Such a distribution \(q(\cdot|\cdot)\) is called the _instrumental_ or _proposal distribution_.

The Metropolis-Hastings algorithm generates a Markov chain \(\{X_{n}\}\) using the densities \(f(\cdot)\) and \(q(\cdot)\) in two steps as follows:

Step 1: Given \(X_{n}=x\), first generate a random variable \(Y_{n}\) with density \(q(\cdot|x)\).

Step 2: Then, set \(X_{n+1}=Y_{n}\) with probability \(p(x,Y_{n})\) and \(=X_{n}\) with probability \(1-p(x,Y_{n})\), where

\[p(x,y)\equiv\min\Big{\{}\frac{f(y)}{f(x)}\frac{q(x|y)}{q(y|x)},\ 1\Big{\}}. \tag{3.1}\]

Thus, the value \(Y_{n}\) is "accepted" as \(X_{n+1}\) with probability \(p(x,Y_{n})\) and if rejected the chain stays where it was, i.e., at \(X_{n}\).

Implicit in the above definition is that the state space of the Markov chain \(\{X_{n}\}\) is simply the set \(A_{f}\equiv\{x:f(x)>0\}\). It is also assumed that for all \(x,y\) in \(A_{f}\), \(q(y|x)>0\). The transition function \(P(x,A)\) for this Markov chain is given by

\[P(x,A)=I_{A}(x)(1-r(x))+\int_{A}p(x,y)q(y|x)\mu(dy) \tag{3.2}\]

where

\[r(x)=\int_{\mathbb{S}}p(x,y)q(y|x)\mu(dy).\]

It turns out that the measure \(\pi(\cdot)\) is a stationary measure for this Markov chain \(\{X_{n}\}\). Indeed, for any \(A\in\mathcal{S}\),

\[\int_{\mathbb{S}}P(x,A)\pi(dx) = \int_{\mathbb{S}}P(x,A)f(x)\mu(dx) \tag{3.3}\] \[= \int_{\mathbb{S}}I_{A}(x)(1-r(x))f(x)\mu(dx)\] \[\mbox{}+\int_{\mathbb{S}}\int_{A}p(x,y)q(y|x)f(x)\mu(dy)\mu(dx).\]

By definition of \(p(x,y)\), the identity

\[q(y|x)f(x)p(x,y)=p(y,x)q(x|y)f(y) \tag{3.4}\]

holds for all \(x,y\). Thus the second integral in (3.3) (using Tonelli's theorem) is

\[= \int_{A}\Big{(}\int_{\mathbb{S}}p(y,x)q(x|y)\mu(dx)\Big{)}f(y)\mu (dy)\]\[= \int_{A}r(y)f(y)\mu(dy).\]

Thus, the right side of (3.3) is

\[\int_{\mathbb{S}}I_{A}(x)f(x)\mu(dx)\equiv\pi(A),\]

verifying stationarity.

From the results of Section 14.2, it follows that if the transition function \(P(\cdot,\cdot)\) is Harris irreducible w.r.t. some reference measure \(\varphi\), then (since it admits \(\pi\) as a stationary measure) the law of large numbers, holds i.e., for any \(h\in L^{1}(\pi)\),

\[\frac{1}{n}\sum_{j=0}^{n-1}h(X_{j})\to\int hd\pi\quad\mbox{as}\quad n\to\infty\]

w.p. 1 for any initial distribution. Thus, a good MCMC approximation to \(\lambda=\int hd\pi\) is \(\hat{\lambda}_{n}\equiv\frac{1}{n}\sum_{j=0}^{n-1}h(X_{j})\). A sufficient condition for irreducibility is that \(q(y|x)>0\) for all \((x,y)\) in \(A_{f}\times A_{f}\).

Summarizing the above discussion leads to

**Theorem 14.3.1:** _Let \(\pi\) be a probability measure on a measurable space \((\mathbb{S},\mathcal{S})\) with probability density \(f(\cdot)\) w.r.t. a \(\sigma\)-finite measure \(\mu\). Let \(A_{f}=\{x:f(x)>0\}\). Let \(q(y|x)\) be a measurable function from \(A_{f}\times A_{f}\to(0,\infty)\) such that \(\int_{S}q(y|x)\mu(dy)=1\) for all \(x\) in \(A_{f}\). Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain generated by the Metropolis-Hastings algorithm (3.1). Then, for any \(h\in L^{1}(\pi)\),_

\[\frac{1}{n}\sum_{j=0}^{n-1}h(X_{j})\to\int hd\pi\quad\mbox{as}\quad n\to\infty \quad\mbox{w.p. }1 \tag{3.5}\]

_for any (initial) distribution of \(X_{0}\)._

The Metropolis-Hastings algorithm does not need the full knowledge of the target density \(f(\cdot)\) of \(\pi(\cdot)\). The function \(f(\cdot)\) enters the algorithm only through the function \(p(x,y)\), which involves only the knowledge of \(\frac{f(y)}{f(x)}\) and \(q(|\cdot|\) and hence this algorithm can be implemented even if \(f\) is known only up to a multiplicative constant. This is often the case in Bayesian statistics. Also, the choice of \(q(x|y)\) depends on \(f(\cdot)\) only through the condition that \(q(x|y)>0\) on \(A_{f}\times A_{f}\). Thus, the Metropolis-Hastings algorithm has wide applicability. Two special cases of this algorithm are given below.

#### 14.3.2.1 Independent Metropolis-Hastings

Let \(q(y|x)\equiv g(y)\) where \(g(\cdot)\) is a probability density such that \(g(y)>0\) if \(f(y)>0\).

Suppose \(\sup\big{\{}\frac{f(y)}{g(y)}:f(y)>0\big{\}}\equiv M<\infty\). Then, in addition to the law of large numbers (3.5) of Theorem 14.3.1, it holds that for any initial value of \(X_{0}\),

\[\|P(X_{n}\in\cdot)-\pi(\cdot)\|\leq 2\Big{(}1-\frac{1}{M}\Big{)}^{n}\]

where \(\|\cdot\|\) is the total variation norm. Thus, the distribution of \(\{X_{n}\}\) converges in total variation at a _geometric rate_. For a proof, see Robert and Casella (1999).

##### 14.3.2.2 Random-walk Metropolis-Hastings

Here the state space is the real line or a subset of some Euclidean space.

Let \(q(y|x)=g(y-x)\) where \(g(\cdot)\) is a probability density such that \(g(y-x)>0\) for all \(x,y\) such that \(f(x)>0\), \(f(y)>0\). This ensures irreducibility and hence the law of large numbers (3.5) holds. A sufficient condition for geometric convergence of the distribution of \(\{X_{n}\}\) in the real line case is the following:

1. The density \(f(\cdot)\) is symmetric about \(0\) and is _asymptotically log concave_, i.e., it holds that for some \(\alpha>0\) and \(x_{0}>0\), \[\log f(x)-\log f(y)\geq\alpha|y-x|\] for all \(y<x<-x_{0}\) or \(x_{0}<x<y\).
2. The density function \(g(\cdot)\) is positive and symmetric.

For further special cases and more results, see Robert and Casella (1999).

##### 14.3.3 The Gibbs sampler

Suppose \(\pi\) is the probability distribution of a bivariate random vector \((X,Y)\). A Markov chain \(\{Z_{n}\}_{n\geq 0}\) can be generated with \(\pi\) as its stationary distribution using only the families of conditional distributions \(Q(\cdot,y)\) of \(X|Y=y\) and \(P(\cdot,x)\) of \(Y|X=x\) for all \(x,y\) generated by the joint distribution \(\pi\) of \((X,Y)\). This Markov chain is known as the _Gibbs sampler_. The algorithm is as follows:

Step 1: Start with some initial value \(X_{0}=x_{0}\). Generate \(Y_{0}\) according to the conditional distribution \(P(Y\in\cdot\mid X_{0}=x_{0})=P(\cdot,x_{0})\).

Step 2: Next, generate \(X_{1}\) according to the conditional distribution \(P(X_{1}\in\cdot\mid Y_{0}=y_{0})=Q(\cdot,y_{0})\).

Step 3: Now generate \(Y_{1}\) as in Step 1 but with conditioning value \(X_{1}\).

Step 4: Now generate \(X_{2}\) as in Step 2 but with conditioning value \(Y_{1}\) and so on.

Thus, starting from \(X_{0}\), one generates successively \(Y_{0},X_{1},Y_{1},X_{2},Y_{2},\ldots\). Clearly, the sequences \(\{X_{n}\}_{n\geq 0}\), \(\{Y_{n}\}_{n\geq 0}\) and \(\{Z_{n}\equiv(X_{n},Y_{n})\}_{n\geq 0}\) are all Markov chains. It is also easy to verify that the marginal distribution \(\pi_{X}\) of \(X\), the marginal distribution \(\pi_{Y}\) of \(Y\), and the distribution \(\pi\) are, respectively, stationary for the \(\{X_{n}\}\), \(\{Y_{n}\}\), and \(\{Z_{n}\}\) chains. Indeed, if \(X_{0}\sim\pi_{X}\), then \(Y_{0}\sim\pi_{Y}\) and hence \(X_{1}\sim\pi_{X}\). Similarly one can verify the other two claims. Recall that a sufficient condition for the law of large numbers (3.5) to hold is irreducibility. A sufficient condition for irreducibility in turn is that the chain \(\{Z_{n}\}_{n\geq 0}\) has a transition function \(R(z,\cdot)\) that, for each \(z=(x,y)\), is absolutely continuous with respect to some fixed dominating measure on \(\mathbb{R}^{2}\).

The above algorithm is easily generalized to cover the \(k\)-variate case (\(k>2\)). Let \((X_{1},X_{2},\ldots,X_{k})\) be a random vector with distribution \(\pi\). For any vector \(x=(x_{1},x_{2},\ldots,x_{k})\) let \(x_{(i)}=(x_{1},x_{2},\ldots,x_{i-1},x_{i+1},\ldots,x_{k})\) and \(P_{i}(\cdot\mid x_{(i)})\) be the conditional distribution of \(X_{i}\) given \(X_{(i)}=x_{(i)}\). Now generate a Markov chain \(Z_{n}\equiv(Z_{n1},Z_{n2},\ldots,Z_{nk})\), \(n\geq 0\) as follows:

Step 1: Start with some initial value \(Z_{0j}=z_{0j}\), \(j=1,2,\ldots,k-1\). Generate \(Z_{0k}\) from the conditional distribution \(P_{k}(\cdot\mid X_{j}=z_{0j},j=1,2,\ldots,k-1)\).

Step 2: Next, generate \(Z_{11}\) from the conditional distribution \(P_{1}(\cdot\mid X_{j}=z_{0j},j=2,\ldots,k-1,X_{k}=Z_{0k})\).

Step 3: Next, generate \(Z_{12}\) from the conditional distribution \(P_{2}(\cdot\mid X_{1}=Z_{11},X_{j}=z_{0j},j=3,\ldots,k-1,X_{k}=Z_{0k})\) and so on until \((Z_{11},Z_{12},\ldots,Z_{1,k-1})\) is generated.

Now go back to Step 1 to generate \(Z_{1k}\) and repeat Steps 2 and 3 and so on. This sequence \(\{Z_{n}\}_{n\geq 0}\) is called the _Gibbs sampler_ Markov chain for the distribution \(\pi\).

A sufficient condition for irreducibility given earlier for the 2-variate case carries over to the \(k\)-variate case. For more on the Gibbs sampler, see Robert and Casella (1999).

### Problems

14.1.1:
1. Show using Definition 14.1.1 that when the state space \(\mathbb{S}\) is countable, for any \(n\), conditioned on \(\{X_{n}=a_{n}\}\), the events \(\{X_{n+j}=a_{n+j},1\leq j\leq k\}\) and \(\{X_{j}=a_{j}:0\leq j\leq n-1\}\) are independent for all choices of \(k\) and \(\{a_{j}\}_{j=0}^{n+k}\). Thus, conditioned on the "present" \(\{X_{n}=a_{n}\}\), the "past" \(\{X_{j}:j\leq n-1\}\) and "future" \(\{X_{j}:j\geq n+1\}\) are two families of independent random variables with respect to the conditional probability measure \(P(\cdot\mid X_{n}=a_{n})\), provided, \(P(X_{n}=a_{n})>0\).

2. Prove Proposition 14.2.2 using induction on \(n\) (cf. Chapter 6).
14.2 In Example 14.1.1 (Frog in the well), verify that 1. if \(\alpha_{i}\equiv 1-\frac{1}{c^{i}}\), \(c>1\), \(i\geq 1\), then 1 is null recurrent, 2. if \(\alpha_{i}\equiv\alpha\), \(0<\alpha<1\), then 1 is positive recurrent, and 3. if \(\alpha_{i}\equiv 1-\frac{1}{2i^{2}}\), then 1 is transient.
14.3 Consider SSRW in \(\mathbb{Z}^{2}\) where the transition probabilities are \(p_{(i,j)(i^{\prime},j^{\prime})}=\frac{1}{4}\) each if \((i^{\prime},j^{\prime})\in\{(i+1,j),(i-1,j),(i,j+1),(i,j-1)\}\) and zero otherwise. Verify that for \(n=2k\) \[p_{(0,0),(0,0)}^{(2k)}=\frac{1}{4^{2k}}\binom{2k}{k}^{2}\sim\frac{1}{\pi}\frac {1}{k}\] and conclude that (0,0) is null recurrent. Extend this calculation to SSRW in \(\mathbb{Z}^{3}\) and conclude that (0,0,0) is transient.
14.4 Show that if \(i\) is absorbing and \(j\to i\), then \(j\) is transient by showing that if \(j\to i\), then \(f_{ji}^{*}=P(T_{i}<T_{j}\mid X_{0}=j)>0\) and \(1-f_{jj}\geq f_{ji}^{*}\).
14.5 1. Let \(i\) be recurrent and \(i\to j\). Show that \(j\) is recurrent using Corollary 14.1.5. (**Hint:** Show that there exist \(n_{0}\) and \(m_{0}\) such that for all \(n\), \(p_{jj}^{(n_{0}+n+m_{0})}\geq p_{ji}^{(n_{0})}p_{ii}^{(n)},p_{ij}^{(m_{0})}\) with \(p_{ji}^{(n_{0})}>0\) and \(p_{ij}^{(m_{0})}>0\).) 2. Let \(i\) and \(j\) communicate. Show that \(d_{i}=d_{j}\).
14.6 Show that in a finite state space irreducible Markov chain \((\mathbb{S},\boldsymbol{P})\), all states are positive recurrent by showing 1. that for any \(i,j\) in \(\mathbb{S}\), there exist \(r\), \(r\leq K\) such that \(p_{ij}^{(r)}>0\), where \(K\) is the number of states in \(\mathbb{S}\), 2. for any \(i\) in \(\mathbb{S}\), there exists a \(0<\alpha<1\), and \(c<\infty\) such that \(P_{i}(T_{i}>n)\leq c\alpha^{n}\). Give an alternate proof by showing that if \(\mathbb{S}\) is finite, then for any initial distribution \(\mu\), the _occupation measures_ \[\mu_{n}^{(\cdot)}\equiv\frac{1}{(n+1)}\sum_{j=0}^{n}P_{\mu}(X_{j}\in\cdot)\] has a subsequence that converges to a probability distribution \(\pi\) that is stationary for \((\mathbb{S},\boldsymbol{P})\).
14.7 Prove Theorem 14.1.3 using the Markov property and induction.

14.8 Adapt the proof of Theorem 14.1.9 to show that for any \(i,j\) \[\frac{1}{n}\sum_{j=1}^{n}p_{ij}^{(k)}\to\frac{f_{ij}}{E_{j}T_{j}}\] if \(j\) is positive recurrent and \(0\) otherwise. Conclude that in a finite state space case, there must be at least one state that is positive recurrent.
14.9 If \(j\to i\) then \(\zeta_{1}\equiv\sum_{j=0}^{T_{i}-1}\delta_{X_{r}j}\), the number of visits to \(j\) before visiting \(i\) satisfies \(P_{i}(\zeta_{1}>n)\leq c\alpha^{n}\) for some \(0<c<\infty\), \(0<\alpha<1\) and all \(n\geq 1\).
14.10 Adapt the proof of Theorem 14.1.9 to establish the following laws of large numbers. Let \((\mathbb{S},\mathbf{P})\) be irreducible and positive recurrent with stationary distribution \(\pi\). 1. Let \(h:\mathbb{S}\to\mathbb{R}\) be such that \(\sum_{j\in\mathbb{S}}|h(j)|\pi_{j}<\infty\). Then, for any initial distribution \(\mu\), \[\frac{1}{n+1}\sum_{j=0}^{n}h(X_{j})\to\sum_{j\in\mathbb{S}}h(j)\pi_{j}\quad \mbox{w.p. 1}\] by first verifying that \[E_{i}\biggl{(}\biggl{|}\sum_{j=0}^{T_{i}-1}h(X_{j})\biggr{|}\biggr{)}<\infty.\] 2. Let \(g:\mathbb{S}\times\mathbb{S}\to R\) be such that \(\sum_{i,j\in\mathbb{S}}|g(i,j)|\pi_{i}p_{ij}<\infty\). Then, for any initial distribution \(\mu\), \[\frac{1}{n+1}\sum_{j=0}^{n}g(X_{j},X_{j+1})\to\sum_{i,j\in\mathbb{S}}g(i,j)\pi_ {i}p_{ij}\quad w.p.\;1.\] 3. Fix two disjoint subsets \(A\) and \(B\) in \(\mathbb{S}\). Evaluate the long run proportion of transitions from \(A\) to \(B\). 4. Extend (b) to conclude that the tail sequence \(Z_{n}\equiv\{X_{n+j}:j\geq 0\}\) of the Markov chain \(\{X_{n}\}_{n\geq 0}\) converges as \(n\to\infty\) in the sense of finite dimensional distributions to the strictly stationary sequence \(\{X_{n}\}_{n\geq 0}\) which is the Markov chain \((\mathbb{S},\mathbf{P})\) with initial distribution \(\pi\).
14.11 Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain that is irreducible and has at least two states. Show that w.p. 1 the trajectories \(\{X_{n}\}\) do not converge, i.e., w.p. 1, \(\lim_{n\to\infty}X_{n}\) does not exist. (**Hint:** Show that w.p. 1, the set of limit points of the set \(\{X_{n}:n\geq 0\}\) coincides with \(\mathbb{S}\).)14.12 Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain with state space \(\mathbb{S}\) and tr. pr. \(\boldsymbol{P}\equiv\big{(}(p_{ij})\big{)}\). A probability distribution \(\pi\equiv\{\pi_{j}:j\in\mathbb{S}\}\) is said to satisfy the condition of _detailed balance_ or _time reversibility_ with respect to \((\mathbb{S},\boldsymbol{P})\) if for all \(i,\,j,\,\pi_{i}p_{ij}=\pi_{j}p_{ji}\). 1. Show that such a \(\pi\) is necessarily a stationary distribution. 2. For the birth and death chain (Example 14.1.4), find a condition in terms of the birth and death rates \(\{\alpha_{i},\beta_{i}\}_{i\geq 0}\) for the existence of a probability distribution \(\pi\) that satisfies the condition of detailed balance.
14.13 (_Absorption probabilities and times_). Let \(0\) be an absorbing state. For any \(i\neq 0\), let \(\theta_{i}=f_{i0}\equiv P_{i}(T_{0}<\infty)\) and \(\eta_{i}=E_{i}T_{0}\). Show using the Markov property that for every \(i\neq 0\), \[\theta_{i} = p_{i0}+\sum_{j\neq 0}\theta_{j}p_{ij}\] \[\eta_{i} = 1+\sum_{j\neq 0}\eta_{j}p_{ij}.\] Apply this to the Gambler's ruin problem with \(\mathbb{S}=\{0,1,2,\ldots,K\}\), \(K<\infty\) and \(p_{00}=1\), \(p_{NN}=1\), \(p_{i,i+1}=p\), \(p_{i,i-1}=1-p\), \(0<p<1\), \(1\leq i\leq N-1\) and find the probability and expected waiting time for _ruin_ (absorption at \(0\)) starting from an initial fortune of \(i,\,1\leq i\leq N-1\).
14.14 (_Renewal theory via Markov chains_). Let \(\{X_{j}\}_{j\geq 1}\) be iid positive integer valued random variables. Let \(S_{0}=0\), \(S_{n}=\sum_{j=1}^{n}X_{j}\), \(n\geq 1\), \(N(n)=k\) if \(S_{k}\leq n<S_{k+1}\), \(k=0,1,2,\ldots\) be the number of _renewals_ up to time \(n\), \(A_{n}=n-S_{N(n)}\) be the _age_ of the current unit at time \(n\). 1. Show that \(\{A_{n}\}_{n\geq 0}\) is a Markov chain and find its state space \(\mathbb{S}\) and transition probabilities. 2. Assuming that \(EX_{1}<\infty\), verify that \[\pi_{j}=\frac{P(X_{1}>j)}{EX_{1}}j=0,1,2,\ldots\] is the unique stationary distribution. 3. Assuming that \(X_{1}\) has an aperiodic distribution and Theorem 14.1.18 holds, show that the discrete renewal theorem holds.
14.15 Prove Proposition 14.2.1 for the countable state space case.
14.16 Prove Proposition 14.2.2.

[MISSING_PAGE_FAIL:494]

**(Hint:** Let \(\{\tilde{X}_{n}\}_{n\geq 0}\) be a Markov chain with state space \(\mathbb{S}\equiv\{0,1,2,\ldots\}\) and transition probabilities same as that of \(\{X_{n}\}_{n\geq 0}\) except that the states \(\{0,1,2,\ldots,K\}\) are absorbing. Verify that \(\{V(\tilde{X}_{n})\}_{n\geq 0}\) is a nonnegative super-martingale and hence that \(\{\tilde{X}_{n}\}_{n\geq 0}\) is bounded w.p. 1. Now conclude that there must exist a state \(x\) that gets visited infinitely often by \(\{X_{n}\}_{n\geq 0}\).)
* Consider the reflecting nonhomogeneous random walk on \(\mathbb{S}\equiv\{0,1,2,\ldots\}\) such that \[p_{ij}=\left\{\begin{array}{ll}p_{i}&\mbox{if}\quad j=i+1\\ 1-p_{i}&\mbox{if}\quad j=i-1\end{array}\right.\] with \(p_{0}=1\), \(0<p_{i}\leq q_{i}\) for all \(i\geq k_{0}\) and some \(1\leq k_{0}<\infty\) and \(0<p_{i}<1\) for all \(i\geq 1\). Show that \(\{X_{n}\}_{n\geq 0}\) is irreducible and recurrent.
* Let \(\{X_{n}\}_{n\geq 0}\) be an irreducible and recurrent Markov chain with a countable state space \(\mathbb{S}\). Let \(V:\mathbb{S}\rightarrow\mathbb{R}_{+}\) be such that \(E_{x}V(X_{1})\leq V(x)\) for all \(x\) in \(\mathbb{S}\). Show that \(V(\cdot)\) is constant on \(\mathbb{S}\).
* Let \(\{C_{n}\}_{n\geq 1}\) be iid random variables with values in \([0,4]\). Let \(\{X_{n}\}_{n\geq 0}\) be a Markov chain with values in \([0,1]\) defined by the random iteration scheme \[X_{n+1}=C_{n+1}X_{n}(1-X_{n}),\ \ n\geq 0.\]
* Show that if \(E\log C_{1}<0\) then \(X_{n}=O(\lambda^{n})\) w.p. 1 for some \(0<\lambda<1\).
* Show also that if \(E\log C_{1}<0\) and \(0<V(\log C_{1})<\infty\) then there exist sequences \(\{a_{n}\}_{n\geq 1}\) and \(\{b_{n}\}_{n\geq 1}\) such that \[\frac{\log X_{n}-a_{n}}{b_{n}}\longrightarrow^{d}N(0,1).\]

### 15 Stochastic Processes

This chapter gives a brief discussion of two special classes of real valued stochastic processes \(\{X(t):t>0\}\) in continuous time \([0,\infty)\). These are continuous time Markov chains with a discrete state space (including _Poisson processes_) and _Brownian motion_ These are very useful in many areas of applications such as queuing theory and mathematical finance.

### 15.1 Continuous time Markov chains

#### Definition

Consider a physical system that can be in one of a finite or countable number of states \(\{0,1,2,\ldots,K\}\), \(K\leq\infty\). Assume that the system evolves in continuous time in the following manner. In each state the system stays a random length of time that is exponentially distributed and then jumps to a new state with a probability distribution that depends only on the current state and not on the past history. Thus, if the state of the system at the time of the \(n\)th transition is denoted by \(y_{n}\), \(n=0,1,2,\ldots\), then \(\{y_{n}\}_{n\geq 0}\) is a Markov chain with state space \(\mathbb{S}\equiv\{0,1,2,\ldots,K\}\), \(K\leq\infty\) and some transition probability matrix \(\boldsymbol{P}\equiv\big{(}(p_{ij})\big{)}\). If \(y_{n}=i_{n}\), then the system stays in \(i_{n}\) a random length of time \(L_{n}\), called the _sojourn time_, such that conditional on \(\{y_{n}=i_{n}\}_{n\geq 0}\), \(\{L_{n}\}_{n\geq 0}\) are independent exponential random variables with \(L_{n}\) having a mean \(\lambda_{i_{n}}^{-1}\). Now set the state of thesystem \(X(t)\) at time \(t\geq 0\) by

\[X(t)=\left\{\begin{array}{ll}y_{0}&\qquad 0\leq t<L_{0}\\ y_{1}&\qquad L_{0}\leq t<L_{0}+L_{1}\\ \vdots&\\ y_{n}&\qquad L_{\nu}+L_{1}+\cdots+L_{n-1}\leq t<L_{0}+L_{1}+\cdots+L_{n}.\end{array}\right. \tag{1.1}\]

Then \(\{X(t):t\geq 0\}\) is called a continuous time Markov chain with state space \(\mathbb{S}\), _jump probabilities_\(\boldsymbol{P}\equiv\bigl{(}(p_{ij})\bigr{)}\), _waiting time parameters_\(\{\lambda_{i}:i\in\mathbb{S}\}\), and _embedded Markov chain_\(\{y_{n}\}_{n\geq 0}\). To make sure that there are only finite number of transitions in finite time, i.e.,

\[\sum_{i=0}^{\infty}L_{i}=\infty\quad\text{w.p. }1\]

one needs to impose the _nonexplosion condition_

\[\sum_{i=0}^{\infty}\frac{1}{\lambda_{y_{n}}}=\infty\quad\text{w.p. }1. \tag{1.2}\]

(Problem 15.1)

Clearly, a sufficient condition for this is that \(\lambda_{i}<\infty\) for all \(i\in S\) and \(\{y_{n}\}_{n\geq 0}\) is an irreducible recurrent Markov chain.

It can be verified using the "memorylessness" property of the exponential distribution (Problem 15.2) that \(\{X(t):t\geq 0\}\) has the _Markov property_, i.e., for any \(0\leq t_{1}\leq t_{2}\leq t_{3}\leq\cdots\leq t_{r}<\infty\) and

\[P\bigl{(}X(t_{r})=i_{r}\mid X(t_{j})=i_{j},0\leq j\leq r-1\bigr{)} \tag{1.3}\] \[= P\bigl{(}X(t_{r})=i_{r}\mid X(t_{r-1})=i_{r-1}\bigr{)}.\]

#### Kolmogorov's differential equations

The functions

\[p_{ij}(t)\equiv P\bigl{(}X(t)=j\mid X(0)=i\bigr{)} \tag{1.4}\]

are called _transition functions_. To determine these functions from the jump probabilities \(\{p_{ij}\}\) and the waiting time parameters \(\{\lambda_{i}\}\), one uses the _Chapman-Kolmogorov_ equations

\[p_{ij}(t+s)=\sum_{k\in\mathbb{S}}p_{ik}(t)p_{kj}(s),\ t,s\geq 0 \tag{1.5}\]

which is an immediate consequence of the Markov property (1.3) and the definition (1.4). In addition to (1.5), one has the continuity condition

\[\lim_{t\downarrow 0}p_{ij}(t)=\delta_{ij}. \tag{1.6}\]Under the nonexplosion hypothesis (1.2), it can be shown (Chung (1967), Feller (1966), Karlin and Taylor (1975)) that \(p_{ij}(t)\) are differentiable as functions of \(t\) and satisfy the _Kolmogorov's forward_ and _backward differential equations_

\[p^{\prime}_{ij}(t)=\sum_{k}p_{ik}(t)p^{\prime}_{kj}(0)\quad\mbox{(forward)}\]

\[p^{\prime}_{ij}(t)=\sum_{k}p^{\prime}_{ik}(0)p_{kj}(t)\quad\mbox{(backward)}\]

Further, \(a_{kj}\equiv p^{\prime}_{kj}(0)\) can be shown to be \(\lambda_{k}p_{kj}\) for \(k\neq j\) and \(-\lambda_{k}\) for \(k=j\). The matrix \(A\equiv\bigl{(}(a_{ij})\bigr{)}\) is called the _infinitesimal matrix_ or _generator_ of the process \(\{X(t):t\geq 0\}\). If the state space \(S\) is finite, i.e., \(K<\infty\), then \(P(t)\equiv\bigl{(}(p_{ij}(t))\bigr{)}\) can be shown to be

\[P(t)=\exp(At)\equiv\sum_{n=0}^{\infty}\frac{A^{n}}{n!}t^{n}.\]

#### Examples

**Example 15.1.1:** (_Birth and death process_). Here

\[p_{i,i+1} = \frac{\alpha_{i}}{\alpha_{i}+\beta_{i}}\quad i\geq 0\] \[p_{i,i-1} = \frac{\beta_{i}}{\alpha_{i}+\beta_{i}}\quad i\geq 1\] \[\lambda_{i} = (\alpha_{i}+\beta_{i})\quad i\geq 0\]

where \(\{\alpha_{i},\beta_{i}\}_{i\geq 0}\) are nonnegative numbers with \(\alpha_{i}\) being the _birth rate_, \(\beta_{i}\) being the _death rate_. This has the meaning that given \(X(t)=i\), for small \(h>0\), \(X(t+h)\) goes up to \((i+1)\) with probability \(\alpha_{i}h+o(h)\) or goes down to \((i-1)\) with probability \(\beta_{i}h+o(h)\) or stays at \(i\) with probability \(1-(\alpha_{i}+\beta_{i})h+o(h)\). In this case the forward and backward equations become

\[p^{\prime}_{ij}(t) = \alpha_{j-1}p_{i,j-1}(t)+\beta_{j+1}p_{i,j+1}(t)-(\alpha_{j}+ \beta_{j})p_{ij}(t),\] \[p^{\prime}_{ij}(t) = \alpha_{i}p_{i+1,j}(t)+\beta_{i}p_{i-1,j}(t)-(\alpha_{i}+\beta_{i })p_{ij}(t)\]

with initial condition \(p_{ij}(0)=\delta_{ij}\).

* _Pure birth process_. A special case of the above is when \(\beta_{i}\equiv 0\) for all \(i\). Here \(p_{ij}(t)=0\) if \(j<i\) and \(X(t)\) is a nondecreasing function of \(t\) and the jumps are of size one.

A further special case of this when \(\alpha_{i}\equiv\alpha\) for all \(i\). In this case, the process waits in each state a random length of time with mean \(\alpha^{-1}\) and jumps one step higher. It can be verified that in this case, the solution of the Kolmogorov's differential equations (1.7a) and (1.7b) are given by

\[p_{ij}(t)=e^{-\alpha t}\frac{(\alpha t)^{j-i}}{(j-i)!}. \tag{1.9}\]

From this it is easy to conclude that \(\{X(t):t\geq 0\}\) is a _Levy process_, i.e., it has stationary and independent increments, i.e., for \(0=t_{0}\leq t_{1}\leq t_{2}\leq t_{3}\leq\cdots\leq t_{r}<\infty\), \(Y_{j}=X(t_{j})-X(t_{j-1})\), \(j=1,2,\ldots,r\) are independent and the distribution of \(Y_{j}\) depends only on \((t_{j}-t_{j-1})\). Further, in this case, \(X(t)-X(0)\) has a Poisson distribution with mean \(\alpha t\). This \(\{X(t):t\geq 0\}\) is called a _Poisson process_ with intensity parameter \(\alpha\).

Another special case is the _linear birth and death process_. Here \(\alpha_{i}=i\alpha\), \(\beta_{i}=i\beta\) for \(i=0,1,2,\ldots\). The _pure death process_ has parameters \(\alpha_{i}\equiv 0\) for \(i\geq 0\). A number of queuing processes can be modeled as a birth and death process and more generally as a continuous time Markov chain. For example, an M/M/s queuing system is one in which customers arrive at a service facility at the jump times of a Poisson process (with parameter \(\alpha\)) and there are \(s\) servers with service time at each server being exponential with the same mean (\(=\beta^{-1}\)). The number \(X(t)\) of customers in the system at time \(t\) evolves a birth and dealt process with parameters \(\alpha_{i}\equiv\alpha\) for \(i\geq 0\) and \(\beta_{i}=i\beta\), \(0\leq i\leq s\), \(=s\beta\) for \(i>s\).

**Example 15.1.2:** (_Markov branching processes_). Here \(X(t)\) is the population size in a process where each particle lives a random length of time with exponential distribution with mean \(\alpha^{-1}\) and on death create a random number of new particles with offspring distribution \(\{p_{j}\}_{j\geq 0}\) and all particles evolve independently of each other. This implies that \(\lambda_{i}=i\alpha\), \(i\geq 0\), \(p_{ij}=p_{j-i+1}\), \(j\geq i-1\) and \(=0\) for \(j<i-1\), \(i\geq 1\), \(p_{00}=1\). Thus \(0\) is an absorbing barrier. The random variable \(T\equiv\inf\{t:t>0,X(t)=0\}\) is called the _extinction time_. It can be shown that

\[\sum_{j=0}^{\infty}p_{ij}(t)s^{j}=\bigg{(}\sum_{j=0}^{\infty}p_{1j}(t)s^{j} \bigg{)}^{i}\quad\text{for}\quad i\geq 0,\quad 0\leq s\leq 1 \tag{1.10}\]

and also that

\[F(s,t)\equiv\bigg{(}\sum_{j=0}^{\infty}p_{1j}(t)s^{j}\bigg{)}\]

satisfies the differential equation

\[\frac{\partial F}{\partial t}(s,t) = u(s)\frac{\partial}{\partial s}F(s,t)\quad\text{(forward equation)} \tag{1.11}\] \[\frac{\partial F}{\partial t}(s,t) = u\big{(}F(s,t)\big{)}\quad\text{(backward equation)} \tag{1.12}\]with \(F(s,0)\equiv s\)

\[\mbox{where}\quad u(s)\equiv\alpha\bigg{(}\sum_{j=0}^{\infty}p_{j}s^{j}-s\bigg{)}. \tag{1.13}\]

Further, if \(q\equiv P(T<\infty\mid X(0)=1)\) is the _extinction probability_, the \(q\) is the smallest solution in [0,1] of the equation \(q=\sum_{j=0}^{\infty}p_{j}q^{j}\) (cf. Chapter 18). (See Athreya and Ney (2004), Chapter III, p. 106.)

**Example 15.1.3:** (_Compound Poisson processes_). Let \(\{L_{i}\}_{i\geq 0}\) and \(\{\xi_{i}\}_{i\geq 1}\) be two independent sequences of random variables such that \(\{L_{i}\}_{i\geq 0}\) are iid exponential with mean \(\alpha^{-1}\) and \(\{\xi_{i}\}_{i\geq 1}\) are iid integer valued random variables with distribution \(\{p_{j}\}\). Let \(X(t)=k\) if \(L_{0}+\cdots+L_{k}\leq t<L_{0}+\cdots+L_{k+1}\). Let

\[X(t)=\left\{\begin{array}{ll}0&\qquad 0\leq t<L_{0}\\ 1&\qquad L_{0}\leq t<L_{0}+L_{1}\\ \vdots&\\ k&\qquad L_{0}+\cdots+L_{k-1}\leq t<L_{0}+\cdots+L_{k},\\ \vdots&\\ \end{array}\right.\]

Let

\[Y(t)=\sum_{i=1}^{X(t)}\xi_{i},\quad t\geq 0. \tag{1.14}\]

Then \(\{Y(t):t\geq 0\}\) is a continuous time Markov chain with state space \(S\equiv\{0,\pm 1,\pm 2,\ldots\}\), jump probabilities \(p_{ij}=P(\xi_{1}=j-i)=p_{j-i}\). It is also a Levy process. It is called a _compound Poisson process_ with jump rate \(\alpha\) and jump distribution \(\{p_{j}\}\). If \(p_{1}\equiv 1\) this reduces to the Poisson process case.

#### Limit theorems

To investigate what happens to \(p_{ij}(t)\equiv P(X(t)=j\mid X(0)=i)\) as \(t\to\infty\), one needs to assume that the embedded chain \(\{y_{n}\}_{n\geq 0}\) is irreducible and recurrent. This implies that for any \(i_{0}\) the random variable

\[T=\min\{t:t>L_{0},\ X(t)=i_{0}\}\]

is finite w.p. 1. Further, the process, starting from \(i_{0}\), returns to \(i_{0}\) infinitely often and hence by the Markov property is regenerative in the sense that the excursions between consecutive returns to \(i_{0}\) are iid. One can use this, laws of large numbers and renewal theory (cf. Section 8.5) to arrive at the following:

**Theorem 15.1.1:**_Let \(P=\{p_{ij}\}\) be irreducible and recurrent and \(0<\lambda_{i}<\infty\) for all \(i\) in \(\mathbb{S}.\) Let there exist a probability distribution \(\{\pi_{i}\}\) such that_

\[\sum_{j\in\mathbb{S}}a_{ij}\pi_{j}=0\quad\text{for all}\quad i \tag{1.15}\]

_where_

\[a_{ij} = \lambda_{i}p_{ij}\quad i\neq j\] \[= -\lambda_{i}\quad i=j.\]

_Then_

* \(\{\pi_{j}\}\) _is_ stationary _for_ \(\{p_{ij}(t)\}\)_, i.e.,_ \(\sum_{i\in\mathbb{S}}\pi_{i}p_{ij}(t)=\pi_{j}\) _for all_ \(j,\)__\(t\geq 0\)_,_
* _for all_ \(i,j\)__ \[\lim_{t\to\infty}p_{ij}(t)=\pi_{j},\] (1.16) _and hence_ \(\{\pi_{j}\}\) _is the unique stationary distribution,_
* _for any function_ \(h:\mathbb{S}\to\mathbb{R}\)_, such that_ \(\sum_{j\in\mathbb{S}}|h(j)|\pi_{j}<\infty\)_,_ \[\lim_{t\to\infty}\frac{1}{t}\int_{0}^{t}h\big{(}X(u)\big{)}du=\sum_{j\in \mathbb{S}}h(j)\pi_{j}\quad\text{w.p. }1\] (1.17) _for any initial distribution of_ \(X(0)\)_._

Note that (1.16) holds without any assumption of aperiodicity on \(P\equiv\big{(}(p_{ij})\big{)}\).

A sufficient condition for a probability distribution \(\{\pi_{j}\}\) to be a stationary distribution is the so-called _detailed balance_ condition

\[\pi_{k}a_{kj}=\pi_{j}a_{jk}. \tag{1.18}\]

One can use this for birth and death chains on a finite state space \(S\equiv\{0,1,2,\ldots,N\}\), \(N<\infty\) to conclude that the stationary distribution is given by

\[\pi_{n}=\frac{\alpha_{n-1}\alpha_{n-2}\ldots\alpha_{0}}{\beta_{n}\beta_{n-1} \ldots\beta_{1}}\pi_{0} \tag{1.19}\]

provided \(\alpha_{i}>0\) for all \(0\leq i\leq N-1,\)\(\beta_{i}>0\) for all \(1\leq i\leq N\) and \(\alpha_{N}=0,\)\(\beta_{0}=0.\) A necessary and sufficient condition for equilibrium, i.e., the existence of a stationary distribution when \(N=\infty\) is

\[\sum_{n=1}^{\infty}\frac{\alpha_{n-1}\ldots\alpha_{0}}{\beta_{n}\ldots\beta_{1 }}<\infty. \tag{1.20}\]This yields in the M/M/s case with arrival rate \(\alpha\) and service rate \(\beta\) (i.e., \(\alpha_{i}\equiv\alpha\), for \(i\geq 0\), \(\beta_{i}\equiv i\beta\) for \(1\leq i\leq s\), \(=s\beta\) for \(i>s\)) the necessary and sufficient condition for the equilibrium, that the _traffic intensity_

\[\rho\equiv\frac{\alpha}{s\beta}<1, \tag{1.21}\]

i.e., the mean number of arrivals per unit time, be less than the mean number of the persons served per unit time. For further discussion and results, see the books of Karlin and Taylor (1975) and Durrett (2001).

### 15.2 Brownian motion

**Definition 15.2.1:** A real valued stochastic process \(\{B(t):t\geq 0\}\) is called _standard Brownian motion_ (SBM) if it satisfies

* \(B(0)=0\),
* \(B(t)\) has \(N(0,t)\) distribution, for each \(t\geq 0\),
* it is a _Levy process_, i.e., it has stationary independent increments.

It follows that \(\{B(t):t\geq 0\}\) is a Gaussian process (i.e., the finite dimensional distributions are Gaussian) with mean function \(m(t)\equiv 0\) and covariance function \(c(s,t)=\min(s,t)\). It can be shown that the trajectories are continuous w.p. 1. Thus, Brownian motion is a Gaussian process, has continuous trajectories and has stationary independent increments (and hence is Markovian). These features make it a very useful process as a building block for many real world phenomena such as the movement of pollen (which was studied by the English Botanist, Robert Brown, and hence the name Brownian motion) movement of a tagged particle in a liquid subject to the bombardment of the molecules of the liquid (studied by Einstein and Slomuchowski) and the fluctuations in stock market prices (studied by the French Economist Bachelier).

#### 15.2.1 Construction of SBM

Let \(\{\eta_{i}\}_{i\geq 1}\) be iid \(N(0,1)\) random variables on some probability space \((\Omega,\mathcal{F},P)\). Let \(\{\phi_{i}(\cdot)\}_{i\geq 1}\) be the sequence of _Haar functions_ on \([0,1]\) defined by the doubly indexed collection

\[H_{00}(t)\equiv 1\]

\[H_{11}(t)=\left\{\begin{array}{ccc}1&\mbox{on}&[0,\frac{1}{2})\\ -1&\mbox{on}&[\frac{1}{2},1]\end{array}\right.\]and for \(n\geq 1\)

\[H_{n,j}(t) = 2^{\frac{n-1}{2}}\quad\mbox{for}\quad t\mbox{ in }\Big{[}\frac{(j-1)} {2^{n}},\frac{j}{2^{n}}\Big{)}\] \[= -2^{\frac{n-1}{2}}\quad\mbox{for}\quad t\mbox{ in }\Big{[}\frac{j} {2^{n}},\frac{j+1}{2^{n}}\Big{]}\] \[= 0\quad\mbox{otherwise}\] \[j = 1,3,\ldots,2^{n-1}.\]

It is known that this family is a complete orthonormal basis for \(L^{2}([0,1])\). Let

\[B_{N}(t,\omega)\equiv\sum_{i=1}^{N}\eta_{i}(\omega)\int_{0}^{t}\phi_{i}(u)du. \tag{2.1}\]

Then, for each \(N\), \(\{B_{N}(t,\omega):0\leq t\leq 1\}\) is a Gaussian process on \((\Omega,{\cal F},P)\) with mean function \(m_{N}(t)\equiv 0\) and covariance function \(c_{N}(s,t)=\sum_{i=1}^{N}\big{(}\int_{0}^{t}\phi_{i}(u)du\big{)}\big{(}\int_{ 0}^{s}\phi_{i}(u)du\big{)}\) and the property that the trajectories \(t\to B_{N}(t,\omega)\) are continuous in \(t\) for each \(\omega\) in \(\Omega\).

It can be shown (Problem 15.11) that w.p. 1 the sequence \(\{B_{N}(\cdot,\omega)\}_{N\geq 1}\) is a Cauchy sequence in the Banach space \(C[0,1]\) of continuous real valued functions on \([0,1]\) with supremum metric. Hence, \(\{B_{N}(\cdot,\omega)\}_{N\geq 1}\) converges w.p. 1 to a limit element \(B(\cdot,\omega)\) which will be a Gaussian process with continuous trajectories and mean and covariance functions \(m(t)\equiv 0\) and \(c(s,t)=\sum_{i=1}^{\infty}\big{(}\int_{0}^{t}\phi_{i}(u)du\big{)}\big{(}\int_{ 0}^{s}\phi_{i}(u)du\big{)}=\int_{0}^{t}I_{[0,t]}(u)I_{[0,s]}(u)du=\min(s,t)\) respectively. (See Section 2.3 of Karatzas and Shreve (1991).)

Thus,

\[B(t,\omega)\equiv\sum_{i=1}^{\infty}\eta_{i}(\omega)\int_{0}^{t}\phi_{i}(u)du \tag{2.2}\]

is a well-defined stochastic process for \(0\leq t\leq 1\) that has all the properties claimed above and is called SBM on [0,1]. Let \(\{B^{(j)}(t,\omega):0\leq t\leq 1\}_{j\geq 1}\) be iid copies of \(\{B(t,\omega):0\leq t\leq 1\}\) as defined as above. Now set

\[B(t,\omega)\equiv\left\{\begin{array}{ll}B^{(1)}(t,\omega),&0\leq t\leq 1\\ B^{(1)}(1,\omega)+B^{(2)}(t-1,\omega),&1\leq t\leq 2\\ \vdots\\ B(n,\omega)+B^{(n+1)}(t-n,\omega),&n\leq t\leq n+1,\ n=1,2,\ldots\end{array}\right. \tag{2.3}\]

Then \(\{B(t,\omega):t\geq 0\}\) satisfies

* \(B(0,\omega)=0\),
* \(t\to B(t,\omega)\) is continuous in \(t\) for all \(\omega\),
* it is Gaussian with mean function \(m(t)\equiv 0\) and covariance function \(c(s,t)\equiv\min(s,t)\),

i.e., it is SBM on \([0,\infty)\). From now on the symbol \(\omega\) may be suppressed.

#### Basic properties of SBM

* _Scaling properties_ Fix \(c>0\) and set \[B_{c}(t)\equiv\frac{1}{\sqrt{c}}B(ct),\ \ t\geq 0.\] (2.4) Then, \(\{B_{c}(t)\}_{t\geq 0}\) is also an SBM. This is easily verified by noting that \(B_{c}(0)=0\), \(B_{c}(t)\sim N(0,t)\), \(\mbox{Cov}(B_{c}(t),B_{c}(s))=\frac{1}{c}\min\{ct,cs\}=\min(t,s)\) and that \(\{B_{c}(\cdot)\}\) is a Levy process and the trajectories are continuous w.p. 1.
* _Reflection_ If \(\{B(\cdot)\}\) is SBM, then so is \(\{-B(\cdot)\}\). This follows from the symmetry of the mean zero Gaussian distribution.
* _Time inversion_ Let \[\tilde{B}(t)=\left\{\begin{array}{ll}tB(\frac{1}{t})&\mbox{for}\ \ \ t>0\\ 0&\mbox{for}\ \ \ t=0.\end{array}\right.\] (2.5) Then \(\{\tilde{B}(t):t\geq 0\}\) is also an SBM. The facts that \(\{\tilde{B}(t):t>0\}\) is a Gaussian process with mean and covariance function same as SBM and the trajectories are continuous in the open interval \((0,\infty)\) are straightforward to verify. It only remains to verify that \(\lim_{t\to 0}\tilde{B}(t)=0\) w.p. 1. Fix \(0<t_{1}<t_{2}\). Then \(\{\tilde{B}(t):t_{1}\leq t\leq t_{2}\}\) is a Gaussian process with mean function \(0\) and covariance function \(\min(s,t)\) and has continuous trajectories, i.e., it has the same distribution as \(\{B(t):t_{1}\leq t\leq t_{2}\}\). Thus \(\tilde{X}_{1}\equiv\sup\{|\tilde{B}(t)|:t_{1}\leq t\leq t_{2}\}\) has the same distribution as \(X_{1}\equiv\sup\{|B(t)|:t_{1}\leq t\leq t_{2}\}\). Since both converge as \(t_{1}\downarrow 0\) to \(\tilde{X}_{2}(t_{2})\equiv\sup\{\tilde{B}(t):0<t\leq t_{2}\}\) and \(X_{2}(t_{2})\equiv\sup\{B(t):0<t\leq t_{2}\}\), respectively, these two have the same distribution. Again, since \(\tilde{X}_{2}(t_{2})\) and \(X_{2}(t_{2})\) both converge as \(t_{2}\downarrow 0\) to \(\tilde{X}_{2}\equiv\overline{\lim}_{t\downarrow 0}|\tilde{B}(t)|\) and \(X_{2}\equiv\overline{\lim}_{t\downarrow 0}|B(t)|\), respectively, \(\tilde{X}_{2}\) and \(X_{2}\) have the same distribution. But \(X_{2}=0\) w.p. 1 since \(B(t)\) is continuous in \([0,\infty)\). Thus \(\tilde{X}_{2}=0\) w.p. 1, i.e., \(\lim_{t\to 0}\tilde{B}(t)=0\) w.p. 1.
* _Translation invariance (after a fixed time \(t_{0}\))_ Fix \(t_{0}>0\) and set \[B_{t_{0}}(t)=B(t+t_{0})-B(t_{0}),\ t\geq 0.\] (2.6) Then \(\{B_{t_{0}}(t)\}_{t\geq 0}\) is also an SBM. This follows from the stationary independent increments property.
* _Translation invariance (after a stopping time \(T_{0}\))_ A random variable \(T(\omega)\) with values in \([0,\infty)\) is called a _stoppingtime_ w.r.t. the SBM \(\{B(t):t\geq 0\}\) if for each \(t\) in \([0,\infty)\) the event \(\{T\leq t\}\) is in the \(\sigma\)-algebra \({\cal F}_{t}\equiv\sigma(B(s):s\leq t)\) generated by the trajectory \(B(s)\) for \(0\leq s\leq t\). Examples of stopping times are \[T_{a}=\min\{t:t\geq 0,\ B(t)\geq a\}\] (2.7) for \(0<a<\infty\) \[T_{a,b}=\min\{t:t>0,\ B(t)\not\in(a,b)\}\] (2.8) where \(a<0<b\). Let \(T_{0}\) be a stopping time w.r.t. SBM \(\{B(t):t\geq 0\}\). Let \[B_{T_{0}}(t)\equiv\{B(T_{0}+t)-B(T_{0}):t\geq 0\}.\] (2.9) Then \(\{B_{T_{0}}(t)\}_{t\geq 0}\) is again an SBM. Here is an outline of the proof. * \(T_{0}\) deterministic is covered by (4) above. * If \(T_{0}\) takes only countably many values, say \(\{a_{j}\}_{j\geq 1}\), then it is not difficult to show that conditioned on the event \(T_{0}=a_{j}\), the process \(B_{T_{0}}(t)\equiv\{B(T_{0}+t)-B(T_{0})\}\) is SBM. Thus the unconditional distribution of \(\{B_{T_{0}}(t):t\geq 0\}\) is again an SBM. * Next given a general stopping time \(T_{0}\), one can approximate it by a sequence \(T_{n}\) of stopping times where for each \(n\), \(T_{n}\) is discrete. By continuity of trajectories, \(\{B_{T_{0}}(t):t\geq 0\}\) has the same distribution as the limit of \(\{B_{T_{n}}(t):t\geq 0\}\) as \(n\to\infty\). A consequence of the above two properties is that SBM has the _Markov_ and the _strong Markov properties_. That is, for each fixed \(t_{0}\), the distribution of \(B(t)\), \(t\geq t_{0}\) given \(B(s):s\leq t_{0}\) depends only on \(B(t_{0})\) (Markov property) and for each stopping time \(T_{0}\), the distribution of \(B(t):t\geq T_{0}\) given \(B(s):s\leq T_{0}\) depends only on \(B(T_{0})\) (strong Markov property). * _The reflection principle_ Fix \(a>0\) and let \(T_{a}=\inf\{t:B(t)\geq a\}\) where \(\{B(t):t\geq 0\}\) is SBM. For any \(t>0\), \(a>0\), \[P(T_{a}\leq t) = P(T_{a}\leq t,\ B(t)>a)\] \[{}+P(T_{a}\leq t,\ B(t)<a).\] Now, by continuity of the trajectory, \(B(T_{a})=a\) on \(\{T_{a}\leq t\}\). Thus \[P\bigl{(}T_{a}\leq t,\ B(t)<a\bigr{)}\] \[= P\bigl{(}T_{a}\leq t,\ B(t)<B(T_{a})\bigr{)}\] \[= P\bigl{(}T_{a}\leq t,\ B(t)-B(T_{a})<0\bigr{)}\] \[= P\bigl{(}T_{a}\leq t,\ B(t)-B(T_{a})>0\bigr{)}\] \[= P\bigl{(}T_{a}\leq t,\ B(t)>a\bigr{)}.\]To see this note that by (4), \(\{B(T_{a}+h)-B(T_{a}):h\geq 0\}\) is independent of \(T_{a}\) and has the same distribution as an SBM and hence \(\big{\{}-\big{(}B(T_{a}+h)-B(T_{a})\big{)}:h\geq 0\big{\}}\) is also independent of \(T_{a}\) and has the same distribution as an SBM. Thus,

\[P(T_{a}\leq t) = 2P\big{(}T_{a}\leq t,\ B(t)>a\big{)} \tag{2.10}\] \[= 2P\big{(}B(t)>a\big{)}\] \[= 2\Big{(}1-\Phi\Big{(}\frac{a}{\sqrt{t}}\Big{)}\Big{)}\]

where \(\Phi(\cdot)\) is the standard \(N(0,1)\) cdf. The above argument is known as the _reflection principle_ as it asserts that the path

\[\tilde{B}(t)\equiv\left\{\begin{array}{ll}B(t)&,\quad t\leq T_{a}\\ B(T_{a})-\big{(}B(t)-B(T_{a})\big{)}&,\quad t>T_{a}\end{array}\right. \tag{2.11}\]

obtained by reflecting the original path on the line \(y=a\) from the point \((T_{a},a)\) for \(t>T_{a}\) yields a path that has the same distribution as the original path. Thus the probability density function of \(T_{a}\) is

\[f_{T_{a}}(t) = 2\phi\Big{(}\frac{a}{\sqrt{t}}\Big{)}\frac{1}{2}\frac{a}{t^{3/2}} \tag{2.12}\] \[= \frac{1}{\sqrt{2\pi}}e^{-\frac{a^{2}}{4T}}\frac{a}{t^{3/2}}\]

implying that \(ET_{a}^{p}<\infty\) for \(p<1/2\) and \(\infty\) for \(p\geq 1/2\). Also, by the strong Markov property the process \(\{T_{a}:a\geq 0\}\) is a process with stationary independent increments, i.e., a Levy process. It is also a _stable process_ of order \(1/2\).

One can use this calculation of \(P(T_{a}\leq t)\) to show that the probability that the SBM crosses zero in the interval \((t_{1},t_{2})\) is \(\frac{2}{\pi}\arcsin\sqrt{\frac{t_{1}}{t_{2}}}\) (Problem 15.12).

If \(M(t)\equiv\sup\{B(s):0\leq s\leq t\}\) then for \(a>0\)

\[P\big{(}M(t)>a\big{)} = P(T_{a}\leq t) \tag{2.13}\] \[= 2P\big{(}B(t)>a\big{)}\] \[= P\big{(}|B(t)|>a\big{)}\]

it follows that \(M(t)\) has the same distribution as \(|B(t)|\) and hence has finite moments of all order. In fact,

\[E\big{(}e^{\theta M(t)}\big{)}<\infty\quad\mbox{for all}\quad\theta>0.\]

#### Some related processes

* Let \(\{B(t):t\geq 0\}\) be a SBM. For \(\mu\) in \((-\infty,\infty)\) and \(\sigma>0\), the process \(B_{\mu,\sigma}(t)\equiv\mu t+\sigma B(t)\), \(t\geq 0\) is called _Brownian motion with constant drift \(\mu\)_ and _constant diffusion \(\sigma\)_.
* Let \(B_{0}(t)=B(t)-tB(1)\), \(0\leq t\leq 1\). The process \(\{B_{0}(t):0\leq t\leq 1\}\) is called the _Brownian bridge_. It is a Gaussian process with mean function \(0\) and covariance \(\min(s,t)-st\) and has continuous trajectories that vanish both at \(0\) and \(1\).
* Let \(Y(t)=e^{-t}B(e^{2t})\), \(-\infty<t<\infty\). Then \(\{Y(t):t\geq 0\}\) is a Gaussian process with mean function \(0\) and covariance function \(c(s,t)=e^{-(s+t)}e^{+2s}=e^{s-t}\) if \(s<t\). This process is called the _Ornstein-Uhlenbeck_ process. It is to be noted that for each \(t\), \(Y(t)\sim N(0,1)\) and in fact \(\{Y(t):-\infty<t<\infty\}\) is a strictly stationary process and is a Markov process as well.

#### Some limit theorems

Let \(\{\xi_{i}\}_{i\geq 1}\) be iid random variables with \(E\xi_{1}=0\), \(E\xi_{1}^{2}=1\). Let \(S_{0}=0\), \(S_{n}=\sum_{i=1}^{n}\xi_{i}\), \(n\geq 1\). Let \(B_{n}(j/n)=\frac{1}{\sqrt{n}}S_{j}\), \(j=0,1,2,\ldots,n\) and \(\{B_{n}(t):0\leq t\leq 1\}\) be obtained by linear interpolation from the values at \(j/n\) for \(j=0,1,2,\ldots,n\). Then for each \(n\), \(\{B_{n}(t):0\leq t\leq 1\}\) is a random continuous trajectory and hence is a random element of the metric space of continuous real valued functions on [0,1] that are zero at zero with the metric

\[\rho(f,g)\equiv\{\sup|f(t)-g(t)|:0\leq t\leq 1\}. \tag{2.14}\]

Let \(\mu_{n}\equiv PB_{n}^{-1}\) be the induced probability measure on \({\cal C}[0,1]\). The following is a generalization of the central limit theorem as noted in Chapter 11.

**Theorem 15.2.1:** (_Donsker_). _In the space \(({\cal C}[0,1],\rho)\) the sequence of probability measures \(\{\mu_{n}\equiv PB_{n}^{-1}\}_{n\geq 1}\) converges weakly to \(\mu\), the probability distribution of the SBM. That is, for any bounded continuous function \(h\) from \(C[0,1]\) to \(\mathbb{R}\), \(\int hd\mu_{n}\to fhd\mu\)._

For a proof, see Billingsley (1968).

**Corollary 15.2.2:** _For any continuous functional \(T\) on \(({\cal C}[0,1],\rho)\) to \(\mathbb{R}^{k}\), \(k<\infty\), the distribution of \(T(B_{n})\) converges to that of \(T(B)\). In particular, the joint distribution of \(\Big{(}\max\limits_{0\leq j\leq n}\frac{S_{j}}{\sqrt{n}},\max\limits_{0\leq j \leq n}\frac{|S_{j}|}{\sqrt{n}}\Big{)}\) converges weakly to that of \(\big{(}\max\limits_{0\leq u\leq 1}B(u),\max\limits_{0\leq u\leq 1}|B(u)|\big{)}\)._There are similar limit theorems asserting the convergence of the empirical processes to the Brownian bridge with applications to the limit distribution of the Kolmogorov-Smirnov statistics (see Billingsley (1968)).

**Theorem 15.2.3:** (_Laws of large numbers_).

\[\lim_{t\to\infty}\frac{B(t)}{t}=0\quad\mbox{w.p.\ 1}. \tag{2.15}\]

**Proof:** By the time inversion property (2.5)

\[\lim_{t\to 0}\tilde{B}(t)=0\quad\mbox{w.p.\ 1}.\]

But \(\lim_{t\to 0}\tilde{B}(t)=\lim_{t\to 0}tB(1/t)=\lim_{\tau\to\infty}\frac{B(\tau)}{\tau}.\)\(\Box\)

**Theorem 15.2.4:** (_Kallianpur-Robbins_). _Let \(f:\mathbb{R}\to\mathbb{R}\) be integrable with respect to Lebesgue measure. Then_

\[\frac{1}{\sqrt{t}}\int_{0}^{t}f(B(u))du\longrightarrow^{d}\bigg{(}\int_{0}^{ \infty}f(u)du\bigg{)}Z \tag{2.16}\]

_where \(Z\) is a random variable with density \(\frac{\pi}{\sqrt{z(1-z)}}\) in [0,1]._

This is a special case of the Darling-Kac formula for Markov processes that can be established here using the regenerative property of SBM due to the fact that starting from 0, SBM will hit level 1 at same time \(T_{1}\) and from there hit level 0 at a later time \(\tau_{1}\). And this can be repeated to produce a sequence of times 0, \(\tau_{1}\), \(\tau_{2}\), \(\ldots\) such that the excursions \(\{B(t):\tau_{i}\leq t<\tau_{i+1}\}_{i\geq 1}\) are iid. The sequence \(\{\tau_{i}\}_{i\geq 1}\) is a renewal sequence with life time distribution \(\tau_{1}\) having a regularly varying tail of order 1/2 and hence infinite mean. One can appeal now to results from renewal theory to complete the proof (see Feller (1966) and Athreya (1986)).

#### Some sample path properties of SBM

The sample paths \(t\to B(t,\omega)\) of the SBM are continuous w.p. 1. It turns out that they are not any more smooth than this. For example, they are not differentiable nor are they of bounded variation on finite intervals. It will be shown now that w.p. 1 Brownian sample paths are not differentiable any where and the quadratic variation over any finite interval is finite and nonrandom. (See also Karatzas and Shreve (1991).)

1. _Nondifferentiability of \(B(\cdot,\omega)\) in [0,1]_ Let \(A_{n,k}=\Big{\{}\omega:\sup_{|t-s|\leq 3/n}\frac{|B(t,\omega)-B(s,\omega)|}{|t-s| }\leq k\) for some \(0\leq s\leq 1\Big{\}}\). Let \(Z_{r,n}=\big{|}B\big{(}\frac{(r+1)}{n}\big{)}-B\big{(}\frac{r}{n}\big{)}\big{|}\), \(r=0,1,2,n-1\). Let\(\max\big{(}Z_{r,n},Z_{r+1,n},Z_{r+2,n}\big{)}\leq\frac{6k}{n}\) for some \(r\big{\}}\). It can be verified that \(A_{n,k}\subset B_{n,k}\). Now \[P(B_{n,k}) \leq \sum_{r=0}^{n-1}P\Big{(}\max\big{(}Z_{r,n},Z_{r+1,n},Z_{r+2,n} \big{)}\leq\frac{6k}{n}\Big{)}\] \[\leq n\Big{(}P\Big{(}|Z_{0,n}|\leq\frac{6k}{n}\Big{)}\Big{)}^{3}\] \[\leq nP\Big{(}\frac{|Z_{0n}|}{\frac{1}{\sqrt{n}}}\leq\Big{(}\frac{6k} {\sqrt{n}}\Big{)}\Big{)}^{3}\] \[\leq n\Big{(}\frac{\text{Const}}{\sqrt{n}}\Big{)}^{3}\quad\text{as} \quad n\rightarrow\infty,\] since \(\frac{Z_{0n}}{\frac{1}{\sqrt{n}}}\sim N(0,1)\). Thus for each \(k<\infty\), \(P(A_{n,k})\leq\frac{\text{Const}}{\sqrt{n}}\). This implies \[\sum_{n=1}^{\infty}P(A_{n^{3},k})<\infty.\] So by the Borel-Cantelli lemma, only finitely many \(A_{n^{3},k}\) can happen w.p. 1. The event \(A\equiv\{\omega:B(t,\omega)\) is differentiable for at least one \(t\) in \([0,1]\}\) is contained in \(C\equiv\bigcup_{k\geq 1}\{\omega:\omega\in A_{n^{3},k}\) for infinitely many \(n\}\) and so \(P(A)\leq P(C)=0\).
2. _Finite quadratic variation of SBM_ Let \[\eta_{n,j} = B(j2^{-n})-B\big{(}(j-1)2^{-n}\big{)},\ j=1,2,\ldots,2^{n}\] \[\Delta_{n} \equiv \sum_{j=1}^{2^{n}}\eta_{nj}^{2}.\] (2.17) Then \[E\Delta_{n}=\sum_{j=1}^{2^{n}}\frac{1}{2^{n}}=1.\] Also by independence and stationarity of increments \[\text{Var}(\Delta_{n})=\sum_{j=1}^{2^{n}}\frac{3}{2^{2n}}=\frac{3}{2^{n}}.\] Thus \(P(|\Delta_{n}-1|>\epsilon)\leq\frac{\text{Var}(\Delta_{n})}{\epsilon^{2}}\) for any \(\epsilon>0\). This implies, by Borel Cantelli, \(\Delta_{n}\to 1\) w.p. 1. By definition the _quadratic variation_ is \[\Delta \equiv \sup\bigg{\{}\sum_{j=0}^{n}\big{|}B(t_{j},\omega)-B(t_{j-1}, \omega)\big{|}^{2}:\ \text{all finite partitions}\] (2.18) \[(t_{0},t_{1},\ldots,t_{n})\ \text{of}\ [0,1]\bigg{\}}.\]It is easy to verify that \(\Delta=\lim_{n}\Delta_{n}\). Thus \(\Delta=1\) w.p. 1. It follows that w.p. 1 the Brownian motion paths are not of bounded variation. By the scaling property of SBM, it follows that the quadratic variation of SBM over \([0,t]\) is \(t\) w.p. 1 for any \(t>0\).

#### Brownian motion and martingales

There are three natural martingales associated with Brownian motion.

**Theorem 15.2.5:**_Let \(\{B(t):t\geq 0\}\) be SBM. Then_

1. _(Linear martingale)_ \(\{B(t):t\geq 0\}\) _is a martingale._
2. _(Quadratic martingale)_ \(\{B^{2}(t)-t:t\geq 0\}\) _is a martingale._
3. _(Exponential martingale)_ _For any_ \(\theta\) _real,_ \(\{e^{\theta B(t)-\frac{\theta^{2}}{2}t}:t\geq 0\}\) _is a martingale._

**Proof:** (i) and (ii). Since \(B(t)\sim N(0,t)\),

\[E|B(t)|<\infty\quad\mbox{and}\quad E|B(t)|^{2}<\infty.\]

By the stationary independent increments property for any \(t\geq 0\), \(s\geq 0\),

\[E\big{(}B(t+s)\mid B(u):u\leq t\big{)}\] \[= E\big{(}\big{(}B(t+s)-B(t)\big{)}\mid B(u):u\leq t\big{)}+B(t)\] \[= 0+B(t)=B(t)\quad\mbox{establishing (i)}.\]

Next,

\[E\big{(}B^{2}(t+s)\mid B(u):u\leq t\big{)}\] \[= E\big{(}\big{(}B(t+s)-B(t)\big{)}^{2}\mid B(u):u\leq t\big{)}\] \[\quad+B^{2}(t)+2E\big{(}\big{(}B(t+s)-B(t)\big{)}B(t)\mid B(u):u \leq t\big{)}\] \[= s+B^{2}(t)+0\]

and hence

\[E\big{(}B^{2}(t+s)-(t+s)\mid B(u):u\leq t\big{)}=B^{2}(t)-t,\mbox{ establishing (ii)}.\]

(iii)

\[E\Big{(}e^{\theta B(t+s)-\frac{\theta^{2}}{2}(t+s)}\Bigm{|}B(u):u \leq t\Big{)}\] \[= E\Big{(}e^{\theta(B(t+s)-B(t))-\frac{\theta^{2}}{2}s}\Bigm{|}B(u ):u\leq t\Big{)}\ e^{\theta B(t)-\frac{\theta^{2}}{2}t}.\]

Again by using the fact that \(B(t+s)-B(t)\) given \(\big{(}B(u):u\leq t\big{)}\) is \(N(0,s)\), the first term on the right side becomes 1 proving (iii). \(\Box\)

#### Some applications

The martingales in Theorem 15.2.5 combined with the optional stopping theorems of Chapter 13 yield the following applications.

* _Exit probabilities_ Let \(B(\cdot)\) be SBM. Fix \(a<0<b\). Let \(T_{a,b}=\min\{t:t>0,B(t)=a\quad\text{or}\quad b\}\). From (i) and the optional sampling theorem, for any \(t>0\); \[EB(T_{a,b}\wedge t)=EB(0)=0.\] (2.19) Also, by continuity, \(B(T_{a,b}\wedge t)\to B(T_{a,b})\). By bounded convergence theorem, this implies \[EB(T_{a,b})=0\] (2.20) i.e., a \(p+b(1-p)=0\) where \(p=P(T_{a}<T_{b})=P(B(\cdot)\) reaches \(a\) before \(b\)). Thus, \(p=\frac{b}{(b-a)}\).
* _Mean exit time_ From (ii) and the optional sampling theorem \[E\big{(}B^{2}(T_{a,b}\wedge t)-(T_{a,b}\wedge t)\big{)}=0\] i.e., \[EB^{2}(T_{a,b}\wedge t)=E(T_{a,b}\wedge t).\] (2.21) By using the bounded convergence theorem on the left and the monotone convergence theorem on the right, one may conclude \[EB^{2}(T_{a,b})=ET_{a,b}\] i.e., \[ET_{a,b} = pa^{2}+(1-p)b^{2}\] (2.22) \[= a^{2}\frac{b}{(b-a)}+b^{2}\frac{(-a)}{(b-a)}\] \[= (-ab).\]
* _The distribution of \(T_{a,b}\)_ From (iii) and the optimal sampling theorem \[E\Big{(}e^{\theta B(T_{a,b}\wedge t)-\frac{\theta^{2}}{2}T_{a,b}}\Big{)}=1.\] By the bounded convergence theorem, this implies \[E\Big{(}e^{\theta B(T_{a,b})-\frac{\theta^{2}}{2}T_{a,b}}\Big{)}=1.\] (2.23)In particular, if \(b=-a\) this reduces to

\[1 = E\Big{(}e^{\theta a-\frac{\theta^{2}}{2}T_{a,-a}}:T_{a}<T_{-a}\Big{)} +E\Big{(}e^{-\theta a}e^{-\frac{\theta^{2}}{2}T_{a,-a}}:T_{a}>T_{-a}\Big{)}\] \[= \big{(}e^{\theta a}+e^{-\theta a}\big{)}\frac{1}{2}E\Big{(}e^{- \frac{\theta^{2}}{2}T_{a,-a}}\Big{)},\] since by symmetry \[E\Big{(}e^{-\frac{\theta^{2}}{2}T_{a,-a}}:T_{a}<T_{-a}\Big{)}=E\Big{(}e^{- \frac{\theta^{2}}{2}T_{a,-a}}:T_{a}>T_{-a}\Big{)}.\] Thus, for \(\lambda\geq 0\) \[E\Big{(}e^{-\lambda T_{a,-a}}\Big{)}=2\Big{(}e^{\sqrt{2\lambda}a}+e^{-\sqrt{2 \lambda}a}\Big{)}^{-1}.\] (2.24) Similarly, it can be shown that for \(\lambda\geq 0\), \(a>0\) \[E\big{(}e^{-\lambda T_{a}}\big{)}=e^{-\sqrt{2\lambda}a}. \tag{2.25}\]

#### The Black-Scholes formula for stock price option

Let \(X(t)\) denote the price of one unit of a stock \(S\) at time \(t\). Due to fluctuations in the market place, it is natural to postulate that \(\{X(t):t\geq 0\}\) is a stochastic process. To build an appropriate model consider the discrete time case first. If \(X_{n}\) denotes the unit price at time \(n\), it is natural to postulate that \(X_{n+1}=X_{n}y_{n+1}\) where \(y_{n+1}\) represents the effects of the market fluctuation in the time interval \([n,n+1)\). This leads to the formula \(X_{n}=X_{0}y_{1}y_{2}\cdots y_{n}\). If one assumes that \(\{y_{n}\}_{n\geq 1}\) are sufficiently independent, then

\[X_{n}=X_{0}\,e^{n\mu+\sum\limits_{i=1}^{n}(\log y_{i}-\mu)}\]

is, by the central limit theorem, approximately Gaussian, leading one to consider a model of the form

\[X(t)=X(0)e^{\mu t+\sigma B(t)} \tag{2.26}\]

where \(\{B(t):t\geq 0\}\) is SBM. Thus, \(\{\log X(t)-\log X(0):t\geq 0\}\) is postulated to be a Brownian motion with drift \(\mu\) and diffusion \(\sigma\). In the language of finance, \(\mu\) is called the _growth rate_ and \(\sigma\) the _volatility rate_.

The so-called European option allows one to buy the stock at a future time \(t_{0}\) for a unit price of \(K\) dollars at time \(0\). If \(X(t_{0})<K\) then one has the option of not buying, whereas if \(X(t_{0})\geq K\), then one can buy it at \(K\) dollars and sell it immediately at the market price \(X(t_{0})\) and realize a profit of \(X(t_{0})-K\). Thus the net revenue from this option is

\[\tilde{X}(t_{0})=\left\{\begin{array}{rcl}0&\mbox{if}&X(t_{0})\leq K\\ X(t_{0})-K&\mbox{if}&X(t_{0})>K.\end{array}\right. \tag{2.27}\]Since the value of money depreciates over time, say at rate \(r\), the net revenue's value at time \(0\) is \(\tilde{X}(t_{0})e^{-t_{0}r}\). So a fair price for this European option is

\[p_{0} = E\tilde{X}(t_{0})e^{-t_{0}r} \tag{2.28}\] \[= E(X(t_{0})-K)^{+}e^{-t_{0}r}.\]

Here the constants \(\mu\), \(\sigma\), \(K\), \(t_{0}\), \(r\) are assumed known. The goal is to compute \(p_{0}\). This becomes feasible if one makes the natural assumption of no _arbitrage_. That is, the discounted value of the stock, i.e., \(X(t)e^{-rt}\), evolves as a martingale. This is a reasonable assumption as otherwise (if it is advantageous) then everybody will want to take advantage of it and start buying the stock, thereby driving the price down and making it unprofitable.

Thus, in effect, this assumption says that

\[X(t)e^{-rt}\equiv X(0)e^{\mu t+\sigma B(t)-rt}\]

evolves as a martingale. But recall that if \(B(\cdot)\) is an SBM then for any \(\theta\) real, \(e^{\theta B(t)-\frac{\theta^{2}}{2}t}\) evolves as a martingale. Thus, \(\mu\), \(\sigma\), \(r\) should satisfy the condition \(-\frac{\sigma^{2}}{2}=(\mu-r)\). With this assumption, the fair price for this European option with \(\mu\), \(\sigma\), \(r\), \(K\), \(t_{0}\) given is

\[p_{0} = E\bigl{(}e^{-t_{0}r}\bigl{(}X_{0}\,e^{\sigma B(t_{0})+\mu t_{0}} -K\bigr{)}^{+}\bigr{)} \tag{2.29}\] \[= \frac{1}{\sqrt{2\pi t_{0}}}\,e^{-t_{0}r}\int\limits_{X_{0}\,e^{ \sigma y+\mu t_{0}}>K}\bigl{(}X_{0}\,e^{\sigma y+\mu t_{0}}-K\bigr{)}e^{-\frac {y^{2}}{2t_{0}}}dy.\]

This is known as the _Black-Scholes formula_.

For more detailed discussions on Brownian motion including the development of Ito stochastic integration and diffusion processes via a martingale formulation, the books of Stroock and Varadhan (1979) and Karlin and Taylor (1975) should be consulted. See also Karatzas and Shreve (1991).

### Problems

15.1: Let \(\{L_{j}\}_{j\geq 0}\) be as in Section 15.1.1. Show that for any \(\theta\geq 0\) 1. \(E\Bigl{(}e^{-\theta\sum_{j=0}^{n}L_{j}}\Bigr{)}=E\biggl{(}\prod\limits_{j=0}^{ n}\frac{\lambda_{y_{j}}}{\theta+\lambda_{y_{j}}}\biggr{)}\) 2. \(E\Bigl{(}e^{-\theta\sum_{j=0}^{\infty}L_{j}}\Bigr{)}=0\) for all \(\theta>0\) iff \(\sum_{j=0}^{\infty}\frac{1}{\lambda_{y_{j}}}=\infty\) w.p. 1 assuming \(0<\lambda_{i}<\infty\) for all \(i\).

15.2 Let \(L\) be an exponential random variable. Verify that for any \(x>0\), \(u>0\) \[P(L>x+u\mid L>x)=P(L>u).\] (This is referred to as the "lack of memory" property.)
15.3 Solve the Kolmogorov's forward and backward equations for the following special cases of birth and death processes: 1. _Poisson process_: \(\alpha_{i}\equiv\alpha\), \(\beta_{i}\equiv 0\), 2. _Yule process_: \(\alpha_{i}\equiv i\alpha\), \(\beta_{i}\equiv 0\), 3. _On-off process_: \(\alpha_{0}=\alpha\), \(\alpha_{i}=0\), \(i\geq 1\), \(\beta_{1}=\beta\), \(\beta_{i}=0\), \(i=0,2,\ldots\), 4. _M/M/1 queue_: \(\alpha_{i}=\alpha\), \(i\geq 0\), \(\beta_{i}=\beta\), \(i\geq 1\), \(\beta_{0}=0\), 5. _M/M/s queue_: \(\alpha_{i}=\alpha\), \(i\geq 0\), \(\beta_{i}=i\beta\), \(1\leq i\leq s\) and \(=s\beta\), \(i>s\), \(\beta_{0}=0\), 6. _Pure death process_: \(\beta_{i}\equiv\beta\), \(i\geq 1\), \(\beta_{0}=0\), \(\alpha_{i}=0\), \(i\geq 0\).
15.4 Find the stationary distributions when they exist for the processes in Problem 15.3.
15.5 Consider 2 independent M/M/1 queues with arrival rate \(\lambda\), service rate \(\mu\) (Case I), and one M/M/1 queue with arrival rate \(2\lambda\) and service rate \(2\mu\) (Case II). Assume \(\lambda<\mu\). Show that in the stationary state the mean number in the system Case I is larger than in Case 2 and their ratio approaches 2 as \(\rho=\frac{\lambda}{\mu}\uparrow 1\).
15.6 Show that for any finite state space irreducible CTMC \(\{X(t):t\geq 0\}\) with all \(\lambda_{i}\in(0,\infty)\), there is a unique stationary distribution.
15.7 (_M/M/\(\infty\) queue_). This is a birth and death process such that \(\alpha_{n}\equiv\alpha\), \(\beta_{n}=n\beta\), \(n\geq 0\), \(0<\alpha,\beta<\infty\). Show that this process has a stationary distribution that is Poisson with mean \(\rho=\frac{\lambda}{\mu}\).
15.8 1. Let \(\{X(t)\}_{t\geq 0}\) be a Poisson process with rate \(\lambda\). Let \(L\) be an exponential random variable with mean \(\mu^{-1}\) and independent of \(\{X(t)\}_{t\geq 0}\). Let \(N(t)=X(t+L)-X(t)\). Find the distribution of \(N(t)\). 2. Let \(\{Y(t)\}_{t\geq 0}\) be also a Poisson process with rate \(\mu\) and independent of \(\{X(t)\}_{t\geq 0}\) in (a). Let \(T\) and \(T^{\prime}\) be two successive 'event epochs' for the \(\{Y(t)\}_{t\geq 0}\) process. Let \(N=X(T^{\prime})-X(T)\). Find the distribution of \(N\). 3. Let \(\{X(t)\}_{t\geq 0}\) be as in (a). Let \(\tau_{0}=0<\tau_{1}<\tau_{2}<\cdots\) be the successive event epochs of \(\{X(t)\}_{t\geq 0}\). Find the joint distribution of \((\tau_{1},\tau_{2},\ldots,\tau_{n})\) conditioned on the event \(\{N(t)=1\}\) for some \(0<t<\infty\).

15.9 Let \(\{X(t):t\geq 0\}\) be a Poisson process with rate \(\lambda\). Suppose at each event epoch of the Poisson process an experiment is performed that results in one of \(k\) possible outcomes \(\{a_{i}:1\leq i\leq k\}\) with probability distribution \(\{p_{i}:1\leq i\leq k\}\). Let \(X_{i}(t)=\) outcomes \(a_{i}\) in \([0,t]\). Assume the experiments are iid. Show that \(\{X_{i}(t):t\geq 0\}\) are independent Poisson processes with rate \(\lambda p_{i}\) for \(1\leq i\leq k\).
15.10 Let \(\{X(t):t\geq 0\}\) be a Poisson process with rate \(\lambda\), \(0<\lambda<\infty\). Let \(\{\xi_{i}\}_{i\geq 1}\) be a sequence of iid random variables independent of \(\{X(t):t\geq 0\}\) with values in a measurable space \((S,\mathcal{S})\). For each \(A\in\mathcal{S}\) define \[N(A,t)\equiv\sum_{j=1}^{N(t)}I(\xi_{j}\in A),\ \ t\geq 0.\] 1. 2. Verify that for each \(A\in\mathcal{S}\), \(\{N(A,t)\}_{t\geq 0}\) is a Poisson process and find its rate. 3. Show that if \(A_{1},A_{2}\in\mathcal{S}\), \(A_{1}\cap A_{2}=\mathcal{S}\), then the two Poisson processes \(\{N(A_{i},t)\}_{t\geq 0}\), \(i=1,2\) are independent. 4. Show that for each \(t>0\), \(\{N(A,t):A\in\mathcal{S}\}\) is a Poisson random field on \(S\), i.e., for each \(A\), \(N(A,t)\) is Poisson and for \(A_{1},A_{2},\ldots,A_{k}\) pairwise disjoint elements of \(\mathcal{S}\), \(\{N(A_{i},t)\}_{1}^{k}\) are independent. 5. Show that \(\{N(\cdot,t)\}_{t\geq 0}\) is a process with stationary independent increments that is Poisson random measure valued.
15.11 Let \(B_{N}(\cdot,\omega)\) be as in (2.1). Show that \(\{B_{N}(\cdot,\omega)\}_{N\geq 1}\) is Cauchy in the Banach space \(\mathcal{C}[0,1]\) with sup norm by completing the following steps: 1. If \(\xi_{nj}(t,\omega)=Z_{nj}(\omega)S_{nj}(t)\) then 1. \(\|\xi_{nj}(\cdot,\omega)\|\equiv\sup\{|\xi_{nj}(t,\omega)|:0\leq t\leq 1\}=|Z_{nj} (\omega)|2^{-\frac{(n+1)}{2}}\) 2. \(\sup\bigg{\{}\sum_{j=1}^{2^{n}-1}|\xi_{nj}(t,\omega):0\leq t\leq 1\bigg{\}}\) \[=(\max\{|Z_{nj}(\omega)|:1\leq j\leq 2^{n}-1\})2^{-\frac{(n+1)}{2}},\] 2. for any sequence \(\{\eta_{i}\}_{i\geq 1}\) of random variables with \(\sup_{i}E(e^{\eta_{i}})<\infty\), w.p. 1, \(\eta_{i}\leq 2\log i\) for all large \(i\), 3. w.p. 1 there is a \(C<\infty\) such that \[\max\{|Z_{nj}(\omega)|:1\leq j\leq 2^{n}-1\}\leq Cn.\]

[MISSING_PAGE_EMPTY:516]

Limit Theorems for Dependent Processes

### 16.1 A central limit theorem for martingales

Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables on \((\Omega,{\cal F},P)\), and let \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration, i.e., a sequence of \(\sigma\)-algebras on \(\Omega\) such that \({\cal F}_{n}\subset{\cal F}_{n+1}\subset{\cal F}\) for all \(n\geq 1\). From Chapter 13, recall that \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\) is called a _martingale_ if \(X_{n}\) is \({\cal F}_{n}\)-measurable for each \(n\geq 1\) and \(E(X_{n+1}\mid{\cal F}_{n})=X_{n}\) for each \(n\geq 1\).

Given a martingale \(\{X_{n},{\cal F}_{n}\}_{n\geq 1}\), define

\[Y_{1} = X_{1}-EX_{1},\] \[Y_{n} = X_{n}-X_{n-1},\ n\geq 1.\]

Note that each \(Y_{n}\) is \({\cal F}_{n}\)-measurable and

\[E(Y_{n}\mid{\cal F}_{n-1})=0\quad\mbox{for all}\quad n\geq 1, \tag{1.1}\]

where \({\cal F}_{0}=\{\Omega,\emptyset\}\).

**Definition 16.1.1:** Let \(\{Y_{n}\}_{n\geq 1}\) be a collection of random variables on a probability space \((\Omega,{\cal F},P)\) and let \(\{{\cal F}_{n}\}_{n\geq 1}\) be a filtration. Then, \(\{Y_{n},{\cal F}_{n}\}_{n\geq 1}\) is called a _martingale difference array (mda)_ if \(Y_{n}\) is \({\cal F}_{n}\)-measurable for each \(n\geq 1\) and (1.1) holds.

For example, if \(\{Y_{n}\}_{n\geq 1}\) is a sequence of zero mean independent random variables, then \(\{Y_{n},{\cal F}_{n}\}_{n\geq 1}\) is a mda w.r.t. the natural filtration \({\cal F}_{n}=\sigma\langle Y_{1},\ldots,Y_{n}\rangle\), \(n\geq 1\). Other examples of mda's can be constructed from the examples given in Chapter 13. The main result of this section shows that for square-integrable mda's satisfying a Lindeberg-type condition, the CLT holds. For more on limit theorems for mdas, see Hall and Heyde (1980).

**Theorem 16.1.1:**_For each \(n\geq 1\), let \(\{Y_{ni},{\cal F}_{ni}\}_{i\geq 1}\) be a mda on \((\Omega,{\cal F},P)\) with \(EY_{ni}^{2}<\infty\) for all \(i\geq 1\) and let \(\tau_{n}\) be a finite stopping time w.r.t. \(\{{\cal F}_{ni}\}_{i\geq 1}\). Suppose that for some constant \(\sigma^{2}\in(0,\infty)\),_

\[\sum_{i=1}^{\tau_{n}}E\Big{(}Y_{ni}^{2}\ \big{|}\ {\cal F}_{n,i-1}\Big{)} \longrightarrow_{p}\sigma^{2}\quad\mbox{as}\quad n\to\infty \tag{1.2}\]

_and that for each \(\epsilon>0\),_

\[\Delta_{n}(\epsilon)\equiv\sum_{i=1}^{\tau_{n}}E\Big{(}Y_{ni}^{2}I(|Y_{ni}|> \epsilon)\ \big{|}\ {\cal F}_{n,i-1}\Big{)}\longrightarrow_{p}0\quad\mbox{as} \quad n\to\infty. \tag{1.3}\]

_Then,_

\[\sum_{i=1}^{\tau_{n}}Y_{ni}\longrightarrow^{d}N(0,\sigma^{2}). \tag{1.4}\]

**Proof:** First the theorem will be proved under the additional condition that

\[\tau_{n} = m_{n}\ \mbox{for all}\ n\geq 1\ \mbox{for some}\ \mbox{ nonrandom}\ \mbox{sequence of} \tag{1.5}\] \[\mbox{positive integers}\ \{m_{n}\}_{n\geq 1}\]

and that for some \(c\in(0,\infty)\),

\[\sum_{i=1}^{m_{n}}E\Big{(}Y_{ni}^{2}\ \big{|}\ {\cal F}_{n,i-1}\Big{)}\leq c \quad\mbox{w.p.}\ 1. \tag{1.6}\]

Let \(\sigma_{ni}^{2}=E\big{(}Y_{ni}^{2}\ |\ {\cal F}_{n,i-1}\big{)}\), \(i\geq 1\), \(n\geq 1\). Also, write \(m\) for \(m_{n}\) to ease the notation. Since \(\sigma_{ni}^{2}\) is \({\cal F}_{n,i-1}\)-measurable, for any \(t\in\mathbb{R}\),

\[\left|E\exp\left(\iota t\sum_{j=1}^{m}Y_{nj}\right)-\exp\big{(}- \sigma^{2}t^{2}/2\big{)}\right|\] \[\leq \left|E\exp\left(\iota t\sum_{j=1}^{m}Y_{nj}\right)-E\exp\left( \iota t\sum_{j=1}^{m-1}Y_{nj}\right)\exp\big{(}-t^{2}\sigma_{nm}^{2}/2\big{)}\right|\] \[+\cdots+\left|E\exp\big{(}\iota tY_{n1}\big{)}\exp\bigg{(}-\sum_{ j=2}^{m}t^{2}\sigma_{nj}^{2}/2\bigg{)}\right.\] \[\left.-\left[\exp\bigg{(}-\sum_{j=1}^{m}t^{2}\sigma_{nj}^{2}/2 \bigg{)}\right]\right|\]\[\left.\begin{array}{rcl}&&+\left|E\exp\bigg{(}-\sum_{j=1}^{m}t^{2} \sigma_{nj}^{2}/2\bigg{)}-\exp(-t^{2}\sigma^{2}/2)\right|\\ \leq&\sum_{k=1}^{m}E\Big{|}E\Big{\{}\exp(\iota tY_{nk})\ \big{|}\ {\cal F}_{n,k-1} \Big{\}}-\exp(-t^{2}\sigma_{nk}^{2}/2)\Big{|}\\ &&+\left|E\exp\bigg{(}-\sum_{j=1}^{m}t^{2}\sigma_{nj}^{2}/2\bigg{)}-\exp(-t^{2 }\sigma^{2}/2)\right|\\ \equiv&I_{1n}+I_{2n},\quad\mbox{say}.\end{array}\right. \tag{1.7}\]

By (1.2), (1.5), and the BCT,

\[I_{2n}\to 0\quad\mbox{as}\quad n\to\infty. \tag{1.8}\]

To estimate \(I_{1n}\), note that for any \(1\leq k\leq n\),

\[E\Big{\{}\exp(\iota tY_{nk})\ \big{|}\ {\cal F}_{n,k-1}\Big{\}} \tag{1.9}\] \[= 1+\iota tE\big{(}Y_{nk}\ |\ {\cal F}_{n,k-1}\big{)}+\frac{( \iota t)^{2}}{2}\,E\big{(}Y_{nk}^{2}\ |\ {\cal F}_{n,k-1}\big{)}+\theta_{nk}(t)\] \[= 1-\frac{t^{2}}{2}\,\sigma_{nk}^{2}+\theta_{nk}(t)\]

and

\[\exp\big{(}-t^{2}\sigma_{nk}^{2}/2\big{)}=1-\frac{t^{2}}{2}\,\sigma_{nk}^{2}+ \gamma_{nk},\quad\mbox{say}. \tag{1.10}\]

It is easy to verify that

\[|\theta_{nk}|\leq E\bigg{[}\min\bigg{\{}(tY_{nk})^{2},\,\frac{|tY_{nk}|^{3}}{6} \bigg{\}}\ \Big{|}\ {\cal F}_{n,k-1}\bigg{]}\]

and

\[|\gamma_{nk}|\leq\big{(}t^{2}\sigma_{nk}^{2}\big{)}^{2}\exp\big{(}t^{2}\sigma_ {nk}^{2}/2\big{)}/8.\]

Hence, by (1.3), (1.6), (1.9), and (1.10), for any \(\epsilon\) in (0,1),

\[I_{1n} \leq \sum_{k=1}^{m}E\Big{\{}|\theta_{nk}|+|\gamma_{nk}|\Big{\}}\] \[\leq\] \[+\ |t|^{3}\epsilon\cdot\sum_{k=1}^{m}E\Big{|}E\big{(}Y_{nk}^{2}\ |\ {\cal F}_{n,k-1}\big{)}\Big{|}+\sum_{k=1}^{m}E\Big{\{}t^{4}\sigma_{nk}^{4}\exp(t^{2}c/2)\Big{\}}\] \[\leq\] \[+\ {t^{4}}\,\exp(t^{2}c/2)\,E\bigg{[}\bigg{(}\sum_{k=1}^{m}\sigma_{nk}^{2} \bigg{)}\Big{\{}\max_{l\leq k\leq m}\sigma_{nk}^{2}\Big{\}}\bigg{]}.\]Note that for any \(\epsilon>0\),

\[E\max_{1\leq k\leq m}\sigma_{nk}^{2} \leq \epsilon^{2}+E\Big{[}\max_{1\leq k\leq m}E\Big{\{}Y_{nk}^{2}I\big{(} \big{|}Y_{nk}|>\epsilon\big{)}\ \big{|}\ {\cal F}_{n,k-1}\Big{\}}\Big{]}\] \[\leq \epsilon^{2}+E\Delta_{n}(\epsilon).\]

Hence, by (1.3), (1.6), and the BCT, for any \(\epsilon\in(0,1)\),

\[\limsup_{n\to\infty}I_{1n} \leq \limsup_{n\to\infty}\Big{[}t^{2}\,E\Delta_{n}(\epsilon)+|t|^{3} \epsilon\cdot c\] \[+\,t^{4}ce^{t^{2}c/2}\Big{\{}\epsilon^{2}+E\Delta_{n}(\epsilon) \Big{\}}\Big{]}\] \[\leq c_{1}(t)\,\epsilon\]

for some \(c_{1}(t)\in(0,\infty)\), not depending on \(\epsilon\). Thus implies that

\[\lim_{n\to\infty}I_{1n}=0. \tag{1.11}\]

Clearly (1.7), (1.8), and (1.11) yield (1.4), whenever (1.5) and (1.6) are true.

Next, suppose that condition (1.6) is not assumed _a priori_ but (1.5) holds true. Fix \(c>\sigma^{2}\) and define the sets \(B_{nk}=\big{\{}\sum_{i=1}^{k}\sigma_{ni}^{2}\leq c\big{\}}\), and the variables \(\check{Y}_{nk}=Y_{nk}I_{B_{nk}}\), \(k\geq 1\), \(n\geq 1\). Note that \(B_{nk}\in{\cal F}_{n,k-1}\) and hence,

\[E\big{(}\check{Y}_{nk}\mid{\cal F}_{n,k-1}\big{)}=I_{B_{nk}}E\big{(}Y_{nk}\mid{ \cal F}_{n,k-1}\big{)}=0,\]

and

\[\check{\sigma}_{nk}^{2}\equiv E\big{(}\check{Y}_{nk}^{2}\mid{\cal F}_{n,k-1} \big{)}=I_{B_{nk}}\,\sigma_{nk}^{2}, \tag{1.12}\]

for all \(k\geq 1\). In particular, \(\big{\{}\check{Y}_{nk},{\cal F}_{n,k}\big{\}}\) is a mda. Since \(B_{n,k-1}\supset B_{nk}\) for all \(k\), by the definitions of the sets \(B_{nk}\)'s, it follows that

\[\sum_{k=1}^{m}\check{\sigma}_{nk}^{2} = \sum_{k=1}^{m}\sigma_{nk}^{2}\,I_{B_{nk}} \tag{1.13}\] \[= \sum_{k=1}^{m}\sigma_{nk}^{2}\,I_{B_{nm}}+\sum_{k=1}^{m-1}\sigma_ {nk}^{2}\big{(}I_{B_{n,m-1}}-I_{B_{nm}}\big{)}\] \[+\cdots+\sigma_{n1}^{2}\big{(}I_{B_{n1}}-I_{B_{n2}}\big{)}\] \[\leq cI_{B_{nm}}+c\big{(}I_{B_{n,m-1}}-I_{B_{nm}}\big{)}+\cdots+ \big{(}cI_{B_{n1}}-I_{B_{n2}}\big{)}\] \[\leq c.\]

Thus, the mda \(\big{\{}\check{Y}_{nk},{\cal F}_{nk}\big{\}}_{k\geq 1}\) satisfies (1.6). Next note that by (1.2) and (1.5),

\[P\big{(}B_{nm}^{c}\big{)}\to 0\quad\mbox{as}\quad n\to\infty. \tag{1.14}\]Also, by (1.12), \(\sum_{k=1}^{m}\check{\sigma}_{nk}^{2}=\sum_{k=1}^{m}\sigma_{nk}^{2}\) on \(B_{nm}\). Hence, it follows that

\[\sum_{k=1}^{m}\check{\sigma}_{nk}^{2}\longrightarrow_{p}\sigma^{2},\]

i.e., the mda \(\big{\{}\check{Y}_{nk},\mathcal{F}_{nk}\big{\}}\) satisfies (1.2). Further, the inequality "\(|\check{Y}_{nk}|\leq|Y_{nk}|\)" and the fact that the function \(h(x)=x^{2}I(|x|>\epsilon)\), \(x>0\) is nondecreasing jointly imply that (1.3) holds for \(\big{\{}\check{Y}_{nk},\mathcal{F}_{nk}\big{\}}\). Hence, by the case already proved,

\[\sum_{k=1}^{m}\check{Y}_{nk}\longrightarrow^{d}N(0,\sigma^{2}). \tag{1.15}\]

But \(\sum_{k=1}^{m}\check{Y}_{nk}=\sum_{k=1}^{m}Y_{nk}\) on \(B_{nm}\). Hence, by (1.14),

\[\sum_{k=1}^{m}Y_{nk}\longrightarrow^{d}N(0,\sigma^{2}), \tag{1.16}\]

and therefore, the CLT holds without the restriction in (1.6).

Next consider relaxing the restrictions in (1.5) (and (1.6)). Since \(P(\tau_{n}<\infty)=1\), there exist positive integers \(m_{n}\) such (Problem 16.2) that

\[P\big{(}\tau_{n}>m_{n}\big{)}\to 0\quad\mbox{as}\quad n\to\infty. \tag{1.17}\]

Next define

\[\tilde{Y}_{nk}=Y_{nk}\,I(\tau_{n}\geq k),\ k\geq 1,\ n\geq 1. \tag{1.18}\]

It is easy to check (Problem 16.3) that \(\{\tilde{Y}_{nk},\mathcal{F}_{nk}\}\) is a mda, and that \(\{\tilde{Y}_{nk},\mathcal{F}_{nk}\}\) satisfies (1.2) and (1.3) with \(\tau_{n}\) replaced by \(m_{n}\) (Problem 16.4). Hence, by the previous case already proved,

\[\sum_{k=1}^{m_{n}}\tilde{Y}_{nk}\longrightarrow^{d}N(0,\sigma^{2}).\]

Next note that (cf. (4.1), Proclem 16.4),

\[\sum_{k=1}^{\tau_{n}}Y_{nk}-\sum_{k=1}^{m_{n}}\tilde{Y}_{nk}\longrightarrow_{ p}0\quad\mbox{as}\quad n\to\infty. \tag{1.19}\]

Hence, (1.4) holds and the proof of the theorem is complete. \(\Box\)

### 16.2 Mixing sequences

This section deals with a class of dependent processes, called the _mixing processes_, where the degree of dependence decreases as the distance (intime) between two given sets of random variables goes to infinity. The 'degree of dependence' is measured by various _mixing coefficients_, which are defined in Section 16.2.1 below. Some basic properties of the mixing coefficients are presented in Section 16.2.2. Limit theorems for sums of mixing random variables are given in Section 16.2.3.

#### Mixing coefficients

**Definition 16.2.1:** Let \((\Omega,\mathcal{F},P)\) be a probability space and \(\mathcal{G}_{1}\), \(\mathcal{G}_{2}\) be sub-\(\sigma\)-algebras of \(\mathcal{F}\).

1. The _\(\alpha\)-mixing_ or _strong mixing_ coefficient between \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) is defined as \[\alpha(\mathcal{G}_{1},\mathcal{G}_{2})\equiv\sup\big{\{}\big{|}P(A\cap B)-P(A )P(B)\big{|}:A\in\mathcal{G}_{1},B\in\mathcal{G}_{2}\big{\}}.\] (2.1)
2. The _\(\beta\)-mixing coefficient_ or the _coefficient of absolute regularity_ between \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) is defined as \[\beta(\mathcal{G}_{1},\mathcal{G}_{2})\equiv\frac{1}{2}\,\sup\sum_{i=1}^{k} \sum_{j=1}^{\ell}\big{|}P(A_{i}\cap B_{j})-P(A_{i})P(B_{j})\big{|},\] (2.2) where the supremum is taken over all finite partitions \(\{A_{1},\ldots,A_{k}\}\) and \(\{B_{1},\ldots,B_{\ell}\}\) of \(\Omega\) by sets \(A_{i}\in\mathcal{G}_{1}\) and \(B_{j}\in\mathcal{G}_{2}\), \(1\leq i\leq k\), \(1\leq j\leq\ell\), \(\ell\), \(k\in\mathbb{N}\).
3. The _\(\rho\)-mixing coefficient_ or _the coefficient of maximal correlation_ between \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) is defined as \[\rho(\mathcal{G}_{1},\mathcal{G}_{2})\equiv\sup\big{\{}\rho_{X_{1},X_{2}}:X_{i }\in L^{2}(\Omega,\mathcal{G}_{i},P),\;i=1,2\big{\}}\] (2.3) where \(\rho_{X_{1},X_{2}}\equiv\frac{\text{Cov}(X_{1},X_{2})}{\sqrt{\text{Var}(X_{1 })\text{Var}(X_{2})}}\) is the correlation coefficient of \(X_{1}\) and \(X_{2}\).

It is easy to check (Problem 16.5 (a) and (d)) that all three mixing coefficients take values in the interval \([0,1]\) and that \(\rho(\mathcal{G}_{1},\mathcal{G}_{2})=\sup\big{\{}|EX_{1}X_{2}|:X_{i}\in L^{2} (\Omega,\mathcal{G}_{i},P)EX_{i}=0,\,EX_{1}^{2}=1,\,i=1,2\big{\}}\). When the \(\sigma\)-algebras \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are independent, these coefficients equal zero, and vice versa. Thus, nonzero values of the mixing coefficients give various measures of the degree of dependence between \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\). It is easy to check (Problem 16.5 (c)) that

\[\alpha(\mathcal{G}_{1},\mathcal{G}_{2})\leq\beta(\mathcal{G}_{1},\mathcal{G}_{2 })\quad\text{and}\quad\alpha(\mathcal{G}_{1},\mathcal{G}_{2})\leq\rho( \mathcal{G}_{1},\mathcal{G}_{2}). \tag{2.4}\]

However, no ordering between the \(\beta(\mathcal{G}_{1},\mathcal{G}_{2})\) and \(\rho(\mathcal{G}_{1},\mathcal{G}_{2})\) exists, in general (Problem 16.6). There are two other mixing coefficients that are also often used in the literature. These are given by the _\(\phi\)-mixing coefficient_

\[\phi(\mathcal{G}_{1},\mathcal{G}_{2})\equiv\sup\big{\{}\big{|}P(A)-P(A\mid B )\big{|}:B\in\mathcal{G}_{1},P(B)>0,A\in\mathcal{G}_{2}\big{\}}, \tag{2.5}\]

[MISSING_PAGE_FAIL:523]

where \(a_{i}\in\mathbb{R}\) and \(a_{i}=0\big{(}\exp(-c_{i})\big{)}\) as \(i\to\infty\), \(c\in(0,\infty)\). If \(\epsilon_{1}\) has an integrable characteristic function, then \(\{X_{i}\}_{i\in\mathbb{Z}}\) is strongly mixing (Chanda (1974), Gorodetskii (1977), Withers (1981), Athreya and Pantula (1986)).

**Example 16.2.3:** Let \(\{X_{i}\}_{i\in\mathbb{Z}}\) be a zero mean stationary Gaussian process. Suppose that \(\{X_{i}\}_{i\in\mathbb{Z}}\) has spectral density \(f:(-\pi,\pi)\to[0,\infty)\), i.e.,

\[EX_{0}X_{k}=\int_{-\pi}^{\pi}e^{\iota kx}f(x)dx,\ \ k\in\mathbb{Z}. \tag{2.10}\]

Then, \(\alpha_{X}(n)\leq\rho_{X}(n)\leq 2\pi\alpha(n)\), \(n\geq 1\) and, therefore, \(\{X_{i}\}_{i\in\mathbb{Z}}\) is \(\alpha\)-mixing iff it is \(\rho\)-mixing (Ibragimov and Rozanov (1978), Chapter 4). Further, \(\{X_{i}\}_{i\in\mathbb{Z}}\) is \(\alpha\)-mixing iff the spectral density \(f\) admits the representation

\[f(t)=\big{|}p(e^{\iota t})\big{|}^{2}\,\exp\big{(}u(e^{\iota t})+\tilde{v}(e^{ \iota t})\big{)}, \tag{2.11}\]

where \(p(\cdot)\) is a polynomial, \(u\) and \(v\) are continuous real-valued functions on the unit circle in the complex plane, and \(\tilde{v}\) is the conjugate function of \(u\). It is also known that if the Gaussian process \(\{X_{i}\}_{i\in\mathbb{Z}}\) is \(\phi\)-mixing, then it is necessarily \(m\)-dependent for some \(m\in\mathbb{Z}_{+}\). Thus, for Gaussian processes, the condition of \(\alpha\)-mixing is as strong as \(\rho\)-mixing and the conditions of \(\phi\)-mixing and \(\Psi\)-mixing are equivalent to \(m\)-dependence. See Ibragimov and Rozanov (1978) for more details.

#### Coupling and covariance inequalities

The mixing coefficients can be seen as measures of deviations from independence. The idea of coupling is to construct independent copies of a given pair of random vectors on a suitable probability space such that the Euclidean distance between these copies admits a bound in terms of the mixing coefficient between the (\(\sigma\)-algebras generated by the) random vectors. Thus, coupling gives a geometric interpretation of the mixing coefficients. The first result is for \(\beta\)-mixing random vectors.

**Theorem 16.2.1:**_(Berbee's theorem). Let \((X,Y)\) be a random vector on a probability space \((\Omega_{0},\mathcal{F}_{0},P_{0})\) such that \(X\) takes values in \(\mathbb{R}^{d}\) and \(Y\) in \(\mathbb{R}^{s}\), \(d,s\in\mathbb{N}\). Then, there exist an enlarged probability space \((\Omega,\mathcal{F},P)\) and a \(s\)-dimensional random vector \(Y^{*}\) such that_

* \((X\)_,_ \(Y\)_,_ \(Y^{*}\)_) are defined on_ \((\Omega,\mathcal{F},P)\)_,_
* \(Y^{*}\) _is independent of_ \(X\) _under_ \(P\) _and_ \((X,Y)\) _have the same distribution under_ \(P\) _and_ \(P_{0}\)_,_
* \(P(Y\neq Y^{*})=\beta(\sigma\langle X\rangle,\sigma\langle Y\rangle)\)_._

**Proof:** See Corollary 4.2.5 of Berbee (1979).

A weaker version of the above result is available for \(\alpha\)-mixing random variables, where the difference between \(Y\) and its independent copy admits a bound in terms of the \(\alpha\)-mixing coefficient.

**Theorem 16.2.2:**_(Bradley's theorem). In Theorem 16.2.1, assume \(s=1\) and \(0<E|Y|^{\gamma}<\infty\) for some \(0<\gamma<\infty\). Then, for all \(0<y\leq(E|Y|^{\gamma})^{1/\gamma}\),_

\[P\bigl{(}|Y-Y^{*}|\geq y\bigr{)} \leq 18\Bigl{[}\alpha\bigl{(}\sigma\langle X\rangle,\sigma\langle Y \rangle\bigr{)}\Bigr{]}^{2\gamma/1+2\gamma} \tag{2.12}\] \[(E|Y|^{\gamma})^{1/1+2\gamma}\,y^{-\gamma/(1+2\gamma)}.\]

**Proof:** See Theorem 3 of Bradley (1983).

Next, some bounds on the covariance between mixing random variables are established. These will be useful for deriving limit theorems for sums of mixing random variables. For a random variable \(X\), define the function

\[Q_{X}(u)=\inf\big{\{}t:P(|X|>t)\leq u\big{\}},\ \ u\in(0,1). \tag{2.13}\]

Thus, \(Q_{X}(u)\) is the quantile function of \(|X|\) at \((1-u)\).

**Theorem 16.2.3:**_(Rio's inequality). Let \(X\) and \(Y\) be two random variables with \(\int_{0}^{1}Q_{X}(u)Q_{Y}(u)du<\infty\). Then,_

\[\bigl{|}\mbox{Cov}(X,Y)\bigr{|}\leq 2\int_{0}^{2\alpha}Q_{X}(u)Q_{Y}(u)du \tag{2.14}\]

_where \(\alpha=\alpha\bigl{(}\sigma\langle X\rangle,\sigma\langle Y\rangle\bigr{)}\)._

**Proof:** By Tonelli's theorem,

\[EX^{+}Y^{+} = E\biggl{[}\Bigl{(}\int_{0}^{X^{+}}du\Bigr{)}\Bigl{(}\int_{0}^{Y^ {+}}du\Bigr{)}\biggr{]}\] \[= E\biggl{(}\int_{0}^{\infty}\int_{0}^{\infty}I(X^{+}>u)I(Y^{+}>v) dudv\biggr{)}\] \[= \int_{0}^{\infty}\int_{0}^{\infty}P(X>u,Y>v)dudv\]

and similarly, \(EX^{+}=\int_{0}^{\infty}P(X>u)du\). Hence, by (2.1), it follows that

\[\bigl{|}\mbox{Cov}(X^{+},Y^{+})\bigr{|} \tag{2.15}\] \[= \biggl{|}\int_{0}^{\infty}\int_{0}^{\infty}\big{[}P(X>u,Y>v)-P(X> u)P(Y>v)\big{]}dudv\biggr{|}\] \[\leq \int_{0}^{\infty}\int_{0}^{\infty}\min\big{\{}\alpha,P(X>u),P(Y>v )\big{\}}dudv.\]Next note that for any real numbers \(a\), \(b\), \(c\), \(d\)

\[(\alpha\wedge a\wedge c)+(\alpha\wedge a\wedge d)+(\alpha\wedge b \wedge c)+(\alpha\wedge b\wedge d) \tag{2.16}\] \[\leq [2(\alpha\wedge a)]\wedge(c+d)+[2(\alpha\wedge b)]\wedge(c+d)\] \[\leq 2[2\alpha\wedge(a+b)\wedge(c+d)].\]

Now using (2.15), (2.16), and the identity \(\mbox{Cov}(X,Y)=\mbox{Cov}(X^{+},Y^{+})+\mbox{Cov}(X^{-},Y^{-})-\mbox{Cov}(X^ {+},Y^{-})-\mbox{Cov}(X^{-},Y^{+})\), one gets

\[\big{|}\mbox{Cov}(X,Y)\big{|} \tag{2.17}\] \[\leq 2\int_{0}^{\infty}\int_{0}^{\infty}\min\big{\{}2\alpha,P(|X|>u), P(|Y|>v)\big{\}}dudv.\]

Hence, it is enough to show that the right sides of (2.14) and (2.17) agree. To that end, let \(U\) be a Uniform (0,1) random variable and define \((W_{1},W_{2})=(0,0)I(U\geq 2\alpha)+\big{(}Q_{X}(U),Q_{Y}(U)\big{)}I(U<2\alpha)\). Then

\[EW_{1}W_{2}=\int_{0}^{2\alpha}Q_{X}(u)Q_{Y}(u)du. \tag{2.18}\]

On the other hand, noting that \(Q_{X}(a)>t\) iff \(P(|X|>t)>a\), one has

\[EW_{1}W_{2} = \int_{0}^{\infty}\int_{0}^{\infty}P\big{(}W_{1}>u,W_{2}>v\big{)} dudv\] \[= \int_{0}^{\infty}\int_{0}^{\infty}P\big{(}U<2\alpha,Q_{X}(U)>u,Q_ {Y}(U)>v\big{)}dudv\] \[= \int_{0}^{\infty}\int_{0}^{\infty}\min\big{\{}2\alpha,P(|X|>u),P( |Y|>v)\big{\}}dudv.\]

Hence, the theorem follows from (2.17), (2.18), and the above identity. \(\Box\)

**Corollary 16.2.4:**_Let \(X\) and \(Y\) be two random variables with \(\alpha(\sigma\langle X\rangle,\sigma\langle Y\rangle)=\alpha\in[0,1]\)._

1. _(Davydov's inequality). Suppose that_ \(E|X|^{p}<\infty\)_,_ \(E|Y|^{q}<\infty\) _for some_ \(p,q\in(1,\infty)\) _with_ \(\frac{1}{p}+\frac{1}{q}<1\)_. Then,_ \(E|XY|<\infty\) _and_ \[\big{|}\mbox{Cov}(X,Y)\big{|}\leq 2r(2\alpha)^{1/r}\big{(}E|X|^{p}\big{)}^{1/p }\big{(}E|Y|^{q}\big{)}^{1/q},\] (2.19) _where_ \(\frac{1}{r}=1-\big{(}\frac{1}{p}+\frac{1}{q}\big{)}\)_._
2. _If_ \(P\big{(}|X|\leq c_{1})=1=P(|Y|\leq c_{2})\) _for some constants_ \(c_{1}\)_,_ \(c_{2}\in(0,\infty)\)_, then_ \[\big{|}\mbox{Cov}(X,Y)\big{|}\leq 4c_{1}c_{2}\alpha.\] (2.20)

**Proof:** Let \(a=(E|X|^{p})^{1/p}\) and \(b=(E|Y|^{q})^{1/q}\). W.l.o.g., suppose that \(a,b\in(0,\infty)\). Then, by Markov's inequality, for any \(0<u<1\),

\[P\big{(}|X|>au^{-1/p}\big{)}\leq E|X|^{p}\big{/}(au^{-1/p})^{p}=u\]

and hence, \(Q_{X}(u)\leq au^{-1/p}\). Similarly, \(Q_{Y}(u)\leq bu^{-1/q}\), \(0<u<1\). Hence, by Theorem 16.2.3,

\[\big{|}{\rm Cov}(X,Y)\big{|} \leq 2\int_{0}^{2\alpha}ab\,u^{-1/p-1/q}du\] \[= 2ab(2\alpha)^{1-1/p-1/q}\Big{/}\Big{(}1-\frac{1}{p}-\frac{1}{q} \Big{)}.\]

which is equivalent to (2.19).

The proof of (2.20) is a direct consequence of Rio's inequality and the bounds \(Q_{X}(u)\leq c_{1}\) and \(Q_{Y}(u)\leq c_{2}\) for all \(0<u<1\). \(\Box\)

### 16.3 Central limit theorems for mixing sequences

In this section, CLTs for sequences of random variables satisfying different mixing conditions are proved.

**Proposition 16.3.1:**_Let \(\{X_{i}\}_{i\in{\mathbb{Z}}}\) be a collection of random variables with strong mixing coefficient \(\alpha(\cdot)\)._

* _Suppose that_ \(\sum_{n=1}^{\infty}\alpha(n)<\infty\) _and for some_ \(c\in(0,\infty)\)_,_ \(P(|X_{i}|\leq c)=1\) _for all_ \(i\)_. Then,_ \[\sum_{n=1}^{\infty}{\it Cov}(X_{1},X_{n+1})\ \ \mbox{converges absolutely}.\] (3.1)
* _Suppose that_ \(\sum_{n=1}^{\infty}\alpha(n)^{\delta/2+\delta}<\infty\) _and_ \(\sup_{i\in{\mathbb{Z}}}E|X_{i}|^{2+\delta}<\infty\) _for some_ \(\delta\in(0,\infty)\)_. Then, (_3.1_) holds._

**Proof:** A direct consequence of Corollary 16.2.4. \(\Box\)

Next suppose that the collection of random variables \(\{X_{i}\}_{i\in{\mathbb{Z}}}\) is stationary and that \({\rm Var}(X_{1})+\sum_{n=1}^{\infty}\big{|}{\rm Cov}(X_{1},X_{1+n})\big{|}<\infty\). Then by the DCT,

\[n{\rm Var}(\bar{X}_{n}) = n^{-1}{\rm Var}\bigg{(}\sum_{i=1}^{n}X_{i}\bigg{)}\] \[= n^{-1}\bigg{[}\sum_{i=1}^{n}{\rm Var}(X_{i})+2\sum_{1\leq i<j \leq n}{\rm Cov}(X_{i},X_{j})\bigg{]}\]

[MISSING_PAGE_FAIL:528]

By letting \(q\to\infty\) suitably and using the mixing condition, one can replace the \(B_{j}\)'s by their independent copies, and appeal to the Lindeberg CLT for sums of independent random variables to conclude that

\[\frac{1}{\sqrt{n}}\sum_{j=1}^{K}B_{j}\longrightarrow^{d}N\bigl{(}0,\sigma_{ \infty}^{2}\bigr{)}.\]

Although the blocking approach is described here for stationary random variables, with minor modifications, it is applicable to certain nonstationary sequences as shown below.

**Theorem 16.3.2:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables (not necessarily stationary) with strong mixing coefficient \(\alpha(\cdot)\). Suppose that there exist constants \(\sigma_{0}^{2}\), \(c\in(0,\infty)\) such that_

\[P(|X_{i}|\leq c)=1\quad\mbox{for all}\quad i\in\mathbb{N}, \tag{3.4}\]

\[\gamma_{n}\equiv\sup_{j\geq 1}\left|n^{-1}\mbox{ Var}\biggl{(}\sum_{i=j}^{j+n-1}X_{i}\biggr{)}-\sigma_{0}^{2}\right|\to 0\quad\mbox{as}\quad n\to\infty, \tag{3.5}\]

_and that_

\[\sum_{n=1}^{\infty}\alpha(n)<\infty. \tag{3.6}\]

_Then,_

\[\sqrt{n}\bigl{(}\bar{X}_{n}-\bar{\mu}_{n}\bigr{)}\longrightarrow^{d}N(0,\sigma _{0}^{2})\quad\mbox{as}\quad n\to\infty \tag{3.7}\]

_where \(\bar{\mu}_{n}=E\bar{X}_{n}\), and \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\), \(n\geq 1\)._

An important special case of Theorem 16.3.2 is the following:

**Corollary 16.3.3:**_If \(\{X_{n}\}_{n\geq 1}\) is a sequence of stationary bounded random variables with \(\sum_{n=1}^{\infty}\alpha(n)<\infty\), and if \(\sigma_{\infty}^{2}\) of (3.2) is positive, then, with \(\mu=EX_{1}\),_

\[\sqrt{n}\bigl{(}\bar{X}_{n}-\mu\bigr{)}\longrightarrow^{d}N(0,\sigma_{\infty}^ {2})\quad\mbox{as}\quad n\to\infty. \tag{3.8}\]

**Proof:** For stationary random variables, (3.5) holds with \(\sigma_{0}^{2}=\sigma_{\infty}^{2}\) (cf. (3.2)). Hence, the Corollary follows from Theorem 16.3.2. \(\Box\)

For proving the theorem, the following auxiliary result will be used.

**Lemma 16.3.4:**_Suppose that the conditions of Theorem 16.3.2 hold. Then,_

\[\sup_{m\geq 1}E\biggl{[}\sum_{i=m}^{m+n-1}(X_{i}-EX_{i})\biggr{]}^{4}=o(n^{3}) \quad\mbox{as}\quad n\to\infty.\]

**Proof:** W.l.o.g., let \(EX_{i}=0\) for all \(i.\) Note that for any \(m\in\mathbb{N},\)

\[E\bigg{(}\sum_{j=m}^{n+m-1}X_{j}\bigg{)}^{4} \tag{3.9}\] \[= \sum_{j}EX_{j}^{4}+6\sum_{i<j}EX_{i}^{2}X_{j}^{2}+4\sum_{i\neq j} EX_{i}^{3}X_{j}\] \[+\ 6\sum_{i\neq j\neq k}EX_{i}^{2}X_{j}X_{k}+\sum_{i\neq j\neq k \neq\ell}EX_{i}X_{j}X_{k}X_{\ell}\] \[\equiv I_{1n}+\cdots+I_{5n},\ \ \mbox{say},\]

where the indices \(i,j,k,\ell\) in the above sums lie between \(m\) and \(m+n-1.\) By (3.4),

\[\big{|}I_{1n}\big{|}+\big{|}I_{2n}\big{|}+\big{|}I_{3n}\big{|}\leq n\cdot c^{4 }+7n(n-1)c^{4}\leq 7n^{2}c^{4}. \tag{3.10}\]

By Corollary 16.2.4 (ii), noting that \(EX_{i}=0\) for all \(i,\)

\[\big{|}I_{4n}\big{|} \leq 12\sum_{i<j<k}\Big{\{}\big{|}EX_{i}^{2}X_{j}X_{k}\big{|}+\big{|} EX_{i}X_{j}^{2}X_{k}\big{|}+\big{|}EX_{i}X_{j}X_{k}^{2}\big{|}\Big{\}} \tag{3.11}\] \[\leq 12\sum_{i<j<k}\big{\{}4c^{4}\alpha(j-k)+4c^{4}\alpha(j-k)+4c^{4} \alpha(j-i)\big{\}}\] \[\leq 144c^{4}n^{2}\bigg{[}\sum_{r=1}^{n-1}\alpha(r)\bigg{]}.\]

Similarly,

\[\big{|}I_{5n}\big{|} \leq (4!)\sum_{i<j<k<\ell}\big{|}EX_{i}X_{j}X_{k}X_{\ell}\big{|} \tag{3.12}\] \[\leq 24\sum_{i<j<k<\ell}4c^{4}\big{[}\alpha(j-i)\wedge\alpha(\ell-k) \big{]}\] \[= 96c^{4}\sum_{i=1}^{n-3}\sum_{j=i+1}^{n-2}\sum_{k=j+1}^{n-1}\sum_ {r=1}^{n-k}\big{[}\alpha(j-i)\wedge\alpha(r)\big{]}\] \[= 96c^{4}\sum_{i=1}^{n-3}\sum_{s=1}^{n-2-i}\sum_{k=i+s+1}^{n-1} \sum_{r=1}^{n-k}\big{[}\alpha(s)\wedge\alpha(r)\big{]}\] \[\leq 96c^{4}n^{2}\sum_{s=1}^{n}\sum_{r=1}^{n}\big{[}\alpha(s)\wedge \alpha(r)\big{]}\] \[= 96c^{4}n^{2}\bigg{[}\sum_{s=1}^{n}\alpha(s)+2\sum_{s=1}^{n-1} \sum_{r=s+1}^{n}\alpha(r)\bigg{]}\] \[\leq 192c^{4}n^{2}\sum_{r=1}^{n}r\alpha(r)\]\[\leq 192c^{4}n^{2}\biggl{[}n^{1/2}\sum_{r=1}^{\infty}\alpha(r)+n\sum_{r\geq \sqrt{n}}\alpha(r)\biggr{]}. \tag{3.12}\]

Since \(\sum_{r\geq\sqrt{n}}\alpha(r)=o(1)\) and the bounds in (3.9)-(3.12) do not depend on \(m\), Lemma 16.3.4 follows. \(\Box\)

**Proof of Theorem 16.3.2:** W.l.o.g., let \(EX_{j}=0\) for all \(j\). Let \(p\equiv p_{n}\), \(q\equiv q_{n}=\lfloor n^{1/2}\rfloor\), \(n\geq 1\) be integers satisfying

\[q/p+p/n=o(1)\quad\mbox{as}\quad n\to\infty, \tag{3.13}\]

when a choice of \(\{p_{n}\}\) will be specified later. Let \(r=p+q\) and \(K=\lfloor n/r\rfloor\). As outlined earlier, for \(i=1,\ldots,K\), let

\[B_{i} = \sum_{j=(i-1)r+1}^{(i-1)r+p}X_{j},\] \[L_{i} = \sum_{j=(i-1)r+p+1}^{ir}X_{j},\]

and let

\[R_{n}=\sum_{j=1}^{n}X_{j}-\sum_{i=1}^{K}(B_{i}+L_{i})\]

respectively denote the sums over the 'big blocks,' the 'little blocks,' and the 'excess' (cf. (3.3)). Thus

\[\sqrt{n}\bar{X}_{n}=\frac{1}{\sqrt{n}}\sum_{i=1}^{K}B_{i}+\frac{1}{\sqrt{n}} \sum_{i=1}^{K}L_{i}+\frac{1}{\sqrt{n}}R_{n}. \tag{3.14}\]

Since \(R_{n}\) is a sum over \((n-Kr)\leq r\) consecutive \(X_{j}\)'s, by Corollary 16.2.4 (ii),

\[E\bigl{(}R_{n}/\sqrt{n}\bigr{)}^{2} \leq n^{-1}\biggl{[}\sum_{j=Kr+1}^{n}EX_{j}^{2}+\sum_{i\neq j}\bigl{|} EX_{i}X_{j}\bigr{|}\biggr{]} \tag{3.15}\] \[\leq 8c^{2}n^{-1}\biggl{[}(n-Kr)+(n-Kr)\sum_{k=1}^{\infty}\alpha(k) \biggr{]}\] \[= O\Bigl{(}\frac{r}{n}\Bigr{)}\to 0\quad\mbox{as}\quad n\to\infty.\]

Next note that the variables \(L_{i}\) and \(L_{i+k}\) depend on two disjoint sets of \(X_{j}\)'s that are separated by a distance of 

[MISSING_PAGE_FAIL:532]

\[\frac{1}{\sqrt{n}}\sum_{i=1}^{K}B_{i}\longrightarrow^{d}N(0,\sigma_{0}^{2}). \tag{3.20}\]

Theorem 16.3.2 now follows from (3.14), (3.15), (3.16), and (3.20). \(\Box\)

For unbounded strongly mixing random variables, a CLT can be proved under moment and mixing conditions similar to Proposition 16.3.1 (ii). The key idea here is to use Theorem 16.3.2 for the sum of a truncated part of the unbounded random variables and then show that the sum over the remaining part is negligible in the limit.

Theorem 16.3.5: Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables (not necessarily stationary) such that for some \(\delta\in(0,\infty)\), \(\zeta_{\delta}\equiv\sup_{n\geq 1}\left(E|X_{n}|^{2+\delta}\right)^{1/2+\delta}<\infty\) and

\[\sum_{n=1}^{\infty}\alpha(n)^{\delta/2+\delta}<\infty. \tag{3.21}\]_Suppose that there exist \(M_{0}\in(0,\infty)\) and a function \(\tau(\cdot):(M_{0},\infty)\to(0,\infty)\) such that for all \(M>M_{0}\),_

\[\gamma_{n}(M)\equiv\sup_{j\geq 1}\left|n^{-1}\mbox{Var}\bigg{(}\sum_{i=j}^{j+n-1}X_ {i}I(|X_{i}|<M)\bigg{)}-\tau(M)\right|\to 0\quad\mbox{as}\quad n\to\infty. \tag{3.22}\]

_If \(\tau(M)\to\sigma_{0}^{2}\) as \(M\to\infty\) and \(\sigma_{0}^{2}\in(0,\infty)\), then (3.7) holds._

**Proof of Theorem 16.3.5:** W.l.o.g., let \(EX_{i}=0\) for all \(i\in\mathbb{N}\). Let \(M\in(1,\infty)\). For \(i\geq 1\), define

\[\tilde{Y}_{i,M}=X_{i}I(|X_{i}|\leq M)\quad\mbox{and}\quad\tilde{Z}_{i,M}=X_{i} I(|X_{i}|>M),\]

\[Y_{i,M}=\tilde{Y}_{i,M}-E\tilde{Y}_{i,M},\ \ Z_{i,M}=\tilde{Z}_{i,M}-E\tilde{Z}_ {i,M}.\]

Then,

\[\sqrt{n}(\bar{X}_{n}-\mu) = \frac{1}{\sqrt{n}}\sum_{i=1}^{n}Y_{i,M}+\frac{1}{\sqrt{n}}\sum_{i= 1}^{n}Z_{i,M}, \tag{3.23}\] \[\equiv S_{n,M}+T_{n,M},\quad\mbox{say}.\]

Note that, with \(a=\lfloor M^{\delta/4}\rfloor\) and \(J_{n}=\{1,\ldots,n\}\), by Cauchy-Schwarz inequality and Corollary 16.2.4 (i),

\[ET_{n,M}^{2}\] \[= n^{-1}\sum_{|i-j|\leq a,i,j\in J_{n}}\left|EZ_{i,M}Z_{j,M}\right| +n^{-1}\sum_{|i-j|>a,i,j\in J_{n}}\left|EZ_{i,M}Z_{j,M}\right|\] \[\leq (2a+1)\sup\left\{EZ_{i,M}^{2}:i\geq 1\right\}+2\sum_{k=a}^{ \infty}\frac{4+2\delta}{\delta}\,\alpha(k)^{\delta/2+\delta}\zeta_{\delta}^{2}\] \[\leq 4(2a+1)M^{-\delta}\zeta_{\delta}^{2+\delta}+\frac{8+4\delta}{ \delta}\,\zeta_{\delta}^{2}\sum_{k=a}^{\infty}\alpha(k)^{\delta/2+\delta}.\]

Hence,

\[\lim_{M\uparrow\infty}\ \sup_{n\geq 1}ET_{n,M}^{2}=0. \tag{3.24}\]

Since \(\tau(M)\to\sigma_{0}^{2}\in(0,\infty)\), there exists \(M_{1}\in(M_{0},\infty)\) such that for all \(M>M_{1}\), \(\tau(M)\in(0,\infty)\). Hence, by Theorem 16.3.2 and (3.22), for all \(M>M_{1}\),

\[S_{n,M}\longrightarrow^{d}N\big{(}0,\tau(M)\big{)}\quad\mbox{as}\quad n\to\infty. \tag{3.25}\]

Next note that for any \(\epsilon\in(0,\infty)\) and \(x\in\mathbb{R}\),

\[P\Big{(}\sqrt{n}\,\bar{X}_{n}\leq x\Big{)} \tag{3.26}\] \[\leq P\Big{(}S_{n,M}+T_{n,M}\leq x,\big{|}T_{n,M}\big{|}<\epsilon \Big{)}+P\Big{(}\big{|}T_{n,M}\big{|}>\epsilon\Big{)}\] \[\leq P\Big{(}S_{n,M}\leq x+\epsilon\Big{)}+P\Big{(}\big{|}T_{n,M} \big{|}>\epsilon\Big{)}\]and similarly,

\[P\Big{(}\sqrt{n}\,\bar{X}_{n}\leq x\Big{)}\geq P\Big{(}S_{n,M}\leq x-\epsilon\Big{)} -P\Big{(}\big{|}T_{n,M}\big{|}>\epsilon\Big{)}. \tag{3.27}\]

Hence, for any \(\epsilon>0\), \(M>M_{1}\), and \(x\in\mathbb{R}\), letting \(n\to\infty\) in (3.26) and (3.27), by (3.25) one gets

\[\Phi\Big{(}\frac{x-\epsilon}{\tau(M)^{1/2}}\Big{)}-\limsup_{n\to \infty}P\Big{(}\big{|}T_{n,M}\big{|}>\epsilon\Big{)}\] \[\leq \liminf_{n\to\infty}P\Big{(}\sqrt{n}\,\bar{X}_{n}\leq x\Big{)} \leq\limsup_{n\to\infty}P\Big{(}\sqrt{n}\,\bar{X}_{n}\leq x\Big{)}\] \[\leq \Phi\Big{(}\frac{x+\epsilon}{\tau(M)^{1/2}}\Big{)}+\limsup_{n\to \infty}P\Big{(}\big{|}T_{n,M}\big{|}>\epsilon\Big{)}.\]

Since \(\tau(M)\to\sigma_{0}^{2}\) as \(M\to\infty\), letting \(M\uparrow\infty\) first and then \(\epsilon\downarrow 0\), and using (3.24), one gets

\[\lim_{n\to\infty}P\Big{(}\sqrt{n}\,\bar{X}_{n}\leq x\Big{)}=\Phi(x/\sigma_{0}) \quad\mbox{for all}\quad x\in\mathbb{R}.\]

This completes the proof of Theorem 16.3.5. \(\Box\)

A direct consequence of Theorem 16.3.5 is the following result:

**Corollary 16.3.6:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of stationary random variables with \(EX_{1}=\mu\in\mathbb{R}\), \(E|X_{1}|^{2+\delta}<\infty\) and \(\sum_{n=1}^{\infty}\alpha(n)^{\delta/2+\delta}<\infty\) for some \(\delta\in(0,\infty)\). If \(\sigma_{\infty}^{2}\) of (3.2) is positive, then with \(\mu=EX_{1}\),_

\[\sqrt{n}\big{(}\bar{X}_{n}-\mu\big{)}\longrightarrow^{d}N(0,\sigma_{\infty}^{2 }).\]

**Proof of Corollary 16.3.6:** W.l.o.g., let \(\mu=0\). Clearly, under the stationarity of \(\{X_{n}\}_{n\geq 1}\), \(\zeta_{\delta}^{2+\delta}=E|X_{1}|^{2+\delta}<\infty\). Let \(a=\lfloor M^{\delta/4}\rfloor\), \(Y_{i,M}\), and \(\tilde{Y}_{i,M}\) be as in the proof of Theorem 16.3.5. Also, let \(\tau(M)=\mbox{Var}(Y_{i,M})+2\sum_{k=1}^{\infty}\mbox{Cov}(Y_{1,M},Y_{1+k,M})\), \(M\geq 1\). Since \(EX_{1}^{2}I(|X_{1}|>M)\leq M^{-\delta}\zeta_{\delta}\), one gets

\[\big{|}\tau(M)-\sigma_{\infty}^{2}\big{|}\] \[\leq \big{|}EY_{1,M}^{2}-EX_{1}^{2}\big{|}+2\sum_{k=1}^{a-1}\big{|}EY_ {1,M}Y_{1+k,M}-EX_{1}X_{1+k}\big{|}\] \[\quad+2\sum_{k=a}^{\infty}\Big{\{}\big{|}EY_{1,M}Y_{1+k,M}\big{|} +\big{|}EX_{1}X_{1+k}\big{|}\Big{\}}\] \[\leq 2\sum_{k=0}^{a-1}\Big{[}\Big{\{}E\big{(}Y_{1,M}-X_{1}\big{)}^{2} \Big{\}}^{1/2}\Big{\{}EY_{1+k,M}^{2}\Big{\}}^{1/2}\]\[\qquad+4\sum_{k=a}^{\infty}\Big{\{}\frac{4+2\delta}{\delta}\alpha(k)^{ \delta/2+\delta}\,\zeta_{\delta}^{2}\,\Big{\}}\] \[\leq 4a\,\zeta_{\delta}\Big{\{}EX_{1}^{2}I\big{(}|X_{1}|>M\big{)} \Big{\}}^{1/2}+4\,\frac{4+2\delta}{\delta}\,\zeta_{\delta}^{2}\bigg{[}\sum_{k= a}^{\infty}\alpha(k)^{\delta/2+\delta}\bigg{]}\] \[\to 0\quad\mbox{as}\quad M\to\infty. \tag{3.28}\]

Further, by Corollary 16.3.1 (ii), the stationarity of \(\{X_{n}\}_{n\geq 1}\), and the arguments in the derivation of (3.2), (3.22) follows. Corollary 16.3.6 now follows from Theorem 16.3.5. \(\Box\)

In the case that the sequence \(\{X_{n}\}_{n\geq 1}\) is stationary, further refinements of Corollaries 16.3.3 and 16.3.6 are possible. The following result, due to Doukhan, Massart and Rio (1994) uses a different method of proof to establish the CLT under stationarity. To describe it, for any nonincreasing sequence of positive real numbers \(\{a_{n}\}_{n\geq 1}\), define the function \(a(t)\) and its inverse \(a^{-1}(t)\), respectively, by

\[a(t)=\sum_{n=1}^{\infty}a_{n}\,I(n-1<t\leq n),\ \ t>0,\]

and

\[a^{-1}(u)=\inf\{t:a(t)\leq u\},\ \ u\in(0,\infty).\]

**Theorem 16.3.7:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of stationary random variables with \(EX_{1}=\mu\in\mathbb{R}\), \(EX_{1}^{2}<\infty\) and strong mixing coefficient \(\alpha(\cdot)\). Suppose that_

\[\int_{0}^{1}\alpha^{-1}(u)Q_{X_{1}}(u)^{2}du<\infty, \tag{3.29}\]

_where \(Q_{X_{1}}(u)=\inf\{t:P(|X_{1}|>t)\leq u\}\), \(0<u<1\) is as in (2.13). Then, \(0\leq\sigma_{\infty}^{2}<\infty\). If \(\sigma_{\infty}^{2}>0\), then_

\[\sqrt{n}(\bar{X}_{n}-\mu)\longrightarrow^{d}N(0,\sigma_{\infty}^{2}).\]

**Proof:** See Doukhan et al. (1994). \(\Box\)

Note that if \(P(|X_{1}|\leq c)=1\) for some \(c\in(0,\infty)\), then \(Q_{X_{1}}(u)\leq c\) for all \(u\in(0,1)\) and (3.29) holds iff \(\int_{0}^{1}\alpha^{-1}(u)du<\infty\) iff \(\sum_{n=1}^{\infty}\alpha(n)<\infty\). Thus, Theorem 16.3.7 yields Corollary 16.3.3 as a special case. Similarly, it can be shown that if \(E|X_{1}|^{2+\delta}<\infty\) and \(\sum_{n=1}^{\infty}\alpha(n)^{\delta/2+\delta}<\infty\), then (3.29) holds and, therefore, Theorem 16.3.7 also yields Corollary 16.3.6 as a special case. For another example, suppose \(\alpha(n)=O\big{(}\exp(-cn)\big{)}\) as \(n\to\infty\) for some \(c\in(0,\infty)\). Then, (3.29) holds if

\[EX_{1}^{2}\log\big{(}1+|X_{1}|\big{)}<\infty. \tag{3.30}\]In this case, the logarithmic factor in (3.30) cannot be dropped. More precisely, it is known that finiteness of the second moment alone (with arbitrarily fast rate of decay of the strong mixing coefficient) is not enough to guarantee the CLT (Herrndorf (1983)) for strong mixing random variables.

For the \(\rho\)-mixing random variables, the following result is known. A process \(\{X_{n}\}_{n\geq 1}\) is called _second order stationary_ if

\[EX_{n}^{2}<\infty,\ EX_{n}=EX_{1}\ {\rm and}\ EX_{n}X_{n+k}=EX_{1}X_{1+k} \tag{3.31}\]

for all \(n\geq 1\), \(k\geq 0\).

**Theorem 16.3.8:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of \(\rho\)-mixing, second order stationary random variables with \(EX_{1}^{2}<\infty\) and \(EX_{1}=\mu\). Suppose that_

\[\sum_{n=1}^{\infty}\rho(2^{n})<\infty. \tag{3.32}\]

_Then, \(\sigma_{\infty}^{2}\equiv\mbox{Var}(X_{1})+2\sum_{k=1}^{\infty}\mbox{Cov}(X_{ 1},X_{1+k})\) converges and_

\[\mbox{Var}\bigl{(}\sqrt{n}\,\bar{X}_{n}\bigr{)}\to\sigma_{\infty}^{2}.\]

_If \(\sigma_{\infty}^{2}\in(0,\infty)\), then \(\sqrt{n}\bigl{(}\bar{X}_{n}-\mu\bigr{)}\longrightarrow^{d}N(0,\sigma_{\infty} ^{2})\)._

**Proof:** See Peligrad (1982). \(\Box\)

Thus, unlike the strong mixing case, in the \(\rho\)-mixing case the CLT holds under finiteness of the second moment, provided (3.32) holds and \(\sigma_{\infty}^{2}>0\).

### Problems

16.1. Deduce (1.16) from (1.15).
16.2. Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of random variables. Show that there exists a sequence of integers \(m_{n}\in(0,\infty)\) such that 1. \(P\bigl{(}|X_{n}|>m_{n}\bigr{)}\to 0\quad{\rm as}\quad n\to\infty\). 2. \(P\bigl{(}|X_{n}|>m_{n}\ {\rm i.o.}\bigr{)}=0\).
16.3. Show that \(\{\tilde{Y}_{nk},{\cal F}_{nk}\}\) of (1.18) is a mda. (**Hint:**\(\{\tau_{n}\geq k\}=\{\tau_{n}\leq k-1\}^{c}\in{\cal F}_{n,k-1}\).)
16.4. Show that \(\{\tilde{Y}_{nk},{\cal F}_{nk}\}\) of (1.18) satisfies (1.2) and (1.3) with \(\tau_{n}\) replaced by \(m_{n}\). (**Hint:** Verify that for any Borel measurable \(g:{\mathbb{R}}\to{\mathbb{R}}\) and \(\epsilon>0\),

\[P\Bigl{(}\Bigl{|}\sum_{k=1}^{\tau_{n}}g(Y_{nk})-\sum_{k=1}^{m_{n}}g(\tilde{Y}_ {nk})\Bigr{|}>\epsilon\Bigr{)}\leq P(\tau_{n}>m_{n}).) \tag{4.1}\]

[MISSING_PAGE_FAIL:538]

16.11 Let \(Y_{i}=x_{i}\beta+\epsilon_{i}\), \(i\geq 1\) be a simple linear regression model where \(\beta\in\mathbb{R}\) is the regression parameter, \(x_{i}\in\mathbb{R}\) is nonrandom and \(\{\epsilon_{i}\}_{i\geq 1}\) is a sequence of stationary random variables with strong mixing coefficient \(\alpha(\cdot)\) such that \(E\epsilon_{1}=0\), \(E|\epsilon_{1}|^{2+\delta}<\infty\) and \(\sum_{n=1}^{\infty}\alpha(n)^{\delta/2+\delta}<\infty\), for some \(\delta\in(0,\infty)\). Suppose that for all \(h\in\mathbb{Z}_{+}\), \[\sum_{i=1}^{n-h}x_{i}x_{i+h}\Big{/}\sum_{i=1}^{n}x_{i}^{2}\to\gamma(h)\] (4.5) for some \(\gamma(h)\in\mathbb{R}\) and \(n^{-1}\sum_{i=1}^{n}|x_{i}|^{2+\delta}=O(1)\) as \(n\to\infty\). Let \(\hat{\beta}_{n}=\sum_{i=1}^{n}x_{i}y_{i}\big{/}\sum_{i=1}^{n}x_{i}^{2}\) denote the least squares estimator of \(\beta\). Show that \[\bigg{(}\sum_{i=1}^{n}x_{i}^{2}\bigg{)}^{1/2}(\hat{\beta}_{n}-\beta)\longrightarrow ^{d}N(0,\sigma_{\infty}^{2})\] for some \(\sigma_{\infty}^{2}\in[0,\infty)\). Find \(\sigma_{\infty}^{2}\). (Note that by definition, \(Z\sim N(0,0)\) if \(P(Z=0)=1\).)
16.12 Let \(Y_{i}=x_{i}\beta^{\prime}+\epsilon_{i}\), \(i\geq 1\), where \(x_{i}\), \(\beta\in\mathbb{R}^{p}\), (\(p\in\mathbb{N}\)) and \(\{\epsilon_{i}\}_{i\geq 1}\) satisfies the conditions of Problem 16.11. Suppose that for all \(h\in\mathbb{Z}_{+}\), \[n^{-1}\sum_{i=1}^{n-h}x_{i}^{\prime}x_{i+h}\to\Gamma(h)\] for some \(p\times p\) matrix \(\Gamma(h)\), with \(\Gamma(h)\) nonsingular, and that \(\max\{\|x_{i}\|:1\leq i\leq n\}=O(1)\) and \(n\to\infty\). Find the limit distribution of \(\sqrt{n}(\hat{\beta}_{n}-\beta)\), where \(\hat{\beta}_{n}=\Big{(}\sum_{i=1}^{n}x_{i}^{\prime}x_{i}\Big{)}^{-1}\sum_{i=1} ^{n}x_{i}Y_{i}\), \(n\geq 1\).
16.13 Let \(\{X_{i}\}_{i\in\mathbb{Z}}\) be a first order stationary autoregressive process \[X_{i}=\rho X_{i-1}+\epsilon_{i},\ i\in\mathbb{Z},\] (4.6) where \(|\rho|<1\) and \(\{\epsilon_{i}\}_{i\in\mathbb{Z}}\) is a sequence of iid random variables with \(E\epsilon_{1}=0\), \(E\epsilon_{1}^{2}<\infty\). 1. Show that \(X_{i}=\sum_{j=0}^{\infty}\rho^{j}\epsilon_{i-j}\), \(i\in\mathbb{Z}\). 2. Find the limit distribution of \[\big{\{}\sqrt{n}(\hat{\rho}_{n}-\rho)\big{\}}_{n\geq 1},\] where \(\hat{\rho}_{n}=\sum_{i=1}^{n-1}X_{i}X_{i+1}\big{/}\sum_{i=1}^{n}X_{i}^{2}\), \(n\geq 1\). (**Hint:** Use Theorem 16.1.1.)The bootstrap method for independent variables

#### A description of the bootstrap method

The bootstrap method, introduced in statistics by Efron (1979), is a powerful tool for solving many statistical inference problems. Let \(X_{1},\ldots,X_{n}\) be iid random variables with common cdf \(F\). Let \(\theta=\theta(F)\) be a parameter and \(\hat{\theta}_{n}\) be an estimator of \(\theta\) based on observations \(X_{1},\ldots,X_{n}\), i.e., \(\hat{\theta}_{n}=t_{n}(X_{1},\ldots,X_{n})\) for some measurable function \(t_{n}(\cdot)\) of the random variables \(X_{1},\ldots,X_{n}\). The parameter \(\theta\) is called a 'level 1' parameter and the parameters related to the distribution of \(\hat{\theta}_{n}\) are called 'level 2' parameters (cf. Lahiri (2003)). For example, \(\mathrm{Var}(\hat{\theta}_{n})\) or a median of (the distribution of) \(\hat{\theta}_{n}\) are 'level 2' parameters. The bootstrap is a general method for estimating such 'level 2' parameters.

To describe the bootstrap method, let

\[R_{n}=r_{n}(X_{1},\ldots,X_{n};F)\] (1a) be a random variable that is a known function of \[X_{1},\ldots,X_{n}\] and \[F\]. An example of \[R_{n}\] is given by \[\tilde{R}_{n}=\sqrt{n}(\bar{X}_{n}-\mu)/\sigma,\] (1b) the normalized sample mean, where \[\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\] is the sample mean, \[\mu=EX_{1}\] and \[\sigma^{2}=\mathrm{Var}(X_{1})\]. The objective here is to approximate (esti

[MISSING_PAGE_FAIL:541]

variance of \(X_{1},\ldots,X_{n}\). Thus, the 'ordinary' bootstrap version of \(\tilde{R}_{n}\) (with \(\hat{F}_{n}=F_{n}\)) is given by

\[\tilde{R}_{m,n}^{*}=\sqrt{n}\bigl{(}\tilde{X}_{m}^{*}-\bar{X}_{n})/s_{n}. \tag{1.4}\]

Some questions that arise naturally in this context are: _Does the bootstrap distribution of \(\tilde{R}_{m,n}^{*}\) give a valid approximation to the distribution of \(\tilde{R}_{n}\)? How good is the approximation generated by the bootstrap? How does the quality of approximation depend on the resample size or on the underlying cdf F?_ Some of these issues are addressed next. Write \(P_{*}\) to denote the conditional probability given \(X_{1},X_{2},\ldots\) and \(\sup_{x}\) to denote supremum over \(x\in\mathbb{R}\).

#### Validity of the bootstrap: Sample mean

**Theorem 17.1.1:** Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=\mu\) and \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\). Let \(\tilde{R}_{n}=\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\) and let \(\tilde{R}_{n,n}^{*}\) be its 'ordinary' bootstrap version given by (1.4) with \(m=n\). Then

\[\Delta_{n}\equiv\sup_{x}\big{|}P(\tilde{R}_{n}\leq x)-P_{*}(\tilde{R}_{n,n}^{* }\leq x)\big{|}\to 0\quad\mbox{as}\quad n\to\infty,\quad\mbox{a.s.} \tag{1.5}\]

**Proof:** W.l.o.g., suppose that \(\mu=0\) and \(\sigma=1\). By the CLT, \(\tilde{R}_{n}\equiv\sqrt{n}\bar{X}_{n}\longrightarrow^{d}N(0,1)\). Hence, it is enough to show that as \(n\to\infty\),

\[\Delta_{1n}\equiv\sup_{x}\big{|}P_{*}\bigl{(}\sqrt{n}(\bar{X}_{n}^{*}-\bar{X}_ {n})\leq s_{n}x\bigr{)}-\Phi(x)\big{|}=o(1)\quad\mbox{a.s.},\]

where \(\Phi(\cdot)\) denotes the cdf of the \(N(0,1)\) distribution. By the Berry-Esseen theorem,

\[\Delta_{1n}\leq(2.75)\frac{E_{*}|X_{1}^{*}-\bar{X}_{n}|^{3}}{\sqrt{n}s_{n}^{3} }\leq\frac{2^{3}\cdot(2.75)}{\sqrt{n}}\frac{E_{*}|X_{1}^{*}|^{3}}{s_{n}^{3}}.\]

By the SLLN, \(s_{n}^{2}\to\sigma^{2}\in(0,\infty)\) and by the Marcinkiewz-Zygmund SLLN

\[\frac{1}{\sqrt{n}}E_{*}|X_{1}^{*}|^{3}=\frac{1}{n^{3/2}}\sum_{i=1}^{n}|X_{i}|^ {3}\to 0\quad\mbox{as}\quad n\to\infty\quad\mbox{a.s.}\]

Hence, \(\Delta_{1n}=o(1)\) as \(n\to\infty\) a.s. and the theorem is proved. \(\Box\)

One can also give a more direct proof of Theorem 17.1.1 using the Lindeberg-Feller CLT (Problem 17.1).

Theorem 17.1.1 shows that the bootstrap approximation to the distribution of the normalized sample mean is valid under the same conditions that guarantee the CLT. Under additional moment conditions, a more precise bound on the order of \(\Delta_{n}\) can be given. If \(E|X_{1}|^{3}<\infty\), then by the SLLN, \(n^{-1}\sum_{j=1}^{n}|X_{j}|^{3}\to E|X_{1}|^{3}\) as \(n\to\infty\), a.s. and hence, the last step in the proof of Theorem 17.1.1, it follows that \(\Delta_{n}=O(n^{-1/2})\) a.s. Thus, in this case, the rate of bootstrap approximation to the distribution of \(\tilde{R}_{n}\) is as good as that of the normal approximation to the distribution of \(\tilde{R}_{n}\). An important result of Singh (1981) shows that the error rate of bootstrap approximation is indeed smaller, provided the distribution of \(X_{1}\) is _nonlattice_ (cf. Chapter 11), i.e.,

\[|E\exp(\iota tX_{1})|\neq 1\quad\mbox{for all}\quad t\in\mathbb{R}\setminus\{0\}.\]

A precise statement of this result is given in Theorem 17.1.2 below.

#### Second order correctness of the bootstrap

**Theorem 17.1.2:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid nonlattice random variables with \(E|X_{1}|^{3}<\infty\). Also, let \(\tilde{R}_{n}\), \(\tilde{R}_{n,n}^{*}\), and \(\Delta_{n}\) be as in Theorem 17.1.1. Then,_

\[\Delta_{n}=o(n^{-1/2})\quad\mbox{as}\quad n\to\infty,\quad\mbox{a.s.} \tag{1.6}\]

**Proof:** Only an outline of the proof will be given here. By Theorem 11.4.4 on Edgeworth expansions, it follows that

\[\sup_{x\in\mathbb{R}}\Big{|}P(\tilde{R}_{n}\leq x)-\Big{[}\Phi(x)-\frac{\mu_{3 }}{6\sqrt{n}\sigma^{3}}(x^{2}-1)\phi(x)\Big{]}\Big{|}=o(n^{-1/2})\quad\mbox{as} \quad n\to\infty, \tag{1.7}\]

where \(\mu_{3}=E(X_{1}-\mu)^{3}\) and \(\phi(x)=(2\pi)^{-1/2}\exp(-x^{2}/2)\), \(x\in\mathbb{R}\). It can be shown that the conditional distribution of \(\tilde{R}_{n,n}^{*}\) given \(X_{1},\ldots,X_{n}\), also admits a similar expansion that is valid almost surely:

\[\sup_{x\in\mathbb{R}}\Big{|}P_{*}(\tilde{R}_{n}\leq x)-\Big{[} \Phi(x)-\frac{\hat{\mu}_{3,n}}{6\sqrt{n}\hat{\sigma}_{n}^{3}}(x^{2}-1)\phi(x) \Big{]}\Big{|}\] \[= o(n^{-1/2})\quad\mbox{as}\quad n\to\infty,\quad\mbox{a.s.}, \tag{1.8}\]

where \(\hat{\mu}_{3,n}=E_{*}(X_{1}^{*}-\bar{X}_{n})^{3}\). By the SLLN, as \(n\to\infty\), \(\hat{\mu}_{3,n}\to\mu_{3}\) a.s. and \(\hat{\sigma}_{n}^{2}\to\sigma^{2}\) a.s. Hence,

\[\Delta_{n} \leq \Big{|}\frac{\hat{\mu}_{3,n}}{\hat{\sigma}_{n}^{3}}-\frac{\mu_{3 }}{\sigma^{3}}\Big{|}\,\Big{\{}\sup_{x\in\mathbb{R}}|x^{2}-1|\phi(x)\Big{\}} \frac{1}{6\sqrt{n}}+o(n^{-1/2})\quad\mbox{a.s.}\] \[= o(n^{-1/2})\quad\mbox{a.s.}\]

\(\Box\)

Under the conditions of Theorem 17.1.2, the bootstrap approximation to the distribution of \(\tilde{R}_{n}\) outperforms the classical normal approximation, since the latter has an error of order \(O(n^{-1/2})\). In the literature,this property is referred to as the _second order correctness_ (s.o.c.) of the bootstrap. It can be shown that under additional conditions, the order of \(\Delta_{n}\) is \(O\bigl{(}n^{-1}\sqrt{\log\log n}\bigr{)}\) a.s., and therefore the bootstrap gives an accurate approximation even for small sample sizes where the asymptotic normal approximation is inadequate. For iid random variables with a smooth cdf \(F\), s.o.c. of the bootstrap has been established in more complex problems, such as in the case of the studentized sample mean \(T_{n}\equiv\sqrt{n}(\tilde{X}_{n}-\mu)/s_{n}\). For a detailed description of the second and higher order properties of the bootstrap for independent random variables, see Hall (1992).

#### Bootstrap for lattice distributions

Next consider the case where the underlying cdf \(F\) does not satisfy the nonlatticeness condition of Theorem 17.1.2. Then,

\[\bigl{|}E\exp(\iota tX_{1})\bigr{|}=1\quad\text{for some}\quad t\neq 0\]

and it can be shown (cf. Chapter 10) that \(X_{1}\) takes all its values in a lattice of the form \(\{a+jh:j\in\mathbb{Z}\}\), \(a\in\mathbb{R}\), \(h>0\). The smallest \(h>0\) satisfying

\[P\bigl{(}X_{1}\in\{a+jh:j\in\mathbb{Z}\}\bigr{)}=1\]

is called the _(maximal) span_ of \(F\). For the normalized sample mean of lattice random variables, s.o.c. of the bootstrap fails under standard metrics, such as the sup-norm metric and the Levy metric. Recall that for any two distribution functions \(F\) and \(G\) on \(\mathbb{R}\),

\[d_{L}(F,G)=\inf\{\epsilon>0:F(x-\epsilon)-\epsilon<G(x)<F(x+\epsilon)+\epsilon \quad\text{for all}\quad x\in\mathbb{R}\} \tag{1.9}\]

defines the Levy metric on the set of probability distribution on \(\mathbb{R}\).

**Theorem 17.1.3:** _Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid lattice random variables with span \(h\in(0,\infty)\) and let \(\tilde{R}_{n}\), \(\tilde{R}_{n,n}^{*}\), and \(\Delta_{n}\) be as in Theorem 17.1.1. If \(E(X_{1}^{4})<\infty\), then_

\[\limsup_{n\to\infty}\sqrt{n}\ \Delta_{n}=\frac{h}{\sqrt{2\pi\sigma^{2}}}\quad \text{a.s.} \tag{1.10}\]

_and_

\[\limsup_{n\to\infty}\sqrt{n}\ d_{L}(G_{n},\hat{G}_{n})=\frac{h}{\sigma(1+\sqrt {2\pi})}\quad\text{a.s.} \tag{1.11}\]

_where \(G_{n}(x)=P(\tilde{R}_{n}\leq x)\) and \(\hat{G}_{n}(x)=P_{*}(\tilde{R}_{n,n}^{*}\leq x)\), \(x\in\mathbb{R}\)._

Thus, the above theorem shows that for lattice random variables, the bootstrap approximation to the distribution of \(R_{n}\) may not be better than the normal approximation in the supremum and the Levy metric. In Theorem 17.1.3, relation (1.10) is due to Singh (1981) and (1.11) is due to Lahiri (1994).

**Proof:** Here again, an outline of the proof is given. By Theorem 11.4.5,

\[\sup_{x}\big{|}P(R_{n}\leq x)-\xi_{n}(x)\big{|}=o(n^{-1/2}) \tag{1.12}\]

where \(\xi_{n}(x)=\Phi(x)+\frac{\mu_{3}}{6\sigma^{3}\sqrt{n}}(1-x^{2})\phi(x)+\frac{h} {\sigma\sqrt{n}}Q([\sqrt{n}x\sigma-nx_{0}]/h)\phi(x)\), \(x\in\mathbb{R}\), \(Q(x)=\frac{1}{2}-x+\lfloor x\rfloor\), and \(x_{0}\in\mathbb{R}\) is such that \(P(X_{1}=x_{0}+\mu)>0\). Also, by Theorem 1 of Singh (1981),

\[\sup_{x}\big{|}P_{*}(R_{n,n}^{*}\leq x)-\hat{\xi}_{n}(x)\big{|}=o(n^{-1/2}) \quad\mbox{as}\quad n\to\infty,\quad\mbox{a.s.}, \tag{1.13}\]

where \(\hat{\xi}_{n}(x)=\Phi(x)+\frac{\hat{\mu}_{3,n}}{6s_{n}^{3}\sqrt{n}}(1-x^{2}) \phi(x)+\frac{h}{\sigma\sqrt{n}}Q(\sqrt{n}\,xs_{n}/h)\), \(x\in\mathbb{R}\) and \(\hat{\mu}_{3,n}=E_{*}(X_{1}^{2}-\bar{X}_{n})^{3}\). Hence, by (1.12) and (1.13),

\[\frac{\sigma\sqrt{n}}{h}\cdot\Delta_{n}=\sup_{x\in\mathbb{R}}\Big{|}Q([\sqrt{n }\,x\sigma-nx_{0}]/h)-Q(\sqrt{n}\,xs_{n}/h)\Big{|}\phi(x)+o(1)\quad\mbox{a.s.}\]

Since \(Q(x)\in[-\frac{1}{2},\frac{1}{2}]\) for all \(x\) and \(\phi(0)=\frac{1}{\sqrt{2\pi}}\), it is clear that

\[\limsup_{n\to\infty}\frac{\sigma\sqrt{n}}{h}\Delta_{n}\leq\frac{1}{\sqrt{2\pi}} \quad\mbox{a.s.} \tag{1.14}\]

To get the lower bound, suppose that for almost all realizations of \(X_{1},X_{2},\ldots\), and for any given \(\epsilon\), \(\delta\in(0,1)\), there is a sequence \(\{x_{n}\}_{n\geq 1}\in(0,\epsilon)\) such that

\[\sqrt{n}\,x_{n}s_{n}/h\in\mathbb{Z}\mbox{ and }\langle[\sqrt{n}\,x_{n}\sigma-nx_ {0}]/h\rangle\in(1-\delta,1)\quad\mbox{i.o.}, \tag{1.15}\]

where \(\langle y\rangle\) denotes the fractional part of \(y\), i.e., \(\langle y\rangle=y-\lfloor y\rfloor\), \(y\in\mathbb{R}\). Then, for each \(n\) satisfying (1.15),

\[\frac{\sigma\sqrt{n}}{h}\Delta_{n} \geq \Big{|}Q([\sqrt{n}\,x_{n}\sigma-nx_{0}]/h)-\frac{1}{2}\Big{|}\phi (x_{n})+o(1) \tag{1.16}\] \[\geq \inf_{y\in(1-\delta,1)}\Big{|}Q(y)-\frac{1}{2}\Big{|}\cdot\inf_{ x\in(0,\epsilon)}\phi(x)+o(1).\]

Since \(\lim_{y\to 1-}Q(y)=-\frac{1}{2}\), (1.14) and (1.16) together yield (1.10). The existence of the sequence \(\{x_{n}\}_{n\geq 1}\) satisfying (1.15) can be established by using the LIL. For an outline of the main arguments, see Problem 17.4.

Next consider (1.11). Let \(c_{n}=\|G_{n}-\hat{G}_{n}\|_{\infty}+n^{-1/2}\). Since \(d_{L}(G_{n},\hat{G}_{n})\leq\|G_{n}-\hat{G}_{n}\|\), \(d_{L}(G_{n},\hat{G}_{n})=\inf\{0<\epsilon<c_{n}:G_{n}(x-\epsilon)-\epsilon<\hat {G}_{n}(x+\epsilon)+\epsilon\mbox{ for all }x\in\mathbb{R}\}\). Using (1.12) and (1.13), it can be shown that for \(0<\epsilon<c_{n}\),

\[\hat{G}_{n}(x)<G_{n}(x+\epsilon)+\epsilon\quad\mbox{for all}\quad x\in \mathbb{R}\] \[\Leftrightarrow \big{(}Q(x)-Q(x\tau_{n}+(\sqrt{n}\,\epsilon\sigma-nx_{0})/h) \big{)}\phi(hx/s_{n}\sqrt{n})\] \[<h^{-1}\epsilon\sigma\sqrt{n}(1+\phi(hx/s_{n}\sqrt{n}))+o(1)\quad \mbox{for all}\quad x\in\mathbb{R},\]where \(\tau_{n}=\sigma/s_{n}\) and the \(o(1)\) term is uniform over \(x\in\mathbb{R}.\) Similarly, for \(0<\epsilon<c_{n},\)

\[G_{n}(x-\epsilon)-\epsilon<\hat{G}_{n}(x)\quad\mbox{for all}\quad x \in\mathbb{R}\] \[\Leftrightarrow \big{(}Q(x)-Q(x\tau_{n}-(\sqrt{n}\,\epsilon\sigma+nx_{0})/h) \big{)}\phi(hx/s_{n}\sqrt{n})\] \[>h^{-1}\epsilon\sigma\sqrt{n}\big{(}1+\phi(hx/s_{n}\sqrt{n})\big{)} +o(1)\quad\mbox{for all}\quad x\in\mathbb{R}.\]

It is easy to check that (1.17) and (1.18) are satisfied if \(\epsilon=h\big{\{}(1+\sqrt{2\pi})\sigma\sqrt{n}\big{\}}^{-1}+o(n^{-1/2}).\) Hence,

\[\limsup_{n\to\infty}\sqrt{n}\,d_{L}(G_{n},\hat{G}_{n})\leq h\big{(}\sigma(1+ \sqrt{2\pi})\big{)}^{-1}\quad\mbox{a.s.}\]

To prove the reverse inequality, note that if there exists a \(\eta>0\) such that, with \(\epsilon_{n}=d_{L}(G_{n},G_{n})+n^{-1},\)

\[\big{(}Q(\tilde{x})-Q(\tau_{n}\tilde{x}+(\sqrt{n}\,\epsilon_{n}\sigma-nx_{0})/h )\big{)}\phi(h\tilde{x}/s_{n}\sqrt{n})\geq(1-\eta)\sqrt{2\pi}\]

for some \(\tilde{x}\epsilon\mathbb{R},\) then

\[h^{-1}\epsilon_{n}\sigma\sqrt{n}\big{(}1+\phi(h\tilde{x}/s_{n} \sqrt{n})\big{)}>(1-\eta)/\sqrt{2\pi}+o(1)\] \[\Rightarrow h^{-1}\sigma\epsilon_{n}\sqrt{n}\big{(}1+\phi(0)\big{)}>(1-\eta) \sqrt{2\pi}+o(1)\] \[\Rightarrow \epsilon_{n}>(1-\eta)h(1+\sqrt{2\pi})^{-1}(\sigma\sqrt{n})^{-1}+o (n^{-1/2})\] \[\Rightarrow d_{L}(G_{n},\hat{G}_{n})>(1-\eta)h(1+\sqrt{2\pi})^{-1}(\sigma \sqrt{n})^{-1}+o(n^{-1/2}).\]

Thus, it is enough to show that for almost all sample sequences, there exists a sequence \(\{x_{n}\}_{n\geq 1}\in\mathbb{R}\) such that with \(\epsilon_{n}=d(\hat{G}_{n},G_{n})+n^{-1},\)

\[\overline{\lim}_{n\to\infty}\,\big{(}Q(x_{n})-Q(\tau_{n}x_{n}+( \sqrt{n}\,\epsilon_{n}\sigma-nx_{0})/h)\big{)}\phi(x_{n}h/s_{n}\sqrt{n})\] \[=\frac{1}{\sqrt{2\pi}}. \tag{1.19}\]

Since \(\overline{\lim}_{n\to\infty}\sqrt{n}(\tau_{n}-1)=\infty\) a.s., for almost all sample sequences, there exists a subsequence \(\{n_{m}\}\) such that \(\limsup_{m\to\infty}\sqrt{n_{m}}(\tau_{n_{m}}-1)=\infty,\) and \(\tau_{n_{m}}>1\) for all \(m.\) Let \(a_{n}\) denote the fractional part of \(\big{(}\sqrt{n}\,\epsilon_{n}\sigma-nx_{0}\big{)}/h.\) Then \(0\leq a_{n}<1.\) Define a sequence of integers \(\{x_{n}\}_{n\geq 1}\) by

\[x_{n}=\lfloor(1-a_{n})/(\tau_{n}-1)\rfloor-1,\quad n\geq 1.\]

Then, it is easy to see that

\[(1-a_{n_{m}})-2(\tau_{n_{m}}-1)\leq x_{n_{m}}(\tau_{n_{m}}-1)\leq(1-a_{n_{m}} )-(\tau_{n_{m}}-1),\]

and

\[\lim_{m\to\infty}Q(x_{n_{m}}\tau_{n_{m}}+a_{n_{m}})=\lim_{m\to\infty}Q\big{(}x _{n_{m}}(\tau_{n_{m}}-1)-1-a_{n_{m}}\big{)}\big{)}=-1/2. \tag{1.20}\]

Now, one can use (1.20) to prove (1.19). This completes the proof.

#### Bootstrap for heavy tailed random variables

Next consider the case where the \(X_{i}\)'s are heavy tailed, i.e., the \(X_{i}\)'s have infinite variance. More specifically, let \(\{X_{n}\}_{n\geq 1}\) be iid with common cdf \(F\) such that as \(x\to\infty\),

\[1-F(x) \sim x^{-2}L(x)\] \[F(-x) \sim cx^{-2}L(x) \tag{1.21}\]

for some \(\alpha\in(1,2)\), \(c\in(0,\infty)\) and for some function \(L(\cdot):(0,\infty)\to(0,\infty)\) that is _slowly varying_ at \(\infty\), i.e., \(L(\cdot)\) is bounded on every bounded interval of \((0,\infty)\) and

\[\lim_{y\to\infty}L(ty)/L(y)=1\quad\text{for all}\quad t\in(0,\infty). \tag{1.22}\]

Since \(\alpha\in(1,2)\), \(E|X_{1}|<\infty\), but \(EX_{1}^{2}=\infty\). Thus, the \(X_{i}\)'s have infinite variance but finite mean. In this case, it is known (cf. Chapter 11) that for some sequence \(\{a_{n}\}_{n\geq 1}\) of normalizing constants

\[T_{n}\equiv\frac{n(\bar{X}_{n}-\mu)}{a_{n}}\longrightarrow^{d}W_{\alpha}, \tag{1.23}\]

where \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\), \(\mu=EX_{1}\) and \(W_{\alpha}\) has a stable distribution of order \(\alpha\). The characteristic function of \(W_{\alpha}\) is given by

\[\phi_{\alpha}(t)=\exp\Big{(}\int h(t,x)d\lambda_{\alpha}(x)\Big{)},\ \ t\in \mathbb{R}, \tag{1.24}\]

where \(h(t,x)=\big{(}e^{\iota tx}-1-\iota t\big{)}\), \(x,t\in\mathbb{R}\) and where \(\lambda_{\alpha}(\cdot)\) is a measure on \(\big{(}\mathbb{R},\mathcal{B}(\mathbb{R})\big{)}\) satisfying

\[\lambda_{\alpha}\big{(}[x,\infty)\big{)}=x^{-\alpha}\quad\text{and}\quad \lambda_{\alpha}\big{(}(-\infty,-x]\big{)}=cx^{-\alpha},\ \ x>0. \tag{1.25}\]

The normalizing constants \(\{a_{n}\}_{n\geq 1}\subset(0,\infty)\) in (1.23) are determined by the relation

\[nP(X_{1}>a_{n})\equiv na_{n}^{-2}L(a_{n})\to 1\quad\text{as}\quad n\to\infty. \tag{1.26}\]

To apply the bootstrap, let \(X_{1}^{*},\ldots,X_{m}^{*}\) denote a random sample of size \(m\), drawn with replacement form \(\{X_{1},\ldots,X_{n}\}\). Then, the bootstrap version of \(T_{n}\) is given by

\[T_{m,n}^{*}=m(\bar{X}_{m}^{*}-\bar{X}_{n})/a_{m} \tag{1.27}\]

where \(\bar{X}_{m}^{*}=m^{-1}\sum_{i=1}^{m}X_{i}^{*}\) is the bootstrap sample mean. The main result of this section shows that \(\mathcal{L}(T_{m,n}^{*}\mid X_{1},\ldots,X_{n})\), the conditional distribution of \(T_{m,n}^{*}\) given \(X_{1},\ldots,X_{n}\) has a _random_ limit for the usual choice of theresample size, \(m=n\) and, hence, is an "inconsistent estimator" of \({\cal L}(T_{n})\) the distribution of \(T_{n}\). In contrast, consistency of the bootstrap approximation holds if \(m=o(n)\), i.e., the resample size is of smaller order than the sample size. To describe the random limit in the \(m=n\) case, the notion of a random Poisson measure is needed, which is introduced in the following definition.

**Definition 17.1.1:**\(M(\cdot)\) is called a _random Poisson measure_ on \(\big{(}\mathbb{R},{\cal B}(\mathbb{R})\big{)}\) with mean measure \(\lambda_{\alpha}(\cdot)\) of (1.25), if \(\{M(A):A\in{\cal B}(\mathbb{R})\}\) is a collection of random variables defined on some probability space \((\Omega,{\cal F},P)\) such that for each \(w\in\Omega\), \(M(\cdot)(w)\) is a measure on \(\big{(}\mathbb{R},{\cal B}(\mathbb{R})\big{)}\), and for any disjoint finite collection of sets \(A_{1},\ldots,A_{m}\in{\cal B}(\mathbb{R})\), \(m\in\mathbb{N}\), \(\{M(A_{1}),\ldots,M(A_{m})\}\) are independent Poisson random variables and \(M(A_{i})\sim\mbox{Poisson}\big{(}\lambda_{\alpha}(A_{i})\big{)}\), \(i=1,\ldots,m\).

**Theorem 17.1.4:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables satisfying (1.21) for some \(\alpha\in(1,2)\) and let \(T_{n}\), \(T^{*}_{m,n}\) be as in (1.23) and (1.27), respectively._

* _If_ \(m=o(n)\) _as_ \(n\to\infty\)_, then_ \[\sup_{x}\big{|}P_{*}(T^{*}_{m,n}\leq x)-P(T_{n}\leq x)\big{|} \longrightarrow_{p}0\quad\mbox{as}\quad n\to\infty.\] (1.28)
* _Suppose that_ \(m=n\) _for all_ \(n\geq 1\)_. Then, for any_ \(t\in\mathbb{R}\)_,_ \[E_{*}\exp\big{(}\iota tT^{*}_{n,n}\big{)}\longrightarrow^{d}\exp\Big{(}\int h (t,x)dM(x)\Big{)},\] (1.29) _where_ \(M(\cdot)\) _is the random Poisson measure defined above and_ \(h(t,x)=(e^{\iota tx}-1-\iota t)\)_._

**Remark 17.1.1:** Part (ii) of Theorem 17.1.4 shows that the bootstrap fails to provide a valid approximation to the distribution of the normalized sample mean of heavy tailed random variables when \(m=n\). Failure of the bootstrap in the heavy tail case was first proved by Athreya (1987_a_), who established weak convergence of the finite dimensional vectors \(\big{(}P_{*}(T^{*}_{n,n}\leq x_{1}),\ldots,P_{*}(\tilde{T}^{*}_{n,n}\leq x_{m })\big{)}\) based on a slightly different bootstrap version \(\tilde{T}^{*}_{n,n}\) of \(T_{n}\), where \(\tilde{T}^{*}_{n,n}=n(\bar{X}^{*}-\bar{X}_{n})/X_{(n)}\) and \(X_{(n)}=\max\{X_{1},\ldots,X_{n}\}\). Extensions of these results are given in Athreya (1987_b_) and Arcones and Gine (1989, 1991). A necessary and sufficient condition for the validity of the bootstrap for the sample mean with resample size \(m=n\) is that \(X_{1}\) belongs to the domain of attraction of the normal distribution (Gine and Zinn (1989)).

**Proof of Theorem 17.1.4:** W.l.o.g., let \(\mu=0\). Let

\[\hat{\phi}_{m,n}(t) = E_{*}\exp\big{(}\iota t(X^{*}_{1}-\bar{X}_{n})/a_{m}\big{)}\]\[= n^{-1}\sum_{j=1}^{n}\exp\big{(}\iota t[X_{1}-\bar{X}_{n}]/a_{m}\big{)}, \;\;\;t\in\mathbb{R}.\]

Then,

\[E_{*}\exp\big{(}\iota tT_{m,n}^{*}\big{)} = \big{(}\hat{\phi}_{m,n}(t)\big{)}^{m} \tag{1.30}\] \[= \Big{[}1-\frac{1}{m}\big{\{}m(1-\hat{\phi}_{m,n}(t))\big{\}} \Big{]}^{m}.\]

First consider part (ii). Note that for any sequence of complex numbers \(\{z_{n}\}_{n\geq 1}\) satisfying, \(z_{n}\to z\in\mathbb{C}\),

\[\Big{(}1+\frac{z_{n}}{n}\Big{)}^{n}\Big{/}e^{z_{n}}\to 1\quad\mbox{as}\quad n\to\infty. \tag{1.31}\]

Hence, by (1.30) (with \(m=n\)) and (1.31), (ii) would follow if

\[n\big{(}\hat{\phi}_{n,n}(t)-1\big{)}\longrightarrow^{d}\int h(x,t)M_{\alpha}( dx). \tag{1.32}\]

Since \(E_{*}(X_{1}^{*}-\bar{X}_{n})=0\), it follows that

\[n\big{(}\hat{\phi}_{n,n}(t)-1\big{)} \tag{1.33}\] \[= n\Big{[}E_{*}\exp\big{(}\iota t[X_{1}^{*}-\bar{X}_{n}]/a_{n} \big{)}-1-\iota tE_{*}\big{(}[X_{1}^{*}-\bar{X}_{n}]/a_{n}\big{)}\Big{]}\] \[= \int h(x,t)dM_{n}(x),\]

where \(M_{n}(A)=\sum_{j=1}^{n}I\big{(}[X_{j}-\bar{X}_{n}]/a_{n}\in A\big{)}\), \(A\in\mathcal{B}(\mathbb{R})\). Note that for a given \(t\in\mathbb{R}\), the function \(h(\cdot,t)\) is continuous on \(\mathbb{R}\), \(|h(x,t)|=O(x^{2})\) as \(x\to 0\) and \(|h(x,t)|=O(|x|)\) as \(|x|\to\infty\). Hence, to prove (1.32), it is enough to show that for any \(\epsilon>0\),

\[\lim_{\eta\downarrow 0}\;\limsup_{n\to\infty}P\Big{(}\int_{|x| \leq\eta}x^{2}dM_{n}(x)>\epsilon\Big{)}=0, \tag{1.34}\] \[\lim_{\eta\downarrow 0}\;\limsup_{n\to\infty}P\Big{(}\int_{\eta|x| >1}|x|dM_{n}(x)>\epsilon\Big{)}=0, \tag{1.35}\]

and for any disjoint intervals \(I_{1},\ldots,I_{k}\) whose closures \(\bar{I}_{1},\ldots,\bar{I}_{k}\) are contained in \(\mathbb{R}\setminus\{0\}\),

\[\big{\{}M_{n}(I_{1}),\ldots,M_{n}(I_{k})\big{\}}\longrightarrow^{d}\big{\{}M_{ \alpha}(I_{1}),\ldots,M_{\alpha}(I_{k})\big{\}}. \tag{1.36}\]

Let \(A_{n}=\{|\bar{X}_{n}|\leq n^{-3/4}a_{n}\}\). Since \(n\bar{X}_{n}/a_{n}\longrightarrow^{d}W_{\alpha}\),

\[P(A_{n}^{c})\to 0\quad\mbox{as}\quad n\to\infty. \tag{1.37}\]Consider (1.34) and fix \(\eta\in(0,\infty)\). Then, on the set \(A_{n}\), for \(n^{1/2}>\eta\),

\[\int_{|x|\leq\eta}x^{2}dM_{n}(x) = a_{n}^{-2}\sum_{j=1}^{n}(X_{j}-\bar{X}_{n})^{2}I(|X_{j}-\bar{X}_{n }|\leq a_{n}\eta)\] \[\leq 2a_{n}^{-2}\sum_{j=1}^{n}X_{j}^{2}I\big{(}|X_{j}|\leq(|\bar{X}_{n }|+a_{n}\eta)\big{)}+2a_{n}^{-2}n\cdot\bar{X}_{n}^{2}\] \[\leq 2a_{n}^{-2}\sum_{j=1}^{n}X_{j}^{2}I(|X_{j}|\leq 2\eta a_{n})+2n^ {-1/2}.\]

Hence,

\[\limsup_{n\to\infty}P\Big{(}\int_{|x|\leq\eta}x^{2}dM_{n}(x)> \epsilon\Big{)} \tag{1.38}\] \[\leq \limsup_{n\to\infty}\Big{[}P\Big{(}\Big{\{}\int_{|x|\leq\eta}x^{2 }dM_{n}(x)>\epsilon\Big{\}}\cap A_{n}\Big{)}+P(A_{n}^{e})\Big{]}\] \[\leq \limsup_{n\to\infty}P\Big{(}2a_{n}^{-2}\sum_{j=1}^{n}X_{j}^{2}I(| X_{j}|\leq 2\eta a_{n})>\frac{\epsilon}{2}\Big{)}\] \[\leq \limsup_{n\to\infty}\frac{4}{\epsilon}\,na_{n}^{-2}EX_{1}^{2}I(|X _{1}|\leq 2\eta a_{n}).\]

By (1.21) and (1.26), \(na_{n}^{-2}EX_{1}^{2}I(|X_{1}|\leq 2\eta a_{n})\) is asymptotically equivalent to \(C_{1}\cdot na_{n}^{-2}(\eta a_{n})^{2-\alpha}L(2\eta a_{n})\) for some \(C_{1}\in(0,\infty)\) (not depending on \(\eta\)). Hence, by (1.21) and (1.22), the right side of (1.38) is bounded above by \(\frac{4}{\epsilon}\cdot C_{1}\cdot\eta^{2-\alpha}\), which \(\to 0\) as \(\eta\downarrow 0\). Hence, (1.34) follows.

By similar arguments,

\[\limsup_{n\to\infty}P\Big{(}\int_{\eta|x|>1}|x|dM_{n}(x)>\epsilon \Big{)}\] \[\leq \limsup_{n\to\infty}\frac{2}{\epsilon}\,na_{n}^{-1}E|X_{1}|I(|X_{ 1}|>\eta^{-1}a_{n}/2)\] \[\leq \mbox{const. }\eta^{\alpha-1},\]

which \(\to 0\) as \(\eta\downarrow 0\). Hence, (1.35) follows.

Next consider (1.36). Let \(M_{n}^{\dagger}(A)\equiv\sum_{j=1}^{n}I(X_{j}/a_{n}\in A)\), \(A\in{\cal B}(\mathbb{R})\). Then, for any \(a<b\), on the set \(A_{n}\),

\[M_{n}^{\dagger}\big{(}[a+\epsilon_{n},b-\epsilon_{n}]\big{)}\leq M_{n}\big{(}[ a,b]\big{)}\leq M_{n}^{\dagger}\big{(}[a-\epsilon_{n},b+\epsilon_{n}]\big{)}\]

for \(\epsilon_{n}=n^{-3/4}\). Further, by (1.21), (1.22), for any \(x\neq 0\),

\[EM_{n}^{\dagger}\big{(}[x-\epsilon_{n},x+\epsilon_{n}]\big{)}\] \[= nP\big{(}X_{1}\in a_{n}[x-\epsilon_{n},x+\epsilon_{n}]\big{)}\to 0 \quad\mbox{as}\quad n\to\infty.\]

[MISSING_PAGE_FAIL:551]

Also, the expected value of the first term on the right side above equals \(2ma_{m}^{-2}EX_{1}^{2}I(|X_{1}|\leq 2a_{m}\eta)\), which is asymptotically equivalent to const. \(\eta^{2-\alpha}\). Hence, it follows that (1.34) holds with \(M_{n}\) replaced by \(\tilde{M}_{n}\). Similarly, one can establish (1.35) for \(\tilde{M}_{n}\). Hence, to prove (1.41), it remains to show that (cf. (1.36)) for any interval \(I_{1}\) whose closure is contained in \(\mathbb{R}\setminus\{0\}\),

\[\tilde{M}_{n}(I_{1})\longrightarrow_{p}\lambda_{\alpha}(I_{1}). \tag{1.43}\]

By (1.23) and (1.42), \(\bar{X}_{n}/a_{m}=\frac{n\bar{X}_{n}}{a_{n}}\frac{a_{n}}{a_{m}}\frac{1}{n} \longrightarrow_{p}0\) as \(n\to\infty\). Hence, by arguments as in the proof of (1.36), it is enough to show that

\[\tilde{M}_{n}^{\dagger}(I_{1})\longrightarrow_{p}\lambda_{\alpha}(I_{1}), \tag{1.44}\]

where \(\tilde{M}_{n}^{\dagger}(A)=\frac{m}{n}\sum_{i=1}^{n}I(X_{i}/a_{m}\in A)\), \(A\in\mathcal{B}(\mathbb{R})\). In view of (1.21) and (1.26), this can be easily verified by showing

\[E\tilde{M}_{n}^{\dagger}(I_{1})\to\lambda_{\alpha}(I_{1})\ \ \mbox{and}\ \ \mbox{Var}\big{(}\tilde{M}_{n}^{\dagger}(I_{1})\big{)}=O\Big{(}\frac{m}{n}\Big{)}\ \ \mbox{as}\ \ n\to\infty, \tag{1.45}\]

and hence \(\to 0\) as \(n\to\infty\). Hence, part (i) of Theorem 17.1.4 follows. \(\Box\)

### 17.2 Inadequacy of resampling single values under dependence

The resampling scheme, introduced by Efron (1979) for iid random variables, may not produce a reasonable approximation under dependence. An example to this effect was given by Singh (1981), which is described next. Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of stationary \(m\)-dependent random variables for some \(m\in\mathbb{N}\), with \(EX_{1}=\mu\) and \(EX_{1}^{2}<\infty\).

Recall that \(\{X_{n}\}_{n\geq 1}\) is called \(m\)-_dependent_ for some integer \(m\geq 0\) if \(\{X_{1},\ldots,X_{k}\}\) and \(\{X_{k+m+1,\ldots}\}\) are independent for all \(k\geq 1\). Thus, any sequence of independent random variables \(\{\epsilon_{n}\}_{n\geq 1}\) is \(0\)-dependent and if \(X_{n}\equiv\epsilon_{n}+0.5\epsilon_{n+1}\), \(n\geq 1\), then \(\{X_{n}\}_{n\geq 1}\) is \(1\)-dependent. For an \(m\)-dependent sequence \(\{X_{n}\}_{n\geq 1}\) with finite variances, it can be checked that

\[\sigma_{\infty}^{2}\equiv\lim_{n\to\infty}n\mbox{Var}(\bar{X}_{n})=\mbox{Var}(X_{1})+2\sum_{i=1}^{m}\mbox{ Cov}(X_{1},X_{1+i}),\]

where \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\). If \(\sigma_{\infty}^{2}\in(0,\infty)\), then by the CLT for \(m\)-dependent variables (cf. Chapter 16),

\[\sqrt{n}(\bar{X}_{n}-\mu)\longrightarrow^{d}N(0,\sigma_{\infty}^{2}). \tag{2.1}\]

Now consider the bootstrap approximation to the distribution of the random variable \(T_{n}=\sqrt{n}(\bar{X}_{n}-\mu)\) under Efron (1979)'s resampling scheme.

For simplicity, assume that the resample size equals the sample size, i.e., from \((X_{1},\ldots,X_{n})\), an equal number of bootstrap variables \(X_{1}^{*},\ldots,X_{n}^{*}\) are generated by resampling one a single observation at a time. Then, the bootstrap version \(T_{n,n}^{*}\) of \(T_{n}\) is given by

\[T_{n,n}^{*}=\sqrt{n}(\bar{X}_{n}^{*}-\bar{X}_{n}),\]

where \(\bar{X}_{n}^{*}=n^{-1}\sum_{i=1}^{n}X_{i}^{*}\). The conditional distribution of \(T_{n,n}^{*}\) still converges to a normal distribution, but with a "wrong" variance, as shown below.

**Theorem 17.2.1:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of stationary \(m\)-dependent random variables with \(EX_{1}=\mu\) and \(\sigma^{2}=\mbox{Var}(X_{1})\in(0,\infty)\), where \(m\in\mathbb{Z}_{+}\). Then_

\[\sup_{x}\big{|}P_{*}(T_{n,n}^{*}\leq x)-\Phi(x/\sigma)\big{|}=o(1)\quad\mbox{ as}\quad n\to\infty,\quad\mbox{a.s.} \tag{2.2}\]

_Note that, if \(m\geq 1\), then \(\sigma^{2}\) need not equal \(\sigma^{2}_{\infty}\)._

For proving the theorem, the following result will be needed.

**Lemma 17.2.2:**_Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of stationary \(m\)-dependent random variables. Let \(f:\mathbb{R}\to\mathbb{R}\) be a Borel measurable function with \(E|f(X_{1})|^{p}<\infty\) for some \(p\in(0,2)\), such that \(Ef(X_{1})=0\) if \(p\geq 1\). Then,_

\[n^{-1/p}\sum_{i=1}^{n}f(X_{i})\to 0\quad\mbox{as}\quad n\to\infty,\quad\mbox{a.s.}\]

**Proof:** This is most easily proved by splitting the given \(m\)-dependent sequence \(\{X_{n}\}_{n\geq 1}\) into \(m+1\) iid subsequences \(\{Y_{ji}\}_{i\geq 1}\), \(j=1,\ldots,m+1\), defined by \(Y_{ji}=X_{j+(i-1)(m+1)}\), and then applying the standard results for iid random variables to \(\{Y_{ji}\}_{i\geq 1}\)'s (cf. Liu and Singh (1992)). For \(1\leq j\leq m+1\), let \(A_{jn}=\{1\leq i\leq n:j+(i-1)(m+1)\leq n\}\) and let \(N_{jn}\) denote the size of the set \(A_{jn}\). Note that \(N_{jn}/n\to(m+1)^{-1}\) as \(n\to\infty\) for all \(1\leq j\leq m+1\). Then, by the Marcinkiewz-Zygmund SLLN (cf. Chapter 8) applied to each of the sequences of iid random variables \(\{Y_{ji}\}_{i\geq 1}\), \(j=1,\ldots,m+1\), one gets

\[n^{-1/p}\sum_{i=1}^{n}f(X_{i})\] \[= \sum_{j=1}^{m+1}\Big{[}N_{jn}^{-1/p}\sum_{i\in A_{jn}}f(Y_{ji}) \Big{]}\cdot(N_{jn}/n)^{1/p}\to 0\ \ \mbox{as}\ \ n\to\infty,\ \ \mbox{a.s.}\]

This completes the proof of Lemma 17.2.2.

**Proof of Theorem 17.2.1:** Note that conditional on \(X_{1},\ldots,X_{n}\), \(X_{1}^{*},\ldots,X_{n}^{*}\) are iid random variables with \(E_{*}X_{1}^{*}=\bar{X}_{n}\), \(\mbox{Var}_{*}(X_{1}^{*})=s_{n}^{2}\), and \(E_{*}|X_{1}^{*}|^{3}=n^{-1}\sum_{i=1}^{n}|X_{i}|^{3}<\infty\). Hence, by the Berry-Esseen theorem,

\[\sup_{x}\big{|}P_{*}(T_{n,n}^{*}\leq x)-\Phi(x/s_{n})\big{|}\leq(2.75)\ \frac{E_{*}|X_{1}^{*}-\bar{X}_{n}|^{3}}{\sqrt{n}\,s_{n}^{3}}. \tag{2.3}\]

By Lemma 17.2.2, w.p. 1,

\[\bar{X}_{n}\to\mu,\ n^{-1}\sum_{i=1}^{n}X_{i}^{2}\to EX_{1}^{2},\ \ \mbox{and}\ \ n^{-3/2}\sum_{i=1}^{n}|X_{i}|^{3}\to 0 \tag{2.4}\]

as \(n\to\infty\). Hence, the theorem follows from (2.3) and (2.4). \(\Box\)

The following result is an immediate consequence of Theorem 17.2.1 and (2.1).

**Corollary 17.2.3:**_Under the conditions of Theorem 17.2.1, if \(\sigma_{\infty}^{2}\neq 0\) and \(\sum_{i=1}^{m}\mbox{Cov}(X_{1},X_{1+i})\neq 0\), then for any \(x\neq 0\),_

\[\lim_{n\to\infty}\big{|}P_{*}(T_{n,n}^{*}\leq x)-P(T_{n}\leq x)\big{|}=\big{|} \Phi(x/\sigma)-\Phi(x/\sigma_{\infty})\big{|}\neq 0\ \ \mbox{ a.s.}\]

Thus, for all \(x\neq 0\), the bootstrap estimator \(P_{*}(T_{n,n}^{*}\leq x)\) of \(P(T_{n}\leq x)\) based on Efron (1979)'s resampling scheme has an error that tends to a _nonzero_ number in the limit. As a result, the bootstrap estimator of \(P(T_{n}\leq x)\) is not consistent. By resampling individual \(X_{i}\)'s, Efron (1979)'s resampling scheme ignores the dependence structure of the sequence \(\{X_{n}\}_{n\geq 1}\) completely, and thus, fails to account for the lag-covariance terms (viz., \(\mbox{Cov}(X_{1},X_{1+i})\), \(1\leq i\leq m\)) in the asymptotic variance.

### Block bootstrap

A new type of resampling scheme that is applicable to a wide class of dependent random variables is given by the block bootstrap methods. Although the idea of using blocks in statistical inference problems for time series is very common (cf. Brillinger (1975)), the development of a suitable version of resampling based on blocks has been slow. In a significant breakthrough, Kunsch (1989) and Liu and Singh (1992) independently formulated a block resampling scheme, called the _moving block bootstrap_ (MBB). In contrast with resampling a single observation at a time, the MBB resamples blocks of consecutive observations at a time, thereby preserving the dependence structure of the original observations within each block. Further, by allowing the block size to grow to infinity, the MBB is able to reproduce the dependence structure of the underlying process asymptotically. Essentially

[MISSING_PAGE_FAIL:555]

the MBB estimators of the variance and of the distribution of the sample mean will be proved.

#### Consistency of MBB variance estimators

For \(T_{n}=\sqrt{n}(\bar{X}_{n}-\mu)\), the MBB variance estimator \(\mbox{Var}_{*}(T_{n}^{*})\) has the desirable property that it can be expressed by simple, closed-form formulas involving the observations. This is possible because of the linearity of the bootstrap sample mean in the resampled observations. Let \(U_{i}=(X_{i}+\cdots+X_{i+\ell-1})/\ell\) denote the average of the block \((X_{i},\ldots,X_{i+\ell-1})\), \(i\geq 1\). Then, using the independence of the resampled blocks, one gets

\[\mbox{Var}_{*}(T_{n}^{*})=\ell\bigg{[}\frac{1}{N}\sum_{i=1}^{N}U_{i}^{2}-\tilde {\mu}_{n}^{2}\bigg{]}, \tag{4.1}\]

where \(N=n-\ell+1\) and when \(\tilde{\mu}_{n}=N^{-1}\sum_{i=1}^{n}U_{i}\) is as in (3.3). Under the conditions of Corollary 16.3.6, the asymptotic variance of \(T_{n}\) is given by the infinite series

\[\sigma_{\infty}^{2}\equiv\lim_{n\to\infty}\mbox{Var}(T_{n})=\sum_{i=-\infty}^{ \infty}EZ_{1}Z_{1+i}, \tag{4.2}\]

where \(Z_{i}=X_{i}-\mu\), \(i\in\mathbb{Z}\). The following result proves consistency of the MBB estimator of the 'level 2' parameter \(\mbox{Var}(T_{n})\) or, equivalently, of \(\sigma_{\infty}^{2}\).

**Theorem 17.4.1:** _Suppose that there exists a \(\delta>0\) such that \(E|X_{1}|^{2+\delta}<\infty\) and that \(\sum_{n=1}^{\infty}\alpha(n)^{\delta/2+\delta}<\infty\). If, in addition, \(\ell^{-1}+n^{-1}\ell=o(1)\) as \(n\to\infty\), then_

\[\mbox{Var}_{*}(T_{n}^{*})\longrightarrow_{p}\sigma_{\infty}^{2}\quad\mbox{as} \quad n\to\infty. \tag{4.3}\]

Theorem 17.4.1 shows that under mild moment and strong mixing conditions on the process \(\{X_{i}\}_{i\in\mathbb{Z}}\), the bootstrap variance estimators \(\mbox{Var}_{*}(T_{n}^{*})\), are consistent for a wide range of bootstrap block sizes \(\ell\), so long as \(\ell\) tends to infinity with \(n\) but at a rate slower than \(n\). Thus, block sizes given by \(\ell=\log\log n\) or \(\ell=n^{1-\epsilon}\), \(0<\epsilon<1\), are all admissible block lengths for the consistency of \(\mbox{Var}_{*}(T_{n}^{*})\).

For proving the theorem, the following lemma from Lahiri (2003) is needed.

**Lemma 17.4.2:** _Let \(f:\mathbb{R}\to\mathbb{R}\) be a Borel measurable function and let \(\{X_{i}\}_{i\in\mathbb{Z}}\) be a (possibly nonstationary) sequence of random vectors with strong mixing coefficient \(\alpha(\cdot)\). Define \(\|f\|_{\infty}=\sup\{|f(x)|:x\in\mathbb{R}\}\) and \(\zeta_{2+\delta,n}=\max\big{\{}\big{(}E|f(U_{1i})|^{2+\delta}\big{)}^{1/(2+ \delta)}:1\leq i\leq N\big{\}}\), \(\delta>0\), where \(U_{1i}\equiv\sqrt{\ell}\,U_{i}=(X_{i}+\cdots+X_{i+\ell-1})/\sqrt{\ell}\), \(i\geq 1\). Let \(\{a_{in}:i\geq 1,n\geq 1\}\subset[-1,1]\) be a collection of real numbers. Then, there exist constants \(C_{1}\) and (not depending on \(f(\cdot)\), \(\ell\), \(n\), and \(a_{in}\)'s), such that for any \(1<\ell<n/2\) and any \(n>2\),_

\[\mbox{Var}\bigg{(}\sum_{i=1}^{N}a_{in}f(U_{1i})\bigg{)} \leq \min\bigg{\{}C_{1}\|f\|_{\infty}^{2}n\ell\Big{[}1+\sum_{1\leq k \leq n/\ell}\alpha(k\ell)\Big{]},\] \[C_{2}(\delta)\zeta_{2+\delta,n}^{2}n\ell\Big{[}1+\sum_{1\leq k \leq n/\ell}\alpha(k\ell)^{\delta/(2+\delta)}\Big{]}\bigg{\}}.\]

**Proof:** Let

\[S(j)=\sum_{i=2(j-1)\ell+1}^{2j\ell}a_{in}f(U_{1i}),\ \ 1\leq j\leq J, \tag{4.5}\]

where \(J=\lfloor N/2\ell\rfloor\) and let \(S(J+1)=\sum_{i=1}^{n}a_{in}f(U_{1i})-\sum_{j=1}^{J}S(j)\). Also, let \(\sum^{(1)}\) and \(\sum^{(2)}\) respectively denote summation over even and odd \(j\in\{1,\ldots,J+1\}\). Note that for any \(1\leq j\), \(j+k\leq J+1\), \(k\geq 2\), the random variables \(S(j)\) and \(S(j+k)\) depend on disjoint sets of \(X_{i}\)'s that are separated by \((k-1)2\ell-\ell\) observations in between. Hence, noting that \(|S(j)|\leq 2\ell\|f\|_{\infty}\) for all \(1\leq j\leq J+1\), by Corollary 16.2.4, one gets \(|{\rm Cov}(S(j),S(j+k))|\leq 4\alpha((k-1)2\ell-\ell)(4\ell\|f\|_{\infty})^{2}\) for all \(k\geq 2\), \(j\geq 1\). Hence,

\[\mbox{Var}\bigg{(}\sum_{i=1}^{n}a_{in}f(U_{1i})\bigg{)} \tag{4.6}\] \[= \mbox{Var}\bigg{(}\sum^{(1)}S(j)+\sum^{(2)}S(j)\bigg{)}\] \[\leq 2\Big{[}\mbox{Var}\Big{(}\sum^{(1)}S(j)\Big{)}+\mbox{Var}\Big{(} \sum^{(2)}S(j)\Big{)}\Big{]}\] \[\leq 2\bigg{[}\sum_{i=1}^{J+1}ES(j)^{2}+(J+1)\cdot\sum_{1\leq k\leq J /2}\alpha\big{(}(2k-1)2\ell-\ell\big{)}\cdot 4(4\ell\|f\|_{\infty})^{2}\bigg{]}\] \[\leq 2\bigg{[}\Big{(}\frac{n}{2\ell}+1\Big{)}4\ell^{2}\|f\|_{\infty} ^{2}+64(n\ell)\|f\|_{\infty}^{2}\sum_{k=1}^{J}\alpha(k\ell)\bigg{]}\] \[\leq C_{1}\|f\|_{\infty}^{2}\cdot\bigg{[}n\ell+(n\ell)\sum_{k=1}^{J} \alpha(k\ell)\bigg{]}.\]

This yields the first term in the upper bound. The second term can be derived similarly by using the inequalities \(\big{|}{\rm Cov}\big{(}S(j),S(j+k))\big{|}\leq C(\delta)\big{(}ES(j)^{2+ \delta}\big{)}^{1/(2+\delta)}\big{(}ES(j+k)^{2+\delta}\big{)}\alpha\big{(}(k-1 )2\ell-\ell\big{)}^{\delta/(2+\delta)}\) and \(\big{(}ES(j)^{2+\delta}\big{)}^{1/(2+\delta)}\leq 2\ell\zeta_{2+\delta,n}\) and is left as an exercise.

**Proof of Theorem 17.4.1:** W.l.o.g., let \(\mu=0\). Then, \(E\bar{X}_{n}^{2}=O(n^{-1})\) as \(n\rightarrow\infty\). Hence, by (3.3) and Lemma 17.4.2

\[nE\big{|}\tilde{\mu}_{n}-\bar{X}_{n}\big{|}^{2} \leq nE\bigg{\{}\Big{|}1-\frac{n}{N}\Big{|}\,|\bar{X}_{n}| \tag{4.7}\] \[\mbox{}+(N\ell)^{-1}\bigg{|}\sum_{i=1}^{\ell}(\ell-i)(X_{i}+X_{n- i+1})\bigg{|}\bigg{\}}^{2}\] \[\leq 2\bigg{\{}(\ell/N)^{2}nE\big{|}\bar{X}_{n}\big{|}^{2}+2nN^{-2} \bigg{[}E\bigg{|}\sum_{i=1}^{\ell}(i/\ell)X_{i}\bigg{|}^{2}\] \[\mbox{}+E\bigg{|}\sum_{i=1}^{\ell}(i/\ell)X_{\ell-i}\bigg{|}^{2} \bigg{]}\bigg{\}}\] \[= O\Big{(}[\ell/n]^{2}\Big{)}+O\big{(}[\ell/n]\big{)}\] \[= O(\ell/n)\quad\mbox{as}\quad n\rightarrow\infty.\]

This implies

\[E\big{\{}\ell\tilde{\mu}_{n}^{2}\big{\}} \leq \ell\cdot\bigg{\{}2E\big{|}\bar{X}\big{|}^{2}+2E\big{|}\bar{X}_{n} -\tilde{\mu}_{n}\big{|}^{2}\bigg{\}} \tag{4.8}\] \[= O(\ell/n).\]

Hence, it remains to show that \(\ell N^{-1}\sum_{i=1}^{N}U_{i}^{2}\longrightarrow_{p}\sigma_{\infty}^{2}\) as \(n\rightarrow\infty\). Let \(V_{in}=U_{1i}^{2}I\big{(}|U_{1i}|\leq(n/\ell)^{1/8}\big{)}\) and \(W_{in}=U_{1i}^{2}-V_{in}\), \(1\leq i\leq N\). Then, by Lemma 17.4.2,

\[E\bigg{|}N^{-1}\sum_{i=1}^{N}\big{(}V_{in}-EV_{in}\big{)}\bigg{|} ^{2} \tag{4.9}\] \[\leq \mbox{const.}(n/\ell)^{1/2}\Big{[}n\ell+n\ell\sum_{1\leq k<n/\ell} \alpha(k\ell)\Big{]}\Big{/}N^{2}\] \[\leq \mbox{const.}(n/\ell)^{-1/2}\Big{[}1+\sum_{k\geq 1}\alpha(k) \Big{]}\] \[= o(1)\quad\mbox{as}\quad n\rightarrow\infty.\]

Next, note that by definition, \(U_{11}=\sqrt{\ell}\bar{X}_{\ell}\). Further, under the conditions of Theorem 17.4.1, \(\sqrt{n}\bar{X}_{n}\longrightarrow^{d}N(0,\sigma_{\infty}^{2})\), by Corollary 16.3.6. Hence, by the EDCT,

\[\lim_{n\rightarrow\infty}EW_{11}=\lim_{n\rightarrow\infty}E|U_{11}|^{2}I\Big{(} |U_{11}|>(n/\ell)^{1/8}\Big{)}=0. \tag{4.10}\]

Therefore, \(|EV_{1n}-\sigma_{\infty}^{2}|\leq E|U_{11}|^{2}I(|U_{11}|^{8}>n/\ell)+|EU_{11}^ {2}-\sigma_{\infty}^{2}|=o(1)\). Hence, for any \(\epsilon>0\), by (4.9), (4.10), and Markov's inequality,

\[\lim_{n\rightarrow\infty}P\bigg{(}\bigg{|}\ell N^{-1}\sum_{i=1}^{N}U_{i}^{2}- \sigma_{\infty}^{2}\bigg{|}>3\epsilon\bigg{)}\]\[\leq \lim_{n\to\infty}P\bigg{(}\bigg{|}N^{-1}\sum_{i=1}^{N}(V_{in}-EV_{in}) \bigg{|}+\big{|}EV_{1n}-\sigma_{\infty}^{2}\big{|}\] \[+\bigg{|}N^{-1}\sum_{i=1}^{N}W_{in}\bigg{|}>3\epsilon\bigg{)}\] \[\leq \lim_{n\to\infty}P\bigg{(}\bigg{|}N^{-1}\sum_{i=1}^{N}(V_{in}-EV_{ in})\bigg{|}>\epsilon\bigg{)}\] \[+\lim_{n\to\infty}P\bigg{(}\bigg{|}N^{-1}\sum_{i=1}^{N}W_{in} \bigg{|}>\epsilon\bigg{)}\] \[\leq \lim_{n\to\infty}\epsilon^{-2}E\bigg{|}N^{-1}\sum_{i=1}^{N}(V_{in }-EV_{in})\bigg{|}^{2}+\lim_{n\to\infty}\epsilon^{-1}E|W_{11}|\] \[= 0.\]

This proves Theorem 17.4.1. \(\Box\)

#### Consistency of MBB cdf estimators

The main result of this section is the following:

**Theorem 17.4.3:** _Let \(\{X_{i}\}_{i\in\mathbb{Z}}\) be a sequence of stationary random variables. Suppose that there exists a \(\delta\in(0,\infty)\) such that \(E|X_{1}|^{2+\delta}<\infty\) and \(\sum_{n=1}^{\infty}\alpha(n)^{\delta/(2+\delta)}<\infty\). Also, suppose that \(\sigma_{\infty}^{2}=\sum_{i\in\mathbb{Z}}\text{Cov}(X_{1},X_{1+i})\in(0,\infty)\) and that \(\ell^{-1}+n^{-1}\ell=o(1)\) and \(n\to\infty\). Then,_

\[\sup_{x\in\mathbb{R}^{d}}\big{|}P_{*}(T_{n}^{*}\leq x)-P(T_{n}\leq x)\big{|} \longrightarrow_{p}0\quad\text{as}\quad n\to\infty, \tag{4.11}\]

_where \(T_{n}=\sqrt{n}(\bar{X}_{n}-\mu)\) and \(T_{n}^{*}=\sqrt{n}(\bar{X}_{n}^{*}-\hat{\mu}_{n})\) is the MBB version of \(T_{n}\) based on blocks of size \(\ell\)._

Theorem 17.4.3 shows that like the MBB variance estimators, the MBB distribution function estimator is consistent for a wide range of values of the block length parameter \(\ell\). Indeed, the conditions on \(\ell\) presented in both Theorems 17.4.1 and 17.4.2 are also _necessary_ for consistency of these bootstrap estimators. If \(\ell\) remains bounded, then the block bootstrap methods fail to capture the dependence structure of the original data sequence and converge to a _wrong_ normal limit as was noted in Corollary 17.2.3. On the other hand, if \(\ell\) goes to infinite at a rate comparable with the sample size \(n\) (violating the condition \(n^{-1}\ell=o(1)\) as \(n\to\infty\)), then it can be shown (cf. Lahiri (2001)) that the MBB estimators converge to certain _random_ probability measures.

**Proof:** Since \(T_{n}\) converges in distribution to \(N(0,\sigma_{\infty}^{2})\), it is enough to show that

\[\sup_{x}\big{|}P_{*}(T_{n}^{*}\leq x)-\Phi(x/\sigma_{\infty})\big{|}\longrightarrow _{p}0\quad\mbox{as}\quad n\to\infty, \tag{4.12}\]

where \(\Phi(\cdot)\) denotes the cdf of the standard normal distribution. Let \(U_{i}^{*}=(X_{(i-1)\ell+1}^{*}+\cdots+X_{i1}^{*})/\ell\), \(1\leq i\leq b\) denote the average of the \(i\)th resampled MBB block. Also, let \(a_{n}=(n/\ell)^{1/4}\), \(n\geq 1\) and let \(\hat{\Delta}_{n}(a)=\ell b^{-1}\sum_{i=1}^{b}E_{*}\big{|}U_{i}^{*}-\hat{\mu}_{ n}\big{|}^{2}I\big{(}\sqrt{\ell}|U_{i}^{*}-\hat{\mu}_{n}|>2a\big{)}\), \(a>0\). Note that conditional on \(X_{1},\ldots,X_{n}\), \(U_{1}^{*},\ldots,U_{b}^{*}\) are iid random vectors with

\[P_{*}\big{(}U_{1}^{*}=U_{j}\big{)}=\frac{1}{N}\quad\mbox{for}\quad j=1,\ldots,N, \tag{4.13}\]

where \(N=n-\ell+1\) and \(U_{j}=\big{(}X_{j}+\cdots+X_{j+\ell-1}\big{)}/\ell\), \(1\leq j\leq N\). Hence, by (4.8) and the EDCT, for any \(\epsilon>0\),

\[P\big{(}\hat{\Delta}_{n}(a_{n})>\epsilon\big{)}\] \[\leq \epsilon^{-1}E\hat{\Delta}_{n}(a_{n})\] \[= \epsilon^{-1}E\Big{\{}\ell E_{*}\big{|}U_{1}^{*}-\tilde{\mu}_{n} \big{|}^{2}I\big{(}\sqrt{\ell}\big{|}U_{1}^{*}-\tilde{\mu}_{n}\big{|}>2a_{n} \big{)}\Big{\}}\] \[= \epsilon^{-1}E\big{|}U_{11}-\sqrt{\ell}\,\tilde{\mu}_{n}\big{|}^{ 2}I\big{(}\big{|}U_{11}-\sqrt{\ell}\,\tilde{\mu}_{n}\big{|}>2a_{n}\big{)}\] \[\leq 4\epsilon^{-1}\Big{[}E|U_{11}|^{2}I\big{(}|U_{11}|>a_{n}\big{)} +\ell E|\tilde{\mu}_{n}|^{2}\Big{]}\to 0\quad\mbox{as}\quad n\to\infty.\]

Thus,

\[\hat{\Delta}_{n}(a_{n})\longrightarrow_{p}0\quad\mbox{as}\quad n\to\infty. \tag{4.14}\]

To prove (4.12), it is enough to show that for any subsequence \(\{n_{k}\}_{k\geq 1}\), there is a further subsequence \(\{n_{ki}\}_{i\geq 1}\subset\{n_{k}\}_{k\geq 1}\) (for notational simplicity, \(n_{k_{i}}\) is written as \(n_{ki}\)) such that

\[\lim_{i\to\infty}\,\sup_{x}\big{|}P_{*}(T_{n_{ki}}^{*}\leq x)-\Phi(x/\sigma_{ \infty})\big{|}=0\quad\mbox{a.s.} \tag{4.15}\]

Fix a subsequence \(\{n_{k}\}_{k\geq 1}\). Then, by (4.14) and Theorem 17.4.1, there exists a subsequence \(\{n_{ki}\}_{i\geq 1}\) of \(\{n_{k}\}_{k\geq 1}\) such that as \(i\to\infty\),

\[\mbox{Var}_{*}\big{(}T_{n_{ki}}^{*}\big{)}\to\sigma_{\infty}^{2}\quad\mbox{a. s.\ and}\quad\hat{\Delta}_{n_{ki}}(a_{n_{ki}})\to 0\quad\mbox{a.s.} \tag{4.16}\]

Note that \(T_{n}^{*}=\sum_{i=1}^{b}(U_{i}^{*}-\tilde{\mu}_{n})\sqrt{\ell/b}\) is a sum of conditionally iid random vectors \((U_{1}^{*}-\tilde{\mu}_{n})\sqrt{\ell/b},\ldots,(U_{b}^{*}-\tilde{\mu}_{n}) \sqrt{\ell/b}\), which, by (4.16), satisfy Lindeberg's condition along the subsequence \(n_{ki}\), a.s. Hence, by the CLT for independent random vectors, the conditional distribution of \(T_{n_{ki}}^{*}\) converges to \(N(0,\sigma_{\infty}^{2})\) as \(i\to\infty\), a.s. Hence, by Polya's theorem (cf. Chapter 8), (4.15) follows.

#### Second order properties of the MBB

Under appropriate regularity conditions, second order correctness (s.o.c.) (cf. Section 17.1.3) of the MBB is known for the normalized and studentized sample mean. As in the independent case, the proof is based on Edgeworth expansions for the given pivotal quantities and their block bootstrap versions. Derivation of the Edgeworth expansion under dependence is rather complicated. In a seminal work, Gotze and Hipp (1983) developed some conditioning argument and established an asymptotic expansion for the normalized sum of weakly dependent random vectors. S.o.c. of the MBB can be established under a similar set of regularity conditions, stated next.

Let \(\{X_{i}\}_{i\in\mathbb{Z}}\) be a sequence of stationary random variables on a probability space \((\Omega,\mathcal{F},P)\) and let \(\{\mathcal{D}_{j}:j\in\mathbb{Z}\}\) be a collection of sub-\(\sigma\)-algebras of \(\mathcal{F}\). For \(-\infty\leq a\leq b\leq\infty\), let \(\mathcal{D}_{a}^{b}=\sigma\langle\cup\{\mathcal{D}_{j}:j\in\mathbb{Z},\,a\leq j \leq b\}\rangle\). The following conditions will be used:

1. \(\sigma_{\infty}^{2}\equiv\lim\limits_{n\to\infty}n^{-1}\)\(\mathrm{Var}\Big{(}\sum\limits_{i=1}^{n}X_{i}\Big{)}\in(0,\infty)\).
2. There exists \(\delta\in(0,1)\) such that for all \(n,m=1,2,\ldots\) with \(m>\delta^{-1}\), there exists a \(\mathcal{D}_{n-m}^{n+m}\)-measurable random vector \(X_{n,m}^{\ddagger}\) satisfying \[E\left|X_{n}-X_{n,m}^{\ddagger}\right|\leq\delta^{-1}\exp(-\delta m).\]
3. There exists \(\delta\in(0,1)\) such that for all \(i\in\mathbb{Z}\), \(m\in\mathbb{N}\), \(A\in\mathcal{D}_{-\infty}^{i}\), and \(B\in\mathcal{D}_{i+m}^{\infty}\), \[\big{|}P(A\cap B)-P(A)P(B)\big{|}\leq\delta^{-1}\exp(-\delta m).\]
4. There exists \(\delta\in(0,1)\) such that for all \(m\), \(n\), \(k=1,2,\ldots\), and \(A\in\mathcal{D}_{n-k}^{n+k}\) \[E\big{|}P(A|\mathcal{D}_{j}:j\neq n)-P(A|\mathcal{D}_{j}:0<|j-n|\leq m+k)\big{|} \leq\delta^{-1}\exp(-\delta m).\]
5. There exists \(\delta\in(0,1)\) such that for all \(m\), \(n=1,2,\ldots\) with \(\delta^{-1}<m<n\) and for all \(t\in\mathbb{R}^{d}\) with \(|t|\geq\delta\), \[E\big{|}E\big{\{}\exp(\iota t\cdot[X_{n-m}+\cdots+X_{n+m}])\mid\mathcal{D}_{j} :j\neq n\big{\}}\big{|}\leq\exp(-\delta).\]

Condition (C.4) is a strong-mixing condition on the underlying auxiliary sequence of \(\sigma\)-algebras \(\mathcal{D}_{j}\)'s, that requires the \(\sigma\)-algebras \(\mathcal{D}_{j}\)'s to be strongly mixing at an exponential rate. For Edgeworth expansions for the normalized sample mean under polynomial mixing rates, see Lahiri (1996). Condition (C.3) connects the strong mixing condition on the \(\sigma\)-fields \(\mathcal{D}_{j}\)'s to the weak-dependence structure of the random vectors \(X_{j}\)'s. If, for all \(j\in\mathbb{Z}\), one sets \(\mathcal{D}_{j}=\sigma\langle X_{j}\rangle\), the \(\sigma\)-field generated by \(X_{j}\), then Condition(C.3) is trivially satisfied with \(X_{n,m}^{\ddagger}=X_{n}\) for all \(m\). However, this choice of \({\cal D}_{j}\) is not always the most useful one for the verification of the rest of the conditions.

Condition (C.4) is an approximate Markov-type property, which trivially holds if \(X_{j}\) is \({\cal D}_{j}\)-measurable and \(\{X_{i}\}_{i\in{\mathbb{Z}}}\) is itself a Markov chain of a finite order. Finally, (C.5) is a version of the Cramer condition in the weakly dependent case. Note that if \(X_{j}\)'s are iid and the \(\sigma\)-algebras \({\cal D}_{j}\)'s are chosen as \({\cal D}_{j}=\sigma\langle X_{j}\rangle\), \(j\in{\mathbb{Z}}\), then Condition (C.5) is equivalent to requiring that for some \(\delta\in(0,1)\),

\[1>e^{-\delta} \geq E\bigl{|}E\{\exp(\iota t\cdot X_{n})\mid X_{j}:j\neq n\}\bigr{|} \tag{4.17}\] \[= \bigl{|}E\exp(\iota t\cdot X_{1})\bigr{|}\quad\mbox{for all}\quad |t|\geq\delta,\]

which, in turn, is equivalent to the standard Cramer condition

\[\limsup_{|t|\to\infty}\bigl{|}E\exp(\iota t\cdot X_{1})\bigr{|}<1. \tag{4.18}\]

However, for weakly dependent stationary \(X_{j}\)'s, the standard Cramer condition on the _marginal_ distribution of \(X_{1}\) is not enough to ensure a smooth Edgeworth expansion for the normalized sample mean (cf. Gotze and Hipp (1983)).

Conditions (C.2)-(C.5) have been verified for different classes of dependent processes, such as, (i) linear processes with iid innovations, (ii) smooth functions of Gaussian processes, and (iii) Markov processes, etc. See Gotze and Hipp (1983) and Lahiri (2003), Chapter 6, for more details.

The next theorem shows that the MBB is s.o.c. for a range of block sizes under some moment condition and under the regularity conditions listed above.

**Theorem 17.4.4:** Let \(\{X_{i}\}_{i\in{\mathbb{Z}}}\) be a collection of stationary random variables satisfying Conditions (C.1)-(C.5). Let \(\tilde{R}_{n}=\sqrt{n}(\bar{X}_{n}-\mu)/\sigma_{\infty}\) and let \(\tilde{R}_{n}^{*}\) be the MBB version of \(\tilde{R}_{n}\) based on blocks of length \(\ell\), where \(\mu=EX_{1}\) and \(\sigma_{\infty}^{2}\) is as in (C.1). Suppose that for some \(\delta\in(0,\frac{1}{3})\), \(E|X_{1}|^{35+\delta}<\infty\) and

\[\delta n^{\delta}\leq\ell\leq\delta^{-1}n^{1/3}\quad\mbox{for all}\quad n\geq \delta^{-1}. \tag{4.19}\]

Then

\[\sup_{x}\bigl{|}P_{*}\bigl{(}\tilde{R}_{n}^{*}\leq x\bigr{)}-P\bigl{(}\tilde{ R}_{n}\leq x\bigr{)}\bigr{|}=O_{p}\bigl{(}n^{-1}\ell+n^{-1/2}\ell^{-1}\bigr{)}. \tag{4.20}\]

**Proof:** See Theorem 6.7 (b) of Lahiri (2003).

The moment condition can be reduced considerably if the error bound on the right side of (4.20) is replaced by \(o(n^{-1/2})\) only, and if the range of \(\ell\) values in (4.19) is restricted further (cf. Lahiri (1991)).

[MISSING_PAGE_FAIL:563]

**(Hint:** First verify (5.1).)
17.4 Show that under the conditions of Theorem 17.1.3, (1.15) holds. (**Hint:** Set \(x_{n}=j_{n}h/(s_{n}\sqrt{n})\) and \(\eta_{n}=\langle nx_{0}/h\rangle\), where \(j_{n}\in\mathbb{Z}\) is to be chosen. Check that \((\frac{\sqrt{n}x_{n}\sigma}{h}-\eta_{n})=\langle j_{n}(\frac{\sigma}{s_{n}}-1)- \eta_{n}\rangle\). By the LIL, \(\big{(}\frac{\sigma}{s_{n}}-1\big{)}>a_{n}\) i.o., a.s. where \(a_{n}=n^{-1/2}(\log\log n)^{1/4}\). Now choose any \(j_{n}\in\big{(}\frac{\sigma}{s_{n}}-1\big{)}^{-1}(1-\delta+\eta_{n},1+\eta_{n} )\cap\mathbb{Z}\).)
17.5 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(X_{1}\sim N(\theta,1)\), \(\theta\in\mathbb{R}\). For \(n\geq 1\), let \(\hat{\theta}_{n}=F_{n}^{-1}(1/2)\), the sample median of \(X_{1},\ldots,X_{n}\), \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\), the sample mean of \(X_{1},\ldots,X_{n}\), and \(Y_{n}^{2}=\max\big{\{}1,n^{-1}\sum_{i=1}^{n}(X_{i}-\bar{X}_{n})^{2}\big{\}}\), where \(F_{n}\) is the empirical cdf of \(X_{1},\ldots,X_{n}\). Suppose that \(X_{1}^{*},\ldots,X_{n}^{*}\) are (conditionally) iid with \(X_{1}^{*}\sim N(\hat{\theta}_{n},Y_{n}^{2})\) and let \(\bar{X}_{n}^{*}=n^{-1}\sum_{i=1}^{n}X_{i}^{*}\) be the (parametric) bootstrap sample mean. Define \[T_{n}=\sqrt{n}(\bar{X}_{n}-\theta),\;T_{n}^{*}=\sqrt{n}(\bar{X}_{n}^{*}-\hat{ \theta}_{n})\quad\mbox{and}\quad T_{n}^{**}=\sqrt{n}(\bar{X}_{n}^{*}-\bar{X}_{ n}).\] 1. Show that \[\lim_{n\to\infty}\;\sup_{x}\big{|}P(T_{n}\leq x)-P_{*}(T_{n}^{*}\leq x)\big{|}=0\quad\mbox{a.s.}\] 2. Show that \[\lim_{n\to\infty}\;\sup_{x}\big{|}P_{*}(T_{n}^{**}\leq x)-\Phi\big{(}Y_{n}^{- 1}[x-W_{n}]\big{)}\big{|}=0\quad\mbox{a.s.}\] for some random variables \(W_{n}\)'s such that \(W_{n}\longrightarrow^{d}W\). 3. Find the distribution of \(W\) in part (b). (**Hint:** Use the Bahadur representation (cf. Bahadur (1966)) for sample quantiles.) 4. Show that \(Y_{n}^{2}\longrightarrow_{p}1\) as \(n\to\infty\). 5. Conclude that there exists \(\epsilon>0\) such that \[\lim_{n\to\infty}P\Big{(}\sup_{x\in\mathbb{R}}\big{|}P(T_{n}\leq x)-P_{*}(T_{ n}^{**}\leq x)\big{|}>\epsilon\Big{)}>\epsilon.\] (_Thus, \(T_{n}^{**}\) is an incorrect parametric bootstrap version of \(T_{n}\)._)
17.6 Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(E(X_{1})=\mu\), \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\) and \(E|X_{1}|^{4}<\infty\). Let \(X_{1}^{*},\ldots,X_{n}^{*}\) denote the nonparametric bootstrap sample drawn randomly with replacement from \(X_{1},\ldots,X_{n}\) and \(\bar{X}_{n}^{*}=n^{-1}\sum_{i=1}^{n}X_{i}^{*}\). Let \[T_{n}=\sqrt{n}(\bar{X}_{n}-\mu)\quad\mbox{and}\quad T_{n}^{*}=\sqrt{n}(\bar{X }_{n}^{*}-\bar{X}_{n}).\]Show that there exists a constant \(K\in(0,\infty)\) such that \[\limsup_{n\to\infty}\frac{\sqrt{n}}{\sqrt{\log\log n}}\ \sup_{x\in\mathbb{R}} \big{|}P(T_{n}\leq x)-P_{*}(T_{n}^{*}\leq x)\big{|}=K\quad\mbox{a.s.}\] Find K. (_This shows that the bootstrap approximation for \(T_{n}\) is less accurate than that for the normalized sample mean._)
17.7 1. Let \(F\) and \(G\) be two cdfs on \(\mathbb{R}\) and let \(F\) be continuous. Show that \[\sup_{x}\big{|}F(x)-G(x)\big{|}=\sup_{x}\big{|}F(x-)-G(x-)\big{|}.\] 2. Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables with \(EX_{1}=\mu\), \(\mbox{Var}(X_{1})=\sigma^{2}\in(0,\infty)\) and \(E|X_{1}|^{3}<\infty\). Let \(\tilde{R}_{n}=\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\), and \(R_{n}^{*}\) be its bootstrap version based on a resample of size \(n\) from the edf \(F_{n}\). 1. Show that \[\sup_{x}\big{|}P(\tilde{R}_{n}<x)-\Phi(x)\big{|}=o(n^{-1/2})\quad\mbox{as} \quad n\to\infty\] where \(\Phi(\cdot)\) denotes the cdf of the \(N(0,1)\) distribution. 2. Show that if the distribution of \(X_{1}\) is nonlattice, then \[\sup_{x}\big{|}P(\tilde{R}_{n}<x)-P_{*}(R_{n}^{*}<x)\big{|}=o(n^{-1/2})\quad \mbox{a.s.}\]
17.8 Let \(R_{m,n}^{*}\) be the bootstrap version of a random variable \(R_{n}\), \(n\geq 1\) and let \(R_{n}\longrightarrow^{d}R_{\infty}\), where \(R_{\infty}\) has a continuous distribution on \(\mathbb{R}\). Let \(\hat{\phi}_{m,n}(t)=E_{*}\exp(\iota tR_{m,n}^{*})\) and \(\phi_{n}(t)=E\exp(\iota tR_{n})\), \(1\leq n\leq\infty\). 1. Show that if \[\sup_{x}\big{|}P_{*}(R_{m,n}^{*}\leq x)-P(R_{n}\leq x)\big{|}\longrightarrow_{ p}0\quad\mbox{as}\quad n\to\infty,\] then for every \(t\in\mathbb{R}\), \(\hat{\phi}_{m,n}(t)\longrightarrow_{p}\phi_{\infty}(t)\) as \(n\to\infty\). 2. Suppose that there exists a sequence \(\{h_{n}\}_{n\geq 1}\) such that \[\sup_{t\in\mathbb{R}}\big{|}\hat{\phi}_{m,n}(t)-\hat{\phi}_{m,n}(t+h_{n}) \big{|}\longrightarrow_{p}0\quad\mbox{as}\quad n\to\infty.\] (5.2) Then, the converse to (a) holds. 3. Let \(W_{\alpha}\) and \(T_{m,n}^{*}\) be as in (1.23) and (1.27), respectively. Show that (1.40) implies (1.39).

(**Hint:** (b) Let \(Y_{n}(t)\equiv|\hat{\phi}_{m,n}(t)-\phi_{\infty}(t)|\), \(t\in\mathbb{R}\) and let \(\mathbb{Q}=\{q_{1},q_{2},\ldots\}\) be an enumeration of the rationals in \(\mathbb{R}\). Given a subsequence \(\{n_{0j}\}_{j\geq 1}\), extract a subsequence \(\{n_{1j}\}_{j\geq 1}\subset\{n_{0j}\}_{j\geq 1}\) such that \(Y_{n_{1j}}(q_{1})\to 0\) as \(j\to\infty\), a.s. Next extract \(\{n_{2j}\}\subset\{n_{1j}\}\) such that \(Y_{n_{2j}}(q_{2})\to 0\) as \(j\to\infty\), a.s., and continue this for each \(q_{k}\in\mathbb{Q}\). Let \(n_{j}\equiv n_{jj}\), \(j\geq 1\). Then, there exists a set \(A\) with \(P(A)=1\), and on \(A\),

\[Y_{n_{j}}(q_{k})\to 0\quad\mbox{as}\quad j\geq\infty\quad\mbox{for all}\quad q_{k} \in\mathbb{Q}.\]

Now use (5.2) to show that w.p. 1, \(Y_{n^{\prime}_{j}}(t)\to 0\) as \(j\to\infty\) for all \(t\in\mathbb{R}\), for some \(\{n^{\prime}_{j}\}\subset\{n_{j}\}\).

(c) Use the inequality

\[\sup_{t}\big{|}E\exp(\iota tX)-E\exp\big{(}\iota(t+h)X\big{)}\big{|}\leq hE|X|\]

and the fact that under (1.21), \(|X_{(n)}|+|X_{(1)}|=O(n^{\beta})\) as \(n\to\infty\), a.s. for any \(\beta>\frac{1}{\alpha}\). )
17.9 Verify (1.45) in the proof of Theorem 17.1.4.
17.10 (Athreya (1987a)). Let \(\{X_{n}\}_{n\geq 1}\) be a sequence of iid random variables satisfying (1.21) and \(T_{n}\) be as in (1.23). Let \(X_{1}^{*},\ldots,X_{n}^{*}\) be conditionally iid random variables with cdf \(F_{n}\) of (1.1). Define an alternative bootstrap version of \(T_{n}\) as

\[\tilde{T}_{n,n}^{*}=n(\bar{X}_{n}^{*}-\bar{X}_{n})/X_{(n)}\]

where \(\bar{X}_{n}^{*}=n^{-1}\sum_{i=1}^{n}X_{i}^{*}\), \(\bar{X}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}\) and \(X_{(n)}=\max\{X_{1},\ldots,X_{n}\}\). (_Here the scaling constants \(a_{n}\)'s are replaced by the bootstrap analog of (1.26), i.e., by \(n\big{(}1-F_{n}(\hat{a}_{n})\big{)}\to 1\), which yields \(\hat{a}_{n}=X_{(n)}\)._) Let

\[\tilde{\phi}_{n}(t)=E_{*}\exp(\iota t\tilde{T}_{n,n}^{*}),\ t\in\mathbb{R}.\]

Show that \(\tilde{\phi}_{n}(t)\) converges in distribution to a random limit as \(n\to\infty\). Identify the limit.
17.11 Suppose that \(\{X_{n}\}_{n\geq 1}\) satisfies conditions of Theorem 17.1.4 and that \(T_{m,n}^{*}\) is as in (1.27). Show that for any \(t_{1},\ldots,t_{k}\in\mathbb{R}\), \(k\in\mathbb{N}\),

\[\big{(}\hat{\phi}_{n}(t_{1}),\ldots,\hat{\phi}_{n}(t_{k})\big{)} \longrightarrow^{d}\Big{(}\exp\Big{(}\int h(t_{1},x)dM(x)\Big{)},\ldots,\] \[\exp\Big{(}\int h(t_{k},x)M(dx)\Big{)}\Big{)},\]

where \(\hat{\phi}_{n}(t)\equiv E_{*}\exp(\iota tT_{n,n}^{*})\), \(t\in\mathbb{R}\).

17.12: Show that (4.17) and (4.18) are equivalent.
17.13: Let \(\{X_{i}\}_{i\in\mathbb{Z}}\) be a sequence of stationary random variables. Let \(T_{N}=\sqrt{n}(\bar{X}_{n}-\mu)\) and \(T_{n}^{*}=\sqrt{n}(\bar{X}_{n}^{*}-\hat{\mu}_{n})\), where \(\bar{X}_{n}^{*}\) is the MBB sample mean based on blocks of length \(\ell\), where \(\ell^{-1}+n^{-1}\ell=o(1)\) as \(n\to\infty\). Suppose that \(X_{1}\) has finite moments of all order and that \(\{X_{i}\}\) is \(m\)-dependent for \(m\in\mathbb{Z}_{+}\). Assume that the Berry-Esseen theorem holds for \(\bar{X}_{n}\). 1. Show that \[\Delta_{n}(\ell)\equiv\sup_{x}\big{|}P(T_{n}\leq x)-P_{n}(T_{n}^{*}\leq x) \big{|}=O_{p}\big{(}(n^{-1}\ell)^{1/2}+\ell^{-1}\big{)}.\] 2. Find the limit distribution of \(n^{1/3}\Delta_{n}(\ell)\) for \(\ell=\lfloor n^{1/3}\rfloor\), where \(\Delta_{n}(\ell)\) is as in part (a).
17.14: Suppose that \(\{X_{i}\}_{i\in\mathbb{Z}}\), \(\bar{X}_{n}^{*}\), \(\ell\), etc. be as in Problem 17.13. Let \(\bar{R}_{n}=\sqrt{n}(\bar{X}_{n}-\mu)/\sigma_{n}\) and \(R_{n}^{**}=\sqrt{n}(\bar{X}_{n}^{*}-\bar{X}_{n})/\hat{\sigma}_{n}(\ell)\) where \(\sigma_{n}^{2}=n\mbox{Var}(\bar{X}_{n})\) and \(\hat{\sigma}_{n}^{2}(\ell)=n\mbox{Var}_{*}(\bar{X}_{n}^{*})\). 1. Show that \[\tilde{\Delta}_{n}(\ell) \equiv \sup_{x}\big{|}P(R_{n}\leq x)-P(R_{n}^{**}\leq x)\big{|}\] \[= O_{p}\big{(}n^{-1/2}\ell^{1/2}\big{)}.\] 2. Find the limit distribution of \(n^{1/2}\ell^{-1/2}\tilde{\Delta}_{n}(\ell)\).
17.15: Let \(\{X_{i}\}_{i\in\mathbb{Z}}\), \(\bar{X}_{n}^{*}\), \(\ell\), be as in Problem 17.13. 1. Find the leading terms in the expansion of MSE \(\big{(}\hat{\sigma}_{n}^{2}(\ell)\big{)}\equiv E\big{(}\hat{\sigma}_{n}^{2}( \ell)-\sigma_{n}^{2}\big{)}^{2}\), where \(\hat{\sigma}_{n}^{2}(\ell)=n\mbox{Var}_{*}(\bar{X}_{n}^{*})\) and \(\sigma_{n}^{2}=n\mbox{Var}(\bar{X}_{n})\). 2. Find the MSE-optimal block size for estimating \(\sigma_{n}^{2}\).
17.16: Let \(\{X_{i}\}_{i\in\mathbb{Z}}\), \(T_{n}\), \(T_{n}^{*}\), \(\ell\) be as in Problem 17.13. For \(\alpha\in(0,1)\), let \(t_{\alpha,n}=\alpha\)-quantile of \(T_{n}=\inf\big{\{}x:x\in\mathbb{R},P(T_{n}\leq x)\geq\alpha\big{\}}\) and \(\hat{t}_{\alpha,n}(\ell)\) be the \(\alpha\)-quantile of \(T_{n}^{*}=\inf\big{\{}x:x\in\mathbb{R},P_{*}(T_{n}^{*}\leq x)\geq\alpha\big{\}}\). 1. Show that \[[\hat{t}_{\alpha,n}(\ell)-t_{\alpha,n}]\longrightarrow_{p}0\quad\mbox{as} \quad n\to\infty.\] 2. Suppose that \(\ell=\lfloor n^{1/3}\rfloor\) and write \(\tilde{t}_{\alpha,n}=\hat{t}_{\alpha,n}\big{(}\lfloor n^{1/3}\rfloor\big{)}\). Find a sequence \(\{a_{n}\}_{n\geq 1}\) such that \[a_{n}\big{(}\tilde{t}_{\alpha,n}-t_{\alpha,n}\big{)}\longrightarrow^{d}Z\] for some nondegenerate random variable \(Z\). Identify the distribution of \(Z\).

## Chapter 18 Branching Processes

The study of the growth and development of many species of animals, plants, and other organisms over time may be approached in the following manner. At some point in time, a set of individuals called ancestors (or the zeroth generation) is identified. These produce offspring, and the collection of offspring of all the ancestors constitutes the first generation. The offspring of these first generation individuals constitute the second generation and so on. If one specifies the rules by which the offspring production takes place, then one could study the behavior of the long-time evolution of such a process, called a _branching process_. Questions of interest are the long-term survival or extinction of such a process, the growth rate of such a population, fluctuation of population sizes, effects of control mechanisms, etc. In this chapter, several simple mathematical models and some results for these will be discussed. Many of the assumptions made, especially the one about independence of lines of descent, are somewhat idealistic and unrealistic. Nevertheless, the models do prove useful in answering some general questions, since many results about long-term behavior stay valid even when the model deviates somewhat from the basic assumptions.

It is worth noting that the models described below are relevant and applicable not only to _population dynamics_ as mentioned above but also to any evolution that has a tree-like structure, such as the process of electron multiplication, gamma ray radiation, growth of sentences in context-free grammars, algorithmic steps, etc. Thus, the theory of branching processes has found applications in cosmic ray showers, data structures, combinatorics, and molecular biology, especially DNA sequencing. Some references to these applications may be found in Athreya and Jagers (1997).

### 18.1 Bienyeme-Galton-Watson branching process

One of the earliest and simplest models is the Bienyeme-Galton-Watson (BGW) branching process. In this model, it is assumed that the offspring of each individual is a random variable with a probability distribution \((p_{j})_{j\geq 0}\) and that the offspring of different individuals are stochastically independent. Thus, if \(Z_{0}\) is the size of the zeroth generation, then \(Z_{1}=\sum_{i=1}^{Z_{0}}\zeta_{0,i}\) where \(\{\zeta_{0,i}\,:\,i=1,2\ldots\}\) are independent random variables with the same distribution \(\{p_{j}\}_{j\geq 0}\) and also independent of \(Z_{0}\). Here \(\zeta_{0,i}\) is to be thought of as the number of offspring of the \(i\)th individual in the zeroth generation. Similarly, if \(Z_{n}\) denotes the \(n\)th generation population, then \(Z_{n+1}=\sum_{i=1}^{Z_{n}}\zeta_{ni}\), where \(\{\zeta_{ni}:i=1,2\ldots\}\) are iid random variables with distribution \(\{p_{j}\}_{j\geq 0}\) and also independent of \(Z_{0},Z_{1},\ldots,Z_{n}\). This implies that the lines of descent initiated by different individuals of a given generation evolve independently of each other (Problem 18.3). This may not be very realistic when there is competition for limited resources such as space and food in the habitat. The long-term behavior of the sequence \(\{Z_{n}\}_{0}^{\infty}\) is crucially dependent on the parameter \(m\equiv\sum_{j=0}^{\infty}jp_{j}\), the _mean offspring size_. The BGW branching process \(\{Z_{n}\}_{0}^{\infty}\) with offspring distribution \(\{p_{j}\}_{j\geq 0}\) is called _supercritical_, _critical_, and _subcritical_ according as \(m>1,=1\), or \(<1\), respectively. In what follows, the case when \(p_{i}=1\) for some \(i\) is excluded. The main results are the following: see Athreya and Ney (2004) for details and full proofs.

**Theorem 18.1.1:** (_Extinction probability_).__

* \(m\leq 1\Rightarrow Z_{n}=0\) _for all large_ \(n\) _with probability one (w.p._ 1_)._
* \(m>1\Rightarrow P(Z_{n}\to\infty\) _as_ \(n\to\infty\mid Z_{0}=1)=1-q>0\)_, where_ \(q\equiv P(Z_{n}=0\) _for all large_ \(n\mid Z_{0}=1)\) _is the smallest root in [0,1] of_ \[s=f(s)\equiv\sum_{j=0}^{\infty}p_{j}s^{j}.\] (1.1)

Theorem 18.1.1 says that if \(m\leq 1\), then the process will be extinct in finite time w.p. 1, whereas if \(m>1\), then the _extinction probability_\(q\) (with \(Z_{0}=1\)) is strictly less than one, and when the process does not become extinct, it grows to infinity.

**Proof:** Since \(\{Z_{n}\}_{n\geq 0}\) is a Markov chain with state space \(\mathbb{Z}_{+}\equiv\{0,1,2,3,\ldots\}\) and \(0\) is an absorbing barrier, all nonzero states are transient. Thus \(P(Z_{n}=0\) for some \(n\geq 1)+P(Z_{n}\to\infty\) as \(n\to\infty)=1\).

Also,

\[q \equiv P(Z_{n}=0\mbox{ for some }n\geq 1\mid Z_{0}=1)\]\[= \lim_{n\to\infty}P(Z_{n}=0|Z_{0}=1)\] \[= \lim_{n\to\infty}f_{n}(0)\]

where \(f_{n}(s)\equiv E(s^{Z_{n}}|Z_{0}=1)\), \(0\leq s\leq 1\).

But by the definition of \(\{Z_{n}\}\),

\[f_{n+1}(s) = E(s^{Z_{n+1}}|Z_{0}=1)\] \[= E\bigl{(}E(s^{Z_{n+1}}|Z_{n})|Z_{0}=1\bigr{)}\] \[= E\bigl{(}(f(s))^{Z_{n}}\bigr{)}=f_{n}(f(s)).\]

Iterating this yields

\[f_{n}(s)=f^{(n)}(s),\ n\geq 0\]

where \(f^{(n)}(s)\) is the \(n\)th iterate of \(f(s)\).

By continuity of \(f(s)\) in [0,1],

\[q=\lim_{n\to\infty}f_{n}(0)=\lim_{n\to\infty}f\bigl{(}f_{n}(0)\bigr{)}=f(q).\]

If \(q^{\prime}\) is any other solution of (1.1) in [0,1], then \(q^{\prime}=f^{(n)}(q^{\prime})\geq f^{(n)}(0)\) for each \(n\) and hence \(q^{\prime}\geq\lim_{n\to\infty}f^{(n)}(0)=q\).

This establishes (1.1). It is not difficult to show that by the convexity of the function \(f(s)\) on [0,1], \(q=1\) if \(m\equiv f^{\prime}(1)\leq 1\) and \(p_{0}\leq q<1\) if \(m>1\). \(\Box\)

The following results are refinements of Theorem 18.1.1.

**Theorem 18.1.2:** (_Supercritical case_). _Let \(m>1\), \(Z_{0}=z_{0}\), \(1\leq z_{0}<\infty\), and \(W_{n}\equiv Z_{n}/m^{n}\). Then_

* \(\{W_{n}\}_{n\geq 0}\) _is a nonnegative martingale and hence, converges w.p. 1 to a limit_ \(W\)_._
* \(\sum_{j=1}^{\infty}(j\log j)p_{j}<\infty\Rightarrow P(W=0)=q^{z_{0}}\)_,_ \(EW=z_{0}\)_, and_ \(W\) _has a strictly positive density on_ \((0,\infty)\)_._
* \(\sum_{j=1}^{\infty}(j\log j)p_{j}=\infty\Rightarrow P(W=0)=1\)_._
* _There always exists a sequence_ \(\{C_{n}\}_{0}^{\infty}\) _such that_ \(\lim_{n\to\infty}Z_{n}/C_{n}\equiv\tilde{W}\) _exists and is finite w.p. 1,_ \(P(\tilde{W}=0)=q\) _and_ \(C_{s+1}/C_{n}\to m\)_._

This theorem says that in a supercritical BGW process in the event of nonextinction, the process grows exponentially fast, confirming the assertion of the economist and demographer Malthus of the 19th century.

**Proof:** Since \(\{Z_{n}\}_{n\geq 0}\) is a Markov chain

\[E\{Z_{n+1}|Z_{0},Z_{1},\ldots,Z_{n}\}=E\{Z_{n+1}|Z_{n}\}=Z_{n}m\]implying \(E(W_{n+1}|W_{0},W_{1},\ldots,W_{n})=W_{n}\) proving (a). \(\Box\)

Parts (b) and (c) are known as the Kesten-Stigum theorem, and for full proof see Athreya and Ney (2004) where a proof of part (d) is also given. For a weaker version of (b), see Problem 18.2.

**Theorem 18.1.3:** (_Critical case_). _Let \(m=1\) and \(Z_{0}=z_{0}\), \(1\leq z_{0}<\infty\). Let \(0<\sigma^{2}=\sum_{j=1}^{\infty}j^{2}p_{j}-1<\infty\). Then_

\[\lim_{n\to\infty}P\left(\frac{Z_{n}}{n}\leq x\mid Z_{n}\neq 0\right)=1-\exp(-2x /\sigma^{2}). \tag{1.2}\]

Thus, in the critical case, conditioned on nonextinction by time \(n\), the population in the \(n\)th generation is of order \(n\), which when divided by \(n\) is distributed approximately as an exponential with mean \(\sigma^{2}/2\).

**Theorem 18.1.4:** (_Subcritical case_). _Let \(m<1\) and \(Z_{0}=z_{0}\), \(1\leq z_{0}<\infty\). Then_

\[\lim_{n\to\infty}P(Z_{n}=j|Z_{n}>0)=\pi_{j} \tag{1.3}\]

_exists for all \(j\geq 1\) and \(\sum_{j=1}^{\infty}\pi_{j}=1\). Furthermore, \(\sum_{j=1}^{\infty}j\pi_{j}<\infty\) if and only if \(\sum_{j=1}^{\infty}(j\log j)p_{j}<\infty\)._

For proof of Theorems 18.1.3 and 18.1.4, see Athreya and Ney (2004).

In the supercritical case with \(p_{0}=0\), it is possible to estimate consistently the mean \(m\) and the variance \(\sigma^{2}\) of the offspring distribution from observing the population size sequence \(\{Z_{n}\}_{n\geq 0}\), but the whole distribution \(\{p_{j}\}_{j\geq 0}\) is not identifiable. However, if the entire tree is available, then \(\{p_{j}\}_{j\geq 0}\) is identifiable.

### 18.2 BGW process: Multitype case

The model discussed in the previous section has a natural generalization. Consider a population with \(k\) types of individuals, \(1<k<\infty\). Assume that a type \(i\) individual can produce offspring of all types with a probability distribution that may depend on \(i\) but is independent of past history as well as the other individuals in the same generation. This ensures that the lines of descent initiated by different individuals in a given generation are independent, and those initiated by the individuals of the same type are iid as well. The dichotomy of extinction or infinite growth continues to hold. The analog of the key parameter \(m\) of the single type case is the maximal eigenvalue \(\rho\) of the _mean matrix_\(M\equiv\left((m_{ij})\right)_{k\times k}\), where \(m_{ij}\) is the mean number of type \(j\) offspring of an individual of type \(i\).

**Theorem 18.2.1:** (_Extinction probability_). _Let \(M\equiv\bigl{(}(m_{ij})\bigr{)}\), the mean matrix, be such that for some \(n_{0}>0\), \(M^{n_{0}}>>0\) i.e., all the entries of \(M^{n_{0}}\) are strictly positive. Let \(\rho\) be the maximal eigenvalue of \(M\). Then_

* \(\rho\leq 1\Rightarrow P\) _(the population is extinct in finite time)_ \(=1\)_, for any initial condition._
* \(\rho>1\Rightarrow P\) _(the population is extinct in finite time)_ \(=1-P\) _(the population grows to infinity)_ \(<1\) _for all initial conditions other than 0 ancestors._

_Furthermore, if \(q_{i}=P\) (extinction starting with one ancestor of type \(i\)), then the vector \(q=(q_{1},q_{2},\ldots,q_{k})\) is the smallest root in \([0,1]^{k}\) of the equation \(s=f(s)\), where \(s=(s_{1},s_{2},\ldots,s_{k})\) and \(f(s)=[f_{1}(s),\ldots,f_{k}(s)]\), with \(f_{k}(s)\) being the generating function of the offspring distribution of a type \(i\) individual._

The following refinements of the above Theorem 18.2.1 are the analogs of Theorems 18.1.2, 18.1.3, and 18.1.4. It is known that for the maximal eigenvalue \(\rho\) of \(M\), there exist strictly positive eigenvectors \(u=(u_{1},u_{2},\ldots,u_{k})\) and \(v=(v_{1},v_{2},\ldots,v_{k})\) such that \(uM=\rho u\) and \(Mv^{T}=\rho v^{T}\) (where superscript \(T\) stands for transpose) normalized to satisfy \(\sum_{i=1}^{k}u_{i}=1\) and \(\sum_{i=1}^{k}u_{i}v_{i}=1\). Let \(Z_{n}=(Z_{n1},Z_{n2},\ldots,Z_{nk})\) denote the vector of population sizes of individuals of the \(k\) types in the \(n\)th generation.

**Theorem 18.2.2:** (_Supercritical case_). _Let \(1<\rho<\infty\). Let_

\[W_{n}=\frac{v\cdot Z_{n}}{\rho^{n}}=\frac{\Bigl{(}\sum_{i=1}^{k}v_{i}Z_{ni} \Bigr{)}}{\rho^{n}}. \tag{2.1}\]

_Then \(\{W_{n}\}\) is a nonnegative martingale and hence converges w.p. 1 to a limit \(W\) and \(Z_{n}/(v\cdot Z_{n})\to u\) on the event of nonextinction._

This theorem says that when the process does not die out it does go to \(\infty\), and the sizes of individuals of different types grow at the same rate and are aligned in the direction of the vector \(u\). There are also appropriate analogs of Theorem 18.1.2 (b), (c), and (d).

If \(\zeta\) is a \(k\)-vector such that \(\zeta\cdot v\neq 0\), then the sequence \(Y_{n}\equiv\frac{\zeta\cdot Z_{n}}{\rho^{n}}\) converges w.p. 1 to \((\zeta\cdot u)W\). If \(\zeta\cdot v=0\), then \(\rho^{n}\) is not the right normalization for \(\zeta\cdot Z_{n}\). The problem of the correct normalization for such vectors as well as the proofs of Theorems 18.2.2-18.2.4 are discussed in Chapter 5 of Athreya and Ney (2004).

**Theorem 18.2.3:** (_Critical case_). _Let \(Z_{0}=z_{0}\neq 0\) and \(\rho=1\). Let all the offspring distributions possess finite second moments. Then_

\[\lim_{n\to\infty}P\Bigl{(}\frac{v\cdot Z_{n}}{n}\leq x\mid Z_{n}\neq 0\Bigr{)}=1 -e^{-\lambda x} \tag{2.2}\]_for some \(0<\lambda<\infty\) and_

\[\lim_{n\to\infty}P\Big{(}\Big{\|}\frac{Z_{n}}{v\cdot Z_{n}}-u\Big{\|}>\epsilon \mid Z_{n}\neq 0\Big{)}=0 \tag{2.3}\]

_for each \(\epsilon>0\), where \(\|\cdot\|\) is the Euclidean distance._

**Theorem 18.2.4:** (_Subcritical case_)_. Let \(\rho<1\) and \(Z_{0}=z_{0}\neq 0.\) Then_

\[\lim_{n\to\infty}P(Z_{n}=j|Z_{n}\neq 0)=\pi_{j} \tag{2.4}\]

_exists for all vectors \(j=(j_{1},j_{2},\ldots,j_{k})\neq 0\) and \(\sum\pi_{j}=1\)._

### 18.3 Continuous time branching processes

The models discussed in the past two sections deal with branching processes in which individuals live for exactly one unit of time and are replaced by a random number of offspring. A model in which individuals live a random length of time and then produce a random number of offspring is discussed below.

Assume that each individual lives a random length of time with _distribution function_\(G(\cdot)\) and then produces a random number of offspring with distribution \(\{p_{j}\}_{j\geq 0}\) and these two random quantities are independent and further independently distributed over all individuals in the population. Let \(Z(t)\) denote the population size and \(Y(t)\) be the set of ages of all individuals alive at time \(t\). Assume \(G(0)=0\) and \(m\equiv\sum_{j=1}^{\infty}jp_{j}<\infty.\) The offspring mean \(m\) continues to be the key parameter.

When \(G(\cdot)\) is exponential, the process \(\{Z(t):t\geq 0\}\) is a continuous time Markov chain. In the general case, the vector process \(\{(Z(t),Y(t)):t\geq 0\}\) is a Markov process.

**Theorem 18.3.1:** (_Extinction probability_)_. Let \(q=P[Z(t)=0\) for some \(t>0]\) when \(Z(0)=1\) and \(Y(0)=\{0\}.\) Then \(m\leq 1\Rightarrow q=1\) and \(m>1\Rightarrow q<1\) and \(P[Z(t)\to\infty]=1-P[Z(t)=0\) for some \(t>0]\)._

The following refinements of the above Theorem 18.3.1 are analogs of Theorems 18.1.2-18.1.4.

When \(m>1\), the effect of random lifetimes is expressed through the _Malthusian parameter_\(\alpha\) defined by

\[m\int_{(0,\infty)}e^{-\alpha t}dG(t)=1. \tag{3.1}\]

The _reproductive age value_\(V(x)\) of an individual of age \(x\) is

\[V(x)\equiv m\Big{(}\int_{[x,\infty)}e^{-\alpha t}dG(t)\Big{)}[1-G(x)]^{-1}. \tag{3.2}\]If \(m(t)\equiv EZ(t)\) when one starts with one individual of age 0, then \(m(\cdot)\) satisfies the integral equation

\[m(t)=1-G(t)+m\int_{(0,t)}m(t-u)dG(u).\]

It can be shown using renewal theory (cf. Section 8.5) that \(m(t)e^{-\alpha t}\) converges to a finite positive constant (Problem 18.6).

Theorem 18.3.2: (Supercritical case). Let \(m>1\). Then

* \(W(t)\equiv e^{-\alpha t}\sum_{i=1}^{Z(t)}V(X_{i})\), where \(\{x_{1},\ldots,x_{Z(t)}\}\) are the ages of the individuals alive at \(t\), is a nonnegative martingale and converges w.p. 1 to a limit \(W\) where \(V(\cdot)\) is as in (3.2),
* \(\sum_{j=1}^{\infty}(j\log j)p_{j}=\infty\Rightarrow W=0\) w.p. 1,
* \(\sum_{j=1}^{\infty}(j\log j)p_{j}<\infty\Rightarrow\) \[P(W=0\mid Y_{0}=\{0\})=q\quad\text{and}\] \(E(W|Y_{0}=\{x\})=V(x)\),
* on the event of nonextinction, w.p. 1 the empirical age distribution at time \(t\), \(A(x,t)\equiv\) number of \(\{\)individuals alive at \(t\) with age \(\leq x\}/\{\)number of individuals alive at \(t\}\) converges in distribution as \(t\to\infty\) to the steady-state age distribution: \[A(x)\equiv\frac{\int_{0}^{x}e^{-\alpha y}[1-G(y)]dy}{\int_{0}^{\infty}e^{- \alpha y}[1-G(y)]dy}\]
* \(\sum_{j=1}^{\infty}(j\log j)p_{j}<\infty\Rightarrow Z(t)e^{-\alpha t}\) converges w.p. 1 to

Theorem 18.3.3: (Critical case). Let \(m=1\) and \(\sigma^{2}=\sum_{j}(j-1)^{2}p_{j}<\infty\). Assume \(\lim_{t\to\infty}t^{2}(1-G(t))=0\) and \(\int_{0}^{\infty}tdG(t)=\mu\). Then, for any initial \(Z_{0}\neq 0\).

* \(\lim_{t}P[Z(t)/t\leq x|Z(t)>0]=1-\exp[(-2\mu/\sigma^{2})x]\), \(0<x<\infty\).
* \(\lim_{t}P[\sup_{x}|A(x,t)-A(x)|>\epsilon|Z(t)>0]=0\) for any \(\epsilon>0\), where \(A(\cdot,t)\) and \(A(x)\) are as in Theorem 18.3.2 (d) with \(\alpha=0\).

Theorem 18.3.4: (Subcritical case). Let \(m<1\). Then for any initial \(Z_{0}\neq 0\), \(G(\cdot)\) nonlattice (cf. Chapter 10),

\[\lim_{t\to\infty}P[Z(t)=j|Z(t)>0]=\pi_{j}\]

exists for all \(j\geq 1\) and \(\sum_{j=1}^{\infty}\pi_{j}=1\).

### 18.4 Embedding of Urn schemes in continuous time branching processes

It turns out that many urn schemes can be embedded in continuous time branching processes. The case of Polya's urn is discussed below.

Recall that Polya's urn scheme is the following. Let an urn have an initial composition of \(R_{0}\) red and \(B_{0}\) black balls. A _draw_ consists of taking a ball at random from the urn, noting its color, and returning it to the urn with one more ball of the color drawn. Let \((R_{n},B_{n})\) denote the composition after \(n\) draws. Clearly, \(R_{n}+B_{n}=R_{0}+B_{0}+n\) for all \(n\geq 0\) and \(\{R_{n},B_{n}\}_{n\geq 0}\) is a Markov chain.

Let \(\{Z_{i}(t):t\geq 0\}\), \(i=1,2\) be two independent continuous time branching processes with unit exponential life times and offspring distribution of binary splitting, i.e., \(p_{2}=1\) and \(Z_{1}(0)=R_{0}\), \(Z_{2}(0)=B_{0}\). Let \(\tau_{0}=0<\tau_{1}<\tau_{2}<\ldots<\tau_{n}<\ldots\) denote the successive times of death of an individual in the combined population. Then the sequence \(\big{(}Z_{1}(\tau_{n}),Z_{2}(\tau_{n})\big{)}_{n\geq 0}\) has the same distribution as \((R_{n},B_{n})_{n\geq 0}\).

To establish this claim, by the Markov property of \(\big{(}Z_{1}(t),Z_{2}(t)\big{)}_{t\geq 0}\), it suffices to show that \(\big{(}Z_{1}(\tau_{1}),Z_{2}(\tau_{1})\big{)}\) has the same distribution as \((R_{1},B_{1})\). It is easy to show that if \(\eta_{i}:i=1,2,\ldots,n\) are independent exponential random variables with parameters \(\lambda_{i}\), \(i=1,2,\ldots,n\) then the \(\eta\equiv\min\{\eta_{i}:1\leq i\leq n\}\) is an Exponential (\(\sum_{i=1}^{n}\lambda_{i}\)) random variable and \(P(\eta=\eta_{i})=\frac{\lambda_{i}}{(\sum_{j=1}^{n}\lambda_{j})}\) (Problem 18.9). This, in turn, leads to the fact at time \(\tau_{1}\), the probability that a split takes place in \(\{Z_{1}(t):t\geq 0\}\) is \(\frac{Z_{1}(0)}{Z_{1}(0)+Z_{2}(0)}\). At this split, the parent is lost but is replaced by two new individuals resulting in a net addition of one more individual, establishing the claim.

The same reasoning yields the embedding of the following general urn scheme. Let \({\bf X}_{n}=(X_{n1},\ldots,X_{nk})\) be the vector of the composition of an urn at time \(n\) where \(X_{ni}\) is the number of balls of color \(i\). Assume that given \(({\bf X}_{0},{\bf X}_{1},\ldots,{\bf X}_{n})\), \({\bf X}_{n+1}\) is generated as follows.

Pick a ball at random from the urn. If it happens to be of color \(i\), then return it to the urn along with a random number \(\zeta_{ij}\) of balls of color \(j=1,2,\ldots,k\) where the joint distribution of \(\zeta_{i}\equiv(\zeta_{i1},\zeta_{i2},\ldots,\zeta_{ik})\) depends on \(i\), \(i=1,2,\ldots,k\). Now set, \({\bf X}_{n+1}={\bf X}_{n}+\zeta_{i}\).

The embedding is done as follows. Consider a continuous time multitype branching process \(\{Z(t):t\geq 0\}\) with Exponential (1) lifetimes and the offspring distribution of the \(i\)th type is the same as that of \(\tilde{\zeta}_{i}\equiv\zeta_{i}+\delta_{i}\) where \(\zeta_{i}\) is as above and \(\delta_{i}\) is \(i\)th the unit vector. Let for \(i=1,2,\ldots,k\), \(\{Z_{i}(t):t\geq 0\}\) be a branching process that evolves as \(\{Z(t):t\geq 0\}\) above but has initial size \(Z_{i}(0)\equiv(0,0,\ldots,X_{0i},0,\ldots,0)\). Let \(0=\tau_{0}<\tau_This embedding has been used to prove limit theorems for urn models. See Athreya and Ney (2004), Chapter 5, for details. For applications to clinical trials, see Rosenberger (2002).

### 18.5 Problems

18.1 Show that for any probability distribution \(\{p_{j}\}_{j\geq 0}\), \(f(s)=\sum_{j=0}^{\infty}p_{j}s^{j}\) is convex in [0,1]. Show also that there exists a \(q\in[0,1)\) such that \(f(q)=q\) iff \(m=f^{\prime}(1\cdot)>1\).
18.2 Assume \(\sum_{j=1}^{\infty}j^{2}p_{j}<\infty\). 1. Let \(v_{n}=V(Z_{n}|Z_{0}=1)\). Show that \(v_{n+1}=V(Z_{n}m|Z_{0}=1)+E(Z_{n}\sigma^{2}|Z_{0}=1)\) where \(m=E(Z_{1}|Z_{0}=1)\) and \(\sigma^{2}=V(Z_{1}|Z_{0}=1)\) and hence \(v_{n+1}=m^{2}v_{n}+\sigma^{2}m^{n}\). 2. Conclude from (a) that \(\sup_{n}EW_{n}^{2}<\infty\), where \(W_{n}=Z_{n}/m^{n}\). 3. Using the fact \(\{W_{n}\}_{n\geq 0}\) is a martingale, show that if \(\sum j^{2}p_{j}<\infty\) then \(\{W_{n}\}\) converges w.p. 1 and in \(L^{2}\) to a random variable \(W\) such that \(E(W|Z_{0}=1)=1\).
18.3 By definition, the sequence \(\{Z_{n}\}_{n\geq 0}\) of population sizes satisfies the random iteration scheme \[Z_{n+1}=\sum_{i=1}^{Z_{n}}\zeta_{ni}\] where \(\{\zeta_{ni},i=1,2,\ldots,n=1,2,\ldots\}\) is a doubly infinite sequence of iid random variable with distribution \(\{p_{j}\}\). 1. (_Independence of lines of descent_). Establish the property that for any \(k\geq 0\) if \(Z_{0}=k\) then \(\{Z_{n}\}_{n\geq 0}\) has the same distribution as \(\big{\{}\sum_{j=1}^{k}Z_{n}^{(j)}\big{\}}_{n\geq 0}\) where \(\big{\{}\{Z_{n}^{(j)}\}_{n\geq 0}\big{\}}\), \(j\geq 1\) are iid copies of \(\{Z_{n}\}_{n\geq 0}\) with \(Z_{0}=1\). 2. In the context of Theorem 18.1.2, show that if \(Z_{0}=1\) then \(W\equiv\lim W_{n}\) can be represented as \[W=\frac{1}{m}\sum_{j=1}^{Z_{1}}W^{(j)}\] where \(Z_{1}\), \(W^{(j)}\), \(j=1,2,\ldots\) are all independent with \(Z_{1}\) having distribution \(\{p_{j}\}_{j\geq 0}\) and \(\{W^{(j)}\}_{j\geq 1}\) are iid with distribution same as \(W\).

3. Let \(\alpha\equiv\sum_{a_{j}\in D}P(W=a_{j})\) where \(D\equiv\{a_{j}\}\) is the set of values such that \(P(W=a_{j})>0\). Show using (b) that \(\alpha=f(\alpha)\) and conclude that if \(\alpha<1\), then \(\alpha=q\) and hence that if \(\alpha<1\), then the distribution of \(W\) conditional on \(W>0\) must be continuous. 4. Let \(\beta\) be the singular component of the distribution of \(W\) in its Lebesgue decomposition. Show using (b) that \(\beta\) satisfies \(\beta\leq f(\beta)\) and hence that if \(\beta<1\), then \(\beta=P(W=0)\) and the distribution of \(W\) conditional on \(W>0\) must be absolutely continuous. 5. Let \(p_{0}=0\). Show that the distribution of \(W\) is of the pure type, i.e., it is either purely discrete, purely singular continuous, or purely absolutely continuous.
18.4. 1. Show using Problem 18.3 (b) that if \(W\) has a lattice distribution with span \(d\), then \(d\) must satisfy \(d=md\) and hence \(d=\infty\). Conclude that if \(P(W=0)<1\), then the distribution of \(W\) on \(\{W>0\}\) must be nonlattice. 2. Let \(p_{0}=0\) and \(P(W=0)=0\). Use (a) to conclude that the characteristic function \(\phi(t)\equiv E(e^{\iota tW})\) of \(W\) satisfies \(\sup_{1\leq|t|\leq m}|\phi(t)|<1\). 3. Let \(p_{0}=0\). Show that for any \(0\leq s_{0}<1\), \(\epsilon>0\), \(f^{(n)}(s_{0})=0(\epsilon^{n})\). (**Hint:** By the mean value theorem, \(f^{(n)}(s)=\prod_{j=0}^{n-1}f^{\prime}(f_{j}(s))\). Now use \(f^{\prime}(0)=p_{0}\), \(f_{j}(s)\to 0\) as \(j\to\infty\).) 4. Let \(p_{0}=0\), \(P(W=0)=0\). Show that for any \(n\geq 1\), \(\phi(m^{n}t)=f^{(n)}(\phi(t))\) and hence \(\int_{-\infty}^{\infty}|\phi(u)|du<\infty\). Conclude that the distribution of \(W\) is absolutely continuous.
18.5. In the multitype case for the martingale defined in (2.1), show that \(\{W_{n}\}_{n\geq 0}\) is \(L^{2}\) bounded if \(E_{i}Z_{1j}^{2}<\infty\) for all \(i,j\) where \(E_{i}\) denotes expectation when one starts with an individual of type \(i\).
18.6. Let \(m(\cdot)\) satisfy the integral equation (3.3). 1. Show that \(m_{\alpha}(t)\equiv m(t)e^{-\alpha t}\) satisfies the renewal equation \[m_{\alpha}(\cdot)=\big{(}1-G(t)\big{)}e^{-\alpha t}+\int_{(0,t]}m_{\alpha}(t- u)dG_{\alpha}(u)\] where \(G_{\alpha}(t)\equiv m\int_{0}^{t}e^{-\alpha u}dG(u)\), \(t\geq 0\). 2. Use the key renewal theorem of Section 8.5 to conclude that \(\lim_{t\to\infty}m_{\alpha}(t)\) exists and identify the limit.

3. Assuming \(\sum_{j=1}^{\infty}j^{2}p_{j}<\infty\) show using the key renewal theorem of Section 8.5 that \(\{W(t):t\geq 0\}\) of Theorem 18.3.2 is \(L^{2}\) bounded.
18.7 Consider an M/G/1 queue with Poisson arrivals and general service time. Let \(Z_{1}\) be the number of customers that arrive during the service time of the first customer. Call these first generation customers. Let \(Z_{2}\) be the number of customers that arrive during the time it takes to serve all the \(Z_{1}\) customers. Call these second generation customers. For \(n\geq 1\), let \(Z_{n+1}\) denote the number of customers that arrive during the time it takes to serve all \(Z_{n}\) of the \(n\)th generation customers. 1. Show that \(\{Z_{n}\}_{n\geq 0}\) is a BGW branching process as in Section 18.1. 2. Find the offspring distribution \(\{p_{j}\}_{0}^{\infty}\) and its mean \(m\) in terms of the rate parameter \(\lambda\) of the Poisson arrival process and the service time distribution \(G(\cdot)\). 3. Show that the queue size goes to \(\infty\) with positive probability iff \(m>1\). 4. Set up a functional equation for the moment generating function of the _busy period_\(U\), i.e., the time interval between when the first service starts and when the server is idle for the first time.
18.8 Let \(\{\eta_{i}:i=1,2,\ldots,n\}\) be independent exponential random variables with \(E\eta_{i}=\lambda_{i}^{-1}\), \(i=1,2,\ldots,n\). Let \(\eta\equiv\min\{\eta_{i}:1\leq i\leq n\}\). Show that \(\eta\) has an exponential distribution with \(E\eta=\big{(}\sum_{i=1}^{n}\lambda_{i}\big{)}^{-1}\) and that \(P(\eta=\eta_{j})=\lambda_{j}\big{(}\sum_{i=1}^{n}\lambda_{i}\big{)}\).
18.9 Using the embedding outlined in Section 18.4 for the Polya urn scheme, show that \(Y_{n}\equiv\frac{R_{n}}{R_{n}+B_{n}}\to Y\) w.p. 1 and that \(Y\) can be represented as \(Y=\frac{\sum_{i=1}^{R_{0}}X_{i}}{\sum_{j=1}^{R_{0}}+B_{0}\;X_{j}}\) where \(\{X_{i}\}_{i\geq 1}\) are iid exponential (1) random variables. Conclude that \(Y\) has Beta \((R_{0},B_{0})\) distribution.

## Appendix A Advanced Calculus: A Review

This Appendix is a brief review of elementary set theory, real numbers, limits, sequences and series, continuity, differentiability, Riemann integration, complex numbers, exponential and trigonometric functions, and metric spaces. For proofs and further details, see Rudin (1976) and Royden (1988).

### Elementary set theory

This section reviews the following: sets, set operations, product sets (finite and infinite), equivalence relation, axiom of choice, countability, and uncountability.

**Definition A.1.1**: **:**__A _set_ is a collection of objects.

It is typically defined as a collection of objects with a _common defining property_. For example, the collection of even integers can be written as \(\mathbb{E}\equiv\{n:n\text{ is an even integer}\}\). In general, a set \(\Omega\) with defining property \(p\) is written as

\[\Omega=\{\omega:\omega\ \text{ has property }\ p\}.\]

The individual elements are denoted by the small letters \(\omega\), \(a\), \(x\), \(s\), \(t\), etc., and the sets by capital letters \(\Omega\), \(A\), \(X\), \(S\), \(T\), etc.

**Example A.1.1**: **:**__The closed interval \([0,1]\equiv\{x:x\text{ a real number},\ 0\leq x\leq 1\}\).

[MISSING_PAGE_FAIL:580]

**Definition A.1.4:** Let \(\Omega\) and \(I\) be nonempty sets. Let \(\{A_{\alpha}:\alpha\in I\}\) be a collection of subsets of \(\Omega\). Then \(I\) is called the _index set_.

The _union_ of \(\{A_{\alpha}:\alpha\in I\}\) is defined as

\[\bigcup_{\alpha\in I}A_{\alpha}\equiv\{\omega:\omega\in A_{\alpha}\mbox{ for some }\alpha\in I\}.\]

The _intersection_ of \(\{A_{\alpha}:\alpha\in I\}\) is defined as

\[\bigcap_{\alpha\in I}A_{\alpha}\equiv\{\omega:\omega\in A_{\alpha}\mbox{ for every }\alpha\in I\}.\]

**Definition A.1.5:** (_Complement of a set_). Let \(A\subset\Omega\). Then the _complement_ of the set \(A\), written as \(A^{c}\) (or \(\tilde{A}\)), is defined by \(A^{c}\equiv\{\omega:\omega\notin A\}\).

**Example A.1.8:** If \(\Omega=\mathbb{N}\) and \(A\) is the set of all integers that are divisible by \(2\), then \(A^{c}\) is the set of all odd integers, i.e., \(A^{c}=\{1,3,5,7,\ldots\}\).

**Proposition A.1.1:** (_DeMorgan's law_). _For any \(\{A_{\alpha}:a\in I\}\) of subsets of \(\Omega\), \((\cup_{\alpha\in I}A_{\alpha})^{c}=\cap_{\alpha\in I}A_{\alpha}^{c}\), \((\cap_{\alpha\in I}A_{\alpha})^{c}=\cup_{\alpha\in I}A_{\alpha}^{c}\)._

**Proof:** To show that two sets A and B are the same, it suffices to show that

\[\omega\in A\Rightarrow\omega\in B\quad\mbox{and}\quad\omega\in B\Rightarrow \omega\in A.\]

Let \(\omega\in(\cup_{\alpha\in I}A_{\alpha})^{c}\). Then \(\omega\notin\cup_{\alpha\in I}A_{\alpha}\)

\[\Rightarrow\quad\omega\notin A_{\alpha}\mbox{ for any }\alpha\in I\] \[\Rightarrow\quad\omega\in A_{\alpha}^{c}\mbox{ for each }\alpha\in I\] \[\Rightarrow\quad\quad\omega\in\bigcap_{\alpha\in I}A_{\alpha}^{c}.\]

Thus \((\cup_{\alpha\in I}A_{\alpha})^{c}\subset\cap_{\alpha\in I}A_{\alpha}^{c}\). The opposite inclusion and the second identity are similarly proved. \(\Box\)

**Definition A.1.6:** (_Product sets_). Let \(\Omega_{1}\) and \(\Omega_{2}\) be two nonempty sets. Then the _product set_ of \(\Omega_{1}\) and \(\Omega_{2}\), denoted by \(\Omega\equiv\Omega_{1}\times\Omega_{2}\), consists of all ordered pairs \((\omega_{1},\omega_{2})\) such that \(\omega_{1}\in\Omega_{1},\omega_{2}\in\Omega_{2}\).

Note that if \(\Omega_{1}=\Omega_{2}\) and \(\omega_{1}\neq\omega_{2}\), then the pair \((\omega_{1},\omega_{2})\) is not the same as \((\omega_{2},\omega_{1})\), i.e., the order is important.

**Example A.1.9:**\(\Omega_{1}=[0,1]\), \(\Omega_{2}=[2,3]\). Then \(\Omega_{1}\times\Omega_{2}=\{(x,y):0\leq x\leq 1,\ 2\leq y\leq 3\}\).

**Definition A.1.7:** (_Finite products_). If \(\Omega_{i},i=1,2\ldots,k\) are nonempty sets, then

\[\Omega=\Omega_{1}\times\Omega_{2}\times\ldots\times\Omega_{k}\]is the set of all ordered \(k\) vectors \((\omega_{1},\omega_{2},\ldots,\omega_{k})\) where \(\omega_{i}\in\Omega_{i}\). If \(\Omega_{i}=\Omega_{1}\) for all \(1\leq i\leq k\), then \(\Omega_{1}\times\Omega_{2}\times\ldots\times\Omega_{k}\) is written as \(\Omega_{1}^{(k)}\) or \(\Omega_{1}^{k}\).

**Definition A.1.8:** (_Infinite products_). Let \(\{\Omega_{\alpha}:\alpha\in I\}\) be an infinite collection of nonempty sets. Then \(\times_{\alpha\in I}\Omega_{\alpha}\), _the product set_, is defined as \(\{f:f\) is a function defined on \(I\) such that for each \(\alpha\), \(f(\alpha)\in\Omega_{\alpha}\}\). If \(\Omega_{\alpha}=\Omega\) for all \(\alpha\in I\), then \(\times_{\alpha\in I}\Omega_{\alpha}\) is also written as \(\Omega^{I}\).

It is a _basic axiom of set theory_, known as the _axiom of choice_ (A.C.), that this space is nonempty. That is, given an arbitrary collection of nonempty sets, it is possible to form a _parliament_ with one representative from each set.

For a long time it was thought this should follow from the other axioms of set theory. But it is shown in Cohen (1966) that it is an independent axiom. That is, both the A.C. and its negation are consistent with the rest of the axioms of set theory.

There are several equivalent versions of A.C. These are Zorn's lemma, Hausdorff's maximality principle, the 'Principle of Well Ordering,' and Tukey's lemma. For a proof of these equivalences, see Hewitt and Stromberg (1965).

**Definition A.1.9:** (_Functions, countability and uncountability_). A _function_\(f\) is a correspondence between the elements of a set \(X\) and another set \(Y\) and is written as \(f:X\to Y\). It satisfies the condition that for each \(x\), there is a unique \(y\) in \(Y\) that corresponds to it and is denoted as

\[y=f(x).\]

The set \(X\) is called the _domain of \(f\)_ and the set \(f(X)\), defined as, \(f(X)\equiv\{y:\) there exists \(x\) in \(X\) such that \(f(x)=y\}\) is called the _range of \(f\)_. It is possible that many \(x\)'s may correspond to the same \(y\) and also there may exist \(y\) in \(Y\) for which there is no \(x\) such that \(f(x)=y\). If \(f(X)\) is all of \(Y\), then the map is called _onto_. If for each \(y\) in \(f(X)\), there is a unique \(x\) in \(X\) such that \(f(x)=y\), then \(f\) is called (1-1) or _one-to-one_. If \(f\) is one-to-one and onto, then \(X\) and \(Y\) are said to have the same _cardinality_.

**Definition A.1.10:** Let \(f:X\to Y\) be (1-1) and onto. Then, for each \(y\) in \(Y\), there is a unique element \(x\) in \(X\) such that \(f(x)=y\). This \(x\) is denoted as \(f^{-1}(y)\). Note that in this case, \(g(y)\equiv f^{-1}(y)\) is a (1-1) onto map from \(Y\) to \(X\) and is called the _inverse_ of \(f\).

**Example A.1.10:** Let \(X=\mathbb{N}\equiv\{1,2,3,\ldots\}\). Let \(Y=\{n:n=2k,k\in\mathbb{N}\}\) be the set of even integers. Then the map \(f(x)=2x\) is a (1-1) onto map from \(X\) to \(Y\).

**Example A.1.11:** Let \(X\) be \(\mathbb{N}\) and let \(\mathbb{P}\) be the set of all prime numbers. Then \(X\) and \(\mathbb{P}\) have the same cardinality.

**Definition A.1.11:** A set \(X\) is _finite_ if there exists \(n\in\mathbb{N}\) such that \(X\) and \(Y\equiv\{1,2,\ldots,n\}\) have the same _cardinality_, i.e., there exists a (1-1) onto map from \(Y\) to \(X\). A set \(X\) is _countable_ if \(X\) and \(\mathbb{N}\) have the _same cardinality_, i.e., there exists a (1-1) onto map from \(\mathbb{N}\) to \(X\). A set \(X\) is _uncountable_ if it is not _finite_ or _countable_.

**Example A.1.12:** The set \(\{0,1,2,\ldots,9\}\) is finite, the set \(\mathbb{N}^{k}\) (\(k\in\mathbb{N}\)) is countable and \(\mathbb{N}^{\mathbb{N}}\) is uncountable (Problem A.6).

**Definition A.1.12:** Let \(\Omega\) be a nonempty set. Then the power set of \(\Omega\), denoted by \(\mathcal{P}(\Omega)\), is the collection of all subsets of \(\Omega\), i.e.,

\[\mathcal{P}(\Omega)\equiv\{A:A\subset\Omega\}.\]

**Remark A.1.1:**\(\mathcal{P}(\mathbb{N})\) is an uncountable set (Problem A.5).

#### The principle of induction

The set \(\mathbb{N}\) of natural numbers has the _well ordering property_ that every nonempty subset \(A\) of \(\mathbb{N}\) has a smallest element \(s\) such that (i) \(s\in A\) and (ii) \(a\in A\Rightarrow a\geq s\). This property is one of the basic postulates in the definition of \(\mathbb{N}\). The _principle of induction_ is a consequence of the well ordering property. It says the following:

_Let \(\{P(n):n\in\mathbb{N}\}\) be a collection of propositions (or statements). Suppose that_

1. \(P(1)\) _is true._
2. _For each_ \(n\in\mathbb{N}\)_,_ \(P(n)\) _true_ \(\Rightarrow P(n+1)\) _true._

_Then, \(P(n)\) is true for all \(n\in\mathbb{N}\)._

See Problem A.9 for some examples.

#### Equivalence relations

**Definition A.1.13:**

1. Let \(\Omega\) be a nonempty set. Let \(G\) be a nonempty subset of \(\Omega\times\Omega\). Write \(x\sim y\) if \((x,y)\in G\) and call it a _relation_ defined by \(G\).
2. A relation defined by \(G\) is an _equivalence relation_ if 1. (_reflexive_) for all \(x\) in \(\Omega\), \(x\sim x\), i.e., \((x,x)\in G\); 2. (_symmetric_) \(x\sim y\Rightarrow y\sim x\), i.e., \((x,y)\in G\Rightarrow(y,x)\in G\); 3. (_transitive_) \(x\sim y\), \(y\sim z\Rightarrow x\sim z\), i.e., \((x,y)\in G\), \((y,z)\in G\Rightarrow(x,z)\in G\).

**Example A.1.13:** Let \(\Omega=\mathbb{Z}\), the set of all integers, \(G=\{(m,n):m-n\) is divisible by \(3\}\). Thus, \(m\sim n\) if \((m-n)\) is a multiple of \(3\). It is easy to verify that this is an equivalence relation.

**Definition A.1.14:** (_Equivalence classes_). Let \(\Omega\) be a nonempty set. Let \(G\) define an equivalence relation on \(\Omega\). For each \(x\) in \(\Omega\), the set \([x]\equiv\{y:x\sim y\}\) is called the _equivalence class generated by \(x\)_.

**Proposition A.1.2:** _Let \(\mathcal{C}\) be the set of all equivalence classes in \(\Omega\) generated by an equivalence relation defined by \(G\). Then_

1. \(C_{1},C_{2}\in\mathcal{C}\Rightarrow C_{1}=C_{2}\) _or_ \(C_{1}\cap C_{2}=\emptyset\)_._
2. \(\bigcup\limits_{C\in\mathcal{C}}C=\Omega\)_._

**Proof:**

1. Suppose \(C_{1}\cap C_{2}\neq\emptyset\). Then there exist \(x_{1}\), \(x_{2}\), \(y\) such that \(C_{1}=[x_{1}]\), \(C_{2}=[x_{2}]\) and \(y\in C_{1}\cap C_{2}\). This implies \(x_{1}\sim y\), \(x_{2}\sim y\). But by symmetry \(y\sim x_{2}\) and this implies by transitivity that \(x_{1}\sim x_{2}\), i.e., \(x_{2}\in C_{1}\) implying \(C_{2}\subset C_{1}\). Similarly, \(C_{1}\subset C_{2}\), i.e., \(C_{1}=C_{2}\).
2. For each \(x\) in \(\Omega\), \((x,x)\in G\) and so \([x]\) is not empty and \(x\in[x]\). \(\Box\)

The above proposition says that every equivalence relation on \(\Omega\) leads to a decomposition of \(\Omega\) into equivalence classes that are disjoint and whose union is all of \(\Omega\). In the example given above, the set \(\mathbb{Z}\) of all integers can be decomposed to three equivalence classes \(\mathcal{C}_{j}\equiv\{n:n=3m+j\) for some \(m\in\mathbb{Z}\}\), \(j=0,1,2\).

### Real numbers, continuity, differentiability, and integration

#### Real numbers

This section reviews the following: integers, rationals, real numbers; algebraic, order, and completeness axioms; Archimedean property, denseness of rationals.

There are at least two approaches to defining the real number system.

Approach 1. Start with the natural numbers \(\mathbb{N}\), construct the set \(\mathbb{Z}\) of all integers \((\mathbb{N}\cup\{0\}\cup(-\mathbb{N}))\), and next, the set \(\mathbb{Q}\) of rationals and then the set \(\mathbb{R}\) of real numbers either as the set of all Cauchy sequences of rationals or as Dedekind cuts. The step going from \(\mathbb{Q}\) to \(\mathbb{R}\) via Cauchy sequences is also available for completing any incomplete metric space (see Section A.4).

**Approach 2**.: Define the set of real numbers \(\mathbb{R}\) as a set that satisfies three sets of axioms. The first set is _algebraic_ involving addition and multiplication. The second set is on _ordering_ that, with the first, makes \(\mathbb{R}\) an ordered field (see Royden (1988) for a definition). The third set is a single axiom known as the _completeness_ axiom. Thus \(\mathbb{R}\) is defined as a complete ordered field.

The _algebraic axioms_ say that there are two binary operations known as addition (\(+\)) and multiplication (\(\cdot\)) that render \(\mathbb{R}\) a field. See Royden (1988) for the nine axioms for this set.

The _order axiom_ says that there is a set \(\mathbb{P}\subset\mathbb{R}\), to be called positive numbers such that

1. \(x,y\in\mathbb{P}\Rightarrow x\cdot y\in\mathbb{P}\), \(x+y\in\mathbb{P}\)
2. \(x\in\mathbb{P}\Rightarrow-x\notin\mathbb{P}\)
3. \(x\in\mathbb{R}\Rightarrow x=0\) or \(x\in\mathbb{P}\) or \(-x\in\mathbb{P}\).

The set \(\mathbb{Q}\) of rational numbers is an ordered field (i.e., it satisfies the algebraic and order axioms). But \(\mathbb{Q}\) does not satisfy the completeness axiom (see below).

Given \(\mathbb{P}\), one can define an order on \(\mathbb{R}\) by defining \(x<y\) (read \(x\) less than \(y\)) to mean \(y-x\in\mathbb{P}\). Since for all \(x,y\) in \(\mathbb{R}\), \((x-y)\) is either \(0\) or \((x-y)\in\mathbb{P}\) or \((y-x)\in\mathbb{P}\), it follows that for all \(x,y\) in \(\mathbb{R}\), either \(x=y\) or \(x<y\) or \(x>y\). This is called _total_ or _linear order_.

**Definition A.2.1**: (_Upper and lower bounds_)_._

1. Let \(A\subset\mathbb{R}\). A real number \(M\) is an _upper bound_ for \(A\) if \(a\in A\Rightarrow a\leq M\) and \(m\) is a _lower bound_ for \(A\) if \(a\in A\Rightarrow a\geq m\).
2. The _supremum_ of a set \(A\), denoted by _sup_\(A\) or the _least upper bound (l.u.b.)_ of \(A\), is defined by the following conditions: 1. \(x\in A\Rightarrow x\leq\sup A\), 2. \(K<\sup A\Rightarrow\) there exists \(x\in A\) such that \(K<x\).

_The completeness axiom_ says that if \(A\subset\mathbb{R}\) has an upper bound \(M\) in \(\mathbb{R}\), then there exists a \(\tilde{M}\) in \(\mathbb{R}\) such that \(\tilde{M}=\sup A\).

That is, every set \(A\) that is bounded above in \(\mathbb{R}\) has a l.u.b. in \(\mathbb{R}\). The ordered field of rationals \(\mathbb{Q}\) does not possess this property. One well-known example is the set

\[A=\{r:r\in\mathbb{Q},r^{2}<2\}.\]

Then \(A\) is bounded above in \(\mathbb{Q}\) but has no l.u.b. in \(\mathbb{Q}\) (Problem A.11).

Next some consequences of the completeness axiom are discussed.

**Proposition A.2.1**: (_Axiom of Eudoxus and Archimedes (AOE)_)_. For all \(x\) in \(\mathbb{R}\), there exists a natural number \(n\) such that \(n>x\)._

[MISSING_PAGE_FAIL:586]

**Definition A.2.4:** (_Cauchy sequences_). A sequence \(\{x_{n}\}_{n\geq 1}\subset\mathbb{R}\) is called a _Cauchy sequence_ if for every \(\varepsilon>0\), there is \(N_{\varepsilon}\) such that \(n,m\geq N_{\varepsilon}\Rightarrow|x_{m}-x_{n}|<\varepsilon\).

**Proposition A.2.3:** _If \(\{x_{n}\}_{n\geq 1}\subset\mathbb{R}\) is convergent in \(\mathbb{R}\) (i.e., \(\lim_{n\to\infty}x_{n}=a\) exists in \(\mathbb{R}\)), then \(\{x_{n}\}_{n\geq 1}\) is Cauchy. Conversely, if \(\{x_{n}\}_{n\geq 1}\subset\mathbb{R}\) is Cauchy, then there exists an \(a\in\mathbb{R}\) such that \(\lim_{n\to\infty}x_{n}=a\)._

The proof is based on the use of the l.u.b. axiom (Problem A.14).

**Definition A.2.5:** Let \(\{x_{n}\}_{n\geq 1}\) be a sequence of real numbers. For \(n\geq 1\), \(s_{n}\equiv\sum_{j=1}^{n}x_{j}\) is called the _\(n\)th partial sum_ of the series \(\sum_{j=1}^{\infty}x_{j}\). The series \(\sum_{j=1}^{\infty}x_{j}\) is said to _converge_ to \(s\) in \(\mathbb{R}\) if \(\lim_{n\to\infty}s_{n}=s\). If \(\lim_{n\to\infty}s_{n}=\pm\infty\), then the series \(\sum_{j=1}^{\infty}x_{j}\) is said to _diverge_ to \(\pm\infty\).

Note that if \(x_{j}\geq 0\) for all \(j\), then either \(\lim_{n\to\infty}s_{n}=s\in\mathbb{R}\), or \(\lim_{n\to\infty}s_{n}=\infty\).

**Example A.2.1:** (_Geometric series_). Fix \(0<r<1\). Let \(x_{n}=r^{n}\), \(n\geq 0\). Then \(s_{n}=1+r+\ldots+r^{n}=\frac{1-r^{n+1}}{1-r}\) and \(\sum_{j=1}^{\infty}r^{j}\) converges to \(s=\frac{1}{1-r}\).

**Example A.2.2:** Consider the series \(\sum_{j=1}^{\infty}\frac{1}{j^{p}}\), \(0<p<\infty\). It can be shown that this converges for \(p>1\) and diverges to \(\infty\) for \(0<p\leq 1\).

**Definition A.2.6:** The series \(\sum_{j=1}^{\infty}x_{j}\)_converges absolutely_ if the series \(\sum_{j=1}^{\infty}|x_{j}|\) converges in \(\mathbb{R}\).

There exist series \(\sum_{j=1}^{\infty}x_{j}\) that converge but not absolutely. For example, \(\sum_{j=1}^{\infty}\frac{(-1)^{j}}{j}\). For further material on convergence properties of series, such as tests for convergence, rates of convergence, etc., see Rudin (1976).

**Definition A.2.7:** (_Power series_). Let \(\{a_{n}\}_{n\geq 0}\) be a sequence of real numbers. For \(x\in\mathbb{R}\), the series \(\sum_{n=0}^{\infty}a_{n}x^{n}\) is called a _power series_. If the series \(\sum_{n=0}^{\infty}a_{n}x^{n}\) converges for all \(x\) in \(B\subset\mathbb{R}\), the _power series_\(\sum_{n=0}^{\infty}a_{n}x^{n}\) is said to be _convergent on \(B\)_.

**Proposition A.2.4:** _Let \(\{a_{n}\}_{n\geq 0}\) be a sequence of real numbers. Let \(\rho=(\limsup_{n\to\infty}|a_{n}|^{\frac{1}{n}})^{-1}\). Then_

* \(|x|<\rho\Rightarrow\sum_{n=0}^{\infty}|a_{n}x^{n}|\) _converges._
* \(|x|>\rho\Rightarrow\sum_{n=0}^{\infty}|a_{n}x^{n}|\) _diverges to_ \(+\infty\)_._

Proof of this is left as an exercise (Problem A.15).

**Definition A.2.8:** \(\rho\equiv(\limsup_{n\to\infty}|a_{n}|^{\frac{1}{n}})^{-1}\) is called the _radius of convergence_ of the power series \(\sum_{n=0}^{\infty}a_{n}x^{n}\).

#### _Continuity and differentiability_

**Definition A.2.9**: Let \(f:A\to\mathbb{R}\), \(A\subset\mathbb{R}\). Then

* \(f\) is _continuous at_ \(x_{0}\) in \(A\) if for every \(\epsilon>0\), there exists a \(\delta>0\) such that \(x\in A\), \(|x-x_{0}|<\delta\), implies \(|f(x)-f(x_{0})|<\epsilon\). (Here, \(\delta\) may depend on \(\epsilon\) and \(x_{0}\).)
* \(f\) is _continuous on_ \(B\subset A\) if it is continuous at every \(x_{0}\) in \(B\).
* \(f\) is _uniformly continuous_ on \(B\subset A\) if for every \(\epsilon>0\), there exists a \(\delta_{\epsilon}>0\) such that \(\sup\{|f(x)-f(y)|:x,y\in B,|x-y|<\delta_{\epsilon}\}<\epsilon\).

Some properties of continuous functions are listed below.

**Proposition A.2.5**:
* \((\)_Sums, products, and ratios of continuous functions_\()\)_. Let_ \(f\)_,_ \(g:A\to\mathbb{R}\)_,_ \(A\subset\mathbb{R}\)_. Let_ \(f\) _and_ \(g\) _be continuous on_ \(B\subset A\)_. Then_
* \(f+g\)_,_ \(f-g\)_,_ \(\alpha\cdot f\) _for any_ \(\alpha\in\mathbb{R}\) _are all continuous on_ \(B\)_._
* \(f(x)/g(x)\) _is continuous at_ \(x_{0}\) _in_ \(B\)_, provided_ \(g(x_{0})\neq 0\)_._
* \((\)_Continuous functions on a closed bounded interval_\()\)_. Let_ \(f\) _be continuous on a closed and bounded interval_ \([a,b]\)_. Then_
* \(f\) _is bounded, i.e.,_ \(\sup\{|f(x)|:a\leq x\leq b\}<\infty\)_,_
* _it achieves its maximum and minimum, i.e., there exist_ \(x_{0}\)_,_ \(y_{0}\) _in_ \([a,b]\) _such that_ \(f(x_{0})\geq f(x)\geq f(y_{0})\) _for all_ \(x\) _in_ \([a,b]\) _and_ \(f\) _attains all values in_ \([f(y_{0}),f(x_{0})]\)_, i.e., for all_ \(\ell\in[f(y_{0}),f(x_{0})]\)_, there exists_ \(z\in[a,b]\) _such that_ \(f(z)=\ell\)_. Thus,_ \(f\) _maps bounded closed intervals onto bounded closed intervals._
* \(f\) _is uniformly continuous on_ \([a,b]\)_._
* \((\)_Composition of functions_\()\)_. Let_ \(f:A\to\mathbb{R}\)_,_ \(g:B\to\mathbb{R}\) _be continuous on_ \(A\) _and_ \(B\)_, respectively. Let_ \(f(A)\subset B\)_, i.e., for any_ \(x\) _in_ \(A\)_,_ \(f(x)\in B\)_. Let_ \(h(x)=g(f(x))\) _for_ \(x\) _in_ \(A\)_. Then_ \(h:A\to\mathbb{R}\) _is continuous._
* \((\)_Uniform limits of continuous functions_\()\)_. Let_ \(\{f_{n}\}_{n\geq 1}\)_, be a sequence of functions continuous on_ \(A\) _to_ \(\mathbb{R}\)_,_ \(A\subset\mathbb{R}\)_. If_ \(\sup\{|f_{n}(x)-f(x)|:x\in A\}\to 0\) _as_ \(n\to\infty\) _for some_ \(f:A\to\mathbb{R}\)_, i.e.,_ \(f_{n}\) _converges to_ \(f\) _uniformly on_ \(A\), _then_ \(f\) _is continuous on_ \(A\)_._

**Remark A.2.2**: The function \(f(x)\equiv x\) is clearly continuous on \(\mathbb{R}\). Now by Proposition A.2.5 (i) and (iv), it follows that all polynomials are continuous on \(\mathbb{R}\), and hence, so are their uniform limits. _Weierstrass' approximation theorem_ is a sort of converse to this. That is, every continuous function on a closed and bounded interval is the uniform limit of polynomials. More precisely, one has the following:Theorem A.2.6: Let \(f:[a,b]\to\mathbb{R}\) be continuous. Then for any \(\epsilon>0\) there is a polynomial \(p(x)=\sum_{0}^{n}a_{j}x^{j}\), \(a_{j}\in\mathbb{R}\), \(j=0,1,2,\ldots,n\) such that \(\sup\{|f(x)-p(x)|:x\in[a,b]\}<\epsilon\).

It should be noted that a power series \(A(x)\equiv\sum_{0}^{\infty}a_{n}x^{n}\) is the uniform limits of polynomials on \([-\lambda,\lambda]\) for any \(0<\lambda<\rho\equiv\bigl{(}\overline{\lim}_{n\to\infty}|a_{n}|^{1/n}\bigr{)}^ {-1}\) and hence is continuous on \((-\rho,\rho)\).

Definition Definition A.2.10: Let \(f:(a,b)\to\mathbb{R}\), \((a,b)\subset\mathbb{R}\). The function \(f\) is said to be differentiable at \(x_{0}\in(a,b)\) if

\[\lim_{h\to 0}\frac{f(x_{0}+h)-f(x_{0})}{h}\equiv f^{\prime}(x_{0})\quad\text{ exists in}\quad\mathbb{R}.\]

A function is differentiable in \((a,b)\) if it is differentiable at each \(x\) in \((a,b)\).

Some important consequences of differentiability are listed below.

Proposition A.2.7: Let \(f\), \(g:(a,b)\to\mathbb{R}\), \((a,b)\subset\mathbb{R}\). Then

1. \(f\) differentiable at \(x_{0}\) in \((a,b)\) implies \(f\) is continuous at \(x_{0}\).
2. \((\)Mean value theorem\()\). \(f\) differentiable on \((a,b)\), \(f\) continuous on \([a,b]\) implies that for some \(a<c<b\), \(f(b)-f(a)=(b-a)f^{\prime}(c)\).
3. \((\)Maxima and minima\()\). \(f\) differentiable at \(x_{0}\) and for some \(\delta>0\), \(f(x)\leq f(x_{0})\) for all \(x\in(x_{0}-\delta,x_{0}+\delta)\) implies that \(f^{\prime}(x_{0})=0\).
4. \((\)Sums, products and ratios\()\). \(f\), \(g\) differentiable at \(x_{0}\) implies that for any \(\alpha\), \(\beta\) in \(\mathbb{R}\), \((\alpha f+\beta g)\), \(f-g\) are differentiable at \(x_{0}\) with \[(\alpha f+\beta g)^{\prime}(x_{0}) = \alpha f^{\prime}(x_{0})+\beta g^{\prime}(x_{0}),\] \[(fg)^{\prime}(x_{0}) = f^{\prime}(x_{0})g(x_{0})+f(x_{0})g^{\prime}(x_{0}),\] and if \(g^{\prime}(x_{0})\neq 0\), then \(f/g\) is differentiable at \(x_{0}\) with \[(f/g)^{\prime}(x_{0})=\frac{f^{\prime}(x_{0})g(x_{0})-f(x_{0})g^{\prime}(x_{0} )}{(g(x_{0}))^{2}}.\]
5. \((\)Chain rule\()\). If \(f\) is differentiable at \(x_{0}\) and \(g\) is differentiable at \(f(x_{0})\), then \(h(x)\equiv g(f(x))\) is differentiable at \(x_{0}\) with \(h^{\prime}(x_{0})=g^{\prime}(f(x_{0}))f^{\prime}(x_{0})\).
6. \((\)Differentiability of power series\()\). Let \(A(x)\equiv\sum_{n=0}^{\infty}a_{n}x^{n}\) be a power series with radius of convergence \(\rho\equiv\bigl{(}\overline{\lim}_{n\to\infty}|a_{n}|^{1/n}\bigr{)}^{-1}>0\). Then \(A(\cdot)\) is differentiable infinitely many times on \((-\rho,\rho)\) and for \(x\) in \((-\rho,\rho)\), \[\frac{d^{k}A(x)}{dx^{k}}=\sum_{n=k}^{\infty}n(n-1)\cdots(n-k+1)x^{n-k},\ k\geq 1.\]

**Remark A.2.3:** It should be noted that the converse to (a) in the above proposition does not hold. For example, the function \(f(x)=|x|\) is continuous at \(x_{0}=0\) but is not differentiable at \(x_{0}\). Indeed, Weierstrass showed that there exists a function \(f:[0,1]\to\mathbb{R}\) such that it is continuous on \([0,1]\) but is not differentiable at any \(x\) in \((0,1)\).

Also note that the mean value theorem implies that if \(f^{\prime}(\cdot)\geq 0\) on \((a,b)\), then \(f\) is nondecreasing on \((a,b)\).

**Definition A.2.11:** (_Taylor series_). Let \(f\) be a map from \(I\equiv(a-\eta,a+\eta)\) to \(\mathbb{R}\) for some \(a\in\mathbb{R}\), \(\eta>0\). Suppose \(f\) is \(n\) times differentiable in \(I\), for each \(n\geq 1\). Let \(a_{n}=\frac{f^{(n)}(a)}{n!}\). Then power series \(\sum_{n=0}^{\infty}a_{n}(x-a)^{n}=\sum_{n=0}^{\infty}\frac{f^{(n)}(a)}{n!}(x-a )^{n}\) is called the _Taylor series_ of \(f\) at \(a\).

**Remark A.2.4:** Let \(f\) be as in Definition A.2.11. _Taylor's remainder theorem_ says that for any \(x\) in \(I\) and any \(n\geq 1\), if \(f\) is \((n+1)\) times differentiable in \(I\), then

\[\left|f(x)-\sum_{j=0}^{n}a_{j}x^{j}\right|\leq\frac{|f^{(n+1)}(y_{n})|}{(n+1)!}\]

for some \(y_{n}\) in \(I\).

Thus, if for some \(\epsilon>0\), \(\sup_{|y-a|<\epsilon}|f^{(k)}(y)|\equiv\lambda_{k}\) satisfies \(\frac{\lambda_{k}}{k!}\to 0\) as \(k\to\infty\), then the Taylor series satisfies

\[\sup_{|x-a|<\epsilon}\left|\sum_{j=0}^{n}(x-a)^{j}\frac{f^{(j)}(a)}{j!}-f(x) \right|\to 0\quad\mbox{as}\quad n\to\infty.\]

#### a.2.4 Riemann integration

Let \(f:[a,b]\to\mathbb{R}\), \([a,b]\subset\mathbb{R}\). Let \(\sup\{|f(x)|:a\leq x\leq b\}<\infty\). For any partition \(P\equiv\{a=x_{0}<x_{1}<x_{2}<x_{k}=b\}\), let

\[U(P,f) \equiv \sum_{i=1}^{n}M_{i}(P)\Delta_{i}(P)\quad\mbox{and}\] \[L(P,f) \equiv \sum_{i=1}^{n}m_{i}(P)\Delta_{i}(P)\]

where

\[M_{i}(P) = \sup\{f(x):x_{i}\leq x\leq x_{i}\},\] \[m_{i}(P) = \inf\{f(x):x_{i-1}\leq x\leq x_{i}\},\] \[\Delta_{i}(P) = (x_{i}-x_{i-1})\]for \(i=1,2,\ldots,k\).

**Definition A.2.12:** The upper and lower Riemann integrals of \(f\) over \([a,b]\), denoted by \(\overline{\int_{a}^{b}}f(x)dx\) and \(\underline{\int_{a}^{b}}f(x)dx\), respectively defined as

\[\overline{\int_{a}^{b}}f(x)dx \equiv \inf\{U(P,f):P\ \mbox{ a partition of }\ [a,b]\}\] \[\underline{\int_{a}^{b}}f(x)dx \equiv \sup\{L(P,f):P\ \mbox{ a partition of }\ [a,b]\}.\]

**Definition A.2.13:** Let \(f:[a,b]\rightarrow\mathbb{R}\), \([a,b]\subset\mathbb{R}\) and let \(\sup\{|f(x)|:a\leq x\leq b\}<\infty\). The function \(f\) is Riemann integrable on \([a,b]\) if

\[\overline{\int_{a}^{b}}f(x)dx=\underline{\int_{a}^{b}}f(x)dx\]

and the common value is denoted as \(\int_{a}^{b}f(x)dx\).

The following are some important results on Riemann integration.

**Proposition A.2.8:**

1. _Let_ \(f:[a,b]\rightarrow\mathbb{R}\)_,_ \([a,b]\subset\mathbb{R}\) _be continuous. Then_ \(f\) _is Riemann integrable and_ \(\int_{a}^{b}f(x)dx=\lim_{n\rightarrow\infty}\frac{1}{k_{n}}\sum_{i=0}^{k_{n}} f(x_{ni})\Delta_{ni}\) _where_ \(\{P_{n}\equiv\{x_{ni}:1\leq i\leq k_{n}\}\}\) _is a sequence of partitions of_ \([a,b]\) _such that_ \(\Delta_{n}\equiv\{\max(x_{ni}-x_{n,i-1}):1\leq i\leq k_{n}\}\to 0\) _as_ \(n\rightarrow\infty\)_._
2. _If_ \(f\)_,_ \(g\) _are Riemann integrable on_ \([a,b]\)_, then_ \(\alpha f+\beta g\)_,_ \(\alpha,\beta\in\mathbb{R}\) _and_ \(fg\) _are Riemann integrable on_ \([a,b]\) _and_ \[\int_{a}^{b}(\alpha f+\beta g)(x)dx=\alpha\int_{a}^{b}f(x)dx+\beta\int_{a}^{b}g (x)dx.\]
3. _Let_ \(f\) _be Riemann integrable on_ \([a,b]\) _and_ \([b,c]\)_,_ \(-\infty<a<b<c<\infty\)_. Then_ \(f\) _is Riemann integrable on_ \([a,c]\) _and_ \[\int_{a}^{c}f(x)dx=\int_{a}^{b}f(x)dx+\int_{b}^{c}f(x)dx.\]
4. \((\)_Fundamental theorem of Riemann integration: Part \(I)\). Let_ \(f\) _be Riemann integrable on_ \([a,b]\)_. Then it is Riemann integrable on_ \([a,c]\) _for all_ \(a\leq c\leq b\)_. Let_ \(F(x)\equiv\int_{a}^{x}f(u)du\)_,_ \(a\leq x\leq b\)_. Then_ 1. \(F(\cdot)\) _is continuous on_ \([a,b]\)_._ 2. _If_ \(f(\cdot)\) _is continuous at_ \(x_{0}\in(a,b)\)_, then_ \(F(\cdot)\) _is differentiable at_ \(x_{0}\) _and_ \(F^{\prime}(x_{0})=f(x_{0})\)_
* (_Fundamental theorem: Part II_)_. If_ \(F:[a,b]\to\mathbb{R}\) _is differentiable on_ \((a,b)\)_,_ \(f:[a,b]\to\mathbb{R}\) _is continuous on_ \([a,b]\) _and_ \(F^{\prime}(x)=f(x)\) _for all_ \(a<x<b\)_, then_ \(F(x)=F(a)+\int_{a}^{c}f(x)dx\)_,_ \(a\leq c\leq b\)_._
* (_Integration by parts_)_. If_ \(f\) _and_ \(g\) _are continuous and differentiable on_ \((a,b)\) _with_ \(f^{\prime}\) _and_ \(g^{\prime}\) _continuous on_ \((a,b)\)_, then_ \[\int_{c}^{d}f(x)g^{\prime}(x)dx+\int_{c}^{d}f^{\prime}(x)g(x)dx\] \[= f(d)g(d)-f(c)g(c)\quad\text{for all}\quad a<c<d<b.\]
* (_Interchange of limits and integration_)_. Let_ \(f_{n}\)_,_ \(f\) _be continuous on_ \([a,b]\) _to_ \(\mathbb{R}\)_. Let_ \(f_{n}\) _converge to_ \(f\) _uniformly on_ \([a,b]\)_. Then_ \[F_{n}(c)\equiv\int_{a}^{c}f_{n}(x)dx\to F(c)\equiv\int_{a}^{c}f(x)dx\] _uniformly on_ \([a,b]\)_._

### Complex numbers, exponential and trigonometric functions

**Definition A.3.1:** The set \(\mathbb{C}\) of complex numbers is defined as the set \(\mathbb{R}\times\mathbb{R}\) of all ordered pairs of real numbers endowed with addition and multiplication as follows:

\[(a_{1},b_{1})+(a_{2},b_{2}) = (a_{1}+a_{2},b_{1}+b_{2})\] \[(a_{1},b_{1})\cdot(a_{2},b_{2}) = (a_{1}a_{2}-b_{1}b_{2},a_{1}b_{2}+a_{2}b_{1}). \tag{3.1}\]

It can be verified that \(\mathbb{C}\) satisfies the field axioms Royden (1988). By defining \(\iota\) to be the element (0,1), one can write the elements of \(\mathbb{C}\) in the form \((a,b)=a+\iota b\) and do addition and multiplication with the rule of replacing \(\iota^{2}\) by \(-1\). Clearly, the set \(\mathbb{R}\) of real numbers can be identified with the set \(\{(a,0):a\in\mathbb{R}\}\). The set \(\{(0,b):b\in\mathbb{R}\}\) is called the set of _purely imaginary numbers_.

**Definition A.3.2:** For any complex number \(z=(a,b)\) in \(\mathbb{C}\),

\[\begin{array}{ll}\text{Re}(z),&\text{called the \emph{real part of} $z$ is $a$},\\ \text{Im}(z),&\text{called the \emph{imaginary part of} $z$ is $b$},\\ \bar{z},&\text{called the \emph{complex conjugate of} $z$ is $\bar{z}=a-\iota b$}\\ |z|,&\text{called the \emph{absolute value of} $z$ is $|z|=\sqrt{a^{2}+b^{2}}$ }.\end{array} \tag{3.2}\]

[MISSING_PAGE_FAIL:593]

_._
* _There is a smallest positive number, called_ \(\pi\)_, such that_ \(e^{\iota\pi/2}=\iota\)_._
* \(e^{z}\) _is a periodic function such that_ \(e^{z}=e^{z+\iota 2\pi}\)_._

**Proof:**

* Since \(\sum_{n=0}^{\infty}\frac{|z|^{n}}{n!}\) converges for all \(z\in\mathbb{C}\), \[e^{z_{1}}\cdot e^{z_{2}} = \lim_{m\to\infty}\bigg{(}\sum_{n=0}^{m}\frac{z_{1}^{n}}{n!} \bigg{)}\bigg{(}\sum_{n=0}^{m}\frac{z_{2}^{n}}{n!}\bigg{)}\] \[= \lim_{m\to\infty}\sum_{0\leq r,s\leq m}\frac{z_{1}^{r}z_{2}^{s}}{ r!s!}\] \[= \lim_{m\to\infty}\sum_{k=0}^{2m}\frac{1}{k!}\sum_{r=0}^{k}\frac{k!}{r!(k-r)!}\,z_{1}^{r}z_{2}^{k-r}\] \[= \lim_{m\to\infty}\sum_{k=0}^{2m}\frac{(z_{1}+z_{2})^{k}}{k!}=e^{ z_{1}+z_{2}}.\] Since \(e^{z}\cdot e^{-z}=e^{0}=1\), \(e^{z}\neq 0\) for any \(z\in\mathbb{C}\).
* Fix \(z\in\mathbb{C}\). For any \(h\in\mathbb{C}\), \(h\neq 0\), by (i), \[\frac{e^{z+h}-e^{z}}{h}=e^{z}\frac{e^{h}-1}{h}.\] But \[\Big{|}\frac{e^{h}-1}{h}-1\Big{|} \leq \bigg{(}\sum_{k=2}^{\infty}\frac{|h|^{k}}{k!}\bigg{)}\frac{1}{|h|}\] \[\leq |h|\sum_{k=0}^{\infty}\frac{1}{k!}\quad\mbox{if}\quad|h|\leq 1.\] Thus \[\lim_{h\to 0}\Big{(}\frac{e^{h}-1}{h}-1\Big{)}=0\] and \[\lim_{h\to 0}\Big{(}\frac{e^{z+h}-e^{z}}{h}\Big{)}=e^{z},\quad\mbox{i.e. (ii) holds}.\]
* That the map \(t\to e^{t}\) is strictly increasing on \([0,\infty)\) and that \(e^{t}\uparrow\infty\) as \(t\uparrow\infty\) is clear from the definition of \(e^{z}\). Since \(e^{-t}e^{t}=e^{0}=1\), \(e^{-t}=\frac{1}{e^{t}}\) for all \(t\in\mathbb{R}\), so that \(e^{t}\) is strictly increasing on all of \(\mathbb{R}\) and \(e^{t}\downarrow 0\) as \(t\downarrow-\infty\).

* From the definition of \(e^{z}\), it follows that \(\overline{e^{z}}=e^{\bar{z}}\) and, in particular, for \(t\) real \(\overline{e^{tt}}=e^{\overline{t}\overline{t}}=e^{-\iota t}\) and hence \[|e^{\iota t}|^{2}=\overline{e^{tt}}e^{\iota t}=e^{-\iota t+\iota t}=e^{0}=1.\] Thus, for all \(t\) real \(|e^{\iota t}|=1\) and since \(e^{\iota t}=\cos t+\iota\sin t\), it follows that \[|e^{\iota t}|^{2}=(\cos t)^{2}+(\sin t)^{2}=1.\] Also from (ii), for \(t\in\mathbb{R}\) \[(e^{\iota t})^{\prime}=(\cos t)^{\prime}+\iota(\sin t)^{\prime}=\iota e^{ \iota t}=-\sin t+\iota\cos t\] yielding \((\cos t)^{\prime}=-\sin t\), \((\sin t)^{\prime}=\cos t\) proving (iv).
* By definition \[\cos 2=\sum_{k=0}^{\infty}(-1)^{k}\frac{2^{2k}}{(2k)!}\.\] Now \(a_{k}\equiv\frac{2^{2k}}{(2k)!}\) satisfies \(a_{k+1}<a_{k}\) for \(k=0,1,2,\ldots\) and hence \(\sum_{k\geq 3}(-1)^{k}a_{k}=-\{(a_{3}-a_{4})+(a_{5}-a_{6})+\cdots\}<0\). Thus, \[\cos 2<1-\frac{2^{2}}{2!}+\frac{2^{4}}{4!}=-\frac{1}{3}<0.\] Since \(\cos t\) is a continuous function on \(\mathbb{R}\) with \(\cos 0=1\) and \(\cos 2<0\), there exists a smallest \(t_{0}>0\) such that \(\cos t_{0}=0\), defined by \(t_{0}=\inf\{t:t>0,\cos t=0\}\). Set \(\pi=2t_{0}\). Since \(\cos\frac{\pi}{2}=0\), (iv) implies \(\sin\frac{\pi}{2}=1\) and hence that \(e^{\iota\frac{\pi}{2}}=\iota\).
* Clearly, \(e^{\iota\frac{\pi}{2}}=\iota\) implies that \(e^{\iota\pi}=-1\) and \(e^{\iota 2\pi}=1\) and \(e^{\iota 2\pi k}=1\) for all integers \(k\). Since \(e^{\iota 2\pi}=1\), it follows that \(e^{z}=e^{z+\iota 2\pi}\) for all \(z\in\mathbb{C}\),

\(\Box\)

It is now possible to prove various results involving \(\pi\) that one learns in calculus from the above definition. For example, that the arc length of the _unit circle_\(\{z:|z|=1\}\) is \(2\pi\) and that \(\int_{-\infty}^{\infty}\frac{1}{1+x^{2}}dx=\pi\), etc. (Problems A.19 and A.20).

The following assertions about \(e^{z}\) can be proved with some more effort.

**Theorem A.3.2:**

* \(e^{z}=1\) _iff_ \(z=2\pi\iota k\) _for some integer_ \(k\)_._
* _The map_ \(t\to e^{\iota t}\) _from_ \(\mathbb{R}\) _is_ onto _the unit circle._
* _For any_ \(\omega\in\mathbb{C}\)_,_ \(\omega\neq 0\) _there is a_ \(z\in\mathbb{C}\) _such that_ \(\omega=e^{z}\)For a proof of this theorem as well as more details on Theorem A.3.1, see Rudin (1987).

**Theorem A.3.3:** (_Orthogonality of \(\{e^{\iota 2\pi nt}\}_{n\in\mathbb{Z}}\)_). For any \(n\in\mathbb{Z}\), \(\int_{0}^{1}e^{\iota 2\pi nt}dt=0\) if \(n\neq 0\) and 1 if \(n=0\)._

**Proof:** Since \((e^{\iota t})^{\prime}=\iota e^{\iota t}\)

\[(e^{\iota 2\pi nt})^{\prime}=\iota 2\pi n\,e^{\iota 2\pi nt},\ n\in\mathbb{Z}\]

and so for \(n\neq 0\),

\[\int_{0}^{1}e^{\iota 2\pi nt}dt = \frac{1}{\iota 2\pi n}\int_{0}^{1}(e^{\iota 2\pi nt})^{\prime}dt\] \[= \frac{1}{\iota 2\pi n}(e^{\iota 2\pi n}-1)=0.\]

**Corollary A.3.4:** _The family \(\{\cos 2\pi nt:n=0,1,2,\ldots\}\cup\{\sin 2\pi nt:n=1,2,\ldots\}\) are orthogonal in \(L^{2}[0,1]\) (Problem A.22), i.e., for any two \(f\), \(g\) in this family, \(\int_{0}^{1}f(x)g(x)dx=0\) for \(f\neq g\)._

### Metric spaces

#### Basic definitions

This section reviews the following: metric spaces, Cauchy sequences, completeness, functions, continuity, compactness, convergence of sequences functions, and uniform convergence.

**Definition A.4.1:** Let \(\mathbb{S}\) be a nonempty set. Let \(d:\mathbb{S}\times\mathbb{S}\rightarrow\mathbb{R}_{+}=[0,\infty)\) be such that

* \(d(x,y)=d(y,x)\) for any \(x,y\) in \(\mathbb{S}\).
* \(d(x,z)\leq d(x,y)+d(y,z)\) for any \(x,y,z\) in \(\mathbb{S}\).
* \(d(x,y)=0\) iff \(x=y\).

Such a \(d\) is called a _metric_ on \(\mathbb{S}\) and the pair \((\mathbb{S},d)\) a _metric space_. Property (ii) is called the _triangle inequality_.

**Example A.4.1:** Let \(\mathbb{R}^{k}\equiv\{(x_{1},\ldots,x_{k}):x_{i}\in\mathbb{R}\), \(1\leq i\leq k\}\) be the \(k\)-dimensional Euclidean space. For \(1\leq p<\infty\) and \(x=(x_{1},\ldots,x_{k}),\ y=(y_{1},y_{2},\ldots,y_{k})\in\mathbb{R}^{k}\), let

\[d_{p}(x,y)=\bigg{(}\sum_{i=1}^{k}|x_{i}-y_{i}|^{p}\bigg{)}^{\frac{1}{p}},\]and \(d_{\infty}(x,y)=\max\{|x_{i}-y_{i}|:1\leq i\leq k\}\).

It can be shown that \(d_{p}(\cdot,\cdot)\) is a metric on \(\mathbb{R}\) for all \(1\leq p\leq\infty\) (Problem A.24).

**Definition A.4.2:** A sequence \(\{x_{n}\}_{n\geq 1}\) in a metric space \((\mathbb{S},d)\)_converges_ to an \(x\) in \(\mathbb{S}\) if for every \(\varepsilon>0\), there is a \(N_{\varepsilon}\) such that \(n\geq N_{\varepsilon}\Rightarrow d(x_{n},x)<\varepsilon\) and is written as \(\lim_{n\to\infty}x_{n}=x\).

**Definition A.4.3:** A sequence \(\{x_{n}\}_{n\geq 1}\) in a metric space \((\mathbb{S},d)\) is _Cauchy_ if for all \(\varepsilon>0\), there exists \(N_{\varepsilon}\) such that \(n\), \(m\geq N_{\varepsilon}\Rightarrow d(x_{n},x_{m})<\varepsilon\).

**Definition A.4.4:** A metric space \((\mathbb{S},d)\) is _complete_ if every Cauchy sequence \(\{x_{n}\}_{n\geq 1}\) converges to some \(x\) in \(\mathbb{S}\).

**Example A.4.2:**

* Let \(\mathbb{S}=\mathbb{Q}\), the set of rationals and \(d(x,y)\equiv|x-y|\). Then \((\mathbb{Q},d)\) is a metric space that is _not_ complete.
* Let \(\mathbb{S}=\mathbb{R}\) and \(d(x,y)=|x-y|\). Then \((\mathbb{R},d)\) is _complete_ (cf. Proposition A.2.3).
* Let \(\mathbb{S}=\mathbb{R}^{k}\). Then \((\mathbb{R}^{k},d_{p})\) is complete for every \(1\leq p\leq\infty\), where \(d_{p}\) is as in Example A.4.1.

**Remark A.4.1:** (_Completion of an incomplete metric space_). Let \((\mathbb{S},d)\) be a metric space. Let \(\tilde{\mathbb{S}}\) be the set of all Cauchy sequences in \(\mathbb{S}\). Identify each \(x\) in \(\mathbb{S}\) with the Cauchy sequence \(\{x_{n}=x\}_{n\geq 1}\). Define a function from \(\tilde{\mathbb{S}}\times\tilde{\mathbb{S}}\) to \(\mathbb{R}_{+}\) by

\[\tilde{d}(\{x_{n}\}_{n\geq 1},\{y_{n}\}_{n\geq 1})=\limsup_{n\to\infty}\ d(x_{n}, y_{n}).\]

It is easy to verify that \(\tilde{d}\) is symmetric and satisfies the triangle inequality. Define \(s_{1}=\{x_{n}\}_{n\geq 1}\) and \(s_{2}=\{y_{n}\}_{n\geq 1}\) to be _equivalent_ (write \(\{x_{n}\}\sim\{y_{n}\}\)) if \(\tilde{d}(s_{1},s_{2})=0\). Let \(\tilde{\mathbb{S}}\) be the set of all equivalence classes in \(\tilde{\mathbb{S}}\) and define \(\tilde{d}(c_{1},c_{2})\equiv\tilde{d}(s_{1},s_{2})\), where \(c_{1},c_{2}\) are equivalence classes and \(s_{1},s_{2}\) are arbitrary elements of \(c_{1}\) and \(c_{2}\), respectively.

It can now be verified that \((\tilde{\mathbb{S}},\tilde{d})\) is a complete metric space and \((\mathbb{S},d)\) is embedded in \((\tilde{\mathbb{S}},\tilde{d})\) by identifying each \(x\) in \(\mathbb{S}\) with the equivalence class containing the sequence \(\{x_{n}=x\}_{n\geq 1}\).

**Definition A.4.5:** A metric space \((\mathbb{S},d)\) is _separable_ if there exists a subset \(D\subset\mathbb{S}\) that is countable and _dense in \(\mathbb{S}\)_, i.e., for each \(x\) in \(\mathbb{S}\) and \(\varepsilon>0\), there is a \(y\) in \(D\) such that \(d(x,y)<\varepsilon\).

**Example A.4.3:** By the Archimedean property, \(\mathbb{Q}\) is dense in \(\mathbb{R}\). Similarly \(\mathbb{Q}^{k}\), the set of all \(k\) vectors with components from \(\mathbb{Q}\), is dense in \(\mathbb{R}^{k}\).

Definition **A.4.6**: _A metric space \((\mathbb{S},d)\) is called Polish if it is complete and separable._

Example **A.4.4**: \((\mathbb{R}^{k},d_{p})\) in Example **A.4.2** is Polish.

#### 4.4.2 Continuous functions

Let \((\mathbb{S},d)\) and \((\mathbb{T},\rho)\) be two metric spaces. Let \(f:\mathbb{S}\rightarrow\mathbb{T}\) be a map from \(\mathbb{S}\) to \(\mathbb{T}\).

Definition **A.4.7**:
* \(f\) is continuous at \(p\) in \(\mathbb{S}\) if for each \(\varepsilon>0\), there exists \(\delta>0\) such that \(d(x,p)<\delta\Rightarrow\rho(f(x),f(p))<\varepsilon\). (Here the \(\delta\) may depend on \(\varepsilon\) and \(p\).)
* \(f\) is continuous on a set \(B\subset\mathbb{S}\) if it is continuous at every \(p\in B\).
* \(f\) is _uniformly continuous_ on \(B\) if for each \(\varepsilon>0\), there exists \(\delta>0\) such that for each pair \(x,y\) in \(\mathbb{S}\), \(d(x,y)<\delta\Rightarrow\rho(f(x),f(y))<\varepsilon\).

Definition **A.4.8**: _Let \((\mathbb{S},d)\) be a metric space._

* A set \(O\subset(\mathbb{S},d)\) is open if \(x\in O\Rightarrow\) there exists \(\delta>0\) such that \(d(x,y)<\delta\Rightarrow y\in O\). That is, at every point \(x\) in \(O\), an open ball \(B_{x}(\delta)\equiv\{y:d(x,y)<\delta\}\) of positive radius \(\delta\) is a subset of \(O\).
* A set \(C\subset(\mathbb{S},d)\) is closed if \(C^{c}\) is open.

Theorem **A.4.1**: _Let \((\mathbb{S},d)\) and \((\mathbb{T},\rho)\) be metric spaces. A map \(f:\mathbb{S}\rightarrow\mathbb{T}\) in continuous on \(\mathbb{S}\) iff for each \(O\) open in \(\mathbb{T}\), \(f^{-1}(O)\) is open in \(\mathbb{S}\)._

Proof is left as an exercise (Problem **A.28**).

#### 4.4.3 Compactness

Definition **A.4.9**: _A collection of open sets \(\{O_{\alpha}:\alpha\in I\}\) is an open cover for a set \(B\subset(\mathbb{S},d)\) if for each \(x\in B\), there exists \(\alpha\in I\) such that \(x\in O_{\alpha}\)._

Example **A.4.5**: _Let \(B=(0,1)\). Then the collection \(\{(\alpha-\frac{\alpha}{2},\alpha+\frac{(1-\alpha)}{2}):\alpha\in\mathbb{Q} \cap(0,1)\}\) is an open cover for \(B\)._

Definition **A.4.10**: _Let \((\mathbb{S},d)\) be a metric space. A set \(K\subset\mathbb{S}\) is called compact if given any open cover \(\{O_{\alpha}:\alpha\in I\}\) for \(K\), there exists a finite subcollection \(\{O_{\alpha_{i}}:\alpha_{i}\in I,i=1,2,\ldots,n,n<\infty\}\) that is an open cover for \(K\)._

Example **A.4.6**: _The set \(B=(0,1)\) is not compact as the open cover in the above Example **A.3.4** does not admit a finite subcover._The next result is the well-known _Heine-Borel theorem_.

**Theorem A.4.2:**

* _For any_ \(-\infty<a<b<\infty\)_, the closed interval_ \([a,b]\) _is compact in_ \(\mathbb{R}\)_._
* _Any_ \(K\subset\mathbb{R}\) _is compact iff it is bounded and closed._

For a proof, see Rudin (1976). From Proposition A.4.1, it is seen that the inverse image of an open set under a continuous function is open but the forward image may not have this property. But the following is true.

**Theorem A.4.3:** _Let \((\mathbb{S},d)\) and \((\mathbb{T},\rho)\) be two metric spaces and let \(f:(\mathbb{S},d)\to(\mathbb{T},\rho)\) be continuous. Let \(K\subset\mathbb{S}\) be compact. Then \(f(K)\) is compact._

The proof is left as an exercise (Problem A.35).

#### 4.4.4 Sequences of functions and uniform convergence

**Definition A.4.11:** Let \((\mathbb{S},d)\) and \((\mathbb{T},\rho)\) be two metric spaces and let \(\{f_{n}\}_{n\geq 1}\) be a sequence of functions from \((\mathbb{S},d)\) to \((\mathbb{T},\rho)\). The sequence \(\{f_{n}\}_{n\geq 1}\) is said to:

* _converge pointwise to_ \(f\) _on a set_ \(A\subset\mathbb{S}\) _if_ \(\lim_{n\to\infty}f_{n}(x)=f(x)\) _for each_ \(x\) _in_ \(A\)_;
* _converges uniformly to_ \(f\) _on a set_ \(A\subset\mathbb{S}\) _if for each_ \(\varepsilon>0\)_, there exists_ \(N_{\varepsilon}>0\) _(depending on_ \(\varepsilon\) _and_ \(A\)_) such that_ \[n\geq N_{\varepsilon}\Rightarrow\rho\bigl{(}f_{n}(x),f(x)\bigr{)}<\varepsilon \ \ \mbox{for all}\ \ x\ \ \mbox{in}\ \ A.\]

A consequence of uniform convergence is the preservation of the continuity property.

**Theorem A.4.4:** _Let \((\mathbb{S},d)\) and \((\mathbb{T},\rho)\) be two metric spaces and let \(\{f_{n}\}_{n\geq 1}\) be a sequence of functions from \((\mathbb{S},d)\) to \((\mathbb{T},\rho)\). Let for each \(n\geq 1\), \(f_{n}\) be continuous on \(A\subset\mathbb{S}\). Let \(\{f_{n}\}_{n\geq 1}\) converge to \(f\) uniformly on \(A\). Then \(f\) is continuous on \(A\)._

**Proof:** The proof is based on the "break up into three parts" idea. By the triangle inequality,

\[\rho\bigl{(}f(x),f(y)\bigr{)}\leq\rho\bigl{(}f(x),f_{n}(x)\bigr{)}+\rho\bigl{(} f_{n}(x),f_{n}(y)\bigr{)}+\rho\bigl{(}f_{n}(y),f(y)\bigr{)}.\]

Fix \(x\) in \(A\). By the uniform convergence on \(A\), \(\sup\{\rho\bigl{(}f_{n}(u),f(u)\bigr{)}:u\in A\}\to 0\) as \(n\to\infty\). So for each \(\varepsilon>0\), there exists \(N_{\varepsilon}<\infty\) such that \(n\geq N_{\varepsilon}\Rightarrow\rho\bigl{(}f_{n}(u),f(u)\bigr{)}<\frac{ \varepsilon}{3}\) for all \(u\) in \(A\). Now since \(f_{N_{\varepsilon}}(\cdot)\) is continuous on \(A\), there exists a \(\delta>0\) (depending on \(N_{\varepsilon}\) and \(x\)), such that \(d(x,y)<\delta,y\in A\Rightarrow\rho\bigl{(}f_{N_{\varepsilon}}(y),f_{N_{ \varepsilon}}(x)\bigr{)}<\frac{\varepsilon}{3}\). Thus, \(y\in A\), \(d(x,y)<\delta\Rightarrow\rho\bigl{(}f(x),f(y)\bigr{)}<\frac{2\varepsilon}{3}+ \frac{\varepsilon}{3}=\varepsilon\). \(\Box\)

### Problems

1. Express the following sets in the form \(\{x:x\text{ has property }p\}\). 1. The set \(A\) of all integers which when divided by \(7\) leave a remainder \(\leq 3\). 2. The set \(B\) of all functions form \([0,1]\) to \(\mathbb{R}\) with at most two discontinuity points. 3. The set \(C\) of all students at a given university who are graduate students with at least one course in mathematics at the graduate level. 4. The set \(D\) of all algebraic numbers. (A number \(x\) is called an _algebraic number_, if it is the root of a polynomial with rational coefficients.) 5. The set \(E\) of all possible sequences whose elements are either \(0\) or \(1\).
2. Give an example of sets \(A_{1},A_{2}\) such that \(A_{1}\cap A_{2}\neq A_{1}\cup A_{2}\).
3. Let \(I=[0,1]\), \(\Omega=\mathbb{R}\) and for \(\alpha\in\mathbb{R}\), \(A_{\alpha}=(\alpha-1,\alpha+1)\), the open interval \(\{x:\alpha-1<x<\alpha+1\}\). 1. Show that \(\cup_{\alpha\in I}A_{\alpha}=(-1,2)\) and \(\cap_{\alpha\in I}A_{\alpha}=(0,1)\). 2. Suppose \(J=\{x:x\in I,x\text{ is rational}\}\). Find \(\cup_{x\in J}A_{x}\) and \(\cap_{x\in J}A_{x}\).
4. With \(\Omega\equiv\mathbb{N}\equiv\{1,2,3,\ldots\}\), find \(A^{c}\) in the following cases: 1. \(A=\{\omega:\omega\text{ is divisible by }2\text{ or }3\text{ or both}\}\). If \(\omega\in A^{c}\), what can be said about its prime factors? 2. \(A=\{\omega:\omega\text{ is divisible by }15\text{ and }16\}\). 3. \(A=\{\omega:\omega\text{ is a perfect square}\}\).
5. Show that \(X\equiv\{0,1\}^{\mathbb{N}}\), the set of all sequences \(\{\omega_{i}\}_{i\in\mathbb{N}}\) where each \(\omega_{i}\in\{0,1\}\), is _uncountable_. Conclude that \(\mathcal{P}(\mathbb{N})\) is uncountable.
6. Show that if \(\Omega_{i}\) is countable for each \(i\in\mathbb{N}\), then for each \(k\in\mathbb{N}\), \(\times_{i=1}^{k}\Omega_{i}\) is countable and \(\cup_{i\in\mathbb{N}}\Omega_{i}\) is also countable but \(\times_{i\in\mathbb{N}}\Omega_{i}\) is _not countable_.
7. Show that the set of all polynomials in \(x\) with integer coefficients is countable.
8. Show that the well ordering property implies the principle of induction.
9. Apply the principle of induction to establish the following:1. For each \(n\in\mathbb{N}\), \(\sum\limits_{j=1}^{n}j^{2}=\frac{n(n+1)(2n+1)}{6}\).
2. For each \(n\in\mathbb{N}\), \(x_{1},x_{2},\ldots,x_{k}\in\mathbb{R}\), 1. (_The binomial formula_). \((x_{1}+x_{2})^{n}=\sum\limits_{r=0}^{n}{n\choose r}x_{1}^{r}x_{2}^{n-r}\). 2. (_The multinomial formula_). \[(x_{1}+x_{2}+\ldots+x_{k})^{n}=\sum\frac{n!}{r_{1}!r_{2}!\ldots r_{k}!}x_{1}^{r _{1}}x_{2}^{r_{2}}\ldots x_{k}^{r_{k}},\] where the summation extends over all \((r_{1},r_{2},\ldots,r_{k})\) such that \(r_{i}\in\mathbb{N}\), \(0\leq r_{i}\leq n\), \(\sum_{r=1}^{k}r_{i}=n\).

10. Verify that on \(\mathbb{R}\), the relation \(x\sim y\) if \(x-y\) is rational is an equivalence relation but the relation \(x\sim y\) if \(x-y\) is irrational is not.

11. Show that the set \(A=\{r:r\in\mathbb{Q},r^{2}<2\}\) is bounded above in \(\mathbb{Q}\) but has no l.u.b. in \(\mathbb{Q}\).

12. Show that for any two sequences \(\{x_{n}\}_{n\geq 1}\), \(\{y_{n}\}_{n\geq 1}\subset\mathbb{R}\), \[\mathop{\underline{\lim}}_{n\to\infty}x_{n}+\mathop{\underline{\lim}}_{n\to \infty}y_{n}\leq\mathop{\underline{\lim}}_{n\to\infty}(x_{n}+y_{n})\leq\mathop{ \overline{\lim}}_{n\to\infty}(x_{n}+y_{n})\] \[\leq\mathop{\overline{\lim}}_{n\to\infty}x_{n}+\mathop{\overline{ \lim}}_{n\to\infty}y_{n}.\]

13. Verify that \(\lim_{n\to\infty}x_{n}=a\in\mathbb{R}\) iff \(\mathop{\underline{\lim}}_{n\to\infty}x_{n}=\mathop{\overline{\lim}}_{n\to \infty}x_{n}=a\).

14. Establish Proposition A.2.3.

(**Hint:** First show that a Cauchy sequence is bounded and then show that \(\mathop{\underline{\lim}}_{n\to\infty}x_{n}=\mathop{\overline{\lim}}_{n\to \infty}x_{n}\).)

15. 1. Prove Proposition A.2.4 by comparison with the geometric series. 2. Show that for integer \(k\geq 1\), the power series \(\sum_{n=k}^{\infty}n(n-1)(n-k+1)a_{n}x^{n-k}\) has the same radius of convergence as \(\sum_{n=0}^{\infty}a_{n}x^{n}\).

16. Show that the series \(\sum_{j=2}^{\infty}\frac{1}{j(\log j)^{p}}\) converges for \(p>1\) and diverges for \(p\leq 1\).

17. Find the radius of convergence, \(\rho\), for the powers series \(A(x)\equiv\sum_{n=0}^{\infty}a_{n}x^{n}\) where 1. \(a_{n}=\frac{n}{(n+1)}\), \(n\geq 0\). 2. \(a_{n}=n^{p}\), \(n\geq 0\), \(p\in\mathbb{R}\).

* \(a_{n}=\frac{1}{n!}\), \(n\geq 0\) (where \(0!=1\)).
* Find the Taylor series at \(a=0\) for the function \(f(x)=\frac{1}{1-x}\) in \(I\equiv(-1,+1)\) and show that it converges to \(f(x)\) on \(I\).
* Find the Taylor series of \(1+x+x^{2}\) in \(I=(1,3)\), centered at \(2\).
* Let \[f(x)=\left\{\begin{array}{ll}e^{-\frac{1}{x^{2}}}&\mbox{if}\quad|x|<1,x\neq 0 \\ 0&\mbox{if}\quad x=0\.\end{array}\right.\]
* Show that \(f\) is infinitely differentiable at \(0\) and compute \(f^{(j)}(0)\) for all \(j\geq 1\).
* Show that the Taylor series at \(a=0\) converges but not to \(f\) on \((-1,1)\).
* Let \(S=\{z:z\in\mathbb{C},|z|=1\}\) be the unit circle. Using the parameterization \(t\to e^{\iota t}=(\cos t+\iota\sin t)\) from \([0,2\pi]\) to \(S\), show that the arc length of \(S\) (i.e., the circumference of the limit circle) is \(2\pi\).
* Set \(\phi(t)=\frac{\sin t}{\cos t}\) for \(-\frac{\pi}{2}<t<\frac{\pi}{2}\). Verify that \(\phi^{\prime}=1+\phi^{2}\) and that \(\phi:(-\frac{\pi}{2},\frac{\pi}{2})\) to \((-\infty,\infty)\) is strictly monotone increasing and onto. Conclude that \[\int_{-\infty}^{\infty}\frac{1}{1+x^{2}}dx=\int_{-\pi/2}^{\pi/2}\frac{\phi^{ \prime}(t)}{1+(\phi(t))^{2}}dt=\pi.\]
* Using the property that \(e^{\iota\frac{\pi}{2}}=\iota\) verify that for all \(t\) in \(\mathbb{R}\) \[\cos(\frac{\pi}{2}-t)=\sin t,\ \sin(\frac{\pi}{2}-t)=\cos t,\] \[\cos(\pi+t)=-\cos t,\ \sin(\pi+t)=-\sin t,\] \[\cos(2\pi+t)=\cos t,\ \sin(2\pi+t)=\sin t.\] Also show that \(\cos t\) is a strictly decreasing map from \([0,\pi]\) onto \([-1,1]\) and that \(\sin t\) is a strictly increasing map from \([-\frac{\pi}{2},\frac{\pi}{2}]\) onto \([-1,1]\).
* Using (i) of Theorem A.3.1, express \(\cos(t_{1}+t_{2})\), \(\sin(t_{1}+t_{2})\) in terms \(\cos t_{i}\), \(\sin t_{i}\), \(i=1,2\) and in turn use this to prove Corollary A.3.4 from Theorem A.3.3.
* Verify that \(p_{n}(z)\equiv(1+\frac{z}{n})^{n}\) converges to \(e^{z}\) uniformly on bounded sets in \(\mathbb{C}\).
* Verify that for \(p=1\), \(p=2\) and \(p=\infty\), \(d_{p}\) is a metric on \(\mathbb{R}^{k}\).
* Show that for fixed \(x\) and \(y\), \(\varphi(p)\equiv d_{p}(x,y)\) is continuous in \(p\) on \([1,\infty]\).
* Draw the open unit ball \(B_{p}\equiv\{x:x\in\mathbb{R}^{2},\ d_{p}(x,0)<1\}\) in \(\mathbb{R}^{2}\) for \(p=1,2\) and \(\infty\).
* Let \(\mathbb{S}=C[0,1]\) be the set of all real valued continuous functions on \([0,1]\). Now let \[d_{1}(f,g) = \int_{0}^{1}|f(x)-g(x)|dx,\quad\mbox{(area metric)}\] \[d_{2}(f,g) = \bigg{(}\int_{0}^{1}|f(x)-g(x)|^{2}dx\bigg{)}^{\frac{1}{2}},\quad \mbox{(least square metric)}\] \[d_{\infty}(f,g) = \sup\{|f(x)-g(x)|:0\leq x\leq 1\}\quad\mbox{(sup metric)}.\] Show that all these are metrics on \(\mathbb{S}\).
* Let \(\mathbb{S}=\mathbb{R}^{\infty}\equiv\{\{x_{n}\}_{n\geq 1}:x_{n}\in\mathbb{R}\) for all \(n\geq 1\}\) be the space of all sequences of real numbers. Let \(d(\{x_{n}\}_{n\geq 1},\{y_{n}\}_{n\geq 1})=\sum_{j=1}^{\infty}(\frac{|x_{j}-y_{j}| }{1+|x_{j}-y_{j}|})\frac{1}{2^{j}}\). Show that \((\mathbb{S},d)\) is a Polish space.
* If \(s_{k}=\{x_{kn}\}_{n\geq 1}\) and \(s=\{x_{n}\}_{n\geq 1}\), are elements of \(\mathbb{S}=\mathbb{R}^{\infty}\) as in Problem A.26, verify that as \(k\to\infty\), \(s_{k}\to s\) iff \(x_{kn}\to x_{n}\) for all \(n\geq 1\).
* Establish Theorem A.4.1.
* Let \(\mathbb{S}=C[0,1]\) and \(d_{p}(f,g)\equiv\big{(}\int_{0}^{1}|f(t)-g(t)|^{p}dt\big{)}^{\frac{1}{p}}\) for \(1\leq p<\infty\) and \(d_{\infty}(f,g)=\sup\{|f(t)-g(t)|:t\in[0,1]\}\).
* Let \(f(x)\equiv 1\). Let \(f_{n}(t)\equiv 1\) for \(0\leq t\leq 1-\frac{1}{n}\), and \(f_{n}(t)=n(1-t)\) for \(1-\frac{1}{n}\leq t\leq 1\). Show that \(d_{p}(f_{n},f)\to 0\) for \(1\leq p<\infty\) but \(d_{\infty}(f_{n},f)\not\to 0\).
* Fix \(f\in C[0,1]\). Let \(g_{n}(t)=f(t)\), \(0\leq t\leq 1-\frac{1}{n}\), and \(g_{n}(t)=f(1-\frac{1}{n})+(f(1)-f(1-\frac{1}{n}))n(t+\frac{1}{n}-1)\), \(1-\frac{1}{n}\leq t\leq 1\). Show that \(d_{p}(g_{n},f)\to 0\) for all \(1\leq p\leq\infty\).
* Show that if \(\{x_{n}\}_{n\geq 1}\) is a convergent sequence in a metric space \((\mathbb{S},d)\), then it is _Cauchy_.
* Verify (b) of Example A.4.2 from the axioms of real numbers (cf. Proposition A.2.3). Verify (c) of the same example from (b).
* Let \(\mathbb{S}=C[0,1]\) and \(d\) be the _supremum metric_, i.e., \[d(f,g)=\sup\{|f(x)-g(x)|:0\leq x\leq 1\}.\] By approximating any continuous function with piecewise linear functions with rational end points and rational values, show that \((\mathbb{S},d)\) is Polish, i.e., it is complete and separable.

* 33 Show that the function \(f(x)=x^{2}\) is continuous on \(\mathbb{R}\), uniformly so on any bounded set \(B\subset\mathbb{R}\) but not uniformly on \(\mathbb{R}\).
* 34 Show that unions of open sets are open and intersection of any two open sets is open. Give an example to show that the intersection of an infinite number of open sets need not be open.
* 35 Prove Theorem A.4.3.
* 36 Let \(f_{n}(x)=x^{n}\) and \(g(x)\equiv 0\) on \(\mathbb{R}\). Then \(\{f_{n}\}_{n\geq 1}\) converges pointwise to \(g\) on \((-1,1)\), uniformly on \([a,b]\) for \(-1<a<b<1\), but not uniformly on \((0,1)\).
* 37 Let \(\{f_{n}\}_{n\geq 1}\), \(f\in C[0,1]\). Let \(\{f_{n}\}_{n\geq 1}\) converge to \(f\) uniformly on \([0,1]\). Show that \(\lim_{n\to\infty}\int_{0}^{1}|f_{n}(x)-f(x)|dx=0\) and \(\lim_{n\to\infty}\int_{0}^{1}f_{n}(x)=\int_{0}^{1}f(x)dx\).
* 38 Give a proof of Proposition A.2.6 (vi) (term by term differentiability of a power series) using Proposition A.2.7 (iv) (the fundamental theorem of Riemann integration).

## Appendix B List of Abbreviations and Symbols

### Appendix

#### Abbreviations

a.c. absolutely continuous (functions)

a.e. almost everywhere

AR(1) autoregressive process of order one

a.s. almost sure(ly)

BCT

bounded convergence theorem

BGW

Gilton-Watson

cdf

cumulative distribution function

CE

conditional expectation

CLT

central limit theorem

CTMC

continuous time Markov chain

DCT

dominated convergence theorem

extended dominated convergence theorem

fdds

finite dimensional distributions

iff

if and only if

IFS

iterated function system

iid

independent and identically distributed

IIIDRM

iterations of iid random maps

i.o.

infinitely often

LIL

law of the iterated logarithm

LLN

laws of large numbersMBB moving block bootstrap

m.c.f.a. monotone continuity from above

m.c.f.b. monotone continuity from below

MCMC Markov chain Monte Carlo

MCT monotone convergence theorem

o.n.b. orthonormal basis

r.c.p. regular conditional probability

SBM standard Brownian motion

SLLN strong law of large numbers

s.o.c. second order correctness

SSRW simple symmetric random walk

UI uniform integrability

WLLN weak law of large numbers

w.p. 1 with probability one

w.r.t. with respect to

w.l.o.g. without loss of generality

### Symbols

\(\ll\)\(\mu\ll\nu\): absolute continuity of a measure

\(\longrightarrow^{d}\) convergence in distribution

\(\longrightarrow^{p}\) convergence in probability

\((\cdot)*(\cdot)\) convolution of measures, functions, etc.

\((\cdot)^{*}\) extension of a measure

\(a\sim b\)\(a\) and \(b\) are equivalent (under an equivalence relation)

\(a_{n}\sim b_{n}\)\(\frac{a_{n}}{b_{n}}\to 1\) as \(n\to\infty\)

\(\lfloor a\rfloor\) the integer part of \(a\), i.e., \(\lfloor a\rfloor=k\) if \(k\leq a<k+1\),

\(k\in\mathbb{Z}\), \(a\in\mathbb{R}\)

\(\lceil a\rceil\) the smallest integer not less than \(a\), i.e., \(\lceil a\rceil=k+1\) if

\(k<a\leq k+1\), \(k\in\mathbb{Z}\), \(a\in\mathbb{R}\)

\(\bar{A}\) closure of \(A\)

\(A^{c}\) complement of a set \(A\)

\(\partial A\) boundary of \(A\)

\(A\bigtriangleup B\) symmetric difference of two sets \(A\) and \(B\),

i.e., \(A\bigtriangleup B=(A\cap B^{c})\cup(A^{c}\cap B)\)

\(\mathcal{B}(\mathbb{S})\) Borel \(\sigma\)-algebra on a metric space \(\mathbb{S}\) such as \(\mathbb{S}=\mathbb{R}\), \(\mathbb{R}^{k}\),

\(\mathbb{R}^{\infty}\)

\(\mathcal{B}(\mathbb{S},\mathbb{R})\)\(\equiv\{f\mid f:\mathbb{S}\to\mathbb{R}\), \(\mathcal{F}\)-measurable, \(\sup\{|f(s)|:s\in\mathbb{S}\}\leq 1\}\)

\(B(x,\epsilon),B_{x}(\epsilon)\) open ball of radius \(\epsilon\) with center at \(x\) in a metric space

\((\mathbb{S},d)\), i.e., \(\{y:d(x,y)<\epsilon\}\)

[MISSING_PAGE_FAIL:607]

\((\mathbb{S},d)\) a metric space \(\mathbb{S}\) with a metric \(d\)

\(\sigma\langle\mathcal{A}\rangle\)\(\sigma\)-algebra generated by a class of sets \(\mathcal{A}\)

\(\sigma\langle\{f_{a}:a\in A\}\rangle\)\(\sigma\)-algebra generated by a collection of mappings

\(\{f_{a}:a\in A\}\)

\(\mathcal{T}\) tail \(\sigma\)-algebra

\(|z|\)\(=\sqrt{a^{2}+b^{2}}\), the absolute value of a complex number

\(z=a+\iota b,a,b\in\mathbb{R}\)

\(\mathrm{Re}(z)\)\(=a\), the real part of a complex number \(z=a+\iota b,a\),

\(b\in\mathbb{R}\)

\(\mathrm{Im}(z)\)\(=b\), the imaginary part of a complex number

\(z=a+\iota b,a,b\in\mathbb{R}\)

\(\mathbb{Z}\) the set of all integers \(=\{0,\pm 1,\pm 2,\ldots\}\)

\(\mathbb{Z}_{+}\) the set of all nonnegative integers \(=\{0,1,2,\ldots\}\)

## References

* [1]
* [Arcones and GineArcones and Gine1989] Arcones, M. A. and Gine, E. (1989), 'The bootstrap of the mean with arbitrary bootstrap sample size', _Ann. Inst. H. Poincare Probab. Statist._**25**(4), 457-481.
* [Arcones and GineArcones and Gine1991] Arcones, M. A. and Gine, E. (1991), 'Additions and correction to: "The bootstrap of the mean with arbitrary bootstrap sample size" [Ann. Inst. H. Poincare Probab. Statist. 25(4) (1989), 457-481]', _Ann. Inst. H. Poincare Probab. Statist._**27**(4), 583-595.
* [Athreya Athreya1986] Athreya, K. B. (1986), 'Darling and Kac revisited', _Sankhya A_**48**(3), 255-266.
* [Athreya Athreya1987_a_] Athreya, K. B. (1987_a_), 'Bootstrap of the mean in the infinite variance case', _Ann. Statist._**15**(2), 724-731.
* [Athreya Athreya1987_b_] Athreya, K. B. (1987_b_), Bootstrap of the mean in the infinite variance case, _in_ '_Proceedings of the 1st World Congress of the Bernoulli Society_', Vol. 2, VNU Sci. Press, Utrecht, pp. 95-98.
* [Athreya Athreya2000] Athreya, K. B. (2000), 'Change of measures for Markov chains and the \(l\log l\) theorem for branching processes', _Bernoulli_**6**, 323-338.
* [Athreya Athreya2004] Athreya, K. B. (2004), 'Stationary measures for some Markov chain models in ecology and economics', _Econom. Theory_**23**(1), 107-122.
* [Athreya, Doss and SethuramanAthreya, Doss and Sethuraman1996] Athreya, K. B., Doss, H. and Sethuraman, J. (1996), 'On the convergence of the Markov chain simulation method', _Ann. Statist._**24**(1), 69-100.

* Athreya and Jagers (1997) Athreya, K. B. and Jagers, P., eds (1997), _Classical and Modern Branching Processes_, Vol. 84 of _The IMA Volumes in Mathematics and its Applications_, Springer-Verlag, New York.
* Athreya and Ney (1978) Athreya, K. B. and Ney, P. (1978), 'A new approach to the limit theory of recurrent Markov chains', _Trans. Amer. Math. Soc._**245**, 493-501.
* Athreya and Ney (2004) Athreya, K. B. and Ney, P. E. (2004), _Branching Processes_, Dover Publications, Inc, Mineola, NY. (Reprint of Band 196, Grundlehren der Mathematischen Wissenschaften, Springer-Verlag, Berlin).
* Athreya and Pantula (1986) Athreya, K. B. and Pantula, S. G. (1986), 'Mixing properties of Harris chains and autoregressive processes', _J. Appl. Probab._**23**(4), 880-892.
* Athreya and Stenflo (2003) Athreya, K. B. and Stenflo, O. (2003), 'Perfect sampling for Doeblin chains', _Sankhya A_**65**(4), 763-777.
* Bahadur (1966) Bahadur, R. R. (1966), 'A note on quantiles in large samples', _Ann. Math. Statist._**37**, 577-580.
* Barnsley (1992) Barnsley, M. F. (1992), _Fractals Everywhere_, 2nd edn, Academic Press, New York.
* Berbee (1979) Berbee, H. C. P. (1979), _Random Walks with Stationary Increments and Renewal Theory_, Mathematical Centre, Amsterdam.
* Berry (1941) Berry, A. C. (1941), 'The accuracy of the Gaussian approximation to the sum of independent variates', _Trans. Amer. Math. Soc._**48**, 122-136.
* Bhatia (2003) Bhatia, R. (2003), _Fourier Series_, 2nd edn, Hindustan Book Agency, New Delhi, India.
* Bhattacharya and Rao (1986) Bhattacharya, R. N. and Rao, R. R. (1986), _Normal Approximation and Asymptotic Expansions_, Robert E. Krieger, Melbourne, FL.
* Billingsley (1968) Billingsley, P. (1968), _Convergence of Probability Measures_, John Wiley, New York.
* Billingsley (1995) Billingsley, P. (1995), _Probability and Measure_, 3rd edn, John Wiley, New York.
* Bradley (1983) Bradley, R. C. (1983), 'Approximation theorems for strongly mixing random variables', _Michigan Math. J._**30**(1), 69-81.
* Brillinger (1975) Brillinger, D. R. (1975), _Time Series. Data Analysis and Theory_, Holt, Rinehart and Winston, Inc, New York.
* Carlstein (1986) Carlstein, E. (1986), 'The use of subseries values for estimating the variance of a general statistic from a stationary sequence', _Ann. Statist._**14**(3), 1171-1179.

* [11]Chanda1974Chanda1974Chanda, K. C. (1974), 'Strong mixing properties of linear stochastic processes', _J. Appl. Probab._**11**, 401-408.
* [12]Chow and Teicher1997Chow, Y.-S. and Teicher, H. (1997), _Probability Theory: Independence, Interchangeability, Martingales_, Springer-Verlag, New York.
* [13]Chung1967Chung, K. L. (1967), _Markov Chains with Stationary Transition Probabilities_, 2nd edn, Springer-Verlag, New York.
* [14]Chung1974Chung, K. L. (1974), _A Course in Probability Theory_, 2nd edn, Academic Press, New York.
* [15]Cohen1966Cohen, P. (1966), _Set Theory and the Continuum Hypothesis_, Benjamin, New York.
* [16]Doob1953Doob, J. L. (1953), _Stochastic Processes_, John Wiley, New York.
* [17]Doukhan, Massart and Rio1994Doukhan, P., Massart, P. and Rio, E. (1994), 'The functional central limit theorem for strongly mixing processes', _Ann. Inst. H. Poincare Probab. Statist._**30**, 63-82.
* [18]Durrett2001Durrett, R. (2001), _Essentials of Stochastic Processes_, Springer-Verlag, New York.
* [19]Durrett2004Durrett, R. (2004), _Probability: Theory and Examples_, 3rd edn, Duxbury Press, San Jose, CA.
* [20]Efron1979Efron, B. (1979), 'Bootstrap methods: Another look at the jackknife', _Ann. Statist._**7**(1), 1-26.
* [21]Esseen1942Esseen, C.-G. (1942), 'Rate of convergence in the central limit theorem', _Ark. Mat. Astr. Fys._**28A**(9).
* [22]Esseen1945Esseen, C.-G. (1945), 'Fourier analysis of distribution functions. a mathematical study of the Laplace-Gaussian law', _Acta Math._**77**, 1-125.
* [23]Etemadi1981Etemadi, N. (1981), 'An elementary proof of the strong law of large numbers', _Z. Wahrsch. Verw. Gebiete_**55**(1), 119-122.
* [24]Feller1966Feller, W. (1966), _An Introduction to Probability Theory and Its Applications_, Vol. II, John Wiley, New York.
* [25]Feller1968Feller, W. (1968), _An Introduction to Probability Theory and Its Applications_, Vol. I, 3rd edn, John Wiley, New York.
* [26]Geman and Geman1984Geman, S. and Geman, D. (1984), 'Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images', _IEEE Trans. Pattern Analysis Mach. Intell._**6**, 721-741.
* [27]Gine and Zinn1989Gine, E. and Zinn, J. (1989), 'Necessary conditions for the bootstrap of the mean', _Ann. Statist._**17**(2), 684-691.

Gnedenko, B. V. and Kolmogorov, A. N. (1968), _Limit Distributions for Sums of Independent Random Variables_, Revised edn, Addison-Wesley, Reading, MA. Gorodetskii, V. V. (1977), 'On the strong mixing property for linear sequences', _Theory Probab._**22**, 411-413. Gotze, F. and Hipp, C. (1978), 'Asymptotic expansions in the central limit theorem under moment conditions', _Z. Wahrsch. Verw. Gebiete_**42**, 67-87. Gotze, F. and Hipp, C. (1983), 'Asymptotic expansions for sums of weakly dependent random vectors', _Z. Wahrsch. Verw. Gebiete_**64**, 211-239. Hall, P. (1985), 'Resampling a coverage pattern', _Stochastic Process. Appl._**20**, 231-246. Hall, P. (1992), _The Bootstrap and Edgeworth Expansion_, Springer-Verlag, New York. Hall, P. G. and Heyde, C. C. (1980), _Martingale Limit Theory and Its Applications_, Academic Press, New York. Hall, P., Horowitz, J. L. and Jing, B.-Y. (1995), 'On blocking rules for the bootstrap with dependent data', _Biometrika_**82**, 561-574. Herrndorf, N. (1983), 'Stationary strongly mixing sequences not satisfying the central limit theorem', _Ann. Probab._**11**, 809-813. Hewitt, E. and Stromberg, K. (1965), _Real and Abstract Analysis_, Springer-Verlag, New York. Hoel, P. G., Port, S. C. and Stone, C. J. (1972), _Introduction to Stochastic Processes_, Houghton-Mifflin, Boston, MA. Ibragimov, I. A. and Rozanov, Y. A. (1978), _Gaussian Random Processes_, Springer-Verlag, Berlin. Karatzas, I. and Shreve, S. E. (1991), _Brownian Motion and Stochastic Calculus_, 2nd edn, Springer-Verlag, New York. Karlin, S. and Taylor, H. M. (1975), _A First Course in Stochastic Processes_, Academic Press, New York. Kifer, Y. (1988), _Random Perturbations of Dynamical Systems_, Birkhauser, Boston, MA. Kolmogorov, A. N. (1956), _Foundations of the Theory of Probability_, 2nd edn, Chelsea, New York.

Korner, T. W. (1989), _Fourier Analysis_, Cambridge University Press, New York.
* Kunsch (1989) Kunsch, H. R. (1989), 'The jackknife and the bootstrap for general stationary observations', _Ann. Statist._**17**, 1217-1261.
* Lahiri (1991) Lahiri, S. N. (1991), 'Second order optimality of stationary bootstrap', _Statist. Probab. Lett._**11**, 335-341.
* Lahiri (1992) Lahiri, S. N. (1992), 'Edgeworth expansions for \(m\)-estimators of a regression parameter', _J. Multivariate Analysis_**43**, 125-132.
* Lahiri (1994) Lahiri, S. N. (1994), 'Rates of bootstrap approximation for the mean of lattice variables', _Sankhya A_**56**, 77-89.
* Lahiri (1996) Lahiri, S. N. (1996), 'Asymptotic expansions for sums of random vectors under polynomial mixing rates', _Sankhya A_**58**, 206-225.
* Lahiri (2001) Lahiri, S. N. (2001), 'Effects of block lengths on the validity of block resampling methods', _Probab. Theory Related Fields_**121**, 73-97.
* Lahiri (2003) Lahiri, S. N. (2003), _Resampling Methods for Dependent Data_, Springer-Verlag, New York.
* Lehmann and Casella (1998) Lehmann, E. L. and Casella, G. (1998), _Theory of Point Estimation_, Springer-Verlag, New York.
* Lindvall (1992) Lindvall, T. (1992), _Lectures on Coupling Theory_, John Wiley, New York.
* Liu and Singh (1992) Liu, R. Y. and Singh, K. (1992), Moving blocks jackknife and bootstrap capture weak dependence, _in_ R. Lepage and L. Billard, eds, '_Exploring the Limits of the Bootstrap_', John Wiley, New York, pp. 225-248.
* Metropolis et al. (1953) Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. and Teller, E. (1953), 'Equations of state calculations by fast computing machines', _J. Chem. Physics_**21**, 1087-1092.
* Meyn and Tweedie (1993) Meyn, S. P. and Tweedie, R. L. (1993), _Markov Chains and Stochastic Stability_, Springer-Verlag, New York.
* Munkres (1975) Munkres, J. R. (1975), _Topology, A First Course_, Prentice Hall, Englewood Cliffs, NJ.
* Nummelin (1978) Nummelin, E. (1978), 'A splitting technique for Harris recurrent Markov chains', _Z. Wahrsch. Verw. Gebiete_**43**(4), 309-318.
* Nummelin (1984) Nummelin, E. (1984), _General Irreducible Markov Chains and Nonnegative Operators_, Cambridge University Press, Cambridge.
* Orey (1971) Orey, S. (1971), _Limit Theorems for Markov Chain Transition Probabilities_, Van Nostrand Reinhold, London.

* [14]Parthasarathy, K. R. (1967), _Probability Measures on Metric Spaces_, Academic Press, San Diego, CA.
* [15]Parthasarathy, K. R. (2005), _Introduction to Probability and Measure_, Vol. 33 of _Texts and Readings in Mathematics_, Hindustan Book Agency, New Delhi, India.
* [16]Peligrad, M. (1982), 'Invariance principles for mixing sequences of random variables', _Ann. Probab._**10**(4), 968-981.
* [17]Petrov, V. V. (1975), _Sums of Independent Random Variables_, Springer-Verlag, New York.
* [18]Reiss, R.-D. (1974), 'On the accuracy of the normal approximation for quantiles', _Ann. Probab._**2**, 741-744.
* [19]Robert, C. P. and Casella, G. (1999), _Monte Carlo Statistical Methods_, Springer-Verlag, New York.
* [20]Rosenberger, W. F. (2002), 'Urn models and sequential design', _Sequential Anal._**21**(1-2), 1-41.
* [21]Royden, H. L. (1988), _Real Analysis_, 3rd edn, Macmillan Publishing Co., New York.
* [22]Rudin, W. (1976), _Principles of Mathematical Analysis_, International Series in Pure and Applied Mathematics, 3rd edn, McGraw-Hill Book Co., New York.
* [23]Rudin, W. (1987), _Real and Complex Analysis_, 3rd edn, McGraw-Hill Book Co., New York.
* [24]Shohat, J. A. and Tamarkin, J. D. (1943), The problem of moments, _in_ '_American Mathematical Society Mathematical Surveys_', Vol. II, American Mathematical Society, New York.
* [25]Singh, K. (1981), 'On the asymptotic accuracy of Efron's bootstrap', _Ann. Statist._**9**, 1187-1195.
* [26]Strassen, V. (1964), 'An invariance principle for the law of the iterated logarithm', _Z. Wahrsch. Verw. Gebiete_**3**, 211-226.
* [27]Stroock, D. W. and Varadhan, S. (1979), _Multidimensional Diffusion Processes_, Band 233, Grundlehren der Mathematischen Wissenschaften, Springer-Verlag, Berlin.
* [28]Szego, G. (1939), _Orthogonal Polynomials_, Vol. 23 of _American Mathematical Society Colloquium Publications_, American Mathematical Society, Providence, RI.
* [29]* [25]Withers, C. S. (1981), 'Conditions for linear processes to be strong-mixing', _Z. Wahrsch. Verw. Gebiete_**57**, 477-480.
* [26]Woodroofe, M. (1982), _Nonlinear Renewal Theory in Sequential Analysis_, SIAM, Philadelphia, PA.

Author Index

Arcones, M. A., 541

Athreya, K. B., 428, 460, 464, 472, 475, 499, 516, 541, 559, 561, 564, 565, 569

Bahadur, R. R., 557

Barnsley, M. F., 460

Berbee, H.C.P., 517

Berry, A. C., 361

Bhatia, R. P., 167

Bhattacharya, R. N., 365, 368

Billingsley, P., 14, 211, 254, 301, 306, 373, 375, 475, 498

Bradley, R. C., 517

Brillinger, D. R., 547

Carlstein, E., 548

Casella, G., 391, 477, 480

Chanda, K. C., 516

Chow, Y. S., 364, 417, 430

Chung, K. L., 14, 250, 323, 359, 489

Cohen, P., 576

Doob, J. L., 211, 399

Doss, H., 472

Doukhan, P., 528

Durrett, R., 274, 278, 372, 393, 429, 493

Efron, B., 533, 545

Esseen, C. G., 361, 368

Etemadi, N., 244

Feller, W., 166, 265, 308, 313, 323, 354, 357, 359, 362, 489, 499

Geman, D., 477

Geman, S., 477

Gine, E., 541

Gnedenko, B. V., 355, 359

Gorodetskii, V. V., 516

Gotze, F., 554

Hall, P. G., 365, 510, 534, 548, 556

Herrndorf, N., 529

Hewitt, E., 30, 576

Heyde, C. C., 510

Hipp, C., 554Hoel, P. G., 455

Horowitz, J. L., 556

Ibragimov, I. A., 516

Jagers, P., 561

Jing, B. Y., 556

Karatzas, I., 494, 499, 504

Karlin, S., 489, 493, 504

Kifer, Y., 460

Kolmogorov, A. N., 170, 200, 225,

244, 355, 359

Korner, T. W., 167, 170

Kunsch, H. R., 547

Lahiri, S. N., 533, 537, 549, 552,

556

Lehmann, E. L., 391

Lindvall, T., 265, 456

Liu, R. Y., 547

Massart, P., 528

Metropolis, N., 477

Meyn, S. P., 456

Munkres, J. R., 71

Ney, P., 464, 564, 565, 569

Nummelin, E., 463, 464

Orey, S., 464

Pantula, S. G., 516

Parthasarathy, K. R., 393

Peligrad, M., 529

Petrov, V. V., 365

Port, S. C., 455

Rao, R. Ranga, 365, 368

Reiss, R. D., 381

Rio, E., 528

Robert, C. P., 477, 480

Rosenberger, W. F., 569

Rosenbluth, A. W., 477

Rosenbluth, M. N., 477

Royden, H. L., 27, 62, 94, 97, 118,

128, 130, 156, 573, 579

Rozanov, Y. A., 516

Rudin, W., 27, 94, 97, 132, 181,

195, 581, 590, 593

Sethuraman, J., 472

Shreve, S. E., 494, 499, 504

Shohat, J. A., 308

Singh, K., 536, 537, 545, 547

Stenflo, O., 460

Stone, C. J., 455

Strassen, V., 279, 576

Stromberg, K., 30

Stroock, D. W., 504

Szego, G.,107

Tamarkin, 308

Taylor, H. M., 489, 493, 504

Teicher, H., 364, 417, 430

Teller, A. H., 477

Teller, E., 477

Tweedie, R. L., 456

Varadhan, S.R.S., 504

Withers, C. S., 516

Woodroofe, M., 407

Zinn, J., 541

Zinn, J., 541Subject Index

\(\lambda\)-system, 13

\(\pi\)-\(\lambda\) theorem, 13

\(\pi\)-system, 13, 220

\(\sigma\)-algebra, 10, 44

Borel, 12, 299

product, 147, 157, 203

tail, 225

trace, 33

trivial, 10

Abel's summation formula, 254

absolute continuity, 53, 113, 128,

319

absorption probability, 484

algebra, 9

analysis of variance formula, 391

arithmetic distribution (_See_ distribution)

BCT (_See_ theorem)

Bahadur representation, 557

Banach space, 96

Bernouilli shift, 272

Bernstein polynomial, 239

betting sequence, 408

binary expansion, 135

Black-Scholes formula, 504

block bootstrap method, 547

bootstrap method, 533

Borel-Cantelli lemma, 245

conditional, 427

first, 223

second, 223, 232

branching process, 402, 431, 561

Bienyeme-Galton-Watson,

562

critical, 562, 564, 565

subcritical, 562, 564, 566

supercritical, 562, 565

Brownian bridge, 375, 498

Brownian motion, 373

laws of large numbers, 499

nondifferentiability, 499

reflection, 495

reflection principle, 496

scaling properties, 495

standard, 493, 507

time inversion, 495

translation invariance, 495cardinality, 576

Carleman's condition, 308

Cauchy sequence, 91, 581

central limit theorem, 343, 510,

519

\(\alpha\)-mixing, 521, 525, 527, 528

\(\rho\)-mixing, 529

functional, 373

iid, 345

Lindeberg, 345, 521, 535

Lyapounov's, 348

martingale, 510

multivariate, 352

random sums, 378

sample quantiles, 379

change of variables, 81, 132, 141,

193

Chapman-Kolmogorov equation,

461, 488

characteristic function (_See_ function)

complete

measure space, 160

metric space, 91, 96, 124, 237

orthogonal basis, 171

orthogonal set, 100

orthogonal system, 169

complex

conjugate, 322, 586

logarithm, 362

numbers, 586

conditional

expectation, 386

independence, 396

probability, 392

variance, 391

consistency, 281

convergence

almost everywhere, 61

almost surely, 238

in distribution, 287, 288, 299

in measure, 62

pointwise, 61

in probability, 237, 289

of moments, 306

radius of, 164, 581

of types, 355

weak, 288, 299

with probability one, 238

vague, 291, 299

convex

function, 84

convolution of

cdfs, 184

functions, 163

sequence, 162

signed measures, 161

correlation coefficient, 248

Cramer-Wold device, 336, 352

coupling, 455, 516

Cramer's condition, 365

cumulative distribution function,

45, 46, 133, 191

joint, 221

marginal, 221

singular, 133

cycles, 445

DCT (_See_ theorem)

de Morgan's laws, 72, 575

Delta method, 310

detailed balance condition, 492

differentiation, 118, 130, 583

chain rule of, 583

directly Riemann integrable, 268

distribution

arithmetic, 264, 319

Cauchy, 353

compound Poisson, 359, 360

initial, 439, 456

nonarithmetic, 265

discrete univariate, 144

finite dimensional, 200

lattice, 264, 319, 364

nonlattice, 265, 364, 536

Pareto, 358

stationary, 440, 451, 469, 492

domain of attraction, 358, 541

Doob's decomposition, 404

dual space, 94EDCT (_See_ theorem)

Edgeworth expansions, 364, 365,

536, 538, 554

empirical distribution function,

241, 375

equivalence relation, 577

ergodic, 273

Birkhoff's (_See_ theorem)

Kingman's (_See_ theorem)

(maximal) inequality, 274

sequence, 273

essential supremum, 89

exchangeable, 397

exit probability, 502

extinction probability, 491, 562,

565

Feller continuity, 473

filtration, 399

first passage time, 443, 462

Fourier

coefficients, 99, 166, 170

series, 187

transform (_See_ transforms)

function

absolutely continuous, 129

Cantor, 114

Cantor ternary, 136

characteristic, 317, 332

continuous, 41, 299, 582, 592

differentiable, 320, 583

exponential, 587

generating, 164

Greens, 463

Haar, 109, 493

integrable, 54

regularly varying, 354

simple, 49

slowly varying, 354

transition, 458, 488

trigonometric, 587

Gaussian process, 209

Gibbs sampler, 480

growth rate, 503

Harris irreducible (_See_

irreducible)

Harris recurrence (_See_

recurrence)

heavy tails, 358, 540

Hilbert space, 98, 388

hitting time, 443, 462

independence, 208, 211, 219, 336

pairwise, 219, 224, 244, 247,

257

inequalities

Bessel's, 99

Burkholder's, 416

Cauchy-Schwarz, 88, 98, 198

Chebychev's, 83, 196

Cramer's, 84

Davydov's, 518

Doob's \(L^{p}\)-maximal, 413

Doob's \(L\log L\) maximal, 414

Doob's maximal, 412

Doob's upcrossing, 418

Holder's, 87, 88, 198

Jensen's, 86, 197, 390

Kolmogorov's first, 249

Kolmogorov's second, 249

Levy's, 250

Markov's, 83, 196

Minkowski's, 88, 198

Rio's, 517

smoothing, 362

infinitely divisible distributions,

358, 360

inner-product, 98

integration

by parts formula, 155, 586

Lebesgue, 51, 54, 61

Riemann, 51, 59, 60, 61, 585

inversion formula, 175, 324, 325,

326, 338

irreducible, 273, 447

Harris, 462

isometry, 94

isomorphism, 100

iterated function system, 441, 460Jordan decomposition, 123

Kolmogorov-Smirnov statistic,

375, 499

\(L^{p}\)-

norm, 89

space, 89

large deviation, 368

law of large numbers, 237, 448,

470

strong (_See_ SLLN)

weak (_See_ WLLN)

law of the iterated logarithm,

278, 538

lemma

C-set, 464

Fatou's, 7, 54, 389

Kronecker's, 255, 433

Riemann-Lebesgue, 171, 320

Wald's, 415

liminf, 223, 580, 595

limits, 580

limsup, 222, 580, 595

Lindeberg condition, 344

linear operator, 96

Lyapounov's condition, 347

MBB, 547

MCMC

MCT (_See_ theorem)

m-dependence, 515, 545

Malthusian parameter, 566

map

co-ordinate, 203

projection, 203

Markov chain, 208, 439, 457

absorbing, 447

aperiodic, 454

communicating, 447

embedded, 488

periodic, 453

semi-, 468

solidarity, 447

Markov process
branching, 490

generator, 489

Markov property, 397, 488

strong, 444, 497

martingale, 399, 501, 509, 563

difference array, 509

Doob's, 401

reverse, 423

sub, 400

super, 400

mean

arithmetic, 87

geometric, 87

mean matrix, 564

measure, 14, 20

complete, 24, 30

counting, 16

finite, 16

induced, 45

Lebesgue, 26

Lebesgue-Stieltjes, 17, 26,

28, 58, 59, 325

negative variation, 121

occupation, 476, 482

outer, 22

positive variation, 121

probability, 16

product, 151

Radon, 29, 131

regularity of, 29

signed, 119

singular, 114

space, 39

total variation, 121

uniqueness of, 29

measurable

rectangle, 147

space, 39

median, 250

method of moments, 307

metric space, 12, 299, 590, 597

complete, 91, 96, 124, 300, 591

discrete, 90

Kolmogorov, 312Levy, 311, 537

Polish, 300, 306, 592

separable, 300, 591

supremum, 300, 537

Metropolis-Hastings algorithm,

478

mixing, 278

\(\alpha\), 514, 515

\(\beta\), 514

\(\phi\), 514

\(\Psi\), 515

\(\rho\), 514

coefficient, 514

process, 513

strongly, 514, 515

moment, 198

convergence of (_See_

convergence)

moment generating function, 194,

198, 314

moment problem, 307, 309

Monte-Carlo, 534

iid, 248

Markov chain (_See_ MCMC)

nonexplosion condition, 488

nonnegative definite, 323

norm, 95

total variation, 123, 269, 271

normal numbers, 248, 281

absolutely, 281

normed vector space, 95

null array, 348

order statistics, 375

orthogonal

basis, 100

polynomials, 107

orthogonality, 99

orthonormality, 99

parameter

level 1, 533

level 2, 533

Parseval identity, 170

partition, 32

Polya's criterion, 323

polynomial

trigonometric, 170

power series, 581

differentiability of, 583

probabililty

distribution, 45, 191

space, 39

process

birth and death, 489

branching (_See_ branching process)

compound Poisson, 491

empirical, 375

Gaussian, 493, 516

Levy, 491, 493, 497

Markov (_See_ Markov process)

Ornstein-Uhlenbeck, 498,

507

Poisson, 490, 505

regenerative, 268, 467, 491

renewal (_See_ renewal)

stable, 497

Yule, 505

product space, 147

projection, 108, 384

quadratic variation, 500

queues

M/G/1, 571

M/M/\(\infty\), 505

M/M/1, 505

busy period, 571

Radon-Nikodym derivative, 118,

404

random map, 442

random Poisson measure, 541

random variable, 39, 191

extended real-valued, 226

tail, 225

random vector, 192

random walk, 401, 431multiplicative, 459

simple symmetric, 446

recurrence, 444, 446, 456

Harris, 463

null, 444, 449

positive, 444

regeneration

times, 269

regression, 280, 351, 384, 531

regular conditional probability,

392

renewal

equation, 266

function, 262, 266

process, 261, 484, 567

sequence, 260

theorem, weak, 264

theorem, strong, 265

theorem, key, 267, 268

SLLN, 240, 259, 424

Borel's, 240

Etemadi's, 244

Kolmogorov's, 257

Marcinkiewz-Zygmund, 256

sample path, 442

sample space, 189

second order correctness, 537, 554

second order stationary, 529

semialgebra, 3, 19

\(\sigma\)-finite, 28

set, 573

Sentor, 37, 134

compact, 592

complement, 575

cylinder, 202

empty, 574

function, 14

intersection, 574

Lebesgue measurable, 26

power, 9

product, 147, 575

union, 574

slowly varying function (_See_ function)

sojourn time, 468, 487

span, 319, 364

stable distribution, 353, 360

stationary, 271

Stirling's approximation, 313

stochastic processes, 199

stochastically bounded (_See_ tight)

stopping time, 262, 406, 462

bounded, 262, 263

finite, 406

sub-martingale (_See_ martingale)

super-martingale (_See_ martingale)

symmetric difference, 4

ternary expansion, 135

theorem

a.s. convergence, sub-

martingale, 419

Berbee's, 516

Berry-Esseen, 361

betting, 408

binomial, 595

Birkhoff's, 274

Bochner-Khinchine, 323

Bolzano-Weirstrass, 291

bounded convergence, 57

Bradley's, 517

continuous mapping, 305

de Finetti's, 430

dominated convergence, 7,

57, 77, 390

Donsker, 498

Doob's optional stopping,

408, 409, 410

Egorov's, 69, 71

extended dominated convergence, 57

extension, 24, 157, 204

Feller's, 348

Frechet-Shohat, 307

Fubini's, 153, 222

Glivenko-Cantelli, 241, 285Hahn decomposition, 122 Heine-Borel, 593 Helly's, 292, 296, 373, 475 Helly-Bray, 293, 305 Kakutani's, 429 Kallianpur-Robbins, 499 Kesten-Stigum, 564 Khinchine, 355 Khinchine-Kolmogorov's 1-series, 252 Kingman's, 277 Kolmogorov's consistency, 200, 210 Kolmogorov's 3-series, 249, 252 \(L^{p}\) convergence, submartingales, 422 Lebesgue decomposition, 115 Levy-Cramer, 330, 360 Levy-Khinchine, 359 Lusin's, 69 mean value, 583 minorization, 464 monotone convergence, 6, 52, 77, 389 multinomial, 595 Polya's, 242, 285, 290, 361, 556 Prohorov-Varadarajan, 303, 373 Radon-Nikodym, 115 Rao-Blackwell, 391, 398 regeneration, 465 Riesz representation, 94, 97, 101, 144 Scheffe's, 64, 241 Shannon-McMillan-Breiman, 276 Skorohod's, 304, 306 Slutsky's, 290, 298 Taylor's, 584 Tonelli's, 152, 222 tight, 295, 303, 307 time reversibility, 484 topological space, 11 transformation linear, 96 measure preserving, 271 transforms Fourier, 173, 320 Laplace, 165 Laplace-Stieltjes, 166 Plancherel, 180 transient, 444, 449 transition function (_See_ function) transition probability, 439, 456 function, 209 matrix, 209 triangular array, 343 uniform integrability, 65, 306 upcrossing inequality (_See_ inequalities) urn schemes, 568 Polya's, 568 vague compactness, 475 variation bounded, 127 negative, 126 positive, 126 total, 126 vector space, 95 volatility rate, 503 WLLN, 238 waiting time, 459, 472 Wald's equation, 263 Weyl's equi-distribution property, 314 zero-one law, 223 Kolmogorov, 225, 422Springer Texts in Statistics (continued from page ii)

_Lehmann and Romano:_ Testing Statistical Hypotheses, Third Edition

_Lehmann and Casella:_ Theory of Point Estimation, Second Edition

_Lindman:_ Analysis of Variance in Experimental Design

_Lindsey:_ Applying Generalized Linear Models

_Madansky:_ Prescriptions for Working Statisticians

_McPherson:_ Applying and Interpreting Statistics: A Comprehensive Guide,

Second Edition

_Mueller:_ Basic Principles of Structural Equation Modeling: An

Introduction to LISREL and EQS

_Nguyen and Rogers:_ Fundamentals of Mathematical Statistics: Volume I:

Probability for Statistics

_Nguyen and Rogers:_ Fundamentals of Mathematical Statistics: Volume II:

Statistical Inference

_Noether:_ Introduction to Statistics: The Nonparametric Way

_Nolan and Speed:_ Stat Labs: Mathematical Statistics Through Applications

_Peters:_ Counting for Something: Statistical Principles and Personalities

_Pfeiffer:_ Probability for Applications

_Pitman:_ Probability

_Rawlings, Pantula and Dickey:_ Applied Regression Analysis

_Robert:_ The Bayesian Choice: From Decision-Theoretic Foundations to

Computational Implementation, Second Edition

_Robert and Casella:_ Monte Carlo Statistical Methods

_Rose and Smith:_ Mathematical Statistics with _Mathematica_

_Ruppert:_ Statistics and Finance: An Introduction

_Santner and Duffy:_ The Statistical Analysis of Discrete Data

_Saville and Wood:_ Statistical Methods: The Geometric Approach

_Sen and Srivastava:_ Regression Analysis: Theory, Methods, and

Applications

_Shao:_ Mathematical Statistics, Second Edition

_Shorack:_ Probability for Statisticians

_Shumway and Stoffer:_ Time Series Analysis and Its Applications:

With R Examples, Second Edition

_Simonoff:_ Analyzing Categorical Data

_Terrell:_ Mathematical Statistics: A Unified Introduction

_Timm:_ Applied Multivariate Analysis

_Toutenburg:_ Statistical Analysis of Designed Experiments, Second Edition

_Wasserman:_ All of Nonparametric Statistics

_Wasserman:_ All of Statistics: A Concise Course in Statistical Inference

_Weiss:_ Modeling Longitudinal Data

_Whittle:_ Probability via Expectation, Fourth Edition

_Zacks:_ Introduction to Reliability Analysis: Probability Models and

Statistical Methods