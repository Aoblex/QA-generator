[MISSING_PAGE_EMPTY:6220]

Springer Texts in Statistics

_Series Editors_

G. Allen, Department of Statistics, Houston, USA

R. De Veaux, Department of Mathematics and Statistics,

Williams College, Williamstown, USA

R. Nugent, Department of Statistics,

Carnegie Mellon University, Pittsburgh, USA_Springer Texts in Statistics (STS)_ includes advanced textbooks from 3rd- to 4th-year undergraduate courses to 1st- to 2nd-year graduate courses. Exercise sets should be included. The series editors are currently Geneva I. Allen, Richard D. De Veaux, and Rebecca Nugent. Stephen Fienberg, George Casella, and Ingram Olkin were editors of the series for many years.

More information about this series at [http://www.springer.com/series/417](http://www.springer.com/series/417)

[MISSING_PAGE_EMPTY:6223]

Johannes Lederer

Statistics, Machine Learning

& Data Science

Ruhr-University Bochum

Bochum, Germany

ISSN 1431-875X

ISSN 2197-4136 (electronic)

Springer Texts in Statistics

ISBN 978-3-030-73791-7

ISBN 978-3-030-73792-4 (eBook)

[https://doi.org/10.1007/978-3-030-73792-4](https://doi.org/10.1007/978-3-030-73792-4)

(c) The Editor(s) (if applicable) and The Author(s), under exclusive licence to Springer Nature Switzerland AG 2022

This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

This Springer imprint is published by the registered company Springer Nature Switzerland AG.

The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

[MISSING_PAGE_EMPTY:6225]

## Preface

This textbook is designed for beginning graduate and advanced undergraduate students in statistics, biostatistics, and bioinformatics, but it may also be useful to a broader audience. Particular emphasis is put on

* Step-by-step introductions to the mathematical tools and principles
* Exercises that complement the main text, many of them with detailed solutions
* Computer labs that convey practical insights and experience
* Suggestions for further reading

This approach should give the reader a smooth start in the field.

I am grateful to Dr. Marco Rossini for the inspiring discussions about drafts of this book and about statistics in general. I also thank Yannick Duren, Shih-Ting Huang, Dr. Tobias Kaufmann, Janosch Kellermann, Mike Laszkiewicz, Mahsa Taheri, and Dr. Fang Xie for their valuable suggestions and corrections.

**Johannes Lederer**

Bochum, Germany

January 2020

## Exercises, Labs, and Literature

In addition to the main text, the book contains exercises, labs, and literature notes. The diamond ratings \(\Diamond/\blacklozenge\), \(\Diamond\Diamond/\blacklozenge\blacklozenge\), \(\Diamond\Diamond\Diamond/\blacklozenge\blacklozenge\blacklozenge\blacklozenge\blacklozenge\) next to an exercise number indicate the difficulty of the problem and if solutions are provided: the more diamonds, the harder or longer the solution of an exercise, and filled diamonds mean that there are solutions at the back of the book (however, I still recommend strongly to attempt all exercises seriously without looking up the solutions first).

The labs are written in R. We propose the use of the Rstudio IDE, which is available for free on the Web. Make sure to have downloaded the packages that are included with the library() command; this can be done conveniently within Rstudio via the Packages panel. To access the manuals of the various functions, you can use the Help panel. \(\blacksquare\) Fig. 1 shows how the lab exercises look like (top panel) and how to solve them (bottom panel). The labs are interpreted with R version 3.5.2; your outputs might slightly differ if you use a different version of R (especially the set.seed() function changed with version 3.6.0).

Further notes and references are indicated by numbered superscripts (such as "[...] copy-number variation (CNV).2") in the main text and stated in the Notes and References sections toward the end of each chapter.

Finally, we denote sections that can be skipped at first reading with an asterisk (such as "2.4 Holder Inequality").

Figure 1: Example R lab (top panel) and corresponding solution (bottom panel). The reader is supposed to replace the keyword REPLACE by the correct code

## Notation

Here, we introduce some notation that we will use throughout the book.

**Basic Quantities**: Lowercase letters \(a\) denote numbers; calligraphic lowercase letters \(\boldsymbol{\ell}\) real-valued functions; boldface lowercase letters \(\boldsymbol{a}\) (column) vectors; boldface, calligraphic lowercase letters \(\boldsymbol{\ell}\) vector-valued functions; capital letters \(A\) matrices; calligraphic capital letters \(\mathcal{A}\) sets; Greek letters \(\lambda\) real-valued parameters; boldface Greek letters \(\lambda\) vector-valued parameters; capital Greek letters \(\Lambda\) matrix-valued parameters; and additional hats \(\widehat{\lambda}\), \(\widehat{\lambda}\), \(\widehat{\Lambda}\) parameter estimates.
**Basic Functions**: The logarithm is taken with respect to the basis \(e\), that is, \(\log e=1\). The smallest integer larger or equal to a given \(a\in\mathbb{R}\) is denoted by \(\lceil a\rceil\). The _support of a vector_\(\boldsymbol{a}\in\mathbb{R}^{p}\) is denoted by \(\mathrm{supp}[\boldsymbol{a}]:=\{j\in\{1,\ldots,p\}:a_{j}\neq 0\}\). Minima over the empty set are set to infinity: \(\min_{\boldsymbol{a}\in\mathcal{Q}}\mathcal{\ell}[\boldsymbol{a}]:=\infty\) for every function \(\mathcal{\ell}\). The _signum function_\(\mathrm{sign}\;:\mathbb{R}\to\mathbb{R}\) is defined via \(\mathrm{sign}[a]:=\mathbb{1}\{a\geq 0\}-\mathbb{1}\{a\leq 0\}\). The _cardinality of a set_\(\mathcal{A}\), that is, the number of elements in \(\mathcal{A}\), is denoted by \(|\mathcal{A}|\in\{0,1,\ldots,\infty\}\). The sum of two sets \(\mathcal{A}\), \(\mathcal{B}\) that are defined over the same vector space is \(\mathcal{A}+\mathcal{B}:=\{\boldsymbol{a}+\boldsymbol{b}:\boldsymbol{a}\in \mathcal{A},\;\boldsymbol{b}\in\mathcal{B}\}\), and \(a\mathcal{B}:=\{\boldsymbol{a}\boldsymbol{b}:\boldsymbol{b}\in\mathcal{B}\}\).
**Norms and the Standard Inner Product**: A function \(\|\cdot\|:\mathbb{R}^{p}\to\mathbb{R}\) is called a _norm_ on \(\mathbb{R}^{p}\) if it 1. satisfies the _triangle inequality_\((\|\boldsymbol{a}+\boldsymbol{b}\|\leq\|\boldsymbol{a}\|+\|\boldsymbol{b}\|\) for all \(\boldsymbol{a}\), \(\boldsymbol{b}\in\mathbb{R}^{p})\), 2. is _absolutely homogeneous_ (\(\|a\boldsymbol{b}\|=|a||\boldsymbol{b}\|\) for all \(a\in\mathbb{R}\), \(\boldsymbol{b}\in\mathbb{R}^{p}\)), and 3. is _positive definite_ (\(\|\boldsymbol{a}\|=0\) if and only if \(\boldsymbol{a}=\boldsymbol{0}_{p}\)).

Assumptions 1-3 imply that norms are non-negative: \(0=|\boldsymbol{0}_{p}|=|\boldsymbol{a}-\boldsymbol{a}|\leq\|\boldsymbol{a}\| +|-\boldsymbol{a}|=|\boldsymbol{a}|+|-1|\boldsymbol{a}|=2\|\boldsymbol{a}|\), that is, \(|\boldsymbol{a}|\geq 0\) for all \(\boldsymbol{a}\in\mathbb{R}^{p}\); Assumption 2 implies that norms are symmetric: \(\|-\boldsymbol{a}\|=|(-1)\cdot\boldsymbol{a}|=|-1|\|\boldsymbol{a}|=| \boldsymbol{a}|\); Assumption 2 also implies that norms are scalable: \(\|\boldsymbol{b}/\|\boldsymbol{b}\|\|=\|\boldsymbol{b}\|/\|\boldsymbol{b}\|=1\) for all \(\boldsymbol{b}\neq\boldsymbol{0}_{p}\). The \(\ell_{q}\)-functions on \(\mathbb{R}^{p}\), where \(q\in[0,\infty]\) and \(p\in\{1,2,\ldots\}\), are defined for \(q\in(0,\infty)\) as

\[\ell_{q} : \mathbb{R}^{p} \to \big{[}0,\infty\big{)}\,;\] \[\boldsymbol{a} \mapsto \|\boldsymbol{a}\|_{q}\,:=\;\left(\sum_{j=1}^{p}|a_{j}|^{q} \right)^{1/q}\,,\]

for \(q=0\) as

\[\ell_{0} : \mathbb{R}^{p} \to \{0,1,\ldots\}\,;\] \[\boldsymbol{a} \mapsto \|\boldsymbol{a}\|_{0}\,:=\,\big{|}\big{\{}j\in\{1,\ldots,p\}\,: \,a_{j}\neq 0\big{\}}\big{|}\,,\]

and for \(q=\infty\) as

\[\ell_{\infty}\;: \mathbb{R}^{p} \to \big{[}0,\infty\big{)}\,;\] \[\boldsymbol{a} \mapsto \|\boldsymbol{a}\|_{\infty}\,:=\,\max_{j\in\{1,\ldots,p\}}|a_{j}|\,.\]The \(\ell_{q}\)-functions are norms if and only if \(q\geq 1\) (see 1. in Exercise 2.2); accordingly, we often refer to those functions as \(\ell_{q}\)-norms. The \(\ell_{2}\)-norm is also called _Euclidean norm_; the \(\ell_{\infty}\)-norm is also called _sup-norm_ or _max-norm_.

The (standard) _inner product_ on \(\mathbb{R}^{p}\) is the function \(\langle\cdot,\ \cdot\rangle:\mathbb{R}^{p}\times\mathbb{R}^{p}\to\mathbb{R}\) defined through \(\langle\boldsymbol{a},\ \boldsymbol{b}\rangle:=\boldsymbol{a}^{\top} \boldsymbol{b}=\sum_{j=1}^{p}a_{j}b_{j}\) for \(\boldsymbol{a},\ \boldsymbol{b}\in\mathbb{R}^{p}\). The inner product is 1. _symmetric_ (\(\langle\boldsymbol{a},\ \boldsymbol{b}\rangle=\langle\boldsymbol{b},\ \boldsymbol{a}\rangle\) for all \(\boldsymbol{a},\ \boldsymbol{b}\in\mathbb{R}^{p}\)), 2. _linear_ (\(\langle\boldsymbol{a}\boldsymbol{b},\ \boldsymbol{c}\rangle=a\langle \boldsymbol{b},\ \boldsymbol{c}\rangle\) and \(\langle\boldsymbol{a}+\boldsymbol{b},\ \boldsymbol{c}\rangle=\langle \boldsymbol{a},\ \boldsymbol{c}\rangle+\langle\boldsymbol{b},\ \boldsymbol{c}\rangle\) for all \(a\in\mathbb{R}\), \(\boldsymbol{a},\ \boldsymbol{b},\ \boldsymbol{c}\in\mathbb{R}^{p}\)), and 3. _positive definite_ (\(\langle\boldsymbol{a},\ \boldsymbol{a}\rangle\geq 0\) for all \(\boldsymbol{a}\in\mathbb{R}^{p}\) and \(\langle\boldsymbol{a},\ \boldsymbol{a}\rangle=0\) if and only if \(\boldsymbol{a}=\boldsymbol{0}_{p}\)). Two vectors \(\boldsymbol{a},\ \boldsymbol{b}\in\mathbb{R}^{p}\) are _orthogonal_ if \(\langle\boldsymbol{a},\ \boldsymbol{b}\rangle=0\).

**Intervals and the Extended Real Line** We denote by \([a,\,b]\) the interval between \(a\in\mathbb{R}\) and \(b\in\mathbb{R}\) that contains the endpoints (\(a,\,b\in[a,\,b]\)), by \([a,\,b)\) and (\(a,\,b]\) the intervals between \(a\) and \(b\) that contain the left end right endpoint, respectively (\(a\in[a,\,b)\), \(b\notin[a,\,b)\), \(a\notin(a,\,b]\), \(b\in(a,\,b]\)), and by (\(a,\,b\)) the interval between \(a\) and \(b\) that does not contain the endpoints (\(a,\,b\notin(a,\,b)\)).

The real line extended by \(\{-\infty,\,+\infty\}\) is denoted by \([-\infty,\,\infty]:=\mathbb{R}\cup\{-\infty,\,+\infty\}\). Similarly, \([0,\,\infty]:=[0,\,\infty)\cup\{\infty\}\), \((0,\,\infty]:=(0,\,\infty)\cup\{\infty\}\), and so forth. We use the conventions \(0\cdot(\pm\infty):=(\pm\infty)\cdot 0:=0\), \(a/(\pm\infty):=0\) for \(a\in\mathbb{R}\), \(a\cdot(\pm\infty):=(\pm\infty)\cdot a:=\pm\infty\) for \(a\in(0,\,\infty]\), and \(a\cdot(\pm\infty):=(\pm\infty)\cdot a:=\mp\infty\) for \(a\in[-\infty,\,0)\), which are all continuous extentions of the rules on \(\mathbb{R}\), and the convention \(0/0:=0\), which renders our expressions most concise (note that \(0/0\) cannot be obtained by extending the rules on \(\mathbb{R}\) continuously: if it were, then \(0/0=\lim_{a\to 0}(a/a)=1\) and at the same time \(0/0=(2\cdot 0)/0=2\cdot(0/0)=2\), which is a contradiction). The ordering of the values in \([-\infty,\,\infty]\) is as expected: for example, \(a<\infty\) for all \(a\in[-\infty,\,\infty)\).

**Index Sets and Matrices** The complement of a set \(\mathcal{A}\) with respect to an ambient set \(\mathcal{B}\) is denoted by \(\mathcal{A}^{\complement}:=\mathcal{B}\setminus\mathcal{A}\). For example, the complement of \(\{1,2\}\) with respect to \(\{1,\ldots,p\}\), where \(p\in\{3,4,\ldots\}\), is \(\{3,\ldots,p\}\). Typically, it is clear what the ambient set is, so that there is no further mention of it.

Consider a vector \(\boldsymbol{c}\in\mathbb{R}^{l}\) and a corresponding index set \(\mathcal{A}\subset\{1,\ldots,l\}\) with size \(a:=|\mathcal{A}|\). We denote \(\boldsymbol{c}_{\mathcal{A}}\in\mathbb{R}^{a}\) as the vector that consists of the coordinates of \(\boldsymbol{c}\) with indexes in \(\mathcal{A}\). For example, for \(\boldsymbol{c}=(3,\,4,\,5)^{\top}\) and \(\mathcal{A}=\{1,3\}\), it holds that \(\boldsymbol{c}_{\mathcal{A}}=(3,\,5)^{\top}\). The special case \(\mathcal{A}=\varnothing\) is taken into account by setting \(\boldsymbol{c}_{\varnothing}:=0\).

Consider a matrix \(C\in\mathbb{R}^{l\times m}\) and corresponding index sets \(\mathcal{A}\subset\{1,\ldots,l\}\) and \(\mathcal{B}\subset\{1,\ldots,m\}\) with sizes \(a:=|\mathcal{A}|\) and \(b:=|\mathcal{B}|\), respectively. We denote \(C_{\mathcal{A}}\in\mathbb{R}^{l\times a}\) as the matrix that consists of the columns of \(C\) with indexes in \(\mathcal{A}\), and we denote \(C_{\mathcal{B}\mathcal{A}}\in\mathbb{R}^{l\times m}\) as the matrix that consists of the rows and columns of \(C\) with indexes in \(\mathcal{B}\) and \(\mathcal{A}\), respectively. For example,

\[C\,=\,\begin{pmatrix}1&2&3\\ 4&5&6\end{pmatrix},\ \mathcal{A}=\{2,3\},\ \mathcal{B}=\{1\}\Rightarrow C_{ \mathcal{A}}=\,\begin{pmatrix}2&3\\ 5&6\end{pmatrix},\ C_{\mathcal{B}\mathcal{A}}=\,\begin{pmatrix}2&3\\ 2&3\end{pmatrix}.\]However, we typically assume that the coordinates of the vectors/the rows and columns of the matrices are shuffled such that \(\mathcal{A}=\{1,\ldots,a\}\) and \(\mathcal{B}=\{1,\ldots,b\}\). This allows us to write, for example, \(\boldsymbol{c}=(\boldsymbol{c}_{\mathcal{A}}^{\top},\boldsymbol{c}_{\mathcal{A} }^{\top})^{\top}\) and

\[C\,=\,\begin{pmatrix}C_{\mathcal{B}\mathcal{A}}&C_{\mathcal{B}\mathcal{A}} \mathfrak{e}\\ C_{\mathcal{B}}\mathfrak{e}_{\mathcal{A}}&C_{\mathcal{B}}\mathfrak{e}_{ \mathcal{A}}\mathfrak{e}\end{pmatrix}\,.\]

We finally use the convention \(C_{\mathcal{A}}^{\top}:=(C_{\mathcal{A}})^{\top}\).

A brief review of matrix algebra can be found in \(\blacktriangleright\) Sect. B.2.

#### Miscellaneous

The expression \(z\sim\mathcal{N}_{p}[\boldsymbol{\mu},\,\Sigma]\), \(\boldsymbol{\mu}\in\mathbb{R}^{p}\), \(\Sigma\in\mathbb{R}^{p\times p}\), states that \(\boldsymbol{x}\) is a random vector that follows a Gauss distribution in \(p\) dimensions with mean vector \(\boldsymbol{\mu}\) and covariance matrix \(\Sigma\). In \(p=1\) dimensions, we write \(z\sim\mathcal{N}[\mu,\sigma^{2}]\), where \(\mu\in\mathbb{R}\) is the mean and \(\sigma^{2}\in(0,\infty)\) the variance.

Given a positive integer \(p\in\{1,\,2,\ldots\}\), we define \(\boldsymbol{0}_{p}:=(0,\ldots,0)^{\top}\in\mathbb{R}^{p}\).

###### Contents

* 1 Introduction
	* 1.1 Embracing High-Dimensionality
	* 1.2 Statistical Limitations of Classical Estimators
	* 1.3 Incorporating Prior Information
	* 1.4 Regularization for Increasing the Numerical Stability
	* 1.5 Outlook
	* 1.6 Exercises
	* 1.7 R Lab: Least-Squares vs. Ridge Estimation
	* 1.8 Notes and References
* 2 Linear Regression
	* 2.1 Overview
	* 2.2 Sparsity-Inducing Prior Functions
	* 2.3 Post-Processing Methods
	* 2.4 Holder Inequality
	* 2.5 Optimality Conditions
	* 2.6 Exercises
	* 2.7 R Lab: Overfitting
	* 2.8 Notes and References
* 3 Graphical Models
	* 3.1 Overview
	* 3.2 Gaussian Graphical Models
	* 3.3 Maximum Regularized Likelihood Estimation
	* 3.4 Neighborhood Selection
	* 3.5 Exercises
	* 3.6 R Lab: Estimating a Gene-Gene Coactivation Network
	* 3.7 Notes and References
* 4 Tuning-Parameter Calibration
	* 4.1 Overview
	* 4.2 Bounds on the Lasso's Effective Noise
	* 4.3 Cross-Validation
	* 4.4 Adaptive Validation
	* 4.5 Exercises
* 5

[MISSING_PAGE_EMPTY:6233]

## 1 Introduction

### 1.1 Embracing

High-Dimensionality - 2

1.2 Statistical Limitations of

Classical Estimators - 5

1.3 Incorporating Prior

Information - 10

1.4 Regularization for Increasing

the Numerical Stability\({}^{\star}\) - 13

1.5 Outlook - 18

1.6 Exercises - 19

1.7 R Lab: Least-Squares vs. Ridge

Estimation - 25

1.8 Notes and References - 34

[MISSING_PAGE_EMPTY:6235]

able_\(u\in\mathbb{R}\). The goal of a statistical analysis is to estimate the unknown quantities of interest \(\beta_{1},\ldots,\beta_{p}\) from the observations of \(y\) and \(x_{1},\ldots,x_{p}\) in \(n\) subjects.

The classical approach to estimating model parameters in linear regression models is least-squares. Why is least-squares problematic here? The first suspect are the _data_: the number of genes for which the copy numbers are available can be of the same order or even larger than the number of study subjects. To see if this causes problems, we first focus on a single gene, that is, \(y=\beta_{0}+\beta_{1}x_{1}\) and \(p=1\). Estimating the parameter \(\beta_{1}\) by using the toy data set \(A\) on the left-hand side in Table 1, which consist of \(n=7\) observations of the biomarker \(y\) and of the gene's copy number \(x_{1}\), is straightforward: a standard least-squares (see 1. in Exercise 1) yields \(\widehat{\beta_{1}}=1.61\), and the corresponding model fits the data reasonably well in terms of the (adjusted) coefficient of determination[4]--see the bottom of Table 1. Estimating \(\beta_{1}\) by using the data set \(B\) on the right-hand side in Table 1, which contains the copy numbers of \(7\) additional genes, seems more difficult. However, we can just neglect the measurements of \(x_{2},\ldots,x_{8}\) and proceed as before:[5] we can run a

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \(A\) & & \multicolumn{3}{c}{\(B\)} & & & & & & & & \\ \hline y & \(x_{1}\) & \(y\) & \(x_{1}\) & \(x_{2}\) & \(x_{3}\) & \(x_{4}\) & \(x_{5}\) & \(x_{6}\) & \(x_{7}\) & \(x_{8}\) \\ \hline
0.0 & 0 & 0 & 0.0 & 0 & 2 & 0 & 0 & 1 & 0 & 1 & 0 \\
2.1 & 1 & 2.1 & 1 & 0 & 2 & 3 & 2 & 0 & 0 & 3 \\
2.7 & 0 & 2.7 & 0 & 0 & 0 & 2 & 2 & 1 & 1 & 1 \\
5.9 & 3 & 5.9 & 3 & 0 & 1 & 0 & 0 & 0 & 2 & 0 \\
7.3 & 3 & 7.3 & 3 & 4 & 0 & 1 & 1 & 1 & 0 & 0 \\
0.0 & 0 & 0.0 & 0 & 2 & 0 & 0 & 3 & 0 & 0 & 0 \\
2.0 & 1 & 2.0 & 1 & 0 & 2 & 1 & 0 & 0 & 0 & 1 \\ \hline
**Estimated model** & & & & & & & & & & & \\ \(y=0.66+1.92x_{1}+u\) & & & & & & & & & & \\ \(y=0.22+1.78x_{1}+0x_{2}+0x_{3}+0x_{4}+0x_{5}\) & & & & & & & & & & \\ \(+2.11x_{6}+0x_{7}+0x_{8}+u\) & & & & & & & & & & \\ \hline \end{tabular}
\end{table}
Table 1: Two regression-type data sets (top panel) and estimated models for \(y\) in terms of the \(x\)’s (bottom panel). Although the data set on the right is more comprehensive than the data set on the left, both data sets are equally suited for regressing \(y\) on \(x_{1}\): one can just ignore the values for \(x_{2},\ldots,x_{8}\). But having the values for \(x_{2},\ldots,x_{8}\) allows for estimating more refined models that are based not only on \(x_{1}\). For example, the lasso model that is based on all of \(x_{1},\ldots,x_{8}\) has a better fit that the simple model (the larger the adjusted coefficient of determination denoted by \(R_{u}^{2}\), the better the fit; \(R_{u}^{2}=1\) means that the model predictions and the observations match perfectly); refer to Exercise 1 for the calculationsleast-squares that uses only the realizations of \(y\) and \(x_{1}\), which yields again \(\widehat{\beta}_{1}=1.61\). Hence, data per se is not the problem.

What causes new challenges is instead how such data is _used_. So far, we fit a model with a small number of parameters as compared to the number of samples: \(p=1\) and \(n=7\), that is, \(p\gg n\). We call such models _low-dimensional_. But the data set B can also be used to fit a model of the form \(y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{8}x_{8}+u\) to account for all eight genes simultaneously. The number of model parameters is now \(p=8\), while the number of samples is still \(n=7\). We call models where \(p\approx n\) or even \(p\gg n\)_high-dimensional_. In such models, the classical least-squares estimator is unreliable, even ambiguous (see \(\blacktriangleright\) Sect. 1.2): for estimating this many parameters, the data just do not contain a sufficient amount of information.

However, information can come in many shapes and forms, not only as observations of \(y\)'s and \(x\)'s. For example, biological research indicates that hereditary diseases are often associated with only a small number of genes. Statistically speaking, we can expect that most of the parameters \(\beta_{1},\ldots,\beta_{8}\) in our model are equal to zero. We call such models _sparse_. High-dimensional estimators, such as the lasso estimator (see \(\blacktriangleright\) Sect. 2.2), include such prior information. Using the lasso estimator yields the model \(\widehat{y}=0.17+1.43x_{1}+0x_{2}+0x_{3}+0x_{4}+0x_{5}+1.97x_{6}+0x_{7}+0x_{8}\) (see 2. in Exercise 1.1), which improves the fit of the one-gene-model substantially--see the bottom of \(\blacksquare\) Table 1.1. The model accounts for _all_ eight genes, but it focuses the attention on two of them. This is the general idea of high-dimensional statistics: exploiting information beyond the bare data to find meaningful patterns (here, an accurate model based on only two genes) in complex data (here, the copy numbers of eight genes).6

High-dimensional statistics comes into play whenever one fits complex models (\(p\) large) and is even indispensable in high-dimensional models (\(p\) large and \(p\approx n\) or even \(p\gg n\))--see \(\blacksquare\) Table 1.2. Complex models arise naturally in _Big Data_, where the total number of measurements is large (both \(n\) and \(p\) large). Applications include predicting shopping habits based on extensive customer profiles, finding weaknesses in opposing soccer teams by tracking every action of their players, and optimizing the fertilization of crops by using fine-grained geospatial data. Nevertheless,high-dimensional statistics is not limited to large data sets: in our toy example, the total number of measurements across \(y\) and \(x_{1}\),..., \(x_{8}\) is only \(n\times(1+p)=7\times(1+8)=63\). Also, high-dimensionality can be an issue in basically any model class. Our workhorse in this book is linear regression, but the insights can be transferred readily to other types of models such as logistic regression, tensor regression, and graphical modeling (see Chap. 3 for the latter). Altogether, high-dimensional statistics is useful in a very wide variety of applications.

### 2 Statistical Limitations of Classical Estimators

Here, we illustrate that classical estimators often fail in high-dimensional models. Consider \(n\) data points (\(y_{1}\), \((x_{1})_{1}\),..., \((x_{p})_{1}\)),..., \((y_{n}\), \((x_{1})_{n}\),..., \((x_{p})_{n}\)) from a linear regression model of the form (1.1). We summarize the observations of y in the _outcome_\(\boldsymbol{y}:=(y_{1},\ldots,y_{n})^{\top}\in\mathbb{R}^{n}\), the observations of \(x_{1}\),..., \(x_{p}\) in the _design matrix_\(X\in\mathbb{R}^{n\times p}\) defined through \(X_{\bar{y}}:=(x_{\bar{y}})_{i}\), the model parameters (which remain the same across all data points) in the _regression vector_\(\boldsymbol{\beta}:=(\beta_{1},\ldots,\beta_{p})^{\top}\in\mathbb{R}^{p}\) (we drop the intercept without loss of generality7), and the instantiations of \(u\) in the _noise_\(\boldsymbol{u}:=(u_{1},\ldots,u_{n})^{\top}\in\mathbb{R}^{n}\). These definitions allow us to coalesce the relationships specified by (1.1) into a single vector-valued equation:

Footnote 7: The _likelihood_\(\boldsymbol{\beta}\) is defined as the _likelihood_\(\boldsymbol{\beta}\).

\[\boldsymbol{y}\,=\,X\boldsymbol{\beta}+\boldsymbol{u}\,. \tag{1.2}\]

\begin{table}
\begin{tabular}{l l l} \hline  & **Classical** & **High-dimensional** \\  & **statistics** & **statistics** \\ \(n\gg 1\) & – & – \\ \(n\gg 1,p\not\gg 1\) & ✓ & – \\ \(n,p\gg 1,p\ll n\) & ✓ & ✓ \\ \(n,p\gg 1,p\approx n\) or \(p\gg n\) & – & ✓ \\ \hline \end{tabular}
\end{table}
Table 2: Typical scopes of classical statistics (such as least-squares estimation) and high-dimensional statistics (such as lasso estimation). The larger the number of parameters \(p\) as compared to the number of samples \(n\), the more essential is high-dimensional statistics The goal of linear regression is to estimate the unknown regression vector \(\mathbf{\beta}\) from the data \((\mathbf{y},\,X)\). The classical approach to this is the _least-squares estimator_

\[\widehat{\mathbf{\beta}}_{\text{Is}}\;\in\;\operatorname*{arg\,min}_{\mathbf{\alpha}\in \mathbb{R}^{p}}\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2}\,, \tag{1.3}\]

which minimizes the data-fitting function \(\mathbf{a}\mapsto\|\mathbf{y}-X\mathbf{a}\|_{2}^{2}=\sum_{i=1}^{n}(y_{i}-(X\mathbf{a})_{i})^{2}\). In other words, the least-squares approach selects model parameters such that the corresponding linear model approximates the data best. However, a good fit to the data is not the same as a good estimation of \(\mathbf{\beta}\). In the following, we study the least-squares' performance at this latter task in terms of the _prediction error_\(\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\text{Is}}\|_{2}^{2}\) and _prediction risk_\(\mathbb{E}[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\text{Is}}\|_{2}^{2}]\). The prediction error measures how well the estimator disentangles the data-generating part \(X\mathbf{\beta}\) from the noise \(\mathbf{u}\).

Since the least-squares function \(\mathbf{a}\mapsto\|\mathbf{y}-X\mathbf{a}\|_{2}^{2}\) is convex (see 1.1 in Exercise 1.4) and differentiable, we can characterize \(\widehat{\mathbf{\beta}}_{\text{Is}}\) through derivatives of that function:

\[\widehat{\mathbf{\beta}}_{\text{Is}}\;\in\;\operatorname*{arg\,min}_{ \mathbf{\alpha}\in\mathbb{R}^{p}}\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2}\quad\quad\Leftrightarrow \\ \frac{\partial}{\partial\alpha_{k}}\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2} \Big{|}_{\mathbf{\alpha}=\widehat{\mathbf{\beta}}_{\text{Is}}}=0\quad\forall\,k\in\{1, \ldots,p\}\,.\]

The derivatives on the right-hand side can be computed explicitly:

\[\frac{\partial}{\partial\alpha_{k}}\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2}\] \[=\;\frac{\partial}{\partial\alpha_{k}}\sum_{i=1}^{n}\Big{(}y_{i} -\Big{(}\sum_{j=1}^{p}X_{ij}\alpha_{j}\Big{)}\Big{)}^{2}\quad\text{definition of the $\ell_{2}$-norm}\] \[=\;\sum_{i=1}^{n}2\Big{(}y_{i}-\Big{(}\sum_{j=1}^{p}X_{ij}\alpha _{j}\Big{)}\Big{)}\cdot(-X_{ik})\quad\text{sum and chain rules}\]

[MISSING_PAGE_EMPTY:6240]

[MISSING_PAGE_EMPTY:6241]

is invertible by assumption (see 1. of Exercise 1.2), we find for all \(i,j\in\{1,\ldots,n\}\)

\[(D(D^{\top}D)^{-1}D^{\top})_{ij}\] \[= \sum_{k,\,l=1}^{p}D_{ik}(D^{\top}D)_{kl}^{-1}D_{lj}\qquad\qquad \qquad\qquad\text{matrix algebra}\] \[= \sum_{k=1}^{p}D_{ik}(1/D_{kk}^{2})D_{kj}\qquad\qquad\qquad\text{ previous observation}\] \[= \left\{\begin{array}{ll}D_{ii}(1/D_{ii}^{2})D_{ii}=1&\text{ if }i=j\text{ and }i\leq p\;;\\ 0&\text{otherwise}\,,\end{array}\right.\]

so that

\[D(D^{\top}D)^{-1}D^{\top}\;=\;\binom{\text{I}_{p\times p}}{\mathbf{0}_{(n-p) \times(n-p)}}\;.\]

We can plug this back into the earlier display to find

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_{2}^{2}\;=\; \|(U^{\top}\boldsymbol{u})_{\{1,\ldots,p\}}\|_{2}^{2}\,.\]

The term \(|(U^{\top}\boldsymbol{u})_{\{1,\ldots,p\}}\|_{2}\) is the _effective noise_ of the least-squares estimator: the larger this term, the larger the prediction error of the least-squares estimator.

As a concrete example, consider \(X\) fixed and \(\boldsymbol{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\,\text{I}_{n\times n}]\) for a \(\sigma\in(0,\infty)\). Then, \((U^{\top}\boldsymbol{u})_{\{1,\ldots,p\}}\sim\mathcal{N}_{p}[\mathbf{0}_{p}, \sigma^{2}\,\text{I}_{p\times p}]\), and the effective noise has a Chi-squared distribution with \(p\) degrees of freedom and a scaling of \(\sigma^{2}\); in particular, taking expectations over the noise, we get the average prediction risk (see 1. in Exercise 5.4)

\[\mathbb{E}\bigg{[}\frac{|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{ \text{ls}}\|_{2}^{2}}{n}\bigg{]}\;=\;\frac{\sigma^{2}p}{n}\,. \tag{1.5}\]

This result demonstrates that least-squares provides accurate prediction only in traditional settings where \(p/n\ll 1\), that is, \(p\ll n\)--or if the variance of the noise \(\sigma^{2}\) is very small. (This observation is related to the overfitting phenomenon, which we discuss in \(\blacktriangleright\) Sect. 2.2.) Thus, for the high-dimensional settings that are the focus of this book, we need to introduce different approaches.

### Incorporating Prior Information

In genome studies, the number of genes \(p\) under consideration can be much larger than the number of subjects \(n\), and we have seen in the previous section that the classical least-squares estimator is then uninformative. But biological research also indicates that phenotypical traits are often determined by only a small number of genes. High-dimensional estimators incorporate such biological information into the data analysis.

Statistical estimators in general ensure that the selected model parameters agree with data. This agreement is measured by _data-fitting functions_. For example, \(\boldsymbol{a}\mapsto\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}\) measures how well the parameters agree with regression data (\(\boldsymbol{y},X\)). Many classical estimators minimize such functions over a set of candidate parameters \(\mathcal{B}\) for given data \(Z\).

_High-dimensional estimators_ ensure in addition that the selected model parameters agree with prior information. This agreement is measured by _prior functions_; for example, \(\boldsymbol{a}\mapsto\|\boldsymbol{a}\|_{0}:=|j:a_{j}\neq 0|\) measures how well the parameters agree with sparsity. Typical high-dimensional estimators then simply combine data-fitting functions and prior functions:

\[\widehat{\boldsymbol{\beta}}\;\in\;\underset{\boldsymbol{\alpha}\in\mathcal{B }}{\arg\min}\big{\{}\operatorname{DataFitting}[Z,\boldsymbol{\alpha}]+r \operatorname{Prior}[\boldsymbol{\alpha}]\big{\}}\,. \tag{1.6}\]

We call the combined function \(\boldsymbol{a}\mapsto\operatorname{DataFitting}[Z,\boldsymbol{\alpha}]+r \operatorname{Prior}[\boldsymbol{\alpha}]\) the estimator's _objective function_. The _tuning parameters_\(r\in[0,\,\infty]\) weigh the prior information: setting \(r=0\) gives classical estimators, which include no prior information, while increasing \(r\) moves the estimates in the direction specified by the prior function.

Traditional names for the prior function are _penalty_ and _regularizer,_ the first name speaks to the idea of penalizing unrealistic or unfavorable models, the second name originates in nonparametric statistics, where one is interested in functions that are sufficiently regular (which typically means smooth). By introducing the word "prior," we want to circumvent the negative connotations that penalty and regularizer might have (do we impose a penalty on unruly model parameters?) and establish a connection to Bayes statistics: similarly as the prior distribution in the Bayes literature, our prior term does not incorporate any data, and it formulates our knowledge or beliefs aboutthe parameter space. Still, we will use the different names interchangeably in the following.

The most important feature of high-dimensional estimators is that they can provide accurate estimates even if the model is high-dimensional. High-dimensional estimators achieve this essentially by using the prior information to trade a small bias against a large gain in variance. In linear regression with fixed design \(X\), the prediction risk, bias, and variance of an estimator \(\widehat{\boldsymbol{\beta}}\) are related through

\[\underbrace{\mathbb{E}\Big{[}|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}} |_{2}^{2}\Big{]}}_{\text{risk}}\ =\ \big{(}\underbrace{|\mathbb{E}[X\widehat{ \boldsymbol{\beta}}]-X\boldsymbol{\beta}|_{2}}_{\text{bias}}\big{)}^{2}+ \underbrace{\mathbb{E}\Big{[}|X\widehat{\boldsymbol{\beta}}-\mathbb{E}[X\widehat {\boldsymbol{\beta}}]|_{2}^{2}\Big{]}}_{\text{variance}}.\]

This decomposition follows from

\[\mathbb{E}\Big{[}|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta} }|_{2}^{2}\Big{]}\] \[=\ \mathbb{E}\Big{[}|X\boldsymbol{\beta}-\mathbb{E}[X\widehat{ \boldsymbol{\beta}}]+\mathbb{E}[X\widehat{\boldsymbol{\beta}}]-X\widehat{ \boldsymbol{\beta}}|_{2}^{2}\Big{]}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \If the noise is Gauss-distributed according to \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathrm{I}_{n\times n}]\) and \(X^{\top}X\) invertible, the bias of the least-squares estimator (3) is zero:

\[\mathbb{E}\big{[}X\widehat{\mathbf{\beta}}_{\mathrm{Is}}\big{]}-X\mathbf{\beta}\] \[=\ \mathbb{E}\big{[}X(X^{\top}X)^{-1}X^{\top}\mathbf{y}\big{]}-X\mathbf{\beta}\] \[=\ \mathbb{E}\big{[}X(X^{\top}X)^{-1}X^{\top}(X\mathbf{\beta}+\mathbf{u}) \big{]}-X\mathbf{\beta}\] \[=\ X\mathbf{\beta}+X(X^{\top}X)^{-1}X^{\top}\mathbb{E}[\mathbf{u}]-X\mathbf{\beta}\] \[(X^{\top}X)^{-1}X^{\top}X=\mathrm{I}_{n\times n};\text{linearity of expectations}\] \[=\ X\mathbf{\beta}-X\mathbf{\beta}\ =\ \mathbf{0}_{n}\,. \mathbb{E}[\mathbf{u}]=\mathbf{0}_{n}\text{ by assumption}\]

The variance of the least-squares estimator is, however,

\[\mathbb{E}\Big{[}\|X\widehat{\mathbf{\beta}}_{\mathrm{Is}}-\mathbb{E} [X\widehat{\mathbf{\beta}}_{\mathrm{Is}}]\|_{2}^{2}\Big{]}\] \[=\ \mathbb{E}\Big{[}\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\mathrm{Is }}\|_{2}^{2}\Big{]}\] \[\mathbb{E}\{X\widehat{\mathbf{\beta}}_{\mathrm{Is}}\}=X\mathbf{\beta}\text{ according to the preceding result}\] \[=\ \sigma^{2}p\,. \tag{5}\]

In other words, the average prediction risk \(\mathbb{E}\{\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\mathrm{Is}}\|_{2}^{2}/n\}= \sigma^{2}p/n\) only consists of variance. In contrast, the prior terms "push" high-dimensional estimators toward the prior knowledge and, thereby, generate a bias: \(\mathbb{E}[X\widehat{\mathbf{\beta}}]\neq X\mathbf{\beta}\)--cf. \(\blacktriangleright\) Chap. 5, especially Exercise (5.3). But this bias is typically small, and there can be a substantial gain in variance. In consequence, high-dimensional estimators can have much lower risks than the least-squares estimator: for example, we will show in \(\blacktriangleright\) Chap. 6 that the lasso estimator allows us to replace the least-squares estimator's effective noise \(\|(U^{\top}\mathbf{u})_{\{1,\ldots,p\}}\|_{2}\), which determines the least-squares' variance, by the lasso estimator's effective noise \(2|X^{\top}\mathbf{u}|_{\infty}\). This then leads to the risk bound8,9

\[\mathbb{E}\bigg{[}\frac{\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\mathrm{ lasso}}\|_{2}^{2}}{n}\bigg{]}\ \leq\ \frac{as\sigma^{2}\log p}{n}\]

for \(s:=\|\mathbf{\beta}\|_{0}=|\{\beta_{j}\neq 0\}|\) the number of actually influential predictors and \(a\in(0,\infty)\) a reasonably small constant if again \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathrm{I}_{n\times n}]\), and the predictors are not too much correlated. The important difference to the earlier risk bound for the least-squares is that this one is linear in the number of _relevant_ predictors \(s\) and only logarithmic in the _total_ number of predictors \(p\), which can allow us to accurately estimate complex models (\(p\) large) if these models are sparse (\(s\) small): instead of \(n\gg\sigma^{2}p\), we now only require \(n\gg\sigma^{2}s\log p\).

### Regularization for Increasing the Numerical Stability\({}^{\star}\)

The idea of prior functions predates high-dimensional statistics considerably. For example, it was understood already in the middle of the last century that prior functions can increase the numerical stability in inverse problems. We connect this early research with the modern statistical topics of this book by comparing least-squares to the ridge estimator, an estimator that has its roots in the classical literature on inverse problems but is also used in contemporary high-dimension statistics. We will find that least-squares estimation is like a dog that calmly walks next to its keeper at most times but immediately goes on a wild chase all around the park when a rabbit comes into sight. The additional prior function in the ridge estimator is the leash for keeping the dog under control, and the corresponding tuning parameter balances between the safety of the wildlife and the dog's freedom: the larger the tuning parameter, the tighter the leash.

To illustrate the stabilizing effect of prior functions, we study least-squares and ridge estimation on the data family in Table 3. These data specify the outcome \(\boldsymbol{y}=(1,2)^{\top}\in\mathbb{R}^{2}\) and the design matrix \(X=((1,2)^{\top},(d,2)^{\top})\in\mathbb{R}^{2\times 2}\), where \(d\) takes values in \(\mathbb{R}\). We will vary \(d\) to evaluate the estimators' robustness against small changes in the data.

\begin{table}
\begin{tabular}{l l l} \hline \hline \(\boldsymbol{y}\) & \(\boldsymbol{x_{1}}\) & \(\boldsymbol{x_{2}}\) \\ \hline
1 & 1 & \(d\) \\
2 & 2 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Regression-type data indexed by \(d\in\mathbb{R}\). The least-squares estimator on these data is not unique when \(d=1\) and not continuous in \(d\) at \(d=1\), while the ridge estimator is always unique and continuous in \(d\). This illustrates that regularization can have numerical benefits even for seemingly simple data 

[MISSING_PAGE_FAIL:28]

[MISSING_PAGE_FAIL:29]

which implies that \(X^{\top}X+r\mathbf{I}_{p\times p}\) is invertible. Hence, we can write

\[\widehat{\boldsymbol{\beta}}_{\text{ridge}}\,=\,\big{(}X^{\top}X+r\mathbf{I}_{p \times p}\big{)}^{-1}X^{\top}\boldsymbol{y}\,, \tag{1.8}\]

irrespective of whether \(X^{\top}X\) itself is invertible or not. This shows in particular that the ridge estimator is always unique.

We now use the formula to calculate the ridge estimator on the data in Table 1.3:

\[\widehat{\boldsymbol{\beta}}_{\text{ridge}}[r] =\,\big{(}X^{\top}X+r\mathbf{I}_{p\times p}\big{)}^{-1}X^{\top} \boldsymbol{y}\] \[=\,\left(\begin{pmatrix}1&2\\ d&2\end{pmatrix}\begin{pmatrix}1&d\\ 2&2\end{pmatrix}+\begin{pmatrix}r&0\\ 0&r\end{pmatrix}\right)^{-1}\begin{pmatrix}1&2\\ d&2\end{pmatrix}\begin{pmatrix}1\\ 2\end{pmatrix}\] \[=\,\frac{1}{4-8d+4d^{2}+(9+d^{2})r+r^{2}}\] \[\quad\times\begin{pmatrix}4+d^{2}+r&-4-d\\ -4-d&5+r\end{pmatrix}\begin{pmatrix}1&2\\ d&2\end{pmatrix}\begin{pmatrix}1\\ 2\end{pmatrix}\] \[=\,\frac{1}{4-8d+4d^{2}+(9+d^{2})r+r^{2}}\] \[\quad\times\begin{pmatrix}4-4d+r&-2d+2d^{2}+2r\\ -4+4d+dr&2-2d+2r\end{pmatrix}\begin{pmatrix}1\\ 2\end{pmatrix}\] \[=\,\frac{1}{4-8d+4d^{2}+(9+d^{2})r+r^{2}}\] \[\quad\times\begin{pmatrix}4-8d+4d^{2}+5r\\ (4+d)r\end{pmatrix}\] \[=\,\frac{1}{4(1-d)^{2}+(9+d^{2})r+r^{2}}\begin{pmatrix}4(1-d)^{2} +5r\\ (4+d)r\end{pmatrix}\,.\]

The individual steps mirror those for the least-squares estimator for \(d\neq 0\). Since the denominator of the factor is strictly positive for every \(r>0\), this result shows in particular that the ridge estimator is continuous in \(d\) (see also Exercise 1.5). Thus, the ridge estimator is stable with respect to small changes in the observations.

The continuity is due to the additional regularization term in the ridge estimator's objective function. We would thus expect that the stronger the regularization, that is, the larger the tuning parameter, the more stable the estimates are with respect to changes in the data. To support this, we derive two limits from the preceding result: First, we find that the limit of \(\widehat{\boldsymbol{\beta}}_{\text{ridge}}[r]\) for \(r\to\infty\) exists for all \(d\in\mathbb{R}\) and is equal to the zero-vector:

\[\lim_{r\to\infty}\widehat{\boldsymbol{\beta}}_{\text{ridge}}[r]=\lim_{r\to \infty}\frac{1}{r^{2}}\begin{pmatrix}5r\\ (4+d)r\end{pmatrix}=\begin{pmatrix}0\\ 0\end{pmatrix}\quad\text{ for all }d\in\mathbb{R}\,.\]

In this sense, the ridge estimator is perfectly stable when \(r\to\infty\). Second, we find that the limit of \(\widehat{\boldsymbol{\beta}}_{\text{ridge}}[r]\) for \(r\to 0^{+}\) also exists for all \(d\) and equals a least-squares solution:

\[\lim_{r\to 0^{+}}\widehat{\boldsymbol{\beta}}_{\text{ridge}}[r]=\] \[\begin{cases}\lim_{r\to 0^{+}}\frac{1}{10r+r^{2}}\begin{pmatrix}5r \\ 5r\end{pmatrix}&=\lim_{r\to 0^{+}}\frac{1}{1+r/10}\begin{pmatrix}1/2\\ 1/2\end{pmatrix}=\begin{pmatrix}1/2\\ 1/2\end{pmatrix}&\text{for }d=1\,;\\ \frac{1}{4(1-d)^{2}}\begin{pmatrix}4(1-d)^{2}\\ 0\end{pmatrix}=\begin{pmatrix}1\\ 0\end{pmatrix}&\text{for }d\neq 1\,,\end{cases}\]

which are the least-squares estimators \(\widehat{\boldsymbol{\beta}}_{\text{Is}}^{1/2}\) and \(\widehat{\boldsymbol{\beta}}_{\text{Is}}=\widehat{\boldsymbol{\beta}}_{\text{Is}} ^{0}\), respectively. That the two limits for \(d=1\) and \(d\neq 1\) differ is another display of the instability of the least-squares estimator; a graphical explanation of where the two different solutions come from is given in Fig. 1.1. More generally, one can derive that the smaller the regularization, the better the approximation of least-squares solutions (see Exercise 1.6 for such a calculation).

In summary, the prior function stabilizes the least-squares estimator and removes potential ambiguity. The corresponding tuning parameter allows one to balance stability and closeness to the standard least-squares estimator. The dimensionality of the data (\(n=p=2\)) shows that these effects are not tied to high dimensions, but the numerical stability is an extra benefit when using high-dimensional estimators in high-dimensional statistics.

### 5 Outlook

In the remainder of this book, we study the principles of high-dimensional statistics in further detail. In Chap. 2, we deepen our discussion of sparse, high-dimensional regression models to highlight different schemes of regularization. These models are also the basis for illustrating tuning-parameter calibration, inference, and theory later in the book. In Chap. 3, however, we also introduce graphical models to describe networks. In Chap. 4, we then calibrate tuning parameters to balance data and prior information. In Chap. 5, we construct confidence intervals for the coordinates of estimators to quantify uncertainty. In Chaps. 6 and 7, we finally develop theories to establish mathematical evidence for the features and limitations of high-dimensional estimators.

Figure 1: An illustration of why the limiting solutions of the ridge estimator on the data of Table 1.3 differ for \(d=1\) and \(d\neq 1\). The round black dots denote \((1,0)^{\top}\), which is a minimum of \(\mathbf{a}\mapsto\left\lVert\mathbf{y}-\mathbf{X}\mathbf{a}\right\rVert_{2}^{2}\) for every \(d\in\mathbb{R}\), and \((0,0)^{\top}\), which is the minimum of \(\mathbf{a}\mapsto r|\mathbf{a}|_{2}^{2}\) for every \(r\in(0,\infty)\). The lines (left plot) and ellipses (right plot) indicate points that have the same values in \(\mathbf{a}\mapsto\left\lVert\mathbf{y}-\mathbf{X}\mathbf{a}\right\rVert_{2}^{2}\). The three concentric circles indicate points that have the same values in \(\mathbf{a}\mapsto r|\mathbf{a}|_{2}^{2}\). The thick brown lines indicate coordinates of the ridge estimators; each point on the brown lines corresponds to an estimator with a given value of the tuning parameter \(r\): the larger \(r\), the closer the corresponding ridge estimator is to the origin. Since the ridge estimators are minimizers of functions of the form \(\mathbf{a}\mapsto\left\lVert\mathbf{y}-\mathbf{X}\mathbf{a}\right\rVert_{2}^{2}+r|\mathbf{a}|_{2}^ {2}\), they correspond to points (indicated by black diamonds) where a line/ellipse (in blue) generated by the least-squares function “touch” a circle (in red) generated by the prior function. See also Exercise 1.7 for details

### 6 Exercises

#### 6.1 Exercises for \(\blacktriangleright\) Sect. 1.1

**Exercise 1.1**\(\blacklozenge\blacklozenge\) In this exercise, we confirm the models of \(\blacksquare\) Table 1.1. If you are not sufficiently familiar with the lasso estimator yet, come back to this exercise after reading \(\blacktriangleright\) Chap. 2.

1. Use R to build the least-squares model based on data set A and to check the model's adjusted \(R^{2}\) value. You can use the lm() function.
2. Use R to build the lasso model based on data set B and to check the model's adjusted \(R^{2}\) value. For this, load the glmnet package for the lasso estimator and run coef(glmnet(y=y, x=x, lambda=1)) with the appropriate y and x, which, in our case, provides the first two model parameters that enter the lasso path (as a bonus exercise, you can confirm this). Then, run the lm() function on the two selected model parameters to obtain the corresponding least-squares model.

Hint: Details on the lasso estimator will follow in the next section.

#### 6.2 Exercises for \(\blacktriangleright\) Sect. 1.2

**Exercise 1.2**\(\blacklozenge\blacklozenge\blacklozenge\) In this exercise, we generalize the results of \(\blacktriangleright\) Sect. 1.2 to cases where the Gram matrix \(X^{\top}X\) is not necessarily invertible. We show in particular that least-squares prediction is expected to be accurate only if \(\text{rank}[X]\ll n\).

Our derivations use the Moore-Penrose matrix inverse [11]. A matrix \(A^{+}\in\mathbb{R}^{p\times n}\) is called a _Moore-Penrose matrix inverse_ of \(A\in\mathbb{R}^{n\times p}\) if the following four conditions are met: 1. \(AA^{+}A=A\); 2. \(A^{+}AA^{+}=A^{+}\); 3. \((AA^{+})^{\top}=AA^{+}\); and 4. \((A^{+}A)^{\top}=A^{+}A\). Moore-Penrose matrix inverses always exist and are unique. One can also check readily that for an invertible square matrix \(A\), the regular matrix inverse \(A^{-1}\) satisfies the mentioned four conditions; hence, Moore-Penrose matrix inverses generalize regular matrix inverses.

1. Show that the Gram matrix \(X^{\top}X\) is always positive _semi_-definite: \((X^{\top}X)^{\top}=X^{\top}X\) and \(\boldsymbol{\alpha}^{\top}X^{\top}X\boldsymbol{\alpha}\geq 0\) for all \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\).

2. Invertibility then requires positive definiteness: \(\boldsymbol{\alpha}^{\top}X^{\top}X\boldsymbol{\alpha}>0\) for all \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) (see 4. in Lemma B.2.5). Show that if the Gram matrix \(X^{\top}X\) is invertible, then \(p\leq n\). Conclude that the treatment in \(\blacktriangleright\) Sect. 1.2 applies only to settings where the number of model parameters is at most as large as the number of samples.
3. Show that \(\arg\min_{\boldsymbol{\alpha}\in\mathbb{R}^{p}}\|\boldsymbol{y}-X\boldsymbol{ \alpha}\|_{2}^{2}\) contains only one point if and only if \(X\boldsymbol{\gamma}\neq\boldsymbol{0}_{n}\) for all \(\boldsymbol{\gamma}\in\mathbb{R}^{p}\setminus\{\boldsymbol{0}_{p}\}\). Conclude that the least-squares estimator is not unique if \(X^{\top}X\) is not invertible.
4. Show that all \(\widehat{\boldsymbol{\gamma}}_{\mathrm{ls}}\), \(\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\in\arg\min_{\boldsymbol{\alpha}\in \mathbb{R}^{p}}\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2}^{2}\) provide the same prediction: \(X\widehat{\boldsymbol{\gamma}}_{\mathrm{ls}}=X\widehat{\boldsymbol{\beta}}_{ \mathrm{ls}}\).
5. Show that \(\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}=X^{+}\boldsymbol{y}\) is always a solution of the least-squares estimator, where \(X^{+}\) is a Moore-Penrose inverse of \(X\).
6. Consider the singular value decomposition \(X=UDV^{\top}\) used in the main text. 1. Show that \(D^{+}\) defined as diagonal matrix with diagonal elements \(D^{+}_{ii}:=1/D_{ii}\) if \(D_{ii}\neq 0\) and \(D^{+}_{ii}:=0\) otherwise is a Moore-Penrose inverse of \(D\). 2. Show that \(DD^{+}\) is diagonal with \(\mathrm{rank}[X]\) ones on its diagonal and zeros everywhere else. 3. Show that \(X^{+}=VD^{+}\,U^{\top}\) is a Moore-Penrose inverse of \(X\).
7. Show that for every least-squares solution \(\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\), it holds that \[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\|_{2}^{2}\,= \,\|\,UDD^{+}\,U^{\top}\boldsymbol{u}\|_{2}^{2}\,.\]
8. Conclude that if \(\boldsymbol{u}\sim\mathcal{N}_{n}[\boldsymbol{0}_{n},\sigma^{2}\mathrm{I}_{n \times n}]\) for a \(\sigma\in(0,\infty)\), we have the risk bound \[\mathbb{E}\bigg{[}\frac{\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{ \mathrm{ls}}\|_{2}^{2}}{n}\bigg{]}=\,\frac{\sigma^{2}\,\mathrm{rank}[X]}{n}\,.\] (1.9) Here, the expectation is taken over the noise \(\boldsymbol{u}\), while the design is assumed to be fixed.
9. Bonus: Show that \((X^{\top}X)^{+}X^{\top}\) is a Moore-Penrose inverse of \(X\), where \((X^{\top}X)^{+}\) is a Moore-Penrose inverse of \(X^{\top}X\). Conclude that \(\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}=(X^{\top}X)^{+}X^{\top}\boldsymbol{y}\) is always a least-squares estimator.

#### Exercises for \(\blacktriangleright\) Sect. 1.3

**Exercise 1.3**\(\blacklozenge\blacklozenge\) In this exercise, we motivate the ridge and lasso estimators from a Bayes perspective. For this, we consider a linear regression model

\[\boldsymbol{y}\,=\,X\boldsymbol{\beta}+\boldsymbol{u}\]

with outcome \(\boldsymbol{y}\in\mathbb{R}^{n}\), design matrix \(X\in\mathbb{R}^{n\times p}\), regression vector \(\boldsymbol{\beta}\in\mathbb{R}^{p}\), and noise \(\boldsymbol{u}\sim\mathcal{N}_{n}[\boldsymbol{0}_{n},\sigma^{2}\mathrm{I}_{n \times n}]\). We assume the design \(X\) to be fixed. In a frequentist framework, \(\boldsymbol{\beta}\) is also fixed, which means in particular that the outcome is distributed according to \(\boldsymbol{y}\sim\mathcal{N}_{n}[X\boldsymbol{\beta},\sigma^{2}\mathrm{I}_{n \times n}]\). In a hierarchical Bayes framework, on the other hand, \(\boldsymbol{\beta}\) follows some prior distribution, and, instead of the outcome itself, the outcome _given_ the regression vector is distributed according to \(\boldsymbol{y}\mid\boldsymbol{\beta}\sim\mathcal{N}_{n}[X\boldsymbol{\beta}, \sigma^{2}\mathrm{I}_{n\times n}]\). Adopting the Bayes viewpoint, we study the _maximum a posteriori estimator_

\[\widehat{\boldsymbol{\beta}}_{\mathrm{map}}\,\in\,\,\operatorname*{arg\,max}_{ \boldsymbol{\alpha}\in\mathbb{R}^{p}}\mathcal{C}[\boldsymbol{\alpha}\mid \boldsymbol{y}]\,,\]

where the posterior likelihood \(\mathcal{C}[\boldsymbol{\alpha}\mid\boldsymbol{y}]\) is the logarithm of the density of \(\boldsymbol{\alpha}\mid\boldsymbol{y}\) as a function of \(\boldsymbol{\alpha}\).

Assume that the regression vector is distributed independently of the noise and has strictly positive density12\(\mathcal{G}\), which implies that \(\mathcal{C}[\boldsymbol{\alpha}\mid\boldsymbol{y}]\) is proportional to \(\mathcal{L}_{\boldsymbol{\alpha}}[\boldsymbol{y}]\mathcal{G}[\boldsymbol{ \alpha}]\), where \(\mathcal{L}_{\boldsymbol{\alpha}}\) is the density of \(\mathcal{N}_{n}[X\boldsymbol{\alpha},\sigma^{2}\mathrm{I}_{n\times n}]\). Establish the following two relationships between the map estimator and the frequentist estimators lasso and ridge.

1. Show that in case of a multivariate normal prior distribution \(\boldsymbol{\beta}\sim\mathcal{N}_{p}[\boldsymbol{0}_{p},\tau^{2}\mathrm{I}_{ p\times p}]\), the map estimator coincides with the ridge estimator \[\widehat{\boldsymbol{\beta}}_{\mathrm{ridge}}\,\in\,\,\operatorname*{arg\,min} _{\boldsymbol{\alpha}\in\mathbb{R}^{p}}\bigl{\{}\,|\boldsymbol{y}-X \boldsymbol{\alpha}|\,_{2}^{2}+r|\boldsymbol{\alpha}|\,_{2}^{2}\bigr{\}}\] with tuning parameter \(r=\sigma^{2}/\tau^{2}\). Hint: Use the identity \(\max_{\boldsymbol{\alpha}\in\mathbb{R}^{p}}\mathcal{C}[\boldsymbol{\alpha}\mid \boldsymbol{y}]=\min_{\boldsymbol{\alpha}\in\mathbb{R}^{p}}-\log[\mathcal{L} _{\boldsymbol{\alpha}}[\boldsymbol{y}]\mathcal{G}[\boldsymbol{\alpha}]]\).
2. Show that in case of a multivariate Laplace (double-exponential) prior distribution with density \(e^{-\mathrm{I}|\widehat{\boldsymbol{\beta}}|_{1}/\tau}/(2\tau)^{p}\), the map estimator coincides with the lasso \[\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\,\in\,\,\operatorname*{arg\, min}_{\boldsymbol{\alpha}\in\mathbb{R}^{p}}\bigl{\{}\,|\boldsymbol{y}-X \boldsymbol{\alpha}|\,_{2}^{2}+r|\boldsymbol{\alpha}|\,_{1}\bigr{\}}\] with tuning parameter \(r=2\sigma^{2}/\tau\).

[MISSING_PAGE_FAIL:36]

\(v>0\) such that for all \(d\), \(d^{\prime}\in\mathbb{R}\), \(|d-d^{\prime}|<v\), it holds that

\[\|\widehat{\boldsymbol{\beta}}_{\mathrm{ridge}}[r,\boldsymbol{y},\,X^{d}]- \widehat{\boldsymbol{\beta}}_{\mathrm{ridge}}[r,\boldsymbol{y},\,X^{d^{\prime}} ]\|_{1}<u\,,\]

where \(X^{d}=((1,2)^{\top}\), \((d,2)^{\top})\) (and analogously \(X^{d^{\prime}}\)) is the design from \(\blacksquare\) Table 1.3 for given \(d\), and \(\widehat{\boldsymbol{\beta}}_{\mathrm{ridge}}[r,\boldsymbol{y},\,X^{d}]\) (and analogously \(\widehat{\boldsymbol{\beta}}_{\mathrm{ridge}}[r,\boldsymbol{y},\,X^{d^{\prime}}]\)) is the ridge estimator with tuning parameter \(r\) on the data (\(\boldsymbol{y},\,X^{d}\)).

**Exercise 1.6**: \(\blacklozenge\) In this exercise, we corroborate our statement that the smaller the tuning parameter, the better the ridge estimator approximates a least-squares solution. For this, consider the data discussed in \(\blacktriangleright\) Sect. 1.4 with \(d=1\). Show that

\[\|\widehat{\boldsymbol{\beta}}_{\mathrm{ridge}}[r]-(1/2,1/2)^{\top}\|_{2}\,=\, \frac{r}{\sqrt{2}(10+r)}\]

and conclude that the smaller the tuning parameter, the closer is the corresponding ridge estimator to a least-squares solution of the form \((1-a,a)^{\top}\).

**Exercise 1.7**: \(\blacklozenge\blacklozenge\) In this exercise, we provide mathematical details on the plots of \(\blacksquare\) Fig. 1.1.

Consider again the data in \(\blacksquare\) Table 1.3.

1. Show that for all \(d\in\mathbb{R}\) and \(\boldsymbol{\alpha}=(\alpha_{1},\alpha_{2})^{\top}\), it holds that \[\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2}^{2}\,=\,5(\alpha_{1}-1)^{2}+(4+d^ {2})\alpha_{2}^{2}+2(4+d)(\alpha_{1}-1)\alpha_{2}\,.\]
2. Verify that for \(d=1\), the equation simplifies to \[\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2}^{2}\,=\,5(\alpha_{1}-1+\alpha_{2} )^{2}\,.\]
3. Conclude that for \(d=1\) and every \(c\in[0,\,\infty)\), it holds that \[\{\boldsymbol{\alpha}\in\mathbb{R}^{2}\,:\,\|\boldsymbol{y}-X\boldsymbol{ \alpha}\|_{2}^{2}=c\}=\left\{\boldsymbol{\alpha}\in\mathbb{R}^{2}\,:\,\alpha_ {2}=1-\alpha_{1}\pm\sqrt{\frac{c}{5}}\right\}\,.\] This means that the function \(\boldsymbol{\alpha}\mapsto\,\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2}^{2}\) has level sets (a level set is a complete set of function arguments that have the same function value) specified by linear relationships between the coordinates. These level sets are indicated by the straight lines in the left panel of \(\blacksquare\) Fig. 1.1.

4. We consider now the level sets of ellipses centered at \((1,0)^{\top}\) and parameterized by \(\omega\in[0,\,180^{\circ})\) and \(a,\,b\in(0,\,\infty)\): \[\left\{\begin{array}{l}\boldsymbol{\alpha}\in\mathbb{R}^{2}\;:\;\frac{\big{(}( \cos\omega)(\alpha_{1}-1)+(\sin\omega)\alpha_{2}\big{)}^{2}}{a^{2}}\\ \qquad+\frac{\big{(}-(\sin\omega)(\alpha_{1}-1)+(\cos\omega)\alpha_{2}\big{)}^ {2}}{b^{2}}=c\end{array}\right\}\,.\] The parameter \(c\) indexes the different level sets. Show that for \(\omega\to 45^{\circ}\), \(a\to 1/\sqrt{10}\), and \(b\to\infty\), these level sets converge to \[\big{\{}\boldsymbol{\alpha}\in\mathbb{R}^{2}\;:\;5(\alpha_{1}-1+\alpha_{2})^{ 2}=c\big{\}}\,.\] For this, recall that a sequence of sets \(\mathcal{A}_{1},\mathcal{A}_{2},\dots\) converges to \(\mathcal{A}\) if for every point \(a\in((\cup_{j}\mathcal{A}_{j})\cup\mathcal{A})\), there is an \(n\in\{1,2,\dots\}\) such that either \(a\in(\mathcal{A}_{m}\cap\mathcal{A})\) for all \(m\geq n\) or \(a\not\in\mathcal{A}_{m},\,a\not\in\mathcal{A}\) for all \(m\geq n\).In the case here, it suffices to show that \(((\cos\omega)(\alpha_{1}-1)+(\sin\omega)\alpha_{2})^{2}/a^{2}+(-(\sin\omega)( \alpha_{1}-1)+(\cos\omega)\alpha_{2})^{2}/b^{2}\to 5(\alpha_{1}-1+\alpha_{2})^{2}\). Conclude with 2. that for \(d=1\), we can think of the least-squares' level sets as \(45^{\circ}\)-rotated ellipses that are stretched infinitely in one direction.
5. We switch to \(d\neq 1\) and write \[\|\boldsymbol{v}-X\boldsymbol{\alpha}\|_{2}^{2}=\frac{\big{(}( \cos\omega)(\alpha_{1}-1)+(\sin\omega)\alpha_{2}\big{)}^{2}}{a^{2}}\\ +\frac{\big{(}-(\sin\omega)(\alpha_{1}-1)+(\cos\omega)\alpha_{2} \big{)}^{2}}{b^{2}}\] for \(\omega\in[0^{\circ},180^{\circ})\), \(a,\,b\in(0,\infty)\). In the following, we compute the parameters \(\omega\), \(a\), and \(b\6. Compare the \((\alpha_{1}-1)\)-terms of the formulations in 5. and 1. to show that if \(a\neq b\), it holds that \[(\cos\omega)^{2}\,=\,\frac{5-\frac{1}{b^{2}}}{\frac{1}{a^{2}}-\frac{1}{b^{2}}}\,.\]
7. Compare the \(\alpha_{2}\)-terms of the formulations in 5. and 1. to show that if \(a\neq b\), it holds that \[\frac{1}{a^{2}}+\frac{1}{b^{2}}\,=\,9+d^{2}\,.\]
8. Use the previous two steps to show that if \(a\neq b\), it holds that \[(\cos\omega)^{2}(\sin\omega)^{2}\,=\,\frac{5(4+d^{2})b^{4}-(9+d^{2})b^{2}+1}{( 9+d^{2})^{2}b^{4}-4(9+d^{2})b^{2}+4}\,.\]
9. Show similarly as in the previous steps that \(a\neq b\) for every \(d\neq 1\). Conclude from 5.-7. that if \(d\neq 1\), the least-squares function can indeed be written in the mentioned elliptic form with the parameters derived in 6.-7.
10. Compare the \((\alpha_{1}-1)\alpha_{2}\)-terms of the formulations in 1. and 5. to show that \[\frac{5(4+d^{2})-\frac{9+d^{2}}{b^{2}}+\frac{1}{b^{4}}}{(9+d^{2})^{2}-\frac{4 (9+d^{2})}{b^{2}}+\frac{4}{b^{2}}}-\frac{(4+d)^{2}}{(9+d^{2}-\frac{2}{b^{2}})^ {2}}\,=\,0\,.\]
11. Use the above insights to draw level sets of the least-squares objective function \(a\mapsto\|y-Xa\|_{2}^{2}\) for \(d=1\) and \(d\neq 1\).

### 7 R Lab: Least-Squares vs. Ridge Estimation

In this lab, we compare the least-squares estimator with the ridge estimator.

Your task is to replace the keyword REPLACE by suitable code and to answer the questions posed in the text.

#### Generating Toy Data

Generate data according to \(\boldsymbol{y}=X\boldsymbol{\beta}+\boldsymbol{u}\) with \(X\in\mathbb{R}^{2\times 2}\) as in \(\blacksquare\) Table 1.3 with \(d=2\), \(\boldsymbol{\beta}=(1,1/2)^{\top}\), and \(\boldsymbol{u}\sim\mathcal{N}_{2}[\mathbf{0}_{2},(0.1)^{2}\,\mathbf{I}_{2\times 2}]\). You might want to use the rnorm() function.

```
set.seed(11) DesignFamily<-function(d) { return(matrix(c(1, 2, d, 2), nrow=2, ncol=2)) } design<- DesignFamily(2) regression.vector<-c(1, 1/2) outcome<-REPLACE
```

#### Implementing the Estimators

Implement the least-squares estimator (for \(d\neq 1\)). You might want to use the solve() function.

```
LeEstimator<-function(y,x) { REPLACE } LsEstimator(outcome,design)
```
###[,1]
##[1,]1.0617625
##[2,]0.4395672 ```
Similarly, implement the ridge estimator.
``` RidgeEstimator<-function(y,X,r) { REPLACE } RidgeEstimator(outcome,design,1)

##[,1]
##[1,]0.6774037
##[2,]0.6469656

#### Showing that the Ridge Estimator

Approximates a Least-Squares Solution

Show that the ridge estimator approaches in \(\ell_{2}\)-norm the least-squares solution when \(r\) goes to zero. For this, establish a function that compares the \(\ell_{2}\)-differences between the ridge estimator and the least-squares: \(\|\widehat{\beta}_{\text{ridge}}-\widehat{\beta}_{\text{ls}}\|_{2}\).

```
BidgetDifference<-function(r,y,x) { EXPLACE } tuning.parameter<-0.001*s(i1:10000) difference<-apply(as.matrix(tuning.parameter),:1,RidgeLaDifference,youtcome,design) plot(x=tuning.parameter,y=difference,typ=-1",lty=1,ylin=e(0,1),yavg=e(0,1,2),las=1,xlab="tuningparameter",ylin="-differences")
```

We find that the smaller the tuning parameter, the closer the ridge estimator is to the least-squares estimator. Eventually, for \(r\to 0\), they coincide.

#### Comparing Estimation Errors

Compare the \(\ell_{2}\)-estimator errors of the ridge estimator and the LS estimator: \(\|\widehat{\beta}_{\text{ridge}}^{r}-\beta\|_{2}\) vs. \(\|\widehat{\beta}_{\text{ls}}-\beta\|_{2}\).

``` RidgeError<-function(r,y,x) { EXPLACE } LaiError<-function(y,x) { EXPLACE } tuning.parameter<-0.001*s(i1:10000) estimation.error<-apply(as.matrix(tuning.parameter),:1,RidgeError,outcome,design)
We observe in particular that the \(\ell_{2}\)-error of the ridge estimator becomes large if the tuning parameter is large. What happens for \(r\to\infty\)?

#### Showing That the Ridge Estimator Is Continuous in the Data

Show that the ridge estimator is continuous in the variation of the design parameter \(d\) around the critical point \(d_{\text{critical}}=1\). For this, compute the \(\ell_{2}\)-distances of ridge estimators on data with different design parameters \(d\) to the ridge estimator on data with fixed design parameter \(d_{\text{critical}}=1\). Set the tuning parameter to \(r=0.5\) throughout.

```
alignment(d,r,y,d_critical) { KESLACK } tuningparameter(d,r,y,d_critical)
``` d_=0.01*e(0.01*0) differences-apply(d,r,s,ridge_variance_fixed,outcome,d_critical) plot(d, difference,type-1',try1,y2-e(0,e,y,2),lini)

[MISSING_PAGE_FAIL:43]

The curve of the second coordinate increases until about \(r=1.5\) and then decreases, which shows that the coordinates are not necessarily monotone in the tuning parameter. Nevertheless, both curves decrease monotonously approximately as \(c/r\) for \(r\) large, where \(c\in(0,\infty)\) is a constant that can depend on the coordinate (and the data, of course), which corroborates our theoretical findings in the main text.

#### Comparing Ridge and Least-Squares on Economic Data

We apply the ridge estimator and the least-squares on the longley data that contains seven economic variables and is included in the standard R distributions. Take as outcome variable the Employed variable, and as predictor variables GNP.deflator, GNP, Unemployed, Armed.Forces, and Population. Also add a column of ones to design matrix to account for the intercept. (We regularize the intercept as any other variable.) Bonus: Write a version of the ridge estimator that does not regularize the intercept.) Hint: For easier manipulations later, use the as.matrix() function to convert all quantities to matrices.

``` outcome<-as.matrix(longley["Employed"]) design<-REPLACE cbind(outcome,design)[1:5,]
#EmployedInterceptGNP.deflatorGNPUnemployedArmed.ForcesPopulation
#194760.231218.0234.28925.6159.0170.008
#1948261.12218.5259.424222.5145.6108.642
#194960.171188.2258.054368.2161.6109.773
#195064.1871189.5284.599215.1616.016.039
#195163.6223126.223975200.929.1129.075

LesEstimator(outcome, design)

## Employed
## Intercept 92.461307827
## GNP.deflator -0.048462828
## GNP 0.072003849
## Unemployed -0.004038711
## Armed.Forces -0.005604956
## Population -0.403508682

RidgeEstimator(outcome, design, 1)

## Employed
## Intercept 0.02227192
## GNP.deflator 0.21920075
## GNP -0.01035088
## Unemployed -0.01394317
## Armed.Forces -0.00579588
## Population 0.45119435

RidgeEstimator(outcome, design, 100)

## Employed
## Intercept 0.004790496
## GNP.deflator 0.252814814
## GNP -0.012145722
## Unemployed -0.012151700
## Armed.Forces -0.003817691
## Population 0.418655390

RidgeEstimator(outcome, design, 10000)

## Employed
## Intercept 0.001724652
## GNP.deflator 0.101214227
## GNP 0.012424157
## Unemployed 0.043045876
## Armed.Forces 0.065315837
## Population 0.156333129

Are the ridge estimator's coordinates monotone in the tuning parameter here? Is the ridge estimator's magnitude in \(\ell_{2}\) monotone in the tuning parameter here?

#### Bonus: Checking the Theoretical Formulae of Sect. 1.4

We show that the formulae in Sect. 1.4 are correct. We first compare the least-squares estimator to the version in the script for three values of \(d\):

```
LeEstimatorTheory<-c(1,0) outcome<-c(1,2) LeEstimator(outcome,DesignFamily(0))-LaEstimatorTheory
###[,1]
##[1,]2.220446e-16
##[2,]-2.220446e-16
``` LeEstimator(outcome,DesignFamily(2))-LaEstimatorTheory
##[,1]
##[1,]2.220446e-15
##[2,]-1.332268e-15

```
LeEstimator(outcome,DesignFamily(1.1))-LaEstimatorTheory
##[,1]
##[1,]-3.375078e-14
##[2,]6.217249e-14
```

These results confirm the theoretical formulae within the bounds of numerical precision.

We now compare the ridge estimator to the version in the script for three values of \(d\) and \(r\):

```
ridgeEstimatorTheory<-function(d,r) { return(:/(4-s+d+s+d^2+(9+d^2)+r+r+r^2)*e(4-s+d+s+d^2+s+r,(4+d+s)) ] RidgeEstimator(outcome,DesignFamily(0),1)- RidgeEstimatorTheory(0,1)
##[,1]-1.110223e-16
##[2,]0.000000e+00
```

### [,1]
## [1,] 1.665335e-15
## [2,] -6.245005e-16

**Algorithm**(outcome, Design/emply(1,1), 0.5) - Algorithm/Theory(1, 0.5)

## [,1]
## [1,] 2.775558e-16
## [2,] 1.110223e-16

We find again equivalence within numerical precision.

#### Bonus: A Technical Note

Consider the following two functions:

Function_1 <- function(y, X) {  return(solve(t(X) %*% X) %*% t(X) %*% y) } Function_2 <- function(y, X) {  return(solve(t(X) %*% X, t(X) %*% y)) } Y <- c(1, 2) X <- matrix(c(1, 2, 0, 2), nrow=2, ncol=2)  Function_1(y, X)

## [,1]
## [1,] 1.000000e+00
## [2,] -2.220446e-16

Function_2(y, X)

## [,1]
## [1,] 1
## [2,] 0

What is the difference between the two functions?

To keep the code as close as possible to the script, we have generated all labs' results by using the solve function as in Function_1. However, the faster and numerically more stable version is Function_2, which solves a systemof equations instead of inverting a matrix and subsequently multiplying that inverse with a vector.

Implement the least-squares and the ridge as suggested by both Function_1 and Function_2 and compare the results for this lab. Are there any differences?

### Notes and References

1 References for the sequencing of the human genome are Kidd et al. (2008) and citations therein. An example for early research on regression-type relationships in this context is Dettling and Buhlmann (2004), for gene-gene regulatory networks Dobra et al. (2004).
2 Other types of genomic variations include single-nucleotide polymorphisms (SNPs) (Judson et al., 2002), which are substitutions of single nucleotides, and insertion-deletions (INDELs) (Mills et al., 2006), which are insertions or deletions of small nucleotide sequences. In general, the abundant ways in which genomes can vary make genomics a prototypical field of application for high-dimensional statistics.
3 Associations between diseases and CNVs of different ranges are described in Almal and Padh (2012).
4 Actually, dropping data is common: for example, data sets usually come with "meta"-data such as the name of the lab the measurements were taken in, the machines used in the experiments, and so on--still, such information is rarely included in statistical analyses.
5 High-dimensional statistics is often associated with the phrase "curse of dimensionality," insinuating that high-dimensionality is an unwanted but inevitable side effect of modern data. This view probably roots in the wealth of mathematical and algorithmic challenges high-dimensional spaces are known to bring about. An archetypical example for those challenges is that volumes are very difficult to estimate in \(\mathbb{R}^{p}\) with \(p\) large, because the number of intersection points needed to form lattices with fixed distances between adjacent points increases exponentially in the number of dimensions of the ambient space (see Fig. 1.2). But from a statistical perspective, we should follow \(\blacktriangleright\) Sect. 1.1 in viewing high-dimensionality as an _opportunity_ generated by modern data: Only rich enough data can bear the large parameter spaces of models that unravel phenomena in fine detail. Accordingly, we could associate high-dimensional statistics with the phrase _blessing of dimensionality_.
6 We just assume that one of the predictors is reserved for modeling the intercept: for example, if \(x_{1}=1\) for all observations, then \(\beta_{1}x_{1}=\beta_{1}\) is the intercept.
7 Some of the earliest theoretical results for the lasso estimator have been derived in Greenshtein and Ritov (2004).
8 Such risk bounds can be derived by combining our power-two bounds of \(\blacktriangleright\) Chap. 6 with ideas from Bellec and Tsybakov (2017).
9 The field of _inverse problems_ studies the reconstruction of the factors that have produced given observations from the observationsthemselves. For example, the calculation of the mass distribution of the earth from the earth's gravity field is an inverse problem (because the earth's gravity field is the effect of the earth's mass distribution and not the other way around); the calculation of the earth's gravity field from the earth's mass distribution would be the corresponding _forward problem_. A general reference for inverse problems is Engl et al. (1996); an early paper that establishes a connection between inverse problems and numerical stability is Tikhonov (1943).
* An early paper on the Ridge estimator in regression is Hoerl and Kennard (1970).
* There are several equivalent definitions of the Moore-Penrose matrix inverse; the stated one is due to Penrose (1955, p. 406), see Albert (1972, Theorem 3.9 on p. 28) for a textbook reference. Existence and uniqueness of Moore-Penrose matrix inverses follow, for example, from Penrose (1955, Theorem 1 on p. 406) and Albert (1972, Theorem 3.4 on p. 19).
* We consider densities with respect to the appropriate Lebesgue measures unless stated otherwise.
* This connection has been highlighted through the introduction of the Bayes lasso estimator (Park and Casella, 2008) and discussed further in the context of tuning-parameter calibration (Bu and Lederer, 2017, Section 2.3).

Figure 2: One can pick \(\lfloor 1/d+1\rfloor\) points from the unit interval such that these points have mutual Euclidean distance \(d\in(0,\infty)\) (the left panel depicts the case \(d=0.5\)), at least \(\lfloor 1/d+1\rfloor^{2}\) of such points from the unit square (middle panel), and at least \(\lfloor 1/d+1\rfloor^{3}\) such points from the unit cube (right panel). In general, one can pick at least \(\lfloor 1/d+1\rfloor^{p}\) points from the \(p\)-dimensional unit cube such that the points have distance \(d\) between each other, which is an exponential increase in \(p\)

## Chapter Linear Regression

### 1 Overview - 38

###### Contents

Linear regression relates predictor variables and outcome variables, such as gene copy numbers and the level of a biomarker. The assumed linearity of the relationships makes the models convenient both mathematically and computationally. And since the data can be arbitrarily transformed beforehand, such as by including polynomials of the copy numbers as predictor variables or by replacing the level of the biomarker in the outcome variable by its logarithm, linear regression can also effectively model non-linear relationships. This simplicity and flexibility have made linear regression the most popular statistical framework across the sciences and standard textbook material. But the standard methods for linear regression, such as the least-squares estimator, premise that the number of parameters is small as compared to the number of samples, which limits their usefulness in modern, data-intensive research, where the increasing granularity of data has prompted interest in increasingly complex models. More recent high-dimensional methods, in contrast, allow for models with many more parameters. These methods are the topic of this chapter.

### 2.1 Overview

We first formalize the framework of linear regression. We assume that there are \(n\) real-valued observations \(y_{1},\ldots,y_{n}\in\mathbb{R}\) and corresponding vector-valued observations \(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{n}\in\mathbb{R}^{p}\); each pair \((y_{i},\boldsymbol{x}_{i})\) is called a _sample_. The samples are modeled according to

\[y_{i}\ =\ \boldsymbol{x}_{i}^{\top}\boldsymbol{\beta}+u_{i}\qquad\qquad \qquad\text{ for all }i\in\left\{1,\ldots,n\right\},\]

where the vector \(\boldsymbol{\beta}\in\mathbb{R}^{p}\) that summarizes the _model parameters_\(\beta_{1},\ldots,\beta_{p}\in\mathbb{R}\) is called the _regression vector_ (which is the same across the samples) and \(u_{1},\ldots,u_{n}\in\mathbb{R}\) the _noise_ (which can be different from one sample to another). The regression vector captures the linear relationship between \(y_{i}\) and \(\boldsymbol{x}_{i}\); the noise comprises everything else, such as measurement errors, non-linear relationships, and so forth. We call these models _linear regression models_.

The most common examples of such models are Gaussian linear regression models with fixed design, where \(u_{1},\ldots,u_{n}\) are i.i.d. distributed according to \(\mathcal{N}_{1}[0,\sigma^{2}]\) for a \(\sigma\in(0,\infty)\), and where \(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{n}\) are fixed vectors. Wewill use Gaussian models throughout the book to illustrate ideas, but we keep otherwise our derivations universal, that is, we allow for arbitrary noise distributions and random designs.

We summarize the above equations in

\[\boldsymbol{y} = X\boldsymbol{\beta}+\boldsymbol{u}\,, \tag{2.1}\]

where \(\boldsymbol{y}=(y_{1},\ldots,y_{n})^{\top}\in\mathbb{R}^{n}\) is called _outcome vector_ or just _outcome_, \(X=(\boldsymbol{x}_{1}^{\top},\ldots,\boldsymbol{x}_{n}^{\top})^{\top}\in \mathbb{R}^{n\times p}\)_design matrix_, and \(\boldsymbol{u}=(u_{1},\ldots,u_{n})^{\top}\in\mathbb{R}^{n}\)_noise vector_ or just _noise_. Moreover, the columns \(\widetilde{\boldsymbol{x}}_{j}\) of the design matrix \(X\) (that is, \(X=(\widetilde{\boldsymbol{x}}_{1},\ldots,\widetilde{\boldsymbol{x}}_{p})\)) are called _predictors_.1

Footnote 1: The _input_output_ is a vector \(\boldsymbol{y}\in\mathbb{R}^{n}\), where \(\boldsymbol{y}\) is the _input_output_.

The pair \((\boldsymbol{y},X)\) is the _data_. The data is known to the statistician, while the regression vector and the noise are not. Our goal is to draw inferences from the data about the regression vector, that is, to unravel the relationships between outcome and design. We call this process _linear regression_.

How difficult linear regression is mainly depends on four model specifications. First, linear regression becomes easier with increasing sample size \(n\) (more information available) and more challenging with increasing model size \(p\) (more parameters to estimate). Most challenging are models with \(p\approx n\) or even \(p\gg n\). We call such models _high-dimensional_.

Second, linear regression becomes easier with increasing amounts of prior information (more specific regularization possible). Estimators that complement least-squares with functions to leverage this prior information are called _regularized least-squares estimators_.

Third, linear regression becomes more challenging with increasing magnitudes of the noise \(u_{i}\) (data less informative). For prediction and estimation, where we are interested in \(X\boldsymbol{\beta}\) and \(\boldsymbol{\beta}\), respectively, the decisive factor is the absolute strength of the noise; for support recovery, where we are interested in \(\text{supp}[\boldsymbol{\beta}]\), the decisive factor is the strength of the noise relative to the magnitudes of the parameters \(\beta_{i}\). We relate the two influences in the _signal-to-noise ratio_\(\|X\boldsymbol{\beta}\|_{2}/\|\boldsymbol{u}\|_{2}\). (But note that the signal-to-noise ratio is typically unknown in practice.)

And finally, estimation and support recovery become easier with increasing differences among the predictors. The reason for this is that the relationships of the outcome with similar predictors are hard to disentangle. This is not necessarily a problem for prediction, where we consider the relationship of the outcome with the collection of all predictors (\(\blacktriangleright\) Chap. 6). But for estimation and support recovery, where we consider the relationship of the outcome with each predictor individually, this can cause ambiguity (\(\blacktriangleright\) Chap. 7).

Since the predictors and their relationships play an important role in linear regression, we establish here some vocabulary and first insights. Predictor vectors are typically _normalized_, which means that they are scaled to Euclidean lengths equal to, say, the square-root of the sample size: \(\|\boldsymbol{x}_{i}\|_{2}=\sqrt{n}\). The differences among the predictors are then captured by their relative spatial orientations, that is, the angle \(\angle_{\boldsymbol{x}_{i},\boldsymbol{x}_{j}}\) between any two predictors \(\boldsymbol{x}_{i},\boldsymbol{x}_{j}\). We call the cosines of these angles the _correlations_:

\[\text{cor}[\boldsymbol{x}_{i},\boldsymbol{x}_{j}]\ \ :=\ \ \cos[\angle_{ \boldsymbol{x}_{i},\boldsymbol{x}_{j}}]\ \ =\ \ \frac{\langle\boldsymbol{x}_{i},\boldsymbol{x}_{j}\rangle}{\|\boldsymbol{x}_{i}\|_{2}\|\boldsymbol{x}_{j}\|_{2}\ \in\ [0,1]\,.\]

Correlations are symmetric: \(\text{cor}[\boldsymbol{x}_{i},\boldsymbol{x}_{j}]=\text{cor}[\boldsymbol{x}_{j},\boldsymbol{x}_{i}]\). Small correlations (that is, large angles) indicate that the corresponding predictors are different; large correlations (small angles) indicate that the predictors are similar. We speak of a _highly correlated design_ if "many" of these correlations are large (\(\text{cor}[\boldsymbol{x}_{i},\boldsymbol{x}_{j}]\approx 1\) for many \(i\neq j\)), a _weakly correlated design_ if "most" of these correlations are small (\(\text{cor}[\boldsymbol{x}_{i},\boldsymbol{x}_{j}]\approx 0\) for most \(i\neq j\)), and a _scaled orthogonal design_ or just _orthogonal design_2 if there are no correlations and the predictors are normalized (\(\text{cor}[\boldsymbol{x}_{i},\boldsymbol{x}_{j}]=0\) for all \(i\neq j\) and \(\|\boldsymbol{x}_{i}\|_{2}=\sqrt{n}\) for all \(i\)).

The predictors are the columns of the design matrix \(X=(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{p})\), so that

\[(X^{\top}X)_{ij} = \sum_{k=1}^{n}X_{ik}^{\top}X_{kj}\] definition of matrix-matrix multiplications \[= \sum_{k=1}^{n}X_{ki}X_{kj}\] Definition B.2.1 of matrix transposes \[= \sum_{k=1}^{n}(\boldsymbol{x}_{i})_{k}(\boldsymbol{x}_{j})_{k} \text{above formulation of }X\] \[= \langle\boldsymbol{x}_{i},\ \boldsymbol{x}_{j}\rangle\ \ \text{ definition of the standard inner product}\]for all \(i,j\in\{1,\ldots,p\}\). Hence, the diagonal elements of the Gram matrix \(X^{\top}X\) are the squared Euclidean lengths of the predictors: \((X^{\top}X)_{ii}=\langle\mathbf{x}_{i},\,\mathbf{x}_{i}\rangle=\|\mathbf{x}_{i}\|_{2}^{2}\). And the off-diagonal elements of the Gram matrix are related to the correlations between predictors: \((X^{\top}X)_{ij}=\mathrm{cor}[\mathbf{x}_{i},\mathbf{x}_{j}]\|\mathbf{x}_{i}\|_{2}\|\mathbf{x}_{ j}\|_{2}\). This means that for normalized designs, the scaled Gram matrix \(X^{\top}X/n\) is

\[\frac{X^{\top}X}{n}\ =\ \begin{pmatrix}1&\mathrm{cor}[\mathbf{x}_{1},\mathbf{x}_{2}]& \cdots&\mathrm{cor}[\mathbf{x}_{1},\mathbf{x}_{p}]\\ \mathrm{cor}[\mathbf{x}_{2},\mathbf{x}_{1}]&1&&\vdots\\ \vdots&&\ddots&\mathrm{cor}[\mathbf{x}_{p-1},\mathbf{x}_{p}]\\ \mathrm{cor}[\mathbf{x}_{p},\mathbf{x}_{1}]&\cdots&\mathrm{cor}[\mathbf{x}_{p},\mathbf{x}_{p-1 }]&1\end{pmatrix}.\]

Thus, the scaled Gram matrix conveniently summarizes the correlations. In particular, \(X^{\top}X/n=\mathrm{I}_{p\times p}\) for orthogonal designs, and more generally, the larger the magnitudes of the off-diagonal entries of \(X^{\top}X/n\), the larger the correlations.

We now proceed as follows: We introduce regularized least-squares estimators in \(\blacktriangleright\) Sect. 2.2 and post-processing strategies for them in \(\blacktriangleright\) Sect. 2.3. The two following sections then form the mathematical basis for the remainder of this book, for example, for deriving guarantees of those high-dimensional estimators in terms of prediction: In \(\blacktriangleright\) Sect. 2.4, we establish the Holder inequality (Theorem 2.4.1), which will allow us to transform basic inequalities into prediction bounds (\(\blacktriangleright\) Sect. 6.3). In \(\blacktriangleright\) Sect. 2.5, we establish KKT conditions (Example 2.5.3), which will allow us to derive some basic inequalities in the first place (\(\blacktriangleright\) Sect. 6.2).

### 2.2 Sparsity-Inducing Prior Functions

In the previous chapter, we have shown that classical estimators can deteriorate rapidly in performance with increasing number of model parameters. This deterioration can manifest in a phenomenon called _overfitting_--see \(\blacktriangleright\) Fig. 2.1: the estimators miss the essential structure of the data-generating process over the peculiarities of the data at hand. The culprit is the noise, because it masks the essential structure; the larger the noise, the higher the risk of overfitting--cf. (1.5). Overfitting can be avoided by complementing the bare measurements with additional information. Such information can stem from previous studies, the experimental design, physical laws, and other sources. Prior functions formulate this information mathematically and funnel it into the statistical analysis.

The type of information that is most frequently encountered in high-dimensional statistics is _sparsity_. A linear regression model of the form (1) is called sparse if only a small number of its model parameters are different from zero: \(|\{j:\beta_{j}\neq 0\}|\ll n,p\). Similarly, an estimator \(\widehat{\mathbf{\beta}}\in\mathbb{R}^{p}\) for the regression parameter \(\mathbf{\beta}\) is called sparse if it produces a sparse model: \(|\{j:\widehat{\beta_{j}}\neq 0\}|\ll n,p\); and more generally, a vector \(\mathbf{\alpha}\in\mathbb{R}^{p}\) is called sparse if it contains only a small number of non-zero-valued coordinates as compared to the model dimensions: \(|\{j:\alpha_{j}\neq 0\}|\ll n,p\).

The first attempt to leverage sparsity is the _best-subset selection estimator_

\[\widehat{\mathbf{\beta}}_{\text{subset}}\ \in\ \operatorname*{arg\,min}_{\mathbf{ \alpha}\in\mathbb{R}^{p}}\left\{\,|\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}+r|\mathbf{\alpha} |_{0}\,\right\},\]

where \(r\in[0,\infty)\) is a tuning parameter and \(|\mathbf{\alpha}|_{0}=\#\{j\in\{1,\ldots,p\}:\alpha_{j}\neq 0\}\) counts the number of non-zero-valued coordinates in \(\mathbf{\alpha}\). The least-squares term \(|\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}\) ensures a good fit to the data, while the \(\ell_{0}\)-prior term favors regression vectors that have only a small number of non-zero-valued coordinates.

Figure 1: This illustration of overfitting involves noisy observations (gray circles) of a simple quadratic function (red, solid curves) for fitting polynomials with the least-squares (blue, solid lines) and the lasso (green, dashed lines). The polynomials used in the estimations have degrees 2, 5, 20 (panels from left to right). The larger the degree of these polynomials, that is, the larger the number of irrelevant predictors (the degree of the true data-generating polynomial is 2), the less the least-squares estimates capture the shape of the underlying function; we call this overfitting. The lasso (with a cross-validated tuning parameter), on the other hand, simply disregards those additional predictors, and therefore, does not suffer from overfitting in this example. This is the case because the lasso assumes correctly that the underlying model is simple. See the lab for details

The \(\ell_{0}\)-prior is a non-convex, non-linear, "combinatorial" function, which makes the objective function \(\boldsymbol{a}\ \mapsto\ |\boldsymbol{y}-X\boldsymbol{a}|_{2}^{2}+r|\boldsymbol{a}|_{0}\) hard to optimize--especially for large \(p\). One approach to reducing this computational burden is to mimic \(|\boldsymbol{\alpha}|_{0}\) through \(|\boldsymbol{\alpha}|_{q}\coloneqq(\sum_{j=1}^{p}|\alpha_{j}|^{q})^{1/q}\) for some \(q\in(0,\infty)\).[3] Especially interesting are \(q\in[1,\infty)\), because these values lead to convex objective functions (see \(\blacktriangleright\) Sect. 2.5), for which there are standard optimization algorithms. We have already seen a corresponding estimator in the previous chapter: the ridge estimator, where \(q=2\). The ridge estimator is particularly easy to compute (it even has an explicit solution), but it does not yield sparse models--see \(\blacksquare\) Fig. 2.2. A closer mimic of the best-subset selection estimator is the _lasso estimator_,[4] where \(q=1\):

\[\widehat{\boldsymbol{\beta}}_{\text{lasso}}\ \in\ \operatorname*{arg\,min}_{ \boldsymbol{\alpha}\in\mathbb{R}^{p}}\left\{\ |\boldsymbol{y}-X\boldsymbol{\alpha}|_{2}^{2}+r| \boldsymbol{\alpha}|_{1}\ \right\}. \tag{2.2}\]

The lasso has the best of two worlds: its objective function is convex and produces sparse solutions; in fact, \(q=1\) is the only choice that ensures both convexity and sparsity--see again \(\blacksquare\) Fig. 2.2.

Refinements of the simple notion of sparsity above are called _structured sparsity_. For example, structured sparsity in the form of _group sparsity_ describes group-wise behaviors: Consider a partition of the index set \(\{1,\ldots,p\}\), that is, a collection of disjoint, non-empty sets \(\mathcal{A}_{1},\ldots,\mathcal{A}_{k}\subset\{1,\ldots,p\}\) that satisfy \(\cup_{j=1}^{k}\mathcal{A}_{j}=\{1,\ldots,p\}\). Sparsity means that only a small number of _individual_ coordinates are not equal to zero: \(|\{j:\beta_{j}\neq 0\}|\ll n,p\); in contrast, group sparsity now means that only a small number of _groups_ of coordinates as indexed by the sets \(\mathcal{A}_{j}\) are not equal to zero: \(|\{j:\boldsymbol{\beta}_{\mathcal{A}_{j}}\neq\boldsymbol{0}_{|\mathcal{A}_{j} |}\}|\ll n,p\). An estimator that leverages this type of structured sparsity is the _group-lasso estimator_:[5]

\[\widehat{\boldsymbol{\beta}}_{\text{group}}\ \in\ \operatorname*{arg\,min}_{ \boldsymbol{\alpha}\in\mathbb{R}^{p}}\left\{\ |\boldsymbol{y}-X\boldsymbol{\alpha}|_{2}^{2}+r\sum_{j=1}^{k}|\boldsymbol{ \alpha}_{\mathcal{A}_{j}}|_{2}\ \right\}. \tag{2.3}\]

The group lasso can be thought of as an intermediate between the lasso and ridge estimators: The group lasso coincides with the lasso when \(k=p\) and \(\mathcal{A}_{j}=\{j\}\) and, more generally, inherits the sparsity of the lasso in the sense that it sets entire subvectors \(\boldsymbol{\alpha}_{\mathcal{A}_{j}}\) to zero. The group lasso coincides with the ridge estimator (or the Figure 2.2: Parameter estimation in two dimensions with least-squares data-fitting function \(\mathbf{a}\mapsto|\mathbf{y}-X\mathbf{a}|_{2}^{2}\) and \(\ell_{2^{*}}\) (ridge), \(\ell_{1^{*}}\) (lasso), and \(\ell_{1/2}\)-prior function. The design is \(X=\mathrm{I}_{2\times 2}\) throughout, while the outcome \(\mathbf{y}\in\mathbb{R}^{2}\) is altered from top to bottom. The tuning parameters are chosen such that all estimators’ fits are equal: \(|\mathbf{y}-X\widehat{\mathbf{\beta}}|_{2}^{2}=b\) for some constant \(b\in(0,\infty)\); the blue circles denote the corresponding level sets \((\mathbf{a}\in\mathbb{R}^{2}:|\mathbf{y}-X\mathbf{a}|_{2}^{2}=b)\). Since the estimators minimize a weighted sum of the data-fitting function and the prior function, they are found where a blue level set of the least-squares function “touches” a level set of the prior function; those latter level sets are drawn in red. The different touching points illustrate that the ridge estimator yields zero-valued coordinates only in very special cases (in the first column, \(\widehat{\beta}_{1}=0\) only on the top), while zero-valued coordinates are more common for the lasso estimator and especially for the estimator with the \(\ell_{1/2}\)-prior function (\(\widehat{\beta}_{1}=0\) on the top and middle of the second column and across all of the third column). The shapes of the red level sets also indicate that the \(\ell_{2^{*}}\)- and \(\ell_{1}\)-prior functions lead to convex objective functions, while the \(\ell_{1/2}\)-prior function does not—see \(\blacktriangleright\) Sect. 2.5 for details. The \(\ell_{1}\)-regularizer is a sweet spot in that it typically yields sparse solutions (in contrast to \(\ell_{q}\) with \(q>1\)) and, at the same time, makes the objective function amenable to convex optimization (in contrast to \(\ell_{q}\) with \(q<1\))

Euclidean distance ridge estimator)[6] when \(k=1\) and \(\mathcal{A}_{1}=\{1,\ldots,p\}\), and more generally, acts like the ridge within the groups in the sense that the objective function is invariant under orthogonal transformations within groups and that if \(|\boldsymbol{\alpha}_{\mathcal{A}_{j}}|_{2}\neq 0\), then typically _all_ coordinates of that subvector are non-zero-valued.

Estimators can also comprise combinations of several prior functions. A popular example is the _elastic net estimator_[7]

\[\widehat{\boldsymbol{\beta}}_{\text{elastic}}\,\in\,\underset{\boldsymbol{ \alpha}\in\mathbb{R}^{p}}{\arg\min}\left\{\,|\boldsymbol{v}-X\boldsymbol{ \alpha}|_{2}^{2}+r(w|\boldsymbol{\alpha}|_{1}+(1-w)|\boldsymbol{\alpha}|_{2}^ {2})\,\right\},\]

which interpolates between the lasso (\(w=1\)) and the ridge (\(w=0\)). The elastic net with an intermediate value \(w\in(0,1)\) inherits properties from both of the two base estimators: On the one hand, the resulting estimator is similar to the lasso in the sense that it tends to be sparse; on the other hand, the resulting estimator is much unlike the lasso but instead similar to the ridge in the sense that it is always unique and tends to select whole groups of highly correlated predictors rather than just one representative among them.[8] The value of \(w\) is often fixed based on subjective reasonings beforehand, because a fully data-driven calibration of both tuning parameters \(r\in[0,\infty)\) and \(w\in[0,1]\) is conceptually and computationally challenging. In general, combining prior functions always involves the problem of having to deal with multiple tuning parameters.

### 2.3 Post-Processing Methods

Prior functions push estimators in the directions indicated by the prior information. This can improve estimation in high dimensions, but it can also generate a bias. This bias is the target of _post-processing methods_. A first method, least-squares refitting, applies a least-squares estimator to the support of the estimator. This uses the fact that the least-squares estimator is unbiased in low dimensions. A second method, thresholding, simply sets small elements of the estimator to zero. This reinforces the effect of sparsity-inducing priors and, thereby, can decrease the amount of regularization that is needed in the first place. Both methods should, however, be applied with care: least-squares refitting can sometimes even increase estimation errors, and thresholding introduces yet another tuning parameter.

The \(\ell_{0}\)-function favors solutions with many zero-valued coordinates, but it completely disregards parameter values otherwise: for example, \(|\boldsymbol{\alpha}|_{0}=1\) whether \(\boldsymbol{\alpha}=(1,0,\ldots,0)^{\top}\) or \(\boldsymbol{\alpha}=(100,0,\ldots,0)^{\top}\). In contrast, the discussed replacements of that function keep increasing in the magnitudes of the parameter values: for example, \(|\boldsymbol{\alpha}|_{1}=1\) for \(\boldsymbol{\alpha}=(1,0,\ldots,0)^{\top}\) but \(|\boldsymbol{\alpha}|_{1}=100\) for \(\boldsymbol{\alpha}=(100,0,\ldots,0)^{\top}\). In addition to setting a fraction of the model parameters exactly to zero as intended, this general favoring of small model parameters can introduce an unwanted _overall_ shrinkage of the estimates. To remove such biases, _least-squares refitting_[9] complements high-dimensional estimators \(\widehat{\boldsymbol{\beta}}\) with subsequent least-squares estimation on its support:

\[\widehat{\boldsymbol{\beta}}_{\text{ls}}[\widehat{\boldsymbol{\beta}}]\in \operatorname*{arg\,min}_{\begin{subarray}{c}\boldsymbol{\alpha}\in\mathbb{ B}\\ \operatorname*{supp}[\boldsymbol{\alpha}]<\operatorname*{supp}(\widehat{ \boldsymbol{\beta}}]\end{subarray}}|\boldsymbol{y}-X\boldsymbol{\alpha}|_{2}^ {2}\,.\]

In other words, the initial estimator is used only for a screening for non-zero-valued coordinates, while the corresponding parameter values are determined entirely by the subsequent least-squares. The rationale is that the least-squares is an accurate and unbiased estimator in low-dimensional and correctly specified models (see Sect. 1.3); hence, the two-stage approach presumes that the initial estimator is sparse, which means here \(|\text{supp}[\widehat{\boldsymbol{\beta}}]|\ll n\), and that the initial estimator provides accurate support recovery, that is, \(\text{supp}[\widehat{\boldsymbol{\beta}}]\) is a good approximation of the true support \(\text{supp}[\boldsymbol{\beta}]\). In the ideal case \(\text{supp}[\widehat{\boldsymbol{\beta}}]=\text{supp}[\boldsymbol{\beta}]\), the least-squares refitted estimator possesses the _strong oracle property_, which means that the refitted estimator \(\widehat{\boldsymbol{\ell}}_{\text{ls}}[\widehat{\boldsymbol{\beta}}]\) equals the oracle "estimator"

\[\widehat{\boldsymbol{\ell}}_{\text{ls}}[\boldsymbol{\beta}]\in\operatorname*{ arg\,min}_{\begin{subarray}{c}\boldsymbol{\alpha}\in\mathbb{B}^{0}\\ \text{supp}[\boldsymbol{\alpha}]<\operatorname*{supp}[\boldsymbol{\beta}] \end{subarray}}|\boldsymbol{y}-X\boldsymbol{\alpha}|_{2}^{2}\,,\]

which knows the true support beforehand; in particular, \(\widehat{\boldsymbol{\ell}}_{\text{ls}}[\boldsymbol{\beta}]\) is unbiased (see Sect. 1.3). But in practice, exact support recovery \(\text{supp}[\widehat{\boldsymbol{\beta}}]=\text{supp}[\boldsymbol{\beta}]\) is often unrealistic and, in any case, unverifiable (see Sect. 7.4 for a discussion about support recovery).10, 11

To a varying extent, least-squares refitting is already integrated in many estimators. For example, least-squares refitting leaves best-subset selection completely unchanged, that is, \(\widehat{\mathbf{\beta}}_{\rm ls}[\widehat{\mathbf{\beta}}_{\rm subset}]=\widehat{\mathbf{ \beta}}_{\rm subset}\). This means that adding least-squares refitting to the best-subset selection estimator would be redundant. Other examples are least-squares with variants of the capped \(\ell_{1}\)-norm as prior. The capped \(\ell_{1}\)-norm with cutoff \(c\in[0,\infty]\), is the map \(\mathbf{a}\!\mapsto\sum_{j=1}^{\beta}\min\{|a_{j}|,\,c\}\). The boundary cases for least-squares with this prior are classical least-squares (for \(c=0\), the prior is zero) and lasso (for \(c=\infty\), the prior is the \(\ell_{1}\)-function); more generally, \(c\) is the value below which model parameters are pulled toward zero--see \(\blacksquare\) Fig. 3. Smooth versions of the capped \(\ell_{1}\)-norm are used in the _scad_ and _mcp_ estimators, for example.[12] Such estimators have been shown to satisfy the strong oracle property if \(\operatorname{supp}[\widehat{\mathbf{\beta}}]=\operatorname{supp}[\mathbf{\beta}]\).[13] However, (i) exact support recovery remains a strict and unverifiable assumption; (ii) capped \(\ell_{1}\)-norms render the objective functions non-convex, which can make computations challenging; and (iii) the prior functions involve one or more additional

Figure 3: A physics analogy for how capping a prior function can avoid bias in large parameter estimates. The toy estimator is \(\widehat{\beta}\in\arg\min_{\alpha\in\mathbb{R}}\{|\mathbf{y}-\alpha\mathbf{x}|_{2}^{2 }+rh[\alpha]\}\), where \(\mathbf{x}\in\mathbb{R}^{n}\) is the only predictor and \(r\in(0,\infty)\) a fixed tuning parameter. On the left, \(h[a]:=|a|\) is the usual \(\ell_{1}\)-prior in \(\mathbb{R}\); on the right, \(h[a]:=\min\{|a|,\,c\}\) is a capped version with \(c\in(0,\infty)\) defining the transition point. One can think of the estimator’s objective function as a physical potential for an iron ball at \(\alpha\) that is subject to three forces: 1. a “magnetic force” (red) that is generated by the magnitude of the least-squares function at that point; this force pulls the ball horizontally to the least-squares solution \(\widehat{\mathbf{\beta}}_{\rm ls}=\mathbf{x}^{\top}\mathbf{y}/|\mathbf{x}|_{2}^{2}\); 2. a “gravitational force” (blue) that is generated by the magnitude of the prior function; this force pulls the ball vertically downwards; 3. a “normal force” (black) that is generated by the slope of the prior function; this force pushes the ball upwards perpendicular to the surface. The three forces are in equilibrium at \(\widehat{\beta}\). For the \(\ell_{1}\)-prior on the left, the inclined surface allows the gravitational force, more precisely, its component parallel to the surface, to pull the estimate away from the (typically unbiased) least-squares solution. For the capped prior on the right, this is still the case if \(|\widehat{\mathbf{\beta}}_{\rm ls}|<c\). If \(|\widehat{\mathbf{\beta}}_{\rm ls}|\geq c\), on the other hand, the gravitational force is perpendicular to the surface and, therefore, cannot work against the magnetic force anymore. The magnetic force then moves the ball all the way to the least-squares solution, which is the minimum of the magnetic potential (and, consequently, the magnetic force is zero there). This means that our toy estimator can coincide with a least-squares solution even though regularization is imposed

tuning parameters, such as \(a\) in the vanilla case of capped \(\ell_{1}\)-regularization, that need to be calibrated.14 In practice, these difficulties have to be weighted against a potential gain in accuracy.

Footnote 14: The model is trained on a \(\ell_{1}\)-regularization, and the model is trained on a \(\ell_{1}\)-regularization.

Thresholding sets small estimates to zero. Given a _cut-off_\(c\in[0,\infty]\), we define the hard-thresholded version \(\widehat{\boldsymbol{\beta}}^{c}\) of an estimator \(\widehat{\boldsymbol{\beta}}\) via

\[(\widehat{\boldsymbol{\beta}}^{c})_{j}\,:=\,\left\{\begin{array}{ll}0&\text{ if }|\widehat{\boldsymbol{\beta}}_{j}|\leq c\\ \widehat{\boldsymbol{\beta}}_{j}&\text{otherwise}\end{array}\right. \tag{4}\]

for \(j\in\{1,\ldots,p\}\). While \(\widehat{\boldsymbol{\beta}}^{0}=\widehat{\boldsymbol{\beta}}\), thresholding with \(c>0\) sets small coordinate values of \(\widehat{\boldsymbol{\beta}}\) to zero. This means that thresholding promotes sparsity further, with the effect that the estimator \(\widehat{\boldsymbol{\beta}}\) can use a smaller tuning parameter--and, therefore, can have smaller bias--and still lead to a sparse output. However, thresholding requires calibration of both the original tuning parameter and of the cutoff \(c\); see Sect. 7.4 for details.

One can also combine least-squares refitting and thresholding--at the expense of an even more complex pipeline. In general, we recommend using post-processing methods cautiously.

### Holder Inequality

The analysis of regularized least-squares estimators centers around inner products of the form \(\langle X^{\top}\boldsymbol{u}\), \(\boldsymbol{a}\rangle\) with \(\boldsymbol{a}\in\mathbb{R}^{p}\). In particular, such inner products determine good tuning parameters--see \(\blacktriangleright\) Chaps. 4, 6, and 7. Our main tool for working with these inner products is the _Holder inequality_. The classical version of Holder inequality bounds inner products in terms of \(\ell_{q}\)-norms and their duals. In this section, we establish a generalized version and use it to connect inner products to prior information and regularization.

The essential ingredients of the Holder inequality are dual functions.

[MISSING_PAGE_EMPTY:6281]

#### General Holder Inequality

Let \(\mathcal{A}:\mathbb{R}^{p}\to[0,\infty]\) be a non-negative function that satisfies

1. (positive definiteness) \(\mathcal{A}[\boldsymbol{a}]=0\) if and only if \(\boldsymbol{a}=\boldsymbol{0}_{p}\);
2. (scalability) \(\mathcal{A}[\boldsymbol{a}/\mathcal{A}[\boldsymbol{a}]]\leq 1\) for all \(\boldsymbol{a}\in\mathbb{R}^{p}\backslash\{\boldsymbol{0}_{p}\}\).

Then, also the dual function \(\overline{\mathcal{A}}\) of \(\mathcal{A}\) is positive definite and

\[\langle\boldsymbol{a},\,\boldsymbol{b}\rangle\,\leq\,\overline{\mathcal{A}}[ \boldsymbol{a}]\mathcal{A}[\boldsymbol{b}]\qquad\text{ for all }\boldsymbol{a},\,\boldsymbol{b}\in\mathbb{R}^{p}\,. \tag{2.5}\]

The algebra rules on the extended real line \([-\infty,\infty]\) are summarized on Page X. The scalability stipulates some sort of linearity; in particular, every linear function (and similarly, every absolute homogeneous function) is scalable.16 The positive definiteness essentially avoids dividing by zero. Both conditions are satisfied by every norm. Finally, since both the dual function and the original function are positive definite (according to the first claim of the theorem and by assumption, respectively), the right-hand side of Inequality (2.5) is always non-negative.

A typical first step in the analysis of a regularized least-squares estimator \(\widehat{\boldsymbol{\beta}}\) is the decomposition of its objective function (\(\blacktriangleright\) Sect. 6.2):

\[\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2}^{2}+r\mathcal{A}[\boldsymbol{ \alpha}]\,=\,\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+2\langle X^{\top}\boldsymbol{a},\,\boldsymbol{\beta}-\boldsymbol{\alpha}\rangle\\ +\|\boldsymbol{u}\|_{2}^{2}+r\mathcal{A}[\boldsymbol{\alpha}]\,,\]

where we have used the linear regression model \(\boldsymbol{y}=X\boldsymbol{\beta}+\boldsymbol{u}\) in (2.1). The next step is then to control the inner product \(2\langle X^{\top}\boldsymbol{u},\,\boldsymbol{\beta}-\boldsymbol{\alpha}\rangle\) (\(\blacktriangleright\) Sect. 6.3). Elementary trigonometry teaches us that \(\langle\boldsymbol{a},\,\boldsymbol{b}\rangle=\cos[\angle_{\boldsymbol{a}, \boldsymbol{b}}]\|\boldsymbol{a}\|_{2}\|\boldsymbol{b}\|_{2}\) for every pair of vectors \(\boldsymbol{a},\,\boldsymbol{b}\in\mathbb{R}^{p}\), where \(\angle_{\boldsymbol{a},\boldsymbol{b}}\) is the angle between \(\boldsymbol{a}\) and \(\boldsymbol{b}\). To avoid working with angles, we can use the estimate \(|\cos[\angle_{\boldsymbol{a},\boldsymbol{b}}]|\leq 1\), which leads to the bound \(\langle X^{\top}\boldsymbol{u},\,\boldsymbol{\beta}-\boldsymbol{\alpha} \rangle\leq\|X^{\top}\boldsymbol{u}|_{2}\|\boldsymbol{\beta}-\boldsymbol{ \alpha}\|_{2}\). This bound nicely separates the model parameters \(\boldsymbol{\alpha},\,\boldsymbol{\beta}\) from the rest of the problem, but it does not relate to the prior term \(r\mathcal{A}[\boldsymbol{\alpha}]\). To bring the separation in accordance with \(r\mathcal{A}[\boldsymbol{\alpha}]\), we instead apply the Holder inequality, which yields \(\langle X^{\top}\boldsymbol{u},\,\boldsymbol{\beta}-\boldsymbol{\alpha} \rangle\leq\overline{\mathcal{A}}[X^{\top}\boldsymbol{u}]\mathcal{A}[ \boldsymbol{\beta}-\boldsymbol{\alpha}]\). This separates \(\boldsymbol{\alpha},\,\boldsymbol{\beta}\) from the rest (which we will identify later as the effective noise of regularized estimators) in a way that makes appear the prior function \(\mathcal{A}\). In this sense,the Holder inequality allows us to connect inner products to prior information and regularization.

This property makes the Holder inequality indispensable for high-dimensional statistics, but the inequality is also important in mathematics more generally. Mostly, it is stated in its classical form:

\[\begin{array}{l}\square\hskip-14.226378pt\mbox{\bf Example 2.41}\\ \hskip 14.226378pt\mbox{\bf Classical H\"{o}lder Inequality}\end{array}\]

In this example, we show that Theorem 2.4.1 implies the classical Holder inequality. The classical version reads

\[\langle\boldsymbol{a},\,\boldsymbol{b}\rangle\,\leq\,|\!|\boldsymbol{a}|\!|_{l }|\!|\boldsymbol{b}|\!|_{m} \tag{2.6}\]

for \(\boldsymbol{a},\boldsymbol{b}\in\mathbb{R}^{p}\) and \(l,m\in[1,\infty]\) such that \(1/l+1/m=1\) (where \(1/\infty=0\) by convention--see Page X). The special case \(l=m=2\) is called _Cauchy-Schwarz inequality_.

One can check that \(\boldsymbol{\mu}:\boldsymbol{a}\mapsto|\!|\boldsymbol{a}|\!|_{m}\) satisfies both conditions in Theorem 2.4.1, which means that we can apply the theorem to \(\boldsymbol{\hat{\mu}}=|\!|\!|_{m}\). The dual of that function is \(\overline{\boldsymbol{\hat{\mu}}}=|\!|\!|_{l}\) (see 3. in Exercise 2.2). Hence, Inequality (2.5) in the theorem applied to \(\boldsymbol{\hat{\mu}}=|\!|\!|_{m}\) coincides with the above classical version of the Holder inequality.\(\blacktriangleleft\)

We conclude this section by proving the Holder inequality.

Proof of Theorem 2.4.1The proof is based on the definition of dual functions and on the linearity of inner products. A subtlety is the calculus with infinite values.

We first prove that \(\overline{\boldsymbol{\hat{\mu}}}\) is positive definite under the stated assumptions. We observe that

\[\overline{\boldsymbol{\hat{\mu}}}[\boldsymbol{0}_{p}] =\,\,\sup\big{\{}\,\langle\boldsymbol{0}_{p},\,\boldsymbol{c} \rangle\,:\,\boldsymbol{c}\in\mathbb{R}^{p},\,\,\boldsymbol{\hat{\mu}}[ \boldsymbol{c}]\leq 1\,\big{\}}\] \[=\,\,\sup\big{\{}\,0\,:\,\boldsymbol{c}\in\mathbb{R}^{p},\,\, \boldsymbol{\hat{\mu}}[\boldsymbol{c}]\leq 1\,\big{\}}\] \[=\,0\,.\hskip 14.226378pt\mbox{Assumption 1. ensures that}\] \[\{\boldsymbol{c}\in\mathbb{R}^{p}:\boldsymbol{\hat{\mu}}[ \boldsymbol{c}]\leq 1\}\supset\{\boldsymbol{0}_{p}\}\neq\varnothing\]

Next, it holds for all \(\boldsymbol{a}\neq\boldsymbol{0}_{p}\) that

\[\sup\big{\{}\,\langle\boldsymbol{a},\,\boldsymbol{c}\rangle\,:\,\boldsymbol {c}\in\mathbb{R}^{p},\,\,\boldsymbol{\hat{\mu}}[\boldsymbol{c}]\leq 1\,\big{\}}\, \geq\,\,\langle\boldsymbol{a},\,\boldsymbol{a}/\boldsymbol{\hat{\mu}}[ \boldsymbol{a}]\rangle\,.\]

[MISSING_PAGE_FAIL:65]

## 2.5 Optimality Conditions\({}^{\star}\)

Many high-dimensional estimators are minimizers of a convex objective function. Some of these objective functions are also differentiable: For example, theridge estimator (1.7) is the minimizer of the objective function \(\ell_{\text{ridge}}:\mathbf{a}\mapsto|\mathbf{y}-X\mathbf{a}|_{2}^{2}+r|\mathbf{a}|_{2}^{2}\), which is both (strictly) convex and differentiable. In \(\blacktriangleright\) Sect. 1.4, we have exploited the differentiability of the ridge estimator's objective function to derive an explicit formulation for the estimator. Our strategy for this was to set the gradient of the objective function to zero and then solve for the estimator. On the other hand, objective functions of high-dimensional estimators are not always differentiable, and, therefore, there might not be a gradient to start with: for example, the lasso estimator (2.2) is the minimizer of \(\ell_{\text{lasso}}:\mathbf{a}\mapsto|\mathbf{y}-X\mathbf{a}|_{2}^{2}+r|\mathbf{a}|_{1}\), which is still convex (see Example 2.5.1 below) but not differentiable. However, as long as objective functions are convex, they can be equipped with _generalized_ gradients called subgradients (see Definition 2.5.2 below). Setting these subgradients to zero does not necessarily provide explicit formulations as in the case of the ridge estimator and, therefore, does not necessarily provide a direct way to compute the estimators, but the equations characterize the estimators in a way that is very useful in developing algorithms and mathematical guarantees (see \(\blacktriangleright\) Sects. 6.2 and 7.3, for example). We call these characterizations optimality conditions or, in the special case of regularized least-squares estimators, KKT conditions.

The crucial concept in this section is convexity. A function is _convex_ if its graph[17] is on or below the closed line segment[18] between any two points in the graph; a function is _strictly convex_ if its graph is strictly below the open line segment between any two points in the graph. The two concepts are illustrated in \(\blacksquare\) Fig. 2.4 and recorded in the following definition:

**Definition 2.5.1**: **Convex Functions** A function \(\ell:\mathbb{R}^{p}\to\mathbb{R}\) is _convex[19]_ if and only if

\[\ell|w\mathbf{a}+(1-w)\mathbf{b}|\,\leq\,w\ell|\mathbf{a}|+(1-w)\ell|\mathbf{b}|\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{for all }w\in[0,1];\;\mathbf{a},\mathbf{b}\in\mathbb{R}^{p}\,.\]

The function \(\ell\) is also _strictly convex_ if

\[\ell|w\mathbf{a}+(1-w)\mathbf{b}|\,<\,w\ell|\mathbf{a}|+(1-w)\ell|\mathbf{b}|\] \[\qquad\qquad\qquad\qquad\qquad\text{for all }w\in(0,1);\;\mathbf{a},\mathbf{b}\in\mathbb{R}^{p},\mathbf{a}\neq\mathbf{b}\,.\]Hence, every strictly convex function is convex. A main convenience in working with convex and strictly convex functions is that there is no ambiguity between local and global minima: every local minimum of a convex/strictly convex function is also a global minimum--see Exercise 2.6 for details.

Another property of convexity that will be very helpful in our analyses is that non-negative combinations of convex functions remain convex:

[frame = ]Equip 2.51 Convexity of Non-negative CombinationsIf the functions \(\ell,\mathpzc{g}:\mathbb{R}^{p}\to\mathbb{R}\) are convex and \(u,v\in[0,\infty)\), then the function \(u\ell+v\mathpzc{g}\) is also convex.

If, additionally, \(\ell\) is strictly convex and \(u\neq 0\), then the function \(u\ell+v\mathpzc{g}\) is also strictly convex.

The objective functions of many high-dimensional estimators are of the form (see Display (1.6) in Chap. 1)

\[\ell_{\text{hd}}\ :\ \mathpzc{a}\ \mapsto\ \text{DataFitting}[Z,\mathpzc{a}]+r \operatorname{Prior}[\mathpzc{a}]\,.\]

Lemma 2.5.1 ensures that such \(\ell_{\text{hd}}\) are convex as long as the data-fitting and prior functions are convex and the tuning parameter is non-negative.

Proof of Lemma 2.5.1The proof is a short calculation based on the formulation of convexity in Definition 2.5.1.

Figure 2.4: The function \(\ell:x\mapsto\max\{x^{2},0.5\}\) displayed on the left-hand side is convex: between every given \(a,b\in\mathbb{R}\), the points on the graph (black line) are on or below the closed line segment between \(a\) and \(b\) (blue line including the end points). The function \(\mathpzc{g}:x\mapsto x^{2}\) displayed in the middle is strictly convex: between every given \(a,b\in\mathbb{R}\), \(a\neq b\), the points on the graph are strictly below the open line segment between \(a\) and \(b\) (blue line excluding the end points). The function \(\mathpzc{h}:x\mapsto x^{2}+0.1\cos(25x)\) displayed on the right-hand side is not convex: the graph is sometimes above the line segment

[MISSING_PAGE_EMPTY:6288]

for all \(w\in[0,1]\) and \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{p}\). The same calculation applies to _every_ norm, that is, norms are convex in general.

We conclude that the lasso's objective function, as well as the least-squares data-fitting function complemented with any norm prior, is convex.

Our main motivation for studying convex functions, however, is that they permit the formulation of _subgradients_, which are generalized gradients that do not require differentiability. Setting these subgradients to zero will allow us to characterize the minima of every convex objective function.

**Definition 2.5.2**: **Subgradients and Subdifferentials** Consider a convex function \(\ell:\mathbb{R}^{p}\to\mathbb{R}\) and a vector \(\mathbf{a}\in\mathbb{R}^{p}\). A vector \(\mathbf{b}\in\mathbb{R}^{p}\) is a _subgradient_ of \(\ell\) at \(\mathbf{a}\) if

\[\ell[\mathbf{c}]\,\geq\,\ell[\mathbf{a}]+\langle\mathbf{b},\,\mathbf{c}-\mathbf{a}\rangle\qquad \qquad\text{ for all }\mathbf{c}\in\mathbb{R}^{p}\,.\]

The _subdifferential_ of \(\ell\) at \(\mathbf{a}\) is the set

\[\partial f[\mathbf{a}]\,:=\,\{\mathbf{b}\in\mathbb{R}^{p}\,:\,\mathbf{b}\text{ is a subgradient of }\ell\text{ at }\mathbf{a}\}\,.\]

It can be shown that subdifferentials are not empty.[20]

For differentiable functions, subgradients simplify to standard gradients:

**Definition 2.5.2**: **Differentiable Functions** If \(\ell:\mathbb{R}^{p}\to\mathbb{R}\) is convex and differentiable at \(\mathbf{a}\) with gradient \(\nabla\ell[\mathbf{a}]\), it holds that \(\partial f[\mathbf{a}]=\{\nabla\ell[\mathbf{a}]\}\).

Hence, the subdifferentials of a differentiable function each contain exactly one element. The subdifferentials of a general convex function, in contrast, can be uncountably infinite.

Proof of Lemma 2.5.2We presume two facts without proof: Fact 1 is that, as mentioned before, subdifferentials of convex functions are never empty. Fact 2 is the basic calculus result that \(b\) is the gradient of a differentiable function \(\ell\) at \(a\) (that is, \(b=\nabla\ell[a]\)) if and only if

\[\langle b,\,\,\mathbf{d}\rangle\,\,=\,\,\lim_{t\to 0^{+}}\frac{\ell[ \mathbf{a}+t\mathbf{d}]-\ell[\mathbf{a}]}{t}\qquad \qquad\mbox{for all}\,\,\mathbf{d}\in\mathbb{R}^{p}\,.\]

In light of Facts 1 and 2, we need to show that

\[b\,\in\,\mathbf{\partial}\ell[\mathbf{a}]\quad\Rightarrow\] \[\langle b,\,\,\mathbf{d}\rangle\,\,=\,\,\lim_{t\to 0^{+}} \frac{\ell[\mathbf{a}+t\mathbf{d}]-\ell[\mathbf{a}] }{t}\,\,\,\mbox{for all}\,\,\mathbf{d}\in\mathbb{R}^{p}\,.\]

The assumption \(b\in\mathbf{\partial}\ell[\mathbf{a}]\) implies via the Definition 2.5.2 of subgradients that

\[\ell[\mathbf{c}]\,\geq\,\ell[\mathbf{a}]+\langle b,\,\, \mathbf{c}-\mathbf{a}\rangle\qquad\quad\mbox{for all}\,\, \mathbf{c}\in\mathbb{R}^{p}\,.\]

Setting in this inequality \(c=\mathbf{a}+t\mathbf{d}\) for arbitrary \(t\in\mathbb{R}\) and \(d\in\mathbb{R}^{p}\) gives

\[\ell[\mathbf{a}+t\mathbf{d}]\,\geq\,\ell[\mathbf{a} ]+\langle\mathbf{b},\,\,(\mathbf{a}+t\mathbf{d})- \mathbf{a}\rangle\,.\]

Simplifying and using the linearity of inner products, we can deduce from the inequality that

\[\ell[\mathbf{a}+t\mathbf{d}]\,\geq\,\ell[\mathbf{a} ]+t\langle\mathbf{b},\,\,\mathbf{d}\rangle\,.\]

Rearranging further, we find for every \(t\neq 0\) that

\[\langle\mathbf{b},\,\,\mathbf{d}\rangle\,\,\leq\,\frac{\ell [\mathbf{a}+t\mathbf{d}]-\ell[\mathbf{a}]}{t}\,.\]

Also, making the replacements \(t\mapsto\,-\,t\) and \(d\mapsto\,-\,\mathbf{d}\) (both \(t\) and \(d\) were arbitrary except for \(t\neq 0\)), we find readily

\[\langle\mathbf{b},\,\,\mathbf{d}\rangle\,\geq\,\frac{\ell[ \mathbf{a}+t\mathbf{d}]-\ell[\mathbf{a}]}{t}\,.\]

Taken together, the two previous displays yield

\[b\,\in\,\mathbf{\partial}\ell[\mathbf{a}]\quad\Rightarrow\] \[\langle\mathbf{b},\,\,\mathbf{d}\rangle\,=\, \frac{\ell[\mathbf{a}+t\mathbf{d}]-\ell[\mathbf{a}] }{t}\,\,\,\mbox{for all}\,\,t\in\mathbb{R}\backslash\{0\},\,\mathbf{d} \in\mathbb{R}^{p}\,.\]We can conclude by taking the limit in \(t\) on the right-hand side. 

Another important property of subdifferentials is that they are linear in the following sense:

[frame = ] **Non-negative Combinations of Functions** Let \(\ell\), \(\varsigma\) : \(\mathbb{R}^{p}\to\mathbb{R}\) be convex functions and \(u\), \(v\in[0,\infty)\) non-negative constants. Then, it holds for every \(\boldsymbol{a}\in\mathbb{R}^{p}\) that

\[\mathfrak{a}\big{(}u\ell[\boldsymbol{a}]+v\varsigma[\boldsymbol{a}]\big{)}\,= \,u\mathfrak{a}\ell[\boldsymbol{a}]+u\mathfrak{a}\varsigma[\boldsymbol{a}]\,,\]

where \(u\mathfrak{a}\ell[\boldsymbol{a}]+v\mathfrak{a}g[\boldsymbol{a}]:=\{u\!c^{1}+ v\!c^{2}:\boldsymbol{c}^{1}\in\mathfrak{a}\ell[\boldsymbol{a}]\), \(\boldsymbol{c}^{2}\in\mathfrak{a}g[\boldsymbol{a}]\}\) by Page IX.

We omit the proof.[21]

[frame = ] **Subdifferentials of the Lasso** In this exercise, we derive the subdifferentials of the lasso's objective function in (2.2):

\[\ell_{\text{lasso}}\,:\,\,\boldsymbol{a}\mapsto\,\,[\boldsymbol{y}-X \boldsymbol{a}]_{2}^{2}+r[\boldsymbol{a}]_{1}\,.\]

This function is not differentiable if \(r>0\), but it is still convex according to Example 2.5.1 above.

The least-squares function \(\ell_{1\!s}:\boldsymbol{a}\mapsto|\boldsymbol{y}-X\boldsymbol{a}|_{2}^{2}\) is convex (see 1. in Exercise 1.4) and differentiable with gradient \(\nabla\ell_{1\!s}:\boldsymbol{a}\mapsto\,-\,2X^{\top}(\boldsymbol{y}-X \boldsymbol{a})\) (see Page 6). The subdifferentials of the least-squares function are, therefore, (see Lemma 2.5.2 about subdifferentials of differentiable functions)

\[\mathfrak{a}\ell_{1\!s}[\boldsymbol{a}]\,=\,\big{\{}\nabla\ell_{1\!s}[ \boldsymbol{a}]\big{\}}\,=\,\big{\{}-2X^{\top}(\boldsymbol{y}-X\boldsymbol{a} )\big{\}}\,.\]

The prior function \(\ell_{\ell_{1}}:\boldsymbol{a}\mapsto|\boldsymbol{a}|_{1}\) can be written as \(\ell_{\ell_{1}}[\boldsymbol{a}]=\sum_{j=1}^{p}|a_{j}|=\sum_{j=1}^{p}\ell_{\ell_ {1}}^{j}[\boldsymbol{a}]\), where \(\ell_{\ell_{1}}^{j}:\boldsymbol{a}\mapsto|a_{j}|\) is convex (see 2. in Exercise 2.7). The subdifferential of \(\ell_{\ell_{1}}\) is, therefore, the sum of the subdifferentials of the \(\ell_{\ell_{1}}^{j}\)'s (see Lemma 2.5.3 about subdifferentials of non-negative combinations of functions).

Each function \(\ell_{\ell_{1}}^{j}\) is differentiable as long as \(a_{j}\neq 0\), and the coordinates of the gradient are \((\nabla\ell_{\ell_{1}}^{j})[\boldsymbol{a}]_{j}=\text{sign}[a_{j}]\), where \(\text{sign}:\,a\mapsto\mathbbm{1}(a\geq 0)-\mathbbm{1}(a\leq 0)\) is the sign function,and \((\nabla f^{j}_{\ell_{1}})[\mathbf{a}]_{i}=0\) for all \(i\in\{1,\ldots,p\}\), \(i\neq j\). Hence, if \(a_{j}\neq 0\), (see Lemma 2.5.2 about the subdifferentials of differentiable functions)

\[\mathfrak{d}\ell^{j}_{\ell_{1}}[\mathbf{a}]=\big{\{}\mathbf{b}\in\mathbb{ R}^{p}:b_{j}=\text{sign}[a_{j}]\text{ and }\\ b_{i}=0\text{ for }i\in\{1,\ldots,p\},\,i\neq j\big{\}}\,.\]

For the case \(a_{j}=0\), we invoke the formulation of subgradients directly. The corresponding Definition 2.5.2 applied to the function \(\ell^{j}_{\ell_{1}}\) yields, since \(\ell^{j}_{\ell_{1}}[\mathbf{a}]=0\) in this case, and since \(\ell^{j}_{\ell_{1}}[\mathbf{c}]=|c_{j}|\) in general, that \(\mathbf{b}\in\mathfrak{d}\ell^{j}_{\ell_{1}}[\mathbf{a}]\) if and only if

\[|c_{j}|\,\geq\,\langle\mathbf{b},\,\mathbf{c}-\mathbf{a}\rangle\qquad\text{ for all }\mathbf{c}\in\mathbb{R}^{p}\,.\]

Writing the inner product out, we can formulate the condition in the form (note that \(a_{j}=0\))

\[|c_{j}|\,\geq\,b_{j}c_{j}+\sum_{\begin{subarray}{c}i=1\\ i\neq j\end{subarray}}^{p}b_{i}(c_{i}-a_{i})\qquad\quad\text{ for all }\mathbf{c}\in\mathbb{R}^{p}\,.\]

This condition must be true for _all_\(\mathbf{c}\in\mathbb{R}^{p}\). For \(\mathbf{c}\in\mathbb{R}^{p}\) such that \(c_{j}=0\) and \(c_{i}=b_{i}+a_{i}\) for \(i\neq j\), it becomes

\[0\,\geq\,\sum_{\begin{subarray}{c}i=1\\ i\neq j\end{subarray}}^{p}b_{i}^{2}\,,\]

which means that \(b_{i}=0\) for all \(i\neq j\). The condition on \(\mathbf{b}\), therefore, can be written as

\[|c_{j}|\,\geq\,b_{j}c_{j}\,\text{ for all }c_{j}\in\mathbb{R}\quad\text{ and }\quad b_{i}=0\text{ for }i\neq j\,.\]

The first part of this condition, as one can check readily, is satisfied if and only if \(|b_{j}|\leq 1\). Therefore, we get for \(a_{j}=0\) that

\[\mathfrak{d}\ell^{j}_{\ell_{1}}[\mathbf{a}]=\big{\{}\mathbf{b}\in\mathbb{ R}^{p}:|b_{j}|\leq 1\text{ and }b_{i}=0\text{ for }\\ i\in\{1,\ldots,p\},i\neq j\big{\}}\,.\]

Combining the subdifferentials of the \(\ell^{j}_{\ell_{1}}\)'s according to Lemma 2.5.3 yields \((|\mathbf{a}|_{1}=\ell_{\ell_{1}}[\mathbf{a}]=\sum_{j=1}^{p}\ell^{j}_{\ell_{1}}[\mathbf{b}])\)

\[\mathfrak{d}\|\mathbf{a}\|_{1}=\big{\{}\mathbf{b}\in\mathbb{R}^{p}:|\mathbf{b}|_{\infty} \leq 1\text{ and }\\ b_{j}=\text{sign}[a_{j}]\text{ for all }j\in\{1,\ldots,p\}\text{ such that }a_{j}\neq 0\big{\}}\,,\]

[MISSING_PAGE_EMPTY:6293]

Lemma 2.5.2 states that \(\mathfrak{d}\mathcal{F}[\boldsymbol{a}]=\{\nabla\mathcal{F}[\boldsymbol{a}]\}\) for every convex function \(\mathcal{F}\) that is differentiable; the theorem here implies, therefore, the classical result that \(\boldsymbol{a}\in\mathbb{R}^{p}\) is a minimum of a convex and differentiable function \(\mathcal{F}\) if and only if \(\nabla\mathcal{F}[\boldsymbol{a}]=\boldsymbol{0}_{p}\). However, the optimality conditions in the theorem apply to _every_ convex function, and consequently Theorem 2.5.1 generalizes that classical result to convex but not necessarily differentiable functions.

Proof of Theorem 2.5.1The proof relies on Definition 2.5.2 of subdifferentials and the fact that, by linearity of the inner product, \(\langle\boldsymbol{0}_{p},\ \boldsymbol{d}\rangle=0\) for all \(\boldsymbol{d}\in\mathbb{R}^{p}\).

Step 1: NecessityWe first prove that

\[\mathcal{F}[\boldsymbol{c}]\,\geq\,\mathcal{F}[\boldsymbol{a}]\text{ for all }\boldsymbol{c}\in\mathbb{R}^{p}\qquad\Rightarrow\qquad\boldsymbol{0}_{p}\, \in\,\mathfrak{d}\mathcal{F}[\boldsymbol{a}]\,.\]

If \(\mathcal{F}[\boldsymbol{c}]\,\,\geq\,\,\mathcal{F}[\boldsymbol{a}]\) for all \(\boldsymbol{c}\in\mathbb{R}^{p}\), we get by linearity of inner products

\[\mathcal{F}[\boldsymbol{c}]\,\geq\,\mathcal{F}[\boldsymbol{a}]+\langle \boldsymbol{0}_{p},\ \boldsymbol{c}-\boldsymbol{a}\rangle\qquad\qquad\text{ for all }\boldsymbol{c}\in\mathbb{R}^{p}\,.\]

By Definition 2.5.2 of the subdifferential, this means that \(\boldsymbol{0}_{p}\in\mathfrak{d}\mathcal{F}[\boldsymbol{a}]\), as desired.

Step 2: SufficiencyWe now prove that

\[\boldsymbol{0}_{p}\,\in\,\mathfrak{d}\mathcal{F}[\boldsymbol{a}]\qquad \Rightarrow\qquad\mathcal{F}[\boldsymbol{c}]\,\geq\,\mathcal{F}[\boldsymbol{a}]\text{ for all }\boldsymbol{c}\in\mathbb{R}^{p}\,.\]

If \(\boldsymbol{0}_{p}\in\mathfrak{d}\mathcal{F}[\boldsymbol{a}]\), Definition 2.5.2 of the subdifferential yields

\[\mathcal{F}[\boldsymbol{c}]\,\geq\,\mathcal{F}[\boldsymbol{a}]+\langle \boldsymbol{0}_{p},\ \boldsymbol{c}-\boldsymbol{a}\rangle\qquad\qquad\text{ for all }\boldsymbol{c}\in\mathbb{R}^{p}\,.\]

By linearity of inner products, this inequality includes

\[\mathcal{F}[\boldsymbol{c}]\,\geq\,\mathcal{F}[\boldsymbol{a}]\qquad\qquad \text{ for all }\boldsymbol{c}\in\mathbb{R}^{p}\,,\]

as desired.

## Chapter 2 Examples

### 2.1 Exercises for \(\boldsymbol{\triangleright}\) Sect. 2.1

**Exercise 2.1**: \(\boldsymbol{\blacklozenge}\boldsymbol{\blacklozenge}\) In this exercise, we illustrate that additional predictors typically increase the complexity of least-squares estimates. We consider data (\(\boldsymbol{y}\), \(X\)) from the linear regression model in \(\boldsymbol{\triangleright}\) Sect. 2.2 and an augmented version of these data (\(\boldsymbol{y}\), \(X^{\prime}\)) with \(X^{\prime}:=(X,\boldsymbol{x})\in\mathbb{R}^{n\times(p+1)}\) for avector \(\mathbf{x}\in\mathbb{R}^{n}\). We denote the corresponding least-squares estimators by \(\widehat{\mathbf{\beta}}_{\text{Is}}\) and \((\widehat{\mathbf{\beta}}_{\text{Is}})^{\prime}\), respectively. Note that \(\widehat{\mathbf{\beta}}_{\text{Is}}\in\mathbb{R}^{p}\), whereas \((\widehat{\mathbf{\beta}}_{\text{Is}})^{\prime}\in\mathbb{R}^{p+1}\).
1. Show that \[\min_{\mathbf{\alpha}^{\prime}\in\mathbb{R}^{p+1}}\|\mathbf{y}-X^{\prime}\mathbf{\alpha}^{ \prime}\|_{2}^{2}\ \leq\ \min_{\mathbf{\alpha}\in\mathbb{R}^{p}}\|\mathbf{y}-X\mathbf{ \alpha}\|_{2}^{2}\,.\]
2. Show that if the additional predictor \(\mathbf{x}\) and the residual \(\mathbf{y}-X\widehat{\mathbf{\beta}}_{\text{Is}}\) are not orthogonal, that is, \(\langle\mathbf{y}-X\widehat{\mathbf{\beta}}_{\text{Is}},\ \mathbf{x}\rangle\neq 0\), it holds that \[\min_{\mathbf{\alpha}^{\prime}\in\mathbb{R}^{p+1}}\|\mathbf{y}-X^{\prime}\mathbf{\alpha}^ {\prime}\|_{2}^{2}\ <\ \min_{\mathbf{\alpha}\in\mathbb{R}^{p}}\|\mathbf{y}-X\mathbf{ \alpha}\|_{2}^{2}\,.\]
3. Show that the strict inequality in the above display is a sufficient condition for \((\widehat{\mathbf{\beta}}_{\text{Is}})^{\prime}_{p+1}\neq 0\).
4. Show that if \(\operatorname{rank}[X]\geq n\), it holds that \[\min_{\mathbf{\alpha}^{\prime}\in\mathbb{R}^{p+1}}\|\mathbf{y}-X^{\prime}\mathbf{\alpha}^ {\prime}\|_{2}^{2}\ =\ \min_{\mathbf{\alpha}\in\mathbb{R}^{p}}\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^ {2}\,.\]
5. Show that \(\operatorname{rank}[X]\geq n\) is nevertheless a sufficient condition for the existence of a least-squares solution with \((\widehat{\mathbf{\beta}}_{\text{Is}})^{\prime}_{p+1}\neq 0\).

This proves that both \(\langle\mathbf{y}-X\widehat{\mathbf{\beta}}_{\text{Is}},\ \mathbf{x}\rangle\neq 0\) (cf. Claims 2 and 3) and \(\operatorname{rank}[X]\geq n\) (cf. Claim 5) are sufficient conditions for a least-squares estimator assigning non-zero weight to the additional predictor \(\mathbf{x}\). Since these conditions seem likely to be satisfied in high-dimensional settings, our results suggest that least-squares estimation typically uses every additionally provided predictor--irrespective of whether that predictor is actually relevant or not.

#### Exercises for \(\mathbf{\triangleright}\) Sect. 2.4

**Exercise 2.2**\(\diamondsuit\diamondsuit\): In this exercise, we review three basic properties of the \(\ell_{q}\)-functions defined on Page IX.

1. Show that for \(q\in[1,\infty]\), the \(\ell_{q}\)-functions are norms on \(\mathbb{R}^{p}\).
2. Show that for \(q\in[0,1)\), the \(\ell_{q}\)-functions are not norms.
3. Show that for \(q,k\in[1,\infty]\) such that \(1/q+1/k=1\), the dual function of \(\mathbf{a}\mapsto\left|\mathbf{a}\right|_{q}\) is \(\mathbf{a}\mapsto\left|\mathbf{a}\right|_{k}\), that is, \(\overline{|\cdot|}_{q}=|\cdot|_{k}\). Conclude that \(\overline{|\cdot|}_{q}\) is a norm on \(\mathbb{R}^{p}\).

**Exercise 2.3**\(\blacklozenge\blacklozenge\) In this exercise, we establish some properties of dual functions. According to Definition 2.4.1, the dual function \(\overline{\mathcal{A}}:\mathbb{R}^{p}\to[-\infty,\infty]\) of a function \(\mathcal{A}:\mathbb{R}^{p}\to[-\infty,\infty]\) is defined via

\[\overline{\mathcal{A}}[\boldsymbol{a}]\,=\,\sup\left\{\,\langle\boldsymbol{a},\,\boldsymbol{c}\rangle\,:\,\boldsymbol{c}\in\mathbb{R}^{p},\,\,\mathcal{A}[ \boldsymbol{c}]\leq 1\right\}\qquad\text{for all }\boldsymbol{a}\in\mathbb{R}^{p}\,,\]

where the supremum over the empty set is defined as \(-\infty\).

1. (Triangle Inequality) Show that \[\overline{\mathcal{A}}[\boldsymbol{a}+\boldsymbol{b}]\,\leq\,\overline{ \mathcal{A}}[\boldsymbol{a}]+\overline{\mathcal{A}}[\boldsymbol{b}]\qquad\quad \text{for all }\boldsymbol{a},\,\boldsymbol{b}\in\mathbb{R}^{p}\,.\]
2. (Absolute Semi-Homogeneity) Show that \[\overline{\mathcal{A}}[\boldsymbol{a}\boldsymbol{b}]\,=\,|a|\overline{ \mathcal{A}}\big{[}\operatorname{sign}[a]\boldsymbol{b}\big{]}\qquad\text{ for all }a\in\mathbb{R},\,\,\boldsymbol{b}\in\mathbb{R}^{p}\,.\] This implies in particular that \(\overline{\mathcal{A}}[\boldsymbol{0}_{p}]\,=\,0\) (set \(a=0\) and keep in mind the convention \(0\cdot(\pm\infty)\,=\,0\)--see Page X).
3. (Absolute Homogeneity) Show that if \(\mathcal{A}[\boldsymbol{a}\boldsymbol{b}]=|a|\mathcal{A}[\boldsymbol{b}]\) for all \(a\in\mathbb{R}\), \(\boldsymbol{b}\in\mathbb{R}^{p}\), then also \[\overline{\mathcal{A}}[\boldsymbol{a}\boldsymbol{b}]\,=\,|a|\overline{ \mathcal{A}}\big{[}\boldsymbol{b}\big{]}\qquad\qquad\text{ for all }a\in\mathbb{R},\,\,\boldsymbol{b}\in\mathbb{R}^{p}\,.\]
4. (Non-negativity) Show that if \(\mathcal{A}[\boldsymbol{0}_{p}]\leq 1\), then \[\overline{\mathcal{A}}[\boldsymbol{a}]\,\geq\,0\qquad\qquad\text{ for all }\boldsymbol{a}\in\mathbb{R}^{p}\,.\]
5. (Symmetry) Show that if \(\mathcal{A}[\boldsymbol{a}]=\mathcal{A}[-\boldsymbol{a}]\) for all \(\boldsymbol{a}\in\mathbb{R}^{p}\), then also \[\overline{\mathcal{A}}[\boldsymbol{a}]\,=\,\overline{\mathcal{A}}[- \boldsymbol{a}]\qquad\quad\text{ for all }\boldsymbol{a}\in\mathbb{R}^{p}\,.\]
6. (Norms) Show that if \(\mathcal{A}\) is a norm, then \(\overline{\mathcal{A}}\) is also a norm.
7. (Convexity) Show that \[\overline{\mathcal{A}}\big{[}w\boldsymbol{a}+(1-w)\boldsymbol{b}\big{]}\, \leq\,w\overline{\mathcal{A}}[\boldsymbol{a}]+(1-w)\overline{\mathcal{A}}[ \boldsymbol{b}]\\ \text{for all }\boldsymbol{a},\,\boldsymbol{b}\in\mathbb{R}^{p},\,\,w\in[0,1]\,.\]
8. Bonus: Give an example function \(\mathcal{A}\) for which \(\overline{\mathcal{A}}=\mathcal{A}\) (that is, the dual function of the dual function is the function itself) and an example function for which this does not hold.

9. Bonus: Give an example function \(\mathcal{A}\) that is not a norm but whose dual function is a norm.

This exercise shows especially that dual functions inherit many properties from their basis functions and are equipped with some other properties "automatically."

**Exercise 2.4**: \(\Diamond\Diamond\) In this example, we study the Holder inequality in Theorem 2.4.1 for typical \(\mathcal{A}\). Give the specific forms (including \(\overline{\mathcal{A}}\)) of Inequality (2.5) for the following functions \(\mathcal{A}\)--or argue why Theorem 2.4.1 does not apply.

1. Best-subset selection prior function: \[\mathcal{A}\ :\ \boldsymbol{a}\ \mapsto\ \|\boldsymbol{a}\|_{0}\,.\]
2. Group-lasso prior function without overlap: \[\mathcal{A}\ :\ \boldsymbol{a}\ \mapsto\ \sum_{j=1}^{k}\|\boldsymbol{a}_{ \mathcal{A}j}\|_{2}\,,\] where \(\mathcal{A}_{1},\ldots,\mathcal{A}_{k}\) is a partition of \(\{1,\ldots,p\}\), that is, \(\cup_{j=1}^{k}\mathcal{A}_{j}=\{1,\ldots,p\}\), \(\mathcal{A}_{j}\neq\varnothing\) for all \(j\), and \(\mathcal{A}_{i}\cap\mathcal{A}_{j}=\varnothing\) for all \(i\neq j\).
3. The group-lasso prior function with overlap: \[\mathcal{A}\ :\ \boldsymbol{a}\ \mapsto\ \sum_{j=1}^{k}\|\boldsymbol{a}_{ \mathcal{A}j}\|_{2}\,,\] where \(\mathcal{A}_{1},\ldots,\mathcal{A}_{k}\subset\{1,\ldots,p\}\) such that \(\cup_{j=1}^{k}\mathcal{A}_{j}=\{1,\ldots,p\}\) and \(\mathcal{A}_{i}\cap\mathcal{A}_{j}\neq\varnothing\) for a pair \((i,j)\), \(i\neq j\).
4. The incomplete group-lasso prior function without overlap: \[\mathcal{A}\ :\ \boldsymbol{a}\ \mapsto\ \sum_{j=1}^{k}\|\boldsymbol{a}_{ \mathcal{A}j}\|_{2}\,,\] where \(\mathcal{A}_{1},\ldots,\mathcal{A}_{k}\subset\{1,\ldots,p\}\) such that \(\cup_{j=1}^{k}\mathcal{A}_{j}\neq\{1,\ldots,p\}\) and \(\mathcal{A}_{i}\cap\mathcal{A}_{j}=\varnothing\) for all \(i\neq j\).
5. The non-negativity constraint: \[\mathcal{A}\ :\ \boldsymbol{a}\mapsto\ \infty\cdot\mathbbm{1}\{a_{1},\ldots,a_{p}<0\}\,.\]

[MISSING_PAGE_FAIL:80]

1. Compute the local minima of \(\ell\), \(\wp\), and \(\lambda\). How many minima are there for each of these three functions? Hint: You may use the fact that if \(\lambda:\mathbb{R}\to\mathbb{R}\) is differentiable, (i) \(\lambda^{\prime}[a]=0\) for every local minimum \(a\in\mathbb{R}\), and (ii) \(\lambda^{\prime}[a]=0\) and \(\lambda^{\prime\prime}[a]>0\) together imply that \(a\in\mathbb{R}\) is a local minimum.
2. Compute the global minima of \(\ell\), \(\wp\), and \(\lambda\).
3. More generally, show that every local minimum of a convex function \(\lambda:\mathbb{R}^{p}\to\mathbb{R}\) is also a global minimum of \(\lambda\).
4. Show also that every strictly convex function \(\lambda:\mathbb{R}^{p}\to\mathbb{R}\) has at most one local and one global minimum, and show that--if they exist--these two points coincide. Bonus: Can you state a strictly convex function \(\lambda:\mathbb{R}^{p}\to\mathbb{R}\) that has neither a local nor a global minimum?
5. Conclude from 1. and 2. that neither of the previous two statements is necessarily true for non-convex functions.

**Exercise 2.7**: \(\diamondsuit\) In this exercise, we study the convexity of some prior functions.

Prove or refute that the convexity of the following functions \(\lambda:\mathbb{R}^{p}\to\mathbb{R}\) defined as:

1. \(\lambda:a\mapsto\|a\|_{0}\);
2. \(a\mapsto|a_{j}|\) for \(j\in\{1,\ldots,p\}\);
3. \(\lambda:a\mapsto\|a\|_{1}\);
4. \(\lambda:a\mapsto\min\{a_{j},b\}\) for \(j\in\{1,\ldots,p\}\) and \(b\in(0,\infty)\).

Hint: For 4., you can use the fact that \(\min\{a,b\}=(a+b)/2-|a-b|/2\) for all \(a\), \(b\in\mathbb{R}\).

**Exercise 2.8**: \(\diamondsuit\) In this exercise, we extend Lemma 2.5.3 about the subdifferentials of combinations of functions. For a given index set \(\lambda\subset\{1,\ldots,p\}\), consider two convex functions \(\lambda_{1}:\mathbb{R}^{|\lambda|}\to\mathbb{R}\) and \(\lambda_{2}:\mathbb{R}^{|\lambda^{\prime\prime}|}\to\mathbb{R}\) and their combination \(\lambda:\mathbb{R}^{p}\to\mathbb{R}\) defined as \(\lambda[a]:=\lambda_{1}[a_{\lambda\lambda}]+\lambda_{2}[a_{\lambda^{\prime} \lambda}]\) for \(a\in\mathbb{R}^{p}\). Use Lemma 2.5.3 to show that

\[\lambda\lambda[a]=\left\{\lambda\in\mathbb{R}^{p}\,:\,\lambda_{\lambda^{\prime \prime}}\in\lambda\lambda_{1}[a_{\lambda^{\prime}\lambda}]\text{ and }\lambda_{ \lambda^{\prime\prime}\lambda}\in\lambda\lambda_{2}[a_{\lambda^{\prime} \lambda}]\right\}.\]

**Exercise 2.9**: \(\diamondsuit\) In this exercise, we corroborate the plots in Example 2.5.2 (subdifferentials of the lasso estimator).

Consider the lasso estimator's objective function

\[\ell_{\rm lasso}\;:\;\mathbf{a}\;\mapsto\;\|\mathbf{y}-X\mathbf{a}\|_{2}^{2}+r\|\mathbf{a}\|_{1}\]

in \(p=2\) dimensions. Compute \(\mathbf{\partial}\ell_{\rm lasso}[\mathbf{a}]\) as a function of \(\mathbf{y}\), \(X\), \(r\), \(\mathbf{a}\) for the following three cases:

1. \(a_{1}\), \(a_{2}\neq 0\);
2. \(a_{1}=0\), \(a_{2}\neq 0\);
3. \(a_{1}=a_{2}=0\).

Compare the Plots A, B, and C in the example.

**Exercise 2.10**: \(\mathbf{\phi}\mathbf{\phi}\) In this exercise, we compute the lasso estimator (2.2) for orthogonal design.

Prove the following three results under the assumption of orthogonality, that is, \(X^{\top}X=nI_{p\times p}\). You may use the KKT conditions for the lasso established in Example 2.5.3.

1. Show that the KKT conditions are in the case of orthogonal design \[\widehat{\mathbf{\beta}}_{\rm lasso}\;=\;\frac{X^{\top}\mathbf{y}}{n}-\frac{r\widehat{ \mathbf{\kappa}}}{2n}\] for a vector \(\widehat{\mathbf{\kappa}}\in\mathbf{\partial}\|\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}\).
2. Use 1. to show that for all \(j\in\{1,\ldots,p\}\), it holds that \[\big{|}(X^{\top}\mathbf{y})_{j}\big{|}\;\leq\;\frac{r}{2} \Leftrightarrow (\widehat{\beta}_{\rm lasso})_{j}\,=\;0\,;\] \[(X^{\top}\mathbf{y})_{j}\;>\;\frac{r}{2} \Leftrightarrow (\widehat{\beta}_{\rm lasso})_{j}\,>\;0\,;\] \[(X^{\top}\mathbf{y})_{j}\;<\;-\;\frac{r}{2} \Leftrightarrow (\widehat{\beta}_{\rm lasso})_{j}\,<\;0\,.\]
3. Use 1. and 2. to show that for all \(j\in\{1,\ldots,p\}\), it holds that \[(\widehat{\beta}_{\rm lasso})_{j}\;=\;{\rm sign}\,\big{[}(X^{\top}\mathbf{y})_{j} \big{]}\bigg{(}\frac{|(X^{\top}\mathbf{y})_{j}|}{n}-\frac{r}{2n}\bigg{)}_{+}\,,\] (2.8) where the _positive part_\((a)_{+}\) of a number \(a\in\mathbb{R}\) is defined as \((a)_{+}:=a\) if \(a>0\) and \((a)_{+}:=0\) otherwise. This means that the lasso problem has a unique and explicit solution.

We conclude that for orthogonal design, the coordinates of the lasso estimator are \((\widehat{\beta}_{\rm lasso})_{j}=f_{r/2}[(X^{\top}\mathbf{y})_{j}]/n,j\in\{1,\ldots,p\}\), where \(f_{c}\) is the _soft-thresholding operator_

\[f_{c}\ :\ \mathbb{R} \to\ \mathbb{R}\] \[x \mapsto\ \operatorname{sign}[x]\big{(}|x|-c\big{)}_{+}\]

for a given cutoff \(c\in[0,\infty]\). Hence, in the orthogonal case, the lasso estimator is a soft-thresholded version of the least-squares estimator \(\widehat{\mathbf{\beta}}_{\rm ls}=(X^{\top}X)^{-1}X^{\top}\mathbf{y}=X^{\top}\mathbf{y}\).

**Exercise 2.11**: \(\blacklozenge\blacklozenge\) In this exercise, we derive further properties of the lasso estimator

\[\widehat{\mathbf{\beta}}_{\rm lasso}[r]\ \in\ \operatorname*{arg\,min}_{\mathbf{\alpha} \in\mathbb{R}^{p}}\big{\{}\,|\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}+r\|\mathbf{\alpha}\|_{1} \,\big{\}}\]

for certain tuning parameters \(r\).

First, we establish another relationship between the lasso and the least-squares estimator. We can use again the lasso's KKT conditions from Example 2.5.3.

1. Show that if the Gram matrix \(X^{\top}X\) is invertible and \(r\in[0,\infty)\), the lasso estimator's KKT conditions are \[\widehat{\mathbf{\beta}}_{\rm lasso}[r]\ =\ \widehat{\mathbf{\beta}}_{\rm ls}-\frac{r}{2}(X^{\top}X)^{-1}\widehat{\mathbf{\kappa}}\] for a \(\widehat{\mathbf{\kappa}}\in\mathbf{\mathfrak{a}}|\widehat{\mathbf{\beta}}_{\rm lasso}[r] \|_{1}\).
2. Show that then \[|X\widehat{\mathbf{\beta}}_{\rm lasso}[r]-X\widehat{\mathbf{\beta}}_{\rm ls}|_{2}^{2} \ \leq\ \frac{r^{2}p}{4m_{1}}\,,\] where \(m_{1}\) is the smallest eigenvalue of the Gram matrix \(X^{\top}X\) (see \(\blacktriangleright\) Sect. B.2).

In view of Eq. (1.5), this means that if the lasso's tuning parameter is sufficiently small, say \(r\ll 2\sqrt{m_{1}}\sigma\), the lasso and the least-squares estimator are about equal in terms of prediction.

Second, we identify--now for arbitrary \(X\)--the range of tuning parameters that set the lasso to zero.

1. Show that \(\widehat{\mathbf{\beta}}_{\rm lasso}[r]=\mathbf{0}_{p}\) is a lasso solution if and only if \(r\geq 2|X^{\top}\mathbf{y}|_{\infty}\).
2. Show that \(\widehat{\mathbf{\beta}}_{\rm lasso}[r]=\mathbf{0}_{p}\) is the _unique_ lasso solution if \(r\geq 2|X^{\top}\mathbf{y}|_{\infty}\)_and_\(r>0\).

[MISSING_PAGE_EMPTY:6303]

Set the degree of the polynomial to \(d=20\), and generate \(n=100\) independent samples according to the above model. The function runif() might be helpful to sample from the uniform distribution, and the function rnorm() to sample from the Gauss distribution. Summarize the outcomes in a vector \(\boldsymbol{y}:=(y_{1},\ldots,y_{n})^{\top}\in\mathbb{R}^{n}\) and the predictors in a matrix \(X\in\mathbb{R}^{n\times(d+1)}\) with coordinates \(X_{ij}=(x_{i})^{j}\). Including the intercept, the number of predictors is \(p=d+1\).

set.seed(87) PolynomialDesign <- function(x.vector, d) { REPLACE } n <- 100; d <- 20 X <- PolynomialDesign(REPLACE, d) regression.vector <- c(0, 0, 2, rep(0, d - 2)) y <- REPLACE cbind(y, X)[1:4, 1:5]

## [,1] [,2] [,3] [,4] [,5]
1,1] 1.319628 1 -0.9678621 0.9367571 -0.90665165 ## [2,1] 1.839104 1 -0.4171889 0.1740466 -0.07261032 ## [3,1] 3.089433 1 -0.9150774 0.8373667 -0.76625536 ## [4,1] 1.657062 1 0.7153142 0.5116745 0.36600803

#### Implementing the Estimators

Implement a least-squares estimator and a lasso estimator. For the latter, use the cv.glmnet() function from the glmnet package with the flag intercept=FALSE. The coef() function might be helpful in extracting the estimated regression vector from the glmnet object (note that the first coordinate of the resulting vector needs to be removed in our case, since that coordinate contains a placeholder for an additional intercept).

library(glmnet) set.seed(98) # glmnet uses randomized cross-validation routines LaEttinator <- function(y, X) { return(REPLACE) } LassoEstimator <- Function(y, X) { return(REPLACE) } chind(LaEttinator(y, X[,1:1]), LassoEstimator(y, X[,1:2])) # a quick check 

[MISSING_PAGE_FAIL:86]

[MISSING_PAGE_FAIL:87]

[MISSING_PAGE_EMPTY:6307]

of \(\blacktriangleright\) Sect. 1.2, which state a linear increase for the least-squares and only a logarithmic increase for the lasso. The data-fitting error above and the prediction error here are closely related to training and validation errors, which we will discuss in the context of cross-validation in \(\blacktriangleright\) Chap. 4.

The estimation errors have even stronger trends than the prediction errors (note the log-scaling of the y-axis).

The lasso estimator always picks only one predictor, while the least-squares estimator picks all available ones. Ambiguities among those predictors can make the least-squares numerically and statistically instable. Since the polynomial layout generates strong dependencies among the predictors, these instabilities are expressed particularly strongly in the estimation errors.

### Notes and References

1 More formally: a linear regression model is a combination of a probability space \((\mathcal{A},\mathfrak{A},\mathbb{P})\), random quantities \(\boldsymbol{y}:(\mathcal{A},\mathfrak{A})\rightarrow(\mathbb{R}^{n},\mathcal{B}^ {n})\), \(X:(\mathcal{A},\mathfrak{A})\rightarrow(\mathbb{R}^{n\times p},\mathcal{B}^{n \times p})\), and \(\boldsymbol{u}:(\mathcal{A},\mathfrak{A})\rightarrow(\mathbb{R}^{n},\mathcal{B }^{n})\), where \(\mathcal{B}^{n},\mathcal{B}^{n\times p}\) denote the appropriate Borel \(\sigma\)-algebras, and a fixed vector \(\boldsymbol{\beta}\in\mathbb{R}^{p}\) such that \(\mathbb{P}[\boldsymbol{y}-X\boldsymbol{\beta}-\boldsymbol{u}=\boldsymbol{0}_{ n}]=1\).
2 A sufficient condition for (scaled) orthogonal design is that \(X/\sqrt{n}\) is an orthogonal matrix--see Definition B.2.8. But this condition requires \(p=n\), which we do not assume in general.
3 Frank and Friedman (1993, Equation (54) on p. 124) introduce least-squares regression with general \(\ell_{q}\)-prior functions, \(q\in[0,\infty]\); these estimators were later termed _bridge estimators_. The paper also motivates these priors from a Bayes perspective; we have adopted this viewpoint in Exercise 1.3. Bridge estimators with \(q\in[1,2]\) can be seen as interpolations between the lasso and the ridge; however, in contrast to the elastic net estimator, which is another such interpolation introduced in the main text, these estimators are sparse only if \(q=1\)--see \(\blacksquare\) Fig. 2.2.
4 The lasso has been introduced in Tibshirani (1996). Its fast penetration into the sciences was also due to the rapid development of corresponding algorithms, such as in Osborne et al. (2000) and Efron et al. (2004).
5 Least-squares regression with a grouped \(\ell_{1}\)-prior was first considered in Bakin (1999, Equation (2.7) on p. 22). Yuan and Lin (2006, Equation (2.1) on p. 51) introduce the more general prior function \(\sum_{j=1}^{k}\sqrt{\boldsymbol{\alpha}_{\frac{1}{\sigma_{dj}^{2}}}^{T}K_{j} \boldsymbol{\alpha}_{dj}}\), where \(K_{j}\in\mathbb{R}^{|d_{j}|\times|d_{j}|}\), and those authors coined the term group-lasso estimator for least-squares estimators with such priors. To facilitate the calibration of the tuning parameter \(r\), Bunea et al. (2014, Definition (4) on p. 1314) combined the group lasso with the square-root lasso (6.2) to the _square-root group-lasso estimator_

\[\widehat{\boldsymbol{\beta}}_{\sqrt{\text{group}}}\,\in\,\operatorname*{arg\, min}_{\boldsymbol{\alpha}\in\mathbb{R}^{p}}\,\left\{\,|\boldsymbol{y}-X\boldsymbol{ \alpha}|_{2}+r\sum_{j=1}^{k}\sqrt{\boldsymbol{\alpha}_{\frac{1}{\sigma_{dj}^{2 }}}^{T}K_{j}\boldsymbol{\alpha}_{dj}}\,\right\}\,. \tag{2.9}\]

Both Yuan and Lin (2006) and Bunea et al. (2014) suggest \(K_{j}:=|\mathcal{A}_{j}||_{\mathcal{A}_{j}^{2}\times|d_{j}|}\) if the predictors are orthogonal within groups (in fact, the predictors can be orthogonalized within groups without loss of generality). The factor \(|\mathcal{A}_{j}|\) balances the selection of small and large groups and can be motivated by empirical process theory: van de Geer and Buhlmann (2011, Lemma 8.1 on p. 254) and Bunea et al. (2014, Lemma 2.1 on p. 1316) show for the original case and for the square-root case, respectively, that the mentioned choice of \(K_{j}\) can allow for a suitable regularization of all groups simultaneously if the noise is i.i.d. Gauss-distributed. The _sparse group-lasso estimator_ of Simon et al. (2013, Equation (3) on p. 232) complements the group regularization with an additional \(\ell_{1}\)-prior to obtain sparsity also within groups. Finally, overlapping groups have been studied in Obozinski et al. (2011) and others.
* Huang et al. (2019) have studied the estimator \[\widehat{\boldsymbol{\beta}}_{\text{edr}}\in\operatorname*{arg\,min}_{ \boldsymbol{\alpha}\in\mathbb{R}^{p}}\big{\{}\,|\boldsymbol{y}-X\boldsymbol{ \alpha}|_{2}^{2}+r|\boldsymbol{\alpha}|_{2}\big{\}}\] and termed it _Euclidean distance ridge (edr) estimator_. For every \(r\in[0,\infty]\), there is a (data-dependent) tuning parameter \(r^{\prime}\) such that \(r|\boldsymbol{\alpha}|_{2}^{2}\)- and \(r^{\prime}|\boldsymbol{\alpha}|_{2}\)-regularization are equivalent, and, for every \(r\in[0,\infty]\), there is a tuning parameter \(r^{\prime}\) such that \(r|\boldsymbol{\alpha}|_{2}\)- and \(r^{\prime}|\boldsymbol{\alpha}|_{2}^{2}\)-regularization are equivalent (Huang et al., 2019, Chapter 3). Hence, swapping the \(\ell_{2}^{2}\)-prior for the \(\ell_{2}\)-functions is essentially a change of tuning.
* The elastic net was introduced in Zou and Hastie (2005, Equation (3) on p. 303).
* Lemma 2 on p. 306 in Zou and Hastie (2005) supports our earlier statement about the different selection tendencies of the ridge estimator/elastic net and the lasso: On the one hand, every least-squares estimator with _strictly_ convex prior terms (such as the elastic net with \(w\in[0,1)\)) assigns the same coordinate estimates \(\widehat{\beta_{i}}=\widehat{\beta_{j}}\) to two equal predictors \(\boldsymbol{x}_{i}[i]=\boldsymbol{x}_{j}\); on the other hand, there is always one solution of the lasso estimator that sets one of those coordinate estimates to zero. Thus, the elastic net selects or disregards perfectly correlated predictors as a group, while the lasso selects at most one representative among such predictors. More generally, Theorem 1 in that paper shows that two coordinate estimates of the elastic net converge as (i) the corresponding predictors become more correlated or (ii) the tuning parameter \((1-w)r\) increases. Further observations in this direction are made in Bien and Wegkamp (2013).
* Least-squares refitting for the _lars estimator_, a sibling of the lasso estimator, has been proposed by Efron et al. (2004, p. 421) under the name _lars-ols hybrid_. Meinshausen (2007, Definition 1 on p. 376) introduced the _relaxed lasso_: a lasso on the support of an initial lasso, where the tuning parameter for the second stage lasso is chosen smaller than the one for the first stage.
* Settings where the standard least-squares refitting described in our text increases the lasso's accuracy are described in Belloni and Chernozhukov (2013); settings where least-squares refitting decreases the lasso's accuracy are described in Lederer (2013). As a rule of thumb, least-squares refitting can render estimations even more accurate in "easy" settings, but it can add considerable error in "difficult" settings.
* A weaker version of the strong oracle property is the _weak oracle property_, which states that an estimator has (approximately) the same accuracy as the oracle estimator. In the case of linear regression models with Gauss-distributed noise, this latter property basically refers to bounds that do not involve the lasso's \(\log p\)-terms. Still, the requirements for those inequalities are usually strict and unverifiable in practice.

12. The scad (smoothly clipped absolute deviation) regularizer was introduced in Fan and Li (2001, Display (2.7) on p. 1350); the mcp (minimax concave penalty) in Zhang (2010, Display (2.2) on p. 897).
13. As two examples among many such results, Zhang and Zhang (2012, Theorem 1 on p. 584) and Kim et al. (2008, Theorem 3 on p. 1668) provide sufficient conditions for such estimators satisfying weak and strong oracle properties, respectively. However, these conditions are strict: For example, they imply \(p\leq n\) in the latter result (see their subsequent remark).
14. Note that \(\ell_{q}\)-regularization, \(q\in(0,1)\), also renders least-squares estimation non-convex, but it is otherwise very different from capped \(\ell_{1}\)-regularization: First, \(\ell_{q}\)-priors change the regularization landscape of the entire _vector_ of parameters, while the capped \(\ell_{1}\)-prior consists of sums over regularizers for each individual _coordinate_. Second, capped \(\ell_{1}\)-norms are bounded, while \(\ell_{q}\)-functions are absolute homogenous, that is, linearly increasing in their function argument; this means in particular that \(\ell_{q}\)-regularization (without additional steps such as refitting) does not satisfy the strong oracle property. Third, the (sub-)gradients of the capped \(\ell_{1}\)-norms are bounded everywhere, while the (sub-)gradients of \(\ell_{q}\)-functions go to infinity as the function argument goes to zero; this divergence at zero can lead to numerical instabilities.
15. In fact, it is a "signed" distance: \(\overline{c}\) is negative if the hyperplane is on the side of the origin opposite to \(\mathbf{a}\). Another subtlety is that \(\overline{c}\) might not exist (this is why the definition has a supremum rather than a maximum), but one can always approximate it to an arbitrary precision. For details, we refer to Hiriart-Urruty and Lemarechal (2004, Section C.2.1 on pp. 134ff): This contains an illustration for _support functions_, for which our dual functions are a special case.
16. The reverse statement is not true: \(\hat{\mathbf{a}}:\mathbf{a}\mapsto 1\) is scalable but not absolute homogenous.
17. The _graph of a function_\(\ell:\mathbb{R}^{p}\to\mathbb{R}\) is the set \(\{(\mathbf{a},\ell[\mathbf{a}])\ :\ \mathbf{a}\in\mathbb{R}^{p}\}\). It should not be confused with the graphs that we discuss in Chap. 3 on graphical models.
18. The _closed line segment_ between two points \(\mathbf{a}\), \(\mathbf{b}\in\mathbb{R}^{p}\) is the set of points \(\{w\mathbf{a}+(1-w)\mathbf{b}\in\mathbb{R}^{p}:w\in[0,1]\}\). The _open line segment_ between two points \(\mathbf{a},\mathbf{b}\in\mathbb{R}^{p}\), \(\mathbf{a}\neq\mathbf{b}\), is the set of points \(\{w\mathbf{a}+(1-w)\mathbf{b}\in\mathbb{R}^{p}:w\in(0,1)\}\).
19. Similarly, we call a function \(\ell\)_(strictly) concave_ if \(-\ell\) is (strictly) convex.
20. See Hiriart-Urruty and Lemarechal (2004, Section VI.1.4 on pp. 247ff) for different approaches to proving the existence of a subgradient.
21. See Hiriart-Urruty and Lemarechal (2004, Theorem 4.1 on p. 183), for example. The result is sometimes referred to as _Moreau-Rockafellar theorem_.
22. The name KKT refers to the authors of Karush (1939) and Kuhn and Tucker (1951).

## Graphical Models

### Contents

The beginning of this book illustrates that linear regression models can describe the relationships between the genes' copy numbers and a biomarker. However, those models do not provide information about the relationships among the copy numbers themselves. To describe such relationships, we use a different type of models, called graphical models. We neglect the biomarker and summarize the measured copy numbers in vector-valued observations, where each vector corresponds to a specific subject and each coordinate of these vectors to a specific gene (in the linear regression model, these vectors are the rows of the design matrix). Graphical models then formulate the relationships among the copy numbers as conditional dependence networks among the coordinates of the vector-valued observations. If the observations follow a multivariate Gauss distribution, we speak of Gaussian graphical models. This is the most common class of graphical models and, therefore, the focus of this chapter.

### 3.1 Overview

Natural and artificial phenomena often consist of many parts that depend on each other in a complex manner. Consider biology, for example: cells feature hundreds of biochemical processes that affect each other through various metabolic pathways; the human gut accommodates thousands of different species that feed off each other or interact in other ways; the human brain contains several billions of neurons that communicate through trillions of synapses. Our goal is to develop mathematical models for those network structures to unravel the underpinning biological, physical, or mechanical principles.

There are a number of different types of mathematical network models, among them _differential equations, Boolean networks,_ and _graphical models._1 In recent years, graphical models have become the dominant type in a number of fields, because they summarize dependence structures in comprehensive, yet concise graphical representations that lend themselves naturally to scientific interpretation.2 The basic concept of graphical models is _conditional dependence_. Consider the two events bike= "you bike to work" and dog = "neighbor walks dog". The events bike and dog are not independent, as sunshine might make you feel tempted to use your bike as well 

[MISSING_PAGE_EMPTY:6314]

connections _edges_, and a pair \(\mathcal{F}=(\mathcal{F},\mathcal{F})\) of a set of nodes \(\mathcal{I}\) and a corresponding set of edges \(\mathcal{E}\) a _graph_. The graph on the left-hand side of \(\blacksquare\) Fig. 3.1 describes the dependence relation between bike and dog: the nodes are bike and dog, and the edge is between bike and dog. The graph on the right-hand side of \(\blacksquare\) Fig. 3.1 describes the dependence relation between bike, dog, and sun: the nodes are bike, dog, and sun, and the edges are between bike and sun and between dog and sun.

In more realistic examples, the nodes are not necessarily simple events but rather real-valued random variables such as the expression levels of genes or the activity levels of neurons. The edges then describe the (conditional) dependencies among those random variables. We denote the random variables by \(z_{1},\ldots,z_{d}\) and summarize them in the vector \(\boldsymbol{z}:=(z_{1},\ldots,z_{d})^{\top}\in\mathbb{R}^{d}\). Now, a pair \((\boldsymbol{z},\mathcal{F})\) of such a random vector \(\boldsymbol{z}\) and a graph \(\mathcal{F}\) that encapsulates the _conditional_ dependence relationships of the random variables that form the vector is called a _graphical model_.4 For ease of notation, we identify the nodes with the indexes of the random variables: \(\mathcal{I}=\{1,\ldots,p\}\); accordingly, we denote an edge between \(z_{i}\) and \(z_{j}\) by the unordered pair \((i,j)\), where unordered means that \((i,j)\) and \((j,i)\) are the same edge.

In practice, we do not know the conditional dependence relationships among the variables. Consequently, our goal for this chapter is to derive the corresponding graph from data. Such data consist of \(n\) snapshots of the network's state, that is, \(n\) realizations \(\boldsymbol{z}_{1},\ldots,\boldsymbol{z}_{n}\in\mathbb{R}^{d}\) of the random vector \(\boldsymbol{z}\). For example, if the graphical model \((\boldsymbol{z},\mathcal{F})\) concerns the genes' coactivation network in a certain type of tissue, then \(\boldsymbol{z}_{i}\) could comprise the expression levels of the \(d\) genes under consideration in the \(i\)th tissue sample. In contrast to linear regression, data for graphical models do not contain separate outcomes \(\boldsymbol{y}\) and designs \(X\); instead, the data vectors \(\boldsymbol{z}_{1},\ldots,\boldsymbol{z}_{n}\) are now both outcome and design simultaneously--see especially \(\blacktriangleright\) Sect. 3.4. This double role also increases the model complexities: In linear regression, the number of model parameters equals the number of predictors; in graphical modeling, the number of model parameters equals the number of potential connections among the nodes and, therefore, quadratic in the number of nodes. More precisely, the number of potential connections, that is, the maximal size of the edge set, is \(d(d-1)/2\); accordingly, a graphical model is low dimensional only if \(d(d-1)/2\ll n\) and high-dimensional whenever \(d(d-1)/2\approx n\) or even \(d(d-1)/2\gg n\).

We focus on the most popular type of graphical models: Gaussian graphical models (\(\blacktriangleright\) Sect. 3.2). The name indicates that the observations are from a Gauss distribution. We introduce two approaches to learning the corresponding graph: Maximum likelihood (\(\blacktriangleright\) Sect. 3.3) and neighborhood selection (\(\blacktriangleright\) Sect. 3.4). The two approaches coincide in low dimensions (Exercise 3.4), but they differ in how they include prior information (regularizing all edges collectively versus regularizing each node's neighborhood individually).

### Gaussian Graphical Models

_Gaussian graphical models_ are graphical models whose random vectors follow a multivariate Gauss distribution. Gaussian graphical models are the most popular type of graphical models, because they are especially easy to handle; in particular, their conditional and unconditional dependence structures are fully determined by the inverse of the covariance matrix of the distribution at hand, and the estimation of the covariance matrix is amenable to standard methodology such as maximum (regularized) likelihood. Hence, observations that are not Gauss-distributed, such as copy number counts of genes, are often transformed to fit a Gauss distribution at least approximately.5

Footnote 5: The covariance matrix of the covariance matrix is a Gaussian graphical model

In mathematical terms, a _Gaussian graphical model_ is a pair \((\boldsymbol{z},\mathcal{F})\) that consists of a Gauss vector \(\boldsymbol{z}\sim\mathcal{N}_{d}[\boldsymbol{\mu},\Sigma]\) for an arbitrary mean vector \(\boldsymbol{\mu}\in\mathbb{R}^{d}\) and a symmetric and positive definite _covariance matrix_\(\Sigma\in\mathbb{R}^{d\times d}\) and of a corresponding conditional independence graph \(\mathcal{F}\). In such models, the graph \(\mathcal{F}\) and the inverse of the covariance matrix, the _precision matrix_\(\Theta:=\Sigma^{-1}\), are intimately related:6

**Hammersley-Clifford, Part I**: Given a Gaussian graphical model (_z_, \(\mathcal{F}\)) with precision matrix \(\Theta\), it holds for all pairs of indexes \(i,j\in\{1,\ldots,d\}\), \(i\neq j\), that

\[z_{i}\perp z_{j}\ \mid\ \boldsymbol{z}_{\{1,\ldots,d\}\setminus\{i,j\}}\quad \Leftrightarrow\quad\Theta_{ij}\,=\,0\,,\]

that is, the edge set of the graph is \(\mathcal{E}=\{(i,j)\in\mathcal{F}\times\mathcal{F}:i\neq j,\Theta_{ij}\neq 0\}\).

The same graph \(\mathcal{F}\) also describes the unconditional dependencies among the coordinates of \(\boldsymbol{z}\):

**Hammersley-Clifford, Part II**: Given a Gaussian graphical model (\(\boldsymbol{z}\), \(\mathcal{F}\)) with precision matrix \(\Theta\), it holds for all pairs of indexes \(i,j\in\{1,\ldots,d\}\), \(i\neq j\), that

\[z_{i}\perp z_{j}\quad\Leftrightarrow\]

\[\nexists k\in\{1,2,\ldots\},\,l_{1},\ldots,l_{k}\in\{1,\ldots,d\}\,:\,\Theta _{il_{1}l_{2}}\ldots\Theta_{il_{d}j}\neq 0\,,\]

that is, two coordinates \(z_{i}\) and \(z_{j}\) are independent if and only if there is no path of edges (\(i,l_{1}\)), (\(l_{1},l_{2}\)),..., (\(l_{k},j\)) \(\mathcal{E}\) between them.

(We omit the proofs.7) For example, if \(\Theta_{ij}\neq 0\), that is, \((i,j)\in\mathcal{E}\), then \(z_{i}\) and \(z_{j}\) are neither conditional independent (given any set of coordinates) nor unconditionally independent.

The covariance matrix \(\Sigma\) is symmetric and positive definite by definition and also invertible according to 1. in Lemma B.2.5. Hence, also the precision matrix \(\Theta\) is symmetric according to 2. in Lemma B.2.6, positive definite according to 2. in Lemma B.2.5, and invertible according to 1. in Lemma B.2.5.

In summary, the non-zero pattern of the precision matrix captures the entire dependence structure of the Gauss-distributed vector \(\boldsymbol{z}\); \(\blacksquare\) Fig. 3.2 contains an illustration. Hence, our goal is to estimate \(\Theta\) (rather than \(\Sigma\)), and we call the elements of \(\Theta\) the _model parameters_ of Gaussian graphical models.

## 3 Maximum Regularized Likelihood Estimation

Having discussed the concept of Gaussian graphical models, we now turn to estimating its parameters. We first think of the models as parameterized by the coordinates of the inverse covariance matrix, which indeed specifies the entire distribution of the observations. The proposed estimators for these parameters are maximum likelihood in low-dimensional settings and regularized variants of it in high-dimensional settings. We then focus in on the elements of the underpinning edge set, which, in view of the Hammersley-Clifford theorem, specifies the conditional dependence network. Since the edge set corresponds to the non-zero-valued entries of the inverse covariance matrix, it can be estimated by simply thresholding the estimate of the inverse covariance matrix.

The density of a single Gauss vector \(z\sim\mathcal{N}_{d}[\boldsymbol{y}\,,\,\Omega^{-1}]\) is

\[\ell[z]\,=\,\frac{1}{\sqrt{(2\pi)^{d}\det[\Omega^{-1}]}}\,\,e^{-(z-\boldsymbol {y})^{\top}\Omega(z-\boldsymbol{y})/2}\,.\]

(A brief review of determinants can be found in \(\blacktriangleright\) Sect. B.2 in the Appendix.) The density of \(n\) independent random vectors \(z_{1},\ldots,z_{n}\sim\mathcal{N}_{d}[\boldsymbol{y}\,,\,\Omega^{-1}]\) is, therefore,

\[\ell[z_{1},\ldots,z_{n}]\\ =\prod_{i=1}^{n}\frac{1}{\sqrt{(2\pi)^{d}\det[\Omega^{-1}]}}\,\, e^{-(z_{i}-\boldsymbol{y})^{\top}\Omega(z_{i}-\boldsymbol{y})/2}\,.\]

Figure 3.2: The right panel contains an example of a precision matrix \(\Theta\) of a Gauss-distributed vector \(z\). Since \(\Theta_{14}=0\) but \(\Theta_{12}\Theta_{24}\neq 0\), the coordinates \(z_{1}\) and \(z_{4}\) are conditionally independent given the other coordinates but not unconditionally independent. This is illustrated also in the corresponding dependence graph \(\mathscr{C}\) on the right: \((1,4)\notin\mathscr{C}\) but \((1,2)\), \((2,4)\in\mathscr{C}\)

[MISSING_PAGE_EMPTY:6319]

optimization is over \(\mathbb{R}^{d}\) and \(\mathcal{S}_{d}^{+}\), the set of symmetric and positive definite matrices in \(\mathbb{R}^{d\times d}\):

\[(\widehat{\mathbf{\mu}}_{\text{ml}},\widehat{\Theta}_{\text{ml}})\,\in \operatorname*{arg\,min}_{\mathbf{y}\in\mathbb{R}^{d},\;\Omega\in\mathcal{S}_{d}^{ +}}\biggm{\{}\;\text{trace}\bigg{[}\frac{1}{n}\sum_{i=1}^{n}(\mathbf{z}_{i}-\mathbf{y} )(\mathbf{z}_{i}-\mathbf{y})^{\top}\Omega\bigg{]}\\ -\log[\det[\Omega]]\biggm{\}}\,.\]

For practical and computational convenience, however, we typically transform the data first. One option is to _center_ the data according to \((\mathbf{z}_{i})_{j}\mapsto(\mathbf{z}_{i})_{j}-\sum_{k=1}^{n}(\mathbf{z}_{k})_{j}/n\) for all \(i\in\{1,\ldots,n\}\) and \(j\in\{1,\ldots,d\}\). Centering allows us to work with centered Gauss distributions (\(\mathbf{\mu}=\mathbf{0}_{d}\)) and, therefore, to avoid intercepts. Another option is to _standardize_ the data according to

\[(\mathbf{z}_{i})_{j}\,\mapsto\,\frac{(\mathbf{z}_{i})_{j}-\sum_{k=1}^{n}(\mathbf{z}_{k})_{ j}/n}{\sqrt{\sum_{l=1}^{n}\big{(}(\mathbf{z}_{l})_{j}-\sum_{k=1}^{n}(\mathbf{z}_{k})_{ j}/n\big{)}^{2}/(n-1)}} \tag{3.1}\]

for all \(i\in\{1,\ldots,n\}\) and \(j\in\{1,\ldots,d\}\). Standardizing additionally homogenizes the variances, which can avoid complications in regularizing and computing the estimators. After either transformation, we can fix the true mean vector to \(\mathbf{\mu}=\mathbf{0}_{d}\), which simplifies the maximum likelihood estimator to

\[\widehat{\Theta}_{\text{ml}}\,=\,\operatorname*{arg\,min}_{\Omega\in\mathcal{ S}_{d}^{+}}\biggm{\{}\;\text{trace}\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\mathbf{z}_{i} \mathbf{z}_{i}^{\top}\Omega\bigg{]}-\log\big{[}\det[\Omega]\big{]}\biggm{\}}\,. \tag{3.2}\]

The maximum likelihood estimator can be effective in estimating the inverse covariance matrix in low-dimensional settings, where the number of samples is much larger than the number of model parameters: \(n\gg d(d-1)/2\). In particular, if \(n\geq d\), the maximum likelihood estimator (3.2) exists, is unique, and is equal to

\[\widehat{\Theta}_{\text{ml}}\,=\,\left(\frac{1}{n}\sum_{i=1}^{n}\mathbf{z}_{i}\bm {z}_{i}^{\top}\right)^{-1}, \tag{3.3}\]

all with probability 1--see 5. in Exercise 3.3. The matrix \(\sum_{i=1}^{n}\mathbf{z}_{i}\mathbf{z}_{i}^{\top}/n\) is called the _empirical covariance matrix_.

In high-dimensional settings, the likelihood function is complemented with a prior function. Given such a function \(\hat{\mathcal{h}}:\mathcal{S}^{+}_{d}\to\mathbb{R}\) and a tuning parameter \(r\in[0,\infty)\), this yields the _maximum regularized likelihood estimator_

\[\widehat{\Theta}_{\text{mrl}}\,\in\,\operatorname*{arg\,min}_{ \Omega\in\mathcal{S}^{+}_{d}}\Bigg{\{} \operatorname*{trace}\bigg{[}\frac{1}{n}\sum_{i=1}^{n}z_{i}z_{i}^{ \top}\Omega\bigg{]}\] \[-\log\big{[}\mathrm{det}[\Omega]\big{]}+r\hat{\mathcal{h}}[\Omega] \bigg{\}}\,. \tag{3.4}\]

The most popular example is the _graphical lasso estimator_,8 where \(\hat{\mathcal{h}}[\Omega]=\sum_{i,j=1}^{d}\lvert\Omega_{ij}\rvert\) or \(\hat{\mathcal{h}}[\Omega]=\sum_{\begin{subarray}{c}i,j=1\\ i\neq j\end{subarray}}^{d}\lvert\Omega_{ij}\rvert\). Similarly as the lasso in linear regression, the graphical lasso estimator leverages sparsity, which means for Gaussian graphical models that the precision matrix \(\Omega\) has only a small number of non-zero-valued entries: \(\lvert\{(i,j):\Omega_{ij}\neq 0\}\rvert\ll n,d(d-1)/2\). The graphical lasso estimator does not have a general closed-form solution but still can be computed efficiently--see 2. in Exercise 3.3.

Graph estimates can finally be obtained through _thresholding_ the estimated precision matrix, similarly as in \(\blacktriangleright\) Sect. 2.3 for linear regression. We define \(\widehat{\mathcal{F}}=(\mathcal{F},\widehat{\mathcal{F}})\) from estimates \(\widehat{\Theta}\in\mathbb{R}^{d\times d}\) of the precision matrix, \(\widehat{\Theta}\in\{\widehat{\Theta}_{\text{ml}},\,\widehat{\Theta}_{\text{ mrl}}\}\), through (cf. \(\blacktriangleright\) Sect. 7.4)

\[\widehat{\mathcal{F}}:=\big{\{}(i,j)\in\mathcal{F}\times\mathcal{F}:\,i\neq j, \,\lvert\widehat{\Theta}_{ij}\rvert>c\big{\}}\,, \tag{3.5}\]

where \(c\,\in[0,\infty]\) is a cutoff--cf. (2.4). The discussed estimators \(\widehat{\Theta}_{\text{ml}},\,\widehat{\Theta}_{\text{mrl}}\) are symmetric by construction and, therefore, generate symmetric edge sets: \((i,j)\in\widehat{\mathcal{F}}\) if and only if \((j,i)\in\widehat{\mathcal{F}}\). The cutoff \(c\) regulates how conservative the graph estimate is: the larger \(c\), the less likely are false positives but the more likely are false negatives (cf. Theorem 7.4.1). In the case of sparse estimators such as the graphical lasso, one may take \(c=0\), that is, the estimated edges are the ones that have non-zero-valued entries in the estimated precision matrix. This choice avoids the introduction of yet another tuning parameter.9 In the case of unregularized maximum likelihood estimation, however, \(c=0\) almost always leads to a full graph. To remove spurious dependencies, a suggested cutoff is then \(c=\sqrt{\log[d]/n}\). The size of this cutoff is related to the size of tuning parameters \(r\) in front of \(\ell_{1}\)-prior terms--cf. \(\blacktriangleright\) Sect. 4.2.

[MISSING_PAGE_EMPTY:6322]

of \(z_{\Delta d}\) given \(z_{\mathcal{B}}\) is \(\mathcal{N}_{k}[-(\Theta_{\Delta d\Delta d})^{-1}\Theta_{\Delta d\Delta\mathbf{z} _{\mathcal{B}}},(\Theta_{\Delta d\Delta d})^{-1}]\), that is,

\[z_{\Delta d}\;=\;-\;(\Theta_{\Delta d\Delta d})^{-1}\Theta_{\Delta d\Delta \mathbf{z}_{\mathcal{B}}}+\boldsymbol{u}_{\Delta d}\,,\]

where \(\boldsymbol{u}_{\Delta d}\sim\mathcal{N}_{k}[\mathbf{0}_{k},(\Theta_{\Delta d \Delta d})^{-1}]\) is independent of \(z_{\mathcal{B}}\).

The key ingredient of the proof is Lemma B.2.15, which relates submatrices and inverses of matrices. We also use properties that are specific to Gauss-distributed data; for example, we invoke the fact that marginals of Gauss distributions are again Gauss distributions.

Lemma 3.4.1 follows by setting \(k=1\) (and reshuffling the model parameters if \(j\neq 1\)).

Writing

\[\Sigma\;=\;\begin{pmatrix}\Sigma_{\Delta d\Delta}&\Sigma_{\Delta d\mathcal{B} }\\ \Sigma_{\mathcal{B}\Delta d}&\Sigma_{\mathcal{B}\mathcal{B}}\end{pmatrix}\,,\]

Lemma B.2.15 (with \(M=\Sigma\) and \(M^{-1}=\Theta\)) yields

1. \((\Sigma_{\mathcal{B}\mathcal{B}})^{-1}\;=\;\Theta_{\mathcal{B}\mathcal{B}}- \Theta_{\mathcal{B}\Delta d}(\Theta_{\Delta d\Delta d})^{-1}\Theta_{\Delta d \mathcal{B}}\quad\text{and}\)
2. \(\frac{\det[\Sigma_{\mathcal{B}\mathcal{B}}]}{\det[\Sigma]}\;=\;\frac{1}{\det[( \Theta_{\Delta d\Delta d})^{-1}]}\,.\)

We denote the conditional density of \(z_{\Delta d}\) given \(z_{\mathcal{B}}\) by \(\ell_{\Delta d[\Delta\mathbf{z}_{\mathcal{A}}}\;|\;z_{\mathcal{B}}]\) and the marginal densities of \(z\) and \(z_{\mathcal{B}}\) by \(\ell[z]\) and \(\ell_{\mathcal{B}\mathcal{B}}[z_{\mathcal{B}}]\), respectively. We then state (without proof) that marginals of Gauss distributions are again Gauss distributions: if \(z\;\sim\;\mathcal{N}_{d}[\mathbf{0}_{d},\,\Sigma]\), then \(z_{\mathcal{B}\mathcal{B}}\sim\mathcal{N}_{d-k}[\mathbf{0}_{d-k},\,\Sigma_{ \mathcal{B}\mathcal{B}}]\). Using this with the two insights above then allows us to derive the following:

\[\ell_{\Delta d[\Delta\mathbf{z}_{\mathcal{A}}}\;|\;z_{\mathcal{B} }]\] \[=\;\ell[z]/\ell_{\mathcal{B}}[z_{\mathcal{B}}]\] Bayes rule \[=\;\frac{1}{\sqrt{(2\pi)^{d}\det[\Sigma]}}\;e^{-z^{\top}\Theta z /2}\big{/}\frac{1}{\sqrt{(2\pi)^{d-k}\det[\Sigma_{\mathcal{B}\mathcal{B}}]}} e^{-z^{\top}_{\mathcal{A}}(\Sigma_{\mathcal{A}\mathcal{B}})^{-1}z_{\mathcal{A} }/2}\] above statement about Gaussian marginals \[=\;\sqrt{\frac{\det[\Sigma_{\mathcal{B}\mathcal{B}}]}{(2\pi)^{k} \det[\Sigma]}}\] \[\quad\times e^{-z^{\top}_{\mathcal{A}}\Theta_{\Delta d\Delta d \Delta d}/2-z^{\top}_{\mathcal{A}}\Theta_{\Delta d\Delta d}z_{\mathcal{A}}/2 -z^{\top}_{\mathcal{A}}\Theta_{\Delta d\Delta d}z_{\mathcal{A}}/2-z^{\top}_{ \mathcal{A}}\Theta_{\Delta d\Delta d}z_{\mathcal{A}}/2+z^{\top}_{\mathcal{A} }(\Sigma_{\mathcal{A}\mathcal{B}\mathcal{B}})^{-1}z_{\mathcal{A}\mathcal{B}}/2}\] simplifying; writing out \(z^{\top}\Theta z\)\[\begin{split}&=\sqrt{\frac{\det[\Sigma_{\otimes\otimes\beta}]}{(2\pi)^ {k}\det[\Sigma]}}\\ &\quad\times e^{-\tau_{d}^{\Sigma}\Theta_{dd}\tau_{d}/2-\tau_{d}^{ \Gamma}\Theta_{dd}\tau_{d}\tau_{d}\tau_{d}/2-\tau_{d}^{\Gamma}\Theta_{dd}\tau_ {d}/2-\tau_{d}^{\Gamma}\Theta_{dd}\tau_{d}\tau_{d}/2-\tau_{d}^{\Gamma}\Theta_{ dd}\tau_{d}\tau_{d}/2}\\ &=\frac{1}{\sqrt{(2\pi)^{k}\det\left[(\Theta_{dd}\tau)^{-1} \right]}}\\ &\quad\times e^{-(\tau_{d}\tau_{d}+(\Theta_{dd}\tau)^{-1}\Theta_{ dd}\tau_{d}\tau_{d}\tau_{d}\tau_{d}\tau_{d}\tau_{d})/2}\,.\end{split}\] (i) above; summarizing terms in the exponent recalling that \(\Theta_{\otimes d}=\Theta_{dd}\) due to the symmetry of \(\Theta\) This is the density of a Gauss distribution with mean \(-(\Theta_{dd})^{-1}\Theta_{dd}\tau_{d}\tau_{d}\) and covariance matrix \((\Theta_{dd})^{-1}\). In other words, the conditional distribution of \(z_{\otimes d}\) given \(z_{\otimes d}\) is \(\mathcal{N}_{k}[-(\Theta_{dd})^{-1}\Theta_{dd}\tau_{d}\tau_{d}\tau_{d},( \Theta_{dd}\tau_{d})^{-1}]\), as desired. 

We now leverage Lemma 3.4.1 to estimate the parameters of interest. For each \(j\in\{1,\ldots,d\}\), we augment the \((d-1)\)-dimensional regression vector \(\mathbf{\beta}^{j}\) defined in the lemma by inserting a coordinate of value \(-1\) at the \(j\)th position, and we denote the resulting \(d\)-dimensional vector by \(\mathbf{\beta}^{j}\); for example, \(\mathbf{\beta}^{1}=(-1,(\mathbf{\beta}^{1})^{\top})^{\top}\in\mathbb{R}^{d}\). Some algebra yields (see Lemma B.2.16 in the Appendix)

\[\Theta_{ij}=\ -\ \frac{\Theta_{ii}(\mathbf{\beta}^{i})_{j}+\Theta_{jj}(\mathbf{\beta} ^{j})_{i}}{2}\qquad\qquad\text{for all $i,j\in\{1,\ldots,d\}$}\,.\]

Given independent realizations \(\mathbf{z}_{1},\ldots,\mathbf{z}_{n}\) of the random vector \(\mathbf{z}\), we then estimate \(\Theta_{ij}\) by estimating the four parameters on the right-hand side of the display. We can do this by regressing for each node \(j\in\{1,\ldots,d\}\) the vector \(\mathbf{y}_{j}:=((z_{1})_{j},\ldots,(z_{n})_{j})^{\top}\in\mathbb{R}^{n}\) on the matrix \(X_{-j}:=((z_{1})_{\{j\}\in\mathbb{C}},\ldots,(z_{n})_{\{j\}\in\mathbb{C}})^{\top }\in\mathbb{R}^{n\times(d-1)}\). We denote the outputs of these regressions by \(\mathbf{\widehat{\beta}}^{j}\equiv\mathbf{\widehat{\beta}}^{j}[\mathbf{y}_{j},X_{-j}]\in \mathbb{R}^{d-1}\) and their augmented versions (again with a coordinate of value \(-1\) inserted at the \(j\)th position) by \(\mathbf{\widehat{\beta}}^{j}\in\mathbb{R}^{d}\). These outputs yield the estimates \(\widehat{\Theta_{jj}}:=n/\|\mathbf{y}_{j}-X_{-j}\mathbf{\widehat{\beta}}^{j}\|_{2}^{2}\) for the diagonal entries and \((\mathbf{\widehat{\beta}}^{j})_{i}:=(\mathbf{\widehat{\beta}}^{j})_{i}\) for the coordinates of the regression vectors. Indeed, the above lemma ensures that \(n/\|\mathbf{y}_{j}-X_{-j}\mathbf{\beta}^{j}\|_{2}^{2}\sim n/\|\mathbf{u}\|_{2}^{2}\) with \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\mathbf{1}_{n\times n}/\Theta_{jj}]\), and the law of large numbers that \(\|\mathbf{u}\|_{2}^{2}\) converges to \(n/\Theta_{jj}\) as \(n\to\infty\). Hence, if the \(\mathbf{\widehat{\beta}}^{j}\)'s are consistently estimating \(\boldsymbol{\beta}^{j}\), also \(\widehat{\Theta}_{jj}\) and \((\widehat{\boldsymbol{\beta}^{j}})_{i}\) are consistently estimating their population counterparts.

In summary, the lemma motivates an estimate \(\widehat{\Theta}_{\text{ns}}\) of the precision matrix \(\Theta\) with elements

\[(\widehat{\Theta}_{\text{ns}})_{ij}\,:=\,\,-\,\frac{\frac{n}{|v_{i}-X_{-i} \boldsymbol{\beta}^{i}|_{2}^{2}}(\frac{\boldsymbol{\beta}^{i}}{2})_{j}+\frac{n} {|v_{j}-X_{-j}\boldsymbol{\beta}^{i}|_{2}^{2}}(\frac{\boldsymbol{\beta}^{j}}{ 2})_{i}}{2}\] \[\text{for all }i,j\in\{1,\ldots,d\}\,. \tag{3.6}\]

We call this the _neighborhood selection estimator_. In the case \(d\ll n\), least-squares can be chosen for the initial estimators \(\widehat{\boldsymbol{\beta}}^{j}\); then, in fact, \(\widehat{\Theta}_{\text{ns}}=\widehat{\Theta}_{\text{ml}}\)--see Exercise 3.4. Otherwise, regularized regression methods can be applied. A particular popular choice is the lasso; we call the corresponding scheme the _neighborhood lasso estimator_.10

Finally, graph estimates are obtained again according to the rule (3.5).

Because neighborhood selection can draw on the abundant algorithmic research and software packages for linear regression, it is often faster and easier to implement than the corresponding maximum likelihood approaches. However, regularized neighborhood methods require tuning-parameter calibration for \(d\) problems; then again, tuning-parameter calibration is better understood for regression than for other likelihood methods. Overall, there is no clear winner between the two approaches--neither theoretically nor empirically.

A limitation of both approaches is their fundamental dependence on the data being Gauss-distributed. Maximum likelihood invokes Gauss distributions when formulating the likelihoods; neighborhood selection invokes Gauss distributions when splitting the problem into multiple regressions. In contrast, because least-squares--while not necessarily optimal beyond Gauss-distributed data--still works quite generally, the methods for linear regression in the previous chapter do not rely on Gauss-distributed data to such an extent.

### 3.5 Exercises

#### 3.5.1 Exercises for Sect. 3.1

**Exercise 3.1**\(\blacklozenge\blacklozenge\blacklozenge\): Give further examples that show that conditional independence does not necessarily imply independence and vice versa.

#### 3.5.2 Exercises for Sect. 3.3

**Exercise 3.2**\(\blacklozenge\blacklozenge\blacklozenge\blacklozenge\): In this exercise, we show that the function \(\Omega\mapsto\operatorname{trace}[A\Omega]-\log\det\Omega\) is strictly convex on \(\delta^{+}_{d}=\{\Omega\in\mathbb{R}^{d\times d}:\Omega\text{ symmetric and positive definite}\}\) for every \(A\in\mathbb{R}^{d\times d}\). This implies in particular that the objective function of the maximum likelihood estimator for the precision matrix \(\Theta\) in Gaussian graphical models (3.2) is strictly convex.

We first make sure that speaking about convexity makes sense here. A non-empty set of matrices \(\mathcal{C}\subset\mathbb{R}^{d\times d}\) is called convex if \(wA+(1-w)B\in\mathcal{C}\) for all \(w\in[0,1]\) and \(A\), \(B\in\mathcal{C}\). In line with Definition 2.5.1, we then call a function \(\ell:\mathcal{C}\rightarrow\mathbb{R}\) for such a \(\mathcal{C}\) convex if

\[\ell[wA+(1-w)B]\leq w\ell[A]+(1-w)\ell[B]\] \[\text{for all }w\in[0,1];\ A,B\in\mathcal{C}\]

and strictly convex if

\[\ell[wA+(1-w)B]<w\ell[A]+(1-w)\ell[B]\\ \text{for all }w\in(0,1);\ A,B\in\mathcal{C},\ A\neq B\,.\]

1. Show that the set \(\delta^{+}_{d}\) is convex.

We then consider the trace function \(\Omega\mapsto\operatorname{trace}[A\Omega]=\sum_{i=1}^{n}(A\Omega)_{ii}\).
2. Show that for every \(A\in\mathbb{R}^{d\times d}\), this function is convex on the entire space \(\mathbb{R}^{d\times d}\) (which is trivially convex), that is, \(\operatorname{trace}[A(w\Omega^{\prime}+(1-w)\Omega^{\prime\prime})]\leq w \operatorname{trace}[A\Omega^{\prime}]+(1-w)\operatorname{trace}[A\Omega^{ \prime\prime}]\) for all \(\Omega^{\prime},\Omega^{\prime\prime}\in\mathbb{R}^{d\times d}\) and \(w\in[0,1]\). Assure yourself that this implies that the function is convex also on \(\delta^{+}_{d}\).

We now consider the log-determinant function \(\Omega\mapsto-\log\det\Omega\).
3. Show that for every matrix \(A\in\mathbb{R}^{d\times d}\), it holds that \(|\text{det}[A]|=\prod_{j=1}^{d}m_{j}\), where \(m_{1},\ldots,m_{d}\) are the singular values of \(A\). Show similarly that if \(A\) is also _symmetric and positive definite_, it holds that \(\text{det}[A]=\prod_{j=1}^{d}m_{j}\), where \(m_{1},\ldots,m_{d}>0\) are the _eigenvalues_ of \(A\).

Hint: You might want to check Lemma B.2.11 (singular value decomposition) and Appendix B.2 more generally.
4. Consider a symmetric and positive definite matrix \(\Omega\in\mathcal{S}_{d}^{+}\), a symmetric matrix \(A\in\mathbb{R}^{d\times d}\setminus\{\mathbf{0}_{d\times d}\}\), and a value \(\tilde{t}\equiv\tilde{t}[\Omega,A]>0\) such that \(\Omega+tA\) is symmetric and positive definite for every \(t\in[0,\tilde{t}]\). Show by using 3. that the function \(t\mapsto-\log\text{det}[\Omega+tA]\) is _strictly_ convex on \([0,\tilde{t}]\). Bonus: Show that such a \(\tilde{t}\) exists in the first place.
5. Show by using 4. that \(\Omega\mapsto-\log\text{det}[\Omega]\) is _strictly_ convex on \(\mathcal{S}_{d}^{+}\).

We can finally derive the desired claim.
6. Conclude from 2. and 5. that for every \(A\in\mathbb{R}^{d\times d}\), the objective function \(\Omega\mapsto\text{trace}[A\Omega]-\log\text{det}[\Omega]\) of the maximum likelihood estimator (3.2) is strictly convex.

**Exercise 3.3**\(\blacklozenge\blacklozenge\) In this exercise, we confirm the closed-form solution of the unregularized maximum likelihood estimator stated in \(\blacktriangleright\) Sect. 3.3.

1. Show that for every given matrix \(A\in\mathbb{R}^{d\times d}\), the (matrix-valued) gradient of the trace function \(\Omega\mapsto\text{trace}[A\Omega]=\sum_{i=1}^{n}(A\Omega)_{ii}\) on \(\mathbb{R}^{d\times d}\) is \(A^{\top}\), that is, \[\frac{\partial}{\partial\Omega}\,\text{trace}[A\Omega]=A^{\top}\,.\]
2. Show that the (matrix-valued) gradient of the log-determinant function \(\Omega\mapsto\log[\text{det}[\Omega]]\) on the invertible matrices in \(\mathbb{R}^{d\times d}\) is \((\Omega^{-1})^{\top}\), that is, \[\frac{\partial}{\partial\Omega}\log\left[\text{det}[\Omega]\right]=(\Omega^{- 1})^{\top}\,.\] Claims 1 and 2 together with Claim 6 of Exercise 3.2 ensure in particular that the objective function of the maximum likelihood estimator, namely \(\Omega\mapsto\sum_{i=1}^{n}\text{trace}[zi_{i}\tau^{\top}\Omega]/n-\log[\text{det}[ \Omega]]\) is differentiable and convex. This makes the objective functions of maximum _regularized_ likelihood estimators with convex prior functions amenable to a variety of gradient-based methods.11

Hint: You might want to use the properties of cofactor matrices in Lemma B.2.12 and the Laplace expansion of determinants in Lemma B.2.13.
3. Conclude that for every given matrix \(A\in\mathcal{S}^{+}_{d}\), where \(\mathcal{S}^{+}_{d}\) are the symmetric and invertible matrices in \(\mathbb{R}^{d\times d}\), the minimization program \[\widehat{\Theta}\in\operatorname*{arg\,min}_{\Omega\in\mathcal{S}^{+}_{d}}\left\{ \text{ trace}[A\Omega]-\log\left[\text{det}[\Omega]\right]\right\}\] has the unique solution \(\widehat{\Theta}=A^{-1}\). Hint: Start from Claim 6 in Exercise 3.2.
4. Show that if \(n\geq d\), the empirical covariance matrix \(\sum_{i=1}^{n}z_{i}z_{i}\tau^{\top}/n\) of \(n\) independent realizations \(z^{1},\ldots,z^{n}\) of \(z\sim\mathcal{N}_{d}[\mathbf{0}_{d},\,\Sigma]\), \(\Sigma\) symmetric and invertible, is symmetric and invertible with probability 1, that is, \(\mathbb{P}\{\sum_{i=1}^{n}z_{i}z_{i}\tau^{\top}/n\in\mathcal{S}^{+}_{d}\}=1\).
5. Conclude that if \(n\geq d\), Identity (3.3) on Page 89 holds true with probability one: \[\mathbb{P}\left\{\widehat{\Theta}_{\text{ml}}\,=\,\left(\frac{1}{n}\sum_{i=1}^ {n}z_{i}z_{i}\tau^{\top}\right)^{-1}\,\right\}\,=\,1\,.\]
6. Bonus: Compute \((\widehat{\boldsymbol{\mu}}_{\text{ml}},\,\widehat{\Theta}_{\text{ml}})\) in the full maximum likelihood approach on Page 89, where the true mean \(\boldsymbol{\mu}\) is not assumed to be equal to \(\mathbf{0}_{d}\).

#### Exercises for \(\boldsymbol{\psi}\) Sect. 3.4

**Exercise 3.4** \(\boldsymbol{\phi}\boldsymbol{\phi}\boldsymbol{\psi}\) In this exercise, we show that the unregularized maximum likelihood estimator and neighborhood selection with the least-squares coincide.

Assume that the empirical covariance matrix \(\sum_{i=1}^{n}z_{i}\tau^{\top}_{i}/n\) is invertible and denote by \(\widehat{\boldsymbol{\beta}}^{j}\equiv\widehat{\boldsymbol{\beta}}^{j}[ \boldsymbol{y}_{j},\,X_{-j}]\in\mathbb{R}^{d-1}\), \(j\in\{1,\ldots,d\}\), the least-squares estimators for regressing \(\boldsymbol{y}_{j}:=((z_{1})_{j},\ldots,(z_{n})_{j})^{\top}\in\mathbb{R}^{n}\) on \(X_{-j}:=((z_{1})_{\{j\}}^{\complement},\ldots,(z_{n})_{\{j\}}^{\complement})^{ \top}\in\mathbb{R}^{n\times(d-1)}\). The augmented versions (with a coordinate of value \(-1\) inserted at the \(j\)th position) of the least-squares estimators are denoted by \(\widehat{\boldsymbol{\beta}}^{j}\in\mathbb{R}^{d}\).
1. Show that for all \(j\in\{1,\ldots,d\}\), it holds that \[\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}\boldsymbol{z}_{i}^{\top}\right)_{\{j\} \in\{1,\ldots,d\}}\widehat{\boldsymbol{\beta}}^{j}\,=\,\mathbf{0}_{d-1}\,.\] This means that the augmented least-squares estimators are in the kernel of a submatrix of the empirical covariance matrix.
2. Show that for all \(j\in\{1,\ldots,d\}\), it holds that \[\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}\boldsymbol{z}_{i}^{\top}\right)_{\{1, \ldots,d\}}\widehat{\boldsymbol{\beta}}^{j}\,=\,\,-\,\frac{\|\boldsymbol{y}_{ j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}\|_{2}^{2}}{n}\,.\] This connects the remaining row of the empirical covariance matrix with the augmented least-squares estimators and the least-squares objective function.
3. Show finally that \[\widehat{\Theta}_{\text{ns}}\,=\,\left(\frac{1}{n}\sum_{i=1}^{n}z_{i} \boldsymbol{z}_{i}^{\top}\right)^{-1}\,.\] We conclude that the estimates in (3.3) and (3.6) coincide under the stated assumptions.

### 3.6 R Lab: Estimating a Gene-Gene Coactivation Network

In this lab, we fit gene-gene coactivation networks to gene expression data. The data consists of vector-valued samples, each of them describing the expression levels of genes in a bacterium called _Bacillus subtilis_. Our assumption is that these vectors are independent and identically distributed according to a multivariate Gauss distribution. We describe the dependence structures of the expression levels by using graphs, and since gene expressions measure gene activities, we can interpret these graphs as gene-gene coactivation networks.

As always, your task is to replace the keyword REPLACE with suitable code and to answer the questions in the text.

[MISSING_PAGE_EMPTY:6330]

matrix: in matrix notation, \(\widehat{\Theta}_{\text{ml}}=(Z^{\top}Z/n)^{-1}\). Compute this estimator for the above data and study the stated visualization pipeline.

invcovariance.ml <- REPLACE invcovariance.ml

## [,1] [,2] [,3] [,4]
## [1,] 1.23981631 -0.29082882 0.02037997 -0.01386750
## [2,] -0.29082882 1.58948298 -0.01016519 0.27151900
## [3,] 0.02037997 -0.01016519 0.94533132 -0.04109434
4,] -0.01386750 0.27151900 -0.04109434 0.49903322 Comparing to the true inverse covariance, we find that maximum likelihood estimation is reasonably accurate in our test case.

library(igraph) network.initial <- graph.adjacency(abs(invcovariance.ml), weighted-TRUE, modes-"undirected", diag-PALGE) network.layout <- layout_in_circle(network.initial, ordere(4, 1, 2, 3)) igraph_options(vertex.size = 40, vertex.color = "lightlyblue", vertex.frame.color = NH, vertex.label.cxx = 3, vertex.label.color = "black", edge.width = 10 * #(network.initial)lowight, edge.color = "corall") plot.igraph(network.initial, layout.network.layout)

What does this plot visualize?

##### 3.6.1.2.2 Estimating the Graph

Compute the graph estimate (3.5) with cutoff \(c=\sqrt{\log[d]/n}\). This estimate should be written in terms of a so-called _adjacency matrix_\(\widehat{A}\in\mathbb{R}^{d\times d}\) defined through \(\widehat{A}_{ij}:=1\) if \((i,j)\in\widehat{\mathfrak{R}}\) and \(\widehat{A}_{ij}:=0\) otherwise.

Then, compare the matrix with the adjacency matrix \(A\) that captures the true graph: \(A_{ij}:=1\) if \((i,j)\in\mathcal{E}\) and \(A_{ij}:=0\) otherwise.

Finally, visualize the estimated adjacency matrix as above.

 adjacencymatrix.ml <- REPLACE # estimated graph adjacencymatrix.ml

## [,1] [,2] [,3] [,4]
## [1,] 0 1 0 0
## [2,] 1 0 0 1
## [3,] 0 0 0 0
## [4,] 0 1 0 0

 adjacencymatrix <- REPLACE # true graph adjacencymatrix

## [,1] [,2] [,3] [,4]
## [1,] 0 1 0 0
## [2,] 1 0 0 1
## [3,] 0 0 0 0
## [4,] 0 1 0 0

Comparing the adjacency matrices, we find that maximum likelihood with the standard cutoff recovers the graph correctly.

 REPLACE # draw the graph

#### 3.6.1.3 Parameter Estimation via Neighborhood Selection

We now implement and apply a neighborhood selection scheme. We do this in four steps.

##### 3.6.1.3.1 Linear Regressions

Implement first a least-squares estimator (without intercept) for usual regression data of the form \(y\in\mathbb{R}^{n},X\in\mathbb{R}^{n\times(d-1)}\). The output of this function is a \((d-1)\)-dimensional vector. Then, study the function RegressionVectors: what does it do?

``` [xEstimator<-function(y,X) { return(HEPLACE) } RegressionVectors<-function(Z,FUN-LaEstimator) { regression.vectors<-NULL  for(j in:dim(Z)[]) #passing through all nodes {  regression.vectors<-child(regression.vectors,FUN(Z[,j],z[,-j]))  return(regression.vectors) } RegressionVectors(Z)
## [,1] [,2] [,3] [,4]
## [1,] 0.23457412 0.182970704 -0.02155855 0.02778872
## [2,] -0.01643790 0.006395282 0.01075305 -0.54409004
## [3,] 0.01118512 -0.170822214 0.04347084 0.08234791

##### [,1] [,2] [,3] [,4]
## [1,] 0.23457412 0.182970704 -0.02155855 0.02778872
## [2,] -0.01643790 0.006395282 0.01075305 -0.54409004
## [3,] 0.01118512 -0.170822214 0.04347084 0.08234791

##### [,1] [,2] [,3] [,4]
## [2,] 0.23457412 -1.00000000 0.01075305 -0.54409004
## [3,] -0.01643790 0.006395282 -1.00000000 0.08234791

##### [4,] 0.01118512 -0.170822214 0.04347084 -1.00000000

DiagonalEntries<-function(Z,regression.vectors) { REPLACE } DiagonalEntries(Z,RegressionVectors(Z))
3.6.1.3.3 Estimating the Inverse Covariance Matrix

We now put the pieces together to estimate the inverse covariance matrix.

```
NeighborhoodSelection<-function(Z,FUN-Left<timator) { regression.vectors<-RegressionVectors(Z,FUN)  regression.vectors.augmented<-AggnormMatrix(regression.vectors)  diagonal.entries<-DiagonalIntrinsic(Z,regression.vectors)  invcovariance.ns<-matrix(data>,nrowdim(Z)[2],ncol:dim(Z)[2])  for(iin1:dim(Z)[2]) {  for(jin1:dim(Z)[2]) {  invcovariance.ns[i,j]<-REPLACE  }  }  return(invcovariance.ns) }
```

```
##[,1][,2][,3][,4]
##[1,1]1.23981631-0.290828820.02037997-0.01386750
##[2,]-0.290828821.58948298-0.010165190.27151900
##[3,]0.02037997-0.010165190.94533132-0.04109434
##[4,]-0.013867500.27151900-0.041094340.49903322
```

We find that neighborhood selection yields the same estimate for the inverse covariance matrix as maximum likelihood--which is commensurate with the theoretical finding in Exercise 3.4.

3.6.1.3.4 Estimating the Graph

Calculate the graph estimate (3.5) with cutoff \(c=\sqrt{\log[d]/n}\) in terms of an adjacency matrix.

``` adjacencymatrix.ns<-REPLACE adjacencymatrix.ns

##[,1][,2][,3][,4]
##[1,]0100
##[2,]1001
##[3,]0000
##[4,]0100

In line with the preceding result, we find that the neighborhood selection scheme provides perfect graph recovery in the test case.

#### 3.6.2 A Low-Dimensional Gene Network

We first estimate a network of only a small number of genes. Throughout the real data analysis, we use neighborhood selection rather than maximum likelihood. A major advantage of neighborhood selection is that once having set up the above pipeline, it is extremely easy to account for high-dimensional data: it suffices to replace the least-squares with a high-dimensional regression method.

##### 3.6.2.1 Loading and Pre-processing the Data

Download the file GraphicalModels_Lab_Data.rda from the book's homepage to your R working directory. Loading this file into R populates a matrix-valued variable data with the measurements. The \(i\)th row of this matrix corresponds to the \(i\)th sample; the \(j\)th column corresponds to the \(j\)th gene.

Store normalized versions of the first \(d=5\) genes' expressions in a matrix \(\mathcal{Z}\in\mathbb{R}^{n\times d}\), where \(n\) is the number of samples and \(d\) the number of genes under consideration. Use the standardization (3.2); the scale() function could be helpful for this.

```
#makeourshatthefileisinthecurrentworkingdirectory load("GraphicalModels_Lab_Data.rda") Z<-REPLACE head(2)

#AADR_atAPA_atABFA_atABH_atABNA_at
##[1,]3.2073238.121298561.403423841.71624431.30339129
##[2,]1.5050796-0.8779609-1.255244651.216860700.04949853
##[3,]2.40099760.5589092.044324250.99872497-0.21917314
##[4,]1.9988562.08045670.57047877.1003241270.18695672
##[5,]-0.1584393-0.1232478-0.42849152-0.323449080.58482838
##[6,]0.58612700.6713774-0.04695200-0.092092390.52639650
```

Verify that the number of samples in Z is \(n=71\) and the number of nodes \(d=5\). This means in particular that the number of samples is much larger than the number of model parameters (\(n=71\gg d(d-1)/2=10\)), so that we can apply unregularized estimators.

##### 3.6.2.2 Estimating the Inverse Covariance Matrix

Estimate the inverse covariance matrix with the above neighborhood selection scheme. Visualize the result.

[MISSING_PAGE_EMPTY:6336]

[MISSING_PAGE_EMPTY:6337]

[MISSING_PAGE_EMPTY:6338]

More precisely, we speak of _undirected, probabilistic graphical models_, also called _Markov random fields_. The word _probabilistic_ indicates that the mathematical basis is probability theory. The word _undirected_ indicates that the relationships among the nodes are symmetric: for example, \(\mathtt{bike}\not\perp\mathtt{sun}\mid\mathtt{dog}\) if and only if \(\mathtt{sun}\not\perp\mathtt{bike}\mid\mathtt{dog}\). Hence, the graph on the right-hand side of \(\blacksquare\)

Fig. 3.1 describes that the events \(\mathtt{bike}\) and \(\mathtt{sun}\) are related, but it does not specify a hierarchy.

_Directed graphical models_, on the other hand, go one step further: for example, sunshine makes us take the bike more likely and also increases the chance that the neighbor walks his dog; on the contrary, as much as we might like that, neither taking the bike nor walking the dog influences the weather. Such relationships among the events \(\mathtt{bike}\), \(\mathtt{dog}\), and \(\mathtt{sun}\) is, therefore, unsymmetric. Hence, the corresponding graph describes not only that \(\mathtt{bike}\) and \(\mathtt{sun}\) are related, but it also specifies a hierarchy between the two: the chance for \(\mathtt{bike}\) is increased by \(\mathtt{sun}\). We call such relationships _causal_. In general, however, causal relationships are much less obvious, and they are associated with a number of statistical and philosophical challenges (Spirtes et al., 2000); therefore, we limit ourselves to undirected graphical models.
* A standard approach for count data is the _Anscombe transformation_(Anscombe, 1948). A standard approach for compositional data is the _centered log-ratio transformation_(Aitchison, 1982), which is particularly interesting for microbiome data (Kurtz et al., 2015).
* See Besag (1974), Grimmett (1973), Preston (1973), Sherman (1973).
* A more comprehensive version of the Hammersley-Clifford Theorem and a proof can be found in Lauritzen (1996, Theorem 3.9 on p. 36).
* Yuan and Lin (2007) introduce \(\ell_{1}\)-regularized maximum likelihood estimation for Gauss-distributed data and discuss its connections to neighborhood selection with the lasso. Further algorithms for the maximum likelihood approach in the Gauss case followed quickly in Banerjee et al. (2008), Friedman et al. (2008). The latter paper also coined the term graphical lasso.
* If the tuning parameter is calibrated via adaptive validation, one can also select the cutoff along the lines of Example 7.4.1. We refer to Laszkiewicz et al. (2020) for details.
* Neighborhood selection with the lasso was introduced in Meinshausen and Buhlmann (2006).
* For the computation of the graphical lasso, for example, see Mazumder and Hastie (2012), Oztoprak et al. (2012).

## Tuning-Parameter Calibration

###### Contents

Regularized estimators consist of two terms, one for comparing model parameters to data and one for including prior information. The tuning parameters define the weighting: small tuning parameters emphasize the data, while large tuning parameters emphasize the prior information. An optimal tuning parameter balances the data and the prior information such that an estimator's error for a given task is minimized. Data-driven calibration schemes try to find such optimal tuning parameters in practice.

In this chapter, we introduce two such calibration schemes: cross-validation and adaptive validation. Cross-validation is recommended for prediction (minimizing \(\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\)), while adaptive validation is recommended for \(\ell_{\infty}\)-estimation (minimizing \(|\mathbf{\beta}-\widehat{\mathbf{\beta}}\|_{\infty}\)) and support recovery (minimizing \(|\{j:\beta_{j}\neq 0,\widehat{\beta_{j}}=0\}|\) and \(|\{j:\beta_{j}=0,\widehat{\beta_{j}}\neq 0\}|\)).

### Overview

We spend about one-third of our lives sleeping. It is, therefore, not surprising that good sleep is indispensable for our health; for example, recent research suggests that deep sleep clears the brain from Alzheimer's toxins.1 Sleep research is based on data that describes brain activities, levels of blood markers, body movements, and other variables of sleeping subjects. These data can be used to fit complex models that reproduce the intricacy of the biology of sleep or, instead, simple models that identify the most important agents in the process under investigation. Regularized estimators such as the lasso can produce both types of models: small tuning parameters generate estimators with many non-zero entries (for example, toxin removal is explained as a concert of many biological factors), and large tuning parameters generate estimates with only a small number of non-zero entries (only the most relevant factors are retained).

However, the margins for adjusting the complexity of a model through the tuning parameter are limited. Data is subject to random noise, which introduces the risk of overfitting--see \(\blacksquare\) Fig. 2.1, for example. Regularized estimators can avoid this overfitting only if the tuning parameters are sufficiently large. In the following, we establish a corresponding lower bound for the tuning parameter.

[MISSING_PAGE_FAIL:123]

Since the first two terms are independent of the parameter \(\boldsymbol{\alpha}\) over which we optimize, the lasso estimator becomes

\[\widehat{\boldsymbol{\beta}}\;\in\;\operatorname*{arg\,min}_{\boldsymbol{\alpha} \in\mathbb{R}^{p}}\left\{\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+ r\|\boldsymbol{\alpha}\|_{1}-2\langle X^{\top}\boldsymbol{u},\;\boldsymbol{ \alpha}\rangle\right\}.\]

This formulation is not useful in practice (because \(\boldsymbol{\beta}\) is unknown), but it allows us to identify the term \(2\langle X^{\top}\boldsymbol{u},\;\boldsymbol{\alpha}\rangle\) as the difference between the lasso estimator with noise and the lasso estimator without noise.

To avoid overfitting, we have to limit the influence of that noise term on the objective function. By the Holder inequality (2.6), the noise term is bounded by \(-2\langle X^{\top}\boldsymbol{u},\;\boldsymbol{\alpha}\rangle\leq 2\|X^{\top} \boldsymbol{u}\|_{\infty}\|\boldsymbol{\alpha}\|_{1}\). If we write design matrix as \(X=(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{p})\) with predictors \(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{p}\in\mathbb{R}^{n}\), the part that comprises the noise becomes (cf. Page 40)

\[2\|X^{\top}\boldsymbol{u}\|_{\infty}\;=\;2\|\boldsymbol{u}\|_{2}\max_{j\in \{1,\ldots,p\}}\left\{|\operatorname{cor}[\boldsymbol{u},\;\boldsymbol{x}_{j} ]|\cdot\|\boldsymbol{x}_{j}\|_{2}\right\}.\]

The factor \(\|\boldsymbol{u}\|_{2}\) measures the overall strength of the noise; for example, \(\|\boldsymbol{u}\|_{2}\approx\sqrt{n}\sigma\) if \(\boldsymbol{u}\sim\mathcal{N}_{n}[\boldsymbol{0}_{n},\sigma^{2}\mathrm{I}_{n \times n}]\)--see Exercise 5.4. The factor \(|\operatorname{cor}[\boldsymbol{x}_{j},\boldsymbol{u}]|\) measures the similarities between the noise and the \(j\)th predictor. The factor \(\|\boldsymbol{x}_{j}\|_{2}\) is just the predictor's normalization. The larger the noise and the more similar the noise and the predictor, the more the noise obscures that predictor and, therefore, the less accuracy we can expect from the estimation of the corresponding parameter. Hence, we can think of \(2\|X^{\top}\boldsymbol{u}\|_{\infty}\) as the maximal effect of the noise on the estimation of a parameter. In line with this, we call \(2\|X^{\top}\boldsymbol{u}\|_{\infty}\) the _effective noise_ of the lasso estimator.

For tuning parameters larger than the effective noise, the prior term dominates the noise term:

\[r\;\geq\;2\|X^{\top}\boldsymbol{u}\|_{\infty}\quad\Rightarrow\quad r\| \boldsymbol{\alpha}\|_{1}\;\geq\;-\;2\langle X^{\top}\boldsymbol{u},\; \boldsymbol{\alpha}\rangle\;.\]

This illustrates that \(r\geq 2\|X^{\top}\boldsymbol{u}\|_{\infty}\) is a sufficient2 condition for avoiding overfitting. We will substantiate the mathematics behind this argument later in the theory chapters. In particular, we will show that \(r\geq 2\|X^{\top}\boldsymbol{u}\|_{\infty}\) is a sufficient condition for prediction guarantees (see Theorem 6.3.1 and Example 6.3.2) and that \(r>2\|X^{\top}\boldsymbol{u}\|_{\infty}\) can additionally lead to estimation guarantees (see Theorems 7.2.1 and 7.3.2 and Examples 7.2.1 and 7.3.1 and also the below Example 4.4.1).

In practice, the effective noise is unknown. Hence, the goal of this chapter is to transform the above insights into practical calibration schemes.

The outline is as follows: In \(\blacktriangleright\) Sect. 4.2, we equip the effective noise \(2\|X^{\top}\boldsymbol{u}\|_{\infty}\) and with a tail bound that shows what to expect from calibration schemes. In \(\blacktriangleright\) Sect. 4.3, we then introduce cross-validation, a calibration scheme based on data splitting. In \(\blacktriangleright\) Sect. 4.4, we introduce adaptive validation, a calibration scheme based on error bounds.

### 4.2 Bounds on the Lasso's Effective Noise

Having identified the effective noise as a lower bound for the tuning parameter, we now quantify this lower bound for noise that is Gauss-distributed. This quantification illustrates what tuning parameters to expect from a successful calibration, while actual calibration schemes will be introduced in the subsequent parts of this chapter.

Our main result is a tail bound for the effective noise:3

Footnote 3: The \(\infty\)-threshold is defined as \(\frac{1}{2}\), where \(\frac{1}{2}\) is the \(\infty\)-threshold.

\[\mathbb{P}\{r_{t}\geq 2\|X^{\top}\boldsymbol{u}\|_{\infty}\,\}\,\geq\,1-t\,.\]

Broadly speaking, using the tuning parameter \(r_{t}\) prevents the lasso estimator from overfitting with probability at least \(1-t\). The constant \(t\) resembles the significance level of a hypothesis test: for example, setting \(t=0.05\) ensures that the effective noise is bounded by \(r_{t}=\sigma\sqrt{8mn\log[p/t]}\) with probability at least \(95\%\). Summarizing and neglecting constants, we say that "the effective noise is bounded by \(\sigma\sqrt{n\log[p]}\)_with high probability_."An illustration of the lemma is in Fig. 4.1.

The tail bound looks like a manual for calibrating the lasso's tuning parameter, but \(r_{t}\) cannot be implemented: first, it involves the standard deviation \(\sigma\), which is usually unknown; second, it requires the coordinates of the noise to be independent and identically Gauss-distributed, which is usually unrealistic; and third, it neglects the correlations among the predictors.4 Rather than practical advice, the tail bound exemplifies what to expect from a calibration scheme.

We conclude this section by proving the lemma.

##### Proof of Lemma 4.2.1

The key tool in the proof is the union bound; the rest is simple algebra.

By definition of the sup-norm, it holds that \(2\|X^{\top}\mathbf{u}\|_{\infty}=\max_{j\in\{1,\ldots,p\}}2|(X^{\top}\mathbf{u})_{j}|\). We can, therefore, write the complement of the event in question as

\[\left\{r_{t}\geq 2\|X^{\top}\mathbf{u}\|_{\infty}\ \right\}^{\mathbb{C}} = \left\{\,2\|X^{\top}\mathbf{u}\|_{\infty}>r_{t}\,\right\}\] \[= \bigcup_{j=1}^{p}\left\{\,2|(X^{\top}\mathbf{u})_{j}|>r_{t}\,\right\}.\]

[MISSING_PAGE_FAIL:127]

We finally use the results to deduce

\[\mathbb{P}\big{\{}r_{t}\geq 2\|X^{\top}\mathbf{u}\|_{\infty}\,\big{\}}\] \[=\,1-\mathbb{P}\big{\{}2\|X^{\top}\mathbf{u}\|_{\infty}>r_{t}\,\big{\}} \mathbb{P}[\alpha]=1-\mathbb{P}[\alpha^{\complement}]\] \[\geq\,1-pe^{-\big{(}\frac{\sigma\sqrt{8mn\log(p/t)}}{2\sigma\sqrt {min}}\big{)}^{2}/2}\] \[=\,1-t\,,\]

as desired. 

### Cross-Validation

Cross-validation calibrates tuning parameters for prediction. It repeatedly partitions the data into two parts: training data for fitting model parameters and validation data for estimating the prediction errors of those fitted parameters. It then selects a tuning parameter that minimizes the average of those estimated errors, trusting that (i) the fitted model parameters resemble the original estimators and (ii) the estimated errors resemble the true ones.5

Given a family of linear regression estimators \(\{\widehat{\mathbf{\beta}}[r]:r\in\mathcal{R}\}\subset\mathbb{R}^{p}\) indexed by a non-empty but otherwise an arbitrary set of tuning parameters \(\mathcal{R}\), an optimal tuning parameter for prediction is

\[r^{*}\,\in\,\operatorname*{arg\,min}_{r\in\mathcal{R}}\,\ell^{*}[r]\quad\quad \text{with}\;\;\ell^{*}[r]\,:=\,\frac{1}{n}\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}} [r]\|_{2}^{2}\,. \tag{4.1}\]

The scaling \(1/n\) has no effect on \(r^{*}\) but makes the below derivations easier. In practice, the regression vector \(\mathbf{\beta}\) is unknown, which means that the optimal tuning parameter \(r^{*}\) is also unknown. The target of this section is, therefore, a data-driven surrogate of \(r^{*}\).

The general strategy is to replace the average prediction error \(\ell^{*}\) by an estimate \(\widehat{\ell}\):

\[\widehat{r}\,\in\,\operatorname*{arg\,min}_{r\in\mathcal{R}}\widehat{\ell}[r]\,, \tag{4.2}\]

where \(\widehat{\ell}:\mathcal{R}\to\mathbb{R}\) involves the data \((\mathbf{y},\,X)\) but not the unknown regression parameter. If \(\widehat{\ell}[r]\approx\ell^{*}[r]\) for all \(\mathcal{R}\), we can expect that \(\widehat{r}\approx r^{*}\) and thus \(\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}[\widehat{r}]\|_{2}^{2}/n \approx\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}[r^{*}]\|_{2}^{2}/n\) (cf. Exercise 4.5).

The seemingly most natural estimate of the average prediction error is the least-squares data fit

\[\widehat{\mathcal{P}}[r]\,:=\,\frac{1}{n}\|\boldsymbol{y}-X\widehat{ \boldsymbol{\beta}}[r]\|_{2}^{2}\,.\]

But since regularized least-squares estimators already include \(\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2}^{2}\) in their objective functions, this estimate induces overfitting. For example, if \(\mathcal{R}=[0,\infty)\) and

\[\widehat{\boldsymbol{\beta}}[r]\,=\,\widehat{\boldsymbol{\beta}}_{\text{lasso }}[r]\,\in\,\operatorname*{arg\,min}_{\boldsymbol{\alpha}\in\mathbb{R}^{p}} \left\{\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2}^{2}+r\|\boldsymbol{\alpha }\|_{1}\right\}\,,\]

it holds that \(\widehat{\mathcal{P}}[0]=\min_{r\in\mathcal{R}}\widehat{\mathcal{P}}[r]\) and, therefore, that (4.2) has the solution \(\widehat{r}=0\); in other words, the calibration based on the least-squares data fit always selects the plain least-squares estimator.

One approach to amend the estimate of the prediction error is to add a function that measures the models' complexity:

\[\widehat{\mathcal{P}}[r]\,:=\,\frac{1}{n}\|\boldsymbol{y}-X\widehat{ \boldsymbol{\beta}}[r]\|_{2}^{2}+t\cdot\text{complexity}\left[\widehat{ \boldsymbol{\beta}}[r]\right].\]

The complexity function resembles the prior function in (1.6): it favors simple models, thereby attempting a trade-off between a good data fit (typically achieved with small \(r\)) and low model complexity (typically achieved with large \(r\)). But the complexity and prior functions can differ: for example, the complexity function has fewer computational constraints than the prior function because the optimization for \(\widehat{r}\) is typically over a subset of \(\mathbb{R}\), while the optimization for \(\widehat{\boldsymbol{\beta}}\) is over \(\mathbb{R}^{p}\). Examples for calibration schemes based on least-squares and a complexity function include _AIC_ and _BIC_. By specifying \(t\,\in\,[0,\infty)\) as a function of \(n\) and \(p\), they avoid calibrating that parameter.

But in the following, we focus on a different approach to amend the naive estimate of the prediction error: the _holdout method_. The holdout method addresses the problem at source, which is that data are used both for fitting regression vectors and for estimating the prediction errors, by using each sample only for either one of the two tasks. The index set of the samples is split into a _training set_training and \(\mathcal{T}\subset\{1,\ldots,n\}\) with size \(n_{\mathcal{T}}\coloneqq|\mathcal{T}|\) and a _validation set_ (or _holdout set_) \(\mathcal{T}\coloneqq\{1,\ldots,n\}\setminus\mathcal{T}\) with size \(n_{\mathcal{V}}\coloneqq|\mathcal{V}|=n-n_{\mathcal{T}}\). The samples that correspond to the training set are assigned to fitting the regression vectors, and the samples that correspond to the validation set are assigned to estimating the prediction errors:

\[\widehat{r}_{\text{holdout}}\in\operatorname*{arg\,min}_{r\in\mathcal{R}} \widehat{\mathcal{f}}[r]\]

with

\[\widehat{\mathcal{f}}[r]\coloneqq\frac{1}{n_{\mathcal{V}}}|\boldsymbol{v}_{ \mathcal{V}}-X_{\mathcal{V}}\widehat{\boldsymbol{\beta}}[r,\boldsymbol{y}_{ \mathcal{T}},X_{\mathcal{T}}]|_{2}^{2}\,,\]

where \(\boldsymbol{y}_{\mathcal{T}}\in\mathbb{R}^{n_{\mathcal{T}}}\) and \(X_{\mathcal{T}}\in\mathbb{R}^{n_{\mathcal{T}}\times p}\) are the vector/matrix-valued training data, \(\boldsymbol{y}_{\mathcal{V}}\in\mathbb{R}^{n_{\mathcal{V}}}\) and \(X_{\mathcal{V}}\in\mathbb{R}^{n_{\mathcal{V}}\times p}\) are the vector/matrix-valued validation data, and \(\widehat{\boldsymbol{\beta}}[r,\boldsymbol{y}_{\mathcal{T}},X_{\mathcal{T}}] \in\mathbb{R}^{p}\) is the estimator evaluated on the training data.

The holdout method is expected to work well if: (i) the fitted parameters \(\widehat{\boldsymbol{\beta}}[r,\boldsymbol{y}_{\mathcal{T}},X_{\mathcal{T}}]\) are close to the original estimators \(\widehat{\boldsymbol{\beta}}[r]=\widehat{\boldsymbol{\beta}}[r,\boldsymbol{y},X]\) and (ii) the estimated errors \(|\boldsymbol{y}_{\mathcal{V}}-X_{\mathcal{V}}\widehat{\boldsymbol{\beta}}[r, \boldsymbol{y}_{\mathcal{T}},X_{\mathcal{T}}]|_{2}^{2}/n_{\mathcal{V}}\) are close to the true errors \(|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}[r]|_{2}^{2}/n\). Large training sets and large validation sets benefit (i) and (ii), respectively, but the sizes of the sets are subject to a trade-off: \(n_{\mathcal{T}}+n_{\mathcal{V}}=n\). Cross-validation attempts to circumvent this trade-off by averaging multiple splits. In linear regression, this amounts to

\[\widehat{r}_{\text{cv}}\in\operatorname*{arg\,min}_{r\in\mathcal{R}}\widehat {\mathcal{f}}[r]\]

with

\[\widehat{\mathcal{f}}[r]\coloneqq\frac{1}{k}\sum_{j=1}^{k}\frac{1}{n_{ \mathcal{V}_{\mathcal{V}_{\mathcal{f}_{j}}}}}|\boldsymbol{y}_{\mathcal{V}_{ \mathcal{V}_{\mathcal{f}_{j}}}}-X_{\mathcal{V}}\widehat{\boldsymbol{\beta}}[ r,\boldsymbol{y}_{\mathcal{T}_{j}},X_{\mathcal{T}_{j}}]|_{2}^{2}\,,\]

where \(k\in\{1,2,\ldots\}\) is the number of splits, \(\mathcal{T}_{1},\ldots,\mathcal{T}_{k}\) training sets of size \(n_{\mathcal{T}_{1}}\coloneqq|\mathcal{T}_{1}|,\ldots,n_{\mathcal{T}_{k}} \coloneqq|\mathcal{T}_{k}|\in\{1,\ldots,n-1\}\), and \(\mathcal{V}_{1}\coloneqq\{1,\ldots,n\}\setminus\mathcal{T}_{1},\ldots, \mathcal{V}_{k}\coloneqq\{1,\ldots,n\}\setminus\mathcal{T}_{k}\) validation sets of size \(n_{\mathcal{V}_{1}}\coloneqq|\mathcal{V}_{1}|=n-n_{\mathcal{T}_{1}},\ldots,n_{ \mathcal{T}_{k}}\coloneqq|\mathcal{V}_{k}|=n-n_{\mathcal{T}_{k}}\in\{1, \ldots,n-1\}\). In this way, cross-validation can use each sample both for training and for validation.

The simplest cross-validation scheme is _Monte Carlo cross-validation_ (also called repeated random subsamplingvalidation). Given \(k\in\{1,2,\ldots\}\) and \(n_{\mathcal{T}}\in\{1,\ldots n-1\}\), it splits the data \(k\) times uniformly at random such that the training sets have common size \(n_{\mathcal{T}}=n_{\mathcal{T}_{1}}=\cdots=n_{\mathcal{T}_{k}}\).

A slightly more intricate scheme is _\(k\)-fold cross-validation_. Given \(k\in\{2,3,\ldots\}\), it first partitions the data at random or not at random into \(k\) sets \(\mathcal{A}_{1},\ldots,\mathcal{A}_{k}\) of common size \(n/k\) (or approximately of that size if \(n\) is not divisible by \(k\)) and then defines the training and validation sets by \(\mathcal{T}_{j}:=\{1,\ldots,n\}\setminus\mathcal{A}_{j}\) and \(\mathcal{V}_{j}:=\mathcal{A}_{j},j\in\{1,\ldots,k\}\). Two differences to Monte Carlo cross-validation are that the size of the training and validation sets are fixed for a given number of folds \(k\) and that each point is used for validation exactly once (see \(\blacksquare\) Fig. 4.2). The special case \(k=n\), which leads to \(n\)-validations that each use one different sample, is called _leave-one-out cross-validation_.

While cross-validation routinely outmatches other approaches in terms of prediction, it produces models that are comparably complex. This tendency to overfit needs to be taken into account when interpreting the results, and more generally, bear in mind that cross-validation is not designed for anything else than prediction.6

Cross-validation is also subject to other limitations. First, the holdout approach implicitly assumes that the training and validation data are independent. In practice,

Figure 4.2: Cross-validation schemes partition the data into training and validation sets multiple times. Monte Carlo cross-validation has two free parameters: the number of folds (here \(k=3\)) and the size of the training sets (here \(n_{\mathcal{T}}=4\)); \(k\)-fold cross-validation has only one free parameter: the number of folds (here \(k=3\)). Another difference between the two schemes is that Monte Carlo cross-validation partitions the data each time completely at random, which means that a sample can be used for validation repeatedly (orange and black circles) or not at all (blue and green circles), while \(k\)-fold cross-validation uses each sample for validation exactly once

this assumption is often violated: for example, time series data are highly dependent by design.

Next, cross-validation uses data and computing resources inefficiently. By design of the holdout scheme, the training and validation steps use only subsets of the data--which means that the effective sample sizes are much smaller than the actual sample size \(n\). Also, each application of one of the discussed cross-validation schemes requires \(k\) trainings and validations in addition to the computation of the actual estimator.

Then, cross-validation lacks theoretical support. For example, there is no finite sample theory for how cross-validated tuning parameters \(\widehat{r}_{\text{cv}}\) compare to the gold standard \(r^{*}\) for lasso-type estimators.7

Moreover, the splitting procedures often comprise random elements: in \(k\)-fold cross-validation, for example, the partitioning of the data is typically performed uniformly at random. Unless the randomness is "fixed" (by using the same random seed in all users' programs, for example), different users can then get different outputs \(\widehat{\boldsymbol{\beta}}[r]\) on the exact same data.

Finally, cross-validation methods trade the original tuning parameters for new tuning parameters. For example, Monte Carlo cross-validation contains two free parameters, the number of data splits, and the size of the training set; \(k\)-fold cross-validation contains one free parameter and the number of folds/data splits. Just as the original tuning parameters, these parameters are subject to trade-offs. Heuristically, the larger the number of data splits, the more stable but computationally demanding the methods are (because the specific compositions of the training and validation sets are averaged out but the training and validation results have to be computed for each split). The larger the training set, the smaller the bias but the larger the variance (because the approximation of \(\widehat{\boldsymbol{\beta}}[r]\) is more accurate but the estimation of the error more volatile). For \(k\)-fold cross-validation, \(k\in\{5,10\}\) have become standard parameters in this trade-off, but there are few theoretical justifications for any specific choice.

Despite all those limitations, cross-validation schemes are hard to beat in practice. Hence, if data and computational resources are sufficient and the samples fairly independent, cross-validation is the standard calibration approach for prediction.

### Adaptive Validation

The calibration schemes discussed so far estimate prediction errors through least-squares data-fitting errors that are adjusted by using the holdout method (included in the various cross-validation schemes) or complexity measures (included in AIC, BIC, and Mallow's \(C_{p}\)). The selected tuning parameters are then the minimizers of these estimated prediction errors. Adaptive validation, in contrast, evaluates pairwise differences of estimators. The selected tuning parameter is then the stopping point of an algorithm that compares these differences with given error bounds. The two main features of adaptive validation are its optimal guarantees and its computational efficiency.

Our statistical framework for adaptive validation consists of three parts: a family of estimators, a loss function to measure the estimators' errors, and bounds for the estimators in that error measure. The family of estimators \(\{\widehat{\boldsymbol{\beta}}[r]:r\in\mathcal{R}\}\subset\mathbb{R}^{p}\) is indexed by a compact,8 non-empty set of tuning parameters \(\mathcal{R}\subset[0,\infty)\). This setup generalizes the one of \(\blacktriangleright\) Sect. 4.3 in that the estimators can now correspond to arbitrary model classes, such as logistic regression9 and graphical modeling,10 while it restricts that setup in that the tuning parameters must now be real-valued.11

Footnote 11: The _loss function_\(\mathcal{L}:\mathbb{R}^{p}\to[0,\infty)\) is assumed to be symmetric (\(\mathcal{L}[-\boldsymbol{a}]=\mathcal{L}[\boldsymbol{a}]\) for all \(\boldsymbol{a}\in\mathbb{R}^{p}\)), to obey the triangle inequality (\(\mathcal{L}[\boldsymbol{a}+\boldsymbol{b}]\leq\mathcal{L}[\boldsymbol{a}]+ \mathcal{L}[\boldsymbol{b}]\) for all \(\boldsymbol{a}\), \(\boldsymbol{b}\in\mathbb{R}^{p}\)), and, without loss of generality, to be definite (\(\mathcal{L}[\boldsymbol{0}_{p}]=0\)). A loss function, together with a target vector \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\), induces a measure \(\mathcal{L}[\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}[r]]\) for the error of the estimator \(\widehat{\boldsymbol{\beta}}[r]\). In linear regression, for example, the loss function \(\mathcal{L}:\boldsymbol{a}\mapsto\boldsymbol{\|a|}_{\infty}\), together with the regression vector \(\boldsymbol{\beta}\), induces the \(\ell_{\infty}\)-error \(\boldsymbol{\|\beta}-\widehat{\boldsymbol{\beta}}[r]_{\infty}\) (cf. Example 7.3.1).

The bounds finally control the estimators' errors in the measure that is induced by the loss function.

#### Assumption 4.4.1

##### Error Bounds

We assume that for a target vector \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\), a (potentially random) factor \(f\in[0,\infty)\) that is known in practice or can be approximated, and a (potentially random) tuning parameter \(r^{*}\in\mathcal{R}\), all estimators \(\widehat{\boldsymbol{\beta}}[r]\) with \(r\in\mathcal{R}\cap[r^{*},\infty)\) satisfy

\[\mathcal{L}\big{[}\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}[r]\big{]} \leq fr\,.\]* Chapters 6 and 7 discuss a variety of error bounds, but not all of those bounds fit here: to make the below algorithm feasible, the factor \(f\)--or an approximation of it--needs to be known. For example, the prior function bounds in Theorem 7.2.1 do not fit because they involve a measure of the design correlations (namely, the compatibility constant) that can depend crucially on the typically unknown sparsity level (see Example 6.4.2); in contrast, the dual function bounds in Theorem 7.3.2 fit because all necessary parts can be approximated--at least for weakly correlated designs. We illustrate the case of dual function bounds in Example 4.4.1 below.

The tuning parameter \(r^{*}\) is, in the sense of Assumption 4.4.1, the optimal tuning parameter: among all tuning parameters for which there is control, namely all \(r\in\mathcal{R}\cap[r^{*},\infty)\), it leads to the smallest bound. In contrast to the factor \(f\), the tuning parameter \(r^{*}\) does _not_ need to be known. We instead think of \(r^{*}\) with its bound

\[\ell[\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}[r^{*}]]\,\leq fr^{*} \tag{4.3}\]

as the inaccessible gold standard for estimation in terms of the \(\ell\)-loss, and our goal is to have similar guarantees in practice.

Adaptive validation tries to reach this goal by contrasting the above error bounds with pairwise differences of estimators:12

\[\widehat{r}_{\text{av}}\,:=\,\min\,\left\{\,r\in\mathcal{R}\,:\, \ell\big{[}\widehat{\boldsymbol{\beta}}[r^{\prime}]-\widehat{\boldsymbol{ \beta}}[r^{\prime\prime}]\big{]}\leq\widetilde{f}r^{\prime}+\widetilde{f}r^{ \prime\prime}\right.\\ \left.\forall\,r^{\prime},r^{\prime\prime}\in\mathcal{R}\cap[r, \infty)\,\right\}\,, \tag{4.4}\]

where \(\widetilde{f}\) is either equal to \(f\) or to an approximation of it. Adaptive validation essentially selects the smallest tuning parameter that is commensurate with the error bounds in Assumption 4.4.1 to recover a maximum of relevant signal while avoiding overfitting: see \(\blacksquare\) Fig. 4.3.

The following theorem shows that this strategy is indeed successful.

**Optimality of Adaptive Validation**: If \(\tilde{f}\geq f\), it holds that

\[\widehat{\tau}_{\rm av}\,\leq\,r^{*} \tag{4.5}\]

and

\[\ell\big{[}\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}\big{[}\widehat{r}_{ \rm av}\big{]}\big{]}\,\leq\,3\tilde{f}r^{*}\,. \tag{4.6}\]

The first bound ensures that the tuning parameter \(\widehat{\tau}_{\rm av}\) selected by adaptive validation is always smaller or equal to the optimal tuning parameter \(r^{*}\). This result is very useful for support recovery because it provides a safe cutoff for thresholding (see Sect. 7.4, especially Example 7.4.1). The second bound ensures that if \(\tilde{f}=f\), the tuning parameter \(\widehat{\tau}_{\rm av}\) satisfies the optimal bound (4.3) up to a constant factor 3; and it ensures more generally that if \(\tilde{f}\) is sufficiently large but still close enough to \(f\), the tuning parameter \(\widehat{\tau}_{\rm av}\) has similar guarantees as the inaccessible gold standard.

Figure 4.3: Relative magnitudes of the estimated parameters in a generic lasso analysis as a function of the tuning parameter. For large tuning parameters, most of the non-zero estimates (\(\widehat{\beta}_{\rm lasso}\))\({}_{j}\,\neq\,0\) are relevant (\(\beta_{j}\,\neq\,0\); red lines), while for small tuning parameters, many of the non-zero estimates are irrelevant (\(\beta_{j}\,=\,0\); blue lines). Broadly speaking, adaptive validation scans such plots to evaluate when the irrelevant parameters get out of hand. The exact stopping point depends on the loss \(\ell\)

[MISSING_PAGE_FAIL:136]

Additionally to its guarantees in Theorem 4.4.1, adaptive validation is practical: Algorithm 4.1 shows that adaptive validation is easy to implement and needs to compute each estimator \(\widehat{\mathbf{\beta}}[r]\) at most once (see Exercise 4.8). In comparison, \(k\)-fold cross-validation is also easy to implement but needs to compute each estimator or surrogates of it \(k+1\) times (see \(\blacktriangleright\) Sect. 4.3).

A limitation of adaptive validation is that the tuning parameters need to be real-valued. In contrast, cross-validation and AIC-BIC-type schemes allow--in principle--for vector-valued tuning parameters of any dimension. But calibrating vector-valued tuning parameters typically involves large candidate sets \(\mathcal{R}\) (recall \(\blacksquare\) Fig. 1.2, which illustrates that covering spaces of increasing dimensions requires exponentially growing numbers of points) and, therefore, the computation of many estimators \(\widehat{\mathbf{\beta}}[r]\). In effect, the calibration of vector-valued tuning parameters is very challenging.13

Footnote 13: The \(k\)-fold cross-validation is a generalization of the \(k\)-fold cross-validation.

```
Data: data, candidate set \(\mathcal{R}\) (finite), loss function \(\ell\), factor \(\tilde{f}\) Result:\(\widehat{r}_{\text{av}}\in\mathcal{R}\) \(r\leftarrow\max\mathcal{R}\) while\(r\neq\min\mathcal{R}\)do  compute \(\widehat{\mathbf{\beta}}[r]\) for the given data \(r^{\prime}\leftarrow\max\mathcal{R}\) // r\({}^{*}\) not needed due to loop structure while\(r^{\prime}>r\)do  if\(\widehat{\mathbf{\beta}}[r^{\prime}]-\widehat{\mathbf{\beta}}[r]>\tilde{f}^{\prime}+ \tilde{f}r\)then \(r\leftarrow\min[\mathcal{R}\cap(r,\infty)]\) exit from both loops \(r^{\prime}\leftarrow\max[\mathcal{R}\cap(0,r^{\prime})]\)  end if \(r\leftarrow\max[\mathcal{R}\cap(0,r)]\) end while \(\widehat{r}_{\text{av}}\gets r\)
```

**Algorithm 4.1**Pseudocode for adaptive validation (4.4)

But the crucial requirement for adaptive validation is a set of suitable error bounds. Since such bounds typically concern parameter estimation and support recovery rather than prediction14 (see the example below), adaptive validation and cross-validation complement each other.

Footnote 14: The \(k\)-fold cross-validation is a generalization of the \(k\)-fold cross-validation.

```
Data: data, candidate set \(\mathcal{R}\) (finite), loss function \(\ell\), factor \(\tilde{f}\) Result:\(\widehat{r}_{\text{av}}\in\mathcal{R}\) \(r\leftarrow\max\mathcal{R}\) while\(r\neq\min\mathcal{R}\)do  compute \(\widehat{\mathbf{\beta}}[r]\) for the given data \(r^{\prime}\leftarrow\max\mathcal{R}\) // r\({}^{*}\) not needed due to loop structure while\(r^{\prime}>r\)do  if\(\widehat{\mathbf{\beta}}[r^{\prime}]-\widehat{\mathbf{\beta}}[r]>\tilde{f}^{\prime}+ \tilde{f}r\)then \(r\leftarrow\min[\mathcal{R}\cap(r,\infty)]\) exit from both loops \(r^{\prime}\leftarrow\max[\mathcal{R}\cap(0,r^{\prime})]\)  end if \(r\leftarrow\max[\mathcal{R}\cap(0,r)]\)  end while \(\widehat{r}_{\text{av}}\gets r\)
```

**Algorithm 4.2**Pseudocode for adaptive validation (4.4)

[MISSING_PAGE_FAIL:138]

Example 7.4.1 demonstrates that adaptive validation together with thresholding can also yield accurate support recovery. Hence, adaptive validation is recommended when calibrating the lasso for support recovery and parameter estimation in terms of the \(\ell_{\infty}\)-loss, while cross-validation is the method of choice for prediction. 

### 4.5 Exercises

#### 4.5.1 Exercises for Sect. 4.2

**Exercise 4.1**\(\Diamond\) In this exercise, we verify the identity that underpins our interpretation of the lasso's effective noise. Show that

\[2\|X^{\top}\boldsymbol{u}\|_{\infty}\,=\,2\|\boldsymbol{u}\|_{2}\max_{j\in[1, \ldots,p]}\big{\{}\,|\,\mathrm{cor}[\boldsymbol{u},\boldsymbol{x}_{j}]|\cdot \|\boldsymbol{x}_{j}\|_{2}\big{\}}\,.\]

Use the definition of the correlation on Page 4.2 and \(X=(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{p})\).

**Exercise 4.2**\(\blacklozenge\blacklozenge\blacklozenge\) In this exercise, we establish the union bound (also called Boole's inequality), which we have used in the proof of Lemma 4.2.1.

Show that for all events \(\mathcal{A}_{1},\ldots,\mathcal{A}_{k}\), \(k\in\{1,2,\ldots\}\), it holds that

\[\mathbb{P}\bigg{\{}\,\bigcup_{j=1}^{k}\mathcal{A}_{j}\bigg{\}}\,\,\leq\,\,\sum _{j=1}^{k}\mathbb{P}\{\mathcal{A}_{j}\}\,.\]

Use only the following two properties of probability measures: (i) finite additivity, that is, \(\mathbb{P}\{\mathcal{A}\cup\mathcal{B}\}=\mathbb{P}\{\mathcal{A}\}+\mathbb{P} \{\mathcal{B}\}\) for all disjoint events \(\mathcal{A}\), \(\mathcal{B}\); (ii) positivity, that is, \(\mathbb{P}\{\mathcal{A}\}\geq 0\) for every event \(\mathcal{A}\). (That these two properties suffice indicates that the union bound also holds for functions other than probability measures.)

**Exercise 4.3**\(\blacklozenge\blacklozenge\) In this exercise, we derive the tail bound for Gauss-distributed random variables that we have used in the proof of Lemma 4.2.1.

Show that any standard Gauss random variable \(z\sim\mathcal{N}_{1}[0,1]\) fulfills

\[\mathbb{P}\big{\{}\,|z|\,\geq\,a\,\big{\}}\,\leq\,e^{-\frac{a^{2}}{2}}\]for every \(a\in[0,\,\infty)\). The plot on the left depicts \(\mathbb{P}\{|z|\geq a\}\) (red, solid line) and \(e^{-\frac{a^{2}}{2}}\) (blue, dashed line) as functions of \(a\).

**Exercise 4.4**: \(\Diamond\) In this exercise, we generalize the tail bound for the effective noise in Lemma 4.2.1.

1. Prove a tail bound as in Lemma 4.2.1 for general \(2v\|X^{\top}\mathbf{u}\|_{\infty}\), \(v\in(0,\infty)\).
2. Prove a tail bound as in Lemma 4.2.1 for \(2|X^{\top}\mathbf{u}\|_{\infty}\) with noise \(\mathbf{u}\) distributed according to a Laplace distribution (also called double-exponential distribution).

#### 4.5.2 Exercises for Sect. 4.3

**Exercise 4.5**: \(\blacklozenge\) In this exercise, we study the optimal tuning parameter defined in the minimization (4.1) and its estimation via the minimization (4.2). For this, we consider some general settings in plots A-D:

1. Consider Plot A: Mark the optimal tuning parameter \(r^{*}\). Motivate that in the depicted case, \(\widehat{r}\approx r^{*}\) yields \(\ell^{*}[\widehat{r}]\approx\ell^{*}[r^{*}]\).
2. Consider Plot B: Assume that the estimated prediction error \(\widehat{\ell}\) is close to the true error \(\ell^{*}\) in the sense that the values \(\widehat{\ell}[r]\) are in the red shaded area around \(\ell^{*}[r]\) for all \(r\in\mathcal{R}\). Can you restrict the possible values of \(\widehat{r}\)? Conclude that typically \(\widehat{\ell}\) being close to the true error \(\ell^{*}\) across all tuning parameters \(r\in\mathcal{R}\) yields \(\widehat{r}\approx r^{*}\).
3. Consider Plot C: Mark \(r^{*}\) and \(\widehat{r}\). Conclude that \(\widehat{\mathcal{F}}\) being close to the true error \(\ell^{*}\) is not a necessary condition for \(\widehat{r}\) being close to \(r^{*}\).
4. Consider Plot D: Assume that the \(x\)-axis covers the entire set of possible tuning parameters \(\mathcal{R}\). Argue that then in the depicted setting, any calibration scheme would do fine in the sense that \(\ell^{*}[\widehat{r}]\approx\ell^{*}[r^{*}]\). Does this require \(\widehat{r}\approx r^{*}\)?

Cross-validation aims at finding a tuning parameter \(\widehat{r}\) such that \(\ell^{*}[\widehat{r}]\approx\ell^{*}[r^{*}]\), where \(\ell^{*}\) is the (scaled) prediction error: \(\ell^{*}[r]:=\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}[r]\|_{2}^{2}/n\), \(r\in\mathcal{R}\). Question 1 indicates that a typical case where this holds is \(\widehat{r}\approx r^{*}\), and Question 2 indicates that a typical case where the latter holds is \(\widehat{\mathcal{F}}[r]\approx\ell^{*}[r]\) for all \(r\in\mathcal{R}\).

On the other hand, Question 3 shows that \(\widehat{\mathcal{F}}\approx\ell^{*}\) is _not a necessary_ condition for \(\widehat{r}\approx r^{*}\), and Question 4 shows that \(\widehat{r}\approx r^{*}\) is _not a necessary_ condition for \(\ell^{*}[\widehat{r}]\approx\ell^{*}[r^{*}]\) either. Hence, the only measure of success is how well \(\ell^{*}[\widehat{r}]\) approximates \(\ell^{*}[r^{*}]\).

**Exercise 4.6**: \(\blacklozenge\) In this exercise, we highlight the limitations of cross-validation outside of prediction.

Assume that we want to find a data-driven version of

\[r^{*}\in\operatorname*{arg\,min}_{r\in\mathcal{R}}\ell^{*}[r]\,,\;\;\;\text{ where }\ell^{*}[r]:=\|\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}[r]\|_{2}^{2}\,.\]

Is cross-validation suited for this?

#### Exercises for \(\blacktriangleright\) Sect. 4.4

**Exercise 4.7**: In this exercise, we verify that \(\widehat{r}_{\text{av}}\) in the adaptive validation scheme (4.4) is well-defined. Indeed: show that the minimum in the formulation of adaptive validation is always attained in \(\mathcal{R}\) and unique. Hint: use the fact that by assumption, \(\mathcal{R}\) is compact and non-empty and \(\mathcal{L}[\boldsymbol{0}_{p}]=0\).

**Exercise 4.8**: In this exercise, we confirm the computational properties of the adaptive validation scheme (4.4).

1. Show that Algorithm 4.1 involves less than \(|\mathcal{R}|^{2}\) contrasts of the form \(\mathcal{L}[\widehat{\boldsymbol{\beta}}[r^{\prime}]-\widehat{\boldsymbol{ \beta}}[r]]>\widehat{f}r^{\prime}+\widehat{f}r\). Evaluatingsuch a contrast is typically cheap in comparison to evaluating an estimator \(\widehat{\boldsymbol{\beta}}[r]\), which can require elaborate descent algorithms, for example. This means that for reasonably small \(\mathcal{R}\), the computational effort for evaluating the contrasts is negligible.
2. Show that for all \(r\), \(s\) such that \(r\geq s\), it holds that \[\exists\,r^{\prime},r^{\prime\prime}\in\mathcal{R}\cap[r,\infty)\;:\;\ell \big{[}\widehat{\boldsymbol{\beta}}[r^{\prime}]-\widehat{\boldsymbol{\beta}}[r^ {\prime\prime}]\big{]}\,>\tilde{f}r^{\prime}+\tilde{f}r^{\prime\prime}\] \[\Downarrow\] \[\exists\,r^{\prime},r^{\prime\prime}\in\mathcal{R}\cap[s,\infty)\;:\;\ell \big{[}\widehat{\boldsymbol{\beta}}[r^{\prime}]-\widehat{\boldsymbol{\beta}}[r^ {\prime\prime}]\big{]}\,>\tilde{f}r^{\prime}+\tilde{f}r^{\prime\prime}\,.\] This inclusion implies that if \(r\) is an infeasible tuning parameter, that is, \(\max\{\ell[\widehat{\boldsymbol{\beta}}[r^{\prime}]-\widehat{\boldsymbol{ \beta}}[r]]-\tilde{f}r^{\prime}-\tilde{f}r\,:\;r^{\prime}\in\mathcal{R}\cap(r,\infty)\}\,>\,0\), then also \(s\) is an infeasible tuning parameter, that is, \(\max\{\ell[\widehat{\boldsymbol{\beta}}[r^{\prime}]-\widehat{\boldsymbol{ \beta}}[s]]-\tilde{f}r^{\prime}-\tilde{f}s\,:\;r^{\prime}\in\mathcal{R}\cap(s,\infty)\}\,>\,0\). This property allows Algorithm 4.1 to compute adaptive validation with _early stopping_: as soon the downward search reaches an infeasible tuning parameter, the computations can be stopped. Hence, some estimators \(\widehat{\boldsymbol{\beta}}[r]\) do not need to be evaluated altogether.

This exercise demonstrates that adaptive validation can be computationally more efficient than AIC-BIC-type methods (which require exactly one evaluation of each estimator) and cross-validation methods (which require more than one evaluation of each estimator).17

**Exercise 4.9**: \(\diamondsuit\diamondsuit\) In this exercise, we generalize adaptive validation to incorporate a slightly broader range of error bounds. We assume that for a target vector \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\), a (potentially random) non-decreasing function \(\ell:\mathbb{R}\to\mathbb{R}\) that is known in practice or can be approximated, and a (potentially random) tuning parameter \(r^{*}\,\in\,\mathcal{R}\), all estimators \(\widehat{\boldsymbol{\beta}}[r]\) with \(r\in\mathcal{R}\cap[r^{*},\infty)\) satisfy

\[\ell\big{[}\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}[r]\big{]}\,\leq\, \ell\big{[}r\big{]}\,.\]

You now have two tasks:

1. Show that the above set of assumptions generalizes Assumption 4.4.1.
2. Modify the adaptive validation scheme (4.4) to accommodate the above assumptions and prove a corresponding version of Theorem 4.4.1.

This generalization suggests that the concept of adaptive validation could be useful also outside the scope of high-dimensional statistics.[18]

### 4.6 R Lab: Cross-Validation

In this lab, we study cross-validation. As always, replace the keyword REPLACE with suitable code and answer the questions in the text.

#### Data Generation

Generate data from a linear regression model \(\boldsymbol{y}=X\boldsymbol{\beta}+\boldsymbol{u}\), where the entries of the design \(X\in\mathbb{R}^{100\times 500}\) are sampled independently according to \(X_{ij}\sim\mathcal{M}[0,1]\), the regression vector is \(\boldsymbol{\beta}=(1,1,1,0,0,\ldots,0)^{\top}\in\mathbb{R}^{500}\), and the noise is sampled independently from the design according to \(\boldsymbol{u}\sim\mathcal{N}_{100}[\boldsymbol{0}_{100},\text{I}_{100},100]\).

 set.seed(1)  n <- 100; P <- 500  design <- REPLACE  regression.vector <- REPLACE  outcome <- REPLACE  cbind(outcome[1:4], design[1:4, 1:4])

## [,1] [,2] [,3] [,4] [,5]
## [1,] -0.3115278 -0.6264538 -0.62036668 0.4094018 0.8936737
## [2,] 1.4270881 0.1836433 0.04211587 1.6888733 -1.0472981
## [3,] 0.9782980 -0.8356286 -0.91092165 1.5865884 1.9713374
## [4,] 2.6375362 1.5952808 0.18502877 -0.3309078 -0.3836321

#### Computing a Set of Estimators

Compute lasso estimates via the glmnet package. Let the glmnet function generate 50 tuning parameters and store these tuning parameters for later use. Set the flag intercept=FALSE throughout. Use glmnet's standard options otherwise. Plot the average prediction error \(|X\boldsymbol{\beta}-X\boldsymbol{\hat{\beta}}|_{2}^{2}/n\) as a function of the tuning parameter.

[MISSING_PAGE_EMPTY:6363]

[MISSING_PAGE_FAIL:145]

[MISSING_PAGE_EMPTY:6365]

[MISSING_PAGE_EMPTY:6366]

suggested by the mentioned condition--see their Corollary 4.2 on p. 308, for example. Further results in this direction have derived in Dalalyan et al. (2017). Hence, although the effective noise \(2\left|X^{\top}\mathbf{\pi}\right|_{\infty}\) is intimately connected with the lasso's tuning parameter and with the lasso more generally, it does not account for the entire intricacy of tuning-parameter calibration.
* See Hebiri and Lederer (2013, Proof of Theorem 3.2 on pp. 16-18), for example.
* This has been noted in Hebiri and Lederer (2013); in particular, their Theorem 3.2 on pp. 16-18 is a refinement of Lemma 4.2.1 for correlated designs--but see the second note above.
* Two classical references for cross-validation are Geisser (1975); Stone (1974).
* For more details on incommensurable goals in estimating \(\mathbf{\beta}\), including the so-called _AIC-BIC dilemma_, see Arlot and Celisse (2010, Section 2.4 on p. 48).
* An overview about the existing theory for cross-validation and about the corresponding literature is provided in Arlot and Celisse (2010). Nonasymptotic expressions for the variance of the (again out-of-sample) risk of projection estimators in regression can be found in Celisse (2008, Proposition 3.4.3 on p. 66). Some theoretical bounds for cross-validated lasso can be found in Chatterjee and Jafarov (2015) and Homrighausen and McDonald (2013a,b, 2014). Much more is known about cross-validated ridge regression, see Golub et al. (1979) for example.
* A set \(\mathcal{A}\subset\mathbb{R}\) is _compact_ if it is bounded and closed: \(\mathcal{A}\subset[b,c]\) for some \(b\), \(c\in\mathbb{R}\), and \(\lim_{i=1}^{\infty}a_{i}\in\mathcal{A}\) for every sequence \(a_{1},a_{2},\ldots\in\mathcal{A}\) that converges in \(\mathbb{R}\). In particular, finite sets are compact, and since all known implementations of both cross-validation and adaptive validation are limited to finite candidate sets \(\mathcal{R}\), compactness does not impose any practical restrictions.
* Adaptive validation for estimation and support recovery in \(\ell_{1}\)-regularized logistic regression is discussed in Li and Lederer (2019). The cross-validation schemes described in \(\blacktriangleright\) Sect. 4.3 can also be adapted to logistic regression--see the general treatment in the classical paper Geisser (1975, Section 2 on pp. 321-322), for example.
* Adaptive validation for estimation and support recovery in \(\ell_{1}\)-regularized graphical modeling is discussed in Laszkiewicz et al. (2020). In graphical modeling, the coordinates of the estimators \(\hat{\mathbf{\beta}}[r]\in\mathbb{R}^{p}\) are identified with the entries of the precision matrix or the edges of the corresponding graph; in particular, \(p=d\times d\) or \(p=d\times(d-1)/2\), respectively, where \(d\) the number of nodes--see \(\blacktriangleright\) Chap. 3. Because graphical models do not generate an immediate prediction task, they are also not amenable to cross-validation directly.
* The property of real values needed for the treatment here is that they are ordered, which allows a well-defined downward search from large to small tuning parameters.
* Adaptive validation was introduced by Chichignoud et al. (2016). Our definition in (4.4) generalizes their Definition 2 on p. 4 beyond the lasso and beyond sup-norm bounds, and our Theorem 4.4.1 generalizes their Theorem 3 on p. 5 accordingly. The original paper defines the optimal tuning parameter via tail bounds (Chichignoud et al., 2016, Definition 1 on p. 4), while we 

[MISSING_PAGE_FAIL:149]

## References

* [1] The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
* [2] J. Lederer, _Fundamentals of High-Dimensional Statistics_, Springer Texts in Statistics, [https://doi.org/10.1007/978-3-030-73792-4.5](https://doi.org/10.1007/978-3-030-73792-4.5)In the preceding sections, we have discussed the estimation of target parameters, such as the elements of the regression vector \(\boldsymbol{\beta}\) in linear regression. In this section, our task is to complement these estimates with measures of uncertainty. We call this task _inference_. Our first step is to introduce an algorithm for computing estimators that are defined through systems of equations. However, rather than the actual computations of these estimators, we are interested in modifying other estimators by applying only a single update step of the algorithm. We call estimators that are generated this way _one-step estimators_ (see Sect. 5.1). Our second step is to show that some one-step estimators can be interpreted as intermediate points between a classical estimator and a high-dimensional estimator and can be separated into three parts: (1) the regression vector of the underlying regression model, which ensures unbiasedness, (2) a distributional term that is inherited from the classical estimator and that is, therefore, amenable to classical distribution theory, and (3) a remainder that depends only on the high-dimensional estimator and that is, therefore, amenable to high-dimensional estimation bounds. These observations finally allow us to derive confidence intervals for individual coordinates of regression vectors (see Sect. 5.2).

### One-Step Estimators

Many classical estimators are Z-estimators, which means that they are solutions of systems of equations. Such estimators can be computed via an iterative scheme called the Newton-Raphson algorithm. In fact, if the initial guess in this algorithm is sufficiently good, a _single_ iteration of the algorithm can already yield an estimator that has the desired properties. Estimators that are based on such a single iteration are called one-step estimators.

A _Z-estimator_1,2 is a solution of a system of equations:

\[\widehat{\boldsymbol{\beta}}\;\in\;\mathscr{B}\;\;\;\mbox{is such that}\;\;\;\boldsymbol{s}_{Z}[\widehat{\boldsymbol{\beta}}]\;=\;\boldsymbol{0}_{d} \tag{5.1}\]

with a given vector-valued function

\[\boldsymbol{s}_{Z}\;:\;\mathscr{B} \;\rightarrow\;\mathbb{R}^{d}\] \[\boldsymbol{\alpha} \;\mapsto\;\boldsymbol{s}_{Z}[\boldsymbol{\alpha}]\]

[MISSING_PAGE_FAIL:152]

and that \(\max_{\mathbf{a}}(b\diagup\mathbf{\ell}[\mathbf{a}]+c)=\min_{\mathbf{a}}-\diagup\mathbf{\ell}[\mathbf{a}]\) for every positive constant \(b\), real-valued constant \(c\), and real-valued function \(\diagup\mathbf{\ell}\))

\[\operatorname*{arg\,max}_{\mathbf{\alpha}\in\mathbb{R}^{p}}\,\ell[\mathbf{\alpha}]\,=\, \operatorname*{arg\,min}_{\mathbf{\alpha}\in\mathbb{R}^{p}}\,[\mathbf{y}-X\mathbf{\alpha}] _{2}^{2}\,.\]

Hence, the maximum likelihood estimator in this case is the least-squares estimator. Taking derivatives of the objective function, we find that \(\widehat{\mathbf{\beta}}_{\text{ls}}\) is a least-squares estimator if and only if \(-2X^{\top}(\mathbf{y}-X\widehat{\mathbf{\beta}}_{\text{ls}})=\mathbf{0}_{p}\) (cf. \(\blacktriangleright\) Sect. 1.2), which means that the least-squares estimator is the following Z-estimator:

\[\widehat{\mathbf{\beta}}_{\text{ls}}\,\in\,\mathbb{R}^{p}\,\,\,\text{ is such that }\,\,\mathbf{\lambda}_{Z}[\widehat{\mathbf{\beta}}_{\text{ls}}]\,=\,\mathbf{0}_{d}\]

with \(\mathbf{\lambda}_{Z}\) the score function

\[\mathbf{\lambda}_{Z}\,:\,\mathbf{\alpha}\,\mapsto\,\,-2X^{\top}(\mathbf{y}-X\mathbf{\alpha})\,.\]

It is clearly visible that the specification of the score function \(\mathbf{\lambda}_{Z}\) depends on the data \(Z=(\mathbf{y},X)\). \(\blacktriangleleft\)

The system of Eqs. (5.1) is typically solved for \(\widehat{\mathbf{\beta}}\) iteratively. Each iteration is an update step \(\widehat{\mathbf{\beta}}^{i}\,\mapsto\,\widehat{\mathbf{\beta}}^{i+1}:=\widehat{\mathbf{ \beta}}^{i}+\mathbf{\delta}[\widehat{\mathbf{\beta}}^{i}]\), where \(\widehat{\mathbf{\beta}}^{i}\in\mathbb{R}^{p}\) is the current state of the algorithm, and \(\mathbf{\delta}:\mathbb{R}^{p}\to\mathbb{R}^{p}\) is a vector-valued function that is supposed to point from its argument to a solution \(\widehat{\mathbf{\beta}}\). An ideal function \(\mathbf{\delta}^{*}[\widehat{\mathbf{\beta}}^{i}]\) would generate a solution within one iteration:

\[\mathbf{\lambda}_{Z}[\widehat{\mathbf{\beta}}^{i}+\mathbf{\delta}^{*}[\widehat{\mathbf{\beta }}^{i}]]\,=\,\mathbf{0}_{d}\,.\]

Finding such ideal update vectors is as difficult as finding solutions to the systems of equations in the first place, but the display motivates a practical iteration: We first Taylor expand the function \(\mathbf{\lambda}_{Z}\) around the current state \(\widehat{\mathbf{\beta}}^{i}\) (assuming that \(\mathbf{\lambda}_{Z}\) is smooth):

\[\mathbf{\lambda}_{Z}[\widehat{\mathbf{\beta}}^{i}+\mathbf{\delta}^{*}[\widehat{\mathbf{\beta }}^{i}]]\,\approx\,\mathbf{\lambda}_{Z}[\widehat{\mathbf{\beta}}^{i}]+\Big{(}\frac{ \mathbf{\partial}\mathbf{\lambda}_{Z}[\mathbf{\alpha}]}{\mathbf{\partial}\mathbf{\alpha}}\Big{|}_{ \mathbf{\alpha}=\widehat{\mathbf{\beta}}^{i}}\Big{)}\mathbf{\delta}^{*}[\widehat{\mathbf{\beta }}^{i}]\]

with the _Jacobi matrix_

\[\Big{(}\frac{\mathbf{\partial}\mathbf{\lambda}_{Z}[\mathbf{\alpha}]}{\mathbf{ \partial}\mathbf{\alpha}}\Big{|}_{\mathbf{\alpha}=\widehat{\mathbf{\beta}}^{i}}\Big{)}_{ kl}:=\frac{\partial\mathbf{\lambda}_{Z}[\mathbf{\alpha}]}{\partial\alpha_{I}}\Big{|}_{\mathbf{ \alpha}=\widehat{\mathbf{\beta}}^{i}}\\ \text{for all }k\in\{1,\ldots,d\},l\in\{1,\ldots,p\}\,.\]Plugging this expansion into the previous display yields

\[\boldsymbol{\mathfrak{z}}_{Z}[\widehat{\boldsymbol{\beta}}^{i}]+\Big{(}\frac{ \boldsymbol{\mathfrak{d}}\boldsymbol{\mathfrak{z}}[\boldsymbol{\mathfrak{a}}]}{ \boldsymbol{\mathfrak{a}}\boldsymbol{\mathfrak{a}}}\Big{|}_{\boldsymbol{ \mathfrak{a}}=\widehat{\boldsymbol{\beta}}^{i}}\Big{)}\boldsymbol{\mathfrak{d}}^ {*}[\widehat{\boldsymbol{\beta}}^{i}]\,\approx\,\boldsymbol{0}_{d}\,.\]

Assuming that the Jacobi is a square matrix (\(d=p\)) and invertible, we can then solve for \(\boldsymbol{\mathfrak{d}}^{*}[\widehat{\boldsymbol{\beta}}^{i}]\):

\[\boldsymbol{\mathfrak{d}}^{*}[\widehat{\boldsymbol{\beta}}^{i}]\,\approx\,\,- \,\Big{(}\frac{\boldsymbol{\mathfrak{d}}\boldsymbol{\mathfrak{z}}_{Z}[ \boldsymbol{\mathfrak{a}}]}{\boldsymbol{\mathfrak{a}}\boldsymbol{\mathfrak{a}}} \Big{|}_{\boldsymbol{\mathfrak{a}}=\widehat{\boldsymbol{\beta}}^{i}}\Big{)}^{-1 }\boldsymbol{\mathfrak{d}}_{Z}[\widehat{\boldsymbol{\beta}}^{i}]\,.\]

This insight is finally formulated in the update rule

\[\widehat{\boldsymbol{\beta}}^{i}\,\mapsto\,\,\widehat{\boldsymbol{\beta}}^{i+ 1}\,:=\,\widehat{\boldsymbol{\beta}}^{i}-\Big{(}\frac{\boldsymbol{\mathfrak{d }}\boldsymbol{\mathfrak{z}}_{Z}[\boldsymbol{\mathfrak{a}}]}{\boldsymbol{ \mathfrak{a}}\boldsymbol{\mathfrak{a}}}\Big{|}_{\boldsymbol{\mathfrak{a}}= \widehat{\boldsymbol{\beta}}^{i}}\Big{)}^{-1}\boldsymbol{\mathfrak{d}}_{Z}[ \widehat{\boldsymbol{\beta}}^{i}]\,, \tag{5.2}\]

which is called the _Newton-Raphson algorithm_.

[style=MyFrame] **Examples:**

**Least-Squares Estimator Revisited 1**: In the previous example, we have demonstrated that the least-squares estimator is a Z-estimator of the form

\[\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\,\in\,\mathbb{R}^{p}\,\,\,\,\,\text{is such that}\,\,\,\,\boldsymbol{\mathfrak{s}}_{Z}[\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}]\,=\,\boldsymbol{0}_{d}\]

with

\[\boldsymbol{\mathfrak{s}}_{Z}\,:\,\boldsymbol{\mathfrak{a}}\,\mapsto\,\,-2X^{ \top}(\boldsymbol{y}-X\boldsymbol{\mathfrak{a}})\,.\]

Assuming that the Gram matrix \(X^{\top}X\) is invertible, we can solve for \(\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\) directly to recover the well-known formula (cf. \(\blacktriangleright\) Sect. 1.2)

\[\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\,=\,(X^{\top}X)^{-1}X^{\top} \boldsymbol{y}\,.\]

But we can also compute \(\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\) via the Newton-Raphson scheme. The Jacobi matrix is a multiple of the Gram matrix:

\[\frac{\boldsymbol{\mathfrak{d}}\boldsymbol{\mathfrak{z}}[\boldsymbol{\mathfrak{ a}}]}{\boldsymbol{\mathfrak{a}}\boldsymbol{\mathfrak{a}}}\,=\,2X^{\top}X\qquad \quad\text{ for all }\boldsymbol{\mathfrak{a}}\,\in\,\mathbb{R}^{p}\,,\]

hence,

\[\Big{(}\frac{\boldsymbol{\mathfrak{d}}\boldsymbol{\mathfrak{z}}[\boldsymbol{ \mathfrak{a}}]}{\boldsymbol{\mathfrak{a}}\boldsymbol{\mathfrak{a}}}\Big{|}_{ \boldsymbol{\mathfrak{a}}=\widehat{\boldsymbol{\beta}}^{i}}\Big{)}^{-1}\,=\, \frac{(X^{\top}X)^{-1}}{2}\]irrespective of \(\widehat{\boldsymbol{\beta}}^{i}\). The first Newton-Raphson update is, consequently,

\[\widehat{\boldsymbol{\beta}}^{1} = \widehat{\boldsymbol{\beta}}^{0}-\Big{(}\frac{\boldsymbol{\hat{ \vartheta}}\boldsymbol{\mathsf{z}}\boldsymbol{\mathsf{z}}\boldsymbol{\mathsf{z}} \boldsymbol{\mathsf{z}}}{\boldsymbol{\mathsf{\vartheta}}\boldsymbol{\mathsf{ \alpha}}}\Big{|}_{\boldsymbol{\mathsf{\alpha}}=\widehat{\boldsymbol{\beta}}^{0 }}\Big{)}^{-1}\boldsymbol{\mathsf{z}}_{Z}[\widehat{\boldsymbol{\beta}}^{0}]\] Newton-Raphson update (5.2) \[= \widehat{\boldsymbol{\beta}}^{0}-\frac{(X^{\top}X)^{-1}}{2}\, \big{(}-2X^{\top}(\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}^{0})\big{)}\] using the previous display and the definition of \[\boldsymbol{\mathsf{z}}_{Z}\] directly \[= \widehat{\boldsymbol{\beta}}^{0}+(X^{\top}X)^{-1}X^{\top} \boldsymbol{y}-(X^{\top}X)^{-1}X^{\top}X\widehat{\boldsymbol{\beta}}^{0}\] linearity of matrix multiplications \[= (X^{\top}X)^{-1}X^{\top}\boldsymbol{y}\,,\] consolidating which, according to (1.4), is the least-squares estimator. Further iterations of the Newton-Raphson scheme have no effect: \[\widehat{\boldsymbol{\beta}}^{2} = \widehat{\boldsymbol{\beta}}^{1}+(X^{\top}X)^{-1}X^{\top} \boldsymbol{y}-(X^{\top}X)^{-1}X^{\top}X\widehat{\boldsymbol{\beta}}^{1}\] as above \[= \widehat{\boldsymbol{\beta}}^{1}+(X^{\top}X)^{-1}X^{\top} \boldsymbol{y}-(X^{\top}X)^{-1}X^{\top}X(X^{\top}X)^{-1}X^{\top}\boldsymbol{y}\] \[\widehat{\boldsymbol{\beta}}^{1}=(X^{\top}X)^{-1}X^{\top} \boldsymbol{y}\text{ according to the previous display}\] \[= \widehat{\boldsymbol{\beta}}^{1}\,.\] consolidating This means that the Newton-Raphson algorithm converges to a solution in a _single_ iteration--irrespective of the starting point.

The Newton-Raphson algorithm can be generalized to avoid computationally challenging inversions of Jacobi matrices or to account for singular or non-square (\(d\neq p\)) Jacobi matrices, which cannot be inverted altogether. The generalization consists of replacing the inverse of the Jacobi matrix by an approximation, which can be the actual inverse if it exists, a Moore-Penrose inverse, or some other approximate inverse. Given such an approximate inverse \(M_{\boldsymbol{\mathsf{z}}_{Z}}\in\mathbb{R}^{p\times d}\), the update rule (5.2) generalizes to

\[\widehat{\boldsymbol{\beta}}^{i}\,\mapsto\,\widehat{\boldsymbol{\beta}}^{i+1} \,:=\,\widehat{\boldsymbol{\beta}}^{i}-M_{\boldsymbol{\mathsf{z}}_{Z}} \boldsymbol{\mathsf{z}}_{Z}[\widehat{\boldsymbol{\beta}}^{i}]\,, \tag{5.3}\]

which is called a _generalized Newton-Raphson algorithm_.

[MISSING_PAGE_EMPTY:6375]

still be valuable from a statistical perspective: roughly speaking, it can transform an estimator that is consistent but has an intractable distribution into an estimator that is amenable to inference. This feature of Newton-Raphson updates has been long known in classical statistics;3 we will show in the next section that it also stands in high-dimensional statistics. For further reference, we thus call the output of single (generalized) Newton-Raphson iteration applied to an initial estimator a _one-step estimator_.

### Confidence Intervals

According to Sect. 1.3, the prior terms in high-dimensional estimators include prior information by favoring certain model parameters. But this favoring also generates a bias. The bias is usually small enough in terms of prediction and estimation, but it can render inference such as the construction of confidence intervals infeasible.4 In this section, we try to remove the bias through single Newton-Raphson iterations in the direction of classical Z-estimators such as the least-squares.5 This debiasing does _not_ usually improve the prediction or estimation accuracy,6 but the distributions of the resulting one-step estimators are suitable for inference.

To fix ideas, we construct confidence intervals for the \(j\)th coordinate of a regression vector \(\boldsymbol{\beta}\). Our starting point is a high-dimensional estimator \(\widehat{\boldsymbol{\beta}}^{0}=\widehat{\boldsymbol{\beta}}\) such as the lasso. We update this initial estimator through a generalized Newton-Raphson step (5.3) in the direction of a least-squares solution (see Example 5.1.3 in the previous section):

\[\widehat{\boldsymbol{\beta}}^{0}\,\mapsto\,\widehat{\boldsymbol{\beta}}^{1}\, :=\,2M_{\boldsymbol{\delta}_{Z}}X^{\top}\boldsymbol{y}+(\mathsf{I}_{p\times p} -2M_{\boldsymbol{\delta}_{Z}}X^{\top}X)\widehat{\boldsymbol{\beta}}^{0}\,, \tag{5.4}\]

where \(M_{\boldsymbol{\delta}_{Z}}\in\mathbb{R}^{p\times p}\) is an approximate inverse of the Jacobi matrix \(2X^{\top}X\). If the Gram matrix \(X^{\top}X\) is invertible and \(M_{\boldsymbol{\delta}_{Z}}=(X^{\top}X)^{-1}/2\), the update yields the least-squares estimator (see Example 5.1.2). In general, the output is different, which is reassuring, because the least-squares estimator is not suitable for high-dimensional inference: for example, 2. in Exercise 1.2 illustrates that least-squares estimation is even ambiguous as soon as \(p>n\). What the above update returns in general is rather a _mix_ of the initial estimator and the least-squares estimator in an attempt to borrow the least-squares' unbiasedness (see Sect. 1.3) while maintaining the initial estimator's ability to cope with high-dimensionality. We thus call the updating step _debiasing_.7

Debiased estimators can be separated into three parts, which are the regression vector, the approximate distribution around the regression vector, and the remainder:

\[\widehat{\boldsymbol{\beta}}^{1} = 2M_{\delta_{Z}}X^{\top}\boldsymbol{y}+\big{(}\mathrm{I}_{p \times p}-2M_{\delta_{Z}}X^{\top}X\big{)}\widehat{\boldsymbol{\beta}}^{0}\] \[= 2M_{\delta_{Z}}X^{\top}(X\boldsymbol{\beta}+\boldsymbol{u})+ \big{(}\mathrm{I}_{p\times p}-2M_{\delta_{Z}}X^{\top}X\big{)}\widehat{ \boldsymbol{\beta}}^{0}\] \[= \underbrace{\boldsymbol{\beta}}_{\text{regression}}+\underbrace{2M _{\delta_{Z}}X^{\top}\boldsymbol{u}}_{\text{distributional}}+\underbrace{(2M_{ \delta_{Z}}X^{\top}X-\mathrm{I}_{p\times p})(\boldsymbol{\beta}-\widehat{ \boldsymbol{\beta}}^{0})}_{\text{remainder}}\,.\]

The distributional term is, given \(X\) and \(M_{\delta Z}\), a linear function of the noise \(\boldsymbol{u}\); this term has, therefore, a tractable distribution given \(X\) and \(M_{\delta Z}\) as long as the noise has a tractable distribution. The remainder is governed by the estimation accuracy of the initial estimator; this term might not have a tractable distribution but is small in comparison to the distributional term if the initial estimator converges to the regression vector of the underlying model sufficiently fast.

If we can handle the distributional part and bound the remainder, we can construct the desired confidence intervals for \(\beta_{j}\) with \(j\in\{1,\ldots,p\}\). Consider \(z_{t/2},m_{t/2}\in[0,\infty)\) such that

\[\mathbb{P}\big{\{}\,|(2M_{\delta_{Z}}X^{\top}\boldsymbol{u})_{j}| >z_{t/2}\,\big{\}}\,\leq\,\frac{t}{2}\] \[\text{and }\,\mathbb{P}\Big{\{}\,|((2M_{\delta_{Z}}X^{\top}X- \mathrm{I}_{p\times p})(\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}^{0})) _{j}|\,>\,m_{t/2}\,\Big{\}}\,\leq\,\frac{t}{2}\,.\]

Then,

\[\mathbb{P}\big{\{}\,\beta_{j}\,\in\,[(\widehat{\beta}^{1})_{j}-z_ {t/2}-m_{t/2},(\widehat{\beta}^{1})_{j}+z_{t/2}+m_{t/2}]\big{\}}\] \[=\,\mathbb{P}\big{\{}\,\beta_{j}-(\widehat{\beta}^{1})_{j}\,\in\, [-z_{t/2}-m_{t/2},z_{t/2}+m_{t/2}]\big{\}}_{\text{subtracting }(\widehat{ \beta}^{1})_{j}\text{ throughout}}\]

[MISSING_PAGE_EMPTY:6378]

the corresponding \(z_{t_{1}},m_{t_{2}}\). If the remainder is negligible (that is, the approximate inverse and/or the initial estimator are accurate), the calibration of \(t_{2}\) and \(m_{t_{2}}\) is uncomplicated: setting \(t_{2}:=t-t_{1}:=0\) and \(m_{t_{2}}:=0\) provides for a good approximation of the stated confidence interval. It then remains to find a corresponding \(z_{t}\). We illustrate the whole procedure in the following two examples.

**Confidence Intervals for Least-Squares Estimator**: It is educational to apply our scheme for constructing confidence intervals first to the classical case where \(n\gg p\) and \(X^{\top}X\) is invertible. We can choose \(M_{\delta_{Z}}=(X^{\top}X)^{-1}/2\), which implies that the debiased lasso coincides with the least-squares estimator (\(\widehat{\mathbf{\beta}}^{1}=\widehat{\mathbf{\beta}}_{\mathrm{ls}}\))--see Example 5.1.2--and that \(2M_{\delta_{Z}}X^{\top}X-\mathrm{I}_{p\times p}=\mathbf{0}_{p\times p}\), which allows us to set \(t_{1}:=t\), \(t_{2}:=0\), and \(m_{t_{2}}:=0\) for a given target level \(1-t\). We now assume for the sake of illustration that \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathrm{I}_{n\times n}]\) and that \(X\) is a non-random matrix [8]. The distributional term then follows marginal distributions of its coordinates are \((2M_{\delta_{Z}}X^{\top}\mathbf{u})_{j}\sim\mathcal{M}[0,\sigma^{2}((2M_{\delta_{ Z}})X^{\top}X(2M_{\delta_{Z}})^{\top})_{j}\|\). Since \(M_{\delta_{Z}}=(X^{\top}X)^{-1}/2\) by assumption, the distributions of the coordinates simplify to \(2(M_{\delta_{Z}}X^{\top}\mathbf{u})_{j}\sim\mathcal{M}[0,\sigma^{2}((X^{\top}X)^{-1 })_{j}\|\). This allows us to set \(z_{t_{1}}:=q_{t/2}\sigma\sqrt{((X^{\top}X)^{-1})_{j}}\), where \(q_{t/2}\) is such that \(\mathbb{P}[|v|\geq q_{t/2}]=t\) for \(v\sim\mathcal{M}[0,1]\). The factor \(\sigma\sqrt{((X^{\top}X)^{-1})_{j}}\) captures the "scaling" of \(2(M_{\delta_{Z}}X^{\top}\mathbf{u})_{j}\), and the factor \(q_{t/2}\) captures the scaled "shape" of \(2(M_{\delta_{Z}}X^{\top}\mathbf{u})_{j}\). Theorem 5.2.1 then yields the confidence interval at level \(1-t\)

\[\left[(\widehat{\beta}_{\mathrm{ls}})_{j}-q_{t/2}\sigma\sqrt{((X^{\top}X)^{-1 })_{j}},(\widehat{\beta}_{\mathrm{ls}})_{j}+q_{t/2}\sigma\sqrt{((X^{\top}X)^{- 1})_{j}}\right]\]

for \(\beta_{j}\).

The standard deviation \(\sigma\) is usually unknown in practice, but it can also be estimated by using the least-squares estimator. Note first that

\[\frac{\|\mathbf{y}-X\widehat{\beta}_{\mathrm{ls}}\|_{2}}{\sqrt{n}}\] \[=\frac{\|X\mathbf{\beta}+\mathbf{u}-X\widehat{\mathbf{\beta}}_{\mathrm{ls}} \|_{2}}{\sqrt{n}}\qquad\text{linear regression model:}\mathbf{y}=X\mathbf{\beta}+\mathbf{u}\] \[\leq\frac{\|\mathbf{u}\|_{2}}{\sqrt{n}}+\frac{\|X\mathbf{\beta}-X\widehat {\beta}_{\mathrm{ls}}\|_{2}}{\sqrt{n}}\;,\qquad\text{triangle inequality for norms}\]and similarly

\[\frac{|\mathbf{v}-X\widehat{\mathbf{\beta}}_{\rm ls}\|_{2}}{\sqrt{n}}\ \ \geq\ \ \frac{ \|\mathbf{u}\|_{2}}{\sqrt{n}}-\frac{|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\rm ls}\|_{2}}{ \sqrt{n}}\,.\]

Since \(|\mathbf{u}|_{2}/\sqrt{n}\) is sharply concentrated around \(\sigma\) (see Exercise 5.4), and since least-squares estimation provides accurate prediction, that is, \(|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\rm ls}\|_{2}/\sqrt{n}\ll 1\), under mild conditions when \(n\gg p\) (see Display (1.5) in \(\blacktriangleright\) Sect. 1.2), the two displays show that \(|\mathbf{y}-X\widehat{\mathbf{\beta}}_{\rm ls}\|_{2}/\sqrt{n}\approx\sigma\) and, therefore, that

\[\left[(\widehat{\mathbf{\beta}}_{\rm ls})_{j}-q_{t/2}|\mathbf{y}-X\widehat{ \mathbf{\beta}}_{\rm ls}\|_{2}\sqrt{((X^{\top}X)^{-1})_{jj^{\prime}}/n},\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \tag{5.5}\]

is an approximate confidence interval at level \(1-t\) for \(\beta_{j}\). \(\blacktriangleleft\)

**Example 5.4**

**Confidence Intervals for Debiased Lasso** The lasso estimator satisfies sharp bounds for prediction and estimation (see \(\blacktriangleright\) Chaps. 6 and 7), but its distribution in terms of the design and the noise is involved. A way to make the distribution more tractable for inference is the one-step update (5.4):

\[\overline{\mathbf{\beta}}\,:=\,2M_{\mathbf{\alpha}_{\mathbf{\beta}}}X^{\top}\mathbf{y}+\big{(} \mathbf{I}_{p\times p}-2M_{\mathbf{\alpha}_{\mathbf{\beta}}}X^{\top}X\big{)}\widehat{ \mathbf{\beta}}_{\rm lasso}\,,\]

where \(\widehat{\mathbf{\beta}}^{0}\,=\,\widehat{\mathbf{\beta}}_{\rm lasso}\) is an initial lasso estimator (see Display (2.2)) and \(M_{\mathbf{\alpha}_{\mathbf{\beta}}}\) an approximate inverse of \(2X^{\top}X\) (see Examples 5.1.2 and 5.1.3). The corresponding one-step estimator \(\widehat{\mathbf{\beta}}^{1}=\overline{\mathbf{\beta}}\) is called the _debiased lasso estimator_.

Example 5.2.1 sets the general direction also for the high-dimensional case, but we need to make two adjustments: 1. we have to replace \((X^{\top}X)^{-1}/2\) by an approximation because the Gram matrix \(X^{\top}X\) is not invertible as soon as \(p>n\) (see 2. in Exercise 1.2); 2. we have to estimate the standard deviation through the lasso because the least-squares estimator does not provide reliable prediction as soon as \(p\ \gtrsim\ n\) (see \(\blacktriangleright\) Sect. 1.2). We again make the assumptions that \(\mathbf{u}\,\sim\,\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathbf{I}_{n\times n}]\) and that \(X\) is non-random. For Adjustment 1, we construct an approximate inverse through defining its rows as minimizers of convex optimization programs, that is,

[MISSING_PAGE_FAIL:162]

\(m_{t_{2}}:=a\tilde{a}[t_{2}]\sigma\log[p]/n\), Theorem 5.2.1 provides us with the following confidence interval at level \(1-t\) for \(\beta_{j}\):

\[\left[\overline{\beta}_{j}-q_{t_{1}/2}\sigma\sqrt{\left((2M_{\delta_{Z}})X^{\top }X(2M_{\delta_{Z}})^{\top}\right)_{jj}}-a\tilde{a}[t_{2}]\sigma\log[p]/n,\right.\]

\[\left.\overline{\beta}_{j}+q_{t_{1}/2}\sigma\sqrt{\left((2M_{\delta_{Z}})X^{ \top}X(2M_{\delta_{Z}})^{\top}\right)_{jj}}+a\tilde{a}[t_{2}]\sigma\log[p]/n \right].\]

The factor \(\tilde{a}[t_{2}]\) is unknown in practice (for example, it can depend on the unknown sparsity level \(|\beta\|_{0}\)), but the remainder term is typically neglected altogether. The rationale is that one hopes that the minimization programs (5.6) have a solution for \(a\approx 1\) (see Note \({}^{10}\) at the end of this chapter) and that the lasso is accurate enough to satisfy bounds with \(\tilde{a}[t_{2}]\lesssim 1\) also for small \(t_{2}\) (see Theorem 7.2.1). Specifically, with the design normalized such that \((X^{\top}X)_{jj}=n\) for all \(j\in\{1,\ldots,p\}\), it might make sense to assume that (i) (if \(2M_{\delta_{Z}}=(X^{\top}X)^{-1}\) and the correlations among the predictors are low, that is, \((X^{\top}X)_{jj}=n\gg(X^{\top}X)_{jj}\) for all \(i\neq j\), then \(\sqrt{((2M_{\delta_{Z}})X^{\top}X(2M_{\delta_{Z}})^{\top})_{jj}}=\sqrt{((X^{ \top}X)^{-1})_{jj}}\approx\sqrt{1/(X^{\top}X)_{jj}}=1/\sqrt{n}\), for example), (ii) \(a\approx 1\) (as commented above), (iii) \(\tilde{a}[t_{2}]\lesssim 1\) for \(t_{1}\approx t\) and \(t_{2}\approx 0\) (as commented above), and (iv) \(q_{t_{1}/2}\gtrsim 1\) (which basically means that \(t_{1}\) is not too close to one). These assumptions taken together yield

\[\frac{a\tilde{a}[t_{2}]\sigma\log[p]/n}{2q_{t_{1}/2}\sigma\sqrt{ \left((2M_{\delta_{Z}})X^{\top}X(2M_{\delta_{Z}})^{\top}\right)_{jj}}}\] \[= \frac{a\tilde{a}[t_{2}]\log[p]/n}{2q_{t_{1}/2}\sqrt{\left((2M_{ \delta_{Z}})X^{\top}X(2M_{\delta_{Z}})^{\top}\right)_{jj}}}\] dividing through by \[\sigma\] \[\ll \frac{\log[p]/n}{1/\sqrt{n}}\] Assumptions (i)-(iv) \[= \frac{\log p}{\sqrt{n}}\.\] consolidating Therefore, if \(\sqrt{n}\gg\log p\), the above confidence interval can be approximated by \[\left[\overline{\beta}_{j}-q_{t/2}\sigma\sqrt{\left((2M_{\delta_{Z}})X^{\top}X(2M_{\delta_{Z}})^{\top}\right)_{jj}},\right.\] \[\left.\overline{\beta}_{j}+q_{t/2}\sigma\sqrt{\left((2M_{\delta_{Z}})X^{\top}X(2M_{\delta_{Z}})^{\top}\right)_{jj}}\right].\]This approximation, and the debiasing scheme in general, requires reasonably large sample sizes but then allows for a comparably large number of model parameters: for example, say, \(\sqrt{n}\geq 2\log p\) requires \(n\geq 85\) if \(p=100\) but only \(n\geq 340\) if \(p=10\,000\).

For Adjustment 2, we simply exchange \(|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\mathrm{ks}}|_{2}/\sqrt{n}\) by \(|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}|_{2}/\sqrt{n}\) in the estimation of \(\sigma\), using the fact that the lasso provides accurate prediction, that is, \(|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}|_{2}/\sqrt {n}\ll 1\), under mild conditions even when \(p\gg n\) (see \(\blacktriangleright\) Chap. 6). Plugging the lasso estimate of \(\sigma\) into the previous display yields the approximate confidence interval

\[\left[\overline{\beta}_{j}-q_{t/2}|\boldsymbol{y}-X\widehat{ \boldsymbol{\beta}}_{\mathrm{lasso}}|_{2}\sqrt{((2M_{\boldsymbol{\mathrm{A}}_{ \mathrm{Z}}})X^{\top}X(2M_{\boldsymbol{\mathrm{A}}_{\mathrm{Z}}})^{\top})_{ \tilde{\mu}j}/n},\right.\] \[\left.\overline{\beta}_{j}+q_{t/2}|\boldsymbol{y}-X\widehat{ \boldsymbol{\beta}}_{\mathrm{lasso}}|_{2}\sqrt{((2M_{\boldsymbol{\mathrm{A}}_{ \mathrm{Z}}})X^{\top}X(2M_{\boldsymbol{\mathrm{A}}_{\mathrm{Z}}})^{\top})_{ \tilde{\mu}j}/n}\right]. \tag{5.7}\]

Two tuning parameters remain to be calibrated: the tuning parameter \(r\) of the lasso (2.2) and the tuning parameter \(a\) in the minimization programs (5.6) for the approximate inverse. For practical choices of the lasso's tuning parameter, we refer to \(\blacktriangleright\) Chap. 4; a suggestion for the programs' tuning parameter is \(a=1.2\times\inf_{\boldsymbol{b}\in\mathbb{R}^{p}}|\boldsymbol{\mathrm{2}}X^{ \top}X\boldsymbol{b}-\boldsymbol{e}^{i}|_{\infty}\)[11]. Importantly, while we cannot check how accurate the lasso is, we can check how accurate the approximate inverse is by computing \(\max_{k}|\boldsymbol{\mathrm{2}}X^{\top}X\boldsymbol{a}^{k}-\boldsymbol{e}^{ k}|_{\infty}\). \(\blacktriangleleft\)

### 5.3 Exercises

#### Exercises for \(\blacktriangleright\) Sect. 5.1

**Exercise 5.1** \(\blacklozenge\): In this exercise, we approach the Z-formulation of the least-squares estimator from another direction. In contrast to Example 5.1.1, where we have established a Z-formulation by taking derivatives of the least-squares' objective function, we start here by assuming that \(\mathbb{E}[\mathbf{u}^{\top}X]=\mathbf{0}_{p}\) (and that all relevant random variables are integrable).12
1. Show first that \[\mathbb{E}\big{[}X^{\top}(\mathbf{y}-X\mathbf{\beta})\big{]}\,=\,\mathbf{0}_{p}\,.\] Hint: Background on conditional expectations can be found in Durrett (2010, Chapter 5.1 on pp. 221ff). Two results might be particularly useful: Theorem 5.1.7 on p. 228 and Display (5.1.5); the latter is called the _law of iterated expectations_ and the _tower rule_--among many other names.
2. Claim 1 motivates the requirement that an estimator \(\widehat{\mathbf{\beta}}\) of \(\mathbf{\beta}\) should satisfy an "empirical version" of that equality: \[X^{\top}(\mathbf{y}-X\widehat{\mathbf{\beta}})\,=\,\mathbf{0}_{p}\,.\] In other words, Claim 1 motivates the \(Z\)-estimator \[\widehat{\mathbf{\beta}}\,\in\,\mathbb{R}^{p}\quad\text{is such that}\quad\tilde{\mathbf{\lambda}}_{Z}[\widehat{\mathbf{\beta}}]\,=\,\mathbf{0}_{p}\] with \[\tilde{\mathbf{\lambda}}_{Z}\,:\,\mathbf{\alpha}\,\mapsto\,\,X^{\top}(\mathbf{y}-X\mathbf{ \alpha})\,.\] Show that this is again the least-squares estimator.

This exercise illustrates the fact that \(Z\)-estimators can have several equivalent yet slightly different formulations that reflect different motivations of the estimator.

**Exercise 5.2** \(\blacklozenge\): In this exercise, we discuss the convergence of the generalized Newton-Raphson algorithm for the least-squares estimator. We have shown in Example 5.1.2 that the regular Newton-Raphson algorithm converges to a least-squares solution irrespective of the starting point. Does the same hold for the _generalized_ Newton-Raphson algorithm as well?

1. Give an \(M_{\mathbf{\lambda}_{Z}}\,\in\,\mathbb{R}^{p\times d}\) such that the generalized Newton-Raphson algorithm converges for all starting points but not necessarily to a least-squares solution.
2. Give an \(M_{\mathbf{\lambda}_{Z}}\,\in\,\mathbb{R}^{p}\) such that the generalized Newton-Raphson algorithm fails to converge for "almost" all starting points. You may assume \(X^{\top}X=\mathrm{I}_{p\times p}\) for simplicity.

These results illustrate the fact that the generalized Newton-Raphson algorithm requires carefully chosen approximate inverses.

#### 5.3.2 Exercises for \(\blacktriangleright\) Sect. 5.2

**Exercise 5.3**\(\blacklozenge\): In this exercise, we show that the update step in \(\blacktriangleright\) Sect. 5.2 can indeed be interpreted as debiasing. We consider estimators of the form

\[\widehat{\boldsymbol{\beta}}\;\in\;\operatorname*{arg\,min}_{\boldsymbol{ \alpha}\in\mathbb{R}^{p}}\left\{\,\|\boldsymbol{y}-X\boldsymbol{\alpha}\|_{2} ^{2}+r\hat{\boldsymbol{\alpha}}[\boldsymbol{\alpha}]\right\},\]

where \(r\in[0,\infty)\) is a tuning parameter and \(\hat{\boldsymbol{\alpha}}:\mathbb{R}^{p}\to\mathbb{R}\) a convex prior function. The KKT conditions for \(\widehat{\boldsymbol{\beta}}\) are according to \(\blacktriangleright\) Sect. 2.5

\[-2X^{\top}(\boldsymbol{y}-X\widehat{\boldsymbol{\beta}})+r\widehat{ \boldsymbol{\kappa}}\;=\;\boldsymbol{0}_{p}\,,\]

where \(\widehat{\boldsymbol{\kappa}}\in\mathfrak{a}\hat{\boldsymbol{\kappa}}[ \widehat{\boldsymbol{\beta}}]\).

1. Use the KKT conditions to show that \[\widehat{\boldsymbol{\beta}}+rM_{\mathfrak{A}_{Z}}\widehat{\boldsymbol{\kappa }}\;=\;2M_{\mathfrak{A}_{Z}}X^{\top}\boldsymbol{y}+\big{(}\mathrm{I}_{p \times p}-2M_{\mathfrak{A}_{Z}}X^{\top}X\big{)}\widehat{\boldsymbol{\beta}}\,.\] This means that the update step \(\widehat{\boldsymbol{\beta}}^{0}\mapsto\widehat{\boldsymbol{\beta}}^{1}=2M_ {\mathfrak{A}_{Z}}X^{\top}\boldsymbol{y}+(\mathrm{I}_{p\times p}-2M_{ \mathfrak{A}_{Z}}X^{\top}X)\widehat{\boldsymbol{\beta}}^{0}\) of \(\blacktriangleright\) Sect. 5.2 applied to the estimator \(\widehat{\boldsymbol{\beta}}^{0}:=\widehat{\boldsymbol{\beta}}\) can be written as \(\widehat{\boldsymbol{\beta}}\mapsto\widehat{\boldsymbol{\beta}}+rM_{\mathfrak{ A}_{Z}}\widehat{\boldsymbol{\kappa}}\).
2. Use 1. to show that if \(M_{\mathfrak{A}_{Z}}X^{\top}\boldsymbol{u}\) is symmetric and the integrated remainder term \(\mathbb{E}[(2M_{\mathfrak{A}_{Z}}X^{\top}X-\mathrm{I}_{p\times p})(\boldsymbol {\beta}-\widehat{\boldsymbol{\beta}})]\) is negligible, it holds that \[\mathbb{E}[\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}]\;\approx\;-\; \mathbb{E}[rM_{\mathfrak{A}_{Z}}\widehat{\boldsymbol{\kappa}}]\,.\] This means that \(-\mathbb{E}[rM_{\mathfrak{A}_{Z}}\widehat{\boldsymbol{\kappa}}]\) is approximately the bias of the estimator \(\widehat{\boldsymbol{\beta}}\), and therefore, adding \(rM_{\mathfrak{A}_{Z}}\widehat{\boldsymbol{\kappa}}\) to \(\widehat{\boldsymbol{\beta}}\) can be interpreted as a measure for reducing the bias (see also \(\blacksquare\) Fig. 2.3).
3. Discuss the results of 1. and 2. for the special case of the least-squares estimator: \(\widehat{\boldsymbol{\beta}}=\widehat{\boldsymbol{\beta}}_{\mathrm{ls}}\).

**Exercise 5.4**\(\blacklozenge\blacklozenge\) In this exercise, we show that \(\|\mathbf{u}\|_{2}/\sqrt{n}\) is sharply concentrated around \(\sigma\) for \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathsf{I}_{n\times n}]\). We will use the Properties (i) \(\mathbb{E}[z^{2}]=1\) and (ii) \(\mathbb{E}[z^{4}]=3\) for \(z\sim\mathcal{N}[0,1]\).

1. Use Property (i) to show that \[\mathbb{E}\|\mathbf{u}\|_{2}^{2}\,=\,n\sigma^{2}\,.\] If \(\sigma=1\), we call the distribution of \(\|\mathbf{u}\|_{2}^{2}\) a _Chisquared distribution_ with \(n\) degrees of freedom. Hence, the equality implies that the mean of a Chi-squared distribution equals its degrees of freedom.
2. Use 1. and both Properties (i) and (ii) to show that \[\mathbb{E}\Big{[}\big{(}\|\mathbf{u}\|_{2}^{2}-n\sigma^{2}\big{)}^{2}\Big{]}\,=\,2 n\sigma^{4}\,.\] This equality implies that the variance of a Chi-squared distribution is equal to two times its degrees of freedom.
3. Use 2. and the Markov inequality to show that for every \(t\in(0,1)\), it holds that \[\mathbb{P}\Bigg{\{}\,\frac{\|\mathbf{u}\|_{2}}{\sqrt{n}}\,\notin\,[\sqrt{1-t}\, \sigma,\sqrt{1+t}\,\sigma]\Bigg{\}}\,\leq\,\frac{2}{nt^{2}}\,.\] Hint: The Markov inequality (or rather one version of it) states that \(\mathbb{P}\{x\geq a\}\leq\mathbb{E}[x]/a\) for every non-negative random variable \(x\) and every constant \(a\in(0,\infty)\). (This inequality is sometimes also called the _Chebyshev inequality_ instead.)13 Footnote 13: The proof is based on the fact that \(\mathbb{P}\{x\geq a\}\leq\mathbb{E}[x]/a\) for every non-negative random variable \(x\) and every constant \(a\in(0,\infty)\).
4. Conclude from 3. that for every fixed \(b\in(0,\infty)\), it holds that \[\lim_{n\to\infty}\mathbb{P}\Bigg{\{}\,\Big{|}\frac{\|\mathbf{u}\|_{2}}{\sqrt{n}}- \sigma\Big{|}\,\,\,\geq\,\,b\,\Bigg{\}}\,=\,0\,,\] which means that \(\|\mathbf{u}\|_{2}/\sqrt{n}\) converges in probability to \(\sigma\).

We can thus corroborate the statements in Example 5.2.1 and 5.2.2 that \(\|\mathbf{y}-X\widehat{\mathbf{\beta}}_{\rm 1s}\|_{2}\) and \(\|\mathbf{y}-X\widehat{\mathbf{\beta}}_{\rm lasso}\|_{2}\) can be reasonable estimators of \(\sigma\).

### R Lab: Confidence Intervals in Low and High Dimensions

In this lab, we establish confidence intervals for coordinates of regression vectors.

Your task is, as always, to replace the keyword REPLACE with suitable code and to answer the questions in the text.

#### Generating Synthetic Data

We first create a generator for synthetic data. Write a function DataGeneration() that generates data according to a linear regression model. The entries of the design matrix should be i.i.d. standard Gauss-distributed and then transformed such that the matrix columns are centered and have Euclidean norm equal to \(\sqrt{n}\). The entries of the noise should be i.i.d. Gauss-distributed (independent from the design) with mean zero and some sigma. The first three entries of the regression vector should be 1, while all other entries should be 0. The outcome vector should also be centered (to avoid dealing with intercepts) but not scaled. Hint: the scale() function might be convenient but make sure to adapt its standard scaling according to our description.

```
set.seed(1) DataGeneration<-function(n,p,sigma) { REPLACE return(list("y"=y,"X"=X)) data.low<-DataGeneration(n=33,p=9,sigma=1) cbind(data.low$[1:5],data.low$X[1:5,1:4])
```
#[,1][,2][,3][,4][,5]
#[1,]0.9276815-0.69672591.34247973-1.2322317-0.6888858
#[2,]-1.73744560.1462850-0.256972060.65393921.0959945
#[3,]-1.2805843-0.91439940.747707360.07991001.0735572
#[4,]5.238521.61527640.945047472.0512395-0.8578577
#[5,]-0.43314120.29807580.06191724-1.06371670.1956718 ```

#### Defining Initial Estimators

We consider two initial estimators: a least-squares and a lasso. Write two functions accordingly. For the lasso, use the cv.glmnet() function from the glmnet package;

[MISSING_PAGE_FAIL:169]

## 5.4 R Lab: Confidence Intervals in Low and High Dimensions

## [1] 0.722643

Interpret also this result. Argue in particular that the approximate inverse satisfies the conditions of all minimization problems (5.6) for the tuning parameter \(a=1\). What does this mean for the remainder in Theorem 5.2.1?

Our naive optimization approach serves us well in this lab but does not necessarily provide a good inverse in a reasonable amount of time in practice; therefore, in practice, it is recommended to implement more scalable alternatives to our optimization such as the one referred to in the following bonus question. In any case, one should always check the quality of the approximate inverse with the ValidateInverse() function in practice.

#### Bonus: A Scalable Optimization Algorithm

Implement a refined inversion scheme along the lines of [https://github.com/LedererLab/HDIV](https://github.com/LedererLab/HDIV).

#### Debiasing

We now implement a debiasing routine. Define a function Debiasing() that takes the data, the initial estimator, and the approximate inverse and returns the one-step estimator in Display (5.4) of the main text. Test the debiasing as indicated. Hint: recall that \(2M\) (rather than \(M\)) is the approximate inverse of \(X^{\top}X\).

```
act.seed(t) DuBias<-function(y,X,initial.estimator,gram.inv) { return(REPUACK) } inst.nguava<-Lenstigaures(data.lowly,data.lowEK) learnt.nguava<-Dabias(data.lowly,data.lowlx,last.nguava,gram.low.inv) child(last.nguava,least.nguava.ded)[iL,]
### [,1] [,2]
### [1,1] 1.05028611 1.05028611
## [2,1] 0.99201291 0.99201291
## [3,1] 1.05737576 1.05737576
## [4,1] -0.01262238 -0.01262238
## [5,1] 0.01558863 0.01558863
``` lamno.low<-Lens(data.lowly,data.lowTX) lamno.deb.low<-Dabias(data.lowly,data.lowTX,lamno.low,gram.low.inv) child(lamno.low,lamno.deb.low)[iL,]
## [,1] [,2]
## [1,1] 0.8909012 1.05028611
## [2,1] 0.8283301 0.99201291
## [3,1] 0.8972300 1.05737576
## [4,1] 0.0000000 -0.01262238
## [5,1] 0.0000000 0.01558863

``` lamno.high<-Lens(data.highly,data.highRX) lamno.deb.high<-Dabias(data.highly,data.highTX,lamno.high,gram.high.inv) child(lamno.high,lamno.deb.high)[iL,]

## [,1] [,2]
## [1,] 0.7790845 0.6886401
## [2,] 0.7859893 0.9681673
## [3,] 0.6282234 1.0068510
## [4,] 0.0000000 -0.1089748
## [5,] 0.0000000 0.4463807

Are the results as expected? Explain.

#### Constructing Confidence Intervals

We now construct confidence intervals. Write a function ConfidenceIntervals that corresponds to our results in \(\blacktriangleright\) Sect. 5.2. Hint: the function qnorm() might be helpful.

  set.mod()  ConfidenceInterval :- function(y, x, gran.inv, coordinate, level, FOL-LanitSquares)  {  REPLACE  }  ConfidenceInterval(data.lowf, data.lowf, gran.low.inv, coordinate, level.of, FOL-LanitSquares)

## $left
## [1] 0.9327692
## # $right
## [1] 1.167803

## $left
## [1] 0.9291717

## $right
## [1] 1.171401

## $right
## [1] 1.171401

## $right
## [1] 0.2893542

## $right

#### Validating the Pipeline

We now check the validity of the confidence intervals by comparing nominal levels with their empirical counterparts. Write a function EmpiricalLevels() that returns the empirical levels for the confidence intervals of the regression vector's first coordinate. The empirical levels are the fraction of draws leading to confidence intervals that contain the first coordinate of the regression vector. Each draw consists of the generation of fresh data and the computation of confidence intervals on these data. Set nbr.iter=10 when calling ApproximateInverse() to speed up the computations (the computations can take a while nonetheless).

``` set.seed(c) EmpiricalLevels<-function(n,p,sigma,FUN,nominal.lewls,nbr.draw) { ZSPLACE ] PlotLevels<-function(n,p,sigma,FUN,nbr.draw=50) { nominal.lewls <- nsq(from=0.05,to=0.95,by=0.05) plot[s=nominal.lewls, y=HumpricalNewls(n,p,sigma,FUN,nominal.lewls,nbr.draws), xlim=c(0,1), xlim=c(0,1), xmap=c(0,1,2), ymap=c(0,1,2), 1ax=1, xil="Normallevel", ylab="Appliedlevel") ] PlotLevels(n=33,p=9,sigma=1,FUN=Lasso)
What do these results indicate?

#### Real Data Analysis

We finally analyze real data. We chose data with many more samples than observations, which allows us to test our pipeline against the standard lm() function.

##### 5.4.9.1 Data Loading

We consider the Fertility data set from the Stat2Data package. Load the data and take the column Embryos as the preliminary outcome vector and the remaining data as the preliminary design matrix. Normalize the data such that the final outcome vector \(\boldsymbol{y}\) and all columns of the final design matrix \(X\) are centered and that the columns of \(X\) have Euclidean norm equal to \(\sqrt{n}\). Hint: to ease later manipulations, transform the data into matrix format by applying the as.matrix() function.

library(Stat2Data) data("Fertility") Y <- REPLACE X <- REPLACE cbind(y, X)[1:5, 1]
## Embryos Age LowAWC MeanArc FSUN H2 MaxH2
## 1 6.2732733 0.9948009 0.5101005 0.511984 0.3274049 0.24693263 -0.1527636
## 2 -0.7527627 0.3552500 4.154807 3.704057 0.604745 0.7731945 -0.9544509
## 3 8.2732733 0.9948009 3.1720684 3.704057 -0.53136003 -0.00198163 3.8130776
## 4 -2.7262767 0.9948009 3.431269 3.232127 -1.0495889 -1.00294157 0.1307870
## 5 5.2732733 -1.11695153 4.312679 3.029871 -0.9975400 0.51006404 1.2568441
-0.8809723 0.9158202 -0.9954916 2.2295757 -0.94201110 -0.7525575 -0.8194950 49 3 1.24034454 1.4744658 2.5683614 ## 4 -0.00320925 -0.0954916 -0.4807093 ## 5 -1.39081295 -0.9715795 1.2132188 The number of samples is \(n=333\) and the number of model parameters is \(p=9\).

#### 5.4.9.2 Estimating Confidence Intervals

We now estimate confidence intervals for all nine model parameters in the above data set. Write a function ConfidenceIntervalsAll() accordingly and compute all (individual) 99% confidence intervals. Hint: Keep the standard settings for ApproximateInverse().

```
set.seed(?) ConfidenceIntervalAll<-function(y,X,level,FUND) { HEPLACE } confidence.intervals<-ConfidenceIntervalAll(y,X,0.99,LeastSquares) confidence.intervals
```

## left right
## [1,] -0.7129082 0.1922226
## [2,] -0.3003823 2.222127
## [3,] -2.000322 0.5117284
## [4,] -0.3931534 0.4753687
## [5,] -0.3518565 0.40498
## [6,] -0.1485149 0.7253007
## [7,] -0.6854799 1.178465
## [8,] -0.8809723 0.9158202
9,] 2.480274 3.413196 What would you need to change in the above code if \(p\) were larger than \(n\)?

#### Plotting the Intervals

We plot the intervals. Write a corresponding plotting function PlotInvervals().

 PlotIntervals <- function(confidence.intervals, estimator)  {  REPLACE  }  least.squares <- LeastSquares(y, x)  least.squares

Embryos  ## Age -0.26034283  ## LowAFC 0.96087230  ## MeanAFC -0.74429689  ## FSH 0.04110764  ## E2 0.02656174  ## MaxE2 0.28839293  ## MaxDailyGn 0.24649258  ## TotalGn 0.01742394  ## Oocytes 2.94673502

 PlotIntervals(confidence.intervals, least.squares)

Which model parameters are significant at a 99% level? Explain.

[MISSING_PAGE_EMPTY:6396]

One could also consider, for example, \(X\) random and \(\mathbf{u}\mid X\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathbb{I}_{n\times n}]\). Then, the results would hold conditionally on \(X\).
* This is Program 3.2 in Gold et al. (2020), which is a modification of the CLIME estimator (Cai et al., 2011, Equation (1) on p. 595). Other programs for finding approximate inverses are formulated by Javanmard and Montanari (2014); van de Geer et al. (2014).
* See Gold et al. (2020, Lemma 4.9).
* See Gold et al. (2020, Section 5).
* If \(\mathbb{E}[\mathbf{u}\mid X]=\mathbf{0}_{p}\) (which implies \(\mathbb{E}[\mathbf{u}^{\top}X]=\mathbf{0}_{p}\) in view of the law of iterated expectations), we call the predictors _exogenous_. Exogeneity and its counterpart _endogeneity_ are important concepts in econometric models such as instrumental variable regression (Stock and Trebbi, 2003).
* See Durrett (2010, pp. 28-29), for example.

## 6 Theory I: Prediction

###### Contents

In the remainder of this book, we establish mathematical guarantees for high-dimensional estimators. The concepts underpinning our derivations are the basis for high-dimensional theories in general, but for the sake of clarity we focus here on linear regression with regularized least-squares estimators. Specifically, we consider the model \(\boldsymbol{y}=X\boldsymbol{\beta}+\boldsymbol{u}\) of (2.1) and the corresponding estimators of the form

\[\widehat{\boldsymbol{\beta}}\;\in\;\operatorname*{arg\,min}_{\boldsymbol{ \alpha}\in\mathbb{R}^{p}}\left\{\,\boldsymbol{\mathpzc{z}}\big{[}|\boldsymbol{ y}-X\boldsymbol{\alpha}|_{2}^{2}\big{]}+r\hat{\boldsymbol{\alpha}}|\,\right\} \tag{6.1}\]

for a _link function_\(\mathpzc{z}:[0,\infty)\to[0,\infty]\), a tuning parameter \(r\in[0,\infty)\), and a prior function \(\hat{\mathpzc{z}}:\mathbb{R}^{p}\to[0,\infty]\). Typical link functions are the identity function \(\mathpzc{z}:a\mapsto a\) (which applies to the lasso (2.2) and the group lasso (2.3), for example) and the square-root function \(\mathpzc{z}:a\mapsto\sqrt{a}\) (which applies to the square-root lasso (6.2) below and the square-root group lasso (2.9), for example).

We derive two types of guarantees: In this chapter, we state conditions under which estimators \(\widehat{\boldsymbol{\beta}}\) can disentangle the signal \(X\boldsymbol{\beta}\) from the noise \(\boldsymbol{u}\). We formulate this type of guarantees in terms of the _prediction error_1\(\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\). In the next chapter, we state conditions under which estimators can recover the regression vector \(\boldsymbol{\beta}\) itself. We formulate that type of guarantees for estimation and support recovery errors, such as, for example, \(\|\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}\|_{\infty}\).

### Overview

Prediction guarantees are derived in two steps: In the first step, the prediction error is separated from the other terms in the estimator's objective function or its KKT conditions. This separation produces unwieldy inner product terms, which are controlled in the second step. The key assumption in that second step is that the regularization is sufficiently strong: in a sense, the influence of the prior term must outbalance the randomness of the problem.

For an illustration, we consider the lasso estimator

\[\widehat{\boldsymbol{\beta}}_{\text{lasso}}\;\in\;\operatorname*{arg\,min}_{ \boldsymbol{\alpha}\in\mathbb{R}^{p}}\left\{\,|\boldsymbol{y}-X\boldsymbol{ \alpha}|_{2}^{2}+r|\boldsymbol{\alpha}|_{1}\,\right\}\,.\]

Since \(\widehat{\boldsymbol{\beta}}_{\text{lasso}}\) minimizes the objective function \(\boldsymbol{a}\mapsto|\boldsymbol{y}-X\boldsymbol{a}|_{2}^{2}+r|\boldsymbol{ a}|_{1}\), we obtain for every \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) the 

[MISSING_PAGE_FAIL:181]

further: \(2\|X^{\top}\mathbf{u}\|_{\infty}(\|\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}+\|\mathbf{ \alpha}\|_{1})\,\leq\,r\|\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}+r\|\mathbf{\alpha}\| _{1}\). This yields \[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\rm lasso}\|_{2}^{2}\,\leq\,\|X\mathbf{\beta}-X \mathbf{\alpha}\|_{2}^{2}+2r\|\mathbf{\alpha}\|_{1}\,.\]

\begin{tabular}{l l} power-one prediction & Since \(\mathbf{\alpha}\) was arbitrary, we eventually find the _power-one bound_ \\ \[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\rm lasso}\|_{2}^{2}\,\leq\,\min_{\mathbf{ \alpha}\in\mathbb{R}^{p}}\left\{\,\|X\mathbf{\beta}-X\mathbf{\alpha}\|_{2}^{2}+2r\| \mathbf{\alpha}\|_{1}\,\right\}.\] \\ \end{tabular} This means that up to the complexity term \(2r\|\mathbf{\alpha}\|_{1}\), the lasso with sufficiently large tuning parameter minimizes the prediction error.

On the other hand, the properties of the inner product (\((\mathbf{u},\,X\widehat{\mathbf{\beta}}_{\rm lasso}-X\mathbf{\alpha})=(X^{\top}\mathbf{u},\, \widehat{\mathbf{\beta}}_{\rm lasso}-\mathbf{\alpha})\)), the Holder inequality (\((X^{\top}\mathbf{u},\,\widehat{\mathbf{\beta}}_{\rm lasso}-\mathbf{\alpha})\,\leq\,\|X^{ \top}\mathbf{u}\|_{\infty}\|\widehat{\mathbf{\beta}}_{\rm lasso}-\mathbf{\alpha}\|_{1}\)), and the triangle inequality (\(|\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}-\|\mathbf{\alpha}\|_{1}\leq\|\widehat{\mathbf{ \beta}}_{\rm lasso}-\mathbf{\alpha}\|_{1}\)) also lead to

\[|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\rm lasso}\|_{2}^{2}\leq|X\mathbf{ \beta}-X\mathbf{\alpha}\|_{2}^{2}+2|X^{\top}\mathbf{u}\|_{\infty}\|\mathbf{\alpha}-\widehat {\mathbf{\beta}}_{\rm lasso}\|_{1}\\ +r\|\mathbf{\alpha}-\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}\,.\]

We assume again that the regularization is strong enough: \(r\geq 2\|X^{\top}\mathbf{u}\|_{\infty}\). Using this assumption, the inequality yields in a similar way as before

\[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\rm lasso}\|_{2}^{2}\,\leq\,\|X\mathbf{\beta} -X\mathbf{\alpha}\|_{2}^{2}+2r\|\mathbf{\alpha}-\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}\,.\]

This result ensures that if (i) \(\mathbf{\alpha}\) is close to the regression vector in terms of the prediction error and (ii) \(\mathbf{\alpha}\) is close to \(\widehat{\mathbf{\beta}}_{\rm lasso}\) in terms of the \(\mathcal{E}_{1}\)-error, the estimator is close to the regression vector \(\mathbf{\beta}\) in terms of prediction.

However, the term \(2r\|\mathbf{\alpha}-\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}\) on the right-hand side of the above inequality still needs more treatment. For this, we relate prediction and \(\mathcal{E}_{1}\)-estimation further: we assume that the estimator \(\widehat{\mathbf{\beta}}_{\rm lasso}\) is close to the regression vector \(\mathbf{\alpha}\) in terms of the \(\mathcal{E}_{1}\)-error whenever \(\widehat{\mathbf{\beta}}_{\rm lasso}\) is close to \(\mathbf{\alpha}\) in terms of the prediction error, that is, \(\mathbf{\alpha}\in\mathcal{B}_{m}\), where

\[\widehat{\mathcal{B}}_{m}\,\coloneqq\,\left\{\,\mathbf{a}\in\mathbb{R}^{p}\,:\,| \mathbf{a}-\widehat{\mathbf{\beta}}_{\rm lasso}\|_{1}\,\leq\,m\|X\mathbf{a}-X\widehat{\mathbf{ \beta}}_{\rm lasso}\|_{2}/\sqrt{n}\,\right\}\]

for a fixed \(m\,\in\,[0,\infty]\). We say that such \(\mathbf{\alpha}\) satisfy the _structural condition_. The previous display entails for those \(\mathbf{\alpha}\)

\[|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\rm lasso}\|_{2}^{2}\,\leq\,\|X\mathbf{\beta}-X \mathbf{\alpha}\|_{2}^{2}+2mr\|X\mathbf{\alpha}-X\widehat{\mathbf{\beta}}_{\rm lasso}\|_{2} /\sqrt{n}\,.\]Using the inequality \(2ab\leq 4a^{2}+b^{2}/4\) for all \(a,b\in\mathbb{R}\) (second part of Lemma B.1.3 on Page 311 with \(p=1\), \(u=2\), and \(v=1/4\)), we further find

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso} }\|_{2}^{2}\leq\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+4m^{2}r^{ 2}/n\\ +\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}_{\mathrm{ lasso}}\|_{2}^{2}/4\,,\]

which becomes with \(\|\boldsymbol{a}+\boldsymbol{b}\|_{2}^{2}\leq 2\|\boldsymbol{a}\|_{2}^{2}+2\| \boldsymbol{b}\|_{2}^{2}\) for all \(\boldsymbol{a}\), \(\boldsymbol{b}\in\mathbb{R}^{p}\) (second part of Lemma B.1.1)

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{\mathrm{ lasso}}\|_{2}^{2}\leq 1X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+4m^{2}r^{ 2}/n+\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}/2\\ +\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{\mathrm{ lasso}}\|_{2}^{2}/2\,.\]

In summary, we find the _power-two prediction bound_

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{2}^{2} \leq\ \min_{\boldsymbol{\alpha}\in\widehat{\boldsymbol{\beta}}_{m}} \left\{\,3\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+\frac{8m^{2}r^ {2}}{n}\,\right\}\,.\]

This means that up to the complexity term \(8m^{2}r^{2}/n\) and a factor 3, the lasso with a sufficiently large tuning parameter minimizes the prediction error over \(\widehat{\boldsymbol{\beta}}_{m}\).

The power-two bound differs from the power-one bound above in three main aspects: the constant in front of \(\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}\) is 3 rather than 1, that is, the bound is not _sharp_; the complexity term involves \(m\) rather than \(\|\boldsymbol{\alpha}\|_{1}\), and it is quadratic in the tuning parameter \(r\) rather than linear (in other words, it contains the tuning parameter raised to the power of two rather than to the power of one--hence the naming);[2] and the set over which the minimum is taken is \(\widehat{\boldsymbol{\beta}}_{m}\) rather than the entire \(\mathbb{R}^{p}\). The two types of bounds cannot be ranked globally: grosso modo, power-two bounds dominate if the regression vector can be approximated well by a sparse vector and the predictors are only weakly correlated, while power-one bounds dominate otherwise--see Sect. 6.4.

In both bounds, the dependence on the random noise is governed by the term \(2\|X^{\top}\boldsymbol{u}\|_{\infty}\). We have called this term earlier the effective noise of the lasso estimator. The larger the effective noise, the larger the tuning parameters have to be, and therefore the larger the right-hand sides of the bounds. Thus, the lasso's effective noise plays the same role in the above bounds as the least-squares' effective noise in the risk bound on Page 9. We can compare, say, the power-one bound to that risk bound by invoking the two assumptions of that section: \(\boldsymbol{u}\sim\mathcal{N}_{n}[\boldsymbol{0}_{n},\sigma^{2}\mathrm{I}_{n\times n}]\) and \((X^{\top}X)_{jj}=n\) for all \(j\in\{1,\ldots,p\}\). We then find that if \(r=2\|X^{\top}\boldsymbol{u}\|_{\infty}\) and \(t\in(0,1]\),3

\[\mathbb{P}\Bigg{\{}\frac{\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}} _{\mathrm{lasso}}\|_{2}^{2}}{n}\leq\sigma\sqrt{\frac{32\log[p/t]}{n}}\| \boldsymbol{\beta}\|_{1}\Bigg{\}}\]

\[\geq\ \mathbb{P}\Bigg{\{}\frac{\min_{\boldsymbol{\alpha}\in\mathbb{ R}^{p}}\Big{\{}\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+4\|X^{ \top}\boldsymbol{u}\|_{\infty}\|\boldsymbol{\alpha}\|_{1}\Big{\}}}{n}\]

\[\leq\ \sigma\sqrt{\frac{32\log[p/t]}{n}}\|\boldsymbol{\beta}\|_{1}\Bigg{\}} \hskip 28.452756pt\text{power-one bound}\]

\[\geq\ \mathbb{P}\Bigg{\{}\frac{4\|X^{\top}\boldsymbol{u}\|_{\infty} \|\boldsymbol{\beta}\|_{1}}{n}\leq\sigma\sqrt{\frac{32\log[p/t]}{n}}\| \boldsymbol{\beta}\|_{1}\Bigg{\}}\]

\[\hskip 28.452756pt\text{setting}\ \boldsymbol{\alpha}=\boldsymbol{\beta}\]

\[=\ \mathbb{P}\big{\{}2\|X^{\top}\boldsymbol{u}\|_{\infty}\leq\sigma\sqrt{8mn \log[p/t]}\big{\}}\hskip 28.452756pt\text{consolidating};m=\max_{j\in\{1,\ldots,p \}}(X^{\top}X)_{jj}/n=1\]

\[\geq\ 1-t\,.\hskip 142.26378pt\text{Lemma \ref{lem:2}}\]

Neglecting constants, we can say that the lasso's average prediction error is bounded by \(\sigma\sqrt{\log[p]/n}|\boldsymbol{\beta}\|_{1}\) with high probability.

The bound's dependence on the dimensions \(n\) and \(p\) of the statistical model is captured by the factor \(\sqrt{\log[p]/n}\). This factor decreases in the number of samples as \(1/\sqrt{n}\) and increases in the number of model parameters \(p\) logarithmically; hence, if the prior assumption is satisfied, that is, the regression vector (or a good approximation of it) is not too large in terms of the \(\ell_{1}\)-norm, and if \(\sigma\) is not too large, the lasso estimator predicts reliably whenever \(n\gg\log[p]\)--which even allows for models with \(p\gg n\). In strong contrast, the risk bound for the least-squares estimator on Page 9 decreases in the number of samples as \(1/n\) but increases in the number of model parameters \(p\) linearly; hence, the least-squares estimator predicts reliably only if \(n\gg p\).

The goal of this chapter is to generalize the above prediction bounds. The outline is depicted in Fig. 1: Mimicking the above derivations, we first establish basic inequalities (Sect. 6.2) and then transform them into power-one and power-two prediction bounds (Sect. 6.3). These bounds generalize the ones above from the lasso to a wide spectrum of regularized least-squares estimators. We then deepen our discussion of power-two bounds in the context of sparsity and correlations (Sect. 6.4). This discussion will also be the basis for estimation bounds later in the book Sect. 7.2). Our proofs in this chapter and in the following one are purely algebraic, which means that we impose assumptions on the noise distribution only in the examples.

### Basic Inequalities

Basic inequalities are our stepping stone for establishing prediction bounds because they express the prediction error through inner product and prior function terms that are readily amenable to further analysis. We introduce two approaches for deriving such inequalities for regularized least-squares estimators. Our first approach generalizes the strategy outlined in the previous section: it uses that, directly by definition, the objective function evaluated at

Figure 1: Overview of this chapter. In Sect. 6.2, we introduce two approaches for deriving basic inequalities. In Sect. 6.3, we then use these basic inequalities to derive two types of probability bounds, which are the main results of this chapter. In Sect. 6.4, we provide further background on the assumptions needed for power-two bounds

[MISSING_PAGE_EMPTY:6405]

[MISSING_PAGE_EMPTY:6406]

While the arguments of the initial link function \(\chi\) are real-valued, the arguments of the induced function \(\tilde{\chi}\) are vector-valued. We then allow for the following link functions:

\(\bullet\)Assumption6.2.1

**Link Functions** We assume that the link function \(\chi\) is differentiable at \(\|\mathbf{y}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\) with strictly positive derivative, that is, \(\chi^{\prime}\big{[}|\mathbf{y}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\big{]}>0\), and we assume that the induced link function \(\tilde{\chi}\) is convex.

One example that satisfies the assumption is again the identity function \(\chi:a\mapsto a:\) it is differentiable with constant derivative equal to \(1\), and its induced version \(\tilde{\chi}:\mathbf{a}\mapsto\|\mathbf{y}-X\mathbf{a}\|_{2}^{2}\) is convex (see 1. in Exercise 1.4). Another example that satisfies the assumption is \(\chi:a\mapsto\sqrt{a}\) (assuming \(\|\mathbf{y}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\neq 0\)).4 it is differentiable (assuming \(a\neq 0\)) with derivative equal to \(1/(2\sqrt{a})\)--which is strictly positive for \(a>0\)), and its induced version \(\tilde{\chi}:\mathbf{a}\mapsto\|\mathbf{y}-X\mathbf{a}\|_{2}\) is convex (because every norm is convex). The square-root function can be used to write the _square-root lasso_5

\[\widehat{\mathbf{\beta}}_{\sqrt{\text{lasso}}}\;\in\;\operatorname*{arg\,min}_{ \mathbf{\alpha}\in\mathbb{R}^{p}}\big{\{}\,\|\mathbf{y}-X\mathbf{\alpha}\|_{2}+r\|\mathbf{ \alpha}\|_{1}\,\big{\}} \tag{6.2}\]

as

\[\widehat{\mathbf{\beta}}_{\sqrt{\text{lasso}}}\;\in\;\operatorname*{arg\,min}_{\bm {\alpha}\in\mathbb{R}^{p}}\big{\{}\,\chi\big{[}|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2} \big{]}+r\|\mathbf{\alpha}\|_{1}\,\big{\}}\,,\]

which is of the assumed form (6.1). Thus, in contrast to the lemma derived above, the lemma to follow applies in particular to the lasso _and_ the square-root lasso.

\(\bullet\)Lemma6.2.1

**Second-Order Basic Inequality** Assume that the link function \(\chi\) satisfies Assumption6.2.1 and that the prior function \(\chi\) is convex. Then, it holds for every \(\mathbf{\alpha}\in\mathbb{R}^{p}\) that

\[|X\hat{\mathbf{\beta}}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\;\leq\;\|X\hat {\mathbf{\beta}}-X\mathbf{\alpha}\|_{2}^{2}+2\langle\mathbf{u},\;X\widehat{\mathbf{\beta}}-X \mathbf{\alpha}\rangle\\ +\frac{r}{\chi^{\prime}\big{[}|\mathbf{y}-X\widehat{\mathbf{\beta}}\|_{2 }^{2}\big{]}^{2}\big{]}}\chi\llbracket\mathbf{\alpha}\rrbracket-\frac{r}{\chi^{ \prime}\big{[}|\mathbf{y}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\big{]}^{2}\big{]}}\chi \llbracket\widehat{\mathbf{\beta}}\rrbracket\,.\]The first-order and second-order basic inequalities differ only in their assumptions (the first-order bound assumes that the link function is the identity, while the second-order bound assumes that the prior function is convex) and sometimes in factors--see Exercise 6.1 and Note [6] at the end of the chapter.

We have introduced the name "first-order basic inequality" for Lemma 6.2.1 to indicate that the proof uses the estimator's defining property directly:

\[\|\mathbf{v}-X\widehat{\mathbf{\beta}}\|_{2}^{2}+r\hat{\mu}[\widehat{\mathbf{\beta}}]\, \leq\,\|\mathbf{v}-X\mathbf{\alpha}\|_{2}^{2}+r\hat{\mu}[\mathbf{\alpha}]\qquad\text{for all}\,\mathbf{\alpha}\in\mathbb{R}^{p}\,.\]

Similarly, we have introduced the name "second-order basic inequality" for Lemma 6.2.2 to indicate that the proof does not use that property directly but instead the KKT conditions it implies.

Proof of Lemma 6.2.2The steps in this proof are more involved than those in the proof of the first-order bound, but the overall strategy is still simple: (i) We first establish the estimator's optimality conditions, that is, we set generalized derivatives of the objective function to zero. (ii) We then transform that equality into an inequality and, thereby, replace the potentially ambiguous subgradient by a term that is based on the prior function. (iii) We finally separate the estimator's prediction error from all other quantities in that inequality.

(i) We first show that the KKT conditions for the estimator \(\widehat{\mathbf{\beta}}\) in (6.1) yield

\[\mathbf{0}_{p}\,=\,\,-\,2\mathpzc{G}\big{[}\|\mathbf{v}-X\widehat{\mathbf{\beta}}\|_{2}^{2 }\big{]}\big{(}X^{\top}(\mathbf{v}-X\widehat{\mathbf{\beta}})\big{)}+r\widehat{\mathbf{ \kappa}}\]

for a \(\widehat{\mathbf{\kappa}}\in\mathfrak{a}\#\widehat{\mathbf{\kappa}}[\widehat{\mathbf{\beta }}]\).

We write (6.1) as

\[\widehat{\mathbf{\beta}}\,\,\in\,\,\operatorname*{arg\,min}_{\mathbf{\alpha}\in \mathbb{R}^{p}}\mathpzc{f}[\mathbf{\alpha}]\]

with objective function

\[\mathpzc{f}\,:\,\mathbb{R}^{p} \,\,\to\,\,\mathbb{R}\] \[\mathbf{a} \,\mapsto\,\mathpzc{f}[\mathbf{\alpha}]\,:=\,\,\widetilde{\mathpzc{G}} [\mathbf{\alpha}]+r\hat{\mu}[\mathbf{a}]\,.\]

The objective function \(\mathpzc{f}\) is convex, because by assumption both the induced link function \(\widetilde{\mathpzc{G}}:\mathbf{a}\mapsto\mathpzc{G}\big{[}\|\mathbf{v}-X\mathbf{a}\|_{2}^ {2}\big{]}\) and the prior function \(\lambda\) are convex and \(r\geq 0\) (see Lemma 2.5.1 about the convexity of non-negative combinations).

The optimality conditions for the estimator \(\widehat{\boldsymbol{\beta}}\) are \(\boldsymbol{0}_{p}\in\mathfrak{a}\mathcal{\ell}[\widehat{\boldsymbol{\beta}}]\) (see Theorem 2.5.1). Using the differentiability of \(\chi\) (see Lemma 2.5.2) and the additivity of subdifferentials (see Lemma 2.5.3), we find that \(\boldsymbol{0}_{p}\in\mathfrak{a}\mathcal{\ell}[\widehat{\boldsymbol{\beta}}]\) is equivalent to

\[\boldsymbol{0}_{p}\;=\;\nabla\tilde{\chi}[\widehat{\boldsymbol{\beta}}]+r \widehat{\boldsymbol{\kappa}}\quad\quad\text{for a }\widehat{\boldsymbol{\kappa}}\in\mathfrak{a}\mathcal{\ell}[\widehat{\boldsymbol{\beta}}]\,.\]

The chain rule for differentials then gives us

\[\nabla\tilde{\chi}[\widehat{\boldsymbol{\beta}}]\;=\;-2\chi^{\prime}\big{[}[ \boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}]_{2}^{2}\big{]}\big{(}X^{\top }(\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}})\big{)}\,.\]

Combining the two displays finally yields

\[\boldsymbol{0}_{p}\;=\;-2\chi^{\prime}\big{[}[\boldsymbol{\psi}-X\widehat{ \boldsymbol{\beta}}]_{2}^{2}\big{]}\big{(}X^{\top}(\boldsymbol{\psi}-X \widehat{\boldsymbol{\beta}})\big{)}+r\widehat{\boldsymbol{\kappa}}\,,\]

as desired.

(ii) We now use (i) to prove that for every \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\)

\[-2\chi^{\prime}\big{[}[\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}]_{2}^ {2}\big{]}\big{(}X^{\top}(\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}) \big{)}^{\top}(\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}})+r\big{(} \mathcal{\ell}[\boldsymbol{\alpha}]-\mathcal{\ell}[\widehat{\boldsymbol{\beta }}]\big{)}\;\geq\;0\,.\]

We first conclude from (i) that for every \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\),

\[0\;=\;\boldsymbol{0}_{p}{}^{\top}(\boldsymbol{\alpha}-\widehat{\boldsymbol{ \beta}})\;=\;(-2\chi^{\prime}[[\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}} ]_{2}^{2}](X^{\top}(\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}))+r \widehat{\boldsymbol{\kappa}})^{\top}(\boldsymbol{\alpha}-\widehat{ \boldsymbol{\beta}})\,.\]

Indeed, the first equality is trivial, and the second equality follows directly from replacing \(\boldsymbol{0}_{p}\) with \(-2\chi^{\prime}\big{[}[\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}]_{2} ^{2}\big{]}\) (\(X^{\top}(\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}))+r\widehat{ \boldsymbol{\kappa}}\) as suggested by (i). Since \(\widehat{\boldsymbol{\kappa}}\in\mathfrak{a}\mathcal{\ell}[\widehat{ \boldsymbol{\beta}}]\), the definition of subgradients (see Definition 2.5.2) implies that for all \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\),

\[\mathcal{\ell}[\boldsymbol{\alpha}]\;\geq\;\mathcal{\ell}[\widehat{ \boldsymbol{\beta}}]+(\widehat{\boldsymbol{\kappa}},\;\boldsymbol{\alpha}- \widehat{\boldsymbol{\beta}})\,,\]

which (recall again that \(r\geq 0\)) can be rearranged to

\[r\big{(}\mathcal{\ell}[\boldsymbol{\alpha}]-\mathcal{\ell}[\widehat{ \boldsymbol{\beta}}]\big{)}\;\;\geq\;\;r\widehat{\boldsymbol{\kappa}}^{\top }(\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}})\,.\]

Combining this with the first equality yields

\[-2\chi^{\prime}\big{[}[\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}]_{2}^{ 2}\big{]}\big{(}X^{\top}(\boldsymbol{\psi}-X\widehat{\boldsymbol{\beta}}) \big{)}^{\top}(\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}})+r\big{(} \mathcal{\ell}[\boldsymbol{\alpha}]-\mathcal{\ell}[\widehat{\boldsymbol{\beta }}]\big{)}\geq 0\,,\]

as desired.

[MISSING_PAGE_EMPTY:6410]

Plugging this back into the result of (ii) and noting that \(\mathcal{A}^{\prime}\big{[}\|\mathbf{v}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\big{]}\) is positive by Assumption 6.2.1 yield \[-2\mathcal{A}^{\prime}\big{[}\|\mathbf{v}-X\widehat{\mathbf{\beta}}\|_{2}^{ 2}\big{]}\big{(}(1-a)\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}-\frac{\|X \mathbf{\alpha}-X\mathbf{\beta}\|_{2}^{2}}{4a}\] \[+\langle\mathbf{v}-X\mathbf{\beta},\;X\mathbf{\alpha}-X\widehat{\mathbf{\beta}} \rangle\big{)}+r\big{(}\lambda\lambda[\mathbf{\alpha}]-\lambda[\widehat{\mathbf{\beta} }\|\big{)}\;\;\geq\;\;0\,.\] Dividing both sides by \(-2(1-a)\mathcal{A}^{\prime}\big{[}\|\mathbf{v}-X\widehat{\mathbf{\beta}}\|_{2}^{2} \big{]}\) (recall again that \(\mathcal{A}^{\prime}\big{[}\|\mathbf{v}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\big{]}>0\) by Assumption 6.2.1 and \(a\in(0,1)\)) gives us \[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}-\frac{\|X\mathbf{\alpha }-X\mathbf{\beta}\|_{2}^{2}}{4(1-a)a}+\frac{\langle\mathbf{v}-X\mathbf{\beta},X\mathbf{\alpha} -X\widehat{\mathbf{\beta}}\rangle}{1-a}\] \[-\frac{r}{2(1-a)\mathcal{A}^{\prime}\big{[}\|\mathbf{v}-X\widehat{ \mathbf{\beta}}\|_{2}^{2}\big{]}}\big{(}\lambda[\mathbf{\alpha}]-\lambda[\widehat{\mathbf{ \beta}}\|\big{)}\;\leq\;0\,.\] Rearranging yields \[|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\,\leq\,\frac{\|X\mathbf{ \beta}-X\mathbf{\alpha}\|_{2}^{2}}{4(1-a)a}+\frac{\langle\mathbf{v}-X\mathbf{\beta},\;X \widehat{\mathbf{\beta}}-X\mathbf{\alpha}\rangle}{1-a}\] \[+\frac{r}{2(1-a)\mathcal{A}^{\prime}\big{[}\|\mathbf{v}-X\widehat{\bm {\beta}}\|_{2}^{2}\big{]}}\lambda[\mathbf{\alpha}]-\frac{r}{2(1-a)\mathcal{A}^{ \prime}\big{[}\|\mathbf{v}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\big{]}}\lambda[\widehat {\mathbf{\beta}}\|,\] from which the desired inequality follows by setting \(a=1/2\) and substituting \(\mathbf{u}\) for \(\mathbf{y}-X\mathbf{\beta}\) according to the linear regression model (2.1). 

### 6.3 Prediction Guarantees

We now establish prediction guarantees by bounding the basic inequalities' inner product term. Our main tool for this is the Holder inequality, which allows us to factor the inner product term into effective noise and a prior function quantity. Assuming that the effective noise is smaller than the tuning parameter, we can consolidate the factored expression with the estimators' regularization terms to obtain upper bounds for the prediction errors. These bounds are the main results of this chapter. They provide insights into how the estimators' prediction accu

[MISSING_PAGE_EMPTY:6412]

2. \(\mathcal{A}\) is symmetric: \(\mathcal{A}[\mathbf{a}]=\mathcal{A}[-\mathbf{a}]\) for all \(\mathbf{a}\in\mathbb{R}^{p}\).
3. \(\mathcal{A}\) satisfies the triangle inequality: \(\mathcal{A}[\mathbf{a}+\mathbf{b}]\leq\mathcal{A}[\mathbf{a}]+\mathcal{A}[\mathbf{b}]\) for all \(\mathbf{a}\), \(\mathbf{b}\in\mathbb{R}^{p}\).

The first part of this assumption is crucial in proving our power-one bounds, and the first and third parts are crucial in proving our power-two bounds. The second part, on the other hand, is used only for simplifications.

**Non-negative Lasso**: In this example, we introduce the _non-negative lasso_9

\[\widehat{\mathbf{\beta}}_{\text{nn}}\;\in\;\operatorname*{arg\,min}_{\mathbf{\alpha} \in\mathbb{R}^{p}}\big{\{}\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2}+r\mathcal{A}[\mathbf{ \alpha}]\big{\}}\]

with prior function

\[\mathcal{A}\;:\;\mathbf{a}\;\mapsto\;\sum_{j=1}^{p}\big{(}a_{j}\cdot 1\{a_{j} \geq 0\}+\infty\cdot 1\{\alpha_{j}<0\}\big{)}\,.\]

The non-negative lasso excludes negative parameter values by setting the objective function at such values to infinity; in other words, the non-negative lasso is the lasso with the additional constraint that all estimated model parameters are non-negative.

The prior function \(\mathcal{A}\) is not a norm: for example, it is not positive definite and not symmetric. However, the non-negative lasso is still of the form (6.1) and meets the conditions of the first-order basic inequality in Lemma 6.2.1,10 so that the basic inequality in Assumption 6.3.1 is satisfied with \(\widehat{w}=1\). Also, the prior function of the non-negative lasso meets 1. and 3. of Assumption 6.3.2 (see 1. of Exercise 6.2). Therefore, the techniques introduced in this section also apply to the non-negative lasso (see 2.-4. of Exercise 6.2).

We first generalize the power-one bound from the overview section.

**Power-One Prediction Bounds** Assume that the estimator \(\widehat{\boldsymbol{\beta}}\) of (6.1) satisfies a basic inequality as specified in Assumption 6.3.1. Now, if the prior function \(\lambda\) satisfies 1. of Assumption 6.3.2, it holds that

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\] \[\leq\ \min_{\boldsymbol{\alpha}\in\mathbb{R}^{\gamma}}\Big{\{} \|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+2\overline{\overline{ \epsilon}}[X^{\top}\boldsymbol{u}]\lambda[-\boldsymbol{\alpha}]+\widehat{w}r \lambda[\boldsymbol{\alpha}]\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\big{(}2\overline{\overline{ \epsilon}}[X^{\top}\boldsymbol{u}]-\widehat{w}r\big{)}\lambda[\widehat{ \boldsymbol{\beta}}]\Big{\}}\.\]

If additionally \(\widehat{w}r\geq 2\overline{\nu}\llbracket X^{\top}\boldsymbol{u}\rrbracket\) for a constant \(v\in[1,\infty)\) and \(\lambda\) satisfies 2. of Assumption 6.3.2, the bound implies (consolidate the \(\lambda\)-terms in the above inequality)

\[|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\ \leq\ \min_{ \boldsymbol{\alpha}\in\mathbb{R}^{\gamma}}\Big{\{} \|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+\Big{(}1+\frac{1}{v} \Big{)}\widehat{w}r\lambda[\boldsymbol{\alpha}]\Big{\}}\,\]

and if further \(v>1\) and \(r>0\), the first bound also implies (set \(\boldsymbol{\alpha}=\boldsymbol{\beta}\) in the first inequality, consolidate the \(\lambda\)-terms, bound the prediction term from below by 0, and note that \(0<\widehat{w}<\infty\) and \(r<\infty\) by assumption)

\[\lambda[\widehat{\boldsymbol{\beta}}]\ \leq\ \frac{v+1}{v-1}\lambda[\boldsymbol{ \beta}].\]

Recall the specification of dual functions in Definition 2.4.1. Dual functions are ubiquitous here and in the following sections because they allow us to bound the inner product term \(2(\boldsymbol{u},\ X\widehat{\boldsymbol{\beta}}-X\boldsymbol{\alpha})\) from Assumption (6.3.1) via the Holder inequality.

The first two bounds in the theorem generalize the power-one bound of the overview section. They involve the tuning parameter to the first power, hence the name power-one bounds. The third bound shows that the size of the estimated model in terms of \(\lambda\) is small if the tuning parameter is sufficiently large.

The bounds in Theorem 6.3.1 and in Theorem 6.3.2 below guarantee prediction accuracies in terms of \(X\), \(\boldsymbol{\beta}\), and \(\boldsymbol{u}\). Since \(\boldsymbol{\beta}\) and \(\boldsymbol{u}\) are unknown in applications, the bounds cannot be computed in practice; in particular, the bounds do not provide any practical confidence intervals. Still, the guarantees can provide a rough idea for what to expect from a given estimator in practice:

[MISSING_PAGE_EMPTY:6415]

Focusing on the sample size \(n\), this bound guarantees the average prediction error of the least-squares estimator to decrease as \(1/n\), while the above bound guarantees the average prediction error of the square-root lasso to decrease only as \(1/\sqrt{n}\); on the other hand, focusing on the number of model parameters \(p\), the bound for the least-squares estimator increases linearly in \(p\), while the bound for the square-root lasso increases only logarithmically in \(p\). These results corroborate our narrative that the larger the number of model parameters is, as compared to the number of samples, the more essential are high-dimensional estimators.

We now prove our first set of probability bounds.

#### Proof of Theorem 6.3.1

The proof goes along the same lines as in the lasso case in the overview section.

The inner product term on the right-hand side of the basic inequality in Assumption 6.3.1 can be bounded via the Holder inequality:

\[2\langle\mathbf{u},\;X\widehat{\mathbf{\beta}}-X\mathbf{\alpha}\rangle\] \[=\;2\langle\mathbf{u},\;X\widehat{\mathbf{\beta}}\rangle+2\langle\mathbf{u}, \;-X\mathbf{\alpha}\rangle\] linearity of inner product \[=\;2\langle X^{\top}\mathbf{u},\;\widehat{\mathbf{\beta}}\rangle+2\langle X ^{\top}\mathbf{u},\;-\mathbf{\alpha}\rangle\] property (B.1) of transposition \[\leq\;2\overline{\mathcal{R}}[X^{\top}\mathbf{u}]\mathcal{R}[\widehat{\mathbf{\beta }}]+2\overline{\mathcal{R}}[X^{\top}\mathbf{u}]\mathcal{R}[-\mathbf{\alpha}]\,.\]

Plugging this back into the assumed bound yields for every \(\mathbf{\alpha}\in\mathbb{R}^{p}\)

\[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\;\leq\;|X\mathbf{\beta} -X\mathbf{\alpha}\|_{2}^{2}\] \[+2\overline{\mathcal{R}}[X^{\top}\mathbf{u}]\mathcal{R}[\widehat{\mathbf{ \beta}}]+2\overline{\mathcal{R}}[X^{\top}\mathbf{u}]\mathcal{R}[-\mathbf{\alpha}]+ \widehat{w}r\mathcal{R}[\mathbf{\alpha}]-\widehat{w}r\mathcal{R}[\widehat{\mathbf{ \beta}}]\,,\]

and therefore, rearranging,

\[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\;\leq\;|X\mathbf{\beta} -X\mathbf{\alpha}\|_{2}^{2}\] \[+2\overline{\mathcal{R}}[X^{\top}\mathbf{u}]\mathcal{R}[-\mathbf{\alpha}] +\widehat{w}r\mathcal{R}[\mathbf{\alpha}]+\left(2\overline{\mathcal{R}}[X^{\top} \mathbf{u}]-\widehat{w}r\right)\mathcal{R}[\widehat{\mathbf{\beta}}]\,,\]

which concludes the proof of the first claim since \(\mathbf{\alpha}\) was arbitrary. The other two claims follow as indicated in the theorem.

We now generalize the power-two bound from the overview section.

**Power-Two Prediction Bounds** Assume that the estimator \(\widehat{\boldsymbol{\beta}}\) in (6.1) satisfies a basic inequality as specified in Assumption 6.3.1. Now, if the prior function \(\hat{\boldsymbol{\kappa}}\) satisfies 1. and 3. of Assumption 6.3.2, it holds for all \(b\in(1,\infty]\), \(m\in(0,\infty)\) that

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\, \leq\min_{\boldsymbol{\alpha}\in\widehat{\mathcal{B}}_{m}}\Big{\{}b\|X \boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}\] \[\qquad+\frac{(b+1)^{2}m^{2}}{4(b-1)n}\big{(}2\widehat{\boldsymbol {\kappa}}[-X^{\top}\boldsymbol{u}]+\widehat{w}r\big{)}^{2}\Big{\}}\;,\]

where \(\widehat{\mathcal{B}}_{m}:=\big{\{}\boldsymbol{a}\in\mathbb{R}^{p}\,:\,\hat{ \boldsymbol{\kappa}}[\boldsymbol{a}-\widehat{\boldsymbol{\beta}}]\,\leq\,m\|X \boldsymbol{a}-X\widehat{\boldsymbol{\beta}}\|_{2}/\sqrt{n}\,\big{\}}\). If additionally 2. of Assumption 6.3.2 holds and \(\widehat{w}r\geq 2\sqrt{\widehat{\boldsymbol{\kappa}}}[X^{\top}\boldsymbol{u}]\) for a positive and finite constant \(v\in(0,\infty)\), the bound implies directly (\(\hat{\boldsymbol{\kappa}}\) symmetric implies \(\overline{\hat{\boldsymbol{\kappa}}}\) symmetric according to 5. in Exercise 2.3)

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\leq\min_{ \boldsymbol{a}\in\widehat{\mathcal{B}}_{m}}\Big{\{}b\|X\boldsymbol{\beta}-X \boldsymbol{\alpha}\|_{2}^{2}+\frac{(b+1)^{2}m^{2}(v+1)^{2}\widehat{w}^{2}r^ {2}}{4(b-1)v^{2}n}\Big{\}}\;.\]

These bounds differ from the power-one bounds of Theorem 6.3.1 mainly in that they involve the tuning parameter to the power two (hence the naming) and in that they impose restrictions on the design. These restrictions come through \(\widehat{\mathcal{B}}_{m}\): rather than minimizing over arbitrary \(\boldsymbol{\alpha}\), the bounds minimize only over \(\boldsymbol{\alpha}\) that satisfy the structural condition, that is, over \(\boldsymbol{\alpha}\in\widehat{\mathcal{B}}_{m}\) for

\[\widehat{\mathcal{B}}_{m}\,=\,\big{\{}\boldsymbol{a}\in\mathbb{R}^{p}\,:\,\, \hat{\boldsymbol{\kappa}}[\boldsymbol{a}-\widehat{\boldsymbol{\beta}}]\,\leq \,m\|X\boldsymbol{a}-X\widehat{\boldsymbol{\beta}}\|_{2}/\sqrt{n}\big{\}}\;. \tag{6.3}\]

This structural condition generalizes the corresponding condition of \(\blacktriangleright\) Sect. 6.1 from the \(\ell_{1}\)-norm to arbitrary prior functions \(\hat{\boldsymbol{\kappa}}\). A detailed discussion about when the set \(\widehat{\mathcal{B}}_{m}\) is large enough to allow for favorable bounds follows in the next section; for the moment, we just observe that power-two bounds involve this additional restriction.

**Power-Two Prediction Bound for the Square-Root Lasso** In this example, we establish a specific power-two bound for the square-root lasso estimator (6.2). Its main advantage over the power-one bound in Example 6.3.2 is the \(1/n\)-rate in the sample size; its main disadvantage is the involvement of the structural condition.

We make the same assumptions as in Example 6.3.2: \(\widehat{w}r=2\|X^{\top}\mathbf{u}\|_{\infty}\), \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathbf{I}_{n\times n}]\) for some standard deviation \(\sigma\in(0,\infty)\), and \((X^{\top}X)_{jj}=n\) for all \(j\in\{1,\ldots,p\}\). Additionally, we make the assumption that the regression vector \(\mathbf{\beta}\) satisfies the structural condition, that is, \(\mathbf{\beta}\in\widehat{\mathcal{B}}_{m}\) for a "reasonably small" \(m\). Setting \(b=3\) in the second part of Theorem 6.3.2 and invoking the mentioned assumptions, we find that

\[|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\sqrt{\text{lasso}}}|_{2}^{2}\leq\min_{\bm {\alpha}\in\widehat{\mathcal{B}}_{m}}\left\{3\|X\mathbf{\beta}-X\mathbf{\alpha}\|_{2 }^{2}+\frac{32m^{2}\|X^{\top}\mathbf{u}\|_{\infty}^{2}}{n}\right\}.\]

Now, if \(\mathbf{\beta}\in\widehat{\mathcal{B}}_{m}\),

\[\mathbb{P}\left\{\frac{\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{ \sqrt{\text{lasso}}}\|_{2}^{2}}{n}\leq\frac{128m^{2}\sigma^{2}\log[p/t]}{n}\right\}\] \[\geq\ \ \mathbb{P}\left\{\frac{\min_{\mathbf{\alpha}\in\widehat{ \mathcal{B}}_{m}}\left\{3\|X\mathbf{\beta}-X\mathbf{\alpha}\|_{2}^{2}+32m^{2}\|X^{\top }\mathbf{u}\|_{\infty}^{2}/n\right\}}{n}\right.\] \[\qquad\leq\frac{128m^{2}\sigma^{2}\log[p/t]}{n}\left\{\begin{array} []{c}\text{power-two bound above}\\ \text{bounding the minimum by setting }\mathbf{\alpha}=\mathbf{\beta}\text{ assuming }\mathbf{\beta}\in\widehat{\mathcal{B}}_{m}\end{array}\right.\] \[=\ \ \mathbb{P}\big{[}\,2\|X^{\top}\mathbf{u}\|_{\infty}\leq\sigma \sqrt{8mm\log[p/t]}\big{\}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{ consolidating and taking square-roots;}\] \[\qquad\qquad\qquad\qquad\qquad\qquad m=\max_{j\in[1,\ldots,p]}\,(X^{ \top}X)_{jj}/n=1\text{ by assumption }\] \[\geq\ \ 1-t\.\qquad\qquad\qquad\qquad\qquad\text{Lemma \ref{eqn:1}}\,(\text{Gauss-distributed noise})\]

Whether the power-two bound here improves on the power-one bound of Example 6.3.2 depends on the model specifications: the bound here decreases faster [13] in the sample size (\(1/n\) as compared to \(1/\sqrt{n}\) before), but it also stipulates an obscure condition about the regression vector and the design matrix (\(\mathbf{\beta}\in\widehat{\mathcal{B}}_{m}\) for a reasonably small \(m\)--see the following section).

The quantity \(2\overline{\hat{\mathbf{\varepsilon}}}[X^{\top}\mathbf{u}]\) in the restrictions on the tuning parameter is called the _effective noise_ of the regularized least-squares estimator \(\widehat{\mathbf{\beta}}\). For example, the effective noise for \(\mathcal{E}_{1}\)-regularization is \(2\overline{\hat{\mathbf{\varepsilon}}}[X^{\top}\mathbf{u}]=2\|X^{\top}\mathbf{u}\|_{\infty}\). If \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\,\sigma^{2}\mathbf{I}_{n\times n}]\) and \(\widehat{w}r=2\|X^{\top}\mathbf{u}\|_{\infty}\) (see the two foregoing examples), then \(\widehat{w}r\) is proportional to the standard deviation: \(2\|X^{\top}\mathbf{u}\|_{\infty}=\sigma\cdot 2\|X^{\top}\mathbf{u}/\sigma\|_{\infty}\) and \(\mathbf{u}/\sigma\sim\mathcal{N}_{n}[\mathbf{0}_{n},\mathbf{I}_{n\times n}]\) is independent of \(\sigma\). This then also means that the prediction bounds in Theorems 6.3.1 and 6.3.2, which involve \(\widehat{w}r\) and \((\widehat{w}r)^{2}\), respectively, increase with \(\sigma\). More generally, the effective noise embodies the influence of the noise vector \(\mathbf{u}\) on the prediction guarantees: the larger \(2\overline{\hat{\mathbf{\varepsilon}}}[X^{\top}\mathbf{u}]\), the larger \(\widehat{w}r\) needs to satisfy the restriction \(\widehat{w}r\geq 2\widehat{w}\overline{\hat{\mathbf{\varepsilon}}}[X^{\top}\mathbf{u}]\) for given \(v\); and since the right-hand sides of the prediction bounds increase with \(\widehat{w}r\), this means that the larger the effective noise, the looser the prediction guarantees. This relationship is commensurate with our naive expectation that prediction accuracies decrease when the noise level increases.

We finally prove our second set of probability bounds.

Proof of Theorem 6.3.2The proof goes again along the same lines as in the lasso case in the overview section.

The inner product term on the right-hand side of the basic inequality in Assumption 6.3.1 can be bounded via the Holder inequality:

\[2\langle\mathbf{u},\,X\widehat{\mathbf{\beta}}-X\mathbf{\alpha}\rangle\] \[=\,2\langle-\mathbf{u},\,X\mathbf{\alpha}-X\widehat{\mathbf{\beta}}\rangle\] linearity of inner products \[=\,2\langle-X^{\top}\mathbf{u},\,\mathbf{\alpha}-\widehat{\mathbf{\beta}}\rangle\] Property (B.1) of transposition \[\leq\,2\overline{\hat{\mathbf{\varepsilon}}}[-X^{\top}\mathbf{u}]\hat{\mathbf{ \varepsilon}}[\mathbf{\alpha}-\widehat{\mathbf{\beta}}]\,.\]

Plugging this back into the assumed bound yields for every \(\mathbf{\alpha}\in\mathbb{R}^{p}\)

\[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\leq|X\mathbf{\beta}-X \mathbf{\alpha}\|_{2}^{2}+2\overline{\hat{\mathbf{\varepsilon}}}[-X^{\top}\mathbf{u}]\hat {\mathbf{\varepsilon}}[\mathbf{\alpha}-\widehat{\mathbf{\beta}}]\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\widehat{w}r\hat{\mathbf{ \varepsilon}}[\mathbf{\alpha}]-\widehat{w}r\hat{\mathbf{\varepsilon}}[\widehat{\mathbf{ \beta}}]\,,\]

[MISSING_PAGE_EMPTY:6420]

Plugging this into the preceding display yields

\[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\] \[\leq\,|X\boldsymbol{\beta}-X\boldsymbol{\alpha}|_{2}^{2}+\frac{(b+ 1)m^{2}}{2(b-1)n}\big{(}2\overline{\boldsymbol{\alpha}}\big{[}-X^{\top} \boldsymbol{u}\big{]}+\widehat{w}r\big{)}^{2}\] \[\quad+\frac{b-1}{b+1}\|X\boldsymbol{\beta}-X\widehat{\boldsymbol {\beta}}\|_{2}^{2}+\frac{b-1}{b+1}\|X\boldsymbol{\beta}-X\boldsymbol{\alpha} \|_{2}^{2}\,.\]

Therefore, by consolidating terms,

\[\frac{2}{b+1}\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}} \|_{2}^{2}\] \[\leq\frac{2b}{b+1}\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2 }^{2}+\frac{(b+1)m^{2}}{2(b-1)n}\big{(}2\overline{\boldsymbol{\alpha}}[-X^{ \top}\boldsymbol{u}]+\widehat{w}r\big{)}^{2}\]

and then

\[|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\ \leq\ b|X \boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}+\frac{(b+1)^{2}m^{2}}{4(b -1)n}\big{(}2\overline{\boldsymbol{\alpha}}[-X^{\top}\boldsymbol{u}]+\widehat{ w}r\big{)}^{2}\,.\]

This concludes the proof. 

### 6.4 Prediction Guarantees for Sparse and Weakly Correlated Models

The minima in power-one bounds are taken over the entire parameter space \(\mathbb{R}^{p}\), while the minima in power-two bounds are taken only over subsets of it. These subsets can well be too small to contain good minimizers, and then power-two bounds are less informative than power-one bounds--or even completely void. On the other hand, there are also cases where power-two bounds do provide very useful guarantees. In this section, we identify such cases. The corresponding criteria are approximate sparsity and weak correlations: the regression vector must allow for a sparse approximation such that the correlations both among the predictors that correspond to the support of that surrogate vector and between those predictors and the remaining ones are low. These criteria essentially ensure correct estimation of the regression vector (see Sect. 7.2), which connects this section to the following chapter on estimation and support recovery theory.

The central quantity is the random set \(\widehat{\mathcal{B}}_{m}\) of (6.3), which has been introduced in Theorem 6.3.2 to formalize the structural condition. The following example demonstrates that this set is closely connected with the correlations among the predictors.

\(\square\) Example 6.6.30

**Link Between \(\widehat{\mathcal{B}}_{m}\) and the Correlations in the Design** In this example, we illustrate that the structural condition is closely linked to the correlations in the design: the larger the correlations, the more restrictive the condition. Assuming that the columns of \(X\) have Euclidean lengths of, say, \(\sqrt{n}\), a measure for the correlations among the predictors is the square-root of the smallest eigenvalue of the scaled Gram matrix \(X^{\top}X/n\):

\(c\,:=\,\,\min_{\boldsymbol{\delta}\in\mathbb{R}^{p}\setminus\boldsymbol{\theta }_{p}}\sqrt{\frac{\boldsymbol{\delta}^{\top}X^{\top}X\boldsymbol{\delta}}{n| \boldsymbol{\delta}|_{2}^{2}}}\,\in\,[0,1]\,.\)

The smaller \(c\), the stronger the correlations; the extreme cases are \(c=0\) (some predictors are linearly dependent) and \(c=1\) (all predictors are orthogonal; cf. 1. in Exercise 2.10).

Assume now that \(c>0\), which ensures for all \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) the inclusion \(X\boldsymbol{\alpha}=X\widehat{\boldsymbol{\beta}}\,\Rightarrow\,\boldsymbol{ \alpha}=\widehat{\boldsymbol{\beta}}\), and consider \(\mathcal{A}:\boldsymbol{\alpha}\mapsto|\boldsymbol{\alpha}|_{2}\). For \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) such that \(X\boldsymbol{\alpha}=X\widehat{\boldsymbol{\beta}}\), we then find

\[\mathcal{A}[\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}]\,=\,\frac{1}{c \sqrt{n}}\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}\|_{2}\,=\,0\,,\]

which means that the inequality in (6.3) is satisfied for every \(m\). For \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) such that \(X\boldsymbol{\alpha}\neq X\widehat{\boldsymbol{\beta}}\), we find

\[\mathcal{A}[\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}]\] \[=\,\|\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}\|_{2}\] choice of \[\mathcal{A}\] \[=\,\sqrt{\frac{n\|\boldsymbol{\alpha}-\widehat{\boldsymbol{ \beta}}\|_{2}^{2}}{(\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}})^{\top}X ^{\top}X(\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}})}}\sqrt{\frac{( \boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}})^{\top}X^{\top}X(\boldsymbol {\alpha}-\widehat{\boldsymbol{\beta}})}{n}}\] multiplying by a one-valued factor \[\leq\,\max_{\boldsymbol{\delta}\in\mathbb{R}^{p}\setminus \boldsymbol{\theta}_{p}}\sqrt{\frac{n\|\boldsymbol{\delta}\|_{2}^{2}}{ \boldsymbol{\delta}^{\top}X^{\top}X\boldsymbol{\delta}}}\sqrt{\frac{( \boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}})^{\top}X^{\top}X(\boldsymbol {\alpha}-\widehat{\boldsymbol{\beta}})}{n}}\] maximizing the first term \[=\,\frac{1}{c}\sqrt{\frac{(\boldsymbol{\alpha}-\widehat{ \boldsymbol{\beta}})^{\top}X^{\top}X(\boldsymbol{\alpha}-\widehat{\boldsymbol {\beta}})}{n}}\] above definition of \[c\] \[=\,\frac{1}{c\sqrt{n}}\|X\boldsymbol{\alpha}-X\widehat{ \boldsymbol{\beta}}\|_{2}\,,\] definition of the \[\ell_{2}\] -normwhich means that the inequality in (6.3) is satisfied for every \(m\geq 1/c\). In addition, for \(\boldsymbol{\alpha}=\boldsymbol{\delta}-\widehat{\boldsymbol{\beta}}\) with \(\boldsymbol{\delta}\) minimizing the expression in the definition of \(c\), we deduce from that definition that

\[\boldsymbol{\alpha}[\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}]\,=\, \frac{1}{c\sqrt{n}}\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}\|_{2}\,.\]

Therefore, \(\widehat{\mathcal{B}}_{m}=\mathbb{R}^{p}\) if and only if \(m\geq 1/c\). This illustrates the general fact that the larger the correlations, the larger \(m\) needs to be to ensure that the structural condition holds for all possible regression vectors.

The power-two bounds of Theorem 6.3.2 are most useful if both \(\widehat{\mathcal{B}}_{m}\) is large (to ensure flexibility in the minimization) and \(m\) is small (to keep the terms quadratic in \(m\) at bay). The preceding example relates these characteristics of power-two bounds to the correlations among the predictors; in particular, the example shows that \(\widehat{\mathcal{B}}_{m}\) is even the full \(\mathbb{R}^{p}\) for reasonably small \(m\) if the eigenvalues of the Gram matrix are sufficiently large, that is, the correlations among all predictors are weak. However, this scenario is unrealistic in high dimensions: the more predictors there are, the more likely there are some strong correlations, maybe even collinearities. If \(p>n\), collinearities are unavoidable altogether, and then \(c=0\) (see 1. in Exercise 1.2), which means that \(\widehat{\mathcal{B}}_{m}\neq\mathbb{R}^{p}\) irrespective of \(m\).

Nevertheless, the remainder of this section identifies cases where power-two bounds are informative even in high dimensions. These cases require certain predictors to be correlated weakly among themselves and with all other predictors, but they allow those other predictors to be correlated among themselves arbitrarily--which relaxes the assumption in the example that _all_ predictors are weakly correlated.

To understand why this relaxation is possible, let us think about connections between prediction and estimation. Having power-two bounds providing good prediction guarantees for an estimator \(\widehat{\boldsymbol{\beta}}\) obviously means that (i) \(\widehat{\boldsymbol{\beta}}\) is a good predictor of the regression vector, that is, \(\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\) is small, and also implies by the form of those bounds that (ii) the set in question contains good prediction surrogates of the regression vector, that is, there are \(\boldsymbol{\alpha}\in\widehat{\mathcal{B}}_{m}\) with \(\|X\boldsymbol{\beta}-X\boldsymbol{\alpha}\|_{2}^{2}\) small. The first one of these two observations ensures that \(|X\boldsymbol{\beta}-X\boldsymbol{\alpha}|_{2}^{2}\approx|X\widehat{\boldsymbol{ \beta}}-X\boldsymbol{\alpha}|_{2}^{2}\) is small, which in turn by the definition of \(\widehat{\mathcal{R}}_{m}\) and the second observation, means that \(\mathcal{R}[\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}]\) is small, that is, every such \(\boldsymbol{\alpha}\) has a small estimation error (in \(\mathcal{R}\)-loss). Hence, power-two bounds, and especially their involvement of \(\widehat{\mathcal{R}}_{m}\), intertwine prediction and estimation.

However, if there are linear dependencies among the predictors (again, this is always the case when \(p>n\)), the regression vector \(\boldsymbol{\beta}\) itself becomes ambiguous: there are then \(\boldsymbol{\gamma}\neq\boldsymbol{0}_{p}\) such that \(X\boldsymbol{\beta}=X(\boldsymbol{\beta}+\boldsymbol{\gamma})\), and no surrogate \(\boldsymbol{\alpha}\) can be close (in Euclidean norm, for example) to all such regression vectors \(\boldsymbol{\beta}+\boldsymbol{\gamma}\) simultaneously. Hence, the connection between prediction and estimation outlined above cannot hold in general, and consequently, power-two bounds cannot provide good prediction guarantees in general.

In special cases, however, the outlined connection can hold indeed--even in high dimensions. Recall that the basic requirement for high-dimensional statistics is prior information. In the theories in this section and all of Chap. 7, we invoke the most common type of prior information, sparsity, which allows us to sharpen our focus from the entire, typically large, parameter space to a small subspace of it. In fact, we invoke the weaker assumption that the regression vector can be _approximated_ by a sparse surrogate: there is an \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) such that \(\boldsymbol{\alpha}\approx\boldsymbol{\beta}\) and \(|\text{supp}[\boldsymbol{\alpha}]|\ll n,p\). We call the corresponding linear regression models _approximately sparse_.

Approximate sparsity allows us to think in terms of a small set of "relevant" predictors indexed by \(\mathcal{S}:=\text{supp}[\boldsymbol{\alpha}]\) and a set of "irrelevant" predictors indexed by \(\mathcal{S}^{\mathcal{C}}=\{1,\ldots,p\}\setminus\text{supp}[\boldsymbol{ \alpha}]\). If we would know the set \(\mathcal{S}\) beforehand, we could replace the original linear regression model by a model of the form

\[\boldsymbol{y}=X_{\mathcal{S}}\boldsymbol{\alpha}_{\mathcal{S}}+\boldsymbol{u}\]

over the much smaller parameter space \(\mathbb{R}^{|\mathcal{S}|}\). This implies in particular that approximate sparsity removes any ambiguity from the regression vector as long as the predictors in the _restricted_ design matrix \(X_{\mathcal{S}}\in\mathbb{R}^{n\times|\mathcal{S}|}\) are linearly independent--compare this to the above example. Moreover, if those predictors are in addition only weakly correlated with all other predictors, we could hope that an estimator can disentangle the relevant predictors (with

[MISSING_PAGE_FAIL:206]

\(v\in(1,\infty)\), we assume that the _compatibility condition_ holds:

\[\begin{array}{l}\alpha[\delta]\leq\,\frac{m|\lambda X\delta|_{2}}{\sqrt{n}}\quad \quad\text{for all}\;\delta\in\mathbb{R}^{p}\;:\;\hat{\pi}_{\delta}\circ[ \delta_{\delta}]\leq\,\frac{v+1}{v-1}\hat{\pi}_{\delta}[\delta_{\delta}]\,. \end{array}\]

We call \(m\) the _compatibility constant_ for \(\mathcal{S}\), \(\hat{\pi}\).

The requirement on the correlations is captured by the inequality on the left-hand side; the requirement on the sparsity is captured by the inequality in the definition of admissible vectors \(\delta\) on the right-hand side. The vectors \(\delta\) play the role of \(\alpha-\widehat{\beta}\); hence, Assumption 6.4.1 ensures that the correlations in the design are small enough such that the set \(\widehat{\mathcal{B}}_{m}\) of the structural condition contains all vectors \(\alpha\) for which \(\alpha-\widehat{\beta}\) has limited mass on the set of irrelevant predictors. The following example and Exercise 6.5 illustrate that the compatibility condition indeed relaxes the previous assumption in Example 6.4.1 on the eigenvalues of the Gram matrix in that it allows for arbitrary correlations among irrelevant predictors.

[style=offset,frametitle=Example 6.4.2,right=6.4.2,leftmargin=*]
**Prototypical Cases Where the Compatibility Condition Holds**

In this example, we highlight a class of settings where the compatibility condition of Assumption 6.4.1 is met. We consider a set of relevant predictors \(\mathcal{S}:=\{1,\ldots,s\}\) with \(s\in\{1,\ldots,p\}\), a compatibility constant \(m\in(0,\infty)\), and a design with scaled Gram matrix of the block-diagonal form

\[\begin{array}{l}\frac{X^{\top}X}{n}\,=\,\begin{pmatrix}A\\ B\end{pmatrix},\end{array}\]

where the smallest eigenvalue of \(A\in\mathbb{R}^{s\times s}\) is assumed to be larger or equal to \((2\sqrt{s}v/(m(v-1)))^{2}\), and \(B\in\mathbb{R}^{(p-s)\times(p-s)}\) is positive semi-definite by construction (see 1. in Exercise 1.2). The lower bound on the eigenvalues of \(A\) limits the correlations among the predictors with indexes in \(\mathcal{S}\), and the block-diagonal form of the Gram matrix stipulates that those relevant predictors are uncorrelated with all remaining predictors. We then find for all \(\boldsymbol{\delta}\in\mathbb{R}^{p}\)

\[\frac{m\|X\delta\|_{2}}{\sqrt{n}} = m\sqrt{\frac{\boldsymbol{\delta}^{\top}X^{\top}X\boldsymbol{ \delta}}{n}}\] definition of the \[\ell_{2}\] -norm \[= m\sqrt{\boldsymbol{\delta}_{\delta}^{\top}A\boldsymbol{\delta}_{ \delta}+\boldsymbol{\delta}_{\delta}\varrho^{\top}B\boldsymbol{\delta}_{\delta}}\] block-diagonal form of \[X^{\top}X/n\] \[\geq m\sqrt{\left(\frac{2\sqrt{sv}}{m(v-1)}\right)^{2}\|\boldsymbol{ \delta}_{\delta}\|_{2}^{2}+0}\] \[A\]'s eigenvalues are equal or larger than \[(2\sqrt{sv}/(m(v-1)))^{2}\] ; \[B\] is positive semi-definite \[= \frac{2\sqrt{sv}}{v-1}\|\boldsymbol{\delta}_{\delta}\|_{2}\] consolidating \[\geq \frac{2v}{v-1}\|\boldsymbol{\delta}_{\delta}\|_{1}\,.\] Exercise 7.1: first inequality of the first claim \[\text{with }k=1,l=2,p=s,\boldsymbol{\epsilon}=\boldsymbol{\delta}_{\delta}\]

We now consider as a prior function \(\boldsymbol{\delta}:\boldsymbol{\alpha}\mapsto\boldsymbol{|\alpha|}_{1}\) with decomposition \(\boldsymbol{\mu}_{\delta}:\mathbb{R}^{s}\to\mathbb{R}\); \(\boldsymbol{\alpha}_{\delta}\mapsto\boldsymbol{|\alpha}_{\delta}\boldsymbol{ \delta}\boldsymbol{|}_{1}\) and \(\boldsymbol{\mu}_{\delta}\varrho:\mathbb{R}^{p-s}\to\mathbb{R}\); \(\boldsymbol{\alpha}_{\delta}\varrho\mapsto|\boldsymbol{\alpha}_{\delta} \boldsymbol{|}_{1}\). For every \(\boldsymbol{\delta}\in\mathbb{R}^{p}\) that satisfies \(\boldsymbol{\mu}_{\delta}\varrho\boldsymbol{|}\boldsymbol{\delta}\varrho \boldsymbol{|}\leq(v+1)\boldsymbol{\mu}_{\delta}\boldsymbol{|\delta}_{\delta} \boldsymbol{|}/(v-1)\), that is, \(|\boldsymbol{|\delta}\varrho\boldsymbol{|}_{1}\leq(v+1)\|\boldsymbol{\delta} \boldsymbol{|}_{1}/(v-1)\), we then find

\[\boldsymbol{\mu}\boldsymbol{|\delta} = \|\boldsymbol{\delta}\boldsymbol{|}_{1}\] choice of \[\boldsymbol{\mu}\] \[= \|\boldsymbol{\delta}_{\delta}\boldsymbol{|}_{1}+\|\boldsymbol{ \delta}_{\delta}\varrho\|_{1}\] described decomposition of the \[\ell_{1}\] -norm \[\leq \|\boldsymbol{\delta}_{\delta}\boldsymbol{|}_{1}+\frac{v+1}{v-1} \|\boldsymbol{\delta}_{\delta}\boldsymbol{|}_{1}\] \[\boldsymbol{|\delta}_{\delta}\varrho\boldsymbol{|}_{1}\leq(v+1) \|\boldsymbol{\delta}_{\delta}\boldsymbol{|}_{1}/(v-1)\] by assumption \[= \frac{2v}{v-1}\|\boldsymbol{\delta}_{\delta}\boldsymbol{|}_{1}\,.\] consolidating

Combing the two displays yields

\[\boldsymbol{\mu}\boldsymbol{|\delta}\boldsymbol{|}\leq\frac{m\|X\boldsymbol{ \delta}\|_{2}}{\sqrt{n}}\qquad\text{for all }\boldsymbol{\delta}\in\mathbb{R}^{p}\,:\,\boldsymbol{\mu}_{\delta}\varrho \boldsymbol{|}\boldsymbol{|}\leq\frac{v+1}{v-1}\boldsymbol{\mu}_{\delta} \boldsymbol{|\delta}\boldsymbol{|}\,,\]

as required by Assumption 6.4. This means that the compatibility condition is satisfied irrespective of \(B\), that is, irrespective of the correlations among the predictors with indexes in \(\mathcal{E}^{\mathsf{C}}\).

In the sequel of this section, we show that Assumption 6.4.1 can indeed lead to informative power-two bounds. We first invoke a basic inequality as usual.

\(\blacksquare\)Assumption 6.4.2

**Generic Basic Inequality Revisited** We assume that there is a potentially random quantity \(\widehat{w}\in(0,\infty)\) such that the estimator \(\widehat{\boldsymbol{\beta}}\) satisfies for all \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) the basic inequality

\[\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\,\leq\,2( \boldsymbol{y}-X\boldsymbol{\alpha},\;X\widehat{\boldsymbol{\beta}}-X \boldsymbol{\alpha})+\widehat{w}r\hbar[\boldsymbol{\alpha}]-\widehat{w}r\hbar[ \widehat{\boldsymbol{\beta}}]\,.\]

The generic basic inequality here equals the generic basic inequality in Assumption 6.3.1 except that the regression vector \(\boldsymbol{\beta}\) is replaced by the general vector \(\boldsymbol{\alpha}\) (and accordingly, \(\boldsymbol{u}=\boldsymbol{y}-X\boldsymbol{\beta}\) is replaced by \(\boldsymbol{y}-X\boldsymbol{\alpha}\)). Making these replacements also in \(\blacktriangleright\) Sect. 6.2 similarly allows one to transfer the sufficient conditions established for the earlier version to the above version. The reason for our switch of focus from \(\boldsymbol{\beta}\) to \(\boldsymbol{\alpha}\) here is that it will facilitate the inclusion of sparse surrogates of the regression vector in the below treatment of the set \(\widehat{\mathcal{B}}_{m}\).

We finally make assumptions on the prior function.

\(\blacksquare\)Assumption 6.4.3

**Prior Functions** Given an index set \(\mathcal{S}\subset\{1,\ldots,p\}\), we impose the following five assumptions on the prior function \(\hbar\):

1. \(\hbar\) meets the conditions of Theorem 2.4.1 (Holder inequality).
2. \(\hbar\) is \(\mathcal{S}\)-decomposable according to Definition 6.4.1.
3. \(\hbar_{\mathcal{S}}\), \(\hbar_{\mathcal{S}}\), \(\hbar_{\mathcal{S}\complement}\) are positive definite: \(\hbar_{\mathcal{S}}\), \(\hbar_{\mathcal{S}}\complement\geq 0\), \(\hbar_{\mathcal{S}}[\boldsymbol{a}]=0\Leftrightarrow\boldsymbol{a}=\boldsymbol{ 0}_{s}\), and \(\hbar_{\mathcal{S}}\complement[\boldsymbol{a}]=0\Leftrightarrow\boldsymbol{a}= \boldsymbol{0}_{p-s}\).
4. \(\hbar_{\mathcal{S}}\) satisfies the triangle inequality: \(\hbar_{\mathcal{S}}[\boldsymbol{a}+\boldsymbol{b}]\leq\hbar_{\mathcal{S}}[ \boldsymbol{a}]+\hbar_{\mathcal{S}}[\boldsymbol{b}]\) for all \(\boldsymbol{a}\), \(\boldsymbol{b}\in\mathbb{R}^{p}\).
5. \(\hbar_{\mathcal{S}}\), \(\hbar_{\mathcal{S}}\), \(\hbar_{\mathcal{S}\complement}\) are symmetric: \(\hbar_{\mathcal{S}}[\boldsymbol{a}]=\hbar_{\mathcal{S}}[-\boldsymbol{a}]\) and \(\hbar_{\mathcal{S}}\complement[\boldsymbol{a}]=\hbar_{\mathcal{S}}\complement [-\boldsymbol{a}]\) for all \(\boldsymbol{a}\in\mathbb{R}^{s}\), \(\boldsymbol{b}\in\mathbb{R}^{p-s}\).

These five properties permit an effective manipulation of the basic inequality, which leads to:

\[\begin{array}{l}\includegraphics[width=142.26378pt]{P_{1}.pdf}\end{array}\]

**Double-Cone**: Consider an estimator \(\widehat{\boldsymbol{\beta}}\) that satisfies the generic basic inequality of Assumption 6.4.2 and whose prior function \(\hat{\mu}\) satisfies Assumption 6.4.3. Then, it holds for every vector \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) with \(\operatorname{supp}[\boldsymbol{\alpha}]\subset\mathcal{S}\) that

\[\begin{array}{l}\includegraphics[width=142.26378pt]{P_{2}.pdf}\end{array}\]

If additionally \(\widehat{w}r\geq 2v\overline{\hat{\mu}}[X^{\top}(\boldsymbol{y}-X\boldsymbol{ \alpha})]>0\) for a constant \(v\in(1,\infty)\), we can deduce from the above display that

\[\hat{\mu}_{\delta}\mathsf{c}[\boldsymbol{\alpha}_{\delta}\mathsf{c}-\widehat{ \boldsymbol{\beta}}_{\delta}\mathsf{c}1\leq\frac{v+1}{v-1}\hat{\mu}_{\delta} \mathsf{c}[\boldsymbol{\alpha}_{\delta}-\widehat{\boldsymbol{\beta}}_{\delta} ]\,.\]

An illustration of the second part of the lemma is provided in \(\blacksquare\) Fig. 6.2. The most important consequence of Lemma 6.4.1 is that the error vectors \(\boldsymbol{\delta}:=\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}\) with respect to any sparse surrogate \(\boldsymbol{\alpha}\) of the regression vector \(\boldsymbol{\beta}\) satisfy the restriction \(\hat{\mu}_{\delta}\mathsf{c}[\boldsymbol{\delta}_{\delta}\mathsf{c}]\leq(v+1) \hat{\mu}_{\delta}\mathsf{c}[\boldsymbol{\delta}_{\delta}\mathsf{c}]/(v-1)\) in the compatibility condition of Assumption 6.4.1. In other words, Lemma 6.4.1 and Assumption 6.4.1 taken together guarantee that the set \(\widehat{\mathcal{B}}_{m}\) of the structural condition contains all sparse vectors that describe the data well:

\[\begin{array}{l}\includegraphics[width=142.26378pt]{P_{1}.pdf}\end{array}\]

\(\widehat{\mathcal{B}}_{m}\)**: Under the Compatibility Condition Under the conditions of Lemma 6.4.1 (double-cone) and Assumption 6.4.1 (compatibility condition), it holds that

\[\begin{array}{l}\includegraphics[width=142.26378pt]{P_{2}.pdf}\end{array}\]

This theorem summarizes the above result and is the main result of this section. We will exploit it below in deriving effective power-two bounds.

\[\widehat{\boldsymbol{\beta}}\ \ \in\ \Big{\{}\,\boldsymbol{a}\in\mathbb{R}^{p}\ :\ \|\boldsymbol{\alpha}_{\mathcal{S}}\mathfrak{e}\,-\,\boldsymbol{a}_{\mathcal{S}}\mathfrak{e}\,\big{|}_{1}\ \ \leq\ \ \frac{v+1}{v-1}\|\boldsymbol{\alpha}_{\mathcal{S}}\,-\,\boldsymbol{a}_{ \mathcal{S}}\,\big{|}_{1}\,\Big{\}}\.\]

The smaller this double-cone, the more control we have over the estimator. The size of the cone is determined by the tuning parameter \(r\) and the set \(\mathcal{S}\): The smaller the tuning parameter \(r\), the smaller values for \(v\) need to be chosen, and therefore, the larger the cone (compare the top left panel to the top right panel). The larger \(\mathcal{S}\), the more dimensions are covered by \(\mathcal{A}_{\mathcal{S}}\), and therefore, the larger the cone (compare the bottom left panel to the bottom right panel)

Proof of Lemma 6.4.1The proof manipulates the basic inequality of Assumption 6.4.2 by exploiting the assumed properties of the prior function.

The inner product term of the basic inequality can be bounded through the Holder inequality (cf. 1. in Assumption 6.4.3)

\[2\langle\boldsymbol{y}-X\boldsymbol{\alpha},\ X\widehat{ \boldsymbol{\beta}}-X\boldsymbol{\alpha}\rangle\] \[=\ 2\langle X^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha}),\ \widehat{\boldsymbol{\beta}}-\boldsymbol{\alpha}\rangle\quad\text{property (\ref{eq:def}) of transposition}\] \[\leq\ 2\overline{\overline{\boldsymbol{\beta}}}[X^{\top}( \boldsymbol{y}-X\boldsymbol{\alpha})]\mathcal{U}[\widehat{\boldsymbol{\beta}}-\boldsymbol{\alpha}]\.\qquad\text{H\"{o}lder inequality (\ref{eq:def})}\]Plugging this into the basic inequality yields

\[|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}|_{2}^{2}\,\leq\,2 \overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha})]\ell[\widehat{ \boldsymbol{\beta}}-\boldsymbol{\alpha}]+\widehat{w}r\ell[\boldsymbol{\alpha}]- \widehat{w}r\ell[\widehat{\boldsymbol{\beta}}]\,.\]

Next, with the decomposability of \(\ell\) assumed in 2. of Assumption 6.4.3, we find

\[|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}|_{2}^{2}\] \[\leq\,2\overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{ \alpha})]\ell_{\delta}[\widehat{\boldsymbol{\beta}}_{\delta}-\boldsymbol{ \alpha}_{\delta}]+2\overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha })]\ell_{\delta}\mathrm{e}[\widehat{\boldsymbol{\beta}}_{\delta}\mathrm{e}- \boldsymbol{\alpha}_{\delta}\mathrm{e}]\] \[\qquad+\widehat{w}r\ell_{\delta}[\boldsymbol{\alpha}_{\delta}]+ \widehat{w}r\ell_{\delta}\mathrm{e}[\boldsymbol{\alpha}_{\delta}\mathrm{e}]- \widehat{w}r\ell_{\delta}[\widehat{\boldsymbol{\beta}}_{\delta}\mathrm{e}]- \widehat{w}r\ell_{\delta}\mathrm{e}[\widehat{\boldsymbol{\beta}}_{\delta} \mathrm{e}]\,.\]

Further, we can use \(\boldsymbol{\alpha}_{\delta}\mathrm{e}=\boldsymbol{0}_{|\delta}\mathrm{e}_{|}\) (since \(\mathrm{supp}[\boldsymbol{\alpha}]\subset\mathcal{S}\) by assumption) to modify the last term on the bottom line and to find that \(\ell_{\delta}\mathrm{e}[\boldsymbol{\alpha}_{\delta}\mathrm{e}]=0\) (by 3. of Assumption 6.4.3):

\[|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}|_{2}^{2}\] \[\leq\,2\overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{ \alpha})]\ell_{\delta}[\widehat{\boldsymbol{\beta}}_{\delta}-\boldsymbol{ \alpha}_{\delta}]+2\overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{ \alpha})]\ell_{\delta}\mathrm{e}[\widehat{\boldsymbol{\beta}}_{\delta}\mathrm{ e}-\boldsymbol{\alpha}_{\delta}\mathrm{e}]\] \[\qquad\qquad\qquad+\widehat{w}r\ell_{\delta}[\boldsymbol{\alpha} _{\delta}]-\widehat{w}r\ell_{\delta}[\widehat{\boldsymbol{\beta}}_{\delta}]- \widehat{w}r\ell_{\delta}\mathrm{e}[\widehat{\boldsymbol{\beta}}_{\delta} \mathrm{e}-\boldsymbol{\alpha}_{\delta}\mathrm{e}]\,.\]

Moreover, we can use the triangle inequality for \(\ell_{\delta}\) (see 4. of Assumption 6.4.3) to combine the first and third terms on the bottom line to derive

\[|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}|_{2}^{2}\] \[\leq\,2\overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{ \alpha})]\ell_{\delta}[\widehat{\boldsymbol{\beta}}_{\delta}-\boldsymbol{ \alpha}_{\delta}]+2\overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha })]\ell_{\delta}\mathrm{e}[\widehat{\boldsymbol{\beta}}_{\delta}\mathrm{e}- \boldsymbol{\alpha}_{\delta}\mathrm{e}]\] \[\qquad\qquad\qquad\qquad+\widehat{w}r\ell_{\delta}[\boldsymbol{ \alpha}_{\delta}-\widehat{\boldsymbol{\beta}}_{\delta}]-\widehat{w}r\ell_{ \delta}\mathrm{e}[\widehat{\boldsymbol{\beta}}_{\delta}\mathrm{e}- \boldsymbol{\alpha}_{\delta}\mathrm{e}]\,.\]

We can finally use the assumed symmetry of \(\ell_{\delta}\) and \(\ell_{\delta}\mathrm{e}\) (see 5. of Assumption 6.4.3) and summarize the terms to

\[\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}|_{2}^{2}\, \leq\,\big{(}2\overline{\ell}[X^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha})]+ \widehat{w}r\big{)}\ell_{\delta}[\boldsymbol{\alpha}_{\delta}-\widehat{ \boldsymbol{\beta}}_{\delta}]\] \[\qquad\qquad+\big{(}2\overline{\ell}[X^{\top}(\boldsymbol{y}-X \boldsymbol{\alpha})]-\widehat{w}r\big{)}\ell_{\delta}\mathrm{e}[\boldsymbol{ \alpha}_{\delta}\mathrm{e}-\widehat{\boldsymbol{\beta}}_{\delta}\mathrm{e}]\,,\]

as claimed in the first part of the lemma.

For the second part, we rearrange the above result and use the fact that \(\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\geq 0\) by the definition of the \(\ell_{2}\)-norm:

\[\big{(}\widehat{w}r-2\overline{\hat{\boldsymbol{\kappa}}} [X^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha})]\big{)}\hat{\varkappa}_{ \mathcal{S}^{\mathcal{E}}}[\boldsymbol{\alpha}_{\mathcal{S}^{\mathcal{E}}}- \widehat{\boldsymbol{\beta}}_{\mathcal{S}^{\mathcal{E}}}]\\ \leq\,\big{(}2\overline{\hat{\boldsymbol{\kappa}}}[X^{\top}( \boldsymbol{y}-X\boldsymbol{\alpha})]+\widehat{w}r\big{)}\hat{\varkappa}_{ \mathcal{S}}[\boldsymbol{\alpha}_{\mathcal{S}}-\widehat{\boldsymbol{\beta}}_{ \mathcal{S}}]\,.\]

Now, if \(\widehat{w}r\geq 2\widehat{w}\overline{\hat{\boldsymbol{\kappa}}}[X^{\top}( \boldsymbol{y}-X\boldsymbol{\alpha})]\), we obtain (recall the non-negativity of \(\hat{\varkappa}_{\mathcal{S}}\) and \(\hat{\varkappa}_{\mathcal{S}^{\mathcal{E}}}\) that follows from 3. of Assumption 6.4.3)

\[\Big{(}1-\frac{1}{v}\Big{)}\widehat{w}r\hat{\varkappa}_{\mathcal{S}^{\mathcal{ E}}}[\boldsymbol{\alpha}_{\mathcal{S}^{\mathcal{E}}}-\widehat{\boldsymbol{\beta}}_{ \mathcal{S}^{\mathcal{E}}}]\,\leq\,\Big{(}1+\frac{1}{v}\Big{)}\widehat{w}r\hat {\varkappa}_{\mathcal{S}}[\boldsymbol{\alpha}_{\mathcal{S}}-\widehat{ \boldsymbol{\beta}}_{\mathcal{S}}]\,.\]

Since \(1/v<1\) by assumption on \(v\), we then find

\[\widehat{w}r\hat{\varkappa}_{\mathcal{S}^{\mathcal{E}}}[ \boldsymbol{\alpha}_{\mathcal{S}^{\mathcal{E}}}-\widehat{\boldsymbol{\beta}}_{ \mathcal{S}^{\mathcal{E}}}] \leq\,\frac{1+\frac{1}{v}}{1-\frac{1}{v}}\widehat{w}r\hat{ \varkappa}_{\mathcal{S}}[\boldsymbol{\alpha}_{\mathcal{S}}-\widehat{ \boldsymbol{\beta}}_{\mathcal{S}}]\] \[=\,\frac{v+1}{v-1}\widehat{w}r\hat{\varkappa}_{\mathcal{S}}[ \boldsymbol{\alpha}_{\mathcal{S}}-\widehat{\boldsymbol{\beta}}_{\mathcal{S}}]\,,\]

from which the second claim can be derived by dividing through \(\widehat{w}r>0\). 

We now use the compatibility condition together with the double-cone property of high-dimensional estimators to rewrite the power-two bounds of the previous section.

**Theorem 6.4.1**: **Power-Two Bounds Revisited** Assume that the estimator \(\widehat{\boldsymbol{\beta}}\) of (6.1) satisfies the conditions of Theorem 6.3.2 (power-two bounds) and Lemma 6.4.1 (double-cone). Assume further that the design is weakly correlated in the sense that:

1. The estimator's prior function \(\hat{\varkappa}\) satisfies Assumption 6.4.1 (compatibility condition) for an index set \(\mathcal{S}\subset\{1,\ldots,p\}\) and constants \(m\in(0,\infty)\), \(v\in(1,\infty)\).

Assume finally that the model is approximately sparse in the sense that there is a vector \(\mathbf{\alpha}\in\mathbb{R}^{p}\) with

\(2.\)\(\text{supp}[\mathbf{\alpha}]\subset\delta\) and \(\widehat{w}r\geq 2\nu\widehat{\mathbf{\alpha}}[X^{\top}(\mathbf{y}-X\mathbf{\alpha})]>0.\)

\(3.\)\(\|X\mathbf{\beta}-X\mathbf{\alpha}\|_{2}^{2}\,\leq\,d\widehat{w}^{2}r^{2}/n\) for a (hopefully small) \(d\in[0,\,\infty].\)

Then,

\[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\,\leq\,\left(2d+\frac{9m^{2}(v+1 )^{2}}{2v^{2}}\right)\frac{\widehat{w}^{2}r^{2}}{n}\,.\]

This result replaces the minimum over the set \(\widehat{\mathscr{D}}_{m}\) in Theorem 6.3.2 by a more explicit bound. This new bound is essentially a factor times \(\widehat{w}^{2}r^{2}/n\), which confirms that the scaled prediction errors \(\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}/n\) of, say, the lasso/square-root lasso with optimal tuning are at most of order \(\sigma\log[p]/n\) (cf. Example 6.3.3) in approximately sparse and weakly correlated regression with Gauss-distributed noise.

Theorem 6.4.2 can be proved in three steps: First, Theorem 6.4.1 implies that \(\mathbf{\alpha}\in\widehat{\mathscr{D}}_{m}.\) Then, the second part of Theorem 6.3.2 with \(b=2\) and \(\mathbf{\beta}\) replaced by \(\mathbf{\alpha}\) (cf. the comments after Assumption 6.4.2) yields a bound for \(\|X\mathbf{\alpha}-X\widehat{\mathbf{\beta}}\|_{2}^{2}.\) Finally, Lemma B.1.1 (\(|\mathbf{a}+\mathbf{b}|_{2}^{2}\,\leq\,2|\mathbf{a}|_{2}^{2}+2|\mathbf{b}|_{2}^{2}\)) allows one to transfer this bound into a bound for \(\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}.\) We leave the details to the reader.

The major limitation of the above result is the compatibility condition, which stipulates approximate sparsity and weak correlations. Approximate sparsity and weak correlations will also be essential to the development of estimation bounds in the following chapter; in particular, we will show there that the assumptions of Theorem 6.4.2 guarantee estimation accuracy as well. Power-one bounds, in stark contrast, refer to prediction only, which reduce the requirements on sparsity and avoid assumptions on the correlation structure altogether.

### 6.5 Exercises

#### 6.5.1 Exercises for Sect. 6.2

**Exercise 6.1**\(\blacklozenge\) In this exercise, we compare the first-order and second-order basic inequalities in Lemmas 6.2.1and 6.2.2, respectively. We assume that the conditions of both lemmas are satisfied, that is, the link function is the identity \(\boldsymbol{\varrho}:a\mapsto a\) (cf. Lemma 6.2.1) and the prior function \(\boldsymbol{\lambda}\) is convex (cf. Lemma 6.2.2).
1. Show that the two bounds coincide under the mentioned conditions.
2. Show that Lemma 6.2.1 implies \[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\,\leq\,2\langle \boldsymbol{u},\ X\widehat{\boldsymbol{\beta}}-X\boldsymbol{\beta}\rangle+r \hat{\kappa}[\boldsymbol{\beta}]-r\hat{\kappa}[\widehat{\boldsymbol{\beta}}]\,,\] while a slight modification of the proof of Lemma 6.2.2 yields \[\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\,\leq\,\frac{1}{2 }\big{(}2\langle\boldsymbol{u},\ X\widehat{\boldsymbol{\beta}}-X\boldsymbol{ \beta}\rangle+r\hat{\kappa}[\boldsymbol{\beta}]-r\hat{\kappa}[\widehat{ \boldsymbol{\beta}}]\big{)}\,.\] This exercise shows that the second-order approach can improve on the first-order approach by a factor \(2\) if the conditions of both approaches are met and \(\boldsymbol{\alpha}=\boldsymbol{\beta}\) (see Claim 2), but the exercise also highlights once more that the two approaches differ mainly in their conditions--identity link function versus convex prior function--rather than in their resulting bounds (see Claim 1).

#### 6.5.2 Exercises for \(\blacktriangleright\) Sect. 6.3

**Exercise 6.2**\(\Diamond\Diamond\) In this exercise, we discuss the prior function of the non-negative lasso introduced in Example 6.3.1.

1. Show that the prior function of the non-negative lasso meets 1. and 3. of Assumption 6.3.2.
2. Generalize all bounds of \(\blacktriangleright\) Sect. 6.3 such that 2. of Assumption 6.3.2 is not required.
3. Compute the dual function of the non-negative lasso's prior function.
4. Use 3. to specify the bounds derived in 2. for the non-negative lasso.

This exercise highlights the fact that the techniques of \(\blacktriangleright\) Sect. 6.3 do not require the prior function to be symmetric or even a norm.

**Exercise 6.3**\(\Diamond\Diamond\) In this exercise, we tailor the probability bounds of \(\blacktriangleright\) Sect. 6.3 to the group-lasso estimator (2.3) and the square-root group-lasso estimator. We assume that the groups form a partition of \(\{1,\ldots,p\}\), that is, \(\cup_{j=1}^{k}\mathcal{A}_{j}=\{1,\ldots,p\}\), \(\mathcal{A}_{j}\neq\varnothing\) for all \(j\), and \(\mathcal{A}_{i}\cap\mathcal{A}_{j}=\varnothing\) for all \(i\neq j\).
1. Establish an analog of Example 6.3.2 for the group lasso.
2. Establish an analog of Example 6.3.3 for the group lasso.
3. Compare the group-lasso bounds to the square-root lasso bounds.
4. Repeat 1.-3. for the square-root group-lasso estimator of the form (cf. (2.9)) \[\widehat{\beta}_{\sqrt{\text{group}}}\;\in\;\operatorname*{arg\,min}_{\mathbf{a} \in\mathbb{R}^{p}}\left\{\|\mathbf{v}-X\mathbf{a}\|_{2}+r\sum_{j=1}^{k}|\mathbf{a}_{ \mathcal{A}_{j}}\|_{2}\right\}.\]

#### Exercises for \(\blacktriangleright\) Sect. 6.4

**Exercise 6.4**\(\blacklozenge\blacklozenge\) In this exercise, we study the \(\mathcal{S}\)-decomposability of Definition 6.4.1 in the context of the group lasso (2.3) (or equivalently the square-root group lasso (2.9)). The corresponding prior function is

\[\mathcal{A}:\mathbf{a}\mapsto\sum_{j=1}^{k}\|\mathbf{a}_{\mathcal{A}_{j}}\|_{2}\]

for a fixed integer \(k\in\{1,\ldots,p\}\) and a fixed partition \(\mathcal{A}_{1},\ldots,\mathcal{A}_{k}\) of \(\{1,\ldots,p\}\). As always, \(\mathbf{a}_{\mathcal{A}_{j}}\in\mathbb{R}^{|\mathcal{A}_{j}|}\) summarizes the coordinates of \(\mathbf{a}\) that have indexes in \(\mathcal{A}_{j}\).

1. Show first that in general, a function is \(\mathcal{S}\)-decomposable if and only if it is \(\mathcal{S}^{\complement}\)-decomposable.
2. Show that the prior function \(\mathbf{a}\mapsto|\mathbf{a}|_{2}\) of the edr estimator is a special case of the above prior function.
3. Show that the prior function of the edr estimator is \(\mathcal{S}\)-decomposable if and only if \(\mathcal{S}=\varnothing\) or \(\mathcal{S}=\{1,\ldots,p\}\). Show that, in contrast, the prior function \(\mathbf{a}\mapsto\|\mathbf{a}\|_{2}^{2}\) of the ridge estimator is decomposable for all \(\mathcal{S}\subset\{1,\ldots,p\}\).
4. Show that the prior function \(\mathbf{a}\mapsto\|\mathbf{a}\|_{1}\) of the lasso estimator is a special case of the above prior function.
5. Show that the prior function of the lasso estimator is \(\mathcal{S}\)-decomposable for all \(\mathcal{S}\subset\{1,\ldots,p\}\).

[MISSING_PAGE_EMPTY:6436]

2. Conclude from 1. that for \(\mathcal{S}=\{1\}\) and \(\mathcal{S}=\{2\}\), it holds that \[\min_{\boldsymbol{\delta}\in\mathcal{C}}\frac{\|X\boldsymbol{\delta}\|_{2}^{2}}{n \|\boldsymbol{\delta}\|_{1}^{2}}\,=\,\frac{(v-1)^{2}}{16v^{2}}\,.\] This means that the compatibility condition with those sets is satisfied if and only if \(m\in[4v/(v-1),\,\infty)\).
3. Conclude from 1. that for \(\mathcal{S}=\{1,2\}\), it also holds that \[\min_{\boldsymbol{\delta}\in\mathcal{C}}\frac{\|X\boldsymbol{\delta}\|_{2}^{2}}{ n\|\boldsymbol{\delta}\|_{1}^{2}}\,=\,\frac{(v-1)^{2}}{16v^{2}}\,.\] This means that the compatibility condition with this set is satisfied again if and only if \(m\in[4v/(v-1),\,\infty)\).
4. Conclude from 1. that if \(\{3\}\subset\mathcal{S}\) or \(\{4\}\subset\mathcal{S}\), it holds that \[\min_{\boldsymbol{\delta}\in\mathcal{C}}\frac{\|X\boldsymbol{\delta}\|_{2}^{2}}{ n\|\boldsymbol{\delta}\|_{1}^{2}}\,=\,0\,.\] This means that the compatibility condition with every such set cannot be satisfied.

The exercise illustrates that the compatibility constant limits correlations (i) among relevant predictors (that is, between any two predictors with indexes in \(\mathcal{S}\)) and (ii) between relevant and irrelevant predictors (that is, between any predictor with index in \(\mathcal{S}\) and any predictor with index in \(\mathcal{S}^{\mathcal{C}}\)), but it allows for correlations among irrelevant predictors (that is, between any two predictors with indexes in \(\mathcal{S}^{\mathcal{C}}\)).

### Notes and References

* 1 The term "prediction" is sometimes used differently in the literature; for example, prediction sometimes refers to the data fit \(|\boldsymbol{y}-X\boldsymbol{\widehat{\beta}}|_{2}^{2}\) or to the prediction of an outcome \(y_{n+1}\in\mathbb{R}\) for a _new_ vector \(\boldsymbol{x}_{n+1}\in\mathbb{R}^{p}\).
* 2 The terms "power-one/power-two" relate to the tuning parameters. Lederer et al. (2019), in contrast, speak of _sparsity bounds_ and _penalty bounds_, respectively, since the constant \(m\) is connected to sparsity (see our \(\blacktriangleright\) Sect. 6.4) and \(\|\boldsymbol{\alpha}\|_{1}\) to the prior term--which is also called penalty.

* 3 Since the effective noise \(2|X^{\top}\mathbf{u}|_{\infty}\) is unobserved and usually even has an unknown distribution, this is not a practical proposal. For tuning-parameter calibration in practice, refer to Chap. 4.
* 4 This is true under minimal assumptions: see Lederer et al. (2019), especially their Lemma A.3 on p. 1238.
* 5 The square-root lasso has been introduced in Belloni et al. (2011, Definition (8) on p. 792); the closely related _scaled lasso_ has been introduced in Sun and Zhang (2012, (3), (4), and (5) on p. 881) based on work by Antoniadis (2010), Stadler et al. (2010), Sun and Zhang (2010). Further theoretical guarantees and algorithms, as well as the version (2.9) with grouped variables, have been established in Bunea et al. (2014).
* 6 The proof of the second-order basic inequality is along the lines of Lederer et al. (2019, Proof of Theorem 2.1 on pp. 1249ff). That paper also discusses the factor two (see 2.in Exercise 6.1) that can be different between the two approaches, and it discusses different proof techniques more generally.
* 7 Finite sample guarantees in high-dimensional statistics are often called _oracle inequalities_ because they involve quantities (such as \(\mathbf{\beta}\) or aspects of it) that in practice would be known only by an _oracle_, that is, "a person (such as a priestess of ancient Greece) through whom a deity is believed to speak" (Merriam-Webster.com, 2019).
* 8 Our proofs in this section, as well as in the following sections, do not require that the estimators be of the form (6.1): in principle, one can take any estimator that satisfies Assumption 6.3.1 for a function \(\beta\) that satisfies (a subset of) the conditions in Assumption 6.3.2. However, it is easier to follow the derivations when one has the specific type of estimators (6.1) in mind.
* 9 This is the Lagrange version of Efron et al. (2004, Program (3.18) on p. 421). While Efron et al. (2004) call the estimator _positive lasso_, we use the name non-negative lasso to clarify that the prior function is infinite only if a parameter value is _strictly_ smaller than zero. Refer to Meinshausen (2013) and references therein for more information about regression with non-negative parameters.
* 10 One can also generalize the second-order basic inequality in Lemma 6.2.2 such that it applies to the non-negative lasso by generalizing the notion of convexity in Definition 2.5.1 from the real line to the extended real line. We omit the details and refer to Hiriart-Urruty and Lemarechal (2004) for background information on convexity.
* 11 The existence of such a tuning parameter is proved in Lederer et al. (2019, Lemma 2.1 on p. 1230).
* 12 In fact, the tuning is the only difference between the lasso and the square-root lasso: one can show that any lasso estimator for a given tuning parameter equals a square-root lasso estimator for some (usually different) tuning parameter and vice versa--we omit the details. A heuristic concept for the square-root lasso's differences to the lasso is established in Lederer and Muller (2015, p. 2730). That concept is also developed further into the _trex estimator_, which attempts \(\ell_{1}\)-regularized regression without any tuning parameters. Further details on that estimator can be found in Bien et al. (2018a,b).

* 13 This is why power-one bounds are often called _slow-rate bounds_ and power-two bounds _fast-rate bounds_. However, this naming is misleading: for example, Dalalyan et al. (2017, Chapter 5 on pp. 562ff) develop refined power-one prediction bounds for the lasso that have an \(1/n\)-rate under certain conditions. Since the square-root lasso and the lasso differ only in the tuning, these bounds also apply to the square-root lasso. In general, the issue of how prediction performances depend on the design of the regression model is intricate: this was pointed out by Hebiri and Lederer (2013) and van de Geer and Lederer (2013) and studied in depth in that mentioned paper by Dalalyan et al. (2017).
* 14 Decomposability has been discussed in Negahban et al. (2012, Section 2.2) and Wainwright (2014, Section 3.2). Our corresponding Definition 6.4.1 is a special case of Negahban et al. (2012, Definition 1 on p. 4) for \(\mathcal{M}:=\overline{\mathcal{M}}:=\{\boldsymbol{\alpha}\in\mathbb{R}^{p}: \operatorname{supp}[\boldsymbol{\alpha}]\subset\mathcal{S}\}\) and of Wainwright (2014, Equation (22) on p. 244) for \(\mathcal{M}:=\{\boldsymbol{\alpha}\in\mathbb{R}^{p}:\operatorname{supp}[ \boldsymbol{\alpha}]\subset\mathcal{S}\}\).
* 15 If \(\mathcal{S}=\varnothing\), we set \(\mathcal{R}_{\mathcal{S}}\equiv 0\) and \(\mathcal{R}_{\mathcal{S}\boldsymbol{0}}\equiv\mathcal{\hat{R}}\); the set of admissible \(\boldsymbol{\delta}\) in the display of Assumption 6.4.1 is then \(\{\boldsymbol{\delta}:\mathcal{\hat{R}}[\boldsymbol{\delta}]\leq 0\}\). Similarly, if \(\mathcal{S}=\{1,\ldots,p\}\), we set \(\mathcal{R}_{\mathcal{S}}\equiv\mathcal{\hat{R}}\) and \(\mathcal{R}_{\mathcal{S}\boldsymbol{0}}\equiv 0\); the set of admissible \(\boldsymbol{\delta}\) in the display of Assumption 6.4.1 is then \(\{\boldsymbol{\delta}:\mathcal{\hat{R}}[\boldsymbol{\delta}]\geq 0\}\).
* 16 Our compatibility condition is a modified and extended version of the one introduced in van de Geer (2007, Section 2.1). The closely related _restricted eigenvalue condition_ was introduced in Bickel et al. (2009, p. 1710). Comparisons among different conditions for the lasso can be found in van de Geer and Buhlmann (2009). A geometric interpretation of compatibility constants and relationships to entropy can be found in van de Geer and Lederer (2013). A generalization of the restricted eigenvalue beyond regression-type data is _restricted strong convexity_, introduced and discussed in Negahban et al. (2012, Section 2.4) and Wainwright (2014, Section 3.1). A refined version of compatibility constants and ideas for how to obtain bounds for them in practice are discussed in Dalalyan et al. (2017, Equation (10) on p. 557 and Appendix on pp. 578ff).

## Theory II: Estimation and Support Recovery

###### Contents

The regression model (1.1) in Sect. 1.1 relates blood levels of a biomarker with characteristics of the subjects' genomes. Corresponding data allow us to analyze this relationship from a variety of different perspectives: For example, we can study how the biomarker levels depend on the _ensemble of genes_, we can study the role of each _individual gene_, or we can study _which genes_ influence the biomarker levels in the first place. Summarizing the data in the vector-valued Eq. (1.2), the three different perspectives concern the three different quantities \(X\mathbf{\beta}\); \(\mathbf{\beta}\); and \(\text{supp}[\mathbf{\beta}]\), respectively, which motivate the prediction error \(\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}\|_{2}^{2}\), estimation errors such as \(\|\mathbf{\beta}-\widehat{\mathbf{\beta}}\|_{2}\), and support recovery errors such as false negatives \(|\{j\in\{1,\ldots,p\}:\beta_{j}\neq 0,\,\widehat{\beta}_{j}=0\}|\), respectively. The prediction error is the focus of the previous chapter; in this chapter, we develop theory in terms of estimation and support recovery errors. The statistical framework remains the same: We consider the linear regression model (2.1) and the regularized least-squares estimators (6.1).

### 7.1 Overview

The power-one bounds in Sect. 6.3 illustrate the fact that prediction is almost always possible. The reason is that prediction views the regression vector through the lens of the design matrix, which essentially reduces the complexity of the problem. Estimation and support recovery, in contrast, concern the regression vector directly. All theories in this chapter, therefore, hinge on strict conditions, namely sparsity and weak correlations.

For an illustration, consider again the genome application from Sect. 1.1. The predictors \(\mathbf{x}_{4}\), \(\mathbf{x}_{6}\), \(\mathbf{x}_{8}\in\{0,\,1,\ldots\}^{\times 7}\) that contain the copy numbers of the fourth, sixth, and eighth gene in the seven subjects are linearly dependent: \(\mathbf{x}_{4}=\mathbf{x}_{6}+\mathbf{x}_{8}\). Therefore, the "true" regression vector \(\mathbf{\beta}\) and every alternate \(\mathbf{\tilde{\beta}}\) with \(\widehat{\beta}_{4}+\widehat{\beta}_{6}=\beta_{4}+\beta_{6}\), \(\widehat{\beta}_{4}+\widehat{\beta}_{8}=\beta_{4}+\beta_{8}\), and \(\widehat{\beta}_{j}=\beta_{j}\) for \(j\notin\{4,\,6,\,8\}\) form the same data-generating process: \(\mathbf{y}=X\mathbf{\beta}+\mathbf{u}=X\widehat{\mathbf{\beta}}+\mathbf{u}\). Simply put, the correlations among the predictors make the regression vector ill-defined. Since \(X\mathbf{\beta}=X\widehat{\mathbf{\beta}}\) still holds, this fuzziness in the modeling is irrelevant for prediction, but it precludes estimation and support recovery. To make estimation and support recovery possible, we need to impose additional assumptions such as sparsity: Assume that \(|\text{supp}[\mathbf{\beta}]|\leq 1\)then, \(\widehat{\mathbf{\beta}}=\mathbf{\beta}\) is the only vector that satisfies the above equations and \(|\text{supp}[\widehat{\mathbf{\beta}}]|\leq 1\). This means that the sparsity counterbalances the correlations such that the target for estimation \(\mathbf{\beta}\) and the target for support recovery \(\text{supp}[\mathbf{\beta}]\) become well-defined.

In this chapter, we formalize such interplay between sparsity and correlations to establish guarantees for estimation and support recovery. The outline is depicted in \(\blacksquare\) Fig. 1: In \(\blacktriangleright\) Sect. 7.2, we use the power-two bounds and the compatibility condition introduced in \(\blacktriangleright\) Sections 6.3 and 6.4, respectively, to derive estimation bounds in the error induced by the prior function. In \(\blacktriangleright\) Sect. 7.3, we define the irrepresentability condition to derive estimation bounds in the error induced by the prior function's dual. In \(\blacktriangleright\) Sect. 7.3, we finally exploit specific examples of these bounds to derive bounds in terms of support recovery.

### Estimation Guarantees in \(\mathcal{L}\)-loss

In this section, we transform power-two prediction bounds into estimation bounds. The facilitator in this transformation is the compatibility condition, because it connects prediction terms \(|X\mathbf{\delta}|_{2}^{2}\) with estimation terms \(\hat{\varkappa}[\mathbf{\delta}]\)

Figure 1: Overview of this chapter. In \(\blacktriangleright\) Sect. 7.2, we transform the prediction guarantees of \(\blacktriangleright\) Sects. 6.3 and 6.4 into estimation guarantees in \(\hat{\varkappa}\)-loss. In \(\blacktriangleright\) Sect. 7.3, we introduce the primal-dual witness technique, which allows us to derive estimation guarantees in \(\mathcal{\widetilde{\bar{\varkappa}}}\)-loss. In \(\blacktriangleright\) Sect. 7.4, we turn specific dual function bounds into support recovery guarantees

[MISSING_PAGE_EMPTY:6443]

[MISSING_PAGE_EMPTY:6444]

[MISSING_PAGE_EMPTY:6445]

Step 3We now combine Step 2 with a slightly modified version of Theorem 6.3.2 (power-two bounds) to show that

\[\hat{w}[\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}]\,\leq\,\left(d+\frac{ \sqrt{2}m^{2}(v+1)}{v}\right)\frac{\widehat{w}r}{n}\,.\]

This is the bound stated in the theorem.

Theorem 6.3.2 bounds the prediction error with respect to the regression vector: \(\|X\boldsymbol{\beta}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\). However, replacing the assumption \(\widehat{w}r\,\geq\,2v\overline{\hat{w}}[X^{\top}\boldsymbol{u}]\) by \(\widehat{w}r\,\geq\,2v\overline{\hat{w}}[X^{\top}(\boldsymbol{y}-X\boldsymbol {\alpha})]\), we can take the exact same steps as in the proof of that theorem to bound the prediction error with respect to \(\boldsymbol{\alpha}\):

\[\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\,\leq\,\frac{(b +1)^{2}m^{2}(v+1)^{2}\widehat{w}^{2}r^{2}}{4(b-1)v^{2}n}\,.\]

#### Details

1. In a sense, this corresponds to writing the linear regression model (2.1) in the form \(\boldsymbol{y}=X\boldsymbol{\alpha}+\widetilde{\boldsymbol{u}}\) with \(\widetilde{\boldsymbol{u}}:=\boldsymbol{u}+X\boldsymbol{\beta}-X\boldsymbol{ \alpha}\); in particular, this reformulation of the model implies \(2v\overline{\hat{w}}[X^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha})]=2v \overline{\hat{w}}[X^{\top}\widetilde{\boldsymbol{u}}]\). 2. The prefactor in the above bound can be reduced by modifying the proof of Theorem 6.3.2 slightly--we leave this to the reader.

Since \(b\) is arbitrary in \((1,\,\infty]\), we can set \(b=3\) (which minimizes \((b+1)^{2}/(b-1)\) over \((1,\,\infty]\)) to find

\[\|X\boldsymbol{\alpha}-X\widehat{\boldsymbol{\beta}}\|_{2}^{2}\,\leq\,\frac{2m ^{2}(v+1)^{2}\widehat{w}^{2}r^{2}}{v^{2}n}\,.\]

Plugging this inequality into the one derived in Step 2 and consolidating yields

\[\hat{w}[\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}] \leq\frac{d\widehat{w}r}{n}+\frac{m^{\frac{\sqrt{2}m(v+1)}{v} \widehat{w}r}}{\sqrt{n}}\] \[=\left(d+\frac{\sqrt{2}m^{2}(v+1)}{v}\right)\frac{\widehat{w}r}{ n}\,,\]

as desired.

### Estimation Guarantees in \(\overline{\mathcal{H}}\)-loss

In this section, we derive bounds for \(\overline{\mathcal{H}}[\boldsymbol{\beta}-\overline{\boldsymbol{\beta}}]\) by using the _primal-dual witness technique_.1 This technique establishes conditions for a solution of an optimization problem that is restricted to a subset of the model's parameter space to be a solution also of the original, full optimization problem. Since it is computationally infeasible to verify these conditions for each potential subset, the primal-dual technique is not a viable algorithm for solving optimization problems. Instead, the primal-dual technique provides "theoretical witnesses" for the existence of sparse solutions.

We consider estimators of the form (6.1) with, for simplicity, the link function equal to the identity: \(g[a]=a\) for all \(a\in[0,\infty)\). We assume that the prior function \(\mathcal{H}:\mathbb{R}^{p}\to[0,\infty]\) is convex and \(\mathcal{S}\)-decomposable according to Definition 6.4.1 for a given index set \(\mathcal{S}\subset\{1,\ldots,p\}\). The convexity of the prior function enables us to apply the KKT conditions--see \(\blacktriangleright\) Sect. 2.5. The decomposability enables us to study the subspaces corresponding to \(\mathcal{S}\) and \(\mathcal{S}^{\complement}\) essentially separately. If an \(\mathcal{S}\)-decomposable prior function \(\mathcal{H}\) is convex, also its subfunctions \(\mathcal{H}_{\mathcal{S}},\mathcal{H}_{\mathcal{S}^{\complement}}\) are convex--see Exercise 7.2.

We now proceed in three steps: We first construct primal-dual pairs to restricted problems. We then establish conditions under which these pairs are solutions of the full problems as well. We then use this connection between the restricted and full problems as a basis for deriving a dual analog of Theorem 7.2.1.

#### Step 1: Construction of Primal-Dual Pairs

Our first step is to construct a primal-dual pair for a version of the optimization problem (6.1) that is restricted to model parameters with zero-valued coordinates outside a given index set \(\mathcal{S}\subset\{1,\ldots,p\}\). The primal vector is the solution of the restricted problem; the dual vector is its counterpart in terms of the KKT conditions.

[MISSING_PAGE_FAIL:229]

Lagrange dual vectors._ But the full names should remind us that the two concepts are very different: dual vectors are related to Lagrange dual functions; dual functions are related to the Holder inequality (see Sect. 2.4).3

#### Step 2: Establishing Conditions for a Successful Construction

Having introduced the primal vector through a solution of a restricted optimization problem in the first step, we show in this second step that this vector is also a solution of the full problem (6.1) (recall that in this section, \(\mathpzc{g}\) is the identity function and \(\mathpzc{h}\) convex and decomposable) under two conditions: 1. The "relevant" predictors indexed by \(\mathpzc{S}\) and the "irrelevant" predictors indexed by \(\mathpzc{S}^{\complement}\) are not too much correlated, so that these two groups can be distinguished based on data; 2. the tuning parameters are large enough to overrule the effective noise.

We formulate 1. in terms of the _irrepresentability condition_.

Assumption 7.3.1: **Irrepresentability Condition** Given an index set \(\mathpzc{S}\subset\{1,\ldots,p\}\), a prior function \(\mathpzc{h}\) that is convex and can be decomposed according to Definition 6.4.1, a corresponding primal-dual pair \((\widehat{\mathpzc{P}},\widehat{\mathpzc{v}})\), and a constant \(c\in(0,\,\infty)\), we assume that the _irrepresentability condition_ holds: \(X_{\mathpzc{S}}^{\top}X_{\mathpzc{S}}\) is invertible and

\[\sup_{\mathbf{a}\in\mathbf{\partial}\,\mathpzc{h}_{\mathpzc{S}}|\widehat{\mathpzc{P}},\mathpzc{S}^{\complement}}\overline{\mathpzc{h}}_{\mathpzc{S}^{\complement}} \big{[}X_{\mathpzc{S}^{\complement}}^{\top}X_{\mathpzc{S}}(X_{\mathpzc{S}}^{ \top}X_{\mathpzc{S}})^{-1}\mathbf{a}\big{]}\,\leq\,1-c\,,\]

where \(\overline{\mathpzc{h}}_{\mathpzc{S}^{\complement}}\,\coloneqq\,\overline{ \mathpzc{h}}_{\mathpzc{S}^{\complement}}\) is the dual function of \(\mathpzc{h}_{\mathpzc{S}^{\complement}}\) (see Definition 2.4.1). We call \(c\) the _irrepresentability constant_ for \(\mathpzc{S},\mathpzc{h}\).

The irrepresentability condition limits the correlations between relevant predictors (the columns of \(X_{\mathpzc{S}}\)) and irrelevant predictors (the columns of \(X_{\mathpzc{S}^{\complement}}\)). Essentially, it ensures that the columns of \(X_{\mathpzc{S}}\) cannot be expressed through the columns of \(X_{\mathpzc{S}^{\complement}}\): Exercise 7.3, for example, shows that for \(\mathpzc{h}\) equal to the lasso's \(\ell_{1}\)-norm, the assumption is satisfied if \(X_{\mathpzc{S}}^{\top}X_{\mathpzc{S}}\) is invertible and \(\max_{j\in\mathpzc{S}^{\complement}}\|\widehat{\mathpzc{h}}_{\mathrm{ls}}^{j} \|_{1}\leq 1-c\), where \(\widehat{\mathpzc{h}}_{\mathrm{ls}}^{j}\equiv\widehat{\mathpzc{h}}_{\mathrm{ ls}}^{j}[X_{\mathpzc{S}},\,X_{\mathpzc{S}}]\) is the least-squares estimator that regresses the \(j\)th predictor onto the predictors with indexes in \(\mathcal{S}\).

In \(\blacktriangleright\) Sect. 6.4, we have introduced another measure for the correlations in the design: the compatibility condition in Assumption 6.4.1.4 Both the compatibility condition and the irrepresentability condition are tailored for mathematical convenience rather than for plausibility in applications; therefore, results based on these assumptions should be taken with a grain of salt.

The following theorem is now the main result of Step 2.

**Theorem 7.2**.: **Successful Primal-Dual Construction** Assume that the design is weakly correlated in the sense that

1. The estimator's prior function \(\hat{\mu}\) satisfies Assumption 7.3.1 (irrepresentability condition) for an index set \(\mathcal{S}\subset\{1,\ldots,p\}\) and a constant \(c\in(0,\infty)\).

Assume that the tuning parameter is large enough in the sense that

1. \(r>2\overline{\hat{\mu}}_{\delta}\mathbb{E}\big{[}X_{\delta^{\complement}}^{ \top}\big{(}\mathbf{I}_{n\times n}-X_{\mathcal{S}}(X_{\delta}^{\top}X_{ \delta})^{-1}X_{\delta}^{\top}\big{)}\boldsymbol{\nu}\big{]}/c\).

Assume also that \(\hat{\mu}_{\delta^{\complement}}\) is a norm (cf. Exercise 2.3, Claim 6) and that \((\widehat{\boldsymbol{\gamma}},\widehat{\boldsymbol{\nu}})\) is a primal-dual pair constructed according to Definition 7.3.1. Then, \(\widehat{\boldsymbol{\gamma}}\) is also a solution of the full problem (6.1), that is,

\[\widehat{\boldsymbol{\gamma}}\,\in\,\operatorname*{arg\,min}_{\boldsymbol{ \alpha}\in\mathbb{R}^{p}}\big{\{}\|\boldsymbol{\nu}-X\boldsymbol{\alpha}\|_{2} ^{2}+r\hat{\mu}[\boldsymbol{\alpha}]\big{\}}\,.\]

We then say that the primal-dual construction was successful.

Assuming that the predictors are not too much correlated and the tuning parameter not too small, the theorem ensures a successful primal-dual construction, which means that the primal vector is also a solution of the original problem (6.1). Since the primal vector is supported on the set \(\mathcal{S}\) (see _(Primal 2)_ in Definition 7.3.1), the theorem guarantees that under the mentioned assumptions, there exists an \(\delta\)-sparse solution of (6.1). This solution is unique under additional conditions--see Exercise 7.4.

In order to establish a proof of Theorem 7.3.1, we first introduce an auxiliary lemma.

[MISSING_PAGE_FAIL:232]

[MISSING_PAGE_FAIL:233]

homogeneity of norms (see 6. in Exercise 2.3)

\[r\overline{\mathcal{R}}_{\mathcal{S}}[\widehat{\boldsymbol{v}}_{ \mathcal{S}}\mathcal{E}]\,\leq\,2\overline{\mathcal{R}}_{\mathcal{S}}\mathcal{E} \big{[}X_{\mathcal{S}}^{\top}\big{(}\mathsf{I}_{n\times n}-X_{\mathcal{S}}(X_{ \mathcal{S}}^{\top}X_{\mathcal{S}})^{-1}X_{\mathcal{S}}^{\top}\big{)} \boldsymbol{y}\big{]}\\ +r\overline{\mathcal{R}}_{\mathcal{S}}\mathcal{E}\big{[}X_{ \mathcal{S}}^{\top}X_{\mathcal{S}}(X_{\mathcal{S}}^{\top}X_{\mathcal{S}})^{-1} \widehat{\boldsymbol{v}}_{\mathcal{S}}\big{]}\,.\]

By assumption on \(r\), the first term on the right-hand side is bounded by

\[2\overline{\mathcal{R}}_{\mathcal{S}}\mathcal{E}\big{[}X_{\mathcal{S}}^{\top} (\mathsf{I}_{n\times n}-X_{\mathcal{S}}(X_{\mathcal{S}}^{\top}X_{\mathcal{S}} )^{-1}X_{\mathcal{S}}^{\top})\boldsymbol{y}\big{]}\,<\,rc\,.\]

By the assumed irrepresentability condition, and since \(\widehat{\boldsymbol{v}}_{\mathcal{S}}\in\mathfrak{a}\ell_{\mathcal{S}}[ \widehat{\boldsymbol{y}}]\) (see (i) in the above proof of Lemma 7.3.1), the second term on the right-hand side is bounded by

\[r\overline{\mathcal{R}}_{\mathcal{S}}\mathcal{E}\big{[}X_{\mathcal{S}}^{\top} X_{\mathcal{S}}(X_{\mathcal{S}}^{\top}X_{\mathcal{S}})^{-1}\widehat{\boldsymbol{v}}_{ \mathcal{S}}\big{]}\,\leq\,r(1-c)\,.\]

Collecting terms gives

\[r\overline{\mathcal{R}}_{\mathcal{S}}\mathcal{E}[\widehat{\boldsymbol{v}}_{ \mathcal{S}}\mathcal{E}]\,\leq\,rc+r(1-c)\,\,\,=\,\,\,r\,,\]

which implies in view of \(r\in(0,\,\infty)\) (\(r>0\) according to an earlier part of the proof; \(r<\infty\) by assumption) that

\[\overline{\mathcal{R}}_{\mathcal{S}}\mathcal{E}[\widehat{\boldsymbol{v}}_{ \mathcal{S}}\mathcal{E}]\,\leq\,1\,,\]

as desired. 

#### Step 3: Deriving Dual Bounds

We finally show that successful primal-dual constructions can lead to guarantees in \(\overline{\mathcal{R}}\)-loss, which is induced by the dual of the prior function.

**Dual Function Bounds** Assume that the primal-dual witness construction (see Step 1) was successful (see Step 2). Assume also that \(\mathcal{R}_{\mathcal{S}}\) and \(\mathcal{R}_{\mathcal{S}}\mathcal{E}\) are norms and that \(X_{\mathcal{S}}^{\top}X_{\mathcal{S}}/n\) is invertible. Assume finally that the model is approximately sparse in the sense that there is a vector \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\) with

1. \(\text{supp}[\boldsymbol{\alpha}]\subset\mathcal{S}\) and \(\overline{\mathcal{R}}_{\mathcal{S}}[(X_{\mathcal{S}}^{\top}X_{\mathcal{S}}/n) ^{-1}\widehat{\boldsymbol{v}}_{\mathcal{S}}]r\geq 2v\overline{\mathcal{R}}_{ \mathcal{S}}\mathcal{I}(X_{\mathcal{S}}^{\top}\)\(X_{\mathcal{S}}/n)^{-1}X_{\mathcal{S}}^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha})\) for a \(v\in(0,\infty)\);
2. \(\overline{\mathcal{R}}[\boldsymbol{\beta}-\boldsymbol{\alpha}]\leq dr/n\) for a (hopefully small) \(d\in[0,\infty]\).

Then, (6.1) has a solution \(\widehat{\boldsymbol{\beta}}\) that satisfies \(\text{supp}[\widehat{\boldsymbol{\beta}}]\subset\mathcal{S}\) and

\[\overline{\mu}[\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}]\leq\Big{(}d+ \frac{\overline{\mu}_{\delta}\big{[}(X_{\mathcal{S}}^{\top}X_{\mathcal{S}}/n)^ {-1}\widehat{\boldsymbol{v}}_{\delta}\big{]}(v+1)}{2v}\Big{)}\frac{r}{n}\,.\]

These bounds resemble the ones of Theorem 7.2.1, especially since both \(m\) and \(\overline{\mathcal{A}}_{\delta}\big{[}(X_{\mathcal{S}}^{\top}X_{\mathcal{S}}/n )^{-1}\widehat{\boldsymbol{v}}_{\delta}\big{]}\) can be viewed as measures for the design correlations. Nevertheless, the bounds here differ from those earlier ones in three main aspects: they limit the correlations among the predictors through the irrepresentability condition (via Step 2) rather than through the restricted eigenvalue condition; they concern the dual of the prior function rather than the original prior function; they also guarantee that the estimated support is included in the true support. The latter two features make Theorem 7.2.1 particularly interesting for support recovery--see the following section.

[] Example 7.2.1

\(\ell_{\infty}\)**-Estimation Bound for the Lasso**: In this example, we show that Theorems 7.3.1 and 7.3.2 generalize the \(\ell_{\infty}\)-bound for the lasso estimator (2.2) that follows from the lasso's explicit expression for orthogonal design. For orthogonal design, that is, \(X^{\top}X=nI_{p\times p}\), the least-squares estimator (1.3) satisfies the explicit expression (1.4) (since \(X^{\top}X=nI_{n\times n}\) implies that \(X^{\top}X\) is invertible) and, therefore,

\[\|\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_ {\infty}\] \[= \|\boldsymbol{\beta}-(X^{\top}X)^{-1}X^{\top}\boldsymbol{y}\|_{ \infty}\] Eq. (1.4) \[= \|\boldsymbol{\beta}-(X^{\top}X)^{-1}X^{\top}(X\boldsymbol{\beta} +\boldsymbol{u})\|_{\infty}\] linear model (2.1) \[= \|-(X^{\top}X)^{-1}X^{\top}\boldsymbol{u}\|_{\infty}\] \[= \|(X^{\top}X/n)^{-1}X^{\top}\boldsymbol{u}\|_{\infty}/n\] absolute homogeneity of norms \[= \|X^{\top}\boldsymbol{u}\|_{\infty}/n\,,\] \[X^{\top}X/n=I_{n\times n}\text{ by assumption}\]

and more generally, the lasso satisfies the explicit expression (2.8) and, therefore, (we leave the details to the reader)

\[\|\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}_{\text{lasso}}\|_{\infty} \leq\|X^{\top}\boldsymbol{u}\|_{\infty}/n+r/(2n)\,.\]For our usual tuning parameter \(r=4\|X^{\top}\mathbf{u}\|_{\infty}\), the latter inequality becomes

\[\|\mathbf{\beta}-\widehat{\mathbf{\beta}}_{\rm lasso}\|_{\infty}\,\leq\,\frac{3}{n}\|X^{ \top}\mathbf{u}\|_{\infty}\,.\]

We now show that the same bound follows from Theorems 7.3.1 and 7.3.2. Recall that the lasso's prior function \(\mathscr{h}\) and its subfunctions \(\mathscr{h}_{\delta}\) and \(\mathscr{h}_{\delta}\) in Definition 6.4.1 are the \(\ell_{1}\)-norms on \(\mathbb{R}^{p}\), \(\mathbb{R}^{s}\), and \(\mathbb{R}^{p-s}\), respectively (cf. Example 6.4.2) and that the corresponding dual functions \(\overline{\mathscr{h}}\), \(\overline{\mathscr{h}}_{\delta}\), and \(\overline{\mathscr{h}}_{\delta}\) are the \(\ell_{\infty}\)-norms on \(\mathbb{R}^{p}\), \(\mathbb{R}^{s}\), and \(\mathbb{R}^{p-s}\), respectively (see 3. in Exercise 2.2). For orthogonal design, the irrepresentability constant in Assumption 7.3.1 can be set to \(c=1\) according to

\[\sup_{\mathbf{a}\in\mathfrak{A}_{\delta}\mid\widehat{\mathbf{\gamma}}_{ \delta}}\overline{\mathscr{h}}_{\delta}\mathbb{C}\big{[}X_{\delta}^{\top}X_{ \delta}(X_{\delta}^{\top}X_{\delta})^{-1}\mathbf{a}\big{]}\] \[= \sup_{\mathbf{a}\in\mathfrak{A}_{\delta}\mid\widehat{\mathbf{\gamma}}_{ \delta}}\overline{\mathscr{h}}_{\delta}\mathbb{C}\big{[}\mathbf{0}_{(p-s)\times s} (X_{\delta}^{\top}X_{\delta})^{-1}\mathbf{a}\big{]}\] \[X^{\top}X=n\mathrm{I}_{p\times p}\text{ entails }X_{\delta} \mathbb{C}^{\top}X_{\delta}=\mathbf{0}_{(p-s)\times s}\] \[= \sup_{\mathbf{a}\in\mathfrak{A}_{\delta}\mid\widehat{\mathbf{\gamma}}_{ \delta}\mid}\overline{\mathscr{h}}_{\delta}\mathbb{C}\big{[}\mathbf{0}_{p-s}\big{]}\] linearity of matrix-vector multiplications \[= \sup_{\mathbf{a}\in\mathfrak{A}\mid\widehat{\mathbf{\gamma}}_{\delta} \mid 1}\|\mathbf{0}_{p-s}\|_{\infty}\] \[= \sup_{\mathbf{a}\in\mathfrak{A}\mid\widehat{\mathbf{\gamma}}_{\delta} \mid 1}0\] norms are positive definite \[= 0\,=\,1-1\,,\] subdifferentials of convex functions are not empty (cf. Page 57)

and the correlation measure \(\overline{\mathscr{h}}_{\delta}\mathbb{C}(X_{\delta}^{\top}X_{\delta}/n)^{-1} \widehat{\mathbf{\gamma}}_{\delta}\mathbb{C}\) in Theorem 7.3.2 is equal to one according to

\[\overline{\mathscr{h}}_{\delta}\mathbb{C}\big{[}(X_{\delta}^{ \top}X_{\delta}/n)^{-1}\widehat{\mathbf{\gamma}}_{\delta}\big{]}\] \[= \|\widehat{\mathbf{\gamma}}_{\delta}\|_{\infty}\] \[X^{\top}X=n\mathrm{I}_{p\times p}\text{ entails }X_{\delta}^{ \top}X_{\delta}/n=\mathrm{I}_{s\times s};\,\mathscr{h}\text{ is the }\ell_{1}\text{-norm}\] \[= 1\,,\] \[\widehat{\mathbf{\gamma}}_{\delta}\in\mathfrak{d}|\widehat{\mathbf{ \gamma}}_{\delta}|_{1}\text{ (see Lemma \ref{lem:delta} and its proof)};\] \[|(\widehat{\upsilon}_{\delta})_{j}|\leq 1\text{ and }|(\widehat{ \upsilon}_{\delta})_{j}|=|\text{sign}|(\widehat{\upsilon}_{\delta})_{j}|=1\] if \((\widehat{\gamma}_{\delta})_{j}\neq 0\) (see Example 2.5.3)

[MISSING_PAGE_EMPTY:6456]

which equals the bound derived above. However, while the derivation based on the explicit expression of the lasso requires exact orthogonality, the derivation based on Theorems 7.3.1 and 7.3.2 can be relaxed readily to approximate orthogonality.

For Gauss-distributed noise \(\mathbf{u}\sim\mathcal{N}_{n}[\mathbf{0}_{n},\sigma^{2}\mathbf{I}_{n\times n}]\), the bound can be combined with Lemma 4.2.1 as usual to obtain

\[\mathbb{P}\left\{\,\mathbf{\mathsf{I}}\mathbf{\beta}-\widehat{\mathbf{\beta}}_{\text{lasso }}\,\mathsf{I}_{\infty}\leq\sigma\sqrt{\frac{18\log[p/t]}{n}}\,\right\}\ \geq\ 1-t\,.\]

We can thus expect that in a sparse,5 weakly correlated regression with Gauss-distributed noise, the sup-norm error of the lasso estimator is bounded by \(\sigma\sqrt{\log[p]/n}\) with high probability.

We conclude this section with the proof of Theorem 7.3.2.

#### Proof of Theorem 7.3.2

Since the primal-dual construction is assumed successful, the primal vector \(\widehat{\mathbf{\beta}}:=\widehat{\mathbf{\gamma}}\) constructed in Definition 6.4.1 is a solution of the full problem (6.1). In view of _(Primal 2)_ in Definition 6.4.1, it thus holds that \(\text{supp}[\widehat{\mathbf{\beta}}]\subset\mathcal{S}\). We establish the remaining bound in three parts: first, we show that dual function \(\overline{\mathbf{\kappa}}\) can be decomposed into a function on \(\mathcal{S}\) and a function on \(\mathcal{S}^{\complement}\); then, we derive a bound on the set \(\mathcal{S}\); finally, we combine Parts 1 and 2 and the assumptions of the theorem to deduce the bound in the theorem.

Part 1We first show that under the conditions stated in the theorem, the function \(\overline{\mathbf{\kappa}}\) is \(\mathcal{S}\)-"sub-"decomposable:

\[\overline{\mathbf{\kappa}}[\mathbf{a}]\ \leq\ \overline{\mathbf{\kappa}}_{\mathcal{S}}[\mathbf{a}_{ \mathcal{S}}]+\overline{\mathbf{\kappa}}_{\mathcal{S}}\mathsf{c}[\mathbf{a}_{ \mathcal{S}}\mathsf{c}]\qquad\qquad\qquad\qquad\text{for all }\mathbf{a}\in\mathbb{R}^{p}\,.\]

The decomposability of the prior function \(\mathbf{\kappa}\) is a main restriction in this section. The subdecomposability of the dual \(\overline{\mathbf{\kappa}}\), on the other hand, then follows "automatically" from the definition of dual functions and the Holder inequality: we find for every \(\mathbf{a}\in\mathbb{R}^{p}\)

\[\overline{\mathbf{\kappa}}[\mathbf{a}] =\sup\left\{\,\langle\mathbf{a},\,\mathbf{c}\rangle\ :\ \mathbf{c}\in \mathbb{R}^{p},\,\,\mathbf{\kappa}[\mathbf{c}]\leq 1\,\right\}\]

\[=\sup\left\{\,\langle\mathbf{a},\,\mathbf{c}\rangle\ :\ \mathbf{c}_{\mathcal{S}}\in \mathbb{R}^{s},\,\,\mathbf{c}_{\mathcal{S}^{\complement}}\in\mathbb{R}^{p-s},\right.\]

[MISSING_PAGE_EMPTY:6458]

Since the restricted Gram matrix \(X_{\delta}^{\top}X_{\delta}\) is invertible by assumption, we find

\[\boldsymbol{\alpha}_{\delta}-\widehat{\boldsymbol{\beta}}_{\delta}\ =\ r(X_{\delta}^{\top}X_{\delta})^{-1}\widehat{\boldsymbol{v}}_{\delta}/2+(X_{\delta}^{\top}X_{\delta})^{-1}X_{\delta}^{\top}(X_{\delta}\boldsymbol{\alpha}_{\delta}-\boldsymbol{y})\,.\]

We apply the function \(\overline{\boldsymbol{\kappa}}_{\delta}\) on both sides and invoke the triangle inequality (1. in Exercise 2.3) and the absolute homogeneity (3. or 6. in Exercise 2.3) on the right-hand side to derive

\[\overline{\boldsymbol{\kappa}}_{\delta}\big{[}\boldsymbol{\alpha}_{ \delta}-\widehat{\boldsymbol{\beta}}_{\delta}\big{]} \leq \overline{\boldsymbol{\kappa}}_{\delta}\big{[}(X_{\delta}^{\top}X_{\delta}/n)^{-1}\widehat{\boldsymbol{v}}_{\delta}\big{]}r/(2n)\] \[+\overline{\boldsymbol{\kappa}}_{\delta}\big{[}(X_{\delta}^{\top}X_{ \delta}/n)^{-1}X_{\delta}^{\top}(\boldsymbol{y}-X_{\delta}\boldsymbol{\alpha}_{\delta})\big{]}/n\,,\]

as desired.

Part 3We finally derive the desired bound for \(\widehat{\boldsymbol{\beta}}=\widehat{\boldsymbol{\gamma}}\) in \(\overline{\boldsymbol{\kappa}}\):

\[\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\beta}-\widehat{\boldsymbol{ \beta}}\big{]}\ \leq\ \Big{(}d+\frac{\overline{\boldsymbol{\kappa}}_{\delta}\big{[}(X_{\delta}^{\top}X_{\delta}/n)^{-1}\widehat{\boldsymbol{v}}_{\delta}\big{]}(v+1)}{2v}\Big{)}\frac{r}{n}\,.\]

We use the subdecomposability of \(\overline{\boldsymbol{\kappa}}\) established in Part 1 and the bound in \(\overline{\boldsymbol{\kappa}}_{\delta}\) established in Part 2 to find

\[\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\beta}-\widehat{\boldsymbol{ \beta}}\big{]}\] \[=\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\beta}- \boldsymbol{\alpha}+\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}\big{]} \qquad\qquad\qquad\qquad\qquad\text{adding a zero-valued term}\] \[\leq\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\beta}- \boldsymbol{\alpha}\big{]}+\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}\big{]}\] triangle inequality (1. in Exercise 2.3) \[\leq\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\beta}- \boldsymbol{\alpha}\big{]}+\overline{\boldsymbol{\kappa}}_{\delta}\big{[} \boldsymbol{\alpha}_{\delta}-\widehat{\boldsymbol{\beta}}_{\delta}\big{]}+ \overline{\boldsymbol{\kappa}}_{\delta}\big{[}\boldsymbol{\alpha}_{\delta} \boldsymbol{\mathrm{e}}-\widehat{\boldsymbol{\beta}}_{\delta}\big{]}\] Part 1 applied to the second term \[\leq\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\beta}- \boldsymbol{\alpha}\big{]}+\overline{\boldsymbol{\kappa}}_{\delta}\big{[}(X_{ \delta}^{\top}X_{\delta}/n)^{-1}\widehat{\boldsymbol{v}}_{\delta}\big{]}r/(2n)\] \[\quad+\overline{\boldsymbol{\kappa}}_{\delta}\big{[}(X_{\delta}^{ \top}X_{\delta}/n)^{-1}X_{\delta}^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha}_{ \delta})\big{]}/n\] \[\quad+\overline{\boldsymbol{\kappa}}_{\delta}\big{[}\boldsymbol{ \alpha}_{\delta}\boldsymbol{\mathrm{e}}-\widehat{\boldsymbol{\beta}}_{\delta} \boldsymbol{\mathrm{e}}\big{]}\qquad\qquad\text{Part 2 applied to the second term}\] \[=\overline{\boldsymbol{\kappa}}\big{[}\boldsymbol{\beta}- \boldsymbol{\alpha}\big{]}+\overline{\boldsymbol{\kappa}}_{\delta}\big{[}(X_{ \delta}^{\top}X_{\delta}/n)^{-1}\widehat{\boldsymbol{v}}_{\delta}\big{]}r/(2n)\] \[\quad+\overline{\boldsymbol{\kappa}}_{\delta}\big{[}(X_{\delta}^ {\top}X_{\delta}/n)^{-1}X_{\delta}^{\top}(\boldsymbol{y}-X\boldsymbol{\alpha}) \big{]}/n\] \[\quad+\overline{\boldsymbol{\kappa}}_{\delta}\big{[}\boldsymbol{ \mathrm{0}}_{p-s}\big{]}\qquad\qquad\boldsymbol{\alpha}_{\delta}\boldsymbol{ \mathrm{e}}=\widehat{\boldsymbol{\beta}}_{\delta}\boldsymbol{\mathrm{e}}= \boldsymbol{\mathrm{0}}_{p-s}\text{ by the 2. assumption in the theorem and (Primal 2), respectively \[=\overline{\#}[\beta-\alpha]+\overline{\#}_{\delta}[(X_{\delta}^{ \top}X_{\delta}/n)^{-1}\overline{\nu}_{\delta}]r/(2n)\] \[\quad+\overline{\#}_{\delta}[(X_{\delta}^{\top}X_{\delta}/n)^{-1}X _{\delta}^{\top}(\mathbf{y}-X\alpha)]/n\] \[=d/n+\overline{\#}_{\delta}[(X_{\delta}^{\top}X_{\delta}/n)^{-1} \overline{\nu}_{\delta}]r/(2n)\] \[\quad+\overline{\#}_{\delta}[(X_{\delta}^{\top}X_{\delta}/n)^{-1} \overline{\nu}_{\delta}]r/(2n)\] \[\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad 1.\text{ and }2.\text{ assumptions of the theorem}\] \[=\Big{(}d+\frac{\overline{\#}_{\delta}[(X_{\delta}^{\top}X_{ \delta}/n)^{-1}\overline{\nu}_{\delta}](v+1)}{2v}\Big{)}\frac{r}{n}\,,\quad \text{consolidating}\]

as desired. 

### Support Recovery Guarantees

We have discussed in Sect. 6.4 the fact that for approximately sparse models, it makes sense to speak of relevant predictors (predictors that correspond to non-zero-valued variables in the sparse vector) and irrelevant predictors (predictors that correspond to zero-valued variables in the sparse vector). In this section, we aim to differentiate these two groups of predictors based on data. This task is called _support recovery_, _variable selection_, or _feature selection_.

How well an estimator \(\widehat{\boldsymbol{\beta}}\in\mathbb{R}^{p}\) performs this task is typically measured in terms of _false negatives_ and _false positives_. False negatives are falsely omitted predictors, and false positives are falsely selected predictors; accordingly, given a sparse surrogate vector \(\boldsymbol{\alpha}\in\mathbb{R}^{p}\), the number of false negatives is \(\operatorname{fn}:=|\{j\in\{1,\ldots,p\}:\widehat{\beta}_{j}=0\text{ and }\alpha_{j}\neq 0\}|\), and the number of false positives is \(\operatorname{fp}:=|\{j\in\{1,\ldots,p\}:\widehat{\beta}_{j}\neq 0\text{ and }\alpha_{j}=0\}|\). The smaller these two numbers, the better the estimator's performance at support recovery.

The following approach to support recovery is based on an accurate estimation of each of the surrogate vector's coordinate values. We thus assume the following:

\(\bullet\bullet\)Assumption 7.4.1

**Generic Sup-Norm Bound** We assume that for a sparse surrogate vector \(\boldsymbol{\alpha}\) of the regression vector \(\boldsymbol{\beta}\) and for a (potentially random) value \(b\in[0,\infty)\), the estimator \(\widehat{\boldsymbol{\beta}}\) satisfies the bound

\[\|\boldsymbol{\alpha}-\widehat{\boldsymbol{\beta}}\|_{\infty}\leq b\,.\]

[MISSING_PAGE_FAIL:242]

estimator \(\widehat{\boldsymbol{\beta}}^{c}\) performs well at support recovery in that it has a small number of false positives.

The following theorem refines these insights:

**Sign Consistent Support Recovery** Consider a thresholded estimator \(\widehat{\boldsymbol{\beta}}^{c}\) as defined in (2.4) based on an initial estimator \(\widehat{\boldsymbol{\beta}}\) that satisfies Assumption 7.4.1 (generic supnorm bound). It then holds:

1. For every cutoff \(c\in[0,\infty]\), the thresholded estimator correctly identifies the signs of all sufficiently large variables of the sparse surrogate vector:

\[\text{sign}\left[\langle\widehat{\boldsymbol{\beta}}^{c}\rangle_{j}\right]\ =\ \text{sign}[\alpha_{j}]\]

for all \(j\in\{1,\ldots,p\}\) with \(|\alpha_{j}|>b+c\).

2. For every cutoff \(c\in[b,\infty]\), the thresholded estimator also finds all zero-valued variables of the sparse surrogate vector:

\[\text{sign}\left[\langle\widehat{\boldsymbol{\beta}}^{c}\rangle_{j}\right]\ =\ \text{sign}[\alpha_{j}]\ =\ 0\]

for all \(j\in\{1,\ldots,p\}\) with \(\alpha_{j}=0\).

The first part of the theorem limits false negatives; the second part limits false positives. If the cutoff is selected appropriately and there is no "ambiguous signal," that is, if \(c\geq b\) and \(\alpha_{1},\ldots,\alpha_{p}\notin[-b-c,b+c]\), the two parts together guarantee perfect support recovery: \(\text{fn}=\text{fp}=0\). The thresholded estimator is then even sign consistent, which means that it identifies also the correct signs of all of the surrogate vector's coordinates values. But again, such perfect support recovery requires 1. the non-zero parameters to be sufficiently large, 2. a well-chosen cutoff, and 3. a suitable bound to start with; therefore, it is often unrealistic in practice.

The condition \(|\alpha_{j}|>b+c\) for ruling out false negatives suggests small cutoffs or no thresholding altogether, while the condition \(c\geq b\) for ruling out false positives suggests large cutoffs. Hence in practice, the size of the cutoff \(c\) needs to reflect the analyst's objective: A small cutoff is in order for _variable screening_, that is, the detection of all signals (for example, if one just wants to decrease the dimensionality of the problem for a subsequent treat ment). For sparse estimators such as the lasso, \(c=0\) may work. A large cutoff is in order when predictors should be selected conservatively (for example, if it is expensive to study predictors further). A possible compromise between these two aspects is provided by adaptive validation as discussed in Sect. 4.4:

\[\square\]

**Thresholding the Lasso via Adaptive Validation** In this example, we apply thresholding for support recovery with the lasso estimator (2.2). This requires the calibration of two tuning parameters: the lasso's original tuning parameter \(r\) and the cutoff \(c\). Because both parameters affect support recovery, they need to be in sync.

Adaptive validation provides such a synchronized calibration. First, adaptive validation suggests \(\widehat{\tau}_{\text{av}}\) in (4.4) for the lasso's original tuning parameter. This choice ensures

\[|\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}}_{\text{lasso}}[\widehat{\tau}_ {\text{av}}]|_{\infty}\,\leq\,\frac{9}{n}|\!|X^{\top}\boldsymbol{u}|_{\infty}\]

for orthogonal designs (and similar guarantees for weakly correlated designs)--see Example 4.4.1. Hence, Assumption 7.4.1 is satisfied with \(\boldsymbol{\alpha}=\boldsymbol{\beta}\) and \(b=9|\!|X^{\top}\boldsymbol{u}|_{\infty}/n\).

Then, adaptive validation prompts the cutoff \(c:=9\widehat{\tau}_{\text{av}}/(4n)\). This choice ensures that \((\widehat{\beta}^{c})_{j}\neq 0\) for all \(\beta_{j}>18|\!|X^{\top}\boldsymbol{u}|_{\infty}/n\)--see 1.1 in Theorem 7.4.1 and note that \(b+c=9|\!|X^{\top}\boldsymbol{u}|_{\infty}/n+9\widehat{\tau}_{\text{av}}/(4n) \geq 18|\!|X^{\top}\boldsymbol{u}|_{\infty}/n\) in view of the bound \(\widehat{\tau}_{\text{av}}\leq 4|\!|X^{\top}\boldsymbol{u}|_{\infty}\) in Example 4.4.1--which means that all sufficiently large predictors are selected. For Gauss-distributed noise \(\boldsymbol{u}\,\sim\,\mathcal{N}_{n}[\boldsymbol{0}_{n},\sigma^{2}\mathsf{I}_{ n\times n}]\), for example, this essentially includes all predictors that are larger than \(\sigma\sqrt{\log[p]/n}\)--see again Example 4.4.1. In this sense, adaptive validation allows us to threshold "safely."\(\blacktriangle\)

Finally, bear in mind that thresholding typically benefits only support recovery (and not prediction and estimation).

**Proof of Theorem 7.4.1** The proof of 1. is a refinement of the above derivations: If \(\alpha_{j}\geq b+c\),

\[\widehat{\beta}_{j} = \alpha_{j}-\alpha_{j}+\widehat{\beta}_{j}\] adding a zero-valued term \[\geq \alpha_{j}-|\alpha_{j}-\widehat{\beta}_{j}|\] \[-|a|\leq a\]\[\geq \alpha_{j}-\|\alpha-\widehat{\beta}\|_{\infty}\] \[> b+c-b\ \ =\ c\,,\qquad\alpha_{j}>b+c\text{ by assumption;}\] \[\quad|\alpha-\widehat{\beta}\|_{\infty}\leq b\text{ by Assumption \ref{prop:1}}\]

which means in view of the definition of the thresholded estimator in (2.4) that \((\widehat{\beta}^{c})_{j}=\widehat{\beta}_{j}>0\), as desired. The case \(\alpha_{j}\leq-b-c\) follows along the same lines.

The proof of 2. has already been stated above the theorem in its entirety. 

### Exercises

#### Exercises for \(\blacktriangleright\) Sect. 7.2

**Exercise 7.1**\(\blacklozenge\blacktriangleright\) In this exercise, we show that the lasso's \(\ell_{1}\)-estimation bounds in Example 7.2.1 also entail estimation bounds with respect to other norms. We proceed in two steps.

1. Use the classical Holder inequality (2.6) to show that for all \(\boldsymbol{c}\in\mathbb{R}^{p}\) and \(k\), \(l\in[1,\infty]\) with \(k<l\), it holds that \[\|\boldsymbol{c}\|_{k}\,\leq\,p^{1/k-1/l}\|\boldsymbol{c}\|_{l}\,,\] and to show that for all \(\boldsymbol{c}\in\mathbb{R}^{p}\) and \(k\), \(l\in[1,\infty]\) with \(k\geq l\), it holds that \[\|\boldsymbol{c}\|_{k}\,\leq\,\|\boldsymbol{c}\|_{l}\,.\]
2. Conclude from Claim 1 and Example 7.2.1 that for every \(q\in[1,\infty]\), it holds that \[\mathbb{P}\bigg{\{}\,\|\boldsymbol{\beta}-\widehat{\beta}_{\text{lasso}}\,\|_{q }\geq 192\sigma s\sqrt{\frac{\log[p/l]}{n}}\,\bigg{\}}\ \leq\ t\] under the conditions of the example.

The compatibility condition in Assumption 6.4.1 is tailored, in a sense, to estimation in \(\lambda\)-loss function, that is, \(\ell_{1}\)-loss in the lasso case. Replacing that condition with (slightly more restrictive) versions that better capture the specifics of other \(\ell_{q}\)-loss functions can reduce the dependence on the sparsity level from \(s\) to \(s^{1/q}\). We omit the details.

#### Exercises for Sect. 7.3

**Exercise 7.2**: \(\diamondsuit\diamond\) In this exercise, we study the convexity of subfunctions of convex prior functions. Consider a finite, \(\delta\)-decomposable prior function \(\mathcal{N}\) with subfunctions \(\mathcal{N}_{\delta}\) and \(\mathcal{N}_{\delta\mathcal{S}}\) (cf. Definition 6.4.1). Show that \(\mathcal{N}\) is convex if and only if both \(\mathcal{N}_{\delta}\) and \(\mathcal{N}_{\delta\mathcal{C}}\) are convex (cf. Definition 2.5.1).

**Exercise 7.3**: \(\blacklozenge\blacklozenge\blacklozenge\) In this exercise, we study the irrepresentability condition of Assumption 7.3.1 for the lasso's \(\ell_{1}\)-prior function. Show that

\[\sup_{\boldsymbol{a}\in\partial\mathcal{N}_{\delta}[\widehat{ \boldsymbol{\gamma}}_{\delta}]}\overline{\mathcal{N}}_{\delta}\mathsf{c} \big{[}X_{\delta}\mathsf{c}^{\top}X_{\delta}(X_{\delta}^{\top}X_{\delta})^{-1 }\boldsymbol{a}\big{]}\\ \leq\ \max_{j\in\delta\mathcal{C}}\|(X_{\delta}^{\top}X_{\delta}) ^{-1}X_{\delta}^{\top}\boldsymbol{x}_{j}\|_{1}\;,\]

where \(\mathcal{N}\) is the \(\ell_{1}\)-norm and \(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{p}\in\mathbb{R}^{n}\) the columns of the design matrix: \(X=(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{p})\).

The vector \((X_{\delta}^{\top}X_{\delta})^{-1}X_{\delta}^{\top}\boldsymbol{x}_{j}\) is the least-squares estimator for regressing the irrelevant predictor \(\boldsymbol{x}_{j}\) on the matrix of relevant predictors \(X_{\delta}\) (cf. Eq. (1.4)); hence, the right-hand side of the above inequality is yet another measure for how much the relevant predictors are correlated with the irrelevant ones.

**Exercise 7.4**: \(\blacklozenge\blacklozenge\blacklozenge\blacklozenge\) In this exercise, we study the uniqueness of the lasso estimator (2.2). Given a corresponding primal-dual pair \((\widehat{\boldsymbol{\gamma}},\widehat{\boldsymbol{v}})\) constructed according to Definition 7.3.1, show the following three claims:

1. The restricted primal vector \(\widehat{\boldsymbol{\gamma}}_{\delta}\) is the unique solution of the restricted lasso problem in _(Primal 1)_ if the restricted Gram matrix \(X_{\delta}^{\top}X_{\delta}\) is invertible.
2. The primal vector \(\widehat{\boldsymbol{\gamma}}\) is not necessarily the unique solution of the full lasso problem (2.2) even if \(X_{\delta}^{\top}X_{\delta}\) is invertible and the primal-dual witness construction was successful. (In contrast, the primal-dual pairs in Definition 7.3.1 are then uniquely defined.)
3. The primal vector \(\widehat{\boldsymbol{\gamma}}\) is the unique solution of the full lasso problem if \(X_{\delta}^{\top}X_{\delta}\) is invertible, \(\|\widehat{\boldsymbol{v}}_{\delta}\mathsf{c}\|_{\infty}<1\), and \(r\in(0,\infty)\). Conclude that this implies in particular that the lasso solution is supported on \(\mathcal{S}\), which relates to the discussion of false positives in \(\blacktriangleright\) Sect. 7.4.

This exercise shows that uniqueness of the lasso estimator requires the _strict_ dual feasibility condition \(|\widehat{\boldsymbol{v}}_{\mathcal{S}}\|_{\infty}<1\) rather than the dual feasibility condition \(|\widehat{\boldsymbol{v}}_{\mathcal{S}}\|_{\infty}\leq 1\) that is used in the main text (cf. Lemma 7.3.1, for example).7

### Notes and References

* [1] The primal-dual witness technique was introduced in Wainwright (2009). Our treatment generalizes the discussion in Hastie et al. (2015, Chapter 11.4 on pp. 301ff) beyond the lasso.
* [2] We could, therefore, call the approach _primal-subgradient witness technique_ instead. In any case, we use the terms _subgradient_ and _dual_ interchangeably, because subgradients and solutions of the "dual problem" are intimately related--see books on optimization, such as Boyd and Vandenberghe (2004), for details
* [3] A definition of Lagrange dual functions can be found in Hiriart-Urruty and Lemarechal (2004, Display (5.2.5) on p. 197). Our definition of Holder dual functions generalizes the concept of _dual norms_; a definition of those can be found in Hiriart-Urruty and Lemarechal (2004, Section C.3.2 on p. 146).
* [4] See van de Geer and Buhlmann (2009) for a comparison among such conditions.
* [5] When the orthogonality is exact, sparsity is not needed: The derivation via the explicit expression of the lasso does not involve \(\mathcal{S}\), and the derivation via the theorems also works with \(\mathcal{S}:=\{1,\ldots,p\}\). In general, however, it is important to keep track of \(\mathcal{S}\): For designs that are only approximately orthogonal (for example, if \(p>n\)), the set \(\mathcal{S}\) must be small to meet the irrespresentability condition and, more generally, to avoid that the quantities that involve correlations, such as \((X_{\mathcal{S}}^{\top}X_{\mathcal{S}}/n)^{-1}\), render the bounds ineffective. The rate \(\sigma\sqrt{\log[p]/n}\), on the other hand, remains the same across different sparsity levels \(s=|\mathcal{S}|\) (in contrast to the rate \(\sigma s\sqrt{\log[p]/n}\) in Example 7.2.1).
* [6] Assumption 7.4.1 can be complemented with further assumptions to guarantee small fp or even \(\mathrm{fp}=0\) without any thresholding--see, for example, 3. in Exercise 7.4 or Zhao and Yu (2006). However, as always, the more assumptions, the less plausible the theoretical framework.
* [7] The uniqueness of the lasso estimator is discussed from a different standpoint in Tibshirani (2013); Schneider and Ewald (2017).

* [19] A. J. L. K. Wilson, "The \(\pi\)-\(\pi\) transition in the \(\pi\)-\(\pi\) transition", _Phys. Rev. Lett._**78**, 1998 (1997).

[MISSING_PAGE_POST]

**7

[MISSING_PAGE_EMPTY:6468]

Hence, \[\boldsymbol{w}^{\top}X^{\top}X\boldsymbol{w} =\,(X\boldsymbol{w})^{\top}X\boldsymbol{w} (AB)^{\top}=B^{\top}A^{\top}\] \[=\,\|X\boldsymbol{w}\|_{2}^{2} \text{definition of the $\ell_{2}$-norm}\] \[=\,\|\boldsymbol{0}_{n}\|_{2}^{2} \text{above display}\] \[=\,0 \text{positive definiteness of norms}\] for \(\boldsymbol{w}\neq\boldsymbol{0}_{n}\), which means that \(X^{\top}X\) is not invertible.
3. We again resort to linear algebra. \(\Rightarrow\): Assume that \(X\boldsymbol{\gamma}=\boldsymbol{0}_{n}\) for a \(\boldsymbol{\gamma}\neq\boldsymbol{0}_{p}\). Then, for every \(\widehat{\boldsymbol{\beta}}_{\text{ls}}\in\arg\min_{\boldsymbol{a}\in\mathbb{ R}^{p}}\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}\), also \(\widehat{\boldsymbol{\beta}}_{\text{ls}}+\boldsymbol{\gamma}\in\arg\min_{ \boldsymbol{a}\in\mathbb{R}^{p}}\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}\). Hence, \(\widehat{\boldsymbol{\beta}}_{\text{ls}}\) is not unique. \(\Leftarrow\): Assume that \(\arg\min_{\boldsymbol{a}\in\mathbb{R}^{p}}\|\boldsymbol{y}-X\boldsymbol{a}\|_ {2}^{2}\) is not unique. Then, there are least-squares estimators \(\widehat{\boldsymbol{\gamma}}_{\text{ls}}\), \(\widehat{\boldsymbol{\beta}}_{\text{ls}}\) such that \(\widehat{\boldsymbol{\gamma}}_{\text{ls}}\neq\widehat{\boldsymbol{\beta}}_{ \text{ls}}\) but \[\|\boldsymbol{y}-X\widehat{\boldsymbol{\gamma}}_{\text{ls}}\|_{2}^{2}\,=\,\| \boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_{2}^{2}\,.\] However, since \(\boldsymbol{a}\mapsto\,\|\boldsymbol{y}-\boldsymbol{a}\|_{2}^{2}\) is _strictly_ convex, this means that \(X\widehat{\boldsymbol{\gamma}}_{\text{ls}}=X\widehat{\boldsymbol{\beta}}_{ \text{ls}}\), which in turn means that \(X\boldsymbol{\gamma}=\boldsymbol{0}_{n}\) for \(\boldsymbol{\gamma}:=\widehat{\boldsymbol{\gamma}}_{\text{ls}}-\widehat{ \boldsymbol{\beta}}_{\text{ls}}\neq\boldsymbol{0}_{p}\). The stated conclusion now follows from the fact that \(X\boldsymbol{\gamma}\neq\boldsymbol{0}_{n}\) for all \(\boldsymbol{\gamma}\in\mathbb{R}^{p}\setminus\{\boldsymbol{0}_{p}\}\) if and only if \(X^{\top}X\) is invertible.
4. We use again that the function \(\boldsymbol{a}\mapsto\,\|\boldsymbol{y}-\boldsymbol{a}\|_{2}^{2}\) is _strictly_ convex. Assume that \(X\widehat{\boldsymbol{\gamma}}_{\text{ls}}\neq X\widehat{\boldsymbol{\beta}}_ {\text{ls}}\). Then, for every \(a\in(0,1)\), \[\|\boldsymbol{y}-X\big{(}a\widehat{\boldsymbol{\gamma}}_{\text{ls}}+(1-a) \widehat{\boldsymbol{\beta}}_{\text{ls}}\big{)}\|_{2}^{2}\] \[=\,\|\boldsymbol{y}-aX\widehat{\boldsymbol{\gamma}}_{\text{ls}}+( 1-a)X\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_{2}^{2}\] multiplying out the design-related part \[<\,a\|\boldsymbol{y}-X\widehat{\boldsymbol{\gamma}}_{\text{ls}}\|_{2}^{2}+( 1-a)\|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_{2}^{2}\] strict convexity of the mentioned function; \[\boldsymbol{a}=aX\widehat{\boldsymbol{\gamma}}_{\text{ls}}\text{ and }\boldsymbol{a}=(1-a)X\widehat{\boldsymbol{\beta}}_{\text{ls}}\text{, respectively}\] \[=\,a\|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_{2}^{2}+( 1-a)\|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_{2}^{2}\] both \(\widehat{\boldsymbol{\gamma}}_{\text{ls}}\) and \(\widehat{\boldsymbol{\beta}}_{\text{ls}}\) are least-squares solutions \[=\,\|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\text{ls}}\|_{2}^{2}\,,\] consolidating termswhich means that \(a\widehat{\mathbf{\gamma}}_{\rm 1s}+(1-a)\widehat{\mathbf{\beta}}_{\rm 1s}\) yields a strictly smaller value in the objective function than \(\widehat{\mathbf{\beta}}_{\rm 1s}\). This contradicts that \(\widehat{\mathbf{\beta}}_{\rm 1s}\) is a least-squares solution.
5. The proof is short: we just need to use the properties of the Moore-Penrose inverse. According to Sect. 1.2, we have to show that \(X^{\top}X(X^{+}\mathbf{y})=X^{\top}\mathbf{y}\). For this, we compute \[X^{\top}X(X^{+}\mathbf{y}) = X^{\top}(XX^{+})^{\top}\mathbf{y}\] Part 3 in the definition of Moore-Penrose inverses \[= (XX^{+}X)^{\top}\mathbf{y}\] \[3.\] in Lemma B.2.1 (reversion of transposition) \[= X^{\top}\mathbf{y}\,,\] Part 1 in the definition of Moore-Penrose inverses as desired. Alternatively, we could show that for all \(\mathbf{\alpha}\in\mathbb{R}^{p}\), \[\mathbf{y}-X\mathbf{\alpha}]_{2}^{2}\ \geq\ |\mathbf{y}-X\widehat{\mathbf{\beta}}_{\rm 1s}|_{2}^{2}\,.\] For this, we first observe that \[2((\mathsf{I}_{n\times n}-XX^{+})\mathbf{y},\ X(X^{+}\mathbf{y}-\mathbf{ \alpha}))\] \[= 2\langle\mathbf{y},\ (\mathsf{I}_{n\times n}-XX^{+})^{\top}X(X^{+} \mathbf{y}-\mathbf{\alpha})\rangle\] property (B.1 of transposition) \[= 2\langle\mathbf{y},\ (\mathsf{I}_{n\times n}-XX^{+})X(X^{+}\mathbf{y}- \mathbf{\alpha})\rangle\] \[3.\] in the definition of Moore-Penrose inverses \[= 2\langle\mathbf{y},\ (X-XX^{+}X)(X^{+}\mathbf{y}-\mathbf{\alpha})\rangle\] reorganizing \[= 0\,\qquad\qquad\qquad 1.\] in the definition of Moore-Penrose inverses Using this, we find for every \[\mathbf{\alpha}\in\mathbb{R}^{p}\] \[\mathbf{y}-X\mathbf{\alpha}]_{2}^{2}\] \[= 1[\mathsf{I}_{n\times n}-XX^{+}\mathbf{y}+XX^{+}\mathbf{y}-X\mathbf{\alpha}]_ {2}^{2}\quad\text{adding a zero-valued term}\] \[= 1[\mathsf{I}_{n\times n}-XX^{+}\mathbf{y}+X(X^{+}\mathbf{y}-\mathbf{\alpha})] _{2}^{2}\qquad\qquad\qquad\qquad\text{factorizing X}\] \[= 1[\mathsf{I}_{n\times n}-XX^{+}\mathbf{y}]_{2}^{2}+1\langle X(X^{+} \mathbf{y}-\mathbf{\alpha})\rangle_{2}^{2}\] \[+2\langle(\mathsf{I}_{n\times n}-XX^{+})\mathbf{y},\ X(X^{+}\mathbf{y}- \mathbf{\alpha})\rangle\qquad\qquad\text{expanding the square}\] \[= 1[\mathsf{I}_{n\times n}-XX^{+}\mathbf{y}]_{2}^{2}+1\langle X(X^{+} \mathbf{y}-\mathbf{\alpha})\rangle_{2}^{2}\qquad\qquad\text{previous display}\] \[= |\mathbf{y}-XX^{+}\mathbf{y}]_{2}^{2}+|XX^{+}\mathbf{y}-X\mathbf{\alpha}]_{2}^{2}\] expanding the bracket in the second term \[= |\mathbf{y}-X\widehat{\mathbf{\beta}}_{\rm 1s}|_{2}^{2}+|X\widehat{\mathbf{\beta}}_{ \rm 1s}-X\mathbf{\alpha}|_{2}^{2}\qquad\qquad\qquad\qquad\text{definition of }\widehat{\mathbf{\beta}}_{\rm 1s}\]

[MISSING_PAGE_FAIL:252]

3. We need to verify that \(X^{+}\) satisfies the four properties of the definition of Moore-Penrose inverses on Page 19. The first property can be derived as follows: \[XX^{+}X\] \[=\mathit{UDV}^{\top}(\mathit{VD}^{+}\mathit{U}^{\top})\mathit{UDV}^{\top}\] SVD and definition of \(X^{+}\) \[=\mathit{UDD}^{+}\mathit{DV}^{\top}\qquad\qquad\mathit{U},\,\mathit{V}\,\,\text{ orthogonal by assumption}\] \[=\mathit{UDV}^{\top}\] \[\qquad\qquad\mathit{D}^{+}\text{ is a Moore-Penrose inverse of }\mathit{D}\text{ by assumption}\] \[=\mathit{X}\,.\] SVD The other three properties can be verified similarly.
7. The short proof is based on the previous findings. We first note that, in view of Claim 3, it is sufficient to show the equality for _one_ least-squares solution. We opt for \(\widehat{\mathbf{\beta}}_{\text{ls}}=X^{+}\mathbf{y}\) motivated by Claim 4. We then find \[\|X\mathbf{\beta}-X\widehat{\mathbf{\beta}}_{\text{ls}}\|_{2}^{2}\] \[=\ \|X\mathbf{\beta}-XX^{+}\mathbf{y}\|_{2}^{2}\] our choice of \[\widehat{\mathbf{\beta}}_{\text{ls}}\] \[=\ \|X\mathbf{\beta}-XX^{+}(X\mathbf{\beta}+\mathbf{u})\|_{2}^{2}\] model assumptions: \[\mathbf{y}=X\mathbf{\alpha}+\mathbf{u}\] \[=\ \|XX^{+}\mathbf{u}\|_{2}^{2}\] 1. in the definition of Moore-Penrose inverses and consolidating \[=\ \|\mathit{UDV}^{\top}\mathit{VD}^{+}\mathit{U}^{\top}\mathbf{u}\|_{2}^{2}\] SVD and 5.(iii) above \[=\ \|\mathit{UDD}^{+}\mathit{U}^{\top}\mathbf{u}\|_{2}^{2}\,,\qquad \qquad\mathit{V}\,\,\text{orthogonal}\,(\text{Definition \ref{def:2000}})\] as desired.
8. After some simple calculations, the claim follows from the above results and the properties of Chi-squared distributions. We first rewrite the right-hand side of the display in 6. as \[\|\mathit{UDD}^{+}\mathit{U}^{\top}\mathbf{u}\|_{2}^{2}\] \[=\ (\mathit{UDD}^{+}\mathit{U}^{\top}\mathbf{u})^{\top}\mathit{UDD}^{+} \mathit{U}^{\top}\mathbf{u}\] definition of the \[\ell_{2}\] -norm

[MISSING_PAGE_EMPTY:6473]

[MISSING_PAGE_EMPTY:6474]

**Solutions for Sect. 1.3**

**Solution 1.3** The proofs are straightforward calculations.

1. By assumption, \(\ell_{\mathbf{\alpha}}\) is the density of \(\mathcal{N}_{n}[X\mathbf{\alpha},\sigma^{2}\mathrm{I}_{n\times n}]\), and \(\mathpzc{g}\) the density of \(\mathcal{N}_{p}[\mathbf{0}_{p},\,\tau^{2}\mathrm{I}_{p\times p}]\), that is, \[\ell_{\mathbf{\alpha}}[\mathbf{y}]\,=\,\frac{1}{(2\pi\sigma^{2})^{n/2}}e^{-|\mathbf{y}-X \mathbf{\alpha}|_{2}^{2}/(2\sigma^{2})}\] and \[\mathpzc{g}[\mathbf{\alpha}]\,=\,\frac{1}{(2\pi\,\tau^{2})^{p/2}}e^{-|\mathbf{\alpha} |_{2}^{2}/(2\tau^{2})}\,.\] Using these formulae and the properties of the logarithm yields \[\log\big{[}\ell_{\mathbf{\alpha}}[\mathbf{y}]\mathpzc{g}[\mathbf{\alpha}]\big{]}\] \[=\,\log\bigg{[}\frac{1}{(2\pi\sigma^{2})^{n/2}}e^{-|\mathbf{y}-X\mathbf{ \alpha}|_{2}^{2}/(2\sigma^{2})}\frac{1}{(2\pi\,\tau^{2})^{p/2}}e^{-|\mathbf{\alpha }|_{2}^{2}/(2\tau^{2})}\bigg{]}\] \[=\,-\,\frac{n}{2}\log[2\pi\,\sigma^{2}]-\frac{1}{2\sigma^{2}}| \mathbf{y}-X\mathbf{\alpha}|_{2}^{2}-\frac{p}{2}\log[2\pi\,\tau^{2}]-\frac{1}{2\tau^{ 2}}|\mathbf{\alpha}|_{2}^{2}\,.\] Then, \[\operatorname*{arg\,max}_{\mathbf{\alpha}\in\mathbb{R}^{p}}\ell[\mathbf{ \alpha}\mid\mathbf{y}]\] \[=\,\operatorname*{arg\,min}_{\mathbf{\alpha}\in\mathbb{R}^{p}}\bigg{\{} \frac{n}{2}\log[2\pi\sigma^{2}]+\frac{1}{2\sigma^{2}}|\mathbf{y}-X\mathbf{\alpha}|_{2} ^{2}\] \[\qquad\qquad\qquad+\,\frac{p}{2}\log[2\pi\,\tau^{2}]+\frac{1}{2 \tau^{2}}|\mathbf{\alpha}|_{2}^{2}\bigg{\}}\] hint and above display \[=\,\operatorname*{arg\,min}_{\mathbf{\alpha}\in\mathbb{R}^{p}}\bigg{\{} \frac{1}{2\sigma^{2}}|\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}+\frac{1}{2\tau^{2}}|\mathbf{ \alpha}|_{2}^{2}\bigg{\}}\] \[=\,\operatorname*{arg\,min}_{\mathbf{\alpha}\in\mathbb{R}^{p}}\bigg{\{} |\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}+\frac{\sigma^{2}}{\tau^{2}}|\mathbf{\alpha}|_{2}^{2} \bigg{\}}\,.\] multiplying through by \(2\sigma^{2}\) and noting that this does not affect any minimizer Hence, \(\widehat{\mathpzc{g}}_{\mathrm{map}}=\widehat{\mathpzc{g}}_{\mathrm{ridge}}\) for \(r=\sigma^{2}/\tau^{2}\), as desired.

2. Now, \[g[\mathbf{\alpha}]\,=\,\frac{1}{(2\tau)^{p}}e^{-|\mathbf{\alpha}|_{1/\tau}}\,,\] but we can proceed as above otherwise. In particular, we find \[\log\big{[}\ell_{\mathbf{\alpha}}[\mathbf{y}]g[\mathbf{\alpha}]\big{]}\] \[\quad=\,\log\bigg{[}\frac{1}{(2\pi\sigma^{2})^{n/2}}e^{-|\mathbf{y}- X\mathbf{\alpha}|_{2}^{2}/(2\sigma^{2})}\frac{1}{(2\tau)^{p}}e^{-|\mathbf{\alpha}|_{1/ \tau}}\bigg{]}\] \[\quad=\,-\,\frac{n}{2}\log[2\pi\sigma^{2}]-\frac{1}{2\sigma^{2}} \|\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}-p\log[2\tau]-\frac{1}{\tau}|\mathbf{\alpha}|_{1}\] and \[\operatorname*{arg\,max}_{\mathbf{\alpha}\in\mathbb{R}^{p}}\alpha[ \mathbf{\alpha}\mid\mathbf{y}]\] \[\quad=\,\operatorname*{arg\,min}_{\mathbf{\alpha}\in\mathbb{R}^{p}} \big{[}\,\big{[}\frac{n}{2}\log[2\pi\sigma^{2}]+\frac{1}{2\sigma^{2}}\|\mathbf{y} -X\mathbf{\alpha}|_{2}^{2}+p\log[2\tau]+\frac{1}{\tau}|\mathbf{\alpha}|_{1}\big{]}\] \[\quad=\,\operatorname*{arg\,min}_{\mathbf{\alpha}\in\mathbb{R}^{p}} \big{[}\,\big{[}\frac{1}{2\sigma^{2}}\|\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}+\frac{1}{ \tau}|\mathbf{\alpha}|_{1}\big{]}\] \[\quad=\,\operatorname*{arg\,min}_{\mathbf{\alpha}\in\mathbb{R}^{p}} \big{[}\,\big{\{}\mathbf{y}-X\mathbf{\alpha}|_{2}^{2}+\frac{2\sigma^{2}}{\tau}|\mathbf{ \alpha}|_{1}\big{\}}\] by following the same lines as above. Hence, \(\widehat{\mathbf{\beta}}_{\text{map}}=\widehat{\mathbf{\beta}}_{\text{lasso}}\) for \(r=2\sigma^{2}/\tau\), as desired.
3. The proof follows the exact same scheme as above.

### Solutions for \(\blacktriangleright\) Sect. 1.4

**Solution 1.4** The solutions are simple applications of the definitions of convexity and strict convexity.

1. There is a number of ways to prove this claim: we choose a route that requires minimal knowledge about calculus. With \(\ell:\mathbf{a}\mapsto|\mathbf{y}-X\mathbf{a}|_{2}^{2}\), we find \[\ell[w\mathbf{a}+(1-w)\mathbf{b}]-w\ell[\mathbf{a}]-(1-w)\ell[\mathbf{b}]\] \[\quad=|\mathbf{y}-X(w\mathbf{a}+(1-w)\mathbf{b})|_{2}^{2}\] \[\quad\quad-w[\mathbf{y}-X\mathbf{a}]_{2}^{2}-(1-w)|\mathbf{y}-X\mathbf{b}|_{2}^{2 }\qquad\qquad\text{choice of }\ell\] \[\quad=|w(\mathbf{y}-X\mathbf{a})+(1-w)(\mathbf{y}-X\mathbf{b})|_{2}^{2}\]\[-w\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}-(1-w)\|\boldsymbol{y}-X \boldsymbol{b}\|_{2}^{2}\] \[= \|w(\boldsymbol{y}-X\boldsymbol{a})\|_{2}^{2}+\|(1-w)(\boldsymbol{y }-X\boldsymbol{b})\|_{2}^{2}\] \[+2(w(\boldsymbol{y}-X\boldsymbol{a}),\;(1-w)(\boldsymbol{y}-X \boldsymbol{b}))\] \[-w\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}-(1-w)\|\boldsymbol{ y}-X\boldsymbol{b}\|_{2}^{2}\] \[\|\boldsymbol{a}+\boldsymbol{b}\|_{2}^{2}=\|\boldsymbol{a}\|_{2} ^{2}+\|\boldsymbol{b}\|_{2}^{2}+2\langle\boldsymbol{a},\;\boldsymbol{b}\rangle\] \[= w^{2}\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}+(1-w)^{2}\| \boldsymbol{y}-X\boldsymbol{b}\|_{2}^{2}\] \[+2w(1-w)\langle\boldsymbol{y}-X\boldsymbol{a},\;\boldsymbol{y}- X\boldsymbol{b}\rangle\] \[-w\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}-(1-w)\|\boldsymbol{ y}-X\boldsymbol{b}\|_{2}^{2}\] absolute homogeneity of norms and \(w\), \(1-w\in[0,1]\); linearity of inner products \[= w^{2}\big{(}\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}+\| \boldsymbol{y}-X\boldsymbol{b}\|_{2}^{2}-2\langle\boldsymbol{y}-X\boldsymbol{a },\;\boldsymbol{y}-X\boldsymbol{b}\rangle\big{)}\] \[-w\big{(}\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}+\|\boldsymbol {y}-X\boldsymbol{b}\|_{2}^{2}-2\langle\boldsymbol{y}-X\boldsymbol{a},\; \boldsymbol{y}-X\boldsymbol{b}\rangle\big{)}\] sorting with respect to \(w\) \[= w^{2}\|\boldsymbol{y}-X\boldsymbol{a}-(\boldsymbol{y}-X \boldsymbol{b})\|_{2}^{2}-w\|\boldsymbol{y}-X\boldsymbol{a}-(\boldsymbol{y}-X \boldsymbol{b})\|_{2}^{2}\] \[\|\boldsymbol{a}+\boldsymbol{b}\|_{2}^{2}=\|\boldsymbol{a}\|_{2} ^{2}+\|\boldsymbol{b}\|_{2}^{2}+2\langle\boldsymbol{a},\;\boldsymbol{b}\rangle\] \[= (w^{2}-w)\|X\boldsymbol{b}-X\boldsymbol{a}\|_{2}^{2}\] consolidating \[\leq 0\,,\qquad\qquad\qquad w^{2}\leq w\,\text{for}\;w\in[0,1]\text{; norms are non-negative as desired.}\]
2. This claim can be proved along the same lines as 1., but for the sake of illustration, we present a different route. A smooth function \(\mathbb{R}^{p}\to\mathbb{R}\) is strictly convex if and only if its _Hesse matrix_, which is the matrix of second-order derivatives, is positive definite everywhere. In our case, \(\boldsymbol{a}\mapsto\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}\) is indeed smooth, and its Hesse matrix is \[\nabla^{2}\|\boldsymbol{y}-X\boldsymbol{a}\|_{2}^{2}\,=\,2X^{\top}X\,.\] Hence, the function in question is strictly convex if and only if \(X^{\top}X\) is positive definite. Since \(X^{\top}X\) is symmetric, and positive definiteness and invertibility are equivalent for symmetric matrices, this yields the desired claim.

3. There is a plethora of such functions; we just look at two simple examples. Considering functions \(\mathbb{R}^{p}\to\mathbb{R}\), a simple example for the first case is the constant function \(\boldsymbol{a}\mapsto 0\), for which every point is a minimum; a simple example for the second case is \(\boldsymbol{a}\mapsto|\boldsymbol{a}|_{1}\), for which \(\boldsymbol{0}_{p}\) is the only minimum. For the bonus question, the answer is no. Indeed, naming the function in question \(\boldsymbol{\ell}\) and \(\boldsymbol{a}:=(1,0)^{\top}\), \(\boldsymbol{b}:=(0,1)^{\top}\) and assuming that \(\boldsymbol{\ell}\) is convex, we find \[1 =\,\boldsymbol{\ell}\big{[}(1,1)^{\top}\big{]}\] definition of \[\boldsymbol{\ell}\] \[=\,\boldsymbol{\ell}\big{[}(2,0)^{\top}/2+(0,2)^{\top}/2\big{]}\] basic vector algebra \[\leq\,\boldsymbol{\ell}\big{[}(2,0)^{\top}\big{]}/2+\boldsymbol{ \ell}\big{[}(0,2)^{\top}\big{]}/2\] assumed convexity \[=\,0/2+0/2\] definition of \[\boldsymbol{\ell}\] \[=\,0\,,\] consolidating which is a contradiction.
4. We prove this by a simple contradiction. Assume that \(\boldsymbol{a}\) and \(\boldsymbol{b}\) are two distinct minima of \(\boldsymbol{\ell}\). The presumed strict convexity implies for every intermediate point \(w\boldsymbol{a}+(1-w)\boldsymbol{b}\), \(w\in(0,1)\), that \[\boldsymbol{\ell}\big{[}w\boldsymbol{a}+(1-w)\boldsymbol{b}\big{]}\,<\,w \boldsymbol{\ell}\big{[}\boldsymbol{a}\big{]}+(1-w)\boldsymbol{\ell}\big{[} \boldsymbol{b}\big{]}\,.\] Since both \(\boldsymbol{a}\) and \(\boldsymbol{b}\) are minima of \(\boldsymbol{\ell}\), it holds that \(\boldsymbol{\ell}\big{[}\boldsymbol{a}\big{]}=\boldsymbol{\ell}\big{[} \boldsymbol{b}\big{]}\). Plugging this into the display and collecting terms yield \[\boldsymbol{\ell}\big{[}w\boldsymbol{a}+(1-w)\boldsymbol{b}\big{]}\,<\,w \boldsymbol{\ell}\big{[}\boldsymbol{a}\big{]}+(1-w)\boldsymbol{\ell}\big{[} \boldsymbol{a}\big{]}\,=\,\boldsymbol{\ell}\big{[}\boldsymbol{a}\big{]}\,,\] which means that \(\boldsymbol{a}\) is not a minimum of \(\boldsymbol{\ell}\) as assumed initially. This is the desired contradiction.

**Solution 1.6**: The proof follows readily from the explicit form of the ridge estimator. \(\blacktriangleright\) Sect. 1.4 provides us with

\[\widehat{\boldsymbol{\beta}}_{\text{ridge}}[r]\,=\,\frac{1}{4-8d+4d^{2}+(9+d^ {2})r+r^{2}}\begin{pmatrix}4-8d+4d^{2}+5r\\ (4+d)r\end{pmatrix}\]

[MISSING_PAGE_FAIL:260]

\[=\ -\begin{pmatrix}\alpha_{1}-1+d\alpha_{2}\\ 2(\alpha_{1}-1)+2\alpha_{2}\end{pmatrix}\,.\qquad\text{rearranging the quantities}\] Hence, \[\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2}\] \[=\ \big{(}\alpha_{1}-1+d\alpha_{2}\big{)}^{2}+\big{(}2(\alpha_{1 }-1)+2\alpha_{2}\big{)}^{2}\] invoking the above display and the definition of the \(\ell_{2}\)-norm as desired.
2. This proof follows from 1. by plugging in \(d\). Indeed, we find \[\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2}\] \[=\ 5(\alpha_{1}-1)^{2}+(4+d^{2})\alpha_{2}^{2}+2(4+d)(\alpha_{1} -1)\alpha_{2}\] \[=\ 5\big{(}(\alpha_{1}-1)^{2}+\alpha_{2}^{2}+2(\alpha_{1}-1) \alpha_{2}\big{)}\] using that \[d=1\] and summarizing terms \[=\ 5(\alpha_{1}-1+\alpha_{2})^{2}\,,\qquad\qquad\qquad\qquad\text{ binomial theorem}\] as desired.
3. The proof follows from rewriting the terms in 2. We find \[\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2} =\ c\] \[\Leftrightarrow\quad\ 5(\alpha_{1}-1+\alpha_{2})^{2} =\ c\] \[\Leftrightarrow\quad\ \ \alpha_{1}-1+\alpha_{2} =\ \pm\sqrt{\frac{c}{5}}\] dividing both sides by 5 and taking square-roots as desired.

4. The proof consists of plugging in the values and taking limits. We find \[\frac{\big{(}(\cos\omega)(\alpha_{1}-1)+(\sin\omega)\alpha_{2}\big{)}^{2}}{a^{2}}\] \[\quad\quad+\frac{\big{(}-(\sin\omega)(\alpha_{1}-1)+(\cos\omega) \alpha_{2}\big{)}^{2}}{b^{2}}\] \[\to 10\big{(}(\cos 45^{\circ})(\alpha_{1}-1)+(\sin 45^{\circ}) \alpha_{2}\big{)}^{2}\] \[\quad\quad+\frac{\big{(}-(\sin 45^{\circ})(\alpha_{1}-1)+(\cos 45^{ \circ})\alpha_{2}\big{)}^{2}}{b^{2}_{\text{plugging in the values for $\omega$ and $a$}}\] \[= \frac{10(\alpha_{1}-1+\alpha_{2})^{2}}{2}+\frac{(\alpha_{1}-1+ \alpha_{2})^{2}}{2b^{2}}\] \[\to 5(\alpha_{1}-1+\alpha_{2})^{2}\,.\] In view of the comment in the exercise, this means that the level sets converge to the specified limit. We conclude with 2. that the straight lines that form the level sets of the least-squares can be interpreted as degenerate ellipses.
5. Just expand the squared terms and summarize the factors.
6. This is a simple algebra exercise. We find \[\frac{(\cos\omega)^{2}}{a^{2}}+\frac{(\sin\omega)^{2}}{b^{2}}\ =\ 5\] \[\Rightarrow \frac{(\cos\omega)^{2}}{a^{2}}+\frac{1-(\cos\omega)^{2}}{b^{2}}\ =\ 5\] \[\Rightarrow (\cos\omega)^{2}\Big{(}\frac{1}{a^{2}}-\frac{1}{b^{2}}\Big{)}\ =\ 5-\frac{1}{b^{2}}\] factorizing and rearranging termsas desired.
7. This works as above. We find \[\frac{(\sin\omega)^{2}}{a^{2}}+\frac{(\cos\omega)^{2}}{b^{2}} = 4+d^{2}\] comparing the \(a_{2}\)-terms \[\Rightarrow\quad\frac{1-(\cos\omega)^{2}}{a^{2}}+\frac{(\cos \omega)^{2}}{b^{2}} = 4+d^{2}\] \[\Rightarrow\quad(\cos\omega)^{2}\Big{(}\frac{1}{b^{2}}-\frac{1}{a ^{2}}\Big{)}+\frac{1}{a^{2}} = 4+d^{2}\] factorizing terms \[\Rightarrow\quad\frac{5-\frac{1}{b^{2}}}{\frac{1}{a^{2}}-\frac{1}{b ^{2}}}\Big{(}\frac{1}{b^{2}}-\frac{1}{a^{2}}\Big{)}+\frac{1}{a^{2}} = 4+d^{2}\] Claim 6 \[\Rightarrow\quad\frac{1}{b^{2}}-5+\frac{1}{a^{2}} = 4+d^{2}\] simplifying the left-hand side \[\Rightarrow\quad\frac{1}{a^{2}}+\frac{1}{b^{2}} = 9+d^{2}\,,\] consolidating as desired.
8. The proof combines the results from the two previous proof steps. We first observe that \[(\cos\omega)^{2} = \frac{5-\frac{1}{b^{2}}}{\frac{1}{a^{2}}-\frac{1}{b^{2}}}\] Claim 6 \[= \frac{5-\frac{1}{b^{2}}}{\frac{1}{a^{2}}+\frac{1}{b^{2}}-\frac{2} {b^{2}}}\] adding a zero-valued term \[= \frac{5-\frac{1}{b^{2}}}{9+d^{2}-\frac{2}{b^{2}}}\] Claim 7

[MISSING_PAGE_EMPTY:6483]

[MISSING_PAGE_EMPTY:6484]

[MISSING_PAGE_FAIL:266]

[MISSING_PAGE_FAIL:267]

[MISSING_PAGE_EMPTY:6487]

[MISSING_PAGE_FAIL:269]

\[= \sup\big{\{}\,|a|\langle\mathbf{b},\;\text{sign}[a]\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \sup\big{\{}\,|a|\langle\mathbf{b},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \sup\big{\{}\,|a|\langle\mathbf{b},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \sup\big{\{}\,|a|\langle\mathbf{b},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \ |a|\sup\big{\{}\,\langle\mathbf{b},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \ |a|\overline{\mu}[\mathbf{b}]\,,\] definition of the dual function as desired. Details: It actually suffices that \(\mathcal{H}\) is symmetric (verify that symmetry is a weaker assumption than absolute homogeneity).
4. For all \(\mathbf{a}\in\mathbb{R}^{p}\), we find \[\overline{\mu}[\mathbf{a}] = \sup\big{\{}\,\langle\mathbf{a},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[\geq \langle\mathbf{a},\;\mathbf{0}_{p}\rangle\] \[= 0\,,\] linearity of inner products as desired.
5. For all \(\mathbf{a}\in\mathbb{R}^{p}\), we find \[\overline{\mu}[-\mathbf{a}] = \sup\big{\{}\,\langle-\mathbf{a},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \sup\big{\{}\,\langle\mathbf{a},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[-\mathbf{c}]\leq 1\,\big{\}}\] \[= \sup\big{\{}\,\langle\mathbf{a},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \sup\big{\{}\,\langle\mathbf{a},\;\mathbf{c}\rangle\ :\ \mathbf{c}\in\mathbb{R}^{p},\;\;\mathcal{H}[\mathbf{c}]\leq 1\,\big{\}}\] \[= \overline{\mu}[\mathbf{a}]\,,\] definition of the dual function as desired.
6. The triangle inequality and the absolute homogeneity follow from 1. and 3., respectively. The non-negativity follows from 4. Hence, what is left to show is the definiteness: \(\overline{\alpha}[\mathbf{0}_{p}]=0\). This follows similarly as in the proofs above, noting that for every \(\mathbf{a}\neq\mathbf{0}_{p}\) and every norm \(\mathcal{A}\), it holds that \(\mathbf{c}:=\mathbf{a}/\mathcal{A}[\mathbf{a}]\) satisfies \(\mathcal{A}[\mathbf{c}]=\mathcal{A}[\mathbf{a}/\mathcal{A}[\mathbf{a}]]=\mathcal{A}[\mathbf{a}]/ \mathcal{A}[\mathbf{a}]=1\):

\[\overline{\alpha}[\mathbf{0}_{p}] = \sup\big{\{}\left(\mathbf{0}_{p},\ \mathbf{c}\right)\ :\ \mathbf{c}\in \mathbb{R}^{p},\ \mathcal{A}[\mathbf{c}]\leq 1\big{\}}\] \[= \sup\big{\{}0\ :\ \mathbf{c}\in\mathbb{R}^{p},\ \mathcal{A}[\mathbf{c}] \leq 1\big{\}}\] \[= 0\,,\] above comment as desired. Details: The last fact \(\overline{\alpha}[\mathbf{0}_{p}]=0\) also follows from Theorem 2.4.1.
7. For all \(\mathbf{a}\), \(\mathbf{b}\in\mathbb{R}^{p}\) and \(w\in(0,1)\), we find \[\overline{\alpha}\big{[}w\mathbf{a}+(1-w)\mathbf{b}\big{]} \leq \overline{\alpha}\big{[}w\mathbf{a}\big{]}+\overline{\alpha}\big{[}( 1-w)\mathbf{b}\big{]}\] Claim 1 (triangle inequality) \[= |w|\overline{\alpha}\big{[}\operatorname{sign}[w]\mathbf{a}\big{]}\] \[+|1-w|\overline{\alpha}\big{[}\operatorname{sign}[1-w]\mathbf{b} \big{]}\] Claim 2 (absolute semi-homogeneity) \[= w\overline{\alpha}[\mathbf{a}]+(1-w)\overline{\alpha}\big{[}\mathbf{b} \big{]}\,,\] \[w\in(0,1)\], which implies \[|w|=w,|1-w|=1-w,\] and \[\operatorname{sign}[w]=\operatorname{sign}[1-w]=1\] as desired.

The same proof route applies to \(w\in\{0,1\}\), but for the sake of clarity we disentangle this case from the above one. For, say, \(w=0\) (the proof for \(w=1\) is very similar), we find

\[\overline{\alpha}\big{[}w\mathbf{a}+(1-w)\mathbf{b}\big{]} = \overline{\alpha}\big{[}(1-w)\mathbf{b}\big{]}\qquad\qquad w=0\] \[= |1-w|\overline{\alpha}\big{[}\operatorname{sign}[1-w]\mathbf{b} \big{]}\] Claim 2 (absolute semi-homogeneity) \[= (1-w)\overline{\alpha}\big{[}\mathbf{b}\big{]}\qquad\qquad\qquad w=0\] \[= w\overline{\alpha}[\mathbf{a}]+(1-w)\overline{\alpha}\big{[}\mathbf{b} \big{]}\,,\] \[w=0;\ 0\cdot(\pm\infty)=0\text{ by our conventions on Page X}\] as desired.

8. and 9. Finding such examples is left to the reader.

### Solutions for \(\blacktriangleright\) Sect. 2.5

**Solution 2.5** We prove the claim by induction in \(k\).

_Induction basis:_ In the case \(k=1\), the assumption \(a_{1}+\dots+a_{k}=1\) specializes to \(a_{1}=1\). Hence,

\[\begin{array}{rcl}\ell\biggl{[}\sum_{j=1}^{k}a_{j}\boldsymbol{b}^{j}\biggr{]}& =&\ell\bigl{[}a_{1}\boldsymbol{b}^{1}\bigr{]}&k=1\\ &=&a_{1}\cdot\ell\bigl{[}\boldsymbol{b}^{1}\bigr{]}&a_{1}=1\\ &=&\sum_{j=1}^{k}a_{j}\ell\bigl{[}\boldsymbol{b}^{j}\bigr{]},&k=1 \end{array}\]

which proves the claim for \(k=1\).

_Induction step:_ Now, we establish the induction step \(k\mapsto k+1\), that is, we show that if the claim is true for a \(k\in\{1,2,\dots\}\), the claim is also true for \(k+1\).

In the case \(a_{k+1}=1\), the assumption \(a_{1},\dots,a_{k+1}\in[0,1]\) such that \(a_{1}+\dots+a_{k+1}=1\) implies \(a_{1}=\dots=a_{k}=0\). The proof of the induction step is then the same as the proof for the induction basis.

In the case \(a_{k+1}<1\), we observe

\[\begin{array}{rcl}\ell\biggl{[}\sum_{j=1}^{k+1}a_{j}\boldsymbol{b}^{j} \biggr{]}&=&\ell\biggl{[}\sum_{j=1}^{k}a_{j}\boldsymbol{b}^{j}+a_{k+1} \boldsymbol{b}^{k+1}\biggr{]}\\ &&\text{ splitting the sum into two parts}\\ &=&\ell\biggl{[}(1-a_{k+1})\sum_{j=1}^{k}\frac{a_{j}}{1-a_{k+1}}\boldsymbol{b}^{j}+a_{k+1} \boldsymbol{b}^{k+1}\biggr{]}\\ &&a_{k+1}<1\text{ by assumption; linearity of sums}\\ &\leq&(1-a_{k+1})\ell\biggl{[}\sum_{j=1}^{k}\frac{a_{j}}{1-a_{k+1}}\boldsymbol{b}^{j} \biggr{]}+a_{k+1}\ell\bigl{[}\boldsymbol{b}^{k+1}\bigr{]}.\end{array}\]

Definition 2.5.1 (convexity) applied with \(w=1-a_{k+1}\), \(\boldsymbol{a}=\sum_{j=1}^{k}a_{j}\boldsymbol{b}^{j}/(1-a_{k+1})\), \(\boldsymbol{b}=\boldsymbol{b}^{k+1}\)

[MISSING_PAGE_FAIL:273]

for a vector \(\widehat{\boldsymbol{\kappa}}\in\mathfrak{d}\|\widehat{\boldsymbol{\beta}}_{\rm lasso }\|_{1}\). Rearranging the terms in the equality yields \[2X^{\top}X\widehat{\boldsymbol{\beta}}_{\rm lasso}\;=\;X^{\top}\boldsymbol{y}-r \widehat{\boldsymbol{\kappa}}\] and dividing through by \(2n\) then \[\frac{X^{\top}X\widehat{\boldsymbol{\beta}}_{\rm lasso}}{n}\;=\;\frac{X^{\top} \boldsymbol{y}}{2n}-\frac{r\widehat{\boldsymbol{\kappa}}}{2n}\,.\] We can finally use \(X^{\top}X=n\mathbf{I}_{p\times p}\) to derive \[\widehat{\boldsymbol{\beta}}_{\rm lasso}\;=\;\frac{X^{\top}\boldsymbol{y}}{n} -\frac{r\widehat{\boldsymbol{\kappa}}}{2n}\,,\] as desired.
2. The three implications can be derived from the equation in 1. through basic algebra. In fact, we only need to prove the \(\Rightarrow\)'s, as the \(\Leftarrow\)'s then follow logically. _First implication:_ Assume that \(|(X^{\top}\boldsymbol{y})_{j}|\;\leq\;r/2\). We do a proof by contradiction. Assume first \((\widehat{\beta}_{\rm lasso})_{j}>0\). Then, the \(j\)th coordinate of 1. multiplied through by \(n\) gives \[(X^{\top}\boldsymbol{y})_{j}-\frac{r\widehat{\boldsymbol{\kappa}}_{j}}{2}\;\;> \;0\,,\] which can be reformulated as \[(X^{\top}\boldsymbol{y})_{j}\;\;>\;\frac{r\widehat{\boldsymbol{\kappa}}_{j}}{2}\,.\] Hence, since \(\widehat{\boldsymbol{\kappa}}_{j}=1\) for \((\widehat{\beta}_{\rm lasso})_{j}>0\) (see Example 2.5.2), \[(X^{\top}\boldsymbol{y})_{j}\;\;>\;\;\frac{r}{2}\,,\] which implies (recall that \(r\geq 0\)) \[\big{|}(X^{\top}\boldsymbol{y})_{j}\big{|}\;\;>\;\;\frac{r}{2}\,,\] contradicting our initial assumption. Similarly, if \((\widehat{\beta}_{\rm lasso})_{j}<0\), Claim 1 yields \[(X^{\top}\boldsymbol{y})_{j}-\frac{r\widehat{\boldsymbol{\kappa}}_{j}}{2}\;\;< \;0\,,\]which can be reformulated as

\[-(X^{\top}\mathbf{y})_{j}\ \ >\ \ -\frac{r\widehat{\kappa}_{j}}{2}\,.\]

Hence, since \(\widehat{\kappa}_{j}=-1\) for \((\widehat{\beta}_{\text{lasso}})_{j}<0\),

\[-(X^{\top}\mathbf{y})_{j}\ \ >\ \ \frac{r}{2}\,,\]

which implies

\[\big{|}(X^{\top}\mathbf{y})_{j}\big{|}\ \ >\ \ \frac{r}{2}\,,\]

contradicting our initial assumption.

_Second implication:_ Assume that \((X^{\top}\mathbf{y})_{j}>r/2\).

We then find

\[(\widehat{\beta}_{\text{lasso}})_{j} =\frac{(X^{\top}\mathbf{y})_{j}}{n}-\frac{r\widehat{\kappa}_{j}}{2n} \qquad\qquad\qquad j\text{th coordinate of }1.\] \[\geq\frac{(X^{\top}\mathbf{y})_{j}}{n}-\frac{r}{2n} \qquad\qquad\widehat{\kappa}_{j}\leq 1\text{ by Example \ref{example:2}}\] \[>0\,, \qquad\qquad\qquad(X^{\top}y)_{j}>r/2\text{ by assumption}\]

as desired.

_Third implication:_ Assume that \((X^{\top}\mathbf{y})_{j}<-r/2\).

We then find

\[(\widehat{\beta}_{\text{lasso}})_{j} =\frac{(X^{\top}\mathbf{y})_{j}}{n}-\frac{r\widehat{\kappa}_{j}}{2n} \qquad\qquad j\text{th coordinate of }1.\] \[\leq\frac{(X^{\top}\mathbf{y})_{j}}{n}+\frac{r}{2n} \qquad\qquad\widehat{\kappa}_{j}\geq-1\text{ by Example \ref{example:2}}\] \[<0\,, \qquad\qquad\qquad(X^{\top}y)_{j}<-r/2\text{ by assumption}\]

as desired.
3. The proof of this part uses 1. and 2. and the properties of the signum function and of the positive part. Note first that for \(|(X^{\top}\mathbf{y})_{j}|\ \ \leq\ r/2\), the right-hand side of the desired equality is equal to zero, which is correct in view of the second implication of 2. We can thus assume \(|(X^{\top}\mathbf{y})_{j}|>r/2\) in the following. If \((X^{\top}\mathbf{y})_{j}>r/2\), it holds that \(\widehat{\kappa}_{j}=\text{sign}[(\widehat{\beta}_{\text{lasso}})_{j}]=1\) through the second implication of Claim 2 and Example 2.5.2. Similarly, if \(-(X^{\top}\mathbf{y})_{j}>r/2\), it holds that \(\widehat{\kappa}_{j}=\operatorname{sign}[(\widehat{\beta}_{\text{lasso}})_{j}]=-1\).

Hence,

\[(\widehat{\beta}_{\text{lasso}})_{j} = \frac{(X^{\top}\mathbf{y})_{j}}{n}-\frac{r\widehat{\kappa}_{j}}{2n} \text{again from 1}.\] \[= \left\{\begin{array}{ll}\frac{|(X^{\top}\mathbf{y})_{j}|}{n}-\frac{ r}{2n}&\text{if }(X^{\top}\mathbf{y})_{j}>r/2\\ -\frac{|(X^{\top}\mathbf{y})_{j}|}{n}+\frac{r}{2n}&\text{if }-(X^{\top}\mathbf{y})_{j}>r/2 \\ &\text{two preceding observations}\end{array}\right.\] \[= \operatorname{sign}\big{[}(X^{\top}\mathbf{y})_{j}\big{]}\bigg{(} \frac{|(X^{\top}\mathbf{y})_{j}|}{n}-\frac{r}{2n}\bigg{)}\] \[= \operatorname{sign}\big{[}(X^{\top}\mathbf{y})_{j}\big{]}\bigg{(} \frac{|(X^{\top}\mathbf{y})_{j}|}{n}-\frac{r}{2n}\bigg{)}_{+},\] \[|(X^{\top}\mathbf{y})_{j}|\geq r/2\text{ by assumption}\]

as desired.

**Solution 2.11**: We prove the five parts in order.

1. This equation is a simple reformulation of the lasso's KKT condition. Equation (2.7) states \[-2X^{\top}\big{(}\mathbf{y}-X\widehat{\mathbf{\beta}}_{\text{lasso}}[r]\big{)}+r \widehat{\mathbf{\kappa}}\,=\,\mathbf{0}_{p}\] for a vector \(\widehat{\mathbf{\kappa}}\,\in\,\mathfrak{d}\|\widehat{\mathbf{\beta}}_{\text{lasso}} [r]\|_{1}\). Rearranging the terms yields \[2X^{\top}X\widehat{\mathbf{\beta}}_{\text{lasso}}[r]\,=\,2X^{\top}\mathbf{y}-r\widehat {\mathbf{\kappa}}\,,\] and then multiplying both sides by \((X^{\top}X)^{-1}/2\) \[\widehat{\mathbf{\beta}}_{\text{lasso}}[r]\,=\,\underbrace{(X^{\top}X)^{-1}X^{\top }\mathbf{y}}_{\widehat{\mathbf{\beta}}_{\text{ls}}}-\frac{r}{2}(X^{\top}X)^{-1}\widehat {\mathbf{\kappa}}\,,\] as desired.
2. This inequality follows readily from 1. Verify first that if \(A\,\in\,\mathbb{R}^{p\times p}\) is a positive definite matrix with ordered eigenvalues \(m_{p}\geq\cdots\geq m_{1}>0\), the ordered eigenvalues of \(A^{-1}\) are \(1/m_{1}\geq\cdots\geq 1/m_{p}>0\), and it holds that \(|A^{-1}\mathbf{a}|_{2}\leq\|\mathbf{a}\|_{2}/m_{1}\).

[MISSING_PAGE_EMPTY:6496]

3. The claim follows again readily from the lasso's KKT conditions. Once more, recall that Eq. (2.7) states that \(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]\) is a solution of the lasso if and only if \[-2X^{\top}\big{(}\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}} [r]\big{)}+r\widehat{\boldsymbol{\kappa}}\,=\,\boldsymbol{0}_{p}\] for a \(\widehat{\boldsymbol{\kappa}}\,\in\,\mathfrak{d}|\widehat{\boldsymbol{\beta}}_{ \mathrm{lasso}}[r]|_{1}\). Setting \(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]\,=\,\boldsymbol{0}_{p}\) and rearranging yield that \(\boldsymbol{0}_{p}\) is a solution if and only if \[2X^{\top}\boldsymbol{y}\,=\,r\widehat{\boldsymbol{\kappa}}\] for a \(\widehat{\boldsymbol{\kappa}}\,\in\,\mathfrak{d}|\boldsymbol{0}_{p}|_{1}\). Next, recall that \(\widehat{\boldsymbol{\kappa}}\,\in\,\mathfrak{d}|\boldsymbol{0}_{p}|_{1}\) is equivalent to \(\widehat{\kappa}_{j}\,\in\,[-1,\,1]\) for all \(j\,\in\,\{1,\ldots,p\}\)--see Example 2.5.2. Consequently, formulated in terms of the individual coordinates of the vectors, \(\boldsymbol{0}_{p}\) is a solution if and only if for all \(j\,\in\,\{1,\ldots,p\}\), there is a \(\widehat{\kappa}_{j}\,\in\,[-1,\,1]\) such that \[2(X^{\top}\boldsymbol{y})_{j}\,=\,r\widehat{\kappa}_{j}\,.\] The latter is true if and only if \(2|(X^{\top}\boldsymbol{y})_{j}|\,\leq\,r\) for all \(j\,\in\,\{1,\ldots,p\}\), that is, \(2|X^{\top}\boldsymbol{y}|_{\infty}\,\leq\,r\), as desired.
4. For this question, we leverage the estimator's definition directly. Assume that \(\boldsymbol{0}_{p}\) is a lasso solution, that is, \(\boldsymbol{0}_{p}\) is a minimizer of the lasso's objective function. Then, the value of the lasso's objective function at any solution \(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]\,\in\,\mathbb{R}^{p}\) must equal the value at \(\boldsymbol{0}_{p}\): \[|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]|_{2}^{2}+r| \widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]|_{1}\,=\,|\boldsymbol{y}-X \boldsymbol{0}_{p}|_{2}^{2}+r|\boldsymbol{0}_{p}|_{1}\,=\,|\boldsymbol{y}|_{2 }^{2}\,.\] Expanding the first term on the left-hand side of this equality yields \[|\boldsymbol{y}|_{2}^{2}-2\langle\boldsymbol{y},\,X\widehat{\boldsymbol{ \beta}}_{\mathrm{lasso}}[r]\rangle+|X\widehat{\boldsymbol{\beta}}_{\mathrm{ lasso}}[r]|_{2}^{2}+r|\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]|_{1}\,=\,| \boldsymbol{y}|_{2}^{2}\] and simplifying and rearranging then \[|X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]|_{2}^{2}+r|\widehat{ \boldsymbol{\beta}}_{\mathrm{lasso}}[r]|_{1}\,=\,2\langle\boldsymbol{y},\,X \widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}[r]\rangle\,.\]

[MISSING_PAGE_FAIL:279]

[MISSING_PAGE_EMPTY:6499]

The positive definiteness of the matrix is proved as follows: for every \(\mathbf{b}\in\mathbb{R}^{d}\setminus\{\mathbf{0}_{d}\}\),

\[\mathbf{b}^{\top}\big{(}w\Omega^{\prime}+(1-w)\Omega^{\prime\prime}\big{)}\mathbf{b}\] \[=\ w\mathbf{b}^{\top}\Omega^{\prime}\mathbf{b}+(1-w)\mathbf{b}^{\top}\Omega^{ \prime\prime}\mathbf{b}\] \[>\ 0\,,\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \Omega^{\prime},\Omega^{\prime\prime}\text{ positive definite by assumption;}\ w,1-w\geq 0\text{ by assumption}\]

as desired.

This concludes the proof of Claim 1.
2. This claim follows directly from the linearity of matrix multiplications and of the trace function.

Indeed, these linearity properties applied in sequence yield for all \(\Omega^{\prime},\Omega^{\prime\prime}\in\mathbb{R}^{d\times d}\) and \(w\in[0,1]\) that

\[\text{trace}\left[A\big{(}w\Omega^{\prime}+(1-w)\Omega^{\prime \prime}\big{)}\right] =\ \text{trace}\left[wA\Omega^{\prime}+(1-w)A\Omega^{\prime\prime}\right] =\ w\,\text{trace}[A\Omega^{\prime}]+(1-w)\,\text{trace}[A\Omega^{\prime }],\]

which implies the desired claim.
3. This proof is again a simple algebra exercise.

According to Lemma B.2.11 (singular value decomposition), there are orthogonal matrices \(U\), \(V\in\mathbb{R}^{d\times d}\) and a diagonal matrix \(D\in\mathbb{R}^{d\times d}\) with diagonal elements and the singular values \(m_{1},\dots,m_{d}\) of \(A\) such that (i) \(A=UDV^{\top}\).

Moreover, it holds by Lemma B.2.8 (determinant of orthogonal matrices) and 2. in Lemma B.2.3 (basic properties of determinants) that (ii) \(|\text{det}[U]|=|\text{det}[V^{\top}]|=1\).

We then find

\[|\text{det}[A]| =\ |\text{det}[UDV^{\top}]| A=UDV^{\top}\text{ by (i)}\] \[=\ |\text{det}[U]\,\text{det}[D]\,\text{det}[V^{\top}]|\] \[\qquad\qquad\qquad\qquad\text{1. in Lemma B.2.3 (multiplicativity of the determinant) applied twice}\] \[=\ |\text{det}[D]| |\text{det}[U]|=|\text{det}[V^{\top}]|=1\text{ by (ii)}\]

[MISSING_PAGE_FAIL:282]

[MISSING_PAGE_FAIL:283]

where \(m_{1},\ldots,m_{d}>0\) are the eigenvalues of \(\mathrm{I}_{d\times d}+t\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\).

Since \(\Omega\), \(\Omega+tA\in\mathcal{S}_{d}^{+}\) by assumption, and \(\mathrm{I}_{d\times d}+t\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\in\mathcal{ S}_{d}^{+}\) by Step 2, Lemma B.2.4 implies that the determinants of all three matrices are strictly positive, and we can therefore take (negative) logarithms in the result of Step 1:

\[- \log\det\left[\Omega+tA\right]\] \[= -\log\left[\det[\Omega]\cdot\det[\mathrm{I}_{d\times d}+t\Omega^{ -\frac{1}{2}}A\Omega^{-\frac{1}{2}}]\right]\qquad\text{Step 1}\] \[= -\log\det[\Omega]-\log\det[\mathrm{I}_{d\times d}+t\Omega^{-\frac{ 1}{2}}A\Omega^{-\frac{1}{2}}]\,.\]

Now, since \(\mathrm{I}_{d\times d}+t\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\) is symmetric and positive definite according to Step 2, we find in view of Claim 3 of the exercise that

\[-\log\det\left[\Omega+tA\right] = -\log\det[\Omega]-\log\left[\prod_{j=1}^{d}m_{j}\right]\] \[= -\log\det[\Omega]-\sum_{j=1}^{d}\log[m_{j}]\,,\]

where \(m_{1},\ldots,m_{d}>0\) are the eigenvalues of \(\mathrm{I}_{d\times d}+t\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\), and where we have used again the rules for the logarithm. This is the desired identity.

_Step 4:_ We now compute the eigenvalues \(m_{1},\ldots,m_{d}\) of \(\mathrm{I}_{d\times d}+t\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\):

\[m_{j}\,=\,1+t\widetilde{m}_{j}\qquad\text{ for all }j\in\{1,\ldots,d\}\,,\]

where \(\widetilde{m}_{1},\ldots,\widetilde{m}_{d}\) are the eigenvalues of \(\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\).

One can show similarly as in Step 2 that \(\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\) is symmetric. Hence, according to the spectral decomposition in Lemma B.2.9, it admits a basis of eigenvectors \(\boldsymbol{q}_{1},\ldots,\boldsymbol{q}_{d}\in\mathbb{R}^{d}\) with eigenvalues \(\widetilde{m}_{1},\ldots,\widetilde{m}_{d}\). By Definition B.2.7 of eigenvectors and eigenvalues and the linearity of matrix multiplications,

\[(\mathrm{I}_{d\times d}+t\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}) \boldsymbol{q}_{j}\,=\,(1+t\widetilde{m}_{j})\boldsymbol{q}_{j}\,.\]This means that \(\boldsymbol{q}_{j}\) is also an eigenvector of \(\mathbf{I}_{d\times d}+t\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\) with eigenvalue \(1+t\widetilde{m}_{j}\). Hence, \(\boldsymbol{q}_{1},\ldots,\boldsymbol{q}_{d}\in\mathbb{R}^{d}\) is a basis of eigenvectors of that matrix with eigenvalues \(1+t\widetilde{m}_{1},\ldots,1+t\widetilde{m}_{d}\). The desired statement then follows from this.

_Step 5:_ We finally show that the function

\[t\ \mapsto\ -\log\det[\Omega+tA]\]

is strictly convex on \([0,\widetilde{t}]\).

In view of Steps 3 and 4, we need to show strict convexity of

\[t\ \mapsto\ \ -\log\det[\Omega]-\sum_{j=1}^{d}\log[1+t\widetilde{m}_{j}]\]

on \([0,\widetilde{t}]\), where \(\widetilde{m}_{1},\ldots,\widetilde{m}_{d}\) are the eigenvalues of \(\Omega^{-\frac{1}{2}}A\Omega^{-\frac{1}{2}}\).

The first term is constant in \(t\) and, therefore, convex. Let us then look at the remaining functions

\[t\ \mapsto\ \ -\log[1+t\widetilde{m}_{j}]\qquad\text{for all}\ j\in\{1,\ldots,d\}\,.\]

Taking derivatives, we find

\[\frac{\partial^{2}}{\partial t^{2}}\bigl{(}-\log[1+t\widetilde{m}_{j}]\bigr{)} \ =\ \frac{\partial}{\partial t}\biggl{(}-\frac{\widetilde{m}_{j}}{1+t \widetilde{m}_{j}}\biggr{)}\ =\ \frac{(\widetilde{m}_{j})^{2}}{(1+t\widetilde{m}_{j})^{2}}\,.\]

The right-hand side is strictly positive (for \(\widetilde{m}_{j}\neq 0\)) or at least non-negative (for \(\widetilde{m}_{j}=0\)). This implies that the function in question is strictly convex (for \(\widetilde{m}_{j}\neq 0\)) or at least convex (for \(\widetilde{m}_{j}=0\))--we omit the details here.

Verify that \(A\neq\boldsymbol{0}_{d\times d}\) implies that \(\widetilde{m}_{j}\neq 0\) for at least one \(j\in\{1,\ldots,d\}\). This means that the function in question is a sum of one or more convex functions and at least one strictly convex function, and one can show similarly as in Lemma 2.5.1 (where the domain is \(\mathbb{R}^{d}\) rather than an interval) that such a function is strictly convex. This concludes the proof.

Details: The assumed positive definiteness of \(\Omega+tA\) ensures that \(1+t\widetilde{m}_{j}>0\), but \(\widetilde{m}_{j}\) itself can be negative.

5. We need to show that for all \(w\in(0,1)\) and \(\Omega^{\prime},\Omega^{\prime\prime}\in\delta_{d}^{+}\) with \(\Omega^{\prime}\neq\Omega^{\prime\prime}\), it holds that \[-\log\det\left[w\Omega^{\prime}+(1-w)\Omega^{\prime\prime}\right]\] \[< -w\log\det[\Omega^{\prime}]-(1-w)\log\det[\Omega^{\prime\prime}]\,.\] We do this via Claim 4. Fix two matrices \(\Omega^{\prime},\Omega^{\prime\prime}\in\delta_{d}^{+}\), \(\Omega^{\prime}\neq\Omega^{\prime\prime}\). It then holds that \(\Omega:=\Omega^{\prime\prime}\in\delta_{d}^{+}\) and that \(A:=\Omega^{\prime}-\Omega^{\prime\prime}\in\mathbb{R}^{d\times d}\setminus\{ \mathbf{0}_{d\times d}\}\) is symmetric. Further, it holds that \(\Omega+tA\) is symmetric and positive definite for every \(t\in[0,\tilde{t}]\) with \(\tilde{t}:=1\). According to Claim 4, this implies that \(t\mapsto\ -\log\det[\Omega+tA]\) is strictly convex on \([0,\tilde{t}]\); hence, by the definition of strict convexity (use the statement in the exercise with \(p=1\)), it holds for every \(w\in(0,1)\) and \(t^{\prime}\), \(t^{\prime\prime}\in[0,\tilde{t}]\) with \(t^{\prime}\neq t^{\prime\prime}\) that \[-\log\det\left[\Omega+(wt^{\prime}+(1-w)t^{\prime\prime})A\right]\] \[< -w\log\det\left[\Omega+t^{\prime}A\right]-(1-w)\log\det\left[ \Omega+t^{\prime\prime}A\right].\] Recalling that \(\Omega=\Omega^{\prime\prime}\) and \(A=\Omega^{\prime}-\Omega^{\prime\prime}\) and setting \(t^{\prime}=1\) and \(t^{\prime\prime}=0\) give \[-\log\det\left[\Omega^{\prime\prime}+(w\cdot 1+(1-w)\cdot 0)( \Omega^{\prime}-\Omega^{\prime\prime})\right]\] \[< -w\log\det\left[\Omega^{\prime\prime}+1\cdot(\Omega^{\prime}- \Omega^{\prime\prime})\right]\] \[-(1-w)\log\det\left[\Omega^{\prime\prime}+0\cdot(\Omega^{\prime}- \Omega^{\prime\prime})\right].\] Consolidating both sides yields \[-\log\det\left[w\Omega^{\prime}+(1-w)\Omega^{\prime\prime}\right]\] \[< -w\log\det[\Omega^{\prime}]-(1-w)\log\det[\Omega^{\prime\prime}]\,,\] as desired.
6. This claim follows directly from 2., 5., and Lemma 2.5.1, which states that the sum of a convex function and a strictly convex function is again a strictly convex function.

**Solution 3.3** We prove the five claims in order.

1. The proof of the first claim is based on the linearity of the trace function.

[MISSING_PAGE_EMPTY:6506]

[MISSING_PAGE_EMPTY:6507]

[MISSING_PAGE_FAIL:289]

Details: The fact that affine subspaces with codimension larger or equal to one have Gauss measure zero can be proved as follows (an affine subspace that has dimension \(s\) has codimension \(d-s\) with respect to the \(\mathbb{R}^{d}\)): Consider an affine subspace \(\mathcal{A}\) of dimension \(k\) in \(\mathbb{R}^{d}\), \(k<d\). There is a rotation \(R\) such that for every element \(\boldsymbol{a}\in R\mathcal{A}\), it holds that \(a_{d}=0\). Denoting the Lebesgue measure on \(\mathbb{R}^{d}\) by \(\mathbb{P}\), we then find \[\mathbb{P}\mathcal{A} =\,\mathbb{P}R\mathcal{A} \qquad\qquad\qquad\qquad\qquad\text{the Lebesgue measure on $\mathbb{R}^{d}$ is rotation invariant (we omit that proof)}\] \[\leq\,\mathbb{P}\{\boldsymbol{a}\in\mathbb{R}^{d}\,:\,a_{d}\,=\,0\}\] \[=\,\mathbb{P}\bigcup_{i=1}^{\infty}\big{\{}\boldsymbol{a}\in \mathbb{R}^{d}\,:\,a_{1},\ldots,a_{d-1}\in[-i,i],a_{d}=0\big{\}}\] \[=\,\lim_{l\to\infty}\mathbb{P}\big{\{}\boldsymbol{a}\in\mathbb{R} ^{d}\,:\,a_{1},\ldots,a_{d-1}\in[-i,i],a_{d}=0\big{\}}\] measures are countably additive \[=\,\lim_{l\to\infty}\big{(}[-i,i]^{d-1}\cdot 0\big{)} \qquad\qquad\qquad\qquad\qquad\text{definition of the Lebesgue measure in Dudley (2002, Theorem 3.2.6 on p. 98 and Chapter 4.4 on pp. 134ff)}\] \[=\,0\,.\] (Note that one cannot take boxes \(\{\boldsymbol{a}\in\mathbb{R}^{d}\,:\,|a_{d}|\leq 1/i\}\) instead, because then the stated definition of the Lebesgue measure does not apply.) Since the Lebesgue measure dominates all (non-degenerate) Gauss distributions, this proves that affine subspaces of dimension at most \(d-1\) are Gauss nullsets in \(\mathbb{R}^{d}\).
5. In view of the formulation of the maximum likelihood estimator on Page 88, this claim follows directly from 3. and 4. when taking \(A=\sum_{i=1}^{n}\boldsymbol{z}i\boldsymbol{z}^{\top}/n\).

### Solutions for \(\blacktriangleright\) Sect. 3.4

**Solution 3.4** In essence, we reformulate basic properties of the least-squares solutions.

[MISSING_PAGE_EMPTY:6510]

Combining this with the foregoing display yields the desired claim.
2. This is a straightforward calculation. We find \[\|\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}|_{2}^{2}\;=\;\langle \boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j},\,\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}\rangle\] definition of the \(\ell_{2}\)-norm \[\;=\;\langle\boldsymbol{y}_{j},\,\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}\rangle\] \[\;-\;\langle X_{-j}\widehat{\boldsymbol{\beta}}^{j},\,\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}\rangle\] linearity of the inner product \[\;=\;\langle\boldsymbol{y}_{j},\,\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}\rangle\] \[\;-\;\langle\frac{\widehat{\boldsymbol{\beta}}^{j}}{2},\;2(X_{-j})^{\top}( \boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j})\rangle\] linearity of inner product; property (B.1) of transposition \[\;=\;\langle\boldsymbol{y}_{j},\,\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}\rangle-(\frac{\widehat{\boldsymbol{\beta}}^{j}}{2},\;\boldsymbol{0}_{d-1})\] \[\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \;\[= -n\sum_{k=1}^{d}\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}z_{i}{}^{\top} \right)_{j,k}\underbrace{(\widehat{\boldsymbol{\beta}}^{j})_{k}}_{\text{ as above}}\] \[= -n\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}z_{i}{}^{\top}\right)_{j \{1,\ldots,d\}}\underbrace{\widehat{\boldsymbol{\beta}}^{j}}_{\text{ formulating in terms of matrix-vector multiplications}}\]

Dividing both sides by \(-n\) yields the desired result.
3. We plug the results from above into the definition of the neighborhood selection estimator.

The two claims above can be combined into

\[\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}z_{i}{}^{\top}\right)\widehat{\boldsymbol {\beta}}^{j}= -\frac{|\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}|_{2}^{2}} {n}e_{j}\,,\]

where \(\boldsymbol{e}_{j}\in\mathbb{R}^{d}\) is the \(j\)th standard unit vector: \((\boldsymbol{e}_{j})_{k}=1\) if \(k=j\) and \((\boldsymbol{e}_{j})_{k}=0\) otherwise. By assumption, we can invert the empirical covariance matrix, so that the equation can be written as

\[\widehat{\boldsymbol{\beta}}^{j}= -\frac{|\boldsymbol{y}_{j}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}| _{2}^{2}}{n}\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}z_{i}{}^{\top}\right)^{-1} \boldsymbol{e}_{j}\,.\]

With this in mind, we find for all \(k,l\in\{1,\ldots,d\}\) that

\[(\widehat{\Theta}_{\text{ns}})_{kl}= -\frac{\frac{n}{|\boldsymbol{y}_{k}-X_{-j}\widehat{\boldsymbol{ \beta}}^{j}|_{2}^{2}}(\widehat{\boldsymbol{\beta}}^{k})_{l}+\frac{n}{|\boldsymbol {y}_{l}-X_{-j}\widehat{\boldsymbol{\beta}}^{j}|_{2}^{2}}(\widehat{\boldsymbol {\beta}}^{j})_{k}}{2}\] Definition (3.6) \[=\frac{\left(\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}z_{i}{}^{\top} \right)^{-1}\boldsymbol{e}_{k}\right)_{l}+\left(\left(\frac{1}{n}\sum_{i=1}^{ n}z_{i}{}^{\top}\right)^{-1}\boldsymbol{e}_{l}\right)_{k}}{2}\] preceding display \[=\frac{\left(\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}{}^{\top}\right)^{-1} \right)_{R}+\left(\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}{}^{\top}\right)^{-1} \right)_{kl}}{\boldsymbol{e}_{k},\boldsymbol{e}_{l}\text{ are the $k$th and $l$th unit vectors, respectively}}\] \[=\left(\left(\frac{1}{n}\sum_{i=1}^{n}z_{i}z_{i}{}^{\top}\right)^{- 1}\right)_{kl},\] the empirical covariance matrix (and, therefore, also its inverse--see 2. in Lemma B.2.6) is symmetric as desired.

### Solutions for \(\blacktriangleright\) Chap. 4

#### Solutions for \(\blacktriangleright\) Sect. 4.2

**Solution 4.2** The proof relies only on Properties (i) and (ii).

Our first observation is that

\[\mathbb{P}\bigg{\{}\bigcup_{j=1}^{k}\mathcal{A}_{j}\bigg{\}}\ =\ \mathbb{P}\bigg{\{}\bigcup_{j=1}^{k}\Big{(}\mathcal{A}_{j}\setminus\bigcup_{i=1}^{j-1} \mathcal{A}_{i}\Big{)}\bigg{\}}\\ \text{verify that }\bigcup_{j=1}^{k}\mathcal{A}_{j}=\cup_{j=1}^{k}(\mathcal{A}_{j}\setminus\cup_{i=1}^{j-1} \mathcal{A}_{i}),\\ \text{where we set }\cup_{i=1}^{0}\mathcal{A}_{i}:=\varnothing\\ =\ \mathbb{P}\bigg{\{}\mathcal{A}_{k}\setminus\bigcup_{i=1}^{k-1}\mathcal{A}_{i}\bigg{\}}+\mathbb{P}\bigg{\{}\bigcup_{j=1}^{k-1}\Big{(}\mathcal{A}_{j}\setminus\bigcup_{i=1}^{j-1}\mathcal{A}_{i}\Big{)}\bigg{\}}\\ \text{the events }\mathcal{A}_{k}\setminus\cup_{i=1}^{k-1}\mathcal{A}_{i}\text{ and }\cup_{j=1}^{k-1}(\mathcal{A}_{j}\setminus\cup_{i=1}^{k-1}\mathcal{A}_{i})\\ \text{are mutually disjoint; Property (i)}\\ =\ \ldots\ =\ \sum_{j=1}^{k}\mathbb{P}\bigg{\{}\mathcal{A}_{j}\setminus\bigcup_{i=1}^{j-1} \mathcal{A}_{i}\bigg{\}}\.\]

Our second observation is that for all \(j\in\{1,\ldots,k\}\),

\[\mathbb{P}\{\mathcal{A}_{j}\}\\ =\ \mathbb{P}\bigg{\{}\Big{(}\mathcal{A}_{j}\setminus\bigcup_{i=1}^{j-1}\mathcal{A}_{i}\Big{)}\cup\Big{(}\mathcal{A}_{j}\cap\bigcup_{i=1}^{j-1}\mathcal{A}_{i}\Big{)}\bigg{\}}\\ \mathcal{A}=(\mathcal{A}\setminus\mathcal{B})\cup(\mathcal{A}\cap\mathcal{B})\text{ for all two sets }\mathcal{A},\mathcal{B}\\ =\ \mathbb{P}\bigg{\{}\mathcal{A}_{j}\setminus\bigcup_{i=1}^{j-1}\mathcal{A}_{i}\bigg{\}}+\mathbb{P}\bigg{\{}\mathcal{A}_{j}\cap\bigcup_{i=1}^{j-1}\mathcal{A}_{i}\bigg{\}}\\ \text{the events }\mathcal{A}_{j}\setminus\cup_{i=1}^{j-1}\mathcal{A}_{i}\text{ and }\mathcal{A}_{j}\cap\cup_{i=1}^{j-1}\mathcal{A}_{i}\\ \text{are disjoint; Property (i)}\\ \geq\ \mathbb{P}\bigg{\{}\mathcal{A}_{j}\setminus\bigcup_{i=1}^{j-1}\mathcal{A}_{i}\bigg{\}}\.\]

Combining the two observations yields the desired inequality.

[MISSING_PAGE_EMPTY:6514]

## Solutions for Sect. 4.3

**Solution 4.5** We use Plots \(\alpha\)-\(\delta\):

1. By definition (4.1), the optimal tuning parameter \(r^{*}\) is the minimizer of the prediction error \(\ell^{*}\): see Plot \(\alpha\). In the depicted case, it seems that \(\ell^{*}\) is continuous; then, by definition of continuity, it holds that: for every \(\delta>0\) there is a \(\varepsilon>0\) such that \(|\ell^{*}[r]-\ell^{*}[r^{*}]|<\delta\) for all \(r\) with \(|r-r^{*}|<\varepsilon\).
2. The fact that the values \(\widehat{\mathcal{H}}[r]\) are in the red shaded area for all \(r\in\mathcal{R}\) implies that \(\widehat{\mathcal{H}}[r^{*}]\) is below the black, dashed, horizontal line in Plot \(\beta\). Now, the estimated tuning parameter \(\widehat{r}\) is the minimizer of the estimated prediction errors \(\widehat{\mathcal{L}}\) by definition (4.2), which implies that \(\widehat{\mathcal{H}}[r]\leq\widehat{\mathcal{H}}[r^{*}]\). Using again that \(\widehat{\mathcal{L}}\) is in the red shaded area, we find that the tuning parameters for which \(\widehat{\mathcal{H}}[r]\) can be below the black line are limited to the blue shaded area in Plot \(\beta\). This is a common pattern: typically, the better the estimate of the prediction error, the closer the corresponding estimate for the tuning parameter is to the optimal one.
3. We mark the tuning parameters according to (4.1) and (4.2): see Plot \(\gamma\). Even though \(\widehat{\mathcal{L}}\) is not a good estimator of \(\ell^{*}\) (for example, \(\widehat{\mathcal{L}}[r^{*}]\gg\ell^{*}[r^{*}]\)), the corresponding tuning parameter \(\widehat{r}\) is close to \(r^{*}\).

[MISSING_PAGE_FAIL:297]

1. For \(M_{{}_{4Z}}=\mathbf{0}_{p\times p}\), the generalized Newton-Raphson algorithm always stays at its starting point \(\widehat{\boldsymbol{\beta}}^{0}\)--see Display (5.3)-- which might not be a least-squares solution.
2. For \(M_{{}_{4Z}}=\mathbf{I}_{p\times p}\) (rather than \(M_{{}_{4Z}}=\mathbf{I}_{p\times p}/2\)), the generalized Newton-Raphson algorithm does not necessarily converge: similarly as in the first display of Example 5.1.3, we can derive \[\widehat{\boldsymbol{\beta}}^{i+1}\,=\,2M_{{}_{4Z}}X^{\top} \boldsymbol{y}+\big{(}\mathbf{I}_{p\times p}-2M_{{}_{4Z}}X^{\top}X\big{)} \widehat{\boldsymbol{\beta}}^{i}\\ =\,2X^{\top}\boldsymbol{y}-\widehat{\boldsymbol{\beta}}^{i}\,,\] so that \[\widehat{\boldsymbol{\beta}}^{1}\,=\,2X^{\top}\boldsymbol{y}- \widehat{\boldsymbol{\beta}}^{0}\,,\] \[\widehat{\boldsymbol{\beta}}^{2}\,=\,2X^{\top}\boldsymbol{y}- \widehat{\boldsymbol{\beta}}^{1}\,=\,2X^{\top}\boldsymbol{y}-(2X^{\top} \boldsymbol{y}-\widehat{\boldsymbol{\beta}}^{0})\,=\,\widehat{\boldsymbol{ \beta}}^{0}\,,\] \[\widehat{\boldsymbol{\beta}}^{3}\,=\,2X^{\top}\boldsymbol{y}- \widehat{\boldsymbol{\beta}}^{2}\,=\,2X^{\top}\boldsymbol{y}-\widehat{ \boldsymbol{\beta}}^{0}\,,\] \[\widehat{\boldsymbol{\beta}}^{4}\,=\,2X^{\top}\boldsymbol{y}- \widehat{\boldsymbol{\beta}}^{3}\,=\,2X^{\top}\boldsymbol{y}-(2X^{\top} \boldsymbol{y}-\widehat{\boldsymbol{\beta}}^{0})\,=\,\widehat{\boldsymbol{ \beta}}^{0}\,,\] \[\vdots\] This means that for the stated choice of surrogate inverse, the algorithm only converges if \(\widehat{\boldsymbol{\beta}}^{0}=X^{\top}\boldsymbol{y}\).

#### Solutions for Sect. 5.2

**Solution 5.3** The proofs of 1. and 2. are simple reformulations of the KKT conditions. The third part follows from the fact that \(r=0\) in the case of the least-squares estimator.

1. This indeed follows almost directly from the stated KKT conditions.

The terms of the KKT conditions can be rearranged to \[r\widehat{\boldsymbol{\kappa}}\,=\,2X^{\top}(\boldsymbol{y}-X\widehat{ \boldsymbol{\beta}})\,,\] which yields after multiplying through by \(M_{{}_{4Z}}\) that \[rM_{{}_{4Z}}\widehat{\boldsymbol{\kappa}}\,=\,2M_{{}_{4Z}}X^{\top}( \boldsymbol{y}-X\widehat{\boldsymbol{\beta}})\,.\]Using this and the linearity of matrix-vector multiplications then gives \[\widehat{\boldsymbol{\beta}}+rM_{\delta_{Z}}\widehat{\boldsymbol{\kappa}} = \widehat{\boldsymbol{\beta}}+2M_{\delta_{Z}}X^{\top}(\boldsymbol{y} -X\widehat{\boldsymbol{\beta}})\] \[= 2M_{\delta_{Z}}X^{\top}\boldsymbol{y}+\left(\mathsf{I}_{p\times p }-2M_{\delta_{Z}}X^{\top}X\right)\widehat{\boldsymbol{\beta}}\,,\] as desired.
2. This is a further reformulation of the equality in 1. We find \[\widehat{\boldsymbol{\beta}}+rM_{\delta_{Z}}\widehat{\boldsymbol{ \kappa}} = 2M_{\delta_{Z}}X^{\top}\boldsymbol{y}\] \[+\left(\mathsf{I}_{p\times p}-2M_{\delta_{Z}}X^{\top}X\right) \widehat{\boldsymbol{\beta}}\] Claim 1 \[\Leftrightarrow \widehat{\boldsymbol{\beta}}+rM_{\delta_{Z}}\widehat{\boldsymbol{ \kappa}} = 2M_{\delta_{Z}}X^{\top}(X\boldsymbol{\beta}+\boldsymbol{u})\] \[+\left(\mathsf{I}_{p\times p}-2M_{\delta_{Z}}X^{\top}X\right) \widehat{\boldsymbol{\beta}}\] linear model: \[\boldsymbol{y}=X\boldsymbol{\beta}+\boldsymbol{u}\] \[\Leftrightarrow \widehat{\boldsymbol{\beta}}-\boldsymbol{\beta} = -rM_{\delta_{Z}}\widehat{\boldsymbol{\kappa}}+2M_{\delta_{Z}}X^{ \top}\boldsymbol{u}\] \[+\left(2M_{\delta_{Z}}X^{\top}X-\mathsf{I}_{p\times p}\right)( \boldsymbol{\beta}-\widehat{\boldsymbol{\beta}})\] linearity of matrix-vector multiplications and rearranging terms \[\Leftrightarrow \mathbb{E}[\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}] = -\mathbb{E}[rM_{\delta_{Z}}\widehat{\boldsymbol{\kappa}}]+2 \mathbb{E}[M_{\delta_{Z}}X^{\top}\boldsymbol{u}]\] \[+\mathbb{E}[\left(2M_{\delta_{Z}}X^{\top}X-\mathsf{I}_{p\times p} \right)(\boldsymbol{\beta}-\widehat{\boldsymbol{\beta}})]\,.\] taking expectations on both sides; linearity of expectations Now, by the assumed symmetry of \(M_{\delta_{Z}}X^{\top}\boldsymbol{u}\) and by the linearity of expectations, \[2\mathbb{E}[M_{\delta_{Z}}X^{\top}\boldsymbol{u}] = \mathbb{E}[M_{\delta_{Z}}X^{\top}\boldsymbol{u}]+\mathbb{E}[-M_{ \delta_{Z}}X^{\top}\boldsymbol{u}]\] \[= \mathbb{E}[M_{\delta_{Z}}X^{\top}\boldsymbol{u}]-\mathbb{E}[M_{ \delta_{Z}}X^{\top}\boldsymbol{u}]\,=\,\boldsymbol{0}_{p}\,,\] and \(\mathbb{E}[(2M_{\delta_{Z}}X^{\top}X-\mathsf{I}_{p\times p})(\boldsymbol{ \beta}-\widehat{\boldsymbol{\beta}})]\approx\boldsymbol{0}_{p}\) by assumption. Plugging these two observations in the above display yields \[\mathbb{E}[\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}] \approx -\mathbb{E}[rM_{\delta_{Z}}\widehat{\boldsymbol{\kappa}}]\,,\] as desired.
3. The least-squares estimator corresponds to \(r=0\). Hence, \(rM_{\delta_{Z}}\widehat{\boldsymbol{\kappa}}=\boldsymbol{0}_{p}\), which implies that the Newton-Raphson update \(\widehat{\boldsymbol{\beta}}_{\mathrm{Is}}\mapsto\widehat{\boldsymbol{\beta} }_{\mathrm{Is}}+rM_{\delta_{Z}}\widehat{\boldsymbol{\kappa}}=\widehat{ \boldsymbol{\beta}}_{\mathrm{Is}}\) leaves _any_least-squares solution unchanged _irrespective_ of \(M_{\lambda_{Z}}\). This generalizes the observations in Examples 5.1.2 and 5.1.3 that the _specific_ least-squares solutions \(\widehat{\boldsymbol{\beta}}_{\text{ls}}=(X^{\top}X)^{-1}X^{\top}\boldsymbol{y}\) and \(\widehat{\boldsymbol{\beta}}_{\text{ls}}=(X^{\top}X)^{+}X^{\top}\boldsymbol{y}\), respectively, remain unchanged under the (generalized) Newton-Raphson algorithm with the _specific_ choices \(M_{\lambda_{Z}}=(X^{\top}X)^{-1}/2\) and \(M_{\lambda_{Z}}=(X^{\top}X)^{+}/2\), respectively.

The fact that \(rM_{\lambda_{Z}}\widehat{\boldsymbol{\kappa}}=\boldsymbol{0}_{p}\) also implies by linearity of expectations that under the conditions of 2., it holds that \(\mathbb{E}[\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}]=\boldsymbol{0}_{p}\) (which is commensurate with the results of 2.), that is, the least-squares estimator is unbiased.

**Solution 5.4**: The proof relies on the properties of standard Gauss random variables.

1. The proof of this equality is a simple calculation based on Property (i). By assumption on \(\boldsymbol{u}\), it holds that \(u_{1}/\sigma,\ldots,u_{n}/\sigma\) are independent standard Gauss random variables. Thus, \[\mathbb{E}\|\boldsymbol{u}\|_{2}^{2}\] \[= \mathbb{E}\sum_{i=1}^{n}u_{i}^{2}\] writing out the squared

\[\ell_{2}\] \[= \sigma^{2}\sum_{i=1}^{n}\mathbb{E}\big{[}(u_{i}/\sigma)^{2}\big{]}\] linearity of expectations and sums \[= \!\!\!n\sigma^{2}\mathbb{E}\big{[}(u_{1}/\sigma)^{2}\big{]}\] identically distributed (previous comment) \[= \!\!\!n\sigma^{2}\,,\qquad z:=u_{1}/\sigma\] is standard Gauss (previous comment); Property (i) as desired.
2. This proof is again straightforward but slightly more tedious. We find \[\mathbb{E}\Big{[}\big{(}\|\boldsymbol{u}\|_{2}^{2}-n\sigma^{2} \big{)}^{2}\Big{]}\] \[= \!\!\!\mathbb{E}\bigg{[}\bigg{(}\sum_{i=1}^{n}u_{i}^{2}-n\sigma^ {2}\bigg{)}^{2}\bigg{]}\] writing out

\[\|\boldsymbol{u}\|_{2}^{2}\]\[= \mathbb{E}\bigg{[}\bigg{(}\sum_{i=1}^{n}(u_{i}^{2}-\sigma^{2})\bigg{)}^ {2}\bigg{]}\] summarizing terms \[= \mathbb{E}\bigg{[}\sum_{i,j=1}^{n}(u_{i}^{2}-\sigma^{2})(u_{j}^{2} -\sigma^{2})\bigg{]}\] multiplying out the square \[= \sum_{i,j=1}^{n}\mathbb{E}\big{[}(u_{i}^{2}-\sigma^{2})(u_{j}^{2} -\sigma^{2})\big{]}\] linearity of expectations \[= \sum_{\begin{subarray}{c}i,j=1\\ i\neq j\end{subarray}}^{n}\mathbb{E}\big{[}(u_{i}^{2}-\sigma^{2})(u_{j}^{2}- \sigma^{2})\big{]}+\sum_{i=1}^{n}\mathbb{E}\big{[}(u_{i}^{2}-\sigma^{2})^{2} \big{]}\] linearity of sums \[= \sum_{\begin{subarray}{c}i,j=1\\ i\neq j\end{subarray}}^{n}\mathbb{E}\big{[}u_{i}^{2}-\sigma^{2}\big{]}\mathbb{E }\big{[}u_{j}^{2}-\sigma^{2}\big{]}+\sum_{i=1}^{n}\mathbb{E}\big{[}(u_{i}^{2} -\sigma^{2})^{2}\big{]}\] \[= \sigma^{4}\sum_{\begin{subarray}{c}i,j=1\\ i\neq j\end{subarray}}^{n}\big{(}\mathbb{E}\big{[}(u_{i}/\sigma)^{2}\big{]}-1 \big{)}\big{(}\mathbb{E}\big{[}(u_{j}/\sigma)^{2}\big{]}-1\big{)}\] \[+\sum_{i=1}^{n}\mathbb{E}\big{[}(u_{i}^{2}-\sigma^{2})^{2}\big{]} \quad\text{linearity of expectations and sums}\] \[= \sum_{i=1}^{n}\mathbb{E}\big{[}(u_{i}^{2}-\sigma^{2})^{2}\big{]} \mathbb{E}\big{[}(u_{i}/\sigma)^{2}\big{]}=1\text{ by Property (i)-- recall that}\] \[z:=u_{i}/\sigma\text{ is standard Gauss-distributed}\] \[= \sum_{i=1}^{n}\mathbb{E}\big{[}u_{i}^{4}-2\sigma^{2}u_{i}^{2}+ \sigma^{4}\big{]}\] expanding the square \[= \sigma^{4}\sum_{i=1}^{n}\big{(}\mathbb{E}\big{[}(u_{i}/\sigma)^{ 4}\big{]}-2\mathbb{E}\big{[}(u_{i}/\sigma)^{2}\big{]}+1\big{)}\] linearity of expectations and sums \[= \sigma^{4}\sum_{i=1}^{n}\big{(}3-2+1\big{)}\] Properties (ii) and (i) \[= 2n\sigma^{4}\,,\] consolidating as desired.

[MISSING_PAGE_EMPTY:6521]

[MISSING_PAGE_EMPTY:6522]

### Solutions for Chap. 6

#### Solutions for Sect. 6.2

**Solution 6.1** Claim 1 follows directly from Lemma 6.2.2 by noting that for the identity link function, \(\mathcal{G}^{\prime}[a]=1\) for all \(a\in\mathbb{R}\) (so that especially \(\mathcal{G}^{\prime}\big{[}|\!|y-X\widehat{\beta}|\!|_{2}^{2}\big{]}=1\)).

The first inequality of Claim 2 follows directly from Lemma 6.2.1 by setting \(\alpha=\beta\); the second inequality of Claim 2 is derived exactly as the inequality in Lemma 6.2.2 except for setting \(\alpha=\beta\) and taking the limit \(a\to 0\) in the end.

#### Solutions for Sect. 6.4

**Solution 6.4** All proofs center around Definition 6.4.1.

1. Swapping \(\mathcal{G}\) and \(\mathcal{G}^{\complement}\) just means swapping \(\mathcal{H}_{\mathcal{G}}\) and \(\mathcal{H}_{\mathcal{G}}\)-- see Definition 6.4.1.
2. Set \(k:=1\) and \(\mathcal{A}_{1}:=\{1,\ldots,p\}\).
3. For \(\mathcal{G}=\varnothing\) or \(\mathcal{G}=\{1,\ldots,p\}\), the edr function--just as any other non-negative function--is clearly decomposable: set \(\mathcal{H}_{\mathcal{G}}[\!|\alpha_{\mathcal{G}}|\!]:=0\), \(\mathcal{H}_{\mathcal{G}}\mathcal{E}[\!|\alpha_{\mathcal{G}}|\!]:=|\!|\alpha_ {\mathcal{G}}|\!|_{2}\) and \(\mathcal{H}_{\mathcal{G}}[\!|\alpha_{\mathcal{G}}|\!]:=|\!|\alpha_{\mathcal{ G}}|\!|_{2}\), \(\mathcal{H}_{\mathcal{G}}\mathcal{E}[\!|\alpha_{\mathcal{G}}|\!]:=0\), respectively.

For other \(\mathcal{S}\), instead, the edr function is not decomposable: If it were, then it must hold that \(\mathcal{H}_{\mathcal{G}}[\!|\alpha_{\mathcal{G}}|\!]=|\!|\alpha_{\mathcal{G}} |\!|_{2}\) and \(\mathcal{H}_{\mathcal{G}}\mathcal{E}[\!|\alpha_{\mathcal{G}}|\!]=|\!|\alpha_{ \mathcal{G}}\mathcal{E}|\!|_{2}\) (consider \(\alpha_{\mathcal{G}}\) variable, \(\alpha_{\mathcal{G}}\)\(=\)\(0\)\(|\!_{\mathcal{G}}\mathcal{E}|\!\) fixed and \(\alpha_{\mathcal{G}}\)\(\!\mathcal{E}\) variable, \(\alpha_{\mathcal{G}}\)\(=\)\(0\)\(|\!_{\mathcal{G}}\mathcal{E}|\!\) fixed, respectively). But then \(\mathcal{H}[\!|\alpha|\!]=|\!|\alpha|\!]_{2}\) and \(\mathcal{H}_{\mathcal{G}}[\!|\alpha_{\mathcal{G}}|\!]+|\!|\alpha_{\mathcal{G}} |\!|_{2}\) are different for many \(\alpha\in\mathbb{R}^{p}\), which contradicts the decomposability.

In contrast, the ridge estimator's squared \(\ell_{2}\)-norm can be written for every \(\mathcal{S}\) as

\[|\!|\alpha|\!|_{2}^{2}\,=\,\sum_{j=1}^{p}(a_{j})^{2}\,=\,\sum_{j\in\mathcal{S }}(a_{j})^{2}+\sum_{j\in\mathcal{S}}(a_{j})^{2}\,=\,|\!|\alpha_{\mathcal{S}} |\!|_{2}^{2}\,+\,|\!|\alpha_{\mathcal{G}}|\!|_{2}^{2}\,,\]

which makes it decomposable with subfunctions \(\mathcal{H}_{\mathcal{S}}\) : \(\alpha_{\mathcal{S}}\)\(\mapsto\)\(|\!|\alpha_{\mathcal{S}}|\!|_{2}^{2}\) and \(\mathcal{H}_{\mathcal{G}}\mathcal{E}\) : \(\alpha_{\mathcal{G}}\mathcal{E}\)\(\mapsto\)\(|\!|\alpha_{\mathcal{G}}|\!|_{2}^{2}\).
4. Set \(k:=p\) and \(\mathcal{A}_{j}:=\{j\}\).
5. Set \(\mathcal{H}_{\mathcal{S}}[\!|\alpha_{\mathcal{S}}|\!]:=|\!|\alpha_{\mathcal{S}} |\!|_{1}\) and \(\mathcal{H}_{\mathcal{G}}\mathcal{E}[\!|\alpha_{\mathcal{G}}|\!]:=|\!|\alpha_{ \mathcal{S}}\mathcal{E}|\!|_{1}\) and recall that \(|\!|\alpha|\!|_{1}=|\!|\alpha_{\mathcal{S}}|\!|_{1}+|\!|\alpha_{\mathcal{S}} \mathcal{E}|\!|_{1}\) by the definition of the \(\ell_{1}\)-norms.

6. One can prove this claim along the lines of the proof of 3. In particular, by the same arguments as above, the subfunctions of an \(\delta\)-decomposable group-lasso prior \(\hat{\alpha}\) must be \[\hat{\alpha}_{\delta}\ :\ \ \mathbf{a}_{\delta}\ \mapsto\ \sum_{j=1}^{k}|\mathbf{a}_{ \delta f/\alpha\delta}|_{2}\quad\text{and}\] \[\hat{\alpha}_{\delta}\mathfrak{C}\ :\ \ \mathbf{a}_{\delta}\mathfrak{C}\ \mapsto\ \sum_{j=1}^{k}|\mathbf{a}_{\delta f/\alpha\delta}|_{2}\,.\] The equality \(\hat{\alpha}[\mathbf{a}]=\hat{\alpha}_{\delta}[\mathbf{a}_{\delta}]+\hat{\alpha}_{ \delta}\mathfrak{C}[\mathbf{a}_{\delta}\mathfrak{C}]\), that is, \[\sum_{j=1}^{k}|\mathbf{a}_{\delta f}|_{2}=\sum_{j=1}^{k}\left(|\mathbf{a}_{\delta f/ \alpha\delta}|_{2}+|\mathbf{a}_{\delta f/\alpha\delta}|_{2}\right)\,,\] then holds if and only if either \(\delta f_{j}\subset\delta\) or \(\delta f_{j}\subset\delta^{\complement}\).

**Solution 6.5**: We prove the four statements in order.

1. The algebra in this proof is slightly tedious, but the educational result justifies the effort: \[\min_{\mathbf{a}\in\mathfrak{C}}\frac{|\mathbf{X}\mathbf{\delta}|_{2}^{2}}{n|\mathbf{\delta}|_{1}^{2}}\ =\ \min_{\mathbf{a}\in\mathfrak{C}}\frac{|\mathbf{X}_{(1,2)}\mathbf{\delta}_{(1,2)}|_{2}^{2}+| \mathbf{X}_{(1,3,4)}|_{2}^{2}}{n\left(|\mathbf{\delta}_{(1,2)}|_{1}+|\mathbf{\delta}_{(3,4)}|_{1}\right)^{2}}\] \[\text{assumed form of }\mathbf{X}/\sqrt{n};\] \[\text{decomposability of the }\mathfrak{C}_{2}^{2}\text{- and }\mathfrak{C}_{1}\text{-functions}\] \[=\ \min_{\mathbf{a}_{1,2}\in\mathfrak{C}^{2}}\min_{\mathbf{a}_{1,4}\in \mathfrak{C}^{2}}\frac{|\mathbf{X}_{(1,2)}\mathbf{\delta}_{(1,2)}|_{2}^{2}+|\mathbf{X}_{(1,3,4)}|_{3}|_{4}|_{2}^{2}}{n\left(|\mathbf{\delta}_{(1,2)}|_{1}+|\mathbf{\delta}_{(1,4)}|_{1}\right)^{2}}\] \[\text{separating the minimum into two parts}\] \[=\ \min_{\mathbf{a}_{1,2}\in\mathfrak{C}^{2}}\min_{\mathbf{a}_{1,4}\in \mathfrak{C}^{2}}\min_{\mathbf{a}_{1,4}\in\mathfrak{C}^{2}}\frac{|\mathbf{X}_{(1,2)} \mathbf{\delta}_{(1,2)}|_{2}^{2}}{n\left(|\mathbf{\delta}_{(1,2)}|_{1}+|\mathbf{\delta}_{(3,4)}|_{1}\right)^{2}}\] take \(\delta_{3}=-\delta_{4}\) for every fixed \(|\mathbf{\delta}_{(3,4)}|_{1}\); verify that \(\mathbf{\delta}\) with such \(\mathbf{\delta}_{(3,4)}\) exist in \(\mathfrak{C}\) for every \(\mathbf{\delta}_{(1,2)}\) \[=\ \min_{\mathbf{a}_{1,2}\in\mathfrak{C}^{2}}\min_{\mathbf{a}_{1,4}\in \mathfrak{C}^{2}}\frac{(\delta_{1}+\delta_{2}/2)^{2}+3\delta_{2}^{2}/4}{\mathbf{a}_{ \delta}\mathfrak{C}}\] \[\text{specific form of }\mathbf{X}/\sqrt{n}\]

[MISSING_PAGE_EMPTY:6525]

Details: Since \(\mathcal{S}\neq\varnothing\), it holds that \(\mathcal{G}\neq\varnothing\). However, for example, the set \(\{\boldsymbol{\delta}_{\{1,2\}}\in\mathbb{R}^{2}:\boldsymbol{\delta}\in\mathcal{ G}\}\) can well be empty for given \(\boldsymbol{\delta}_{\{3,4\}}\). This means that above (and similarly below), we take minima over empty sets. As a convention, we have set those minima to infinity: \(\min_{\boldsymbol{a}\in\varnothing}\mathcal{I}[\boldsymbol{a}]:=\infty\) for every function \(\ell\)--see Page 2. One can check that this is commensurate with our proof.
2. Again, the proof is a somewhat tedious exercise in linear algebra. We first observe that \[\min_{\boldsymbol{\delta}\in\mathcal{G}}\frac{\|X\boldsymbol{\delta}\|_{2}^{2 }}{n|\boldsymbol{\delta}\|_{1}^{2}} = \min_{\boldsymbol{\delta}\in\mathcal{G}}\frac{|\boldsymbol{\delta} _{\{1,2\}}|_{1}^{2}}{4\big{(}|\boldsymbol{\delta}|_{1}+|\boldsymbol{\delta}_{ \{3,4\}}|_{1}\big{)}^{2}}\] Claim 1 \[= \min_{\boldsymbol{\delta}\in\mathcal{G}}\min_{\begin{subarray}{c} \boldsymbol{\delta}_{\{1,2\}}|_{1}^{2}\\ \boldsymbol{\delta}\in\mathcal{G}\end{subarray}}\frac{|\boldsymbol{\delta}_{\{1,2\}}|_{1}^{2}}{4\big{(}|\boldsymbol{\delta}_{\{1,2\}}|_{1}+|\boldsymbol{ \delta}_{\{3,4\}}|_{1}\big{)}^{2}}\] \[= \min_{\boldsymbol{\delta}_{\{1,2\}}\in\mathbb{R}^{2}}\min_{ \begin{subarray}{c}\boldsymbol{\delta}_{\{0,4\}}\in\mathbb{R}^{2}\\ \boldsymbol{\delta}\in\mathcal{G}\end{subarray}}\frac{|\boldsymbol{\delta}_{\{1,2\}}|_{1}^{2}}{4\big{(}|\boldsymbol{\delta}_{\{1,2\}}|_{1}+|\boldsymbol{\delta }_{\{3,4\}}|_{1}\big{)}^{2}}\,.\] Separating the minimum into two parts We assume now \(\mathcal{S}=\{1\}\) (the case \(\mathcal{S}=\{2\}\) can be studied similarly). Then, since \(|\boldsymbol{\delta}_{\{\mathcal{G}\}}|_{1}\leq(v+1)|\boldsymbol{\delta}_{\{ \mathcal{G}\}}|_{1}/(v-1)\) for \(\boldsymbol{\delta}\in\mathcal{G}\), it holds for all \(\boldsymbol{\delta}\in\mathcal{G}\) that \((v+1)|\delta_{1}|\geq(v-1)(|\delta_{2}|+|\boldsymbol{\delta}_{\{3,4\}}|_{1})\). Since \(|\boldsymbol{\delta}_{\{3,4\}}|_{1}\geq 0\), it is sufficient to consider the outer minimum over \(\boldsymbol{\delta}_{\{1,2\}}\in\mathbb{R}^{2}\) such that (i) \((v+1)|\delta_{1}|\geq(v-1)|\delta_{2}|\). Next, by similar reasons and since \(\boldsymbol{0}_{4}\notin\mathcal{G}\) by definition, it is sufficient to consider (ii) \(\delta_{1}\neq 0\). Moreover, \(|\boldsymbol{\delta}_{\{\mathcal{G}\}}|_{1}\leq(v+1)|\boldsymbol{\delta}_{\{ \mathcal{G}\}}|_{1}/(v-1)\) for \(\boldsymbol{\delta}\in\mathcal{G}\) together with \(\mathcal{S}=\{1\}\) imply that \(|\boldsymbol{\delta}_{\{3,4\}}|_{1}\leq(v+1)|\delta_{1}|/(v-1)-|\delta_{2}|\). With the restrictions (i) and (ii) on \(\boldsymbol{\delta}_{\{1,2\}}\), the inner minimum in the last line of the display above is always over a non-empty set, and the minimum is attained if \(|\boldsymbol{\delta}_{\{3,4\}}|_{1}\) is maximally large; therefore, it is sufficient to consider the inner minimum over \(\boldsymbol{\delta}\) such that (iii) \(|\boldsymbol{\delta}_{\{3,4\}}|_{1}=(v+1)|\delta_{1}|/(v-1)-|\delta_{2}|\). Plugging (i)-(iii) into the above display and reformulating yields

\[\min_{\delta\in\mathcal{C}}\frac{|X\delta|_{2}^{2}}{n|\delta|_{1}^{2}}\]

\[=\min_{\begin{subarray}{c}\delta_{\{1,2\}}\in\mathbb{R}^{2}\\ (r+1)|\delta_{\{1\}}|\geq(r-1)|\delta_{2}|\\ \delta_{1}\neq 0\end{subarray}}\frac{|\delta_{\{1,2\}}|_{1}^{2}}{4\big{(}| \delta_{\{1,2\}}|_{1}+(r+1)|\delta_{1}|/(r-1)-|\delta_{2}|\big{)}^{2}}\]

(i), (ii), and (iii)

\[=\min_{\begin{subarray}{c}\delta_{\{1,2\}}\in\mathbb{R}^{2}\\ (r+1)|\delta_{\{1\}}|\geq(r-1)|\delta_{2}|\\ \delta_{1}\neq 0\end{subarray}}\frac{\big{(}|\delta_{1}|+|\delta_{2}|\big{)}^{2}} {4\big{(}2r|\delta_{1}|/(r-1)\big{)}^{2}}\]

definition of the \(\ell_{1}\)-norm

\[=\min_{\delta_{1}\in\mathbb{R}\setminus\{0\}}\min_{\begin{subarray}{c}\delta_{ \geqslant}\in\mathbb{R}\\ (r+1)|\delta_{\{1\}}|\geq(r-1)|\delta_{2}|\end{subarray}}\frac{\big{(}|\delta_ {1}|+|\delta_{2}|\big{)}^{2}}{4\big{(}2r|\delta_{1}|/(r-1)\big{)}^{2}}\]

separating the minimum into two parts

\[=\min_{\delta_{1}\in\mathbb{R}\setminus\{0\}}\frac{|\delta_{1}|^{2}}{4\big{(}2 r|\delta_{1}|/(r-1)\big{)}^{2}}\]

setting \(\delta_{2}=0\)

\[=\frac{(r-1)^{2}}{16r^{2}}\,,\] simplifying

as desired.
3. Similarly as above, \[\min_{\delta\in\mathcal{C}}\frac{|X\delta|_{2}^{2}}{|\delta|_{1}^{2}}\] \[=\min_{\delta\in\mathcal{C}}\frac{|\delta_{\{1,2\}}|_{1}^{2}}{4 |\delta|_{1}^{2}}\] Claim 1 \[=\min_{\delta\in\mathcal{C}}\frac{|\delta_{\{1,2\}}|_{1}^{2}}{4 \big{(}|\delta_{\{1,2\}}|_{1}+|\delta_{\{3,4\}}|_{1}\big{)}^{2}}\] \[=\min_{\delta_{\{1,2\}}\in\mathbb{R}^{2}}\min_{\begin{subarray}{c} \delta_{\{3,4\}}\in\mathbb{R}^{2}\\ \delta\in\mathcal{C}\end{subarray}}\frac{|\delta_{\{1,2\}}|_{1}^{2}}{4\big{(}| \delta_{\{1,2\}}|_{1}+|\delta_{\{3,4\}}|_{1}\big{)}^{2}}\] separating the minimum into two parts \[=\min_{\delta_{\{1,2\}}\in\mathbb{R}^{2}\setminus\{\emptyset_{2}\}}\frac{| \delta_{\{1,2\}}|_{1}^{2}}{4\big{(}|\delta_{\{1,2\}}|_{1}+(r+1)|\delta_{\{1,2 \}}|_{1}/(r-1)\big{)}^{2}}\] same argument as in 2. above \[= \min_{\boldsymbol{\delta}_{\{1,2\}}\in\mathbb{R}^{2}\setminus\{ \boldsymbol{\theta}_{2}\}}\frac{1}{4\big{(}1+(v+1)/(v-1)\big{)}^{2}}\] \[= \frac{(v-1)^{2}}{16v^{2}}\,,\] simplifying as desired.
4. We approach this again via 1. Since the argument of the minimum of 1.'s right-hand side is non-negative for all \(\boldsymbol{\delta}\), it suffices to show that there is a \(\boldsymbol{\delta}\in\mathcal{C}\) such that (i) \(|\boldsymbol{\delta}_{\{1,2\}}|_{1}^{2}=0\) and (ii) \(4|\boldsymbol{\delta}|_{1}^{2}>0\).

For this, we assume without loss of generality that \(\{3\}\subset\mathcal{S}\) and define \(\boldsymbol{\delta}:=(0,0,1,0)^{\top}\). Then, \(|\boldsymbol{\delta}_{\mathcal{S}}|_{1}=0\leq(v+1)/(v-1)=(v+1)|\boldsymbol{ \delta}_{\mathcal{S}}|_{1}/(v-1)\), that is, \(\boldsymbol{\delta}\in\mathcal{C}\), and one can also check that (i) and (ii) are indeed satisfied.

Details: Alternatively, one can start directly from 1.'s left-hand side and take \(\boldsymbol{\delta}:=(0,0,1,-1)^{\top}\).

### Solutions for \(\blacktriangleright\) Chap. 7

Solutions for \(\blacktriangleright\) Sect. 7.2

**Solution 7.1** We prove the two statements in order.

1. The proof of the first inequality is based on the Holder inequality; the proof of the second inequality is just basic algebra. _First inequality:_ If \(k<l<\infty\), we find for every \(\boldsymbol{c}\in\mathbb{R}^{p}\), \[|\boldsymbol{c}|_{k}^{k} = \sum_{j=1}^{p}|c_{j}|^{k}\] definition of the \[\ell_{q}\] -norm \[= \sum_{j=1}^{p}\big{(}|c_{j}|^{k}\cdot 1\big{)}\] multiplying with a one-valued factor \[= \langle\boldsymbol{a},\,\boldsymbol{b}\rangle\] where \[\boldsymbol{a}:=(|c_{1}|^{k},\ldots,|c_{p}|^{k})^{\top}\] and \[\boldsymbol{b}:=(1,\ldots,1)^{\top}\] \[\leq |\boldsymbol{a}|_{l/k}|\boldsymbol{b}|_{1/(1-k/l)}\] Holder inequality (2.6) applied with \[l\leftrightarrow l/k\] and \[m\leftrightarrow 1/(1-k/l)\]\[= \Big{(}\sum_{j=1}^{p}\bigl{|}|c_{j}|^{k}\bigl{|}^{l/k}\bigr{)}^{k/l} \Big{(}\sum_{j=1}^{p}|1|^{1/(1-k/l)}\Big{)}^{1-k/l}\] \[= \Big{(}\sum_{j=1}^{p}|c_{j}|^{l}\Big{)}^{k/l}p^{1-k/l}\] simplifying \[= p^{1-k/l}|c|c|_{l}^{k}\,.\qquad\text{definition of the $\ell_{q}$-norm; rearranging}\]

Taking the \(k\)th root on both sides yields

\[|c|_{k} \leq p^{1/k-1/l}|c|_{l}\,,\]

which proves the first inequality.

If \(k<l=\infty\), we find for every \(c\in\mathbb{R}^{p}\),

\[|c|_{k}^{k} = \sum_{j=1}^{p}|c_{j}|^{k}\] definition of the \[\ell_{q}\] -norm \[\leq \sum_{j=1}^{p}\Big{(}\max_{j\in\{1,\ldots,p\}}\bigl{\{}|c_{j}| \bigr{\}}\Big{)}^{k}\] \[= p\cdot\Big{(}\max_{j\in\{1,\ldots,p\}}\bigl{\{}|c_{j}|\bigr{\}} \Big{)}^{k}\] evaluating the sum \[= p^{1-k/l}|c|_{l}^{k}\,,\quad k/\infty=0\text{ by the conventions on Pages X; definition of the $\ell_{q}$-norm}\]

which--again after taking the \(k\)th root--proves the first inequality.

_Second inequality_: If \(l\leq k<\infty\), we first find that for each \(c\in\mathbb{R}^{p}\), it holds by the definition of the \(\ell_{q}\)-norm that

\[|c_{j}|\,=\,\bigl{(}|c_{j}|^{l}\bigr{)}^{1/l}\ \leq\ \bigg{(}\sum_{i=1}^{p}|c_{i}|^{l} \bigg{)}^{1/l}\,=\,\|c|_{l}\,.\]

We then use this to find for each \(c\in\mathbb{R}^{p}\setminus\{\mathbf{0}_{p}\}\) (the case \(c=\mathbf{0}_{p}\) follows from the linearity of norms)

\[|c|_{k}^{k}\,=\,|c|_{l}^{k}\cdot|c|/|c|_{l}|_{k}^{k}\] scalability of norms;

\[c\neq\mathbf{0}_{p}\]\[= \left\|\mathbf{c}\right\|_{l}^{k}\cdot\sum_{j=1}^{p}\left(\left|c_{j} \right|/\left\|\mathbf{c}\right\|_{l}\right)^{k}\] definition of the \[\ell_{q}\] norm \[\leq \left\|\mathbf{c}\right\|_{l}^{k}\cdot\sum_{j=1}^{p}\left(\left|c_{j} \right|/\left\|\mathbf{c}\right\|_{l}\right)^{l}\] \[= \left\|\mathbf{c}\right\|_{l}^{k}\cdot\Big{(}\sum_{j=1}^{p}\left|c_{j }\right|^{l}\Big{)}/\left\|\mathbf{c}\right\|_{l}^{l}\] linearity of sums and exponentiation rules \[= \left\|\mathbf{c}\right\|_{l}^{k}\cdot\left\|\mathbf{c}\right\|_{l}^{l}/ \left\|\mathbf{c}\right\|_{l}^{l}\] definition of the \[\ell_{q}\] norm \[= \left\|\mathbf{c}\right\|_{l}^{k}.\] consolidating Taking the \[k\] th root on both sides yields \[\left\|\mathbf{c}\right\|_{k} \leq \left\|\mathbf{c}\right\|_{l},\] which proves the second inequality. The case \[l=k=\infty\] should be obvious. If \[l<k=\infty\], it holds for every \[\mathbf{c}\in\mathbb{R}^{p}\], \[\left\|\mathbf{c}\right\|_{k} = \max_{j\in\{1,\ldots,p\}}\left|c_{j}\right|\] \[= \left(\max_{j\in\{1,\ldots,p\}}\left|c_{j}\right|^{l}\right)^{1/l}\] monotonicity of the maximum \[\leq \Big{(}\sum_{j=1}^{p}\left|c_{j}\right|^{l}\Big{)}^{1/l}\] sum of non-negative terms \[= \left\|\mathbf{c}\right\|_{l},\] definition of the \[\ell_{q}\] norm which proves the second inequality.
2. The conclusion follows directly from combining the second inequality of 1. (with \(k=q\) and \(l=1\)) and the last bound in Example 7.2.1.

### Solutions for \(\blacktriangleright\) Sect. 7.3

**Solution 7.3** The proof is a short exercise in convex analysis and linear algebra.

[MISSING_PAGE_EMPTY:6531]

Exercise 2.10, the unique solution of the restricted lasso problem is \(\widehat{\gamma}_{\delta}=x^{\top}\mathbf{y}/n-r/(2n)>0\).

Combining this observation with _(Dual 1)_ yields

\[\widehat{\gamma}_{\delta} = 2\mathbf{x}^{\top}\Big{(}\mathbf{y}-\big{(}\mathbf{x}^{\top}\mathbf{y}/n-r/(2n) \big{)}\mathbf{x}\Big{)}/r\] \[\text{plugging the restricted lasso solution into (\emph{Dual 1})}\] \[= 2\mathbf{x}^{\top}\mathbf{y}/r-2\mathbf{x}^{\top}\mathbf{y}/r\cdot\mathbf{x}^{\top} \mathbf{x}/n+\mathbf{x}^{\top}\mathbf{x}/n\qquad\text{expanding}\] \[= 1\,,\qquad\qquad\qquad\qquad\qquad\mathbf{x}^{\top}\mathbf{x}=\mathbf{x};\, \text{consolidating}\]

and similarly, using _(Dual 2)_ yields \(\widehat{\gamma}_{\delta}\varepsilon=1\). Hence, comparing to Example 2.5.2, we find that \(\widehat{\mathbf{\nu}}=(\widehat{\gamma}_{\delta},\widehat{\gamma}_{\delta})^{\top }\in\mathfrak{a}\|\widehat{\mathbf{\nu}}\|_{1}\), and then comparing to Example 2.5.3, we find that \(\widehat{\mathbf{\nu}}\) is a solution of the full lasso problem. In other words, the primal-dual construction was successful.

Details: The irrepresentability condition in Assumption 7.3.1 is not satisfied, though: since \(\widehat{\gamma}_{\delta}>0\), it holds that \(a:=1\in\mathfrak{a}\|\widehat{\gamma}_{\delta}\|_{1}\), and therefore,

\[\|X_{\delta}\varepsilon^{\top}X_{\delta}(X_{\delta}{}^{\top}X_{\delta})^{-1}a \|_{\infty}\;=\;|n\cdot(1/n)\cdot 1\|_{\infty}\;=\;1\,.\]

On the other hand, one can check readily that \((0,\widehat{\gamma}_{\delta})^{\top}\) is yet another solution of the full lasso problem, which means that the primal vector \(\widehat{\mathbf{\nu}}\) is not the unique solution of the full problem.
3. This proof is a bit more challenging. However, it is still mainly based on convexity arguments.

We first observe that a sufficient condition for the claim to hold is \(\text{supp}[\widehat{\mathbf{\beta}}_{\text{lasso}}]\subset\,\mathcal{S}\) for all lasso solutions

\[\widehat{\mathbf{\beta}}_{\text{lasso}}\;\in\;\operatorname*{arg\,min}_{\mathbf{ \alpha}\in\mathbb{R}\rho}\big{\{}\|\mathbf{y}-X\mathbf{\alpha}\|_{2}^{2}+r\|\mathbf{\alpha }\|_{1}\big{\}}\,.\]

Indeed, if \(\text{supp}[\widehat{\mathbf{\beta}}_{\text{lasso}}]\subset\mathcal{S}\), we can write \(\widehat{\mathbf{\beta}}=(\widehat{\mathbf{\gamma}}_{\delta}{}^{\top},\mathbf{0}_{\rho-s} {}^{\top})^{\top}\) with \(\widehat{\mathbf{\gamma}}_{\delta}\) unique as discussed in Claim 1.

We then proceed in five steps.

_Step 1:_ We show that the primal-dual construction was successful, which implies that \(\widehat{\mathbf{\nu}}\in\mathfrak{a}\|\widehat{\mathbf{\nu}}\|_{1}\) and

\[\|\mathbf{y}-X\widehat{\mathbf{\beta}}_{\text{lasso}}\,|_{2}^{2}+r|\widehat{\mathbf{\beta }}_{\text{lasso}}\,\|_{1}\;=\;\|\mathbf{y}-X\widehat{\mathbf{\nu}}\|_{2}^{2}+r|\widehat {\mathbf{\nu}}\|_{1}\]

for every lasso solution \(\widehat{\mathbf{\beta}}_{\text{lasso}}\).

The fact (i) \(\widehat{\boldsymbol{\tau}}_{\mathcal{S}}\in\mathfrak{a}\|\widehat{\boldsymbol{ \gamma}}_{\mathcal{S}}\|_{1}\) follows as in the proof of Lemma 7.3.1 (use \(r\in(0,\infty)\)); the fact (ii) \(\widehat{\boldsymbol{\nu}}_{\mathcal{S}}\in\mathfrak{a}\|\widehat{\boldsymbol{ \gamma}}_{\mathcal{S}}\|_{1}\) follows from the dual feasibility condition \(\|\widehat{\boldsymbol{\nu}}_{\mathcal{S}}\|_{\infty}\leq 1\) that is implied by the stipulated strict dual feasibility condition \(|\widehat{\boldsymbol{\nu}}_{\mathcal{S}}\|_{\infty}<1\) and from _(Primal 2)_ (which ensures \(\widehat{\boldsymbol{\gamma}}_{\mathcal{S}}\in\mathfrak{a}_{p-s}\)) and Example 2.5.2 (which states that \(\mathfrak{a}\|\mathfrak{0}_{p-s}\|_{1}=\{\boldsymbol{b}\in\mathbb{R}^{p-s}:\| \boldsymbol{b}\|_{\infty}\leq 1\}\)). In view of Example 2.5.3, these two facts ensure that the primal-dual construction was successful, which is expressed by the two statements.

_Step 2:_ We now rearrange the equality of Step 1 to

\[r|\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{1}-r(\widehat{ \boldsymbol{\sigma}},\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})\] \[=\,|\boldsymbol{v}-X\widehat{\boldsymbol{\gamma}}|_{2}^{2}+\langle- 2X^{\top}(\boldsymbol{y}-X\widehat{\boldsymbol{\gamma}}),\;\widehat{ \boldsymbol{\beta}}_{\mathrm{lasso}}-\widehat{\boldsymbol{\gamma}}\rangle-| \boldsymbol{v}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{2}^{2}\,.\]

The quantities on the right-hand side will be upper-bounded by zero in Step 3; the quantities on the left-hand side will then be analyzed further in Steps 4 and 5.

Since \(\widehat{\boldsymbol{v}}\in\mathfrak{a}\|\widehat{\boldsymbol{\gamma}}\|_{1}\) according to Step 1, Example 2.5.2 yields for all \(j\in\{1,\ldots,p\}\)

\[\widehat{\boldsymbol{\nu}}_{j}\widehat{\boldsymbol{\gamma}}\;=\;\left\{\begin{array}{ ll}\mathrm{sign}[\widehat{\boldsymbol{\gamma}}_{j}]\widehat{\boldsymbol{\gamma}}_{j}\,=\,|\widehat{\boldsymbol{\gamma}}_{j}|&\text{if }j\in\mathrm{supp}[\widehat{\boldsymbol{\gamma}}]\,;\\ \widehat{\boldsymbol{\gamma}}_{j}\cdot 0\,=\,0\,=\,|\widehat{\boldsymbol{\gamma}}_{j}|&\text{if }j\notin\mathrm{supp}[\widehat{\boldsymbol{\gamma}}]\,.\end{array}\right.\]

Hence, \(|\widehat{\boldsymbol{\gamma}}\|_{1}=\sum_{j=1}^{p}|\widehat{\boldsymbol{ \gamma}}_{j}|=\sum_{j=1}^{p}\widehat{\boldsymbol{\gamma}}_{j}\widehat{ \boldsymbol{\gamma}}_{j}=\langle\widehat{\boldsymbol{v}},\;\widehat{ \boldsymbol{\gamma}}\rangle\).

Plugging this observation into the equality derived in Step 1 gives

\[|\boldsymbol{v}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{2}^{2}+r| \widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{1}\,=\,|\boldsymbol{v}-X \widehat{\boldsymbol{\gamma}}\|_{2}^{2}+r(\widehat{\boldsymbol{v}},\;\widehat{ \boldsymbol{\gamma}})\,.\]

We then subtract \(r(\widehat{\boldsymbol{v}},\;\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})\) from both sides and use the linearity of the inner product to obtain

\[|\boldsymbol{v}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{2}^{2}+r| \widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{1}-r(\widehat{\boldsymbol{v}},\; \widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})\\ =\,|\boldsymbol{v}-X\widehat{\boldsymbol{\gamma}}|_{2}^{2}+\langle r \widehat{\boldsymbol{v}},\;\widehat{\boldsymbol{\gamma}}-\widehat{\boldsymbol{ \beta}}_{\mathrm{lasso}}\rangle\,.\]

_(Primal 2)_ and _(Dual 1&2)_ taken together yield \(r\widehat{\boldsymbol{v}}=2X^{\top}(\boldsymbol{y}-X\widehat{\boldsymbol{ \gamma}})\), so that we can further deduce from the premultimate display that

\[|\boldsymbol{y}-X\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{2}^{2}+r| \widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\|_{1}-r(\widehat{\boldsymbol{v}},\; \widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})\\ =\,|\boldsymbol{y}-X\widehat{\boldsymbol{\gamma}}|_{2}^{2}+\langle 2X^{\top}(\boldsymbol{y}-X\widehat{\boldsymbol{\gamma}}),\;\widehat{ \boldsymbol{\gamma}}-\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\rangle\,.\]

[MISSING_PAGE_FAIL:315]

Since the primal-dual witness construction was successful according to Step 1, Example 2.5.3 implies \(|\overline{\boldsymbol{v}}\|_{\infty}\leq 1\). Using this and the Holder inequality gives

\[\begin{array}{l}(\widehat{\boldsymbol{v}},\;\widehat{\boldsymbol{\beta}}_{ \mathrm{lasso}})\\ =\;\langle\widehat{\boldsymbol{v}}_{\mathcal{G}},\;(\widehat{\boldsymbol{ \beta}}_{\mathrm{lasso}})_{\mathcal{G}}\rangle+\langle\widehat{\boldsymbol{v}} _{\mathcal{G}},\;(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G}} \rangle\\ \text{decomposability of the inner product}\\ \leq\;|\widehat{\boldsymbol{v}}_{\mathcal{G}}|_{\infty}|(\widehat{ \boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G}}|_{1}+\langle\widehat{ \boldsymbol{v}}_{\mathcal{G}},\;(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}} )_{\mathcal{G}}\mathfrak{C}\rangle\\ \text{H\"{o}lder inequality (\ref{eq:Holder}) applied to the first term}\\ \leq\;|(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G}}|_{1}+ \langle\widehat{\boldsymbol{v}}_{\mathcal{G}},\;(\widehat{\boldsymbol{\beta }}_{\mathrm{lasso}})_{\mathcal{G}}\mathfrak{C}\rangle\;,\\ |\widehat{\boldsymbol{v}}|_{\infty}\leq 1\text{ as mentioned above}\end{array}\]

so that together with Step 3

\[\begin{array}{l}\|(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G }}\mathfrak{C}\|_{1}\\ =\;|\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}|_{1}-\|(\widehat{\boldsymbol{ \beta}}_{\mathrm{lasso}})_{\mathcal{G}}\|_{1}\\ \leq\;\langle\widehat{\boldsymbol{v}},\;\widehat{\boldsymbol{\beta}}_{\mathrm{ lasso}}\rangle-\|(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G}}\|_{1}\\ \|\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}l_{1}\leq\langle\widehat{ \boldsymbol{v}},\;\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}\rangle\text{ by Step 3}\\ \leq\;|(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G}}|_{1}+ \langle\widehat{\boldsymbol{v}}_{\mathcal{G}},\;(\widehat{\boldsymbol{\beta }}_{\mathrm{lasso}})_{\mathcal{G}}\mathfrak{C}\rangle-\|(\widehat{\boldsymbol{ \beta}}_{\mathrm{lasso}})_{\mathcal{G}}\|_{1}\\ \text{ }(\widehat{\boldsymbol{v}},\;\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}} )\leq\|(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G}}\mathfrak{ L}_{1}+\langle\widehat{\boldsymbol{v}}_{\mathcal{G}},\;(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{ \mathcal{G}}\mathfrak{C}\rangle\\ \text{ by the foregoing display}\\ \leq\;\langle\widehat{\boldsymbol{v}}_{\mathcal{G}}\mathfrak{C},\;(\widehat{ \boldsymbol{\beta}}_{\mathrm{lasso}})_{\mathcal{G}}\mathfrak{C}\rangle\;,\\ \text{consolidating}\end{array}\]

as desired.

_Step 5:_ We finally derive from Step 4 that

\[\mathrm{supp}[\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}]\;\subset\; \mathcal{S}.\]

This inclusion implies Claim 3 as explained at the beginning.

This final step is the only part where the _strict_ dual feasibility condition \(|\widehat{\boldsymbol{v}}_{\mathcal{G}}\mathfrak{C}|_{\infty}=\max_{j\in \mathcal{G}}|(\widehat{\boldsymbol{v}}_{\mathcal{G}}\mathfrak{C})_{j}|<1\) is invoked. It entails for all \(j\in\mathcal{G}\mathfrak{C}\)

\[\widehat{v}_{j}\cdot(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{j}\; \left\{\begin{array}{lcl}<&|(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_ {j}|&\text{ if }j\in\mathrm{supp}[\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}] \,;\\ =&0=\;|(\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}})_{j}|&\text{ if }j\notin\mathrm{supp}[\widehat{\boldsymbol{\beta}}_{\mathrm{lasso}}].\end{array}\right.\]Consequently, if \(\text{supp}[\widehat{\boldsymbol{\beta}}_{\text{lasso}}]\not\subset\mathcal{S}\), that is, \(\delta^{\mathbf{C}}\cap\text{supp}[\widehat{\boldsymbol{\beta}}_{\text{lasso}}]\not=\varnothing\), then

\[(\widehat{\boldsymbol{\nu}}_{\delta^{\mathbf{C}}},\,\,(\widehat{ \boldsymbol{\beta}}_{\text{lasso}})_{\delta^{\mathbf{C}}})\] \[=\,\,\sum_{j\in\delta^{\mathbf{C}}}\widehat{\nu}_{j}\cdot( \widehat{\boldsymbol{\beta}}_{\text{lasso}})_{j}\quad\text{ definition of the standard inner product}\] \[<\,\,\sum_{j\in\delta^{\mathbf{C}}}|(\widehat{\boldsymbol{\beta}}_{ \text{lasso}})_{j}|\] \[\text{ previous display and assumption supp}[\widehat{\boldsymbol{\beta}}_{\text{lasso}}]\not\subset\mathcal{S}\] \[=\,\,|(\widehat{\boldsymbol{\beta}}_{\text{lasso}})_{\delta^{ \mathbf{C}}}\|_{1}\,,\qquad\qquad\qquad\qquad\text{ definition of the $\ell_{1}$-norm}\]

which contradicts the result of Step 4. Hence, \(\text{supp}[\widehat{\boldsymbol{\beta}}_{\text{lasso}}]\subset\mathcal{S}\), as desired.

This concludes the proof of Claim 3.

[MISSING_PAGE_EMPTY:6537]

\[\leq\ 2^{c}\Big{(}\frac{1}{2}\big{|}|a|\big{|}^{c}+\frac{1}{2}\big{|}|b| \big{|}^{c}\Big{)}\] above-derived convexity \[=\ 2^{c-1}|a|^{c}+2^{c-1}|b|^{c}\,,\] simplification as desired.

We then look at the case \(a,b\in[0,\infty)\), \(c\in(0,1]\). One can check readily the fact that the stated inequality holds for \(a+b=0\), so that we can assume \(a+b>0\) in the following. We then get

\[1 =\ \frac{a+b}{a+b}\] basic algebra \[=\ \frac{a}{a+b}+\frac{b}{a+b}\qquad\text{separating the fraction into two parts}\] \[\leq\ \left(\frac{a}{a+b}\right)^{c}+\left(\frac{b}{a+b}\right)^{ c}\,,\qquad\qquad a,b\leq a+b;c\leq 1\]

which can be brought in the desired form by multiplying all terms with \((a+b)^{c}\). 

**Proposition 2.1.2**: _For all \(a,b\in\mathbb{R}^{p}\), it holds that_

\[|a+b|_{2}^{2}=\ |a|_{2}^{2}+2(a,b)+|b|_{2}^{2}\,.\]

Proof of Lemma 2.1.2.: The proof is a simple application of the definitions of the \(\ell_{2}\)-norm on Page 2.1.2 and the standard inner product on Page 2.1.2:

\[\|a+b\|_{2}^{2}=\ \sum_{j=1}^{p}(a_{j}+b_{j})^{2}=\ \sum_{j=1}^{p} \big{(}a_{j}^{2}+2a_{j}b_{j}+b_{j}^{2}\big{)}\] \[=\ \sum_{j=1}^{p}a_{j}^{2}+2\sum_{j=1}^{p}a_{j}b_{j}+\sum_{j=1}^{ p}b_{j}^{2}\ =\ |a|_{2}^{2}+2(a,b)+|b|_{2}^{2}\,.\]

**Lemma B.1.3**.: _We derive_

\[0 \leq \|\sqrt{2/c}\,\boldsymbol{a}-\sqrt{c/2}\,\boldsymbol{b}\|_{2}^{2}\] \[= \sum_{j=1}^{p}\big{(}\sqrt{2/c}\,a_{j}-\sqrt{c/2}\,\boldsymbol{b} _{j}\big{)}^{2}\] \[= \sum_{j=1}^{p}\big{(}2/c\cdot a_{j}^{2}+c/2\cdot\boldsymbol{b}_{ j}^{2}-2a_{j}\boldsymbol{b}_{j}\big{)}\] \[= 2/c\cdot\boldsymbol{|a|}_{2}^{2}+c/2\cdot\boldsymbol{|b|}_{2}^{2 }-2\langle\boldsymbol{a},\,\boldsymbol{b}\rangle\,.\]

_Rearranging then yields_

\[\langle\boldsymbol{a},\,\boldsymbol{b}\rangle\,\leq\,\frac{\|\boldsymbol{a}\|_ {2}^{2}}{c}+\frac{c\|\boldsymbol{b}\|_{2}^{2}}{4}\]

_as desired for the first part._

_For the second part, we find for all \(c,c^{\prime},d\,\in\,(0,\,\infty)\), \(c^{\prime}=dc\),_

\[d\langle\boldsymbol{a},\,\boldsymbol{b}\rangle \leq d\Big{(}\frac{\|\boldsymbol{a}\|_{2}^{2}}{c^{\prime}}+\frac{c^{ \prime}\|\boldsymbol{b}\|_{2}^{2}}{4}\Big{)}\] \[\leq \frac{\|\boldsymbol{a}\|_{2}^{2}}{c}+\frac{d^{2}c\|\boldsymbol{b }\|_{2}^{2}}{4}\,,\] \[c^{\prime}=dc\text{ by assumption}\]

_as desired._The right-hand inequalities finally follow from setting \(\boldsymbol{a}\rightarrow-\boldsymbol{a}\). 

### Matrix Algebra

In this section, we review matrices and their properties. Example matrices that can accompany the below discussions are in Table 1.

### Basic Definitions

Matrices over the real values generalize single numbers to rectangular arrays of numbers. Specifically, a matrix \(A\in\mathbb{R}^{n\times p}\) is an array that consists of \(n\) rows and \(p\) columns, which amounts to \(n\cdot p\) numbers in total. We call these individual numbers the _elements_ of the matrix and denote them by \(A_{ij}\), where \(i\in\{1,\ldots,n\}\) identifies the row and \(j\in\{1,\ldots,p\}\) the column:

\[A\;=\;\begin{pmatrix}A_{11}&\cdots&A_{1p}\\ \vdots&\ddots&\vdots\\ A_{n1}&\cdots&A_{np}\end{pmatrix}\;.\]

For example, the last matrix in the first line of Table 1 has three rows and two columns with a total of six entries, and \(A_{22}=1\) while \(A_{32}=0\). Special cases of matrices are numbers \(A=a\in\mathbb{R}^{1\times 1}=\mathbb{R}\) and (column) vectors \(A=\boldsymbol{a}\in\mathbb{R}^{p\times 1}=\mathbb{R}^{p}\).

Addition and multiplication generalize from numbers to matrices as follows: The _sum of two matrices_\(A,B\in\mathbb{R}^{n\times p}\) is the matrix \(A+B\in\mathbb{R}^{n\times p}\) with elements \((A+B)_{ij}=A_{ij}+B_{ij}\) for all \(i\in\{1,\ldots,n\},j\in\{1,\ldots,p\}\). The difference of two matrices \(A,B\in\mathbb{R}^{n\times p}\) is then \(A-B:=A+(-B)\), where \(-B\in\mathbb{R}^{p\times p}\) is defined through \((-B)_{ij}:=-B_{ij}\). Observe that the two matrices must have the same dimensions. The _product of two matrices_\(A\in\mathbb{R}^{n\times p},B\in\mathbb{R}^{p\times q}\) is the matrix \(AB\in\mathbb{R}^{n\times q}\) with elements \((AB)_{ij}=\sum_{k=1}^{p}(A_{ik}B_{kj})\) for \(i\in\{1,\ldots,n\},j\in\{1,\ldots,q\}\). Observe that the number of columns of the first matrix must be equal to the number of rows of the second matrix. One can check readily that addition and multiplication for matrices have similarly properties as for real-valued numbers--except commutation: \(AB\) is not necessarily \(BA\).

[MISSING_PAGE_EMPTY:6541]

The _multiplication of a matrix \(A\in\mathbb{R}^{n\times p}\) with a scalar \(b\in\mathbb{R}\)_ is the matrix \(bA\in\mathbb{R}^{n\times p}\) defined through \((bA)_{ij}:=bA_{ij}\) for \(i\in\{1,\ldots,n\},j\in\{1,\ldots,p\}\). Three examples for addition and multiplication are

\[\begin{pmatrix}1&1\\ 1&2\\ 1&3\end{pmatrix}+\begin{pmatrix}2&-4\\ 2&-5\\ 2&-6\end{pmatrix}+\begin{pmatrix}3&7\\ 3&8\\ 3&9\end{pmatrix}=\begin{pmatrix}1+2+3&1-4+7\\ 1+2+3&2-5+8\\ 1+2+3&3-6+9\end{pmatrix}=\begin{pmatrix}6&4\\ 6&5\\ 6&6\end{pmatrix}\,.\]

\[\begin{pmatrix}1&2&3\\ 4&5&6\end{pmatrix}\cdot\begin{pmatrix}0.1\\ 0.2\\ 0.3\end{pmatrix}=\begin{pmatrix}1\cdot 0.1+2\cdot 0.2+3\cdot 0.3\\ 4\cdot 0.1+5\cdot 0.2+6\cdot 0.3\end{pmatrix}=\begin{pmatrix}1.4\\ 3.2\end{pmatrix}\,.\]

and

\[-2\begin{pmatrix}1&2\\ 3&4\end{pmatrix}\ =\ \begin{pmatrix}(-2)\cdot 1&(-2)\cdot 2\\ (-2)\cdot 3&(-2)\cdot 4\end{pmatrix}\ =\ \begin{pmatrix}-2&-4\\ -6&-8\end{pmatrix}\,.\]

Finally, the matrix \(\mathrm{I}_{p\times p}\in\mathbb{R}^{p\times p}\) defined through \((\mathrm{I}_{p\times p})_{ii}=1\) for \(i\in\{1,\ldots,p\}\) and \((\mathrm{I}_{p\times p})_{ij}=0\) for \(i,j\in\{1,\ldots,p\}\) with \(i\neq j\) is called the _identity matrix_. One can check readily that \(A\mathrm{I}_{p\times p}=A\) for \(A\in\mathbb{R}^{n\times p}\) and \(\mathrm{I}_{p\times p}A=A\) for \(A\in\mathbb{R}^{p\times n}\), which generalizes the identities \(a\cdot 1=1\cdot a=a\) for \(a\in\mathbb{R}\).

### Transpose of Matrices and Symmetric Matrices

The transposition of a matrix interchanges the matrix's rows and columns. This operation plays an important role in manipulating inner products, for example.

#### Definition b.2.1 Transposes of Matrices and Symmetric Matrices

The _transpose_ of a matrix \(A\in\mathbb{R}^{n\times p}\) is the matrix \(A^{\top}\in\mathbb{R}^{p\times n}\) defined through \((A^{\top})_{ij}:=A_{ji}\) for \(i\in\{1,\ldots,p\}\), \(j\in\{1,\ldots,n\}\). A square matrix \(A\in\mathbb{R}^{p\times p}\) that satisfies \(A=A^{\top}\) is called _symmetric_.

We use the convention \(A_{ij}^{\top}:=(A^{\top})_{ij}\). The definition of transposes applies in particular to vectors: for example,

\[\begin{pmatrix}1\\ 2\end{pmatrix}^{\top}\,=\,\begin{pmatrix}1&2\end{pmatrix}\,.\]

This observation allows us to reformulate the standard inner product: for all \(b\), \(c\in\mathbb{R}^{p}\) (which means \(b\), \(c\in\mathbb{R}^{p\times 1}\) in the language of matrices),

\[\langle b,\,c\rangle\,=\,\sum_{j=1}^{p}b_{j}c_{j}\,=\,\sum_{j=1}^{p}(b^{\top}) _{1j}(c)_{j1}\,=\,(b^{\top}c)_{11}\,=\,b^{\top}c\,,\]

where we have also used the definition of the standard inner product (Page X) and the definition of matrix-matrix multiplications including the fact that \(a^{\top}b\in\mathbb{R}^{1\times 1}=\mathbb{R}\). And similarly, we find for all \(A\in\mathbb{R}^{n\times p}\) and \(b\in\mathbb{R}^{p}\), \(c\in\mathbb{R}^{n}\) that

\[\langle Ab,\,c\rangle\,=\,\sum_{i=1}^{n}\sum_{j=1}^{p}A_{ij}b_{j}c_{i}\,=\, \sum_{j=1}^{p}\sum_{i=1}^{n}b_{j}(A^{\top})_{ji}c_{i}\,=\,\langle b,\,A^{\top} c\rangle\,.\] (B.1)

Hence, the transposition allows us to swap matrices from one argument of the inner product to the other one.

A type of matrices that are naturally symmetric are diagonal square matrices.

**Definition B.2.2**

**Diagonal Matrices** A matrix \(A\in\mathbb{R}^{n\times p}\) that satisfies \(A_{ij}=0\) for all \(i\in\{1,\ldots,n\}\) and \(j\in\{1,\ldots,p\}\) with \(i\neq j\) is called _diagonal_.

Both square _and_ non-square matrices can be diagonal--cf. \(\blacksquare\) Table B.1. But since \(A^{\top}\in\mathbb{R}^{p\times n}\) for \(A\in\mathbb{R}^{n\times p}\), only square diagonal matrices, where \(n=p\), are symmetric.

We conclude with useful rules for working with transposes.

Proof of Lemma 8.2.1The proof consists of simple calculations:

For 1., we find by applying Definition B.2.1 on transposes twice that

\[((A^{\top})^{\top})_{ij}\,=\,(A^{\top})_{ji}\,=\,A_{ij}\]

for all \(i\in\{1,\ldots,n\}\) and \(j\in\{1,\ldots,p\}\), as desired.

For 2., we find by alternating use of Definition B.2.1 and the definitions of the basic matrix operations that

\[(A+cB)_{ij}^{\top}\,=\,(A+cB)_{ji}\,=\,A_{ji}+cB_{ji}\,=\,A_{ij}^{\top}+cB_{ ij}^{\top}\]

for all \(i\in\{1,\ldots,n\},j\in\{1,\ldots,p\}\), as desired.

For 3., we find by alternating use of Definition B.2.1 and the definition of the matrix-matrix multiplication that

\[(AB)_{ij}^{\top}\,=\,(AB)_{ji}\,=\,\sum_{k=1}^{p}A_{jk}B_{ki}\,=\, \sum_{k=1}^{p}A_{kj}^{\top}B_{ik}^{\top}\\ =\,\sum_{k=1}^{p}B_{ik}^{\top}A_{kj}^{\top}\,=\,(B^{\top}A^{\top}) _{ij}\]

for all \(i\in\{1,\ldots,q\},j\in\{1,\ldots,n\}\), as desired. 

### Positive (Semi-)Definite Matrices

In the definition of Gaussian graphical models of \(\blacktriangleright\) Sect. 3.2, we have required the covariance matrices to be positive definite. This has three advantages: the covariance matrix is invertible (1. in Lemma B.2.5 below), which 

[MISSING_PAGE_FAIL:326]

```
\(\boldsymbol{\mathrm{G}}\)
```

**Algorithm 2.4** **Determinant of Matrices** The _determinant function_ \(\boldsymbol{\mathrm{det}}\,:\,\mathbb{R}^{p\times p}\to\mathbb{R}\) is defined as

\[\boldsymbol{\mathrm{det}}\,:\,A\,\mapsto\,\sum_{m\in\mathcal{M}^{p}}\Big{(}\, \mathrm{sign}[m]\prod_{i=1}^{p}A_{i\mu[i]}\Big{)}\,,\]

where \(\mathcal{M}:=\{m:\{1,\ldots,p\}\to\{1,\ldots,p\}\), \(m\) is bijective\(\}\) is the set of _permutations_ of \(\{1,\ldots,p\}\), and \(\mathrm{sign}[m]:=1\) if the permutation consists of an even number of successive interchanges of two indexes and \(\mathrm{sign}[m]:=-1\) otherwise. This formulation of the determinant is called the _Leibniz formula_.

A function \(m:\{1,\ldots,p\}\to\{1,\ldots,p\}\) is called bijective if for each \(j\in\{1,\ldots,p\}\), there is exactly one \(i\in\{1,\ldots,p\}\) such that \(j=m[i]\); for example, the function \(m:\{1,2,3\}\to\{1,2,3\}\) with \(m[1]=1\), \(m[2]=3\), and \(m[3]=2\) is bijective and, therefore, a permutation, while the function \(m:\{1,2,3\}\to\{1,2,3\}\) with 1, _m_[2] = 2, and _m_[3] = 2 is not bijective and, therefore, not a permutation. Hence, a permutation reshuffles indexes and, therefore, can be seen as a sequence of operations that each interchanges two indexes; for example, \(m\) : {1, 2, 3} \(\rightarrow\) {1, 2, 3} with _m_[1] = 1, _m_[2] = 3, and _m_[3] = 2 interchanges just indexes 2 and 3, which means that sign[_m_] = -1, while \(m\) : {1, 2, 3} \(\rightarrow\) {1, 2, 3} with _m_[1] = 2, _m_[2] = 3, and _m_[3] = 1 first interchanges indexes 1 and 2 and then 2 and 3 (or equivalently, first interchanges indexes 2 and 3 and then 1 and 3), which means that sign[_m_] = 1.

Consequently, the determinant of a matrix \(A \in \mathbb{R}^{1 \times 1}\) is det[_A_] = \(A\)11; the determinant of a matrix \(A \in \mathbb{R}^{2 \times 2}\) is det[_A_] = \(A\)11\(A\)22 - \(A\)12\(A\)21, and so forth.

Geometrically speaking, the determinant in absolute value is the volume of the parallelepiped spanned by the matrix's columns. We can verify this readily for (2 x 2)-matrices \(A = (\boldsymbol{a},\boldsymbol{b}) \in \mathbb{R}^{2 \times 2}\). Basic geometry teaches us that the volume of the parallelepiped spanned by \(\boldsymbol{a}\) and \(\boldsymbol{b}\) is vol\({}_{\boldsymbol{a},\boldsymbol{b}} = \|\boldsymbol{a}\|_{2}\|\boldsymbol{b}\|_{2}\sin[\angle_{\boldsymbol{a}, \boldsymbol{b}}]\), where \(\angle_{\boldsymbol{a},\boldsymbol{b}}\) is the angle between \(\boldsymbol{a}\) and \(\boldsymbol{b}\). But \(\sin^{2}[\angle_{\boldsymbol{a},\boldsymbol{b}}] + \cos^{2}[\angle_{\boldsymbol{a},\boldsymbol{b}}] = 1\) and \(\cos[\angle_{\boldsymbol{a},\boldsymbol{b}}] = \langle\boldsymbol{a},\boldsymbol{b}\rangle/(|\boldsymbol{a}|_{2}|\boldsymbol{b }|_{2})\), so that vol\({}_{\boldsymbol{a},\boldsymbol{b}} = \sqrt{|\boldsymbol{a}|_{2}^{2}|\boldsymbol{b}|_{2}^{2}-\langle\boldsymbol{a}, \boldsymbol{b}\rangle^{2}}\). A quick computation then yields

\[\text{vol}_{\boldsymbol{a},\boldsymbol{b}} = \sqrt{|\boldsymbol{a}|_{2}^{2}|\boldsymbol{b}|_{2}^{2}-\langle \boldsymbol{a},\boldsymbol{b}\rangle^{2}}\] above insights \[= \sqrt{\langle a_{1}^{2}+a_{2}^{2}\rangle(b_{1}^{2}+b_{2}^{2})-( a_{1}b_{1}+a_{2}b_{2})^{2}}\] definition of Euclidean norms and standard inner products \[= \sqrt{a_{1}^{2}b_{1}^{2}+a_{1}^{2}b_{2}^{2}+a_{2}^{2}b_{1}^{2}+a _{2}^{2}b_{2}^{2}-a_{1}^{2}b_{1}^{2}-2a_{1}^{2}b_{1}^{2}-2a_{2}^{2}b_{2}^{2}}\] expanding the squared terms \[= \sqrt{a_{1}^{2}b_{2}^{2}+b_{1}^{2}a_{2}^{2}-2a_{1}b_{2}b_{1}a_{2}}\] consolidating \[= \sqrt{(a_{1}b_{2}-b_{1}a_{2})^{2}}\] summarizing the terms

[MISSING_PAGE_EMPTY:6548]

Verify now that for every positive definite \(A\in\mathbb{R}^{p\times p}\), the matrix \((1-w)\mathsf{I}_{p\times p}+wA\) is also positive definite for every \(w\in[0,1]\). By 1. in Lemma B.2.5 and 3. in Lemma B.2.6, it then holds that \(\ell_{A}[w]\neq 0\). Hence, in view of the above condition, \(\ell_{A}[1]=\det[A]>0\), as desired. 

#### Inverse of Matrices

Recall that it holds for every number \(a\in\mathbb{R}\) that \(a\neq 0\) if and only if there is another number, namely \(1/a\in\mathbb{R}\setminus\{0\}\), that satisfies \(a\cdot(1/a)=(1/a)\cdot a=1\). We call \(1/a\) the _reciprocal number_ of \(a\). For example, the reciprocal number of \(2\) is \(1/2\): indeed, \(2\cdot(1/2)=(1/2)\cdot 2=1\). Matrix inverses now generalize this notion of reciprocal numbers to matrices.

#### Definition B.2.5

##### Inverse of Matrices

A square matrix \(A\in\mathbb{R}^{p\times p}\) for which there is another matrix \(A^{-1}\in\mathbb{R}^{p\times p}\) such that \(AA^{-1}=A^{-1}A=\mathsf{I}_{p\times p}\) is called _invertible_. We then call \(A^{-1}\) the _matrix inverse_ of \(A\).

In \(\mathbb{R}^{1\times 1}\), matrix inverses indeed become reciprocal numbers: a matrix \(A=a\in\mathbb{R}^{1\times 1}\) is invertible if and only if \(a\neq 0\), and the inverse of \(A\) is then \(A^{-1}=1/a\).

The notion of matrix inverses harmonizes with positive definiteness:

##### Inverse of Positive Definite Matrices

Let \(A\in\mathbb{R}^{p\times p}\) be a positive definite matrix. Then, 1. \(A\) is invertible; 2. the inverse \(A^{-1}\) is also positive definite; and 3. the diagonal elements of \(A\) are positive: \(A_{ii}>0\) for all \(i\in\{1,\ldots,p\}\).

On the other hand, let \(A\in\mathbb{R}^{p\times p}\) be a symmetric and positive semi-definite matrix. Then, 4. \(A\) is invertible if and only if \(A\) is positive definite.

The second example in 

Table B.1 illustrates that invertible matrices do not need to be positive definite and the

[MISSING_PAGE_EMPTY:6550]

### Trace of Matrices

The trace function enables concise formulations of maximum (regularized) likelihood estimators for Gaussian graphical models in \(\blacktriangleright\) Sect. 3.3. In mathematics more broadly, the trace function is used, for example, in generalizing the standard inner product from vectors to matrices.

**Definition 2.6**: **Trace of Matrices** The _trace function_\(\operatorname{trace}:\mathbb{R}^{p\times p}\to\mathbb{R}\) is defined as

\[\operatorname{trace}\ :\ A\ \mapsto\ \sum_{i=1}^{p}A_{ii}\,.\]

Hence, the trace sums the elements of the diagonal of a matrix. The trace of the identity matrix \(\operatorname{I}_{p\times p}\) is the ambient dimension: \(\operatorname{trace}[\operatorname{I}_{p\times p}]=p\). The trace of a scalar \(A=a\in\mathbb{R}^{1\times 1}\) is the scalar itself: \(\operatorname{trace}[A]=a\). The latter implies in particular that \(\langle\boldsymbol{a},\ \boldsymbol{b}\rangle=\operatorname{trace}[\boldsymbol{a}^{\top}\boldsymbol{b}]\) for \(\boldsymbol{a},\ \boldsymbol{b}\in\mathbb{R}^{p}\), which motivates generalizing the inner product from vectors to matrices through \(\langle A,\ B\rangle:=\operatorname{trace}[A^{\top}B]\) for \(A,\ B\in\mathbb{R}^{n\times p}\).

Important for us are the following properties of the trace:

**Lemma 2.7**: **Linearity and Symmetry of the Trace Function** The trace function is linear in the following sense: for all \(A,B\in\mathbb{R}^{p\times p}\) and \(c\in\mathbb{R}\), it holds that

\[\operatorname{trace}[cA] =\ c\operatorname{trace}[A]\quad\text{and}\] \[\operatorname{trace}[A+B] =\ \operatorname{trace}[A]+\operatorname{trace}[B]\,.\]

The trace function is also symmetric in the following sense: for all \(A\in\mathbb{R}^{n\times p}\) and \(B\in\mathbb{R}^{p\times n}\), it holds that

\[\operatorname{trace}[AB] =\ \operatorname{trace}[BA]\,.\]ProofThe proof consists of three short calculations.

The linearity follows from

\[\text{trace}[cA] = \sum_{i=1}^{p}(cA)_{ii}\] Definition B.2.6 of the trace \[= \sum_{i=1}^{p}cA_{ii}\] definition of scalar-matrix multiplication \[= c\sum_{i=1}^{p}A_{ii}\] linearity of the sum \[= c\,\text{trace}[A]\] Definition B.2.6 of the trace and \[\text{trace}[A+B] = \sum_{i=1}^{p}(A+B_{ii})\] definition of matrix-matrix addition \[= \sum_{i=1}^{p}A_{ii}+\sum_{i=1}^{p}B_{ii}\] linearity of the sum \[= \text{trace}[A]+\text{trace}[B]\,.\] Definition B.2.6 of the trace \[\text{trace}[A]+\text{trace}[B]\,.\] Definition B.2.6 of the trace \[\text{trace}[A]+\text{trace}[B]\,.\] Definition B.2.6 of the trace \[\text{trace}[A]+\text{trace}[B]\,.\] Definition B.2.6 of the trace \[\text{trace}[A]+\text{trace}[B]\,.\] Definition B.2.6 of the trace \[\text{Definition B.2.6 of the trace }\] Definition B.2.6 of the trace \[\text{trace}[A] = \sum_{i=1}^{n}(AB)_{ii}\] Definition B.2.6 of the trace \[= \sum_{i=1}^{n}\sum_{j=1}^{p}A_{ij}B_{ji}\] definition of matrix-matrix multiplication \[= \sum_{j=1}^{p}\sum_{i=1}^{n}B_{ji}A_{ij}\] rewriting the sum 

[MISSING_PAGE_FAIL:334]

Eigenvectors and eigenvalues are also useful in physics, for example. Let \(\mathbf{b}[t]\in\mathbb{R}^{p}\) characterize the state of a physical system at time \(t\) and \(A\in\mathbb{R}^{p\times p}\) how the system evolves with time:

\[\frac{\partial}{\partial t}\mathbf{b}[t]\;=\;A\mathbf{b}[t]\,.\]

We speak of a _linear dynamical system_. If \(A\) has an eigenvector \(\mathbf{q}\in\mathbb{R}^{p}\) with eigenvalue \(m\in\mathbb{R}\), one can verify readily that a solution of the system is

\[\mathbf{b}[t]\;=\;e^{m}\mathbf{q}\,.\]

This solution remains bounded if and only if \(m\leq 1\); we then speak of a stable system. Thus, eigenvectors and eigenvalues can tell us something about a system's stability.

Important for us is that symmetric matrices admit an orthonormal basis of eigenvectors. To show this, we first define the concept of orthogonality.

**Definition B.2.8**: **Orthogonal Matrices** An invertible matrix \(A\in\mathbb{R}^{p\times p}\) that satisfies \(A^{-1}=A^{\top}\) is called _orthogonal_.

Consider a matrix \(A\in\mathbb{R}^{p\times p}\) with columns \(\mathbf{a}_{1},\ldots,\mathbf{a}_{p}\in\mathbb{R}^{p}\), that is, \(A=(\mathbf{a}_{1},\ldots,\mathbf{a}_{p})\). As in  Sect. 2.1, one can show readily that \((A^{\top}A)_{ij}=\langle\mathbf{a}_{i},\,\mathbf{a}_{j}\rangle\). For orthogonal matrices, this becomes \((A^{-1}A)_{ij}=\langle\mathbf{a}_{i},\,\,\mathbf{a}_{j}\rangle\), which means in view of Definition B.2.5 of matrix inverses that \(\langle\mathbf{a}_{i},\,\,\mathbf{a}_{j}\rangle=\|\mathbf{a}_{i}\|_{2}^{2}=1\) for \(i=j\) and \(\langle\mathbf{a}_{i},\,\,\mathbf{a}_{j}\rangle=0\) for \(i\neq j\) and, therefore, that \(\mathbf{a}_{1},\ldots,\mathbf{a}_{p}\) is an orthonormal basis of \(\mathbb{R}^{p}\). You can check that the reverse statement is also true: if \(\mathbf{a}_{1},\ldots,\mathbf{a}_{p}\in\mathbb{R}^{p}\) is an orthonormal basis of \(\mathbb{R}^{p}\), then \(A=(\mathbf{a}_{1},\ldots,\mathbf{a}_{p})\in\mathbb{R}^{p}\) is orthogonal. We conclude that orthogonal matrices are those matrices whose columns form an orthonormal basis of the ambient space.

The orthonormality of the columns ensures in particular that the parallelepiped spanned by these columns has volume one. In the language of determinants:

[MISSING_PAGE_FAIL:336]

[MISSING_PAGE_EMPTY:6556]

Combining the two insights and noting that \(|\boldsymbol{q}|_{2}^{2}>0\) for \(\boldsymbol{q}\neq\boldsymbol{0}_{p}\) by the positive definiteness of norms, we find that \(m\geq 0\).

The second statement can be proved in a very similar way. 

## Singular Value Decomposition

We now review the singular value decomposition, which is used for computing the prediction errors in \(\blacktriangleright\) Sect. 1.2 and Exercise 1.2, for example. The singular value decomposition roughly says that every matrix can be factorized into three parts: a change of basis, a scaling of the axes in this coordinate system, and another change of basis. Hence, the singular value decomposition generalizes the spectral decomposition above.

The general idea is as now follows: Consider the absolute value of the diagonal matrix in Lemma B.2.9, that is, define a diagonal matrix \(S\in\mathbb{R}^{p\times p}\) through \(S_{ij}:=|D_{ij}|\). Verify then that the matrix \(U\in\mathbb{R}^{p\times p}\) defined through \(U_{ij}:=\text{sign}[D_{ij}]Q_{ij}\) is orthogonal (cf. the discussion after Definition B.2.8) and that \(US=QD\). Verify similarly that the matrix \(V:=Q\) is also orthogonal, which implies that \(V^{\top}=Q^{-1}\). We can then write the spectral decomposition for symmetric matrices in Lemma B.2.9 as

\[A\,=\,USV^{\top}\,.\]

The main difference from the earlier formulation is that the diagonal matrix \(S\) here contains the eigenvalues in absolute value. Or stated differently, the diagonal matrix here contains the non-negative square-roots of the eigenvalues of \(A^{\top}A=AA\). The key observation is now that \(A^{\top}A\) is always a square symmetric matrix--irrespective of what the base matrix \(A\) is--which makes it possible to generalize the decomposition to arbitrary matrices.

[MISSING_PAGE_FAIL:339]

**Definition B.2.9**:
**Cofactor Matrices** The _cofactor matrix_\(C\equiv C[A]\in\mathbb{R}^{p\times p}\) for a matrix \(A\in\mathbb{R}^{p\times p}\) is defined through \(C_{ij}:=(-1)^{i+j}m_{ij}\) for \(i,j\in\{1,\ldots,p\}\), where \(m_{ij}:=\det[A_{\{i\}}\circ_{(p)}]\circ 1\) is the determinant of the \(((p-1)\times(p-1))\)-matrix that results from deleting the \(i\)th row and the \(j\)th column of \(A\).

**Lemma B.2.2**:
**Properties of Cofactor Matrices** Let \(A\in\mathbb{R}^{p\times p}\) be a square matrix and \(C\) its cofactor matrix according to Definition B.2.9. Then, (i) the values of \(C_{1j},\ldots,C_{pj}\) do not depend on \(A_{ij}\), and (ii) \(A^{-1}=C^{\top}/\det[A]\).

Claim (i) follows readily from the definition of the cofactors; Claim (ii) should be contrasted with _Cramer's rule_--we omit the details.

**Lemma B.2.1**:
**Laplace Expansion of Determinants** Let \(A\in\mathbb{R}^{p\times p}\) be a square matrix and \(C\) its cofactor matrix according to Definition B.2.9. Then,

\[\det[A]\ =\ \sum_{k=1}^{p}C_{kj}A_{kj}\ =\ \sum_{k=1}^{p}C_{ik}A_{ik}\,\]

for all \(i,j\in\{1,\ldots,p\}\). This identity is called the _Laplace expansion_ of the determinant.

These formulas expand the determinant along the rows/columns of the matrix. We omit the proof.

Now, we relate special block matrices to their submatrices. We will need these results for proving that Gaussian graphical models are amenable to neighborhood selection: see \(\blacktriangleright\) Sect. 3.4.

[MISSING_PAGE_EMPTY:6560]

stated differently, it is sufficient to consider permutations of the last \(q\) indexes:

\[\det[A^{\top}]\,=\,\sum_{m\in\mathcal{M}^{q}}\Big{(}\operatorname{sign}[m]\prod_{ j=1}^{p}A_{jj}^{\top}\prod_{i=1}^{q}A_{(p+i)(p+m[i])}^{\top}\Big{)}\,,\]

where \(\mathcal{M}^{q}\) is the set of permutations of \(\{1,\ldots,q\}\).

Recall now that \(A_{jj}^{\top}=(\mathfrak{l}_{p\times p})_{jj}=1\) and \(A_{(p+i)(p+m[i])}^{\top}=C_{im[i]}^{\top}\). Plugging this into the above equality gives

\[\det[A^{\top}]\,=\,\sum_{m\in\mathcal{M}^{q}}\Big{(}\operatorname{sign}[m] \prod_{i=1}^{q}C_{im[i]}^{\top}\Big{)}\,.\]

In view of Definition B.2.4, the right-hand side of this identity is equal to \(\det[C^{\top}]\), so that we can conclude that \(\det[A]=\det[C]\) (using again 2. in Lemma B.2.3), as desired.

The second claim can be proved along the same lines. 

**Submatrices of Inverse Matrices** Consider a symmetric and positive matrix \(M\in\mathbb{R}^{(p+q)\times(p+q)}\) of the form

\[M\,=\,\begin{pmatrix}A&B\\ B^{\top}&C\end{pmatrix}\]

with \(A\in\mathbb{R}^{p\times p},B\in\mathbb{R}^{p\times q},\,C\in\mathbb{R}^{q \times q}\). We write its inverse \(M^{-1}\) (which exists and is also positive definite by 1. and 2. in Lemma B.2.5 and symmetric by 2. in Lemma B.2.6) in the form

\[M^{-1}\,=\,\begin{pmatrix}\widetilde{A}&\widetilde{B}\\ \widetilde{B}^{\top}&\widetilde{C}\end{pmatrix}\]

with \(\widetilde{A}\in\mathbb{R}^{p\times p},\,\widetilde{B}\in\mathbb{R}^{p\times q },\,\widetilde{C}\in\mathbb{R}^{q\times q}\). It then holds that

1. \(C^{-1}\,=\,\widetilde{C}\,-\widetilde{B}^{\top}\widetilde{A}^{-1}\widetilde{B }\) and (ii) \(\frac{\det[C]}{\det[M]}\,=\,\frac{1}{\det[A^{-1}]}\,.\)

[MISSING_PAGE_FAIL:343]

\[=\ \widetilde{C}-\widetilde{B}^{\top}\widetilde{A}^{-1}\widetilde{B}\,,\] \[C^{-1}(-C)=-\mathrm{I}_{q\times q}\text{ by the linearity of matrix--matrix multiplications and Definition \ref{def:main}}\]

as desired.

For proving (ii), we first calculate

\[\det[M^{-1}] =\ \det\begin{pmatrix}\widetilde{A}&\widetilde{B}\\ \widetilde{B}^{\top}&\widetilde{C}\end{pmatrix}\ \text{ writing }M^{-1}\text{ as stated in the lemma}\] \[=\ \det\begin{bmatrix}\begin{pmatrix}\widetilde{A}&\mathbf{0}_{p \times q}\\ \widetilde{B}^{\top}&\mathrm{I}_{q\times q}\end{pmatrix}\begin{pmatrix} \mathrm{I}_{p\times p}&\widetilde{A}^{-1}\widetilde{B}\\ \mathbf{0}_{q\times p}&\widetilde{C}-\widetilde{B}^{\top}\widetilde{A}^{-1} \widetilde{B}\end{pmatrix}\end{bmatrix}\] \[=\ \det\begin{pmatrix}\widetilde{A}&\mathbf{0}_{p\times q}\\ \widetilde{B}^{\top}&\mathrm{I}_{q\times q}\end{pmatrix}\det\begin{pmatrix} \mathrm{I}_{p\times p}&\widetilde{A}^{-1}\widetilde{B}\\ \mathbf{0}_{q\times p}&\widetilde{C}-\widetilde{B}^{\top}\widetilde{A}^{-1} \widetilde{B}\end{pmatrix}\] \[=\ \det\begin{pmatrix}\widetilde{A}&\widetilde{B}\\ \mathbf{0}_{p\times q}&\mathrm{I}_{q\times q}\end{pmatrix}\det\begin{pmatrix} \mathrm{I}_{p\times p}&\widetilde{A}^{-1}\widetilde{B}\\ \mathbf{0}_{q\times p}&\widetilde{C}-\widetilde{B}^{\top}\widetilde{A}^{-1} \widetilde{B}\end{pmatrix}\] \[\text{ using 2. in Lemma \ref{lemma:main}}\] \[=\ \det[\widetilde{A}]\det[\widetilde{C}-\widetilde{B}^{\top} \widetilde{A}^{-1}\widetilde{B}]\] \[\text{ using the second part of Lemma \ref{lemma:main}}\] \[=\ \det[\widetilde{A}]\det[C^{-1}]\,.\]

Hence, using again that \(\widetilde{A}\) and \(C\) are invertible and then using the fact that the determinants of matrices and their inverses are reciprocal according to 3. in Lemma \ref{lemma:main}, we find

\[\frac{1}{\det[M]}\ =\ \frac{1}{\det[\widetilde{A}^{-1}]\det[C]}\,,\]

which can be rearranged to

\[\frac{\det[C]}{\det[M]}\ =\ \frac{1}{\det[\widetilde{A}^{-1}]}\,,\]

as desired. 

We finally prove a statement in the main part of the book.

\[\begin{array}{l}\hline\hline\text{\sf{Count}}\,\mathbb{R}^{d}\,\text{\sf{.}}\\ \hline\text{\sf{Parameters in Neighborhood Selection}}\,\text{\sf{.}}\end{array}\]

With the notation on Page 93, it holds that

\[\Theta_{\bar{\theta}}\,=\,-\,\frac{\Theta_{ii}(\mathbf{\beta}^{i})_{j}+\Theta_{ \bar{\theta}j}(\mathbf{\beta}^{i})_{i}}{2}\qquad\text{for all }i,j\in\{1,\ldots,d\}\,\text{\sf{.}}\]

This equation expresses arbitrary elements of the precision matrix by diagonal elements and the regression vectors defined in Lemma 3.4.1. The equation is the motivation for the neighborhood selection estimator (3.6).

Proof of Lemma b.2.16The proof is simple yet slightly tedious algebra.

Recall first that \(\mathbf{\beta}^{j}\in\mathbb{R}^{d}\) augments the \((d-1)\)-dimensional regression vector \(\mathbf{\beta}^{j}\) of Lemma 3.4.1 with a coordinate of value \(-1\) that is inserted at the \(j\)th position (see Page 93). Recall also that the precision matrix \(\Theta\in\mathbb{R}^{d\times d}\) is symmetric and positive definite (see Page 86) and, therefore, (i) has strictly positive diagonal elements (see 3. in Lemma B.2.5) and (ii) is symmetric (see 2. in Lemma B.2.6).

We now derive for all \(i,j\in\{1,\ldots,d\}\)

\[\mathbf{\beta}^{j}\,=\,-\,(\Theta_{j(j)}\epsilon)^{\top}/\Theta_{\bar{\mu}j}\qquad \qquad\text{by definition in Lemma \ref{lemma:b.2.1}}\]

\[\Rightarrow\quad(\Theta_{j(j)}\epsilon)^{\top}\,=\,-\,\Theta_{\bar{\mu}j}\mathbf{ \beta}^{j}\]

\[\Rightarrow\quad\begin{pmatrix}\Theta_{j1}\\ \vdots\\ \Theta_{j(j-1)}\\ \Theta_{j(j+1)}\\ \vdots\\ \Theta_{jd}\end{pmatrix}=\,-\,\Theta_{\bar{\mu}j}\begin{pmatrix}(\mathbf{\beta}^{ j})_{1}\\ \vdots\\ (\mathbf{\beta}^{j})_{j-1}\\ (\mathbf{\beta}^{j})_{j}\\ \vdots\\ (\mathbf{\beta}^{j})_{d-1}\end{pmatrix}\]

writing the previous equation more explicitly Similarly, \(\Theta_{ij}=-\Theta_{ii}(\boldsymbol{\beta}^{i})_{j}\). Moreover, since \(\Theta\) is symmetric by (ii), it holds that \(\Theta_{ij}=\Theta_{ji}\). In summary,

\[\Theta_{ij}\;=\;-\;\frac{\Theta_{ii}(\boldsymbol{\beta}^{i})_{j}+\Theta_{jj}( \boldsymbol{\beta}^{j})_{i}}{2}\;,\]

as desired.

## References

* Aitchison (1982) Aitchison, J. (1982). The statistical analysis of compositional data. _Journal of the Royal Statistical Society, Series B: Statistical Methodology, 44_(2), 139-160.
* Albert (1972) Albert, A. (1972). _Regression and the Moore-Penrose pseudoinverse_. Elsevier.
* Almal & Padh (2012) Almal, S., & Padh, H. (2012). Implications of gene copy-number variation in health and diseases. _Journal of Human Genetics_. _57_(1), 6.
* Anscombe (1948) Anscombe, F. (1948). The transformation of Poisson, binomial and negative-binomial data. _Biometrika, 35_(3/4), 246-254.
* Antoniadis (2010) Antoniadis, A. (2010). Comments on: \(\ell_{1}\)-penalization for mixture regression models. _Test, 19_, 257-258.
* Arlot & Celisse (2010) Arlot, S., & Celisse, A. (2010). A survey of cross-validation procedures for model selection. _Statistics Surveys, 4_, 40-79.
* Bakin (1999) Bakin, S. (1999). Adaptive regression and model selection in data mining problems, PhD thesis, The Australian National University, Canberra.
* Banerjee et al. (2008) Banerjee, O., El Ghaoui, L., & d'Aspremont, A. (2008). Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data. _Journal of Machine Learning Research_, 9, 485-516.
* Bellec & Tsybakov (2017) Bellec, P., & Tsybakov, A. (2017). Bounds on the prediction error of penalized least squares estimators with convex penalty. _Modern Problems of Stochastic Analysis and Statistics, 208_, 315-333.
* Belloni & Chernozhukov (2013) Belloni, A., & Chernozhukov, V. (2013). Least squares after model selection in high-dimensional sparse models. _Bernoulli, 19_(2), 521-547.
* Belloni et al. (2011) Belloni, A., Chernozhukov, V., & Wang, L. ( 2011). Square-root lasso: Pivotal recovery of sparse signals via conic programming. _Biometrika, 98_(4), 791-806.
* Besag (1974) Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. _Journal of the Royal Statistical Society, Series B: Statistical Methodology, 36_(2), 192-236.
* Bickel et al. (1993) Bickel, P., Klaassen, C., Ritov, Y., & Wellner, J. (1993). _Efficient and adaptive estimation for semiparametric models_. Johns Hopkins University Press.
* Bickel et al. (2009) Bickel, P., Ritov, Y., & Tsybakov, A. (2009). Simultaneous analysis of lasso and Dantzig selector. _The Annals of Statistics, 37_(4), 1705-1732.
* Bien et al. (2018a) Bien, J., Gaynanova, I., Lederer, J., & Muller, C. (2018a). Non-convex global minimization and false discovery rate control for the TREX. _Journal of Computational and Graphical Statistics, 27_(1), 23-33.
* Bien et al. (2018b) Bien, J., Gaynanova, I., Lederer, J., & Muller, C. (2018b). Prediction error bounds for linear regression with the TREX. _Test, 28_(2), 451-474.
* Bien & Wegkamp (2013) Bien, J., & Wegkamp, M. (2013). Discussion of: Correlated variables in regression: Clustering and sparse estimation. _Journal of Statistical Planning and Inference, 143_(11), 1859-1862.
* Borgelt & Kruse (2002) Borgelt, C., & Kruse, R. (2002). _Graphical models: Methods for data analysis and mining_. Wiley.
* Boucheron et al. (2013) Boucheron, S., Lugosi, G., & Massart, P. (2013), _Concentration inequalities: A nonasymptotic theory of independence_. Oxford University Press.
* Boyd & Vandenberghe (2004) Boyd, S., & Vandenberghe, L. (2004). _Convex optimization_. Cambridge University Press.

* Bu & Lederer (2017) Bu, Y., & Lederer, J. (2017). Integrating additional knowledge into estimation of graphical models. arXiv:1704.02739.
* Bunea et al. (2014) Bunea, F., Lederer, J., & She, Y. (2014). The group square-root lasso: Theoretical properties and fast algorithms. _IEEE Transactions on Information Theory_, _60_(2), 1313-1325.
* Cai et al. (2011) Cai, T., Liu, W., & Luo, X. (2011). A constrained \(\ell_{1}\) minimization approach to sparse precision matrix estimation. _Journal of the American Statistical Association, 106_(494), 594-607.
* Celisse (2008) Celisse, A. (2008), Model selection via cross-validation in density estimation, regression, and change-points detection, PhD thesis, Universite Paris Sud-Paris XI.
* Chatterjee & Jafarov (2015) Chatterjee, S., & Jafarov, J. (2015). Prediction error of cross-validated lasso. arXiv:1502.06291.
* Chetelat et al. (2017) Chetelat, D., Lederer, J., & Salmon, J. (2017). Optimal two-step prediction in regression. _Electronic Journal of Statistics_, _11_(1), 2519-2546.
* Chichignoud et al. (2016) Chichignoud, M., Lederer, J., & Wainwright, M. (2016). A practical scheme and fast algorithm to tune the lasso with optimality guarantees. _Journal of Machine Learning Research_, _17_(1), 1-20.
* Dalalyan et al. (2017) Dalalyan, A., Hebiri, M., & Lederer, J. (2017). On the prediction performance of the lasso. _Bernoulli_, _23_(1), 552-581.
* Dettling & Buhlmann (2004) Dettling, M., & Buhlmann, P. (2004). Finding predictive gene groups from microarray data. _Journal of Multivariate Analysis_, _90_(1), 106-131.
* Diesner & Carley (2005) Diesner, J., & Carley, K. (2005). Exploration of communication networks from the Enron email corpus. In _SIAM International Conference on Data Mining_ (pp. 3-14).
* Dobra et al. (2004) Dobra, A., Hans, C., Jones, B., Nevins, J., Yao, G., & West, M. (2004). Sparse graphical models for exploring gene expression data. _Journal of Multivariate Analysis_, _90_(1), 196-212.
* Dudley (2002) Dudley, R. (2002), _Real analysis and probability_ (Vol. 74). Cambridge University Press.
* Durrett (2010) Durrett, R. (2010), _Probability: Theory and examples_ (4th ed.). Cambridge University Press.
* Edwards (2012) Edwards, D. (2012), _Introduction to graphical modelling_. Springer.
* Efron et al. (2004) Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression. _Annals of Statistics_, _32_(2), 407-499.
* Engl et al. (1996) Engl, H., Hanke, M., & Neubauer, A. (1996). _Regularization of inverse problems_ (Vol. 375). Springer.
* Fan & Li (2001) Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. _Journal of the American Statistical Association_, _96_(456), 1348-1360.
* Frank & Friedman (1993) Frank, I., & Friedman, J. (1993). A statistical view of some chemometrics regression tools. _Technometrics_, _35_(2), 109-135.
* Friedman et al. (2008) Friedman, J., Hastie, T., & Tibshirani, R. (2008). Sparse inverse covariance estimation with the graphical lasso. _Biostatistics_, _9_(3), 432-441.
* Fultz et al. (2019) Fultz, N., Bonmassar, G., Setsompop, K., Stickgold, R., Rosen, B., Polimeni, J., & Lewis, L. (2019). Coupled electrophysiological, hemodynamic, and cerebrospinal fluid oscillations in human sleep. _Science_, _366_(6465), 628-631.
* Gallavotti (2013) Gallavotti, G. (2013), _Statistical mechanics: A short treatise_. Springer.
* Geisser (1975) Geisser, S. (1975). The predictive sample reuse method with applications. _Journal of the American Statistical Association_, _70_(350), 320-328.
* Gold et al. (2020) Gold, D., Lederer, J., & Tau, J. (2020). Inference for high-dimensional nested regression. _Journal of Econometrics_, _217_(1), 79-111.

Golub, G., Heath, M., & Wahba, G. (1979). Generalized cross-validation as a method for choosing a good ridge parameter. _Technometrics, 21_(2), 215-223.
* [Greenshtein & RitovGreenshtein & Ritov19] Greenshtein, E., & Ritov, Y. (2004). Persistence in high-dimensional linear predictor selection and the virtue of overparametrization. _Bernoulli, 10_(6), 971-988.
* [GrimmettGrimmett1973] Grimmett, G. (1973). A theorem about random fields. _Bulletin of the London Mathematical Society, 5_(1), 81-84.
* [Hastie _et al._Hastie _et al._2015] Hastie, T., Tibshirani, R., & Wainwright, M. (2015), _Statistical learning with sparsity: The lasso and generalizations_. Chapman and Hall.
* [Hebiri and LedererHebi and Lederer2013] Hebiri, M., & Lederer, J. (2013). How correlations influence lasso prediction. _IEEE Transactions on Information Theory, 59_(3), 1846-1854.
* [Hiriart-Urruty & LemarechalHiriart-Urruty & Lemarechal2004] Hiriart-Urruty, J.-B., & Lemarechal, C. (2004). _Convex analysis and minimization algorithms_. Springer.
* [Hoerl and KennardHoerl and Kennard1970] Hoerl, A., & Kennard, R. (1970). Ridge regression: Biased estimation for nonorthogonal problems. _Technometrics, 12_(1), 55-67.
* [Homrighausen and McDonaldHomrighausen and McDonald2013a] Homrighausen, D., & McDonald, D. (2013a). The lasso, persistence, and cross-validation. In _Proceedings of machine learning research_ (Vol. 28, pp. 1031-1039).
* [Homrighausen and McDonaldHomrighausen and McDonald2013b] Homrighausen, D., & McDonald, D. (2013b). Risk-consistency of cross-validation with lasso-type procedures. _Statistica Sinica, 27_(3), 1017-1036.
* [Homrighausen and McDonaldHomrighausen and McDonald2014] Homrighausen, D., & McDonald, D. (2014). Leave-one-out cross-validation is risk consistent for lasso. _Machine Learning, 97_(1-2), 65-78.
* [Huang _et al._Huang _et al._2019] Huang, S.-T., Duren, Y., Hellton, K., & Lederer, J. (2019). Tuning parameter calibration for prediction in personalized medicine. arXiv:1909.10635.
* [Javanmard and MontanariJavanmard and Montanari2014] Javanmard, A., & Montanari, A. (2014). Confidence intervals and hypothesis testing for high-dimensional regression. _Journal of Machine Learning Research, 15_(1), 2869-2909.
* [Judson _et al._Judson _et al._2002] Judson, R., Salisbury, B., Schneider, J., Windemuth, A., & Stephens, J. (2002). How many SNPs does a genome-wide haplotype map require? _Pharmacogenomics, 3_(3), 379-391.
* [KarushKarush1939] Karush, W. (1939), Minima of functions of several variables with inequalities as side constraints, aster's thesis, University of Chicago.
* [Kidd _et al._Kidd _et al._2008] Kidd, J. et al. (2008). Mapping and sequencing of structural variation from eight human genomes. _Nature, 453_(7191), 56-64.
* [Kim _et al._Kim _et al._2008] Kim, Y., Choi, H., & Oh, H.-S. (2008). Smoothly clipped absolute deviation on high dimensions. _Journal of the American Statistical Association, 103_(484), 1665-1673.
* [Knight and FuKnight and Fu2000] Knight, K., & Fu, W. (2000). Asymptotics for lasso-type estimators. _Annals of Statistics, 28_(5), 1356-1378.
* [Kuhn and TuckerKuhn and Tucker1951] Kuhn, H., & Tucker, A. (1951). Nonlinear programming. In _Proceedings of Second Berkeley Symposium_ (pp. 481-492). University of California Press.
* [Kurtz _et al._Kurtz _et al._2015] Kurtz, Z., Muller, C., Miraldi, E., Littman, D., Blaser, M., & Bonneau, R. (2015). Sparse and compositionally robust inference of microbial ecological networks. _PLoS Computational Biology, 11_(5), e1004226.
* [Laszkiewicz _et al._Laszkiewicz _et al._2020] Laszkiewicz, M., Fischer, A., & Lederer, J. (2020). Thresholded adaptive validation: Tuning the graphical lasso for graph recovery. arXiv:2005.00466.
* [LauritzenLauritzen1996] Lauritzen, S. (1996). _Graphical models_. Oxford University Press.
* [LedererLederer2013] Lederer, J. (2013). Trust, but verify: Benefits and pitfalls of least-squares refitting in high dimensions. arXiv:1306.0113.

* Lederer & Muller (2015) Lederer, J., & Muller, C. (2015). Don't fall for tuning parameters: Tuning-free variable selection in high dimensions with the TREX. In _AAAI Conference on Artificial Intelligence_.
* Lederer et al. (2019) Lederer, J., Yu, L., & Gaynanova, I. (2019). Oracle inequalities for high-dimensional prediction. _Bernoulli, 25_(2), 1225-1255.
* Lepski et al. (1997) Lepski, O., Mammen, E., & Spokoiny, V. (1997). Optimal spatial adaptation to inhomogeneous smoothness: An approach based on kernel estimates with variable bandwidth selectors. _Annals of Statistics, 25_(3), 929-947.
* Lepskii (1991) Lepskii, O. (1991). On a problem of adaptive estimation in Gaussian white noise. _Theory of Probability and its Applications, 35_(3), 454-466.
* Li & Lederer (2019) Li, W., & Lederer, J. (2019). Tuning parameter calibration for \(\ell_{1}\)-regularized logistic regression. _Journal of Statistical Planning and Inference, 202_, 80-98.
* Mazumder & Hastie (2012) Mazumder, R., & Hastie, T. (2012). The graphical lasso: New insights and alternatives. _Electronic Journal of Statistics, 6_, 2125-2149.
* Meinshausen (2007) Meinshausen, N. (2007). Relaxed lasso. _Computational Statistics and Data Analysis, 52_(1), 374-393.
* Meinshausen (2013) Meinshausen, N. (2013). Sign-constrained least squares estimation for high-dimensional regression. _Electronic Journal of Statistics, 7_, 1607-1631.
* Meinshausen & Buhlmann (2006) Meinshausen, N., & Buhlmann, P. (2006). High-dimensional graphs and variable selection with the lasso. _Annals of Statistics, 34_(1), 1436-1462.
* Merriam-Webster (2019) Merriam-Webster.com (2019). Oracle. Retrieved November 11, 2019 from [https://www.merriam-webster.com](https://www.merriam-webster.com)
* Mills et al. (2006) Mills, R., Luttig, C., Larkins, C., Beauchamp, A., Tsui, C., Pittard, W., & Devine, S. (2006). An initial map of insertion and deletion (INDEL) variation in the human genome. _Genome Research, 16_(9), 1182-1190.
* Negahban et al. (2012) Negahban, S., Yu, B., Wainwright, M., & Ravikumar, P. (2012). A unified framework for high-dimensional analysis of \(M\)-estimators with decomposable regularizers. _Statistical Science, 27_(4), 538-557.
* Obozinski et al. (2011) Obozinski, G., Jacob, L., & Vert, J.-P. (2011). Group lasso with overlaps: The latent group lasso approach. arXiv:1110.0413.
* Osborne et al. (2000) Osborne, M., Presnell, B., & Turlach, B. (2000). On the lasso and its dual. _Journal of Computational and Graphical Statistics, 9_(2), 319-337.
* Oztoprak et al. (2012) Oztoprak, F., Nocedal, J., Rennie, S., & Olsen, P. (2012), Newton-like methods for sparse inverse covariance estimation. In _Advances in neural information processing systems_ (pp. 755-763).
* Park & Casella (2008) Park, T., & Casella, G. (2008). The Bayesian lasso. _Journal of the American Statistical Association, 103_(482), 681-686.
* Penrose (1955) Penrose, R. (1955). A generalized inverse for matrices. _Mathematical Proceedings of the Cambridge Philosophical Society, 51_(3), 406-413.
* Perrone et al. (2018) Perrone, V., Jenatton, R., Seeger, M., & Archambeau, C. (2018). Scalable hyperparameter transfer learning. In _Advances in neural information processing systems_ (pp. 6845-6855).
* Preston (1973) Preston, C. (1973). Generalized Gibbs states and Markov random fields. _Advances in Applied Probability, 5_(2), 242-261.
* Schneider & Ewald (2017) Schneider, U., & Ewald, K. (2017). On the distribution, model selection properties and uniqueness of the lasso estimator in low and high dimensions. arXiv:1708.09608.
* Sherman (1973) Sherman, S. (1973). Markov random fields and Gibbs random fields. _Israel Journal of Mathematics, 14_(1), 92-103.

Simon, N., Friedman, J., Hastie, T., & Tibshirani, R. (2013). A sparse-group lasso. _Journal of Computational and Graphical Statistics, 22_(2), 231-245.
* Spirtes et al. (2000) Spirtes, P., Glymour, C., Scheines, R., Heckerman, D., Meek, C., Cooper, G., & Richardson, T. (2000). _Causation, prediction, and search_. MIT Press.
* Studler et al. (2010) Studler, N., Buhlmann, P., & van de Geer, S. (2010). \(\ell_{1}\)-penalization for mixture regression models. _Test, 19_, 209-285.
* Stock & Trebbi (2003) Stock, J., & Trebbi, F. (2003). Retrospectives: Who invented instrumental variable regression?_Journal of Economic Perspectives, 17_(3), 177-194.
* Stone (1974) Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. _Journal of the Royal Statistical Society, Series B: Statistical Methodology, 36_(2), 111-133.
* Sun & Zhang (2010) Sun, T., & Zhang, C.-H. (2010). Comments on: \(\ell_{1}\)-penalization for mixture regression models. _Test, 19_, 270-275
* Sun & Zhang (2012) Sun, T., & Zhang, C.-H. (2012). Scaled sparse linear regression. _Biometrika, 99(4)_, 879-898.
* Taheri et al. (2020) Taheri, M., Lim, N., & Lederer, J. (2020). Efficient feature selection with large and high-dimensional data. arXiv:1609.07195.
* Tibshirani (1996) Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society, Series B: Statistical Methodology, 58_(1), 267-288.
* Tibshirani (2013) Tibshirani, R. (2013). The lasso problem and uniqueness. _Electronic Journal of Statistics_, 7, 1456-1490.
* Tikhonov (1943) Tikhonov, A. (1943). On the stability of inverse problems. _Doklady Akademii Nauk SSSR, 39_(5), 195-198.
* van de Geer (2007) van de Geer, S. (2007), The deterministic lasso. In _JSM Proceedings_.
* van de Geer & Buhlmann (2009) van de Geer, S., & Buhlmann, P. (2009). On the conditions used to prove oracle results for the lasso. _Electronic Journal of Statistics, 3_, 1360-1392.
* van de Geer & Buhlmann (2011) van de Geer, S., & Buhlmann, P. (2011). _Statistics for high-dimensional data: Methods, theory and applications_. Springer.
* van de Geer et al. (2014) van de Geer, S., Buhlmann, P., Ritov, Y., & Dezeure, R. (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. _Annals of Statistics, 42_(3), 1166-1202.
* van de Geer & Lederer (2013) van de Geer, S., & Lederer, J. (2013). The lasso, correlated design, and improved oracle inequalities. In _From probability to statistics and back: High-dimensional models and processes-a festschrift in honor of Jon A. Wellner', IMS_ (pp. 303-316).
* van der Vaart (2000) van der Vaart, A. (2000). _Asymptotic statistics_ (Vol. 3). Cambridge University Press.
* Wainwright (2009) Wainwright, M. (2009). Sharp thresholds for high-dimensional and noisy sparsity recovery using \(\ell_{1}\)-constrained quadratic programming (lasso). _IEEE Transactions on Information Theory, 55_(5), 2183-2202.
* Wainwright (2014) Wainwright, M. (2014). Structured regularizers for high-dimensional problems: Statistical and computational issues. _Annual Review of Statistics and Its Application, 1_, 233-253.
* Yuan & Lin (2006) Yuan, M., & Lin, Y. (2006). Model selection and estimation in regression with grouped variables. _Journal of the Royal Statistical Society, Series B: Statistical Methodology, 68_(1), 49-67.
* Yuan & Lin (2007) Yuan, M., & Lin, Y. (2007). Model selection and estimation in the Gaussian graphical model. _Biometrika, 94_(1), 19-35.
* Zhang (2010) Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. _The Annals of Statistics, 38_(2), 894-942.

* Zhang & Zhang (2012) Zhang, C.-H., & Zhang, T. (2012). A general theory of concave regularization for high-dimensional sparse estimation problems. _Statistical Science, 27_(4), 576-593.
* Zhao & Yu (2006) Zhao, P., & Yu, B. (2006). On model selection consistency of lasso. _Journal of Machine Learning Research, 7_, 2541-2563.
* Zou & Hastie (2005) Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. _Journal of the Royal Statistical Society, Series B: Statistical Methodology, 67_(2), 301-320.
* Zuber & Itzykson (1977) Zuber, J.-B., & Itzykson, C. (1977). Quantum field theory and the two-dimensional Ising model. _Physical Review D, 15_(10), 2875.

[MISSING_PAGE_FAIL:353]

Compatibility condition (_cont._)
* _See also_ Power-two prediction bound; Prior function bound
* Compatibility constant, **197**
* _See also_ Compatibility condition Compositional data, transformation for, **108** Concave function, **79**
* Conditional (in)dependence, **82-83**
* Hammersley-Clifford theorem, **86**, 108
* reference, **107**
* _See also_ Graphical models Conditional expectation
* another property, **154**, 288
* law of iterated expectations, **154**
* literature, **154** Confidence interval, **146-153**
* for debiased lasso estimator, **153**
* for least-squares estimator, **150**
* _See also_ One-step estimator Convex function, **54**
* can have zero, one, or multiple minima, **22**, **67**
* combinations of convex functions are, **55**
* and _d_-decomposability, **236**
* dual function is, **65**
* examples/counterexamples, **22**, **55**, 68
* further reading, **209**
* generalization to [-\(\infty,\infty\)]-valued functions, **209**
* generalization to functions of matrices, **95**
* illustration, **55**
* Jensen inequality, **67**
* lasso estimator is, **56**
* least-squares estimator is, **22**
* link function, **178**
* local minimum is global minimum, **67**
* _d__q_-function is (not), **44**
* many high-dimensional estimators are, **55**
* norm is, **56**
* power-one/power-two basic inequality, 176, **179**
* trace function, **95**
* _See also_ Strictly convex function; Optimality conditions; Subdifferential Convex set, **95** Correlated predictors, **40**
* affect suitability of tuning parameter, **135**
* elastic net estimator selects, 45, **78**
* Gram matrix summarizes correlations, **40-41**
* lasso estimator does not select, **78**
* orthogonal design, **40**
* power-one bounds hold with, **185**
* power-two bounds do not hold with, **192-204**
* are problematic for estimation and support recovery, 39-40, **212**, 214, 221, 232
* weakly highly correlated design, **40**
* _See also_ Compatibility/irrepresentability condition; Power-one/power-two prediction bound
* Count data, transformation for, **108** Covariance matrix, **XI**, 85
* _See also_ Gaussian graphical models Cramer-Chernoff method, **286**
* computational complexity, **120**, 124
* holdout method, **117**, 119
* _k_-fold, **119**, 120
* leave-one-out, **119**
* Monte Carlo, **118**, 119, 120
* optimal tuning parameter, **116**
* references, **136**
* training set, **117**, 120
* validation/holdout set, **118**
* _See also_ Thresholding CV, _see_ Cross-validation

Appendix D Data are not the statistical challenge, **3**, 34 Data-fitting function, **6**, **10**, 55, 110-113, 208 - _See also_ Cross-validation; Overfitting, \(R^{2}\); Tuning parameter Data splitting, _see_ Cross-validation Data transformation
* Anscombe, **108**
* centered log-ratio, **108**
* centering, **89**
* centering and standardizing avoids intercepts, **89**, 91
* to fit Gaussian graphical models, **85**, 108
* to fit linear regression models, **38**
* normalization, **40**
* standardizing, **89**
* Debiased lasso estimator, **150**
* confidence interval for, **150**
* is not suitable for prediction/estimation, **146**, 166
* _See also_ Bias; One-step estimator Decomposable function, _see_\(\delta\)-decomposable function **194**, 197, 208, 216, 217* convex prior function, **234**
* for empty and full sets, **208**
* examples, **204**
* lasso estimator is, **204**
* norm is, 194, **204**
* subdecomposable function, **226**
* Definite function, **121**
* Dependence/independence, **82**
* Hammersley-Clifford theorem, **86**, 108
* _See also_ Conditional (in)dependence; Graphical models Design matrix, **39**, 40-41
* _See also_ Linear regression Desparsifying, **166**
* _See also_ Debiasing Determinant of a matrix, **318**, 318-321
* basic properties, **320**
* Laplace expansion of, **331**
* of orthogonal matrix equals \(\pm\) 1, **327**
* is product of eigenvalues/singular values, **96**
* _See also_ Log-determinant function Diagonal matrix, **8**, **315**, 330**
* in singular value decomposition, **330**
* Differentiable function
* least-squares estimator, **6**
* link function, **178**
* log-determinant function, **96**
* maximum likelihood function
* for precision matrix, **97**
* in **Z**-estimation, **141**
* ridge estimator, **15**
* subdifferential of, **57**
* trace function, **96**
* _See also_ Subdifferential Differential equations, **82**
* Dinosaur, **209**
* Directed graphical models, **107-108**
* _See also_ Graphical models Double-cone, 200, **201**
* apex of, **201**
* Double-exponential distribution, _see_ Laplace distribution Dual feasibility, **237**
* Dual function, **49**
* bounds, **224**
* _vs._ dual vector, **219**
* of edr estimator, **67**
* has many properties, **65**
* intuition, **49**
* of \(\ell_{q}\)-norm, **64**
* of non-negative lasso estimator, 66, **205**
* is positive definite, **50**
* of ridge estimator, **67**
* of (square-root) group lasso estimator, **66**
* _See also_ Holder inequality Dual norm, **237**
* _See also_ Dual function Dual problem, **237**
* _See also_ Primal-dual pair Dual vector, **219**
* _See also_ Primal-dual pair

Edge, in graphical models, **84**
* edr estimator, **78**
* dual function of, **67**
* is not \(\mathcal{E}\)-decomposable, **206**
* Effective noise, **190**
* in Holder inequality, **50**
* of lasso estimator, 12, **112**, 113-116, 127, 135-136, 173
* tail bound, **113**, 128
* of least-squares estimator, **9**, 12, 173
* _See also_ Estimation error; Power-one/power-two prediction bounds; Tuning parameter Eigenvector/eigenvalue of a matrix, **325**, 325-329
* of positive (semi-)definite matrix, **328**
* relation to determinant, **96**
* relation to structural condition, **193-194**, 197-198
* _See also_ Compatibility condition; (\(\ell_{\infty}\)-)restricted eigenvalue condition; Singular value; Spectral decomposition Elastic net estimator, **45**
* difference to bridge estimator, **77**
* references, **78**
* selects correlated predictors, **78**
* is strictly convex, **78**
* Empirical covariance matrix, **89**, 96-98
* _See also_ Gaussian graphical models Endogenous predictor, **167**
* Estimation error
* in \(\bar{\mu}\)-loss, **213-217**
* in \(\bar{\mu}\)-loss, **218-231**
* of lasso estimator, 125-127, **214-217**, 225-228
* relationship to prediction error, 194-196, **212-213**, 213-217
* _See also_ Compatibility condition; Effective noise; Irrepresentability condition Euclidean distance ridge estimator, _see_ edr estimator Euclidean norm, **X**
* _See also_\(\ell_{q}\)-function Exogeneous predictor, **167**
* Extended real line, **X**
Fast-rate bound, **210**
* _See also_ Power-two prediction bound Feature selection, **231**
* _See also_ Support recovery First-order basic inequality, 171, **176**
* difference to second-order basic inequality, **178-179**, 204-205
* name, **179**
* _See also_ Basic inequality Forward problem, **35**

G

Gauss distribution, **XI**
* as Bayes prior distribution, **21**
* centered Gauss distribution, **89** (_see also_ Data transformation)
* covariance matrix, **XI**
* density, **87**
* effective noise with Gauss noise, **113**
* Gaussian linear regression model, **38**
* marginals are Gauss distributions, **92**
* mean vector, **XI**
* measure of affine subspace, **281**
* tail bound for, **127-128**, 156, 286
* _See also_ Gaussian graphical models Gaussian graphical models, **85-86**
* covariance and precision matrices, **85**
* Hammersley-Clifford theorem, **86**, 108
* model parameter, **86**
* _See also_ Maximum likelihood estimator; Neighborhood selection estimator Gaussian linear regression models, **38**
* Generalized gradient, **57**
* _See also_ Subdifferential Genome application, **2-5**, 10, 25-34, 38, 82, 212-213 Global minimum, **67**
* _See also_ Optimality conditions; (Strictly) convex function Gradient
* of least-squares estimator, 6, **59**
* of log-determinant function, **96**
* of maximum likelihood function in graphical models, **97**
* is special case of subgradient, **57**
* of trace function, **96**
* _See also_ Differentiable function; Subdifferential Gram matrix, **7**
* connection to compatibility condition, **193-194**, 197-198
* is half of Jacobi matrix in Newton-Raphson approach, **143-144**, 145-146, 149-153 (_see also_ Approximate/Moore-Penrose matrix inverse)
* is not invertible in high dimensions, **20**
* summarizes correlations, **40-41**
* _See also_ Correlated predictors Graph, **79**, **84**
* _See also_ Graphical models Graphical lasso estimator, **90**
* computation, **108**
* references, **108**
* tuning parameter, **136**
* _See also_ Maximum likelihood estimator; Neighborhood lasso estimator Graphical models, **84**, 81-107
* data, **84**, 91
* high-dimensional, **85**
* illustration, **82-84**
* references, **107**
* (un)directed, **107**
* _See also_ Conditional (in)dependence; Gaussian graphical models; Maximum likelihood estimator; Neighborhood selection estimator Graph of a function, 54, 55, **79*
* Group-lasso estimator, **43*
* is \(\delta\)-decomposable, **206-207*
* dual function, **66*
* prediction error, **205-206*
* references, **77*
* _See also_ Square-root group-lasso estimator Group sparsity, **43*
* _See also_ Sparsity

H

Hammersley-Clifford theorem, **86** * references, **108** * _See also_ Conditional (in)dependence Hard thresholding, **48** * _See also_ Thresholding Hesse matrix and strict convexity, **249** High-dimensional estimator, **4**, **10-13** * examples in graphical models, **90**, 94 * examples in linear regression, **42-45** * _See also_ Data-fitting function; Prior function/information; Tuning parameter High-dimensional model * curse of dimensionality, **34** * graphical models, **85** (_see also_ Graphical models) * linear regression, **4**, **39** (_see also_ Linear regression) Highly correlated design, **40** * _See also_ Correlated predictors Holder dual function, _see_ Dual function Holder inequality, **48-53** * classical, **51** * general, **50**

[MISSING_PAGE_EMPTY:6576]

Lasso estimator (_cont._) * effective noise, 12, **112**, 113-116, 127, 135-136, 173 (_see also_ Tuning-parameter calibration) * estimation error in \(\ell_{1}\)-loss, **214-215** * estimation error in \(\ell_{\infty}\)-loss, **215-225** * estimation error in \(\ell_{q}\)-loss, **235-236** * explicit form for orthogonal design, **69** * KKT conditions, **63**, 69, 70 * least-squares refitting of, **77**, 78, 137 * maximal tuning parameter, **71** * non-negative lasso estimator, **184**, 205, 209 * prediction error, 35, **170-172**, 210 * reference, **77** * subdifferential, **59**, 68 * support recovery bound, **234**, 236, 237 * thresholding with adaptive validation, **234** (_see also_ Support recovery) * tuning-parameter calibration with adaptive validation, **125** * uniqueness, 69, 70, **236**, 237 * _See also_ Graphical lasso estimator; Group-lasso estimator; Neighborhood lasso estimator; Sparsity; Square-root lasso estimator Law of iterated expectations, **154** Least-squares estimator, **6** * confidence interval for, **149** * effective noise, **9**, 12, 173 * estimation error in \(\ell_{\infty}\)-loss, **235** * explicit form, **7**, 20 * level sets are lines/ellipses, 18, **23-25** * as maximum likelihood estimator, **142** * is not suitable in high dimensions, **5-9** * is numerically unstable, **14-15** * overfits, **42**, 63 * prediction error, **9**, 12, 20, 186 * regularized least-squares estimator, **39**, 170 * as result of Neuron-Raphson, **144**, 145, 154 * role in high-dimensional inference, **146-147** * soft-thresholded least-squares estimator is lasso estimator, **70** * is (strictly) convex and differentiable, **6**, 22, 59 * is unbiased, **12** * uniqueness, **7**, 14, 15 (_see also_ (Strictly) convex function) * as Z-estimator, **142**, 153 * _See also_ Least-squares refitting Least-squares refitting, **46**, 46-48, 78, 137 * _See also_ Strong/weak oracle property Leave-one-out cross-validation **119** * _See also_ Cross-validation Leibniz formula for the determinant, **318** Likelihood function, **88**, 141 * _See also_ Maximum likelihood estimator Linear dynamical system, **326** * _See also_ Eigenvector/eigenvalue of a matrix Linear regression, **39** * confidence interval, **146-153** * data, 5, 13, **39** * design matrix, 5, **39**, 40-41 (_see also_ Correlated predictors; Gram matrix) * singular value decomposition of, **7** * Gaussian linear regression models, **38** * high-dimensional, **39** * intercept, **2**, 5, 34 (_see also_ Data standardization) * model, 2, 5, **38**, 77 * model parameter, 2, 5, **38** (_see also_ Regression vector) * noise, 2, 38, **39**, 111-113 (_see also_ Effective noise) outcome, 2, 5, **39** * predictor, **2**, **39** * 39-41 (_see also_ Design matrix) * exogenous/endogenous, **16** * regression vector, 5, **6**, **38**, 39, 140 * augmented, **93** * can be ambiguous, **195**, 212 * in neighborhood selection, **91-93** * sample, **38** * signal-to-noise ratio, **39** * theory for estimation, **213-231** * theory for prediction, 5-9, **170-210** * theory for support recovery, **231-235** * _See also_ Neighborhood selection Line segment, 54, 55, **79** Link function, **170** * examples, **170** * induced link function, **177** Local minimum, **67** * _See also_ Optimality conditions; (Strictly) convex function Logarithm has basis \(e\), **IX** Log-determinant function Log-gradient of, **96** * is strictly convex, **96** Logistic regression, tuning-parameter calibration for, **136** * loss function, **121** Low-dimensional model * graphical models, **84**, 97 * linear regression, **4**

Mallow's C\({}_{p}\), **121** * _See also_ Tuning-parameter calibration Markov inequality, **156**, 186 Markov random fields, **107** * _See also_ Graphical modelsMatrix algebra, **312**-**336**

Maximum a posteriori estimator, **21**

- _See also_ Bayes

Maximum likelihood estimator

- in Gaussian graphical models, **89**, 87-90

- is differentiable, **97**

- equals neighborhood selection, **97**

- explicit form, 89, **97**

- is strictly convex, **96**

- least-squares estimator, **142**

- regularized version in Gaussian graphical models, **90** (_see also_ Graphical lasso estimator)

- computation, **97**

- _vs._ neighborhood selection, **94**

- as **Z**-estimator, **141**

Max-norm, **X**

- _See also_\(\ell_{q}\)-function

mp estimator, **47**, 79**

- _See also_ Least-squares refitting; Strong/weak oracle property

Metadata, dropping of, **34**

Minimum, local and global, **67**

- _See also_ Optimality conditions; (Strictly) convex function

Minimum over empty set is \(\infty\), **IX**

Minimum trick, **68**

Model parameter, _see_ Linear regression model parameter

Monte Carlo cross-validation, **118**, 120

- illustration, **119**

- _see also_ Cross-validation

Moore-Penrose matrix inverse, **19**

- always exist and are unique, **19**

- generalizes regular matrix inverse, **19**

- in least-squares estimator, **19**

- in Newton-Raphson algorithm, **145**

- references, **35**

- _See also_ Approximate matrix inverse

Moreau-Rockafellar theorem, **59**, 79**

## Appendix N

Neighborhood lasso estimator, **94**

- reference, **108**

- _See also_ Graphical lasso estimator

Neighborhood selection estimator, **94**, 91-94

- equals maximum likelihood estimator, **97**

Newton-Raphson algorithm, **143**, 142-146

- generalized version, **144**, 144-146, 154

- _See also_ Confidence interval; Debiasing

Node, in graphical models, **83**, 84

- Noise vector, **39**

- _See also_ Linear regression

Non-negative lasso estimator, **184**

- in basic inequality, **184**, 209

- dual function, 66, **205**

- prediction error, **205**

- references, **209**

Norm, **IX**

- is convex, **56**

- is \(\delta\)-decomposable, 196, **206**

- dual function is, **65**

- dual norm, **237**

- has many properties, **IX**

- _See also_\(\ell_{q}\)-function

Normal distribution, _see_ Gauss distribution

Normalized predictor/design, **40**

- _See also_ Data transformation

## Appendix O

Objective function, **10**

- _See also_ Convex function; Differentiable

function; First/second-order basic inequality;

Subdifferential

One-step estimator, **146**, 140-146

- _See also_ Confidence interval; Debiasing;

Newton-Raphson algorithm

Optimality conditions, **61**, 53-63

- KKT conditions are special case of, **63**

- _See also_ Convex function; KKT conditions;

Subdifferential

Oracle inequality, **209**

Oracle property

- strong, **47**, 79

- weak, **78**

- _See also_ Least-squares refitting

Orthogonal design, **40**

- lasso estimator in, **69**, 125, 225, 234

- _vs_ orthogonal matrix, **77**

- _See also_ Correlated predictors

Orthogonal matrix, **326**

- determinant equals \(\pm 1\), **327**

- in singular value decomposition, **330**

- _See also_ Singular value decomposition

Orthogonal vectors, **X**

Outcome vector, **39**

- _See also_ Linear regression

Overfitting, **41**

- cross-validation is prone to, **119**

- illustration, **42**
Penalty, **10**

_- See also_ Prior function

Penalty bound, **208**

_- See also_ Power-one prediction bound

Permutation, **318**-**319**

Positive definite function, **IX**

- dual function is, **50**

- in Holder inequality, **50**

- inner product is, **X**

- non-negative lasso estimator is not, **184**

- norm is, **IX**

- prior function might be, **199**

Positive lasso, **209**

- _See also_ Non-negative lasso

Positive part of a number, **69**

Positive (semi-)definite matrix, **317**, 316-318

- covariance and precision matrices are, **86**

- determinant of, **320**

- eigenvalue of, **328**

- gram matrix is, **19**

- inversion of, **321**

Post-processing method, **45**-**48**

- _See also_ Bias; Least-squares refitting; Thresholding

Power-one prediction bound, **185**

- difference to power-two prediction bound, 173, **183**, 188-189, 204, 210

- for lasso estimator, **172**, 174

- name, **185**, 208, 210

- for square-root lasso estimator, **186**

Power-two prediction bound, **188**

- difference to power-one prediction bound, 173, **183**, 188-189, 204, 210

- and estimation, **213**-**217**

- for lasso estimator, **173**

- name, **188**, 208, 210

- in sparse and weakly correlated models, **192**-**204**

- for square-root lasso estimator, **189**

- _See also_ Compatibility condition

Precision matrix, **85**

- _See also_ Gaussian graphical models

Prediction error, **6**, **170**

- ambiguous use of term, **208**

- and estimation of noise variance, **149**-**156**

- dependence on design matrix, 192-204, **210**

- difference to data fit, **6**, 117

- of least-squares estimator, **9**, 12, 20, 186

- of regularized least-squares estimators, **169**-**210**

- relationship to estimation error, 194-196, **212**-**213**, 213-217

- of square-root lasso estimator, **186**-**187**, 189

- _See also_ Cross-validation; Effective noise; Power-one/two prediction bounds

Prediction risk, **6**

- _See also_ Prediction error

Predictor, **2**, **39**, 39-41

- _See also_ Linear regression

Primal-dual pair, **219**

- _vs._ dual function, **219**

- naming, **237**

- references, **237**

- sparsity, **219**

- _vs._ subdifferential, **237**

- successful construction, **221**

- uniqueness, **219**, 236

- _See also_ Bounds in \(\overline{A}\)-loss; Irrepresentability condition; Support recovery

Primal-dual witness technique, **218**

- _See also_ Primal-dual pair; Subdifferential

Primal vector, **219**

- _See also_ Primal-dual pair

Prior function, **10-11**

- can avoid overfitting, **42**

- Bayes perspective on, **21-22**

- convex prior function, **43**, **55**, 56-57, 68, 97 (_see also_ Convex function)

- \(\delta\)-decomposable prior function, **196**, 205-207 (_see also_\(\delta\)-decomposable function)

- dual as loss function, **218**-**231**

- generates bias, **12** (_see also_ Bias)

- Holder inequality for, **50**-**51**, 66-67 (_see also_ Dual function; Holder inequality)

- can increase numerical stability, **13**-**17**

- as loss function, **213**-**217**

- sparsity-inducing prior function, **41**-**45** (_see also_ Sparsity)

- strictly convex prior function selects correlated predictors, **78**

- _See also_ Bayes perspective on regularization; Compatibility condition; Irrepresentability condition; Tuning parameter

Prior information, **2**, **4**, **10**-**13**

- _See also_ Prior function; Sparsity

Probabilistic graphical models, **107**

- _See also_ Graphical models

Probability theory, reference for, **107**

Pseudoinverse, _see_ Approximate matrix inverse; Moore-Penrose matrix inverse

Pythia, **209**

R

\(R\)2, 3, **34**

- adjusted, **34**Rank of a matrix, **330**

Reciprocal number, **321**

- _See also_ Inverse of a matrix

Refitting, _see_ Least-squares refitting

Regression vector, **38**, 147

- _See also_ Linear regression

Regularized least-squares estimator, **39**, 170

- effective noise of, **190** (_see also_ Effective noise)

- examples, **42-45**

- and Holder inequality, **50-51**

- KKT conditions, **63**

- _See also_ Least-squares estimator; Prior

function/information; Tuning parameter

Regularized maximum likelihood estimator, **90**

- _See also_ Maximum likelihood estimator

Regularizer, **10**

- _See also_ Prior function

Relaxed lasso, **78**

- _See also_ Least-squares refitting

Repeated random subsampling validation, **118**

- _See also_ Cross-validation

Restricted eigenvalue condition, **210**

- _See also_ Compatibility condition

Restricted strong convexity, **210**

- _See also_ Compatibility condition

Ridge estimator, **15**

- Bayes perspective on, **21**

- cross-validation of, **136**

- is \(\delta\)-decomposable, 196, **206**

- difference to bridge estimator, **77**

- difference to elastic net estimator, **45**

- difference to lasso estimator, 43, **44**

- does not satisfy Holder inequality, **67**

- dual function, **67**

- _vs._ edr estimator, **78**

- explicit expression, **16**

- _vs._ least-squares estimator, **13-17**, 23

- is not sparse, 43, **44**

- is numerically stable, **13-17**, 22

- reference, **35**

- is strictly convex and differentiable, **15**, **54**

- uniqueness, **16**

* _See also_ Least-squares refitting; Strong/weak oracle property

Scalable function, **50**

- is not necessarily absolute homogenous, **50**, **79**

- norm is, **IX**

- Scaled lasso estimator, **209**

- _See also_ Square-root lasso estimator

Score function, **141**

- _See also_ Maximum likelihood estimator

Second-order basic inequality, **178**, 177-182, 209

- difference to first-order basic inequality,

**178-179**, 203-205

- name, **179**

- reference, **209**

- _See also_ Basic inequality; Subdifferential

Sharp bound, **173**

Signal-to-noise ratio, **39**

Sign consistent support recovery, **233**

Signum function **IX**, 59, 61, 233

- of a permutation, **318-319**

Singular value decomposition, 7, 20, **330**,

329-330

- _See also_ Spectral decomposition

Singular value of a matrix, **330**

- relation to determinant, **96**

Slow-rate bound, **210**

- _See also_ Power-one prediction bound

Soft thresholding, **70**

Sparse group-lasso estimator, **77**

- _See also_ Lasso/group-lasso estimator

Sparsity, 4, 13, **42**, 90, 195

- approximate sparsity, **195**

- desparsifying, **168** (_see also_ Debiasing)

- group sparsity, **43** (_see also_ Square-root

group-lasso estimator)

- and \(\delta\)-estimation bounds, **213-217**

- and \(\overline{\delta}\)-estimation bounds, **218-231**

- and prediction bounds, **192-204**

- sparse estimators, **41-45**

- structured sparsity, **43**

- and support recovery, **231-235**

- and tuning parameters, **110-113**

- _See also_ Prior function

Sparsity bound, **208**

- _See also_ Power-two prediction bound

Spectral decomposition, **327**

- _See also_ Singular value decomposition

Square-root group-lasso estimator, **77**, 168

- prediction error, **203-204**

- _See also_ Square-root lasso estimator;

Group-lasso estimator

Square-root lasso estimator, 170, **178**

- difference to lasso estimator, **209**

- prediction error, **186-187**, 189, 210

- references, **209**

- _See also_ Square-root group-lasso estimator;

Trex estimator, Tuning parameter

Square-root of a matrix, **328**

Standard inner product, _see_ Inner product

Standardizing data, **89**

- _See also_ Data transformation Strictly convex function, **54** * can have zero or one minimum, 22, **68** * combinations of strictly convex functions are, **55** * connection to Hesse matrix, **249** * elastic net estimator is, **78** * examples/counterexamples, 22, **55** * generalization to functions of matrices, **95** * illustration, **55** * least-squares estimator is, **22** * local minimum is global minimum, **68** * log-determinant function is, **96** * maximum likelihood estimator in Gaussian graphical models is, **96** * restricted strong convexity, **210** * ridge estimator is, **54**, 55 * sufficient and necessary condition, **249** * _See also_ Convex function Strong oracle property, **47**, 78, 79 * _See also_ Least-squares refitting Structural condition, 172, **188** * limits design correlations, **193** * is related to compatibility condition, 197, **200** * _See also_ Compatibility condition; Double-cone; Power-two prediction bound Structured sparsity, **43** * _See also_ Sparsity Subdecomposable function, **228** * _See also_\(\delta\)-decomposable function Subdifferentiant, **57** * generalizes gradient, **57** * illustration, **61**, 68 * lasso estimator, **59**, 68-69 * is linear, **59**, 68 * of \(\ell_{1}\)-norm, **59** * is not empty, **79** * r. primal-dual pair, **237** * _See also_ Convex function; Optimality conditions; Primal-dual witness technique; Second-order basic inequality Subfunction, **196** * _See also_\(\delta\)-decomposable function Subgradient, **57** * _See also_ Subdifferential Sup-norm, **X** * _See also_\(\ell_{q}\)-function Support function, **79** * _See also_ Dual function Support of a vector, **IX** Support recovery, **231-235** * bounds for sign consistent support recovery, **233** * false positives/negatives, **231** * with the lasso, **234** * variable screening, **233** * _See also_ Refitting; Thresholding Symmetric and positive definite matrix, **89** * covariance and precision matrices are, **86** Symmetric function * dual function is, **65** * non-negative lasso estimator is not, **184** * norm is, **IX** * trace function is, **323** * _See also_ Absolute homogeneous function Symmetric matrix, **314**, 314-316 * _See also_ Spectral decomposition

Tail bound * for effective noise, **113**, 128 * for Gauss distribution, **127-128**, 156, 286 Taylor expansion in Newton-Raphson approach, **142-143** * _See also_ Newton-Raphson algorithm Thresholding, **48**, 70, 90, 232-233 * hard, **48** * soft, **70** * _See also_ Adaptive validation; Support recovery Tower rule, **154** Trace of a matrix, **323**, 323-325 * is convex, **95** * gradient, **96** * is linear and symmetric, **323** * Training set, **117** * _See also_ Cross-validation Transposition of a matrix, **314**, 314-316 * properties, **316** * relation to inner product, **315** * Test estimator, **209** Triangle inequality * assumed for loss function, **121** * assumed for prior function, **184** * assumed for subfunction, **199** * dual function satisfies, **65** * norm satisfies, **IX** * prediction loss does not satisfy, **137** Tuning parameter, **10** * calibration, **110-137** (_see also_ Adaptive validation; AIC; BIC; Cross-validation; edr estimator; Square-root (group-)lasso estimator; Trex estimator) * connection to Bayes statistics, **36** * in correlated designs, **135** * effect on double-cone, **201** * effect on numerical stability, **13-17** * lasso estimator, **70** * multiple/vector-valued, 46, 47, **125** * and numerical stability, **23*** _See also_ Effective noise; Power-one/power-two prediction bound; Thresholding
* Undirected graphical models, **107**
* _See also_ Graphical models Union bound, 114, **127**

Validation set, **118**

* _See also_ Cross-validation Variable screening, **233**
* _See also_ Support recovery Variable selection, **231**
* _See also_ Support recovery Variance, XI, **11**
* bias-variance decomposition, **11**
* covariance matrix, XI, 85 (_see also_ Graphical models)
* in cross-validation, **120**, 136
* of least-squares estimator, **12*
* Vertex, in graphical models, **83**