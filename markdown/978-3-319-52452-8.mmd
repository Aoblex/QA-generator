[MISSING_PAGE_EMPTY:1]

## Springer Texts in Statistics

_Series Editors_

Richard DeVeux

Stephen E. Fienberg

Ingram Olkin

More information about this series at [http://www.springer.com/series/417](http://www.springer.com/series/417)Robert H. Shumway  David S. Stoffer

**Time Series Analysis**

**and Its Applications**

**With R Examples**

**Fourth Edition**Robert H. Shumway

Department of Statistics

University of California, Davis

Davis, CA, USA

David S. Stoffer

Department of Statistics

University of Pittsburgh

Pittsburgh, PA, USA

ISSN 1431-875X

ISSN 2197-4136 (electronic)

ISBN 978-3-319-52451-1

ISBN 978-3-319-52452-8 (eBook)

Library of Congress Control Number: 2017930675

(c) Springer International Publishing AG 1999, 2012, 2016, 2017

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

### Printed on acid-free paper

This Springer imprint is published by Springer Nature

The registered company is Springer International Publishing AG

The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

## Preface to the Fourth Edition

The fourth edition follows the general layout of the third edition but includes some modernization of topics as well as the coverage of additional topics. The preface to the third edition--which follows--still applies, so we concentrate on the differences between the two editions here. As in the third edition, R code for each example is given in the text, even if the code is excruciatingly long. Most of the examples with seemingly endless coding are in the latter chapters. The R package for the text, astsa, is still supported and details may be found in Appendix R. The global temperature deviation series have been updated to 2015 and are included in the newest version of the package; the corresponding examples and problems have been updated accordingly.

Chapter 1 of this edition is similar to the previous edition, but we have included the definition of trend stationarity and the concept of prewhitening when using cross-correlation. The New York Stock Exchange data set, which focused on an old financial crisis, was replaced with a more current series of the Dow Jones Industrial Average, which focuses on a newer financial crisis. In Chap. 2, we rewrote some of the regression review, changing the smoothing examples from the mortality data example to the Southern Oscillation Index and finding El Nino. We also expanded on the lagged regression example and carried it on to Chap. 3.

In Chap. 3, we removed normality from definition of ARMA models; while the assumption is not necessary for the definition, it is essential for inference and prediction. We added a section on regression with ARMA errors and the corresponding problems; this section was previously in Chap. 5. Some of the examples have been modified and we added some examples in the seasonal ARMA section. Finally, we included a discussion of lagged regression with autocorrelated errors.

In Chap. 4, we improved and added some examples. The idea of modulated series is discussed using the classic star magnitude data set. We moved some of the filtering section forward for easier access to information when needed. We removed the reliance on spec.pgram (from the stats package) to mvspec (from the astsa package) so we can avoid having to spend pages explaining the quirks of spec.pgram,which tended to take over the narrative. The section on wavelets was removed because there are so many accessible texts available. The spectral representation theorems are discussed in a little more detail using examples based on simple harmonic processes.

The general layout of Chap. 5 and of Chap. 7 is the same, although we have revised some of the examples. As previously mentioned, we moved regression with ARMA errors to Chap. 3.

Chapter 6 sees the biggest change in this edition. We have added a section on smoothing splines, and a section on hidden Markov models and switching autoregressions. The Bayesian section is completely rewritten and is on linear Gaussian state space models only. The nonlinear material in the previous edition is removed because it was old, and the newer material is in Douc, Moulines, and Stoffer [53]. Many of the examples have been rewritten to make the chapter more accessible.

The appendices are similar, with some minor changes to Appendix A and Appendix B. We added material to Appendix C, including a discussion of Riemann-Stieltjes and stochastic integration, a proof of the fact that the spectra of autoregressive processes are dense in the space of spectral densities, and a proof of the fact that spectra are approximately the eigenvalues of the covariance matrix of a stationary process.

We tweaked, rewrote, improved, or revised some of the exercises, but the overall ordering and coverage is roughly the same. And, of course, we moved regression with ARMA errors problems to Chap. 3 and removed the Chap. 4 wavelet problems. The exercises for Chap. 6 have been updated accordingly to reflect the new and improved version of the chapter.

\begin{tabular}{l c} Davis, CA, USA & Robert H. Shumway \\ Pittsburgh, PA, USA & David S. Stoffer \\ December 2016 & \\ \end{tabular}

## Preface to the Third Edition

The goals of this book are to develop an appreciation for the richness and versatility of modern time series analysis as a tool for analyzing data, and still maintain a commitment to theoretical integrity, as exemplified by the seminal works of Brillinger [33] and Hannan [86] and the texts by Brockwell and Davis [36] and Fuller [66]. The advent of inexpensive powerful computing has provided both real data and new software that can take one considerably beyond the fitting of simple time domain models, such as have been elegantly described in the landmark work of Box and Jenkins [30]. This book is designed to be useful as a text for courses in time series on several different levels and as a reference work for practitioners facing the analysis of time-correlated data in the physical, biological, and social sciences.

We have used earlier versions of the text at both the undergraduate and graduate levels over the past decade. Our experience is that an undergraduate course can be accessible to students with a background in regression analysis and may include Sects. 1.1-1.5, Sects. 2.1-2.3, the results and numerical parts of Sects. 3.1-3.9, and briefly the results and numerical parts of Sects. 4.1-4.4. At the advanced undergraduate or master's level, where the students have some mathematical statistics background, more detailed coverage of the same sections, with the inclusion of extra topics from Chaps. 5 or 6, can be used as a one-semester course. Often, the extra topics are chosen by the students according to their interests. Finally, a two-semester upper-level graduate course for mathematics, statistics, and engineering graduate students can be crafted by adding selected theoretical appendices. For the upper-level graduate course, we should mention that we are striving for a broader but less rigorous level of coverage than that which is attained by Brockwell and Davis [36], the classic entry at this level.

The major difference between this third edition of the text and the second edition is that we provide R code for almost all of the numerical examples. An R package called astsa is provided for use with the text; see Sect. R.2 for details. R code is provided simply to enhance the exposition by making the numerical examples reproducible.

We have tried, where possible, to keep the problem sets in order so that an instructor may have an easy time moving from the second edition to the third edition. However, some of the old problems have been revised and there are some new problems. Also, some of the data sets have been updated. We added one section in Chap. 5 on unit roots and enhanced some of the presentations throughout the text. The exposition on state-space modeling, ARMAX models, and (multivariate) regression with autocorrelated errors in Chap. 6 have been expanded. In this edition, we use standard R functions as much as possible, but we use our own scripts (included in astsa) when we feel it is necessary to avoid problems with a particular R function; these problems are discussed in detail on the website for the text under R Issues.

We thank John Kimmel, Executive Editor, Springer Statistics, for his guidance in the preparation and production of this edition of the text. We are grateful to Don Percival, University of Washington, for numerous suggestions that led to substantial improvement to the presentation in the second edition, and consequently in this edition. We thank Doug Wiens, University of Alberta, for help with some of the R code in Chaps. 4 and 7, and for his many suggestions for improvement of the exposition. We are grateful for the continued help and advice of Pierre Duchesne, University of Montreal, and Alexander Aue, University of California, Davis. We also thank the many students and other readers who took the time to mention typographical errors and other corrections to the first and second editions. Finally, work on this edition was supported by the National Science Foundation while one of us (D.S.S.) was working at the Foundation under the Intergovernmental Personnel Act.

Davis, CA, USA Robert H. Shumway

Pittsburgh, PA, USA David S. Stoffer

September 2010

[MISSING_PAGE_FAIL:9]

* 4Spectral Analysis and Filtering
	* 4.1 Cyclical Behavior and Periodicity
	* 4.2 The Spectral Density
	* 4.3 Periodogram and Discrete Fourier Transform
	* 4.4 Nonparametric Spectral Estimation
	* 4.5 Parametric Spectral Estimation
	* 4.6 Multiple Series and Cross-Spectra
	* 4.7 Linear Filters
	* 4.8 Lagged Regression Models
	* 4.9 Signal Extraction and Optimum Filtering
* 4.10 Spectral Analysis of Multidimensional Series Problems
* 5Additional Time Domain Topics
	* 5.1 Long Memory ARMA and Fractional Differencing
	* 5.2 Unit Root Testing
	* 5.3 GARCH Models
	* 5.4 Threshold Models
	* 5.5 Lagged Regression and Transfer Function Modeling
	* 5.6 Multivariate ARMAX Models
	* 5.2 Unit Root Testing
	* 5.3 GARCH Models
	* 5.4 Threshold Models
	* 5.5 Lagged Regression and Transfer Function Modeling
	* 5.6 Multivariate ARMAX Models
	* 5.7 Problems
* 6State Space Models
	* 6.1 Linear Gaussian Model
	* 6.2 Filtering, Smoothing, and Forecasting
	* 6.3 Maximum Likelihood Estimation
	* 6.4 Missing Data Modifications
	* 6.5 Structural Models: Signal Extraction and Forecasting
	* 6.6 State-Space Models with Correlated Errors
		* 6.6.1 ARMAX Models
		* 6.6.2 Multivariate Regression with Autocorrelated Errors
	* 6.7 Bootstrapping State Space Models
	* 6.8 Smoothing Splines and the Kalman Smoother
	* 6.9 Hidden Markov Models and Switching Autoregression
* 6.10 Dynamic Linear Models with Switching
* 6.11 Stochastic Volatility
* 6.12 Bayesian Analysis of State Space Models
* 6.13 Problems
* 7Statistical Methods in the Frequency Domain
	* 7.1 Introduction
	* 7.2 Spectral Matrices and Likelihood Functions
	* 7.3 Regression for Jointly Stationary Series
	* 7.4 Regression with Deterministic Inputs
	* 7.5 Random Coefficient Regression
Analysis of Designed Experiments * 7.7 Discriminant and Cluster Analysis * 7.8 Principal Components and Factor Analysis * 7.9 The Spectral Envelope Problems
	* 7.6 Analysis of Designed Experiments
	* 7.7 Discriminant and Cluster Analysis
	* 7.8 Principal Components and Factor Analysis
	* 7.9 The Spectral Envelope Problems
	* 7.8 Large Sample Theory
	* 7.1 Convergence Modes
	* 7.2 Central Limit Theorems
	* 7.3 The Mean and Autocorrelation Functions
	* 7.1 Time Domain Theory
	* 7.1 Hilbert Spaces and the Projection Theorem
	* 7.2 Causal Conditions for ARMA Models
	* 7.3 Large Sample Distribution of the AR Conditional Least Squares Estimators
	* 7.4 The Wold Decomposition
	* 7.5 Spectral Domain Theory
	* 7.6 Spectral Representation Theorems
	* 7.7 Large Sample Distribution of the Smoothed Periodogram
	* 7.7 The Complex Multivariate Normal Distribution
	* 7.8 Principal Components and Factor Analysis
	* 7.9 The Spectral Envelope Problems
	* 7.1 First Things First
	* 7.3 Getting Started
	* 7.4 Time Series Primer
		* 7.4.1 Graphics
* 7.5

## Biography

**Robert H. Shumway** is Professor Emeritus of Statistics at the University of California, Davis. He is a Fellow of the American Statistical Association and a member of the International Statistical Institute. He won the 1986 American Statistical Association Award for Outstanding Statistical Application and the 1992 Communicable Diseases Center Statistics Award; both awards were for joint papers on time series applications. He is the author of a previous 1988 Prentice-Hall text on applied time series analysis and served as a Departmental Editor for the _Journal of Forecasting_ and Associate Editor for the _Journal of the American Statistical Association_.

**David S. Stoffer** is Professor of Statistics at the University of Pittsburgh. He is a Fellow of the American Statistical Association. He has made seminal contributions to the analysis of categorical time series and won the 1989 American Statistical Association Award for Outstanding Statistical Application in a joint paper analyzing categorical time series arising in infant sleep-state cycling. He is also coauthor of the highly acclaimed text, _Nonlinear Time Series: Theory, Methods and Applications with R Examples_. He is currently Co-Editor of the _Journal of Time Series Analysis_, Departmental Editor of the _Journal of Forecasting_, and an Associate Editor of the _Annals of Statistical Mathematics_. He has served as a Program Director in the Division of Mathematical Sciences at the National Science Foundation and as an Associate Editor for the _Journal of Business and Economic Statistics_ and the _Journal of the American Statistical Association_.

## Chapter 1 Characteristics of Time Series

The analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference. The obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed. The systematic approach by which one goes about answering the mathematical and statistical questions posed by these time correlations is commonly referred to as time series analysis.

The impact of time series analysis on scientific applications can be partially documented by producing an abbreviated listing of the diverse fields in which important time series problems may arise. For example, many familiar time series occur in the field of economics, where we are continually exposed to daily stock market quotations or monthly unemployment figures. Social scientists follow population series, such as birthrates or school enrollments. An epidemiologist might be interested in the number of influenza cases observed over some time period. In medicine, blood pressure measurements traced over time could be useful for evaluating drugs used in treating hypertension. Functional magnetic resonance imaging of brain-wave time series patterns might be used to study how the brain reacts to certain stimuli under various experimental conditions.

In our view, the first step in any time series investigation always involves careful examination of the recorded data plotted over time. This scrutiny often suggests the method of analysis as well as statistics that will be of use in summarizing the information in the data. Before looking more closely at the particular statistical methods, it is appropriate to mention that two separate, but not necessarily mutually exclusive, approaches to time series analysis exist, commonly identified as the _time domain approach_ and the _frequency domain approach_. The time domain approach views the investigation of lagged relationships as most important (e.g., how does what happened today affect what will happen tomorrow), whereas the frequencydomain approach views the investigation of cycles as most important (e.g., what is the economic cycle through periods of expansion and recession). We will explore both types of approaches in the following sections.

### The Nature of Time Series Data

Some of the problems and questions of interest to the prospective time series analyst can best be exposed by considering real experimental data taken from different subject areas. The following cases illustrate some of the common kinds of experimental time series data as well as some of the statistical questions that might be asked about such data.

**Example 1**: **Johnson & Johnson Quarterly Earnings**

Figure 1 shows quarterly earnings per share for the U.S. company Johnson & Johnson, furnished by Professor Paul Griffin (personal communication) of the Graduate School of Management, University of California, Davis. There are 84 quarters (21 years) measured from the first quarter of 1960 to the last quarter of 1980. Modeling such series begins by observing the primary patterns in the time history. In this case, note the gradually increasing underlying trend and the rather regular variation superimposed on the trend that seems to repeat over quarters. Methods for analyzing data such as these are explored in Chaps. 2 and 6. To plot the data using the R statistical package, type the following1:

Footnote 1: Throughout the text, we assume that the R package for the book, astsa, has been installed and loaded. See Sect. R.2 for further details.

library(astsa) # SEE THE FOOTNOTE plot(jj, type="o", ylab="Quarterly Earnings per Share")

Figure 1: Johnson & Johnson quarterly earnings per share, 84 quarters, 1960-I to 1980-IV

**Example 1.2**: _Global Warming Consider the global temperature series record shown in Fig. 1.2. The data are the global mean land-ocean temperature index from 1880 to 2015, with the base period 1951-1980. In particular, the data are deviations, measured in degrees centigrade, from the 1951-1980 average, and are an update of Hansen et al. [89]. We note an apparent upward trend in the series during the latter part of the twentieth century that has been used as an argument for the global warming hypothesis. Note also the leveling off at about 1935 and then another rather sharp upward trend at about 1970. The question of interest for global warming proponents and opponents is whether the overall trend is natural or whether it is caused by some human-induced interface. Problem 2.8 examines 634 years of glacial sediment data that might be taken as a long-term temperature proxy. Such percentage changes in temperature do not seem to be unusual over a time period of 100 years. Again, the question of trend is of more interest than particular periodicities. The R code for this example is similar to the code in Example 1.1: plot(globtemp, type="o", ylab="Global Temperature Deviations")

**Example 1.3**: _Speech Data Figure 1.3 shows a small.1 second (1000 point) sample of recorded speech for the phrase \(aaa\cdots hhh\), and we note the repetitive nature of the signal and the rather regular periodicities. One current problem of great interest is computer recognition of speech, which would require converting this particular signal into the recorded phrase \(aaa\cdots hhh\). Spectral analysis can be used in this context to produce a signature of this phrase that can be compared with signatures of various library syllables to look for a match. One can immediately notice the rather regular repetition of small wavelets. The separation between the packets is known as the pitch period and represents the response of the vocal tract filter to a periodic sequence of pulses stimulated by the opening and closing of the glottis. In R, you can reproduce Fig. 1.3 using plot(speech)._

Figure 1.2: Yearly average global temperature deviations (1880–2015) in degrees centigrade

**Example 4**: **Dow Jones Industrial Average**

As an example of financial time series data, Fig. 4 shows the daily _returns_ (or percent change) of the Dow Jones Industrial Average (DJIA) from April 20, 2006 to April 20, 2016. It is easy to spot the financial crisis of 2008 in the figure. The data shown in Fig. 4 are typical of return data. The mean of the series appears to be stable with an average return of approximately zero, however, highly volatile (variable) periods tend to be clustered together. A problem in the analysis of these type of financial data is to forecast the volatility of future returns. Models such as _ARCH_ and _GARCH_ models (Engle [57]; Bollerslev [28]) and _stochastic volatility_ models (Harvey, Ruiz and Shephard [94]) have been developed to handle these problems. We will discuss these models and the analysis of financial data in Chaps. 5 and 6. The data were obtained using the Technical Trading Rules (TTR) package to download the data from Yahoo(tm) and then plot it. We then used the fact that if \(x_{t}\) is the actual value of the DJIA and \(r_{t}=(x_{t}-x_{t-1})/x_{t-1}\) is the return, then

Figure 4: The daily returns of the Dow Jones Industrial Average (DJIA) from April 20, 2006 to April 20, 2016

Figure 3: Speech recording of the syllable \(aaa\cdots hhh\) sampled at 10,000 points per second with \(n=1020\) points\(1+r_{t}=x_{t}/x_{t-1}\) and \(\log(1+r_{t})=\log(x_{t}/x_{t-1})=\log(x_{t})-\log(x_{t-1})\approx r_{t}\).2 The data set is also available in astsa, but xts must be loaded.

Footnote 2: \(\log(1+p)=p-\frac{p^{2}}{2}+\frac{p^{3}}{3}-\cdots\) for \(-1<p\leq 1\). If \(p\) is near zero, the higher-order terms in the expansion are negligible.

```
#library(TTR)
#djia=getYahooData("DJI",start=20060420,end=20160420,freq="daily") library(xts) djia=diff(log(djiaClose))[-1]#approximatereturns plot(djia,main="DJIa Returns",type="n") lines(djia)
```

**Example 5**: **El Nino and Fish Population**

We may also be interested in analyzing several time series at once. Figure 5 shows monthly values of an environmental series called the _Southern Oscillation Index_ (SOI) and associated Recruitment (number of new fish) furnished byDr. Roy Mendelssohn of the Pacific Environmental Fisheries Group (personal communication). Both series are for a period of 453 months ranging over the years 1950-1987. The SOI measures changes in air pressure, related to sea surface temperatures in the central Pacific Ocean. The central Pacific warms every three to seven years due to the El Nino effect, which has been blamed for various global extreme weather events. Both series in Fig. 5 exhibit repetitive behavior, with regularly repeating _cycles_ that are easily visible. This periodic behavior is of interest because underlying processes of interest may be regular and the rate or _frequency_ of oscillation characterizing the behavior of the underlying series would help to identify them. The series show two basic oscillations types, an obvious annual cycle (hot in the summer, cold in the winter), and a slower frequency that seems to repeat about every 4 years. The study of the kinds of cycles and their strengths is the subject of Chap. 4. The two series are also related; it is easy to imagine the fish population is dependent on the ocean temperature. This possibility suggests trying some version of regression analysis as a procedure for relating the two series. _Transfer function modeling_, as considered in Chap. 5, can also be applied in this case. The following R code will reproduce Fig. 5:

```
par(mfrow=c(2,1))#setupthegraphics plot(soi,ylab="",xlab="",main="SouthernOscillationIndex") plot(rec,ylab="",xlab="",main="Recruitment")
```

**Example 6**: **fMRI Imaging**

A fundamental problem in classical statistics occurs when we are given a collection of independent series or vectors of series, generated under varying experimental conditions or treatment configurations. Such a set of series is shown in Fig. 6, where we observe data collected from various locations in the brain via functional magnetic resonance imaging (fMRI). In this example, five subjects were given periodic brushing on the hand. The stimulus was applied for 32 seconds and then stopped for 32 seconds; thus, the signal period is 64 seconds. The sampling rate was one observation every 2 seconds for 256 seconds (\(n=128\)). For this example, we averaged the results over subjects (these were evoked responses, and all subjects were in phase). The series shown in Fig. 6 are consecutive measures of blood oxygenation-level dependent (bold) signal intensity, which measures areas of activation in the brain. Notice that the periodicities appear strongly in the motor cortex series and less strongly in the thalamus and cerebellum. The fact that one has series from different areas of the brain suggests testing whether the areas are responding differently to the brush stimulus. Analysis of variance techniques accomplish this in classical statistics, and we show in Chap. 7 how these classical techniques extend to the time series case, leading to a spectral analysis of variance. The following R commands can be used to plot the data:

par(mfrow=c(2,1)) ts.plot(fmri1[,2:5], col=1:4, ylab="BOLD", main="Cortex") ts.plot(fmri1[,6:9], col=1:4, ylab="BOLD", main="Thalamus & Cerebellum")

**Example 7**: **Earthquakes and Explosions**

As a final example, the series in Fig. 7 represent two phases or arrivals along the surface, denoted by P (\(t=1,\ldots,1024\)) and S (\(t=1025,\ldots,2048\)), at a seismic

Figure 5: Monthly SOI and Recruitment (estimated new fish), 1950–1987

recording station. The recording instruments in Scandinavia are observing earthquakes and mining explosions with one of each shown in Fig. 7. The general problem of interest is in distinguishing or discriminating between waveforms generated by earthquakes and those generated by explosions. Features that may be important are the rough amplitude ratios of the first phase P to the second phase S, which tend to be smaller for earthquakes than for explosions. In the case of the two events in Fig. 7, the ratio of maximum amplitudes appears to be somewhat less than.5 for the earthquake and about 1 for the explosion. Otherwise, note a subtle difference exists in the periodic nature of the S phase for the earthquake. We can again think about spectral analysis of variance for testing the equality of the periodic components of earthquakes and explosions. We would also like to be able to classify future P and S components from events of unknown origin, leading to the _time series discriminant analysis_ developed in Chap. 7.

To plot the data as in this example, use the following commands in R:

par(mfrow=c(2,1)) plot(E05, main="Earthquake") plot(EXP6, main="Explosion")

Figure 6: fMRI data from various locations in the cortex, thalamus, and cerebellum; \(n=128\) points, one observation taken every 2 s

### Time Series Statistical Models

The primary objective of time series analysis is to develop mathematical models that provide plausible descriptions for sample data, like that encountered in the previous section. In order to provide a statistical setting for describing the character of data that seemingly fluctuate in a random fashion over time, we assume a _time series_ can be defined as a collection of random variables indexed according to the order they are obtained in time. For example, we may consider a time series as a sequence of random variables, \(x_{1},x_{2},x_{3},\ldots\), where the random variable \(x_{1}\) denotes the value taken by the series at the first time point, the variable \(x_{2}\) denotes the value for the second time period, \(x_{3}\) denotes the value for the third time period, and so on. In general, a collection of random variables, \(\{x_{t}\}\), indexed by \(t\) is referred to as a _stochastic process_. In this text, \(t\) will typically be discrete and vary over the integers \(t=0,\pm 1,\pm 2,\ldots\), or some subset of the integers. The observed values of a stochastic process are referred to as a _realization_ of the stochastic process. Because it will be clear from the context of our discussions, we use the term _time series_ whether we are referring generically to the process or to a particular realization and make no notational distinction between the two concepts.

Figure 7: Arrival phases from an earthquake (_top_) and explosion (_bottom_) at 40 points per secondIt is conventional to display a sample time series graphically by plotting the values of the random variables on the vertical axis, or ordinate, with the time scale as the abscissa. It is usually convenient to connect the values at adjacent time periods to reconstruct visually some original hypothetical continuous time series that might have produced these values as a discrete sample. Many of the series discussed in the previous section, for example, could have been observed at any continuous point in time and are conceptually more properly treated as _continuous time series_. The approximation of these series by _discrete time parameter series_ sampled at equally spaced points in time is simply an acknowledgment that sampled data will, for the most part, be discrete because of restrictions inherent in the method of collection. Furthermore, the analysis techniques are then feasible using computers, which are limited to digital computations. Theoretical developments also rest on the idea that a continuous parameter time series should be specified in terms of finite-dimensional _distribution functions_ defined over a finite number of points in time. This is not to say that the selection of the sampling interval or rate is not an extremely important consideration. The appearance of data can be changed completely by adopting an insufficient sampling rate. We have all seen wheels in movies appear to be turning backwards because of the insufficient number of frames sampled by the camera. This phenomenon leads to a distortion called _aliasing_ (see Sect. 4.1).

The fundamental visual characteristic distinguishing the different series shown in Example 1.1-Example 1.7 is their differing degrees of smoothness. One possible explanation for this smoothness is that it is being induced by the supposition that adjacent points in time are _correlated_, so the value of the series at time \(t\), say, \(x_{t}\), depends in some way on the past values \(x_{t-1},x_{t-2},\ldots\). This model expresses a fundamental way in which we might think about generating realistic-looking time series. To begin to develop an approach to using collections of random variables to model time series, consider Example 1.8.

**Example 1.8**: **White Noise (3 Flavors)**

A simple kind of generated series might be a collection of uncorrelated random variables, \(w_{t}\), with mean 0 and finite variance \(\sigma_{w}^{2}\). The time series generated from uncorrelated variables is used as a model for noise in engineering applications, where it is called _white noise_; we shall denote this process as \(w_{t}\sim\operatorname{wn}(0,\sigma_{w}^{2})\). The designation white originates from the analogy with white light and indicates that all possible periodic oscillations are present with equal strength.

We will sometimes require the noise to be independent and identically distributed (iid) random variables with mean 0 and variance \(\sigma_{w}^{2}\). We distinguish this by writing \(w_{t}\sim\operatorname{iid}(0,\sigma_{w}^{2})\) or by saying _white independent noise_ or _iid noise_. A particularly useful white noise series is _Gaussian white noise_, wherein the \(w_{t}\) are independent normal random variables, with mean 0 and variance \(\sigma_{w}^{2}\); or more succinctly, \(w_{t}\sim\operatorname{iid}\operatorname{N}(0,\sigma_{w}^{2})\). Figure 1.8 shows in the upper panel a collection of 500 such random variables, with \(\sigma_{w}^{2}=1\), plotted in the order in which they were drawn. The resulting series bears a slight resemblance to the explosion in Fig. 1.7 but is not smooth enough to serve as a plausible model for any of the other experimental series. The plot tends to show visually a mixture of many different kinds of oscillations in the white noise series.

If the stochastic behavior of all time series could be explained in terms of the white noise model, classical statistical methods would suffice. Two ways of introducing serial correlation and more smoothness into time series models are given in Example 1.9 and Example 1.10.

**Example 1.9**: **Moving Averages and Filtering**

We might replace the white noise series \(w_{t}\) by a _moving average_ that smooths the series. For example, considerreplacing \(w_{t}\) in Example 1.8 by an average of its current value and its immediate neighbors in the past and future. That is, let

\[v_{t}=\tfrac{1}{3}\big{(}w_{t-1}+w_{t}+w_{t+1}\big{)}, \tag{1.1}\]

which leads to the series shown in the lower panel of Fig. 1.8. Inspecting the series shows a smoother version of the first series, reflecting the fact that the slower oscillations are more apparent and some of the faster oscillations are taken out. We begin to notice a similarity to the SOI in Fig. 1.5, or perhaps, to some of the fMRI series in Fig. 1.6.

Figure 1.8: Gaussian white noise series (_top_) and three-point moving average of the Gaussian white noise series (_bottom_)

A linear combination of values in a time series such as in (1) is referred to, generically, as a filtered series; hence the command filter in the following code for Fig. 8.

```
w=rnorm(500,0,1)#500N(0,1)variates v=filter(w,sides=2,filter=rep(1/3,3))#movingaverage par(mfrown=c(2,1)) plot.ts(w,main="whitenoise") plot.ts(v,ylim=c(-3,3),main="movingaverage")
```

The speech series in Fig. 3 and the Recruitment series in Fig. 5, as well as some of the MRI series in Fig. 6, differ from the moving average series because one particular kind of oscillatory behavior seems to predominate, producing a sinusoidal type of behavior. A number of methods exist for generating series with this quasi-periodic behavior; we illustrate a popular one based on the autoregressive model considered in Chap. 3.

**Example 10**: **Autoregressions**__

Suppose we consider the white noise series \(w_{t}\) of Example 1.8 as input and calculate the output using the second-order equation

\[x_{t}=x_{t-1}-.9x_{t-2}+w_{t} \tag{2}\]

successively for \(t=1,2,\ldots,500\). Equation (2) represents a regression or prediction of the current value \(x_{t}\) of a time series as a function of the past two values of the series, and, hence, the term _autoregression_ is suggested for this model. A problem with startup values exists here because (2) also depends on the initial conditions \(x_{0}\) and \(x_{-1}\), but assuming we have the values, we generate the succeeding values by substituting into (2). The resulting output series is shown in Fig. 9, and we note the periodic behavior of the series, which is similar to that displayed by the speech series in Fig. 3. The autoregressive model above and its generalizations can be used as an underlying model for many observed series and will be studied in detail in Chap. 3.

As in the previous example, the data are obtained by a filter of white noise. The function filter uses zeros for the initial values. In this case, \(x_{1}=w_{1}\), and \(x_{2}=x_{1}+w_{2}=w_{1}+w_{2}\), and so on, so that the values do not satisfy (2). An easy fix is to run the filter for longer than needed and remove the initial values.

```
w=rnorm(550,0,1)#50extratoavoidstartupproblems x=filter(w,filter=c(1,-.9),method="recursive")[-(1:50)]#removefirst50plot.ts(x,main="autoregression")
```

**Example 11**: **Random Walk with Drift**__

A model for analyzing trend such as seen in the global temperature data in Fig. 2, is the _random walk with drift_ model given by

\[x_{t}=\delta+x_{t-1}+w_{t} \tag{3}\]

for \(t=1,2,\ldots\), with initial condition \(x_{0}=0\), and where \(w_{t}\) is white noise. The constant \(\delta\) is called the _drift_, and when \(\delta=0\), (3) is called simply a _random walk_.

The term random walk comes from the fact that, when \(\delta=0\), the value of the time series at time \(t\) is the value of the series at time \(t-1\) plus a completely random movement determined by \(w_{t}\). Note that we may rewrite (3) as a cumulative sum of white noise variates. That is,

\[x_{t}=\delta\,t+\sum_{j=1}^{t}w_{j} \tag{4}\]

for \(t=1,2,\ldots\); either use induction, or plug (4) into (3) to verify this statement. Figure 10 shows 200 observations generated from the model with \(\delta=0\) and \(.2\), and with \(\sigma_{w}=1\). For comparison, we also superimposed the straight line \(.2t\) on the graph. To reproduce Fig. 10 in R use the following code (notice the use of multiple commands per line using a semicolon).

 set.seed(154) # so you can reproduce the results  w = norm(200); x = cumsum(w) # two commands in one line  wd = w +.2; xd = cumsum(wd)  plot.ts(xd, ylim=c(-5,55), main="random walk", ylab=')  lines(x, col=4); abline(h=0, col=4, lty=2); abline(a=0, b=.2, lty=2)

**Example 12**: **Signal in Noise**

Many realistic models for generating time series assume an underlying signal with some consistent periodic variation, contaminated by adding a random noise. For example, it is easy to detect the regular cycle fMRI series displayed on the top of Fig. 6. Consider the model

\[x_{t}=2\cos(2\pi\tfrac{t+15}{50})+w_{t} \tag{5}\]

for \(t=1,2,\ldots,500\), where the first term is regarded as the signal, shown in the upper panel of Fig. 11. We note that a sinusoidal waveform can be written as

\[A\cos(2\pi\omega t+\phi), \tag{6}\]

Figure 9: Autoregressive series generated from model (2)

where \(A\) is the amplitude, \(\omega\) is the frequency of oscillation, and \(\phi\) is a phase shift. In (1.5), \(A=2,\omega=1/50\) (one cycle every 50 time points), and \(\phi=2\pi 15/50=.6\pi\).

An additive noise term was taken to be white noise with \(\sigma_{w}=1\) (middle panel) and \(\sigma_{w}=5\) (bottom panel), drawn from a normal distribution. Adding the two together obscures the signal, as shown in the lower panels of Fig. 1.11. Of course, the degree to which the signal is obscured depends on the amplitude of the signal and the size of \(\sigma_{w}\). The ratio of the amplitude of the signal to \(\sigma_{w}\) (or some function of the ratio) is sometimes called the _signal-to-noise ratio (SNR)_; the larger the SNR, the easier it is to detect the signal. Note that the signal is easily discernible in the middle panel of Fig. 1.11, whereas the signal is obscured in the bottom panel. Typically, we will not observe the signal but the signal obscured by noise.

To reproduce Fig. 1.11 in R, use the following commands:

 cs = 2*cos(2*pi*1:500/50 +.6*pi); w = rnorm(500,0,1)  par(mflow=c(3,1), mar=c(3,2,2,1), cex.main=1.5)  plot.ts(cs, main=expression(2*cos(2*pi*t/50+.6*pi)))  plot.ts(cs+w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,1)))  plot.ts(cs+5*w, main=expression(2*cos(2*pi*t/50+.6*pi) + N(0,25)))

In Chap. 4, we will study the use of _spectral analysis_ as a possible technique for detecting regular or periodic signals, such as the one described in Example 1.12. In general, we would emphasize the importance of simple additive models such as given above in the form

\[x_{t}=s_{t}+v_{t}, \tag{1.7}\]

where \(s_{t}\) denotes some unknown signal and \(v_{t}\) denotes a time series that may be white or correlated over time. The problems of detecting a signal and then in estimating or extracting the waveform of \(s_{t}\) are of great interest in many areas of engineering and the physical and biological sciences. In economics, the underlying signal may be a trend or it may be a seasonal component of a series. Models such as (1.7), where the

Figure 1.10: Random walk, \(\sigma_{w}=1\), with drift \(\delta=.2\) (_upper jagged line_), without drift, \(\delta=0\) (_lower jagged line_), and _straight (dashed) lines_ with slope \(\delta\)

signal has an autoregressive structure, form the motivation for the state-space model of Chap. 6.

In the above examples, we have tried to motivate the use of various combinations of random variables emulating real time series data. Smoothness characteristics of observed time series were introduced by combining the random variables in various ways. Averaging independent random variables over adjacent time points, as in Example 1.9, or looking at the output of difference equations that respond to white noise inputs, as in Example 1.10, are common ways of generating correlated data. In the next section, we introduce various theoretical measures used for describing how time series behave. As is usual in statistics, the complete description involves the multivariate distribution function of the jointly sampled values \(x_{1},x_{2},\ldots,x_{n}\), whereas more economical descriptions can be had in terms of the mean and autocorrelation functions. Because correlation is an essential feature of time series analysis, the most useful descriptive measures are those expressed in terms of covariance and correlation functions.

Figure 11: Cosine wave with period 50 points (_top panel_) compared with the cosine wave contaminated with additive white Gaussian noise, \(\sigma_{w}=1\) (_middle panel_) and \(\sigma_{w}=5\) (_bottom panel_); see (5)

### 1.3 Measures of Dependence

A complete description of a time series, observed as a collection of \(n\) random variables at arbitrary time points \(t_{1},t_{2},\ldots,t_{n}\), for any positive integer \(n\), is provided by the joint distribution function, evaluated as the probability that the values of the series are jointly less than the \(n\) constants, \(c_{1},c_{2},\ldots,c_{n}\); i.e.,

\[F_{t_{1},t_{2},\ldots,t_{n}}(c_{1},c_{2},\ldots,c_{n})=\Pr\bigl{(}x_{t_{1}} \leq c_{1},x_{t_{2}}\leq c_{2},\ldots,x_{t_{n}}\leq c_{n}\bigr{)}. \tag{1.8}\]

Unfortunately, these multidimensional distribution functions cannot usually be written easily unless the random variables are jointly normal, in which case the joint density has the well-known form displayed in (1.33).

Although the joint distribution function describes the data completely, it is an unwieldy tool for displaying and analyzing time series data. The distribution function (1.8) must be evaluated as a function of \(n\) arguments, so any plotting of the corresponding multivariate density functions is virtually impossible. The marginal distribution functions

\[F_{t}(x)=P\{x_{t}\leq x\}\]

or the corresponding marginal density functions

\[f_{t}(x)=\frac{\partial F_{t}(x)}{\partial x},\]

when they exist, are often informative for examining the marginal behavior of a series.3 Another informative marginal descriptive measure is the mean function.

Footnote 3: If \(x_{t}\) is Gaussian with mean \(\mu_{t}\) and variance \(\sigma_{t}^{2}\), abbreviated as \(x_{t}\sim\mathrm{N}(\mu_{t},\sigma_{t}^{2})\), the marginal density is given by \(f_{t}(x)=\frac{1}{\sigma_{t}\sqrt{2\pi}}\exp\left\{-\frac{1}{2\sigma_{t}^{2}}( x-\mu_{t})^{2}\right\},x\in\mathbb{R}\).

**Definition 1.1**: _The_ **mean function** _is defined as_

\[\mu_{xt}=\mathrm{E}(x_{t})=\int_{-\infty}^{\infty}x\,f_{t}(x)\;dx, \tag{1.9}\]

_provided it exists, where \(\mathrm{E}\) denotes the usual expected value operator. When no confusion exists about which time series we are referring to, we will drop a subscript and write \(\mu_{xt}\) as \(\mu_{t}\)._

**Example 1.13**: _Mean Function of a Moving Average Series_

If \(w_{t}\) denotes a white noise series, then \(\mu_{wt}=\mathrm{E}(w_{t})=0\) for all \(t\). The top series in Fig. 1.8 reflects this, as the series clearly fluctuates around a mean value of zero. Smoothing the series as in Example 1.9 does not change the mean because we can write

\[\mu_{wt}=\mathrm{E}(v_{t})=\tfrac{1}{3}[\mathrm{E}(w_{t-1})+\mathrm{E}(w_{t}) +\mathrm{E}(w_{t+1})]=0.\]

**Example 14**: **Mean Function of a Random Walk with Drift**

Consider the random walk with drift model given in (4),

\[x_{t}=\delta\,t+\sum_{j=1}^{t}w_{j},\qquad t=1,2,\ldots\.\]

Because \(\mathrm{E}(w_{t})=0\) for all \(t\), and \(\delta\) is a constant, we have

\[\mu_{xt}=\mathrm{E}(x_{t})=\delta\,t+\sum_{j=1}^{t}\mathrm{E}(w_{j})=\delta\,t\]

which is a straight line with slope \(\delta\). A realization of a random walk with drift can be compared to its mean function in Fig. 10.

**Example 15**: **Mean Function of Signal Plus Noise**

A great many practical applications depend on assuming the observed data have been generated by a fixed signal waveform superimposed on a zero-mean noise process, leading to an additive signal model of the form (5). It is clear, because the signal in (5) is a fixed function of time, we will have

\[\mu_{xt}\ =\ \mathrm{E}(x_{t}) =\mathrm{E}\big{[}2\cos(2\pi\tfrac{t+15}{50})+w_{t}\big{]}\] \[=2\cos(2\pi\tfrac{t+15}{50})+\mathrm{E}(w_{t})\] \[=2\cos(2\pi\tfrac{t+15}{50}),\]

and the mean function is just the cosine wave.

The lack of independence between two adjacent values \(x_{s}\) and \(x_{t}\) can be assessed numerically, as in classical statistics, using the notions of covariance and correlation. Assuming the variance of \(x_{t}\) is finite, we have the following definition.

**Definition 2**: _The_ **autocovariance function** _is defined as the second moment product_

\[\gamma_{x}(s,t)=\mathrm{cov}(x_{s},x_{t})=\mathrm{E}[(x_{s}-\mu_{s})(x_{t}-\mu _{t})], \tag{10}\]

_for all \(s\) and \(t\). When no possible confusion exists about which time series we are referring to, we will drop the subscript and write \(\gamma_{x}(s,t)\) as \(\gamma(s,t)\). Note that \(\gamma_{x}(s,t)=\gamma_{x}(t,s)\) for all time points \(s\) and \(t\)._

The autocovariance measures the _linear_ dependence between two points on the same series observed at different times. Very smooth series exhibit autocovariance functions that stay large even when the \(t\) and \(s\) are far apart, whereas choppy series tend to have autocovariance functions that are nearly zero for large separations. Recall from classical statistics that if \(\gamma_{x}(s,t)=0\), \(x_{s}\) and \(x_{t}\) are not linearly related, but there still may be some dependence structure between them. If, however, \(x_{s}\) and \(x_{t}\) are bivariate normal, \(\gamma_{x}(s,t)=0\) ensures their independence. It is clear that, for \(s=t\), the autocovariance reduces to the (assumed finite) _variance_, because

\[\gamma_{x}(t,t)=\mathrm{E}\big{[}(x_{t}-\mu_{t})^{2}\big{]}=\mathrm{var}(x_{t }). \tag{11}\]

[MISSING_PAGE_FAIL:29]

\[\gamma_{v}(s,t)=\begin{cases}\frac{3}{9}\sigma_{w}^{2}&s=t,\\ \frac{2}{9}\sigma_{w}^{2}&|s-t|=1,\\ \frac{1}{9}\sigma_{w}^{2}&|s-t|=2,\\ 0&|s-t|>2.\end{cases} \tag{14}\]

Example 17 shows clearly that the smoothing operation introduces a covariance function that decreases as the separation between the two time points increases and disappears completely when the time points are separated by three or more time points. This particular autocovariance is interesting because it only depends on the time separation or \(lag\) and not on the absolute location of the points along the series. We shall see later that this dependence suggests a mathematical model for the concept of _weak stationarity_.

**Example 18**: **Autocovariance of a Random Walk**__

For the random walk model, \(x_{t}=\sum_{j=1}^{t}w_{j}\), we have

\[\gamma_{x}(s,t)=\text{cov}(x_{s},x_{t})=\text{cov}\left(\sum_{j=1}^{s}w_{j}, \sum_{k=1}^{t}w_{k}\right)=\min\{s,t\}\,\sigma_{w}^{2},\]

because the \(w_{t}\) are uncorrelated random variables. Note that, as opposed to the previous examples, the autocovariance function of a random walk depends on the particular time values \(s\) and \(t\), and not on the time separation or lag. Also, notice that the variance of the random walk, \(\text{var}(x_{t})=\gamma_{x}(t,t)=t\,\sigma_{w}^{2}\), increases without bound as time \(t\) increases. The effect of this variance increase can be seen in Fig. 10 where the processes start to move away from their mean functions \(\delta\,t\) (note that \(\delta=0\) and \(.2\) in that example).

As in classical statistics, it is more convenient to deal with a measure of association between \(-1\) and \(1\), and this leads to the following definition.

**Definition 3**: _The_ **autocorrelation function (ACF)** _is defined as_

\[\rho(s,t)=\frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}}. \tag{15}\]

The ACF measures the linear predictability of the series at time \(t\), say \(x_{t}\), using only the value \(x_{s}\). We can show easily that \(-1\leq\rho(s,t)\leq 1\) using the Cauchy-Schwarz inequality.4 If we can predict \(x_{t}\)_perfectly_ from \(x_{s}\) through a linear relationship, \(x_{t}=\beta_{0}+\beta_{1}x_{s}\), then the correlation will be \(+1\) when \(\beta_{1}>0\), and \(-1\) when \(\beta_{1}<0\). Hence, we have a rough measure of the ability to forecast the series at time \(t\) from the value at time \(s\).

Footnote 4: The Cauchy–Schwarz inequality implies \(|\gamma(s,t)|^{2}\leq\gamma(s,s)\gamma(t,t)\).

Often, we would like to measure the predictability of another series \(y_{t}\) from the series \(x_{s}\). Assuming both series have finite variances, we have the following definition.

**Definition 1.4**: _The_ **cross-covariance function** _between two series, \(x_{t}\) and \(y_{t}\), is_

\[\gamma_{xy}(s,t)=\mathrm{cov}(x_{s},y_{t})=\mathrm{E}[(x_{s}-\mu_{xs})(y_{t}-\mu _{yt})]. \tag{1.16}\]

There is also a scaled version of the cross-covariance function.

**Definition 1.5**: _The_ **cross-correlation function (CCF)** _is given by_

\[\rho_{xy}(s,t)=\frac{\gamma_{xy}(s,t)}{\sqrt{\gamma_{x}(s,s)\gamma_{y}(t,t)}}. \tag{1.17}\]

We may easily extend the above ideas to the case of more than two series, say, \(x_{t1},x_{t2},\ldots,x_{tr}\); that is, _multivariate time series_ with \(r\) components. For example, the extension of (1.10) in this case is

\[\gamma_{jk}(s,t)=\mathrm{E}[(x_{sj}-\mu_{sj})(x_{tk}-\mu_{tk})]\qquad j,k=1,2, \ldots,r. \tag{1.18}\]

In the definitions above, the autocovariance and cross-covariance functions may change as one moves along the series because the values depend on both \(s\) and \(t\), the locations of the points in time. In Example 1.17, the autocovariance function depends on the separation of \(x_{s}\) and \(x_{t}\), say, \(h=|s-t|\), and not on where the points are located in time. As long as the points are separated by \(h\) units, the location of the two points does not matter. This notion, called _weak stationarity_, when the mean is constant, is fundamental in allowing us to analyze sample time series data when only a single series is available.

## 1.4 Stationary Time Series

The preceding definitions of the mean and autocovariance functions are completely general. Although we have not made any special assumptions about the behavior of the time series, many of the preceding examples have hinted that a sort of regularity may exist over time in the behavior of a time series. We introduce the notion of regularity using a concept called _stationarity_.

**Definition 1.6**: \(A\) **strictly stationary** _time series is one for which the probabilistic behavior of every collection of values_

\[\{x_{t_{1}},x_{t_{2}},\ldots,x_{t_{k}}\}\]

_is identical to that of the time shifted set_

\[\{x_{t_{1}+h},x_{t_{2}+h},\ldots,x_{t_{k}+h}\}.\]

_That is,_

\[\Pr\{x_{t_{1}}\leq c_{1},\ldots,x_{t_{k}}\leq c_{k}\}=\Pr\{x_{t_{1}+h}\leq c_{ 1},\ldots,x_{t_{k}+h}\leq c_{k}\} \tag{1.19}\]

_for all \(k=1,2,\ldots\), all time points \(t_{1},t_{2},\ldots,t_{k}\), all numbers \(c_{1},c_{2},\ldots,c_{k}\), and all time shifts \(h=0,\pm 1,\pm 2,\ldots\)._If a time series is strictly stationary, then all of the multivariate distribution functions for subsets of variables must agree with their counterparts in the shifted set for all values of the shift parameter \(h\). For example, when \(k=1\), (19) implies that

\[\Pr\{x_{s}\leq c\}=\Pr\{x_{t}\leq c\} \tag{20}\]

for any time points \(s\) and \(t\). This statement implies, for example, that the probability the value of a time series sampled hourly is negative at \(1\)am is the same as at \(10\)am. In addition, if the mean function, \(\mu_{t}\), of the series exists, (20) implies that \(\mu_{s}=\mu_{t}\) for all \(s\) and \(t\), and hence \(\mu_{t}\) must be constant. Note, for example, that a random walk process with drift is _not_ strictly stationary because its mean function changes with time; see Example 14.

When \(k=2\), we can write (19) as

\[\Pr\{x_{s}\leq c_{1},x_{t}\leq c_{2}\}=\Pr\{x_{s+h}\leq c_{1},x_{t+h}\leq c_{2}\} \tag{21}\]

for any time points \(s\) and \(t\) and shift \(h\). Thus, if the variance function of the process exists, (20)-(21) imply that the autocovariance function of the series \(x_{t}\) satisfies

\[\gamma(s,t)=\gamma(s+h,t+h)\]

for all \(s\) and \(t\) and \(h\). We may interpret this result by saying the autocovariance function of the process depends only on the time difference between \(s\) and \(t\), and not on the actual times.

The version of stationarity in Definition 6 is too strong for most applications. Moreover, it is difficult to assess strict stationarity from a single data set. Rather than imposing conditions on all possible distributions of a time series, we will use a milder version that imposes conditions only on the first two moments of the series. We now have the following definition.

**Definition 7**: \(A\) **weakly stationary** _time series, \(x_{t}\), is a finite variance process such that_

1. _the mean value function,_ \(\mu_{t}\)_, defined in (_9_) is constant and does not depend on time_ \(t\)_, and_
2. _the autocovariance function,_ \(\gamma(s,t)\)_, defined in (_10_) depends on_ \(s\) _and_ \(t\) _only through their difference_ \(|s-t|\)_._

_Henceforth, we will use the term_ **stationary** _to mean weakly stationary; if a process is stationary in the strict sense, we will use the term strictly stationary._

Stationarity requires regularity in the mean and autocorrelation functions so that these quantities (at least) may be estimated by averaging. It should be clear from the discussion of strict stationarity following Definition 6 that a strictly stationary, finite variance, time series is also stationary. The converse is not true unless there are further conditions. One important case where stationarity implies strict stationarity is if the time series is Gaussian [meaning all finite distributions, (19), of the series are Gaussian]. We will make this concept more precise at the end of this section.

Because the mean function, \(\mathrm{E}(x_{t})=\mu_{t}\), of a stationary time series is independent of time \(t\), we will write

\[\mu_{t}=\mu. \tag{1.22}\]

Also, because the autocovariance function, \(\gamma(s,t)\), of a stationary time series, \(x_{t}\), depends on \(s\) and \(t\) only through their difference \(|s-t|\), we may simplify the notation. Let \(s=t+h\), where \(h\) represents the time shift or \(lag\). Then

\[\gamma(t+h,t)=\mathrm{cov}(x_{t+h},x_{t})=\mathrm{cov}(x_{h},x_{0})=\gamma(h,0)\]

because the time difference between times \(t+h\) and \(t\) is the same as the time difference between times \(h\) and \(0\). Thus, the autocovariance function of a stationary time series does not depend on the time argument \(t\). Henceforth, for convenience, we will drop the second argument of \(\gamma(h,0)\).

**Definition 1.8**: _The_ **autocovariance function of a stationary time series** _will be written as_

\[\gamma(h)=\mathrm{cov}(x_{t+h},x_{t})=\mathrm{E}[(x_{t+h}-\mu)(x_{t}-\mu)]. \tag{1.23}\]

**Definition 1.9**: _The_ **autocorrelation function** (**ACF**) of a stationary time series** _will be written using (1.15) as_

\[\rho(h)=\frac{\gamma(t+h,t)}{\sqrt{\gamma(t+h,t+h)\gamma(t,t)}}=\frac{\gamma(h )}{\gamma(0)}. \tag{1.24}\]

The Cauchy-Schwarz inequality shows again that \(-1\leq\rho(h)\leq 1\) for all \(h\), enabling one to assess the relative importance of a given autocorrelation value by comparing with the extreme values \(-1\) and \(1\).

**Example 1.19**: _Stationarity of White Noise_

The mean and autocovariance functions of the white noise series discussed in Example 1.8 and Example 1.16 are easily evaluated as \(\mu_{wt}=0\) and

\[\gamma_{w}(h)=\mathrm{cov}(w_{t+h},w_{t})=\begin{cases}\sigma_{w}^{2}&h=0,\\ 0&h\neq 0.\end{cases}\]

Thus, white noise satisfies the conditions of Definition 1.7 and is weakly stationary or stationary. If the white noise variates are also normally distributed or Gaussian, the series is also strictly stationary, as can be seen by evaluating (1.19) using the fact that the noise would also be iid. The autocorrelation function is given by \(\rho_{w}(0)=1\) and \(\rho(h)=0\) for \(h\neq 0\).

**Example 1.20**: **Stationarity of a Moving Average**

The three-point moving average process of Example 1.9 is stationary because, from Example 1.13 and Example 1.17, the mean and autocovariance functions \(\mu_{vt}=0\), and

\[\gamma_{v}(h)=\begin{cases}\frac{3}{9}\sigma_{w}^{2}&h=0,\\ \frac{2}{9}\sigma_{w}^{2}&h=\pm 1,\\ \frac{1}{9}\sigma_{w}^{2}&h=\pm 2,\\ 0&|h|>2\end{cases}\]

are independent of time \(t\), satisfying the conditions of Definition 1.7.

The autocorrelation function is given by

\[\rho_{v}(h)=\begin{cases}1&h=0,\\ \frac{2}{3}&h=\pm 1,\\ \frac{1}{3}&h=\pm 2,\\ 0&|h|>2.\end{cases}\]

Figure 1.12 shows a plot of the autocorrelations as a function of lag \(h\). Note that the ACF is symmetric about lag zero.

**Example 1.21**: **A Random Walk is Not Stationary**

A random walk is not stationary because its autocovariance function, \(\gamma(s,t)=\min\{s,t\}\sigma_{w}^{2}\), depends on time; see Example 1.18 and Problem 1.8. Also, the random walk with drift violates both conditions of Definition 1.7 because, as shown in Example 1.14, the mean function, \(\mu_{xt}=\delta t\), is also a function of time \(t\).

**Example 1.22**: **Trend Stationarity**

For example, if \(x_{t}=\alpha+\beta t+y_{t}\), where \(y_{t}\) is stationary, then the mean function is \(\mu_{x,t}=\mathrm{E}(x_{t})=\alpha+\beta t+\mu_{y}\), which is not independent of time. Therefore, the process is not stationary. The autocovariance function, however, is independent of time, because \(\gamma_{x}(h)=\mathrm{cov}(x_{t+h},x_{t})=\mathrm{E}[(x_{t+h}-\mu_{x,t+h})(x_{ t}-\mu_{x,t})]=\mathrm{E}[(y_{t+h}-\mu_{y})(y_{t}-\mu_{y})]=\gamma_{y}(h)\). Thus, the model may be considered as having stationary behavior around a linear trend; this behavior is sometimes called _trend stationarity_. An example of such a process is the price of chicken series displayed in Fig. 2.1.

The autocovariance function of a stationary process has several special properties. First, \(\gamma(h)\) is _non-negative definite_ (see Problem 1.25) ensuring that variances of linear combinations of the variates \(x_{t}\) will never be negative. That is, for any \(n\geq 1\), and constants \(a_{1},\ldots,a_{n}\),

\[0\leq\mathrm{var}(a_{1}x_{1}+\cdots+a_{n}x_{n})=\sum_{j=1}^{n}\sum_{k=1}^{n}a_ {j}a_{k}\gamma(j-k)\,, \tag{1.25}\]

using Chap. 1.1. Also, the value at \(h=0\), namely\[\gamma(0)=\mathrm{E}[(x_{t}-\mu)^{2}] \tag{26}\]

is the variance of the time series and the Cauchy-Schwarz inequality implies

\[|\gamma(h)|\leq\gamma(0).\]

A final useful property, noted in a previous example, is that the autocovariance function of a stationary series is symmetric around the origin; that is,

\[\gamma(h)=\gamma(-h) \tag{27}\]

for all \(h\). This property follows because

\[\gamma((t+h)-t)=\mathrm{cov}(x_{t+h},x_{t})=\mathrm{cov}(x_{t},x_{t+h})=\gamma (t-(t+h)),\]

which shows how to use the notation as well as proving the result.

When several series are available, a notion of stationarity still applies with additional conditions.

**Definition 10**: _Two time series, say, \(x_{t}\) and \(y_{t}\), are said to be_ **jointly stationary** _if they are each stationary, and the_ cross-covariance function__

\[\gamma_{xy}(h)=\mathrm{cov}(x_{t+h},y_{t})=\mathrm{E}[(x_{t+h}-\mu_{x})(y_{t}- \mu_{y})] \tag{28}\]

_is a function only of lag \(h\)._

**Definition 11**: _The_ **cross-correlation function (CCF)** _of jointly stationary time series \(x_{t}\) and \(y_{t}\) is defined as_

\[\rho_{xy}(h)=\frac{\gamma_{xy}(h)}{\sqrt{\gamma_{x}(0)\gamma_{y}(0)}}. \tag{29}\]

Again, we have the result \(-1\leq\rho_{xy}(h)\leq 1\) which enables comparison with the extreme values \(-1\) and \(1\) when looking at the relation between \(x_{t+h}\) and \(y_{t}\). The cross-correlation function is not generally symmetric about zero, i.e., typically \(\rho_{xy}(h)\neq\rho_{xy}(-h)\). This is an important concept; it should be clear that \(\mathrm{cov}(x_{2},y_{1})\) and \(\mathrm{cov}(x_{1},y_{2})\) need not be the same. It is the case, however, that

\[\rho_{xy}(h)=\rho_{yx}(-h), \tag{30}\]

which can be shown by manipulations similar to those used to show (27).

Figure 12: Autocorrelation function of a three-point moving average

**Example 1.23**: **Joint Stationarity**

Consider the two series, \(x_{t}\) and \(y_{t}\), formed from the sum and difference of two successive values of a white noise process, say,

\[x_{t}=w_{t}+w_{t-1}\quad\text{and}\quad y_{t}=w_{t}-w_{t-1},\]

where \(w_{t}\) are independent random variables with zero means and variance \(\sigma_{w}^{2}\). It is easy to show that \(\gamma_{x}(0)=\gamma_{y}(0)=2\sigma_{w}^{2}\) and \(\gamma_{x}(1)=\gamma_{x}(-1)=\sigma_{w}^{2},\gamma_{y}(1)=\gamma_{y}(-1)=- \sigma_{w}^{2}\). Also,

\[\gamma_{xy}(1)=\text{cov}(x_{t+1},y_{t})=\text{cov}(w_{t+1}+w_{t},w_{t}-w_{t-1 })=\sigma_{w}^{2}\]

because only one term is nonzero. Similarly, \(\gamma_{xy}(0)=0,\gamma_{xy}(-1)=-\sigma_{w}^{2}\). We obtain, using (1.29),

\[\rho_{xy}(h)=\begin{cases}\ \ 0&h=0,\\ \ \ 1/2&h=1,\\ -1/2&h=-1,\\ \ \ 0&|h|\geq 2.\end{cases}\]

Clearly, the autocovariance and cross-covariance functions depend only on the lag separation, \(h\), so the series are jointly stationary.

**Example 1.24**: **Prediction Using Cross-Correlation**

As a simple example of cross-correlation, consider the problem of determining possible leading or lagging relations between two series \(x_{t}\) and \(y_{t}\). If the model

\[y_{t}=Ax_{t-\ell}+w_{t}\]

holds, the series \(x_{t}\) is said to _lead_\(y_{t}\) for \(\ell>0\) and is said to _lag_\(y_{t}\) for \(\ell<0\). Hence, the analysis of leading and lagging relations might be important in predicting the value of \(y_{t}\) from \(x_{t}\). Assuming that the noise \(w_{t}\) is uncorrelated with the \(x_{t}\) series, the cross-covariance function can be computed as

\[\gamma_{yx}(h) =\text{cov}(y_{t+h},x_{t})=\text{cov}(Ax_{t+h-\ell}+w_{t+h},x_{t})\] \[=\text{cov}(Ax_{t+h-\ell},x_{t})=A\gamma_{x}(h-\ell).\]

Since (Cauchy-Schwarz) the largest absolute value of \(\gamma_{x}(h-\ell)\) is \(\gamma_{x}(0)\), i.e., when \(h=\ell\), the cross-covariance function will look like the autocovariance of the input series \(x_{t}\), and it will have a peak on the positive side if \(x_{t}\) leads \(y_{t}\) and a peak on the negative side if \(x_{t}\) lags \(y_{t}\). Below is the R code of an example where \(x_{t}\) is white noise, \(\ell=5\), and with \(\hat{\gamma}_{yx}(h)\) shown in Fig. 13.

``` x=rnorm(100) y=lag(x,-5)+rnorm(100) ccf(y,x,ylab='CCovF',type='covariance')The concept of weak stationarity forms the basis for much of the analysis performed with time series. The fundamental properties of the mean and autocovariance functions (22) and (23) are satisfied by many theoretical models that appear to generate plausible sample realizations. In Example 1.9 and Example 1.10, two series were generated that produced stationary looking realizations, and in Example 1.20, we showed that the series in Example 1.9 was, in fact, weakly stationary. Both examples are special cases of the so-called linear process.

**Definition 1.12**: \(A\) **linear process**_, \(x_{t}\), is defined to be a linear combination of white noise variates \(w_{t}\), and is given by_

\[x_{t}=\mu+\sum_{j=-\infty}^{\infty}\psi_{j}w_{t-j},\qquad\sum_{j=-\infty}^{ \infty}|\psi_{j}|<\infty. \tag{31}\]

For the linear process (see Problem 1.11), we may show that the autocovariance function is given by

\[\gamma_{x}(h)=\sigma_{w}^{2}\sum_{j=-\infty}^{\infty}\psi_{j+h}\psi_{j} \tag{32}\]

for \(h\geq 0\); recall that \(\gamma_{x}(-h)=\gamma_{x}(h)\). This method exhibits the autocovariance function of the process in terms of the lagged products of the coefficients. We only need \(\sum_{j=-\infty}^{\infty}\psi_{j}^{2}<\infty\) for the process to have finite variance, but we will discuss this further in Chap. 5. Note that, for Example 1.9, we have \(\psi_{0}=\psi_{-1}=\psi_{1}=1/3\) and the result in Example 1.20 comes out immediately. The autoregressive series in Example 1.10 can also be put in this form, as can the general autoregressive moving average processes considered in Chap. 3.

Notice that the linear process (31) is dependent on the future (\(j<0\)), the present (\(j=0\)), and the past (\(j>0\)). For the purpose of forecasting, a future dependent model will be useless. Consequently, we will focus on processes that do not depend on the future. Such models are called _causal_, and a causal linear process has \(\psi_{j}=0\) for \(j<0\); we will discuss this further in Chap. 3.

Finally, as previously mentioned, an important case in which a weakly stationary series is also strictly stationary is the normal or Gaussian series.

Figure 13: Demonstration of the results of Example 1.24 when \(\ell=5\). The title shows which side leads

**Definition 1.13**: _A process, \(\{x_{t}\}\), is said to be a_ **Gaussian process** _if the \(n\)-dimensional vectors \(x=(x_{t_{1}},x_{t_{2}},\ldots,x_{t_{n}})^{\prime}\), for every collection of distinct time points \(t_{1},t_{2},\ldots,t_{n}\), and every positive integer \(n\), have a multivariate normal distribution._

Defining the \(n\times 1\) mean vector \(\mathrm{E}(x)\equiv\mu=(\mu_{t_{1}},\mu_{t_{2}},\ldots,\mu_{t_{n}})^{\prime}\) and the \(n\times n\) covariance matrix as \(\mathrm{var}(x)\equiv\Gamma=\{\gamma(t_{i},t_{j});\;i,j=1,\ldots,n\}\), which is assumed to be positive definite, the multivariate normal density function can be written as

\[f(x)=(2\pi)^{-n/2}|\Gamma|^{-1/2}\exp\left\{-\frac{1}{2}(x-\mu)^{\prime}\Gamma ^{-1}(x-\mu)\right\}\,, \tag{1.33}\]

for \(x\in\mathbb{R}^{n}\), where \(|\cdot|\) denotes the determinant.

We list some important items regarding linear and Gaussian processes.

* If a Gaussian time series, \(\{x_{t}\}\), is weakly stationary, then \(\mu_{t}\) is constant and \(\gamma(t_{i},t_{j})=\gamma(|t_{i}-t_{j}|)\), so that the vector \(\mu\) and the matrix \(\Gamma\) are independent of time. These facts imply that all the finite distributions, (1.33), of the series \(\{x_{t}\}\) depend only on time lag and not on the actual times, and hence the series must be strictly stationary. In a sense, weak stationarity and normality go hand-in-hand in that we will base our analyses on the idea that it is enough for the first two moments to behave nicely. We use the multivariate normal density in the form given above as well as in a modified version, applicable to complex random variables throughout the text.
* A result called the _Wold Decomposition_ (Theorem B.5) states that a stationary non-deterministic time series is a causal linear process (but with \(\sum\psi_{j}^{2}<\infty\)). A linear process need not be Gaussian, but if a time series is Gaussian, then it is a causal linear process with \(w_{t}\sim\mathrm{iid}\;\mathrm{N}(0,\sigma_{w}^{2})\). Hence, stationary Gaussian processes form the basis of modeling many time series.
* It is not enough for the marginal distributions to be Gaussian for the process to be Gaussian. It is easy to construct a situation where \(X\) and \(Y\) are normal, but \((X,Y)\) is not bivariate normal; e.g., let \(X\) and \(Z\) be independent normals and let \(Y=Z\) if \(XZ>0\) and \(Y=-Z\) if \(XZ\leq 0\).

### Estimation of Correlation

Although the theoretical autocorrelation and cross-correlation functions are useful for describing the properties of certain hypothesized models, most of the analyses must be performed using sampled data. This limitation means the sampled points \(x_{1},x_{2},\ldots,x_{n}\) only are available for estimating the mean, autocovariance, and autocorrelation functions. From the point of view of classical statistics, this poses a problem because we will typically not have iid copies of \(x_{t}\) that are available for estimating the covariance and correlation functions. In the usual situation with only one realization, however, the assumption of stationarity becomes critical. Somehow, we must use averages over this single realization to estimate the population means and covariance functions.

Accordingly, if a time series is stationary, the mean function (22) \(\mu_{t}=\mu\) is constant so that we can estimate it by the _sample mean_,

\[\bar{x}=\frac{1}{n}\sum_{t=1}^{n}x_{t}. \tag{34}\]

In our case, \(\mathrm{E}(\bar{x})=\mu\), and the standard error of the estimate is the square root of \(\mathrm{var}(\bar{x})\), which can be computed using first principles (recall Chap. 1.1), and is given by

\[\mathrm{var}(\bar{x}) = \mathrm{var}\left(\frac{1}{n}\sum_{t=1}^{n}x_{t}\right)=\frac{1}{ n^{2}}\mathrm{cov}\left(\sum_{t=1}^{n}x_{t},\sum_{s=1}^{n}x_{s}\right) \tag{35}\] \[= \frac{1}{n^{2}}\left(n\gamma_{x}(0)+(n-1)\gamma_{x}(1)+(n-2) \gamma_{x}(2)+\cdots+\gamma_{x}(n-1)\right.\] \[\qquad\qquad+\left.(n-1)\gamma_{x}(-1)+(n-2)\gamma_{x}(-2)+\cdots +\gamma_{x}(1-n)\right)\] \[= \frac{1}{n}\sum_{h=-n}^{n}\left(1-\frac{|h|}{n}\right)\gamma_{x} (h).\]

If the process is white noise, (35) reduces to the familiar \(\sigma_{x}^{2}/n\) recalling that \(\gamma_{x}(0)=\sigma_{x}^{2}\). Note that, in the case of dependence, the standard error of \(\bar{x}\) may be smaller or larger than the white noise case depending on the nature of the correlation structure (see Problem 1.19)

The theoretical autocovariance function, (23), is estimated by the sample autocovariance function defined as follows.

**Definition 14**: _The_ **sample autocovariance function** _is defined as_

\[\hat{\gamma}(h)=n^{-1}\sum_{t=1}^{n-h}(x_{t+h}-\bar{x})(x_{t}-\bar{x}), \tag{36}\]

_with \(\hat{\gamma}(-h)=\hat{\gamma}(h)\) for \(h=0,1,\ldots,n-1\)._

The sum in (36) runs over a restricted range because \(x_{t+h}\) is not available for \(t+h>n\). The estimator in (36) is preferred to the one that would be obtained by dividing by \(n-h\) because (36) is a non-negative definite function. Recall that the autocovariance function of a stationary process is non-negative definite [(25); also, see Problem 1.25] ensuring that variances of linear combinations of the variates \(x_{t}\) will never be negative. And because a variance is never negative, the estimate of that variance

\[\widehat{\mathrm{var}}(a_{1}x_{1}+\cdots+a_{n}x_{n})=\sum_{j=1}^{n}\sum_{k=1} ^{n}a_{j}a_{k}\hat{\gamma}(j-k),\]

should also be non-negative. The estimator in (36) guarantees this result, but no such guarantee exists if we divide by \(n-h\). Note that neither dividing by \(n\) nor \(n-h\) in (36) yields an unbiased estimator of \(\gamma(h)\).

**Definition 1.15**: _The_ **sample autocorrelation function** _is defined, analogously to (1.24), as_

\[\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}. \tag{1.37}\]

The sample autocorrelation function has a sampling distribution that allows us to assess whether the data comes from a completely random or white series or whether correlations are statistically significant at some lags.

**Example 1.25**: **Sample ACF and Scatterplots**

Estimating autocorrelation is similar to estimating of correlation in the usual setup where we have pairs of observations, say \((x_{i},y_{i})\), for \(i=1,\ldots,n\). For example, if we have time series data \(x_{t}\) for \(t=1,\ldots,n\), then the pairs of observations for estimating \(\rho(h)\) are the \(n-h\) pairs given by \(\{(x_{t},x_{t+h});\,t=1,\ldots,n-h\}\). Figure 1.14 shows an example using the SOI series where \(\hat{\rho}(1)=.604\) and \(\hat{\rho}(6)=-.187\). The following code was used for Fig. 1.14.

(r = round(acf(soi, 6, plot=FALSE)$acf[-1], 3)) # first 6 sample acf values [1] 0.604 0.374 0.214 0.050 -0.107 -0.187 par(mfrow=c(1,2)) plot(lag(soi,-1), soi); legend('topleft', legend=r[1]) plot(lag(soi,-6), soi); legend('topleft', legend=r[6])

**Property 1.2**: **Large-Sample Distribution of the ACF**

_Under general conditions, 5 if \(x_{t}\) is white noise, then for \(n\) large, the sample ACF, \(\hat{\rho}_{x}(h)\), for \(h=1,2,\ldots,H\), where \(H\) is fixed but arbitrary, is approximately normally distributed with zero mean and standard deviation given by_

\[\sigma_{\hat{\rho}_{x}(h)}=\frac{1}{\sqrt{n}}. \tag{1.38}\]

Figure 1.14: Display for Example 1.25. For the SOI series, the scatterplots show pairs of values one month apart (_left_) and six months apart (_right_). The estimated correlation is displayed in the boxBased on the previous result, we obtain a rough method of assessing whether peaks in \(\hat{\rho}(h)\) are significant by determining whether the observed peak is outside the interval \(\pm 2/\sqrt{n}\) (or plus/minus two standard errors); for a white noise sequence, approximately 95% of the sample ACFs should be within these limits. The applications of this property develop because many statistical modeling procedures depend on reducing a time series to a white noise series using various kinds of transformations. After such a procedure is applied, the plotted ACFs of the residuals should then lie roughly within the limits given above.

**Example 1.26**: **A Simulated Time Series**

To compare the sample ACF for various sample sizes to the theoretical ACF, consider a contrived set of data generated by tossing a fair coin, letting \(x_{t}=1\) when a head is obtained and \(x_{t}=-1\) when a tail is obtained. Then, construct \(y_{t}\) as

\[y_{t}=5+x_{t}-.7x_{t-1}. \tag{1.39}\]

To simulate data, we consider two cases, one with a small sample size (\(n=10\)) and another with a moderate sample size (\(n=100\)).

```
set.seed(101010) x1=2+rbinom(11,1,.5)-1#simulatedsequenceofcointosses x2=2+rbinom(101,1,.5)-1 y1=5+filter(x1,sides=1,filter=c(1,-.7))[-1] y2=5+filter(x2,sides=1,filter=c(1,-.7))[-1] plot.ts(y1,type='s');plot.ts(y2,type='s')#plotbothseries(notshown) c(mean(y1),mean(y2))#thesamplemeans [1]5.0805.002 acf(y1,lag.max=4,plot-FALSE)#1/\(\sqrt{10}=.32\) Autocorrelationsofseries'y1',bylag 012341.000-0.6880.425-0.306-0.007 acf(y2,lag.max=4,plot-FALSE)#1/\(\sqrt{100}=.1\) Autocorrelationsofseries'y2',bylag 012341.000-0.480-0.002-0.0040.000
#NotethatthesampleACFatlagzeroisalways1(Why?).
```

The theoretical ACF can be obtained from the model (1.39) using the fact that the mean of \(x_{t}\) is zero and the variance of \(x_{t}\) is one. It can be shown that

\[\rho_{y}(1)=\frac{-.7}{1+.7^{2}}=-.47\]

and \(\rho_{y}(h)=0\) for \(|h|>1\) (Problem 1.24). It is interesting to compare the theoretical ACF with sample ACFs for the realization where \(n=10\) and the other realization where \(n=100\); note the increased variability in the smaller size sample.

**Example 1.27**: **ACF of a Speech Signal**

Computing the sample ACF as in the previous example can be thought of as matching the time series \(h\) units in the future, say, \(x_{t+h}\) against itself, \(x_{t}\). Figure 1.15shows the ACF of the speech series of Fig. 1.3. The original series appears to contain a sequence of repeating short signals. The ACF confirms this behavior, showing repeating peaks spaced at about 106-109 points. Autocorrelation functions of the short signals appear, spaced at the intervals mentioned above. The distance between the repeating signals is known as the _pitch period_ and is a fundamental parameter of interest in systems that encode and decipher speech. Because the series is sampled at 10,000 points per second, the pitch period appears to be between.0106 and.0109 seconds. To compute the sample ACF in R, use acf(speech, 250).

**Definition 1.16**: _The estimators for the cross-covariance function, \(\gamma_{xy}(h)\), as given in (1.28) and the cross-correlation, \(\rho_{xy}(h)\), in (1.11) are given, respectively, by the_ **sample cross-covariance function**__

\[\hat{\gamma}_{xy}(h)=n^{-1}\sum_{t=1}^{n-h}(x_{t+h}-\bar{x})(y_{t}-\bar{y}), \tag{1.40}\]

_where \(\hat{\gamma}_{xy}(-h)=\hat{\gamma}_{yx}(h)\) determines the function for negative lags, and the_ **sample cross-correlation function**__

\[\hat{\rho}_{xy}(h)=\frac{\hat{\gamma}_{xy}(h)}{\sqrt{\hat{\gamma}_{x}(0)\hat{ \gamma}_{y}(0)}}. \tag{1.41}\]

The sample cross-correlation function can be examined graphically as a function of lag \(h\) to search for leading or lagging relations in the data using the property mentioned in Example 1.24 for the theoretical cross-covariance function. Because \(-1\leq\hat{\rho}_{xy}(h)\leq 1\), the practical importance of peaks can be assessed by comparing their magnitudes with their theoretical maximum values. Furthermore, for \(x_{t}\) and \(y_{t}\) independent linear processes of the form (1.31), we have the following property.

Figure 1.15: ACF of the speech series

**Property 1.3**: **Large-Sample Distribution of Cross-Correlation**__

_The large sample distribution of \(\hat{\rho}_{xy}(h)\) is normal with mean zero and_

\[\sigma_{\hat{\rho}_{xy}}\,=\,\frac{1}{\sqrt{n}} \tag{1.42}\]

_if at least one of the processes is independent white noise (see Theorem A.8)._

**Example 1.28**: **SOI and Recruitment Correlation Analysis**__

The autocorrelation and cross-correlation functions are also useful for analyzing the joint behavior of two stationary series whose behavior may be related in some unspecified way. In Example 1.5 (see Fig. 1.5), we have considered simultaneous monthly readings of the SOI and the number of new fish (Recruitment) computed from a model. Figure 1.16 shows the autocorrelation and cross-correlation functions (ACFs and CCF) for these two series. Both of the ACFs exhibit periodicities corresponding to the correlation between values separated by 12 units. Observations 12 months or one year apart are strongly positively correlated, as are observations at multiples such as 24, 36, 48,... Observations separated by six months are negatively correlated.

Figure 1.16: Sample ACFs of the SOI series (_top_) and of the Recruitment series (_middle_), and the sample CCF of the two series (_bottom_); negative lags indicate SOI leads Recruitment. The lag axes are in terms of seasons (12 months)correlated, showing that positive excursions tend to be associated with negative excursions six months removed.

The sample CCF in Fig. 16, however, shows some departure from the cyclic component of each series and there is an obvious peak at \(h=-6\). This result implies that SOI measured at time \(t-6\) months is associated with the Recruitment series at time \(t\). We could say the SOI leads the Recruitment series by six months. The sign of the CCF is negative, leading to the conclusion that the two series move in different directions; that is, increases in SOI lead to decreases in Recruitment and vice versa. We will discover in Chap. 2 that there is a relationship between the series, but the relationship is nonlinear. The dashed lines shown on the plots indicate \(\pm 2/\sqrt{453}\) [see (42)], but since neither series is noise, these lines do not apply. To reproduce Fig. 16 in R, use the following commands:

```
par(mfrow=c(3,1)) acf(soi,48,main="SouthernOscillationIndex") acf(rec,48,main="Recruitment") ccf(soi,rec,48,main="SOIvsRecruitment",ylab="CCF")
```

**Example 1.29**: Prewhitening and Cross Correlation Analysis

Although we do not have all the tools necessary yet, it is worthwhile to discuss the idea of prewhitening a series prior to a cross-correlation analysis. The basic idea is simple; in order to use Property 1.3, at least one of the series must be white noise. If this is not the case, there is no simple way to tell if a cross-correlation estimate is significantly different from zero. Hence, in Example 1.28, we were only guessing at the linear dependence relationship between SOI and Recruitment.

For example, in Fig. 17 we generated two series, \(x_{t}\) and \(y_{t}\), for \(t=1,\ldots,120\) independently as

\[x_{t}=2\cos(2\pi\,t\,\tfrac{1}{12})+w_{t1}\quad\text{and}\quad y_{t}=2\cos(2 \pi\,[t+5]\tfrac{1}{12})+w_{t2}\]

where \(\{w_{t1},w_{t2};\,t=1,\ldots,120\}\) are all independent standard normals. The series are made to resemble SOI and Recruitment. The generated data are shown in the top row of the figure. The middle row of Fig. 17 shows the sample ACF of each series, each of which exhibits the cyclic nature of each series. The bottom row (left) of Fig. 17 shows the sample CCF between \(x_{t}\) and \(y_{t}\), which appears to show cross-correlation even though the series are independent. The bottom row (right) also displays the sample CCF between \(x_{t}\) and the prewhitened \(y_{t}\), which shows that the two sequences are uncorrelated. By prewhitening \(y_{t}\), we mean that the signal has been removed from the data by running a regression of \(y_{t}\) on \(\cos(2\pi t)\) and \(\sin(2\pi t)\) [see Example 2.10] and then putting \(\hat{y}_{t}=y_{t}-\hat{y}_{t}\), where \(\hat{y}_{t}\) are the predicted values from the regression.

The following code will reproduce Fig. 17.

``` set.seed(1492) num=120;t=1:num X=ts(2"cos(2"pi*t/12)+rnorm(num),freq=12) Y=ts(2"cos(2"pi*(t+5)/12)+rnorm(num),freq=12) Yw=resid(lm(Y-cos(2"pi*t/12)+sin(2"pi*t/12),na.action=NULL)) par(mfrow=c(3,2),mgp=c(1.6,6,0),mar=c(3,3,1,1))plot(X) plot(Y) acf(X,48, ylab='ACF(X)') acf(Y,48, ylab='ACF(Y)') ccf(X,Y,24, ylab='CCF(X,Y)') ccf(X,Yw,24, ylab='CCF(X,Yw)', ylim=c(-.6,.6))

### 6 Vector-Valued and Multidimensional Series

We frequently encounter situations in which the relationships between a number of jointly measured time series are of interest. For example, in the previous sections, we considered discovering the relationships between the SOI and Recruitment series. Hence, it will be useful to consider the notion of a _vector time series_\(x_{t}=(x_{t1},x_{t2},\ldots,x_{tp})^{\prime}\), which contains as its components \(p\) univariate time series. We denote the \(p\times 1\) column vector of the observed series as \(x_{t}\). The row vector \(x_{t}^{\prime}\) is its transpose. For the stationary case, the \(p\times 1\) mean vector

\[\mu=\mathrm{E}(x_{t}) \tag{43}\]

Figure 17: Display for Example 29. _Top row;_ The generated series. _Middle row:_ The sample ACF of each series. _Bottom row;_ The sample CCF of the series (_left_) and the sample CCF of the first series with the prewhitened second series (_right_)

of the form \(\mu=(\mu_{t1},\mu_{t2},\ldots,\mu_{tP})^{\prime}\) and the \(p\times p\) autocovariance matrix

\[\Gamma(h)=\mathrm{E}[(x_{t+h}-\mu)(x_{t}-\mu)^{\prime}] \tag{1.44}\]

can be defined, where the elements of the matrix \(\Gamma(h)\) are the cross-covariance functions

\[\gamma_{ij}(h)=\mathrm{E}[(x_{t+h,i}-\mu_{i})(x_{tj}-\mu_{j})] \tag{1.45}\]

for \(i,j=1,\ldots,p\). Because \(\gamma_{ij}(h)=\gamma_{ji}(-h)\), it follows that

\[\Gamma(-h)=\Gamma^{\prime}(h). \tag{1.46}\]

Now, the _sample autocovariance matrix_ of the vector series \(x_{t}\) is the \(p\times p\) matrix of sample cross-covariances, defined as

\[\hat{\Gamma}(h)=n^{-1}\sum_{t=1}^{n-h}(x_{t+h}-\bar{x})(x_{t}-\bar{x})^{\prime}, \tag{1.47}\]

where

\[\bar{x}=n^{-1}\sum_{t=1}^{n}x_{t} \tag{1.48}\]

denotes the \(p\times 1\)_sample mean vector_. The symmetry property of the theoretical autocovariance (1.46) extends to the sample autocovariance (1.47), which is defined for negative values by taking

\[\hat{\Gamma}(-h)=\hat{\Gamma}(h)^{\prime}. \tag{1.49}\]

In many applied problems, an observed series may be indexed by more than time alone. For example, the position in space of an experimental unit might be described by two coordinates, say, \(s_{1}\) and \(s_{2}\). We may proceed in these cases by defining a _multidimensional process_\(x_{s}\) as a function of the \(r\times 1\) vector \(s=(s_{1},s_{2},\ldots,s_{r})^{\prime}\), where \(s_{i}\) denotes the coordinate of the \(i\)th index.

**Example 1.30**: **Soil Surface Temperatures**

As an example, the two-dimensional (\(r=2\)) temperature series \(x_{s_{1},s_{2}}\) in Fig. 1.18 is indexed by a row number \(s_{1}\) and a column number \(s_{2}\) that represent positions on a \(64\times 36\) spatial grid set out on an agricultural field. The value of the temperature measured at row \(s_{1}\) and column \(s_{2}\), is denoted by \(x_{s}=x_{s1,s2}\). We can note from the two-dimensional plot that a distinct change occurs in the character of the two-dimensional surface starting at about row 40, where the oscillations along the row axis become fairly stable and periodic. For example, averaging over the 36 columns, we may compute an average value for each \(s_{1}\) as in Fig. 1.19. It is clear that the noise present in the first part of the two-dimensional series is nicely averaged out, and we see a clear and consistent temperature signal.

To generate Figs. 1.18 and 1.19 in R, use the following commands:

``` persp(1:64,1:36,soiltemp,phi=25,theta=25,scale=FALSE,expand=4, ticktype="detailed",xlab="rows",ylab="cols",zlab="temperature") plot.ts(rowMeans(soiltemp),xlab="row",ylab="AverageTemperature")The _autocovariance function_ of a stationary multidimensional process, \(x_{s}\), can be defined as a function of the multidimensional lag vector, say, \(h=(h_{1},h_{2},\ldots,h_{r})^{\prime}\), as

\[\gamma(h)=\mathrm{E}[(x_{s+h}-\mu)(x_{s}-\mu)], \tag{50}\]

where

\[\mu=\mathrm{E}(x_{s}) \tag{51}\]

does not depend on the spatial coordinate \(s\). For the two dimensional temperature process, (50) becomes

\[\gamma(h_{1},h_{2})=\mathrm{E}[(x_{s_{1}+h_{1},s_{2}+h_{2}}-\mu)(x_{s_{1},s_{2 }}-\mu)], \tag{52}\]

which is a function of lag, both in the row (\(h_{1}\)) and column (\(h_{2}\)) directions.

The _multidimensional sample autocovariance function_ is defined as

\[\hat{\gamma}(h)=(S_{1}S_{2}\cdots S_{r})^{-1}\sum_{s_{1}}\sum_{s_{2}}\cdots \sum_{s_{r}}(x_{s+h}-\bar{x})(x_{s}-\bar{x}), \tag{53}\]

where \(s=(s_{1},s_{2},\ldots,s_{r})^{\prime}\) and the range of summation for each argument is \(1\leq s_{i}\leq S_{i}-h_{i}\), for \(i=1,\ldots,r\). The mean is computed over the \(r\)-dimensional array, that is,

\[\bar{x}=(S_{1}S_{2}\cdots S_{r})^{-1}\sum_{s_{1}}\sum_{s_{2}}\cdots\sum_{s_{r} }x_{s_{1},s_{2},\cdots,s_{r}}, \tag{54}\]

where the arguments \(s_{i}\) are summed over \(1\leq s_{i}\leq S_{i}\). The multidimensional sample autocorrelation function follows, as usual, by taking the scaled ratio

\[\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}. \tag{55}\]

Figure 18: Two-dimensional time series of temperature measurements taken on a rectangular field (\(64\times 36\) with 17-foot spacing). Data are from Bazza et al. [15]

**Example 1.31**: **Sample ACF of the Soil Temperature Series**

The autocorrelation function of the two-dimensional (2d) temperature process can be written in the form

\[\hat{\rho}(h_{1},h_{2})=\frac{\hat{\gamma}(h_{1},h_{2})}{\hat{\gamma}(0,0)},\]

where

\[\hat{\gamma}(h_{1},h_{2})=(S_{1}S_{2})^{-1}\sum_{s_{1}}\sum_{s_{2}}(x_{s_{1}+h _{1},s_{2}+h_{2}}-\bar{x})(x_{s_{1},s_{2}}-\bar{x})\]

Figure 1.20 shows the autocorrelation function for the temperature data, and we note the systematic periodic variation that appears along the rows. The autocovariance over columns seems to be strongest for \(h_{1}=0\), implying columns may form replicates of some underlying process that has a periodicity over the rows. This idea can be investigated by examining the mean series over columns as shown in Fig. 1.19.

The easiest way (that we know of) to calculate a 2d ACF in R is by using the fast Fourier transform (FFT) as shown below. Unfortunately, the material needed to understand this approach is given in Chap. 4, Sect. 4.3. The 2d autocovariance function is obtained in two steps and is contained in cs below; \(\hat{\gamma}(0,0)\) is the (1,1) element so that \(\hat{\rho}(h_{1},h_{2})\) is obtained by dividing each element by that value. The 2d ACF is contained in rs below, and the rest of the code is simply to arrange the results to yield a nice display.

 fs = Mod(fft(soiltemp-mean(soiltemp)))*2/(64*36)  cs = Re(fft(fs, inverse-TRUE)/sqrt(64*36)) # ACovF  rs = cs/cs[1,1] # ACF  rs2 = cbind(rs[1:41,21:2], rs[1:41,1:21])  rs3 = rbind(rs2[41:2,], rs2)  par(mar = c(1,2.5,0,0)+.1)  persp(-40:40, -20:20, rs3, phi=30, theta=30, expand=30, scale="FALSE",  ticktype="detailed", xlab="row lags", ylab="column lags",  zlab="ACF")

Figure 1.19: Row averages of the two-dimensional soil temperature profile. \(\bar{x}_{s_{1},*}=\sum_{s_{2}}x_{s_{1},s_{2}}/36\)The sampling requirements for multidimensional processes are rather severe because values must be available over some uniform grid in order to compute the ACF. In some areas of application, such as in soil science, we may prefer to sample a limited number of rows or _transects_ and hope these are essentially replicates of the basic underlying phenomenon of interest. One-dimensional methods can then be applied. When observations are irregular in time space, modifications to the estimators need to be made. Systematic approaches to the problems introduced by irregularly spaced observations have been developed by Journel and Huijbregts [109] or Cressie [45]. We shall not pursue such methods in detail here, but it is worth noting that the introduction of the _variogram_

\[2V_{x}(h)=\text{var}\{x_{s+h}-x_{s}\} \tag{56}\]

and its sample estimator

\[2\hat{V}_{x}(h)=\frac{1}{N(h)}\sum_{s}(x_{s+h}-x_{s})^{2} \tag{57}\]

play key roles, where \(N(h)\) denotes both the number of points located within \(h\), and the sum runs over the points in the neighborhood. Clearly, substantial indexing difficulties will develop from estimators of the kind, and often it will be difficult to find non-negative definite estimators for the covariance function. Problem 27 investigates the relation between the variogram and the autocovariance function in the stationary case.

Figure 20: Two-dimensional autocorrelation function for the soil temperature data

## Problems

### Section 1.1

**1.1** To compare the earthquake and explosion signals, plot the data displayed in Fig. 1.7 on the same graph using different colors or different line types and comment on the results. (The R code in Example 1.11 may be of help on how to add lines to existing plots.)

**1.2** Consider a signal-plus-noise model of the general form \(x_{t}=s_{t}+w_{t}\), where \(w_{t}\) is Gaussian white noise with \(\sigma_{w}^{2}=1\). Simulate and plot \(n=200\) observations from each of the following two models.

(a) \(x_{t}=s_{t}+w_{t}\), for \(t=1,\ldots,200\), where

\[s_{t}=\cases{0,&$t=1,\ldots,100$\cr 10\exp\{-\frac{(t-100)}{20}\}\cos(2\pi t/4),&$t=101, \ldots,200$.\cr}\]

Hint:

s = c(rep(0,100), 10*exp(-(1:100)/20)*cos(2*pi*1:100/4))

x = s + rnorm(200)

plot.ts(x)

(b) \(x_{t}=s_{t}+w_{t}\), for \(t=1,\ldots,200\), where

\[s_{t}=\cases{0,&$t=1,\ldots,100$\cr 10\exp\{-\frac{(t-100)}{200}\}\cos(2\pi t/4),&$t=101, \ldots,200$.\cr}\]

(c) Compare the general appearance of the series (a) and (b) with the earthquake series and the explosion series shown in Fig. 1.7. In addition, plot (or sketch) and compare the signal modulators (a) \(\exp\{-t/20\}\) and (b) \(\exp\{-t/200\}\), for \(t=1,2,\ldots,100\).

### Section 1.2

**1.3** (a) Generate \(n=100\) observations from the autoregression

\[x_{t}=-.9x_{t-2}+w_{t}\]

with \(\sigma_{w}=1\), using the method described in Example 1.10. Next, apply the moving average filter

\[v_{t}=(x_{t}+x_{t-1}+x_{t-2}+x_{t-3})/4\]

to \(x_{t}\), the data you generated. Now plot \(x_{t}\) as a line and superimpose \(v_{t}\) as a dashed line. Comment on the behavior of \(x_{t}\) and how applying the moving average filter changes that behavior. [_Hints:_ Use v = filter(x, rep(1/4, 4), sides = 1) for the filter and note that the R code in Example 1.11 may be of help on how to add lines to existing plots.](b) Repeat (a) but with

\[x_{t}=\cos(2\pi t/4).\]

(c) Repeat (b) but with added N(0, 1) noise,

\[x_{t}=\cos(2\pi t/4)+w_{t}.\]

(d) Compare and contrast (a)-(c); i.e., how does the moving average change each series.

_Sect. 1.3_

**1.4** Show that the autocovariance function can be written as

\[\gamma(s,t)=\mathrm{E}[(x_{s}-\mu_{s})(x_{t}-\mu_{t})]=\mathrm{E}(x_{s}x_{t})- \mu_{s}\mu_{t},\]

where \(\mathrm{E}[x_{t}]=\mu_{t}\).

**1.5** For the two series, \(x_{t}\), in Problem 1.2 (a) and (b):

(a) Compute and plot the mean functions \(\mu_{x}(t)\), for \(t=1,\ldots,200\).

(b) Calculate the autocovariance functions, \(\gamma_{x}(s,t)\), for \(s,t=1,\ldots,200\).

_Sect. 1.4_

**1.6** Consider the time series

\[x_{t}=\beta_{1}+\beta_{2}t+w_{t},\]

where \(\beta_{1}\) and \(\beta_{2}\) are known constants and \(w_{t}\) is a white noise process with variance \(\sigma_{w}^{2}\).

(a) Determine whether \(x_{t}\) is stationary.

(b) Show that the process \(y_{t}=x_{t}-x_{t-1}\) is stationary.

(c) Show that the mean of the moving average

\[v_{t}=\frac{1}{2q+1}\sum_{j=-q}^{q}x_{t-j}\]

is \(\beta_{1}+\beta_{2}t\), and give a simplified expression for the autocovariance function.

**1.7** For a moving average process of the form

\[x_{t}=w_{t-1}+2w_{t}+w_{t+1},\]

where \(w_{t}\) are independent with zero means and variance \(\sigma_{w}^{2}\), determine the autocovariance and autocorrelation functions as a function of lag \(h=s-t\) and plot the ACF as a function of \(h\).

**1.8** Consider the random walk with drift model

\[x_{t}=\delta+x_{t-1}+w_{t},\]

for \(t=1,2,\ldots,\) with \(x_{0}=0\), where \(w_{t}\) is white noise with variance \(\sigma_{w}^{2}\).

(a) Show that the model can be written as \(x_{t}=\delta t+\sum_{k=1}^{t}w_{k}\).

(b) Find the mean function and the autocovariance function of \(x_{t}\).

(c) Argue that \(x_{t}\) is not stationary.

(d) Show \(\rho_{x}(t-1,t)=\sqrt{\frac{t-1}{t}}\to 1\) as \(t\to\infty\). What is the implication of this result?

(e) Suggest a transformation to make the series stationary, and prove that the transformed series is stationary. (Hint: See Problem 1.6b.)

**1.9** A time series with a periodic component can be constructed from

\[x_{t}=U_{1}\sin(2\pi\omega_{0}t)+U_{2}\cos(2\pi\omega_{0}t),\]

where \(U_{1}\) and \(U_{2}\) are independent random variables with zero means and \(\mathrm{E}(U_{1}^{2})=\mathrm{E}(U_{2}^{2})=\sigma^{2}\). The constant \(\omega_{0}\) determines the period or time it takes the process to make one complete cycle. Show that this series is weakly stationary with autocovariance function

\[\gamma(h)=\sigma^{2}\cos(2\pi\omega_{0}h).\]

**1.10** Suppose we would like to predict a single stationary series \(x_{t}\) with zero mean and autocorrelation function \(\gamma(h)\) at some time in the future, say, \(t+\ell\), for \(\ell>0\).

(a) If we predict using only \(x_{t}\) and some scale multiplier \(A\), show that the mean-square prediction error

\[MSE(A)=\mathrm{E}[(x_{t+\ell}-Ax_{t})^{2}]\]

is minimized by the value

\[A=\rho(\ell).\]

(b) Show that the minimum mean-square prediction error is

\[MSE(A)=\gamma(0)[1-\rho^{2}(\ell)].\]

(c) Show that if \(x_{t+\ell}=Ax_{t}\), then \(\rho(\ell)=1\) if \(A>0\), and \(\rho(\ell)=-1\) if \(A<0\).

**1.11** Consider the linear process defined in (1.31).

(a) Verify that the autocovariance function of the process is given by (1.32). Use the result to verify your answer to Problem 1.7. _Hint:_ For \(h\geq 0\), \(\mathrm{cov}(x_{t+h},x_{t})=\mathrm{cov}(\sum_{k}\psi_{k}w_{t+h-k},\sum_{j}\psi _{j}w_{t-j})\). For each \(j\in\mathbb{Z}\), the only "survivor" will be when \(k=h+j\).

(b) Show that \(x_{t}\) exists as a limit in mean square (see Appendix A).

**1.12** For two weakly stationary series \(x_{t}\) and \(y_{t}\), verify (1.30).

**1.13** Consider the two series

\[x_{t}=w_{t}\]

\[y_{t}=w_{t}-\theta w_{t-1}+u_{t},\]

where \(w_{t}\) and \(u_{t}\) are independent white noise series with variances \(\sigma_{w}^{2}\) and \(\sigma_{u}^{2}\), respectively, and \(\theta\) is an unspecified constant.

(a) Express the ACF, \(\rho_{y}(h)\), for \(h=0,\pm 1,\pm 2,\ldots\) of the series \(y_{t}\) as a function of \(\sigma_{w}^{2},\sigma_{u}^{2}\), and \(\theta\).

(b) Determine the CCF, \(\rho_{xy}(h)\) relating \(x_{t}\) and \(y_{t}\).

(c) Show that \(x_{t}\) and \(y_{t}\) are jointly stationary.

**1.14** Let \(x_{t}\) be a stationary normal process with mean \(\mu_{x}\) and autocovariance function \(\gamma(h)\). Define the nonlinear time series

\[y_{t}=\exp\{x_{t}\}.\]

(a) Express the mean function \(\mathrm{E}(y_{t})\) in terms of \(\mu_{x}\) and \(\gamma(0)\). The moment generating function of a normal random variable \(x\) with mean \(\mu\) and variance \(\sigma^{2}\) is

\[M_{x}(\lambda)=\mathrm{E}[\exp\{\lambda x\}]=\exp\biggl{\{}\mu\lambda+\frac{1}{ 2}\sigma^{2}\lambda^{2}\biggr{\}}.\]

(b) Determine the autocovariance function of \(y_{t}\). The sum of the two normal random variables \(x_{t+h}+x_{t}\) is still a normal random variable.

**1.15** Let \(w_{t}\), for \(t=0,\pm 1,\pm 2,\ldots\) be a normal white noise process, and consider the series

\[x_{t}=w_{t}w_{t-1}.\]

Determine the mean and autocovariance function of \(x_{t}\), and state whether it is stationary.

**1.16** Consider the series

\[x_{t}=\sin(2\pi Ut),\]

\(t=1,2,\ldots\), where \(U\) has a uniform distribution on the interval \((0,1)\).

(a) Prove \(x_{t}\) is weakly stationary.

(b) Prove \(x_{t}\) is not strictly stationary.

**1.17** Suppose we have the linear process \(x_{t}\) generated by

\[x_{t}=w_{t}-\theta w_{t-1},\]

\(t=0,1,2,\ldots\), where \(\{w_{t}\}\) is independent and identically distributed with characteristic function \(\phi_{w}(\cdot)\), and \(\theta\) is a fixed constant. [Replace "characteristic function" with "moment generating function" if instructed to do so.]

(a) Express the joint characteristic function of \(x_{1},x_{2},\ldots,x_{n}\), say,

\[\phi_{x_{1},x_{2},\ldots,x_{n}}(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}),\]

in terms of \(\phi_{w}(\cdot)\).

(b) Deduce from (a) that \(x_{t}\) is strictly stationary.

**1.18** Suppose that \(x_{t}\) is a linear process of the form (1.31). Prove

\[\sum_{h=-\infty}^{\infty}|\gamma(h)|<\infty.\]_Sect. 1.5_

**1.19**: Suppose \(x_{t}=\mu+w_{t}+\theta w_{t-1}\), where \(w_{t}\sim wn(0,\sigma_{w}^{2})\).

(a) Show that mean function is \(\mathrm{E}(x_{t})=\mu\).

(b) Show that the autocovariance function of \(x_{t}\) is given by \(\gamma_{x}(0)=\sigma_{w}^{2}(1+\theta^{2})\), \(\gamma_{x}(\pm 1)=\sigma_{w}^{2}\theta\), and \(\gamma_{x}(h)=0\) otherwise.

(c) Show that \(x_{t}\) is stationary for all values of \(\theta\in\mathbb{R}\).

(d) Use (1.35) to calculate \(\mathrm{var}(\bar{x})\) for estimating \(\mu\) when (i) \(\theta=1\), (ii) \(\theta=0\), and (iii) \(\theta=-1\)

(e) In time series, the sample size \(n\) is typically large, so that \(\frac{(n-1)}{n}\approx 1\). With this as a consideration, comment on the results of part (d); in particular, how does the accuracy in the estimate of the mean \(\mu\) change for the three different cases?

**1.20**: (a) Simulate a series of \(n=500\) Gaussian white noise observations as in Example 1.8 and compute the sample ACF, \(\hat{\rho}(h)\), to lag 20. Compare the sample ACF you obtain to the actual ACF, \(\rho(h)\). [Recall Example 1.19.]

(b) Repeat part (a) using only \(n=50\). How does changing \(n\) affect the results?

**1.21**: (a) Simulate a series of \(n=500\) moving average observations as in Example 1.9 and compute the sample ACF, \(\hat{\rho}(h)\), to lag 20. Compare the sample ACF you obtain to the actual ACF, \(\rho(h)\). [Recall Example 1.20.]

(b) Repeat part (a) using only \(n=50\). How does changing \(n\) affect the results?

**1.22**: Although the model in Problem 1.2(a) is not stationary (Why?), the sample ACF can be informative. For the data you generated in that problem, calculate and plot the sample ACF, and then comment.

**1.23**: Simulate a series of \(n=500\) observations from the signal-plus-noise model presented in Example 1.12 with \(\sigma_{w}^{2}=1\). Compute the sample ACF to lag 100 of the data you generated and comment.

**1.24**: For the time series \(y_{t}\) described in Example 1.26, verify the stated result that \(\rho_{y}(1)=-.47\) and \(\rho_{y}(h)=0\) for \(h>1\).

**1.25**: A real-valued function \(g(t)\), defined on the integers, is non-negative definite if and only if

\[\sum_{i=1}^{n}\sum_{j=1}^{n}a_{i}g(t_{i}-t_{j})a_{j}\geq 0\]

for all positive integers \(n\) and for all vectors \(a=(a_{1},a_{2},\ldots,a_{n})^{\prime}\) and \(t=(t_{1},t_{2},\ldots,t_{n})^{\prime}\). For the matrix \(G=\{g(t_{i}-t_{j});\,i,j=1,2,\ldots,n\}\), this implies that \(a^{\prime}Ga\geq 0\) for all vectors \(a\). It is called positive definite if we can replace '\(\geq\)' with '\(>\)' for all \(a\neq 0\), the zero vector.

(a) Prove that \(\gamma(h)\), the autocovariance function of a stationary process, is a non-negative definite function.

(b) Verify that the sample autocovariance \(\hat{\gamma}(h)\) is a non-negative definite function.

_Sect. 1.6_

**1.26** Consider a collection of time series \(x_{1t},x_{2t},\ldots,x_{Nt}\) that are observing some common signal \(\mu_{t}\) observed in noise processes \(e_{1t},e_{2t},\ldots,e_{Nt}\), with a model for the \(j\)-th observed series given by

\[x_{jt}=\mu_{t}+e_{jt}.\]

Suppose the noise series have zero means and are uncorrelated for different \(j\). The common autocovariance functions of all series are given by \(\gamma_{e}(s,t)\). Define the sample mean

\[\bar{x}_{t}=\frac{1}{N}\sum_{j=1}^{N}x_{jt}.\]

(a) Show that \(\mathrm{E}[\bar{x}_{t}]=\mu_{t}\).

(b) Show that \(\mathrm{E}[(\bar{x}_{t}-\mu)^{2})]=N^{-1}\gamma_{e}(t,t)\).

(c) How can we use the results in estimating the common signal?

**1.27** A concept used in _geostatistics_, see Journel and Huijbregts [109] or Cressie [45], is that of the _variogram_, defined for a spatial process \(x_{s}\), \(s=(s_{1},s_{2})\), for \(s_{1},s_{2}=0,\pm 1,\pm 2,\ldots\), as

\[V_{x}(h)=\frac{1}{2}\mathrm{E}[(x_{s+h}-x_{s})^{2}],\]

where \(h=(h_{1},h_{2})\), for \(h_{1}\), \(h_{2}=0,\pm 1,\pm 2,\ldots\) Show that, for a stationary process, the variogram and autocovariance functions can be related through

\[V_{x}(h)=\gamma(0)-\gamma(h),\]

where \(\gamma(h)\) is the usual lag \(h\) covariance function and \(0=(0,0)\). Note the easy extension to any spatial dimension.

_The following problems require the material given in Appendix A_

**1.28** Suppose \(x_{t}=\beta_{0}+\beta_{1}t\), where \(\beta_{0}\) and \(\beta_{1}\) are constants. Prove as \(n\to\infty\), \(\hat{\rho}_{x}(h)\to 1\) for fixed \(h\), where \(\hat{\rho}_{x}(h)\) is the ACF (1.37).

**1.29** (a) Suppose \(x_{t}\) is a weakly stationary time series with mean zero and with absolutely summable autocovariance function, \(\gamma(h)\), such that

\[\sum_{h=-\infty}^{\infty}\gamma(h)=0.\]

Prove that \(\sqrt{n}\ \bar{x}\overset{p}{\to}0\), where \(\bar{x}\) is the sample mean (1.34).

(b) Give an example of a process that satisfies the conditions of part (a). What is special about this process?

**1.30** Let \(x_{t}\) be a linear process of the form (A.43)-(A.44). If we define

\[\tilde{\gamma}(h)=n^{-1}\sum_{t=1}^{n}(x_{t+h}-\mu_{x})(x_{t}-\mu_{x}),\]

show that

\[n^{1/2}\big{(}\tilde{\gamma}(h)-\tilde{\gamma}(h)\big{)}=o_{p}(1).\]

Hint: The Markov Inequality

\[\Pr\{|x|\geq\epsilon\}<\frac{\mathbb{E}|x|}{\epsilon}\]

can be helpful for the cross-product terms.

**1.31** For a linear process of the form

\[x_{t}=\sum_{j=0}^{\infty}\phi^{j}w_{t-j},\]

where \(\{w_{t}\}\) satisfies the conditions of Theorem A.7 and \(|\phi|<1\), show that

\[\sqrt{n}\frac{(\hat{\rho}_{x}(1)-\rho_{x}(1))}{\sqrt{1-\rho_{x}^{2}(1)}} \stackrel{{ d}}{{\to}}N(0,1),\]

and construct a \(95\%\) confidence interval for \(\phi\) when \(\hat{\rho}_{x}(1)=.64\) and \(n=100\).

**1.32** Let \(\{x_{t};\ t=0,\pm 1,\pm 2,\ldots\}\) be \(\mathrm{iid}(0,\sigma^{2})\).

(a) For \(h\geq 1\) and \(k\geq 1\), show that \(x_{t}x_{t+h}\) and \(x_{s}x_{s+k}\) are uncorrelated for all \(s\neq t\).

(b) For fixed \(h\geq 1\), show that the \(h\times 1\) vector

\[\sigma^{-2}n^{-1/2}\sum_{t=1}^{n}(x_{t}x_{t+1},\ldots,x_{t}x_{t+h})^{\prime} \stackrel{{ d}}{{\to}}(z_{1},\ldots,z_{h})^{\prime}\]

where \(z_{1},\ldots,z_{h}\) are iid N\((0,1)\) random variables. [Hint: Use the Cramer-Wold device.]

(c) Show, for each \(h\geq 1\),

\[n^{-1/2}\left[\sum_{t=1}^{n}x_{t}.x_{t+h}-\sum_{t=1}^{n-h}(x_{t}-\bar{x})(x_{t+ h}-\bar{x})\right]\stackrel{{ p}}{{\to}}0\qquad\mbox{as $n\to\infty$}\]

where \(\bar{x}=n^{-1}\sum_{t=1}^{n}x_{t}\).

(d) Noting that \(n^{-1}\sum_{t=1}^{n}x_{t}^{2}\stackrel{{ p}}{{\to}}\sigma^{2}\) by the WLLN, conclude that

\[n^{1/2}\left[\hat{\rho}(1),\ldots,\hat{\rho}(h)\right]^{\prime}\stackrel{{ d}}{{\to}}(z_{1},\ldots,z_{h})^{\prime}\]

where \(\hat{\rho}(h)\) is the sample ACF of the data \(x_{1},\ldots,x_{n}\).

## Chapter 2 Time Series Regression and Exploratory Data Analysis

In this chapter we introduce classical multiple linear regression in a time series context, model selection, exploratory data analysis for preprocessing nonstationary time series (for example trend removal), the concept of differencing and the backshift operator, variance stabilization, and nonparametric smoothing of time series.

### 2.1 Classical Regression in the Time Series Context

We begin our discussion of linear regression in the time series context by assuming some output or _dependent_ time series, say, \(x_{t}\), for \(t=1,\ldots,n\), is being influenced by a collection of possible inputs or _independent_ series, say, \(z_{t1},z_{t2},\ldots,z_{tq}\), where we first regard the inputs as fixed and known. This assumption, necessary for applying conventional linear regression, will be relaxed later on. We express this relation through the _linear regression model_

\[x_{t}=\beta_{0}+\beta_{1}z_{t1}+\beta_{2}z_{t2}+\cdots+\beta_{q}z_{tq}+w_{t}, \tag{2.1}\]

where \(\beta_{0},\beta_{1},\ldots,\beta_{q}\) are unknown fixed regression coefficients, and \(\{w_{t}\}\) is a random error or noise process consisting of independent and identically distributed (iid) normal variables with mean zero and variance \(\sigma_{w}^{2}\). For time series regression, it is rarely the case that the noise is white, and we will need to eventually relax that assumption. A more general setting within which to embed mean square estimation and linear regression is given in Appendix B, where we introduce Hilbert spaces and the Projection Theorem.

**Example 2.1**: **Estimating a Linear Trend**

Consider the monthly price (per pound) of a chicken in the US from mid-2001 to mid-2016 (180 months), say \(x_{t}\), shown in Fig. 2.1. There is an obvious upward trend in the series, and we might use simple linear regression to estimate that trend by fitting the model\[x_{t}=\beta_{0}+\beta_{1}z_{t}+w_{t},\quad z_{t}=2001\tfrac{7}{12},2001\tfrac{8}{12},\ldots,2016\tfrac{6}{12}.\]

This is in the form of the regression model (1) with \(q=1\). Note that we are making the assumption that the errors, \(w_{t}\), are an iid normal sequence, which may not be true; the problem of autocorrelated errors is discussed in detail in Chap. 3.

In ordinary least squares (OLS), we minimize the error sum of squares

\[Q=\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}(x_{t}-[\beta_{0}+\beta_{1}z_{t}])^{2}\]

with respect to \(\beta_{i}\) for \(i=0,1\). In this case we can use simple calculus to evaluate \(\partial Q/\partial\beta_{i}=0\) for \(i=0,1\), to obtain two equations to solve for the \(\beta\)s. The OLS estimates of the coefficients are explicit and given by

\[\hat{\beta}_{1}=\frac{\sum_{t=1}^{n}(x_{t}-\bar{x})(z_{t}-\bar{z})}{\sum_{t=1} ^{n}(z_{t}-\bar{z})^{2}}\quad\text{and}\quad\hat{\beta}_{0}=\bar{x}-\hat{\beta }_{1}\,\bar{z},\]

where \(\bar{x}=\sum_{t}x_{t}/n\) and \(\bar{z}=\sum_{t}z_{t}/n\) are the respective sample means.

Using R, we obtained the estimated slope coefficient of \(\hat{\beta}_{1}=3.59\) (with a standard error of.08) yielding a significant estimated increase of about 3.6 cents per year. Finally, Fig. 2.1 shows the data with the estimated trend line superimposed. R code with partial output:

```
summary(fit<-lm(chicken-time(chicken),na.action-NULL)) EstimateStd.Errort.value (Intercept)-7131.02162.41-43.9 time(chicken)3.590.0844.4 -- Residualstandarderror:4.7on178degreesoffreedom plot(chicken,ylab="centsperpound") abline(fit)#addthefittedline
```

The multiple linear regression model described by (1) can be conveniently written in a more general notation by defining the column vectors

Figure 2.1: The price of chicken: monthly whole bird spot price, Georgia docks, US cents per pound, August 2001 to July 2016, with fitted linear trend line

and \(\beta=(\beta_{0},\beta_{1},\ldots,\beta_{q})^{\prime}\), where \({}^{\prime}\) denotes transpose, so (2.1) can be written in the alternate form

\[x_{t}=\beta_{0}+\beta_{1}z_{t1}+\cdots+\beta_{q}z_{tq}+w_{t}=\beta^{\prime}z_{t} +w_{t}. \tag{2.2}\]

where \(w_{t}\sim\text{iid N}(0,\sigma_{w}^{2})\). As in the previous example, OLS estimation finds the coefficient vector \(\beta\) that minimizes the error sum of squares

\[Q=\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}(x_{t}-\beta^{\prime}z_{t})^{2}, \tag{2.3}\]

with respect to \(\beta_{0},\beta_{1},\ldots,\beta_{q}\). This minimization can be accomplished by differentiating (2.3) with respect to the vector \(\beta\) or by using the properties of projections. Either way, the solution must satisfy \(\sum_{t=1}^{n}(x_{t}-\hat{\beta}^{\prime}z_{t})z_{t}^{\prime}=0\). This procedure gives the _normal equations_

\[\left(\sum_{t=1}^{n}z_{t}z_{t}^{\prime}\right)\hat{\beta}=\sum_{t=1}^{n}z_{t} x_{t}. \tag{2.4}\]

If \(\sum_{t=1}^{n}z_{t}z_{t}^{\prime}\) is non-singular, the least squares estimate of \(\beta\) is

\[\hat{\beta}=\left(\sum_{t=1}^{n}z_{t}z_{t}^{\prime}\right)^{-1}\sum_{t=1}^{n}z _{t}x_{t}.\]

The minimized error sum of squares (2.3), denoted \(SSE\), can be written as

\[SSE=\sum_{t=1}^{n}(x_{t}-\hat{\beta}^{\prime}z_{t})^{2}. \tag{2.5}\]

The ordinary least squares estimators are unbiased, i.e., \(\text{E}(\hat{\beta})=\beta\), and have the smallest variance within the class of linear unbiased estimators.

If the errors \(w_{t}\) are normally distributed, \(\hat{\beta}\) is also the maximum likelihood estimator for \(\beta\) and is normally distributed with

\[\text{cov}(\hat{\beta})=\ \sigma_{w}^{2}C\,, \tag{2.6}\]

where

\[C=\left(\sum_{t=1}^{n}z_{t}z_{t}^{\prime}\right)^{-1} \tag{2.7}\]

is a convenient notation. An unbiased estimator for the variance \(\sigma_{w}^{2}\) is

\[s_{w}^{2}=MSE=\frac{SSE}{n-(q+1)}, \tag{2.8}\]

where \(MSE\) denotes the _mean squared error_. Under the normal assumption,

\[\text{t}=\frac{(\hat{\beta}_{i}-\beta_{i})}{s_{w}\sqrt{c_{ii}}} \tag{2.9}\]has the t-distribution with \(n-(q+1)\) degrees of freedom; \(c_{ii}\) denotes the \(i\)-th diagonal element of \(C\), as defined in (2.7). This result is often used for individual tests of the null hypothesis H\({}_{0}\): \(\beta_{i}=0\) for \(i=1,\ldots,q\).

Various competing models are often of interest to isolate or select the best subset of independent variables. Suppose a proposed model specifies that only a subset \(r<q\) independent variables, say, \(z_{t,1:r}=\{z_{t1},z_{t2},\ldots,z_{tr}\}\) is influencing the dependent variable \(x_{t}\). The reduced model is

\[x_{t}=\beta_{0}+\beta_{1}z_{t1}+\cdots+\beta_{r}z_{tr}+w_{t} \tag{2.10}\]

where \(\beta_{1},\beta_{2},\ldots,\beta_{r}\) are a subset of coefficients of the original \(q\) variables.

The null hypothesis in this case is H\({}_{0}\): \(\beta_{r+1}=\cdots=\beta_{q}=0\). We can test the reduced model (2.10) against the full model (2.2) by comparing the error sums of squares under the two models using the \(F\)-statistic

\[F=\frac{(SSE_{r}-SSE)/(q-r)}{SSE/(n-q-1)}=\frac{MSR}{MSE}, \tag{2.11}\]

where \(SSE_{r}\) is the error sum of squares under the reduced model (2.10). Note that \(SSE_{r}\geq SSE\) because the full model has more parameters. If H\({}_{0}\): \(\beta_{r+1}=\cdots=\beta_{q}=0\) is true, then \(SSE_{r}\approx SSE\) because the estimates of those \(\beta\)s will be close to 0. Hence, we do not believe H\({}_{0}\) if \(SSR=SSE_{r}-SSE\) is big. Under the null hypothesis, (2.11) has a central \(F\)-distribution with \(q-r\) and \(n-q-1\) degrees of freedom when (2.10) is the correct model.

These results are often summarized in an _Analysis of Variance (ANOVA)_ table as given in Table 2.1 for this particular case. The difference in the numerator is often called the regression sum of squares (\(SSR\)). The null hypothesis is rejected at level \(\alpha\) if \(F>F_{n-q-1}^{q-r}(\alpha)\), the \(1-\alpha\) percentile of the \(F\) distribution with \(q-r\) numerator and \(n-q-1\) denominator degrees of freedom.

A special case of interest is the null hypothesis H\({}_{0}\): \(\beta_{1}=\cdots=\beta_{q}=0\). In this case \(r=0\), and the model in (2.10) becomes

\[x_{t}=\beta_{0}+w_{t}\.\]

We may measure the proportion of variation accounted for by all the variables using

\[R^{2}=\frac{SSE_{0}-SSE}{SSE_{0}}, \tag{2.12}\]

where the residual sum of squares under the reduced model is

\[SSE_{0}=\sum_{t=1}^{n}(x_{t}-\bar{x})^{2}. \tag{2.13}\]

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Source & df & Sum of squares & Mean square & \(F\) \\ \hline \(z_{t,r+1:q}\) & \(q-r\) & \(SSR=SSE_{r}-SSE\) & \(MSR=SSR/(q-r)\) & \(F=\frac{MSR}{MSE}\) \\ Error & \(n-(q+1)\) & \(SSE\) & \(MSE=SSE/(n-q-1)\) & \\ \hline \hline \end{tabular}
\end{table}
Table 2.1: Analysis of variance for regression In this case \(SSE_{0}\) is the sum of squared deviations from the mean \(\bar{x}\) and is otherwise known as the adjusted total sum of squares. The measure \(R^{2}\) is called the _coefficient of determination_.

The techniques discussed in the previous paragraph can be used to test various models against one another using the \(F\) test given in (2.11). These tests have been used in the past in a stepwise manner, where variables are added or deleted when the values from the \(F\)-test either exceed or fail to exceed some predetermined levels. The procedure, called _stepwise multiple regression_, is useful in arriving at a set of useful variables. An alternative is to focus on a procedure for _model selection_ that does not proceed sequentially, but simply evaluates each model on its own merits. Suppose we consider a normal regression model with \(k\) coefficients and denote the _maximum likelihood estimator_ for the variance as

\[\hat{\sigma}_{k}^{2}=\frac{SSE(k)}{n}, \tag{2.14}\]

where \(SSE(k)\) denotes the residual sum of squares under the model with \(k\) regression coefficients. Then, Akaike [1, 2, 3] suggested measuring the goodness of fit for this particular model by balancing the error of the fit against the number of parameters in the model; we define the following.1

Footnote 1: Formally, AIC is defined as \(-2\log L_{k}+2k\) where \(L_{k}\) is the maximized likelihood and \(k\) is the number of parameters in the model. For the normal regression problem, AIC can be reduced to the form given by (2.15). AIC is an estimate of the Kullback-Leibler discrepancy between a true model and a candidate model; see Problem 2.4 and Problem 2.5 for further details.

**Definition 2.1 Akaike's Information Criterion (AIC)**

\[\text{AIC}=\log\ \hat{\sigma}_{k}^{2}+\frac{n+2k}{n}, \tag{2.15}\]

_where \(\hat{\sigma}_{k}^{2}\) is given by (2.14) and \(k\) is the number of parameters in the model._

The value of \(k\) yielding the minimum AIC specifies the best model. The idea is roughly that minimizing \(\hat{\sigma}_{k}^{2}\) would be a reasonable objective, except that it decreases monotonically as \(k\) increases. Therefore, we ought to penalize the error variance by a term proportional to the number of parameters. The choice for the penalty term given by (2.15) is not the only one, and a considerable literature is available advocating different penalty terms. A corrected form, suggested by Sugiura [196], and expanded by Hurvich and Tsai [100], can be based on small-sample distributional results for the linear regression model (details are provided in Problem 2.4 and Problem 2.5). The corrected form is defined as follows.

**Definition 2.2 AIC, Bias Corrected (AICc)**

\[\text{AICc}=\log\ \hat{\sigma}_{k}^{2}+\frac{n+k}{n-k-2}, \tag{2.16}\]_where \(\hat{\sigma}_{k}^{2}\) is given by (2.14), \(k\) is the number of parameters in the model, and \(n\) is the sample size._

We may also derive a correction term based on Bayesian arguments, as in Schwarz [175], which leads to the following.

**Definition 2.3** **Bayesian Information Criterion (BIC)**: \[\mathrm{BIC}=\log\ \hat{\sigma}_{k}^{2}+\frac{k\log n}{n},\] (2.17)

_using the same notation as in Definition 2.2._

BIC is also called the Schwarz Information Criterion (SIC); see also Rissanen [166] for an approach yielding the same statistic based on a minimum description length argument. Notice that the penalty term in BIC is much larger than in AIC, consequently, BIC tends to choose smaller models. Various simulation studies have tended to verify that BIC does well at getting the correct order in large samples, whereas AICc tends to be superior in smaller samples where the relative number of parameters is large; see McQuarrie and Tsai [138] for detailed comparisons. In fitting regression models, two measures that have been used in the past are adjusted R-squared, which is essentially \(s_{w}^{2}\), and Mallows \(C_{p}\), Mallows [133], which we do not consider in this context.

**Example 2.2**: **Pollution, Temperature and Mortality**:

The data shown in Fig. 2.2 are extracted series from a study by Shumway et al. [183] of the possible effects of temperature and pollution on weekly mortality in Los Angeles County. Note the strong seasonal components in all of the series, corresponding to winter-summer variations and the downward trend in the cardiovascular mortality over the 10-year period.

A scatterplot matrix, shown in Fig. 2.3, indicates a possible linear relation between mortality and the pollutant particulates and a possible relation to temperature. Note the curvilinear shape of the temperature mortality curve, indicating that higher temperatures as well as lower temperatures are associated with increases in cardiovascular mortality.

Based on the scatterplot matrix, we entertain, tentatively, four models where \(M_{t}\) denotes cardiovascular mortality, \(T_{t}\) denotes temperature and \(P_{t}\) denotes the particulate levels. They are

\[M_{t} = \beta_{0}+\beta_{1}t+w_{t} \tag{2.18}\] \[M_{t} = \beta_{0}+\beta_{1}t+\beta_{2}(T_{t}-T_{\cdot})+w_{t}\] (2.19) \[M_{t} = \beta_{0}+\beta_{1}t+\beta_{2}(T_{t}-T_{\cdot})+\beta_{3}(T_{t}-T _{\cdot})^{2}+w_{t}\] (2.20) \[M_{t} = \beta_{0}+\beta_{1}t+\beta_{2}(T_{t}-T_{\cdot})+\beta_{3}(T_{t}-T _{\cdot})^{2}+\beta_{4}P_{t}+w_{t} \tag{2.21}\]

where we adjust temperature for its mean, \(T_{\cdot}=74.26\), to avoid collinearity problems. It is clear that (2.18) is a trend only model, (2.19) is linear temperature, (2.20)is curvilinear temperature and (2.21) is curvilinear temperature and pollution. We summarize some of the statistics given for this particular case in Table 2.2.

We note that each model does substantially better than the one before it and that the model including temperature, temperature squared, and particulates does the best, accounting for some 60% of the variability and with the best value for AIC and BIC (because of the large sample size, AIC and AICc are nearly the same). Note that one can compare any two models using the residual sums of squares and (2.11). Hence, a model with only trend could be compared to the full model, H\({}_{0}\): \(\beta_{2}=\beta_{3}=\beta_{4}=0\), using \(q=4,r=1,n=508\), and

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Model & \(k\) & SSE & df & MSE & \(R^{2}\) & AIC & BIC \\ \hline (2.18) & 2 & 40,020 & 506 & 79.0 &.21 & 5.38 & 5.40 \\ (2.19) & 3 & 31,413 & 505 & 62.2 &.38 & 5.14 & 5.17 \\ (2.20) & 4 & 27,985 & 504 & 55.5 &.45 & 5.03 & 5.07 \\ (2.21) & 5 & 20,508 & 503 & 40.8 &.60 & 4.72 & 4.77 \\ \hline \end{tabular}
\end{table}
Table 2.2: Summary statistics for mortality models

Figure 2.2: Average weekly cardiovascular mortality (_top_), temperature (_middle_) and particulate pollution (_bottom_) in Los Angeles County. There are 508 six-day smoothed averages obtained by filtering daily values over the 10 year period 1970–1979

\[F_{3,503}=\frac{(40,020-20,508)/3}{20,508/503}=160,\]

which exceeds \(F_{3,503}(.001)=5.51\). We obtain the best prediction model,

\[\hat{M}_{t} = 2831.5-1.396_{(.10)}t-.472_{(.032)}(T_{t}-74.26)\] \[+.023_{(.003)}(T_{t}-74.26)^{2}+.255_{(.019)}P_{t},\]

for mortality, where the standard errors, computed from (2.6)-(2.8), are given in parentheses. As expected, a negative trend is present in time as well as a negative coefficient for adjusted temperature. The quadratic effect of temperature can clearly be seen in the scatterplots of Fig. 2.3. Pollution weights positively and can be interpreted as the incremental contribution to daily deaths per unit of particulate pollution. It would still be essential to check the residuals \(\hat{w}_{t}=M_{t}-\hat{M}_{t}\) for autocorrelation (of which there is a substantial amount), but we defer this question to Sect. 3.8 when we discuss regression with correlated errors.

Below is the R code to plot the series, display the scatterplot matrix, fit the final regression model (2.21), and compute the corresponding values of AIC, AICc and BIC.2 Finally, the use of na.action in lm() is to retain the time series attributes for the residuals and fitted values.

Figure 2.3: Scatterplot matrix showing relations between mortality, temperature, and pollution

```
par(mfrow=c(3,1)) #plotthedata plot(cmort,main="CardiovascularMortality",xlab="",ylab="") plot(tempr,main="Temperature",xlab="",ylab="") plot(part,main="Particulates",xlab="",ylab="") dev.new() #openarewgraphicdevice ts.plot(cmort,tempr,part,col=1:3) #allonsameplot(notshown) dev.new() pairs(cbind(Mortality=cmort,Temperature=tempr,Particulates=part)) temp=temp-mean(tempr) #centertemperature temp2=temp^2 trend=time(cmort) #time fit=lm(cmort-trend+temp+temp2+part,na.action=NULL) summary(fit) #regressionresults summary(aov(fit)) #ANOVAtable(comparetonextline) summary(aov(lm(cmort-cbind(trend,temp,temp2,part)))) #Table2.1 num=length(cmort) #samplesize AIC(fit)/num-log(2*pi) #AIC BIC(fit)/num-log(2*pi) #RIC (AICc=log(sum(resid(fit)^2)/num)+(num+5)/(num-5-2)) #AICc
```

As previously mentioned, it is possible to include lagged variables in time series regression models and we will continue to discuss this type of problem throughout the text. This concept is explored further in Problem 2.2 and Problem 2.10. The following is a simple example of lagged regression.

**Example 2.3**: **Regression With Lagged Variables**

In Example 1.28, we discovered that the Southern Oscillation Index (SOI) measured at time \(t-6\) months is associated with the Recruitment series at time \(t\), indicating that the SOI leads the Recruitment series by six months. Although there is evidence that the relationship is not linear (this is discussed further in Example 2.8 and Example 2.9), consider the following regression,

\[R_{t}=\beta_{0}+\beta_{1}S_{t-6}+w_{t}, \tag{2.22}\]

where \(R_{t}\) denotes Recruitment for month \(t\) and \(S_{t-6}\) denotes SOI six months prior. Assuming the \(w_{t}\) sequence is white, the fitted model is

\[\hat{R}_{t}=65.79-44.28_{(2.78)}S_{t-6} \tag{2.23}\]

with \(\hat{\sigma}_{w}=22.5\) on 445 degrees of freedom. This result indicates the strong predictive ability of SOI for Recruitment six months in advance. Of course, it is still essential to check the model assumptions, but again we defer this until later.

Performing lagged regression in R is a little difficult because the series must be aligned prior to running the regression. The easiest way to do this is to create a data frame (that we call fish) using ts.intersect, which aligns the lagged series.

fish=ts.intersect(rec,soil6=lag(soi,-6),dframe=TRUE) summary(fit1<-lm(rec-soil6,data=fish,na.action=NULL)) The headache of aligning the lagged series can be avoided by using the R package dynlm, which must be downloaded and installed.

library(dynlm) summary(fit2<-dynlm(rec-L(soi,6)))We note that fit2 is similar to the fit1 object, but the time series attributes are retained without any additional commands.

### Exploratory Data Analysis

In general, it is necessary for time series data to be stationary so that averaging lagged products over time, as in the previous section, will be a sensible thing to do. With time series data, it is the dependence between the values of the series that is important to measure; we must, at least, be able to estimate autocorrelations with precision. It would be difficult to measure that dependence if the dependence structure is not regular or is changing at every time point. Hence, to achieve any meaningful statistical analysis of time series data, it will be crucial that, if nothing else, the mean and the autocovariance functions satisfy the conditions of stationarity (for at least some reasonable stretch of time) stated in Definition 1.7. Often, this is not the case, and we will mention some methods in this section for playing down the effects of nonstationarity so the stationary properties of the series may be studied.

A number of our examples came from clearly nonstationary series. The Johnson & Johnson series in Fig. 1 has a mean that increases exponentially over time, and the increase in the magnitude of the fluctuations around this trend causes changes in the covariance function; the variance of the process, for example, clearly increases as one progresses over the length of the series. Also, the global temperature series shown in Fig. 2 contains some evidence of a trend over time; human-induced global warming advocates seize on this as empirical evidence to advance the hypothesis that temperatures are increasing.

Perhaps the easiest form of nonstationarity to work with is the _trend stationary_ model wherein the process has stationary behavior around a trend. We may write this type of model as

\[x_{t}=\mu_{t}+y_{t} \tag{2.24}\]

where \(x_{t}\) are the observations, \(\mu_{t}\) denotes the trend, and \(y_{t}\) is a stationary process. Quite often, strong trend will obscure the behavior of the stationary process, \(y_{t}\), as we shall see in numerous examples. Hence, there is some advantage to removing the trend as a first step in an exploratory analysis of such time series. The steps involved are to obtain a reasonable estimate of the trend component, say \(\hat{\mu}_{t}\), and then work with the residuals

\[\hat{y}_{t}=x_{t}-\hat{\mu}_{t}. \tag{2.25}\]

**Example 2.4**: **Detrending Chicken Prices**__

Here we suppose the model is of the form of (2.24),

\[x_{t}=\mu_{t}+y_{t},\]

where, as we suggested in the analysis of the chicken price data presented in Example 2.1, a straight line might be useful for detrending the data; i.e.,\[\mu_{t}=\beta_{0}+\beta_{1}\,t.\]

In that example, we estimated the trend using ordinary least squares and found

\[\hat{\mu}_{t}=-7131+3.59\,t\]

where we are using \(t\) instead of \(z_{t}\) for time. Figure 2.1 shows the data with the estimated trend line superimposed. To obtain the detrended series we simply subtract \(\hat{\mu}_{t}\) from the observations, \(x_{t}\), to obtain the detrended series3

Footnote 3: Because the error term, \(y_{t}\), is not assumed to be iid, the reader may feel that weighted least squares is called for in this case. The problem is, we do not know the behavior of \(y_{t}\) and that is precisely what we are trying to assess at this stage. A notable result by Grenander and Rosenblatt [82, Ch 7], however, is that under mild conditions on \(y_{t}\), for polynomial regression or periodic regression, asymptotically, ordinary least squares is equivalent to weighted least squares with regard to efficiency.

\[\hat{y}_{t}=x_{t}+7131-3.59\,t.\]

The top graph of Fig. 2.4 shows the detrended series. Figure 2.5 shows the ACF of the original data (top panel) as well as the ACF of the detrended data (middle panel).

In Example 1.11 and the corresponding Fig. 1.10 we saw that a random walk might also be a good model for trend. That is, rather than modeling trend as fixed (as in Example 2.4), we might model trend as a stochastic component using the random walk with drift model,

\[\mu_{t}=\delta+\mu_{t-1}+w_{t}, \tag{2.26}\]

where \(w_{t}\) is white noise and is independent of \(y_{t}\). If the appropriate model is (2.24), then _differencing_ the data, \(x_{t}\), yields a stationary process; that is,

Figure 2.4: Detrended (_top_) and differenced (_bottom_) chicken price series. The original data are shown in Fig. 2.1

\[x_{t}-x_{t-1} =(\mu_{t}+y_{t})-(\mu_{t-1}+y_{t-1}) \tag{2.27}\] \[=\delta+w_{t}+y_{t}-y_{t-1}.\]

It is easy to show \(z_{t}=y_{t}-y_{t-1}\) is stationary using Chap. 1.1. That is, because \(y_{t}\) is stationary,

\[\gamma_{z}(h) =\operatorname{cov}(z_{t+h},z_{t})=\operatorname{cov}(y_{t+h}-y_ {t+h-1},y_{t}-y_{t-1})\] \[=2\gamma_{y}(h)-\gamma_{y}(h+1)-\gamma_{y}(h-1)\]

is independent of time; we leave it as an exercise (Problem 2.7) to show that \(x_{t}-x_{t-1}\) in (2.27) is stationary.

One advantage of differencing over detrending to remove trend is that no parameters are estimated in the differencing operation. One disadvantage, however, is that differencing does not yield an estimate of the stationary process \(y_{t}\) as can be seen in (2.27). If an estimate of \(y_{t}\) is essential, then detrending may be more appropriate. If the goal is to coerce the data to stationarity, then differencing may be more appropriate. Differencing is also a viable tool if the trend is fixed, as in Example 2.4. That is, e.g., if \(\mu_{t}=\beta_{0}+\beta_{1}\,t\) in the model (2.24), differencing the data produces stationarity (see Problem 2.6):

\[x_{t}-x_{t-1}=(\mu_{t}+y_{t})-(\mu_{t-1}+y_{t-1})=\beta_{1}+y_{t}-y_{t-1}.\]

Because differencing plays a central role in time series analysis, it receives its own notation. The first difference is denoted as

\[\nabla x_{t}=x_{t}-x_{t-1}. \tag{2.28}\]

As we have seen, the first difference eliminates a linear trend. A second difference, that is, the difference of (2.28), can eliminate a quadratic trend, and so on. In order to define higher differences, we need a variation in notation that we will use often in our discussion of ARIMA models in Chap. 3.

**Definition 2.4**: _We define the_ **backshift operator** _by_

\[Bx_{t}=x_{t-1}\]

_and extend it to powers \(B^{2}x_{t}=B(Bx_{t})=Bx_{t-1}=x_{t-2}\), and so on. Thus,_

\[B^{k}x_{t}=x_{t-k}. \tag{2.29}\]

The idea of an inverse operator can also be given if we require \(B^{-1}B=1\), so that

\[x_{t}=B^{-1}Bx_{t}=B^{-1}x_{t-1}.\]

That is, \(B^{-1}\) is the _forward-shift operator_. In addition, it is clear that we may rewrite (2.28) as \[\nabla x_{t}=(1-B)x_{t}, \tag{30}\]

and we may extend the notion further. For example, the second difference becomes

\[\nabla^{2}x_{t}=(1-B)^{2}x_{t}=(1-2B+B^{2})x_{t}=x_{t}-2x_{t-1}+x_{t-2} \tag{31}\]

by the linearity of the operator. To check, just take the difference of the first difference \(\nabla(\nabla x_{t})=\nabla(x_{t}-x_{t-1})=(x_{t}-x_{t-1})-(x_{t-1}-x_{t-2})\).

**Definition 5** **Differences of order \(d\) _are defined as_

\[\nabla^{d}=(1-B)^{d}, \tag{32}\]

_where we may expand the operator \((1-B)^{d}\) algebraically to evaluate for higher integer values of \(d\). When \(d=1\), we drop it from the notation._

The first difference (28) is an example of a _linear filter_ applied to eliminate a trend. Other filters, formed by averaging values near \(x_{t}\), can produce adjusted series that eliminate other kinds of unwanted fluctuations, as in Chap. 4. The differencing technique is an important component of the ARIMA model of Box and Jenkins [30] (see also Box et al. [31]), to be discussed in Chap. 3.

Figure 5: Sample ACFs of chicken prices (_top_), and of the detrended (_middle_) and the differenced (_bottom_) series. Compare the top plot with the sample ACF of a _straight line_: acf(1:100)

### Differencing Chicken Prices

The first difference of the chicken prices series, also shown in Fig. 2.4, produces different results than removing trend by detrending via regression. For example, the differenced series does not contain the long (five-year) cycle we observe in the detrended series. The ACF of this series is also shown in Fig. 2.5. In this case, the differenced series exhibits an annual cycle that was obscured in the original or detrended data.

The R code to reproduce Figs. 2.4 and 2.5 is as follows.

fit = lm(chicken-time(chicken), na.action-NULL) # regress chicken on time par(mfrow=c(2,1)) plot(resid(fit), type="o", main-"detrended") plot(diff(chicken), type="o", main="first difference") par(mfrow=c(3,1)) # plot ACFs acf(chicken, 48, main="chicken") acf(resid(fit), 48, main="detrended") acf(diff(chicken), 48, main="first difference")

### Differencing Global Temperature

The global temperature series shown in Fig. 1.2 appears to behave more as a random walk than a trend stationary series. Hence, rather than detrend the data, it would be more appropriate to use differencing to coerce it into stationarity. The detrended data are shown in Fig. 2.6 along with the corresponding sample ACF. In this case it appears that the differenced process shows minimal autocorrelation, which may imply the global temperature series is nearly a random walk with drift. It is interesting to note that if the series is a random walk with drift, the mean of the differenced series, which is an estimate of the drift, is about.008, or an increase of about one degree centigrade per 100 years.

The R code to reproduce Figs. 2.4 and 2.5 is as follows.

par(mfrow=c(2,1)) plot(diff(globtemp), type="o") mean(diff(globtemp)) # drift estimate =.008 acf(diff(gtemp), 48)

An alternative to differencing is a less-severe operation that still assumes stationarity of the underlying time series. This alternative, called _fractional differencing_, extends the notion of the difference operator (2.32) to fractional powers \(-.5<d<.5\), which still define stationary processes. Granger and Joyeux [79] and Hosking [97] introduced long memory time series, which corresponds to the case when \(0<d<.5\). This model is often used for environmental time series arising in hydrology. We will discuss long memory processes in more detail in Sect. 5.1. Often, obvious aberrations are present that can contribute nonstationary as well as nonlinear behavior in observed time series. In such cases, _transformations_ may be useful to equalize the variability over the length of a single series. A particularly useful transformation is

\[y_{t}=\log x_{t}, \tag{2.33}\]

which tends to suppress larger fluctuations that occur over portions of the series where the underlying values are larger. Other possibilities are _power transformations_ in theBox-Cox family of the form

\[y_{t}=\begin{cases}(x_{t}^{\lambda}-1)/\lambda&\lambda\neq 0,\\ \log x_{t}&\lambda=0.\end{cases} \tag{34}\]

Methods for choosing the power \(\lambda\)are available (see Johnson and Wichern [106, SS4.7]) but we do not pursue them here. Often, transformations are also used to improve the approximation to normality or to improve linearity in predicting the value of one series from another.

**Example 7**: **Paleoclimatic Glacial Varves**

Melting glaciers deposit yearly layers of sand and silt during the spring melting seasons, which can be reconstructed yearly over a period ranging from the time deglaciation began in New England (about 12,600 years ago) to the time it ended (about 6,000 years ago). Such sedimentary deposits, called _varves_, can be used as proxies for paleoclimatic parameters, such as temperature, because, in a warm year, more sand and silt are deposited from the receding glacier. Figure 7 shows the thicknesses of the yearly curves collected from one location in Massachusetts for 634 years, beginning 11,834 years ago. For further information, see Shumway and Verosub [185]. Because the variation in thicknesses increases in proportion to the amount deposited, a logarithmic transformation could remove the nonstationarity observable in the variance as a function of time. Figure 7 shows the original and transformed curves, and it is clear that this improvement has occurred. We may also plot the histogram of the original and transformed data, as in Problem 2.8, to argue that the approximation to normality is improved. The ordinary first differences (30) are also computed in Problem 2.8, and we note that the first differences have a

Figure 6: Differenced global temperature series and its sample ACF

significant negative correlation at lag \(h=1\). Later, in Chap. 5, we will show that perhaps the curve series has long memory and will propose using fractional differencing. Figure 7 was generated in R as follows:

```
par(mfrow=c(2,1)) plot(varve,main="varve",ylab="") plot(log(varve),main="log(varve)",ylab="")
```

Next, we consider another preliminary data processing technique that is used for the purpose of visualizing the relations between series at different lags, namely, _scatterplot matrices_. In the definition of the ACF, we are essentially interested in relations between \(x_{t}\) and \(x_{t-h}\); the autocorrelation function tells us whether a substantial linear relation exists between the series and its own lagged values. The ACF gives a profile of the linear correlation at all possible lags and shows which values of \(h\) lead to the best predictability. The restriction of this idea to linear predictability, however, may mask a possible nonlinear relation between current values, \(x_{t}\), and past values, \(x_{t-h}\). This idea extends to two series where one may be interested in examining scatterplots of \(y_{t}\) versus \(x_{t-h}\)

**Example 8**: Scatterplot Matrices, SOI and Recruitment

To check for nonlinear relations of this form, it is convenient to display a lagged scatterplot matrix, as in Fig. 8, that displays values of the SOI, \(S_{t}\), on the vertical axis plotted against \(S_{t-h}\) on the horizontal axis. The sample autocorrelations are displayed in the upper right-hand corner and superimposed on the scatterplots are locally weighted scatterplot smoothing (lowess) lines that can be used to help

Figure 7: Glacial curve thicknesses (_top_) from Massachusetts for \(n=634\) years compared with log transformed thicknesses (_bottom_)

discover any nonlinearities. We discuss smoothing in the next section, but for now, think of lowess as a robust method for fitting local regression.

In Fig. 8, we notice that the lowess fits are approximately linear, so that the sample autocorrelations are meaningful. Also, we see strong positive linear relations at lags \(h=1,2,11,12\), that is, between \(S_{t}\) and \(S_{t-1}\), \(S_{t-2}\), \(S_{t-11}\), \(S_{t-12}\), and a negative linear relation at lags \(h=6,7\). These results match up well with peaks noticed in the ACF in Fig. 16.

Similarly, we might want to look at values of one series, say Recruitment, denoted \(R_{t}\) plotted against another series at various lags, say the SOI, \(S_{t-h}\), to look for possible nonlinear relations between the two series. Because, for example, we might wish to predict the Recruitment series, \(R_{t}\), from current or past values of the SOI series, \(S_{t-h}\), for \(h=0,1,2,\dots\) it would be worthwhile to examine the scatterplot matrix. Figure 9 shows the lagged scatterplot of the Recruitment series

Figure 8: Scatterplot matrix relating current SOI values, \(S_{t}\), to past SOI values, \(S_{t-h}\), at lags \(h=1,2,\dots,12\). The values in the upper right corner are the sample autocorrelations and the lines are a lowess fit

\(R_{t}\) on the vertical axis plotted against the SOI index \(S_{t-h}\) on the horizontal axis. In addition, the figure exhibits the sample cross-correlations as well as lowess fits.

Figure 9 shows a fairly strong nonlinear relationship between Recruitment, \(R_{t}\), and the SOI series at \(S_{t-5}\), \(S_{t-6}\), \(S_{t-7}\), \(S_{t-8}\), indicating the SOI series tends to lead the Recruitment series and the coefficients are negative, implying that increases in the SOI lead to decreases in the Recruitment. The nonlinearity observed in the scatterplots (with the help of the superimposed lowess fits) indicates that the behavior between Recruitment and the SOI is different for positive values of SOI than for negative values of SOI.

Simple scatterplot matrices for one series can be obtained in R using the lag.plot command. Figures 8 and 9 may be reproduced using the following scripts provided with astsa:

lag1.plot(soi, 12) # Fig. 2.8 lag2.plot(soi, rec, 8) # Fig. 2.9

Figure 9: Scatterplot matrix of the Recruitment series, \(R_{t}\), on the vertical axis plotted against the SOI series, \(S_{t-h}\), on the horizontal axis at lags \(h=0\), \(1,\ldots,8\). The values in the upper right corner are the sample cross-correlations and the lines are a lowess fit

**Example 2.9**: **Regression with Lagged Variables (cont)**

In Example 2.3 we regressed Recruitment on lagged SOI,

\[R_{t}=\beta_{0}+\beta_{1}S_{t-6}+w_{t}\,.\]

However, in Example 2.8, we saw that the relationship is nonlinear and different when SOI is positive or negative. In this case, we may consider adding a dummy variable to account for this change. In particular, we fit the model

\[R_{t}=\beta_{0}+\beta_{1}S_{t-6}+\beta_{2}D_{t-6}+\beta_{3}D_{t-6}\,S_{t-6}+w_{ t}\,,\]

where \(D_{t}\) is a dummy variable that is 0 if \(S_{t}<0\) and 1 otherwise. This means that

\[R_{t}=\begin{cases}\beta_{0}+\beta_{1}S_{t-6}+w_{t}&\text{if }\ S_{t-6}<0\,,\\ (\beta_{0}+\beta_{2})+(\beta_{1}+\beta_{3})S_{t-6}+w_{t}&\text{if }\ S_{t-6}\geq 0 \,.\end{cases}\]

The result of the fit is given in the R code below. Figure 2.10 shows \(R_{t}\) vs \(S_{t-6}\) with the fitted values of the regression and a lowess fit superimposed. The piecewise regression fit is similar to the lowess fit, but we note that the residuals are not white noise (see the code below). This is followed up in Example 3.45.

 dummy = ifelse(soi<0, 0, 1)

 fish = ts.intersect(rec, soil6=lag(soi,-6), dL6-lag(dummy,-6), dframe=TRUE)

 summary(fit <- lm(rec~ soil6*dL6, data=fish, na.action=NULL))

 Coefficients:  Estimate Std.Error t.value

 (Intercept) 74.479 2.865 25.998

 soil6 -15.358 7.401 -2.075

 dL6 -1.139 3.711 -0.307

 soil6:dL6 -51.244 9.523 -5.381

 ---

 Residual standard error: 21.84 on 443 degrees of freedom

 Multiple R-squared: 0.4024

 F-statistic: 99.43 on 3 and 443 DF

 attach(fish)

 plot(soiL6, rec)

 lines(lowess(soil6, rec), col=4, lwd=2)

 points(soil6, fitted(fit), pch='+', col=2)

 plot(resid(fit)) # not shown...

 acf(resid(fit)) #... but obviously not noise

Figure 2.10: Display for Example 2.9: Plot of Recruitment (\(R_{t}\)) vs SOI lagged 6 months (\(S_{t-6}\)) with the fitted values of the regression as points (+) and a lowess fit (—)As a final exploratory tool, we discuss assessing periodic behavior in time series data using regression analysis. In Example 12, we briefly discussed the problem of identifying cyclic or periodic signals in time series. A number of the time series we have seen so far exhibit periodic behavior. For example, the data from the pollution study example shown in Fig. 2 exhibit strong yearly cycles. The Johnson & Johnson data shown in Fig. 1 make one cycle every year (four quarters) on top of an increasing trend and the speech data in Fig. 2 is highly repetitive. The monthly SOI and Recruitment series in Fig. 6 show strong yearly cycles, which obscures the slower El Nino cycle.

**Example 10**: **Using Regression to Discover a Signal in Noise**

In Example 12, we generated \(n=500\) observations from the model

\[x_{t}=A\cos(2\pi\omega t+\phi)+w_{t}, \tag{35}\]

where \(\omega=1/50\), \(A=2\), \(\phi=.6\pi\), and \(\sigma_{w}=5\); the data are shown on the bottom panel of Fig. 11. At this point we assume the frequency of oscillation \(\omega=1/50\) is known, but \(A\) and \(\phi\) are unknown parameters. In this case the parameters appear in (35) in a nonlinear way, so we use a trigonometric identity4 and write

Footnote 4: \(\cos(\alpha\pm\beta)=\cos(\alpha)\cos(\beta)\mp\sin(\alpha)\sin(\beta)\).

\[A\cos(2\pi\omega t+\phi)=\beta_{1}\cos(2\pi\omega t)+\beta_{2}\sin(2\pi\omega t),\]

where \(\beta_{1}=A\cos(\phi)\) and \(\beta_{2}=-A\sin(\phi)\). Now the model (35) can be written in the usual linear regression form given by (no intercept term is needed here)

\[x_{t}=\beta_{1}\cos(2\pi t/50)+\beta_{2}\sin(2\pi t/50)+w_{t}. \tag{36}\]

Using linear regression, we find \(\hat{\beta}_{1}=-.74_{(.33)}\), \(\hat{\beta}_{2}=-1.99_{(.33)}\) with \(\hat{\sigma}_{w}=5.18\); the values in parentheses are the standard errors. We note the actual values of the coefficients for this example are \(\beta_{1}=2\cos(.6\pi)=-.62\), and \(\beta_{2}=-2\sin(.6\pi)=-1.90\). It is clear that we are able to detect the signal in the noise using regression, even though the signal-to-noise ratio is small. Figure 11 shows data generated by (35) with the fitted line superimposed.

To reproduce the analysis and Fig. 11 in R, use the following:

set.seed(90210) # so you can reproduce these results x = 2*cos(2*pi*1:500/50 +.6*pi) + rnorm(500,0,5) z1 = cos(2*pi*1:500/50) z2 = sin(2*pi*1:500/50) summary(fit <- lm(x-0+z1+z2)) # zero to exclude the intercept Coefficients: Estimate Std. Error t value z1 -0.7442 0.3274 -2.273 z2 -1.9949 0.3274 -6.093 Residual standard error: 5.177 on 498 degrees of freedom par(mfrow=c(2,1)) plot.ts(x) plot.ts(x, col=8, ylab=expression(hat(x))) lines(fitted(fit), col=2) We will discuss this and related approaches in more detail in Chap. 4.

### Smoothing in the Time Series Context

In Sect. 1.2, we introduced the concept of filtering or smoothing a time series, and in Example 1.9, we discussed using a moving average to smooth white noise. This method is useful in discovering certain traits in a time series, such as long-term trend and seasonal components. In particular, if \(x_{t}\) represents the observations, then

\[m_{t}=\sum_{j=-k}^{k}a_{j}x_{t-j}, \tag{2.37}\]

where \(a_{j}=a_{-j}\geq 0\) and \(\sum_{j=-k}^{k}a_{j}=1\) is a symmetric moving average of the data.

**Example 2.11**: **Moving Average Smoother**

For example, Fig. 2.12 shows the monthly SOI series discussed in Example 1.5 smoothed using (2.37) with weights \(a_{0}=a_{\pm 1}=\cdots=a_{\pm 5}=1/12\), and \(a_{\pm 6}=1/24\); \(k=6\). This particular method removes (filters out) the obvious annual temperature cycle and helps emphasize the El Nino cycle. To reproduce Fig. 2.12 in R:

```
wgts=c(.5,rep(1,11),.5)/12soif=filter(soi,sides=2,filter=wgts)plot(soi)lines(soif,lwd=2,col=4)par(fig=c(.65,1,.65,1),new=TRUE)#theinsertnwgts=c(rep(@,20),wgts,rep(0,20))plot(mwgts,type="l",ylim=c(-.02,.1),xart='n',yart='n',ann=FALSE)
```

Although the moving average smoother does a good job in highlighting the El Nino effect, it might be considered too choppy. We can obtain a smoother fit using the normal distribution for the weights, instead of boxcar-type weights of (2.37).

Figure 2.11: Data generated by (2.35) [_top_] and the fitted line superimposed on the data [_bottom_]

**Example 2.12**: **Kernel Smoothing**

Kernel smoothing is a moving average smoother that uses a weight function, or kernel, to average the observations. Figure 2.13 shows kernel smoothing of the SOI series, where \(m_{t}\) is now

\[m_{t}=\sum_{i=1}^{n}w_{i}(t)x_{i}, \tag{2.38}\]

where

\[w_{i}(t)=K\left(\frac{t-i}{b}\right)\Bigm{/}\sum_{j=1}^{n}K\left(\frac{t-j}{b}\right) \tag{2.39}\]

are the weights and \(K(\cdot)\) is a kernel function. This estimator, which was originally explored by Parzen [148] and Rosenblatt [170], is often called the Nadaraya-Watson estimator (Watson [207]). In this example, and typically, the normal kernel, \(K(z)=\frac{1}{\sqrt{2\pi}}\exp(-z^{2}/2)\), is used.

Figure 2.12: Moving average smoother of SOI. The insert shows the shape of the moving average (“boxcar”) kernel [not drawn to scale] described in (2.39)

Figure 2.13: Kernel smoother of SOI. The insert shows the shape of the normal kernel [not drawn to scale]To implement this in R, use the ksmooth function where a bandwidth can be chosen. The wider the bandwidth, \(b\), the smoother the result. From the R ksmooth help file: The kernels are scaled so that their quartiles (viewed as probability densities) are at \(\pm\) 0.25*bandwidth. For the standard normal distribution, the quartiles are \(\pm\).674. In our case, we are smoothing over time, which is of the form \(t/12\) for the SOI time series. In Fig. 13, we used the value of \(b=1\) to correspond to approximately smoothing a little over one year. Figure 13 can be reproduced in R as follows.

plot(soi) lines(ksmooth(time(soi), soi, "normal", bandwidth=1), lwd=2, col=4) par(fig = c(.65, 1,.65, 1), new = TRUE) # the insert gauss = function(x) { 1/sqrt(2*pi) * exp(-(x^2)/2) } x = seq(from = -3, to = 3, by = 0.001) plot(x, gauss(x), type ="l", ylim=c(-.02,.45), xat='n', yat='n', ann=FALSE)

**Example 2.13**: **Lowess**

Another approach to smoothing a time plot is nearest neighbor regression. The technique is based on \(k\)-nearest neighbors regression, wherein one uses only the data \(\{x_{t-k/2},\ldots,x_{t},\ldots,x_{t+k/2}\}\) to predict \(x_{t}\) via regression, and then sets \(m_{t}=\hat{x}_{t}\).

Lowess is a method of smoothing that is rather complex, but the basic idea is close to nearest neighbor regression. Figure 14 shows smoothing of SOI using the R function lowess (see Cleveland [42]). First, a certain proportion of nearest neighbors to \(x_{t}\) are included in a weighting scheme; values closer to \(x_{t}\) in time get more weight. Then, a robust weighted regression is used to predict \(x_{t}\) and obtain the smoothed values \(m_{t}\). The larger the fraction of nearest neighbors included, the smoother the fit will be. In Fig. 14, one smoother uses 5% of the data to obtain an estimate of the EI Nino cycle of the data.

In addition, a (negative) trend in SOI would indicate the long-term warming of the Pacific Ocean. To investigate this, we used lowess with the default smoother span of f=2/3 of the data. Figure 14 can be reproduced in R as follows.

plot(soi) lines(lowess(soi, f=.05), lwd=2, col=4) # EI Nino cycle lines(lowess(soi), lty=2, lwd=2, col=2) # trend (with default span)

Figure 14: Locally weighted scatterplot smoothers (lowess) of the SOI series

**Example 2.14**: **Smoothing Splines**

An obvious way to smooth data would be to fit a polynomial regression in terms of time. For example, a cubic polynomial would have \(x_{t}=m_{t}+w_{t}\) where

\[m_{t}=\beta_{0}+\beta_{1}t+\beta_{2}t^{2}+\beta_{3}t^{3}.\]

We could then fit \(m_{t}\) via ordinary least squares.

An extension of polynomial regression is to first divide time \(t=1,\ldots,n\), into \(k\) intervals, \([t_{0}=1,t_{1}]\), \([t_{1}+1,t_{2}]\),..., \([t_{k-1}+1,t_{k}=n]\); the values \(t_{0},t_{1},\ldots,t_{k}\) are called _knots_. Then, in each interval, one fits a polynomial regression, typically the order is 3, and this is called _cubic splines_.

A related method is _smoothing splines_, which minimizes a compromise between the fit and the degree of smoothness given by

\[\sum_{t=1}^{n}\left[x_{t}-m_{t}\right]^{2}+\lambda\int\left(m_{t}^{\prime \prime}\right)^{2}dt, \tag{2.40}\]

where \(m_{t}\) is a cubic spline with a knot at each \(t\) and primes denote differentiation. The degree of smoothness is controlled by \(\lambda>0\).

Think of taking a long drive where \(m_{t}\) is the position of your car at time \(t\). In this case, \(m_{t}^{\prime\prime}\) is instantaneous acceleration/deceleration, and \(\int(m_{t}^{\prime\prime})^{2}dt\) is a measure of the total amount of acceleration and deceleration on your trip. A smooth drive would be one where a constant velocity, is maintained (i.e., \(m_{t}^{\prime\prime}=0\)). A choppy ride would be when the driver is constantly accelerating and decelerating, such as beginning drivers tend to do.

If \(\lambda=0\), we don't care how choppy the ride is, and this leads to \(m_{t}=x_{t}\), the data, which are not smooth. If \(\lambda=\infty\), we insist on no acceleration or deceleration (\(m_{t}^{\prime\prime}=0\)); in this case, our drive must be at constant velocity, \(m_{t}=c+vt\), and

Figure 2.15: Smoothing splines fit to the SOI seriesconsequently very smooth. Thus, \(\lambda\) is seen as a trade-off between linear regression (completely smooth) and the data itself (no smoothness). The larger the value of \(\lambda\), the smoother the fit.

In R, the smoothing parameter is called spar and it is monotonically related to \(\lambda\); type?smooth.spline to view the help file for details. Figure 15 shows smoothing spline fits on the SOI series using spar=.5 to emphasize the El Nino cycle, and spar=1 to emphasize the trend. The figure can be reproduced in R as follows.

plot(soi) lines(smooth.spline(time(soi), soi, spar=.5), lwd=2, col=4) lines(smooth.spline(time(soi), soi, spar= 1), lty=2, lwd=2, col=2)

**Example 2.15**: **Smoothing One Series as a Function of Another**

In addition to smoothing time plots, smoothing techniques can be applied to smoothing a time series as a function of another time series. We have already seen this idea used in Example 2.8 when we used lowess to visualize the nonlinear relationship between Recruitment and SOI at various lags. In this example, we smooth the scatterplot of two contemporaneously measured time series, mortality as a function of temperature. In Example 2.2, we discovered a nonlinear relationship between mortality and temperature. Continuing along these lines, Fig. 16 show a scatterplot of mortality, \(M_{t}\), and temperature, \(T_{t}\), along with \(M_{t}\) smoothed as a function of \(T_{t}\) using lowess. Note that mortality increases at extreme temperatures, but in an asymmetric way; mortality is higher at colder temperatures than at hotter temperatures. The minimum mortality rate seems to occur at approximately \(83^{\circ}\) F.

Figure 16 can be reproduced in R as follows using the defaults.

plot(tempr, cmort, xlab="Temperature", ylab="Mortality") lines(lowess(tempr, cmort))

Figure 16: Smooth of mortality as a function of temperature using lowess

## Problems

### _Sect. 2.1_

#### A Structural Model

For the Johnson & Johnson data, say \(y_{t}\), shown in Fig. 1.1, let \(x_{t}=\log(y_{t})\). In this problem, we are going to fit a special type of structural model, \(x_{t}=T_{t}+S_{t}+N_{t}\) where \(T_{t}\) is a trend component, \(S_{t}\) is a seasonal component, and \(N_{t}\) is noise. In our case, time \(t\) is in quarters (\(1960.00,1960.25,\dots\)) so one unit of time is a year.

(a) Fit the regression model

\[x_{t}=\underbrace{\beta t}_{\text{trend}}+\underbrace{\alpha_{1}Q_{1}(t)+ \alpha_{2}Q_{2}(t)+\alpha_{3}Q_{3}(t)+\alpha_{4}Q_{4}(t)}_{\text{seasonal}}+ \underbrace{w_{t}}_{\text{noise}}\]

where \(Q_{i}(t)=1\) if time \(t\) corresponds to quarter \(i=1,2,3,4\), and zero otherwise. The \(Q_{i}(t)\)'s are called indicator variables. We will assume for now that \(w_{t}\) is a Gaussian white noise sequence. _Hint:_ Detailed code is given in Code R.4, the last example of Sect. R.4.

(b) If the model is correct, what is the estimated average annual increase in the logged earnings per share?

(c) If the model is correct, does the average logged earnings rate increase or decrease from the third quarter to the fourth quarter? And, by what percentage does it increase or decrease?

(d) What happens if you include an intercept term in the model in (a)? Explain why there was a problem.

(e) Graph the data, \(x_{t}\), and superimpose the fitted values, say \(\hat{x}_{t}\), on the graph. Examine the residuals, \(x_{t}-\hat{x}_{t}\), and state your conclusions. Does it appear that the model fits the data well (do the residuals look white)?

#### For the mortality data examined in Example 2.2:

(a) Add another component to the regression in (2.21) that accounts for the particulate count four weeks prior; that is, add \(P_{t-4}\) to the regression in (2.21). State your conclusion.

(b) Draw a scatterplot matrix of \(M_{t},T_{t},P_{t}\) and \(P_{t-4}\) and then calculate the pairwise correlations between the series. Compare the relationship between \(M_{t}\) and \(P_{t}\) versus \(M_{t}\) and \(P_{t-4}\).

#### In this problem, we explore the difference between a random walk and a trend stationary process.

(a) Generate _four_ series that are random walk with drift, (1.4), of length \(n=100\) with \(\delta=.01\) and \(\sigma_{w}=1\). Call the data \(x_{t}\) for \(t=1,\dots,100\). Fit the regression \(x_{t}=\beta t+w_{t}\) using least squares. Plot the data, the true mean function (i.e., \(\mu_{t}=.01\,t\)) and the fitted line, \(\hat{x}_{t}=\beta\,t\), on the same graph. _Hint:_ The following R code may be useful.

[MISSING_PAGE_FAIL:83]

of the above discrimination information is as claimed. As models with differing dimensions \(k\) are considered, only the second and third terms in (2.43) will vary and we only need unbiased estimators for those two terms. This gives the form of AICc quoted in (2.16) in the chapter. You will need the two distributional results

\[\frac{n\hat{\sigma}^{2}}{\sigma_{1}^{2}}\sim\chi_{n-k}^{2}\quad\text{and}\quad \frac{(\hat{\beta}-\beta_{1})^{\prime}Z^{\prime}Z(\hat{\beta}-\beta_{1})}{\sigma _{1}^{2}}\sim\chi_{k}^{2}\]

The two quantities are distributed independently as chi-squared distributions with the indicated degrees of freedom. If \(x\sim\chi_{n}^{2}\), \(\mathrm{E}(1/x)=1/(n-2)\).

#### Sect. 2.2

**2.6** Consider a process consisting of a linear trend with an additive noise term consisting of independent random variables \(w_{t}\) with zero means and variances \(\sigma_{w}^{2}\), that is,

\[x_{t}=\beta_{0}+\beta_{1}t+w_{t},\]

where \(\beta_{0},\beta_{1}\) are fixed constants.

(a) Prove \(x_{t}\) is nonstationary.

(b) Prove that the first difference series \(\nabla x_{t}=x_{t}-x_{t-1}\) is stationary by finding its mean and autocovariance function.

(c) Repeat part (b) if \(w_{t}\) is replaced by a general stationary process, say \(y_{t}\), with mean function \(\mu_{y}\) and autocovariance function \(\gamma_{y}(h)\).

**2.7** Show (2.27) is stationary.

**2.8** The glacial curve record plotted in Fig. 2.7 exhibits some nonstationarity that can be improved by transforming to logarithms and some additional nonstationarity that can be corrected by differencing the logarithms.

(a) Argue that the glacial varves series, say \(x_{t}\), exhibits heteroscedasticity by computing the sample variance over the first half and the second half of the data. Argue that the transformation \(y_{t}=\log x_{t}\) stabilizes the variance over the series. Plot the histograms of \(x_{t}\) and \(y_{t}\) to see whether the approximation to normality is improved by transforming the data.

(b) Plot the series \(y_{t}\). Do any time intervals, of the order 100 years, exist where one can observe behavior comparable to that observed in the global temperature records in Fig. 1.2?

(c) Examine the sample ACF of \(y_{t}\) and comment.

(d) Compute the difference \(u_{t}=y_{t}-y_{t-1}\), examine its time plot and sample ACF, and argue that differencing the logged varve data produces a reasonably stationary series. Can you think of a practical interpretation for \(u_{t}\)? _Hint_: Recall Footnote 2.

(e) Based on the sample ACF of the differenced transformed series computed in (c), argue that a generalization of the model given by Example 1.26 might be reasonable. Assume \[u_{t}=\mu+w_{t}+\theta w_{t-1}\] is stationary when the inputs \(w_{t}\) are assumed independent with mean 0 and variance \(\sigma_{w}^{2}\). Show that \[\gamma_{u}(h)=\begin{cases}\sigma_{w}^{2}(1+\theta^{2})&\text{if }h=0,\\ \theta\,\sigma_{w}^{2}&\text{if }h=\pm 1,\\ 0&\text{if }|h|>1.\end{cases}\] (f) Based on part (e), use \(\hat{\rho}_{u}(1)\) and the estimate of the variance of \(u_{t}\), \(\hat{\gamma}_{u}(0)\), to derive estimates of \(\theta\) and \(\sigma_{w}^{2}\). This is an application of the method of moments from classical statistics, where estimators of the parameters are derived by equating sample moments to theoretical moments.

**2.9**: In this problem, we will explore the periodic nature of \(S_{t}\), the SOI series displayed in Fig. 1.5.

(a) Detrend the series by fitting a regression of \(S_{t}\) on time \(t\). Is there a significant trend in the sea surface temperature? Comment.

(b) Calculate the periodogram for the detrended series obtained in part (a). Identify the frequencies of the two main peaks (with an obvious one at the frequency of one cycle every 12 months). What is the probable El Nino cycle indicated by the minor peak?

#### Sect. 2.3

**2.10**: Consider the two weekly time series oil and gas. The oil series is in dollars per barrel, while the gas series is in cents per gallon.

(a) Plot the data on the same graph. Which of the simulated series displayed in Sect. 1.2 do these series most resemble? Do you believe the series are stationary (explain your answer)? (b) In economics, it is often the percentage change in price (termed _growth rate_ or _return_), rather than the absolute price change, that is important. Argue that a transformation of the form \(y_{t}=\nabla\log x_{t}\) might be applied to the data, where \(x_{t}\) is the oil or gas price series. _Hint_: Recall Footnote 2. (c) Transform the data as described in part (b), plot the data on the same graph, look at the sample ACFs of the transformed data, and comment. (d) Plot the CCF of the transformed data and comment The small, but significant values when gas leads oil might be considered as feedback. (e) Exhibit scatterplots of the oil and gas growth rate series for up to three weeks of lead time of oil prices; include a nonparametric smoother in each plot and comment on the results (e.g., Are there outliers? Are the relationships linear?).

(f) There have been a number of studies questioning whether gasoline prices respond more quickly when oil prices are rising than when oil prices are falling ("asymmetry"). We will attempt to explore this question here with simple lagged regression; we will ignore some obvious problems such as outliers and autocorrelated errors, so this will not be a definitive analysis. Let \(G_{t}\) and \(O_{t}\) denote the gas and oil growth rates.

(i) Fit the regression (and comment on the results)

\[G_{t}=\alpha_{1}+\alpha_{2}I_{t}+\beta_{1}O_{t}+\beta_{2}O_{t-1}+w_{t},\]

where \(I_{t}=1\) if \(O_{t}\geq 0\) and \(0\) otherwise (\(I_{t}\) is the indicator of no growth or positive growth in oil price). _Hint:_

poil = diff(log(oil))

pgas = diff(log(gas))

indi = ifelse(poil < 0, 0, 1)

mess = ts.intersect(pgas, poil, poill = lag(poil,-1), indi)

summary(fit <- lm(pgas- poil + poill + indi, data=mess))

(ii) What is the fitted model when there is negative growth in oil price at time \(t\)? What is the fitted model when there is no or positive growth in oil price?

Do these results support the asymmetry hypothesis?

(iii) Analyze the residuals from the fit and comment.

**2.11** Use two different smoothing techniques described in Sect. 2.3 to estimate the trend in the global temperature series globtemp. Comment.

## Chapter 3 ARIMA Models

Classical regression is often insufficient for explaining all of the interesting dynamics of a time series. For example, the ACF of the residuals of the simple linear regression fit to the price of chicken data (see Example 2.4) reveals additional structure in the data that regression did not capture. Instead, the introduction of correlation that may be generated through lagged linear relations leads to proposing the _autoregressive (AR)_ and _autoregressive moving average (ARMA)_ models that were presented in Whittle [209]. Adding nonstationary models to the mix leads to the _autoregressive integrated moving average (ARIMA)_ model popularized in the landmark work by Box and Jenkins [30]. The _Box-Jenkins method_ for identifying ARIMA models is given in this chapter along with techniques for _parameter estimation_ and _forecasting_ for these models. A partial theoretical justification of the use of ARMA models is discussed in Sect. B.4.

### 3.1 Autoregressive Moving Average Models

The classical regression model of Chap. 2 was developed for the static case, namely, we only allow the dependent variable to be influenced by current values of the independent variables. In the time series case, it is desirable to allow the dependent variable to be influenced by the past values of the independent variables and possibly by its own past values. If the present can be plausibly modeled in terms of only the past values of the independent inputs, we have the enticing prospect that forecasting will be possible.

#### Introduction to Autoregressive Models

Autoregressive models are based on the idea that the current value of the series, \(x_{t}\), can be explained as a function of \(p\) past values, \(x_{t-1},x_{t-2},\ldots,x_{t-p}\), where \(p\) determinesthe number of steps into the past needed to forecast the current value. As a typical case, recall Example 1.10 in which data were generated using the model

\[x_{t}=x_{t-1}-.90x_{t-2}+w_{t},\]

where \(w_{t}\) is white Gaussian noise with \(\sigma_{w}^{2}=1\). We have now assumed the current value is a particular _linear_ function of past values. The regularity that persists in Fig. 1.9 gives an indication that forecasting for such a model might be a distinct possibility, say, through some version such as

\[x_{n+1}^{n}=x_{n}-.90x_{n-1},\]

where the quantity on the left-hand side denotes the forecast at the next period \(n+1\) based on the observed data, \(x_{1},x_{2},\ldots,x_{n}\). We will make this notion more precise in our discussion of forecasting (Sect. 3.4).

The extent to which it might be possible to forecast a real data series from its own past values can be assessed by looking at the autocorrelation function and the lagged scatterplot matrices discussed in Chap. 2. For example, the lagged scatterplot matrix for the Southern Oscillation Index (SOI), shown in Fig. 2.8, gives a distinct indication that lags 1 and 2, for example, are linearly associated with the current value. The ACF shown in Fig. 1.16 shows relatively large positive values at lags 1, 2, 12, 24, and 36 and large negative values at 18, 30, and 42. We note also the possible relation between the SOI and Recruitment series indicated in the scatterplot matrix shown in Fig. 2.9. We will indicate in later sections on transfer function and vector AR modeling how to handle the dependence on values taken by other series.

The preceding discussion motivates the following definition.

**Definition 3.1**: _An_ **autoregressive model** _of order \(p\), abbreviated_ **AR(\(p\))**_, is of the form_

\[x_{t}=\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+\cdots+\phi_{p}x_{t-p}+w_{t}, \tag{3.1}\]

_where \(x_{t}\) is stationary, \(w_{t}\sim wn(0,\sigma_{w}^{2})\), and \(\phi_{1},\phi_{2},\ldots,\phi_{p}\) are constants (\(\phi_{p}\neq 0\)). The mean of \(x_{t}\) in (3.1) is zero. If the mean, \(\mu\), of \(x_{t}\) is not zero, replace \(x_{t}\) by \(x_{t}-\mu\) in (3.1),_

\[x_{t}-\mu=\phi_{1}(x_{t-1}-\mu)+\phi_{2}(x_{t-2}-\mu)+\cdots+\phi_{p}(x_{t-p}- \mu)+w_{t},\]

_or write_

\[x_{t}=\alpha+\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+\cdots+\phi_{p}x_{t-p}+w_{t}, \tag{3.2}\]

_where \(\alpha=\mu(1-\phi_{1}-\cdots-\phi_{p})\)._

We note that (3.2) is similar to the regression model of Sect. 2.1, and hence the term auto (or self) regression. Some technical difficulties, however, develop from applying that model because the regressors, \(x_{t-1},\ldots,x_{t-p}\), are random components, whereas \(z_{t}\) was assumed to be fixed. A useful form follows by using the backshift operator (2.29) to write the AR(\(p\)) model, (3.1), as

\[(1-\phi_{1}B-\phi_{2}B^{2}-\cdots-\phi_{p}B^{p})x_{t}=w_{t}, \tag{3.3}\]

or even more concisely as

\[\phi(B)x_{t}=w_{t}. \tag{3.4}\]

The properties of \(\phi(B)\) are important in solving (3.4) for \(x_{t}\). This leads to the following definition.

**Definition 3.2**: _The_ **autoregressive operator** _is defined to be_

\[\phi(B)=1-\phi_{1}B-\phi_{2}B^{2}-\cdots-\phi_{p}B^{p}. \tag{3.5}\]

**Example 3.1**: _The_ **AR(1) Model**__

We initiate the investigation of AR models by considering the first-order model, AR(1), given by \(x_{t}=\phi x_{t-1}+w_{t}\). Iterating backwards \(k\) times, we get

\[\begin{array}{l}x_{t}=\phi x_{t-1}+w_{t}=\phi(\phi x_{t-2}+w_{t-1})+w_{t}\\ =\phi^{2}x_{t-2}+\phi w_{t-1}+w_{t}\\ \vdots\\ =\phi^{k}x_{t-k}+\sum_{j=0}^{k-1}\phi^{j}w_{t-j}.\end{array}\]

This method suggests that, by continuing to iterate backward, and provided that \(|\phi|<1\) and \(\sup_{t}\text{var}(x_{t})<\infty\), we can represent an AR(1) model as a linear process given by1

Footnote 1: Note that \(\lim_{k\to\infty}\text{E}\left(x_{t}-\sum_{j=0}^{k-1}\phi^{j}w_{t-j}\right)^{ 2}=\lim_{k\to\infty}\phi^{2k}\text{E}\left(x_{t-k}^{2}\right)=0\), so (3.6) exists in the mean square sense (see Appendix A for a definition).

\[x_{t}=\sum_{j=0}^{\infty}\phi^{j}w_{t-j}. \tag{3.6}\]

Representation (3.6) is called the stationary solution of the model. In fact, by simple substitution,

\[\underbrace{\sum_{j=0}^{\infty}\phi^{j}w_{t-j}}_{x_{t}}=\phi\underbrace{\left( \sum_{k=0}^{\infty}\phi^{k}w_{t-1-k}\right)}_{x_{t-1}}+w_{t}.\]

The AR(1) process defined by (3.6) is stationary with mean

\[\text{E}(x_{t})=\sum_{j=0}^{\infty}\phi^{j}\text{E}(w_{t-j})=0,\]

and autocovariance function,

\[\begin{array}{l}\gamma(h)=\text{cov}(x_{t+h},x_{t})=\text{E}\left[\left(\sum _{j=0}^{\infty}\phi^{j}w_{t+h-j}\right)\left(\sum_{k=0}^{\infty}\phi^{k}w_{t- k}\right)\right]\\ \qquad\qquad=\text{E}\left[\left(w_{t+h}+\cdots+\phi^{h}w_{t}+\phi^{h+1}w_{t-1} +\cdots\right)\left(w_{t}+\phi w_{t-1}+\cdots\right)\right]\\ \qquad\qquad=\sigma_{w}^{2}\sum_{j=0}^{\infty}\phi^{h+j}\phi^{j}=\sigma_{w}^{2 }\phi^{h}\sum_{j=0}^{\infty}\phi^{2j}=\frac{\sigma_{w}^{2}\phi^{h}}{1-\phi^{2 }},\quad h\geq 0.\end{array} \tag{3.7}\]Recall that \(\gamma(h)=\gamma(-h)\), so we will only exhibit the autocovariance function for \(h\geq 0\). From (3.7), the ACF of an AR(1) is

\[\rho(h)=\frac{\gamma(h)}{\gamma(0)}=\phi^{h},\quad h\geq 0, \tag{3.8}\]

and \(\rho(h)\) satisfies the recursion

\[\rho(h)=\phi\,\rho(h-1),\quad h=1,2,\ldots. \tag{3.9}\]

We will discuss the ACF of a general AR(\(p\)) model in Sect. 3.3.

**Example 3.2**: **The Sample Path of an AR(1) Process**

Figure 3.1 shows a time plot of two AR(1) processes, one with \(\phi=.9\) and one with \(\phi=-.9\); in both cases, \(\sigma_{w}^{2}=1\). In the first case, \(\rho(h)=.9^{h}\), for \(h\geq 0\), so observations close together in time are positively correlated with each other. This result means that observations at contiguous time points will tend to be close in value to each other; this fact shows up in the top of Fig. 3.1 as a very smooth sample path for \(x_{t}\). Now, contrast this with the case in which \(\phi=-.9\), so that \(\rho(h)=(-.9)^{h}\), for \(h\geq 0\). This result means that observations at contiguous time points are negatively correlated but observations two time points apart are positively correlated. This fact shows up in the bottom of Fig. 3.1, where, for example, if an observation, \(x_{t}\), is positive, the next observation, \(x_{t+1}\), is typically negative, and the next observation, \(x_{t+2}\), is typically positive. Thus, in this case, the sample path is very choppy.

The following R code can be used to obtain a figure similar to Fig. 3.1:

```
par(mfrown=c(2,1)) plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",  main=(expression(AR(1)--phi=+.9))) plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",  main=(expression(AR(1)--phi=-.9)))
```

**Example 3.3**: **Explosive AR Models and Causality**

In Example 1.18, it was discovered that the random walk \(x_{t}=x_{t-1}+w_{t}\) is not stationary. We might wonder whether there is a stationary AR(1) process with \(|\phi|>1\). Such processes are called explosive because the values of the time series quickly become large in magnitude. Clearly, because \(|\phi|^{j}\) increases without bound as \(j\to\infty\), \(\sum_{j=0}^{k-1}\phi^{j}w_{t-j}\) will not converge (in mean square) as \(k\to\infty\), so the intuition used to get (3.6) will not work directly. We can, however, modify that argument to obtain a stationary model as follows. Write \(x_{t+1}=\phi x_{t}+w_{t+1}\), in which case,

\[x_{t} = \phi^{-1}x_{t+1}-\phi^{-1}w_{t+1}=\phi^{-1}\left(\phi^{-1}x_{t+2} -\phi^{-1}w_{t+2}\right)-\phi^{-1}w_{t+1} \tag{3.10}\] \[\vdots\] \[= \phi^{-k}x_{t+k}-\sum_{j=1}^{k-1}\phi^{-j}w_{t+j},\]by iterating forward \(k\) steps. Because \(|\phi|^{-1}<1\), this result suggests the stationary future dependent AR(1) model

\[x_{t}=-\sum_{j=1}^{\infty}\phi^{-j}w_{t+j}. \tag{3.11}\]

The reader can verify that this is stationary and of the AR(1) form \(x_{t}=\phi x_{t-1}+w_{t}\). Unfortunately, this model is useless because it requires us to know the future to be able to predict the future. When a process does not depend on the future, such as the AR(1) when \(|\phi|<1\), we will say the process is _causal_. In the explosive case of this example, the process is stationary, but it is also future dependent, and not causal.

**Example 3.4**: **Every Explosion Has a Cause**

Excluding explosive models from consideration is not a problem because the models have causal counterparts. For example, if

\[x_{t}=\phi x_{t-1}+w_{t}\quad\mbox{with}\quad|\phi|>1\]

and \(w_{t}\sim\mbox{iid N}(0,\sigma_{w}^{2})\), then using (3.11), \(\{x_{t}\}\) is a non-causal stationary Gaussian process with \(\mbox{E}(x_{t})=0\) and

\[\gamma_{x}(h) = \mbox{cov}(x_{t+h},x_{t})=\mbox{cov}\left(-\sum_{j=1}^{\infty} \phi^{-j}w_{t+h+j},-\sum_{k=1}^{\infty}\phi^{-k}w_{t+k}\right)\] \[= \sigma_{w}^{2}\phi^{-2}\,\phi^{-h}/(1-\phi^{-2}).\]Thus, using (3.7), the causal process defined by

\[y_{t}=\phi^{-1}y_{t-1}+v_{t}\]

where \(v_{t}\sim\text{iid N}(0,\sigma_{w}^{2}\phi^{-2})\) is stochastically equal to the \(x_{t}\) process (i.e., all finite distributions of the processes are the same). For example, if \(x_{t}=2x_{t-1}+w_{t}\) with \(\sigma_{w}^{2}=1\), then \(y_{t}=\frac{1}{2}y_{t-1}+v_{t}\) with \(\sigma_{v}^{2}=1/4\) is an equivalent causal process (see Problem 3.3). This concept generalizes to higher orders, but it is easier to show using Chap. 4 techniques; see Example 4.8.

The technique of iterating backward to get an idea of the stationary solution of AR models works well when \(p=1\), but not for larger orders. A general technique is that of matching coefficients. Consider the AR(1) model in operator form

\[\phi(B)x_{t}=w_{t}, \tag{3.12}\]

where \(\phi(B)=1-\phi B\), and \(|\phi|<1\). Also, write the model in equation (3.6) using operator form as

\[x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}=\psi(B)w_{t}, \tag{3.13}\]

where \(\psi(B)=\sum_{j=0}^{\infty}\psi_{j}B^{j}\) and \(\psi_{j}=\phi^{j}\). Suppose we did not know that \(\psi_{j}=\phi^{j}\). We could substitute \(\psi(B)w_{t}\) from (3.13) for \(x_{t}\) in (3.12) to obtain

\[\phi(B)\psi(B)w_{t}=w_{t}. \tag{3.14}\]

The coefficients of \(B\) on the left-hand side of (3.14) must be equal to those on right-hand side of (3.14), which means

\[(1-\phi B)(1+\psi_{1}B+\psi_{2}B^{2}+\cdots+\psi_{j}B^{j}+\cdots)=1. \tag{3.15}\]

Reorganizing the coefficients in (3.15),

\[1+(\psi_{1}-\phi)B+(\psi_{2}-\psi_{1}\phi)B^{2}+\cdots+(\psi_{j}-\psi_{j-1} \phi)B^{j}+\cdots=1,\]

we see that for each \(j=1,2,\ldots\), the coefficient of \(B^{j}\) on the left must be zero because it is zero on the right. The coefficient of \(B\) on the left is \((\psi_{1}-\phi)\), and equating this to zero, \(\psi_{1}-\phi=0\), leads to \(\psi_{1}=\phi\). Continuing, the coefficient of \(B^{2}\) is \((\psi_{2}-\psi_{1}\phi)\), so \(\psi_{2}=\phi^{2}\). In general,

\[\psi_{j}=\psi_{j-1}\phi,\]

with \(\psi_{0}=1\), which leads to the solution \(\psi_{j}=\phi^{j}\).

Another way to think about the operations we just performed is to consider the AR(1) model in operator form, \(\phi(B)x_{t}=w_{t}\). Now multiply both sides by \(\phi^{-1}(B)\) (assuming the inverse operator exists) to get

\[\phi^{-1}(B)\phi(B)x_{t}=\phi^{-1}(B)w_{t},\]or

\[x_{t}=\phi^{-1}(B)w_{t}.\]

We know already that

\[\phi^{-1}(B)=1+\phi B+\phi^{2}B^{2}+\cdots+\phi^{j}B^{j}+\cdots,\]

that is, \(\phi^{-1}(B)\) is \(\psi(B)\) in (3.13). Thus, we notice that working with operators is like working with polynomials. That is, consider the polynomial \(\phi(z)=1-\phi z\), where \(z\) is a complex number and \(|\phi|<1\). Then,

\[\phi^{-1}(z)=\frac{1}{(1-\phi z)}=1+\phi z+\phi^{2}z^{2}+\cdots+\phi^{j}z^{j}+ \cdots,\quad|z|\leq 1,\]

and the coefficients of \(B^{j}\) in \(\phi^{-1}(B)\) are the same as the coefficients of \(z^{j}\) in \(\phi^{-1}(z)\). In other words, we may treat the backshift operator, \(B\), as a complex number, \(z\). These results will be generalized in our discussion of ARMA models. We will find the polynomials corresponding to the operators useful in exploring the general properties of ARMA models.

#### Introduction to Moving Average Models

As an alternative to the autoregressive representation in which the \(x_{t}\) on the left-hand side of the equation are assumed to be combined linearly, the moving average model of order \(q\), abbreviated as MA(\(q\)), assumes the white noise \(w_{t}\) on the right-hand side of the defining equation are combined linearly to form the observed data.

**Definition 3.3**: _The_ **moving average model** _of order \(q\), or_ **MA(\(q\))** _model, is defined to be_

\[x_{t}=w_{t}+\theta_{1}w_{t-1}+\theta_{2}w_{t-2}+\cdots+\theta_{q}w_{t-q}, \tag{3.16}\]

_where \(w_{t}\sim wn(0,\sigma_{w}^{2})\), and \(\theta_{1},\theta_{2},\ldots,\theta_{q}\) (\(\theta_{q}\neq 0\)) are parameters.2_

Footnote 2: Some texts and software packages write the MA model with negative coefficients; that is, \(x_{t}=w_{t}-\theta_{1}w_{t-1}-\theta_{2}w_{t-2}-\cdots-\theta_{q}w_{t-q}\).

The system is the same as the infinite moving average defined as the linear process (3.13), where \(\psi_{0}=1\), \(\psi_{j}=\theta_{j}\), for \(j=1,\ldots,q\), and \(\psi_{j}=0\) for other values. We may also write the MA(\(q\)) process in the equivalent form

\[x_{t}=\theta(B)w_{t}, \tag{3.17}\]

using the following definition.

**Definition 3.4**: _The_ **moving average operator** _is_

\[\theta(B)=1+\theta_{1}B+\theta_{2}B^{2}+\cdots+\theta_{q}B^{q}. \tag{3.18}\]

Unlike the autoregressive process, the moving average process is stationary for any values of the parameters \(\theta_{1},\ldots,\theta_{q}\); details of this result are provided in Sect. 3.3.

**Example 3.5**: **The MA(1) Process**

Consider the MA(1) model \(x_{t}=w_{t}+\theta w_{t-1}\). Then, \(\mathrm{E}(x_{t})=0\),

\[\gamma(h)=\begin{cases}(1+\theta^{2})\sigma_{w}^{2}&h=0,\\ \theta\sigma_{w}^{2}&h=1,\\ 0&h>1,\end{cases}\]

and the ACF is

\[\rho(h)=\begin{cases}\frac{\theta}{(1+\theta^{2})}&h=1,\\ 0&h>1.\end{cases}\]

Note \(|\rho(1)|\leq 1/2\) for all values of \(\theta\) (Problem 3.1). Also, \(x_{t}\) is correlated with \(x_{t-1}\), but not with \(x_{t-2},x_{t-3},\ldots\). Contrast this with the case of the AR(1) model in which the correlation between \(x_{t}\) and \(x_{t-k}\) is never zero. When \(\theta=.9\), for example, \(x_{t}\) and \(x_{t-1}\) are positively correlated, and \(\rho(1)=.497\). When \(\theta=-.9\), \(x_{t}\) and \(x_{t-1}\) are negatively correlated, \(\rho(1)=-.497\). Figure 3.2 shows a time plot of these two processes with \(\sigma_{w}^{2}=1\). The series for which \(\theta=.9\) is smoother than the series for which \(\theta=-.9\).

A figure similar to Fig. 3.2 can be created in R as follows:

``` par(mfrow=c(2,1)) plot(arima.sim(list(order=c(0,0,1), ma=.9), n=100), ylab="x",  main=(expression(MA(1)---theta==+.5))) plot(arima.sim(list(order=c(0,0,1), ma=.9), n=100), ylab="x",  main=(expression(MA(1)---theta==-.5)))

**Example 3.6**: **Non-uniqueness of MA Models and Invertibility**

Using Example 3.5, we note that for an MA(1) model, \(\rho(h)\) is the same for \(\theta\) and \(\frac{1}{\theta}\); try 5 and \(\frac{1}{5}\), for example. In addition, the pair \(\sigma_{w}^{2}=1\) and \(\theta=5\) yield the same autocovariance function as the pair \(\sigma_{w}^{2}=25\) and \(\theta=1/5\), namely,

\[\gamma(h)=\begin{cases}26&h=0,\\ 5&h=1,\\ 0&h>1.\end{cases}\]

Thus, the MA(1) processes

\[x_{t}=w_{t}+\tfrac{1}{5}w_{t-1},\quad w_{t}\sim\mathrm{iid}\;\;\mathrm{N}(0,25)\]

and

\[y_{t}=v_{t}+5v_{t-1},\quad v_{t}\sim\mathrm{iid}\;\;\mathrm{N}(0,1)\]

are the same because of normality (i.e., all finite distributions are the same). We can only observe the time series, \(x_{t}\) or \(y_{t}\), and not the noise, \(w_{t}\) or \(v_{t}\), so we cannot distinguish between the models. Hence, we will have to choose only one of them. For convenience, by mimicking the criterion of causality for AR models, we will choose the model with an infinite AR representation. Such a process is called an _invertible_ process.

To discover which model is the invertible model, we can reverse the roles of \(x_{t}\) and \(w_{t}\) (because we are mimicking the AR case) and write the MA(1) model as \(w_{t}=-\theta w_{t-1}+x_{t}\). Following the steps that led to (3.6), if \(|\theta|<1\), then \(w_{t}=\sum_{j=0}^{\infty}(-\theta)^{j}x_{t-j}\), which is the desired infinite AR representation of the model. Hence, given a choice, we will choose the model with \(\sigma_{w}^{2}=25\) and \(\theta=1/5\) because it is invertible.

As in the AR case, the polynomial, \(\theta(z)\), corresponding to the moving average operators, \(\theta(B)\), will be useful in exploring general properties of MA processes. For example, following the steps of equations (3.12)-(3.15), we can write the MA(1) model as \(x_{t}=\theta(B)w_{t}\), where \(\theta(B)=1+\theta B\). If \(|\theta|<1\), then we can write the model as \(\pi(B)x_{t}=w_{t}\), where \(\pi(B)=\theta^{-1}(B)\). Let \(\theta(z)=1+\theta z\), for \(|z|\leq 1\), then \(\pi(z)=\theta^{-1}(z)=1/(1+\theta z)=\sum_{j=0}^{\infty}(-\theta)^{j}z^{j}\), and we determine that \(\pi(B)=\sum_{j=0}^{\infty}(-\theta)^{j}B^{j}\).

### Autoregressive Moving Average Models

We now proceed with the general development of autoregressive, moving average, and mixed _autoregressive moving average_ (ARMA), models for stationary time series.

**Definition 3.5**: _A time series \(\{x_{t};\;t=0,\pm 1,\pm 2,\ldots\}\) is_ **ARMA(\(p,q\))** _if it is stationary and_

\[x_{t}=\phi_{1}x_{t-1}+\cdots+\phi_{p}x_{t-p}+w_{t}+\theta_{1}w_{t-1}+\cdots+ \theta_{q}w_{t-q}, \tag{3.19}\]

_with \(\phi_{p}\neq 0\), \(\theta_{q}\neq 0\), and \(\sigma_{w}^{2}>0\). The parameters \(p\) and \(q\) are called the autoregressive and the moving average orders, respectively. If \(x_{t}\) has a nonzero mean \(\mu\), we set \(\alpha=\mu(1-\phi_{1}-\cdots-\phi_{p})\) and write the model as_\[x_{t}=\alpha+\phi_{1}x_{t-1}+\cdots+\phi_{p}x_{t-p}+w_{t}+\theta_{1}w_{t-1}+\cdots+ \theta_{q}w_{t-q}\,, \tag{3.20}\]

_where \(w_{t}\sim wn(0,\sigma_{w}^{2})\)._

As previously noted, when \(q=0\), the model is called an autoregressive model of order \(p\), AR(\(p\)), and when \(p=0\), the model is called a moving average model of order \(q\), MA(\(q\)). To aid in the investigation of ARMA models, it will be useful to write them using the AR operator, (3.5), and the MA operator, (3.18). In particular, the ARMA(\(p,q\)) model in (3.19) can then be written in concise form as

\[\phi(B)x_{t}=\theta(B)w_{t}. \tag{3.21}\]

The concise form of the model points to a potential problem in that we can unnecessarily complicate the model by multiplying both sides by another operator, say

\[\eta(B)\phi(B)x_{t}=\eta(B)\theta(B)w_{t}\,,\]

without changing the dynamics. Consider the following example.

**Example 3.7**: **Parameter Redundancy**__

Consider a white noise process \(x_{t}=w_{t}\). If we multiply both sides of the equation by \(\eta(B)=1-.5B\), then the model becomes \((1-.5B)x_{t}=(1-.5B)w_{t}\), or

\[x_{t}=.5x_{t-1}-.5w_{t-1}+w_{t}, \tag{3.22}\]

which looks like an ARMA(\(1,1\)) model. Of course, \(x_{t}\) is still white noise; nothing has changed in this regard [i.e., \(x_{t}=w_{t}\) is the solution to (3.22)], but we have hidden the fact that \(x_{t}\) is white noise because of the _parameter redundancy_ or over-parameterization.

The consideration of parameter redundancy will be crucial when we discuss estimation for general ARMA models. As this example points out, we might fit an ARMA(\(1,1\)) model to white noise data and find that the parameter estimates are significant. If we were unaware of parameter redundancy, we might claim the data are correlated when in fact they are not (Problem 3.20). Although we have not yet discussed estimation, we present the following demonstration of the problem. We generated 150 iid normals and then fit an ARMA(\(1,1\)) to the data. Note that \(\hat{\phi}=-.96\) and \(\hat{\theta}=.95\), and both are significant. Below is the R code (note that the estimate called 'intercept' is really the estimate of the mean).

 set.seed(8675309) # Jenny, I got your number  x = rnorm(150, mean=5) # generate iid N(5,1)s  arima(x, order=c(1,0,1)) # estimation  Coefficients:  ar1 mal intercept<= misnomer  -0.9595 0.9527 5.0462  5.e. 0.1688 0.1750 0.0727Thus, forgetting the mean estimate, the fitted model looks like

\[(1+.96B)x_{t}=(1+.95B)w_{t},\]

which we should recognize as an over-parametrized model.

Example 3.3, Example 3.6, and Example 3.7 point to a number of problems with the general definition of ARMA(\(p,q\)) models, as given by (3.19), or, equivalently, by (3.21). To summarize, we have seen the following problems:

1. parameter redundant models,
2. stationary AR models that depend on the future, and
3. MA models that are not unique.

To overcome these problems, we will require some additional restrictions on the model parameters. First, we make the following definitions.

**Definition 3.6**: _The_ **AR and MA polynomials** _are defined as_

\[\phi(z)=1-\phi_{1}z-\cdots-\phi_{p}z^{p},\quad\phi_{p}\neq 0, \tag{3.23}\]

_and_

\[\theta(z)=1+\theta_{1}z+\cdots+\theta_{q}z^{q},\quad\theta_{q}\neq 0, \tag{3.24}\]

_respectively, where \(z\) is a complex number._

To address the first problem, we will henceforth refer to an ARMA(\(p,q\)) model to mean that it is in its simplest form. That is, in addition to the original definition given in equation (3.19), _we will also require that \(\phi(z)\) and \(\theta(z)\) have no common factors_. So, the process, \(x_{t}=.5x_{t-1}-.5w_{t-1}+w_{t}\), discussed in Example 3.7 is not referred to as an ARMA(\(1,1\)) process because, in its reduced form, \(x_{t}\) is white noise.

To address the problem of future-dependent models, we formally introduce the concept of _causality_.

**Definition 3.7**: _An ARMA(\(p,q\)) model is said to be_ **causal**_, if the time series \(\{x_{t};\ t=0,\pm 1,\pm 2,\ldots\}\) can be written as a one-sided linear process:_

\[x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}=\psi(B)w_{t}, \tag{3.25}\]

_where \(\psi(B)=\sum_{j=0}^{\infty}\psi_{j}B^{j}\), and \(\sum_{j=0}^{\infty}|\psi_{j}|<\infty\); we set \(\psi_{0}=1\)._

In Example 3.3, the AR(\(1\)) process, \(x_{t}=\phi x_{t-1}+w_{t}\), is causal only when \(|\phi|<1\). Equivalently, the process is causal only when the root of \(\phi(z)=1-\phi z\) is bigger than one in absolute value. That is, the root, say, \(z_{0}\), of \(\phi(z)\) is \(z_{0}=1/\phi\) (because \(\phi(z_{0})=0\)) and \(|z_{0}|>1\) because \(|\phi|<1\). In general, we have the following property.

**Property 3.1 Causality of an ARMA\((p,q)\) Process**

_An ARMA\((p,q)\) model is causal if and only if \(\phi(z)\neq 0\) for \(|z|\leq 1\). The coefficients of the linear process given in (3.25) can be determined by solving_

\[\psi(z)=\sum_{j=0}^{\infty}\psi_{j}z^{j}=\frac{\theta(z)}{\phi(z)},\quad|z|\leq 1.\]

Another way to phrase Property 3.1 is that _an ARMA process is causal only when the roots of \(\phi(z)\) lie outside the unit circle_; that is, \(\phi(z)=0\) only when \(|z|>1\). Finally, to address the problem of uniqueness discussed in Example 3.6, we choose the model that allows an infinite autoregressive representation.

**Definition 3.8**: _An ARMA(\(p,q\)) model is said to be_ **invertible**_, if the time series \(\{x_{t};\ t=0,\pm 1,\pm 2,\ldots\}\) can be written as_

\[\pi(B)x_{t}=\sum_{j=0}^{\infty}\pi_{j}x_{t-j}=w_{t}, \tag{3.26}\]

_where \(\pi(B)=\sum_{j=0}^{\infty}\pi_{j}B^{j}\), and \(\sum_{j=0}^{\infty}|\pi_{j}|<\infty\); we set \(\pi_{0}=1\)._

Analogous to Property 3.1, we have the following property.

**Property 3.2**: **Invertibility of an ARMA\((p,q)\) Process**

_An ARMA(\(p,q\)) model is invertible if and only if \(\theta(z)\neq 0\) for \(|z|\leq 1\). The coefficients \(\pi_{j}\) of \(\pi(B)\) given in (3.26) can be determined by solving_

\[\pi(z)=\sum_{j=0}^{\infty}\pi_{j}z^{j}=\frac{\phi(z)}{\theta(z)},\quad|z|\leq 1.\]

Another way to phrase Property 3.2 is that _an ARMA process is invertible only when the roots of \(\theta(z)\) lie outside the unit circle_; that is, \(\theta(z)=0\) only when \(|z|>1\). The proof of Property 3.1 is given in Section B.2 (the proof of Property 3.2 is similar). The following examples illustrate these concepts.

**Example 3.8**: **Parameter Redundancy, Causality, Invertibility**

Consider the process

\[x_{t}=.4x_{t-1}+.45x_{t-2}+w_{t}+w_{t-1}+.25w_{t-2},\]

or, in operator form,

\[(1-.4B-.45B^{2})x_{t}=(1+B+.25B^{2})w_{t}.\]

At first, \(x_{t}\) appears to be an ARMA\((2,2)\) process. But notice that

\[\phi(B)=1-.4B-.45B^{2}=(1+.5B)(1-.9B)\]\[\theta(B)=(1+B+.25B^{2})=(1+.5B)^{2}\]

have a common factor that can be canceled. After cancellation, the operators are \(\phi(B)=(1-.9B)\) and \(\theta(B)=(1+.5B)\), so the model is an ARMA\((1,1)\) model, \((1-.9B)x_{t}=(1+.5B)w_{t}\), or

\[x_{t}=.9x_{t-1}+.5w_{t-1}+w_{t}. \tag{3.27}\]

The model is causal because \(\phi(z)=(1-.9z)=0\) when \(z=10/9\), which is outside the unit circle. The model is also invertible because the root of \(\theta(z)=(1+.5z)\) is \(z=-2\), which is outside the unit circle.

To write the model as a linear process, we can obtain the \(\psi\)-weights using Property 3.1, \(\phi(z)\psi(z)=\theta(z)\), or

\[(1-.9z)(1+\psi_{1}z+\psi_{2}z^{2}+\cdots+\psi_{j}z^{j}+\cdots)=1+.5z.\]

Rearranging, we get

\[1+(\psi_{1}-.9)z+(\psi_{2}-.9\psi_{1})z^{2}+\cdots+(\psi_{j}-.9\psi_{j-1})z^{j }+\cdots=1+.5z.\]

Matching the coefficients of \(z\) on the left and right sides we get \(\psi_{1}-.9=.5\) and \(\psi_{j}-.9\psi_{j-1}=0\) for \(j>1\). Thus, \(\psi_{j}=1.4(.9)^{j-1}\) for \(j\geq 1\) and (3.27) can be written as

\[x_{t}=w_{t}+1.4\sum_{j=1}^{\infty}.9^{j-1}w_{t-j}.\]

The values of \(\psi_{j}\) may be calculated in R as follows:

ARMAtoMA(ar =.9, ma =.5, 10) # first 10 psi-weights
[1] 1.40 1.26 1.13 1.02 0.92 0.83 0.74 0.67 0.60 0.54

The invertible representation using Property 3.1 is obtained by matching coefficients in \(\theta(z)\pi(z)=\phi(z)\),

\[(1+.5z)(1+\pi_{1}z+\pi_{2}z^{2}+\pi_{3}z^{3}+\cdots)=1-.9z.\]

In this case, the \(\pi\)-weights are given by \(\pi_{j}=(-1)^{j}\)\(1.4\)\((.5)^{j-1}\), for \(j\geq 1\), and hence, because \(w_{t}=\sum_{j=0}^{\infty}\pi_{j}x_{t-j}\), we can also write (3.27) as

\[x_{t}=1.4\sum_{j=1}^{\infty}(-.5)^{j-1}x_{t-j}+w_{t}.\]

The values of \(\pi_{j}\) may be calculated in R as follows by reversing the roles of \(w_{t}\) and \(x_{t}\); i.e., write the model as \(w_{t}=-.5w_{t-1}+x_{t}-.9x_{t-1}\):

ARMAtoMA(ar = -.5, ma = -.9, 10) # first 10 pi-weights
[1] -1.400.700 -.350.175 -.087.044 -.022.011 -.006.003

**Example 3.9**: **Causal Conditions for an AR(2) Process**

For an AR(1) model, \((1-\phi B)x_{t}=w_{t}\), to be causal, the root of \(\phi(z)=1-\phi z\) must lie outside of the unit circle. In this case, \(\phi(z)=0\) when \(z=1/\phi\), so it is easy to go from the causal requirement on the root, \(|1/\phi|>1\), to a requirement on the parameter, \(|\phi|<1\). It is not so easy to establish this relationship for higher order models.

For example, the AR(2) model, \((1-\phi_{1}B-\phi_{2}B^{2})x_{t}=w_{t}\), is causal when the two roots of \(\phi(z)=1-\phi_{1}z-\phi_{2}z^{2}\) lie outside of the unit circle. Using the quadratic formula, this requirement can be written as

\[\left|\frac{\phi_{1}\pm\sqrt{\phi_{1}^{2}+4\phi_{2}}}{-2\phi_{2}}\right|>1.\]

The roots of \(\phi(z)\) may be real and distinct, real and equal, or a complex conjugate pair. If we denote those roots by \(z_{1}\) and \(z_{2}\), we can write \(\phi(z)=(1-z_{1}^{-1}z)(1-z_{2}^{-1}z)\); note that \(\phi(z_{1})=\phi(z_{2})=0\). The model can be written in operator form as \((1-z_{1}^{-1}B)(1-z_{2}^{-1}B)x_{t}=w_{t}\). From this representation, it follows that \(\phi_{1}=(z_{1}^{-1}+z_{2}^{-1})\) and \(\phi_{2}=-(z_{1}z_{2})^{-1}\). This relationship and the fact that \(|z_{1}|>1\) and \(|z_{2}|>1\) can be used to establish the following equivalent condition for causality:

\[\phi_{1}+\phi_{2}<1,\quad\phi_{2}-\phi_{1}<1,\quad\text{and}\quad|\phi_{2}|<1. \tag{3.28}\]

This causality condition specifies a triangular region in the parameter space; see Fig. 3.3 We leave the details of the equivalence to the reader (Problem 3.5).

### Difference Equations

The study of the behavior of ARMA processes and their ACFs is greatly enhanced by a basic knowledge of difference equations, simply because they are difference equations.

Figure 3.3: Causal region for an AR(2) in terms of the parametersWe will give a brief and heuristic account of the topic along with some examples of the usefulness of the theory. For details, the reader is referred to Mickens [142].

Suppose we have a sequence of numbers \(u_{0},u_{1},u_{2},\ldots\) such that

\[u_{n}-\alpha u_{n-1}=0,\quad\alpha\neq 0,\quad n=1,2,\ldots. \tag{3.29}\]

For example, recall (3.9) in which we showed that the ACF of an AR(1) process is a sequence, \(\rho(h)\), satisfying

\[\rho(h)-\phi\rho(h-1)=0,\quad h=1,2,\ldots\.\]

Equation (3.29) represents a _homogeneous difference equation of order 1_. To solve the equation, we write:

\[u_{1} =\alpha u_{0}\] \[u_{2} =\alpha u_{1}=\alpha^{2}u_{0}\] \[\vdots\] \[u_{n} =\alpha u_{n-1}=\alpha^{n}u_{0}.\]

Given an initial condition \(u_{0}=c\), we may solve (3.29), namely, \(u_{n}=\alpha^{n}c\).

In operator notation, (3.29) can be written as \((1-\alpha B)u_{n}=0\). The polynomial associated with (3.29) is \(\alpha(z)=1-\alpha z\), and the root, say, \(z_{0}\), of this polynomial is \(z_{0}=1/\alpha\); that is \(\alpha(z_{0})=0\). We know a solution (in fact, _the_ solution) to (3.29), with initial condition \(u_{0}=c\), is

\[u_{n}=\alpha^{n}c=\left(z_{0}^{-1}\right)^{n}c. \tag{3.30}\]

That is, the solution to the difference equation (3.29) depends only on the initial condition and the inverse of the root to the associated polynomial \(\alpha(z)\).

Now suppose that the sequence satisfies

\[u_{n}-\alpha_{1}u_{n-1}-\alpha_{2}u_{n-2}=0,\quad\alpha_{2}\neq 0,\quad n=2,3,\ldots \tag{3.31}\]

This equation is a _homogeneous difference equation of order 2_. The corresponding polynomial is

\[\alpha(z)=1-\alpha_{1}z-\alpha_{2}z^{2},\]

which has two roots, say, \(z_{1}\) and \(z_{2}\); that is, \(\alpha(z_{1})=\alpha(z_{2})=0\). We will consider two cases. First suppose \(z_{1}\neq z_{2}\). Then the general solution to (3.31) is

\[u_{n}=c_{1}z_{1}^{-n}+c_{2}z_{2}^{-n}, \tag{3.32}\]

where \(c_{1}\) and \(c_{2}\) depend on the initial conditions. The claim it is \(a\) solution can be verified by direct substitution of (3.32) into (3.31):

\[\underbrace{\left(c_{1}z_{1}^{-n}+c_{2}z_{2}^{-n}\right)}_{u_{n} }-\alpha_{1}\underbrace{\left(c_{1}z_{1}^{-(n-1)}+c_{2}z_{2}^{-(n-1)}\right)}_ {u_{n-1}}-\alpha_{2}\underbrace{\left(c_{1}z_{1}^{-(n-2)}+c_{2}z_{2}^{-(n-2)} \right)}_{u_{n-2}}\] \[\qquad=c_{1}z_{1}^{-n}\left(1-\alpha_{1}z_{1}-\alpha_{2}z_{1}^{2} \right)+c_{2}z_{2}^{-n}\left(1-\alpha_{1}z_{2}-\alpha_{2}z_{2}^{2}\right)\] \[\qquad=c_{1}z_{1}^{-n}\alpha(z_{1})+c_{2}z_{2}^{-n}\alpha(z_{2})=0.\]

[MISSING_PAGE_EMPTY:102]

multiplicity \(m_{r}\), such that \(m_{1}+m_{2}+\cdots+m_{r}=p\). The general solution to the difference equation (3.36) is

\[u_{n}=z_{1}^{-n}P_{1}(n)+z_{2}^{-n}P_{2}(n)+\cdots+z_{r}^{-n}P_{r}(n), \tag{3.37}\]

where \(P_{j}(n)\), for \(j=1,2,\ldots,r\), is a polynomial in \(n\), of degree \(m_{j}-1\). Given \(p\) initial conditions \(u_{0},\ldots,u_{p-1}\), we can solve for the \(P_{j}(n)\) explicitly.

**Example 3.10**: **The ACF of an AR(2) Process**

Suppose \(x_{t}=\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+w_{t}\) is a causal AR(2) process. Multiply each side of the model by \(x_{t-h}\) for \(h>0\), and take expectation:

\[\mathrm{E}(x_{t}x_{t-h})=\phi_{1}\mathrm{E}(x_{t-1}x_{t-h})+\phi_{2}\mathrm{E} (x_{t-2}x_{t-h})+\mathrm{E}(w_{t}x_{t-h}).\]

The result is

\[\gamma(h)=\phi_{1}\gamma(h-1)+\phi_{2}\gamma(h-2),\quad h=1,2,\ldots. \tag{3.38}\]

In (3.38), we used the fact that \(\mathrm{E}(x_{t})=0\) and for \(h>0\),

\[\mathrm{E}(w_{t}x_{t-h})=\mathrm{E}\Big{(}w_{t}\sum_{j=0}^{\infty}\psi_{j}w_{t- h-j}\Big{)}=0.\]

Divide (3.38) through by \(\gamma(0)\) to obtain the difference equation for the ACF of the process:

\[\rho(h)-\phi_{1}\rho(h-1)-\phi_{2}\rho(h-2)=0,\quad h=1,2,\ldots. \tag{3.39}\]

The initial conditions are \(\rho(0)=1\) and \(\rho(-1)=\phi_{1}/(1-\phi_{2})\), which is obtained by evaluating (3.39) for \(h=1\) and noting that \(\rho(1)=\rho(-1)\).

Using the results for the homogeneous difference equation of order two, let \(z_{1}\) and \(z_{2}\) be the roots of the associated polynomial, \(\phi(z)=1-\phi_{1}z-\phi_{2}z^{2}\). Because the model is causal, we know the roots are outside the unit circle: \(|z_{1}|>1\) and \(|z_{2}|>1\). Now, consider the solution for three cases:

(i) When \(z_{1}\) and \(z_{2}\) are real and distinct, then

\[\rho(h)=c_{1}z_{1}^{-h}+c_{2}z_{2}^{-h},\]

so \(\rho(h)\to 0\) exponentially fast as \(h\to\infty\).

(ii) When \(z_{1}=z_{2}\) (\(=z_{0}\)) are real and equal, then

\[\rho(h)=z_{0}^{-h}(c_{1}+c_{2}h),\]

so \(\rho(h)\to 0\) exponentially fast as \(h\to\infty\).

(iii) When \(z_{1}=\bar{z}_{2}\) are a complex conjugate pair, then \(c_{2}=\bar{c}_{1}\) (because \(\rho(h)\) is real), and

\[\rho(h)=c_{1}z_{1}^{-h}+\bar{c}_{1}\bar{z}_{1}^{-h}.\]

Write \(c_{1}\) and \(z_{1}\) in polar coordinates, for example, \(z_{1}=|z_{1}|e^{i\theta}\), where \(\theta\) is the angle whose tangent is the ratio of the imaginary part and the real part of (sometimes called \(\arg(z_{1})\); the range of \(\theta\) is \([-\pi,\pi]\)). Then, using the fact that \(e^{i\alpha}+e^{-i\alpha}=2\cos(\alpha)\), the solution has the form \[\rho(h)=a|z_{1}|^{-h}\cos(h\theta+b),\] where \(a\) and \(b\) are determined by the initial conditions. Again, \(\rho(h)\) dampens to zero exponentially fast as \(h\to\infty\), but it does so in a sinusoidal fashion. The implication of this result is shown in the next example.

**Example 3.11**: **An AR(2) with Complex Roots**

Figure 3.4 shows \(n=144\) observations from the AR(2) model

\[x_{t}=1.5x_{t-1}-.75x_{t-2}+w_{t},\]

with \(\sigma_{w}^{2}=1\), and with complex roots chosen so the process exhibits pseudo-cyclic behavior at the rate of one cycle every 12 time points. The autoregressive polynomial for this model is \(\phi(z)=1-1.5z+.75z^{2}\). The roots of \(\phi(z)\) are \(1\pm i/\sqrt{3}\), and \(\theta=\tan^{-1}(1/\sqrt{3})=2\pi/12\) radians per unit time. To convert the angle to cycles per unit time, divide by \(2\pi\) to get 1/12 cycles per unit time. The ACF for this model is shown in left-hand-side of Fig. 3.5.

To calculate the roots of the polynomial and solve for arg in R:

z = c(1,-1.5,.75) # coefficients of the polynomial (a = polyroot(z)[1]) # print one root = 1 + i/sqrt(3)  [1] 1+0.57735i  arg = Arg(a)/(2*pi) # arg in cycles/pt  1/arg # the pseudo period  [1] 12 To reproduce Fig. 3.4:

set.seed(8675309)  ar2 = arima.sim(list(order=c(2,0,0), ar=c(1.5,-.75)), n = 144)plot(ar2, axes=FALSE, xlab="Time")  axis(2); axis(1, at=seq(@,144,by=12)); box()  abline(v=seq(@,144,by=12), lty=2)  To calculate and display the ACF for this model: ACF = ARMAacf(ar=c(1.5,..75), ma=@, 50)  plot(ACF, type="h", xlab="lag")  abline(h=@)

**Example 3.12**: **The \(\psi\)-weights for an ARMA Model**

For a causal ARMA(\(p,q\)) model, \(\phi(B)x_{t}=\theta(B)w_{t}\), where the zeros of \(\phi(z)\) are outside the unit circle, recall that we may write

\[x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j},\]

where the \(\psi\)-weights are determined using Property 3.1.

For the pure MA(\(q\)) model, \(\psi_{0}=1\), \(\psi_{j}=\theta_{j}\), for \(j=1,\ldots,q\), and \(\psi_{j}=0\), otherwise. For the general case of ARMA(\(p,q\)) models, the task of solving for the \(\psi\)-weights is much more complicated, as was demonstrated in Example 3.8. The use of the theory of homogeneous difference equations can help here. To solve for the \(\psi\)-weights in general, we must match the coefficients in \(\phi(z)\psi(z)=\theta(z)\):

\[(1-\phi_{1}z-\phi_{2}z^{2}-\cdots)(\psi_{0}+\psi_{1}z+\psi_{2}z^{2}+\cdots)=(1 +\theta_{1}z+\theta_{2}z^{2}+\cdots).\]

The first few values are

\[\begin{array}{rcl}\psi_{0}&=&1\\ \psi_{1}-\phi_{1}\psi_{0}&=&\theta_{1}\\ \psi_{2}-\phi_{1}\psi_{1}-\phi_{2}\psi_{0}&=&\theta_{2}\\ \psi_{3}-\phi_{1}\psi_{2}-\phi_{2}\psi_{1}-\phi_{3}\psi_{0}&=&\theta_{3}\\ &\vdots&\end{array}\]

where we would take \(\phi_{j}=0\) for \(j>p\), and \(\theta_{j}=0\) for \(j>q\). The \(\psi\)-weights satisfy the homogeneous difference equation given by

\[\psi_{j}-\sum_{k=1}^{p}\phi_{k}\psi_{j-k}=0,\quad j\geq\max(p,q+1), \tag{3.40}\]

with initial conditions

\[\psi_{j}-\sum_{k=1}^{j}\phi_{k}\psi_{j-k}=\theta_{j},\quad 0\leq j<\max(p,q+1). \tag{3.41}\]

The general solution depends on the roots of the AR polynomial \(\phi(z)=1-\phi_{1}z-\cdots-\phi_{p}z^{p}\), as seen from (3.40). The specific solution will, of course, depend on the initial conditions.

Consider the ARMA process given in (3.27), \(x_{t}=.9x_{t-1}+.5w_{t-1}+w_{t}\). Because \(\max(p,q+1)=2\), using (3.41), we have \(\psi_{0}=1\) and \(\psi_{1}=.9+.5=1.4\). By (3.40), for \(j=2,3,\dots\), the \(\psi\)-weights satisfy \(\psi_{j}-.9\psi_{j-1}=0\). The general solution is \(\psi_{j}=c\cdot 9^{j}\). To find the specific solution, use the initial condition \(\psi_{1}=1.4\), so \(1.4=.9c\) or \(c=1.4/.9\). Finally, \(\psi_{j}=1.4(.9)^{j-1}\), for \(j\geq 1\), as we saw in Example 3.8.

To view, for example, the first 50 \(\psi\)-weights in R, use:

ARMAtoMA(ar=.9, ma=.5, 50) # for a list

plot(ARMAtoMA(ar=.9, ma=.5, 50)) # for a graph

### Autocorrelation and Partial Autocorrelation

We begin by exhibiting the ACF of an MA(\(q\)) process, \(x_{t}=\theta(B)w_{t}\), where \(\theta(B)=1+\theta_{1}B+\dots+\theta_{q}B^{q}\). Because \(x_{t}\) is a finite linear combination of white noise terms, the process is stationary with mean

\[\mathrm{E}(x_{t})=\sum_{j=0}^{q}\theta_{j}\mathrm{E}(w_{t-j})=0,\]

where we have written \(\theta_{0}=1\), and with autocovariance function

\[\gamma(h)=\mathrm{cov}\left(x_{t+h},x_{t}\right) =\mathrm{cov}\Big{(}\sum_{j=0}^{q}\theta_{j}w_{t+h-j},\;\sum_{k=0 }^{q}\theta_{k}w_{t-k}\Big{)}\] \[=\begin{cases}\sigma_{w}^{2}\sum_{j=0}^{q-h}\theta_{j}\theta_{j+h },&0\leq h\leq q\\ 0&h>q.\end{cases} \tag{3.42}\]

Recall that \(\gamma(h)=\gamma(-h)\), so we will only display the values for \(h\geq 0\). Note that \(\gamma(q)\) cannot be zero because \(\theta_{q}\neq 0\). The cutting off of \(\gamma(h)\) after \(q\) lags is the signature of the MA(\(q\)) model. Dividing (3.42) by \(\gamma(0)\) yields the _ACF of an MA(\(q\))_:

\[\rho(h)=\begin{cases}\dfrac{\sum_{j=0}^{q-h}\theta_{j}\theta_{j+h}}{1+\theta_{ 1}^{2}+\dots+\theta_{q}^{2}}&1\leq h\leq q\\ 0&h>q.\end{cases} \tag{3.43}\]

For a causal ARMA(\(p,q\)) model, \(\phi(B)x_{t}=\theta(B)w_{t}\), where the zeros of \(\phi(z)\) are outside the unit circle, write

\[x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}. \tag{3.44}\]

It follows immediately that \(\mathrm{E}(x_{t})=0\) and the autocovariance function of \(x_{t}\) is

\[\gamma(h)=\mathrm{cov}(x_{t+h},x_{t})=\sigma_{w}^{2}\sum_{j=0}^{\infty}\psi_{j }\psi_{j+h},\quad h\geq 0. \tag{3.45}\]We could then use (3.40) and (3.41) to solve for the \(\psi\)-weights. In turn, we could solve for \(\gamma(h)\), and the ACF \(\rho(h)=\gamma(h)/\gamma(0)\). As in Example 3.10, it is also possible to obtain a homogeneous difference equation directly in terms of \(\gamma(h)\). First, we write

\[\begin{split}\gamma(h)&=\text{cov}(x_{t+h},x_{t})= \text{cov}\Big{(}\sum_{j=1}^{p}\phi_{j}x_{t+h-j}+\sum_{j=0}^{q}\theta_{j}w_{t+h -j},\ x_{t}\Big{)}\\ &=\sum_{j=1}^{p}\phi_{j}\gamma(h-j)+\sigma_{w}^{2}\sum_{j=h}^{q} \theta_{j}\psi_{j-h},\quad h\geq 0,\end{split} \tag{3.46}\]

where we have used the fact that, for \(h\geq 0\),

\[\text{cov}(w_{t+h-j},\ x_{t})=\text{cov}\Big{(}w_{t+h-j},\ \sum_{k=0}^{\infty} \psi_{k}w_{t-k}\Big{)}=\psi_{j-h}\sigma_{w}^{2}.\]

From (3.46), we can write a _general homogeneous equation for the ACF of a causal ARMA process_:

\[\gamma(h)-\phi_{1}\gamma(h-1)-\cdots-\phi_{p}\gamma(h-p)=0,\quad h\geq\max(p,q +1), \tag{3.47}\]

with initial conditions

\[\gamma(h)-\sum_{j=1}^{p}\phi_{j}\gamma(h-j)=\sigma_{w}^{2}\sum_{j=h}^{q}\theta _{j}\psi_{j-h},\quad 0\leq h<\max(p,q+1). \tag{3.48}\]

Dividing (3.47) and (3.48) through by \(\gamma(0)\) will allow us to solve for the ACF, \(\rho(h)=\gamma(h)/\gamma(0)\).

**Example 3.13**: **The ACF of an AR(\(p\))**

In Example 3.10 we considered the case where \(p=2\). For the general case, it follows immediately from (3.47) that

\[\rho(h)-\phi_{1}\rho(h-1)-\cdots-\phi_{p}\rho(h-p)=0,\quad h\geq p. \tag{3.49}\]

Let \(z_{1},\ldots,z_{r}\) denote the roots of \(\phi(z)\), each with multiplicity \(m_{1},\ldots,m_{r}\), respectively, where \(m_{1}+\cdots+m_{r}=p\). Then, from (3.37), the general solution is

\[\rho(h)=z_{1}^{-h}P_{1}(h)+z_{2}^{-h}P_{2}(h)+\cdots+z_{r}^{-h}P_{r}(h),\quad h \geq p, \tag{3.50}\]

where \(P_{j}(h)\) is a polynomial in \(h\) of degree \(m_{j}-1\).

Recall that for a causal model, all of the roots are outside the unit circle, \(|z_{i}|>1\), for \(i=1,\ldots,r\). If all the roots are real, then \(\rho(h)\) dampens exponentially fast to zero as \(h\to\infty\). If some of the roots are complex, then they will be in conjugate pairs and \(\rho(h)\) will dampen, in a sinusoidal fashion, exponentially fast to zero as \(h\to\infty\). In the case of complex roots, the time series will appear to be cyclic in nature. This, of course, is also true for ARMA models in which the AR part has complex roots.

**Example 3.14**: **The ACF of an ARMA\((1,1)\)**

Consider the ARMA\((1,1)\) process \(x_{t}=\phi x_{t-1}+\theta w_{t-1}+w_{t}\), where \(|\phi|<1\). Based on (3.47), the autocovariance function satisfies

\[\gamma(h)-\phi\gamma(h-1)=0,\quad h=2,3,\ldots,\]

and it follows from (3.29)-(3.30) that the general solution is

\[\gamma(h)=c\;\phi^{h},\quad h=1,2,\ldots\;. \tag{3.51}\]

To obtain the initial conditions, we use (3.48):

\[\gamma(0)=\phi\gamma(1)+\sigma_{w}^{2}[1+\theta\phi+\theta^{2}]\quad\text{and} \quad\gamma(1)=\phi\gamma(0)+\sigma_{w}^{2}\theta.\]

Solving for \(\gamma(0)\) and \(\gamma(1)\), we obtain:

\[\gamma(0)=\sigma_{w}^{2}\frac{1+2\theta\phi+\theta^{2}}{1-\phi^{2}}\quad\text {and}\quad\gamma(1)=\sigma_{w}^{2}\frac{(1+\theta\phi)(\phi+\theta)}{1-\phi^{ 2}}.\]

To solve for \(c\), note that from (3.51), \(\gamma(1)=c\;\phi\) or \(c=\gamma(1)/\phi\). Hence, the specific solution for \(h\geq 1\) is

\[\gamma(h)=\frac{\gamma(1)}{\phi}\,\phi^{h}=\sigma_{w}^{2}\frac{(1+\theta\phi)( \phi+\theta)}{1-\phi^{2}}\,\phi^{h-1}.\]

Finally, dividing through by \(\gamma(0)\) yields the ACF

\[\rho(h)=\frac{(1+\theta\phi)(\phi+\theta)}{1+2\theta\phi+\theta^{2}}\,\phi^{h- 1},\quad h\geq 1. \tag{3.52}\]

Notice that the general pattern of \(\rho(h)\) versus \(h\) in (3.52) is not different from that of an AR(1) given in (3.8). Hence, it is unlikely that we will be able to tell the difference between an ARMA(1,1) and an AR(1) based solely on an ACF estimated from a sample. This consideration will lead us to the partial autocorrelation function.

#### The Partial Autocorrelation Function (PACF)

We have seen in (3.43), for MA(\(q\)) models, the ACF will be zero for lags greater than \(q\). Moreover, because \(\theta_{q}\neq 0\), the ACF will not be zero at lag \(q\). Thus, the ACF provides a considerable amount of information about the order of the dependence when the process is a moving average process. If the process, however, is ARMA or AR, the ACF alone tells us little about the orders of dependence. Hence, it is worthwhile pursuing a function that will behave like the ACF of MA models, but for AR models, namely, the _partial autocorrelation function (PACF)_.

Recall that if \(X\), \(Y\), and \(Z\) are random variables, then the partial correlation between \(X\) and \(Y\) given \(Z\) is obtained by regressing \(X\) on \(Z\) to obtain \(\hat{X}\), regressing \(Y\) on \(Z\) to obtain \(\hat{Y}\), and then calculating

\[\rho_{XY|Z}=\text{corr}\{X-\hat{X},\,Y-\hat{Y}\}.\]The idea is that \(\rho_{XY|Z}\) measures the correlation between \(X\) and \(Y\) with the linear effect of \(Z\) removed (or partialled out). If the variables are multivariate normal, then this definition coincides with \(\rho_{XY|Z}=\operatorname{corr}(X,Y\mid Z)\).

To motivate the idea for time series, consider a causal AR(1) model, \(x_{t}=\phi x_{t-1}+w_{t}\). Then,

\[\gamma_{x}(2)=\operatorname{cov}(x_{t},x_{t-2}) =\operatorname{cov}(\phi x_{t-1}+w_{t},x_{t-2})\] \[=\operatorname{cov}(\phi^{2}x_{t-2}+\phi w_{t-1}+w_{t},x_{t-2})= \phi^{2}\gamma_{x}(0).\]

This result follows from causality because \(x_{t-2}\) involves \(\{w_{t-2},w_{t-3},\ldots\}\), which are all uncorrelated with \(w_{t}\) and \(w_{t-1}\). The correlation between \(x_{t}\) and \(x_{t-2}\) is not zero, as it would be for an MA(1), because \(x_{t}\) is dependent on \(x_{t-2}\) through \(x_{t-1}\). Suppose we break this chain of dependence by removing (or partial out) the effect \(x_{t-1}\). That is, we consider the correlation between \(x_{t}-\phi x_{t-1}\) and \(x_{t-2}-\phi x_{t-1}\), because it is the correlation between \(x_{t}\) and \(x_{t-2}\) with the linear dependence of each on \(x_{t-1}\) removed. In this way, we have broken the dependence chain between \(x_{t}\) and \(x_{t-2}\). In fact,

\[\operatorname{cov}(x_{t}-\phi x_{t-1},x_{t-2}-\phi x_{t-1})=\operatorname{cov} (w_{t},x_{t-2}-\phi x_{t-1})=0.\]

Hence, the tool we need is partial autocorrelation, which is the correlation between \(x_{s}\) and \(x_{t}\) with the linear effect of everything "in the middle" removed.

To formally define the PACF for mean-zero stationary time series, let \(\hat{x}_{t+h}\), for \(h\geq 2\), denote the regression3 of \(x_{t+h}\) on \(\{x_{t+h-1},x_{t+h-2},\ldots,x_{t+1}\}\), which we write as

Footnote 3: The term regression here refers to regression in the population sense. That is, \(\hat{x}_{t+h}\) is the linear combination of \(\{x_{t+h-1},x_{t+h-2},\ldots,x_{t+1}\}\) that minimizes the mean squared error \(\operatorname{E}(x_{t+h}-\sum_{j=1}^{h-1}\alpha_{j}x_{t+j})^{2}\).

\[\hat{x}_{t+h}=\beta_{1}x_{t+h-1}+\beta_{2}x_{t+h-2}+\cdots+\beta_{h-1}x_{t+1}. \tag{3.53}\]

No intercept term is needed in (3.53) because the mean of \(x_{t}\) is zero (otherwise, replace \(x_{t}\) by \(x_{t}-\mu_{x}\) in this discussion). In addition, let \(\hat{x}_{t}\) denote the regression of \(x_{t}\) on \(\{x_{t+1},x_{t+2},\ldots,x_{t+h-1}\}\), then

\[\hat{x}_{t}=\beta_{1}x_{t+1}+\beta_{2}x_{t+2}+\cdots+\beta_{h-1}x_{t+h-1}. \tag{3.54}\]

Because of stationarity, the coefficients, \(\beta_{1},\ldots,\beta_{h-1}\) are the same in (3.53) and (3.54); we will explain this result in the next section, but it will be evident from the examples.

**Definition 3.9**: _The_ **partial autocorrelation function (PACF)** _of a stationary process, \(x_{t}\), denoted \(\phi_{hh}\), for \(h=1,2,\ldots,\) is_

\[\phi_{11}=\operatorname{corr}(x_{t+1},x_{t})=\rho(1) \tag{3.55}\]

_and_

\[\phi_{hh}=\operatorname{corr}(x_{t+h}-\hat{x}_{t+h},\,x_{t}-\hat{x}_{t}),\quad h \geq 2. \tag{3.56}\]

The reason for using a double subscript will become evident in the next section. The PACF, \(\phi_{hh}\), is the correlation between \(x_{t+h}\) and \(x_{t}\) with the linear dependence of \(\{x_{t+1},\ldots,x_{t+h-1}\}\) on each, removed. If the process \(x_{t}\) is Gaussian, then \(\phi_{hh}=\operatorname{corr}(x_{t+h},x_{t}\mid x_{t+1},\ldots,x_{t+h-1})\); that is, \(\phi_{hh}\) is the correlation coefficient between \(x_{t+h}\) and \(x_{t}\) in the bivariate distribution of \((x_{t+h},x_{t})\) conditional on \(\{x_{t+1},\ldots,x_{t+h-1}\}\).

**Example 3.15**: **The PACF of an AR(1)**

Consider the PACF of the AR(1) process given by \(x_{t}=\phi x_{t-1}+w_{t}\), with \(|\phi|<1\). By definition, \(\phi_{11}=\rho(1)=\phi\). To calculate \(\phi_{22}\), consider the regression of \(x_{t+2}\) on \(x_{t+1}\), say, \(\hat{x}_{t+2}=\beta x_{t+1}\). We choose \(\beta\) to minimize

\[\mathrm{E}(x_{t+2}-\hat{x}_{t+2})^{2}=\mathrm{E}(x_{t+2}-\beta x_{t+1})^{2}= \gamma(0)-2\beta\gamma(1)+\beta^{2}\gamma(0).\]

Taking derivatives with respect to \(\beta\) and setting the result equal to zero, we have \(\beta=\gamma(1)/\gamma(0)=\rho(1)=\phi\). Next, consider the regression of \(x_{t}\) on \(x_{t+1}\), say \(\hat{x}_{t}=\beta x_{t+1}\). We choose \(\beta\) to minimize

\[\mathrm{E}(x_{t}-\hat{x}_{t})^{2}=\mathrm{E}(x_{t}-\beta x_{t+1})^{2}=\gamma(0 )-2\beta\gamma(1)+\beta^{2}\gamma(0).\]

This is the same equation as before, so \(\beta=\phi\). Hence,

\[\phi_{22} =\mathrm{corr}(x_{t+2}-\hat{x}_{t+2},x_{t}-\hat{x}_{t})=\mathrm{ corr}(x_{t+2}-\phi x_{t+1},x_{t}-\phi x_{t+1})\] \[=\mathrm{corr}(w_{t+2},x_{t}-\phi x_{t+1})=0\]

by causality. Thus, \(\phi_{22}=0\). In the next example, we will see that in this case, \(\phi_{hh}=0\) for all \(h>1\).

**Example 3.16**: **The PACF of an AR(\(p\))**

The model implies \(x_{t+h}=\sum_{j=1}^{p}\phi_{j}x_{t+h-j}+w_{t+h}\), where the roots of \(\phi(z)\) are outside the unit circle. When \(h>p\), the regression of \(x_{t+h}\) on \(\{x_{t+1},\ldots,x_{t+h-1}\}\), is

\[\hat{x}_{t+h}=\sum_{j=1}^{p}\phi_{j}x_{t+h-j}.\]

We have not proved this obvious result yet, but we will prove it in the next section. Thus, when \(h>p\),

\[\phi_{hh}=\mathrm{corr}(x_{t+h}-\hat{x}_{t+h},\;x_{t}-\hat{x}_{t})=\mathrm{ corr}(w_{t+h},\;x_{t}-\hat{x}_{t})=0,\]

Figure 3.5: The ACF and PACF of an AR(2) model with \(\phi_{1}=1.5\) and \(\phi_{2}=-.75\)because, by causality, \(x_{t}-\hat{x}_{t}\) depends only on \(\{w_{t+h-1},w_{t+h-2},\ldots\}\); recall equation (3.54). When \(h\leq p\), \(\phi_{pp}\) is not zero, and \(\phi_{11},\ldots,\phi_{p-1,p-1}\) are not necessarily zero. We will see later that, in fact, \(\phi_{pp}=\phi_{p}\). Figure 3.5 shows the ACF and the PACF of the AR(2) model presented in Example 3.11. To reproduce Fig. 3.5 in R, use the following commands:

```
ACF=ARMAacf(ar=c(1.5,-.75),ma=0,24)[-1] PACF=ARMAacf(ar=c(1.5,-.75),ma=0,24,pacf=TRUE) par(mfrow=c(1,2)) plot(ACF, type="h", xlab="lag", ylim=c(-.8,1));abline(h=0) plot(PACF, type="h", xlab="lag", ylim=c(-.8,1));abline(h=0)
```

**Example 3.17**: **The PACF of an Invertible MA(q)**

For an invertible MA(\(q\)), we can write \(x_{t}=-\sum_{j=1}^{\infty}\pi_{j}x_{t-j}+w_{t}\). Moreover, no finite representation exists. From this result, it should be apparent that the PACF will never cut off, as in the case of an AR(\(p\)).

For an MA(1), \(x_{t}=w_{t}+\theta w_{t-1}\), with \(|\theta|<1\), calculations similar to Example 3.15 will yield \(\phi_{22}=-\theta^{2}/(1+\theta^{2}+\theta^{4})\). For the MA(1) in general, we can show that

\[\phi_{hh}=-\frac{(-\theta)^{h}(1-\theta^{2})}{1-\theta^{2(h+1)}},\quad h\geq 1.\]

In the next section, we will discuss methods of calculating the PACF. The PACF for MA models behaves much like the ACF for AR models. Also, the PACF for AR models behaves much like the ACF for MA models. Because an invertible ARMA model has an infinite AR representation, the PACF will not cut off. We may summarize these results in Table 3.1.

**Example 3.18**: **Preliminary Analysis of the Recruitment Series**

We consider the problem of modeling the Recruitment series shown in Fig. 1.5. There are 453 months of observed recruitment ranging over the years 1950-1987. The ACF and the PACF given in Fig. 3.6 are consistent with the behavior of an AR(2). The ACF has cycles corresponding roughly to a 12-month period, and the PACF has large values for \(h=1,2\) and then is essentially zero for higher order lags. Based on Table 3.1, these results suggest that a second-order (\(p=2\)) autoregressive model might provide a good fit. Although we will discuss estimation

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & AR(\(p\)) & MA(\(q\)) & ARMA(\(p,q\)) \\ \hline ACF & Tails off & Cuts off & Tails off \\  & after lag \(q\) & & \\ PACF & Cuts off & Tails off & Tails off \\  & after lag \(p\) & & \\ \hline \end{tabular}
\end{table}
Table 3.1: Behavior of the ACF and PACF for ARMA modelsin detail in Sect. 3.5, we ran a regression (see Sect. 2.1) using the data triplets \(\{(x;z_{1},z_{2}):(x_{3};x_{2},x_{1}),(x_{4};x_{3},x_{2}),\ldots,(x_{453};x_{452},x_{451})\}\) to fit a model of the form

\[x_{t}=\phi_{0}+\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+w_{t}\]

for \(t=3,4,\ldots,453\). The estimates and standard errors (in parentheses) are \(\hat{\phi}_{0}=6.74_{(1.11)}\), \(\hat{\phi}_{1}=1.35_{(.04)}\), \(\hat{\phi}_{2}=-.46_{(.04)}\), and \(\hat{\sigma}_{w}^{2}=89.72\).

The following R code can be used for this analysis. We use acf2 from assta to print and plot the ACF and PACF.

acf2(rec, 48) # will produce values and a graphic (regr = ar.ols(rec, order=2, demean=FALSE, intercept=TRUE)) regr$asy.se.coef # standard errors of the estimates

### Forecasting

In forecasting, the goal is to predict future values of a time series, \(x_{n+m}\), \(m=1,2,\ldots\), based on the data collected to the present, \(x_{1:n}=\{x_{1},x_{2},\ldots,x_{n}\}\). Throughout this section, we will assume \(x_{t}\) is stationary and the model parameters are known. The problem of forecasting when the model parameters are unknown will be discussed in the next section; also, see Problem 3.26. The minimum mean square error predictor of \(x_{n+m}\) is

\[x_{n+m}^{n}=\mathrm{E}(x_{n+m}\mid x_{1:n}) \tag{3.57}\]

because the conditional expectation minimizes the mean square error

\[\mathrm{E}\left[x_{n+m}-g(x_{1:n})\right]^{2}, \tag{3.58}\]

where \(g(x_{1:n})\) is a function of the observations \(x_{1:n}\); see Problem 3.14.

Figure 3.6: ACF and PACF of the Recruitment series. Note that the lag axes are in terms of season (12 months in this case)

First, we will restrict attention to predictors that are linear functions of the data, that is, predictors of the form

\[x_{n+m}^{n}=\alpha_{0}+\sum_{k=1}^{n}\alpha_{k}x_{k}, \tag{3.59}\]

where \(\alpha_{0}\), \(\alpha_{1},\ldots,\alpha_{n}\) are real numbers. We note that the \(\alpha\)s depend on \(n\) and \(m\), but for now we drop the dependence from the notation. For example, if \(n=m=1\), then \(x_{2}^{1}\) is the one-step-ahead linear forecast of \(x_{2}\) given \(x_{1}\). In terms of (3.59), \(x_{2}^{1}=\alpha_{0}+\alpha_{1}x_{1}\). But if \(n=2\), \(x_{3}^{2}\) is the one-step-ahead linear forecast of \(x_{3}\) given \(x_{1}\) and \(x_{2}\). In terms of (3.59), \(x_{3}^{2}=\alpha_{0}+\alpha_{1}x_{1}+\alpha_{2}x_{2}\), and in general, the \(\alpha\)s in \(x_{2}^{1}\) and \(x_{3}^{2}\) will be different.

Linear predictors of the form (3.59) that minimize the mean square prediction error (3.58) are called _best linear predictors_ (BLPs). As we shall see, linear prediction depends only on the second-order moments of the process, which are easy to estimate from the data. Much of the material in this section is enhanced by the theoretical material presented in Appendix B. For example, Theorem B.3 states that if the process is Gaussian, minimum mean square error predictors and best linear predictors are the same. The following property, which is based on the Projection Theorem, Theorem B.1, is a key result.

**Property 3.3**: **Best Linear Prediction for Stationary Processes**__

_Given data \(x_{1},\ldots,x_{n}\), the best linear predictor, \(x_{n+m}^{n}=\alpha_{0}+\sum_{k=1}^{n}\alpha_{k}x_{k}\), of \(x_{n+m}\), for \(m\geq 1\), is found by solving_

\[\mathrm{E}\left[\left(x_{n+m}-x_{n+m}^{n}\right)x_{k}\right]=0,\quad k=0,1, \ldots,n, \tag{3.60}\]

_where \(x_{0}=1\), for \(\alpha_{0},\alpha_{1},\ldots\alpha_{n}\)._

The equations specified in (3.60) are called the _prediction equations_, and they are used to solve for the coefficients \(\{\alpha_{0},\alpha_{1},\ldots,\alpha_{n}\}\). The results of Property 3.3 can also be obtained via least squares; i.e., to minimize \(Q=\mathrm{E}(x_{n+m}-\sum_{k=0}^{n}\alpha_{k}x_{k})^{2}\) with respect to the \(\alpha\)s, solve \(\partial Q/\partial\alpha_{j}=0\) for the \(\alpha_{j}\), \(j=0,1,\ldots,n\). This leads to (3.60).

If \(\mathrm{E}(x_{t})=\mu\), the first equation (\(k=0\)) of (3.60) implies

\[\mathrm{E}(x_{n+m}^{n})=\mathrm{E}(x_{n+m})=\mu.\]

Thus, taking expectation in (3.59), we have

\[\mu=\alpha_{0}+\sum_{k=1}^{n}\alpha_{k}\,\mu\qquad\text{ or }\qquad\alpha_{0}= \mu\Big{(}1-\sum_{k=1}^{n}\alpha_{k}\Big{)}.\]

Hence, the form of the BLP is

\[x_{n+m}^{n}=\mu+\sum_{k=1}^{n}\alpha_{k}(x_{k}-\mu).\]

Thus, until we discuss estimation, there is no loss of generality in considering the case that \(\mu=0\), in which case, \(\alpha_{0}=0\).

First, consider _one-step-ahead prediction_. That is, given \(\{x_{1},\ldots,x_{n}\}\), we wish to forecast the value of the time series at the next time point, \(x_{n+1}\). The BLP of \(x_{n+1}\) is of the form

\[x_{n+1}^{n}=\phi_{n1}x_{n}+\phi_{n2}x_{n-1}+\cdots+\phi_{nn}x_{1}, \tag{3.61}\]

where we now display the dependence of the coefficients on \(n\); in this case, \(\alpha_{k}\) in (3.59) is \(\phi_{n,n+1-k}\) in (3.61), for \(k=1,\ldots,n\). Using Property 3.3, the coefficients \(\{\phi_{n1},\phi_{n2},\ldots,\phi_{nn}\}\) satisfy

\[\mathrm{E}\Big{[}\Big{(}x_{n+1}-\sum_{j=1}^{n}\phi_{nj}x_{n+1-j}\Big{)}x_{n+1- k}\Big{]}=0,\quad k=1,\ldots,n,\]

or

\[\sum_{j=1}^{n}\phi_{nj}\gamma(k-j)=\gamma(k),\quad k=1,\ldots,n. \tag{3.62}\]

The prediction equations (3.62) can be written in matrix notation as

\[\Gamma_{n}\phi_{n}=\gamma_{n}, \tag{3.63}\]

where \(\Gamma_{n}=\{\gamma(k-j)\}_{j,k=1}^{n}\) is an \(n\times n\) matrix, \(\phi_{n}=(\phi_{n1},\ldots,\phi_{nn})^{\prime}\) is an \(n\times 1\) vector, and \(\gamma_{n}=(\gamma(1),\ldots,\gamma(n))^{\prime}\) is an \(n\times 1\) vector.

The matrix \(\Gamma_{n}\) is nonnegative definite. If \(\Gamma_{n}\) is singular, there are many solutions to (3.63), but, by the Projection Theorem (Theorem B.1), \(x_{n+1}^{n}\) is unique. If \(\Gamma_{n}\) is nonsingular, the elements of \(\phi_{n}\) are unique, and are given by

\[\phi_{n}=\Gamma_{n}^{-1}\gamma_{n}. \tag{3.64}\]

For ARMA models, the fact that \(\sigma_{w}^{2}>0\) and \(\gamma(h)\to 0\) as \(h\to\infty\) is enough to ensure that \(\Gamma_{n}\) is positive definite (Problem 3.12). It is sometimes convenient to write the one-step-ahead forecast in vector notation

\[x_{n+1}^{n}=\phi_{n}^{\prime}x, \tag{3.65}\]

where \(x=(x_{n},x_{n-1},\ldots,x_{1})^{\prime}\).

The _mean square one-step-ahead prediction error_ is

\[P_{n+1}^{n}=\mathrm{E}(x_{n+1}-x_{n+1}^{n})^{2}=\gamma(0)-\gamma_{n}^{\prime} \Gamma_{n}^{-1}\gamma_{n}. \tag{3.66}\]

To verify (3.66) using (3.64) and (3.65),

\[\mathrm{E}(x_{n+1}-x_{n+1}^{n})^{2} =\mathrm{E}(x_{n+1}-\phi_{n}^{\prime}x)^{2}=\mathrm{E}(x_{n+1}- \gamma_{n}^{\prime}\Gamma_{n}^{-1}x)^{2}\] \[=\mathrm{E}(x_{n+1}^{2}-2\gamma_{n}^{\prime}\Gamma_{n}^{-1}x_{n+ 1}+\gamma_{n}^{\prime}\Gamma_{n}^{-1}xx^{\prime}\Gamma_{n}^{-1}\gamma_{n})\] \[=\gamma(0)-2\gamma_{n}^{\prime}\Gamma_{n}^{-1}\gamma_{n}+\gamma_{ n}^{\prime}\Gamma_{n}^{-1}\Gamma_{n}\Gamma_{n}^{-1}\gamma_{n}\] \[=\gamma(0)-\gamma_{n}^{\prime}\Gamma_{n}^{-1}\gamma_{n}.\]

**Example 3.19**: **Prediction for an AR(2)**

Suppose we have a causal AR(2) process \(x_{t}=\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+w_{t}\), and one observation \(x_{1}\). Then, using equation (3.64), the one-step-ahead prediction of \(x_{2}\) based on \(x_{1}\) is

\[x_{2}^{1}=\phi_{11}x_{1}=\frac{\gamma(1)}{\gamma(0)}x_{1}=\rho(1)x_{1}.\]

Now, suppose we want the one-step-ahead prediction of \(x_{3}\) based on two observations \(x_{1}\) and \(x_{2}\); i.e., \(x_{3}^{2}=\phi_{21}x_{2}+\phi_{22}x_{1}\). We could use (3.62)

\[\phi_{21}\gamma(0)+\phi_{22}\gamma(1) =\gamma(1)\] \[\phi_{21}\gamma(1)+\phi_{22}\gamma(0) =\gamma(2)\]

to solve for \(\phi_{21}\) and \(\phi_{22}\), or use the matrix form in (3.64) and solve

\[\begin{pmatrix}\phi_{21}\\ \phi_{22}\end{pmatrix}=\begin{pmatrix}\gamma(0)&\gamma(1)\\ \gamma(1)&\gamma(0)\end{pmatrix}^{-1}\begin{pmatrix}\gamma(1)\\ \gamma(2)\end{pmatrix},\]

but, it should be apparent from the model that \(x_{3}^{2}=\phi_{1}x_{2}+\phi_{2}x_{1}\). Because \(\phi_{1}x_{2}+\phi_{2}x_{1}\) satisfies the prediction equations (3.60),

\[\mathrm{E}\{[x_{3}-(\phi_{1}x_{2}+\phi_{2}x_{1})]x_{1}\} =\mathrm{E}(w_{3}x_{1})=0,\] \[\mathrm{E}\{[x_{3}-(\phi_{1}x_{2}+\phi_{2}x_{1})]x_{2}\} =\mathrm{E}(w_{3}x_{2})=0,\]

it follows that, indeed, \(x_{3}^{2}=\phi_{1}x_{2}+\phi_{2}x_{1}\), and by the uniqueness of the coefficients in this case, that \(\phi_{21}=\phi_{1}\) and \(\phi_{22}=\phi_{2}\). Continuing in this way, it is easy to verify that, for \(n\geq 2\),

\[x_{n+1}^{n}=\phi_{1}x_{n}+\phi_{2}x_{n-1}.\]

That is, \(\phi_{n1}=\phi_{1},\phi_{n2}=\phi_{2}\), and \(\phi_{nj}=0\), for \(j=3,4,\ldots,n\).

From Example 3.19, it should be clear (Problem 3.45) that, if the time series is a causal AR(\(p\)) process, then, for \(n\geq p\),

\[x_{n+1}^{n}=\phi_{1}x_{n}+\phi_{2}x_{n-1}+\cdots+\phi_{p}x_{n-p+1}. \tag{3.67}\]

For ARMA models in general, the prediction equations will not be as simple as the pure AR case. In addition, for \(n\) large, the use of (3.64) is prohibitive because it requires the inversion of a large matrix. There are, however, iterative solutions that do not require any matrix inversion. In particular, we mention the recursive solution due to Levinson [127] and Durbin [54].

**Property 3.4**: **The Durbin-Levinson Algorithm**__

_Equations (3.64) and (3.66) can be solved iteratively as follows:_

\[\phi_{00}=0,\quad P_{1}^{0}=\gamma(0). \tag{3.68}\]_For \(n\geq 1\),_

\[\phi_{nn}=\frac{\rho(n)-\sum_{k=1}^{n-1}\phi_{n-1,k}\;\rho(n-k)}{1-\sum_{k=1}^{n-1 }\phi_{n-1,k}\;\rho(k)},\quad P_{n+1}^{n}=P_{n}^{n-1}(1-\phi_{nn}^{2}), \tag{3.69}\]

_where, for \(n\geq 2\),_

\[\phi_{nk}=\phi_{n-1,k}-\phi_{nn}\phi_{n-1,n-k},\quad k=1,2,\ldots,n-1. \tag{3.70}\]

The proof of Property 3.4 is left as an exercise; see Problem 3.13.

**Example 3.20**: **Using the Durbin-Levinson Algorithm**

To use the algorithm, start with \(\phi_{00}=0\), \(P_{1}^{0}=\gamma(0)\). Then, for \(n=1\),

\[\phi_{11}=\rho(1),\quad P_{2}^{1}=\gamma(0)[1-\phi_{11}^{2}].\]

For \(n=2\),

\[\phi_{22}=\frac{\rho(2)-\phi_{11}\;\rho(1)}{1-\phi_{11}\;\rho(1)},\;\phi_{21}=\phi_{11}-\phi_{22}\phi_{11},\] \[P_{3}^{2}=P_{2}^{1}[1-\phi_{22}^{2}]=\gamma(0)[1-\phi_{11}^{2}][ 1-\phi_{22}^{2}].\]

For \(n=3\),

\[\phi_{33}=\frac{\rho(3)-\phi_{21}\;\rho(2)-\phi_{22}\;\rho(1)}{1- \phi_{21}\;\rho(1)-\phi_{22}\;\rho(2)},\] \[\phi_{32}=\phi_{22}-\phi_{33}\phi_{21},\;\phi_{31}=\phi_{21}-\phi _{33}\phi_{22},\] \[P_{4}^{3}=P_{3}^{2}[1-\phi_{33}^{2}]=\gamma(0)[1-\phi_{11}^{2}][ 1-\phi_{22}^{2}][1-\phi_{33}^{2}],\]

and so on. Note that, in general, the standard error of the one-step-ahead forecast is the square root of

\[P_{n+1}^{n}=\gamma(0)\prod_{j=1}^{n}[1-\phi_{jj}^{2}]. \tag{3.71}\]

An important consequence of the Durbin-Levinson algorithm is (see Problem 3.13) as follows.

**Property 3.5**: **Iterative Solution for the PACF**

_The PACF of a stationary process \(x_{t}\), can be obtained iteratively via (3.69) as \(\phi_{nn}\), for \(n=1,2,\ldots\)._

Using Property 3.5 and putting \(n=p\) in (3.61) and (3.67), it follows that for an AR(\(p\)) model,

\[x_{p+1}^{p} =\phi_{p1}\,x_{p}+\phi_{p2}\,x_{p-1}+\cdots+\phi_{pp}\,x_{1} \tag{3.72}\] \[=\phi_{1}\,x_{p}+\phi_{2}\,x_{p-1}+\cdots+\phi_{p}\,x_{1}.\]

Result (3.72) shows that for an AR(\(p\)) model, the partial autocorrelation coefficient at lag \(p\), \(\phi_{pp}\), is also the last coefficient in the model, \(\phi_{p}\), as was claimed in Example 3.16.

**Example 3.21**: **The PACF of an AR(2)**

We will use the results of Example 3.20 and Property 3.5 to calculate the first three values, \(\phi_{11},\ \phi_{22},\ \phi_{33}\), of the PACF. Recall from Example 3.10 that \(\rho(h)-\phi_{1}\rho(h-1)-\phi_{2}\rho(h-2)=0\) for \(h\geq 1\). When \(h=1,2,3\), we have \(\rho(1)=\phi_{1}/(1-\phi_{2}),\ \rho(2)=\phi_{1}\rho(1)+\phi_{2},\ \rho(3)-\phi_{1} \rho(2)-\phi_{2}\rho(1)=0\). Thus,

\[\phi_{11} =\rho(1)=\frac{\phi_{1}}{1-\phi_{2}}\] \[\phi_{22} =\frac{\rho(2)-\rho(1)^{2}}{1-\rho(1)^{2}}=\frac{\left[\phi_{1} \left(\frac{\phi_{1}}{1-\phi_{2}}\right)+\phi_{2}\right]-\left(\frac{\phi_{1}} {1-\phi_{2}}\right)^{2}}{1-\left(\frac{\phi_{1}}{1-\phi_{2}}\right)^{2}}=\phi _{2}\] \[\phi_{21} =\rho(1)[1-\phi_{2}]=\phi_{1}\] \[\phi_{33} =\frac{\rho(3)-\phi_{1}\rho(2)-\phi_{2}\rho(1)}{1-\phi_{1}\rho(1) -\phi_{2}\rho(2)}=0.\]

Notice that, as shown in (3.72), \(\phi_{22}=\phi_{2}\) for an AR(2) model.

So far, we have concentrated on one-step-ahead prediction, but Property 3.3 allows us to calculate the BLP of \(x_{n+m}\) for any \(m\geq 1\). Given data, \(\{x_{1},\ldots,x_{n}\}\), the \(m\)-step-ahead predictor is

\[x_{n+m}^{n}=\phi_{n1}^{(m)}x_{n}+\phi_{n2}^{(m)}x_{n-1}+\cdots+\phi_{nn}^{(m)}x _{1}, \tag{3.73}\]

where \(\{\phi_{n1}^{(m)},\phi_{n2}^{(m)},\ldots,\phi_{nn}^{(m)}\}\) satisfy the prediction equations,

\[\sum_{j=1}^{n}\phi_{nj}^{(m)}\mathrm{E}(x_{n+1-j}x_{n+1-k})=\mathrm{E}(x_{n+m} x_{n+1-k}),\quad k=1,\ldots,n,\]

or

\[\sum_{j=1}^{n}\phi_{nj}^{(m)}\ \gamma(k-j)=\gamma(m+k-1),\quad k=1,\ldots,n. \tag{3.74}\]

The prediction equations can again be written in matrix notation as

\[\Gamma_{n}\phi_{n}^{(m)}=\gamma_{n}^{(m)}, \tag{3.75}\]

where \(\gamma_{n}^{(m)}=(\gamma(m),\ldots,\gamma(m+n-1))^{\prime}\), and \(\phi_{n}^{(m)}=(\phi_{n1}^{(m)},\ldots,\phi_{nn}^{(m)})^{\prime}\) are \(n\times 1\) vectors. The _mean square \(m\)-step-ahead prediction error_ is

\[P_{n+m}^{n}=\mathrm{E}\left(x_{n+m}-x_{n+m}^{n}\right)^{2}=\gamma(0)-\gamma_{n }^{(m)^{\prime}}\Gamma_{n}^{-1}\gamma_{n}^{(m)}. \tag{3.76}\]

Another useful algorithm for calculating forecasts was given by Brockwell and Davis [36, Chapter 5]. This algorithm follows directly from applying the projection theorem (Theorem B.1) to the _innovations_, \(x_{t}-x_{t}^{t-1}\), for \(t=1,\ldots,n\), using the fact that the innovations \(x_{t}-x_{t}^{t-1}\) and \(x_{s}-x_{s}^{s-1}\) are uncorrelated for \(s\neq t\) (see Problem 3.46). We present the case in which \(x_{t}\) is a mean-zero stationary time series.

**Property 3.6**: **The Innovations Algorithm**__

_The one-step-ahead predictors, \(x_{t+1}^{t}\), and their mean-squared errors, \(P_{t+1}^{t}\), can be calculated iteratively as_

\[x_{1}^{0}=0,\quad P_{1}^{0}=\gamma(0)\]

\[x_{t+1}^{t}=\sum_{j=1}^{t}\theta_{tj}(x_{t+1-j}-x_{t+1-j}^{t-j}),\quad t=1,2,\ldots \tag{3.77}\]

\[P_{t+1}^{t}=\gamma(0)-\sum_{j=0}^{t-1}\theta_{t,t-j}^{2}P_{j+1}^{j}\quad t=1,2,\ldots, \tag{3.78}\]

_where, for \(j=0,1,\ldots,t-1\),_

\[\theta_{t,t-j}=\left(\gamma(t-j)-\sum_{k=0}^{j-1}\theta_{j,j-k}\theta_{t,t-k}P _{k+1}^{k}\right)\,\big{/}\,\,P_{j+1}^{j}. \tag{3.79}\]

Given data \(x_{1},\ldots,x_{n}\), the innovations algorithm can be calculated successively for \(t=1\), then \(t=2\) and so on, in which case the calculation of \(x_{n+1}^{n}\) and \(P_{n+1}^{n}\) is made at the final step \(t=n\). The \(m\)-step-ahead predictor and its mean-square error based on the innovations algorithm (Problem 3.46) are given by

\[x_{n+m}^{n}=\sum_{j=m}^{n+m-1}\theta_{n+m-1,j}(x_{n+m-j}-x_{n+m-j}^{n+m-j-1}), \tag{3.80}\]

\[P_{n+m}^{n}=\gamma(0)-\sum_{j=m}^{n+m-1}\theta_{n+m-1,j}^{2}P_{n+m-j}^{n+m-j-1}, \tag{3.81}\]

where the \(\theta_{n+m-1,j}\) are obtained by continued iteration of (3.79).

**Example 3.22**: **Prediction for an MA(1)**__

The innovations algorithm lends itself well to prediction for moving average processes. Consider an MA(1) model, \(x_{t}=w_{t}+\theta w_{t-1}\). Recall that \(\gamma(0)=(1+\theta^{2})\sigma_{w}^{2}\), \(\gamma(1)=\theta\sigma_{w}^{2}\), and \(\gamma(h)=0\) for \(h>1\). Then, using Property 3.6, we have

\[\theta_{n1} =\theta\sigma_{w}^{2}/P_{n}^{n-1}\] \[\theta_{nj} =0,\quad j=2,\ldots,n\] \[P_{1}^{0} =(1+\theta^{2})\sigma_{w}^{2}\] \[P_{n+1}^{n} =(1+\theta^{2}-\theta\theta_{n1})\sigma_{w}^{2}.\]

Finally, from (3.77), the one-step-ahead predictor is

\[x_{n+1}^{n}=\theta\left(x_{n}-x_{n}^{n-1}\right)\sigma_{w}^{2}/P_{n}^{n-1}.\]

### Forecasting ARMA Processes

The general prediction equations (3.60) provide little insight into forecasting for ARMA models in general. There are a number of different ways to express these forecasts, and each aids in understanding the special structure of ARMA prediction. Throughout, we assume \(x_{t}\) is a causal and invertible ARMA(\(p,q\)) process, \(\phi(B)x_{t}=\theta(B)w_{t}\), where \(w_{t}\sim\text{iid N}(0,\sigma_{w}^{2})\). In the non-zero mean case, \(\text{E}(x_{t})=\mu_{x}\), simply replace \(x_{t}\) with \(x_{t}-\mu_{x}\) in the model. First, we consider two types of forecasts. We write \(x_{n+m}^{n}\) to mean the minimum mean square error predictor of \(x_{n+m}\) based on the data \(\{x_{n},\ldots,x_{1}\}\), that is,

\[x_{n+m}^{n}=\text{E}(x_{n+m}\bigm{|}x_{n},\ldots,x_{1}).\]

For ARMA models, it is easier to calculate the predictor of \(x_{n+m}\), assuming we have the complete history of the process \(\{x_{n},x_{n-1},\ldots,x_{1},x_{0},x_{-1},\ldots\}\). We will denote the predictor of \(x_{n+m}\)_based on the infinite past_ as

\[\tilde{x}_{n+m}=\text{E}(x_{n+m}\bigm{|}x_{n},x_{n-1},\ldots,x_{1},x_{0},x_{-1 },\ldots).\]

In general, \(x_{n+m}^{n}\) and \(\tilde{x}_{n+m}\) are not the same, but the idea here is that, for large samples, \(\tilde{x}_{n+m}\) will provide a good approximation to \(x_{n+m}^{n}\).

Now, write \(x_{n+m}\) in its causal and invertible forms:

\[x_{n+m}=\sum_{j=0}^{\infty}\psi_{j}w_{n+m-j},\quad\psi_{0}=1 \tag{3.82}\]

\[w_{n+m}=\sum_{j=0}^{\infty}\pi_{j}x_{n+m-j},\quad\pi_{0}=1. \tag{3.83}\]

Then, taking conditional expectations in (3.82), we have

\[\tilde{x}_{n+m}=\sum_{j=0}^{\infty}\psi_{j}\tilde{w}_{n+m-j}=\sum_{j=m}^{\infty }\psi_{j}w_{n+m-j}, \tag{3.84}\]

because, by causality and invertibility,

\[\tilde{w}_{t}=\text{E}(w_{t}\bigm{|}x_{n},x_{n-1},\ldots,x_{0},x_{-1},\ldots)= \begin{cases}0&t>n\\ w_{t}&t\leq n.\end{cases}\]

Similarly, taking conditional expectations in (3.83), we have

\[0=\tilde{x}_{n+m}+\sum_{j=1}^{\infty}\pi_{j}\tilde{x}_{n+m-j},\]

or

\[\tilde{x}_{n+m}=-\sum_{j=1}^{m-1}\pi_{j}\tilde{x}_{n+m-j}-\sum_{j=m}^{\infty} \pi_{j}x_{n+m-j}, \tag{3.85}\]using the fact \(\mathrm{E}(x_{t}\ \big{|}\ x_{n},x_{n-1},\ldots,x_{0},x_{-1},\ldots)=x_{t}\), for \(t\leq n\). Prediction is accomplished recursively using (3.85), starting with the one-step-ahead predictor, \(m=1\), and then continuing for \(m=2,3,\ldots\). Using (3.84), we can write

\[x_{n+m}-\tilde{x}_{n+m}=\sum_{j=0}^{m-1}\psi_{j}w_{n+m-j},\]

so the _mean-square prediction error_ can be written as

\[P_{n+m}^{n}=\mathrm{E}(x_{n+m}-\tilde{x}_{n+m})^{2}=\sigma_{w}^{2}\sum_{j=0}^{m -1}\psi_{j}^{2}. \tag{3.86}\]

Also, we note, for a fixed sample size, \(n\), the prediction errors are correlated. That is, for \(k\geq 1\),

\[\mathrm{E}\{(x_{n+m}-\tilde{x}_{n+m})(x_{n+m+k}-\tilde{x}_{n+m+k})\}=\sigma_{w }^{2}\sum_{j=0}^{m-1}\psi_{j}\psi_{j+k}. \tag{3.87}\]

**Example 3.23**: **Long-Range Forecasts**

Consider forecasting an ARMA process with mean \(\mu_{x}\). Replacing \(x_{n+m}\) with \(x_{n+m}-\mu_{x}\) in (3.82), and taking conditional expectation as in (3.84), we deduce that the \(m\)-step-ahead forecast can be written as

\[\tilde{x}_{n+m}=\mu_{x}+\sum_{j=m}^{\infty}\psi_{j}w_{n+m-j}. \tag{3.88}\]

Noting that the \(\psi\)-weights dampen to zero exponentially fast, it is clear that

\[\tilde{x}_{n+m}\rightarrow\mu_{x} \tag{3.89}\]

exponentially fast (in the mean square sense) as \(m\rightarrow\infty\). Moreover, by (3.86), the mean square prediction error

\[P_{n+m}^{n}\rightarrow\sigma_{w}^{2}\sum_{j=0}^{\infty}\psi_{j}^{2}=\gamma_{x }(0)=\sigma_{x}^{2}, \tag{3.90}\]

exponentially fast as \(m\rightarrow\infty\).

It should be clear from (3.89) and (3.90) that ARMA forecasts quickly settle to the mean with a constant prediction error as the forecast horizon, \(m\), grows. This effect can be seen in Fig. 3.7 where the Recruitment series is forecast for 24 months; see Example 3.25.

When \(n\) is small, the general prediction equations (3.60) can be used easily. When \(n\) is large, we would use (3.85) by truncating, because we do not observe \(x_{0},x_{-1},x_{-2},\ldots\), and only the data \(x_{1},x_{2},\ldots,x_{n}\) are available. In this case, we can truncate (3.85) by setting \(\sum_{j=n+m}^{\infty}\pi_{j}x_{n+m-j}=0\). The _truncated predictor_ is then written as

\[\tilde{x}_{n+m}^{n}=-\sum_{j=1}^{m-1}\pi_{j}\tilde{x}_{n+m-j}^{n}-\sum_{j=m}^{n+ m-1}\pi_{j}x_{n+m-j}, \tag{3.91}\]

which is also calculated recursively, \(m=1,2,\ldots\). The mean square prediction error, in this case, is approximated using (3.86).

For AR(\(p\)) models, and when \(n>p\), equation (3.67) yields the exact predictor, \(x_{n+m}^{n}\), of \(x_{n+m}\), and there is no need for approximations. That is, for \(n>p\), \(\tilde{x}_{n+m}^{n}=\tilde{x}_{n+m}=x_{n+m}^{n}\). Also, in this case, the one-step-ahead prediction error is \(\mathrm{E}(x_{n+1}-x_{n+1}^{n})^{2}=\sigma_{w}^{2}\). For pure MA(\(q\)) or ARMA(\(p,q\)) models, truncated prediction has a fairly nice form.

**Property 3.7**: **Truncated Prediction for ARMA**__

_For ARMA(\(p,q\)) models, the truncated predictors for \(m=1,2,\ldots,\) are_

\[\tilde{x}_{n+m}^{n}=\phi_{1}\tilde{x}_{n+m-1}^{n}+\cdots+\phi_{p}\tilde{x}_{n+m -p}^{n}+\theta_{1}\tilde{w}_{n+m-1}^{n}+\cdots+\theta_{q}\tilde{w}_{n+m-q}^{n}, \tag{3.92}\]

_where \(\tilde{x}_{t}^{n}=x_{t}\) for \(1\leq t\leq n\) and \(\tilde{x}_{t}^{n}=0\) for \(t\leq 0\). The truncated prediction errors are given by: \(\tilde{w}_{t}^{n}=0\) for \(t\leq 0\) or \(t>n\), and_

\[\tilde{w}_{t}^{n}=\phi(B)\tilde{x}_{t}^{n}-\theta_{1}\tilde{w}_{t-1}^{n}-\cdots -\theta_{q}\tilde{w}_{t-q}^{n}\]

_for \(1\leq t\leq n\)._

**Example 3.24**: **Forecasting an ARMA\((1,1)\) Series**__

Given data \(x_{1},\ldots,x_{n}\), for forecasting purposes, write the model as

\[x_{n+1}=\phi x_{n}+w_{n+1}+\theta w_{n}.\]

Then, based on (3.92), the one-step-ahead truncated forecast is

\[\tilde{x}_{n+1}^{n}=\phi x_{n}+0+\theta\tilde{w}_{n}^{n}.\]

For \(m\geq 2\), we have

\[\tilde{x}_{n+m}^{n}=\phi\tilde{x}_{n+m-1}^{n},\]

which can be calculated recursively, \(m=2,3,\ldots\).

To calculate \(\tilde{w}_{n}^{n}\), which is needed to initialize the successive forecasts, the model can be written as \(w_{t}=x_{t}-\phi x_{t-1}-\theta w_{t-1}\) for \(t=1,\ldots,n\). For truncated forecasting using (3.92), put \(\tilde{w}_{0}^{n}=0\), \(x_{0}=0\), and then iterate the errors forward in time

\[\tilde{w}_{t}^{n}=x_{t}-\phi x_{t-1}-\theta\tilde{w}_{t-1}^{n},\quad t=1, \ldots,n.\]

The approximate forecast variance is computed from (3.86) using the \(\psi\)-weights determined as in Example 3.12. In particular, the \(\psi\)-weights satisfy \(\psi_{j}=(\phi+\theta)\phi^{j-1}\), for \(j\geq 1\). This result gives

\[P_{n+m}^{n}=\sigma_{w}^{2}\left[1+(\phi+\theta)^{2}\sum_{j=1}^{m-1}\phi^{2(j-1 )}\right]=\sigma_{w}^{2}\left[1+\frac{(\phi+\theta)^{2}(1-\phi^{2(m-1)})}{(1- \phi^{2})}\right].\]To assess the precision of the forecasts, _prediction intervals_ are typically calculated along with the forecasts. In general, \((1-\alpha)\) prediction intervals are of the form

\[x_{n+m}^{n}\pm c_{\frac{\alpha}{2}}\sqrt{P_{n+m}^{n}}, \tag{3.93}\]

where \(c_{\alpha/2}\) is chosen to get the desired degree of confidence. For example, if the process is Gaussian, then choosing \(c_{\alpha/2}=2\) will yield an approximate 95% prediction interval for \(x_{n+m}\). If we are interested in establishing prediction intervals over more than one time period, then \(c_{\alpha/2}\) should be adjusted appropriately, for example, by using Bonferroni's inequality [see (4.63) in Chapter 4 or Johnson and Wichern, 1992, Chapter 5][106].

**Example 3.25**: _Forecasting the Recruitment Series_

Using the parameter estimates as the actual parameter values, Fig. 3.7 shows the result of forecasting the Recruitment series given in Example 3.18 over a 24-month horizon, \(m=1,2,\ldots,24\). The actual forecasts are calculated as

\[x_{n+m}^{n}=6.74+1.35x_{n+m-1}^{n}-.46x_{n+m-2}^{n}\]

for \(n=453\) and \(m=1,2,\ldots,12\). Recall that \(x_{t}^{s}=x_{t}\) when \(t\leq s\). The forecasts errors \(P_{n+m}^{n}\) are calculated using (3.86). Recall that \(\hat{\sigma}_{w}^{2}=89.72\), and using (3.40) from Example 3.12, we have \(\psi_{j}=1.35\psi_{j-1}-.46\psi_{j-2}\) for \(j\geq 2\), where \(\psi_{0}=1\) and \(\psi_{1}=1.35\). Thus, for \(n=453\),

\[\begin{array}{l}P_{n+1}^{n}=89.72,\\ P_{n+2}^{n}=89.72(1+1.35^{2}),\\ P_{n+3}^{n}=89.72(1+1.35^{2}+[1.35^{2}-.46]^{2}),\end{array}\]

and so on.

Figure 3.7: Twenty-four month forecasts for the Recruitment series. The actual data shown are from about January 1980 to September 1987, and then the forecasts plus and minus one standard error are displayed

Note how the forecast levels off quickly and the prediction intervals are wide, even though in this case the forecast limits are only based on one standard error; that is, \(x_{n+m}^{n}\pm\sqrt{P_{n+m}^{n}}\).

To reproduce the analysis and Fig. 3.7, use the following commands:

```
regr=ar.ols(rec,order=2,demean-FALSE,intercept=TRUE) fore=predict(regr,n.ahead=24) ts.plot(rec,fore$pred,col=1:2,xlim=c(1980,1990),ylab="Recruitment") U=fore$pred+fore$se;L=fore$pred-fore$se xx=c(time(U),rev(time(U)));yy=c(L,rev(U)) polygon(xx,yy,border=8,col=gray(.6,alpha=.2)) lines(fore$pred,type="p",col=2)
```

We complete this section with a brief discussion of _backcasting_. In backcasting, we want to predict \(x_{1-m}\), for \(m=1,2,\ldots\), based on the data \(\{x_{1},\ldots,x_{n}\}\). Write the backcast as

\[x_{1-m}^{n}=\sum_{j=1}^{n}\alpha_{j}x_{j}. \tag{3.94}\]

Analogous to (3.74), the prediction equations (assuming \(\mu_{x}=0\)) are

\[\sum_{j=1}^{n}\alpha_{j}\mathrm{E}(x_{j}x_{k})=\mathrm{E}(x_{1-m}x_{k}),\quad k =1,\ldots,n, \tag{3.95}\]

or

\[\sum_{j=1}^{n}\alpha_{j}\gamma(k-j)=\gamma(m+k-1),\quad k=1,\ldots,n. \tag{3.96}\]

These equations are precisely the prediction equations for forward prediction. That is, \(\alpha_{j}\equiv\phi_{nj}^{(m)}\), for \(j=1,\ldots,n\), where the \(\phi_{nj}^{(m)}\) are given by (3.75). Finally, the backcasts are given by

\[x_{1-m}^{n}=\phi_{n1}^{(m)}x_{1}+\cdots+\phi_{nn}^{(m)}x_{n},\quad m=1,2,\ldots. \tag{3.97}\]

**Example 3.26**: Backcasting an \(\mathbf{ARMA}(1,1)\)

Consider an \(\mathbf{ARMA}(1,1)\) process, \(x_{t}=\phi x_{t-1}+\theta w_{t-1}+w_{t}\); we will call this the _forward model_. We have just seen that best linear prediction backward in time is the same as best linear prediction forward in time for stationary models. Assuming the models are Gaussian, we also have that minimum mean square error prediction backward in time is the same as forward in time for ARMA models.4 Thus, the process can equivalently be generated by the _backward model_,

Footnote 4: In the stationary Gaussian case, (a) the distribution of \(\{x_{n+1},x_{n},\ldots,x_{1}\}\) is the same as (b) the distribution of \(\{x_{0},x_{1}\ldots,x_{n}\}\). In forecasting we use (a) to obtain \(\mathrm{E}(x_{n+1}\mid x_{n},\ldots,x_{1})\); in backcasting we use (b) to obtain \(\mathrm{E}(x_{0}\mid x_{1},\ldots,x_{n})\). Because (a) and (b) are the same, the two problems are equivalent.

\[x_{t}=\phi x_{t+1}+\theta v_{t+1}+v_{t},\]where \(\{v_{t}\}\) is a Gaussian white noise process with variance \(\sigma_{w}^{2}\). We may write \(x_{t}=\sum_{j=0}^{\infty}\psi_{j}v_{t+j}\), where \(\psi_{0}=1\); this means that \(x_{t}\) is uncorrelated with \(\{v_{t-1},v_{t-2},\ldots\}\), in analogy to the forward model.

Given data \(\{x_{1},\ldots,x_{n}\}\), truncate \(v_{n}^{n}=\operatorname{E}(v_{n}\,|\,x_{1},\ldots,x_{n})\) to zero and then iterate backward. That is, put \(\tilde{v}_{n}^{n}=0\), as an initial approximation, and then generate the errors backward

\[\tilde{v}_{t}^{n}=x_{t}-\phi x_{t+1}-\theta\tilde{v}_{t+1}^{n},\quad t=(n-1),(n- 2),\ldots,1.\]

Then,

\[\tilde{x}_{0}^{n}=\phi x_{1}+\theta\tilde{v}_{1}^{n}+\tilde{v}_{0}^{n}=\phi x_{ 1}+\theta\tilde{v}_{1}^{n},\]

because \(\tilde{v}_{t}^{n}=0\) for \(t\leq 0\). Continuing, the general truncated backcasts are given by

\[\tilde{x}_{1-m}^{n}=\phi\tilde{x}_{2-m}^{n},\quad m=2,3,\ldots\.\]

To backcast data in R, simply reverse the data, fit the model and predict. In the following, we backcasted a simulated ARMA(1,1) process; see Fig. 3.8.

``` set.seed(90210) x=arima.sim(list(order=c(1,0,1),ar=.9,ma=.5),n=100) xr=rev(x)#xristhereverseddata pxr=predict(arima(xr,order=c(1,0,1)),10)#predictthereverseddata pxrp=rev(pxr5pred)#reorderthepredictors(forplotting) pxrse=rev(pxr5se)#reordertheSEs nx=ts(c(pxrp,x),start=-9)#attachthebackcaststothedata plot(nx,ylab=expression(X[-t]),main='Backcasting') U=nx[1:10]+pxrse;L=nx[1:10]-pxrse xx=c(-9:0,0:-9);yy=c(L,rev(U)) polygon(xx,yy,border=8,col=gray(0.6,alpha=0.2)) lines(-9:0,nx[1:10],col=2,type='o')

Figure 3.8: Display for Example 3.26; backcasts from a simulated ARMA(1,1)

### Estimation

Throughout this section, we assume we have \(n\) observations, \(x_{1},\ldots,x_{n}\), from a causal and invertible Gaussian ARMA(\(p,q\)) process in which, initially, the order parameters, \(p\) and \(q\), are known. Our goal is to estimate the parameters, \(\phi_{1},\ldots,\phi_{p}\), \(\theta_{1},\ldots,\theta_{q}\), and \(\sigma_{w}^{2}\). We will discuss the problem of determining \(p\) and \(q\) later in this section.

We begin with _method of moments_ estimators. The idea behind these estimators is that of equating population moments to sample moments and then solving for the parameters in terms of the sample moments. We immediately see that, if \(\mathrm{E}(x_{t})=\mu\), then the method of moments estimator of \(\mu\) is the sample average, \(\bar{x}\). Thus, while discussing method of moments, we will assume \(\mu=0\). Although the method of moments can produce good estimators, they can sometimes lead to suboptimal estimators. We first consider the case in which the method leads to optimal (efficient) estimators, that is, AR(\(p\)) models,

\[x_{t}=\phi_{1}x_{t-1}+\cdots+\phi_{p}x_{t-p}+w_{t},\]

where the first \(p+1\) equations of (3.47) and (3.48) lead to the following:

**Definition 3.10**: _The_ **Yule-Walker equations** _are given by_

\[\gamma(h) =\phi_{1}\gamma(h-1)+\cdots+\phi_{p}\gamma(h-p),\quad h=1,2, \ldots,p, \tag{3.98}\] \[\sigma_{w}^{2} =\gamma(0)-\phi_{1}\gamma(1)-\cdots-\phi_{p}\gamma(p). \tag{3.99}\]

In matrix notation, the Yule-Walker equations are

\[\Gamma_{p}\phi=\gamma_{p},\quad\sigma_{w}^{2}=\gamma(0)-\phi^{\prime}\gamma_{p}, \tag{3.100}\]

where \(\Gamma_{p}=\{\gamma(k-j)\}_{j,k=1}^{p}\) is a \(p\times p\) matrix, \(\phi=(\phi_{1},\ldots,\phi_{p})^{\prime}\) is a \(p\times 1\) vector, and \(\gamma_{p}=(\gamma(1),\ldots,\gamma(p))^{\prime}\) is a \(p\times 1\) vector. Using the method of moments, we replace \(\gamma(h)\) in (3.100) by \(\hat{\gamma}(h)\) [see equation (1.36)] and solve

\[\hat{\phi}=\hat{\Gamma}_{p}^{-1}\hat{\gamma}_{p},\quad\hat{\sigma}_{w}^{2}= \hat{\gamma}(0)-\hat{\gamma}_{p}^{\prime}\hat{\Gamma}_{p}^{-1}\hat{\gamma}_{p}. \tag{3.101}\]

These estimators are typically called the _Yule-Walker estimators_. For calculation purposes, it is sometimes more convenient to work with the sample ACF. By factoring \(\hat{\gamma}(0)\) in (3.101), we can write the Yule-Walker estimates as

\[\hat{\phi}=\hat{R}_{p}^{-1}\hat{\rho}_{p},\quad\hat{\sigma}_{w}^{2}=\hat{ \gamma}(0)\left[1-\hat{\rho}_{p}^{\prime}\hat{R}_{p}^{-1}\hat{\rho}_{p}\right], \tag{3.102}\]

where \(\hat{R}_{p}=\{\hat{\rho}(k-j)\}_{j,k=1}^{p}\) is a \(p\times p\) matrix and \(\hat{\rho}_{p}=(\hat{\rho}(1),\ldots,\hat{\rho}(p))^{\prime}\) is a \(p\times 1\) vector.

For AR(\(p\)) models, if the sample size is large, the Yule-Walker estimators are approximately normally distributed, and \(\hat{\sigma}_{w}^{2}\) is close to the true value of \(\sigma_{w}^{2}\). We state these results in Property 3.8; for details, see Sect. B.3.

**Property 3.8**: **Large Sample Results for Yule-Walker Estimators**__

_The asymptotic (\(n\to\infty\)) behavior of the Yule-Walker estimators in the case of causal AR(p) processes is as follows:_

\[\sqrt{n}\left(\hat{\phi}-\phi\right)\,\stackrel{{ d}}{{\to}}\,N \left(0,\sigma_{w}^{2}\Gamma_{p}^{-1}\right),\qquad\hat{\sigma}_{w}^{2}\, \stackrel{{ p}}{{\to}}\,\sigma_{w}^{2}. \tag{3.103}\]

The Durbin-Levinson algorithm, (3.68)-(3.70), can be used to calculate \(\hat{\phi}\) without inverting \(\hat{I}_{p}\) or \(\hat{R}_{p}\), by replacing \(\gamma(h)\) by \(\hat{\gamma}(h)\) in the algorithm. In running the algorithm, we will iteratively calculate the \(h\times 1\) vector, \(\hat{\phi}_{h}=(\hat{\phi}_{h1},\ldots,\hat{\phi}_{hh})^{\prime}\), for \(h=1,2,\ldots\). Thus, in addition to obtaining the desired forecasts, the Durbin-Levinson algorithm yields \(\hat{\phi}_{hh}\), the sample PACF. Using (3.103), we can show the following property.

**Property 3.9**: **Large Sample Distribution of the PACF**__

_For a causal AR(p) process, asymptotically (\(n\to\infty\)),_

\[\sqrt{n}\;\hat{\phi}_{hh}\,\stackrel{{ d}}{{\to}}\,N\left(0,1 \right),\quad\mbox{for}\quad h>p. \tag{3.104}\]

**Example 3.27**: **Yule-Walker Estimation for an AR(2) Process**__

The data shown in Fig. 3.4 were \(n=144\) simulated observations from the AR(2) model

\[x_{t}=1.5x_{t-1}-.75x_{t-2}+w_{t},\]

where \(w_{t}\sim\mbox{id N}(0,1)\). For these data, \(\hat{\gamma}(0)=8.903\), \(\hat{\rho}(1)=.849\), and \(\hat{\rho}(2)=.519\). Thus,

\[\hat{\phi}=\begin{pmatrix}\hat{\phi}_{1}\\ \hat{\phi}_{2}\end{pmatrix}=\begin{bmatrix}1&.849\\.849&1\end{bmatrix}^{-1}\begin{pmatrix}.849\\.519\end{pmatrix}=\begin{pmatrix}1.463\\ -.723\end{pmatrix}\]

and

\[\hat{\sigma}_{w}^{2}=8.903\,\left[1-(.849,.519)\begin{pmatrix}1.463\\ -.723\end{pmatrix}\right]=1.187.\]

By Property 3.8, the asymptotic variance-covariance matrix of \(\hat{\phi}\) is

\[\frac{1}{144}\frac{1.187}{8.903}\begin{bmatrix}1&.849\\.849&1\end{bmatrix}^{-1}=\begin{bmatrix}.058^{2}&-.003\\ -.003&.058^{2}\end{bmatrix},\]

and it can be used to get confidence regions for, or make inferences about \(\hat{\phi}\) and its components. For example, an approximate \(95\%\) confidence interval for \(\phi_{2}\) is \(-.723\pm 2(.058)\), or \((-.838,-.608)\), which contains the true value of \(\phi_{2}=-.75\).

For these data, the first three sample partial autocorrelations are \(\hat{\phi}_{11}=\hat{\rho}(1)=.849\), \(\hat{\phi}_{22}=\hat{\phi}_{2}=-.721\), and \(\hat{\phi}_{33}=-.085\). According to Property 3.9, the asymptotic standard error of \(\hat{\phi}_{33}\) is \(1/\sqrt{144}=.083\), and the observed value, \(-.085\), is about only one standard deviation from \(\phi_{33}=0\).

**Example 3.28**: **Yule-Walker Estimation of the Recruitment Series**

In Example 3.18 we fit an AR(2) model to the recruitment series using ordinary least squares (OLS). For AR models, the estimators obtained via OLS and Yule-Walker are nearly identical; we will see this when we discuss conditional sum of squares estimation in (3.111)-(3.116).

Below are the results of fitting the same model using Yule-Walker estimation in R, which are nearly identical to the values in Example 3.18.

rec.yw = ar.yw(rec, order=2)

rec.yw$x.mean _#_ = 62.26 (mean estimate)

rec.yw$ar _#_ = 1.33, -.44 (coefficient estimates)

sqrt(diag(rec.yw$asy.var.coef)) _#_ =.04,.04 (standard errors)

rec.yw$var.pred _#_ = 94.80 (error variance estimate)

To obtain the 24 month ahead predictions and their standard errors, and then plot the results (not shown) as in Example 3.25, use the R commands:

rec.pr = predict(rec.yw, n.ahead=24)

ts.plot(rec, rec.pr$pred, col=1:2)

lines(rec.pr$pred + rec.pr$see, col=4, lty=2)

lines(rec.pr$pred - rec.pr$se, col=4, lty=2)

In the case of AR(\(p\)) models, the Yule-Walker estimators given in (3.102) are optimal in the sense that the asymptotic distribution, (3.103), is the best asymptotic normal distribution. This is because, given initial conditions, AR(\(p\)) models are linear models, and the Yule-Walker estimators are essentially least squares estimators. If we use method of moments for MA or ARMA models, we will not get optimal estimators because such processes are nonlinear in the parameters.

**Example 3.29**: **Method of Moments Estimation for an MA(1)**

Consider the time series

\[x_{t}=w_{t}+\theta w_{t-1},\]

where \(|\theta|<1\). The model can then be written as

\[x_{t}=\sum_{j=1}^{\infty}(-\theta)^{j}x_{t-j}+w_{t},\]

which is nonlinear in \(\theta\). The first two population autocovariances are \(\gamma(0)=\sigma_{w}^{2}(1+\theta^{2})\) and \(\gamma(1)=\sigma_{w}^{2}\theta\), so the estimate of \(\theta\) is found by solving:

\[\hat{\rho}(1)=\frac{\hat{\gamma}(1)}{\hat{\gamma}(0)}=\frac{\hat{\theta}}{1+ \hat{\theta}^{2}}.\]

Two solutions exist, so we would pick the invertible one. If \(|\hat{\rho}(1)|\leq\frac{1}{2}\), the solutions are real, otherwise, a real solution does not exist. Even though \(|\rho(1)|<\frac{1}{2}\) for an invertible MA(1), it may happen that \(|\hat{\rho}(1)|\geq\frac{1}{2}\) because it is an estimator. For example, the following simulation in R produces a value of \(\hat{\rho}(1)=.507\) when the true value is \(\rho(1)=.9/(1+.9^{2})=.497\).

set.seed(2)  mal = arima.sim(list(order = c(0,0,1), ma = 0.9), n = 50)  acf(mal, plot=FALSE)[1] # =.507 (lag 1 sample ACF) When \(|\hat{\rho}(1)|<\frac{1}{2}\), the invertible estimate is \[\hat{\theta}=\frac{1-\sqrt{1-4\hat{\rho}(1)^{2}}}{2\hat{\rho}(1)}.\] (3.105) It can be shown that5 \[\hat{\theta}\sim\text{AN}\left(\theta,\ \frac{1+\theta^{2}+4\theta^{4}+ \theta^{6}+\theta^{8}}{n(1-\theta^{2})^{2}}\right);\] AN is read _asymptotically normal_ and is defined in Definition A.5. The maximum likelihood estimator (which we discuss next) of \(\theta\), in this case, has an asymptotic variance of \((1-\theta^{2})/n\). When \(\theta=.5\), for example, the ratio of the asymptotic variance of the method of moments estimator to the maximum likelihood estimator of \(\theta\) is about 3.5. That is, for large samples, the variance of the method of moments estimator is about 3.5 times larger than the variance of the MLE of \(\theta\) when \(\theta=.5\).

Footnote 5: The result follows from Theorem A.7 and the delta method. See the proof of Theorem A.7 for details on the delta method.

Maximum Likelihood and Least Squares Estimation

To fix ideas, we first focus on the causal AR(1) case. Let

\[x_{t}=\mu+\phi(x_{t-1}-\mu)+w_{t} \tag{3.106}\]

where \(|\phi|<1\) and \(w_{t}\sim\text{id N}(0,\sigma_{w}^{2})\). Given data \(x_{1}\), \(x_{2}\), \(\ldots\), \(x_{n}\), we seek the likelihood

\[L(\mu,\phi,\sigma_{w}^{2})=f\left(x_{1},x_{2},\ldots,x_{n}\ \big{|}\ \mu,\phi, \sigma_{w}^{2}\right).\]

In the case of an AR(1), we may write the likelihood as

\[L(\mu,\phi,\sigma_{w}^{2})=f(x_{1})f(x_{2}\ \big{|}\ x_{1})\cdots f(x_{n}\ \big{|}\ x_{n-1}),\]

where we have dropped the parameters in the densities, \(f(\cdot)\), to ease the notation. Because \(x_{t}\ \big{|}\ x_{t-1}\sim\text{N}\left(\mu+\phi(x_{t-1}-\mu),\,\sigma_{w}^{2}\right)\), we have

\[f(x_{t}\ \big{|}\ x_{t-1})=f_{w}[(x_{t}-\mu)-\phi(x_{t-1}-\mu)],\]

where \(f_{w}(\cdot)\) is the density of \(w_{t}\), that is, the normal density with mean zero and variance \(\sigma_{w}^{2}\). We may then write the likelihood as

\[L(\mu,\phi,\sigma_{w})=f(x_{1})\ \prod_{t=2}^{n}\ f_{w}\left[(x_{t}-\mu)-\phi(x_{t-1}-\mu)\right].\]To find \(f(x_{1})\), we can use the causal representation

\[x_{1}=\mu+\sum_{j=0}^{\infty}\phi^{j}w_{1-j}\]

to see that \(x_{1}\) is normal, with mean \(\mu\) and variance \(\sigma_{w}^{2}/(1-\phi^{2})\). Finally, for an AR(1), the likelihood is

\[L(\mu,\phi,\sigma_{w}^{2})=(2\pi\sigma_{w}^{2})^{-n/2}(1-\phi^{2})^{1/2}\exp \left[-\frac{S(\mu,\phi)}{2\sigma_{w}^{2}}\right], \tag{3.107}\]

where

\[S(\mu,\phi)=(1-\phi^{2})(x_{1}-\mu)^{2}+\sum_{t=2}^{n}\left[(x_{t}-\mu)-\phi(x_ {t-1}-\mu)\right]^{2}. \tag{3.108}\]

Typically, \(S(\mu,\phi)\) is called the _unconditional sum of squares_. We could have also considered the estimation of \(\mu\) and \(\phi\) using _unconditional least squares_, that is, estimation by minimizing \(S(\mu,\phi)\).

Taking the partial derivative of the log of (3.107) with respect to \(\sigma_{w}^{2}\) and setting the result equal to zero, we get the typical normal result that for any given values of \(\mu\) and \(\phi\) in the parameter space, \(\sigma_{w}^{2}=n^{-1}S(\mu,\phi)\) maximizes the likelihood. Thus, the maximum likelihood estimate of \(\sigma_{w}^{2}\) is

\[\hat{\sigma}_{w}^{2}=n^{-1}S(\hat{\mu},\hat{\phi}), \tag{3.109}\]

where \(\hat{\mu}\) and \(\hat{\phi}\) are the MLEs of \(\mu\) and \(\phi\), respectively. If we replace \(n\) in (3.109) by \(n-2\), we would obtain the unconditional least squares estimate of \(\sigma_{w}^{2}\).

If, in (3.107), we take logs, replace \(\sigma_{w}^{2}\) by \(\hat{\sigma}_{w}^{2}\), and ignore constants, \(\hat{\mu}\) and \(\hat{\phi}\) are the values that minimize the criterion function

\[l(\mu,\phi)=\log\left[n^{-1}S(\mu,\phi)\right]-n^{-1}\log(1-\phi^{2}); \tag{3.110}\]

that is, \(l(\mu,\phi)\propto-2\log L(\mu,\phi,\hat{\sigma}_{w}^{2})\).6 Because (3.108) and (3.110) are complicated functions of the parameters, the minimization of \(l(\mu,\phi)\) or \(S(\mu,\phi)\) is accomplished numerically. In the case of AR models, we have the advantage that, conditional on initial values, they are linear models. That is, we can drop the term in the likelihood that causes the nonlinearity. Conditioning on \(x_{1}\), the _conditional likelihood_ becomes

Footnote 6: The criterion function is sometimes called the profile or concentrated likelihood.

\[L(\mu,\phi,\sigma_{w}^{2}\bigm{|}x_{1}) =\prod_{t=2}^{n}\ f_{w}\left[(x_{t}-\mu)-\phi(x_{t-1}-\mu)\right]\] \[=(2\pi\sigma_{w}^{2})^{-(n-1)/2}\exp\left[-\frac{S_{c}(\mu,\phi)} {2\sigma_{w}^{2}}\right], \tag{3.111}\]where the _conditional sum of squares_ is

\[S_{c}(\mu,\phi)=\sum_{t=2}^{n}\left[(x_{t}-\mu)-\phi(x_{t-1}-\mu)\right]^{2}. \tag{3.112}\]

The conditional MLE of \(\sigma_{w}^{2}\) is

\[\hat{\sigma}_{w}^{2}=S_{c}(\hat{\mu},\hat{\phi})/(n-1), \tag{3.113}\]

and \(\hat{\mu}\) and \(\hat{\phi}\) are the values that minimize the conditional sum of squares, \(S_{c}(\mu,\phi)\). Letting \(\alpha=\mu(1-\phi)\), the conditional sum of squares can be written as

\[S_{c}(\mu,\phi)=\sum_{t=2}^{n}\left[x_{t}-(\alpha+\phi x_{t-1})\right]^{2}. \tag{3.114}\]

The problem is now the linear regression problem stated in Sect. 2.1. Following the results from least squares estimation, we have \(\hat{\alpha}=\bar{x}_{(2)}-\hat{\phi}\bar{x}_{(1)}\), where \(\bar{x}_{(1)}=(n-1)^{-1}\sum_{t=1}^{n-1}x_{t}\), and \(\bar{x}_{(2)}=(n-1)^{-1}\sum_{t=2}^{n}x_{t}\), and the conditional estimates are then

\[\hat{\mu}=\frac{\bar{x}_{(2)}-\hat{\phi}\bar{x}_{(1)}}{1-\hat{\phi}} \tag{3.115}\]

\[\hat{\phi}=\frac{\sum_{t=2}^{n}(x_{t}-\bar{x}_{(2)})(x_{t-1}-\bar{x}_{(1)})}{ \sum_{t=2}^{n}(x_{t-1}-\bar{x}_{(1)})^{2}}. \tag{3.116}\]

From (3.115) and (3.116), we see that \(\hat{\mu}\approx\bar{x}\) and \(\hat{\phi}\approx\hat{\rho}(1)\). That is, the Yule-Walker estimators and the conditional least squares estimators are approximately the same. The only difference is the inclusion or exclusion of terms involving the endpoints, \(x_{1}\) and \(x_{n}\). We can also adjust the estimate of \(\sigma_{w}^{2}\) in (3.113) to be equivalent to the least squares estimator, that is, divide \(S_{c}(\hat{\mu},\hat{\phi})\) by \((n-3)\) instead of \((n-1)\) in (3.113).

For general AR(\(p\)) models, maximum likelihood estimation, unconditional least squares, and conditional least squares follow analogously to the AR(1) example. For general ARMA models, it is difficult to write the likelihood as an explicit function of the parameters. Instead, it is advantageous to write the likelihood in terms of the _innovations_, or one-step-ahead prediction errors, \(x_{t}-x_{t}^{t-1}\). This will also be useful in Chap. 6 when we study state-space models.

For a normal ARMA(\(p,q\)) model, let \(\beta=(\mu,\phi_{1},\ldots,\phi_{p},\ \theta_{1},\ldots,\theta_{q})^{\prime}\) be the \((p+q+1)\)-dimensional vector of the model parameters. The likelihood can be written as

\[L(\beta,\sigma_{w}^{2})=\prod_{t=1}^{n}f(x_{t}\ \big{|}\ x_{t-1},\ldots,x_{1}).\]

The conditional distribution of \(x_{t}\) given \(x_{t-1},\ldots,x_{1}\) is Gaussian with mean \(x_{t}^{t-1}\) and variance \(P_{t}^{t-1}\). Recall from (3.71) that \(P_{t}^{t-1}=\gamma(0)\prod_{j=1}^{t-1}(1-\phi_{jj}^{2})\). For ARMA models, \(\gamma(0)=\sigma_{w}^{2}\sum_{j=0}^{\infty}\psi_{j}^{2}\), in which case we may write

\[P_{t}^{t-1}=\sigma_{w}^{2}\left\{\left(\sum_{j=0}^{\infty}\psi_{j}^{2}\right) \left[\prod_{j=1}^{t-1}(1-\phi_{jj}^{2})\right]\right\}\stackrel{{ \rm def}}{{=}}\sigma_{w}^{2}\ r_{t},\]where \(r_{t}\) is the term in the braces. Note that the \(r_{t}\) terms are functions only of the regression parameters and that they may be computed recursively as \(r_{t+1}=(1-\phi_{tt}^{2})r_{t}\) with initial condition \(r_{1}=\sum_{j=0}^{\infty}\psi_{j}^{2}\). The likelihood of the data can now be written as

\[L(\beta,\sigma_{w}^{2})=(2\pi\sigma_{w}^{2})^{-n/2}\left[r_{1}(\beta)r_{2}( \beta)\cdots r_{n}(\beta)\right]^{-1/2}\exp\left[-\frac{S(\beta)}{2\sigma_{w}^ {2}}\right], \tag{3.117}\]

where

\[S(\beta)=\sum_{t=1}^{n}\left[\frac{(x_{t}-x_{t}^{t-1}(\beta))^{2}}{r_{t}(\beta) }\right]. \tag{3.118}\]

Both \(x_{t}^{t-1}\) and \(r_{t}\) are functions of \(\beta\) alone, and we make that fact explicit in (3.117)-(3.118). Given values for \(\beta\) and \(\sigma_{w}^{2}\), the likelihood may be evaluated using the techniques of Sect. 3.4. Maximum likelihood estimation would now proceed by maximizing (3.117) with respect to \(\beta\) and \(\sigma_{w}^{2}\). As in the AR(1) example, we have

\[\hat{\sigma}_{w}^{2}=n^{-1}S(\hat{\beta}), \tag{3.119}\]

where \(\hat{\beta}\) is the value of \(\beta\) that minimizes the concentrated likelihood

\[l(\beta)=\log\left[n^{-1}S(\beta)\right]+n^{-1}\sum_{t=1}^{n}\log r_{t}(\beta). \tag{3.120}\]

For the AR(1) model (3.106) discussed previously, recall that \(x_{1}^{0}=\mu\) and \(x_{t}^{t-1}=\mu+\phi(x_{t-1}-\mu)\), for \(t=2,\ldots,n\). Also, using the fact that \(\phi_{11}=\phi\) and \(\phi_{hh}=0\) for \(h>1\), we have \(r_{1}=\sum_{j=0}^{\infty}\phi^{2j}=(1-\phi^{2})^{-1}\), \(r_{2}=(1-\phi^{2})^{-1}(1-\phi^{2})=1\), and in general, \(r_{t}=1\) for \(t=2,\ldots,n\). Hence, the likelihood presented in (3.107) is identical to the innovations form of the likelihood given by (3.117). Moreover, the generic \(S(\beta)\) in (3.118) is \(S(\mu,\phi)\) given in (3.108) and the generic \(l(\beta)\) in (3.120) is \(l(\mu,\phi)\) in (3.110).

Unconditional least squares would be performed by minimizing (3.118) with respect to \(\beta\). Conditional least squares estimation would involve minimizing (3.118) with respect to \(\beta\) but where, to ease the computational burden, the predictions and their errors are obtained by conditioning on initial values of the data. In general, numerical optimization routines are used to obtain the actual estimates and their standard errors.

**Example 3.30**: **The Newton-Raphson and Scoring Algorithms**

Two common numerical optimization routines for accomplishing maximum likelihood estimation are Newton-Raphson and scoring. We will give a brief account of the mathematical ideas here. The actual implementation of these algorithms is much more complicated than our discussion might imply. For details, the reader is referred to any of the _Numerical Recipes_ books, for example, Press et al. [156].

Let \(l(\beta)\) be a criterion function of \(k\) parameters \(\beta=(\beta_{1},\ldots,\beta_{k})\) that we wish to minimize with respect to \(\beta\). For example, consider the likelihood function given by (3.110) or by (3.120). Suppose \(l(\beta)\) is the extremum that we are interested in finding, and \(\hat{\beta}\) is found by solving \(\partial l(\beta)/\partial\beta_{j}=0\), for \(j=1,\ldots,k\). Let \(l^{(1)}(\beta)\) denote the \(k\times 1\) vector of partials

\[l^{(1)}(\beta)=\left(\frac{\partial l(\beta)}{\partial\beta_{1}},\ldots,\frac{ \partial l(\beta)}{\partial\beta_{k}}\right)^{\prime}.\]

Note, \(l^{(1)}(\hat{\beta})=0\), the \(k\times 1\) zero vector. Let \(l^{(2)}(\beta)\) denote the \(k\times k\) matrix of second-order partials

\[l^{(2)}(\beta)=\left\langle-\frac{\partial l^{2}(\beta)}{\partial\beta_{i} \partial\beta_{j}}\right\rangle_{i,j=1}^{k},\]

and assume \(l^{(2)}(\beta)\) is nonsingular. Let \(\beta_{(0)}\) be a "sufficiently good" initial estimator of \(\beta\). Then, using a Taylor expansion, we have the following approximation:

\[0=l^{(1)}(\hat{\beta})\approx l^{(1)}(\beta_{(0)})-l^{(2)}(\beta_{(0)})\left[ \hat{\beta}-\beta_{(0)}\right].\]

Setting the right-hand side equal to zero and solving for \(\hat{\beta}\) [call the solution \(\beta_{(1)}\)], we get

\[\beta_{(1)}=\beta_{(0)}+\left[l^{(2)}(\beta_{(0)})\right]^{-1}l^{(1)}(\beta_{( 0)}).\]

The Newton-Raphson algorithm proceeds by iterating this result, replacing \(\beta_{(0)}\) by \(\beta_{(1)}\) to get \(\beta_{(2)}\), and so on, until convergence. Under a set of appropriate conditions, the sequence of estimators, \(\beta_{(1)}\), \(\beta_{(2)}\), \(\ldots\), will converge to \(\hat{\beta}\), the MLE of \(\beta\).

For maximum likelihood estimation, the criterion function used is \(l(\beta)\) given by (3.120); \(l^{(1)}(\beta)\) is called the score vector, and \(l^{(2)}(\beta)\) is called the _Hessian_. In the method of scoring, we replace \(l^{(2)}(\beta)\) by \(\operatorname{E}[l^{(2)}(\beta)]\), the _information_ matrix. Under appropriate conditions, the inverse of the information matrix is the asymptotic variance-covariance matrix of the estimator \(\hat{\beta}\). This is sometimes approximated by the inverse of the Hessian at \(\hat{\beta}\). If the derivatives are difficult to obtain, it is possible to use quasi-maximum likelihood estimation where numerical techniques are used to approximate the derivatives.

**Example 3.31**: **MLE for the Recruitment Series**

So far, we have fit an AR(2) model to the Recruitment series using ordinary least squares (Example 3.18) and using Yule-Walker (Example 3.28). The following is an R session used to fit an AR(2) model via maximum likelihood estimation to the Recruitment series; these results can be compared to the results in Example 3.18 and Example 3.28.

rec.mle = ar.mle(rec, order-2)

rec.mle5x.mean # 62.26

rec.mle5ar # 1.35, -.46

sqrt(diag(rec.mle5asy.var.coef)) #.04,.04

rec.mle5var.pred # 89.34

We now discuss least squares for ARMA(\(p,q\)) models via _Gauss-Newton_. For general and complete details of the Gauss-Newton procedure, the reader is referredto Fuller [66]. As before, write \(\beta=(\phi_{1},\ldots,\phi_{p},\theta_{1},\ldots,\theta_{q})^{\prime}\), and for the ease of discussion, we will put \(\mu=0\). We write the model in terms of the errors

\[w_{t}(\beta)=x_{t}-\sum_{j=1}^{p}\phi_{j}x_{t-j}-\sum_{k=1}^{q}\theta_{k}w_{t-k }(\beta), \tag{3.121}\]

emphasizing the dependence of the errors on the parameters.

For conditional least squares, we approximate the residual sum of squares by conditioning on \(x_{1},\ldots,x_{p}\) (if \(p>0\)) and \(w_{p}=w_{p-1}=w_{p-2}=\cdots=w_{1-q}=0\) (if \(q>0\)), in which case, given \(\beta\), we may evaluate (3.121) for \(t=p+1,p+2,\ldots,n\). Using this conditioning argument, the conditional error sum of squares is

\[S_{c}(\beta)=\sum_{t=p+1}^{n}w_{t}^{2}(\beta). \tag{3.122}\]

Minimizing \(S_{c}(\beta)\) with respect to \(\beta\) yields the conditional least squares estimates. If \(q=0\), the problem is linear regression and no iterative technique is needed to minimize \(S_{c}(\phi_{1},\ldots,\phi_{p})\). If \(q>0\), the problem becomes nonlinear regression and we will have to rely on numerical optimization.

When \(n\) is large, conditioning on a few initial values will have little influence on the final parameter estimates. In the case of small to moderate sample sizes, one may wish to rely on unconditional least squares. The unconditional least squares problem is to choose \(\beta\) to minimize the unconditional sum of squares, which we have generically denoted by \(S(\beta)\) in this section. The unconditional sum of squares can be written in various ways, and one useful form in the case of \(\text{ARMA}(p,q)\) models is derived in Box et al. [31, Appendix A7.3]. They showed (see Problem 3.19) the unconditional sum of squares can be written as

\[S(\beta)=\sum_{t=-\infty}^{n}\tilde{w}_{t}^{2}(\beta), \tag{3.123}\]

where \(\tilde{w}_{t}(\beta)=\text{E}(w_{t}\mid x_{1},\ldots,x_{n})\). When \(t\leq 0\), the \(\hat{w}_{t}(\beta)\) are obtained by backcasting. As a practical matter, we approximate \(S(\beta)\) by starting the sum at \(t=-M+1\), where \(M\) is chosen large enough to guarantee \(\sum_{t=-\infty}^{-M}\tilde{w}_{t}^{2}(\beta)\approx 0\). In the case of unconditional least squares estimation, a numerical optimization technique is needed even when \(q=0\).

To employ Gauss-Newton, let \(\beta_{(0)}=(\phi_{1}^{(0)},\ldots,\phi_{p}^{(0)},\theta_{1}^{(0)},\ldots, \theta_{q}^{(0)})^{\prime}\) be an initial estimate of \(\beta\). For example, we could obtain \(\beta_{(0)}\) by method of moments. The first-order Taylor expansion of \(w_{t}(\beta)\) is

\[w_{t}(\beta)\approx w_{t}(\beta_{(0)})-\left(\beta-\beta_{(0)}\right)^{\prime }z_{t}(\beta_{(0)}), \tag{3.124}\]

where

\[z_{t}^{\prime}(\beta_{(0)})=\left(-\frac{\partial w_{t}(\beta)}{\partial\beta _{1}},\ldots,-\frac{\partial w_{t}(\beta)}{\partial\beta_{p+q}}\right)\Bigg{|} _{\beta=\beta_{(0)}},\quad t=1,\ldots,n.\]The linear approximation of \(S_{c}(\beta)\) is

\[Q(\beta)=\sum_{t=p+1}^{n}\left[w_{t}(\beta_{(0)})-\left(\beta-\beta_{(0)}\right)^{ \prime}z_{t}(\beta_{(0)})\right]^{2} \tag{3.125}\]

and this is the quantity that we will minimize. For approximate unconditional least squares, we would start the sum in (3.125) at \(t=-M+1\), for a large value of \(M\), and work with the backcasted values.

Using the results of ordinary least squares (Sect. 2.1), we know

\[(\widehat{\beta-\beta_{(0)}})=\left(n^{-1}\sum_{t=p+1}^{n}z_{t}(\beta_{(0)})z_{ t}^{\prime}(\beta_{(0)})\right)^{-1}\left(n^{-1}\sum_{t=p+1}^{n}z_{t}(\beta_{(0)} )w_{t}(\beta_{(0)})\right) \tag{3.126}\]

minimizes \(Q(\beta)\). From (3.126), we write the _one-step Gauss-Newton estimate_ as

\[\beta_{(1)}=\beta_{(0)}+\Delta(\beta_{(0)}), \tag{3.127}\]

where \(\Delta(\beta_{(0)})\) denotes the right-hand side of (3.126). Gauss-Newton estimation is accomplished by replacing \(\beta_{(0)}\) by \(\beta_{(1)}\) in (3.127). This process is repeated by calculating, at iteration \(j=2,3,\ldots\),

\[\beta_{(j)}=\beta_{(j-1)}+\Delta(\beta_{(j-1)})\]

until convergence.

**Example 3.32**: **Gauss-Newton for an MA(1)**

Consider an invertible MA(1) process, \(x_{t}=w_{t}+\theta w_{t-1}\). Write the truncated errors as

\[w_{t}(\theta)=x_{t}-\theta w_{t-1}(\theta),\quad t=1,\ldots,n, \tag{3.128}\]

where we condition on \(w_{0}(\theta)=0\). Taking derivatives and negating,

\[-\frac{\partial w_{t}(\theta)}{\partial\theta}=w_{t-1}(\theta)+\theta\frac{ \partial w_{t-1}(\theta)}{\partial\theta},\quad t=1,\ldots,n, \tag{3.129}\]

where \(\partial w_{0}(\theta)/\partial\theta=0\). We can also write (3.129) as

\[z_{t}(\theta)=w_{t-1}(\theta)-\theta z_{t-1}(\theta),\quad t=1,\ldots,n, \tag{3.130}\]

where \(z_{t}(\theta)=-\partial w_{t}(\theta)/\partial\theta\) and \(z_{0}(\theta)=0\).

Let \(\theta_{(0)}\) be an initial estimate of \(\theta\), for example, the estimate given in Example 3.29. Then, the Gauss-Newton procedure for conditional least squares is given by

\[\theta_{(j+1)}=\theta_{(j)}+\frac{\sum_{t=1}^{n}z_{t}(\theta_{(j)})w_{t}( \theta_{(j)})}{\sum_{t=1}^{n}z_{t}^{2}(\theta_{(j)})},\quad j=0,1,2,\ldots, \tag{3.131}\]

where the values in (3.131) are calculated recursively using (3.128) and (3.130). The calculations are stopped when \(|\theta_{(j+1)}-\theta_{(j)}|\), or \(|Q(\theta_{(j+1)})-Q(\theta_{(j)})|\), are smaller than some preset amount.

**Example 3.33**: **Fitting the Glacial Varve Series**

Consider the series of glacial varve thicknesses from Massachusetts for \(n=634\) years, as analyzed in Example 2.7 and in Problem 2.8, where it was argued that a first-order moving average model might fit the logarithmically transformed and differenced varve series, say,

\[\nabla\log(x_{t})=\log(x_{t})-\log(x_{t-1})=\log\left(\frac{x_{t}}{x_{t-1}} \right),\]

which can be interpreted as being approximately the percentage change in the thickness.

The sample ACF and PACF, shown in Fig. 3.9, confirm the tendency of \(\nabla\log(x_{t})\) to behave as a first-order moving average process as the ACF has only a significant peak at lag one and the PACF decreases exponentially. Using Table 3.1, this sample behavior fits that of the MA(1) very well.

Since \(\hat{\rho}(1)=-.397\), our initial estimate is \(\theta_{(0)}=-.495\) using (3.105). The results of eleven iterations of the Gauss-Newton procedure, (3.131), starting with \(\theta_{(0)}\) are given in Table 3.2. The final estimate is \(\hat{\theta}=\theta_{(11)}=-.773\); interim values and the corresponding value of the conditional sum of squares, \(S_{c}(\theta)\) given in (3.122), are also displayed in the table. The final estimate of the error variance is \(\hat{\sigma}_{w}^{2}=148.98/632=.236\) with 632 degrees of freedom (one is lost in differencing). The value of the sum of the squared derivatives at convergence is \(\sum_{t=1}^{n}z_{t}^{2}(\theta_{(11)})=368.741\), and consequently, the estimated standard error of \(\hat{\theta}\) is \(\sqrt{.236/368.741}=.025\)7; this leads to a \(t\)-value of \(-.773/.025=-30.92\) with 632 degrees of freedom.

Footnote 7: To estimate the standard error, we are using the standard regression results from (2.6) as an approximation

Figure 3.10 displays the conditional sum of squares, \(S_{c}(\theta)\) as a function of \(\theta\), as well as indicating the values of each step of the Gauss-Newton algorithm. Note that the Gauss-Newton procedure takes large steps toward the minimum initially,

Figure 3.9: ACF and PACF of transformed glacial varvesand then takes very small steps as it gets close to the minimizing value. When there is only one parameter, as in this case, it would be easy to evaluate \(S_{c}(\theta)\) on a grid of points, and then choose the appropriate value of \(\theta\) from the grid search. It would be difficult, however, to perform grid searches when there are many parameters.

The following code was used in this example.

``` x=diff(log(varve))
#EvaluateSconafid c(@)->w->z c()->Sc->Sz->Szw num=length(x) th=seq(-.3,-.94,-.@1) for(pin1:length(th)){ for(iin2:num){w[i]=x[i]-th[p]*w[i-1]}  Sc[p]=sum(w*2)} plot(th,Sc,type="1",ylab=expression(S[c](theta)),xlab=expression(theta), lwd=2)
#Gauss-NewtonEstimation r=acf(x,lag1,plot=FALSE)$acf[-1] rstart=(1-sqrt(1-4*(r*2)))/(2*r)#from(3.105) c(@)->w->z c()->Sc->Sz->Szw->para niter=12 para[1]=rstart for(pin1:niter){ for(iin2:num){w[i]=x[i]-para[p]*w[i-1] z[i]=w[i-1]-para[p]*z[i-1]} Sc[p]=sum(w*2) Sz[p]=sum(z*2)  Szw[p]=sum(z*w) para[p+1]=para[p]+Szw[p]/Sz[p]} round(cbind(iteration=@:(niter-1),theta=para[1:niter],Sc,Sz),3) abline(v=para[1:12],1ty=2) points(para[1:12],Sc[1:12],pch=16)

Figure 3.10: Conditional sum of squares versus values of the moving average parameter for the glacial curve example, Example 3.33. _Vertical lines_ indicate the values of the parameter obtained via Gauss–Newton; see Table 3.2 for the actual values

In the general case of causal and invertible ARMA(\(p,q\)) models, maximum likelihood estimation and conditional and unconditional least squares estimation (and Yule-Walker estimation in the case of AR models) all lead to optimal estimators. The proof of this general result can be found in a number of texts on theoretical time series analysis (for example, Brockwell and Davis [36], or Hannan [86], to mention a few). We will denote the ARMA coefficient parameters by \(\beta=(\phi_{1},\ldots,\phi_{p},\theta_{1},\ldots,\theta_{q})^{\prime}\).

**Property 3.10**: **Large Sample Distribution of the Estimators**__

_Under appropriate conditions, for causal and invertible ARMA processes, the maximum likelihood, the unconditional least squares, and the conditional least squares estimators, each initialized by the method of moments estimator, all provide optimal estimators of \(\sigma_{\text{w}}^{2}\) and \(\beta\), in the sense that \(\hat{\sigma}_{\text{w}}^{2}\) is consistent, and the asymptotic distribution of \(\hat{\beta}\) is the best asymptotic normal distribution. In particular, as \(n\to\infty\),_

\[\sqrt{n}\left(\beta-\beta\right)\;\overset{d}{\to}N\left(0,\sigma_{\text{w}}^ {2}\;\Gamma_{p,q}^{-1}\right). \tag{3.132}\]

_The asymptotic variance-covariance matrix of the estimator \(\hat{\beta}\) is the inverse of the information matrix. In particular, the \((p+q)\times(p+q)\) matrix \(\Gamma_{p,q}\), has the form_

\[\Gamma_{p,q}=\begin{pmatrix}\Gamma_{\phi\phi}&\Gamma_{\phi\theta}\\ \Gamma_{\theta\phi}&\Gamma_{\theta\theta}\end{pmatrix}. \tag{3.133}\]

_The \(p\times p\) matrix \(\Gamma_{\phi\phi}\) is given by (3.100), that is, the \(ij\)-th element of \(\Gamma_{\phi\phi}\), for \(i,j=1,\ldots,p\), is \(\gamma_{x}(i-j)\) from an AR(\(p\)) process, \(\phi(B)x_{t}=w_{t}\). Similarly, \(\Gamma_{\theta\theta}\) is a \(q\times q\) matrix with the \(ij\)-th element, for \(i,j=1,\ldots,q\), equal to \(\gamma_{y}(i-j)\) from an AR(\(q\)) process, \(\theta(B)y_{t}=w_{t}\). The \(p\times q\) matrix \(\Gamma_{\phi\theta}=\{\gamma_{xy}(i-j)\}\), for \(i=1,\ldots,p\); \(j=1,\ldots,q\); that is, the \(ij\)-th element is the cross-covariance between the two AR processes given by \(\phi(B)x_{t}=w_{t}\) and \(\theta(B)y_{t}=w_{t}\). Finally, \(\Gamma_{\theta\phi}=\Gamma_{\phi\theta}^{\prime}\) is \(q\times p\)._

Further discussion of Property 3.10, including a proof for the case of least squares estimators for AR(\(p\)) processes, can be found in Sect. B.3.

\begin{table}
\begin{tabular}{l r r r} \hline \hline \(j\) & \(\theta_{(j)}\) & \(S_{c}(\theta_{(j)})\) & \(\sum_{t=1}^{n}z_{t}^{2}(\theta_{(j)})\) \\ \hline \(0\) & \(-0.495\) & \(158.739\) & \(171.240\) \\ \(1\) & \(-0.668\) & \(150.747\) & \(235.266\) \\ \(2\) & \(-0.733\) & \(149.264\) & \(300.562\) \\ \(3\) & \(-0.756\) & \(149.031\) & \(336.823\) \\ \(4\) & \(-0.766\) & \(148.990\) & \(354.173\) \\ \(5\) & \(-0.769\) & \(148.982\) & \(362.167\) \\ \(6\) & \(-0.771\) & \(148.980\) & \(365.801\) \\ \(7\) & \(-0.772\) & \(148.980\) & \(367.446\) \\ \(8\) & \(-0.772\) & \(148.980\) & \(368.188\) \\ \(9\) & \(-0.772\) & \(148.980\) & \(368.522\) \\ \(10\) & \(-0.773\) & \(148.980\) & \(368.673\) \\ \(11\) & \(-0.773\) & \(148.980\) & \(368.741\) \\ \hline \end{tabular}
\end{table}
Table 3.2: Gauss–Newton results for Example 3.33

**Example 3.34**: **Some Specific Asymptotic Distributions**

The following are some specific cases of Property 3.10.

**AR(1):**: \(\gamma_{x}(0)=\sigma_{w}^{2}/(1-\phi^{2})\), so \(\sigma_{w}^{2}\Gamma_{1,0}^{-1}=(1-\phi^{2})\). Thus,

\[\hat{\phi}\sim\mathrm{AN}\left[\phi,n^{-1}(1-\phi^{2})\right]\,. \tag{3.134}\]
**AR(2):**: The reader can verify that

\[\gamma_{x}(0)=\left(\frac{1-\phi_{2}}{1+\phi_{2}}\right)\frac{\sigma_{w}^{2}}{ (1-\phi_{2})^{2}-\phi_{1}^{2}}\]

and \(\gamma_{x}(1)=\phi_{1}\gamma_{x}(0)+\phi_{2}\gamma_{x}(1)\). From these facts, we can compute \(\Gamma_{2,0}^{-1}\). In particular, we have

\[\left(\begin{matrix}\hat{\phi}_{1}\\ \hat{\phi}_{2}\end{matrix}\right)\sim\mathrm{AN}\left[\left(\begin{matrix}\phi _{1}\\ \phi_{2}\end{matrix}\right),\ n^{-1}\left(\begin{matrix}1-\phi_{2}^{2}&-\phi_{1 }(1+\phi_{2})\\ \mathrm{sym}&1-\phi_{2}^{2}\end{matrix}\right)\right]\,. \tag{3.135}\]
**MA(1):**: In this case, write \(\theta(B)y_{t}=w_{t}\), or \(y_{t}+\theta y_{t-1}=w_{t}\). Then, analogous to the AR(1) case, \(\gamma_{y}(0)=\sigma_{w}^{2}/(1-\theta^{2})\), so \(\sigma_{w}^{2}\Gamma_{0,1}^{-1}=(1-\theta^{2})\). Thus,

\[\hat{\theta}\sim\mathrm{AN}\left[\theta,n^{-1}(1-\theta^{2})\right]\,. \tag{3.136}\]
**MA(2):**: Write \(y_{t}+\theta_{1}y_{t-1}+\theta_{2}y_{t-2}=w_{t}\), so, analogous to the AR(2) case, we have

\[\left(\begin{matrix}\hat{\theta}_{1}\\ \hat{\theta}_{2}\end{matrix}\right)\sim\mathrm{AN}\left[\left(\begin{matrix} \theta_{1}\\ \theta_{2}\end{matrix}\right),\ n^{-1}\left(\begin{matrix}1-\theta_{2}^{2}& \theta_{1}(1+\theta_{2})\\ \mathrm{sym}&1-\theta_{2}^{2}\end{matrix}\right)\right]\,. \tag{3.137}\]
**ARMA(1,1):**: To calculate \(\Gamma_{\phi\theta}\), we must find \(\gamma_{xy}(0)\), where \(x_{t}-\phi x_{t-1}=w_{t}\) and \(y_{t}+\theta y_{t-1}=w_{t}\). We have

\[\gamma_{xy}(0) = \mathrm{cov}(x_{t},y_{t})=\mathrm{cov}(\phi x_{t-1}+w_{t},-\theta y _{t-1}+w_{t})\] \[= -\phi\theta\gamma_{xy}(0)+\sigma_{w}^{2}.\]

Solving, we find, \(\gamma_{xy}(0)=\sigma_{w}^{2}/(1+\phi\theta)\). Thus,

\[\left(\begin{matrix}\hat{\phi}\\ \hat{\theta}\end{matrix}\right)\sim\mathrm{AN}\left[\left(\begin{matrix}\phi \\ \theta\end{matrix}\right),\ n^{-1}\left[\left(\begin{matrix}1-\phi^{2}\\ \mathrm{sym}&(1-\theta^{2})^{-1}\end{matrix}\right)^{-1}\right]^{-1}\right]\,. \tag{3.138}\]

**Example 3.35**: **Overfitting Caveat**

The asymptotic behavior of the parameter estimators gives us an additional insight into the problem of fitting ARMA models to data. For example, suppose a time series follows an AR(1) process and we decide to fit an AR(2) to the data. Do any problems occur in doing this? More generally, why not simply fit large-order AR models to make sure that we capture the dynamics of the process? After all, if the process is truly an AR(1), the other autoregressive parameters will not be significant. The answer is that if we _overfit_, we obtain less efficient, or less precise parameter estimates. For example, if we fit an AR(1) to an AR(1) process, for large \(n\), \(\mathrm{var}(\hat{\phi}_{1})\approx n^{-1}(1-\phi_{1}^{2})\). But, if we fit an AR(2) to the AR(1) process, for large \(n\)\(\mathrm{var}(\hat{\phi}_{1})\approx n^{-1}(1-\phi_{2}^{2})=n^{-1}\) because \(\phi_{2}=0\). Thus, the variance of \(\phi_{1}\) has been inflated, making the estimator less precise.

We do want to mention, however, that overfitting can be used as a diagnostic tool. For example, if we fit an AR(2) model to the data and are satisfied with that model, then adding one more parameter and fitting an AR(3) should lead to approximately the same model as in the AR(2) fit. We will discuss model diagnostics in more detail in Sect. 3.7.

The reader might wonder, for example, why the asymptotic distributions of \(\hat{\phi}\) from an AR(1) and \(\hat{\theta}\) from an MA(1) are of the same form; compare (3.134) to (3.136). It is possible to explain this unexpected result heuristically using the intuition of linear regression. That is, for the normal regression model presented in Sect. 2.1 with no intercept term, \(x_{t}=\beta z_{t}+w_{t}\), we know \(\hat{\beta}\) is normally distributed with mean \(\beta\), and from (2.6),

\[\mathrm{var}\left\{\sqrt{n}\left(\hat{\beta}-\beta\right)\right\}=n\sigma_{w} ^{2}\left(\sum_{t=1}^{n}z_{t}^{2}\right)^{-1}=\sigma_{w}^{2}\left(n^{-1}\sum_ {t=1}^{n}z_{t}^{2}\right)^{-1}.\]

For the causal AR(1) model given by \(x_{t}=\phi x_{t-1}+w_{t}\), the intuition of regression tells us to expect that, for \(n\) large,

\[\sqrt{n}\left(\hat{\phi}-\phi\right)\]

is approximately normal with mean zero and with variance given by

\[\sigma_{w}^{2}\left(n^{-1}\sum_{t=2}^{n}x_{t-1}^{2}\right)^{-1}.\]

Now, \(n^{-1}\sum_{t=2}^{n}x_{t-1}^{2}\) is the sample variance (recall that the mean of \(x_{t}\) is zero) of the \(x_{t}\), so as \(n\) becomes large we would expect it to approach \(\mathrm{var}(x_{t})=\gamma(0)=\sigma_{w}^{2}/(1-\phi^{2})\). Thus, the large sample variance of \(\sqrt{n}\left(\hat{\phi}-\phi\right)\) is

\[\sigma_{w}^{2}\gamma_{x}(0)^{-1}=\sigma_{w}^{2}\left(\frac{\sigma_{w}^{2}}{1- \phi^{2}}\right)^{-1}=(1-\phi^{2});\]

that is, (3.134) holds.

In the case of an MA(1), we may use the discussion of Example 3.32 to write an approximate regression model for the MA(1). That is, consider the approximation (3.130) as the regression model

\[z_{t}(\hat{\theta})=-\theta z_{t-1}(\hat{\theta})+w_{t-1},\]

where now, \(z_{t-1}(\hat{\theta})\) as defined in Example 3.32, plays the role of the regressor. Continuing with the analogy, we would expect the asymptotic distribution of \(\sqrt{n}\left(\hat{\theta}-\theta\right)\)to be normal, with mean zero, and approximate variance

\[\sigma_{w}^{2}\left(n^{-1}\sum_{t=2}^{n}z_{t-1}^{2}(\hat{\theta})\right)^{-1}.\]

As in the AR(1) case, \(n^{-1}\sum_{t=2}^{n}z_{t-1}^{2}(\hat{\theta})\) is the sample variance of the \(z_{t}(\hat{\theta})\) so, for large \(n\), this should be \(\operatorname{var}\{z_{t}(\theta)\}=\gamma_{z}(0)\), say. But note, as seen from (3.130), \(z_{t}(\theta)\) is approximately an AR(1) process with parameter \(-\theta\). Thus,

\[\sigma_{w}^{2}\gamma_{z}(0)^{-1}=\sigma_{w}^{2}\left(\frac{\sigma_{w}^{2}}{1-( -\theta)^{2}}\right)^{-1}=(1-\theta^{2}),\]

which agrees with (3.136). Finally, the asymptotic distributions of the AR parameter estimates and the MA parameter estimates are of the same form because in the MA case, the "regressors" are the differential processes \(z_{t}(\theta)\) that have AR structure, and it is this structure that determines the asymptotic variance of the estimators. For a rigorous account of this approach for the general case, see Fuller [66, Theorem 5.5.4].

In Example 3.33, the estimated standard error of \(\hat{\theta}\) was.025. In that example, we used regression results to estimate the standard error as the square root of

\[n^{-1}\hat{\sigma}_{w}^{2}\left(n^{-1}\sum_{t=1}^{n}z_{t}^{2}(\hat{\theta}) \right)^{-1}=\frac{\hat{\sigma}_{w}^{2}}{\sum_{t=1}^{n}z_{t}^{2}(\hat{\theta})},\]

where \(n=632\), \(\hat{\sigma}_{w}^{2}=.236\), \(\sum_{t=1}^{n}z_{t}^{2}(\hat{\theta})=368.74\) and \(\hat{\theta}=-.773\). Using (3.136), we could have also calculated this value using the asymptotic approximation, the square root of \((1-(-.773)^{2})/632\), which is also.025.

If \(n\) is small, or if the parameters are close to the boundaries, the asymptotic approximations can be quite poor. The _bootstrap_ can be helpful in this case; for a broad treatment of the bootstrap, see Efron and Tibshirani [56]. We discuss the case of an AR(1) here and leave the general discussion for Chap. 6. For now, we give a simple example of the bootstrap for an AR(1) process.

**Example 3.36**: **Bootstrapping an AR(1)**

We consider an AR(1) model with a regression coefficient near the boundary of causality and an error process that is symmetric but not normal. Specifically, consider the causal model

\[x_{t}=\mu+\phi(x_{t-1}-\mu)+w_{t}, \tag{3.139}\]

where \(\mu=50\), \(\phi=.95\), and \(w_{t}\) are iid double exponential (Laplace) with location zero, and scale parameter \(\beta=2\). The density of \(w_{t}\) is given by

\[f(w)=\frac{1}{2\beta}\exp\left\{-|w|/\beta\right\}\quad-\infty<w<\infty.\]

In this example, \(\operatorname{E}(w_{t})=0\) and \(\operatorname{var}(w_{t})=2\beta^{2}=8\). Figure 3.11 shows \(n=100\) simulated observations from this process. This particular realization is interesting;the data look like they were generated from a nonstationary process with three different mean levels. In fact, the data were generated from a well-behaved, albeit non-normal, stationary and causal model. To show the advantages of the bootstrap, we will act as if we do not know the actual error distribution. The data in Fig. 3.11 were generated as follows.

```
set.seed(101010) e=rexp(150,rate=.5);u=runif(150,-1,1);de=e*sign(u) dex=50+arima.sim(n=100,list(ar=.95),innov=de,n.start=50) plot.ts(dex,type='o',ylab-expression(X[-t]))
```

Using these data, we obtained the Yule-Walker estimates \(\hat{\mu}=45.25\), \(\hat{\phi}=.96\), and \(\hat{\sigma}_{w}^{2}=7.88\), as follows.

```
fit=ar.yw(dex,order=1) round(cbind(fit%x.mean,fit%ar,fit%var.pred),2) [1,]45.250.967.88
```

To assess the finite sample distribution of \(\hat{\phi}\) when \(n=100\), we simulated 1000 realizations of this AR(1) process and estimated the parameters via Yule-Walker. The finite sampling density of the Yule-Walker estimate of \(\phi\), based on the 1000 repeated simulations, is shown in Fig. 3.12. Based on Property 3.10, we would say that \(\hat{\phi}\) is approximately normal with mean \(\phi\) (which we supposedly do not know) and variance \((1-\phi^{2})/100\), which we would approximate by \((1-.96^{2})/100=.03^{2}\); this distribution is superimposed on Fig. 3.12. Clearly the sampling distribution is not close to normality for this sample size. The R code to perform the simulation is as follows. We use the results at the end of the example

```
set.seed(111) phi.yw=rep(NA,1000) for(iin1:1000){ e=rexp(150,rate=.5);u=runif(150,-1,1);de=e*sign(u) x=50+arima.sim(n=100,list(ar=.95),innov=de,n.start=50) phi.yw[i]=ar.yw(x,order=1)%ar}
```

The preceding simulation required full knowledge of the model, the parameter values and the noise distribution. Of course, in a sampling situation, we would not

Figure 3.11: One hundred observations generated from the model in Example 3.36

have the information necessary to do the preceding simulation and consequently would not be able to generate a figure like Fig. 3.12. The bootstrap, however, gives us a way to attack the problem.

To simplify the discussion and the notation, we condition on \(x_{1}\) throughout the example. In this case, the one-step-ahead predictors have a simple form,

\[x_{t}^{t-1}=\mu+\phi(x_{t-1}-\mu),\qquad t=2,\ldots,100.\]

Consequently, the innovations, \(\epsilon_{t}=x_{t}-x_{t}^{t-1}\), are given by

\[\epsilon_{t}=(x_{t}-\mu)-\phi(x_{t-1}-\mu),\qquad t=2,\ldots,100, \tag{3.140}\]

each with \(\text{MSPE}\ P_{t}^{t-1}=\text{E}(\epsilon_{t}^{2})=\text{E}(w_{t}^{2})=\sigma _{w}^{2}\) for \(t=2,\ldots,100\). We can use (3.140) to write the model in terms of the innovations,

\[x_{t}=x_{t}^{t-1}+\epsilon_{t}=\mu+\phi(x_{t-1}-\mu)+\epsilon_{t}\qquad t=2, \ldots,100. \tag{3.141}\]

To perform the bootstrap simulation, we replace the parameters with their estimates in (3.141), that is, \(\hat{\mu}=45.25\) and \(\hat{\phi}=.96\), and denote the resulting sample innovations as \(\{\hat{e}_{2},\ldots,\hat{e}_{100}\}\). To obtain one bootstrap sample, first randomly sample, with replacement, \(n=99\) values from the set of sample innovations; call the sampled values \(\{e_{2}^{*},\ldots,e_{100}^{*}\}\). Now, generate a bootstrapped data set sequentially by setting

\[x_{t}^{*}=45.25+.96(x_{t-1}^{*}-45.25)+\epsilon_{t}^{*},\qquad t=2,\ldots,100. \tag{3.142}\]

with \(x_{1}^{*}\) held fixed at \(x_{1}\). Next, estimate the parameters as if the data were \(x_{t}^{*}\). Call these estimates \(\hat{\mu}(1)\), \(\hat{\phi}(1)\), and \(\sigma_{w}^{2}(1)\). Repeat this process a large number, \(B\), of times, generating a collection of bootstrapped parameter estimates, \(\{\hat{\mu}(b),\hat{\phi}(b),\sigma_{w}^{2}(b);\,b=1,\ldots,B\}\). We can then approximate the finite sample distribution of an estimator from the bootstrapped parameter values. For example, we can approximate the distribution of \(\hat{\phi}-\phi\) by the empirical distribution of \(\hat{\phi}(b)-\hat{\phi}\), for \(b=1,\ldots,B\).

Figure 3.12 shows the bootstrap histogram of 500 bootstrapped estimates of \(\phi\) using the data shown in Fig. 3.11. Note that the bootstrap distribution of \(\hat{\phi}\) is close to the distribution of \(\hat{\phi}\) shown in Fig. 3.12. The following code was used to perform the bootstrap.

``` set.seed(666)#notthat666 fit=ar.yw(dex,order=1)#assumesthedatawereretained m=fit$x.mean#estimateofmean phi=fit$ar#estimateofphi nboot=500 resids=fitsresid[[-1]#the99innovations x.star=dex#initializex^ phi.star.yw=rep(NA,nboot)#Bootstrap for(iin1:nboot){ resid.star=sample(resids,replace=TRUE) for(tin1:99){ x.star[t+1]=m+phi*(x.star[t]-m)+resid.star[t]}

phi.star.yw[i] = ar.yw(x.star, order=1)$ar } # Picture  cuter = rgb(.5,.7,1,.5)  hist(phi.star.yw, 15, main="", prob=TRUE, xlim=c(.65,1.05), ylim=c(0,14),  col=culer, xlab=expression(hat(phi)))  lines(density(phi.yw, bw=.02), lwd=2) # from previous simulation  u = seq(.75, 1.1, by=.001) # normal approximation  lines(u, dnorm(u, mean=.96, sd=.03), lty=2, lwd=2)  legend(.65, 14, legend=c('true distribution', 'bootstrap distribution',  'normal approximation'), bty='n', lty=c(1,0,2), lwd=c(2,0,2),  col=1, pch=c(NA,22,NA), pt.bg=c(NA,culer,NA), pt.cex=2.5)

### 3.6 Integrated Models for Nonstationary Data

In Chaps. 1 and 2, we saw that if \(x_{t}\) is a random walk, \(x_{t}=x_{t-1}+w_{t}\), then by differencing \(x_{t}\), we find that \(\nabla x_{t}=w_{t}\) is stationary. In many situations, time series can be thought of as being composed of two components, a nonstationary trend component and a zero-mean stationary component. For example, in Sect. 2.1 we considered the model

\[x_{t}=\mu_{t}+y_{t}, \tag{3.143}\]

where \(\mu_{t}=\beta_{0}+\beta_{1}t\) and \(y_{t}\) is stationary. Differencing such a process will lead to a stationary process:

\[\nabla x_{t}=x_{t}-x_{t-1}=\beta_{1}+y_{t}-y_{t-1}=\beta_{1}+\nabla y_{t}.\]

Another model that leads to first differencing is the case in which \(\mu_{t}\) in (3.143) is stochastic and slowly varying according to a random walk. That is,

\[\mu_{t}=\mu_{t-1}+v_{t}\]

Figure 3.12: Finite sample density of the Yule–Walker estimate of \(\phi\) (_solid line_) in Example 3.36 and the corresponding asymptotic normal density (_dashed line_). Bootstrap histogram of \(\hat{\phi}\) based on 500 bootstrapped samples

where \(v_{t}\) is stationary. In this case,

\[\nabla x_{t}=v_{t}+\nabla y_{t},\]

is stationary. If \(\mu_{t}\) in (3.143) is a \(k\)-th order polynomial, \(\mu_{t}=\sum_{j=0}^{k}\beta_{j}t^{j}\), then (Problem 3.27) the differenced series \(\nabla^{k}x_{t}\) is stationary. Stochastic trend models can also lead to higher order differencing. For example, suppose

\[\mu_{t}=\mu_{t-1}+v_{t}\quad\text{and}\quad v_{t}=v_{t-1}+e_{t},\]

where \(e_{t}\) is stationary. Then, \(\nabla x_{t}=v_{t}+\nabla y_{t}\) is not stationary, but

\[\nabla^{2}x_{t}=e_{t}+\nabla^{2}y_{t}\]

is stationary.

The _integrated_ ARMA, or ARIMA, model is a broadening of the class of ARMA models to include differencing.

**Definition 3.11**: _A process \(x_{t}\) is said to be_ **ARIMA**_(\(p,d,q\)) if_

\[\nabla^{d}x_{t}=(1-B)^{d}x_{t}\]

_is ARMA(\(p,q\)). In general, we will write the model as_

\[\phi(B)(1-B)^{d}x_{t}=\theta(B)w_{t}. \tag{3.144}\]

_If \(\operatorname{E}(\nabla^{d}x_{t})=\mu\), we write the model as_

\[\phi(B)(1-B)^{d}x_{t}=\delta+\theta(B)w_{t},\]

_where \(\delta=\mu(1-\phi_{1}-\cdots-\phi_{p})\)._

Because of the nonstationarity, care must be taken when deriving forecasts. For the sake of completeness, we discuss this issue briefly here, but we stress the fact that both the theoretical and computational aspects of the problem are best handled via state-space models. We discuss the theoretical details in Chap. 6. For information on the state-space based computational aspects in R, see the ARIMA help files (?arima and?predict.Arima); our scripts sarima and sarima.for are basically wrappers for these R scripts.

It should be clear that, since \(y_{t}=\nabla^{d}x_{t}\) is ARMA, we can use Sect. 3.4 methods to obtain forecasts of \(y_{t}\), which in turn lead to forecasts for \(x_{t}\). For example, if \(d=1\), given forecasts \(y_{n+m}^{n}\) for \(m=1,2,\ldots\), we have \(y_{n+m}^{n}=x_{n+m}^{n}-x_{n+m-1}^{n}\), so that

\[x_{n+m}^{n}=y_{n+m}^{n}+x_{n+m-1}^{n}\]

with initial condition \(x_{n+1}^{n}=y_{n+1}^{n}+x_{n}\) (noting \(x_{n}^{n}=x_{n}\)).

It is a little more difficult to obtain the prediction errors \(P_{n+m}^{n}\), but for large \(n\), the approximation used in Sect. 3.4, (3.86), works well. That is, the mean-squaredprediction error can be approximated by

\[P^{n}_{n+m}=\sigma_{w}^{2}\sum_{j=0}^{m-1}\psi_{j}^{*2}, \tag{3.145}\]

where \(\psi_{j}^{*}\) is the coefficient of \(z^{j}\) in \(\psi^{*}(z)=\theta(z)/\phi(z)(1-z)^{d}\).

To better understand integrated models, we examine the properties of some simple cases; Problem 3.29 covers the ARIMA\((1,1,0)\) case.

**Example 3.37**: **Random Walk with Drift**

To fix ideas, we begin by considering the random walk with drift model first presented in Example 1.11, that is,

\[x_{t}=\delta+x_{t-1}+w_{t},\]

for \(t=1,2,\ldots\), and \(x_{0}=0\). Technically, the model is not ARIMA, but we could include it trivially as an ARIMA\((0,1,0)\) model. Given data \(x_{1},\ldots,x_{n}\), the one-step-ahead forecast is given by

\[x_{n+1}^{n}=\operatorname{E}(x_{n+1}\bigm{|}x_{n},\ldots,x_{1})=\operatorname{ E}(\delta+x_{n}+w_{n+1}\bigm{|}x_{n},\ldots,x_{1})=\delta+x_{n}.\]

The two-step-ahead forecast is given by \(x_{n+2}^{n}=\delta+x_{n+1}^{n}=2\delta+x_{n}\), and consequently, the \(m\)-step-ahead forecast, for \(m=1,2,\ldots\), is

\[x_{n+m}^{n}=m\,\delta+x_{n}, \tag{3.146}\]

To obtain the forecast errors, it is convenient to recall equation (1.4); i.e., \(x_{n}=n\,\delta+\sum_{j=1}^{n}w_{j}\), in which case we may write

\[x_{n+m}=(n+m)\,\delta+\sum_{j=1}^{n+m}w_{j}=m\,\delta+x_{n}+\sum_{j=n+1}^{n+m} w_{j}.\]

From this it follows that the \(m\)-step-ahead prediction error is given by

\[P^{n}_{n+m}=\operatorname{E}(x_{n+m}-x_{n+m}^{n})^{2}=\operatorname{E}(\sum_{ j=n+1}^{n+m}w_{j})^{2}=m\,\sigma_{w}^{2}. \tag{3.147}\]

Hence, unlike the stationary case (see Example 3.23), as the forecast horizon grows, the prediction errors, (3.147), increase without bound and the forecasts follow a straight line with slope \(\delta\) emanating from \(x_{n}\). We note that (3.145) is exact in this case because \(\psi^{*}(z)=1/(1-z)=\sum_{j=0}^{\infty}z^{j}\) for \(|z|<1\), so that \(\psi_{j}^{*}=1\) for all \(j\).

The \(w_{t}\) are Gaussian, so estimation is straightforward because the differenced data, say \(y_{t}=\nabla x_{t}\), are independent and identically distributed normal variates with mean \(\delta\) and variance \(\sigma_{w}^{2}\). Consequently, optimal estimates of \(\delta\) and \(\sigma_{w}^{2}\) are the sample mean and variance of the \(y_{t}\), respectively.

**Example 3.38**: **IMA\((1,1)\) and EWMA**

The ARIMA(0,1,1), or IMA(1,1) model is of interest because many economic time series can be successfully modeled this way. In addition, the model leads to a frequently used, and abused, forecasting method called exponentially weighted moving averages (EWMA). We will write the model as

\[x_{t}=x_{t-1}+w_{t}-\lambda w_{t-1}, \tag{3.148}\]

with \(|\lambda|<1\), for \(t=1,2,\ldots\), and \(x_{0}=0\), because this model formulation is easier to work with here, and it leads to the standard representation for EWMA. We could have included a drift term in (3.148), as was done in the previous example, but for the sake of simplicity, we leave it out of the discussion. If we write

\[y_{t}=w_{t}-\lambda w_{t-1},\]

we may write (3.148) as \(x_{t}=x_{t-1}+y_{t}\). Because \(|\lambda|<1\), \(y_{t}\) has an invertible representation, \(y_{t}=\sum_{j=1}^{\infty}\lambda^{j}y_{t-j}+w_{t}\), and substituting \(y_{t}=x_{t}-x_{t-1}\), we may write

\[x_{t}=\sum_{j=1}^{\infty}(1-\lambda)\lambda^{j-1}x_{t-j}+w_{t}. \tag{3.149}\]

as an approximation for large \(t\) (put \(x_{t}=0\) for \(t\leq 0\)). Verification of (3.149) is left to the reader (Problem 3.28). Using the approximation (3.149), we have that the approximate one-step-ahead predictor, using the notation of Sect. 3.4, is

\[\tilde{x}_{n+1} =\sum_{j=1}^{\infty}(1-\lambda)\lambda^{j-1}x_{n+1-j}\] \[=(1-\lambda)x_{n}+\lambda\sum_{j=1}^{\infty}(1-\lambda)\lambda^{ j-1}x_{n-j}\] \[=(1-\lambda)x_{n}+\lambda\tilde{x}_{n}. \tag{3.150}\]

From (3.150), we see that the new forecast is a linear combination of the old forecast and the new observation. Based on (3.150) and the fact that we only observe \(x_{1},\ldots,x_{n}\), and consequently \(y_{1},\ldots,y_{n}\) (because \(y_{t}=x_{t}-x_{t-1};\ x_{0}=0\)), the truncated forecasts are

\[\tilde{x}_{n+1}^{n}=(1-\lambda)x_{n}+\lambda\tilde{x}_{n}^{n-1},\quad n\geq 1, \tag{3.151}\]

with \(\tilde{x}_{1}^{0}=x_{1}\) as an initial value. The mean-square prediction error can be approximated using (3.145) by noting that \(\psi^{*}(z)=(1-\lambda z)/(1-z)=1+(1-\lambda)\sum_{j=1}^{\infty}z^{j}\) for \(|z|<1\); consequently, for large \(n\), (3.145) leads to

\[P_{n+m}^{n}\approx\sigma_{w}^{2}[1+(m-1)(1-\lambda)^{2}].\]

In EWMA, the parameter \(1-\lambda\) is often called the smoothing parameter and is restricted to be between zero and one. Larger values of \(\lambda\) lead to smoother forecasts.

This method of forecasting is popular because it is easy to use; we need only retain the previous forecast value and the current observation to forecast the next time period. Unfortunately, as previously suggested, the method is often abused because some forecasters do not verify that the observations follow an IMA\((1,1)\) process, and often arbitrarily pick values of \(\lambda\). In the following, we show how to generate 100 observations from an IMA\((1,1)\) model with \(\lambda=-\theta=.8\) and then calculate and display the fitted EWMA superimposed on the data. This is accomplished using the Holt-Winters command in R (see the help file?HoltWinters for details; no output is shown):  set.seed(666) x = arima.sim(list(order = c(0,1,1), ma = -0.8), n = 100)  (x.ima = HoltWinters(x, beta-FALSE, gamma-FALSE)) # \(\alpha\) below is 1 - \(\lambda\)  Smoothing parameter: alpha: 0.1663072  plot(x.ima)

### 3.7 Building ARIMA Models

There are a few basic steps to fitting ARIMA models to time series data. These steps involve

* plotting the data,
* possibly transforming the data,
* identifying the dependence orders of the model,
* parameter estimation,
* diagnostics, and
* model choice.

First, as with any data analysis, we should construct a time plot of the data, and inspect the graph for any anomalies. If, for example, the variability in the data grows with time, it will be necessary to transform the data to stabilize the variance. In such cases, the Box-Cox class of power transformations, equation (2.34), could be employed. Also, the particular application might suggest an appropriate transformation. For example, we have seen numerous examples where the data behave as \(x_{t}=(1+p_{t})x_{t-1}\), where \(p_{t}\) is a small percentage change from period \(t-1\) to \(t\), which may be negative. If \(p_{t}\) is a relatively stable process, then \(\nabla\log(x_{t})\approx p_{t}\) will be relatively stable. Frequently, \(\nabla\log(x_{t})\) is called the _return_ or _growth rate_. This general idea was used in Example 3.33, and we will use it again in Example 3.39.

After suitably transforming the data, the next step is to identify preliminary values of the autoregressive order, \(p\), the order of differencing, \(d\), and the moving average order, \(q\). A time plot of the data will typically suggest whether any differencing is needed. If differencing is called for, then difference the data once, \(d=1\), and inspect the time plot of \(\nabla x_{t}\). If additional differencing is necessary, then try differencing again and inspect a time plot of \(\nabla^{2}x_{t}\). Be careful not to overdifference because this may introduce dependence where none exists. For example, \(x_{t}=w_{t}\) is serially uncorrelated, but \(\nabla x_{t}=w_{t}-w_{t-1}\) is MA\((1)\). In addition to time plots, the sample ACF can help in indicating whether differencing is needed. Because the polynomial \(\phi(z)(1-z)^{d}\) has a unit root, the sample ACF, \(\hat{\rho}(h)\), will not decay to zero fast as \(h\) increases. Thus, a slow decay in \(\hat{\rho}(h)\) is an indication that differencing may be needed.

When preliminary values of \(d\) have been settled, the next step is to look at the sample ACF and PACF of \(\nabla^{d}x_{t}\) for whatever values of \(d\) have been chosen. Using Table 3.1 as a guide, preliminary values of \(p\) and \(q\) are chosen. Note that it cannot be the case that both the ACF and PACF cut off. Because we are dealing with estimates, it will not always be clear whether the sample ACF or PACF is tailing off or cutting off. Also, two models that are seemingly different can actually be very similar. With this in mind, we should not worry about being so precise at this stage of the model fitting. At this point, a few preliminary values of \(p\), \(d\), and \(q\) should be at hand, and we can start estimating the parameters.

**Example 3.39**: **Analysis of GNP Data**

In this example, we consider the analysis of quarterly U.S. GNP from 1947(1) to 2002(3), \(n=223\) observations. The data are real U.S. gross national product in billions of chained 1996 dollars and have been seasonally adjusted. The data were obtained from the Federal Reserve Bank of St. Louis ([http://research.stlouisfed.org/](http://research.stlouisfed.org/)). Figure 3.13 shows a plot of the data, say, \(y_{t}\). Because strong trend tends to obscure other effects, it is difficult to see any other variability in data except for periodic large dips in the economy. When reports of GNP and similar economic indicators are given, it is often in growth rate (percent change) rather than in actual (or adjusted) values that is of interest. The growth rate, say, \(x_{t}=\nabla\log(y_{t})\), is plotted in Fig. 3.14, and it appears to be a stable process.

Figure 3.13: _Top:_ Quarterly U.S. GNP from 1947(1) to 2002(3). _Bottom:_ Sample ACF of the GNP data. Lag is in terms of years

The sample ACF and PACF of the quarterly growth rate are plotted in Fig. 3.15. Inspecting the sample ACF and PACF, we might feel that the ACF is cutting off at lag 2 and the PACF is tailing off. This would suggest the GNP growth rate follows an MA(2) process, or log GNP follows an ARIMA(\(0,1,2\)) model. Rather than focus on one model, we will also suggest that it appears that the ACF is tailing off and the PACF is cutting off at lag 1. This suggests an AR(1) model for the growth rate, or ARIMA(\(1,1,0\)) for log GNP. As a preliminary analysis, we will fit both models. Using MLE to fit the MA(2) model for the growth rate, \(x_{t}\), the estimated model is

\[\hat{x}_{t}=.008_{(.001)}+.303_{(.065)}\hat{w}_{t-1}+.204_{(.064)}\hat{w}_{t-2} +\hat{w}_{t}, \tag{3.152}\]

where \(\hat{\sigma}_{w}=.0094\) is based on 219 degrees of freedom. The values in parentheses are the corresponding estimated standard errors. All of the regression coefficients are significant, including the constant. _We make a special note of this because, as a default, some computer packages do not fit a constant in a differenced model._ That is, these packages assume, by default, that there is no drift. In this example, not including a constant leads to the wrong conclusions about the nature of the U.S. economy. Not including a constant assumes the average quarterly growth rate is zero, whereas the U.S. GNP average quarterly growth rate is about 1% (which can be seen easily in Fig. 3.14). We leave it to the reader to investigate what happens when the constant is not included.

The estimated AR(1) model is

\[\hat{x}_{t}=.008_{(.001)}\left(1-.347\right)+.347_{(.063)}\hat{x}_{t-1}+\hat{w }_{t}, \tag{3.153}\]

where \(\hat{\sigma}_{w}=.0095\) on 220 degrees of freedom; note that the constant in (3.153) is \(.008\left(1-.347\right)=.005\).

We will discuss diagnostics next, but assuming both of these models fit well, how are we to reconcile the apparent differences of the estimated models (3.152)

Figure 3.14: U.S. GNP quarterly growth rate. The _horizontal line_ displays the average growth of the process, which is close to 1%

and (3.153)? In fact, the fitted models are nearly the same. To show this, consider an AR(1) model of the form in (3.153) without a constant term; that is,

\[x_{t}=.35x_{t-1}+w_{t},\]

and write it in its causal form, \(x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}\), where we recall \(\psi_{j}=.35^{j}\). Thus, \(\psi_{0}=1,\psi_{1}=.350,\psi_{2}=.123,\psi_{3}=.043,\psi_{4}=.015,\psi_{5}=.0 05,\psi_{6}=.002,\psi_{7}=.001,\psi_{8}=0,\psi_{9}=0,\psi_{10}=0\), and so forth. Thus,

\[x_{t}\approx.35w_{t-1}+.12w_{t-2}+w_{t},\]

which is similar to the fitted MA(2) model in (3.153).

The analysis can be performed in R as follows.

plot(gnp)

acf2(gnp, 50)

gnppr = diff(log(gnp)) # growth rate

plot(gnppr)

acf2(gnppr, 24)

sarima(gnppr, 1, 0, 0) # AR(1)

sarima(gnppr, 0, 0, 2) # MA(2)

ARMAtoMA(ar-.35, ma=0, 10) # prints psi-weights

The next step in model fitting is diagnostics. This investigation includes the analysis of the residuals as well as model comparisons. Again, the first step involves a time plot of the _innovations_ (or residuals), \(x_{t}-\hat{x}_{t}^{t-1}\), or of the _standardized innovations_

\[e_{t}=\left(x_{t}-\hat{x}_{t}^{t-1}\right)/\sqrt{\hat{P}_{t}^{t-1}}, \tag{3.154}\]

where \(\hat{x}_{t}^{t-1}\) is the one-step-ahead prediction of \(x_{t}\) based on the fitted model and \(\hat{P}_{t}^{t-1}\) is the estimated one-step-ahead error variance. If the model fits well, the standardized

Figure 3.15: Sample ACF and PACF of the GNP quarterly growth rate. Lag is in terms of years

residuals should behave as an iid sequence with mean zero and variance one. The time plot should be inspected for any obvious departures from this assumption. Unless the time series is Gaussian, it is not enough that the residuals are uncorrelated. For example, it is possible in the non-Gaussian case to have an uncorrelated process for which values contiguous in time are highly dependent. As an example, we mention the family of GARCH models that are discussed in Chap. 5.

Investigation of marginal normality can be accomplished visually by looking at a histogram of the residuals. In addition to this, a normal probability plot or a Q-Q plot can help in identifying departures from normality. See Johnson and Wichern [106, Chapter 4] for details of this test as well as additional tests for multivariate normality.

There are several tests of randomness, for example the runs test, that could be applied to the residuals. We could also inspect the sample autocorrelations of the residuals, say, \(\hat{\rho}_{e}(h)\), for any patterns or large values. Recall that, for a white noise sequence, the sample autocorrelations are approximately independently and normally distributed with zero means and variances \(1/n\). Hence, a good check on the correlation structure of the residuals is to plot \(\hat{\rho}_{e}(h)\) versus \(h\) along with the error bounds of \(\pm 2/\sqrt{n}\). The residuals from a model fit, however, will not quite have the properties of a white noise sequence and the variance of \(\hat{\rho}_{e}(h)\) can be much less than \(1/n\). Details can be found in Box and Pierce [29] and McLeod [137]. This part of the diagnostics can be viewed as a visual inspection of \(\hat{\rho}_{e}(h)\) with the main concern being the detection of obvious departures from the independence assumption.

In addition to plotting \(\hat{\rho}_{e}(h)\), we can perform a general test that takes into consideration the magnitudes of \(\hat{\rho}_{e}(h)\) as a group. For example, it may be the case that, individually, each \(\hat{\rho}_{e}(h)\) is small in magnitude, say, each one is just slightly less that \(2/\sqrt{n}\) in magnitude, but, collectively, the values are large. The _Ljung-Box-Pierce Q-statistic_ given by

\[Q=n(n+2)\sum_{h=1}^{H}\ \frac{\hat{\rho}_{e}^{2}(h)}{n-h} \tag{3.155}\]

can be used to perform such a test. The value \(H\) in (3.155) is chosen somewhat arbitrarily, typically, \(H=20\). Under the null hypothesis of model adequacy, asymptotically (\(n\to\infty\)), \(Q\sim\chi^{2}_{H-p-q}\). Thus, we would reject the null hypothesis at level \(\alpha\) if the value of \(Q\) exceeds the \((1-\alpha)\)-quantile of the \(\chi^{2}_{H-p-q}\) distribution. Details can be found in Box and Pierce [30], Ljung and Box [129], and Davies et al. [49]. The basic idea is that if \(w_{t}\) is white noise, then by Property 1.2, \(n\hat{\rho}_{w}^{2}(h)\), for \(h=1,\ldots,H\), are asymptotically independent \(\chi^{2}_{1}\) random variables. This means that \(n\sum_{h=1}^{H}\hat{\rho}_{w}^{2}(h)\) is approximately a \(\chi^{2}_{H}\) random variable. Because the test involves the ACF of residuals from a model fit, there is a loss of \(p+q\) degrees of freedom; the other values in (3.155) are used to adjust the statistic to better match the asymptotic chi-squared distribution.

**Example 3.40**: **Diagnostics for GNP Growth Rate Example**

We will focus on the MA(2) fit from Example 3.39; the analysis of the AR(1) residuals is similar. Figure 3.16 displays a plot of the standardized residuals, the ACF of the residuals, a boxplot of the standardized residuals, and the p-valuesassociated with the Q-statistic, (3.155), at lags \(H=3\) through \(H=20\) (with corresponding degrees of freedom \(H-2\)).

Inspection of the time plot of the standardized residuals in Fig. 3.16 shows no obvious patterns. Notice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude. The ACF of the standardized residuals shows no apparent departure from the model assumptions, and the Q-statistic is never significant at the lags shown. The normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the possible outliers.

The model appears to fit well. The diagnostics shown in Fig. 3.16 are a by-product of the sarima command from the previous example.8

Footnote 8: The script tsdiag is available in R to run diagnostics for an ARIMA object, however, the script has errors and we do not recommend using it.

#### 3.4.2 Diagnostics for the Glacial Varve Series

In Example 3.33, we fit an ARIMA\((0,1,1)\) model to the logarithms of the glacial varve data and there appears to be a small amount of autocorrelation left in the residuals and the Q-tests are all significant; see Fig. 3.17.

Figure 3.16: Diagnostics of the residuals from MA(2) fit on GNP growth rate

To adjust for this problem, we fit an ARIMA\((1,1,1)\) to the logged curve data and obtained the estimates

\[\hat{\phi}=.23_{(.05)},\ \hat{\theta}=-.89_{(.03)},\ \hat{\sigma}_{w}^{2}=.23.\]

Hence the AR term is significant. The Q-statistic p-values for this model are also displayed in Fig. 3.17, and it appears this model fits the data well.

As previously stated, the diagnostics are byproducts of the individual sarima runs. We note that we did not fit a constant in either model because there is no apparent drift in the differenced, logged curve series. This fact can be verified by noting the constant is not significant when the command no.constant=TRUE is removed in the code:

```
sarima(log(varve),0,1,1,no.constant=TRUE)#ARIMA(0,1,1) sarima(log(varve),1,1,1,no.constant=TRUE)#ARIMA(1,1,1)
```

In Example 3.39, we have two competing models, an AR(1) and an MA(2) on the GNP growth rate, that each appear to fit the data well. In addition, we might also consider that an AR(2) or an MA(3) might do better for forecasting. Perhaps combining both models, that is, fitting an ARMA(1,2) to the GNP growth rate, would be the best. As previously mentioned, we have to be concerned with _overfitting_ the model; it is not always the case that more is better. Overfitting leads to less-precise estimators, and adding more parameters may fit the data better but may also lead to bad forecasts. This result is illustrated in the following example.

**Example 3.42**: **A Problem with Overfitting**

Figure 3.18 shows the U.S. population by official census, every ten years from 1910 to 1990, as points. If we use these nine observations to predict the future population, we can use an eight-degree polynomial so the fit to the nine observations is perfect. The model in this case is

\[x_{t}=\beta_{0}+\beta_{1}t+\beta_{2}t^{2}+\cdots+\beta_{8}t^{8}+w_{t}.\]

Figure 3.17: Q-statistic p-values for the ARIMA\((0,1,1)\) fit (_top_) and the ARIMA\((1,1,1)\) fit (_bottom_) to the logged curve data

The fitted line, which is plotted in the figure, passes through the nine observations. The model predicts that the population of the United States will be close to zero in the year 2000, and will cross zero sometime in the year 2002!

The final step of model fitting is model choice or model selection. That is, we must decide which model we will retain for forecasting. The most popular techniques, AIC, AICc, and BIC, were described in Sect. 2.1 in the context of regression models.

**Example 3.43**: **Model Choice for the U.S. GNP Series**

Returning to the analysis of the U.S. GNP data presented in Example 3.39 and Example 3.40, recall that two models, an AR(1) and an MA(2), fit the GNP growth rate well. To choose the final model, we compare the AIC, the AICc, and the BIC for both models. These values are a byproduct of the sarima runs displayed at the end of Example 3.39, but for convenience, we display them again here (recall the growth rate data are in gnpqr):

 sarima(gnpqr, 1, 0, 0) # AR(1)

  SAIC: -8.294403 $AICc: -8.284898 $BIC: -9.263748

  sarima(gnpqr, 0, 0, 2) # MA(2)

  SAIC: -8.297693 $AICc: -8.287854 $BIC: -9.251711

The AIC and AICc both prefer the MA(2) fit, whereas the BIC prefers the simpler AR(1) model. It is often the case that the BIC will select a model of smaller order than the AIC or AICc. In either case, it is not unreasonable to retain the AR(1) because pure autoregressive models are easier to work with.

### Regression with Autocorrelated Errors

In Sect. 2.1, we covered the classical regression model with uncorrelated errors \(w_{t}\). In this section, we discuss the modifications that might be considered when the errors are correlated. That is, consider the regression model

\[y_{t}=\sum_{j=1}^{r}\beta_{j}z_{tj}+x_{t} \tag{3.156}\]

Figure 3.18: A perfect fit and a terrible forecast

where \(x_{t}\) is a process with some covariance function \(\gamma_{x}(s,t)\). In ordinary least squares, the assumption is that \(x_{t}\) is white Gaussian noise, in which case \(\gamma_{x}(s,t)=0\) for \(s\neq t\) and \(\gamma_{x}(t,t)=\sigma^{2}\), independent of \(t\). If this is not the case, then weighted least squares should be used.

Write the model in vector notation, \(y=Z\beta+x\), where \(y=(y_{1},\ldots,y_{n})^{\prime}\) and \(x=(x_{1},\ldots,x_{n})^{\prime}\) are \(n\times 1\) vectors, \(\beta=(\beta_{1},\ldots,\beta_{r})^{\prime}\) is \(r\times 1\), and \(Z=[z_{1}\mid z_{2}\mid\cdots\mid z_{n}]^{\prime}\) is the \(n\times r\) matrix composed of the input variables. Let \(\Gamma=\{\gamma_{x}(s,t)\}\), then \(\Gamma^{-1/2}y=\Gamma^{-1/2}Z\beta+\Gamma^{-1/2}x\), so that we can write the model as

\[y^{*}=Z^{*}\beta+\delta\,,\]

where \(y^{*}=\Gamma^{-1/2}y\), \(Z^{*}=\Gamma^{-1/2}Z\), and \(\delta=\Gamma^{-1/2}x\). Consequently, the covariance matrix of \(\delta\) is the identity and the model is in the classical linear model form. It follows that the weighted estimate of \(\beta\) is \(\hat{\beta}_{w}=(Z^{*^{\prime}}Z^{*})^{-1}Z^{*^{\prime}}y^{*}=(Z^{\prime}\Gamma ^{-1}Z)^{-1}Z^{\prime}\Gamma^{-1}y\), and the variance-covariance matrix of the estimator is \(\text{var}(\hat{\beta}_{w})=(Z^{\prime}\Gamma^{-1}Z)^{-1}\). If \(x_{t}\) is white noise, then \(\Gamma=\sigma^{2}I\) and these results reduce to the usual least squares results.

In the time series case, it is often possible to assume a stationary covariance structure for the error process \(x_{t}\) that corresponds to a linear process and try to find an ARMA representation for \(x_{t}\). For example, if we have a pure AR(\(p\)) error, then

\[\phi(B)x_{t}=w_{t},\]

and \(\phi(B)=1-\phi_{1}B-\cdots-\phi_{p}B^{p}\) is the linear transformation that, when applied to the error process, produces the white noise \(w_{t}\). Multiplying the regression equation through by the transformation \(\phi(B)\) yields,

\[\underbrace{\phi(B)y_{t}}_{y_{t}^{*}}=\sum_{j=1}^{r}\beta_{j}\underbrace{\phi (B)z_{tj}}_{z_{tj}^{*}}+\underbrace{\phi(B)x_{t}}_{w_{t}},\]

and we are back to the linear regression model where the observations have been transformed so that \(y_{t}^{*}=\phi(B)y_{t}\) is the dependent variable, \(z_{tj}^{*}=\phi(B)z_{tj}\) for \(j=1,\ldots,r\), are the independent variables, but the \(\beta\)s are the same as in the original model. For example, if \(p=1\), then \(y_{t}^{*}=y_{t}-\phi y_{t-1}\) and \(z_{tj}^{*}=z_{tj}-\phi z_{t-1,j}\).

In the AR case, we may set up the least squares problem as minimizing the error sum of squares

\[S(\phi,\beta)=\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}\Bigl{[}\phi(B)y_{t}-\sum _{j=1}^{r}\beta_{j}\phi(B)z_{tj}\Bigr{]}^{2}\]

with respect to all the parameters, \(\phi=\{\phi_{1},\ldots,\phi_{p}\}\) and \(\beta=\{\beta_{1},\ldots,\beta_{r}\}\). Of course, the optimization is performed using numerical methods.

If the error process is ARMA(\(p,q\)), i.e., \(\phi(B)x_{t}=\theta(B)w_{t}\), then in the above discussion, we transform by \(\pi(B)x_{t}=w_{t}\), where \(\pi(B)=\theta(B)^{-1}\phi(B)\). In this case the error sum of squares also depends on \(\theta=\{\theta_{1},\ldots,\theta_{q}\}\):

\[S(\phi,\theta,\beta)=\sum_{t=1}^{n}w_{t}^{2}=\sum_{t=1}^{n}\Bigl{[}\pi(B)y_{t}- \sum_{j=1}^{r}\beta_{j}\pi(B)z_{tj}\Bigr{]}^{2}\]

At this point, the main problem is that we do not typically know the behavior of the noise \(x_{t}\) prior to the analysis. An easy way to tackle this problem was first presented in Cochrane and Orcut [43], and with the advent of cheap computing is modernized below:

1. First, run an ordinary regression of \(y_{t}\) on \(z_{t1},\ldots,z_{tr}\) (acting as if the errors are uncorrelated). Retain the residuals, \(\hat{x}_{t}=y_{t}-\sum_{j=1}^{r}\hat{\beta}_{j}z_{tj}\).
2. Identify ARMA model(s) for the residuals \(\hat{x}_{t}\).
3. Run weighted least squares (or MLE) on the regression model with autocorrelated errors using the model specified in step (ii).
4. Inspect the residuals \(\hat{w}_{t}\) for whiteness, and adjust the model if necessary.

**Example 3.44**: **Mortality, Temperature and Pollution**

We consider the analyses presented in Example 2.2, relating mean adjusted temperature \(T_{t}\), and particulate levels \(P_{t}\) to cardiovascular mortality \(M_{t}\). We consider the regression model

\[M_{t}=\beta_{1}+\beta_{2}t+\beta_{3}T_{t}+\beta_{4}T_{t}^{2}+\beta_{5}P_{t}+x_ {t}, \tag{3.157}\]

where, for now, we assume that \(x_{t}\) is white noise. The sample ACF and PACF of the residuals from the ordinary least squares fit of (3.157) are shown in Fig. 3.19, and the results suggest an AR(2) model for the residuals.

Our next step is to fit the correlated error model (3.157), but where \(x_{t}\) is AR(2),

\[x_{t}=\phi_{1}x_{t-1}+\phi_{2}x_{t-2}+w_{t}\]

and \(w_{t}\) is white noise. The model can be fit using the sarima function as follows (partial output shown).

 trend = time(cmort); temp = tempr - mean(tempr); temp2 = temp^2  summary(fit <- lm(cmort-trend + temp + temp2 + part, na.action=NULL))  acf2(resid(fit), 52) # implies AR2  sarima(cmort, 2,0,0, xreg=cbind(trend,temp,temp2,part))  Coefficients:  ar1 ar2 intercept trend temp temp2 part  0.3848 0.4326 80.2116 -1.5165 -0.0190 0.0154 0.1545  s.e. 0.0436 0.0400 1.8072 0.4226 0.0495 0.0020 0.0272  sigma^2 estimated as 26.01: loglikelihood = -1549.04, aic = 3114.07 The residual analysis output from sarima (not shown) shows no obvious departure of the residuals from whiteness.

**Example 3.45**: **Regression with Lagged Variables (cont)**

In Example 2.9 we fit the model

\[R_{t}=\beta_{0}+\beta_{1}S_{t-6}+\beta_{2}D_{t-6}+\beta_{3}D_{t-6}\,S_{t-6}+w_{t},\]

where \(R_{t}\) is Recruitment, \(S_{t}\) is SOI, and \(D_{t}\) is a dummy variable that is 0 if \(S_{t}<0\) and 1 otherwise. However, residual analysis indicates that the residuals are not white noise. The sample (P)ACF of the residuals indicates that an AR(2) model might be appropriate, which is similar to the results of Example 3.44. We display partial results of the final model below.

```
dummy=ifelse(soi<0,0,1) fish=ts.intersect(rec,soil6=lag(soi,-6),dL6=lag(dummy,-6),dframe=TRUE) summary(fit<-lm(rec~soil6*dL6,data=fish,na.action=NULL)) attach(fish) plot(resid(fit)) acf2(resid(fit))
#indicatesAR(2) intract=soil6*dL6#interactionterm sarima(rec,2,0,0,xreg=cbind(soil6,dL6,intract)) Sttable EstimateSEt.valuep.value ar11.36240.044030.93030.0000 ar2-0.47030.0444-10.59020.0000 intercept64.80284.11215.75900.0000 soiL68.66712.22053.90330.0001 dL6-2.59450.9535-2.72090.0068 intract-10.3092.8311-3.64150.0003
```

### 3.9 Multiplicative Seasonal ARIMA Models

In this section, we introduce several modifications made to the ARIMA model to account for seasonal and nonstationary behavior. Often, the dependence on the past tends to occur most strongly at multiples of some underlying seasonal lag \(s\)

Figure 3.19: Sample ACF and PACF of the mortality residuals indicating an AR(2) processFor example, with monthly economic data, there is a strong yearly component occurring at lags that are multiples of \(s=12\), because of the strong connections of all activity to the calendar year. Data taken quarterly will exhibit the yearly repetitive period at \(s=4\) quarters. Natural phenomena such as temperature also have strong components corresponding to seasons. Hence, the natural variability of many physical, biological, and economic processes tends to match with seasonal fluctuations. Because of this, it is appropriate to introduce autoregressive and moving average polynomials that identify with the seasonal lags. The resulting _pure seasonal autoregressive moving average model_, say, ARMA(\(P,Q\))\({}_{s}\), then takes the form

\[\Phi_{P}(B^{s})x_{t}=\Theta_{Q}(B^{s})w_{t}, \tag{3.158}\]

where the operators

\[\Phi_{P}(B^{s})=1-\Phi_{1}B^{s}-\Phi_{2}B^{2s}-\cdots-\Phi_{P}B^{Ps} \tag{3.159}\]

and

\[\Theta_{Q}(B^{s})=1+\Theta_{1}B^{s}+\Theta_{2}B^{2s}+\cdots+\Theta_{Q}B^{Qs} \tag{3.160}\]

are the **seasonal autoregressive operator** and the **seasonal moving average operator** of orders \(P\) and \(Q\), respectively, with seasonal period \(s\).

Analogous to the properties of nonseasonal ARMA models, the pure seasonal ARMA(\(P,Q\))\({}_{s}\) is _causal_ only when the roots of \(\Phi_{P}(z^{s})\) lie outside the unit circle, and it is _invertible_ only when the roots of \(\Theta_{Q}(z^{s})\) lie outside the unit circle.

**Example 3.46**: **A Seasonal AR Series**

A first-order seasonal autoregressive series that might run over months could be written as

\[(1-\Phi B^{12})x_{t}=w_{t}\]

or

\[x_{t}=\Phi x_{t-12}+w_{t}.\]

This model exhibits the series \(x_{t}\) in terms of past lags at the multiple of the yearly seasonal period \(s=12\) months. It is clear from the above form that estimation and forecasting for such a process involves only straightforward modifications of the unit lag case already treated. In particular, the causal condition requires \(|\Phi|<1\).

We simulated 3 years of data from the model with \(\Phi=.9\), and exhibit the _theoretical_ ACF and PACF of the model. See Fig. 3.20.

set.seed(666)

phi = c(rep(0,11),.9)

sAR = arima.sim(list(order=c(12,0,0), ar=phi), n=37)

sAR = ts(sAR, freq=12)

layout(matrix(c(1,1,2,1,1,3), nc=2))

par(mar(2,3,2,1), mgp=c(1,6,.6,0))

plot(sAR, axes=FALSE, main='seasonal AR(1)', xlab="year", type='c')

Months = c("J","E","M","A","J","J","A","S","O","N","D")

points(sAR, pch=Months, cex=1.25, font=4, col=1:4)

axis(1, 1:4); abline(v=1:4, lty=2, col=gray(.7))

axis(2); box()ACF = ARMAacf(ar=phi, ma=0, 100) PACF = ARMAacf(ar=phi, ma=0, 100, pacf=TRUE) plot(ACF,type="h", xlab="LAG", ylim<c(-.1,1)); abline(h=0) plot(PACF, type="h", xlab="LAG", ylim<c(-.1,1)); abline(h=0)

For the first-order seasonal (\(s=12\)) MA model, \(x_{t}=w_{t}+\Theta w_{t-12}\), it is easy to verify that

\[\gamma(0) = (1+\Theta^{2})\sigma^{2}\] \[\gamma(\pm 12) = \Theta\sigma^{2}\] \[\gamma(h) = 0,\quad\text{ otherwise.}\]

Thus, the only nonzero correlation, aside from lag zero, is

\[\rho(\pm 12)=\Theta/(1+\Theta^{2}).\]

For the first-order seasonal (\(s=12\)) AR model, using the techniques of the nonseasonal AR(1), we have

\[\gamma(0) = \sigma^{2}/(1-\Phi^{2})\] \[\gamma(\pm 12k) = \sigma^{2}\Phi^{k}/(1-\Phi^{2})\quad k=1,2,\ldots\] \[\gamma(h) = 0,\quad\text{ otherwise.}\]

In this case, the only non-zero correlations are

\[\rho(\pm 12k)=\Phi^{k},\quad k=0,1,2,\ldots\.\]

These results can be verified using the general result that \(\gamma(h)=\Phi\gamma(h-12)\), for \(h\geq 1\). For example, when \(h=1\), \(\gamma(1)=\Phi\gamma(11)\), but when \(h=11\), we have \(\gamma(11)=\Phi\gamma(1)\)

Figure 3.20: Data generated from a seasonal (\(s=12\)) AR(1), and the true ACF and PACF of the model \(x_{t}=.9x_{t-12}+w_{t}\)

which implies that \(\gamma(1)=\gamma(11)=0\). In addition to these results, the PACF have the analogous extensions from nonseasonal to seasonal models. These results are demonstrated in Fig. 3.20.

As an initial diagnostic criterion, we can use the properties for the pure seasonal autoregressive and moving average series listed in Table 3.3. These properties may be considered as generalizations of the properties for nonseasonal models that were presented in Table 3.1.

In general, we can combine the seasonal and nonseasonal operators into a _multiplicative seasonal autoregressive moving average model_, denoted by \(\text{ARMA}(p,q)\times(P,Q)_{s}\), and write

\[\varPhi_{P}(B^{s})\phi(B)x_{t}=\varTheta_{Q}(B^{s})\theta(B)w_{t} \tag{3.161}\]

as the overall model. Although the diagnostic properties in Table 3.3 are not strictly true for the overall mixed model, the behavior of the ACF and PACF tends to show rough patterns of the indicated form. In fact, for mixed models, we tend to see a mixture of the facts listed in Tables 3.1 and 3.3. In fitting such models, focusing on the seasonal autoregressive and moving average components first generally leads to more satisfactory results.

**Example 3.47**: **A Mixed Seasonal Model**

Consider an \(\text{ARMA}(0,1)\times(1,0)_{12}\) model

\[x_{t}=\varPhi x_{t-12}+w_{t}+\theta w_{t-1},\]

where \(|\varPhi|<1\) and \(|\theta|<1\). Then, because \(x_{t-12}\), \(w_{t}\), and \(w_{t-1}\) are uncorrelated, and \(x_{t}\) is stationary, \(\gamma(0)=\varPhi^{2}\gamma(0)+\sigma_{w}^{2}+\theta^{2}\sigma_{w}^{2}\), or

\[\gamma(0)=\frac{1+\theta^{2}}{1-\varPhi^{2}}\;\sigma_{w}^{2}.\]

In addition, multiplying the model by \(x_{t-h}\), \(h>0\), and taking expectations, we have \(\gamma(1)=\varPhi\gamma(11)+\theta\sigma_{w}^{2}\), and \(\gamma(h)=\varPhi\gamma(h-12)\), for \(h\geq 2\). Thus, the ACF for this model is

\[\rho(12h) =\varPhi^{h}\quad h=1,2,\ldots\] \[\rho(12h-1) =\rho(12h+1)=\frac{\theta}{1+\theta^{2}}\varPhi^{h}\quad h=0,1,2,\ldots,\] \[\rho(h) =0,\quad\text{otherwise}.\]

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \(\text{AR}(P)_{s}\) & \(\text{MA}(Q)_{s}\) & \(\text{ARMA}(P,Q)_{s}\) \\ \hline ACF* & Tails off at lags \(ks\), & Cuts off after & Tails off at \\  & \(k=1,2,\ldots\), & lag \(Qs\) & lags \(ks\) \\ PACF* & Cuts off after & Tails off at lags \(ks\) & Tails off at \\  & lag \(Ps\) & \(k=1,2,\ldots\), & lags \(ks\) \\ \hline \multicolumn{4}{l}{*The values at nonseasonal lags \(h\neq ks\), for \(k=1,2,\ldots\), are zero} \\ \end{tabular}
\end{table}
Table 3.3: Behavior of the ACF and PACF for pure SARMA modelsThe ACF and PACF for this model, with \(\Phi=.8\) and \(\theta=-.5\), are shown in Fig. 3.21. These type of correlation relationships, although idealized here, are typically seen with seasonal data.

To reproduce Fig. 3.21 in R, use the following commands:

phi = c(rep(0,11),.8) ACF = ARMAcf(ar-phi, ma=-.5, 50)[-1] # [-1] removes @lag PACF = ARMAcf(ar-phi, ma=-.5, 50, pacf-TRUE) par(mfromw=c(1,2)) plot(ACF, type='h", xlab="LAG", ylim=c(-.4,.8)); abline(h=0) plot(PACF, type='h", xlab="LAG", ylim=c(-.4,.8)); abline(h=0)

Seasonal persistence occurs when the process is nearly periodic in the season. For example, with average monthly temperatures over the years, each January would be approximately the same, each February would be approximately the same, and so on. In this case, we might think of average monthly temperature \(x_{t}\) as being modeled as

\[x_{t}=S_{t}+w_{t},\]

where \(S_{t}\) is a seasonal component that varies a little from one year to the next, according to a random walk,

\[S_{t}=S_{t-12}+v_{t}.\]

In this model, \(w_{t}\) and \(v_{t}\) are uncorrelated white noise processes. The tendency of data to follow this type of model will be exhibited in a sample ACF that is large and decays very slowly at lags \(h=12k\), for \(k=1,2,\dots\). If we subtract the effect of successive years from each other, we find that

\[(1-B^{12})x_{t}=x_{t}-x_{t-12}=v_{t}+w_{t}-w_{t-12}.\]

This model is a stationary MA(1)\({}_{12}\), and its ACF will have a peak only at lag 12. In general, seasonal differencing can be indicated when the ACF decays slowly at multiples of some season \(s\), but is negligible between the periods. Then, a _seasonal

Figure 3.21: ACF and PACF of the mixed seasonal ARMA model \(x_{t}=.8x_{t-12}+w_{t}-.5w_{t-1}\)

difference of order \(D\)_ is defined as

\[\nabla_{s}^{D}x_{t}=(1-B^{s})^{D}x_{t}, \tag{3.162}\]

where \(D=1,2,\ldots\), takes positive integer values. Typically, \(D=1\) is sufficient to obtain seasonal stationarity. Incorporating these ideas into a general model leads to the following definition.

**Definition 3.12**: _The multiplicative seasonal autoregressive integrated moving average model, or_ **SARIMA** _model is given by_

\[\Phi_{P}(B^{s})\phi(B)\nabla_{s}^{D}\nabla^{d}x_{t}=\delta+\Theta_{Q}(B^{s}) \theta(B)w_{t}, \tag{3.163}\]

_where \(w_{t}\) is the usual Gaussian white noise process. The general model is denoted as \(\mathbf{ARIMA}(p,d,q)\times(P,D,Q)_{s}\). The ordinary autoregressive and moving average components are represented by polynomials \(\phi(B)\) and \(\theta(B)\) of orders \(p\) and \(q\), respectively, and the seasonal autoregressive and moving average components by \(\Phi_{P}(B^{s})\) and \(\Theta_{Q}(B^{s})\) of orders \(P\) and \(Q\) and ordinary and seasonal difference components by \(\nabla^{d}=(1-B)^{d}\) and \(\nabla_{s}^{D}=(1-B^{s})^{D}\)._

**Example 3.48**: **An SARIMA Model**

Consider the following model, which often provides a reasonable representation for seasonal, nonstationary, economic time series. We exhibit the equations for the model, denoted by ARIMA\((0,1,1)\times(0,1,1)_{12}\) in the notation given above, where the seasonal fluctuations occur every 12 months. Then, with \(\delta=0\), the model (3.163) becomes

\[\nabla_{12}\nabla x_{t}=\Theta(B^{12})\theta(B)w_{t}\]

or

\[(1-B^{12})(1-B)x_{t}=(1+\Theta B^{12})(1+\theta B)w_{t}. \tag{3.164}\]

Expanding both sides of (3.164) leads to the representation

\[(1-B-B^{12}+B^{13})x_{t}=(1+\theta B+\Theta B^{12}+\Theta\theta B^{13})w_{t},\]

or in difference equation form

\[x_{t}=x_{t-1}+x_{t-12}-x_{t-13}+w_{t}+\theta w_{t-1}+\Theta w_{t-12}+\Theta \theta w_{t-13}.\]

Note that the multiplicative nature of the model implies that the coefficient of \(w_{t-13}\) is the product of the coefficients of \(w_{t-1}\) and \(w_{t-12}\) rather than a free parameter. The multiplicative model assumption seems to work well with many seasonal time series data sets while reducing the number of parameters that must be estimated.

Selecting the appropriate model for a given set of data from all of those represented by the general form (3.163) is a daunting task, and we usually think first in terms of finding difference operators that produce a roughly stationary series and then in terms of finding a set of simple autoregressive moving average or multiplicative seasonal ARMA to fit the resulting residual series. Differencing operations are applied first, and then the residuals are constructed from a series of reduced length. Next, the ACF and the PACF of these residuals are evaluated. Peaks that appear in these functions can often be eliminated by fitting an autoregressive or moving average component in accordance with the general properties of Tables 3.1 and 3.3. In considering whether the model is satisfactory, the diagnostic techniques discussed in Sect. 3.7 still apply.

**Example 3.49**: **Air Passengers**

We consider the R data set AirPassengers, which are the monthly totals of international airline passengers, 1949 to 1960, taken from Box and Jenkins [30]. Various plots of the data and transformed data are shown in Fig. 3.22 and were obtained as follows:

x = AirPassengers lx = log(x); dlx = diff(lx); ddlx = diff(dlx, 12) plot.ts(cbind(x,lx,dlx,ddlx), main="")
below of interest for showing seasonal RW (not shown here): par(mfrow=c(2,1)) monthplot(dlx); monthplot(ddlx)

Figure 3.22: R data set AirPassengers, which are the monthly totals of international airline passengers x, and the transformed data: \(\mathtt{lx}=\log x_{t}\), \(\mathtt{dlx}=\nabla\log x_{t}\), and \(\mathtt{ddlx}=\nabla_{12}\nabla\log x_{t}\)

Note that x is the original series, which shows trend plus increasing variance. The logged data are in 1x, and the transformation stabilizes the variance. The logged data are then differenced to remove trend, and are stored in dlx. It is clear the there is still persistence in the seasons (i.e., \(\texttt{dlx}_{t}\approx\texttt{dlx}_{t-12}\)), so that a twelfth-order difference is applied and stored in ddlx. The transformed data appears to be stationary and we are now ready to fit a model.

The sample ACF and PACF of ddlx (\(\nabla_{12}\nabla\log x_{t}\)) are shown in Fig. 3.23. The R code is:

acf2(ddlx,50)

_Seasonsal Component:_ It appears that at the seasons, the ACF is cutting off a lag \(1s\) (\(s=12\)), whereas the PACF is tailing off at lags \(1s,2s,3s,4s,\dots\). These results implies an SMA(1), \(P=0\), \(Q=1\), in the season (\(s=12\)).

_Non-Seasonsal Component:_ Inspecting the sample ACF and PACF at the lower lags, it appears as though both are tailing off. This suggests an ARMA(\(1,1\)) within the seasons, \(p=q=1\).

Thus, we first try an ARIMA(\(1,1,1\)) \(\times\) (\(0,1,1\))\({}_{12}\) on the logged data:

sarima(lx, 1,1,1,0,1,1,12)

Coefficients:

ar1 mal sma1

0.1960 -0.5784 -0.5643

s.e. 0.2475 0.2132 0.0747

sigma^2 estimated as 0.001341

SAIC -5.5726 5AICC -5.556713 5BIC -6.510729

However, the AR parameter is not significant, so we should try dropping one parameter from the within seasons part. In this case, we try both an ARIMA(\(0,1,1\))\(\times\) (\(0,1,1\))\({}_{12}\) and an ARIMA(\(1,1,0\)) \(\times\) (\(0,1,1\))\({}_{12}\) model:

sarima(lx, 0,1,1,0,1,1,12)

Coefficients:

mal sma1

-0.4018 -0.5569

s.e. 0.0896 0.0731

[MISSING_PAGE_EMPTY:165]

## Problems

### Section 3.1

**3.1** For an MA(1), \(x_{t}=w_{t}+\theta w_{t-1}\), show that \(|\rho_{x}(1)|\leq 1/2\) for any number \(\theta\). For which values of \(\theta\) does \(\rho_{x}(1)\) attain its maximum and minimum?

**3.2** Let \(\{w_{t};t=0,1,\dots\}\) be a white noise process with variance \(\sigma_{w}^{2}\) and let \(|\phi|<1\) be a constant. Consider the process \(x_{0}=w_{0}\), and

\[x_{t}=\phi x_{t-1}+w_{t},\quad t=1,2,\dots\.\]

We might use this method to simulate an AR(1) process from simulated white noise.

(a) Show that \(x_{t}=\sum_{j=0}^{t}\phi^{j}w_{t-j}\) for any \(t=0,1,\dots\).

(b) Find the E(\(x_{t}\)).

(c) Show that, for \(t=0,1,\dots\),

\[\text{var}(x_{t})=\frac{\sigma_{w}^{2}}{1-\phi^{2}}(1-\phi^{2(t+1)})\]

(d) Show that, for \(h\geq 0\),

\[\text{cov}(x_{t+h},x_{t})=\phi^{h}\text{var}(x_{t})\]

(e) Is \(x_{t}\) stationary?

(f) Argue that, as \(t\to\infty\), the process becomes stationary, so in a sense, \(x_{t}\) is "asymptotically stationary."

(g) Comment on how you could use these results to simulate \(n\) observations of a stationary Gaussian AR(1) model from simulated iid N(0,1) values.

(h) Now suppose \(x_{0}=w_{0}/\sqrt{1-\phi^{2}}\). Is this process stationary? _Hint_: Show var(\(x_{t}\)) is constant.

Figure 3.25: Twelve month forecast using the ARIMA(\(0,1,1)\times(0,1,1)_{12}\) model on the logged air passenger data set

**3.3** Verify the calculations made in Example 3.4 as follows.

(a) Let \(x_{t}=\phi x_{t-1}+w_{t}\) where \(|\phi|>1\) and \(w_{t}\sim\) iid N(\(0,\sigma_{w}^{2}\)). Show E(\(x_{t}\)) = 0 and \(\gamma_{x}(h)=\sigma_{w}^{2}\phi^{-2}\phi^{-h}/(1-\phi^{-2})\) for \(h\geq 0\).

(b) Let \(y_{t}=\phi^{-1}y_{t-1}+v_{t}\) where \(v_{t}\sim\) iid N(\(0,\sigma_{w}^{2}\phi^{-2}\)) and \(\phi\) and \(\sigma_{w}\) are as in part (a).

Argue that \(y_{t}\) is causal with the same mean function and autocovariance function as \(x_{t}\).

**3.4** Identify the following models as ARMA(\(p,q\)) models (watch out for parameter redundancy), and determine whether they are causal and/or invertible:

(a) \(x_{t}=.80x_{t-1}-.15x_{t-2}+w_{t}-.30w_{t-1}\).

(b) \(x_{t}=x_{t-1}-.50x_{t-2}+w_{t}-w_{t-1}\).

**3.5** Verify the causal conditions for an AR(2) model given in (3.28). That is, show that an AR(2) is causal if and only if (3.28) holds.

_Section 3.2_

**3.6** For the AR(2) model given by \(x_{t}=-.9x_{t-2}+w_{t}\), find the roots of the autoregressive polynomial, and then plot the ACF, \(\rho(h)\).

**3.7** For the AR(2) series shown below, use the results of Example 3.10 to determine a set of difference equations that can be used to find the ACF \(\rho(h),\ h=0,1,\ldots\); solve for the constants in the ACF using the initial conditions. Then plot the ACF values to lag 10 (use ARMAacf as a check on your answers).

(a) \(x_{t}+1.6x_{t-1}+.64x_{t-2}=w_{t}\).

(b) \(x_{t}-.40x_{t-1}-.45x_{t-2}=w_{t}\).

(c) \(x_{t}-1.2x_{t-1}+.85x_{t-2}=w_{t}\).

_Section 3.3_

**3.8** Verify the calculations for the autocorrelation function of an ARMA(1, 1) process given in Example 3.14. Compare the form with that of the ACF for the ARMA(\(1,0\)) and the ARMA(\(0,1\)) series. Plot the ACFs of the three series on the same graph for \(\phi=.6,\ \theta=.9\), and comment on the diagnostic capabilities of the ACF in this case.

**3.9** Generate \(n=100\) observations from each of the three models discussed in Problem 3.8. Compute the sample ACF for each model and compare it to the theoretical values. Compute the sample PACF for each of the generated series and compare the sample ACFs and PACFs with the general results given in Table 3.1.

#### Section 3.4

**3.10** Let \(x_{t}\) represent the cardiovascular mortality series (cmort) discussed in Example 2.2.

(a) Fit an AR(2) to \(x_{t}\) using linear regression as in Example 3.18.

(b) Assuming the fitted model in (a) is the true model, find the forecasts over a four-week horizon, \(x_{n+m}^{n}\), for \(m=1,2,3,4\), and the corresponding 95% prediction intervals.

**3.11** Consider the MA(1) series

\[x_{t}=w_{t}+\theta w_{t-1},\]

where \(w_{t}\) is white noise with variance \(\sigma_{w}^{2}\).

(a) Derive the minimum mean-square error one-step forecast based on the infinite past, and determine the mean-square error of this forecast.

(b) Let \(\tilde{x}_{n+1}^{n}\) be the truncated one-step-ahead forecast as given in (3.92). Show that

\[\mathrm{E}\left[(x_{n+1}-\tilde{x}_{n+1}^{n})^{2}\right]=\sigma^{2}(1+\theta^{ 2+2n}).\]

Compare the result with (a), and indicate how well the finite approximation works in this case.

**3.12** In the context of equation (3.63), show that, if \(\gamma(0)>0\) and \(\gamma(h)\to 0\) as \(h\to\infty\), then \(\Gamma_{n}\) is positive definite.

**3.13** Suppose \(x_{t}\) is stationary with zero mean and recall the definition of the PACF given by (3.55) and (3.56). That is, let

\[\epsilon_{t}=x_{t}-\sum_{i=1}^{h-1}a_{i}x_{t-i}\quad\text{and}\quad\delta_{t- h}=x_{t-h}-\sum_{j=1}^{h-1}b_{j}x_{t-j}\]

be the two residuals where \(\{a_{1},\ldots,a_{h-1}\}\) and \(\{b_{1},\ldots,b_{h-1}\}\) are chosen so that they minimize the mean-squared errors

\[\mathrm{E}[\epsilon_{t}^{2}]\quad\text{and}\quad\mathrm{E}[\delta_{t-h}^{2}].\]

The PACF at lag \(h\) was defined as the cross-correlation between \(\epsilon_{t}\) and \(\delta_{t-h}\); that is,

\[\phi_{hh}=\frac{\mathrm{E}(\epsilon_{t}\delta_{t-h})}{\sqrt{\mathrm{E}( \epsilon_{t}^{2})\mathrm{E}(\delta_{t-h}^{2})}}.\]

Let \(R_{h}\) be the \(h\times h\) matrix with elements \(\rho(i-j)\) for \(i,j=1,\ldots,h\), and let \(\rho_{h}=(\rho(1),\rho(2),\ldots,\rho(h))^{\prime}\) be the vector of lagged autocorrelations, \(\rho(h)=\mathrm{corr}(x_{t+h},x_{t})\). Let \(\tilde{\rho}_{h}=(\rho(h),\rho(h-1),\ldots,\rho(1))^{\prime}\) be the reversed vector. In addition, let \(x_{t}^{h}\) denote the BLP of \(x_{t}\) given \(\{x_{t-1},\ldots,x_{t-h}\}\):

\[x_{t}^{h}=\alpha_{h1}x_{t-1}+\cdots+\alpha_{hh}x_{t-h},\]as described in Property 3.3. Prove

\[\phi_{hh}=\frac{\rho(h)-\tilde{\rho}^{\prime}_{h-1}R^{-1}_{h-1}\rho_{h}}{1-\tilde {\rho}^{\prime}_{h-1}R^{-1}_{h-1}\tilde{\rho}_{h-1}}=\alpha_{hh}.\]

In particular, this result proves Property 3.4.

_Hint:_ Divide the prediction equations [see (3.63)] by \(\gamma(0)\) and write the matrix equation in the partitioned form as

\[\begin{pmatrix}R_{h-1}&\tilde{\rho}_{h-1}\\ \tilde{\rho}^{\prime}_{h-1}&\rho(0)\end{pmatrix}\begin{pmatrix}\alpha_{1}\\ \alpha_{hh}\end{pmatrix}=\begin{pmatrix}\rho_{h-1}\\ \rho(h)\end{pmatrix},\]

where the \(h\times 1\) vector of coefficients \(\alpha=(\alpha_{h1},\ldots,\alpha_{hh})^{\prime}\) is partitioned as \(\alpha=(\alpha^{\prime}_{1},\alpha_{hh})^{\prime}\).

**3.14** Suppose we wish to find a prediction function \(g(x)\) that minimizes

\[MSE=\operatorname{E}[(y-g(x))^{2}],\]

where \(x\) and \(y\) are jointly distributed random variables with density function \(f(x,y)\).

(a) Show that MSE is minimized by the choice

\[g(x)=\operatorname{E}(y\bigm{|}x).\]

_Hint:_

\[MSE=\operatorname{EE}[(y-g(x))^{2}\bigm{|}x].\]

(b) Apply the above result to the model

\[y=x^{2}+z,\]

where \(x\) and \(z\) are independent zero-mean normal variables with variance one. Show that \(MSE=1\).

(c) Suppose we restrict our choices for the function \(g(x)\) to linear functions of the form

\[g(x)=a+bx\]

and determine \(a\) and \(b\) to minimize \(MSE\). Show that \(a=1\) and

\[b=\frac{\operatorname{E}(xy)}{\operatorname{E}(x^{2})}=0\]

and \(MSE=3\). What do you interpret this to mean?

**3.15** For an AR(1) model, determine the general form of the \(m\)-step-ahead forecast \(x^{t}_{t+m}\) and show

\[\operatorname{E}[(x_{t+m}-x^{t}_{t+m})^{2}]=\sigma_{w}^{2}\frac{1-\phi^{2m}}{1 -\phi^{2}}.\]

**3.16** Consider the ARMA(1,1) model discussed in Example 3.8, equation (3.27); that is, \(x_{t}=.9x_{t-1}+.5w_{t-1}+w_{t}\). Show that truncated prediction as defined in (3.91) is equivalent to truncated prediction using the recursive formula (3.92).

**3.17** Verify statement (3.87), that for a fixed sample size, the ARMA prediction errors are correlated.

_Section 3.5_

**3.18** Fit an AR(2) model to the cardiovascular mortality series (cmort) discussed in Example 2.2. using linear regression and using Yule-Walker.

(a) Compare the parameter estimates obtained by the two methods.

(b) Compare the estimated standard errors of the coefficients obtained by linear regression with their corresponding asymptotic approximations, as given in Property 3.10.

**3.19** Suppose \(x_{1},\ldots,x_{n}\) are observations from an AR(1) process with \(\mu=0\).

(a) Show the backcasts can be written as \(x_{t}^{n}=\phi^{1-t}x_{1}\), for \(t\leq 1\).

(b) In turn, show, for \(t\leq 1\), the backcasted errors are

\[\tilde{w}_{t}(\phi)=x_{t}^{n}-\phi x_{t-1}^{n}=\phi^{1-t}(1-\phi^{2})x_{1}.\]

(c) Use the result of (b) to show \(\sum_{t=-\infty}^{1}\tilde{w}_{t}^{2}(\phi)=(1-\phi^{2})x_{1}^{2}\).

(d) Use the result of (c) to verify the unconditional sum of squares, \(S(\phi)\), can be written as \(\sum_{t=-\infty}^{n}\tilde{w}_{t}^{2}(\phi)\).

(e) Find \(x_{t}^{t-1}\) and \(r_{t}\) for \(1\leq t\leq n\), and show that

\[S(\phi)=\sum_{t=1}^{n}(x_{t}-x_{t}^{t-1})^{2}\ \big{/}\ r_{t}.\]

**3.20** Repeat the following numerical exercise three times. Generate \(n=500\) observations from the ARMA model given by

\[x_{t}=.9x_{t-1}+w_{t}-.9w_{t-1},\]

with \(w_{t}\sim\text{id N}(0,1)\). Plot the simulated data, compute the sample ACF and PACF of the simulated data, and fit an ARMA(1, 1) model to the data. What happened and how do you explain the results?

**3.21** Generate 10 realizations of length \(n=200\) each of an ARMA(1,1) process with \(\phi=.9,\theta=.5\) and \(\sigma^{2}=1\). Find the MLEs of the three parameters in each case and compare the estimators to the true values.

**3.22** Generate \(n=50\) observations from a Gaussian AR(1) model with \(\phi=.99\) and \(\sigma_{w}=1\). Using an estimation technique of your choice, compare the approximate asymptotic distribution of your estimate (the one you would use for inference) with the results of a bootstrap experiment (use \(B=200\)).

**3.23** Using Example 3.32 as your guide, find the Gauss-Newton procedure for estimating the autoregressive parameter, \(\phi\), from the AR(1) model, \(x_{t}=\phi x_{t-1}+w_{t}\), given data \(x_{1},\ldots,x_{n}\). Does this procedure produce the unconditional or the conditional estimator? _Hint:_ Write the model as \(w_{t}(\phi)=x_{t}-\phi x_{t-1}\); your solution should work out to be a non-recursive procedure.

**3.24** Consider the stationary series generated by

\[x_{t}=\alpha+\phi x_{t-1}+w_{t}+\theta w_{t-1},\]

where \(\text{E}(x_{t})=\mu\), \(|\theta|<1,|\phi|<1\) and the \(w_{t}\) are iid random variables with zero mean and variance \(\sigma_{w}^{2}\).

(a) Determine the mean as a function of \(\alpha\) for the above model. Find the autocovariance and ACF of the process \(x_{t}\), and show that the process is weakly stationary.

Is the process strictly stationary?

(b) Prove the limiting distribution as \(n\to\infty\) of the sample mean,

\[\bar{x}=n^{-1}\sum_{t=1}^{n}x_{t},\]

is normal, and find its limiting mean and variance in terms of \(\alpha\), \(\phi\), \(\theta\), and \(\sigma_{w}^{2}\). (Note: This part uses results from Appendix A.)

**3.25** A problem of interest in the analysis of geophysical time series involves a simple model for observed data containing a signal and a reflected version of the signal with unknown amplification factor \(a\) and unknown time delay \(\delta\). For example, the depth of an earthquake is proportional to the time delay \(\delta\) for the P wave and its reflected form pP on a seismic record. Assume the signal, say \(s_{t}\), is white and Gaussian with variance \(\sigma_{s}^{2}\), and consider the generating model

\[x_{t}=s_{t}+as_{t-\delta}.\]

(a) Prove the process \(x_{t}\) is stationary. If \(|a|<1\), show that

\[s_{t}=\sum_{j=0}^{\infty}(-a)^{j}x_{t-\delta j}\]

is a mean square convergent representation for the signal \(s_{t}\), for \(t=1,\pm 1,\pm 2,\ldots\). (b) If the time delay \(\delta\) is assumed to be known, suggest an approximate computational method for estimating the parameters \(a\) and \(\sigma_{s}^{2}\) using maximum likelihood and the Gauss-Newton method. (c) If the time delay \(\delta\) is an unknown integer, specify how we could estimate the parameters including \(\delta\). Generate a \(n=500\) point series with \(a=.9\), \(\sigma_{w}^{2}=1\) and \(\delta=5\). Estimate the integer time delay \(\delta\) by searching over \(\delta=3,4,\ldots,7\).

**3.26**: _Forecasting with estimated parameters_: Let \(x_{1},x_{2},\ldots,x_{n}\) be a sample of size \(n\) from a causal AR(1) process, \(x_{t}=\phi x_{t-1}+w_{t}\). Let \(\hat{\phi}\) be the Yule-Walker estimator of \(\phi\).

(a) Show \(\hat{\phi}-\phi=O_{p}(n^{-1/2})\). See Appendix A for the definition of \(O_{p}(\cdot)\).

(b) Let \(x_{n+1}^{n}\) be the one-step-ahead forecast of \(x_{n+1}\) given the data \(x_{1},\ldots,x_{n}\), based on the known parameter, \(\phi\), and let \(\hat{x}_{n+1}^{n}\) be the one-step-ahead forecast when the parameter is replaced by \(\hat{\phi}\). Show \(x_{n+1}^{n}-\hat{x}_{n+1}^{n}=O_{p}(n^{-1/2})\).

**3.27**: Suppose

\[y_{t}=\beta_{0}+\beta_{1}t+\cdots+\beta_{q}t^{q}+x_{t},\quad\beta_{q}\neq 0,\]

where \(x_{t}\) is stationary. First, show that \(\nabla^{k}x_{t}\) is stationary for any \(k=1,2,\ldots\), and then show that \(\nabla^{k}y_{t}\) is not stationary for \(k<q\), but is stationary for \(k\geq q\).

**3.28**: Verify that the IMA(1,1) model given in (3.148) can be inverted and written as (3.149).

**3.29**: For the ARIMA(\(1,1,0\)) model with drift, \((1-\phi B)(1-B)x_{t}=\delta+w_{t}\), let \(y_{t}=(1-B)x_{t}=\nabla x_{t}\).

(a) Noting that \(y_{t}\) is AR(1), show that, for \(j\geq 1\),

\[y_{n+j}^{n}=\delta\left[1+\phi+\cdots+\phi^{j-1}\right]+\phi^{j}\,y_{n}.\]

(b) Use part (a) to show that, for \(m=1,2,\ldots\),

\[x_{n+m}^{n}=x_{n}+\frac{\delta}{1-\phi}\left[m-\frac{\phi(1-\phi^{m})}{(1-\phi) }\right]+(x_{n}-x_{n-1})\frac{\phi(1-\phi^{m})}{(1-\phi)}.\]

_Hint:_ From (a), \(x_{n+j}^{n}-x_{n+j-1}^{n}=\delta\frac{1-\phi^{j}}{1-\phi}+\phi^{j}(x_{n}-x_{n- 1})\). Now sum both sides over \(j\) from 1 to \(m\).

(c) Use (3.145) to find \(P_{n+m}^{n}\) by first showing that \(\psi_{0}^{*}=1\), \(\psi_{1}^{*}=(1+\phi)\), and \(\psi_{j}^{*}-(1+\phi)\psi_{j-1}^{*}+\phi\psi_{j-2}^{*}=0\) for \(j\geq 2\), in which case \(\psi_{j}^{*}=\frac{1-\phi^{j+1}}{1-\phi}\), for \(j\geq 1\).

Note that, as in Example 3.37, equation (3.145) is exact here.

**3.30**: For the
In Example 3.40, we presented the diagnostics for the MA(2) fit to the GNP growth rate series. Using that example as a guide, complete the diagnostics for the AR(1) fit.

**3.32** Crude oil prices in dollars per barrel are in oil. Fit an ARIMA(\(p,d,q\)) model to the growth rate performing all necessary diagnostics. Comment.

**3.33** Fit an ARIMA(\(p,d,q\)) model to the global temperature data globtemp performing all of the necessary diagnostics. After deciding on an appropriate model, forecast (with limits) the next 10 years. Comment.

**3.34** Fit an ARIMA(\(p,d,q\)) model to the sulfur dioxide series, so2, performing all of the necessary diagnostics. After deciding on an appropriate model, forecast the data into the future four time periods ahead (about one month) and calculate 95% prediction intervals for each of the four forecasts. Comment. (Sulfur dioxide is one of the pollutants monitored in the mortality study described in Example 2.2.)

#### 3.8.

Let \(S_{t}\) represent the monthly sales data in sales (\(n=150\)), and let \(L_{t}\) be the leading indicator in lead.

(a) Fit an ARIMA model to \(S_{t}\), the monthly sales data. Discuss your model fitting in a step-by-step fashion, presenting your (A) initial examination of the data, (B) transformations, if necessary, (C) initial identification of the dependence orders and degree of differencing, (D) parameter estimation, (E) residual diagnostics and model choice.

(b) Use the CCF and lag plots between \(\nabla S_{t}\) and \(\nabla L_{t}\) to argue that a regression of \(\nabla S_{t}\) on \(\nabla L_{t-3}\) is reasonable. [_Note that in_ lag2.plot()_, the first named series is the one that gets lagged_.]

(c) Fit the regression model \(\nabla S_{t}=\beta_{0}+\beta_{1}\nabla L_{t-3}+x_{t}\), where \(x_{t}\) is an ARMA process (explain how you decided on your model for \(x_{t}\)). Discuss your results. [_See Example 3.45 for help on coding this problem._]

**3.36** One of the remarkable technological developments in the computer industry has been the ability to store information densely on a hard drive. In addition, the cost of storage has steadily declined causing problems of _too much data_ as opposed to _big data_. The data set for this assignment is cpg, which consists of the median annual retail price per GB of hard drives, say \(c_{t}\), taken from a sample of manufacturers from 1980 to 2008.

(a) Plot \(c_{t}\) and describe what you see.

(b) Argue that the curve \(c_{t}\) versus \(t\) behaves like \(c_{t}\approx\alpha\mathrm{e}^{\beta t}\) by fitting a linear regression of \(\log c_{t}\) on \(t\) and then plotting the fitted line to compare it to the logged data. Comment.

(c) Inspect the residuals of the linear regression fit and comment.

(d) Fit the regression again, but now using the fact that the errors are autocorrelated.

Comment.

**3.37** Redo Problem 2.2 without assuming the error term is white noise.

_Sect. 3.9_

**3.38** Consider the ARIMA model

\[x_{t}=w_{t}+\Theta w_{t-2}.\]

(a) Identify the model using the notation ARIMA\((p,d,q)\times(P,D,Q)_{s}\).

(b) Show that the series is invertible for \(|\Theta|<1\), and find the coefficients in the representation

\[w_{t}=\sum_{k=0}^{\infty}\pi_{k}x_{t-k}.\]

(c) Develop equations for the \(m\)-step ahead forecast, \(\tilde{x}_{n+m}\), and its variance based on the infinite past, \(x_{n},x_{n-1},\ldots\).

**3.39** Plot the ACF of the seasonal ARIMA\((0,1)\times(1,0)_{12}\) model with \(\Phi=.8\) and \(\theta=.5\).

**3.40** Fit a seasonal ARIMA model of your choice to the chicken price data in chicken. Use the estimated model to forecast the next 12 months.

**3.41** Fit a seasonal ARIMA model of your choice to the unemployment data in unemp. Use the estimated model to forecast the next 12 months.

**3.42** Fit a seasonal ARIMA model of your choice to the unemployment data in unempRate. Use the estimated model to forecast the next 12 months.

**3.43** Fit a seasonal ARIMA model of your choice to the U.S. Live Birth Series (birth). Use the estimated model to forecast the next 12 months.

**3.44** Fit an appropriate seasonal ARIMA model to the log-transformed Johnson and Johnson earnings series (jj) of Example 1.1. Use the estimated model to forecast the next 4 quarters.

_The following problems require supplemental material given in Appendix B._

**3.45** Suppose \(x_{t}=\sum_{j=1}^{P}\phi_{j}x_{t-j}+w_{t}\), where \(\phi_{P}\neq 0\) and \(w_{t}\) is white noise such that \(w_{t}\) is uncorrelated with \(\{x_{k};k<t\}\). Use the Projection Theorem, Theorem B.1, to show that, for \(n>p\), the BLP of \(x_{n+1}\) on \(\overline{\text{sp}}\{x_{k},k\leq n\}\) is

\[\hat{x}_{n+1}=\sum_{j=1}^{P}\phi_{j}x_{n+1-j}.\]

**3.46** Use the Projection Theorem to derive the Innovations Algorithm, Property 3.6, equations (3.77)-(3.79). Then, use Theorem B.2 to derive the \(m\)-step-ahead forecast results given in (3.80) and (3.81).

**3.47** Consider the series \(x_{t}=w_{t}-w_{t-1}\), where \(w_{t}\) is a white noise process with mean zero and variance \(\sigma_{w}^{2}\). Suppose we consider the problem of predicting \(x_{n+1}\), based on only \(x_{1},\ldots,x_{n}\). Use the Projection Theorem to answer the questions below.

(a) Show the best linear predictor is

\[x_{n+1}^{n}=-\frac{1}{n+1}\sum_{k=1}^{n}k\;x_{k}.\]

(b) Prove the mean square error is

\[\text{E}(x_{n+1}-x_{n+1}^{n})^{2}=\frac{n+2}{n+1}\,\sigma_{w}^{2}.\]

**3.48** Use Theorem B.2 and Theorem B.3 to verify (3.117).

**3.49** Prove Theorem B.2.

**3.50** Prove Property 3.2.

## Chapter 4 Spectral Analysis and Filtering

In this chapter, we focus on the _frequency domain_ approach to time series analysis. We argue that the concept of regularity of a series can best be expressed in terms of periodic variations of the underlying phenomenon that produced the series. Many of the examples in Sect. 1.1 are time series that are driven by periodic components. For example, the speech recording in Fig. 1.3 contains a complicated mixture of frequencies related to the opening and closing of the glottis. The monthly SOI displayed in Fig. 1.5 contains two periodicities, a seasonal periodic component of 12 months and an El Nino component of about three to seven years. Of fundamental interest is the return period of the El Nino phenomenon, which can have profound effects on local climate.

An important part of analyzing data in the frequency domain, as well as the time domain, is the investigation and exploitation of the properties of the time-invariant linear filter. This special linear transformation is used similarly to linear regression in conventional statistics, and we use many of the same terms in the time series context.

We also introduce coherency as a tool for relating the common periodic behavior of two series. Coherency is a frequency based measure of the correlation between two series at a given frequency, and we show later that it measures the performance of the best linear filter relating the two series.

Many frequency scales will often coexist, depending on the nature of the problem. For example, in the Johnson & Johnson data set in Fig. 1.1, the predominant frequency of oscillation is one cycle per year (4 quarters), or \(\omega=.25\) cycles per observation. The predominant frequency in the SOI and fish populations series in Fig. 1.5 is also one cycle per year, but this corresponds to 1 cycle every 12 months, or \(\omega=.083\) cycles per observation. Throughout the text, we measure frequency, \(\omega\), at cycles per time point rather than the alternative \(\lambda=2\pi\omega\) that would give radians per point. Of descriptive interest is the _period_ of a time series, defined as the number of points in a cycle, i.e., \(1/\omega\). Hence, the predominant period of the Johnson & Johnson series is \(1/.25\) or 4 quarters per cycle, whereas the predominant period of the SOI series is 12 months per cycle.

### Cyclical Behavior and Periodicity

We have already encountered the notion of periodicity in numerous examples in Chapters 1, 2 and 3. The general notion of periodicity can be made more precise by introducing some terminology. In order to define the rate at which a series oscillates, we first define a _cycle_ as one complete period of a sine or cosine function defined over a unit time interval. As in (5), we consider the periodic process

\[x_{t}=A\cos(2\pi\omega t+\phi) \tag{4.1}\]

for \(t=0,\pm 1,\pm 2,\ldots\), where \(\omega\) is a _frequency_ index, defined in cycles per unit time with \(A\) determining the height or _amplitude_ of the function and \(\phi\), called the _phase_, determining the start point of the cosine function. We can introduce random variation in this time series by allowing the amplitude and phase to vary randomly.

As discussed in Example 2.10, for purposes of data analysis, it is easier to use a trigonometric identity1 and write (4.1) as

Footnote 1: \(\cos(\alpha\pm\beta)=\cos(\alpha)\cos(\beta)\mp\sin(\alpha)\sin(\beta)\).

\[x_{t}=U_{1}\cos(2\pi\omega t)+U_{2}\sin(2\pi\omega t), \tag{4.2}\]

where \(U_{1}=A\cos\phi\) and \(U_{2}=-A\sin\phi\) are often taken to be normally distributed random variables. In this case, the amplitude is \(A=\sqrt{(U_{1}^{2}+U_{2}^{2})}\) and the phase is \(\phi=\tan^{-1}(-U_{2}/U_{1})\). From these facts we can show that if, and only if, in (4.1), \(A\) and \(\phi\) are independent random variables, where \(A^{2}\) is chi-squared with 2 degrees of freedom, and \(\phi\) is uniformly distributed on \((-\pi,\pi)\), then \(U_{1}\) and \(U_{2}\) are independent, standard normal random variables (see Problem 4.3).

If we assume that \(U_{1}\) and \(U_{2}\) are uncorrelated random variables with mean 0 and variance \(\sigma^{2}\), then \(x_{t}\) in (4.2) is stationary with mean \(\mathrm{E}(x_{t})=0\) and, writing \(c_{t}=\cos(2\pi\omega t)\) and \(s_{t}=\sin(2\pi\omega t)\), autocovariance function

\[\begin{split}\gamma_{x}(h)=\mathrm{cov}(x_{t+h},x_{t})& =\mathrm{cov}(U_{1}c_{t+h}+U_{2}s_{t+h},U_{1}c_{t}+U_{2}s_{t})\\ &=\mathrm{cov}(U_{1}c_{t+h},U_{1}c_{t})+\mathrm{cov}(U_{1}c_{t+h},U_{2}s_{t})\\ &\quad+\mathrm{cov}(U_{2}s_{t+h},U_{1}c_{t})+\mathrm{cov}(U_{2}s_ {t+h},U_{2}s_{t})\\ &=\sigma^{2}c_{t+h}c_{t}+0+0+\sigma^{2}s_{t+h}s_{t}=\sigma^{2} \cos(2\pi\omega h),\end{split} \tag{4.3}\]

using Footnote 1 and noting that \(\mathrm{cov}(U_{1},U_{2})=0\). From (4.3), we see that

\[\mathrm{var}(x_{t})=\gamma_{x}(0)=\sigma^{2}\,.\]

Thus, if we observe \(U_{1}=a\) and \(U_{2}=b\), an estimate of \(\sigma^{2}\) is the sample variance of these two observations, which in this case is simply \(S^{2}=\frac{a^{2}+b^{2}}{2-1}=a^{2}+b^{2}\).

The random process in (4.2) is function of its frequency, \(\omega\). For \(\omega=1\), the series makes one cycle per time unit; for \(\omega=.50\), the series makes a cycle every two time units; for \(\omega=.25\), every four units, and so on. In general, for data that occur at discrete time points, we will need at least two points to determine a cycle, so thehighest frequency of interest is.5 cycles per point. This frequency is called the _folding frequency_ and defines the highest frequency that can be seen in discrete sampling. Higher frequencies sampled this way will appear at lower frequencies, called _aliases_; an example is the way a camera samples a rotating wheel on a moving automobile in a movie, in which the wheel appears to be rotating at a different rate, and sometimes backwards (the _wagon wheel effect_). For example, most movies are recorded at 24 frames per second (or 24 Hertz). If the camera is filming a wheel that is rotating at 24 Hertz, the wheel will appear to stand still.

Consider a generalization of (4.2) that allows mixtures of periodic series with multiple frequencies and amplitudes,

\[x_{t}=\sum_{k=1}^{q}\big{[}U_{k1}\cos(2\pi\omega_{k}t)+U_{k2}\sin(2\pi\omega_{k }t)\big{]}, \tag{4.4}\]

where \(U_{k1},U_{k2}\), for \(k=1,2,\ldots,q\), are uncorrelated zero-mean random variables with variances \(\sigma_{k}^{2}\), and the \(\omega_{k}\) are distinct frequencies. Notice that (4.4) exhibits the process as a sum of uncorrelated components, with variance \(\sigma_{k}^{2}\) for frequency \(\omega_{k}\). As in (4.3), it is easy to show (Problem 4.4) that the autocovariance function of the process is

\[\gamma_{x}(h)=\sum_{k=1}^{q}\sigma_{k}^{2}\cos(2\pi\omega_{k}h), \tag{4.5}\]

and we note the autocovariance function is the sum of periodic components with weights proportional to the variances \(\sigma_{k}^{2}\). Hence, \(x_{t}\) is a mean-zero stationary processes with variance

\[\gamma_{x}(0)=\text{var}(x_{t})=\sum_{k=1}^{q}\sigma_{k}^{2}, \tag{4.6}\]

exhibiting the overall variance as a sum of variances of each of the component parts.

As in the simple case, if we observe \(U_{k1}=a_{k}\) and \(U_{k2}=b_{k}\) for \(k=1,\ldots,q\), then an estimate of the \(k\)th variance component, \(\sigma_{k}^{2}\), of \(\text{var}(x_{t})\), would be the sample variance \(S_{k}^{2}=a_{k}^{2}+b_{k}^{2}\). In addition, an estimate of the total variance of \(x_{t}\), namely, \(\gamma_{x}(0)\) would be the sum of the sample variances,

\[\hat{\gamma}_{x}(0)=\hat{\text{var}}(x_{t})=\sum_{k=1}^{q}(a_{k}^{2}+b_{k}^{2})\,. \tag{4.7}\]

Hold on to this idea because we will use it in Example 4.2.

**Example 4.1**: **A Periodic Series**

Figure 4.1 shows an example of the mixture (4.4) with \(q=3\) constructed in the following way. First, for \(t=1,\ldots,100\), we generated three series

\(x_{t1}=2\cos(2\pi t\,6/100)+3\sin(2\pi t\,6/100)\)

\(x_{t2}=4\cos(2\pi t\,10/100)+5\sin(2\pi t\,10/100)\)

\(x_{t3}=6\cos(2\pi t\,40/100)+7\sin(2\pi t\,40/100)\)These three series are displayed in Fig. 4.1 along with the corresponding frequencies and squared amplitudes. For example, the squared amplitude of \(x_{t1}\) is \(A^{2}=2^{2}+3^{2}=13\). Hence, the maximum and minimum values that \(x_{t1}\) will attain are \(\pm\sqrt{13}=\pm 3.61\).

Finally, we constructed

\[x_{t}=x_{t1}+x_{t2}+x_{t3}\]

and this series is also displayed in Fig. 4.1. We note that \(x_{t}\) appears to behave as some of the periodic series we saw in Chapters 1 and 2. The systematic sorting out of the essential frequency components in a time series, including their relative contributions, constitutes one of the main objectives of spectral analysis. The R code to reproduce Fig. 4.1 is

 x1 = 2*cos(2*pi^1:100*6/100) + 3*sin(2*pi^1:100*6/100)  x2 = 4*cos(2*pi^1:100*10/100) + 5*sin(2*pi^1:100*10/100)  x3 = 6*cos(2*pi^1:100*40/100) + 7*sin(2*pi^1:100*40/100)  x = x1 + x2 + x3  par(mfrow=c(2,2))  plot.ts(x1, ylim=c(-10,10), main=expression(omega=6/100---A^2=13))  plot.ts(x2, ylim=c(-10,10), main=expression(omega=10/100---A^2=41))  plot.ts(x3, ylim=c(-10,10), main=expression(omega=40/100---A^2=85))  plot.ts(x, ylim=c(-16,16), main="sum")

The model given in (4.4) along with the corresponding autocovariance function given in (4.5) are population constructs. Although, in (4.7), we hinted as to how we would estimate the variance components, we now discuss the practical aspects of how, given data \(x_{1},\ldots,x_{n}\), to actually estimate the variance components \(\sigma_{k}^{2}\) in (4.6).

Figure 4.1: Periodic components and their sum as described in Example 4.1

**Example 4.2**: **Estimation and the Periodogram**

For any time series sample \(x_{1},\ldots,x_{n}\), where \(n\) is odd, we may write, _exactly_

\[x_{t}=a_{0}+\sum_{j=1}^{(n-1)/2}\left[a_{j}\cos(2\pi t\,j/n)+b_{j}\sin(2\pi t\,j /n)\right], \tag{4.8}\]

for \(t=1,\ldots,n\) and suitably chosen coefficients. If \(n\) is even, the representation (4.8) can be modified by summing to \((n/2-1)\) and adding an additional component given by \(a_{n/2}\cos(2\pi t\,\frac{1}{2})=a_{n/2}(-1)^{t}\). The crucial point here is that (4.8) is exact for any sample. Hence (4.4) may be thought of as an approximation to (4.8), the idea being that many of the coefficients in (4.8) may be close to zero.

Using the regression results from Chap. 2, the coefficients \(a_{j}\) and \(b_{j}\) are of the form \(\sum_{t=1}^{n}x_{t}z_{tj}/\sum_{t=1}^{n}z_{tj}^{2}\), where \(z_{tj}\) is either \(\cos(2\pi t\,j/n)\) or \(\sin(2\pi t\,j/n)\). Using Problem 4.1, \(\sum_{t=1}^{n}z_{tj}^{2}=n/2\) when \(j/n\neq 0,1/2\), so the regression coefficients in (4.8) can be written as (\(a_{0}=\bar{x}\)),

\[a_{j}=\frac{2}{n}\sum_{t=1}^{n}x_{t}\,\cos(2\pi tj/n)\quad\text{and}\quad b_{j }=\frac{2}{n}\sum_{t=1}^{n}x_{t}\,\sin(2\pi tj/n).\]

We then define the _scaled periodogram_ to be

\[P(j/n)=a_{j}^{2}+b_{j}^{2}, \tag{4.9}\]

and it is of interest because it indicates which frequency components in (4.8) are large in magnitude and which components are small. _The scaled periodogram is simply the sample variance at each frequency component and consequently is an estimate of \(\sigma_{j}^{2}\)_ corresponding to the sinusoid oscillating at a frequency of \(\omega_{j}=j/n\). These particular frequencies are called the _Fourier_ or _fundamental frequencies_. Large values of \(P(j/n)\) indicate which frequencies \(\omega_{j}=j/n\) are predominant in the series, whereas small values of \(P(j/n)\) may be associated with noise. The periodogram was introduced in Schuster [173] and used in Schuster [174] for studying the periodicities in the sunspot series (shown in Fig. 4.22).

Fortunately, it is not necessary to run a large regression to obtain the values of \(a_{j}\) and \(b_{j}\) because they can be computed quickly if \(n\) is a highly composite integer. Although we will discuss it in more detail in Sect. 4.3, the _discrete Fourier transform (DFT)_ is a complex-valued weighted average of the data given by2

Footnote 2: Euler’s formula: \(e^{t\alpha}=\cos(\alpha)+i\sin(\alpha)\). Consequently, \(\cos(\alpha)=\frac{\pi i\alpha+i\alpha-i\alpha}{2}\), and \(\sin(\alpha)=\frac{e^{t\alpha}-e^{-t\alpha}}{2i}\). Also, \(\frac{1}{T}=-i\) because \(-i\times i=1\). If \(z=a+ib\) is complex, then \(|z|^{2}=z\,z^{*}=(a+ib)(a-ib)=a^{2}+b^{2}\); the * denotes conjugation.

\[\begin{split} d(j/n)&=n^{-1/2}\sum_{t=1}^{n}x_{t} \,\exp(-2\pi itj/n)\\ &=n^{-1/2}\left(\sum_{t=1}^{n}x_{t}\,\cos(2\pi tj/n)-i\sum_{t=1}^ {n}x_{t}\,\sin(2\pi tj/n)\right),\end{split} \tag{4.10}\]for \(j=0,1,\ldots,n-1\), where the frequencies \(j/n\) are the Fourier or fundamental frequencies. Because of a large number of redundancies in the calculation, (4.10) may be computed quickly using the _fast Fourier transform (FFT)_. Note that

\[|d(j/n)|^{2}=\frac{1}{n}\left(\sum_{t=1}^{n}x_{t}\cos(2\pi tj/n)\right)^{2}+ \frac{1}{n}\left(\sum_{t=1}^{n}x_{t}\sin(2\pi tj/n)\right)^{2} \tag{4.11}\]

and it is this quantity that is called the _periodogram_. We may calculate the scaled periodogram, (4.9), using the periodogram as

\[P(j/n)=\frac{4}{n}\,|d(j/n)|^{2}. \tag{4.12}\]

The scaled periodogram of the data, \(x_{t}\), simulated in Example 4.1 is shown in Fig. 4.2, and it clearly identifies the three components \(x_{t1},x_{t2}\), and \(x_{t3}\) of \(x_{t}\). Note that

\[P(j/n)=P(1-j/n),\quad j=0,1,\ldots,n-1,\]

so there is a mirroring effect at the folding frequency of 1/2; consequently, the periodogram is typically not plotted for frequencies higher than the folding frequency. In addition, note that the heights of the scaled periodogram shown in the figure are

\[P(\tfrac{6}{100})=P(\tfrac{94}{100})=13,\quad P(\tfrac{10}{100})=P(\tfrac{90}{ 100})=41,\quad P(\tfrac{40}{100})=P(\tfrac{60}{100})=85,\]

and \(P(j/n)=0\) otherwise. These are exactly the values of the squared amplitudes of the components generated in Example 4.1.

Assuming the simulated data, x, were retained from the previous example, the R code to reproduce Fig. 4.2 is

 P = Mod(2*fft(x)/100)^2; Fr = 0:99/100  plot(Fr, P, type="0", xlab="frequency", ylab="scaled periodogram")  Different packages scale the FFT differently, so it is a good idea to consult the documentation. R computes it without the factor \(n^{-1/2}\) and with an additional factor of \(e^{2\pi i\omega j}\) that can be ignored because we will be interested in the squared modulus.

Figure 4.2: The scaled periodogram (4.12) of the data generated in Example 4.1

If we consider the data \(x_{t}\) in Example 4.1 as a color (waveform) made up of primary colors \(x_{t1},x_{t2},x_{t3}\) at various strengths (amplitudes), then we might consider the periodogram as a prism that decomposes the color \(x_{t}\) into its primary colors (spectrum). Hence the term _spectral analysis_. The following is an example using actual data.

**Example 4.3**: **Star Magnitude**

The data in Fig. 4.3 are the magnitude of a star taken at midnight for 600 consecutive days. The data are taken from the classic text, _The Calculus of Observations, a Treatise on Numerical Mathematics_, by E.T. Whittaker and G. Robinson, (1923, Blackie & Son, Ltd.).

The periodogram for frequencies less than.08 is also displayed in the figure; the periodogram ordinates for frequencies higher than.08 are essentially zero. Note that the 29 (\(\approx 1/.035\)) day cycle and the 24 (\(\approx 1/.041\)) day cycle are the most prominent periodic components of the data.

We can interpret this result as we are observing an _amplitude modulated_ signal. For example, suppose we are observing signal-plus-noise, \(x_{t}=s_{t}+v_{t}\), where \(s_{t}=\cos(2\pi\omega t)\cos(2\pi\delta t)\), and \(\delta\) is very small. In this case, the process will oscillate at frequency \(\omega\), but the amplitude will be modulated by \(\cos(2\pi\delta t)\). Since \(2\cos(\alpha)\cos(\delta)=\cos(\alpha+\delta)+\cos(\alpha-\delta)\), the periodogram of data generated as \(x_{t}\) will have two peaks close to each other at \(\alpha\pm\delta\). Try this on your own:

t = 1:200

plot.ts(x <- 2*cos(2*pi*.2*t)*cos(2*pi*.01*t)) # not shown lines(cos(2*pi*.19*t)+cos(2*pi*.21*t), col=2) # the same Px = Mod(fft(x))*2; plot(0:199/200, Px, type='o') # the periodogram

The R code to reproduce Fig. 4.3 is

n = length(star)

par(mfrow=c(2,1), mar=c(3,3,1,1), mgp=c(1.6,.6,0))

Figure 4.3: Star magnitudes and part of the corresponding periodogram

plot(star, ylab="star magnitude", xlab="day")  Per = Mod(fft(star-mean(star)))^2/n  Freq = (1:n -1)/m  plot(Freq[1:50], Per[1:50], type='h', lwd=3, ylab="Periodogram",  xlab="Frequency")  u = which.max(Per[1:50]) #22freq=21/600=.035cycles/day  uu = which.max(Per[1:50][-u]) #25freq=25/600=.041cycles/day  1/Freq[22]; 1/Freq[26] #period=days/cycle  text(.05, 7000, "24daycycle"); text(.027, 9000, "29daycycle")  ###anotherwaytofindthetwopeaksistorderonPer  y = cbind(1:50, Freq[1:50], Per[1:50]); y[order(y[,3]),]

### The Spectral Density

In this section, we define the fundamental frequency domain tool, the spectral density. In addition, we discuss the spectral representations for stationary processes. Just as the Wold decomposition (Theorem B.5) theoretically justified the use of regression for analyzing time series, the spectral representation theorems supply the theoretical justifications for decomposing stationary time series into periodic components appearing in proportion to their underlying variances. This material is enhanced by the results presented in Appendix C.

**Example 4.4**: **A Periodic Stationary Process**

Consider a periodic stationary random process given by (4.2), with a fixed frequency \(\omega_{0}\), say,

\[x_{t}=U_{1}\cos(2\pi\omega_{0}t)+U_{2}\sin(2\pi\omega_{0}t), \tag{4.13}\]

where \(U_{1}\) and \(U_{2}\) are uncorrelated zero-mean random variables with equal variance \(\sigma^{2}\). The number of time periods needed for the above series to complete one cycle is exactly \(1/\omega_{0}\), and the process makes exactly \(\omega_{0}\) cycles per point for \(t=0,\pm 1,\pm 2,\ldots\). Recalling (4.3) and using Footnote 2, we haveit has the representation

\[\gamma(h) =\sigma^{2}\cos(2\pi\omega_{0}h)=\frac{\sigma^{2}}{2}\mathrm{e}^{ -2\pi i\omega_{0}h}+\frac{\sigma^{2}}{2}\mathrm{e}^{2\pi i\omega_{0}h}\] \[=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}dF(\omega)\]

using Riemann-Stieltjes integration (see Sect. C.4.1), where \(F(\omega)\) is the function defined by

\[F(\omega)=\begin{cases}0&\omega<-\omega_{0},\\ \sigma^{2}/2&-\omega_{0}\leq\omega<\omega_{0},\\ \sigma^{2}&\omega\geq\omega_{0}.\end{cases}\]

The function \(F(\omega)\) behaves like a cumulative distribution function for a discrete random variable, except that \(F(\infty)=\sigma^{2}=\mathrm{var}(x_{t})\) instead of one. In fact, \(F(\omega)\) is a cumulative distribution function, not of probabilities, but rather of variances, with \(F(\infty)\) being the total variance of the process \(x_{t}\). Hence, we term \(F(\omega)\) the _spectral distribution function_. This example is continued in Example 4.9.

A representation such as the one given in Example 4.4 always exists for a stationary process. For details, see Theorem C.1 and its proof; Riemann-Stieltjes integration is described in Sect. C.4.1.

**Property 4.1**: **Spectral Representation of an Autocovariance Function**__

_If \(\{x_{t}\}\) is stationary with autocovariance \(\gamma(h)=\operatorname{cov}(x_{t+h},x_{t})\), then there exists a unique monotonically increasing function \(F(\omega)\), called the spectral distribution function, with \(F(-\infty)=F(-1/2)=0\), and \(F(\infty)=F(1/2)=\gamma(0)\) such that_

\[\gamma(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\ dF( \omega). \tag{4.14}\]

An important situation we use repeatedly is the case when the autocovariance function is absolutely summable, in which case the spectral distribution function is absolutely continuous with \(dF(\omega)=f(\omega)\;d\omega\), and the representation (4.14) becomes the motivation for the property given below.

**Property 4.2**: **The Spectral Density**__

_If the autocovariance function, \(\gamma(h)\), of a stationary process satisfies_

\[\sum_{h=-\infty}^{\infty}|\gamma(h)|<\infty, \tag{4.15}\]

_then it has the representation_

\[\gamma(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\ f( \omega)\;d\omega\quad h=0,\pm 1,\pm 2,\ldots \tag{4.16}\]

_as the inverse transform of the spectral density,_

\[f(\omega)=\sum_{h=-\infty}^{\infty}\gamma(h)\mathrm{e}^{-2\pi i\omega h}\quad -1/2\leq\omega\leq 1/2. \tag{4.17}\]

This spectral density is the analogue of the probability density function; the fact that \(\gamma(h)\) is non-negative definite ensures

\[f(\omega)\geq 0\]

for all \(\omega\). It follows immediately from (4.17) that

\[f(\omega)=f(-\omega)\]

verifying the spectral density is an even function. Because of the evenness, we will typically only plot \(f(\omega)\) for \(0\leq\omega\leq 1/2\). In addition, putting \(h=0\) in (4.16) yields

\[\gamma(0)=\operatorname{var}(x_{t})=\int_{-\frac{1}{2}}^{\frac{1}{2}}f(\omega) \;d\omega,\]which expresses the total variance as the integrated spectral density over all of the frequencies. We show later on, that a linear filter can isolate the variance in certain frequency intervals or \(bands\).

It should now be clear that the autocovariance and the spectral distribution functions contain the same information. That information, however, is expressed in different ways. The autocovariance function expresses information in terms of lags, whereas the spectral distribution expresses the same information in terms of cycles. Some problems are easier to work with when considering lagged information and we would tend to handle those problems in the time domain. Nevertheless, other problems are easier to work with when considering periodic information and we would tend to handle those problems in the spectral domain.

We note that the autocovariance function, \(\gamma(h)\), in (4.16) and the spectral density, \(f(\omega)\), in (4.17) are Fourier transform pairs. In particular, this means that if \(f(\omega)\) and \(g(\omega)\) are two spectral densities for which

\[\gamma_{f}(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}f(\omega)\mathrm{e}^{2\pi i \omega h}\ d\omega=\int_{-\frac{1}{2}}^{\frac{1}{2}}g(\omega)\mathrm{e}^{2\pi i \omega h}\ d\omega=\gamma_{g}(h) \tag{4.18}\]

for all \(h=0,\pm 1,\pm 2,\ldots\), then

\[f(\omega)=g(\omega). \tag{4.19}\]

Finally, the absolute summability condition, (4.15), is not satisfied by (4.5), the example that we have used to introduce the idea of a spectral representation. The condition, however, is satisfied for ARMA models.

It is illuminating to examine the spectral density for the series that we have looked at in earlier discussions.

**Example 4.5**: **White Noise Series**

As a simple example, consider the theoretical power spectrum of a sequence of uncorrelated random variables, \(w_{t}\), with variance \(\sigma_{w}^{2}\). A simulated set of data is displayed in the top of Fig. 1.8. Because the autocovariance function was computed in Example 1.16 as \(\gamma_{w}(h)=\sigma_{w}^{2}\) for \(h=0\), and zero, otherwise, it follows from (4.17), that

\[f_{w}(\omega)=\sigma_{w}^{2}\]

for \(-1/2\leq\omega\leq 1/2\). Hence the process contains equal power at all frequencies. This property is seen in the realization, which seems to contain all different frequencies in a roughly equal mix. In fact, the name white noise comes from the analogy to white light, which contains all frequencies in the color spectrum at the same level of intensity. The top of Fig. 4.4 shows a plot of the white noise spectrum for \(\sigma_{w}^{2}=1\). The R code to reproduce the figure is given at the end of Example 4.7.

Since the linear process is an essential tool, it is worthwhile investigating the spectrum of such a process. In general, a linear filter uses a set of specified coefficients, say \(a_{j}\), for \(j=0,\pm 1,\pm 2,\ldots\), to transform an input series, \(x_{t}\), producing an output series, \(y_{t}\), of the form \[y_{t}=\sum_{j=-\infty}^{\infty}a_{j}x_{t-j},\qquad\sum_{j=-\infty}^{\infty}|a_{j}|<\infty. \tag{4.20}\]

The form (4.20) is also called a _convolution_ in some statistical contexts. The coefficients are collectively called the _impulse response function_, and the Fourier transform

\[A(\omega)=\sum_{j=-\infty}^{\infty}a_{j}\,\mathrm{e}^{-2\pi i\omega j}\,, \tag{4.21}\]

is called the _frequency response function_. If, in (4.20), \(x_{t}\) has spectral density \(f_{x}(\omega)\), we have the following result.

**Property 4.3**: **Output Spectrum of a Filtered Stationary Series**__

_For the process in (4.20), if \(x_{t}\) has spectrum \(f_{x}(\omega)\), then the spectrum of the filtered output, \(y_{t}\), say \(f_{y}(\omega)\), is related to the spectrum of the input \(x_{t}\) by_

\[f_{y}(\omega)=|A(\omega)|^{2}\ f_{x}(\omega), \tag{4.22}\]

_where the frequency response function \(A(\omega)\) is defined in (4.21)._

_Proof:_ The autocovariance function of the filtered output \(y_{t}\) in (4.20) is

\[\gamma_{y}(h) = \,\mathrm{cov}(x_{t+h},x_{t})\] \[= \,\mathrm{cov}\left(\sum_{r}a_{r}x_{t+h-r},\sum_{s}a_{s}x_{t-s}\right)\] \[= \,\sum_{r}\sum_{s}a_{r}\gamma_{x}(h-r+s)a_{s}\] \[\stackrel{{(1)}}{{=}} \,\sum_{r}\sum_{s}a_{r}\left[\int_{-\frac{1}{2}}^{\frac{1}{2}} \mathrm{e}^{2\pi i\omega(h-r+s)}f_{x}(\omega)d\omega\right]\,a_{s}\] \[= \,\int_{-\frac{1}{2}}^{\frac{1}{2}}\left(\sum_{r}a_{r}\mathrm{e}^ {-2\pi i\omega r}\right)\left(\sum_{s}a_{s}\mathrm{e}^{2\pi i\omega s}\right) \mathrm{e}^{2\pi i\omega h}f_{x}(\omega)\ d\omega\] \[\stackrel{{(2)}}{{=}} \,\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h} \underbrace{|A(\omega)|^{2}f_{x}(\omega)}_{f_{y}(\omega)}\ d\omega,\]

where we have, (1) replaced \(\gamma_{x}(\cdot)\) by its representation (4.16), and (2) substituted \(A(\omega)\) from (4.21). The result holds by exploiting the uniqueness of the Fourier transform. \(\square\)

The use of Property 4.3 is explored further in Sect. 4.7. If \(x_{t}\) is ARMA, its spectral density can be obtained explicitly using the fact that it is a linear process, i.e., \(x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}\), where \(\sum_{j=0}^{\infty}|\psi_{j}|<\infty\). The following property is a direct consequence of Property 4.3, by using the additional facts that the spectral density of white noise is \(f_{w}(\omega)=\sigma_{w}^{2}\), and by Property 3.1, \(\psi(z)=\theta(z)/\phi(z)\).

**Property 4.4**: **The Spectral Density of ARMA**__

_If \(x_{t}\) is ARMA(\(p,q\)), \(\phi(B)x_{t}=\theta(B)w_{t}\), its spectral density is given by_

\[f_{x}(\omega)=\sigma_{w}^{2}\frac{|\theta(\mathrm{e}^{-2\pi i\omega})|^{2}}{| \phi(\mathrm{e}^{-2\pi i\omega})|^{2}} \tag{4.23}\]

_where \(\phi(z)=1-\sum_{k=1}^{p}\phi_{k}z^{k}\) and \(\theta(z)=1+\sum_{k=1}^{q}\theta_{k}z^{k}\)._

**Example 4.6**: **Moving Average**__

As an example of a series that does not have an equal mix of frequencies, we consider a moving average model. Specifically, consider the MA(1) model given by

\[x_{t}=w_{t}+.5w_{t-1}.\]

A sample realization is shown in the top of Fig. 3.2 and we note that the series has less of the higher or faster frequencies. The spectral density will verify this observation.

The autocovariance function is displayed in Example 3.5, and for this particular example, we have

\[\gamma(0)=(1+.5^{2})\sigma_{w}^{2}=1.25\sigma_{w}^{2};\quad\gamma(\pm 1)=.5 \sigma_{w}^{2};\quad\gamma(\pm h)=0\text{ for }h>1.\]

Substituting this directly into the definition given in (4.17), we have

\[\begin{split} f(\omega)&=\sum_{h=-\infty}^{\infty} \gamma(h)\,\mathrm{e}^{-2\pi i\omega h}=\sigma_{w}^{2}\left[1.25+.5\left( \mathrm{e}^{-2\pi i\omega}+\mathrm{e}^{2\pi\omega}\right)\right]\\ &=\sigma_{w}^{2}\left[1.25+\cos(2\pi\omega)\right].\end{split} \tag{4.24}\]

We can also compute the spectral density using Property 4.4, which states that for an MA, \(f(\omega)=\sigma_{w}^{2}|\theta(\mathrm{e}^{-2\pi i\omega})|^{2}\). Because \(\theta(z)=1+.5z\), we have

\[\begin{split}|\theta(\mathrm{e}^{-2\pi i\omega})|^{2}& =|1+.5\mathrm{e}^{-2\pi i\omega}|^{2}=(1+.5\mathrm{e}^{-2\pi i \omega})(1+.5\mathrm{e}^{2\pi i\omega})\\ &=1.25+.5\left(\mathrm{e}^{-2\pi i\omega}+\mathrm{e}^{2\pi\omega} \right)\end{split}\]

which leads to agreement with (4.24).

Plotting the spectrum for \(\sigma_{w}^{2}=1\), as in the middle of Fig. 4.4, shows the lower or slower frequencies have greater power than the higher or faster frequencies.

**Example 4.7**: **A Second-Order Autoregressive Series**__

We now consider the spectrum of an AR(2) series of the form

\[x_{t}-\phi_{1}x_{t-1}-\phi_{2}x_{t-2}=w_{t},\]

for the special case \(\phi_{1}=1\) and \(\phi_{2}=-.9\). Figure 1.9 shows a sample realization of such a process for \(\sigma_{w}=1\). We note the data exhibit a strong periodic component that makes a cycle about every six points.

To use Property 4.4, note that \(\theta(z)=1\), \(\phi(z)=1-z+.9z^{2}\) and

\[|\phi(e^{-2\pi i\omega})|^{2} =(1-\mathrm{e}^{-2\pi i\omega}+.9\mathrm{e}^{-4\pi i\omega})(1- \mathrm{e}^{2\pi i\omega}+.9\mathrm{e}^{4\pi i\omega})\] \[=2.81-1.9(\mathrm{e}^{2\pi i\omega}+\mathrm{e}^{-2\pi i\omega})+.9 (\mathrm{e}^{4\pi i\omega}+\mathrm{e}^{-4\pi i\omega})\] \[=2.81-3.8\cos(2\pi\omega)+1.8\cos(4\pi\omega).\]

Using this result in (4.23), we have that the spectral density of \(x_{t}\) is

\[f_{x}(\omega)=\frac{\sigma_{w}^{2}}{2.81-3.8\cos(2\pi\omega)+1.8\cos(4\pi \omega)}.\]

Setting \(\sigma_{w}=1\), the bottom of Fig. 4.4 displays \(f_{x}(\omega)\) and shows a strong power component at about \(\omega=.16\) cycles per point or a period between six and seven cycles per point and very little power at other frequencies. In this case, modifying the white noise series by applying the second-order AR operator has concentrated the power or variance of the resulting series in a very narrow frequency band.

The spectral density can also be obtained from first principles, without having to use Property 4.4. Because \(w_{t}=x_{t}-x_{t-1}+.9x_{t-2}\) in this example, we have

\[\gamma_{w}(h) =\mathrm{cov}(w_{t+h},w_{t})\] \[=\mathrm{cov}(x_{t+h}-x_{t+h-1}+.9x_{t+h-2},\;x_{t}-x_{t-1}+.9x_{ t-2})\] \[=2.81\gamma_{x}(h)-1.9[\gamma_{x}(h+1)+\gamma_{x}(h-1)]+.9[\gamma _{x}(h+2)+\gamma_{x}(h-2)]\]

Figure 4.4: Theoretical spectra of white noise (_top_), a first-order moving average (_middle_), and a second-order autoregressive process (_bottom_)

Now, substituting the spectral representation (4.16) for \(\gamma_{x}(h)\) in the above equation yields

\[\gamma_{w}(h) = \int_{-\frac{1}{2}}^{\frac{1}{2}}\bigl{[}2.81-1.9(\mathrm{e}^{2\pi i \omega}+\mathrm{e}^{-2\pi i\omega})+.9(\mathrm{e}^{4\pi i\omega}+\mathrm{e}^{-4 \pi i\omega})\bigr{]}\mathrm{e}^{2\pi i\omega h}f_{x}(\omega)d\omega\] \[= \int_{-\frac{1}{2}}^{\frac{1}{2}}\bigl{[}2.81-3.8\cos(2\pi\omega) +1.8\cos(4\pi\omega)\bigr{]}\mathrm{e}^{2\pi i\omega h}f_{x}(\omega)d\omega.\]

If the spectrum of the white noise process, \(w_{t}\), is \(g_{w}(\omega)\), the uniqueness of the Fourier transform allows us to identify

\[g_{w}(\omega)=\bigl{[}2.81-3.8\cos(2\pi\omega)+1.8\cos(4\pi\omega)\bigr{]}f_{x }(\omega).\]

But, as we have already seen, \(g_{w}(\omega)=\sigma_{w}^{2}\), from which we deduce that

\[f_{x}(\omega)=\frac{\sigma_{w}^{2}}{2.81-3.8\cos(2\pi\omega)+1.8\cos(4\pi \omega)}\]

is the spectrum of the autoregressive series.

To reproduce Fig. 4.4, use arma.spec from astsa:

par(mfrow=c(3,1)) arma.spec(log="no", main="White Noise") arma.spec(ma=.5, log="no", main="Moving Average") arma.spec(ar=c(1,-.9), log="no", main="Autoregression")

**Example 4.8**: _Every Explosion has a Cause (cont) In Example 3.4, we discussed the fact that explosive models have causal counterparts. In that example, we also indicated that it was easier to show this result in general in the spectral domain. In this example, we give the details for an AR(1) model, but the techniques used here will indicate how to generalize the result._

_As in Example 3.4, we suppose that \(x_{t}=2x_{t-1}+w_{t}\), where \(w_{t}\sim\mathrm{id}\ \mathrm{N}(0,\sigma_{w}^{2})\). Then, the spectral density of \(x_{t}\) is_

\[f_{x}(\omega)=\sigma_{w}^{2}\,|1-2e^{-2\pi i\omega}|^{-2}. \tag{4.25}\]

_But, \(|1-2e^{-2\pi i\omega}|=|1-2e^{2\pi i\omega}|=|(2e^{2\pi i\omega})\,(\tfrac{1}{ 2}e^{-2\pi i\omega}-1)|=2\,|1-\tfrac{1}{2}e^{-2\pi i\omega}|\). Thus, (4.25) can be written as_

\[f_{x}(\omega)=\tfrac{1}{4}\sigma_{w}^{2}\,|1-\tfrac{1}{2}e^{-2\pi i\omega}|^{- 2},\]

_which implies that \(x_{t}=\tfrac{1}{2}x_{t-1}+v_{t}\), with \(v_{t}\sim\mathrm{id}\ \mathrm{N}(0,\tfrac{1}{4}\sigma_{w}^{2})\) is an equivalent form of the model._

We end this section by mentioning another spectral representation that deals with the process directly. In nontechnical terms, the result suggests that (4.4) is approximately true for any stationary time series, and this gives an additional theoretical justification for decomposing time series into harmonic components.

**Example 4.9**: **A Periodic Stationary Process (cont)**

In Example 4.4, we considered the periodic stationary process given in (4.13), namely, \(x_{t}=U_{1}\cos(2\pi\omega_{0}t)+U_{2}\sin(2\pi\omega_{0}t)\). Using Footnote 2, we may write this as

\[x_{t}=\tfrac{1}{2}(U_{1}+iU_{2})\mathrm{e}^{-2\pi i\omega_{0}t}+\tfrac{1}{2}(U_ {1}-iU_{2})\mathrm{e}^{2\pi i\omega_{0}t},\]

where we recall that \(U_{1}\) and \(U_{2}\) are uncorrelated, mean-zero, random variables each with variance \(\sigma^{2}\). If we call \(Z=\tfrac{1}{2}(U_{1}+iU_{2})\), then \(Z^{*}=\tfrac{1}{2}(U_{1}-iU_{2})\), where * denotes conjugation. In this case, \(\mathrm{E}(Z)=\tfrac{1}{2}[\mathrm{E}(U_{1})+i\mathrm{E}(U_{2})]=0\) and similarly \(\mathrm{E}(Z^{*})=0\). For mean-zero complex random variables, say \(X\) and \(Y\), \(\mathrm{cov}(X,Y)=\mathrm{E}(XY^{*})\). Thus

\[\mathrm{var}(Z) =\mathrm{E}(|Z|^{2})=\mathrm{E}(ZZ^{*})=\tfrac{1}{4}\mathrm{E}[( U_{1}+iU_{2})(U_{1}-iU_{2})]\] \[=\tfrac{1}{4}[\mathrm{E}(U_{1}^{2})+\mathrm{E}(U_{2}^{2})]=\frac {\sigma^{2}}{2}.\]

Similarly, \(\mathrm{var}(Z^{*})=\sigma^{2}/2\). Moreover, since \(Z^{**}=Z\),

\[\mathrm{cov}(Z,Z^{*})=\mathrm{E}(ZZ^{**})=\tfrac{1}{4}\mathrm{E}[(U_{1}+iU_{2}) (U_{1}+iU_{2})]=\tfrac{1}{4}[\mathrm{E}(U_{1}^{2})-\mathrm{E}(U_{2}^{2})]=0.\]

Hence, (4.13) may be written as

\[x_{t}=Z\,\mathrm{e}^{-2\pi i\omega_{0}t}+Z^{*}\mathrm{e}^{2\pi i\omega_{0}t}= \int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\mathrm{e}^{2\pi i\omega t}\,dZ(\omega),\]

where \(Z(\omega)\) is a complex-valued random process that makes uncorrelated jumps at \(-\omega_{0}\) and \(\omega_{0}\) with mean-zero and variance \(\sigma^{2}/2\). Stochastic integration is discussed further in Sect. C.4.2. This notion generalizes to all stationary series in the following property (also, see Theorem C.2).

**Property 4.5**: **Spectral Representation of a Stationary Process**__

_If \(x_{t}\) is a mean-zero stationary process, with spectral distribution \(F(\omega)\) as given in Property 4.1, then there exists a complex-valued stochastic process \(Z(\omega)\), on the interval \(\omega\in[-1/2,1/2]\), having stationary uncorrelated non-overlapping increments, such that \(x_{t}\) can be written as the stochastic integral (see Sect. C.4.2)_

\[x_{t}=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\mathrm{e}^{2\pi i\omega t}\,dZ( \omega),\]

_where, for \(-1/2\leq\omega_{1}\leq\omega_{2}\leq 1/2\),_

\[\mathrm{var}\left\{Z(\omega_{2})-Z(\omega_{1})\right\}=F(\omega_{2})-F(\omega _{1}).\]

### Periodogram and Discrete Fourier Transform

We are now ready to tie together the periodogram, which is the sample-based concept presented in Sect. 4.1, with the spectral density, which is the population-based concept of Sect. 4.2.

**Definition 4.1**: _Given data \(x_{1},\ldots,x_{n}\), we define the_ **discrete Fourier transform (DFT)** _to be_

\[d(\omega_{j})=n^{-1/2}\sum_{t=1}^{n}x_{t}\mathrm{e}^{-2\pi i\omega_{j}t} \tag{4.26}\]

_for \(j=0,1,\ldots,n-1\), where the frequencies \(\omega_{j}=j/n\) are called the_ **Fourier** _or_ **fundamental frequencies**_._

If \(n\) is a highly composite integer (i.e., it has many factors), the DFT can be computed by the fast Fourier transform (FFT) introduced in Cooley and Tukey [44]. Also, different packages scale the FFT differently, so it is a good idea to consult the documentation. R computes the DFT defined in (4.26) without the factor \(n^{-1/2}\), but with an additional factor of \(e^{2\pi i\omega_{j}}\) that can be ignored because we will be interested in the squared modulus of the DFT. Sometimes it is helpful to exploit the inversion result for DFTs, which shows the linear transformation is one-to-one. For the _inverse DFT_ we have,

\[x_{t}=n^{-1/2}\sum_{j=0}^{n-1}d(\omega_{j})\mathrm{e}^{2\pi i\omega_{j}t} \tag{4.27}\]

for \(t=1,\ldots,n\). The following example shows how to calculate the DFT and its inverse in R for the data set \(\{1,2,3,4\}\); note that R writes a complex number \(z=a+ib\) as a+bi.

(dft = fft(1:4)/sqrt(4))  [1] 5+@1 -1+1 -1+@1 -1-1i  (idft = fft(dft, inverse=TRUE)/sqrt(4))  [1] 1+@1 2+@i 3+@1 4+@1  (Re(idft)) # keep it real  [1] 1 2 3 4 We now define the periodogram as the squared modulus of the DFT.

**Definition 4.2**: _Given data \(x_{1},\ldots,x_{n}\), we define the_ **periodogram** _to be_

\[I(\omega_{j})=\left|d(\omega_{j})\right|^{2} \tag{4.28}\]

_for \(j=0,1,2,\ldots,n-1\)._

Note that \(I(0)=n\bar{x}^{2}\), where \(\bar{x}\) is the sample mean. Also, \(\sum_{t=1}^{n}\exp(-2\pi it\frac{\dot{J}}{n})=0\) for \(j\neq 0\),3 so we can write the DFT as

Footnote 3: \(\sum_{t=1}^{n}z^{t}=z\frac{1-z^{n}}{1-z}\) for \(z\neq 1\). In this case, \(z^{n}=\mathrm{e}^{-2\pi i\omega_{j}t}\)

\[d(\omega_{j})=n^{-1/2}\sum_{t=1}^{n}(x_{t}-\bar{x})\mathrm{e}^{-2\pi i\omega_{ j}t} \tag{4.29}\]

for \(j\neq 0\). Thus,\[I(\omega_{j}) =\left|d(\omega_{j})\right|^{2}=n^{-1}\sum_{t=1}^{n}\sum_{s=1}^{n}(x _{t}-\bar{x})(x_{s}-\bar{x})\mathrm{e}^{-2\pi i\omega_{j}(t-s)}\] \[=n^{-1}\sum_{h=-(n-1)}^{n-1}\sum_{t=1}^{n-|h|}(x_{t+|h|}-\bar{x}) (x_{t}-\bar{x})\mathrm{e}^{-2\pi i\omega_{j}h}\] \[=\sum_{h=-(n-1)}^{n-1}\hat{\gamma}(h)e^{-2\pi i\omega_{j}h} \tag{4.30}\]

for \(j\neq 0\), where we have put \(h=t-s\), with \(\hat{\gamma}(h)\) as given in (1.36).4 In view of (4.30), the periodogram, \(I(\omega_{j})\), is the sample version of \(f(\omega_{j})\) given in (4.17). That is, we may think of the periodogram as the _sample spectral density_ of \(x_{t}\).

Footnote 4: Note that (4.30) can be used to obtain \(\hat{\gamma}(h)\) by taking the inverse DFT of \(I(\omega_{j})\). This approach was used in Example 1.31 to obtain a two-dimensional ACF.

At first, (4.30) seems to be an obvious way to estimate a spectral density (4.17); i.e, simply put a hat on \(\gamma(h)\) and sum as far as the sample size will allow. However, after further consideration, it turns out that this is not a very good estimator because it uses some bad estimates of \(\gamma(h)\). For example, there is only one pair of observations, \((x_{1},x_{n})\) for estimating \(\gamma(n-1)\), and only two pairs \((x_{1},x_{n-1})\), and \((x_{2},x_{n})\) that can be used to estimate \(\gamma(n-2)\), and so on. We will discuss this problem further as we progress, but an obvious improvement over (4.30) would be something like \(\hat{f}(\omega)=\sum_{|h|\leq m}\hat{\gamma}(h)e^{-2\pi i\omega h}\), where \(m\) is much smaller than \(n\).

It is sometimes useful to work with the real and imaginary parts of the DFT individually. To this end, we define the following transforms.

**Definition 4.3**: _Given data \(x_{1},\ldots,x_{n}\), we define the_ **cosine transform**__

\[d_{c}(\omega_{j})=n^{-1/2}\sum_{t=1}^{n}x_{t}\,\cos(2\pi\omega_{j}t) \tag{4.31}\]

_and the_ **sine transform**__

\[d_{s}(\omega_{j})=n^{-1/2}\sum_{t=1}^{n}x_{t}\,\sin(2\pi\omega_{j}t) \tag{4.32}\]

_where \(\omega_{j}=j/n\) for \(j=0,1,\ldots,n-1\)._

We note that \(d(\omega_{j})=d_{c}(\omega_{j})-i\,d_{s}(\omega_{j})\) and hence

\[I(\omega_{j})=d_{c}^{2}(\omega_{j})+d_{s}^{2}(\omega_{j}). \tag{4.33}\]

We have also discussed the fact that spectral analysis can be thought of as an analysis of variance. The next example examines this notion.

**Example 4.10**: **Spectral ANOVA**

Let \(x_{1},\ldots,x_{n}\) be a sample of size \(n\), where for ease, \(n\) is odd. Then, recalling Example 4.2,

\[x_{t}=a_{0}+\sum_{j=1}^{m}\left[a_{j}\cos(2\pi\omega_{j}t)+b_{j}\sin(2\pi\omega_ {j}t)\right], \tag{4.34}\]

where \(m=(n-1)/2\), is exact for \(t=1,\ldots,n\). In particular, using multiple regression formulas, we have \(a_{0}=\bar{x}\),

\[a_{j} =\frac{2}{n}\sum_{t=1}^{n}x_{t}\cos(2\pi\omega_{j}t)=\frac{2}{ \sqrt{n}}d_{c}(\omega_{j})\] \[b_{j} =\frac{2}{n}\sum_{t=1}^{n}x_{t}\sin(2\pi\omega_{j}t)=\frac{2}{ \sqrt{n}}d_{s}(\omega_{j}).\]

Hence, we may write

\[(x_{t}-\bar{x})=\frac{2}{\sqrt{n}}\sum_{j=1}^{m}\left[d_{c}(\omega_{j})\cos(2 \pi\omega_{j}t)+d_{s}(\omega_{j})\sin(2\pi\omega_{j}t)\right]\]

for \(t=1,\ldots,n\). Squaring both sides and summing we obtain

\[\sum_{t=1}^{n}(x_{t}-\bar{x})^{2}=2\sum_{j=1}^{m}\left[d_{c}^{2}(\omega_{j})+d _{s}^{2}(\omega_{j})\right]=2\sum_{j=1}^{m}I(\omega_{j})\]

using the results of Problem 4.1. Thus, we have partitioned the sum of squares into harmonic components represented by frequency \(\omega_{j}\) with the periodogram, \(I(\omega_{j})\), being the mean square regression. This leads to the ANOVA table for \(n\) odd:

\begin{tabular}{c c c c} \hline Source & df & SS & MS \\ \hline \(\omega_{1}\) & 2 & \(2I(\omega_{1})\) & \(I(\omega_{1})\) \\ \(\omega_{2}\) & 2 & \(2I(\omega_{2})\) & \(I(\omega_{2})\) \\ \(\vdots\) & \(\vdots\) & \(\vdots\) & \(\vdots\) \\ \(\omega_{m}\) & 2 & \(2I(\omega_{m})\) & \(I(\omega_{m})\) \\ \hline Total & \(n-1\) & \(\sum_{t=1}^{n}(x_{t}-\bar{x})^{2}\) & \\ \end{tabular}

The following is an R example to help explain this concept. We consider \(n=5\) observations given by \(x_{1}=1,x_{2}=2,x_{3}=3,x_{4}=2,x_{5}=1\). Note that the data complete one cycle, but not in a sinusoidal way. Thus, we should expect the \(\omega_{1}=1/5\) component to be relatively large but not exhaustive, and the \(\omega_{2}=2/5\) component to be small.

x = c(1, 2, 3, 2, 1)  c1 = cos(2*pi*1:5*1/5); s1 = sin(2*pi*1:5*1/5)  c2 = cos(2*pi*1:5*2/5); s2 = sin(2*pi*1:5*2/5)  omega1 = cbind(c1, s1); omega2 = cbind(c2, s2)  anova(lm(x-omega1+omega2)) # ANOVA Table  Df Sum Sq Mean Sq  omega1 2 2.74164 1.37082  omega2 2.05836.02918  Residuals @.00000 Mod(fft(x))^2/5 # the periodogram (as a check)  [1] 16.2 1.37082.029179.029179 1.37082  # I(0) I(1/5) I(2/5) I(3/5) I(4/5) Note that \(I(0)=n\bar{x}^{2}=5\times 1.8^{2}=16.2\). Also, the sum of squares associated with the residuals (SSE) is zero, indicating an exact fit.

**Example 4.11**: **Spectral Analysis as Principal Component Analysis**

It is also possible to think of spectral analysis as a principal component analysis. In Sect. C.5, we show that the spectral density may be though of as the approximate eigenvalues of the covariance matrix of a stationary process. If \(X=(x_{1},\ldots,x_{n})\) are \(n\) values of a mean-zero time series, \(x_{t}\) with spectral density \(f_{x}(\omega)\), then

\[\mathrm{cov}(X)=\Gamma_{n}=\begin{bmatrix}\gamma(0)&\gamma(1)&\cdots&\gamma(n- 1)\\ \gamma(1)&\gamma(0)&\cdots&\gamma(n-2)\\ \vdots&\vdots&\ddots&\vdots\\ \gamma(n-1)&\gamma(n-2)&\cdots&\gamma(0)\end{bmatrix}\,.\]

For \(n\) sufficiently large, the eigenvalues of \(\Gamma_{n}\) are

\[\lambda_{j}\approx f(\omega_{j})=\sum_{h=-\infty}^{\infty}\gamma(h)\,\mathrm{e} ^{-2\pi ihj/n}\,,\]

with approximate eigenvectors

\[g_{j}^{*}=\frac{1}{\sqrt{n}}(\mathrm{e}^{-2\pi i0j/n},\,\mathrm{e}^{-2\pi i1j/n },\ldots,\mathrm{e}^{-2\pi i(n-1)j/n}),\]

for \(j=0,1,\ldots,n-1\). If we let \(G\) be the complex matrix with columns \(g_{j}\), then the complex vector \(Y=G^{*}X\) has elements that are the DFTs,

\[y_{j}=\frac{1}{\sqrt{n}}\sum_{t=1}^{n}x_{t}\mathrm{e}^{-2\pi itj/n}\]

for \(j=0,1,\ldots,n-1\). In this case, the elements of \(Y\) are asymptotically uncorrelated complex random variables, with mean-zero and variance \(f(\omega_{j})\). Also, \(X\) may be recovered as \(X=GY\), so that \(x_{t}=\frac{1}{\sqrt{n}}\sum_{j=0}^{n-1}y_{j}\mathrm{e}^{2\pi itj/n}\).

We are now ready to present some large sample properties of the periodogram. First, let \(\mu\) be the mean of a stationary process \(x_{t}\) with absolutely summable autocovariance function \(\gamma(h)\) and spectral density \(f(\omega)\). We can use the same argument as in (4.30), replacing \(\bar{x}\) by \(\mu\) in (4.29), to write \[I(\omega_{j})=n^{-1}\sum_{h=-(n-1)}^{n-1}\sum_{t=1}^{n-|h|}(x_{t+|h|}-\mu)(x_{t}- \mu)e^{-2\pi i\omega_{j}h} \tag{4.35}\]

where \(\omega_{j}\) is a non-zero fundamental frequency. Taking expectation in (4.35) we obtain

\[\mathrm{E}\left[I(\omega_{j})\right]=\sum_{h=-(n-1)}^{n-1}\left(\frac{n-|h|}{n }\right)\,\gamma(h)e^{-2\pi i\omega_{j}h}. \tag{4.36}\]

For any given \(\omega\neq 0\), choose a sequence of fundamental frequencies \(\omega_{j:n}\to\omega^{s}\) from which it follows by (4.36) that, as \(n\to\infty\)6

Footnote 6: From Definition 4.2 we have \(I(0)=n\bar{x}^{2}\), so the analogous result of (4.37) for the case \(\omega=0\) is \(\mathrm{E}[I(0)]-n\mu^{2}=n\,\mathrm{var}(\bar{x})\to f(0)\) as \(n\to\infty\).

\[\mathrm{E}\left[I(\omega_{j:n})\right]\to f(\omega)=\sum_{h=-\infty}^{\infty} \gamma(h)e^{-2\pi ih\omega}. \tag{4.37}\]

In other words, under absolute summability of \(\gamma(h)\), the spectral density is the long-term average of the periodogram.

Additional asymptotic properties may be established under the condition that the autocovariance function satisfies

\[\theta=\sum_{h=-\infty}^{\infty}|h||\gamma(h)|\,<\infty\,. \tag{4.38}\]

First, we note that straight-forward calculations lead to

\[\mathrm{cov}[d_{c}(\omega_{j}),d_{c}(\omega_{k})] =n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma(s-t)\cos(2\pi\omega_{j} s)\cos(2\pi\omega_{k}t)\,, \tag{4.39}\] \[\mathrm{cov}[d_{c}(\omega_{j}),d_{s}(\omega_{k})] =n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma(s-t)\cos(2\pi\omega_{j} s)\sin(2\pi\omega_{k}t)\,,\] (4.40) \[\mathrm{cov}[d_{s}(\omega_{j}),d_{s}(\omega_{k})] =n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma(s-t)\sin(2\pi\omega_{j} s)\sin(2\pi\omega_{k}t)\,, \tag{4.41}\]

where the variance terms are obtained by setting \(\omega_{j}=\omega_{k}\) in (4.39) and (4.41). In Appendix C, Sect. C.2, we show the terms in (4.39)-(4.41) have interesting properties under assumption that (4.38) holds. In particular, for \(\omega_{j},\omega_{k}\neq 0\) or \(1/2\),

\[\mathrm{cov}[d_{c}(\omega_{j}),d_{c}(\omega_{k})]=\begin{cases}f(\omega_{j})/2 +\varepsilon_{n}&\omega_{j}=\omega_{k},\\ \varepsilon_{n}&\omega_{j}\neq\omega_{k},\end{cases} \tag{4.42}\]\[\mathrm{cov}[d_{s}(\omega_{j}),d_{s}(\omega_{k})]=\begin{cases}f(\omega_{j})/2+ \varepsilon_{n}&\omega_{j}=\omega_{k},\\ \varepsilon_{n}&\omega_{j}\neq\omega_{k},\end{cases} \tag{4.43}\]

and

\[\mathrm{cov}[d_{c}(\omega_{j}),d_{s}(\omega_{k})]=\varepsilon_{n}\,, \tag{4.44}\]

where the error term \(\varepsilon_{n}\) in the approximations can be bounded,

\[|\varepsilon_{n}|\leq\theta/n\,, \tag{4.45}\]

and \(\theta\) is given by (4.38). If \(\omega_{j}=\omega_{k}=0\) or \(1/2\) in (4.42), the multiplier \(1/2\) disappears; note that \(d_{s}(0)=d_{s}(1/2)=0\), so (4.43) does not apply in these cases.

**Example 4.12**: **Covariance of Sine and Cosine Transforms**

For the three-point moving average series of Example 1.9 and \(n=256\) observations, the theoretical covariance matrix of the vector \(D=(d_{c}(\omega_{26}),d_{s}(\omega_{26}),\)\(d_{c}(\omega_{27}),d_{s}(\omega_{27}))^{\prime}\) using (4.39)-(4.41) is

\[\mathrm{cov}(D)=\begin{pmatrix}.3752&-.0009&\vdots&-.0022&-.0010\\ -.0009&.3777&\vdots&-.0009&.0003\\ -.0022&-.0009&\vdots&3667&-.0010\\ -.0010&.0003&\vdots&-.0010&.3692\end{pmatrix}.\]

The diagonal elements can be compared with half the theoretical spectral values of \(\frac{1}{2}f(\omega_{26})=.3774\) for the spectrum at frequency \(\omega_{26}=26/256\), and of \(\frac{1}{2}f(\omega_{27})=.3689\) for the spectrum at \(\omega_{27}=27/256\). Hence, the cosine and sine transforms produce nearly uncorrelated variables with variances approximately equal to one half of the theoretical spectrum. For this particular case, the uniform bound is determined from \(\theta=8/9\), yielding \(|\varepsilon_{256}|\leq.0035\) for the bound on the approximation error.

If \(x_{t}\sim\mathrm{iid}(0,\sigma^{2})\), then it follows from (4.38)-(4.44), and a central limit theorem7 that

Footnote 7: If \(\{Y_{j}\}\sim\mathrm{iid}(0,\sigma^{2})\) and \(\{a_{j}\}\) are constants for which \(\sum_{j=1}^{n}a_{j}^{2}/\mathrm{max}_{1\leq j\leq n}\,a_{j}^{2}\to\infty\) as \(n\to\infty\), then \(\sum_{j=1}^{n}a_{j}Y_{j}\sim\mathrm{AN}\!\left(0,\sigma^{2}\sum_{j=1}^{n}a_{j} ^{2}\right)\). AN is read _asymptotically normal_; see Definition A.5.

\[d_{c}(\omega_{j:n})\sim\mathrm{AN}\!\left(0,\sigma^{2}/2\right)\quad\text{and}\quad d_{s}(\omega_{j:n})\sim\mathrm{AN}\!\left(0,\sigma^{2}/2\right) \tag{4.46}\]

jointly and independently, and independent of \(d_{c}(\omega_{k:n})\) and \(d_{s}(\omega_{k:n})\) provided \(\omega_{j:n}\to\omega_{1}\) and \(\omega_{k:n}\to\omega_{2}\) where \(0<\omega_{1}\neq\omega_{2}<1/2\). We note that in this case, \(f_{x}(\omega)=\sigma^{2}\). In view of (4.46), it follows immediately that as \(n\to\infty\),

\[\frac{2I(\omega_{j:n})}{\sigma^{2}}\to\chi_{2}^{2}\quad\text{and}\quad\frac{2I (\omega_{k:n})}{\sigma^{2}}\to\chi_{2}^{2} \tag{4.47}\]

with \(I(\omega_{j:n})\) and \(I(\omega_{k:n})\) being asymptotically independent, where \(\chi_{\nu}^{2}\) denotes a chi-squared random variable with \(\nu\) degrees of freedom. If the process is also Gaussian, then the above statements are true for any sample size.

Using the central limit theory of Sect. C.2, it is fairly easy to extend the results of the iid case to the case of a linear process.

**Property 4.6**: **Distribution of the Periodogram Ordinates**__

_If_

\[x_{t}=\sum_{j=-\infty}^{\infty}\psi_{j}w_{t-j},\qquad\sum_{j=-\infty}^{\infty}| \psi_{j}|<\infty \tag{4.48}\]

_where \(w_{t}\sim\text{iid}(0,\sigma_{w}^{2})\), and (4.38) holds, then for any collection of \(m\) distinct frequencies \(\omega_{j}\in(0,1/2)\) with \(\omega_{j:n}\to\omega_{j}\)_

\[\frac{2I(\omega_{j:n})}{f(\omega_{j})}\overset{d}{\to}\text{iid}\ \chi_{2}^{2} \tag{4.49}\]

_provided \(f(\omega_{j})>0\), for \(j=1,\ldots,m\)._

This result is stated more precisely in Theorem C.7. Other approaches to large sample normality of the periodogram ordinates are in terms of cumulants, as in Brillinger [35], or in terms of mixing conditions, such as in Rosenblatt [169]. Here, we adopt the approach used by Hannan [86], Fuller [66], and Brockwell and Davis [36].

The distributional result (4.49) can be used to derive an approximate _confidence interval for the spectrum_ in the usual way. Let \(\chi_{\nu}^{2}(\alpha)\) denote the lower \(\alpha\) probability tail for the chi-squared distribution with \(\nu\) degrees of freedom; that is,

\[\Pr\{\chi_{\nu}^{2}\leq\chi_{\nu}^{2}(\alpha)\}=\alpha. \tag{4.50}\]

Then, an approximate \(100(1-\alpha)\%\) confidence interval for the spectral density function would be of the form

\[\frac{2\ I(\omega_{j:n})}{\chi_{2}^{2}(1-\alpha/2)}\leq f(\omega)\leq\frac{2\ I(\omega_{j:n})}{\chi_{2}^{2}(\alpha/2)}. \tag{4.51}\]

Often, trends are present that should be eliminated before computing the periodogram. Trends introduce extremely low frequency components in the periodogram that tend to obscure the appearance at higher frequencies. For this reason, it is usually conventional to center the data prior to a spectral analysis using either mean-adjusted data of the form \(x_{t}-\bar{x}\) to eliminate the zero or d-c component or to use detrended data of the form \(x_{t}-\hat{\beta}_{1}-\hat{\beta}_{2}t\) to eliminate the term that will be considered a half cycle by the spectral analysis. Note that higher order polynomial regressions in \(t\) or nonparametric smoothing (linear filtering) could be used in cases where the trend is nonlinear.

As previously indicated, it is often convenient to calculate the DFTs, and hence the periodogram, using the fast Fourier transform algorithm. The FFT utilizes a number of redundancies in the calculation of the DFT when \(n\) is highly composite; that is, an integer with many factors of \(2,3,\) or \(5\), the best case being when \(n=2^{p}\) is a factor of \(2\). Details may be found in Cooley and Tukey [44]. To accommodate this property, we can pad the centered (or detrended) data of length \(n\) to the next highly composite integer \(n^{\prime}\) by adding zeros, i.e., setting \(x_{n+1}^{c}=x_{n+2}^{c}=\cdots=x_{n^{\prime}}^{c}=0\), where \(x_{t}^{c}\) denotes the centered data. This means that the fundamental frequency ordinates willbe \(\omega_{j}=j/n^{\prime}\) instead of \(j/n\). We illustrate by considering the periodogram of the SOI and Recruitment series shown in Fig. 5. Recall that they are monthly series and \(n=453\) months. To find \(n^{\prime}\) in R, use the command next(453) to see that \(n^{\prime}=480\) will be used in the spectral analyses by default.

**Example 4.13**: **Periodogram of SOI and Recruitment Series**

Figure 5 shows the periodograms of each series, where the frequency axis is labeled in multiples of \(\Delta=1/12\). As previously indicated, the centered data have been padded to a series of length 480. We notice a narrow-band peak at the obvious yearly (12 month) cycle, \(\omega=1\Delta=1/12\). In addition, there is considerable power in a wide band at the lower frequencies that is centered around the four-year (48 month) cycle \(\omega=\frac{1}{4}\Delta=1/48\) representing a possible El Nino effect. This wide band activity suggests that the possible El Nino cycle is irregular, but tends to be around four years on average. We will continue to address this problem as we move to more sophisticated analyses.

Figure 5: Periodogram of SOI and Recruitment, \(n=453\) (\(n^{\prime}=480\)), where the frequency axis is labeled in multiples of \(\Delta=1/12\). Note the common peaks at \(\omega=1\Delta=1/12\), or one cycle per year (12 months), and some larger values near \(\omega=\frac{1}{4}\Delta=1/48\), or one cycle every four years (48 months)

Noting \(\chi^{2}_{2}(.025)=.05\) and \(\chi^{2}_{2}(.975)=7.38\), we can obtain approximate 95% confidence intervals for the frequencies of interest. For example, the periodogram of the SOI series is \(I_{S}(1/12)=.97\) at the yearly cycle. An approximate 95% confidence interval for the spectrum \(f_{S}(1/12)\) is then \[[2(.97)/7.38,\,2(.97)/.05]=[.26,\,38.4],\] which is too wide to be of much use. We do notice, however, that the lower value of \(.26\) is higher than any other periodogram ordinate, so it is safe to say that this value is significant. On the other hand, an approximate 95% confidence interval for the spectrum at the four-year cycle, \(f_{S}(1/48)\), is \[[2(.05)/7.38,\,2(.05)/.05]=[.01,\,2.12],\] which again is extremely wide, and with which we are unable to establish significance of the peak. We now give the R commands that can be used to reproduce Fig. 4.5. To calculate and graph the periodogram, we used the mvspec command in available from astsa. We note that the value of \(\Delta\) is the reciprocal of the value of frequency for the data of a time series object. If the data are not a time series object, frequency is set to 1. Also, we set log="no" because the periodogram is plotted on a log\({}_{10}\) scale by default. Figure 4.5 displays a bandwidth. We will discuss bandwidth in the next section, so ignore this for the time being. par(mfrow=c(2,1))  soi.per = mvspec(soi, log="no")  albine(v=1/4, lty=2)  rec.per = mvspec(rec, log="no")  albine(v=1/4, lty=2) The confidence intervals for the SOI series at the yearly cycle, \(\omega=1/12=40/480\), and the possible El Nino cycle of four years \(\omega=1/48=10/480\) can be computed in R as follows:  soi.perspec[40] # 0.97223; soi pgram at freq 1/12 = 40/480  soi.perspec[10] # 0.05372; soi pgram at freq 1/48 = 10/480  # conf intervals - returned value:  U = qchisq(.025,2) # 0.05063  L = qchisq(.975,2) # 7.37775 2*soi.perspec[10]/L # 0.01456  2*soi.perspec[10]/U # 2.12220  2*soi.perspec[40]/L # 0.26355  2*soi.perspec[40]/U # 38.40108 The preceding example made it clear that the periodogram as an estimator is susceptible to large uncertainties, and we need to find a way to reduce the variance. Not surprisingly, this result follows if consider (4.49) and the fact that, for any \(n\), the periodogram is based on only two observations. Recall that the mean and variance of the \(\chi^{2}_{\nu}\) distribution are \(\nu\) and \(2\nu\), respectively. Thus, using (4.49), we have \(I(\omega)\stackrel{{\cdot}}{{\sim}}\frac{1}{2}f(\omega)\chi^{2}_{2}\), implying \[\mathrm{E}[I(\omega)]\approx f(\omega)\quad\text{and}\quad\mathrm{var}[I(\omega)] \approx f^{2}(\omega).\]Consequently, \(\mathrm{var}[I(\omega)]\not\to 0\) as \(n\to\infty\) and thus the periodogram is not a consistent estimator of the spectral density. The solution to this dilemma can be resolved by smoothing the periodogram.

### 4.4 Nonparametric Spectral Estimation

To continue the discussion that ended the previous section, we introduce a _frequency band_, \(\mathcal{B}\), of \(L\ll n\) contiguous fundamental frequencies, centered around frequency \(\omega_{j}=j/n\), which is chosen close to a frequency of interest, \(\omega\). For frequencies of the form \(\omega^{*}=\omega_{j}+k/n\), let

\[\mathcal{B}=\bigg{\{}\omega^{*}\colon\,\omega_{j}-\frac{m}{n}\leq\omega^{*} \leq\omega_{j}+\frac{m}{n}\bigg{\}}, \tag{4.52}\]

where

\[L=2m+1 \tag{4.53}\]

is an odd number, chosen such that the spectral values in the interval \(\mathcal{B}\),

\[f(\omega_{j}+k/n),\quad k=-m,\ldots,0,\ldots,m\]

are approximately equal to \(f(\omega)\). This structure can be realized for large sample sizes, as shown formally in Sect. C.2. Values of the spectrum in this band should be relatively constant for the smoothed spectra defined below to be good estimators. For example, to see a small section of the AR(2) spectrum (near the peak) shown in Fig. 4.4, use

arma.spec(ar=c(1,-.9), xlim=c(.15,.151), n.freq=100000)

which is displayed in Fig. 4.6.

We now define an averaged (or smoothed) periodogram as the average of the periodogram values, say,

Figure 4.6: A small section (near the peak) of the AR(2) spectrum shown in Fig. 4.4

[MISSING_PAGE_EMPTY:201]

some of the main power components. Taking logs in (4.58), we obtain an interval for the logged spectrum given by

\[\left[\log\tilde{f}(\omega)-a_{L},\,\log\tilde{f}(\omega)+b_{L}\right] \tag{4.59}\]

where

\[a_{L}=-\log 2L+\log\chi_{2L}^{2}(1-\alpha/2)\quad\text{and}\quad b_{L}=\log 2L- \log\chi_{2L}^{2}(\alpha/2)\]

do not depend on \(\omega\).

If zeros are appended before computing the spectral estimators, we need to adjust the degrees of freedom (because you do not get more information by padding) and an approximation is to replace \(2L\) by \(2Ln/n^{\prime}\). Hence, we define the _adjusted degrees of freedom_ as

\[df=\frac{2Ln}{n^{\prime}} \tag{4.60}\]

and use it instead of \(2L\) in the confidence intervals (4.58) and (4.59). For example, (4.58) becomes

\[\frac{df\tilde{f}(\omega)}{\chi_{df}^{2}(1-\alpha/2)}\leq f(\omega)\leq\frac{ df\tilde{f}(\omega)}{\chi_{df}^{2}(\alpha/2)}. \tag{4.61}\]

A number of assumptions are made in computing the approximate confidence intervals given above, which may not hold in practice. In such cases, it may be reasonable to employ resampling techniques such as one of the parametric bootstraps proposed by Hurvich and Zeger [99] or a nonparametric _local bootstrap_ proposed by Paparoditis and Politis [147]. To develop the bootstrap distributions, we assume that the contiguous DFTs in a frequency band of the form (4.52) all came from a time series with identical spectrum \(f(\omega)\). This, in fact, is exactly the same assumption made in deriving the large-sample theory. We may then simply resample the \(L\) DFTs in the band, with replacement, calculating a spectral estimate from each bootstrap sample. The sampling distribution of the bootstrap estimators approximates the distribution of the nonparametric spectral estimator. For further details, including the theoretical properties of such estimators, see Paparoditis and Politis [147].

Before proceeding further, we consider computing the average periodograms for the SOI and Recruitment series.

**Example 4.14**: **Averaged Periodogram for SOI and Recruitment**

Generally, it is a good idea to try several bandwidths that seem to be compatible with the general overall shape of the spectrum, as suggested by the periodogram. We will discuss this problem in more detail after the example. The SOI and Recruitment series periodograms, previously computed in Fig. 4.5, suggest the power in the lower El Nino frequency needs smoothing to identify the predominant overall period. Trying values of \(L\) leads to the choice \(L=9\) as a reasonable value, and the result is displayed in Fig. 4.7.

The smoothed spectra shown provide a sensible compromise between the noisy version, shown in Fig. 4.5, and a more heavily smoothed spectrum, which might lose some of the peaks. An undesirable effect of averaging can be noticed at the yearly cycle, \(\omega=1\Delta\), where the narrow band peaks that appeared in the periodograms in Fig. 4.5 have been flattened and spread out to nearby frequencies. We also notice, and have marked, the appearance of _harmonics_ of the yearly cycle, that is, frequencies of the form \(\omega=k\Delta\) for \(k=1,2,\dots\). Harmonics typically occur when a periodic non-sinusoidal component is present; see Example 4.15.

Figure 4.7 can be reproduced in R using the following commands. To compute averaged periodograms, use the Daniell kernel, and specify \(m\), where \(L=2m+1\) (\(L=9\) and \(m=4\) in this example). We will explain the kernel concept later in this section, specifically just prior to Example 4.16.

```
soi.ave=mvspec(soi,kernel('daniell',4)),log='no') abline(v=c(.25,1,2,3),lty=2) soi.ave$bandwidth#=0.225
#Repeatabovelinesusingrecinplaceofsoionline3
```

Figure 4.7: The averaged periodogram of the SOI and Recruitment series \(n=453,\ n^{\prime}=480\), \(L=9,\ df=17\), showing common peaks at the four year period, \(\omega=\frac{1}{4}\Delta=1/48\) cycles/month, the yearly period, \(\omega=1\Delta=1/12\) cycles/month and some of its harmonics \(\omega=k\Delta\) for \(k=2,3\)

The displayed bandwidth (.225) is adjusted for the fact that the frequency scale of the plot is in terms of cycles per year instead of cycles per month. Using (4.56), the bandwidth in terms of months is \(9/480=.01875\); the displayed value is simply converted to years, \(.01875\times 12=.225\).

The adjusted degrees of freedom are \(df=2(9)(453)/480\approx 17\). We can use this value for the 95% confidence intervals, with \(\chi^{2}_{df}(.025)=7.56\) and \(\chi^{2}_{df}(.975)=30.17\). Substituting into (4.61) gives the intervals in Table 4.1 for the two frequency bands identified as having the maximum power. To examine the two peak power possibilities, we may look at the 95% confidence intervals and see whether the lower

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Series & \(\omega\) & Period & Power & Lower & Upper \\ \hline SOI & 1/48 & 4 years &.05 &.03 &.11 \\  & 1/12 & 1 year &.12 &.07 &.27 \\ \hline Recruits & 1/48 & 4 years & 6.59 & 3.71 & 14.82 \\ \(\times 10^{2}\) & 1/12 & 1 year & 2.19 & 1.24 & 4.93 \\ \hline \end{tabular}
\end{table}
Table 4.1: Confidence intervals for the spectra of the SOI and recruitment series

Figure 4.8: Figure 4.7 with the average periodogram ordinates plotted on a \(\log_{10}\) scale. The display in the upper right-hand corner represents a generic 95% confidence interval where the middle tick mark is the width of the bandwidth

[MISSING_PAGE_FAIL:205]

for \(0\leq t\leq 1\). Notice that the signal is non-sinusoidal in appearance and rises quickly then falls slowly.

A figure similar to Fig. 4.9 can be generated in R as follows.

t = seq(0, 1, by=1/200) amps = c(1,.5,.4,.3,.2,.1) x = matrix(0, 201, 6) for (jin 1:6){ x[,j] = amps[j]*sin(2*pi*t*2*j) } x = ts(cbind(x, rowSums(x)), start=theta, deltat=1/200) ts.plot(x, lty=c(1:6, 1), lwd=c(rep(1,6), 2), ylab="Simusoids") names = c("Fundamental", "2nd Harmonic", "3rd Harmonic", "4th Harmonic", "5th Harmonic", "6th Harmonic", "Formed Signal") legend("topright", names, lty=c(1:6, 1), lwd=c(rep(1,6), 2))

Example 4.14 points out the necessity for having some relatively systematic procedure for deciding whether peaks are significant. The question of deciding whether a single peak is significant usually rests on establishing what we might think of as a baseline level for the spectrum, defined rather loosely as the shape that one would expect to see if no spectral peaks were present. This profile can usually be guessed by looking at the overall shape of the spectrum that includes the peaks; usually, a kind of baseline level will be apparent, with the peaks seeming to emerge from this baseline level. If the lower confidence limit for the spectral value is still greater than the baseline level at some predetermined level of significance, we may claim that frequency value as a statistically significant peak. To be consistent with our stated indifference to the upper limits, we might use a one-sided confidence interval.

Figure 4.9: A signal (_thick solid line_) formed by a fundamental sinusoid (_thin solid line_) oscillating at two cycles per unit time and its harmonics as specified in (4.62)

An important aspect of interpreting the significance of confidence intervals and tests involving spectra is that typically, more than one frequency will be of interest, so that we will potentially be interested in _simultaneous statements_ about a whole collection of frequencies. For example, it would be unfair to claim in Table 4.1 the two frequencies of interest as being statistically significant and all other potential candidates as nonsignificant at the overall level of \(\alpha=.05\). In this case, we follow the usual statistical approach, noting that if \(K\) statements \(S_{1},S_{2},\ldots,S_{k}\) are made at significance level \(\alpha\), i.e., \(P\{S_{k}\}=1-\alpha\), then the overall probability all statements are true satisfies the _Bonferroni inequality_

\[P\{\text{all }S_{k}\text{ true}\}\geq 1-K\alpha. \tag{4.63}\]

For this reason, it is desirable to set the significance level for testing each frequency at \(\alpha/K\) if there are \(K\) potential frequencies of interest. If, a priori, potentially \(K=10\) frequencies are of interest, setting \(\alpha=.01\) would give an overall significance level of bound of \(.10\).

The use of the confidence intervals and the necessity for smoothing requires that we make a decision about the bandwidth B over which the spectrum will be essentially constant. Taking too broad a band will tend to smooth out valid peaks in the data when the constant variance assumption is not met over the band. Taking too narrow a band will lead to confidence intervals so wide that peaks are no longer statistically significant. Thus, we note that there is a conflict here between variance properties or _bandwidth stability_, which can be improved by increasing B and _resolution_, which can be improved by decreasing B. A common approach is to try a number of different bandwidths and to look qualitatively at the spectral estimators for each case.

To address the problem of resolution, it should be evident that the flattening of the peaks in Figs. 4.7 and 4.8 was due to the fact that simple averaging was used in computing \(\tilde{f}(\omega)\) defined in (4.54). There is no particular reason to use simple averaging, and we might improve the estimator by employing a weighted average, say

\[\hat{f}(\omega)=\sum_{k=-m}^{m}h_{k}\ I(\omega_{j}+k/n), \tag{4.64}\]

using the same definitions as in (4.54) but where the weights \(h_{k}>0\) satisfy

\[\sum_{k=-m}^{m}h_{k}=1.\]

In particular, it seems reasonable that the resolution of the estimator will improve if we use weights that decrease as distance from the center weight \(h_{0}\) increases; we will return to this idea shortly. To obtain the averaged periodogram, \(\tilde{f}(\omega)\), in (4.64), set \(h_{k}=L^{-1}\), for all \(k\), where \(L=2m+1\). The asymptotic theory established for \(\tilde{f}(\omega)\) still holds for \(\hat{f}(\omega)\) provided that the weights satisfy the additional condition that if \(m\to\infty\) as \(n\to\infty\) but \(m/n\to 0\), then

\[\sum_{k=-m}^{m}h_{k}^{2}\to 0.\]

[MISSING_PAGE_EMPTY:208]

which simplifies to

\[\hat{\hat{u}}_{t}=\tfrac{1}{9}u_{t-2}+\tfrac{2}{9}u_{t-1}+\tfrac{3}{9}u_{t}+\tfrac {2}{9}u_{t+1}+\tfrac{1}{9}u_{t+2}.\]

The _modified Daniell kernel_ puts half weights at the end points, so with \(m=1\) the weights are \(\{h_{k}\}=\{\tfrac{1}{4},\tfrac{2}{4},\tfrac{1}{4}\}\) and

\[\hat{u}_{t}=\tfrac{1}{4}u_{t-1}+\tfrac{1}{2}u_{t}+\tfrac{1}{4}u_{t+1}.\]

Applying the same kernel again to \(\hat{u}_{t}\) yields

\[\hat{\hat{u}}_{t}=\tfrac{1}{16}u_{t-2}+\tfrac{4}{16}u_{t-1}+\tfrac{6}{16}u_{t} +\tfrac{4}{16}u_{t+1}+\tfrac{1}{16}u_{t+2}.\]

These coefficients can be obtained in R by issuing the kernel command. For example, kernel("modified.daniell", c(1,1)) would produce the coefficients of the last example. The other kernels that are currently available in R are the Dirichlet kernel and the Fejer kernel, which we will discuss shortly.

It is interesting to note that these kernel weights form a probability distribution. If \(X\) and \(Y\) are independent discrete uniforms on the integers \(\{-1,0,1\}\) each with probability \(\tfrac{1}{3}\), then the convolution \(X+Y\) is discrete on the integers \(\{-2,-1,0,1,2\}\) with corresponding probabilities \(\{\tfrac{1}{9},\tfrac{2}{9},\tfrac{3}{9},\tfrac{2}{9},\tfrac{1}{9}\}\).

**Example 4.16**: **Smoothed Periodogram for SOI and Recruitment**

In this example, we estimate the spectra of the SOI and Recruitment series using the smoothed periodogram estimate in (4.64). We used a modified Daniell kernel twice, with \(m=3\) both times. This yields \(L_{h}=1/\sum_{k=-m}^{m}h_{k}^{2}=9.232\), which is close to the value of \(L=9\) used in Example 4.14. In this case, the bandwidth is \(\text{B}=9.232/480=.019\) and the modified degrees of freedom is \(df=2L_{h}453/480=17.43\). The weights, \(h_{k}\), can be obtained and graphed in R as follows:

kernel("modified.daniell", c(3,3))

coef[-6]=0.006944=coef[6]

coef[-5]=0.027778=coef[5]

coef[-4]=0.055556=coef[4]

coef[-3]=0.083333=coef[3]

coef[-2]=0.111111=coef[2]

coef[-1]=0.138889=coef[1]

coef[0]=0.152778

plot(kernel("modified.daniell", c(3,3))) # not shown

The resulting spectral estimates can be viewed in Fig. 10 and we notice that the estimates more appealing than those in Fig. 7. Figure 10 was generated in R as follows; we also show how to obtain the associated bandwidth and degrees of freedom.

k = kernel("modified.daniell", c(3,3))

soi.smo = mwspec(soi, kernel=k, taper=.1, log="no")

albime(v=c(.25,1), lty=2)

## Repeat above lines with rec replacing soil in line 3

df = soi.smo$df#df=17.42618

soi.smo$bandwidth#B=0.2308103Note that a _taper_ was applied in the estimation process; we discuss tapering in the next part. Reissuing the mvspec commands with log="no" removed will result in a figure similar to Fig. 4.8. Finally, we mention that the modified Daniell kernel is used by default and an easier way to obtain soi.smo is to issue the command:

soi.smo = mvspec(soi, taper=.1, spans=c(7,7))

Notice that spans is a vector of odd integers, given in terms of \(L=2m+1\) instead of \(m\).

There have been many attempts at dealing with the problem of smoothing the periodogram in a automatic way; an early reference is Wahba [205]. It is apparent from Example 4.16 that the smoothing bandwidth for the broadband El Nino behavior (near the 4 year cycle), should be much larger than the bandwidth for the annual cycle (the 1 year cycle). Consequently, it is perhaps better to perform automatic adaptive smoothing for estimating the spectrum. We refer interested readers to Fan and Kreutzberger [61] and the numerous references within.

### Tapering

We are now ready to introduce the concept of _tapering_; a more detailed discussion may be found in Bloomfield [25, SS9.5]. Suppose \(x_{t}\) is a mean-zero, stationary process

Figure 4.10: Smoothed (tapered) spectral estimates of the SOI and Recruitment series; see Example 4.16 for details

with spectral density \(f_{x}(\omega)\). If we replace the original series by the tapered series

\[y_{t}=h_{t}x_{t}, \tag{4.69}\]

for \(t=1,2,\ldots,n\), use the modified DFT

\[d_{y}(\omega_{j})=n^{-1/2}\sum_{t=1}^{n}h_{t}x_{t}\mathrm{e}^{-2\pi i\omega_{j} t}, \tag{4.70}\]

and let \(I_{y}(\omega_{j})=|d_{y}(\omega_{j})|^{2}\), we obtain (see Problem 4.17)

\[\mathrm{E}[I_{y}(\omega_{j})]=\int_{-\frac{1}{2}}^{\frac{1}{2}}W_{n}(\omega_{j }-\omega)\;f_{x}(\omega)\;d\omega \tag{4.71}\]

where

\[W_{n}(\omega)=|H_{n}(\omega)|^{2} \tag{4.72}\]

and

\[H_{n}(\omega)=n^{-1/2}\sum_{t=1}^{n}h_{t}\mathrm{e}^{-2\pi i\omega t}. \tag{4.73}\]

The value \(W_{n}(\omega)\) is called a _spectral window_ because, in view of (4.71), it is determining which part of the spectral density \(f_{x}(\omega)\) is being "seen" by the estimator \(I_{y}(\omega_{j})\) on average. In the case that \(h_{t}=1\) for all \(t\), \(I_{y}(\omega_{j})=I_{x}(\omega_{j})\) is simply the periodogram of the data and the window is

\[W_{n}(\omega)=\frac{\sin^{2}(n\pi\omega)}{n\sin^{2}(\pi\omega)} \tag{4.74}\]

with \(W_{n}(0)=n\), which is known as the Fejer or modified Bartlett kernel. If we consider the averaged periodogram in (4.54), namely

\[\bar{f}_{x}(\omega)=\frac{1}{L}\sum_{k=-m}^{m}I_{x}(\omega_{j}+k/n),\]

the window, \(W_{n}(\omega)\), in (4.71) will take the form

\[W_{n}(\omega)=\frac{1}{nL}\sum_{k=-m}^{m}\frac{\sin^{2}[n\pi(\omega+k/n)]}{ \sin^{2}[\pi(\omega+k/n)]}\;. \tag{4.75}\]

Tapers generally have a shape that enhances the center of the data relative to the extremities, such as a cosine bell of the form

\[h_{t}=.5\Bigg{[}1+\cos\Bigg{(}\frac{2\pi(t-\overline{t})}{n}\Bigg{)}\Bigg{]}, \tag{4.76}\]

where \(\overline{t}=(n+1)/2\), favored by Blackman and Tukey [23]. The shape of this taper is shown in the insert to Fig. 12. In Fig. 11, we have plotted the shapes of two windows, \(W_{n}(\omega)\), for \(n=480\) and \(L=9\), when (i) \(h_{t}\equiv 1\), in which case, (4.75) applies,and (ii) \(h_{t}\) is the cosine taper in (4.76). In both cases the predicted bandwidth should be \(\mathrm{B}=9/480=.01875\) cycles per point, which corresponds to the "width" of the windows shown in Fig. 4.11. Both windows produce an integrated average spectrum over this band but the untapered window in the top panels shows considerable ripples over the band and outside the band. The ripples outside the band are called sidelobes and tend to introduce frequencies from outside the interval that may contaminate the desired spectral estimate within the band. For example, a large dynamic range for the values in the spectrum introduces spectra in contiguous frequency intervals several orders of magnitude greater than the value in the interval of interest. This effect is sometimes called _leakage_. Figure 4.11 emphasizes the suppression of the sidelobes in the Fejer kernel when a cosine taper is used.

**Example 4.17**: **The Effect of Tapering the SOI Series**

The estimates in Example 4.16 were obtained by tapering the upper and lower 10% of the data. In this example, we examine the effect of tapering on the estimate of the spectrum of the SOI series (the results for the Recruitment series are similar). Figure 4.12 shows two spectral estimates plotted on a log scale. The dashed line in Fig. 4.12 shows the estimate without any tapering. The solid line shows the result

Figure 4.11: Averaged Fejer window (_top row_) and the corresponding cosine taper window (_bottom row_) for \(L=9,\ n=480\). The extra tic marks on the horizontal axis of the left-hand plots exhibit the predicted bandwidth, \(\mathrm{B}=9/480=.01875\)

with full tapering. Notice that the tapered spectrum does a better job in separating the yearly cycle (\(\omega=1\)) and the El Nino cycle (\(\omega=1/4\)).

The following R session was used to generate Fig. 12. We note that, by default, mvspec does not taper. For full tapering, we use the argument taper=.5 to instruct mvspec to taper 50% of each end of the data; any value between 0 and.5 is acceptable. In Example 4.16, we used taper=.1.

 s0 = mvspec(soi, spans=c(7,7), plot=FALSE) # no taper  s50 = mvspec(soi, spans=c(7,7), taper=.5, plot=FALSE) # full taper  plot(s50%freq, s50%spec, log="y", type="1", ylab="spectrum",  xlab="frequency") # solid line  lines(s0%freq, s0%spec, lty=2) # dashed line

We close this section with a brief discussion of _lag window_ estimators. First, consider the periodogram, \(I(\omega_{j})\), which was shown in (4.30) to be

\[I(\omega_{j})=\ \sum_{|h|<n}\hat{\gamma}(h)e^{-2\pi i\omega_{j}h}.\]

Thus, (4.64) can be written as

\[\hat{f}(\omega) = \sum_{|k|\leq m}h_{k}\ I(\omega_{j}+k/n)=\ \sum_{|k|\leq m}h_{k} \ \sum_{|h|<n}\hat{\gamma}(h)e^{-2\pi i(\omega_{j}+k/n)h} \tag{4.77}\] \[= \sum_{|h|<n}g(\tfrac{h}{n})\ \hat{\gamma}(h)e^{-2\pi i\omega_{j}h}.\]

where \(g(\tfrac{h}{n})=\sum_{|k|\leq m}h_{k}\ \exp(-2\pi ikh/n)\). Equation (4.77) suggests estimators of the form

\[\tilde{f}(\omega)=\sum_{|h|\leq r}w(\tfrac{h}{r})\ \hat{\gamma}(h)e^{-2\pi i \omega h} \tag{4.78}\]

Figure 12: Smoothed spectral estimates of the SOI without tapering (_dashed line_) and with full tapering (_solid line_); see Example 4.17. The insert shows a full cosine bell taper, (4.76), with horizontal axis \((t-\tilde{t})/n\), for \(t=1,\ldots,n\)

where \(w(\cdot)\) is a weight function, called the lag window, that satisfies

1. \(w(0)=1\)
2. \(|w(x)|\leq 1\) and \(w(x)=0\) for \(|x|>1\),
3. \(w(x)=w(-x)\).

Note that if \(w(x)=1\) for \(|x|<1\) and \(r=n\), then \(\tilde{f}(\omega_{j})=I(\omega_{j})\), the periodogram. This result indicates the problem with the periodogram as an estimator of the spectral density is that it gives too much weight to the values of \(\hat{\gamma}(h)\) when \(h\) is large, and hence is unreliable [e.g, there is only one pair of observations used in the estimate \(\hat{\gamma}(n-1)\), and so on]. The smoothing window is defined to be

\[W(\omega)=\sum_{h=-r}^{r}w(\tfrac{h}{r})e^{-2\pi i\omega h}, \tag{4.79}\]

and it determines which part of the periodogram will be used to form the estimate of \(f(\omega)\). The asymptotic theory for \(\hat{f}(\omega)\) holds for \(\tilde{f}(\omega)\) under the same conditions and provided \(r\to\infty\) as \(n\to\infty\) but with \(r/n\to 0\). That is,

\[\operatorname{E}\{\tilde{f}(\omega)\}\to f(\omega), \tag{4.80}\]

\[\frac{n}{r}\text{cov}\left(\tilde{f}(\omega),\tilde{f}(\lambda)\right)\to f^{ 2}(\omega)\int_{-1}^{1}w^{2}(x)dx\qquad\omega=\lambda\neq 0,1/2. \tag{4.81}\]

In (4.81), replace \(f^{2}(\omega)\) by \(0\) if \(\omega\neq\lambda\) and by \(2f^{2}(\omega)\) if \(\omega=\lambda=0\) or \(1/2\).

Many authors have developed various windows and Brillinger [35, Ch 3] and Brockwell and Davis [36, Ch 10] are good sources of detailed information on this topic.

### 4.5 Parametric Spectral Estimation

The methods of the previous section lead to what is generally referred to as _nonparametric spectral estimators_ because no assumption is made about the parametric form of the spectral density. In Property 4.4, we exhibited the spectrum of an ARMA process and we might consider basing a spectral estimator on this function, substituting the parameter estimates from an ARMA(\(p,q\)) fit on the data into the formula for the spectral density \(f_{x}(\omega)\) given in (4.23). Such an estimator is called a parametric spectral estimator. For convenience, a parametric spectral estimator is obtained by fitting an AR(\(p\)) to the data, where the order \(p\) is determined by one of the model selection criteria, such as AIC, AICc, and BIC, defined in (2.15)-(2.17). Parametric autoregressive spectral estimators will often have superior resolution in problems when several closely spaced narrow spectral peaks are present and are preferred by engineers for a broad variety of problems (see Kay [115]). The development of autoregressive spectral estimators has been summarized by Parzen [149].

If \(\hat{\phi}_{1},\hat{\phi}_{2},\ldots,\hat{\phi}_{p}\) and \(\hat{\sigma}_{w}^{2}\) are the estimates from an AR(\(p\)) fit to \(x_{t}\), then based on Property 4.4, a parametric spectral estimate of \(f_{x}(\omega)\) is attained by substituting these estimates into (4.23), that is,\[\hat{f}_{x}(\omega)=\frac{\hat{\sigma}_{w}^{2}}{|\hat{\phi}(\mathrm{e}^{-2\pi i \omega})|^{2}}, \tag{4.82}\]

where

\[\hat{\phi}(z)=1-\hat{\phi}_{1}z-\hat{\phi}_{2}z^{2}-\cdots-\hat{\phi}_{p}z^{p}. \tag{4.83}\]

The asymptotic distribution of the autoregressive spectral estimator has been obtained by Berk [19] under the conditions \(p\to\infty,\ p^{3}/n\to 0\) as \(p,\ n\to\infty\), which may be too severe for most applications. The limiting results imply a confidence interval of the form

\[\frac{\hat{f}_{x}(\omega)}{(1+Cz_{\alpha/2})}\leq f_{x}(\omega)\leq\frac{\hat{ f}_{x}(\omega)}{(1-Cz_{\alpha/2})}, \tag{4.84}\]

where \(C=\sqrt{2p/n}\) and \(z_{\alpha/2}\) is the ordinate corresponding to the upper \(\alpha/2\) probability of the standard normal distribution. If the sampling distribution is to be checked, we suggest applying the bootstrap estimator to get the sampling distribution of \(\hat{f}_{x}(\omega)\) using a procedure similar to the one used for \(p=1\) in Example 3.36. An alternative for higher order autoregressive series is to put the AR(\(p\)) in state-space form and use the bootstrap procedure discussed in Sect. 6.7.

An interesting fact about rational spectra of the form (4.23) is that any spectral density can be approximated, arbitrarily close, by the spectrum of an AR process.

**Property 4.7**: **AR Spectral Approximation**__

_Let \(g(\omega)\) be the spectral density of a stationary process. Then, given \(\epsilon>0\), there is a time series with the representation_

\[x_{t}=\sum_{k=1}^{p}\phi_{k}x_{t-k}+w_{t}\]

_where \(w_{t}\) is white noise with variance \(\sigma_{w}^{2}\), such that_

\[|f_{x}(\omega)-g(\omega)|<\epsilon\quad\text{ for all }\ \omega\in[-1/2,1/2].\]

_Moreover, \(p\) is finite and the roots of \(\phi(z)=1-\sum_{k=1}^{p}\phi_{k}z^{k}\) are outside the unit circle._

One drawback of the property is that it does not tell us how large \(p\) must be before the approximation is reasonable; in some situations \(p\) may be extremely large. Property 4.7 also holds for MA and for ARMA processes in general, and a proof of the result may be found in Sect. C.6. We demonstrate the technique in the following example.

**Example 4.18**: **Autoregressive Spectral Estimator for SOI**__

Consider obtaining results comparable to the nonparametric estimators shown in Fig. 4.7 for the SOI series. Fitting successively higher order AR(\(p\)) models for \(p=1,2,\ldots,30\) yields a minimum BIC and a minimum AIC at \(p=15\), as shown in Fig. 4.13. We can see from Fig. 4.13 that BIC is very definite about which model it chooses; that is, the minimum BIC is very distinct. On the other hand, it is not clear what is going to happen with AIC; that is, the minimum is not so clear, and there is some concern that AIC will start decreasing after \(p=30\). Minimum AICc selects the \(p=15\) model, but suffers from the same uncertainty as AIC. The spectrum is shown in Fig. 4.14, and we note the strong peaks near the four year and one year cycles as in the nonparametric estimates obtained in Sect. 4.4. In addition, the harmonics of the yearly period are evident in the estimated spectrum.

To perform a similar analysis in R, the command spec.ar can be used to fit the best model via AIC and plot the resulting spectrum. A quick way to obtain the AIC values is to run the ar command as follows.

```
spaic=spec.ar(soi,log="no") ahline(v=frequency(soi)*1/52,lty=3) (soi.ar=ar(soi,order.max=30)) dew.new() plot(1:30,soi.ar$aic[-1],type="0")
```

No likelihood is calculated here, so the use of the term AIC is loose. To generate Fig. 4.13 we used the following code to (loosely) obtain AIC, AICc, and BIC. Because AIC and AICc are nearly identical in this example, we only graphed AIC and BIC+1; we added 1 to the BIC to reduce white space in the graphic.

```
n=length(soi) AIC=rep(0,30)->AICc->BIC for(kin1:30){ sigma2=ar(soi,order=k,aic=FALSE)$var.pred BIC[k]=log(sigma2)+(k*log(n)/n AICc[k]=log(sigma2)+((n+k)/(n-k-2)) AIC[k]=log(sigma2)+((n+2*k)/n) } TC=cbind(AIC,BIC+1) ts.plot(IC,type="0",xlab="p",ylab="AIC/BIC")
```

Finally, it should be mentioned that any parametric spectrum, say \(f(\omega;\theta)\), depending on the vector parameter \(\theta\) can be estimated via the Whittle likelihood (Whittle [210]), using the approximate properties of the discrete Fourier transform derived

Figure 4.13: Model selection criteria AIC and BIC as a function of order \(p\) for autoregressive models fitted to the SOI series

in C. We have that the DFTs, \(d(\omega_{j})\), are approximately complex normally distributed with mean zero and variance \(f(\omega_{j};\theta)\) and are approximately independent for \(\omega_{j}\neq\omega_{k}\). This implies that an approximate log likelihood can be written in the form

\[\ln L(x;\theta)\approx-\sum_{0<\omega_{j}<1/2}\left(\ln f_{x}(\omega_{j};\theta )+\frac{|d(\omega_{j})|^{2}}{f_{x}(\omega_{j};\theta)}\right), \tag{4.85}\]

where the sum is sometimes expanded to include the frequencies \(\omega_{j}=0,1/2\). If the form with the two additional frequencies is used, the multiplier of the sum will be unity, except for the purely real points at \(\omega_{j}=0,1/2\) for which the multiplier is \(1/2\). For a discussion of applying the Whittle approximation to the problem of estimating parameters in an ARMA spectrum, see Anderson [6]. The Whittle likelihood is especially useful for fitting long memory models that will be discussed in Chap. 5.

### Multiple Series and Cross-Spectra

The notion of analyzing frequency fluctuations using classical statistical ideas extends to the case in which there are several jointly stationary series, for example, \(x_{t}\) and \(y_{t}\). In this case, we can introduce the idea of a correlation indexed by frequency, called the _coherence_. The results in Sect. C.2 imply the covariance function

\[\gamma_{xy}(h)=\mathrm{E}[(x_{t+h}-\mu_{x})(y_{t}-\mu_{y})]\]

Figure 4.14: Autoregressive spectral estimator for the SOI series using the AR(15) model selected by AIC, AICc, and BIC

has the representation

\[\gamma_{xy}(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\,f_{xy}(\omega)\,\mathrm{e}^{2\pi i \omega h}\,\,d\omega\quad h=0,\pm 1,\pm 2,\ldots, \tag{4.86}\]

where the _cross-spectrum_ is defined as the Fourier transform

\[f_{xy}(\omega)=\sum_{h=-\infty}^{\infty}\gamma_{xy}(h)\,\mathrm{e}^{-2\pi i \omega h}\quad-1/2\leq\omega\leq 1/2, \tag{4.87}\]

assuming that the cross-covariance function is absolutely summable, as was the case for the autocovariance. The cross-spectrum is generally a complex-valued function, and it is often written as

\[f_{xy}(\omega)=c_{xy}(\omega)-iq_{xy}(\omega), \tag{4.88}\]

where

\[c_{xy}(\omega)=\sum_{h=-\infty}^{\infty}\gamma_{xy}(h)\,\cos(2\pi\omega h) \tag{4.89}\]

and

\[q_{xy}(\omega)=\sum_{h=-\infty}^{\infty}\gamma_{xy}(h)\,\sin(2\pi\omega h) \tag{4.90}\]

are defined as the _cospectrum_ and _quadspectrum_, respectively. Because of the relationship \(\gamma_{yx}(h)=\gamma_{xy}(-h)\), it follows, by substituting into (4.87) and rearranging, that

\[f_{yx}(\omega)=f_{xy}^{*}(\omega), \tag{4.91}\]

with \(*\) denoting conjugation. This result, in turn, implies that the cospectrum and quadspectrum satisfy

\[c_{yx}(\omega)=c_{xy}(\omega) \tag{4.92}\]

and

\[q_{yx}(\omega)=-q_{xy}(\omega). \tag{4.93}\]

An important example of the application of the cross-spectrum is to the problem of predicting an output series \(y_{t}\) from some input series \(x_{t}\) through a linear filter relation such as the three-point moving average considered below. A measure of the strength of such a relation is the _squared coherence_ function, defined as

\[\rho_{y\cdot x}^{2}(\omega)=\frac{|f_{yx}(\omega)|^{2}}{f_{xx}(\omega)f_{yy}( \omega)}, \tag{4.94}\]

where \(f_{xx}(\omega)\) and \(f_{yy}(\omega)\) are the individual spectra of the \(x_{t}\) and \(y_{t}\) series, respectively. Although we consider a more general form of this that applies to multiple inputs later, it is instructive to display the single input case as (4.94) to emphasize the analogy with conventional squared correlation, which takes the form \[\rho_{yx}^{2}=\frac{\sigma_{yx}^{2}}{\sigma_{x}^{2}\sigma_{y}^{2}},\]

for random variables with variances \(\sigma_{x}^{2}\) and \(\sigma_{y}^{2}\) and covariance \(\sigma_{yx}=\sigma_{xy}\). This motivates the interpretation of squared coherence and the squared correlation between two time series at frequency \(\omega\).

**Example 4.19**: **Three-Point Moving Average**

As a simple example, we compute the cross-spectrum between \(x_{t}\) and the three-point moving average \(y_{t}=(x_{t-1}+x_{t}+x_{t+1})/3\), where \(x_{t}\) is a stationary input process with spectral density \(f_{xx}(\omega)\). First,

\[\gamma_{xy}(h)=\operatorname{cov}(x_{t+h},y_{t}) =\tfrac{1}{3}\operatorname{cov}(x_{t+h},\;x_{t-1}+x_{t}+x_{t+1})\] \[=\tfrac{1}{3}\left[\gamma_{xx}(h+1)+\gamma_{xx}(h)+\gamma_{xx}(h- 1)\right]\] \[=\tfrac{1}{3}\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\left(\mathrm{e}^ {2\pi i\omega}+1+\mathrm{e}^{-2\pi i\omega}\right)\mathrm{e}^{2\pi i\omega h} f_{xx}(\omega)\,d\omega\] \[=\tfrac{1}{3}\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\left[1+2 \cos(2\pi\omega)\right]f_{xx}(\omega)\,\mathrm{e}^{2\pi i\omega h}\,d\omega,\]

where we have use (4.16). Using the uniqueness of the Fourier transform, we argue from the spectral representation (4.86) that

\[f_{xy}(\omega)=\tfrac{1}{3}\left[1+2\cos(2\pi\omega)\right]\,f_{xx}(\omega)\]

so that the cross-spectrum is real in this case. Using Property 4.3, the spectral density of \(y_{t}\) is

\[f_{yy}(\omega)=\tfrac{1}{9}\left|\mathrm{e}^{2\pi i\omega}+1+\mathrm{e}^{-2\pi i \omega}\right|^{2}f_{xx}(\omega)=\tfrac{1}{9}\left[1+2\cos(2\pi\omega)\right]^ {2}f_{xx}(\omega)\,.\]

Substituting into (4.94) yields,

\[\rho_{y\cdot x}^{2}(\omega)=\frac{\left|\tfrac{1}{3}\left[1+2\cos(2\pi\omega) \right]f_{xx}(\omega)\right|^{2}}{f_{xx}(\omega)\cdot\tfrac{1}{9}\left[1+2\cos (2\pi\omega)\right]^{2}f_{xx}(\omega)}=1\,;\]

that is, the squared coherence between \(x_{t}\) and \(y_{t}\) is unity over all frequencies. This is a characteristic inherited by more general linear filters; see Problem 4.30. However, if some noise is added to the three-point moving average, the coherence is not unity; these kinds of models will be considered in detail later.

**Property 4.8**: **Spectral Representation of a Vector Stationary Process**

_If \(x_{t}=(x_{t1},x_{t2},\ldots,x_{tp})^{\prime}\) is a \(p\times 1\) stationary process with autocovariance matrix \(\Gamma(h)=\mathrm{E}[(x_{t+h}-\mu)(x_{t}-\mu)^{\prime}]=\{\gamma_{jk}(h)\}\) satisfying_\[\sum_{h=-\infty}^{\infty}|\gamma_{jk}(h)|<\infty \tag{4.95}\]

_for all \(j,k=1,\ldots,p\), then \(\Gamma(h)\) has the representation_

\[\Gamma(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\ f( \omega)\ d\omega\quad h=0,\pm 1,\pm 2,\ldots, \tag{4.96}\]

_as the inverse transform of the spectral density matrix, \(f(\omega)=\{f_{jk}(\omega)\}\), for \(j,k=1,\ldots,p\). The matrix \(f(\omega)\) has the representation_

\[f(\omega)=\sum_{h=-\infty}^{\infty}\Gamma(h)\mathrm{e}^{-2\pi i\omega h}\quad- 1/2\leq\omega\leq 1/2. \tag{4.97}\]

_The spectral matrix \(f(\omega)\) is Hermitian, \(f(\omega)=f^{*}(\omega)\), where \(*\) means to conjugate and transpose._

**Example 4.20**: **Spectral Matrix of a Bivariate Process**

Consider a jointly stationary bivariate process \((x_{t},y_{t})\). We arrange the autocovariances in the matrix

\[\Gamma(h)=\begin{pmatrix}\gamma_{xx}(h)\ \gamma_{xy}(h)\\ \gamma_{y\cdot x}(h)\ \gamma_{yy}(h)\end{pmatrix}.\]

The spectral matrix would be given by

\[f(\omega)=\begin{pmatrix}f_{xx}(\omega)\ f_{xy}(\omega)\\ f_{yx}(\omega)\ f_{yy}(\omega)\end{pmatrix},\]

where the Fourier transform (4.96) and (4.97) relate the autocovariance and spectral matrices.

The extension of spectral estimation to vector series is fairly obvious. For the vector series \(x_{t}=(x_{t1},x_{t2},\ldots,x_{tp})^{\prime}\), we may use the vector of DFTs, say \(d(\omega_{j})=(d_{1}(\omega_{j}),d_{2}(\omega_{j}),\ldots,d_{p}(\omega_{j}))^{\prime}\), and estimate the spectral matrix by

\[\tilde{f}(\omega)=L^{-1}\sum_{k=-m}^{m}I(\omega_{j}+k/n) \tag{4.98}\]

where now

\[I(\omega_{j})=d(\omega_{j})\,d^{*}(\omega_{j}) \tag{4.99}\]

is a \(p\times p\) complex matrix. The series may be tapered before the DFT is taken in (4.98) and we can use weighted estimation,

\[\hat{f}(\omega)=\sum_{k=-m}^{m}h_{k}\ I(\omega_{j}+k/n) \tag{4.100}\]where \(\{h_{k}\}\) are weights as defined in (4.64). The estimate of squared coherence between two series, \(y_{t}\) and \(x_{t}\) is

\[\hat{\rho}^{2}_{y\cdot x}(\omega)=\frac{|\hat{f}_{y\cdot x}(\omega)|^{2}}{\hat{f }_{xx}(\omega)\hat{f}_{yy}(\omega)}. \tag{4.101}\]

If the spectral estimates in (4.101) are obtained using equal weights, we will write \(\bar{\rho}^{2}_{y\cdot x}(\omega)\) for the estimate.

Under general conditions, if \(\rho^{2}_{y\cdot x}(\omega)>0\) then

\[|\hat{\rho}_{y\cdot x}(\omega)|\sim AN\left(|\rho_{y\cdot x}(\omega)|,\left(1- \rho^{2}_{y\cdot x}(\omega)\right)^{2}\big{/}2L_{h}\right) \tag{4.102}\]

where \(L_{h}\) is defined in (4.65); the details of this result may be found in Brockwell and Davis [36, Ch 11]. We may use (4.102) to obtain approximate confidence intervals for the squared coherence, \(\rho^{2}_{y\cdot x}(\omega)\).

We may also test the null hypothesis that \(\rho^{2}_{y\cdot x}(\omega)=0\) if we use \(\bar{\rho}^{2}_{y\cdot x}(\omega)\) for the estimate with \(L>1\),11 that is,

Footnote 11: If \(L=1\) then \(\bar{\rho}^{2}_{y\cdot x}(\omega)\equiv 1\).

\[\bar{\rho}^{2}_{y\cdot x}(\omega)=\frac{|\bar{f}_{y\cdot x}(\omega)|^{2}}{\bar {f}_{xx}(\omega)\bar{f}_{yy}(\omega)}. \tag{4.103}\]

In this case, under the null hypothesis, the statistic

\[F=\frac{\bar{\rho}^{2}_{y\cdot x}(\omega)}{(1-\bar{\rho}^{2}_{y\cdot x}(\omega) )}(L-1) \tag{4.104}\]

has an approximate \(F\)-distribution with \(2\) and \(2L-2\) degrees of freedom. When the series have been extended to length \(n^{\prime}\), we replace \(2L-2\) by \(df-2\), where \(df\) is defined in (4.60). Solving (4.104) for a particular significance level \(\alpha\) leads to

\[C_{\alpha}=\frac{F_{2,2L-2}(\alpha)}{L-1+F_{2,2L-2}(\alpha)} \tag{4.105}\]

as the approximate value that must be exceeded for the original squared coherence to be able to reject \(\rho^{2}_{y\cdot x}(\omega)=0\) at an a priori specified frequency.

**Example 4.21**: **Coherence Between SOI and Recruitment**

Figure 4.15 shows the squared coherence between the SOI and Recruitment series over a wider band than was used for the spectrum. In this case, we used \(L=19,\ df=2(19)(453/480)\approx 36\) and \(F_{2,df-2}(.001)\approx 8.53\) at the significance level \(\alpha=.001\). Hence, we may reject the hypothesis of no coherence for values of \(\bar{\rho}^{2}_{y\cdot x}(\omega)\) that exceed \(C_{.001}=.32\). We emphasize that this method is crude because, in addition to the fact that the \(F\)-statistic is approximate, we are examining the squared coherence across all frequencies with the Bonferroni inequality, (4.63), in mind. Figure 4.15

also exhibits confidence bands as part of the R plotting routine. We emphasize that these bands are only valid for \(\omega\) where \(\rho_{y\cdot x}^{2}(\omega)>0\).

In this case, the two series are obviously strongly coherent at the annual seasonal frequency. The series are also strongly coherent at lower frequencies that may be attributed to the El Nino cycle, which we claimed had a 3 to 7 year period. The peak in the coherency, however, occurs closer to the 9 year cycle. Other frequencies are also coherent, although the strong coherence is less impressive because the underlying power spectrum at these higher frequencies is fairly small. Finally, we note that the coherence is persistent at the seasonal harmonic frequencies.

This example may be reproduced using the following R commands.

sr = mvspec(cbind(soi,rec), kernel("daniell",9), plot=FALSE) sr$df # df = 35.8625 f = qf(.999, 2, sr$df-2) # = 8.529792 C = f/(18+f) # = 0.321517 plot(sr, plot.type = "coh", ci.lty = 2) abline(h = C)

### 4.7 Linear Filters

Some of the examples of the previous sections have hinted at the possibility the distribution of power or variance in a time series can be modified by making a linear transformation. In this section, we explore that notion further by showing how linear filters can be used to extract signals from a time series. These filters modify the spectral characteristics of a time series in a predictable way, and the systematic development of methods for taking advantage of the special properties of linear filters is an important topic in time series analysis.

Figure 4.15: Squared coherency between the SOI and Recruitment series; \(L=19,\ n=453,\ n^{\prime}=480\), and \(\alpha=.001\). The _horizontal line_ is \(C_{.001}\)

Recall Property 4.3 that stated if

\[y_{t}=\sum_{j=-\infty}^{\infty}a_{j}x_{t-j},\qquad\sum_{j=-\infty}^{\infty}|a_{j}| <\infty,\]

and \(x_{t}\) has spectrum \(f_{xx}(\omega)\), then \(y_{t}\) has spectrum

\[f_{yy}(\omega)=|A_{y,x}(\omega)|^{2}f_{xx}(\omega),\]

where

\[A_{yx}(\omega)=\sum_{j=-\infty}^{\infty}a_{j}\,\mathrm{e}^{-2\pi i\omega j}\]

is the _frequency response function_. This result shows that the filtering effect can be characterized as a frequency-by-frequency multiplication by the squared magnitude of the frequency response function.

**Example 4.22**: **First Difference and Moving Average Filters**

We illustrate the effect of filtering with two common examples, the first difference filter

\[y_{t}=\nabla x_{t}=x_{t}-x_{t-1}\]

and the annual symmetric moving average filter,

\[y_{t}=\tfrac{1}{24}\left(x_{t-6}+x_{t+6}\right)+\tfrac{1}{12}\sum_{r=-5}^{5}x _{t-r},\]

which is a modified Daniell kernel with \(m=6\). The results of filtering the SOI series using the two filters are shown in the middle and bottom panels of Fig. 4.16. Notice that the effect of differencing is to roughen the series because it tends to retain the higher or faster frequencies. The centered moving average smoothes the series because it retains the lower frequencies and tends to attenuate the higher frequencies. In general, differencing is an example of a _high-pass filter_ because it retains or passes the higher frequencies, whereas the moving average is a _low-pass filter_ because it passes the lower or slower frequencies.

Notice that the slower periods are enhanced in the symmetric moving average and the seasonal or yearly frequencies are attenuated. The filtered series makes about 9 cycles in the length of the data (about one cycle every 52 months) and the moving average filter tends to enhance or _extract_ the El Nino signal. Moreover, by low-pass filtering the data, we get a better sense of the El Nino effect and its irregularity.

Now, having done the filtering, it is essential to determine the exact way in which the filters change the input spectrum. We shall use (4.21) and (4.22) for this purpose. The first difference filter can be written in the form (4.20) by letting \(a_{0}=1,a_{1}=-1\), and \(a_{r}=0\) otherwise. This implies that

\[A_{yx}(\omega)=1-\mathrm{e}^{-2\pi i\omega},\]and the squared frequency response becomes

\[|A_{yx}(\omega)|^{2}=(1-\mathrm{e}^{-2\pi i\omega})(1-\mathrm{e}^{2\pi i\omega})=2[1 -\cos(2\pi\omega)]. \tag{4.106}\]

The top panel of Fig. 4.17 shows that the first difference filter will attenuate the lower frequencies and enhance the higher frequencies because the multiplier of the spectrum, \(|A_{yx}(\omega)|^{2}\), is large for the higher frequencies and small for the lower frequencies. Generally, the slow rise of this kind of filter does not particularly recommend it as a procedure for retaining only the high frequencies.

For the centered 12-month moving average, we can take \(a_{-6}=a_{6}=1/24\), \(a_{k}=1/12\) for \(-5\leq k\leq 5\) and \(a_{k}=0\) elsewhere. Substituting and recognizing the cosine terms gives

\[A_{yx}(\omega)=\tfrac{1}{12}\Big{[}1+\cos(12\pi\omega)+2\sum_{k=1}^{5}\cos(2 \pi\omega k)\Big{]}. \tag{4.107}\]

Plotting the squared frequency response of this function as in the bottom of Fig. 4.17 shows that we can expect this filter to cut most of the frequency content above.05 cycles per point, and nearly all of the frequency content above \(1/12\approx.083\). In particular, this drives down the yearly components with periods of 12 months and enhances the El Nino frequency, which is somewhat lower. The filter is not completely efficient at attenuating high frequencies; some power contributions are left at higher frequencies, as shown in the function \(|A_{yx}(\omega)|^{2}\).

Figure 4.16: SOI series (_top_) compared with the differenced SOI (_middle_) and a centered 12-month moving average (_bottom_)

The following R session shows how to filter the data, perform the spectral analysis of a filtered series, and plot the squared frequency response curves of the difference and moving average filters.

par(mfrown=c(3,1), mar=c(3,3,1,1), mgp=c(1.6,.6,0)) plot(soi) # plot data plot(diff(soi)) # plot first difference k = kernel("modified.daniell", 6) # filter weights plot(soif <- kernapply(soi, k)) # plot 12 month filter dev.new() spectrum(soif, spans=9, log="no") # spectral analysis (not shown) abline(v=12/52, lty="dashed") dev.new()

##- frequency responses --## par(mfrown=c(2,1), mar=c(3,3,1,1), mgp=c(1.6,.6,0)) w = seq(0,.5, by=.01) FRdiff = abs(1-exp(2i"pi"w))^2 plot(w, FRdiff, type='1', xlab='frequency') u = cos(2*pi"w)+cos(4*pi"w)+cos(6*pi"w)+cos(8*pi"w)+cos(10*pi"w) FRma = ((1 + cos(12*pi"w) + 2*u)/12)^2 plot(w, FRma, type='1', xlab='frequency')

The two filters discussed in the previous example were different in that the frequency response function of the first difference was complex-valued, whereas the frequency response of the moving average was purely real. A short derivation similar to that used to verify (4.22) shows, when \(x_{t}\) and \(y_{t}\) are related by the linear filter relation (4.20), the cross-spectrum satisfies

\[f_{yx}(\omega)=A_{yx}(\omega)f_{xx}(\omega),\]

Figure 4.17: Squared frequency response functions of the first difference (_top_) and twelve-month moving average (_bottom_) filters

so the frequency response is of the form

\[A_{yx}(\omega) =\frac{f_{y.x}(\omega)}{f_{xx}(\omega)} \tag{4.108}\] \[=\frac{c_{yx}(\omega)}{f_{xx}(\omega)}-i\frac{q_{y.x}(\omega)}{f_{ xx}(\omega)}, \tag{4.109}\]

where we have used (4.88) to get the last form. Then, we may write (4.109) in polar coordinates as

\[A_{yx}(\omega)=\left|A_{yx}(\omega)\right|\ \exp\{-i\ \phi_{y.x}(\omega)\}, \tag{4.110}\]

where the _amplitude_ and _phase_ of the filter are defined by

\[\left|A_{yx}(\omega)\right|=\frac{\sqrt{c_{yx}^{2}(\omega)+q_{y.x}^{2}(\omega)} }{f_{xx}(\omega)} \tag{4.111}\]

and

\[\phi_{yx}(\omega)=\tan^{-1}\left(-\frac{q_{yx}(\omega)}{c_{yx}(\omega)}\right). \tag{4.112}\]

A simple interpretation of the phase of a linear filter is that it exhibits time delays as a function of frequency in the same way as the spectrum represents the variance as a function of frequency. Additional insight can be gained by considering the simple delaying filter

\[y_{t}=Ax_{t-D},\]

where the series gets replaced by a version, amplified by multiplying by \(A\) and delayed by \(D\) points. For this case,

\[f_{yx}(\omega)=A\mathrm{e}^{-2\pi i\omega D}f_{xx}(\omega),\]

and the amplitude is \(\left|A\right|\), and the phase is

\[\phi_{yx}(\omega)=-2\pi\omega D,\]

or just a linear function of frequency \(\omega\). For this case, applying a simple time delay causes phase delays that depend on the frequency of the periodic component being delayed. Interpretation is further enhanced by setting

\[x_{t}=\cos(2\pi\omega t),\]

in which case

\[y_{t}=A\cos(2\pi\omega t-2\pi\omega D).\]

Thus, the output series, \(y_{t}\), has the same period as the input series, \(x_{t}\), but the amplitude of the output has increased by a factor of \(\left|A\right|\) and the phase has been changed by a factor of \(-2\pi\omega D\).

**Example 4.23**: **Difference and Moving Average Filters**

We consider calculating the amplitude and phase of the two filters discussed in Example 4.22. The case for the moving average is easy because \(A_{yx}(\omega)\) given in (4.107) is purely real. So, the amplitude is just \(|A_{y,x}(\omega)|\) and the phase is \(\phi_{yx}(\omega)=0\). In general, symmetric (\(a_{j}=a_{-j}\)) filters have zero phase. The first difference, however, changes this, as we might expect from the example above involving the time delay filter. In this case, the squared amplitude is given in (4.106). To compute the phase, we write

\[A_{yx}(\omega) = 1-\mathrm{e}^{-2\pi i\omega}=\mathrm{e}^{-i\pi\omega}(\mathrm{e}^ {i\pi\omega}-\mathrm{e}^{-i\pi\omega})\] \[= 2i\mathrm{e}^{-i\pi\omega}\sin(\pi\omega)=2\sin^{2}(\pi\omega)+2i \cos(\pi\omega)\sin(\pi\omega)\] \[= \frac{c_{y,x}(\omega)}{f_{xx}(\omega)}-i\frac{q_{yx}(\omega)}{f_{ xx}(\omega)},\]

so

\[\phi_{y,x}(\omega)=\tan^{-1}\left(-\frac{q_{y,x}(\omega)}{c_{y,x}(\omega)} \right)=\tan^{-1}\left(\frac{\cos(\pi\omega)}{\sin(\pi\omega)}\right).\]

Noting that

\[\cos(\pi\omega)=\sin(-\pi\omega+\pi/2)\]

and that

\[\sin(\pi\omega)=\cos(-\pi\omega+\pi/2),\]

we get

\[\phi_{yx}(\omega)=-\pi\omega+\pi/2,\]

and the phase is again a linear function of frequency.

The above tendency of the frequencies to arrive at different times in the filtered version of the series remains as one of two annoying features of the difference type filters. The other weakness is the gentle increase in the frequency response function. If low frequencies are really unimportant and high frequencies are to be preserved, we would like to have a somewhat sharper response than is obvious in Fig. 4.17. Similarly, if low frequencies are important and high frequencies are not, the moving average filters are also not very efficient at passing the low frequencies and attenuating the high frequencies. Improvement is possible by designing better and longer filters, but we do not discuss this here.

We will occasionally use results for multivariate series \(x_{t}=(x_{t1},\ldots,x_{tp})^{\prime}\) that are comparable to the simple property shown in (4.22). Consider the _matrix filter_

\[y_{t}=\sum_{j=-\infty}^{\infty}A_{j}x_{t-j}, \tag{4.113}\]

where \(\{A_{j}\}\) denotes a sequence of \(q\times p\) matrices such that \(\sum_{j=-\infty}^{\infty}\|A_{j}\|<\infty\) and \(\|\cdot\|\) denotes any matrix norm, \(x_{t}=(x_{t1},\ldots,x_{tp})^{\prime}\) is a \(p\times 1\) stationary vector process with mean vector \(\mu_{x}\) and \(p\times p\), matrix covariance function \(\Gamma_{xx}(h)\) and spectral matrix \(f_{xx}(\omega)\), and \(y_{t}\) is the \(q\times 1\) vector output process. Then, we can obtain the following property.

**Property 4.9**: **Output Spectral Matrix of Filtered Vector Series**__

_The spectral matrix of the filtered output \(y_{t}\) in (4.113) is related to the spectrum of the input \(x_{t}\) by_

\[f_{yy}(\omega)=\mathcal{A}(\omega)f_{xx}(\omega)\mathcal{A}^{*}(\omega), \tag{4.114}\]

_where the matrix frequency response function \(\mathcal{A}(\omega)\) is defined by_

\[\mathcal{A}(\omega)=\sum_{j=-\infty}^{\infty}A_{j}\exp(-2\pi i\omega j). \tag{4.115}\]

### 4.8 Lagged Regression Models

One of the intriguing possibilities offered by the coherence analysis of the relation between the SOI and Recruitment series discussed in Example 4.21 would be extending classical regression to the analysis of lagged regression models of the form

\[y_{t}=\sum_{r=-\infty}^{\infty}\beta_{r}x_{t-r}+v_{t}, \tag{4.116}\]

where \(v_{t}\) is a stationary noise process, \(x_{t}\) is the observed input series, and \(y_{t}\) is the observed output series. We are interested in estimating the filter coefficients \(\beta_{r}\) relating the adjacent lagged values of \(x_{t}\) to the output series \(y_{t}\).

In the case of SOI and Recruitment series, we might identify the El Nino driving series, SOI, as the input, \(x_{t}\), and \(y_{t}\), the Recruitment series, as the output. In general, there will be more than a single possible input series and we may envision a \(q\times 1\) vector of driving series. This multivariate input situation is covered in Chap. 7. The model given by (4.116) is useful under several different scenarios, corresponding to different assumptions that can be made about the components.

We assume that the inputs and outputs have zero means and are jointly stationary with the \(2\times 1\) vector process \((x_{t},y_{t})^{\prime}\) having a spectral matrix of the form

\[f(\omega)=\left(\begin{array}{cc}f_{xx}(\omega)&f_{xy}(\omega)\\ f_{yx}(\omega)&f_{yy}(\omega)\end{array}\right). \tag{4.117}\]

Here, \(f_{xy}(\omega)\) is the cross-spectrum relating the input \(x_{t}\) to the output \(y_{t}\), and \(f_{xx}(\omega)\) and \(f_{yy}(\omega)\) are the spectra of the input and output series, respectively. Generally, we observe two series, regarded as input and output and search for regression functions \(\{\beta_{t}\}\) relating the inputs to the outputs. We assume all autocovariance functions satisfy the absolute summability conditions of the form (4.38).

Then, minimizing the mean squared error

\[MSE=\mathrm{E}\left(y_{t}-\sum_{r=-\infty}^{\infty}\beta_{r}x_{t-r}\right)^{2} \tag{4.118}\]leads to the usual orthogonality conditions

\[\mathrm{E}\left[\left(y_{t}-\sum_{r=-\infty}^{\infty}\beta_{r}.x_{t-r}\right)x_{t- s}\right]=0 \tag{4.119}\]

for all \(s=0,\pm 1,\pm 2,\ldots\). Taking the expectations inside leads to the normal equations

\[\sum_{r=-\infty}^{\infty}\beta_{r}\ \gamma_{xx}(s-r)=\gamma_{yx}(s) \tag{4.120}\]

for \(s=0,\pm 1,\pm 2,\ldots\). These equations might be solved, with some effort, if the covariance functions were known exactly. If data \((x_{t},y_{t})\) for \(t=1,\ldots,n\) are available, we might use a finite approximation to the above equations with \(\hat{\gamma}_{xx}(h)\) and \(\hat{\gamma}_{yx}(h)\) substituted into (4.120). If the regression vectors are essentially zero for \(|s|\geq M/2\), and \(M<n\), the system (4.120) would be of full rank and the solution would involve inverting an \((M-1)\times(M-1)\) matrix.

A frequency domain approximate solution is easier in this case for two reasons. First, the computations depend on spectra and cross-spectra that can be estimated from sample data using the techniques of Sect. 4.5. In addition, no matrices will have to be inverted, although the frequency domain ratio will have to be computed for each frequency. In order to develop the frequency domain solution, substitute the representation (4.96) into the normal equations, using the convention defined in (4.117). The left side of (4.120) can then be written in the form

\[\int_{-\frac{1}{2}}^{\frac{1}{2}}\sum_{r=-\infty}^{\infty}\beta_{r}\ \mathrm{e}^{2\pi i\omega(s-r)}\ f_{xx}(\omega)\ d\omega=\int_{-\frac{1}{2}}^{ \frac{1}{2}}\mathrm{e}^{2\pi i\omega s}B(\omega)f_{xx}(\omega)\ d\omega,\]

where

\[B(\omega)=\sum_{r=-\infty}^{\infty}\beta_{r}\ \mathrm{e}^{-2\pi i\omega r} \tag{4.121}\]

is the Fourier transform of the regression coefficients \(\beta_{t}\). Now, because \(\gamma_{yx}(s)\) is the inverse transform of the cross-spectrum \(f_{yx}(\omega)\), we might write the system of equations in the frequency domain, using the uniqueness of the Fourier transform, as

\[B(\omega)f_{xx}(\omega)=f_{yx}(\omega), \tag{4.122}\]

which then become the analogs of the usual normal equations. Then, we may take

\[\hat{B}(\omega_{k})=\frac{\hat{f}_{yx}(\omega_{k})}{\hat{f}_{xx}(\omega_{k})} \tag{4.123}\]

as the estimator for the Fourier transform of the regression coefficients, evaluated at some subset of fundamental frequencies \(\omega_{k}=k/M\) with \(M<\!\!<n\). Generally, we assume smoothness of \(B(\cdot)\) over intervals of the form \(\{\omega_{k}+\ell/n;\ \ell=-m,\ldots,0,\ldots,m\}\), with \(L=2m+1\). The inverse transform of the function \(\hat{B}(\omega)\) would give \(\hat{\beta}_{t}\), and we note that the discrete time approximation can be taken as \[\hat{\beta}_{t}=M^{-1}\sum_{k=0}^{M-1}\hat{B}(\omega_{k})\mathrm{e}^{2\pi i\omega_{k }t} \tag{4.124}\]

for \(t=0,\pm 1,\pm 2,\ldots,\pm(M/2-1)\). If we were to use (4.124) to define \(\hat{\beta}_{t}\) for \(|t|\geq M/2\), we would end up with a sequence of coefficients that is periodic with a period of \(M\). In practice we define \(\hat{\beta}_{t}=0\) for \(|t|\geq M/2\) instead. Problem 4.32 explores the error resulting from this approximation.

**Example 4.24**: **Lagged Regression for SOI and Recruitment**

The high coherence between the SOI and Recruitment series noted in Example 4.21 suggests a lagged regression relation between the two series. A natural direction for the implication in this situation is implied because we feel that the sea surface temperature or SOI should be the input and the Recruitment series should be the output. With this in mind, let \(x_{t}\) be the SOI series and \(y_{t}\) the Recruitment series.

Although we think naturally of the SOI as the input and the Recruitment as the output, two input-output configurations are of interest. With SOI as the input, the model is

\[y_{t}=\sum_{r=-\infty}^{\infty}a_{r}x_{t-r}+w_{t}\]

whereas a model that reverses the two roles would be

\[x_{t}=\sum_{r=-\infty}^{\infty}b_{r}y_{t-r}+v_{t},\]

where \(w_{t}\) and \(v_{t}\) are white noise processes. Even though there is no plausible environmental explanation for the second of these two models, displaying both possibilities helps to settle on a parsimonious transfer function model.

Figure 4.18: Estimated impulse response functions relating SOI to Recruitment (_top_) and Recruitment to SOI (_bottom_) \(L=15,M=32\)

Based on the script LagReg in astsa, the estimated regression or impulse response function for SOI, with \(M=32\) and \(L=15\) is

 LagReg(soi, rec, L=15, M=32, threshold=6)  lags beta(s)  [1,] 5 -18.479306  [2,] 6 -12.263296  [3,] 7 -8.539368  [4,] 8 -6.984553 The prediction equation is  rec(t) = alpha + sum_s[ beta(s)*soi(t-s) ], where alpha = 65.97  MSE = 414.08 Note the negative peak at a lag of five points in the top of Fig. 4.18; in this case, SOI is the input series. The fall-off after lag five seems to be approximately exponential and a possible model is

\[y_{t}=66-18.5x_{t-5}-12.3x_{t-6}-8.5x_{t-7}-7x_{t-8}+w_{t}.\]

If we examine the inverse relation, namely, a regression model with the Recruitment series \(y_{t}\) as the input, the bottom of Fig. 4.18 implies a much simpler model,

 LagReg(rec, soi, L=15, M=32, inverse=TRUE, threshold=.01)  lags beta(s)  [1,] 4 0.01593167  [2,] 5 -0.02120013 The prediction equation is  soi(t) = alpha + sum_s[ beta(s)*rec(t+s) ], where alpha = 0.41  MSE = 0.07  depending on only two coefficients, namely,

\[x_{t}=.41+.016y_{t+4}-.02y_{t+5}+v_{t}.\]

Multiplying both sides by \(50B^{5}\) and rearranging, we have

\[(1-.8B)y_{t}=20.5-50B^{5}x_{t}+\epsilon_{t}\.\]

Finally, we check whether the noise, \(\epsilon_{t}\), is white. In addition, at this point, it simplifies matters if we rerun the regression with autocorrelated errors and reestimate the coefficients. The model is referred to as an ARMAX model (the X stands for exogenous; see Sects. 5.6 and Section 6.6.1):

 fish = ts.intersect(R=rec, RL1=lag(rec,-1), SL5=lag(soi,-5))  (u = lm(fish[,1]-fish[,2:3], na.action=NULL))  acf2(resid(u)) # suggests ar1  sarima(fish[,1], 1, 0, 0, xreg=fish[,2:3]) # armax model  Coefficients:  ar1 intercept RL1 SL5  0.4487 12.3323 0.8005 -21.0307  s.e. 0.0503 1.5746 0.0234 1.0915  sigma^2 estimated as 49.93 Our final parsimonious fitted model is (with rounding)

\[y_{t}=12+.8y_{t-1}-21x_{t-5}+\epsilon_{t}\,\quad\text{and}\quad\epsilon_{t}=.45 \epsilon_{t-1}+w_{t}\,,\]

where \(w_{t}\) is white noise with \(\sigma_{w}^{2}=50\). This example is also examined in Chap. 5 and the fitted values for the final model can be viewed Fig. 5.12.

The example shows we can get a clean estimator for the transfer functions relating the two series if the coherence \(\hat{\rho}^{2}_{xy}(\omega)\) is large. The reason is that we can write the minimized mean squared error (4.118) as

\[MSE=\mathrm{E}\bigg{[}\big{(}y_{t}-\sum_{r=-\infty}^{\infty}\beta_{r}x_{t-r} \big{)}y_{t}\bigg{]}=\gamma_{yy}(0)-\sum_{r=-\infty}^{\infty}\beta_{r}\gamma_{ xy}(-r),\]

using the result about the orthogonality of the data and error term in the Projection theorem. Then, substituting the spectral representations of the autocovariance and cross-covariance functions and identifying the Fourier transform (4.121) in the result leads to

\[MSE = \int_{-\frac{1}{2}}^{\frac{1}{2}}[f_{yy}(\omega)-B(\omega)f_{xy}( \omega)]\;d\omega \tag{4.125}\] \[= \int_{-\frac{1}{2}}^{\frac{1}{2}}f_{yy}(\omega)[1-\rho^{2}_{yx}( \omega)]d\omega,\]

where \(\rho^{2}_{yx}(\omega)\) is just the squared coherence given by (4.94). The similarity of (4.125) to the usual mean square error that results from predicting \(y\) from \(x\) is obvious. In that case, we would have

\[\mathrm{E}(y-\beta x)^{2}=\sigma^{2}_{y}(1-\rho^{2}_{xy})\]

for jointly distributed random variables \(x\) and \(y\) with zero means, variances \(\sigma^{2}_{x}\) and \(\sigma^{2}_{y}\), and covariance \(\sigma_{xy}=\rho_{xy}\sigma_{x}\sigma_{y}\). Because the mean squared error in (4.125) satisfies \(MSE\geq 0\) with \(f_{yy}(\omega)\) a non-negative function, it follows that the coherence satisfies

\[0\leq\rho^{2}_{xy}(\omega)\leq 1\]

for all \(\omega\). Furthermore, Problem 4.33 shows the squared coherence is one when the output are linearly related by the filter relation (4.116), and there is no noise, i.e., \(v_{t}=0\). Hence, the multiple coherence gives a measure of the association or correlation between the input and output series as a function of frequency.

The matter of verifying that the \(F\)-distribution claimed for (4.104) will hold when the sample coherence values are substituted for theoretical values still remains. Again, the form of the \(F\)-statistic is exactly analogous to the usual \(t\)-test for no correlation in a regression context. We give an argument leading to this conclusion later using the results in Sect. C.3. Another question that has not been resolved in this section is the extension to the case of multiple inputs \(x_{t1},x_{t2},\ldots,x_{tq}\). Often, more than just a single input series is present that can possibly form a lagged predictor of the output series \(y_{t}\). An example is the cardiovascular mortality series that depended on possibly a number of pollution series and temperature. We discuss this particular extension as a part of the multivariate time series techniques considered in Chap. 7.

### Signal Extraction and Optimum Filtering

A model closely related to regression can be developed by assuming again that

\[y_{t}=\sum_{r=-\infty}^{\infty}\beta_{r}x_{t-r}+v_{t}, \tag{4.126}\]

but where the \(\beta\)s are known and \(x_{t}\) is some unknown random _signal_ that is uncorrelated with the _noise_ process \(v_{t}\). In this case, we observe only \(y_{t}\) and are interested in an estimator for the signal \(x_{t}\) of the form

\[\hat{x}_{t}=\sum_{r=-\infty}^{\infty}a_{r}y_{t-r}. \tag{4.127}\]

In the frequency domain, it is convenient to make the additional assumptions that the series \(x_{t}\) and \(v_{t}\) are both mean-zero stationary series with spectra \(f_{xx}(\omega)\) and \(f_{vv}(\omega)\), often referred to as the _signal spectrum_ and _noise spectrum_, respectively. Often, the special case \(\beta_{t}=\delta_{t}\), in which \(\delta_{t}\) is the Kronecker delta, is of interest because (4.126) reduces to the simple _signal plus noise_ model

\[y_{t}=x_{t}+v_{t} \tag{4.128}\]

in that case. In general, we seek the set of filter coefficients \(a_{t}\) that minimize the mean squared error of estimation, say,

\[MSE=\mathrm{E}\left[\left(x_{t}-\sum_{r=-\infty}^{\infty}a_{r}y_{t-r}\right)^{ 2}\right]. \tag{4.129}\]

This problem was originally solved by Kolmogorov [120] and by Wiener [211], who derived the result in 1941 and published it in classified reports during World War II.

We can apply the orthogonality principle to write

\[\mathrm{E}\left[\left(x_{t}-\sum_{r=-\infty}^{\infty}a_{r}y_{t-r}\right)y_{t- s}\right]=0\]

for \(s=0,\pm 1,\pm 2,\ldots\), which leads to

\[\sum_{r=-\infty}^{\infty}a_{r}\gamma_{yy}(s-r)=\gamma_{xy}(s),\]

to be solved for the filter coefficients. Substituting the spectral representations for the autocovariance functions into the above and identifying the spectral densities through the uniqueness of the Fourier transform produces

\[A(\omega)f_{yy}(\omega)=f_{xy}(\omega), \tag{4.130}\]where \(A(\omega)\) and the optimal filter \(a_{t}\) are Fourier transform pairs for \(B(\omega)\) and \(\beta_{t}\). Now, a special consequence of the model is that (see Problem 4.30)

\[f_{xy}(\omega)=B^{*}(\omega)f_{xx}(\omega) \tag{4.131}\]

and

\[f_{yy}(\omega)=|B(\omega)|^{2}f_{xx}(\omega)+f_{vv}(\omega), \tag{4.132}\]

implying the optimal filter would be Fourier transform of

\[A(\omega)=\frac{B^{*}(\omega)}{\left(|B(\omega)|^{2}+\frac{f_{vv}(\omega)}{f_{ xx}(\omega)}\right)}, \tag{4.133}\]

where the second term in the denominator is just the inverse of the _signal to noise ratio_, say,

\[\text{SNR}(\omega)=\frac{f_{xx}(\omega)}{f_{vv}(\omega)}. \tag{4.134}\]

The result shows the optimum filters can be computed for this model if the signal and noise spectra are both known or if we can assume knowledge of the signal-to-noise ratio SNR(\(\omega\)) as function of frequency. In Chap. 7, we show some methods for estimating these two parameters in conjunction with random effects analysis of variance models, but we assume here that it is possible to specify the signal-to-noise ratio a priori. If the signal-to-noise ratio is known, the optimal filter can be computed by the inverse transform of the function \(A(\omega)\). It is more likely that the inverse transform will be intractable and a finite filter approximation like that used in the previous section can be applied to the data. In this case, we will have

\[a_{t}^{M}=M^{-1}\sum_{k=0}^{M-1}A(\omega_{k})\text{e}^{2\pi i\omega_{k}t} \tag{4.135}\]

as the estimated filter function. It will often be the case that the form of the specified frequency response will have some rather sharp transitions between regions where the signal-to-noise ratio is high and regions where there is little signal. In these cases, the shape of the frequency response function will have ripples that can introduce frequencies at different amplitudes. An aesthetic solution to this problem is to introduce tapering as was done with spectral estimation in (4.69)-(4.76). We use below the tapered filter \(\tilde{a}_{t}=h_{t}a_{t}\) where \(h_{t}\) is the cosine taper given in (4.76). The squared frequency response of the resulting filter will be \(|\tilde{A}(\omega)|^{2}\), where

\[\tilde{A}(\omega)=\sum_{t=-\infty}^{\infty}a_{t}\,h_{t}\text{e}^{-2\pi i\omega t}. \tag{4.136}\]

The results are illustrated in the following example that extracts the El Nino component of the sea surface temperature series.

**Example 4.25**: **Estimating the El Nino Signal via Optimal Filters**

Figure 4.7 shows the spectrum of the SOI series, and we note that essentially two components have power, the El Nino frequency of about.02 cycles per month (the four-year cycle) and a yearly frequency of about.08 cycles per month (the annual cycle). We assume, for this example, that we wish to preserve the lower frequency as signal and to eliminate the higher order frequencies, and in particular, the annual cycle. In this case, we assume the simple signal plus noise model

\[y_{t}=x_{t}+v_{t},\]

so that there is no convolving function \(\beta_{t}\). Furthermore, the signal-to-noise ratio is assumed to be high to about.06 cycles per month and zero thereafter. The optimal frequency response was assumed to be unity to.05 cycles per point and then to decay linearly to zero in several steps. Figure 4.19 shows the coefficients as specified by (4.135) with \(M=64\), as well as the frequency response function given by (4.136), of the cosine tapered coefficients; recall Fig. 4.11, where we demonstrated the need for tapering to avoid severe ripples in the window. The constructed response function is compared to the ideal window in Fig. 4.19.

Figure 4.20 shows the original and filtered SOI index, and we see a smooth extracted signal that conveys the essence of the underlying El Nino signal. The frequency response of the designed filter can be compared with that of the symmetric 12-month moving average applied to the same series in Example 4.22. The filtered series, shown in Fig. 4.16, shows a good deal of higher frequency chatter riding on the smoothed version, which has been introduced by the higher frequencies that leak through in the squared frequency response, as in Fig. 4.17.

The analysis can be replicated using the script SigExtract.

**SigExtract(soi, L=9, M=64, max.freq=.05)**

Figure 4.19: Filter coefficients (_top_) and frequency response functions (_bottom_) for designed SOI filtersThe design of finite filters with a specified frequency response requires some experimentation with various target frequency response functions and we have only touched on the methodology here. The filter designed here, sometimes called a low-pass filter reduces the high frequencies and keeps or passes the low frequencies. Alternately, we could design a high-pass filter to keep high frequencies if that is where the signal is located. An example of a simple high-pass filter is the first difference with a frequency response that is shown in Fig. 4.17. We can also design band-pass filters that keep frequencies in specified bands. For example, seasonal adjustment filters are often used in economics to reject seasonal frequencies while keeping both high frequencies, lower frequencies, and trend (see, for example, Grether and Nerlove [83]).

The filters we have discussed here are all symmetric two-sided filters, because the designed frequency response functions were purely real. Alternatively, we may design recursive filters to produce a desired response. An example of a recursive filter is one that replaces the input \(x_{t}\) by the filtered output

\[y_{t}=\sum_{k=1}^{p}\phi_{k}y_{t-k}+x_{t}-\sum_{k=1}^{q}\theta_{k}x_{t-k}. \tag{4.137}\]

Note the similarity between (4.137) and the ARMA(\(p,q\)) model, in which the white noise component is replaced by the input. Transposing the terms involving \(y_{t}\) and using the basic linear filter result in Property 4.3 leads to

\[f_{y}(\omega)=\frac{|\theta(\mathrm{e}^{-2\pi i\omega})|^{2}}{|\phi(\mathrm{e }^{-2\pi i\omega})|^{2}}f_{x}(\omega), \tag{4.138}\]

Figure 4.20: Original SOI series (_top_) compared to filtered version showing the estimated El Niño temperature signal (_bottom_)

where

\[\phi(\mathrm{e}^{-2\pi i\omega})=1-\sum_{k=1}^{p}\phi_{k}\mathrm{e}^{-2\pi ik\omega}\]

and

\[\theta(\mathrm{e}^{-2\pi i\omega})=1-\sum_{k=1}^{q}\theta_{k}\mathrm{e}^{-2\pi ik \omega}.\]

Recursive filters such as those given by (4.138) distort the phases of arriving frequencies, and we do not consider the problem of designing such filters in any detail.

### Spectral Analysis of Multidimensional Series

Multidimensional series of the form \(x_{s}\), where \(s=(s_{1},s_{2},\ldots,s_{r})^{\prime}\) is an \(r\)-dimensional vector of spatial coordinates or a combination of space and time coordinates, were introduced in Sect. 1.6. The example given there, shown in Fig. 1.18, was a collection of temperature measurements taking on a rectangular field. These data would form a two-dimensional process, indexed by row and column in space. In that section, the multidimensional autocovariance function of an \(r\)-dimensional stationary series was given as \(\gamma_{x}(h)=\mathrm{E}[x_{s+h}x_{s}]\), where the multidimensional lag vector is \(h=(h_{1},h_{2},\ldots,h_{r})^{\prime}\).

The multidimensional _wavenumber spectrum_ is given as the Fourier transform of the autocovariance, namely,

\[f_{x}(\omega)=\sum\cdots\sum_{h}\gamma_{x}(h)\mathrm{e}^{-2\pi i\omega^{ \prime}h}. \tag{4.139}\]

Again, the inverse result

\[\gamma_{x}(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\cdots\int_{-\frac{1}{2}}^{ \frac{1}{2}}f_{x}(\omega)e^{2\pi i\omega^{\prime}h}d\omega \tag{4.140}\]

holds, where the integral is over the multidimensional range of the vector \(\omega\). The wavenumber argument is exactly analogous to the frequency argument, and we have the corresponding intuitive interpretation as the cycling rate \(\omega_{i}\) per distance traveled \(s_{i}\) in the \(i\)-th direction.

Two-dimensional processes occur often in practical applications, and the representations above reduce to

\[f_{x}(\omega_{1},\omega_{2})=\sum_{h_{1}=-\infty}^{\infty}\sum_{h_{2}=-\infty} ^{\infty}\gamma_{x}(h_{1},h_{2})\mathrm{e}^{-2\pi i(\omega_{1}h_{1}+\omega_{2 }h_{2})} \tag{4.141}\]

and

\[\gamma_{x}(h_{1},h_{2})=\int_{-\frac{1}{2}}^{\frac{1}{2}}\int_{-\frac{1}{2}}^{ \frac{1}{2}}f_{x}(\omega_{1},\omega_{2})\mathrm{e}^{2\pi i(\omega_{1}h_{1}+ \omega_{2}h_{2})}d\omega_{1}\;d\omega_{2} \tag{4.142}\]in the case \(r=2\). The notion of linear filtering generalizes easily to the two-dimensional case by defining the impulse response function \(a_{s_{1},s_{2}}\) and the spatial filter output as

\[y_{s_{1},s_{2}}=\sum_{u_{1}}\sum_{u_{2}}a_{u_{1},u_{2}}x_{s_{1}-u_{1},s_{2}-u_{2 }}. \tag{4.143}\]

The spectrum of the output of this filter can be derived as

\[f_{y}(\omega_{1},\omega_{2})=|A(\omega_{1},\omega_{2})|^{2}f_{x}(\omega_{1}, \omega_{2}), \tag{4.144}\]

where

\[A(\omega_{1},\omega_{2})=\sum_{u_{1}}\sum_{u_{2}}a_{u_{1},u_{2}}\mathrm{e}^{-2 \pi i(\omega_{1}u_{1}+\omega_{2}u_{2})}. \tag{4.145}\]

These results are analogous to those in the one-dimensional case, described by Property 4.3.

The multidimensional DFT is also a straightforward generalization of the univariate expression. In the two-dimensional case with data on a rectangular grid, \(\{x_{s_{1},s_{2}};\;s_{1}=1,\ldots,n_{1},\;s_{2}=1,\ldots,n_{2}\}\), we will write, for \(-1/2\leq\omega_{1},\omega_{2}\leq 1/2\),

\[d(\omega_{1},\omega_{2})=(n_{1}n_{2})^{-1/2}\sum_{s_{1}=1}^{n_{1}}\sum_{s_{2}= 1}^{n_{2}}x_{s_{1},s_{2}}\mathrm{e}^{-2\pi i(\omega_{1}s_{1}+\omega_{2}s_{2})} \tag{4.146}\]

as the two-dimensional DFT, where the frequencies \(\omega_{1},\omega_{2}\) are evaluated at multiples of \((1/n_{1},1/n_{2})\) on the spatial frequency scale. The two-dimensional wavenumber spectrum can be estimated by the smoothed _sample wavenumber spectrum_

\[\bar{f}_{x}(\omega_{1},\omega_{2})=(L_{1}L_{2})^{-1}\sum_{\ell_{1},\ell_{2}}|d (\omega_{1}+\ell_{1}/n_{1},\omega_{2}+\ell_{2}/n_{2})|^{2}\,, \tag{4.147}\]

where the sum is taken over the grid \(\{-m_{j}\leq\ell_{j}\leq m_{j};\;j=1,2\}\), where \(L_{1}=2m_{1}+1\) and \(L_{2}=2m_{2}+1\). The statistic

\[\frac{2L_{1}L_{2}\bar{f}_{x}(\omega_{1},\omega_{2})}{f_{x}(\omega_{1},\omega_{ 2})}\stackrel{{\cdot}}{{\sim}}\chi^{2}_{2L_{1}L_{2}} \tag{4.148}\]

can be used to set confidence intervals or make approximate tests against a fixed assumed spectrum \(f_{0}(\omega_{1},\omega_{2})\).

**Example 4.26**: **Soil Surface Temperatures**

As an example, consider the periodogram of the two-dimensional temperature series shown in Fig. 1.18 and analyzed by Bazza et al. [15]. We recall the spatial coordinates in this case will be \((s_{1},s_{2})\), which define the spatial coordinates rows and columns so that the frequencies in the two directions will be expressed as cycles per row and cycles per column. Figure 4.21 shows the periodogram of the two-dimensional temperature series, and we note the ridge of strong spectral peaks running over rows at a column frequency of zero. An obvious periodic component appears at frequencies of \(.0625\) and \(-.0625\) cycles per row, which corresponds to 16 rows or about 272 ft. On further investigation of previous irrigation patterns over this field, treatment levels of salt varied periodically over columns. This analysis is extended in Problem 4.24, where we recover the salt treatment profile over rows and compare it to a signal, computed by averaging over columns.

Figure 4.21 may be reproduced in R as follows. In the code for this example, the periodogram is computed in one step as per; the rest of the code is simply manipulation to obtain a nice graphic.

```
per=Mod(fft(soiltemp-mean(soiltemp))/sqrt(64*36))*2 per2=cbind(per[1:32,18:2],per[1:32,1:18]) per3=rbind(per2[32:2,],per2) par(mar=c(1,2.5,0,0)+.1) persp(-31:31/64,-17:17/36,per3,phi=30,theta=30,expand=.6, ticktype="detailed",xlab="cycles/row",ylab="cycles/column", zlab="PeriodogramOrdinate")
```

Another application of two-dimensional spectral analysis of agricultural field trials is given in McBratney and Webster [134], who used it to detect ridge and furrow patterns in yields. The requirement for regular, equally spaced samples on fairly large grids has tended to limit enthusiasm for strict two-dimensional spectral analysis. An exception is when a propagating signal from a given velocity and azimuth is present so predicting the wavenumber spectrum as a function of velocity and azimuth becomes feasible (see Shumway et al. [186]).

Figure 4.21: Two-dimensional periodogram of soil temperature profile showing peak at.0625 cycles/row. The period is 16 rows, and this corresponds to \(16\times 17\) ft = 272 ft

## Problems

### Section 4.1

**4.1** Verify that for any positive integer \(n\) and \(j\), \(k=0,1,\ldots,\llbracket n/2\rrbracket\), where \(\llbracket\cdot\rrbracket\) denotes the greatest integer function:

(a) Except for \(j=0\) or \(j=n/2\),12

Footnote 12: _Hint: We’ll do part of the problem._

\[\sum_{t=1}^{n}\cos^{2}(2\pi t\,j/n) =\tfrac{1}{4}\sum_{t=1}^{n}(\mathrm{e}^{2\pi it\,j/n}+\mathrm{e}^ {-2\pi it\,j/n})(\mathrm{e}^{2\pi it\,j/n}+\mathrm{e}^{-2\pi it\,j/n})\] \[=\tfrac{1}{4}\sum_{t=1}^{n}(\mathrm{e}^{4\pi it\,j/n}+1+1+\mathrm{ e}^{-4\pi it\,j/n})=\frac{n}{2}.\]

(b) When \(j=0\) or \(j=n/2\),

\[\sum_{t=1}^{n}\cos^{2}(2\pi tj/n)=n\ \ \text{but}\ \ \sum_{t=1}^{n}\sin^{2}(2\pi tj/n)=0.\]

(c) For \(j\neq k\),

\[\sum_{t=1}^{n}\cos(2\pi tj/n)\cos(2\pi tk/n)=\sum_{t=1}^{n}\sin(2\pi tj/n)\sin (2\pi tk/n)=0.\]

Also, for any \(j\) and \(k\),

\[\sum_{t=1}^{n}\cos(2\pi tj/n)\sin(2\pi tk/n)=0.\]

**4.2** Repeat the simulations and analyses in Example 4.1 and Example 4.2 with the following changes:

(a) Change the sample size to \(n=128\) and generate and plot the same series as in Example 4.1:

\[x_{t1} =2\cos(2\pi\,.06\,t)+3\sin(2\pi\,.06\,t),\] \[x_{t2} =4\cos(2\pi\,.10\,t)+5\sin(2\pi\,.10\,t),\] \[x_{t3} =6\cos(2\pi\,.40\,t)+7\sin(2\pi\,.40\,t),\] \[x_{t} =x_{t1}+x_{t2}+x_{t3}.\]

What is the major difference between these series and the series generated in Example 4.1? (Hint: The answer is _fundamental_. But if your answer is the series are longer, you may be punished severely.)(b) As in Example 4.2, compute and plot the periodogram of the series, \(x_{t}\), generated in (a) and comment. (c) Repeat the analyses of (a) and (b) but with \(n=100\) (as in Example 4.1), and adding noise to \(x_{t}\); that is \[x_{t}=x_{t1}+x_{t2}+x_{t3}+w_{t}\] where \(w_{t}\sim\text{iid N}(0,25)\). That is, you should simulate and plot the data, and then plot the periodogram of \(x_{t}\) and comment.

**4.3** With reference to equations (4.1) and (4.2), let \(Z_{1}=U_{1}\) and \(Z_{2}=-U_{2}\) be independent, standard normal variables. Consider the polar coordinates of the point \((Z_{1},Z_{2})\), that is,

\[A^{2}=Z_{1}^{2}+Z_{2}^{2}\quad\text{and}\quad\phi=\tan^{-1}(Z_{2}/Z_{1}).\]

(a) Find the joint density of \(A^{2}\) and \(\phi\), and from the result, conclude that \(A^{2}\) and \(\phi\) are independent random variables, where \(A^{2}\) is a chi-squared random variable with 2 df, and \(\phi\) is uniformly distributed on \((-\pi,\pi)\). (b) Going in reverse from polar coordinates to rectangular coordinates, suppose we assume that \(A^{2}\) and \(\phi\) are independent random variables, where \(A^{2}\) is chi-squared with 2 df, and \(\phi\) is uniformly distributed on \((-\pi,\pi)\). With \(Z_{1}=A\cos(\phi)\) and \(Z_{2}=A\sin(\phi)\), where \(A\) is the positive square root of \(A^{2}\), show that \(Z_{1}\) and \(Z_{2}\) are independent, standard normal random variables.

**4.4** Verify (4.5).

_Section 4.2_

**4.5** A time series was generated by first drawing the white noise series \(w_{t}\) from a normal distribution with mean zero and variance one. The observed series \(x_{t}\) was generated from

\[x_{t}=w_{t}-\theta w_{t-1},\quad t=0,\pm 1,\pm 2,\ldots,\]

where \(\theta\) is a parameter.

(a) Derive the theoretical mean value and autocovariance functions for the series \(x_{t}\) and \(w_{t}\). Are the series \(x_{t}\) and \(w_{t}\) stationary? Give your reasons.

(b) Give a formula for the power spectrum of \(x_{t}\), expressed in terms of \(\theta\) and \(\omega\).

**4.6** A first-order autoregressive model is generated from the white noise series \(w_{t}\) using the generating equations

\[x_{t}=\phi x_{t-1}+w_{t},\]

where \(\phi\), for \(|\phi|<1\), is a parameter and the \(w_{t}\) are independent random variables with mean zero and variance \(\sigma_{w}^{2}\).

(a) Show that the power spectrum of \(x_{t}\) is given by

\[f_{x}(\omega)=\frac{\sigma_{w}^{2}}{1+\phi^{2}-2\phi\cos(2\pi\omega)}.\]

(b) Verify the autocovariance function of this process is

\[\gamma_{x}(h)=\frac{\sigma_{w}^{2}\ \phi^{|h|}}{1-\phi^{2}},\]

\(h=0,\pm 1,\pm 2,\ldots\), by showing that the inverse transform of \(\gamma_{x}(h)\) is the spectrum derived in part (a).

**4.7** In applications, we will often observe series containing a signal that has been delayed by some unknown time \(D\), i.e.,

\[x_{t}=s_{t}+As_{t-D}+n_{t},\]

where \(s_{t}\) and \(n_{t}\) are stationary and independent with zero means and spectral densities \(f_{s}(\omega)\) and \(f_{n}(\omega)\), respectively. The delayed signal is multiplied by some unknown constant \(A\). Show that

\[f_{x}(\omega)=[1+A^{2}+2A\cos(2\pi\omega D)]f_{s}(\omega)+f_{n}(\omega).\]

**4.8** Suppose \(x_{t}\) and \(y_{t}\) are stationary zero-mean time series with \(x_{t}\) independent of \(y_{s}\) for all \(s\) and \(t\). Consider the product series

\[z_{t}=x_{t}y_{t}.\]

Prove the spectral density for \(z_{t}\) can be written as

\[f_{z}(\omega)=\int_{-\frac{1}{2}}^{\frac{1}{2}}f_{x}(\omega-\nu)f_{y}(\nu)\,d\nu.\]

_Section 4.3_

**4.9** Figure 4.22 shows the biyearly smoothed (12-month moving average) number of sunspots from June 1749 to December 1978 with \(n=459\) points that were taken twice per year; the data are contained in sunspotz. With Example 4.13 as a guide, perform a periodogram analysis identifying the predominant periods and obtaining confidence intervals for the identified periods. Interpret your findings.

**4.10** The levels of salt concentration known to have occurred over rows, corresponding to the average temperature levels for the soil science data considered in Figs. 1.18 and 1.19, are in salt and saltemp. Plot the series and then identify the dominant frequencies by performing separate spectral analyses on the two series. Include confidence intervals for the dominant frequencies and interpret your findings.

**4.11**: Let the observed series \(x_{t}\) be composed of a periodic signal and noise so it can be written as

\[x_{t}=\beta_{1}\cos(2\pi\omega_{k}t)+\beta_{2}\sin(2\pi\omega_{k}t)+w_{t},\]

where \(w_{t}\) is a white noise process with variance \(\sigma_{w}^{2}\). The frequency \(\omega_{k}\) is assumed to be known and of the form \(k/n\) in this problem. Suppose we consider estimating \(\beta_{1}\), \(\beta_{2}\) and \(\sigma_{w}^{2}\) by least squares, or equivalently, by maximum likelihood if the \(w_{t}\) are assumed to be Gaussian.

(a) Prove, for a fixed \(\omega_{k}\), the minimum squared error is attained by

\[\begin{pmatrix}\hat{\beta}_{1}\\ \hat{\beta}_{2}\end{pmatrix}=2n^{-1/2}\begin{pmatrix}d_{c}(\omega_{k})\\ d_{s}(\omega_{k})\end{pmatrix},\]

where the cosine and sine transforms (4.31) and (4.32) appear on the right-hand side.

(b) Prove that the error sum of squares can be written as

\[\text{SSE}=\sum_{t=1}^{n}x_{t}^{2}-2I_{x}(\omega_{k})\]

so that the value of \(\omega_{k}\) that minimizes squared error is the same as the value that maximizes the periodogram \(I_{x}(\omega_{k})\) estimator (4.28).

(c) Under the Gaussian assumption and fixed \(\omega_{k}\), show that the \(F\)-test of no regression leads to an \(F\)-statistic that is a monotone function of \(I_{x}(\omega_{k})\).

**4.12**: Prove the convolution property of the DFT, namely,

\[\sum_{s=1}^{n}a_{s}x_{t-s}=\sum_{k=0}^{n-1}d_{A}(\omega_{k})d_{x}(\omega_{k}) \exp\{2\pi\omega_{k}t\},\]

for \(t=1,2,\ldots,n\), where \(d_{A}(\omega_{k})\) and \(d_{x}(\omega_{k})\) are the discrete Fourier transforms of \(a_{t}\) and \(x_{t}\), respectively, and we assume that \(x_{t}=x_{t+n}\) is periodic.

Figure 4.22: Smoothed 12-month sunspot numbers (sunspotz) sampled twice per year
Analyze the chicken price data (chicken) using a nonparametric spectral estimation procedure. Aside from the obvious annual cycle discovered in Example 2.5, what other interesting cycles are revealed?

**4.14** Repeat Problem 4.9 using a nonparametric spectral estimation procedure. In addition to discussing your findings in detail, comment on your choice of a spectral estimate with regard to smoothing and tapering.

**4.15** Repeat Problem 4.10 using a nonparametric spectral estimation procedure. In addition to discussing your findings in detail, comment on your choice of a spectral estimate with regard to smoothing and tapering.

**4.16** Cepstral Analysis.: The periodic behavior of a time series induced by echoes can also be observed in the spectrum of the series; this fact can be seen from the results stated in Problem 4.7. Using the notation of that problem, suppose we observe \(x_{t}=s_{t}+As_{t-D}+n_{t}\), which implies the spectra satisfy \(f_{x}(\omega)=[1+A^{2}+2A\cos(2\pi\omega D)]f_{s}(\omega)+f_{n}(\omega)\). If the noise is negligible (\(f_{n}(\omega)\approx 0\)) then \(\log f_{x}(\omega)\) is approximately the sum of a periodic component, \(\log[1+A^{2}+2A\cos(2\pi\omega D)]\), and \(\log f_{s}(\omega)\). Bogart et al. [27] proposed treating the detrended log spectrum as a pseudo time series and calculating its spectrum, or _cepstrum_, which should show a peak at a _quefrency_ corresponding to \(1/D\). The cepstrum can be plotted as a function of quefrency, from which the delay \(D\) can be estimated.

For the speech series presented in Example 1.3, estimate the pitch period using cepstral analysis as follows. The data are in speech.

(a) Calculate and display the log-periodogram of the data. Is the periodogram periodic, as predicted? (b) Perform a cepstral (spectral) analysis on the detrended logged periodogram, and use the results to estimate the delay \(D\). How does your answer compare with the analysis of Example 1.27, which was based on the ACF?

**4.17** Use Property 4.2 to verify (4.71). Then verify (4.74) and (4.75).

**4.18** Consider two time series

\[x_{t}=w_{t}-w_{t-1},\]

\[y_{t}=\tfrac{1}{2}(w_{t}+w_{t-1}),\]

formed from the white noise series \(w_{t}\) with variance \(\sigma_{w}^{2}=1\).

(a) Are \(x_{t}\) and \(y_{t}\) jointly stationary? Recall the cross-covariance function must also be a function only of the lag \(h\) and cannot depend on time. (b) Compute the spectra \(f_{y}(\omega)\) and \(f_{x}(\omega)\), and comment on the difference between the two results.

(c) Suppose sample spectral estimators \(\bar{f}_{y}(.10)\) are computed for the series using \(L=3\). Find \(a\) and \(b\) such that

\[P\bigg{\{}a\leq\bar{f}_{y}(.10)\leq b\bigg{\}}=.90.\]

This expression gives two points that will contain 90% of the sample spectral values. Put 5% of the area in each tail.

#### 4.5.2 Section 4.5

**4.19** Often, the periodicities in the sunspot series are investigated by fitting an autoregressive spectrum of sufficiently high order. The main periodicity is often stated to be in the neighborhood of 11 years. Fit an autoregressive spectral estimator to the sunspot data using a model selection method of your choice. Compare the result with a conventional nonparametric spectral estimator found in Problem 4.9.

**4.20** Analyze the chicken price data (chicken) using a parametric spectral estimation procedure. Compare the results to Problem 4.13.

**4.21** Fit an autoregressive spectral estimator to the Recruitment series and compare it to the results of Example 4.16.

**4.22** Suppose a sample time series with \(n=256\) points is available from the first-order autoregressive model. Furthermore, suppose a sample spectrum computed with \(L=3\) yields the estimated value \(\bar{f}_{x}(1/8)=2.25\). Is this sample value consistent with \(\sigma_{w}^{2}=1,\phi=.5\)? Repeat using \(L=11\) if we just happen to obtain the same sample value.

**4.23** Suppose we wish to test the noise alone hypothesis \(H_{0}:x_{t}=n_{t}\) against the signal-plus-noise hypothesis \(H_{1}:x_{t}=s_{t}+n_{t}\), where \(s_{t}\) and \(n_{t}\) are uncorrelated zero-mean stationary processes with spectra \(f_{s}(\omega)\) and \(f_{n}(\omega)\). Suppose that we want the test over a band of \(L=2m+1\) frequencies of the form \(\omega_{j:n}+k/n\), for \(k=0,\pm 1,\pm 2,\ldots,\pm m\) near some fixed frequency \(\omega\). Assume that both the signal and noise spectra are approximately constant over the interval.

(a) Prove the approximate likelihood-based test statistic for testing \(H_{0}\) against \(H_{1}\) is proportional to

\[T=\sum_{k}|d_{x}(\omega_{j:n}+k/n)|^{2}\left(\frac{1}{f_{n}(\omega)}-\frac{1}{ f_{s}(\omega)+f_{n}(\omega)}\right).\]

(b) Find the approximate distributions of \(T\) under \(H_{0}\) and \(H_{1}\).

(c) Define the false alarm and signal detection probabilities as \(P_{F}=P\{T>K|H_{0}\}\) and \(P_{d}=P\{T>k|H_{1}\}\), respectively. Express these probabilities in terms of the signal-to-noise ratio \(f_{s}(\omega)/f_{n}(\omega)\) and appropriate chi-squared integrals.

_Section 4.6_

**4.24**: Analyze the coherency between the temperature and salt data discussed in Problem 4.10. Discuss your findings.

**4.25**: Consider two processes

\[x_{t}=w_{t}\quad\text{and}\quad y_{t}=\phi x_{t-D}+v_{t}\]

where \(w_{t}\) and \(v_{t}\) are independent white noise processes with common variance \(\sigma^{2}\), \(\phi\) is a constant, and \(D\) is a fixed integer delay.

(a) Compute the coherency between \(x_{t}\) and \(y_{t}\).

(b) Simulate \(n=1024\) normal observations from \(x_{t}\) and \(y_{t}\) for \(\phi=.9\), \(\sigma^{2}=1\), and \(D=0\). Then estimate and plot the coherency between the simulated series for the following values of \(L\) and comment:

(i) \(L=1\), (ii) \(L=3\), (iii) \(L=41\), and (iv) \(L=101\).

_Section 4.7_

**4.26**: For the processes in Problem 4.25:

(a) Compute the phase between \(x_{t}\) and \(y_{t}\).

(b) Simulate \(n=1024\) observations from \(x_{t}\) and \(y_{t}\) for \(\phi=.9\), \(\sigma^{2}=1\), and \(D=1\). Then estimate and plot the phase between the simulated series for the following values of \(L\) and comment:

(i) \(L=1\), (ii) \(L=3\), (iii) \(L=41\), and (iv) \(L=101\).

**4.27**: Consider the bivariate time series records containing monthly U.S. production (prod) as measured by the Federal Reserve Board Production Index and the monthly unemployment series (unemp).

(a) Compute the spectrum and the log spectrum for each series, and identify statistically significant peaks. Explain what might be generating the peaks. Compute the coherence, and explain what is meant when a high coherence is observed at a particular frequency.

(b) What would be the effect of applying the filter

\[u_{t}=x_{t}-x_{t-1}\quad\text{followed by}\quad v_{t}=u_{t}-u_{t-12}\]

to the series given above? Plot the predicted frequency responses of the simple difference filter and of the seasonal difference of the first difference.

(c) Apply the filters successively to one of the two series and plot the output. Examine the output after taking a first difference and comment on whether stationarity is a reasonable assumption. Why or why not? Plot after taking the seasonal difference of the first difference. What can be noticed about the output that is consistent with what you have predicted from the frequency response? Verify by computing the spectrum of the output after filtering.

**4.28** Determine the theoretical power spectrum of the series formed by combining the white noise series \(w_{t}\) to form

\[y_{t}=w_{t-2}+4w_{t-1}+6w_{t}+4w_{t+1}+w_{t+2}.\]

Determine which frequencies are present by plotting the power spectrum.

**4.29** Let \(x_{t}=\cos(2\pi\omega t)\), and consider the output

\[y_{t}=\sum_{k=-\infty}^{\infty}a_{k}x_{t-k},\]

where \(\sum_{k}|a_{k}|<\infty\). Show

\[y_{t}=|A(\omega)|\cos(2\pi\omega t+\phi(\omega)),\]

where \(|A(\omega)|\) and \(\phi(\omega)\) are the amplitude and phase of the filter, respectively. Interpret the result in terms of the relationship between the input series, \(x_{t}\), and the output series, \(y_{t}\).

**4.30** Suppose \(x_{t}\) is a stationary series, and we apply two filtering operations in succession, say,

\[y_{t}=\sum_{r}a_{r}x_{t-r}\quad\mbox{then}\quad z_{t}=\sum_{s}b_{s}y_{t-s}.\]

(a) Show the spectrum of the output is

\[f_{z}(\omega)=|A(\omega)|^{2}|B(\omega)|^{2}f_{x}(\omega),\]

where \(A(\omega)\) and \(B(\omega)\) are the Fourier transforms of the filter sequences \(a_{t}\) and \(b_{t}\), respectively.

(b) What would be the effect of applying the filter

\[u_{t}=x_{t}-x_{t-1}\quad\mbox{followed by}\quad v_{t}=u_{t}-u_{t-12}\]

to a time series?

(c) Plot the predicted frequency responses of the simple difference filter and of the seasonal difference of the first difference. Filters like these are called seasonal adjustment filters in economics because they tend to attenuate frequencies at multiples of the monthly periods. The difference filter tends to attenuate low-frequency trends.

**4.31** Suppose we are given a stationary zero-mean series \(x_{t}\) with spectrum \(f_{x}(\omega)\) and then construct the derived series

\[y_{t}=ay_{t-1}+x_{t},\quad t=\pm 1,\pm 2,\ldots.\]

(a) Show how the theoretical \(f_{y}(\omega)\) is related to \(f_{x}(\omega)\).

(b) Plot the function that multiplies \(f_{x}(\omega)\) in part (a) for \(a=.1\) and for \(a=.8\). This filter is called a recursive filter.

_Section 4.8_

**4.32** Consider the problem of approximating the filter output

\[y_{t}=\sum_{k=-\infty}^{\infty}a_{k}x_{t-k},\quad\sum_{-\infty}^{\infty}|a_{k}|<\infty,\]

by

\[y_{t}^{M}=\sum_{|k|<M/2}a_{k}^{M}x_{t-k}\]

for \(t=M/2-1,M/2,\ldots,n-M/2\), where \(x_{t}\) is available for \(t=1,\ldots,n\) and

\[a_{t}^{M}=M^{-1}\sum_{k=0}^{M-1}A(\omega_{k})\exp\{2\pi i\omega_{k}t\}\]

with \(\omega_{k}=k/M\). Prove

\[\mathrm{E}\{(y_{t}-y_{t}^{M})^{2}\}\leq 4\gamma_{x}(0)\biggl{(}\sum_{|k|\geq M /2}|a_{k}|\biggr{)}^{2}.\]

**4.33** Prove the squared coherence \(\rho_{y\cdot x}^{2}(\omega)=1\) for all \(\omega\) when

\[y_{t}=\sum_{r=-\infty}^{\infty}a_{r}x_{t-r},\]

that is, when \(x_{t}\) and \(y_{t}\) can be related exactly by a linear filter.

**4.34** The data set climbwd, contains 454 months of measured values for six climatic variables: (i) air temperature [Temp], (ii) dew point [DewPt], (iii) cloud cover [CldCvr], (iv) wind speed [WndSpd], (v) precipitation [Precip], and (vi) inflow [Inflow], at Lake Shasta in California; the data are displayed in Fig. 7.3. We would like to look at possible relations among the weather factors and between the weather factors and the inflow to Lake Shasta.

(a) First transform the inflow and precipitation series as follows: \(I_{t}=\log i_{t}\), where \(i_{t}\) is inflow, and \(P_{t}=\sqrt{p}_{t}\), where \(p_{t}\) is precipitation. Then, compute the square coherencies between all the weather variables and transformed inflow and argue that the strongest determinant of the inflow series is (transformed) precipitation. [_Tip:_ If x contains multiple time series, then the easiest way to display all the squared coherencies is to plot the coherencies suppressing the confidence intervals, e.g., mvspec(x, spans=c(7,7), taper=.5, plot.type="coh", ci=-1). (b) Fit a lagged regression model of the form

\[I_{t}=\beta_{0}+\sum_{j=0}^{\infty}\beta_{j}P_{t-j}+w_{t},\]

using thresholding, and then comment of the predictive ability of precipitation for inflow.

[MISSING_PAGE_EMPTY:249]

**4.38** Consider the two-dimensional linear filter given as the output (4.143).

(a) Express the two-dimensional autocovariance function of the output, say, \(\gamma_{y}(h_{1},h_{2})\), in terms of an infinite sum involving the autocovariance function of \(x_{s}\) and the filter coefficients \(a_{s_{1},s_{2}}\).

(b) Use the expression derived in (a), combined with (4.142) and (4.145) to derive the spectrum of the filtered output (4.144).

_The following problems require supplemental material from Appendix C._

**4.39** Let \(w_{t}\) be a Gaussian white noise series with variance \(\sigma_{w}^{2}\). Prove that the results of Theorem C.4 hold without error for the DFT of \(w_{t}\).

**4.40** Show that condition (4.48) implies (C.19) by showing

\[n^{-1/2}\sum_{h\geq 0}h\,|\gamma(h)|\leq\sigma_{w}^{2}\sum_{k\geq 0}|\psi_{k}| \sum_{j\geq 0}\sqrt{j}\,|\psi_{j}|.\]

**4.41** Prove Lemma C.4.

**4.42** Finish the proof of Theorem C.5.

**4.43** For the zero-mean complex random vector \(\boldsymbol{z}=x_{c}-ix_{s}\), with \(\operatorname{cov}(z)=\varSigma=C-iQ\), with \(\varSigma=\varSigma^{*}\), define

\[w=2\text{Re}(a^{*}\boldsymbol{z}),\]

where \(a=a_{c}-ia_{s}\) is an arbitrary non-zero complex vector. Prove

\[\operatorname{cov}(w)=2a^{*}\varSigma a.\]

Recall \({}^{*}\) denotes the complex conjugate transpose.

## Chapter Additional Time Domain Topics

In this chapter, we present material that may be considered special or advanced topics in the time domain. Chapter 6 is devoted to one of the most useful and interesting time domain topics, state-space models. Consequently, we do not cover state-space models or related topics--of which there are many--in this chapter. This chapter contains sections of independent topics that may be read in any order. Most of the sections depend on a basic knowledge of ARMA models, forecasting and estimation, which is the material that is covered in Chap. 3. A few sections, for example the section on long memory models, require some knowledge of spectral analysis and related topics covered in Chap. 4. In addition to long memory, we discuss unit root testing, GARCH models, threshold models, lagged regression or transfer functions, and selected topics in multivariate ARMAX models.

### 5.1 Long Memory ARMA and Fractional Differencing

The conventional ARMA(\(p,q\)) process is often referred to as a short-memory process because the coefficients in the representation

\[x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j},\]

obtained by solving

\[\phi(z)\psi(z)=\theta(z),\]

are dominated by exponential decay. As pointed out in Sects. 3.2 and 3.3, this result implies the ACF of the short memory process satisfies \(\rho(h)\to 0\) exponentially fast as \(h\to\infty\). When the sample ACF of a time series decays slowly, the advice given in Chap. 3 has been to difference the series until it seems stationary. Following this advice with the glacial curve series first presented in Example 3.33 leads tothe first difference of the logarithms of the data being represented as a first-order moving average. In Example 3.41, further analysis of the residuals leads to fitting an ARIMA\((1,1,1)\) model,

\[\nabla x_{t}=\phi\nabla x_{t-1}+w_{t}+\theta w_{t-1},\]

where we understand \(x_{t}\) is the log-transformed curve series. In particular, the estimates of the parameters (and the standard errors) were \(\hat{\phi}=.23(.05)\), \(\hat{\theta}=-.89(.03)\), and \(\hat{\sigma}_{w}^{2}=.23\).

The use of the first difference \(\nabla x_{t}=(1-B)x_{t}\), however, can sometimes be too severe a modification in the sense that the nonstationary model might represent an overdifferencing of the original process. Long memory (or persistent) time series were considered in Hosking [97] and Granger and Joyeux [79] as intermediate compromises between the short memory ARMA type models and the fully integrated nonstationary processes in the Box-Jenkins class. The easiest way to generate a long memory series is to think of using the difference operator \((1-B)^{d}\) for fractional values of \(d\), say, \(0<d<.5\), so a basic long memory series gets generated as

\[(1-B)^{d}x_{t}=w_{t}, \tag{5.1}\]

where \(w_{t}\) still denotes white noise with variance \(\sigma_{w}^{2}\). The fractionally differenced series (5.1), for \(|d|<.5\), is often called _fractional noise_ (except when \(d\) is zero). Now, \(d\) becomes a parameter to be estimated along with \(\sigma_{w}^{2}\). Differencing the original process, as in the Box-Jenkins approach, may be thought of as simply assigning a value of \(d=1\). This idea has been extended to the class of fractionally integrated ARMA, or ARFIMA models, where \(-.5<d<.5\); when \(d\) is negative, the term antipersistent is used. Long memory processes occur in hydrology (see Hurst [98] and McLeod and Hipel [137]) and in environmental series, such as the curve data we have previously analyzed, to mention a few examples. Long memory time series data tend to exhibit sample autocorrelations that are not necessarily large (as in the case of \(d=1\)), but persist for a long time. Figure 5.1 shows the sample ACF, to lag 100, of the log-transformed curve series, which exhibits classic long memory behavior:

Figure 5.1: Sample ACF of the log transformed curve series

To investigate its properties, we can use the binomial expansion (\(d>-1\)) to write

\[w_{t}=(1-B)^{d}x_{t}=\sum_{j=0}^{\infty}\pi_{j}B^{j}x_{t}=\sum_{j=0}^{\infty}\pi_ {j}x_{t-j} \tag{5.2}\]

where

\[\pi_{j}=\frac{\Gamma(j-d)}{\Gamma(j+1)\Gamma(-d)} \tag{5.3}\]

with \(\Gamma(x+1)=x\Gamma(x)\) being the gamma function. Similarly (\(d<1\)), we can write

\[x_{t}=(1-B)^{-d}w_{t}=\sum_{j=0}^{\infty}\psi_{j}B^{j}w_{t}=\sum_{j=0}^{\infty} \psi_{j}w_{t-j} \tag{5.4}\]

where

\[\psi_{j}=\frac{\Gamma(j+d)}{\Gamma(j+1)\Gamma(d)}. \tag{5.5}\]

When \(|d|<.5\), the processes (5.2) and (5.4) are well-defined stationary processes (see Brockwell and Davis [36], for details). In the case of fractional differencing, however, the coefficients satisfy \(\sum\pi_{j}^{2}<\infty\) and \(\sum\psi_{j}^{2}<\infty\) as opposed to the absolute summability of the coefficients in ARMA processes.

Using the representation (5.4)-(5.5), and after some nontrivial manipulations, it can be shown that the ACF of \(x_{t}\) is

\[\rho(h)=\frac{\Gamma(h+d)\Gamma(1-d)}{\Gamma(h-d+1)\Gamma(d)}\sim h^{2d-1} \tag{5.6}\]

for large \(h\). From this we see that for \(0<d<.5\)

\[\sum_{h=-\infty}^{\infty}|\rho(h)|=\infty\]

and hence the term _long memory_.

In order to examine a series such as the curve series for a possible long memory pattern, it is convenient to look at ways of estimating \(d\). Using (5.3) it is easy to derive the recursions

\[\pi_{j+1}(d)=\frac{(j-d)\pi_{j}(d)}{(j+1)}, \tag{5.7}\]

for \(j=0,1,\ldots\), with \(\pi_{0}(d)=1\). Maximizing the joint likelihood of the errors under normality, say, \(w_{t}(d)\), will involve minimizing the sum of squared errors\[Q(d)=\sum w_{t}^{2}(d).\]

The usual Gauss-Newton method, described in Sect. 3.5, leads to the expansion

\[w_{t}(d)=w_{t}(d_{0})+w_{t}^{\prime}(d_{0})(d-d_{0}),\]

where

\[w_{t}^{\prime}(d_{0})=\frac{\partial w_{t}}{\partial d}\bigg{|}_{d=d_{0}}\]

and \(d_{0}\) is an initial estimate (guess) at to the value of \(d\). Setting up the usual regression leads to

\[d=d_{0}-\frac{\sum_{t}w_{t}^{\prime}(d_{0})w_{t}(d_{0})}{\sum_{t}w_{t}^{\prime} (d_{0})^{2}}. \tag{5.8}\]

The derivatives are computed recursively by differentiating (5.7) successively with respect to \(d\):

\[\pi_{j+1}^{\prime}(d)=\frac{(j-d)\pi_{j}^{\prime}(d)-\pi_{j}(d)}{j+1},\]

where \(\pi_{0}^{\prime}(d)=0\). The errors are computed from an approximation to (5.2), namely,

\[w_{t}(d)=\sum_{j=0}^{t}\pi_{j}(d)x_{t-j}. \tag{5.9}\]

It is advisable to omit a number of initial terms from the computation and start the sum, (5.8), at some fairly large value of \(t\) to have a reasonable approximation.

**Example 5.1**: **Long Memory Fitting of the Glacial Varve Series**

We consider analyzing the glacial varve series discussed in various examples and first presented in Example 2.7. Figure 2.7 shows the original and log-transformed series (which we denote by \(x_{t}\)). In Example 3.41, we noted that \(x_{t}\) could be modeled as an ARIMA\((1,1,1)\) process. We fit the fractionally differenced model, (5.1), to the mean-adjusted series, \(x_{t}-\bar{x}\). Applying the Gauss-Newton iterative procedure previously described, starting with \(d=.1\) and omitting the first 30 points from the computation, leads to a final value of \(d=.384\), which implies the set of coefficients \(\pi_{j}(.384)\), as given in Fig. 5.2 with \(\pi_{0}(.384)=1\). We can compare roughly the performance of the fractional difference operator with the ARIMA model by examining the autocorrelation functions of the two residual series as shown in Fig. 5.3. The ACFs of the two residual series are roughly comparable with the white noise model.

To perform this analysis in R, first download and install the fracdiff package. Then use

library(fracdiff)

varve = log(varve)-mean(log(varve))

varve.fd = fracdiff(varve, mar=0, mma=0, M=30)

varve.fdSd # = 0.3841688

varve.fdSstderr.dpq # = 4.589514e-06 (questionable result!!)p = rep(1,31)  for (k in 1:30){ p[k+1] = (k-varve.fdSd)*p[k]/(k+1) }  plot(1:30, p[-1], ylab-expression(pi(d)), xlab="Index", type="h")  res.fd = diffseries(log(varve), varve.fdSd) # frac diff resids  res.arima = resid(arima(log(varve), order=c(1,1,1))) # arima resids  par(mfrow=c(2,1))  acf(res.arima, 100, xlim=c(4,97), ylim=c(-.2,.2), main="")  acf(res.fd, 100, xlim=c(4,97), ylim=c(-.2,.2), main="") The R package uses a truncated maximum likelihood procedure that was discussed in Haslett and Raftery [95], which is a little more elaborate than simply zeroing out initial values. The default truncation value in R is \(M=100\). In the default case, the estimate is \(\hat{d}=.37\) with approximately the same _questionable_ standard error. The standard error is (supposedly) obtained from the Hessian as described in Example 3.30. A more believable standard error is given in Example 5.2.

Forecasting long memory processes is similar to forecasting ARIMA models. That is, (5.2) and (5.7) can be used to obtain the truncated forecasts

\[\ddot{x}_{n+m}^{n}=-\sum_{j=1}^{n}\pi_{j}(\hat{d})\,\ddot{x}_{n+m-j}^{n}\,, \tag{5.10}\]

for \(m=1,2,\ldots\). Error bounds can be approximated by using

\[P_{n+m}^{n}=\hat{\sigma}_{w}^{2}\left(\sum_{j=0}^{m-1}\psi_{j}^{2}(\hat{d})\right) \tag{5.11}\]

where, as in (5.7),

\[\psi_{j}(\hat{d})=\frac{(j+\hat{d})\psi_{j}(\hat{d})}{(j+1)}\,, \tag{5.12}\]

with \(\psi_{0}(\hat{d})=1\).

No obvious short memory ARMA-type component can be seen in the ACF of the residuals from the fractionally differenced curve series shown in Fig. 5.3. It is natural, however, that cases will exist in which substantial short memory-type components will also be present in data that exhibits long memory. Hence, it is natural to define the general ARFIMA(\(p,d,q\)), \(-.5<d<.5\) process as \[\phi(B)\nabla^{d}(x_{t}-\mu)=\theta(B)w_{t}, \tag{5.13}\]

where \(\phi(B)\) and \(\theta(B)\) are as given in Chap. 3. Writing the model in the form

\[\phi(B)\pi_{d}(B)(x_{t}-\mu)=\theta(B)w_{t} \tag{5.14}\]

makes it clear how we go about estimating the parameters for the more general model. Forecasting for the ARFIMA\((p,d,q)\) series can be easily done, noting that we may equate coefficients in

\[\phi(z)\psi(z)=(1-z)^{-d}\theta(z) \tag{5.15}\]

and

\[\theta(z)\pi(z)=(1-z)^{d}\phi(z) \tag{5.16}\]

to obtain the representations

\[x_{t}=\mu+\sum_{j=0}^{\infty}\psi_{j}w_{t-j}\quad\text{and}\quad w_{t}=\sum_{j =0}^{\infty}\pi_{j}(x_{t-j}-\mu).\]

We then can proceed as discussed in (5.10) and (5.11).

Comprehensive treatments of long memory time series models are given in the texts by Beran [18], Palma [145], and Robinson [168], and it should be noted that several other techniques for estimating the parameters, especially, the long memory parameter, can be developed in the frequency domain. In this case, we may think of the equations as generated by an infinite order autoregressive series with coefficients \(\pi_{j}\) given by (5.7). Using the same approach as before, we obtain

Figure 5.3: ACF of residuals from the ARIMA\((1,1,1)\) fit to the logged varve series (_top_) and of the residuals from the long memory model fit, \((1-B)^{d}x_{t}=w_{t}\), with \(d=.384\) (_bottom_)

\[f_{x}(\omega) = \frac{\sigma_{w}^{2}}{|\sum_{k=0}^{\infty}\pi_{k}\mathrm{e}^{-2\pi \mathrm{i}\mathbf{k}\omega}|^{2}}\] \[= \sigma_{w}^{2}|1-e^{-2\pi i\omega}|^{-2d}=[4\sin^{2}(\pi\omega)]^{- d}\sigma_{w}^{2}\]

as equivalent representations of the spectrum of a long memory process. The long memory spectrum approaches infinity as the frequency \(\omega\to 0\).

The main reason for defining the Whittle approximation to the log likelihood is to propose its use for estimating the parameter \(d\) in the long memory case as an alternative to the time domain method previously mentioned. The time domain approach is useful because of its simplicity and easily computed standard errors. One may also use an exact likelihood approach by developing an innovations form of the likelihood as in Brockwell and Davis [36].

For the approximate approach using the Whittle likelihood (4.85), we consider using the approach of Fox and Taqqu [62] who showed that maximizing the Whittle log likelihood leads to a consistent estimator with the usual asymptotic normal distribution that would be obtained by treating (4.85) as a conventional log likelihood (see also Dahlhaus [46]; Robinson [167]; Hurvich et al. [102]). Unfortunately, the periodogram ordinates are not asymptotically independent (Hurvich and Beltrao [101]), although a quasi-likelihood in the form of the Whittle approximation works well and has good asymptotic properties.

To see how this would work for the purely long memory case, write the long memory spectrum as

\[f_{x}(\omega_{k};\ d,\sigma_{w}^{2})=\sigma_{w}^{2}g_{k}^{-d}, \tag{5.18}\]

where

\[g_{k}=4\sin^{2}(\pi\omega_{k}). \tag{5.19}\]

Then, differentiating the log likelihood, say,

\[\ln L(x;d,\sigma_{w}^{2})\approx-m\ln\sigma_{w}^{2}+d\sum_{k=1}^{m}\ln g_{k}- \frac{1}{\sigma_{w}^{2}}\sum_{k=1}^{m}g_{k}^{d}\ I(\omega_{k}) \tag{5.20}\]

at \(m=n/2-1\) frequencies and solving for \(\sigma_{w}^{2}\) yields

\[\sigma_{w}^{2}(d)=\frac{1}{m}\sum_{k=1}^{m}g_{k}^{d}\ I(\omega_{k}) \tag{5.21}\]

as the approximate maximum likelihood estimator for the variance parameter. To estimate \(d\), we can use a grid search of the concentrated log likelihood

\[\ln L(x;d)\approx-m\ln\sigma_{w}^{2}(d)+d\sum_{k=1}^{m}\ln g_{k}-m \tag{5.22}\]

over the interval \((0,.5)\), followed by a Newton-Raphson procedure to convergence.

**Example 5.2**: **Long Memory Spectra for the Varve Series**

In Example 5.1, we fit a long memory model to the glacial curve data via time domain methods. Fitting the same model using frequency domain methods and the Whittle approximation above gives \(\hat{d}=.380\), with an estimated standard error of \(.028\). The earlier time domain method gave \(\hat{d}=.384\) with \(M=30\) and \(\hat{d}=.370\) with \(M=100\). Both estimates obtained via time domain methods had a standard error of about \(4.6\times 10^{-6}\), which seems implausible. The error variance estimate in this case is \(\hat{\sigma}_{w}^{2}=.2293\); in Example 5.1, we could have used var(res.fd) as an estimate, in which case we obtain \(.2298\). The R code to perform this analysis is

series = log(varve) # specify series to be analyzed d# =.1 # initial value of d n.per = nextn(length(series)) m = (n.per)/2 - 1 per = Mod(fft(series-mean(series))[-1])^2 # remove # freq and per = per/n.per # scale the periodgram g = 4*(sin(pi*((1:m)/n.per))^2) # Function to calculate -log.likelihood whit.like = function(d){ g.d=9*d sig2 = (sum(g.d*per[1:m])/m)  log.like = m*log(sig2) - d*sum(log(g)) + m return(log.like) }
Estimation (output not shown) (est = optim(d#, whit.like, gr-NULL, method="L-BFGS-B", hessian=TRUE,  lower=-.5, upper=.5, control=list(trace=1,REPORT=1)))
##-- Results: d.hat =.380, se(dhat) =.028, and sig2hat =.229 --# cat("d.hat =", estSpar, "se(dhat) = ",1/sqrt(estShesian),"\n") g.dhat = g*estSpar; sig2 = sum(g.dhat*per[1:m])/m cat("sig2hat =",sig2,"\n")

One might also consider fitting an autoregressive model to these data using a procedure similar to that used in Example 4.18. Following this approach gave an autoregressive model with \(p=8\) and \(\hat{\phi}_{1:8}=\{.34,.11,.04,.09,.08,.02,.09\}\), with \(\hat{\sigma}_{w}^{2}=.23\) as the error variance. The two log spectra are plotted in Fig. 5.4 for

Figure 5.4: Long Memory (\(d=.380\)) [_solid line_] and autoregressive AR(8) [_dashed line_] spectral estimators for the paleoclimatic glacial curve series\(\omega>0\), and we note that long memory spectrum will eventually become infinite, whereas the AR(8) spectrum is finite at \(\omega=0\). The R code used for this part of the example (assuming the previous values have been retained) is

 u = spec.ar(log(urve), plot-FALSE) # produces AR(8)  g = 4*(sin(pi*((1:500)/2000))^2)  flat = sig2*a{-estpar} # long memory spectral estimate  plot(1:500/2000, log(fhat), type="l", ylab="log(spectrum)", xlab="frequency")  lines(u$freq[1:250], log(u$spec[1:250]), lty="dashed")  ar.mle(log(varve)) # to get AR(8) estimates

Often, time series are not purely long memory. A common situation has the long memory component multiplied by a short memory component, leading to an alternate version of (5.18) of the form

\[f_{x}(\omega_{k};\ d,\theta)=g_{k}^{-d}f_{0}(\omega_{k};\theta), \tag{5.23}\]

where \(f_{0}(\omega_{k};\theta)\) might be the spectrum of an autoregressive moving average process with vector parameter \(\theta\), or it might be unspecified. If the spectrum has a parametric form, the Whittle likelihood can be used. However, there is a substantial amount of semiparametric literature that develops the estimators when the underlying spectrum \(f_{0}(\omega;\theta)\) is unknown. A class of _Gaussian semi-parametric_ estimators simply uses the same Whittle likelihood (5.22), evaluated over a sub-band of low frequencies, say \(m^{\prime}=\sqrt{n}\). There is some latitude in selecting a band that is relatively free from low frequency interference due to the short memory component in (5.23). If the spectrum is highly parameterized, one might estimate using the Whittle log likelihood (5.19) under (5.23) and jointly estimate the parameters \(d\) and \(\theta\) using the Newton-Raphson method. If we are interested in a nonparametric estimator, using the conventional smoothed spectral estimator for the periodogram, adjusted for the long memory component, say \(g_{k}^{d}\ I(\omega_{k})\) might be a possible approach.

Geweke and Porter-Hudak [72] developed an approximate method for estimating \(d\) based on a regression model, derived from (5.22). Note that we may write a simple equation for the logarithm of the spectrum as

\[\ln f_{x}(\omega_{k};d)=\ln f_{0}(\omega_{k};\theta)-d\ln[4\sin^{2}(\pi\omega_{ k})], \tag{5.24}\]

with the frequencies \(\omega_{k}=k/n\) restricted to a range \(k=1,2,\ldots,m\) near the zero frequency with \(m=\sqrt{n}\) as the recommended value. Relationship (5.24) suggests using a simple linear regression model of the form,

\[\ln I(\omega_{k})=\beta_{0}-d\ln[4\sin^{2}(\pi\omega_{k})]+e_{k} \tag{5.25}\]

for the periodogram to estimate the parameters \(\sigma_{w}^{2}\) and \(d\). In this case, one performs least squares using \(\ \ln I(\omega_{k})\\) as the dependent variable, and \(\ \ln[4\sin^{2}(\pi\omega_{k})]\\) as the independent variable for \(k=1,2,\ldots,m\). The resulting slope estimate is then used as an estimate of \(-d\). For a good discussion of various alternative methods for selecting \(m\), see Hurvich and Deo [103]. The R package fracdiff also provides this method via the command fdGPP(); see the help file for further information. Here is a quick example using the logged varve data.

library(fracdiff) fdGPH(log(varve), bandw=.9) # m = n*bandw  dhat = 0.383 se(dhat) = 0.041

### Unit Root Testing

As discussed in the previous section, the use of the first difference \(\nabla x_{t}=(1-B)x_{t}\) can be too severe a modification in the sense that the nonstationary model might represent an overdifferencing of the original process. For example, consider a causal AR(1) process (we assume throughout this section that the noise is Gaussian),

\[x_{t}=\phi x_{t-1}+w_{t}. \tag{5.26}\]

Applying \((1-B)\) to both sides shows that differencing, \(\nabla x_{t}=\phi\nabla x_{t-1}+\nabla w_{t}\), or

\[y_{t}=\phi y_{t-1}+w_{t}-w_{t-1},\]

where \(y_{t}=\nabla x_{t}\), introduces extraneous correlation and invertibility problems. That is, while \(x_{t}\) is a causal AR(1) process, working with the differenced process \(y_{t}\) will be problematic because it is a non-invertible ARMA(1, 1).

A unit root test provides a way to test whether (5.26) is a random walk (the null case) as opposed to a causal process (the alternative). That is, it provides a procedure for testing

\[H_{0}\colon\phi=1\quad\text{versus}\quad H_{1}:|\phi|<1.\]

An obvious test statistic would be to consider \((\hat{\phi}-1)\), appropriately normalized, in the hope to develop an asymptotically normal test statistic, where \(\hat{\phi}\) is one of the optimal estimators discussed in Chap. 3. Unfortunately, the theory of Sect. 3.5 will not work in the null case because the process is nonstationary. Moreover, as seen in Example 3.36, estimation near the boundary of stationarity produces highly skewed sample distributions (see Fig. 3.12) and this is a good indication that the problem will be atypical.

To examine the behavior of \((\hat{\phi}-1)\) under the null hypothesis that \(\phi=1\), or more precisely that the model is a random walk, \(x_{t}=\sum_{j=1}^{t}w_{j}\), or \(x_{t}=x_{t-1}+w_{t}\) with \(x_{0}=0\), consider the least squares estimator of \(\phi\). Noting that \(\mu_{x}=0\), the least squares estimator can be written as

\[\hat{\phi}=\frac{\sum_{t=1}^{n}x_{t}x_{t-1}}{\sum_{t=1}^{n}x_{t-1}^{2}}=1+\frac {\frac{1}{n}\sum_{t=1}^{n}w_{t}x_{t-1}}{\frac{1}{n}\sum_{t=1}^{n}x_{t-1}^{2}}, \tag{5.27}\]

where we have written \(x_{t}=x_{t-1}+w_{t}\) in the numerator; recall that \(x_{0}=0\) and in the least squares setting, we are regressing \(x_{t}\) on \(x_{t-1}\) for \(t=1,\ldots,n\). Hence, under \(H_{0}\), we have that

\[\hat{\phi}-1=\frac{\frac{1}{n\sigma_{w}^{2}}\sum_{t=1}^{n}w_{t}x_{t-1}}{\frac{ 1}{n\sigma_{w}^{2}}\sum_{t=1}^{n}x_{t-1}^{2}}. \tag{5.28}\]Consider the numerator of (5.28). Note first that by squaring both sides of \(x_{t}=x_{t-1}+w_{t}\), we obtain \(x_{t}^{2}=x_{t-1}^{2}+2x_{t-1}w_{t}+w_{t}^{2}\) so that

\[x_{t-1}w_{t}=\frac{1}{2}(x_{t}^{2}-x_{t-1}^{2}-w_{t}^{2}),\]

and summing,

\[\frac{1}{n\sigma_{w}^{2}}\sum_{t=1}^{n}x_{t-1}w_{t}=\frac{1}{2}\Bigg{(}\frac{x_ {n}^{2}}{n\sigma_{w}^{2}}-\frac{\sum_{t=1}^{n}w_{t}^{2}}{n\sigma_{w}^{2}}\Bigg{)}.\]

Because \(x_{n}=\sum_{1}^{n}w_{t}\), we have that \(x_{n}\sim N(0,n\sigma_{w}^{2})\), so that \(\chi_{1}^{2}=\frac{1}{n\sigma_{w}^{2}}x_{n}^{2}\) has a chi-squared distribution with one degree of freedom. Moreover, because \(w_{t}\) is white Gaussian noise, \(\frac{1}{n}\sum_{1}^{n}w_{t}^{2}\to_{p}\sigma_{w}^{2}\), or \(\frac{1}{n\sigma_{w}^{2}}\sum_{1}^{n}w_{t}^{2}\to_{p}1\). Consequently (\(n\to\infty\)),

\[\frac{1}{n\sigma_{w}^{2}}\sum_{t=1}^{n}x_{t-1}w_{t}\xrightarrow{d}\tfrac{1}{2} \big{(}\chi_{1}^{2}-1\big{)}\,. \tag{5.29}\]

Next we focus on the denominator of (5.28). First, we introduce standard Brownian motion.

**Definition 5.1**: _A continuous time process \(\{W(t);\ t\geq 0\}\) is called_ **standard Brownian motion** _if it satisfies the following conditions:_

1. \(W(0)=0\)_;_
2. \(\{W(t_{2})-W(t_{1}),W(t_{3})-W(t_{2}),\ldots,W(t_{n})-W(t_{n-1})\}\) _are independent for any collection of points,_ \(0\leq t_{1}<t_{2}\cdots<t_{n}\)_, and integer_ \(n>2\)_;_
3. \(W(t+\Delta t)-W(t)\sim N(0,\Delta t)\) _for_ \(\Delta t>0\)_._

In addition to (i)-(iii), it is assumed that almost all sample paths of \(W(t)\) are continuous in \(t\). The result for the denominator uses the functional central limit theorem, which can be found in Billlingsley [22, SS2.8]. In particular, if \(\xi_{1},\ldots,\xi_{n}\) is a sequence of iid random variables with mean \(0\) and variance \(1\), then, for \(0\leq t\leq 1\), the continuous time process1

Footnote 1: The intuition here is, for \(k=\llbracket nt\rrbracket\) and fixed \(t\), the central limit theorem has \(\sqrt{t}\frac{1}{\sqrt{k}}\sum_{j=1}^{k}\xi_{j}\sim\text{AN}(0,t)\) with \(n\to\infty\).

\[S_{n}(t)=\frac{1}{\sqrt{n}}\sum_{j=1}^{\llbracket nt\rrbracket}\xi_{j} \xrightarrow{d}\ W(t), \tag{5.30}\]

as \(n\to\infty\), where \(\llbracket\ \rrbracket\) is the greatest integer function and \(W(t)\) is standard Brownian motion on \([0,1]\). Note the under the null hypothesis, \(x_{s}=w_{1}+\cdots+w_{s}\sim\text{N}(0,s\sigma_{w}^{2})\), and based on (5.30), we have \(\frac{x_{s}}{\sigma_{w}\sqrt{n}}\to_{d}W(s)\). From this fact, we can show that (\(n\to\infty\))

\[\sum_{t=1}^{n}\Bigg{(}\frac{x_{t-1}}{\sigma_{w}\sqrt{n}}\Bigg{)}^{2}\frac{1}{n }\xrightarrow{d}\int_{0}^{1}W^{2}(t)\,dt\,. \tag{5.31}\]The denominator in (5.28) is off from the left side of (5.31) by a factor of \(n^{-1}\), and we adjust accordingly to finally obtain (\(n\to\infty\)),

\[n(\hat{\phi}-1)=\frac{\frac{1}{n\sigma_{w}^{2}}\sum_{t=1}^{n}w_{t}x_{t-1}}{\frac {1}{n^{2}\sigma_{w}^{2}}\sum_{t=1}^{n}x_{t-1}^{2}}\xrightarrow{d}\frac{\frac{1} {2}\left(\chi_{1}^{2}-1\right)}{\int_{0}^{1}W^{2}(t)\,dt}. \tag{5.32}\]

The test statistic \(n(\hat{\phi}-1)\) is known as the unit root or Dickey-Fuller (DF) statistic (see Fuller [65] or 1996), although the actual DF test statistic is normalized a little differently. Related derivations were discussed in Rao [162, Correction 1980] and in Evans and Savin [59]. Because the distribution of the test statistic does not have a closed form, quantiles of the distribution must be computed by numerical approximation or by simulation. The R package tseries provides this test along with more general tests that we mention briefly.

Toward a more general model, we note that the DF test was established by noting that if \(x_{t}=\phi x_{t-1}+w_{t}\), then \(\nabla x_{t}=(\phi-1)x_{t-1}+w_{t}=\gamma x_{t-1}+w_{t}\), and one could test \(H_{0}\) : \(\gamma=0\) by regressing \(\nabla x_{t}\) on \(x_{t-1}\). They formed a Wald statistic and derived its limiting distribution [the previous derivation based on Brownian motion is due to Phillips [154]]. The test was extended to accommodate AR(\(p\)) models, \(x_{t}=\sum_{j=1}^{p}\phi_{j}x_{t-j}+w_{t}\), as follows. Subtract \(x_{t-1}\) from both sides to obtain

\[\nabla x_{t}=\gamma x_{t-1}+\sum_{j=1}^{p-1}\psi_{j}\nabla x_{t-j}+w_{t}, \tag{5.33}\]

where \(\gamma=\sum_{j=1}^{p}\phi_{j}-1\) and \(\psi_{j}=-\sum_{i=j}^{p}\phi_{i}\) for \(j=2,\ldots,p\). For a quick check of (5.33) when \(p=2\), note that \(x_{t}=(\phi_{1}+\phi_{2})x_{t-1}-\phi_{2}(x_{t-1}-x_{t-2})+w_{t}\); now subtract \(x_{t-1}\) from both sides. To test the hypothesis that the process has a unit root at 1 (i.e., the AR polynomial \(\phi(z)=0\) when \(z=1\)), we can test \(H_{0}\) : \(\gamma=0\) by estimating \(\gamma\) in the regression of \(\nabla x_{t}\) on \(x_{t-1}\), \(\nabla x_{t-1}\), \(\ldots\), \(\nabla x_{t-p+1}\), and forming a Wald test based on \(t_{\gamma}=\hat{\gamma}/\text{se}(\hat{\gamma})\). This test leads to the so-called augmented Dickey-Fuller test (ADF). While the calculations for obtaining the asymptotic null distribution change, the basic ideas and machinery remain the same as in the simple case. The choice of \(p\) is crucial, and we will discuss some suggestions in the example. For ARMA(\(p,q\)) models, the ADF test can be used by assuming \(p\) is large enough to capture the essential correlation structure; another alternative is the Phillips-Perron (PP) test, which differs from the ADF tests mainly in how they deal with serial correlation and heteroskedasticity in the errors.

One can extend the model to include a constant, or even non-stochastic trend. For example, consider the model

\[x_{t}=\beta_{0}+\beta_{1}t+\phi x_{t-1}+w_{t}.\]

If we assume \(\beta_{1}=0\), then under the null hypothesis, \(\phi=1\), the process is a random walk with drift \(\beta_{0}\). Under the alternate hypothesis, the process is a causal AR(1) with mean \(\mu_{x}=\beta_{0}(1-\phi)\). If we cannot assume \(\beta_{1}=0\), then the interest here is testing the null that \((\beta_{1},\phi)=(0,1)\), simultaneously, versus the alternative that \(\beta_{1}\neq 0\) and \(|\phi|<1\). In this case, the null hypothesis is that the process is a random walk with drift, versus the alternative hypothesis that the process is trend stationary such as might be considered for the chicken price series in Example 2.1.

**Example 5.3**: **Testing Unit Roots in the Glacial Varve Series**

In this example we use the R package tseries to test the null hypothesis that the log of the glacial varve series has a unit root, versus the alternate hypothesis that the process is stationary. We test the null hypothesis using the available DF, ADF and PP tests; note that in each case, the general regression equation incorporates a constant and a linear trend. In the ADF test, the default number of AR components included in the model, say \(k\), is \(\llbracket(n-1)^{\frac{1}{2}}\rrbracket\), which corresponds to the suggested upper bound on the rate at which the number of lags, \(k\), should be made to grow with the sample size for the general ARMA(\(p,q\)) setup. For the PP test, the default value of \(k\) is \(\llbracket.04n^{\frac{1}{2}}\rrbracket\).

library(tseries)  adf.test(log(varve), k=0) # DF test  Dickey-Fuller = -12.8572, Lag order = 0, p-value < 0.01  alternative hypothesis: stationary  adf.test(log(varve)) # ADF test  Dickey-Fuller = -3.5166, Lag order = 8, p-value = 0.04071  alternative hypothesis: stationary  pp.test(log(varve)) # PP test  Dickey-Fuller Z(alpha) = -304.5376,  Truncation lag parameter = 6, p-value < 0.01  alternative hypothesis: stationary In each test, we reject the null hypothesis that the logged varve series has a unit root. The conclusion of these tests supports the conclusion of the previous section that the logged varve series is long memory rather than integrated.

### 5.3 GARCH Models

Various problems such as option pricing in finance have motivated the study of the _volatility_, or variability, of a time series. ARMA models were used to model the conditional mean of a process when the conditional variance was constant. Using an AR(1) as an example, we assumed

\[\mathrm{E}(x_{t}\mid x_{t-1},x_{t-2},\dots)=\phi x_{t-1},\quad\mathrm{and}\quad \mathrm{var}(x_{t}\mid x_{t-1},x_{t-2},\dots)=\mathrm{var}(w_{t})=\sigma_{w}^{2}\,.\]

In many problems, however, the assumption of a constant conditional variance will be violated. Models such as the _autoregressive conditionally heteroscedastic_ or ARCH model, first introduced by Engle [57] were developed to model changes in volatility. These models were later extended to generalized ARCH, or GARCH models by Bollerslev [28].

In these problems, we are concerned with modeling the return or growth rate of a series. For example, if \(x_{t}\) is the value of an asset at time \(t\), then the return or relative gain, \(r_{t}\), of the asset at time \(t\) is

\[r_{t}=\frac{x_{t}-x_{t-1}}{x_{t-1}}. \tag{5.34}\]

Definition (5.34) implies that \(x_{t}=(1+r_{t})x_{t-1}\). Thus, based on the discussion in Sect. 3.7, if the return represents a small (in magnitude) percentage change then

\[\nabla\log(x_{t})\approx r_{t}. \tag{5.35}\]

Either value, \(\nabla\log(x_{t})\) or \((x_{t}-x_{t-1})/x_{t-1}\), will be called the _return_,2 and will be denoted by \(r_{t}\). An alternative to the GARCH model is the _stochastic volatility model_; we will discuss these models in Chap. 6 because they are state-space models.

Footnote 2: Recall from Footnote 2 that if \(r_{t}=(x_{t}-x_{t-1})/x_{t-1}\) is a small percentage, then \(\log(1+r_{t})\approx r_{t}\). It is easier to program \(\nabla\log x_{t}\), so this is often used instead of calculating \(r_{t}\) directly. Although it is a misnomer, \(\nabla\log x_{t}\) is often called the _log-return_; but the returns are nor being logged.

Typically, for financial series, the return \(r_{t}\), does not have a constant conditional variance, and highly volatile periods tend to be clustered together. In other words, there is a strong dependence of sudden bursts of variability in a return on the series own past. For example, Fig. 1.4 shows the daily returns of the Dow Jones Industrial Average (DJIA) from April 20, 2006 to April 20, 2016. In this case, as is typical, the return \(r_{t}\) is fairly stable, except for short-term bursts of high volatility.

The simplest ARCH model, the ARCH(1), models the return as

\[r_{t} =\sigma_{t}\epsilon_{t} \tag{5.36}\] \[\sigma_{t}^{2} =\alpha_{0}+\alpha_{1}r_{t-1}^{2}, \tag{5.37}\]

where \(\epsilon_{t}\) is standard Gaussian white noise, \(\epsilon_{t}\sim\text{iid N}(0,1)\). The normal assumption may be relaxed; we will discuss this later. As with ARMA models, we must impose some constraints on the model parameters to obtain desirable properties. An obvious constraint is that \(\alpha_{0}\), \(\alpha_{1}\geq 0\) because \(\sigma_{t}^{2}\) is a variance.

As we shall see, the ARCH(1) models return as a white noise process with non-constant conditional variance, and that conditional variance depends on the previous return. First, notice that the conditional distribution of \(r_{t}\) given \(r_{t-1}\) is Gaussian:

\[r_{t}\ \big{|}\ r_{t-1}\sim\text{N}(0,\alpha_{0}+\alpha_{1}r_{t-1}^{2}). \tag{5.38}\]

In addition, it is possible to write the ARCH(1) model as a non-Gaussian AR(1) model in the square of the returns \(r_{t}^{2}\). First, rewrite (5.36)-(5.37) as

\[r_{t}^{2} =\sigma_{t}^{2}\epsilon_{t}^{2}\] \[\alpha_{0}+\alpha_{1}r_{t-1}^{2} =\sigma_{t}^{2},\]and subtract the two equations to obtain

\[r_{t}^{2}-(\alpha_{0}+\alpha_{1}r_{t-1}^{2})=\sigma_{t}^{2}\epsilon_{t}^{2}-\sigma _{t}^{2}.\]

Now, write this equation as

\[r_{t}^{2}=\alpha_{0}+\alpha_{1}r_{t-1}^{2}+v_{t}, \tag{5.39}\]

where \(v_{t}=\sigma_{t}^{2}(\epsilon_{t}^{2}-1)\). Because \(\epsilon_{t}^{2}\) is the square of a N(0, 1) random variable, \(\epsilon_{t}^{2}-1\) is a shifted (to have mean-zero), \(\chi_{1}^{2}\) random variable.

To explore the properties of ARCH, we define \(\mathcal{R}_{s}=\{r_{s},r_{s-1},\dots\}\). Then, using (5.38), we immediately see that \(r_{t}\) has a zero mean:

\[\mathrm{E}(r_{t})=\mathrm{E}(r_{t}\ \big{|}\ \mathcal{R}_{t-1})=\mathrm{E} (r_{t}\ \big{|}\ r_{t-1})=0. \tag{5.40}\]

Because \(\mathrm{E}(r_{t}\ |\ \mathcal{R}_{t-1})=0\), the process \(r_{t}\) is said to be a _martingale difference_.

Because \(r_{t}\) is a martingale difference, it is also an uncorrelated sequence. For example, with \(h>0\),

\[\mathrm{cov}(r_{t+h},r_{t}) =\mathrm{E}(r_{t}r_{t+h})=\mathrm{E}(r_{t}r_{t+h}\ |\ \mathcal{R}_{t+h-1})\] \[=\mathrm{E}\left\{r_{t}\mathrm{E}(r_{t+h}\ |\ \mathcal{R}_{t+h-1})\right\}=0. \tag{5.41}\]

The last line of (5.41) follows because \(r_{t}\) belongs to the information set \(\mathcal{R}_{t+h-1}\) for \(h>0\), and, \(\mathrm{E}(r_{t+h}\ |\ \mathcal{R}_{t+h-1})=0\), as determined in (5.40).

An argument similar to (5.40) and (5.41) will establish the fact that the error process \(v_{t}\) in (5.39) is also a martingale difference and, consequently, an uncorrelated sequence. If the variance of \(v_{t}\) is finite and constant with respect to time, and \(0\leq\alpha_{1}<1\), then based on Property 3.1, (5.39) specifies a causal AR(1) process for \(r_{t}^{2}\). Therefore, \(\mathrm{E}(r_{t}^{2})\) and \(\mathrm{var}(r_{t}^{2})\) must be constant with respect to time \(t\). This, implies that

\[\mathrm{E}(r_{t}^{2})=\mathrm{var}(r_{t})=\frac{\alpha_{0}}{1-\alpha_{1}} \tag{5.42}\]

and, after some manipulations,

\[\mathrm{E}(r_{t}^{4})=\frac{3\alpha_{0}^{2}}{(1-\alpha_{1})^{2}}\frac{1-\alpha _{1}^{2}}{1-3\alpha_{1}^{2}}, \tag{5.43}\]

provided \(3\alpha_{1}^{2}<1\). Note that

\[\mathrm{var}(r_{t}^{2})=\mathrm{E}(r_{t}^{4})-\left[\mathrm{E}(r_{t}^{2}) \right]^{2},\]

which exists only if \(0<\alpha_{1}<1/\sqrt{3}\approx.58\). In addition, these results imply that the kurtosis, \(\kappa\), of \(r_{t}\) is

\[\kappa=\frac{\mathrm{E}(r_{t}^{4})}{[\mathrm{E}(r_{t}^{2})]^{2}}=3\frac{1- \alpha_{1}^{2}}{1-3\alpha_{1}^{2}}, \tag{5.44}\]

which is never smaller than 3, the kurtosis of the normal distribution. Thus, the marginal distribution of the returns, \(r_{t}\), is leptokurtic, or has "fat tails." Summarizing,if \(0\leq\alpha_{1}<1\), the process \(r_{t}\) itself is white noise and its unconditional distribution is symmetrically distributed around zero; this distribution is leptokurtic. If, in addition, \(3\alpha_{1}^{2}<1\), the square of the process, \(r_{t}^{2}\), follows a causal AR(1) model with ACF given by \(\rho_{y^{2}}(h)=\alpha_{1}^{h}\geq 0\), for all \(h>0\). If \(3\alpha_{1}\geq 1\), but \(\alpha_{1}<1\), it can be shown that \(r_{t}^{2}\) is strictly stationary with infinite variance (see Douc, et al. [53]).

Estimation of the parameters \(\alpha_{0}\) and \(\alpha_{1}\) of the ARCH(1) model is typically accomplished by conditional MLE. The conditional likelihood of the data \(r_{2},\ldots,r_{n}\) given \(r_{1}\), is given by

\[L(\alpha_{0},\alpha_{1}\bigm{|}r_{1})=\prod_{t=2}^{n}f_{\alpha_{0},\alpha_{1}} (r_{t}\bigm{|}r_{t-1}), \tag{5.45}\]

where the density \(f_{\alpha_{0},\alpha_{1}}(r_{t}\bigm{|}r_{t-1})\) is the normal density specified in (5.38). Hence, the criterion function to be minimized, \(l(\alpha_{0},\alpha_{1})\propto-\ln L(\alpha_{0},\alpha_{1}\bigm{|}r_{1})\) is given by

\[l(\alpha_{0},\alpha_{1})=\frac{1}{2}\sum_{t=2}^{n}\ln(\alpha_{0}+\alpha_{1}r_{ t-1}^{2})+\frac{1}{2}\sum_{t=2}^{n}\left(\frac{r_{t}^{2}}{\alpha_{0}+\alpha_{1} r_{t-1}^{2}}\right). \tag{5.46}\]

Estimation is accomplished by numerical methods, as described in Sect. 3.5. In this case, analytic expressions for the gradient vector, \(l^{(1)}(\alpha_{0},\alpha_{1})\), and Hessian matrix, \(l^{(2)}(\alpha_{0},\alpha_{1})\), as described in Example 3.30, can be obtained by straight-forward calculations. For example, the \(2\times 1\) gradient vector, \(l^{(1)}(\alpha_{0},\alpha_{1})\), is given by

\[\begin{pmatrix}\partial l/\partial\alpha_{0}\\ \partial l/\partial\alpha_{1}\end{pmatrix}=\sum_{t=2}^{n}\begin{pmatrix}1\\ r_{t-1}^{2}\end{pmatrix}\times\frac{\alpha_{0}+\alpha_{1}r_{t-1}^{2}-r_{t}^{2 }}{2\left(\alpha_{0}+\alpha_{1}r_{t-1}^{2}\right)^{2}}.\]

The calculation of the Hessian matrix is left as an exercise (Problem 5.8). The likelihood of the ARCH model tends to be flat unless \(n\) is very large. A discussion of this problem can be found in Shephard [177].

It is also possible to combine a regression or an ARMA model for the mean with an ARCH model for the errors. For example, a regression with ARCH(1) errors model would have the observations \(x_{t}\) as linear function of \(p\) regressors, \(z_{t}=(z_{t1},\ldots,z_{tP})^{\prime}\), and ARCH(1) noise \(y_{t}\), say,

\[x_{t}=\beta^{\prime}z_{t}+y_{t},\]

where \(y_{t}\) satisfies (5.36)-(5.37), but, in this case, is unobserved. Similarly, for example, an AR(1) model for data \(x_{t}\) exhibiting ARCH(1) errors would be

\[x_{t}=\phi_{0}+\phi_{1}x_{t-1}+y_{t}.\]

These types of models were explored by Weiss [208].

**Example 5.4**: **Analysis of U.S. GNP**

In Example 3.39, we fit an MA(2) model and an AR(1) model to the U.S. GNP series and we concluded that the residuals from both fits appeared to behave like a white noise process. In Example 3.43 we concluded that the AR(1) is probably the better model in this case. It has been suggested that the U.S. GNP series has ARCH errors, and in this example, we will investigate this claim. If the GNP noise term is ARCH, the squares of the residuals from the fit should behave like a non-Gaussian AR(1) process, as pointed out in (5.39). Figure 5.5 shows the ACF and PACF of the squared residuals it appears that there may be some dependence, albeit small, left in the residuals. The figure was generated in R as follows.

u = sarima(diff(log(gmp)), 1, 0, 0)

acf2(resid(uSfit)^2, 20)

We used the R package fGarch to fit an AR(1)-ARCH(1) model to the U.S. GNP returns with the following results. A partial output is shown; we note that garch(1,0) specifies an ARCH(1) in the code below (details later).

library(fGarch) summary(garchFit(-arma(1,0)+garch(1,0), diff(log(gmp)))) Estimate Std.Error t.value p.value mu 0.005 0.001 5.867 0.000 ar1 0.367 0.075 4.878 0.000 omega 0.000 0.000 8.135 0.000 alpha1 0.194 0.096 2.035 0.042 -- Standardised Residuals Tests: Statistic p-Value Jarque-Bera Test R Chi^2 9.118 0.010 Shapiro-Wilk Test R W 0.984 0.014 Ljung-Box Test R Q(20) 23.414 0.269 Ljung-Box Test R^2 Q(20) 37.743 0.010 Note that the p-values given in the estimation paragraph are two-sided, so they should be halved when considering the ARCH parameters. In this example, we obtain \(\hat{\phi}_{0}=.005\) (called mu in the output) and \(\hat{\phi}_{1}=.367\) (called ar1) for the AR(1) parameter estimates; in Example 3.39 the values were.005 and.347, respectively. The ARCH(1) parameter estimates are \(\hat{\alpha}_{0}=0\) (called omega) for the constant and \(\hat{\alpha}_{1}=.194\), which is significant with a p-value of about.02. There are a number

Figure 5.5: ACF and PACF of the squares of the residuals from the AR(1) fit on U.S. GNPof tests that are performed on the residuals [R] or the squared residuals [R*2]. For example, the Jarque-Bera statistic tests the residuals of the fit for normality based on the observed skewness and kurtosis, and it appears that the residuals have some non-normal skewness and kurtosis. The Shapiro-Wilk statistic tests the residuals of the fit for normality based on the empirical order statistics. The other tests, primarily based on the Q-statistic, are used on the residuals and their squares.

The ARCH(1) model can be extended to the general ARCH(\(p\)) model in an obvious way. That is, (5.36), \(r_{t}=\sigma_{t}\epsilon_{t}\), is retained, but (5.37) is extended to

\[\sigma_{t}^{2}=\alpha_{0}+\alpha_{1}r_{t-1}^{2}+\cdots+\alpha_{p}r_{t-p}^{2}. \tag{5.47}\]

Estimation for ARCH(\(p\)) also follows in an obvious way from the discussion of estimation for ARCH(1) models. That is, the conditional likelihood of the data \(r_{p+1},\ldots,r_{n}\) given \(r_{1},\ldots,r_{p}\), is given by

\[L(\alpha\ \big{|}\ r_{1},\ldots,r_{p})=\prod_{t=p+1}^{n}f_{\alpha}(r_{t}\ \big{|}\ r_{t-1},\ldots,r_{t-p}), \tag{5.48}\]

where \(\alpha=(\alpha_{0},\alpha_{1},\ldots,\alpha_{p})\) and, under the assumption of normality, the conditional densities \(f_{\alpha}(\cdot|\cdot)\) in (5.48) are, for \(t>p\), given by

\[r_{t}\ \big{|}\ r_{t-1},\ldots,r_{t-p}\sim\mathrm{N}(0,\alpha_{0}+\alpha_{1}r_{t -1}^{2}+\cdots+\alpha_{p}r_{t-p}^{2}).\]

Another extension of ARCH is the generalized ARCH or GARCH model developed by Bollerslev [28]. For example, a GARCH(1, 1) model retains (5.36), \(r_{t}=\sigma_{t}\epsilon_{t}\), but extends (5.37) as follows:

\[\sigma_{t}^{2}=\alpha_{0}+\alpha_{1}r_{t-1}^{2}+\beta_{1}\sigma_{t-1}^{2}. \tag{5.49}\]

Under the condition that \(\alpha_{1}+\beta_{1}<1\), using similar manipulations as in (5.39), the GARCH(1, 1) model, (5.36) and (5.49), admits a non-Gaussian ARMA(1, 1) model for the squared process

\[r_{t}^{2}=\alpha_{0}+(\alpha_{1}+\beta_{1})r_{t-1}^{2}+v_{t}-\beta_{1}v_{t-1}, \tag{5.50}\]

where \(v_{t}\) is as defined in (5.39). Representation (5.50) follows by writing (5.36) as

\[r_{t}^{2}-\sigma_{t}^{2} =\sigma_{t}^{2}(\epsilon_{t}^{2}-1)\] \[\beta_{1}(r_{t-1}^{2}-\sigma_{t-1}^{2}) =\beta_{1}\sigma_{t-1}^{2}(\epsilon_{t-1}^{2}-1),\]

subtracting the second equation from the first, and using the fact that, from (5.49), \(\sigma_{t}^{2}-\beta_{1}\sigma_{t-1}^{2}=\alpha_{0}+\alpha_{1}r_{t-1}^{2}\), on the left-hand side of the result. The GARCH(\(p,q\)) model retains (5.36) and extends (5.49) to

\[\sigma_{t}^{2}=\alpha_{0}+\sum_{j=1}^{p}\alpha_{j}r_{t-j}^{2}+\sum_{j=1}^{q} \beta_{j}\sigma_{t-j}^{2}. \tag{5.51}\]Conditional maximum likelihood estimation of the GARCH(\(p,q\)) model parameters is similar to the ARCH(\(p\)) case, wherein the conditional likelihood, (5.48), is the product of N(\(0,\sigma_{t}^{2}\)) densities with \(\sigma_{t}^{2}\) given by (5.51) and where the conditioning is on the first max(\(p,q\)) observations, with \(\sigma_{1}^{2}=\cdots=\sigma_{q}^{2}=0\). Once the parameter estimates are obtained, the model can be used to obtain _one-step-ahead forecasts_ of the volatility, say \(\hat{\sigma}_{t+1}^{2}\), given by

\[\hat{\sigma}_{t+1}^{2}=\hat{\alpha}_{0}+\sum_{j=1}^{p}\hat{\alpha}_{j}r_{t+1-j} ^{2}+\sum_{j=1}^{q}\hat{\beta}_{j}\hat{\sigma}_{t+1-j}^{2}. \tag{5.52}\]

We explore these concepts in the following example.

**Example 5.5**: ARCH Analysis of the DJIA Returns

As previously mentioned, the daily returns of the DJIA shown in Fig. 1.4 exhibit classic GARCH features. In addition, there is some low level autocorrelation in the series itself, and to include this behavior, we used the R fGarch package to fit an AR(1)-GARCH(\(1,1\)) model to the series using t errors:

library(xts) djiar = diff(log(djiaClose))[-1] acf2(djiar) # exhibits some autocorrelation (not shown) acf2(djiar^2) # oozes autocorrelation (not shown) library(fGarch) summary(djia.g <- garchFit(-arma(1,0)+garch(1,1), data=djiar, cond.dist='std')) plot(djia.g) # to see all plot options  Estimate Std.Error t.value p.value  mu 8.585e-04 1.470e-04 5.842 5.16e-09  ar1 -5.531e-02 2.023e-02 -2.735 0.006239  omega 1.610e-06 4.459e-07 3.611 0.000305  alpha1 1.244e-01 1.660e-02 7.497 6.55e-14  beta1 8.700e-01 1.526e-02 57.022 < 2e-16  shape 5.979e+00 7.917e-01 7.552 4.31e-14 ---  Standardised Residuals Tests:  Statistic p-Value  Ljung-Box Test R Q(10) 16.81507 0.0785575  Ljung-Box Test R^2 Q(10) 15.39137 0.1184312 To explore the GARCH predictions of volatility, we calculated and plotted part of the data surrounding the financial crises of 2008 along with the one-step-ahead predictions of the corresponding volatility, \(\sigma_{t}^{2}\) as a solid line in Fig. 5.6.

Another model that we mention briefly is the _asymmetric power ARCH_ model. The model retains (5.36), \(r_{t}=\sigma_{t}\epsilon_{t}\), but the conditional variance is modeled as

\[\sigma_{t}^{\delta}=\alpha_{0}+\sum_{j=1}^{p}\alpha_{j}(|r_{t-j}|-\gamma_{j}r_{ t-j})^{\delta}+\sum_{j=1}^{q}\beta_{j}\sigma_{t-j}^{\delta}\,. \tag{5.53}\]Note that the model is GARCH when \(\delta=2\) and \(\gamma_{j}=0\), for \(j\in\{1,\ldots,p\}\). The parameters \(\gamma_{j}\) (\(|\gamma_{j}|\leq 1\)) are the _leverage_ parameters, which are a measure of asymmetry, and \(\delta>0\) is the parameter for the power term. A positive [negative] value of \(\gamma_{j}\)'s means that past negative [positive] shocks have a deeper impact on current conditional volatility than past positive [negative] shocks. This model couples the flexibility of a varying exponent with the asymmetry coefficient to take the _leverage effect_ into account. Further, to guarantee that \(\sigma_{t}>0\), we assume that \(\alpha_{0}>0\), \(\alpha_{j}\geq 0\) with at least one \(\alpha_{j}>0\), and \(\beta_{j}\geq 0\).

We continue the analysis of the DJIA returns in the following example.

**Example 5.6**: **APARCH Analysis of the DJIA Returns**

The R package fGarch was used to fit an AR-APARCH model to the DJIA returns discussed in Example 5.5. As in the previous example, we include an AR(1) in the model to account for the conditional mean. In this case, we may think of the model as \(r_{t}=\mu_{t}+y_{t}\) where \(\mu_{t}\) is an AR(1), and \(y_{t}\) is APARCH noise with conditional variance modeled as (5.53) with t-errors. A partial output of the analysis is given below. We do not include displays, but we show how to obtain them. The predicted volatility is, of course, different than the values shown in Fig. 5.6, but appear similar when graphed.

library(xts) library(fGarch) summary(djia.ap <- garchFit(-arma(1,0)+aparch(1,1), data=djiar,  cond.dist='std')) plot(djia.ap) # to see all plot options (none shown) Estimate Std. Error t value p.value mu 5.234e-04 1.525e-04 3.432 0.000598 ar1 -4.818e-02 1.934e-02 -2.491 0.012727 omega 1.798e-04 3.443e-05 5.222 1.77e-07

Figure 5.6: GARCH one-step-ahead predictions of the DJIA volatility, \(\hat{\sigma}_{T}\), superimposed on part of the data including the financial crisis of 2008

alpha1 9.809e-02 1.030e-02 9.525 < 2e-16  gamma1 1.009e+00 1.045e-02 95.731 < 2e-16  beta1 8.945e-01 1.049e-02 85.280 < 2e-16  delta 1.070e+00 1.350e-01 7.928 2.22e-15  shape 7.286e+00 1.123e+00 6.489 8.61e-11  ---  Standardised Residuals Tests:  Statistic p-Value  Ljung-Box Test R Q(10) 15.71403 0.108116  Ljung-Box Test R^2 Q(10) 16.87473 0.077182

In most applications, the distribution of the noise, \(\epsilon_{t}\) in (5.36), is rarely normal. The R package fGarch allows for various distributions to be fit to the data; see the help file for information. Some drawbacks of GARCH and related models are as follows. (i) The GARCH model assumes positive and negative returns have the same effect because volatility depends on squared returns; the asymmetric models help alleviate this problem. (ii) These models are often restrictive because of the tight constraints on the model parameters (e.g., for an ARCH(1), \(0\leq\alpha_{1}^{2}<\frac{1}{3}\)). (iii) The likelihood is flat unless \(n\) is very large. (iv) The models tend to overpredict volatility because they respond slowly to large isolated returns.

Various extensions to the original model have been proposed to overcome some of the shortcomings we have just mentioned. For example, we have already discussed the fact that fGarch allows for asymmetric return dynamics. In the case of persistence in volatility, the integrated GARCH (IGARCH) model may be used. Recall (5.50) where we showed the GARCH(1, 1) model can be written as

\[r_{t}^{2}=\alpha_{0}+(\alpha_{1}+\beta_{1})r_{t-1}^{2}+v_{t}-\beta_{1}v_{t-1}\]

and \(r_{t}^{2}\) is stationary if \(\alpha_{1}+\beta_{1}<1\). The IGARCH model sets \(\alpha_{1}+\beta_{1}=1\), in which case the IGARCH(1, 1) model is

\[r_{t}=\sigma_{t}\epsilon_{t}\quad\text{and}\quad\sigma_{t}^{2}=\alpha_{0}+(1- \beta_{1})r_{t-1}^{2}+\beta_{1}\sigma_{t-1}^{2}.\]

There are many different extensions to the basic ARCH model that were developed to handle the various situations noticed in practice. Interested readers might find the general discussions in Engle et al. [58] and Shephard [177] worthwhile reading. Also, Gourieroux [78] gives a detailed presentation of ARCH and related models with financial applications and contains an extensive bibliography. Two excellent texts on financial time series analysis are Chan [40] and Tsay [204].

Finally, we briefly discuss _stochastic volatility models_; a detailed treatment of these models is given in Chap. 6. The volatility component, \(\sigma_{t}^{2}\), in GARCH and related models are conditionally nonstochastic. For example, in the ARCH(1) model, any time the previous return is valued at, say \(c\), i.e., \(r_{t-1}=c\), it must be the case that \(\sigma_{t}^{2}=\alpha_{0}+\alpha_{1}c^{2}\). This assumption seems a bit unrealistic. The stochastic volatility model adds a stochastic component to the volatility in the following way. In the GARCH model, a return, say \(r_{t}\), is

\[r_{t}=\sigma_{t}\epsilon_{t}\quad\Rightarrow\quad\log r_{t}^{2}=\log\sigma_{t} ^{2}+\log\epsilon_{t}^{2}. \tag{5.54}\]Thus, the observations \(\log r_{t}^{2}\) are generated by two components, the unobserved volatility, \(\log\sigma_{t}^{2}\), and the unobserved noise, \(\log\epsilon_{t}^{2}\). While, for example, GARCH(1, 1) models volatility without error, \(\sigma_{t+1}^{2}=\alpha_{0}+\alpha_{1}r_{t}^{2}+\beta_{1}\sigma_{t}^{2}\), the basic stochastic volatility model assumes the logged latent variable is an autoregressive process,

\[\log\sigma_{t+1}^{2}=\phi_{0}+\phi_{1}\log\sigma_{t}^{2}+w_{t} \tag{5.55}\]

where \(w_{t}\sim\text{iid N}(0,\sigma_{w}^{2})\). The introduction of the noise term \(w_{t}\) makes the latent volatility process stochastic. Together (5.54) and (5.55) comprise the stochastic volatility model. Given \(n\) observations, the goals are to estimate the parameters \(\phi_{0}\), \(\phi_{1}\) and \(\sigma_{w}^{2}\), and then predict future volatility. Details are provided in Sect. 6.11.

### Threshold Models

In Sect. 3.4 we discussed the fact that, for a stationary time series, best linear prediction forward in time is the same as best linear prediction backward in time. This result followed from the fact that the variance-covariance matrix of \(x_{1:n}=\{x_{1},x_{2},\ldots,x_{n}\}\), say, \(\Gamma=\{\gamma(i-j)\}_{i,j=1}^{n}\), is the same as the variance-covariance matrix of \(x_{n:1}=\{x_{n},x_{n-1},\ldots,x_{1}\}\). In addition, if the process is Gaussian, the distributions of \(x_{1:n}\) and \(x_{n:1}\) are identical. In this case, a time plot of \(x_{1:n}\) (that is, the data plotted forward in time) should look similar to a time plot of \(x_{n:1}\) (that is, the data plotted backward in time).

There are, however, many series that do not fit into this category. For example, Fig. 5.7 shows a plot of monthly pneumonia and influenza deaths per 10,000 in the U.S. for 11 years, 1968 to 1978. Typically, the number of deaths tends to increase faster than it decreases (\(\top\)), especially during epidemics. Thus, if the data were plotted backward in time, that series would tend to increase slower than it decreases. Also, if monthly pneumonia and influenza deaths followed a linear Gaussian process, we would not expect to see such large bursts of positive and negative changes that occur

Figure 5.7: U.S. monthly pneumonia and influenza deaths per 10,000

periodically in this series. Moreover, although the number of deaths is typically largest during the winter months, the data are not perfectly seasonal. That is, although the peak of the series often occurs in January, in other years, the peak occurs in February or in March. Hence, seasonal ARMA models would not capture this behavior.

Many approaches to modeling nonlinear series exist that could be used (see Priestley [159]); here, we focus on the class of _threshold models_ (TARMA) presented in Tong [202, 203]. The basic idea of these models is that of fitting local linear ARMA models, and their appeal is that we can use the intuition from fitting global linear ARMA models. For example, a \(k\)-regimes self-exciting threshold (SETARMA) model has the form

\[x_{t}=\begin{cases}\phi_{0}^{(1)}+\sum_{i=1}^{p_{1}}\phi_{i}^{(1)}x_{t-i}+w_{t }^{(1)}+\sum_{j=1}^{q_{1}}\theta_{j}^{(1)}w_{t-j}^{(1)}&\text{if }x_{t-d}\leq r_{1}\,,\\ \phi_{0}^{(2)}+\sum_{i=1}^{p_{2}}\phi_{i}^{(2)}x_{t-i}+w_{t}^{(2)}+\sum_{j=1}^{ q_{2}}\theta_{j}^{(2)}w_{t-j}^{(2)}&\text{if }r_{1}<x_{t-d}\leq r_{2}\,,\\ \vdots&\vdots\\ \phi_{0}^{(k)}+\sum_{i=1}^{p_{k}}\phi_{i}^{(k)}x_{t-i}+w_{t}^{(k)}+\sum_{j=1}^{ q_{k}}\theta_{j}^{(k)}w_{t-j}^{(k)}&\text{if }r_{k-1}<x_{t-d}\,,\end{cases} \tag{5.56}\]

where \(w_{t}^{(j)}\sim\text{iid N}(0,\sigma_{j}^{2})\), for \(j=1,\ldots,k\), the positive integer \(d\) is a specified em delay, and \(-\infty<r_{1}<\cdots<r_{k-1}<\infty\) is a partition of \(\mathbb{R}\).

These models allow for changes in the ARMA coefficients over time, and those changes are determined by comparing previous values (back-shifted by a time lag equal to \(d\)) to fixed threshold values. Each different ARMA model is referred to as a _regime_. In the definition above, the values \((p_{j},q_{j})\) of the order of ARMA models can differ in each regime, although in many applications, they are equal. Stationarity and invertibility are obvious concerns when fitting time series models. For the threshold time series models, such as TAR, TMA and TARMA models, however, the stationary and invertible conditions in the literature are less well-known in general and often restricted models of order one.

The model can be generalized to include the possibility that the regimes depend on a collection of the past values of the process, or that the regimes depend on an exogenous variable (in which case the model is not self-exciting) such in predator-prey cases. For example, Canadian lynx have been thoroughly studied (see the R data set lynx) and the series is typically used to demonstrate the fitting of threshold models. The lynx prey varies from small rodents to deer, with the Snowshoe Hare being its overwhelmingly favored prey. In fact, in certain areas the lynx is so closely tied to the Snowshoe that its population rises and falls with that of the hare, even though other food sources may be abundant. In this case, it seems reasonable to replace \(x_{t-d}\) in (5.56) with say \(y_{t-d}\), where \(y_{t}\) is the size of the Snowshoe Hare population.

The popularity of TAR models is due to their being relatively simple to specify, estimate, and interpret as compared to many other nonlinear time series models. In addition, despite its apparent simplicity, the class of TAR models can reproduce many nonlinear phenomena. In the following example, we use these methods to fit a threshold model to monthly pneumonia and influenza deaths series previously mentioned.

**Example 5.7**: **Threshold Modeling of the Influenza Series**

As previously discussed, examination of Fig. 5.7 leads us to believe that the monthly pneumonia and influenza deaths time series, say \(\text{flu}_{t}\), is not linear. It is also evident from Fig. 5.7 that there is a slight negative trend in the data. We have found that the most convenient way to fit a threshold model to these data, while removing the trend, is to work with the first differences. The differenced data, say

\[x_{t}=\text{flu}_{t}-\text{flu}_{t-1}\]

is exhibited in Fig. 5.9 as points (+) representing the observations.

The nonlinearity of the data is more pronounced in the plot of the first differences, \(x_{t}\). Clearly \(x_{t}\) slowly rises for some months and, then, sometime in the winter, has a possibility of jumping to a large number once \(x_{t}\) exceeds about.05. If the process does make a large jump, then a subsequent significant decrease occurs in \(x_{t}\). Another telling graphic is the lag plot of \(x_{t}\) versus \(x_{t-1}\) shown in Fig. 5.8, which suggests the possibility of two linear regimes based on whether or not \(x_{t-1}\) exceeds.05.

As an initial analysis, we fit the following threshold model

\[\begin{split} x_{t}&=\alpha^{(1)}+\sum_{j=1}^{p} \phi_{j}^{(1)}x_{t-j}+w_{t}^{(1)},\qquad x_{t-1}<.05\,;\\ x_{t}&=\alpha^{(2)}+\sum_{j=1}^{p}\phi_{j}^{(2)}x_{ t-j}+w_{t}^{(2)},\qquad x_{t-1}\geq.05\,,\end{split} \tag{5.57}\]

with \(p=6\), assuming this would be larger than necessary. Model (5.57) is easy to fit using two linear regression runs, one when \(x_{t-1}<.05\) and the other when \(x_{t-1}\geq.05\). Details are provided in the R code at the end of this example.

An order \(p=4\) was finally selected and the fit was

\[\hat{x}_{t} = 0+.51_{(.08)}x_{t-1}-.20_{(.06)}x_{t-2}+.12_{(.05)}x_{t-3}\] \[-.11_{(.05)}x_{t-4}+\hat{w}_{t}^{(1)},\qquad\text{for}\;\;x_{t-1}<.05\;;\] \[\hat{x}_{t} =.40-.75_{(.17)}x_{t-1}-1.03_{(.21)}x_{t-2}-2.05_{(1.05)}x_{t-3}\] \[-6.71_{(1.25)}x_{t-4}+\hat{w}_{t}^{(2)},\quad\text{for}\;\;x_{t-1} \geq.05\,,\]

where \(\hat{\sigma}_{1}=.05\) and \(\hat{\sigma}_{2}=.07\). The threshold of.05 was exceeded 17 times.

Using the final model, one-month-ahead predictions can be made, and these are shown in Fig. 5.9 as a line. The model does extremely well at predicting a flu epidemic; the peak at 1976, however, was missed by this model. When we fit a model with a smaller threshold of.04, flu epidemics were somewhat underestimated, but the flu epidemic in the eighth year was predicted one month early. We chose the model with a threshold of.05 because the residual diagnostics showed no obvious departure from the model assumption (except for one outlier at 1976); the model with a threshold of.04 still had some correlation left in the residuals and there was more than one outlier. Finally, prediction beyond one-month-ahead for this model is complicated, but some approximate techniques exist (see Tong [202]). The following commands can be used to perform this analysis in R.

Plot data with month initials as points plot(flu, type="C") Months = c("J","F","H","A","M","J","J","A","S","O","W","D") points(flu, pch=Months, cex=.8, font=2)

Start analysis dflu = diff(flu) lag1.plot(dflu, corr=FALSE) # scatterplot with lowess fit thrsh =.05 # threshold Z = ts.intersect(dflu, lag(dflu,-1), lag(dflu,-2), lag(dflu,-3), lag(dflu,-4) ) ind1 = ifelse(Z[,2] < thrsh, 1, NA) # indicator < thrsh

Figure 5.9: First differenced U.S. monthly pneumonia and influenza deaths (+); one-month-ahead predictions (_solid line_) with \(\pm 2\) prediction error bounds. The _horizontal line_ is the threshold

ind2 = ifelse(Z[,2] < thrsh, NA, 1) # indicator >= thrsh X1 = Z[,1]*ind1 X2 = Z[,1]*ind2 summary(fit1 <- lm(X1- Z[,2:5]) ) # case 1 summary(fit2 <- lm(X2- Z[,2:5]) ) # case 2 D = cbind(rep(1, nrow(Z)), Z[,2:5]) # design matrix p1 = D %%C coef(fit1) # get predictions p2 = D %% coef(fit2) prd = ifelse(Z[,2] < thrsh, p1, p2) plot(dflu, ylim-c(-.5,.5), type-'p', pch=3) lines(prd) prdel = sqrt(sum(resid(fit1)^2)/df.residual(fit1) ) prdEd2 = sqrt(sum(resid(fit2)^2)/df.residual(fit2) ) prdEd = ifelse(Z[,2] < thrsh, prdEd, prde2) tx = time(dflu)[-(1:4)] xx = c(tx, rev(tx)) yy = c(prd-2*prde, rev(prd+2*prde)) polygon(xx, yy, border=8, col=gray(.6, alpha=.25) ) abline(h=.05, col=4, lty=6) Finally, we note that there is an R package called tsDyn that can be used to fit these models; we assume dflu already exits. library(tsDyn) # load package - install it if you don't have it # vignette("tsDyn") # for package details (u = setar(dflu, m=4, thDelay=0, th-.05)) # fit model and view results (u = setar(dflu, m=4, thDelay=0)) # let program fit threshold (=.036) BIC(u); AIC(u) # if you want to try other models; m=3 works well too plot(u) # graphics -?plot.setar for information The threshold found here is.036, which includes a few more observations than using.04, but suffers from the same drawbacks previously noted.

### 5.5 Lagged Regression and Transfer Function Modeling

In Sect. 4.8, we considered lagged regression in a frequency domain approach based on coherency. For example, consider the SOI and Recruitment series that were analyzed in Example 4.24; the series are displayed in Fig. 1.5. In that example, the interest was in predicting the output Recruitment series, say, \(y_{t}\), from the input SOI, say \(x_{t}\).

We considered the lagged regression model

\[y_{t}=\sum_{j=0}^{\infty}\alpha_{j}x_{t-j}+\eta_{t}=\alpha(B)x_{t}+\eta_{t}, \tag{5.58}\]

where \(\sum_{j}|\alpha_{j}|<\infty\). We assume the input process \(x_{t}\) and noise process \(\eta_{t}\) in (5.58) are both stationary and mutually independent. The coefficients \(\alpha_{0},\alpha_{1},\ldots\) describe the weights assigned to past values of \(x_{t}\) used in predicting \(y_{t}\) and we have used the notation

\[\alpha(B)=\sum_{j=0}^{\infty}\alpha_{j}B^{j}. \tag{5.59}\]In the Box and Jenkins [30] formulation, we assign ARIMA models, say, ARIMA(\(p,d,q\)) and ARIMA(\(p_{\eta_{\tau}},d_{\eta},q_{\eta}\)), to the series \(x_{t}\) and \(\eta_{t}\), respectively. In Sect. 4.8, we assumed the noise, \(\eta_{t}\), was white. The components of (5.58) in back-shift notation, for the case of simple ARMA(\(p,q\)) modeling of the input and noise, would have the representation

\[\phi(B)x_{t}=\theta(B)w_{t} \tag{5.60}\]

and

\[\phi_{\eta}(B)\eta_{t}=\theta_{\eta}(B)z_{t}, \tag{5.61}\]

where \(w_{t}\) and \(z_{t}\) are independent white noise processes with variances \(\sigma_{w}^{2}\) and \(\sigma_{z}^{2}\), respectively. Box and Jenkins [30] proposed that systematic patterns often observed in the coefficients \(\alpha_{j}\), for \(j=1,2,\dots\), could often be expressed as a ratio of polynomials involving a small number of coefficients, along with a specified delay, \(d\), so

\[\alpha(B)=\frac{\delta(B)B^{d}}{\omega(B)}, \tag{5.62}\]

where

\[\omega(B)=1-\omega_{1}B-\omega_{2}B^{2}-\dots-\omega_{r}B^{r} \tag{5.63}\]

and

\[\delta(B)=\delta_{0}+\delta_{1}B+\dots+\delta_{s}B^{s} \tag{5.64}\]

are the indicated operators; in this section, we find it convenient to represent the inverse of an operator, say, \(\omega(B)^{-1}\), as \(1/\omega(B)\).

Determining a parsimonious model involving a simple form for \(\alpha(B)\) and estimating all of the parameters in the above model are the main tasks in the transfer function methodology. Because of the large number of parameters, it is necessary to develop a sequential methodology. Suppose we focus first on finding the ARIMA model for the input \(x_{t}\) and apply this operator to both sides of (5.58), obtaining the new model

\[\tilde{y}_{t}=\frac{\phi(B)}{\theta(B)}y_{t}=\alpha(B)\frac{\phi(B)}{\theta(B) }x_{t}+\frac{\phi(B)}{\theta(B)}\eta_{t}=\alpha(B)w_{t}+\tilde{\eta}_{t},\]

where \(w_{t}\) and the transformed noise \(\tilde{\eta}_{t}\) are independent.

The series \(w_{t}\) is a _prewhitened_ version of the input series, and its cross-correlation with the transformed output series \(\tilde{y}_{t}\) will be just

\[\gamma_{\tilde{y}w}(h)=\mathrm{E}[\tilde{y}_{t+h}w_{t}]=\mathrm{E}\left[\sum_{ j=0}^{\infty}\alpha_{j}w_{t+h-j}w_{t}\right]=\sigma_{w}^{2}\alpha_{h}, \tag{5.65}\]

because the autocovariance function of white noise will be zero except when \(j=h\) in (5.65). Hence, by computing the cross-correlation between the prewhitened input series and the transformed output series should yield a rough estimate of the behavior of \(\alpha(B)\).

**Example 5.8**: **Relating the Prewhitened SOI to the Transformed Recruitment Series**

We give a simple example of the suggested procedure for the SOI and the Recruitment series. Figure 5.10 shows the sample ACF and PACF of the detrended SOI, and it is clear, from the PACF, that an autoregressive series with \(p=1\) will do a reasonable job. Fitting the series gave \(\hat{\phi}=.588\) with \(\hat{\sigma}_{w}^{2}=.092\), and we applied the operator \((1-.588B)\) to both \(x_{t}\) and \(y_{t}\) and computed the cross-correlation function, which is shown in Fig. 5.11. Noting the apparent shift of \(d=5\) months and the decrease thereafter, it seems plausible to hypothesize a model of the form

\[\alpha(B)=\delta_{0}B^{5}(1+\omega_{1}B+\omega_{1}^{2}B^{2}+\cdots)=\frac{\delta _{0}B^{5}}{1-\omega_{1}B}\]

for the transfer function. In this case, we would expect \(\omega_{1}\) to be negative. The following R code was used for this example.

``` soi.d=resid(lm(soi-time(soi),na.action=NULL))#detrendedSOI acf2(soi.d) fit=arima(soi.d,order=c(1,0,0)) ar1=as.numeric(coef(fit)[1])#=0.5875 soi.pw=resid(fit) rec.fil=filter(rec,filter=c(1,-ar1),sides=l) ccf(soi.pw,rec.fil,ylab="CCF",na.action=na.omit,panel.first=grid()) In the code above, soi.pw is the prewhitened detrended SOI series, and rec.fil is the filtered Recruitment series.

In some cases, we may postulate the form of the separate components \(\delta(B)\) and \(\omega(B)\), so we might write the equation

\[y_{t}=\frac{\delta(B)B^{d}}{\omega(B)}x_{t}+\eta_{t}\]

Figure 5.10: Sample ACF and PACF of detrended SOIas

\[\omega(B)y_{t}=\delta(B)B^{d}x_{t}+\omega(B)\eta_{t},\]

or in regression form

\[y_{t}=\sum_{k=1}^{r}\omega_{k}y_{t-k}+\sum_{k=0}^{s}\delta_{k}x_{t-d-k}+u_{t}, \tag{5.66}\]

where

\[u_{t}=\omega(B)\eta_{t}. \tag{5.67}\]

Once we have (5.66), it will be easy to fit the model if we forget about \(\eta_{t}\) and allow \(u_{t}\) to have any ARMA behavior. We illustrate this technique in the following example.

**Example 5.9**: **Transfer Function Model for SOI and Recruitment**

We illustrate the procedure for fitting a lagged regression model of the form suggested in Example 5.8 to the detrended SOI series (\(x_{t}\)) and the Recruitment series (\(y_{t}\)). The results reported here are practically the same as the the results obtained from the frequency domain approach used in Example 4.24.

Based on Example 5.8, we have determined that

\[y_{t}=\alpha+\omega_{1}y_{t-1}+\delta_{0}x_{t-5}+u_{t}\]

is a reasonable model. At this point, we simply run the regression allowing for autocorrelated errors based on the techniques discussed in Sect. 3.8. Based on these techniques, the fitted model is the same as the one obtained in Example 4.24, namely,

\[y_{t}=12+.8y_{t-1}-21x_{t-5}+u_{t}\,,\quad\text{and}\quad u_{t}=.45u_{t-1}+w_{ t}\,,\]

where \(w_{t}\) is white noise with \(\sigma_{w}^{2}=50\).

Figure 5.11: Sample CCF of the prewhitened, detrended SOI and the similarly transformed Recruitment series; negative lags indicate that SOI leads Recruitment

Figure 12 displays the ACF and PACF of the estimated noise \(u_{t}\), showing that an AR(1) is appropriate. In addition, the figure displays the Recruitment series and the one-step-ahead predictions based on the final model. The following R code was used for this example.

 soi.d = resid(lm(soi-time(soi), na.action-NULL))  fish = ts.intersect(rec, RL1=lag(rec,-1), SL5=lag(soi.d,-5))  (u = lm(fish[,1]-fish[,2:3], na.action-NULL))  acf2(resid(u))  # suggests ar1  (arx = sarima(fish[,1], 1, 0, 0, xreg=fish[,2:3]))  # final model  Coefficients:  ar1 intercept RL1 SL5  0.4487 12.3323 0.8005 -21.0307  s.e. 0.0503 1.5746 0.0234 1.0915  sigma^2 estimated as 49.93  pred = rec + resid(arx$fit)  # l-step-ahead predictions  ts.plot(pred, rec, col=c('gray90',1), lwd=c(7,1))

For completeness, we finish the discussion of the more complicated Box-Jenkins method for fitting transfer function models. We note, however, that the method has no recognizable overall optimality, and is not generally better or worse than the method previously discussed.

The form of (5.66) suggests doing a regression on the lagged versions of both the input and output series to obtain \(\hat{\beta}\), the estimate of the \((r+s+1)\times 1\) regression vector

\[\beta=(\omega_{1},\ldots,\omega_{r},\delta_{0},\delta_{1},\ldots,\delta_{s})^{ \prime}.\]

The residuals from the regression, say,

\[\hat{u}_{t}=y_{t}-\hat{\beta}^{\prime}z_{t},\]

Figure 12: _Top_: ACF and PACF of the estimated noise \(u_{t}\). _Bottom_: The recruitment series (_line_) and the one-step-ahead predictions (_gray swatch_) based on the final transfer function model

where

\[z_{t}=(y_{t-1},\ldots,y_{t-r},x_{t-d},\ldots,x_{t-d-s})^{\prime}\]

denotes the usual vector of independent variables, could be used to approximate the best ARMA model for the noise process \(\eta_{t}\), because we can compute an estimator for that process from (5.67), using \(\hat{u}_{t}\) and \(\hat{\omega}(B)\) and applying the moving average operator to get \(\hat{\eta}_{t}\). Fitting an ARMA(\(p_{\eta},q_{\eta}\)) model to the this estimated noise then completes the specification. The preceding suggests the following sequential procedure for fitting the transfer function model to data.

1. Fit an ARMA model to the input series \(x_{t}\) to estimate the parameters \(\phi_{1},\ldots,\phi_{p}\), \(\theta_{1},\ldots,\theta_{q},\sigma_{w}^{2}\) in the specification (5.60). Retain ARMA coefficients for use in step (ii) and the fitted residuals \(\hat{w}_{t}\) for use in Step (iii).
2. Apply the operator determined in step (i), that is, \[\hat{\phi}(B)y_{t}=\hat{\theta}(B)\tilde{y}_{t},\] to determine the transformed output series \(\tilde{y}_{t}\).
3. Use the cross-correlation function between \(\tilde{y}_{t}\) and \(\hat{w}_{t}\) in steps (i) and (ii) to suggest a form for the components of the polynomial \[\alpha(B)=\frac{\delta(B)B^{d}}{\omega(B)}\] and the estimated time delay \(d\).
4. Obtain \(\hat{\beta}=(\hat{\omega}_{1},\ldots,\hat{\omega}_{r},\hat{\delta}_{0},\hat{ \delta}_{1},\ldots,\hat{\delta}_{s})\) by fitting a linear regression of the form (5.66). Retain the residuals \(\hat{u}_{t}\) for use in step (v).
5. Apply the moving average transformation (5.67) to the residuals \(\hat{u}_{t}\) to find the noise series \(\hat{\eta}_{t}\), and fit an ARMA model to the noise, obtaining the estimated coefficients in \(\hat{\phi}_{\eta}(B)\) and \(\hat{\theta}_{\eta}(B)\).

The above procedure is fairly reasonable, but as previously mentioned, is not optimal in any sense. Simultaneous least squares estimation, based on the observed \(x_{t}\) and \(y_{t}\), can be accomplished by noting that the transfer function model can be written as

\[y_{t}=\frac{\delta(B)B^{d}}{\omega(B)}x_{t}+\frac{\theta_{\eta}(B)}{\phi_{\eta }(B)}z_{t},\]

which can be put in the form

\[\omega(B)\phi_{\eta}(B)y_{t}=\phi_{\eta}(B)\delta(B)B^{d}x_{t}+\omega(B)\theta _{\eta}(B)z_{t}, \tag{5.68}\]

and it is clear that we may use least squares to minimize \(\sum_{t}z_{t}^{2}\), as in earlier sections. In Example 5.9, we simply allowed \(u_{t}=\frac{\theta_{\eta}(B)}{\phi_{\eta}(B)}z_{t}\) in (5.68) to have any ARMA structure. Finally, we mention that we may also express the transfer function in state-space form as an ARMAX model; see Sects. 5.6 and 6.6.1.

### Multivariate ARMAX Models

To understand multivariate time series models and their capabilities, we first present an introduction to multivariate time series regression techniques. Since all processes are vector processes, we suspend the use of boldface for vectors. A useful extension of the basic univariate regression model presented in Sect. 2.1 is the case in which we have more than one output series, that is, _multivariate regression analysis_. Suppose, instead of a single output variable \(y_{t}\), a collection of \(k\) output variables \(y_{t1},y_{t2},\ldots,y_{tk}\) exist that are related to the inputs as

\[y_{ti}=\beta_{i1}z_{t1}+\beta_{i2}z_{t2}+\cdots+\beta_{ir}z_{tr}+w_{ti} \tag{5.69}\]

for each of the \(i=1,2,\ldots,k\) output variables. We assume the \(w_{ti}\) variables are correlated over the variable identifier \(i\), but are still independent over time. Formally, we assume \(\text{cov}\{w_{si},w_{tj}\}=\sigma_{ij}\) for \(s=t\) and is zero otherwise. Then, writing (5.69) in matrix notation, with \(y_{t}=(y_{t1},y_{t2},\ldots,y_{tk})^{\prime}\) being the vector of outputs, and \(\mathcal{B}=\{\beta_{ij}\},i=1,\ldots,k\), \(j=1,\ldots,r\) being a \(k\times r\) matrix containing the regression coefficients, leads to the simple looking form

\[y_{t}=\mathcal{B}z_{t}+w_{t}. \tag{5.70}\]

Here, the \(k\times 1\) vector process \(w_{t}\) is assumed to be a collection of independent vectors with common covariance matrix \(\text{E}\{w_{t}w_{t}^{\prime}\}=\Sigma_{w}\), the \(k\times k\) matrix containing the covariances \(\sigma_{ij}\). Under the assumption of normality, the maximum likelihood estimator for the regression matrix is

\[\hat{\mathcal{B}}=\left(\sum_{t=1}^{n}y_{t}z_{t}^{\prime}\right)\left(\sum_{t=1 }^{n}z_{t}z_{t}^{\prime}\right)^{-1}. \tag{5.71}\]

The error covariance matrix \(\Sigma_{w}\) is estimated by

\[\hat{\Sigma}_{w}=\frac{1}{n-r}\sum_{t=1}^{n}(y_{t}-\hat{\mathcal{B}}z_{t})(y_{t }-\hat{\mathcal{B}}z_{t})^{\prime}. \tag{5.72}\]

The uncertainty in the estimators can be evaluated from

\[\text{se}(\hat{\beta}_{ij})=\sqrt{c_{ii}\hat{\sigma}_{jj}}, \tag{5.73}\]

for \(i=1,\ldots,r\), \(j=1,\ldots,k\), where se denotes estimated standard error, \(\hat{\sigma}_{jj}\) is the \(j\)-th diagonal element of \(\hat{\Sigma}_{w}\), and \(c_{ii}\) is the \(i\)-th diagonal element of \(\left(\sum_{t=1}^{n}z_{t}z_{t}^{\prime}\right)^{-1}\).

Also, the information theoretic criterion changes to

\[\text{AIC}=\ln|\hat{\Sigma}_{w}|+\frac{2}{n}\left(kr+\frac{k(k+1)}{2}\right). \tag{5.74}\]

and BIC replaces the second term in (5.74) by \(K\ln n/n\) where \(K=kr+k(k+1)/2\). Bedrick and Tsai [16] have given a corrected form for AIC in the multivariate case as\[\text{AICc}=\ln|\hat{\Sigma}_{w}|+\frac{k(r+n)}{n-k-r-1}. \tag{5.75}\]

Many data sets involve more than one time series, and we are often interested in the possible dynamics relating all series. In this situation, we are interested in modeling and forecasting \(k\times 1\) vector-valued time series \(x_{t}=(x_{t1},\ldots,x_{tk})^{\prime}\), \(t=0,\pm 1,\pm 2,\ldots\). Unfortunately, extending univariate ARMA models to the multivariate case is not so simple. The multivariate autoregressive model, however, is a straight-forward extension of the univariate AR model.

For the first-order _vector autoregressive model_, VAR(1), we take

\[x_{t}=\alpha+\Phi x_{t-1}+w_{t}, \tag{5.76}\]

where \(\Phi\) is a \(k\times k\)_transition matrix_ that expresses the dependence of \(x_{t}\) on \(x_{t-1}\). The _vector white noise_ process \(w_{t}\) is assumed to be multivariate normal with mean-zero and covariance matrix

\[\text{E}\left(w_{t}w_{t}^{\prime}\right)=\Sigma_{w}. \tag{5.77}\]

The vector \(\alpha=(\alpha_{1},\alpha_{2},\ldots,\alpha_{k})^{\prime}\) appears as the constant in the regression setting. If \(E(x_{t})=\mu\), then \(\alpha=(I-\Phi)\mu\).

Note the similarity between the VAR model and the multivariate linear regression model (5.70). The regression formulas carry over, and we can, on observing \(x_{1},\ldots,x_{n}\), set up the model (5.76) with \(y_{t}=x_{t}\), \(\mathcal{B}=(\alpha,\Phi)\) and \(z_{t}=(1,x_{t-1}^{\prime})^{\prime}\). Then, write the solution as (5.71) with the conditional maximum likelihood estimator for the covariance matrix given by

\[\hat{\Sigma}_{w}=(n-1)^{-1}\sum_{t=2}^{n}(x_{t}-\hat{\alpha}-\hat{\Phi}x_{t-1}) (x_{t}-\hat{\alpha}-\hat{\Phi}x_{t-1})^{\prime}. \tag{5.78}\]

The special form assumed for the constant component, \(\alpha\), of the vector AR model in (5.76) can be generalized to include a fixed \(r\times 1\) vector of inputs, \(u_{t}\). That is, we could have proposed the _vector ARX model_,

\[x_{t}=\Gamma u_{t}+\sum_{j=1}^{p}\Phi_{j}x_{t-j}+w_{t}, \tag{5.79}\]

where \(\Gamma\) is a \(p\times r\) parameter matrix. The X in ARX refers to the exogenous vector process we have denoted here by \(u_{t}\). The introduction of exogenous variables through replacing \(\alpha\) by \(\Gamma u_{t}\) does not present any special problems in making inferences and we will often drop the X for being superfluous.

**Example 5.10**: **Pollution, Weather, and Mortality**

For example, for the three-dimensional series composed of cardiovascular mortality \(x_{t1}\), temperature \(x_{t2}\), and particulate levels \(x_{t3}\), introduced in Example 2.2, take \(x_{t}=(x_{t1},x_{t2},x_{t3})^{\prime}\) as a vector of dimension \(k=3\). We might envision dynamic relations among the three series defined as the first order relation,

[MISSING_PAGE_FAIL:284]

\[\hat{M}_{t}=73.23-.014t+.46M_{t-1}-.36T_{t-1}+.10P_{t-1}.\]

Comparing observed and predicted mortality with this model leads to an \(R^{2}\) of about.69.

It is easy to extend the VAR(1) process to higher orders, VAR(\(p\)). To do this, we use the notation of (5.70) and write the vector of regressors as

\[z_{t}=(1,x^{\prime}_{t-1},x^{\prime}_{t-2},\ldots x^{\prime}_{t-p})^{\prime}\]

and the regression matrix as \(\mathcal{B}=(\alpha,\Phi_{1},\Phi_{2},\ldots,\Phi_{p})\). Then, this regression model can be written as

\[x_{t}=\alpha+\sum_{j=1}^{p}\Phi_{j}x_{t-j}+w_{t} \tag{5.80}\]

for \(t=p+1,\ldots,n\). The \(k\times k\) error sum of products matrix becomes

\[\text{SSE}=\sum_{t=p+1}^{n}(x_{t}-\mathcal{B}z_{t})(x_{t}-\mathcal{B}z_{t})^{ \prime}, \tag{5.81}\]

so that the conditional maximum likelihood estimator for the _error covariance matrix_\(\Sigma_{w}\) is

\[\hat{\Sigma}_{w}=\text{SSE}/(n-p), \tag{5.82}\]

as in the multivariate regression case, except now only \(n-p\) residuals exist in (5.81). For the multivariate case, we have found that the Schwarz criterion

\[\text{BIC}=\log|\hat{\Sigma}_{w}|+k^{2}p\ln n/n, \tag{5.83}\]

gives more reasonable classifications than either AIC or corrected version AICc. The result is consistent with those reported in simulations by Lutkepohl [130]. Of course, estimation via Yule-Walker, unconditional least squares and MLE follow directly from the univariate counterparts.

**Example 5.11**: **Pollution, Weather, and Mortality (cont)**

We used the R package first to select a VAR(\(p\)) model and then fit the model. The selection criteria used in the package are AIC, Hannan-Quinn (HQ; Hannan and Quinn [87]), BIC (SC), and Final Prediction Error (FPE). The Hannan-Quinn procedure is similar to BIC, but with \(\ln n\) replaced by \(2\ln(\ln(n))\) in the penalty term. FPE finds the model that minimizes the approximate mean squared one-step-ahead prediction error (see Akaike [1] for details); it is rarely used.

VARselect(x, lag.max=10, type="both")

$selection  AIC(n) HQ(n) SC(n) FPE(n)  9 5 2 9  $criteria 1 2 3 4 5 6 7 8 9 10  AIC(n) 11.738 11.302 11.268 11.230 11.176 11.153 11.152 11.129 11.119 11.120  HO(n) 11.788 11.381 11.377 11.370 11.346 11.352 11.381 11.388 11.408 11.439  SC(n) 11.865 11.505 11.547 11.585 11.608 11.660 11.736 11.788 11.855 11.932 Note that BIC picks the order \(p=2\) model while AIC and FPE pick an order \(p=9\) model and Hannan-Quinn selects an order \(p=5\) model.

Fitting the model selected by BIC we obtain

\[\hat{\alpha}=(56.1,49.9,59.6)^{\prime},\qquad\hat{\beta}=(-0.011,-0.005,-0.008)^{ \prime},\]

\[\hat{\Phi}_{1}=\left(\begin{array}{ccc}.30(.04)&-.20(.04)&.04(.02)\\ -.11(.05)&.26(.05)&-.05(.03)\\.08(.09)&-.39(.09)&.39(.05)\end{array}\right),\]

\[\hat{\Phi}_{2}=\left(\begin{array}{ccc}.28(.04)&-.08(.03)&.07(.03)\\ -.04(.05)&.36(.05)&-.10(.03)\\ -.33(.09)&.05(.09)&.38(.05)\end{array}\right),\]

where the standard errors are given in parentheses. The estimate of \(\Sigma_{w}\) is

\[\hat{\Sigma}_{w}=\left(\begin{array}{ccc}28.03&7.08&16.33\\ 7.08&37.63&40.88\\ 16.33&40.88&123.45\end{array}\right).\]

To fit the model using the vars package use the following:

summary(fit <- VAR(x, p=2, type="both")) # partial results displayed cmort = cmort.l1 + tempr.l1 + part.l1 + cmort.l2 + tempr.l2 + part.l2 + const + trend Estimate Std. Error t value p.value cmort.l1 0.297059 0.043734 6.792 3.15e-11 tempr.l1 -0.199510 0.044274 -4.506 8.23e-06 part.l1 0.042523 0.024034 1.769 0.07745 cmort.l2 0.276194 0.041938 6.586 1.15e-10 tempr.l2 -0.079337 0.044679 -1.776 0.07639 part.l2 0.068082 0.025286 2.692 0.00733 const 56.098652 5.916618 9.482 < 2e-16 trend -0.011042 0.001992 -5.543 4.84e-08 Covariance matrix of residuals: cmort tempr part cmort 28.034 7.076 16.33 tempr 7.076 37.627 40.88 part 16.325 40.880 123.45

Using the notation of the previous example, the prediction model for cardiovascular mortality is estimated to be

\[\hat{M}_{t}=56-.01t+.3M_{t-1}-.2T_{t-1}+.04P_{t-1}+.28M_{t-2}-.08T_{t-2}+.07P_{ t-2}.\]

To examine the residuals, we can plot the cross-correlations of the residuals and examine the multivariate version of the Q-test as follows:

acf(resid(fit), 52) serial.test(fit, lags.pt=12, type="PT.adjusted") Portmanteau Test (adjusted) data: Residuals of VAR object fit Chi-squared = 162.3502, df = 90, p-value = 4.602e-06 The cross-correlation matrix is shown in Figure 13. The figure shows the ACFs of the individual residual series along the diagonal. For example, the first diagonal graph is the ACF of \(M_{t}-\hat{M}_{t}\), and so on. The off diagonals display the CCFs between pairs of residual series. If the title of the off-diagonal plot is x & y, then y leads in the graphic; that is, on the upper-diagonal, the plot shows corr[x(t+Lag), y(t)] whereas in the lower-diagonal, if the title is x & y, you get a plot of corr[x(t+Lag), y(t)] (yes, it is the same thing, but the lags are negative in the lower diagonal). The graphic is labeled in a strange way, just remember the second named series is the one that leads. In Figure 5.13 we notice that most of the correlations in the residual series are negligible, however, the zero-order correlation of mortality with temperature residuals is about.22 and mortality with particulate residuals is about.28 (type acf(resid(fit),52)5acf) to see the actual values. This means that the AR model is not capturing the concurrent effect of temperature and pollution on mortality (recall the data evolves over a week). It is possible to fit simultaneous models; see Reinsel [163] for further details. Thus, not unexpectedly, the Q-test rejects the null hypothesis that the noise is white. The Q-test statistic is given by

\[Q=n^{2}\sum_{h=1}^{H}\frac{1}{n-h}\operatorname{tr}\left[\hat{\Gamma}_{w}(h) \hat{\Gamma}_{w}(0)^{-1}\hat{\Gamma}_{w}(h)\hat{\Gamma}_{w}(0)^{-1}\right], \tag{5.84}\]

where

\[\hat{\Gamma}_{w}(h)=n^{-1}\sum_{t=1}^{n-h}\hat{w}_{t+h}\hat{w}_{t}^{\prime},\]

and \(\hat{w}_{t}\) is the residual process. Under the null that \(w_{t}\) is white noise, (5.84) has an asymptotic \(\chi^{2}\) distribution with \(k^{2}(H-p)\) degrees of freedom.

Finally, prediction follows in a straight forward manner from the univariate case. Using the R package vars, use the predict command and the fanchart command, which produces a nice graphic:

(fit.pr = predict(fit, n.ahead = 24, ci = 0.95)) # 4 weeks ahead

fanchart(fit.pr) # plot prediction + error

The results are displayed in Figure 5.14; we note that the package stripped time when plotting the fanchart and the horizontal axis is labeled \(1,2,3,\ldots\).

For pure VAR(\(p\)) models, the autocovariance structure leads to the multivariate version of the _Yule-Walker equations_:

\[\Gamma(h)=\sum_{j=1}^{p}\Phi_{j}\Gamma(h-j),\quad h=1,2,\ldots, \tag{5.85}\]

\[\Gamma(0)=\sum_{j=1}^{p}\Phi_{j}\Gamma(-j)+\Sigma_{w}. \tag{5.86}\]

where \(\Gamma(h)=\operatorname{cov}(x_{t+h},x_{t})\) is a \(k\times k\) matrix and \(\Gamma(-h)=\Gamma(h)^{\prime}\).

Estimation of the autocovariance matrix is similar to the univariate case, that is, with \(\bar{x}=n^{-1}\sum_{t=1}^{n}x_{t}\), as an estimate of \(\mu=\operatorname{E}x_{t}\),

\[\hat{\Gamma}(h)=n^{-1}\sum_{t=1}^{n-h}(x_{t+h}-\bar{x})(x_{t}-\bar{x})^{\prime },\quad h=0,1,2,..,n-1, \tag{5.87}\]and \(\hat{\Gamma}(-h)=\hat{\Gamma}(h)^{\prime}\). If \(\hat{\gamma}_{i,j}(h)\) denotes the element in the \(i\)-th row and \(j\)-th column of \(\hat{\Gamma}(h)\), the cross-correlation functions (CCF), as discussed in (35), are estimated by

\[\hat{\rho}_{i,j}(h)=\frac{\hat{\gamma}_{i,j}(h)}{\sqrt{\hat{\gamma}_{i,i}(0)} \sqrt{\hat{\gamma}_{j,j}(0)}}\quad h=0,1,2,..,n-1. \tag{5.88}\]

When \(i=j\) in (5.88), we get the estimated autocorrelation function (ACF) of the individual series.

Although least squares estimation was used in Example 5.10 and Example 5.11, we could have also used Yule-Walker estimation, conditional or unconditional maximum likelihood estimation. As in the univariate case, the Yule-Walker estimators, the maximum likelihood estimators, and the least squares estimators are asymptotically

Figure 5.13: ACFs (diagonals) and CCFs (off-diagonals) for the residuals of the three-dimensional VAR(2) fit to the LA mortality – pollution data set. On the off-diagonals, the second-named series is the one that leads

equivalent. To exhibit the asymptotic distribution of the autoregression parameter estimators, we write

\[\phi=\text{vec}\left(\Phi_{1},\ldots,\Phi_{p}\right),\]

where the _vec operator_ stacks the columns of a matrix into a vector. For example, for a bivariate AR(2) model,

\[\phi=\text{vec}\left(\Phi_{1},\Phi_{2}\right)=\left(\Phi_{1_{11}},\Phi_{1_{21} },\Phi_{1_{12}},\Phi_{1_{12}}\Phi_{2_{21}},\Phi_{2_{21}},\Phi_{2_{21}},\Phi_{2 _{22}},\Phi_{2_{22}}\right)^{\prime},\]

where \(\Phi_{\ell_{ij}}\) is the \(ij\)-th element of \(\Phi_{\ell}\), \(\ell=1,2\). Because \(\left(\Phi_{1},\ldots,\Phi_{p}\right)\) is a \(k\times kp\) matrix, \(\phi\) is a \(k^{2}p\times 1\) vector. We now state the following property.

**Property 5.1**: **Large-Sample Distribution of VAR Estimators**__

_Let \(\hat{\phi}\) denote the vector of parameter estimators (obtained via Yule-Walker, least squares, or maximum likelihood) for a \(k\)-dimensional AR(p) model. Then,_

\[\sqrt{n}\left(\hat{\phi}-\phi\right)\sim AN(0,\Sigma_{w}\otimes\Gamma_{pp}^{-1 }), \tag{5.89}\]

_where \(\Gamma_{pp}=\{\Gamma(i-j)\}_{i,j=1}^{p}\) is a \(kp\times kp\) matrix and \(\Sigma_{w}\otimes\Gamma_{pp}^{-1}=\{\sigma_{ij}\Gamma_{pp}^{-1}\}_{i,j=1}^{k}\) is a \(k^{2}p\times k^{2}p\) matrix with \(\sigma_{ij}\) denoting the \(ij\)-th element of \(\Sigma_{w}\)._

The variance-covariance matrix of the estimator \(\hat{\phi}\) is approximated by replacing \(\Sigma_{w}\) by \(\hat{\Sigma}_{w}\), and replacing \(\Gamma(h)\) by \(\hat{\Gamma}(h)\) in \(\Gamma_{pp}\). The square root of the diagonal elements of \(\hat{\Sigma}_{w}\otimes\hat{\Gamma}_{pp}^{-1}\) divided by \(\sqrt{n}\) gives the individual standard errors. For the mortality data example, the estimated standard errors for the VAR(2) fit are listed in Example 5.11; although those standard errors were taken from a regression run, they could have also been calculated using Property 5.1.

Figure 5.14: Predictions from a VAR(2) fit to the LA mortality – pollution data

A \(k\times 1\) vector-valued time series \(x_{t}\), for \(t=0,\pm 1,\pm 2,\ldots\), is said to be VARMA\((p,q)\) if \(x_{t}\) is stationary and

\[x_{t}=\alpha+\Phi_{1}x_{t-1}+\cdots+\Phi_{p}x_{t-p}+w_{t}+\Theta_{1}w_{t-1}+ \cdots+\Theta_{q}w_{t-q}, \tag{5.90}\]

with \(\Phi_{p}\neq 0\), \(\Theta_{q}\neq 0\), and \(\Sigma_{w}>0\) (that is, \(\Sigma_{w}\) is positive definite). The coefficient matrices \(\Phi_{j}\); \(j=1,\ldots,p\) and \(\Theta_{j}\); \(j=1,\ldots,q\) are, of course, \(k\times k\) matrices. If \(x_{t}\) has mean \(\mu\) then \(\alpha=(I-\Phi_{1}-\cdots-\Phi_{p})\mu\). As in the univariate case, we will have to place a number of conditions on the multivariate ARMA model to ensure the model is unique and has desirable properties such as causality. These conditions will be discussed shortly.

As in the VAR model, the special form assumed for the constant component can be generalized to include a fixed \(r\times 1\) vector of inputs, \(u_{t}\). That is, we could have proposed the _vector ARMAX model_,

\[x_{t}=\Gamma u_{t}+\sum_{j=1}^{p}\Phi_{j}x_{t-j}+\sum_{k=1}^{q}\Theta_{k}w_{t -k}+w_{t}, \tag{5.91}\]

where \(\Gamma\) is a \(p\times r\) parameter matrix.

While extending univariate AR (or pure MA) models to the vector case is fairly easy, extending univariate ARMA models to the multivariate case is not a simple matter. Our discussion will be brief, but interested readers can get more details in Lutkepohl [131], Reinsel [163], and Tiao and Tsay [200].

In the multivariate case, the _autoregressive operator_ is

\[\Phi(B)=I-\Phi_{1}B-\cdots-\Phi_{p}B^{p}, \tag{5.92}\]

and the _moving average operator_ is

\[\Theta(B)=I+\Theta_{1}B+\cdots+\Theta_{q}B^{q}, \tag{5.93}\]

The zero-mean VARMA\((p,q)\) model is then written in the concise form as

\[\Phi(B)x_{t}=\Theta(B)w_{t}. \tag{5.94}\]

The model is said to be _causal_ if the roots of \(|\Phi(z)|\) (where \(|\cdot|\) denotes determinant) are outside the unit circle, \(|z|>1\); that is, \(|\Phi(z)|\neq 0\) for any value \(z\) such that \(|z|\leq 1\). In this case, we can write

\[x_{t}=\Psi(B)w_{t},\]

where \(\Psi(B)=\sum_{j=0}^{\infty}\Psi_{j}B^{j}\), \(\Psi_{0}=I\), and \(\sum_{j=0}^{\infty}||\Psi_{j}||<\infty\). The model is said to be _invertible_ if the roots of \(|\Theta(z)|\) lie outside the unit circle. Then, we can write

\[w_{t}=\Pi(B)x_{t},\]

where \(\Pi(B)=\sum_{j=0}^{\infty}\Pi_{j}B^{j}\), \(\Pi_{0}=I\), and \(\sum_{j=0}^{\infty}||\Pi_{j}||<\infty\). Analogous to the univariate case, we can determine the matrices \(\Psi_{j}\) by solving \(\Psi(z)=\Phi(z)^{-1}\Theta(z),|z|\leq 1\), and the matrices \(\Pi_{j}\) by solving \(\Pi(z)=\Theta(z)^{-1}\Phi(z),|z|\leq 1\).

For a causal model, we can write \(x_{t}=\Psi(B)w_{t}\) so the general autocovariance structure of an ARMA(\(p,q\)) model is (\(h\geq 0\))

\[\Gamma(h)=\text{cov}(x_{t+h},x_{t})=\sum_{j=0}^{\infty}\Psi_{j+h}\Sigma_{w}\Psi_ {j}^{\prime}. \tag{5.95}\]

and \(\Gamma(-h)=\Gamma(h)^{\prime}\). For pure MA(\(q\)) processes, (5.95) becomes

\[\Gamma(h)=\sum_{j=0}^{q-h}\Theta_{j+h}\Sigma_{w}\Theta_{j}^{\prime}, \tag{5.96}\]

where \(\Theta_{0}=I\). Of course, (5.96) implies \(\Gamma(h)=0\) for \(h>q\).

As in the univariate case, we will need conditions for model uniqueness. These conditions are similar to the condition in the univariate case that the autoregressive and moving average polynomials have no common factors. To explore the uniqueness problems that we encounter with multivariate ARMA models, consider a bivariate AR(1) process, \(x_{t}=(x_{t,1},x_{t,2})^{\prime}\), given by

\[x_{t,1} = \phi x_{t-1,2}+w_{t,1},\] \[x_{t,2} = w_{t,2},\]

where \(w_{t,1}\) and \(w_{t,2}\) are independent white noise processes and \(|\phi|<1\). Both processes, \(x_{t,1}\) and \(x_{t,2}\) are causal and invertible. Moreover, the processes are jointly stationary because \(\text{cov}(x_{t+h,1},x_{t,2})=\phi\ \text{cov}(x_{t+h-1,2},x_{t,2})\equiv\phi\ \gamma_{2,2}(h-1)=\phi\sigma_{w_{2}}^{2}\delta_{1}^{h}\) does not depend on \(t\); note, \(\delta_{1}^{h}=1\) when \(h=1\), otherwise, \(\delta_{1}^{h}=0\). In matrix notation, we can write this model as

\[x_{t}=\Phi x_{t-1}+w_{t}\,,\quad\text{where}\quad\Phi=\begin{bmatrix}0&\phi\\ 0&0\end{bmatrix}. \tag{5.97}\]

We can write (5.97) in operator notation as

\[\Phi(B)x_{t}=w_{t}\quad\text{where}\quad\Phi(z)=\begin{bmatrix}1&-\phi z\\ 0&1\end{bmatrix}.\]

In addition, model (5.97) can be written as a bivariate ARMA(1,1) model

\[x_{t}=\Phi_{1}x_{t-1}+\Theta_{1}w_{t-1}+w_{t}, \tag{5.98}\]

where

\[\Phi_{1}=\begin{bmatrix}0&\phi+\theta\\ 0&0\end{bmatrix}\quad\text{and}\quad\Theta_{1}=\begin{bmatrix}0&-\theta\\ 0&0\end{bmatrix},\]

and \(\theta\) is arbitrary. To verify this, we write (5.98), as \(\Phi_{1}(B)x_{t}=\Theta_{1}(B)w_{t}\), or

\[\Theta_{1}(B)^{-1}\Phi_{1}(B)x_{t}=w_{t},\]where

\[\Phi_{1}(z)=\begin{bmatrix}1&-(\phi+\theta)z\\ 0&1\end{bmatrix}\quad\text{and}\quad\Theta_{1}(z)=\begin{bmatrix}1&-\theta z\\ 0&1\end{bmatrix}.\]

Then,

\[\Theta_{1}(z)^{-1}\Phi_{1}(z)=\begin{bmatrix}1&\theta z\\ 0&1\end{bmatrix}\begin{bmatrix}1&-(\phi+\theta)z\\ 0&1\end{bmatrix}=\begin{bmatrix}1&-\phi z\\ 0&1\end{bmatrix}=\Phi(z),\]

where \(\Phi(z)\) is the polynomial associated with the bivariate AR(1) model in (5.97). Because \(\theta\) is arbitrary, the parameters of the ARMA(1,1) model given in (5.98) are not identifiable. No problem exists, however, in fitting the AR(1) model given in (5.97).

The problem in the previous discussion was caused by the fact that both \(\Theta(B)\) and \(\Theta(B)^{-1}\) are finite; such a matrix operator is called _unimodular_. If \(U(B)\) is unimodular, \(|U(z)|\) is constant. It is also possible for two seemingly different multivariate ARMA(\(p,q\)) models, say, \(\Phi(B)x_{t}=\Theta(B)w_{t}\) and \(\Phi_{*}(B)x_{t}=\Theta_{*}(B)w_{t}\), to be related through a unimodular operator, \(U(B)\) as \(\Phi_{*}(B)=U(B)\Phi(B)\) and \(\Theta_{*}(B)=U(B)\Theta(B)\), in such a way that the orders of \(\Phi(B)\) and \(\Theta(B)\) are the same as the orders of \(\Phi_{*}(B)\) and \(\Theta_{*}(B)\), respectively. For example, consider the bivariate ARMA(1,1) models given by

\[\Phi x_{t}\equiv\begin{bmatrix}1&-\phi B\\ 0&1\end{bmatrix}x_{t}=\begin{bmatrix}1&\theta B\\ 0&1\end{bmatrix}w_{t}\equiv\theta w_{t}\]

and

\[\Phi_{*}(B)x_{t}\equiv\begin{bmatrix}1&(\alpha-\phi)B\\ 0&1\end{bmatrix}x_{t}=\begin{bmatrix}1&(\alpha+\theta)B\\ 0&1\end{bmatrix}w_{t}\equiv\Theta_{*}(B)w_{t},\]

where \(\alpha\), \(\phi\), and \(\theta\) are arbitrary constants. Note,

\[\Phi_{*}(B)\equiv\begin{bmatrix}1&(\alpha-\phi)B\\ 0&1\end{bmatrix}=\begin{bmatrix}1&\alpha B\\ 0&1\end{bmatrix}\begin{bmatrix}1&-\phi B\\ 0&1\end{bmatrix}\equiv U(B)\Phi(B)\]

and

\[\Theta_{*}(B)\equiv\begin{bmatrix}1&(\alpha+\theta)B\\ 0&1\end{bmatrix}=\begin{bmatrix}1&\alpha B\\ 0&1\end{bmatrix}\begin{bmatrix}1&\theta B\\ 0&1\end{bmatrix}\equiv U(B)\Theta(B).\]

In this case, both models have the same infinite MA representation \(x_{t}=\Psi(B)w_{t}\), where

\[\Psi(B)=\Phi(B)^{-1}\Theta(B)=\Phi(B)^{-1}U(B)^{-1}U(B)\Theta(B)=\Phi_{*}(B)^{ -1}\Theta_{*}(B).\]

This result implies the two models have the same autocovariance function \(\Gamma(h)\). Two such ARMA(\(p,q\)) models are said to be _observationally equivalent_.

As previously mentioned, in addition to requiring causality and invertibility, we will need some additional assumptions in the multivariate case to make sure that the model is unique. To ensure the _identifiability_ of the parameters of the multivariate ARMA(\(p,q\)) model, we need the following additional two conditions: (i) the matrix operators \(\Phi(B)\) and \(\Theta(B)\) have no common left factors other than unimodular ones [that is, if \(\Phi(B)=U(B)\Phi_{*}(B)\) and \(\Theta(B)=U(B)\Theta_{*}(B)\), the common factor must be unimodular] and (ii) with \(q\) as small as possible and \(p\) as small as possible for that \(q\), the matrix \([\Phi_{p},\Theta_{q}]\) must be full rank, \(k\). One suggestion for avoiding most of the aforementioned problems is to fit only vector AR(\(p\)) models in multivariate situations. Although this suggestion might be reasonable for many situations, this philosophy is not in accordance with the law of parsimony because we might have to fit a large number of parameters to describe the dynamics of a process.

Asymptotic inference for the general case of vector ARMA models is more complicated than pure AR models; details can be found in Reinsel [163] or Lutkepohl [131], for example. We also note that estimation for VARMA models can be recast into the problem of estimation for state-space models that will be discussed in Chap. 6.

**Example 5.12**: **The Spliid Algorithm for Fitting Vector ARMA**

A simple algorithm for fitting vector ARMA models from Spliid [189] is worth mentioning because it repeatedly uses the multivariate regression equations. Consider a general ARMA(\(p,q\)) model for a time series with a nonzero mean

\[x_{t}=\alpha+\Phi_{1}x_{t-1}+\cdots+\Phi_{p}x_{t-p}+w_{t}+\Theta_{1}w_{t-1}+ \cdots+\Theta_{q}w_{t-q}. \tag{5.99}\]

If \(\mu=\mathrm{E}x_{t}\), then \(\alpha=(I-\Phi_{1}-\cdots-\Phi_{p})\mu\). If \(w_{t-1},\ldots,w_{t-q}\) were observed, we could rearrange (5.99) as a multivariate regression model

\[x_{t}=\mathcal{B}z_{t}+w_{t}, \tag{5.100}\]

with

\[z_{t}=(1,x^{\prime}_{t-1},\ldots,x^{\prime}_{t-p},w^{\prime}_{t-1},\ldots,w^{ \prime}_{t-q})^{\prime} \tag{5.101}\]

and

\[\mathcal{B}=[\alpha,\Phi_{1},\ldots,\Phi_{p},\Theta_{1},\ldots,\Theta_{q}], \tag{5.102}\]

for \(t=p+1,\ldots,n\). Given an initial estimator \(\mathcal{B}_{0}\), of \(\mathcal{B}\), we can reconstruct \(\{w_{t-1},\ldots,w_{t-q}\}\) by setting

\[w_{t-j}=x_{t-j}-\mathcal{B}_{0}z_{t-j},\quad t=p+1,\ldots,n,\quad j=1,\ldots,q, \tag{5.103}\]

where, if \(q>p\), we put \(w_{t-j}=0\) for \(t-j\leq 0\). The new values of \(\{w_{t-1},\ldots,w_{t-q}\}\) are then put into the regressors \(z_{t}\) and a new estimate, say, \(\mathcal{B}_{1}\), is obtained. The initial value, \(\mathcal{B}_{0}\), can be computed by fitting a pure autoregression of order \(p\) or higher, and taking \(\Theta_{1}=\cdots=\Theta_{q}=0\). The procedure is then iterated until the parameter estimates stabilize. The algorithm often converges, but not to the maximum likelihood estimators. Experience suggests the estimators can be reasonably close to the maximum likelihood estimators. The algorithm can be considered as a quick and easy way to fit an initial VARMA model as a starting point to using maximum likelihood estimation, which is best done via state-space models covered in the next chapter.

We used the R package marima to fit a vector ARMA(\(2,1\)) to the mortality-pollution data set and part of the output is displayed. We note that mortality is detrended prior to the analysis. The one-step-ahead predictions for mortality are displayed in Fig. 5.15.

library(marima) model=define.model(kvar=3,ar=c(1,2),ma=c(1))  arp=model5ar.pattern;map=model$ma.pattern  cmort.d=resid(detr<-lm(cmort-time(cmort),na.action=NULL))  xdata=matrix(cbind(cmort.d,tempr,part),ncol=3)#striptsattributes  fit=marima(xdata,ar.pattern=arp,ma.pattern=map,means=c(0,1,1),  penalty=1)  #residanalysis(notdisplayed)  innov=t(resid(fit));plot.ts(innov);acf(innov,na.action=na.pass)  #fittedvaluesforcmort pred=ts(t(fitted(fit))[,1],start=start(cmort),freq=frequency(cmort))+  detr%coef[1]+detr%coef[2]*time(cmort)  plot(pred,ylab="CardiovascularMortality",lwd=2,col=4);points(cmort)  #printestimatesandcorrespondingt^2-statistic  short.form(fit$ar.estimates,leading=FALSE)  short.form(fit$ar.fvalues,leading=FALSE)  short.form(fit$ma.estimates,leading=FALSE)  short.form(fit$ma.fvalues,leading=FALSE)  parameterestimatet^2statistic  AR1  -0.3110.000-0.11451.210.07.9  0.000-0.6560.0480.0041.73.1  -0.1090.000-0.8611.570.0113.3  AR2:  -0.3330.133-0.04767.2411.892.52  0.000-0.20000.0550.008.102.90  0.179-0.102-0.1514.861.776.48  MA1:  0.000-0.187-0.10600.0014.514.75  -0.114-0.4460.00004.6816.380.00  0.000-0.278-0.6730.008.0847.56  fitSresid.cov#estimateofnoisecovmatrix  27.36.513.8  6.536.238.1  13.838.1109.2

Figure 5.15: Predictions (line) from a VARMA(2,1) fit to the LA mortality (points) data using Spliid’s algorithm

## Problems

### Section 5.1

**5.1** The data set \(\mathtt{arf}\) is 1000 simulated observations from an \(\mathrm{ARFIMA}(1,1,0)\) model with \(\phi=.75\) and \(d=.4\).

(a) Plot the data and comment.

(b) Plot the ACF and PACF of the data and comment.

(c) Estimate the parameters and test for the significance of the estimates \(\hat{\phi}\) and \(\hat{d}\).

(d) Explain why, using the results of parts (a) and (b), it would seem reasonable to difference the data prior to the analysis. That is, if \(x_{t}\) represents the data, explain why we might choose to fit an ARMA model to \(\nabla x_{t}\).

(e) Plot the ACF and PACF of \(\nabla x_{t}\) and comment.

(f) Fit an ARMA model to \(\nabla x_{t}\) and comment.

**5.2** Compute the sample ACF of the absolute values of the NYSE returns displayed in Fig. 1.4 up to lag 200, and comment on whether the ACF indicates long memory. Fit an ARFIMA model to the absolute values and comment.

### Section 5.2

**5.3** Plot the global temperature series, \(\mathtt{globtemp}\), and then test whether there is a unit root versus the alternative that the process is stationary using the three tests, DF, ADF, and PP, discussed in Example 5.3. Comment.

**5.4** Plot the GNP series, \(\mathtt{gnp}\), and then test for a unit root against the alternative that the process is explosive. State your conclusion.

**5.5** Verify (5.33).

### Section 5.3

**5.6** Weekly crude oil spot prices in dollars per barrel are in \(\mathtt{oil}\); see Problem Problem 2.10 and Appendix R for more details. Investigate whether the growth rate of the weekly oil price exhibits GARCH behavior. If so, fit an appropriate model to the growth rate.

**5.7** The \(\mathtt{stats}\) package of R contains the daily closing prices of four major European stock indices; type \(\mathtt{help}(\mathtt{EuStockMarkets})\) for details. Fit a GARCH model to the returns of one of these series and discuss your findings. (Note: The data set contains actual values, and not returns. Hence, the data must be transformed prior to the model fitting.)

**5.8** The \(2\times 1\) gradient vector, \(l^{(1)}(\alpha_{0},\alpha_{1})\), given for an ARCH(1) model was displayed in (5.47). Verify (5.47) and then use the result to calculate the \(2\times 2\) Hessian matrix

\[l^{(2)}(\alpha_{0},\alpha_{1})=\begin{pmatrix}\partial^{2}l/\partial\alpha_{0} ^{2}&\partial^{2}l/\partial\alpha_{0}\partial\alpha_{1}\\ \partial^{2}l/\partial\alpha_{0}\partial\alpha_{1}&\partial^{2}l/\partial\alpha _{1}^{2}\end{pmatrix}.\]

[MISSING_PAGE_EMPTY:296]

(e) Discuss the problem of forecasting \(y_{t+m}\) using the infinite past of \(y_{t}\) and the present and infinite past of \(x_{t}\). Determine the predicted value and the forecast variance.

#### Section 5.6

**5.12** Consider the data set econ5 containing quarterly U.S. unemployment, GNP, consumption, and government and private investment from 1948-III to 1988-II. The seasonal component has been removed from the data. Concentrating on unemployment (\(U_{t}\)), GNP (\(G_{t}\)), and consumption (\(C_{t}\)), fit a vector ARMA model to the data after first logging each series, and then removing the linear trend. That is, fit a vector ARMA model to \(x_{t}=(x_{1t},x_{2t},x_{3t})^{\prime}\), where, for example, \(x_{1t}=\log(U_{t})-\hat{\beta}_{0}-\hat{\beta}_{1}t\), where \(\hat{\beta}_{0}\) and \(\hat{\beta}_{1}\) are the least squares estimates for the regression of \(\log(U_{t})\) on time, \(t\). Run a complete set of diagnostics on the residuals.

## Chapter 6 State Space Models

A very general model that subsumes a whole class of special cases of interest in much the same way that linear regression does is the state-space model or the dynamic linear model, which was introduced in Kalman [112] and Kalman and Bucy [113]. The model arose in the space tracking setting, where the state equation defines the motion equations for the position or state of a spacecraft with location \(x_{t}\) and the data \(y_{t}\) reflect information that can be observed from a tracking device such as velocity and azimuth. Although introduced as a method primarily for use in aerospace-related research, the model has been applied to modeling data from economics (Harrison and Stevens [90]; Harvey and Pierse [92]; Harvey and Todd [91]; Kitagawa and Gersch [119], Shumway and Stoffer [181]), medicine (Jones [108]) and the soil sciences (Shumway [183], SS3.4.5). An excellent treatment of time series analysis based on the state space model is the text by Durbin and Koopman [55]. A modern treatment of nonlinear state space models can be found in Douc, Moulines and Stoffer [53].

In this chapter, we focus primarily on linear Gaussian state space models. We present various forms of the model, introduce the concepts of prediction, filtering and smoothing state space models and include their derivations. We explain how to perform maximum likelihood estimation using various techniques, and include methods for handling missing data. In addition, we present several special topics such as hidden Markov models (HMM), switching autoregressions, smoothing splines, ARMAX models, bootstrapping, stochastic volatility, and state space models with switching. Finally, we discuss a Bayesian approach to fitting state space models using Markov chain Monte Carlo (MCMC) techniques. The essential material is supplied in Sections 6.1, 6.2, and 6.3. After that, the other sections may be read in any order with some occasional backtracking.

In general, the state space model is characterized by two principles. First, there is a hidden or latent process \(x_{t}\) called the state process. The state process is assumed to be a Markov process; this means that the future \(\{x_{s};\;s>t\}\), and past \(\{x_{s};\;s<t\}\),are independent conditional on the present, \(x_{t}\). The second condition is that the observations, \(y_{t}\) are independent given the states \(x_{t}\). This means that the dependence among the observations is generated by states. The principles are displayed in Fig. 6.1.

### Linear Gaussian Model

The linear Gaussian state space model or dynamic linear model (DLM), in its basic form, employs an order one, \(p\)-dimensional vector autoregression as the _state equation_,

\[x_{t}=\varPhi x_{t-1}+w_{t}\,. \tag{6.1}\]

The \(w_{t}\) are \(p\times 1\) independent and identically distributed, zero-mean normal vectors with covariance matrix \(Q\); we write this as \(w_{t}\sim\mathrm{iid}\,\mathrm{N}_{p}(0,Q)\). In the DLM, we assume the process starts with a normal vector \(x_{0}\), such that \(x_{0}\sim\mathrm{N}_{p}(\mu_{0},\Sigma_{0})\).

We do not observe the state vector \(x_{t}\) directly, but only a linear transformed version of it with noise added, say

\[y_{t}=A_{t}x_{t}+v_{t}, \tag{6.2}\]

where \(A_{t}\) is a \(q\times p\)_measurement_ or _observation matrix_; (6.2) is called the _observation equation_. The observed data vector, \(y_{t}\), is \(q\)-dimensional, which can be larger than or smaller than \(p\), the state dimension. The additive observation noise is \(v_{t}\sim\mathrm{iid}\,\mathrm{N}_{q}(0,R)\). In addition, we initially assume, for simplicity, \(x_{0}\), \(\{w_{t}\}\) and \(\{v_{t}\}\) are uncorrelated; this assumption is not necessary, but it helps in the explanation of first concepts. The case of correlated errors is discussed in Sect. 6.6.

As in the ARMAX model of Sect. 5.6, exogenous variables, or fixed inputs, may enter into the states or into the observations. In this case, we suppose we have an \(r\times 1\) vector of inputs \(u_{t}\), and write the model as

\[x_{t}=\varPhi x_{t-1}+\Upsilon u_{t}+w_{t} \tag{6.3}\]

\[y_{t}=A_{t}x_{t}+\Gamma u_{t}+v_{t} \tag{6.4}\]

where \(\Upsilon\) is \(p\times r\) and \(\Gamma\) is \(q\times r\); either of these matrices may be the zero matrix.

Figure 6.1: Diagram of a state space model

[MISSING_PAGE_FAIL:300]

[MISSING_PAGE_FAIL:301]

where

\[R=\text{var}\begin{pmatrix}v_{t1}\\ v_{t2}\end{pmatrix}=\begin{pmatrix}r_{11}&r_{12}\\ r_{21}&r_{22}\end{pmatrix}.\]

It is reasonable to suppose that the unknown common signal, \(x_{t}\), can be modeled as a random walk with drift of the form

\[x_{t}=\delta+x_{t-1}+w_{t}, \tag{6.9}\]

with \(Q=\text{var}(w_{t})\). In terms of the model (6.3)-(6.4), this example has, \(p=1,q=2\), \(\varPhi=1\), and \(\gamma^{\prime}=\delta\) with \(u_{t}\equiv 1\).

The introduction of the state-space approach as a tool for modeling data in the social and biological sciences requires model identification and parameter estimation because there is rarely a well-defined differential equation describing the state transition. The questions of general interest for the dynamic linear model (6.3) and (6.4) relate to estimating the unknown parameters contained in \(\varPhi,\gamma,Q,\Gamma,A_{t}\), and \(R\), that define the particular model, and estimating or forecasting values of the underlying unobserved process \(x_{t}\). The advantages of the state-space formulation are in the ease with which we can treat various missing data configurations and in the incredible array of models that can be generated from (6.3) and (6.4). The analogy between the observation matrix \(A_{t}\) and the design matrix in the usual regression and analysis of variance setting is a useful one. We can generate fixed and random effect structures that are either constant or vary over time simply by making appropriate choices for the matrix \(A_{t}\) and the transition structure \(\varPhi\).

Before continuing our investigation of the general model, it is instructive to consider a simple univariate state-space model wherein an AR(1) process is observed using a noisy instrument.

**Example 6.3**: **An AR(1) Process with Observational Noise**

Consider a univariate state-space model where the observations are noisy,

\[y_{t}=x_{t}+v_{t}, \tag{6.10}\]

and the signal (state) is an AR(1) process,

\[x_{t}=\phi x_{t-1}+w_{t}, \tag{6.11}\]

where \(v_{t}\!\sim\!\text{iid}\,\,\,\text{N}(0,\sigma_{v}^{2})\), \(w_{t}\!\sim\!\text{iid}\,\,\,\text{N}(0,\sigma_{w}^{2})\), and \(x_{0}\sim\text{N}\big{(}0,\frac{\sigma_{w}^{2}}{1-\phi^{2}}\big{)}\); \(\{v_{t}\}\), \(\{w_{t}\}\), and \(x_{0}\) are independent, and \(t=1,2,\ldots\).

In Chap. 3, we investigated the properties of the state, \(x_{t}\), because it is a stationary AR(1) process (recall Problem 3.2). For example, we know the autocovariance function of \(x_{t}\) is

\[\gamma_{x}(h)=\frac{\sigma_{w}^{2}}{1-\phi^{2}}\,\,\phi^{h},\quad h=0,1,2, \ldots. \tag{6.12}\]

But here, we must investigate how the addition of observation noise affects the dynamics. Although it is not a necessary assumption, we have assumed in this example that \(x_{t}\) is stationary. In this case, the observations are also stationary because \(y_{t}\) is the sum of two independent stationary components \(x_{t}\) and \(v_{t}\). We have

\[\gamma_{y}(0)=\text{var}(y_{t})=\text{var}(x_{t}+v_{t})=\frac{\sigma_{w}^{2}}{1- \phi^{2}}+\sigma_{v}^{2}, \tag{6.13}\]

and, when \(h\geq 1\),

\[\gamma_{y}(h)=\text{cov}(y_{t},y_{t-h})=\text{cov}(x_{t}+v_{t},x_{t-h}+v_{t-h}) =\gamma_{x}(h). \tag{6.14}\]

Consequently, for \(h\geq 1\), the ACF of the observations is

\[\rho_{y}(h)=\frac{\gamma_{y}(h)}{\gamma_{y}(0)}=\left(1+\frac{\sigma_{v}^{2}}{ \sigma_{w}^{2}}(1-\phi^{2})\right)^{-1}\phi^{h}. \tag{6.15}\]

It should be clear from the correlation structure given by (6.15) that the observations, \(y_{t}\), are not AR(1) unless \(\sigma_{v}^{2}=0\). In addition, the autocorrelation structure of \(y_{t}\) is identical to the autocorrelation structure of an ARMA(1,1) process, as presented in Example 3.14. Thus, the observations can also be written in an ARMA(1,1) form,

\[y_{t}=\phi y_{t-1}+\theta u_{t-1}+u_{t},\]

where \(u_{t}\) is Gaussian white noise with variance \(\sigma_{u}^{2}\), and with \(\theta\) and \(\sigma_{u}^{2}\) suitably chosen. We leave the specifics of this problem alone for now and defer the discussion to Sect. 6.6; in particular, see Example 6.11.

Although an equivalence exists between stationary ARMA models and stationary state-space models (see Sect. 6.6), it is sometimes easier to work with one form than another. As previously mentioned, in the case of missing data, complex multivariate systems, mixed effects, and certain types of nonstationarity, it is easier to work in the framework of state-space models.

### Filtering, Smoothing, and Forecasting

From a practical view, a primary aim of any analysis involving the state space model, (6.3)-(6.4), would be to produce estimators for the underlying unobserved signal \(x_{t}\), given the data \(y_{1:s}=\{y_{1},\ldots,y_{s}\}\), to time \(s\). As will be seen, state estimation is an essential component of parameter estimation. When \(s<t\), the problem is called _forecasting_ or _prediction_. When \(s=t\), the problem is called _filtering_, and when \(s>t\), the problem is called _smoothing_. In addition to these estimates, we would also want to measure their precision. The solution to these problems is accomplished via the _Kalman filter and smoother_ and is the focus of this section.

Throughout this chapter, we will use the following definitions:

\[x_{t}^{s}=\text{E}(x_{t}\mid y_{1:s}) \tag{6.16}\]and

\[P^{s}_{t_{1},t_{2}}=\mathrm{E}\left\{(x_{t_{1}}-x^{s}_{t_{1}})(x_{t_{2}}-x^{s}_{t_ {2}})^{\prime}\right\}. \tag{6.17}\]

When \(t_{1}=t_{2}\) (\(=t\) say) in (6.17), we will write \(P^{s}_{t}\) for convenience.

In obtaining the filtering and smoothing equations, we will rely heavily on the Gaussian assumption. Some knowledge of the material covered in Appendix B will be helpful in understanding the details of this section (although these details may be skipped on a casual reading of the material). Even in the non-Gaussian case, the estimators we obtain are the minimum mean-squared error estimators within the class of linear estimators. That is, we can think of \(\mathrm{E}\) in (6.16) as the projection operator in the sense of Sect. B.1 rather than expectation and \(y_{1:s}\) as the space of linear combinations of \(\{y_{1},\ldots,y_{s}\}\); in this case, \(P^{s}_{t}\) is the corresponding mean-squared error. Since the processes are Gaussian, (6.17) is also the conditional error covariance; that is,

\[P^{s}_{t_{1},t_{2}}=\mathrm{E}\left\{(x_{t_{1}}-x^{s}_{t_{1}})(x_{t_{2}}-x^{s} _{t_{2}})^{\prime}\ \big{|}\ y_{1:s}\right\}.\]

This fact can be seen, for example, by noting the covariance matrix between \((x_{t}-x^{s}_{t})\) and \(y_{1:s}\), for any \(t\) and \(s\), is zero; we could say they are orthogonal in the sense of Sect. B.1. This result implies that \((x_{t}-x^{s}_{t})\) and \(y_{1:s}\) are independent (because of the normality), and hence, the conditional distribution of \((x_{t}-x^{s}_{t})\) given \(y_{1:s}\) is the unconditional distribution of \((x_{t}-x^{s}_{t})\). Derivations of the filtering and smoothing equations from a Bayesian perspective are given in Meinhold and Singpurwalla [139]; more traditional approaches based on the concept of projection and on multivariate normal distribution theory are given in Jazwinski [105] and Anderson and Moore [5].

First, we present the Kalman filter, which gives the filtering and forecasting equations. The name filter comes from the fact that \(x^{t}_{t}\) is a linear filter of the observations \(y_{1:t}\); that is, \(x^{t}_{t}=\sum_{s=1}^{t}B_{s}y_{s}\) for suitably chosen \(p\times q\) matrices \(B_{s}\). The advantage of the Kalman filter is that it specifies how to update the filter from \(x^{t-1}_{t-1}\) to \(x^{t}_{t}\) once a new observation \(y_{t}\) is obtained, without having to reprocess the entire data set \(y_{1:t}\).

**Property 6.1**: _The Kalman Filter_

_For the state-space model specified in (6.3) and (6.4), with initial conditions \(x^{0}_{0}=\mu_{0}\) and \(P^{0}_{0}=\Sigma_{0}\), for \(t=1,\ldots,n\),_

\[x^{t-1}_{t}=\Phi x^{t-1}_{t-1}+\gamma u_{t}, \tag{6.18}\]

\[P^{t-1}_{t}=\Phi P^{t-1}_{t-1}\Phi^{\prime}+Q, \tag{6.19}\]

_with_

\[x^{t}_{t}=x^{t-1}_{t}+K_{t}(y_{t}-A_{t}x^{t-1}_{t}-\Gamma u_{t}), \tag{6.20}\]

\[P^{t}_{t}=[I-K_{t}A_{t}]P^{t-1}_{t(prediction errors)_

\[\epsilon_{t}=y_{t}-\mathrm{E}(y_{t}\ \big{|}\ y_{1:t-1})=y_{t}-A_{t}x_{t}^{t-1}- \Gamma u_{t}, \tag{6.23}\]

_and the corresponding variance-covariance matrices_

\[\Sigma_{t}\stackrel{{\mathrm{def}}}{{=}}\mathrm{var}(\epsilon_{t}) \ =\mathrm{var}[A_{t}(x_{t}-x_{t}^{t-1})+v_{t}]=A_{t}P_{t}^{t-1}A_{t}^{\prime}+R \tag{6.24}\]

_for \(t=1,\ldots,n\). We assume that \(\Sigma_{t}>0\) (is positive definite), which is guaranteed, for example, if \(R>0\). This assumption is not necessary and may be relaxed._

_Proof:_ The derivations of (6.18) and (6.19) follow from straight forward calculations, because from (6.3) we have

\[x_{t}^{t-1}=\mathrm{E}(x_{t}\ |\ y_{1:t-1})=\mathrm{E}(\Phi x_{t-1}+\gamma u_{ t}+w_{t}\ |\ y_{1:t-1})=\Phi x_{t-1}^{t-1}+\gamma u_{t},\]

and thus

\[P_{t}^{t-1} =\mathrm{E}\left\{(x_{t}-x_{t}^{t-1})(x_{t}-x_{t}^{t-1})^{\prime}\right\}\] \[=\mathrm{E}\left\{\left[\Phi(x_{t-1}-x_{t-1}^{t-1})+w_{t}\right] \left[\Phi(x_{t-1}-x_{t-1}^{t-1})+w_{t}\right]^{\prime}\right\}\] \[=\Phi P_{t-1}^{t-1}\Phi^{\prime}+Q.\]

To derive (6.20), we note that \(\mathrm{cov}(\epsilon_{t},y_{s})=0\) for \(s<t\), which in view of the fact the innovation sequence is a Gaussian process, implies that the innovations are independent of the past observations. Furthermore, the conditional covariance between \(x_{t}\) and \(\epsilon_{t}\) given \(y_{1:t-1}\) is

\[\mathrm{cov}(x_{t},\ \epsilon_{t}\ |\ y_{1:t-1}) =\mathrm{cov}(x_{t},\ y_{t}-A_{t}x_{t}^{t-1}-\Gamma u_{t}\ |\ y_{1:t-1})\] \[=\mathrm{cov}(x_{t}-x_{t}^{t-1},\ y_{t}-A_{t}x_{t}^{t-1}-\Gamma u _{t}\ |\ y_{1:t-1})\] \[=\mathrm{cov}[x_{t}-x_{t}^{t-1},\ A_{t}(x_{t}-x_{t}^{t-1})+v_{t}]\] \[=P_{t}^{t-1}A_{t}^{\prime}. \tag{6.25}\]

Using these results we have that the joint conditional distribution of \(x_{t}\) and \(\epsilon_{t}\) given \(y_{1:t-1}\) is normal

\[\begin{pmatrix}x_{t}\\ \epsilon_{t}\end{pmatrix}\ \Big{|}\ y_{1:t-1}\sim\mathrm{N}\left(\begin{bmatrix}x_{t}^{t-1}\\ 0\end{bmatrix},\ \begin{bmatrix}P_{t}^{t-1}&P_{t}^{t-1}A_{t}^{\prime}\\ A_{t}P_{t}^{t-1}&\Sigma_{t}\end{bmatrix}\right). \tag{6.26}\]

Thus, using (B.9) of Appendix B, we can write

\[x_{t}^{t}=\mathrm{E}(x_{t}\ \big{|}\ y_{1:t})=\mathrm{E}(x_{t}\ \big{|}\ y_{1:t-1},\epsilon_{t})=x_{t}^{t-1}+K_{t}\epsilon_{t}, \tag{6.27}\]

where

\[K_{t}=P_{t}^{t-1}A_{t}^{\prime}\Sigma_{t}^{-1}=P_{t}^{t-1}A_{t}^{\prime}(A_{t} P_{t}^{t-1}A_{t}^{\prime}+R)^{-1}.\]The evaluation of \(P^{t}_{t}\) is easily computed from (6.26) [see (B.10)] as

\[P^{t}_{t}=\operatorname{cov}\left(x_{t}\ \big{|}\ y_{1:t-1},\epsilon_{t}\right)=P^{t-1}_{t}-P^{t-1}_{t}A^{\prime}_{t}\Sigma^{-1}_{t}A_{t}P^{t-1}_{t},\]

which simplifies to (6.21). 

Nothing in the proof of Property 6.1 precludes the cases where some or all of the parameters vary with time, or where the observation dimension changes with time, which leads to the following corollary.

**Corollary 6.1**: **Kalman Filter: The Time-Varying Case**__

_If, in (6.3) and (6.4), any or all of the parameters are time dependent, \(\varPhi=\varPhi_{t},\ \varUpsilon=\varUpsilon_{t},\ Q=Q_{t}\) in the state equation or \(\varGamma=\varGamma_{t},\ R=R_{t}\) in the observation equation, or the dimension of the observational equation is time dependent, \(q=q_{t},\) Property 6.1 holds with the appropriate substitutions._

Next, we explore the model, prediction, and filtering from a density point of view. To ease the notation, we will drop the inputs from the model. There are two key ingredients to the state space model. Letting \(\operatorname{p}\!\!\operatorname{\rho}(\cdot)\) denote a generic density function with parameters represented by \(\varTheta\), we have the state process is Markovian:

\[\operatorname{p}\!\!\operatorname{\rho}(x_{t}\ \big{|}\ x_{t-1},x_{t-2},\ldots,x_{0})=\operatorname{p}\!\!\operatorname{\rho}(x_{t}\ \big{|}\ x_{t-1}), \tag{6.28}\]

and the observations are conditionally independent given the states:

\[\operatorname{p}\!\!\operatorname{\rho}(y_{1:n}\ \big{|}\ x_{1:n})=\prod_{t=1}^{n}\operatorname{p}\!\!\operatorname{\rho}(y_{t}\ \big{|}\ x_{t}), \tag{6.29}\]

Since we are focusing on the linear Gaussian model, if we let \(\mathfrak{g}(x;\ \mu,\varSigma)\) denote a multivariate normal density with mean \(\mu\) and covariance matrix \(\varSigma\) as given in (1.33), then

\[\operatorname{p}\!\!\operatorname{\rho}(x_{t}\ \big{|}\ x_{t-1})=\mathfrak{g}(x_{t};\ \varPhi x_{t-1},Q)\quad\text{and}\quad\operatorname{p}\!\!\operatorname{\rho}(y_{t}\ \big{|}\ x_{t})=\mathfrak{g}(y_{t};\ A_{t}x_{t},R)\,.\]

with initial condition \(\operatorname{p}\!\!\operatorname{\rho}(x_{0})=\mathfrak{g}(x_{0};\ \mu_{0},\varSigma_{0})\).

In terms of densities, the Kalman filter can be seen as a simple updating scheme, where, to determine the forecast densities, we have,

\[\operatorname{p}\!\!\operatorname{\rho}(x_{t}\ \big{|}\ y_{1:t-1}) =\int_{\mathbb{R}^{P}}\operatorname{p}\!\!\operatorname{\rho}(x_{t},x_{t-1}\ \big{|}\ y_{1:t-1})\,dx_{t-1}\] \[=\int_{\mathbb{R}^{P}}\operatorname{p}\!\!\operatorname{\rho}(x_{t} \ \big{|}\ x_{t-1})\operatorname{p}\!\!\operatorname{\rho}(x_{t-1}\ \big{|}\ y_{1:t-1})\,dx_{t-1}\] \[=\int_{\mathbb{R}^{P}}\mathfrak{g}(x_{t};\ \varPhi x_{t-1},Q)\,\mathfrak{g}(x_{t-1};\ x_{t-1}^{t-1},P^{t-1}_{t-1})\,dx_{t-1}\] \[=\mathfrak{g}(x_{t};\ x_{t}^{t-1},\ P^{t-1}_{t}), \tag{6.30}\]

where the values of \(x_{t}^{t-1}\) and \(P^{t-1}_{t}\) are given in (6.18) and (6.19). These values are obtained upon evaluating the integral using the usual trick of completing the square; see Example 6.4. Since we were seeking an iterative procedure, we introduced \(x_{t-1}\) in (6.30) because we have (presumably) previously evaluated the filter density \(\mathsf{p}_{\Theta}(x_{t-1}\bigm{|}y_{1:t-1})\). Once we have the predictor, the filter density is obtained as

\[\mathsf{p}_{\Theta}(x_{t}\bigm{|}y_{1:t}) =\mathsf{p}_{\Theta}(x_{t}\bigm{|}y_{t},y_{1:t-1})\propto\mathsf{ p}_{\Theta}(y_{t}\bigm{|}x_{t})\,\mathsf{p}_{\Theta}(x_{t}\bigm{|}y_{1:t-1}),\] \[=\mathfrak{g}(y_{t};\,A_{t}x_{t},R)\,\mathfrak{g}(x_{t};\,x_{t}^{t -1},P_{t}^{t-1})\,, \tag{6.31}\]

from which we deduce is \(\mathfrak{g}(x_{t};\,x_{t}^{t},P_{t}^{t})\) where \(x_{t}^{t}\) and \(P_{t}^{t}\) are given in (6.20) and (6.21). The following example illustrates these ideas for a simple univariate case.

**Example 6.4**: **Local Level Model**

In this example, we suppose that we observe a univariate series \(y_{t}\) that consists of a trend component, \(\mu_{t}\), and a noise component, \(v_{t}\), where

\[y_{t}=\mu_{t}+v_{t} \tag{6.32}\]

and \(v_{t}\sim\text{iid N}(0,\sigma_{v}^{2})\). In particular, we assume the trend is a random walk given by

\[\mu_{t}=\mu_{t-1}+w_{t} \tag{6.33}\]

where \(w_{t}\sim\text{iid N}(0,\sigma_{w}^{2})\) is independent of \(\{v_{t}\}\). Recall Example 6.2, where we suggested this type of trend model for the global temperature series.

The model is, of course, a state-space model with (6.32) being the observation equation, and (6.33) being the state equation. We will use the following notation introduced in Blight [24]. Let

\[\{x;\,\mu,\sigma^{2}\}=\exp\left\{-\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\}, \tag{6.34}\]

then simple manipulation shows

\[\{x;\,\mu,\sigma^{2}\}=\{\mu;\,x,\sigma^{2}\} \tag{6.35}\]

and by completing the square,

\[\{x;\,\mu_{1},\sigma_{1}^{2}\}\{x;\,\mu_{2},\sigma_{2}^{2}\} =\left\{x;\,\frac{\mu_{1}/\sigma_{1}^{2}+\mu_{2}/\sigma_{2}^{2}} {1/\sigma_{1}^{2}+1/\sigma_{2}^{2}},(1/\sigma_{1}^{2}+1/\sigma_{2}^{2})^{-1}\right\} \tag{6.36}\] \[\quad\times\,\big{\{}\mu_{1};\,\mu_{2},\sigma_{1}^{2}+\sigma_{2} ^{2}\big{\}}\,.\]

Thus, using (6.30), (6.35) and (6.36) we have

\[p(\mu_{t}\bigm{|}y_{1:t-1}) \propto\int\big{\{}\mu_{t};\,\mu_{t-1},\sigma_{w}^{2}\big{\}}\, \big{\{}\mu_{t-1};\,\mu_{t-1}^{t-1},P_{t-1}^{t-1}\big{\}}\,d\mu_{t-1}\] \[=\,\int\big{\{}\mu_{t-1};\,\mu_{t},\sigma_{w}^{2}\big{\}}\,\big{\{} \mu_{t-1};\,\mu_{t-1}^{t-1},P_{t-1}^{t-1}\big{\}}\,d\mu_{t-1}\] \[=\,\big{\{}\mu_{t};\,\mu_{t-1}^{t-1},\,P_{t-1}^{t-1}+\sigma_{w}^{ 2}\big{\}}\,. \tag{6.37}\]From (6.37) we conclude that

\[\mu_{t}\ |\ y_{1:t-1}\sim\mathrm{N}(\mu_{t}^{t-1},P_{t}^{t-1}) \tag{6.38}\]

where

\[\mu_{t}^{t-1}=\mu_{t-1}^{t-1}\quad\text{and}\quad P_{t}^{t-1}=P_{t-1}^{t-1}+ \sigma_{w}^{2} \tag{6.39}\]

which agrees with the first part of Property 6.1. To derive the filter density using (6.31) and (6.35) we have

\[p(\mu_{t}\ \big{|}\ y_{1:t}) \propto\big{\{}y_{t};\ \mu_{t},\sigma_{v}^{2}\big{\}}\ \big{\{}\mu_{t};\ \mu_{t}^{t-1},P_{t}^{t-1}\big{\}}\] \[=\ \big{\{}\mu_{t};\ y_{t},\sigma_{v}^{2}\big{\}}\ \big{\{}\mu_{t};\ \mu_{t}^{t-1},P_{t}^{t-1}\big{\}}. \tag{6.40}\]

An application of (6.36) gives

\[\mu_{t}\ |\ y_{1:t}\sim\mathrm{N}(\mu_{t}^{t},P_{t}^{t}) \tag{6.41}\]

with

\[\mu_{t}^{t}=\frac{\sigma_{v}^{2}\mu_{t}^{t-1}}{P_{t}^{t-1}+\sigma_{v}^{2}}+ \frac{P_{t}^{t-1}y_{t}}{P_{t}^{t-1}+\sigma_{v}^{2}}=\mu_{t}^{t-1}+K_{t}(y_{t}-\mu_{t}^{t-1}), \tag{6.42}\]

where we have defined

\[K_{t}=\frac{P_{t}^{t-1}}{P_{t}^{t-1}+\sigma_{v}^{2}}, \tag{6.43}\]

and

\[P_{t}^{t}=\left(\frac{1}{\sigma_{v}^{2}}+\frac{1}{P_{t}^{t-1}}\right)^{-1}= \frac{\sigma_{v}^{2}P_{t}^{t-1}}{P_{t}^{t-1}+\sigma_{v}^{2}}=(1-K_{t})P_{t}^{t-1}. \tag{6.44}\]

The filter for this specific case, of course, agrees with Property 6.1.

Next, we consider the problem of obtaining estimators for \(x_{t}\) based on the entire data sample \(y_{1},\ldots,y_{n}\), where \(t\leq n\), namely, \(x_{t}^{n}\). These estimators are called smoothers because a time plot of the sequence \(\{x_{t}^{n};\ t=1,\ldots,n\}\) is typically smoother than the forecasts \(\{x_{t}^{t-1};\ t=1,\ldots,n\}\) or the filters \(\{x_{t}^{t};\ t=1,\ldots,n\}\). As is obvious from the above remarks, smoothing implies that each estimated value is a function of the present, future, and past, whereas the filtered estimator depends on the present and past. The forecast depends only on the past, as usual.

**Property 6.2**: **The Kalman Smoother**__

_For the state-space model specified in (6.3) and (6.4), with initial conditions \(x_{n}^{n}\) and \(P_{n}^{n}\) obtained via Property 6.1, for \(t=n,n-1,\ldots,1\),_

\[x_{t-1}^{n}=x_{t-1}^{t-1}+J_{t-1}\left(x_{t}^{n}-x_{t}^{t-1}\right), \tag{6.45}\]

\[P_{t-1}^{n}=P_{t-1}^{t-1}+J_{t-1}\left(P_{t}^{n}-P_{t}^{t-1}\right)J_{t-1}^{ \prime}, \tag{6.46}\]

_where_

\[J_{t-1}=P_{t-1}^{t-1}\Phi^{\prime}\left[P_{t}^{t-1}\right]^{-1}. \tag{6.47}\]_Proof:_ The smoother can be derived in many ways. Here we provide a proof that was given in Ansley and Kohn [9]. First, for \(1\leq t\leq n\), define

\[y_{1:t-1}=\{y_{1},\ldots,y_{t-1}\}\quad\text{and}\quad\eta_{t}=\{v_{t},\ldots,v_ {n},w_{t+1},\ldots,w_{n}\},\]

with \(y_{1:0}\) being empty, and let

\[m_{t-1}=\mathrm{E}\{x_{t-1}\bigm{|}y_{1:t-1},\;x_{t}-x_{t}^{t-1},\;\eta_{t}\}.\]

Then, because \(y_{1:t-1}\), \(\{x_{t}-x_{t}^{t-1}\}\), and \(\eta_{t}\) are mutually independent, and \(x_{t-1}\) and \(\eta_{t}\) are independent, using (B.9) we have

\[m_{t-1}=x_{t-1}^{t-1}+J_{t-1}(x_{t}-x_{t}^{t-1}), \tag{6.48}\]

where

\[J_{t-1}=\mathrm{cov}(x_{t-1},x_{t}-x_{t}^{t-1})[P_{t}^{t-1}]^{-1}=P_{t-1}^{t-1} \Phi^{\prime}[P_{t}^{t-1}]^{-1}.\]

Finally, because \(y_{1:t-1}\), \(x_{t}-x_{t}^{t-1}\), and \(\eta_{t}\) generate \(y_{1:n}=\{y_{1},\ldots,y_{n}\}\),

\[x_{t-1}^{n}=\mathrm{E}\{x_{t-1}\bigm{|}y_{1:n}\}=\mathrm{E}\{m_{t-1}\bigm{|}y_{ 1:n}\}=x_{t-1}^{t-1}+J_{t-1}(x_{t}^{n}-x_{t}^{t-1}),\]

which establishes (6.45).

The recursion for the error covariance, \(P_{t-1}^{n}\), is obtained by straight-forward calculation. Using (6.45) we obtain

\[x_{t-1}-x_{t-1}^{n}=x_{t-1}-x_{t-1}^{t-1}-J_{t-1}\left(x_{t}^{n}-\Phi x_{t-1}^ {t-1}\right),\]

or

\[\left(x_{t-1}-x_{t-1}^{n}\right)+J_{t-1}x_{t}^{n}=\left(x_{t-1}-x_{t-1}^{t-1} \right)+J_{t-1}\Phi x_{t-1}^{t-1}. \tag{6.49}\]

Multiplying each side of (6.49) by the transpose of itself and taking expectation, we have

\[P_{t-1}^{n}+J_{t-1}\mathrm{E}(x_{t}^{n}x_{t}^{n^{\prime}})J_{t-1}^{\prime}=P_{ t-1}^{t-1}+J_{t-1}\Phi\mathrm{E}(x_{t-1}^{t-1}x_{t-1}^{t-1^{\prime}})\Phi^{ \prime}J_{t-1}^{\prime}, \tag{6.50}\]

using the fact the cross-product terms are zero. But,

\[\mathrm{E}(x_{t}^{n}x_{t}^{n^{\prime}})=\mathrm{E}(x_{t}x_{t}^{\prime})-P_{t}^ {n}=\Phi\mathrm{E}(x_{t-1}x_{t-1}^{\prime})\Phi^{\prime}+Q-P_{t}^{n},\]

and

\[\mathrm{E}(x_{t-1}^{t-1}x_{t-1}^{t-1^{\prime}})=\mathrm{E}(x_{t-1}x_{t-1}^{ \prime})-P_{t-1}^{t-1},\]

so (6.50) simplifies to (6.46).

**Example 6.5**: **Prediction, Filtering and Smoothing for the Local Level Model**

For this example, we simulated \(n=50\) observations from the local level trend model discussed in Example 6.4. We generated a random walk

\[\mu_{t}=\mu_{t-1}+w_{t} \tag{6.51}\]

with \(w_{t}\sim\text{iid N}(0,1)\) and \(\mu_{0}\sim\text{N}(0,1)\). We then supposed that we observe a univariate series \(y_{t}\) consisting of the trend component, \(\mu_{t}\), and a noise component, \(v_{t}\sim\text{iid N}(0,1)\), where

\[y_{t}=\mu_{t}+v_{t}. \tag{6.52}\]

The sequences \(\{w_{t}\}\), \(\{v_{t}\}\) and \(\mu_{0}\) were generated independently. We then ran the Kalman filter and smoother, Property 6.1 and Property 6.2, using the actual parameters. The top panel of Fig. 6.4 shows the actual values of\(\mu_{t}\) as points, and the predictions \(\mu_{t}^{t-1}\), for \(t=1,2,\ldots,50\), superimposed on the graph as a line. In addition, we display \(\mu_{t}^{t-1}\pm 2\sqrt{P_{t}^{t-1}}\) as dashed lines on the plot. The middle panel displays the filter, \(\mu_{t}^{t}\), for \(t=1,\ldots,50\), as a line with \(\mu_{t}^{t}\pm 2\sqrt{P_{t}^{t}}\) as dashed lines. The bottom panel of Fig. 6.4 shows a similar plot for the smoother \(\mu_{t}^{n}\).

Table 6.1 shows the first 10 observations as well as the corresponding state values, the predictions, filters and smoothers. Note that one-step-ahead prediction

[MISSING_PAGE_FAIL:311]

lines(ks$xs-2*sqrt(ks$Ps), lty=2, col=4) mu[1]; ks$x@n; sqrt(ks$P@n) # initial value info

When we discuss maximum likelihood estimation via the EM algorithm in the next section, we will need a set of recursions for obtaining \(P^{n}_{t,t-1}\), as defined in (6.17). We give the necessary recursions in the following property.

**Property 6.3**: **The Lag-One Covariance Smoother**__

_For the state-space model specified in (6.3) and (6.4), with \(K_{t}\), \(J_{t}\) (\(t=1,\ldots,n\)), and \(P^{n}_{n}\) obtained from Property 6.1 and Property 6.2, and with initial condition_

\[P^{n}_{n,n-1}=(I-K_{n}A_{n})\Phi P^{n-1}_{n-1}, \tag{6.53}\]

_for \(t=n,n-1,\ldots,2\),_

\[P^{n}_{t-1,t-2}=P^{t-1}_{t-1}J^{\prime}_{t-2}+J_{t-1}\left(P^{n}_{t,t-1}-\Phi P ^{t-1}_{t-1}\right)J^{\prime}_{t-2}. \tag{6.54}\]

_Proof:_ Because we are computing covariances, we may assume \(u_{t}\equiv 0\) without loss of generality. To derive the initial term (6.53), we first define

\[\tilde{x}^{s}_{t}=x_{t}-x^{s}_{t}.\]

Then, using (6.20) and (6.45), we write

\[P^{t}_{t,t-1} = \mathrm{E}\left(\tilde{x}^{t}_{t}\ \tilde{x}^{t^{\prime}}_{t-1}\right)\] \[= \mathrm{E}\left\{[\tilde{x}^{t-1}_{t}-K_{t}(y_{t}-A_{t}x^{t-1}_{t })][\tilde{x}^{t-1}_{t-1}-J_{t-1}K_{t}(y_{t}-A_{t}x^{t-1}_{t})]^{\prime}\right\}\] \[= \mathrm{E}\left\{[\tilde{x}^{t-1}_{t}-K_{t}(A_{t}\tilde{x}^{t-1} _{t}+v_{t})][\tilde{x}^{t-1}_{t-1}-J_{t-1}K_{t}(A_{t}\tilde{x}^{t-1}_{t}+v_{t}) ]^{\prime}\right\}.\]

Expanding terms and taking expectation, we arrive at

\[P^{t}_{t,t-1}=P^{t-1}_{t,t-1}-P^{t-1}_{t}A^{\prime}_{t}K^{\prime}_{t}J^{\prime }_{t-1}-K_{t}A_{t}P^{t-1}_{t,t-1}+K_{t}(A_{t}P^{t-1}_{t}A^{\prime}_{t}+R)K^{ \prime}_{t}J^{\prime}_{t-1},\]

noting \(\mathrm{E}(\tilde{x}^{t-1}_{t}v^{\prime}_{t})=0\). The final simplification occurs by realizing that \(K_{t}(A_{t}P^{t-1}_{t}A^{\prime}_{t}+R)=P^{t-1}_{t}A^{\prime}_{t}\), and \(P^{t-1}_{t,t-1}=\Phi P^{t-1}_{t-1}\). These relationships hold for any \(t=1,\ldots,n\), and (6.53) is the case \(t=n\).

We give the basic steps in the derivation of (6.54). The first step is to use (6.45) to write

\[\tilde{x}^{n}_{t-1}+J_{t-1}x^{n}_{t}=\tilde{x}^{t-1}_{t-1}+J_{t-1}\Phi x^{t-1} _{t-1} \tag{6.55}\]

and

\[\tilde{x}^{n}_{t-2}+J_{t-2}x^{n}_{t-1}=\tilde{x}^{t-2}_{t-2}+J_{t-2}\Phi x^{t- 2}_{t-2}. \tag{6.56}\]

Next, multiply the left-hand side of (6.55) by the transpose of the left-hand side of (6.56), and equate that to the corresponding result of the right-hand sides of (6.55) and (6.56). Then, taking expectation of both sides, the left-hand side result reduces to

\[P^{n}_{t-1,t-2}+J_{t-1}\mathrm{E}(x^{n}_{t}x^{n^{\prime}}_{t-1})J^{\prime}_{t-2} \tag{6.57}\]and the right-hand side result reduces to

\[\begin{split} P_{t-1,t-2}^{t-2}&-K_{t-1}A_{t-1}P_{t-1,t- 2}^{t-2}+J_{t-1}\Phi K_{t-1}A_{t-1}P_{t-1,t-2}^{t-2}\\ &+J_{t-1}\Phi\mathrm{E}(x_{t-1}^{t-1}x_{t-2}^{t-2^{\prime}})\Phi^ {\prime}J_{t-2}^{\prime}.\end{split} \tag{6.58}\]

In (6.57), write

\[\mathrm{E}(x_{t}^{n}x_{t-1}^{n^{\prime}})=\mathrm{E}(x_{t}x_{t-1}^{\prime})-P_{ t,t-1}^{n}=\Phi\mathrm{E}(x_{t-1}x_{t-2}^{\prime})\Phi^{\prime}+\Phi Q-P_{t,t-1} ^{n},\]

and in (6.58), write

\[\mathrm{E}(x_{t-1}^{t-1}x_{t-2}^{t-2^{\prime}})=\mathrm{E}(x_{t-1}^{t-2}x_{t- 2}^{t-2^{\prime}})=\mathrm{E}(x_{t-1}x_{t-2}^{\prime})-P_{t-1,t-2}^{t-2}.\]

Equating (6.57) to (6.58) using these relationships and simplifying the result leads to (6.54). 

### Maximum Likelihood Estimation

Estimation of the parameters that specify the state space model, (6.3) and (6.4), is quite involved. We use \(\Theta\) to represent the vector of unknown parameters in the initial mean and covariance \(\mu_{0}\) and \(\Sigma_{0}\), the transition matrix \(\Phi\), and the state and observation covariance matrices \(Q\) and \(R\) and the input coefficient matrices, \(\gamma^{\prime}\) and \(\Gamma\). We use maximum likelihood under the assumption that the initial state is normal, \(x_{0}\sim\mathrm{N}_{p}(\mu_{0},\Sigma_{0})\), and the errors are normal, \(w_{t}\sim\mathrm{iid}\ \mathrm{N}_{p}(0,Q)\) and \(v_{t}\sim\mathrm{iid}\ \mathrm{N}_{q}(0,R)\). We continue to assume, for simplicity, \(\{w_{t}\}\) and \(\{v_{t}\}\) are uncorrelated.

The likelihood is computed using the _innovations_\(\epsilon_{1},\epsilon_{2},\ldots,\epsilon_{n}\), defined by (6.23),

\[\epsilon_{t}=y_{t}-A_{t}x_{t}^{t-1}-\Gamma u_{t}.\]

The innovations form of the likelihood of the data \(y_{1:n}\), which was first given by Schweppe [176], is obtained using an argument similar to the one leading to (3.117) and proceeds by noting the innovations are independent Gaussian random vectors with zero means and, as shown in (6.24), covariance matrices

\[\Sigma_{t}=A_{t}P_{t}^{t-1}A_{t}^{\prime}+R. \tag{6.59}\]

Hence, ignoring a constant, we may write the likelihood, \(L_{Y}(\Theta)\), as

\[-\ln L_{Y}(\Theta)=\frac{1}{2}\sum_{t=1}^{n}\ln|\Sigma_{t}(\Theta)|+\frac{1}{2 }\sum_{t=1}^{n}\epsilon_{t}(\Theta)^{\prime}\Sigma_{t}(\Theta)^{-1}\epsilon_{t }(\Theta), \tag{6.60}\]

where we have emphasized the dependence of the innovations on the parameters \(\Theta\). Of course, (6.60) is a highly nonlinear and complicated function of the unknown parameters. The usual procedure is to fix \(x_{0}\) and then develop a set of recursions for the log likelihood function and its first two derivatives (for example, Gupta and Mehra [84]). Then, a Newton-Raphson algorithm (see 3.30) can be used successively to update the parameter values until the negative of the log likelihood is minimized. This approach is advocated, for example, by Jones [107], who developed ARMA estimation by putting the ARMA model in state-space form. For the univariate case, (6.60) is identical, in form, to the likelihood for the ARMA model given in (3.117).

The steps involved in performing a Newton-Raphson estimation procedure are as follows.

1. Select initial values for the parameters, say, \(\Theta^{(0)}\).
2. Run the Kalman filter, Property 6.1, using the initial parameter values, \(\Theta^{(0)}\), to obtain a set of innovations and error covariances, say, \(\{\epsilon^{(0)}_{t};\ t=1,\ldots,n\}\) and \(\{\Sigma^{(0)}_{t};\ t=1,\ldots,n\}\).
3. Run one iteration of a Newton-Raphson procedure with \(-\ln L_{Y}(\Theta)\) as the criterion function (refer to Example 3.30 for details), to obtain a new set of estimates, say \(\Theta^{(1)}\).
4. At iteration \(j\), (\(j=1,2,\ldots\)), repeat step 2 using \(\Theta^{(j)}\) in place of \(\Theta^{(j-1)}\) to obtain a new set of innovation values \(\{\epsilon^{(j)}_{t};\ t=1,\ldots,n\}\) and \(\{\Sigma^{(j)}_{t};\ t=1,\ldots,n\}\). Then repeat step 3 to obtain a new estimate \(\Theta^{(j+1)}\). Stop when the estimates or the likelihood stabilize; for example, stop when the values of \(\Theta^{(j+1)}\) differ from \(\Theta^{(j)}\), or when \(L_{Y}(\Theta^{(j+1)})\) differs from \(L_{Y}(\Theta^{(j)})\), by some predetermined, but small amount.

**Example 6.6**: **Newton-Raphson for Example 6.3**

In this example, we generated \(n=100\) observations, \(y_{1:100}\), from the AR with noise model given in Example 6.3, to perform a Newton-Raphson estimation of the parameters \(\phi\), \(\sigma^{2}_{w}\), and \(\sigma^{2}_{v}\). In the notation of Sect. 6.2, we would have \(\Phi=\phi\), \(Q=\sigma^{2}_{w}\) and \(R=\sigma^{2}_{v}\). The actual values of the parameters are \(\phi=.8\), \(\sigma^{2}_{w}=\sigma^{2}_{v}=1\).

In the simple case of an AR(1) with observational noise, initial estimation can be accomplished using the results of Example 6.3. For example, using (6.15), we set

\[\phi^{(0)}=\hat{\rho}_{y}(2)/\hat{\rho}_{y}(1).\]

Similarly, from (6.14), \(\gamma_{x}(1)=\gamma_{y}(1)=\phi\sigma^{2}_{w}/(1-\phi^{2})\), so that, initially, we set

\[\sigma^{2^{(0)}}_{w}=(1-\phi^{(0)^{2}})\hat{\gamma}_{y}(1)/\phi^{(0)}.\]

Finally, using (6.13) we obtain an initial estimate of \(\sigma^{2}_{v}\), namely,

\[\sigma^{2^{(0)}}_{v}=\hat{\gamma}_{y}(0)-[\sigma^{2^{(0)}}_{w}/(1-\phi^{(0)^{ 2}})].\]

Newton-Raphson estimation was accomplished using the R program optim. The code used for this example is given below. In that program, we must provide an evaluation of the function to be minimized, namely, \(-\ln L_{Y}(\Theta)\). In this case, the function call combines steps 2 and 3, using the current values of the parameters, \(\Theta^{(j-1)}\), to obtain first the filtered values, then the innovation values, and then calculating the criterion function, \(-\ln L_{Y}(\Theta^{(j-1)})\), to be minimized. We can also provide analytic forms of the gradient or _score vector_, \(-\partial\ln L_{Y}(\Theta)/\partial\Theta\), and the _Hessian matrix_, \(-\partial^{2}\ln L_{Y}(\Theta)/\partial\Theta\ \partial\Theta^{\prime}\), in the optimization routine, or allow the program to calculate these values numerically. In this example, we let the program proceed numerically and we note the need to be cautious when calculating gradients numerically. It it suggested in Press et al. [156, Ch. 10] that it is better to use numerical methods for the derivatives, at least for the Hessian, along with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method. Details on the gradient and Hessian are provided in Problem 6.9 and Problem 6.10; see Gupta and Mehra [84].

```
#GenerateData set.seed(999);num=100 x=arima.sim(n-num+1,list(ar=.8),sd=1) y=ts(x[-1]+rnorm(num,0,1))
#InitialEstimates u=ts.intersect(y,lag(y,-1),lag(y,-2)) varu=var(u);coru=cor(u) phi=coru[1,3]/coru[1,2] q=(1-phi*2)*varu[1,2]/phi r=varu[1,1]-q/(1-phi*2) (init.par=c(phi,sqrt(q),sqrt(r)))#=.91,.51,1.03
#Functiontoevaluatethelikelihood Linn=function(para){ phi=para[1];sigw=para[2];sigv=para[3] Sigma0=(sigw*2)/(1-phi*2);Sigma0[Sigma0[0]=0 kf=Kfilter0(num,y,1,mu0=0,Sigma0,phi,sigw,sigv) return(kfSlike)}
#Estimation(partialoutputshown) (est=optim(init.par,Linn,gr=NULL,method='BFGS',hessian=TRUE, control=list(trace=1,REPORT=1))) SE=sqrt(diag(solve(estShessian))) cbind(estimate=c(phi=estpar[1],sigw=estSpar[2],sigv=estSpar[3]),SE) estimateSE phi0.8140.081 sigw0.8510.175 sigv0.8740.143 As seen from the output, the final estimates, along with their standard errors (in parentheses), are \(\hat{\phi}=.81\) (.08), \(\hat{\sigma}_{w}=.85\) (.18), \(\hat{\sigma}_{v}=.87\) (.14). The report from optim yielded the following results of the estimation procedure: initialvalue81.313627 iter2value80.169051 iter3value79.866131 iter4value79.222846 iter5value79.021504 iter6value79.014723 iter7value79.014453 iter7value79.014452 iter7value79.014452 iter8value79.014452 finalvalue79.014452 converged
```

Note that the algorithm converged in seven steps with the final value of the negative of the log likelihood being 79.014452. The standard errors are a byproduct of the estimation procedure, and we will discuss their evaluation later in this section, after Property 6.4.

**Example 6.7**: **Newton-Raphson for the Global Temperature Deviations**

In Example 6.2, we considered two different global temperature series of \(n=136\) observations each, and they are plotted in Figure 6.3. In that example, we argued that both series should be measuring the same underlying climatic signal, \(x_{t}\), which we model as a random walk with drift,

\[x_{t}=\delta+x_{t-1}+w_{t}.\]

Recall that the observation equation was written as

\[\begin{pmatrix}y_{t1}\\ y_{t2}\end{pmatrix}=\begin{pmatrix}1\\ 1\end{pmatrix}x_{t}+\begin{pmatrix}v_{t1}\\ v_{t2}\end{pmatrix},\]

and the model covariance matrices are given by \(Q=q_{11}\) and

\[R=\begin{pmatrix}r_{11}&r_{12}\\ r_{21}&r_{22}\end{pmatrix}.\]

Hence, there are five parameters to estimate, \(\delta\), the drift, and the variance components, \(q_{11},r_{11},r_{12},r_{22}\), noting that \(r_{21}=r_{12}\) We hold the the initial state parameters fixed in this example at \(\mu_{0}=-.35\) and \(\Sigma_{0}=1\), which is large relative to the data. The final estimates were (the R matrix is reassembled in the code).

 estimate SE  signw 0.055 0.011  cR11 0.074 0.010  cR22 0.127 0.015  cR12 0.129 0.038  drift 0.006 0.005

The observations and the smoothed estimate of the signal, \(\hat{x}_{t}^{n}\pm 2\sqrt{\hat{P}_{t}^{n}}\), are displayed in Figure 6.5. The code, which uses Kfilter1 and Ksmooth1, is as follows.

Setup y = cbind(globtem, globtempl); num = nrow(y); input = rep(1,num)  A = array(rep(1,2), dim=c(2,1,num))  mu0 = -.35; Sigma0 = 1; Phi = 1  # Function to Calculate Likelihood  Linn = function(para){  cQ = para[1] # sigma_w  cR1 = para[2] # 11 element of chol(R)  cR2 = para[3] # 22 element of chol(R)  cR12 = para[4] # 12 element of chol(R)  cR = matrix(c(cR1,0,cR12,cR2),2) # put the matrix together  drift = para[5]  kf = Kfilter1(num,y,A,mu0,Sigma0,Phi,drift,0,cQ,cR,input)  return(kfSlike) }
Estimation  init.par = c(.1,.1,.1,0.0.05) # initial values of parameters  (est = optim(init.par, Linn, NULL, method='BFGS', hessian=TRUE,  control=list(trace=1,REPORT=1))) # output not shown SE = sqrt(diag(solve(estShessian)))  # Displayestimates  u = cbind(estimate=estpar, SE)  rownames(u)=c('sigw','cR11', 'CR22', 'CR12', 'drift'); u  # Smooth(first set parameters to their final estimates)  cQ = estSpar[1]  cR1 = estSpar[2]  cR2 = estSpar[3]  cR12 = estSpar[4]  cR = matrix(c(cR1,0,cR12,cR2), 2)  (R = t(cR)%*cR) # to view the estimated R matrix  drift = estSpar[5]  ks = Ksmooth1(num,y,A,mu0,Sigma@,Phi,drift,0,cQ,cR,input)  # Plot  xsm = ts(as.vector(ksSxs), start=1880)  rmse = ts(sqrt(as.vector(ksSPs)), start=1880)  plot(xsm, ylim=c(-.6, 1), ylab-'Temperature Deviations')  xx = c(time(xsm), rev(time(xsm)))  yy = c(xsm-2*rmse, rev(xsm+2*rmse))  polygon(xx, yy, border-NA, col=gray(.6, alpha=.25))  lines(globtemp, type='o', pch=2, col=4, lty=6)  lines(globtempl, type='o', pch=3, col=3, lty=6)

In addition to Newton-Raphson, Shumway and Stoffer [181] presented a conceptually simpler estimation procedure based on the Baum-Welch algorithm (Baum et al. [14]), also known as the EM (_expectation-maximization_) algorithm (Dempster et al. [51]). For the sake of brevity, we ignore the inputs and consider the model in the form of (6.1) and (6.2). The basic idea is that if we could observe the states, \(x_{0:n}=\{x_{0},x_{1},\ldots,x_{n}\}\), in addition to the observations \(y_{1:n}=\{y_{1},\ldots,y_{n}\}\), then we

Figure 6.5: Plot for Example 6.7. The _dashed lines_ with points (+ and \(\vartriangle\)) are the two average global temperature deviations shown in Figure 6.3. The _solid line_ is the estimated smoother \(\hat{x}^{n}_{t}\), and the corresponding two root mean square error bound is the gray swatch. Only the values later than 1900 are shown

would consider \(\{x_{0:n},y_{1:n}\}\) as the _complete data_, with joint density

\[\mathrm{p}_{\Theta}(x_{0:n},y_{1:n})=\mathrm{p}_{\mu_{0},\Sigma_{0}}(x_{0})\prod_ {t=1}^{n}\mathrm{p}_{\Phi,Q}(x_{t}\mid x_{t-1})\prod_{t=1}^{n}\mathrm{p}_{R}(y_ {t}\mid x_{t}). \tag{6.61}\]

Under the Gaussian assumption and ignoring constants, the complete data likelihood, (6.61), can be written as

\[\begin{split}-2\ln L_{X,Y}(\Theta)&=\ln|\Sigma_{0}| +(x_{0}-\mu_{0})^{\prime}\Sigma_{0}^{-1}(x_{0}-\mu_{0})\\ &\quad+n\ln|Q|+\sum_{t=1}^{n}(x_{t}-\Phi x_{t-1})^{\prime}Q^{-1}(x _{t}-\Phi x_{t-1})\\ &\quad+n\ln|R|+\sum_{t=1}^{n}(y_{t}-A_{t}x_{t})^{\prime}R^{-1}(y_ {t}-A_{t}x_{t}).\end{split} \tag{6.62}\]

Thus, in view of (6.62), if we did have the complete data, we could then use the results from multivariate normal theory to easily obtain the MLEs of \(\Theta\). Although we do not have the complete data, the EM algorithm gives us an iterative method for finding the MLEs of \(\Theta\) based on the _incomplete data_, \(y_{1:n}\), by successively maximizing the conditional expectation of the complete data likelihood. To implement the EM algorithm, we write, at iteration \(j\), (\(j=1,2,\ldots\)),

\[Q\left(\Theta\bigm{|}\Theta^{(j-1)}\right)=\mathrm{E}\left\{-2\ln L_{X,Y}( \Theta)\bigm{|}y_{1:n},\Theta^{(j-1)}\right\}. \tag{6.63}\]

Calculation of (6.63) is the _expectation step_. Of course, given the current value of the parameters, \(\Theta^{(j-1)}\), we can use Property 6.2 to obtain the desired conditional expectations as smoothers. This property yields

\[\begin{split} Q\left(\Theta\bigm{|}\Theta^{(j-1)}\right)& =\ln|\Sigma_{0}|+\mathrm{tr}\left\{\Sigma_{0}^{-1}[P_{0}^{n}+(x_{0}^{n}-\mu_{0 })(x_{0}^{n}-\mu_{0})^{\prime}]\right\}\\ &\quad+n\ln|Q|+\mathrm{tr}\left\{Q^{-1}[S_{11}-S_{10}\Phi^{\prime} -\Phi S_{10}^{\prime}+\Phi S_{00}\Phi^{\prime}]\right\}\\ &\quad+n\ln|R|+\mathrm{tr}\Big{\{}R^{-1}\sum_{t=1}^{n}[(y_{t}-A_ {t}x_{t}^{n})(y_{t}-A_{t}x_{t}^{n})^{\prime}+A_{t}P_{t}^{n}A_{t}^{\prime}] \Big{\}},\end{split} \tag{6.64}\]

where

\[S_{11}=\sum_{t=1}^{n}(x_{t}^{n}{x_{t}^{n}}^{\prime}+P_{t}^{n}), \tag{6.65}\]

\[S_{10}=\sum_{t=1}^{n}(x_{t}^{n}{x_{t-1}^{n}}^{\prime}+P_{t,t-1}^{n}), \tag{6.66}\]

and

\[S_{00}=\sum_{t=1}^{n}(x_{t-1}^{n}{x_{t-1}^{n}}^{\prime}+P_{t-1}^{n}). \tag{6.67}\]In (6.64)-(6.67), the smoothers are calculated under the current value of the parameters \(\Theta^{(j-1)}\); for simplicity, we have not explicitly displayed this fact. In obtaining \(Q(\cdot\mid\cdot)\), we made repeated use of fact \(\mathrm{E}(x_{s}x_{t}\,^{\prime}\mid y_{1:n})=x_{s}^{n}x_{t}^{n\prime}+P_{s,t}^ {n}\); it is important to note that one does not simply replace \(x_{t}\) with \(x_{t}^{n}\) in the likelihood.

Minimizing (6.64) with respect to the parameters, at iteration \(j\), constitutes the _maximization step_, and is analogous to the usual multivariate regression approach, which yields the updated estimates

\[\Phi^{(j)}=S_{10}S_{00}^{-1}, \tag{6.68}\]

\[Q^{(j)}=n^{-1}\left(S_{11}-S_{10}S_{00}^{-1}S_{10}^{\prime}\right), \tag{6.69}\]

and

\[R^{(j)}=n^{-1}\sum_{t=1}^{n}[(y_{t}-A_{t}x_{t}^{n})(y_{t}-A_{t}x_{t}^{n})^{ \prime}+A_{t}P_{t}^{n}A_{t}^{\prime}]. \tag{6.70}\]

The updates for the initial mean and variance-covariance matrix are

\[\mu_{0}^{(j)}=x_{0}^{n}\quad\text{and}\quad\Sigma_{0}^{(j)}=P_{0}^{n} \tag{6.71}\]

obtained from minimizing (6.64).

The overall procedure can be regarded as simply alternating between the Kalman filtering and smoothing recursions and the multivariate normal maximum likelihood estimators, as given by (6.68)-(6.71). Convergence results for the EM algorithm under general conditions can be found in Wu [212]. A thorough discussion of the convergence of the EM algorithm and related methods may be found in Douc et al. [53, Appendix D]. We summarize the iterative procedure as follows.

1. Initialize by choosing starting values for the parameters in \(\{\mu_{0},\Sigma_{0},\Phi,Q,R\}\), say \(\Theta^{(0)}\), and compute the incomplete-data likelihood, \(-\ln L_{Y}(\Theta^{(0)})\); see (6.60).

On iteration \(j\), (\(j=1,2,\ldots\)):

1. Perform the E-Step: Using the parameters \(\Theta^{(j-1)}\), use Properties 6.1, 6.2, and 6.3 to obtain the smoothed values \(x_{t}^{n}\), \(P_{t}^{n}\) and \(P_{t,t-1}^{n}\), \(t=1,\ldots,n\), and calculate \(S_{11},S_{10},S_{00}\) given in (6.65)-(6.67).
2. Perform the M-Step: Update the estimates in \(\{\mu_{0},\Sigma_{0},\Phi,Q,R\}\) using (6.68)-(6.71), obtaining \(\Theta^{(j)}\).
3. Compute the incomplete-data likelihood, \(-\ln L_{Y}(\Theta^{(j)})\).
4. Repeat Steps (ii)-(iv) to convergence.

**Example 6.8**: EM Algorithm for Example 6.3

Using the same data generated in Example 6.6, we performed an EM algorithm estimation of the parameters \(\phi\), \(\sigma_{w}^{2}\) and \(\sigma_{v}^{2}\) as well as the initial parameters \(\mu_{0}\) and \(\Sigma_{0}\) using the script EM0. The convergence rate of the EM algorithm compared with the Newton-Raphson procedure is slow. In this example, with convergence being claimed when the relative change in the log likelihood is less than.00001; convergence was attained after 59 iterations. The final estimates, along with their standard errors are listed below and the results are close those in Example 6.6.

estimate SE phi 0.810 0.078 sigw 0.853 0.164 sigv 0.864 0.136 mu0 -1.981 NA Sigma0 0.022 NA Evaluation of the standard errors used a call to fdHess in the nlme R package to evaluate the Hessian at the final estimates. The nlme package must be loaded prior to the call to fdHess. library(nlme) #loads package nlme
#Generate data (same as Example 6.6) set.seed(999); num = 100 x = arima.sim(n=num+1, list(ar =.8), sd=1) y = ts(x[-1] + rnorm(num,0,1))
#Initial Estimates (same as Example 6.6) u = ts.intersect(y, lag(y,-1), lag(y,-2)) varu = var(u); coru = cor(u) phi = coru[1,3]/coru[1,2] q = (1-phi*2)*varu[1,2]/phi r = varu[1,1] - q/(1-phi*2)
#EM procedure - output not shown (em = EM0(num, y, A=1, mu0-0, Sigma0=2.8, Phi=phi, cQ=sqrt(q), cR=sqrt(r), max.iter=75, tol=.00001))
#Standard Errors (this uses nlme) phi = em$Phi; cq = sqrt(em$Q); cr = sqrt(em$R) mu0 = em$mu0; Sigma0 = em$Sigma0 para = c(phi, cq, cr) Linn = function(para){ # to evaluate likelihood at estimates  kf = Kfilter@(num, y, 1, mu0, Sigma0, para[1], para[2], para[3]) return(kf$like) } emhess = fdHess(para, function(para) Linn(para)) SE = sqrt(diag(solve(emhess$Hessian)))
#Display Summary of Estimation estimate = c(para, em$mu0, em$Sigma0); SE = c(SE, NA, NA) u = cbind(estimate, SE) rownames(u) = c('phi','sigw','sigv','mu0','Sigma0'); u

### Steady State and Asymptotic Distribution of the MLEs

The asymptotic distribution of estimators of the model parameters, say, \(\widehat{\Theta}_{n}\), is studied in very general terms in Douc, Moulines, and Stoffer [53, Chapter 13]. Earlier treatments can be found in Caines [38, Chapters 7 and 8], and in Hannan and Deistler [88, Chapter 4]. In these references, the consistency and asymptotic normality of the estimators are established under general conditions. An essential condition is the stability of the filter. Stability of the filter assures that, for large \(t\), the innovations \(\epsilon_{t}\) are basically copies of each other with a stable covariance matrix \(\Sigma\) that does not depend on \(t\) and that, asymptotically, the innovations contain all of the information about the unknown parameters. Although it is not necessary, for simplicity, we shall assume here that \(A_{t}\equiv A\) for all \(t\). Details on departures from this assumption can be found in Jazwinski [105, Sections 7.6 and 7.8]. We also drop the inputs and use the model in the form of (6.1) and (6.2).

For stability of the filter, we assume the eigenvalues of \(\Phi\) are less than one in absolute value; this assumption can be weakened (for example, see Harvey [93], Section 4.3), but we retain it for simplicity. This assumption is enough to ensure the stability of the filter in that, as \(t\to\infty\), the filter error covariance matrix \(P_{t}^{t}\) converges to \(P\), the steady-state error covariance matrix, and the gain matrix \(K_{t}\) converges to \(K\), the steady-state gain matrix. From these facts, it follows that the innovation covariance matrix \(\Sigma_{t}\) converges to \(\Sigma\), the steady-state covariance matrix of the stable innovations; details can be found in Jazwinski [105, Sections 7.6 and 7.8] and Anderson and Moore [5, Section 4.4]. In particular, the steady-state filter error covariance matrix, \(P\), satisfies the Riccati equation:

\[P=\Phi[P-PA^{\prime}(APA^{\prime}+R)^{-1}AP]\Phi^{\prime}+Q;\]

the steady-state gain matrix satisfies \(K=PA^{\prime}[APA^{\prime}+R]^{-1}\). In Example 6.5 (see Table 6.1), for all practical purposes, stability was reached by the third observation.

When the process is in steady-state, we may consider \(x_{t+1}^{t}\) as the steady-state predictor and interpret it as \(x_{t+1}^{t}=\mathrm{E}(x_{t+1}\bigm{|}y_{t},y_{t-1},\ldots)\). As can be seen from (6.18) and (6.20), the steady-state predictor can be written as

\[x_{t+1}^{t}=\Phi[I-KA]x_{t}^{t-1}+\Phi Ky_{t}=\Phi x_{t}^{t-1}+\Phi K\epsilon _{t}, \tag{6.72}\]

where \(\epsilon_{t}\) is the steady-state innovation process given by

\[\epsilon_{t}=y_{t}-\mathrm{E}(y_{t}\bigm{|}y_{t-1},y_{t-2},\ldots).\]

In the Gaussian case, \(\epsilon_{t}\sim\mathrm{iid}\)\(\mathrm{N}(0,\Sigma)\), where \(\Sigma=APA^{\prime}+R\). In steady-state, the observations can be written as

\[y_{t}=Ax_{t}^{t-1}+\epsilon_{t}. \tag{6.73}\]

Together, (6.72) and (6.73) make up the _steady-state innovations form_ of the dynamic linear model.

In the following property, we assume the Gaussian state space model (6.1) and (6.2), is time invariant, i.e., \(A_{t}\equiv A\), the eigenvalues of \(\Phi\) are within the unit circle and the model has the smallest possible dimension (see Hannan and Deistler [88, Section 2.3 for details]). We denote the true parameters by \(\Theta_{0}\), and we assume the dimension of \(\Theta_{0}\) is the dimension of the parameter space. Although it is not necessary to assume \(w_{t}\) and \(v_{t}\) are Gaussian, certain additional conditions would have to apply and adjustments to the asymptotic covariance matrix would have to be made; see Douc et al. [53, Chapter 13].

**Property 6.4**: **Asymptotic Distribution of the Estimators**__

_Under general conditions, let \(\widehat{\Theta}_{n}\) be the estimator of \(\Theta_{0}\) obtained by maximizing the innovations likelihood, \(L_{Y}(\Theta)\), as given in (6.60). Then, as \(n\to\infty\),_

\[\sqrt{n}\left(\widehat{\Theta}_{n}-\Theta_{0}\right)\xrightarrow{d}\mathrm{N} \left[0,\ \mathcal{I}(\Theta_{0})^{-1}\right],\]

_where \(\mathcal{I}(\Theta)\) is the asymptotic information matrix given by_

\[\mathcal{I}(\Theta)=\lim_{n\to\infty}\ n^{-1}\mathrm{E}\left[-\partial^{2}\ln L _{Y}(\Theta)/\partial\Theta\ \partial^{\prime}\right].\]For a Newton procedure, the Hessian matrix (as described in Example 6.6) at the time of convergence can be used as an estimate of \(n\mathcal{I}(\Theta_{0})\) to obtain estimates of the standard errors. In the case of the EM algorithm, no derivatives are calculated, but we may include a numerical evaluation of the Hessian matrix at the time of convergence to obtain estimated standard errors. Also, extensions of the EM algorithm exist, such as the SEM algorithm (Meng and Rubin [140]), that include a procedure for the estimation of standard errors. In the examples of this section, the estimated standard errors were obtained from the numerical Hessian matrix of \(-\ln L_{Y}(\widehat{\boldsymbol{\Theta}})\), where \(\widehat{\boldsymbol{\Theta}}\) is the vector of parameters estimates at the time of convergence.

### Missing Data Modifications

An attractive feature available within the state space framework is its ability to treat time series that have been observed irregularly over time. For example, Jones [107] used the state-space representation to fit ARMA models to series with missing observations, and Palma and Chan [146] used the model for estimation and forecasting of ARFIMA series with missing observations. Shumway and Stoffer [181] described the modifications necessary to fit multivariate state-space models via the EM algorithm when data are missing. We will discuss the procedure in detail in this section. Throughout this section, for notational simplicity, we assume the model is of the form (6.1) and (6.2).

Suppose, at a given time \(t\), we define the partition of the \(q\times 1\) observation vector into two parts, \(y_{t}^{(1)}\), the \(q_{1t}\times 1\) component of observed values, and \(y_{t}^{(2)}\), the \(q_{2t}\times 1\) component of unobserved values, where \(q_{1t}+q_{2t}=q\). Then, write the partitioned observation equation

\[\begin{pmatrix}y_{t}^{(1)}\\ y_{t}^{(2)}\end{pmatrix}=\begin{bmatrix}A_{t}^{(1)}\\ A_{t}^{(2)}\end{bmatrix}x_{t}+\begin{pmatrix}v_{t}^{(1)}\\ v_{t}^{(2)}\end{pmatrix}, \tag{6.74}\]

where \(A_{t}^{(1)}\) and \(A_{t}^{(2)}\) are, respectively, the \(q_{1t}\times p\) and \(q_{2t}\times p\) partitioned observation matrices, and

\[\text{cov}\begin{pmatrix}v_{t}^{(1)}\\ v_{t}^{(2)}\end{pmatrix}=\begin{bmatrix}R_{11t}&R_{12t}\\ R_{21t}&R_{22t}\end{bmatrix} \tag{6.75}\]

denotes the covariance matrix of the measurement errors between the observed and unobserved parts.

In the missing data case where \(y_{t}^{(2)}\) is not observed, we may modify the observation equation in the DLM, (6.1)-(6.2), so that the model is

\[x_{t}=\Phi x_{t-1}+w_{t}\quad\text{and}\quad y_{t}^{(1)}=A_{t}^{(1)}x_{t}+v_{t }^{(1)}, \tag{6.76}\]

where now, the observation equation is \(q_{1t}\)-dimensional at time \(t\). In this case, it follows directly from Corollary 6.1 that the filter equations hold with the appropriate notational substitutions. If there are no observations at time \(t\), then set the gain matrix, \(K_{t}\), to the \(p\times q\) zero matrix in Property 6.1, in which case \(x_{t}^{t}=x_{t}^{t-1}\) and \(P_{t}^{t}=P_{t}^{t-1}\).

Rather than deal with varying observational dimensions, it is computationally easier to modify the model by zeroing out certain components and retaining a \(q\)-dimensional observation equation throughout. In particular, Corollary 6.1 holds for the missing data case if, at update \(t\), we substitute

\[y_{(t)}=\begin{pmatrix}y_{t}^{(1)}\\ 0\end{pmatrix},\quad A_{(t)}=\begin{bmatrix}A_{t}^{(1)}\\ 0\end{bmatrix},\quad R_{(t)}=\begin{bmatrix}R_{11t}&0\\ 0&I_{22t}\end{bmatrix}, \tag{6.77}\]

for \(y_{t}\), \(A_{t}\), and \(R\), respectively, in (6.20)-(6.22), where \(I_{22t}\) is the \(q_{2t}\times q_{2t}\) identity matrix. With the substitutions (6.77), the innovation values (6.23) and (6.24) will now be of the form

\[\epsilon_{(t)}=\begin{pmatrix}\epsilon_{t}^{(1)}\\ 0\end{pmatrix},\qquad\mathcal{I}_{(t)}=\begin{bmatrix}A_{t}^{(1)}P_{t}^{t-1}A _{t}^{(1)^{\prime}}+R_{11t}&0\\ 0&I_{22t}\end{bmatrix}, \tag{6.78}\]

so that the innovations form of the likelihood given in (6.60) is correct for this case. Hence, with the substitutions in (6.77), maximum likelihood estimation via the innovations likelihood can proceed as in the complete data case.

Once the missing data filtered values have been obtained, Stoffer [190] also established the smoother values can be processed using Property 6.2 and Property 6.3 with the values obtained from the missing data-filtered values. In the missing data case, the state estimators are denoted

\[x_{t}^{(s)}=\mathrm{E}\left(x_{t}\ \big{|}\ y_{1}^{(1)},\ldots,y_{s}^{(1)} \right), \tag{6.79}\]

with error variance-covariance matrix

\[P_{t}^{(s)}=\mathrm{E}\left\{\left(x_{t}-x_{t}^{(s)}\right)\left(x_{t}-x_{t}^{ (s)}\right)^{\prime}\right\}. \tag{6.80}\]

The missing data lag-one smoother covariances will be denoted by \(P_{t,t-1}^{(n)}\).

The maximum likelihood estimators in the EM procedure require further modifications for the case of missing data. Now, we consider

\[y_{1:n}^{(1)}=\{y_{1}^{(1)},\ldots,y_{n}^{(1)}\} \tag{6.81}\]

as the incomplete data, and \(\{x_{0:n},y_{1:n}\}\), as defined in (6.61), as the complete data. In this case, the complete data likelihood, (6.61), or equivalently (6.62), is the same, but to implement the E-step, at iteration \(j\), we must calculate

\[\begin{split} Q&\big{(}\Theta\ \big{|}\ \Theta^{(j-1)}\big{)}=\mathrm{E}\big{\{}-2\ln L_{X,Y}(\Theta)\ \big{|}\ y_{1:n}^{(1)},\Theta^{(j-1)}\big{\}}\\ &\quad=\mathrm{E}_{*}\Big{\{}\ln|\mathcal{I}_{0}|+\mathrm{tr}\ \mathcal{I}_{0}^{-1}(x_{0}-\mu_{0})(x_{0}-\mu_{0})^{\prime}\ \big{|}\ y_{1:n}^{(1)}\Big{\}}\\ &\quad+\mathrm{E}_{*}\Big{\{}n\ln|Q|+\sum_{t=1}^{n}\mathrm{tr}\ \big{[}Q^{-1}(x_{t}-\Phix_{t-1})(x_{t}-\Phix_{t-1})^{\prime}\big{]}\ \big{|}\ y_{1:n}^{(1)}\Big{\}}\\ &\quad+\mathrm{E}_{*}\Big{\{}n\ln|R|+\sum_{t=1}^{n}\mathrm{tr}\ \big{[}R^{-1}(y_{t}-A_{t}x_{t})(y_{t}-A_{t}x_{t})^{\prime}\big{]}\ \big{|}\ y_{1:n}^{(1)}\Big{\}},\end{split} \tag{6.82}\]

[MISSING_PAGE_EMPTY:324]

where the smoothers are calculated under the present value of the parameters \(\Theta^{(j-1)}\) using the missing data modifications, at iteration \(j\), the _maximization step_ is

\[\Phi^{(j)}=S_{(10)}S_{(00)}^{-1}, \tag{6.88}\]

\[Q^{(j)}=n^{-1}\left(S_{(11)}-S_{(10)}S_{(00)}^{-1}S_{(10)}^{\prime}\right), \tag{6.89}\]

and

\[R^{(j)}=n^{-1}\sum_{t=1}^{n}D_{t}\left\{\left(y_{(t)}-A_{(t)}x_{ t}^{(n)}\right)\left(y_{(t)}-A_{(t)}x_{t}^{(n)}\right)^{\prime}\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\left.+\ A_{(t)}P_{t}^{(n)}A_ {(t)}^{\prime}+\begin{pmatrix}0&0\\ 0&R_{22t}^{(j-1)}\end{pmatrix}\right\}D_{t}^{\prime}, \tag{6.90}\]

where \(D_{t}\) is a permutation matrix that reorders the variables at time \(t\) in their original order and \(y_{(t)}\) and \(A_{(t)}\) are defined in (6.77). For example, suppose \(q=3\) and at time \(t\), \(y_{t2}\) is missing. Then,

\[y_{(t)}=\begin{pmatrix}y_{t1}\\ y_{t3}\\ 0\end{pmatrix},\quad A_{(t)}=\begin{bmatrix}A_{t1}\\ A_{t3}\\ 0^{\prime}\end{bmatrix},\quad\text{ and }\quad D_{t}=\begin{bmatrix}1&0&0\\ 0&0&1\\ 0&1&0\end{bmatrix},\]

where \(A_{t}i\) is the \(i\)th row of \(A_{t}\) and \(0^{\prime}\) is a \(1\times p\) vector of zeros. In (6.90), only \(R_{11t}\) gets updated, and \(R_{22t}\) at iteration \(j\) is simply set to its value from the previous iteration, \(j-1\). Of course, if we cannot assume \(R_{12t}=0\), (6.90) must be changed accordingly using (6.83), but (6.88) and (6.89) remain the same. As before, the parameter estimates for the initial state are updated as

\[\mu_{0}^{(j)}=x_{0}^{(n)}\quad\text{and}\quad\mathcal{Z}_{0}^{(j)}=P_{0}^{(n)}. \tag{6.91}\]

**Example 6.9**: **Longitudinal Biomedical Data**

We consider the biomedical data in Example 6.1, which have portions of the three-dimensional vector missing after the 40th day. The maximum likelihood procedure yielded the estimators (code at the end of the example):

 5Phi  [,1] [,2] [,3]  [1,] 0.984 -0.041 0.009  [2,] 0.061 0.921 0.007  [3,] -1.495 2.289 0.794   SQ  [,1] [,2] [,3]  [1,] 0.014 -0.002 0.012  [2,] -0.002 0.003 0.018  [3,] 0.012 0.018 3.494   SR  [,1] [,2] [,3]  [1,] 0.007 0.000 0.000  [2,] 0.000 0.017 0.000  [3,] 0.000 0.000 1.147for the transition, state error covariance and observation error covariance matrices, respectively. The coupling between the first and second series is relatively weak, whereas the third series HCT is strongly related to the first two; that is, \[\hat{x}_{t3}=-1.495x_{t-1,1}+2.289x_{t-1,2}+.794x_{t-1,3}.\] Hence, the HCT is negatively correlated with white blood count (WBC) and positively correlated with platelet count (PLT). Byproducts of the procedure are estimated trajectories for all three longitudinal series and their respective prediction intervals. In particular, Figure 6.6 shows the data as points, the estimated smoothed values \(\hat{x}_{t}^{(n)}\) as solid lines, and error bounds, \(\pm 2\sqrt{\hat{P}_{t}^{(n)}}\) as a gray swatch.

In the following R code we use the script EM1. In this case the observation matrices \(A_{t}\) are either the identity or zero matrix because all the series are either observed or not observed. y = cbind(WBC, PLT, HCT); num = nrow(y) # make array of obs matrices A = array(0, dim=c(3,3,num)) for(k in 1:num) { if (y[k,1] > 0) A[,,k]= diag(1,3) } # Initial values  mu0 = matrix(0, 3, 1); Sigma0 = diag(c(.1,.1, 1), 3)  Phi = diag(1, 3); cQ = diag(c(.1,.1, 1), 3); cR = diag(c(.1,.1, 1), 3)  # EM procedure - some output previously shown (em = EM1(num, y, A, mu0, Sigma0, Phi, cQ, cR, 100,.001))  # Graph smoother ks = Ksmooth1(num, y, A, em$mu0, em$Sigma0, em$Phi, 0, 0, chol(em$Q),  chol(em$R), 0)  y1s = ks$xs[1,,]; y2s = ks$xs[2,,]; y3s = ks$xs[3,,]

Figure 6.6: Smoothed values for various components in the blood parameter tracking problem. The actual data are shown as points, the smoothed values are shown as _solid lines_, and \(\pm 2\) standard error bounds are shown as a gray swatch; tick marks indicate days with no observation

p1 = 2*sqrt(ksSPs[1,1,1]); p2 = 2*sqrt(ksSPs[2,2,]); p3 = 2*sqrt(ksSPs[3,3,])  par(mfrow=c(3,1))  plot(WBC, type='p', pch=19, ylim=c(1,5), xlab='day')  lines(y1s); lines(y1s+p1, lty=2, col=4); lines(y1s-p1, lty=2, col=4)  plot(PLT, type='p', ylim=c(3,6), pch=19, xlab='day')  lines(y2s); lines(y2s+p2, lty=2, col=4); lines(y2s-p2, lty=2, col=4)  plot(HCT, type='p', pch=19, ylim=c(20,40), xlab='day')  lines(y3s); lines(y3s+p3, lty=2, col=4); lines(y3s-p3, lty=2, col=4)

### Structural Models: Signal Extraction and Forecasting

Structural models are component models in which each component may be thought of as explaining a specific type of behavior. The models are often some version of the classical time series decomposition of data into trend, seasonal, and irregular components. Consequently, each component has a direct interpretation as to the nature of the variation in the data. Furthermore, the model fits into the state space framework quite easily. To illustrate these ideas, we consider an example that shows how to fit a sum of trend, seasonal, and irregular components to the quarterly earnings data that we have considered before.

**Example 6.10**: **Johnson & Johnson Quarterly Earnings**

Here, we focus on the quarterly earnings series from the U.S. company Johnson & Johnson as displayed in Fig. 1. The series is highly nonstationary, and there is both a trend signal that is gradually increasing over time and a seasonal component that cycles every four quarters or once per year. The seasonal component is getting larger over time as well. Transforming into logarithms or even taking the \(n\)th root does not seem to make the series trend stationary, however, such a transformation does help with stabilizing the variance over time; this is explored in Problem 6.13. Suppose, for now, we consider the series to be the sum of a trend component, a seasonal component, and a white noise. That is, let the observed series be expressed as

\[y_{t}=T_{t}+S_{t}+v_{t}, \tag{6.92}\]

where \(T_{t}\) is trend and \(S_{t}\) is the seasonal component. Suppose we allow the trend to increase exponentially; that is,

\[T_{t}=\phi T_{t-1}+w_{t1}, \tag{6.93}\]

where the coefficient \(\phi>1\) characterizes the increase. Let the seasonal component be modeled as

\[S_{t}+S_{t-1}+S_{t-2}+S_{t-3}=w_{t2}, \tag{6.94}\]

which corresponds to assuming the component is expected to sum to zero over a complete period or four quarters. To express this model in state-space form, let \(x_{t}=(T_{t},S_{t},S_{t-1},S_{t-2})^{\prime}\) be the state vector so the observation equation (6.2) can be written as \[y_{t}=\begin{pmatrix}1&1&0&0\end{pmatrix}\begin{pmatrix}T_{t}\\ S_{t}\\ S_{t-1}\\ S_{t-2}\end{pmatrix}+v_{t},\]

with the state equation written as

\[\begin{pmatrix}T_{t}\\ S_{t}\\ S_{t-1}\\ S_{t-2}\end{pmatrix}=\begin{pmatrix}\phi&0&0&0\\ 0&-1&-1&-1\\ 0&1&0&0\\ 0&0&1&0\end{pmatrix}\begin{pmatrix}T_{t-1}\\ S_{t-1}\\ S_{t-2}\\ S_{t-3}\end{pmatrix}+\begin{pmatrix}w_{t1}\\ w_{t2}\\ 0\\ 0\end{pmatrix},\]

where \(R=r_{11}\) and

\[Q=\begin{pmatrix}q_{11}&0&0&0\\ 0&q_{22}&0&0\\ 0&0&0&0\\ 0&0&0&0\end{pmatrix}.\]

The model reduces to state-space form, (6.1) and (6.2), with \(p=4\) and \(q=1\). The parameters to be estimated are \(r_{11}\), the noise variance in the measurement equations, \(q_{11}\) and \(q_{22}\), themodel variances corresponding to the trend and seasonal components and \(\phi\), the transition parameter that models the growth rate. Growth

Figure 6.7: Estimated trend component, \(T_{t}^{n}\), and seasonal component, \(S_{t}^{n}\), of the Johnson and Johnson quarterly earnings series. _Gray areas_ are three root MSE bounds

is about 3% per year, and we began with \(\phi=1.03\). The initial mean was fixed at \(\mu_{0}=(.7,0,0,0)^{\prime}\), with uncertainty modeled by the diagonal covariance matrix with \(\Sigma_{0ii}=.04\), for \(i=1,\ldots,4\). Initial state covariance values were taken as \(q_{11}=.01,q_{22}=.01\). The measurement error covariance was started at \(r_{11}=.25\).

After about 20 iterations of a Newton-Raphson, the transition parameter estimate was \(\hat{\phi}=1.035\), corresponding to exponential growth with inflation at about 3.5% per year. The measurement uncertainty was small at \(\sqrt{\hat{r}_{11}}=.0005\), compared with the model uncertainties \(\sqrt{\hat{q}_{11}}=.1397\) and \(\sqrt{\hat{q}_{22}}=.2209\). Figure 6.7 shows the smoothed trend estimate and the exponentially increasing seasonal components. We may also consider forecasting the Johnson & Johnson series, and the result of a 12-quarter forecast is shown in Figure 6.8 as basically an extension of the latter part of the observed data.

This example uses the Kfilter0 and Ksmooth0 scripts as follows.

num = length(jj) A = cbind(1,1,0,0)

Function to Calculate Likelihood Linn =function(para){ Phi = diag(0,4); Phi[1,1] = para[1] Phi[2,]=c(0,-1,-1,-1); Phi[3,]=c(0,1,0,0); Phi[4,]=c(0,0,1,0) cQ1 = para[2]; cQ2 = para[3] # sqrt q11 and q22 cQ = diag(0,4); cQ[1,1]=cQ1; cQ[2,2]=cQ2 cR = para[4] # sqrt r11 kf = Kfilter0(num, jj, A, mu0, Sigma0, Phi, cQ, cR) return(kf8like) }

Initial Parameters mu0 = c(.7,0,0,0); Sigma0 = diag(.04,4) init.par = c(1.03,.1,.1,.5) # Phi[1,1], the 2 cQs and cR

Estimation and Results est = optim(init.par, Linn,NULL, method='BFGS', hessian=TRUE, control=list(trace=1,REPORT=1)) SE = sqrt(diag(solve(estShesian)))

Figure 6.8: A 12-quarter forecast for the Johnson & Johnson quarterly earnings series. The forecasts are shown as a continuation of the data (points connected by a _solid line_). The _gray area_ represents two root MSPE bounds

u = cbind(estimate=est$par, SE) rownames(u)=c('Phi11','sigw1','sigw2','sigv'); u  # Smooth  Phi = diag(0,4); Phi[1,1] = est$par[1]  Phi[2,]=c(0,-1,-1,-1); Phi[3,]=c(0,1,0,0); Phi[4,]=c(0,0,1,0)  cQ1 = est$par[2]; cQ2 = est$par[3]  cQ = diag(1,4); cQ[1,1]=cQ1; cQ[2,2]=cQ2  cR = est$par[4]  ks = Ksmooth@(num,jj,A,mu0,Sigma@,Phi,cQ,cR)  # Plots  Tsm = ts(ks$xs[1,.], start=1960, freq=4)  Ssm = ts(ks$xs[2,.], start=1960, freq=4)  p1 = 3*sqrt(ks$Ps[1,1,]); p2 = 3*sqrt(ks$Ps[2,2,])  par(mfrow=c(2,1))  plot(Tsm, main='Trend Component', ylab='Trend')  xx = c(time(jj), rev(time(jj)))  yy = c(Tsm-p1, rev(Tsm+p1))  polygon(xx, yy, border=NA, col=gray(.5, alpha =.3))  plot(jj, main='Data & Trend+Season', ylab='J&J QE/Share', ylim=c(-.5,17))  xx = c(time(jj), rev(time(jj)) )  yy = c((Tsm+Ssm)-(p1+p2), rev((Tsm+Ssm)+(p1+p2)) )  polygon(xx, yy, border=NA, col=gray(.5, alpha =.3))  # Forecast  n.ahead = 12;  y = ts(append(jj, rep(@,n.ahead)), start=1960, freq=4)  rmspe = rep(@,n.ahead); x00 = ks$xf[,.,num]; P00 = ks$Pf[,.,num]  Q = t(c0)%*%cQ; R = t(CR)%*%(CR)  for (m in 1:n.ahead){  xp = Phi%*%w0; Pp = Phi%*%P00%*%t(Phi)+Q  sig = A%*%Ppk%%t(A)+R; K = Pp%*%t(A)%*%(1/sig)  x00 = xp; P00 = Pp-K%*%A%*%Pp  y[num+m] = A%*%xp; rmspe[m] = sqrt(sig) }  plot(y, type='o', main=', ylab='J&J QE/Share', ylim=c(5,30),  xlim=c(1975,1984))  upp = ts(y[(num+1):(num+n.ahead)]+2*rmspe, start=1981, freq=4)  low = ts(y[(num+1):(num+n.ahead)]-2*rmspe, start=1981, freq=4)  xx = c(time(low), rev(time(upp)))  yy = c(low, rev(upp))  polygon(xx, yy, border=8, col=gray(.5, alpha =.3))  abline(v=1981, lty=3)

Note that the Cholesky decomposition of Q does not exist here, however, the diagonal form allows us to use standard deviations for the first two diagonal elements of cQ. This technicality can be avoided using a form of the model that we present in the next section.

### State-Space Models with Correlated Errors

Sometimes it is advantageous to write the state-space model in a slightly different way, as is done by numerous authors; for example, Anderson and Moore [5] and Hannan and Deistler [88]. Here, we write the state-space model as 

[MISSING_PAGE_FAIL:331]

In the next two subsections, we show how to use the model (6.95)-(6.96) for fitting ARMAX models and for fitting (multivariate) regression models with autocorrelated errors. To put it succinctly, for ARMAX models, the inputs enter in the state equation and for regression with autocorrelated errors, the inputs enter in the observation equation. It is, of course, possible to combine the two models and we give an example of this at the end of the section.

#### ARMAX Models

Consider a \(k\)-dimensional ARMAX model given by

\[y_{t}=\Upsilon u_{t}+\sum_{j=1}^{p}\Phi_{j}y_{t-j}+\sum_{k=1}^{q}\Theta_{k}v_{t -k}+v_{t}. \tag{6.103}\]

The observations \(y_{t}\) are a \(k\)-dimensional vector process, the \(\Phi\)s and \(\Theta\)s are \(k\times k\) matrices, \(\Upsilon\) is \(k\times r\), \(u_{t}\) is the \(r\times 1\) input, and \(v_{t}\) is a \(k\times 1\) white noise process; in fact, (6.103) and (5.91) are identical models, but here, we have written the observations as \(y_{t}\). We now have the following property.

**Property 6.6**: **A State-Space Form of ARMAX**__

_For \(p\geq q\), let_

\[F=\left[\begin{array}{ccccc}\Phi_{1}&I&0&\cdots&0\\ \Phi_{2}&0&I&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ \Phi_{p-1}&0&0&\cdots&I\\ \Phi_{p}&0&0&\cdots&0\end{array}\right]\quad G=\left[\begin{array}{c}\Theta_ {1}+\Phi_{1}\\ \vdots\\ \Theta_{q}+\Phi_{q}\\ \Phi_{q+1}\\ \vdots\\ \Phi_{p}\end{array}\right]\quad H=\left[\begin{array}{c}\Upsilon^{*}\\ 0\\ \vdots\\ 0\end{array}\right] \tag{6.104}\]

_where \(F\) is \(kp\times kp\), \(G\) is \(kp\times k\), and \(H\) is \(kp\times r\). Then, the state-space model given by_

\[x_{t+1} =Fx_{t}+Hu_{t+1}+Gv_{t}, \tag{6.105}\] \[y_{t} =Ax_{t}+v_{t}, \tag{6.106}\]

_where \(A=\left[I,0,\cdots,0\right]\) is \(k\times pk\) and \(I\) is the \(k\times k\) identity matrix, implies the ARMAX model (6.103). If \(p<q\), set \(\Phi_{p+1}=\cdots=\Phi_{q}=0\), in which case \(p=q\) and (6.105)-(6.106) still apply. Note that the state process is \(kp\)-dimensional, whereas the observations are \(k\)-dimensional._

We do not prove Property 6.6 directly, but the following example should suggest how to establish the general result.

**Example 6.11**: **Univariate ARMAX\((1,1)\) in State-Space Form**

Consider the univariate ARMAX\((1,1)\) model

\[y_{t}=\alpha_{t}+\phi y_{t-1}+\theta v_{t-1}+v_{t},\]

where \(\alpha_{t}=\varUpsilon u_{t}\) to ease the notation. For a simple example, if \(\varUpsilon=(\beta_{0},\beta_{1})\) and \(u_{t}=(1,t)^{\prime}\), the model for \(y_{t}\) would be ARMA(1,1) with linear trend, \(y_{t}=\beta_{0}+\beta_{1}t+\phi y_{t-1}+\theta v_{t-1}+v_{t}\). Using Property 6.6, we can write the model as

\[x_{t+1}=\phi x_{t}+\alpha_{t+1}+(\theta+\phi)v_{t}, \tag{6.107}\]

and

\[y_{t}=x_{t}+v_{t}. \tag{6.108}\]

In this case, (6.107) is the state equation with \(w_{t}\equiv v_{t}\) and (6.108) is the observation equation. Consequently, \(\mathrm{cov}(w_{t},v_{t})=\mathrm{var}(v_{t})=R\), and \(\mathrm{cov}(w_{t},v_{s})=0\) when \(s\neq t\), so Property 6.5 would apply. To verify (6.107) and (6.108) specify an ARMAX\((1,1)\) model, we have

\[\begin{array}{llll}y_{t}&=&x_{t}+v_{t}&\text{from (\ref{eq:108})}\\ &=&\phi x_{t-1}+\alpha_{t}+(\theta+\phi)v_{t-1}+v_{t}&\text{from (\ref{eq:107})}\\ &=&\alpha_{t}+\phi(x_{t-1}+v_{t-1})+\theta v_{t-1}+v_{t}&\text{rearrange terms}\\ &=&\alpha_{t}+\phi y_{t-1}+\theta v_{t-1}+v_{t},&\text{from (\ref{eq:108})}.\end{array}\]

Together, Property 6.5 and Property 6.6 can be used to accomplish maximum likelihood estimation as described in Sect. 6.3 for ARMAX models. The ARMAX model is only a special case of the model (6.95)-(6.96), which is quite rich, as will be discovered in the next subsection.

#### Multivariate Regression with Autocorrelated Errors

In regression with autocorrelated errors, we are interested in fitting the regression model

\[y_{t}=\varGamma u_{t}+\varepsilon_{t} \tag{6.109}\]

to a \(k\times 1\) vector process, \(y_{t}\), with \(r\) regressors \(u_{t}=(u_{t1},\ldots,u_{tr})^{\prime}\) where \(\varepsilon_{t}\) is vector ARMA\((p,q)\) and \(\varGamma\) is a \(k\times r\) matrix of regression parameters. We note that the regressors do not have to vary with time (e.g., \(u_{t1}\equiv 1\) includes a constant in the regression) and that the case \(k=1\) was treated in Sect. 3.8.

To put the model in state-space form, we simply notice that \(\varepsilon_{t}=y_{t}-\varGamma u_{t}\) is a \(k\)-dimensional ARMA\((p,q)\) process. Thus, if we set \(H=0\) in (6.105), and include \(\varGamma u_{t}\) in (6.106), we obtain

\[x_{t+1} =Fx_{t}+Gv_{t}, \tag{6.110}\] \[y_{t} =\varGamma u_{t}+Ax_{t}+v_{t}, \tag{6.111}\]where the model matrices \(A\), \(F\), and \(G\) are defined in Property 6.6. The fact that (6.110)-(6.111) is multivariate regression with autocorrelated errors follows directly from Property 6.6 by noticing that together, \(x_{t+1}=Fx_{t}+Gv_{t}\) and \(\varepsilon_{t}=Ax_{t}+v_{t}\) imply \(\varepsilon_{t}=y_{t}-\Gamma u_{t}\) is vector ARMA(\(p,q\)).

As in the case of ARMAX models, regression with autocorrelated errors is a special case of the state-space model, and the results of Property 6.5 can be used to obtain the innovations form of the likelihood for parameter estimation.

**Example 6.12**: **Mortality, Temperature and Pollution**

This example combines both techniques of Section 6.6.1 and Section 6.6.2. We will fit an ARMAX model to the detrended mortality series cmort. The detrending part of the example constitutes the regression with autocorrelated errors.

Here, we let \(M_{t}\) denote the weekly cardiovascular mortality series, \(T_{t}\) as the corresponding temperature series tempr, and \(P_{t}\) as the corresponding particulate series. A preliminary analysis suggests the following considerations (no output is shown):

* An AR(2) model fits well to detrended \(M_{t}\): fit1 = sarima(cmort, 2,0,0, xreg=time(cmort))
* The CCF between the mortality residuals, the temperature series and the particulates series, shows a strong correlation with temperature lagged one week (\(T_{t-1}\)), concurrent particulate level (\(P_{t}\)) and the particulate level about one month prior (\(P_{t-4}\)). \[\text{acf(cbind(dmort <- resid(fit1Sfit), tempr, part))}\] \[\text{lag2.plot(tempr, dmort, 8)}\] \[\text{lag2.plot(part, dmort, 8)}\]

From these results, we decided to fit the ARMAX model

\[\widetilde{M}_{t}=\phi_{1}\widetilde{M}_{t-1}+\phi_{2}\widetilde{M}_{t-2}+ \beta_{1}T_{t-1}+\beta_{2}P_{t}+\beta_{3}P_{t-4}+v_{t} \tag{6.112}\]

to the detrended mortality series, \(\widetilde{M}_{t}=M_{t}-(\alpha+\beta_{4}t)\), where \(v_{t}\sim\text{id N}(0,\sigma_{v}^{2})\). To write the model in state-space form using Property 6.6, let

\[x_{t+1}=\Phi x_{t}+\gamma u_{t+1}+\Theta v_{t}\qquad t=0,1,\ldots,n\] \[y_{t}=\alpha+Ax_{t}+\Gamma u_{t}+v_{t}\qquad\quad t=1,\ldots,n\]

with

\[\Phi=\left[\begin{array}{ccc}\phi_{1}&1\\ \phi_{2}&0\end{array}\right]\quad\gamma=\left[\begin{array}{ccc}\beta_{1}& \beta_{2}&\beta_{3}&0&0\\ 0&0&0&0\end{array}\right]\quad\Theta=\left[\begin{array}{c}\phi_{1}\\ \phi_{2}\end{array}\right]\]

\(A=[\begin{array}{cc}1&0\end{array}]\), \(\Gamma=[\begin{array}{cc}0&0&0&\beta_{4}&\alpha\end{array}]\), \(u_{t}=(T_{t-1},P_{t},P_{t-4},t,1)^{\prime}\), \(y_{t}=M_{t}\). Note that the state process is bivariate and the observation process is univariate.

Some additional data analysis notes are: (1) Time is centered as \(t-\bar{t}\). In this case, \(\alpha\) should be close to the average value of \(M_{t}\). (2) \(P_{t}\) and \(P_{t-4}\) are highly correlated, so orthogonalizing these two inputs would be advantageous (although we did not do it here), perhaps by partialling out \(P_{t-4}\) from \(P_{t}\) using simple linear regression. (3) \(T_{t}\) and \(T_{t}^{2}\), as in Chap. 2, are not needed in the model when \(T_{t-1}\) is included. (4)Initial values of the parameters are taken from a preliminary investigation that we discuss now.

A quick and dirty method for fitting the model is to first detrend cmort and then fit (6.112) using lm on the detrended series. Rather than use lm in the second phase, we use sarima because it also provides a thorough analysis of the residuals. The code for this run is quite simple; the residual analysis (not displayed) supports the model.

 trend = time(cmort) - mean(time(cmort)) # center time  dcmort = resid(fit2 <- lm(cmort-trend, na.action-NULL)); fit2  (Intercept) trend  88.699 -1.625  u = ts.intersect(dM=dcmort, dM1=lag(dcmort,-1), dM2=lag(dcmort,-2),  T1=lag(tempr,-1), P=part, P4=lag(part,-4))  # lm(dM-, data=u, na.action=NULL) # and then analyze residuals... or  sarima(u[,1], 0,0,0, xreg=u[,2:6]) # get residual analysis as a byproduct  Coefficients:  intercept dM1 dM2 T1 P4  5.9884 0.3164 0.2989 -0.1826 0.1107 0.0495  s.e. 2.6401 0.0370 0.0395 0.0309 0.0177 0.0195  sigma^2 estimated as 25.42

We can now use Newton-Raphson and the Kalman filter to fit all the parameters simultaneously because the quick method has given us reasonable starting values.

 The results are close to the quick and dirty method:

 estimate SE

 phi1 0.315 0.037 # \(\hat{\phi}_{1}\)  phi2 0.318 0.041 # \(\hat{\phi}_{2}\)  sigv 5.061 0.161 # \(\hat{\sigma}_{v}\)  T1 -0.119 0.031 # \(\hat{\beta}_{1}\)  P 0.119 0.018 # \(\hat{\beta}_{2}\)  P4 0.067 0.019 # \(\hat{\beta}_{3}\)  trend -1.340 0.220 # \(\hat{\beta}_{4}\)  constant 88.752 7.015 # \(\hat{\alpha}\)

 The R code for the complete analysis is as follows:

 trend = time(cmort) - mean(time(cmort)) # center time  const = time(cmort)/time(cmort) # appropriate timeseries of 1s  ded = ts.intersect(M=cmort, T1=lag(tempr,-1), P=part, P4=lag(part,-4),  trend, const)  y = ded[,1]  input = ded[,2:6]  num = length(y)  A = array(c(1,0), dim = c(1,2,num))  # Function to Calculate Likelihood  Linn=function(para)[  phi1=para[1]; phi2=para[2]; cR=para[3]; b1=para[4]  b2=para[5]; b3=para[6]; b4=para[7]; alf=para[8]  mu0 = matrix(c(0,0), 2, 1)  Sigma0 = diag(100, 2)  Phi = matrix(c(phi1, phi2, 1, 0), 2)  Theta = matrix(c(phi1, phi2), 2)  Ups = matrix(c(b1, 0, b2, 0, b3, 0, 0, 0, 0, 0), 2, 5)  Gam = matrix(c(0, 0, 0, b4, alf), 1, 5); cQ = cR; S = cR^2kf = Kfilter2(num, y, A, mu0, Sigma0, Phi, Ups, Gam, Theta, cQ, cR, S, input)  return(kflike) }
Estimation init.par = c(phi1=.3, phi2=.3, cR=5, bl=-.2, b2=.1, b3=.05, b4=-1.6,  alf=mean(cmort)) # initial parameters L = c( 0, 0, 1, -1, 0, 0, -2, 70) # lower bound on parameters U = c(.5,.5, 10, 0,.5,.5, 0, 90) # upper bound - used in optim est = optim(init.par, Linn, NULL, method='L-BFGS-B', lower=l, upper=U,  hessian=TRUE, control-list(trace=1, REPORT=1, factr=10*8)) SE = sqrt(diag(solve(estShessian))) round(cbind(estimate=estSpar, SE), 3) # results The residual analysis involves running the Kalman filter with the final estimated values and then investigating the resulting innovations. We do not display the results, but the analysis supports the model.

Residual Analysis (not shown) phi1 = estSpar[1]; phi2 = estSpar[2]  cR = estSpar[3]; b1 = estSpar[4]  b2 = estSpar[5]; b3 = estSpar[6]  b4 = estSpar[7]; alf = estSpar[8] mu0 = matrix(c(0,0), 2, 1); Sigma0 = diag(100, 2) Phi = matrix(c(phi1, phi2, 1, 0), 2) Theta = matrix(c(phi1, phi2), 2) Ups = matrix(c(b1, 0, b2, 0, b3, 0, 0, 0, 0), 2, 5) Gam = matrix(c(0, 0, 0, b4, alf), 1, 5) cQ = cR S = cR^2 kf = Kfilter2(num, y, A, mu0, Sigma0, Phi, Ups, Gam, Theta, cQ, cR, S,  input) res = ts(as.vector(kf$innov), start=start(cmort), freq=frequency(cmort)) sarima(res, 0,0,0, no.constant=TRUE) # gives a full residual analysis

Finally, a similar and simpler analysis can be fit using a complete ARMAX model. In this case the model would be

\[M_{t} = \alpha + \phi_{1}M_{t-1} + \phi_{2}M_{t-2} + \beta_{1}T_{t-1} + \beta_{2}P_{t} + \beta_{3}P_{t-4} + \beta_{4}t + v_{t}\]

where \(v_{t}\sim\) iid N(0, \(\sigma_{v}^{2}\)). This model is different from (6.112) in that the mortality process is not detrended, but trend appears as an exogenous variable. In this case, we may use sarima to easily perform the regression and get the residual analysis as a byproduct.

trend = time(cmort) - mean(time(cmort)) u = ts.intersect(M=cmort, M1=lag(cmort,-1), M2=lag(cmort,-2),  T1=lag(tempr,-1), P=part, P4=lag(part,-4), trend) sarima(u[,1], 0,0,0, xreg=u[,2:7]) # could use lm, but it's more work  Coefficients:  intercept M1 M2 T1 P4 trend 40.3838 0.315 0.2971 -0.1845 0.1113 0.0513 -0.5214  s.e. 4.5982 0.037 0.0394 0.0309 0.0177 0.0195 0.0956  sigma*2 estimated as 25.32 We note that the residuals look fine, and the model fit is similar to the fit of (6.112).

### Bootstrapping State Space Models

Although in Sect. 6.3 we discussed the fact that under general conditions (which we assume to hold in this section) the MLEs of the parameters of a DLM are consistent and asymptotically normal, time series data are often of short or moderate length. Several researchers have found evidence that samples must be fairly large before asymptotic results are applicable (Dent and Min [50]; Ansley and Newbold [8]). Moreover, as we discussed in Example 3.36, problems occur if the parameters are near the boundary of the parameter space. In this section, we discuss an algorithm for bootstrapping state space models; this algorithm and its justification, including the non-Gaussian case, along with numerous examples, can be found in Stoffer and Wall [192] and in Stoffer and Wall [195]. In view of Sect. 6.6, anything we do or say here about DLMs applies equally to ARMAX models.

Using the DLM given by (6.95)-(6.97) and Property 6.5, we write the _innovations form of the filter_ as

\[\epsilon_{t} = y_{t}-A_{t}x_{t}^{t-1}-\Gamma u_{t}, \tag{6.114}\] \[\Sigma_{t} = A_{t}P_{t}^{t-1}A_{t}^{\prime}+R,\] (6.115) \[K_{t} = [\Phi P_{t}^{t-1}A_{t}^{\prime}+\Theta S]\Sigma_{t}^{-1},\] (6.116) \[x_{t+1}^{t-1} = \Phi x_{t}^{t-1}+\gamma u_{t+1}+K_{t}\epsilon_{t},\] (6.117) \[P_{t+1}^{t} = \Phi P_{t}^{t-1}\Phi^{\prime}+\Theta Q\Theta^{\prime}-K_{t} \Sigma_{t}K_{t}^{\prime}. \tag{6.118}\]

This form of the filter is just a rearrangement of the filter given in Property 6.5.

In addition, we can rewrite the model to obtain its innovations form,

\[x_{t+1}^{t} = \Phi x_{t}^{t-1}+\gamma u_{t+1}+K_{t}\epsilon_{t}, \tag{6.119}\] \[y_{t} = A_{t}x_{t}^{t-1}+\Gamma u_{t}+\epsilon_{t}. \tag{6.120}\]

This form of the model is a rewriting of (6.114) and (6.117), and it accommodates the bootstrapping algorithm.

As discussed in Example 6.5, although the innovations \(\epsilon_{t}\) are uncorrelated, initially, \(\Sigma_{t}\) can be vastly different for different time points \(t\). Thus, in a resampling procedure, we can either ignore the first few values of \(\epsilon_{t}\) until \(\Sigma_{t}\) stabilizes or we can work with the _standardized innovations_

\[e_{t}=\Sigma_{t}^{-1/2}\epsilon_{t}, \tag{6.121}\]

so we are guaranteed these innovations have, at least, the same first two moments. In (6.121), \(\Sigma_{t}^{1/2}\) denotes the unique square root matrix of \(\Sigma_{t}\) defined by \(\Sigma_{t}^{1/2}\Sigma_{t}^{1/2}=\Sigma_{t}\). In what follows, we base the bootstrap procedure on the standardized innovations, but we stress the fact that, even in this case, ignoring startup values might be necessary, as noted by Stoffer and Wall [192].

The model coefficients and the correlation structure of the model are uniquely parameterized by a \(k\times 1\) parameter vector \(\Theta_{0}\); that is, \(\Phi=\Phi(\Theta_{0})\), \(\Upsilon=\Upsilon(\Theta_{0})\)\(Q=Q(\Theta_{0})\), \(A_{t}=A_{t}(\Theta_{0})\), \(\Gamma=\Gamma(\Theta_{0})\), and \(R=R(\Theta_{0})\). Recall the innovations form of the Gaussian likelihood (ignoring a constant) is

\[-2\ln L_{Y}(\Theta) = \sum_{t=1}^{n}\left[\ln|\Sigma_{t}(\Theta)|+\epsilon_{t}(\Theta)^ {\prime}\Sigma_{t}(\Theta)^{-1}\epsilon_{t}(\Theta)\right] \tag{6.122}\] \[= \sum_{t=1}^{n}\left[\ln|\Sigma_{t}(\Theta)|+e_{t}(\Theta)^{\prime }e_{t}(\Theta)\right].\]

We stress the fact that it is not necessary for the model to be Gaussian to consider (6.122) as the criterion function to be used for parameter estimation.

Let \(\hat{\Theta}\) denote the MLE of \(\Theta_{0}\), that is, \(\hat{\Theta}=\operatorname*{argmax}_{\Theta}L_{Y}(\Theta)\), obtained by the methods discussed in Sect. 6.3. Let \(\epsilon_{t}(\hat{\Theta})\) and \(\Sigma_{t}(\hat{\Theta})\) be the innovation values obtained by running the filter, (6.114)-(6.118), under \(\hat{\Theta}\). Once this has been done, the nonparametric2 bootstrap procedure is accomplished by the following steps.

Footnote 2: Nonparametric refers to the fact that we use the empirical distribution of the innovations rather than assuming they have a parametric form.

1. Construct the standardized innovations \[e_{t}(\hat{\Theta})=\Sigma_{t}^{-1/2}(\hat{\Theta})\epsilon_{t}(\hat{\Theta}).\]
2. Sample, with replacement, \(n\) times from the set \(\{e_{1}(\hat{\Theta}),\ldots,e_{n}(\hat{\Theta})\}\) to obtain \(\{e_{1}^{*}(\hat{\Theta}),\ldots,e_{n}^{*}(\hat{\Theta})\}\), a bootstrap sample of standardized innovations.
3. Construct a bootstrap data set \(\{y_{1}^{*},\ldots,y_{n}^{*}\}\) as follows. Define the \((p+q)\times 1\) vector \(\xi_{t}=(x_{t+1}^{\prime},y_{t}^{\prime})^{\prime}\). Stacking (6.119) and (6.120) results in a vector first-order equation for \(\xi_{t}\) given by \[\xi_{t}=F_{t}\xi_{t-1}+Gu_{t}+H_{t}e_{t},\] (6.123) where \[F_{t}=\begin{bmatrix}\Phi&0\\ A_{t}&0\end{bmatrix},\quad G=\begin{bmatrix}\Upsilon\\ \Gamma\end{bmatrix},\quad H_{t}=\begin{bmatrix}K_{t}\Sigma_{t}^{1/2}\\ \Sigma_{t}^{1/2}\end{bmatrix}.\] Thus, to construct the bootstrap data set, solve (6.123) using \(e_{t}^{*}(\hat{\Theta})\) in place of \(e_{t}\). The exogenous variables \(u_{t}\) and the initial conditions of the Kalman filter remain fixed at their given values, and the parameter vector is held fixed at \(\hat{\Theta}\).
4. Using the bootstrap data set \(y_{1:n}^{*}\), construct a likelihood, \(L_{Y^{*}}(\Theta)\), and obtain the MLE of \(\Theta\), say, \(\hat{\Theta}^{*}\).
5. Repeat steps 2 through 4, a large number, \(B\), of times, obtaining a bootstrapped set of parameter estimates \(\{\hat{\Theta}_{b}^{*};\ b=1,\ldots,B\}\). The finite sample distribution of \(\hat{\Theta}-\Theta_{0}\) may be approximated by the distribution of \(\hat{\Theta}_{b}^{*}-\hat{\Theta}\), \(b=1,\ldots,B\).

In the next example, we discuss the case of a linear regression model, but where the regression coefficients are stochastic and allowed to vary with time. The state space model provides a convenient setting for the analysis of such models.

**Example 6.13**: **Stochastic Regression**

Figure 6.9 shows the quarterly inflation rate (solid line), \(y_{t}\), in the Consumer Price Index and the quarterly interest rate recorded for Treasury bills (dashed line), \(z_{t}\), from the first quarter of 1953 through the second quarter of 1980, \(n=110\) observations. These data are taken from Newbold and Bos [143].

In this example, we consider one analysis that was discussed in Newbold and Bos [143, pp. 61-73], that focused on the first 50 observations and where quarterly inflation was modeled as being stochastically related to quarterly interest rate,

\[y_{t}=\alpha+\beta_{t}z_{t}+v_{t},\]

where \(\alpha\) is a fixed constant, \(\beta_{t}\) is a stochastic regression coefficient, and \(v_{t}\) is white noise with variance \(\sigma_{v}^{2}\). The stochastic regression term, which comprises the state variable, is specified by a first-order autoregression,

\[(\beta_{t}-b)=\phi(\beta_{t-1}-b)+w_{t},\]

where \(b\) is a constant, and \(w_{t}\) is white noise with variance \(\sigma_{w}^{2}\). The noise processes, \(v_{t}\) and \(w_{t}\), are assumed to be uncorrelated.

Using the notation of the state-space model (6.95) and (6.96), we have in the state equation, \(x_{t}=\beta_{t}\), \(\Phi=\phi\), \(u_{t}\equiv 1\), \(\Upsilon=(1-\phi)b\), \(Q=\sigma_{w}^{2}\), and in the observation equation, \(A_{t}=z_{t}\), \(\Gamma=\alpha\), \(R=\sigma_{v}^{2}\), and \(S=0\). The parameter vector is \(\Theta=(\phi,\alpha,b,\sigma_{w},\sigma_{v})^{\prime}\). The results of the Newton-Raphson estimation procedure are listed in Table 6.2. Also shown in the Table 6.2 are the corresponding standard errors obtained from \(B=500\) runs of the bootstrap. These standard errors are simply the standard deviations of the bootstrapped estimates, that is, the square root of \(\sum_{b=1}^{B}(\Theta_{ib}^{*}-\hat{\Theta}_{i})^{2}/(B-1)\), where \(\hat{\Theta}_{i}\), represents the MLE of the \(i\)th parameter, \(\Theta_{i}\), for \(i=1,\ldots,5\),

The asymptotic standard errors listed in Table 6.2 are typically much smaller than those obtained from the bootstrap. For most of the cases, the bootstrapped

Figure 6.9: Quarterly interest rate for Treasury bills (_dashed line_) and quarterly inflation rate (_solid line_) in the Consumer Price Indexstandard errors are at least 50% larger than the corresponding asymptotic value. Also, asymptotic theory prescribes the use of normal theory when dealing with the parameter estimates. The bootstrap, however, allows us to investigate the small sample distribution of the estimators and, hence, provides more insight into the data analysis.

For example, Fig. 6.10 shows the bootstrap distribution of the estimator of \(\phi\) in the upper left-hand corner. This distribution is highly skewed with values concentrated around.8, but with a long tail to the left. Some quantiles are -.09 (5%),.11 (10%),.34 (25%),.73 (50%),.86 (75%),.96 (90%),.98 (95%), and they can be used to obtain confidence intervals. For example, a 90% confidence interval for \(\phi\) would be approximated by (-.09,.98). This interval is ridiculously wide and includes 0 as a plausible value of \(\phi\); we will interpret this after we discuss the results of the estimation of \(\sigma_{w}\).

Figure 6.10 shows the bootstrap distribution of \(\hat{\sigma}_{w}\) in the lower right-hand corner. The distribution is concentrated at two locations, one at approximately \(\hat{\sigma}_{w}=.25\) (which is the median of the distribution of values away from 0) and the other at \(\hat{\sigma}_{w}=0\). The cases in which \(\hat{\sigma}_{w}\approx 0\) correspond to deterministic state dynamics. When \(\sigma_{w}=0\) and \(|\phi|<1\), then \(\beta_{t}\approx b\) for large \(t\), so the approximately 25% of the cases in which \(\hat{\sigma}_{w}\approx 0\) suggest a fixed state, or constant coefficient model. The cases in which \(\hat{\sigma}_{w}\) is away from zero would suggest a truly stochastic regression parameter. To investigate this matter further, the off-diagonals of Fig. 6.10 show the joint bootstrapped estimates, (\(\hat{\phi},\hat{\sigma}_{w}\)), for positive values of \(\hat{\phi}^{*}\). The joint distribution suggests \(\hat{\sigma}_{w}>0\) corresponds to \(\hat{\phi}\approx 0\). When \(\phi=0\), the state dynamics are given by \(\beta_{t}=b+w_{t}\). If, in addition, \(\sigma_{w}\) is small relative to \(b\), the system is nearly deterministic; that is, \(\beta_{t}\approx b\). Considering these results, the bootstrap analysis leads us to conclude the dynamics of the data are best described in terms of a fixed regression effect.

The following R code was used for this example. We note that the first few lines of the code set the relative tolerance for determining convergence of the numerical optimization and the number of bootstrap replications. _Using the current settings may result in a long run time of the algorithm_ and we suggest the tolerance and the number of bootstrap replicates be decreased on slower machines or for demonstration purposes. For example, setting tol=.001 and nboot=200 yields

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \multicolumn{2}{c}{Asymptotic} & Bootstrap \\ Parameter & MLE & standard error & standard error \\ \hline \(\phi\) &.865 &.223 &.463 \\ \(\alpha\) & \(-.686\) &.487 &.557 \\ \(b\) &.788 &.226 &.821 \\ \(\sigma_{w}\) &.115 &.107 &.216 \\ \(\sigma_{v}\) & 1.135 &.147 &.340 \\ \hline \end{tabular}
\end{table}
Table 6.2: Comparison of standard errorsreasonable results. In this example, we fixed the first three values of the data for the resampling scheme.

library(plyr) # used for displaying progress tol = sqrt(.Machine$double.eps) # determines convergence of optimizer nboot = 500 # number of bootstrap replicates y = window(qinfl, c(1953,1), c(1965,2)) # inflation z = window(qintr, c(1953,1), c(1965,2)) # interest num = length(y) A = array(z, dim=c(1,1,num)) input = matrix(1,num,1)
Function to Calculate Likelihood Linn = function(para, y.data){ # pass data also phi = para[1]; alpha = para[2] b = para[3]; Ups = (1-phi)*b c0 = para[4]; cR = para[5] kf = Kfilter2(num,y.data,A,mu0,Sigma0,phi,Ups,alpha,1,cQ,cR,0,input) return(kf$like) }
Parameter Estimation mu0 = 1; Sigma0 =.01 init.par = c(phi.84, alpha=.77, b=.85, cQ=.12, cR=1.1) # initial values est = optim(init.par, Linn, NULL, y.data=y, method="BFGS", hessian-TRUE, control=list(trace=1, REPORT=1, reltol=tol)) SE = sqrt(diag(solve(estShessian))) phi = estSpar[1]; alpha = estSpar[2] b = estSpar[3]; Ups = (1-phi)*b cQ = estSpar[4]; cR = estSpar[5]round(cbind(estimate=est$par, SE), 3)  estimate SE  phi 0.865 0.223  alpha -0.686 0.487  b 0.788 0.226  cQ 0.115 0.107  cR 1.135 0.147
#BEGINBOOTSTRAP
#Runthefilterattheestimates  kf = Kfilter2(num,y,A,mu0,Sigma0,phi,Ups,alpha,1,cQ,cR,0,input)
#Pulloutnecessaryvaluesfromthefilterandinitialize  xp = kf$xp  innov = kf$innov  sig = kf$sig  K = kf$K  e = innov/sqrt(sig)  e.star = e #initializevalues  y.star = yp.star = xp  k = 4:50 #holdfirst3observationsfixed  para.star = matrix(0,nboot, 5) #tostoresestimates  init.par = c(.84, -.77,.85,.12, 1.1)  pr <-progress_text() #displaysprogress  prSinit(nboot)  for (i in 1:nboot){  pr$step()  e.star[k]=sample(e[k],replace=TRUE)  for (jin k){xp.star[j]=phi*xp.star[j-1]+  Ups+K[j]*sqrt(sig[j])*e.star[j] }  y.star[k]=z[k]*xp.star[k]+alpha+sqrt(sig[k])*e.star[k]  est.star=optim(init.par,Linn,NULL,y.data=y.star,method="BFGS",  control=list(reltol=tol))  para.star[i,]=cbind(est.star$par[1],est.star$par[2],est.star$par[3],  abs(est.star$par[4]),abs(est.star$par[5])) }
#Somesummarystatistics  rmse=rep(NA,5) #SEsfromthebootstrap  for(i in 1:5){rmse[i]=sqrt(sum((para.star[,i]-est$par[i])^2)/nboot)  cat(i,rmse[i],"\n") }
#Plotphiandsigw  phi=para.star[,1]  sigw=abs(para.star[,4])  phi=ifelse(phi<0,NA,phi)#anyphi<0notplotted  library(psych) #loadpsychpackageforscatter.hist  scatter.hist(sigw, phi,ylab=expression(phi),xlab=expression(sigma[-w]),  smooth=FALSE,correl-FALSE,density=FALSE,ellipse-FALSE,  title='',pch=19,col=gray(.1,alpha=.33),  panel.first=grid(lty=2),cex.lab=1.2)

### Smoothing Splines and the Kalman Smoother

There is a connection between smoothing splines, e.g., Eubank [60], Green and Silverman [81], or Wahba [206] and state space models. The basic idea of smoothing splines (recall Example 2.14) in discrete time is we suppose that data \(y_{t}\) are generated

[MISSING_PAGE_FAIL:343]

where we have kept only the terms involving the states, \(\mu_{t}\). If we set \(\lambda=\sigma_{v}^{2}/\sigma_{w}^{2}\), we can write

\[-2\log\mathrm{p}_{\theta}(x_{1:n},y_{1:n})\propto\lambda\sum_{t=1}^{n}\left( \nabla^{2}\mu_{t}\right)^{2}+\sum_{t=1}^{n}(y_{t}-\mu_{t})^{2}, \tag{6.128}\]

so that maximizing \(\log\mathrm{p}_{\theta}(x_{1:n},y_{1:n})\) with respect to the states is equivalent to minimizing (6.128), which is the original problem stated in (6.124).

In the general state space setting, we would estimate \(\sigma_{w}^{2}\) and \(\sigma_{v}^{2}\) via maximum likelihood as described in Sect. 6.3, and then obtain the smoothed state values by running Property 6.2 with the estimated variances, say \(\hat{\sigma}_{w}^{2}\) and \(\hat{\sigma}_{v}^{2}\). In this case, the estimated value of the smoothing parameter would be given by \(\hat{\lambda}=\hat{\sigma}_{v}^{2}/\hat{\sigma}_{w}^{2}\).

**Example 6.14**: **Smoothing Splines**

In this example, we generated the signal, or state process, \(\mu_{t}\) and observations \(y_{t}\) from the model (6.125) with \(n=50\), \(\sigma_{w}=.1\) and \(\sigma_{v}=1\). The state is displayed in Fig. 6.11 as a thick solid line, and the observations are displayed as points. We then estimated \(\sigma_{w}\) and \(\sigma_{v}\) using Newton-Raphson techniques and obtained \(\hat{\sigma}_{w}=.08\) and \(\hat{\sigma}_{v}=.94\). We then used Property 6.2 to generate the estimated smoothers, say, \(\hat{\mu}_{t}^{n}\), and those values are displayed in Fig. 6.11 as a thick dashed line along with a corresponding 95% (pointwise) confidence band as thin dashed lines. Finally, we used the R function smooth.spline to fit a smoothing spline to the data based on the method of generalized cross-validation (gcv). The fitted spline is displayed in Fig. 6.11 as a thin solid line, which is close to \(\hat{\mu}_{t}^{n}\).

The R code to reproduce Fig. 6.11 is given above.

set.seed(123)

num = 50

w = rnorm(num,0,.1)

x = cumsum(cumsum(w))

Figure 6.11: Display for Example 6.14: Simulated state process, \(\mu_{t}\) and observations \(y_{t}\) from the model (6.125) with \(n=50\), \(\sigma_{w}=.1\) and \(\sigma_{v}=1\). Estimated smoother (_dashed lines_): \(\hat{\mu}_{t|n}\) and corresponding 95% confidence band.gcv smoothing spline (_thin solid line_)

y = x + rnorm(num,@,1)  plot.ts(x, ylab="", lwd=2, ylim=c(-1,8))  lines(y, type='o', col=8)  ## State Space ##  Phi = matrix(c(2,1,-1,@),2); A = matrix(c(1,@),1)  mu@ = matrix(@,2); Sigma@ = diag(1,2)  Linn = function(para){  sigw = para[1]; sigv = para[2]  cQ = diag(c(sigw,@))  kf = Kfilter@(num, y, A, mu@, Sigma@, Phi, cQ, sigv)  return(kfSlike) }  ## Estimation ##  init.par = c(.1, 1)  (est = optim(init.par, Linn, NULL, method="BFGS", hessian=TRUE,  control=list(trace=1,REPORT=1)))  SE = sqrt(diag(solve(estShessian)))  ## Summary of estimation  estimate = estSpar; u = cbind(estimate, SE)  rownames(u) = c("sigw","sigv"); u  # Smooth  sigw = estSpar[1]  cQ = diag(c(sigw,@))  sigv = estSpar[2]  ks = Ksmooth@(num, y, A, mu@, Sigma@, Phi, cQ, sigv)  xsmoo = ts(ks$xs[1,1,]); psmoo = ts(ks$Ps[1,1])  upp = xsmoo+2*sqrt(psmoo); low = xsmoo-2*sqrt(psmoo)  lines(xsmoo, col=4, lty=2, lwd=3)  lines(upp, col=4, lty=2); lines(low, col=4, lty=2)  lines(smooth.spline(y), lty=1, col=2)  legend("topleft", c("Observations","State"), pch=c(1,-1), lty=1, lwd=c(1,2),  col=c(8,1))  legend("bottomright", c("Smoother", "GCV Spline"), lty=c(2,1), lwd=c(3,1),  col=c(4,2))

### Hidden Markov Models and Switching Autoregression

In the introduction to this chapter, we mentioned that the state space model is characterized by two principles. First, there is a hidden state process, \(\{x_{t};\ t=0,1,\dots\}\), that is assumed to be Markovian. Second, the observations, \(\{y_{t};\ t=1,2,\dots\}\), are independent given the states. The principles were displayed in Fig. 6.1 and written in terms of densities in (6.28) and (6.29).

We have been focusing primarily on linear Gaussian state space models, but there is an entire area that has developed around the case where the states \(x_{t}\) are a discrete-valued Markov chain, and that will be the focus in this section. The basic idea is that the value of the state at time \(t\) specifies the distribution of the observation at time \(t\). These models were developed in Goldfeld and Quandt [74] and Lindgren [128]. Changes can also be modeled in the classical regression setting by allowing the value of the state to determine the design matrix, as in Quandt [160]. An early application to speech recognition was considered by Juang and Rabiner [110]. An application of the idea of switching to the tracking of multiple targets was considered in Bar-Shalom[12], who obtained approximations to Kalman filtering in terms of weighted averages of the innovations. As another example, some authors (for example, Hamilton [85], or McCulloch and Tsay [135]) have explored the possibility that the dynamics of a country's economy might be different during expansion than during contraction.

In the Markov chain approach, we declare the dynamics of the system at time \(t\) are generated by one of \(m\) possible regimes evolving according to a Markov chain over time. The case in which the particular regime is unknown to the observer comes under the heading of _hidden Markov models_ (HMM), and the techniques related to analyzing these models are summarized in Rabiner and Juang [161]. Although the model satisfies the conditions for being a state space model, HMMs were developed in parallel. If the state process is discrete-valued, one typically uses the term "hidden Markov model" and if the state process is continuous-valued, one uses the term "state space model" or one of its variants. Texts that cover the theory and methods in whole or in part are Cappe, Moulines, and Ryden [37] and Douc, Moulines, and Stoffer [53]. A recent introductory text that uses R is Zucchini and MacDonald [214].

Here, we assume the states, \(x_{t}\), are a Markov chain taking values in a finite state space \(\{1,\ldots,m\}\), with stationary distribution

\[\pi_{j}=\Pr(x_{t}=j)\,, \tag{6.129}\]

and stationary transition probabilities

\[\pi_{ij}=\Pr(x_{t+1}=j\mid x_{t}=i)\,, \tag{6.130}\]

for \(t=0,1,2,\ldots,\) and \(i,j=1,\ldots,m\). Since the second component of the model is that the observations are conditionally independent, we need to specify the distributions, and we denote them by

\[\mathrm{p}_{j}(y_{t})=\mathrm{p}(y_{t}\mid x_{t}=j)\,. \tag{6.131}\]

**Example 6.15**: **Poisson HMM - Number of Major Earthquakes**

Consider the time series of annual counts of major earthquakes displayed in Fig. 6.12 that were discussed in Zucchini and MacDonald [214]. A natural model for unbounded count data is a Poisson distribution, in which case the mean and variance are equal. However, the sample mean and variance of the data are \(\bar{x}=19.4\) and \(s^{2}=51.6\), so this model is clearly inappropriate. It would be possible to take into account the overdispersion by using other distributions for counts such as the negative binomial distribution or a mixture of Poisson distributions. This approach, however, ignores the sample ACF and PACF displayed Fig. 6.12, which indicate the observations are serially correlated, and further suggest an AR(1)-type correlation structure.

A simple and convenient way to capture both the marginal distribution and the serial dependence is to consider a Poisson-HMM model. Let \(y_{t}\) denote the number of major earthquakes in year \(t\), and consider the state, or latent variable, \(x_{t}\) to be a stationary two-state Markov chain taking values in \(\{1,2\}\). Using the notation in (6.129) and (6.130), we have \(\pi_{12}=1-\pi_{11}\) and \(\pi_{21}=1-\pi_{22}\). The stationary distribution of this Markov chain is given by4

Footnote 4: The stationary distribution must satisfy \(\pi_{j}=\sum_{i}\pi_{i}\pi_{ij}\).

\[\pi_{1}=\frac{\pi_{21}}{\pi_{12}+\pi_{21}}\,,\quad\text{and}\quad\pi_{2}=\frac{ \pi_{12}}{\pi_{12}+\pi_{21}}\,.\]

For \(j\in\{1,2\}\), denote \(\lambda_{j}>0\) as the parameter of a Poisson distribution,

\[\mathrm{p}_{j}(y)=\frac{\lambda_{j}^{y}\,\mathrm{e}^{-\lambda_{j}}}{y!}\,,\quad y =0,1,\ldots\,.\]

Since the states are stationary, the marginal distribution of \(y_{t}\) is stationary and a mixture of Poissons,

\[\mathrm{p}\Theta(y_{t})=\pi_{1}\mathrm{p}_{1}(y_{t})+\pi_{2}\mathrm{p}_{2}(y_ {t})\]

with \(\Theta=\{\lambda_{1},\lambda_{2}\}\). The mean of the stationary distribution is

\[\mathrm{E}(y_{t})=\pi_{1}\lambda_{1}+\pi_{2}\lambda_{2} \tag{6.132}\]

and the variance5 is

Footnote 5: Recall \(\mathrm{var}(U)=\mathrm{E}[\mathrm{var}(U\mid V)]+\mathrm{var}[\mathrm{E}(U \mid V)]\).

\[\mathrm{var}(y_{t})=\mathrm{E}(y_{t})+\pi_{1}\pi_{2}(\lambda_{2}-\lambda_{1})^ {2}\geq\mathrm{E}(y_{t})\,, \tag{6.133}\]

implying that the two-state Poisson HMM is overdispersed. Similar calculations (see Problem 6.21) show that the autocovariance function of \(y_{t}\) is given by

\[\gamma_{y}(h)=\sum_{i=1}^{2}\sum_{j=1}^{2}\pi_{i}(\pi_{ij}^{h}-\pi_{j})\lambda_ {i}\lambda_{j}=\pi_{1}\pi_{2}(\lambda_{2}-\lambda_{1})^{2}(1-\pi_{12}-\pi_{21} )^{h}\,. \tag{6.134}\]

Thus, a two-state Poisson-HMM has an exponentially decaying autocorrelation function, and this is consistent with the sample ACF seen in Fig. 6.12. It is worthwhile to note that if we increase the number of states, more complex dependence structures may be obtained.

As in the linear Gaussian case, we need filters and smoothers of the state in their own right, and additionally for estimation and prediction. We then write

\[\pi_{j}(t\mid s)=\mathrm{Pr}(x_{t}=j\mid y_{1:s})\,. \tag{6.135}\]

Straight forward calculations (see Problem 6.22) give the filter equations as:

**Property 6.7**: HMM Filter

_For \(t=1,\ldots,n\),_

\[\pi_{j}(t\mid t-1) =\sum_{i=1}^{m}\pi_{i}(t-1\mid t-1)\,\pi_{ij}\,, \tag{6.136}\] \[\pi_{j}(t\mid t) =\frac{\pi_{j}(t)\mathrm{p}_{j}(y_{t})}{\sum_{i=1}^{m}\pi_{i}(t) \mathrm{p}_{i}(y_{t})}\,, \tag{6.137}\]

_with initial condition \(\pi_{j}(1\mid 0)=\pi_{j}\)._

Let \(\Theta\) denote the parameters of interest. Given data \(y_{1:n}\), the likelihood is given by

\[L_{Y}(\Theta)=\prod_{t=1}^{n}\mathrm{p}_{\Theta}(y_{t}\mid y_{1:t-1})\,.\]

But, by the conditional independence,

\[\mathrm{p}\Theta(y_{t}\mid y_{1:t-1}) =\sum_{j=1}^{m}\mathrm{Pr}(x_{t}=j\mid y_{1:t-1})\,\mathrm{p}_{ \Theta}(y_{j}\mid x_{t}=j,y_{1:t-1})\] \[=\sum_{j=1}^{m}\pi_{j}(t\mid t-1)\,\mathrm{p}_{j}(y_{t})\,.\]

Consequently,

\[\ln L_{Y}(\Theta)=\sum_{t=1}^{n}\ln\left(\sum_{j=1}^{m}\pi_{j}(t\mid t-1)\, \mathrm{p}_{j}(y_{t})\right)\,. \tag{6.138}\]

Figure 6.12: _Top_: Series of annual counts of major earthquakes (magnitude 7 and above) in the world between 1900–2006. _Bottom_: Sample ACF and PACF of the countsMaximum likelihood can then proceed as in the linear Gaussian case discussed in Sect. 6.3.

In addition, the Baum-Welch (or EM) algorithm discussed in Sect. 6.3 applies here as well. First, the general complete data likelihood still has the form of (6.61), that is,

\[\ln\mathrm{p}_{\Theta}(x_{0:n},y_{1:n})=\ln\mathrm{p}_{\Theta}(x_{0})+\sum_{t=1 }^{n}\ln\mathrm{p}_{\Theta}(x_{t}\mid x_{t-1})+\sum_{t=1}^{n}\ln\mathrm{p}_{ \Theta}(y_{t}\mid x_{t})\,.\]

It is more useful to define \(I_{j}(t)=1\) if \(x_{t}=j\) and \(0\) otherwise, and \(I_{ij}(t)=1\) if \((x_{t-1},x_{t})=(i,j)\) and \(0\) otherwise, for \(i,j=1,\ldots,m\). Recall \(\Pr[I_{j}(t)=1]=\pi_{j}\) and \(\Pr[I_{ij}(t)=1]=\pi_{ij}\,\pi_{i}\). Then the complete data likelihood can be written as (we drop \(\Theta\) from some of the notation for convenience)

\[\ln\mathrm{p}_{\Theta}(x_{0:n},y_{1:n}) =\sum_{j=1}^{m}I_{j}(0)\ln\pi_{j}+\sum_{t=1}^{n}\sum_{i=1}^{m} \sum_{j=1}^{m}I_{ij}(t)\ln\pi_{ij}(t)\] \[+\sum_{t=1}^{n}\sum_{j=1}^{m}I_{j}(t)\ln\mathrm{p}_{j}(y_{t})\,, \tag{6.139}\]

and, as before, we need to maximize \(Q(\Theta\mid\Theta^{\prime})=\mathrm{E}[\ln\mathrm{p}_{\Theta}(x_{0:n},y_{1:n} )\mid y_{1:n},\Theta^{\prime}]\). In this case, it should be clear that in addition to the filter, (6.137), we will need

\[\pi_{j}(t\mid n)=\mathrm{E}(I_{j}(t)\mid y_{1:n})=\Pr(x_{t}=j\mid y_{1:n}) \tag{6.140}\]

for the first and third terms, and

\[\pi_{ij}(t\mid n)=\mathrm{E}(I_{ij}(t)\mid y_{1:n})=\Pr(x_{t}=i,x_{t+1}=j\mid y _{1:n})\,. \tag{6.141}\]

for the second term. In the evaluation of the second term, as will be seen, we must also evaluate

\[\varphi_{j}(t)=\mathrm{p}(y_{t+1:n}\mid x_{t}=j)\,. \tag{6.142}\]

**Property 6.8**: **HMM Smoother**__

_For \(t=n-1,\ldots,0\),_

\[\pi_{j}(t\mid n) =\frac{\pi_{j}(t\mid t)\varphi_{j}(t)}{\sum_{j=1}^{m}\pi_{j}(t \mid t)\varphi_{j}(t)}\,, \tag{6.143}\] \[\pi_{ij}(t\mid n) =\pi_{i}(t\mid n)\pi_{ij}\mathrm{p}_{j}(y_{t+1})\varphi_{j}(t+1) /\varphi_{i}(t)\,,\] (6.144) \[\varphi_{i}(t) =\sum_{j=1}^{m}\pi_{ij}\mathrm{p}_{j}(y_{t+1})\varphi_{j}(t+1)\,, \tag{6.145}\]

_where \(\varphi_{j}(n)=1\) for \(j=1,\ldots,m\).__Proof:_ We leave the proof of (6.143) to the reader; see Problem 6.22. To verify (6.145), note that

\[\varphi_{i}(t) =\sum_{j=1}^{m}\operatorname{p}(y_{t+1:m},x_{t+1}=j\mid x_{t}=i)\] \[=\sum_{j=1}^{m}\operatorname{Pr}(x_{t+1}=j\mid x_{t}=i) \operatorname{p}(y_{t+1}\mid x_{t+1}=j)\operatorname{p}(y_{t+2:n}\mid x_{t+1}=j)\] \[=\sum_{j=1}^{m}\pi_{ij}\operatorname{p}_{j}(y_{t+1})\varphi_{j}(t +1)\,.\]

To verify (6.144), we have

\[\pi_{ij}(t\mid n) \propto\operatorname{Pr}(x_{t}=i,x_{t+1}=j,y_{t+1},y_{t+2:n}\mid y _{1:t})\] \[=\operatorname{Pr}(x_{t}=i\mid y_{1:t})\operatorname{Pr}(x_{t+1} =j\mid x_{t}=i)\] \[\qquad\times\operatorname{p}(y_{t+1}\mid x_{t+1}=j)\operatorname{ p}(y_{t+2:n}\mid x_{t+1}=j)\] \[=\pi_{i}(t\mid t)\,\pi_{ij}\operatorname{p}_{j}(y_{t+1})\, \varphi_{j}(t+1)\,.\]

Finally, to find the constant of proportionality, say \(C_{t}\), if we sum over \(j\) on both sides we get, \(\sum_{j=1}^{m}\pi_{ij}(t\mid n)=\pi_{i}(t\mid n)\) and \(\sum_{j=1}^{m}\pi_{ij}\operatorname{p}_{j}(y_{t+1})\,\varphi_{j}(t+1)=\varphi_{ i}(t)\). This means that \(\pi_{i}(t\mid n)=C_{t}\,\pi_{i}(t\mid t)\,\varphi_{i}(t)\), and (6.144) follows. \(\square\)

For the Baum-Welch (or EM) algorithm, given the current value of the parameters, say \(\Theta^{\prime}\), run the filter Property 6.7 and smoother Property 6.8, and then, as is evident from (6.139), update the first two estimates as

\[\hat{\pi}_{j}=\pi^{\prime}_{j}(0\mid n)\quad\text{and}\quad\hat{\pi}_{ij}= \frac{\sum_{t=1}^{n}\pi^{\prime}_{i}(t\mid n)}{\sum_{t=1}^{n}\sum_{k=1}^{m}\pi ^{\prime}_{ik}(t\mid n)}\,. \tag{6.146}\]

Of course, the prime indicates that values have been obtain under \(\Theta^{\prime}\) and the hat denotes the update. Although not the MLE, it has been suggested by Lindgren [128] that a natural estimate of the stationary distribution of the chain would be

\[\hat{\pi}_{j}=n^{-1}\sum_{t=1}^{n}\pi^{\prime}_{j}(t\mid n)\,,\]

rather than the value given in (6.146). Finally, the third term in (6.139) will require knowing the distribution of \(p_{j}(y_{t})\), and this will depend on the particular model. We will discuss the Poisson distribution in Example 6.15 and the normal distribution in Example 6.17

**Example 6.16**: **Poisson HMM - Number of Major Earthquakes (cont)**

To run the EM algorithm in this case, we still need to maximize the conditional expectation of the third term of (6.139). The conditional expectation of the third term at the current parameter value \(\Theta^{\prime}\) is

\[\sum_{t=1}^{n}\sum_{j=1}^{m}\pi^{\prime}_{j}(t\mid t-1)\ln\mathrm{p}_{j}(y_{t})\,,\]

where

\[\log\mathrm{p}_{j}(y_{t})\propto y_{t}\log\lambda_{j}-\lambda_{j}\,.\]

Consequently, maximization with respect to \(\lambda_{j}\) yields

\[\hat{\lambda}_{j}=\frac{\sum_{t=1}^{n}\pi^{\prime}_{j}(t|n)\,y_{t}}{\sum_{t=1} ^{n}\pi^{\prime}_{j}(t|n)}\,,\qquad j=1,\ldots,m.\]

We fit the model to the time series of earthquake counts using the R package depmx54. The package, which uses the EM algorithm, does not provide standard errors, so we obtained them by a parametric bootstrap procedure; see Remillard [164] for justification. The MLEs of the intensities, along with their standard errors, were \((\hat{\lambda}_{1},\hat{\lambda}_{2})=(15.4_{(.7)},26.0_{(1.1)})\). The MLE of the transition matrix was \([\hat{\pi}_{11},\hat{\pi}_{12},\hat{\pi}_{21},\hat{\pi}_{22}]=[.93_{(.04)}.07_ {(.04)}.12_{(.09)},.88_{(.09)}]\). Figure 13 displays the counts, the estimated state (displayed as points) and the smoothing distribution for the earthquakes data, modeled as a two-state Poisson HMM model with parameters fitted using the MLEs. Finally, a histogram of the data is displayed along with the two estimated Poisson densities superimposed as solid lines.

Figure 13: _Top_: Earthquake count data and estimated states. _Bottom left_: Smoothing probabilities. _Bottom right_: Histogram of the data with the two estimated Poisson densities superimposed (_solid lines_)

The R code for this example is as follows.

library(depmix$4) model<-depmix(EQcount-1,nstates=2,data=data.frame(EQcount),family-poisson()) set.seed(90210) summary(fm<-fit(model))#estimationresults
##-GentParameters-## u=as.vector(getpars(fm))#ensurstate1hassmallerlambda if(u[7]<=u[8]){para.mle=c(u[3:6],exp(u[7]),exp(u[8])) }else{para.mle=c(u[6:3],exp(u[8]),exp(u[7]))} mtrans=matrix(para.mle[1:4],byrow=TRUE,nrow=2) lams=para.mle[5:6] pi1=mtrans[2,1]/(2-mtrans[1,1]-mtrans[2,2]);pi2=1-pi1
##-Graphics-## layout(matrix(c(1,2,1,3),2)) par(mar=c(3,3,1,1),mpp=c(1.6,6,0))
#dataandstates plot(EQcount,main="",ylab='EQcount',type='h',col=gray(.7)) text(EQcount,col=6'posterior(fm)[,1]-2,labels=posterior(fm)[,1],cex=.9)
#probofstate2 plot(ts(posterior(fm)[,3],start=1900),ylab=expression(hat(pi)[-2]^"(t|n'));abline(h=.5,lty=2)
#histogram hist(EQcount,breaks=30,prob=TRUE,main="") xvals=seq(1,45) u1=pi1*dpois(xvals,lams[1]) u2=pi2*dpois(xvals,lams[2]) lines(xvals,u1,col=4);lines(xvals,u2,col=2)
##-Bootstrap-##
#functiontogeneratedata pois.HMM.generate_sample=function(n,m,lambda,Mtrans,StatDist=NULL){
#n=datalength,m=numberofstates,Mtrans=transitionmatrix, StatDist=stationarydistn if(is.null(StatDist))StatDist=solve(t(diag(m)-Mtrans+1),rep(1,m)) mvect=1:m state=numeric(n) state[1]=sample(mvect,1,prob=StatDist) for(iin2:n) state[i]=sample(mvect,1,prob=Mtrans[state[i-1],]) y=rpois(n,lambda=lambda[state]) list(y=y,state=state) }
#startifup set.seed(10101101) nboot=100 nobs=length(EQcount) para.star=matrix(NA,nrow=nboot,ncol=6) for(jin1:nboot){ x.star=pois.HMM.generate_sample(n=nobs,m=2,lambda=lams,Mtrans=mtrans)$y model<-depmix(x.star-1,nstates=2,data=data.frame(x.star), family=poisson()) u=as.vector(getpars(fit(model,verbose=0)))
#makesurestate1istheonewiththesmallerintensityparameter if(u[7]<=u[8]){para.star[j,]=c(u[3:6],exp(u[7]),exp(u[8]))} else{para.star[j,]=c(u[6:3],exp(u[8]),exp(u[7]))}
#bootstrappedstderrors

SE = sqrt(apply(para.star,2,var) +  (apply(para.star,2,mean)-para.mle)^2)[c(1,4:6)]  names(SE)=c('seM11/M12','seM21/M22','seLam1','seLam2'); SE

Next, we present an example using a mixture of normal distributions.

**Example 6.17**: **Normal HMM - S&P500 Weekly Returns**

Estimation in the Gaussian case is similar to the Poisson case given in Example 6.16, except that now, \(\mathrm{p}_{j}(y_{t})\) is the normal density; i.e., \((y_{t}\mid x_{t}=j)\sim\mathrm{N}(\mu_{j},\sigma_{j}^{2})\) for \(j=1,\ldots,m\). Then, dealing with the third term in (6.139) in this case yields

\[\hat{\mu}_{j}=\frac{\sum_{t=1}^{n}\pi^{\prime}_{j}(t|n)\,y_{t}}{\sum_{t=1}^{n} \pi^{\prime}_{j}(t|n)}\,,\qquad\hat{\sigma}_{j}^{2}=\frac{\sum_{t=1}^{n}\pi^{ \prime}_{j}(t|n)\,y_{t}^{2}}{\sum_{t=1}^{n}\pi^{\prime}_{j}(t|n)}-\hat{\mu}_{j }^{2}\,.\]

In this example, we fit a normal HMM using the R package depmixS4 to the weekly S&P 500 returns displayed in Fig. 6.14. We chose a three-state model and we leave it to the reader to investigate a two-state model (see Problem 6.24). Standard errors (shown in parentheses below) were obtained via a parametric bootstrap based on a simulation script provided with the package.

If we let \(P=\{\pi_{ij}\}\) denote the \(3\times 3\) matrix of transition probabilities, the fitted transition matrix was

\[\widehat{P}=\left[\begin{array}{ccc}.945_{(.074)}&.055_{(.074)}&.000_{(.000)} \\.739_{(.275)}&.000_{(.000)}&.261_{(.275)}\\.032_{(.122)}&.027_{(.057)}&.942_{(.147)}\end{array}\right]\,,\]

and the three fitted normals were \(\mathrm{N}(\hat{\mu}_{1}=.004_{(.173)},\hat{\sigma}_{1}=.014_{(.968)})\), \(\mathrm{N}(\hat{\mu}_{2}=-.034_{(.909)},\hat{\sigma}_{2}=.009_{(.777)})\), and \(\mathrm{N}(\hat{\mu}_{3}=-.003_{(.317)},\hat{\sigma}_{3}=.044_{(.910)})\). The data, along with the predicted state (based on the smoothing distribution), are plotted in Fig. 6.14.

Note that regime 2 appears to represent a somewhat large-in-magnitude negative return, and may be a lone dip, or the start or end of a highly volatile period. States 1 and 3 represent clusters of regular or high volatility, respectively. Note that there is a large amount of uncertainty in the fitted normals, and in the transition matrix involving transitions from state 2 to states 1 or 3. The R code for this example is:

library(depmixS4) y = ts(sp50@w, start=20@3, freq=52) # make data depmix friendly mod3 <- depmix(y-1, nstates=3, data=data.frame(y)) set.seed(2) summary(fm3 <- fit(mod3))
##- Graphics --## layout(matrix(c(1,2,1,3),2), heights=c(1,.75)) par(mar=c(2.5,2.5,.5,.5), mpg=c(1.6,.6,0)) plot(y, main="", ylab='S&P50@ Weekly Returns', col=gray(.7),  ylim=c(-.11,.11))  culer = 4-posterior(fm3)[,1]; culer[culer==3]=4 # switch labels 1 and 3 text(y, col=culer, labels=4-posterior(fm3)[,1])
##- MLEs --## para.mle = as.vector(getpars(fm3)[-(1:3)])

permu = matrix(c(0,0,1,0,1,0,1,0,0), 3,3) # for the label switch (mtrans.mle = permu%*%round(t(matrix(para.mle[1:9],3,3)),3)%*%permu) (norms.mle = round(matrix(para.mle[10:15],2,3),3)%*%permu) acf(y*2, xlim-c(.02,.5), ylim=c(-.09,.5), panel.first=grid(lty=2) ) hist(y, 25, prob=TRUE, main='')  culer=c(1,2,4); pi.hat = colSums(posterior(fm3)[-1,2:4])/length(y)  for (i in 1:3) { mu-norms.mle[1,i]; sig = norms.mle[2,i]  x = seq(-.15,.12, by=.001)  lines(x, pi.hat[4-i]*dnorm(x, mean=mu, sd=sig), col=culer[i]) }  ##- - Bootstrap -##  set.seed(666); n.obs = length(y); n.boot = 100  para.star = matrix(NA, nrow=n.boot, ncol = 15)  respst <- para.mle[10:15]; trst <- para.mle[1:9]  for ( nb in 1:n.boot) {  mod <- simulate(mod3)  y.star = as.vector(mod@response[[1]][[1]]@y)  dfy = data.frame(y.star)  mod.star <- depmix(y.star-1, data=dfy, respst=respst, trst=trst, nst-3)  fm.star = fit(mod.star, emcontrol=em.control(tol = 1e-5), verbose=FALSE)  para.star[nb,] = as.vector(getpars(fm.star)[-(1:3)]) }  # bootstrap std errors  SE = sqrt(apply(para.star,2,var) + (apply(para.star,2,mean)-para.mle)^2)  (SE.mtrans.mle = permu%*%round(t(matrix(SE[1:9],3,3)),3)%*%permu)  (SE.norms.mle = round(matrix(SE[10:15], 2,3),3)%*%permu)

Figure 6.14: _Top_: S&P 500 weekly returns with estimated regimes labeled as a number, 1, 2, or 3 The minimum value of \(-20\%\) during the financial crisis has been truncated to improve the graphics. _Bottom left_: Sample ACF of the squared returns. _Bottom right_: Histogram of the data with the three estimated normal densities superimposed

It is worth mentioning that _switching regressions_ also fits into this framework. In this case, we would change \(\mu_{j}\) in the model in Example 6.17 to depend on independent inputs, say \(z_{t1},\ldots,z_{tr}\), so that

\[\mu_{j}=\beta_{0}^{(j)}+\sum_{i=1}^{r}\beta_{i}^{(j)}z_{ti}\,.\]

This type of model is easily handled using the depmixS4 R package.

By conditioning on the first few observations, it is also possible to include simple switching linear autoregression into this framework. In this case, we model the observations as being an AR(\(p\)), with parameters depending on the state; that is,

\[y_{t}=\phi_{0}^{(x_{t})}+\sum_{i=1}^{p}\phi_{i}^{(x_{t})}y_{t-i}+\sigma^{(x_{t })}v_{t}\,, \tag{6.147}\]

and \(v_{t}\sim\text{id }\text{ N}(0,1)\). The model is similar to the threshold model discussed in Sect. 5.4, however, the process is not self-exciting or influenced by an observed exogenous process. In (6.147), we are saying that the parameters are random, and the regimes are changing due to a latent Markov process. In a similar fashion to (6.131), we write the conditional distribution of the observations as

\[\text{p}_{j}(y_{t})=\text{p}(y_{t}\mid x_{t}=j,\ y_{t-1:t-p})\,, \tag{6.148}\]

and we note that for \(t>p\), \(\text{p}_{j}(y_{t})\) is the normal density (\(\mathfrak{g}\)),

\[\text{p}_{j}(y_{t})=\mathfrak{g}\left(y_{t};\ \phi_{0}^{(j)}+\sum_{i=1}^{p} \phi_{i}^{(j)}y_{t-i},\sigma^{2^{(j)}}\right)\,. \tag{6.149}\]

As in (6.138), the conditional likelihood is given by

\[\ln L_{Y}(\Theta\mid y_{1:p})=\sum_{t=p+1}^{n}\ln\left(\sum_{j=1}^{m}\pi_{j}(t \mid t-1)\,\text{p}_{j}(y_{t})\right)\,.\]

where Property 6.7 still applies, but with the updated evaluation of \(\text{p}_{j}(y_{t})\) given in (6.149). In addition, the EM algorithm may be used analogously by assessing the smoothers. The smoothers in this case are symbolically the same as given in Property 6.8 with the appropriate definition changes, \(\text{p}_{j}(y_{t})\) as given in (6.148) and with \(\varphi_{j}(t)=\text{p}(y_{t+1:n}\mid x_{t}=j,y_{t+1-p:t})\) for \(t>p\).

**Example 6.18**: _Switching AR - Influenza Mortality_

In Example 5.7, we discussed the monthly pneumonia and influenza mortality series shown in Fig. 5.7. We pointed out the non-reversibility of the series, which rules out the possibility that the data are generated by a linear Gaussian process. In addition, note that the series is irregular, and while mortality is highest during the winter, the peak does not occur in the same month each year. Moreover, some seasonshave very large peaks, indicating flu epidemics, whereas other seasons are mild. In addition, it can be seen from Fig. 5.7 that there is a slight negative trend in the data set, indicating that flu prevention is getting better over the eleven year period.

As in Example 5.7, we focus on the differenced data, which removes the trend. In this case, we denote \(y_{t}=\nabla\text{flu}_{t}\), where \(\text{flu}_{t}\) represents the data displayed in Fig. 5.7. Since we already fit a threshold model to \(y_{t}\), we might also consider a switching autoregressive model where there are two hidden regimes, one for epidemic periods and one for more mild periods. In this case, the model is given by

\[y_{t}=\begin{cases}\phi_{0}^{(1)}+\sum_{j=1}^{p}\phi_{j}^{(1)}y_{t-j}+\sigma^{ (1)}v_{t}\,,&\text{for }x_{t}=1\,,\\ \phi_{0}^{(2)}+\sum_{j=1}^{p}\phi_{j}^{(2)}y_{t-j}+\sigma^{(2)}v_{t}\,,&\text{ for }x_{t}=2\,,\end{cases} \tag{6.150}\]

where \(v_{t}\sim\text{iid N}(0,1)\), and \(x_{t}\) is a latent, two-state Markov chain.

We used the R package MSwM to fit the model specified in (6.150), with \(p=2\). The results were

\[\hat{y}_{t}=\begin{cases}.006_{(.003)}+.293_{(.039)}y_{t-1}+\.097_{(.031)}y_{t-2}+.024\,v_{t}\,,&\text{for }x_{t}=1\,,\\.199_{(.063)}-.313_{(.281)}y_{t-1}-1.604_{(.276)}y_{t-2}+.112\,v_{t}\,,&\text{ for }x_{t}=2\,,\end{cases}\]

with estimated transition matrix

\[\hat{P}=\begin{bmatrix}.93&.07\\.30&.70\end{bmatrix}\,.\]

Figure 6.15 displays the data \(y_{t}=\nabla\text{flu}_{t}\) along with the estimated states (displayed as points labeled 1 or 2). The smoothed state 2 probabilities are displayed in the bottom of the figure as a straight line. The filtered state 2 probabilities are displayed in the same graph as vertical lines. The code for this example is as follows.

Figure 6.15: The differenced flu mortality data along with the estimated states (displayed as points). The smoothed state 2 probabilities are displayed in the bottom of the figure as a straight line. The filtered state 2 probabilities are displayed as vertical lines

library(MSwM)  set.seed(90210)  dflu = diff(flu)  model = lm(dflu- 1)  mod = msmFit(model, k=2, p=2, sw=rep(TRUE,4)) # 2 regimes, AR(2)s  summary(mod)  plotProb(mod, which=3)

### Dynamic Linear Models with Switching

In this section, we extend the hidden Markov model discussed in Sect. 6.9 to more general problems. As previously indicated, the problem of modeling changes in regimes for time series has been of interest in many different fields, and we have explored these ideas in Sect. 5.4 as well as in Sect. 6.9.

Generalizations of the state space model to include the possibility of changes occurring over time have been approached by allowing changes in the error covariances (Harrison and Stevens [90], Gordon and Smith [76, 77]) or by assigning mixture distributions to the observation errors \(v_{t}\) (Pena and Guttman [151]). Approximations to filtering were derived in all of the aforementioned articles. An application to monitoring renal transplants was described in Smith and West [188] and in Gordon and Smith [77]. Gerlach et al. [69] considered an extension of the switching AR model to allow for level shifts and outliers in both the observations and innovations. An application of the idea of switching to the tracking of multiple targets has been considered in Bar-Shalom [12], who obtained approximations to Kalman filtering in terms of weighted averages of the innovations. For a thorough coverage of these and related techniques, see Cappe, Moulines, and Ryden [37] and Douc, Moulines, and Stoffer [53].

In this section, we will concentrate on the method presented in Shumway and Stoffer [184]. One way of modeling change in an evolving time series is by assuming the dynamics of some underlying model changes discontinuously at certain undetermined points in time. Our starting point is the DLM given by (6.1) and (6.2), namely,

\[x_{t}=\Phi x_{t-1}+w_{t}, \tag{6.151}\]

to describe the \(p\times 1\) state dynamics, and

\[y_{t}=A_{t}x_{t}+v_{t} \tag{6.152}\]

to describe the \(q\times 1\) observation dynamics. Recall \(w_{t}\) and \(v_{t}\) are Gaussian white noise sequences with \(\mathrm{var}(w_{t})=Q\), \(\mathrm{var}(v_{t})=R\), and \(\mathrm{cov}(w_{t},v_{s})=0\) for all \(s\) and \(t\).

**Example 6.19**: **Tracking Multiple Targets**__

The approach of Shumway and Stoffer [184] was motivated primarily by the problem of tracking a large number of moving targets using a vector \(y_{t}\) of sensors. In this problem, we do not know at any given point in time which target any given sensor has detected. Hence, it is the structure of the measurement matrix \(A_{t}\) in (6.152)that is changing, and not the dynamics of the signal \(x_{t}\) or the noises, \(w_{t}\) or \(v_{t}\). As an example, consider a \(3\times 1\) vector of satellite measurements \(y_{t}=(y_{t1},y_{t2},y_{t3})^{\prime}\) that are observations on some combination of a \(3\times 1\) vector of targets or signals, \(x_{t}=(x_{t1},x_{t2},x_{t3})^{\prime}\). For the measurement matrix \[A_{t}=\begin{bmatrix}0&1&0\\ 1&0&0\\ 0&0&1\end{bmatrix}\] for example, the first sensor, \(y_{t1}\), observes the second target, \(x_{t2}\); the second sensor, \(y_{t2}\), observes the first target, \(x_{t1}\); and the third sensor, \(y_{t3}\), observes the third target, \(x_{t3}\). All possible detection configurations will define a set of possible values for \(A_{t}\), say, \(\{M_{1},M_{2},\ldots,M_{m}\}\), as a collection of plausible measurement matrices.

**Example 6.20**: **Modeling Economic Change**

As another example of the switching model presented in this section, consider the case in which the dynamics of the linear model changes suddenly over the history of a given realization. For example, Lam [125] has given the following generalization of Hamilton [85] model for detecting positive and negative growth periods in the economy. Suppose the data are generated by

\[y_{t}=z_{t}+n_{t}, \tag{6.153}\]

where \(z_{t}\) is an autoregressive series and \(n_{t}\) is a random walk with a drift that switches between two values \(\alpha_{0}\) and \(\alpha_{0}+\alpha_{1}\). That is,

\[n_{t}=n_{t-1}+\alpha_{0}+\alpha_{1}S_{t}, \tag{6.154}\]

with \(S_{t}=0\) or \(1\), depending on whether the system is in state \(1\) or state \(2\). For the purpose of illustration, suppose

\[z_{t}=\phi_{1}z_{t-1}+\phi_{2}z_{t-2}+w_{t} \tag{6.155}\]

is an AR(2) series with \(\text{var}(w_{t})=\sigma_{w}^{2}\). Lam [125] wrote (6.153) in a differenced form

\[\nabla y_{t}=z_{t}-z_{t-1}+\alpha_{0}+\alpha_{1}S_{t}, \tag{6.156}\]

which we may take as the observation equation (6.152) with state vector

\[x_{t}=(z_{t},z_{t-1},\alpha_{0},\alpha_{1})^{\prime} \tag{6.157}\]

and

\[M_{1}=[1,-1,1,0]\quad\text{and}\quad M_{2}=[1,-1,1,1] \tag{6.158}\]

determining the two possible economic conditions. The state equation, (6.151), is of the form

\[\begin{pmatrix}z_{t}\\ z_{t-1}\\ \alpha_{0}\\ \alpha_{1}\end{pmatrix}=\begin{bmatrix}\phi_{1}&\phi_{2}&0&0\\ 1&0&0&0\\ 0&0&1&0\\ 0&0&0&1\end{pmatrix}\begin{pmatrix}z_{t-1}\\ z_{t-2}\\ \alpha_{0}\\ \alpha_{1}\end{pmatrix}+\begin{pmatrix}w_{t}\\ 0\\ 0\\ 0\end{pmatrix}. \tag{6.159}\]The observation equation, (6.156), can be written as

\[\nabla y_{t}=A_{t}x_{t}+v_{t}, \tag{6.160}\]

where we have included the possibility of observational noise, and where \(\Pr(A_{t}=M_{1})=1-\Pr(A_{t}=M_{2})\), with \(M_{1}\) and \(M_{2}\) given in (6.158).

To incorporate a reasonable switching structure for the measurement matrix into the DLM that is compatible with both practical situations previously described, we assume that the \(m\) possible configurations are states in a nonstationary, independent process defined by the time-varying probabilities

\[\pi_{j}(t)=\Pr(A_{t}=M_{j}), \tag{6.161}\]

for \(j=1,\ldots,m\) and \(t=1,2,\ldots,n\). Important information about the current state of the measurement process is given by the filtered probabilities of being in state \(j\), defined as the conditional probabilities

\[\pi_{j}(t\mid t)=\Pr(A_{t}=M_{j}\mid y_{1:t}), \tag{6.162}\]

which also vary as a function of time. Recall that \(y_{s^{\prime}:s}=\{y_{s^{\prime}},\ldots,y_{s}\}\). The filtered probabilities (6.162) give the time-varying estimates of the probability of being in state \(j\) given the data to time \(t\).

It will be important for us to obtain estimators of the configuration probabilities, \(\pi_{j}(t\mid t)\), the predicted and filtered state estimators, \(x_{t}^{t-1}\) and \(x_{t}^{t}\), and the corresponding error covariance matrices \(P_{t}^{t-1}\) and \(P_{t}^{t}\). Of course, the predictor and filter estimators will depend on the parameters, \(\Theta\), of the DLM. In many situations, the parameters will be unknown and we will have to estimate them. Our focus will be on maximum likelihood estimation, but other authors have taken a Bayesian approach that assigns priors to the parameters, and then seeks posterior distributions of the model parameters; see, for example, Gordon and Smith [77], Pena and Guttman [151], or McCulloch and Tsay [135].

We now establish the recursions for the filters associated with the state \(x_{t}\) and the switching process, \(A_{t}\). As discussed in Sect. 6.3, the filters are also an essential part of the maximum likelihood procedure. The predictors, \(x_{t}^{t-1}=\mathrm{E}(x_{t}\mid y_{1:t-1})\), and filters, \(x_{t}^{t}=\mathrm{E}(x_{t}\mid y_{1:t})\), and their associated error variance-covariance matrices, \(P_{t}^{t-1}\) and \(P_{t}^{t}\), are given by

\[x_{t}^{t-1}=\Phi x_{t-1}^{t-1}, \tag{6.163}\]

\[P_{t}^{t-1}=\Phi P_{t-1}^{t-1}\Phi^{\prime}+Q, \tag{6.164}\]

\[x_{t}^{t}=x_{t}^{t-1}+\sum_{j=1}^{m}\pi_{j}(t|t)K_{tj}\epsilon_{tj}, \tag{6.165}\]

\[P_{t}^{t}=\sum_{j=1}^{m}\pi_{j}(t|t)(I-K_{tj}M_{j})P_{t}^{t-1}, \tag{6.166}\]

\[K_{tj}=P_{t}^{t-1}M_{j}^{\prime}\sum_{tj}^{-1}, \tag{6.167}\]where the innovation values in (6.165) and (6.167) are

\[\epsilon_{j}=y_{t}-M_{j}x_{t}^{t-1}, \tag{6.168}\]

\[\Sigma_{tj}=M_{j}P_{t}^{t-1}M_{j}^{\prime}+R, \tag{6.169}\]

for \(j=1,\ldots,m\).

Equations (6.163)-(6.167) exhibit the filter values as weighted linear combinations of the \(m\) innovation values, (6.168)-(6.169), corresponding to each of the possible measurement matrices. The equations are similar to the approximations introduced by Bar-Shalom and Tse [13], by Gordon and Smith [77], and Pena and Guttman [151].

To verify (6.165), let the indicator \(I(A_{t}=M_{j})=1\) when \(A_{t}=M_{j}\), and zero otherwise. Then, using (6.20),

\[x_{t}^{t} = \mathrm{E}(x_{t}\mid y_{1:t})=\mathrm{E}[\mathrm{E}(x_{t}\mid y_{ 1:t},A_{t})\mid y_{1:t}]\] \[= \mathrm{E}\left\{\sum_{j=1}^{m}\mathrm{E}(x_{t}\mid y_{1:t},A_{t} =M_{j})I(A_{t}=M_{j})\mid y_{1:t}\right\}\] \[= \mathrm{E}\left\{\sum_{j=1}^{m}[x_{t}^{t-1}+K_{tj}(y_{t}-M_{j}x_{ t}^{t-1})]I(A_{t}=M_{j})\mid y_{1:t}\right\}\] \[= \sum_{j=1}^{m}\pi_{j}(t\mid t)[x_{t}^{t-1}+K_{tj}(y_{t}-M_{j}x_{t }^{t-1})]\,,\]

where \(K_{tj}\) is given by (6.167). Equation (6.166) is derived in a similar fashion; the other relationships, (6.163), (6.164), and (6.167), follow from straightforward applications of the Kalman filter results given in Property 6.1.

Next, we derive the filters \(\pi_{j}(t|t)\). Let \(\mathrm{p}_{j}(t\mid t-1)\) denote the conditional density of \(y_{t}\) given the past \(y_{1:t-1}\), and \(A_{t}=M_{j}\), for \(j=1,\ldots,m\). Then,

\[\pi_{j}(t\mid t)=\frac{\pi_{j}(t)\mathrm{p}_{j}(t\mid t-1)}{\sum_{k=1}^{m}\pi_ {k}(t)\mathrm{p}_{k}(t\mid t-1)}, \tag{6.170}\]

where we assume the distribution \(\pi_{j}(t)\), for \(j=1,\ldots,m\) has been specified before observing \(y_{1:t}\) (details follow as in Example 6.21 below). If the investigator has no reason to prefer one state over another at time \(t\), the choice of uniform priors, \(\pi_{j}(t)=m^{-1}\), for \(j=1,\ldots,m\), will suffice. Smoothness can be introduced by letting

\[\pi_{j}(t)=\sum_{i=1}^{m}\pi_{i}(t-1\mid t-1)\,\pi_{ij}, \tag{6.171}\]

where the non-negative weights \(\pi_{ij}\) are chosen so \(\sum_{i=1}^{m}\pi_{ij}=1\). If the \(A_{t}\) process was Markov with transition probabilities \(\pi_{ij}\), then (6.171) would be the update for the filter probability, as shown in the next example.

**Example 6.21**: **Hidden Markov Chain Model**

If \(\{A_{t}\}\) is a hidden Markov chain with stationary transition probabilities \(\pi_{ij}=\Pr(A_{t}=M_{j}\mid A_{t-1}=M_{i})\), for \(i,j=1,\ldots,m\), we have

\[\pi_{j}(t\mid t) = \frac{\mathrm{p}(A_{t}=M_{j},y_{t}\mid y_{1:t-1})}{\mathrm{p}(y_{ t}\mid y_{1:t-1})} \tag{6.172}\] \[= \frac{\Pr(A_{t}=M_{j}\mid y_{1:t-1})\,\mathrm{p}(y_{t}\mid A_{t}= M_{j},y_{1:t-1})}{\mathrm{p}(y_{t}\mid y_{1:t-1})}\] \[= \frac{\pi_{j}(t\mid t-1)\,\mathrm{p}_{j}(t\mid t-1)}{\sum_{k=1}^ {m}\pi_{k}(t\mid t-1)\,\mathrm{p}_{k}(t\mid t-1)}.\]

In the Markov case, the conditional probabilities

\[\pi_{j}(t\mid t-1)=\Pr(A_{t}=M_{j}\mid y_{1:t-1})\]

in (6.172) replace the unconditional probabilities, \(\pi_{j}(t)=\Pr(A_{t}=M_{j})\), in (6.170).

To evaluate (6.172), we must be able to calculate \(\pi_{j}(t\mid t-1)\) and \(\mathrm{p}_{j}(t\mid t-1)\). We will discuss the calculation of \(\mathrm{p}_{j}(t\mid t-1)\) after this example. To derive \(\pi_{j}(t\mid t-1)\), note,

\[\pi_{j}(t\mid t-1) = \Pr(A_{t}=M_{j}\mid y_{1:t-1}) \tag{6.173}\] \[= \sum_{i=1}^{m}\Pr(A_{t}=M_{j},A_{t-1}=M_{i}\mid y_{1:t-1})\] \[= \sum_{i=1}^{m}\Pr(A_{t}=M_{j}\mid A_{t-1}=M_{i})\Pr(A_{t-1}=M_{i} \mid y_{1:t-1})\] \[= \sum_{i=1}^{m}\pi_{ij}\pi_{i}(t-1\mid t-1).\]

Expression (6.171) comes from equation (6.173), where, as previously noted, we replace \(\pi_{j}(t\mid t-1)\) by \(\pi_{j}(t)\).

The difficulty in extending the approach here to the Markov case is the dependence among the \(y_{t}\), which makes it necessary to enumerate over all possible histories to derive the filtering equations. This problem will be evident when we derive the conditional density \(\mathrm{p}_{j}(t\mid t-1)\). Equation (6.171) has \(\pi_{j}(t)\) as a function of the past observations, \(y_{1:t-1}\), which is inconsistent with our model assumption. Nevertheless, this seems to be a reasonable compromise that allows the data to modify the probabilities \(\pi_{j}(t)\), without having to develop a highly computer-intensive technique.

As previously suggested, the computation of \(\mathrm{p}_{j}(t\mid t-1)\), without some approximations, is highly computer-intensive. To evaluate \(\mathrm{p}_{j}(t\mid t-1)\), consider the event

\[\left\{A_{1}=M_{j_{1}},\ \ldots,A_{t-1}=M_{j_{t-1}}\right\}, \tag{6.174}\]

for \(j_{i}=1,\ldots,m\), and \(i=1,\ldots,t-1\), which specifies a specific set of measurement matrices through the past; we will write this event as \(A_{(t-1)}=M_{(\ell)}\). Becausepossible outcomes exist for \(A_{1},\ldots,A_{t-1}\), the index \(\ell\) runs through \(\ell=1,\ldots,m^{t-1}\). Using this notation, we may write

\[\mathrm{p}_{j}(t\mid t-1)\] \[\qquad=\sum_{\ell=1}^{m^{t-1}}\Pr\{A_{(t-1)}=M_{(\ell)}\bigm{|}y_ {1:t-1}\}\mathrm{p}(y_{t}\bigm{|}y_{1:t-1},A_{t}=M_{j},A_{(t-1)}=M_{(\ell)})\] \[\qquad\stackrel{{\mathrm{def}}}{{=}}\sum_{\ell=1}^{m ^{t-1}}\alpha(\ell)\,\mathfrak{g}\left(y_{t};\;\mu_{j}(\ell),\Sigma_{j}(\ell) \right),\quad j=1,\ldots,m, \tag{6.175}\]

where \(\mathfrak{g}(\cdot;\,\mu,\Sigma)\) represents the normal density with mean vector \(\mu\) and variance-covariance matrix \(\Sigma\). Thus, \(\mathrm{p}_{j}(t\mid t-1)\) is a mixture of normals with non-negative weights \(\alpha(\ell)=\Pr\{A_{(t-1)}=M_{(\ell)}\mid y_{1:t-1}\}\) such that \(\sum_{\ell}\alpha(\ell)=1\), and with each normal distribution having mean vector

\[\mu_{tj}(\ell)=M_{j}x_{t}^{t-1}(\ell)=M_{j}\,\mathrm{E}[x_{t}\mid y_{1:t-1},A _{(t-1)}=M_{(\ell)}] \tag{6.176}\]

and covariance matrix

\[\Sigma_{tj}(\ell)=M_{j}P_{t}^{t-1}(\ell)M_{j}^{\prime}+R. \tag{6.177}\]

This result follows because the conditional distribution of \(y_{t}\) in (6.175) is identical to the fixed measurement matrix case presented in Sect. 6.2. The values in (6.176) and (6.177), and hence the densities, \(\mathrm{p}_{j}(t\mid t-1)\), for \(j=1,\ldots,m\), can be obtained directly from the Kalman filter, Property 6.1, with the measurement matrices \(A_{(t-1)}\) fixed at \(M_{(\ell)}\).

Although \(\mathrm{p}_{j}(t\mid t-1)\) is given explicitly in (6.175), its evaluation is highly computer intensive. For example, with \(m=2\) states and \(n=20\) observations, we have to filter over \(2+2^{2}+\cdots+2^{20}\) possible sample paths (\(2^{20}=1,048,576\)). There are a few remedies to this problem. An algorithm that makes it possible to efficiently compute the most likely sequence of states given the data is known as the _Viterbi algorithm_, which is based on the well-known dynamic programming principle. Details may be found in Douc et al. [53, SS9.2]. Another remedy is to trim (remove), at each \(t\), highly improbable sample paths; that is, remove events in (6.174) with extremely small probability of occurring, and then evaluate \(\mathrm{p}_{j}(t\mid t-1)\) as if the trimmed sample paths could not have occurred. Another rather simple alternative, as suggested by Gordon and Smith [77] and Shumway and Stoffer [184], is to approximate \(\mathrm{p}_{j}(t\mid t-1)\) using the closest (in the sense of Kulback-Leibler distance) normal distribution. In this case, the approximation leads to choosing normal distribution with the same mean and variance associated with \(\mathrm{p}_{j}(t\mid t-1)\); that is, we approximate \(\mathrm{p}_{j}(t\mid t-1)\) by a normal with mean \(M_{j}x_{t}^{t-1}\) and variance \(\Sigma_{tj}\) given in (6.169).

To develop a procedure for maximum likelihood estimation, the joint density of the data is \[f(y_{1},\ldots,y_{n}) =\prod_{t=1}^{n}f(y_{t}\mid y_{1:t-1})\] \[=\prod_{t=1}^{n}\sum_{j=1}^{m}\Pr(A_{t}=M_{j}\mid y_{1:t-1})\text{p }(y_{t}\mid A_{t}=M_{j},y_{1:t-1}),\]

and hence, the likelihood can be written as

\[\ln L_{Y}(\Theta)=\sum_{t=1}^{n}\ln\left(\sum_{j=1}^{m}\pi_{j}(t)\text{p}_{j}(t \mid t-1)\right). \tag{6.178}\]

For the hidden Markov model, \(\pi_{j}(t)\) would be replaced by \(\pi_{j}(t\mid t-1)\). In (6.178), we will use the normal approximation to \(\text{p}_{j}(t\mid t-1)\). That is, henceforth, we will consider \(\text{p}_{j}(t\mid t-1)\) as the normal, \(\text{N}(M_{j}x_{t}^{t-1}\), \(\Sigma_{tj})\), density, where \(x_{t}^{t-1}\) is given in (6.163) and \(\Sigma_{tj}\) is given in (6.169). We may consider maximizing (6.178) directly as a function of the parameters \(\Theta\) in \(\{\mu_{0},\Phi,Q,R\}\) using a Newton method, or we may consider applying the EM algorithm to the complete data likelihood.

To apply the EM algorithm as in Sect. 6.3, we call \(x_{0:n}\), \(A_{1:n}\), and \(y_{1:n}\), the complete data, with likelihood given by

\[\begin{split}-2\ln L_{X,A,Y}(\Theta)&=\ln|\Sigma_{0 }|+(x_{0}-\mu_{0})^{\prime}\Sigma_{0}^{-1}(x_{0}-\mu_{0})\\ &+n\ln|Q|+\sum_{t=1}^{n}(x_{t}-\Phi x_{t-1})^{\prime}Q^{-1}(x_{t} -\Phi x_{t-1})\\ &-2\sum_{t=1}^{n}\sum_{j=1}^{m}I(A_{t}=M_{j})\ln\pi_{j}(t)+n\ln|R |\\ &+\sum_{t=1}^{n}\sum_{j=1}^{m}I(A_{t}=M_{j})(y_{t}-A_{t}x_{t})^{ \prime}R^{-1}(y_{t}-A_{t}x_{t}).\end{split} \tag{6.179}\]

As discussed in Sect. 6.3, we require the minimization of the conditional expectation

\[Q\left(\Theta\bigm{|}\Theta^{(k-1)}\right)=\text{E}\left\{-2\ln L_{X,A,Y}( \Theta)\Bigm{|}y_{:n},\Theta^{(k-1)}\right\}, \tag{6.180}\]

with respect to \(\Theta\) at each iteration, \(k=1,2,\ldots.\) The calculation and maximization of (6.180) is similar to the case of (6.63). In particular, with

\[\pi_{j}(t\mid n)=\text{E}[I(A_{t}=M_{j})\bigm{|}y_{1:n}], \tag{6.181}\]

we obtain on iteration \(k\),

\[\pi_{j}^{(k)}(t)=\pi_{j}(t\mid n), \tag{6.182}\]

\[\mu_{0}^{(k)}=x_{0}^{n}, \tag{6.183}\]

\[\Phi^{(k)}=S_{10}S_{00}^{-1}, \tag{6.184}\]

\[Q^{(k)}=n^{-1}\left(S_{11}-S_{10}S_{00}^{-1}S_{10}^{\prime}\right), \tag{6.185}\]and

\[R^{(k)}=n^{-1}\sum_{t=1}^{n}\sum_{j=1}^{m}\pi_{j}(t|n)\left[(y_{t}-M_{j}x_{t}^{n})( y_{t}-M_{j}x_{t}^{n})^{\prime}+M_{j}P_{t}^{n}M_{j}^{\prime}\right]. \tag{6.186}\]

where \(S_{11},S_{10},S_{00}\) are given in (6.65)-(6.67). As before, at iteration \(k\), the filters and the smoothers are calculated using the current values of the parameters, \(\Theta^{(k-1)}\), and \(\Sigma_{0}\) is held fixed. Filtering is accomplished by using (6.163)-(6.167). Smoothing is derived in a similar manner to the derivation of the filter, and one is led to the smoother given in Property 6.2 and Property 6.3, with one exception, the initial smoother covariance, (6.53), is now

\[P_{n,n-1}^{n}=\sum_{j=1}^{m}\pi_{j}(n|n)(I-K_{tj}M_{j})\Phi P_{n-1}^{n-1}. \tag{6.187}\]

Unfortunately, the computation of \(\pi_{j}(t\mid n)\) is excessively complicated, and requires integrating over mixtures of normal distributions. Shumway and Stoffer [184] suggest approximating the smoother \(\pi_{j}(t\mid n)\) by the filter \(\pi_{j}(t\mid t)\), and find the approximation works well.

**Example 6.22**: **Analysis of the Influenza Data**

We use the results of this section to analyze the U.S. monthly pneumonia and influenza mortality data plotted in Fig. 5.7. Letting \(y_{t}\) denote the observations at month \(t\), we model \(y_{t}\) in terms of a structural component model coupled with a hidden Markov process that determines whether a flu epidemic exists.

The model consists of three structural components. The first component, \(x_{t1}\), is an AR(2) process chosen to represent the periodic (seasonal) component of the data,

\[x_{t1}=\alpha_{1}x_{t-1,1}+\alpha_{2}x_{t-2,1}+w_{t1}, \tag{6.188}\]

where \(w_{t1}\) is white noise, with \(\text{var}(w_{t1})=\sigma_{1}^{2}\). The second component, \(x_{t2}\), is an AR(1) process with a nonzero constant term, which is chosen to represent the sharp rise in the data during an epidemic,

\[x_{t2}=\beta_{0}+\beta_{1}x_{t-1,2}+w_{t2}, \tag{6.189}\]

where \(w_{t2}\) is white noise, with \(\text{var}(w_{t2})=\sigma_{2}^{2}\). The third component, \(x_{t3}\), is a fixed trend component given by,

\[x_{t3}=x_{t-1,3}+w_{t3}, \tag{6.190}\]

where \(\text{var}(w_{t3})=0\). The case in which \(\text{var}(w_{t3})>0\), which corresponds to a stochastic trend (random walk), was tried here, but the estimation became unstable, and lead to us fitting a fixed, rather than stochastic, trend. Thus, in the final model, the trend component satisfies \(\nabla x_{t3}=0\); recall in Example 6.18 the data were also differenced once before fitting the model.

Throughout the years, periods of normal influenza mortality (state 1) are modeled as

\[y_{t}=x_{t1}+x_{t3}+v_{t}, \tag{6.191}\]

where the measurement error, \(v_{t}\), is white noise with \(\text{var}(v_{t})=\sigma_{v}^{2}\). When an epidemic occurs (state 2), mortality is modeled as

\[y_{t}=x_{t1}+x_{t2}+x_{t3}+v_{t}. \tag{6.192}\]

The model specified in (6.188)-(6.192) can be written in the general state-space form. The state equation is

\[\begin{pmatrix}x_{t1}\\ x_{t-1,1}\\ x_{t2}\\ x_{t3}\end{pmatrix}=\begin{bmatrix}\alpha_{1}&\alpha_{2}&0&0\\ 1&0&0&0\\ 0&0&\beta_{1}&0\\ 0&0&0&1\end{bmatrix}\begin{pmatrix}x_{t-1,1}\\ x_{t-2,1}\\ x_{t-1,2}\\ x_{t-1,3}\end{pmatrix}+\begin{pmatrix}0\\ 0\\ \beta_{0}\\ 0\end{pmatrix}+\begin{pmatrix}w_{t1}\\ 0\\ w_{t2}\\ 0\end{pmatrix}. \tag{6.193}\]

Of course, (6.193) can be written in the standard state-equation form as

\[x_{t}=\Phi x_{t-1}+\gamma u_{t}+w_{t}, \tag{6.194}\]

where \(x_{t}=(x_{t1},x_{t-1,1},x_{t2},x_{t3})^{\prime}\), \(\Upsilon=(0,0,\beta_{0},0)^{\prime}\), \(u_{t}\equiv 1\), and \(Q\) is a \(4\times 4\) matrix with \(\sigma_{1}^{2}\) as the (1,1)-element, \(\sigma_{2}^{2}\) as the (3,3)-element, and the remaining elements set equal to zero. The observation equation is

\[y_{t}=A_{t}x_{t}+v_{t}, \tag{6.195}\]

where \(A_{t}\) is \(1\times 4\), and \(v_{t}\) is white noise with \(\text{var}(v_{t})=R=\sigma_{v}^{2}\). We assume all components of variance \(w_{t1}\), \(w_{t2}\), and \(v_{t}\) are uncorrelated.

As discussed in (6.191) and (6.192), \(A_{t}\) can take one of two possible forms

\[A_{t} = M_{1}=[1,0,0,1]\quad\text{no epidemic},\] \[A_{t} = M_{2}=[1,0,1,1]\quad\text{epidemic},\]

corresponding to the two possible states of (1) no flu epidemic and (2) flu epidemic, such that \(\text{Pr}(A_{t}=M_{1})=1-\text{Pr}(A_{t}=M_{2})\). In this example, we will assume \(A_{t}\) is a hidden Markov chain, and hence we use the updating equations given in Example 6.21, (6.172) and (6.173), with transition probabilities \(\pi_{11}=\pi_{22}=.75\) (and, thus, \(\pi_{12}=\pi_{21}=.25\)).

Parameter estimation was accomplished using a quasi-Newton-Raphson procedure to maximize the approximate log likelihood given in (6.178), with initial values of \(\pi_{1}(1\mid 0)=\pi_{2}(1\mid 0)=.5\). Table 6.3 shows the results of the estimation procedure. On the initial fit, two estimates are not significant, namely, \(\hat{\beta}_{1}\) and \(\hat{\sigma}_{v}\). When \(\sigma_{v}^{2}=0\), there is no measurement error, and the variability in data is explained solely by the variance components of the state system, namely, \(\sigma_{1}^{2}\) and \(\sigma_{2}^{2}\). The case in which \(\beta_{1}=0\) corresponds to a simple level shift during a flu epidemic. In the final model, with \(\beta_{1}\) and \(\sigma_{v}^{2}\) removed, the estimated level shift (\(\hat{\beta}_{0}\)) corresponds to an increase in mortality by about.2 per 1000 during a flu epidemic. The estimates for the final model are also listed in Table 6.3.

Figure 6.16a shows a plot of the data, \(y_{t}\), for the ten-year period of 1969-1978 as well as an indicator that takes the value of 1 if \(\hat{\pi}_{1}(t\mid t-1)\geq.5\), or 2 if \(\hat{\pi}_{2}(t\mid t-1)>.5\). The estimated prediction probabilities do a reasonable job of predicting a flu epidemic, although the peak in 1972 is missed.

Figure 6.16b shows the estimated filtered values (that is, filtering is done using the parameter estimates) of the three components of the model, \(x_{t1}^{t}\), \(x_{t2}^{t}\), and \(x_{t3}^{t}\). Except for initial instability (which is not shown), \(\hat{x}_{t1}^{t}\) represents the seasonal (cyclic) aspect of the data, \(\hat{x}_{t2}^{t}\) represents the spikes during a flu epidemic, and \(\hat{x}_{t3}^{t}\) represents the slow decline in flu mortality over the ten-year period of 1969-1978.

One-month-ahead prediction, say, \(\hat{y}_{t}^{t-1}\), is obtained as

\[\hat{y}_{t}^{t-1} = M_{1}\hat{x}_{t}^{t-1}\quad\text{if}\quad\hat{\pi}_{1}(t\mid t-1)>\hat{\pi}_{2}(t\mid t-1),\] \[\hat{y}_{t}^{t-1} = M_{2}\hat{x}_{t}^{t-1}\quad\text{if}\quad\hat{\pi}_{1}(t\mid t-1)\leq\hat{\pi}_{2}(t\mid t-1).\]

Of course, \(\hat{x}_{t}^{t-1}\) is the estimated state prediction, obtained via the filter presented in (6.163)-(6.167) (with the addition of the constant term in the model) using the estimated parameters. The results are shown in Fig. 6.16(c). The precision of the forecasts can be measured by the innovation variances, \(\Sigma_{t1}\) when no epidemic is predicted, and \(\Sigma_{t2}\) when an epidemic is predicted. These values become stable quickly, and when no epidemic is predicted, the estimated standard prediction error is approximately.02 (this is the square root of \(\Sigma_{t1}\) for \(t\) large); when a flu epidemic is predicted, the estimated standard prediction error is approximately.11.

The results of this analysis are impressive given the small number of parameters and the degree of approximation that was made to obtain a computationally simple method for fitting a complex model.

Further evidence of the strength of this technique can be found in the example given in Shumway and Stoffer [184].

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Initial model & Final model \\ Parameter & estimates & estimates \\ \hline \(\alpha_{1}\) & 1.422 (.100) & 1.406 (.079) \\ \(\alpha_{2}\) & \(-.634\) (.089) & \(-.622\) (.069) \\ \(\beta_{0}\) &.276 (.056) &.210 (.025) \\ \(\beta_{1}\) & \(-.312\) (.218) & — \\ \(\sigma_{1}\) &.023 (.003) &.023 (.005) \\ \(\sigma_{2}\) &.108 (.017) &.112 (.017) \\ \(\sigma_{v}\) &.002 (.009) & — \\ \hline \end{tabular} Estimated standard errors in parentheses

\end{table}
Table 6.3: Estimation results for influenza data The R code for the final model estimation is as follows.

 y = as.matrix(flu); num = length(y); nstate = 4;  M1 = as.matrix(cbind(1,0,0,1)) # obs matrix normal  M2 = as.matrix(cbind(1,0,1,1)) # obs matrix flu epi  prob = matrix(@,num,1); yp = y # to store pi2(t/t-1) & y(t/t-1)  xfilter = array(0, dim=c(nstate,1,num)) # to store x(t/t)  # Function to Calculate Likelihood  Linn = function(para){  alpha1 = para[1]; alpha2 = para[2]; beta@ = para[3]  sQ1 = para[4]; sQ2 = para[5]; like=0  xf = matrix(@, nstate, 1) # x filter  xp = matrix(@, nstate, 1) # x pred  Pf = diag(.1, nstate) # filter cov  Pp = diag(.1, nstate) # pred cov  pi11 <.75 > pi21; pi12 <.25 -> pi21; pif1 <.5 -> pif2  phi = matrix(@, nstate,nstate)  phi[1,1] = alpha1; phi[1,2] = alpha2; phi[2,1]=1; phi[4,4]=1  Ups = as.matrix(rbind(@,0,beta@,0))  Q = matrix(@, nstate,nstate)  Qi[1,1] = sQ1*2; Qi[3,3] = sQ2*2; R=0 # R=0 in final model  # begin filtering #

Figure 6.16: **(a)** Influenza data, \(y_{t}\), (line–points) and a prediction indicator (1 or 2) that an epidemic occurs in month \(t\) given the data up to month \(t-1\) (_dashed line_). **(b)** The three filtered structural components of influenza mortality: \(\hat{x}_{t1}^{t}\) (_cyclic trace_), \(\hat{x}_{t2}^{t}\) (_spiked trace_), and \(\hat{x}_{t3}^{t}\) (_negative linear trace_). **(c)** One-month-ahead predictions shown as upper and lower limits \(\hat{y}_{t}^{t-1}\pm 2\sqrt{\hat{\beta}_{t}^{t-1}}\) (_gray swatch_), of the number of pneumonia and influenza deaths, and \(y_{t}\) (_points_)

for(i in 1:num){  xp = phi%*%xf + Ups; Pp = phi%*%Pf%*%t(phi) + Q  sig1 = as.numeric(M1%*%Pp%*%t(M1) + R)  sig2 = as.numeric(M2%*%Pp%*%t(M2) + R)  k1 = Pp%*%t(M1)/sig1; k2 = Pp%*%t(M2)/sig2  e1 = y[i]-M1%*%xp; e2 = y[i]-M2%*%xp  pip1 = pif1*pi11 + pif2*pi21; pip2 = pif1*pi12 + pif2*pi22  den1 = (1/sqrt(sig1))*exp(-.5*e1*2/sig1)  den2 = (1/sqrt(sig2))*exp(-.5*e2*2/sig2)  denm = pip1*den1 + pip2*den2  pif1 = pip1*den1/denm; pif2 = pip2*den2/denm  pif1 = as.numeric(pif1); pif2 = as.numeric(pif2)  e1 = as.numeric(e1); e2-as.numeric(e2)  xf = xp + pif1*k1*e1 + pif2*k2*e2  eye = diag(1, nstate)  Pf = pif1*(eye-k1%*%M1)%*%P + pif2*(eye-k2%*%M2)%*%P  like = like - log(pip1*den1 + pip2*den2)  prob[i]<<-pip2; xfilter[,,i]<<-xf; innov.sig<<-c(sig1,sig2) yp[i]<<-ifelse(pip1 > pip2, M1%*%xp, M2%*%xp) } return(like) }
Estimation alpha1 = 1.4; alpha2 = -.5; beta@ =.3; sQ1 =.1; sQ2 =.1  init.par = c(alpha1, alpha2, beta@, sQ1, sQ2) (est = optim(init.par, Linn, NULL, method='BFGS', hessian-TRUE,  control=list(trace=1,REPORT=1)))  SE = sqrt(diag(solve(estShessian))) u = cbind(estimate=estSpar, SE)  rownames(u)=c('alpha1','alpha2','beta@','sQ1','sQ2'); u  estimate SE  alpha1 1.40570967 0.078587727  alpha2 -0.62198715 0.068733109  beta@ 0.21049042 0.024625302  sQ1 0.02310306 0.001635291  sQ2 0.11217287 0.016684663
Graphics predepi = ifelse(prob<.5,@,1); k = 6:length(y)  Time = time(flu)[k]  regime = predepi[k]+1  par(mfrow=c(3, 1, marc=c(2,3,1,1)+.1)  plot(Time, y[k], type="n", ylab="")  grid(lty=2); lines(Time, y[k], col=gray(.7))  text(Time, y[k], col=regime, labels=regime, cex=1.1)  text(1979,.95,"(a)")  plot(Time, xfilter[1,.k], type="n", ylim<c(-.1,.4), ylab="")  grid(lty=2); lines(Time, xfilter[1,.k])  lines(Time, xfilter[3,.k]); lines(Time, xfilter[4,.k])  text(1979,.35,"(b)")  plot(Time, y[k], type="n", ylim<c(.1,.9),ylab="")  grid(lty=2); points(Time, y[k], pch=19)  prde1 = 2*sqrt(innov.sig[1]); prde2 = 2*sqrt(innov.sig[2])  prde = ifelse(predepi[k]<.5, prde1,prde2)  xx = c(Time, rev(Time))  yy = c(yp[k]-prde, rev(yp[k]+prde))  polygon(xx, yy, border=8, col=gray(.6, alpha=.3))  text(1979,.85,"(c)")

### Stochastic Volatility

Stochastic volatility (SV) models are an alternative to GARCH-type models that were presented in Chap. 5. Throughout this section, we let \(r_{t}\) denote the returns of some financial asset. Most models for return data used in practice are of a multiplicative form that we have seen in Sect. 5.3,

\[r_{t}=\sigma_{t}\varepsilon_{t}\,, \tag{6.196}\]

where \(\epsilon_{t}\) is an iid sequence and the _volatility process_, \(\sigma_{t}\), is a non-negative stochastic process such that \(\varepsilon_{t}\) is independent of \(\sigma_{s}\) for all \(s\leq t\). It is often assumed that \(\varepsilon_{t}\) has zero mean and unit variance.

In SV models, the volatility is a nonlinear transform of a hidden linear autoregressive process where the hidden volatility process, \(x_{t}=\log\sigma_{t}^{2}\), follows a first order autoregression,

\[x_{t} =\phi x_{t-1}+w_{t}\,, \tag{6.197a}\] \[r_{t} =\beta\exp(x_{t}/2)\varepsilon_{t}\,, \tag{6.197b}\]

where \(w_{t}\!\sim\!\mbox{iid}\,\,\,\,\mbox{N}(0,\sigma_{w}^{2})\) and \(\epsilon_{t}\) is iid noise having finite moments. The error processes \(w_{t}\) and \(\varepsilon_{t}\) are assumed to be mutually independent and \(|\phi|<1\). As \(w_{t}\) is normally distributed, \(x_{t}\) is also normally distributed. All moments of \(\varepsilon_{t}\) exist, so that all moments of \(r_{t}\) in (6.197) exist as well. Assuming that \(x_{0}\sim\mbox{N}(0,\sigma_{w}^{2}/(1-\phi^{2}))\) [the stationary distribution] the kurtosis6 of \(r_{t}\) is given by

Footnote 6: For an integer \(m\) and a random variable \(U\), \(\kappa_{m}(U):=\mbox{E}[|U|^{m}]/(\mbox{E}[|U|^{2}])^{m/2}\). Typically, \(\kappa_{3}\) is called _skewness_ and \(\kappa_{4}\) is called _kurtosis_.

\[\kappa_{4}(r_{t})=\kappa_{4}(\varepsilon_{t})\exp(\sigma_{x}^{2}), \tag{6.198}\]

where \(\sigma_{x}^{2}=\sigma_{w}^{2}/(1-\phi^{2})\) is the (stationary) variance of \(x_{t}\). Thus \(\kappa_{4}(r_{t})>\kappa_{4}(\varepsilon_{t})\), so that if \(\varepsilon_{t}\!\sim\!\mbox{iid}\,\,\,\mbox{N}(0,1)\), the distribution of \(r_{t}\) is leptokurtic. The autocorrelation function of \(\{r_{t}^{2m};t=1,2,\dots\}\) for any integer \(m\) is given by (see Problem 6.29)

\[\mbox{corr}(r_{t+h}^{2m},r_{t}^{2m})=\frac{\exp(m^{2}\sigma_{x}^{2}\phi^{h})- 1}{\kappa_{4m}(\varepsilon_{t})\exp(m^{2}\sigma_{x}^{2})-1}\,. \tag{6.199}\]

The decay rate of the autocorrelation function is faster than exponential at small time lags and then stabilizes to \(\phi\) for large lags.

Sometimes it is easier to work with the linear form of the model where we define

\[y_{t}=\log r_{t}^{2}\quad\mbox{and}\quad v_{t}=\log\varepsilon_{t}^{2}\,,\]

in which case we may write

\[y_{t}=\alpha+x_{t}+v_{t}. \tag{6.200}\]

A constant is usually needed in either the state equation or the observation equation (but not typically both), so we write the state equation as

\[x_{t}=\phi_{0}+\phi_{1}x_{t-1}+w_{t}, \tag{6.201}\]where \(w_{t}\) is white Gaussian noise with variance \(\sigma_{w}^{2}\). The constant \(\phi_{0}\) is sometimes referred to as the _leverage effect_. Together, (6.200) and (6.201) make up the stochastic volatility model due to Taylor [199].

If \(\varepsilon_{t}^{2}\) had a log-normal distribution, (6.200)-(6.201) would form a Gaussian state-space model, and we could then use standard DLM results to fit the model to data. Unfortunately, that assumption does not seem to work well. Instead, one often keeps the ARCH normality assumption on \(\varepsilon_{t}\!\sim\!\text{iid N}(0,1)\), in which case, \(v_{t}=\log\varepsilon_{t}^{2}\) is distributed as the log of a chi-squared random variable with one degree of freedom. This density is given by

\[f(v)=\tfrac{1}{\sqrt{2\pi}}\exp\left\{-\tfrac{1}{2}\left(\mathrm{e}^{v}-v \right)\right\}\quad-\infty<v<\infty\,. \tag{6.202}\]

The mean of the distribution is \(-(\gamma+\log\ 2)\), where \(\gamma\approx 0.5772\) is Euler's constant, and the variance of the distribution is \(\pi^{2}/2\). It is a highly skewed density (see Figure 6.18) but it is not flexible because there are no free parameters to be estimated.

Various approaches to the fitting of stochastic volatility models have been examined; these methods include a wide range of assumptions on the observational noise process. A good summary of the proposed techniques, both Bayesian (via MCMC) and non-Bayesian approaches (such as quasi-maximum likelihood estimation and the EM algorithm), can be found in Jacquier et al. [104], and Shephard [177]. Simulation methods for classical inference applied to stochastic volatility models are discussed in Danielson [48] and Sandmann and Koopman [171].

Kim, Shephard and Chib [118] proposed modeling the log of a chi-squared random variable by a mixture of seven normals to approximate the first four moments of the observational error distribution; the mixture is fixed and no additional model parameters are added by using this technique. The basic model assumption that \(\varepsilon_{t}\) is Gaussian is unrealistic for most applications. In an effort to keep matters simple but more general (in that we allow the observational error dynamics to depend on parameters that will be fitted), our method of fitting stochastic volatility models is to retain the Gaussian state equation (6.201), but to write the observation equation, as

\[y_{t}=\alpha+x_{t}+\eta_{t}, \tag{6.203}\]

where \(\eta_{t}\) is white noise, whose distribution is a mixture of two normals, one centered at zero. In particular, we write

\[\eta_{t}=I_{t}z_{t0}+(1-I_{t})z_{t1}, \tag{6.204}\]

where \(I_{t}\) is an iid Bernoulli process, \(\Pr\{I_{t}=0\}=\pi_{0}\), \(\Pr\{I_{t}=1\}=\pi_{1}\) (\(\pi_{0}+\pi_{1}=1\)), \(z_{t0}\sim\text{iid N}(0,\sigma_{0}^{2})\), and \(z_{t1}\sim\text{iid N}(\mu_{1},\sigma_{1}^{2})\).

The advantage to this model is that it is easy to fit because it uses normality. In fact, the model equations (6.201) and (6.203)-(6.204) are similar to those presented in Pena and Guttman [151], who used the idea to obtain a robust Kalman filter, and, as previously mentioned, in Kim et al. [118]. The material presented in Sect. 6.10 applies here, and in particular, the filtering equations for this model are \[x_{t+1}^{t} = \phi_{0}+\phi_{1}x_{t}^{t-1}+\sum_{j=0}^{1}\pi_{tj}K_{tj}\epsilon_{tj}, \tag{6.205}\] \[P_{t+1}^{t} = \phi_{1}^{2}P_{t}^{t-1}+\sigma_{w}^{2}-\sum_{j=0}^{1}\pi_{tj}K_{tj} ^{2}\Sigma_{tj},\] (6.206) \[\epsilon_{t0} = y_{t}-\alpha-x_{t}^{t-1}, \epsilon_{t1} = y_{t}-\alpha-x_{t}^{t-1}-\mu_{1},\] (6.207) \[\Sigma_{t0} = P_{t}^{t-1}+\sigma_{0}^{2}, \Sigma_{t1} = P_{t}^{t-1}+\sigma_{1}^{2},\] (6.208) \[K_{t0} = \phi_{1}P_{t}^{t-1}\ \big{/}\ \Sigma_{t0}, K_{t1} = \phi_{1}P_{t}^{t-1}\ \big{/}\ \Sigma_{t1}. \tag{6.209}\]

To complete the filtering, we must be able to assess the probabilities \(\pi_{t1}=\Pr(I_{t}=1\mid y_{1:t})\), for \(t=1,\ldots,n\); of course, \(\pi_{t0}=1-\pi_{t1}\). Let \(\mathrm{p}_{j}(t\mid t-1)\) denote the conditional density of \(y_{t}\) given the past \(y_{1:t-1}\), and \(I_{t}=j\) for \(j=0,1\). Then,

\[\pi_{t1}=\frac{\pi_{1}\mathrm{p}_{1}(t\mid t-1)}{\pi_{0}\mathrm{p}_{0}(t\mid t- 1)+\pi_{1}\mathrm{p}_{1}(t\mid t-1)}, \tag{6.210}\]

where we assume the distribution \(\pi_{j}\), for \(j=0,1\) has been specified _a priori_. If the investigator has no reason to prefer one state over another the choice of uniform priors, \(\pi_{1}=1/2\), will suffice. Unfortunately, it is computationally difficult to obtain the exact values of \(\mathrm{p}_{j}(t\mid t-1)\); although we can give an explicit expression of \(\mathrm{p}_{j}(t\mid t-1)\), the actual computation of the conditional density is prohibitive. A viable approximation, however, is to choose \(\mathrm{p}_{j}(t\mid t-1)\) to be the normal density, \(\mathrm{N}(x_{t}^{t-1}+\mu_{j},\,\Sigma_{tj})\), for \(j=0,1\) and \(\mu_{0}=0\); see Sect. 6.10 for details.

The innovations filter given in (6.205)-(6.210) can be derived from the Kalman filter by a simple conditioning argument; e.g., to derive (6.205), write

\[\mathrm{E}\left(x_{t+1}\mid y_{1:t}\right) = \sum_{j=0}^{1}\mathrm{E}\left(x_{t+1}\mid y_{1:t},I_{t}=j\right) \Pr(I_{t}=j\mid y_{1:t})\] \[= \sum_{j=0}^{1}\left(\phi_{0}+\phi_{1}x_{t}^{t-1}+K_{tj}\epsilon_{ tj}\right)\pi_{tj}\] \[= \phi_{0}+\phi_{1}x_{t}^{t-1}+\sum_{j=0}^{1}\pi_{tj}K_{tj}\epsilon_ {tj}.\]

Estimation of the parameters, \(\Theta=(\phi_{0},\phi_{1},\sigma_{0}^{2},\mu_{1},\sigma_{1}^{2},\sigma_{w}^{2})\), is accomplished via MLE based on the likelihood given by

\[\ln L_{Y}(\Theta)=\sum_{t=1}^{n}\ln\left(\sum_{j=0}^{1}\pi_{j}\ f_{j}(t\mid t- 1)\right), \tag{6.211}\]

where the density \(\mathrm{p}_{j}(t\mid t-1)\) is approximated by the normal density, \(\mathrm{N}(x_{t}^{t-1}+\mu_{j},\sigma_{j}^{2})\), previously mentioned. We may consider maximizing (6.211) directly as a function of the parameters \(\Theta\) using a Newton method, or we may consider applying the EM algorithm to the complete data likelihood.

**Example 6.23**: **Analysis of the New York Stock Exchange Returns**

Figure 6.17 shows the returns, \(r_{t}\), for about 400 of the 2000 trading days of the NYSE. Model (6.201) and (6.203)-(6.204), with \(\pi_{1}\) fixed at.5, was fit to the data using a quasi-Newton-Raphson method to maximize (6.211). The results are given in Table 6.4. Figure 6.18 compares the density of the log of a \(\chi^{2}_{1}\) with the fitted normal mixture; we note the data indicate a substantial amount of probability in the upper tail that the log-\(\chi^{2}_{1}\) distribution misses.

Finally, Figure 6.17 also displays the one-step-ahead predicted log volatility, \(\hat{x}^{t-1}_{t}\) where \(x_{t}=\log\sigma^{2}_{t}\), surrounding the crash of October 19, 1987. The analysis indicates that \(\phi_{0}\) is not needed. The R code when \(\phi_{0}\) is included in the model is as follows.

y = log(nyse^2) num = length(y)
Initial Parameters phi0 = 0; phi1 =.95; sQ =.2; alpha = mean(y) sR0 = 1; mu1 = -3; sR1 =2 init.par = c(phi0, phi1, sQ, alpha, sR0, mu1, sR1)
Innovations Likelihood Linn = function(para){ phi0 = para[1]; phi1 = para[2]; sQ = para[3]; alpha = para[4] sR0 = para[5]; mu1 = para[6]; sR1 = para[7] sv = SVfilter(num, y, phi0, phi1, sQ, alpha, sR0, mu1, sR1) return(svlike) # Estimation (est = optim(init.par, Linn, NULL, method-'BFGS', hessian-TRUE, control-list(trace-1,REPORT=1))) SE = sqrt(diag(solve(estShessian))) u = cbind(estimates=est$par, SE) rownames(u)=c('phi0','phi1','s0','alpha','sigv0','mu1','sigv1'); u
Graphics (need filters at the estimated parameters) phi0 = est$par[1]; phi1 = est$par[2]; sQ = est$par[3]; alpha = est$par[4] sR0 = est$par[5]; mu1 = est$par[6]; sR1 = est$par[7] sv = SVfilter(num,y,phi0,phi1,sQ,alpha,sR0,mu1,sR1)

Figure 6.17: Approximately four hundred observations of \(r_{t}\), the daily returns of the NYSE surrounding the crash of October 19, 1987. Also displayed is the corresponding one-step-ahead predicted log volatility, \(\hat{x}^{t-1}_{t}\) where \(x_{t}=\log\sigma^{2}_{t}\), scaled by.1 to fit on the plot

\begin{table}
\begin{tabular}{c c c} \hline \hline  & \multicolumn{2}{c}{Estimated} \\ Parameter & Estimate & standard error \\ \hline \(\phi_{0}\) & \(-.006\) & \(.016^{\dagger}\) \\ \(\phi_{1}\) &.988 &.007 \\ \(\sigma_{w}\) &.091 &.027 \\ \(\alpha\) & \(-9.613\) & 1.269 \\ \(\sigma_{0}\) & 1.220 &.065 \\ \(\mu_{1}\) & \(-2.292\) &.205 \\ \(\sigma_{1}\) & 2.683 &.105 \\ \hline \multicolumn{3}{l}{\({}^{\dagger}\) not significant} \\ \end{tabular}
\end{table}
Table 4: Estimation results for the NYSE fitwhere

\[F=\left[\begin{array}{cc}\phi_{1}&0\\ 1&0\end{array}\right],\quad G_{t}=\left[\begin{array}{c}\phi_{0}\\ \alpha+\pi_{t1}\mu_{1}\end{array}\right],\quad H_{t}=\left[\begin{array}{cc} \pi_{t0}K_{t0}\Sigma_{t0}^{1/2}&\pi_{t1}K_{t1}\Sigma_{t1}^{1/2}\\ \pi_{t0}\Sigma_{t0}^{1/2}&\pi_{t1}\Sigma_{t1}^{1/2}\end{array}\right].\]

Hence, the steps in bootstrapping for this case are the same as steps (i) through (v) described in Sect. 6.7, but with (6.123) replaced by the following first-order equation:

\[\xi_{t}^{*}=F(\hat{\Theta})\xi_{t-1}^{*}+G_{t}(\hat{\Theta};\hat{\pi}_{t1})+H_{ t}(\hat{\Theta};\hat{\pi}_{t1})e_{t}^{*}, \tag{6.215}\]

where \(\hat{\Theta}=\{\hat{\phi}_{0},\hat{\phi}_{1},\hat{\sigma}_{0}^{2},\hat{\alpha},\hat{\mu}_{1},\hat{\sigma}_{1}^{2},\hat{\sigma}_{w}^{2}\}\) is the MLE of \(\Theta\), and \(\hat{\pi}_{t1}\) is estimated via (6.210), replacing \(\mathrm{p}_{1}(t\mid t-1)\) and \(\mathrm{p}_{0}(t\mid t-1)\) by their respective estimated normal densities (\(\hat{\pi}_{t0}=1-\hat{\pi}_{t1}\)).

**Example 6.24**: **Analysis of the U.S. GNP Growth Rate**

In Example 5.4, we fit an ARCH model to the U.S. GNP growth rate. In this example, we will fit a stochastic volatility model to the residuals from the AR(1) fit on the growth rate (see Example 3.39). Figure 6.19 shows the log of the squared residuals, say \(y_{t}\), from the fit on the U.S. GNP series. The stochastic volatility model (6.200)-(6.204) was then fit to \(y_{t}\). Table 6.5 shows the MLEs of the model parameters along with their asymptotic SEs assuming the model is correct. Also displayed in Table 6.5 are the SEs of \(B=500\) bootstrapped samples. There is little agreement between most of the asymptotic values and the bootstrapped values. The interest here, however, is not so much in the SEs, but in the actual sampling distribution of the estimates. For example, Fig. 6.19 compares the bootstrap histogram and asymptotic normal distribution of \(\hat{\phi}_{1}\). In this case, the bootstrap distribution exhibits positive kurtosis and skewness which is missed by the assumption of asymptotic normality.

The R code for this example is as follows. We held \(\phi_{0}\) at 0 for this analysis because it was not significantly different from 0 in an initial analysis.

n.boot = 500 # number of bootstrap replicates

tol = sqrt(.MachineSdouble.eps) # convergence tolerance

gnpgr = diff(log(gnp))

Figure 6.18: Density of the log of a \(\chi_{1}^{2}\) as given by (6.202) (_solid line_) and the fitted normal mixture (_dashed line_) from Example 6.23

fit = arima(gnpgr, order=c(1,0,0)) y = as.matrix(log(resid(fit)^2)) num = length(y) plot.ts(y, ylab='')
Initial Parameters phi1 =.9; SQ =.5; alpha = mean(y); sR0 = 1; mm1 = -3; sR1 = 2.5 init.par = c(phi1, SQ, alpha, sR0, mm1, sR1)
Innovations Likelihood Linn = function(para, y.data){ phi1 = para[1]; sQ = para[2]; alpha = para[3] sR0 = para[4]; mm1 = para[5]; sR1 = para[6] sv = S\filter(num, y.data, 0, phi1, sQ, alpha, sR0, mm1, sR1) return(sv5like) # Estimation (est = optim(init.par, Linn, NULL, y.data=y, method='BFGS', hessian=TRUE, control=list(trace=1,REPORT=1))) SE = sqrt(diag(solve(estShessian))) u = rbind(estimates=estPpar, SE) colnames(u)=c('phi1','sQ','alpha','sig0','mu1','sig1'); round(u, 3) phi1 sQ alpha sig0 mu1 sig1 estimates 0.884 0.381 -9.654 0.835 -2.350 2.453 SE 0.109 0.221 0.343 0.204 0.495 0.293
Bootstrap para.star = matrix(0, n.boot, 6) # to store parameter estimates for (jb in 1:n.boot){ cat('iteration', jb, '\n') phi1 = estPpar[1]; sQ = estPpar[2]; alpha = estPpar[3] sR0 = estPpar[4]; mu1 = estPpar[5]; sR1 = estPpar[6] Q = sQ^2; R0 = sR0^2; R1 = sR1^2 sV = SVfilter(num, y, @, phi1, sQ, alpha, sR0, mm1, sR1) sig0 = sv5Pp+R0; sig1 = sv5Pp+R1; K0 = sv5Pp/sig0; K1 = sv5Pp/sig1 inn0 = y-sv5xp-alpha; inn1 = y-sv5xp-mul-alpha den1 = (1/sqrt(sig1))*exp(-.5"inn1^2/sig1) den0 = (1/sqrt(sig0))*exp(-.5"inn0^2/sig0)

Figure 19: Results for Example 6.24: Log of the squared residuals from an AR(1) fit on GNP growth rate. Bootstrap histogram and asymptotic distribution of \(\hat{\phi}_{1}\)

fpi1 = den1/(den0+den1)  # start resampling at t=4  e0 = inn0/sqrt(sig0); e1 = inn1/sqrt(sig1)  indx = sample(4:num, replace=TRUE)  sinn = cbind(c(e0[1:3], e0[indx]), c(e1[1:3], e1[indx]))  ef = matrix(c(phi1, 1, 0, 0), 2, 2)  xi = cbind(sv$xp,y) # initialize  for (i in 4:num){ # generate boot sample  G = matrix(c(0, alpha+fpi1[i]*mul), 2, 1)  h21 = {fpi1[i]*sqrt(sig0[1]); h11 = h21*K0[i]  h22 = fpi1[i]*sqrt(sig1[1]); h12 = h22*K1[i]  H = matrix(c(h11,h21,h12,h22),2,2)  xi[i,] = {eY%as.matrix(xi[i-1,],2) + G + HK*%as.matrix(sinn[i,],2))}  # Estimates from boot data  y.star = xi[,2]  phi1=.9; SQ=.5; alpha=mean(y.star); sR0=1; mul=-3; sR1=2.5  init.par = c(phi1, sQ, alpha, sR0, mul, sR1) # same as for data  est.star = optim(init.par, Linn, NULL, y.data=y.star, method-'BFGS',  control=list(reltol=tol))  para.star[jb,] = cbind(est.star$par[1], abs(est.star$par[2]),  est.star$par[3], abs(est.star$par[4]), est.star$par[5],  abs(est.star$par[6])) }  # Some summary statistics and graphics  rmse = rep(NA,6) # SEs from the bootstrap  for(i in 1:6){  rmse[i] = sqrt(sum((para.star[,i]-est$par[i])^2)/n.boot)  cat(i, rmse[i], '\n'}  dev.new(); phi = para.star[,1]  hist(phi, 15, prob=TRUE, main='', xlim=c(.4,1.2), xlab='')  xx = seq(.4, 1.2, by=.01)  lines(xx, dnorm(xx, mean=u[1,1], sd=u[2,1]), lty='dashed', lwd=2)

### 6.12 Bayesian Analysis of State Space Models

We now consider some Bayesian approaches to fitting linear Gaussian state space models via Markov chain Monte Carlo (MCMC) methods. We assume that the model is given by (6.1)-(6.2); inputs are allowed in the model, but we do not display them for the sake of brevity. In this case, Fruhwirth-Schnatter [64] and Carter and Kohn [39]

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & & Asymptotic & Bootstrap\(\dagger\) \\ Parameter & MLE & SE & SE \\ \hline \(\phi_{1}\) & 0.884 & 0.109 & 0.057 \\ \(\sigma_{w}\) & 0.381 & 0.221 & 0.324 \\ \(\alpha\) & \(-9.654\) & 0.343 & 1.529 \\ \(\sigma_{0}\) & 0.835 & 0.204 & 0.527 \\ \(\mu_{1}\) & \(-2.350\) & 0.495 & 0.410 \\ \(\sigma_{1}\) & 2.453 & 0.293 & 0.375 \\ \hline \end{tabular} \(\dagger\) Based on 500 bootstrapped samples

\end{table}
Table 6.5: Estimates and standard errors for GNP example established the MCMC procedure that we will discuss here. A comprehensive text that we highly recommend for this case is Petris et al. [153] and the corresponding R package dlm. For nonlinear and non-Gaussian models, the reader is referred to Douc, Moulines, and Stoffer [53]. As in previous sections, we have \(n\) observations denoted by \(y_{1:n}=\{y_{1},\ldots,y_{n}\}\), whereas the states are denoted as \(x_{0:n}=\{x_{0},x_{1},\ldots,x_{n}\}\), with \(x_{0}\) being the initial state.

MCMC methods refer to Monte Carlo integration methods that use a Markovian updating scheme to sample from intractable posterior distributions. The most common MCMC method is the Gibbs sampler, which is essentially a modification of the Metropolis algorithm (Metropolis et al. [141]) developed by Hastings [96] in the statistical setting and by Geman and Geman [68] in the context of image restoration. Later, Tanner and Wong [198] used the ideas in their substitution sampling approach, and Gelfand and Smith [67] developed the Gibbs sampler for a wide class of parametric models. The basic strategy is to use conditional distributions to set up a Markov chain to obtain samples from a joint distribution. The following simple case demonstrates this idea.

**Example 6.25**: **Gibbs Sampling for the Bivariate Normal**__

Suppose we wish to obtain samples from a bivariate normal distribution,

\[\begin{pmatrix}X\\ Y\end{pmatrix}\sim\mathrm{N}\left[\begin{pmatrix}0\\ 0\end{pmatrix},\ \begin{pmatrix}1&\rho\\ \rho&1\end{pmatrix}\right]\,,\]

where \(|\rho|<1\), but we can only generate samples from a univariate normal.

* The univariate conditionals are [see (B.9)-(B.10)] \[(X\mid Y=y)\sim\mathrm{N}(\rho y,1-\rho^{2})\quad\text{and}\quad(Y\mid X=x) \sim\mathrm{N}(\rho x,1-\rho^{2}),\] and we can simulate from these distributions.
* Construct a Markov chain: Pick \(X^{(0)}=x_{0}\), and then iterate the process \(X^{(0)}=x_{0}\mapsto Y^{(0)}\mapsto X^{(1)}\mapsto Y^{(1)}\mapsto\cdots\mapsto X ^{(k)}\mapsto Y^{(k)}\mapsto\cdots\), where \[(Y^{(k)}\mid X^{(k)}=x_{k})\sim\mathrm{N}(\rho x_{k},1-\rho^{2})\] \[(X^{(k)}\mid Y^{(k-1)}=y_{k-1})\sim\mathrm{N}(\rho y_{k-1},1-\rho ^{2}).\]
* The joint distribution of \((X^{(k)},Y^{(k)})\) is (see Problem 3.2) \[\begin{pmatrix}X^{(k)}\\ Y^{(k)}\end{pmatrix}\sim\mathrm{N}\left[\begin{pmatrix}\rho^{2k}x_{0}\\ \rho^{2k+1}x_{0}\end{pmatrix},\ \begin{pmatrix}1-\rho^{4k}&\rho(1-\rho^{4k})\\ \rho(1-\rho^{4k})&1-\rho^{4k+2}\end{pmatrix}\right]\,.\]
* Thus, for any starting value, \(x_{0}\), \((X^{(k)},Y^{(k)})\to_{d}(X,Y)\) as \(k\to\infty\); the speed depends on \(\rho\). Then one would run the chain and throw away the initial \(n_{0}\) sampled values (burnin) and retain the rest.

For state space models, the main objective is to obtain the posterior density of the parameters \(\mathrm{p}(\Theta\mid y_{1:n})\) or \(\mathrm{p}(x_{0:n}\mid y_{1:n})\) if the states are meaningful. For example,the states do not have any meaning for an ARMA model, but they are important for a stochastic volatility model. It is generally easier to get samples from the full posterior \(\mathrm{p}(\Theta,x_{0:n}\mid y_{1:n})\) and then marginalize ("average") to obtain \(\mathrm{p}(\Theta\mid y_{1:n})\) or \(\mathrm{p}(x_{0:n}\mid y_{1:n})\). As previously mentioned, the most popular method is to run a full Gibbs sampler, alternating between sampling model parameters and latent state sequences from their respective full conditional distributions.

**Procedure 6.1**: **Gibbs Sampler for State Space Models**

(i) Draw \(\Theta^{\prime}\sim\mathrm{p}(\Theta\mid x_{0:n},y_{1:n})\)

(ii) Draw \(x^{\prime}_{0:n}\sim\mathrm{p}(x_{0:n}\mid\Theta^{\prime},y_{1:n})\)

Procedure 6.1-(i) is generally much easier because it conditions on the complete data \(\{x_{0:n},y_{1:n}\}\), which we saw in Sect. 6.3 can simplify the problem. Procedure 6.1-(ii) amounts to sampling from the joint smoothing distribution of the latent state sequence and is generally difficult. For linear Gaussian models, however, both parts of Procedure 6.1 are relatively easy to perform.

To accomplish Procedure 6.1-(i), note that

\[\mathrm{p}(\Theta\mid x_{0:n},y_{1:n})\propto\pi(\Theta)\;\mathrm{p}(x_{0} \mid\Theta)\prod_{t=1}^{n}\;\mathrm{p}(x_{t}\mid x_{t-1},\Theta)\;\mathrm{p}(y_ {t}\mid x_{t},\Theta) \tag{6.216}\]

where \(\pi(\Theta)\) is the prior on the parameters. The prior often depends on "hyperparameters" that add another level to the hierarchy. For simplicity, these hyperparameters are assumed to be known. The parameters are typically conditionally independent with distributions from standard parametric families (at least as long as the prior distribution is conjugate relative to the Bayesian model specification). For non-conjugate models, one option is to replace Procedure 6.1-(i) with a Metropolis-Hastings step, which is feasible since the complete data density \(\mathrm{p}(\Theta,x_{0:n},y_{1:n})\) can be evaluated pointwise.

For example, in the univariate model

\[x_{t}=\phi x_{t-1}+w_{t}\quad\text{and}\quad y_{t}=x_{t}+v_{t}\]

where \(w_{t}\!\sim\!\mathrm{id}\,\mathrm{N}(0,\sigma_{w}^{2})\) independent of \(v_{t}\!\sim\!\mathrm{id}\,\mathrm{N}(0,\sigma_{v}^{2})\), we can use the normal and inverse gamma (IG) distributions for priors. In this case, the priors on the variance components are chosen from a conjugate family, that is, \(\sigma_{w}^{2}\sim\mathrm{IG}(a_{0}/2,b_{0}/2)\) independent of \(\sigma_{v}^{2}\sim\mathrm{IG}(c_{0}/2,d_{0}/2)\), where IG denotes the inverse (reciprocal) gamma distribution. Then, for example, if the prior on \(\phi\) is Gaussian, \(\phi\sim\mathrm{N}(\mu_{\phi},\sigma_{\phi}^{2})\), then \(\phi\mid\sigma_{w},x_{0:n},y_{1:n}\sim\mathrm{N}(Bb,B)\), where

\[B^{-1}=\frac{1}{\sigma_{\phi}^{2}}+\frac{1}{\sigma_{w}^{2}}\sum_{t=1}^{n}x_{t- 1}^{2},\qquad b=\frac{\mu_{\phi}}{\sigma_{\phi}^{2}}+\frac{1}{\sigma_{w}^{2}} \sum_{t=1}^{n}x_{t}x_{t-1}.\]

and

\[\sigma_{w}^{2}\mid\phi,x_{0:n},y_{1:n}\sim\mathrm{IG}\Big{(}\tfrac{1}{2}(a_{0 }+n),\;\tfrac{1}{2}\big{\{}b_{0}+\sum_{t=1}^{n}[x_{t}-\phi x_{t-1}]^{2}\big{\}} \Big{)}\,;\]

\[\sigma_{v}^{2}\mid x_{0:n},y_{1:n}\sim\mathrm{IG}\Big{(}\tfrac{1}{2}(c_{0}+n),\;\tfrac{1}{2}\big{\{}c_{0}+\sum_{t=1}^{n}[y_{t}-x_{t}]^{2}\big{\}}\Big{)}.\]

[MISSING_PAGE_FAIL:379]

along with the posterior means. Figure 6.21 compares the actual smoother \(x_{t}^{n}\) with the posterior mean of the sampled smoothed values. In addition, a pointwise 95% credible interval is displayed as a filled area.

The following code was used in this example.

```
#--Notation--#
#y(t)=x(t)+w(t);v(t)-iidN(0,V)
#x(t)=x(t-1)+w(t);w(t)-iidN(0,W)
#priors:x(0)-N(m0,C0);V-IG(a,b);W-IG(c,d)
#FFBS:x(t/t)-N(m,C);x(t/n)-N(mm,CC);x(t/t+1)-N(a,R)
#--ffbs=function(y,V,W,m0,C0){ n=length(y);a=rep(0,n);R=rep(0,n) m=rep(0,n);C=rep(0,n);B=rep(0,n-1) H=rep(0,n-1);mm=rep(0,n);CC=rep(0,n) x=rep(0,n);llike=0.0 for(tin1:n){ if(t==1){a[1]=m0;R[1]=C0+W }else{a[t]=m[t-1];R[t]=C[t-1]+W} f=a[t] Q=R[t]+V A=R[t]/Q m[t]=a[t]+A*(y[t]-f) C[t]=R[t]-Q*A**2 B[t-1]=C[t-1]/R[t]

Figure 6.20: Display for Example 6.26: _Left_: Generated states, \(x_{t}\) and data \(y_{t}\). Contours of the likelihood (_solid line_) of the data and sampled posterior values as points. _Right_: Marginal sampled posteriors and posterior means (_vertical lines_) of each variance component. The true values are \(\sigma_{w}^{2}=.5\) and \(\sigma_{v}^{2}=1\)

H[t-1] = C[t-1]-R[t]*B[t-1]**2  llike = llike + dnorm(y[t],f,sqrt(Q),log=TRUE) }  mm[n] = m[n]; CC[n] = C[n]  x[n] = rnorm(1,m[n],sqrt(C[n]))  for (t in (n-1):1){  mm[t] = m[t] + C[t]/R[t+1]*(mm[t+1]-a[t+1])  CC[t] = C[t] - (C[t]^2)/(R[t+1]^2)*(R[t+1]-CC[t+1])  x[t] = rnorm(1,m[t]+B[t]*(x[t+1]-a[t+1]),sqrt(H[t])) }  return(list(x-x,m-m,C=C,mm-mm,CC=CC,llike-llike)) }  # Simulate states and data  set.seed(1); W = 0.5; V = 1.0  n = 100; m0 = 0.0; C0 = 10.0; x0 = 0  w = rnorm(n,0,sqrt(W))  v = rnorm(n,0,sqrt(V))  x = y = rep(0,n)  x[1] = x0 + w[1]  y[1] = x[1] + v[1]  for (t in 2:n){  x[t] = x[t-1] + w[t]  y[t] = x[t] + v[t] }  # actual smoother (for plotting)  ks = Ksmooth@(num-n, y, A=1, m0, C0, Phi=1, cQ=sqrt(W), cR=sqrt(V))  xsmooth = as.vector(ks5xs)  #  run = fDs(y,V,W,m0,C0)  m = run5m; C = run5C; mm = run5mm  CC = run5CC; L1 = m-2*C; U1 = m+2*C  L2 = mm-2*CC; U2 = mm+2*CC  N = 50  Vs = seq(0.1,2,length=N)  Ws = seq(0.1,2,length=N)  likes = matrix(0,N,N)  for (i in 1:N){

Figure 21: Display for Example 6.26: True smoother, \(x_{t}^{n}\), the data \(y_{t}\), and the posterior mean of the sampled smoother values; the _filled in area_ shows 2.5% to 97.5%-tiles of the draws

for (jin 1:N){  V = Vs[i]  W = Ws[j]  run = ffbs(y,V,W,m0,C0)  likes[i,j] = run$llike } }
# Hyperparameters a = 0.01; b = 0.01; c = 0.01; d = 0.01
MCMC step set.seed(90210) burn = 10; M = 1000 niter = burn + M V1 = V; W1 = W draws = NULL all_draws = NULL for (iter in 1:niter){  run = fbs(y,V1,W1,m0,C0)  x = run$x  V1 = 1/gamma(1,a+n/2,b+sum((y-x)^2)/2)  W1 = 1/gamma(1,c+(n-1)/2,d+sum(diff(x)^2)/2)  draws = rbind(draws,c(V1,W1,x)) } all_draws = draws[,1:2]  q025 = function(x)[quantile(x,0.025)]  q975 = function(x)[quantile(x,0.975)]  draws = draws[(burn+1):(niter),]  xs = draws[,3:(n+2)] lx = apply(xs,2,q025)  mx = apply(xs,2,mean)  ux = apply(xs,2,q975)
plot of the data  par(mfrom=c(2,2), mgp=c(1.6,.6,0), mar=c(3,3.2,1,1))  ts.plot(ts(x), ts(y), ylab='', col=c(1,8), lwd=2)  points(y)  legend(0, 11, legend=c("x(t)","y(t)"), lty=1, col=c(1,8), lwd=2, bty="n",  pch=c(-1,1))  contour(Vs, Ws, exp(likes), xlab=expression(sigma[v]^2),  ylab=expression(sigma[w]^2),  points(draws[,1:2], pch=16, col=rgb(.9,0,0,0.3), cex=.7)  hist(draws[,1], ylab="Density",main=", xlab=expression(sigma[v]^2))  abline(v=mean(draws[,1]), col=3, lwd=3)  hist(draws[,2],main="", ylab="Density", xlab=expression(sigma[w]^2))  abline(v=mean(draws[,2]), col=3, lwd=3)
plot states  par(mmp=c(1.6,.6,0), mar=c(2,1.5,0)+.5)  plot(ts(mx), ylab='', type='n', ylim=c(min(y),max(y)))  grid(lty=2); points(y)  lines(xsmooth, lwd=4, col=rgb(1,0,1,alpha=.4))  lines(mx, col= 4)  xx=c(1:100, 100:1)  yy=c(lx, rev(ux))  polygon(xx, yy, border=NA, col= gray(.6,alpha=.2))  lines(y, col=gray(.4))  legend('topleft', c('true smoother', 'data', 'posterior mean', '95% of  draws'), lty=1, lwd=c(3,1,1,10), pch=c(-1,1,-1,-1), col=c(6,  gray(.4),4, gray(.6, alpha=.5)), bg='white' }

Next, we consider a more complicated model.

**Example 6.27**: **Structural Model**

Consider the Johnson & Johnson quarterly earnings per share series that was discussed in Example 6.10. Recall that the model is

\[y_{t} = \left( 1\ 1\ 0\ 0\right)x_{t}+v_{t},\] \[x_{t} = \begin{pmatrix}T_{t}\\ S_{t}\\ S_{t-1}\\ S_{t-2}\end{pmatrix} = \begin{pmatrix}\phi&0&0&0\\ 0&-1&-1&-1\\ 0&1&0&0\\ 0&0&1&0\end{pmatrix}\begin{pmatrix}T_{t-1}\\ S_{t-1}\\ S_{t-2}\end{pmatrix}+\begin{pmatrix}w_{t1}\\ w_{t2}\\ 0\\ 0\end{pmatrix}\]

where \(R=\sigma_{v}^{2}\) and

\[Q=\begin{pmatrix}\sigma_{w,11}^{2}&0&0&0\\ 0&\sigma_{w,22}^{2}&0&0\\ 0&0&0&0\\ 0&0&0&0\end{pmatrix}.\]

The parameters to be estimated are the transition parameter associated with the growth rate, \(\phi>1\), the observation noise variance, \(\sigma_{v}^{2}\), and the state noise variances associated with the trend and the seasonal components, \(\sigma_{w,11}^{2}\) and \(\sigma_{w,22}^{2}\), respectively.

In this case, sampling from \(\text{p}(x_{0:n}\mid\Theta,\,y_{1:n})\) follows directly from (6.217)-(6.218). Next, we discuss how to sample from \(\text{p}(\Theta\mid x_{0:n},\,y_{1:n})\). For the transition parameter, write \(\phi=1+\beta\), where \(0<\beta\ll 1\); recall that in Example 6.10, \(\phi\) was estimated to be 1.035, which indicated a growth rate, \(\beta\), of 3.5%. Note that the trend component may be rewritten as

\[\nabla T_{t}=T_{t}-T_{t-1}=\beta T_{t-1}+w_{t1}\,.\]

Consequently, conditional on the states, the parameter \(\beta\) is the slope in the linear regression (through the origin) of \(\nabla T_{t}\) on \(T_{t-1}\), for \(t=1,\ldots,n\), and \(w_{t1}\) is the error. As is typical, we put a Normal-Inverse Gamma (IG) prior on \((\beta,\sigma_{w,11}^{2})\), i.e., \(\beta\mid\sigma_{w,11}^{2}\sim\text{N}(b_{0},\,\sigma_{w,11}^{2}B_{0})\) and \(\sigma_{w,11}^{2}\sim\text{IG}(n_{0}/2,\,n_{0}s_{0}^{2}/2)\), with known hyperparameters \(b_{0},B_{0},n_{0},s_{0}^{2}\).

We also used IG priors for the other two variance components, \(\sigma_{v}^{2}\) and \(\sigma_{w,22}^{2}\). In this case, if the prior \(\sigma_{v}^{2}\sim\text{IG}(n_{0}/2,\,n_{0}s_{0}^{2}/2)\), then the posterior is

\[\sigma_{v}^{2}\mid x_{0:n},\,y_{1:n}\sim\text{IG}(n_{v}/2,\,n_{v}s_{v}^{2}/2)\,,\]

where \(n_{v}=n_{0}+n\), and \(n_{v}s_{v}^{2}=n_{0}s_{0}^{2}+\sum_{t=1}^{n}(Y_{t}-T_{t}-S_{t})^{2}\). Similarly, if the prior \(\sigma_{w,22}^{2}\sim\text{IG}(n_{0}/2,\,n_{0}s_{0}^{2}/2)\), then the posterior is

\[\sigma_{w,22}^{2}\mid x_{0:n},\,y_{1:n}\sim\text{IG}(n_{w}/2,\,n_{w}s_{w}^{2}/ 2)\,,\]

where \(n_{w}=n_{0}+(n-3)\), and \(n_{w}s_{w}^{2}=n_{0}s_{0}^{2}+\sum_{t=1}^{n-3}(S_{t}-S_{t-1}-S_{t-2}-S_{t-3})^ {2}\).

Figure 22 displays the results of the posterior estimates of the parameters. The top row of the figure displays the traces of 1000 draws, after a burn-in of 100, witha step size of 10 (i.e., every 10th sampled value is retained). The middle row of the figure displays the ACF of the traces, and the sampled posteriors are displayed in the last row of the figure. The results of this analysis are comparable to the results obtained in Example 6.10; the posterior mean and median for \(\phi\) indicates a 3.7% growth rate in the Johnson & Johnson quarterly earnings over this time period.

Figure 6.23 displays the smoothers of trend (\(T_{t}\)) and season (\(T_{t}+S_{t}\)) along with 99% credible intervals. Again, these results are comparable to the results obtained in Example 6.10. The R code for this example is as follows:

library(plyr) # used to view progress (install it if you don't have it) y = jj ### setup - model and initial parameters set.seed(90210) n = length(y) F = c(1,1,0,0) # this is A G = diag(0,4) # G is Phi G[1,1] = 1.03 G[2,] = c(0,-1,-1,-1); G[3,]=c(0,1,0,0); G[4,]=c(0,0,1,0) a1 = rbind(.7,0,0,0) # this is mu0 R1 = diag(.04,4) # this is Sigma0 V =.1 W11 =.1 W22 =.1
## #-- FFBS --## fbs = function(y,F,G,V,W11,W22,a1,R1){

Figure 6.22: Parameter estimation results for Example 6.27. The top row displays the traces of 1000 draws after burn-in. The middle row displays the ACF of the traces. The sampled posteriors are displayed in the last row (the mean is marked by a _solid vertical line_)

n = length(y)  Ws = diag(c(W11,W22,1,1)) # this is Q with ls as a device only  iW = diag(1/diag(Ws),4)  a = matrix(0,n,4) # this is m_t  R = array(0,c(n,4,4)) # this is V_t  m = matrix(0,n,4)  C = array(0,c(n,4,4))  a[1,] = a1[,1]  R[1,.] = R1  f = t(F)%*%a[1,]  Q = t(F)%*%R[1,.]%*%F + V  A = R[1,.]%*%F/Q[1,1]  m[1,] = a[1,]*A%*%(y[1]-f)  C[1,.] = R[1,.]-A%*%t(A)*Q[1,1]  for (t in 2:n){  a[t,] = GK*%m[t-1,]  R[t,.] = GK*%C[t-1,.]%*%t(G) + Ws  f = t(F)%*%a[t,]  Q = t(F)%*%R[t,]%*%F + V  A = R[t,.]%*%F/Q[1,1]  m[t,] = a[t,] + A%*%(y[t]-f)  C[t,.] = R[t,.] - A%*%t(A)*Q[1,1] }  xb = matrix(0,n,4)  xb[n,] = m[n,] + t(chol(C[n,.]))%*%norm(4)  for (t in (n-1):1){  iC = solve(C[t,.])  CCC = solve(t(G)%*%iW%*%G + iC)  mmm = CCC%*%(t(G)%*%iW%*%R[t+1,] + iC%*%m[t,])  xb[t,] = mmm + t(chol(CCC))%*%norm(4) }

Figure 6.23: Example 6.27 smoother estimates of trend (\(T_{t}\)) and trend plus season (\(T_{t}+S_{t}\)) along with corresponding 99% credible intervals

return(xb) }
##- Prior hyperparameters -##
# b0 = 0 # mean for beta = phi -1
B0 = Inf # var for beta (non-informative => use OLS for sampling beta) n0 = 10 # use same for all- the prior is 1/Gamma(n0/2, n0*s20_/2) s20v =.001 # for V s20w =.05 # for Ws
##- MCMC scheme -## set.seed(90210) burnin = 100 step = 10 M = 1000 niter = burnin+step*M pars = matrix(@,niter,4) xbs = array(@,c(niter,n,4)) pr <- progress_text() # displays progress pr$init(niter) for (iter in 1:niter){ xb = ffbs(y,F,G,V,W11,W22,a1,R1) u = xb[,1] yu = diff(u); xu = u[-n] # for phihat and se(phihat) regu = lm(yu-@+xu) # est of beta = phi-1 phies = as.vector(coef(summary(regu)))[1:2] + c(1,0) # phi estimate and SE dft = df.residual(regu) G[1,1] = phies[1] + rt(1,dft)*phies[2] # use a t V = 1/rgamma(1, (n0*n)/2, (n0*s20w/2) + sum((y-xb[,1]-xb[,2])^2)/2) W11 = 1/rgamma(1, (n0*n-1)/2, (n0*s20w/2) + sum((xb[-1,1]-phies[1]*xb[-n,1])^2)/2) W22 = 1/rgamma(1, (n0*n-3)/2, (n0*s20w/2) + sum((xb[4:n,2] + xb[3:(n-1),2]+ xb[2:(n-2),2] +xb[1:(n-3),2])^2)/2) xbs[iter,.] = xb pars[iter,] = c(G[1,1], sqrt(V), sqrt(W11), sqrt(W22)) pr$step() }
Plot results ind = seq(burnin+1,niter,by=step) names= c(expression(phi), expression(sigma[v]), expression(sigma[w-11]), expression(sigma[w-22])) dev.new(height=5) par(mfcol=c(3,4),mar=c(2,2,.25,0)+.75, mgp=c(1.6,.6,0), oma=c(0,0,1,0)) for (i in 1:4){ plot.ts(pars[ind,i],xlab="iterations", ylab="trace", main="") mtext(names[i], side=3, line=.5, cex=1) acf(pars[ind,i],main="",lag.max=25, xlim=c(1,25), ylim=c(-.4,.4)) hist(pars[ind,i],main="",xlab="") abline(v=mean(pars[ind,i]), lwd=2, col=3) } par(mfrow=c(2,1),mar=c(2,2,0,0)+.7, mgp=c(1.6,.6,0)) mxb = cbind(apply(xbs[ind,.1],2,mean), apply(xbs[,.2],2,mean)) lxb = cbind(apply(xbs[ind,.1],2,quantile,0.005), apply(xbs[ind,.2],2,quantile,0.005)) uxb = cbind(apply(xbs[ind,.1],2,quantile,0.995), apply(xbs[ind,.2],2,quantile,0.995)) mxb = ts(cbind(mxb,rowSums(mxb)),start = tsp(jj)[1],freq=4) lxb = ts(cbind(lxb,rowSums(lxb)),start = tsp(jj)[1],freq=4) uxb = ts(cbind(uxb,rowSums(uxb)),start = tsp(jj)[1],freq=4) names=c('Trend', 'Season', 'Trend + Season') L = min(lxb[,1])-.01;U = max(uxb[,1]) +.01plot(mxb[,1], ylab=names[1], ylim=c(L,U), type='n')  grid(lty=2); lines(mxb[,1])  xx=c(time(jj), rev(time(jj)))  yy=c(lxb[,1], rev(uxb[,1]))  polygon(xx, yy, border=NA, col=gray(.4, alpha =.2))  L = min(lxb[,3])-.01; U = max(uxb[,3]) +.01  plot(mxb[,3], ylab=names[3], ylim=c(L,U), type='n')  grid(lty=2); lines(mxb[,3])  xx=c(time(jj), rev(time(jj)))  yy=c(lxb[,3], rev(uxb[,3]))  polygon(xx, yy, border=NA, col=gray(.4, alpha =.2))

## Problems

### Section 6.1

Consider a system process given by

\[x_{t}=-.9x_{t-2}+w_{t}\quad t=1,\ldots,n\]

where \(x_{0}\sim\text{N}(0,\sigma_{0}^{2})\), \(x_{-1}\sim N(0,\sigma_{1}^{2})\), and \(w_{t}\) is Gaussian white noise with variance \(\sigma_{w}^{2}\). The system process is observed with noise, say,

\[y_{t}=x_{t}+v_{t},\]

where \(v_{t}\) is Gaussian white noise with variance \(\sigma_{v}^{2}\). Further, suppose \(x_{0}\), \(x_{-1}\), \(\{w_{t}\}\) and \(\{v_{t}\}\) are independent.

(a) Write the system and observation equations in the form of a state space model.

(b) Find the values of \(\sigma_{0}^{2}\) and \(\sigma_{1}^{2}\) that make the observations, \(y_{t}\), stationary.

(c) Generate \(n=100\) observations with \(\sigma_{w}=1\), \(\sigma_{v}=1\) and using the values of \(\sigma_{0}^{2}\) and \(\sigma_{1}^{2}\) found in (b). Do a time plot of \(x_{t}\) and of \(y_{t}\) and compare the two processes.

Also, compare the sample ACF and PACF of \(x_{t}\) and of \(y_{t}\).

(d) Repeat (c), but with \(\sigma_{v}=10\).

Consider the state-space model presented in Example 6.1. Let \(x_{t}^{t-1}=\text{E}(x_{t}\mid y_{t-1},\ldots,y_{1})\) and let \(P_{t}^{t-1}=\text{E}(x_{t}-x_{t}^{t-1})^{2}\). The innovation sequence or residuals are \(\epsilon_{t}=y_{t}-y_{t}^{t-1}\), where \(y_{t}^{t-1}=\text{E}(y_{t}\mid y_{t-1},\ldots,y_{1})\). Find \(\text{cov}(\epsilon_{s},\epsilon_{t})\) in terms of \(x_{t}^{t-1}\) and \(P_{t}^{t-1}\) for (i) \(s\neq t\) and (ii) \(s=t\).

### Section 6.2

Simulate \(n=100\) observations from the following state-space model:

\[x_{t}=.8x_{t-1}+w_{t}\quad\text{and}\quad y_{t}=x_{t}+v_{t}\]

where \(x_{0}\sim\text{N}(0,2.78)\), \(w_{t}\sim\text{iid N}(0,1)\), and \(v_{t}\sim\text{iid N}(0,1)\) are all mutually independent. Compute and plot the data, \(y_{t}\), the one-step-ahead predictors, \(y_{t}^{t-1}\) along with the root mean square prediction errors, \(\text{E}^{1/2}(y_{t}-y_{t}^{t-1})^{2}\) using Example 6.1 as a guide.

**6.4**: Suppose the vector \(z=(x^{\prime},y^{\prime})^{\prime}\), where \(x\) (\(p\times 1\)) and \(y\) (\(q\times 1\)) are jointly distributed with mean vectors \(\mu_{x}\) and \(\mu_{y}\) and with covariance matrix

\[\mathrm{cov}(z)=\begin{pmatrix}\Sigma_{xx}&\Sigma_{xy}\\ \Sigma_{yx}&\Sigma_{yy}\end{pmatrix}.\]

Consider projecting \(x\) on \(\mathcal{M}=\overline{\mathrm{sp}}\{1,y\}\), say, \(\widehat{x}=\boldsymbol{b}+By\).

1. Show the orthogonality conditions can be written as \[\mathrm{E}(x-\boldsymbol{b}-By)=0,\] \[\mathrm{E}[(x-\boldsymbol{b}-By)y^{\prime}]=0,\] leading to the solutions \[\boldsymbol{b}=\mu_{x}-B\mu_{y}\quad\text{and}\quad B=\Sigma_{xy}\Sigma_{yy}^ {-1}.\]
2. Prove the mean square error matrix is \[MSE=\mathrm{E}[(x-\boldsymbol{b}-By)x^{\prime}]=\Sigma_{xx}-\Sigma_{xy}\Sigma_ {yy}^{-1}\Sigma_{yx}.\]
3. How can these results be used to justify the claim that, in the absence of normality, Property 6.1 yields the best linear estimate of the state \(x_{t}\) given the data \(Y_{t}\), namely, \(x_{t}^{t}\), and its corresponding MSE, namely, \(P_{t}^{t}\)?

**6.5**: _Projection Theorem Derivation of Property 6.2._ Throughout this problem, we use the notation of Property 6.2 and of the Projection Theorem given in Appendix B, where \(\mathcal{H}\) is \(L^{2}\). If \(\mathcal{L}_{k+1}=\overline{\mathrm{sp}}\{y_{1},\ldots,y_{k+1}\}\), and \(\mathcal{V}_{k+1}=\overline{\mathrm{sp}}\{y_{k+1}-y_{k+1}^{k}\}\), for \(k=0,1,\ldots,n-1\), where \(y_{k+1}^{k}\) is the projection of \(y_{k+1}\) on \(\mathcal{L}_{k}\), then, \(\mathcal{L}_{k+1}=\mathcal{L}_{k}\oplus\mathcal{V}_{k+1}\). We assume \(P_{0}^{0}>0\) and \(R>0\).

(a) Show the projection of \(x_{k}\) on \(\mathcal{L}_{k+1}\), that is, \(x_{k}^{k+1}\), is given by

\[x_{k}^{k+1}=x_{k}^{k}+H_{k+1}(y_{k+1}-y_{k+1}^{k}),\]

where \(H_{k+1}\) can be determined by the orthogonality property

\[\mathrm{E}\left\{\left(x_{k}-H_{k+1}(y_{k+1}-y_{k+1}^{k})\right)\left(y_{k+1}- y_{k+1}^{k}\right)^{\prime}\right\}=0.\]

Show

\[H_{k+1}=P_{k}^{k}\Phi^{\prime}A_{k+1}^{\prime}\left[A_{k+1}P_{k+1}^{k}A_{k+1}^ {\prime}+R\right]^{-1}.\]

(b) Define \(J_{k}=P_{k}^{k}\Phi^{\prime}[P_{k+1}^{k}]^{-1}\), and show

\[x_{k}^{k+1}=x_{k}^{k}+J_{k}(x_{k+1}^{k+1}-x_{k+1}^{k}).\](c) Repeating the process, show

\[x_{k}^{k+2}=x_{k}^{k}+J_{k}(x_{k+1}^{k+1}-x_{k+1}^{k})+H_{k+2}(y_{k+2}-y_{k+2}^{k+ 1}),\]

solving for \(H_{k+2}\). Simplify and show

\[x_{k}^{k+2}=x_{k}^{k}+J_{k}(x_{k+1}^{k+2}-x_{k+1}^{k}).\]

(d) Using induction, conclude

\[x_{k}^{n}=x_{k}^{k}+J_{k}(x_{k+1}^{n}-x_{k+1}^{k}),\]

which yields the smoother with \(k=t-1\).

#### Section 6.3

**6.6** Consider the univariate state-space model given by state conditions \(x_{0}=w_{0}\), \(x_{t}=x_{t-1}+w_{t}\) and observations \(y_{t}=x_{t}+v_{t}\), \(t=1,2,\ldots\), where \(w_{t}\) and \(v_{t}\) are independent, Gaussian, white noise processes with \(\text{var}(w_{t})=\sigma_{w}^{2}\) and \(\text{var}(v_{t})=\sigma_{v}^{2}\).

(a) Show that \(y_{t}\) follows an IMA(1,1) model, that is, \(\nabla y_{t}\) follows an MA(1) model.

(b) Fit the model specified in part (a) to the logarithm of the glacial varve series and compare the results to those presented in Example 3.33.

**6.7** Consider the model

\[y_{t}=x_{t}+v_{t},\]

where \(v_{t}\) is Gaussian white noise with variance \(\sigma_{v}^{2}\), \(x_{t}\) are independent Gaussian random variables with mean zero and \(\text{var}(x_{t})=r_{t}\sigma_{x}^{2}\) with \(x_{t}\) independent of \(v_{t}\), and \(r_{1},\ldots,r_{n}\) are known constants. Show that applying the EM algorithm to the problem of estimating \(\sigma_{x}^{2}\) and \(\sigma_{v}^{2}\) leads to updates (represented by hats)

\[\hat{\sigma}_{x}^{2}=\frac{1}{n}\sum_{t=1}^{n}\frac{\sigma_{t}^{2}+\mu_{t}^{2} }{r_{t}}\quad\text{and}\quad\hat{\sigma}_{v}^{2}=\frac{1}{n}\sum_{t=1}^{n}[(y_ {t}-\mu_{t})^{2}+\sigma_{t}^{2}],\]

where, based on the current estimates (represented by tildes),

\[\mu_{t}=\frac{r_{t}\hat{\sigma}_{x}^{2}}{r_{t}\hat{\sigma}_{x}^{2}+\hat{\sigma }_{v}^{2}}\;y_{t}\quad\text{and}\quad\sigma_{t}^{2}=\frac{r_{t}\hat{\sigma}_{x }^{2}\hat{\sigma}_{v}^{2}}{r_{t}\hat{\sigma}_{x}^{2}+\hat{\sigma}_{v}^{2}}.\]

**6.8** To explore the stability of the filter, consider a univariate state-space model. That is, for \(t=1,2,\ldots\), the observations are \(y_{t}=x_{t}+v_{t}\) and the state equation is \(x_{t}=\phi x_{t-1}+w_{t}\), where \(\sigma_{w}=\sigma_{v}=1\) and \(|\phi|<1\). The initial state, \(x_{0}\), has zero mean and variance one.

(a) Exhibit the recursion for \(P_{t}^{t-1}\) in Property 6.1 in terms of \(P_{t-1}^{t-2}\).

(b) Use the result of (a) to verify \(P_{t}^{t-1}\) approaches a limit (\(t\to\infty\)) \(P\) that is the positive solution of \(P^{2}-\phi^{2}P-1=0\).

3. With \(K=\lim_{t\to\infty}K_{t}\) as given in Property 6.1, show \(|1-K|<1\).
4. Show, in steady-state, the one-step-ahead predictor, \(y_{n+1}^{n}=\mathrm{E}(y_{n+1}\bigm{|}y_{n},y_{n-1},\ldots)\), of a future observation satisfies \[y_{n+1}^{n}=\sum_{j=0}^{\infty}\phi^{j}K(1-K)^{j-1}y_{n+1-j}.\]

**6.9** In Sect. 6.3, we discussed that it is possible to obtain a recursion for the gradient vector, \(-\partial\ln L_{Y}(\Theta)/\partial\Theta\). Assume the model is given by (6.1) and (6.2) and \(A_{t}\) is a known design matrix that does not depend on \(\Theta\), in which case Property 6.1 applies. For the gradient vector, show

\[\partial\ln L_{Y}(\Theta)/\partial\Theta_{i}=\sum_{t=1}^{n}\left\{ \epsilon_{t}^{\prime}\Sigma_{t}^{-1}\,\frac{\partial\epsilon_{t}}{\partial \Theta_{i}}-\frac{1}{2}\epsilon_{t}^{\prime}\Sigma_{t}^{-1}\,\frac{\partial \Sigma_{t}}{\partial\Theta_{i}}\Sigma_{t}^{-1}\epsilon_{t}\right.\] \[+\left.\frac{1}{2}\mathrm{tr}\left(\Sigma_{t}^{-1}\frac{\partial \Sigma_{t}}{\partial\Theta_{i}}\right)\right\},\]

where the dependence of the innovation values on \(\Theta\) is understood. In addition, with the general definition \(\partial_{i}g=\partial g(\Theta)/\partial\Theta_{i}\), show the following recursions, for \(t=2,\ldots,n\) apply:

1. \(\partial_{i}\epsilon_{t}=-A_{t}\ \partial_{i}x_{t}^{t-1}\),
2. \(\partial_{i}x_{t}^{t-1}=\ \partial_{i}\Phi\ x_{t-1}^{t-2}+\Phi\ \partial_{i}x_{t-1}^{t-2}+\ \partial_{i}K_{t-1}\ \epsilon_{t-1}+K_{t-1}\ \partial_{i}\epsilon_{t-1}\),
3. \(\partial_{i}\Sigma_{t}=A_{t}\ \partial_{i}P_{t}^{t-1}A_{t}^{\prime}+\ \partial_{i}R\),
4. \(\partial_{i}K_{t}=\bigm{[}\ \partial_{i}\Phi\ P_{t}^{t-1}A_{t}^{\prime}+\Phi\ \partial_{i}P_{t}^{t-1}\ A_{t}^{\prime}-K_{t}\ \partial_{i}\Sigma_{t}\bigm{]}\Sigma_{t}^{-1}\),
5. \(\partial_{i}P_{t}^{t-1}=\ \partial_{i}\Phi\ P_{t}^{t-2}\Phi^{\prime}+\Phi\ \partial_{i}P_{t}^{t-2}\ \Phi^{\prime}+\Phi\ P_{t-1}^{t-2}\ \partial_{i}\Phi^{\prime}+\ \partial_{i}Q\), \(\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad- \partial_{i}K_{t-1}\ \Sigma_{t}\,K_{t-1}^{\prime}-K_{t-1}\ \partial_{i}\Sigma_{t}\ K_{t-1}^{\prime}-K_{t-1}\Sigma_{t}\ \partial_{i}K_{t-1}^{\prime}\),

using the fact that \(P_{t}^{t-1}=\Phi P_{t-1}^{t-2}\Phi^{\prime}+Q-K_{t-1}\Sigma_{t}K_{t-1}^{\prime}\).

**6.10** Continuing with the previous problem, consider the evaluation of the Hessian matrix and the numerical evaluation of the asymptotic variance-covariance matrix of the parameter estimates. The information matrix satisfies

\[\mathrm{E}\left\{-\frac{\partial^{2}\ln L_{Y}(\Theta)}{\partial\Theta\ \partial\Theta^{\prime}}\right\}=\mathrm{E}\left\{\left(\frac{\partial\ln L_{Y}(\Theta)}{\partial\Theta}\right)\left(\frac{\partial\ln L_{Y}(\Theta)}{\partial\Theta}\right)^{\prime}\right\};\]

see Anderson [7, Section 4.4], for example. Show the \((i,j)\)-th element of the information matrix, say, \(\mathcal{I}_{ij}(\Theta)=\mathrm{E}\left\{-\partial^{2}\ln L_{Y}(\Theta)/ \partial\Theta_{i}\ \partial\Theta_{j}\right\}\), is

\[\mathcal{I}_{ij}(\Theta)=\sum_{t=1}^{n}\mathrm{E}\Big{\{}\ \partial_{i}\epsilon_{t}^{\prime}\ \Sigma_{t}^{-1}\ \ \partial_{j}\epsilon_{t}+\frac{1}{2}\mathrm{tr}\big{(}\Sigma_{t}^{-1}\ \partial_{i}\Sigma_{t}\ \Sigma_{t}^{-1}\ \partial_{j}\Sigma_{t}\big{)}\] \[+\frac{1}{4}\mathrm{tr}\left(\Sigma_{t}^{-1}\ \partial_{i}\Sigma_{t}\right)\mathrm{tr}\big{(}\Sigma_{t}^{-1}\ \partial_{j}\Sigma_{t}\big{)}\Big{\}}.\]

Consequently, an approximate Hessian matrix can be obtained from the sample by dropping the expectation, \(\mathrm{E}\), in the above result and using only the recursions needed to calculate the gradient vector.

#### Section 6.4

**6.11** As an example of the way the state-space model handles the missing data problem, suppose the first-order autoregressive process

\[x_{t}=\phi x_{t-1}+w_{t}\]

has an observation missing at \(t=m\), leading to the observations \(y_{t}=A_{t}x_{t}\), where \(A_{t}=1\) for all \(t\), except \(t=m\) wherein \(A_{t}=0\). Assume \(x_{0}=0\) with variance \(\sigma_{w}^{2}/(1-\phi^{2})\), where the variance of \(w_{t}\) is \(\sigma_{w}^{2}\). Show the Kalman smoother estimators in this case are

\[x_{t}^{n}=\begin{cases}\phi y_{1}&t=0,\\ \frac{\phi}{1+\phi^{2}}(y_{m-1}+y_{m+1})&t=m,\\ y,&t\neq 0,m,\end{cases}\]

with mean square covariances determined by

\[P_{t}^{n}=\begin{cases}\sigma_{w}^{2}&t=0,\\ \sigma_{w}^{2}/(1+\phi^{2})&t=m,\\ 0&t\neq 0,m.\end{cases}\]

**6.12** The data set \(\mathtt{arlmiss}\) is \(n=100\) observations generated from an AR(1) process, \(x_{t}=\phi x_{t-1}+w_{t}\), with \(\phi=.9\) and \(\sigma_{w}=1\), where \(10\%\) of the data have been deleted at random (replaced with NA). Use the results of Problem 6.11 to estimate the parameters of the model, \(\phi\) and \(\sigma_{w}\), using the EM algorithm, and then estimate the missing values.

#### Section 6.5

**6.13** Redo Example 6.10 on the _logged_ Johnson & Johnson quarterly earnings per share.

**6.14** Fit a structural model to quarterly unemployment as follows. Use the data in unemp, which are monthly. The series can be made quarterly by aggregating and averaging: y = aggregate(unemp, nfrequency=4, FUN=mean), so that y is the quarterly average unemployment. Use Example 6.10 as a guide.

#### Section 6.6

**6.15** (a) Fit an AR(2) to the recruitment series, \(R_{t}\) in rec, and consider a lag-plot of the residuals from the fit versus the SOI series, \(S_{t}\) in soi, at various lags, \(S_{t-h}\), for \(h=0,1,\ldots\). Use the lag-plot to argue that \(S_{t-5}\) is reasonable to include as an exogenous variable.

(b) Fit an ARX(2) to \(R_{t}\) using \(S_{t-5}\) as an exogenous variable and comment on the results; include an examination of the innovations.

**6.16** Use Property 6.6 to complete the following exercises.

1. Write a univariate AR(1) model, \(y_{t}=\phi y_{t-1}+v_{t}\), in state-space form. Verify your answer is indeed an AR(1).
2. Repeat (a) for an MA(1) model, \(y_{t}=v_{t}+\theta v_{t-1}\).
3. Write an IMA(1,1) model, \(y_{t}=y_{t-1}+v_{t}+\theta v_{t-1}\), in state-space form.

**6.17** Verify Property 6.5.

**6.18** Verify Property 6.6.

#### Section 6.7

**6.19** Repeat the bootstrap analysis of Example 6.13 on the entire three-month Treasury bills and rate of inflation data set of 110 observations. Do the conclusions of Example 6.13--that the dynamics of the data are best described in terms of a fixed, rather than stochastic, regression--still hold?

#### Section 6.8

**6.20** Let \(y_{t}\) represent the global temperature series (globtemp) shown in Fig. 1.2.

1. Fit a smoothing spline using gcv (the default) to \(y_{t}\) and plot the result superimposed on the data. Repeat the fit using spar=.7; the gcv method yields spar=.5 approximately. (Example 2.14 on page 2.14). Also in R, see the help file?smooth.spline.)
2. Write the model \(y_{t}=x_{t}+v_{t}\) with \(\nabla^{2}x_{t}=w_{t}\), in state-space form. Fit this state-space model to \(y_{t}\), and exhibit a time plot the estimated smoother, \(\widehat{x}_{t}^{n}\) and the corresponding error limits, \(\widehat{x}_{t}^{n}\pm 2\sqrt{\widehat{\rho}_{t}^{n}}\) superimposed on the data.
3. Superimpose all the fits from parts (a) and (b) [include the error bounds] on the data and briefly compare and contrast the results.

#### Section 6.9

**6.21** Verify (6.132), (6.133), and (6.134).

**6.22** Prove Property 6.7 and verify (6.143).

**6.23** Fit a Poisson-HMM to the dataset polio from the gamlss.data package. The data are reported polio cases in the U.S. for the years 1970 to 1983. To get started, install the package and then type

library(gamlss.data) # load package

plot(polio, type='s') # view the data

**6.24** Fit a two-state HMM model to the weekly S&P 500 returns that were analyzed in Example 6.17 and compare the results.

**6.25** Fit the switching model described in Example 6.20 to the growth rate of GNP. The data are in gnp and, in the notation of the example, \(y_{t}\) is log-GNP and \(\nabla y_{t}\) is the growth rate. Use the code in Example 6.22 as a guide.

**6.26** Argue that a switching model is reasonable in explaining the behavior of the number of sunspots (see Fig. 4.22) and then fit a switching model to the sunspot data.

**6.27** Fit a stochastic volatility model to the returns of one (or more) of the four financial time series available in the R datasets package as EuStockMarkets.

**6.28** Fit a stochastic volatility model to the residuals of the GNP (gnp) returns analyzed in Example 3.39.

**6.29** We consider the stochastic volatility model (6.197).

(a) Show that for any integer \(m\),

\[\mathrm{E}[r_{t}^{2m}]=\beta^{2m}\mathrm{E}[r_{t}^{2m}]\exp(m^{2}\sigma_{x}^{2 }/2),\]

where \(\sigma_{x}^{2}=\sigma^{2}/(1-\phi^{2})\).

(b) Show (6.198).

(c) Show that for any positive integer \(h\), \(\mathrm{var}(X_{t}+X_{t+h})=2\sigma_{X}^{2}(1+\phi^{h})\).

(d) Show that

\[\mathrm{cov}(r_{t}^{2m},r_{t+h}^{2m})=\beta^{4m}\left(\mathrm{E}[v_{t}^{2m}] \right)^{2}\left(\exp(m^{2}\sigma_{x}^{2}(1+\phi^{h}))-\exp(m^{2}\sigma_{x}^{2 })\right)\.\]

(e) Establish (6.199).

**6.30** Verify the distributional statements made in Example 6.25.

**6.31** Repeat Example 6.27 on the log of the Johnson & Johnson data.

**6.32** Fit an AR(1) to the returns of the US GNP (gnp) using a Bayesian approach via MCMC.

## Chapter Statistical Methods in the Frequency Domain

In previous chapters, we saw many applied time series problems that involved relating series to each other or to evaluating the effects of treatments or design parameters that arise when time-varying phenomena are subjected to periodic stimuli. In many cases, the nature of the physical or biological phenomena under study are best described by their Fourier components rather than by the difference equations involved in ARIMA or state-space models. The fundamental tools we use in studying periodic phenomena are the discrete Fourier transforms (DFTs) of the processes and their statistical properties. Hence, in Sect. 7.2, we review the properties of the DFT of a multivariate time series and discuss various approximations to the likelihood function based on the large-sample properties and the properties of the complex multivariate normal distribution. This enables extension of the classical techniques such as ANOVA and principal component analysis to the multivariate time series case, which is the focus of this chapter.

### 7.1 Introduction

An extremely important class of problems in classical statistics develops when we are interested in relating a collection of input series to some output series. For example, in Chap. 2, we have previously considered relating temperature and various pollutant levels to daily mortality, but have not investigated the frequencies that appear to be driving the relation and have not looked at the possibility of leading or lagging effects. In Chap. 4, we isolated a definite lag structure that could be used to relate sea surface temperature to the number of new recruits. In Problem 5.10, the possible driving processes that could be used to explain inflow to Lake Shasta were hypothesized in terms of the possible inputs precipitation, cloud cover, temperature, and other variables. Identifying the combination of input factors that produce the bestprediction for inflow is an example of multiple regression in the frequency domain, with the models treated theoretically by considering the regression, conditional on the random input processes.

A situation somewhat different from that above would be one in which the input series are regarded as fixed and known. In this case, we have a model analogous to that occurring in _analysis of variance_, in which the analysis now can be performed on a frequency by frequency basis. This analysis works especially well when the inputs are dummy variables, depending on some configuration of treatment and other design effects and when effects are largely dependent on periodic stimuli. As an example, we will look at a designed experiment measuring the fMRI brain responses of a number of awake and mildly anesthetized subjects to several levels of periodic brushing, heat, and shock effects. Some limited data from this experiment have been discussed previously in Example 1.6. Figure 7.1 shows mean responses to various levels of periodic heat, brushing, and shock stimuli for subjects awake and subjects under mild anesthesia. The stimuli were periodic in nature, applied alternately for 32 s (16 points) and then stopped for 32 s. The periodic input signal comes through under all three design conditions when the subjects are awake, but is somewhat attenuated under anesthesia. The mean shock level response hardly shows on the input signal; shock levels were designed to simulate surgical incision without inflicting tissue damage. The means in Fig. 7.1 are from a single location. Actually, for each individual,

Figure 7.1: Mean response of subjects to various combinations of periodic stimulae measured at the cortex (primary somatosensory, contralateral). In the first column, the subjects are awake, in the second column the subjects are under mild anesthesia. In the first row, the stimulus is a brush on the hand, the second row involves the application of heat, and the third row involves a low level shock

some nine series were recorded at various locations in the brain. It is natural to consider testing the effects of brushing, heat, and shock under the two levels of consciousness, using a time series generalization of analysis of variance. The R code used to generate Fig. 7.1 is:

```
x=matrix(0,128,6) for(iin1:6){x[,i]=rowMeans(fmri[[i]])} colnames(x)=c("Brush","Heat","Shock","Brush","Heat","Shock") plot.ts(x,main="") mtext("Awake",side=3,line=1.2,adj=.05,cex=1.2) mtext("Sedated",side=3,line=1.2,adj=.85,cex=1.2)
```

A generalization to random coefficient regression is also considered, paralleling the univariate approach to signal extraction and detection presented in Sect. 4.9. This method enables a treatment of multivariate ridge-type regressions and _inversion problems_. Also, the usual random effects analysis of variance in the frequency domain becomes a special case of the random coefficient model.

The extension of frequency domain methodology to more classical approaches to multivariate discrimination and clustering is of interest in the frequency dependent case. Many time series differ in their means and in their autocovariance functions, making the use of both the mean function and the spectral density matrices relevant. As an example of such data, consider the bivariate series consisting of the P and S components derived from several earthquakes and explosions, such as those shown in Fig. 7.2, where the P and S components, representing different arrivals have been separated from the first and second halves, respectively, of waveforms like those shown originally in Fig. 1.7.

Two earthquakes and two explosions from a set of eight earthquakes and explosions are shown in Fig. 7.2 and some essential differences exist that might be used to characterize the two classes of events. Also, the frequency content of the two components of the earthquakes appears to be lower than those of the explosions, and relative amplitudes of the two classes appear to differ. For example, the ratio of the S to P amplitudes in the earthquake group is much higher for this restricted subset. Spectral differences were also noticed in Chap. 4, where the explosion processes had a stronger high-frequency component relative to the low-frequency contributions. Examples like these are typical of applications in which the essential differences between multivariate time series can be expressed by the behavior of either the frequency-dependent mean value functions or the spectral matrix. In _discriminant analysis_, these types of differences are exploited to develop combinations of linear and quadratic classification criteria. Such functions can then be used to classify events of unknown origin, such as the Novaya Zemlya event shown in Fig. 7.2, which tends to bear a visual resemblance to the explosion group. The R code used to produce Fig. 7.2 is:

``` attach(eexpr)#soyouancusethenamesofthereseries P=1:1024;S=P+1024 x=cbind(E05[P],E06[P],EX5[P],EX6[P],NZ[P],E05[S],EQ6[S],EX5[S], EX6[S],NZ[S]) x.name=c("E05","EQ6","EX5","EX6","NZ") colnames(x)=c(x.name,x.name) plot.ts(x,main="") mtext("Pwaves",side=3,line=1.2,adj=.05,cex=1.2) mtext("Swaves",side=3,line=1.2,adj=.85,cex=1.2)Finally, for multivariate processes, the structure of the spectral matrix is also of great interest. We might reduce the dimension of the underlying process to a smaller set of input processes that explain most of the variability in the cross-spectral matrix as a function of frequency. _Principal component analysis_ can be used to decompose the spectral matrix into a smaller subset of component factors that explain decreasing amounts of power. For example, the hydrological data might be explained in terms of a component process that weights heavily on precipitation and inflow and one that weights heavily on temperature and cloud cover. Perhaps these two components could explain most of the power in the spectral matrix at a given frequency. The ideas behind principal component analysis can also be generalized to include an optimal scaling methodology for categorical data called the _spectral envelope_ (see Stoffer et al. [193]).

### Spectral Matrices and Likelihood Functions

We have previously argued for an approximation to the log likelihood based on the joint distribution of the DFTs in (4.85), where we used approximation as an aid in estimating parameters for certain parameterized spectra. In this chapter, we make

Figure 2: Various bivariate earthquakes (EQ) and explosions (EX) recorded at 40 pts/s compared with an event NZ (Novaya Zemlya) of unknown origin. Compressional waves, also known as primary or P waves, travel fastest in the Earth’s crust and are first to arrive. Shear waves propagate more slowly through the Earth and arrive second, hence they are called secondary or S waves

heavy use of the fact that the sine and cosine transforms of the \(p\times 1\) vector process \(x_{t}=(x_{t1},x_{t2},\ldots,x_{tp})^{\prime}\) with mean \(\mathrm{E}x_{t}=\mu_{t}\), say, with DFT1

Footnote 1: In previous chapters, the DFT of a process \(x_{t}\) was denoted by \(d_{x}(\omega_{k})\). In this chapter, we will consider the Fourier transforms of many different processes and so, to avoid the overuse of subscripts and to ease the notation, we use a capital letter, e.g., \(X(\omega_{k})\), to denote the DFT of \(x_{t}\). This notation is standard in the digital signal processing (DSP) literature.

\[X(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}x_{t}\ \mathrm{e}^{-2\pi i\omega_{k}t}=X_{c} (\omega_{k})-iX_{s}(\omega_{k}) \tag{7.1}\]

and mean

\[M(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}\mu_{t}\ \mathrm{e}^{-2\pi i\omega_{k}t}=M_{c }(\omega_{k})-iM_{s}(\omega_{k}) \tag{7.2}\]

will be approximately uncorrelated, where we evaluate at the usual Fourier frequencies \(\{\omega_{k}=k/n,\ 0<|\omega_{k}|<1/2\}\). By Theorem C.6, the approximate \(2p\times 2p\) covariance matrix of the cosine and sine transforms, say, \(X(\omega_{k})=(X_{c}(\omega_{k})^{\prime},X_{s}(\omega_{k})^{\prime})^{\prime}\), is

\[\Sigma(\omega_{k})=\tfrac{1}{2}\begin{pmatrix}C(\omega_{k})&-Q(\omega_{k})\\ Q(\omega_{k})&C(\omega_{k})\end{pmatrix}, \tag{7.3}\]

and the real and imaginary parts are jointly normal. This result implies, by the results stated in Appendix C, the density function of the vector DFT, say, \(X(\omega_{k})\), can be approximated as

\[\mathrm{p}(\omega_{k})\approx|f(\omega_{k})|^{-1}\exp\bigl{\{}-\bigl{(}X( \omega_{k})-M(\omega_{k})\bigr{)}^{*}f^{-1}(\omega_{k})\bigl{(}X(\omega_{k})- M(\omega_{k})\bigr{)}\bigr{\}},\]

where the spectral matrix is the usual

\[f(\omega_{k})=C(\omega_{k})-iQ(\omega_{k}). \tag{7.4}\]

Certain computations that we do in the section on discriminant analysis will involve approximating the joint likelihood by the product of densities like the one given above over subsets of the frequency band \(0<\omega_{k}<1/2\).

To use the likelihood function for estimating the spectral matrix, for example, we appeal to the limiting result implied by Theorem C.7 and again choose \(L\) frequencies in the neighborhood of some target frequency \(\omega\), say, \(X(\omega_{k}\pm k/n)\), for \(k=1,\ldots,m\) and \(L=2m+1\). Then, let \(X_{\ell}\) denote the indexed values, and note the DFTs of the mean adjusted vector process are approximately jointly normal with mean zero and complex covariance matrix \(f=f(\omega)\). Then, write the log likelihood over the \(L\) sub-frequencies as

\[\ln L_{X}(f(\omega_{k}))\approx-L\ln|f(\omega_{k})|-\sum_{\ell=-m}^{m}(X_{\ell }-M_{\ell})^{*}f(\omega_{k})^{-1}(X_{\ell}-M_{\ell})\,. \tag{7.5}\]

The use of spectral approximations to the likelihood has been fairly standard, beginning with the work of Whittle [210] and continuing in Brillinger [35] and Hannan [86].

Assuming the mean adjusted series are available, i.e., \(M_{\ell}\) is known, we obtain the maximum likelihood estimator for \(f\), namely,

\[\hat{f}(\omega_{k})=L^{-1}\sum_{\ell=-m}^{m}(X_{\ell}-M_{\ell})(X_{\ell}-M_{\ell })^{*}; \tag{7.6}\]

see Problem 7.2.

### Regression for Jointly Stationary Series

In Sect. 4.7, we considered a model of the form

\[y_{t}=\sum_{r=-\infty}^{\infty}\beta_{1r}x_{t-r,1}+v_{t}, \tag{7.7}\]

where \(x_{t1}\) is a single observed input series and \(y_{t}\) is the observed output series, and we are interested in estimating the filter coefficients \(\beta_{1r}\) relating the adjacent lagged values of \(x_{t1}\) to the output series \(y_{t}\). In the case of the SOI and Recruitment series, we identified the El Nino driving series as \(x_{t1}\), the input and \(y_{t}\), the Recruitment series, as the output. In general, more than a single plausible input series may exist. For example, the Lake Shasta inflow hydrological data (climhyd) shown in Fig. 7.3 suggests there may be at least five possible series driving the inflow; see Example 7.1 for more details. Hence, we may envision a \(q\times 1\) input vector of driving series,

Figure 7.3: Monthly values of weather and inflow at Lake Shasta (climhyd)

say, \(x_{t}=(x_{t1},x_{t2},\ldots,x_{tq})^{\prime}\), and a set of \(q\times 1\) vector of regression functions \(\beta_{r}=(\beta_{1r},\beta_{2r},\ldots,\beta_{qr})^{\prime}\), which are related as

\[y_{t}=\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}x_{t-r}+v_{t}=\sum_{j=1}^{q} \sum_{r=-\infty}^{\infty}\beta_{jr}x_{t-r,j}+v_{t}, \tag{7.8}\]

which shows that the output is a sum of linearly filtered versions of the input processes and a stationary noise process \(v_{t}\), assumed to be uncorrelated with \(x_{t}\). Each filtered component in the sum over \(j\) gives the contribution of lagged values of the \(j\)th input series to the output series. We assume the regression functions \(\beta_{jr}\) are fixed and unknown.

The model given by (7.8) is useful under several different scenarios, corresponding to a number of different assumptions that can be made about the components. Assuming the input and output processes are jointly stationary with zero means leads to the conventional regression analysis given in this section. The analysis depends on theory that assumes we observe the output process \(y_{t}\) conditional on fixed values of the input vector \(x_{t}\); this is the same as the assumptions made in conventional regression analysis. Assumptions considered later involve letting the coefficient vector \(\beta_{t}\) be a random unknown signal vector that can be estimated by Bayesian arguments, using the conditional expectation given the data. The answers to this approach, given in Sect. 7.5, allow signal extraction and deconvolution problems to be handled. Assuming the inputs are fixed allows various experimental designs and analysis of variance to be done for both fixed and random effects models. Estimation of the frequency-dependent random effects variance components in the analysis of variance model is also considered in Sect. 7.5.

For the approach in this section, assume the inputs and outputs have zero means and are jointly stationary with the \((q+1)\times 1\) vector process \((x_{t}^{\prime},y_{t})^{\prime}\) of inputs \(x_{t}\) and outputs \(y_{t}\) assumed to have a spectral matrix of the form

\[f(\omega)=\begin{pmatrix}f_{xx}(\omega)&f_{xy}(\omega)\\ f_{yx}(\omega)&f_{yy}(\omega)\end{pmatrix}, \tag{7.9}\]

where \(f_{yx}(\omega)=(f_{yx_{1}}(\omega),f_{yx_{2}}(\omega),\ldots,f_{yx_{q}}(\omega))\) is the \(1\times q\) vector of cross-spectra relating the \(q\) inputs to the output and \(f_{xx}(\omega)\) is the \(q\times q\) spectral matrix of the inputs. Generally, we observe the inputs and search for the vector of regression functions \(\beta_{t}\) relating the inputs to the outputs. We assume all autocovariance functions satisfy the absolute summability conditions of the form

\[\sum_{h=-\infty}^{\infty}|h||\gamma_{jk}(h)|<\infty. \tag{7.10}\]

(\(j,k=1,\ldots,q+1\)), where \(\gamma_{jk}(h)\) is the autocovariance corresponding to the cross-spectrum \(f_{jk}(\omega)\) in (7.9). We also need to assume a linear process of the form (C.35) as a condition for using Theorem C.7 on the joint distribution of the discrete Fourier transforms in the neighborhood of some fixed frequency.

#### Estimation of the Regression Function

In order to estimate the regression function \(\beta_{r}\), the Projection Theorem (Appendix B) applied to minimizing

\[\text{MSE}=\text{E}\Big{[}(y_{t}-\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}x_{t- r})^{2}\Big{]} \tag{7.11}\]

leads to the orthogonality conditions

\[\text{E}\Big{[}(y_{t}-\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}x_{t-r})\,x_{t -s}^{\prime}\Big{]}=0^{\prime} \tag{7.12}\]

for all \(s=0,\pm 1,\pm 2,\ldots\), where \(0^{\prime}\) denotes the \(1\times q\) zero vector. Taking the expectations inside and substituting for the definitions of the autocovariance functions appearing and leads to the normal equations

\[\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}\,\,\Gamma_{xx}(s-r)=\gamma_{y,x}^{ \prime}(s), \tag{7.13}\]

for \(s=0,\pm 1,\pm 2,\ldots\), where \(\Gamma_{xx}(s)\) denotes the \(q\times q\) autocovariance matrix of the vector series \(x_{t}\) at lag \(s\) and \(\gamma_{y,x}(s)=(\gamma_{y,x_{1}}(s),\ldots,\gamma_{y,x_{q}}(s))\) is a \(1\times q\) vector containing the lagged covariances between \(y_{t}\) and \(x_{t}\). Again, a frequency domain approximate solution is easier in this case because the computations can be done frequency by frequency using cross-spectra that can be estimated from sample data using the DFT. In order to develop the frequency domain solution, substitute the representation into the normal equations, using the same approach as used in the simple case derived in Sect. 4.7. This approach yields

\[\int_{-1/2}^{1/2}\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}\,\,\text{e}^{2\pi i \omega(s-r)}\,\,f_{xx}(\omega)\,\,d\omega=\gamma_{y,x}^{\prime}(s).\]

Now, because \(\gamma_{yx}^{\prime}(s)\) is the Fourier transform of the cross-spectral vector \(f_{yx}(\omega)=f_{xy}^{*}(\omega)\), we might write the system of equations in the frequency domain, using the uniqueness of the Fourier transform, as

\[B^{\prime}(\omega)f_{xx}(\omega)=f_{xy}^{*}(\omega), \tag{7.14}\]

where \(f_{xx}(\omega)\) is the \(q\times q\) spectral matrix of the inputs and \(B(\omega)\) is the \(q\times 1\) vector Fourier transform of \(\beta_{t}\). Multiplying (7.14) on the right by \(f_{xx}^{-1}(\omega)\), assuming \(f_{xx}(\omega)\) is nonsingular at \(\omega\), leads to the _frequency domain estimator_

\[B^{\prime}(\omega)=f_{xy}^{*}(\omega)f_{xx}^{-1}(\omega). \tag{7.15}\]

Note, (7.15) implies the regression function would take the form

\[\beta_{t}=\int_{-1/2}^{1/2}B(\omega)\,\,\text{e}^{2\pi i\omega t}\,\,d\omega. \tag{7.16}\]As before, it is conventional to introduce the DFT as the approximate estimator for the integral (7.16) and write

\[\beta_{t}^{M}=M^{-1}\sum_{k=0}^{M-1}B(\omega_{k})\,\e^{2\pi i\omega_{k}t}, \tag{7.17}\]

where \(\omega_{k}=k/M,M<\!<n\). The approximation was shown in Problem 4.35 to hold exactly as long as \(\beta_{t}=0\) for \(|t|\geq M/2\) and to have a mean-squared-error bounded by a function of the zero-lag autocovariance and the absolute sum of the neglected coefficients.

The mean-squared-error (7.11) can be written using the orthogonality principle, giving

\[\MSE=\int_{-1/2}^{1/2}f_{y\cdot x}(\omega)\;d\omega, \tag{7.18}\]

where

\[f_{y\cdot x}(\omega)=f_{yy}(\omega)-f_{xy}^{*}(\omega)f_{xx}^{-1}(\omega)f_{xy} (\omega) \tag{7.19}\]

denotes the residual or error spectrum. The resemblance of (7.19) to the usual equations in regression analysis is striking. It is useful to pursue the multiple regression analogy further by noting a _squared multiple coherence_ can be defined as

\[\rho_{y\cdot x}^{2}(\omega)=\frac{f_{xy}^{*}(\omega)f_{xx}^{-1}(\omega)f_{xy}( \omega)}{f_{yy}(\omega)}. \tag{7.20}\]

This expression leads to the mean squared error in the form

\[\MSE=\int_{-1/2}^{1/2}f_{yy}(\omega)[1-\rho_{y\cdot x}^{2}(\omega)]\;d\omega, \tag{7.21}\]

and we have an interpretation of \(\rho_{y\cdot x}^{2}(\omega)\) as the _proportion of power_ accounted for by the lagged regression on \(x_{t}\) at frequency \(\omega\). If \(\rho_{y\cdot x}^{2}(\omega)=0\) for all \(\omega\), we have

\[\MSE=\int_{-1/2}^{1/2}f_{yy}(\omega)\;d\omega=\E[y_{t}^{2}],\]

which is the mean squared error when no predictive power exists. As long as \(f_{xx}(\omega)\) is positive definite at all frequencies, \(\MSE\geq 0\), and we will have

\[0\leq\rho_{y\cdot x}^{2}(\omega)\leq 1 \tag{7.22}\]

for all \(\omega\). If the multiple coherence is unity for all frequencies, the mean squared error in (7.21) is zero and the output series is perfectly predicted by a linearly filtered combination of the inputs. Problem 7.3 shows the ordinary squared coherence between the series \(y_{t}\) and the linearly filtered combinations of the inputs appearing in (7.11) is exactly (7.20).

### Estimation Using Sampled Data

Clearly, the matrices of spectra and cross-spectra will not ordinarily be known, so the regression computations need to be based on sampled data. We assume, therefore, the inputs \(x_{t1},x_{t2},\ldots,x_{tq}\) and output \(y_{t}\) series are available at the time points \(t=1,2,\ldots,n\), as in Chap. 4. In order to develop reasonable estimates for the spectral quantities, some replication must be assumed. Often, only one replication of each of the inputs and the output will exist, so it is necessary to assume a band exists over which the spectra and cross-spectra are approximately equal to \(f_{xx}(\omega)\) and \(f_{xy}(\omega)\), respectively. Then, let \(Y(\omega_{k}+\ell/n)\) and \(X(\omega_{k}+\ell/n)\) be the DFTs of \(y_{t}\) and \(x_{t}\) over the band, say, at frequencies of the form

\[\omega_{k}\pm\ell/n,\quad\ell=1,\ldots,m,\]

where \(L=2m+1\) as before. Then, simply substitute the sample spectral matrix

\[\hat{f}_{xx}(\omega)=L^{-1}\sum_{\ell=-m}^{m}X(\omega_{k}+\ell/n)X^{*}(\omega_ {k}+\ell/n) \tag{7.23}\]

and the vector of sample cross-spectra

\[\hat{f}_{xy}(\omega)=L^{-1}\sum_{\ell=-m}^{m}X(\omega_{k}+\ell/n)\overline{Y (\omega_{k}+\ell/n)} \tag{7.24}\]

for the respective terms in (7.15) to get the regression estimator \(\hat{B}(\omega)\). For the regression estimator (7.17), we may use

\[\hat{\beta}_{t}^{M}=\frac{1}{M}\sum_{k=0}^{M-1}\hat{f}_{xy}^{*}(\omega_{k}) \hat{f}_{xx}^{-1}(\omega_{k})\,\mathrm{e}^{2\pi i\omega_{k}t} \tag{7.25}\]

for \(t=0,\pm 1,\pm 2,\ldots,\pm(M/2-1)\), as the estimated regression function.

### Tests of Hypotheses

The estimated squared multiple coherence, corresponding to the theoretical coherence (7.20), becomes

\[\hat{\rho}_{y\cdot x}^{2}(\omega)=\frac{\hat{f}_{xy}^{*}(\omega)\hat{f}_{xx}^{ -1}(\omega)\hat{f}_{xy}(\omega)}{\hat{f}_{yy}(\omega)}. \tag{7.26}\]

We may obtain a distributional result for the multiple coherence function analogous to that obtained in the univariate case by writing the multiple regression model in the frequency domain, as was done in Sect. 4.5. We obtain the statistic

\[F_{2q,2(L-q)}=\frac{(L-q)}{q}\frac{\hat{\rho}_{y\cdot x}^{2}(\omega)}{[1-\hat {\rho}_{y\cdot x}^{2}(\omega)]}, \tag{7.27}\]which has an \(F\)-distribution with \(2q\) and \(2(L-q)\) degrees of freedom under the null hypothesis that \(\rho^{2}_{y\cdot x}(\omega)=0\), or equivalently, that \(B(\omega)=0\), in the model

\[Y(\omega_{k}+\ell/n)=B^{\prime}(\omega)X(\omega_{k}+\ell/n)+V(\omega_{k}+\ell/n), \tag{7.28}\]

where the spectral density of the error \(V(\omega_{k}+\ell/n)\) is \(f_{y\cdot x}(\omega)\). Problem 7.4 sketches a derivation of this result.

A second kind of hypothesis of interest is one that might be used to test whether a full model with \(q\) inputs is significantly better than some submodel with \(q_{1}<q\) components. In the time domain, this hypothesis implies, for a partition of the vector of inputs into \(q_{1}\) and \(q_{2}\) components (\(q_{1}+q_{2}=q\)), say, \(x_{t}=(x^{\prime}_{t1},x^{\prime}_{t2})^{\prime}\), and the similarly partitioned vector of regression functions \(\beta_{t}=(\beta^{\prime}_{1t},\beta^{\prime}_{2t})^{\prime}\), we would be interested in testing whether \(\beta_{2t}=0\) in the partitioned regression model

\[y_{t}=\sum_{r=-\infty}^{\infty}\beta^{\prime}_{1r}x_{t-r,1}+\sum_{r=-\infty}^{ \infty}\beta^{\prime}_{2r}x_{t-r,2}+v_{t}. \tag{7.29}\]

Rewriting the regression model (7.29) in the frequency domain in a form that is similar to (7.28) establishes that, under the partitions of the spectral matrix into its \(q_{i}\times q_{j}\) (\(i,j=1,2\)) submatrices, say,

\[\hat{f}_{xx}(\omega)=\begin{pmatrix}\hat{f}_{11}(\omega)&\hat{f}_{12}(\omega) \\ \hat{f}_{21}(\omega)&\hat{f}_{22}(\omega)\end{pmatrix}, \tag{7.30}\]

and the cross-spectral vector into its \(q_{i}\times 1\) (\(i=1,2\)) subvectors,

\[\hat{f}_{xy}(\omega)=\begin{pmatrix}\hat{f}_{1y}(\omega)\\ \hat{f}_{2y}(\omega)\end{pmatrix}, \tag{7.31}\]

we may test the hypothesis \(\beta_{2t}=0\) at frequency \(\omega\) by comparing the estimated residual power

\[\hat{f}_{y\cdot x}(\omega)=\hat{f}_{yy}(\omega)-\hat{f}_{xy}^{*}(\omega)\hat{ f}_{xx}^{-1}(\omega)\hat{f}_{xy}(\omega) \tag{7.32}\]

under the full model with that under the reduced model, given by

\[\hat{f}_{y\cdot 1}(\omega)=\hat{f}_{yy}(\omega)-\hat{f}_{1y}^{*}(\omega)\hat{ f}_{11}^{-1}(\omega)\hat{f}_{1y}(\omega). \tag{7.33}\]

The power due to regression can be written as

\[\text{SSR}(\omega)=L[\hat{f}_{y\cdot 1}(\omega)-\hat{f}_{y\cdot x}(\omega)], \tag{7.34}\]

with the usual error power given by

\[\text{SSE}(\omega)=L\hat{f}_{y\cdot x}(\omega). \tag{7.35}\]

The test of no regression proceeds using the \(F\)-statistic

\[F_{2q_{2},2(L-q)}=\frac{(L-q)}{q_{2}}\frac{\text{SSR}(\omega)}{\text{SSE}( \omega)}. \tag{7.36}\]

[MISSING_PAGE_FAIL:405]

(square root) precipitation produces the most consistently high squared coherence values at all frequencies (\(L=25\)), with the seasonal period contributing most significantly. Other inputs, with the exception of wind speed, also appear to be plausible contributors. Figure 7.4a-e shows a.001 threshold corresponding to the \(F\)-statistic, separately, for each possible predictor of inflow.

Next, we focus on the analysis with two predictor series, temperature and transformed precipitation. The additional contribution of temperature to the model seems somewhat marginal because the multiple coherence (7.26), shown in the top panel of Fig. 7.4f seems only slightly better than the univariate coherence with precipitation shown in Fig. 7.4e. It is, however, instructive to produce the multiple regression functions, using (7.25) to see if a simple model for inflow exists that would involve some regression combination of inputs temperature and precipitation that would be useful for predicting inflow to Shasta Lake. The top of Fig. 7.5 shows the partial \(F\)-statistic, (7.36), for testing if temperature is predictive of inflow when precipitation is in the model. In addition, threshold values corresponding to a false discovery rate (FDR) of.001 (see Benjamini and Hochberg [17]) and the corresponding null \(F\) quantile are displayed in that figure.

Although the contribution of temperature is marginal, it is instructive to produce the multiple regression functions, using (7.25), to see if a simple model for inflow

Figure 7.5: Partial \(F\)-statistics [_top_] for testing whether temperature adds to the ability to predict Lake Shasta inflow when precipitation is included in the model. The _dashed line_ indicates the.001 FDR level and the _solid line_ represents the corresponding quantile of the null \(F\) distribution. Multiple impulse response functions for the regression relations of temperature [_middle_] and precipitation [_bottom_]

exists that would involve some regression combination of inputs temperature and precipitation that would be useful for predicting inflow to Lake Shasta. With this in mind, denoting the possible inputs \(P_{t}\) for transformed precipitation and \(T_{t}\) for transformed temperature, the regression function has been plotted in the lower two panels of Fig. 7.5 using a value of \(M=100\) for each of the two inputs. In that figure, the time index runs over both positive and negative values and are centered at time \(t=0\). Hence, the relation with temperature seems to be instantaneous and positive and an exponentially decaying relation to precipitation exists that has been noticed previously in the analysis in Problem 4.37. The plots suggest a transfer function model of the general form fitted to the Recruitment and SOI series in Example 5.8. We might propose fitting the inflow output, say, \(I_{t}\), using the model

\[I_{t}=\alpha_{0}+\frac{\delta_{0}}{(1-\omega_{1}B)}P_{t}+\alpha_{2}T_{t}+\eta_{ t},\]

which is the transfer function model, without the temperature component, considered in that section. The R code for this example is as follows.

plot.ts(climhyd) # Figure 7.3 Y = climbyd # Y holds the transformed series Y[,6] = log(Y[,6]) # log inflow Y[,5] = sqrt(Y[,5]) # sqrt precipitation L = 25; M = 100; alpha =.001; fdr =.001 nq = 2 # number of inputs (Temp and Precip) # Spectral Matrix Yspec = mwspec(Y, spans=L, kernel="daniell", detrend=TRUE, demean=FALSE, taper=.1) n = YspecEn.used # effective sample size Fr = Yspecfreg # fundamental freqs n.freq = length(Fr) # number of frequencies YspecSbandwidth*sqrt(12) # = 0.050 - the bandwidth # Coherencies Fq = qf(1-alpha, 2, L-2) cn = Fq/(L-1+Fq) plt.name = c("(a)","(b)","(c)","(d)","(e)","(f)") dev.new(); par(mfrow=c(2,3), cex.lab=1.2) # The coherencies are listed as 1,2,...,15=choose(6,2) for (i in 11:15){ plot(Fr, YspecSch[,i], type="1", ylab="Sq Coherence", xlab="Frequency", ylim=c(@,1), main=c("Info with", names(climhyd[i-10]))) abline(h = cn); text(.45,.98, plt.name[i-10], cex=1.2) # Multiple Coherency coh.15 = stoch.reg(Y, cols.full = c(1,5), cols.red = NULL, alpha, L, M, plot.which = "coh") text(.45,.98, plt.name[6], cex=1.2) title(main = c("Info with", "Temp and Precip")) # Partial F (called ef; avoid use of f alone) numer.df = 2*nq; denom.df = YspecSdf-2*nq dev.new() par(mfrow=c(3,1), mar=c(3,3,2,1)+.5, mpp = c(1.5,@.4,@), cex.lab=1.2) out.15 = stoch.reg(Y, cols.full = c(1,5), cols.red = 5, alpha, L, M, plot.which = "F.stat") eF = out.155eF pvals = pf(eF, numer.df, denom.df, lower.tail = FALSE)pID = FDR(pvals, fdr); abline(h=c(eF[pID]), lty=2)  title(main = "Partial F Statistic")  # Regression Coefficients  S = seq(from = -M/2+1, to = M/2 - 1, length = M-1)  plot(S, coh.15$Betahat[,1], type = "h", xlab = "", ylab = names(climhyd[1]),  ylim = c(-.025,.055), lwd=2)  abline(h=0); title(main = "Impulse Response Functions")  plot(S, coh.15$Betahat[,2], type = "h", xlab = "Index", ylab = names(climhyd[5]), ylim = c(-.015,.055), lwd=2)  abline(h=0)

### Regression with Deterministic Inputs

The previous section considered the case in which the input and output series were jointly stationary, but there are many circumstances in which we might want to assume that the input functions are fixed and have a known functional form. This happens in the analysis of data from designed experiments. For example, we may want to take a collection of earthquakes and explosions such as are shown in Fig. 7.2 and test whether the mean functions are the same for either the P or S components or, perhaps, for them jointly. In certain other signal detection problems using arrays, the inputs are used as dummy variables to express lags corresponding to the arrival times of the signal at various elements, under a model corresponding to that of a plane wave from a fixed source propagating across the array. In Fig. 7.1, we plotted the mean responses of the cortex as a function of various underlying design configurations corresponding to various stimuli applied to awake and mildly anesthetized subjects.

It is necessary to introduce a replicated version of the underlying model to handle even the univariate situation, and we replace (7.8) by

\[y_{jt}=\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}z_{j,t-r}+v_{jt} \tag{7.37}\]

for \(j=1,2,\ldots,N\) series, where we assume the vector of known deterministic inputs, \(z_{jt}=(z_{jt1},\ldots,z_{jtq})^{\prime}\), satisfies

\[\sum_{t=-\infty}^{\infty}|t||z_{jtk}|<\infty\]

for \(j=1,\ldots,N\) replicates of an underlying process involving \(k=1,\ldots,q\) regression functions. The model can also be treated under the assumption that the deterministic function satisfy Grenander's conditions, as in Hannan [86], but we do not need those conditions here and simply follow the approach in Shumway [182, 183].

It will sometimes be convenient in what follows to represent the model in matrix notation, writing (7.37) as

\[y_{t}=\sum_{r=-\infty}^{\infty}z_{t-r}\ \beta_{r}+v_{t}, \tag{7.38}\]where \(z_{t}=(z_{1t},\ldots,z_{Nt})^{\prime}\) are the \(N\times q\) matrices of independent inputs and \(y_{t}\) and \(v_{t}\) are the \(N\times 1\) output and error vectors. The error vector \(v_{t}=(v_{1t},\ldots,v_{Nt})^{\prime}\) is assumed to be a multivariate, zero-mean, stationary, normal process with spectral matrix \(f_{v}(\omega)I_{N}\) that is proportional to the \(N\times N\) identity matrix. That is, we assume the error series \(v_{jt}\) are independently and identically distributed with spectral densities \(f_{v}(\omega)\).

An Infrasonic Signal from a Nuclear Explosion

Often, we will observe a common signal, say, \(\beta_{t}\) on an array of sensors, with the response at the \(j\)th sensor denoted by \(y_{jt},j=1,\ldots,N\). For example, Fig. 7.6 shows an infrasonic or low-frequency acoustic signal from a nuclear explosion, as observed on a small triangular array of \(N=3\) acoustic sensors. These signals appear at slightly different times. Because of the way signals propagate, a plane wave signal of this kind, from a given source, traveling at a given velocity, will arrive at elements in the array at predictable time delays. In the case of the infrasonic signal in Fig. 7.6, the delays were approximated by computing the cross-correlation between elements and simply reading off the time delay corresponding to the maximum. For a detailed discussion of the statistical analysis of array signals, see Shumway et al. [186].

A simple additive signal-plus-noise model of the form

\[y_{jt}=\beta_{t-\tau_{j}}+v_{jt} \tag{7.39}\]

can be assumed, where \(\tau_{j},j=1,2,\ldots,N\) are the time delays that determine the start point of the signal at each element of the array. The model (7.39) is written in the form (7.37) by letting \(z_{jt}=\delta_{t-\tau_{j}}\), where \(\delta_{t}=1\) when \(t=0\) and is zero otherwise. In this case, we are interested in both the problem of detecting the presence of the

Figure 7.6: Three series for a nuclear explosion detonated 25 km south of Christmas Island and the delayed average or beam. The time scale is 10 points per second

signal and in estimating its waveform \(\beta_{t}\). In this case, a plausible estimator of the waveform would be the unbiased \(beam\), say,

\[\hat{\beta}_{t}=\frac{\sum_{j=1}^{N}y_{j,t+\tau_{j}}}{N}, \tag{7.40}\]

where time delays in this case were measured as \(\tau_{1}=17,\tau_{2}=0\), and \(\tau_{3}=-22\) from the cross-correlation function. The bottom panel of Fig. 7.6 shows the computed beam in this case, and the noise in the individual channels has been reduced and the essential characteristics of the common signal are retained in the average. The R code for this example is

 attach(beamd) tau = rep(0,3) u = ccf(sensor1, sensor2, plot=FALSE) tau[1] = uSlag[which.max(uSacf)] # 17 u = ccf(sensor3, sensor2, plot=FALSE) tau[3] = uSlag[which.max(uSacf)] # -22 Y = ts.union(lag(sensor1,tau[1]), lag(sensor2, tau[2]), lag(sensor3, tau[3])) Y = ts.union(Y, rowMeans(Y)) colnames(Y) = c('sensor1','sensor2','sensor3', 'beam') plot.ts(Y)

The above discussion and example serve to motivate a more detailed look at the estimation and detection problems in the case in which the input series \(z_{jt}\) are fixed and known. We consider the modifications needed for this case in the following sections.

### Estimation of the Regression Relation

Because the regression model (7.37) involves fixed functions, we may parallel the usual approach using the Gauss-Markov theorem to search for linear-filtered estimators of the form

\[\hat{\beta}_{t}=\sum_{j=1}^{N}\sum_{r=-\infty}^{\infty}h_{jr}y_{j,t-r}, \tag{7.41}\]

where \(h_{jt}=(h_{jt1}\ldots,h_{jtq})^{\prime}\) is a vector of filter coefficients, determined so the estimators are unbiased and have minimum variance. The equivalent matrix form is

\[\hat{\beta}_{t}=\sum_{r=-\infty}^{\infty}h_{r}\ y_{t-r}, \tag{7.42}\]

where \(h_{t}=(h_{1t},\ldots,h_{Nt})\) is a \(q\times N\) matrix of filter functions. The matrix form resembles the usual classical regression case and is more convenient for extending the Gauss-Markov Theorem to lagged regression. The unbiased condition is considered in Problem 7.6. It can be shown (see Shumway and Dean [178]) that \(h_{js}\) can be taken as the Fourier transform of

\[H_{j}(\omega)=S_{j}^{-1}(\omega)\overline{Z_{j}(\omega)}, \tag{7.43}\]where

\[Z_{j}(\omega)=\sum_{t=-\infty}^{\infty}z_{jt}\mathrm{e}^{-2\pi i\omega t} \tag{7.44}\]

is the infinite Fourier transform of \(z_{jt}\). The matrix

\[S_{z}(\omega)=\sum_{j=1}^{N}\overline{Z_{j}(\omega)}Z_{j}^{\prime}(\omega) \tag{7.45}\]

can be written in the form

\[S_{z}(\omega)=Z^{*}(\omega)Z(\omega), \tag{7.46}\]

where the \(N\times q\) matrix \(Z(\omega)\) is defined by \(Z(\omega)=(Z_{1}(\omega),\ldots,Z_{N}(\omega))^{\prime}\). In matrix notation, the Fourier transform of the optimal filter becomes

\[H(\omega)=S_{z}^{-1}(\omega)Z^{*}(\omega), \tag{7.47}\]

where \(H(\omega)=(H_{1}(\omega),\ldots,H_{N}(\omega))\) is the \(q\times N\) matrix of frequency response functions. The optimal filter then becomes the Fourier transform

\[h_{t}=\int_{-1/2}^{1/2}H(\omega)\mathrm{e}^{2\pi i\omega t}\ d\omega. \tag{7.48}\]

If the transform is not tractable to compute, an approximation analogous to (7.25) may be used.

**Example 7.3**: **Estimation of the Infrasonic Signal in Example 7.2**

We consider the problem of producing a best linearly filtered unbiased estimator for the infrasonic signal in Example 7.2. In this case, \(q=1\) and (7.44) becomes

\[Z_{j}(\omega)=\sum_{t=-\infty}^{\infty}\delta_{t-\tau_{j}}\mathrm{e}^{-2\pi i \omega t}=\mathrm{e}^{-2\pi i\omega\tau_{j}}\]

and \(S_{z}(\omega)=N\). Hence, we have

\[H_{j}(\omega)=\frac{1}{N}\ \mathrm{e}^{2\pi i\omega\tau_{j}}.\]

Using (7.48), we obtain \(h_{jt}=\frac{1}{N}\delta(t+\tau_{j})\). Substituting in (7.41), we obtain the best linear unbiased estimator as the beam, computed as in (7.40).

#### Tests of Hypotheses

We consider first testing the hypothesis that the complete vector \(\beta_{t}\) is zero, i.e., that the vector signal is absent. We develop a test at each frequency \(\omega\) by taking single adjacent frequencies of the form \(\omega_{k}=k/n\), as in the initial section. We may approximate the DFT of the observed vector in the model (7.37) using a representation of the form \[Y_{j}(\omega_{k})=B^{\prime}(\omega_{k})Z_{j}(\omega_{k})+V_{j}(\omega_{k}) \tag{7.49}\]

for \(j=1,\ldots,N\), where the error terms will be uncorrelated with common variance \(f(\omega_{k})\), the spectral density of the error term. The independent variables \(Z_{j}(\omega_{k})\) can either be the infinite Fourier transform, or they can be approximated by the DFT. Hence, we can obtain the matrix version of a complex regression model, written in the form

\[Y(\omega_{k})=Z(\omega_{k})B(\omega_{k})+V(\omega_{k}), \tag{7.50}\]

where the \(N\times q\) matrix \(Z(\omega_{k})\) has been defined previously below (7.46) and \(Y(\omega_{k})\) and \(V(\omega_{k})\) are \(N\times 1\) vectors with the error vector \(V(\omega_{k})\) having mean zero, with covariance matrix \(f(\omega_{k})I_{N}\). The usual regression arguments show that the maximum likelihood estimator for the regression coefficient will be

\[\hat{B}(\omega_{k})=S_{z}^{-1}(\omega_{k})s_{zy}(\omega_{k}), \tag{7.51}\]

where \(S_{z}(\omega_{k})\) is given by (7.46) and

\[s_{zy}(\omega_{k})=Z^{*}(\omega_{k})Y(\omega_{k})=\sum_{j=1}^{N}\overline{Z_{j }(\omega_{k})}Y_{j}(\omega_{k}). \tag{7.52}\]

Also, the maximum likelihood estimator for the error spectral matrix is proportional to

\[s_{y\cdot z}^{2}(\omega_{k}) = \sum_{j=1}^{N}|Y_{j}(\omega_{k})-\hat{B}(\omega_{k})^{\prime}Z_{j }(\omega_{k})|^{2} \tag{7.53}\] \[= Y^{*}(\omega_{k})Y(\omega_{k})-Y^{*}(\omega_{k})Z(\omega_{k})[Z^ {*}(\omega_{k})Z(\omega_{k})]^{-1}Z^{*}(\omega_{k})Y(\omega_{k})\] \[= s_{y}^{2}(\omega_{k})-s_{zy}^{*}(\omega_{k})S_{z}^{-1}(\omega_{k })s_{zy}(\omega_{k}),\]

where

\[s_{y}^{2}(\omega_{k})=\sum_{j=1}^{N}|Y_{j}(\omega_{k})|^{2}. \tag{7.54}\]

Under the null hypothesis that the regression coefficient \(B(\omega_{k})=0\), the estimator for the error power is just \(s_{y}^{2}(\omega_{k})\). If smoothing is needed, we may replace the (7.53) and (7.54) by smoothed components over the frequencies \(\omega_{k}+\ell/n\), for \(\ell=-m,\ldots,m\) and \(L=2m+1\), close to \(\omega\). In that case, we obtain the regression and error spectral components as

\[\text{SSR}(\omega)=\sum_{\ell=-m}^{m}s_{zy}^{*}(\omega_{k}+\ell/n)S_{z}^{-1}( \omega_{k}+\ell/n)s_{zy}(\omega_{k}+\ell/n) \tag{7.55}\]

and

\[\text{SSE}(\omega)=\sum_{\ell=-m}^{m}s_{y\cdot z}^{2}(\omega_{k}+\ell/n). \tag{7.56}\]The \(F\)-statistic for testing no regression relation is

\[F_{2Lq,2L(N-q)}=\frac{N-q}{q}\frac{\text{SSR}(\omega)}{\text{SSE}(\omega)}. \tag{7.57}\]

The analysis of power pertaining to this situation appears in Table 7.2.

In the fixed regression case, the partitioned hypothesis that is the analog of \(\beta_{2t}=0\) in (7.27) with \(x_{t1},x_{t2}\) replaced by \(z_{t1},z_{t2}\). Here, we partition \(S_{z}(\omega)\) into \(q_{i}\times q_{j}\) (\(i,j=1,2\)) submatrices, say,

\[S_{z}(\omega_{k})=\begin{pmatrix}S_{11}(\omega_{k})&S_{12}(\omega_{k})\\ S_{21}(\omega_{k})&S_{22}(\omega_{k})\end{pmatrix}, \tag{7.58}\]

and the cross-spectral vector into its \(q_{i}\times 1\), for \(i=1,2\), subvectors

\[s_{zy}(\omega_{k})=\begin{pmatrix}s_{1y}(\omega_{k})\\ s_{2y}(\omega_{k})\end{pmatrix}. \tag{7.59}\]

Here, we test the hypothesis \(\beta_{2t}=0\) at frequency \(\omega\) by comparing the residual power (7.53) under the full model with the residual power under the reduced model, given by

\[s_{y\cdot 1}^{2}(\omega_{k})=s_{y}^{2}(\omega_{k})-s_{1y}^{*}(\omega_{k})S_{ 11}^{-1}(\omega_{k})s_{1y}(\omega_{k}). \tag{7.60}\]

Again, it is desirable to add over adjacent frequencies with roughly comparable spectra so the regression and error power components can be taken as

\[\text{SSR}(\omega)=\sum_{\ell=-m}^{m}\left[s_{y\cdot 1}^{2}(\omega_{k}+\ell/n) -s_{y\cdot z}^{2}(\omega_{k}+\ell/n)\right] \tag{7.61}\]

and

\[\text{SSE}(\omega)=\sum_{\ell=-m}^{m}s_{y\cdot z}^{2}(\omega_{k}+\ell/n). \tag{7.62}\]

The information can again be summarized as in Table 7.3, where the ratio of mean power regression and error components leads to the \(F\)-statistic

\[F_{2Lq,2L(N-q)}=\frac{(N-q)}{q_{2}}\frac{\text{SSR}(\omega)}{\text{SSE}(\omega )}. \tag{7.63}\]

We illustrate the analysis of power procedure using the infrasonic signal detection procedure of Example 7.2.

\begin{table}
\begin{tabular}{c c c} \hline \hline Source & Power & Degrees of freedom \\ \hline Regression & \(\text{SSR}(\omega)\)(7.55) & \(2Lq\) \\ Error & \(\text{SSE}(\omega)\)(7.56) & \(2L(N-q)\) \\ \hline Total & \(\text{SST}(\omega)\) & \(2LN\) \\ \hline \hline \end{tabular}
\end{table}
Table 7.2: Analysis of power (ANOPOW) for testing no contribution from the independent series at frequency \(\omega\) in the fixed input case 
**Example 7.4**: _Detecting the Infrasonic Signal Using ANOPOW_

We consider the problem of detecting the common signal for the three infrasonic series observing the common signal, as shown in Fig. 7.4. The presence of the signal is obvious in the waveforms shown, so the test here mainly confirms the statistical significance and isolates the frequencies containing the strongest signal components. Each series contained \(n=2048\) points, sampled at 10 points per second. We use the model in (7.39) so \(Z_{j}(\omega)=\mathrm{e}^{-2\pi i\omega\tau_{j}}\) and \(S_{z}(\omega)=N\) as in Example 7.3, with \(s_{zy}(\omega_{k})\) given as

\[s_{zy}(\omega_{k})=\sum_{j=1}^{N}\mathrm{e}^{2\pi i\omega\tau_{j}}Y_{j}(\omega_ {k}),\]

using (7.45) and (7.52). The above expression can be interpreted as being proportional to the weighted mean or _beam_, computed in frequency, and we introduce the notation

\[B_{w}(\omega_{k})=\frac{1}{N}\ \sum_{j=1}^{N}\mathrm{e}^{2\pi i\omega\tau_{j}}Y_{j} (\omega_{k}) \tag{7.64}\]

for that term. Substituting for the power components in Table 7.3 yields

\[s_{zy}^{*}(\omega_{k})S_{z}^{-1}(\omega_{k})s_{zy}(\omega_{k})=N|B_{w}(\omega_ {k})|^{2}\]

and

\[s_{y\cdot z}^{2}(\omega_{k})=\sum_{j=1}^{N}|Y_{j}(\omega_{k})-B_{w}(\omega_{k} )|^{2}=\sum_{j=1}^{N}|Y_{j}(\omega_{k})|^{2}-N|B_{w}(\omega_{k})|^{2}\]

for the regression signal and error components, respectively. Because only three elements in the array and a reasonable number of points in time exist, it seems advisable to employ some smoothing over frequency to obtain additional degrees of freedom. In this case, \(L=9\), yielding \(2(9)=18\) and \(2(9)(3-1)=36\) degrees of freedom for the numerator and denominator of the \(F\)-statistic (7.57). The top of Fig. 7.7 shows the analysis of power components due to error and the total power. The power is maximum at about.002 cycles per point or about.02 cycles per second. The \(F\)-statistic is compared with the.001 FDR and the corresponding null significance in the bottom panel and has the strongest detection at about.02 cycles

\begin{table}
\begin{tabular}{c c c} \hline \hline Source & Power & Degrees of freedom \\ \hline Regression & SSR(\(\omega\))(7.61) & \(2Lq_{2}\) \\ Error & SSE(\(\omega\)) (7.62) & \(2L(N-q)\) \\ \hline Total & SST(\(\omega\)) & \(2L(N-q_{1})\) \\ \hline \hline \end{tabular}
\end{table}
Table 7.3: Analysis of Power (ANOPOW) for testing no contribution from the last \(q_{2}\) inputs in the fixed input case per second. Little power of consequence appears to exist elsewhere, however, there is some marginally significant signal power near the.5 cycles per second frequency band.

The R code for this example is as follows.

 attach(beamd) L = 9; fdr =.001; N = 3 Y = cbind(beamd, beam-rowMeans(beamd) ) n = nextn(nrow(Y)) Y.fft = mwfft(as.ts(Y))/sqrt(n) Df = Y.fft[,1:3] # fft of the data Bf = Y.fft[,4] # beam fft  ssr = N*Re(Bf*Conj(Bf)) # raw signal spectrum  sse = Re(rowSums(Df*Conj(Df))) - ssr # raw error spectrum # Smooth  SSE = filter(sse, sides=2, filter=rep(1/L,L), circular=TRUE) SSR = filter(ssr, sides=2, filter=rep(1/L,L), circular=TRUE) SST = SSE + SSR par(mfrow=c(2,1), mar=c(4,4,2,1)+.1) Fr = 0:(n-1)/n # the fundamental frequencies nFr = 1:200 # number of freqs to plot plot(Fr[nFr], SST[nFr], type="1", ylab="log Power", xlab="", main="Sum of Squares", log="y") lines(Fr[nFr], SSE[nFr], type="1", lty=2) eF = (N-1)*SSR/SSE; df1 = 2*L; df2 = 2*L*(N-1) pvals = pf(eF, df1, df2, lower=FALSE) # p values for FDR pID = FDR(pvals, fdr); Fq = qf(1-fdr, df1, df2) plot(Fr[nFr], eF[nFr], type="1", ylab="F-statistic", xlab="Frequency", main="F Statistic")  abline(h=c(Fq, eF[pID]), lty=1:2)

Figure 7.7: Analysis of power for infrasound array on a log scale (_top panel_) with SST(\(\omega\)) shown as a _solid line_ and SSE(\(\omega\)) as a _dashed line_. The _F_-statistics (_bottom panel_) showing detections with the _dashed line_ based on an FDR level of.001 and the _solid line_ corresponding null \(F\) quantile

Although there are examples of detecting multiple regression functions of the general type considered above (see, for example, Shumway [182]), we do not consider additional examples of partitioning in the fixed input case here. The reason is that several examples exist in the section on designed experiments that illustrate the partitioned approach.

### Random Coefficient Regression

The lagged regression models considered so far have assumed the input process is either stochastic or fixed and the components of the vector of regression function \(\beta_{t}\) are fixed and unknown parameters to be estimated. There are many cases in time series analysis in which it is more natural to regard the regression vector as an unknown stochastic signal. For example, we have studied the state-space model in Chap. 6, where the state equation can be considered as involving a random parameter vector that is essentially a multivariate autoregressive process. In Sect. 4.8, we considered estimating the univariate regression function \(\beta_{t}\) as a signal extraction problem.

In this section, we consider a _random coefficient regression model_ of (7.38) in the equivalent form

\[y_{t}=\sum_{r=-\infty}^{\infty}z_{t-r}\ \beta_{r}+v_{t}, \tag{7.65}\]

where \(y_{t}=(y_{1t},\ldots,y_{Nt})^{\prime}\) is the \(N\times 1\) response vector and \(z_{t}=(z_{1t},\ldots,z_{Nt})^{\prime}\) are the \(N\times q\) matrices containing the fixed input processes. Here, the components of the \(q\times 1\) regression vector \(\beta_{t}\) are zero-mean, uncorrelated, stationary series with common spectral matrix \(f_{\beta}(\omega)I_{q}\) and the error series \(v_{t}\) have zero-means and spectral matrix \(f_{v}(\omega)I_{N}\), where \(I_{N}\) is the \(N\times N\) identity matrix. Then, defining the \(N\times q\) matrix \(Z(\omega)=(Z_{1}(\omega),Z_{2}(\omega),\ldots,Z_{N}(\omega))^{\prime}\) of Fourier transforms of \(z_{t}\), as in (7.44), it is easy to show the spectral matrix of the response vector \(y_{t}\) is given by

\[f_{y}(\omega)=f_{\beta}(\omega)Z(\omega)Z^{*}(\omega)+f_{v}(\omega)I_{N}. \tag{7.66}\]

The regression model with a stochastic stationary signal component is a general version of the simple additive noise model

\[y_{t}=\beta_{t}+v_{t},\]

considered by Wiener [211] and Kolmogorov [120], who derived the minimum mean squared error estimators for \(\beta_{t}\), as in Sect. 4.8. The more general multivariate version (7.65) represents the series as a convolution of the signal vector \(\beta_{t}\) and a known set of vector input series contained in the matrix \(z_{t}\). Restricting the covariance matrices of signal and noise to diagonal form is consistent with what is done in statistics using random effects models, which we consider here in a later section. The problem of estimating the regression function \(\beta_{t}\) is often called _deconvolution_ in the engineering and geophysical literature.

#### Estimation of the Regression Relation

The regression function \(\beta_{t}\) can be estimated by a general filter of the form (7.42), where we write that estimator in matrix form

\[\hat{\beta}_{t}=\sum_{r=-\infty}^{\infty}h_{r}\ y_{t-r}, \tag{7.67}\]

where \(h_{t}=(h_{1t},\ldots,h_{Nt})\), and apply the orthogonality principle, as in Sect. 4.8. A generalization of the argument in that section (see Problem 7.7) leads to the estimator

\[H(\omega)=[S_{z}(\omega)+\theta(\omega)I_{q}]^{-1}Z^{*}(\omega) \tag{7.68}\]

for the Fourier transform of the minimum mean-squared error filter, where the parameter

\[\theta(\omega)=\frac{f_{v}(\omega)}{f_{\beta}(\omega)} \tag{7.69}\]

is the inverse of the signal-to-noise ratio. It is clear from the frequency domain version of the linear model (7.50), the comparable version of the estimator (7.51) can be written as

\[\hat{B}(\omega)=[S_{z}(\omega)+\theta(\omega)I_{q}]^{-1}s_{zy}(\omega). \tag{7.70}\]

This version exhibits the estimator in the stochastic regressor case as the usual estimator, with a _ridge correction_, \(\theta(\omega)\), that is proportional to the inverse of the signal-to-noise ratio.

The mean-squared covariance of the estimator is shown to be

\[\mathrm{E}[(\hat{B}-B)(\hat{B}-B)^{*}]=f_{v}(\omega)[S_{z}(\omega)+\theta( \omega)I_{q}]^{-1}, \tag{7.71}\]

which again exhibits the close connection between this case and the variance of the estimator (7.51), which can be shown to be \(f_{v}(\omega)S_{z}^{-1}(\omega)\).

**Example 7.5**: _Estimating the Random Infrasonic Signal_

In Example 7.4, we have already determined the components needed in (7.68) and (7.69) to obtain the estimators for the random signal. The Fourier transform of the optimum filter at series \(j\) has the form

\[H_{j}(\omega)=\frac{e^{2\pi i\omega\tau_{j}}}{N+\theta(\omega)} \tag{7.72}\]

with the mean-squared error given by \(f_{v}(\omega)/[N+\theta(\omega)]\) from (7.71). The net effect of applying the filters will be the same as filtering the beam with the frequency response function

\[H_{0}(\omega)=\frac{N}{N+\theta(\omega)}=\frac{Nf_{\beta}(\omega)}{f_{v}( \omega)+Nf_{\beta}(\omega)}, \tag{7.73}\]

where the last form is more convenient in cases in which portions of the signal spectrum are essentially zero.

The optimal filters \(h_{t}\) have frequency response functions that depend on the signal spectrum \(f_{\beta}(\omega)\) and noise spectrum \(f_{v}(\omega)\), so we will need estimators for these parameters to apply the optimal filters. Sometimes, there will be values, suggested from experience, for the signal-to-noise ratio \(1/\theta(\omega)\) as a function of frequency. The analogy between the model here and the usual variance components model in statistics, however, suggests we try an approach along those lines as in the next section.

##### Detection and Parameter Estimation

The analogy to the usual variance components situation suggests looking at the regression and error components of Table 7.2 under the stochastic signal assumptions. We consider the components of (7.55) and (7.56) at a single frequency \(\omega_{k}\). In order to estimate the spectral components \(f_{\beta}(\omega)\) and \(f_{v}(\omega)\), we reconsider the linear model (7.50) under the assumption that \(B(\omega_{k})\) is a random process with spectral matrix \(f_{\beta}(\omega_{k})I_{q}\). Then, the spectral matrix of the observed process is (7.66), evaluated at frequency \(\omega_{k}\).

Consider first the component of the regression power, defined as

\[\text{SSR}(\omega_{k}) =s_{z\gamma}^{*}(\omega_{k})S_{z}^{-1}(\omega_{k})s_{z\gamma}( \omega_{k})\] \[=Y^{*}(\omega_{k})Z(\omega_{k})S_{z}^{-1}(\omega_{k})Z^{*}(\omega _{k})Y(\omega_{k}).\]

A computation shows

\[\text{E}[\text{SSR}(\omega_{k})]=f_{\beta}(\omega_{k})\;\text{tr}\{S_{z}( \omega_{k})\}+qf_{v}(\omega_{k}),\]

where \(\text{tr}\) denotes the trace of a matrix. If we can find a set of frequencies of the form \(\omega_{k}+\ell/n\), where the spectra and the Fourier transforms \(S_{z}(\omega_{k}+\ell/n)\approx S_{z}(\omega)\) are relatively constant, the expectation of the averaged values in (7.55) yields

\[\text{E}[\text{SSR}(\omega)]=Lf_{\beta}(\omega)\text{tr}\;[S_{z}(\omega)]+Lqf_ {v}(\omega). \tag{7.74}\]

A similar computation establishes

\[\text{E}[\text{SSE}(\omega)]=L(N-q)f_{v}(\omega). \tag{7.75}\]

We may obtain an approximately unbiased estimator for the spectra \(f_{v}(\omega)\) and \(f_{\beta}(\omega)\) by replacing the expected power components by their values and solving (7.74) and (7.75).

### Analysis of Designed Experiments

An important special case (see Brillinger [32, 34]) of the regression model (7.49) occurs when the regression (7.38) is of the form

\[y_{t}=z\beta_{t}+v_{t}, \tag{7.76}\]where \(z=(z_{1},z_{2},\ldots,z_{N})^{\prime}\) is a matrix that determines what is observed by the \(j\)th series; i.e.,

\[y_{jt}=z_{j}^{\prime}\beta_{t}+v_{jt}. \tag{7.77}\]

In this case, the matrix \(z\) of independent variables is constant and we will have the frequency domain model.

\[Y(\omega_{k})=ZB(\omega_{k})+V(\omega_{k}) \tag{7.78}\]

corresponding to (7.50), where the matrix \(Z(\omega_{k})\) was a function of frequency \(\omega_{k}\). The matrix is purely real, in this case, but the equations (7.51)-(7.57) can be applied with \(Z(\omega_{k})\) replaced by the constant matrix \(Z\).

Equality of Means

A typical general problem that we encounter in analyzing real data is a simple _equality of means test_ in which there might be a collection of time series \(y_{ijt}\), \(i=1,\ldots,I\), \(j=1,\ldots,N_{i}\), belonging to \(I\) possible groups, with \(N_{i}\) series in group \(i\). To test equality of means, we may write the regression model in the form

\[y_{ijt}=\mu_{t}+\alpha_{it}+v_{ijt}, \tag{7.79}\]

where \(\mu_{t}\) denotes the overall mean and \(\alpha_{it}\) denotes the effect of the \(i\)th group at time \(t\) and we require that \(\sum_{i}\alpha_{it}=0\) for all \(t\). In this case, the full model can be written in the general regression notation as

\[y_{ijt}=z_{ij}^{\prime}\beta_{t}+v_{ijt}\]

where

\[\beta_{t}=(\mu_{t},\alpha_{1t},\alpha_{2t},\ldots,\alpha_{I-1,t})^{\prime}\]

denotes the regression vector, subject to the constraint. The reduced model becomes

\[y_{ijt}=\mu_{t}+v_{ijt} \tag{7.80}\]

under the assumption that the group means are equal. In the full model, there are \(I\) possible values for the \(I\times 1\) design vectors \(z_{ij}\); the first component is always one for the mean, and the rest have a one in the \(i\)th position for \(i=1,\ldots,I-1\) and zeros elsewhere. The vectors for the last group take the value \(-1\) for \(i=2,3,\ldots,I-1\). Under the reduced model, each \(z_{ij}\) is a single column of ones. The rest of the analysis follows the approach summarized in (7.51)-(7.57). In this particular case, the power components in Table 7.3 (before smoothing) simplify to

\[\mbox{SSR}(\omega_{k})=\sum_{i=1}^{I}\sum_{j=1}^{N_{i}}|Y_{i\cdot}(\omega_{k}) -Y_{\cdot\cdot\cdot}(\omega_{k})|^{2} \tag{7.81}\]and

\[\text{SSE}(\omega_{k})=\sum_{i=1}^{I}\sum_{j=1}^{N_{i}}|Y_{ij}(\omega_{k})-Y_{i}.( \omega_{k})|^{2}, \tag{7.82}\]

which are analogous to the usual sums of squares in analysis of variance. Note that a dot (\(\cdot\)) stands for a mean, taken over the appropriate subscript, so the regression power component \(\text{SSR}(\omega_{k})\) is basically the power in the residuals of the group means from the overall mean and the error power component \(\text{SSE}(\omega_{k})\) reflects the departures of the group means from the original data values. Smoothing each component over \(L\) frequencies leads to the usual \(F\)-statistic (7.63) with \(2L(I-1)\) and \(2L(\sum_{i}N_{i}-I)\) degrees of freedom at each frequency \(\omega\) of interest.

**Example 7.6**: Means Test for the fMRI Data

Figure 7.1 showed the mean responses of subjects to various levels of periodic stimulation while awake and while under anesthesia, as collected in a pain perception experiment of Antognini et al. [10]. Three types of periodic stimuli were presented to awake and anesthetized subjects, namely, brushing, heat, and shock. The periodicity was introduced by applying the stimuli, brushing, heat, and shocks in on-off sequences lasting 32 s each and the sampling rate was one point every 2 s. The blood oxygenation level (BOLD) signal intensity (Ogawa et al. [144]) was measured at nine locations in the brain. Areas of activation were determined using a technique first described by Bandettini et al. [11]. The specific locations of the brain where the signal was measured were Cortex 1: Primary Somatosensory, Contralateral, Cortex 2: Primary Somatosensory, Ipsilateral, Cortex 3: Secondary Somatosensory, Contralateral, Cortex 4: Secondary Somatosensory, Ipsilateral, Caudate, Thalamus 1: Contralateral, Thalamus 2: Ipsilateral, Cerebellum 1: Contralateral and Cerebellum 2: Ipsilateral. Figure 7.1 shows the mean response of subjects at Cortex 1 for each of the six treatment combinations, 1: Awake-Brush (5 subjects), 2: Awake-Heat (4 subjects), 3: Awake-Shock (5 subjects), 4: Low-Brush (3 subjects), 5: Low-Heat (5 subjects), and 6: Low-Shock( 4 subjects). The objective of this first analysis is to test equality of these six group means, paying special attention to the 64-s period band (1/64 cycles per second) expected from the periodic driving stimuli. Because a test of equality is needed at each of the nine brain locations, we took \(\alpha=.001\) to control for the overall error rate. Figure 7.8 shows \(F\)-statistics, computed from (7.63), with \(L=3\), and we see substantial signals for the four cortex locations and for the second cerebellum trace, but the effects are nonsignificant in the caudate and thalamus regions. Hence, we will retain the four cortex locations and the second cerebellum location for further analysis.

The R code for this example is as follows.

 n = 128 # length of series  n.freq = 1 + n/2 # number of frequencies  Fr = (@:(n.freq-1))/n # the frequencies  N = c(5,4,5,3,5,4) # number of series for each cell  n.subject = sum(N) # number of subjects (26)  n.trt = 6 # number of treatments  L = 3 # for smoothing  num.df = 2*L*(n.trt-1) # df for F test den.df = 2*L*(n.subject-n.trt)

_# Design Matrix (Z):_ Z1 = outer(rep(1,N[1]), c(1,1,0,0,0,0)) Z2 = outer(rep(1,N[2]), c(1,0,1,0,0,0)) Z3 = outer(rep(1,N[3]), c(1,0,0,1,0,0)) Z4 = outer(rep(1,N[4]), c(1,0,0,0,1,0)) Z5 = outer(rep(1,N[5]), c(1,0,0,0,0,1)) Z6 = outer(rep(1,N[6]), c(1,-1,-1,-1,-1)) Z = rbind(Z1, Z2, Z3, Z4, Z5, Z6) ZZ = t(Z)%*%Z <- rep(NA, n) -> SSER HatF = 2%*%solve(ZZ, t(Z)) HatR = Z[,1]%*%t(Z(1,1))/ZZ[1,1] par(mfrow=c(3,3), mar=c(3.5,4,0,0), oma=c(0,0,2,2), mgp = c(1.6,.6,0)) loc.name = c("Cortex 1","Cortex 2","Cortex 3","Cortex 4","Caudate","Thalamus 1","Thalamus 2","Cerebellum 1","Cerebellum 2") for(Loc in 1:9) { i = n.trt*(Loc-1) Y = cbind(fmri[[i+1]], fmri[[i+2]], fmri[[i+3]], fmri[[i+4]], fmri[[i+5]], fmri[[i+6]]] Y = mwfft(spec.taper(Y, p=.5))/sqrt(n) Y = t(Y) # Y is now 26 x 128 FFTs

_#_ Calculation of Error Spectra for (kin 1:n) {  SSY = Re(Conj(t(Y[,k]))%*%[t,k])  SSReg = Re(Conj(t(Y[,k]))%*%HatF%*%[t,k])  SSEF[k] = SSY - SSReg  SSReg = Re(Conj(t(Y[,k]))%*%HatR%*%[t,k])  SSER[k] = SSY - SSReg }

Figure 7.8: Frequency-dependent equality of means tests for fMRI data at nine brain locations. \(L=3\) and critical value \(F_{.001}(30,120)=2.26\)

Smooth  sSSEF = filter(SSEF, rep(1/L, L), circular = TRUE)  sSSER = filter(SSER, rep(1/L, L), circular = TRUE)  eF = (den.df/num.df)*(sSSER-sSSEF)/sSSEF  plot(Fr, eF[1:n.freq], type="1", xlab="Frequency", ylab="F Statistic",  ylim-c(0,7))  abline(h=qf(.999, num.df, den.df),lty=2)  text(.25, 6.5, loc.name[Loc], cex=1.2) }

An Analysis of Variance ModelThe arrangement of treatments for the fMRI data in Fig. 7.1 suggests more information might be available than was obtained from the simple equality of means test. Separate effects caused by state of consciousness as well as the separate treatments brush, heat, and shock might exist. The reduced signal present in the low shock mean suggests a possible interaction between the treatments and level of consciousness. The arrangement in the classical two-way table suggests looking at the analog of the two factor analysis of variance as a function of frequency. In this case, we would obtain a different version of the regression model (7.79) of the form

\[y_{ijkt}=\mu_{t}+\alpha_{it}+\beta_{jt}+\gamma_{ijt}+v_{ijkt} \tag{7.83}\]

for the \(k\)th individual undergoing the \(i\)th level of some factor A and the \(j\)th level of some other factor B, \(i=1,\ldots I,j=1\ldots,J,k=1,\ldots n_{ij}\). The number of individuals in each cell can be different, as for the fMRI data in the next example. In the above model, we assume the response can be modeled as the sum of a mean, \(\mu_{t}\), a _row effect_ (type of stimulus), \(\alpha_{it}\), a _column effect_ (level of consciousness), \(\beta_{jt}\) and an _interaction_, \(\gamma_{ijt}\), with the usual restrictions

\[\sum_{i}\alpha_{it}=\sum_{j}\beta_{jt}=\sum_{i}\gamma_{ijt}=\sum_{j}\gamma_{ ijt}=0\]

required for a full rank design matrix \(Z\) in the overall regression model (7.78). If the number of observations in each cell were the same, the usual simple analogous version of the power components (7.81) and (7.82) would exist for testing various hypotheses. In the case of (7.83), we are interested in testing hypotheses obtained by dropping one set of terms at a time out of (7.83), so an A factor (testing \(\alpha_{it}=0\)), a B factor (\(\beta_{jt}=0\)), and an interaction term (\(\gamma_{ijt}=0\)) will appear as components in the analysis of power. Because of the unequal numbers of observations in each cell, we often put the model in the form of the regression model (7.76)-(7.78).

**Example 7.7**: **Analysis of Power Tests for the fMRI Series**

For the fMRI data given as the means in Fig. 7.1, a model of the form (7.83) is plausible and will yield more detailed information than the simple equality of means test described earlier. The results of that test, shown in Fig. 7.8, were that the means were different for the four cortex locations and for the second cerebellum location. We may examine these differences further by testing whether the mean differencesare because of the nature of the stimulus or the consciousness level, or perhaps due to an interaction between the two factors. Unequal numbers of observations exist in the cells that contributed the means in Fig. 7.1. For the regression vector,

\[(\mu_{t},\alpha_{1t},\alpha_{2t},\beta_{1t},\gamma_{11t},\gamma_{21t})^{\prime},\]

the rows of the design matrix are as specified in Table 7.4. Note the restrictions given above for the parameters.

The results of testing the three hypotheses are shown in Fig. 7.9 for the four cortex locations and the cerebellum, the components that showed some significant differences in the means in Fig. 7.8. Again, the regression power components were smoothed over \(L=3\) frequencies. Appealing to the ANOPOW results summarized in Table 7.3 for each of the subhypotheses, \(q_{2}=1\) when the stimulus effect is dropped, and \(q_{2}=2\) when either the consciousness effect or the interaction terms are dropped. Hence, \(2Lq_{2}=6,12\) for the two cases, with \(N=\sum_{ij}n_{ij}=26\) total observations. Here, the state of consciousness (Awake, Sedated) has the major effect at the signal frequency. The level of stimulus was less significant at the signal frequency. A significant interaction occurred, however, at the ipsilateral component of the primary somatosensory cortex location.

The R code for this example is similar to Example 7.6.

Figure 7.9: Analysis of power for fMRI data at five locations, \(L=3\) and critical values \(F_{.001}(6,120)=4.04\) for stimulus and \(F_{.001}(12,120)=3.02\) for consciousness and interaction

n = 128 n.freq = 1 + n/2 Fr = (0:(n.freq-1))/n nFr = 1:(n.freq/2) N = c(5,4,5,3,5,4) n.subject = sum(N) n.para = 6 # number of parameters L = 3 # for smoothing df.stm = 2*L*(3-1) # stimulus (3 levels: Brush,Heat,Shock) df.con = 2*L*(2-1) # conscious (2 levels: Awake,Sedated) df.int = 2*L*(3-1)*(2-1) # interaction den.df = 2*L*(n.subject-n.para) # df for full model
Design Matrix: mu a1 a2 b1 g2 Z1 = outer(rep(1,N[1]), c(1, 1, 0, 1, 1, 0)) Z2 = outer(rep(1,N[2]), c(1, 0, 1, 1, 0, 1)) Z3 = outer(rep(1,N[3]), c(1, -1, -1, 1, -1, -1)) Z4 = outer(rep(1,N[4]), c(1, 1, 0, -1, -1, 0)) Z5 = outer(rep(1,N[5]), c(1, 0, 1, -1, 0, -1)) Z6 = outer(rep(1,N[6]), c(1, -1, -1, -1, 1, 1)) Z = rbind(Z1, Z2, Z3, Z4, Z5, Z6) ZZ = t(Z)%%Z rep(NA, m)-> SSF-> SSE.stm-> SSE.com> SSE.int HatF = 2*%Solve(ZZ,t(Z) Hat.stm = Z[,-(2:3)]%*Solve(ZZ[-(2:3),-(2:3)], t(Z[,-(2:3)])) Hat.con = Z[,-4]%*Solve(ZZ[-4,-4], t(Z[,-4])) Hat.int = Z[,-(5:6)]%*Solve(ZZ[-(5:6),-(5:6)], t(Z[,-(5:6)])) par(mfrow=c(5,3), mar=c(3.5,4,0,0), oma=c(0,0,2,2), mpp = c(1.6,_6,0)) loc.name = c("Cortex 1","Cortex 2","Cortex 3","Cortex 4","Caudate", " Thalamus 1","Thalamus 2","Cerebellum 1","Cerebellum 2") for(Loc in c(1:4,9)) { # only Loc 1 to 4 and 9 used  i = 6*(Loc-1)  Y = cbind(fmri[[i+1]], fmri[[i+2]], fmri[[i+3]], fmri[[i+4]], fmri[[i+5], fmri[[i+6]])  Y = mvfft(spec.taper(Y, p=.5))/sqrt(n); Y = t(Y) for (kin 1:n) {  SSY = Re(Conj(t(Y[,k]))%*XY[,k])  SSReg = Re(Conj(t(Y[,k]))%*MAtF%*XY[,k])  SSEF[k] = SSY - SSReg  SSReg = Re(Conj(t(Y[,k]))%*MAt.stm%*XY[,k])  SSE.stm[k] = SSY-SSReg  SSReg = Re(Conj(t(Y[,k]))%*MAt.con%*XY[,k])  SSE.con[k] = SSY-SSReg  SSReg = Re(Conj(t(Y[,k]))%*MAt.int%*XY[,k])  SSE.int[k] = SSY-SSReg } # Smooth  sSSEF = filter(SSEF, rep(1/L, L), circular = TRUE)  SSSE.stm = filter(SSE.stm, rep(1/L, L), circular = TRUE)  SSSE.stm = filter(SSE.int, rep(1/L, L), circular = TRUE)  ef.stm = (den.df/df.stm)*(sSSE.stm-sSSEF)/sSSEF  ef.com = (den.df/df.con)*(sSSE.con-sSSEF)/sSSEF  ef.int = (den.df/df.int)*(sSSE.int-sSSEF)/sSSEF plot(Fr[nFr],ef.stm[nFr], type="l", xlab="Frequency", ylab="F Statistic",  ylim=c(0,12))  abline(h=qf(.999, df.stm, den.df),lty=2)  if(Loc==1) mtext("Stimulus", side=3, line=.3, cex=1)mtext(loc.name[Loc], side=2, line=3, cex=.9) plot(Fr[nFr], eF.con[nFr], type="l", xlab="Frequency", ylab="F Statistic", ylim=c(@,12))  abline(h=qf(.999, df.con, den.df),lty=2)  if(Loc==l) mtext("Consciousness", side=3, line=.3, cex=1) plot(Fr[nFr], eF.int[nFr], type="l", xlab="Frequency", ylab="F Statistic", ylim=c(@,12))  abline(h=qf(.999, df.int, den.df),lty=2)  if(Loc==l) mtext("Interaction", side=3, line=.3, cex=1) }

Simultaneous Inference

In the previous examples involving the fMRI data, it would be helpful to focus on the components that contributed most to the rejection of the equal means hypothesis. One way to accomplish this is to develop a test for the significance of an arbitrary _linear compound_ of the form

\[\Psi(\omega_{k})=A^{*}(\omega_{k})B(\omega_{k}), \tag{7.84}\]

where the components of the vector \(A(\omega_{k})=(A_{1}(\omega_{k}),A_{2}(\omega_{k}),\ldots,A_{q}(\omega_{k}))^{\prime}\) are chosen in such a way as to isolate particular linear functions of parameters in the regression vector \(B(\omega_{k})\) in the regression model (7.78). This argument suggests developing a test of the hypothesis \(\Psi(\omega_{k})=0\) for _all possible_ values of the linear coefficients in the compound (7.84) as is done in the conventional analysis of variance approach (see, for example, Scheffe [172]).

Recalling the material involving the regression models of the form (7.50), the linear compound (7.84) can be estimated by

\[\hat{\Psi}(\omega_{k})=A^{*}(\omega_{k})\hat{B}(\omega_{k}), \tag{7.85}\]

where \(\hat{B}(\omega_{k})\) is the estimated vector of regression coefficients given by (7.51) and independent of the error spectrum \(s_{y\cdot z}^{2}(\omega_{k})\) in (7.53). It is possible to show the maximum of the ratio

\[F(A)=\frac{N-q}{q}\ \frac{|\hat{\Psi}(\omega_{k})-\Psi(\omega_{k})|^{2}}{s_{y \cdot z}^{2}(\omega_{k})Q(A)}, \tag{7.86}\]

where

\[Q(A)=A^{*}(\omega_{k})S_{z}^{-1}(\omega_{k})A(\omega_{k}) \tag{7.87}\]

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c c} \hline \hline  & \multicolumn{10}{c}{Awake} & \multicolumn{10}{c}{Low anesthesia} \\ \hline Brush & 1 & 1 & 0 & 1 & 1 & 0 & (5) & 1 & 1 & 0 & \(-1\) & \(-1\) & 0 & (3) \\ Heat & 1 & 0 & 1 & 1 & 0 & 1 & (4) & 1 & 0 & 1 & \(-1\) & 0 & \(-1\) & (5) \\ Shock & 1 & \(-1\) & \(-1\) & 1 & \(-1\) & \(-1\) & (5) & 1 & \(-1\) & \(-1\) & \(-1\) & 1 & 1 & (4) \\ \hline \end{tabular}
\end{table}
Table 7.4: Rows of the design matrix for Example 7.7is bounded by a statistic that has an \(F\)-distribution with \(2q\) and \(2(N-q)\) degrees of freedom. Testing the hypothesis that the compound has a particular value, usually \(\Psi(\omega_{k})=0\), then proceeds naturally, by comparing the statistic (7.86) evaluated at the hypothesized value with the \(\alpha\) level point on an \(F_{2q,2(N-q)}\) distribution. We can choose an infinite number of compounds of the form (7.84) and the test will still be valid at level \(\alpha\). As before, arguing the error spectrum is relatively constant over a band enables us to smooth the numerator and denominator of (7.86) separately over \(L\) frequencies so distribution involving the smooth components is \(F_{2Lq,2L(N-q)}\).

**Example 7.8**: Simultaneous Inference for the fMRI Series

As an example, consider the previous tests for significance of the fMRI factors, in which we have indicated the primary effects are among the stimuli but have not investigated which of the stimuli, heat, brushing, or shock, had the most effect. To analyze this further, consider the means model (7.79) and a \(6\times 1\) contrast vector of the form

\[\hat{\Psi}=A^{*}(\omega_{k})\hat{B}(\omega_{k})=\sum_{i=1}^{6}A_{i}^{*}(\omega_ {k})Y_{i}.(\omega_{k}), \tag{7.88}\]

where the means are easily shown to be the regression coefficients in this particular case. In this case, the means are ordered by columns; the first three means are the three levels of stimuli for the awake state, and the last three means are the levels for the anesthetized state. In this special case, the denominator terms are

\[Q=\sum_{i=1}^{6}\frac{|A_{i}(\omega_{k})|^{2}}{N_{i}}, \tag{7.89}\]

with \(SSE(\omega_{k})\) available in (7.82). In order to evaluate the effect of a particular stimulus, like brushing over the two levels of consciousness, we may take \(A_{1}(\omega_{k})=A_{4}(\omega_{k})=1\) for the two brush levels and \(A(\omega_{k})=0\) zero otherwise. From Fig. 7.10, we see that, at the first and third cortex locations, brush and heat are both significant, whereas the fourth cortex shows only brush and the second cerebellum shows only heat. Shock appears to be transmitted relatively weakly, when averaged over the awake and mildly anesthetized states.

The R code for this example is as follows.

 n = 128; n.freq = 1 + n/2  Fr = (0:(n.freq-1))/n; nFr = 1:(n.freq/2)  N = c(5,4,5,3,5,4); n.subject = sum(N); L = 3  # Design Matrix

Z1 = outer(rep(1,N[1]), c(1,0,0,0,0,0))  Z2 = outer(rep(1,N[2]), c(0,1,0,0,0,0))  Z3 = outer(rep(1,N[3]), c(0,0,1,0,0,0))  Z4 = outer(rep(1,N[4]), c(0,0,0,1,0,0))  Z5 = outer(rep(1,N[5]), c(0,0,0,0,1,0))  Z6 = outer(rep(1,N[6]), c(0,0,0,0,0,0,1))  Z = rbind(Z1, Z2, Z3, Z4, Z5, Z6); ZZ = t(Z)%*%Z  # Contrasts: 6 by 3  A = rbind(diag(1,3), diag(1,3))  nq = nrow(A); num.df = 2*L*nq; den.df = 2*L*(n.subject-nq)HatF=Z%*%solve(ZZ,t(Z))#fullmodel rep(NA,n)->SSEF->SSER;ef=matrix(th,n,3) par(mffrow=c(5,3),mar=c(3.5,4,th,th),oma=c(th,th,2,2),mpp=c(1.6,.6,th)) loc.name=c("Cortex1","Cortex2","Cortex3","Cortex4","Caudate","Thalamus1","Thalamus2","Cerebellum1","Cerebellum2") cond.name=c("Brush","Heat","Shock") for(Locinc(1:4,9)){ i=6*(Loc-1) Y=cbind(fmri[[i+1]],fmri[[i+2]],fmri[[i+3]],fmri[[i+4]],fmri[[i+5]], fmri[[i+6]]) Y=mvfft(spec.taper(Y,p.=5))/sqrt(n);Y=t(Y) for(condin1:3){ Q=t(A[,cond])%*%solve(ZZ,A[,cond]) HR=A[,cond]%*%solve(ZZ,t(Z)) for(kin1:n){ SSF=Re(Conj(t(Y[,k]))%*XY[,k]) SSReg=Re(Conj(t(Y[,k]))%*%HatF%*%Y[,k]) SSEF[k]=(SSY-SSReg)*Q SSReg=HR%*%Y[,k] SSER[k]=Re(SSReg+Conj(SSReg))#Smooth sSSEF=filter(SSEF,rep(1/L,L),circular=TRUE) sSSER=filter(SSER,rep(1/L,L),circular=TRUE) eF[,cond]=(den.df/num.df)*(sSSER/sSSEF){ plot(Fr[nFr],eF[nFr,1],type="l",xlab="Frequency",ylab="FStatistic",ylim=c(th,5))  abline(h=qf(.999,num.df,den.df),lty=2)  if(Loc=l)mtext("Brush",side=3,line.3,cex=1)  mtext(loc.name[Loc],side=2,line=3,cex=.9)  plot(Fr[nFr],eF[nFr,2],type="l",xlab="Frequency",ylab="FStatistic",ylim=c(th,5))  abline(h=qf(.999,num.df,den.df),lty=2)  if(Loc=l)mtext("Heat",side=3,line.3,cex=1) plot(Fr[nFr],eF[nFr,3],type="l",xlab="Frequency",ylab="FStatistic",ylim=c(th,5))  abline(h=qf(.999,num.df,den.df),lty=2)  if(Loc==l)mtext("Shock",side=3,line.3,cex=1)}

### Multivariate Tests

Although it is possible to develop multivariate regression along lines analogous to the usual real valued case, we will only look at tests involving equality of group means and spectral matrices, because these tests appear to be used most often in applications. For these results, consider the \(p\)-variate time series \(y_{ijt}=(y_{ijt1},\ldots,y_{ijtp})^{\prime}\) to have arisen from observations on \(j=1,\ldots,N_{i}\) individuals in group \(i\), all having mean \(\mu_{it}\) and stationary autocovariance matrix \(\Gamma_{i}(h)\). Denote the DFTs of the group mean vectors as \(Y_{i}.(\omega_{k})\) and the \(p\times p\) spectral matrices as \(\hat{f}_{i}(\omega_{k})\) for the \(i=1,2,\ldots,I\) groups. Assume the same general properties as for the vector series considered in Sect. 7.3.

In the multivariate case, we obtain the analogous versions of (7.81) and (7.82) as the _between cross-power_ and _within cross-power_ matrices

\[\text{SPR}(\omega_{k})=\sum_{i=1}^{I}\sum_{j=1}^{N_{i}}\bigl{(}Y_{i}.(\omega_{ k})-Y_{*}.(\omega_{k})\bigr{)}\bigl{(}Y_{i}.(\omega_{k})-Y_{*}.(\omega_{k}) \bigr{)}^{*} \tag{7.90}\]and

\[\text{SPE}(\omega_{k})=\sum_{i=1}^{I}\sum_{j=1}^{N_{i}}\bigl{(}Y_{ij}(\omega_{k})-Y_ {i}.(\omega_{k})\bigr{)}\bigl{(}Y_{ij}(\omega_{k})-Y_{i}.(\omega_{k})\bigr{)}^{ *}. \tag{7.91}\]

The equality of means test is rejected using the fact that the likelihood ratio test yields a monotone function of

\[\Lambda(\omega_{k})=\frac{|\text{SPE}(\omega_{k})|}{|\text{SPE}(\omega_{k})+ \text{SPR}(\omega_{k})|}. \tag{7.92}\]

Khatri [117] and Hannan [86] give the approximate distribution of the statistic

\[\chi^{2}_{2(I-1)p}=-2\biggl{(}\sum_{i}N_{i}-I-p-1\biggr{)}\log\Lambda(\omega_{ k}) \tag{7.93}\]

as chi-squared with \(2(I-1)p\) degrees of freedom when the group means are equal.

The case of \(I=2\) groups reduces to Hotelling's \(T^{2}\), as has been shown by Giri [73], where

\[T^{2}=\frac{N_{1}N_{2}}{(N_{1}+N_{2})}\bigl{[}Y_{1}.(\omega_{k})-Y_{2}.(\omega _{k})\bigr{]}^{*}\hat{f}_{v}^{-1}(\omega_{k})\bigl{[}Y_{1}.(\omega_{k})-Y_{2}. (\omega_{k})\bigr{]}, \tag{7.94}\]

Figure 7.10: Power in simultaneous linear compounds at five locations, enhancing brush, heat, and shock effects, \(L=3,F_{.001}(36,120)=2.16\)

where

\[\hat{f}_{v}(\omega_{k})=\frac{\text{SPE}(\omega_{k})}{\sum_{i}N_{i}-I} \tag{7.95}\]

is the pooled error spectrum given in (7.91),with \(I=2\). The test statistic, in this case, is

\[F_{2p,2(N_{1}+N_{2}-p-1)}=\frac{(N_{1}+N_{2}-2)p}{(N_{1}+N_{2}-p-1)}T^{2}, \tag{7.96}\]

which was shown by Giri [73] to have the indicated limiting \(F\)-distribution with \(2p\) and \(2(N_{1}+N_{2}-p-1)\) degrees of freedom when the means are the same. The classical \(t\)-test for inequality of two univariate means will be just (7.95) and (7.96) with \(p=1\).

Testing equality of the spectral matrices is also of interest, not only for discrimination and pattern recognition, as considered in the next section, but also as a test indicating whether the equality of means test, which assumes equal spectral matrices, is valid. The test evolves from the likelihood ration criterion, which compares the single group spectral matrices

\[\hat{f}_{i}(\omega_{k})=\frac{1}{N_{i}-1}\sum_{j=1}^{N_{i}}\bigl{(}Y_{ij}( \omega_{k})-Y_{i}.(\omega_{k})\bigr{)}\bigl{(}Y_{ij}(\omega_{k})-Y_{i}.(\omega _{k})\bigr{)}^{*} \tag{7.97}\]

with the pooled spectral matrix (7.95). A modification of the likelihood ratio test, which incorporates the degrees of freedom \(M_{i}=N_{i}-1\) and \(M=\sum M_{i}\) rather than the sample sizes into the likelihood ratio statistic, uses

\[L^{\prime}(\omega_{k})=\frac{M^{Mp}}{\prod_{i=1}^{I}M_{i}^{M_{i}P}}\frac{\prod |M_{i}\hat{f}_{i}(\omega_{k})|^{M_{i}}}{|M\hat{f}_{v}(\omega_{k})|^{M}}. \tag{7.98}\]

Krishnaiah et al. [121] have given the moments of \(L^{\prime}(\omega_{k})\) and calculated 95% critical points for \(p=3,4\) using a Pearson Type I approximation. For reasonably large samples involving smoothed spectral estimators, the approximation involving the first term of the usual chi-squared series will suffice and Shumway [180] has given

\[\chi^{2}_{(I-1)p^{2}}=-2r\log L^{\prime}(\omega_{k}), \tag{7.99}\]

where

\[1-r=\frac{(p+1)(p-1)}{6p(I-1)}\biggl{(}\sum_{i}M_{i}^{-1}-M^{-1}\biggr{)}, \tag{7.100}\]

with an approximate chi-squared distribution with \((I-1)p^{2}\) degrees of freedom when the spectral matrices are equal. Introduction of smoothing over \(L\) frequencies leads to replacing \(M_{j}\) and \(M\) by \(LM_{j}\) and \(LM\) in the equations above.

Of course, it is often of great interest to use the above result for testing equality of two univariate spectra, and it is obvious from the material in Chap. 4,

\[F_{2LM_{1},2LM_{2}}=\frac{\hat{f}_{1}(\omega)}{\hat{f}_{2}(\omega)} \tag{7.101}\]

will have the requisite \(F\)-distribution with \(2LM_{1}\) and \(2LM_{2}\) degrees of freedom when spectra are smoothed over \(L\) frequencies.

**Example 7.9**: **Equality of Means and Spectral Matrices**

An interesting problem arises when attempting to develop a methodology for discriminating between waveforms originating from explosions and those that came from the more commonly occurring earthquakes. Figure 7.2 shows a small subset of a larger population of bivariate series consisting of two phases from each of eight earthquakes and eight explosions. If the large-sample approximations to normality hold for the DFTs of these series, it is of interest to known whether the differences between the two classes are better represented by the mean functions or by the spectral matrices. The tests described above can be applied to look at these two questions. The upper left panel of Fig. 7.11 shows the test statistic (7.96) with the straight line denoting the critical level for \(\alpha=.001\), i.e., \(F_{.001}(4,26)=7.36\), for equal means using \(L=1\), and the test statistic remains well below its critical value at all frequencies, implying that the means of the two classes of series are not significantly different. Checking Fig. 7.2 shows little reason exists to suspect that either the earthquakes or explosions have a nonzero mean signal. Checking the equality of the spectra and the spectral matrices, however, leads to a different conclusion. Some smoothing (\(L=21\)) is useful here, and univariate tests on both the P and S components using (7.101) and \(N_{1}=N_{2}=8\) lead to strong rejections of the equal spectra hypotheses. The rejection seems stronger for the S component and we might tentatively identify that component as being dominant. Testing equality of the spectral matrices using (7.99) and \(\chi^{2}_{.001}(4)=18.47\) shows a similar strong rejection of the equality of spectral matrices. We use these results to suggest optimal discriminant functions based on spectral differences in the next section.

Figure 7.11: Tests for equality of means, spectra, and spectral matrices for the earthquake and explosion data \(p=2,L=21,n=1024\) points at 40 points per secondThe R code for this example is as follows. We make use of the recycling feature of R and the fact that the data are bivariate to produce simple code specific to this problem in order to avoid having to use multiple arrays.

 P = 1:1024; S = P+1024; N = 8; n = 1024; p.dim = 2; m = 10; L = 2*m+1  eq.P = as.ts(eqexp[P,1:8]); eq.S = as.ts(eqexp[S,1:8])  eq.m = cbind(rowMeans(eq.P), rowMeans(eq.S))  ex.P = as.ts(eqexp[P,9:16]); ex.S = as.ts(eqexp[S,9:16])  ex.m = cbind(rowMeans(ex.P), rowMeans(ex.S))  m.diff = mwfft(eq.m - ex.m)/sqrt(n)  eq.Pf = mwfft(eq.P-eq.m,[1])/sqrt(n)  eq.sf = mwfft(eq.S-eq.m,[2])/sqrt(n)  ex.pf = mwfft(ex.P-ex.m,[1])/sqrt(n)  ex.Sf = mwfft(ex.S-ex.m,[2])/sqrt(n)  fu11 = rowSums(eq.Pf*Conj(eq.Pf))+rowSums(ex.Pf*Conj(ex.Pf))/(2*(N-1))  fu12 = rowSums(eq.Pf*Conj(eq.Sf))+rowSums(ex.Pf*Conj(ex.Sf))/(2*(N-1))  fu22 = convSums(eq.Sf*Conj(eq.Sf))+rowSums(ex.Sf*Conj(ex.Sf))/(2*(N-1))  fu21 = Conj(fv12)  # Equal Means  T2 = rep(NA, 512)  for (k in 1:512){  fvk = matrix(c(fv11[k], fv21[k], fv12[k], fv22[k]), 2, 2)  dk = as.matrix(m.diff[k,])  T2[k] = Re((N/2)*Conj(t(dk))%*solve(fvk,dk)) }  eF = T2*(2*p.dim*(N-1))/(2*N-p.dim-1)  par(mfrow=c(2,2), mar=c(3,3,2,1), mgp = c(1.6,.6,0), cex.main=1.1)  freq = 40*(0:511)/n # Hz  plot(freq, eF, type="1", xlab="Frequency (Hz)", ylab="F Statistic",  main="Equal Means")  abline(h = qf(.999, 2*p.dim, 2*(2*N-p.dim-1)))  # Equal P  kd = kernel("daniell",m);  u = Re(rowSums(eq.Pf*Conj(eq.Pf)/(N-1))  feq.P = kernapply(u, kd, circular=TRUE)  u = Re(rowSums(ex.Pf*Conj(ex.Pf)/(N-1))  fex.P = kernapply(u, kd, circular=TRUE)  plot(freq, feq.P[1:512]/fex.P[1:512], type="1", xlab="Frequency (Hz)",  ylab="F Statistic", main="Equal P-Spectra")  abline(h=qf(.999, 2*L*(N-1), 2*L*(N-1)))  # Equal Spectra  u = rowSums(eq.Pf*Conj(eq.Sf))/(N-1)  feq.PS = kernapply(u, kd, circular=TRUE)  u = rowSums(ex.Pf*Conj(ex.Sf)/(N-1))  fex.PS = kernapply(u, kd, circular=TRUE)  fv11 = kernapply(fv11, kd, circular=TRUE)  fv22 = kernapply(fv22, kd, circular=TRUE)  fv12 = kernapply(fv12, kd, circular=TRUE)  Mi = L*(N-1); M = 2*Mi  TS = rep(NA,512)for (k in 1:512){  det.feq.k = Re(feq.P[k]*feq.S[k] - feq.PS[k]*Conj(feq.PS[k]))  det.fex.k = Re(fex.P[k]*fex.S[k] - fex.PS[k]*Conj(fex.PS[k]))  det.fv.k = Re(fv11[k]*fv22[k] - fv12[k]*Conj(fv12[k]))  log.n1 = log(M)*(M*p.dim); log.d1 = log(Mi)*(2*Mi*p.dim)  log.n2 = log(Mi)*2 +log(det.feq.k)*Mi  log.d2 = (log(M)+log(det.fv.k))*M  r = 1 - ((p.dim+1)*(p.dim-1)/6*p.dim*(2-1))*(2/Mi - 1/M)  TS[k] = -2*r*(log.n1+log.n2-log.d1-log.d2)  plot(freq, TS, type="1", xlab="Frequency (Hz)", ylab="Chi-Sq Statistic",  main="Equal Spectral Matrices")  abline(h = qchisq(.999, p.dim*2))

### 7.7 Discriminant and Cluster Analysis

The extension of classical pattern-recognition techniques to experimental time series is a problem of great practical interest. A series of observations indexed in time often produces a pattern that may form a basis for discriminating between different classes of events. As an example, consider Fig. 7.2, which shows regional (100-2000 km) recordings of several typical Scandinavian earthquakes and mining explosions measured by stations in Scandinavia. A listing of the events is given in Kakizawa et al. [111]. The problem of discriminating between mining explosions and earthquakes is a reasonable proxy for the problem of discriminating between nuclear explosions and earthquakes. This latter problem is one of critical importance for monitoring a comprehensive test-ban treaty. Time series classification problems are not restricted to geophysical applications, but occur under many and varied circumstances in other fields. Traditionally, the detection of a signal embedded in a noise series has been analyzed in the engineering literature by statistical pattern recognition techniques (see Problems 7.10 and 7.11).

The historical approaches to the problem of discriminating among different classes of time series can be divided into two distinct categories. The _optimality_ approach, as found in the engineering and statistics literature, makes specific Gaussian assumptions about the probability density functions of the separate groups and then develops solutions that satisfy well-defined minimum error criteria. Typically, in the time series case, we might assume the difference between classes is expressed through differences in the theoretical mean and covariance functions and use likelihood methods to develop an optimal classification function. A second class of techniques, which might be described as a _feature extraction_ approach, proceeds more heuristically by looking at quantities that tend to be good visual discriminators for well-separated populations and have some basis in physical theory or intuition. Less attention is paid to finding functions that are approximations to some well-defined optimality criterion.

As in the case of regression, both time domain and frequency domain approaches to discrimination will exist. For relatively short univariate series, a time domain approach that follows conventional multivariate discriminant analysis as described in conventional multivariate texts, such as Anderson [7] or Johnson and Wichern [106]may be preferable. We might even characterize differences by the autocovariance functions generated by different ARMA or state-space models. For longer multivariate time series that can be regarded as stationary after the common mean has been subtracted, the frequency domain approach will be easier computationally because the \(np\) dimensional vector in the time domain, represented here as \(x=(x_{1}^{\prime},x_{t}^{\prime},\ldots,x_{n}^{\prime})^{\prime}\), with \(x_{t}=(x_{t1},\ldots,x_{tp})^{\prime}\), will reduced to separate computations made on the \(p\)-dimensional DFTs. This happens because of the approximate independence of the DFTs, \(X(\omega_{k}),0\leq\omega_{k}\leq 1\), a property that we have often used in preceding chapters.

Finally, the grouping properties of measures like the discrimination information and likelihood-based statistics can be used to develop measures of _disparity_ for clustering multivariate time series. In this section, we define a measure of disparity between two multivariate times series by the spectral matrices of the two processes and then apply hierarchical clustering and partitioning techniques to identify natural groupings within the bivariate earthquake and explosion populations.

### The General Discrimination Problem

The general problem of classifying a vector time series \(x\) occurs in the following way. We observe a time series \(x\) known to belong to one of \(g\) populations, denoted by \(\Pi_{1},\Pi_{2},\ldots,\Pi_{g}\). The general problem is to assign or _classify_ this observation into one of the \(g\) groups in some optimal fashion. An example might be the \(g=2\) populations of earthquakes and explosions shown in Fig. 7.2. We would like to classify the unknown event, shown as NZ in the bottom two panels, as belonging to either the earthquake (\(\Pi_{1}\)) or explosion (\(\Pi_{2}\)) populations. To solve this problem, we need an optimality criterion that leads to a statistic \(T(x)\) that can be used to assign the NZ event to either the earthquake or explosion populations. To measure the success of the classification, we need to evaluate errors that can be expected in the future relating to the number of earthquakes classified as explosions (false alarms) and the number of explosions classified as earthquakes (missed signals).

The problem can be formulated by assuming the observed series \(x\) has a probability density \(p_{i}(x)\) when the observed series is from population \(\Pi_{i}\) for \(i=1,\ldots,g\). Then, partition the space spanned by the \(np\)-dimensional process \(x\) into \(g\) mutually exclusive regions \(R_{1},R_{2},\ldots,R_{g}\) such that, if \(x\) falls in \(R_{i}\), we assign \(x\) to population \(\Pi_{i}\). The _misclassification probability_ is defined as the probability of classifying the observation into population \(\Pi_{j}\) when it belongs to \(\Pi_{i}\), for \(j\neq i\) and would be given by the expression

\[P(j\mid i)=\int_{R_{j}}p_{i}(x)\;dx. \tag{7.102}\]

The overall _total error probability_ depends also on the _prior probabilities_, say, \(\pi_{1},\pi_{2},\ldots,\pi_{g}\), of belonging to one of the \(g\) groups. For example, the probability that an observation \(x\) originates from \(\Pi_{i}\) and is then classified into \(\Pi_{j}\) is obviously \(\pi_{i}P(j\mid i)\), and the total error probability becomes \[P_{e}=\sum_{i=1}^{g}\pi_{i}\sum_{j\neq i}P(j\mid i). \tag{7.103}\]

Although costs have not been incorporated into (7.103), it is easy to do so by multiplying \(P(j\mid i)\) by \(C(j\mid i)\), the cost of assigning a series from population \(\Pi_{i}\) to \(\Pi_{j}\).

The overall error \(P_{e}\) is minimized by classifying \(x\) into \(\Pi_{i}\) if

\[\frac{p_{i}(x)}{p_{j}(x)}>\frac{\pi_{j}}{\pi_{i}} \tag{7.104}\]

for all \(j\neq i\) (see, for example, Anderson [7]). A quantity of interest, from the Bayesian perspective, is the _posterior probability_ an observation belongs to population \(\Pi_{i}\), conditional on observing \(x\), say,

\[P(\Pi_{i}\mid x)=\frac{\pi_{i}p_{i}(x)}{\sum_{j}\pi_{j}(x)p_{j}(x)}. \tag{7.105}\]

The procedure that classifies \(x\) into the population \(\Pi_{i}\) for which the posterior probability is largest is equivalent to that implied by using the criterion (7.104). The posterior probabilities give an intuitive idea of the relative odds of belonging to each of the plausible populations.

Many situations occur, such as in the classification of earthquakes and explosions, in which there are only \(g=2\) populations of interest. For two populations, the _Neyman-Pearson lemma_ implies, in the absence of prior probabilities, classifying an observation into \(\Pi_{1}\) when

\[\frac{p_{1}(x)}{p_{2}(x)}>K \tag{7.106}\]

minimizes each of the error probabilities for a fixed value of the other. The rule is identical to the Bayes rule (7.104) when \(K=\pi_{2}/\pi_{1}\).

The theory given above takes a simple form when the vector \(x\) has a \(p\)-variate normal distribution with mean vectors \(\mu_{j}\) and covariance matrices \(\Sigma_{j}\) under \(\Pi_{j}\) for \(j=1,2,\ldots,g\). In this case, simply use

\[p_{j}(x)=(2\pi)^{-p/2}|\Sigma_{j}|^{-1/2}\exp\biggl{(}-\frac{1}{2}(x-\mu_{j})^ {\prime}\Sigma_{j}^{-1}(x-\mu_{j})\biggr{)}. \tag{7.107}\]

The classification functions are conveniently expressed by quantities that are proportional to the logarithms of the densities, say,

\[g_{j}(x)=-\frac{1}{2}\ \ln|\Sigma_{j}|-\frac{1}{2}\ x^{\prime}\Sigma_{j}^{-1}x+ \mu_{j}^{\prime}\Sigma_{j}^{-1}x-\frac{1}{2}\ \mu_{j}^{\prime}\Sigma_{j}^{-1}\mu_{j}+\ln\pi_{j}. \tag{7.108}\]

In expressions involving the log likelihood, we will generally ignore terms involving the constant \(-\ln 2\pi\). For this case, we may assign an observation \(x\) to population \(\Pi_{i}\) whenever \[g_{i}(x)>g_{j}(x) \tag{7.109}\]

for \(j\neq i,j=1,\ldots,g\) and the posterior probability (7.105) has the form

\[P(\Pi_{i}|x)=\frac{\exp\{g_{i}(x)\}}{\sum_{j}\exp\{g_{j}(x)\}}.\]

A common situation occurring in applications involves classification for \(g=2\) groups under the assumption of multivariate normality and equal covariance matrices; i.e., \(\Sigma_{1}=\Sigma_{2}=\Sigma\). Then, the criterion (7.109) can be expressed in terms of the _linear discriminant function_

\[d_{l}(x) = g_{1}(x)-g_{2}(x) \tag{7.110}\] \[= (\mu_{1}-\mu_{2})^{\prime}\Sigma^{-1}x-\frac{1}{2}(\mu_{1}-\mu_{2 })^{\prime}\Sigma^{-1}(\mu_{1}+\mu_{2})+\ln\frac{\pi_{1}}{\pi_{2}},\]

where we classify into \(\Pi_{1}\) or \(\Pi_{2}\) according to whether \(d_{l}(x)\geq 0\) or \(d_{l}(x)<0\). The linear discriminant function is clearly a combination of normal variables and, for the case \(\pi_{1}=\pi_{2}=.5\), will have mean \(D^{2}/2\) under \(\Pi_{1}\) and mean \(-D^{2}/2\) under \(\Pi_{2}\), with variances given by \(D^{2}\) under both hypotheses, where

\[D^{2}=(\mu_{1}-\mu_{2})^{\prime}\Sigma^{-1}(\mu_{1}-\mu_{2}) \tag{7.111}\]

is the _Mahalanobis distance_ between the mean vectors \(\mu_{1}\) and \(\mu_{2}\). In this case, the two misclassification probabilities (7.1) are

\[P(1|2)=P(2|1)=\Phi\Biggl{(}-\frac{D}{2}\Biggr{)}, \tag{7.112}\]

and the performance is directly related to the Mahalanobis distance (7.111).

For the case in which the covariance matrices cannot be assumed to be the same, the discriminant function takes a different form, with the difference \(g_{1}(x)-g_{2}(x)\) taking the form

\[d_{q}(x) = -\frac{1}{2}\ \ln\frac{|\Sigma_{1}|}{|\Sigma_{2}|}-\frac{1}{2}\ x^{ \prime}(\Sigma_{1}^{-1}-\Sigma_{2}^{-1})x \tag{7.113}\] \[+(\mu_{1}^{\prime}\Sigma_{1}^{-1}-\mu_{2}^{\prime}\Sigma_{2}^{-1 })x+\ln\frac{\pi_{1}}{\pi_{2}}\]

for \(g=2\) groups. This discriminant function differs from the equal covariance case in the linear term and in a nonlinear quadratic term involving the differing covariance matrices. The distribution theory is not tractable for the quadratic case so no convenient expression like (7.112) is available for the error probabilities for the quadratic discriminant function.

A difficulty in applying the above theory to real data is that the group mean vectors \(\mu_{j}\) and covariance matrices \(\Sigma_{j}\) are seldom known. Some engineering problems, such as the detection of a signal in white noise, assume the means and covariance parameters are known exactly, and this can lead to an optimal solution (see Problems 7.14and 7.15). In the classical multivariate situation, it is possible to collect a sample of \(N_{i}\)_training_ vectors from group \(\Pi_{i}\), say, \(x_{ij}\), for \(j=1,\ldots,N_{i}\), and use them to estimate the mean vectors and covariance matrices for each of the groups \(i=1,2,\ldots,g\); i.e., simply choose \(x_{i}\). and

\[S_{i}=(N_{i}-1)^{-1}\sum_{j=1}^{N_{i}}(x_{ij}-x_{i\cdot})(x_{ij}-x_{i\cdot})^{\prime} \tag{7.114}\]

as the estimators for \(\mu_{i}\) and \(\Sigma_{i}\), respectively. In the case in which the covariance matrices are assumed to be equal, simply use the pooled estimator

\[S=\left(\sum_{i}N_{i}-g\right)^{-1}\sum_{i}(N_{i}-1)S_{i}. \tag{7.115}\]

For the case of a linear discriminant function, we may use

\[\hat{g}_{i}(x)=x_{i\cdot}^{\prime}S^{-1}x-\frac{1}{2}\ x_{i\cdot}^{\prime}S^{- 1}x_{i\cdot}+\log\pi_{i} \tag{7.116}\]

as a simple estimator for \(g_{i}(x)\). For large samples, \(x_{i\cdot}\) and \(S\) converge to \(\mu_{i}\) and \(\Sigma\) in probability so \(g_{i}(x)\) converges in distribution to \(g_{i}(x)\) in that case. The procedure works reasonably well for the case in which \(N_{i},i=1,\ldots g\) are large, relative to the length of the series \(n\), a case that is relatively rare in time series analysis. For this reason, we will resort to using spectral approximations for the case in which data are given as long time series.

The performance of sample discriminant functions can be evaluated in several different ways. If the population parameters are known, (7.111) and (7.112) can be evaluated directly. If the parameters are estimated, the estimated Mahalanobis distance \(\hat{D}^{2}\) can be substituted for the theoretical value in very large samples. Another approach is to calculate the _apparent error rates_ using the result of applying the classification procedure to the training samples. If \(n_{ij}\) denotes the number of observations from population \(\Pi_{j}\) classified into \(\Pi_{i}\), the sample error rates can be estimated by the ratio

\[\hat{P}(i\mid j)=\frac{n_{ij}}{\sum_{i}n_{ij}} \tag{7.117}\]

for \(i\neq j\). If the training samples are not large, this procedure may be biased and a resampling option like cross-validation or the bootstrap can be employed. A simple version of cross-validation is the jackknife procedure proposed by Lachenbruch and Mickey [124], which holds out the observation to be classified, deriving the classification function from the remaining observations. Repeating this procedure for each of the members of the training sample and computing (7.117) for the _holdout_ samples leads to better estimators of the error rates.

**Example 7.10**: **Discriminant Analysis Using Amplitudes**

We can give a simple example of applying the above procedures to the logarithms of the amplitudes of the separate P and S components of the original earthquake and explosion traces. The logarithms (base 10) of the maximum peak-to-peak amplitudes of the P and S components, denoted by \(\log_{10}P\) and \(\log_{10}S\), can be considered as two-dimensional feature vectors, say, \(x=(x_{1},x_{2})^{\prime}=(\log_{10}P,\log_{10}S)^{\prime}\), from a bivariate normal population with differing means and covariances. The original data, from Kakizawa et al. [111], are shown in Fig. 7.12. The figure includes the Novaya Zemlya (NZ) event of unknown origin. The tendency of the earthquakes to have higher values for \(\log_{10}S\), relative to \(\log_{10}P\) has been noted by many and the use of the logarithm of the ratio, i.e., \(\log_{10}P-\log_{10}S\) in some references (see Lay [126], pp. 40-41) is a tacit indicator that a linear function of the two parameters will be a useful discriminant.

The sample means \(x_{1}\). \(=(.346,1.024)^{\prime}\) and \(x_{2}\). \(=(.922,.993)^{\prime}\), and covariance matrices

\[S_{1}=\begin{pmatrix}.026&-.007\\ -.007&.010\end{pmatrix}\quad\text{and}\quad S_{2}=\begin{pmatrix}.025&-.001\\ -.001&.010\end{pmatrix}\]

are immediate from (7.114), with the pooled covariance matrix given by

\[S=\begin{pmatrix}.026&-.004\\ -.004&.010\end{pmatrix}\]

from (7.115). Although the covariance matrices are not equal, we try the linear discriminant function anyway, which yields (with equal prior probabilities \(\pi_{1}=\pi_{2}=.5\)) the sample discriminant functions

\[\hat{g}_{1}(x)=30.668x_{1}+111.411x_{2}-62.401\]

Figure 7.12: Classification of earthquakes and explosions based on linear discriminant analysis using the magnitude features\[\hat{g}_{2}(x)=54.048x_{1}+117.255x_{2}-83.142\]

from (7.116), with the estimated linear discriminant function (7.110) as

\[\hat{d}_{l}(x)=-23.380x_{1}-5.843x_{2}+20.740.\]

The jackknifed posterior probabilities of being an earthquake for the earthquake group ranged from.621 to 1.000, whereas the explosion probabilities for the explosion group ranged from.717 to 1.000. The unknown event, NZ, was classified as an explosion, with posterior probability.960.

The R code for this example is as follows.

P = 1:1024; S = P+1024 mag.P = log10(apply(eqexp[P,], 2, max) - apply(eqexp[P,], 2, min)) mag.S = log10(apply(eqexp[S,], 2, max) - apply(eqexp[S,], 2, min)) eq.P = mag.P[1:8]; eq.S = mag.S[1:8] ex.P = mag.P[9:16]; ex.S = mag.S[9:16] NZ.P = mag.P[17]; NZ.S = mag.S[17]
Compute linear discriminant function cov.eq = var(cbind(eq.P, eq.S)) cov.ex = var(cbind(ex.P, ex.S)) cov.pooled = (cov.ex + cov.eq)/2 means.eq = colMeans(cbind(eq.P, eq.S)) means.ex = colMeans(cbind(ex.P, ex.S)) slopes.eq = solve(cov.pooled, means.eq) inter.eq = -sum(slopes.eq*means.eq)/2 slopes.ex = solve(cov.pooled, means.ex) inter.ex = -sum(slopes.ex*means.ex)/2 d.slopes = slopes.eq - slopes.ex d.inter = inter.eq - inter.ex # Classify new observation new.data = cbind(NZ.P, NZ.S) d = sum(d.slopes*new.data) + d.inter post.eq = exp(d)/(1*exp(d)) # Print (disc function, posteriors) and plot results cat(d.slopes[1], "mag.P +", d.slopes[2], "mag.S +", d.inter,"\n") cat("P(EQ|data) =", post.eq, " P(EX|data) =", 1-post.eq, "\n" ) plot(eq.P, eq.S, xlim-c(0,1.5), ylim-c(.75,1.25), xlab="log mag(P)", ylab ="log mag(S)", pch = 8, cex=1.1, lwd=2, main="Classification Based on Magnitude Features") points(ex.P, ex.S, pch = 6, cex=1.1, lwd=2) points(new.data, pch = 3, cex=1.1, lwd=2) abline(a = -d.inter/d.slopes[2], b = -d.slopes[1]/d.slopes[2]) text(eq.P.-07,eq.S+.005, label=names(eqexp[1:8]), cex=.8) text(ex.P.+07,ex.S+.003, label=names(eqexp[9:16]), cex=.8) text(NZ.P.+05,NZ.S+.003, label=names(eqexp[17]), cex=.8) legend("topright",c("EQ","EX","NZ"),pch=c(8,6,3),pt.lwd=2,cex=1.1)
Cross-validation all.data = rhind(cbind(eq.P, eq.S), cbind(ex.P, ex.S)) post.eq < -rep(NA, 8) -> post.ex for(jin 1:16) { if (j <= 8)(samp.eq = all.data[-c(j, 9:16),]  samp.ex = all.data[9:16,]  if (j > 8)(samp.eq = all.data[1:8,]  samp.ex = all.data[-c(j, 1:8),] }

df.eq = nrow(samp.eq)-1; df.ex = nrow(samp.ex)-1  mean.eq = colMeans(samp.eq); mean.ex = colMeans(samp.ex) cov.eq = var(samp.eq); cov.ex = var(samp.ex) cov.pooled = (df.eq^cov.eq + df.ex^cov.ex)/(df.eq + df.ex)  slopes.eq = solve(cov.pooled, mean.eq)  inter.eq = -sum(slopes.eq^mean.eq)/2  slopes.ex = solve(cov.pooled, mean.ex)  inter.ex = -sum(slopes.ex^mean.ex)/2  d.slopes = slopes.eq - slopes.ex  d.inter = inter.eq - inter.ex  d = sum(d.slopes^all.data[j,]) + d.inter  if (j <= 8) post.eq[j] = exp(d)/(1+exp(d))  if (j > 8) post.ex[j-8] = 1/(1+exp(d))  Psterior = cbind(1:8, post.eq, 1:8, post.ex)  colnames(Posterior) = c("E0","(E0|data)","EX","P(EX|data)")  round(Posterior,3) # Results from Cross-validation (not shown)

### Frequency Domain Discrimination

The feature extraction approach often works well for discriminating between classes of univariate or multivariate series when there is a simple low-dimensional vector that seems to capture the essence of the differences between the classes. It still seems sensible, however, to develop optimal methods for classification that exploit the differences between the multivariate means and covariance matrices in the time series case. Such methods can be based on the Whittle approximation to the log likelihood given in Sect. 7.2. In this case, the vector DFTs, say, \(X(\omega_{k})\), are assumed to be approximately normal, with means \(M_{j}(\omega_{k})\) and spectral matrices \(f_{j}(\omega_{k})\) for population \(\Pi_{j}\) at frequencies \(\omega_{k}=k/n\), for \(k=0,1,\ldots[n/2]\), and are approximately uncorrelated at different frequencies, say, \(\omega_{k}\) and \(\omega_{\ell}\) for \(k\neq\ell\). Then, writing the complex normal densities as in Sect. 7.2 leads to a criterion similar to (7.108); namely,

\[\begin{split} g_{j}(X)&=\ln\pi_{j}-\sum_{0<\omega_{k }<1/2}\biggl{[}\ln|f_{j}(\omega_{k})|+X^{*}(\omega_{k})f_{j}^{-1}(\omega_{k})X (\omega_{k})\\ &\quad-2M_{j}^{*}(\omega_{k})f_{j}^{-1}(\omega_{k})X(\omega_{k})+ M_{j}^{*}(k)f_{j}^{-1}(\omega_{k})M_{j}(\omega_{k})\biggr{]},\end{split} \tag{7.118}\]

where the sum goes over frequencies for which \(|f_{j}(\omega_{k})|\neq 0\). The periodicity of the spectral density matrix and DFT allows adding over \(0<k<1/2\). The classification rule is as in (7.109).

In the time series case, it is more likely the discriminant analysis involves assuming the covariance matrices are different and the means are equal. For example, the tests, shown in Fig. 7.11, imply, for the earthquakes and explosions, the primary differences are in the bivariate spectral matrices and the means are essentially the same. For this case, it will be convenient to write the Whittle approximation to the log likelihood in the form

\[\ln p_{j}(X)=\sum_{0<\omega_{k}<1/2}\biggl{[}-\ln|f_{j}(\omega_{k})|-X^{*}( \omega_{k})f_{j}^{-1}(\omega_{k})X(\omega_{k})\biggr{]}, \tag{7.119}\]where we have omitted the prior probabilities from the equation. The quadratic detector in this case can be written in the form

\[\ln p_{j}(X)=\sum_{0<\omega_{k}<1/2}\left[-\ln|f_{j}(\omega_{k})|-\operatorname{ tr}\bigl{\{}I(\omega_{k})f_{j}^{-1}(\omega_{k})\bigr{\}}\right], \tag{7.120}\]

where

\[I(\omega_{k})=X(\omega_{k})X^{*}(\omega_{k}) \tag{7.121}\]

denotes the _periodogram matrix_. For equal prior probabilities, we may assign an observation \(x\) into population \(\Pi_{i}\) whenever

\[\ln p_{i}(X)>\ln p_{j}(X) \tag{7.122}\]

for \(j\neq i,j=1,2,\ldots,g\).

Numerous authors have considered various versions of discriminant analysis in the frequency domain. Shumway and Unger [179] considered (7.118) for \(p=1\) and equal covariance matrices, so the criterion reduces to a simple linear one. They apply the criterion to discriminating between earthquakes and explosions using teleseismic P wave data in which the means over the two groups might be considered as fixed. Alagon [4] and Dargahi-Noubary and Laycock [47] considered discriminant functions of the form (7.118) in the univariate case when the means are zero and the spectra for the two groups are different. Taniguchi et al. [197] adopted (7.119) as a criterion and discussed its _non-Gaussian robustness_. Shumway [180] reviews general discriminant functions in both the univariate and multivariate time series cases.

### Measures of Disparity

Before proceeding to examples of discriminant and cluster analysis, it is useful to consider the relation to the Kullback-Leibler (K-L) _discrimination information_, as defined in Problem 2.4. Using the spectral approximation and noting the periodogram matrix has the approximate expectation

\[E_{j}I(\omega_{k})=f_{j}(\omega_{k})\]

under the assumption that the data come from population \(\Pi_{j}\), and approximating the ratio of the densities by

\[\ln\frac{p_{1}(X)}{p_{2}(X)}=\sum_{0<\omega_{k}<1/2}\left[-\ln\frac{|f_{1}( \omega_{k})|}{|f_{2}(\omega_{k})|}-\operatorname{tr}\biggl{\{}\bigl{(}f_{2}^{- 1}(\omega_{k})-f_{1}^{-1}(\omega_{k})\bigr{)}I(\omega_{k})\biggr{\}}\right],\]

we may write the approximate discrimination information as

\[I(f_{1};f_{2}) = \frac{1}{n}\ E_{1}\ln\frac{p_{1}(X)}{p_{2}(X)} \tag{7.123}\] \[= \frac{1}{n}\sum_{0<\omega_{k}<1/2}\left[\operatorname{tr}\left\{ f_{1}(\omega_{k})f_{2}^{-1}(\omega_{k})\right\}-\ln\frac{|f_{1}(\omega_{k})|}{|f_{2}( \omega_{k})|}-p\right].\]The approximation may be carefully justified by noting the multivariate normal time series \(x=(x_{1}^{\prime},x_{2}^{\prime}\,\ldots,x_{n}^{\prime})\) with zero means and \(np\times np\) stationary covariance matrices \(\Gamma_{1}\) and \(\Gamma_{2}\) will have \(p\), \(n\times n\) blocks, with elements of the form \(\gamma_{ij}^{(I)}(s-t)\), \(s,t=1,\ldots,n,i,j=1,\ldots,p\) for population \(\Pi_{\ell},\ell=1,2\). The discrimination information, under these conditions, becomes

\[I(1;2:x)=\frac{1}{n}\;E_{1}\ln\frac{p_{1}(x)}{p_{2}(x)}=\frac{1}{n}\left[\mbox {tr}\left\{\Gamma_{1}\Gamma_{2}^{-1}\right\}-\ln\frac{|\Gamma_{1}|}{|\Gamma_{ 2}|}-np\right]\,. \tag{7.124}\]

The limiting result

\[\lim_{n\to\infty}I(1;2:x)=\frac{1}{2}\int_{-1/2}^{1/2}\!\left[\mbox{tr}\{f_{1} (\omega)f_{2}^{-1}(\omega)\}-\ln\frac{|f_{1}(\omega)|}{|f_{2}(\omega)|}-p \right]\,d\omega\]

has been shown, in various forms, by Pinsker [155], Hannan [86], and Kazakos and Papantoni-Kazakos [116]. The discrete version of (7.123) is just the approximation to the integral of the limiting form. The K-L measure of disparity is not a true distance, but it can be shown that \(I(1;2)\geq 0\), with equality if and only if \(f_{1}(\omega)=f_{2}(\omega)\) almost everywhere. This result makes it potentially suitable as a measure of disparity between the two densities.

A connection exists, of course, between the discrimination information number, which is just the expectation of the likelihood criterion and the likelihood itself. For example, we may measure the disparity between the sample and the process defined by the theoretical spectrum \(f_{j}(\omega_{k})\) corresponding to population \(\Pi_{j}\) in the sense of Kullback [123], as \(I(\hat{f};f_{j})\), where

\[\hat{f}(\omega_{k})=\sum_{\ell=-m}^{m}h_{\ell}I(\omega_{k}+\ell/n) \tag{7.125}\]

denotes the smoothed spectral matrix with weights \(\{h_{\ell}\}\). The likelihood ratio criterion can be thought of as measuring the disparity between the periodogram and the theoretical spectrum for each of the populations. To make the discrimination information finite, we replace the periodogram implied by the log likelihood by the sample spectrum. In this case, the classification procedure can be regarded as finding the population closest, in the sense of minimizing disparity between the sample and theoretical spectral matrices. The classification in this case proceeds by simply choosing the population \(\Pi_{j}\) that minimizes \(I(\hat{f};f_{j})\), i.e., assigning \(x\) to population \(\Pi_{i}\) whenever

\[I(\hat{f};f_{i})<I(\hat{f};f_{j}) \tag{7.126}\]

for \(j\neq i,j=1,2,\ldots,g\).

Kakizawa et al. [111] proposed using the _Chernoff (CH) information measure_ (Chernoff [41], Renyi [165]), defined as

\[B_{\alpha}(1;2)=-\ln E_{2}\Bigg{\{}\Bigg{(}\frac{p_{2}(x)}{p_{1}(x)}\Bigg{)}^ {\alpha}\Bigg{\}}, \tag{7.127}\]where the measure is indexed by a _regularizing parameter_\(\alpha\), for \(0<\alpha<1\). When \(\alpha=.5\), the Chernoff measure is the _symmetric divergence_ proposed by Bhattacharya [21]. For the multivariate normal case,

\[B_{\alpha}(1;2:x)=\frac{1}{n}\left[\ln\frac{|\alpha\Gamma_{1}+(1-\alpha)\Gamma_ {2}|}{|\Gamma_{2}|}-\alpha\ln\frac{|\Gamma_{1}|}{|\Gamma_{2}|}\right]. \tag{7.128}\]

The large sample spectral approximation to the Chernoff information measure is analogous to that for the discrimination information, namely,

\[\begin{split} B_{\alpha}(f_{1};f_{2})=\frac{1}{2n}\sum_{0<\omega_ {k}<1/2}&\left[\ln\frac{|\alpha f_{1}(\omega_{k})+(1-\alpha)f_{2} (\omega_{k})|}{|f_{2}(\omega_{k})|}\right.\\ &\left.-\alpha\ln\frac{|f_{1}(\omega_{k})|}{|f_{2}(\omega_{k})|} \right].\end{split} \tag{7.129}\]

The Chernoff measure, when divided by \(\alpha(1-\alpha)\), behaves like the discrimination information in the limit in the sense that it converges to \(I(1;2:x)\) for \(\alpha\to 0\) and to \(I(2;1:x)\) for \(\alpha\to 1\). Hence, near the boundaries of the parameter \(\alpha\), it tends to behave like discrimination information and for other values represents a compromise between the two information measures. The classification rule for the Chernoff measure reduces to assigning \(x\) to population \(\Pi_{i}\) whenever

\[B_{\alpha}(\hat{f};f_{i})<B_{\alpha}(\hat{f};f_{j}) \tag{7.130}\]

for \(j\neq i,j=1,2,\ldots,g\).

Although the classification rules above are well defined if the group spectral matrices are known, this will not be the case in general. If there are \(g\) training samples, \(x_{ij},j=1,\ldots,N_{i},i=1\ldots,g\), with \(N_{i}\) vector observations available in each group, the natural estimator for the spectral matrix of the group \(i\) is just the average spectral matrix (7.97), namely, with \(\hat{f}_{ij}(\omega_{k})\) denoting the estimated spectral matrix of series \(j\) from the \(i\)th population,

\[\hat{f}_{i}(\omega_{k})=\frac{1}{N_{i}}\sum_{j=1}^{N_{i}}\hat{f}_{ij}(\omega_{k }). \tag{7.131}\]

A second consideration is the choice of the regularization parameter \(\alpha\) for the Chernoff criterion, (7.129). For the case of \(g=2\) groups, it should be chosen to maximize the disparity between the two group spectra, as defined in (7.129). Kakizawa et al. [111] simply plot (7.129) as a function of \(\alpha\), using the estimated group spectra in (7.131), choosing the value that gives the maximum disparity between the two groups.

**Example 7.11**: **Discriminant Analysis on Seismic Data**__

The simplest approaches to discriminating between the earthquake and explosion groups have been based on either the relative amplitudes of the P and S phases, as in Fig. 7.5 or on relative power components in various frequency bands. Considerable effort has been expended on using various spectral ratios involving the bivariate P and S phases as discrimination features. Kakizawa et al. [111] mention a number of measures that have be used in the seismological literature as features. These features include ratios of power for the two phases and ratios of power components in high- and low-frequency bands. The use of such features of the spectrum suggests an optimal procedure based on discriminating between the spectral matrices of two stationary processes would be reasonable. The fact that the hypothesis that the spectral matrices were equal, tested in Example 7.9, was also soundly rejected suggests the use of a discriminant function based on spectral differences. Recall the sampling rate is 40 points per second, leading to a folding frequency of 20 Hz.

Figure 13 displays the diagonal elements of the average spectral matrices for each group. The maximum value of the estimated Chernoff disparity \(B_{\alpha}(\hat{f_{1}};\hat{f_{2}})\) occurs for \(\alpha=.4\), and we use that value in the discriminant criterion (7.129). Figure 14 shows the results of using the Chernoff differences along with the Kullback-Leibler differences for classification. The differences are the measures for earthquakes minus explosions, so negative values of the differences indicate earthquake and positive values indicate explosion. Hence, points in the first quadrant of Fig. 14 are classified an explosion and points in the third quadrant are classified as earthquakes. We note that Explosion 6 is misclassified as an earthquake. Also, Earthquake 1, which falls in the fourth quadrant has an uncertain classification, the Chernoff distance classifies it as an earthquake, however, the Kullback-Leibler difference classifies it as an explosion.

The NZ event of unknown origin was also classified using these distance measures, and, as in Example 7.10, it is classified as an explosion. The Russians have asserted no mine blasting or nuclear testing occurred in the area in question, so the event remains as somewhat of a mystery. The fact that it was relatively removed

Figure 13: Average P-spectra and S-spectra of the earthquake and explosion series. The _insert_ on the _upper right_ shows the smoothing kernel used; the resulting bandwidth is about.75 Hz

geographically from the test set may also have introduced some uncertainties into the procedure. The R code for this example is as follows.

 P = 1:1024; S = P+1024; p.dim = 2; n =1024  eq = as.ts(eqexp[, 1:8])  ex = as.ts(eqexp[, 9:16])  nz = as.ts(eqexp[, 17])  f.eq <- array(dim=c(8, 2, 2, 512)) -> f.ex  f.NZ = array(dim=c(2, 2, 512))  # below calculates determinant for 2x2 Hermitian matrix  det.c <- function(mat){return(Re(mat[1,1]*mat[2,2]-mat[1,2]*mat[2,1]))}  L = c(15,13,5) # for smoothing  for (i in 1:8){  # compute spectral matrices  f.eq[1,..] = mvspec(cbind(eq[P,i], eq[S,i]), spans=L, taper=.5)$fxx  f.ex[i,..] = mvspec(cbind(ex[P,i], ex[S,i]), spans=L, taper=.5)$fxx}  u = mvspec(cbind(nz[P], nz[S]), spans=L, taper=.5)  f.NZ = u$fxx  hndwidth = u$bandwidth*sqrt(12)*40 # about.75 Hz  flat.eq = apply(f.eq, 2:4, mean)  # average spectra  flat.ex = apply(f.ex, 2:4, mean)  # plot the average spectra  par(mfrow=c(2,2), mar=c(3,3,2,1), mpp = c(1.6,..6,0))  Fr = 40*(1:512)/m  plot(Fr,Re(flat.eq[1,1,]),type="1",xlab="Frequency (Hz)",yllab="")  plot(Fr,Re(flat.eq[2,2,]),type="1",xlab="Frequency (Hz)",yllab="")  plot(Fr,Re(flat.ex[1,1,]),type="1",xlab="Frequency (Hz)",yllab="")  plot(Fr,Re(flat.ex[2,2,]),type="1",xlab="Frequency (Hz)",yllab="")

Figure 14: Classification (by quadrant) of earthquakes and explosions using the Chernoff and Kullback-Leibler differences

mtext("Average P-spectra", side=3, line=-1.5, adj=.2, outer=TRUE)  mtext("Earthquakes", side=2, line=-1, adj=.8, outer=TRUE)  mtext("Average S-spectra", side=3, line=-1.5, adj=.82, outer=TRUE)  mtext("Explosions", side=2, line=-1, adj=.2, outer=TRUE)  par(fig = c(.75, 1,.75, 1), new = TRUE)  ker = kernel("modified.daniell", L)$coef; ker = c(rev(ker),ker[-1])  plot((-33:33)/40, ker, type="l", ylab="", xlab="", cex.axis=.7,  yaxp=c(0.04,2))  # Choose alpha  alpha = rep(0,19)  for (i in 1:19){ alf=i/20  for (k in 1:256){  Balpha[i]= Balpha[i] + Re(log(det.c(alf*flat.ex[,.,k] +  (1-alf)*flat.eq[,.,k])/det.c(flat.eq[,.,k])) -  alf*log(det.c(flat.ex[,.,k])/det.c(flat.eq[,.,k])))} }  alf = which.max(Balpha)/20 # alpha =.4  # Calculate Information Criteria  rep(0,17) -> KLDiff -> BDiff -> Kleq -> KLex -> Beq -> Bex  for (i in 1:17){  if (i <= 8) f0 = f.eq[i,.,]  if (i > 8 & i <= 16) f0 = f.ex[i-8,..]  if (i == 17) f0 = f.NZ  for (k in 1:256){ # only use freqs out to.25  tr = Re(sum(diag(solve(flat.eq[,.,k],f0[,.,k]))))  KLeq[i] = Kleq[i] + tr + log(det.c(flat.eq[,.,k])) - log(det.c(f0[,.,k]))  Seq[i] = Seq[i] +  Re(log(det.c(alf*f0[,.,k]+(1-alf)*flat.eq[,.,k])/det.c(flat.eq[,.,k]))  - alf*log(det.c(f0[,.,k])/det.c(flat.eq[,.,k])))  tr = Re(sum(diag(solve(flat.ex[,.,k],f0[,.,k]))))  KLex[i] = KLex[i] + tr + log(det.c(flat.ex[,.,k])) - log(det.c(f0[,.,k]))  Box[i] = Bez[i] +  Re(log(det.c(alf*f0[,.,k]+(1-alf)*flat.ex[,.,k])/det.c(flat.ex[,.,k])))  #1%[log(det.c(f0[,.,k])/det.c(flat.ex[,.,k]))) }  KLDiff[i] = (Kleq[i] - KLex[i])/n  RDiff[i] = (Seq[i] - Ber[i])/(2^n)  x.b = max(KLDiff)+.1; x.a = min(KLDiff)-.1  y.b = max(BDiff)+.01; y.a = min(BDiff)-.01  dev.new()  plot(KLDiff[9:16], BDiff[9:16], type="p", xlim=c(x.a,x.b), ylim=c(y.a,y.b),  cex=1.1,lwd=2, xlab="Kullback-Leibler Difference",ylab="Chernoff  Difference", main="Classification Based on Chernoff and K-L  Distances", pch=6)  points(KLDiff[1:8], BDiff[1:8], pch=8, cex=1.1, lwd=2)  points(KLDiff[17], BDiff[17], pch=3, cex=1.1, lwd=2)  legend("topleft", legend=c("EQ", "EX", "NZ"), pch=c(8,6,3), pt.lwd=2)  abline(h=0, v=0, lty=2, col="gray")  text(KLDiff[-c(1,2,3,7,14)]-.075, BDiff[-c(1,2,3,7,14)],  label=names(eqexp[-c(1,2,3,7,14)]), cex=.7)  text(KLDiff[c(1,2,3,7,14)]+.075, BDiff[c(1,2,3,7,14)],  label=names(eqexp[c(1,2,3,7,14)]), cex=.7) Cluster Analysis

For the purpose of clustering, it may be more useful to consider a _symmetric disparity measures_ and we introduce the _J-Divergence_ measure

\[J(f_{1};f_{2})=I(f_{1};f_{2})+I(f_{2};f_{1}) \tag{7.132}\]and the symmetric Chernoff number

\[JB_{\alpha}(f_{1};f_{2})=B_{\alpha}(f_{1};f_{2})+B_{\alpha}(f_{2};f_{1}) \tag{7.133}\]

for that purpose. In this case, we define the disparity between the sample spectral matrix of a single vector, \(x\), and the population \(\Pi_{j}\) as

\[J(\hat{f};f_{j})=I(\hat{f};f_{j})+I(f_{j};\hat{f}) \tag{7.134}\]

and

\[JB_{\alpha}(\hat{f};f_{j})=B_{\alpha}(\hat{f};f_{j})+B_{\alpha}(f_{j};\hat{f}), \tag{7.135}\]

respectively and use these as quasi-distances between the vector and population \(\Pi_{j}\).

The measures of disparity can be used to cluster multivariate time series. The symmetric measures of disparity, as defined above ensure that the disparity between \(f_{i}\) and \(f_{j}\) is the same as the disparity between \(f_{j}\) and \(f_{i}\). Hence, we will consider the symmetric forms (7.134) and (7.135) as quasi-distances for the purpose of defining a distance matrix for input into one of the standard clustering procedures (see Johnson and Wichern [106]). In general, we may consider either _hierarchical_ or _partitioned_ clustering methods using the quasi-distance matrix as an input.

For purposes of illustration, we may use the symmetric divergence (7.134), which implies the quasi-distance between sample series with estimated spectral matrices \(\hat{f}_{i}\) and \(\hat{f}_{j}\) would be (7.134); i.e.,

\[J(\hat{f}_{i};\hat{f}_{j})=\frac{1}{n}\sum_{0<\omega_{k}<1/2}\left[\text{tr} \big{\{}\hat{f}_{i}(\omega_{k})\hat{f}_{j}^{-1}(\omega_{k})\big{\}}+\text{tr} \big{\{}\hat{f}_{j}(\omega_{k})\hat{f}_{i}^{-1}(\omega_{k})\big{\}}-2p\right], \tag{7.136}\]

for \(i\neq j\). We can also use the comparable form for the Chernoff divergence, but we may not want to make an assumption for the regularization parameter \(\alpha\).

For hierarchical clustering, we begin by clustering the two members of the population that minimize the disparity measure (7.136). Then, these two items form a cluster, and we can compute distances between unclustered items as before. The distance between unclustered items and a current cluster is defined here as the average of the distances to elements in the cluster. Again, we combine objects that are closest together. We may also compute the distance between the unclustered items and clustered items as the closest distance, rather than the average. Once a series is in a cluster, it stays there. At each stage, we have a fixed number of clusters, depending on the merging stage.

Alternatively, we may think of clustering as a partitioning of the sample into a prespecified number of groups. MacQueen [132] has proposed this using _k-means clustering_, using the Mahalonobis distance between an observation and the group mean vectors. At each stage, a reassignment of an observation into its closest affinity group is possible. To see how this procedure applies in the current context, consider a preliminary partition into a fixed number of groups and define the disparity between the spectral matrix of the observation, say, \(\hat{f}\), and the average spectral matrix of the group, say, \(\hat{f}_{i}\), as \(J(\hat{f};\hat{f}_{i})\), where the group spectral matrix can be estimated by (7.131). At any pass, a single series is reassigned to the group for which its disparity is minimized. The reassignment procedure is repeated until all observations stay in their current groups. Of course, the number of groups must be specified for each repetition of the partitioning algorithm and a starting partition must be chosen. This assignment can either be random or chosen from a preliminary hierarchical clustering, as described above.

**Example 7.12**: **Cluster Analysis for Earthquakes and Explosions**

It is instructive to try a clustering procedure on the population of known earthquakes and explosions. Figure 7.15 shows the results of applying the Partitioning Around Medoids (PAM) clustering algorithm, which is essentially a robustification of the k-means procedure (see Kaufman and Rousseeuw [114], Ch. 2), under the assumption that two groups are appropriate. The two-group partition tends to produce a final partition that agrees closely with the known configuration with earthquake 1 (EQ1) and explosion 8 (EX8) being misclassified; as in previous examples, the NZ event is classified as an explosion.

The R code for this example uses the cluster package and our mvspec script for estimating spectral matrices.

 library(cluster) P = 1:1024; S = P+1024; p.dim = 2; n =1024  eq = as.ts(eqexp[, 1:8])  ex = as.ts(eqexp[, 9:16])

Figure 7.15: Clustering results for the earthquake and explosion series based on symmetric divergence using a robust version of _k_-means clustering with two groups. _Circles_ indicate Group I classification, triangles indicate Group II classification

mz = as.ts(eqexp[, 17])  f = array(dim=c(17, 2, 2, 512))  L = c(15, 15)  # for smoothing  for (i in 1:8){  # compute spectral matrices  f[i,,.,] = mvspec(cbind(eq[P,i], eq[S,i]), spans=L, taper=.5)$fxx  f[i+8,,.,] = mvspec(cbind(ex[P,i], ex[S,i]), spans=L, taper=.5)$fxx }  f[17,,.,] = mvspec(cbind(nz[P], nz[S]), spans=L, taper=.5)$fxx  JD = matrix(0, 17, 17)  # Calculate Symmetric Information Criteria  for (i in 1:16){  for (jin (i+1):17){  for (kin 1:256){  # only use frogs out to.25  tr1 = Re(sum(diag(solve(f[i,,.,k], f[j,,.,k]))))  tr2 = Re(sum(diag(solve(f[j,,.,k], f[i,,.,k])))))  JD[i,j] = JD[i,j] + (tr1 + tr2 - 2*p.dim))}}  JD = (DJ + t(D))/n  colnames(JD) = c(colnames(eq), colnames(ex), "NZ")  rownames(JD) = colnames(JD)  cluster.2 = pam(JD, k = 2, diss = TRUE)  summary(cluster.2) # print results  par(mpp = c(1.6,,0), cex=3/4, cex.lab=4/3, cex.main=4/3)  clusplot(JD, cluster.2$cluster, col.clus=1, labels=3, lines=0, col.p=1,  main="Clustering Results for Explosions and Earthquakes")  text(-7,-.5, "Group I", cex=1.1, font=2)  text(1, 5, "Group II", cex=1.1, font=2)

### 7.8 Principal Components and Factor Analysis

In this section, we introduce the related topics of spectral domain principal components analysis and factor analysis for time series. The topics of principal components and canonical analysis in the frequency domain are rigorously presented in Brillinger (1981, Chaps. 9 and 10) and many of the details concerning these concepts can be found there.

The techniques presented here are related to each other in that they focus on extracting pertinent information from spectral matrices. This information is important because dealing directly with a high-dimensional spectral matrix \(f(\omega)\) itself is somewhat cumbersome because it is a function into the set of complex, nonnegative-definite, Hermitian matrices. We can view these techniques as easily understood, parsimonious tools for exploring the behavior of vector-valued time series in the frequency domain with minimal loss of information. Because our focus is on spectral matrices, we assume for convenience that the time series of interest have zero means; the techniques are easily adjusted in the case of nonzero means.

In this and subsequent sections, it will be convenient to work occasionally with _complex-valued time series_. A \(p\times 1\) complex-valued time series can be represented as \(x_{t}=x_{1t}-ix_{2t}\), where \(x_{1t}\) is the real part and \(x_{2t}\) is the imaginary part of \(x_{t}\). The process is said to be stationary if \(\mathrm{E}(x_{t})\) and \(\mathrm{E}(x_{t+h}x_{t}^{*})\) exist and are independent of time \(t\). The \(p\times p\) autocovariance function,

\[\Gamma_{xx}(h)=\mathrm{E}(x_{t+h}x_{t}^{*})-\mathrm{E}(x_{t+h})\mathrm{E}(x_{t} ^{*}),\]of \(x_{t}\) satisfies conditions similar to those of the real-valued case. Writing \(\Gamma_{xx}(h)=\{\gamma_{ij}(h)\}\), for \(i,j=1,\ldots,p\), we have (1) \(\gamma_{ii}(0)\geq 0\) is real, (2) \(|\gamma_{ij}(h)|^{2}\leq\gamma_{ii}(0)\gamma_{jj}(0)\) for all integers \(h\), and (3) \(\Gamma_{xx}(h)\) is a non-negative definite function. The spectral theory of complex-valued vector time series is analogous to the real-valued case. For example, if \(\sum_{h}||\Gamma_{xx}(h)||<\infty\), the spectral density matrix of the complex series \(x_{t}\) is given by

\[f_{xx}(\omega)=\sum_{h=-\infty}^{\infty}\Gamma_{xx}(h)\exp(-2\pi ih\omega).\]

Principal ComponentsClassical principal component analysis (PCA) is concerned with explaining the variance-covariance structure among \(p\) variables, \(x=(x_{1},\ldots,x_{p})^{\prime}\), through a few linear combinations of the components of \(x\). Suppose we wish to find a linear combination

\[y=c^{\prime}x=c_{1}x_{1}+\cdots+c_{p}x_{p} \tag{7.137}\]

of the components of \(x\) such that \(\mbox{var}(y)\) is as large as possible. Because \(\mbox{var}(y)\) can be increased by simply multiplying \(c\) by a constant, it is common to restrict \(c\) to be of unit length; that is, \(c^{\prime}c=1\). Noting that \(\mbox{var}(y)=c^{\prime}\Sigma_{xx}c\), where \(\Sigma_{xx}\) is the \(p\times p\) variance-covariance matrix of \(x\), another way of stating the problem is to find \(c\) such that

\[\max_{c\neq 0}\frac{c^{\prime}\Sigma_{xx}c}{c^{\prime}c}. \tag{7.138}\]

Denote the _eigenvalue-eigenvector pairs_ of \(\Sigma_{xx}\) by \(\{(\lambda_{1},e_{1}),\ldots,(\lambda_{p},e_{p})\}\), where \(\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{p}\geq 0\), and the eigenvectors are of unit length. The solution to (7.138) is to choose \(c=e_{1}\), in which case the linear combination \(y_{1}=e_{1}^{\prime}x\) has maximum variance, \(\mbox{var}(y_{1})=\lambda_{1}\). In other words,

\[\max_{c\neq 0}\frac{c^{\prime}\Sigma_{xx}c}{c^{\prime}c}=\frac{e_{1}^{\prime} \Sigma_{xx}e_{1}}{e_{1}^{\prime}e_{1}}=\lambda_{1}. \tag{7.139}\]

The linear combination, \(y_{1}=e_{1}^{\prime}x\), is called the _first principal component_. Because the eigenvalues of \(\Sigma_{xx}\) are not necessarily unique, the first principal component is not necessarily unique.

The _second principal component_ is defined to be the linear combination \(y_{2}=c^{\prime}x\) that maximizes \(\mbox{var}(y_{2})\) subject to \(c^{\prime}c=1\) and such that \(\mbox{cov}(y_{1},y_{2})=0\). The solution is to choose \(c=e_{2}\), in which case, \(\mbox{var}(y_{2})=\lambda_{2}\). In general, the \(k\)th principal component, for \(k=1,2,\ldots,p\), is the linear combination \(y_{k}=c^{\prime}x\) that maximizes \(\mbox{var}(y_{k})\) subject to \(c^{\prime}c=1\) and such that \(\mbox{cov}(y_{k},y_{j})=0\), for \(j=1,2,\ldots,k-1\). The solution is to choose \(c=e_{k}\), in which case \(\mbox{var}(y_{k})=\lambda_{k}\).

One measure of the importance of a principal component is to assess the proportion of the total variance attributed to that principal component. The _total variance_ of \(x\) is defined to be the sum of the variances of the individual components; that is, \(\mbox{var}(x_{1})+\cdots+\mbox{var}(x_{p})=\sigma_{11}+\cdots+\sigma_{pp}\), where \(\sigma_{jj}\) is the \(j\)th diagonal element of \(\Sigma_{xx}\). This sum is also denoted as \(\mbox{tr}(\Sigma_{xx})\), or the _trace_ of \(\Sigma_{xx}\). Because\(\operatorname{tr}(\Sigma_{xx})=\lambda_{1}+\dots+\lambda_{p}\), the _proportion of the total variance attributed to the \(k\)th principal component_ is given simply by \(\operatorname{var}(y_{k})\ \big{/}\ \operatorname{tr}(\Sigma_{xx})=\lambda_{k}\ \big{/}\ \sum_{j=1}^{p}\lambda_{j}\).

Given a random sample \(x_{1},\dots,x_{n}\), the _sample principal components_ are defined as above, but with \(\Sigma_{xx}\) replaced by the sample variance-covariance matrix, \(S_{xx}=(n-1)^{-1}\sum_{i=1}^{n}(x_{i}-\bar{x})(x_{i}-\bar{x})^{\prime}\). Further details can be found in the introduction to classical principal component analysis in Johnson and Wichern [106], Chap. 9.

For the case of time series, suppose we have a zero mean, \(p\times 1\), stationary vector process \(x_{t}\) that has a \(p\times p\) spectral density matrix given by \(f_{xx}(\omega)\). Recall \(f_{xx}(\omega)\) is a complex-valued, nonnegative-definite, Hermitian matrix. Using the analogy of classical principal components, and in particular (7.137) and (7.138), suppose, for a fixed value of \(\omega\), we want to find a complex-valued univariate process \(y_{t}(\omega)=c(\omega)^{*}x_{t}\), where \(c(\omega)\) is complex, such that the spectral density of \(y_{t}(\omega)\) is maximized at frequency \(\omega\), and \(c(\omega)\) is of unit length, \(c(\omega)^{*}c(\omega)=1\). Because, at frequency \(\omega\), the spectral density of \(y_{t}(\omega)\) is \(f_{y}(\omega)=c(\omega)^{*}f_{xx}(\omega)c(\omega)\), the problem can be restated as: Find complex vector \(c(\omega)\) such that

\[\max_{c(\omega)\neq 0}\frac{c(\omega)^{*}f_{xx}(\omega)c(\omega)}{c(\omega)^{*} c(\omega)}. \tag{7.140}\]

Let \(\{(\lambda_{1}(\omega),e_{1}(\omega))\,\dots,\ (\lambda_{p}(\omega),e_{p}(\omega))\}\) denote the eigenvalue-eigenvector pairs of \(f_{xx}(\omega)\), where \(\lambda_{1}(\omega)\geq\lambda_{2}(\omega)\geq\dots\geq\lambda_{p}(\omega)\geq 0\), and the eigenvectors are of unit length. We note that the eigenvalues of a Hermitian matrix are real. The solution to (7.140) is to choose \(c(\omega)=e_{1}(\omega)\); in which case the desired linear combination is \(y_{t}(\omega)=e_{1}(\omega)^{*}x_{t}\). For this choice,

\[\max_{c(\omega)\neq 0}\frac{c(\omega)^{*}f_{xx}(\omega)c(\omega)}{c(\omega)^{*} c(\omega)}=\frac{e_{1}(\omega)^{*}f_{x}(\omega)e_{1}(\omega)}{e_{1}(\omega)^{*} e_{1}(\omega)}=\lambda_{1}(\omega). \tag{7.141}\]

This process may be repeated for any frequency \(\omega\), and the complex-valued process, \(y_{t1}(\omega)=e_{1}(\omega)^{*}x_{t}\), is called the _first principal component at frequency \(\omega\)_. The \(k\)th principal component at frequency \(\omega\), for \(k=1,2,\dots,p\), is the complex-valued time series \(y_{tk}(\omega)=e_{k}(\omega)^{*}x_{t}\), in analogy to the classical case. In this case, the spectral density of \(y_{tk}(\omega)\) at frequency \(\omega\) is \(f_{y_{k}}(\omega)=e_{k}(\omega)^{*}f_{xx}(\omega)e_{k}(\omega)=\lambda_{k}(\omega)\).

The previous development of spectral domain principal components is related to the _spectral envelope_ methodology first discussed in Stoffer et al. [193]. We will present the spectral envelope in the next section, where we motivate the use of principal components as it is presented above. Another way to motivate the use of principal components in the frequency domain was given in Brillinger (1981, Chap. 9). Although this technique leads to the same analysis, the motivation may be more satisfactory to the reader at this point. In this case, we suppose we have a stationary, \(p\)-dimensional, vector-valued process \(x_{t}\) and we are only able to keep a univariate process \(y_{t}\) such that, when needed, we may reconstruct the vector-valued process, \(x_{t}\), according to an optimality criterion.

Specifically, we suppose we want to approximate a mean-zero, stationary, vector-valued time series, \(x_{t}\), with spectral matrix \(f_{xx}(\omega)\), by a univariate process \(y_{t}\) defined by \[y_{t}=\sum_{j=-\infty}^{\infty}c_{t-j}^{*}x_{j}, \tag{7.142}\]

where \(\{c_{j}\}\) is a \(p\times 1\) vector-valued filter, such that \(\{c_{j}\}\) is absolutely summable; that is, \(\sum_{j=-\infty}^{\infty}|c_{j}|<\infty\). The approximation is accomplished so the reconstruction of \(x_{t}\) from \(y_{t}\), say,

\[\hat{x}_{t}=\sum_{j=-\infty}^{\infty}b_{t-j}y_{j}, \tag{7.143}\]

where \(\{b_{j}\}\) is an absolutely summable \(p\times 1\) filter, is such that the mean square approximation error

\[\mathrm{E}\{(x_{t}-\hat{x}_{t})^{*}(x_{t}-\hat{x}_{t})\} \tag{7.144}\]

is minimized.

Let \(b(\omega)\) and \(c(\omega)\) be the transforms of \(\{b_{j}\}\) and \(\{c_{j}\}\), respectively. For example,

\[c(\omega)=\sum_{j=-\infty}^{\infty}c_{j}\exp(-2\pi ij\omega), \tag{7.145}\]

and, consequently,

\[c_{j}=\int_{-1/2}^{1/2}c(\omega)\exp(2\pi ij\omega)d\omega. \tag{7.146}\]

Brillinger (35, Theorem 9.3.1) shows the solution to the problem is to choose \(c(\omega)\) to satisfy (7.140) and to set \(b(\omega)=\overline{c(\omega)}\). This is precisely the previous problem, with the solution given by (7.141). That is, we choose \(c(\omega)=e_{1}(\omega)\) and \(b(\omega)=\overline{e_{1}(\omega)}\); the filter values can be obtained via the inversion formula given by (7.146). Using these results, in view of (7.142), we may form the _first principal component series_, say \(y_{t1}\).

This technique may be extended by requesting another series, say, \(y_{t2}\), for approximating \(x_{t}\) with respect to minimum mean square error, but where the coherency between \(y_{t2}\) and \(y_{t1}\) is zero. In this case, we choose \(c(\omega)=e_{2}(\omega)\). Continuing this way, we can obtain the first \(q\leq p\) principal components series, say, \(y_{t}=(y_{t1},\ldots,y_{tq})^{\prime}\), having spectral density \(f_{q}(\omega)=\mathrm{diag}\{\lambda_{1}(\omega),\ldots,\lambda_{q}(\omega)\}\). The series \(y_{tk}\) is the \(k\)th principal component series.

As in the classical case, given observations, \(x_{1},x_{2},\ldots,x_{n}\), from the process \(x_{t}\), we can form an estimate \(\hat{f}_{xx}(\omega)\) of \(f_{xx}(\omega)\) and define the _sample principal component series_ by replacing \(f_{xx}(\omega)\) with \(\hat{f}_{xx}(\omega)\) in the previous discussion. Precise details pertaining to the asymptotic (\(n\to\infty\)) behavior of the principal component series and their spectra can be found in Brillinger (1981, Chapter 9). To give a basic idea of what we can expect, we focus on the first principal component series and on the spectral estimator obtained by smoothing the periodogram matrix, \(I_{n}(\omega_{j})\); that is

\[\hat{f}_{xx}(\omega_{j})=\sum_{\ell=-m}^{m}h_{\ell}I_{n}(\omega_{j}+\ell/n), \tag{7.147}\]where \(L=2m+1\) is odd and the weights are chosen so \(h_{\ell}=h_{-\ell}\) are positive and \(\sum_{\ell}h_{\ell}=1\). Under the conditions for which \(\hat{f}_{xx}(\omega_{j})\) is a well-behaved estimator of \(f_{xx}(\omega_{j})\), and for which the largest eigenvalue of \(f_{xx}(\omega_{j})\) is unique,

\[\left\{\eta_{n}\frac{\hat{\lambda}_{1}(\omega_{j})-\lambda_{1}(\omega_{j})}{ \lambda_{1}(\omega_{j})};\;\eta_{n}\left[\hat{e}_{1}(\omega_{j})-e_{1}(\omega_ {j})\right]\;;\;j=1,\ldots,J\right\} \tag{7.148}\]

converges (\(n\to\infty\)) jointly in distribution to independent, zero-mean normal distributions, the first of which is standard normal. In (7.148), \(\eta_{n}^{-2}=\sum_{\ell=-m}^{m}h_{\ell}^{2}\), noting we must have \(L\to\infty\) and \(\eta_{n}\to\infty\), but \(L/n\to 0\) as \(n\to\infty\). The asymptotic variance-covariance matrix of \(\hat{e}_{1}(\omega)\), say, \(\Sigma_{e_{1}}(\omega)\), is given by

\[\Sigma_{e_{1}}(\omega)=\eta_{n}^{-2}\lambda_{1}(\omega)\sum_{\ell=2}^{p}\lambda _{\ell}(\omega)\left\{\lambda_{1}(\omega)-\lambda_{\ell}(\omega)\right\}^{-2} e_{\ell}(\omega)e_{\ell}^{*}(\omega). \tag{7.149}\]

The distribution of \(\hat{e}_{1}(\omega)\) depends on the other latent roots and vectors of \(f_{x}(\omega)\). Writing \(\hat{e}_{1}(\omega)=(\hat{e}_{11}(\omega),\hat{e}_{12}(\omega),\ldots,\hat{e}_ {1p}(\omega))^{\prime}\), we may use this result to form confidence regions for the components of \(\hat{e}_{1}\) by approximating the distribution of

\[\frac{2\left|\hat{e}_{1,j}(\omega)-e_{1,j}(\omega)\right|^{2}}{s_{j}^{2}( \omega)}, \tag{7.150}\]

for \(j=1,\ldots,p\), by a \(\chi^{2}\) distribution with two degrees of freedom. In (7.150), \(s_{j}^{2}(\omega)\) is the \(j\)th diagonal element of \(\hat{\Sigma}_{e_{1}}(\omega)\), the estimate of \(\Sigma_{e_{1}}(\omega)\). We can use (7.150) to check whether the value of zero is in the confidence region by comparing \(2|\hat{e}_{1,j}(\omega)|^{2}/s_{j}^{2}(\omega)\) with \(\chi^{2}_{2}(1-\alpha)\), the \(1-\alpha\) upper tail cutoff of the \(\chi^{2}_{2}\) distribution.

Figure 7.16: The individual periodograms of \(x_{t\,k}\), for \(k=1,\ldots,8\), in Example 7.13

**Example 7.13**: **Principal Component Analysis of the fMRI Data**

Recall Example 1.6 where the vector time series \(x_{t}=(x_{t}1,\ldots,x_{t}8)^{\prime},t=1,\ldots,128\), represents consecutive measures of average blood oxygenation level dependent (bold) signal intensity, which measures areas of activation in the brain. Recall subjects were given a non-painful brush on the hand and the stimulus was applied for 32 s and then stopped for 32 s; thus, the signal period is 64 s (the sampling rate was one observation every 2 s for 256 s). The series \(x_{t}k\) for \(k=1,2,3,4\) represent locations in cortex, series \(x_{t}5\) and \(x_{t}6\) represent locations in the thalamus, and \(x_{t}7\) and \(x_{t}8\) represent locations in the cerebellum.

As is evident from Fig. 1.6, different areas of the brain are responding differently, and a principal component analysis may help in indicating which locations are responding with the most spectral power, and which locations do not contribute to the spectral power at the stimulus signal period. In this analysis, we will focus primarily on the signal period of 64 s, which translates to four cycles in 256 s or \(\omega=4/128\) cycles per time point.

Figure 16 shows individual periodograms of the series \(x_{t}k\) for \(k=1,\ldots,8\). As was evident from Fig. 1.6, a strong response to the brush stimulus occurred in areas of the cortex. To estimate the spectral density of \(x_{t}\), we used (7.147) with \(L=5\) and \(\{h_{0}=3/9,h_{\pm 1}=2/9,h_{\pm 2}=1/9\}\); this is a Daniell kernel with \(m=1\) passed twice. Calling the estimated spectrum \(\hat{f}_{xx}(j/128)\), for \(j=0,1,\ldots,64\), we can obtain the estimated spectrum of the first principal component series \(y_{t}1\) by calculating the largest eigenvalue, \(\hat{\lambda}_{1}(j/128)\), of \(\hat{f}_{xx}(j/128)\) for each \(j=0,1,\ldots,64\). The result, \(\hat{\lambda}_{1}(j/128)\), is shown in Fig. 7.17. As expected, there is a large peak at the stimulus frequency \(4/128\), wherein \(\hat{\lambda}_{1}(4/128)=2\). The total power at the stimulus frequency is \(\text{tr}(\hat{f}_{xx}(4/128))=2.05\), so the proportion of the power at frequency \(4/128\) attributed to the first principal component series is about \(2/2.05\) or roughly 98%. Because the first principal component explains nearly all of the total power at the stimulus frequency, there is no need to explore the other principal component series at this frequency.

Figure 17: The estimated spectral density, \(\hat{\lambda}_{1}(j/128)\), of the first principal component series in Example 7.13The estimated first principal component series at frequency \(4/128\) is given by \(\hat{y}_{t}(14/128)=\hat{e}_{t}^{*}(4/128)x_{t}\), and the components of \(\hat{e}_{1}(4/128)\) can give insight as to which locations of the brain are responding to the brush stimulus. Table 7.5 shows the magnitudes of \(\hat{e}_{1}(4/128)\). In addition, an approximate 99% confidence interval was obtained for each component using (7.150). As expected, the analysis indicates that location 6 is not contributing to the power at this frequency, but surprisingly, the analysis suggests location 5 (cerebellum 1) is responding to the stimulus.

The R code for this example is as follows.

 n = 128; Per = abs(mwfft(fmri1[,-1]))*2/n  par(mfrow=c(2,4), mar=c(3,2,2,1), mgp = c(1.6,6,0), oma=c(0,1,0,0))  for (i in 1:8){ plot(0:20, Per[1:21,i], type="1", ylim=c(0,8),  main-columns(fmri1)[i+1], xlab="Cycles",ylab=", xapp=c(0,20,5))}  mtext("Periodogram", side=2, line=-.3, outer-TRUE, adj=c(.2,.8))  dev.new()  fxx = mvspec(fmri1[,-1], kernel("daniell", c(1,1)), taper=.5, plot=FALSE)$fxx  l.val = rep(NA,64)  for (kin 1:64) {  u = eigen(fxx[,.k], symmetric=TRUE, only.values = TRUE)  l.val[k] = uSvalues[1]] # largest e-value  plot(l.val, type="n", xart="n", xlab="Cycles (Frequency x 128)", ylab="First  Principal Component")  axis(1, seq(4,60,by=8)); grid(lty=2, nx-NA, ny=NULL)  abline(v=seq(4,60,by=8), col='lightgray', lty=2); lines(l.val)  # At freq 4/128  u = eigen(fxx[,.4], symmetric=TRUE)  lam=uSvalues; evec=uSvectors  lam[1]/sum(lam)  # % of variance explained  sig.el = matrix(0,8,8)  for (l in 2:5){ # last 3 evs are 0  sig.el = sig.el + lam[1]*evec[,1]%*Cwon(tsev[,1]))/(lam[1]-lam[1])^2}  sig.el = Re(sig.el)^lam[1]*sum(kernel("daniell", c(1,1))$coef^2)  p.val = round(pchisq(2*abs(evec[,1])^2/diag(sig.el), 2, lower.tail=FALSE), 3)  chbind(colnames(fmri1)[-1], abs(evec[,1]), p.val) # table values

### Factor Analysis

Classical factor analysis is similar to classical principal component analysis. Suppose \(x\) is a mean-zero, \(p\times 1\), random vector with variance-covariance matrix \(\Sigma_{xx}\). The factor model proposes that \(x\) is dependent on a few unobserved common factors, \(z_{1},\ldots,z_{q}\), plus error. In this model, one hopes that \(q\) will be much smaller than \(p\). The _factor model_ is given by

\[x=\mathcal{B}z+\epsilon, \tag{7.151}\]

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Location & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ \hline \(\hat{e}_{1}(\frac{4}{128})\) &.64 &.36 &.36 &.22 &.32 &.05* &.13 &.39 \\ \hline \end{tabular}
\end{table}
Table 7.5: Magnitudes of the PC vector at the stimulus frequency where \(\mathcal{B}\) is a \(p\times q\) matrix of _factor loadings_, \(z=(z_{1},\ldots,z_{q})^{\prime}\) is a random \(q\times 1\) vector of _factors_ such that \(E(z)=0\) and \(E(zz^{\prime})=I_{q}\), the \(q\times q\) identity matrix. The \(p\times 1\) unobserved error vector \(\epsilon\) is assumed to be independent of the factors, with zero mean and diagonal variance-covariance matrix \(D=\operatorname{diag}\{\delta_{1}^{2},\ldots,\delta_{p}^{2}\}\). Note, (7.151) differs from the multivariate regression model in Sect. 5.6 because the factors, \(z\), are unobserved. Equivalently, the factor model, (7.151), can be written in terms of the covariance structure of \(x\),

\[\Sigma_{xx}=\mathcal{B}\mathcal{B}^{\prime}+D; \tag{7.152}\]

i.e., the variance-covariance matrix of \(x\) is the sum of a symmetric, nonnegative-definite rank \(q\leq p\) matrix and a nonnegative-definite diagonal matrix. If \(q=p\), then \(\Sigma_{xx}\) can be reproduced exactly as \(\mathcal{B}\mathcal{B}^{\prime}\), using the fact that \(\Sigma_{xx}=\lambda_{1}e_{1}e_{1}^{\prime}+\cdots+\lambda_{p}e_{p}e_{p}^{ \prime}\), where \((\lambda_{i},e_{i})\) are the eigenvalue-eigenvector pairs of \(\Sigma_{xx}\). As previously indicated, however, we hope \(q\) will be much smaller than \(p\). Unfortunately, most covariance matrices cannot be factored as (7.152) when \(q\) is much smaller than \(p\).

To motivate factor analysis, suppose the components of \(x\) can be grouped into meaningful groups. Within each group, the components are highly correlated, but the correlation between variables that are not in the same group is small. A group is supposedly formed by a single construct, represented as an unobservable factor, responsible for the high correlations within a group. For example, a person competing in a decathlon performs \(p=10\) athletic events, and we may represent the outcome of the decathlon as a \(10\times 1\) vector of scores. The events in a decathlon involve running, jumping, or throwing, and it is conceivable the \(10\times 1\) vector of scores might be able to be factored into \(q=4\) factors, (1) arm strength, (2) leg strength, (3) running speed, and (4) running endurance. The model (7.151) specifies that \(\operatorname{cov}(x,z)=\mathcal{B}\), or \(\operatorname{cov}(x_{i},z_{j})=b_{ij}\) where \(b_{ij}\) is the \(ij\)th component of the _factor loading matrix_\(\mathcal{B}\), for \(i=1,\ldots,p\) and \(j=1,\ldots,q\). Thus, the elements of \(\mathcal{B}\) are used to identify which hypothetical factors the components of \(x\) belong to, or load on.

At this point, some ambiguity is still associated with the factor model. Let \(Q\) be a \(q\times q\) orthogonal matrix; that is \(Q^{\prime}Q=QQ^{\prime}=I_{q}\). Let \(\mathcal{B}_{*}=\mathcal{B}Q\) and \(z_{*}=Q^{\prime}z\) so (7.151) can be written as

\[x=\mathcal{B}z+\epsilon=\mathcal{B}QQ^{\prime}z+\epsilon=\mathcal{B}_{*}z_{*} +\epsilon. \tag{7.153}\]

The model in terms of \(\mathcal{B}_{*}\) and \(z_{*}\) fulfills all of the factor model requirements, for example, \(\operatorname{cov}(z_{*})=Q^{\prime}\operatorname{cov}(z)Q=QQ^{\prime}=I_{q}\), so

\[\Sigma_{xx}=\mathcal{B}_{*}\operatorname{cov}(z_{*})\mathcal{B}_{*}^{\prime} +D=\mathcal{B}QQ^{\prime}\mathcal{B}^{\prime}+D=\mathcal{B}\mathcal{B}^{ \prime}+D. \tag{7.154}\]

Hence, on the basis of observations on \(x\), we cannot distinguish between the loadings \(\mathcal{B}\) and the rotated loadings \(\mathcal{B}_{*}=\mathcal{B}Q\). Typically, \(Q\) is chosen so the matrix \(\mathcal{B}\) is easy to interpret, and this is the basis of what is called _factor rotation_.

Given a sample \(x_{1},\ldots,x_{n}\), a number of methods are used to estimate the parameters of the factor model, and we discuss two of them here. The first method is the _principal component method_. Let \(S_{xx}\) denote the sample variance-covariance matrix, and let \((\lambda_{i},\hat{e}_{i})\) be the eigenvalue-eigenvector pairs of \(S_{xx}\). The \(p\times q\) matrix of estimated factor loadings is found by setting \[\hat{\mathcal{B}}=\Big{[}\hat{\lambda}_{1}^{1/2}\;\hat{e}_{1}\;\Big{|}\;\hat{ \lambda}_{2}^{1/2}\;\hat{e}_{2}\;\Big{|}\cdots\Big{|}\;\hat{\lambda}_{q}^{1/2} \;\hat{e}_{q}\Big{]}. \tag{7.155}\]

The argument here is that if \(q\) factors exist, then

\[S_{xx}\approx\hat{\lambda}_{1}\hat{e}_{1}\hat{e}_{1}^{\prime}+\cdots+\hat{ \lambda}_{q}\hat{e}_{q}\hat{e}_{q}^{\prime}=\hat{\mathcal{B}}\hat{\mathcal{B}} ^{\prime}, \tag{7.156}\]

because the remaining eigenvalues, \(\hat{\lambda}_{q+1},\ldots,\hat{\lambda}_{p}\), will be negligible. The estimated diagonal matrix of error variances is then obtained by setting \(\hat{D}=\text{diag}\{\delta_{1}^{2},\ldots,\delta_{p}^{2}\}\), where \(\hat{\delta}_{j}^{2}\) is the \(j\)th diagonal element of \(S_{xx}-\hat{\mathcal{B}}\hat{\mathcal{B}}^{\prime}\).

The second method, which can give answers that are considerably different from the principal component method is maximum likelihood. Upon further assumption that in (7.151), \(z\) and \(\epsilon\) are multivariate normal, the log likelihood of \(\mathcal{B}\) and \(D\) ignoring a constant is

\[-2\ln L(\mathcal{B},D)=n\ln|\Sigma_{xx}|+\sum_{j=1}^{n}x_{j}^{\prime}\Sigma_{ xx}^{-1}x_{j}. \tag{7.157}\]

The likelihood depends on \(\mathcal{B}\) and \(D\) through (7.152), \(\Sigma_{xx}=\mathcal{B}\mathcal{B}^{\prime}+D\). As discussed in (7.153)-(7.154), the likelihood is not well defined because \(\mathcal{B}\) can be rotated. Typically, restricting \(\mathcal{B}D^{-1}\mathcal{B}^{\prime}\) to be a diagonal matrix is a computationally convenient uniqueness condition. The actual maximization of the likelihood is accomplished using numerical methods.

One obvious method of performing maximum likelihood for the Gaussian factor model is the EM algorithm. For example, suppose the factor vector \(z\) is known. Then, the factor model is simply the multivariate regression model given in Sect. 5.6, that is, write \(X^{\prime}=[x_{1},x_{2},\ldots,x_{n}]\) and \(Z^{\prime}=[z_{1},z_{2},\ldots,z_{n}]\), and note that \(X\) is \(n\times p\) and \(Z\) is \(n\times q\). Then, the MLE of \(\mathcal{B}\) is

\[\hat{\mathcal{B}}=X^{\prime}Z(Z^{\prime}Z)^{-1}=\Big{(}n^{-1}\sum_{j=1}^{n}x_ {j}z_{j}^{\prime}\Big{)}\Big{(}n^{-1}\sum_{j=1}^{n}z_{j}z_{j}^{\prime}\Big{)} ^{-1}\overset{\text{def}}{=}C_{xz}C_{zz}^{-1} \tag{7.158}\]

and the MLE of \(D\) is

\[\hat{D}=\text{diag}\Big{\{}n^{-1}\sum_{j=1}^{n}\Big{(}x_{j}-\hat{\mathcal{B}} z_{j}\Big{)}\;\Big{(}x_{j}-\hat{\mathcal{B}}z_{j}\Big{)}^{\prime}\Big{\}}; \tag{7.159}\]

that is, only the diagonal elements of the right-hand side of (7.159) are used. The bracketed quantity in (7.159) reduces to

\[C_{xx}-C_{xz}C_{zz}^{-1}C_{xz}^{\prime}, \tag{7.160}\]

where \(C_{xx}=n^{-1}\sum_{j=1}^{n}x_{j}x_{j}^{\prime}\).

Based on the derivation of the EM algorithm for the state-space model, (4.66)-(4.75), we conclude that, to employ the EM algorithm here, given the current parameter estimates, in \(C_{xz}\), we replace \(x_{j}z_{j}^{\prime}\) by \(x_{j}\tilde{z}_{j}^{\prime}\), where \(\tilde{z}_{j}=E(z_{j}\;|\;x_{j})\), and in \(C_{zz}\), we replace \(z_{j}z_{j}^{\prime}\) by \(P_{z}+\tilde{z}_{j}\tilde{z}_{j}^{\prime}\), where \(P_{z}=\text{var}(z_{j}\;\big{|}\;x_{j})\). Using the fact that the \((p+q)\times 1\) vector \((x_{j}^{\prime},z_{j}^{\prime})^{\prime}\) is multivariate normal with mean-zero, and variance-covariance matrix given by \[\begin{pmatrix}\mathcal{B}\mathcal{B}^{\prime}+D&\mathcal{B}\\ \mathcal{B}^{\prime}&I_{q}\end{pmatrix}, \tag{7.161}\]

we have

\[\tilde{z}_{j}\equiv E(z_{j}\bigm{|}x_{j})=\mathcal{B}^{\prime}(\mathcal{B}^{ \prime}\mathcal{B}+D)^{-1}x_{j} \tag{7.162}\]

and

\[P_{z}\equiv\text{var}(z_{j}\bigm{|}x_{j})=I_{q}-\mathcal{B}^{\prime}(\mathcal{ B}^{\prime}\mathcal{B}+D)^{-1}\mathcal{B}. \tag{7.163}\]

For time series, suppose \(x_{t}\) is a stationary \(p\times 1\) process with \(p\times p\) spectral matrix \(f_{xx}(\omega)\). Analogous to the classical model displayed in (7.152), we may postulate that at a given frequency of interest, \(\omega\), the spectral matrix of \(x_{t}\) satisfies

\[f_{xx}(\omega)=\mathcal{B}(\omega)\mathcal{B}(\omega)^{*}+D(\omega), \tag{7.164}\]

where \(\mathcal{B}(\omega)\) is a complex-valued \(p\times q\) matrix with \(\text{rank}\left(\mathcal{B}(\omega)\right)=q\leq p\) and \(D(\omega)\) is a real, nonnegative-definite, diagonal matrix. Typically, we expect \(q\) will be much smaller than \(p\).

As an example of a model that gives rise to (7.164), let \(x_{t}=(x_{t1},\ldots,x_{tp})^{\prime}\), and suppose

\[x_{tj}=c_{j}s_{t-\tau_{j}}+\epsilon_{tj},\quad j=1,\ldots,p, \tag{7.165}\]

where \(c_{j}\geq 0\) are individual amplitudes and \(s_{t}\) is a common unobserved signal (factor) with spectral density \(f_{ss}(\omega)\). The values \(\tau_{j}\) are the individual phase shifts. Assume \(s_{t}\) is independent of \(\epsilon_{t}=(\epsilon_{t1},\ldots,\epsilon_{tp})^{\prime}\) and the spectral matrix of \(\epsilon_{t}\), \(D_{\epsilon\epsilon}(\omega)\), is diagonal. The DFT of \(x_{tj}\) is given by

\[X_{j}(\omega)=n^{-1/2}\sum_{t=1}^{n}x_{tj}\exp(-2\pi it\omega)\]

and, in terms of the model (7.165),

\[X_{j}(\omega)=a_{j}(\omega)X_{s}(\omega)+X_{\epsilon_{j}}(\omega), \tag{7.166}\]

where \(a_{j}(\omega)=c_{j}\exp(-2\pi i\tau_{j}\omega)\), and \(X_{s}(\omega)\) and \(X_{\epsilon_{j}}(\omega)\) are the respective DFTs of the signal \(s_{t}\) and the noise \(\epsilon_{tj}\). Stacking the individual elements of (7.166), we obtain a complex version of the classical factor model with one factor,

\[\begin{pmatrix}X_{1}(\omega)\\ \vdots\\ X_{p}(\omega)\end{pmatrix}=\begin{pmatrix}a_{1}(\omega)\\ \vdots\\ a_{p}(\omega)\end{pmatrix}X_{s}(\omega)+\begin{pmatrix}X_{\epsilon_{1}}(\omega )\\ \vdots\\ X_{\epsilon_{p}}(\omega)\end{pmatrix},\]

or more succinctly,

\[X(\omega)=a(\omega)X_{s}(\omega)+X_{\epsilon}(\omega). \tag{7.167}\]

From (7.167), we can identify the spectral components of the model; that is,

\[f_{xx}(\omega)=b(\omega)b(\omega)^{*}+D_{\epsilon\epsilon}(\omega), \tag{7.168}\]where \(b(\omega)\) is a \(p\times 1\) complex-valued vector, \(b(\omega)b(\omega)^{*}=a(\omega)f_{ss}(\omega)a(\omega)^{*}\). Model (7.168) could be considered the one-factor model for time series. This model can be extended to more than one factor by adding other independent signals into the original model (7.165). More details regarding this and related models can be found in Stoffer [194].

**Example 7.14**: **Single Factor Analysis of the fMRI Data**

The fMRI data analyzed in Example 7.13 is well suited for a single factor analysis using the model (7.165), or, equivalently, the complex-valued, single factor model (7.167). In terms of (7.165), we can think of the signal \(s_{t}\) as representing the brush stimulus signal. As before, the frequency of interest is \(\omega=4/128\), which corresponds to a period of 32 time points, or 64 s.

A simple way to estimate the components \(b(\omega)\) and \(D_{\epsilon\epsilon}(\omega)\), as specified in (7.168), is to use the principal components method. Let \(\hat{f}_{xx}(\omega)\) denote the estimate of the spectral density of \(x_{t}=(x_{t1},\ldots,x_{t8})^{\prime}\) obtained in Example 7.13. Then, analogous to (7.155) and (7.156), we set

\[\hat{b}(\omega)=\sqrt{\hat{\lambda}_{1}(\omega)}\;\hat{e}_{1}(\omega),\]

where \(\left(\hat{\lambda}_{1}(\omega),\hat{e}_{1}(\omega)\right)\) is the first eigenvalue-eigenvector pair of \(\hat{f}_{xx}(\omega)\). The diagonal elements of \(\hat{D}_{\epsilon\epsilon}(\omega)\) are obtained from the diagonal elements of \(\hat{f}_{xx}(\omega)-\hat{b}(\omega)\hat{b}(\omega)^{*}\). The appropriateness of the model can be assessed by checking the elements of the residual matrix, \(\hat{f}_{xx}(\omega)-[\hat{b}(\omega)\hat{b}(\omega)^{*}+\hat{D}_{\epsilon \epsilon}(\omega)]\), are negligible in magnitude.

Concentrating on the stimulus frequency, recall \(\hat{\lambda}_{1}(4/128)=2\). The magnitudes of \(\hat{e}_{1}(4/128)\) are displayed in Table 7.5, indicating all locations load on the stimulus factor except for location 6, and location 7 could be considered borderline. The diagonal elements of \(\hat{f}_{xx}(\omega)-\hat{b}(\omega)\hat{b}(\omega)^{*}\) yield

\[\hat{D}_{\epsilon\epsilon}(4/128)=0.001\times\text{diag}\{1.36,2.04,6.22,11.3 0,0.73,13.26,6.93,5.88\}.\]

The magnitudes of the elements of the residual matrix at \(\omega=4/128\) are

\[0.001\times\left[\begin{array}{ccccccc}0.00&1.73&3.88&3.61&0.88&2.04&1.60&2. 81\\ 2.41&0.00&1.17&3.77&1.49&5.58&3.68&4.21\\ 8.49&5.34&0.00&2.94&7.58&10.91&8.36&10.64\\ 12.65&11.84&6.12&0.00&12.56&14.64&13.34&16.10\\ 0.32&0.29&2.10&2.01&0.00&1.18&2.01&1.18\\ 10.34&16.69&17.09&15.94&13.49&0.00&5.78&14.74\\ 5.71&8.51&8.94&10.18&7.56&0.97&0.00&8.66\\ 6.25&8.00&10.31&10.69&5.95&8.69&7.64&0.00\end{array}\right],\]

indicating the model fit is good. Assuming the results of the previous example are available, use the following R code.

bhat = sqrt(lam[1])*evec[,1] Dhat = Re(diag(fxx[,.4] - bhat%*Conj(t(bhat)))) res = Mod(fxx[,.4] - Dhat - bhat%*Conj(t(bhat)))A number of authors have considered factor analysis in the spectral domain, for example Priestley et al. [157]; Priestley and Subba Rao [158]; Geweke [70], and Geweke and Singleton [71], to mention a few. An obvious extension of simple model (7.165) is the factor model

\[x_{t}=\sum_{j=-\infty}^{\infty}\Lambda_{j}s_{t-j}+\epsilon_{t}, \tag{7.169}\]

where \(\{\Lambda_{j}\}\) is a real-valued \(p\times q\) filter, \(s_{t}\) is a \(q\times 1\) stationary, unobserved signal, with independent components, and \(\epsilon_{t}\) is white noise. We assume the signal and noise process are independent, \(s_{t}\) has \(q\times q\) real, diagonal spectral matrix \(f_{ss}(\omega)=\mathrm{diag}\{f_{s1}(\omega),\ldots,f_{sq}(\omega)\}\), and \(\epsilon_{t}\) has a real, diagonal, \(p\times p\) spectral matrix given by \(D_{\epsilon\epsilon}(\omega)=\mathrm{diag}\{f_{\epsilon 1}(\omega),\ldots,f_{ \epsilon p}(\omega)\}\). If, in addition, \(\sum||\Lambda_{j}||<\infty\), the spectral matrix of \(x_{t}\) can be written as

\[f_{xx}(\omega)=\Lambda(\omega)f_{ss}(\omega)\Lambda(\omega)^{*}+D_{\epsilon \epsilon}(\omega)=\mathcal{B}(\omega)\mathcal{B}(\omega)^{*}+D_{\epsilon \epsilon}(\omega), \tag{7.170}\]

where

\[\Lambda(\omega)=\sum_{t=-\infty}^{\infty}\Lambda_{t}\,\exp(-2\pi it\omega) \tag{7.171}\]

and \(\mathcal{B}(\omega)=\Lambda(\omega)f_{ss}^{1/2}(\omega)\). Thus, by (7.170), the model (7.169) is seen to satisfy the basic requirement of the spectral domain factor analysis model; that is, the \(p\times p\) spectral density matrix of the process of interest, \(f_{xx}(\omega)\), is the sum of a rank \(q\leq p\) matrix, \(\mathcal{B}(\omega)\mathcal{B}(\omega)^{*}\), and a real, diagonal matrix, \(D_{\epsilon\epsilon}(\omega)\). For the purpose of identifiability we set \(f_{ss}(\omega)=I_{q}\) for all \(\omega\); in which case, \(\mathcal{B}(\omega)=\Lambda(\omega)\). As in the classical case [see (7.154)], the model is specified only up to rotations; for details, see Bloomfield and Davis [26].

Parameter estimation for the model (7.169), or equivalently (7.170), can be accomplished using the principal component method. Let \(\hat{f}_{xx}(\omega)\) be an estimate of \(f_{xx}(\omega)\), and let \((\hat{\lambda}_{j}(\omega),\hat{e}_{j}(\omega))\), for \(j=1,\ldots,p\), be the eigenvalue-eigenvector pairs, in the usual order, of \(\hat{f}_{xx}(\omega)\). Then, as in the classical case, the \(p\times q\) matrix \(\mathcal{B}\) is estimated by

\[\hat{\mathcal{B}}(\omega)=\left[\hat{\lambda}_{1}(\omega)^{1/2}\ \hat{e}_{1}(\omega)\ \middle|\ \hat{\lambda}_{2}(\omega)^{1/2}\ \hat{e}_{2}(\omega)\ \middle|\ \cdots\ \middle|\ \hat{\lambda}_{q}(\omega)^{1/2}\ \hat{e}_{q}(\omega)\right]. \tag{7.172}\]

The estimated diagonal spectral density matrix of errors is then obtained by setting \(\hat{D}_{\epsilon\epsilon}(\omega)=\mathrm{diag}\{\hat{f}_{\epsilon 1}(\omega),\ldots,\hat{f}_{\epsilon p}(\omega)\}\), where \(\hat{f}_{\epsilon j}(\omega)\) is the \(j\)th diagonal element of \(\hat{f}_{xx}(\omega)-\hat{\mathcal{B}}(\omega)\hat{\mathcal{B}}(\omega)^{*}\).

Alternatively, we can estimate the parameters by approximate likelihood methods. As in (7.167), let \(X(\omega_{j})\) denote the DFT of the data \(x_{1},\ldots,x_{n}\) at frequency \(\omega_{j}=j/n\). Similarly, let \(X_{s}(\omega_{j})\) and \(X_{\epsilon}(\omega_{j})\) be the DFTs of the signal and of the noise processes, respectively. Then, under certain conditions (see Pawitan and Shumway [150]), for \(\ell=0,\pm 1,\ldots,\pm m\),

\[X(\omega_{j}+\ell/n)=\Lambda(\omega_{j})X_{s}(\omega_{j}+\ell/n)+X_{\epsilon}( \omega_{j}+\ell/n)+\omega_{as}(n^{-\alpha}), \tag{7.173}\]where \(\Lambda(\omega_{j})\) is given by (7.171) and \(\mathsf{o}_{as}(n^{-\alpha})\to 0\) almost surely for some \(0\leq\alpha<1/2\) as \(n\to\infty\). In (7.173), the \(X(\omega_{j}+\ell/n)\) are the DFTs of the data at the \(L\) odd frequencies \(\{\omega_{j}+\ell/n;\ \ell=0,\pm 1,\ldots,\pm m\}\) surrounding the central frequency of interest \(\omega_{j}=j/n\).

Under appropriate conditions \(\{X(\omega_{j}+\ell/n);\ \ell=0,\pm 1,\ldots,\pm m\}\) in (7.173) are approximately (\(n\to\infty\)) independent, complex Gaussian random vectors with variance-covariance matrix \(f_{xx}(\omega_{j})\). The approximate likelihood is given by

\[-2\ln L\left(\mathcal{B}(\omega_{j}),D_{\epsilon\epsilon}(\omega_ {j})\right)\] \[\qquad=n\ln\bigl{|}f_{xx}(\omega_{j})\bigr{|}+\sum_{\ell=-m}^{m}X^ {*}(\omega_{j}+\ell/n)f_{xx}^{-1}(\omega_{j})X(\omega_{j}+\ell/n), \tag{7.174}\]

with the constraint \(f_{xx}(\omega_{j})=\mathcal{B}(\omega_{j})\mathcal{B}(\omega_{j})^{*}+D_{ \epsilon\epsilon}(\omega_{j})\). As in the classical case, we can use various numerical methods to maximize \(L\bigl{(}\mathcal{B}(\omega_{j}),D_{\epsilon\epsilon}(\omega_{j})\bigr{)}\) at every frequency, \(\omega_{j}\), of interest. For example, the EM algorithm discussed for the classical case, (7.158)-(7.163), can easily be extended to this case.

Assuming \(f_{ss}(\omega)=I_{q}\), the estimate of \(\mathcal{B}(\omega_{j})\) is also the estimate of \(\Lambda(\omega_{j})\). Calling this estimate \(\hat{\Lambda}(\omega_{j})\), the time domain filter can be estimated by

\[\hat{\Lambda}_{t}^{M}=M^{-1}\sum_{j=0}^{M-1}\hat{\Lambda}(\omega_{j})\exp(2\pi ij t/n), \tag{7.175}\]

Figure 7.18: The seasonally adjusted, quarterly growth rate (as percentages) of five macroeconomic series, unemployment, GNP, consumption, government investment, and private investment in the United States between 1948 and 1988, \(n=160\) values

for some \(0<M\leq n\), which is the discrete and finite version of the inversion formula given by

\[\Lambda_{t}=\int_{-1/2}^{1/2}\Lambda(\omega)\exp(2\pi i\omega t)d\omega. \tag{7.176}\]

Note that we have used this approximation earlier in Chap. 4, (4.124), for estimating the time response of a frequency response function defined over a finite number of frequencies.

**Example 7.15**: **Government Spending, Private Investment, and Unemployment**

Figure 7.18 shows the seasonally adjusted, quarterly growth rate (as percentages) of five macroeconomic series, unemployment, GNP, consumption, government investment, and private investment in the United States between 1948 and 1988, \(n=160\) values. These data are analyzed in the time domain by Young and Pedregal [213], who were investigating how government spending and private capital investment influenced the rate of unemployment.

Spectral estimation was performed on the detrended, standardized, and tapered growth rate values; see the R code at the end of this example for details. Figure 7.19 shows the individual estimated spectra of each series. We focus on three interesting frequencies. First, we note the lack of spectral power near the annual cycle (\(\omega=1\), or one cycle every four quarters), indicating the data have been seasonally adjusted. In addition, because of the seasonal adjustment, some spectral power appears near the seasonal frequency; this is a distortion apparently caused by the method of seasonally adjusting the data. Next, we note the spectral power near \(\omega=.25\), or one cycle every 4 years, in unemployment, GNP, consumption, and, to lesser degree, in private investment. Finally, spectral power appears near \(\omega=.125\), or one cycle every 8 years in government investment, and perhaps to lesser degrees in unemployment, GNP, and consumption.

Figure 7.20 shows the coherences among various series. At the frequencies of interest, \(\omega=.125\) and \(.25\), pairwise, GNP, Unemployment, Consumption, and

Figure 7.19: The individual estimated spectra (scaled by 1000) of each series show in Fig. 7.18 in terms of the number of cycles in 160 quarters

Private Investment (except for Unemployment and Private Investment) are coherent. Government Investment is either not coherent or minimally coherent with the other series.

Figure 21 shows \(\hat{\lambda}_{1}(\omega)\) and \(\hat{\lambda}_{2}(\omega)\), the first and second eigenvalues of the estimated spectral matrix \(\hat{f}_{xx}(\omega)\). These eigenvalues suggest the first factor is identified by the frequency of one cycle every 4 years, whereas the second factor is identified by the frequency of one cycle every 8 years. The modulus of the corresponding eigenvectors at the frequencies of interest, \(\hat{e}_{1}(10/160)\) and \(\hat{e}_{2}(5/160)\), are shown in Table 6. These values confirm Unemployment, GNP, Consumption, and Private Investment load on the first factor, and Government Investment loads on the second factor. The remainder of the details involving the factor analysis of these data is left as an exercise.

Figure 20: The squared coherencies between the various series displayed in Fig. 18

The following code was used to perform the analysis is R.

gr = diff(log(ts(econ5, start=1948, frequency=4))) # growth rate plot(100*gr, main="Growth Rates (%)")
scale each series to have variance 1 gr = ts(apply(gr,2,scale), freq=4) # scaling strips ts attributes L = c(7,7) # degree of smoothing gr.spec = mvspec(gr, spans=L, demean=FALSE, detrend=FALSE, taper=.25) dev.new() plot(kernel("modified.daniell", L)) # view the kernel - not shown dev.new() plot(gr.spec, log="no", main="Individual Spectra", lty=1:5, lwd=2) legend("topright", colnames(econ5), lty=1:5, lwd=2) dev.new() plot.spec.coherency(gr.spec, ci=NA, main="Squared Coherencies") # PCs n.freq = length(gr.specfreq) lam = matrix(@,n.freq,5) for (ki in 1:n.freq) lam[k,] = eigen(gr.specfxx[,,k], symmetric=TRUE, only.values=TRUE)$values dev.new() par(mfrow=c(2,1), mar=c(4,2,2,1), mnp=c(1.6,.6,0)) plot(gr.specffreq, lam[,1], type="l", ylab="", xlab="Frequency", main="First Eigenvalue")

\begin{table}
\begin{tabular}{|c|c c c c c|} \hline \hline  & Unemp & GNP & Cons & G. Inv. & P. Inv. \\ \hline \(\hat{e}_{1}(\frac{10}{160})\) & 0.53 & 0.50 & 0.51 & 0.06 & 0.44 \\ \hline \(\hat{e}_{2}(\frac{5}{160})\) & 0.19 & 0.14 & 0.23 & 0.93 & 0.16 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Magnitudes of the eigenvectors in Example 7.15

Figure 21: The first, \(\hat{\lambda}_{1}(\omega)\), and second, \(\hat{\lambda}_{2}(\omega)\), eigenvalues of the estimated spectral matrix \(\hat{f}_{xx}(\omega)\). The _vertical dashed lines_ at the peaks are \(\omega=.25\) and \(.125\), respectively

### 7.9 The Spectral Envelope

The concept of spectral envelope for the spectral analysis and _scaling_ of categorical time series was first introduced in Stoffer et al. [193]. Since then, the idea has been extended in various directions (not only restricted to categorical time series), and we will explore these problems as well. First, we give a brief introduction to the concept of scaling time series.

The spectral envelope was motivated by collaborations with researchers who collected categorical-valued time series with an interest in the cyclic behavior of the data. For example Table 7.7 shows the per minute sleep-state of an infant taken from a study on the effects of prenatal exposure to alcohol. Details can be found in Stoffer et al. [191], but briefly, an electroencephalographic (eeg) sleep recording of approximately 2 h is obtained on a full term infant 24-36 h after birth, and the recording is scored by a pediatric neurologist for sleep state. There are two main types of sleep, Non-Rapid Eye Movement (non-rem), also known as _quiet sleep_ and Rapid Eye Movement (rem), also known as _active sleep_. In addition, there are four stages of non-rem (NR1-NR4), with NR1 being the "most active" of the four states, and finally awake (AW), which naturally occurs briefly through the night. This particular infant was never awake during the study.

It is not too difficult to notice a pattern in the data if one concentrates on rem versus non-rem sleep states. But, it would be difficult to try to assess patterns in a longer sequence, or if there were more categories, without some graphical aid. One simple method would be to _scale_ the data, that is, _assign numerical values to the

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline REM & NR2 & NR4 & NR2 & NR1 & NR2 & NR3 & NR4 & NR1 & NR1 & REM \\ REM & REM & NR4 & NR1 & NR1 & NR2 & NR4 & NR4 & NR1 & NR1 & REM \\ REM & REM & NR4 & NR1 & NR1 & REM & NR4 & NR4 & NR1 & NR1 & REM \\ REM & NR3 & NR4 & NR1 & REM & REM & NR4 & NR4 & NR1 & NR1 & REM \\ REM & NR4 & NR4 & NR1 & REM & REM & NR4 & NR4 & NR1 & NR1 & REM \\ REM & NR4 & NR4 & NR1 & REM & REM & NR4 & NR4 & NR1 & NR1 & REM \\ REM & NR4 & NR4 & NR2 & REM & NR2 & NR4 & NR4 & NR1 & NR1 & NR2 \\ REM & NR4 & NR4 & REM & REM & NR2 & NR4 & NR4 & NR1 & REM \\ NR2 & NR4 & NR4 & NR1 & REM & NR2 & NR4 & NR4 & NR1 & REM \\ REM & NR2 & NR4 & NR1 & REM & NR3 & NR4 & NR2 & NR1 & REM \\ \hline \hline \end{tabular}
\end{table}
Table 7.7: Per minute infant eeg sleep states (read down and across)categories_ and then draw a time plot of the scales. Because the states have an order, one obvious scaling is

\[\text{NR4}=1,\quad\text{NR3}=2,\quad\text{NR2}=3,\quad\text{NR1}=4,\quad\text{REM }=5,\quad\text{AW}=6, \tag{7.177}\]

and Fig. 7.22 shows the time plot using this scaling. Another interesting scaling might be to combine the quiet states and the active states:

\[\text{NR4}=\text{NR3}=\text{NR2}=\text{NR1}=0,\quad\text{REM}=1,\quad\text{AW }=2. \tag{7.178}\]

The time plot using (7.178) would be similar to Fig. 7.22 as far as the cyclic (in and out of quiet sleep) behavior of this infant's sleep pattern. Figure 7.22 shows the periodogram of the sleep data using the scaling in (7.177). A large peak exists at the frequency corresponding to one cycle every 60 min. As we might imagine, the general appearance of the periodogram using the scaling (7.178) (not shown) is similar to Fig. 7.22. Most of us would feel comfortable with this analysis even though we made an arbitrary and ad hoc choice about the particular scaling. It is evident from the data (without any scaling) that if the interest is in infant sleep cycling, this particular sleep study indicates an infant cycles between active and quiet sleep at a rate of about one cycle per hour.

The intuition used in the previous example is lost when we consider a long DNA sequence. Briefly, a DNA strand can be viewed as a long string of linked nucleotides. Each nucleotide is composed of a nitrogenous base, a five carbon sugar, and a phosphate group. There are four different bases, and they can be grouped by size; the pyrimidines, thymine (T) and cytosine (C), and the purines, adenine (A) and guanine (G). The nucleotides are linked together by a backbone of alternating sugar and phosphate groups with the 5\({}^{\prime}\) carbon of one sugar linked to the 3\({}^{\prime}\) carbon of the

Figure 7.22: [_Top_] Time plot of the EEG sleep state data in Table 7.7 using the scaling in (7.177). [_Bottom_] Periodogram of the EEG sleep state data in Fig. 7.22 based on the scaling in (7.177). The _peak_ corresponds to a frequency of approximately one cycle every 60 min

[MISSING_PAGE_FAIL:466]

1234123412341234..., or one cycle every four bp. In this example, both scalings (that is, {A, C, G, T} = {0, 1, 0, 1} and {A, C, G, T} = {1, 2, 3, 4}) of the nucleotides are interesting and bring out different properties of the sequence. Hence, we do not want to focus on only one scaling. Instead, the focus should be on finding all possible scalings that bring out all of the interesting features in the data. Rather than choose values arbitrarily, the spectral envelope approach selects scales that help emphasize any periodic feature that exists in a categorical time series of virtually any length in a quick and automated fashion. In addition, the technique can help in determining whether a sequence is merely a random assignment of categories.

### The Spectral Envelope for Categorical Time Series

As a general description, the spectral envelope is a frequency-based principal components technique applied to a multivariate time series. First, we will focus on the basic concept and its use in the analysis of categorical time series. Technical details can be found in Stoffer et al. [193].

Briefly, in establishing the spectral envelope for categorical time series, the basic question of how to efficiently discover periodic components in categorical time series was addressed. This, was accomplished via nonparametric spectral analysis as follows. Let \(x_{t}\), \(t=0\), \(\pm 1\), \(\pm 2\), \(\ldots\), be a categorical-valued time series with finite state-space \(\mathcal{C}=\{c_{1}\), \(c_{2}\), \(\ldots\), \(c_{k}\}\). Assume \(x_{t}\) is stationary and \(p_{j}=\Pr\{x_{t}=c_{j}\}>0\) for \(j=1\), \(2\), \(\ldots\), \(k\). For \(\beta=(\beta_{1}\), \(\beta_{2}\), \(\ldots\), \(\beta_{k})^{\prime}\in\mathbb{R}^{k}\), denote by \(x_{t}(\beta)\) the real-valued stationary time series corresponding to the scaling that assigns the category \(c_{j}\) the numerical value \(\beta_{j}\), \(j=1\), \(2\), \(\ldots\), \(k\). The spectral density of \(x_{t}(\beta)\) will be denoted by \(f_{xx}(\omega;\beta)\). The goal is to find scalings \(\beta\), so the spectral density is in some sense interesting, and to summarize the spectral information by what is called the spectral envelope.

In particular, \(\beta\) is chosen to maximize the power at each frequency, \(\omega\), of interest, relative to the total power \(\sigma^{2}(\beta)=\operatorname{var}\{x_{t}(\beta)\}\). That is, we chose \(\beta(\omega)\), at each \(\omega\) of interest, so

\[\lambda(\omega)=\max_{\beta}\left\{\frac{f_{xx}(\omega;\beta)}{\sigma^{2}(\beta )}\right\}, \tag{7.179}\]

over all \(\beta\) not proportional to \(1_{k}\), the \(k\times 1\) vector of ones. Note, \(\lambda(\omega)\) is not defined if \(\beta=a1_{k}\) for \(a\in\mathbb{R}\) because such a scaling corresponds to assigning each category the same value \(a\); in this case, \(f_{xx}(\omega\) ; \(\beta)\equiv 0\) and \(\sigma^{2}(\beta)=0\). The optimality criterion \(\lambda(\omega)\) possesses the desirable property of being invariant under location and scale changes of \(\beta\).

As in most scaling problems for categorical data, it is useful to represent the categories in terms of the unit vectors \(u_{1}\), \(u_{2}\), \(\ldots\), \(u_{k}\), where \(u_{j}\) represents the \(k\times 1\) vector with a one in the \(j\)th row, and zeros elsewhere. We then define a \(k\)-dimensional stationary time series \(y_{t}\) by \(y_{t}=u_{j}\) when \(x_{t}=c_{j}\). The time series \(x_{t}(\beta)\) can be obtained from the \(y_{t}\) time series by the relationship \(x_{t}(\beta)=\beta^{\prime}y_{t}\). Assume the vector process \(y_{t}\) has a continuous spectral density denoted by \(f_{yy}(\omega)\). For each \(\omega\), \(f_{yy}(\omega)\) is, of course, a \(k\times k\) complex-valued Hermitian matrix. The relationship \(x_{t}(\beta)=\beta^{\prime}y_{t}\) implies \(f_{xx}(\omega;\beta)=\beta^{\prime}f_{yy}(\omega)\beta=\beta^{\prime}f_{yy}^{ re}(\omega)\beta\), where \(f_{yy}^{re}(\omega)\) denotes the real part2 of \(f_{yy}(\omega)\). The imaginary part disappears from the expression because it is skew-symmetric, that is, \(f_{yy}^{im}(\omega)^{\prime}=-f_{yy}^{im}(\omega)\). The optimality criterion can thus be expressed as

\[\lambda(\omega)=\max_{\beta}\left\{\frac{\beta^{\prime}f_{yy}^{re}(\omega)\beta} {\beta^{\prime}V\beta}\right\}, \tag{7.180}\]

where \(V\) is the variance-covariance matrix of \(y_{t}\). The resulting scaling \(\beta(\omega)\) is called the optimal scaling.

The \(y_{t}\) process is a multivariate point process, and any particular component of \(y_{t}\) is the individual point process for the corresponding state (for example, the first component of \(y_{t}\) indicates whether the process is in state \(c_{1}\) at time \(t\)). For any fixed \(t\), \(y_{t}\) represents a single observation from a simple multinomial sampling scheme. It readily follows that \(V=D-p\,p^{\prime}\), where \(p=(p_{1},\ldots,p_{k})^{\prime}\), and \(D\) is the \(k\times k\) diagonal matrix \(D=\mbox{diag}\{p_{1},\ldots,p_{k}\}\). Because, by assumption, \(p_{j}>0\) for \(j=1\), \(2\),..., \(k\), it follows that \(\mbox{rank}(V)=k-1\) with the null space of \(V\) being spanned by \(1_{k}\). For any \(k\times(k-1)\) full rank matrix \(Q\) whose columns are linearly independent of \(1_{k}\), \(Q^{\prime}VQ\) is a \((k-1)\times(k-1)\) positive-definite symmetric matrix.

With the matrix \(Q\) as previously defined, define \(\lambda(\omega)\) to be the largest eigenvalue of the determinantal equation

\[|Q^{\prime}f_{yy}^{re}(\omega)Q-\lambda(\omega)Q^{\prime}VQ|=0,\]

and let \(b(\omega)\in\mathbb{R}^{k-1}\) be any corresponding eigenvector, that is,

\[Q^{\prime}f_{yy}^{re}(\omega)Qb(\omega)=\lambda(\omega)Q^{\prime}VQb(\omega).\]

The eigenvalue \(\lambda(\omega)\geq 0\) does not depend on the choice of \(Q\). Although the eigenvector \(b(\omega)\) depends on the particular choice of \(Q\), the equivalence class of scalings associated with \(\beta(\omega)=Qb(\omega)\) does not depend on \(Q\). A convenient choice of \(Q\) is \(Q=[I_{k-1}\mid 0\mid^{\prime}\), where \(I_{k-1}\) is the \((k-1)\times(k-1)\) identity matrix and \(0\) is the \((k-1)\times 1\) vector of zeros. For this choice, \(Q^{\prime}f_{yy}^{re}(\omega)Q\) and \(Q^{\prime}VQ\) are the upper \((k-1)\times(k-1)\) blocks of \(f_{yy}^{re}(\omega)\) and \(V\), respectively. This choice corresponds to setting the last component of \(\beta(\omega)\) to zero.

The value \(\lambda(\omega)\) itself has a useful interpretation; specifically, \(\lambda(\omega)d\omega\) represents the largest proportion of the total power that can be attributed to the frequencies \((\omega,\omega+d\omega)\) for any particular scaled process \(x_{t}(\beta)\), with the maximum being achieved by the scaling \(\beta(\omega)\). Because of its central role, \(\lambda(\omega)\) is defined to be the _spectral envelope of a stationary categorical time series_.

The name spectral envelope is appropriate since \(\lambda(\omega)\) envelopes the standardized spectrum of any scaled process. That is, given any \(\beta\) normalized so that \(x_{t}(\beta)\) has total power one, \(f_{xx}(\omega\;;\beta)\leq\lambda(\omega)\) with equality if and only if \(\beta\) is proportional to \(\beta(\omega)\).

Given observations \(x_{t}\), for \(t=1,\ldots,n\), on a categorical time series, we form the multinomial point process \(y_{t}\), for \(t=1,\ldots,n\). Then, the theory for estimating the spectral density of a multivariate, real-valued time series can be applied to estimating \(f_{yy}(\omega)\), the \(k\times k\) spectral density of \(y_{t}\). Given an estimate \(\hat{f}_{yy}(\omega)\) of \(f_{yy}(\omega)\), estimates \(\hat{\lambda}(\omega)\) and \(\hat{\beta}(\omega)\) of the spectral envelope, \(\lambda(\omega)\), and the corresponding scalings, \(\beta(\omega)\)can then be obtained. Details on estimation and inference for the sample spectral envelope and the optimal scalings can be found in Stoffer et al. [193], but the main result of that paper is as follows: If \(\hat{f}_{yy}(\omega)\) is a consistent spectral estimator and if for each \(j=1,\ldots\), \(J\), the largest root of \(f^{re}_{yy}(\omega_{j})\) is distinct, then

\[\big{\{}\eta_{n}[\hat{\lambda}(\omega_{j})-\lambda(\omega_{j})]/\lambda(\omega_ {j}),\;\eta_{n}[\hat{\beta}(\omega_{j})-\beta(\omega_{j})];\;j=1,\ldots,J\big{\}} \tag{7.181}\]

converges (\(n\to\infty\)) jointly in distribution to independent zero-mean, normal, distributions, the first of which is standard normal; the asymptotic covariance structure of \(\hat{\beta}(\omega_{j})\) is discussed in Stoffer et al. [193]. Result (7.181) is similar to (7.148), but in this case, \(\beta(\omega)\) and \(\hat{\beta}(\omega)\) are real. The term \(\eta_{n}\) is the same as in (7.181), and its value depends on the type of estimator being used. Based on these results, asymptotic normal confidence intervals and tests for \(\lambda(\omega)\) can be readily constructed. Similarly, for \(\beta(\omega)\), asymptotic confidence ellipsoids and chi-square tests can be constructed; details can be found in Stoffer et al. [193], Theorems 3.1-3.3).

Peak searching for the smoothed spectral envelope estimate can be aided using the following approximations. Using a first-order Taylor expansion, we have

\[\log\hat{\lambda}(\omega)\approx\log\lambda(\omega)+\frac{\hat{\lambda}(\omega )-\lambda(\omega)}{\lambda(\omega)}, \tag{7.182}\]

so \(\eta_{n}[\log\hat{\lambda}(\omega)-\log\lambda(\omega)]\) is approximately standard normal. It follows that \(E[\log\hat{\lambda}(\omega)]\approx\log\lambda(\omega)\) and \(\text{var}[\log\hat{\lambda}(\omega)]\approx\eta_{n}^{-2}\). If no signal is present in a sequence of length \(n\), we expect \(\lambda(j/n)\approx 2/n\) for \(1<j<n/2\), and hence approximately \((1-\alpha)\times 100\%\) of the time, \(\log\hat{\lambda}(\omega)\) will be less than \(\log(2/n)+(z_{\alpha}/\eta_{n})\), where \(z_{\alpha}\) is the \((1-\alpha)\) upper tail cutoff of the standard normal distribution. Exponentiating, the \(\alpha\) critical value for \(\hat{\lambda}(\omega)\) becomes \((2/n)\exp(z_{\alpha}/\eta_{n})\). Useful values of \(z_{\alpha}\) are \(z_{.001}=3.09\), \(z_{.0001}=3.71\), and \(z_{.00001}=4.26\), and from our experience, thresholding at these levels works well.

**Example 7.16**: **Spectral Analysis of DNA Sequences**__

To help understand the methodology, we give explicit instructions for the calculations involved in estimating the spectral envelope of a DNA sequence, \(x_{t}\), for \(t=1,\ldots,n\), using the nucleotide alphabet.

(i) In this example, we hold the scale for T fixed at zero. In this case, we form the \(3\times 1\) data vectors \(y_{t}\):

\[\begin{array}{l}y_{t}=(1,0,0)^{\prime}\;\;\text{if}\;\;x_{t}=\mathbb{A}; \qquad y_{t}=(0,1,0)^{\prime}\;\;\text{if}\;\;x_{t}=\mathbb{C};\\ y_{t}=(0,0,1)^{\prime}\;\;\text{if}\;\;x_{t}=\mathbb{G};\qquad y_{t}=(0,0,0)^{ \prime}\;\;\text{if}\;\;x_{t}=\mathbb{T}.\end{array}\]

The scaling vector is \(\beta=(\beta_{1},\beta_{2},\beta_{3})^{\prime}\), and the scaled process is \(x_{t}(\beta)=\beta^{\prime}y_{t}\).

(ii) Calculate the DFT of the data

\[Y(j/n)=n^{-1/2}\sum_{t=1}^{n}y_{t}\exp(-2\pi itj/n).\]

Note \(Y(j/n)\) is a \(3\times 1\) complex-valued vector. Calculate the periodogram, \(I(j/n)=Y(j/n)Y^{*}(j/n)\), for \(j=1,\ldots,[n/2]\), and retain only the real part, say, \(I^{re}(j/n)\).

* Smooth the \(I^{re}(j/n)\) to obtain an estimate of \(f^{re}_{yy}(j/n)\). Let \(\{h_{k};\ k=0,\pm 1,\ldots,\pm m\}\) be weights as described in (4.64). Calculate \[\hat{f}^{re}_{yy}(j/n)=\sum_{k=-m}^{m}h_{k}\ I^{re}(j/n+k/n).\]
* Calculate the \(3\times 3\) sample variance-covariance matrix, \[S_{yy}=n^{-1}\sum_{t=1}^{n}(y_{t}-\bar{y})(y_{t}-\bar{y})^{\prime},\] where \(\bar{y}=n^{-1}\sum_{t=1}^{n}y_{t}\) is the sample mean of the data.
* For each \(\omega_{j}=j/n\), \(j=0,1,\ldots,[n/2]\), determine the largest eigenvalue and the corresponding eigenvector of the matrix \(2n^{-1}S_{yy}^{-1/2}\hat{f}^{re}_{yy}(\omega_{j})S_{yy}^{-1/2}\). Note, \(S_{yy}^{1/2}\) is the unique square root matrix of \(S_{yy}\).
* The sample spectral envelope \(\hat{\lambda}(\omega_{j})\) is the eigenvalue obtained in the previous step. If \(b(\omega_{j})\) denotes the eigenvector obtained in the previous step, the optimal sample scaling is \(\hat{\beta}(\omega_{j})=S_{yy}^{-1/2}b(\omega_{j})\); this will result in three values, the value corresponding to the fourth category, T, being held fixed at zero.

**Example 7.17**: **Analysis of an Epstein-Barr Virus Gene**

In this example, we focus on a dynamic (or sliding-window) analysis of the gene labeled BNRF1 (bp 1736-5689) of Epstein-Barr. Figure 7.23 shows the spectral envelope estimate of the entire coding sequence (3954 bp long). The figure also shows a strong signal at frequency 1/3; the corresponding optimal scaling was \(\mathtt{A}=.10,\mathtt{C}=.61,\mathtt{G}=.78,\mathtt{T}=0\), which indicates the signal is in the strong-weak bonding alphabet, \(S=\{\mathtt{C},\mathtt{G}\}\) and \(W=\{\mathtt{A},\mathtt{T}\}\).

Figure 7.24 shows the result of computing the spectral envelope over three nonoverlapping 1000-bp windows and one window of 954 bp, across the CDS,

Figure 7.23: Smoothed sample spectral envelope of the BNRF1 gene from the Epstein–Barr virus

namely, the first, second, third, and fourth quarters of BNRF1. An approximate 0.0001 significance threshold is.69%. The first three quarters contain the signal at the frequency 1/3 (Fig. 7.24a-c); the corresponding sample optimal scalings for the first three windows were (a) \(\mathtt{A}=.01,\mathtt{C}=.71,\mathtt{G}=.71,\mathtt{T}=0\); (b) \(\mathtt{A}=.08,\mathtt{C}=0.71,\mathtt{G}=.70,\mathtt{T}=0\); (c) \(\mathtt{A}=.20,\mathtt{C}=.58,\mathtt{G}=.79,\mathtt{T}=0\). The first two windows are consistent with the overall analysis. The third section, however, shows some minor departure from the strong-weak bonding alphabet. The most interesting outcome is that the fourth window shows that no signal is present. This leads to the conjecture that the fourth quarter of BNRF1 of Epstein-Barr is actually noncoding.

The R code for the first part of the example is as follows.

 u = factor(bnrfilebv) # first, input the data as factors and then x = model.matrix(-u-1)[,1:3] # make an indicator matrix
x = x[1:1000,] # select subsequence if desired Var = var(x) # var-cov matrix xspec = mvspec(x, spans=c(7,7), plot=FALSE) fxrr = Re(xspec$fxx) # fxrr is real(fxx)
compute \(Q=\) Var\({}^{A}\)-1/2 ev = eigen(Var) Q = evSvectors%*%diag(1/sqrt(ev$values))%%%(ev$vectors)
compute spec envelope and scale vectors num = xspec$n.used # sample size used for FFT nfreq = length(xspec$freq) # number of freqs used specenv = matrix(@,nfreq,1) # initialize the spec envelope beta = matrix(@,nfreq,3) # initialize the scale vectors for (kin 1:nfreq){ ev = eigen(2*Q%*%Krr[,k]%*%Q/num, symmetric=TRUE)

Figure 7.24: Smoothed sample spectral envelope of the BNRF1 gene from the Epstein–Barr virus: (**a**) first 1000 bp, (**b**) second 1000 bp, (**c**) third 1000 bp, and (**d**) last 954 bp

specenv[k] = ev$values[1] # spec env at freq k/n is max evalue  b = 0%*%evSvectors[,1] # beta at freq k/n  beta[k,] = b/sqrt(sum(b^2)) } # helps to normalize beta  # output and graphics  frequency = xspec$freq  plot(frequency, 100*specenv, type="l", ylab="Spectral Envelope (%)")  # add significance threshold to plot  m = xspec$kernel5m  etiinv = sqrt(sum(xspec$kernel[-m:m]^2))  thresh=100*(2/num)*exp(qnorm(.9999)*etainv)  abline(h=thresh, lty=6, col=4)  # details  output = cbind(frequency, specenv, beta)  colnames(output) = c("freq","specenv", "A", "C", "G")  round(output,3)

### The Spectral Envelope for Real-Valued Time Series

The concept of the spectral envelope for categorical time series was extended to real-valued time series, \(\{x_{t};t=0,\pm 1,\pm 2,\ldots,\}\), in McDougall et al. [136]. The process \(x_{t}\) can be vector-valued, but here we will concentrate on the univariate case. Further details can be found in McDougall et al. [136]. The concept is similar to projection pursuit (Friedman and Stuetzle [63]). Let \(\mathcal{G}\) denote a \(k\)-dimensional vector space of continuous real-valued transformations with \(\{g_{1},\ldots,g_{k}\}\) being a set of basis functions satisfying \(E[g_{i}(x_{t})^{2}]<\infty\), \(i=1,\ldots,k\). Analogous to the categorical time series case, define the scaled time series with respect to the set \(\mathcal{G}\) to be the real-valued process

\[x_{t}(\beta)=\beta^{\prime}y_{t}=\beta_{1}g_{1}(x_{t})+\cdots+\beta_{k}g_{k}(x_ {t})\]

obtained from the vector process

\[y_{t}=\Big{(}g_{1}(X_{t}),\ldots,g_{k}(X_{t})\Big{)}^{\prime},\]

where \(\beta=(\beta_{1},\ldots,\beta_{k})^{\prime}\in\mathbb{R}^{k}\). If the vector process, \(y_{t}\), is assumed to have a continuous spectral density, say, \(f_{yy}(\omega)\), then \(x_{t}(\beta)\) will have a continuous spectral density \(f_{xx}(\omega;\beta)\) for all \(\beta\neq 0\). Noting, \(f_{xx}(\omega;\beta)=\beta^{\prime}f_{yy}(\omega)\beta=\beta^{\prime}f_{yy}^{ re}(\omega)\beta\), and \(\sigma^{2}(\beta)=\text{var}[x_{t}(\beta)]=\beta^{\prime}V\beta\), where \(V=\text{var}(y_{t})\) is assumed to be positive definite, the optimality criterion

\[\lambda(\omega)=\sup_{\beta\neq 0}\left\{\frac{\beta^{\prime}f_{yy}^{re}( \omega)\beta}{\beta^{\prime}V\beta}\right\}, \tag{7.183}\]

is well defined and represents the largest proportion of the total power that can be attributed to the frequency \(\omega\) for any particular scaled process \(x_{t}(\beta)\). This interpretation of \(\lambda(\omega)\) is consistent with the notion of the spectral envelope introduced in the previous section and provides the following working definition: _The spectral envelope of a time series with respect to the space \(\mathcal{G}\) is defined to be \(\lambda(\omega)\)_.

The solution to this problem, as in the categorical case, is attained by finding the largest scalar \(\lambda(\omega)\) such that \[f_{yy}^{re}(\omega)\beta(\omega)=\lambda(\omega)V\beta(\omega) \tag{7.184}\]

for \(\beta(\omega)\neq 0\). That is, \(\lambda(\omega)\) is the largest eigenvalue of \(f_{yy}^{re}(\omega)\) in the metric of \(V\), and the optimal scaling, \(\beta(\omega)\), is the corresponding eigenvector.

If \(x_{t}\) is a categorical time series taking values in the finite state-space \(\mathcal{S}=\{c_{1},c_{2},\ldots,c_{k}\}\), where \(c_{j}\) represents a particular category, an appropriate choice for \(\mathcal{G}\) is the set of indicator functions \(g_{j}(x_{t})=I(x_{t}=c_{j})\). Hence, this is a natural generalization of the categorical case. In the categorical case, \(\mathcal{G}\) does not consist of linearly independent \(g\)'s, but it was easy to overcome this problem by reducing the dimension by one. In the vector-valued case, \(x_{t}=(x_{1t},\ldots,x_{pt})^{\prime}\), we consider \(\mathcal{G}\) to be the class of transformations from \(\mathbb{R}^{p}\) into \(\mathbb{R}\) such that the spectral density of \(g(x_{t})\) exists. One class of transformations of interest are linear combinations of \(x_{t}\). In Tiao et al. [201], for example, linear transformations of this type are used in a time domain approach to investigate contemporaneous relationships among the components of multivariate time series. Estimation and inference for the real-valued case are analogous to the methods described in the previous section for the categorical case. We consider an example here; numerous other examples can be found in McDougall et al. [136].

**Example 7.18**: **Optimal Transformations for Financial Data: NYSE Returns**__

In many financial applications, one typically addresses the analysis of the squared returns, such as was done in Sects. 5.3 and 6.11. However, there may be other transformations that supply more information than simply squaring the data. For example, Ding et al. [52] who applied transformations of the form \(|x_{t}|^{d}\), for \(d\in(0,3]\), to the S&P 500 stock market series. They found that power transformation of the absolute return has quite high autocorrelation for long lags, and this property is strongest when \(d\) is around 1. They concluded that the "result appears to argue against ARCH type specifications based upon squared returns."

In this example, we examine the NYSE returns (nyse). We used with the generating set \(\mathcal{G}=\{x,\,|x|,\,x^{2}\}\)--which seems natural for this analysis--to estimate the spectral envelope for the data, and the result is plotted in Fig. 7.25. Although

Figure 7.25: Spectral envelope with respect to \(\mathcal{G}=\{x,|x|,x^{2}\}\) for the NYSE returns

the data are white noise, they are clearly not iid, and considerable power is present at the low frequencies. The presence of spectral power at very low frequencies in detrended economic series has been frequently reported and is typically associated with long-range dependence. The estimated optimal transformation near the zero frequency, \(\omega=.001\), was \(\beta(.001)=(-1,921,-2596)^{\prime}\), which leads to the transformation

\[g(x)=-x+921|x|-2596x^{2}. \tag{7.185}\]

This transformation is plotted in Fig. 7.26. The transformation given in (7.185) is basically the absolute value (with some slight curvature and asymmetry) for most of the values, but the effect of extremes is dampened.

The following R code was used in this example.

u = astsa:nyse # accept no substitutes x = cbind(u, abs(u), u^2) Var = var(x) # var-cov matrix xspec = mvspec(x, spans=c(5,3), taper-5, plot-FALSE) fxxxr = Re(xspecfxx) # fxxxr is real(fxx) # compute Q = Var^-1/2 ev = eigen(Var) Q = evSvectors%^*diag(1/sqrt(evSvalues))%^%t(evSvectors)
compute spec env and scale vectors num = xspecSn.used # sample size used for FFT nfreq = length(xspec5freq) # number of freqs used specenv = matrix(@,nfreq,1) # initialize the spec envelope beta = matrix(@,nfreq,3) # initialize the scale vectors for (k in 1:nfreq)  ev = eigen(2*Q%*fxxxr[,_k]%*%Q/num) # get values of normalized spectral matrix at freq k/n  specenv[k] = evSvalues[1] # spec env at freq k/n  b = 0%*fevSvectors[,1] # beta at freq k/n  beta[k,] = b/b[1] # first coef is always 1
output and graphics par(mar=c(2.5,2.75,.5,.5), mpp=c(1.5,.6,0)) frequency = xspecffreq plot(frequency, 100*specenv, type="l", ylab="Spectral Envelope (%)")

Figure 7.26: Estimated optimal transformation, (7.185), for the NYSE returns at \(\omega=.001\). The _dashed line_ indicates the pure absolute value transformation

m = xspecSkernel5m  etainv = sqrt(sum(xspecSkernel[-m:m]^2))  thresh = 100*(2/num)^exp(qnorm(.9999)*etainv)^matrix(1,nfreq,1)  lines(frequency, thresh, lty=2, col=4)  # details  b = sign(b[2])*output[2,3:5] # sign of |x| positive for beauty  output = cbind(frequency, specenv, beta)  colnames(output)=c("freq","specenv","x", "|x|", "x^2"); round(output, 4)  dev.new(); par(mar=c(2.5,2.5,.5,.5), mgp=c(1.5,.6,0))  # plot transform  g = function(x) { b[1]*x+b[2]*abs(x)+b[3]*x^2 }  curve(g, -.2,.2, panel.first=grid(lty=2))  g2 = function(x) { b[2]*abs(x) } # corresponding |x|  curve(g2, -.2,.2, add=TRUE, lty=6, col=4)

## Problems

#### Section 7.2

Consider the complex Gaussian distribution for the random variable \(X=X_{c}-iX_{s}\), as defined in (7.1)-(7.3), where the argument \(\omega_{k}\) has been suppressed. Now, the \(2p\times 1\) real random variable \(Z=(X^{\prime}_{c},X^{\prime}_{s})^{\prime}\) has a multivariate normal distribution with density

\[p(Z)=(2\pi)^{-p}|\Sigma|^{-1/2}\exp\Biggl{\{}-\frac{1}{2}(Z-\mu)^{\prime} \Sigma^{-1}(Z-\mu)\Biggr{\}},\]

where \(\mu=(M^{\prime}_{c},M^{\prime}_{s})^{\prime}\) is the mean vector. Prove

\[|\Sigma|=\Biggl{(}\frac{1}{2}\Biggr{)}^{2p}|C-iQ|^{2},\]

using the result that the eigenvectors and eigenvalues of \(\Sigma\) occur in pairs, i.e., \((v^{\prime}_{c},v^{\prime}_{s})^{\prime}\) and \((v^{\prime}_{s},-v^{\prime}_{c})^{\prime}\), where \(v_{c}-iv_{s}\) denotes the eigenvector of \(f_{xx}\). Show that

\[\frac{1}{2}(Z-\mu)^{\prime}\Sigma^{-1}(Z-\mu))=(X-M)^{*}f^{-1}(X-M)\]

so \(p(X)=p(Z)\) and we can identify the density of the complex multivariate normal variable \(X\) with that of the real multivariate normal \(Z\).

Prove \(\hat{f}\) in (7.6) maximizes the log likelihood (7.5) by minimizing the negative of the log likelihood

\[L\ln|f|+L\ \mathrm{tr}\{\hat{\mathrm{f}}\mathrm{f}^{-1}\}\]

in the form

\[L\sum_{i}\bigl{(}\lambda_{i}-\ln\lambda_{i}-1\bigr{)}+Lp+L\ln|\hat{f}|,\]

where the \(\lambda_{i}\) values correspond to the eigenvalues in a simultaneous diagonalization of the matrices \(f\) and \(\hat{f}\); i.e., there exists a matrix \(P\) such that \(P^{*}fP=I\) and \(P^{*}\hat{f}P=\text{diag}\ (\lambda_{1},\ldots,\lambda_{p})=\Lambda\). Note, \(\lambda_{i}-\ln\lambda_{i}-1\geq 0\) with equality if and only if \(\lambda_{i}=1\), implying \(\Lambda=I\) maximizes the log likelihood and \(f=\hat{f}\) is the maximizing value.

#### Section 7.3

**7.3** Verify (7.18) and (7.19) for the mean-squared prediction error MSE in (7.11). Use the orthogonality principle, which implies

\[MSE=E\left[(y_{t}-\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}x_{t-r})y_{t}\right]\]

and gives a set of equations involving the autocovariance functions. Then, use the spectral representations and Fourier transform results to get the final result. Next, consider the predicted series

\[\hat{y}_{t}=\sum_{r=-\infty}^{\infty}\beta_{r}^{\prime}x_{t-r},\]

where \(\beta_{r}\) satisfies (7.13). Show the ordinary coherence between \(y_{t}\) and \(\hat{y}_{t}\) is exactly the multiple coherence (7.20).

**7.4** Consider the complex regression model (7.28) in the form

\[Y=XB+V,\]

where \(Y=(Y_{1},Y_{2},\ldots Y_{L})^{\prime}\) denotes the observed DFTs after they have been re-indexed and \(X=(X_{1},X_{2},\ldots,X_{L})^{\prime}\) is a matrix containing the reindexed input vectors. The model is a complex regression model with \(Y=Y_{c}-iY_{s}\), \(X=X_{c}-iX_{s}\), \(B=B_{c}-iB_{s}\), and \(V=V_{c}-iV_{s}\) denoting the representation in terms of the usual cosine and sine transforms. Show the partitioned real regression model involving the \(2L\times 1\) vector of cosine and sine transforms, say,

\[\begin{pmatrix}Y_{c}\\ Y_{s}\end{pmatrix}=\begin{pmatrix}X_{c}&-X_{s}\\ X_{s}&X_{c}\end{pmatrix}\begin{pmatrix}B_{c}\\ B_{s}\end{pmatrix}+\begin{pmatrix}V_{c}\\ V_{s}\end{pmatrix},\]

is _isomorphic_ to the complex regression model in the sense that the real and imaginary parts of the complex model appear as components of the vectors in the real regression model. Use the usual regression theory to verify (7.27) holds. For example, writing the real regression model as

\[y=xb+v,\]

the isomorphism would imply

\[L(\hat{f}_{yy}-\hat{f}_{xy}^{*}\hat{f}_{xx}^{-1}\hat{f}_{xy}) =Y^{*}Y-Y^{*}X(X^{*}X)^{-1}X^{*}Y\] \[=y^{\prime}y-y^{\prime}x(x^{\prime}x)^{-1}x^{\prime}y.\]
Consider estimating the function

\[\psi_{t}=\sum_{r=-\infty}^{\infty}a_{r}^{\prime}\beta_{t-r}\]

by a linear filter estimator of the form

\[\hat{\psi}_{t}=\sum_{r=-\infty}^{\infty}a_{r}^{\prime}\hat{\beta}_{t-r},\]

where \(\hat{\beta}_{t}\) is defined by (7.42). Show a sufficient condition for \(\hat{\psi}_{t}\) to be an unbiased estimator; i.e., \(E\)\(\hat{\psi}_{t}=\psi_{t}\), is

\[H(\omega)Z(\omega)=I\]

for all \(\omega\). Similarly, show any other unbiased estimator satisfying the above condition has minimum variance (see Shumway and Dean [178]), so the estimator given is a best linear unbiased (BLUE) estimator.

Consider a linear model with mean value function \(\mu_{t}\) and a signal \(\alpha_{t}\) delayed by an amount \(\tau_{j}\) on each sensor, i.e.,

\[y_{jt}=\mu_{t}+\alpha_{t-\tau_{j}}+v_{jt}.\]

Show the estimators (7.42) for the mean and the signal are the Fourier transforms of

\[\hat{M}(\omega)=\frac{Y.(\omega)-\overline{\phi(\omega)}B_{w}(\omega)}{1-| \phi(\omega)|^{2}}\]

and

\[\hat{A}(\omega)=\frac{B_{w}(\omega)-\phi(\omega)Y.(\omega)}{1-|\phi(\omega)|^ {2}},\]

where

\[\phi(\omega)=\frac{1}{N}\ \sum_{j=1}^{N}e^{2\pi i\omega\tau_{j}}\]

and \(B_{w}(\omega)\) is defined in (7.64).

Consider the estimator (7.67) as applied in the context of the random coefficient model (7.65). Prove the filter coefficients for the minimum mean square estimator can be determined from (7.68) and the mean square covariance is given by (7.71).

**7.8** For the random coefficient model, verify the expected mean square of the regression power component is

\[E[SSR(\omega_{k})] = E[Y^{*}(\omega_{k})Z(\omega_{k})S_{z}^{-1}(\omega_{k})Z^{*}(\omega _{k})Y(\omega_{k})]\] \[= Lf_{\beta}(\omega_{k})\text{tr}\{S_{z}(\omega_{k})\}+Lqf_{v}( \omega_{k}).\]

Recall, the underlying frequency domain model is

\[Y(\omega_{k})=Z(\omega_{k})B(\omega_{k})+V(\omega_{k}),\]

where \(B(\omega_{k})\) has spectrum \(f_{\beta}(\omega_{k})I_{q}\) and \(V(\omega_{k})\) has spectrum \(f_{v}(\omega_{k})I_{N}\) and the two processes are uncorrelated.

_Section 7.6_

**7.9** Suppose we have \(I=2\) groups and the models

\[y_{1jt}=\mu_{t}+\alpha_{1t}+v_{1jt}\]

for the \(j=1,\ldots,N\) observations in group 1 and

\[y_{2jt}=\mu_{t}+\alpha_{2t}+v_{2jt}\]

for the \(j=1,\ldots,N\) observations in group 2, with \(\alpha_{1t}+\alpha_{2t}=0\). Suppose we want to test equality of the two group means; i.e.,

\[y_{ijt}=\mu_{t}+v_{ijt},\quad i=1,2.\]

(a) Derive the residual and error power components corresponding to (7.81) and (7.82) for this particular case.

(b) Verify the forms of the linear compounds involving the mean given in (7.88) and (7.89), using (7.86) and (7.87).

(c) Show the ratio of the two smoothed spectra in (7.101) has the indicated \(F\)-distribution when \(f_{1}(\omega)=f_{2}(\omega)\). When the spectra are not equal, show the variable is proportional to an \(F\)-distribution, where the proportionality constant depends on the ratio of the spectra.

_Section 7.7_

**7.10** The problem of detecting a signal in noise can be considered using the model

\[x_{t}=s_{t}+w_{t},\quad t=1,\ldots,n,\]

for \(p_{1}(x)\) when a signal is present and the model

\[x_{t}=w_{t},\quad t=1,\ldots,n,\]for \(p_{2}(x)\) when no signal is present. Under multivariate normality, we might specialize even further by assuming the vector \(w=(w_{1},\ldots,w_{n})^{\prime}\) has a multivariate normal distribution with mean \(\mathbf{0}\) and covariance matrix \(\Sigma=\sigma_{w}^{2}I_{n}\), corresponding to white noise. Assuming the signal vector \(s=(s_{1},\ldots,s_{n})^{\prime}\) is fixed and known, show the discriminant function (7.110) becomes the _matched filter_

\[\frac{1}{\sigma_{w}^{2}}\sum_{t=1}^{n}s_{t}x_{t}-\frac{1}{2}\bigg{(}\frac{S}{N }\bigg{)}+\ln\frac{\pi_{1}}{\pi_{2}},\]

where

\[\bigg{(}\frac{S}{N}\bigg{)}=\frac{\sum_{t=1}^{n}s_{t}^{2}}{\sigma_{w}^{2}}\]

denotes the _signal-to-noise ratio_. Give the decision criterion if the prior probabilities are assumed to be the same. Express the false alarm and missed signal probabilities in terms of the normal cdf and the signal-to-noise ratio.

**7.11**: Assume the same additive signal plus noise representations as in the previous problem, except, the signal is now a random process with a zero mean and covariance matrix \(\sigma_{s}^{2}I\). Derive the comparable version of (7.113) as a _quadratic detector_, and characterize its performance under both hypotheses in terms of constant multiples of the chi-squared distribution.

#### Section 7.8

**7.12**: Perform principal component analyses on the stimulus conditions (1) awake-heat and (2) awake-shock, and compare your results to the results of Example 7.13. Use the data in fmri and average across subjects.

**7.13**: For this problem, consider the first three earthquake series (EQ1, EQ2, EQ3) listed in eqexp.

(a) Estimate and compare the spectral density of the P component and then of the S component for each individual earthquake.

(b) Estimate and compare the squared coherency between the P and S components of each individual earthquake. Comment on the strength of the coherence.

(c) Let \(x_{ti}\) be the P component of earthquake \(i=1,2,3\), and let \(x_{t}=(x_{t1},x_{t2},x_{t3})^{\prime}\) be the \(3\times 1\) vector of P components. Estimate the spectral density, \(\lambda_{1}(\omega)\), of the first principal component series of \(x_{t}\). Compare this to the corresponding spectra calculated in (a).

(d) Analogous to part (c), let \(y_{t}\) denote the \(3\times 1\) vector series of S components of the first three earthquakes. Repeat the analysis of part (c) on \(y_{t}\).

**7.14**: In the factor analysis model (7.152), let \(p=3\), \(q=1\), and

\[\Sigma_{xx}=\left[\begin{array}{ccc}1&.4&.9\\.4&1&.7\\.9&.7&1\end{array}\right].\]

Show there is a unique choice for \(\mathcal{B}\) and \(D\), but \(\delta_{3}^{2}<0\), so the choice is not valid.

**7.15** Extend the EM algorithm for classical factor analysis, (7.158)-(7.163), to the time series case of maximizing \(\ln L\big{(}\mathcal{B}(\omega_{j}),D_{\boldsymbol{\epsilon}\boldsymbol{\epsilon }}(\omega_{j})\big{)}\) in (7.174). Then, for the data used in Example 7.15, find the approximate maximum likelihood estimates of \(\mathcal{B}(\omega_{j})\) and \(D_{\boldsymbol{\epsilon}\boldsymbol{\epsilon}}(\omega_{j})\), and, consequently, \(\Lambda_{t}\).

_Section 7.9_

**7.16** Verify, as stated in (7.179), the imaginary part of a \(k\times k\) spectral matrix, \(f^{im}(\omega)\), is skew symmetric, and then show \(\beta^{\prime}f^{im}_{yy}(\omega)\beta=0\) for a real \(k\times 1\) vector, \(\beta\).

**7.17** Repeat the analysis of Example 7.17 on BNRF1 of herpesvirus saimiri (the data file is bnrflhvs), and compare the results with the results obtained for Epstein-Barr.

**7.18** For the S&P500 weekly returns, say, \(r_{t}\), analyzed in Example 6.17

(a) Estimate the spectrum of the \(r_{t}\). Does the spectral estimate appear to support the hypothesis that the returns are white? (b) Examine the possibility of spectral power near the zero frequency for a transformation of the returns, say, \(g(r_{t})\), using the spectral envelope with Example 7.18 as your guide. Compare the optimal transformation near or at the zero frequency with the usual transformation \(y_{t}=r_{t}^{2}\).

## Appendix A Large Sample Theory

### Convergence Modes

The study of the optimality properties of various estimators (such as the sample autocorrelation function) depends, in part, on being able to assess the large-sample behavior of these estimators. We summarize briefly here the kinds of convergence useful in this setting, namely, _mean square convergence_, _convergence in probability_, and _convergence in distribution_.

We consider first a particular class of random variables that plays an important role in the study of _second-order time series_, namely, the class of random variables belonging to the space \(L^{2}\), satisfying \(\mathrm{E}|x|^{2}<\infty\). In proving certain properties of the class \(L^{2}\) we will often use, for random variables \(x\), \(y\in L^{2}\), the _Cauchy-Schwarz inequality_,

\[|\mathrm{E}(xy)|^{2}\leq\mathrm{E}(|x|^{2})\mathrm{E}(|y|^{2}),\] (A.1)

and the _Tchebycheff inequality_,

\[\Pr\{|x|\geq a\}\leq\frac{\mathrm{E}(|x|^{2})}{a^{2}},\] (A.2)

for \(a>0\).

Next, we investigate the properties of _mean square convergence_ of random variables in \(L^{2}\).

**Definition A.1**_A sequence of \(L^{2}\) random variables \(\{x_{n}\}\) is said to converge in_ **mean square** _to a random variable \(x\in L^{2}\), denoted by_

\[x_{n}\stackrel{{ ms}}{{\rightarrow}}x,\] (A.3)

_if and only if_

\[\mathrm{E}|x_{n}-x|^{2}\to 0\] (A.4)

_as \(n\rightarrow\infty\)._

**Example A.1**: **Mean Square Convergence of the Sample Mean**

Consider the white noise sequence \(w_{t}\) and the _signal plus noise_ series

\[x_{t}=\mu+w_{t}.\]

Then, because

\[\mathrm{E}|\bar{x}_{n}-\mu|^{2}=\frac{\sigma_{w}^{2}}{n}\to 0\]

as \(n\to\infty\), where \(\bar{x}_{n}=n^{-1}\sum_{t=1}^{n}x_{t}\) is the sample mean, we have \(\bar{x}_{n}\xrightarrow{ms}\mu\).

We summarize some of the properties of mean square convergence as follows. If \(x_{n}\xrightarrow{ms}x\), and \(y_{n}\xrightarrow{ms}y\), then, as \(n\to\infty\),

\[\mathrm{E}(x_{n})\to\mathrm{E}(x);\] (A.5) \[\mathrm{E}(|x_{n}|^{2})\to\mathrm{E}(|x|^{2});\] (A.6) \[\mathrm{E}(x_{n}y_{n})\to\mathrm{E}(xy).\] (A.7)

We also note the \(L^{2}\) completeness theorem known as the _Riesz-Fischer Theorem_.

**Theorem A.1**: _Let \(\{x_{n}\}\) be a sequence in \(L^{2}\). Then, there exists a \(x\) in \(L^{2}\) such that \(x_{n}\xrightarrow{ms}x\) if and only if_

\[\lim_{m\to\infty}\sup_{n\geq m}\mathrm{E}|x_{n}-x_{m}|^{2}=0\,.\] (A.8)

Often the condition of Theorem A.1 is easier to verify to establish that a mean square limit \(x\) exists without knowing what it is. Sequences that satisfy (A.8) are said to be _Cauchy sequences_ in \(L^{2}\) and (A.8) is also known as the _Cauchy criterion_ for \(L^{2}\).

**Example A.2**: **Time Invariant Linear Filter**

As an important example of the use of the Riesz-Fisher Theorem and the properties of mean square convergent series given in (A.5)-(A.7), a _time-invariant linear filter_ is defined as a convolution of the form

\[y_{t}=\sum_{j=-\infty}^{\infty}a_{j}x_{t-j}\] (A.9)

for each \(t=0,\pm 1,\pm 2,\ldots\), where \(x_{t}\) is a weakly stationary input series with mean \(\mu_{x}\) and autocovariance function \(\gamma_{x}(h)\), and \(a_{j}\), for \(j=0,\pm 1,\pm 2,\ldots\) are constants satisfying

\[\sum_{j=-\infty}^{\infty}|a_{j}|<\infty.\] (A.10)

The output series \(y_{t}\) defines a _filtering_ or _smoothing_ of the input series that changes the character of the time series in a predictable way. We need to know the conditions under which the outputs \(y_{t}\) in (A.9) and the linear process (1.31) exist.

Considering the sequence \[y_{t}^{n}=\sum_{j=-n}^{n}a_{j}x_{t-j},\] (A.11) \(n=1,2,\ldots\), we need to show first that \(y_{t}^{n}\) has a mean square limit. By Theorem A.1, it is enough to show that \[\mathrm{E}\left|y_{t}^{n}-y_{t}^{m}\right|^{2}\to 0\] as \(m,n\to\infty\). For \(n>m>0\), \[\mathrm{E}\left|y_{t}^{n}-y_{t}^{m}\right|^{2} =\mathrm{E}\Big{|}\sum_{m<|j|\leq n}a_{j}x_{t-j}\Big{|}^{2}\] \[=\sum_{m<|j|\leq n}\sum_{m\leq|k|\leq n}a_{j}a_{k}\mathrm{E}(x_{t -j}x_{t-k})\] \[\leq\sum_{m<|j|\leq n}\sum_{m\leq|k|\leq n}|a_{j}||a_{k}||\mathrm{ E}(x_{t-j}x_{t-k})|\] \[\leq\sum_{m<|j|\leq n}\sum_{m\leq|k|\leq n}|a_{j}||a_{k}|(\mathrm{ E}|x_{t-j}|^{2})^{1/2}(\mathrm{E}|x_{t-k}|^{2})^{1/2}\] \[=\left[\gamma_{x}(0)+\mu_{x}^{2}\right]\left(\sum_{m\leq|j|\leq n} |a_{j}|\right)^{2}\to 0\] as \(m,n\to\infty\), because \(\gamma_{x}(0)\) is a constant and \(\{a_{j}\}\) is absolutely summable (the second inequality follows from the Cauchy-Schwarz inequality). Although we know that the sequence \(\{y_{t}^{n}\}\) given by (A.11) converges in mean square, we have not established its mean square limit. If \(S\) denotes the mean square limit of \(y_{t}^{n}\), then using Fatou's Lemma, \(\mathrm{E}|S-y_{t}|^{2}=\mathrm{E}\liminf_{n\to\infty}|S-y_{t}^{n}|^{2}\leq \liminf_{n\to\infty}\mathrm{E}|S-y_{t}^{n}|^{2}=0\), which establishes that \(y_{t}\) is the mean square limit of \(y_{t}^{n}\). Finally, we may use (A.5) and (A.7) to establish the mean, \(\mu_{y}\) and autocovariance function, \(\gamma_{y}(h)\) of \(y_{t}\). In particular we have, \[\mu_{y}=\mu_{x}\sum_{j=-\infty}^{\infty}a_{j},\] (A.12) and \[\gamma_{y}(h) =\mathrm{E}\sum_{j=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{ j}(x_{t+h-j}-\mu_{x})a_{j}(x_{t-k}-\mu_{x})\] \[=\sum_{j=-\infty}^{\infty}\sum_{k=-\infty}^{\infty}a_{j}\gamma_{ x}(h-j+k)a_{k}.\] (A.13)

A second important kind of convergence is _convergence in probability_.

**Definition A.2**: _The sequence \(\{x_{n}\}\), for \(n=1,2,\ldots\),_ **converges in probability** _to a random variable \(x\), denoted by_

\[x_{n}\stackrel{{ P}}{{\rightarrow}}x,\] (A.14)

_if and only if_

\[\Pr\{|x_{n}-x|>\epsilon\}\to 0\] (A.15)

_for all \(\epsilon>0\), as \(n\rightarrow\infty\)._

An immediate consequence of the Tchebycheff inequality, (A.2), is that

\[\Pr\{|x_{n}-x|\geq\epsilon\}\leq\frac{\mathrm{E}(|x_{n}-x|^{2})}{\epsilon^{2}},\]

so convergence in mean square implies convergence in probability, i.e.,

\[x_{n}\stackrel{{ ms}}{{\rightarrow}}x\ \Rightarrow\ x_{n} \stackrel{{ p}}{{\rightarrow}}x.\] (A.16)

This result implies, for example, that the filter (A.9) exists as a limit in probability because it converges in mean square [it is also easily established that (A.9) exists with probability one]. We mention, at this point, the useful _Weak Law of Large Numbers_ which states that, for an independent identically distributed sequence \(x_{n}\) of random variables with mean \(\mu\), we have

\[\bar{x}_{n}\stackrel{{ P}}{{\rightarrow}}\mu\] (A.17)

as \(n\rightarrow\infty\), where \(\bar{x}_{n}=n^{-1}\sum_{t=1}^{n}x_{t}\) is the usual sample mean.

We also will make use of the following concepts.

**Definition A.3**: _For_ **order in probability** _we write_

\[x_{n}=o_{p}(a_{n})\] (A.18)

_if and only if_

\[\frac{x_{n}}{a_{n}}\stackrel{{ p}}{{\rightarrow}}0.\] (A.19)

_The term_ **boundedness in probability**_, written \(x_{n}=O_{p}(a_{n})\), means that for every \(\epsilon>0\), there exists a \(\delta(\epsilon)>0\) such that_

\[\Pr\left\{\left|\frac{x_{n}}{a_{n}}\right|>\delta(\epsilon)\right\}\leq\epsilon\] (A.20)

_for all \(n\)._

Under this convention, e.g., the notation for \(x_{n}\stackrel{{ p}}{{\rightarrow}}x\) becomes \(x_{n}-x=o_{p}(1)\). The definitions can be compared with their nonrandom counterparts, namely, for a fixed sequence \(x_{n}=o(1)\) if \(x_{n}\to 0\) and \(x_{n}=O(1)\) if \(x_{n}\), for \(n=1,2,\ldots\) is bounded. Some handy properties of \(o_{p}(\cdot)\) and \(O_{p}(\cdot)\) are as follows.

1. If \(x_{n}=o_{p}(a_{n})\) and \(y_{n}=o_{p}(b_{n})\), then \(x_{n}y_{n}=o_{p}(a_{n}b_{n})\) and \(x_{n}+y_{n}=o_{p}(\max(a_{n},b_{n}))\).
2. If \(x_{n}=o_{p}(a_{n})\) and \(y_{n}=O_{p}(b_{n})\), then \(x_{n}y_{n}=o_{p}(a_{n}b_{n})\).
3. Statement (i) is true if \(O_{p}(\cdot)\) replaces \(o_{p}(\cdot)\).

**Example A.3**: **Convergence and Order in Probability for the Sample Mean**

For the sample mean, \(\bar{x}_{n}\), of iid random variables with mean \(\mu\) and variance \(\sigma^{2}\), by the Tchebycheff inequality,

\[\Pr\{|\bar{x}_{n}-\mu|>\epsilon\} \leq \frac{\mathbb{E}[(\bar{x}_{n}-\mu)^{2}]}{\epsilon^{2}}\] \[= \frac{\sigma^{2}}{n\epsilon^{2}}\to 0,\]

as \(n\to\infty\). It follows that \(\bar{x}_{n}\stackrel{{ p}}{{\to}}\mu\), or \(\bar{x}_{n}-\mu=o_{p}(1)\). To find the rate, it follows that, for \(\delta(\epsilon)>0\),

\[\Pr\left\{\sqrt{n}\ |\bar{x}_{n}-\mu|>\delta(\epsilon)\right\}\leq\frac{ \sigma^{2}/n}{\delta^{2}(\epsilon)/n}=\frac{\sigma^{2}}{\delta^{2}(\epsilon)}\]

by Tchebycheff's inequality, so taking \(\epsilon=\sigma^{2}/\delta^{2}(\epsilon)\) shows that \(\delta(\epsilon)=\sigma/\sqrt{\epsilon}\) does the job and

\[\bar{x}_{n}-\mu=O_{p}(n^{-1/2}).\]

For \(k\times 1\) random vectors \(x_{n}\), convergence in probability, written \(x_{n}\stackrel{{ p}}{{\to}}x\) or \(x_{n}-x=o_{p}(1)\) is defined as element-by-element convergence in probability, or equivalently, as convergence in terms of the Euclidean distance

\[\|x_{n}-x\|\stackrel{{ p}}{{\to}}0,\]

where \(\|a\|=\sum_{j}a_{j}^{2}\) for any vector \(a\). In this context, we note the result that if \(x_{n}\stackrel{{ p}}{{\to}}x\) and \(g(x_{n})\) is a continuous mapping,

\[g(x_{n})\stackrel{{ p}}{{\to}}g(x).\]

Furthermore, if \(x_{n}-a=O_{p}(\delta_{n})\) with \(\delta_{n}\to 0\) and \(g(\cdot)\) is a function with continuous first derivatives continuous in a neighborhood of \(a=(a_{1},a_{2},\ldots,a_{k})^{\prime}\), we have the _Taylor series expansion in probability_

\[g(x_{n})=g(a)+\frac{\partial g(x)}{\partial x}\bigg{|}_{x=a}^{\prime}(x_{n}-a )+O_{p}(\delta_{n}),\]

where

\[\frac{\partial g(x)}{\partial x}\bigg{|}_{x=a}=\left(\frac{\partial g(x)}{ \partial x_{1}}\bigg{|}_{x=a},\ldots,\frac{\partial g(x)}{\partial x_{k}} \bigg{|}_{x=a}\right)^{\prime}\]

denotes the vector of partial derivatives with respect to \(x_{1}\), \(x_{2}\), \(\ldots,x_{k}\), evaluated at \(a\

**Example A.4**: **Expansion for the Logarithm of the Sample Mean**

With the same conditions as Example A.3, consider \(g(\bar{x}_{n})=\log\bar{x}_{n}\), which has a derivative at \(\mu\), for \(\mu>0\). Then, because \(\bar{x}_{n}-\mu=O_{P}(n^{-1/2})\) from Example A.3, the conditions for the Taylor expansion in probability, (A.23), are satisfied and we have

\[\log\bar{x}_{n}=\log\mu+\mu^{-1}(\bar{x}_{n}-\mu)+O_{P}(n^{-1/2}).\]

The large sample distributions of sample mean and sample autocorrelation functions defined earlier can be developed using the notion of convergence in distribution.

**Definition A.4**: _A sequence of \(k\times 1\) random vectors \(\{x_{n}\}\) is said to_ **converge in distribution**_, written_

\[x_{n}\overset{d}{\rightarrow}x\] (A.24)

_if and only if_

\[F_{n}(x)\to F(x)\] (A.25)

_at the continuity points of distribution function \(F(\cdot)\)._

**Example A.5**: **Convergence in Distribution**

Consider a sequence \(\{x_{n}\}\) of iid normal random variables with mean zero and variance \(1/n\). Using the standard normal cdf, \(\Phi(z)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z}\exp\left\{-\frac{1}{2}\mu^{2 }\right\}du\), we have \(F_{n}(z)=\Phi(\sqrt{n}z)\), so

\[F_{n}(z)\rightarrow\begin{cases}0&z<0,\\ 1/2&z=0\\ 1&z>0\end{cases}\]

and we may take

\[F(z)=\begin{cases}0&z<0,\\ 1&z\geq 0,\end{cases}\]

because the point where the two functions differ is not a continuity point of \(F(z)\).

The distribution function relates uniquely to the _characteristic function_ through the Fourier transform, defined as a function with vector argument \(\lambda=(\lambda_{1},\lambda_{2},\ldots,\lambda_{k})^{\prime}\), say

\[\phi(\lambda)=\mathrm{E}(\exp\{i\lambda^{\prime}x\})=\int\exp\{i\lambda^{ \prime}x\}\ dF(x).\] (A.26)

Hence, for a sequence \(\{x_{n}\}\) we may characterize convergence in distribution of \(F_{n}(\cdot)\) in terms of convergence of the sequence of characteristic functions \(\phi_{n}(\cdot)\), i.e.,

\[\phi_{n}(\lambda)\rightarrow\phi(\lambda)\ \Leftrightarrow\ F_{n}(x)\overset{d}{\rightarrow}F(x),\] (A.27)

where \(\Leftrightarrow\) means that the implication goes both directions. In this connection, we have

**Proposition A.1**: **The Cramer-Wold device.** _Let \(\{x_{n}\}\) be a sequence of \(k\times 1\) random vectors. Then, for every \(c=(c_{1},c_{2},\ldots,c_{k})^{\prime}\in\mathbb{R}^{k}\)_

\[c^{\prime}x_{n}\xrightarrow{d}c^{\prime}x\ \Leftrightarrow x_{n}\xrightarrow{d}x.\] (A.28)

Proposition A.1 can be useful because sometimes it easier to show the convergence in distribution of \(c^{\prime}x_{n}\) than \(x_{n}\) directly.

Convergence in probability implies convergence in distribution, namely,

\[x_{n}\xrightarrow{P}x\ \Rightarrow\ x_{n}\xrightarrow{d}x,\] (A.29)

but the converse is only true when \(x_{n}\xrightarrow{d}c\), where \(c\) is a constant vector. If \(x_{n}\xrightarrow{d}x\) and \(y_{n}\xrightarrow{d}c\) are two sequences of random vectors and \(c\) is a constant vector,

\[x_{n}+y_{n}\xrightarrow{d}x+c\qquad\text{and}\qquad y_{n}^{\prime}x_{n} \xrightarrow{d}c^{\prime}x.\] (A.30)

For a continuous mapping \(h(x)\),

\[x_{n}\xrightarrow{d}x\ \Rightarrow\ h(x_{n})\xrightarrow{d}h(x).\] (A.31)

A number of results in time series depend on making a series of approximations to prove convergence in distribution. For example, we have that if \(x_{n}\xrightarrow{d}x\) can be _approximated_ by the sequence \(y_{n}\) in the sense that

\[y_{n}-x_{n}=o_{P}(1),\] (A.32)

then we have that \(y_{n}\xrightarrow{d}x\), so the approximating sequence \(y_{n}\) has the same limiting distribution as \(x\). We present the following _Basic Approximation Theorem (BAT)_ that will be used later to derive asymptotic distributions for the sample mean and ACF.

**Theorem A.2** (Basic Approximation Theorem (BAT)): _Let \(x_{n}\) for \(n=1,2,\ldots,\) and \(y_{mn}\) for \(m=1,2,\ldots,\) be random \(k\times 1\) vectors such that_

1. \(y_{mn}\xrightarrow{d}y_{m}\) _as_ \(n\to\infty\) _for each_ \(m\)_;_
2. \(y_{m}\xrightarrow{d}y\) _as_ \(m\to\infty\)_;_
3. \(\lim_{m\to\infty}\limsup_{n\to\infty}\Pr\{|x_{n}-y_{mn}|>\epsilon\}=0\) _for every_ \(\epsilon>0\)_._

_Then, \(x_{n}\xrightarrow{d}y\)._

As a practical matter, the BAT condition (\(iii\)) is implied by the Tchebycheff inequality if

\[(iii^{\prime})\quad\text{E}\{|x_{n}-y_{mn}|^{2}\}\to 0\] (A.33)

as \(m,n\to\infty\), and (\(iii^{\prime}\)) is often much easier to establish than (\(iii\)).

The theorem allows approximation of the underlying sequence in two steps, through the intermediary sequence \(y_{mn}\), depending on two arguments. In the time series case, \(n\) is generally the sample length and \(m\) is generally the number of terms in an approximation to the linear process of the form (A.11).

Proof.: The proof of the theorem is a simple exercise in using the characteristic functions and appealing to (A.27). We need to show

\[|\phi_{x_{n}}-\phi_{y}|\to 0,\]

where we use the shorthand notation \(\phi\equiv\phi(\lambda)\) for ease. First,

\[|\phi_{x_{n}}-\phi_{y}|\leq|\phi_{x_{n}}-\phi_{y_{mn}}|+|\phi_{y_{mn}}-\phi_{y_ {m}}|+|\phi_{y_{m}}-\phi_{y}|.\] (A.34)

By the condition \((ii)\) and (A.27), the last term converges to zero, and by condition \((i)\) and (A.27), the second term converges to zero and we only need consider the first term in (A.34). Now, write

\[\left|\phi_{x_{n}}-\phi_{y_{mn}}\right| =\left|\mathrm{E}(e^{i\lambda^{\prime}x_{n}}-e^{i\lambda^{\prime} y_{mn}})\right|\] \[\leq\mathrm{E}\left|e^{i\lambda^{\prime}x_{n}}\big{(}1-e^{i \lambda^{\prime}(y_{mn}-x_{n})}\big{)}\right|\] \[=\mathrm{E}\left|1-e^{i\lambda^{\prime}(y_{mn}-x_{n})}\right|\] \[=\mathrm{E}\left\{\left|1-e^{i\lambda^{\prime}(y_{mn}-x_{n})} \right|I\{|y_{mn}-x_{n}|<\delta\}\right\}\] \[\quad+\left.\mathrm{E}\left\{\left|1-e^{i\lambda^{\prime}(y_{mn} -x_{n})}\right|I\{|y_{mn}-x_{n}|\geq\delta\}\right.\right\},\]

where \(\delta>0\) and \(I\{A\}\) denotes the indicator function of the set \(A\). Then, given \(\lambda\) and \(\epsilon>0\), choose \(\delta(\epsilon)>0\) such that

\[\left|1-e^{i\lambda^{\prime}(y_{mn}-x_{n})}\right|<\epsilon\]

if \(|y_{mn}-x_{n}|<\delta\), and the first term is less than \(\epsilon\), an arbitrarily small constant. For the second term, note that

\[\left|1-e^{i\lambda^{\prime}(y_{mn}-x_{n})}\right|\leq 2\]

and we have

\[\mathrm{E}\left\{\left|1-e^{i\lambda^{\prime}(y_{mn}-x_{n})}\right|I\{|y_{mn} -x_{n}|\geq\delta\}\right.\right\}\leq 2\Pr\bigl{\{}|y_{mn}-x_{n}|\geq\delta \bigr{\}},\]

which converges to zero as \(n\to\infty\) by property \((iii)\). 

### Central Limit Theorems

We will generally be concerned with the large-sample properties of estimators that turn out to be normally distributed as \(n\to\infty\).

**Definition A.5**: _A sequence of random variables \(\{x_{n}\}\) is said to be_ **asymptotically normal** _with mean \(\mu_{n}\) and variance \(\sigma_{n}^{2}\) if, as \(n\to\infty\),_

\[\sigma_{n}^{-1}(x_{n}-\mu_{n})\overset{d}{\to}z,\]

_where \(z\) has the standard normal distribution. We shall abbreviate this as_

\[x_{n}\sim\mathrm{AN}(\mu_{n},\sigma_{n}^{2}),\] (A.35)

_where \(\sim\) will denote_ is distributed as.

We state the important _Central Limit Theorem_, as follows.

**Theorem A.3**: _Let \(x_{1},\ldots,x_{n}\) be independent and identically distributed with mean \(\mu\) and variance \(\sigma^{2}\). If \(\bar{x}_{n}=(x_{1}+\cdots+x_{n})/n\) denotes the sample mean, then_

\[\bar{x}_{n}\sim\mathrm{AN}(\mu,\sigma^{2}/n).\] (A.36)

Often, we will be concerned with a sequence of \(k\times 1\) vectors \(\{x_{n}\}\). The following property is motivated by the Cramer-Wold device, Proposition A.1.

**Proposition A.2**: _A sequence of random vectors is asymptotically normal, i.e.,_

\[x_{n}\sim\mathrm{AN}(\mu_{n},\Sigma_{n}),\] (A.37)

_if and only if_

\[c^{\prime}x_{n}\sim\mathrm{AN}(c^{\prime}\mu_{n},c^{\prime}\Sigma_{n}c)\] (A.38)

_for all \(c\in\mathbb{R}^{k}\) and \(\Sigma_{n}\) is positive definite._

In order to begin to consider what happens for dependent data in the limiting case, it is necessary to define, first of all, a particular kind of dependence known as \(M\)-dependence. We say that a time series \(x_{t}\) is \(M\)-dependent if the set of values \(x_{s}\), \(s\leq t\) is independent of the set of values \(x_{s}\), \(s\geq t+M+1\), so time points separated by more than \(M\) units are independent. A central limit theorem for such dependent processes, used in conjunction with the Basic Approximation Theorem, will allow us to develop large-sample distributional results for the sample mean \(\bar{x}\) and the sample ACF \(\widehat{\rho}_{x}(h)\) in the stationary case.

In the arguments that follow, we often make use of the formula for the variance of \(\bar{x}_{n}\) in the stationary case, namely,

\[\mathrm{var}\ \bar{x}_{n}=n^{-1}\sum_{u=-(n-1)}^{(n-1)}\left(1-\frac{|u|}{n} \right)\gamma(u),\] (A.39)

which was established in (1.35) on page 1. We shall also use the fact that, for

\[\sum_{u=-\infty}^{\infty}|\gamma(u)|\ <\infty,\]we would have, by dominated convergence,1

Footnote 1: Dominated convergence technically relates to convergent sequences (with respect to a sigma-additive measure \(\mu\)) of measurable functions \(f_{n}\to f\) bounded by an integrable function \(g,\int g\ d\mu<\infty\). For such a sequence,

\[\int f_{n}\ d\mu\rightarrow\int f\ d\mu.\]

 For the case in point, take \(f_{n}(u)=(1-|u|/n)\gamma(u)\) for \(|u|<n\) and as zero for \(|u|\geq n\). Take \(\mu(u)=1,\,u=\pm 1,\,\pm 2,\,\ldots\) to be counting measure.

1. Applying the Central Limit Theorem to the sum \(y_{mn}\) gives \[y_{mn}=n^{-1/2}\sum_{i=1}^{r}z_{i}=(n/r)^{-1/2}r^{-1/2}\sum_{i=1}^{r}z_{i}.\] Because \((n/r)^{-1/2}\to m^{1/2}\) and \[r^{-1/2}\sum_{i=1}^{r}z_{i}\stackrel{{ d}}{{\to}}N(0,S_{m-M}),\] it follows from (A.30) that \[y_{mn}\stackrel{{ d}}{{\to}}y_{m}\sim N(0,S_{m-M}/m).\] as \(n\to\infty\), for a fixed \(m\).
2. Note that as \(m\to\infty\), \(S_{m-M}/m\to V_{M}\) using dominated convergence, where \(V_{M}\) is defined in (A.41). Hence, the characteristic function of \(y_{m}\), say, \[\phi_{m}(\lambda)=\exp\biggl{\{}-\frac{1}{2}\lambda^{2}\;\frac{S_{m-M}}{m} \biggr{\}}\to\exp\biggl{\{}-\frac{1}{2}\lambda^{2}\;V_{M}\biggr{\}},\] as \(m\to\infty\), which is the characteristic function of a random variable \(y\sim N(0,V_{M})\) and the result follows because of (A.27).
3. To verify the last condition of the BAT theorem, \[n^{1/2}\tilde{x}_{n}-y_{mn} = n^{-1/2}[(x_{m-M+1}+\cdots+x_{m})\] \[+(x_{2m-M+1}+\cdots+x_{2m})\] \[+(x_{(r-1)m-M+1}+\cdots+x_{(r-1)m})\] \[\vdots\] \[+(x_{rm-M+1}+\cdots+x_{n})]\] \[=n^{-1/2}(w_{1}+w_{2}+\cdots+w_{r}),\] so the error is expressed as a scaled sum of iid variables with variance \(S_{M}\) for the first \(r-1\) variables and \[\text{var}(w_{r}) = \sum_{|u|\leq m-M}\biggl{(}n-[n/m]m+M-|u|\biggr{)}\gamma(u)\] \[\leq\sum_{|u|\leq m-M}(m+M-|u|)\gamma(u).\] Hence, \[\text{var}\;[n^{1/2}\tilde{x}-y_{mn}]=n^{-1}[(r-1)S_{M}+\text{var}\;w_{r}],\] which converges to \(m^{-1}S_{M}\) as \(n\to\infty\). Because \(m^{-1}S_{M}\to 0\) as \(m\to\infty\), the condition of (iii) holds by the Tchebycheff inequality.

### The Mean and Autocorrelation Functions

The background material in the previous two sections can be used to develop the asymptotic properties of the sample mean and ACF used to evaluate statistical significance. In particular, we are interested in verifying Property 1.2.

We begin with the distribution of the sample mean \(\bar{x}_{n}\), noting that (A.40) suggests a form for the limiting variance. In all of the asymptotics, we will use the assumption that \(x_{t}\) is a linear process, as defined in Definition 1.12, but with the added condition that \(\{w_{t}\}\) is iid. That is, throughout this section we assume

\[x_{t}=\mu_{x}+\sum_{j=-\infty}^{\infty}\psi_{j}w_{t-j}\] (A.43)

where \(w_{t}\sim\mathrm{iid}(0,\sigma_{w}^{2})\), and the coefficients satisfy

\[\sum_{j=-\infty}^{\infty}|\psi_{j}|<\infty.\] (A.44)

Before proceeding further, we should note that the exact sampling distribution of \(\bar{x}_{n}\) is available if the distribution of the underlying vector \(x=(x_{1},x_{2},\ldots,x_{n})^{\prime}\) is multivariate normal. Then, \(\bar{x}_{n}\) is just a linear combination of jointly normal variables that will have the normal distribution

\[\bar{x}_{n}\sim N\left(\mu_{x},\ n^{-1}\sum_{|u|<n}\left(1-\frac{|u|}{n} \right)\gamma_{x}(u)\right),\] (A.45)

by (A.39). In the case where \(x_{t}\) are not jointly normally distributed, we have the following theorem.

**Theorem A.5**: _If \(x_{t}\) is a linear process of the form (A.43) and \(\sum_{j}\psi_{j}\neq 0\), then_

\[\bar{x}_{n}\sim\mathrm{AN}(\mu_{x},n^{-1}V),\] (A.46)

_where_

\[V=\sum_{h=-\infty}^{\infty}\gamma_{x}(h)=\sigma_{w}^{2}\left(\sum_{j=-\infty}^ {\infty}\psi_{j}\right)^{2}\] (A.47)

_and \(\gamma_{x}(\cdot)\) is the autocovariance function of \(x_{t}\)._

_Proof:_ To prove the above, we can again use the Basic Approximation Theorem, Theorem A.2, by first defining the strictly stationary \(2m\)-dependent linear process with finite limits

\[x_{t}^{m}=\sum_{j=-m}^{m}\psi_{j}w_{t-j}\]as an approximation to \(x_{t}\) to use in the approximating mean

\[\bar{x}_{n,m}=n^{-1}\sum_{t=1}^{n}x_{t}^{m}.\]

Then, take

\[y_{mn}=n^{1/2}(\bar{x}_{n,m}-\mu_{x})\]

as an approximation to \(n^{1/2}(\bar{x}_{n}-\mu_{x})\).

1. Applying Theorem A.4, we have \[y_{mn}\overset{d}{\to}y_{m}\sim N(0,V_{m}),\] as \(n\to\infty\), where \[V_{m}=\sum_{h=-2m}^{2m}\gamma_{x}(h)=\sigma_{w}^{2}\left(\sum_{j=-m}^{m}\psi_{ j}\right)^{2}.\] To verify the above, we note that for the general linear process with infinite limits, (1.32) implies that \[\sum_{h=-\infty}^{\infty}\gamma_{x}(h)=\sigma_{w}^{2}\sum_{h=-\infty}^{\infty} \sum_{j=-\infty}^{\infty}\psi_{j+h}\psi_{j}=\sigma_{w}^{2}\left(\sum_{j=-\infty }^{\infty}\psi_{j}\right)^{2},\] so taking the special case \(\psi_{j}=0\), for \(|j|>m\), we obtain \(V_{m}\).
2. Because \(V_{m}\to V\) in (A.47) as \(m\to\infty\), we may use the same characteristic function argument as under (ii) in the proof of Theorem A.4 to note that \[y_{m}\overset{d}{\to}y\sim N(0,V),\] where \(V\) is given by (A.47).
3. Finally, \[\text{var}\left[n^{1/2}(\bar{x}_{n}-\mu_{x})-y_{mn}\right] =n\text{ var}\left[n^{-1}\sum_{t=1}^{n}\sum_{|j|>m}\psi_{j}w_{t-j}\right]\] as \(m\to\infty\).

In order to develop the sampling distribution of the sample autocovariance function, \(\widetilde{\gamma}_{x}(h)\), and the sample autocorrelation function, \(\widetilde{\rho}_{x}(h)\), we need to develop some idea as to the mean and variance of \(\widehat{\gamma}_{x}(h)\) under some reasonable assumptions. These computations for \(\widetilde{\gamma}_{x}(h)\) are messy, and we consider a comparable quantity

\[\widetilde{\gamma}_{x}(h)=n^{-1}\sum_{t=1}^{n}(x_{t+h}-\mu_{x})(x_{t}-\mu_{x}) \tag{111}\]

as an approximation. By Problem 1.30,

\[n^{1/2}[\widetilde{\gamma}_{x}(h)-\widehat{\gamma}_{x}(h)]=o_{p}(1),\]

so that limiting distributional results proved for \(n^{1/2}\widetilde{\gamma}_{x}(h)\) will hold for \(n^{1/2}\widetilde{\gamma}_{x}(h)\) by (109).

We begin by proving formulas for the variance and for the limiting variance of \(\widetilde{\gamma}_{x}(h)\) under the assumptions that \(x_{t}\) is a linear process of the form (102), satisfying (103) with the white noise variates \(w_{t}\) having variance \(\sigma_{w}^{2}\) as before, but also required to have fourth moments satisfying

\[\mathrm{E}(w_{t}^{4})=\eta\sigma_{w}^{4}<\infty, \tag{112}\]

where \(\eta\) is some constant. We seek results comparable with (111) and (112) for \(\widetilde{\gamma}_{x}(h)\). To ease the notation, we will henceforth drop the subscript \(x\) from the notation.

Using (111), \(\mathrm{E}[\widetilde{\gamma}(h)]=\gamma(h)\). Under the above assumptions, we show now that, for \(p,q=0,1,2,\ldots\),

\[\mathrm{cov}\;[\widetilde{\gamma}(p),\widetilde{\gamma}(q)]=n^{-1}\sum_{u=-(n- 1)}^{(n-1)}\biggl{(}1-\frac{|u|}{n}\biggr{)}V_{u}, \tag{113}\]

where

\[V_{u} = \gamma(u)\gamma(u+p-q)+\gamma(u+p)\gamma(u-q) \tag{114}\] \[+(\eta-3)\sigma_{w}^{4}\sum_{i}\psi_{i+u+q}\psi_{i+u}\psi_{i+p} \psi_{i}.\]

The absolute summability of the \(\psi_{j}\) can then be shown to imply the absolute summability of the \(V_{u}\).2 Thus, the dominated convergence theorem implies

Footnote 2: Note: \(\sum_{j=-\infty}^{\infty}|a_{j}|<\infty\) and \(\sum_{j=-\infty}^{\infty}|b_{j}|<\infty\) implies \(\sum_{j=-\infty}^{\infty}|a_{j}b_{j}|<\infty\).

\[n\;\mathrm{cov}\;[\widetilde{\gamma}(p),\widetilde{\gamma}(q)] \to \sum_{u=-\infty}^{\infty}V_{u} \tag{115}\] \[= (\eta-3)\gamma(p)\gamma(q)\] \[+\;\sum_{u=-\infty}^{\infty}\biggl{[}\gamma(u)\gamma(u+p-q)+ \gamma(u+p)\gamma(u-q)\biggr{]}.\]

To verify (113) is somewhat tedious, so we only go partially through the calculations, leaving the repetitive details to the reader. First, rewrite (102) as

\[x_{t}=\mu+\sum_{i=-\infty}^{\infty}\psi_{t-i}w_{i},\]so that

\[\mathrm{E}[\widetilde{\gamma}(p)\widetilde{\gamma}(q)]=n^{-2}\sum_{s,t}\sum_{i,j,k, \ell}\psi_{s+p-i}\psi_{s-j}\psi_{t+q-k}\ \psi_{t-\ell}\mathrm{E}(w_{i}w_{j}w_{k}w_{\ell}).\]

Then, evaluate, using the easily verified properties of the \(w_{t}\) series

\[\mathrm{E}(w_{i}w_{j}w_{k}w_{\ell})=\begin{cases}\eta\sigma_{w}^{4}&\text{if} \ i=j=k=\ell\\ \sigma_{w}^{4}&\text{if}\ i=j\neq k=\ell\\ 0&\text{if}\ i\neq j,i\neq k\text{ and }i\neq\ell.\end{cases}\]

To apply the rules, we break the sum over the subscripts \(i,j,k,\ell\) into four terms, namely,

\[\sum_{i,j,k,\ell}=\sum_{i=j=k=\ell}+\sum_{i=j\neq k=\ell}+\sum_{i=k\neq j=\ell }+\sum_{i=\ell\neq j=k}=S_{1}+S_{2}+S_{3}+S_{4}.\]

Now,

\[S_{1}=\eta\sigma_{w}^{4}\sum_{i}\psi_{s+p-i}\psi_{s-i}\psi_{t+q-i}\psi_{t-i}= \eta\sigma_{w}^{4}\sum_{i}\psi_{i+s-t+p}\psi_{i+s-t}\psi_{i+q}\psi_{i},\]

where we have let \(i^{\prime}=t-i\) to get the final form. For the second term,

\[S_{2} =\sum_{i=j\neq k=\ell}\psi_{s+p-i}\psi_{s-j}\psi_{t+q-k}\psi_{t- \ell}\mathrm{E}(w_{i}w_{j}w_{k}w_{\ell})\] \[=\sum_{i\neq k}\psi_{s+p-i}\psi_{s-i}\psi_{t+q-k}\psi_{t-k} \mathrm{E}(w_{i}^{2})\mathrm{E}(w_{k}^{2}).\]

Then, using the fact that

\[\sum_{i\neq k}=\sum_{i,k}-\sum_{i=k},\]

we have

\[S_{2} =\sigma_{w}^{4}\sum_{i,k}\psi_{s+p-i}\psi_{s-i}\psi_{t+q-k}\psi_{t -k}-\sigma_{w}^{4}\sum_{i}\psi_{s+p-i}\psi_{s-i}\psi_{t+q-i}\psi_{t-i}\] \[=\gamma(p)\gamma(q)-\sigma_{w}^{4}\sum_{i}\psi_{i+s-t+p}\psi_{i+s -t}\psi_{i+q}\psi_{i},\]

letting \(i^{\prime}=s-i,k^{\prime}=t-k\) in the first term and \(i^{\prime}=s-i\) in the second term. Repeating the argument for \(S_{3}\) and \(S_{4}\) and substituting into the covariance expression yields

\[\mathrm{E}[\widetilde{\gamma}(p)\widetilde{\gamma}(q)] =n^{-2}\sum_{s,t}\biggl{[}\gamma(p)\gamma(q)+\gamma(s-t)\gamma(s- t+p-q)\] \[\quad+\gamma(s-t+p)\gamma(s-t-q)\] \[\quad+(\eta-3)\sigma_{w}^{4}\sum_{i}\psi_{i+s-t+p}\psi_{i+s-t} \psi_{i+q}\psi_{i}\biggr{]}.\]Then, letting \(u=s-t\) and subtracting \(\mathrm{E}[\tilde{\gamma}(p)]\mathrm{E}[\tilde{\gamma}(q)]=\gamma(p)\gamma(q)\) from the summation leads to the result (A.51). Summing (A.51) over \(u\) and applying dominated convergence leads to (A.52).

The above results for the variances and covariances of the approximating statistics \(\widetilde{\gamma}(\cdot)\) enable proving the following central limit theorem for the autocovariance functions \(\widetilde{\gamma}(\cdot)\).

**Theorem A.6**: _If \(x_{t}\) is a stationary linear process of the form (A.43) satisfying the fourth moment condition (A.49), then, for fixed \(K\),_

\[\begin{pmatrix}\widetilde{\gamma}(0)\\ \widetilde{\gamma}(1)\\ \vdots\\ \widetilde{\gamma}(K)\end{pmatrix}\sim\mathrm{AN}\left[\begin{pmatrix}\gamma(0 )\\ \gamma(1)\\ \vdots\\ \gamma(K)\end{pmatrix},n^{-1}V\right],\]

_where \(V\) is the matrix with elements given by_

\[v_{pq} = (\eta-3)\gamma(p)\gamma(q)\] (A.53) \[+\sum_{u=-\infty}^{\infty}\left[\gamma(u)\gamma(u-p+q)+\gamma(u+ q)\gamma(u-p)\right].\]

_Proof:_ It suffices to show the result for the approximate autocovariance (A.48) for \(\widetilde{\gamma}(\cdot)\) by the remark given below it (see also Problem 1.30). First, define the strictly stationary \((2m+K)\)-dependent \((K+1)\times 1\) vector

\[y_{t}^{m}=\begin{pmatrix}(x_{t}^{m}-\mu)^{2}\\ (x_{t+1}^{m}-\mu)(x_{t}^{m}-\mu)\\ \vdots\\ (x_{t+K}^{m}-\mu)(x_{t}^{m}-\mu)\end{pmatrix},\]

where

\[x_{t}^{m}=\mu+\sum_{j=-m}^{m}\psi_{j}w_{t-j}\]

is the usual approximation. The sample mean of the above vector is

\[\widetilde{\gamma}_{mn}=n^{-1}\sum_{t=1}^{n}y_{t}^{m}=\begin{pmatrix}\widetilde {\gamma}^{mn}(0)\\ \widetilde{\gamma}^{mn}(1)\\ \vdots\\ \widetilde{\gamma}^{mn}(K)\end{pmatrix},\]

where

\[\widetilde{\gamma}^{mn}(h)=n^{-1}\sum_{t=1}^{n}(x_{t+h}^{m}-\mu)(x_{t}^{m}-\mu)\]denotes the sample autocovariance of the approximating series. Also,

\[\mathrm{Ey}_{t}^{m}=\begin{pmatrix}\gamma^{m}(0)\\ \gamma^{m}(1)\\ \vdots\\ \gamma^{m}(K)\end{pmatrix},\]

where \(\gamma^{m}(h)\) is the theoretical covariance function of the series \(x_{t}^{m}\). Then, consider the vector

\[y_{mn}=n^{1/2}[\bar{y}_{mn}-\mathrm{E}(\bar{y}_{mn})]\]

as an approximation to

\[y_{n}=n^{1/2}\left[\begin{pmatrix}\tilde{\gamma}(0)\\ \tilde{\gamma}(1)\\ \vdots\\ \tilde{\gamma}(K)\end{pmatrix}-\begin{pmatrix}\gamma(0)\\ \gamma(1)\\ \vdots\\ \gamma(K)\end{pmatrix}\right],\]

where \(\mathrm{E}(\bar{y}_{mn})\) is the same as \(\mathrm{E}(y_{t}^{m})\) given above. The elements of the vector approximation \(y_{mn}\) are clearly \(n^{1/2}(\tilde{\gamma}^{mn}(h)-\tilde{\gamma}^{m}(h))\). Note that the elements of \(y_{n}\) are based on the linear process \(x_{t}\), whereas the elements of \(y_{mn}\) are based on the \(m\)-dependent linear process \(x_{t}^{m}\). To obtain a limiting distribution for \(y_{n}\), we apply the Basic Approximation Theorem, Theorem A.2, using \(y_{mn}\) as our approximation. We now verify (i), (ii), and (iii) of Theorem A.2.

1. First, let \(c\) be a \((K+1)\times 1\) vector of constants, and apply the central limit theorem to the \((2m+K)\)-dependent series \(c^{\prime}y_{mn}\) using the Cramer-Wold device (A.28). We obtain \[c^{\prime}y_{mn}=n^{1/2}c^{\prime}[\bar{y}_{mn}-\mathrm{E}(\bar{y}_{mn})] \overset{d}{\to}c^{\prime}y_{m}\sim N(0,c^{\prime}V_{m}c),\] as \(n\to\infty\), where \(V_{m}\) is a matrix containing the finite analogs of the elements \(v_{pq}\) defined in (A.53).
2. Note that, since \(V_{m}\to V\) as \(m\to\infty\), it follows that \[c^{\prime}y_{m}\overset{d}{\to}c^{\prime}y\sim N(0,c^{\prime}Vc),\] so, by the Cramer-Wold device, the limiting \((K+1)\times 1\) multivariate normal variable is \(N(0,V)\).
3. For this condition, we can focus on the element-by-element components of \[\Pr\bigl{\{}|y_{n}-y_{mn}|>\epsilon\bigr{\}}.\] For example, using the Tchebycheff inequality, the \(h\)-th element of the probability statement can be bounded by \[n\epsilon^{-2}\mathrm{var}\,(\tilde{\gamma}(h)-\tilde{\gamma}^{m}(h)) \\ =\epsilon^{-2}\left\{n\,\mathrm{var}\,\tilde{\gamma}(h)+n\, \mathrm{var}\,\tilde{\gamma}^{m}(h)-2n\,\mathrm{cov}\bigl{[}\tilde{\gamma}(h), \tilde{\gamma}^{m}(h)\bigr{]}\right\}.\]Using the results that led to (A.52), we see that the preceding expression approaches

\[(v_{hh}+v_{hh}-2v_{hh})/\epsilon^{2}=0,\]

as \(m,n\to\infty\).

To obtain a result comparable to Theorem A.6 for the autocorrelation function ACF, we note the following theorem.

**Theorem A.7**: _If \(x_{t}\) is a stationary linear process of the form (1.31) satisfying the fourth moment condition (A.49), then for fixed \(K\),_

\[\begin{pmatrix}\widetilde{\rho}(1)\\ \vdots\\ \widetilde{\rho}(K)\end{pmatrix}\sim\mathrm{AN}\begin{bmatrix}\rho(1)\\ \vdots\\ \rho(K)\end{bmatrix},n^{-1}W\Bigg{]},\]

_where \(W\) is the matrix with elements given by_

\[w_{pq} = \sum_{u=-\infty}^{\infty}\Bigg{[}\rho(u+p)\rho(u+q)+\rho(u-p)\rho( u+q)+2\rho(p)\rho(q)\rho^{2}(u)\] (A.54) \[\qquad-2\rho(p)\rho(u)\rho(u+q)-2\rho(q)\rho(u)\rho(u+p)\Bigg{]}\] \[= \sum_{u=1}^{\infty}[\rho(u+p)+\rho(u-p)-2\rho(p)\rho(u)]\] \[\qquad\times[\rho(u+q)+\rho(u-q)-2\rho(q)\rho(u)],\]

_where the last form is more convenient._

_Proof:_ To prove the theorem, we use the delta method3 for the limiting distribution of a function of the form

Footnote 3: The _delta method_ states that if a \(k\)-dimensional vector sequence \(x_{n}\sim\mathrm{AN}(\mu,a_{n}^{2}\Sigma)\), with \(a_{n}\to 0\), and \(g(x)\) is an \(r\times 1\) continuously differentiable vector function of \(x\), then \(g(x_{n})\sim\mathrm{AN}(g(\mu),a_{n}^{2}D\Sigma D^{\prime})\) where \(D\) is the \(r\times k\) matrix with elements \(d_{ij}=\frac{\partial g_{i}(x)}{\partial x_{j}}\big{|}_{\mu}\).

\[g(x_{0},x_{1},\ldots,x_{K})=(x_{1}/x_{0},\ldots,x_{K}/x_{0})^{\prime},\]

where \(x_{h}=\widehat{\gamma}(h)\), for \(h=0,1,\ldots,K\). Hence, using the delta method and Theorem A.6,

\[g(\widehat{\gamma}(0),\widehat{\gamma}(1),\ldots,\widehat{\gamma}(K))=( \widehat{\rho}(1),\ldots,\widehat{\rho}(K))^{\prime}\]

is asymptotically normal with mean vector \((\rho(1),\ldots,\rho(K))^{\prime}\) and covariance matrix

\[n^{-1}W=n^{-1}DVD^{\prime},\]where \(V\) is defined by (A.53) and \(D\) is the \((K+1)\times K\) matrix of partial derivatives

\[D=\frac{1}{x_{0}^{2}}\begin{pmatrix}-x_{1}&x_{0}&0&\ldots&0\\ -x_{2}&0&x_{0}&\ldots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ -x_{K}&0&0&\ldots&x_{0}\end{pmatrix}\]

Substituting \(\gamma(h)\) for \(x_{h}\), we note that \(D\) can be written as the patterned matrix

\[D=\frac{1}{\gamma(0)}\left(-\rho\ I_{K}\right),\]

where \(\rho=(\rho(1),\rho(2),\ldots,\rho(K))^{\prime}\) is the \(K\times 1\) matrix of autocorrelations and \(I_{K}\) is the \(K\times K\) identity matrix. Then, it follows from writing the matrix \(V\) in the partitioned form

\[V=\begin{pmatrix}v_{00}&v_{1}^{\prime}\\ v_{1}&V_{22}\end{pmatrix}\]

that

\[W=\gamma^{-2}(0)\big{[}v_{00}\rho\rho^{\prime}-\rho v_{1}^{\prime}-v_{1}\rho^{ \prime}+V_{22}\big{]},\]

where \(v_{1}=(v_{10},v_{20},\ldots,v_{K0})^{\prime}\) and \(V_{22}=\{v_{pa};\,p,q=1,\ldots,K\}\). Hence,

\[w_{pq} =\gamma^{-2}(0)\big{[}v_{pq}-\rho(p)v_{0q}-\rho(q)v_{p0}+\rho(p) \rho(q)v_{00}\big{]}\] \[=\sum_{u=-\infty}^{\infty}\bigg{[}\rho(u)\rho(u-p+q)+\rho(u-p) \rho(u+q)+2\rho(p)\rho(q)\rho^{2}(u)\] \[\qquad\qquad-2\rho(p)\rho(u)\rho(u+q)-2\rho(q)\rho(u)\rho(u-p) \bigg{]}.\]

Interchanging the summations, we get the \(w_{pq}\) specified in the statement of the theorem, finishing the proof. 

Specializing the theorem to the case of interest in this chapter, we note that if \(\{x_{t}\}\) is iid with finite fourth moment, then \(w_{pq}=1\) for \(p=q\) and is zero otherwise. In this case, for \(h=1,\ldots,K\), the \(\widehat{\rho}(h)\) are asymptotically independent and jointly normal with

\[\widehat{\rho}(h)\sim\mathrm{AN}(0,n^{-1}).\] (A.55)

This justifies the use of (1.38) and the discussion below it as a method for testing whether a series is white noise.

For the cross-correlation, it has been noted that the same kind of approximation holds and we quote the following theorem for the bivariate case, which can be proved using similar arguments (see Brockwell and Davis, 1991[36], p. 410).

**Theorem A.8**: _If_

\[x_{t}=\sum_{j=-\infty}^{\infty}\alpha_{j}w_{t-j,1}\]and

\[y_{t}=\sum_{j=-\infty}^{\infty}\beta_{j}w_{t-j,2}\]

are two linear processes with absolutely summable coefficients and the two white noise sequences are iid and independent of each other with variances \(\sigma_{1}^{2}\) and \(\sigma_{2}^{2}\), then for \(h\geq 0\),

\[\widehat{\rho}_{xy}(h)\sim\mathrm{AN}\left(\rho_{xy}(h),n^{-1}\sum_{j}\rho_{x} (j)\rho_{y}(j)\right)\] (A.56)

and the joint distribution of \((\widehat{\rho}_{xy}(h),\widehat{\rho}_{xy}(k))^{\prime}\) is asymptotically normal with mean vector zero and

\[\mathrm{cov}\left(\widehat{\rho}_{xy}(h),\widehat{\rho}_{xy}(k)\right)=n^{-1} \sum_{j}\rho_{x}(j)\rho_{y}(j+k-h).\] (A.57)

Again, specializing to the case of interest in this chapter, as long as at least one of the two series is white (iid) noise, we obtain

\[\widehat{\rho}_{xy}(h)\sim\mathrm{AN}\big{(}0,n^{-1}\big{)},\] (A.58)

which justifies Property 1.3.

## Appendix B Time Domain Theory

### Hilbert Spaces and the Projection Theorem

Most of the material on mean square estimation and regression can be embedded in a more general setting involving an inner product space that is also complete (that is, satisfies the Cauchy condition). Two examples of inner products are \(\mathrm{E}(xy^{*})\), where the elements are random variables, and \(\sum x_{i}y_{i}^{*}\), where the elements are sequences. These examples include the possibility of complex elements, in which case, \({}^{*}\) denotes the conjugation. We denote an inner product, in general, by the notation \(\left\langle x,y\right\rangle\). Now, define an _inner product space_ by its properties, namely,

1. \(\left\langle x,y\right\rangle=\left\langle y,x\right\rangle^{*}\)
2. \(\left\langle x+y,z\right\rangle=\left\langle x,z\right\rangle+\left\langle y,z\right\rangle\)
3. \(\left\langle\alpha x,y\right\rangle=\alpha\left\langle x,y\right\rangle\)
4. \(\left\langle x,x\right\rangle=\|x\|^{2}\geq 0\)
5. \(\left\langle x,x\right\rangle=0\) iff \(x=0\).

We introduced the notation \(\|\cdot\|\) for the _norm_ or distance in property (iv). The norm satisfies the _triangle inequality_

\[\|x+y\|\leq\|x\|+\|y\| \tag{1}\]

and the _Cauchy-Schwarz inequality_

\[|\left\langle x,y\right\rangle|^{2}\leq\|x\|^{2}\|y\|^{2}, \tag{2}\]

which we have seen before for random variables in (10). Now, a _Hilbert space_, \(\mathcal{H}\), is defined as an inner product space with the Cauchy property. In other words, \(\mathcal{H}\) is a _complete inner product space_. This means that every Cauchy sequence converges in norm; that is, \(x_{n}\to x\in\mathcal{H}\) if an only if \(\|x_{n}-x_{m}\|\to 0\) as \(m,n\to\infty\). This is just the \(L^{2}\) completeness Theorem A.1 for random variables.

For a broad overview of Hilbert space techniques that are useful in statistical inference and in probability, see Small and McLeish [187]. Also, Brockwell and Davis (1991, Chapter 2)[36] is a nice summary of Hilbert space techniques that are useful in time series analysis. In our discussions, we mainly use the _projection theorem_ (Theorem B.1) and the associated _orthogonality principle_ as a means for solving various kinds of linear estimation problems.

**Theorem B.1** (Projection Theorem): _Let \(\mathcal{M}\) be a closed subspace of the Hilbert space \(\mathcal{H}\) and let \(y\) be an element in \(\mathcal{H}\). Then, \(y\) can be uniquely represented as_

\[y=\hat{y}+z,\] (B.3)

_where \(\hat{y}\) belongs to \(\mathcal{M}\) and \(z\) is orthogonal to \(\mathcal{M}\); that is, \(\left\langle z,w\right\rangle=0\) for all \(w\) in \(\mathcal{M}\). Furthermore, the point \(\hat{y}\) is closest to \(y\) in the sense that, for any \(w\) in \(\mathcal{M}\), \(\|y-w\|\geq\|y-\hat{y}\|\), where equality holds if and only if \(w=\hat{y}\)._

We note that (B.3) and the statement following it yield the _orthogonality property_

\[\left\langle y-\hat{y},w\right\rangle=0\] (B.4)

for any \(w\) belonging to \(\mathcal{M}\), which can sometimes be used easily to find an expression for the projection. The norm of the error can be written as

\[\|y-\hat{y}\|^{2} = \left\langle y-\hat{y},y-\hat{y}\right\rangle\] (B.5) \[= \left\langle y-\hat{y},y\right\rangle-\left\langle y-\hat{y},\hat {y}\right\rangle\] \[= \left\langle y-\hat{y},y\right\rangle\]

because of orthogonality.

Using the notation of Theorem B.1, we call the mapping \(\mathrm{P}_{\mathcal{M}}y=\hat{y}\), for \(y\in\mathcal{H}\), the _projection mapping of \(\mathcal{H}\) onto \(\mathcal{M}\)_. In addition, the _closed span_ of a finite set \(\left\{x_{1},\ldots,x_{n}\right\}\) of elements in a Hilbert space, \(\mathcal{H}\), is defined to be the set of all linear combinations \(w=a_{1}x_{1}+\cdots+a_{n}x_{n}\), where \(a_{1},\ldots,a_{n}\) are scalars. This subspace of \(\mathcal{H}\) is denoted by \(\mathcal{M}=\overline{\mathrm{sp}}\{x_{1},\ldots,x_{n}\}\). By the projection theorem, the projection of \(y\in\mathcal{H}\) onto \(\mathcal{M}\) is unique and given by

\[\mathrm{P}_{\mathcal{M}}y=a_{1}x_{1}+\cdots+a_{n}x_{n},\]

where \(\left\{a_{1},\ldots,a_{n}\right\}\) are found using the orthogonality principle

\[\left\langle y-\mathrm{P}_{\mathcal{M}}y,x_{j}\right\rangle=0\quad j=1,\ldots,n.\]

Evidently, \(\left\{a_{1},\ldots,a_{n}\right\}\) can be obtained by solving

\[\sum_{i=1}^{n}a_{i}\left\langle x_{i},x_{j}\right\rangle=\left\langle y,x_{j} \right\rangle\quad j=1,\ldots,n.\] (B.6)

When the elements of \(\mathcal{H}\) are vectors, this problem is the linear regression problem.

Example B.1: **Linear Regression Analysis**

For the regression model introduced in Sect. 2.1, we want to find the regression coefficients \(\beta_{i}\) that minimize the residual sum of squares. Consider the vectors \(y=(y_{1},\ldots,y_{n})^{\prime}\) and \(z_{i}=(z_{1i},\ldots,z_{ni})^{\prime}\), for \(i=1,\ldots,q\) and the inner product

\[\langle z_{i},y\rangle=\sum_{t=1}^{n}z_{ti}y_{t}=z_{i}^{\prime}\ y.\]

We solve the problem of finding a projection of the observed \(y\) on the linear space spanned by \(\beta_{1}z_{1}+\cdots+\beta_{q}z_{q}\), that is, linear combinations of the \(z_{i}\). The orthogonality principle gives

\[\left\langle y-\sum_{i=1}^{q}\beta_{i}z_{i},\ z_{j}\right\rangle=0\]

for \(j=1,\ldots,q\). Writing the orthogonality condition, as in (B.6), in vector form gives

\[y^{\prime}z_{j}=\sum_{i=1}^{q}\beta_{i}z_{i}^{\prime}z_{j}\quad j=1,\ldots,q,\] (B.7)

which can be written in the usual matrix form by letting \(Z=(z_{1},\ldots,z_{q})\), which is assumed to be full rank. That is, (B.7) can be written as

\[y^{\prime}Z=\beta^{\prime}(Z^{\prime}Z),\] (B.8)

where \(\beta=(\beta_{1},\ldots,\beta_{q})^{\prime}\). Transposing both sides of (B.8) provides the solution for the coefficients,

\[\hat{\beta}=(Z^{\prime}Z)^{-1}Z^{\prime}y.\]

The mean-square error in this case would be

\[\left\|y-\sum_{i=1}^{q}\hat{\beta}_{i}z_{i}\right\|^{2}=\left\langle y-\sum_{i =1}^{q}\hat{\beta}_{i}z_{i}\,\ y\right\rangle=\left\langle y,y\right\rangle-\sum_{i=1}^ {q}\hat{\beta}_{i}\left\langle z_{i}\,\ y\right\rangle=y^{\prime}y-\hat{\beta}^{ \prime}Z^{\prime}y,\]

which is in agreement with Sect. 2.1.

The extra generality in the above approach hardly seems necessary in the finite dimensional case, where differentiation works perfectly well. It is convenient, however, in many cases to regard the elements of \(\mathcal{H}\) as infinite dimensional, so that the orthogonality principle becomes of use. For example, the projection of the process \(\{x_{t};\ t=0\pm 1,\pm 2,\ldots\}\) on the linear manifold spanned by all filtered convolutions of the form

\[\hat{x}_{t}=\sum_{k=-\infty}^{\infty}a_{k}x_{t-k}\]

would be in this form.

There are some useful results, which we state without proof, pertaining to projection mappings.

**Theorem B.2**: _Under established notation and conditions:_

1. \(\mathrm{P}_{\mathcal{M}}(ax+by)=a\mathrm{P}_{\mathcal{M}}x+b\mathrm{P}_{\mathcal{M }}y\)_, for_ \(x,y\in\mathcal{H}\)_, where_ \(a\) _and_ \(b\) _are scalars._
2. _If_ \(||y_{n}-y||\to 0\)_, then_ \(\mathrm{P}_{\mathcal{M}}y_{n}\to\mathrm{P}_{\mathcal{M}}y\)_, as_ \(n\to\infty\)_._
3. \(w\in\mathcal{M}\) _if and only if_ \(\mathrm{P}_{\mathcal{M}}w=w\)_. Consequently, a projection mapping can be characterized by the property that_ \(\mathrm{P}_{\mathcal{M}}^{2}=\mathrm{P}_{\mathcal{M}}\)_, in the sense that, for any_ \(y\in\mathcal{H}\)_,_ \(\mathrm{P}_{\mathcal{M}}(\mathrm{P}_{\mathcal{M}}y)=\mathrm{P}_{\mathcal{M}}y\)_._
4. _Let_ \(\mathcal{M}_{1}\) _and_ \(\mathcal{M}_{2}\) _be closed subspaces of_ \(\mathcal{H}\)_. Then,_ \(\mathcal{M}_{1}\subseteq\mathcal{M}_{2}\) _if and only if_ \(\mathrm{P}_{\mathcal{M}_{1}}(\mathrm{P}_{\mathcal{M}_{2}}y)=\mathrm{P}_{ \mathcal{M}_{1}}y\) _for all_ \(y\in\mathcal{H}\)_._
5. _Let_ \(\mathcal{M}\) _be a closed subspace of_ \(\mathcal{H}\) _and let_ \(\mathcal{M}_{\perp}\) _denote the orthogonal complement of_ \(\mathcal{M}\)_. Then,_ \(\mathcal{M}_{\perp}\) _is also a closed subspace of_ \(\mathcal{H}\)_, and for any_ \(y\in\mathcal{H}\)_,_ \(y=\mathrm{P}_{\mathcal{M}}y+\mathrm{P}_{\mathcal{M}_{\perp}}y\)_._

Part (iii) of Theorem B.2 leads to the well-known result, often used in linear models, that a square matrix \(M\) is a projection matrix if and only if it is symmetric and idempotent (that is, \(M^{2}=M\)). For example, using the notation of Example B.1 for linear regression, the projection of \(y\) onto \(\overline{\mathrm{sp}}\{z_{1},\ldots,z_{q}\}\), the space generated by the columns of \(Z\), is \(P_{Z}(y)=Z\beta=Z(Z^{\prime}Z)^{-1}Z^{\prime}y\). The matrix \(M=Z(Z^{\prime}Z)^{-1}Z^{\prime}\) is an \(n\times n\), symmetric and idempotent matrix of rank \(q\) (which is the dimension of the space that \(M\) projects \(y\) onto). Parts (iv) and (v) of Theorem B.2 are useful for establishing recursive solutions for estimation and prediction.

By imposing extra structure, _conditional expectation_ can be defined as a projection mapping for random variables in \(L^{2}\) with the equivalence relation that, for \(x,y\in L^{2}\), \(x=y\) if \(\mathrm{Pr}(x=y)=1\). In particular, for \(y\in L^{2}\), if \(\mathcal{M}\) is a closed subspace of \(L^{2}\) containing \(1\), the conditional expectation of \(y\) given \(\mathcal{M}\) is defined to be the projection of \(y\) onto \(\mathcal{M}\), namely, \(\mathrm{E}_{\mathcal{M}}y=\mathrm{P}_{\mathcal{M}}y\). This means that conditional expectation, \(\mathrm{E}_{\mathcal{M}}\), must satisfy the orthogonality principle of the Projection Theorem and that the results of Theorem B.2 remain valid (the most ly used tool in this case is item (iv) of the theorem). If we let \(\mathcal{M}(x)\) denote the closed subspace of all random variables in \(L^{2}\) that can be written as a (measurable) function of \(x\), then we may define, for \(x,y\in L^{2}\), the _conditional expectation of \(y\) given \(x\)_ as \(\mathrm{E}(y\mid x)=\mathrm{E}_{\mathcal{M}(x)}y\). This idea may be generalized in an obvious way to define the conditional expectation of \(y\) given \(x_{1:n}=(x_{1},\ldots,x_{n})\); that is \(\mathrm{E}(y\mid x)=\mathrm{E}_{\mathcal{M}(x)}y\). Of particular interest to us is the following result which states that, in the Gaussian case, conditional expectation and linear prediction are equivalent.

**Theorem B.3**: _Under established notation and conditions, if \((y,x_{1},\ldots,x_{n})\) is multivariate normal, then_

\[\mathrm{E}(y\mid x_{1:n})=\mathrm{P}_{\overline{\mathrm{sp}}\{1,x_{1},\ldots, x_{n}\}}y.\]

_Proof:_ First, by the projection theorem, the conditional expectation of \(y\) given \(x_{1:n}\) is the unique element \(\mathrm{E}_{\mathcal{M}(x)}y\) that satisfies the orthogonality principle,

\[\mathrm{E}\left\{\left(y-\mathrm{E}_{\mathcal{M}(x)}y\right)w\right\}=0\quad \mathrm{for\ all\ }w\in\mathcal{M}(x).\]

[MISSING_PAGE_FAIL:505]

Hence, \(\phi^{-1}(B)\) exists and we may apply it to both sides of the ARMA model, \(\phi(B)x_{t}=\theta(B)w_{t}\), to obtain

\[x_{t}=\phi^{-1}(B)\phi(B)x_{t}=\phi^{-1}(B)\theta(B)w_{t}.\]

Thus, putting \(\psi(B)=\phi^{-1}(B)\theta(B)\), we have

\[x_{t}=\psi(B)w_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j},\]

where the \(\psi\)-weights, which are absolutely summable, can be evaluated by \(\psi(z)=\phi^{-1}(z)\theta(z)\), for \(|z|\leq 1\).

Now, suppose \(x_{t}\) is a causal process; that is, it has the representation

\[x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j},\qquad\sum_{j=0}^{\infty}|\psi_{j}|<\infty.\]

In this case, we write

\[x_{t}=\psi(B)w_{t},\]

and premultiplying by \(\phi(B)\) yields

\[\phi(B)x_{t}=\phi(B)\psi(B)w_{t}.\] (B.13)

In addition to (B.13), the model is ARMA, and can be written as

\[\phi(B)x_{t}=\theta(B)w_{t}.\] (B.14)

From (B.13) and (B.14), we see that

\[\phi(B)\psi(B)w_{t}=\theta(B)w_{t}.\] (B.15)

Now, let

\[a(z)=\phi(z)\psi(z)=\sum_{j=0}^{\infty}a_{j}z^{j}\quad|z|\leq 1\]

and, hence, we can write (B.15) as

\[\sum_{j=0}^{\infty}a_{j}w_{t-j}=\sum_{j=0}^{q}\theta_{j}w_{t-j}.\] (B.16)

Next, multiply both sides of (B.16) by \(w_{t-h}\), for \(h=0,1,2,\ldots\), and take expectation. In doing this, we obtain

\[a_{h}=\theta_{h},\quad h=0,1,\ldots,q\] \[a_{h}=0,\quad h>q.\] (B.17)

From (B.17), we conclude that

\[\phi(z)\psi(z)=a(z)=\theta(z),\quad|z|\leq 1.\] (B.18)If there is a complex number in the unit circle, say \(z_{0}\), for which \(\phi(z_{0})=0\), then by (B.18), \(\theta(z_{0})=0\). But, if there is such a \(z_{0}\), then \(\phi(z)\) and \(\theta(z)\) have a common factor which is not allowed. Thus, we may write \(\psi(z)=\theta(z)/\phi(z)\). In addition, by hypothesis, we have that \(|\psi(z)|<\infty\) for \(|z|\leq 1\), and hence

\[|\psi(z)|=\left|\frac{\theta(z)}{\phi(z)}\right|<\infty,\quad\text{for}\;\;|z| \leq 1.\] (B.19)

Finally, (B.19) implies \(\phi(z)\neq 0\) for \(|z|\leq 1\); that is, the roots of \(\phi(z)\) lie outside the unit circle. 

### Large Sample Distribution of the AR Conditional Least Squares Estimators

In Sect. 3.5 we discussed the conditional least squares procedure for estimating the parameters \(\phi_{1},\phi_{2},\ldots,\phi_{p}\) and \(\sigma_{w}^{2}\) in the AR(\(p\)) model

\[x_{t}=\sum_{k=1}^{p}\phi_{k}x_{t-k}+w_{t},\]

where we assume \(\mu=0\), for convenience. Write the model as

\[x_{t}=\phi^{\prime}x_{t-1}+w_{t},\] (B.20)

where \(x_{t-1}=(x_{t-1},x_{t-2},\ldots,x_{t-p})^{\prime}\) is a \(p\times 1\) vector of lagged values, and \(\phi=(\phi_{1},\phi_{2},\ldots,\phi_{p})^{\prime}\) is the \(p\times 1\) vector of regression coefficients. Assuming observations are available at \(x_{1},\ldots,x_{n}\), the conditional least squares procedure is to minimize

\[S_{c}(\phi)=\sum_{t=p+1}^{n}(x_{t}-\phi^{\prime}x_{t-1})^{2}\]

with respect to \(\phi\). The solution is

\[\hat{\phi}=\left(\sum_{t=p+1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}\sum_{t=p+ 1}^{n}x_{t-1}x_{t}\] (B.21)

for the regression vector \(\phi\); the conditional least squares estimate of \(\sigma_{w}^{2}\) is

\[\hat{\sigma}_{w}^{2}=\frac{1}{n-p}\sum_{t=p+1}^{n}\left(x_{t}-\hat{\phi}^{ \prime}x_{t-1}\right)^{2}.\] (B.22)

As pointed out following (3.116), Yule-Walker estimators and least squares estimators are approximately the same in that the estimators differ only by inclusion or exclusion of terms involving the endpoints of the data. Hence, it is easy to show the asymptotic equivalence of the two estimators; this is why, for AR(\(p\)) models, (3.103) and (3.132), are equivalent. Details on the asymptotic equivalence can be found in Brockwell and Davis (1991, Chapter 8)[36].

Here, we use the same approach as in Appendix A, replacing the lower limits of the sums in (B.21) and (B.22) by one and noting the asymptotic equivalence of the estimators

\[\tilde{\phi}=\left(\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}\sum_{t=1} ^{n}x_{t-1}x_{t}\] (B.23)

and

\[\tilde{\sigma}_{w}^{2}=\frac{1}{n}\sum_{t=1}^{n}\left(x_{t}-\tilde{\phi}^{ \prime}x_{t-1}\right)^{2}\] (B.24)

to those two estimators. In (B.23) and (B.24), we are acting as if we are able to observe \(x_{1-p},\ldots,x_{0}\) in addition to \(x_{1},\ldots,x_{n}\). The asymptotic equivalence is then seen by arguing that for \(n\) sufficiently large, it makes no difference whether or not we observe \(x_{1-p},\ldots,x_{0}\). In the case of (B.23) and (B.24), we obtain the following theorem.

**Theorem B.4**: _Let \(x_{t}\) be a causal AR(\(p\)) series with white (iid) noise \(w_{t}\) satisfying \(\mathrm{E}(w_{t}^{4})=\eta\sigma_{w}^{4}\). Then,_

\[\tilde{\phi}\sim\mathrm{AN}\left(\phi,\;n^{-1}\sigma_{w}^{2}F_{p}^{-1}\right),\] (B.25)

_where \(\Gamma_{p}=\{\gamma(i-j)\}_{i,j=1}^{p}\) is the \(p\times p\) autocovariance matrix of the vector \(x_{t-1}\). We also have, as \(n\to\infty\),_

\[n^{-1}\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\overset{p}{\to}\Gamma_{p}\quad \mathrm{and}\quad\tilde{\sigma}_{w}^{2}\overset{p}{\to}\sigma_{w}^{2}.\] (B.26)

_Proof:_ First, (B.26) follows from the fact that \(\mathrm{E}(x_{t-1}x_{t-1}^{\prime})=\Gamma_{p}\), recalling that from Theorem A.6, second-order sample moments converge in probability to their population moments for linear processes in which \(w_{t}\) has a finite fourth moment. To show (B.25), we can write

\[\tilde{\phi} =\left(\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}\sum_{t= 1}^{n}x_{t-1}(x_{t-1}^{\prime}\phi+w_{t})\] \[=\phi+\left(\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}\sum _{t=1}^{n}x_{t-1}w_{t},\]

so that

\[n^{1/2}(\tilde{\phi}-\phi) =\left(n^{-1}\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}n^ {-1/2}\sum_{t=1}^{n}x_{t-1}w_{t}\] \[=\left(n^{-1}\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}n^ {-1/2}\sum_{t=1}^{n}u_{t},\]where \(u_{t}=x_{t-1}w_{t}\). We use the fact that \(w_{t}\) and \(x_{t-1}\) are independent to write \(\mathrm{E}u_{t}=\mathrm{E}(x_{t-1})\mathrm{E}(w_{t})=0\), because the errors have zero means. Also,

\[\mathrm{E}u_{t}u_{t}^{\prime}=\mathrm{E}x_{t-1}w_{t}w_{t}x_{t-1}^{\prime}= \mathrm{E}x_{t-1}x_{t-1}^{\prime}\mathrm{E}w_{t}^{2}=\sigma_{w}^{2}\Gamma_{p}.\]

In addition, we have, for \(h>0\),

\[\mathrm{E}u_{t+h}u_{t}^{\prime}=\mathrm{E}x_{t+h-1}w_{t+h}w_{t}x_{t-1}^{\prime }=\mathrm{E}x_{t+h-1}w_{t}x_{t-1}^{\prime}\mathrm{E}w_{t+h}=0.\]

A similar computation works for \(h<0\).

Next, consider the mean square convergent approximation

\[x_{t}^{m}=\sum_{j=0}^{m}\psi_{j}w_{t-j}\]

for \(x_{t}\), and define the \((m+p)\)-dependent process \(u_{t}^{m}=w_{t}(x_{t-1}^{m},x_{t-2^{2}}^{m},\ldots,x_{t-p}^{m})^{\prime}\). Note that we need only look at a central limit theorem for the sum

\[y_{nm}=n^{-1/2}\sum_{t=1}^{n}\lambda^{\prime}u_{t}^{m},\]

for arbitrary vectors \(\lambda=(\lambda_{1},\ldots,\lambda_{p})^{\prime}\), where \(y_{nm}\) is used as an approximation to

\[S_{n}=n^{-1/2}\sum_{t=1}^{n}\lambda^{\prime}u_{t}.\]

First, apply the \(m\)-dependent central limit theorem to \(y_{nm}\) as \(n\to\infty\) for fixed \(m\) to establish (i) of Theorem A.2. This result shows \(y_{nm}\overset{d}{\to}y_{m}\), where \(y_{m}\) is asymptotically normal with covariance \(\lambda^{\prime}\Gamma_{p}^{(m)}\lambda\), where \(\Gamma_{p}^{(m)}\) is the covariance matrix of \(u_{t}^{m}\). Then, we have \(\Gamma_{p}^{(m)}\to\Gamma_{p}\), so that \(y_{m}\) converges in distribution to a normal random variable with mean zero and variance \(\lambda^{\prime}\Gamma_{p}\lambda\) and we have verified part (ii) of Theorem A.2. We verify part (iii) of Theorem A.2 by noting that

\[\mathrm{E}[(S_{n}-y_{nm})^{2}]=n^{-1}\sum_{t=1}^{n}\lambda^{\prime}\mathrm{E} [(u_{t}-u_{t}^{m})(u_{t}-u_{t}^{m})^{\prime}]\lambda\]

clearly converges to zero as \(n,m\to\infty\) because

\[x_{t}-x_{t}^{m}=\sum_{j=m+1}^{\infty}\psi_{j}w_{t-j}\]

form the components of \(u_{t}-u_{t}^{m}\).

Now, the form for \(\sqrt{n}(\delta-\phi)\) contains the premultiplying matrix

\[\left(n^{-1}\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}\overset{p}{\to} \Gamma_{p}^{-1},\]because (A.22) can be applied to the function that defines the inverse of the matrix. Then, applying (A.30), shows that

\[n^{1/2}\left(\tilde{\phi}-\phi\right)\stackrel{{ d}}{{\to}}\mathrm{N }\left(0,\sigma_{w}^{2}\Gamma_{P}^{-1}\Gamma_{P}\Gamma_{P}^{-1}\right),\]

so we may regard it as being multivariate normal with mean zero and covariance matrix \(\sigma_{w}^{2}\Gamma_{P}^{-1}\).

To investigate \(\tilde{\sigma}_{w}^{2}\), note

\[\tilde{\sigma}_{w}^{2} = n^{-1}\sum_{t=1}^{n}\left(x_{t}-\tilde{\phi}^{\prime}x_{t-1} \right)^{2}\] \[= n^{-1}\sum_{t=1}^{n}x_{t}^{2}-n^{-1}\sum_{t=1}^{n}x_{t-1}^{ \prime}x_{t}\left(n^{-1}\sum_{t=1}^{n}x_{t-1}x_{t-1}^{\prime}\right)^{-1}n^{- 1}\sum_{t=1}^{n}x_{t-1}x_{t}\] \[\stackrel{{ p}}{{\to}}\gamma(0)-\gamma_{P}^{\prime} \Gamma_{P}^{-1}\gamma_{P}\] \[= \sigma_{w}^{2},\]

and we have that the sample estimator converges in probability to \(\sigma_{w}^{2}\), which is written in the form of (3.66). 

The arguments above imply that, for sufficiently large \(n\), we may consider the estimator \(\hat{\phi}\) in (B.21) as being approximately multivariate normal with mean \(\phi\) and variance-covariance matrix \(\sigma_{w}^{2}\Gamma_{P}^{-1}/n\). Inferences about the parameter \(\phi\) are obtained by replacing the \(\sigma_{w}^{2}\) and \(\Gamma_{P}\) by their estimates given by (B.22) and

\[\hat{\Gamma}_{P}=n^{-1}\sum_{t=p+1}^{n}x_{t-1}x_{t-1}^{\prime},\]

respectively. In the case of a nonzero mean, the data \(x_{t}\) are replaced by \(x_{t}-\bar{x}\) in the estimates and the results of Theorem A.2 remain valid.

### The Wold Decomposition

The ARMA approach to modeling time series is generally implied by the assumption that the dependence between adjacent values in time is best explained in terms of a regression of the current values on the past values. This assumption is partially justified, in theory, by the Wold decomposition.

In this section we assume that \(\{x_{t};\ t=0,\pm 1,\pm 2,\ldots\}\) is a stationary, mean-zero process. Using the notation of Sect. B.1, we define

\[\mathcal{M}_{n}^{x}=\overline{\mathrm{sp}}\{x_{t},\ -\infty<t\leq n\},\quad \text{with}\quad\mathcal{M}_{-\infty}^{x}=\bigcap_{n=-\infty}^{\infty}\mathcal{ M}_{n}^{x},\]\[\sigma_{x}^{2}=\mathrm{E}\left(x_{n+1}-\mathrm{P}_{\mathcal{M}_{n}^{x}}x_{n+1} \right)^{2}.\]

We say that \(x_{t}\) is a _deterministic process_ if and only if \(\sigma_{x}^{2}=0\). That is, a deterministic process is one in which its future is perfectly predictable from its past; a simple example is the process given in (4.1). We are now ready to present the decomposition.

**Theorem B.5** (The Wold Decomposition): _Under the conditions and notation of this section, if \(\sigma_{x}^{2}>0\), then \(x_{t}\) can be expressed as_

\[x_{t}=\sum_{j=0}^{\infty}\psi_{j}w_{t-j}+v_{t}\]

_where_

1. \(\sum_{j=0}^{\infty}\psi_{j}^{2}<\infty\)__ _(_\(\psi_{0}=1\)_)_
2. \(\{w_{t}\}\) _is white noise with variance_ \(\sigma_{w}^{2}\)__
3. \(w_{t}\in\mathcal{M}_{t}^{x}\)__
4. \(\mathrm{cov}(w_{s},v_{t})=0\) _for all_ \(s,t=0,\pm 1,\pm 2,\ldots\)__._
5. \(v_{t}\in\mathcal{M}_{-\infty}^{x}\)__
6. \(\{v_{t}\}\) _is deterministic._

The proof of the decomposition follows from the theory of Sect. B.1 by defining the unique sequences:

\[w_{t} = x_{t}-\mathrm{P}_{\mathcal{M}_{t-1}^{x}}x_{t},\] \[\psi_{j} = \sigma_{w}^{-2}\left\langle x_{t},w_{t-j}\right\rangle=\sigma_{w} ^{-2}\mathrm{E}(x_{t}w_{t-j}),\] \[v_{t} = x_{t}-\sum_{j=0}^{\infty}\psi_{j}w_{t-j}.\]

Although every stationary process can be represented by the Wold decomposition, it does not mean that the decomposition is the best way to describe the process. In addition, there may be some dependence structure among the \(\{w_{t}\}\); we are only guaranteed that the sequence is an uncorrelated sequence. The theorem, in its generality, falls short of our needs because we would prefer the noise process, \(\{w_{t}\}\), to be white independent noise. But, the decomposition does give us the confidence that we will not be completely off the mark by fitting ARMA models to time series data.

## Appendix C Spectral Domain Theory

### C.1 Spectral Representation Theorems

In this section, we present a spectral representation for the process \(x_{t}\) itself, which allows us to think of a stationary process as a random sum of sines and cosines as described in (4.4). In addition, we present results that justify representing the autocovariance function of a weakly stationary process in terms of a spectral distribution function.

First, we consider developing a representation for the autocovariance function of a stationary, possibly complex, series \(x_{t}\) with zero mean and autocovariance function \(\gamma_{x}(h)=\mathrm{E}(x_{t+h}x_{t}^{*})\). An autocovariance function, \(\gamma(h)\), is non-negative definite in that, for any set of complex constants, \(\{a_{t}\in\mathbb{C};\,t=1,\ldots,n\}\), and any integer \(n>0\),

\[\sum_{s=1}^{n}\sum_{t=1}^{n}a_{s}^{*}\gamma(s-t)a_{t}\geq 0.\]

Likewise, any non-negative definite function, say \(\gamma(h)\), on the integers is an autocovariance of some stationary process. To see this, let \(\Gamma_{n}=\{\gamma(t_{i}-t_{j})\}_{i,j=1}^{n}\) be the \(n\times n\) matrix with \(i,j\)th equal to \(\gamma(t_{i}-t_{j})\). Then choose \(\{x_{t}\}\) such that \((x_{t_{1}},\ldots,x_{t_{n}})\sim\mathrm{N}_{n}(0,\Gamma_{n})\).

We now establish the relationship of such functions to a spectral distribution function; Riemann-Stieljes integration is explained in Sect. C.4.1.

**Theorem C.1**_A function \(\gamma(h)\), for \(h=0,\pm 1,\pm 2,\ldots\), is non-negative definite if and only if it can be expressed as_

\[\gamma(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\exp\{2\pi i\omega h\}dF(\omega),\] (C.1)

_where \(F(\cdot)\) is nondecreasing. The function \(F(\cdot)\) is right continuous, bounded and uniquely determined by the conditions \(F(\omega)=F(-1/2)=0\) for \(\omega\leq-1/2\) and \(F(\omega)=F(1/2)=\gamma(0)\) for \(\omega\geq 1/2\).__Proof:_ If \(\gamma(h)\) has the representation (C.1), then

\[\begin{split}\sum_{s=1}^{n}\sum_{t=1}^{n}a_{s}^{*}\gamma(s-t)a_{t}& =\int_{-\frac{1}{2}}^{\frac{1}{2}}\sum_{s=1}^{n}\sum_{t=1}^{n}a_{s}^ {*}\,a_{t}\,\mathrm{e}^{2\pi i\omega(s-t)}dF(\omega)\\ &=\int_{-\frac{1}{2}}^{\frac{1}{2}}\left|\sum_{t=1}^{n}a_{t}\, \mathrm{e}^{-2\pi i\omega t}\right|^{2}dF(\omega)\geq 0\end{split}\]

and \(\gamma(h)\) is non-negative definite.

Conversely, suppose \(\gamma(h)\) is a non-negative definite function. Define the non-negative function

\[\begin{split} f_{n}(\omega)&=n^{-1}\sum_{s=1}^{n} \sum_{t=1}^{n}\mathrm{e}^{-2\pi i\omega s}\gamma(s-t)\mathrm{e}^{2\pi i\omega t }\\ &=n^{-1}\sum_{h=-(n-1)}^{(n-1)}(n-|h|)e^{-2\pi i\omega h}\gamma(h) \geq 0\end{split}\] (C.2)

Now, let \(F_{n}(\omega)\) be the distribution function corresponding to \(f_{n}(\omega)I_{(-1/2,1/2]}\), where \(I_{(\cdot)}\) denotes the indicator function of the interval in the subscript. Note that \(F_{n}(\omega)=0,\omega\leq-1/2\) and \(F_{n}(\omega)=F_{n}(1/2)\) for \(\omega\geq 1/2\). Then,

\[\begin{split}\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i \omega h}\,dF_{n}(\omega)&=\int_{-\frac{1}{2}}^{\frac{1}{2}} \mathrm{e}^{2\pi i\omega h}\,f_{n}(\omega)\;d\omega\\ &=\begin{cases}(1-|h|/n)\gamma(h),\,|h|<n\\ 0,\qquad\qquad\qquad\text{elsewhere}.\end{cases}\end{split}\]

We also have

\[\begin{split} F_{n}(1/2)&=\int_{-\frac{1}{2}}^{\frac {1}{2}}f_{n}(\omega)\;d\omega\\ &=\int_{-\frac{1}{2}}^{\frac{1}{2}}\sum_{|h|<n}(1-|h|/n)\gamma(h) \mathrm{e}^{-2\pi i\omega h}d\omega=\gamma(0).\end{split}\]

Now, by Helly's first convergence theorem (Bhat [20, p. 157]), there exists a subsequence \(F_{n_{k}}\) converging to \(F\), and by the Helly-Bray Lemma (see Bhat, p. 157), this implies

\[\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\,dF_{n_{k}}( \omega)\to\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}dF(\omega)\]

and, from the right-hand side of the earlier equation,

\[(1-|h|/n_{k})\gamma(h)\to\gamma(h)\]

as \(n_{k}\to\infty\), and the required result follows.

Next, we present the version of the spectral representation theorem of a mean-zero, stationary process, \(x_{t}\) in terms of an orthogonal increment process. This version allows us to think of a stationary process as being generated (approximately) by a random sum of sines and cosines such as described in (4.4). We refer the reader to Hannan (1970, SS2.3)[86] for details.

**Theorem C.2**: _If \(x_{t}\) is a mean-zero stationary process, with spectral distribution \(F(\omega)\) as given in Theorem C.1, then there exists a complex-valued stochastic process \(Z(\omega)\), on the interval \(\omega\in[-1/2,1/2]\), having stationary uncorrelated increments, such that \(x_{t}\) can be written as the stochastic integral_

\[x_{t}=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega t}\,dZ(\omega),\]

_where, for \(-1/2\leq\omega_{1}\leq\omega_{2}\leq 1/2\),_

\[\mathrm{var}\left\{Z(\omega_{2})-Z(\omega_{1})\right\}=F(\omega_{2})-F(\omega_ {1}).\]

The theorem uses stochastic integration and orthogonal increment processes, which are described in further detail in Sect. C.4.2.

In general, the spectral distribution function can be a mixture of discrete and continuous distributions. The special case of greatest interest is the absolutely continuous case, namely, when \(dF(\omega)=f(\omega)d\omega\), and the resulting function is the spectral density considered in Sect. 4.2. What made the proof of Theorem C.1 difficult was that, after we defined

\[f_{n}(\omega)=\sum_{h=-(n-1)}^{(n-1)}\left(1-\frac{|h|}{n}\right)\gamma(h)e^{- 2\pi i\omega h}\]

in (C.2), we could not simply allow \(n\to\infty\) because \(\gamma(h)\) may not be absolutely summable. If, however, \(\gamma(h)\) is absolutely summable we may define \(f(\omega)=\lim_{n\to\infty}f_{n}(\omega)\), and we have the following result.

**Theorem C.3**: _If \(\gamma(h)\) is the autocovariance function of a stationary process, \(x_{t}\), with_

\[\sum_{h=-\infty}^{\infty}|\gamma(h)|<\infty,\] (C.3)

_then the spectral density of \(x_{t}\) is given by_

\[f(\omega)=\sum_{h=-\infty}^{\infty}\gamma(h)\mathrm{e}^{-2\pi i\omega h}.\] (C.4)

We may extend the representation to the vector case \(x_{t}=(x_{t1},\ldots,x_{tP})^{\prime}\) by considering linear combinations of the form

\[y_{t}=\sum_{j=1}^{P}a_{j}^{*}x_{tj},\]which will be stationary with autocovariance functions of the form

\[\gamma_{y}(h)=\sum_{j=1}^{P}\sum_{k=1}^{P}a_{j}^{*}\gamma_{jk}(h)a_{k},\]

where \(\gamma_{jk}(h)\) is the usual cross-covariance function between \(x_{tf}\) and \(x_{tk}\). To develop the spectral representation of \(\gamma_{jk}(h)\) from the representations of the univariate series, consider the linear combinations

\[y_{t1}=x_{tj}+x_{tk}\qquad\text{and}\qquad y_{t2}=x_{tj}+ix_{tk},\]

which are both stationary series with respective covariance functions

\[\gamma_{1}(h) =\gamma_{jj}(h)+\gamma_{jk}(h)+\gamma_{kj}(h)+\gamma_{kk}(h)\] \[=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}dG_{ 1}(\omega),\] \[\gamma_{2}(h) =\gamma_{jj}(h)+i\gamma_{kj}(h)-i\gamma_{jk}(h)+\gamma_{kk}(h)\] \[=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}dG_ {2}(\omega).\]

Introducing the spectral representations for \(\gamma_{jj}(h)\) and \(\gamma_{kk}(h)\) yields

\[\gamma_{jk}(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}dF_ {jk}(\omega),\]

with

\[F_{jk}(\omega)=\frac{1}{2}\left[G_{1}(\omega)+iG_{2}(\omega)-(1+i)\Big{(}F_{ jj}(\omega)+F_{kk}(\omega)\Big{)}\right].\]

Now, under the summability condition

\[\sum_{h=-\infty}^{\infty}|\gamma_{jk}(h)|<\infty,\]

we have the representation

\[\gamma_{jk}(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}f_{ jk}(\omega)d\omega,\]

where the cross-spectral density function has the inverse Fourier representation

\[f_{jk}(\omega)=\sum_{h=-\infty}^{\infty}\gamma_{jk}(h)\mathrm{e}^{-2\pi i \omega h}.\]The cross-covariance function satisfies \(\gamma_{jk}(h)=\gamma_{kj}(-h)\), which implies \(f_{jk}(\omega)=f_{kj}(-\omega)\) using the above representation.

Then, defining the autocovariance function of the general vector process \(x_{t}\) as the \(p\times p\) matrix

\[\Gamma(h)=\mathrm{E}[(x_{t+h}-\mu_{x})(x_{t}-\mu_{x})^{\prime}],\]

and the \(p\times p\) spectral matrix as \(f(\omega)=\{f_{jk}(\omega);\,j,k=1,\ldots,p\}\), we have the representation in matrix form, written as

\[\Gamma(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}f(\omega )\ d\omega,\] (C.5)

and the inverse result

\[f(\omega)=\sum_{h=-\infty}^{\infty}\Gamma(h)\mathrm{e}^{-2\pi i\omega h}.\] (C.6)

which appears as Property 4.8 in Sect. 4.5. Theorem C.2 can also be extended to the multivariate case.

### Large Sample Distribution of the Smoothed Periodogram

We have previously introduced the DFT, for the stationary zero-mean process \(x_{t}\), observed at \(t=1,\ldots,n\) as

\[d(\omega)=n^{-1/2}\sum_{t=1}^{n}x_{t}\ \mathrm{e}^{-2\pi i\omega t},\] (C.7)

as the result of matching sines and cosines of frequency \(\omega\) against the series \(x_{t}\). We will suppose now that \(x_{t}\) has an absolutely continuous spectrum \(f(\omega)\) corresponding to the absolutely summable autocovariance function \(\gamma(h)\). Our purpose in this section is to examine the statistical properties of the complex random variables \(d(\omega_{k})\), for \(\omega_{k}=k/n\), \(k=0,1,\ldots,n-1\) in providing a basis for the estimation of \(f(\omega)\). To develop the statistical properties, we examine the behavior of

\[S_{n}(\omega,\omega) =\mathrm{E}\left|d(\omega)\right|^{2}=n^{-1}\mathrm{E}\biggl{[} \sum_{s=1}^{n}x_{s}\ \mathrm{e}^{-2\pi i\omega s}\sum_{t=1}^{n}x_{t}\ \mathrm{e}^{2\pi i\omega t}\biggr{]}\] \[=n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}\mathrm{e}^{-2\pi i\omega s} \mathrm{e}^{2\pi i\omega t}\gamma(s-t)\] \[=\sum_{h=-(n-1)}^{n-1}(1-|h|/n)\gamma(h)\mathrm{e}^{-2\pi i\omega h},\] (C.8)

where we have let \(h=s-t\). Using dominated convergence,

\[S_{n}(\omega,\omega)\to\sum_{h=-\infty}^{\infty}\gamma(h)\mathrm{e}^{-2\pi i \omega h}=f(\omega),\]as \(n\to\infty\), making the large sample variance of the Fourier transform equal to the spectrum evaluated at \(\omega\). We have already seen this result in Theorem C.3. For exact bounds it is also convenient to add an absolute summability assumption for the autocovariance function, namely,

\[\theta=\sum_{h=-\infty}^{\infty}|h||\gamma(h)|<\infty.\] (C.9)

**Example C.1**: **Condition** (C.9) **Verified for ARMA Models**

For pure MA(\(q\)) models \([\text{ARMA}(0,q)]\), \(\gamma(h)=0\) for \(|h|>q\), so the condition holds trivially. In Sect. 3.3, we showed that when \(p>0\), the autocovariance function \(\gamma(h)\) behaves like the inverse of the roots of the AR polynomial to the power \(h\). Recalling (3.50), we can write

\[\gamma(h)\sim|h|^{k}\xi^{h}\,,\]

for large \(h\), where \(\xi=|z|^{-1}\in(0,1)\), \(z\) is a root of the AR polynomial, and \(0\leq k\leq p-1\) is some integer depending on the multiplicity of the root.

We show that \(\sum_{h\geq 0}h\xi^{h}\) is finite, the other cases follow in a similar manner. Note the \(\sum_{h\geq 0}\xi^{h}=1/(1-\xi)\) because it is a geometric sum. Taking derivatives, we have \(\sum_{h\geq 0}h\xi^{h-1}=1/(1-\xi)^{2}\) and multiplying through by \(\xi\), we have \(\sum_{h\geq 0}h\xi^{h}=\xi/(1-\xi)^{2}\). For other values of \(k\), follow the recipe but take \(k\)th derivatives.

To elaborate further, we derive two approximation lemmas.

**Lemma C.1**: _For \(S_{n}(\omega,\omega)\) as defined in (C.8) and \(\theta\) in (C.9) finite, we have_

\[|S_{n}(\omega,\omega)-f(\omega)|\leq\frac{\theta}{n}\] (C.10)

_or_

\[S_{n}(\omega,\omega)=f(\omega)+O(n^{-1}).\] (C.11)

_Proof:_ To prove the lemma, write

\[n|S_{n}(\omega,\omega)-f_{x}(\omega)| =\left|\sum_{|u|<n}(n-|u|)\gamma(u)\mathrm{e}^{-2\pi i\omega u}- n\sum_{u=-\infty}^{\infty}\gamma(u)\mathrm{e}^{-2\pi i\omega u}\right|\] \[=\left|-n\sum_{|u|\geq n}\gamma(u)\mathrm{e}^{-2\pi i\omega u}- \sum_{|u|<n}|u|\gamma(u)\mathrm{e}^{-2\pi i\omega u}\right|\] \[\leq\sum_{|u|\geq n}|u||\gamma(u)|+\sum_{|u|<n}|u||\gamma(u)|\] \[=\theta,\]

which establishes the lemma.

**Lemma C.2**: _For \(\omega_{k}=k/n\), \(\omega_{\ell}=\ell/n\), \(\omega_{k}-\omega_{\ell}\neq 0,\pm 1,\pm 2,\pm 3,\ldots\), and \(\theta\) in (C.9), we have_

\[|S_{n}(\omega_{k},\omega_{\ell})|\leq\frac{\theta}{n}=O(n^{-1}),\] (C.12)

_where_

\[S_{n}(\omega_{k},\omega_{\ell})=\mathrm{E}\{d(\omega_{k})d^{*}(\omega_{\ell})\}.\] (C.13)

_Proof:_ Write

\[n|S_{n}(\omega_{k},\omega_{\ell})| = \sum_{u=-(n-1)}^{-1}\gamma(u)\sum_{v=-(u-1)}^{n}\mathrm{e}^{-2\pi i (\omega_{k}-\omega_{\ell})v}\mathrm{e}^{-2\pi i\omega_{k}u}\] \[+\sum_{u=0}^{n-1}\gamma(u)\sum_{v=1}^{n-u}\mathrm{e}^{-2\pi i( \omega_{k}-\omega_{\ell})v}\mathrm{e}^{-2\pi i\omega_{k}u}.\]

Now, for the first term, with \(u<0\),

\[\sum_{v=-(u-1)}^{n}\mathrm{e}^{-2\pi i(\omega_{k}-\omega_{\ell})v} = \left(\sum_{v=1}^{n}-\sum_{v=1}^{-u}\right)\mathrm{e}^{-2\pi i( \omega_{k}-\omega_{\ell})v}\] \[= 0-\sum_{v=1}^{-u}\mathrm{e}^{-2\pi i(\omega_{k}-\omega_{\ell})v}.\]

For the second term with \(u\geq 0\),

\[\sum_{v=1}^{n-u}\mathrm{e}^{-2\pi i(\omega_{k}-\omega_{\ell})v} = \left(\sum_{v=1}^{n}-\sum_{v=n-u+1}^{n}\right)\mathrm{e}^{-2\pi i (\omega_{k}-\omega_{\ell})v}\] \[= 0-\sum_{v=n-u+1}^{n}\mathrm{e}^{-2\pi i(\omega_{k}-\omega_{\ell} )v}.\]

Consequently,

\[n|S_{n}(\omega_{k},\omega_{\ell})| = \left|-\sum_{u=-(n-1)}^{-1}\gamma(u)\sum_{v=1}^{-u}\mathrm{e}^{-2 \pi i(\omega_{k}-\omega_{\ell})v}\mathrm{e}^{-2\pi i\omega_{k}u}\right.\] \[\left.\qquad-\sum_{u=1}^{n-1}\gamma(u)\sum_{v=n-u+1}^{n}\mathrm{e} ^{-2\pi i(\omega_{k}-\omega_{\ell})v}\mathrm{e}^{-2\pi i\omega_{k}u}\right|\] \[\leq \sum_{u=-(n-1)}^{0}(-u)|\gamma(u)|+\sum_{u=1}^{n-1}u|\gamma(u)|\] \[= \sum_{u=-(n-1)}^{(n-1)}|u|\ |\gamma(u)|.\]Hence, we have

\[S_{n}(\omega_{k},\omega_{\ell})\leq\frac{\theta}{n},\]

and the asserted relations of the follow. 

Because the DFTs are approximately uncorrelated, say, of order \(1/n\), when the frequencies are of the form \(\omega_{k}=k/n\), we shall compute at those frequencies. The behavior of \(f(\omega)\) at neighboring frequencies will often be of interest and we shall use Lemma C.3 below to handle such cases.

**Lemma C.3**: _For \(|\omega_{k}-\omega|\leq L/2n\) and \(\theta\) in (C.9), we have_

\[|f(\omega_{k})-f(\omega)|\leq\frac{\pi\theta L}{n}\] (C.14)

_or_

\[f(\omega_{k})-f(\omega)=O(L/n).\] (C.15)

_Proof:_ Write the difference

\[|f(\omega_{k})-f(\omega)| =\Big{|}\sum_{h=-\infty}^{\infty}\gamma(h)\Big{(}\mathrm{e}^{-2 \pi i\omega_{k}h}-\mathrm{e}^{-2\pi i\omega h}\Big{)}\Big{|}\] \[\leq\sum_{h=-\infty}^{\infty}|\gamma(h)|\,\big{|}\mathrm{e}^{-\pi i (\omega_{k}-\omega)h}-\mathrm{e}^{\pi i(\omega_{k}-\omega)h}\big{|}\] \[=2\sum_{h=-\infty}^{\infty}|\gamma(h)|\,\big{|}\sin[\pi(\omega_{ k}-\omega)h]\big{|}\] \[\leq 2\pi|\omega_{k}-\omega|\sum_{h=-\infty}^{\infty}|h||\gamma(h)|\] \[\leq\frac{\pi\theta L}{n}\]

because \(|\sin x|\leq|x|\). 

The main use of the properties described by Lemma C.1 and Lemma C.2 is in identifying the covariance structure of the DFT, say,

\[d(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}x_{t}\ e^{-2\pi i\omega_{k}t}=d_{c}(\omega _{k})-id_{s}(\omega_{k}),\]

where

\[d_{c}(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}x_{t}\,\cos(2\pi\omega_{k}t)\]

and

\[d_{s}(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}x_{t}\,\sin(2\pi\omega_{k}t)\]

[MISSING_PAGE_FAIL:520]

and we have that

\[\frac{2\ I(\omega_{k:n})}{f(\omega)}\xrightarrow{d}\chi_{2}^{2},\]

where \(\chi_{\nu}^{2}\) denotes a chi-squared random variable with \(\nu\) degrees of freedom, as usual. Unfortunately, the distribution does not become more concentrated as \(n\to\infty\), because the variance of the periodogram estimator does not go to zero.

We develop a fix for the deficiencies mentioned above by considering the average of the periodogram over a set of frequencies in the neighborhood of \(\omega\). For example, we can always find a set of \(L=2m+1\) frequencies of the form \(\{\omega_{j:n}+k/n;\ k=0,\pm 1,\pm 2,\ldots,m\}\), for which

\[f(\omega_{j:n}+k/n)=f(\omega)+O(Ln^{-1})\]

by Lemma C.3. As \(n\) increases, the values of the separate frequencies change.

Now, we can consider the smoothed periodogram estimator, \(\hat{f}(\omega)\), given in (4.64); this case includes the averaged periodogram, \(\bar{f}(\omega)\). First, we note that (C.9), \(\theta=\sum_{h=-\infty}^{\infty}|h||\gamma(h)|<\infty\), is a crucial condition in the estimation of spectra. In investigating local averages of the periodogram, we will require a condition on the rate of (C.9), namely

\[\sum_{h=-n}^{n}|h||\gamma(h)|=O(n^{-1/2}).\] (C.19)

One can show that a sufficient condition for (C.19) is that the time series is the linear process given by,

\[x_{t}=\sum_{j=-\infty}^{\infty}\psi_{j}w_{t-j},\qquad\sum_{j=0}^{\infty}\sqrt{ j}\ |\psi_{j}|<\infty\] (C.20)

where \(w_{t}\sim\mathrm{iid}(0,\sigma_{w}^{2})\) and \(w_{t}\) has finite fourth moment,

\[\mathrm{E}(w_{t}^{4})=\eta\sigma_{w}^{4}<\infty.\]

We leave it to the reader (see Problem 4.40 for more details) to show (C.20) implies (C.19). If \(w_{t}\sim wn(0,\sigma_{w}^{2})\), then (C.20) implies (C.19), but we will require the noise to be iid in the following lemma.

**Lemma C.4**: _Suppose \(x_{t}\) is the linear process given by (C.20), and let \(I(\omega_{j})\) be the periodogram of the data \(\{x_{1},\ldots,x_{n}\}\). Then_

\[\mathrm{cov}\left(I(\omega_{j}),I(\omega_{k})\right)=\left\{\begin{array}{ ll}2f^{2}(\omega_{j})+o(1)&\omega_{j}=\omega_{k}=0,\ 1/2\\ f^{2}(\omega_{j})+o(1)&\omega_{j}=\omega_{k}\neq 0,\ 1/2\\ O(n^{-1})&\omega_{j}\neq\omega_{k}.\end{array}\right.\]

The proof of Lemma C.4 is straightforward but tedious, and details may be found in Fuller (1976, Theorem 7.2.1) [65] or in Brockwell and Davis (1991, Theorem 10.3.2) [36]. For demonstration purposes, we present the proof of the lemma for the pure white noise case; i.e., \(x_{t}=w_{t}\), in which case \(f(\omega)\equiv\sigma_{w}^{2}\). By definition, the periodogram in this case is

\[I(\omega_{j})=n^{-1}\sum_{s=1}^{n}\sum_{t=1}^{n}w_{s}w_{t}e^{2\pi i\omega_{j}(t- s)},\]

where \(\omega_{j}=j/n\), and hence

\[\mathrm{E}\{I(\omega_{j})I(\omega_{k})\}=n^{-2}\sum_{s=1}^{n}\sum_{t=1}^{n}\sum _{u=1}^{n}\sum_{v=1}^{n}\mathrm{E}(w_{s}w_{t}w_{u}w_{v})e^{2\pi i\omega_{j}(t- s)}e^{2\pi i\omega_{k}(u-v)}.\]

Now when all the subscripts match, \(\mathrm{E}(w_{s}w_{t}w_{u}w_{v})=\eta\sigma_{w}^{4}\), when the subscripts match in pairs (e.g., \(s=t\neq u=v\)), \(\mathrm{E}(w_{s}w_{t}w_{u}w_{v})=\sigma_{w}^{4}\), otherwise, \(\mathrm{E}(w_{s}w_{t}w_{u}w_{v})=0\). Thus,

\[\mathrm{E}\{I(\omega_{j})I(\omega_{k})\}=n^{-1}(\eta-3)\sigma_{w}^{4}+\sigma_{ w}^{4}\left(1+n^{-2}[A(\omega_{j}+\omega_{k})+A(\omega_{k}-\omega_{j})]\right),\]

where

\[A(\lambda)=\left|\sum_{t=1}^{n}e^{2\pi i\lambda t}\right|^{2}.\]

Noting that \(\mathrm{E}I(\omega_{j})=n^{-1}\sum_{t=1}^{n}\mathrm{E}(w_{t}^{2})=\sigma_{w}^{2}\), we have

\[\mathrm{cov}\{I(\omega_{j}),I(\omega_{k})\} =\mathrm{E}\{I(\omega_{j})I(\omega_{k})\}-\sigma_{w}^{4}\] \[=n^{-1}(\eta-3)\sigma_{w}^{4}+n^{-2}\sigma_{w}^{4}[A(\omega_{j}+ \omega_{k})+A(\omega_{k}-\omega_{j})].\]

Thus we conclude that

\[\mathrm{var}\{I(\omega_{j})\} =n^{-1}(\eta-3)\sigma_{w}^{4}+\sigma_{w}^{4} \text{for }\omega_{j} \neq 0,\,1/2\] \[\mathrm{var}\{I(\omega_{j})\} =n^{-1}(\eta-3)\sigma_{w}^{4}+2\sigma_{w}^{4} \text{for }\omega_{j} =0,\,1/2\] \[\mathrm{cov}\{I(\omega_{j}),I(\omega_{k})\} =n^{-1}(\eta-3)\sigma_{w}^{4} \text{for }\omega_{j} \neq\omega_{k},\]

which establishes the result in this case. We also note that if \(w_{t}\) is Gaussian, then \(\eta=3\) and the periodogram ordinates are independent. Using Lemma C.4, we may establish the following fundamental result.

**Theorem C.5**: _Suppose \(x_{t}\) is the linear process given by (C.20). Then, with \(\hat{f}(\omega)\) defined in (4.64) and corresponding conditions on the weights \(h_{k}\), we have, as \(n\to\infty\),_

1. \(\mathrm{E}\left(\hat{f}(\omega)\right)\to f(\omega)\)__
2. \(\left(\sum_{k=-m}^{m}h_{k}^{2}\right)^{-1}\mathrm{cov}\left(\hat{f}(\omega), \hat{f}(\lambda)\right)\to f^{2}(\omega)\quad\mathrm{for}\ \omega= \lambda\neq 0,1/2.\)__

_In (ii), replace \(f^{2}(\omega)\) by 0 if \(\omega\neq\lambda\) and by \(2f^{2}(\omega)\) if \(\omega=\lambda=0\) or \(1/2\).__Proof:_ (i): First, recall (4.36)

\[\mathrm{E}\left[I(\omega_{j:n})\right]=\sum_{h=-(n-1)}^{n-1}\left(\frac{n-|h|}{n} \right)\,\gamma(h)e^{-2\pi i\omega_{j:n}h}\stackrel{{\mathrm{def}}} {{=}}f_{n}(\omega_{j:n}).\]

But since \(f_{n}(\omega_{j:n})\to f(\omega)\) uniformly, and \(|f(\omega_{j:n})-f(\omega_{j:n}+k/n)|\to 0\) by the continuity of \(f\), we have

\[\mathrm{E}\hat{f}(\omega) =\sum_{k=-m}^{m}h_{k}\mathrm{E}I(\omega_{j:n}+k/n)=\sum_{k=-m}^{m} h_{k}f_{n}(\omega_{j:n}+k/n)\] \[=\sum_{k=-m}^{m}h_{k}\left[f(\omega)+o(1)\right]\to f(\omega),\]

because \(\sum_{k=-m}^{m}h_{k}=1\).

(ii): First, suppose we have \(\omega_{j:n}\to\omega_{1}\) and \(\omega_{\ell:n}\to\omega_{2}\), and \(\omega_{1}\neq\omega_{2}\). Then, for \(n\) large enough to separate the bands, using Lemma C.4, we have

\[\left|\mathrm{cov}\left(\hat{f}(\omega_{1}),\hat{f}(\omega_{2}) \right)\right| =\left|\sum_{|k|\leq m}\sum_{|r|\leq m}h_{k}\;h_{r}\mathrm{cov} \left[I(\omega_{j:n}+k/n),I(\omega_{\ell:n}+r/n)\right]\right|\] \[=\left|\sum_{|k|\leq m}\sum_{|r|\leq m}h_{k}\;h_{r}\;O(n^{-1})\right|\] \[\leq\frac{c}{n}\left(\sum_{|k|\leq m}h_{k}\right)^{2}\qquad\text {(where c is a constant)}\] \[\leq\frac{cL}{n}\left(\sum_{|k|\leq m}h_{k}^{2}\right),\]

which establishes (ii) for the case of different frequencies. The case of the same frequencies, i.e., \(\omega=\lambda\), is established in a similar manner to the above arguments. \(\square\)

Theorem C.5 justifies the distributional properties used throughout Sect. 4.4 and Chap. 7. We may extend the results of this section to vector series of the form \(x_{t}=(x_{t1},\ldots,x_{tp})^{\prime}\), when the cross-spectrum is given by

\[f_{ij}(\omega)=\sum_{h=-\infty}^{\infty}\gamma_{ij}(h)\mathrm{e}^{-2\pi i\omega h }=c_{ij}(\omega)-iq_{ij}(\omega),\] (C.21)

where

\[c_{ij}(\omega)=\sum_{h=-\infty}^{\infty}\gamma_{ij}(h)\cos(2\pi\omega h)\] (C.22)and

\[q_{ij}(\omega)=\sum_{h=-\infty}^{\infty}\gamma_{ij}(h)\sin(2\pi\omega h)\] (C.23)

denote the cospectrum and quadspectrum, respectively. We denote the DFT of the series \(x_{tj}\) by

\[d_{j}(\omega_{k}) = n^{-1/2}\sum_{t=1}^{n}x_{tj}\ \mathrm{e}^{-2\pi i\omega_{k}t}\] \[= d_{cj}(\omega_{k})-id_{sj}(\omega_{k}),\]

where \(d_{cj}\) and \(d_{sj}\) are the cosine and sine transforms of \(x_{tj}\), for \(j=1,2,\ldots,p\). We bound the covariance structure as before and summarize the results as follows.

**Theorem C.6**: _The covariance structure of the multivariate cosine and sine transforms, subject to_

\[\theta_{ij}=\sum_{h=-\infty}^{\infty}|h||\gamma_{ij}(h)|<\infty,\] (C.24)

_is given by_

\[\mathrm{E}[d_{ci}(\omega_{k})d_{cj}(\omega_{\ell})]=\left\{\begin{array}{ll} \frac{1}{2}c_{ij}(\omega_{k})+O(n^{-1}),&k=\ell\\ O(n^{-1}),&k\neq\ell.\end{array}\right.\] (C.25)

\[\mathrm{E}[d_{ci}(\omega_{k})d_{sj}(\omega_{\ell})]=\left\{\begin{array}{ll} -\frac{1}{2}q_{ij}(\omega_{k})+O(n^{-1}),&k=\ell\\ O(n^{-1}),&k\neq\ell\end{array}\right.\] (C.26)

\[\mathrm{E}[d_{si}(\omega_{k})d_{cj}(\omega_{\ell})]=\left\{\begin{array}{ll} \frac{1}{2}q_{ij}(\omega_{k})+O(n^{-1}),&k=\ell\\ O(n^{-1}),&k\neq\ell\end{array}\right.\] (C.27)

\[\mathrm{E}[d_{si}(\omega_{k})d_{sj}(\omega_{\ell})]=\left\{\begin{array}{ll} \frac{1}{2}c_{ij}(\omega_{k})+O(n^{-1}),&k=\ell\\ O(n^{-1}),&k\neq\ell.\end{array}\right.\] (C.28)

_Proof:_ We define

\[S_{n}^{ij}(\omega_{k},\omega_{\ell})=\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma_{ij}(s -t)\mathrm{e}^{-2\pi i\omega_{k}s}\mathrm{e}^{2\pi i\omega_{\ell}t}.\] (C.29)

Then, we may verify the theorem with manipulations like \[\mathrm{E}[d_{ci}(\omega_{k})d_{sj}(\omega_{k})]\] \[\qquad=\frac{1}{4i}\sum_{s=1}^{n}\sum_{t=1}^{n}\gamma_{ij}(s-t)( \mathrm{e}^{2\pi i\omega_{k}s}+\mathrm{e}^{-2\pi i\omega_{k}s})(\mathrm{e}^{2\pi i \omega_{k}t}-\mathrm{e}^{-2\pi i\omega_{k}t})\] \[\qquad=\frac{1}{4i}\left[S_{n}^{ij}(-\omega_{k},\omega_{k})+S_{n} ^{ij}(\omega_{k},\omega_{k})-S_{n}^{ij}(\omega_{k},\omega_{k})-S_{n}^{ij}( \omega_{k},-\omega_{k})\right]\] \[\qquad=\frac{1}{4i}\left[c_{ij}(\omega_{k})-iq_{ij}(\omega_{k})-( c_{ij}(\omega_{k})+iq_{ij}(\omega_{k}))+O(n^{-1})\right]\] \[\qquad=-\frac{1}{2}q_{ij}(\omega_{k})+O(n^{-1}),\]

where we have used the fact that the properties given in Lemma C.1-Lemma C.3 can be verified for the cross-spectral density functions \(f_{ij}(\omega),\;i,j=1,\ldots,p\). 

Now, if the underlying multivariate time series \(x_{t}\) is a normal process, it is clear that the DFTs will be jointly normal and we may define the vector DFT, \(d(\omega_{k})=(d_{1}(\omega_{k}),\ldots,d_{p}(\omega_{k}))^{\prime}\) as

\[d(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}x_{t}\;\mathrm{e}^{-2\pi i\omega_{k}t}=d_{c }(\omega_{k})-id_{s}(\omega_{k}),\] (C.30)

where

\[d_{c}(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}x_{t}\;\cos(2\pi\omega_{k}t)\] (C.31)

and

\[d_{s}(\omega_{k})=n^{-1/2}\sum_{t=1}^{n}x_{t}\;\sin(2\pi\omega_{k}t)\] (C.32)

are the cosine and sine transforms, respectively, of the observed vector series \(x_{t}\). Then, constructing the vector of real and imaginary parts \((d_{c}^{\prime}(\omega_{k}),d_{s}^{\prime}(\omega_{k}))^{\prime}\), we may note it has mean zero and \(2p\times 2p\) covariance matrix

\[\Sigma(\omega_{k})=\frac{1}{2}\begin{pmatrix}C(\omega_{k})&-Q(\omega_{k})\\ Q(\omega_{k})&C(\omega_{k})\end{pmatrix}\] (C.33)

to order \(n^{-1}\) as long as \(\omega_{k}-\omega=O(n^{-1})\). We have introduced the \(p\times p\) matrices \(C(\omega_{k})=\{c_{ij}(\omega_{k})\}\) and \(Q=\{q_{ij}(\omega_{k})\}\). The complex random variable \(d(\omega_{k})\) has covariance

\[S(\omega_{k}) =\mathrm{E}[d(\omega_{k})d^{*}(\omega_{k})]\] \[=\mathrm{E}\left[\big{(}d_{c}(\omega_{k})-id_{s}(\omega_{k}) \big{)}\big{(}d_{c}(\omega_{k})-id_{s}(\omega_{k})\big{)}^{*}\right]\] \[=\mathrm{E}[d_{c}(\omega_{k})d_{c}(\omega_{k})^{\prime}]+\mathrm{ E}[d_{s}(\omega_{k})d_{s}(\omega_{k})^{\prime}]\] \[\quad-i\big{(}\mathrm{E}[d_{s}(\omega_{k})d_{c}(\omega_{k})^{ \prime}]-\mathrm{E}[d_{c}(\omega_{k})d_{s}(\omega_{k})^{\prime}]\big{)}\] \[=C(\omega_{k})-iQ(\omega_{k}).\] (C.34)If the process \(x_{t}\) has a multivariate normal distribution, the complex vector \(d(\omega_{k})\) has approximately the _complex multivariate normal distribution_ with mean zero and covariance matrix \(S(\omega_{k})=C(\omega_{k})-iQ(\omega_{k})\) if the real and imaginary parts have the covariance structure as specified above. In the next section, we work further with this distribution and show how it adapts to the real case. If we wish to estimate the spectral matrix \(S(\omega)\), it is natural to take a band of frequencies of the form \(\omega_{k:n}+\ell/n\), for \(\ell=-m,\ldots,m\) as before, so that the estimator becomes (4.98) of Sect. 4.5. A discussion of further properties of the multivariate complex normal distribution is deferred.

It is also of interest to develop a large sample theory for cases in which the underlying distribution is not necessarily normal. If \(x_{t}\) is not necessarily a normal process, some additional conditions are needed to get asymptotic normality. In particular, introduce the notion of a _generalized linear process_

\[y_{t}=\sum_{r=-\infty}^{\infty}A_{r}w_{t-r},\] (C.35)

where \(w_{t}\) is a \(p\times 1\) vector white noise process with \(p\times p\) covariance \(\mathrm{E}[w_{t}w_{t}^{\prime}]=G\) and the \(p\times p\) matrices of filter coefficients \(A_{t}\) satisfy

\[\sum_{t=-\infty}^{\infty}\mathrm{tr}\{A_{t}A_{t}^{\prime}\}=\sum_{t=-\infty}^ {\infty}\|A_{t}\|^{2}<\infty.\] (C.36)

In particular, stable vector ARMA processes satisfy these conditions. For generalized linear processes, we state the following general result from Hannan [86, p.224].

**Theorem C.7**: _If \(x_{t}\) is generated by a generalized linear process with a continuous spectrum that is not zero at \(\omega\) and \(\omega_{k:n}+\ell/n\) are a set of frequencies within \(L/n\) of \(\omega\), the joint density of the cosine and sine transforms (C.31) and (C.32) converges to that of \(L\) independent \(2p\times 1\) normal vectors with covariance matrix \(\Sigma(\omega)\) with structure given by (C.33). At \(\omega=0\) or \(\omega=1/2\), the distribution is real with covariance matrix \(2\Sigma(\omega)\)._

The above result provides the basis for inference involving the Fourier transforms of stationary series because it justifies approximations to the likelihood function based on multivariate normal theory. We make extensive use of this result in Chap. 7, but will still need a simple form to justify the distributional result for the sample coherence given in (4.104). The next section gives an elementary introduction to the complex normal distribution.

### The Complex Multivariate Normal Distribution

The multivariate normal distribution will be the fundamental tool for expressing the likelihood function and determining approximate maximum likelihood estimators and their large sample probability distributions. A detailed treatment of the multivariatenormal distribution can be found in standard texts such as Anderson [7]. We will use the multivariate normal distribution of the \(p\times 1\) vector \(x=(x_{1},x_{2},\ldots,x_{p})^{\prime}\), as defined by its density function

\[p(x)=(2\pi)^{-p/2}|\Sigma|^{-1/2}\exp\left\{-\tfrac{1}{2}(x-\mu)^{\prime}\Sigma^ {-1}(x-\mu)\right\},\] (C.37)

which has mean vector \(\mathrm{E}[x]=\mu=(\mu_{1},\ldots,\mu_{p})^{\prime}\) and covariance matrix

\[\Sigma=\mathrm{E}[(x-\mu)(x-\mu)^{\prime}].\] (C.38)

We use the notation \(x\sim N_{p}(\mu,\Sigma)\) for densities of the form (C.37) and note that linearly transformed multivariate normal variables of the form \(y=Ax\), with \(A\) a \(q\times p\) matrix \(q\leq p\), will also be multivariate normal with distribution

\[y\sim N_{q}(A\mu,A\Sigma A^{\prime}).\] (C.39)

Often, the partitioned multivariate normal, based on the vector \(x=(x_{1}^{\prime},x_{2}^{\prime})^{\prime}\), split into two \(p_{1}\times 1\) and \(p_{2}\times 1\) components \(x_{1}\) and \(x_{2}\), respectively, will be used where \(p=p_{1}+p_{2}\). If the mean vector \(\mu=(\mu_{1}^{\prime},\mu_{2}^{\prime})^{\prime}\) and covariance matrices

\[\Sigma=\begin{pmatrix}\Sigma_{11}&\Sigma_{12}\\ \Sigma_{21}&\Sigma_{22}\end{pmatrix}\] (C.40)

are also compatibly partitioned, the marginal distribution of any subset of components is multivariate normal, say,

\[x_{1}\sim N_{p_{1}}\{\mu_{1},\Sigma_{11}\},\]

and that the conditional distribution \(x_{2}\) given \(x_{1}\) is normal with mean

\[\mathrm{E}[x_{2}\mid x_{1}]=\mu_{2}+\Sigma_{21}\Sigma_{11}^{-1}(x_{1}-\mu_{1})\] (C.41)

and conditional covariance

\[\mathrm{cov}[x_{2}\mid x_{1}]=\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{1 2}.\] (C.42)

In the previous section, the real and imaginary parts of the DFT had a partitioned covariance matrix as given in (C.33), and we use this result to say the complex \(p\times 1\) vector

\[z=x_{1}-ix_{2}\] (C.43)

has a _complex multivariate normal distribution_, with mean vector \(\mu_{z}=\mu_{1}-i\mu_{2}\) and \(p\times p\) covariance matrix

\[\Sigma_{z}=C-iQ\] (C.44)

if the real multivariate \(2p\times 1\) normal vector \(x=(x_{1}^{\prime},x_{2}^{\prime})^{\prime}\) has a real multivariate normal distribution with mean vector \(\mu=(\mu_{1}^{\prime},\mu_{2}^{\prime})^{\prime}\) and covariance matrix

\[\Sigma=\frac{1}{2}\begin{pmatrix}C-Q\\ Q&C\end{pmatrix}.\] (C.45)The restrictions \(C^{\prime}=C\) and \(Q^{\prime}=-Q\) are necessary for the matrix \(\mathcal{I}\) to be a covariance matrix, and these conditions then imply \(\Sigma_{z}=\Sigma_{z}^{*}\) is Hermitian. The probability density function of the complex multivariate normal vector \(z\) can be expressed in the concise form

\[p_{z}(z)=\pi^{-p}|\Sigma_{z}|^{-1}\exp\{-(z-\mu_{z})^{*}\Sigma_{z}^{-1}(z-\mu_{ z})\},\] (C.46)

and this is the form that we will often use in the likelihood. The result follows from showing that \(p_{x}(x_{1},x_{2})=p_{z}(z)\) exactly, using the fact that the quadratic and Hermitian forms in the exponent are equal and that \(|\Sigma_{x}|=|\Sigma_{z}|^{2}\). The second assertion follows directly from the fact that the matrix \(\Sigma_{x}\) has repeated eigenvalues, \(\lambda_{1},\lambda_{2},\ldots,\lambda_{p}\) corresponding to eigenvectors \((\alpha_{1}^{\prime},\alpha_{2}^{\prime})^{\prime}\) and the same set, \(\lambda_{1}\), \(\lambda_{2},\ldots,\lambda_{p}\) corresponding to \((\alpha_{2}^{\prime},-\alpha_{1}^{\prime})^{\prime}\). Hence

\[|\Sigma_{x}|=\prod_{i=1}^{p}\lambda_{i}^{2}=|\Sigma_{z}|^{2}.\]

For further material relating to the complex multivariate normal distribution, see Goodman [75], Giri [73], or Khatri [117].

**Example C.2**: _A Complex Normal Random Variable_

To fix ideas, consider a very simple complex random variable

\[z=\Re(z)-i\Im(z)=z_{1}-iz_{2},\]

where \(z_{1}\sim N(0,\frac{1}{2}\sigma^{2})\) independent of \(z_{2}\sim N(0,\frac{1}{2}\sigma^{2})\). Then the joint density of \((z_{1},z_{2})\) is

\[p(z_{1},z_{2})\propto\sigma^{-1}\exp\left(-\frac{z_{1}^{2}}{\sigma^{2}}\right) \times\sigma^{-1}\exp\left(-\frac{z_{2}^{2}}{\sigma^{2}}\right)=\sigma^{-2} \exp\left\{-\left(\frac{z_{1}^{2}+z_{2}^{2}}{\sigma^{2}}\right)\right\}.\]

More succinctly, we write \(z\sim N_{c}(0,\sigma^{2})\), and

\[p(z)\propto\sigma^{-2}\exp\left(-\frac{z^{*}\,z}{\sigma^{2}}\right).\]

In Fourier analysis, \(z_{1}\) would be the cosine transform of the data at a fundamental frequency (excluding the end points) and \(z_{2}\) the corresponding sine transform. If the process is Gaussian, \(z_{1}\) and \(z_{2}\) are independent normals with zero means and variances that are half of the spectral density at the particular frequency. Consequently, the definition of the complex normal distribution is natural in the context of spectral analysis.

**Example C.3**: _A Bivariate Complex Normal Distribution_

Consider the joint distribution of the complex random variables \(u_{1}=x_{1}-ix_{2}\) and \(u_{2}=y_{1}-iy_{2}\), where the partitioned vector \((x_{1},x_{2},y_{1},y_{2})^{\prime}\) has a real multivariate normal distribution with mean \((0,0,0,0)^{\prime}\) and covariance matrix \[\Sigma=\frac{1}{2}\left(\begin{array}{ccc|ccc}c_{xx}&0&c_{xy}&-q_{xy}\\ 0&c_{xx}&q_{xy}&c_{xy}\\ \hline c_{xy}&q_{xy}&c_{yy}&0\\ -q_{xy}&c_{yx}&0&c_{yy}\end{array}\right).\] (C.47)

Now, consider the conditional distribution of \(y=(y_{1},y_{2})^{\prime}\), given \(x=(x_{1},x_{2})^{\prime}\). Using (C.41), we obtain

\[\mathrm{E}(y\bigm{|}x)=\left(\begin{array}{ccc}x_{1}&-x_{2}\\ x_{2}&x_{1}\end{array}\right)\left(\begin{array}{ccc}b_{1}\\ b_{2}\end{array}\right),\] (C.48)

where

\[(b_{1},b_{2})=\left(\frac{c_{yx}}{c_{xx}},\frac{q_{yx}}{c_{xx}}\right).\] (C.49)

It is natural to identify the cross-spectrum

\[f_{xy}=c_{xy}-iq_{xy},\] (C.50)

so that the complex variable identified with the pair is just

\[b=b_{1}-ib_{2}=\frac{c_{yx}-iq_{yx}}{c_{xx}}=\frac{f_{yx}}{f_{xx}},\]

and we identify it as the complex regression coefficient. The conditional covariance follows from (C.42) and simplifies to

\[\mathrm{cov}(y\bigm{|}x)=\tfrac{1}{2}f_{y\cdot x}\ I_{2},\] (C.51)

where \(I_{2}\) denotes the \(2\times 2\) identity matrix and

\[f_{y\cdot x}=c_{yy}-\frac{c_{xy}^{2}+q_{xy}^{2}}{c_{xx}}=f_{yy}-\frac{|f_{xy}| ^{2}}{f_{xx}}\] (C.52)

Example C.3 leads to an approach for justifying the distributional results for the function coherence given in (4.104). That equation suggests that the result can be derived using the regression results that lead to the F-statistics in Sect. 2.1. Suppose that we consider \(L\) values of the sine and cosine transforms of the input \(x_{t}\) and output \(y_{t}\), which we will denote by \(d_{x,c}(\omega_{k}+\ell/n),d_{x,s}(\omega_{k}+\ell/n),d_{y,c}(\omega_{k}+\ell/ n),d_{y,s}(\omega_{k}+\ell/n)\), sampled at \(L=2m+1\) frequencies, \(\ell=-m,\ldots,m\), in the neighborhood of some target frequency \(\omega\). Suppose these cosine and sine transforms are re-indexed and denoted by \(d_{x,cj},d_{x,sj},d_{y,cj},d_{y,sj}\), for \(j=1,2,\ldots,L\), producing \(2L\) real random variables with a large sample normal distribution that have limiting covariance matrices of the form (C.47) for each \(j\). Then, the conditional normal distribution of the \(2\times 1\) vector \(d_{y,cj},d_{y,sj}\) given \(d_{x,cj},d_{x,sj}\), given in Example C.3, shows that we may write, approximately, the regression model

\[\left(\begin{array}{ccc}d_{y,cj}\\ d_{y,sj}\end{array}\right)=\left(\begin{array}{ccc}d_{x,cj}&-d_{x,sj}\\ d_{x,sj}&d_{x,cj}\end{array}\right)\left(\begin{array}{ccc}b_{1}\\ b_{2}\end{array}\right)+\left(\begin{array}{ccc}V_{cj}\\ V_{sj}\end{array}\right),\]where \(V_{cj}\), \(V_{sj}\) are approximately uncorrelated with approximate variances

\[\mathbb{E}[V_{cj}^{2}]=\mathbb{E}[V_{sj}^{2}]=(1/2)f_{y\cdot x}.\]

Now, construct, by stacking, the \(2L\times 1\) vectors \(y_{c}=(d_{y,c1},\ldots,d_{y,cL})^{\prime}\), \(y_{s}=(d_{y,s1},\ldots,d_{y,sL})^{\prime}\), \(x_{c}=(d_{x,c1},\ldots,d_{x,cL})^{\prime}\) and \(x_{s}=(d_{x,s1},\ldots,d_{x,sL})^{\prime}\), and rewrite the regression model as

\[\left(\begin{array}{c}y_{c}\\ y_{s}\end{array}\right)=\left(\begin{array}{cc}x_{c}&-x_{s}\\ x_{s}&x_{c}\end{array}\right)\left(\begin{array}{c}b_{1}\\ b_{2}\end{array}\right)+\left(\begin{array}{c}v_{c}\\ v_{s}\end{array}\right)\]

where \(v_{c}\) and \(v_{s}\) are the error stacks. Finally, write the overall model as the regression model in Chap. 2, namely,

\[y=Zb+v,\]

making the obvious identifications in the previous equation. Conditional on \(Z\), the model becomes exactly the regression model considered in Chap. 2 where there are \(q=2\) regression coefficients and \(2L\) observations in the observation vector \(y\). To test the hypothesis of no regression for that model, we use an F-Statistic that depends on the difference between the residual sum of squares for the full model, say,

\[SSE=y^{\prime}y-y^{\prime}Z(Z^{\prime}Z)^{-1}Z^{\prime}y\] (C.53)

and the residual sum of squares for the reduced model, \(SSE_{0}=y^{\prime}y\). Then,

\[F_{2,2L-2}=(L-1)\frac{SSE_{0}-SSE}{SSE}\] (C.54)

has the F-distribution with \(2\) and \(2L-2\) degrees of freedom. Also, it follows by substitution for \(y\) that

\[SSE_{0}=y^{\prime}y=y_{c}^{\prime}y_{c}+y_{s}^{\prime}y_{s}=\sum_{j=1}^{L}(d_{ y,cj}^{2}+d_{y,sj}^{2})=L\hat{f}_{y}(\omega),\]

which is just the sample spectrum of the output series. Similarly,

\[Z^{\prime}Z=\left(\begin{array}{cc}L\hat{f}_{x}&0\\ 0&L\hat{f}_{x}\end{array}\right)\]

and

\[Z^{\prime}y =\left(\begin{array}{c}(x_{c}^{\prime}y_{c}+x_{s}^{\prime}y_{s })\\ (x_{c}^{\prime}y_{s}-x_{s}^{\prime}y_{c})\end{array}\right)\] \[=\left(\begin{array}{c}\sum_{j=1}^{L}(d_{x,cj}d_{y,cj}+d_{x,sj} d_{y,sj})\\ \sum_{j=1}^{L}(d_{x,cj}d_{y,sj}-d_{x,sj}d_{y,cj})\end{array}\right)\] \[=\left(\begin{array}{c}L\hat{c}_{yx}\\ L\hat{q}_{yx}\end{array}\right).\]together imply that

\[y^{\prime}Z(Z^{\prime}Z)^{-1}Z^{\prime}y=L\;|\hat{f}_{xy}|^{2}/\hat{f}_{x}.\]

Substituting into (C.54) gives

\[F_{2,2L-2}=(L-1)\frac{|\hat{f}_{xy}|^{2}/\hat{f}_{x}}{\left(\hat{f}_{y}-|\hat{f }_{xy}|^{2}/\hat{f}_{x}\right)},\]

which converts directly into the F-statistic (4.104), using the sample coherence defined in (4.103).

### Integration

In Chap. 4 and in this appendix, we use Riemann-Stieltjes integration and stochastic integration. We now give a cursory introduction to these concepts for readers unfamiliar with the techniques.

#### Riemann-Stieltjes Integration

Rather than work in complete generality, we focus on the meaning of (4.14),

\[\gamma(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\;dF( \omega).\]

Here, we are concerned with the integration of a bounded, continuous (complex-valued) function \(g(\omega)=\mathrm{e}^{2\pi i\omega h}\) with respect to a monotonically increasing, right continuous (real-valued) function \(F(\omega)\).

Let \(\mathcal{Q}=\{-\frac{1}{2}=\omega_{0},\omega_{1},\ldots,\omega_{n}=\frac{1}{2}\}\) be a partition of the interval, and define the sum

\[S_{\mathcal{Q}}(g,F)=\sum_{j=1}^{n}g(u_{j})[F(\omega_{j})-F(\omega_{j-1})]\] (C.55)

where \(u_{j}\in[\omega_{j-1},\omega_{j}]\). In our case, there is a unique number, say \(\mathcal{I}(g,F)\) such that for any \(\epsilon>0\), there is a \(\delta>0\) for which

\[|S_{\mathcal{Q}}(g,F)-\mathcal{I}(g,F)|<\epsilon\]

for any partition \(\mathcal{Q}\) with \(\max_{j}|\omega_{j}-\omega_{j-1}|<\delta\) and any \(u_{j}\in[\omega_{j-1},\omega_{j}]\) for \(j=1,\ldots,n\). In this case, we define

\[\mathcal{I}(g,F)=\int_{-\frac{1}{2}}^{\frac{1}{2}}g(\omega)dF(\omega)\,.\] (C.56)

In the _absolutely continuous case_, such as in Property 4.2, \(dF(\omega)=f(\omega)d\omega\) and, as stated in the property,\[\gamma(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\,dF(\omega)= \int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\,f(\omega)d\omega\,.\]

Another case that we discussed was the _discrete case_ such as in Example 4.4 where the spectral distribution \(F(\omega)\) makes jumps at specific values of \(\omega\). First, consider the case where \(F(\omega)\) has only one jump of size \(c>0\) at \(\omega^{*}\in(-\frac{1}{2},\frac{1}{2})\), so that \(F(\omega)=0\) if \(\omega<\omega^{*}\) and \(F(\omega)=c\) if \(\omega\geq\omega^{*}\). Then considering \(S_{\Omega}(g,F)\) in (C.55), note that \(F(\omega_{j})-F(\omega_{j-1})=0\) for all intervals that do not include \(\omega^{*}\). Now suppose in some \(k\)th interval of the partition, \(\omega^{*}\in(\omega_{k-1},\omega_{k}]\) for a \(k\in\{1,\ldots,n\}\). Then

\[S_{\Omega}(g,F)=\sum_{j=1}^{n}g(u_{j})[F(\omega_{j})-F(\omega_{j-1})]=g(u_{k}) \,c\,,\]

where \(u_{k}\in[\omega_{k-1},\omega_{k}]\). Thus,

\[|S_{\Omega}(g,F)-g(\omega^{*})\,c|=c\,|g(u_{k})-g(\omega^{*})|.\]

Since \(g\) is continuous, given \(\epsilon>0\), there is a \(\delta>0\) such that \(|g(u_{k})-g(\omega^{*})|<\epsilon/c\) when \(|u_{k}-\omega^{*}|<\delta\). Hence, for any partition \(\Omega\) with \(\max_{j}|\omega_{j}-\omega_{j-1}|<\delta\), we have \(|S_{\Omega}(g,F)-g(\omega^{*})\,c|<\epsilon\), and consequently,

\[\int_{-\frac{1}{2}}^{\frac{1}{2}}g(\omega)\,dF(\omega)=g(\omega^{*})\,c\,.\]

This result may be extended in an obvious way to the case where \(F\) makes jumps at more than one value as was the case in Example 4.4.

**Example C.4**: **Complex Harmonic Process**__

Recall (4.4) where we considered a mix of periodic components. In that example, the process was real, but it is possible to consider a complex-valued process in a similar way. In this case, we define

\[x_{t}=\sum_{j=1}^{q}Z_{j}\,\mathrm{e}^{2\pi it\omega_{j}},\quad-\tfrac{1}{2}< \omega_{1}<\cdots<\omega_{q}<\tfrac{1}{2}\,,\] (C.57)

where the \(Z_{j}\) are uncorrelated complex-valued random variables such that \(\mathrm{E}[Z_{j}]=0\) and \(\mathrm{E}[|Z_{j}|^{2}]=\sigma_{j}>0\). As discussed in Example 4.9, the case where \(x_{t}\) is real-valued is a special case of (C.57). Extending Example 4.4 to the case of (C.57), we have

\[F(\omega)=\begin{cases}0&-\frac{1}{2}\leq\ \omega<\omega_{1}\,,\\ \sigma_{1}^{2}&\omega_{1}\leq\ \omega<\omega_{2}\,,\\ \sigma_{1}^{2}+\sigma_{2}^{2}&\omega_{2}\leq\ \omega<\omega_{3}\,,\\ \sigma_{1}^{2}+\sigma_{2}^{2}+\sigma_{3}^{2}&\omega_{3}\leq\ \omega<\omega_{4}\,,\\ \vdots&\vdots\\ \sigma_{1}^{2}+\sigma_{2}^{2}+\cdots+\sigma_{q}^{2}&\omega_{q}\leq\ \omega\leq\tfrac{1}{2}\,.\end{cases}\] (C.58)Thus, for the process in this example,

\[\gamma_{x}(h)=\int_{-\frac{1}{2}}^{\frac{1}{2}}\mathrm{e}^{2\pi i\omega h}\ dF( \omega)=\sum_{j=1}^{q}\sigma_{j}^{2}\,\mathrm{e}^{2\pi ih\omega_{j}}\.\]

Note that \(\gamma_{x}(h)\) is complex, but satisfies the properties of an autocovariance function: (i) \(\gamma_{x}(h)\) is a Hermitian function, \(\gamma_{x}(h)=\gamma_{x}^{*}(-h)\); (ii) \(0\leq|\gamma_{x}(h)|\leq\gamma_{x}(0)\), and (iii) \(\gamma_{x}(h)\) is non-negative definite. As in the real case, the total variance of the process is the sum of the variances of the individual components, \(\mathrm{var}(x_{t})=\gamma_{x}(0)=\sum_{j=1}^{q}\sigma_{j}^{2}\).

#### Stochastic Integration

We first used stochastic integration in Example 4.9, although it was not necessary for that particular example. There is an analogy of stochastic integration to Riemann-Stieltjes integration defined in the previous subsection, however, we will have to deal with convergence of random processes rather than convergence of numbers. We focus on the case of interest to us; namely the stochastic integral in Theorem C.2,

\[x_{t}=\int_{-\frac{1}{2}}^{\frac{1}{2}}g(\omega)dZ(\omega)\,,\]

where \(Z(\omega)\) is a complex-valued _orthogonal increment process_ and \(g(\omega)=\mathrm{e}^{2\pi i\omega t}\). For \(\{Z(\omega);\ \omega\in[-\frac{1}{2},\frac{1}{2}]\}\) and \(-\frac{1}{2}\leq\omega_{1}<\omega_{2}<\omega_{3}<\omega_{4}\leq\frac{1}{2}\), we have

* \(Z(-\frac{1}{2})=0\),
* \(\mathrm{E}[Z(\omega)]=0\),
* \(\mathrm{var}[Z(\omega)]=\mathrm{E}[|Z(\omega)|^{2}]=\mathrm{E}[Z(\omega)\,Z^{ *}(\omega)]<\infty\),
* \(\mathrm{E}\{[Z(\omega_{4})-Z(\omega_{3})]\,[Z(\omega_{2})-Z(\omega_{1})]^{*}\}=0\).

As an example, recall Brownian motion in Definition 5.1.

We say \(\{Z(\omega)\}\) is _mean square (m.s.) right continuous_ if \(\mathrm{E}|Z(\omega+\delta)-Z(\omega)|^{2}\to 0\) as \(\delta\downarrow 0\). An important result is that such a process admits a spectral distribution.

**Theorem C.8**: _If \(\{Z(\omega);\ \omega\in[-\frac{1}{2},\frac{1}{2}]\}\) is an orthogonal increment process that is m.s. right continuous, then there is a unique spectral distribution function \(F\) such that_

_(1) \(F(\omega)=0\) if \(\omega\leq-\frac{1}{2}\)._

_(2) \(F(\omega)=F(\frac{1}{2})\) if \(\omega\geq\frac{1}{2}\)._

_(3) \(F(\omega_{2})-F(\omega_{1})=\mathrm{E}|Z(\omega_{2})-Z(\omega_{1})|^{2}\) if \(-\frac{1}{2}\leq\omega_{1}\leq\omega_{2}\leq\frac{1}{2}\)._

_Proof:_ Define \(F(\omega)=\mathrm{E}|Z(\omega)|^{2}\) for \(\omega\in[-\frac{1}{2},\frac{1}{2}]\), with \(F(\omega)=0\) for \(\omega\leq-\frac{1}{2}\) and \(F(\omega)=F(\frac{1}{2})\) for \(\omega\geq\frac{1}{2}\). It is immediate from the assumptions that \(F\) is right continuous and satisfies (1)-(3). To show that \(F\) is monotonically increasing, note that for \(\omega_{2}\geq\omega_{1}\),\[F(\omega_{2}) = \operatorname{E}\lvert Z(\omega_{2})-Z(\omega_{1})+Z(\omega_{1})-Z(- \tfrac{1}{2})\rvert^{2}\] \[= \operatorname{E}\lvert Z(\omega_{2})-Z(\omega_{1})\rvert^{2}+ \operatorname{E}\lvert Z(\omega_{1})\rvert^{2}\] \[\geq F(\omega_{1})\,,\]

since \([-\tfrac{1}{2},\omega_{1}]\) and \([\omega_{1},\omega_{2}]\) are non-overlapping intervals. 

Similar to the previous subsection, let \(\Omega=\{-\tfrac{1}{2}=\omega_{0},\omega_{1},\ldots,\omega_{n}=\tfrac{1}{2}\}\) be a partition of the interval, and define the random sum

\[S_{\Omega}(g,Z)=\sum_{j=1}^{n}g(u_{j})[Z(\omega_{j})-Z(\omega_{j-1})]\] (C.59)

where \(u_{j}\in[\omega_{j-1},\omega_{j}]\). We emphasize the fact that \(S_{\Omega}(g,Z)\) is a complex-valued random variable with mean and variance given by

\[\operatorname{E}[S_{\Omega}(g,Z)]=0\quad\text{and}\quad\operatorname{E}[|S_{ \Omega}(g,Z)|^{2}]=\sum_{j=1}^{n}g(u_{j})[F(\omega_{j})-F(\omega_{j-1})]\]

where \(F\) is defined in Theorem C.8. In our case, there is a unique (except on a set of probability zero) complex-valued random variable, say \(\mathcal{I}(g,Z)\) such that for any \(\epsilon>0\), there is a \(\delta>0\) for which

\[\operatorname{E}|S_{\Omega}(g,Z)-\mathcal{I}(g,Z)|^{2}<\epsilon\]

for any partition \(\Omega\) with \(\varDelta_{\Omega}=\max_{j}|\omega_{j}-\omega_{j-1}|<\delta\) and any \(u_{j}\in[\omega_{j-1},\omega_{j}]\) for \(j=1,\ldots,n\). In this case, define

\[\mathcal{I}(g,Z)=\int_{-\tfrac{1}{2}}^{\frac{1}{2}}g(\omega)dZ(\omega)\,.\] (C.60)

We see that the stochastic integral is the mean-square limit of the random sum as \(n\to\infty\) (\(\varDelta_{\Omega}\to 0\)).

Recalling Example 4.9, as in the deterministic case, it is easy to show that, if \(Z(\omega)\) is an orthogonal increment process that makes uncorrelated jumps at \(-\omega_{0}\) and \(\omega_{0}\) with mean-zero and variance \(\sigma^{2}/2\), then

\[x_{t}=\int_{-\tfrac{1}{2}}^{\frac{1}{2}}\operatorname{e}^{2\pi i\omega t}dZ( \omega)=Z(-\omega_{0})\operatorname{e}^{-2\pi i\omega_{0}t}+Z(\omega_{0}) \operatorname{e}^{2\pi i\omega_{0}t}\,.\]

In this case, the spectral distribution is (recall Example 4.4)

\[F(\omega)=\begin{cases}0&\omega<-\omega_{0},\\ \sigma^{2}/2&-\omega_{0}\leq\omega<\omega_{0},\\ \sigma^{2}&\omega\geq\omega_{0}\,,\end{cases}\]

and the autocovariance function is

\[\gamma_{x}(h)=\int_{-\tfrac{1}{2}}^{\frac{1}{2}}\operatorname{e}^{2\pi i \omega h}\,dF(\omega)=\frac{\sigma^{2}}{2}\operatorname{e}^{-2\pi i\omega_{0} h}+\frac{\sigma^{2}}{2}\operatorname{e}^{2\pi i\omega_{0}h}=\sigma^{2}\cos(2\pi \omega_{0}h)\,.\]

## Appendix C Spectral Analysis as Principal Component Analysis

In Chap. 4, we presented many different ways to view the spectral density. In this section, we show that the spectral density may be though of as the approximate eigenvalues of the covariance matrix of a stationary process. Suppose \(X=(x_{1},\ldots,x_{n})\) are \(n\) values of a real, mean-zero, time series, \(x_{t}\) with spectral density \(f_{x}(\omega)\). Then

\[\operatorname{cov}(X)=\Gamma_{n}=\begin{bmatrix}\gamma(0)&\gamma(1)&\cdots& \gamma(n-1)\\ \gamma(1)&\gamma(0)&\cdots&\gamma(n-2)\\ \vdots&\vdots&\ddots&\vdots\\ \gamma(n-1)&\gamma(n-2)&\cdots&\gamma(0)\end{bmatrix}\]

is a non-negative definite, symmetric Toeplitz matrix. Hence, there is an \(n\times n\) orthogonal matrix \(M\), such that \(M^{\prime}\Gamma_{n}M=\operatorname{diag}(\lambda_{0},\ldots,\lambda_{n-1})\), where \(\lambda_{j}\geq 0\) for \(j=0,\ldots,n-1\) are the latent roots of \(\Gamma_{n}\). In this section, we will show that, for \(n\) sufficiently large,

\[\lambda_{j}\approx f_{x}(\omega_{j}),\quad j=0,1,\ldots,n-1\,,\]

where \(\omega_{j}=j/n\) are the Fourier frequencies.

To start the approximation, we introduce a circulant matrix defined as

\[\Gamma_{c}=\begin{bmatrix}c(0)&c(1)\cdots&c(n-2)&c(n-1)\\ c(n-1)&c(0)\cdots&c(n-3)&c(n-2)\\ \vdots&\vdots&\ddots&\vdots&\vdots\\ c(2)&c(3)\cdots&c(0)&c(1)\\ c(1)&c(2)\cdots&c(n-1)&c(0)\end{bmatrix}\,;\]

the matrix has \(c(0)\) on the diagonal, then continue to the right \(c(1)\), \(c(2)\),..., and wrap the sequence around to the first column after the last column is reached. Using direct substitution, it can be shown that the latent roots and vectors of \(\Gamma_{c}\) are

\[\lambda_{j}=\sum_{h=0}^{n-1}c(h)\,\mathrm{e}^{-2\pi ihj/n}\,,\]

and

\[g_{j}^{*}=\tfrac{1}{\sqrt{n}}\Big{(}\mathrm{e}^{-2\pi i0\frac{j}{n}},\mathrm{ e}^{-2\pi i1\frac{j}{n}},\ldots,\mathrm{e}^{-2\pi i(n-1)\frac{j}{n}}\Big{)}\,,\]

for \(j=0,1,\ldots,n-1\).

If \(\Gamma_{c}\) is symmetric [\(c(j)=c(n-j)\)], call it \(\Gamma_{s}\) and let \(c(h)=c(-h)\). Noting that \(\mathrm{e}^{-2\pi ihj/n}=\mathrm{e}^{-2\pi i(n-h)j/n}\), we have for \(n\) odd,

\[\lambda_{j}=\sum_{|h|\leq\frac{n-1}{2}}c(h)\,\mathrm{e}^{-2\pi ihj/n}=\sum_{|h |\leq\frac{n-1}{2}}c(h)\,\cos(2\pi hj/n)\]

for \(j=0,1,\ldots,n-1\). If \(n\) is even, the sum would include one extra term for \(j/n=1/2\).

We see that \(\lambda_{0}\) is a distinct root, and \(\lambda_{j}=\lambda_{n-j}\) are repeated roots for \(j=1,\ldots,\frac{n-1}{2}\). For each repeated root, we can find a pair of eigenvectors corresponding to \(\lambda_{j}\), namely

\[\begin{array}{l}v^{\prime}_{j}=\frac{1}{\sqrt{2}}(g^{*}_{j}+g^{*}_{n-j})= \frac{\sqrt{2}}{\sqrt{n}}\Big{(}1,\cos(2\pi j/n),\ldots,\cos(2\pi(n-1)j/n)\Big{)} \,;\\ u^{\prime}_{j}=\frac{1}{\sqrt{2}}i(g^{*}_{j}-g^{*}_{n-j})=\frac{\sqrt{2}}{\sqrt {n}}\Big{(}0,\sin(2\pi j/n),\ldots,\sin(2\pi(n-1)j/n)\Big{)}\,.\end{array}\]

For \(\lambda_{0}\), the corresponding eigenvector is \(v^{\prime}_{0}=g^{*}_{0}=\frac{1}{\sqrt{n}}(1,1,\ldots,1)=\frac{\sqrt{2}}{ \sqrt{n}}(\frac{1}{\sqrt{2}},\ldots,\frac{1}{\sqrt{2}})\). Now define the matrix \(Q\) as

\[Q=\left[\begin{array}{c}v^{\prime}_{0}\\ v^{\prime}_{1}\\ u^{\prime}_{1}\\ \vdots\\ v^{\prime}_{\frac{n-1}{2}}\\ u^{\prime}_{\frac{n-1}{2}}\end{array}\right]=\frac{\sqrt{2}}{\sqrt{n}}\left[ \begin{array}{cccc}\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}&\cdots&\frac{1}{ \sqrt{2}}\\ 1&\cos(2\pi\frac{1}{n})&\cdots&\cos(2\pi\frac{n-1}{n})\\ 0&\sin(2\pi\frac{1}{n})&\cdots&\sin(2\pi\frac{n-1}{n})\\ \vdots&\vdots&\cdots&\vdots\\ 1&\cos(2\pi\frac{n-1}{2}\frac{1}{n})&\cdots&\cos(2\pi\frac{n-1}{2}\frac{n-1}{n })\\ 0&\sin(2\pi\frac{n-1}{2}\frac{1}{n})&\cdots&\sin(2\pi\frac{n-1}{2}\frac{n-1}{n })\end{array}\right]\,.\] (C.61)

Thus, with \(m=\frac{n-1}{2}\),

\[Q\Gamma_{s}Q^{\prime}=\mathrm{diag}(\lambda_{0},\lambda_{1},\lambda_{1}, \lambda_{2},\lambda_{2},\ldots,\lambda_{m},\lambda_{m})\]

where \(\lambda_{j}=\sum_{|h|\leq m}c(h)\,\cos(2\pi hj/n)\) for \(j=0,1,\ldots,m\).

**Theorem C.9**: _Let \(\Gamma_{n}\) be the covariance matrix of \(n\) (odd) realizations from a stationary process \(\{x_{t}\}\) with spectral density \(f_{x}(\omega)\). Let \(Q\) be as defined in (C.61) and let \(D_{n}=\mathrm{diag}\{d_{0},d_{1},\ldots,d_{n-1}\}\) be the diagonal matrix with entries \(d_{0}=f_{x}(0)=\sum_{-\infty}^{\infty}\gamma(h)\) and_

\[d_{2j-1}=d_{2j}=f_{x}(\omega_{j})=\sum_{-\infty}^{\infty}\gamma(h)\mathrm{e}^{ -2\pi ihj/n}\,,\]

_for \(j=1,\ldots,\frac{n-1}{2}\) and \(\omega_{j}=j/n\). Then_

\[Q\Gamma_{n}Q-D_{n}\to 0\quad\mbox{uniformly as $n\to\infty$}\,.\]

_Proof:_ Although \(\Gamma_{n}\) is symmetric, it is not circulant (or the proof would be done). Let \(\Gamma_{n,s}\) be the symmetric circulant matrix with elements \(c(h)=\gamma(h)\), and latent roots, \(\lambda_{j}=\sum_{|h|\leq\frac{n-1}{2}}\gamma(h)\mathrm{e}^{-2\pi ihj/n}\). Note that

\[|\lambda_{j}-f_{x}(\omega_{j})|\leq\sum_{|h|>\frac{n-1}{2}}|\gamma(h)|\to 0\]

as \(n\to\infty\). Hence, we must show that \(Q\Gamma_{n,s}Q^{\prime}-Q\Gamma_{n}Q^{\prime}\to 0\) as \(n\to\infty\).

The \(ij\)th element of the difference of the two matrices is 

[MISSING_PAGE_FAIL:537]

for \(j=0,1,\ldots,n-1\). In this case, the elements of \(Y\) are asymptotically uncorrelated complex random variables, with mean-zero and variance \(f(\omega_{j})\). Also, \(X\) may be recovered as \(X=GY\), so that \(x_{t}=\frac{1}{\sqrt{n}}\sum_{j=0}^{n-1}y_{j}\mathrm{e}^{2\pi itj/n}\).

In this section, we focused on the case where \(n\) is odd. For the \(n\) even case, everything follows through as in the odd case, but with the addition of one more term when \(\frac{n-1}{2}\) becomes \(\frac{n}{2}-1\), and with the addition of one more row in \(Q\) or \(G\), and all in a manner that is so obvious, it would be too simple to be a good homework question.

### Parametric Spectral Estimation

In this section we prove Property 4.7. The basic idea of the result is that a spectral density can be approximated arbitrarily close by the spectrum of an AR(\(p\)) process.

**Proof of Property 4.7.** If \(g(\omega)\equiv 0\), then put \(p=0\) and \(\sigma_{w}=0\). When \(g(\omega)>0\) over some \(\omega\in[-\frac{1}{2},\frac{1}{2}]\), let \(\epsilon>0\) and define

\[d(\omega)=\begin{cases}g^{-1}(\omega)&\text{if }g(\omega)>\epsilon/2\,,\\ 2/\epsilon&\text{if }g(\omega)\leq\epsilon/2\,,\end{cases}\]

so that \(d^{-1}(\omega)=\max\{g(\omega),\,\epsilon/2\}\). Define \(G=\max_{\omega}\{g(\omega)\}\) and let \(0<\delta<\epsilon[G(2G+\epsilon]^{-1}\). Define the sum

\[S_{n}[d(\omega)]=\sum_{|j|\leq n}\langle d,\mathrm{e}_{j}\rangle\mathrm{e}_{j} (\omega)\]

where \(\mathrm{e}_{j}(\omega)=\mathrm{e}^{2\pi ij\omega}\) and \(\langle d,\mathrm{e}_{j}\rangle=\int_{-\frac{1}{2}}^{\frac{1}{2}}d(\omega) \mathrm{e}^{-2\pi ij\omega}d\omega\). Now define the Cesaro sum

\[C_{m}(\omega)=\frac{1}{m}\sum_{n=0}^{m-1}S_{n}[d(\omega)]\,,\]

which is a cumulative average of \(S_{n}[\cdot]\). In this case, \(C_{m}(\omega)=\sum_{|j|\leq m}c_{j}\mathrm{e}^{-2\pi ij\omega}\) where \(c_{j}=(1-\frac{|j|}{m})\langle d,\mathrm{e}_{j}\rangle\). The Cesaro sum converges uniformly on \([-\frac{1}{2},\frac{1}{2}]\) for \(d\in L^{2}\), consequently there is a finite \(p\) such that

\[\Big{|}\sum_{|j|\leq p}c_{j}\mathrm{e}^{-2\pi ij\omega}-d(\omega)\Big{|}<\delta \quad\text{for all }\ \omega\in[-\tfrac{1}{2},\tfrac{1}{2}]\,.\]

Note that \(C_{p}(\omega)\) is a spectral density. In fact, it is the spectral density of an MA(\(p\)) process with \(\gamma(h)=c_{h}\) for \(|h|\leq p\) and \(\gamma(h)=0\) for \(|h|>p\); it is easy to check that \(\gamma(h)\) defined this way is non-negative definite. Hence, the is an invertible MA(\(p\)) process, say

\[y_{t}=u_{t}+\alpha_{1}u_{t-1}+\cdots+\alpha_{p}u_{t-p}\]where \(u_{t}\sim wn(0,\sigma_{u}^{2})\) and \(\alpha(z)\) has roots outside the unit circle. Thus,

\[C_{p}(\omega)=\sum_{|j|\leq p}c_{j}\mathrm{e}^{-2\pi ij\omega}=\sigma_{u}^{2}| \alpha(\mathrm{e}^{-2\pi i\omega})|^{2}\,,\]

and

\[\left|\sigma_{u}^{2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{2}-d(\omega)\right|< \delta<\epsilon[G(2G+\epsilon)]^{-1}\stackrel{{\mathrm{def}}}{{=}} \epsilon^{*}\,.\]

Now define \(f_{x}(\omega)=\left[\sigma_{u}^{2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{2} \right]^{-1}\). We will show that \(|f_{x}(\omega)-g(\omega)|<\epsilon\), in which case the result follows with \(\alpha_{1},\ldots,\alpha_{p}\) being the required AR(\(p\)) coefficients, and \(\sigma_{w}^{2}=\sigma_{u}^{-2}\) being the noise variance. Consider that

\[|f_{x}(\omega)-g(\omega)|\leq|f_{x}(\omega)-d^{-1}(\omega)|+|d^{-1}(\omega)-g (\omega)|<|f_{x}(\omega)-d^{-1}(\omega)|+\epsilon/2\,.\]

Also,

\[|f_{x}(\omega)-d^{-1}(\omega)| =\left|\sigma_{w}^{2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{-2}-d^ {-1}(\omega)\right|\] \[=\left|\sigma_{w}^{-2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{2}-d( \omega)\right|\cdot\left[\sigma_{w}^{2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{-2 }d^{-1}(\omega)\right]\] \[<\delta\sigma_{w}^{2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{-2}G\,.\]

But \(\epsilon^{*}-d(\omega)<\sigma_{w}^{-2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{2} <\epsilon^{*}+d(\omega)\), so that

\[\sigma_{w}^{2}|\alpha(\mathrm{e}^{-2\pi i\omega})|^{-2}<\frac{1}{\epsilon^{*} -d(\omega)}<\frac{1}{\epsilon^{*}-G^{-1}}=\frac{1}{\epsilon[G(2G+\epsilon)]^{ -1}-G^{-1}}=G+\epsilon/2\,.\]

We now have that

\[|f_{x}(\omega)-d^{-1}(\omega)|<\epsilon[G(2G+\epsilon)]^{-1}\cdot G+\epsilon/2 \cdot G=\epsilon/2\,.\]

Finally,

\[|f_{x}(\omega)-g(\omega)|<\epsilon/2+\epsilon/2=\epsilon\,,\]

as was to be shown. 

It should be obvious from the proof of the result, that the property holds if AR(\(p\)) is replaced by MA(\(q\)) or even ARMA(\(p,q\)). As a practical point, it is easier to fit autoregressions of successively increasing order to data, and this is why the property is stated for an AR, even though the MA case is easier to establish.

## Appendix R Supplement

### First Things First

If you do not already have R, point your browser to the Comprehensive R Archive Network (CRAN), [http://cran.r-project.org/](http://cran.r-project.org/) and download and install it. The installation includes help files and some user manuals. You can find helpful tutorials by following CRAN's link to _Contributed Documentation_. If you are a novice, then RStudio ([https://www.rstudio.com/](https://www.rstudio.com/)) will make using R easier.

### astsa

There is an R package for the text called astsa (_Applied Statistical Time Series Analysis_), which was the name of the software distributed with the first and second editions of Shumway and Stoffer (2000), and the original version, Shumway [183]. The package can be obtained from CRAN and its mirrors in the usual way. To download and install astsa, start R and type install.packages("astsa")

You will be asked to choose the closest CRAN mirror to you. As with all packages, you have to load astsa before you use it by issuing the command library(astsa)

All the data are loaded when the package is loaded. If you create a.First function as follows,

.First <- fumctionO[library(astsa)]

and save the workspace when you quit, astsa will be loaded at every start until you change.First.

R is not consistent with help files across different operating systems. The best help system is the html help, which can be started issuing the command help.start()and then following the _Packages_ link to astsa. A useful command to see all the data files available to you, including those loaded with astsa, is data()

### Getting Started

The convention throughout the text is that R code is in blue, output is purple and comments are _# green_. Get comfortable, then start her up and try some simple tasks.

2+2 _# addition_

[1] 5

5*5 + 2 _# multiplication and addition_

[1] 27

5/5 - 3 _# division and subtraction_

[1] -2

log(exp(pi)) _#_ log, exponential, pi_

[1] 3.141593

sin(pi/2) _#_ sinusoids_

[1] 1

exp(1)*(-2) _#_ power

[1] 0.1353353

sqrt(8) _#_ square root

[1] 2.828427

1:5 _#_ sequences

[1] 1 2 3 4 5

seq(1, 10, by=2) _#_ sequences

[1] 1 3 5 7 9

rep(2, 3) _#_ repeat 2 three times

[1] 2 2 2

Next, we'll use _assignment_ to make some _objects_:

x <- 1 + 2 _#_ put 1 + 2 in object x

x = 1 + 2 _#_ same as above with fewer keystrokes

1 + 2 -> x _#_ same

x _#_ view object x

[1] 3

(y = 9 * 3) _#_ put 9 times 3 in y and view the result

[1] 27

(z = rnorm(5)) _#_ put 5 standard normals into z and print z

[1] 0.96607946 1.98135811 -0.06064527 0.31028473 0.02046853

In general, <- and = are not the same; <- can be used anywhere, whereas the use of = is restricted. But when they are the same, we prefer to code using the least number of keystrokes.

It is worth pointing out R's _recycling rule_ for doing arithmetic. In the code below, c() [concatenation] is used to create a vector. Note the use of the semicolon for multiple commands on one line.

x = c(1, 2, 3, 4); y = 2*x; z = c(10, 20); w = c(8, 3, 2)

x * y # 1*2, 2*4, 3*6, 4*8

[1] 2 8 18 32

x + z # 1+10, 2+20, 3+10, 4+20

[1] 11 22 13 24

x + w _#_ what happened here?[1] 9 5 5 12  Warning message:  In y + w : longer object length is not a multiple of  shorter object length To work your objects, use the following commands:

 ls()  # list all objects  "dummy" "mydata" "x" "y" "z"  ls(pattern = "my") # list every object that contains "my"  "dummy" "mydata"  rm(dummy) # remove object "dummy"  rm(list=ls()) # remove almost everything (use with caution)  help.start() # html help and documentation  data() # list of available data sets  help(exp) # specific help (?exp is the same)  getd() # get working directory  setwd() # change working directory  q() # end the session (keep reading) When you quit, R will prompt you to save an image of your current workspace. Answering _yes_ will save the work you have done so far, and load it when you next start R. We have never regretted selecting _yes_, but we have regretted answering _no_.

To create your own data set inside R, you can make a data vector as follows:

mydata = c(1,2,3,2,1) Now you have an object called mydata that contains five elements. R calls these objects _vectors_ even though they have no dimensions (no rows, no columns); they do have order and length:

 mydata # display the data  [1] 1 2 3 2 1  mydata[3] # the third element  [1] 3  mydata[3:5] # elements three through five  [1] 3 2 1  mydata[-(1:2)] # everything except the first two elements  [1] 3 2 1  length(mydata) # number of elements  [1] 5  dim(mydata) # no dimensions  NULL mydata = as.matrix(mydata) # make it a matrix  dim(mydata) # now it has dimensions  [1] 5 1 If you have an external data set, you can use scan or read.table (or some variant) to input the data. For example, suppose you have an asci (text) data file called dummy.txt in your working directory, and the file looks like this:

 1 2 3 2 1  9 0 2 1 0  (dummy = scan("dummy.txt") ) # scan and view it  Read 10 items  [1] 1 2 3 2 1 9 0 2 1 0  (dummy = read.table("dummy.txt") ) # read and view it  V1 V2 V3 V4 V51 2 3 2 1  9 0 2 1 0 There is a difference between scan and read.table. The former produced a data vector of 10 items while the latter produced a _data frame_ with names V1 to V5 and two observations per variate. In this case, if you want to list (or use) the second variate, V2, you would use

dummy$V2  [1] 2 0 and so on. You might want to look at the help files?scan and?read.table now. Data frames (?data.frame) are "used as the fundamental data structure by most of R's modeling software." Notice that R gave the columns of dummy generic names, V1,..., V5. You can provide your own names and then use the names to access the data without the use of $ as above.

colnames(dummy) = c("Dog", "Cat", "Rat", "Pig", "Man") attach(dummy) Cat  [1] 2 0 Rat*(Pig - Man) # animal arithmetic  [1] 3 2 head(dummy) # view the first few lines of a data file detach(dummy) # clean up (if desired) R is case sensitive, thus cat and Cat are different. Also, cat is a reserved name (?cat) in R, so using "cat" instead of "Cat" may cause problems later. You may also include a _header_ in the data file to avoid colnames(). For example, if you have a _comma separated values_ file dummy.csv that looks like this,

Dog,Cat,Rat,Pig,Man  1,2,3,2,1  9,0,2,1,0 then use the following command to read the data.

(dummy = read.csv("dummy.csv"))  Dog Cat Rat Pig Man  1 1 2 3 2 1  2 9 0 2 1 0 The default for.csv files is header=TRUE; type?read.table for further information on similar types of files.

Some commands that are used frequently to manipulate data are c() for _concatenation_, cbind() for _column binding_, and rbind() for _row binding_.

x = 1:3; y = 4:6 (u = c(x, y)) # an R vector  [1] 1 2 3 4 5 6 (u1 = cbind(x, y)) # a 3 by 2 matrix   x y  [1,] 1 4  [2,] 2 5  [3,] 3 6 (u2 = rbind(x,y)) # a 2 by 3 matrix  [,1] [,2] [,3] x 1 2 3  y 4 5 6For example, u1[,2] is the second column of the matrix u1, whereas u2[1,] is the first row of u2.

Summary statistics are fairly easy to obtain. We will simulate 25 normals with \(\mu=10\) and \(\sigma=4\) and then perform some basic analyses. The first line of the code is set.seed, which fixes the seed for the generation of pseudorandom numbers. Using the same seed yields the same results; to expect anything else would be insanity.

set.seed(90210) # so you can reproduce these results x = rnorm(25, 10, 4) # generate the data c( mean(x), median(x), var(x), sd(x) ) # guess [1] 9.473883 9.448511 13.926701 3.731850 c( min(x), max(x) ) # smallest and largest values [1] 2.678173 17.326089 which.max(x) # index of the max (x[25] in this case) [1] 25 summary(x) # a five number summary with six numbers Min. 1st Qu. Median Mean 3rd Qu. Max. 2.678 7.824 9.449 9.474 11.180 17.330 boxplot(x); hist(x); stem(x) # visual summaries (not shown)

It can't hurt to learn a little about programming in R because you will see some of it along the way. Consider a simple program that we will call crazy to produce a graph of a sequence of sample means of increasing sample sizes from a Cauchy distribution with location parameter zero.

1 crazy <- function(num) { x <- c()  for(n in 1:num) { x[n] <- mean(rcauchy(n)) } plot(x, type="1", xlab="sample size", ylab="sample mean") }

The first line creates the function crazy and gives it one argument, num, that is the sample size that will end the sequence. Line 2 makes an empty vector, x, that will be used to store the sample means. Line 3 generates n random Cauchy variates [rcauchy(n)], finds the mean of those values, and puts the result into x[n], the \(n\)-th value of x. The process is repeated in a "do loop" num times so that x[1] is the sample mean from a sample of size one, x[2] is the sample mean from a sample of size two, and so on, until finally, x[num] is the sample mean from a sample of size num. After the do loop is complete, the fourth line generates a graphic (see Fig. R.1). The fifth line closes the function. To use crazy ending with sample of size of 200, type crazy(200) and you will get a graphic that looks like Fig. R.1.

Figure R.1: Crazy example

Finally, a word of caution: TRUE and FALSE are reserved words, whereas T and F are initially set to these. Get in the habit of using the words rather than the letters T or F because you may get into trouble if you do something like F = qf(p=.01, df1=3, df2=9) so that F is no longer FALSE, but a quantile of the specified \(F\)-distribution.

### Time Series Primer

In this section, we give a brief introduction on using R for time series. _We assume that_astsa _has been loaded._ To create a time series object, use the command ts. Related commands are as.ts to coerce an object to a time series and is.ts to test whether an object is a time series. First, make a small data set:

(mydata = c(1,2,3,2,1) ) # make it and view it [1] 1 2 3 2 1 Now make it a time series:

(mydata = as.ts(mydata) )  Time Series:  Start = 1  End = 5  Frequency = 1  [1] 1 2 3 2 1 Make it an annual time series that starts in 1950:

(mydata = ts(mydata, start=1950) )  Time Series:  Start = 1950  End = 1954  Frequency = 1  [1] 1 2 3 2 1 Now make it a quarterly time series that starts in 1950-III:

(mydata = ts(mydata, start=c(1950,3), frequency=4) )  Qtr1 Qtr2 Qtr3 Qtr4  1950 1 2  1951 3 2 1  time(mydata) # view the sampled times  Qtr1 Qtr2 Qtr3 Qtr4  1950 1950.50 1950.75  1951 1951.00 1951.25 1951.50 To use part of a time series object, use window(): (x = window(mydata, start=c(1951,1), end=c(1951,3) ))  Qtr1 Qtr2 Qtr3  1951 3 2 1 Next, we'll look at lagging and differencing. First make a simple series, \(x_{t}\): x = ts(1:5) Now, column bind (cbind) lagged values of \(x_{t}\) and you will notice that lag(x) is _forward_ lag, whereas lag(x, -1) is _backward_ lag.

cbind(x, lag(x), lag(x,-1))  x lag(x) lag(x, -1)  # NA 1 NA  1 1 2 NA  2 2 3 1  3 3 4 2 <- in this row, for example, x is 3,  4 4 5 3 lag(x) is ahead at 4, and  5 5 NA 4 lag(x,-1) is behind at 2  6 NA NA 5 Compare cbind and ts.intersect: ts.intersect(x, lag(x,1), lag(x,-1))  Time Series: Start = 2 End = 4 Frequency = 1  x lag(x, 1) lag(x, -1)  2 2 3 1  3 3 4 2  4 4 5 3 To difference a series, \(\nabla x_{t}=x_{t}-x_{t-1}\), use diff(x) but note that diff(x, 2) is _not_ second order differencing, it is \(x_{t}-x_{t-2}\). For second order differencing, that is, \(\nabla^{2}x_{t}\), do one of these: diff(diff(x)) diff(x, diff=2) # same thing and so on for higher order differencing.

We will also make use of regression via lmO. First, suppose we want to fit a simple linear regression, \(y=\alpha+\beta x+\epsilon\). In R, the formula is written as y-x: set.seed(1999) x = rnorm(10) y = x + rnorm(10) summary(fit <- lm(y-x) )  Coefficients:  Estimate Std. Error t value Pr(>|t|)  (Intercept) 0.2576 0.1892 1.362 0.2104  x 0.4577 0.2016 2.270 0.0529  --  Residual standard error: 0.58 on 8 degrees of freedom  Multiple R-squared: 0.3918, Adjusted R-squared: 0.3157  F-statistic: 5.153 on 1 and 8 DF, p-value: 0.05289 plot(x, y) # draw a scatterplot of the data (not shown)  abline(fit) # add the fitted line to the plot (not shown) All sorts of information can be extracted from the lm object, which we called fit. For example,

resid(fit) # will display the residuals (not shown) fitted(fit) # will display the fitted values (not shown) lm(y - 0 + x) # will exclude the intercept (not shown) You have to be careful if you use lm() for lagged values of a time series. If you use lmO, then what you have to do is align the series using ts.intersect. Please read the warning _Using time series_ in the lm() help file [help(lm)]. Here is an example regressing astsa data, weekly cardiovascular mortality (cmort) on particulate pollution (part) at the present value and lagged four weeks (part4). First, we create ded, which consists of the intersection of the three series:

```
ded=ts.intersect(cmort,part,part4=lag(part,-4))
```

Now the series are all aligned and the regression will work.

```
summary(fit<-lm(cmort-part+part4,data=ded,na.action=NULL)) Coefficients: EstimateStd.ErrortvaluePr(>|t|) (Intercept)69.010201.3749850.190<2e-16 part0.15140.028985.2252.56e-07 part40.262970.028999.071<2e-16 --- Residualstandarderror:8.323on501degreesoffreedom MultipleR-squared:0.3091,AdjustedR-squared:0.3063 F-statistic:112.1on2and501DF,p-value:<2.2e-16
```

There was no need to rename lag(part,-4) to part4, it's just an example of what you can do.

An alternative to the above is the package dynlm, which has to be installed. After the package is installed, the previous example may be run as follows:

```
library(dynlm)#loadthepackage fit=dynlm(cmort-part+L(part,4))#nonewdatafileneeded summary(fit)
```

The output is identical to the lm output. To fit another model, for example, add the temperature series tempr, the advantage of dynlm is that a new data file does not have to be created. We could just run

```
summary(dynlm(cmort-tempr+part+L(part,4)))
```

In Problem 2.1, you are asked to fit a regression model

\[x_{t}=\beta t+\alpha_{1}Q_{1}(t)+\alpha_{2}Q_{2}(t)+\alpha_{3}Q_{3}(t)+\alpha_ {4}Q_{4}(t)+w_{t}\]

where \(x_{t}\) is logged Johnson & Johnson quarterly earnings (\(n=84\)), and \(Q_{i}(t)\) is the indicator of quarter \(i=1,2,3,4\). The indicators can be made using factor.

```
trend=time(jj)-1970#helpsto'center'time Q=factor(cycle(jj))#make(Q)uarterfactors reg=lm(log(jj)-0+trend+Q,na.action=NULL)#nointercept model.matrix(reg)#viewthemodeldesignmatrix trendQ1Q2Q3Q4
1-10.001000
2-9.7501000
3-9.500010
4-9.2500001
5.-....... summary(reg)#viewtheresults(notshown)
```

The workhorse for ARIMA simulations is arima.sim. Here are some examples; no output is shown here so you're on your own.

``` x=arima.sim(list(order=c(1,0,0),ar=.9),n=100)+50#AR(1)w/mean50 x=arima.sim(list(order=c(2,0,0),ar=c(1,-.9)),n=100)#AR(2) x=arima.sim(list(order=c(1,1,1),ar=.9,ma=-.5),n=200)#ARIMA(1,1,1)An easy way to fit ARIMA models is to use sarima from astsa. The script is used in Chap. 3 and is introduced in Sect. 3.7.

#### Graphics

We introduced some graphics without saying much about it. Many people use the graphics package ggplot2, but for quick and easy graphing of time series, the R base graphics does fine and is what we discuss here. As seen in Chap. 1, a time series may be plotted in a few lines, such as

plot(speech) in Example 1.3, or the multifigure plot

plot.ts(cbind(soi, rec) ) which we made little fancier in Example 1.5:

par(mfrow = c(2,1)) plot(soi, ylab='', xlab='', main='Southern Oscillation Index') plot(rec, ylab='', xlab='', main='Recruitment')

But, if you compare the results of the above to what is displayed in the text, there is a slight difference because we improved the aesthetics by adding a grid and cutting down on the margins. This is how we actually produced Fig. 1.3:

```
1dev.new(width=7,height=4)#defaultis7x7inches
2par(mar=c(3,3,1,1), mgp=c(1.6,.6,0))#changethemargins(?par)
3plot(speech,type='n')
4grid(lty=1,col=gray(.9));lines(speech)
```

In line 1, the dimensions are in inches. Line 2 adjusts the margins; see help(par) for a complete list of settings. In line 3, the type='n' means to set up the graph, but don't actually plot anything yet. Line 4 adds a grid and then plots the lines. The reason for using type='n' is to avoid having the grid lines on top of the data plot. You can print the graphic directly to a pdf, for example, by replacing line 1 with something like

pdf(file="speech.pdf", width=7,height=4) but you have to turn the device off to complete the file save:

dev.off()

Here is the code we used to plot two series individually in Fig. 1.5:

```
dev.new(width=7,height=6) par(mfrow = c(2,1),mar=c(2,2,1,0)+.5,mgp=c(1.6,.6,0)) plot(soi, ylab='',xlab='',main='SouthernOscillationIndex',type='n') grid(lty=1,col=gray(.9));lines(soi) plot(rec,ylab='',main='Recruitment',type='n') grid(lty=1,col=gray(.9));lines(rec)
```

For plotting many time series, plot.ts and ts.plot are available. If the series are all on the same scale, it might be useful to do the following:

```
ts.plot(comort,tempr,part,col=1:3) legend('topright',legend=c('M','T','P'),lty=1,col=1:3)
```

This produces a plot of all three series on the same axes with different colors, and then adds a legend. We are not restricted to using basic colors; an internet search on 'R colors' is helpful. The following code gives separate plots of each different series (with a limit of 10):plot.ts(cbind(cmort, tempr, part) ) plot.ts(eqexp) # you will get a warning plot.ts(eqexp[,9:16], main='Explosions') # but this works

Finally, we mention that size matters when plotting time series. Figure R.2 shows the sunspot numbers discussed in Problem 4.9 plotted with varying dimension size as follows.

layout(matrix(c(1:2, 1:2), ncol=2), height=c(.2,.8)) par(mar=c(.2,3.5,0.5), oma=c(3.5,0.5,0), mgp=c(2,.6,0), tcl=-.3, las=1) plot(sunspotz, type='n', xart='no', ylab='')  grid(lty=1, col=gray(.9))  lines(sunspotz) plot(sunspotz, type='n', ylab='')  grid(lty=1, col=gray(.9))  lines(sunspotz)  title(xlab="Time", outer=TRUE, cex.lab=1.2)  mtext(side=2, "Sunspot Numbers", line=2, las=0, adj=.75)

Figure R.2: The sunspot numbers plotted in different-sized boxes, demonstrating that the dimensions of the graphic matters when displaying time series data

The result is shown in Fig. 24. The top plot is wide and narrow, revealing the fact that the series rises quickly \(\uparrow\) and falls slowly \(\searrow\). The bottom plot, which is more square, obscures this fact. You will notice that in the main part of the text, we never plotted a series in a square box. The ideal shape for plotting time series, in most instances, is when the time axis is much wider than the value axis.

## References

* [1] Akaike H (1969) Fitting autoregressive models for prediction. Ann Inst Stat Math 21:243-247
* [2] Akaike H (1973) Information theory and an extension of the maximum likelihood principal. In: Petrov BN, Csake F (eds) 2nd Int Symp Inform Theory. Akademia Kiado, Budapest, pp 267-281
* [3] Akaike H (1974) A new look at statistical model identification. IEEE Trans Automat Contr AC-19:716-723
* [4] Alagon J (1989) Spectral discrimination for two groups of time series. J Time Ser Anal 10:203-214
* [5] Anderson BDO, Moore JB (1979) Optimal filtering. Prentice-Hall, Englewood Cliffs
* [6] Anderson TW (1978) Estimation for autoregressive moving average models in the time and frequency domain. Ann Stat 5:842-865
* [7] Anderson TW (1984) An introduction to multivariate statistical analysis, 2nd edn. Wiley, New York
* [8] Ansley CF, Newbold P (1980) Finite sample properties of estimators for autoregressive moving average processes. J Econ 13:159-183
* [9] Ansley CF, Kohn R (1982) A geometrical derivation of the fixed interval smoothing algorithm. Biometrika 69:486-487
* [10] Antognini JF, Buonocore MH, Disbrow EA, Carstens E (1997) Isoflurane anesthesia blunts cerebral responses to noxious and innocuous stimuli: a fMRI study. Life Sci 61:PL349-PL354
* [11] Bandettini A, Jesmanowicz A, Wong EC, Hyde JS (1993) Processing strategies for time-course data sets in functional MRI of the human brain. Magnetic Res Med 30:161-173
* [12] Bar-Shalom Y (1978) Tracking methods in a multi-target environment. IEEE Trans Automat Contr AC-23:618-626
* [13] Bar-Shalom Y, Tse E (1975) Tracking in a cluttered environment with probabilistic data association. Automatica 11:4451-4460* [14] Baum LE, Petrie T, Soules G, Weiss N (1970) A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. Ann Math Stat 41:164-171
* [15] Bazza M, Shumway RH, Nielsen DR (1988) Two-dimensional spectral analysis of soil surface temperatures. Hilgardia 56:1-28
* [16] Bedrick EJ, Tsai C-L (1994) Model selection for multivariate regression in small samples. Biometrics 50:226-231
* [17] Benjamini Y, Hochberg Y (1995) Controlling the false discovery rate: a practical and powerful approach to multiple testing. J Roy Stat Soc Ser B 289-300
* [18] Beran J (1994) Statistics for long memory processes. Chapman and Hall, New York
* [19] Berk KN (1974) Consistent autoregressive spectral estimates. Ann Stat 2:489-502
* [20] Bhat RR (1985) Modern probability theory, 2nd edn. Wiley, New York
* [21] Bhattacharya A (1943) On a measure of divergence between two statistical populations. Bull Calcutta Math Soc 35:99-109
* [22] Billingsley P (1999) Convergence of probability measures, 2nd edn. Wiley, New York
* [23] Blackman RB, Tukey JW (1959) The measurement of power spectra from the point of view of communications engineering. Dover, New York
* [24] Blight BJN (1974) Recursive solutions for the estimation of a stochastic parameter. J Am Stat Assoc 69:477-481
* [25] Bloomfield P (2000) Fourier analysis of time series: an introduction, 2nd edn. Wiley, New York
* [26] Bloomfield P, Davis JM (1994) Orthogonal rotation of complex principal components. Int J Climatol 14:759-775
* [27] Bogart BP, Healy MJR, Tukey JW (1962) The quefrency analysis of time series for echoes: cepstrum, pseudo-autocovariance, cross-cepstrum and saphe cracking. In: Proc. of the symposium on time series analysis, pp 209-243, Brown University, Providence, USA
* [28] Bollerslev T (1986) Generalized autoregressive conditional heteroscedasticity. J Econ 31:307-327
* [29] Box GEP, Pierce DA (1970) Distributions of residual autocorrelations in autoregressive integrated moving average models. J Am Stat Assoc 72:397-402
* [30] Box GEP, Jenkins GM (1970) Time series analysis, forecasting, and control. Holden-Day, Oakland
* [31] Box GEP, Jenkins GM, Reinsel GC (1994) Time series analysis, forecasting, and control, 3rd edn. Prentice Hall, Englewood Cliffs
* [32] Brillinger DR (1973) The analysis of time series collected in an experimental design. In: Krishnaiah PR (ed) Multivariate analysis-III, pp 241-256. Academic Press, New York
* [33] Brillinger DR (1975) Time series: data analysis and theory. Holt, Rinehart & Winston Inc., New York* [34] Brillinger DR (1980) Analysis of variance and problems under time series models. In: Krishnaiah PR, Brillinger DR (eds) Handbook of statistics, Vol I, pp 237-278. North Holland, Amsterdam
* [35] Brillinger DR (1981, 2001) Time series: data analysis and theory, 2nd edn. Holden-Day, San Francisco. Republished in 2001 by the Society for Industrial and Applied Mathematics, Philadelphia
* [36] Brockwell PJ, Davis RA (1991) Time series: theory and methods, 2nd edn. Springer, New York
* [37] Cappe O, Moulines E, Ryden T (2009) Inference in hidden Markov models. Springer, New York
* [38] Caines PE (1988) Linear stochastic systems. Wiley, New York
* [39] Carter CK, Kohn R (1994) On Gibbs sampling for state space models. Biometrika 81:541-553
* [40] Chan NH (2002) Time series: applications to finance. Wiley, New York
* [41] Chernoff H (1952) A measure of asymptotic efficiency for tests of a hypothesis based on the sum of the observations. Ann Math Stat 25:573-578
* [42] Cleveland WS (1979) Robust locally weighted regression and smoothing scatterplots. J Am Stat Assoc 74:829-836
* [43] Cochrane D, Orcutt GH (1949) Applications of least squares regression to relationships containing autocorrelated errors. J Am Stat Assoc 44:32-61
* [44] Cooley JW, Tukey JW (1965) An algorithm for the machine computation of complex Fourier series. Math Comput 19:297-301
* [45] Cressie NAC (1993) Statistics for spatial data. Wiley, New York
* [46] Dahlhaus R (1989) Efficient parameter estimation for self-similar processes. Ann Stat 17:1749-1766
* [47] Dargahi-Noubary GR, Laycock PJ (1981) Spectral ratio discriminants and information theory. J Time Ser Anal 16:201-219
* [48] Danielson J (1994) Stochastic volatility in asset prices: Estimation with simulated maximum likelihood. J Econometrics 61:375-400
* [49] Davies N, Triggs CM, Newbold P (1977) Significance levels of the Box-Pierce portmanteau statistic in finite samples. Biometrika 64:517-522
* [50] Dent W, Min A-S (1978) A Monte Carlo study of autoregressive-integrated-moving average processes. J Econ 7:23-55
* [51] Dempster AP, Laird NM, Rubin DB (1977) Maximum likelihood from incomplete data via the EM algorithm. J R Stat Soc B 39:1-38
* [52] Ding Z, Granger CWJ, Engle RF (1993) A long memory property of stock market returns and a new model. J Empirical Finance 1:83-106
* [53] Douc R, Moulines E, Stoffer DS (2014) Nonlinear time series: theory, methods, and applications with R examples. CRC Press, Boca Raton
* [54] Durbin J (1960) Estimation of parameters in time series regression models. J R Stat Soc B 22:139-153
* [55] Durbin J, Koopman SJ (2001) Time series analysis by state space methods. Oxford University Press, Oxford
* [56] Efron B, Tibshirani R (1994) An introduction to the bootstrap. Chapman and Hall, New York* [57] Engle RF (1982) Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. Econometrica 50:987-1007
* [58] Engle RF, Nelson D, Bollerslev T (1994) ARCH models. In: Engle R, McFadden D (eds) Handbook of econometrics, Vol IV, pp 2959-3038. North Holland, Amsterdam
* [59] Evans GBA, Savin NE (1981) The calculation of the limiting distribution of the least squares estimator of the parameter in a random walk model. Ann Stat 1114-1118. [http://projecteuclid.org/euclid.aos/1176345591](http://projecteuclid.org/euclid.aos/1176345591)
* [60] Eubank RL (1999) Nonparametric regression and spline smoothing, vol 157. Chapman & Hall, New York
* [61] Fan J, Kreutzberger E (1998) Automatic local smoothing for spectral density estimation. Scand J Stat 25:359-369
* [62] Fox R, Taqqu MS (1986) Large sample properties of parameter estimates for strongly dependent stationary Gaussian time series. Ann Stat 14:517-532
* [63] Friedman JH, Stuetzle W (1981) Projection pursuit regression. J Am Stat Assoc 76:817-823
* [64] Fruhwirth-Schnatter S (1994) Data augmentation and dynamic linear models. J Time Ser Anal 15:183-202
* [65] Fuller WA (1976) Introduction to statistical time series. Wiley, New York
* [66] Fuller WA (1996) Introduction to statistical time series, 2nd edn. Wiley, New York
* [67] Gelfand AE, Smith AFM (1990) Sampling-based approaches to calculating marginal densities. J Am Stat Assoc 85:398-409
* [68] Geman S, Geman D (1984) Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans Pattern Anal Machine Intell 6:721-741
* [69] Gerlach R, Carter C, Kohn R (2000) Efficient Bayesian inference for dynamic mixture models. J Am Stat Assoc 95:819-828
* [70] Geweke JF (1977) The dynamic factor analysis of economic time series models. In: Aigner D, Goldberger A (eds) Latent variables in socio-economic models, pp 365-383. North Holland, Amsterdam
* [71] Geweke JF, Singleton KJ (1981) Latent variable models for time series: A frequency domain approach with an application to the Permanent Income Hypothesis. J Econ 17:287-304
* [72] Geweke JF, Porter-Hudak S (1983) The estimation and application of long-memory time series models. J Time Ser Anal 4:221-238
* [73] Giri N (1965) On complex analogues of \(T^{2}\) and \(R^{2}\) tests. Ann Math Stat 36:664-670
* [74] Goldfeld SM, Quandt RE (1973) A Markov model for switching regressions. J Econ 1:3-16
* [75] Goodman NR (1963) Statistical analysis based on a certain multivariate complex Gaussian distribution. Ann Math Stat 34:152-177
* [76] Gordon K, Smith AFM (1988) Modeling and monitoring discontinuous changes in time series. In: Bayesian analysis of time series and dynamic models, pp 359-392* [77] Gordon K, Smith AFM (1990) Modeling and monitoring biomedical time series. J Am Stat Assoc 85:328-337
* [78] Gourieroux C (1997) ARCH models and financial applications. Springer, New York
* [79] Granger CW, Joyeux R (1980) An introduction to long-memory time series models and fractional differencing. J Time Ser Anal 1:15-29
* [80] Grenander U (1951) On empirical spectral analysis of stochastic processes. Arkiv for Mathematik 1:503-531
* [81] Green PJ, Silverman BW (1993) Nonparametric regression and generalized linear models: a roughness penalty approach, vol 58. Chapman & Hall, New York
* [82] Grenander U, Rosenblatt M (1957) Statistical analysis of stationary time series. Wiley, New York
* [83] Grether DM, Nerlove M (1970) Some properties of optimal seasonal adjustment. Econometrica 38:682-703
* [84] Gupta NK, Mehra RK (1974) Computational aspects of maximum likelihood estimation and reduction in sensitivity function calculations. IEEE Trans Automat Contr AC-19:774-783
* [85] Hamilton JD (1989) A new approach to the economic analysis of nonstationary time series and the business cycle. Econometrica 57:357-384
* [86] Hannan EJ (1970) Multiple time series. Wiley, New York
* [87] Hannan EJ, Quinn BG (1979) The determination of the order of an autoregression. J R Stat Soc B 41:190-195
* [88] Hannan EJ, Deistler M (1988) The statistical theory of linear systems. Wiley, New York
* [89] Hansen J, Sato M, Ruedy R, Lo K, Lea DW, Medina-Elizade M (2006) Global temperature change. Proc Natl Acad Sci 103:14288-14293
* [90] Harrison PJ, Stevens CF (1976) Bayesian forecasting (with discussion). J R Stat Soc B 38:205-247
* [91] Harvey AC, Todd PHJ (1983) Forecasting economic time series with structural and Box-Jenkins models: A case study. J Bus Econ Stat 1:299-307
* [92] Harvey AC, Pierse RG (1984) Estimating missing observations in economic time series. J Am Stat Assoc 79:125-131
* [93] Harvey AC (1991) Forecasting, structural time series models and the Kalman filter. Cambridge University Press, Cambridge
* [94] Harvey AC, Ruiz E, Shephard N (1994) Multivariate stochastic volatility models. Rev Econ Stud 61:247-264
* [95] Haslett J, Raftery AE (1989) Space-time modelling with long-memory dependence: Assessing Ireland's wind power resource (C/R: 89V38 p21-50). Appl Stat 38:1-21
* [96] Hastings WK (1970) Monte Carlo sampling methods using Markov chains and their applications. Biometrika 57:97-109
* [97] Hosking JRM (1981) Fractional differencing. Biometrika 68:165-176
* [98] Hurst H (1951) Long term storage capacity of reservoirs. Trans Am Soc Civil Eng 116:778-808* [99] Hurvich CM, Zeger S (1987) Frequency domain bootstrap methods for time series. Tech. Report 87-115, Department of Statistics and Operations Research, Stern School of Business, New York University
* [100] Hurvich CM, Tsai C-L (1989) Regression and time series model selection in small samples. Biometrika 76:297-307
* [101] Hurvich CM, Beltrao KI (1993) Asymptotics for the low-frequency ordinates of the periodogram for a long-memory time series. J Time Ser Anal 14:455-472
* [102] Hurvich CM, Deo RS, Brodsky J (1998) The mean squared error of Geweke and Porter-Hudak's estimator of the memory parameter of a long-memory time series. J Time Ser Anal 19:19-46
* [103] Hurvich CM, Deo RS (1999) Plug-in selection of the number of frequencies in regression estimates of the memory parameter of a long-memory time series. J Time Ser Anal 20:331-341
* [104] Jacquier E, Polson NG, Rossi PE (1994) Bayesian analysis of stochastic volatility models. J Bus. Econ Stat 12:371-417
* [105] Jazwinski AH (1970) Stochastic processes and filtering theory. Academic Press, New York
* [106] Johnson RA, Wichern DW (1992) Applied multivariate statistical analysis, 3rd edn. Prentice-Hall, Englewood Cliffs
* [107] Jones RH (1980) Maximum likelihood fitting of ARMA models to time series with missing observations. Technometrics 22:389-395
* [108] Jones RH (1984) Fitting multivariate models to unequally spaced data. In: Parzen E (ed) Time series analysis of irregularly observed data, pp 158-188. Lecture notes in statistics, 25. Springer, New York
* [109] Journel AG, Huijbregts CH (1978) Mining geostatistics. Academic Press, New York
* [110] Juang BH, Rabiner LR (1985) Mixture autoregressive hidden Markov models for speech signals. IEEE Trans Acoust Speech Signal Process ASSP-33:1404-1413
* [111] Kakizawa Y, Shumway RH, Taniguchi M (1998) Discrimination and clustering for multivariate time series. J Am Stat Assoc 93:328-340
* [112] Kalman RE (1960) A new approach to linear filtering and prediction problems. Trans ASME J Basic Eng 82:35-45
* [113] Kalman RE, Bucy RS (1961) New results in filtering and prediction theory. Trans ASME J Basic Eng 83:95-108
* [114] Kaufman L, Rousseeuw PJ (1990) Finding groups in data: an introduction to cluster analysis. Wiley, New York
* [115] Kay SM (1988) Modern spectral analysis: theory and applications. Prentice-Hall, Englewood Cliffs
* [116] Kazakos D, Papantoni-Kazakos P (1980) Spectral distance measuring between Gaussian processes. IEEE Trans Automat Contr AC-25:950-959
* [117] Khatri CG (1965) Classical statistical analysis based on a certain multivariate complex Gaussian distribution. Ann Math Stat 36:115-119
* [118] Kim S, Shephard N, Chib S (1998) Stochastic volatility: likelihood inference and comparison with ARCH models. Rev Econ Stud 65:361-393* [119] Kitagawa G, Gersch W (1984) A smoothness priors modeling of time series with trend and seasonality. J Am Stat Assoc 79:378-389
* [120] Kolmogorov AN (1941) Interpolation und extrapolation von stationaren zufalligen Folgen. Bull Acad Sci URSS 5:3-14
* [121] Krishnaiah PR, Lee JC, Chang TC (1976) The distribution of likelihood ratio statistics for tests of certain covariance structures of complex multivariate normal populations. Biometrika 63:543-549
* [122] Kullback S, Leibler RA (1951) On information and sufficiency. Ann Math Stat 22:79-86
* [123] Kullback S (1958) Information theory and statistics. Peter Smith, Gloucester
* [124] Lachenbruch PA, Mickey MR (1968) Estimation of error rates in discriminant analysis. Technometrics 10:1-11
* [125] Lam PS (1990) The Hamilton model with a general autoregressive component: Estimation and comparison with other models of economic time series. J Monetary Econ 26:409-432
* [126] Lay T (1997) Research required to support comprehensive nuclear test ban treaty monitoring. National Research Council Report, National Academy Press, 2101 Constitution Ave., Washington, DC 20055
* [127] Levinson N (1947) The Wiener (root mean square) error criterion in filter design and prediction. J Math Phys 25:262-278
* [128] Lindgren G (1978) Markov regime models for mixed distributions and switching regressions. Scand J Stat 5:81-91
* [129] Ljung GM, Box GEP (1978) On a measure of lack of fit in time series models. Biometrika 65:297-303
* [130] Lutkepohl H (1985) Comparison of criteria for estimating the order of a vector autoregressive process. J Time Ser Anal 6:35-52
* [131] Lutkepohl H (1993) Introduction to multiple time series analysis, 2nd edn. Springer, Berlin
* [132] MacQueen JB (1967) Some methods for classification and analysis of multivariate observations. In: Proceedings of 5-th Berkeley symposium on mathematical statistics and probability. University of California Press, Berkeley, 1:281-297
* [133] Mallows CL (1973) Some comments on \(C_{p}\). Technometrics 15:661-675
* [134] McBratney AB, Webster R (1981) Detection of ridge and furrow pattern by spectral analysis of crop yield. Int Stat Rev 49:45-52
* [135] McCulloch RE, Tsay RS (1993) Bayesian inference and prediction for mean and variance shifts in autoregressive time series. J Am Stat Assoc 88:968-978
* [136] McDougall AJ, Stoffer DS, Tyler DE (1997) Optimal transformations and the spectral envelope for real-valued time series. J Stat Plan Infer 57:195-214
* [137] McLeod AI (1978) On the distribution of residual autocorrelations in Box-Jenkins models. J R Stat Soc B 40:296-302
* [138] McQuarrie ADR, Tsai C-L (1998) Regression and time series model selection World Scientific, Singapore
* [139] Meinhold RJ, Singpurwalla ND (1983) Understanding the Kalman filter. Am Stat 37:123-127* [140] Meng XL, Rubin DB (1991) Using EM to obtain asymptotic variance-covariance matrices: The SEM algorithm. J Am Stat Assoc 86:899-909
* [141] Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH, Teller E (1953) Equations of state calculations by fast computing machines. J Chem Phys 21:1087-1091
* [142] Mickens RE (1990) Difference equations: theory and applicatons, 2nd edn. Springer, New York
* [143] Newbold P, Bos T (1985) Stochastic parameter regression models. Sage, Beverly Hills
* [144] Ogawa S, Lee TM, Nayak A, Glynn P (1990) Oxygenation-sensititive contrast in magnetic resonance image of rodent brain at high magnetic fields. Magn Reson Med 14:68-78
* [145] Palma W (2007) Long-memory time series: theory and methods. Wiley, New York
* [146] Palma W, Chan NH (1997) Estimation and forecasting of long-memory time series with missing values. J Forecast 16:395-410
* [147] Paparoditis E, Politis DN (1999) The local bootstrap for periodogram statistics. J Time Ser Anal 20:193-222
* [148] Parzen E (1962) On estimation of a probability density and mode. Ann Math Stat 35:1065-1076
* [149] Parzen E (1983) Autoregressive spectral estimation. In: Brillinger DR, Krishnaiah PR (eds) Time series in the frequency domain, handbook of statistics, Vol 3, pp 211-243. North Holland, Amsterdam
* [150] Pawitan Y, Shumway RH (1989) Spectral estimation and deconvolution for a linear time series model. J Time Ser Anal 10:115-129
* [151] Pena D, Guttman I (1988) A Bayesian approach to robustifying the Kalman filter. In: Spall JC (ed) Bayesian analysis of time series and dynamic linear models, pp 227-254. Marcel Dekker, New York
* [152] Percival DB, Walden AT (1993) _Spectral analysis for physical applications: multitaper and conventional univariate techniques_. Cambridge University Press, Cambridge
* [153] Petris G, Petrone S, Campagnoli P (2009) Dynamic linear models with R. Springer, New York
* [154] Phillips PCB (1987) Time series regression with a unit root. Econometrica 55:227-301
* [155] Pinsker MS (1964) Information and information stability of random variables and processes. Holden Day, San Francisco
* [156] Press WH, Teukolsky SA, Vetterling WT, Flannery BP (1993) Numerical recipes in C: the art of scientific computing, 2nd edn. Cambridge University Press, Cambridge
* [157] Priestley MB, Subba-Rao T, Tong H (1974) Applications of principal components analysis and factor analysis in the identification of multi-variable systems. IEEE Trans Automat Contr AC-19:730-734
* [158] Priestley MB, Subba-Rao T (1975) The estimation of factor scores and Kalman filtering for discrete parameter stationary processes. Int J Contr 21:971-975* [159] Priestley MB (1988) Nonlinear and nonstationary time series analysis. Academic Press, London
* [160] Quandt RE (1972) A new approach to estimating switching regressions. J Am Stat Assoc 67:306-310
* [161] Rabiner LR, Juang BH (1986) An introduction to hidden Markov models. IEEE Acoust. Speech Signal Process ASSP-34:4-16
* [162] Rao MM (1978) Asymptotic distribution of an estimator of the boundary parameter of an unstable process. Ann Stat 185-190
* [163] Reinsel GC (1997) Elements of multivariate time series analysis, 2nd edn. Springer, New York
* [164] Remillard B (2011) Validity of the parametric bootstrap for goodness-of-fit testing in dynamic models. Available at SSRN 1966476
* [165] Renyi A (1961) On measures of entropy and information. In: Proceedings of 4th Berkeley symp. math. stat. and probability, pp 547-561. Univ. of California Press, Berkeley
* [166] Rissanen J (1978) Modeling by shortest data description. Automatica 14:465-471
* [167] Robinson PM (1995) Gaussian semiparametric estimation of long range dependence. Ann Stat 23:1630-1661
* [168] Robinson PM (2003) Time series with long memory. Oxford University Press, Oxford
* [169] Rosenblatt M (1956a) A central limit theorem and a strong mixing condition. Proc Natl Acad Sci 42:43-47
* [170] Rosenblatt M (1956b) Remarks on some nonparametric estimates of a density functions. Ann Math Stat 27:642-669
* [171] Sandmann G, Koopman SJ (1998) Estimation of stochastic volatility models via Monte Carlo maximum likelihood. J Econometrics 87:271-301
* [172] Scheffe H (1959) The analysis of variance. Wiley, New York
* [173] Schuster A (1898) On the investigation of hidden periodicities with application to a supposed 26 day period of meteorological phenomena. Terrestrial magnetism, III, pp 11-41
* [174] Schuster A (1906) On the periodicities of sunspots. Phil Trans R Soc Ser A 206:69-100
* [175] Schwarz F (1978) Estimating the dimension of a model. Ann Stat 6:461-464
* [176] Schweppe FC (1965) Evaluation of likelihood functions for Gaussian signals. IEEE Trans Inform Theory IT-4:294-305
* [177] Shephard N (1996) Statistical aspects of ARCH and stochastic volatility. In: Cox DR, Hinkley DV, Barndorff-Nielson OE (eds) Time series models in econometrics, finance and other fields, pp 1-100. Chapman and Hall, London
* [178] Shumway RH, Dean WC (1968) Best linear unbiased estimation for multivariate stationary processes. Technometrics 10:523-534
* [179] Shumway RH, Unger AN (1974) Linear discriminant functions for stationary time series. J Am Stat Assoc 69:948-956* [180] Shumway RH (1982) Discriminant analysis for time series. In: Krishnaiah PR, Kanal LN (eds) Classification, pattern recognition and reduction of dimensionality, handbook of statistics, vol 2, pp 1-46. North Holland, Amsterdam
* [181] Shumway RH, Stoffer DS (1982) An approach to time series smoothing and forecasting using the EM algorithm. J Time Ser Anal 3:253-264
* [182] Shumway RH (1983) Replicated time series regression: An approach to signal estimation and detection. In: Brillinger DR, Krishnaiah PR (eds) time series in the frequency domain, handbook of statistics, vol 3, pp 383-408. North Holland, Amsterdam
* [183] Shumway RH (1988) Applied statistical time series analysis. Prentice-Hall, Englewood Cliffs
* [184] Shumway RH, Stoffer DS (1991) Dynamic linear models with switching. J Am Stat Assoc 86:763-769 (Correction: V87 p. 913)
* [185] Shumway RH, Verosub KL (1992) State space modeling of paleoclimatic time series. In: Pro. 5th int. meeting stat. climatol., Toronto, pp. 22-26, June, 1992
* [186] Shumway RH, Kim SE, Blandford RR (1999) Nonlinear estimation for time series observed on arrays. In: Ghosh S (ed) Asymptotics, nonparametrics and time series, Chapter 7, pp 227-258. Marcel Dekker, New York
* [187] Small CG, McLeish DL (1994) Hilbert space methods in probability and statistical inference. Wiley, New York
* [188] Smith AFM, West M (1983) Monitoring renal transplants: An application of the multiprocess Kalman filter. Biometrics 39:867-878
* [189] Spliid H (1983) A fast estimation method for the vector autoregressive moving average model with exogenous variables. J Am Stat Assoc 78:843-849
* [190] Stoffer DS (1982) Estimation of parameters in a linear dynamic system with missing observations. Ph.D. Dissertation. Univ. California, Davis
* [191] Stoffer DS, Scher M, Richardson G, Day N, Coble P (1988) A Walsh-Fourier analysis of the effects of moderate maternal alcohol consumption on neonatal sleep-state cycling. J Am Stat Assoc 83:954-963
* [192] Stoffer DS, Wall KD (1991) Bootstrapping state space models: Gaussian maximum likelihood estimation and the Kalman filter. J Am Stat Assoc 86:1024-1033
* [193] Stoffer DS, Tyler DE, McDougall AJ (1993) Spectral analysis for categorical time series: Scaling and the spectral envelope. Biometrika 80:611-622.
* [194] Stoffer DS (1999) Detecting common signals in multiple time series using the spectral envelope. J Am Stat Assoc 94:1341-1356
* [195] Stoffer DS, Wall KD (2004) Resampling in state space models. In: Harvey A, Koopman SJ, Shephard N (eds) State space and unobserved component models theory and applications, Chapter 9, pp 227-258. Cambridge University Press, Cambridge
* [196] Sugiura N (1978) Further analysis of the data by Akaike's information criterion and the finite corrections. Commun. Stat A Theory Methods 7:13-26
* [197] Taniguchi M, Puri ML, Kondo M (1994) Nonparametric approach for non-Gaussian vector stationary processes. J Mult Anal 56:259-283* [198] Tanner M, Wong WH (1987) The calculation of posterior distributions by data augmentation (with discussion). J Am Stat Assoc 82:528-554
* A study of daily sugar prices, 1961-79. In: Anderson OD (ed) Time series analysis: theory and practice, Vol 1, pp 203-226. Elsevier/North-Holland, New York
* [200] Tiao GC, Tsay RS (1989) Model specification in multivariate time series (with discussion). J Roy Stat Soc B 51:157-213
* [201] Tiao GC, Tsay RS, Wang T (1993) Usefulness of linear transformations in multivariate time series analysis. Empir Econ 18:567-593
* [202] Tong H (1983) Threshold models in nonlinear time series analysis. Springer lecture notes in statistics, vol 21. Springer, New York
* [203] Tong H (1990) Nonlinear time series: a dynamical system approach. Oxford Univ. Press, Oxford
* [204] Tsay RS (2002) Analysis of financial time series. Wiley, New York
* [205] Wahba G (1980) Automatic smoothing of the log periodogram. J Am Stat Assoc 75:122-132
* [206] Wahba G (1990) Spline models for observational data, vol 59. Society for Industrial Mathematics, Philadelphia
* [207] Watson GS (1966) Smooth regression analysis. Sankhya 26:359-378
* [208] Weiss AA (1984) ARMA models with ARCH errors. J Time Ser Anal 5:129-143
* [209] Whitle P (1951) Hypothesis testing in time series analysis. Almqvist & Wiksells, Uppsala
* [210] Whittle P (1961) Gaussian estimation in stationary time series. Bull Int Stat Inst 33:1-26
* [211] Wiener N (1949) The extrapolation, interpolation and smoothing of stationary time series with engineering applications. Wiley, New York
* [212] Wu CF (1983) On the convergence properties of the EM algorithm. Ann Stat 11:95-103
* [213] Young PC, Pedregal DJ (1998) Macro-economic relativity: Government spending, private investment and unemployment in the USA. Centre for Research on Environmental Systems and Statistics, Lancaster University, U.K.
* [214] Zucchini W, MacDonald IL (2009) Hidden Markov models for time series: An introduction using R. CRC Press, Boca Raton

## Index

* [] ACF, 18, 21 large sample distribution, 28, 490 multidimensional, 35 of an AR(_p_), 95 of an AR(1), 78 of an AR(2), 91 of an ARMA(1,1), 96 of an MA(_q_), 94 sample, 28 AIC, 49, 142, 204 multivariate case, 272 AICc, 49, 142 multivariate case, 272 Aliasing, 9, 167 Amplitude, 166 of a filter, 215 Analysis of Power, _see_ ANOPOW ANOPOW, 396, 404, 405 designed experiments, 409 APARCH, 259 AR model, 11, 76 conditional sum of squares, 118 bootstrap, 128 conditional likelihood, 117 estimation large sample distribution, 114, 500 likelihood, 117 maximum likelihood estimation, 116 missing data, 382 operator, 77 polynomial, 85 spectral density, 176 unconditional sum of squares, 117 vector, _see_ VAR with observational noise, 293 ARCH model ARCH(_p_), 258 ARCH(1), 254 Asymmetric power, 259 estimation, 256 GARCH, 258 ARFIMA model, 242, 245 ARIMA model, 132 fractionally integrated, 245 multiplicative seasonal models, 150 multivariate, 272 ARMA model, 83 \(\psi\)-weights, 93 conditional least squares, 119 pure seasonal models behavior of ACF and PACF, 148 unconditional least squares, 119 backcasts, 111 behavior of ACF and PACF, 99 bootstrap, 329 causality of, 86 functional least squares, 121 forecasts, 107 mean square prediction error, 108 based on infinite past, 107 prediction intervals, 110 truncated prediction, 109Gauss-Newton, 121

in state-space form, 324

invertibilty of, 86

large sample distribution of estimators, 125

likelihood, 118

MLE, 119

multiplicative seasonal model, 148

pure seasonal model, 146

unconditional least squares, 121

vector, _see_ VARMA model

ARMAX model, 220, 280, 323

bootstrap, 329

in state-space form, 323

ARX model, 273

Autocorrelation function, _see_ ACF

Autocovariance

calculation, 17

Autocovariance function, 16, 21, 77

multidimensional, 35

random sum of sines and cosines, 167

sample, 27

Autocovariance matrix, 34

sample, 34

Autoregressive Integrated Moving Average

Model, _see_ ARIMA model

Autoregressive models, _see_ AR model

Autoregressive Moving Average Models, _see_

ARMA model

Backcasting, 111

Backshift operator, 56

Bandwidth, 190

Bartlett kernel, 200

Beam, 401

Best linear predictor, _see_ BLP

BIC, 50, 142, 204

multivariate case, 272, 275

BLP, 101

_m_-step-ahead prediction, 105

mean square prediction error, 105

one-step-ahead prediction, 102

definition, 101

one-step-ahead prediction

mean square prediction error, 102

stationary processes, 101

Bone marrow transplant series, 291,

316

Bonferroni inequality, 196

Bootstrap, 128, 191, 204, 329

stochastic volatility, 364

Bounded in probability \(O_{P}\), 476

Brownian motion, 251

Cauchy sequence, 493

Cauchy-Schwarz inequality, 473, 493

Causal, 79, 85, 497

conditions for an AR(2), 88

vector model, 280

CCF, 19, 23

large sample distribution, 31

sample, 30

Central Limit Theorem, 481

M-dependent, 482

Cepstral analysis, 233

Characteristic function, 478

Chernoff information, 432

Chicken prices, 54

Cluster analysis, 436

Coherence, 207

estimation, 210

hypothesis test, 210, 524

multiple, 393

Completeness of \(L\)2, 474

Complex normal distribution, 519

Complex roots, 92

Conditional least squares, 119

Convergence in distribution, 478

Basic Approximation Theorem, 479

Convergence in probability, 475

Convolution, 175

Cosine transform

large sample distribution, 509

of a vector process, 389

properties, 184

Cospectrum, 207

of a vector process, 389

Cramer-Wold device, 479

Cross-correlation function, _see_ CCF

Cross-covariance function, 19

sample, 30

Cross-spectrum, 207

Cycle, 166

Daniell kernel, 197, 198

modified, 198

Deconvolution, 407

Density function, 15Designed experiments, _see_ ANOPOW

Deterministic process, 503

Detrending, 45

DFT, 169

inverse, 180

large sample distribution, 509

multidimensional, 227

of a vector process, 389

likelihood, 389

Differencing, 55-57

Discriminant analysis, 424

DJIA, _see_ Dow Jones Industrial Average, _see_

Dow Jones Industrial Average

DLM, 290, 321

Bayesian approach, 367

bootstrap, 329

innovations form, 328

maximum likelihood estimation

large sample distribution, 312

via EM algorithm, 310, 316

via Newton-Raphson, 305

MCMC methods, 374

observation equation, 290

state equation, 290

steady-state, 312

with switching, 348

EM algorithm, 354

maximum likelihood estimation, 353

DNA series, 457, 461

Dow Jones Industrial Average, 4

Durbin-Levinson algorithm, 103

Earthquake series, 6, 387, 421, 428, 433, 438

EM algorithm, 308

complete data likelihood, 309

DLM with missing observations, 316

expectation step, 309

maximization step, 310

Explosion series, 6, 387, 421, 428, 433, 438

Exponentially Weighted Moving Averages, 134

Factor analysis, 445

EM algorithm, 447

Fejer kernel, 200

FFT, 170

Filter, 57

amplitude, 215, 216

band-pass, 225

design, 225

high-pass, 212, 225

linear, 211

low-pass, 212, 225

matrix, 216, 217

optimum, 223

phase, 215, 216

recursive, 225

seasonal adjustment, 225

spatial, 227

time-invariant, 474

fMRI, _see_ Functional magnetic resonance

imaging series

Folding frequency, 167, 170

Fourier frequency, 170, 180

Fractional difference, 58, 242

fractional noise, 242

Frequency bands, 174, 189

Frequency response function, 175

of a first difference filter, 212

of a moving average filter, 212

Functional magnetic resonance imaging

series, 5, 386, 411, 413, 417, 444, 449

Fundamental frequency, 169, 170, 180

Glacial curve series, 59, 123, 140, 244, 253

Global temperature series, 3, 58, 292

Gradient vector, 306, 381

Growth rate, 135, 254

Harmonics, 194

Hessian matrix, 306, 381

Hidden Markov Model, _see_ HMM

Hidden Markov model, 337, 352

estimation, 354

Hilbert space, 493

closed span, 494

conditional expectation, 496

projection mapping, 494

regression, 495

HMM, 348

Poisson, 337, 341

Homogeneous difference equation

first order, 89

general solution, 91

second order, 89

solution, 90

Impulse response function, 175

Influenza series, 264, 355Infrasound series, 400, 402, 405, 408 Inner product space, 493 Innovations, 138, 304 standardized, 138 steady-state, 312 Innovations algorithm, 106 Integrated models, 131, 134, 150 forecasting, 133 Interest rate and inflation rate series, 330 Invertible, 83 vector model, 280 J-divergence measure, 436 Johnson & Johnson quarterly earnings series, 2, 318 Joint distribution function, 15 Kalman filter, 295 correlated noise, 322 innovations form, 328 Riccati equation, 312 stability, 311, 312 with missing observations, 314 with switching, 350 with time-varying parameters, 297 Kalman smoother, 299, 379 as a smoothing spline, 333 for the lag-one covariance, 303 spline smoothing, 335 with missing observations, 314 Kronecker's Lemma, 530 Kullback-Leibler information, 71, 431 Kurtosis, 360 LA Pollution - Mortality Study, 50, 69, 144, 273, 275, 325 Lag, 18, 24 Lag window estimator, 202 Laged regression model, 266 Lake Shasta series, 385, 390, 396 Lead, 24 Leakage, 201 sidelobe, 201 Least squares estimation, _see_ LSE Likelihood AR(1) model, 117 conditional, 117 innovations form, 118, 304 Linear filter, _see_ Filter Linear process, 25, 85 Ljung-Box-Pierce statistic, 139 multivariate, 277 Local level model, 298, 301, 370 Long memory, 58, 242 estimation, 243 estimation of \(d\), 248 spectral density, 247 LSE conditional sum of squares, 118 Gauss-Newton, 120 unconditional, 117 MA model, 10, 81 autocovariance function, 17, 94 Gauss-Newton, 122 mean function, 15 operator, 81 polynomial, 85 spectral density, 176 Maximum likelihood estimation, _see_ MLE Mean function, 15 Mean square convergence, 473 Method of moments estimators, _see_ Yule-Walker Minimum mean square error predictor, 100 Missing data, 316 MLE ARMA model, 119 conditional likelihood, 117 DLM, 305 state-space model, 305 via EM algorithm, 308 via Newton-Raphson, 119, 305 via scoring, 119 Moving average model, _see_ MA model New York Stock Exchange, 363 Newton-Raphson, 119 Non-negative definite, 22 Normal distribution marginal density, 15 multivariate, 26, 519 NYSE, 464 Order in probability \(o_{p}\), 476 Ordinary Least Squares, 46 Orthogonality property, 494PACK, 97 of an MA(1), 99 iterative solution, 104 large sample results, 114 of an AR(_p_), 98 of an AR(1), 98 of an MA(_q_), 99 Parameter redundancy, 84 Partial autocorrelation function, _see_ PACF Period, 165 Periodogram, 170, 180 distribution, 186 matrix, 431 Phase, 166 of a filter, 215 Pitch period, 3 Prediction equations, 101 Prewhiten, 32, 267 Principal components, 440 Projection Theorem, 494

Q-test, _see_ Ljung-Box-Pierce statistic Quadspectrum, 207 of a vector process, 389

Random sum of sines and cosines, 167, 505, 507 Random walk, 11, 16, 20, 133 autocovariance function, 18 Recruitment series, 5, 31, 60, 99, 110, 187, 191, 198, 210, 219, 268 Regression ANOVA table, 48 autocorrelated errors, 142, 324 Cochrane-Orcutt procedure, 144 coefficient of determination, 49 for jointly stationary series, 390 ANOPOW table, 396 Hilbert space, 495 lagged, 217 model, 45 multivariate, 272, 324 normal equations, 47 random coefficients, 407 spectral domain, 390 stochastic, 330, 407 ridge correction, 408 with deterministic inputs, 399

Return, 4, 135, 254 log-, 254 Riesz-Fischer Theorem, 474 Scatterplot matrix, 52, 60 Scatterplot smoothers kernel, 66 lowess, 67, 69 nearest neighbors, 67 splines, 68 Score vector, 306 SIC, 50 Signal plus noise, 12, 13, 222, 400 mean function, 16 Signal-to-noise ratio, 13, 223 Sine transform large sample distribution, 509 of a vector process, 389 properties, 184 Smoothing splines, 68, 333 Soil surface temperature series, 34, 36, 227 Southern Oscillation Index, 5, 31, 60, 187, 191, 198, 201, 204, 210, 212, 219, 224, 268 Spectral density, 173 autoregression, 204, 531 estimation, 189 adjusted degrees of freedom, 191 bandwidth stability, 196 confidence interval, 191 degrees of freedom, 190 large sample distribution, 190 nonparametric, 203 parametric, 203 resolution, 196 matrix, 209 linear filter, 217 of a filtered series, 175 of a moving average, 176 of an AR(2), 176 of white noise, 174 wavenumber, 226 Spectral distribution function, 173 Spectral envelope, 455 categorical time series, 458 real-valued time series, 463 Spectral Representation Theorem, 173, 179, 505, 507 vector process, 208, 507Speech series, 3, 29

Spline smoothing, 335

State-space model

Bayesian approach, 367

linear, _see_ DLM

Stationary

Gaussian series, 26

jointly, 23, 24

strictly, 19

weakly, 20

Stochastic process, 8

realization, 8

Stochastic regression, 330

Stochastic trend, 132

Stochastic volatility model, 361

bootstrap, 364

estimation, 362

Structural component model, 70, 318, 355

Taper, 200, 201

cosine bell, 200

Taylor series expansion in probability, 477

Tchebycheff inequality, 473

Time series, 8

categorical, 458

complex-valued, 439

multidimensional, 34, 226

multivariate, 19, 33

two-dimensional, 226

Toepliz Matrix, 528

Transfer function model, 266

Transformation

Box-Cox, 58

Trend stationarity, 22

Triangle inequality, 493

U.S. GNP series, 136, 139, 142, 257

U.S. macroeconomic series, 452

U.S. population series, 141

Unconditional least squares, 119

Unit root tests, 250

Augmented Dickey-Fuller test, 252

Dickey-Fuller test, 252

Phillips-Perron test, 252

VAR model, 273, 275

estimation

large sample distribution, 279

operator, 280

Variogram, 37, 43

VARMA model, 280

autocovariance function, 281

estimation

Spliid algorithm, 283

identifiability of, 282

Varve series, 248

Viterbi algorithm, 353

VMA model, 280

operator, 280

Volatility, 4, 253

Wavenumber spectrum, 226

estimation, 227

Weak law of large numbers, 476

White noise, 9

autocovariance function, 17

Gaussian, 9

vector, 273

Whittle likelihood, 205, 430

Wold Decomposition, 503

Yule-Walker

equations, 113

vector model, 277

estimators, 113

AR(2), 114

MA(1), 115

large sample results, 114