[MISSING_PAGE_EMPTY:1]

_Springer Texts in Statistics (STS)_ includes advanced textbooks from 3rd- to 4th-year undergraduate courses to 1st- to 2nd-year graduate courses. Exercise sets should be included. The series editors are currently Genevera I. Allen, Richard D. De Veaux, and Rebecca Nugent. Stephen Fienberg, George Casella, and Ingram Olkin were editors of the series for many years.

More information about this series at [http://www.springer.com/series/417](http://www.springer.com/series/417)

[MISSING_PAGE_EMPTY:9310]

Ronald Christensen

Department of Mathematics and Statistics

University of New Mexico

Albuquerque, NM, USA

ISSN 1431-875X

ISSN 2197-4136 (electronic)

Springer Texts in Statistics

ISBN 978-3-030-29163-1

ISBN 978-3-030-29164-8 (eBook)

[https://doi.org/10.1007/978-3-030-29164-8](https://doi.org/10.1007/978-3-030-29164-8)

(c) Springer Nature Switzerland AG 1991, 2001, 2019

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

This Springer imprint is published by the registered company Springer Nature Switzerland AG.

The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland_To Pete, Russ, and Scot, my pals from high school;_

_also_

_Wes, Ed, and everyone from graduate school._

## Preface to the Third Edition

This is the third edition of _Advanced Linear Modeling (ALM)_. It is roughly 50% longer than the previous edition. It discusses the extension of linear models into areas beyond those usually addressed in regression and analysis of variance. As in previous editions, its primary emphasis is on models in which the data display some form of dependence and many of the changes from the previous edition were made to systematize this emphasis on dependent data. Nonetheless, it begins with topics in modern regression analysis related to nonparametric regression and penalized estimation (regularization). R code for the analyses in the book is available at [http://www.stat.unm.edu/~fletcher/R-ALMIII.pdf](http://www.stat.unm.edu/~fletcher/R-ALMIII.pdf).

_Mathematical background is contained in Appendix A on differentiation and Kronecker products. Also some notation used throughout the book is set in Sect. 1.1._

This edition has been written in conjunction with the fifth edition of Christensen (2011), often hereafter referred to as _PA_. Some discussions that previously appeared in _PA_ have been moved here. Obviously, you cannot do advanced linear modeling without previously learning about linear modeling. I have tried to make this book readable to people who have studied linear model theory from sources other than _PA_, but I need to cite some source for basic results on linear models, so obviously I cite _PA_. In cases where I need to cite results for which the new version of _PA_ is different from the previous edition(s), the citations are given as _PA-V_. I have rearranged the topics from the previous edition of _ALM_ so that the material related to independent data comes first followed by the material on dependent data. The chapter on response surfaces has been dropped but is available in a new volume downloadable from my website: [http://www.stat.unm.edu/~fletcher/TopicsInDesign](http://www.stat.unm.edu/~fletcher/TopicsInDesign). Some familiarity with inner products is assumed, especially in Chaps. 1 and 3. The required familiarity can be acquired from _PA_.

Chapter 1 expands the previous introduction to nonparametric regression. The discussion follows what is commonly known as the basis function approach, despite the fact that many of the techniques do not actually involve the use of basis functions per se. In fact, when dealing with spaces of functions the very idea of a basis is subject to competing definitions. Tim Hanson pointed out to me the obvious fact that if a group of functions are linearly independent, they always form a basis forthe space that they span, but I think that in nonparametric regression the idea is to approximate wider collections of functions than just these spanning sets. Chapter 1 now also includes a short introduction to models involving an entire function of predictor variables.

Chapter 2 is an expanded version of the discussion of penalized regression from Christensen (2011). A new Chap. 3 extends this by introducing reproducing kernel Hilbert spaces.

Chapter 4 is new except for the last section. It gives results on an extremely general linear model for dependent or heteroscedastic data. It owes an obvious debt to Christensen (2011, Chapter 12). It contains several particularly useful exercises. In a standard course on linear model theory, the theory of estimation and testing for dependent data is typically introduced but not developed, see for example Christensen (2011, Sections 2.7 and 3.8). Section 4.1 of this book reviews, but does not re-prove, those results. This book then applies those fundamental results to develop theory for a wide variety of practical models.

I finally figured out how, without overwhelming the ideas in abstruse notation, to present MINQUE as linear modeling, so I have done that in Chap. 4. In a technical subsection, I give in to the abstruse notation so as to derive the MINQUE equations. Previously, I just referred the reader to Rao for the derivation.

Chapter 5 on mixed models originally appeared in _PA_. It has been shortened in places due of overlap with Chap. 4 but includes several new examples and exercises. It contains a new emphasis on linear covariance structures that leads not only to variance component models but the new Sect. 5.6 that examines a quite general longitudinal data model. The details of the recovery of interblock information for a balanced incomplete block design from _PA_ no longer seem relevant, so they were relegated, along with the response surface material, to the volume on my website.

Chapters 6 and 7 introduce time series: first the frequency domain which uses models from Chap. 1 but with random effects as in Chap. 5 and then the time domain approach which can be viewed as applications of ideas from the frequency domain.

Chapter 8 on spatial data is little changed from the previous edition. Mostly, the references have been updated.

The former chapter on multivariate models has been split into three: Chap. 9 on general theory with a new section relating multivariate models to spatial and time series models and a new discussion of multiple comparisons, Chap. 10 on applications to specific models, and Chap. 11 with an expanded discussion of generalized multivariate linear models (also known as generalized multivariate analysis of variance (GMANOVA) and growth curve models).

Chapters 12 and 14 are updated versions of the previous chapters on discriminant analysis and principal components. Chapter 13 is a new chapter on binary regression and discrimination. Its raison d'etre is that it devotes considerable attention to support vector machines. Chapter 14 contains a new section on classical multidimensional scaling.

From time to time, I mention the virtues of Bayesian approaches to problems discussed in the book. One place to look for more information is _BIDA_, i.e., Christensen, Johnson, Branscum, and Hanson (2010).

Thanks to my son Fletcher who is always the first person I ask when I have doubts. Joe Cavanaugh and Mohammad Hattab have been particularly helpful as have Tim Hanson, Wes Johnson, and Ed Bedrick. Finally, my thanks to Al Nosedal-Sanchez, Curt Storlie, and Thomas Lee for letting me modify our joint paper into Chap. 3.

As I have mentioned elsewhere, the large number of references to my other works is as much about slot as it is ego. In some sense, with the exception of _BIDA_, all of my books are variations on a theme.

Albuquerque, NM, USA Ronald Christensen

February 2019

## Preface to the Second Edition

This is the second edition of _Linear Models for Multivariate, Time Series and Spatial Data_. It has a new title to indicate that it contains much new material. The primary changes are the addition of two new chapters: one on nonparametric regression and the other on response surface maximization. As before, the presentations focus on the linear model aspects of the subject. For example, in the nonparametric regression chapter there is very little about kernel regression estimation but quite a bit about series approximations, splines, and regression trees, all of which can be viewed as linear modeling.

The new edition also includes various smaller changes. Of particular note are a subsection in Chap. 1 on modeling longitudinal (repeated measures) data and a section in Chap. 6 on covariance structures for spatial lattice data. I would like to thank Dale Zimmerman for the suggestion of incorporating material on spatial lattices. Another change is that the subject index is now entirely alphabetical.

Albuquerque, NM, USA Ronald Christensen

May 9, 2000

## Preface to the First Edition

This is a companion volume to _Plane Answers to Complex Questions: The Theory of Linear Models_. It consists of six additional chapters written in the same spirit as the last six chapters of the earlier book. Brief introductions are given to topics related to linear model theory. No attempt is made to give a comprehensive treatment of the topics. Such an effort would be futile. Each chapter is on a topic so broad that an in-depth discussion would require a book-length treatment.

People need to impose structure on the world in order to understand it. There is a limit to the number of unrelated facts that anyone can remember. If ideas can be put within a broad, sophisticatedly simple structure, not only are they easier to remember but often new insights become available. In fact, sophisticatedly simple models of the world may be the only ones that work. I have often heard Arnold Zellner say that, to the best of his knowledge, this is true in econometrics. The process of modeling is fundamental to understanding the world.

In Statistics, the most widely used models revolve around linear structures. Often the linear structure is exploited in ways that are peculiar to the subject matter. Certainly, this is true of frequency domain time series and geostatistics. The purpose of this volume is to take three fundamental ideas from standard linear model theory and exploit their properties in examining multivariate, time series and spatial data. In decreasing order of importance to the presentation, the three ideas are: best linear prediction, projections, and Mahalanobis distance. (Actually, Mahalanobis distance is a fundamentally multivariate idea that has been appropriated for use in linear models.) Numerous references to results in _Plane Answers_ are made. Nevertheless, I have tried to make this book as independent as possible. Typically, when a result from _Plane Answers_ is needed not only is the reference given but also the result itself. Of course, for proofs of these results the reader will have to refer to the original source.

I want to reemphasize that this is a book about linear models. It is not traditional multivariate analysis, time series, or geostatistics. Multivariate linear models are viewed as linear models with a nondiagonal covariance matrix. Discriminant analysis is related to the Mahalanobis distance and multivariate analysis of variance. Principal components are best linear predictors. Frequency domain time series involves linear models with a peculiar design matrix. Time domain analysis involves models that are linear in the parameters but have random design matrices. Best linear predictors are used for forecasting time series; they are also fundamental to the estimation techniques used in time domain analysis. Spatial data analysis involves linear models in which the covariance matrix is modeled from the data; a primary objective in analyzing spatial data is making best linear unbiased predictions of future observables. While other approaches to these problems may yield different insights, there is value in having a unified approach to looking at these problems. Developing such a unified approach is the purpose of this book.

There are two well-known models with linear structure that are conspicuous by their absence in my two volumes on linear models. One is Cox's (1972) proportional hazards model. The other is the generalized linear model of Nelder and Wedderburn (1972). The proportional hazards methodology is a fundamentally nonparametric technique for dealing with censored data having linear structure. The emphasis on nonparametrics and censored data would make its inclusion here awkward. The interested reader can see Kalbfleisch and Prentice (1980). Generalized linear models allow the extension of linear model ideas to many situations that involve independent non-normally distributed observations. Beyond the presentation of basic linear model theory, these volumes focus on methods for analyzing correlated observations. While it is true that generalized linear models can be used for some types of correlated data, such applications do not flow from the essential theory. McCullagh and Nelder (1989) give a detailed exposition of generalized linear models, and Christensen (1997) contains a short introduction.

## Acknowledgments

I would like to thank MINITAB1 for providing me with a copy of release 6.1.1, BMDP with copies of their programs 4M, 1T, 2T, and 4V, and Dick Lund for providing me with a copy of MSUSTAT. Nearly all of the computations were performed with one of these programs. Many were performed with more than one.

Footnote 1: MINITAB is a registered trademark of Minitab, Inc., 3081 Enterprise Drive, State College, PA 16801, telephone: (814) 238â€“3280.

I would not have tackled this project but for Larry Blackwood and Bob Shumway. Together Larry and I reconfirmed, in my mind anyway, that multivariate analysis is just the same old stuff. Bob's book put an end to a specter that has long haunted me: a career full of half-hearted attempts at figuring out basic time series analysis.

At my request, Ed Bedrick, Bert Koopmans, Wes Johnson, Bob Shumway, and Dale Zimmerman tried to turn me from the errors of my ways. I sincerely thank them for their valuable efforts. The reader must judge how successful they were with a recalcitrant subject. As always, I must thank my editors, Steve Fienberg and Ingram Olkin, for their suggestions. Jackie Damrau did an exceptional job in typing the first draft of the manuscript.

Finally, I have to recognize the contribution of Magic Johnson. I was so upset when the 1987-88 Lakers won a second consecutive NBA title that I began writing this book in order to block the mental anguish. I am reminded of Woody Allen's dilemma: is the importance of life more accurately reflected in watching _The Sorrow and the Pity_ or in watching the Knicks? (In my case, the Jazz and the Celtics.) It's a tough call. Perhaps life is about actually making movies and doing statistics.

Albuquerque, NM, USA Ronald Christensen April 19, 1990

## References

* Christensen (1997) Christensen, Ronald (1997). _Log-Linear Models and Logistic Regression_, Second Edition. Springer-Verlag, New York.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York: Springer.
* Christensen et al. (2010) Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton, FL: Chapman and Hall/CRC Press.
* Cox (1972) Cox, D. R. (1972). Regression models and life tables (with discussion). _Journal of the Royal Statistical Society, Series B_, _34_, 187-220.
* Kalbfleisch & Prentice (1980) Kalbfleisch, J. D., & Prentice, R. L. (1980). _The statistical analysis of failure time data_. New York: Wiley.
* McCullagh & Nelder (1989) McCullagh, P., & Nelder, J. A. (1989). _Generalized linear models_ (2nd Ed.). London: Chapman and Hall.
* Nelder & Wedderburn (1972) Nelder, J. A., & Wedderburn, R. W. M. (1972). Generalized linear models. _Journal of the Royal Statistical Society, Series A_, _135_, 370-384.

###### Contents

* 1 Nonparametric Regression
	* 1.1 Basic Notation
	* 1.2 Linear Approximations
	* 1.3 Simple Nonparametric Regression
	* 1.4 Estimation
		* 1.4.1 Polynomials
		* 1.4.2 Cosines
		* 1.4.3 Haar Wavelets
		* 1.4.4 Cubic Splines
		* 1.4.5 Orthonormal Series Estimation
	* 1.5 Variable Selection
	* 1.6 Heteroscedastic Simple Nonparametric Regression
	* 1.7 Approximating-Functions with Small Support
		* 1.7.1 Polynomial Splines
		* 1.7.2 Fitting Local Functions
		* 1.7.3 Local Regression
	* 1.8 Nonparametric Multiple Regression
		* 1.8.1 Redefining \(\phi\) and the Curse of Dimensionality
		* 1.8.2 Reproducing Kernel Hilbert Space Regression
	* 1.9 Testing Lack of Fit in Linear Models
* 1.10 Regression Trees
* 1.11 Regression on Functional Predictors
* 1.12 Density Estimation
* 1.13 Exercises
* 2Penalized Estimation
	* 2.1 Introduction
		* 2.1.1 Reparameterization and RKHS Regression: It's All About the Penalty
		* 2.1.2 Nonparametric Regression
	* 2.2 Ridge Regression
		* 2.2.1 Generalized Ridge Regression
		* 2.2.2 Picking \(k\)
		* 2.2.3 Nonparametric Regression
	* 2.3 Lasso Regression
	* 2.4 Bayesian Connections
	* 2.5 Another Approach
		* 2.5.1 Geometry
		* 2.5.2 Equivalence of Approaches
	* 2.6 Two Other Penalty Functions
		* 2.7.1 References
* 3Reproducing Kernel Hilbert Spaces
	* 3.1 Introduction
		* 3.1.1 Interpolating Splines
	* 3.2 Banach and Hilbert Spaces
		* 3.2.1 Banach Spaces
		* 3.2.2 Hilbert Spaces
	* 3.3 Reproducing Kernel Hilbert Spaces
		* 3.3.1 The Projection Principle for an RKHS
	* 3.4 Two Approaches
		* 3.4.1 Testing Lack of Fit
	* 3.5 Penalized Regression with RKHSs
		* 3.5.1 Ridge and Lasso Regression
		* 3.5.2 Smoothing Splines
		* 3.5.3 Solving the General Penalized Regression Problem
		* 3.5.4 General Solution Applied to Ridge Regression
		* 3.5.5 General Solution Applied to Cubic Smoothing Splines
	* 3.6 Choosing the Degree of Smoothness
	* 3.6 Choosing the Degree of Smoothness
		* 3.6.1 References
* 4Covariance Parameter Estimation
	* 4.1 Introduction and Review
		* 4.1.1 Estimation of \(\beta\)
		* 4.1.2 Testing
		* 4.1.3 Prediction
		* 4.1.4 Quadratic Estimation of \(\theta\)
	* 4.2 Maximum Likelihood
		* 4.2.1 Generalized Likelihood Ratio Tests
	* 4.3 Restricted Maximum Likelihood Estimation
	* 4.4 Linear Covariance Structures
	* 4.5 MINQUE
		* 4.5.1 Deriving the MINQUE Equations
	* 4.6 MIVQUE
	* 4.7 The Effect of Estimated Covariances
		* 4.7.1 Mathematical Results
	* 4.8 References
* 5 Mixed Models and Variance Components
	* 5.1 Mixed Models
	* 5.2 Mixed Model Equations
	* 5.3 Equivalence of Random Effects and Ridge Regression
	* 5.4 Partitioning and Linear Covariance Structures
	* 5.5 Variance Component Models
		* 5.5.1 Variance Component Estimation
	* 5.6 A Longitudinal Model
	* 5.7 Henderson's Method 3
		* 5.7.1 Additional Estimates
	* 5.8 Exact \(F\) Tests for Variance Components
		* 5.8.1 Wald's Test
		* 5.8.2 Ofversten's Second Method
		* 5.8.3 Comparison of Tests
		* 5.8.3 References
* 6 Frequency Analysis of Time Series
	* 6.1 Stationary Processes
	* 6.2 Basic Data Analysis
	* 6.3 Spectral Approximation of Stationary Time Series
	* 6.4 The Random Effects Model
	* 6.5 The White Noise Model
		* 6.5.1 The Reduced Model: Estimation
		* 6.5.2 The Reduced Model: Prediction
		* 6.5.3 Summary of Sects. 6.2, 6.3, 6.4, and 6.5
	* 6.6 Linear Filtering
		* 6.6.1 Recursive Filters
		* 6.6.2 Summary
	* 6.7 The Coherence of Two Time Series
	* 6.8 Fourier Analysis
	* 6.9 Additional Exercises
* 7 Time Domain Analysis
	* 7.1 Correlations and Prediction
		* 7.1.1 Partial Correlation and Best Linear Prediction
		* 7.1.2 The Durbin-Levinson Algorithm
		* 7.1.3 Innovations Algorithm
	* 7.2 Time Domain Models
		* 7.2.1 Autoregressive Models: \(AR(p)\)s
		* 7.2.2 Moving Average Models: \(MA(q)\)s
		* 7.2.3 Autoregressive Moving Average Models: \(ARMA(p,q)\)s
		* 7.2.4 Autoregressive Integrated Moving Average Models: \(ARIMA(p,d,q)\)s
	* 7.3 Time Domain Prediction
		* 7.3.1 Conditioning on \(Y_{\infty}\)
	* 7.4 Nonlinear Least Squares
		* 7.4.1 The Gauss-Newton Algorithm
		* 7.4.2 Nonlinear Regression
	* 7.5 Estimation
		* 7.5.1 Correlations
		* 7.5.2 Conditional Estimation for \(AR(p)\) Models
		* 7.5.3 Conditional Least Squares for \(ARMA(p,q)\)s
		* 7.5.4 Conditional MLEs for \(ARMA(p,q)\)s
		* 7.5.5 Unconditional Estimation for \(ARMA(p,q)\) Models
		* 7.5.6 Estimation for \(ARIMA(p,d,q)\) Models
	* 7.6 Model Selection
		* 7.6.1 Box-Jenkins
		* 7.6.2 Model Selection Criteria
		* 7.6.3 An Example
	* 7.7 Seasonal Adjustment
	* 7.8 The Multivariate State-Space Model and the Kalman Filter
		* 7.8.1 The Kalman Filter
		* 7.8.2 Parameter Estimation
		* 7.8.3 Missing Values
	* 7.9 Additional Exercises
* 8.10.11 Statonarity
	* 8.2 Best Linear Unbiased Prediction of Spatial Data: Kriging
		* 8.2.1 Block Kriging
		* 8.2.2 Gaussian Process Regression
	* 8.3 Prediction Based on the Semivariogram: Geostatistical Kriging
	* 8.4 Measurement Error and the Nugget Effect
	* 8.5 The Effect of Estimated Covariances on Prediction
	* 8.6 Models for Covariance Functions and Semivariograms
		* 8.6.1 The Linear Covariance Model
		* 8.6.2 Nonlinear Isotropic Covariance Models
		* 8.6.3 Modeling Anisotropic Covariance Functions
		* 8.6.4 Nonlinear Semivariograms

[MISSING_PAGE_FAIL:18]

11.5 Longitudinal Data 440 11.5.1 Full Data 440 11.5.2 Incomplete Data 443 11.6 Functional Data Analysis 446 11.7 Generalized Split Plot Models 451 11.7.1 GMLMs Are GSP Models 452 11.8 Additional Exercises 453 References
* 12 **Discrimination and Allocation** 457 12.1 The General Allocation Problem 461 12.1.1 Mahalanobis Distance 461 12.1.2 Maximum Likelihood 461 12.1.3 Bayesian Methods 462 12.2 Estimated Allocation and QDA 465 12.3 Linear Discrimination Analysis: LDA 468 12.4 Cross-Validation 471 12.5 Discussion 473 12.6 Stepwise LDA 475 12.7 Linear Discrimination Coordinates 481 12.7.1 Finding Linear Discrimination Coordinates 486 12.7.2 Using Linear Discrimination Coordinates 490 12.7.3 Relationship to Mahalanobis Distance Allocation 492 12.7.4 Alternate Choice of Linear Discrimination Coordinates 494 12.8 Linear Discrimination 495 12.9 Additional Exercises 497 References
* 13 **Binary Discrimination and Regression** 503 13.1 Binomial Regression 504 13.1.1 Data Augmentation Ridge Regression 507 13.2 Binary Prediction 508 13.3 Binary Generalized Linear Model Estimation 510 13.4 Linear Prediction Rules 511 13.4.1 Loss Functions 514 13.4.2 Least Squares Binary Prediction 515 13.5 Support Vector Machines 515 13.5.1 Probability Estimation 517 13.5.2 Parameter Estimation 517 13.5.3 Advantages of SVMs 521 13.5.4 Separating Hyper-Hogwash 521 13.6 Best Prediction and Probability Estimation 523 13.7 Binary Discrimination 525

[MISSING_PAGE_EMPTY:9326]

## Chapter 1 Nonparametric Regression

**Abstract** This chapter introduces nonparametric regression for a single predictor variable, discusses the curse of dimensionality that plagues nonparametric regression with multiple predictor variables, and discusses the kernel trick and related ideas as methods for overcoming the curse of dimensionality.

In the late 1990s, the orthogonal series approach to nonparametric regression became increasingly popular; see Hart (1997), Ogden (1997), and Efromovich (1999). In this approach, _orthogonal series_ of functions are used to approximate the regression function. Later, the orthogonality was de-emphasized so that now a series of more general _basis functions_ is often used to approximate the regression function. Basis functions, and other linear-approximation methods for which many of the series elements have small support, methods such as splines and wavelets, seem particularly useful. We discuss these approaches to nonparametric regression as fitting linear models.

Suppose we have a dependent variable \(y\) and a vector of predictor variables \(x\). Regression is about estimating \(\text{E}(y|x)\). In linear regression, we assume that \(\text{E}(y|x)=x^{\prime}\beta\) for some unknown parameter vector \(\beta\). Recall that this includes fitting indicator variables and polynomials as special cases. In nonlinear regression we assume that \(\text{E}(y|x)=f(x;\beta)\), where the function \(f\) is known but the vector \(\beta\) is unknown; see Sect. 7.4 and Christensen (1996, Chapter 18; 2015, Chapter 23). A special case of nonlinear regression involves linearizable models, including generalized linear models, that assume \(\text{E}(y|x)=f(x^{\prime}\beta)\) for \(f\) known, cf. Christensen (1997, Chapter 9). The key idea in nonlinear regression is using calculus to linearize the model. In _nonparametric regression_, we assume that \(\text{E}(y|x)=f(x)\), where the function \(f\) is unknown. Note the absence of a vector of parameters \(\beta\), hence the name nonparametric. Often, \(f\) is assumed to be continuous or to have some specified number of derivatives. In reality, nonparametric regression is exactly the opposite of what its name suggests. Nonparametric regression involves fitting far more parameters than either standard linear or nonlinear regression.

[MISSING_PAGE_FAIL:22]

### Basic Notation

Before proceeding we set some notation that is used throughout the book, unless defined otherwise for particular purposes. A linear model has \(Y=X\beta+e\) where \(Y\) is an \(n\times 1\) vector of observable random variables, \(X\) is an \(n\times p\) matrix of known values, \(\beta\) is a \(p\times 1\) vector of fixed but unknown coefficients, and \(e\) is an \(n\times 1\) vector of unobservable random errors. For this to be a linear model we need \(\mathrm{E}(e)=0\) so that \(\mathrm{E}(Y)=X\beta\). A _standard linear model_ assumes that an individual observation or error has variance \(\sigma^{2}\) and that \(\mathrm{Cov}(Y)=\mathrm{Cov}(e)=\sigma^{2}I\). The assumption that the observations have a multivariate normal distribution can be written \(Y\sim N(X\beta,\sigma^{2}I)\). A partitioned linear model is written \(Y=X\beta+Z\gamma+e\) where \(Z\) is also a matrix of known values and \(\gamma\) is also a vector of fixed, unknown coefficients. If \(Z\) has \(s\) columns, write

\[X=[X_{1},\ldots,X_{p}]=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix};\quad Z=[Z_{1},\ldots,Z_{s}]=\begin{bmatrix}z_{1}^ {\prime}\\ \vdots\\ z_{n}^{\prime}\end{bmatrix}.\]

For any vector \(v\), \(\|v\|^{2}\equiv v^{\prime}v\) is the squared (Euclidean) length of \(v\). The Euclidean inner product between two vectors \(u\) and \(v\) is \(u^{\prime}v\). They are perpendicular (orthogonal), written \(v\perp u\), if \(v^{\prime}u=0\). \(A^{-}\) denotes the generalized inverse of the matrix \(A\), \(r(A)\) denotes its rank, and \(\mathrm{tr}(A)\) denotes its trace. \(M\) denotes the perpendicular projection operator (ppo) onto the column space of \(X\). The column space of \(X\) is denoted \(C(X)\). (With tongue slightly in cheek) the _Fundamental Theorem of Least Squares Estimation_ is that in a linear model, \(\hat{\beta}\) is a least squares estimate if and only if

\[X\hat{\beta}=MY.\]

More generally, \(M_{A}\) denotes the ppo onto \(C(A)\). \(C(A)^{\perp}\) denotes the orthogonal complement of \(C(A)\), i.e. all the vectors that are orthogonal to \(C(A)\). If \(C(X)\subset C(A)\), \(C(X)_{C(A)}^{\perp}\) denotes the orthogonal complement of \(C(X)\) with respect to \(C(A)\), i.e. all vectors in \(C(A)\) that are orthogonal to \(C(X)\). An \(r\times c\) matrix of 1s is denoted \(J_{r}^{c}\) with \(J_{n}\equiv J_{n}^{1}\) and \(J\equiv J_{n}\).

This is all common notation and, except for the use of \(M\) and \(J\), it is pretty much standard notation. (Some authors prefer \(P\) and \(\mathbf{1}\).) It is important to understand the theory associated with this notation. For example, I expect the reader to know (or at least believe when I write) that the ppo onto \(C(X)_{C(A)}^{\perp}\) is \(M_{A}-M\). Such background can be found in many places including \(PA\).

### Linear Approximations

The key idea behind linear approximations is that a finite linear combination of some known functions can approximate a wide variety of functions on a closed bounded set, cf. the famous Stone-Weierstrass theorem. For convenience, we initially assume that \(f\) is defined on the interval \([0,1]\) and is continuous. There are many ways to approximate \(f\) including polynomials, sines and cosines, step functions, and also by things similar to step functions called _wavelets_. Most often we assume that for some predictor variable \(x\)

\[f(x)=\sum_{j=0}^{\infty}\beta_{j}\phi_{j}(x),\]

where the \(\phi_{j}\)s are known functions that can be defined in many ways. Later we will use this characterization with \(x\) being a \(p\) vector instead of a scalar. In particular, with \(p=1\) and functions defined on the unit interval, we can take for \(j=0,1,2,\ldots\)

\[\phi_{j}(x)=x^{j}, \tag{1.2.1}\]

or

\[\phi_{j}(x)=\cos(\pi jx), \tag{1.2.2}\]

or

\[\phi_{2j}(x)=\cos(\pi jx)\quad\phi_{2j+1}(x)=\sin(\pi jx). \tag{1.2.3}\]

When using (1.2.2), it should be noted that the derivative of every \(\cos(\pi jx)\) function is \(0\) at \(x=0\), so the derivative of \(f(x)\) should be \(0\) at \(x=0\).

In practice we approximate \(f\) with a finite number of terms which determines a linear model in which only the \(\beta_{j}\)s are unknown. We need to determine an appropriate finite approximation and estimate the corresponding \(\beta_{j}\)s

With a single predictor, another obvious approximation uses step functions but some care must be used. Let \(\mathcal{I}_{A}\) be the _indicator function_ for the set \(A\), namely

\[\mathcal{I}_{A}(x)=\left\{\begin{array}{ll}1&\mbox{if $x\in A$}\\ 0&\mbox{otherwise.}\end{array}\right.\]

Obviously, if we define

\[\phi_{j}(x)=\mathcal{I}_{(\frac{j-1}{\min},\frac{j}{\min})}(x),\quad j=0,1, \ldots,m,\]

we can approximate any continuous function \(f\), and as \(m\to\infty\) we can approximate \(f\) arbitrarily well. Note that \(\phi_{0}(x)\) is essentially \(\mathcal{I}_{\{0\}}(x)\). Technically, rather than the infinite sum characterization, we are defining a _triangular array_ of functions \(\phi_{jm}\), \(j=1,\ldots,m\); \(m=1,2,3,\ldots\) and assuming that

\[f(x)=\lim_{m\to\infty}\sum_{j=0}^{m}\beta_{jm}\phi_{jm}(x). \tag{1.2.4}\]

[MISSING_PAGE_FAIL:25]

It is customary to call \(\phi_{0}(x)\) the _father_ wavelet function and \(\phi_{1}(x)\) the _mother_ function. Note that all of the subsequent functions are obtained from the mother function by changing the location and scale, for example, \(\phi_{3}(x)=\phi_{1}(2x-1)\), \(\phi_{7}(x)=\phi_{1}(4x-3)\), and, in general, if \(j=2^{r}+k\) for \(k=0,1,\ldots,2^{r}-1\), then \(\phi_{j}(x)=\phi_{1}(2^{r}x-k)\).

Actually, _this idea of changing location and scale can be applied to any mother function \(\phi_{1}\) that is 0 outside the unit interval and integrates to 0 over the unit interval_, hence generating different families of wavelets to be used as a basis series. (Rather than integrating to 0, theoretical developments often impose a stronger admissibility condition on \(\phi_{1}\).) For simplicity we restrict ourselves to looking at Haar wavelets but my impression is that they are rarely used in practice. The _Mexican hat (Ricker) wavelet_ seems to be quite popular.

_Orthogonal series_ approximations use basis functions that are orthogonal in an appropriate inner product. Typically, the functions \(\phi_{j}\) would be defined to be orthonormal in \(\mathcal{L}^{2}\) space. \(\mathcal{L}^{2}\) is the space of all functions that are square integrable, that is,

\[\int_{0}^{1}f(x)^{2}dx<\infty.\]

The inner product of two functions, say \(f\) and \(g\), is

\[\langle f,g\rangle\equiv\int_{0}^{1}f(x)g(x)dx,\]

so \(f\) and \(g\) are defined to be orthogonal if their inner product is 0. In other words, orthogonal functions \(\phi_{j}\) are defined to have \(\int_{0}^{1}\phi_{j}(x)\phi_{k}(x)dx=0\) for \(j\neq k\). In particular, the polynomial functions given in (1.2.1) would have to be adjusted using the Gram-Schmidt theorem to make them orthogonal. The norm of a function \(f\) in \(\mathcal{L}^{2}\) is

\[\|f\|\equiv\left[\int_{0}^{1}f(x)^{2}dx\right]^{1/2}.\]

Thus, for the \(\phi_{j}\)s to be orthonormal, they must be orthogonal and \(\phi_{j}(x)\) needs to have \(\int_{0}^{1}[\phi_{j}(x)]^{2}dx=1\). Methods based on Legendre polynomials (Gram-Schmidt-ing the polynomials), cosines and/or sines, and many wavelets are designed as orthonormal series. Our definition of wavelets needs rescaling before they square integrate to 1, i.e., redefine \(\phi_{j}(x)=2^{r/2}\phi_{1}(2^{r}x-k)\) where \(\phi_{1}\) has norm 1. The choice of the mother function \(\phi_{1}\) determines whether the sequence is orthogonal.

#### Exercise 1.1

Show that the Haar wavelets are orthogonal.

As we will see, for most regression problems \(\mathcal{L}^{2}\) orthogonality is largely irrelevant, except perhaps for extremely large data sets.

_A technical note:_ It is presumed that the reader has some familiarity with the concepts of random variables converging in distribution, in probability, with probability one (almost surely), and in \(\mathcal{L}^{2}\). Recall that \(\mathcal{L}^{2}\) convergence and almost sure convergence imply convergence in probability, which implies convergence in distribution but neither \(\mathcal{L}^{2}\) nor almost sure convergence imply the other. Similar concepts apply to the functions \(f\) wherein almost sure convergence is analogous to convergence almost everywhere. In the relationships \(f(x)=\sum_{j=0}^{\infty}\beta_{j}\phi_{j}(x)\equiv\lim_{m\to\infty}\sum_{j=0}^{m }\beta_{j}\phi_{j}(x)\) and \(f(x)=\lim_{m\to\infty}\sum_{j=0}^{m}\beta_{jm}\phi_{jm}(x)\), the convergence involved might be for every single \(x\in[0,1]\) when \(f\) is continuous or it might be almost everywhere convergence or the convergence might be \(\mathcal{L}^{2}\) convergence, i.e.,

\[\lim_{m\to\infty}\left\|f(x)-\sum_{j=0}^{m}\beta_{jm}\phi_{jm}(x)\right\|=0,\]

for any \(f\in\mathcal{L}^{2}\). Such issues have little practical effect on fitting approximating linear models to finite data.

_If \(x\) is a vector instead of a scalar, alternative \(\phi_{j}\) functions need to be used; see Sect. 8._

### Simple Nonparametric Regression

The simple nonparametric regression model is

\[y_{i}=f(x_{i})+\varepsilon_{i},\quad\mathrm{E}(\varepsilon_{i})=0,\]

\(i=1,\ldots,n\), where \(y_{i}\) is a random variable, \(x_{i}\) is a known (scalar) constant, \(f\) is an unknown continuous function, and the \(\varepsilon_{i}\)s are unobservable independent errors with \(\mathrm{Var}(\varepsilon_{i})=\sigma^{2}\). Traditionally, the errors are assumed independent, rather than just uncorrelated, to facilitate asymptotic results. In matrix form, write

\[\begin{bmatrix}y_{1}\\ \vdots\\ y_{n}\end{bmatrix}=\begin{bmatrix}f(x_{1})\\ \vdots\\ f(x_{n})\end{bmatrix}+\begin{bmatrix}\varepsilon_{1}\\ \vdots\\ \varepsilon_{n}\end{bmatrix}\]

or

\[Y=F(X)+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I,\]

where \(X\equiv(x_{1},\ldots,x_{n})^{\prime}\) and \(F(X)\equiv[f(x_{1}),\ldots,f(x_{n})]^{\prime}\). Again, for ease of exposition, we assume that \(x_{i}\in[0,1]\) for all \(i\).

Using the infinite basis representation

\[f(x)=\sum_{j=0}^{\infty}\beta_{j}\phi_{j}(x),\]

the nonparametric regression model becomes an infinite linear model,\[y_{i}=\sum_{j=0}^{\infty}\beta_{j}\phi_{j}(x_{i})+\varepsilon_{i}.\]

This is not useful because it involves an infinite sum, so we use a finite linear model approximation,

\[y_{i}=\sum_{j=0}^{s-1}\beta_{j}\phi_{j}(x_{i})+\varepsilon_{i}. \tag{1.3.1}\]

Essentially the same approximation results from a triangular array representation of \(f\). If we define \(\Phi_{j}\equiv[\phi_{j}(x_{1}),\ldots,\phi_{j}(x_{n})]^{\prime}\), in matrix terms model (1.3.1) becomes

\[Y=[\Phi_{0},\Phi_{1},\ldots,\Phi_{s-1}]\begin{bmatrix}\beta_{0}\\ \beta_{1}\\ \vdots\\ \beta_{s-1}\end{bmatrix}+e,\]

or, defining \(\Phi\equiv[\Phi_{0},\Phi_{1},\ldots,\Phi_{s-1}]\), we get

\[Y=\Phi\beta+e.\]

The linear model (1.3.1) is only an approximation, so in reality the errors will be biased. For basis functions \(\mathrm{E}(\varepsilon_{i})=\sum_{j=s}^{\infty}\beta_{j}\phi_{j}(x_{i})\). It is important to know that for \(s\) large, these bias terms are small; see Efromovich (1999, Section 2.2).

Perhaps the two most important statistical questions are how to estimate the \(\beta_{j}\)s and how to choose an appropriate value of \(s\). These issues are addressed in the next two sections.

### Estimation

Choose \(s\) so that, for all practical purposes,

\[Y=\Phi\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I. \tag{1.4.1}\]

Clearly, in this model, least squares estimates are BLUEs, so

\[\hat{\beta}=(\Phi^{\prime}\Phi)^{-1}\Phi^{\prime}Y.\]

To construct tests or confidence intervals, we would need to assume independent normal errors. The regression function is estimated by

\[\hat{f}(x)=\sum_{j=0}^{s-1}\hat{\beta}_{j}\phi_{j}(x).\]This methodology requires \(r(\Phi)\leq n\). Often the model will fit the data perfectly when \(s=n\), but this would not occur if the \(\Phi_{j}\)s are linearly dependent (i.e., if \(r(\Phi)<s\)). In the next chapter we consider alternatives to least squares estimation.

For the voltage drop data we now examine the use of several methods of nonparametric regression: fitting polynomials, cosines, Haar wavelets, and cubic splines. We begin with the most familiar of these methodologies, fitting polynomials.

#### Polynomials

Fitting high-order polynomials becomes difficult numerically unless we do something toward orthogonalizing them. We will only fit a sixth degree polynomial, so for the battery data we can get by with simply subtracting the mean before defining the polynomials. The fitted sixth degree regression is

\[\hat{y}=14.6+7.84(x-0.5)-66.3(x-0.5)^{2}-28.7(x-0.5)^{3}\\ +199(x-0.5)^{4}+10.2(x-0.5)^{5}-92(x-0.5)^{6}\]

with \(R^{2}=0.991\). The regression coefficients, ANOVA table, and sequential sums of squares are:

\begin{tabular}{l r r r r} \hline \multicolumn{6}{l}{Table of coefficients: 6th degree polynomial} \\ Predictor & \multicolumn{1}{c}{\(\hat{\beta}_{k}\)} & \multicolumn{1}{c}{\(\mathrm{SE}(\hat{\beta}_{k})\)} & \multicolumn{1}{c}{\(t\)} & \multicolumn{1}{c}{\(P\)} \\ \hline Constant & 14.6156 & 0.0901 & 162.24 & 0.000 \\ \((x-0.5)\) & 7.8385 & 0.6107 & 12.83 & 0.000 \\ \((x-0.5)^{2}\) & \(-66.259\) & 4.182 & \(-15.84\) & 0.000 \\ \((x-0.5)^{3}\) & \(-28.692\) & 9.190 & \(-3.12\) & 0.004 \\ \((x-0.5)^{4}\) & 199.03 & 43.87 & 4.54 & 0.000 \\ \((x-0.5)^{5}\) & 10.17 & 30.84 & 0.33 & 0.744 \\ \((x-0.5)^{6}\) & \(-91.6\) & 121.2 & \(-0.76\) & 0.455 \\ \hline \multicolumn{6}{l}{Analysis of variance: 6th degree polynomial} \\ Source & \multicolumn{1}{c}{\(df\)} & \multicolumn{1}{c}{\(SS\)} & \multicolumn{1}{c}{\(MS\)} & \multicolumn{1}{c}{\(F\)} & \multicolumn{1}{c}{\(P\)} \\ \hline Regression & 6 259.256 & 43.209 & 624.77 & 0.000 \\ Error & 34 & 2.351 & 0.069 & \\ \hline Total & 40 261.608 & & & \\ \hline \multicolumn{6}{l}{Source} & \multicolumn{1}{c}{\(df\)} & Seq. \(SS\) \\ \hline \((x-0.5)\) & 1 & 47.081 & & \\ \((x-0.5)^{2}\) & 1 & 170.159 & & \\ \((x-0.5)^{3}\) & 1 & 11.155 & & \\ \((x-0.5)^{4}\) & 1 & 30.815 & & \\ \((x-0.5)^{5}\) & 1 & 0.008 & & \\ \((x-0.5)^{6}\) & 1 & 0.039 & & \\ \hline \end{tabular}

From the sequential sums of squares, the \(F\) test for dropping to a fourth degree polynomial is

\[F=\frac{[0.039+0.008]/2}{0.069}<1,\]

so, refitting, we can get by with the regression equation

\[\hat{y}=14.6+7.67(x-0.5)-63.4(x-0.5)^{2}-25.7(x-0.5)^{3}+166(x-0.5)^{4},\]

which still has \(R^{2}=0.991\). The regression coefficients and ANOVA table are

\begin{tabular}{l r r r r} \hline \multicolumn{5}{l}{Table of coefficients: 4th degree polynomial} \\ Predictor & \(\beta_{k}\) & \multicolumn{1}{c}{\(\mathrm{SE}(\beta_{k})\)} & \multicolumn{1}{c}{\(t\)} & \multicolumn{1}{c}{\(P\)} \\ \hline Constant & 14.5804 & 192.64 & 0.0757 & 0.000 \\ \((x-0.5)\) & 7.6730 & 22.47 & 0.3414 & 0.000 \\ \((x-0.5)^{2}\) & \(-63.424\) & \(-34.99\) & 1.812 & 0.000 \\ \((x-0.5)^{3}\) & \(-25.737\) & \(-12.94\) & 1.989 & 0.000 \\ \((x-0.5)^{4}\) & 166.418 & 21.51 & 7.738 & 0.000 \\ \hline \multicolumn{5}{l}{Analysis of variance: 4th degree polynomial} \\ Source & \(df\) & \(SS\) & \(MS\) & \(F\) & \(P\) \\ \hline Regression & 4 259.209 & 64.802 & 972.66 & 0.000 \\ Error & 36 & 2.398 & 0.0676 & \\ \hline Total & 40 261.608 & & & \\ \hline \end{tabular} Note that the estimated regression coefficients have changed with the dropping of the fifth and sixth degree terms. Figure 1 displays the data and the fitted curve.

Polynomials fit these data very well. Other linear approximations may fit the data better or worse. What fits well depends on the particular data being analyzed.

### Cosines

For fitting cosines, define the variable \(c_{j}\equiv\cos(\pi jx)\). I arbitrarily decided to fit cosines up to \(j=30\). The fitted regression equation is

\[\hat{y}= 11.4-1.63c_{1}-3.11c_{2}+0.457c_{3}+0.216c_{4}+0.185c_{5}\] \[+0.150c_{6}+0.0055c_{7}+0.0734c_{8}+0.0726c_{9}+0.141c_{10}\] \[+0.0077c_{11}+0.0603c_{12}+0.125c_{13}+0.120c_{14}+0.0413c_{15}\] \[+0.0184c_{16}+0.0223c_{17}-0.0320c_{18}+0.0823c_{19}+0.0409c_{20}\] \[-0.0005c_{21}+0.0017c_{22}+0.0908c_{23}+0.0036c_{24}-0.0660c_{25}\] \[+0.0104c_{26}+0.0592c_{27}-0.0726c_{28}-0.0760c_{29}+0.0134c_{30}\]

with \(R^{2}=0.997\) and ANOVA table The table of regression coefficients is Table 2 and Fig. 2 displays the data and the fitted model. Note that most of the action in Table 2 takes place from \(j=0,\ldots,6\) with no other terms having \(P\) values less than \(0.05\). However, these all are tests of effects fitted last and are not generally appropriate for deciding on the smallest level of \(j\). In this case, the \(x\)s are equally spaced, so the \(c_{j}\)s are very nearly orthogonal, so a model based on \(j=0,\ldots,6\) will probably work well.

The regression equation based on only \(j=0,\ldots,6\) is

\[\hat{y}=11.4-1.61c_{1}-3.10c_{2}+0.473c_{3}+0.232c_{4}+0.201c_{5}+0.166c_{6}\]

with \(MSE=0.094=3.195/34\) and \(R^{2}=98.8\%\). Notice the slight changes in the regression coefficients relative to the first 7 terms in Table 2 due to nonorthogonality. The correlation matrix of \(c_{1}\) to \(c_{6}\) is not quite the identity:

Figure 1: Fourth-degree polynomial fit to battery data

[MISSING_PAGE_EMPTY:9338]

[MISSING_PAGE_FAIL:33]

\[\hat{y} = 11.3-1.05m_{0}-2.31m_{11}+1.78m_{12}\] \[-0.527m_{21}-1.36m_{22}+0.472m_{23}+0.814m_{24}\] \[+0.190m_{31}-0.444m_{32}-0.708m_{33}-0.430m_{34}\] \[-0.058m_{35}+0.317m_{36}+0.567m_{37}+0.071m_{38}\] \[+0.530m_{4,1}-0.181m_{4,2}-0.180m_{4,3}-0.248m_{4,4}\] \[-0.325m_{4,5}-0.331m_{4,6}-0.290m_{4,7}+0.139m_{4,8}\] \[+0.275m_{4,9}-0.131m_{4,10}+0.265m_{4,11}+0.349m_{4,12}\] \[+0.005m_{4,13}+0.229m_{4,14}+0.150m_{4,15}+0.012m_{4,16}\]

with \(R^{2}=0.957\) and ANOVA table

\[\begin{array}{l c 

[MISSING_PAGE_FAIL:35]

[MISSING_PAGE_EMPTY:9342]

[MISSING_PAGE_FAIL:37]

\(\Phi_{j}\) vectors is unlikely to occur unless the \(x_{i}\)s are chosen to be equally spaced, and, as we have seen, orthonormality of the vectors may not happen even then.

A basic orthonormal series estimation method is to take

\[\hat{\beta}_{k}=\frac{1}{n}\Phi_{k}^{\prime}Y=\frac{1}{n}\sum_{i=1}^{n}y_{i}\phi _{k}(x_{i});\]

see Hart (1997, 165), Ogden (1997, p. 108), Efromovich (1999, p. 121). This only gives the least squares estimates under the orthonormality condition. If \(n\) is very large and the \(x_{i}\)s are equally spaced, this is probably a cost effective approximation to the least squares estimate because you do not have to pay to find the inverse of \(\Phi^{\prime}\Phi\), which we know will be close to \((1/n)I\).

Another popular estimation method is to apply shrinkage estimators. Under the orthonormality assumption, Goldstein and Smith (1974) have shown that, for constants \(h_{j}\), if

\[\tilde{\beta}_{j}=h_{j}\hat{\beta}_{j}\]

and

\[\frac{\beta_{j}^{2}}{\sigma^{2}/n}<\frac{1+h_{j}}{1-h_{j}}, \tag{4.2}\]

then \(\tilde{\beta}_{j}\) is a better estimate in that

\[\mathrm{E}(\tilde{\beta}_{j}-\beta_{j})^{2}\leq\mathrm{E}(\hat{\beta}_{j}- \beta_{j})^{2}.\]

Figure 6: Cubic spline fit with 4 interior knots for the battery data

Efromovich (1999, p. 125) recommends adaptive shrinkage estimates \(\hat{\beta}_{j}\), where \(h_{j}=[(F_{j}-1)/F_{j}]_{+}\), the subscript \(+\) indicates that \(h_{j}\) is taken to be 0 if the right-hand side is negative, and \(F_{j}\) is the \(F\) statistic for testing \(H_{0}:\beta_{j}=0\), namely

\[F_{j}\equiv\frac{\hat{\beta}_{j}^{2}}{MSE/n}, \tag{4.3}\]

under orthonormality. This amounts to dropping the \(j\)th term if \(F_{j}\) is less than 1 and giving it progressively more weight up to a value of 1 as \(F_{j}\) increases. Although inequality (1.4.2) does not apply directly because \(F_{j}\) and thus \(h_{j}\) are random, this should work reasonably well. Ogden (1997, p. 124) discusses other methods of shrinkage. But again, this is all based on the assumption of orthonormality. Standard methods for shrinkage estimation in nonorthogonal linear models are principal component regression, generalized inverse regression, Bayesian regression, ridge regression, and lasso regression. The first three topics are discussed in \(PA\), the last two topics are discussed in the next chapter.

Efromovich (1999, p. 128) proposes to deal with unequally spaced \(x_{i}\) data by using the estimator

\[\hat{\beta}_{j}=\frac{1}{n}\sum_{i=1}^{n}\frac{y_{i}\phi_{j}(x_{i})}{h(x_{i}) }=\frac{1}{n}\Phi_{j}^{\prime}[D(h(X))]^{-1}Y,\]

where \(h(x)\) is the density for the randomly distributed \(x_{i}\)s. Efromovich shows that this has a nice unbiased property when integrating over both \(Y\) and \(X\); however, in model (1.4.1), the least squares estimates are superior in that they are conditionally unbiased given the \(x_{i}\)s and therefore unconditionally unbiased, besides being BLUEs. Moreover, incorporating \(h\) does not seem to deal with the collinearity caused by unequal spacings, and it requires one to know, or at least estimate, \(h\).

Another popular estimation method (see Efromovich 1999, p. 129 and Ogden 1997, pp. 43, 55) is

\[\hat{\beta}_{j}=\frac{1}{n}\sum_{i=1}^{n}y_{i}\tilde{\phi}_{j}(x_{i}),\]

where for some \(r\)

\[\tilde{\phi}_{j}(x_{i})=\frac{1}{2r}\int_{x_{i}-r}^{x_{i}+r}\phi_{j}(x)dx.\]

Obviously, the idea is to smooth out the \(\phi_{j}\) functions in the neighborhood of the observed \(x_{i}\) values. From a linear models viewpoint, all this does is change the model matrix \(\Phi\) into a new matrix that we could call \(\tilde{\Phi}\), but this substitution seems difficult to justify from the linear models viewpoint. If model (1.4.1) is appropriate, why would we want to replace the \(\phi_{j}(x_{i})\)s with \(\tilde{\phi}_{j}(x_{i})\)s?

### Variable Selection

Variable selection is of key importance in these problems because the linear model is only an approximation. The problem is to select an appropriate value of \(s\) in model (1.3.1).

In the special case where the vectors \(\frac{1}{\sqrt{n}}\Phi_{j}\) are orthonormal, the situation is analogous to identifying the important features in a \(2^{n}\) factorial design; see Christensen (1996, Sections 17.3 and 17.4) or [http://www.stat.unm.edu/~fletcher/TopicsInDesign](http://www.stat.unm.edu/~fletcher/TopicsInDesign). For example, we could begin by taking \(s=n\) and construct a normal or half-normal plot of the \(\hat{\beta}_{j}s\) to identify the important \(\phi_{j}\) functions. Similarly, we could construct a \(\chi^{2}(1)\) plot for the sequential sums of squares. Here the orthonormality condition ensures that the estimates and sums of squares are independent under normality and that the sequencing of the sequential sums of squares is irrelevant. These ideas are applied in Chap. 6 on frequency domain time series analysis in which the model matrix _is_ orthogonal. For the case against using such methods, see Lenth (2015).

Another method of choosing \(s\) is by cross-validation. For example, one can minimize the PRESS statistic; see _PA-V_ Sect. 12.5 (Christensen 2011, Section 13.5) or Hart (1997, Section 4.2.1).

An alternative to variable selection is using a penalized estimation procedure as discussed in the next chapter.

In the remainder of this section we will assume that we have fitted a model with \(s\) predictors where \(s\) was chosen to be so large that it clearly gives a reasonable approximation. We want to find a reduced model with \(p\) predictors that does not over fit the model.

Hart (1997, Section 4.2.2) and Efromovich (1999, p. 125) suggest selecting \(p\) to maximize \(A_{p}\equiv\sum_{j=0}^{p-1}(F_{j}-2)\) when using the cosine \(\phi\)s but the same idea applies whenever the \(\phi_{j}\) are ordered, e.g., polynomials. Here, the definition of \(F_{j}\) is based on (1.4.3) and orthonormality. If the \(\phi_{j}\)s are ordered, one can define \(F_{j}\) statistics more generally as sequential \(F\) tests and thus account for collinearity. In particular, redefine

\[F_{j}\equiv\frac{\mathit{SSR}(\Phi_{j}|\Phi_{0},\ldots,\Phi_{j-1})}{\mathit{ MSE}}. \tag{1.5.1}\]

Selecting \(p\) by maximizing \(A_{p}\) does not allow dropping lower-order terms if higher ones are included (i.e., it is similar, in polynomial regression, to not allowing \(x^{2}\) to be eliminated if \(x^{3}\) remains in the model). Efromovich suggests picking \(s=6\tilde{p}\), where \(\tilde{p}\) is the smallest value of \(p\) for which

\[\mathit{MSE}<2[1.48\,\mathrm{median}|y_{i}-\hat{y}_{i}|]^{2}.\]

Based on Hart's discussion of Hurvich and Tsai (1995), another crude upper bound might be \(s=\sqrt{n}\), although in practice this seems to give too small values of \(s\). Based on the coefficient of variation for the variance estimator, _PA-V_ Subsection 14.3.6 suggests that \(n-s\) should be at least 8 with values of 18 or more preferable. In Example 1.5.1, \(s\) was chosen by the seat of my pants.

[MISSING_PAGE_FAIL:41]

For the sine-cosine functions of (2.3), the \(\phi_{j}\)s occur in natural pairs having a common frequency of oscillation with the pairs ordered by their frequencies. One can easily use the \(C_{p}\) statistic to determine the largest frequency needed. If the orthonormality conditions holds, instead of finding the largest important frequency, we could identify all of the important frequencies. In Chap. 6 we will have orthogonality and periodograms are defined as mean squares for different sine-cosine pairs.

Similarly, wavelets are partially ordered, so one could decide on a level of partitioning needing in fitting wavelets. (Some regression functions are smoother in some places than others, so the required level of partitioning might vary with location.) To find an appropriate \(p\) for the Haar wavelets, we create ordered groups of \(2^{j}\) predictors. For example, test the reduced model

\[Y=[\Phi_{0},\Phi_{1},\ldots,\Phi_{7}]\begin{bmatrix}\beta_{0}\\ \beta_{1}\\ \vdots\\ \beta_{7}\end{bmatrix}+e\]

against the full model

Figure 7: Cosine fit with \(s-1=6,10,14,30\) for the battery data. Read across and down

\[Y=[\Phi_{0},\Phi_{1},\ldots,\Phi_{15}]\left[\begin{array}{c}\beta_{0}\\ \beta_{1}\\ \vdots\\ \beta_{15}\end{array}\right]+e\]

to see if the \(m_{3,k+1}(x)=\mathcal{I}_{(2k/16,(2k+1)/16]}(x)-\mathcal{I}_{((2k+1)/16,(2k+2)/ 16]}(x)\) terms, \(k=0,\ldots,2^{3}-1\), are needed at all. Recall that when simultaneously testing for the effects of a large group of predictors, it is easy for some worthwhile predictors to get overlooked (averaged out) in a small \(F\) statistic.

Example 1.5.2.  For fitting the Haar wavelets to the battery data, we have obvious groups of variables that occur in powers of 2. We can consider the highest-order group that we need, or we could consider including individual terms from any order group. In the first case, we would consider tests based on the ANOVA tables reported in Table 1.6.

To test whether we can drop the \(m_{4,k}\)s, the test statistic is

\[F=\frac{[13.568-11.118]/16}{1.235}<1.\]

To test whether we can drop the \(m_{3k}\)s, the test statistic is

\[F=\frac{[20.902-13.568]/8}{0.543}\doteq 2\]

or, using the \(MSE\) from the largest model fitted,

\[F=\frac{[20.902-13.568]/8}{1.235}<1\]

\begin{table}
\begin{tabular}{|l r r r r r|} \hline \multicolumn{6}{|c|}{Analysis of variance: fitting \(p_{0}\) to \(m_{4,16}\).} \\ Source & \(df\) & \(SS\) & \(MS\) & \(F\) & \(p\) \\ \hline Regression & 31 & 250.489 & 8.080 & 6.54 & 0.003 \\ Error & 9 & 11.118 & 1.235 & & \\ \hline \hline Total & 40 & 261.6076 & & & & \\ \hline \multicolumn{6}{|c|}{Analysis of variance: fitting \(p_{0}\) to \(m_{3,8}\).} \\ Source & \(df\) & \(SS\) & \(MS\) & \(F\) & \(p\) \\ \hline Regression & 15 & 248.040 & 16.536 & 30.47 & 0.000 \\ Residual error & 25 & 13.568 & 0.543 & & \\ \hline Total & 40 & 261.608 & & & \\ \hline \multicolumn{6}{|c|}{Analysis of variance: fitting \(p_{0}\) to \(m_{2,4}\).} \\ Source & \(df\) & \(SS\) & \(MS\) & \(F\) & \(p\) \\ \hline Regression & 7 & 240.705 & 34.386 & 54.29 & 0.000 \\ Residual error & 33 & 20.902 & 0.633 & & \\ \hline Total & 40 & 261.608 & & & \\ \hline \end{tabular}
\end{table}
Table 6: ANOVA tables for Haar waveletsIf we allow elimination of individual variables from any group, the problem becomes a traditional variable selection problem. The number of wavelets needed is related to the smoothness of \(f\), and the smoothness can change on different subsets of [0,1]. Figure 8 gives the fitted Haar wavelet curves for \(s=8\) and \(s=16\). Relative to the \(s=16\) fit, the \(s=8\) wavelets work pretty well from 0.5 to 0.625 and also from 0.875 to 1 but not very well anywhere else. 

One could even impose order on fitting splines by defining the knots as a sequence of refining partitions of [0,1], i.e., just keep adding more knots to the existing ones. Perhaps a more appealing idea would be, if you have \(m+1\) knots, add another \(m\) knots with one inside each of the previous partition sets. That is essentially what wavelets are doing only wavelets specify that the partition sets are always of equal size.

### Heteroscedastic Simple Nonparametric Regression

The heteroscedastic simple nonparametric regression model is

\[y_{i}=f(x_{i})+\sigma(x_{i})\varepsilon_{i},\]

\(i=1,\ldots,n\), where \(y_{i}\) is a random variable, \(x_{i}\) is a known (scalar) constant, \(f\) is an unknown continuous function, and the \(\varepsilon_{i}\)s are unobservable independent errors with

Figure 8: Haar wavelet fit with \(s=8,16\) for the battery data

\(\mathrm{E}(\varepsilon_{i})=0\) and \(\mathrm{Var}(\varepsilon_{i})=1\). The function \(\sigma(x)\) is assumed to be nonnegative and is often assumed to be monotone. Note that \(\mathrm{Var}(y_{i})=[\sigma(x_{i})]^{2}\).

Treating \(\sigma(\cdot)\) as known, this is simply a weighted least squares model. Let

\[\sigma^{2}\equiv[\sigma(x_{1})^{2},\ldots,\sigma(x_{n})^{2}]^{\prime} \tag{6.1}\]

and let \(D(\sigma^{2})\) be a diagonal matrix with the elements of \(\sigma^{2}\) along the diagonal. The approximate linear model is

\[Y=\Phi\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=D(\sigma^{2}).\]

Weighted least squares estimates are BLUEs, so

\[\hat{\beta}=[\Phi^{\prime}D(\sigma^{2})^{-1}\Phi]^{-1}\Phi^{\prime}D(\sigma^{ 2})^{-1}Y. \tag{6.2}\]

In orthogonal series estimation it seems to be standard practice to ignore the heteroscedastic variances in the estimation of \(\beta\); see Efromovich (1999, Section 4.3).

Typically, the function \(\sigma(x)\) will be unknown and must be estimated. Given an estimate of \(\sigma(x)\), just plug the estimate into Eqs. (1.6.1) and (1.6.2) to get estimated regression coefficients. To estimate \(\sigma(x)\), note that

\[\mathrm{E}[y_{i}-f(x_{i})]^{2}=\sigma(x_{i})^{2},\]

so if \(f\) is known, estimating \(\sigma(x_{i})^{2}\) is just a (heteroscedastic) regression problem. (For normal data, \(\mathrm{Var}([y_{i}-f(x_{i})]^{2})\) is \(3\sigma(x_{i})^{4}\).) Of course, \(f\) is not known, but we can estimate it to get \(\hat{y}_{i}\equiv\hat{f}(x_{i})\). Without assuming structure on the form of \(\sigma(x_{i})^{2}\), we can use nonparametric regression methods on the pairs \((x_{i},[y_{i}-\hat{y}_{i}]^{2})\) to estimate \(\sigma(x_{i})^{2}\). Initially, we would use ordinary least squares to get the predicted values \(\hat{y}_{i}\), but the estimate of \(\sigma(x_{i})^{2}\) will lead to a new weighted least squares estimate of \(\beta\), which leads to new predicted values \(\hat{y}_{i}\) and a new estimate of \(\sigma(x_{i})^{2}\). This process can be iterated once or iterated until the estimates of \(\sigma(x_{i})^{2}\) settle down. Carroll and Ruppert (1988) discuss how to handle heteroscedasticity in nonlinear regression models. Estimating heteroscedastic variances is a special case of estimating the covariance matrix which is the subject of Chap. 4. In particular, the results from Chap. 4 on how estimating the covariance matrix affects estimation of \(\beta\) apply.

## 7 Approximating-Functions with Small Support

Except for a finite number of 0s, the _support_ of the polynomials, sines, and cosines is the entire interval [0,1], i.e., the functions are nonzero except for a finite number of points. This can cause strange behavior when \(s\) gets close to \(n\), especially when the \(x_{i}\)s are unevenly spaced. As regards polynomials, this strange behavior is a well-known fact, but it is, perhaps, less well known as regards sines and cosines. Christensen (2015, Chapter 8) illustrates these phenomena. The fundamental idea of polynomial splines, b-splines, and wavelets is to fit functions that are nonzero only on small subsets of the domain space [0,1].

As discussed in Sect. 2, splines and b-splines both involve knots. Earlier we needed a subscript \(m\) on the knots to indicate how many knots were being used. In this section, we will drop \(m\) from the subscript when \(m\) is not subject to change.

#### Polynomial Splines

The basic idea behind using polynomial splines is to connect the dots. Suppose we have data \((x_{i},y_{i})\), \(i=1,\ldots,n\) in which the \(x_{i}\)s are ordered from smallest to largest. Linear splines quite simply give, as a regression function, the function that fits a line segment between the consecutive pairs of points. Cubic splines fit a cubic polynomial between every pair of points rather than a line. Note that all of the action here has nothing to do with fitting a model to the data. The data are being fitted perfectly (at least in this simplest form of spline fitting). The key issue is how to model what goes on between data points.

As discussed earlier, in practice, splines often do not actually connect the dots, they create smooth functions between knots. Christensen (2015, Section 8.5) illustrates the use of a linear spline, whereas here we focus on cubic splines. We introduce the topic by discussing how to connect the dots from the data smoothly and then move on to fitting with knots.

The reason for using cubic splines is to make the curve look smooth. With cubic splines we require the fitted regression function to have continuous second derivatives. Recall that if we have a linear model \(Y=XB+e\) and a constraint \(\Lambda^{\prime}\beta=0\), that together these define another linear model; see _PA_ Sect. 3.3. Depending on the nature of \(\Lambda\), this can be either a reduced model or a reparameterization. In fitting cubic splines, we will define an overparameterized saturated model based on the cubic polynomials and then incorporate nonestimable constraints so as to create a particular parameterization which is the function fitted by cubic splines. Remember that, when connecting the dots, none of this has anything to do with fitting the data, only with what the model says about places where we have no data. In the next chapter we will introduce generalized ridge regression which would create some additional smoothing of the fitted regression function so that it does not fit every data point perfectly. Alternatively, introducing \(m-1\) arbitrarily chosen knots with \(m<n\) will also further smooth the fitted regression.

The basic idea of fitting cubic splines to interpolate between data points is to fit the function

\[f(x)=a_{i}+b_{i}(x-x_{i})+c_{i}(x-x_{i})^{2}+d_{i}(x-x_{i})^{3},\quad x\in[x_{ i},x_{i+1}].\]

Note that \(f\) is defined in two distinct ways at all \(x_{i}\), except the first and last, that is, for \(i=2,\ldots,n-1\)

\[f(x_{i}) = a_{i}+b_{i}(x_{i}-x_{i})+c_{i}(x_{i}-x_{i})^{2}+d_{i}(x_{i}-x_{i })^{3}=a_{i}\] \[= a_{i-1}+b_{i-1}(x_{i}-x_{i-1})+c_{i-1}(x_{i}-x_{i-1})^{2}+d_{i-1 }(x_{i}-x_{i-1})^{3}.\]For \(f\) to be continuous, indeed for \(f\) to be a function, these must be equal, so we are led to the constraints

\[a_{i}=a_{i-1}+b_{i-1}(x_{i}-x_{i-1})+c_{i-1}(x_{i}-x_{i-1})^{2}+d_{i-1}(x_{i}-x_{ i-1})^{3},\]

\(i=2,\ldots,n-1\). Similarly, we want the right first and second derivatives of \(f\) to be equal to the left first and second derivatives of \(f\) at every interior point, which leads to the constraints

\[b_{i}=b_{i-1}+2c_{i-1}(x_{i}-x_{i-1})+3d_{i-1}(x_{i}-x_{i-1})^{2},\quad i=2, \ldots,n-1\]

and

\[c_{i}=c_{i-1}+3d_{i-1}(x_{i}-x_{i-1}),\quad i=2,\ldots,n-1.\]

Altogether, for \(n\) data points, we have \(n-1\) cubic polynomials being fitted, so we have \(4(n-1)\) parameters being fitted to \(n\) data points. The model is somewhat overparameterized. The constraints for continuity and first and second derivatives give us \(3(n-2)\) constraints, leaving us with \(4(n-1)-3(n-2)=n+2\) free parameters to fit to \(n\) data points. We need two more constraints if we are going to get a unique set of parameter estimates. These two constraints are that the second derivative should be \(0\) at \(x_{1}\) and \(x_{n}\)--two points that are not involved in the previous continuity and derivative constraints. The additional constraints reduce to

\[0=c_{1}=c_{n-1}+d_{n-1}(x_{n}-x_{n-1}).\]

We will illustrate the linear modeling ideas for the case of \(n=4\). We can write a linear model \(Y=X\beta+e\) for fitting the cubic splines by taking

\[\beta=(a_{1},b_{1},c_{1},d_{1},a_{2},b_{2},c_{2},d_{2},a_{3},b_{3},c_{3},d_{3}) ^{\prime} \tag{1.7.1}\]

and the transpose of \(X\) as

\[X^{\prime}=\begin{bmatrix}1&1&0&0\\ 0&(x_{2}-x_{1})&0&0\\ 0&(x_{2}-x_{1})^{2}&0&0\\ 0&(x_{2}-x_{1})^{3}&0&0\\ 0&0&1&0\\ 0&0&(x_{3}-x_{2})&0\\ 0&0&(x_{3}-x_{2})^{2}&0\\ 0&0&(x_{3}-x_{2})^{3}&0\\ 0&0&0&1\\ 0&0&0&(x_{4}-x_{3})\\ 0&0&0&(x_{4}-x_{3})^{2}\\ 0&0&0&(x_{4}-x_{3})^{3}\end{bmatrix}.\]

Defining \(t_{2}\equiv(x_{2}-x_{1})\), \(t_{3}\equiv(x_{3}-x_{2})\), \(t_{4}\equiv(x_{4}-x_{3})\), and \[\Lambda\equiv\begin{bmatrix}1&0&0&0&0&0&0&0\\ t_{2}&1&0&0&0&0&0&0\\ t_{3}^{2}&2t_{2}&1&0&0&0&1&0\\ t_{3}^{2}&3t_{2}^{2}&3t_{2}&0&0&0&0&0\\ -1&0&0&1&0&0&0\\ 0&-1&0&t_{3}&1&0&0&0\\ 0&0&-1&t_{2}^{2}&2t_{3}&1&0&0\\ 0&0&0&t_{3}^{3}&3t_{3}^{2}&3t_{3}&0&0\\ 0&0&0&-1&0&0&0&0\\ 0&0&0&0&-1&0&0&0\\ 0&0&0&0&0&-1&0&1\\ 0&0&0&0&0&0&0&3t_{4}\end{bmatrix},\]

the constraints are \(\Lambda^{\prime}\beta=0\).

If, as in \(PA\) Sect. 3.3, we find a matrix \(U\) such that \(C(U)=C(\Lambda)^{\perp}\) and \(X_{0}=XU\), then the linear model \(Y=X_{0}\gamma+e\) has the constraint \(\Lambda^{\prime}\beta=0\) built into it. The constrained least squares estimate of \(\beta\) is \(\hat{\beta}=U\hat{\gamma}\). Obviously, from the choice of \(U\), \(\Lambda^{\prime}\hat{\beta}=\Lambda^{\prime}U\hat{\gamma}=0\). Because our spline model is saturated, \(X_{0}\) will be nonsingular and we can obtain the estimates \(\hat{\gamma}=(X_{0}^{\prime}X_{0})^{-1}X_{0}^{\prime}Y=X_{0}^{-1}Y\) and then \(\hat{\beta}=U\hat{\gamma}=U(XU)^{-1}Y\).

We have used cubic splines to _interpolate_ between the observed data points. An alternative is that we could (a) have some fixed set of \(m+1\) knots \(0=\tilde{x}_{0}<\tilde{x}_{1}<\tilde{x}_{2}<\cdots<\tilde{x}_{m}=1\) as endpoints for the cubic polynomials, (b) observe data \((x_{i},y_{i})\), \(i=1,\ldots,n\), and (c) fit low order polynomials to the data between the knots. As such, each fitted polynomial has support only between the knots. As with interpolation, we require continuity of the fitted function and, depending on the order of the polynomial being fitted, perhaps require some continuous derivatives. The discussion so far has used \(m-1=n\) and \(\tilde{x}_{k}=x_{k}\) but has ignored fitting polynomials on \([0,x_{1}]\) and \([x_{n},1]\).

Using knots, consider fitting \(m\) cubic polynomials. For \(j=1\ldots,m\)

\[f(x)=a_{j}+b_{j}(x-\tilde{x}_{j-1})+c_{j}(x-\tilde{x}_{j-1})^{2}+d_{j}(x- \tilde{x}_{j-1})^{3},\quad x\in[\tilde{x}_{j-1},\tilde{x}_{j}]\]

so we have \(4m\) parameters. The \(3(m-1)\) constraints for continuity and equality of the right and left first and second derivatives at the \(m-1\) interior knots leave us \(m+3\) free parameters. We _do not_ require that the second derivatives be zero at the boundaries \(x=0,1\). To illustrate, for \(m=3\), the constraints are \(\Lambda^{\prime}\beta=0\) where \(\beta\) is as in Eq. (1.7.1) and

\[\Lambda\equiv\begin{bmatrix}1&0&0&0&0&0&0\\ t_{1}&1&0&0&0&0&0\\ t_{1}^{2}&2t_{1}&1&0&0&0&0\\ t_{1}^{3}&3t_{1}^{2}&3t_{1}&0&0&0\\ -1&0&0&1&0&0\\ 0&-1&0&t_{2}&1&0\\ 0&0&-1&t_{2}^{2}&2t_{2}&1\\ 0&0&0&t_{3}^{2}&3t_{2}^{2}&3t_{2}\\ 0&0&0&-1&0&0\\ 0&0&0&0&-1&0\\ 0&0&0&0&0&-1\\ 0&0&0&0&0&-1\\ 0&0&0&0&0&0\end{bmatrix},\]where \(t_{j}=\tilde{x}_{j}-\tilde{x}_{j-1}\). In general, the cubic spline regression model is

\[y_{i}=f(x_{i})+\epsilon_{i},\]

where, if \(x_{i}\in[\tilde{x}_{j-1},\tilde{x}_{j}]\),

\[y_{i}=a_{j}+b_{j}(x_{i}-\tilde{x}_{j-1})+c_{j}(x_{i}-\tilde{x}_{j-1})^{2}+d_{j }(x_{i}-\tilde{x}_{j-1})^{3}+\epsilon_{i}.\]

For us to be able to estimate all of the parameters, we have a necessary condition of \(n\geq m+3\). But even this is not sufficient for estimability of all parameters. It is not hard to see that if two adjacent intervals \([\tilde{x}_{j},\tilde{x}_{j+1}]\) both contain no data, the spline function will not be estimable. In particular, if \(r(X_{0})<m+3\), we are in trouble. Moreover, we can get bad fitting models by having \(m+3\doteq n\). For example, fitting a cubic polynomial to only 4 data points can lead to strange results. Even with \(m-1=30\) we see some undesirable behavior in Fig. 5 between \(x_{1}=0\) and \(x_{2}=1/40\). The behavior of the spline fit to the battery data gets worse as \(m-1\) increases above 30.

In Sect. 1.4 we claimed that the cubic spline model can be conveniently written as

\[y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\beta_{3}x^{3}+\sum_{j=1}^{m -1}\beta_{j+3}[(x_{i}-\tilde{x}_{j})_{+}]^{3}+\epsilon_{i}.\]

This has precisely the \(m+3\) estimable parameters that we need. Christensen (2015, Exercise 8.7.8) and Sect. 1.7.1.2 both examine the relationship between this model and the constrained model discussed above. The equivalence relies on the fact that having separate polynomials between pairs of knots is equivalent to adding a new polynomial that begins at each knot, i.e.,

\[f(x)=\sum_{j=1}^{m}\left[\alpha_{j}+\eta_{j}(x-\tilde{x}_{j-1})+\gamma_{j}(x- \tilde{x}_{j-1})^{2}+\delta_{j}(x-\tilde{x}_{j-1})^{3}\right]\mathcal{I}_{[ \tilde{x}_{j-1},1]}(x),\]

and showing that imposing the continuity and derivative constraints requires \(0=\alpha_{j}=\eta_{j}=\gamma_{j}\), \(j=2,\ldots,m\). Under these conditions \(f\) reduces to

\[f(x)=\alpha_{1}+\eta_{1}x+\gamma_{1}x^{2}+\delta_{1}x^{3}+\sum_{j=2}^{m} \delta_{j}(x-\tilde{x}_{j-1})^{3}\mathcal{I}_{[\tilde{x}_{j-1},1]}(x),\]

which is equivalent to the regression function in the model written with \(\beta_{j}\)s.

In general you can write a \(d\) order spline model as

\[y_{i}=\sum_{k=0}^{d}\beta_{k}x^{k}+\sum_{j=1}^{m-1}\beta_{j+d}[(x_{i}-\tilde{x }_{j})_{+}]^{d}+\epsilon_{i}. \tag{1.7.2}\]

#### B-Splines

B-splines are supposed to be _basis splines_ but, as we have seen earlier, they do not actually define a meaningful basis in the space of functions. (Obviously they define a basis for their spanning space.) _B-splines provide the same fit as splines_ but do so by defining a mother spline and then defining \(\phi_{j}\) functions by rescaling and relocating the mother.

The mother spline is itself a low order polynomial spline. The mother spline of degree 2 is nonzero over \((0,3)\) and defined as

\[\Psi_{2}(x)=\frac{x^{2}}{2}\mathcal{I}_{[0,1]}(x)-\left\{[x-1.5]^{2}-0.75\right\} \mathcal{I}_{(1,2]}(x)+\frac{[3-x]^{2}}{2}\mathcal{I}_{(2,3]}(x).\]

This is a bell-shaped curve, similar to a normal density centered at 1.5, but it is 0 outside the interval \([0,3]\) while still being smooth in that it is differentiable everywhere. \(\Psi_{2}\) is itself a quadratic spline, i.e., quadratics have been pasted together as a smooth function.

A mother spline \(\Psi_{d}^{\prime}\) of degree \(d\) has support on the interval \((0,d+1)\). It splices together \((d+1)\) different \(d\)-degree polynomials, each defined on a length 1 interval, so that the whole function is differentiable \(d-1\) times and looks like a mean-shifted Gaussian density. Commonly \(d\) is either 2 or 3. For \(d=3\), the cubic mother spline on [0,4] is

\[\Psi_{3}(x)=\frac{x^{3}}{3}\mathcal{I}_{[0,1]}(x)+\left\{-x^{3}+4 x^{2}-4x+\frac{4}{3}\right\}\mathcal{I}_{(1,2]}(x)\\ +\left\{-[4-x]^{3}+4[4-x]^{2}-4[4-x]+\frac{4}{3}\right\}\mathcal{ I}_{(2,3]}(x)+\frac{[4-x]^{3}}{3}\mathcal{I}_{(3,4]}(x).\]

Figure 1.9 shows these b-spline mother functions. Other than the domain on which they are defined, they look quite unremarkable. There is a body of theory associated with b-splines that includes defining the \(d+1\) order mother spline recursively from the \(d\) order mother.

The approximating functions \(\phi_{j}(x)\) are defined by rescaling and relocating the mother splines. For simplicity, consider \(d=2\) with \(m-1\) equally spaced interior knots. If the knots are equally spaced, the same rescaling of \(\Psi_{2}\) works for all \(\phi_{j}\). \(\Psi_{2}\) is defined on [0,3] and pastes together 3 polynomials on three intervals of length one. To define \(\phi_{0}\) we rescale \(\Psi_{2}\) to live on \([0,3/m]\) and then shift it to the left \(2/m\) units so that only the polynomial originally defined on [2,3] now overlaps the interval \([0,1/m]\) and \(\phi_{0}\) is 0 elsewhere in [0,1]. To define \(\phi_{1}\), again rescale \(\Psi_{2}\) to live on \([0,3/m]\) but now shift it to the left only \(1/m\) units so that the polynomial originally defined on [2,3] now overlaps the interval \([1/m,2/m]\) and the polynomial originally defined on [1,2] now overlaps the interval \([0/m,1/m]\). \(\phi_{1}\) is 0 elsewhere in [0,1]. \(\phi_{2}\) is just the rescaled version of \(\Psi_{2}\). \(\phi_{3}\) is the rescaled \(\Psi_{2}\) shifted to the _right_ by \(1/m\). More generally, \(\phi_{2+j}\) is the rescaled \(\Psi_{2}\) shifted to the _right_ by \(j/m\).

For arbitrary \(d\), \(\Psi_{d}\) is rescaled so that its support is \((0,\{d+1\}/m)\) and \(\phi_{0}\) is the rescaled \(\Psi_{d}\) shifted to the left \(d/m\) units. Each successive \(\phi_{j}\) is shifted to the right by an additional \(1/m\) so that \(\phi_{d}\) is the rescaled version of \(\Psi_{d}\) and \(\phi_{d+j}\) is the rescaled \(\Psi_{d}\) shifted to the right by \(j/m\).

For a quadratic spline model with \(m-1\) equally spaced interior knots,

\[y_{i}=f(x_{i})+\varepsilon_{i},\]

where, if \(x_{i}\in[(j-1)/m,j/m]\),

\[y_{i}=a_{j}+b_{j}\{x_{i}-(j-1)/m\}+c_{j}\{x_{i}-(j-1)/m\}^{2}+\varepsilon_{i}\]

More generally, splines fit a \(d\) dimensional polynomial between the knots with continuity of the function and of the \(d-1\) order derivatives at each interior knot. As mentioned earlier and shown later, this is equivalent to the model

\[y_{i}=\sum_{k=0}^{d}\beta_{k}x^{k}+\sum_{j=1}^{m-1}\beta_{j+d}[(x_{i}-\tilde{x }_{j})_{+}]^{d}+\varepsilon_{i}\]

which involves \(m+d\) parameters. The equivalent b-spline model is

Figure 9: B-spline mother functions for \(d=2,3\)

\[y_{i}=\sum_{j=0}^{m+d-1}\gamma_{j}\,\phi_{j}(x_{i})+\varepsilon_{i}\]

where

\[\phi_{j}(x)=\Psi_{d}\left(mx-j+d\right).\]

Example 1.7.1: The computer code for this book on my website provides programs for fitting both splines and b-splines to the battery data and illustrates that the two approaches are equivalent. Notably, they give the same fitted values, Error degrees of freedom, and Error sums of squares. The code is currently written only for using \(\Psi_{2}\) and \(\Psi_{3}\). \(\Box\)

Pedagogically, I do not see why anyone would use b-splines rather than fitting model (1.7.2) but Eilers and Marx (1996) argue that B-splines have numerical advantages. Christensen, Johnson, Branscum, and Hanson (2010, Subsection 15.2) contains a discussion of b-splines in a Bayesian setting.

#### Equivalence of Spline Methods*

We now provide a more formal discussion of why our three different approaches to fitting splines are equivalent.

The idea of fitting splines is that the regression function \(f\) is a general \(d\) order polynomial between the knots but with the restrictions that the regression function be continuous at all interior knots, as are the derivatives of order up to \(d-1\). For the purpose of discussing equivalence we will break up \(f\) at the knots, i.e.,

\[f_{j}(x)=f(x)\mathcal{I}_{[\bar{x}_{j-1},\bar{x}_{j}]}(x),\qquad j=1,\ldots,m.\]

It is convenient to define \(f_{j}\) on closed intervals so we can discuss the value of the function at the end knots rather than the function's limit. The spline function is \(f(x)=\sum_{j=1}^{m}f_{j}(x)\)_except at the interior knots_ where \(\sum_{j=1}^{m}f_{j}(\bar{x}_{k})=2f(\bar{x}_{k})\), \(k=1,\ldots,m-1\)

We change our basic polynomial spline notation for \(f\) in this discussion. The basic definition of the spline function will be the arbitrary polynomial

\[f_{j}(x)=\left[\sum_{k=0}^{d}\beta_{k,j}(x-\bar{x}_{j-1})^{k}\right]\mathcal{I }_{[\bar{x}_{j-1},\bar{x}_{j}]}(x),\qquad j=1,\ldots,m\]

but these polynomials are subject to the continuity and differentiability constraints

\[f_{j}(\bar{x}_{j})=f_{j+1}(\bar{x}_{j}),\qquad j=1,\ldots,m-1\]

and

\[\mathbf{d}^{k}f_{j}(\bar{x}_{j})=\mathbf{d}^{k}f_{j+1}(\bar{x}_{j}),\qquad k= 1,\ldots,d-1,\quad j=1,\ldots,m-1\]where \(\mathbf{d}^{k}\) denotes the \(k\)th order left or right derivative as is appropriate, evaluated at the indicated point.

To establish equivalences between this and the other approaches to fitting splines, we provide alternative inductive definitions of \(f_{j}\) that satisfy the general criterion of fitting an arbitrary polynomial between the knots subject to constraints and establish how these lead to Eq. (1.7.2) and to b-splines. In these inductive definitions, \(f_{1}\) always needs to be an arbitrary polynomial of order \(d\), i.e.,

\[f_{1}(x)\equiv\left[\sum_{k=0}^{d}\beta_{k,1}(x-\tilde{x}_{0})^{k}\right] \mathcal{I}_{[\tilde{x}_{0},\tilde{x}_{1}]}(x).\]

Then, assuming that \(f_{j}\) is defined appropriately, we define \(f_{j+1}\) so that it is an arbitrary \(d\) order polynomial subject to the continuity and differentiation constraints and also takes the form of either Eq. (1.7.2) or b-splines.

We begin with establishing (1.7.2). The idea is to take \(f_{j+1}=f_{j}+p_{j+1}\) where \(p_{j+1}\) is an arbitrary polynomial of order \(d\) and show that the continuity and differentiability constraints lead to adding a term like the one used in (1.7.2). The technicalities get a little more complex. What we really do is extend the polynomial from \(f_{j}\) onto the new domain and add a new arbitrary polynomial, i.e.,

\[f_{j+1}(x)=\mathcal{I}_{[\tilde{x}_{j},\tilde{x}_{j+1}]}(x)\left\{\left[\sum_{ k=0}^{d}\beta_{k,j}(x-\tilde{x}_{j-1})^{k}\right]+\left[\sum_{k=0}^{d}\beta_{k,j+ 1}(x-\tilde{x}_{j})^{k}\right]\right\}.\]

To get \(f_{j}(\tilde{x}_{j})=f_{j+1}(\tilde{x}_{j})\), we need \(\beta_{0,j+1}=0\). Similarly, to get \(\mathbf{d}^{k}f_{j}(\tilde{x}_{j})=\mathbf{d}^{k}f_{j+1}(\tilde{x}_{j})\), \(k=1,\ldots,d-1\), we need \(\beta_{k,j+1}=0\), \(k=1,\ldots,d-1\). So after incorporating the constraints,

\[f_{j+1}(x)=\mathcal{I}_{[\tilde{x}_{j},\tilde{x}_{j+1}]}(x)\left\{\left[\sum_{ k=0}^{d}\beta_{k,j}(x-\tilde{x}_{j-1})^{k}\right]+\beta_{d,j+1}(x-\tilde{x}_{j} )^{d}\right\}.\]

It is now easy to see that if you define \(f\) from model (1.7.2) you get exactly the necessary structure. Model (1.7.2) defines some polynomial on \([\tilde{x}_{j-1},\tilde{x}_{j}]\) and any polynomial can be written in the form indicated for \(f_{j}\). Plus, model (1.7.2) extends the \(f_{j}\) polynomial to \([\tilde{x}_{j},\tilde{x}_{j+1}]\) and makes the necessary addition.

Alas, the argument for b-splines is considerably more complicated. Before doing it for general \(d\) we examine the case \(d=2\). Write second degree polynomials \(p_{1}(x)\equiv x^{2}/2\), \(p_{2}(x)\equiv-[(x-.5)^{2}-0.75]\), and \(p_{3}(x)\equiv(1-x)^{2}/2=p_{1}(1-x)\). Our interest is in how these polynomials behave on [0,1]. Define the mother spline in terms of these polynomials.

\[\Psi_{2}(x)=p_{1}(x)\mathcal{I}_{[0,1]}(x)+p_{2}(x-1)\mathcal{I}_{(0,1]}(x-1)+ p_{3}(x-2)\mathcal{I}_{(0,1]}(x-2).\]

Note that continuity of \(\Psi_{2}\) derives from the facts that

\[p_{1}(1)=p_{2}(0);\quad p_{2}(1)=p_{3}(0).\]

[MISSING_PAGE_FAIL:54]

to equal

\[\mathbf{d}^{1}f_{*}(1/m) = \gamma_{1}\mathbf{d}^{1}p_{3}(0)+\gamma_{2}\mathbf{d}^{1}p_{2}(0)+ \beta_{1,*}+2\beta_{2,*}(0)\] \[= \gamma_{1}\mathbf{d}^{1}p_{3}(0)+\gamma_{2}\mathbf{d}^{1}p_{2}(0)+ \beta_{1,*},\]

so we must have \(\beta_{1,*}=0\). Thus, in the arbitrary polynomial the constraints imposed reduce it to \(\beta_{2,*}(x-1/m)^{2}\) where \(\beta_{2,*}\) is an arbitrary parameter. Overall we get

\[f_{*}(x) = \gamma_{1}p_{3}(mx-1)+\gamma_{2}p_{2}(mx-1)+\beta_{2,*}(x-1/m)^{2}\] \[= \gamma_{1}p_{3}(mx-1)+\gamma_{2}p_{2}(mx-1)+\gamma_{3}p_{1}(mx-1) ^{2}=f_{2}(x)\]

after we relabel the free parameter \(2\beta_{2,*}/m^{2}\) as \(\gamma_{3}\).

The same ideas work in general. For a \(d\) order b-spline define the mother spline in terms of \(d\) order component polynomials \(p_{1},p_{2},\ldots,p_{d+1}\) where \(p_{1}(x)\propto x^{d}\), with \(p_{d+1}(x)=p_{1}(1-x)\) so that \(0=p_{d+1}(1)=\mathbf{d}^{1}p_{d+1}(1)=\cdots=\mathbf{d}^{d-1}p_{d+1}(1)\), and the other \(p_{j}\)s are defined so that the mother spline is

\[\Psi_{d}(x)=p_{1}(x)\mathcal{I}_{[0,1]}(x)+\sum_{k=2}^{d+1}p_{k}(x-k+1)\mathcal{ I}_{(0,1]}(x-k+1).\]

It is an exercise to figure out how to write the \(p_{j}\)s for \(d=3\).

A key feature is that \(\Psi_{d}\) is itself a spline so

\[p_{j}(1)=p_{j+1}(0),\qquad j=1,\ldots,d\]

and

\[\mathbf{d}^{k}p_{j}(1)=\mathbf{d}^{k}p_{j+1}(0),\qquad k=1,\ldots,d-1,\quad j= 1,\ldots,d.\]

In b-splines the regression function is

\[f(x)=\sum_{j=0}^{m+d-1}\gamma_{j}\,\phi_{j}(x)\]

where

\[\phi_{j}(x)=\Psi_{d}\left(mx-j+d\right).\]

In particular, between the first two knots

\[f_{1}(x)=\mathcal{I}_{[0,1]}(mx)\sum_{k=0}^{d}\gamma_{k}\,p_{d+1-k}(mx).\]

The \(p_{k}\)s are linearly independent polynomials so for arbitrary \(\gamma_{j}\)s, \(f_{1}\) is an arbitrary \(d\) order polynomial as we need it to be for our induction. According the b-spline model,

\[f_{j}(x)=\mathcal{I}_{[0,1]}(mx-j+1)\sum_{k=0}^{d}\gamma_{k+j-1}\,p_{d+1-k}(mx -j+1).\]The idea is to take

\[f_{*}(x)=\mathcal{I}_{[0,1]}(mx-j)\left[p_{*}(x)+\sum_{k=0}^{d-1}\mathcal{V}_{k+j} p_{d+1-k}(mx-j)\right].\]

where \(p_{*}\) is an arbitrary polynomial of order \(d\) but the \(\gamma\)s are determined by \(f_{j}\) and show that the continuity and differentiability constraints lead \(f_{*}\) to agree with the definition of \(f_{j+1}\) from the b-spline model. Write \(p_{*}(x)=\sum_{k=0}^{d}\beta_{k,*}(mx-j)^{k}\) so that

\[\begin{split}& f_{*}(x)=\\ &\mathcal{I}_{[0,1]}(mx-j)\left\{\left[\sum_{k=0}^{d}\beta_{k,*}( mx-j)^{k}\right]+\left[\sum_{k=0}^{d-1}\mathcal{V}_{k+j}p_{d+1-k}(mx-j)\right] \right\}.\end{split} \tag{1.7.3}\]

Continuity requires

\[f_{j}(j/m)=f_{*}(j/m).\]

Using the fact that \(p_{d+1}(1)=0\) and the spline properties of \(\Psi_{d}\)

\[\begin{split} f_{j}(j/m)&=\sum_{k=0}^{d}\gamma_{k+ j-1}\,p_{d+1-k}(1)\\ &=0+\sum_{k=1}^{d}\gamma_{k+j-1}\,p_{d+1-k}(1)\\ &=\sum_{k=0}^{d-1}\gamma_{k+j}\,p_{d-k}(1)\\ &=\sum_{k=0}^{d-1}\gamma_{k+j}\,p_{d+1-k}(0).\end{split}\]

From (1.7.3) it follows that

\[f_{*}(j/m)=\left[\beta_{0,*}+\sum_{k=1}^{d}\beta_{k,*}0^{k}\right]+\left[\sum_ {k=0}^{d-1}\gamma_{k+j}\,p_{d+1-k}(0)\right],\]

so to achieve continuity we must have \(\beta_{0,*}=0\). Similar arguments for the requirement of continuous derivatives force \(0=\beta_{1,*}=\dots=\beta_{d-1,*}\). Incorporating these requirements, recalling that \(p_{1}(x)\) is proportional to \(x^{d}\), and replacing the arbitrary parameter \(\beta_{d,*}\) with an arbitrary \(\gamma_{d+j}\) that incorporates the proportionality constant, we get

\[\begin{split} f_{*}(x)&=\mathcal{I}_{[0,1]}(mx-j) \left\{\beta_{d,*}(mx-j)^{d}+\sum_{k=0}^{d-1}\gamma_{k+j}\,p_{d+1-k}(mx-j) \right\}\\ &=\mathcal{I}_{[0,1]}(mx-j)\left\{\gamma_{d+j}p_{1}(mx-j)+\sum_{k =0}^{d-1}\gamma_{k+j}\,p_{d+1-k}(mx-j)\right\}\end{split}\]\[=\mathcal{I}_{[0,1]}(mx-j)\sum_{k=0}^{d}\gamma_{k+j}\,P_{d+1-k}(mx-j)=f_{j+1}(x).\]

Exercise 1.2: Show, for \(d=2\) b-splines, that \(\sum_{k=0}^{m+1}\phi_{k}(x_{i})\) is a constant. Hint: Recall that exactly three nonzero \(\phi_{j}\) functions overlap on every subinterval and argue that it is enough to show that for \(x\in[0,1]\) the function \(x^{2}/2-[(1+x)-1.5]^{2}+[3-(2+x)]^{2}/2\) is a constant.

#### Fitting Local Functions

In discussing b-splines for \(m+1\) equally spaced knots we carefully defined \(\phi_{j}\equiv\phi_{jm}\) functions so that they were equivalent to fitting polynomial splines. But in another sense, we merely fitted a bunch of bell shaped curves that were rescaled to have small support and shifted so that the functions had centers that were spread over the entire unit interval.

Just about any function \(\Psi\) that goes to \(0\) as \(|x|\to\infty\) can be used as a "mother" to define a triangular array \(\phi_{jm}\) of approximating-functions with small (practical) support. These include indicator functions, mother splines, mother wavelets, normal densities, etc. Given a set of knots \(\tilde{x}_{j,m}\), take \(s-1=m\) with \(\phi_{jm}\) a rescaled mother function with a location tied to (often centered at) \(\tilde{x}_{j,m}\). The success of this enterprize will depend on the number and placement of the knots and how the mother function is rescaled. The process becomes a method based on approximating functions with small support when the mother function is rescaled in such a way that it becomes, for all practical purposes, \(0\) outside of a small interval.

#### Local Regression

_Local (polynomial) regression_, often referred to as _loess_ or _lowess_ (_local_ weighted _s_catterplot _s_moothing) provides fitted values by fitting a separate low order polynomial for every prediction. It provides a collection of \((x,\hat{y})\) values that can be plotted, but it does not provide a formula for the estimated regression curve. As with splines, we assume that in the data \((x_{i},y_{i})\), the \(x_{i}\)s are ordered.

The key to local regression is that it uses weighted regression with weights determined by the distance between the actual data \(x_{i}\) and the location being fitted \(x\). What makes this "local" regression is that the weights are either zero, or close to zero, outside a small region around \(x\). The weights are determined by a _kernel_ function (not to be confused with the reproducing kernels introduced in Chap. 3). (I jokingly once proposed such a function to my son who has always referred to it as Pop's corny kernel.)Originally, this procedure was performed using 0 order polynomials and is known as _kernel smoothing_, see Green and Silverman (1994) or Efromovich (1999). The idea of kernel smoothing is to base estimation on the continuity of \(f(x)\). The estimate \(\hat{f}(x)\) is a weighted average of the \(y_{i}\) values in a small neighborhood of \(x\). Less weight is given to a \(y_{i}\) for which the corresponding \(x_{i}\) is far from \(x\). The weights are defined by a nonnegative kernel function \(K(z)\) that gets small rapidly as \(z\) gets away from 0. The _Nadaraya-Watson kernel estimate_ is

\[\hat{f}(x)=\sum_{i=1}^{n}y_{i}K[(x-x_{i})]\bigg{/}\sum_{i=1}^{n}K[(x-x_{i})],\]

which is just a weighted average, as advertised.

More generally, take a low order polynomial model, say,

\[Y=X\beta+0,\quad\mathrm{E}(e)=0\]

on which we perform generalized least squares with a diagonal weighting matrix \(D(w)\) having some vector of weights \(w\) down the diagonal. The definition of the weights is all-important. The \(i\)th element of \(w\) is

\[w_{i}\equiv K\left[(x_{i}-x)/h\right]\]

for some scalar tuning parameter \(h\) and kernel function \(K\). From fitting this model we obtain only one thing, the fitted value \(\hat{y}\) for the new data point \(x\). You do this for a lot of \(x\)s and plot the result. Obviously, fitting a separate linear model for every fitted value requires modern computing power.

In loess the most commonly used weighting seems to be the _tri-weight_ where the kernel function is

\[K(z)=\left\{\begin{array}{ll}(1-|z|^{3})^{3}&\mbox{if $|z|<1$}\\ 0&\mbox{if $|z|\geq 1$}.\end{array}\right.\]

In R the default is to fit a quadratic polynomial.

For the battery data the default loess fit in R seems to me to oversmooth the data. It gives \(R^{2}=0.962\). Code for fitting and plotting it is included with the other code on my website.

### Nonparametric Multiple Regression

Nonparametric multiple regression involves using a \(p\) vector \(x\) as the argument for \(\phi_{j}(\cdot)\) in an infinite sum or \(\phi_{jm}(\cdot)\) in a triangular array. The difficulty is in choosing which \(\phi\) functions to use. There are two common approaches. One is to construct the vector functions \(\phi\) from the scalar \(\phi\) functions already discussed. The other method uses the _kernel trick_ to replace explicit consideration of the \(\phi\) functions with evaluation of a _reproducing kernel_ function.

#### Redefining \(\phi\) and the Curse of Dimensionality

In nonparametric multiple regression, the scalars \(x_{i}\) are replaced by vectors \(x_{i}=(x_{i1},x_{i2},\ldots,x_{ip})^{\prime}\). Theoretically, the only real complication is that the \(\phi_{j}\) functions have to be redefined as functions of vectors rather than scalars.

In practice, we often construct vector \(\phi\) functions from scalar \(\phi\) functions. The ideas become clear in the case of \(p=2\). For variables \(x_{1}\) and \(x_{2}\), define

\[\phi_{jk}(x_{1},x_{2})\equiv\phi_{j}(x_{1})\phi_{k}(x_{2}),\]

and the regression function approximation is

\[f(x_{1},x_{2})\doteq\sum_{j=0}^{s_{1}-1}\sum_{k=0}^{s_{2}-1}\beta_{jk}\phi_{jk} (x_{1},x_{2}). \tag{1.8.1}\]

In general, for \(x=(x_{1},\ldots,x_{p})^{\prime}\),

\[f(x)\doteq\sum_{k_{1}=0}^{s_{1}-1}\cdots\sum_{k_{p}=0}^{s_{p}-1}\beta_{k_{1} \ldots k_{p}}\phi_{k_{1}}(x_{1})\cdots\phi_{k_{p}}(x_{p}), \tag{1.8.2}\]

where most often \(\phi_{0}\equiv 1\).

Note that there are a lot of \(\phi\) functions involved. For example, if we needed \(s_{1}=10\) functions to approximate a function in \(x_{1}\) and \(s_{2}=8\) functions to approximate a function in \(x_{2}\), it takes 80 functions to approximate a function in \((x_{1},x_{2})\), and this is a very simple case. It is not uncommon to have \(p=5\) or more. If we need \(s_{*}=8\) for each dimension, we are talking about fitting \(s=8^{5}=32,768\) parameters for a very moderately sized problem. Clearly, this approach to nonparametric multiple regression is only practical for very large data sets if \(p>2\). However, nonparametric multiple regression seems to be a reasonable approach for \(p=2\) with moderately large amounts of data, such as are often found in problems such as two-dimensional image reconstruction and smoothing two-dimensional spatial data. Another way to think of the dimensionality problems is that, roughly, if we need \(n\) observations to do a good job of estimation with one predictor, we might expect to need \(n^{2}\) observations to do a good job with two predictors and \(n^{p}\) observations to do a good job with \(p\) predictors. For example, if we needed 40 observations to get a good fit in one dimension, and we have \(p=5\) predictors, we need about 100 million observations. (An intercept can be included as either \(\phi_{0}\equiv 1\) or \(x_{i1}\equiv 1\). In the latter case, \(s_{*}^{p-1}\) or \(n^{p-1}\) would be more appropriate.) This _curse of dimensionality_ can easily make it impractical to fit nonparametric regression models.

One way to deal with having too many parameters is to use _generalized additive models_. Christensen (2015, Section 9.10) contains some additional details about writing out generalized additive models but the fundamental idea is an analogy to multifactor analysis of variance. Fitting the full model with \(p=5\) and, say, \(s=8^{5}\) parameters is analogous to fitting a 5 factor interaction term. If we fit the model withonly the 10 three-factor interaction terms, we could get by with \(10(8^{3})=5120\) parameters. If we fit the model with only the 10 two-factor interaction terms, we could get by with \(10(8^{2})=640\) parameters. In particular, with the two-factor interactions, \(f(x_{1},\ldots,x_{5})\) is modeled as the sum of 10 terms each looking like Eq. (1.8.1) but with each involving a different pair of predictor variables.

The all three-factor and all two-factor models still seem like a lot of parameters but the decrease is enormous compared to the five-factor model. The price for this decrease is the simplifying assumptions being made. And if we cannot fit the 5-factor interaction model, we cannot test the validity of those simplifying assumptions, e.g., whether it is alright to drop, say, all of the 4-factor interactions. Of course we don't have to restrict ourselves to the all 4-factor, all three-factor, all two-factor, and main-effects only models. We can create models with some three-factor interactions, some two-factors, and some main effects. Like ANOVA, we need to be concerned about creating linear dependencies in the model matrix \(\Phi\).

The most difficult part of computing least squares estimates is that they generally involve finding the inverse or generalized inverse of the \(p\times p\) matrix \(X^{\prime}X\) (or some similarly sized computation). When \(p\) is large, the computation is difficult. When applying linear-approximation nonparametric methods the problem is finding the generalized inverse of the \(s\times s\) matrix \(\Phi^{\prime}\Phi\), which typically has \(s\) much larger than \(p\). This becomes particularly awkward when \(s>n\). We now consider a device that gives us a model matrix that is always \(n\times n\).

#### Reproducing Kernel Hilbert Space Regression

In Chap. 3 we introduce the theory of _reproducing kernel Hilbert spaces (RKHSs)_ but for now we introduce a simple way to use them in nonparametric regression.

An RKHS transforms a \(p\) vector \(x_{i}\) into an \(s\) vector \(\phi_{i}=[\phi_{0}(x_{i}),\ldots,\phi_{s-1}(x_{i})]^{\prime}\), where not infrequently \(s=\infty\). Just as \(X\) has rows made up of the \(x_{i}^{\prime}\)s, \(\Phi\) has rows made up of the \(\phi_{i}^{\prime}\)s. Just as \(XX^{\prime}=[x_{i}^{\prime}x_{j}]\) is an \(n\times n\) matrix of inner products of the \(x_{i}\)s, the whole point of RKHSs is that there exists a _reproducing kernel (r.k.)_ function \(R(\cdot,\cdot)\) with the property that

\[\tilde{R}\equiv[R(x_{i},x_{j})]=[\phi_{i}^{\prime}D(\eta)\phi_{j}]=\Phi D(\eta) \Phi^{\prime}\]

is an \(n\times n\) inner product matrix of the \(\phi_{i}\)s where \(D(\eta)\) is a positive definite diagonal matrix. Moreover, for \(s\) finite, \(C[\Phi D(\eta)\Phi^{\prime}]=C(\Phi)\) (see _PA_ Section B.4), so fitting the r.k. model

\[Y=\tilde{R}\gamma+e\]

is equivalent to fitting the nonparametric model

\[Y=\Phi\beta+e.\]The r.k. model is just a reparameterization with \(\beta=D(\eta)\Phi^{\prime}\gamma\). In particular, predictions are easy using the r.k. model,

\[\hat{y}(x)=[R(x,x_{1}),\ldots,R(x,x_{n})]\hat{\gamma}.\]

This equivalence between fitting a linear structure with \(\Phi\) and fitting one with the \(n\times n\) matrix \(\tilde{R}\) is sometimes known as the _kernel trick_.

A primary advantage of the kernel trick is simply that, for a known function \(R(\cdot,\cdot)\), it is very easy to construct the matrix \(\tilde{R}\). (It is time consuming to specify \(s\) different \(\phi_{j}(\cdot)\) functions, as opposed to one \(R(\cdot,\cdot)\) function.) Moreover, the \(n\times s\) matrix \(\Phi\) is awkward to use when \(s\) is large. \(\tilde{R}\) is always \(n\times n\), which limits how awkward it can become to use, but also prevents the simplifications that arise when \(s<n\).

When \(s\geq n\) and the \(x_{i}\)s are distinct, it is to be expected that \(\tilde{R}\) will be an \(n\times n\) matrix of rank \(n\), so it defines a saturated model. Least squares estimates (and, for generalized linear models, maximum likelihood estimates) will give fitted values that equal the observations and zero degrees of freedom for error. Nothing interesting will come of fitting a saturated model. We need to deal with this overfitting. Indeed, the kernel trick is typically used together with a penalized (regularized) estimation method such as those discussed in Chap. 2.

If the \(x_{i}\)s are not all distinct, as in the discussion of Fisher's Lack-of-Fit Test from Chap. 6 of \(PA\), the row structures of \(X\), \(\Phi\), and \(\tilde{R}\) (no longer nonsingular) are the same. Fitting any of \(X\xi\), \(\Phi\beta\), and \(\tilde{R}\gamma\) by least squares would give exactly the same _pure error_ sum of squares (_SSPE_) and degrees of freedom (\(dfPE\)). Moreover, fitting \(\Phi\beta\) and \(\tilde{R}\gamma\) would give exactly the same _lack-of-fit_ sum of squares and degrees of freedom but, depending on the size of \(s\), there is a good chance that fitting \(\Phi\beta\) and \(\tilde{R}\gamma\) would give \(SSLF=0\) on \(0\)\(dfLF\). (This is the equivalent of fitting a saturated model when the \(x_{i}\)s are not all distinct.)

Different choices of \(R(\cdot,\cdot)\), if they have \(s\geq n\), typically all give the same \(C(\tilde{R})\), which defines either a saturated model or a model with no lack of fit. Thus different choices of \(R(\cdot,\cdot)\) typically all give the same model, but they typically are reparameterizations of each other. They give the same least squares fits. But we will see in the next chapter that if you have two different parameterizations of the same model, and obtain estimates by penalizing parameters in the same way (i.e. use the same penalty function for every parameterization), that having the same penalties applied to different parameters leads to different fitted models. So, even though different \(R(\cdot,\cdot)\) functions define essentially the same model, applying any standard penalty like ridge regression or lasso, will lead to different fitted values because the equivalent linear models have different parameters that are being shrunk in the same way. The process of shrinking is the same but the parameters are different, thus the end results are different. We saw that different \(\phi_{j}\)s work better or worse on the battery data and there is no way to tell ahead of time which collection will work best. Similarly, different \(R(\cdot,\cdot)\)s (with the same penalty) work better or worse on different data and there is no way to tell, ahead of time, which will work best.

If you know what \(\phi_{j}\) functions you want to use, there is not much mathematical advantage to using r.k.s. But you can use \(R\) functions that are known to be r.k.s for which it is difficult or, in the case of \(s=\infty\), impossible to write down all the \(\phi_{j}\)s. In Chap. 3 we will examine r.k.s that correspond to finite polynomial regression and to fitting splines. But there are a wide variety of potential r.k.s, many that correspond to \(s=\infty\).

Table 7 gives some commonly used r.k.s. More generally, any function that can serve as a covariance function in the sense of Chap. 8 can serve as an r.k. Any r.k. that depends only on \(\|u-v\|\) is a _radial basis function_ r.k.

The hyperbolic tangent in Table 7 is not really an r.k. because it can give \(\tilde{R}\) matrices that are not nonnegative definite. But any function \(R(u,v)\) that is continuous in \(u\) can give plausible answers because it leads to fitting models of the form

\[m(x)=\sum_{j=1}^{n}\gamma_{j}R(x,x_{j}). \tag{8.3}\]

This idea can be viewed as extending the use of approximating functions with small support, cf. Sect. 1.7.2, from one to higher dimensions in a way that limits the curse of dimensionality. With local support methods, in one dimension you partition the line into say \(s_{*}\) sets and fit a separate one-dimensional wavelet, B spline, or other function for each partition set. The problem is that in \(p\) dimensions the number of partition sets (obtained by Cartesian products) quickly gets out of control, \(s_{*}^{p}\). The key idea behind kernel methods is to fit a \(p\)-dimensional function, not for each partition set but for each observed data point. The number of functions being fitted is \(n\), which is large but manageable, rather than \(s_{*}^{p}\) which rapidly becomes unmanageably large. The \(p\)-dimensional functions used in fitting can be defined as a product of \(p\) one-dimensional wavelet, spline, or other functions or they can be defined directly as \(p\)-dimensional functions via some kernel function. The tuning values \(b\) and \(c\) in Table 7 can be viewed as tools for getting the functions centered and scaled appropriately. Fitting \(n\) functions to \(n\) data points would typically result in overfitting, so penalizing the coefficients, as discussed in the next chapter, is appropriate. As mentioned earlier, when \(\tilde{R}\) is a nonsingular matrix (or more generally has the column space associated with finding pure error), it does not matter what function

\begin{table}
\begin{tabular}{|l c|} \hline Names & \(R(u,v)\) \\ \hline Polynomial of degree \(d\) & \((1+u^{\prime}v)^{d}\) \\ Polynomial of degree \(d\) & \(b(c+u^{\prime}v)^{d}\) \\ Gaussian (Radial Basis) & \(\exp(-b\|u-v\|^{2})\) \\ Sigmoid (Hyperbolic Tangent) & \(\tanh(bu^{\prime}v+c)\) \\ Linear Spline (\(u\), \(v\) scalars) & \(\min(u,v)\) \\ Cubic Spline (\(u\), \(v\) scalars) & \(\max(u,v)\min^{2}(u,v)/2-\min^{3}(u,v)/6\) \\ Thin Plate Spline (2 dimensions) & \(\|u-v\|^{2}\log(\|u-v\|)\) \\ \hline \end{tabular}
\end{table}
Table 7: Some common r.k. functions. \(b\) and \(c\) are scalarsyou used to define \(\tilde{R}\) because all such matrices are reparameterizations of each other and give the same least squares fitted values. But if you penalize the parameters in a fixed way, the parameterization penalty will have different effects on different parameterizations.

Example 1.8.1.: I fitted the battery data with the R language's lm command using the polynomial functions \(R(u,v)=(u^{\prime}v)^{4}\), \(R(u,v)=(1+u^{\prime}v)^{4}\), \(R(u,v)=5(7+u^{\prime}v)^{4}\) and the Gaussian functions \(R(u,v)=\exp(-\|u-v\|^{2})\) and \(R(u,v)=\exp(-1000\|u-v\|^{2})\). I defined \(x_{i}\) to include the intercept. The three polynomial functions gave fitted values \(\hat{y}_{i}\) identical to those from fitting a fourth degree polynomial. (I fitted the fourth degree polynomial several ways including using \(\Phi\Phi^{\prime}\) as the model matrix.) The Gaussian r.k.s have \(s=\infty\). The first Gaussian function gave an \(\tilde{R}\) matrix that was computationally singular and gave fitted values that were (to me) unexplainable except as a convenient fitting device similar to the hyperbolic tangent discussed later. The last function gave an \(\tilde{R}\) that was computationally invertible and hence gave fitted values with \(\hat{y}_{i}=y_{i}\). This has overfit the model so penalizing the regression coefficients, as discussed in the next chapter, is advisable.

Figure 10 contains the fit of the hyperbolic tangent "kernel" to the battery data using \(b=1\) and \(c=0\). It turns out that (at least computationally) \(\tilde{R}\) is a rank 8 matrix with R's lm command including only the 1st, 2nd, 3rd, 4th, 11th, 16th, 29th, and 41st columns of \(\tilde{R}\). For an 8 parameter model this has a remarkably high value of \(R^{2}=0.9996\). Incidentally, this \(\tilde{R}\) has negative eigenvalues so is not nonnegative definite. With \(b=5\) and \(c=0\) lm uses columns 1, 2, 3, 6, 12, 22, 37 of \(\tilde{R}\) and again has \(R^{2}=0.9996\). With \(b=10\) and \(c=10\) lm fits only the first column of \(\tilde{R}\) yet has \(R^{2}=0.9526\). In none of these cases has the hyperbolic tangent led to serious overfitting (although it is quite clear from inspecting the R output that we could drop at least one of the columns used in each \(c=0\) example). 

### 9 Testing Lack of Fit in Linear Models

Given a linear model

\[Y=X\beta+e,\qquad\mathrm{E}(e)=0, \tag{9.1}\]

any form of fitting a nonparametric regression determines a potential lack-of-fit test procedure. When fitting nonparametric regression via the linear-approximation models discussed in this chapter, lack-of-fit tests are easy to specify. Because the procedure is based on having a linear-approximation model, essentially the same procedure works regardless of whether one is fitting polynomials, trigonometric functions, wavelets, or splines. (Local polynomials [lowess], because they do not fit a single linear model, do not seem to fit into this procedure.)

Suppose we have the linear model (9.1) based on predictor variables \(x_{1},\ldots,x_{s}\). Given enough data, it may be feasible to produce a nonparametric multiple regression model, say \(Y=\Phi\gamma+e\). In practice, this may need to be some generalized additive model. If \(C(X)\subset C(\Phi)\), we could just test the reduced model against the full model. Unless \(\Phi\) is based on polynomials (including polynomial splines), more often than not \(C(X)\not\subset C(\Phi)\). In that case we can test the reduced model (1.9.1) against the analysis of covariance full model

\[Y=X\beta+\Phi\gamma+e. \tag{1.9.2}\]

The \(F\) statistic is the standard

\[F=\frac{[SSE(1)-SSE(2)][dfE(1)-dfE(2)]}{MSE(2)}\]

and should be about 1 if the original model is correct. If \(e\sim N(0,\sigma^{2}I)\) is a good approximation, the test will have an approximate \(F\) distribution.

This procedure does not define a single lack-of-fit test. Every different method of picking \(\Phi\) defines a different test. Which test is best? It seems pretty clear that no best test can possibly exist. If the lack of fit is due to the true model involving cosine curves that were not included in model (1.9.1), picking a \(\Phi\) based on cosine curves should work better than picking a \(\Phi\) based on polynomials or wavelets. If the lack of fit is due to the true model involving polynomial terms not included in model (1.9.1), picking \(\Phi\) based on polynomials should work better than picking a \(\Phi\) based on sines, cosines, or wavelets.

Figure 1.10: Hyperbolic tangent fit to battery data

What works best will depend on the true nature of the lack of fit. Unfortunately, we do not know the true model. We won't know which of these tests will work best unless we have a very good idea about the true lack of fit and how to model it. If we had those good ideas, we probably wouldn't be thinking about doing a lack-of-fit test.

Incidentally, it is clearly impossible for such tests to be sensitive only to lack of fit in the mean structure. As will be discussed in Chap. 5, it is perfectly reasonable to assume that \(\gamma\) in (9.2) is a random vector with mean 0. In such a case, \(\operatorname{E}(Y)=X\beta\), so there is no lack of fit. However the \(F\) test will still be sensitive to seeing random values of \(\gamma\) that are very different from 0. Such values of \(\gamma\) will be the result of some combination of heteroscedasticity or dependence among the observations in \(Y\). There is no way to tell from the test itself whether lack of fit or heteroscedasticity or dependence or some combination is causing a large \(F\) statistic.

The ACOVA lack-of-fit test examined in Christensen (1989, 1991) can be thought of as adding a nonparametric component to the original model that is simply a step function with steps defined for groups of near replicates among the predictor variables. Neill and Johnson (1989) and Christensen (1991) established that the ACOVA test tends to be worse than another test that is UMPI for orthogonal lack of fit between clusters of near replicates.

In fact, most traditional forms of lack-of-fit testing can be viewed through a lens of performing some kind of nonparametric regression on subsets of the data. Most traditional lack-of-fit tests rely on partitioning the data and fitting some kind of linear model within the partition sets. Fitting models on partitions is nothing more than fitting approximating-functions with small support. Lack-of-fit tests are reviewed in _PA_ as well as Christensen (2015). Atwood and Ryan's idea for testing lack of fit is just fitting the original model on subsets of the data, so it is essentially fitting multivariate linear splines _without_ the requirement that the fitted splines be continuous. Utts' method relies on fitting the original model on only a central group of points, so it implicitly puts each point not in the central group into a separate partition set and fits a separate parameter to each of those noncentral data points. (As alluded to earlier, the irony is that the more parameters you fit the more "nonparametric" your procedure.) Fisher's test fits the biggest model possible that maintains the row structure of the data, cf. _PA-V_ Sect. 6.7.2, i.e., the data are partitioned into sets where the predictor variables are identical and a separate parameter if fitted to each set.

Clearly, this model based approach to performing lack-of-fit tests can be extended to testing lack of fit in logistic regression and other generalized linear models.

### 10 Regression Trees

Regression trees can be viewed as a form of linear modeling. In fact, they can be thought of as using forward selection to deal with the dimensionality problems of nonparametric multiple regression. But, unlike standard forward selection, the variables considered for inclusion in the model change with each step of the process.

There are a number of different algorithms available for constructing regression trees, cf. Loh (2011). We merely discuss their general motivation. Constructing trees is also known as _recursive partitioning_.

A simple approach to nonparametric regression is to turn the problem into a multifactor ANOVA. With \(p\) predictor variables, partition each predictor variable into \(s_{*}\) groups. In other words, define \(s_{*}\) indicator functions to partition each variable. Construct the predictor functions as in (1.8.2) by multiplying the indicator functions. This amounts to partitioning \(p\) dimensional space into \(s_{*}^{p}\) subsets. Fitting the regression function (1.8.2) amounts to fitting an ANOVA with \(p\) factors each at \(s_{*}\) levels, i.e., an \(s_{*}^{p}\) ANOVA. Fitting the ANOVA model that includes the \(p\)-factor interaction is equivalent to fitting a one-way ANOVA with \(s_{*}^{p}\) groups. If you want to make a prediction for a new point \(x=(x_{1},\ldots,x_{p})^{\prime}\), just figure out which of the \(s_{*}^{p}\) partition sets includes \(x\) and the prediction is the sample mean of the \(y\) observations that fell into that set. Of course this does nothing to help with the curse of dimensionality, but fitting generalized additive models is clearly nothing more than fitting a model that eliminates many of the interactions in the \(s_{*}^{p}\) ANOVA.

How do you pick the partition sets? The more partition sets you have, the more "nonparametric" the model will be. A reasonable rule of thumb might be to require that if a partition set includes any data at all, it has to include, say, 5 observations. The sets with no data we will ignore and never make predictions there. Five observations in a partition set is not a crazy small number of observations on which to base a prediction. Once the partition has be determined, we could use backward elimination to find partition sets that can be pooled together. (It would probably be wise to require that partition sets to be pooled must be contiguous in \(p\) dimensional space.)

Fitting regression trees is basically the same idea except that they are based on forward selection rather than backward elimination. By using forward selection, the procedure avoids the curse of dimensionality. Usually forward selection can easily miss important features. A nice feature of regression trees is that they pick the partition sets as well as deciding which partition sets need further dividing. In other words, they search through far more than a single set of \(s_{*}^{p}\) partition sets. In practice, regression trees are often used with bagging or random forests as discussed in _PA-V_, Chap. 14.

We now consider two examples. A very simple one to illustrate the ideas and a slightly more complicated one that examines the process.

Example 1.10.1.: Consider a simple example with \(n=7\) observations and two predictor variables \(x_{1},x_{2}\), specifically

\[Y=\begin{bmatrix}y_{1}\\ y_{2}\\ y_{3}\\ y_{4}\\ y_{5}\\ y_{6}\\ y_{7}\end{bmatrix}\qquad[X_{1},X_{2}]=\begin{bmatrix}1&2\\ 2&4\\ 3&6\\ 4&1\\ 5&5\\ 6&7\\ 7&3\end{bmatrix}.\]The first step is to split the data into two parts based on the size of \(X_{1}\) or \(X_{2}\). For instance, we can consider a split that consists of the smallest \(x_{1}\) value and the six largest; or the two smallest \(x_{1}\) values and the five largest; or the smallest three \(x_{2}\) values and the largest four. We consider all such splits and posit an initial regression tree model \(Y=\Phi^{(1)}\beta+e\), where

\[\Phi^{(1)}=\begin{bmatrix}1&1&1&1&1&1&1&0&1&1&1&1\\ 1&0&1&1&1&1&1&0&0&0&1&1&1\\ 1&0&0&1&1&1&1&0&0&0&0&0&1\\ 1&0&0&0&1&1&1&1&1&1&1\\ 1&0&0&0&0&1&1&0&0&0&0&1&1\\ 1&0&0&0&0&0&1&0&0&0&0&0\\ 1&0&0&0&0&0&0&0&0&0&1&1&1\end{bmatrix}.\]

The last 12 columns identify all of the possible splits. Columns 2 through 7 are the splits based on \(x_{1}\) and columns 8 through 13 are the splits based on \(x_{2}\), with, for example, the tenth column identifying the smallest three \(x_{2}\) values and, by default since a column of 1's is included, the largest four. Obviously, this initial model is overparameterized; it has 13 predictor variables to explain 7 observations. The first (intercept) column is forced into the model and one other column is chosen by forward selection. Suppose that column is the fifth, so at the second stage we have the columns

\[\begin{bmatrix}1&1\\ 1&1\\ 1&1\\ 1&1\\ 1&0\\ 1&0\\ 1&0\end{bmatrix}\text{ or equivalently }\begin{bmatrix}1&0\\ 1&0\\ 1&0\\ 0&1\\ 0&1\\ 0&1\end{bmatrix}\]

forced into the second-stage model matrix. We now consider possible splits _within_ the two groups that we have already identified. The first four observations can be split based on the sizes of either \(x_{1}\) or \(x_{2}\) and similarly for the last three. The second stage model is \(Y=\Phi^{(2)}\beta+e\), where

\[\Phi^{(2)}=\begin{bmatrix}1&0&1&1&1&0&1&0&0&0&0\\ 1&0&0&1&1&0&0&1&0&0&0\\ 1&0&0&0&1&0&0&0&0&0&0\\ 1&0&0&0&0&1&1&1&0&0&0&0\\ 0&1&0&0&0&0&0&0&1&1&0&1\\ 0&1&0&0&0&0&0&0&0&1&0&0\\ 0&1&0&0&0&0&0&0&0&1&1\end{bmatrix}.\]

Here, columns 3, 4, and 5 are splits of the first group based on the size of \(x_{1}\) and columns 6, 7, and 8 are splits of the first group based on the size of \(x_{2}\). Columns 9 and 10 are splits of the second group based on \(x_{1}\) and columns 11 and 12 are based on \(x_{2}\). Again, the model is grossly overparameterized. Columns 1 and 2 are forced into the model, and one more column is chosen by forward selection. Suppose it is column 7, so at the third stage we have forced into the model. We now have three groups, and again we consider splitting within groups. At the third stage, we have \(Y=\Phi^{(3)}\beta+e\), where

\[\Phi^{(3)}=\begin{bmatrix}0&0&1&0&0&0&0&0\\ 1&0&0&0&1&0&0&0\\ 1&0&0&0&0&0&0&0\\ 0&0&1&1&0&0&0&0&0\\ 0&1&0&0&0&1&1&0&1\\ 0&1&0&0&0&0&1&0&0\\ 0&1&0&0&0&0&0&1&1\end{bmatrix}.\]

Again, we add a column by forward selection. If no column can be added, we return to the model with the three forced variables,

\[Y=\begin{bmatrix}0&0&1\\ 1&0&0\\ 0&0&1\\ 0&1&0\\ 0&1&0\\ 0&1&0\\ 0&1&0\end{bmatrix}\beta+e.\]

Note that this is just a one-way ANOVA model, so the parameter estimates are group means. We can identify the groups as (1) \(x_{1}<4.5\), \(x_{2}<2.5\); (2) \(x_{1}>4.5\); and (3) \(x_{1}<4.5\), \(x_{2}>2.5\). Predictions are based on identifying the appropriate group and use the group mean as a point prediction. Note that this is essentially fitting a step function to the data.

Going back to the original parameterization of the model (i.e., the original choices of columns), the model is

\[Y=\begin{bmatrix}1&1&1\\ 1&1&0\\ 1&1&0\\ 1&1&1\\ 1&0&0\\ 1&0&0\\ 1&0&0\end{bmatrix}\beta+e.\]

[MISSING_PAGE_FAIL:69]

We examine a partitioning created using the R package rpart. Details are given in the computing document on my website. For illustrative purposes, I required that there could be no fewer than two data points in any partition set. Typically one would do this on a bigger set of data and perhaps require more observations in every partition set. Figures 11 and 12 illustrate the recursive partitioning process. Figure 12 is the tree diagram produced by rpart. The algorithm begins by partitioning \(x_{3}\) four times before it involves \(x_{5}\). I set up rpart to keep running until it did a partition based on \(x_{5}\).

Table 9 contains the statistics that determine the first partitioning of the data. The values \(x_{3(i)}\) are the ordered values \(x_{3}\) with \(y_{3(i)}\) the corresponding \(y\) values (order statistics and induced order statistics). \(x_{5(j)}\) and \(y_{5(j)}\) are the corresponding values for \(x_{5}\). For \(i=3\), the partition sets consist of the 3 smallest \(x_{3}\) observations, and the 17 largest. For \(i=18\) the partition sets are the 18 smallest \(x_{3}\) observations, and the 2 largest. For \(j=18\) the partition sets are the 18 smallest \(x_{5}\) observations, and the 2 largest. Built in is the requirement that each partition set contain at least 2 observations. The _SSE_s are from fitting a one-way ANOVA on the two groups. Note

Figure 11: Partition sets for \(x_{3}\), \(x_{5}\)

that the smallest \(SSE\) corresponds to \(i=3\), so that is the first partition used. The first split illustrated in Figs. 11 and 12 is at \(-11.35=[x_{3(3)}+x_{3(4)}]/2\) so that the two partition sets include the 3 smallest \(x_{3}\) observations, and the 17 largest.

For the second split we consider all the splits of each of the two partition sets from the first stage. Fortunately for our illustration, the partition set \(x_{3}<-11.35\) has only 3 observations, so we are not allowed to split it further because splitting it has to create a partition set with less than 2 observations. Thus we only need to consider all splits of the set \(x_{3}\geq-11.35\). In Table 10 I have blanked out the observations with \(x_{3}<-11.35\) but remember that these three observations are still included in the \(SSE\)s. The minimum \(SSE\) occurs when \(i=13\) so the next partition set goes from \(-11.35\) to \(8.525=[x_{3(13)}+x_{3(14)}]/2\), as illustrated in Figs. 11 and 12.

While rpart created the partition just mentioned, in Table 10 the value \(j=13\) gives the same \(SSE\) as \(i=13\). The alternative partition of the \((x_{3},x_{5})\) plane with \(x_{3}\geq-11.35\) and \(x_{5}\) divided at \(6.46=[x_{5(13)}+x_{5(14)}]/2\) is given in the bottom right of Fig. 11. It separates the data into exactly the same three groups as the rpart partition. I have no idea why rpart chose the partition based on \(x_{3}\) rather than the alternative partition based on \(x_{5}\). It looks like, after incorporating the alternative partition, the subsequent partitions would continue to divide _the data_ in the same way. However, the final partition sets would be different, which means that predictions could be different. There are 6 final partition sets, so there are only 6 distinct values that will be used to predict, and they will be the same 6 numbers for either partitioning. But the ranges of \((x_{3},x_{5})\) values over which those 6 predictions are applied change with the different partitions.

Figure 12: Regression tree for \(x_{3}\), \(x_{5}\)

\begin{table}
\begin{tabular}{|l l l|l l l l l|} \hline \(i\) & \(x_{3(i)}\) & \(y_{3(i)}\) & \(SSE\) & \(j\) & \(x_{5(j)}\) & \(y_{5(j)}\) & \(SSE\) \\ \hline
1 & & & & 1 & & & \\

[MISSING_PAGE_POST]

 \hline \end{tabular}
\end{table}
Table 1.9: First tree split

\begin{table}
\begin{tabular}{|l l l|l l l l|} \hline \(i\) & \(x_{3(i)}\) & \(y_{3(i)}\) & \(SSE\) & \(j\) & \(x_{5(j)}\) & \(y_{5(j)}\) & \(SSE\) \\ \hline
1 & & & & 1 & & & \\
2 & & & & 2 & 5.34 & 35.20 & \\
3 & & & & 3 & 5.57 & 37.20 & 221.2 \\
4 & \(-\)10.

The set \(x_{3}<-11.35\) cannot be split further because of our requirement that all partition sets include two data points. But \(-11.35\leq x_{3}<8.525\) has 10 data points, so it can be split \(14=2\times(10-3)\) ways, and \(x_{3}\geq 8.525\) has 7 points, so can be split \(8=2\times(7-3)\) ways. That is another 22 ANOVAs to run from which we pick the one with the smallest \(SSE\). The minimum occurs when splitting \(x_{3}\) between \(x_{3(11)}\) and \(x_{3(12)}\), cf. Figs. 1.11 and 1.12. We discontinue the detailed illustration.

An advantage of doing one full \(s_{*}^{p}\) partition is that you can easily identify empty cells and cells with little data. Prediction variances will reflect that. With a forward selection partition, the algorithms typically create partition sets that restrict the minimum number of observations in a partition. However, looking a Fig. 1.11, an observation with, for example, a small value of \(x_{3}\) and a large value of \(x_{5}\) is far from the other data in its partition set, so it is unlikely to be predicted well by the mean of the observations in that set. It is not clear to me how one could identify that troublesome phenomenon when fitting a regression tree in higher dimensions 

Exercise 1.3: Without a computer, find the predictions for the point (15,6) from the two partitions. Hint: the rpart prediction is based on the average of four \(y\) values and the alternative partition prediction is based on two.

There is a strong tendency for regression trees to _overfit_ the data, causing poor predictive performance from tree models. Random forests, bagging, and boosting were developed to improve the predictive performance of tree models. Those topics are discussed in the last section of the last chapter of _PA-V_.

### 1.11 Regression on Functional Predictors

For each dependent variable \(y_{i}\), \(i=1,\ldots,n\), suppose we observe a function of predictor variables, say \(\mathcal{X}_{i}(t)\), \(t\in\mathcal{T}\subset\mathbf{R}^{d}\). The predictor function might be observed over time or might result from some sort of medical imaging. For some unknown function \(\gamma(t)\) of regression coefficients, we assume the model

\[y_{i}=\alpha+\int_{\mathcal{T}}\mathcal{X}_{i}(t)\gamma(t)dt+e_{i},\qquad \mathrm{E}(e_{i})=0.\]

As a practical matter, we incorporate a nonparametric characterization of the regression coefficient function using a standard spanning set of functions \(\phi_{j}\) to get

\[\gamma(t)\doteq\sum_{j=0}^{s-1}\beta_{j}\phi_{j}(t)=\phi(t)^{\prime}\beta.\]

where

\[\phi(t)^{\prime}\equiv[\phi_{0}(t),\ldots,\phi_{s-1}(t)]^{\prime},\qquad\beta \equiv(\beta_{0},\ldots,\beta_{s-1})^{\prime}.\]This leads to a standard linear model for the observations

\[y_{i} = \alpha+\int_{\mathcal{T}}\mathcal{X}_{i}(t)\gamma(t)dt+e_{i}\] \[\doteq \alpha+\int_{\mathcal{T}}\mathcal{X}_{i}(t)\phi(t)^{\prime}\beta dt +e_{i}\] \[= \alpha+\left[\int_{\mathcal{T}}\mathcal{X}_{i}(t)\phi(t)^{\prime} dt\right]\beta+e_{i}\] \[= \alpha+x_{i}^{\prime}\beta+e_{i}\]

where

\[x_{i}^{\prime}\equiv(x_{i0},\ldots,x_{i,s-1}),\quad\text{and}\quad x_{ij} \equiv\int_{\mathcal{T}}\mathcal{X}_{i}(t)\phi_{j}(t)dt.\]

See Reiss, Goldsmith, Shang, and Ogden (2017) for additional discussion.

In reality, it is impossible to observe \(\mathcal{X}_{i}(t)\) for every \(t\) in an infinite set of points \(\mathcal{T}\). At most we can observe \(\mathcal{X}_{i}(t)\) at \(t_{ik}\), \(k=1,\ldots,N_{i}\) where we would expect \(N_{i}\) to be a very large number. In this case, we would want to use numerical approximations to the integrals, perhaps even something as simple as

\[x_{ij}=\frac{1}{N_{i}}\sum_{k=1}^{N_{i}}\mathcal{X}_{i}(t_{ik})\phi_{j}(t_{ik}).\]

### Density Estimation

Let \(y_{1},\ldots,y_{N}\) be a random sample with density \(f(y)\). Without loss of generality, we assume that the support of the density is the unit interval \([0,1]\). Partition the unit interval into \(S\) very small equal-sized intervals centered at \(t_{h}=(2h-1)/2S\), \(h=1,\ldots,S\). Let \(n\equiv(n_{1},\ldots,n_{S})^{\prime}\) be the vector of counts in each interval, that is, the number of the \(y_{i}\) that fall into each interval. For \(S\) large, these should all be \(0\)'s and \(1\)'s.

The vector \(n\) has a multinomial distribution. The vector of expected values is \(\mathrm{E}(n)\equiv m\). The vector of probabilities for the cells is \((1/N)\mathrm{E}(n)=p\). By assumption, \(p_{h}\doteq f(t_{h})/S\). Define \(f\equiv[f(t_{1}),\ldots,f(t_{S})]^{\prime}\). We want to estimate the vector

\[f\doteq Sp=\frac{S}{N}\mathrm{E}(n)=\frac{S}{N}m.\]

As in other aspects of nonparametric regression, we define the matrix \(\Phi\) such that \(\Phi=[\Phi_{0},\ldots,\Phi_{s-1}]\) and \(\Phi_{j}=[\phi_{j}(t_{1}),\ldots,\phi_{j}(t_{S})]^{\prime}\). Because the \(t_{h}\)s are equally spaced, there is little problem in taking \((1/\sqrt{S})\Phi\) to have orthonormal columns. Standard methods based on function series apparently use a linear model

\[m=\mathrm{E}(n)=\Phi\beta.\]Estimating via least squares gives

\[\hat{m}=\Phi\hat{\beta}=\frac{1}{S}\Phi\Phi^{\prime}n\]

and

\[\hat{f}=\frac{S}{N}\hat{m}=\frac{S}{N}\Phi\hat{\beta}=\frac{1}{N}\Phi\Phi^{ \prime}n.\]

In particular, for \(j=0,\ldots,s-1\)

\[\frac{S}{N}\hat{\beta}_{j}=\frac{1}{N}\Phi^{\prime}_{j}n=\frac{1}{N}\sum_{h=1} ^{S}\phi_{j}(t_{h})n_{h}\doteq\frac{1}{N}\sum_{i=1}^{N}\phi_{j}(y_{i}).\]

The right-hand side is the estimator used in Efromovich (1999, Equation 3.1.4).

The complete density can be estimated by

\[\hat{f}(y)=\frac{S}{N}\sum_{j=0}^{s-1}\hat{\beta}_{j}\phi_{j}(y),\]

but this function need not integrate to 1 nor even be positive.

It would be more common to analyze count data using a log-linear model, say,

\[\log(m)=\log\mathrm{E}(n)=\Phi\beta,\]

with estimate

\[\hat{f}=\frac{S}{N}\exp(\Phi\hat{\beta})\]

or more generally

\[\hat{f}(y)=\frac{S}{N}\exp\left[\sum_{j=0}^{s-1}\hat{\beta}_{j}\phi_{j}(y) \right].\]

One advantage of this is that the density estimates are forced to be positive, unlike the linear model estimates. Christensen (1997) discusses log-linear modeling. In particular, the log-linear model estimate of \(\hat{\beta}\) is a solution

\[\Phi^{\prime}[n-\exp(\Phi\hat{\beta})]=0. \tag{1.12.1}\]

With \(\phi_{0}(x)\equiv 1\), a consequence of (1.12.1) is \((1/S)J^{\prime}\hat{f}=1\), so \(\hat{f}(y)\) approximately integrates to 1. There is still no assurance that \(\hat{f}(y)\) will integrate exactly to 1. In any case, it is a simple matter to standardize the estimate.

### 1.13 Exercises

The first six exercises reexamine _The Coleman Report_ data of Table 1.8. The first 5 consider only two variables: \(y\), the mean verbal test score for sixth graders, and \(x_{3}\), the composite measure of socioeconomic status.

##### Exercise 1.13.1

Rescale \(x_{3}\) to make its values lie between 0 and 1. Plot the data. Using least squares, fit models with \(p=10\) using polynomials and cosines. Plot the regression lines along with the data. Which family works better on these data, cosines or polynomials?

##### Exercise 1.13.2

Using \(s=8\), fit the _Coleman Report_ data using Haar wavelets. How well does this do compared to the cosine and polynomial fits?

##### Exercise 1.13.3

Based on the \(s=10\) polynomial and cosine models fitted in Exercise 1.13.1, use \(C_{p}\) to determine a best submodel for each fit. Plot the regression lines for the best submodels. Which family works better on these data, cosines or polynomials? Use \(C_{p}\) to determine the highest frequency needed when fitting sines and cosines.

##### Exercise 1.13.4

Investigate whether there is a need to consider heteroscedastic variances with the _Coleman Report_ data. If appropriate, refit the data.

##### Exercise 1.13.5

Fit a cubic spline nonparametric regression to the _Coleman Report_ data.

##### Exercise 1.13.6

Fit a regression tree to the _Coleman Report_ data using just variable \(x_{4}\).

##### Exercise 1.13.7

In Sect. 1.7 we set up the interpolating cubic spline problem as one of fitting a saturated linear model that is forced be continuous, have continuous first and second derivatives, and have 0 as the second derivative on the boundaries. In our discussion, the dots are only connected implicitly because a saturated model must fit every data point perfectly. Show that you can find the parameters of the fitted polynomials without using least squares by setting up a system of linear equations requiring the polynomials to connect the \((x_{i},y_{i})\) dots along with satisfying the derivative conditions.

##### Exercise 1.13.8

Fit a tree model to the battery data and compare the results to fitting Haar wavelets.

## References

* Breiman et al. (1984) Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). _Classification and regression trees_. Belmont, CA: Wadsworth.
* Carroll & Ruppert (1988) Carroll, R. J., & Ruppert, D. (1988). _Transformations and weighting in regression_. New York: Chapman and Hall.
* Christensen (1989) Christensen, R. (1989). Lack of fit tests based on near or exact replicates. _Annals of Statistics_, _17_, 673-683.
* Christensen (1991) Christensen, R. (1991). Small sample characterizations of near replicate lack of fit tests. _Journal of the American Statistical Association_, _86_, 752-756.
* Christensen (1996) Christensen, R. (1996). _Analysis of variance, design, and regression: Applied statistical methods_. London: Chapman and Hall.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton, FL: Chapman and Hall/CRC Press.
* Christensen et al. (2010) Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton, FL: Chapman and Hall/CRC Press.
* Efromovich (1999) Efromovich, S. (1999). _Nonparametric curve estimation: Methods, theory, and applications_. New York: Springer.
* Eilers & Marx (1996) Eilers, P. H. C., & Marx, B. D. (1996). Flexible smoothing with b-splines and penalties (with discussion). _Statistical Science, 11_, 89-121.
* Eubank (1988) Eubank, R. L. (1988). _Spline smoothing and nonparametric regression_. New York: Marcel Dekker.
* Goldstein & Smith (1974) Goldstein, M., & Smith, A. F. M. (1974). Ridge-type estimators for regression analysis. _Journal of the Royal Statistical Society, Series B_, _26_, 284-291.
* Green & Silverman (1994) Green, P. J., & Silverman, B. W. (1994). _Nonparametric regression and generalized linear models: A roughness penalty approach_. London: Chapman and Hall.
* Hart (1997) Hart, J. D. (1997). _Nonparametric smoothing and lack-of-fit tests_. New York: Springer.
* Hurvich & Tsai (1995) Hurvich, C. M., & Tsai, C.-L. (1995). Relative rates of convergence for efficient model selection criteria in linear regression. _Biometrika_, _82_, 418-425.
* Lenth (2015) Lenth, R. V. (2015). The case against normal plots of effects (with discussion). _Journal of Quality Technology, 47_, 91-97.
* Loh (2011) Loh, W.-Y. (2011). Classification and regression trees. _WIREs Data Mining and Knowledge Discovery, 1_, 14-23.
* Montgomery & Peck (1982) Montgomery, D. C., & Peck, E. A. (1982). _Introduction to linear regression analysis_. New York: Wiley.
* Mosteller & Tukey (1977) Mosteller, F., & Tukey, J. W. (1977). _Data analysis and regression_. Reading, MA: Addison-Wesley.
* O'Hagan & Peck (1998)Neill, J. W., & Johnson, D. E. (1989). A comparison of some lack of fit tests based on near replicates. _Communications in Statistics, Part A--Theory and Methods,__18_, 3533-3570.
* Ogden (1997) Ogden, R. T. (1997). _Essential wavelets for statistical applications and data analysis_. Boston: Birkhauser.
* Reiss et al. (2017) Reiss, P. T., Goldsmith, J., Shang, H. L., & Ogden, R. T. (2017). Methods for scalar-on-function regression. _International Statistical Review,__85_, 228-249.

## Chapter 2 Penalized Estimation

**Abstract** The nonparametric methods of Chap. 1 are really highly parametric methods and suffer from issues of overfitting, i.e., fitting so many parameters to the data that the models lose their ability to make effective predictions. One way to stop overfitting is by using penalized estimation (regularization) methods. Penalized estimation provides an automated method of keeping the estimates from tracking the data more closely than is justified.

### 2.1 Introduction

In applications of linear model theory to situations where the number of model parameters is large relative to the sample size \(n\), it is not uncommon to replace least squares estimates with estimates that incorporate a penalty on (some of) the regression coefficients. The nonparametric regression models of the previous chapter and _PA_ Sect. 6.2.1 are germane examples. Penalty functions are often used to avoid _overfitting_ a model. They can make it possible to use numbers of parameters that are similar to the number of observations without overfitting the model. As a tool to avoid overfitting, penalized estimation constitutes an alternative to variable selection, cf. _PA-V_, Chap. 14. Penalized estimation generally results in biased estimates (but not necessarily so biased as to be a bad thing). Chapter 1 and Christensen (2015, Chapter 8) contain some plots of overfitted models.

Penalized estimates are determined by adding some multiple of a nonnegative _penalty function_ to the least squares criterion function and minimizing this new criterion function. For example, in the partitioned linear model, say,

\[Y=X\beta+Z\gamma+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I, \tag{2.1.1}\]

if \(\gamma_{s\times 1}\) is a high dimensional vector, one might minimize

\[(Y-X\beta-Z\gamma)^{\prime}(Y-X\beta-Z\gamma)+k\,\mathcal{P}(\gamma), \tag{2.1.2}\]where \(\mathcal{P}(\gamma)\) is a nonnegative penalty function and \(k\geq 0\) is a _tuning parameter_. Obviously, if \(k=0\), the estimates are least squares estimates. Typical penalty functions are minimized at the vector \(\gamma=0\), so as \(k\) gets large, the penalty function dominates the minimization and the procedure, in some fashion, shrinks the least squares estimate of \(\gamma\) towards \(0\). Incorporating a penalty function is sometimes referred to as _regularization_.

For model (2.1.1) the estimates can be viewed as a combination of least squares estimation and penalized least squares estimation. As in \(P\!A\) Chap. 9, rewrite model (2.1.1) as

\[Y=X\delta+(I-M)Z\gamma+e\]

and rewrite the estimation criterion as

\[\begin{array}{l}[Y-X\delta-(I-M)Z\gamma]^{\prime}[Y-X\delta-(I-M)Z\gamma]+k \,\mathcal{P}(\gamma)\\ =\,[MY-X\delta]^{\prime}[MY-X\delta]\\ \qquad+\,[(I-M)Y-(I-M)Z\gamma]^{\prime}[(I-M)Y-(I-M)Z\gamma]+k\,\mathcal{P}( \gamma).\end{array} \tag{2.1.3}\]

The parameter \(\delta\) appears only in the first term of the second expression for the estimation criterion, so any minimizing estimate of \(\delta\) has \(X\hat{\delta}=MY\), i.e., \(\hat{\delta}\) is least squares. The parameter \(\gamma\) only appears in the last two terms, so the penalized estimate of \(\gamma\) is, say, \(\tilde{\gamma}_{k}\) that minimizes

\[[(I-M)Y-(I-M)Z\gamma]^{\prime}[(I-M)Y-(I-M)Z\gamma]+k\,\mathcal{P}(\gamma).\]

If we first fit \(Y=X\delta+e\) to obtain least squares residuals \(\hat{e}\equiv(I-M)Y\), then \(\tilde{\gamma}_{k}\) is a penalized estimate from fitting \(\hat{e}=(I-M)Z\gamma+e\). Since \(X\delta=X\beta+MZ\gamma\), the penalized estimate of \(X\beta\) satisfies \(X\hat{\delta}=X\tilde{\beta}_{k}+MZ\tilde{\gamma}_{k}\) or

\[X\tilde{\beta}_{k}=M(Y-Z\tilde{\gamma}_{k}),\]

which is analogous to a formula for least squares estimation in \(P\!A\) Chap. 9.

Rarely is there an obvious choice for \(k\). Extending the idea of Hoerl and Kennard (1970), we can use a _trace_ plot to pick \(k\). Denote the penalized estimates for given \(k\) and the \(j\)th predictor variable as \(\tilde{\gamma}_{kj}\). The trace plot is, for all \(j\), a simultaneous plot of the curves defined by \((k,\tilde{\gamma}_{kj})\) as \(k\) varies. As mentioned, for \(k=0\) the \(\tilde{\gamma}_{0j}\)s are the least squares estimates and as \(k\) increases they typically all shrink towards \(0\). For the purpose of dealing with collinearity issues, Hoerl and Kennard suggested picking a small \(k\) for which the estimates settle down, i.e., stop varying wildly. Draper and van Nostrand (1979) conclude that for ridge regression the problems with picking \(k\) using trace plots outweigh their benefits. More modern methods of picking \(k\) include cross-validation and generalized cross-validation, cf. Hastie, Tibshirani, and Friedman (2016) or any number of other sources.

As discussed in \(P\!A\), least squares is a geometric estimation criterion, not a statistical criterion, but least squares has many nice statistical properties. Penalized least squares is also a geometric criterion, not a statistical one. Unfortunately, it is harder to establish nice statistical properties for penalized estimates. That is _not_ to say that they don't have any. Section 2.5 illustrates some geometry related to penalized least squares.

Many of the commonly used \(\mathcal{P}\)s penalize each variable the same amount. As a result, it is often suggested that the predictors in the model matrix \(Z\) should be standardized onto a common scale. If the height of my doghouse is a predictor variable, the appropriate regression coefficient depends a great deal on whether the height is measured in miles or microns. For a penalty function to be meaningful, it needs to be defined on an appropriate scale for each predictor variable.

Typically we assume that \(J\), the vector of 1s, satisfies \(J\in C(X)\), i.e., the \(X\beta\) portion of the model contains an intercept or its equivalent. Frequently, \(Z\) is standardized in some way that does not change \(C(J,Z)\) but so that \(C(Z)\subset C(J)^{\perp}\), e.g., \(Z\) is replaced by \([I-(1/n)JJ^{\prime}]Z=Z-\vec{z}J\), or more commonly \(Z\) is additionally modified so that \(Z^{\prime}Z\) also has 1's (or some other constant) down it's diagonal, cf. _PA-V_ Sect. 13.1 (Christensen 2011, Section 15.1). The latter standardization puts all the regression variables in \(Z\) on the same scale, i.e., every column of \(Z\) has the same length, so it makes some sense to penalize their regression coefficients equally.

Any linear model can be written in the form (2.1.1) and any penalty function can be written as a function of \(\gamma\) alone. The partitioned model merely allows us to focus on penalizing some, but not all, of the coefficients. The same thing could be accomplished by defining a penalty function that nominally depends on both \(\beta\) and \(\gamma\) but actually only depends on \(\gamma\). The most common form for (2.1.1) probably puts only the intercept in \(X\beta\) and all other predictors in \(Z\gamma\).

On occasion it may be simpler to think about fitting an unpartitioned model

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I, \tag{2.1.4}\]

with estimation criterion

\[(Y-X\beta)^{\prime}(Y-X\beta)+k\mathcal{P}(\beta). \tag{2.1.5}\]

One instance is the following argument.

If model (2.1.4) has multivariate normal errors, the least squares residuals will be independent of the penalized fitted values, so if (2.1.4) provides an adequate number of degrees of freedom for error, it may be advantageous to use the least squares residuals, rather than residuals based on the penalized estimates, to estimate the variance and to check model assumptions. To establish this, we merely need to show that the penalized estimates are a function of the least squares estimates. Using ideas similar to the proof of _PA_ Theorem 2.2.1 with \(\hat{\beta}\) a least squares estimate, write

\[\left\|Y-X\beta\right\|^{2} = (Y-X\beta)^{\prime}(Y-X\beta) \tag{2.1.6}\] \[= (Y-MY+MY-X\beta)^{\prime}(Y-MY+MY-X\beta)\] \[= (Y-MY)^{\prime}(Y-MY)+(MY-X\beta)^{\prime}(MY-X\beta)\] \[= (Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})+(X\hat{\beta}-X\beta)^{ \prime}(X\hat{\beta}-X\beta)\] \[= (Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})+(\beta-\hat{\beta})^{ \prime}X^{\prime}X(\beta-\hat{\beta}).\]The estimation criterion (2.1.5) becomes

\[(Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})+(\beta-\hat{\beta})^{\prime}X^{\prime}X( \beta-\hat{\beta})+k\mathcal{P}(\beta)\]

in which the first term \((Y-X\hat{\beta})^{\prime}(Y-X\hat{\beta})\) does not involve \(\beta\), so is irrelevant to the minimization, and the other terms depend on \(Y\) only through \(\hat{\beta}\). Since the residuals \(\hat{e}=(I-M)Y\) are independent of \(\hat{\beta}\), the residuals are independent of the penalized estimate which must be a function of \(\hat{\beta}\). Moreover, if the penalized estimate is a _linear_ function of \(\hat{\beta}\) (e.g. ridge regression), \(\hat{e}\) is uncorrelated with the penalized estimate even without the assumption of multivariate normality. The decomposition of the squared distance in (2.1.6) will be used again in Sect. 2.5.1 to facilitate geometric interpretations.

#### Reparameterization and RKHS Regression:

It's All About the Penalty

In traditional linear models it is well know that reparameterizations are irrelevant, i.e., two models for the same data, say, \(Y=X_{1}\beta_{1}+e\) and \(Y=X_{2}\beta_{2}+e\) are equivalent if \(C(X_{1})=C(X_{2})\). In particular, least squares gives the same fitted values \(\hat{Y}\) and residuals \(\hat{e}\) for each model. Moreover, the least squares estimates for either \(\beta_{1}\) or \(\beta_{2}\) may not be uniquely defined, but we don't much care. In penalized least squares, if you use the same penalty function \(\mathcal{P}(\cdot)\) for each of two equivalent models, you typically get different results, i.e., minimizing \(\|Y-X_{1}\beta_{1}\|^{2}+\mathcal{P}(\beta_{1})\) does not give the same fitted values and residuals as minimizing \(\|Y-X_{2}\beta_{2}\|^{2}+\mathcal{P}(\beta_{2})\). Moreover, incorporating the penalty function typically generates unique estimates, even when ordinary least squares does not. This reparameterization issue is a substantial one when using software that provides a default penalty function or even a menu of penalty functions.

To get equivalent results from equivalent models you need appropriate penalty functions. In particular, if \(X_{1}\) and \(X_{2}\) are both regression models so that \(X_{1}=X_{2}T\) for some invertible matrix \(T\), then \(\beta_{2}=T\beta_{1}\) and minimizing \(\|Y-X_{1}\beta_{1}\|^{2}+\mathcal{P}(\beta_{1})\) clearly gives the same fitted values and residuals as minimizing \(\|Y-X_{2}\beta_{2}\|^{2}+\mathcal{P}(T^{-1}\beta_{2})\).

This discussion is particularly germane when applying the kernel trick as in Sect. 8.2. With two different reproducing kernels \(R_{1}(\cdot,\cdot)\) and \(R_{2}(\cdot,\cdot)\), for which \(C(\tilde{R}_{1})=C(\tilde{R}_{2})\), the models \(Y=\tilde{R}_{1}\gamma_{1}+e\) and \(Y=\tilde{R}_{2}\gamma_{2}+e\) are reparameterizations of each other. Their least squares fits will be identical and with many kernels \(Y=\tilde{Y}_{1}=\tilde{Y}_{2}\) (when the \(x_{i}\) are distinct). If, to avoid overfitting, we use an off the shelf penalty function \(\mathcal{P}(\cdot)\) to estimate the \(\gamma_{i}\)s, that common penalty function will be entirely responsible for the differences between the fitted values \(\tilde{Y}_{1}\equiv\tilde{R}_{1}\tilde{\gamma}_{1}\) and \(\tilde{Y}_{2}\equiv\tilde{R}_{2}\tilde{\gamma}_{2}\) and the differences in other predictions made with the two models.

#### Nonparametric Regression

As discussed in the previous chapter, one approach to nonparametric regression of \(y\) on a scalar predictor \(x\) is to fit a linear model, say,

\[y_{i}=\beta_{0}+\gamma_{1}\phi_{1}(x_{i})+\cdots+\gamma_{s}\phi_{s}(x_{i})+ \varepsilon_{i}\]

for known functions \(\phi_{j}\), e.g., polynomial regression. Penalized estimates are used in nonparametric regression to ensure smoothness. Penalized regression typically shrinks all regression estimates towards 0, some more than others when applied to nonparametric regression. Variable selection differs in that it shrinks the estimates of the eliminated variables to (not towards) 0 but lets least squares decide what happens to the estimates of the remaining variables.

The functions \(\phi_{j}(x)\) in nonparametric regression are frequently subjected to some form of standardization (such as \(\mathcal{L}^{2}\) normalization) when they are defined, thus obviating a strong need for further standardization of the vectors \(\Phi_{j}\equiv[\phi_{j}(x_{1}),\cdots,\phi_{j}(x_{n})]^{\prime}\), especially when the \(x_{i}\)s are equally spaced. For example, with \(x_{i}\)s equally spaced from 0 to 1 and \(\phi_{j}(x)=\cos(\pi jx)\), there is little need to standardize \(Z\equiv[\Phi_{1},\ldots,\Phi_{s}]\) further. When using simple polynomials \(\phi_{j}(x)=x^{j}\), the model matrix should be standardized. When using the corresponding _Legendre polynomials_ on equally spaced data, \(Z\) need not be.

In the context of nonparametric regression, not overfitting the model typically means ensuring appropriate smoothness. For example, with \(\phi_{j}(x)=\cos(\pi jx)\), when \(j\) is large the cosine functions oscillate very rapidly, leading to _nonsmooth_ or _noisy_ behavior. Typically, with linear-approximation approaches to nonparametric regression, large \(j\) is indicative of more noisy behavior. We want to allow noisy behavior if the data require it, but we prefer smooth functions if they seem reasonable. It therefore makes sense to place larger penalties on the regression coefficients for large \(j\). In other words, for large values of \(j\) we shrink the least squares estimate \(\hat{\gamma}_{j}\) towards 0 more than when \(j\) is small.

### 2.2 Ridge Regression

_Classical ridge regression_ provides one application of penalty functions. Write the multiple linear regression model \(y_{i}=\beta_{0}+\sum_{j=1}^{p-1}\beta_{j}x_{ij}+\varepsilon_{i}\) in matrix form as \(Y=X\beta+e\) or, following _PA_ Sect. 6.2, as

\[Y=J\beta_{0}+Z\beta_{*}+e=[J,Z]\begin{bmatrix}\beta_{0}\\ \beta_{*}\end{bmatrix}+e\]

where the elements of \(Z\) are \(x_{ij}\), \(i=1,\ldots,n\), \(j=1,\ldots,p-1\). Relative to model (2.1.1), \(J=X\), \(\beta_{0}=\beta\), and \(\beta_{*}=\gamma\).

Classical ridge regression estimates \(\beta_{0}\) and \(\beta_{*}\) by minimizing

\[[Y-J\beta_{0}-Z\beta_{*}]^{\prime}[Y-J\beta_{0}-Z\beta_{*}]+k\beta_{*}^{\prime }\beta_{*} \tag{2.2.1}\]which amounts to using the penalty function

\[\mathcal{P}_{R}(\beta)\equiv\beta_{*}^{\prime}\beta_{*}=\sum_{j=1}^{p-1}\beta_{j}^{ 2}.\]

Other than \(\beta_{0}\), this penalizes each \(\beta_{j}\) the same amount, so it is important that the columns of \(Z\) be standardized to a common length or that they be defined in such a way that they are already nearly standardized. It is easy to see that the function (2.2.1) is the least squares criterion function for the model

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}J\\ 0\end{bmatrix}\beta_{0}+\begin{bmatrix}Z\\ \sqrt{k}I\end{bmatrix}\beta_{*}+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}.\]

Using ANCOVA to fit this augmented model by least squares yields the classical ridge regression estimates

\[\tilde{\beta}_{R*}=\left\{Z^{\prime}[I-(1/n)JJ^{\prime}]Z+kl\right\}^{-1}Z^{ \prime}[I-(1/n)JJ^{\prime}]Y\quad\text{and}\quad\tilde{\beta}_{R0}=\bar{y}.- \bar{x}^{\prime}\tilde{\beta}_{R*}.\]

**Exercise 2.1**.: Prove the formulae for \(\tilde{\beta}_{R*}\) and \(\tilde{\beta}_{R0}\).

The augmented regression model shows quite clearly that ridge regression is shrinking the regression parameters toward \(0\). The bottom part of the augmented model specifies

\[0=\sqrt{k}\beta_{*}+\tilde{e},\]

so we are acting like \(0\) is an observation with mean vector \(\sqrt{k}\beta_{*}\), which will shrink the estimate of \(\beta_{*}\) toward the \(0\) vector. Note that if \(\sqrt{k}\) is already a very small number, then one expects \(\sqrt{k}\beta_{*}\) to be small, so the shrinking effect of the artificial observations \(0\) will be small. If \(\sqrt{k}\) is large, say \(1\), then we are acting like we have seen that \(\beta_{*}\) is near \(0\) and the shrinkage will be larger.

Like many sources, the discussion of classical ridge regression in _PA-V_ Sect. 13.3(Christensen 2011, 15.3) penalizes all the parameters and leaves any mean correction implicit, which results in a simpler ridge estimate. For model (2.1.4), minimizing (2.1.5) with \(\mathcal{P}(\beta)=\beta^{\prime}\beta\), the ridge estimate is \(\tilde{\beta}_{R}=(X^{\prime}X+kl)^{-1}X^{\prime}Y\), which can be arrived at by least squares fitting of

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}X\\ \sqrt{k}I\end{bmatrix}\beta+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}.\]

#### Generalized Ridge Regression

Returning to the partitioned model (2.1.1), _generalized ridge regression_ takes the form of a penalty

\[\mathcal{P}_{GR}(\gamma)\equiv\gamma^{\prime}Q\gamma,\]where \(Q\) is a nonnegative definite matrix. Most often \(Q\) is diagonal so that \(\mathcal{P}_{GR}(\gamma)=\sum_{j=1}^{s}q_{jj}\gamma_{j}^{2}\). In \(PA\)-\(V\) Sect. 13.3 (Christensen 2011, 15.3), the generalized ridge estimates used \(k_{j}\equiv k\,q_{jj}\).

We can minimize

\[(Y-X\beta-Z\gamma)^{\prime}(Y-X\beta-Z\gamma)+k\gamma^{\prime}Q\gamma \tag{2.2.2}\]

using the least squares fit to the augmented linear model

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}X&Z\\ 0&\sqrt{k}\tilde{Q}\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix} \tag{2.2.3}\]

where \(Q=\tilde{Q}^{\prime}\tilde{Q}\). Note that if the \(j\)th row and column of \(Q\) contain only \(0\)s, \(\gamma_{j}\) is not penalized. That can only happen when the \(j\)th column of \(\tilde{Q}\) is \(0\), in which case we could redefine \(\gamma_{j}\) as part of \(\beta\) and redefine \(Q\) eliminating the row and column of \(0\)s.

The least squares estimates of model (2.2.3) minimize the quantity

\[\begin{bmatrix}Y-X\beta-Z\gamma\\ -\sqrt{k}\tilde{Q}\gamma\end{bmatrix}^{\prime}\begin{bmatrix}Y-X\beta-Z\gamma \\ -\sqrt{k}\tilde{Q}\gamma\end{bmatrix}=(Y-X\beta-Z\gamma)(Y-X\beta-Z\gamma)+k \gamma^{\prime}Q\gamma.\]

Using ideas from \(PA\) Chap. 9 with \(M\) the perpendicular projection operator onto \(C(X)\), it is not difficult to show that

\[\tilde{\gamma}=[Z^{\prime}(I-M)Z+kQ]^{-}Z^{\prime}(I-M)Y;\qquad\tilde{\beta}=[ X^{\prime}X]^{-}X^{\prime}(Y-Z\tilde{\gamma}) \tag{2.2.4}\]

are least squares estimates for model (2.2.3) and thus are generalized ridge estimates. In most regression problems the generalized inverses in (2.2.4) can be replaced by true inverses.

Alternatively, when \(Q\) is nonsingular, we can minimize (2.2.2) using the generalized least squares fit to the augmented linear model

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}X&Z\\ 0&I\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix},\,\mathrm{E}\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}=\begin{bmatrix}0\\ 0\end{bmatrix},\,\mathrm{Cov}\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}=\sigma^{2}\begin{bmatrix}I&0\\ 0&(1/k)Q^{-1}\end{bmatrix}. \tag{2.2.5}\]

The generalized least squares estimates of model (2.2.5) minimize the quantity

\[\begin{bmatrix}Y-X\beta-Z\gamma\\ -\gamma\end{bmatrix}^{\prime}\begin{bmatrix}I&0\\ 0&kQ\end{bmatrix}\begin{bmatrix}Y-X\beta-Z\gamma\\ -\gamma\end{bmatrix}\\ =(Y-X\beta-Z\gamma)^{\prime}(Y-X\beta-Z\gamma)+k\gamma^{\prime}Q\gamma,\]

which is the generalized ridge regression criterion (2.2.2).

Green and Silverman (1994, Section 3.6) discuss different choices for \(Q\). Those choices generally follow the pattern of more shrinkage for \(\gamma_{j}\)s that incorporate noisier behavior into the model and are discussed in Sect. 2.2.1.

The generalized ridge augmented model (2.2.5) becomes a classical ridge augmented model when \(Q=I\). It then involves fitting the simplest form of generalized least squares which is weighted least squares wherein the weights vector is \(w^{\prime}=[J^{\prime}_{n},kJ^{\prime}_{s}]\). Finding a computer program to fit generalized least squares can be difficult but almost all regression software fits weighted least squares. This idea will be useful in Chap. 13 when we generalize ridge regression to binomial generalized linear models.

Section 5.3 examines a relationship between ridge regression and fitting models with random effects.

#### Picking \(k\)

As alluded to in Sect. 2.1, to pick \(k\) in (2.2.1) for classical ridge regression, Hoerl and Kennard (1970) suggested using a _ridge trace_. If the classical ridge regression estimates are denoted \(\tilde{\beta}_{kj}\) for the \(j\)th predictor variable and tuning parameter \(k\), the ridge trace is, for all \(j\), a simultaneous plot of the curves defined by \((k,\tilde{\beta}_{kj})\) as \(k\) varies. For \(k=0\) the \(\tilde{\beta}_{0j}\)s are the least squares estimates and as \(k\) increases they all shrink towards \(0\) (except \(\beta_{k0}\) which is not penalized). The idea is to pick \(k\) just big enough to stabilize the regression coefficients. Hoerl and Kennard's original idea was using ridge to deal with high collinearity in \([I-(1/n)JJ^{\prime}]Z\), rather than using shrinkage as an alternative to variable selection. As mentioned earlier, the trace idea applies to all penalized regression but Draper and van Nostrand (1979) found it lacking for ridge regression.

More recently, cross-validation and generalized cross-validation have been used to pick \(k\), for example see Green and Silverman (1994, Sections 3.1 and 3.2).

#### Nonparametric Regression

Again we use the notation from Chap. 1 for simple nonparametric regression but the ideas extend immediately to multiple nonparametric regression.

Assuming that \(f(x)=\sum_{j=0}^{p-1}\beta_{j}\phi_{j}(x)\), the generalized ridge regression estimate minimizes

\[(Y-\Phi\beta)^{\prime}(Y-\Phi\beta)+k\beta^{\prime}Q\beta,\]

where \(Q\) is a nonnegative definite matrix of penalties for unsmoothness and \(k\) is a tuning parameter which for many purposes is considered fixed but which ultimately is estimated. It seems most common not to penalize the intercept parameter when one exists, but that is optional.

In the special case in which \(\frac{1}{\sqrt{n}}\Phi\) has orthonormal columns and \(Q\) is diagonal, it is not difficult to see that the generalized ridge estimate is

\[\tilde{\beta} = [nI+kD(q_{ii})]^{-1}\Phi^{\prime}Y\] \[= [D(n+kq_{ii})]^{-1}\Phi^{\prime}Y\] \[= D\left(\frac{n}{n+kq_{ii}}\right)\hat{\beta},\]where \(\hat{\beta}\) is the least squares estimate. By letting \(\alpha=k/n\), we get

\[\tilde{\beta}_{j}=\frac{1}{1+\alpha q_{ii}}\hat{\beta}_{j},\]

which shows quite clearly the nature of the shrinkage.

A frequently used penalty matrix (that does not seem to require \(\Phi\) to have columns of near equal length) has

\[Q=[q_{rs}],\qquad q_{rs}=\int_{0}^{1}\phi_{r}^{(2)}(x)\phi_{s}^{(2)}(x)dx\]

with \(\phi_{r}^{(2)}(x)\equiv\mathbf{d}^{2}\phi_{r}(x)\) is the second derivative of \(\phi_{r}(x)\). This penalty function does not depend on the data \((X,Y)\). Whenever \(\phi_{0}\equiv 1\), \(\phi_{0}^{(2)}\equiv 0\), so the first row and column of the second derivative \(Q\) will be \(0\). This places no penalty on the intercept and we could choose to think of penalizing a partitioned model.

Clearly, any constant multiple of the matrix \(Q\) works equivalently to \(Q\) if we make a corresponding change to \(k\). If we use the cosine basis of (1.2.2), the second derivative matrix is proportional to

\[Q=\text{Diag}(0,1^{4},2^{4},\ldots,(s-1)^{4}).\]

For the sines and cosines of (1.2.3),

\[Q=\text{Diag}(0,1^{4},1^{4},2^{4},2^{4},\ldots).\]

In both cases, we have the diagonal matrix form of generalized ridge regression. Moreover, it is clear that the terms getting the greatest shrinkage are the terms with the largest values of \(j\) in (1.2.2) and (1.2.3), i.e., the highest frequency terms.

Example 2.2.1.: For the voltage drop data of Chap. 1, using cosines with \(j=0,\ldots,10\) and least squares, the estimated regression equation is

\[y=11.4-1.61c_{1}-3.11c_{2}+0.468c_{3}+0.222c_{4}+0.196c_{5}\\ +0.156c_{6}+0.0170c_{7}+0.0799c_{8}+0.0841c_{9}+0.148c_{10}.\]

Using the generalized ridge regression augmented model

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}\Phi\\ \sqrt{k}\tilde{Q}\end{bmatrix}\beta+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}\]

with \(\sqrt{k}=0.2\) and \(\tilde{Q}=\text{Diag}(0,1,4,9,\ldots,100)\), the estimated regression equation is

\[y=11.4-1.60c_{1}-3.00c_{2}+0.413c_{3}+0.156c_{4}+0.0925c_{5}\\ +0.0473c_{6}+0.0049c_{7}+0.0102c_{8}+0.0068c_{9}+0.0077c_{10}.\]Note the shrinkage towards \(0\) of the coefficients relative to least squares, with more shrinkage for higher values of \(j\).

Figure 1 gives the data along with the generalized ridge regression fitted cosine curve using \(j=0,\ldots,10\). With \(k=0.04\), the plot is very similar to the unpenalized cosine curve using \(j=0,\ldots,6\) which is also plotted. Defining \(R^{2}\) as the squared correlation between the observations and the fitted values, cf. _PA_ Chap. 6, the ridge regression gives \(R^{2}=0.985\) which is less than the value \(0.988\) from the least squares fit with \(6\) cosines. 

#### Exercise 2.2

Perform a classical ridge regression with equal weights on the \(10\) term cosine model and compare the results to those from Example 2.2.1.

The second derivative penalty approach is worthless for Haar wavelets because it gives \(Q=0\) and thus the least squares estimates. Theoretically, one could use penalized least squares with other wavelets. The integral of the product of the second derivatives would be difficult to find for many wavelets. Fitting functions with small support inherently makes the fitted functions less smooth. Choosing how small to make the supports, e.g. choosing how many wavelets to fit, is already a choice of

Figure 1: Solid: Generalized ridge regression cosine fit with \(k=0.04\), \(s-1=10\) and second derivative weights for the battery data. Dashed: Least squares cosine fit with \(s-1=6\)

how smooth to make the fitted function. In such cases a smoothing penalty associated with \(\phi_{j}\) should increase as the size (length, area, volume) of the support of \(\phi_{j}\) gets smaller.

**Exercise 2.3**.: Find the second derivative penalty function matrix \(Q\) for the cubic spline model of the form (1.7.2) with 3 interior knots. Find \(Q\) for the corresponding linear spline model.

In Sect. 1.7.1 we discussed connecting the dots using splines by fitting a linear model \(\mathrm{E}(Y)=X\beta\) that incorporates linear constraints. We also discussed the reparameterized reduced model \(\mathrm{E}(Y)=X_{0}\gamma\) that incorporated the spline smoothness linear constraints into the original linear model. To introduce more smoothing (i.e., smoothing at the observed data), we can perform generalized ridge regression on the reduced model to obtain

\[\tilde{\gamma}=(X_{0}^{\prime}X_{0}+kQ)^{-1}X_{0}^{\prime}Y.\]

Note that when we back transform to \(\tilde{\beta}=U\tilde{\gamma}\), \(\tilde{\beta}\) satisfies all of the constraints necessary to make the fitted spline function smooth. Green and Silverman (1994, Section 2.1) give an appropriate matrix \(Q\) based on a roughness penalty. (\(Q=X_{0}^{\prime}KX_{0}\), where they define \(K\) in their Eq. (1.4).)

If \(x\) is a vector, the second derivative of \(\phi_{j}(x)\) is a square matrix as is the product of the second derivatives for different \(j\). One might use something like the determinant of the integral of the matrix product to define \(Q\).

### Lasso Regression

Currently, a very popular penalty function is Tibshirani's (1996) _lasso (least absolute shrinkage and selection operator)_,

\[\mathcal{P}_{L}(\gamma)\equiv\sum_{j=1}^{s}|\gamma_{j}|\equiv\|\gamma\|_{1}. \tag{2.3.1}\]

The book by Hastie, Tibshirani, and Wainwright (2015) provides a wealth of information on this procedure. For applications with \(p+s>n\) see Buhlmann and van de Geer (2011) or Buhlmann, Kalisch, and Meier (2014).

Because the lasso penalty function is not a quadratic form in \(\gamma\), unlike ridge regression the estimate cannot be obtained by fitting an augmented linear model. Lasso estimates can be computed efficiently for a variety of values \(k\) using a modification of the _LARS_ algorithm of Efron et al. (2004).

Less computationally efficient than LARS, but easier to understand, is an algorithm that involves obtaining estimates \(\tilde{\beta}^{h+1}\), \(\tilde{\gamma}^{h+1}\) for model (2.1.1) by repeatedly fitting

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}X&Z\\ 0&I\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix},\,\mathrm{E}\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}=\begin{bmatrix}0\\ 0\end{bmatrix},\,\mathrm{Cov}\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}=\sigma^{2}\begin{bmatrix}I&0\\ 0&(1/k)Q_{h}^{-1}\end{bmatrix},\]wherein \(Q_{h}\) is a diagonal matrix with elements \(q_{jj}=1/|\hat{\gamma}_{j}^{h}|\). This has a penalty function

\[\mathcal{P}(\gamma)=\sum_{j=1}^{s}\frac{\gamma_{j}^{2}}{|\hat{\gamma}_{j}^{h}|} \doteq\sum_{j=1}^{s}|\gamma_{j}|.\]

When \(|\hat{\gamma}_{j}^{h}|\) gets small, \(\gamma_{j}\) becomes very highly penalized, thus forcing \(\hat{\gamma}^{h+1}\) even closer to 0.

As the actual name (not the acronym) suggests, one of the benefits of the lasso penalty is that it automates variable selection. Rather than gradually shrinking all regression coefficients towards 0 like ridge regression, lasso can make some of the regression coefficients collapse to 0.

Just as _PA-V_ Sect. 13.3 (Christensen 2011, 15.3) used canonical regression to explore ridge estimation, we can use canonical regression to explicate the behavior of lasso regression. Rather than analyzing model (2.1.1), consider the standard linear model \(Y=X\beta+e\) and its canonical version

\[Y_{*}=\begin{bmatrix}L\\ 0\end{bmatrix}\gamma+e,\]

where \(L=D(\lambda_{j})\) is positive definite. Applying the lasso criterion to canonical regression we need to minimize

\[\sum_{j=1}^{p}\left[(y_{*j}-\lambda_{j}\gamma_{j})^{2}+k|\gamma_{j}|\right].\]

Because of the simple structure of canonical regression, the lasso criterion acts independently on each coefficient. Without loss of generality, assume \(y_{*j}>0\). Clearly, \(\gamma_{j}<0\) will not minimize the criterion, because \(\gamma_{j}=0\) will be better. We therefore want to minimize \((y_{*j}-\lambda_{j}\gamma_{j})^{2}+k\gamma_{j}\) for \(\gamma_{j}\geq 0\).

With \(\hat{\gamma}_{j}=y_{*j}/\lambda_{j}\) being the least squares estimate, a little bit of work shows that the derivative of the criterion function with respect to \(\gamma_{j}\) is zero at \(\gamma_{j}=\hat{\gamma}_{j}-k/2\lambda_{j}^{2}\). However, if \(\hat{\gamma}_{j}<k/2\lambda_{j}^{2}\), the critical point is outside the domain of the function, so the minimum must occur at the boundary. Therefore, the lasso estimate is

\[\tilde{\gamma}_{Lj}=\begin{cases}\hat{\gamma}_{j}-k/2\lambda_{j}^{2},&\text{ if }\hat{\gamma}_{j}\geq k/2\lambda_{j}^{2}\\ 0,&\text{ if }|\hat{\gamma}_{j}|<k/2\lambda_{j}^{2}\\ \hat{\gamma}_{j}+k/2\lambda_{j}^{2},&\text{ if }\hat{\gamma}_{j}\leq-k/2 \lambda_{j}^{2}\end{cases}. \tag{2.3.2}\]

Clearly, if the least squares estimate is too close to 0, the lasso estimate is zero and the variable is effectively removed from the model.

The lasso penalty (2.3.1) treats every coefficient the same. An obvious modification of lasso to penalize coefficients at different rates has

\[\mathcal{P}_{GL}(\gamma)=\sum_{j=0}^{s}q_{jj}|\gamma_{j}|\]

with \(q_{jj}\) often increasing in \(j\).

Christensen (2015, Section 10.5) contains an example of the lasso applied to a 5 predictor regression problem. (The _Coleman Report_ data.) Here we illustrate its use in nonparametric regression.

For the battery data of Chap. 1, Fig. 2 shows the least squares cosine fits for \(p-1=6\), 30 and the R package lasso2's default fit except that, with equally spaced cosine predictors, the predictors were not standardized. (I also looked at the standardized version and it made little difference.) The default lasso fit has \(k=12.2133\), which is a lot of shrinkage. (The default is actually \(\delta=0.5\|\hat{\beta}_{*}\|_{1}\) where \(\hat{\beta}_{*}\) is the \(p-1=30\) least squares estimate vector without the intercept, \(\|\hat{\beta}_{*}\|_{1}\equiv\sum_{j=1}^{30}|\hat{\beta}_{j}|\), and \(\delta\) is defined in Sect. 2.5.)

The default is a shockingly bad fit. It gives \(R^{2}=0.951\), which is poor for this problem. It has zeroed out too many of the cosine terms. A more reasonable lasso fit is given in Fig. 3. The fit in Fig. 3 has nonzero coefficients on precisely the first six cosine terms (and the constant) and it gives \(R^{2}=0.981\), which cannot be greater than the \(R^{2}\) provided by the least squares fit on the six cosine terms. Unlike our ridge regression example, in neither of the lasso fits have we put larger penalties on more noisy variables. 

**Exercise 2.4**: Fit the 30 cosine model using generalized lasso with the second derivative weights \(q_{jj}\) defined as in the ridge regression section.

### 2.4 Bayesian Connections

The augmented model (2.2.3) used to find ridge regression estimates is a special case of the augmented model used to fit Bayesian linear models in _PA-V_ Section 2.10 (Christensen 2011, Section 2.9), see also Christensen, Johnson, Branscum, and Hanson (2010). Quite generally, penalized least squares estimates can be viewed as the mode of the posterior distribution of \(\beta\) when the penalty function is used to determine a specific prior distribution for \(\beta\) given \(\sigma^{2}\). We begin by looking at a standard linear model and conclude with the partitioned model (2.1.1).

The likelihood function for \(Y=X\beta+e\) with independent homoscedastic normal data is

\[L(\beta,\sigma^{2})=(2\pi)^{-n/2}[\det(\sigma^{2}I)]^{-1/2}\exp\left[-(Y-X\beta )^{\prime}(Y-X\beta)/2\sigma^{2}\right].\]

We take a (possibly improper) prior density of the form \(\pi(\beta,\sigma^{2})\equiv\pi_{1}(\beta|\sigma^{2})\pi_{2}(\sigma^{2})\) where the conditional density of \(\beta\) given \(\sigma^{2}\) is written as

\[\pi_{1}(\beta|\sigma^{2})=h(\sigma^{2})\exp\left[-k\mathcal{P}(\beta)/2\sigma^ {2}\right],\]

with \(\mathcal{P}(\beta)\) once again being the nonnegative penalty function. The posterior is proportional to the likelihood times the prior, so it has the form Figure 2: Solid: Lasso cosine fit with \(k=12.2133\) (\(\delta=0.5\|\hat{\beta}_{\ast}\|_{1}\)), \(p-1=30\). Dot-dash: Least squares cosine fit with \(p-1=30\). Dashed: Least squares cosine fit with \(p-1=6\)

Figure 3: Solid: Lasso cosine fit with \(k=3.045046\) (\(\delta=0.7\|\hat{\beta}_{\ast}\|_{1}\)), \(p-1=30\). Dot-dash: Least squares cosine fit with \(p-1=30\). Dashed: Cosine least squares fit with \(p-1=6\)

\[\pi(\beta,\sigma^{2}|Y)\propto\left(\sigma^{2}\right)^{-n/2}\pi_{2}( \sigma^{2})h(\sigma^{2})\times\\ \exp\left\{-\frac{1}{2\sigma^{2}}\left[(Y-X\beta)^{\prime}(Y-X \beta)+k\mathcal{P}(\beta)\right]\right\}.\]

The posterior mode consists of the values \(\tilde{\beta}\), \(\tilde{\sigma}^{2}\) that maximize the posterior. Similar to finding maximum likelihood estimates in a standard linear model, \(\tilde{\beta}\) minimizes \([(Y-X\beta)^{\prime}(Y-X\beta)+k\mathcal{P}(\beta)]\) regardless of the value of \(\sigma^{2}\). Thus the posterior mode of \(\beta\) is also the penalized least squares estimate. A further prior can be placed on \(k\).

Ridge regression amounts to placing a normal prior on \(\beta\) and using the one number that is the posterior mean, median, and mode as an estimate of \(\beta\). In particular, the generalized ridge estimate devolves from the prior distribution

\[\beta|\sigma^{2}\sim N\left(0,\frac{\sigma^{2}}{k}Q^{-1}\right).\]

When \(Q\) is diagonal, large penalties clearly correspond to small prior variances, i.e., strong prior beliefs that \(\beta_{j}\) is near the prior mean of \(0\). Classical ridge regression uses independent, homoscedastic, mean \(0\) normal priors on the \(\beta_{j}\)s given \(\sigma\), i.e., \(\beta_{j}\sim N(0,\sigma^{2}k)\), except possibly a flat prior on the intercept \(\beta_{0}\).

Lasso-regression estimates are the posterior mode of \(\beta\) when placing independent, homoscedastic, mean \(0\) Laplace (double exponential) priors on the \(\beta_{j}\)s given \(\sigma\), except possibly a flat prior on the intercept \(\beta_{0}\). Given the discontinuous nature of the lasso minimization problem, it is not surprising that technical difficulties can arise. Park and Casella (2008) provide a good discussion, but use a slightly different prior.

An alternative Bayesian method for avoiding overfitting is _thresholding_, see Smith and Kohn (1996), Clyde and George (2004), or Christensen et al. (2010, Section 15.2). The idea is to put positive prior probability on each regression coefficient being \(0\), so there will be positive, although perhaps very small, posterior probability of it being \(0\). For example, with a linear-approximation nonparametric regression model, a form of generalized ridge regression corresponds to independent normal priors on \(\beta_{j}\) with mean \(0\) and a variance \(\sigma^{2}/k\,q_{jj}\), decreasing in \(j\) when high \(j\) indicates noisier behavior. Instead, a thresholding model prior might write

\[\beta_{j}\equiv\delta_{j}\beta_{j}^{*}\]

with \(\beta_{j}^{*}\sim N(0,\sigma^{2}/k)\) independent of \(\delta_{j}\sim\text{Bern}(p_{j})\), i.e., \(\Pr[\delta_{j}=1]=p_{j}=1-\Pr[\delta_{j}=0]\) where \(p_{j}\) decreases with \(j\). This obviously makes it harder, but not impossible, for \(\beta_{j}\) to be nonzero as \(j\) increases (unless \(p_{j}=0\)).

For the partitioned model (2.1.1) with multivariate normal data,

\[L(\beta,\gamma,\sigma^{2})=(2\pi)^{-n/2}[\det(\sigma^{2}I)]^{-1/2}\exp\left[- (Y-X\beta-Z\gamma)^{\prime}(Y-X\beta-Z\gamma)/2\sigma^{2}\right].\]We take a prior density of the form \(\pi(\beta,\gamma,\sigma^{2})\equiv\pi_{1}(\gamma|\sigma^{2})\pi_{2}(\sigma^{2})\) where the conditional density of \(\gamma\) given \(\sigma^{2}\) is written as

\[\pi_{1}(\gamma|\sigma^{2})=h(\sigma^{2})\exp\bigl{[}-k\,\mathcal{P}(\gamma)/2 \sigma^{2}\bigr{]}\,,\]

with \(\mathcal{P}(\gamma)\) once again being the nonnegative penalty function. Implicitly, this prior puts an improper flat prior on \(\beta|\gamma,\sigma^{2}\), i.e., \(\pi(\beta|\gamma,\sigma^{2})\) is a constant. Again, the posterior mode equals the penalized least squares estimate.

### Another Approach

For an unpartitioned linear model we can think about penalized least squares estimation as minimizing

\[\|Y-X\beta\|^{2}+k\,\mathcal{P}(\beta) \tag{2.5.1}\]

for some tuning parameter \(k\geq 0\). Alternatively, the procedure can be defined as choosing \(\beta\) to minimize the least squares criterion

\[\|Y-X\beta\|^{2} \tag{2.5.2}\]

subject to a restriction on the regression coefficients,

\[\mathcal{P}(\beta)\leq\delta. \tag{2.5.3}\]

We want to establish the equivalence of these two procedures and explore the geometry of the alternative procedure. In penalized regression, we do not have a good reason for choosing any particular \(\delta\) in (2.5.3), so we look at all possible values of \(\delta\) or, more often and equivalently, all possible values of \(k\) in (2.5.1). In practice we will want to introduce the refinements associated with model (2.1.1) but for simplicity we examine the unpartitioned model.

#### Geometry

The restricted least squares problem of minimizing (2.5.2) subject to the inequality constraint (2.5.3) lends itself to a geometric interpretation. Our discussion is reasonably general but most illustrations are of the lasso in two dimensions. For simplicity, we examine a standard linear model \(Y=X\beta+e\) but in practice penalized regression is always applied to some version of the partitioned model (2.1.1), even if the first term in the partition corresponds only to an intercept term.

To explore the geometry, we want to facilitate our ability to create contour maps of the least squares criterion surface as a function of \(\beta\). Using the decomposition of \(\|Y-X\beta\|^{2}\) given in (2.1.6), rewrite (2.5.2) in terms of a quadratic function of \(\beta\) minimized at the least squares estimate \(\hat{\beta}\) plus a constant. The first term of the last Figure 4: Lasso shrinkage without variable selection

Figure 5: Lasso shrinkage and variable selection

line in (2.1.6) is the constant that does not depend on \(\beta\) and the second term, since it is a quadratic function, has contours that are ellipsoids in \(\beta\) centered at \(\hat{\beta}\). The function minimum is at \(\hat{\beta}\), for which the function value is \(SSE\). The contours are ellipsoids in \(\beta\) for which

\[SSE+(\beta-\hat{\beta})^{\prime}X^{\prime}X(\beta-\hat{\beta})=D\]

for some \(D\). As \(D\) gets larger, the contours get farther from \(\hat{\beta}\). The geometry of ellipsoids is discussed more in Sect. 14.1.3. The shape of an ellipsoid is determined by the eigenvalues and eigenvectors of \(X^{\prime}X\)--the major axis is in the direction of the eigenvector with the largest eigenvalue and is proportional in length to the square root of the eigenvalue. Note that, with multivariate normal data, each ellipsoid is also the confidence region for \(\beta\) corresponding to some confidence level. The least squares estimate subject to the constraint (2.5.3) is a \(\beta\) vector on the smallest elliptical contour that intersects the region defined by (2.5.3). Of course if the least squares estimates already satisfy (2.5.3), there is nothing more to find.

If the least squares estimate does not already satisfy (2.5.3), a multiple regression lasso that penalizes the intercept minimizes

\[SSE+(\beta-\hat{\beta})^{\prime}X^{\prime}X(\beta-\hat{\beta})\]

subject to

\[\sum_{j=0}^{p-1}|\beta_{j}|=\delta.\]

We need a \(\beta\) vector on the smallest elliptical contour that intersects the region \(\sum_{j=0}^{p-1}|\beta_{j}|=\delta\). Where that intersection occurs depends on the value of \(\hat{\beta}\), the orientation of the ellipsoid, and the size of \(\delta\).

For \(y_{i}=\beta_{1}x_{i1}+\beta_{2}x_{i2}+\epsilon_{i}\), the lasso penalty constraint \(|\beta_{1}|+|\beta_{2}|\leq\delta\) is a square (diamond) centered at (0,0) with diameter \(2\delta\). To find the lasso estimate, grow the ellipses centered at \(\hat{\beta}\) until they just touch the edge of the square. The point of contact is the lasso estimate, i.e., the point that has the minimum value of the least squares criterion (2.5.2) subject to the penalty constraint (2.5.3). The point of contact can either be on the face of the square, as illustrated in Fig. 2.4, or it can be a corner of the square as in Fig. 2.5. When the contact is on a corner, one of the regression estimates has been zeroed out. In Fig. 2.5, \(\delta=1\), the lasso estimate of \(\beta_{1}\) is 0 and the lasso estimate of \(\beta_{2}\) is 1. For classical ridge regression, the diamonds in the two figures are replaced by circles of radius 1. Using a circle would definitely change the point of contact in Fig. 2.4 and almost certainly change the point of contact in Fig. 2.5.

#### More Lasso Geometry

In two dimensions the lasso estimate feels easy to find. With \(\delta=1\) and \(\hat{\beta}\) in the first quadrant like it is in the two figures, the lasso estimate feels like it should be \((1,0)^{\prime}\) or \((0,1)^{\prime}\) or it should be the least squares estimate subject to the linear constraint \(\beta_{1}+\beta_{2}=1\). Finding the least squares estimate subject to a linear equality constraint is straightforward. It is precisely what one needs to do to find spline estimates as in the previous chapter. Unfortunately, things are not that simple. Figure 6 shows that it is possible for the lasso estimate of \(\beta_{1}\) to be negative even when the least squares estimate is positive. And things get much more complicated in higher dimensions.

Figures 7, 8 and 9 illustrate the geometry behind a trace plot. Remember that varying \(\delta\) is equivalent to varying \(k\), although the exact relationship is not simple. With both least squares \(\hat{\beta}_{j}\)s positive as in Fig. 7 and with \(\delta\) large enough, the lasso estimate is just the least squares estimate constrained to be on \(\beta_{1}+\beta_{2}=\delta\), unless \(\delta\) is big enough that \(\hat{\beta}_{1}+\hat{\beta}_{2}\leq\delta\) in which case least squares is lasso. Figure 8 has smaller \(\delta\)s than Fig. 7 but both plots in its top row have the same \(\delta\) with the second plot being a closeup. The bottom row of Fig. 8 shows that as \(\delta\) decreases, the penalized estimates remain on \(\hat{\beta}_{1}+\hat{\beta}_{2}=\delta\) until \(\beta_{1}\) becomes 0. The top row of

Figure 6: Lasso sign changeFig. 9 has the lasso estimate of \(\beta_{1}\) zeroed out for two additional \(\delta\)s. The bottom row shows the lasso estimate becoming negative.

In three dimensions the lasso geometry is of throwing an American football (or a rugby ball) at an octohedron. Technically, the football should have someone sitting on it and, instead of throwing the football, we should blow it up until it hits the octohedron. The squashed football denotes the ellipsoids of the least squares criterion. The octohedron, see Fig. 10, is the lasso penalty region and should be centered at 0. The octohedron has 8 sides consisting of isosceles triangles, 12 edges between the sides, and 6 corners. The football can hit any of these 26 features. If we knew which of the 26 features the ellipsoid was hitting, it would be easy to find the restricted least squares estimate because it would be the least squares estimate subject to a set of linear constraints. The problem is knowing which of the 26 features the ellipsoid is hitting. If the ellipsoid hits a corner, two regression estimates are zeroed out and the third takes a value \(\pm\delta\). If it hits an edge, one estimate is zeroed out and the other two are shrunk towards but not to 0. If it hits a surface, no estimates are zeroed but all are shrunk.

Typically, if you make \(\delta\) big enough \(\mathcal{P}(\hat{\beta})\leq\delta\), so the penalty function has no effect on the least squares estimates. As soon as \(\delta<\mathcal{P}(\hat{\beta})\), penalized least squares should be different from least squares. For the lasso, if all the elements of \(\hat{\beta}\) are positive and \(\delta\) is below, but sufficiently close to, \(\sum_{j=0}^{p-1}|\hat{\beta}_{j}|\), the lasso estimate equals the least squares estimate subject to the linear constraint \(\sum_{j=0}^{p-1}\beta_{j}=\delta\). More generally, the pattern of positive and negative values in \(\hat{\beta}\) determines the pattern of positive and negative values in the linear constraint, i.e., \(\sum_{j=0}^{p-1}\operatorname{sign}(\hat{\beta}_{j})\,\beta_{j}=\delta\). As you continue to decrease \(\delta\), the penalized least squares estimates gradually change, continuing to satisfy the constraint \(\sum_{j=0}^{p-1}\operatorname{sign}(\hat{\beta}_{j})\,\beta_{j}=\delta\) until one of the estimates satisfies an additional linear constraint associated with \(\sum_{j=0}^{p-1}\pm\hat{\beta}_{j}=\delta\), one that changes only one coefficient sign from the original constraint, so that together they cause the coefficient with the sign change to be zero. As \(\delta\) further decreases, typically both linear constraints continue to hold for a while and then it is possible that the first linear constraint is supplanted by the second one.

Figure 2.8: Lasso trace

Figure 10: Octohedron

Figure 9: Lasso trace

[MISSING_PAGE_FAIL:101]

\[F_{\delta}(\tilde{\beta}(k_{0}),k_{0})=\min_{\beta,k}F_{\delta}(\beta,k)\]

and \(f(\tilde{\beta}(k_{0}))\) minimizes \(f(\beta)\) subject to \(\mathcal{P}(\beta)=\delta\).

All of this was for fixed \(\delta\) with \(k\) a variable. Picking \(\delta\) and picking \(k\) are interchangable. For every value of the tuning parameter \(k\), say \(k_{0}\), there exists a \(\delta_{0}\) for which \(\tilde{\beta}(k_{0})\) is the optimal estimate, namely \(\delta_{0}=\mathcal{P}(\tilde{\beta}(k_{0}))\). In this case \(F_{\delta_{0}}(\tilde{\beta}(k_{0}),k_{0})\) is the global minimum of \(F_{\delta_{0}}(\beta,k)\) so that \(f(\tilde{\beta}(k_{0}))\) minimizes \(f(\beta)\) on the boundary \(\delta_{0}=\mathcal{P}(\beta)\).

Ideally, we would find \(\tilde{\beta}(k)\) by setting equal to \(0\) the partial derivative with respect to \(\beta\) of \(F_{\delta}(\beta,k)\). This requires a solution to

\[\mathbf{d}_{\beta}f(\beta)+k\mathbf{d}_{\beta}\mathcal{P}(\beta)=0 \tag{2.5.4}\]

for fixed \(k\). Unfortunately, in applications like lasso regression, the function \(\mathcal{P}\) is not differentiable everywhere, so we cannot rely on this to get us a lasso solution.

In practice, we look at all solutions \(\tilde{\beta}(k)\).

Consider the simplest practical version of lasso. For a multiple regression model with an intercept take

\[\mathcal{P}_{L}(\beta)=\sum_{j=1}^{p-1}|\beta_{j}|.\]

Unless the least squares estimate \(\tilde{\beta}\) already has \(\sum_{j=1}^{p-1}|\tilde{\beta}_{j}|\leq\delta\), the minimum will occur at a \(\beta\) vector with \(\sum_{j=1}^{p-1}|\beta_{j}|=\delta\). As discussed, it is equivalent to minimize

\[\|Y-X\beta\|^{2}+k\left[\sum_{j=1}^{p-1}|\beta_{j}|-\delta\right]\]

with respect to \(k\) and \(\beta\) and the lasso estimate \(\tilde{\beta}(k)\) is found by minimizing

\[\|Y-X\beta\|^{2}+k\sum_{j=1}^{p-1}|\beta_{j}|.\]

The squared error function \(f(\beta)\equiv\|Y-X\beta\|^{2}\) is very well behaved but our lasso restricting function \(\mathcal{P}_{L}(\beta)\equiv\sum_{j=1}^{p-1}|\beta_{j}|\), while continuous, has points where the derivatives are not defined, namely, anywhere that a \(\beta_{j}=0\), \(j\neq 0\). Indeed, the whole point of lasso regression is that the minimums often occur at points where the derivatives do not exist. What this does is make the task of programming a solution \(\tilde{\beta}(k)\) much more difficult because you have to check for minimums wherever the derivatives do not exist. And in lasso regression, that is where we like to find them, at \(k\) values where many of the \(\tilde{\beta}_{j}(k)\)s are \(0\).

Some computer programs (including lasso2) redefine the restriction

\[\mathcal{P}(\beta)\leq\delta\]as

\[\mathcal{P}(\beta)\leq\eta\,\mathcal{P}(\hat{\beta})\]

where \(0\leq\eta\leq 1\) and \(\hat{\beta}\) is the least squares estimate. The beauty of this redefinition is that for \(\eta<1\), the minimizer of \(f(\beta)\) cannot occur in the interior, it must occur on the boundary (provided \(\mathcal{P}(\hat{\beta})>0\)). For \(\eta=1\), the least squares estimate will be the penalized estimate (so \(k=0\)). In particular, for lasso regression with an intercept,

\[\sum_{j=1}^{p-1}|\beta_{j}|\leq\delta\]

is redefined as

\[\sum_{j=1}^{p-1}|\beta_{j}|\leq\eta\,\sum_{j=1}^{p-1}|\hat{\beta}_{j}|\]

where \(0\leq\eta\leq 1\). For \(\eta=1\), the least squares estimates will be the lasso estimates. For \(\eta=0\), the estimates of \(\beta_{1},\ldots,\beta_{p-1}\) are all required to be 0, so the lasso estimate will be the least squares estimate for the intercept only model (so, essentially, \(k=\infty\)). lasso2's default is \(\eta=0.5\).

For generalized linear models, we could replace \(f(\beta)=\|Y-X\beta\|^{2}\) with the negative log-likelihood, add a penalty function, and similar arguments continue to hold providing _maximum penalized likelihood estimates_.

### Two Other Penalty Functions

For the partitioned model (2.1.1) recent approaches to regularization minimize

\[(Y-X\beta-Z\gamma)^{\prime}(Y-X\beta-Z\gamma)+\mathcal{P}_{\theta}(\gamma),\]

where \(\theta\) is a vector of tuning parameters involved in defining the penalty function \(\mathcal{P}_{\theta}(\gamma)\).

The _Elastic Net_ penalty combines the ridge and lasso penalties but incorporates another tuning parameter \(\alpha\in[0,1]\),

\[\mathcal{P}_{EN}(\gamma)\equiv\alpha\mathcal{P}_{R}(\gamma)+(1-\alpha) \mathcal{P}_{L}(\gamma)=\alpha\sum_{j=1}^{s}\gamma_{j}^{2}+(1-\alpha)\sum_{j= 1}^{s}|\gamma_{j}|.\]

Thus \(\mathcal{P}_{\theta}(\gamma)=k\mathcal{P}_{EN}(\gamma)\) with \(\theta=(k,\alpha)^{\prime}\).

Fan and Li (2001) suggested the _SCAD (Smoothly Clipped Absolute Deviation)_ penalty. The advantage of SCAD is that, like the LASSO, it shrinks small estimates to zero but unlike LASSO, it does not shrink large estimates at all. The penalty function is

\[\mathcal{P}_{S}(\gamma)\equiv\sum_{j=1}^{s}P_{S}(\gamma_{j}),\]where for \(a>2\),

\[P_{S}(\gamma_{j})\equiv\left\{\begin{array}{ll}|\gamma_{j}|&\mbox{if }|\gamma_{j}| \leq k,\\ -\left(\frac{|\gamma_{j}|^{2}-2ak|\gamma_{j}|+k^{2}}{2(a-1)k}\right)&\mbox{if }k<| \gamma_{j}|\leq ak,\\ \frac{(a+1)k}{2}&|\gamma_{j}|>ak.\end{array}\right.\]

The SCAD penalty function depends not only on a new tuning parameter \(a\) but also on the original tuning parameter \(k\), so \(\mathcal{P}_{\theta}(\gamma)=k\mathcal{P}_{S}(\gamma)\) with \(\theta=(k,a)^{\prime}\). Fan and Li suggest that \(a=3.7\) often works well.

When \(X\) is vacuous and the columns of \(Z\) are orthonormal, it can be shown that SCAD results in the following modifications to the least squares estimates \(\hat{\gamma}_{j}\),

\[\hat{\gamma}_{Sj}=\left\{\begin{array}{ll}0,&\mbox{if }|\hat{\gamma}_{j}| \leq k,\\ \hat{\gamma}_{j}-\mbox{sign}(\hat{\gamma}_{j})\,k,&\mbox{if }k<|\hat{\gamma}_{j}| \leq 2k,\\ \frac{(a-1)\hat{\gamma}_{j}-\mbox{sign}(\hat{\gamma}_{j})ak}{a-2},&\mbox{if }2k<| \hat{\gamma}_{j}|\leq ak,\\ \hat{\gamma}_{j},&\mbox{if }|\hat{\gamma}_{j}|>ak.\end{array}\right.\]

A similar result for LASSO can be obtained from (2.3.2) by doubling the LASSO tuning parameter and taking all \(\lambda_{j}=1\),

\[\hat{\gamma}_{j}=\left\{\begin{array}{ll}0,&\mbox{if }|\hat{\gamma}_{j}| <k,\\ \hat{\gamma}_{j}-\mbox{sign}(\hat{\gamma}_{j})\,k,&\mbox{if }|\hat{\gamma}_{j}| \geq k.\end{array}\right.\]

The estimates agree for \(|\hat{\gamma}_{j}|\leq 2k\) but SCAD does less shrinkage on larger least squares estimates.

## Bibliography

* Buhlmann & van de Geer (2011) Buhlmann, P. & van de Geer, S. (2011). _Statistics for high-dimensional data: Methods, theory and applications_. Heidelberg: Springer.
* Buhlmann et al. (2014) Buhlmann, P., Kalisch, M., & Meier, L. (2014). High-dimensional statistics with a view toward applications in biology. _Annual Review of Statistics and Its Applications, 1_, 255-278.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York, NY: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton, FL: Chapman and Hall/CRC Press
* Christensen et al. (2010) Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton, FL: Chapman and Hall/CRC Press.
* Clyde & George (2004) Clyde, M., & George, E. I. (2004). Model uncertainty. _Statistical Science, 19_, 81-94.
* Draper & Van Nostrand (1979) Draper, N. R., & Van Nostrand, R. C. (1979). Ridge regression and James-Stein estimation: Review and comments _Technometrics, 21_, 451-466.
* Clyde & George (2004)* Efron et al. (2004) Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). Least angle regression, with discussion. _The Annals of Statistics, 32_, 407-499.
* Fan & Li (2001) Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. _Journal of the American Statistical Association, 96_, 1348-1360.
* Green & Silverman (1994) Green, P. J., & Silverman, B. W. (1994). _Nonparametric regression and generalized linear models: A roughness penalty approach_. London: Chapman and Hall.
* Hastie et al. (2016) Hastie, T., Tibshirani, R., & Friedman, J. (2016). _The elements of statistical learning: Data mining, inference, and prediction_ (2nd ed.). New York, NY: Springer.
* Hastie et al. (2015) Hastie, T., Tibshirani, R., & Wainwright, M. (2015). _Statistical learning with sparcity: The lasso and generalizations_. Boca Raton, FL: Chapman and Hall.
* Hoerl & Kennard (1970) Hoerl, A. E., & Kennard, R. (1970). Ridge regression: Biased estimation for non-orthogonal problems. _Technometrics, 12_, 55-67.
* Park & Casella (2008) Park, T., & Casella, G. (2008). The Bayesian lasso. _Journal of the American Statistical Association, 103_, 681-686.
* Smith & Kohn (1996) Smith, M., & Kohn, R. (1996). Nonparametric regression using Bayesian variable selection. _Journal of Econometrics, 75_, 317-343.
* Tibshirani (1996) Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society: Series B, 58_, 267-288.

## Chapter 3 Reproducing Kernel Hilbert Spaces

Chapter 1 included a brief description of using reproducing kernel Hilbert spaces in nonparametric regression. We now introduce their theory.

The ideas used in analyzing linear models extend naturally to much more general spaces than \(\mathbf{R}^{n}\). One theoretical extension was introduced in _PA_ Sect. 6.3.5. These extensions lead to interesting tools for examining penalized estimation in regression (Pearce & Wand 2006) and support vector machines in classification/discrimination problems (Moguerza & Munoz 2006 and Zhu 2008), They are commonly used in areas such as functional data analysis (Ramsay & Silverman 2005), computer model analysis (Storlie, Swiler, Helton, & Sallaberry 2009), image processing (Berman 1994), and various applications of spatial statistics (Bivand, Pebesma, & Gomez-Rubio 2013 or Storlie, Bondell, & Reich 2010), to name a few. The flexibility and elegance of the methods are remarkable. Texts and survey articles on related subjects include Wahba (1990), Eubank (1999), Hastie, Tibshirani, and Friedman (2016), Gu (2002), Berlinet and Thomas-Agnan (2004), Buhlmann and van de Geer (2011), Heckman (2012), Buhlmann, Kalisch, and Meier (2014), and Wainwright (2014). These include a wealth of additional references as do the papers mentioned earlier.

In particular, the key ideas of linear models extend completely to _finite dimensional Hilbert spaces_ whereas much of the newer theory has been developed for infinite dimensional _Reproducing Kernel Hilbert Spaces_ (_RKHS_s). We provide an introduction to the mathematical ideas behind this work emphasizing its connections to linear model theory and two applications to problems that we have previously solved using linear model theory: ridge regression from Chap. 2 and (without a penalty function) spline regression from Chap. 1. We also provide an illustration of using reproducing kernels to test lack of fit in a linear model. Our development follows closely that of Nosedal-Sanchez, Storlie, Lee, and Christensen (2012).

Ridge regression and smoothing splines can both be viewed as solutions to minimization problems in a function space. If such a minimization problem is posed on an RKHS, the solution is guaranteed to exist and has a very simple form. We beginby solving some simple problems that relate to our broader goals. Section 3.2 introduces Banach spaces and Hilbert spaces. Section 3.3 provides basic ideas of RKHSs. Section 3.4 discusses the two distinct ways of using RKHS results and exploits the one not primarily used here to look at testing lack of fit in a linear model. Section 3.5 discusses penalized regression with RKHSs and the two specific examples of ridge regression and smoothing splines.

### 3.1 Introduction

For a known \(n\times p\) matrix

\[X=[X_{1},\cdots,X_{p}]=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}\]

and vector \(Y\), consider solving the system of \(n\) equations in \(p\) unknowns

\[X\beta=Y. \tag{3.1.1}\]

For a solution to exist, we must have \(Y\in C(X)\). In standard linear model problems, we typically have \(n>r(X)\), which typically causes \(Y\not\in C(X)\) and precludes us from finding a solution. That is precisely why we go to the trouble of finding (generalized) least squares estimates for \(\beta\). In the current era of big data, people have become interested in solving such problems when \(r(X)\geq n\) in which case solutions often exist.

We wish to find the solution that minimizes the norm \(\|\beta\|=\sqrt{\beta^{\prime}\beta}\) but we solve the problem using concepts that extend to RKHSs. As mentioned, a solution \(\tilde{\beta}\) (not necessarily a minimum norm solution) exists whenever \(Y\in C(X)\). Given one solution \(\tilde{\beta}\), all solutions \(\beta\) must satisfy

\[X\tilde{\beta}=X\beta\]

or

\[X(\tilde{\beta}-\beta)=0.\]

Write \(\tilde{\beta}\) uniquely as \(\tilde{\beta}=\beta_{0}+\beta_{1}\) with \(\beta_{0}\in C(X^{\prime})=\text{span}\{x_{1},\ldots,x_{n}\}\) and \(\beta_{1}\in C(X^{\prime})^{\perp}\), then \(\beta_{0}\) is a solution because \(X(\tilde{\beta}-\beta_{0})=X\beta_{1}=0\).

In fact, \(\beta_{0}\) is both the unique solution in \(C(X^{\prime})\) and the minimum norm solution. If \(\beta\) is any other solution in \(C(X^{\prime})\) then \(X(\beta-\beta_{0})=0\) so we have both \((\beta-\beta_{0})\in C(X^{\prime})^{\perp}\) and \((\beta-\beta_{0})\in C(X^{\prime})\), two sets whose intersection is only the \(0\) vector. Thus \(\beta-\beta_{0}=0\) and \(\beta=\beta_{0}\). In other words, every solution \(\tilde{\beta}\) has the same \(\beta_{0}\) vector. Finally, \(\beta_{0}\) is also the minimum norm solution because the arbitrary solution \(\tilde{\beta}\) has

\[\beta_{0}^{\prime}\beta_{0}\leq\beta_{0}^{\prime}\beta_{0}+\beta_{1}^{\prime} \beta_{1}=\tilde{\beta}^{\prime}\tilde{\beta}.\]We have established the existence of a unique, minimum norm solution in \(C(X^{\prime})\) that can be written as

\[\beta_{0}=X^{\prime}\xi=\sum_{i=1}^{n}\xi_{i}x_{i},\]

for some \(\xi_{i}\), \(i=1,\ldots,n\). To find \(\beta_{0}\) explicitly, write \(\beta_{0}=X^{\prime}\xi\) and the defining Eq. (3.1.1) becomes

\[XX^{\prime}\xi=Y, \tag{3.1.2}\]

which is again a system of linear equations. Even if there exist multiple solutions \(\xi\), \(X^{\prime}\xi\) is unique.

Example 3.1.1. We use this framework to illustrate the "smallest" solution to the system of equations \(\tilde{\beta}_{1}+\tilde{\beta}_{3}=0\) and \(\tilde{\beta}_{2}=1\). In the general framework (3.1.1), these become

\[X\beta\equiv\begin{bmatrix}1&0&1\\ 0&1&0\end{bmatrix}\begin{bmatrix}\tilde{\beta}_{1}\\ \tilde{\beta}_{2}\\ \tilde{\beta}_{3}\end{bmatrix}=\begin{bmatrix}0\\ 1\end{bmatrix},\]

whereas (3.1.2) becomes

\[\begin{bmatrix}2&0\\ 0&1\end{bmatrix}\begin{bmatrix}\xi_{1}\\ \xi_{2}\end{bmatrix}=\begin{bmatrix}0\\ 1\end{bmatrix}.\]

The unique solution is \((\xi_{1},\xi_{2})^{\prime}=(0,1)^{\prime}\) which implies that the solution to the original problem is \(\beta_{0}=X^{\prime}\xi=0x_{1}+1x_{2}=(0,1,0)^{\prime}\). \(\Box\)

Virtually the same methods can be used to solve similar problems in any inner-product space \(\mathcal{M}\). As discussed in _PA_ Sect. 6.3.5, an inner product \(\langle\cdot,\cdot\rangle\) assigns real numbers to pairs of vectors. There the notion was used to treat random variables as vectors whereas in most of this book and _PA_, vectors are elements of \(\mathbf{R}^{n}\) or \(\mathbf{R}^{p}\). In this chapter we often use functions from some set \(\mathcal{E}\) into \(\mathbf{R}\) as vectors. Frequently we take \(\mathcal{E}\subset\mathbf{R}\) but \(\mathcal{E}\) can be a subset of a more general vector space. Note that an element of \(\mathbf{R}^{n}\) can be thought of as a function from the integers \(\{1,2,\ldots,n\}\) into \(\mathbf{R}\).

We generalize the problem of solving a system of linear equations as follows. For \(n\) given vectors \(x_{i}\in\mathcal{M}\) and numbers \(y_{i}\in\mathbf{R}\), we might want to find the vector \(\beta\in\mathcal{M}\) such that

\[\langle x_{i},\beta\rangle=y_{i},\qquad i=1,2,\ldots,n \tag{3.1.3}\]

for which the norm \(\|\beta\|\equiv\sqrt{\langle\beta,\beta\rangle}\) is minimal. The solution (if one exists) has the form

\[\beta_{0}=\sum_{i=1}^{n}\xi_{i}x_{i}, \tag{3.1.4}\]

with \(\xi_{i}\) satisfying the linear equations

\[\sum_{k=1}^{n}\langle x_{i},x_{k}\rangle\xi_{k}=y_{i},\qquad i=1,\ldots,n \tag{3.1.5}\]or, equivalently, we can solve the matrix equation

\[\tilde{R}\xi=Y\]

where the \(n\times n\) matrix \(\tilde{R}\) has elements \(\tilde{r}_{ij}\) with

\[\tilde{r}_{ij}=\langle x_{i},x_{j}\rangle.\]

For a formal proof see Mate (1990). From the matrix equation it is easy to check whether a solution exists. After a brief digression, we apply this result to the interpolating spline problem.

The illustrations in this chapter focus on three vector spaces: \(\mathbf{R}^{n}\), because it is familiar;

\[\mathcal{L}^{2}[0,1]=\left\{f:\int_{0}^{1}[f(t)]^{2}dt<\infty\right\},\]

because it relates to fitting splines; and the vector space of all functions from \(\mathbf{R}^{p-1}\) into \(\mathbf{R}\)

\[\mathcal{F}=\left\{f:f(x)\in\mathbf{R},\;x\in\mathbf{R}^{p-1}\right\},\]

because it relates to fitting multiple regression models that include an intercept. We will examine two different inner products on \(\mathbf{R}^{n}\). We will look at the standard inner product on \(\mathcal{L}^{2}[0,1]\) and also at two subspaces with different inner products on each. For \(\mathcal{F}\) we focus on linear multiple regression by focusing on subspaces of constant functions, linear functions, and affine functions along with the orthogonality properties that they display.

The first illustration looks at a subset of \(\mathcal{L}^{2}[0,1]\). Throughout, \(f^{(m)}(t)\) denotes the \(m\)-th derivative of \(f\) with \(\dot{f}\equiv f^{(1)}\) and \(\ddot{f}\equiv f^{(2)}\). From Appendix A we also have the notations \(\dot{f}(t)\equiv\mathbf{d}_{t}f(t)\) and \(\ddot{f}(t)\equiv\mathbf{d}_{tt}^{2}f(t)\).

#### Interpolating Splines

Suppose we want to find a function \(f(t)\) that interpolates between the points \((t_{i},y_{i})\), \(i=0,1,\ldots,n\), where \(y_{0}\equiv 0\) and \(0=t_{0}<t_{1}<t_{2}<\cdots<t_{n}\leq 1\). We restrict attention to functions \(f\in\mathcal{W}_{0}^{1}\) where

\[\mathcal{W}_{0}^{1}=\left\{f\in\mathcal{L}^{2}[0,1]:f(0)=0,\int_{0}^{1}[\dot{f }(t)]^{2}dt<\infty\right\}\]

and it is understood that the derivatives exist because they are assumed to be square integrable. The restriction that \(y_{0}=f(0)=0\) is not really necessary, but simplifies the presentation.

We want to find the smoothest function \(f(t)\) that satisfies \(f(t_{i})=y_{i}\), \(i=1,\ldots,n\). Defining an inner product on \(\mathcal{W}_{0}^{1}\) by \[\langle f,g\rangle=\int_{0}^{1}\dot{f}(x)\dot{g}(x)dx, \tag{3.1.6}\]

this determines a norm (length) \(\|f\|\equiv\sqrt{\langle f,f\rangle}\) for \(\mathcal{W}_{0}^{1}\) that is small for "smooth" functions. To address the interpolation problem, define the indicator function \(\mathcal{I}_{A}(t)\) for a set \(A\) to be \(1\) if \(t\in A\) and \(0\) if \(t\not\in A\). Note that the functions

\[R_{i}(s)\equiv\min(s,t_{i})=s\mathcal{I}_{[0,t_{i}]}(s)+t_{i}\mathcal{I}_{(t_{ i},1]}(s),\quad i=1,2,\ldots,n,\]

have \(R_{i}(0)=0\), \(\dot{R}_{i}(s)=\mathcal{I}_{[0,t_{i}]}(s)\) so that \(R_{i}(\cdot)\in\mathcal{W}_{0}^{1}\), and have the property that \(\langle R_{i},f\rangle=f(t_{i})\) because

\[\langle f,R_{i}\rangle = \int_{0}^{1}\dot{f}(s)\dot{R}_{i}(s)ds\] \[= \int_{0}^{t_{i}}\dot{f}(s)ds=f(t_{i})-f(0)=f(t_{i}).\]

Thus, any interpolator \(f\) satisfies a system of equations like (3.1.3), namely

\[f(t_{i})=\langle R_{i},f\rangle=y_{i},\qquad\quad i=1,\ldots,n. \tag{3.1.7}\]

and by (3.1.4), the smoothest function \(f\) (minimum norm) that satisfies the requirements has the form

\[\hat{f}(t)=\sum_{i=1}^{n}\xi_{i}R_{i}(t).\]

The \(\xi_{j}\)'s are the solutions to the system of real linear equations obtained by substituting \(\hat{f}\) into (3.1.7), that is

\[\sum_{j=1}^{n}\langle R_{i},R_{j}\rangle\xi_{j}=y_{i},\qquad\quad i=1,2,\ldots,n\]

or

\[\tilde{R}\xi=Y.\]

Note that

\[\langle R_{i},R_{j}\rangle=R_{j}(t_{i})=R_{i}(t_{j})=\min(t_{i},t_{j})\]

and we define the function

\[R(s,t)=\min(s,t)\]

that turns out to be a reproducing kernel.

Example 3.1.2.: Given points \(f(t_{i})=y_{i}\), say, \(f(0)=0\), \(f(0.1)=0.1\), \(f(0.25)=1\), \(f(0.5)=2\), \(f(0.75)=1.5\), and \(f(1)=1.75\), we can now find \(f\in\mathcal{W}_{0}^{1}\) that satisfies these six conditions and minimizes the norm associated with (3.1.6). The vector \(\xi\in\mathbf{R}^{5}\) that satisfies the system of equations\[\begin{bmatrix}0.1&0.1&0.1&0.1&0.1\\ 0.1&0.25&0.25&0.25&0.25\\ 0.1&0.25&0.5&0.5&0.5\\ 0.1&0.25&0.5&0.75&0.75\\ 0.1&0.25&0.5&0.75&1\end{bmatrix}\begin{bmatrix}\xi_{1}\\ \xi_{2}\\ \xi_{3}\\ \xi_{4}\\ \xi_{5}\end{bmatrix}=\begin{bmatrix}0.1\\ 1\\ 2\\ 1.5\\ 1.75\end{bmatrix}\]

is \(\xi=(-5,2,6,-3,1)^{\prime}\), which implies that the smoothest interpolating function is

\[\hat{f}(t) = -5R_{1}(t)+2R_{2}(t)+6R_{3}(t)-3R_{4}(t)+1R_{5}(t)\] \[= -5R(t,t_{1})+2R(t,t_{2})+6R(t,t_{3})-3R(t,t_{4})+1R(t,t_{5})\]

or, adding together the slopes for \(t>t_{i}\) and finding the intercepts,

\[\hat{f}(t)=\begin{cases}t&0\leq t\leq 0.1\\ 6t-0.5&0.1\leq t\leq 0.25\\ 4t&0.25\leq t\leq 0.5\\ -2t+3&0.5\leq t\leq 0.75\\ t+0.75&0.75\leq t\leq 1.\end{cases}\]

This is the linear interpolating spline as can be seen graphically in Fig. 3.1. 

For this illustration we restricted \(f\) so that \(f(0)=0\) but only for convenience of presentation. It can be shown that the form of the solution remains the same with any shift to the function, so that in general the solution takes the form \(\hat{f}(t)=\beta_{0}+\sum_{j=1}^{n}\xi_{j}R_{j}(t)\) where \(\beta_{0}=y_{0}\).

The two two key points are (a) that the functions \(R_{j}(t)\) allow us to express a function evaluated at a point as an inner-product constraint and (b) the restriction to functions in \(\mathcal{W}_{0}^{1}\). \(\mathcal{W}_{0}^{1}\) is a very special function space, a reproducing kernel Hilbert space, and \(R_{j}\) is determined by a reproducing kernel function \(R\).

Figure 3.1: Linear interpolating spline

Ultimately, our goal is to address more complicated regression problems like

**Example 3.1.3**: _Smoothing Splines_.

Consider simple regression data \((x_{i},y_{i})\), \(i=1,\ldots,n\) and finding the function that minimizes

\[\frac{1}{n}\sum_{i=1}^{n}\left[y_{i}-f(x_{i})\right]^{2}+\lambda\int_{0}^{1} \left[f^{(m)}(x)\right]^{2}dx. \tag{3.1.9}\]

If \(f(x)\) is restricted to be in an appropriate class of functions, minimizing only the first term gives least squares estimation within the class. If the class contains functions with \(f(x_{i})=y_{i}\) for all \(i\), such functions minimize the first term but are typically very "unsmooth," i.e., have large second term. For example, an \(n-1\) order polynomial will always fit the data perfectly but is typically very unsmooth. The second "penalty" term is minimized whenever the \(m\)th derivative is \(0\) everywhere, but (at least for small \(m\)) that rarely has a small first term. For \(m=1\), \(0\leq x\leq 1\), and a suitable class of functions, as we will see later, the minimizer takes the form

\[\hat{f}(x)=\beta_{0}+\sum_{i=1}^{n}\xi_{i}R_{i}(x),\]

where the \(R_{i}\)s were given earlier and the coefficients are found by solving a slightly different system of linear equations. Choosing \(m=1,2\) determines linear and cubic smoothing splines, respectively. \(\Box\)

If our goal is only to derive the solution to the linear smoothing spline problem with one predictor variable, RKHS theory is overkill. The value of RKHS theory lies in its generality. For example, the spline penalty can be replaced by many other penalties having associated inner products, and the \(x_{i}\)'s can be vectors. Using RKHS results, we can solve the general problem of finding the minimizer of \(\frac{1}{n}\sum_{i=1}^{n}\left[y_{i}-f(x_{i})\right]^{2}+\lambda Q(f)\) for quite general functions \(Q\) that correspond to a squared norm in a Hilbert subspace. See Wahba (1990) or Gu (2002) for a full treatment.

### Banach and Hilbert Spaces

As discussed in \(PA\)'s Appendix A, a vector space is a set \(\mathcal{M}\) that contains elements called _vectors_ and supports two kinds of operations: addition of vectors and multiplication by scalars. The scalars are real numbers in this book and in \(PA\) but in general they can be from any _field_. We rely on context to distinguish between the vector \(0\in\mathcal{M}\) and the scalar \(0\in\mathbf{R}\). For vectors \(u,v\in\mathcal{M}\), we also write \(u+(-1\times v)\) as \(u-v\). Any subset of \(\mathcal{M}\) that is closed under vector addition and scalar multiplication is a subspace of \(\mathcal{M}\). The classic book on finite dimensional vector spaces is Halmos (1958). Harville (1997) also contains a wealth of information. For more on vector spaces and the other topics in this section, see, for example, Naylor and Sell (1982), Young (1988), Mate (1990) or Rustagi (1994).

#### Banach Spaces

A Banach space is a vector space that has some additional structure. First, a Banach space has a length measure, called a _norm_, associated with it and, second, a Banach space is _complete_ under that norm. Banach spaces provide a convenient introduction to Hilbert spaces, who have another bit of structure, namely an inner product.

**Definition 3.2.1**.: A _norm_ of a vector space \(\mathcal{M}\), denoted \(||\cdot||\), is a nonnegative real valued function satisfying the following properties for all \(u,v\in\mathcal{M}\) and all \(a\in\mathbf{R}\).

1. Non-negative: \(||u||\geq 0\).
2. Strictly positive: \(||u||\) = 0 implies \(u=0\).
3. Homogeneous: \(||au||\) = \(|a|\) ||\(u||\).
4. Triangle inequality: \(||u+v||\leq||u||\) + \(||v||\).

A vector space is called a _normed vector space_ when a norm is defined on the space. The norm of a vector is also called its _length_. For vectors \(u,v\in\mathcal{M}\), the _distance_ between \(u\) and \(v\) is defined as \(||u-v||\).

**Definition 3.2.2**.: A sequence \(\{v_{n}\}\) in a normed vector space \(\mathcal{M}\) is said to _converge_ to \(v_{0}\in\mathcal{M}\) if

\[\lim_{n\to\infty}||v_{n}-v_{0}||=0.\]

**Definition 3.2.3**.: A sequence \(\{v_{n}\}\subset\mathcal{M}\) is called a _Cauchy sequence_ if for any given \(\varepsilon>0\), there exists an integer \(N\) such that

\[||v_{m}-v_{n}||<\varepsilon,\hskip 30.0pt\text{whenever}\hskip 8.0ptm,n\geq N.\]

Convergence of sequences in normed vector spaces follows the same general idea as sequences of real numbers except that the distance between two vectors of the space is measured by the norm of the difference between the two vectors.

**Definition 3.2.4**.: **(Banach Space).** A normed vector space \(\mathcal{M}\) is _complete_ if every Cauchy sequence in \(\mathcal{M}\) converges to an element of \(\mathcal{M}\). A complete normed vector space is a _Banach Space_.

Example 3.2.5.: \(\mathcal{M}=\mathbf{R}\) with the absolute value norm \(\|x\|\equiv|x|\) is a complete, normed vector space over \(\mathbf{R}\), and is thus a Banach space.

Example 3.2.6. For \(\mathcal{M}=\mathbf{R}^{n}\), let \(x=(x_{1},\ldots,x_{n})^{\prime}\) be a vector. The \(\mathcal{L}_{p}\) norm on \(\mathbf{R}^{n}\) is defined by

\[\|x\|_{p}\equiv\left[\sum_{i=1}^{n}|x_{i}|^{p}\right]^{1/p}\qquad\quad\text{ for }1\leq p<\infty.\]

One can verify properties 1-4 of Definition 3.2.1 for each \(p\), validating that \(\|x\|_{p}\) is a norm on \(\mathbf{R}^{n}\). Under the \(\mathcal{L}_{p}\) norm, \(\mathbf{R}^{n}\) is complete and thus a Banach space. Euclidean distance on \(\mathbf{R}^{n}\) corresponds to choosing \(p=2\).

Alternatively, if \(A\) is a positive definite matrix,

\[||x||_{A}\equiv\sqrt{x^{\prime}Ax}\]

defines a norm and a Banach space on \(\mathbf{R}^{n}\). Euclidean distance on \(\mathbf{R}^{n}\) corresponds to choosing \(I\) for \(A\). \(\Box\)

#### Hilbert Spaces

A Hilbert Space is a Banach space in which the norm is defined by an inner-product (also called a dot-product) that maps any two vectors into a real number. Banach spaces incorporate concepts of length and distance; Hilbert spaces add the concept of orthogonality (perpendicularity).

We typically denote Hilbert spaces by \(\mathcal{H}\). For elements \(u,v\in\mathcal{H}\), write the inner product of \(u\) and \(v\) as either \(\langle u,v\rangle_{\mathcal{H}}\) or, when it is clear from the context that the inner product is taking place in \(\mathcal{H}\), as \(\langle u,v\rangle\). An _inner product_ must satisfy four properties for all \(u,v,w\in\mathcal{H}\) and all \(a\in\mathbf{R}\).

1. Associative: \(\langle au,v\rangle=a\langle u,v\rangle\).
2. Commutative: \(\langle u,v\rangle=\langle v,u\rangle\).
3. Distributive: \(\langle u,v+w\rangle=\langle u,v\rangle+\langle u,w\rangle\).
4. Positive Definite: \(\langle u,u\rangle\geq 0\) with equality holding only if \(u=0\).

Definition 3.2.7. A vector space \(\mathcal{M}\) with an inner product defined on it is called an _inner product space_. Vectors \(u\) and \(v\) are _orthogonal_, written \(u\perp v\), if \(\langle u,v\rangle=0\). Two sets of vectors are said to be orthogonal if every vector in one set is orthogonal to every vector in the other. The set of all vectors orthogonal to a subspace \(\mathcal{N}\) of \(\mathcal{M}\) is called the _orthogonal complement_ of \(\mathcal{N}\) with respect to \(\mathcal{M}\) and is written \(\mathcal{N}_{\mathcal{M}}^{\perp}\), or just \(\mathcal{N}^{\perp}\). The _norm_ of \(u\) in an inner product space is \(\|u\|\equiv\sqrt{\langle u,u\rangle}\). The angle \(\theta\) between two vectors \(u\) and \(v\) is defined by

\[\cos(\theta)\equiv\frac{\langle x,y\rangle}{\|u\|\,\|v\|}.\]

A complete inner-product space is called a _Hilbert space_.

Most of the ideas related to orthogonality that we have exploited here and in \(PA\) extend immediately to finite dimensional Hilbert spaces. It is easy to see that an orthogonal complement is a subspace because it is closed under vector addition and scalar multiplication. Moreover, because only the \(0\) vector can be orthogonal to itself, \(\mathcal{N}\cap\mathcal{N}^{\perp}_{\mathcal{M}}=\{0\}\), which means that for any vector \(x\) that can be written as \(x=x_{0}+x_{1}\) with \(x_{0}\in\mathcal{N}\) and \(x_{1}\in\mathcal{N}^{\perp}_{\mathcal{M}}\), the representation is unique. If \(\mathcal{M}=\text{span}\{x_{1},\cdots,x_{n}\}\), Gram-Schmidt applies so that we can find an orthonormal spanning set \(\{o_{1},\cdots,o_{n}\}\) in which all vectors are orthogonal to each other and each \(\|o_{j}\|\) is \(0\) or \(1\) and \(\text{span}\{x_{1},\cdots,x_{r}\}=\text{span}\{o_{1},\cdots,o_{r}\}\), \(r=1,\ldots,n\). Eliminating the \(0\) vectors from \(\{o_{1},\cdots,o_{n}\}\) gives an orthonormal basis for \(\mathcal{M}\). The key idea in the inductive proof of Gram-Schmidt is to set

\[w_{s+1}=x_{s+1}-\sum_{j=1}^{s}o_{j}\langle o_{j},x_{s+1}\rangle\quad\text{and }\quad o_{s+1}=\left\{\begin{array}{ll}\frac{1}{\|w_{s+1}\|}w_{s+1}&\text{ if }\|w_{s+1}\|>0\\ 0&\text{ if }\|w_{s+1}\|=0\.\end{array}\right.\]

By taking spanning sets \(\{x_{1},\cdots,x_{n}\}\) for \(\mathcal{M}\) and \(\{v_{1},\cdots,v_{s}\}\) for a subspace \(\mathcal{N}\), we can Gram-Schmidt the spanning set \(\{v_{1},\cdots,v_{s},x_{1},\cdots,x_{n}\}\) of \(\mathcal{M}\) to get orthonormal bases for \(\mathcal{N}\) and \(\mathcal{N}^{\perp}_{\mathcal{M}}\) that combine to give a basis for \(\mathcal{M}\) thus establishing that any vector \(x\in\mathcal{M}\) can be written uniquely as \(x=x_{0}+x_{1}\) with \(x_{0}\in\mathcal{N}\) and \(x_{1}\in\mathcal{N}^{\perp}_{\mathcal{M}}\) and allowing us to define \(x_{0}\) as the perpendicular projection of \(x\) onto \(\mathcal{N}\). Sometimes the perpendicular projection of \(x\) into a subspace \(\mathcal{N}\) of \(\mathcal{M}\) is defined as the unique vector \(x_{0}\in\mathcal{N}\) with the property that \(\langle x-x_{0},u\rangle=0\) for any \(u\in\mathcal{N}\).

Example 3.2.8.: For \(\mathbf{R}^{n}\) we can define a Hilbert space with the inner product

\[\langle u,v\rangle\equiv u^{\prime}v=\sum_{i=1}^{n}u_{i}v_{i}\]

that conforms with Euclidean geometry. More generally, for any positive definite matrix \(A\), we can define a Hilbert space with the inner product \(\langle u,v\rangle\equiv u^{\prime}Av\). 

Example 3.2.9.: For

\[\mathcal{L}^{2}[0,1]=\left\{f:\int_{0}^{1}[f(t)]^{2}dt<\infty\right\},\]

we can define a Hilbert space with the inner product

\[\langle f,g\rangle_{\mathcal{L}^{2}[0,1]}\equiv\int_{0}^{1}f(t)g(t)dt.\]

The space is well-known to be complete, see de Barra (1981).

For the subspace

\[\mathcal{W}_{0}^{1}=\left\{f\in\mathcal{L}^{2}[0,1]:f(0)=0,\int_{0}^{1}[\dot{f}(t) ]^{2}dt<\infty\right\},\]

define the inner product

\[\langle f,g\rangle_{\mathcal{W}_{0}^{1}}=\int_{0}^{1}\dot{f}(t)\dot{g}(t)dt. \tag{3.2.1}\]

Note that \(\langle f,f\rangle_{\mathcal{W}_{0}^{1}}=0\) if and only if \(f=0\) because if \(\langle f,f\rangle_{\mathcal{W}_{0}^{1}}=0\), \(\dot{f}(t)=0\) for all \(t\), so \(f(t)\) must be a constant, however \(f(0)=0\), so \(f(t)=0\) for all \(t\).

For the subspace

\[\mathcal{W}_{0}^{2}=\left\{f\in\mathcal{L}^{2}[0,1]:f(0)=\dot{f}(0)=0,\int_{0} ^{1}[\ddot{f}(t)]^{2}dt<\infty\right\},\]

define an inner product

\[\langle f,g\rangle_{\mathcal{W}_{0}^{2}}=\int_{0}^{1}\ddot{f}(t)\ddot{g}(t)dt. \tag{3.2.2}\]

Note that \(\langle f,f\rangle_{\mathcal{W}_{0}^{2}}=0\) if and only if \(f=0\) because if \(\langle f,f\rangle_{\mathcal{W}_{0}^{2}}=0\), \(\ddot{f}(t)=0\) for all \(t\), so \(\dot{f}(t)\) must be a constant, however \(\dot{f}(0)=0\), so \(\dot{f}(t)=0\) for all \(t\), hence \(f(t)\) must be a constant, however \(f(0)=0\), so \(f(t)=0\) for all \(t\). \(\Box\)

Example 3.2.10.: Consider the vector space of all functions from \(\mathbf{R}^{p-1}\) to \(\mathbf{R}\),

\[\mathcal{F}=\left\{f:f(x)\in\mathbf{R},\;x\in\mathbf{R}^{p-1}\right\}.\]

The subspace of all constant functions on \(\mathbf{R}^{p-1}\) is

\[\mathcal{F}_{0}=\left\{f_{a}\in\mathcal{F}:f_{a}(x)=a,\;a\in\mathbf{R},\;x\in \mathbf{R}^{p-1}\right\}\]

and define the inner product

\[\langle f_{a},f_{b}\rangle_{\mathcal{F}_{0}}=ab.\]

Since \(\mathbf{R}^{p-1}\) is a Hilbert Space, so is \(\mathcal{F}_{0}\).

The subspace of all linear functions on \(\mathbf{R}^{p-1}\) passing through the origin is

\[\mathcal{F}_{1}=\left\{f_{\gamma}\in\mathcal{F}:f_{\gamma}(x)=x^{\prime}\gamma,\;\gamma\in\mathbf{R}^{p-1},\;x\in\mathbf{R}^{p-1}\right\}\]

and define the inner product

\[\langle f_{\eta},f_{\gamma}\rangle_{\mathcal{F}_{1}}=\eta^{\prime}\gamma=\eta_ {1}\gamma_{1}+\eta_{2}\gamma_{2}+\cdots+\eta_{p-1}\gamma_{p-1}.\]

Again, since \(\mathbf{R}^{p-1}\) is a Hilbert Space, so is \(\mathcal{F}_{1}\)

Now consider the subspace of all affine (i.e., linear plus a constant) functions on \(\mathbf{R}^{p-1}\).

\[\mathcal{F}_{*}=\{f_{\beta}\in\mathcal{F}:f_{\beta}(x)=\beta_{0}+\beta_{1}x_{1}+ \ldots+\beta_{p-1}x_{p-1},\;\beta\in\mathbf{R}^{p},\;x\in\mathbf{R}^{p-1}\},\]

with the inner product

\[\langle f_{\beta},f_{\eta}\rangle_{\mathcal{F}_{*}}=\beta_{0}\eta_{0}+\beta_{1 }\eta_{1}+\ldots+\beta_{p-1}\eta_{p-1}.\]

This too is a Hilbert space.

The subspace of \(\mathcal{F}_{*}\) that contains constant functions,

\[\mathcal{F}_{0}=\{f_{\beta}\in\mathcal{F}_{*}:f_{\beta}(x)=\beta_{0},\;0= \beta_{1}=\cdots=\beta_{p}\}\]

has an orthogonal complement with respect to \(\mathcal{F}_{*}\) of

\[\mathcal{F}_{0}^{\perp}=\{f_{\beta}\in\mathcal{F}_{*}:f_{\beta}(x)=\beta_{1}x _{1}+\ldots+\beta_{p-1}x_{p-1},\;0=\beta_{0}\}=\mathcal{F}_{1}.\]

For any \(f_{\beta},f_{\eta}\in\mathcal{F}_{*}\), write \(\beta=[\beta_{0},\beta_{*}^{\prime}]\) and \(\eta=[\eta_{0},\eta_{*}^{\prime}]\). We have the unique decompositions \(f_{\beta}=f_{\beta_{0}}+f_{\beta_{*}}\) and \(f_{\eta}=f_{\eta_{0}}+f_{\eta_{*}}\), and

\[\langle f_{\beta},f_{\eta}\rangle_{\mathcal{F}_{*}}=\langle f_{\beta_{0}},f_{ \eta_{0}}\rangle_{\mathcal{F}_{0}}+\langle f_{\gamma},f_{\eta_{*}}\rangle_{ \mathcal{F}_{1}}.\]

### 3.3 Reproducing Kernel Hilbert Spaces

Hilbert spaces that display certain properties on certain linear operators are called reproducing kernel Hilbert spaces.

**Definition 3.3.1.**  A function (operator) \(T\) mapping a vector space \(\mathcal{X}\) into another vector space \(\mathcal{Y}\) is called _linear_ if \(T(\lambda_{1}x_{1}+\lambda_{2}x_{2})=\lambda_{1}T(x_{1})+\lambda_{2}T(x_{2})\) for any \(x_{1},x_{2}\in\mathcal{X}\) and any \(\lambda_{1},\lambda_{2}\in\mathbf{R}\).

Any \(p\times n\) matrix \(A\) maps vectors in \(\mathbf{R}^{n}\) into vectors in \(\mathbf{R}^{p}\) via \(T_{A}(x)\equiv Ax\) and is linear.

**Exercise 3.1.**  Consider a finite dimensional Hilbert space \(\mathcal{H}\) (one that contains a finite basis) and a subspace \(\mathcal{H}_{0}\). The operator \(M\) is a _perpendicular projection operator_ onto \(\mathcal{H}_{0}\) if \(M(x)=x\) for any \(x\in\mathcal{H}_{0}\) and \(M(w)=0\) for any \(w\in\mathcal{H}_{0}^{\perp}\). Show that \(M\) must be both unique and linear. Let \(\{o_{1},\cdots,o_{r}\}\) be an orthonormal basis for \(\mathcal{H}_{0}\) and show that

\[M(x)=\sum_{j=1}^{r}o_{j}\langle o_{j},x\rangle.\]

Do these results hold for subspaces with infinite dimensions? In particular, if \(\mathcal{H}\) has a countable basis and a subspace \(\mathcal{H}_{0}\) is finite dimensional, can any \(x\in\mathcal{H}\) be decomposed into \(x=x_{0}+x_{1}\) with \(x_{0}\in\mathcal{H}_{0}\) and \(x_{1}\in\mathcal{H}_{0}^{\perp}\)?

**Definition 3.3.2**.: The operator \(T:\mathcal{X}\to\mathcal{Y}\) mapping a Banach space into a Banach space is _continuous_ at \(x_{0}\in\mathcal{X}\) if and only if for every \(\varepsilon>0\) there exists \(\delta=\delta(\varepsilon)>0\) such that for every \(x\) with \(||x-x_{0}||_{\mathcal{X}}<\delta\) we have \(||T(x)-T(x_{0})||_{\mathcal{Y}}<\varepsilon\).

Linear operators are continuous everywhere if they are continuous at \(0\).

We can write \(x_{m}\to x_{0}\), if \(||x_{m}-x_{0}||_{\mathcal{X}}\to 0\). Continuity occurs if \(x_{m}\to x_{0}\) implies \(T(x_{m})\to T(x_{0})\) in the sense that \(||T(x_{m})-T(x_{0})||_{\mathcal{Y}}\to 0\).

**Definition 3.3.3**.: A real valued function defined on a vector space is called a _functional_.

Any vector \(a\in\mathbf{R}^{n}\) defines a linear functional on \(\mathbf{R}^{n}\) via \(\phi_{a}(x)\equiv a^{\prime}x\).

Example 3.3.4.: Let \(\mathcal{S}\) be the set of bounded real valued differentiable functions \(\{f(x)\}\) defined on the real line. Then \(\mathcal{S}\) is a vector space with the usual \(+\) and \(\times\) operations for functions. Some linear functionals on \(\mathcal{S}\) are \(\phi(f)=\int_{a}^{b}f(x)dx\) and \(\phi_{a}(f)=\dot{f}(a)\) for some fixed \(a\). A nonlinear functional is \(\phi_{a,b}(f)=\sup_{x\in[a,b]}f(x)\).

\(\Box\)

A linear functional of particular importance is the evaluation functional.

**Definition 3.3.5**.: Let \(\mathcal{M}\) be a vector space of functions defined from \(\mathcal{E}\) into \(\mathbf{R}\). For any \(t\in\mathcal{E}\), denote by \(e_{t}\) the _evaluation functional_ at the point \(t\), i.e., for \(g\in\mathcal{M}\), the mapping is \(e_{t}(g)=g(t)\).

For \(\mathcal{M}=\mathbf{R}^{n}\), vectors can be viewed as functions from the set \(\mathcal{E}=\{1,2,\ldots,n\}\) into \(\mathbf{R}\). An evaluation functional is \(e_{i}(x)=x_{i}\).

While it is simplest to take \(\mathcal{E}\subset\mathbf{R}\) in Definition 3.3.5, we will need to consider \(\mathcal{E}\subset\mathbf{R}^{p}\), and there is no reason not to use even more general vector spaces to define \(\mathcal{E}\).

In a Hilbert space (or any normed vector space) of functions, the notion of pointwise convergence is related to the continuity of the evaluation functionals. The following are equivalent for a normed vector space \(\mathcal{H}\) of real valued functions defined on \(\mathcal{E}\).

(i) The evaluation functionals are continuous for all \(t\in\mathcal{E}\).

(ii) If \(f,f_{1},f_{2},\ldots\in\mathcal{H}\) and \(||f_{n}-f||\to 0\) then \(f_{n}(t)\to f(t)\) for every \(t\in\mathcal{E}\).

(iii) For every \(t\in\mathcal{E}\) there exists \(K_{t}>0\) such that \(|f(t)|\leq K_{t}||f||\) for all \(f\in\mathcal{H}\).

Here (ii) is the definition of (i). See Mate (1990) for a proof of (iii).

To define a reproducing kernel, we need the famous _Riesz Representation Theorem_.

**Theorem 3.3.6**.: Let \(\mathcal{H}\) be a Hilbert space and let \(\phi\) be a continuous linear functional on \(\mathcal{H}\). Then there is one and only one vector \(g\in\mathcal{H}\) such that\[\phi(f)=\langle g,f\rangle,\hskip 28.452756pt\mbox{for all }f\in{\cal H}.\]

The vector \(g\) is sometimes called the _representation_ of \(\phi\). Nonetheless, \(\phi\) and \(g\) are different objects: \(\phi\) is a linear functional on \({\cal H}\) and \(g\) is a vector in \({\cal H}\). For a proof of this theorem, see Naylor and Sell (1982) or Mate (1990).

For \({\cal H}={\bf R}^{n}\) with the Euclidean inner product, the representation theorem is well known because for \(\phi(x)\) to be a linear functional there must exist a vector \(g\) such that

\[\phi(x)=g^{\prime}x.\]

In particular, an evaluation functional is \(e_{i}(x)=x_{i}\). The representation of this linear functional is the indicator vector \(R_{i}\in{\bf R}^{n}\) that is \(0\) everywhere except has a \(1\) in the \(i\)th place because

\[e_{i}(x)=x_{i}=R^{\prime}_{i}x.\]

In the future we will use \(e_{i}\) to denote both the indicator vector in \({\bf R}^{n}\) that is \(0\) everywhere except has a \(1\) in the \(i\)th place and the evaluation functional that the indicator vector represents.

An element of a set of functions, say \(f\), from \({\cal E}\) into \({\bf R}\), is sometimes denoted \(f(\cdot)\) to be explicit that the elements are functions, whereas \(f(t)\) is the value of \(f(\cdot)\) evaluated at \(t\in{\cal E}\).

Applying the Riesz Representation Theorem to a Hilbert space \({\cal H}\) of real valued functions in which all evaluation functionals are continuous, for every \(t\in{\cal E}\) there is a unique symmetric function \(R:{\cal E}\times{\cal E}\to{\bf R}\) for which \(R(\cdot,t)\in{\cal H}\) is the representation of the evaluation functional \(e_{t}\), so that

\[f(t)=e_{t}(f)=\langle R(\cdot,t),f(\cdot)\rangle,\hskip 28.452756ptf\in{\cal H}.\]

The function \(R\) is called a _reproducing kernel_ (r.k.) and \(f(t)=\langle R(\cdot,t),f(\cdot)\rangle\) is called the _reproducing property_ of \(R\). In particular, by the reproducing property

\[R(t,s)=\langle R(\cdot,t),R(\cdot,s)\rangle.\]

The fact that \(\langle R(\cdot,t),R(\cdot,s)\rangle=\langle R(\cdot,s),R(\cdot,t)\rangle\) is why \(R\) must be a symmetric function.

Again, our use of \(t\) is suggestive of it being a real number but in general it can be a vector.

**Definition 3.3.7.** A Hilbert space \({\cal H}\) of functions defined on \({\cal E}\) into \({\bf R}\) is called a _reproducing kernel Hilbert space_ if all evaluation functionals are continuous.

Example 3.3.8. For \({\bf R}^{n}\) with inner product \(\langle u,v\rangle\equiv u^{\prime}Av\) where the \(n\times n\) matrix \(A\) is positive definite, the r.k. \(R(\cdot,\cdot)\) maps \(s,t=1,\ldots,n\) into \({\bf R}\), so it is really just a matrix. To see that \([R(s,t)]=A^{-1}\), note that \(R(\cdot,t)\) is the \(t\)th column of \([R(s,t)]\) and we must have that \(R(\cdot,t)=A^{-1}e_{t}\) because for any \(x\)\[\langle R(\cdot,t),x\rangle\equiv x_{t}=e^{\prime}_{t}x=e^{\prime}_{t}A^{-1}Ax= \langle A^{-1}e_{t},x\rangle.\]

As earlier, \(e_{t}\) is the vector with 0s everywhere except a 1 in the \(t\)th place. For Euclidean \(\mathbf{R}^{n}\), the r.k. is \(I\). 

Example 3.3.9. : For

\[\mathcal{L}^{2}[0,1]=\left\{f:\int_{0}^{1}[f(t)]^{2}dt<\infty\right\}\]

with the inner product

\[\langle f,g\rangle_{\mathcal{L}^{2}[0,1]}\equiv\int_{0}^{1}f(t)g(t)dt,\]

the evaluation functionals are not continuous, so no r.k. exists. For example, if \(t_{n}=t_{0}-(1/n)\) and we define the \(\mathcal{L}^{2}[0,1]\) functions \(f(t)=\mathcal{I}_{[0,t_{0}]}(t)\) and \(f_{n}(t)=\mathcal{I}_{[0,t_{n})}(t)\) we have

\[||f-f_{n}||=\sqrt{\int_{0}^{1}\mathcal{I}_{[t_{n},t_{0}]}(t)dt}=\frac{1}{\sqrt {n}}\to 0\]

but

\[f_{n}(t_{0})=0\not\to 1=f(t_{0}).\]

Example 3.3.10. : Consider the Hilbert space

\[\mathcal{W}_{0}^{1}=\left\{f\in\mathcal{L}^{2}[0,1]:f(0)=0,\int_{0}^{1}[\dot{f }(t)]^{2}dt<\infty\right\}\]

with inner product

\[\langle f,g\rangle_{\mathcal{W}_{0}^{1}}=\int_{0}^{1}\dot{f}(t)\dot{g}(t)dt. \tag{3.3.1}\]

In Sect. 3.1.1, we found the reproducing kernel to be \(R(s,t)=\min(s,t)\). For fixed \(t\), \(R(\cdot,t)\) is an element of the function space \(\mathcal{W}_{0}^{1}\), since \(R(0,t)=0\) and \(\int_{0}^{1}[\mathbf{d}_{s}R(s,t)]^{2}ds<\infty\). Also, as shown earlier, \(R(\cdot,\cdot)\) has the reproducing property.

\(\Box\)

Example 3.3.11. : Consider the Hilbert space

\[\mathcal{W}_{0}^{2}=\left\{f\in\mathcal{L}^{2}[0,1]:f(0)=\dot{f}(0)=0,\int_{0}^ {1}[\ddot{f}(t)]^{2}dt<\infty\right\}\]

with inner product

\[\langle f,g\rangle_{\mathcal{W}_{0}^{2}}=\int_{0}^{1}\ddot{f}(t)\ddot{g}(t)dt.\]We begin by finding the second derivative of the representation of the evaluation functional. In particular, we show that

\[f(s)=\int_{0}^{1}{(s-u)_{+}\ddot{f}(u)du}\,, \tag{3.3.2}\]

where \((a)_{+}\) is \(a\) for \(a>0\) and \(0\) for \(a\leq 0\).

Given any arbitrary and fixed \(s\in[0,1]\),

\[\int_{0}^{1}{(s-u)_{+}\ddot{f}(u)du}=\int_{0}^{s}{(s-u)\ddot{f}(u)du}.\]

Integrating by parts

\[\int_{0}^{s}{(s-u)\ddot{f}(u)du}=(s-s)\dot{f}(s)-(s-0)\dot{f}(0)+\int_{0}^{s}{ \dot{f}(u)du}=\int_{0}^{s}{\dot{f}(u)du}\]

and applying the Fundamental Theorem of Calculus to the last term,

\[\int_{0}^{s}{(s-u)\ddot{f}(u)du}=f(s)-f(0)=f(s).\]

Since the r.k. of the space \(\mathcal{W}_{2}^{0}\) must satisfy \(f(s)=\langle f(\cdot),R(\cdot,s)\rangle\), we see that \(R(\cdot,s)\) is a function such that

\[\mathbf{d}_{uu}^{2}R(u,s)=(s-u)_{+}.\]

We also know that \(R(\cdot,s)\in\mathcal{W}_{2}^{0}\), so using \(R(s,t)=\langle R(\cdot,t),R(\cdot,s)\rangle\)

\[R(s,t)=\int_{0}^{1}{(t-u)_{+}(s-u)_{+}du}=\frac{\max(s,t)\min^{2}(s,t)}{2}- \frac{\min^{3}(s,t)}{6}. \tag{3.3.3}\]

\(\Box\)

**Exercise 3.2.**  Do the calculus to establish Eq. (3.3.1).

Example 3.3.12.: Consider all constant functionals on \(\mathbf{R}^{p-1}\),

\[\mathcal{F}_{0}=\{f_{a}\in\mathcal{F}:f_{a}(x)=a,\;a\in\mathbf{R},\;x\in \mathbf{R}^{p-1}\},\]

with inner product

\[\langle f_{a},f_{b}\rangle_{\mathcal{F}_{0}}=ab.\]

\(\mathcal{F}_{0}\) has continuous evaluation functionals \(e_{x}(f)=f(x)\), so it is an RKHS and has a unique reproducing kernel. To find the r.k., observe that \(R(\cdot,x)\in\mathcal{F}_{0}\), so it is a constant for any \(x\). Write \(R(x)\equiv R(\cdot,x)\). By the representation theorem and the defined inner product

\[a=f_{a}(x)=\langle f_{a}(\cdot),R(\cdot,x)\rangle_{\mathcal{F}_{0}}=aR(x)\]

for any \(x\) and \(a\). This implies that \(R(x)\equiv 1\) so that \(R(\cdot,x)\equiv 1\) and \(R(\cdot,\cdot)\equiv 1\). \(\Box\)Example 3.13.13.: Consider all linear functionals on \(\mathbf{R}^{p-1}\) passing through the origin,

\[\mathscr{F}_{1}=\{f_{\gamma}\in\mathscr{F}:f_{\gamma}(x)=x^{\prime}\gamma,\; \gamma\in\mathbf{R}^{p-1},\;x\in\mathbf{R}^{p-1}\},\]

with inner product

\[\langle f_{\eta},f_{\gamma}\rangle_{\mathscr{F}_{1}}=\eta^{\prime}\gamma=\eta_ {1}\gamma_{1}+\eta_{2}\gamma_{2}+\dots+\eta_{p-1}\gamma_{p-1}.\]

The r.k. \(R\) must satisfy

\[f_{\gamma}(x)=\langle f_{\gamma}(\cdot),R(\cdot,x)\rangle_{\mathscr{F}_{1}}\]

for all \(\gamma\) and any \(x\). Since \(R(\cdot,x)\in\mathscr{F}_{1}\), \(R(v,x)=v^{\prime}u\) for some \(u\) that depends on \(x\), i.e., \(u(x)\) is the vector in \(\mathbf{R}^{p-1}\) that determines the functional \(R(\cdot,x)\in\mathscr{F}_{1}\), so \(R(\cdot,x)=f_{u(x)}(\cdot)\) and \(R(v,x)=v^{\prime}u(x)\). By our definition of \(\mathscr{F}_{1}\) we have

\[x^{\prime}\gamma=f_{\gamma}(x)=\langle f_{\gamma}(\cdot),R(\cdot,x)\rangle_{ \mathscr{F}_{1}}=\langle f_{\gamma}(\cdot),f_{u(x)}(\cdot)\rangle_{\mathscr{F }_{1}}=\gamma^{\prime}u(x),\]

so we need \(u(x)\) such that for any \(\gamma\) and \(x\) we have

\[x^{\prime}\gamma=u(x)^{\prime}\gamma.\]

It follows that \(x=u(x)\). For example, taking \(\gamma\) to be the indicator vector \(e_{i}\) implies that \(x_{i}=u_{i}(x)\) for every \(i=1,\dots,p-1\). We now have \(R(\cdot,x)=f_{x}(\cdot)\) so that

\[R(\tilde{x},x)=\langle R(\cdot,\tilde{x}),R\cdot,x)\rangle_{\mathscr{F}_{1}}= \langle f_{\tilde{x}},f_{x}\rangle_{\mathscr{F}_{1}}=\tilde{x}^{\prime}x=x_{1} \tilde{x}_{1}+x_{2}\tilde{x}_{2}+\dots+x_{p-1}\tilde{x}_{p-1}.\qed\]

One point of these examples is that if you can characterize the evaluation functional \(e_{t}(\cdot)\), then frequently you can find \(R(s,t)\). For further examples of RKHSs with various inner products, see Berlinet and Thomas-Agnan (2004).

One further concept is useful in RKHS approaches to regression problems.

#### The Projection Principle for an RKHS

Consider the connection between the reproducing kernel \(R\) of the RKHS \(\mathscr{H}\) and the reproducing kernel \(R_{0}\) for a subspace \(\mathscr{H}_{0}\subset\mathscr{H}\). When any vector \(f\in\mathscr{H}\) can be written uniquely as \(f=f_{0}+f_{1}\) with \(f_{0}\in\mathscr{H}_{0}\) and \(f_{1}\in\mathscr{H}_{0}^{\perp}\), more particularly, \(R(\cdot,t)=R_{0}(\cdot,t)+R_{1}(\cdot,t)\) with \(R_{0}(\cdot,t)\in\mathscr{H}_{0}\) and \(R_{1}(\cdot,t)\in\mathscr{H}_{0}^{\perp}\) if and only if \(R_{0}\) is the r.k. of \(\mathscr{H}_{0}\) and \(R_{1}\) is the r.k. of \(\mathscr{H}_{0}^{\perp}\). For a proof see Gu (2002).

Example 3.14.: Consider the affine functionals on \(\mathbf{R}^{p-1}\),

\[\mathscr{F}_{*}=\{f_{\beta}\in\mathscr{F}:f_{\beta}(x)=\beta_{0}+\beta_{1}x_{1 }+\dots+\beta_{p-1}x_{p-1},\;\beta\in\mathbf{R}^{p},\;x\in\mathbf{R}^{p-1}\}\]with inner product

\[\langle f_{\beta},f_{\eta}\rangle_{\mathscr{F}_{*}}=\beta^{\prime}\eta=\beta_{0} \eta_{0}+\beta_{1}\eta_{1}+\ldots+\beta_{p-1}\eta_{p-1}.\]

We have already derived the r.k.'s for the constant functionals \(\mathscr{F}_{0}\) and linear functionals \(\mathscr{F}_{1}\equiv\mathscr{F}_{0}^{\perp}\) (call them \(R_{0}\) and \(R_{1}\), respectively) in Examples 3.3.12 and 3.3.13. Applying the projection principle, the r.k. for \(\mathscr{F}_{*}\) is the sum of \(R_{0}\) and \(R_{1}\), i.e.,

\[R(\tilde{x},x)=1+\tilde{x}^{\prime}x.\qed\]

For two subspaces \(\mathscr{A}\) and \(\mathscr{B}\) contained in a vector space \(\mathscr{H}\), the _direct sum_ is the space \(\mathscr{D}=\{a+b:a\in\mathscr{A},b\in\mathscr{B}\}\), written \(\mathscr{D}=\mathscr{A}\oplus\mathscr{B}\) or just \(\mathscr{D}=\mathscr{A}+\mathscr{B}\). Any elements \(d_{1},d_{2}\in\mathscr{D}\) can be written as \(d_{1}=a_{1}+b_{1}\) and \(d_{2}=a_{2}+b_{2}\), respectively, for some \(a_{1},a_{2}\in\mathscr{A}\) and \(b_{1},b_{2}\in\mathscr{B}\). When the two subspaces have \(\mathscr{A}\cap\mathscr{B}=\{0\}\), those decompositions are unique. If the vector space \(\mathscr{H}\) is also a Hilbert space and \(\mathscr{A}\perp\mathscr{B}\), the decomposition is unique and the inner product between \(d_{1}\) and \(d_{2}\) is \(\langle d_{1},d_{2}\rangle=\langle a_{1},a_{2}\rangle+\langle b_{1},b_{2}\rangle\). In particular, for any finite dimensional subspace \(\mathscr{H}_{0}\) of a Hilbert space \(\mathscr{H}\), we have \(\mathscr{H}=\mathscr{H}_{0}\oplus\mathscr{H}_{0}^{\perp}\) and \(\mathscr{H}_{0}\perp\mathscr{H}_{0}^{\perp}\).

In something of a converse, suppose \(\mathscr{A}\) and \(\mathscr{B}\) are Hilbert space subspaces of a vector space \(\mathscr{M}\) that has no norm, and suppose \(\mathscr{A}\cap\mathscr{B}=\{0\}\). Then we can define a Hilbert space on \(\mathscr{D}=\mathscr{A}\oplus\mathscr{B}\) by defining \(\langle d_{1},d_{2}\rangle_{\mathscr{D}}\equiv\langle a_{1},a_{2}\rangle_{ \mathscr{A}}+\langle b_{1},b_{2}\rangle_{\mathscr{B}}\). With respect to the Hilbert space \(\mathscr{D}\), \(\mathscr{A}\perp\mathscr{B}\). For more information about direct sum decompositions see, for example, Berlinet and Thomas-Agnan (2004) or Gu (2002).

### Two Approaches

There seems to be at least two ways to use RKHS results. One is to define a penalty function for estimation that relates to an RKHS and figure out (or at least demonstrate) what the appropriate reproducing kernel is. Our applications up to this point and in the next section focus on that approach. The other approach is to choose an appropriate "kernel" function for vectors in \(\mathscr{M}\), say, \(R(u,v)\) and to build an RKHS around the kernel. The idea then is to work within this RKHS without really ever leaving \(\mathscr{M}\). We now show how to construct such an RKHS. The next subsection illustrates the use of such an RKHS.

For an appropriate kernel function \(R(\cdot,\cdot)\), one can find (but the point is that one need not find) a positive definite eigen-decomposition (_singular value decomposition_) of the function,

\[R(u,v)=\sum_{j=1}^{\infty}\eta_{j}\phi_{j}(u)\phi_{j}(v). \tag{3.4.1}\]

The existence of such an eigen-decomposition implies that \[R(\cdot,v)=\sum_{j=1}^{\infty}\eta_{j}\phi_{j}(\cdot)\phi_{j}(v).\]

Associated with the eigen-decomposition is a Hilbert space

\[\mathcal{H}=\left\{f:f=\sum_{j=1}^{\infty}\alpha_{j}\phi_{j}\quad\text{with} \quad\sum_{j=1}^{\infty}\alpha_{j}^{2}/\eta_{j}<\infty\right\}\]

having inner product

\[\left\langle\sum_{j=1}^{\infty}\alpha_{j}\phi_{j},\sum_{j=1}^{\infty}\beta_{j} \phi_{j}\right\rangle\equiv\left\langle\sum_{j=1}^{\infty}\alpha_{j}\phi_{j}( \cdot),\sum_{j=1}^{\infty}\beta_{j}\phi_{j}(\cdot)\right\rangle\equiv\sum_{j=1} ^{\infty}\alpha_{j}\beta_{j}/\eta_{j}.\]

Although the eigen-decomposition in (3.4.1) need not be unique because eigenvalues \(\eta_{j}\) may have multiplicities greater than 1, nonetheless every decomposition involves a set of basis functions \(\phi_{j}\) that define a unique representation and all span the same space \(\mathcal{H}\).

The chosen kernal function is the r.k. on \(\mathcal{H}\) because \(R(\cdot,v)\) represents the evaluation functional, i.e.,

\[\left\langle R(\cdot,v),\sum_{j=1}^{\infty}\alpha_{j}\phi_{j}( \cdot)\right\rangle =\left\langle\sum_{j=1}^{\infty}\eta_{j}\phi_{j}(\cdot)\phi_{j}( v),\sum_{j=1}^{\infty}\alpha_{j}\phi_{j}(\cdot)\right\rangle\] \[=\sum_{j=1}^{\infty}\eta_{j}\phi_{j}(v)\alpha_{j}/\eta_{j}\] \[=\sum_{j=1}^{\infty}\alpha_{j}\phi_{j}(v).\]

In this context, some commonly used kernels are, when \(\mathcal{M}\) is a Hilbert space, the polynomial kernels for positive integers \(r\)

\[R(u,v)\equiv(1+\left\langle u,v\right\rangle_{\mathcal{M}})^{r}\]

and, when \(\mathcal{M}\) is a Banach space, the radial basis kernel

\[R(u,v)\equiv\exp(-\gamma\|u-v\|_{\mathcal{M}})\]

for some positive scalar \(\gamma.\) Most often the space \(\mathcal{M}\) is Euclidean \(\mathbb{R}^{p}.\)

We now briefly apply these ideas, and the idea of never actually using the space \(\mathcal{H},\) to the problem of testing lack of fit in a linear model.

#### Testing Lack of Fit

To translate these ideas back into linear regression theory, consider a linear model \(Y=X\beta+e\) with \(X\) written in terms of its \(p\) dimensional row vectors as

\[X=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}.\]

For a kernel function defined on \(\mathbf{R}^{p}\), whenever the eigen-decomposition is finite, say,

\[R(u,v)=\sum_{j=1}^{s}\eta_{j}\phi_{j}(u)\phi_{j}(v)\]

we can define

\[\Phi_{k}=[\phi_{k}(x_{1}),\cdots,\phi_{k}(x_{n})]^{\prime},\quad\Phi=[\Phi_{1},\cdots,\Phi_{s}],\]

and fit a new regression model

\[Y=\Phi\alpha+e,\quad\mathrm{E}(e)=0. \tag{3.4.2}\]

This new model is appropriate for testing lack of fit because either we get \(C(X)\subset C(\Phi)\) or, if not, we could use the model matrix \([X,\Phi]\) instead, with little change to our discussion.

Because the \(\eta_{k}\)s are positive, using \(PA\) Proposition B.51 we see that

\[C(\Phi)=C\,(\Phi D(\sqrt{\eta_{i}}))=C(\Phi D(\eta_{i})\Phi^{\prime}).\]

The key fact is that

\[\Phi D(\eta_{i})\Phi^{\prime}=[R(x_{h},x_{i})]_{n\times n}\equiv\bar{R},\]

so instead of fitting the linear model (3.4.2) we can fit the equivalent model

\[Y=\bar{R}\xi+e\]

to obtain \(\hat{\xi}\), fitted values, and residuals. Moreover, we can make a prediction for a new observed vector \(x_{0}\) by using

\[\hat{y}_{0}=[R(x_{0},x_{1}),\cdots,R(x_{0},x_{n})]\hat{\xi}.\]

Thus we can execute the analysis without ever specifying the \(\phi_{k}\)s or \(\eta_{k}\)s, but it would be useful to know \(s\) in order to find the MSE. The fact that the matrix \(\Phi\) can be replaced by the matrix \(\bar{R}\) is known as the _kernel trick_.

Typically, one would also want to modify this discussion to deal with an intercept in the model and thus use vectors in \(\mathbf{R}^{p-1}\) rather than \(\mathbf{R}^{p}\). Moreover, there is almost nothing in this discussion that precludes the eigen-decomposition from being infinite. The only problem with an infinite decomposition is the likelihood of getting \(C(\tilde{R})=\mathbf{R}^{n}\) when the \(x_{i}\)s are distinct and a not very interesting regression model (even when the \(x_{i}\)s are not distinct). In fact, even for finite \(s\) we need to worry about overfitting the data.

Example 3.4.1.: Consider a linear model \(y_{i}=\beta_{0}+x_{i}^{\prime}\beta_{s}+\varepsilon_{i}\) or, in matrix form,

\[Y=X\beta+e=[J,Z]\begin{bmatrix}\beta_{0}\\ \beta_{s}\end{bmatrix}+e\]

with

\[Z=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}.\]

This model was used in Chap. 2 and in _PA_ Sect. 6.2.

We illustrate the use of

\[R(x_{i},x_{h})\equiv(1+x_{h}^{\prime}x_{i})^{2}\]

when \(p-1=2\) so that \(x_{i}^{\prime}=(x_{i1},x_{i2})\). Write

\[Z=[X_{1},X_{2}],\quad X_{1}=[x_{i1}],\quad X_{2}=[x_{i2}]\]

and define the \(n\) dimensional vectors

\[X_{1}^{2}\equiv[x_{i1}^{2}],\quad X_{2}^{2}\equiv[x_{i2}^{2}],\quad X_{1}X_{2 }\equiv[x_{i1}x_{i2}].\]

Note that the linear model

\[Y=W\gamma+e\equiv\gamma_{00}+\gamma_{10}X_{1}+\gamma_{01}X_{2}+\gamma_{20}X_{ 1}^{2}+\gamma_{02}X_{2}^{2}+\gamma_{11}X_{1}X_{2}+e\]

is a quadratic model in the two predictor variables. The \(W\) model contains the linear by linear interaction term \(X_{1}X_{2}\). It does not contain the analogously defined linear by quadratic terms \(X_{1}X_{2}^{2}\), \(X_{1}^{2}X_{2}\) or the quadratic by quadratic term \(X_{1}^{2}X_{2}^{2}\)

It is not difficult to see that in our example

\[R(x_{i},x_{h})\equiv(1+x_{h}^{\prime}x_{i})^{2}=1+2x_{h1}x_{i1}+2x_{h2}x_{i2}+x _{h1}^{2}x_{i1}^{2}+x_{h2}^{2}x_{i2}^{2}+2x_{h1}x_{i1}x_{h2}x_{i2}\]

which leads to the \(h\)th column of \(\tilde{R}\) being

\[\tilde{R}_{h}=J+2x_{h1}X_{1}+2x_{h2}X_{2}+x_{h1}^{2}X_{1}^{2}+x_{h2}^{2}X_{2}^ {2}+2x_{h1}x_{h2}X_{1}X_{2}.\]

Clearly, \(\tilde{R}_{h}\in C(J,X_{1},X_{2},X_{1}^{2},X_{2}^{2},X_{1}X_{2})=C(W)\), so \(C(\tilde{R})\subset C(W)\). Moreover, for regression data it is almost inconceivable that no 6 of the columns of the \(n\times n\) matrix \(\tilde{R}\) would be linearly independent, so in all likelihood \(C(\tilde{R})=C(W)\). In this case, fitting the model using the kernel trick is equivalent to fitting a quadratic model with linear by linear interaction to the data. We will come back to this with a slightly different and more precise argument shortly.

In this illustration, the point of the kernel trick is that instead of having to go to the trouble of defining \(W\), one can use \(R(x_{i},x_{h})\) to define \(\tilde{R}\). Doesn't seem like much of an advantage in this problem, does it? In fact, it is hard to see where using the kernel trick with the polynomial kernels would ever provide much of an advantage. On the other hand, we saw in Chap. 1 that fitting cubic splines involved fitting a linear model with some pretty horrible linear constraints. (Admittedly, we found other ways to fit spline models without using the horrible linear constraints.) In the next section, an appropriate r.k. enables us to fit the cubic spline model without linear constraints.

A slightly more detailed argument makes the quadratic polynomial kernel relationship more precise. Redefine \(x^{\prime}_{1}=(1,a,b)\), \(x^{\prime}_{2}=(1,c,d)\). Then \(x^{\prime}_{1}x_{2}=1+\{ac+bd\}\). Now consider transforming \(x_{1}\) and \(x_{2}\) into \(\phi^{\prime}_{1}\equiv[\phi_{1}(x_{1}),\ldots,\phi_{6}(x_{1})]=(2,2a,2b,a^{2},\sqrt{2}ab,b^{2})\) and \(\phi^{\prime}_{2}=(2,2c,2d,c^{2},\sqrt{2}cd,d^{2})\). With the inner product defined by the second order polynomial r.k.,

\[R(x_{1},x_{2}) = \left(1+x^{\prime}_{1}x_{2}\right)^{2}\] \[= 1+2x^{\prime}_{1}x_{2}+\left(x^{\prime}_{1}x_{2}\right)^{2}\] \[= 1+2\left(1+\{ac+bd\}\right)+\left(1+\{ac+bd\}\right)^{2}\] \[= 1+2+2\{ac+bd\}+\left[1+2\{ac+bd\}+\left(\{ac+bd\}\right)^{2}\right]\] \[= 4+4ac+4bd+a^{2}c^{2}+2abcd+b^{2}d^{2}\] \[= \phi^{\prime}_{1}\phi_{2}.\]

Now, with

\[\Phi=\begin{bmatrix}\phi^{\prime}_{1}\\ \vdots\\ \phi^{\prime}_{n}\end{bmatrix},\]

we have

\[\tilde{R}=[R(x_{i},x_{j})]=[\phi^{\prime}_{i}\phi_{j}]=\Phi\Phi^{\prime},\]

and we know from \(PA\) Appendix B that \(C(\tilde{R})=C(\Phi\Phi^{\prime})=C(\Phi)\). It is quite obvious from the construction of \(\Phi\) that \(C(\Phi)=C(W)\). In this case, typically \(r(\tilde{R})=6\). Again, this model only includes a linear by linear interaction term \(X_{1}X_{2}\) whereas the basic proposal for multivariate nonparametric regression in Chap. 1 also included the linear by quadratic term \(X_{1}X_{2}^{2}\), the quadratic by linear term \(X_{1}^{2}X_{2}\), and the quadratic by quadratic term \(X_{1}^{2}X_{2}^{2}\). 

Exercise 3.3: In terms of the notation for this section, the argument in Example 3.4.1 using \(x^{\prime}_{i}=(x_{i1},x_{i2})\) involves \(\mathcal{M}=\mathbf{R}^{2}\). What is \(\mathcal{H}^{\prime}\)?Typically, with distinct \(x_{i}\)s, the radial basis kernel leads to \(\phi_{i}\) being an infinite vector and \(r(\tilde{R})=n\). Even the polynomial models quickly rise in rank when \(p\) gets to be of moderate size. In such cases, one should worry about overfitting. One way to address overfitting is penalized estimation.

### Penalized Regression with RKHSs

The nonparametric regression model is

\[y_{i}=f(x_{i})+\varepsilon_{i},\quad i=1,2,\ldots,n,\]

where \(f\) is an unknown regression function and the \(\varepsilon_{i}\) are independent, mean 0 error terms. We start this section with three common examples of penalized regression: ridge regression, lasso regression, and smoothing splines.

#### Ridge and Lasso Regression

The classical linear regression setting is notationally identical to Example 3.4.1, i.e., \(y_{i}=\beta_{0}+x_{i}^{\prime}\beta_{*}+\varepsilon_{i}\) with \(\beta^{\prime}=(\beta_{0},\beta_{*}^{\prime})\). The classical ridge regression estimator \(\hat{\beta}_{R}\) proposed by Hoerl and Kennard (1970) minimizes

\[\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p-1}\beta_{j}x_{ij}\right)^{2} +\lambda n\sum_{j=1}^{p-1}\beta_{j}^{2} \tag{3.5.1}\]

where \(x_{ij}\) is the \(i\)th observation on the \(j\)th predictor variable. The resulting estimate is biased but can reduce the variance relative to least squares estimates. The tuning parameter \(\lambda\geq 0\) is a constant that controls the trade-off between bias and variance in \(\hat{\beta}_{R}\), and is often selected by some form of cross validation, see Sect. 3.6.

The lasso regression estimator \(\hat{\beta}_{L}\) proposed by Tibshirani (1996) minimizes

\[\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p-1}\beta_{j}x_{ij}\right)^{2} +\lambda n\sum_{j=1}^{p-1}|\beta_{j}|\]

The \(\mathcal{L}_{1}\) norm for the penalty defines a Banach space but does not lend itself to an inner product, so RKHS results do not readily apply to lasso problems. However, Lin and Zhang (2006) and Storlie et al. (2010) used the RKHS framework to develop smoothing spline versions of the lasso and the adaptive lasso (cf. Zou 2006), respectively.

#### Smoothing Splines

Smoothing splines are among the most popular methods for the estimation of \(f\) due to their good empirical performance and sound theoretical support. We assume that the domain of \(f\) is \([0,1]\). With \(f^{(m)}\) the \(m\)th derivative of \(f\), a smoothing spline estimate \(\hat{f}\) is a minimizer of

\[\sum_{i=1}^{n}\left[y_{i}-f(x_{i})\right]^{2}+\lambda n\int\left[f^{(m)}(x) \right]^{2}dx \tag{3.5.2}\]

The minimization of (3.5.2) is implicitly over functions with square integrable \(m\)-th derivatives. The first term in (3.5.2) encourages the fitted \(f\) to be close to the data, while the second term penalizes the roughness of \(f\). The smoothing parameter \(\lambda\) controls the trade-off between the two conflicting goals. The special case of \(m=1\) is the linear smoothing spline problem.

In practice it is common to choose \(m=2\) in which case the minimizer \(\hat{f}\) of (3.5.2) is called a cubic smoothing spline. As \(\lambda\to\infty\), \(\hat{f}\) approaches the least squares simple linear regression line, while as \(\lambda\to 0\), \(\hat{f}\) approaches the minimum curvature interpolant.

#### Solving the General Penalized Regression Problem

We now review a general framework that allows us to minimize (3.5.1) and (3.5.2) and many other similar problems, cf. O'Sullivan, Yandell, and Raynor (1986), Lin and Zhang (2006), Storlie et al. (2009, 2010), Gu (2002). The data model is

\[y_{i}=f(x_{i})+\epsilon_{i},\quad\operatorname{E}(\epsilon_{i})=0,\quad i=1,2, \ldots,n, \tag{3.5.3}\]

where the \(\epsilon_{i}\) are error terms and \(f\in\mathcal{M}\), a given vector space of functions on \(\mathcal{E}\).

Let \(\mathcal{P}\) be a nonnegative penalty functional on \(\mathcal{M}\) with restrictions that are discussed later. An estimate of \(f\) is obtained by minimizing

\[\frac{1}{n}\sum_{i=1}^{n}[y_{i}-f(x_{i})]^{2}+\lambda\mathcal{P}(f), \tag{3.5.4}\]

over \(f\in\mathcal{S}\subset\mathcal{M}\) where \(\mathcal{S}\) is chosen in a specific way.

We require that \(\mathcal{P}\), in addition to being nonnegative, has a null set \(\mathcal{N}=\{f\in\mathcal{M}:\mathcal{P}(f)=0\}\) that is a subspace, that for \(f_{N}\in\mathcal{N}\) and \(f\in\mathcal{M}\), we have \(\mathcal{P}(f_{N}+f)=\mathcal{P}(f)\), and that there exists an RKHS \(\mathcal{H}\subset\mathcal{M}\) for which the inner product satisfies \(\langle f,f\rangle=\mathcal{P}(f)\). This condition forces \(\mathcal{N}\cap\mathcal{H}=\{0\}\). Finally, we consider a finite dimensional subspace of \(\mathcal{N}\), say \(\mathcal{N}_{0}\), with basis functions \(\{\psi_{1},\ldots,\psi_{s}\}\) and define

\[\mathcal{S}\equiv\mathcal{N}_{0}\oplus\mathcal{H}.\]In many applications \(\mathcal{N}_{0}=\mathcal{N}\).

**Example 3.5.1**: _Linear Interpolating Splines._

Using Example 3.3.10 we can take \(\mathcal{M}\) as the subspace of \(\mathcal{L}^{2}[0,1]\) with finite values of

\[\mathcal{P}(f)\equiv\int_{0}^{1}\left[\hat{f}(t)\right]^{2}dt.\]

This \(\mathcal{P}(f)\) satisfies our four conditions with \(\mathcal{H}=\mathcal{W}_{0}^{1}\). The constant functions \(f(t)=a\) comprise \(\mathcal{N}=\mathcal{N}_{0}\), so we take \(\psi_{1}(x)\equiv 1\). Note that Eq. (3.1.6) defines an inner product on \(\mathcal{W}_{0}^{1}\) but does not define an inner product on all of \(\mathcal{M}\) because nonzero functions can have a zero inner product with themselves, hence the nontrivial nature of \(\mathcal{N}\). \(\Box\)

Some nice things happen if \(\mathcal{M}\) is a Hilbert space. In particular, for any two subspaces \(\mathcal{N}\) and \(\mathcal{H}\) with \(\mathcal{N}\perp\mathcal{H}\), we have for any \(f\) in their direct sum a unique decomposition \(f=f_{0}+f_{1}\) with \(f_{0}\in\mathcal{N}\) and \(f_{1}\in\mathcal{H}\). This allows us to define \(\mathcal{P}(f)\equiv\langle f_{1},f_{1}\rangle\) and have all our assumptions met. (Technically, we should define \(\mathcal{P}\) for all functions \(f\in\mathcal{M}\) but how we extend it beyond the direct sum is irrelevant.) The orthogonal decomposition of \(\mathcal{S}\) is closely related to generalized additive models, cf. Wood (2006), and more generally to smoothing spline ANOVA models, cf. Gu (2002), which also include tensor product splines as a special case. Thin plate splines also fall nicely into the general RKHS framework, cf. Wahba (1990).

The key result, Wahba's Representation Theorem, also known as the _dual form_ or _kernel trick_, cf. Pearce and Wand (2006), is that the minimizer of (3.5.4) is a finite linear combination of known basis functions and functions involving the reproducing kernel on \(\mathcal{H}\). This allows us to find the coefficients of the linear combination by solving a quadratic minimization problem similar to those in standard linear models.

**Theorem 3.5.2**: _Representation Theorem._

Any minimizer \(\hat{f}\in\mathcal{S}\) of Eq. (3.5.4) has the form

\[\hat{f}(x)=\sum_{j=1}^{s}\beta_{j}\psi_{j}(x)+\sum_{i=1}^{n}\xi_{i}R(x_{i},x), \tag{3.5.5}\]

where \(R(\cdot,\cdot)\) is the r.k. for \(\mathcal{H}\).

Proof: An informal proof is given. See Wahba (1990) or Gu (2002) for a formal proof.

Since we are working in \(\mathcal{S}\), clearly, any minimizer \(\hat{f}\) must have \(\hat{f}=\hat{f}_{0}+\hat{f}_{1}\) with \(\hat{f}_{0}\in\mathcal{N}_{0}\) and \(\hat{f}_{1}\in\mathcal{H}\). We want to establish that \(\hat{f}_{1}(\cdot)=\sum_{i=1}^{n}\xi_{i}R(x_{i},\cdot)\) for some \(\xi_{i}\)s. Decompose \(\mathcal{H}\) as \(\mathcal{H}=\mathcal{H}_{0}\oplus\mathcal{H}_{0}^{\perp}\) where \(\mathcal{H}_{0}=\text{span}\{R(x_{i},\cdot),i=1,\ldots,n\}\) so that

\[\hat{f}_{1}(\cdot)=\hat{f}_{R}(\cdot)+\eta(\cdot),\]with

\[\hat{f}_{R}(\cdot)\equiv\sum_{i=1}^{n}\xi_{i}R(x_{i},\cdot) \tag{3.5.6}\]

and \(\eta(\cdot)\in\mathscr{H}_{0}^{\perp}\). By orthogonality and the reproducing property of the r.k.,

\[0=\langle R(x_{i},\cdot),\eta(\cdot)\rangle=\eta(x_{i}),\quad i=1,\ldots,n.\]

We now establish the representation theorem. Using our assumptions about \(\mathscr{P}\),

\[\frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{f}(x_{i})]^{2}+\lambda \mathscr{P}(\hat{f}) = \frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{f}_{0}(x_{i})-\hat{f}_{1}(x _{i})]^{2}+\lambda\mathscr{P}(\hat{f}_{0}+\hat{f}_{1})\] \[= \frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{f}_{0}(x_{i})-\hat{f}_{1}(x _{i})]^{2}+\lambda\mathscr{P}(\hat{f}_{1})\] \[= \frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{f}_{0}(x_{i})-\hat{f}_{R}(x _{i})-\eta(x_{i})]^{2}+\lambda\mathscr{P}(\hat{f}_{R}+\eta).\]

Because \(\eta(x_{i})=0\) and using orthogonality within \(\mathscr{H}\)

\[\frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{f}(x_{i})]^{2}+\lambda \mathscr{P}(\hat{f}) = \frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{f}_{0}(x_{i})-\hat{f}_{R}(x _{i})]^{2}+\lambda\left[\mathscr{P}(\hat{f}_{R})+\mathscr{P}(\eta)\right]\] \[\geq \frac{1}{n}\sum_{i=1}^{n}[y_{i}-\hat{f}_{0}(x_{i})-\hat{f}_{R}(x _{i})]^{2}+\lambda\mathscr{P}(\hat{f}_{R}).\]

Clearly, any \(\eta\neq 0\) makes the inequality strict, so minimizers have \(\eta=0\) and \(\hat{f}=\hat{f}_{0}+\hat{f}_{R}\) with the last inequality an equality. \(\Box\)

**Corollary 3.5.3.** : Among \(f\in\mathscr{S}\), the least squares estimate with minimum penalty satisfies the relation (3.5.5).

Proof: As in the proof of the Theorem 3.5.2, any \(f\in\mathscr{S}\) can be written as \(f_{0}+f_{R}+\eta\) and

\[\frac{1}{n}\sum_{i=1}^{n}[y_{i}-f_{0}(x_{i})-f_{R}(x_{i})-\eta(x_{i})]^{2}= \frac{1}{n}\sum_{i=1}^{n}[y_{i}-f_{0}(x_{i})-f_{R}(x_{i})]^{2}.\]

However,

\[\mathscr{P}(f)=\mathscr{P}(f_{0}+f_{R}+\eta)=\mathscr{P}(f_{R}+\eta)=\mathscr{ P}(f_{R})+\mathscr{P}(\eta)\geq\mathscr{P}(f_{R})=\mathscr{P}(f_{0}+f_{R}).\]

Thus for any \(f\in\mathscr{S}\), there is a function of the form (3.5.5) that has the same squared error and at least as small a penalty. \(\Box\)

A remarkable feature of the result in (3.5.5) is that the form of the minimizer is represented by a finite dimensional basis, regardless of the dimension of \(\mathscr{H}\)For example, \(\mathcal{H}=\mathcal{W}_{0}^{1}\) requires an infinite expansion of basis functions to represent all functions in the space, yet the _solution_ of the minimization can be represented by a finite basis!

Once we know that the minimizer takes the form (3.5.5), we can find the coefficients of the linear combination by solving a quadratic minimization problem similar to those in standard linear models. This occurs because we can write \(\mathcal{P}(\hat{f})=\mathcal{P}(\hat{f}_{R})\) as a quadratic form in \(\xi=(\xi_{1},\ldots,\xi_{n})^{\prime}\). Define \(\tilde{R}\) as the \(n\times n\) matrix where the \(i,j\) entry is \(\tilde{r}_{ij}=R(x_{i},x_{j})\). The matrix \(\tilde{R}\) is commonly referred to as the _Gram_ matrix. Now, using the reproducing property of \(R\), write

\[\mathcal{P}(\hat{f}_{R})=\left\langle\sum_{i=1}^{n}\xi_{i}R(x_{i},\cdot),\sum_{ j=1}^{n}\xi_{j}R(x_{j},\cdot)\right\rangle=\sum_{i=1}^{n}\sum_{j=1}^{n}\xi_{i} \xi_{j}R(x_{i},x_{j})=\xi^{\prime}\tilde{R}\xi.\]

Define the observation vector \(Y=[y_{1},\ldots,y_{n}]^{\prime}\), and let \(W\) be the \(n\times s\) matrix defined by \(w_{ij}=\psi_{j}(x_{i})\). With the usual norm for Euclidean \(\mathbf{R}^{n}\), the minimization of (3.5.4) takes the form

\[\min_{\beta,\xi}\left\{\frac{1}{n}\|Y-W\beta-\tilde{R}\xi\|^{2}+\lambda\xi^{ \prime}\tilde{R}\xi\right\}. \tag{3.5.7}\]

The minimization in (3.5.7) is a special case of the generalized ridge regression minimization problem (2.2.2) and has the solutions of (2.2.4),

\[\hat{\xi}=[\tilde{R}(I-M_{W})\tilde{R}+\lambda n\tilde{R}]^{-}\tilde{R}(I-M_{W })Y\qquad\hat{\beta}=[W^{\prime}W]^{-}W^{\prime}(Y-\tilde{R}\hat{\xi}), \tag{3.5.8}\]

where \(M_{W}\) is the ppo onto \(C(W)\).

Alternatively, the solution to the minimization of (3.5.4) can be found from the normal equations associated with the linear model (2.2.3) used for solving generalized ridge regression. In this application the model becomes

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}W&\tilde{R}\\ 0&\sqrt{\lambda n}\tilde{S}\end{bmatrix}\begin{bmatrix}\beta\\ \xi\end{bmatrix}+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix} \tag{3.5.9}\]

where \(\tilde{R}=\tilde{S}^{\prime}\tilde{S}\). The normal equations are

\[\begin{bmatrix}W^{\prime}W&W^{\prime}\tilde{R}\\ \tilde{R}W&\tilde{R}\tilde{R}+\lambda n\tilde{R}\end{bmatrix}\begin{bmatrix} \beta\\ \xi\end{bmatrix}=\begin{bmatrix}W^{\prime}Y\\ \tilde{R}Y\end{bmatrix} \tag{3.5.10}\]

or they can be reduced to

\[\begin{bmatrix}W^{\prime}W&0\\ 0&\tilde{R}(I-M_{W})\tilde{R}+\lambda n\tilde{R}\end{bmatrix}\begin{bmatrix} \delta\\ \xi\end{bmatrix}=\begin{bmatrix}W^{\prime}Y\\ \tilde{R}(I-M_{W})Y\end{bmatrix} \tag{3.5.11}\]

with \(W\delta\equiv W\beta+M_{W}\tilde{R}\xi\). Both of these require solving a system of \(s+n\) linear equations to find the estimates.

The simplest way to _program_ these results may be to find

\[\begin{bmatrix}\hat{\beta}\\ \hat{\xi}\end{bmatrix}=\begin{bmatrix}W^{\prime}W&W^{\prime}\hat{R}\\ \hat{R}W&\hat{R}\hat{R}+\lambda n\hat{R}\end{bmatrix}^{-}\begin{bmatrix}W^{ \prime}Y\\ \hat{R}Y\end{bmatrix}\]

where it is easy to find the Moore-Penrose generalized inverse of a nonnegative definite matrix by using its eigen-decomposition as demonstrated in the proof of _PA_ Theorem B.38. See the supplemental material to Nosedal-Sanchez et al. (2012) for examples of programs in the R language. However, it may be more efficient to find the two generalized inverses of dimensions \(s\) and \(n\) associated with solving (3.5.11) rather than one generalized inverse of dimension \(s+n\) to solve (3.5.10).

For clarity, we have restricted our attention to minimizing (3.5.4), which incorporates squared error loss between the observations and the unknown function evaluations. The representation theorem holds for more general loss functions, e.g., those from logistic or Poisson regression, see Gu (2002).

Example 3.5.4.: Reconsider fitting linear splines to the data of Example 3.1.2 but without the requirement that \(f(0)=0=y_{0}\). The observed data are

\[Y\equiv\begin{bmatrix}y_{1}\\ \vdots\\ y_{n}\end{bmatrix}=\begin{bmatrix}0.1\\ 1\\ 2\\ 1.5\\ 1.75\end{bmatrix},\quad X\equiv\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}\equiv\begin{bmatrix}t_{1}\\ \vdots\\ t_{n}\end{bmatrix}=\begin{bmatrix}0.1\\ 0.25\\ 0.5\\ 0.75\\ 1\end{bmatrix}\]

We have

\[\mathcal{P}(f)\equiv\int_{0}^{1}[\dot{f}(t)]^{2}dt,\]

\(\mathcal{N}=\mathcal{N}_{0}\), the one dimensional space spanned by \(\psi(x)=1\), and \(\mathcal{H}=\mathcal{W}_{0}^{1}\), so \(R(s,t)=\min(s,t)\). It follows that \(W=J_{n}\) and for these data

\[\hat{R}=\begin{bmatrix}0.1&0.1&0.1&0.1&0.1\\ 0.1&0.25&0.25&0.25&0.25\\ 0.1&0.25&0.5&0.5&0.5\\ 0.1&0.25&0.5&0.75&0.75\\ 0.1&0.25&0.5&0.75&1\end{bmatrix}.\]

A similar structure holds whenever \(t_{1}\leq t_{2}\leq\cdots\leq t_{n}\), which we henceforth assume.

If we set \(\lambda=0\) to get linear interpolating splines, we are just solving a least squares problem and the least squares problem is minimized by any function with \(f(x_{i})=y_{i}\), \(i=1,\ldots,n\). In particular, with five distinct \(x_{i}\) values, a quartic (fourth degree) polynomial will fit the data perfectly as would appropriate sine and cosine models as discussed in Chap. 1. One problem with these solutions is that they will not be particularly smooth as judged by having small values of \(\mathcal{P}(f)\). In any case, Theorem 3.5.2 tells us that there is also a solution that can be found by fitting the model

\[Y=J\beta_{0}+\tilde{R}\xi+e.\]The first column of \(\tilde{R}\) is \(t_{1}J\), so the estimates are not unique. We could fit this model using a standard regression package deleting the first column of \(\tilde{R}\), which is equivalent to imposing a side condition of \(\xi_{1}=0\). If we do so, we get \(\hat{\beta}_{0}=-0.5\) and \(\hat{\xi}=(0,2,6,-3,1)^{\prime}\). A predicted value for a new \(t\) is

\[\hat{f}(t)=\hat{\beta}_{0}+[R(t,t_{1}),\cdots,R(t,t_{n})]\hat{\xi}\\ =-0.5+0R(t,t_{1})+2R(t,t_{2})+6R(t,t_{3})-3R(t,t_{4})+1R(t,t_{5}).\]

Or we could drop the "intercept" by imposing the side condition \(\beta_{0}=0\) and fit only \(Y=\tilde{R}\xi+e\). Now a predicted value is

\[\hat{f}(t)=\hat{\beta}_{0}+[R(t,t_{1}),\cdots,R(t,t_{n})]\hat{\xi}\\ =0-5R(t,t_{1})+2R(t,t_{2})+6R(t,t_{3})-3R(t,t_{4})+1R(t,t_{5}).\]

which is the solution we found in Example 3.1.2. These two solutions are different for \(0\leq t<t_{1}\) but agree for \(t_{1}\leq t\leq 1\). There is no unique solution. In fact, for these data we need only have \(\hat{\beta}_{0}\) and \(\hat{\xi}_{1}\) satisfying

\[-0.5=\hat{\beta}_{0}+\hat{\xi}_{1}\times 0.1=\hat{\beta}_{0}+\hat{\xi}_{1}R(t_{1 },t_{1}).\]

Moreover all solutions give a horizontal line from \(t_{n}\) to \(1\), an issue hidden in this example by the fact that \(t_{n}=1\). Actually, to fit interpolating splines, you do not need to do regression, you merely need to solve the system of linear equations \(Y=J\beta_{0}+\tilde{R}\xi\). 

We can find an interpolating function that uses the form of (3.5.11) but what if we wanted an interpolator that minimized \(\mathcal{P}(f)\)? Corollary 3.5.3 tells us the minimizer has this form, but not how to find the minimizer itself. Intuitively, the answer is quite clear. We need an interpolator that is a flat a possible. Which of the possible answers from the linear model will minimize the roughness penalty? Clearly, pick \(\hat{\xi}_{1}\) to give a flat function on \([0,t_{1}]\), so the derivative is \(0\) and its contribution to \(\mathcal{P}\) is minimized. Moreover, this shows that having a horizontal line from \(t_{n}\) to \(1\) is actually a feature and not a bug. We return to this question after the next example.

Example 3.5.5.: Reconsider Example 3.5.1 but now including the point at \((t_{0},y_{0})=(0,0)\). The observed data are

\[Y\equiv\begin{bmatrix}y_{0}\\ \vdots\\ y_{n-1}\end{bmatrix}=\begin{bmatrix}0\\ 0.1\\ 1\\ 2\\ 1.5\\ 1.75\end{bmatrix},\quad\begin{bmatrix}t_{0}\\ \vdots\\ t_{n-1}\end{bmatrix}=\begin{bmatrix}0\\ 0.1\\ 0.25\\ 0.5\\ 0.75\\ 1\end{bmatrix}\]

As before \(W=J_{n}\) (but \(n\) is \(6\) rather than \(5\)) and for these data \[\hat{R}=\begin{bmatrix}0&0&0&0&0&0\\ 0&0.1&0.1&0.1&0.1&0.1\\ 0&0.1&0.25&0.25&0.25&0.25\\ 0&0.1&0.25&0.5&0.5&0.5\\ 0&0.1&0.25&0.5&0.75&0.75\\ 0&0.1&0.25&0.5&0.75&1\end{bmatrix}.\]

For \(\lambda=0\), as with the previous example, any interpolation function minimizes the squared errors. Similar to the previous example, fitting a fifth degree polynomial provides an interpolator, but not one that is necessarily very smooth. As before, fitting the linear model \(Y=J\beta_{0}+\tilde{R}\xi^{2}+e\) with \(\xi=(\xi_{0},\ldots,\xi_{5})\) is one way to produce an interpolator. The estimate of \(\xi_{0}\) can be anything. \(\xi_{0}\) is irrelevant to fitting the data because it corresponds to a column of 0s. A predicted value is

\[\hat{f}(t)=\hat{\beta}_{0}+[R(t,t_{0}), \cdots,R(t,t_{n})]\hat{\xi}\\ =0-5R(t,t_{1})+2R(t,t_{2})+6R(t,t_{3})-3R(t,t_{4})+1R(t,t_{5})\]

because \(R(t,t_{0})=0\), making the estimate of \(\xi_{0}\) irrelevant. This is the solution we found in Example 3.1.2.

Now suppose \(y_{0}\neq 0\). We get a similar solution but \(\hat{\beta}_{0}=y_{0}\) with a different \(\hat{\xi}_{1}\) (the other \(\hat{\xi}_{k}\)s remain the same). In particular, if \(y_{0}=2\), we get \(\hat{\xi}_{1}=-25\) so that

\[y_{1}=0.1=2-25R(t_{1},t_{1})+2R(t_{1},t_{2})+6R(t_{1},t_{3})-3R(t_ {1},t_{4})+1R(t_{1},t_{5})\\ =2+(-25+2+6-3+1)(0.1).\qed\]

Typically, for bivariate data \((x_{i},y_{i})\), \(i=1,\ldots,n\), the predictor variable \(x_{i}\) is not between 0 and 1, although our procedure requires that it be. If we standardize the predictor so that

\[t_{i}=\frac{x_{i}-\min_{k}(x_{k})}{\max_{k}(x_{k})-\min_{k}(x_{k})}\]

neither the nonuniqueness at the beginning of \(\hat{f}(t)\) or the flatness of the end of \(\hat{f}(t)\) remain issues in linear interpolation. In that case, as in Example 3.5.3, \(t_{1}=0\), so the first column of \(\hat{R}\) is 0, the estimates of all parameters are unique except for \(\xi_{1}\), but \(R(t,t_{1})=0\) so \(\xi_{1}\) is irrelevant to predictions. Of course, this standardization disallows any possibility of extrapolating the results beyond the observed data.

**Proposition 3.5.6.**  For simple regression data \((t_{i},y_{i})\), \(i=1,\ldots,n\) with strictly increasing \(t_{i}\), there exists a minimum "linear spline" penalty interpolator for \(f\in\mathcal{S}\) that has the form \(\hat{f}(t)=\hat{\beta}_{0}+\sum_{i=1}^{n}\hat{\xi}_{i}\min(t_{i},t)\) with \(\sum_{i=1}^{n}\hat{\xi}_{i}=0\) and thus has \(\hat{f}(t)=y_{1}\) for \(0\leq t\leq t_{1}\) and \(\hat{f}(t)=y_{n}\) for \(t_{n}\leq t\leq 1\). This minimizer is unique.

Proof: We obtain interpolating coefficients (that minimize the sum of squared errors), say, \(\hat{\beta}_{0}\) and \(\hat{\xi}_{i}\)s from solving \(Y=J\beta_{0}+\tilde{R}\xi\). Using the structure of \(\tilde{R}\), if we set \(\hat{\xi}_{1}=0\), there is a unique solution with, say, \(\hat{\beta}_{00}\), but in general we merely have \(\hat{\beta}_{00}=\hat{\beta}_{0}+\hat{\xi}_{1}t_{1}\) with no other restrictions on the choices of \(\hat{\beta}_{0}\) and \(\hat{\xi}_{1}\). Choose \(\hat{\xi}_{1}\) to minimize the penalty by writing

\[\mathcal{P}(\hat{f})=\hat{\xi}^{\prime}\hat{R}\hat{\xi}=\left[\begin{array}{ cc}\hat{\xi}_{1}&\hat{\xi}_{*}\end{array}\right]\left[\begin{array}{cc}t_{1}&t_ {1}J_{n-1}^{\prime}\\ t_{1}J_{n-1}&\hat{R}_{2}\end{array}\right]\left[\begin{array}{cc}\hat{\xi}_ {1}\\ \hat{\xi}_{*}\end{array}\right]\]

or

\[\mathcal{P}(\hat{f})=t_{1}\hat{\xi}_{1}^{2}+2t_{1}\hat{\xi}_{1}\hat{\xi}_{*}^{ \prime}J_{n-1}+\hat{\xi}_{*}^{\prime}\hat{R}_{2}\hat{\xi}_{*}.\]

For \(t_{1}=0\), \(\hat{\xi}_{1}\) can be anything, so it can be chosen so that the proposition holds, noting that \(\hat{f}\) is unique regardless of how \(\hat{\xi}_{1}\) is chosen. For \(t_{1}>0\), to minimize the penalty as a function of \(\hat{\xi}_{1}\) set the derivative equal to \(0\) yielding \(0=\hat{\xi}_{1}+\hat{\xi}_{*}^{\prime}J_{n-1}=\sum_{i=1}^{n}\xi_{i}\). Since \(\hat{f}\) is an interpolator,

\[y_{1}=\hat{f}(t_{1})=\hat{\beta}_{0}+\sum_{i=1}^{n}\hat{\xi}_{i}\min(t_{i},t_{ 1})=\hat{\beta}_{0}+\sum_{i=1}^{n}\hat{\xi}_{i}t_{1}=\hat{\beta}_{0}.\]

and for \(0\leq t\leq t_{1}\),

\[\hat{f}(t)=\hat{\beta}_{0}+\sum_{i=1}^{n}\hat{\xi}_{i}\min(t_{i},t)=\hat{\beta }_{0}+\sum_{i=1}^{n}\hat{\xi}_{i}t=\hat{\beta}_{0}.\]

Finally, any interpolator of this form based on \(\min(t_{i},t)\) has, for \(t_{n}\leq t\leq 1\), \(y_{n}=\hat{f}(t_{n})=\hat{f}(t)\). \(\Box\)

It is pretty clear that we do not want to use this procedure to extrapolate the data!

In general, to fit linear smoothing splines with \(\lambda>0\) using a regression program, we need to incorporate

\[\tilde{S}=\left[\begin{array}{ccccc}\sqrt{t_{1}}&\sqrt{t_{1}}&\sqrt{t_{1}}& \cdots&\sqrt{t_{1}}&\sqrt{t_{1}}\\ 0&\sqrt{t_{2}-t_{1}}&\sqrt{t_{2}-t_{1}}&\cdots&\sqrt{t_{2}-t_{1}}&\sqrt{t_{2}- t_{1}}\\ 0&0&\sqrt{t_{3}-t_{2}}&\cdots&\sqrt{t_{3}-t_{2}}&\sqrt{t_{3}-t_{2}}\\ \vdots&\vdots&&\ddots&&\vdots\\ 0&0&0&\cdots&0&\sqrt{t_{n}-t_{n-1}}\end{array}\right]\]

into model (3.5.9).

**Exercise 3.4.**  Fit linear smoothing splines to the data of Example 3.5.2 using \(\lambda=0.001,0.01,0.1,0.5,1,2\) and compare the results. Graph the results.

#### General Solution Applied to Ridge Regression

In Chap. 2, we used standard linear model ideas to solve the generalized ridge regression problem. In this section we used the generalized ridge regression results of Chap. 2, along with RKHS theory, to solve the general penalized estimation problem. Now, somewhat circularly, we demonstrate how the general penalized estimation results apply to the classical ridge regression problem.

To solve the classical ridge problem we use the same notation as in Example 3.4.1, which is that of Chap. 2 and _PA_ Sect. 6.2,

\[Y=X\beta+e=[J,Z]\begin{bmatrix}\beta_{0}\\ \beta_{*}\end{bmatrix}+e\]

where the rows of \(Z\) are the predictor vectors \(x_{i}\), \(i=1,\ldots,n\). In particular, the classical ridge regression problem estimates \(\beta_{0}\) and \(\beta_{*}\) by minimizing

\[[Y-J\beta_{0}-Z\beta_{*}]^{\prime}[Y-J\beta_{0}-Z\beta_{*}]+\lambda n\beta_{*}^ {\prime}\beta_{*},\]

where \(\lambda n=k\) from Chap. 2. We will see that RKHS theory essentially rewrites the model as \(Y=[J,ZZ^{\prime}]\,[\,\beta_{0},\xi\,]+e\) before solving the generalized ridge regression problem of minimizing

\[[Y-J\beta_{0}-ZZ^{\prime}\xi]^{\prime}[Y-J\beta_{0}-ZZ^{\prime}\xi]+\lambda n \xi^{\prime}ZZ^{\prime}\xi\]

in which, quite clearly, \(\beta_{*}=Z^{\prime}\xi\).

To put the ridge regression problem in the framework of Theorem 3.5.2, reconsider Example 3.3.14. Take the affine functions

\[\mathcal{M}=\mathcal{F}_{*}=\left\{f:f(x)=\beta_{0}+\sum_{j=1}^{p-1}\beta_{j} x_{j}\right\}. \tag{3.5.12}\]

with the penalty function

\[\mathcal{P}(f)=\sum_{j=1}^{p-1}\beta_{j}^{2}.\]

Take \(\mathcal{N}\) as \(\mathcal{F}_{0}\) and \(\mathcal{H}\) as \(\mathcal{F}_{0}^{\perp}\) (which is \(\mathcal{F}_{1}\) from Example 3.3.13). The dimension of \(\mathcal{N}=\mathcal{N}_{0}\) is \(s=1\) with \(\psi_{1}(x)=1\). The r.k. on \(\mathcal{H}\) comes from Example 3.3.13 and is

\[R(\tilde{x},x)=x^{\prime}\tilde{x}.\]

All of this leads us to

\[W=J,\qquad\tilde{R}=ZZ^{\prime},\qquad\tilde{S}=Z^{\prime}\]

in the minimization (3.5.7) and the model (3.5.9).

The estimates from (3.5.8) reduce to

\[\hat{\xi}=\left[ZZ^{\prime}\left(I-\frac{1}{n}JJ^{\prime}\right)ZZ^{\prime}+ \lambda nZZ^{\prime}\right]^{-}ZZ^{\prime}\left(I-\frac{1}{n}JJ^{\prime}\right)Y,\]

\[\hat{\beta}_{0}=(J^{\prime}J)^{-1}J^{\prime}(Y-ZZ^{\prime}\hat{\xi})\]or

\[\hat{\xi}=\left\{Z\left[Z^{\prime}\left(I-\frac{1}{n}JJ^{\prime}\right)Z+\lambda nI \right]Z^{\prime}\right\}^{-}ZZ^{\prime}\left(I-\frac{1}{n}JJ^{\prime}\right)Y, \quad\hat{\beta}_{0}=\frac{1}{n}J^{\prime}(Y-ZZ^{\prime}\hat{\xi}).\]

It is easy to see by checking the definition of a generalized inverse that we can take

\[\left\{Z\left[Z^{\prime}\left(I-\frac{1}{n}JJ^{\prime}\right)Z+ \lambda nI\right]Z^{\prime}\right\}^{-}\\ =Z(Z^{\prime}Z)^{-1}\left[Z^{\prime}\left(I-\frac{1}{n}JJ^{\prime} \right)Z+\lambda nI\right]^{-1}(Z^{\prime}Z)^{-1}Z^{\prime}\]

so

\[\hat{\xi} =Z(Z^{\prime}Z)^{-1}\left[Z^{\prime}\left(I-\frac{1}{n}JJ^{\prime} \right)Z+\lambda nI\right]^{-1}(Z^{\prime}Z)^{-1}Z^{\prime}ZZ^{\prime}\left(I- \frac{1}{n}JJ^{\prime}\right)Y\] \[=Z(Z^{\prime}Z)^{-1}\left[Z^{\prime}\left(I-\frac{1}{n}JJ^{\prime }\right)Z+\lambda nI\right]^{-1}Z^{\prime}\left(I-\frac{1}{n}JJ^{\prime}\right)Y\] \[=Z(Z^{\prime}Z)^{-1}\hat{\gamma}\]

where \(\hat{\gamma}\) is the classical ridge estimator from (2.2.4) in which "classical" means that \(Q=I\) in that formula. With \(\beta_{*}=Z^{\prime}\xi\),

\[\hat{\beta}_{*R}=Z^{\prime}\hat{\xi}=Z^{\prime}Z(Z^{\prime}Z)^{-1}\hat{\gamma} =\hat{\gamma}.\]

This version of \(\hat{\xi}\) leads to

\[\hat{\beta}_{0R}=\frac{1}{n}J^{\prime}[Y-ZZ^{\prime}Z(Z^{\prime}Z)^{-1}\hat{ \gamma}]=\frac{1}{n}J^{\prime}(Y-Z\hat{\beta}_{*R}).\]

which also agrees with the classical ridge estimates from (2.2.4). Finally we have

\[\hat{Y}=\hat{\beta}_{0R}J+ZZ^{\prime}\hat{\xi}=\hat{\beta}_{0R}J+ZZ^{\prime} \left[Z(Z^{\prime}Z)^{-1}\hat{\gamma}\right]=\hat{\beta}_{0R}J+Z\hat{\beta}_{* R}.\]

In particular, for prediction at a new vector \(x_{0}\) we have

\[\hat{f}(x_{0})=\hat{\beta}_{0R}+[R(x_{0},x_{1}),\cdots,R(x_{0},x_{1})]\hat{\xi }=\hat{\beta}_{0R}+x_{0}^{\prime}Z^{\prime}\hat{\xi}=\hat{\beta}_{0R}+x_{0}^{ \prime}\hat{\beta}_{*R}.\]

#### General Solution Applied to Cubic Smoothing Splines

Consider again the bivariate regression problem \(y_{i}=f(t_{i})+\varepsilon_{i}\), \(i=1,2,\ldots,n\) where \(t_{i}\in[0,1]\) and \(\mathrm{E}(\varepsilon_{i})=0\). Now consider the cubic smoothing spline estimates obtained by finding a function that minimizes\[\sum_{i=1}^{n}[y_{i}-f(t_{i})]^{2}+\lambda n\int\ddot{f}(u)^{2}du.\]

For the general solution we take \(\mathcal{M}\) as the subset of \(\mathcal{L}^{2}[0,1]\) with square integrable second derivatives. We take \(\mathcal{P}(f)\) as in (3.2.2) which is also the inner product on \(\mathcal{H}=\mathcal{W}_{0}^{2}\) from Example 3.3.11. The r.k. was given in Example 3.3.11 as Eq. (3.3) and is also given later. With this \(\mathcal{P}\), the space \(\mathcal{N}\) is all functions with \(\ddot{f}=0\). We take \(\mathcal{N}_{0}\) to be linear functions \(f_{0}(t)=\beta_{0}+\beta_{1}t\), so basis functions are \(\psi_{1}(t)=1\) and \(\psi_{2}(t)=t\).

This structure leads to

\[W=\left[\begin{array}{cc}1&t_{1}\\ \vdots&\vdots\\ 1&t_{n}\end{array}\right]\]

and an \(\tilde{R}\) that is ugly, but easily computed, in the minimization (3.5.7) and the model (3.5.9). Using

\[R(s,t)=\int_{0}^{1}(t-u)_{+}(s-u)_{+}du=\frac{\max(s,t)\min^{2}(s,t)}{2}-\frac {\min^{3}(s,t)}{6},\]

it is well known that the basis functions \(R(t_{h},t)\), that determine the columns \(\tilde{R}_{h}\) of \(\tilde{R}\) when evaluated at the data points \(t_{i}\), form a natural cubic spline with knots at the distinct values of \(t_{h}\). See Wahba (1990) for a justification, which just involves some algebra. The \(\max(t_{h},t)\) and \(\min(t_{h},t)\) in \(R(t_{h},t)\) combine in a way to produce knots at the \(t_{h}\), while the degree of the polynomial spline is clearly three, since it is the highest power present in \(R(s,t)\). This is the reason that the minimization problem in this section has been given the name _cubic smoothing spline_.

Nosedal-Sanchez et al. (2012) demonstrate with R code the fitting of cubic smoothing splines on some real data. The demonstration also includes searching for the best value of the tuning parameter \(\lambda\) which is briefly discussed in the next section.

### Choosing the Degree of Smoothness

With the penalized regression procedures described above, the choice of the smoothing parameter \(\lambda\) is an important issue. There are many methods available for this task, e.g., visual inspection of the fit; \(m\)-fold cross-validation, Kohavi (1995); AIC/unbiased risk estimation; generalized maximum likelihood, Wahba (1990); generalized cross-validation (GCV), Craven and Wahba (1979); among others. Nosedal-Sanchez et al. (2012) used the GCV approach which works as follows. Suppose that for fixed \(\lambda\) the estimates determine fitted values \(\hat{Y}=A(\lambda)Y\). The GCV choice of \(\lambda\) is the minimizer of \[V(\lambda)=\frac{\frac{1}{n}\|I-A(\lambda)Y\|^{2}}{\big{\{}\frac{1}{n}\text{tr}[I-A( \lambda)]\big{\}}^{2}}.\]

For more details about GCV and other methods of finding \(\lambda\) see Golub, Heath, and Wahba (1979), Allen (1974), Wecker and Ansley (1983), and Wahba (1990).

## References

* Allen (1974) Allen, D. (1974). The relationship between variable selection and data augmentation and a method for prediction. _Technometrics,__16_, 125-127.
* Berman (1994) Berman, M. (1994). Automated smoothing of image and other regularly spaced data. _IEEE Transactions on Pattern Analysis and Machine Intelligence,__16_, 460-468.
* Berlinet & Thomas-Agnan (2004) Berlinet, A., & Thomas-Agnan, C. (2004). _Reproducing kernel Hilbert spaces in probability and statistics_. Norwell, MA: Kluwer Academic Publishers.
* Bivand et al. (2013) Bivand, R., Pebesma, E., & Gomez-Rubio, V. (2013). _Applied spatial data analysis with R_ (2nd ed.). New York, NY: Springer.
* Buhlmann & van de Geer (2011) Buhlmann, P., & van de Geer, S. (2011). _Statistics for high-dimensional data: Methods, theory and applications_. Heidelberg, NY: Springer.
* Buhlmann et al. (2014) Buhlmann, P., Kalisch, M., & Meier, L. (2014). High-dimensional statistics with a view toward applications in biology. _Annual Review of Statistics and Its Applications,__1_, 255-278.
* Craven & Wahba (1979) Craven, P., & Wahba, G. (1979). Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation. _Numerical Mathematics,__31_, 377-403.
* de Barra (1981) de Barra, G. (1981). _Measure theory and integration_. West Sussex: Horwood Publishing.
* Eubank (1999) Eubank, R. (1999). _Nonparametric regression and spline smoothing_. New York, NY: Marcel Dekker.
* Golub et al. (1979) Golub, G., Heath, M., & Wahba, G. (1979). Generalized cross-validation as a method for choosing a good ridge parameter. _Technometrics,__21_, 215-223.
* Gu (2002) Gu, C. (2002). _Smoothing spline ANOVA models_. New York, NY: Springer.
* Halmos (1958) Halmos, P. R. (1958). _Finite-dimensional vector spaces_ (2nd ed.). Princeton, NJ: Van Nostrand.
* Harville (1997) Harville, D. A. (1997). _Matrix algebra from a statistician's perspective_. New York, NY: Springer.
* Hastie et al. (2016) Hastie, T., Tibshirani, R., & Friedman, J. (2016). _The elements of statistical learning: Data mining, inference, and prediction_ (2nd ed.). New York, NY: Springer.
* Heckman (2012) Heckman, N. (2012). The theory and application of penalized methods or Reproducing Kernel Hilbert Spaces made easy. _Statistics Surveys,__6_, 113-141.
* Hoerl & Kennard (1970) Hoerl, A. E., & Kennard, R. (1970). Ridge regression: Biased estimation for non-orthogonal problems. _Technometrics,__12_, 55-67.
* Hoerl et al. (2016)* Kohavi (1995) Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. _Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, 2_(12), 1137-1143.
* Lin & Zhang (2006) Lin, Y., & Zhang, H. (2006). Component selection and smoothing in smoothing spline analysis of variance models. _Annals of Statistics, 34_, 2272-2297.
* Mate (1990) Mate, L. (1990). _Hilbert space methods in science and engineering_. London: Taylor & Francis.
* Moguerza & Munoz (2006) Moguerza, J. M., & Munoz, A. (2006). Support vector machines with applications. _Statistical Science, 21_, 322-336.
* Naylor & Sell (1982) Naylor, A., & Sell, G. (1982). _Linear operator theory in engineering and science_. New York, NY: Springer.
* Nosedal-Sanchez et al. (2012) Nosedal-Sanchez, A., Storlie, C.B., Lee, T. C. M., & Christensen, R. (2012). Reproducing kernel Hilbert spaces for penalized regression: A tutorial. _The American Statistician, 66_, 50-60
* O'Sullivan et al. (1986) O'Sullivan, F., Yandell, B., & Raynor, W. (1986). Automatic smoothing of regression functions in generalized linear models. _Journal of the American Statistical Association, 81_, 96-103.
* Pearce & Wand (2006) Pearce, N. D., & Wand, M. P. (2006). Penalized splines and reproducing kernel methods. _The American Statistician, 60_, 233-240.
* Ramsay & Silverman (2005) Ramsay, J., & Silverman, B. (2005). _Functional data analysis_. New York, NY: Springer.
* Rustagi (1994) Rustagi, J. (1994). _Optimization techniques in statistics_. London: Academic Press.
* Storlie et al. (2010) Storlie, C., Bondell, H., & Reich, B. (2010). A locally adaptive penalty for estimation of functions with varying roughness. _Journal of Computational and Graphical Statistics, 19_, 569-589.
* Storlie et al. (2010) Storlie, C., Bondell, H., Reich, B., & Zhang, H. (2010). Surface estimation, variable selection, and the nonparametric oracle property. _Statistica Sinica, 21_, 679-705.
* Storlie et al. (2009) Storlie, C., Swiler, L., Helton, J., & Sallaberry, C. (2009). Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models. _Reliability Engineering and System Safety, 94_, 1735-1763.
* Tibshirani (1996) Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society: Series B, 58_, 267-288.
* Wahba (1990) Wahba, G. (1990). _Spline models for observational data_ (Vol. 59). CBMS-NSF Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM.
* Wainwright (2014) Wainwright, M. J. (2014). Structured regularizers for high-dimensional problems: Statistical and computational issues. _Annual Review of Statistics and Its Applications, 1_, 233-253.
* Wecker & Ansley (1983) Wecker, W., & Ansley, C. (1983). The signal extraction approach to nonlinear regression and spline smoothing. _Journal of the American Statistical Association, 78_, 81-89.
* Wood (2006) Wood, S. (2006). _Generalized additive models: An introduction with R_. Boca Raton, FL: CRC Press.
* Young (1988) Young, N. (1988). _An introduction to Hilbert space_. Cambridge: Cambridge University Press.

* Zhu (2008) Zhu, M. (2008). Kernels and ensembles. _The American Statistician, 62_, 97-109.
* Zou (2006) Zou, H. (2006). The adaptive lasso and its oracle properties. _Journal of the American Statistical Association, 101_, 1418-1429.

## Chapter 4 Covariance Parameter Estimation

**Abstract** This chapter reviews fundamental ideas from linear model theory for dealing with dependent or heteroscedastic data when the nature of the dependence or heteroscedasticity is known. It then introduces general ideas for estimating dependence or heteroscedasticity when their exact natures are unknown. Most of the book, after this chapter, consists of applications of these ideas to specific models.

About three fourths of this book deals with dependent data in some sort of linear model. In particular, Chaps. 4 through 12 and 14 all examine dependent data to which some form of linear model theory is relevant. Many issues in these applications rely on results that are valid quite generally. Rather than treating these issues piecemeal, as they come up, we discuss them all at once in this chapter and refer back to this discussion as necessary. By far the most important of these issues are procedures for estimating parameters in the covariance matrix of a linear model. We also discuss the effect that estimation of covariance parameters has on estimates of mean parameters and on predictions. Parts of this chapter rely heavily on Appendix A. Some mathematical issues related to residual maximum likelihood estimation have been relegated to Appendix C.

### 4.1 Introduction and Review

Linear models have the form

\[Y=X\beta+e,\qquad\mathrm{E}(e)=0.\]

In this chapter we look at linear models that have covariance matrices that are more complicated than those considered in _PA_. Primarily, _PA_ considered \(\mathrm{Cov}(e)=\sigma^{2}I\) or, for \(V\) known and positive definite, \(\mathrm{Cov}(e)=\sigma^{2}V\). Chapter 11 of _PA_ considered \(\mathrm{Cov}(e)=\sigma_{s}^{2}I+\sigma_{w}^{2}X_{1}X_{1}^{\prime}\) for a known matrix of indicator variables \(X_{1}\) but unknown variance parameters \(\sigma_{s}^{2}\) and \(\sigma_{w}^{2}\).

We now consider normal theory general linear models wherein the covariance matrix depends on a vector of parameters \(\theta\), say,

\[Y=X\beta+e,\qquad e\sim N[0,V(\theta)]\]

or, equivalently,

\[Y\sim N[X\beta,V(\theta)].\]

To a lesser extent we examine models with the same mean and covariance but without the normality assumption. This chapter focuses on methods for estimating the vector of covariance parameters \(\theta\) and on some consequences of that estimation. Here \(\theta\) is taken to be functionally distinct from the unknown mean parameter \(\beta\). Some authors, notably Carroll and Ruppert (1988), assume a diagonal covariance matrix but also allow the matrix to depend on \(\beta\). We will not consider such models.

In subsequent chapters we look at special cases of this very general model. Chapter 5 looks at mixed linear models. Chapter 6 looks at the application of mixed models to time series data. Chapter 8 looks at spatial data models. Chapters 9 and 10 look at multivariate linear models. Chapter 11 examines generalized multivariate linear models and linear models for longitudinal data. Our main approaches to estimating covariance parameters are two versions of maximum likelihood estimation: standard maximum likelihood and another procedure, called residual maximum likelihood (or restricted maximum likelihood), that eliminates the effect of the unknown mean vector prior to maximizing the likelihood. Residual/restricted maximum likelihood is almost universally referred to as REML. To maximize the (restricted) likelihood, we take partial derivatives of the (restricted) log-likelihood and set them equal to zero, i.e., we solve the (restricted) likelihood equations. We also show that both the REML and likelihood methods can be recast as method of moments (estimating equation) estimators which suggests that the methods are at least reasonable even when the data are not normally distributed.

In this chapter we begin by reviewing results on estimation and prediction that apply when \(\theta\), and therefore \(V(\theta)\), is known. The chapter then examines maximum likelihood estimation and REML estimation of \(\theta\). Next we present a useful linear model for the covariance matrix that leads to minimum norm quadratic unbiased estimation (MINQUE) of \(\theta\). The procedure misnamed as minimum variance quadratic unbiased estimation (MIVQUE) is addressed. Finally, we examine the effect that estimating covariance parameters has on the procedures that were developed assuming that the covariance parameters were known.

In \(P\!A\) we established a number of optimal estimation and prediction properties that hold when \(\theta\) is known or, more generally, when \(V(\theta)\) is known up to a scalar multiple. These are now reviewed. Typically we assume \(V(\theta)\) is positive definite for all \(\theta\) and, just to be clear, we write

\[V^{-1}(\theta)\equiv[V(\theta)]^{-1}.\]

#### Estimation of \(\beta\)

Following _PA_ Sect. 2.7, by definition the generalized least squares estimates of \(\beta\) minimize \((Y-X\beta)^{\prime}[V^{-1}(\theta)](Y-X\beta)\). These estimates are vectors \(\hat{\beta}(\theta)\) that satisfy

\[X\hat{\beta}(\theta)=A(\theta)Y, \tag{4.1.1}\]

where

\[A(\theta)\equiv X[X^{\prime}V^{-1}(\theta)X]^{-}X^{\prime}V^{-1}(\theta). \tag{4.1.2}\]

\(A(\theta)\) is an oblique projection operator onto the column space of \(X\), \(C(X)\). In particular, \(A(\theta)X=X\) and \(C[A(\theta)]=C(X)\). The generalized least squares estimates are also best linear unbiased estimates (BLUEs) without the assumption of normality, and are maximum likelihood estimates (MLEs) and uniformly minimum variance unbiased estimates (UMVUs) under normality. Alternatively, the generalized least squares estimates can be obtained from the normal equations

\[X^{\prime}V^{-1}(\theta)X\beta=X^{\prime}V^{-1}(\theta)Y.\]

For an estimable function \(\lambda^{\prime}\beta\) with \(\lambda^{\prime}=\rho^{\prime}X\) for some \(\rho\), the generalized least squares estimate is \(\lambda^{\prime}\hat{\beta}(\theta)=\rho^{\prime}[A(\theta)]Y\). The estimate is unbiased and its variance is

\[\text{Var}[\lambda^{\prime}\hat{\beta}(\theta)]=\rho^{\prime}A(\theta)V( \theta)A^{\prime}(\theta)\rho=\lambda^{\prime}[X^{\prime}V^{-1}(\theta)X]^{-} \lambda. \tag{4.1.3}\]

_Of course if \(\theta\) is unknown, \(\hat{\beta}(\theta)\) is not an estimate except in special cases, like those discussed in_ PA_, for which \(\hat{\beta}(\theta)\) does not depend on \(\theta\)_. If we have an estimate of \(\theta\), say \(\tilde{\theta}\), we can define an (empirical or plug-in) estimate \(\tilde{\beta}\equiv\hat{\beta}(\tilde{\theta})\). Under reasonable conditions discussed in Sect. 4.7, \(\lambda^{\prime}\tilde{\beta}\) is an unbiased estimate.

Knowing \(\theta\) should be better than estimating it, so one would expect to have

\[\text{Var}[\lambda^{\prime}\hat{\beta}(\theta)]\leq\text{Var}[\lambda^{\prime} \hat{\beta}(\tilde{\theta})]\]

and Sect. 4.7 provides conditions under which this is true. Tarpey, Ogden, Petkova, and Christensen (2014, 2015) discuss an example where knowing the parameter is not always better than estimating it. Section 4.7 also provides conditions under which

\[\text{E}\left\{\lambda^{\prime}[X^{\prime}V^{-1}(\tilde{\theta})X]^{-} \lambda\right\}\leq\lambda^{\prime}[X^{\prime}V^{-1}(\theta)X]^{-}\lambda\]

so that

\[\text{E}\left\{\lambda^{\prime}[X^{\prime}V^{-1}(\tilde{\theta})X]^{-} \lambda\right\}\leq\text{Var}[\lambda^{\prime}\hat{\beta}(\theta)]\leq\text{ Var}[\lambda^{\prime}\hat{\beta}(\tilde{\theta})].\]

Thus we see that using \(\lambda^{\prime}[X^{\prime}V^{-1}(\tilde{\theta})X]^{-}\lambda\) as an estimate of \(\text{Var}[\lambda^{\prime}\hat{\beta}(\tilde{\theta})]\) can seriously underestimate the variability of the plug-in estimate. (Bayesian analyses do not suffer from these problems!)

**Exercise 4.1**.: Consider the linear model

\[\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}=\begin{bmatrix}J_{N_{1}}\\ J_{N_{2}}\end{bmatrix}\mu+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}\left(\begin{bmatrix} e_{1}\\ e_{2}\end{bmatrix}\right)=\begin{bmatrix}\sigma_{1}^{2}I_{N_{1}}&0\\ 0&\sigma_{2}^{2}I_{N_{2}}\end{bmatrix}.\]

Find the BLUE \(\hat{\mu}\) and \(\mathrm{Var}(\hat{\mu})\) assuming that the variance parameters are known. How does the result change if \(\sigma_{1}^{2}\) is unknown but \(\sigma_{2}^{2}/\sigma_{1}^{2}\) is known? (In general \(J_{a}^{b}\) denotes a matrix of 1s with \(a\) rows and \(b\) columns; \(J_{a}\equiv J_{a}^{1}\); \(J\equiv J_{n}\).)

#### Testing

Exact results for testing mean parameters are well known when \(V\) is either completely known or known up to a scalar multiple, cf. _PA_ Sect. 3.8. A consistent estimate of \(\theta\) is one that comes very close to \(\theta\) whenever the sample sizes are large (whatever "large" means). If an estimate \(\tilde{\theta}\) is sufficiently close to \(\theta\), the results for known \(\theta\) should be nearly correct.

To test an estimable hypothesis \(H_{0}:\Lambda^{\prime}\beta=d\), assuming that \(\tilde{\theta}\) is a consistent estimate of \(\theta\), asymptotic theory suggests using a result similar to _PA_ Theorem 3.8.3 (with \(\sigma^{2}=1\)),

\[(\Lambda^{\prime}\tilde{\beta}-d)^{\prime}\left[\Lambda^{\prime}(X^{\prime} \tilde{V}^{-1}X)^{-}\Lambda\right]^{-}(\Lambda^{\prime}\tilde{\beta}-d)\sim \chi^{2}[r(\Lambda)]\,, \tag{4.1.4}\]

wherein \(\tilde{V}\equiv V(\tilde{\theta})\). Similarly, for testing a reduced model

\[Y=X_{0}\gamma+e,\qquad C(X_{0})\subset C(X),\]

with \(\tilde{\Lambda}\equiv A(\tilde{\theta})\) and \(\tilde{A}_{0}\) defined in the obvious way, one could base a test on the asymptotic result

\[Y^{\prime}(\tilde{A}-\tilde{A}_{0})^{\prime}\tilde{V}^{-1}(\tilde{A}-\tilde{A }_{0})Y\sim\chi^{2}[r(X)-r(X_{0})]\,. \tag{4.1.5}\]

Note that \(\tilde{V}\), \(\tilde{\beta}\), \(\tilde{A}\), and \(\tilde{A}_{0}\) are all quite complicated functions of \(Y\).

Many covariance models can be written to involve a scalar parameter times a matrix. Let \(\theta^{\prime}=[\theta_{1},\theta_{*}^{\prime}]\) and suppose

\[V(\theta)\equiv\theta_{1}V_{*}(\theta_{*}).\]

A test that is asymptotically equivalent to (4.1.4) is based on

\[\frac{(\Lambda^{\prime}\tilde{\beta}-d)^{\prime}\left[\Lambda^{\prime}(X^{ \prime}\tilde{V}_{*}^{-1}X)^{-}\Lambda\right]^{-}(\Lambda^{\prime}\tilde{ \beta}-d)/r(\Lambda)}{MSE(\tilde{\theta}_{*})}\sim F[r(\Lambda),n-r(X)] \tag{4.1.6}\]

where \(\tilde{V}_{*}\equiv V_{*}(\tilde{\theta}_{*})\),

\[MSE(\theta_{*})\equiv\frac{Y^{\prime}[I-A(\theta_{*})]^{\prime}\{V_{*}(\theta _{*})\}^{-1}[I-A(\theta_{*})]Y}{n-r(X)},\]and

\[A(\theta_{*})\equiv X[X^{\prime}V_{*}^{-1}(\theta_{*})X]^{-}X^{\prime}V_{*}^{-1}( \theta_{*}).\]

There are several points worth noting:

* \(A(\theta)=A(\theta_{*})\), see Exercise 4.5, so that \[MSE(\theta_{*})=\frac{[Y-X\hat{\beta}(\theta)]^{\prime}[V_{*}(\theta_{*})]^{-1} [Y-X\hat{\beta}(\theta)]}{n-r(X)}\] and \[MSE(\tilde{\theta}_{*})=\frac{(Y-X\tilde{\beta})^{\prime}\tilde{V}_{*}^{-1}(Y-X \tilde{\beta})}{n-r(X)}.\]
* The distributions in (4.1.4) and (4.1.6) are exact for multivariate normal data with \(\tilde{\theta}=\theta\) and \(\tilde{\theta}_{*}=\theta_{*}\), respectively.
* \(MSE(\tilde{\theta}_{*})\) provides an estimate for \(\theta_{1}\) that may or may not be different from \(\tilde{\theta}_{1}\), see Exercise 4.5.
* Since \(V^{-1}(\theta)=(1/\theta_{1})V_{*}^{-1}(\theta_{*})\), if \(MSE(\tilde{\theta}_{*})=\tilde{\theta}_{1}\), the test statistic in (4.1.4), divided by the known constant \(r(\Lambda)\), is the test statistic in (4.1.6).

Both (4.1.4) and (4.1.6) probably underrepresent the true variability of their test statistics, but (4.1.6), when applicable, probably does it a little less than (4.1.4). Similar conclusions apply to the test in (4.1.5) and the following test.

To test a reduced model, the \(F\) test corresponding to (4.1.5) uses the asymptotic approximation

\[\frac{Y^{\prime}(\tilde{A}-\tilde{A}_{0})^{\prime}\tilde{V}_{*}^{-1}(\tilde{A} -\tilde{A}_{0})Y/[r(X)-r(X_{0})]}{MSE(\tilde{\theta}_{*})}\sim F[r(X)-r(X_{0}),n-r(X)]\,.\]

Generalized likelihood ratio tests are discussed in Sect. 4.2.1.

##### 4.1.2.1 Improved Tests: Satterthwaite and Kenward-Roger

When testing for equal means in random samples from two normal distributions with possibly different variances, the usual \(t\) statistic does not have a \(t\) distribution, e.g. Christensen (2015, Section 4.3). It is common practice to use an approximate distribution proposed by Satterthwaite (1946). The same idea can be applied quite generally. In fact, the Satterthwaite approximation has been improved by Kenward and Roger (1997, 2009).

#### 4.1.3 Prediction

Point prediction with independent observations is relatively easy, one just predicts with the estimated mean of the observation being predicted. The situation is more complicated with dependent observations. We now review the key ideas of Best Linear Unbiased Prediction as presented in _PA-V_ Sect. 6.6 (Christensen 2011, Sec. 12.2).

Consider a set of random variables \(y_{i}\), \(i=0,1,\ldots,n\). We want to use \(y_{1},\ldots,y_{n}\) to predict \(y_{0}\). Let \(Y=(y_{1},\ldots,y_{n})^{\prime}\). In _PA_ Sect. 6.3 it is shown that the best predictor (BP) of \(y_{0}\) is \(\mathrm{E}(y_{0}|Y)\). Typically, the joint distribution of \(y_{0}\) and \(Y\) is not available, so \(\mathrm{E}(y_{0}|Y)\) cannot be found. If the means and covariances of the \(y_{i}\)s are available, then we can find the best linear predictor (BLP) of \(y_{0}\). If the \(y_{i}\)s have a multivariate normal joint distribution, the best predictor and the best linear predictor are identical.

Let \(\mathrm{Cov}(Y)=V(\theta)\), \(\mathrm{Cov}(Y,y_{0})=V_{y0}(\theta)\), \(\mathrm{E}(y_{i})=\mu_{i}\), \(i=0,1,\ldots,n\), and \(\mu=(\mu_{1},\ldots,\mu_{n})^{\prime}\). From _PA_ Sect. 6.3.4, the BLP of \(y_{0}\) is

\[\hat{E}(y_{0}|Y)\equiv\mu_{0}+[\delta_{*}(\theta)]^{\prime}(Y-\mu), \tag{4.1.7}\]

where \(\delta_{*}(\theta)\) satisfies \([V(\theta)]\delta_{*}(\theta)=V_{y0}(\theta)\). In our discussions of Principal Components (Chap. 14) and Time Domain (Chap. 7) analysis we will need additional properties of BLPs that are developed in Appendix B. Of course we cannot actually predict anything unless all of these parameters are known (or we can estimate them).

Following _PA-V_ Sect. 6.6 (Christensen 2011, Section 12.2), we now weaken the assumptions that \(\mu\) and \(\mu_{0}\) are known. Since the prediction is based on \(Y\) and there are as many unknown parameters in \(\mu\) as there are observations in \(Y\), we need to impose some structure on the mean vector \(\mu\) before we can generalize the theory. This is done by specifying a linear model for \(Y\). Since \(y_{0}\) is being predicted and has not been observed, it is necessary either to know \(\mu_{0}\) or to know that \(\mu_{0}\) is related to \(\mu\) in a specified manner. In the theory below, it is assumed that \(\mu_{0}\) is related to the linear model for \(Y\).

Suppose that a vector of known concomitant variables \(x_{i}^{\prime}=(x_{i1},\ldots,x_{ip})\) is associated with each random observation \(y_{i}\), \(i=0,\ldots,n\). We impose structure on the \(\mu_{i}\)s by assuming that \(\mu_{i}=x_{i}^{\prime}\beta\) for some vector of unknown parameters \(\beta\) and all \(i=0,1,\ldots,n\).

We can now reset our notation in terms of linear model theory. Let

\[X=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}.\]

The observed vector \(Y\) satisfies the linear model

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=V(\theta).\]

With this additional structure, the BLP of \(y_{0}\) becomes

\[\hat{E}(y_{0}|Y)=x_{0}^{\prime}\beta+[\delta_{*}(\theta)]^{\prime}(Y-X\beta), \tag{4.1.8}\]

where again \([V(\theta)]\delta_{*}(\theta)=V_{y0}(\theta)\).

The standard assumption that \(\mu\) and \(\mu_{0}\) are known now amounts to the assumption that \(\beta\) is known. It is this assumption that we renounce. By weakening the 

[MISSING_PAGE_FAIL:149]

Similar to the discussion on estimation, when estimating \(\theta\) with \(\tilde{\theta}\), Sect. 4.7 establishes that under certain conditions

\[\mathrm{E}\left\{y_{0}-[b(\theta)]^{\prime}Y\right\}^{2}\leq\mathrm{E}\left\{y_{ 0}-[b(\tilde{\theta})]^{\prime}Y\right\}^{2},\]

so, not surprisingly, knowing \(\theta\) gives better results than using the plug-in predictor. Section 4.7 also shows that under some conditions

\[\mathrm{E}\left\{\sigma_{0}^{2}(\tilde{\theta})-2[b(\tilde{\theta })]^{\prime}V_{y0}(\tilde{\theta})+[b(\tilde{\theta})]^{\prime}V(\tilde{\theta })b(\tilde{\theta})\right\}\\ \leq\sigma_{0}^{2}(\theta)-2[b(\theta)]^{\prime}V_{y0}(\theta)+[b (\theta)]^{\prime}V(\theta)b(\theta),\]

so, as with estimation, the plug-in variance estimate \(\sigma_{0}^{2}(\tilde{\theta})-2[b(\tilde{\theta})]^{\prime}V_{y0}(\tilde{ \theta})+[b(\tilde{\theta})]^{\prime}V(\tilde{\theta})b(\tilde{\theta})\) can seriously underestimate the variability of the plug-in predictor.

#### Quadratic Estimation of \(\theta\)

Consider the linear model

\[\mathrm{E}(Y)=X\beta;\quad\mathrm{Cov}(Y)=V(\theta).\]

We can base estimation of \(\theta\) on the idea of setting quadratic forms equal to their expectations.

To estimate the \(p\) parameters in \(\beta\) and the \(s\) parameters in \(\theta\) we can solve a system of \(p+s\) equations in \(p+s\) unknowns. Based on our previous theory, we will always start with the \(p\) normal equations

\[X^{\prime}V^{-1}(\theta)X\beta=X^{\prime}V^{-1}(\theta)Y\]

but we want to think about them in a little different way, as setting a linear function of the data equal to its expectation,

\[X^{\prime}V^{-1}(\theta)Y=\mathrm{E}\left[X^{\prime}V^{-1}(\theta)Y\right]=X^{ \prime}V^{-1}(\theta)X\beta.\]

To get \(s\) more equations pick any symmetric matrices \(W_{1}(\theta),\ldots,W_{s}(\theta)\) to define quadratic forms. One way to proceed is to set those equal to their expectations

\[Y^{\prime}W_{j}(\theta)Y=\mathrm{E}\left[Y^{\prime}W_{j}(\theta)Y\right]= \mathrm{tr}\left[W_{j}(\theta)V(\theta)\right]+\beta^{\prime}X^{\prime}W_{j}( \theta)X\beta,\quad j=1,\ldots,s.\]

A slightly different way to proceed is to set

\[(Y-X\beta)^{\prime}W_{j}(\theta)(Y-X\beta)=\mathrm{E}\left[(Y-X \beta)^{\prime}W_{j}(\theta)(Y-X\beta)\right]=\mathrm{tr}\left[W_{j}(\theta)V (\theta)\right],\\ j=1,\ldots,s.\]

In either case, the equations all involve setting functions of observations equal to their expectations, so the equations are statistically reasonable and solving them should give reasonable estimates. Whether the solutions actually give good estimamates depends on how one chooses the \(W_{j}(\theta)\)s. This is similar to the fact that least squares estimates are always reasonable for any linear model but, depending on the covariance structure, they may not always be good. We will see in the next section that maximum likelihood estimation fits the second paradigm with a particular choice of \(W_{j}(\theta)\)s, namely

\[W_{j}(\theta)=V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V(\theta)]V^{-1}(\theta).\]

REML estimation is discussed in Sect. 4.3 and fits into a slightly different paradigm that involves only \(s\) equations. Rather than picking just any old \(W_{j}(\theta)\)s, we now restrict ourselves to picking matrices \(\tilde{W}_{j}(\theta)\) with the property that \(\tilde{W}_{j}(\theta)X=0\). The defining equations for estimating \(\theta\) become

\[Y^{\prime}\tilde{W}_{j}(\theta)Y=\operatorname{E}\left[Y^{\prime}\tilde{W}_{j}( \theta)Y\right]=\operatorname{tr}\left[\tilde{W}_{j}(\theta)V(\theta)\right], \quad j=1,\ldots,s.\]

Again, REML determines a particular choice of \(\tilde{W}_{j}(\theta)\)s that should be good for normal data but that should be reasonable even for nonnormal data. In particular, REML uses

\[\tilde{W}_{j}(\theta)=[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{d}_{\theta_ {j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)].\]

MINQUE from Sect. 4.5 and Henderson's Method 3 from Sect. 5.7 fit into this paradigm. They involve choosing matrices \(\tilde{W}_{j}\) that do not depend on \(\theta\) but have \(\tilde{W}_{j}X=0\). These methods apply to models with linear covariance structures like

\[V(\theta)\equiv\sum_{k=1}^{s}\theta_{k}V_{k}\]

for known nonnegative definite \(V_{k}\)s. The system of equations becomes

\[Y^{\prime}\tilde{W}_{j}Y=\operatorname{E}\left[Y^{\prime}\tilde{W}_{j}Y\right] =\operatorname{tr}\left[\tilde{W}_{j}V(\theta)\right]=\sum_{k=1}^{s}\theta_{k }\operatorname{tr}(\tilde{W}_{j}V_{k}),\quad j=1,\ldots,s.\]

The \(Y^{\prime}\tilde{W}_{j}Y\)s are observable numbers, so this method involves merely solving a system of linear equations,

\[\begin{bmatrix}Y^{\prime}\tilde{W}_{1}Y\\ \vdots\\ Y^{\prime}\tilde{W}_{s}Y\end{bmatrix}=\begin{bmatrix}\operatorname{tr}( \tilde{W}_{1}V_{1})&\cdots&\operatorname{tr}(\tilde{W}_{1}V_{s})\\ \vdots&\ddots&\vdots\\ \operatorname{tr}(\tilde{W}_{s}V_{1})&\ldots&\operatorname{tr}(\tilde{W}_{s}V _{s})\end{bmatrix}\theta,\]

and gives unbiased estimates. For a known vector of weights \(w\), MINQUE uses

\[\tilde{W}_{j}=[I-A(w)]^{\prime}V^{-1}(w)[\mathbf{d}_{\theta_{j}}V(\theta)|_{ \theta=w}]V^{-1}(w)[I-A(w)].\]

Unfortunately, there is no compelling rationale (like maximum likelihood, REML, or MINQUE) for picking a particular set of \(\tilde{W}_{j}\)s for Henderson's Method 3. In Method 3 the \(\tilde{W}_{j}\)s are a convenient set of ppos.

### Maximum Likelihood

Assume that for \(\theta=(\theta_{1},\ldots,\theta_{s})^{\prime}\),

\[Y\sim N\left[X\beta,V(\theta)\right]\]

and that \(V(\theta)\) is nonsingular for all (allowable) \(\theta\), so that the density of \(Y\) exists. We write determinants as \(|V|\equiv\det(V)\). (The notation \(\det(V)\) becomes awkward.) The density/likelihood associated with \(Y\) is

\[(2\pi)^{-n/2}|V(\theta)|^{-1/2}\exp\bigl{[}-(Y-X\beta)^{\prime}[V^{-1}(\theta) ](Y-X\beta)/2\bigr{]}\,.\]

The log-likelihood is

\[\ell(\beta,\theta)\equiv-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log[|V(\theta)|]- \frac{1}{2}(Y-X\beta)^{\prime}[V^{-1}(\theta)](Y-X\beta).\]

The (partial) derivative of the log-likelihood with respect to \(\beta\) uses Propositions A.1.1 and A.1.2 from Appendix A and the chain rule to get

\[\mathbf{d}_{\beta}\ell(\beta,\theta) = \frac{-1}{2}\mathbf{d}_{\beta}\left[(Y-X\beta)^{\prime}[V^{-1}( \theta)](Y-X\beta)\right]\] \[= \frac{-1}{2}\left\{\mathbf{d}_{(Y-X\beta)}\left[(Y-X\beta)^{ \prime}[V^{-1}(\theta)](Y-X\beta)\right]\right\}\mathbf{d}_{\beta}(Y-X\beta)\] \[= \frac{-1}{2}\left\{2(Y-X\beta)^{\prime}[V^{-1}(\theta)]\right\}( -X)\] \[= (Y-X\beta)^{\prime}[V^{-1}(\theta)]X.\]

The (partial) derivatives of the log-likelihood with respect to \(\theta_{j}\), \(j=1,\ldots,s\), use Proposition A.1.2 parts (e), (c) and (b) to get

\[\mathbf{d}_{\theta_{j}}\ell(\beta,\theta)=\] \[\quad-\frac{1}{2}\text{tr}\left\{V^{-1}(\theta)[\mathbf{d}_{ \theta_{j}}V(\theta)]\right\}+\frac{1}{2}(Y-X\beta)^{\prime}V^{-1}(\theta)[ \mathbf{d}_{\theta_{j}}V(\theta)]V^{-1}(\theta)(Y-X\beta)\,.\]

Setting the partial derivatives equal to zero leads to solving the likelihood equations \(\mathbf{d}_{\beta}\ell(\beta,\theta)=0\) or equivalently

\[X^{\prime}V^{-1}(\theta)X\beta=X^{\prime}V^{-1}(\theta)Y\]

and, for \(j=1,\ldots,s\), \(\mathbf{d}_{\theta_{j}}\ell(\beta,\theta)=0\) which is equivalent to

\[\text{tr}\left\{V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V(\theta)]\right\}=(Y- X\beta)^{\prime}V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V(\theta)]V^{-1}(\theta)(Y-X \beta). \tag{4.2.1}\]

Most of the time, some iterative computational technique is needed to solve the equations in (4.2.1). (SAS's "proc mixed" seems to use Newton-Raphson.) For thegeneralized split plot models of _PA_ Chap. 11, relatively nice closed form solutions can be found. Even nicer solutions can be found for the multivariate linear models of Chaps. 9 and 10. If one uses structured covariances matrices in multivariate linear models, such as those inspired by time series models for longitudinal data, closed form estimates are no longer available. For the linear covariance models introduced in Sect. 4.4, relatively easy computational methods can be used, methods that merely involve iteratively solving systems of linear equations. Most of the covariance models discussed for mixed models in Chap. 5 and some of the covariance models discussed for spatial data in Chap. 8 have linear covariance structure. The covariance structures for most spatial models provide no obvious simplifications in solving these equations.

From _PA_ Chap. 2, the likelihood is maximized for any \(\theta\) by taking \(\hat{\beta}(\theta)\) to satisfy

\[X\hat{\beta}(\theta)=A(\theta)Y,\]

where

\[A(\theta)=X[X^{\prime}V^{-1}(\theta)X]^{-}X^{\prime}V^{-1}(\theta).\]

This allows us to solve the equations in (4.2.1) without reference to the likelihood equations for \(\beta\). Replacing \(\beta\) with its maximum likelihood estimate \(\hat{\beta}(\theta)\) in (4.2.1), it follows that the MLE of \(\theta\) satisfies

\[\begin{split}\operatorname{tr}\Big{\{}V^{-1}(\theta)[\mathbf{d}_ {\theta_{j}}V(\theta)]\Big{\}}&=[Y-X\hat{\beta}(\theta)]^{\prime }V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V(\theta)]V^{-1}(\theta)[Y-X\hat{\beta} (\theta)]\\ &=Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{d}_{ \theta_{j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)]Y\end{split} \tag{4.2.2}\]

for \(j=1,\ldots,s\).

Some applications use parameterizations in terms of \(V^{-1}(\theta)\) rather than \(V(\theta)\) so that it is easier to find \(\mathbf{d}_{\theta_{j}}V(\theta)^{-1}\) directly. In that case, using \(\mathbf{d}_{\theta_{j}}V(\theta)^{-1}=V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V (\theta)]V^{-1}(\theta)\) gives the equations

\[\operatorname{tr}\Big{\{}V(\theta)[\mathbf{d}_{\theta_{j}}V^{-1}(\theta)] \Big{\}}=Y^{\prime}[I-A(\theta)]^{\prime}[\mathbf{d}_{\theta_{j}}V^{-1}( \theta)][I-A(\theta)]Y.\]

It is interesting to note that solving the full set of likelihood equations also gives method of moments (or estimating equation) estimates. The likelihood equations can be viewed as setting

\[\operatorname{E}[X^{\prime}V^{-1}(\theta)Y]=X^{\prime}V^{-1}(\theta)Y\]

and for \(j=1,\ldots,s\)

\[\begin{split}\operatorname{E}\{(Y-X\beta)^{\prime}V^{-1}( \theta)[\mathbf{d}_{\theta_{j}}V(\theta)]V^{-1}(\theta)(Y-X\beta)\}\\ =(Y-X\beta)^{\prime}V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V( \theta)]V^{-1}(\theta)(Y-X\beta).\end{split}\]

The last equations follow because, similar to the proof of _PA_ Theorem 1.3.2,\[\mathrm{E}\left\{(Y-X\beta)^{\prime}V^{-1}(\theta)[\mathbf{d}_{ \theta_{j}}V(\theta)]V^{-1}(\theta)(Y-X\beta)\right\}\\ =\mathrm{tr}\{V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V(\theta)]V^{- 1}(\theta)V(\theta)\}=\mathrm{tr}\{V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V( \theta)]\}.\]

Note that the validity of this estimation procedure does not depend on the data being normal, although the motivation for using _these_ estimating equations, as opposed to some others, was based on the data being normal.

Typically, there are constraints on the parameters in \(\theta\). Variances must be non-negative, correlations must be between \(-1\) and \(1\), and \(V(\theta)\) must be positive definite. _Solutions to the likelihood equations are not MLEs if they do not satisfy these constraints_. In models this general, it frequently occurs that the likelihood has more than one local maximum. Spatial data models are particularly notorious for this. Solutions to the likelihood equations can occur at local maxima, local minima, and at saddlepoints. Personally, I find the existence of multiple modes enough to call in question the entire idea of maximizing the likelihood. I personally think it makes far more sense in such problems to use Bayesian techniques that seek to explore the entire likelihood function, although they do so somewhat indirectly through the posterior distribution. These issues can be difficult even in simple models, cf. Lavine, Bray, and Hodges (2015).

#### Generalized Likelihood Ratio Tests

Consider testing the full model already considered against a reduced linear model with \(C(X_{0})\subset C(X)\), i.e.,

\[Y\sim N[X_{0}\gamma,V(\theta)]\,.\]

For \(\theta\) known, the generalized likelihood ratio test statistic reduces to the usual linear model test

\[[X_{0}\hat{\gamma}(\theta)-X\hat{\beta}(\theta)]^{\prime}[V^{-1}( \theta)][X_{0}\hat{\gamma}(\theta)-X\hat{\beta}(\theta)]\\ \sim\chi^{2}[r(X)-r(X_{0}),(X_{0}\gamma-X\beta)^{\prime}[V^{-1}( \theta)](X_{0}\gamma-X\beta)/2],\]

where \(\chi^{2}(df,\pi)\) denotes a chi-squared distribution with \(df\) degrees of freedom and noncentrality parameter \(\pi\).

For \(\theta\) unknown but \(\hat{\theta}\) and \(\hat{\theta}_{0}\) denoting its MLEs under the full and reduced models respectively, we get the generalized likelihood ratio test statistic

\[\log[|V(\hat{\theta}_{0})|]+[Y-X_{0}\hat{\gamma}(\hat{\theta}_{0 })]^{\prime}[V^{-1}(\hat{\theta}_{0})][Y-X_{0}\hat{\gamma}(\hat{\theta}_{0})] \\ -\log[|V(\hat{\theta})|]-[Y-X\hat{\beta}(\hat{\theta})]^{\prime}[V^ {-1}(\hat{\theta})][Y-X\hat{\beta}(\hat{\theta})].\]

If standard likelihood theory applies, cf. Ferguson (1996), the test statistic should converge to \(\chi^{2}[r(X)-r(X_{0}),0]\) under the null (reduced) model.

Under the null model we expect both the full model MLE \(\hat{\theta}\) and the reduced model MLE \(\hat{\theta}_{0}\) to converge in probability to \(\theta\), so if \(V(\theta)\) is well behaved,\[\log[|V(\hat{\theta})|]-\log[|V(\hat{\theta}_{0})|]\stackrel{{ P}}{{ \rightarrow}}0\]

and we could drop the determinant terms without changing the asymptotic distribution of the test. Moreover, for any consistent estimate \(\tilde{\theta}\) of \(\theta\), the generalized likelihood ratio test statistic should be asymptotically equivalent under the null model to the following, which I think is the most appealing test statistic,

\[[X_{0}\hat{\gamma}(\tilde{\theta})-X\hat{\beta}(\tilde{\theta})]^{\prime}[V^{-1 }(\tilde{\theta})][X_{0}\hat{\gamma}(\tilde{\theta})-X\hat{\beta}(\tilde{\theta })].\]

I would expect choosing \(\tilde{\theta}=\hat{\theta}\) to have better power than \(\tilde{\theta}=\hat{\theta}_{0}\). Note that this is the same test statistic as presented in relation (4.1.5).

We will see in Chap. 5 that merely letting the sample size \(n\) go to infinity is no guarantee for consistent estimation of \(\theta\).

This subsection contains a lot of hand waving because I have never wanted my linear models books to get into asymptotic theory. However, Christensen and Sun (2010) and Christensen and Lin (2015) contain some reasonably nice asymptotic theory for linear models.

### Restricted Maximum Likelihood Estimation

The _residual (restricted) maximum likelihood (REML) estimation_ procedure of Patterson and Thompson (1974) can be applied to linear models with general covariance structures. The method incorporates a matrix \(B\) that is a full column rank matrix with

\[C(B)=C(X)^{\perp}\]

and the method maximizes the likelihood associated with

\[B^{\prime}Y\sim N[0,B^{\prime}V(\theta)B]. \tag{4.3.1}\]

Fortunately, it turns out that the particular choice of \(B\) does not matter.

Incorporating such a matrix \(B\) certainly places a restriction on the distribution of \(Y\) and thus on the likelihood, but what has that got to do with residuals? It turns out that we can arrive at the same point by maximizing the likelihood associated with the residuals. Unfortunately, that is a more complicated matter. First of all, the residuals have a singular covariance matrix, so they do not have an \(n\) dimensional density. In Appendix C we examine maximum likelihood estimation for normal distributions with singular covariance matrices and we apply those results to finding the maximum likelihood estimates of covariance parameters using residuals. For now we focus on maximizing the likelihood of model (4.3.1).

Similar to the previous section the log-likelihood for \(B^{\prime}Y\) is

\[\ell_{*}(\theta)\equiv-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log[|B^{\prime}V( \theta)B|]-\frac{1}{2}(B^{\prime}Y)^{\prime}[B^{\prime}V(\theta)B]^{-1}(B^{ \prime}Y).\]Setting the partial derivatives of the log-likelihood function equal to zero leads to solving

\[\mbox{tr}\left\{[B^{\prime}V(\theta)B]^{-1}[\mathbf{d}_{\theta_{j}}B^{\prime}V( \theta)B]\right\}=Y^{\prime}B[B^{\prime}V(\theta)B]^{-1}[\mathbf{d}_{\theta_{j }}B^{\prime}V(\theta)B][B^{\prime}V(\theta)B]^{-1}B^{\prime}Y\,,\]

for \(j=1,\ldots,s\). Noting that

\[\mathbf{d}_{\theta_{j}}B^{\prime}V(\theta)B=B^{\prime}[\mathbf{d}_{\theta_{j}}V (\theta)]B\,,\]

it is clearly equivalent to solve

\[\mbox{tr}\{[B^{\prime}V(\theta)B]^{-1}B^{\prime}[\mathbf{d}_{\theta_{j}}V( \theta)]B\}=Y^{\prime}B(B^{\prime}V(\theta)B)^{-1}B^{\prime}[\mathbf{d}_{\theta _{j}}V(\theta)]B[B^{\prime}V(\theta)B]^{-1}B^{\prime}Y\,. \tag{4.3.2}\]

As demonstrated later in Lemmas 4.3.1 and 4.3.2,

\[[I-A(\theta)]=V(\theta)B(B^{\prime}V(\theta)B)^{-1}B^{\prime} \tag{4.3.3}\]

and thus

\[V^{-1}(\theta)[I-A(\theta)]=B(B^{\prime}V(\theta)B)^{-1}B^{\prime}=[I-A( \theta)]^{\prime}V^{-1}(\theta). \tag{4.3.4}\]

Observing that

\[\mbox{tr}\{[B^{\prime}V(\theta)B]^{-1}B^{\prime}[\mathbf{d}_{\theta_{j}}V( \theta)]B\}=\mbox{tr}\{B[B^{\prime}V(\theta)B]^{-1}B^{\prime}[\mathbf{d}_{ \theta_{j}}V(\theta)]\}\]

and using (4.3.4), Eq. (4.3.2) can be rewritten as

\[\mbox{tr}\left\{V^{-1}(\theta)[I-A(\theta)][\mathbf{d}_{\theta_{j }}V(\theta)]\right\}\] \[\quad=Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{d}_{ \theta_{j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)]Y, \tag{4.3.5}\]

\(j=1,\ldots,s\), where \([I-A(\theta)]Y\) consists of the residuals from the BLUE of \(X\beta\), i.e., \([I-A(\theta)]Y=Y-X\hat{\beta}(\theta)\).

Note that the equations in (4.3.5) do not depend on the particular choice of the matrix \(B\), since \(B\) no longer appears in the equations. In \(PA\) we established that projections operators like \(A(\theta)\) are unique. Note also that the only differences between the REML Eq. (4.3.5) and the corresponding MLE equations for \(\theta\) in (4.2.2) is the existence of the term \([I-A(\theta)]\) in the trace in (4.3.5).

Some applications use parameterizations in terms of \(V^{-1}(\theta)\) rather than \(V(\theta)\) so that it is easier to find \(\mathbf{d}_{\theta_{j}}V(\theta)^{-1}\) directly. In that case, using \(\mathbf{d}_{\theta_{j}}V(\theta)^{-1}=V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V (\theta)]V^{-1}(\theta)\) on both sides of Eq. (4.3.5) gives

\[\mbox{tr}\left\{[I-A(\theta)]V(\theta)[\mathbf{d}_{\theta_{j}}V^{-1}(\theta)] \right\}=Y^{\prime}[I-A(\theta)]^{\prime}[\mathbf{d}_{\theta_{j}}V^{-1}( \theta)][I-A(\theta)]Y.\]

We now present two results that justify Eq. (4.3.3). Let \(r(X)=r\) so that \(r(B)=n-r\).

**Lemma 4.3.1**: _Let \(A=X(X^{\prime}V^{-1}X)^{-}X^{\prime}V^{-1}\). Then \(I-A\) is the projection operator onto \(C(VB)\) along \(C(X)\)._

From \(PA\) Sect. 2.7 we know that \(A\) is an oblique projection operator onto \(C(X)\). Since \(C(B)=C(X)^{\perp}\) and \(V\) is nonsingular, \(r(VB)=n-r=r(I-A)\). Also, \((I-A)VB=VB-X(X^{\prime}V^{-1}X)^{-}X^{\prime}B=VB\), so \(I-A\) is a projection onto \(C(VB)\). It is along \(C(X)\) because \(r(X)=r\) and \((I-A)X=X-X=0\).

**Lemma 4.3.2**: \(VB(B^{\prime}VB)^{-1}B^{\prime}\) is the projection operator onto \(C(VB)\) along \(C(X)\).

\(VB(B^{\prime}VB)^{-1}B^{\prime}=VB[(B^{\prime}V)V^{-1}(VB)]^{-1}(B^{\prime}V)V ^{-1}\) is a projection operator onto \(C(VB)\). Since \(C(B)=C(X)^{\perp}\), \(VB(B^{\prime}VB)^{-1}B^{\prime}X=0\). Moreover, \(r(VB)=n-r\) and \(r(X)=r\).

Similar to maximum likelihood estimation, the REML Eq. (4.3.5) also give method of moments (estimating equation) estimates. The REML equations can be viewed as finding a solution to

\[\mathrm{E}\{Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{ d}_{\theta_{j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)]Y\}\\ =Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{d}_{ \theta_{j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)]Y,\]

for \(j=1,\ldots,s\). The equivalence holds because \(A(\theta)\) is a projection operator onto \(C(X)\), \([I-A(\theta)]^{\prime}V^{-1}(\theta)=V^{-1}(\theta)[I-A(\theta)]\) and, similar to the proof of \(PA\) Theorem 1.3.2,

\[\mathrm{E}\{Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{ d}_{\theta_{j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)]Y\}\] \[=\mathrm{E}\{(Y-X\beta)^{\prime}[I-A(\theta)]^{\prime}V^{-1}( \theta)[\mathbf{d}_{\theta_{j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)](Y-X\beta)\}\] \[=\mathrm{tr}\{[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{d}_{ \theta_{j}}V(\theta)]V^{-1}(\theta)[I-A(\theta)]V(\theta)\}\] \[=\mathrm{tr}\{V^{-1}(\theta)[I-A(\theta)]V(\theta)[I-A(\theta)]^{ \prime}V^{-1}(\theta)[\mathbf{d}_{\theta_{j}}V(\theta)]\}\] \[=\mathrm{tr}\{V^{-1}(\theta)[I-A(\theta)]V(\theta)V^{-1}(\theta)[ I-A(\theta)][\mathbf{d}_{\theta_{j}}V(\theta)]\}\] \[=\mathrm{tr}\{V^{-1}(\theta)[I-A(\theta)][\mathbf{d}_{\theta_{j} }V(\theta)]\}.\]

As with the corresponding argument from the previous section, the validity of this estimation procedure does not depend on the assumption of normality even though the equations that define the procedure were determined using normality.

**Exercise 4.2**: _Consider the model \(Y=X\beta+e\), \(e\sim N(0,\sigma^{2}I)\). Show that the \(MSE\) is the REML estimate of \(\sigma^{2}\)._

**Exercise 4.3**: _Consider the heteroscedastic two-sample problem, \(y_{ij}=\mu_{i}+\varepsilon_{ij}\), \(i=1,2\), \(j=1,\ldots,N_{i}\), with \(\varepsilon_{ij}\)s independent \(N(0,\sigma_{i}^{2})\). Write the linear model as_\(\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}=\begin{bmatrix}J_{N_{1}}&0\\ 0&J_{N_{2}}\end{bmatrix}\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix}+\begin{bmatrix}e_{1}\\ e_{2}\end{bmatrix},\quad\operatorname{E}(e)=0,\quad\operatorname{Cov}\left( \begin{bmatrix}e_{1}\\ e_{2}\end{bmatrix}\right)=\begin{bmatrix}\sigma_{1}^{2}I_{N_{1}}&0\\ 0&\sigma_{2}^{2}I_{N_{2}}\end{bmatrix}\).

1. What is the vector \(\theta\)? Show that the least squares estimates are always the BLUEs, i.e., show \(C\left[V(\theta)X\right]\subset C(X)\) for any \(\theta\).
2. Show that \[[I-A(\theta)]=\operatorname{Blk\ diag}\left[I_{N_{i}}-\frac{1}{N_{i}}J_{N_{i}}^ {N_{i}}\right].\] Find \(\operatorname{Cov}(\hat{\beta})\).
3. Show that for \(i=1,2\), \[\operatorname{tr}\left\{V^{-1}(\theta)[I-A(\theta)][\mathbf{d}_{\theta_{i}}V( \theta)]\right\}=\left(N_{i}-1\right)\Big{/}\sigma_{i}^{2}.\]
4. Show that for \(i=1,2\), \[Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)[\mathbf{d}_{\theta_{i}}V( \theta)]V^{-1}(\theta)[I-A(\theta)]Y=\left(Y_{i}-\bar{y}_{i}J_{N_{i}}\right)^{ \prime}\frac{1}{\sigma_{i}^{4}}\left(Y_{i}-\bar{y}_{i}J_{N_{i}}\right).\]
5. Show that the REML estimate of \(\sigma_{i}^{2}\) is the sample variance of the \(i\)th sample.
6. Using the REML estimates and \(\operatorname{Cov}(\hat{\beta})\), find a standard error for \(\hat{\mu}_{1}-\hat{\mu}_{2}\).

**Exercise 4.4**.: Generalizing the previous exercise, let

\[\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}=\begin{bmatrix}X_{1}&0\\ 0&X_{2}\end{bmatrix}\begin{bmatrix}\beta_{1}\\ \beta_{2}\end{bmatrix}+e,\quad\operatorname{E}(e)=0,\quad\operatorname{Cov} \left(\begin{bmatrix}e_{1}\\ e_{2}\end{bmatrix}\right)=\begin{bmatrix}\sigma_{1}^{2}I_{N_{1}}&0\\ 0&\sigma_{2}^{2}I_{N_{2}}\end{bmatrix}.\]

1. Show that the REML estimate of \(\sigma_{i}^{2}\) is \(Y_{i}^{\prime}(I-M_{i})Y_{i}/[N_{i}-r(X_{i})]\) where \(M_{i}\) is the perpendicular projection operator (ppo) onto \(C(X_{i})\).
2. Show that the MLE of \(\sigma_{i}^{2}\) is \(Y_{i}^{\prime}(I-M_{i})Y_{i}/N_{i}\).

**Exercise 4.5**.: Consider the linear model

\[Y=X\beta+e,\qquad e\sim N\left[0,V(\theta)\right].\]

Partition \(\theta\) as \(\theta^{\prime}=[\theta_{1},\theta_{*}^{\prime}]\) and suppose

\[V(\theta)\equiv\theta_{1}V_{*}(\theta_{*}),\]

where \(\theta_{1}>0\) and \(V_{*}(\theta_{*})\) is positive definite for all allowable \(\theta_{*}\). Define

\[SSE(\theta)=Y^{\prime}[I-A(\theta)]^{\prime}V_{*}(\theta_{*})^{-1}[I-A(\theta) ]Y. \tag{4.3.6}\]1. Recalling its definition in (4.1.2), show that \(A(\theta)=X[X^{\prime}V_{*}(\theta_{*})^{-1}X]^{-}X^{\prime}V_{*}(\theta_{*})^{-1}\). Show that \(SSE(\theta)\) does not depend on \(\theta_{1}\).
2. Show that if \(\hat{\theta}\) is a solution to the REML equations, then \[\hat{\theta}_{1}=\frac{SSE(\hat{\theta})}{n-r(X)}.\]
3. Show that if \(\hat{\theta}\) is a solution to the likelihood equations, then \[\hat{\theta}_{1}=\frac{SSE(\hat{\theta})}{n}.\]

Hint: You need only look at the equations with \(s=1\).

The following exercise examines a quite general model in which solutions to the REML equations are available in closed form.

**Exercise 4.6**.: Consider a generalized split plot model as defined in _PA_ Chap. 11, or the JSTOR-available (Christensen 1987). In particular you will need the definitions and properties of the perpendicular projections operators \(M_{1}\), \(M_{*}\), and \(M_{2}\). The covariance matrix is

\[V(\theta)=\sigma_{0}^{2}I+\sigma_{1}^{2}X_{1}X_{1}^{\prime}=\sigma^{2}[(1- \rho)I+m\rho M_{1}]\]

where \(\sigma_{0}^{2}\) is the subplot variance, \(\sigma_{1}^{2}\) is the whole plot variance, \(X_{1}\) is a matrix of indicator variables for the whole plots, \(m\) is the number of subplots in each whole plot, \(\sigma^{2}\equiv\sigma_{0}^{2}+\sigma_{1}^{2}\), and \(\rho\equiv\sigma_{1}^{2}/(\sigma_{0}^{2}+\sigma_{1}^{2})\). Reparameterize the problem as

\[\theta_{1}=\sigma^{2}(1-\rho)=\sigma_{0}^{2},\qquad\theta_{2}=\frac{m\rho}{(1- \rho)}\]

so that

\[V(\theta)=\theta_{1}[I+\theta_{2}M_{1}].\]

Note that \(A(\theta)=M_{*}+M_{2}\) for any vector \(\theta\), that \(C(M_{*})\subset C(M_{1})\), that \(C(M_{1})\perp C(M_{2})\), and that \(V^{-1}\) is available from _PA-V_ Proposition B.57 (or Christensen 2011, Proposition 12.11.1).

With this \(\theta\) parameterization, the solution to the first REML equation, as discussed in Exercise 4.5, provides an estimate for \(\theta_{1}\). Show that the estimate of \(\theta_{1}\) reduces in this problem to the closed form \(Y^{\prime}(I-M_{*}-M_{2})Y/r(I-M_{*}-M_{2})\), which is the error estimate used in the subplot analysis of a generalized split plot model. Show that the second REML equation has a solution that involves estimating \(\sigma^{2}[(1-\rho)+m\rho]=\sigma_{0}^{2}+m\sigma_{1}^{2}\) by \(Y^{\prime}(M_{1}-M_{*})Y/r(M_{1}-M_{*})\), which is the error estimate used in the whole plot analysis of a generalized split plot model.

### Linear Covariance Structures

Consider the linear model

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=V(\theta),\]

wherein \(V(\theta)\) has the linear structure

\[V(\theta)=V_{0}+\theta_{1}V_{1}+\cdots+\theta_{s}V_{s} \tag{4.4.1}\]

for known matrices \(V_{j}\) that are most often symmetric but often fail to be nonnegative definite. The matrix \(V_{0}\) allows some elements of the covariance matrix to be known as something other than 0. We continue to assume that \(V(\theta)\) is nonsingular for all allowable \(\theta\). This covariance structure simplifies solving the likelihood and REML equations, it is fundamental to the development of MINQUE estimates in the next section, but it also arises naturally in mixed models (Chap. 5), spatial models (Chap. 8), and generalized multivariate linear models and their extensions to other longitudinal data (Chap. 11).

The remainder of this section is devoted to demonstrating that linear covariance structure allows one to solve the REML equations by iteratively solving systems of linear equations. This is also true for the likelihood equations but we only demonstrate it for the REML equations. In either case,

\[\mathbf{d}_{\theta_{j}}V(\theta)=V_{j},\]

so, in particular, the REML equations reduce to

\[\mathrm{tr}\left\{V^{-1}(\theta)[I-A(\theta)]V_{j}\right\}=Y^{\prime}[I-A( \theta)]^{\prime}V^{-1}(\theta)V_{j}V^{-1}(\theta)[I-A(\theta)]Y,\]

\(j=1,\ldots,s\). To simplify notation, henceforth we write \(V(\theta)\) as \(V\) and \(A(\theta)\) as \(A\). Similar to the previous section when establishing that REML has an estimating equation justification,

\[\mathrm{tr}[V^{-1}(I-A)V_{j}]\] \[=\mathrm{tr}[V^{-1}(I-A)(I-A)V_{j}]\] \[=\mathrm{tr}[(I-A)V_{j}V^{-1}(I-A)]\] \[=\mathrm{tr}[VV^{-1}(I-A)V_{j}V^{-1}(I-A)]\] \[=\mathrm{tr}[V(I-A)^{\prime}V^{-1}V_{j}V^{-1}(I-A)]\] \[=\mathrm{tr}\Bigg{[}\left(V_{0}+\sum_{k=1}^{s}\theta_{k}V_{k} \right)(I-A)^{\prime}V^{-1}V_{j}V^{-1}(I-A)\Bigg{]}\] \[=\sum_{k=1}^{s}\theta_{k}\mathrm{tr}\big{[}V_{k}(I-A)^{\prime}V^{ -1}V_{j}V^{-1}(I-A)\big{]}+\mathrm{tr}\big{[}V_{0}(I-A)^{\prime}V^{-1}V_{j}V^ {-1}(I-A)\big{]}\,.\]The equations for finding the REML estimates can now be written

\[\sum_{k=1}^{s}\theta_{k}\text{tr}\big{[}V_{k}(I-A)^{\prime}V^{-1}V_{j }V^{-1}(I-A)\big{]}\\ =Y^{\prime}(I-A)^{\prime}V^{-1}V_{j}V^{-1}(I-A)Y-\text{tr}\big{[} V_{0}(I-A)^{\prime}V^{-1}V_{j}V^{-1}(I-A)\big{]}\,, \tag{1}\]

\(j=1,\ldots,s\). Since \(V\) is unknown, typically an initial guess for \(V\) will be made by making an initial guess for \(\theta\). With \(V\) and thus \(A\) fixed, estimates of the \(\theta_{j}\)s are computed as the solution to the system of linear equations. These estimates of the covariance parameters determine a new choice of \(V\) that can be used to get updated values of the \(\theta_{j}\)s. This iterative procedure is repeated until the \(\theta_{j}\)s converge. Since the equations are linear, solutions are easy to find. As mentioned, similar methods can be used to find unrestricted MLEs. In fact, doing so only involves removing the terms \((I-A)\) from the traces.

## 4.5 Minque

Before beginning on MINQUE, we quickly review generalized least squares estimation. A linear model has \(\text{E}(Y)=X\beta\) and, for a positive definite weighting matrix \(W\), the generalized least squares estimates are values of \(\beta\) that minimize the _norm_

\[(Y-X\beta)^{\prime}W^{-1}(Y-X\beta).\]

Such estimates \(\hat{\beta}_{W}\) satisfy the generalized normal equations \(X^{\prime}W^{-1}X\hat{\beta}_{W}=X^{\prime}W^{-1}Y\). Recall that generalized least squares is a geometric optimality property, not a statistical optimality property. We will use these results to define MINQUE estimation.

Consider again the linear model

\[Y=X\beta+e,\quad\text{E}(e)=0,\quad\text{Cov}(e)=V(\theta),\]

with \(V(\theta)\) having the linear structure

\[V(\theta)=V_{0}+\theta_{1}V_{1}+\cdots+\theta_{s}V_{s}.\]

MINQUE provides a method for estimating the covariance parameter \(\theta\). MINQUE is short for _minimum norm quadratic unbiased (translation invariant) estimation_. Attention is restricted to estimates that are translation invariant unbiased quadratic forms in the observations. Our discussion follows Christensen (1993) which takes more care to discuss issues related to invariance.

Take \(B\) to be a full rank matrix with \(C(B)=C(X)^{\perp}\). With such a choice, \(\text{E}(B^{\prime}Y)=0\), so \(\text{Cov}(B^{\prime}Y)=\text{E}[B^{\prime}YY^{\prime}B]\), and with linear covariance structure, the expected value of the observable random matrix \(B^{\prime}YY^{\prime}B\) has

\[\text{E}[B^{\prime}YY^{\prime}B]=B^{\prime}V(\theta)B=B^{\prime}V_{0}B+\theta_ {1}B^{\prime}V_{1}B+\cdots+\theta_{s}B^{\prime}V_{s}B.\]Using Vec operators and Kronecker products as reviewed in Appendix A.2, this becomes

\[\begin{array}{l}\mathrm{E}[\mathrm{Vec}(B^{\prime}YY^{\prime}B)]\\ =\mathrm{Vec}(B^{\prime}V_{0}B)+\theta_{1}\mathrm{Vec}(B^{\prime}V_{1}B)+\cdots +\theta_{s}\mathrm{Vec}(B^{\prime}V_{s}B)\\ =[B^{\prime}\otimes B^{\prime}]\mathrm{Vec}(V_{0})+\theta_{1}[B^{\prime} \otimes B^{\prime}]\mathrm{Vec}(V_{1})+\cdots+\theta_{s}[B^{\prime}\otimes B^ {\prime}]\mathrm{Vec}(V_{s}),\end{array}\]

which defines a linear model for the parameters \(\theta\). In particular, we write a linear model

\[\mathrm{E}(\tilde{Y})=\tilde{X}\theta+\tilde{v}\]

with model matrix \(\tilde{X}\) and offset vector \(\tilde{v}\), wherein

\[\tilde{Y}=\mathrm{Vec}(B^{\prime}YY^{\prime}B)=[B^{\prime}Y\otimes B^{\prime} Y],\qquad\tilde{v}=\mathrm{Vec}(B^{\prime}V_{0}B)=[B^{\prime}\otimes B^{\prime}] \mathrm{Vec}(V_{0}).\]

\[\tilde{X}=\{\tilde{X}_{1},\ldots,\tilde{X}_{s}\}=[B^{\prime}\otimes B^{\prime }][\mathrm{Vec}(V_{1}),\ldots,\mathrm{Vec}(V_{s})]\equiv[B^{\prime}\otimes B^ {\prime}]\tilde{U}.\]

Equivalently, we can write the linear model as

\[\mathrm{E}(\tilde{Y}-\tilde{v})=\tilde{X}\theta \tag{4.5.1}\]

with observable dependent variable vector \(\tilde{Y}-\tilde{v}\).

MINQUE estimates of \(\theta\) use generalized least squares, i.e., they minimize

\[(\tilde{Y}-\tilde{v}-\tilde{X}\theta)^{\prime}\tilde{W}^{-1}(\tilde{Y}-\tilde{ v}-\tilde{X}\theta)^{\prime}\]

where, for some \(s\) dimensional weight vector \(w\) we define,

\[\tilde{W}\equiv[B^{\prime}V(w)B\otimes B^{\prime}V(w)B].\]

The weight vector \(w\) might consist of a vector of prior guesses for the parameters. MINQUE(1) takes the guess for \(\theta\) as \(J_{s}\). MINQUE(0) takes the guess for \(\theta\) as \((1,0,\ldots,0)^{\prime}\) but only when \(V_{1}=I_{n}\). In particular, estimates are typically obtained by solving the generalized normal equations

\[\tilde{X}^{\prime}\tilde{W}^{-1}\tilde{X}\theta=\tilde{X}^{\prime}\tilde{W}^{- 1}(\tilde{Y}-\tilde{v}).\]

As seen below, for symmetric \(V_{j}\)s, the generalized normal equations reduce to the well known MINQUE equations,

\[\begin{array}{l}\sum_{k=1}^{s}\theta_{k}\mathrm{tr}\big{\{}V_{k}[I-A(w)]^{ \prime}V^{-1}(w)V_{j}V^{-1}(w)[I-A(w)]\big{\}}\\ =Y^{\prime}[I-A(w)]^{\prime}V^{-1}(w)V_{j}V^{-1}(w)[I-A(w)]Y\\ \qquad-\mathrm{tr}\big{\{}V_{0}[I-A(w)]^{\prime}V^{-1}(w)V_{j}V^{-1}(w)[I-A(w)] \big{\}}\end{array},\]

Note the similarity to the REML equations of (4.4.1) wherein \(V\equiv V(\theta)\) and \(A\equiv A(\theta)\). The iterative process of solving the REML equations amounts to solving the MINQUE equations once for an arbitrary \(w\) and then iteratively replacing \(w\) with the solutions to the previous set of equations. As a rule, this iterative MINQUE process provides neither minimum norm, nor quadratic, nor unbiased estimates.

Searle, Casella, and McCulloch (1992, Chapter 12), Christensen (1993), and Cressie (1993, Section 2.6) all discuss versions of this linear model approach. Justus Seely originally came up with the idea that quadratic forms could be recast as linear functions and Friedrich Pukelsheim importantly developed notation for working with them.

The remainder of this section is devoted to simplifying the generalized normal equations into the MINQUE equations.

#### Deriving the MINQUE Equations

The key to simplifying the generalized normal equations is that

\[[B\otimes B][B^{\prime}V(w)B\otimes B^{\prime}V(w)B]^{-1}[B^{ \prime}\otimes B^{\prime}]\\ =\left[V^{-1}(w)[I-A(w)]\otimes V^{-1}(w)[I-A(w)]\right].\]

It suffices to show that

\[B[B^{\prime}V(w)B]^{-1}B^{\prime}=V^{-1}(w)[I-A(w)],\]

but this was established in Sect. 4.3 using Lemmas 4.3.1 and 4.3.2.

Consider the normal equations in pieces. To simplify notation, write \(V\equiv V(w)\) and \(A\equiv A(w)\).

\[\tilde{X}\tilde{W}^{-1}\tilde{Y}\] \[=\tilde{U}^{\prime}[B\otimes B]\left\{[B^{\prime}VB]^{-1}\otimes[ B^{\prime}VB]^{-1}\right\}[B^{\prime}\otimes B^{\prime}][Y\otimes Y]\] \[=\tilde{U}^{\prime}\left\{V^{-1}(I-A)\otimes V^{-1}(I-A)\right\}[ Y\otimes Y]\] \[=[\text{Vec}(V_{1}),\ldots,\text{Vec}(V_{s})]^{\prime}\left\{V^{- 1}(I-A)Y\otimes V^{-1}(I-A)Y\right\}\] \[=[\text{Vec}(V_{1}),\ldots,\text{Vec}(V_{s})]^{\prime}\text{Vec}[ V^{-1}(I-A)YY^{\prime}(I-A)^{\prime}V^{-1}].\]

The \(j\)th element of the vector is

\[\text{Vec}(V_{j})^{\prime}\text{Vec}[V^{-1}(I-A)YY^{\prime}(I-A) ^{\prime}V^{-1}] =\text{tr}\left[V_{j}^{\prime}V^{-1}(I-A)YY^{\prime}(I-A)^{\prime }V^{-1}\right]\] \[=\text{tr}\left[Y^{\prime}(I-A)^{\prime}V^{-1}V_{j}^{\prime}V^{- 1}(I-A)Y\right]\] \[=Y^{\prime}(I-A)^{\prime}V^{-1}V_{j}^{\prime}V^{-1}(I-A)Y.\]

Now consider

\[\tilde{X}\tilde{W}^{-1}\tilde{v}\] \[=\tilde{U}^{\prime}[B\otimes B]\left\{[B^{\prime}VB]^{-1}\otimes[ B^{\prime}VB]^{-1}\right\}[B^{\prime}\otimes B^{\prime}]\text{Vec}(V_{0})\]\[= \tilde{U}^{\prime}\left[V^{-1}(I-A)\otimes V^{-1}(I-A)\right] \operatorname{Vec}(V_{0})\] \[= \left[\operatorname{Vec}(V_{1}),\ldots,\operatorname{Vec}(V_{s}) \right]\bigr{]}^{\prime}\operatorname{Vec}[V^{-1}(I-A)V_{0}(I-A)^{\prime}V^{-1}].\]

The \(j\)th element of the vector is

\[\operatorname{Vec}(V_{j})^{\prime}\operatorname{Vec}[V^{-1}(I-A)V_{0}(I-A)^{ \prime}V^{-1}] = \operatorname{tr}\left[V_{j}^{\prime}V^{-1}(I-A)V_{0}(I-A)^{ \prime}V^{-1}\right]\] \[= \operatorname{tr}\left[V_{0}(I-A)^{\prime}V^{-1}V_{j}^{\prime}V^{ -1}(I-A)\right].\]

Similarly,

\[\tilde{X}^{\prime}\tilde{W}^{-1}\tilde{X}\theta\] \[= \tilde{U}^{\prime}[B\otimes B]\left\{[B^{\prime}VB]^{-1}\otimes[B ^{\prime}VB]^{-1}\right\}[B^{\prime}\otimes B^{\prime}]\tilde{U}\theta\] \[= \tilde{U}^{\prime}\left\{V^{-1}(I-A)\otimes V^{-1}(I-A)\right\}[ \operatorname{Vec}(V_{1}),\ldots,\operatorname{Vec}(V_{s})]\theta\] \[= [\operatorname{Vec}(V_{1}),\ldots,\operatorname{Vec}(V_{s})]^{\prime}\] \[\times\left\{\operatorname{Vec}[V^{-1}(I-A)V_{1}(I-A)^{\prime}V^{ -1}],\ldots,\operatorname{Vec}[V^{-1}(I-A)V_{s}(I-A)^{\prime}V^{-1}]\right\}\theta.\]

The \(j\)th element of the vector is

\[\operatorname{Vec}(V_{j})^{\prime}\left\{\operatorname{Vec}[V^{-1}(I-A)V_{1}(I- A)^{\prime}V^{-1}],\ldots,\operatorname{Vec}[V^{-1}(I-A)V_{s}(I-A)^{\prime}V^{-1}] \right\}\theta\] \[= \left\{\operatorname{tr}[V_{j}^{\prime}V^{-1}(I-A)V_{1}(I-A)^{ \prime}V^{-1}],\ldots,\operatorname{tr}[V_{j}^{\prime}V^{-1}(I-A)V_{s}(I-A)^{ \prime}V^{-1}]\right\}\theta\] \[= \sum_{k=1}^{s}\theta_{k}\operatorname{tr}[V_{j}^{\prime}V^{-1}(I- A)V_{k}(I-A)^{\prime}V^{-1}]\] \[= \sum_{k=1}^{s}\theta_{k}\operatorname{tr}[V_{k}(I-A)^{\prime}V^{ -1}V_{j}^{\prime}V^{-1}(I-A)].\]

All together, the \(j\)th normal equation is

\[\sum_{k=1}^{s}\theta_{k}\operatorname{tr}[V_{k}(I-A)^{\prime}V^{ -1}V_{j}^{\prime}V^{-1}(I-A)]\\ =Y^{\prime}(I-A)^{\prime}V^{-1}V_{j}^{\prime}V^{-1}(I-A)Y- \operatorname{tr}\left[V_{0}(I-A)^{\prime}V^{-1}V_{j}^{\prime}V^{-1}(I-A) \right].\]

If \(V_{1},\ldots,V_{s}\) are symmetric, these are the MINQUE equations.

## 0.6 Mivque

_Minimum variance quadratic unbiased translation invariant estimates (MIVQUEs)_ would be the best linear unbiased estimates (BLUEs) in model (4.5.1). Quadratic unbiased translation invariant estimates are precisely the linear unbiased estimates in this model. By definition, the best unbiased estimates are those with minimum variance. However, the model does not specify the covariance matrix of \(\tilde{Y}=\operatorname{Vec}[B^{\prime}YY^{\prime}B]\), say \(\tilde{V}\). The covariance matrix is needed to establish that something has minimum variance. Typically, \(\tilde{V}\) is a singular matrix because the vector \([Y\otimes Y]\) contains both \(y_{i}y_{j}\) and \(y_{j}y_{i}\). While best linear unbiased estimation in linear models with an arbitrary singular covariance matrix is quite a complicated problem, a simple solution exists, cf. _PA_ Chap. 10. For model (4.5.1), the BLUE of \(\tilde{X}\theta\) is \(\tilde{X}(\tilde{X}^{\prime}\tilde{W}^{-}\tilde{X})^{-}\tilde{X}^{\prime} \tilde{W}^{-}(\tilde{Y}-\tilde{v})\) where \(\tilde{W}=\tilde{V}+\tilde{X}\tilde{X}^{\prime}\) and it is assumed that \(\tilde{v}\in C(\tilde{V},\tilde{X})\) (which is usually true since \(\tilde{v}\) is usually 0). However, finding the estimate requires us to know \(\tilde{V}\), at least up to a scalar multiple. Even if we assume that \(Y\) is multivariate normal, the covariance matrix of \(\tilde{Y}\) is unknown because it depends on the parameter vector \(\theta\). In particular, one can show that \(B^{\prime}YY^{\prime}B\) has a central Wishart distribution in the sense of Chap. 9 so that \(\tilde{V}=[B^{\prime}V(\theta)B\otimes B^{\prime}V(\theta)B](I+T)\) where \(T\) is the \([n-r(X)]^{2}\times[n-r(X)]^{2}\) matrix that transforms \(\mathrm{Vec}(Q)\) into \(\mathrm{Vec}(Q^{\prime})\). In short, there is no reason to think that anyone can find a MIVQUE estimate in anything but the most degenerate of models.

Typically, when people find "MIVQUE" estimates, they pretend that they know \(V\), plug it into \(\tilde{V}=[B^{\prime}V(\theta)B\otimes B^{\prime}V(\theta)B](I+T)\), and use linear model theory to get what would be the optimal estimate of \(\theta\) if one really knew \(V\). Then, because they know that they do not know \(V\), they iterate the procedure. Modulo some results in Exercise 4.7, this is the same thing as iterated MINQUE, which is no excuse for the fact that the names MINQUE and MIVQUE are often used interchangeably. And like iterated MINQUE, these "MIVQUE" estimates are typically not minimum variance, nor quadratic, nor unbiased.

**Theorem 4.6.1**.: Suppose \(Y\sim N(\mu,V)\) and \(V\) is nonsingular. Then \(\mathrm{Var}(Y^{\prime}QY)=2\mathrm{tr}\big{[}(QV)^{2}\big{]}+4\mu^{\prime}QVQ\mu\).

Proof.: See Searle (1971, Section 2.5). For \(\mu=0\), the result also follows from looking at \(Y^{\prime}QY=\mathrm{Vec}(Q)^{\prime}\mathrm{Vec}(YY^{\prime})\) where \(YY^{\prime}\) is Wishart with the appropriate covariance matrix and \(Q=Q^{\prime}\). 

**Exercise 4.7**.:
1. Show that \([B^{\prime}V(\theta)B\otimes B^{\prime}V(\theta)B](I+T)\) is not positive definite. Hint: For a non-symmetric matrix \(Q\), check the vector \(\mathrm{Vec}(Q-Q^{\prime})\).
2. For any nonsymmetric matrix \(Q\), show that \(\mathrm{Vec}(Q)^{\prime}\tilde{Y}=\mathrm{Vec}(Q+Q^{\prime})\tilde{Y}/2\). Argue that in model (4.5.1) we can restrict attention to linear estimates \(\mathrm{Vec}(Q)^{\prime}\tilde{Y}\) where \(Q\) is symmetric. In particular, show that any MINQUE estimate of an estimable function \(\lambda^{\prime}\theta\) can be written as \(\mathrm{Vec}(Q)^{\prime}(\tilde{Y}-\tilde{v})\) where \(Q\) is symmetric.
3. Show that if we restrict attention to symmetric linear combinations of \(\tilde{Y}\), MIVQUE as discussed in this section is equivalent to MINQUE(\(\theta\)).
4. Consider the vector space of \(p\times p\) matrices and let \(e_{i}\) be the \(i\)th column of \(I_{p}\). Let the inner product between two matrices \(Q_{1}\) and \(Q_{2}\) be \(\mathrm{Vec}(Q_{1})^{\prime}\mathrm{Vec}(Q_{2})=\mathrm{tr}(Q_{1}^{\prime}Q_{ 2})\). Show that the matrices \(e_{i}e_{j}^{\prime}\) constitute an orthonormal basis for this \(p^{2}\) dimensional space. Show that the vectors \(e_{i}e_{j}^{\prime}+e_{j}e_{i}^{\prime}\), \(i=1,\ldots,p\), \(j\leq i\) constitute an orthogonal basis for the \(p(p+1)/2\) dimensional subspace of symmetric matrices. Show that the vectors \(e_{i}e_{j}^{\prime}-e_{j}e_{i}^{\prime}\), \(i=1,\ldots,p\), \(j<i\) constitute an orthogonal basis for the \(p(p-1)/2\) dimensional subspace of matrices that are orthogonal to the symmetric matrices. Using the orthonormal basis, find the \(p^{2}\times p^{2}\) matrix \(T\) that maps \(\operatorname{Vec}(Q)\) into \(\operatorname{Vec}(Q^{\prime})\).
5. Show that \(I=TT\) and that \((1/2)[I+T]\) is the ppo onto the space of symmetric matrices.
6. Show that \((1/2)[I-T]\) is the ppo onto the orthogonal complement of the symmetric matrices.
7. Show that if \(Q\) is in this orthogonal complement, then \(q_{ij}=-q_{ji}\).
8. Show that if the \(V_{j}\)s in model (4.5.1) are symmetric, the entire linear model problem exists within in the subspace of \(\mathbf{R}^{[n-r(X)]^{2}}\) that consists of vectors \(\operatorname{Vec}(Q)\) with \(Q=Q^{\prime}\).
9. Show that \[[B^{\prime}V(\theta)B\otimes B^{\prime}V(\theta)B](I+T)=2\frac{1}{2}(I+T)[B^{ \prime}V(\theta)B\otimes B^{\prime}V(\theta)B]\frac{1}{2}(I+T).\] The second version is clearly nonnegative definite, unlike the first version. Hint: Pre- and post- multiply by \(\operatorname{Vec}(Q_{1})^{\prime}\) and \(\operatorname{Vec}(Q_{2})\), respectively.

### The Effect of Estimated Covariances

Typically, \(V\equiv V(\theta)\) is not known, nor does it have a special form that allows a simple analysis. In practice, \(V\) is estimated with a function of \(Y\), say \(\tilde{V}(Y)=\tilde{V}\), that is nonnegative definite. More specifically, we estimate \(\theta\) by \(\tilde{\theta}\) and define \(\tilde{V}(Y)=\tilde{V}=V(\tilde{\theta})\).

Estimation of \(\beta\) is performed by plugging \(\tilde{V}\) in for \(V\), that is, an estimate is \(\tilde{\beta}\), where

\[X\tilde{\beta}=\tilde{A}Y\]

and \(\tilde{A}\) is the random projection operator

\[\tilde{A}=X(X^{\prime}\tilde{V}^{-1}X)^{-}X^{\prime}\tilde{V}^{-1}\,.\]

\(\tilde{A}\) is random because \(\tilde{V}\) is a function of \(Y\), hence random. For simplicity, in the discussion that follows, we will assume that \(V\) and \(\tilde{V}\) are positive definite. \(X\tilde{\beta}\) is called either an _empirical estimate_ or a _plug-in estimate_.

For the prediction problem of Sect. 4.1.2, estimates \(\tilde{V}(Y)\) and \(\tilde{V}_{y0}(Y)\) are required. The empirical (plug-in) predictor is taken by analogy with Theorem 4.1.3 as

\[\tilde{y}_{0}=x_{0}^{\prime}\tilde{\beta}+\tilde{\delta}^{\prime}(Y-X\tilde{ \beta}),\]

where \(\tilde{\beta}\) is defined as before,

\[\tilde{\delta}=\tilde{V}^{-1}\tilde{V}_{y0}\,,\]

and \(x_{0}^{\prime}\beta\) is assumed to be estimable.

The empirical estimate of a function \(\lambda^{\prime}\beta\) or prediction of a point \(y_{0}\) are complicated functions of \(Y\). In this section we present general mathematical results originally due to Eaton (1985) and Harville (1985) showing that under reasonable conditions the empirical estimates and predictors are unbiased.

When \(V(\theta)\) is known, there are formula given in Sect. 4.1 for the variances of BLUEs \(\lambda^{\prime}\hat{\beta}\) and BLUPs \(\hat{y}_{0}\) that depend on \(V(\theta)\). It is tempting to use these formulae with \(V(\theta)\) replaced by \(V(\tilde{\theta})\) to provide standard errors for the empirical estimates and predictors. We also show that such standard errors systematically underestimate the true standard errors. It seems reasonable that the empirical estimates and predictors would be worse (have higher variance) than the BLUEs and BLUPs, since they involve estimating parameters that are taken as known in the BLUEs and BLUPs. We demonstrate, not only that the empirical values have larger variances, but that the naive standard error formulae that uses \(\tilde{\theta}\) to replace \(\theta\) tends to underestimate the true variability because it has an expected value bounded above by the formulae that use the true \(\theta\).

As discussed in Sect. 4.1.2, assume that \(\theta^{\prime}=[\theta_{1},\theta^{\prime}_{s}]\) with

\[V(\theta)\equiv\theta_{1}V_{\rm s}(\theta_{\rm s}).\]

One way to proceed is to find estimates \(\tilde{\theta}_{1}\) and \(\tilde{\theta}_{\rm s}\), treat \(V_{\rm s}(\tilde{\theta}_{\rm s})\) as the true value \(V_{\rm s}(\theta_{\rm s})\), and construct standard errors and \(F\) statistics as usual for weighted least squares models. Based on the results of the next subsection, there should be a tendency for standard errors to be underestimated and tests to appear more significant than they are. The \(F\) statistics will not have exact \(F\) distributions. Part of the difficulty is that the theory for \(F\) tests involves \(\tilde{\theta}_{1}\) being independent of the estimates of the fixed effects and having a distribution related to the \(\chi^{2}\) with some degrees of freedom. There is little reason to expect the independence and \(\chi^{2}\) properties to be true. In fact, to perform ad hoc \(F\) tests at all, we need to identify some number of degrees of freedom for \(\tilde{\theta}_{1}\).

#### Mathematical Results*

Eaton (1985) and Harville (1985) have given conditions under which the empirical estimates and predictors are unbiased and have variances at least as great as the corresponding BLUEs and BLUPs. This is one of those cases in which two people have developed very similar ideas simultaneously and independently. We follow Eaton's development. To estimate the variance of a plug-in estimate or predictor, the variance formula for the BLUE or BLUP has frequently been used, with \(V\) replaced by \(\tilde{V}\). Under mild conditions, when \(\tilde{V}\) is unbiased for \(V\), the expected value of this estimated variance is less than or equal to the variance of the BLUE or BLUP (which in turn is less than or equal to the true variance of the plug-in estimator or predictor). These results establish a theoretical basis for the often-observed phenomenon that these estimated variances for plug-in estimators (predictors) are often misleadingly small. Although Eaton's results do not explicitly use any parameterization for \(V\), it is typically the case that the covariance matrix depends on a parameter vector \(\theta\), that is, \(V=V(\theta)\), and that the estimate of \(V(\theta)\) is \(\tilde{V}=V(\tilde{\theta})\), where \(\tilde{\theta}\) is an estimate of \(\theta\).

The first results on the unbiasedness of plug-in estimates and predictors are apparently due to Kackar and Harville (1981). Other results on improved variance estimation for plug-in predictors are given by Kackar and Harville (1984), Harville and Jeske (1992) and Zimmerman and Cressie (1992).

**Definition 4.7.1.** \(\tilde{B}(Y)\) is a residual type statistic if

\[\tilde{B}(Y)=\tilde{B}(Y-X\beta)\qquad\hbox{for any}\quad\beta \tag{4.7.1}\]

and

\[\tilde{B}(Y)=\tilde{B}(-Y)\,. \tag{4.7.2}\]

Note that any residual type statistic has \(\tilde{B}(Y)=\tilde{B}(Y-X\hat{\beta})=\tilde{B}\bigl{(}(I-A)Y\bigr{)}\), so residual type statistics can be viewed as functions of the residual vector.

In the following discussion, we will assume that \(\tilde{V}\) and \(\tilde{V}_{y0}\) are residual type statistics. Most standard methods for estimating covariance matrices satisfy the conditions of Definition 4.7.1. Clearly, if \(\tilde{V}\) and \(\tilde{V}_{y0}\) are residual type statistics, any functions of them are also of the residual type. In particular, functions such as \(\tilde{V}^{-1}\),

\[\tilde{A}=X(X^{\prime}\tilde{V}^{-1}X)^{-}X^{\prime}\tilde{V}^{-1}\,,\]

and

\[\tilde{\delta}=\tilde{V}^{-1}\tilde{V}_{y0}\]

are residual type statistics.

**Exercise 4.8.**  Show that solving the likelihood or REML equations yield covariance matrix estimates that are residual type statistics.

The key result in establishing that plug-in estimators are unbiased is the following proposition.

**Proposition 4.7.2.**  If \(e\) and \(-e\) have the same distribution and if \(\tilde{B}(Y)\) is a residual type statistic, then

\[{\rm E}[\tilde{B}(Y)Y]={\rm E}[\tilde{B}(Y)X\beta]\,.\]

Proof.

\[{\rm E}[\tilde{B}(Y)Y]={\rm E}[\tilde{B}(Y)X\beta]+{\rm E}[\tilde{B}(Y)e]\,.\]

It suffices to show that \({\rm E}[\tilde{B}(Y)e]=0\). By Definition 4.7.1,\[\tilde{B}(Y)=\tilde{B}(Y-X\beta)=\tilde{B}(e),\]

and by the symmetry property of \(e\) assumed in the proposition,

\[\mathrm{E}[\tilde{B}(e)e]=-\mathrm{E}[\tilde{B}(-e)e]=-\mathrm{E}[\tilde{B}(e)e].\]

The only way a real vector can equal its negative is if the vector is zero, thus completing the proof. 

Henceforth assume that \(e\) and \(-e\) have the same distribution.

Proposition 4.7.2 leads immediately to two results on unbiased estimation.

**Corollary 4.7.3**.: \(\mathrm{E}[X\tilde{\beta}]=X\beta\)_._

Proof. By definition, \(X\tilde{\beta}=\tilde{A}Y\), where \(\tilde{A}\) is a residual type statistic, so by Proposition 4.7.2, \(\mathrm{E}[X\tilde{\beta}]=\mathrm{E}[\tilde{A}X\beta]\). Because \(\tilde{A}\) is a projection operator onto \(C(X)\) for any \(\tilde{V}\), \(\mathrm{E}[\tilde{A}X\beta]=\mathrm{E}[X\beta]=X\beta\). 

**Corollary 4.7.4**.: \(\mathrm{If}\ \lambda^{\prime}\beta\) is estimable, then

\[\mathrm{E}[\lambda^{\prime}\tilde{\beta}]=\lambda^{\prime}\beta\.\]

Proof. By estimability, \(\lambda^{\prime}=\rho^{\prime}X\) and

\[\mathrm{E}[\lambda^{\prime}\tilde{\beta}]=\mathrm{E}[\rho^{\prime}X\tilde{ \beta}]=\rho^{\prime}\mathrm{E}[X\tilde{\beta}]=\rho^{\prime}X\beta=\lambda^{ \prime}\beta\.\]

Now, consider the prediction problem. We seek to predict \(y_{0}\), where \(\mathrm{E}(y_{0})=x_{0}^{\prime}\beta\) and \(x_{0}^{\prime}\beta\) is estimable. The plug-in predictor is

\[\tilde{y}_{0} = x_{0}^{\prime}\tilde{\beta}+\tilde{\delta}^{\prime}(Y-X\tilde{ \beta})\] \[= x_{0}^{\prime}\tilde{\beta}+\tilde{\delta}^{\prime}(I-\tilde{A})Y,\]

where \(\tilde{\delta}=\tilde{V}^{-1}\tilde{V}_{y0}\) is a residual type statistic. Before proving that the plug-in predictor is unbiased, we establish another result.

**Lemma 4.7.5**.: \(\mathrm{E}[\tilde{\delta}^{\prime}(I-\tilde{A})Y]=0\)_._

Proof. Because \(\tilde{\delta}\) and \((I-\tilde{A})\) are residual type statistics, \(\tilde{\delta}^{\prime}(I-\tilde{A})\) is also of residual type. Applying Proposition 4.7.2 and using the fact that for each \(Y\), \(\tilde{A}\) is a projection operator onto \(C(X)\),

\[\mathrm{E}[\tilde{\delta}^{\prime}(I-\tilde{A})Y]=\mathrm{E}[\tilde{\delta}^ {\prime}(I-\tilde{A})X\beta]=0.\]The plug-in predictor is unbiased.

**Proposition 4.7.6**: \(\mathrm{E}[\tilde{y}_{0}]=x^{\prime}_{0}\beta=\mathrm{E}[y_{0}]\)_._

Proof.  By Corollary 4.7.4 and Lemma 4.7.5

\[\mathrm{E}[\tilde{y}_{0}]=\mathrm{E}[x^{\prime}_{0}\hat{\beta}+\tilde{\delta}^{ \prime}(I-\bar{A})Y]=\mathrm{E}[x^{\prime}_{0}\hat{\beta}]+\mathrm{E}[\tilde{ \delta}(I-\bar{A})Y]=x^{\prime}_{0}\beta\,.\]

The next two propositions establish conditions under which the variance of the plug-in estimate and the prediction variance of the plug-in predictor are known to be no less than the variance of the BLUE and the BLUP. After proving the results, a brief discussion of the conditions necessary for the results will be given.

**Proposition 4.7.7**: \(\mathrm{If}\;\mathrm{E}[Ae|(I-A)e]=0\) _and \(\lambda^{\prime}=\rho^{\prime}X\), then_

\[\mathrm{Var}[\lambda^{\prime}\tilde{\beta}]=\mathrm{Var}[\lambda^{\prime}\hat {\beta}]+\mathrm{Var}[\lambda^{\prime}\tilde{\beta}-\lambda^{\prime}\hat{ \beta}].\]

Proof.

\[\mathrm{Var}(\lambda^{\prime}\tilde{\beta}) = \mathrm{Var}(\lambda^{\prime}\tilde{\beta}-\lambda^{\prime}\hat{ \beta}+\lambda^{\prime}\hat{\beta})\] \[= \mathrm{Var}(\lambda^{\prime}\hat{\beta})+\mathrm{Var}(\lambda^{ \prime}\tilde{\beta}-\lambda^{\prime}\hat{\beta})+2\mathrm{Cov}\left(\lambda^ {\prime}\hat{\beta},\lambda^{\prime}(\tilde{\beta}-\hat{\beta})\right),\]

so it suffices to show that

\[\mathrm{Cov}\left(\lambda^{\prime}\hat{\beta},\lambda^{\prime}(\tilde{\beta}- \hat{\beta})\right)=0\,.\]

Because \(\mathrm{E}[\lambda^{\prime}\tilde{\beta}-\lambda^{\prime}\hat{\beta}]=0\), it is enough to show that

\[\mathrm{E}[\lambda^{\prime}\hat{\beta}\{\lambda^{\prime}\tilde{\beta}-\lambda^ {\prime}\hat{\beta}\}]=0\,. \tag{4.7.3}\]

Now, observe that because \(AY\in C(X)\) and \(\tilde{A}\) is a projection operator onto \(C(X)\),

\[X\tilde{\beta}=\tilde{A}Y=\tilde{A}[AY+(I-A)Y]=AY+\tilde{A}(I-A)Y\,.\]

Hence,

\[\lambda^{\prime}\hat{\beta}=\rho^{\prime}X\tilde{\beta}=\rho^{\prime}AY+\rho^ {\prime}\tilde{A}(I-A)Y=\lambda^{\prime}\hat{\beta}+\rho^{\prime}\tilde{A}(I- A)Y\]

and

\[\lambda^{\prime}\tilde{\beta}-\lambda^{\prime}\hat{\beta}=\rho^{\prime}\tilde {A}(I-A)Y\,.\]

Thus, (4.7.3) is equivalent to

\[\mathrm{E}[\lambda^{\prime}\hat{\beta}\{\rho^{\prime}\tilde{A}(I-A)Y\}]=0\,.\]

[MISSING_PAGE_EMPTY:9477]

\[=x_{0}^{\prime}\hat{\beta}+\tilde{b}^{\prime}(I-A)Y\] \[=x_{0}^{\prime}\hat{\beta}+\delta^{\prime}(I-A)Y-\delta^{\prime}(I-A )Y+\tilde{b}^{\prime}(I-A)Y\] \[=\hat{y}_{0}+(\tilde{b}-\delta)^{\prime}(I-A)Y\,,\]

and

\[\hat{y}_{0}-\tilde{y}_{0}=(\delta-\tilde{b})^{\prime}(I-A)Y,\]

which is a function of \((I-A)Y\) because \(\tilde{b}\) is a residual type statistic.

As in the previous proof, evaluate the conditional expectation. This gives

\[\mbox{E}[(y_{0}-\hat{y}_{0})(\hat{y}_{0}-\tilde{y}_{0})|(I-A)Y]=(\hat{y}_{0}- \tilde{y}_{0})\mbox{E}[(y_{0}-\hat{y}_{0})|(I-A)Y]=0\]

by assumption and the fact that \((I-A)Y=(I-A)e\). Because the conditional expectation is zero for all \((I-A)Y\), the unconditional expectation is zero and the result is proven. 

Proposition 4.7.7 shows that the variance of the plug-in estimator equals the BLUE variance plus a nonnegative quantity. Thus, the plug-in variance is at least as large as the BLUE variance. Proposition 4.7.8 gives a similar result for prediction.

Eaton (1985) discusses situations under which the conditions \(\mbox{E}[Ae|(I-A)e]=0\) and \(\mbox{E}[y_{0}-\hat{y}_{0}](I-A)e]=0\) hold. In particular, the first condition holds if \(e\) has an elliptical distribution, and the second condition holds if \((e_{0},e^{\prime})\) has an elliptical distribution. Elliptical distributions can be generated as follows. \(Y\) has an elliptical distribution centered at \(0\) if \(Y\) has a density that is proportional to \(\varphi(y^{\prime}By)\), where \(B\) is a positive definite matrix and \(\varphi(u)\) is a density on the nonnegative real numbers. From \(PA\) Sect. 1.3, we see that if \(z_{i}\sim N(0,1)\), we get the multivariate normal distribution as a special case of elliptical distributions.

Eaton's last results involve concave functions and Jensen's inequality. A set \(\zeta\) is convex if, for any \(\alpha\in[0,1]\) and \(s_{1},s_{2}\in\zeta\), the point \(\alpha s_{1}+(1-\alpha)s_{2}\in\zeta\). A function \(\Psi:\zeta\rightarrow\mathbf{R}\) is concave if \(\Psi(\alpha s_{1}+(1-\alpha)s_{2})\geq\alpha\Psi(s_{1})+(1-\alpha)\Psi(s_{2})\). Finally, if \(s\) is random and defined on \(\zeta\), Jensen's inequality states that \(\Psi(\mbox{E}[s])\geq\mbox{E}[\Psi(s)]\). See Ferguson (1967) for a more complete discussion of convexity and a proof of Jensen's inequality.

Let \(\zeta\) be the set of all positive definite matrices \(V\), and observe that \(\zeta\) is a convex set. Let \(\mathscr{P}\) be the set of all matrices that are projection operators onto spaces that contain \(C(X)\), namely

\[\mathscr{P}=\{P|PP=P\mbox{ and }C(X)\subset C(P)\}\,.\]

For any \(\rho\in\mathbf{R}^{n}\) and each \(P\in\mathscr{P}\) define

\[\Psi_{P}(V)=\rho^{\prime}PVP^{\prime}\rho\,.\]

Observe that \(\Psi_{P}(V)\) is a concave function. To see this, note that

\[\Psi_{P}(\alpha V_{1}+(1-\alpha)V_{2})=\rho^{\prime}P\{\alpha V_{1}+(1-\alpha )V_{2}\}P^{\prime}\rho\]\[= \alpha\rho^{\prime}PV_{1}P^{\prime}\rho+(1-\alpha)\rho^{\prime}PV_{2}P^ {\prime}\rho\] \[= \alpha\Psi_{P}(V_{1})+(1-\alpha)\Psi_{P}(V_{2})\,.\]

For any \(P\), \(\rho^{\prime}PY\) is a linear unbiased estimate of \(\rho^{\prime}X\beta\), so the variance of the BLUE, \(\rho^{\prime}AY\), is at least as small as the variance of \(\rho^{\prime}PY\), that is,

\[\rho^{\prime}AVA^{\prime}\rho=\inf_{P\in\mathcal{P}}\rho^{\prime}PVP^{\prime} \rho\,.\]

Define

\[\Psi(V)\equiv\rho^{\prime}AVA^{\prime}\rho,\]

so we see that

\[\Psi(V)=\inf_{P\in\mathcal{P}}\Psi_{P}(V)\,.\]

The infinum of a set of concave functions is also concave, so \(\rho^{\prime}AVA^{\prime}\rho\) is concave. (Note that a direct proof of concavity is difficult because \(A\) is a function of \(V\).)

**Exercise 4.9.**  Show that if \(f_{\lambda}(x)\) is concave for any \(\lambda\in\Lambda\), then \(f(x)=\inf_{\lambda\in\Lambda}f_{\lambda}(x)\) is also concave. Hint: By definition, for any point \(x_{0}\) and any \(\varepsilon>0\), there exists \(\lambda\) such that \(f(x_{0})\geq f_{\lambda}(x_{0})-\varepsilon\).

The estimated variance of the plug-in estimator is \(\rho^{\prime}\tilde{A}\tilde{V}\tilde{A}^{\prime}\rho\). Define

\[\widetilde{\mathrm{Var}}(\rho^{\prime}X\tilde{\beta})\equiv\rho^{\prime}\tilde {A}\tilde{V}\tilde{A}^{\prime}\rho\,.\]

Recalling that

\[\mathrm{Var}(\rho^{\prime}X\hat{\beta})=\rho^{\prime}AVA^{\prime}\rho\]

and that by Proposition 4.7.7

\[\mathrm{Var}(\rho^{\prime}X\hat{\beta})\leq\mathrm{Var}(\rho^{\prime}X\tilde{ \beta})\,,\]

we can prove the following.

**Proposition 4.7.9.**  If \(\tilde{V}\) is unbiased for \(V\) and if Proposition 4.7.7 holds, then

\[\mathrm{E}[\widetilde{\mathrm{Var}}(\rho^{\prime}X\tilde{\beta})]\leq\mathrm{ Var}(\rho^{\prime}X\hat{\beta})\leq\mathrm{Var}(\rho^{\prime}X\tilde{\beta})\,.\]

Proof.  We need only prove that

\[\mathrm{E}[\rho^{\prime}\tilde{A}\tilde{V}\tilde{A}^{\prime}\rho]\leq\rho^{ \prime}AVA^{\prime}\rho\,.\]

Because \(\Psi(V)\) is concave, Jensen's inequality gives

\[\rho^{\prime}AVA^{\prime}\rho=\Psi(V)=\Psi(\mathrm{E}[\tilde{V}])\geq\mathrm{ E}[\Psi(\tilde{V})]=\mathrm{E}[\rho^{\prime}\tilde{A}\tilde{V}\tilde{A}^{\prime} \rho]\,.\]Thus, for an unbiased covariance matrix estimate \(\tilde{V}\), the expected value of the estimated variance of \(\rho^{\prime}X\tilde{\beta}\) is no greater than the variance of \(\rho^{\prime}X\hat{\beta}\) while the true variance of \(\rho^{\prime}X\tilde{\beta}\) is no less than the variance of \(\rho^{\prime}X\hat{\beta}\). This establishes that there is a tendency for the estimated variance of \(\rho^{\prime}X\tilde{\beta}\) to underestimate the true variance and illustrates how the underestimation could be very substantial.

The result for prediction follows similarly. Consider the set

\[\mathcal{D}=\{d|d^{\prime}X=x_{0}^{\prime}\}.\]

Then, any linear unbiased predictor can be written as \(d^{\prime}Y\) for some \(d\in\mathcal{D}\). Let \(V_{00}=\text{Var}(y_{0})\),

\[V_{pred}=\begin{bmatrix}V_{00}&V_{0y}\\ V_{y0}&V\end{bmatrix},\]

and take \(\zeta\) as the set of all positive definite \(V_{pred}\). Define

\[\Psi_{d}(V_{pred})\equiv\text{Var}(y_{0}-d^{\prime}Y)=V_{00}-2d^{\prime}V_{y0 }+d^{\prime}Vd\,.\]

It is easily seen that

\[\Psi_{d}(\alpha V_{pred1}+(1-\alpha)V_{pred2})=\alpha\Psi_{d}(V_{pred1})+(1- \alpha)\Psi_{d}(V_{pred2}),\]

so \(\Psi_{d}(V_{pred})\) is concave. Define

\[\Psi(V_{pred})\equiv\inf_{d\in\mathcal{D}}\Psi_{d}(V_{pred})\,.\]

Note that \(\Psi(V_{pred})\) is concave and

\[\Psi(V_{pred})=\text{Var}(y_{0}-b^{\prime}Y)=\text{Var}(y_{0}-\hat{y}_{0})\,.\]

Once again, for \(\tilde{V}_{pred}\) unbiased and writing (4.1.7) with \(\tilde{V}_{pred}\) substituted for \(V_{pred}\) as \(\widetilde{\text{Var}}(y_{0}-\tilde{y}_{0})\), Jensen's inequality gives

\[\text{E}[\widetilde{\text{Var}}(y_{0}-\tilde{y}_{0})]=\text{E}[\Psi(\tilde{V} _{pred})]\leq\Psi(\text{E}[\tilde{V}_{pred}])=\Psi(V_{pred})=\text{Var}(y_{0} -\hat{y}_{0})\,.\]

We have proved the following proposition.

**Proposition 4.7.10.**  If \(\tilde{V}_{pred}\) is unbiased for \(V_{pred}\) and if Proposition 4.7.8 holds, then

\[\text{E}[\widetilde{\text{Var}}(y_{0}-\tilde{y}_{0})]\leq\text{Var}(y_{0}- \hat{y}_{0})\leq\text{Var}(y_{0}-\tilde{y}_{0})\,.\]

Note that these results depend on the assumption that the procedure used for estimating \(V_{pred}\) does not depend on the true value of \(\beta\). Residual type estimators take on the same value when based on \(Y\) or on \(e=Y-X\beta\). (The fact that \(e\) is unobservable is irrelevant to our argument.) Because the distribution of \(e\) does not depend on \(\beta\), neither does the distribution of \(\tilde{V}_{pred}(Y)\). For example, if \(V_{pred}(\theta)\) is the covariance matrix for observations from a process with a stationary covariance function, then clearly, because the covariance function does not depend on \(\beta\), residual type estimators for \(V_{pred}(\theta)\) are reasonable. However, situations exist in which residual type estimators are not reasonable.

A common problem in regression analysis is the presence of heteroscedastic errors; see _PA-V_ Sect. 12.4 (Christensen 2011, Section 13.4). To deal with this problem, one often assumes a model

\[V(\theta)=[V_{ij}(\theta)],\]

where

\[V_{ij}(\theta)=0,\quad i\neq j,\]

and

\[V_{ii}(\theta)=h_{i}(\alpha,\beta).\]

Here,

\[\theta^{\prime}=(\alpha^{\prime},\beta^{\prime}).\]

Two common choices for the function \(h_{i}\) take \(\alpha\) as a scalar and

\[h_{i}(\alpha,\beta)=\alpha(x_{i}^{\prime}\beta)^{2}\]

or

\[h_{i}(\alpha,\beta)=\alpha x_{i}^{\prime}\beta;\]

see Carroll and Ruppert (1988).

When the variance function depends on \(\beta\), it is counter intuitive to use residual type estimation procedures. In particular, MLEs will not be residual type estimates for these variance functions. For normal errors, van Houwelingen (1988) has established that the variance of the optimal weighted least squares estimate \(\hat{\beta}\) (based on known variances) is at least as great as the asymptotic variance of \(\beta^{*}-\beta\), where \(\beta^{*}\) is the MLE of \(\beta\). (Actually, the proper comparison is between \(\sqrt{n}\hat{\beta}\) and \(\sqrt{n}(\beta^{*}-\beta)\), so that both quantities have nontrivial asymptotic distributions.) This remarkable result occurs because there may be extra information to be gained about \(\beta\) from the variability of the observations. Moreover, van Houwelingen also established that for such variance models the MLE \(\beta^{*}\)_may_ not even be consistent.

## References

* Carroll & Ruppert (1988) Carroll, R. J., & Ruppert, D. (1988). _Transformations and weighting in regression_. New York, NY: Chapman and Hall.
* Christensen (1987) Christensen, R. (1987). The analysis of two-stage sampling data by ordinary least squares. _Journal of the American Statistical Association, 82_, 492-498.
* Christensen (1993) Christensen, R. (1993). Quadratic covariance estimation and equivalence of predictions. _Mathematical Geology, 25_, 541-558.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York, NY: Springer.

Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton, FL: Chapman and Hall/CRC Press.
* Christensen & Sun (2010) Christensen, R., & Sun, S. K. (2010). Alternative goodness-of-fit tests for linear models. _Journal of the American Statistical Association,__105_, 291-301.
* Christensen & Lin (2015) Christensen, R., & Lin, Y. (2015). Lack-of-fit tests based on partial sums of residuals. _Communications in Statistics: Theory and Methods,__44_, 2862-2880.
* Cressie (1993) Cressie, N. A. C. (1993). _Statistics for spatial data_ (rev. ed.). New York, NY: Wiley.
* Eaton (1985) Eaton, M. L. (1985). The Gauss-Markov theorem in multivariate analysis. In P. R. Krishnaiah _Multivariate analysis--VI_. Amsterdam: Elsevier.
* Ferguson (1967) Ferguson, T. S. (1967). _Mathematical statistics: A decision theoretic approach_. New York, NY: Academic Press.
* Ferguson (1996) Ferguson, T. S. (1996). _A Course in large sample theory_. Boca Raton, FL: Chapman & Hall/CRC.
* Harville (1985) Harville, D. A. (1985). Decomposition of prediction error. _Journal of the American Statistical Association,__80_, 132-138.
* Harville & Jeske (1992) Harville, D. A., & Jeske, D. R. (1992). Mean squared error of estimation or prediction under a general linear model. _Journal of the American Statistical Association,__87_, 724-731.
* Kackar & Harville (1981) Kackar, R. N., & Harville, D. A. (1981). Unbiasedness of two-stage estimation and prediction procedures for mixed linear models. _Communications in Statistics: Theory and Methods,__A10_, 1249-1261.
* Kackar & Harville (1984) Kackar, R. N., & Harville, D. A. (1984). Approximations for standard errors of estimators of fixed and random effects in mixed linear models. _Journal of the American Statistical Association,__79_, 853-862.
* Kenward & Roger (1997) Kenward, M. G., & Roger, J. H. (1997). Small sample inference for fixed effects from restricted maximum likelihood, _Biometrics,__53_, 983-997.
* Kenward & Roger (2009) Kenward, M. G., Roger, J. H. (2009). An improved approximation to the precision of fixed effects from restricted maximum likelihood. _Computational Statistics and Data Analysis,__53_, 2583-2595.
* Lavine et al. (2015) Lavine, M., Bray, A., Hodges, J., & Hodges, J. (2015). Approximately exact calculations for linear mixed models. _Electronic Journal of Statistics,__9_, 2293-2323.
* Patterson & Thompson (1974) Patterson, H. D., & Thompson, R. (1974). Maximum likelihood estimation of variance components. In _Proceedings of the 8th International Biometric Conference_ (pp. 197-207).
* Satterthwaite (1946) Satterthwaite, F. E. (1946). An approximate distribution of estimates of variance components. _Biometrics,__2_, 110-114.
* Searle (1971) Searle, S. R. (1971). _Linear models_. New York, NY: Wiley.
* Searle et al. (1992) Searle, S. R., Casella, G., & McCulloch, C. (1992). _Variance components_. New York, NY: Wiley.
* Tarpey et al. (2014) Tarpey, T., Ogden, R. T., Petkova, E., & Christensen, R. (2014). A paradoxical result in estimating regression coefficients. _The American Statistician,__68_, 271-276.
* Tarpey et al. (2015) Tarpey, T., Ogden, R., Petkova, E., & Christensen, R. (2015). Reply. _The American Statistician,__69_, 254-255.

* van Houwelingen (1988) van Houwelingen, J. C. (1988). Use and abuse of variance models in regression. _Biometrics,__44_, 1073-1081.
* Zimmerman & Cressie (1992) Zimmerman, D. L., & Cressie, N. (1992). Mean squared prediction error in the spatial linear model with estimated covariance parameters. _Annals of the Institute of Statistical Mathematics,__44_, 27-43.

## Chapter Mixed Models and Variance Components

**Abstract** This chapter particularizes the results of Chap. 4 for linear mixed models with special emphasis on variance component models and a particular longitudinal data model.

Traditionally, linear models have been divided into three categories: _fixed effects models_, _random effects models_, and _mixed models_. The categorization depends on whether the \(\beta\) vector in \(Y=X\beta+e\) is fixed, random, or has both fixed and random elements. Random effects models always assume that there is a fixed overall mean for the observations, so random effects models are actually mixed.

_Variance components_ are the variances of the random elements of \(\beta\) (particularly when the random elements are uncorrelated). Sections 5.1 and 5.2 discuss mixed models in general and prediction for mixed models. Section 5.3 introduces partitioning of the random effects vector and an associated variance component model as well as a particular longitudinal mixed model. Both of these special cases are examples in which the random effects have a linear covariance structure. A linear covariance structure for the random effects leads to a linear covariance structure for the data as discussed in Sect. 4.4. Traditionally, most mixed models have used linear covariance structures. The primary current exception is for some of the longitudinal models discussed in Chap. 11. Sections 5.4 and 5.5 discuss in more detail the variance component model and the longitudinal model. Section 5.6 introduces a method of unbiased estimation for variance components based on least squares fitting. Section 5.7 examines exact tests for variance components.

Searle, Casella, and McCulloch (1992) give an extensive discussion of variance component estimation. Khuri, Mathew, and Sinha (1998) give an extensive discussion of testing in mixed models. More recent books include Jiang (2007), McCulloch, Searle, and Neuhaus (2008), and Hodges (2014).

The methods considered in this chapter are presented in terms of fitting general linear models. In many special cases, considerable simplification results. For example, the RCB models of _PA_ (11.1.5) and Exercise 11.4, the split plot model of _PA_ (11.3.1), and the subsampling model of _PA_ (11.4.2) are all mixed models with very special structures.

### Mixed Models

The _mixed model_ is a linear model in which some of the parameters, instead of being fixed effects, are random. The model can be written

\[Y=X\beta+Z\gamma+e,\qquad\mathrm{E}(e)=0, \tag{5.1.1}\]

where \(X\) and \(Z\) are known matrices with \(p\) and \(q\) columns respectively, \(\beta\) is an unobservable vector of fixed effects, and \(\gamma\) is an unobservable vector of random effects with \(\mathrm{E}(\gamma)=0\), \(\mathrm{Cov}(\gamma)=D\), and \(\mathrm{Cov}(\gamma,e)=0\). Let \(\mathrm{Cov}(e)=R\). Typically, \(D\) and \(R\) depend on some vector of parameters, say \(\theta\), and typically they depend on different subsets of the parameters.

Technically, mixed models are neither more nor less general than the models of Chap. 4. The mixed model is a special case of the Chap. 4 model in which the error term is all of \(Z\gamma+e\). On the other hand, the Chap. 4 model is a special case of the mixed model with \(\mathrm{Pr}[e=0]=1\), \(Z=I_{n}\), and \(D(\theta)=V(\theta)\). However, the mixed model lends itself to numerous useful applications.

#### 5.1.1 One-Way ANOVA.

Let \(y_{ij}=\mu+\alpha_{i}+e_{ij}\), \(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\), with the \(\alpha_{i}\)s independent \(N(0,\sigma_{1}^{2})\), the \(e_{ij}\)s independent \(N(0,\sigma_{0}^{2})\), and the \(\alpha_{i}\)s and \(e_{ij}\)s independent. Here \(p=1\), \(\beta=\mu\), \(q=a\), \(\gamma=(\alpha_{1},\ldots,\alpha_{a})\), \(D=\sigma_{1}^{2}I_{a}\) and \(R=\sigma_{0}^{2}I_{n}\). Since the \(\alpha_{i}\)s are known to have mean 0, if we could observe the \(\alpha_{i}\)s, a natural estimate of \(\sigma_{1}^{2}\) would be \(\sum_{i=1}^{a}\alpha_{i}^{2}/a\). Note that the only way you can get a good estimate of \(\sigma_{1}^{2}\) is to have \(a\) large. No method, not maximum likelihood, not REML, not MINQUE; no method will produce good estimates of \(\sigma_{1}^{2}\) if \(a\) is small.

#### 5.1.2 Random Intercepts Model.

Consider the one-way ACOVA model \(y_{ij}=\alpha_{i}+x_{ij}^{\prime}\beta+e_{ij}\), \(i=1,\ldots,a\equiv q\), \(j=1,\ldots,N_{i}\), but with intercepts \(\alpha_{i}\) independent \(N(0,\sigma_{1}^{2})\). As usual, take the \(e_{ij}\)s independent \(N(0,\sigma_{0}^{2})\), but we also take the \(\alpha_{i}\)s and \(e_{ij}\)s independent. This model would be appropriate if we viewed each group \(i\) as a sample from some larger population rather than being of special importance in itself. The groups could be randomly chosen individuals on whom \(N_{i}\) observations are taken, randomly chosen classrooms from which \(N_{i}\) students are measured, or different lots of raw material for an industrial process. Typically, in this model the fixed effects term \(x_{ij}^{\prime}\beta\) would contain an intercept (grand mean).

Consider \(a=3\) and \((N_{1},N_{2},N_{3})=(3,2,3)\). Write a model \(Y=X\beta+Z\gamma+e\) with\[Y=\begin{bmatrix}y_{11}\\ y_{12}\\ y_{13}\\ y_{21}\\ y_{22}\\ y_{31}\\ y_{32}\\ y_{33}\end{bmatrix},\quad X=\begin{bmatrix}x_{11}^{\prime}\\ x_{12}^{\prime}\\ x_{13}^{\prime}\\ x_{11}^{\prime}\\ x_{21}^{\prime}\\ x_{22}^{\prime}\\ x_{31}^{\prime}\\ x_{32}^{\prime}\\ x_{33}^{\prime}\end{bmatrix},\quad Z=\begin{bmatrix}1&0&0\\ 1&0&0\\ 1&0&0\\ 0&1&0\\ 0&0&1\\ 0&0&1\\ 0&0&1\\ 0&0&1\end{bmatrix},\]

\[\gamma=(\alpha_{1},\alpha_{2},\alpha_{3})^{\prime},\]

\(q=3\), \(D=\sigma_{1}^{2}I_{3}\), \(n=8\), \(R=\sigma_{0}^{2}I_{8}\), and \(\theta^{\prime}=(\sigma_{0}^{2},\sigma_{1}^{2})\).

If the \(\alpha_{i}\)s are fixed effects, it does not matter whether an overall intercept term is included in the model, but with random \(\alpha_{i}\)s having mean 0, one typically wants a fixed intercept or, more generally, \(J\in C(X)\). 

For estimation of the fixed effects \(\beta\), the mixed model can be written as a general Gauss-Markov model. Write

\[V=\operatorname{Cov}(Y)=\operatorname{Cov}(Z\gamma+e)=ZDZ^{\prime}+R\]

or, more accurately,

\[V(\theta)=\operatorname{Cov}(Y)=\operatorname{Cov}(Z\gamma+e)=ZD(\theta)Z^{ \prime}+R(\theta).\]

\(V\) is assumed to be nonsingular. Model (5.1.1) is equivalent to

\[Y=X\beta+\xi,\quad\operatorname{E}(\xi)=0,\quad\operatorname{Cov}(\xi)=V.\]

The BLUE of \(X\beta\) can be found using the theory for general Gauss-Markov models. Unfortunately, finding the BLUE requires knowledge (at least up to a constant multiple) of \(V\). This is rarely available. The best procedure available for estimating \(X\beta\) is to estimate \(V\) and then act as if the estimate is the real value of \(V\). In other words, if \(V\) is estimated with \(\tilde{V}\), then \(X\beta\) is estimated with

\[X\tilde{\beta}=X\left[X^{\prime}\tilde{V}^{-1}X\right]^{-}X^{\prime}\tilde{V} ^{-1}Y. \tag{5.1.2}\]

If \(\tilde{V}\) is close to \(V\), then the estimate \(X\tilde{\beta}\) should be close to the BLUE, \(X\hat{\beta}\). Corresponding standard errors tend to be underestimated, cf. Sects. 4.1 and 4.7.

It is important to be able to invert \(V\) in order to perform estimation and prediction. \(V\) can be hard to invert because its dimension \(n\), the number of data points, is often large. Inverting \(V\) can be performed indirectly. Often \(R\) is chosen so that it is easy to invert and \(D\), being \(q\times q\) rather than \(n\times n\), is easier to invert than \(V\). Not infrequently, \(D\) is diagonal or block diagonal which makes it easier to invert. By \(PA\) Proposition B.56,

\[V^{-1}=R^{-1}-R^{-1}Z[D^{-1}+Z^{\prime}R^{-1}Z]^{-1}Z^{\prime}R^{-1}. \tag{5.1.3}\]Note that a second \(q\times q\) inverse is required in this formula. Frequently people assume \(R=\sigma_{0}^{2}I_{n}\), in which case

\[V^{-1}=\frac{1}{\sigma_{0}^{2}}\left\{I-Z[\sigma_{0}^{2}D^{-1}+Z^{\prime}Z]^{-1} Z^{\prime}\right\}. \tag{5.1.4}\]

If \(\gamma\) were a fixed effect, we would be interested in estimating estimable functions like \(\lambda^{\prime}\gamma\). Since \(\gamma\) is random, we cannot estimate \(\lambda^{\prime}\gamma\), but we can predict \(\lambda^{\prime}\gamma\) from the observable vector \(Y\). As discussed in Sect. 4.1.3, a best linear predictor requires knowledge of \(\mathrm{E}(\lambda^{\prime}\gamma)\), \(\mathrm{Cov}(\lambda^{\prime}\gamma,Y)\), \(\mathrm{E}(Y)\), and \(\mathrm{Cov}(Y)\). We know that \(\mathrm{E}(\lambda^{\prime}\gamma)=0\), \(\mathrm{Cov}(\lambda^{\prime}\gamma,Y)=\mathrm{Cov}(\lambda^{\prime}\gamma,Z \gamma+e)=\lambda^{\prime}DZ^{\prime}\), \(\mathrm{E}(Y)=X\beta\), and \(\mathrm{Cov}(Y)=V\). If we assume that \(D\), \(R\), and thus \(V\) are known,

\[\lambda^{\prime}\hat{\gamma}=\lambda^{\prime}\left[0+DZ^{\prime}V^{-1}(Y-X \beta)\right] \tag{5.1.5}\]

and our only problem in predicting \(\lambda^{\prime}\gamma\) is not knowing \(\mathrm{E}(Y)=X\beta\). We could then replace \(\beta\) with a generalized least squares estimate \(\hat{\beta}\) to get the _best linear unbiased predictor (BLUP)_ of \(\lambda^{\prime}\gamma\). Alas, \(D\) and \(R\) are not known, but with good estimates of \(D\) and \(R\), we can get empirical predictions of \(\lambda^{\prime}\gamma\) that are close to the BLUP.

Sometimes we would like to obtain an estimate of \(X\beta\) without going to the trouble and expense of finding \(\tilde{V}^{-1}\), which is needed to apply Eq. (5.1.2). One simple estimate is the least squares estimate, \(MY\). This gives an unbiased estimate of \(X\beta\), but ignores the existence of the random effects. An alternative method is to fit the model

\[Y=X\beta+Z\delta+Z\gamma+e,\quad R=\sigma_{0}^{2}I, \tag{5.1.6}\]

where \(\delta\) is a vector of fixed effects corresponding to the vector of random effects \(\gamma\). In model (5.1.6), there is no hope of estimating the covariance parameters in \(D\) because the fixed effects \(\delta\) and the random effects \(\gamma\) are completely confounded, see Exercise 5.9 at the end of the chapter. However, it is easily seen that \(C(V[X,Z])\subset C(X,Z)\), so by \(P\!A\) Proposition 2.7.5 or Theorem 10.4.5, least squares estimates are BLUEs in model (5.1.6). To see that \(C(V[X,Z])\subset C(X,Z)\), observe that

\[C(V[X,Z]) =C(\{\sigma_{0}^{2}I+ZDZ^{\prime}\}[X,Z])\] \[=C(\sigma_{0}^{2}X+ZDZ^{\prime}X,\sigma_{0}^{2}Z+ZDZ^{\prime}Z)\] \[\subset C(X,Z).\]

From \(P\!A\) Chap. 9, a least squares estimate of \(\beta\) is

\[\hat{\beta}=\left[X^{\prime}(I-M_{Z})X\right]^{-}X^{\prime}(I-M_{Z})Y, \tag{5.1.7}\]

where \(M_{Z}=Z(Z^{\prime}Z)^{-}Z^{\prime}\). Although estimates obtained using (5.1.7) are not typically BLUEs for model (5.1.1), since model (5.1.6) is a larger model than model (5.1.1), the estimates should be reasonable. The only serious problem with using Eq. (5.1.7) is that it is not clear which functions of \(\beta\) are estimable in model (5.1.6).

### Mixed Model Equations

We now develop the well-known _mixed model equations_. These equations are similar in spirit to normal equations, cf. _PA_ Sect. 2.8. However, the mixed model equations simultaneously provide BLUEs and BLUPs, cf. Sect. 4.1.

Suppose that \(R\) is nonsingular. If \(\gamma\) were not random, the normal equations for model (5.1.1) would be

\[\begin{bmatrix}X^{\prime}\\ Z^{\prime}\end{bmatrix}R^{-1}[X,Z]\begin{bmatrix}\beta\\ \gamma\end{bmatrix}=\begin{bmatrix}X^{\prime}\\ Z^{\prime}\end{bmatrix}R^{-1}Y\]

or

\[\begin{bmatrix}X^{\prime}R^{-1}X&X^{\prime}R^{-1}Z\\ Z^{\prime}R^{-1}X&Z^{\prime}R^{-1}Z\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}=\begin{bmatrix}X^{\prime}R^{-1}Y\\ Z^{\prime}R^{-1}Y\end{bmatrix}.\]

For \(D\) nonsingular, the mixed model equations are defined as

\[\begin{bmatrix}X^{\prime}R^{-1}X&X^{\prime}R^{-1}Z\\ Z^{\prime}R^{-1}X&D^{-1}+Z^{\prime}R^{-1}Z\end{bmatrix}\begin{bmatrix}\beta \\ \gamma\end{bmatrix}=\begin{bmatrix}X^{\prime}R^{-1}Y\\ Z^{\prime}R^{-1}Y\end{bmatrix}. \tag{5.2.1}\]

**Theorem 5.2.1**.: _If \([\hat{\beta}^{\prime},\hat{\gamma}^{\prime}]\) is a solution to the mixed model equations, then \(X\hat{\beta}\) is a BLUE of \(X\beta\) and \(\hat{\gamma}\) is a BLUP of \(\gamma\)._

Proof.: From _PA_ Sect. 2.8, \(X\hat{\beta}\) will be a BLUE of \(X\beta\) if \(\hat{\beta}\) is a solution to \(X^{\prime}V^{-1}X\beta=X^{\prime}V^{-1}Y\). To use this equation we need a form for \(V^{-1}\) in terms of \(Z\), \(D\), and \(R\), i.e., we need Eq. (5.1.3).

If \(\hat{\beta}\) and \(\hat{\gamma}\) are solutions, then the second row of the mixed model equations gives

\[Z^{\prime}R^{-1}X\hat{\beta}+\left[D^{-1}+Z^{\prime}R^{-1}Z\right]\hat{\gamma} =Z^{\prime}R^{-1}Y\]

or

\[\hat{\gamma}=\left[D^{-1}+Z^{\prime}R^{-1}Z\right]^{-1}Z^{\prime}R^{-1}(Y-X \hat{\beta}). \tag{5.2.2}\]

The first row of the equations is

\[X^{\prime}R^{-1}X\hat{\beta}+X^{\prime}R^{-1}Z\hat{\gamma}=X^{\prime}R^{-1}Y.\]

Substituting for \(\hat{\gamma}\) gives

\[X^{\prime}R^{-1}X\hat{\beta}+X^{\prime}R^{-1}Z\left[D^{-1}+Z^{\prime}R^{-1}Z \right]^{-1}Z^{\prime}R^{-1}(Y-X\hat{\beta})=X^{\prime}R^{-1}Y\]

or, putting \(\hat{\beta}\) on one side and \(Y\) on the other,

\[X^{\prime}R^{-1}X\hat{\beta}-X^{\prime}R^{-1}Z\left[D^{-1}+Z^{ \prime}R^{-1}Z\right]^{-1}Z^{\prime}R^{-1}X\hat{\beta}\\ =X^{\prime}R^{-1}Y-X^{\prime}R^{-1}Z\left[D^{-1}+Z^{\prime}R^{-1} Z\right]^{-1}Z^{\prime}R^{-1}Y,\]which, by Eq. (5.1.3), is \(X^{\prime}V^{-1}X\hat{\beta}=X^{\prime}V^{-1}Y\). Thus, \(\hat{\beta}\) is a generalized least squares solution and \(X\hat{\beta}\) is a BLUE.

\(\hat{\gamma}\) in (5.2.2) can be rewritten as

\[\hat{\gamma} = \left(D\left[D^{-1}+Z^{\prime}R^{-1}Z\right]-DZ^{\prime}R^{-1}Z \right)\left[D^{-1}+Z^{\prime}R^{-1}Z\right]^{-1}Z^{\prime}R^{-1}(Y-X\hat{ \beta})\] \[= \left(DZ^{\prime}R^{-1}-DZ^{\prime}R^{-1}Z\left[D^{-1}+Z^{\prime} R^{-1}Z\right]^{-1}Z^{\prime}R^{-1}\right)(Y-X\hat{\beta})\] \[= DZ^{\prime}V^{-1}(Y-X\hat{\beta}),\]

which is the BLUP of \(\gamma\). Equation (5.1.5) suggests that this is the BLUP but, more formally, see _PA-V_ Exercise 6.10 (Christensen 2011, Exercise 12.3) taking \(Q=V^{-1}ZD\). 

The mixed model equations' primary usefulness is that they are relatively easy to solve. Finding the solution to the generalized normal equations

\[X^{\prime}V^{-1}X\beta=X^{\prime}V^{-1}Y\]

requires inversion of the \(n\times n\) matrix \(V\). The mixed model equations

\[\left[\begin{matrix}X^{\prime}R^{-1}X&X^{\prime}R^{-1}Z\\ Z^{\prime}R^{-1}X&D^{-1}+Z^{\prime}R^{-1}Z\end{matrix}\right]\left[\begin{matrix} \beta\\ \gamma\end{matrix}\right]=\left[\begin{matrix}X^{\prime}R^{-1}Y\\ Z^{\prime}R^{-1}Y\end{matrix}\right]\]

require computation of two inverses, \(D^{-1}\), which is \(q\) dimensional, and \(R^{-1}\), which generally is taken to be a diagonal matrix. If there are many observations relative to the number of random effects, it is easier to solve the mixed model equations. Of course, using Eq. (5.1.3) to obtain \(V^{-1}\) for the generalized normal equations accomplishes the same thing.

An equivalent form of the mixed model equations that does not require \(D\) to be nonsingular is

\[\left[\begin{matrix}X^{\prime}R^{-1}X&X^{\prime}R^{-1}ZD\\ Z^{\prime}R^{-1}X&I+Z^{\prime}R^{-1}ZD\end{matrix}\right]\left[\begin{matrix} \beta\\ \xi\end{matrix}\right]=\left[\begin{matrix}X^{\prime}R^{-1}Y\\ Z^{\prime}R^{-1}Y\end{matrix}\right]. \tag{5.2.3}\]

Solutions \(\hat{\beta}\), \(\hat{\xi}\) have \(X\hat{\beta}\) a BLUE of \(X\beta\) and \(D\hat{\xi}=\hat{\gamma}\) a BLUP of \(\hat{\gamma}\).

Exercise 5.2: Even when \(D\) is nonsingular, Eq. (5.2.3) has an advantage over Eq. (5.2.1) in that Eq. (5.2.3) does not require \(D^{-1}\). Show that Eqs. (5.2.1) and (5.2.3) are equivalent when \(D\) is nonsingular.

The mixed model equations can also be arrived at from a Bayesian argument. Consider the model

\[Y=X\beta+Z\gamma+e,\quad e\sim N(0,R),\]and, as discussed in _PA_ Sect. 2.9, incorporate partial prior information in the form

\[\gamma\sim N(0,D),\]

where \(D\) is again assumed to be nonsingular. A minor generalization of _PA_ Eq. (5.9.3) allows the data \(Y\) to have an arbitrary nonsingular covariance matrix, so the Bayesian analysis can be obtained from fitting the generalized least squares model

\[\begin{bmatrix}Y\\ 0\end{bmatrix}=\begin{bmatrix}X&Z\\ 0&I\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}+\begin{bmatrix}e\\ \tilde{e}\end{bmatrix},\quad\begin{bmatrix}e\\ \tilde{e}\end{bmatrix}\sim N\bigg{(}\begin{bmatrix}0_{n\times 1}\\ 0_{r\times 1}\end{bmatrix},\begin{bmatrix}R&0\\ 0&D\end{bmatrix}\bigg{)}.\]

The generalized least squares estimates from this model will be the "objective" posterior means of \(\beta\) and \(\gamma\), respectively. However, the generalized least squares estimates can be obtained from the corresponding normal equations, and the normal equations are the mixed model equations (5.2.1). See also Sect. 5.3.

### Equivalence of Random Effects and Ridge Regression

Models (2.1.1) and (5.1.1) are both partitioned linear models with, most commonly,

\[Y=X\beta+Z\gamma+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\sigma^{2}I. \tag{5.3.1}\]

In both we were interested in estimating \(\beta\) but in (2.1.1) we were interested in penalizing the estimation of the fixed \(\gamma\) parameters whereas in (5.1.1) we were interested in predicting the random \(\gamma\) parameters. We now establish a numerical equivalence between obtaining \(\hat{\beta}\) and \(\hat{\gamma}\) using generalized ridge regression and obtaining \(\hat{\beta}\) and \(\hat{\gamma}\) by finding BLUEs and BLUPs. The fact of this equivalence was brought to my attention by Jim Hodges and Murry Clayton, see also Brumback et al. (1999), Pearce and Wand (2006), and Hodges (2014).

Consider again fitting the augmented linear model (2.2.3). We have already shown that the generalized least squares estimates for model (2.2.3) give the generalized ridge regression estimates for model (5.3.1). Now we show that they are also the BLUEs and BLUPs for a mixed model version of (5.3.1).

Consider model (5.3.1) as a mixed model with \(\mathrm{E}(\gamma)=0\), \(\mathrm{Cov}(\gamma)=\sigma^{2}(1/k)Q^{-1}\), \(\mathrm{Cov}(\gamma,e)=0\). The mixed model equations of Sect. 5.2 for obtaining the BLUEs and BLUPs become

\[\begin{bmatrix}X^{\prime}X&X^{\prime}Z\\ Z^{\prime}X&Z^{\prime}Z+kQ\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}=\begin{bmatrix}X^{\prime}Y\\ Z^{\prime}Y\end{bmatrix}.\]

It is easy to see that these mixed model equations are identical to the (generalized) normal equations, cf. _PA_ Sect. 2.8, for model (2.2.3), namely

\[\begin{bmatrix}X&Z\\ 0&I\end{bmatrix}^{\prime}\begin{bmatrix}I&0\\ 0&kQ\end{bmatrix}\begin{bmatrix}X&Z\\ 0&I\end{bmatrix}\begin{bmatrix}\beta\\ \gamma\end{bmatrix}=\begin{bmatrix}X&Z\\ 0&I\end{bmatrix}^{\prime}\begin{bmatrix}I&0\\ 0&kQ\end{bmatrix}\begin{bmatrix}Y\\ 0\end{bmatrix}.\]Thus generalized least squares estimates for model (2.2.3) are both the BLUEs and BLUPs for the mixed model version of (5.3.1) and the generalized ridge regression estimates for model (5.3.1).

### Partitioning and Linear Covariance Structures

The basic mixed model is

\[Y=X\beta+Z\gamma+e,\]

where \(X\) is \(n\times p\), \(Z\) is \(n\times q\),

\[\mathrm{E}(\gamma)=0,\qquad\mathrm{E}(e)=0,\]

\[\mathrm{Cov}(\gamma)=D,\qquad\mathrm{Cov}(e)=R,\]

and

\[\mathrm{Cov}(\gamma,e)=0.\]

When \(D\) and \(R\) are unknown, the positive definite matrix \(R\) can contain up to \(n(n+1)/2\) unknown parameters and the positive definite matrix \(D\) can contain up to \(q(q+1)/2\) unknown parameters. Obviously, we cannot do a good job of estimating that many parameters from the \(n\) observations in \(Y\). We need to model \(R\) and \(D\).

By far the most common model for \(R\) is

\[R=\sigma_{0}^{2}I_{n}.\]

Henceforth we incorporate this assumption denoting \(\theta_{1}\equiv\sigma_{0}^{2}\) as needed. But our problem with estimation of \(D\) remains. We write an arbitrary parameterization of \(D\) as \(D(\theta_{\ast})\) where \(\theta_{\ast}\equiv[\theta_{2},\ldots,\theta_{s}]^{\prime}\). As alluded to, \(s-1\) can be as large as \(q(q+1)/2\).

In estimating \(D\), the best thing that could possibly happen is that we would observe \(\gamma\). Under multivariate normality, if you observed both \(Y\) and \(\gamma\), the likelihood would factor into two pieces based on \(Y-Z\gamma\sim N(X\beta,\sigma_{0}^{2}I)\); \(\gamma\sim N[0,D(\theta_{\ast})]\) with all the information on \(\theta_{s}\) coming from \(\gamma\) alone. From our assumptions,

\[\mathrm{E}(\gamma\gamma^{\prime})=D,\]

so without further assumptions the best estimate we could possibly get is \(\hat{D}=\gamma\gamma^{\prime}\). This estimate is based on one observation of \(\gamma\), so it is a horrible estimate. Moreover, this "best estimate" is not even available because we do not really observe \(\gamma\). We have to make further assumptions about \(D\) to be productive.

Without loss of generality, partition \(Z\) and \(\gamma\) as

\[Z=[Z_{1},\cdots,Z_{r}]\qquad\text{and}\qquad\gamma^{\prime}=(\gamma_{1}^{ \prime},\cdots,\gamma_{r}^{\prime}).\]

where \(Z_{k}\) is \(n\times q(k)\), \(\gamma_{k}\) is a \(q(k)\) vector, and \(q=\sum_{k=1}^{r}q(k)\). The linear model becomes \[Y=X\beta+\sum_{k=1}^{r}Z_{k}\gamma_{k}+e.\]

Writing \(D\) in conformance with the partition gives

\[D=\begin{bmatrix}D_{11}&\cdots&D_{1r}\\ \vdots&\ddots&\vdots\\ D_{r1}&\cdots&D_{rr}\end{bmatrix}.\]

In particular, if we observed \(\gamma\), we could estimate \(D_{jk}\) using \(\gamma_{j}\gamma_{k}^{\prime}\) since \(\mathrm{E}[\gamma_{j}\gamma_{k}^{\prime}]=D_{jk}\), but the estimation is still horrible.

In terms of modeling \(D\), the first thing we might do is assume the \(\gamma_{k}\)s are uncorrelated. This leads to

\[D=\begin{bmatrix}D_{11}&&0\\ &\ddots&\\ 0&&D_{rr}\end{bmatrix},\]

but it does nothing to improve the quality of estimation for the terms that remain in the model. In the next two sections we examine models that do help with the quality of estimation.

In Sect. 5.5 we consider variance component models that have the form

\[D=\begin{bmatrix}\sigma_{1}^{2}I_{q(1)}&&0\\ &\ddots&\\ 0&&\sigma_{r}^{2}I_{q(r)}\end{bmatrix}. \tag{5.4.1}\]

In the variance component model we have at least a hope of estimating the parameters well. Upon observing \(\gamma\) and using \(PA\) Proposition 1.3.2,

\[\mathrm{E}\left[\gamma_{k}\gamma_{k}\right]=\mathrm{tr}\left[I_{q(k)}\sigma_{ k}^{2}I_{q(k)}\right]=\sigma_{k}^{2}q(k).\]

Thus \(\frac{1}{q(k)}\gamma_{k}^{\prime}\gamma_{k}\) gives an unbiased estimate of \(\sigma_{k}^{2}\) with \(q(k)\) degrees of freedom. The larger \(q(k)\), the better our estimate should be. Of course, this is the best we could hope to do. In reality, we do not observe \(\gamma\), so our estimates will be worse.

In Sect. 5.6 we consider longitudinal data models that assume \(q(1)=\cdots=q(r)\equiv\tilde{q}\) with

\[D=\begin{bmatrix}\Sigma&&0\\ &\ddots&\\ 0&&\Sigma\end{bmatrix}, \tag{5.4.2}\]

where \(\Sigma\) is a positive definite covariance matrix of \(\tilde{q}\) dimensions. Upon observing \(\gamma\) we have \(\mathrm{E}[\gamma_{j}\gamma_{j}]=\Sigma\) and

\[\mathrm{E}\left[\frac{1}{r}\sum_{j=1}^{r}\gamma_{j}\gamma_{j}^{\prime}\right]=\Sigma.\]Thus \(\frac{1}{r}\sum_{j=1}^{r}\gamma_{j}\gamma_{j}^{\prime}\) provides \(r\) degrees of freedom for estimating \(\Sigma\). Without actually observing \(\gamma\), we must expect to do less well.

The simplest example of both a variance component model and this longitudinal model is the random intercepts model of Example 5.1.2. In that model, \(D=\sigma_{1}^{2}I_{q}\), which fits form (5.4.1) with \(r=1\) and form (5.4.2) with \(\tilde{q}=1\).

The salient characteristic of models (5.4.1) and (5.4.2) is that they are both linear in their unknown parameters in the sense that for some \(s\)

\[D(\theta_{*})=D_{0}+\theta_{2}D_{2}+\cdots+\theta_{s}D_{s}, \tag{5.4.3}\]

where \(D_{0},\ldots,D_{s}\) are known matrices. Typically the \(D_{j}\)s are symmetric but they may or may not be nonnegative definite. Note that \(D_{j}\) has no relationship to any \(D_{jk}\) and that \(D_{0}\) has no relationship to \(\sigma_{0}^{2}\). The variance component model (5.4.1) has

\[\left[\begin{array}{cccc}\sigma_{1}^{2}I_{q(1)}&&&0\\ &\sigma_{2}^{2}I_{q(2)}&&&\\ &&\ddots&\\ 0&&&\sigma_{r}^{2}I_{q(r)}\end{array}\right]=\] \[\sigma_{1}^{2}\left[\begin{matrix}I_{q(1)}&0&\cdots&0\\ 0&0&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&0\end{matrix}\right]+\cdots+\sigma_{r}^{2}\left[\begin{matrix}0& \cdots&0&0\\ \vdots&\ddots&\vdots&\vdots\\ 0&\cdots&0&0\\ 0&\cdots&0&I_{q(r)}\end{matrix}\right].\]

The longitudinal model (5.4.2), for \(\tilde{q}=2\), has

\[\left[\begin{matrix}\sigma_{11}&\sigma_{12}&0&0\\ \sigma_{12}&\sigma_{22}&&0&0\\ &&\ddots&\\ 0&0&&\sigma_{11}&\sigma_{12}\\ 0&0&&\sigma_{12}&\sigma_{22}\end{matrix}\right]=\sigma_{11}\left[\begin{matrix} 1&0&0&0\\ 0&0&&0\\ &&\ddots&\\ 0&0&&1&0\\ 0&0&&0&0\end{matrix}\right] \tag{5.4.4}\] \[+\sigma_{12}\left[\begin{matrix}0&1&0&0\\ 1&0&&0&0\\ &&\ddots&\\ 0&0&&0&1\\ 0&0&&1&0\end{matrix}\right]+\sigma_{22}\left[\begin{matrix}0&0&0&0\\ 0&1&&0&0\\ &&\ddots&\\ 0&0&&0&0\\ 0&0&&0&1\end{matrix}\right].\]

In (5.4.4) we have quite properly imposed the symmetry condition \(\sigma_{12}=\sigma_{21}\). Note that in both examples \(D_{0}=0\), all the \(D_{j}\)s are symmetric, but in the longitudinal model (5.4.4) \(D_{2}\) (corresponding to \(\sigma_{12}\)) is not nonnegative definite. It is pretty obvious how the decomposition changes if one does not impose symmetry.

The linear covariance structure imposed on \(D(\theta_{*})\) in (5.4.3) defines a linear covariance structure on \(V(\theta)\) as discussed in Sect. 4.4. In particular, with \(\theta_{1}\equiv\sigma_{0}^{2}\),

\[V(\theta)=\theta_{1}I_{n}+ZD(\theta_{*})Z^{\prime}\]\[= \theta_{1}I_{n}+Z\left(D_{0}+\theta_{2}D_{2}+\cdots+\theta_{s}D_{s} \right)Z^{\prime}\] \[= ZD_{0}Z^{\prime}+\theta_{1}I_{n}+\theta_{2}ZD_{2}Z^{\prime}+ \cdots+\theta_{s}ZD_{s}Z^{\prime}\] \[= V_{0}+\theta_{1}V_{1}+\theta_{2}V_{2}+\cdots+\theta_{s}V_{s}.\]

Here \(V_{j}=ZD_{j}Z^{\prime}\) for \(j\neq 1\) and \(V_{1}=I_{n}\).

### Variance Component Models

In the old days, the terms "mixed model" and "variance component model" were essentially synonymous. That is no longer true. We describe traditional variance component models.

Variance component models assume a partitioned model

\[Y=X\beta+\sum_{k=1}^{r}Z_{k}\gamma_{k}+e,\]

\[\mathrm{E}(e)=0,\qquad\mathrm{E}(\gamma)=0,\]

\[\mathrm{Cov}(e)=\sigma_{0}^{2}I_{n},\qquad\mathrm{Cov}(\gamma,e)=0,\]

and add the assumptions that \(\mathrm{Cov}(\gamma_{k})=\sigma_{k}^{2}I_{q(k)}\) and, for \(j\neq k\), \(\mathrm{Cov}(\gamma_{j},\gamma_{k})=0\). The covariance matrix of \(\gamma\) is given in Eq. (5.4.1), which, of course, is a diagonal matrix. With these conventions we can write

\[V(\theta)=\sigma_{0}^{2}I_{n}+\sum_{k=1}^{r}\sigma_{k}^{2}Z_{k}Z_{k}^{\prime}= \sum_{k=0}^{r}\sigma_{k}^{2}Z_{k}Z_{k}^{\prime}, \tag{5.5.1}\]

where we take \(Z_{0}\equiv I_{n}\) and \(\theta\equiv(\sigma_{0}^{2},\cdots,\sigma_{r}^{2})^{\prime}\). Equation (5.5.1) constitutes a linear covariance structure with, relative to Sect. 4.4, \(s=r+1\), \(V_{0}=0\), and \(V_{j+1}=Z_{j}Z_{j}^{\prime}\), \(j=0,\ldots,r\). To justify the use of the asymptotic testing methods described in Chap. 4, we need \(n\) to get large but, as discussed in Sect. 5.4, we also need all of the \(q(k)\)s, \(k=1,\ldots,r\), to get large.

Traditionally, variance component models were used in analysis of variance situations in which the columns of \(Z_{k}\) consisted of 0-1 indicators of group membership. In such cases the diagonal elements of \(Z_{k}Z_{k}^{\prime}\) are all 1s and the variance of any individual observation is \(\sum_{k=0}^{r}\sigma_{k}^{2}\), so every term \(\sigma_{k}^{2}\) has a clear interpretation as a component of the overall variance. Even for general \(Z_{k}\)s, the variance of an individual observation is a linear combination of the variance components, the \(\sigma_{k}^{2}\)s.

Example 5.5.1. _Random Intercepts Model_.

Example 5.1.2 gives explicit versions of \(Y\), \(X\), \(Z\), and \(\gamma\) for \(n=8\); \(q=3\). The partitioning in this model is degenerate in that we pick \(r=1\), so \(Z=Z_{1}\), \(q=q(1)=3\), \(\gamma=\gamma_{1}=(\alpha_{1},\alpha_{2},\alpha_{3})^{\prime}\). With \(\mathrm{Cov}(\gamma_{1})=\sigma_{1}^{2}I_{3}=D\), we get 

#### Variance Component Estimation

The variance component model is a general linear model so the likelihood theories of Sects. 4.2 and 4.3 apply with

\[\mathbf{d}_{\sigma_{i}^{2}}V(\theta)=Z_{i}Z_{i}^{\prime}.\]

It follows that the likelihood equations for the variance components become

\[\operatorname{tr}\left\{V^{-1}(\theta)Z_{j}Z_{j}^{\prime}\right\}=Y^{\prime}[I- A(\theta)]^{\prime}V^{-1}(\theta)Z_{j}Z_{j}^{\prime}V^{-1}(\theta)[I-A(\theta)]Y,\]

\(j=0,\ldots,r\), or, since \(V(\theta)\) has the linear covariance structure of Sect. 4.4,

\[\sum_{k=0}^{r}\sigma_{k}^{2}\operatorname{tr}\left\{Z_{k}Z_{k}^{\prime}V^{-1}( \theta)Z_{j}Z_{j}^{\prime}V^{-1}(\theta)\right\}=Y^{\prime}[I-A(\theta)]^{ \prime}V^{-1}(\theta)Z_{j}Z_{j}^{\prime}V^{-1}(\theta)[I-A(\theta)]Y,\]

\(j=0,\ldots,r\).

The derivative also provides the REML equations for variance component models,

\[\operatorname{tr}\left\{V^{-1}(\theta)[I-A(\theta)]Z_{j}Z_{j}^{\prime}\right\} =Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)Z_{j}Z_{j}^{\prime}V^{-1}( \theta)[I-A(\theta)]Y,\]

\(j=0,\ldots,r\). Again, since the covariance matrix is linear, the REML equations can be rewritten as

\[\sum_{k=0}^{r}\sigma_{k}^{2}\operatorname{tr}\left\{Z_{k}Z_{k}^{ \prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)Z_{j}Z_{j}^{\prime}V^{-1}(\theta)[I -A(\theta)]\right\}\\ =Y^{\prime}[I-A(\theta)]^{\prime}V^{-1}(\theta)Z_{j}Z_{j}^{\prime }V^{-1}(\theta)[I-A(\theta)]Y,\]

\(j=0,\ldots,r\).

There are many questions about using the likelihood or REML equations. The MLEs or REML estimates may be solutions to the equations with \(\sigma_{k}^{2}>0\) for all \(k\), or they may not be solutions, but rather be on a boundary of the parameter space.

There may be solutions other than the maximum. Such questions are beyond the scope of this book.

##### Exercise 5.3.

In Exercise 4.6 we found closed form solutions to the REML equations for generalized split plot (GSP) models. The parameterization used in Exercise 4.6 was not the variance component parameterization. What we call \(Z\) in a mixed model is the matrix \(X_{1}\) from a GSP. Show that the REML equations for the variance components of a GSP model lead to the same estimates as found in Exercise 4.6.

Following Sect. 4.5, the MINQUE equations for a variance component model can be written

\[\sum_{k=0}^{r}\sigma_{k}^{2}\text{tr}\left\{Z_{k}Z_{k}^{\prime}[I- A(w)]^{\prime}V^{-1}(w)Z_{j}Z_{j}^{\prime}V^{-1}(w)[I-A(w)]\right\}\\ =Y^{\prime}[I-A(w)]^{\prime}V^{-1}(w)Z_{j}Z_{j}^{\prime}V^{-1}(w) [I-A(w)]Y,\]

\(j=0,\ldots,r\).

##### Example 5.5.2.

_Balanced One-Way ANOVA._

Let \(y_{ij}=\mu+\alpha_{i}+e_{ij}\), \(i=1,\ldots,a\), \(j=1,\ldots,N\), with the \(\alpha_{i}\)s independent \(N(0,\sigma_{1}^{2})\), the \(e_{ij}\)s independent \(N(0,\sigma_{0}^{2})\), and the \(\alpha_{i}\)s and \(e_{ij}\)s independent.

The matrix \([X,Z]\) for the general mixed model is just the model matrix from \(PA\) Chap. 4, where now \(X=J_{n}^{1}\), \(q=a\), \(r=1\), \(Z_{1}=Z=[X_{1},\ldots,X_{a}]\) and \(X_{i}\) is a 0-1 indicator vector for an observation being in group \(i\). The covariance matrix is the same as that considered in \(PA\) Chap. 11 but in any case it is not difficult to see that

\[V=\sigma_{0}^{2}I+\sigma_{1}^{2}ZZ^{\prime}=\sigma_{0}^{2}I+N\sigma_{1}^{2}M_{Z},\]

where \(M_{Z}\) is the perpendicular projection matrix onto \(C(Z)\). The inverse of \(V\) is easily seen to be

\[V^{-1}=\frac{1}{\sigma_{0}^{2}}\left[I-\frac{N\sigma_{1}^{2}}{\sigma_{0}^{2}+N \sigma_{1}^{2}}M_{Z}\right].\]

This follows from Eq. (5.1.4) and simplifying but also follows from \(PA\)-\(V\) Proposition B.57 (Christensen 2011, Proposition 12.10.1).

We can now find the estimates. It is easily seen that \(C(VX)\subset C(X)\), so, as discussed in \(PA\)Sects. 2.7 and 2.8, least squares estimates provide solutions to the normal equations \(X^{\prime}V^{-1}X\beta=X^{\prime}V^{-1}Y\) which are also the likelihood equations for \(\beta\). Simply put, \(\hat{\mu}=\bar{y}\)...

For \(j=0\), the likelihood equation is

\[\text{tr}(V^{-1})=(Y-\hat{\mu}J)^{\prime}V^{-2}(Y-\hat{\mu}J).\]

Observe that \[V^{-1}(Y-\hat{\mu}J) = \frac{1}{\sigma_{0}^{2}}\left[I-\frac{N\sigma_{1}^{2}}{\sigma_{0}^{2 }+N\sigma_{1}^{2}}M_{Z}\right]\left(I-\frac{1}{n}J_{n}^{n}\right)Y\] \[= \frac{1}{\sigma_{0}^{2}}\left[\left(I-\frac{1}{n}J_{n}^{n}\right) Y-\frac{N\sigma_{1}^{2}}{\sigma_{0}^{2}+N\sigma_{1}^{2}}\left(M_{Z}-\frac{1}{n}J_{n}^{ n}\right)Y\right]\] \[= \frac{1}{\sigma_{0}^{2}}\left[\left(I-M_{Z}\right)Y-\left(\frac{N \sigma_{1}^{2}}{\sigma_{0}^{2}+N\sigma_{1}^{2}}-1\right)\left(M_{Z}-\frac{1}{n }J_{n}^{n}\right)Y\right]\] \[= \frac{1}{\sigma_{0}^{2}}\left(I-M_{Z}\right)Y-\frac{1}{\sigma_{0} ^{2}+N\sigma_{1}^{2}}\left(M_{Z}-\frac{1}{n}J_{n}^{n}\right)Y.\]

Thus, evaluating \(\text{tr}(V^{-1})\) on the lefthand side of the likelihood equation and computing the squared length on the righthand side leads to the equation

\[\frac{Na[\sigma_{0}^{2}+(N-1)\sigma_{1}^{2}]}{\sigma_{0}^{2}(\sigma_{0}^{2}+N \sigma_{1}^{2})}=\frac{SSE}{\sigma_{0}^{4}}+\frac{SSTrts}{(\sigma_{0}^{2}+N \sigma_{1}^{2})^{2}}. \tag{5.5.2}\]

For \(j=1\), the likelihood equation is

\[\text{tr}(V^{-1}ZZ^{\prime})=(Y-\hat{\mu}J)^{\prime}V^{-1}ZZ^{\prime}V^{-1}(Y- \hat{\mu}J). \tag{5.5.3}\]

Using \(ZZ^{\prime}=NM_{Z}\) and the characterization of \(V^{-1}(Y-\hat{\mu}J)\), the righthand side of (5.5.3) can be written

\[(Y-\hat{\mu}J)^{\prime}V^{-1}ZZ^{\prime}V^{-1}(Y-\hat{\mu}J) = N(Y-\hat{\mu}J)^{\prime}V^{-1}M_{Z}V^{-1}(Y-\hat{\mu}J)\] \[= \frac{N}{\left(\sigma_{0}^{2}+N\sigma_{1}^{2}\right)^{2}}Y^{ \prime}\left(M_{Z}-\frac{1}{n}J_{n}^{n}\right)Y\] \[= \frac{NSSTrts}{\left(\sigma_{0}^{2}+N\sigma_{1}^{2}\right)^{2}}.\]

To evaluate the lefthand side of (5.5.3), note that

\[\text{tr}(V^{-1}ZZ^{\prime}) = \text{tr}\left\{\frac{1}{\sigma_{0}^{2}}\left[I-\frac{N\sigma_{1} ^{2}}{\sigma_{0}^{2}+N\sigma_{1}^{2}}M_{Z}\right]NM_{Z}\right\}\] \[= \frac{N}{\sigma_{0}^{2}}\text{tr}\left[M_{Z}-\frac{N\sigma_{1}^{ 2}}{\sigma_{0}^{2}+N\sigma_{1}^{2}}M_{Z}\right]\] \[= \frac{N}{\sigma_{0}^{2}}\frac{\sigma_{0}^{2}}{\sigma_{0}^{2}+N \sigma_{1}^{2}}\text{tr}(M_{Z})\] \[= \frac{Na}{\sigma_{0}^{2}+N\sigma_{1}^{2}}.\]

Equation (5.5.3) becomes

\[\frac{Na}{\sigma_{0}^{2}+N\sigma_{1}^{2}}=\frac{NSSTrts}{\left(\sigma_{0}^{2} +N\sigma_{1}^{2}\right)^{2}}\]or

\[\sigma_{0}^{2}+N\sigma_{1}^{2}=SSTrts\big{/}a. \tag{5.5.4}\]

Substituting equation (5.5.4) into Eq. (5.5.2) and multiplying through by \(\sigma_{0}^{4}\) gives

\[\frac{Na\sigma_{0}^{2}[\sigma_{0}^{2}+(N-1)\sigma_{1}^{2}]}{(\sigma_{0}^{2}+N \sigma_{1}^{2})}=SSE+\frac{a\sigma_{0}^{4}}{(\sigma_{0}^{2}+N\sigma_{1}^{2})}\]

or, less obviously,

\[a(N-1)\sigma_{0}^{2}=SSE.\]

The maximum likelihood estimates appear to be \(\hat{\sigma}_{0}^{2}=MSE\) and \(\hat{\sigma}_{1}^{2}=[(\hat{\sigma}_{0}^{2}+N\hat{\sigma}_{1}^{2})-\hat{ \sigma}_{0}^{2}]/N=[SSTrts/a-MSE]/N\). However, this is true only if \(SSTrts/a-MSE>0\). Otherwise, the maximum is on a boundary, so \(\hat{\sigma}_{1}^{2}=0\) and \(\hat{\sigma}_{0}^{2}=SSE/aN\). \(\Box\)

The maximum likelihood procedure tends to ignore the fact that mean parameters are being fitted. In the one-way ANOVA example, the estimate of \(\sigma_{0}^{2}+N\sigma_{1}^{2}\) was \(SSTrts/a\) instead of the unbiased estimate \(MSTrts\). No correction was included for fitting the parameter \(\mu\).

Note that the balanced one-way model of Example 5.5.2 is actually a special case of a generalized split plot model as discussed in Exercise 4.6.

### A Longitudinal Model

Longitudinal models are characterized by having a number of individuals (often assumed to be independent), with multiple observations taken on each individual over time. In the random intercepts model of Example 5.1.2, the individuals would be identified as the groups \(i\) with the multiple observations then indexed by \(j\). Many mixed models for longitudinal data have the block diagonal covariance structure of Eq. (5.4.2).

Example 5.6.1. _Random Intercepts Model_.

To view the random intercepts model of Examples 5.1.2 and 5.4.1 as having the covariance structure of (5.4.2), pick \(r=q=a\) with the \(k\)th column of \(Z\) as \(Z_{k}\), \(q(k)=1\), and \(\gamma_{k}=\alpha_{k}\). \(\Box\)

Example 5.6.2. _Random Slopes and Intercepts Model_.

Starting with the random intercepts model and assuming an additional known predictor variable \(z\), we incorporate random slopes \(\eta_{i}\). For example, \(z_{ij}\) might be the \(j\)th time at which the \(i\)th individual was observed in which case \(\alpha_{i}+\eta_{i}z_{ij}\), \(j=1,\ldots,N_{i}\), is a random linear time trend associated with the observations \(y_{ij}\) on the \(i\)th individual. Again, this is only appropriate when we have no particular interest in the individuals but rather think of them as a sample from a larger population. Let\[y_{ij}=x^{\prime}_{ij}\beta+\alpha_{i}+\eta_{i}z_{ij}+e_{ij},\]

\(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\), with the \((\alpha_{i},\eta_{i})^{\prime}\)s independent \(N(0,\Sigma)\), the \(e_{ij}\)s independent \(N(0,\sigma_{0}^{2})\), and the \((\alpha_{i},\eta_{i})^{\prime}\)s and \(e_{ij}\)s independent. Since \(\alpha_{i}\) and \(\eta_{i}\) are both effects for the \(i\)th individual, it seems unlikely that they would be independent. Typically such models also include a fixed intercept and slope that do not depend on the individual \(i\), but a fixed term \(\beta_{0}+\beta_{1}z_{ij}\) can be incorporated into \(x^{\prime}_{ij}\beta\). If individuals are grouped according to some condition, typically there would be a fixed simple linear regression for each condition. Here \(q=2a\), \(r=a\), and \(q(k)=\tilde{q}=2\).

Similar to the random intercepts model of Example 5.1.2, consider \(a=3\) and \((N_{1},N_{2},N_{3})=(3,2,3)\). Write a model \(Y=X\beta+Z\gamma+e\) with \(Y\) and \(X\) defined as in Example 5.1.2,

\[Z=[Z_{1},Z_{2},Z_{3}]=\begin{bmatrix}1&z_{11}&0&0&0&0\\ 1&z_{12}&0&0&0&0\\ 1&z_{13}&0&0&0&0\\ 0&0&1&z_{21}&0&0\\ 0&0&1&z_{22}&0&0\\ 0&0&0&0&1&z_{31}\\ 0&0&0&0&1&z_{32}\\ 0&0&0&0&1&z_{33}\end{bmatrix},\]

and \(\gamma_{i}=(\alpha_{i},\eta_{i})^{\prime}\). For this model \(r=3\), \(q=6\), and \(q(k)=2=\tilde{q}\). By assumption, \(\gamma\) has the covariance structure of (5.4.2). Writing the \(gh\) element of \(\Sigma\) as \(\sigma_{gh}\) leads to writing \(\theta=(\sigma_{0}^{2},\sigma_{11},\sigma_{12},\sigma_{22})^{\prime}\) and

\[V(\theta)=\sigma_{0}^{2}I_{n}+[Z_{1},\quad Z_{2},\quad Z_{3}]\begin{bmatrix} \Sigma&0&0\\ 0&\Sigma&0\\ 0&0&\Sigma\end{bmatrix}\begin{bmatrix}Z^{\prime}_{1}\\ Z^{\prime}_{2}\\ Z^{\prime}_{3}\end{bmatrix}=\sigma_{0}^{2}I_{n}+\sum_{k=1}^{3}Z_{k}\Sigma Z^{ \prime}_{k}.\]

While this is a nice looking way to write \(V(\theta)\), it does not have the linear covariance structure of Sect. 4.4. However, as demonstrated in Eq. (5.4.4), these models do have linear covariance structure for \(D(\theta_{*})\) and therefore, as shown in Sect. 5.4, have the linear covariance structure of Sect. 4.4.

If we assume that the intercepts and slopes are independent, this longitudinal model also becomes a variance component model. If

\[\begin{bmatrix}\alpha_{i}\\ \eta_{i}\end{bmatrix}\sim N\left(\begin{bmatrix}0\\ 0\end{bmatrix},\begin{bmatrix}\sigma_{1}^{2}&0\\ 0&\sigma_{2}^{2}\end{bmatrix}\right),\]

we could redefine \(Z\) with \(r=2\) and \(q(k)=3\) as \[Z=\left[\begin{array}{cccccc}1&0&0&z_{11}&0&0\\ 1&0&0&z_{12}&0&0\\ 1&0&0&z_{13}&0&0\\ 0&1&0&0&z_{21}&0\\ 0&1&0&0&z_{22}&0\\ 0&0&1&0&0&z_{31}\\ 0&0&1&0&0&z_{32}\\ 0&0&1&0&0&z_{33}\end{array}\right]=[Z_{1},Z_{2}].\]

It follows that \(\gamma_{1}=(\alpha_{1},\alpha_{2},\alpha_{3})^{\prime}\) and \(\gamma_{2}=(\eta_{1},\eta_{2},\eta_{3})^{\prime}\) with \(\operatorname{Cov}(\gamma_{k})=\sigma_{k}^{2}I_{3}\). Most importantly, and perhaps unrealistically, this model now assumes that \(\operatorname{Cov}(\gamma_{1},\gamma_{2})=0\). 

Example 5.6.3.: _Random Regression Models_.

More generally we can write longitudinal models that have

\[y_{ij}=x_{ij}^{\prime}\beta+z_{ij}^{\prime}\gamma_{i}+e_{ij},\]

\(i=1,\ldots,r\), \(j=1,\ldots,N_{i}\). Here \(\beta\) and the \(x_{ij}\)s are \(p\) vectors, the \(\gamma_{i}\)s and \(z_{ij}\)s are \(\tilde{q}\) vectors, and the \(\gamma_{i}\)s are uncorrelated having \(\operatorname{E}(\gamma_{i})=0\) and \(\operatorname{Cov}(\gamma_{i})=\Sigma\). As usual, the \(e_{ij}\)s are uncorrelated with each other as well as the \(\gamma_{i}\)s, have \(\operatorname{E}(e_{ij})=0\), and \(\operatorname{Var}(e_{ij})=\sigma_{0}^{2}\).

For the \(i\)th individual, we can write their \(N_{i}\) observations as a vector \(Y_{i}\), with corresponding matrices \(X_{i}\) having rows consisting of the \(x_{ij}^{\prime}\)s, and matrices \(Z_{ii}\) having rows consisting of the \(z_{ij}^{\prime}\)s. The overall model becomes

\[\left[\begin{array}{c}Y_{1}\\ \vdots\\ Y_{r}\end{array}\right]=\left[\begin{array}{c}X_{1}\\ \vdots\\ X_{r}\end{array}\right]\beta+\left[\begin{array}{ccc}Z_{11}&&0\\ &\ddots&\\ 0&&Z_{rr}\end{array}\right]\left[\begin{array}{c}\gamma_{1}\\ \vdots\\ \gamma_{r}\end{array}\right]+e.\]

It is not difficult to see that this model gives the block diagonal covariance structure for \(\gamma\) of Eq. (5.4.2). Moreover,

\[V(\theta)=\left[\begin{array}{cccc}\sigma_{0}^{2}I_{N_{1}}+Z_{11}\Sigma Z_{ 11}^{\prime}&&0\\ &&\ddots&\\ 0&&&\sigma_{0}^{2}I_{N_{r}}+Z_{rr}\Sigma Z_{rr}^{\prime}\end{array}\right].\]

This block diagonal form has the advantage of making \(V^{-1}(\theta)\) easy to compute but does not seem to generate much additional simplification to the process of estimating parameters. In particular, using a result similar to (5.1.4),

\[V^{-1}(\theta) = \text{Blk diag}\left[(\sigma_{0}^{2}I_{N_{j}}+Z_{jj}\Sigma Z_{jj }^{\prime})^{-1}\right]\] \[= \text{Blk diag}\left[\frac{1}{\sigma_{0}^{2}}\left\{I_{N_{j}}-Z_ {jj}[\sigma_{0}^{2}\Sigma^{-1}+Z_{jj}^{\prime}Z_{jj}]^{-1}Z_{jj}^{\prime} \right\}\right].\]

In the last equality, the inverses are of \(\tilde{q}\times\tilde{q}\) matrices and typically \(\tilde{q}\) is quite small.

It is commonly assumed that

\[C\left(\begin{bmatrix}\mathsf{Z}_{11}\\ \vdots\\ \mathsf{Z}_{rr}\end{bmatrix}\right)\subset C\left(\begin{bmatrix}X_{1}\\ \vdots\\ X_{r}\end{bmatrix}\right).\]

This specifies that there is a common fixed regression from which the individual random coefficient regressions deviate. If individuals are divided into categories (like treatment groups) there is typically a fixed regression for each category.

Relative to the partitioning notation of Sect. 5.3, the random regression model has \(Z_{j}=Z_{jj}\otimes e_{j}\), where \(e_{j}\) is the \(j\)th column of \(I_{r}\). 

The model of \(PA\)-\(V\) Exercise 12.6b, used to illustrate that _Sandwich Estimators_ can work, is a degenerate form of a random regressions model with \(N_{i}\equiv N\), \(Z_{ii}\equiv I_{N}\), and \(\sigma_{0}^{2}\equiv 0\).

Consider the partitioned model

\[Y=X\beta+\sum_{j=1}^{r}Z_{j}\gamma_{j}+e\]

where \(Z_{j}\) is \(n\times\tilde{q}\), the \(\gamma_{i}\)s are uncorrelated with \(\mathrm{E}(\gamma_{i})=0\) and \(\mathrm{Cov}(\gamma_{i})=\Sigma\), \(e\) has \(\mathrm{E}(e)=0\) and \(\mathrm{Cov}(e)=\sigma_{0}^{2}I_{n}\), and \(\mathrm{Cov}(\gamma_{j},e)=0\) for all \(j\). This gives the block diagonal covariance structure for \(D(\theta_{*})\) of (5.4.2) and

\[V(\theta)=\sigma_{0}^{2}I_{n}+\sum_{k=1}^{r}Z_{k}\Sigma Z_{k}^{\prime}. \tag{5.6.1}\]

It is not uncommon for people to refer to the covariance parameters in \(\Sigma\) as variance components, but that is infelicitous because variance components should provide a natural decomposition of the variance. While the linear structure of \(V(\theta)\) in (5.6.1) is _not_ the linear covariance structure of Sect. 4.4, similar to Sect. 5.3 we will see later that both \(D(\theta_{*})\) and \(V(\theta)\) have linear covariance structure.

To use the asymptotic testing methods described in Chap. 4, we need consistent estimates. As discussed in Sect. 5.3, to get consistent estimation, we need \(r\) to be large. Often, that means a large number of individuals. Typically, if the number of individuals \(r\) is large, \(n\) automatically gets large in an appropriate way for consistent estimation. However, if \(\tilde{q}\) is a reasonably large number, it may become difficult to get \(\tilde{q}\) observations on each individual. For example, a random regression model having some values \(N_{i}<\tilde{q}\) might cause problems. Such models share properties with the missing data models of Chap. 11.

We now address the issue of linear covariance structures for this model. Define the \(gh\) element of the \(\tilde{q}\times\tilde{q}\) matrix \(\Sigma\) as \(\sigma_{gh}\) but recall that \(\sigma_{gh}=\sigma_{hg}\), so we use a parameterization with \(g\leq h\). The derivative of \(\Sigma\) with respect to \(\sigma_{gh}\) is a \(\tilde{q}\) square matrix \(T_{gh}\) with elements \(t_{uv}\) wherein \(t_{uv}=1\) if \((u,v)=(g,h)\) or \((u,v)=(h,g)\) and is \(0\) otherwise. These derivatives are precisely the matrices we need to decompose \(\Sigma\), and thus \(D\), linearly. We can write \[\Sigma=\sum_{g=1}^{\tilde{q}}\sum_{h\leq g}\sigma_{gh}T_{gh}\]

and thus

\[D(\theta_{*})=\sum_{g=1}^{\tilde{q}}\sum_{h\leq g}\sigma_{gh}\text{Blk diag}(T_{ gh}),\]

which is of the linear form (5.4.3). We also get

\[V(\theta)=\sigma_{0}^{2}I_{n}+\sum_{g=1}^{\tilde{q}}\sum_{h\leq g}\sigma_{gh} \sum_{k=1}^{r}Z_{k}T_{gh}Z_{k}^{\prime}\]

so that \(V_{0},V_{1},\ldots,V_{s}\) from Sect. 4.4 are reindexed as \(V_{0}=0\), \(V_{1}=I_{n}\), \(V_{gh}=\sum_{k=1}^{r}Z_{k}T_{gh}\)\(Z_{k}^{\prime},g=1,\ldots,\tilde{q}\), \(h\leq g\). Some slight additional simplifications can be made by writing the partition matrices \(Z_{k}\) as

\[Z_{k}=\left[Z_{k1},\cdots,Z_{k\tilde{q}}\right].\]

Then,

\[V_{gg}=\sum_{k=1}^{r}Z_{kg}Z_{kg}^{\prime}\]

and for \(g<h\),

\[V_{gh}=\sum_{k=1}^{r}\left[Z_{kg}Z_{kh}^{\prime}+Z_{kh}Z_{kg}^{\prime}\right].\]

With these identifications, the covariance parameter estimation methods are precisely as stated in Sects. 4.4 and 4.5.

### Henderson's Method 3

Before the development of the estimation methods described in Chap. 4, Henderson (1953) presented a way of obtaining unbiased method of moment estimates for variance components using least squares computations. The method applies to variance component models but can also be applied to somewhat more general models. Consider a partitioned mixed linear model with

\[Y=X\beta+Z_{*}\gamma_{*}+Z_{r}\gamma_{r}+e, \tag{5.7.1}\]

where we have accumulated all but the last term of the partition into \(Z_{*}=[Z_{1},\ldots,\)\(Z_{r-1}]\) and \(\gamma_{*}^{\prime}=[\gamma_{1}^{\prime},\ldots,\gamma_{r-1}^{\prime}]\). We introduce a generalization of the variance component model by assuming

\[\mathrm{E}(\gamma_{*})=0,\quad\mathrm{E}(\gamma_{r})=0,\quad\mathrm{E}(e)=0,\]\[\mathrm{Cov}(\gamma_{*})=D_{*},\quad\mathrm{Cov}(\gamma_{r})=\sigma_{r}^{2}I_{q(r)},\quad\mathrm{Cov}(e)=\sigma_{0}^{2}I_{n},\]

with all of these random vectors uncorrelated. Clearly, any variance component model can be written in this form.

As a general linear model we have

\[\mathrm{E}(Y)=X\beta;\qquad\mathrm{Cov}(Y)=V(\theta)=\sigma_{0}^{2}I+Z_{*}D_{*} Z_{*}^{\prime}+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}.\]

Henderson's method is based on least squares fitting and thus on perpendicular projection operators. For a general partitioned model define \(P_{k}\) as the ppo onto \(C(X,Z_{1},\ldots,Z_{k})\) for \(k=1,\ldots,r\) and identify \(P_{0}\equiv M\). Obviously, \(P_{k}X=X\) and, for \(j\leq k\), \(P_{k}Z_{j}=Z_{j}\). In particular, \(P_{r}Z_{*}=P_{r-1}Z_{*}=Z_{*}\).

An unbiased estimate of \(\sigma_{0}^{2}\) is

\[\hat{\sigma}_{0}^{2}\equiv\frac{Y^{\prime}(I-P_{r})Y}{n-r(P_{r})},\]

which is the MSE when treating the \(\gamma\)s in the mixed model as fixed effects, rather than random effects. To see that this estimate is unbiased, observe that _PA_ Proposition 1.3.2 gives

\[\mathrm{E}\left[Y^{\prime}(I-P_{r})Y\right] = \mathrm{tr}\left[(I-P_{r})V(\theta)\right]+\beta^{\prime}X^{ \prime}(I-P_{r})X\beta\] \[= \mathrm{tr}\left\{(I-P_{r})\left[\sigma_{0}^{2}I+Z_{*}D_{*}Z_{*}^ {\prime}+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}\right]\right\}+0\] \[= \mathrm{tr}\left[\sigma_{0}^{2}(I-P_{r})\right]+\mathrm{tr}\left[ (I-P_{r})Z_{*}D_{*}Z_{*}^{\prime}\right]+\sigma_{r}^{2}\mathrm{tr}\left[(I-P_{ r})Z_{r}Z_{r}^{\prime}\right]\] \[= \sigma_{0}^{2}\mathrm{tr}[I-P_{r}].\]

To get an unbiased estimate of \(\sigma_{r}^{2}\), consider \(Y^{\prime}(P_{r}-P_{r-1})Y\). Based on

\[\mathrm{E}\left[Y^{\prime}(P_{r}-P_{r-1})Y\right] = \mathrm{tr}\left\{(P_{r}-P_{r-1})\left[\sigma_{0}^{2}I+Z_{*}D_{* }Z_{*}^{\prime}+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}\right]\right\}\] \[= \sigma_{0}^{2}\mathrm{tr}[(P_{r}-P_{r-1})]+\sigma_{r}^{2}\mathrm{ tr}[(P_{r}-P_{r-1})Z_{r}Z_{r}^{\prime}],\]

it is not hard to see that an unbiased estimate of \(\sigma_{r}^{2}\) is

\[\hat{\sigma}_{r}^{2}\equiv\frac{Y^{\prime}(P_{r}-P_{r-1})Y-\hat{\sigma}_{0}^{2 }\mathrm{tr}(P_{r}-P_{r-1})}{\mathrm{tr}[Z_{r}^{\prime}(P_{r}-P_{r-1})Z_{r}]}. \tag{5.7.2}\]

Example 5.7.1. _Balanced One-Way ANOVA, continued from Example 5.4.2._

We relate the notation of this section to that of the earlier example:

\[r=1,\qquad P_{r}=P_{1}=M_{Z},\qquad P_{r-1}=P_{0}=(1/n)J_{n}^{n},\]

\[Y^{\prime}(I-P_{1})Y=SSE,\qquad Y^{\prime}(P_{1}-P_{0})Y=SSTrts,\]

\[\mathrm{tr}(I-P_{1})=a(N-1),\qquad\mathrm{tr}(P_{1}-P_{0})=a-1,\]

\[Y^{\prime}(I-P_{1})Y\big{/}\mathrm{tr}(I-P_{1})=MSE=\hat{\sigma}_{0}^{2}.\]Recall that \(ZZ^{\prime}=NM_{Z}=NP_{1}\), so

\[\begin{split}\operatorname{tr}\bigl{[}(P_{1}-P_{0})ZZ^{\prime} \bigr{]}&=\operatorname{tr}[(P_{1}-P_{0})NP_{1}]\\ &=N\operatorname{tr}[(P_{1}-P_{0})]\\ &=N(a-1).\end{split}\]

From (5.7.2),

\[\hat{\sigma}_{1}^{2}=\frac{SSTrts-MSE(a-1)}{N(a-1)}=\frac{MSTrts-MSE}{N}.\qed\]

**Exercise 5.4**.: Consider the estimates of \(\sigma_{0}^{2}\) and \(\sigma_{1}^{2}\) in Example 5.7.1.

1. Show that these are also the REML estimates.
2. Show that the vector \((Y^{\prime}Y,Y^{\prime}M_{Z}Y,J^{\prime}Y)^{\prime}\) is a complete sufficient statistic for the balanced one-way random effects model.
3. Show that the Method 3 estimates are minimum variance unbiased.
4. Find the distributions of \(SSTrts\) and \(SSE\).
5. Find a generalized likelihood ratio test of level \(\alpha\) for \(H_{0}:\sigma_{1}^{2}=0\).

Hint: Use the concepts of \(PA\) Sect. 2.5 and the methods of Example 5.4.2.

Example 5.7.2.: _Unbalanced One-Way ANOVA._

For unbalanced one-way ANOVA, the main things that change from the previous example are that \(\operatorname{tr}(I-P_{1})=n-a\) and

\[\operatorname{tr}\bigl{[}(P_{1}-P_{0})ZZ^{\prime}\bigr{]}=\operatorname{tr} \bigl{[}Z^{\prime}(P_{1}-P_{0})Z\bigr{]}=\operatorname{tr}\biggl{[}Z^{\prime} \left(I-\frac{1}{n}J_{n}^{n}\right)Z\biggr{]}=n-\sum_{i=1}^{a}(N_{i}^{2}/n).\]

We still get \(MSE=\hat{\sigma}_{0}^{2}\) but now

\[\hat{\sigma}_{1}^{2}=\frac{SSTrts-MSE(a-1)}{n-\sum_{i=1}^{a}(N_{i}^{2}/n)}.\qed\]

Henderson's Method 3 has no known (to me) optimality properties. Henderson himself recommended the use of other techniques. Method 3's greatest advantage is that it is easy to compute. It uses standard techniques from fitting linear models by least squares, except that it requires the computation of \(\operatorname{tr}[(P-P_{r-1})Z_{r}Z_{r}^{\prime}]\). Even this can be computed using standard techniques. Note that

\[\operatorname{tr}\bigl{[}(P_{r}-P_{r-1})Z_{r}Z_{r}^{\prime}\bigr{]}= \operatorname{tr}\bigl{[}Z_{r}^{\prime}(P_{r}-P_{r-1})Z_{r}\bigr{]}= \operatorname{tr}\bigl{[}Z_{r}^{\prime}(I-P_{r-1})Z_{r}\bigr{]}.\]

Write \(Z_{r}=\bigl{[}Z_{r1},Z_{r2},\ldots,Z_{rq(r)}\bigr{]}\), where each \(Z_{rj}\) is a vector. By fitting the model

\[Z_{rj}=X\beta+\sum_{i=1}^{r-1}Z_{i}\gamma_{i}+e\]as if the \(\gamma_{i}\)s are all fixed, we can obtain \((Z_{rj})^{\prime}(I-P_{r-1})(Z_{rj})\) as the sum of squares error, and thus

\[\mathrm{tr}\big{[}Z_{r}^{\prime}(I-P_{r-1})Z_{r}\big{]}=\sum_{j=1}^{q(r)}(Z_{rj} )^{\prime}(I-P_{r-1})(Z_{rj}).\]

In other words, all of the numbers required for estimating \(\sigma_{r}^{2}\) can be obtained from a standard least squares computer program.

#### Additional Estimates

Henderson's method is not limited to estimating \(\sigma_{0}^{2}\) and \(\sigma_{r}^{2}\). Consider a partitioned mixed linear model with

\[Y=X\beta+Z_{\dagger}\gamma_{\dagger}+Z_{r-1}\gamma_{r-1}+Z_{r}\gamma_{r}+e, \tag{5.7.3}\]

where now \(Z_{\dagger}=[Z_{1},\ldots,Z_{r-2}]\), \(\gamma_{\dagger}^{\prime}=[\gamma_{1}^{\prime},\ldots,\gamma_{r-2}^{\prime}]\),

\[\mathrm{E}(\gamma_{\dagger})=0,\quad\mathrm{E}(\gamma_{r-1})=0,\quad\mathrm{E} (\gamma_{r})=0,\quad\mathrm{E}(e)=0,\]

\[\mathrm{Cov}(\gamma_{\dagger})=D_{\dagger},\quad\mathrm{Cov}(\gamma_{r-1})= \sigma_{r-1}^{2}I_{q(r-1)},\quad\mathrm{Cov}(\gamma_{r})=\sigma_{r}^{2}I_{q(r) },\quad\mathrm{Cov}(e)=\sigma_{0}^{2}I_{n},\]

with all of the random vectors uncorrelated. Again, any variance component model can be written in this form. We now have

\[\mathrm{E}(Y)=X\beta;\qquad\mathrm{Cov}(Y)=V(\theta)=\sigma_{0}^{2}I+Z_{ \dagger}D_{\dagger}Z_{\dagger}^{\prime}+\sigma_{r-1}^{2}Z_{r-1}Z_{r-1}^{\prime }+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}.\]

To get an unbiased estimate of \(\sigma_{r-1}^{2}\), consider \(Y^{\prime}(P_{r-1}-P_{r-2})Y\). In particular

\[\mathrm{E}\big{[}Y^{\prime}(P_{r-1}-P_{r-2})Y\big{]}\] \[=\mathrm{tr}\left\{(P_{r-1}-P_{r-2})\left[\sigma_{0}^{2}I+Z_{ \dagger}D_{\dagger}Z_{\dagger}^{\prime}+\sigma_{r-1}^{2}Z_{r-1}Z_{r-1}^{\prime }+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}\right]\right\}\] \[=\sigma_{0}^{2}\mathrm{tr}[(P_{r-1}-P_{r-2})]+\sigma_{r}^{2} \mathrm{tr}[(P_{r-1}-P_{r-2})Z_{r}Z_{r}^{\prime}]+\sigma_{r-1}^{2}\mathrm{tr} [(P_{r-1}-P_{r-2})Z_{r-1}Z_{r-1}^{\prime}].\]

Write \(t_{0}\equiv\mathrm{tr}[(P_{r-1}-P_{r-2})]\), \(t_{r}\equiv\mathrm{tr}[Z_{r}^{\prime}(P_{r-1}-P_{r-2})Z_{r}]\), and \(t_{r-1}\equiv\mathrm{tr}[Z_{r-1}^{\prime}(P_{r-1}-P_{r-2})Z_{r-1}]\), so that an unbiased estimate becomes

\[\hat{\sigma}_{r-1}^{2}\equiv\frac{Y^{\prime}(P_{r-1}-P_{r-2})Y-\hat{\sigma}_{0 }^{2}\,t_{0}-\hat{\sigma}_{r}^{2}\,t_{r}}{t_{r-1}}.\]

For a general variance component model, to get an unbiased estimate of \(\sigma_{k}^{2}\), consider \(Y^{\prime}(P_{k}-P_{k-1})Y\).

\[\begin{array}{rl}\mathrm{E}\left[Y^{\prime}(P_{k}-P_{k-1})Y\right]&=\mathrm{tr} \left[(P_{k}-P_{k-1})\sum_{j=0}^{r}\sigma_{j}^{2}Z_{j}Z_{j}^{\prime}\right]\\ &=\sigma_{0}^{2}\mathrm{tr}[P_{k}-P_{k-1}]+\sum_{j=k}^{r}\sigma_{j}^{2}\mathrm{ tr}[(P_{k}-P_{k-1})Z_{j}Z_{j}^{\prime}].\end{array}\]

Define \(s_{j}\equiv\mathrm{tr}[Z_{j}^{\prime}(P_{k}-P_{k-1})Z_{j}]\). It is not difficult to get an unbiased estimate using the previous unbiased estimates \(\hat{\sigma}_{0}^{2},\hat{\sigma}_{r}^{2},\hat{\sigma}_{r-1}^{2},\ldots,\hat{ \sigma}_{k+1}^{2}\). In particular,

\[\hat{\sigma}_{k}^{2}\equiv\frac{Y^{\prime}(P_{k}-P_{k-1})Y-\hat{\sigma}_{0}^{2 }\,s_{0}-\sum_{j=k+1}^{r}\hat{\sigma}_{j}^{2}\,s_{j}}{s_{k}}.\]

For estimating more than one variance component, Method 3 is not, in general, well defined. We can permute the labels on the matrices \(Z_{1},\ldots,Z_{r}\) to get another set of estimates.

Example 5.7.3.: _Two-Way ANOVA Without Interaction_.

The fixed effects analysis was discussed in _PA_ Sect. 7.5 and we use similar notation, i.e.,

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+e_{ijk},\]

\(i=1,\ldots,a,\ j=1,\ldots,b\), \(k=1,\ldots,N_{ij}\). In the variance component model we assume that the main effects are random and in particular that the \(\alpha_{i}\)s are independent \(N(0,\sigma_{\alpha}^{2})\) and that the \(\eta_{j}\)s are independent \(N(0,\sigma_{\eta}^{2})\).

We can identify \(\sigma_{r}^{2}\) with \(\sigma_{\eta}^{2}\) so that the sum of squares for fitting the \(\eta\) effects after the \(\alpha\) effects, i.e. \(R(\eta|\mu,\alpha)\), is identified with \(Y^{\prime}(P_{r}-P_{r-1})Y\). This leads to an estimate \(\hat{\sigma}_{\eta,r}^{2}\). Alternatively, we could identify \(\sigma_{r}^{2}\) with \(\sigma_{\alpha}^{2}\) so that the sum of squares for fitting the \(\alpha\) effects after the \(\eta\) effects, i.e. \(R(\alpha|\mu,\eta)\), is identified with \(Y^{\prime}(P_{r}-P_{r-1})Y\). This leads to an estimate \(\hat{\sigma}_{\alpha,r}^{2}\).

We can also identify \(\sigma_{r-1}^{2}\) with \(\sigma_{\eta}^{2}\). Now we would identify \(Y^{\prime}(P_{r-1}-P_{r-2})Y\) with \(R(\eta|\mu)\), the sum of squares for fitting the \(\eta\) effects ignoring the \(\alpha\) effects, leading to an estimate \(\hat{\sigma}_{\eta,r-1}^{2}\). Similarly, we can identify \(\sigma_{r-1}^{2}\) with \(\sigma_{\alpha}^{2}\) and \(Y^{\prime}(P_{r-1}-P_{r-2})Y\) with \(R(\alpha|\mu)\), the sum of squares for fitting the \(\alpha\) effects ignoring the \(\eta\) effects, leading to an estimate \(\hat{\sigma}_{\alpha,r-1}^{2}\).

Typically, \(\hat{\sigma}_{\eta,r}^{2}\neq\hat{\sigma}_{\eta,r-1}^{2}\) and \(\hat{\sigma}_{\alpha,r}^{2}\neq\hat{\sigma}_{\alpha,r-1}^{2}\). When the ANOVA is balanced or has proportional numbers, the estimates will be the same. 

For _nested models_, the order in which variance components can be estimated is restricted. If \(C(Z_{r-1})\subset C(Z_{r})\), we say that \(\gamma_{r}\) is nested within \(\gamma_{r-1}\). This definition of nested effects is somewhat nonstandard. To many people, interaction effects are not considered nested. As used here, interaction effects are nested in more than one effect. Interaction effects are always nested within their main effects and within any lower order interaction effects. In Henderson's method, a nested effect has to have its variance components estimated before that of any effect it is nested within. In general, with \(\gamma_{r}\) is nested within \(\gamma_{r-1}\), we can estimate \(\sigma_{r}^{2}\) first and then use it to estimate \(\sigma_{r-1}^{2}\). The alternative order is not possible. If we desire to estimate \(\sigma_{r-1}^{2}\) first,we require the perpendicular projection operator onto the orthogonal complement of \(C(X,Z_{1},\ldots,Z_{r-2},Z_{r})\) with respect to \(C(X,Z_{1},\ldots,Z_{r})\). Because \(C(Z_{r-1})\subset C(Z_{r})\), we have \(C(X,Z_{1},\ldots,Z_{r-2},Z_{r})=C(X,Z_{1},\ldots,Z_{r})\). The orthogonal complement is the zero space, and the projection matrix is the zero matrix which makes estimation impossible. Thus an interaction effect's variance component must be estimated before any lower order variance components.

**Example 5.7.2**: _Two-Way ANOVA with Interaction._

The fixed effects analysis was discussed in _PA_ Sect. 7.5 and we use similar notation, i.e.,

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\xi_{ij}+e_{ijk},\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), \(k=1,\ldots,N_{ij}\). In the variance component model we assume that the main effects and interaction are random and in particular that the \(\alpha_{i}\)s are independent \(N(0,\sigma_{\alpha}^{2})\), that the \(\eta_{j}\)s are independent \(N(0,\sigma_{\eta}^{2})\), and that the \(\xi_{ij}\)s are independent \(N(0,\sigma_{\xi}^{2})\). In the variance component version of this model we have \(X=J\) and \(Z=[Z_{\alpha},Z_{\eta},Z_{\xi}]\). In this model, \(C(X,Z)=C(Z_{\xi})\).

We can identify \(\sigma_{r}^{2}\) with \(\sigma_{\xi}^{2}\) so that the sum of squares for fitting the "interaction" \(\xi\) effects after the \(\alpha\) and \(\eta\) effects, i.e. \(R(\xi|\eta,\mu,\alpha)\), is identified with \(Y^{\prime}(P_{r}-P_{r-1})Y\). This leads to an estimate of the interaction variance \(\hat{\sigma}_{\xi}^{2}\). In this model, we cannot permute the order of the \(Z_{j}\)s in order to estimate either \(\sigma_{\alpha}^{2}\) or \(\sigma_{\eta}^{2}\) because, due to nesting, that would involve identifying \(Y^{\prime}(P_{r}-P_{r-1})Y\) with either \(R(\alpha|\mu,\eta,\xi)\equiv 0\) or \(R(\eta|\mu,\alpha,\xi)\equiv 0\).

However, we can identify \(\sigma_{r-1}^{2}\) with \(\sigma_{\eta}^{2}\) so that the sum of squares for fitting the \(\eta\) effects after the \(\alpha\) effects, i.e. \(R(\eta|\mu,\alpha)\), is identified with \(Y^{\prime}(P_{r-1}-P_{r-2})Y\). This leads to an estimate \(\hat{\sigma}_{\eta,r-1}^{2}\). Similarly, we can identify \(\sigma_{r-1}^{2}\) with \(\sigma_{\alpha}^{2}\). Now we would identify \(Y^{\prime}(P_{r-1}-P_{r-2})Y\) with \(R(\alpha|\mu,\eta)\), the sum of squares for fitting the \(\alpha\) effects after the \(\eta\) effects, leading to an estimate \(\hat{\sigma}_{\alpha,r-1}^{2}\). Moreover, we can identify \(\sigma_{r-2}^{2}\) with \(\sigma_{\eta}^{2}\) and \(Y^{\prime}(P_{r-2}-P_{r-3})Y\) with \(R(\eta|\mu)\), the sum of squares for fitting the \(\eta\) effects ignoring the \(\alpha\) effects, leading to an estimate \(\hat{\sigma}_{\eta,r-2}^{2}\). Similarly, we can identify \(\sigma_{r-2}^{2}\) with \(\sigma_{\alpha}^{2}\). Now we would identify \(Y^{\prime}(P_{r-2}-P_{r-3})Y\) with \(R(\alpha|\mu)\), the sum of squares for fitting the \(\alpha\) effects ignoring the \(\eta\) effects, leading to an estimate \(\hat{\sigma}_{\alpha,r-2}^{2}\). Typically, \(\hat{\sigma}_{\eta,r-1}^{2}\neq\hat{\sigma}_{\eta,r-2}^{2}\) and \(\hat{\sigma}_{\alpha,r-1}^{2}\neq\hat{\sigma}_{\alpha,r-2}^{2}\). When the ANOVA is balanced or has proportional numbers, the estimates will be the same. \(\Box\)

Another situation in which no estimates exist is if you fit a random main effect with a fixed interaction. One usually argues that it is philosophically nonsensical to have a fixed interaction that corresponds to a random main effect, but the mathematics also degenerate. If \(C(Z_{k})\subset C(X)\), we have \(P_{k}=P_{k-1}\). Even the ML and REML equations degenerate because terms that involve \([I-A]^{\prime}V^{-1}Z_{k}\) can be rewritten as \(V^{-1}[I-A]Z_{k}=0\).

For balanced ANOVA models, Henderson's Method 3 gives unique answers, because all of the effects are either orthogonal (e.g., main effects) or nested (e.g., a two-factor interaction is nested within both of the main effects).

### Exercise 5.5.

Data were generated according to the model

\[y_{ijk}=\mu+\alpha_{i}+\eta_{j}+\gamma_{ij}+e_{ijk},\]

\(i=1,2\), \(j=1,2,3\), \(k=1,\ldots,N_{ij}\), where \(\mathrm{E}(\eta_{j})=\mathrm{E}(\gamma_{ij})=\mathrm{E}(e_{ijk})=0\), \(\mathrm{Var}(e_{ijk})=\sigma_{0}^{2}=64\), \(\mathrm{Var}(\eta_{j})=\sigma_{1}^{2}=784\), and \(\mathrm{Var}(\gamma_{ij})=\sigma_{2}^{2}=25\). All of the random effects were taken independently and normally distributed. The fixed effects were taken as \(\mu+\alpha_{1}=200\) and \(\mu+\alpha_{2}=150\). The data are

\[\begin{array}{c|cccc||c|c c}&\omit\span\omit\span\omit\span\omit\span\omit j \\ i&1&2&3&i&1&2&3\\ \hline 1&250&211&199&2&195&153&131\\ &262&198&184&187&150&133\\ &251&199&200&203&135&135\\ &&198&187&192&\\ &&184&184&209&\\ &&&&&184&\\ \end{array}\]

Estimate the variance components using MINQUE(0), MINQUE(1), and Henderson's Method 3. Estimate the variance components using each of these three sets of estimates as starting values for REML and as starting values for maximum likelihood. For each method of estimation, find estimates of the fixed effects. Compare all the estimates with each other and with the true values. What tentative conclusions can you draw about the relative merits of the estimation procedures?

### Exact \(F\) Tests for Variance Components

In this section we examine two procedures for testing whether a variance component is zero. The first test method is closely related to Henderson's estimation method. In fact, as discussed by Christensen and Bedrick (1999), it is actually quite easy to come up with exact tests of a single variance component.

#### Wald's Test

Seely and El-Bassiouni (1983) considered extensions of Wald's variance component test. They examined the mixed linear model

\[Y=X\beta+Z_{*}\gamma_{*}+Z_{r}\gamma_{r}+e \tag{5.8.1}\]

which is similar to model (5.7.1). Here, \(Z_{*}\) and \(Z_{r}\) are, respectively, \(n\times q_{*}\) and \(n\times q(r)\) matrices of known quantities. \(\gamma_{*}\), \(\gamma_{r}\), and \(e\) are independent random vectors with\[\gamma_{*}\sim N(0,D_{*}),\quad\gamma_{r}\sim N(0,\sigma_{r}^{2}I_{q(r)}),\quad e \sim N(0,\sigma_{0}^{2}I_{n}).\]

The null hypothesis \(H_{0}:\sigma_{r}^{2}=0\) can be tested by using ordinary least squares calculations treating the \(\gamma\)s as fixed effects. Let \(SSE(1)\) be the sum of squares error from fitting model (5.8.1). The degrees of freedom error are \(dfE(1)\). Also let \(SSE(2)\) be the sum of squares error from the least squares fit of

\[Y=X\beta+Z_{*}\gamma_{*}+e \tag{5.8.2}\]

with degrees of freedom error \(dfE(2)\). The Wald test is simply based on the fact that under \(H_{0}\)

\[\frac{[SSE(2)-SSE(1)]/[dfE(2)-dfE(1)]}{SSE(1)/dfE(1)}\sim F[dfE(2)-dfE(1),dfE(1 )].\]

Mohammad Hattab has pointed out to me that when \(\gamma_{*}\) and \(\gamma_{r}\) have a joint multivariate normal distribution, even if they are not independent, the proof given below continues to hold. In longitudinal models it is not uncommon to assume that different individuals are independent but that each individual's results involve a linear time effect with random slope and intercept wherein the slope and intercept are correlated. This observation allows one to perform a Wald test of whether the variance of the slope or intercept is 0. Extensions to other longitudinal models with independent subjects but multiple correlated random effects within subjects are obvious.

Mohammad also pointed out that this statistic can be used to test \(D_{r}=0\) whenever \(\gamma_{r}\sim N(0,D_{r})\), although it would probably be better to view it as a test of whether \(\operatorname{tr}[Z_{r}^{\prime}(P_{r}-P_{*})Z_{r}D_{r}]=0\), since \(\operatorname{E}[Y^{\prime}(P_{r}-P_{*})Y]=\sigma_{0}^{2}\operatorname{tr}(P_ {r}-P_{*})+\operatorname{tr}[Z_{r}^{\prime}(P_{r}-P_{*})Z_{r}D_{r}]\).

Of course, if the two model matrix column spaces are the same, that is, if \(C(X,Z_{*},Z_{r})=C(X,Z_{*})\), then there is no test because both the numerator sum of squares and degrees of freedom are zero. For example, in a two-way ANOVA with interaction and both main effects random (so that the interaction must also be random), this provides only a test of whether the interaction variance component is zero. Neither of the main effect variance components could be tested this way. On the other hand, in a two-way ANOVA without interaction and both main effects random, the procedure can be applied in different ways to test whether each main effect variance component is zero. The next subsection addresses a very general way of testing whether a main effect variance component is zero when the interaction variance component is not.

We now verify the distribution given above. Note that under the mixed model,

\[\operatorname{Cov}(Y)\equiv V(\theta)=\sigma_{0}^{2}I+Z_{*}D_{*}Z_{*}^{\prime }+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}.\]

Let \(P_{*}=P_{r-1}\) be the perpendicular projection operator onto the column space \(C(X,Z_{*})\), and let \(P_{r}\) be the perpendicular projection operator onto \(C(X,Z_{*},Z_{r})\). It follows that \(SSE(2)-SSE(1)=Y^{\prime}(P_{r}-P_{*})Y\), \(dfE(2)-dfE(1)=r(P_{r}-P_{*})\), \(SSE(1)=Y^{\prime}(I-P_{r})Y\), and \(dfE(1)=r(I-P_{r})\). Note that \(P_{r}Z_{*}=Z_{*}\) and \(P_{r}P_{*}=P_{*}\). We need to show 1. that \(Y^{\prime}(I-P_{r})Y/\sigma_{0}^{2}\sim\chi^{2}[r(I-P_{r})]\),
2. that \(Y^{\prime}(I-P_{r})Y\) and \(Y^{\prime}(P_{r}-P_{*})Y\) are independent, and
3. that, under \(H_{0}\), \(Y^{\prime}(P_{r}-P_{*})Y/\sigma_{0}^{2}\sim\chi^{2}[r(P_{r}-P_{*})]\).

Using results from \(PA\) Sect. 1.3, we need only show that \(\sigma_{0}^{-4}(I-P_{r})V(I-P_{r})=\sigma_{0}^{-2}(I-P_{r})\), that \((I-P_{r})V(P_{r}-P_{*})=0\), and that, when \(\sigma_{r}^{2}=0\), \(\sigma_{0}^{-4}(P_{r}-P_{*})V(P_{r}-P_{*})=\sigma_{0}^{-2}(P_{r}-P_{*})\). Verifying these results involves only straightforward linear algebra along with properties of projection operators. In general, the distribution of \(Y^{\prime}(P_{r}-P_{*})Y\) seems to be intractable without the assumption that \(\sigma_{r}^{2}=0\), but see Sect. 5.3.

To facilitate extensions of this test in the next subsection, we use a simple generalization of the Seely and El-Bassiouni results. The mixed model considered in (5.8.1) can also be written as

\[Y\sim N\big{(}X\beta,\sigma_{0}^{2}I+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}+Z_{*}D_ {*}Z_{*}^{\prime}\big{)}\,,\]

so the test applies for any data with a distribution of this form. If we let \(\Sigma\) be any known nonnegative definite matrix, the method also applies to

\[Y\sim N\big{(}X\beta,\sigma_{0}^{2}\Sigma+\sigma_{r}^{2}Z_{r}Z_{r}^{\prime}+Z_ {*}D_{*}Z_{*}^{\prime}\big{)}\,. \tag{5.8.3}\]

Simply write \(\Sigma=QQ^{\prime}\). Then there exists a matrix \(T\) such that \(TQ=I\); so

\[TY\sim N\big{(}TX\beta,\sigma_{0}^{2}I+\sigma_{r}^{2}(TZ_{r})(TZ_{r})^{\prime} +(TZ_{*})D_{*}(TZ_{*})^{\prime}\big{)}\,,\]

and the method applies to \(TY\). Obviously, for \(\Sigma\) positive definite, \(T=Q^{-1}\). _The test based on (5.8.3) is simply the standard generalized least squares test of model (5.8.2) versus (5.8.1) when \(\operatorname{Cov}(Y)=\sigma_{0}^{2}\Sigma\), see \(PA\) Sect. 3.8._

#### 5.8.2 Ofversten's Second Method

Consider again model (5.7.3),

\[Y=X\beta+Z_{\dagger}\gamma_{\dagger}+Z_{r-1}\gamma_{r-1}+Z_{r}\gamma_{r}+e,\]

where now

\[\gamma_{\dagger}\sim N(0,D_{\dagger})\,,\quad\gamma_{r-1}\sim N\big{(}0, \sigma_{r-1}^{2}I_{q(r-1)}\big{)}\,,\quad\gamma_{r}\sim N\big{(}0,\sigma_{r}^ {2}I_{q(r)}\big{)}\,,\quad e\sim N\left(0,\sigma_{0}^{2}I_{n}\right),\]

with all of the random vectors independent. The object of the second method is to obtain an exact \(F\) test for \(H_{0}:\sigma_{r-1}^{2}=0\).

Example 5.8.1. _Two-Way ANOVA with Interaction_.

The fixed effects analysis was discussed in \(PA\) Sect. 7.5 and we use similar notation, i.e.,\[y_{i,jk}=\mu+\alpha_{i}+\eta_{j}+\xi_{ij}+e_{ijk},\]

\(i=1,\ldots,a\), \(j=1,\ldots,b\), \(k=1,\ldots,N_{ij}\). In this variance component model we assume that the \(\alpha\) main effects are fixed, but that the \(\eta_{j}\)s and \(\xi_{ij}\)s are independent and random and in particular that the \(\eta_{j}\)s are independent \(N(0,\sigma_{\eta}^{2})\), and that the \(\xi_{ij}\)s are independent \(N(0,\sigma_{\xi}^{2})\). We now have \(X=[J,X_{\alpha}]\) and \(Z=[Z_{\eta},Z_{\xi}]\) where \(X_{\alpha}\), \(Z_{\eta}\), and \(Z_{\xi}\) consist of columns of indicators for the subscripted effect. We want to test \(H_{0}:\sigma_{\eta}^{2}=0\). In a balanced ANOVA it turns out that an \(F\) test statistic is just \(MS(\eta)/MS(\xi)\) where the terms are defined as for a fixed effects model. It is not clear how to do a test if the data are unbalanced. 

The notation for developing this test in a general partitioned model gets pretty hairy. To simplify, let's look at the model

\[Y=X\beta+Z_{1}\gamma_{1}+Z_{2}\gamma_{2}+e, \tag{5.8.4}\]

where now

\[\gamma_{1}\sim N\left(0,\sigma_{1}^{2}I_{q(1)}\right),\quad\gamma_{2}\sim N \left(0,\sigma_{2}^{2}I_{q(2)}\right),\quad e\sim N\left(0,\sigma_{0}^{2}I_{n }\right).\]

It will be easy to see that the test development works just as well when \(\beta\) contains some random effects. In this model we are interested in testing whether \(\sigma_{1}^{2}=0\).

Generalizing the ANOVA example, the test is of primary interest when \(C(X,Z_{1})\subset C(X,Z_{2})\), e.g., main effects \(\gamma_{1}\) are nested within interaction \(\gamma_{2}\). If \(C(X,Z_{1})\not\subset C(X,Z_{2})\), a Wald test of \(H_{0}:\sigma_{1}^{2}=0\) is available from the previous subsection by simply interchanging the roles of \(Z_{1}\gamma_{1}\) and \(Z_{2}\gamma_{2}\). If \(C(X,Z_{1})\subset C(X,Z_{2})\), this interchange does not provide a test because \(C(X,Z_{2})=C(X,Z_{1},Z_{2})\). As developed here, if Ofversten's second method provides a test, that test is valid regardless of the relationship of \(C(X,Z_{1})\) and \(C(X,Z_{2})\).

Ofversten's (1993) second method as presented in Christensen (1996) involves using an orthonormal basis. For example, use Gram-Schmidt on the columns of \([X,Z_{2},Z_{1},I_{n}]\) to obtain an orthonormal basis for \(\mathbf{R}^{n}\), say \(c_{1},\ldots,c_{n}\). Write these as columns of a matrix \(C=[c_{1},\ldots,c_{n}]\). Partition \(C\) as \(C=[C_{1},C_{2},C_{3},C_{4}]\), where the columns of \(C_{1}\) are an orthonormal basis for \(C(X)\), the columns of \(C_{2}\) are an orthonormal basis for the orthogonal complement of \(C(X)\) with respect to \(C(X,Z_{2})\), the columns of \(C_{3}\) are an orthonormal basis for the orthogonal complement of \(C(X,Z_{2})\) with respect to \(C(X,Z_{2},Z_{1})\), and the columns of \(C_{4}\) are an orthonormal basis for the orthogonal complement of \(C(X,Z_{2},Z_{1})\). Note that if \(C(X,Z_{1})\subset C(X,Z_{2})\), then \(C_{3}\) is vacuous.

Note that \(M=C_{1}C_{1}^{\prime}\). We define \(M_{2}\equiv M+C_{2}C_{2}^{\prime}\) to be the ppo onto \(C(X,Z_{2})\) and \(P\equiv P_{2}=M_{2}+C_{3}C_{3}^{\prime}\) is the ppo onto \(C(X,Z_{1},Z_{2})\). Finally, \(C_{4}C_{4}^{\prime}=I-P\).

The basic idea of this method is to choose a matrix \(K\) so that the extended Wald's test for model (5.8.3) can be applied to \(C_{2}^{\prime}Y+KC_{4}^{\prime}Y\). In executing the test of \(H_{0}:\sigma_{1}^{2}=0\), \(\sigma_{1}^{2}\) plays the role assigned to \(\sigma_{r}^{2}\) in (5.8.3), a function of \(\sigma_{0}^{2}\) and \(\sigma_{2}^{2}\) plays the role assigned to \(\sigma_{0}^{2}\) in (5.8.3), and the role of \(D_{1}\) in (5.8.3) is vacuous. In particular, for some number \(\lambda\) and some matrix \(K\), we want to have \[C_{2}^{\prime}Y+KC_{4}^{\prime}Y\sim N\big{[}0,(\sigma_{2}^{2}+\sigma_{0}^{2}/ \lambda)C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}+\sigma_{1}^{2}C_{2}^{\prime}Z_{1 }Z_{1}^{\prime}C_{2}\big{]}\,. \tag{5.8.5}\]

This is of the form (5.8.3). As shown in Exercise 5.6, \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}\) is a positive definite matrix, so the test follows immediately from generalized least squares. The test cannot be performed if \(C_{2}^{\prime}Z_{1}=0\), which occurs, for example, if \(C(Z_{1})\subset C(X)\). Note that \(C_{4}C_{4}^{\prime}Y\) is the vector of residuals from treating the random \(\gamma\) effects as fixed. Thus, in using \(KC_{4}^{\prime}Y=KC_{4}^{\prime}C_{4}C_{4}^{\prime}Y\) we are using some of the residual variability to construct the test.

To get the degrees of freedom for the test, we identify correspondences between (5.8.3) and (5.8.5). There are \(r(C_{2})\) "observations" available in (5.8.5). In (5.8.3), the numerator degrees of freedom for the test are \(r(X,Z_{1},Z_{2})-r(X,Z_{1})\). With mean zero in (5.8.5) there is no linear mean structure, i.e, nothing corresponding to \(X\) in (5.8.3), \(Z_{1}\) from (5.8.3) is also vacuous in (5.8.5), and \(C_{2}^{\prime}Z_{1}\) is playing the role of \(Z_{2}\) in (5.8.3). Thus the numerator degrees of freedom for the test are \(r(C_{2}^{\prime}Z_{1})\) and the denominator degrees of freedom are \(r(C_{2})-r(C_{2}^{\prime}Z_{1})\). In model (5.8.5), \(r(C_{2})=r(X,Z_{2})-r(X)\). If \(C(Z_{1})\subset C(X,Z_{2})\), it is shown in Exercise 5.6 that \(r(C_{2}^{\prime}Z_{1})=r(X,Z_{1})-r(X)\) and the degrees of freedom for the test are \(r(X,Z_{1})-r(X)\) and \(r(X,Z_{1},Z_{2})-r(X,Z_{1})\), respectively. In the two-way example, these numbers are \((a+b-1)-a=b-1\) and \(ab-(a+b-1)=(a-1)(b-1)\) as they are in the balanced ANOVA \(F\) test.

Observe that

\[C_{2}^{\prime}Y\sim N\big{(}0,\sigma_{0}^{2}I+\sigma_{2}^{2}C_{2}^{\prime}Z_{2 }Z_{2}^{\prime}C_{2}+\sigma_{1}^{2}C_{2}^{\prime}Z_{1}Z_{1}^{\prime}C_{2}\big{)}\,.\]

In many interesting cases, \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=\lambda I\); in which case an ordinary least squares Wald test can be applied immediately without any use of \(C_{4}^{\prime}Y\) as long as \(C_{2}^{\prime}Z_{1}\neq 0\). In the two-way ANOVA example, \(C(Z_{2})=C(X,Z_{1},Z_{2})\), so \(C(C_{2})\subset C(Z_{2})\). In the balanced case, \(N_{ij}\equiv N\), \(Z_{2}Z_{2}^{\prime}=NP\), so \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=NI\).

It is not difficult to see that in balanced ANOVA problems either \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=\lambda I\) when a standard balanced ANOVA test is available, or \(C_{2}^{\prime}Z_{1}=0\) when such a test is not available. For example, consider (5.8.4) as modeling a balanced two-way ANOVA without interaction \(y_{ijk}=\mu+\gamma_{li}+\gamma_{2j}+e_{ijk}\), \(i=1,\ldots,q(1)\), \(j=1,\ldots,q(2)\), \(k=1,\ldots,N\), with \(X\beta\) being the vector \(J\mu\). The \(\gamma_{1}\) and \(\gamma_{2}\) treatments are often said to be orthogonal. Letting \(M\) be the perpendicular projection operator onto \(C(X)\), this orthogonality means precisely that \(C[(I-M)Z_{1}]\) and \(C[(I-M)Z_{2}]\) are orthogonal, i.e., \(Z_{2}^{\prime}(I-M)Z_{1}=0\), cf. \(PA\) Sect. 7.1. Now, by the definition of \(C_{2}\), \(C(C_{2})=C[(I-M)Z_{2}]\), so \(C_{2}^{\prime}Z_{1}=0\) iff \(Z_{2}^{\prime}(I-M)Z_{1}=0\), which we know is true from orthogonality. Hence, no test of \(H_{0}:\sigma_{1}^{2}=0\) is available _from this method_. Of course the standard Wald test of the previous subsection does provide a test here.

Now consider a balanced nested model \(y_{ijk}=\mu+\gamma_{li}+\gamma_{2ij}+e_{ijk}\) with \(i\), \(j\), and \(k\) as above. In this model, \(C(X,Z_{1})\subset C(Z_{2})\) and \(\frac{1}{N}Z_{2}Z_{2}^{\prime}\equiv P\) is the perpendicular projection operator onto \(C(Z_{2})\). Observing that \(C(C_{2})\subset C(Z_{2})\) and using the orthonormality of the columns of \(C_{2}\),

\[C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=NC_{2}^{\prime}PC_{2}=NC_{2}^{\prime}C_ {2}=NI.\]Thus an ordinary least squares Wald test based on \(C_{2}^{\prime}Y\) is available. Given that a Wald test simply compares models in the usual way, for \(H_{0}:\sigma_{1}^{2}=0\) this test is simply the standard balanced ANOVA test for no fixed \(\gamma_{1i}\) effects when \(\gamma_{2}\) is random, i.e, \(MS(\gamma_{1})/MS(\gamma_{2})\). Similar orthogonality and containment results hold in more general balanced ANOVAs. For the special case of \(C(Z_{1})\subset C(X,Z_{2})\) with \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=\lambda I\), a general explicit form for the test statistic is given in (5.8.8).

In general, \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}\neq\lambda I\), so the test requires \(C_{4}^{\prime}Y\). If \(r(X,Z_{2},Z_{1})\equiv t\),

\[C_{4}^{\prime}Y\sim N\big{(}0,\sigma_{0}^{2}I_{n-t}\big{)}\,.\]

It is easy to see that \(C_{2}^{\prime}VC_{4}=0\), so \(C_{2}^{\prime}Y\) and \(C_{4}^{\prime}Y\) are independent. To obtain (5.8.5), simply pick \(K\) so that

\[KC_{4}^{\prime}Y\sim N\big{(}0,\sigma_{0}^{2}\left[\lambda^{-1}C_{2}^{\prime} Z_{2}Z_{2}^{\prime}C_{2}-I\right]\big{)}\,.\]

Obviously one can do this provided that \(\lambda^{-1}C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}-I\) is a nonnegative definite matrix. \(\lambda\) is chosen to ensure that the matrix is nonnegative definite. By the choice of \(C_{2}\), \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}\) is a positive definite matrix and \(\lambda\) is taken as its smallest eigenvalue. Then if we use the eigenvalue-eigenvector decomposition \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=WD(\lambda_{i})W^{\prime}\) with \(W\) orthonormal,

\[\lambda^{-1}C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}-I=WD\bigg{(}\frac{\lambda_{ i}}{\lambda}-1\bigg{)}\,W^{\prime},\]

which is clearly nonnegative definite.

Note that this development makes it obvious why \(\lambda\) needs to be the smallest eigenvalue. Actually, the test would still work if \(\lambda\) were chosen to be any positive number less than the smallest eigenvalue, but we want \(KC_{4}^{\prime}Y\) to increase the variability of \(C_{2}^{\prime}Y\) as little as possible, and this is accomplished by taking \(\lambda\) as large as possible. In particular, choosing \(\lambda\) as the smallest eigenvalue gives \(KC_{4}^{\prime}Y\) a singular covariance matrix and thus no variability in at least one direction. Other valid choices of \(\lambda\) can only increase variability.

Also note that \(KC_{4}^{\prime}Y=0\) a.s. if the eigenvalues of \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}\) all happen to be the same. In this case, \(\lambda^{-1}C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}-I=0\) and \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=\lambda I\), so we get the simpler Wald test alluded to earlier.

The difficulty with the second method is that \(K\) is not unique, and typically the results of the test depend on the choice of \(K\). In particular, \(K\) is a \(w\times(n-t)\) matrix, where typically \(w\equiv r(X,Z_{2})-r(X)<(n-t)\), while \(\lambda^{-1}C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}-I\) is a \(w\times w\) matrix. Thus, we can take \(K=\left[WD\bigg{(}\sqrt{\frac{\lambda_{i}}{\lambda}-1}\bigg{)}\,,0\right]\) or \(K=\left[0,WD\bigg{(}\sqrt{\frac{\lambda_{i}}{\lambda}-1}\bigg{)}\right]\) or any number of other matrices. Modifying Ofversten, a reasonable procedure might be just to pick one of these convenient \(K\) matrices, but first randomly permute the rows of \(C_{4}^{\prime}Y\).

This method applies quite generally. A proof consists of observing that the test of \(H_{0}:\sigma_{1}^{2}=0\) remains valid when \(X=[X_{1},X_{2}]\), \(\beta=(\beta_{1}^{\prime},\beta_{2}^{\prime})^{\prime}\) with \(\beta_{2}\sim N(0,D_{2})\), and \(\beta_{2}\) independent of \(\gamma_{1}\) and \(\gamma_{2}\). Relative to the model at the beginning of the subsection,make the identifications \(X=[X_{1},X_{2}]\leftarrow[X,Z_{\uparrow}]\), \(\beta=(\beta^{\prime}_{1},\beta^{\prime}_{2})^{\prime}\leftarrow(\beta^{\prime}, \gamma^{\prime}_{\dagger})^{\prime}\) with \(\gamma_{\dagger}\leftarrow\gamma_{r-1}\) and \(\gamma_{2}\leftarrow\gamma_{r}\) for a test of \(H_{0}:\sigma^{2}_{r-1}=0\). This model allows for interaction between two random factors and arbitrary numbers of factors.

To repeat, this method will be most useful when \(C(X,Z_{1})\subset C(X,Z_{2})\); if this is not the case, the simpler Wald test is available. Whenever \(C^{\prime}_{2}Z_{1}=0\), no test is available. For example, this will occur whenever \(C(Z_{1})\subset C(X)\), which is precisely what happens when one tries to test the variance component of a random main effect in a two-way analysis of variance with a fixed interaction.

#### Comparison of Tests

When \(C(Z_{1})\not\subset C(X,Z_{2})\) in model (5.8.4), we have two tests of \(H_{0}:\sigma^{2}_{1}=0\) available. Let \(M\), \(M_{2}\), \(P\), and \(P_{1}\) be perpendicular projection matrices onto \(C(X)\), \(C(X,Z_{2})\), \(C(X,Z_{2},Z_{1})\), and \(C(X,Z_{1})\), respectively. The simple Wald test has the \(F\) statistic

\[F=\frac{Y^{\prime}(P-M_{2})Y/r(P-M_{2})}{Y^{\prime}(I-P)Y/r(I-P)}.\]

See Lin and Harville (1991) and Christensen and Bedrick (1999) for some alternatives to Wald's test other than the second method just developed.

It can be of interest to examine the power (probability of rejecting the test) under some alternative to the null model, e.g., the model when the null hypothesis is false. The power of this test is quite complicated, but for given values of the parameters the power can be computed as in Davies (1980). Software is available through STATLIB. See also Christensen and Bedrick (1997).

Intuitively, the power depends in part (and only in part) on the degrees of freedom, \(r(X,Z_{2},Z_{1})-r(X,Z_{2})\), \(n-r(X,Z_{2},Z_{1})\) and the ratio of the expected mean squares,

\[1+\frac{\sigma^{2}_{1}}{\sigma^{2}_{0}}\frac{\operatorname{tr}\left[Z^{\prime}_ {1}(P-M_{2})Z_{1}\right]}{r(X,Z_{2},Z_{1})-r(X,Z_{2})}. \tag{5.8.6}\]

The basic idea behind \(F\) tests is that under the null hypothesis the test statistic is the ratio of two estimates of a common variance. Obviously, since the two are estimating the same thing under \(H_{0}\), the ratio should be about 1. The \(F\) distribution quantifies the null variability about 1 for this ratio of estimates. If the numerator and denominator are actually estimates of very different things, the ratio should deviate substantially from the target value of 1. In fixed effects models, the power of an \(F\) test under the full model is simply a function of the ratio of expected values of the two estimates and the degrees of freedom of the estimates. In mixed models, the power is generally much more complicated, but the ratio of expected values can still provide some insight into the behavior of the tests. The ratio in (5.8.6) is strictly greater than one whenever the test exists and \(\sigma^{2}_{1}>0\), thus indicating that larger values of the test statistic can be expected under the alternative. The power of the test should tend to increase as this ratio increases but in fact the power is quite complicated. Note that \(Y^{\prime}(P-M_{2})Y=Y^{\prime}C_{3}C_{3}^{\prime}Y\) and \(Y^{\prime}(I-P)Y=Y^{\prime}C_{4}C_{4}^{\prime}Y\); so this test uses only \(C_{3}^{\prime}Y\) and \(C_{4}^{\prime}Y\).

The second test is based on \(C_{2}^{\prime}Y+KC_{4}^{\prime}Y\). Again, exact powers can be computed as in Davies (1980). As shown in Exercise 5.6, the ratio of the expected mean squares for the second test is

\[1+\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}+\sigma_{0}^{2}/\lambda}\frac{\mbox{tr} \left[(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2})^{-1}C_{2}^{\prime}Z_{1}Z_{1}^{ \prime}C_{2}\right]}{r(C_{2}^{\prime}Z_{1})}. \tag{5.8.7}\]

Again, this is strictly greater than one whenever the test exists and \(\sigma_{1}^{2}>0\).

The degrees of freedom for the second test were given earlier. To compare the degrees of freedom for the two tests, observe that

\[C(X)\subset C(X,\{M_{2}-M\}Z_{1})=C(X,M_{2}Z_{1})\subset C(X,Z_{2})\subset C( X,Z_{2},Z_{1}).\]

The degrees of freedom for the second test are, respectively, the ranks of the orthogonal complement of \(C(X)\) with respect to \(C(X,\{M_{2}-M\}Z_{1})\) and the orthogonal complement of \(C(X,\{M_{2}-M\}Z_{1})\) with respect to \(C(X,Z_{2})\). (The first orthogonal complement is \(C(C_{2}C_{2}^{\prime}Z_{1})\) with the same rank as \(C_{2}^{\prime}Z_{1}\), and the second orthogonal complement has rank \(r(X,Z_{2})-[r(X)+r(C_{2}^{\prime}Z_{1})]\).) The degrees of freedom for the simple Wald test are, respectively, the ranks of the orthogonal complement of \(C(X,Z_{2})\) with respect to \(C(X,Z_{2},Z_{1})\) and the orthogonal complement of \(C(X,Z_{2},Z_{1})\). In practice, the simple Wald test would typically have an advantage in having larger denominator degrees of freedom, but that could be outweighed by other factors in a given situation. We also see that, in some sense, the second test is being constructed inside \(C(X,Z_{2})\); it focuses on the overlap of \(C(X,Z_{2})\) and \(C(Z_{1})\). On the other hand, the simple Wald test is constructed from the overlap of \(C(Z_{1})\) with the orthogonal complement of \(C(X,Z_{2})\).

In the special case of \(C(Z_{1})\subset C(X,Z_{2})\) with \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=\lambda I\), the second method gives the test statistic

\[F=\frac{Y^{\prime}(P_{1}-M)Y/r(P_{1}-M)}{Y^{\prime}(M_{2}-P_{1})Y/r(M_{2}-P_{1 })}. \tag{5.8.8}\]

See Exercise 5.6 for a proof. For example, in a two-way ANOVA, \(X\) can indicate the grand mean and a fixed main effect, \(Z_{1}\) can indicate the random main effect to be tested, and \(Z_{2}\) can indicate the interaction. When the two-way is balanced, \(C_{2}^{\prime}Z_{2}Z_{2}^{\prime}C_{2}=\lambda I\) and we have the traditional test, i.e., the mean square for the random main effect divided by the mean square for interaction.

It should be noted that under \(H_{0}:\sigma_{1}^{2}=0\), \(C_{3}^{\prime}Y\) also has a \(N(0,\sigma_{0}^{2}I)\) distribution so it could also be used, along with \(C_{4}^{\prime}Y\), to adjust the distribution of \(C_{2}^{\prime}Y\) and still maintain a valid \(F\) test. However, this would be likely to have a deleterious effect on the power since then both the expected numerator mean square and the expected denominator mean square would involve positive multiples of \(\sigma_{1}^{2}\) under the alternative.

[MISSING_PAGE_FAIL:210]

* Christensen & Bedrick (1999) Christensen, R., & Bedrick, E. J. (1999). A survey of some new alternatives to Wald's variance component test. _Tatra Mountains Mathematical Publications, 17_, 91-102.
* Davies (1980) Davies, R. B. (1980). The distribution of linear combinations of \(\chi^{2}\) random variables. _Applied Statistics, 29_, 323-333.
* Henderson (1953) Henderson, C. R. (1953). Estimation of variance and covariance components. _Biometrics, 9_, 226-252.
* Hodges (2014) Hodges, J. S. (2014). _Richly parameterized linear models: Additive, time series, and spatial models using random effects_. Boca Raton, FL: Chapman and Hall/CRC.
* Jiang (2007) Jiang, J. (2007). _Linear and generalized linear mixed models and their applications_. New York: Springer.
* Khuri et al. (1998) Khuri, A. I., Mathew, T., & Sinha, B. K. (1998). _Statistical tests for mixed linear models_. New York: Wiley.
* Lin & Harville (1991) Lin, T.-H., & Harville, D. A. (1991). Some alternatives to Wald's confidence interval and test. _Journal of the American Statistical Association, 86_, 179-187.
* McCulloch et al. (2008) McCulloch, C. E., Searle, S. R., & Neuhaus, J. M. (2008). _Generalized, linear, and mixed models_ (2nd ed.). New York: Wiley.
* Ofversten (1993) Ofversten, J. (1993). Exact tests for variance components in unbalanced linear models. _Biometrics, 49_, 45-57.
* Pearce & Wand (2006) Pearce, N. D., & Wand, M. P. (2006). Penalized splines and reproducing kernel methods. _The American Statistician, 60_, 233-240.
* Searle et al. (1992) Searle, S. R., Casella, G., & McCulloch, C. (1992). _Variance components_. New York: Wiley.
* Seely & El-Bassiouni (1983) Seely, J. F. & El-Bassiouni, Y. (1983). Applying Wald's variance component test. _The Annals of Statistics, 11_, 197-201.

## Chapter 6 Frequency Analysis of Time Series

**Abstract** This chapter examines the linear mixed models from Chap. 5 that have traditionally been used to analyze time series data. It also examines spectral distributions/densities and linear filtering of time series.

Consider a sequence of observations \(y_{1},y_{2},\ldots,y_{n}\) taken at equally spaced time intervals. Some of the many sources of such data are production from industrial units, national economic data, and regularly recorded biological data (e.g., blood pressures taken at regular intervals). The distinguishing characteristic of time series data is that, because data are being taken on the same object over time, the individual observations are correlated.

Example 6.0.1. Tukey (1977) reported data, extracted from _The World Almanac_, on the yearly production of bituminous coal in the United States between 1920 and 1968. In 1969, the method of reporting the production figures changed. Bituminous coal was subdivided into bituminous, sub-bituminous, and lignite. These three figures were combined to yield the data in Table 1 and Fig. 1 Coal production was high during the economic boom of the 1920s, low during the great depression, high again during World War II, dropped after the war, and grew from the 1960s to 1980. \(\Box\)

A key feature of time series data is that they are often _cyclical_ in nature. For example, retail sales go up every year at Christmas. The number of people who vacation in Montana goes up every summer, down in the fall and spring, and up during the ski season. Time series analysis is designed to model cyclical elements.

There are two main schools of time series analysis: _frequency domain_ analysis and _time domain_ analysis. The frequency domain can be viewed as regression on independent variables that isolate the frequencies of the cyclical behavior. The regression predictor variables are cosines and sines evaluated at known frequencies and times. Most properly, the regression coefficients are taken as random variables, so the appropriate linear model is a mixed model as in Chap. 5. The justification for this approach is based on a very powerful result in probability theory called the Spectral Representation Theorem. Frequency domain analysis is the subject of this chapter.

Chapter 7 discusses time domain analysis. Time domain analysis involves the modeling of observed time series as processes generated by a series of random errors. An important idea in the time domain is that of an autoregressive model. For example, the autoregressive model of order 2 is

\[y_{t}=\beta_{1}y_{t-1}+\beta_{2}y_{t-2}+e_{t}, \tag{6.0.1}\]

\begin{table}
\begin{tabular}{|c|c c c c c c c|} \hline  & 1920 & 1930 & 1940 & 1950 & 1960 & 1970 & 1980 \\ \hline
0 & 569 & 468 & 461 & 516 & 416 & 602.9 & 823.7 \\
1 & 416 & 382 & 511 & 534 & 403 & 552.2 & \\
2 & 422 & 310 & 583 & 467 & 422 & 595.3 & \\
3 & 565 & 334 & 590 & 457 & 459 & 591.7 & \\
4 & 484 & 359 & 620 & 392 & 467 & 603.4 & \\
5 & 520 & 372 & 578 & 467 & 512 & 648.4 & \\
6 & 573 & 439 & 534 & 500 & 534 & 678.7 & \\
7 & 518 & 446 & 631 & 493 & 552 & 691.3 & \\
8 & 501 & 349 & 600 & 410 & 545 & 665.2 & \\
9 & 505 & 395 & 438 & 412 & 560.5 & 776.3 & \\ \hline \end{tabular} Values are in millions of short tons.

\end{table}
Table 6.1: Coal production data

Figure 6.1: Coal production data: 1920â€“1980

where the \(e_{t}\)s are uncorrelated errors. The current observation \(y_{t}\) is being regressed on the previous two observations. Because the model matrix for model (6.0.1) consists of random observations, it does not satisfy the standard linear model assumption that the model matrix is fixed. Recall from Chap. 4 that regression is closely related to best linear prediction. Prediction theory is based on having random predictors such as in model (6.0.1). Best linear prediction is an important tool in time domain analysis.

In addition to cyclical elements, there may also be _trends_ in the data. For example, from year to year, the retail sales at Christmas time may display a relatively steady increase. The blood pressure of an overweight person on a diet may display a steady tendency to decrease. This is an important aspect of the analysis of time series. One way to handle trend is to remove it prior to analyzing the cyclical behavior of the time series. Another way of viewing, say, an increasing trend is to consider it as a cycle for which the downturn is nowhere in sight. In this view, trend is just a very slowly oscillating cycle. One can also fit linear models with both cycles and trends but that requires the more general ideas of Chap. 8.

Time series analysis is a large and important subject and there is a huge literature available. The books by Shumway and Stoffer (2011), Brockwell and Davis (1991, 2002) and Fuller (1996) discuss both the frequency and time domains. Prado and West (2010) devote roughly 100 pages to these topics and then get into more advanced models including state space models. Bloomfield (1976) and Koopmans (1974) are devoted to the frequency domain. At a more advanced level are the frequency domain books by Brillinger (1981) and Hannan (1970). Many other books are also available that examine frequency domain ideas.

It is common these days to combine time series analysis and spatial data analysis. One notable book is Cressie and Wikle (2011).

We begin with an introduction to stationary processes. This is followed in Sect. 6.2 with some basic ideas on analyzing data to identify important frequencies. The methods and models of Sect. 6.2 are related to stochastic processes in Sect. 6.3. The relationship suggests that random effects models are more appropriate than fixed effects models for stationary time series. Two random effects models are examined in Sects. 6.4 and 6.5: one with and one without uncorrelated residual errors.

A traditional data-analytic tool for time series is linear filtering. In Sect. 6.6, various types of linear filters are defined and their frequency properties are examined. The linear filters discussed in Sect. 6.6 are the basis for the time domain models of Chap. 7. The chapter closes with some ideas on the relationship between two different time series and a discussion of the commonly used (by other people) Fourier series notation for data analysis.

### 6.1 Stationary Processes

One fruitful approach to modeling time series data is through the use of stationary processes. A stationary random process is just a group of random variables that exhibit a property called stationarity. In a sense, they remain the same, i.e., stationary.

Typically, the groups of random variables are large: either countably or uncountably infinite. In this chapter and the next, we consider countable sequences of random variables that begin at a fixed time, say, \(y_{1},y_{2},y_{3},\ldots\) or, more generally, doubly infinite sequences \(\ldots,y_{-1},y_{0},y_{1},\ldots\).

In Chap. 8, we consider random variables \(y(u)\) for arbitrary vectors \(u\) in a subset of \(\mathbf{R}^{d}\) and in Sect. 8.1.1 we define stationarity for these more general processes. Random variables observable in time can be characterized as \(y(t)\) for \(t\in\mathbf{R}\). Here we assume that this continuous time process will only be observed at equally spaced times \(t_{1},t_{2},\ldots\). (In other words, \(t_{k}-t_{k-1}\) is the same for all \(k\).) The time series we are concerned with is \(y_{1}=y(t_{1})\), \(y_{2}=y(t_{2})\),.... If the \(y(t)\) process is stationary as defined in Chap. 8, then the sequence \(y_{1},y_{2},\ldots\) is also stationary here. Of course, we cannot actually observe more than a finite number of these random variables.

The property of stationarity is simply that the mechanisms generating the process do not vary--they remain stationary. The sense in which this occurs is that no matter where you start to examine the process, the distribution of the process looks the same. A formal definition of a (strictly) stationary sequence can be based on examining an arbitrary number of random variables, say \(k\), both at the start of the process and at any other time \(t\). Let \(C_{1},\ldots,C_{k}\) be arbitrary (Borel) sets, then \(\ldots,y_{1},y_{2},\ldots\) is a _discrete time stationary process_ if, for any \(t\),

\[\Pr[y_{1}\in C_{1},\ldots,y_{k}\in C_{k}]=\Pr[y_{t+1}\in C_{1},\ldots,y_{t+k} \in C_{k}]\,. \tag{6.1.1}\]

Thus, the random vectors \((y_{1},\ldots,y_{k})^{\prime}\) and \((y_{t+1},\ldots,y_{t+k})^{\prime}\) have the same distribution for any values \(t\) and \(k\). In particular, if the expectation exists,

\[\mathrm{E}(y_{t})=\mu \tag{6.1.2}\]

for any \(t\) and some scalar \(\mu\). If second moments exist, then for any \(t\) and \(k\)

\[\mathrm{Cov}(y_{t},y_{t+k})=\sigma(k) \tag{6.1.3}\]

for some _covariance_ (_autocorrelate_) _function_\(\sigma(\cdot)\) that does not depend on \(t\). Note that \(\sigma(0)\) is the variance of \(y_{t}\).

A concept related to stationarity is that of _second-order stationarity_, also known as _weak stationarity_, _covariance stationarity_, and _stationarity in the wide sense_. A process is said to be second-order stationary if it satisfies conditions (6.1.2) and (6.1.3). The name derives from the fact that conditions (6.1.2) and (6.1.3) only involve second-order moments of the process. As mentioned in the previous paragraph, any process with second moments that satisfies the stationarity condition (6.1.1) also satisfies (6.1.2) and (6.1.3), so any stationary process with second moments is second-order stationary. The converse, of course, does not hold. That random variables have the same first and second moments does not imply that they have the same distributions.

Interestingly, there is an important subclass of stationary processes for which stationarity and second-order stationarity are equivalent. If \((y_{t+1},\ldots,y_{t+k})\) has a multivariate normal distribution for all \(t\) and \(k\), the process is called a _Gaussianprocess. Because multivariate normal distributions are completely determined by their means and covariances, conditions (6.1.2) and (6.1.3) imply that a Gaussian process is stationary.

In applying linear models to observations from stochastic processes with a known covariance function, the assumption of second-order stationarity leads to BLUEs. To obtain tests and confidence intervals or maximum likelihood estimates, the data need to result from a stationary Gaussian process. In practice the covariance function is unknown and we are in a situation similar to that discussed in Chap. 4.

### 6.2 Basic Data Analysis

Frequency domain analysis of an observed time series is based on identifying the frequencies associated with cycles displayed by the data. The most familiar mathematical functions that display cyclical behavior are the sine and cosine functions. The frequency domain analysis of time series can be viewed as doing regression on sines and cosines. In particular, if \(n\) is an odd number, we can fit the regression model for \(t=1,\ldots,n\),

\[y_{t}=\alpha_{0}+\sum_{k=1}^{\frac{n-1}{2}}\left[\alpha_{k}\cos\left(2\pi\frac{ k}{n}t\right)+\beta_{k}\sin\left(2\pi\frac{k}{n}t\right)\right]\,. \tag{6.2.1}\]

Here the \(\alpha\)s and \(\beta\)s are unknown regression parameters. The predictor variables are the sine and cosine functions evaluated as indicated. The independent variables are grouped by their frequency of oscillation. If \(\cos\left(2\pi\frac{k}{n}t\right)\) is graphed on \([0,n]\), the function will complete \(k\) cycles. Thus, the frequency, the number of cycles in one unit of time, is \(k/n\). Similarly, the frequency of \(\sin\left(2\pi\frac{k}{n}t\right)\) is \(k/n\). Notice that the model has \(1+2\left(\frac{n-1}{2}\right)=n\) independent variables, so the model is saturated (fits the data perfectly). For this reason, we have not included an error term \(e_{t}\). Figure 6.2 illustrates some of these predictor variables as functions of time for \(n=61\), the number of observations in the coal data. The figure illustrates the lowest frequency, the middle frequency, and the highest frequency. Solid diamonds indicated the actual data points with the underlying function also graphed for the years of the time series. (If you decide to count the number of cycles in the graphs, remember that they are essentially being graphed on \([1,n]\) rather than on \([0,n]\).)

If the number of observations \(n\) is even, a slightly different model is used: for \(t=1,\ldots,n\),

\[y_{t}=\alpha_{0}+\sum_{k=1}^{\frac{n}{2}-1}\left[\alpha_{k}\cos\left(2\pi\frac {k}{n}t\right)+\beta_{k}\sin\left(2\pi\frac{k}{n}t\right)\right]+\alpha_{\frac {n}{2}}\left(-1\right)^{t}\,. \tag{6.2.2}\]

Again, there are \(n\) predictors in the model, one for each observation, so again the data are fitted perfectly. Note that the upper limit of the sums in (6.2.1) and (6.2.2) can both be written as \(\lfloor\frac{n-1}{2}\rfloor\), where \(\lfloor\frac{n-1}{2}\rfloor\) is the greatest integer contained in (floor of) \(\frac{n-1}{2}\). Considering that a saturated model is always fitted to the data, it may be more appropriate to view these models as data-analytic tools rather than as realistic models for the data.

It is convenient to write these models in matrix form. Let

\[Y=(y_{1},\ldots,y_{n})^{\prime},\]

\[C_{k}=\left[\cos\left(2\pi\frac{k}{n}1\right),\cos\left(2\pi\frac{k}{n}2\right), \ldots,\cos\left(2\pi\frac{k}{n}n\right)\right]^{\prime},\]

\[S_{k}=\left[\sin\left(2\pi\frac{k}{n}1\right),\sin\left(2\pi\frac{k}{n}2\right),\ldots,\sin\left(2\pi\frac{k}{n}n\right)\right]^{\prime},\]

and

\[Z_{k}=[C_{k},S_{k}].\]

Also, let \(\chi_{k}=[\alpha_{k},\beta_{k}]^{\prime}\). The model for both \(n\) even and \(n\) odd can be written

\[Y=J\alpha_{0}+\sum_{k=1}^{\lfloor\frac{n-1}{2}\rfloor}Z_{k}\chi_{k}+\delta_{ \lfloor\frac{n}{2}\rfloor\frac{n}{2}}C_{\frac{n}{2}}\alpha_{\frac{n}{2}}, \tag{6.2.3}\]

where \(J\) is a column of 1s, \(\lfloor\frac{n}{2}\rfloor\) is the greatest integer in (floor of) \(\frac{n}{2}\) (i.e., for \(n\) even \(\lfloor\frac{n}{2}\rfloor=\frac{n}{2}\) and for \(n\) odd \(\lfloor\frac{n}{2}\rfloor=\frac{n-1}{2}\)), and

Figure 6.2: Coal production data: predictor variables for \(k/n=1/61,15/61,30/61\)

\[\delta_{ab}=\left\{\begin{array}{ll}1&a=b\\ 0&a\neq b.\end{array}\right.\]

Note that, for \(n\) even, \(C_{\frac{n}{2}}=(-1,1,-1,1,\ldots,-1,1)^{\prime}\). Also observe that \(C_{0}=J\).

One reason that these particular models are used is that the independent variable vectors in (6.2.3) are orthogonal. In particular,

\[C_{i}^{\prime}C_{j}=\left\{\begin{array}{ll}n&\mbox{if }i=j\in\{0,n/2\}\\ n/2&\mbox{if }i=j\not\in\{0,n/2\}\\ 0&\mbox{if }i\neq j\end{array}\right., \tag{6.2.4}\]

\[S_{i}^{\prime}S_{j}=\left\{\begin{array}{ll}n/2&\mbox{if }i=j\not\in\{0,n/2\}\\ 0&\mbox{otherwise}\end{array}\right., \tag{6.2.5}\]

\[C_{i}^{\prime}S_{j}=0\qquad\mbox{any }i\mbox{ and }j. \tag{6.2.6}\]

See Exercise 6.9.15 for a proof of these relationships. Because the vectors are orthogonal, the sum of squares associated with each independent variable does not depend on the other variables that may or may not be included in any submodel. The order of fitting of variables is also irrelevant. Denote the sum of squares associated with \(C_{k}\) as \(SS(C_{k})\) and the sum of squares for \(S_{k}\) as \(SS(S_{k})\). The total sum of squares associated with the frequency \(k/n\) is \(SS(C_{k})+SS(S_{k})\).

The _periodogram_ is a function \(P(\nu)\) of the frequencies that indicates how important a particular frequency is in the time series. The periodogram is defined only for \(\nu=k/n\), \(k=1,\ldots,\lfloor\frac{n-1}{2}\rfloor\). The periodogram is

\[P(k/n)\equiv\{SS(C_{k})+SS(S_{k})\}/2.\]

This is precisely _the mean square associated with the frequency \(k/n\)_. Clearly, if the mean square for the frequency \(k/n\) is large relative to the mean squares for the other frequencies, then the frequency \(k/n\) is important in explaining the time series.

_We will not define \(P(0.5)\)_ but for \(n\) even, some people define

\[P(0.5)\equiv Y^{\prime}C_{\frac{n}{2}}C_{\frac{n}{2}}^{\prime}Y/n=SS(C_{\frac{ n}{2}}).\]

This is still the mean square associated with the frequency because \(S_{\frac{n}{2}}=0\). A couple of reasons for not defining \(P(0.5)\) are that an oscillation that fast would rarely be interesting and that it is inconvenient to have 2 degrees of freedom associated with every periodogram value except this one. The details of a data analysis can depend on whether \(P(0.5)\) is defined. Since the programming language \(R\) defines it, some of our output will use it.

**Exercise 6.1**.: Show that for \(k=1,\ldots,\lfloor\frac{n-1}{2}\rfloor\), \(P(k/n)=\{(C_{k}^{\prime}Y)^{2}+(S_{k}^{\prime}Y)^{2}\}/n\). Show that for \(n\) even, \(P(0.5)=\{(C_{\frac{n}{2}}^{\prime}Y)^{2}+(S_{\frac{n}{2}}^{\prime}Y)^{2}\}/n\) so that the same formula applies even though this has 1 degree of freedom, not 2.

As always, our linear model is only an approximation to reality. The true frequencies associated with a time series are unlikely to be among the values \(k/n\). If the true frequency is, say, \(\nu\in\left[\frac{k-1}{n},\frac{k}{n}\right]\), then we could expect the effect of this frequency to show up in \(\SS(C_{k-1})\), \(\SS(C_{k})\), \(\SS(S_{k-1})\), and \(\SS(S_{k})\). If we want a measure of the importance of all the frequencies in a neighborhood of \(\frac{k}{n}\), it makes sense to compute the mean square for a group of frequencies near \(\frac{k}{n}\). This idea is used to define a smoothed version of the periodogram called an _estimate of the spectral density_ or more simply a _spectral estimator_. For \(r\) odd, define

\[\hat{f}_{r}(k/n)=\frac{1}{r}\sum_{i=-(r-1)/2}^{(r-1)/2}P\left(\frac{k+i}{n} \right), \tag{6.2.7}\]

which is _the mean square for the \(2r\) variables \(C_{k+\ell},S_{k+\ell}:\ell=-(r-1)/2,\ldots,(r-1)/2\)_. This process is sometimes referred to as applying a _Daniell kernel_ of order \(r\) to the periodogram. The frequencies \((k+\ell)/n\) for \(\ell=-(r-1)/2,\ldots,(r-1)/2\) will be called the _\(r\) neighborhood (Daniell spectral window)_ of \(k/n\). Picking an \(r\) neighborhood is equivalent to picking frequencies in a band having _bandwidth_\(r/n\). Choosing \(r=3\) (i.e., examining the mean square for the frequencies \(\frac{k-1}{n}\), \(\frac{k}{n}\), \(\frac{k+1}{n}\), seems particularly appealing to the author but may not be particularly common in applications. (For consistent estimation of the spectral density that is defined later, \(r\) must be an increasing function of \(n\).)

Strictly speaking, \(\hat{f}_{r}(k/n)\) is only defined for \(k=1+(r-1)/2,\ldots,\lfloor\frac{n-1}{2}\rfloor-(r-1)/2\). To get values for all \(k=1,\ldots,\lfloor\frac{n-1}{2}\rfloor\), it is common practice to replace any values of \((k+i)/n\) in Eq. (6.2.7) that are not defined with the frequency that is closest to it for which the periodogram actually exists. So, if \((k+i)/n\leq 0\), use \(1/n\). For high frequencies, the adjustment depends on whether you have defined \(P(0.5)\).

Rather than using the simple average of the periodograms in the \(r\) neighborhood, the spectral density estimate is often taken as a weighted average of the periodogram values. The weights can be defined by evaluating a _weighting (kernel) function_ at appropriate points. One common choice for a weighting function is the cosine. Within this context, the simple average corresponds to a rectangular weighting function. An indirect method of changing the weights is to apply the Daniellal kernal first to the periodogram and then to the resulting spectral estimate.

Although the spectral density function estimator \(\hat{f}_{r}(\cdot)\) is certainly a reasonable thing to examine, the name of the function must seem totally bizarre to anyone without a previous knowledge of time series analysis. The genesis of this ghostly (ghastly?) name will be discussed in the next section.

Example 6.2.1. Table 6.2 contains three versions of the periodogram for the coal production data of Example 6.0.1. As will be discussed in Sect. 6.8, the periodogram can be written as the squared absolute value of the discrete Fourier transform of the time series divided by the sample size. As such, something called the _fast Fourier transform (FFT)_ is often used to compute the periodogram. We begin by discussing the fact that different computer software can give different periodograms.

[MISSING_PAGE_FAIL:220]

three observations to the time series before producing the periodogram to make the sample size \(64=2^{6}\). The R documentation indicates that these added observations are 0s, but adding three 0s (or any other numbers I could think of) to the data file does not reproduce the numbers given by spec.pgram. You can tell how many observations have been added because the frequencies reported are \(k\) divided by the number of observations used. I also computed the periodogram directly from the definitions given here by (6.2.1) essentially computing the orthogonal ppos, as well as (6.2.2) by using R's fft command, and by (6.2.3) telling spec.pgram not to add observations. These numbers all agreed. R's fft does not add observations; it merely exploits any factorization available for \(n\). R's spec.pgram, by default, speeds up the computations by modifying the sample size to some highly factorable number but specifying fast=FALSE stops that. Incidentally, setting spec.pgram to fast=FALSE is not the only default that has to be reset to get the periodogram.

Figure 6.3 plots the three versions of the periodogram on the natural log scale, i.e., it plots \((\nu,\log[P(\nu)])\). I did this with natural logs but it is more commonly plotted with base 10 logs. You can see substantial differences, especially with the default spec.pgram results. Figure 6.4 plots the three corresponding versions of \(\log[\hat{f}_{5}(\nu)]\). Table 6.2 repeats the definitional periodogram and gives the corresponding spectral density estimates based on \(r=3\) and \(r=5\).

Figure 6.3: Coal production data: three versions of \(P(\nu)\)

As noted earlier, the frequencies \(\nu\) reported in Table 6.1 for BMDP are not the Fourier frequencies \(k/61\) but rather \(k/63\). In the interest of computational efficiency, BMDP has extended the time series from 61 to 63 observations by including artificial values \(y_{62}=y_{63}=\bar{y}\).. Clearly, this procedure will not affect the estimate of the mean. All effects for positive frequencies are orthogonal to the mean, so working with \(y_{i}-\bar{y}\). is equivalent to working with the original data. After correcting for the mean, the artificial observations are zero; thus the artificial observations do not increase the sums of squares for positive frequencies. I assume that this is also what spec.pgram intends to produce, but the documentation I read was not clear, nor could I reproduce spec.pgram's results. In any case, it cannot be denied that the different methods provide solutions to slightly different problems than we set out to solve and Fig. 6.3 illustrates how different the solutions are.

Figure 6.5 contain plots, on a logarithmic scale, of the periodogram and the spectral density estimates. In Example 6.5.1 we will discuss confidence intervals for the true periodogram values, i.e., for the \(\sigma_{k}^{2}\)s. Perhaps the two most noteworthy aspects of these plots are that there seem to be substantial effects for low frequencies and small effects for high frequencies. Low frequencies are consistent with trend in the data. To eliminate the effect of a trend, one can perform the frequency analysis on the residuals from a simple linear regression on time. Figure 6.6 gives \(\hat{f}_{5}\) with the

Figure 6.4: Coal production data: three versions of \(\hat{f}_{5}(\nu)\)

[MISSING_PAGE_EMPTY:9529]

11.6) is probably easier to follow, but the discussion is less clearly applicable to the problem at hand.

In this section, applications of various results in probability are discussed. The results themselves are given without proof. In particular, we will use the fact that any second-order stationary process can be approximated by model (6.2.3) with the regression coefficients taken as random variables. This defines a random effects model as in Chap. 5. The variance components of the mixed model are related to the autocorrelate function through something called the _spectral distribution function_. Estimation of the variance components is via least squares, thus mimicking Henderson's method 3, which was discussed in Sect. 5.6. It is also equivalent to doing REML, cf. Exercise 6.3.

Let \(\ldots y_{-2},y_{-1},y_{0},y_{1},y_{2},\ldots\) be a second-order stationary process with \(\mathrm{E}(y_{t})=\mu\) and covariance (autocorrelate) function \(\sigma(k)=\mathrm{Cov}(y_{t},y_{t+k})\). As Doob (1953, p. 486) points out, the Spectral Representation Theorem implies that the process \(y_{t}-\mu\) can be approximated arbitrarily closely by a process based on sines and cosines. In particular, for \(n\) large and some \(\alpha_{k}\)s and \(\beta_{k}\)s,

\[y_{t}-\mu\doteq z_{t}, \tag{6.3.1}\]where

\[z_{t}=\sum_{k=0}^{\lfloor\frac{n-1}{2}\rfloor}\left[\alpha_{k}\cos\left(2\pi\frac{k }{n}t\right)+\beta_{k}\sin\left(2\pi\frac{k}{n}t\right)\right]+\delta_{\lfloor \frac{n}{2}\rfloor\frac{n}{2}}\cos(\pi t)\alpha_{\frac{n}{2}}\]

with, for all \(k\) and \(k^{\prime}\), \(\mathrm{E}(\alpha_{k})=\mathrm{E}(\beta_{k})=0\), \(\mathrm{Var}(\alpha_{k})=\mathrm{Var}(\beta_{k})=\sigma_{k}^{2}\), \(\mathrm{Cov}(\alpha_{k},\beta_{k^{\prime}})=0\), and, for \(k\neq k^{\prime}\), \(\mathrm{Cov}(\alpha_{k},\alpha_{k^{\prime}})=\mathrm{Cov}(\beta_{k},\beta_{k^ {\prime}})=0\). Although it has been suppressed in the notation, \(\sigma_{k}^{2}\) also depends on \(n\). Note that, for \(k=0\), \(\sin\left(2\pi\frac{k}{n}t\right)=0\) and \(\cos\left(2\pi\frac{k}{n}t\right)=1\) for all \(t\). Thus, the right-hand side of (6.3.1) is identical to the right-hand sides of models (6.2.1) and (6.2.2), except that we had not previously assumed random regression coefficients.

In our later data analysis, the random effects \(\alpha_{0}\) and \(\beta_{0}\) will not be considered. This requires some justification. First, \(\beta_{0}\) is multiplied by \(0\equiv\sin(2\pi\Omega/nt)\), so \(\beta_{0}\) has no effect. The random effect \(\alpha_{0}\) cannot be analyzed because it is statistically indistinguishable from the parameter \(\mu\). Because \(1\equiv\cos(2\pi\Omega/nt)\), \(\alpha_{0}\) is added to _every_ observation \(y_{t}\). Similarly, \(\mu\) is added to every \(y_{t}\). On the basis of one realization of the series, the two effects are hopelessly confounded. Fortunately, our interest is often in predicting the future of this realization or in simply explaining the behavior of these observations. For either purpose, it is reasonable to consider \(\alpha_{0}\) as fixed.

Figure 6.6: Coal production data: plots of \(\hat{f}_{5}(\nu)\) from detrended data and from original data

With \(\alpha_{0}\) fixed, the mean of \(y_{t}\) is \(\mu+\alpha_{0}\). Because this involves two parameters for one object, we will suppress \(\mu\) in the notation and use \(\alpha_{0}\) to denote the mean.

The spectral approximation (6.3.1) along with the discussion in the previous paragraph is the justification for using model (6.2.3) as an approximate model for stationary time series. Based on the approximation, in model (6.2.3) we assume that

\[\mathrm{E}(\gamma_{k}) = 0\,,\] \[\mathrm{Cov}(\gamma_{k}) = \sigma_{k}^{2}I_{2}\,,\] \[\mathrm{Cov}(\gamma_{k},\gamma_{k^{\prime}}) = 0\qquad k\neq k^{\prime}\,,\]

\(\alpha_{0}\) is a fixed effect, and for \(n\) even \(\alpha_{\frac{n}{2}}\) is a random effect with zero mean, variance \(\sigma_{\frac{n}{2}}^{2}\), and zero covariance with the other random effects. Under these assumptions, model (6.2.3) is a variance component as in Chap. 5.

The key parameters in model (6.2.3) are the variance components, the \(\sigma_{k}^{2}\)s. These are closely tied to additional aspects of the Spectral Representation Theorem. To illustrate these aspects, assume that \(\mu=0\) and \(\alpha_{0}\) is a random effect. The Spectral Representation Theorem implies that there is a _unique_ right continuous function \(F\) called the _spectral distribution function_ that is (a) defined on \([-1/2,1/2]\), (b) symmetric about zero [i.e., \(F(v-)-F(0)=F(0)-F(-v)\), where \(F(v-)=\lim_{\eta\nearrow v}F(\eta)\)], and (c) satisfies

\[\sigma(k) = \int_{-1/2}^{1/2}e^{2\pi ivk}dF(\nu) \tag{6.3.2}\] \[= \int_{-1/2}^{1/2}\cos(2\pi\nu k)dF(\nu)\,.\]

By definition, \(e^{2\pi ivk}=\cos(2\pi\nu k)+i\sin(2\pi\nu k)\). The integrals are Riemann-Stieltjes integrals. They are similar to the standard Riemann integrals. If \(\mathscr{P}_{n}=(v_{0n},\ldots,v_{nn})\) defines a sequence of partitions of \([-1/2,1/2]\) with \(v_{in}-v_{i-1,n}\) approaching zero for all \(i\) and \(n\), then

\[\int_{-1/2}^{1/2}\cos(2\pi\nu k)dF(\nu)=\lim_{n\to\infty}\sum_{i=0}^{n-1}\cos(2 \pi\nu_{in}k)[F(v_{i+1,n})-F(v_{in})]\,.\]

The second equality in (6.3.2) follows from the symmetry of \(F\) and the fact that \(\sin(2\pi\nu k)\) is an odd function in \(\nu\).

Example 6.3.1.: Consider the stochastic process defined by

\[z_{t}=\sum_{j=0}^{\frac{n-1}{2}}\left[\alpha_{j}\cos\left(2\pi\frac{j}{n}t \right)+\beta_{j}\sin\left(2\pi\frac{j}{n}t\right)\right]\,,\]where \(n\) is odd and the \(\alpha\)s and \(\beta\)s are random effects that satisfy the assumptions made along with Eq. (6.3.1). Note that this process is just the right-hand side of (6.3.1). In this example, we examine properties of the process that is used to approximate second-order stationary time series. Using the assumptions about the random effects and applying the formula \(\cos(a-b)=\cos(a)\cos(b)+\sin(a)\sin(b)\) to \(\cos(2\pi\frac{j}{n}k)=\cos(-2\pi\frac{j}{n}k)=\cos(\{2\pi(j/n)(t+k)\}-\{2\pi(j /n)t\})\), we get

\[\sigma(k)=\text{Cov}(z_{t},z_{t+k}) = \text{E}(z_{t}z_{t+k}) \tag{6.3.3}\] \[= \sum_{j=0}^{\frac{n-1}{2}}\sigma_{j}^{2}\cos\left(2\pi\frac{j}{n} t\right)\cos\left(2\pi\frac{j}{n}(t+k)\right)\] \[+\sum_{j=0}^{\frac{n-1}{2}}\sigma_{j}^{2}\sin\left(2\pi\frac{j}{n }t\right)\sin\left(2\pi\frac{j}{n}(t+k)\right)\] \[= \sum_{j=0}^{\frac{n-1}{2}}\sigma_{j}^{2}\cos\left(2\pi\frac{j}{n} k\right)\,.\]

Noticing that \(\cos(2\pi(j/n)k)=\cos(-2\pi(j/n)k)\), the spectral distribution function \(F\) must be a step function that is zero at \(v=-\frac{1}{2}\), has a jump of \(\sigma_{0}^{2}\) at \(v=0\), and jumps of \(\sigma_{j}^{2}/2\) at \(v=\pm j/n\), \(j=1,\ldots,(n-1)/2\). [Computing (6.3.2) for this function \(F\) is just like computing the "expected value" of \(\cos(2\pi vk)\), where \(v\) is a random variable that takes on the values \(\pm j/n\) with "probability" \(\sigma_{j}^{2}/2\) and the value \(0\) with "probability" \(\sigma_{0}^{2}\). The only difference is that the "probabilities" do not add up to one.]

This random effects process is used to approximate an arbitrary time series process. The fact that it makes a good approximation to the series does not imply that it makes a realistic model for the time series. Realistic models for time series should probably have continuous spectral distributions. This process has a discrete spectral distribution. It may be reasonable to approximate a continuous distribution with a discrete distribution; it is less reasonable to _model_ a continuous distribution as a discrete distribution.

Most realistic models for time series do not "remember" forever what has happened in the past. More technically, they have a covariance function \(\sigma(k)\) that converges to zero as \(k\to\infty\). From Eq. (6.3.3), that does not occur with the approximation process. Nonetheless, the approximation process provides a good tool for introducing basic concepts of frequency domain analysis to people with a background in linear models. 

A second-order stationary process \(y_{t}\) has a spectral distribution function \(F\) defined by (6.3.2). Typically, \(F\) will not be a discrete distribution. The important point is that the distribution \(F\) determines the spectral distribution of the approximation process \(z_{t}\) and thus the variance components of the approximation model. In particular, the Spectral Representation Theorem implies that the approximation in (6.3.1) has\[\sigma_{0}^{2} = F\left(\frac{1}{n}-\right)-F\left(-\frac{1}{n}\right),\] \[\sigma_{k}^{2} = \left[F\left(\frac{k+1}{n}-\right)-F\left(\frac{k}{n}-\right) \right]+\left[F\left(-\frac{k}{n}\right)-F\left(-\frac{k+1}{n}\right)\right]\] \[= 2\left[F\left(\frac{k+1}{n}-\right)-F\left(\frac{k}{n}-\right) \right].\]

Much of standard frequency domain time series relates to the spectral density function. Assuming that

\[\sum_{k=0}^{\infty}|\sigma(k)|<\infty,\]

the spectral distribution function \(F(\nu)\) has a derivative \(f(\nu)\) and

\[\sigma(k) = \int_{-1/2}^{1/2}e^{2\pi i\nu k}f(\nu)d\nu\] \[= \int_{-1/2}^{1/2}\cos(2\pi\nu k)f(\nu)d\nu\,.\]

The function \(f(\nu)\) is called the _spectral density_. From (6.3.4), we see that for \(k\neq 0\)

\[\sigma_{k}^{2}\doteq 2f\left(\frac{k}{n}\right)\left[\frac{k+1}{n}-\frac{k}{n}\right]\]

and

\[\frac{n}{2}\sigma_{k}^{2}\doteq f\left(\frac{k}{n}\right). \tag{6.3.6}\]

We have assumed that the process has gone on in the infinite past. Thus, by second-order stationarity, the random variables \(\ldots\),\(y_{-2}\),\(y_{-1}\),\(y_{0}\),\(y_{1}\),\(y_{2}\),\(\ldots\) have the property that

\[\sigma(k)=\sigma(-k)\,.\]

Equation (6.3.5) leads to a well-known inverse relation,

\[f(\nu) = \sum_{k=-\infty}^{\infty}\sigma(k)e^{-2\pi i\nu k} \tag{6.3.7}\] \[= \sum_{k=-\infty}^{\infty}\sigma(k)\cos(2\pi\nu k)\,.\]

The last equality follows from the symmetry of \(\sigma(\cdot)\).

White noise is a term used to describe one of the simplest yet most important examples of a second-order stationary process. A process \(e_{t}\) is said to be _white noise_ if

\[\mathsf{E}(e_{t})=0\]\[\sigma(k)=\left\{\begin{array}{ll}\sigma^{2}&\text{if }k=0\\ 0&\text{if }k\neq 0\end{array}\right..\]

Such processes are of particular importance in defining time domain models in Chap. 7. Note that random sampling (with replacement) from any population with mean zero and finite variance generates white noise.

**Exercise 6.2**.: Let \(e_{t}\) be a white noise process. Show that the spectral density of \(e_{t}\) is

\[f_{e}(\nu)=\sigma^{2}\,.\]

### The Random Effects Model

Consider now the matrix form of the spectral approximation random effects model (6.3.1) in all its gory detail. The model is

\[Y=J\mu+\sum_{k=0}^{\lfloor\frac{n-1}{2}\rfloor}Z_{k}\gamma_{k}+\delta_{\lfloor \frac{n}{2}\rfloor\frac{n}{2}}C_{\frac{n}{2}}\alpha_{\frac{n}{2}}\,, \tag{6.4.1}\]

\[\text{E}(\gamma_{k})=0,\qquad\text{Cov}(\gamma_{k})=\sigma_{k}^{2}I_{2}\,,\]

\[\text{E}(\alpha_{\frac{n}{2}})=0,\qquad\text{Var}(\alpha_{\frac{n}{2}})= \sigma_{\frac{n}{2}}^{2}\,,\]

\[\text{Cov}(\gamma_{k},\gamma_{k^{\prime}})=0,\qquad k\neq k^{\prime}\,,\]

\[\text{Cov}(\gamma_{k},\alpha_{\frac{n}{2}})=0\,.\]

Let \(\text{Cov}(Y)=V\); then,

\[V=\sum_{j=0}^{\lfloor\frac{n-1}{2}\rfloor}\sigma_{j}^{2}Z_{j}Z_{j}^{\prime}+ \delta_{\lfloor\frac{n}{2}\rfloor\frac{n}{2}}\sigma_{\frac{n}{2}}^{2}C_{\frac {n}{2}}C_{\frac{n}{2}}^{\prime}\,.\]

Model (6.4.1) can be rewritten as

\[Y=J\mu+\xi,\,\text{E}(\xi)=0,\,\text{Cov}(\xi)=V.\]

Typically \(V\) will be nonsingular and \(C(VJ)\subset C(J)\) so the least squares estimate of \(\mu\), i.e. \(\bar{y}\)., is the BLUE, cf. Exercise 6.3.

This random effects model is exactly correct for observations generated by the approximation process \(z_{t}\) of Sect. 6.3. It is approximately correct for other second-order stationary processes. As discussed in Example 6.3.1, the process \(z_{t}\) based on sines and cosines approximates the behavior of the true process but in broader terms may not be a very realistic model for the true process. In this section, we rely only on the quality of the approximation. We derive results assuming that model (6.4.1) is correct and mention the appropriate interpretation when model (6.4.1) is only an approximation.

Let \(P_{k}\) be the perpendicular projection operator onto the column space \(C(C_{k},S_{k})\). Note that by the choice of the \(C_{j}\)s and \(S_{j}\)s, for \(k=1,\ldots,\lfloor\frac{n-1}{2}\rfloor\),

\[P_{k}=\frac{2}{n}[C_{k}C_{k}^{\prime}+S_{k}S_{k}^{\prime}]=\frac{2}{n}Z_{k}Z_{k }^{\prime}.\]

The periodogram is

\[P(k/n)=Y^{\prime}P_{k}Y/2.\]

**Proposition 6.4.1**: _For \(k=1,\ldots,\lfloor\frac{n-1}{2}\rfloor\),_

\[\operatorname{E}\left[P(k/n)\right]=\frac{n}{2}\sigma_{k}^{2}\doteq f\left( \frac{k}{n}\right).\]

Proof.  By Theorem 1.3.2 in \(PA\),

\[\operatorname{E}(Y^{\prime}P_{k}Y) = \operatorname{tr}[P_{k}V]+(J\mu)^{\prime}P_{k}(J\mu)\] \[= \sum_{j}\sigma_{j}^{2}\operatorname{tr}[P_{k}Z_{j}Z_{j}^{\prime} ]+0\] \[= \sigma_{k}^{2}\operatorname{tr}[P_{k}Z_{k}Z_{k}^{\prime}]\] \[= \sigma_{k}^{2}\operatorname{tr}[Z_{k}^{\prime}P_{k}Z_{k}]\] \[= \sigma_{k}^{2}\operatorname{tr}[C_{k}^{\prime}C_{k}+S_{k}^{ \prime}S_{k}]\] \[= \sigma_{k}^{2}\left[\frac{n}{2}+\frac{n}{2}\right]\] \[= n\sigma_{k}^{2}.\]

Dividing by two and recalling (6.3.6) gives the result. \(\Box\)

**Proposition 6.4.2**: _If the data have a multivariate normal distribution and model (6.4.1) is correct, then for \(k=1,\ldots,\lfloor\frac{n-1}{2}\rfloor\),_

\[\frac{2P\left(\frac{k}{n}\right)}{\frac{n}{2}\sigma_{k}^{2}}\sim\chi^{2}(2).\]

Proof.  This follows from checking the conditions of \(PA\) Theorem 1.3.6. Note that

\[2P\left(\frac{k}{n}\right)\left/\frac{n}{2}\sigma_{k}^{2}=Y^{\prime}P_{k}Y \right/\frac{n}{2}\sigma_{k}^{2}=Y^{\prime}\left[\frac{2}{n\sigma_{k}^{2}}P_{ k}\right]Y.\]

Because the only fixed effect is \(J\alpha_{0}\), and because \(P_{k}J=0\), it suffices to show that \(V\frac{2}{n\sigma_{k}^{2}}P_{k}V\frac{2}{n\sigma_{k}^{2}}P_{k}V=V\frac{2}{n \sigma_{k}^{2}}P_{k}V\). In fact, it suffices to show that \[\left(\frac{2}{n\sigma_{k}^{2}}\right)^{2}P_{k}VP_{k}V=\left(\frac{2}{n\sigma_{k}^ {2}}\right)P_{k}V.\]

Note that

\[P_{k}V = P_{k}\sum_{j}\sigma_{j}^{2}Z_{j}Z_{j}^{\prime}\] \[= \sigma_{k}^{2}P_{k}[Z_{k}Z_{k}^{\prime}]\] \[= \sigma_{k}^{2}P_{k}\left[\frac{n}{2}P_{k}\right]\] \[= \frac{n}{2}\sigma_{k}^{2}P_{k}\,.\]

Clearly, \(\left(\frac{2}{n\sigma_{k}^{2}}\right)^{2}P_{k}VP_{k}V=\left(\frac{2}{n\sigma _{k}^{2}}\right)P_{k}V\). 

Of course, in practice, model (6.4.1) will not be true. However, if the \(y_{t}\) process is a stationary Gaussian process and \(n\) is large, then by (6.3.1), model (6.4.1) is approximately correct, so \(P\left(\frac{k}{n}\right)/\frac{n}{2}\sigma_{k}^{2}\) is approximately \(\chi^{2}(2)\).

We can now derive confidence intervals for the \(\frac{n}{2}\sigma_{k}^{2}\)s. A \((1-\alpha)100\%\) confidence interval for \(\frac{n}{2}\sigma_{k}^{2}\) is based on

\[1-\alpha = \Pr\left[\chi^{2}\left(\frac{\alpha}{2},2\right)\leq\frac{2P\left( \frac{k}{n}\right)}{\frac{n}{2}\sigma_{k}^{2}}\leq\chi^{2}\left(1-\frac{\alpha }{2},2\right)\right]\] \[= \Pr\left[\frac{2P\left(\frac{k}{n}\right)}{\chi^{2}\left(1-\frac{ \alpha}{2},2\right)}\leq\frac{n}{2}\sigma_{k}^{2}\leq\frac{2P\left(\frac{k}{n} \right)}{\chi\left(\frac{\alpha}{2},2\right)}\right].\]

The confidence interval is

\[\left(\frac{2P\left(\frac{k}{n}\right)}{\chi^{2}\left(1-\frac{\alpha}{2},2 \right)}\,,\;\frac{2P\left(\frac{k}{n}\right)}{\chi^{2}\left(\frac{\alpha}{2}, 2\right)}\right).\]

From (6.3.6), this is also an approximate confidence interval for \(f\left(\frac{k}{n}\right)\). If you define \(P(0.5)\), the confidence interval for \(f(0.5)\) needs to be based on 1 degree of freedom rather than 2.

Note that, as \(n\) increases, the periodogram estimates the spectral density at more points, but the quality of the individual estimates does not improve. We are observing one realization of the process. Without combining information from nearby frequencies, to get improved periodogram estimates of the spectral density requires independent replication of the _series_, not additional observations on the realization at hand.

The spectral density estimate \(\hat{f}_{r}\left(\frac{k}{n}\right)\) was defined in (6.2.7). Arguments similar to those just given establish that under model (6.4.1)\[E\left[\hat{f}_{r}\left(\frac{k}{n}\right)\right] = \frac{1}{r}\sum_{i=-(r-1)/2}^{(r-1)/2}\frac{n}{2}\sigma_{k+i}^{2} \tag{6.4.2}\] \[\doteq \frac{1}{r}\sum_{i=-(r-1)/2}^{(r-1)/2}f\left(\frac{k+i}{n}\right),\]

which is the average of \(f(\nu)\) in the \(r\) neighborhood of \(\frac{k}{n}\). If \(f(\nu)\) is continuous and \(n\) is large, all of these values should be similar so

\[\mathrm{E}\left[\hat{f}_{r}\left(\frac{k}{n}\right)\right]\doteq f\left(\frac{ k}{n}\right).\]

In fact, if we let \(r\) be an increasing function of \(n\), under reasonable conditions on \(r\) and the process, it is possible to achieve consistent estimation of the spectral density from only one realization of the process.

If all the \(\sigma_{k+i}^{2}\)s are _equal_ in the \(r\) neighborhood,

\[\frac{2r\hat{f}_{r}\left(\frac{k}{n}\right)}{\frac{n}{2}\sigma_{k}^{2}}\sim \chi^{2}(2r). \tag{6.4.3}\]

Again, if \(f(\nu)\) is continuous and \(n\) is large, the distribution should hold approximately. This yields a confidence interval for \(\frac{n}{2}\sigma_{k}^{2}\doteq f(k/n)\) of

\[\left(\frac{2r\hat{f}_{r}\left(\frac{k}{n}\right)}{\chi^{2}\left(1-\frac{ \alpha}{2},2r\right)},\ \frac{2r\hat{f}_{r}\left(\frac{k}{n}\right)}{\chi^{2} \left(\frac{\alpha}{2},2r\right)}\right).\]

This is only really appropriate for frequencies for which \(\hat{f}_{r}\) is the average of distinct periodogram values that each have 2 degrees of freedom. In other words, it is only appropriate for \(k/n\) with \(k=1+(r-1)/2,\ldots,\lfloor\frac{n-1}{2}\rfloor-(r-1)/2\).

It is often convenient to have the length of the confidence intervals independent of the frequency. This can be accomplished by reporting the confidence intervals based on \(\log\hat{f}_{r}\left(\frac{k}{n}\right)\) rather than \(\hat{f}_{r}\left(\frac{k}{n}\right)\). The confidence interval for \(\log\left(\frac{n}{2}\sigma_{k}^{2}\right)\) is

\[\left(\log\hat{f}_{r}\left(\frac{k}{n}\right)-\log\left[\frac{\chi^{2}\left(1- \frac{\alpha}{2},2r\right)}{2r}\right],\log\hat{f}_{r}\left(\frac{k}{n}\right) -\log\left[\frac{\chi^{2}\left(\frac{\alpha}{2},2r\right)}{2r}\right]\right).\]

As discussed earlier, \(\hat{f}_{r}\left(\frac{k}{n}\right)\) is the mean square for the \(r\) neighborhood of \(\frac{k}{n}\) and is a good indicator of the relative importance of frequencies near \(\frac{k}{n}\). Confidence intervals allow more rigorous comparisons of the relative importance of the various frequencies.

#### Exercise 6.3.

1. In model (6.4.1), \(Z_{0}=[J,0]\). Show that \(C(VJ)\subset C(J)\).

2. In model (6.4.1), show that for \(j=1,\ldots,\lfloor\frac{n-1}{2}\rfloor\), the REML equations amount to setting \(P(j/n)=\frac{n}{2}\sigma_{j}^{2}\). When \(n\) is even, a similar result holds for \(j/n=0.5\).
3. Show that the matrix \[\sum_{j=1}^{\lfloor\frac{n-1}{2}\rfloor}\sigma_{j}^{2}Z_{j}Z_{j}^{\prime}+ \delta_{\lfloor\frac{n}{2}\rfloor\frac{n}{2}}\sigma_{\frac{n}{2}}^{2}C_{\frac{ n}{2}}C_{\frac{n}{2}}^{\prime}\] is singular.
4. In model (6.4.1), \(C(Z_{0})=C(J)\) with ppo \((1/n)JJ^{\prime}\). Find \(\mathrm{E}[Y^{\prime}(1/n)JJ^{\prime}Y]\) and discuss the problems with estimating both \(\mu\) and \(\sigma_{0}^{2}\).
5. Assuming that the random effects model (6.4.1) is Gaussian, prove that (6.4.2) and (6.4.3) hold.

### The White Noise Model

It would be nice if we could arrive at some justification for looking at reduced models. Predictions based on saturated models are notoriously poor. Unfortunately, reduced models are not possible in model (6.4.1) because the estimates for all of the \(\frac{n}{2}\sigma_{k}^{2}\)s will be positive and there is no reason to conclude that any can be zero. In using model (6.4.1), we have overlooked the nearly ubiquitous fact that multiple measurements on the same unit differ. It seems reasonable to model the observations as the sum of a simple stationary process and individual uncorrelated measurement errors, i.e., white noise. (Although the measurement errors can be incorporated into (6.4.1), it is convenient to isolate them.) Dropping \(Z_{0}\) from the random effects to eliminate the confounding of \(\mu\) and \(\sigma_{0}^{2}\), this suggests the model

\[Y=J\alpha_{0}+\sum_{k=1}^{\lfloor\frac{n-1}{2}\rfloor}Z_{k}\gamma_{k}+\delta_{ \lfloor\frac{n}{2}\rfloor\frac{n}{2}}C_{\frac{n}{2}}\alpha_{\frac{n}{2}}+e, \tag{6.5.1}\]

\[\mathrm{Cov}(e)=\sigma^{2}I_{n},\quad\mathrm{Cov}(e,\gamma_{k})=0,\quad\mathrm{ Cov}(e,\alpha_{\frac{n}{2}})=0,\]

\[\mathrm{Cov}(\gamma_{k})=\sigma_{k}^{2}I_{2},\quad\mathrm{Var}(\alpha_{\frac{n }{2}})=\sigma_{\frac{n}{2}}^{2},\quad\mathrm{Cov}(\gamma_{k},\gamma_{k^{\prime }})=0,\;k\neq k^{\prime},\quad\mathrm{Cov}(\gamma_{k},\alpha_{\frac{n}{2}})=0.\]

The analysis of this model is similar to that of model (6.4.1). For \(k=1+(r-1)/2,\ldots,\lfloor\frac{n-1}{2}\rfloor-(r-1)/2\), when all variances in the \(r\) neighborhood of \(\frac{k}{n}\) are equal,

\[\mathrm{E}\biggl{[}\hat{f}_{r}\biggl{(}\frac{k}{n}\biggr{)}\biggr{]}=\sigma^{ 2}+\frac{n}{2}\sigma_{k}^{2} \tag{6.5.2}\]

and

\[\frac{2r\hat{f}_{r}\bigl{(}\frac{k}{n}\bigr{)}}{\sigma^{2}+\frac{n}{2}\sigma_{ k}^{2}}\sim\chi^{2}(2r). \tag{6.5.3}\]This leads to confidence intervals for the values

\[\sigma^{2}+\frac{n}{2}\sigma_{k}^{2}\,.\]

If all of the \(\sigma_{k}^{2}\)s are zero, the various confidence intervals are estimates of the same thing, \(\sigma^{2}\). Confidence intervals containing distinctly larger values suggest the existence of a nonzero variance \(\sigma_{k}^{2}\).

Another way of identifying important frequencies is to modify an approach used for identifying important effects in saturated models for designed experiments, cf. _PA-V_ Example 12.2.4 (Christensen 2011, Example 13.2.4). Let \(s=\lfloor\frac{n-1}{2}\rfloor\) and let \(w_{1},\ldots,w_{s}\) be i.i.d. \(\chi^{2}(2r)\). Construct the order statistics \(w_{(1)}\leq\cdots\leq w_{(s)}\), and compute the expected order statistics \(\mathrm{E}[w_{(1)}],\ldots,\mathrm{E}[w_{(s)}]\). If all the \(\sigma_{k}^{2}\)s are zero, \(\hat{f}_{r}\big{(}\frac{1}{n}\big{)},\hat{f}_{r}\big{(}\frac{2}{n}\big{)}, \ldots,\hat{f}_{r}\big{(}\frac{s}{n}\big{)}\) are each \(\sigma^{2}\) times a \(\chi^{2}(2r)\) random variable, and the plot of \(\big{(}\mathrm{E}[w_{(k)}],\hat{f}_{r}\big{(}\frac{k}{n}\big{)}\big{)}\) should form an approximate straight line. In any case, the unimportant frequencies should form a straight line. Values \(\hat{f}_{r}\big{(}\frac{k}{n}\big{)}\) that are so large as to be inconsistent with the straight line indicate important frequencies.

There is one obvious problem with this method. The \(w_{k}\)s were assumed to be independent, but the \(\hat{f}_{r}\big{(}\frac{k}{n}\big{)}\)s are only independent when \(r=1\). If \(r\geq 3\) (recall that \(r\) is odd), then the neighborhoods overlap so, for example, \(\hat{f}_{r}\big{(}\frac{k}{n}\big{)}\) and \(\hat{f}_{r}\big{(}\frac{k-1}{n}\big{)}\) both involve \(P\big{(}\frac{k}{n}\big{)}\) and \(P\big{(}\frac{k-1}{n}\big{)}\), hence the \(\hat{f}_{r}\big{(}\frac{k}{n}\big{)}\)s are not independent. It is interesting to recall that normal plots are also plagued by a correlation problem when the residuals are used to check for normality. However, the order of magnitude of the correlation problem is very different in the two cases, see Example 6.5.1. Simulations provide an obvious method for dealing with such dependence.

##### Exercise 6.4

1. Assuming that model (6.5.1) is Gaussian with, for convenience, \(n\) odd, prove that (6.5.2) and (6.5.3) hold.
2. Why is it impossible to learn what \(\sigma^{2}\) is?
3. How does this procedure differ from testing that all the \(\sigma_{k}^{2}\)s are equal?

If a group of \(s\) frequencies, say \(k_{1}/n,\ldots,k_{s}/n\), have been identified as important, the random effects model becomes

\[Y=J\alpha_{0}+\sum_{i=1}^{s}Z_{k_{i}}\gamma_{k_{i}}+e,\]

so best linear unbiased prediction methods can be used to obtain predictions of both current and future observations. As always, our approach here has been based on linear models. Brockwell and Davis (1991, Section 5.6) discuss a prediction method for the frequency domain that is founded on Fourier analysis.

Example 6.5.1. Consider again the coal production data of Example 6.2.1. Tables 6.4, 6.5, and 6.6 give 95% confidence intervals for the spectral density based on 

[MISSING_PAGE_EMPTY:9541]

[MISSING_PAGE_FAIL:236]

[MISSING_PAGE_FAIL:237]

#### The Reduced Model: Estimation

If coal production is a stationary process, we probably would not have seen data like that observed from 1970 to 1980, cf. Fig. 6.1. If stationary, the data seem overdue for a downturn. During the entire time period, coal production should be tied to general economic level, since the period probably predates serious efforts to reduce coal consumption. But only the last 10 years of the data seem like clear evidence of nonstationarity. Our model comes to the same conclusion and predicts a downturn. Moreover, the actual data from 1981 to 1987 are also unwilling to cooperate with our model. Analyses in previous versions of this book were based on the BMDP periodogram and reflected adjustments for how that was computed. The current analysis is based on the definitional periodogram.

If there were an overall trend, we could have removed it using regression and performed frequency analysis on the residuals (probably ignoring the complications involved with the residuals not being the actual errors, the errors being what might reasonably be assumed stationary). Incidentally, in frequency analysis, trends are often picked up as very low frequency effects. Or perhaps it is just too much to hope that an economy affected by depression, world war, and a microelectronics

Figure 6.7: \(\chi^{2}(10)\) plot for 30 \(\hat{f}_{5}\) observationsrevolution could provide stationary data. In any case, our analysis won't really be quick and it certainly won't be clean.

We begin with a reduced version of the white noise random effects model (6.5.1),

\[Y=J\mu+Z_{1}\gamma_{1}+Z_{2}\gamma_{2}+Z_{4}\gamma_{4}+Z_{5}\gamma_{5}+Z_{6} \gamma_{6}+e, \tag{6.5.4}\]

with

\[\mathrm{E}(e)=0,\qquad\mathrm{Cov}(e)=\sigma^{2}I,\]

where the rows of \(Z_{k}\) are

\[z^{\prime}_{kt}=\left(\cos\left(2\pi\frac{k}{61}t\right),\sin\left(2\pi\frac{k }{61}t\right)\right),\]

\(t=1,\ldots,61\). We also write \(Z\equiv[Z_{1},Z_{2},Z_{4},Z_{5},Z_{6}]\) and \(\gamma^{\prime}\equiv[\gamma^{\prime}_{1},\gamma^{\prime}_{2},\gamma^{\prime} _{4},\gamma^{\prime}_{5},\gamma^{\prime}_{6}]\) so as to rewrite the model as a standard mixed model

\[Y=J\mu+Z\gamma+e.\]

Getting even dirtier, for illustrative purposes I decided to incorporate the following assumptions about the variance components:

Figure 6.8: \(\chi^{2}(10)\) plot for 26 smallest \(\hat{f}_{S}\) observations

\[\mathrm{Cov}(\gamma_{1})=\mathrm{Cov}(\gamma_{2})=\sigma_{a}^{2}I\]

and

\[\mathrm{Cov}(\gamma_{4})=\mathrm{Cov}(\gamma_{5})=\mathrm{Cov}(\gamma_{6})= \sigma_{b}^{2}I.\]

The disadvantage of these assumptions is that they may not be true. The advantage is that, if they approximate the truth, we should get better estimates of the variance components. (This is not dissimilar to averaging periodograms to define spectral density estimates.)

The entire mixed model can be fitted from the periodogram values and the mean of the observations, \(\bar{y}_{\cdot}=511.8131\). In particular, the white noise variance estimate is the usual \(MSE\) for the model, which in this case is

\[\hat{\sigma}^{2}=\left[P(3/61)+\sum_{j=7}^{30}P(j/61)\right]\Big{/} [61-11]\\ =Y^{\prime}(I-P)Y\Big{/}[61-11]=150902/50=3018,\]

wherein \(P\equiv(1/n)JJ^{\prime}+P_{1}+P_{2}+P_{4}+P_{5}+P_{6}\).

Figure 6.9: \(\chi^{2}(2)\) plot for 30 periodogram observations

[MISSING_PAGE_EMPTY:9547]

\[\hat{\sigma}_{b}^{2}=\left(\left[P(4/61)+P(5/61)+P(6/61)\right]/3- \hat{\sigma}^{2}\right)\left(\frac{2}{61}\right)\\ =\left(14,926.6-3018\right)\left(\frac{2}{61}\right)=390.4.\]

These are a direct applications of Henderson's method 3. Generally, Henderson's method does not provide unique estimates of variance components; they depend on the order in which the components are estimated. However, due to the orthogonality of the columns involved, the estimates based on the periodogram are unique.

#### The Reduced Model: Prediction

We are now in a position to estimate the best linear unbiased predictors. We have to estimate the predictors because we do not know the variance components. Prediction is performed as in Sect. 5.1, see also Sect. 4.1.3. It is easily seen that the best linear unbiased predictor (BLUP) for an observation at time \(t\) is

\[\hat{y}_{t}=\hat{\mu}+\tilde{z}_{t}^{\prime}\hat{\gamma},\]

where

\[\hat{\mu}=\tilde{y}.=511.8\,,\]

\[\tilde{z}_{t}=\left[\begin{array}{c}\cos\left(2\pi(\frac{1}{61})t\right)\\ \sin\left(2\pi(\frac{1}{61})t\right)\\ \cos\left(2\pi(\frac{2}{61})t\right)\\ \sin\left(2\pi(\frac{2}{61})t\right)\\ \cos\left(2\pi(\frac{4}{61})t\right)\\ \sin\left(2\pi(\frac{4}{61})t\right)\\ \cos\left(2\pi(\frac{5}{61})t\right)\\ \sin\left(2\pi(\frac{5}{61})t\right)\\ \cos\left(2\pi(\frac{6}{61})t\right)\\ \sin\left(2\pi(\frac{6}{61})t\right)\\ \sin\left(2\pi(\frac{6}{61})t\right)\end{array}\right],\qquad\gamma=\left[ \begin{array}{c}\gamma_{1}\\ \gamma_{2}\\ \gamma_{4}\\ \gamma_{5}\\ \gamma_{6}\end{array}\right],\]

and \(\hat{\gamma}\) is the BLUP of \(\gamma\).

Let

\[\tilde{Y}\equiv\left[\begin{array}{c}y_{62}\\ \vdots\\ y_{68}\end{array}\right],\quad\tilde{Z}\equiv\left[\begin{array}{ccccc}z_{1,62}^{\prime},&z_{2,62}^{\prime},&z_{4,62}^{\prime},&z_{5,62}^{\prime},&z_{6,62}^{\prime}\\ \vdots&\vdots&\vdots&\vdots&\vdots\\ z_{1,68}^{\prime},&z_{2,68}^{\prime},&z_{4,68}^{\prime},&z_{5,68}^{\prime},&z_{ 6,68}^{\prime}\end{array}\right]\equiv[\tilde{Z}_{1},\tilde{Z}_{2},\tilde{Z}_ {4},\tilde{Z}_{5},\tilde{Z}_{6}].\]

We want to predict a \(\tilde{Y}\) that follows the model

\[\tilde{Y}=J_{7}\mu+\tilde{Z}\gamma+\tilde{e},\]where \(\tilde{e}\) is a white noise vector uncorrelated with \(e\) and \(\gamma\). In other words, we want to estimate the BLP

\[\hat{E}(\tilde{Y}|Y)=J_{12}\mu+\text{Cov}(\tilde{Y},Y)[\text{Cov}(Y)]^{-1}(Y-J_{ 61}\mu).\]

From model (4) we have

\[V \equiv \text{Cov}(Y)=\text{Cov}(e)\] \[= \sigma^{2}I+\sigma_{a}^{2}Z_{1}Z_{1}^{\prime}+\sigma_{a}^{2}Z_{2} Z_{2}^{\prime}+\sigma_{b}^{2}Z_{4}Z_{4}^{\prime}+\sigma_{b}^{2}Z_{5}Z_{5}^{ \prime}+\sigma_{b}^{2}Z_{6}Z_{6}^{\prime}\] \[= \sigma^{2}I+\sigma_{a}^{2}(Z_{1}Z_{1}^{\prime}+Z_{2}Z_{2}^{\prime })+\sigma_{b}^{2}(Z_{4}Z_{4}^{\prime}+Z_{5}Z_{5}^{\prime}+Z_{6}Z_{6}^{\prime})\] \[= \sigma^{2}I+\sigma_{a}^{2}\frac{n}{2}(P_{1}+P_{2})+\sigma_{b}^{2} \frac{n}{2}(P_{4}+P_{5}+P_{6})\] \[= \sigma^{2}(I-\tilde{P})+\left(\sigma^{2}+\frac{n}{2}\sigma_{a}^{2 }\right)(P_{1}+P_{2})+\left(\sigma^{2}+\frac{n}{2}\sigma_{b}^{2}\right)(P_{4} +P_{5}+P_{6}),\]

where \(\tilde{P}\equiv P_{1}+P_{2}+P_{4}+P_{5}+P_{6}\). We need to estimate

\[V^{-1}=\frac{1}{\sigma^{2}}(I-\tilde{P})+\frac{1}{\left(\sigma^{2}+\frac{n}{2} \sigma_{a}^{2}\right)}(P_{1}+P_{2})+\frac{1}{\left(\sigma^{2}+\frac{n}{2} \sigma_{b}^{2}\right)}(P_{4}+P_{5}+P_{6}).\]

We also need

\[\text{Cov}(\tilde{Y},Y) = \text{Cov}(\tilde{Z}\gamma+\tilde{e},Z\gamma+e)\] \[= \text{Cov}(\tilde{Z}\gamma,Z\gamma)=\tilde{Z}\text{Cov}(\gamma)Z^ {\prime}\] \[= \tilde{Z}\begin{bmatrix}\sigma_{a}^{2}I_{4}&0\\ 0&\sigma_{b}^{2}I_{6}\end{bmatrix}Z^{\prime}\] \[= \sigma_{a}^{2}(\tilde{Z}_{1}Z_{1}^{\prime}+\tilde{Z}_{2}Z_{2}^{ \prime})+\sigma_{b}^{2}(\tilde{Z}_{4}Z_{4}^{\prime}+\tilde{Z}_{5}Z_{5}^{ \prime}+\tilde{Z}_{6}Z_{6}^{\prime}).\]

This leads to

\[\text{Cov}(\tilde{Y},Y)V^{-1}=\frac{\sigma_{a}^{2}}{\left(\sigma^{2}+\frac{n} {2}\sigma_{a}^{2}\right)}(\tilde{Z}_{1}Z_{1}^{\prime}+\tilde{Z}_{2}Z_{2}^{ \prime})+\frac{\sigma_{b}^{2}}{\left(\sigma^{2}+\frac{n}{2}\sigma_{b}^{2} \right)}(\tilde{Z}_{4}Z_{4}^{\prime}+\tilde{Z}_{5}Z_{5}^{\prime}+\tilde{Z}_{6 }Z_{6}^{\prime})\]

and the BLUP of \(\tilde{Y}\) is \(\tilde{y}.J_{7}\) plus

\[\text{Cov}(\tilde{Y},Y)V^{-1}(Y-J_{61}\tilde{y}.)\] \[= \frac{\sigma_{a}^{2}}{\left(\sigma^{2}+\frac{n}{2}\sigma_{a}^{2} \right)}(\tilde{Z}_{1}Z_{1}^{\prime}Y+\tilde{Z}_{2}Z_{2}^{\prime}Y)+\frac{ \sigma_{b}^{2}}{\left(\sigma^{2}+\frac{n}{2}\sigma_{b}^{2}\right)}(\tilde{Z}_{ 4}Z_{4}^{\prime}+\tilde{Z}_{5}Z_{5}^{\prime}+\tilde{Z}_{6}Z_{6}^{\prime})\]

### The White Noise Model

\[=[Z_{1},Z_{2},Z_{4},Z_{5},Z_{6}]\left[\begin{array}{c}\frac{\sigma_{a}^{2}}{( \sigma^{2}+\frac{\alpha}{2}\sigma_{a}^{2})}Z_{1}^{\prime}Y\\ \frac{\sigma_{a}^{2}}{(\sigma^{2}+\frac{\alpha}{2}\sigma_{a}^{2})}Z_{2}^{\prime }Y\\ \frac{\sigma_{b}^{2}}{(\sigma^{2}+\frac{\alpha}{2}\sigma_{b}^{2})}Z_{4}^{\prime }Y\\ \frac{\sigma_{b}^{2}}{(\sigma^{2}+\frac{\alpha}{2}\sigma_{b}^{2})}Z_{5}^{\prime }Y\\ \frac{\sigma_{b}^{2}}{(\sigma^{2}+\frac{\alpha}{2}\sigma_{b}^{2})}Z_{6}^{\prime }Y\end{array}\right]\]

where \(\hat{\gamma}\) is the BLUP (and the BLP) of \(\gamma\). Note that if \(\sigma^{2}=0\), \(\hat{\gamma}\) becomes the least squares estimate of \(\gamma\), so in general the BLUP is a shrinkage estimate. Of course we have to estimate the variance components as indicated earlier to obtain an empirical BLUP \(\hat{\gamma}\).

Figure 6.11 gives all the data from 1920 to 1987 along with the mixed model predicted values and least squares predicted values from just doing a regression on the 10 sine and cosine terms. As suggested earlier, the models, which were fitted only to the data from 1920 to 1980, turn down after 1980. In point of fact, coal production continued to increase during the 1980s, so the model and its predictions are not very

Figure 6.11: Coal production with mixed model and least squares predictions: 1920â€“1987good after 1980. Perhaps the data are simply not regular enough to allow predictions based on only 61 observations, or perhaps the process generating the data changed around 1960. The behavior of the plotted data certainly looks different in the last years than it did up to 1975. Another possibility is that, given the discussion of Example 6.1, the very act of prediction may involve taking the random effects model more seriously than is appropriate.

For a time domain analysis of these data, see Example 7.6.1.

#### Summary of Sects. 6.2, 6.3, 6.4, and 6.5

We began Sect. 6.2 by showing that the least squares fit of a model based on sines and cosines yields useful information about the frequencies that are important in describing the cyclical behavior of a time series. We then explained that any second-order stationary process can be approximated by a random effects model based on sines and cosines. The variances of the random effects were related to the spectral distribution function \(F\), which is implicitly defined by the covariance function \(\sigma(k)\). Estimates of \(\frac{n}{2}\) times the variance components were obtained and related to the spectral density function \(f(\nu)\equiv{\bf d}_{\nu}F(\nu)\). Confidence intervals and a \(\chi^{2}\) plot were suggested as methods for identifying important frequencies. Best linear unbiased prediction was suggested for obtaining forecasts based on important frequencies.

### Linear Filtering

Many traditional ways of dealing with time series consist of computing simple linear functions of the observations to create a new series that is, in some sense, representative of the original series but is also better behaved than the original series. By "better behaved" we mean that the important structure of the series is clarified in the transformed series.

Example 6.6.1. A series that oscillates very quickly and erratically has very high frequencies that contribute substantially to the spectral approximation. (High frequencies are those near 1/2.) To examine the structure of such a series, it might be wise to try to eliminate the high frequencies while retaining the relative importance of the low and moderate-size frequencies. A traditional method of attenuating high frequencies is taking _moving averages_. For instance, a running average of order 5 (also called a centered 5-term moving average or applying a Daniell kernel of order 2) is

\[w_{t}^{(5)}=\frac{1}{5}[y_{t-2}+y_{t-1}+y_{t}+y_{t+1}+y_{t+2}].\]

A running average of order 6 (applying a modified Daniell kernel of order 3) is \[w_{t}^{(6)}=\frac{1}{6}\left[\frac{1}{2}y_{t-3}+y_{t-2}+y_{t-1}+y_{t}+y_{t+1}+y_{t+2 }+\frac{1}{2}y_{t+3}\right].\]

**Example 6.6.2**.: Consider a nonstationary process

\[y_{t}=\gamma_{0}+\gamma_{1}t+y_{t}^{(1)},\]

where \(\gamma_{0}\) and \(\gamma_{1}\) are fixed and \(y_{t}^{(1)}\) is second-order stationary. This is a process with a linear time trend. The difference series

\[w_{t}\equiv y_{t}-y_{t-1}\]

is second-order stationary. To see this, observe that

\[\mathrm{E}(w_{t}) = \mathrm{E}(y_{i}-y_{t-1})\] \[= \gamma_{0}+\gamma_{1}t+\mathrm{E}\big{(}y_{t}^{(1)}\big{)}\] \[-\gamma_{0}-\gamma_{1}(t-1)-\mathrm{E}\big{(}y_{t-1}^{(1)}\big{)}\] \[= \gamma_{1}\]

and

\[\mathrm{Cov}(w_{t},w_{t+k}) = \mathrm{Cov}\big{(}y_{t}^{(1)}-y_{t-1}^{(1)},y_{t+k}^{(1)}-y_{t+k -1}^{(1)}\big{)}\] \[= \mathrm{Cov}\big{(}y_{t}^{(1)},y_{t+k}^{(1)}\big{)}-\mathrm{Cov} \big{(}y_{t}^{(1)},y_{t+k-1}^{(1)}\big{)}\] \[-\mathrm{Cov}\big{(}y_{t-1}^{(1)},y_{t+k}^{(1)}\big{)}+\mathrm{ Cov}\big{(}y_{t-1}^{(1)},y_{t+k-1}^{(1)}\big{)}\] \[= 2\sigma^{(1)}(k)-\sigma^{(1)}(k-1)-\sigma^{(1)}(k+1),\]

where \(\sigma^{(1)}(\cdot)\) is the covariance function for \(y_{t}^{(1)}\). Note that the covariance depends only on \(k\). Often, the _first difference operator_ is denoted \(\nabla\), so that \(w_{t}\) is

\[\nabla y_{t}\equiv y_{t}-y_{t-1}\,.\]

The idea that trends can be thought of as low-frequency cyclical effects was mentioned earlier. In this example, we can think of the frequency generated by \(\gamma_{0}+\gamma_{1}t\) as being so low that the oscillation will never be observed. The first difference process eliminates low frequencies from the spectral approximation. 

**Exercise 6.5**.: Consider the process \(y_{t}=\gamma_{0}+\gamma_{1}t+\gamma_{2}t^{2}+y_{t}^{(1)}\), where \(y_{t}^{(1)}\) is second-order stationary. Let \(w_{t}=\nabla y_{t}\) and \(z_{t}=\nabla w_{t}\). In other words,

\[z_{t} = \nabla(\nabla y_{t})\] \[\equiv \nabla^{2}y_{t}.\]Show that \(z_{t}\) is second-order stationary.

These examples are special cases of the _general linear filter_

\[w_{t}=\sum_{s=-\infty}^{\infty}a_{s}y_{t-s}\,. \tag{6.6.1}\]

The special case

\[w_{t}=\sum_{s=0}^{\infty}a_{s}y_{t-s}\]

in which \(w_{t}\) depends only on the current and previous values of \(y_{t}\) will be referred to as a _causal_ linear filter. The process \(y_{t}\) is viewed as causing \(w_{t}\).

Example 6.6.1 continued.  The process \(w_{t}^{(5)}\) has \(a_{-2}=a_{-1}=a_{0}=a_{1}=a_{2}=\frac{1}{5}\) and \(a_{s}=0\) for all other \(s\). For \(w_{t}^{(6)}\), \(a_{-3}=a_{3}=\frac{1}{12}\), \(a_{-2}=a_{-1}=a_{0}=a_{1}=a_{2}=\frac{1}{6}\), and \(a_{s}=0\) for all others. Neither filter is causal. \(\Box\)

Example 6.6.2 continued.  The process \(\nabla y_{t}\) has \(a_{0}=1\), \(a_{1}=-1\), and \(a_{s}=0\) for all other \(s\). The filter is causal. \(\Box\)

Collectively, the series \(\ldots,a_{-1},a_{0},a_{1},\ldots\) is called the _impulse response function_; it is a function from the integers to the reals. If \(\sum_{k=-\infty}^{\infty}|a_{k}|<\infty\), we can draw an analogy between \(a_{k}\) and \(\sigma(k)\), thus defining a function similar to the spectral density as given in (6.3.7), say

\[A(\nu)=\sum_{k=-\infty}^{\infty}a_{k}e^{-2\pi ivk}\,. \tag{6.6.2}\]

(Because we need not have \(a_{k}=a_{-k}\), we cannot, in general, reduce \(A(\nu)\) to a sum involving only cosines.) The complex valued function \(A(\nu)\) is called the _frequency response_ function. As will be seen later, \(A(\nu)\) identifies the effect that the filter has on different frequencies in the spectral approximation.

The behavior of the spectral approximation (6.3.1) is determined by the spectral density of \(y_{t}\), say \(f_{y}(\nu)\). Similarly, the process \(w_{t}\) determined by (6.6.1) has a spectral density \(f_{w}(\nu)\). We wish to show that

\[f_{w}(\nu)=|A(\nu)|^{2}f_{y}(\nu), \tag{6.6.3}\]

where \(|A(\nu)|^{2}=A(\nu)\overline{A(\nu)}\) and \(\overline{A(\nu)}\) is the _complex conjugate_ of \(A(\nu)\). (The conjugate of the complex number \(a+ib\) with \(a\) and \(b\) real is \(\overline{a+ib}\equiv a-ib\).) If, for example,

\[|A(\nu)|^{2}=\cases{1&$v\in[-0.4,0.4]$\cr 0&otherwise\cr}\,,\]

then in the spectral approximation to \(w_{t}\), the variance components \(\sigma_{k}^{2}\) corresponding to frequencies \(\frac{k}{n}\in[-0.4,0.4]\) are identical to the corresponding variance components in the spectral approximation to \(y_{t}\). At the same time, the spectral approximation to \(w_{t}\) has \(\sigma_{k}^{2}=0\) for \(|\frac{k}{n}|>0.4\) regardless of the size of the corresponding variance components in the approximation to \(y_{t}\). A linear filter with \(A(\nu)\) as given earlier perfectly eliminates high frequencies (greater than 0.4) while it leaves the contributions of the lower frequencies unchanged. In fact, a function

\[|A(\nu)|^{2}=\cases{7&$v\in[-0.4,0.4]$\cr 0&otherwise\cr}\]

would be equally effective because it retains the same relative contributions of the frequencies below 0.4.

A filter that eliminates high frequencies but does little to low frequencies is called a _low-pass_ filter. Symmetric moving averages such as those defined in Example 6.6.1 are low-pass filters; see Exercise 6.9.7d. Filters that eliminate low frequencies but have little effect on high frequencies are called _high-pass_ filters. The first difference filter of Example 6.6.2 is such a filter; see Exercise 6.9.7a.

It is by no means clear that an impulse response function (i.e., a sequence \(\ldots,a_{-1},a_{0},a_{1},\ldots\)) exists that generates either of the functions \(|A(\nu)|^{2}\) given earlier but the examples illustrate the potential usefulness of the frequency response function in interpreting the result of applying a linear filter to a process.

Of course, to discuss \(f_{w}(\nu)\) presupposes that \(w_{t}\) is second-order stationary. If \(\sum_{s=-\infty}^{\infty}|a_{s}|<\infty\),

\[{\rm E}(w_{t}) = \sum_{s=-\infty}^{\infty}a_{s}{\rm E}(y_{t-s})\] \[= \sum_{s=-\infty}^{\infty}a_{s}\mu\] \[= \mu\sum_{s=-\infty}^{\infty}a_{s},\]

which is a constant. Covariances for the \(w_{t}\) process depend only on \(k\) because

\[{\rm Cov}(w_{t},w_{t+k}) = {\rm Cov}\left(\sum_{s}a_{s}y_{t-s},\sum_{s^{\prime}}a_{s^{\prime }}y_{t+k-s^{\prime}}\right) \tag{6.6.4}\] \[= \sum_{s}\sum_{s^{\prime}}a_{s}a_{s^{\prime}}{\rm Cov}(y_{t-s},y_{ t+k-s^{\prime}})\] \[= \sum_{s=-\infty}^{\infty}\sum_{s^{\prime}=-\infty}^{\infty}a_{s} a_{s^{\prime}}\sigma_{y}(k-s^{\prime}+s)\] \[\equiv \sigma_{w}(k).\]

Thus, \(w_{t}\) is second-order stationary.

To establish (6.6.3), we use (6.6.4) and the representation of \(\sigma_{y}(k)\) from (6.3.5).

\[\sigma_{w}(k)=\sum_{s}\sum_{s^{\prime}}a_{s}a_{s^{\prime}}\sigma_{y}(k-s+s^{ \prime})\]

[MISSING_PAGE_FAIL:249]

\[w_{t}-\sum_{s=1}^{b}a_{s}w_{t-s}=z_{t}=y_{t}-\sum_{s=1}^{q}b_{s}y_{t-s}\,.\]

The process \(z_{t}\) is the result of a linear filter in the \(w\) process, so

\[f_{z}(\nu)=|A(\nu)|^{2}f_{w}(\nu)\,.\]

It is also a linear filter in the \(y\) process so

\[f_{z}(\nu)=|B(\nu)|^{2}f_{y}(\nu)\,.\]

Clearly,

\[f_{w}(\nu)=\frac{|B(\nu)|^{2}}{|A(\nu)|^{2}}f_{y}(\nu)\,,\]

thus \(|B(\nu)|^{2}/|A(\nu)|^{2}\) is the frequency response function for the recursive filter.

A _simple recursive filter_, also called an _autoregressive filter_, is the special case

\[w_{t}=\sum_{s=1}^{p}a_{s}w_{t-s}+y_{t}\,.\]

Figure 6.12: Coal production data: Low pass filter is a running average of 5. High pass filter is the lag 1 difference

**Exercise 6.6**.: Show that for a simple recursive filter

\[f_{w}(\nu)=\frac{1}{|A(\nu)|^{2}}f_{y}(\nu)\]

and thus that the frequency response function of the \(w\) process is \(1/|A(\nu)|^{2}\).

#### Summary

Properties of linear filters are of interest because they are commonly used in traditional data analysis to clarify the significant aspects of the time series. _Time domain analysis consists of the application of recursive filters to white noise._

### The Coherence of Two Time Series

Suppose we have two time series \(y_{1}=(y_{11},y_{12},\ldots)^{\prime}\) and \(y_{2}=(y_{21},y_{22},\ldots)^{\prime}\) with observations \(Y_{1}=(y_{11},\ldots,y_{1n})^{\prime}\) and \(Y_{2}=(y_{21},\ldots,y_{2n})\). We wish to measure the correlation between \(y_{1}\) and \(y_{2}\) relative to the frequency \(k/n\). One way to do that is to look at the sample partial correlation between \(Y_{1}\) and \(Y_{2}\) after eliminating all of the frequencies other than \(k/n\); see Appendix B and Example 9.3.1. Because the vectors \(C_{0},\ldots,C_{\lfloor\frac{n}{2}\rfloor},S_{1},\ldots,S_{\lfloor\frac{n-1}{ 2}\rfloor}\) form an orthogonal basis for \(\mathbf{R}^{n}\), eliminating all of the frequencies other than \(k/n\) amounts to projecting onto the column space \(C(C_{k},S_{k})\). In particular, the squared sample partial correlation is

\[\frac{Y_{1}^{\prime}[P_{k}Y_{2}(Y_{2}^{\prime}P_{k}Y_{2})^{-1}Y_{2}^{\prime}P_ {k}]Y_{1}}{Y_{1}^{\prime}P_{k}Y_{1}},\]

or equivalently

\[(Y_{1}^{\prime}P_{k}Y_{2})^{2}\Big{/}(Y_{1}^{\prime}P_{k}Y_{1})(Y_{2}^{\prime} P_{k}Y_{2})\,.\]

Recall that \(Y_{1}^{\prime}P_{k}Y_{1}=2P_{1}(k/n)\), twice the periodogram for \(Y_{1}\) evaluated at \(k/n\), and \(Y_{2}^{\prime}P_{k}Y_{2}=2P_{2}(k/n)\). The other term, \(Y_{1}^{\prime}P_{k}Y_{2}/2\) also has strong connections too the periodogram.

As in Sect. 6.2, it may be desired to pool results over an \(r\) neighborhood of the frequency \(k/n\). Let

\[M_{k}=\sum_{\ell=-(r-1)/2}^{(r-1)/2}P_{k+\ell}\,.\]

The sample partial correlation between \(Y_{1}\) and \(Y_{2}\) eliminating all frequencies except those in the \(r\) neighborhood of \(k/n\) is called the _squared sample real coherencefunction_. It is

\[\hat{\gamma}_{12}^{2(R)}(k/n) \equiv \frac{Y_{1}^{\prime}[M_{k}Y_{2}(Y_{2}^{\prime}M_{k}Y_{2})^{-1}Y_{2} ^{\prime}M_{k}]Y_{1}}{Y_{1}^{\prime}M_{k}Y_{1}}\] \[= \left(Y_{1}^{\prime}M_{k}Y_{2}\right)^{2}\Big{/}(Y_{1}^{\prime}M_ {k}Y_{1})(Y_{2}^{\prime}M_{k}Y_{2})\,.\]

The superscript \((R)\) in \(\hat{\gamma}_{12}^{2(R)}\) stands for "real" and will be explained in Sect. 6.8.

Consider the geometric interpretation of \(\hat{\gamma}_{12}^{(R)}(k/n)\). \(M_{k}\) is the perpendicular projection operator onto the space spanned by \(\{C_{k+\ell},S_{k+\ell}:\ell=-(r-1)/2,\ldots,(r-1)/2\}\). This space has \(2r\) degrees of freedom. The perpendicular projection operator \([M_{k}Y_{2}(Y_{2}^{\prime}M_{k}Y_{2})^{-1}Y_{2}^{\prime}M_{k}]\) projects onto the one-dimensional subspace \(C(M_{k}Y_{2})\subset C(M_{k})\). \(M_{k}Y_{2}\) is the projection of \(Y_{2}\) into the space associated with the \(r\) neighborhood of \(k/n\). The value \(\hat{\gamma}_{12}^{(R)}(k/n)\) is the squared length of the projection of \(Y_{1}\) into \(C(M_{k}Y_{2})\) divided by the squared length of \(Y_{1}\) projected into \(C(M_{k})\). \(Y_{1}^{\prime}M_{k}Y_{1}\) is the fraction of \(Y_{1}^{\prime}Y_{1}\) that is associated with the frequency \(k/n\). \(\hat{\gamma}_{12}^{2(R)}(k/n)\) is the proportion of the squared length of the vector \(Y_{1}\) that can be attributed to the association between \(Y_{1}\) and \(Y_{2}\) (actually \(Y_{1}\) and \(M_{k}Y_{2}\)). In particular, if \(Y_{1}=aY_{2}\) for some scalar \(a\), it is easily seen that \(\hat{\gamma}_{12}^{(R)}(k/n)=1\) for any \(k\).

It is interesting to consider the special case \(Y_{1}=C_{k+\ell}\) and \(Y_{2}=C_{k+j}\) for \(j,\ell=-(r-1)/2,\ldots,(r-\ell)/2\). If \(j=\ell\), as indicated earlier, \(\hat{\gamma}_{12}^{2(R)}(k/n)=1\), but if \(j\neq\ell\), \(\hat{\gamma}_{12}^{2(R)}(k/n)=0\). The same results hold for \(Y_{1}=S_{k+\ell}\) and \(Y_{2}=S_{k+j}\). If the two series have the same frequency and that frequency is in the \(r\) neighborhood, \(\hat{\gamma}_{12}^{2(R)}(k/n)=1\). However, if there are two different frequencies (even though both are in the \(r\) neighborhood),\(\hat{\gamma}_{12}^{2(R)}(k/n)\) shows no relationship.

There is one problem with \(\hat{\gamma}_{12}^{2(R)}(k/n)\) as a measure of the correlation between \(y_{1}\) and \(y_{2}\) relative to the frequency \(k/n\). Suppose \(Y_{1}=C_{k}\) and \(Y_{2}=S_{k}\). Then, both series are completely determined by the frequency \(k/n\). More to the point, the relationship between these series is completely determined by \(k/n\). However, it is easily seen that \(\hat{\gamma}_{12}^{2(R)}(k/n)=0\). Simply observe that \(M_{k}Y_{2}=S_{k}\) and thus \(Y_{1}^{\prime}M_{k}Y_{2}=C_{k}^{\prime}S_{k}=0\). Obviously, we need another measure that can pick up relationships that are in the same frequency but orthogonal to one another. We begin by considering just the frequency \(k/n\). The discussion is then extended to \(r\) neighborhoods.

Within \(C(C_{k},S_{k})\), we began by looking at

\[Y_{1}^{\prime}[P_{k}Y_{2}(Y_{2}^{\prime}P_{k}Y_{2})^{-1}Y_{2}^{\prime}P_{k}]Y_ {1}\Big{/}Y_{1}^{\prime}P_{k}Y_{1}\,.\]

To detect an orthogonal relationship between \(Y_{1}\) and \(Y_{2}\) within \(C(P_{k})\), we should rotate \(P_{k}Y_{2}\) by \(90^{\circ}\). This rotation is well-defined because \(C(P_{k})\) is a two-dimensional space.

Let \(c_{k}=C_{k}/\sqrt{n/2}\) and let \(s_{k}=S_{k}/\sqrt{n/2}\). Thus, \(c_{k}^{\prime}c_{k}=1\), \(s_{k}^{\prime}s_{k}=1\), and

\[P_{k}=c_{k}c_{k}^{\prime}+s_{k}s_{k}^{\prime}\,.\]A \(90^{\circ}\) rotation of \(P_{k}Y_{2}\) is \(F_{k}Y_{2}\), where

\[F_{k}=(s_{k}c^{\prime}_{k}-c_{k}s^{\prime}_{k})\,.\]

To see this we must establish that \((P_{k}Y_{2})^{\prime}(F_{k}Y_{2})=0\) and \((P_{k}Y_{2})^{\prime}(P_{k}Y_{2})=(F_{k}Y_{2})^{\prime}(F_{k}Y_{2})\). To see the first of these, note that

\[(P_{k}Y_{2})^{\prime}(F_{k}Y_{2})=Y^{\prime}_{2}P_{k}F_{k}Y_{2}=Y^{\prime}_{2}F _{k}Y_{2}\]

and that for any vector \(v\)

\[v^{\prime}F_{k}v=(v^{\prime}s_{k})(c^{\prime}_{k}v)-(v^{\prime}c_{k})(s^{\prime }_{k}v)=0\,.\]

To see that \((P_{k}Y_{2})^{\prime}(P_{k}Y_{2})=(F_{k}Y_{2})^{\prime}(F_{k}Y_{2})\), observe that

\[F^{\prime}_{k}F_{k} = (c_{k}s^{\prime}_{k}-s_{k}c^{\prime}_{k})(s_{k}c^{\prime}_{k}-c_{ k}s^{\prime}_{k})\] \[= c_{k}s^{\prime}_{k}s_{k}c^{\prime}_{k}-s_{k}c^{\prime}_{k}s_{k}c ^{\prime}_{k}-c_{k}s^{\prime}_{k}c_{k}s_{k}+s_{k}c^{\prime}_{k}c_{k}s^{\prime} _{k}\] \[= c_{k}c^{\prime}_{k}-0-0+s_{k}s^{\prime}_{k}\] \[= P_{k}\,.\]

We can now measure the orthogonal relationship between \(Y_{1}\) and \(Y_{2}\) in the space \(C(C_{k},S_{k})\) by projecting \(Y_{1}\) into \(C(F_{k}Y_{2})\) and comparing the squared length to \(Y^{\prime}_{1}P_{k}Y_{1}\), namely,

\[\frac{Y^{\prime}_{1}[F_{k}Y_{2}(Y^{\prime}_{2}F^{\prime}_{k}F_{k}Y_{2})^{-1}Y^{ \prime}_{2}F_{k}]Y_{1}}{Y^{\prime}_{1}P_{k}Y_{1}}\,.\]

Because \(F^{\prime}_{k}F_{k}=P_{k}\), this can be written as

\[(Y^{\prime}_{1}F_{k}Y_{2})^{2}\Big{/}(Y^{\prime}_{1}P_{k}Y_{1})(Y^{\prime}_{2} P_{k}Y_{2})\,.\]

We have projected \(Y_{1}\) onto the space \(C(P_{k}Y_{2})\subset C(P_{k})\) and onto \(C(F_{k}Y_{2})\subset C(P_{k})\), where \(C(P_{k}Y_{2})\perp C(F_{k}Y_{2})\). Because \(r(P_{k})=2\),

\[Y^{\prime}_{1}P_{k}Y_{1}=Y^{\prime}_{1}[P_{k}Y_{2}(Y^{\prime}_{2}P_{k}Y_{2})^{- 1}Y^{\prime}_{2}P_{k}]Y_{1}+Y^{\prime}_{1}[F_{k}Y_{2}(Y^{\prime}_{2}P_{k}Y_{2} )^{-1}Y^{\prime}_{2}F_{k}]Y_{1}\,.\]

Thus, all of the variability of \(P_{k}Y_{1}\) can be accounted for in one of the two directions. In particular, the two correlation measures add up to 1, that is,

\[\frac{(Y^{\prime}_{1}P_{k}Y_{2})^{2}}{(Y^{\prime}_{1}P_{k}Y_{1})(Y^{\prime}_{2 }P_{k}Y_{2})}+\frac{(Y^{\prime}_{1}F_{k}Y_{2})^{2}}{(Y^{\prime}_{1}P_{k}Y_{1} )(Y^{\prime}_{2}P_{k}Y_{2})}=1\,.\]

In a two-dimensional space, the partial correlation and the orthogonal partial correlation must add up to one.

The situation is not quite so degenerate when dealing with \(r\) neighborhoods. Let\[G_{k}=\sum_{\ell=-(r-1)/2}^{(r-1)/2}F_{k+\ell}\,.\]

The partial correlation involves projecting \(Y_{1}\) into \(C(M_{k}Y_{2})\). The orthogonal partial correlation is obtained by projecting \(Y_{1}\) into \(C(G_{k}Y_{2})\). It is easily seen that \(F_{k+\ell}^{\prime}F_{k+j}=0\) for any \(\ell\neq j\). It follows that

\[G_{k}^{\prime}G_{k} = \sum_{\ell}\sum_{j}F_{k+\ell}^{\prime}F_{k+j}=\sum_{\ell}F_{k+\ell }^{\prime}F_{k+\ell}\] \[= \sum_{\ell}P_{k}=M_{k}\,.\]

Also, for any vector \(v\),

\[v^{\prime}G_{k}v=\sum_{\ell}v^{\prime}F_{k+\ell}v=0\,.\]

From these facts, it is easily established that \((M_{k}Y_{2})^{\prime}(G_{k}Y_{2})=0\) and \((M_{k}Y_{2})^{\prime}(M_{k}Y_{2})=(G_{k}Y_{2})^{\prime}(G_{k}Y_{2})\). Thus, \(G_{k}Y_{2}\) is an orthogonal rotation of \(M_{k}Y_{2}\). In particular, \(G_{k}Y_{2}\) is the sum of the orthogonal rotations in the spaces \(C(P_{k+\ell})\).

As a measure of the orthogonal correlation between \(Y_{1}\) and \(Y_{2}\) in the frequency \(k/n\), use the _squared sample imaginary coherence function_

\[\hat{\gamma}_{12}^{2(I)}(k/n) \equiv \frac{Y_{1}^{\prime}[G_{k}Y_{2}(Y_{2}^{\prime}M_{k}Y_{2})^{-1}Y_{ 2}^{\prime}G_{k}]Y_{1}}{Y_{1}^{\prime}M_{k}Y_{1}}\] \[= \bigl{(}Y_{1}^{\prime}G_{k}Y_{2}\bigr{)}^{2}\Bigl{/}\bigl{(}Y_{1} ^{\prime}M_{k}Y_{1}\bigr{)}(Y_{2}^{\prime}M_{k}Y_{2}\bigr{)}\,.\]

The superscript \((I)\) in \(\hat{\gamma}_{12}^{2(I)}\) stands for "imaginary" and will be discussed in Sect. 6.8. The value \(\hat{\gamma}_{12}^{2(I)}(k/n)\) is the fraction of \(Y_{1}^{\prime}M_{k}Y_{1}\) that is associated with projecting \(Y_{1}\) into \(C(G_{k}Y_{2})\).

To see how the imaginary sample coherence works, suppose \(Y_{1}=C_{k+\ell}\) and \(Y_{2}=S_{k+j}\). For \(\ell\neq j\), \(\hat{\gamma}_{12}^{2(I)}(k/n)=0\) and for \(\ell=j\), \(\hat{\gamma}_{12}^{2(I)}(k/n)=1\). Of course, the same results hold for \(Y_{1}=S_{k+\ell}\) and \(Y_{2}=C_{k+j}\). The squared sample imaginary coherence identifies a relationship only if the frequency is the same but one process is a cosine while the other is a sine. These are analogous to the results for \(\hat{\gamma}_{12}^{2(R)}(k/n)\) with \(Y_{1}=C_{k+\ell}\), \(Y_{2}=C_{k+j}\), and \(Y_{1}=S_{k+\ell}\), \(Y_{2}=S_{k+j}\).

As a total measure of the correlation between \(Y_{1}\) and \(Y_{2}\) relative to the frequency \(k/n\), define the _squared sample coherence function_

\[\hat{\gamma}_{12}^{2}(k/n)=\hat{\gamma}_{12}^{2(R)}(k/n)+\hat{\gamma}_{12}^{2 (I)}(k/n)\,.\]

**Exercise 6.7.** Let \(Y_{1}=a_{1}C_{k+\ell}+b_{1}S_{k+\ell}\) and let \(Y_{2}=a_{2}C_{k+j}+b_{2}S_{k+j}\). Show that for \(j\neq\ell\), \(\hat{\gamma}_{12}^{2}=0\) and for \(j=\ell\), \(\hat{\gamma}_{12}^{2}(k/n)=1\). What is \(\hat{\gamma}_{12}^{2(I)}(k/n)\) when \(b_{1}=b_{2}=0\)? What is \(\hat{\gamma}_{12}^{2(I)}(k/n)\) when \(a_{1}=a_{2}=0\)?We now provide a test for the coherence. Let

\[Q_{k}=M_{k}Y_{2}(Y_{2}^{\prime}M_{k}Y_{2})^{-1}Y_{2}^{\prime}M_{k}+G_{k}Y_{2}(Y_{2 }^{\prime}M_{k}Y_{2})^{-1}Y_{2}^{\prime}G_{k}\,.\]

The matrix \(Q_{k}\) is a perpendicular projection operator with rank two and \(C(Q_{k})\subset C(M_{k})\). Note that

\[\hat{\gamma}_{12}^{2}(k/n)=\frac{Y_{1}^{\prime}Q_{k}Y_{1}}{Y_{1}^{\prime}M_{k}Y _{1}}\,,\]

\[\frac{\hat{\gamma}_{12}^{2}(k/n)}{1-\hat{\gamma}_{12}^{2}(k/n)}=\frac{Y_{1}^{ \prime}Q_{k}Y_{1}}{Y_{1}^{\prime}(M_{k}-Q_{k})Y_{1}}\,,\]

and

\[\frac{2(r-1)}{2}\,\frac{\hat{\gamma}_{12}(k/n)}{1-\hat{\gamma}_{12}(k/n)}= \frac{Y_{1}^{\prime}Q_{k}Y_{1}/2}{Y_{1}^{\prime}(M_{k}-Q_{k})Y_{1}/2(r-1)}\,. \tag{6.7.1}\]

Let \(X\) be the model matrix for a linear model including all frequencies except those in the \(r\) neighborhood of \(k/n\) (i.e., the column space of \(X\) contains \(C_{i},S_{i}\) for all \(i\) other than those near \(k\)). The test statistic (6.7.1) is appropriate for testing

\[Y_{1}=X\beta+e\]

against

\[Y_{1}=X\beta+(M_{k}Y_{2})\gamma_{1}+(G_{k}Y_{2})\gamma_{2}+e\,.\]

Under a variety of conditions, if there is no relationship between \(Y_{1}\) and \(Y_{2}\) in the \(k/n\) frequencies, then (6.7.1) has either an \(F(2,2(r-1),0)\) distribution or has this approximate distribution for large samples.

### Fourier Analysis

Our discussion of the frequency domain has been based on linear models. The traditional way of developing this material is via Fourier analysis. We now present the basic terminology used in Fourier analysis.

Consider observations on two time series \(Y_{1}=(y_{11},\cdots,y_{1n})^{\prime}\) and \(Y_{2}=(y_{21},\cdots,y_{2n})^{\prime}\). Define the _discrete Fourier transform_ of \(Y_{1}\) as

\[Y_{1}(k)\equiv\frac{1}{\sqrt{n}}[Y_{1}^{\prime}C_{k}-iY_{1}^{\prime}S_{k}]= \frac{1}{\sqrt{n}}\sum_{t=1}^{n}y_{1t}e^{-2\pi i\frac{k}{n}t}\]

and define \(Y_{2}(k)\) similarly. It is easily shown that the periodogram of \(Y_{1}\) is

\[P_{1}(k/n)=Y_{1}(k)\overline{Y_{1}(k)},\]

where \(\overline{Y_{1}(k)}\) is the complex conjugate of \(Y_{1}(k)\).

Continuing to use the notation of Sect. 6.7, define the _crossperiodogram_ as \[P_{12}(k/n)\equiv Y_{1}(k)\overline{Y_{2}(k)}=Y_{1}^{\prime}P_{k}Y_{2}+iY_{1}^{ \prime}F_{k}Y_{2}\,.\]

This is, in general, a complex valued function. Define the smoothed cross-spectral estimator from the \(r\) neighborhood of \(k/n\) as

\[\hat{f}_{12}(k/n)\equiv\frac{1}{r}\sum_{\ell=-(r-1)/2}^{(r-1)/2}P_{12}\biggl{(} \frac{k+\ell}{n}\biggr{)}\,.\]

The sample squared coherence equals

\[\hat{\gamma}_{12}^{2}(k/n)=\frac{|\hat{f}_{12}(k/n)|^{2}}{\hat{f}_{1}(k/n)\hat{ f}_{2}(k/n)},\]

where \(\hat{f}_{1}(k/n)=\frac{1}{2r}Y_{1}^{\prime}M_{k}Y_{1}\), \(\hat{f}_{2}(k/n)=\frac{1}{2r}Y_{2}^{\prime}M_{k}Y_{2}\), and

\[|\hat{f}_{12}(k/n)|^{2}=\hat{f}_{12}(k/n)\overline{\hat{f}_{12}(k/n)}\,.\]

Write the complex function \(\hat{f}_{12}(k/n)\) as

\[\hat{f}_{12}(k/n)=\hat{f}_{12}^{(R)}(k/n)+i\hat{f}_{12}^{(I)}(k/n)\,.\]

Then,

\[\hat{\gamma}_{12}^{2(R)}(k/n)=\frac{[\hat{f}_{12}^{(R)}(k/n)]^{2}}{\hat{f}_{1}( k/n)\hat{f}_{2}(k/n)}\]

and

\[\hat{\gamma}_{12}^{(I)}(k/n)=\frac{[\hat{f}_{12}^{(I)}(k/n)]^{2}}{\hat{f}_{1}( k/n)\hat{f}_{2}(k/n)}\,.\]

As discussed earlier, \(\hat{f}_{1}(k/n)\) and \(\hat{f}_{2}(k/n)\) are estimates of the corresponding theoretical spectral densities \(f_{1}(\nu)\) and \(f_{2}(\nu)\) of the two processes. If the covariances between the two processes are stationary (i.e., depend only on the time difference between the observations), define the crosscovariance function

\[\sigma_{12}(k)=\mathrm{Cov}(y_{1,J+k},y_{2,J})\,.\]

The bivariate process \((y_{1},y_{2})\) is defined to be second-order stationary if each marginal process is second-order stationary and if the crosscovariances are stationary. The spectral representation of \(\sigma_{12}(k)\) for a bivariate second-order stationary process is

\[\sigma_{12}(k)=\int_{-1/2}^{1/2}e^{2\pi i\nu k}f_{12}(\nu)d\nu,\]

which can be inverted as

\[f_{12}(\nu)=\sum_{k=-\infty}^{\infty}\sigma_{12}(k)e^{-2\pi i\nu k}\,.\]The statistic \(\hat{f}_{12}(k/n)\) can be viewed as an estimate of \(f_{12}(k/n)\), and \(\hat{\gamma}_{12}^{2}(k/n)\) can be viewed as an estimate of

\[\gamma_{12}^{2}(\nu)=\frac{|f_{12}(\nu)|^{2}}{f_{1}(\nu)f_{2}(\nu)}\,.\]

### Additional Exercises

#### Exercise 6.9.1

Show that the following functions are nonnegative definite.

(a)

\[\sigma(k)=\left\{\begin{array}{ll}1&\mbox{if $k=0$}\\ \frac{14}{27}&\mbox{if $k=\pm 1$}\\ \frac{4}{27}&\mbox{if $k=\pm 2$}\\ 0&\mbox{other $k$}\,.\end{array}\right.\]

(b)

\[\sigma(k)=\left\{\begin{array}{ll}1&\mbox{if $k=0$}\\ \rho&\mbox{if $k=\pm 1$}\\ \rho^{2}&\mbox{if $k=\pm 2$}\\ 0&\mbox{other $k$}\,.\end{array}\right.\]

The function \(\sigma(k)\) is nonnegative definite if, for any \(n\), the \(n\times n\) matrix \(\Sigma\equiv[\sigma(i-j)]\) is nonnegative definite.

#### Exercise 6.9.2

Let \(\alpha\) and \(\beta\) be random variables with \(\mbox{E}(\alpha)=\mbox{E}(\beta)=0,\mbox{Var}(\alpha)=\mbox{Var}(\beta)=\sigma ^{2}\), and \(\mbox{Cov}(\alpha,\beta)=0\). Show that

\[y_{t}=\alpha\cos(2\pi\nu t)+\beta\sin(2\pi\nu t)\]

is a second-order stationary process.

#### Exercise 6.9.3

Suppose \(e_{t}\) is second-order stationary. Which of the following processes are second-order stationary?

(a) \(y_{t}=\exp[e_{t}]\).

(b) \(y_{t}=y_{t-1}+e_{t}\).

(c) \(y_{t}=x_{t}e_{t}\), where \(x_{t}\) is another second-order stationary process independent of \(e_{t}\).

#### Exercise 6.9.4

Consider the simple linear regression model \(y_{t}=\alpha+\beta t+e_{t}\), where \(e_{t}\) is a white noise process. Let \(w_{t}\) be the symmetric moving average of order 5 introduced in Example 6.6.1 as applied to the \(y_{t}\) process. Find \(\mbox{E}(w_{t})\) and \(\mbox{Cov}(w_{t},w_{t+k})\). Is the \(w_{t}\) process stationary? Why or why not?

**Exercise 6.9.5.** \(\quad\) Shumway (1988) reports data from Waldmeier (1960-1978, 1961) on the number of sunspots from 1748 to 1978. The data are collected monthly, and a symmetric moving average of length 12 has been applied. The data in Table 6.7 are the moving averages corresponding to June and December (read across rows). Do a frequency analysis of these data including appropriate confidence intervals and plots.

**Exercise 6.9.6.** \(\quad\) Box, Jenkins, and Reinsel (1994, p. 597) report data on international air travel. The values in Table 6.8 are the number of passengers in thousands. Do a frequency analysis of these data including appropriate transformations of the data, confidence intervals, and plots.

**Exercise 6.9.7.** \(\quad\) Let \(e_{t}\) be a white noise process. The spectral density of \(e_{t}\) is \(f_{e}(v)=\sigma^{2}\); see Exercise 6.2. Let \(|\phi_{1}|<1\) and \(|\theta_{1}|<1\).

(a) Find the spectral density of

\[y_{t}=e_{t}-\theta_{1}e_{t-1}\,.\]

Sketch the graph of the spectral density for \(\theta_{1}=.5\) and \(\theta_{1}=1\). Take \(\sigma^{2}=1\). Which frequencies are most important in \(y_{t}\)?

(b) Show that the spectral density of

\[y_{t}=\phi_{1}y_{t-1}+e_{t}\]

is

\[f_{y}(v)=\frac{\sigma^{2}}{1-2\phi_{1}\cos(2\pi v)+\phi_{1}^{2}}.\]

Sketch the graph of the spectral density for \(\phi_{1}=.5\) and \(\sigma^{2}=1\). Which frequencies are most important in \(y_{t}\)?

(c) Find the spectral density of

\[y_{t}=\phi_{1}y_{t-1}+e_{t}-\theta_{1}e_{t-1}\,.\]

Sketch the graph of the spectral density for \(\phi_{1}=.5\), \(\theta_{1}=.5\), and \(\sigma^{2}=1\). Which frequencies are most important in \(y_{t}\)?

(d) Find the spectral density of the symmetric moving average of order 5,

\[w_{t}=\frac{1}{5}\left(e_{t-2}+e_{t-1}+e_{t}+e_{t+1}+e_{t+2}\right).\]

Sketch the graph of the spectral density for \(\sigma^{2}=1\). Which frequencies are most important in \(w_{t}\)?

[MISSING_PAGE_EMPTY:9565]

[MISSING_PAGE_FAIL:260]

**Exercise 6.9.10.** Let \(y_{t}\) and \(w_{t}\) be two second-order stationary processes and suppose that \(f_{w}(\nu)\leq f_{y}(\nu)\) for all \(\nu\in[-\pi,\pi]\). Show that

\[\Sigma_{yy}-\Sigma_{ww}\]

is nonnegative definite, where \(\Sigma_{yy}\) and \(\Sigma_{ww}\) are the covariance matrices of \((y_{1},\cdots,y_{n})^{\prime}\) and \((w_{1},\cdots,w_{n})^{\prime}\), respectively.

**Exercise 6.9.11.** Show that the spectral density

\[f(\nu)=\frac{\pi-|\nu|}{\pi^{2}}\]

determines the covariance function

\[\sigma(k)=\left\{\begin{array}{ll}\sigma^{2}&\mbox{if $k=0$}\\ 4\sigma^{2}/(\pi|k|)^{2}&\mbox{if $k$ is odd}\\ 0&\mbox{otherwise.}\end{array}\right.\]

**Exercise 6.9.12.** If \(y_{t}\) and \(w_{t}\) are independent second-order stationary processes with spectral densities \(f_{y}\) and \(f_{w}\), find the spectral distribution of the stationary process

\[z_{t}=y_{t}+w_{t}.\]

**Exercise 6.9.13.** Suppose \(y_{t}\) is second-order stationary and \(f_{y}(\nu)\) is nonnegative, bounded, and \(f_{y}(1/2)\neq 0\). Let

\[w_{t}=y_{t}-y_{t-1}.\]

Find \(f_{w}(\nu)\) in terms of \(f_{y}(\nu)\).

**Exercise 6.9.14.** For \(\nu\in[0,\pi]\), define

\[f_{y}(\nu)=\left\{\begin{array}{ll}50&\nu\in[\frac{1}{4}-.01,\frac{1}{4}+.01] \\ 0&\mbox{otherwise}\end{array}\right.\]

and \(f_{y}(-\nu)=f_{y}(\nu)\). Find \(\sigma(0),\,\sigma(1),\,\sigma(2)\).

**Exercise 6.9.15.**

(a) Use the relationships

\[\sin(a+b)=\sin(a)\cos(b)+\cos(a)\sin(b)\]

and\[\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)\]

to show the following.

\[\cos(a)\cos(b) = \frac{1}{2}\left\{\cos(a+b)+\cos(a-b)\right\}.\] \[\cos(a)\sin(b) = \frac{1}{2}\left\{\sin(a+b)-\sin(a-b)\right\}.\] \[\sin(a)\sin(b) = \frac{1}{2}\left\{\cos(a-b)-\cos(a+b)\right\}.\]

(b) Recall that for complex numbers \(x\) with \(|x|<1\),

\[\sum_{t=1}^{n}x^{t}=\frac{x-x^{n+1}}{1-x}.\]

Apply this fact to \(\exp\Bigl{(}2\pi i\frac{j}{n}\Bigr{)}\) to show that, for any \(j=1,\ldots,n-1\),

\[\sum_{t=1}^{n}\cos\biggl{(}2\pi\frac{j}{n}t\biggr{)}=0=\sum_{t=1}^{n}\sin \biggl{(}2\pi\frac{j}{n}t\biggr{)}\.\]

(c) Prove Eqs. (6.2.4), (6.2.5), and (6.2.6).

**Exercise 6.9.16.**  Apply the relationship in Exercise 6.9.15a on the sine of a sum to the process in Exercise 6.9.2 to show that the process can be rewritten as

\[y_{t}=A\sin(2\pi\nu t+\phi),\]

where \(A\) is the random amplitude and \(\phi\) is the random phase of the sine curve. Find the relationship between \((A,\phi)\) and \((\alpha,\beta)\). How does this relate to the basic spectral approximation in (6.3.1)? If, instead, we write \(y_{t}=A_{*}\cos(2\pi\nu t+\phi_{*})\), how do \((A_{*},\phi_{*})\) differ from \((A,\phi)\)?

## References

* Bloomfield (1976) Bloomfield, P. (1976). _Fourier analysis of time series: An introduction_. New York: Wiley.
* Box et al. (1994) Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (1994). _Time series analysis: Forecasting and control_ (3rd ed.). New York: Wiley.
* Breiman (1968) Breiman, L. (1968). _Probability_. Reading: Addison-Wesley.
* Brillinger (1981) Brillinger, D. R. (1981). _Time series: Data analysis and theory_ (2nd ed.). San Francisco: Holden Day.

* Brockwell and Davis (1991) Brockwell, P. J., & Davis, R. A. (1991). _Time series: Theory and methods_ (2nd ed.). New York: Springer.
* Brockwell and Davis (2002) Brockwell, P. J., & Davis, R. A. (2002). _Introduction to time series and forecasting_ (2nd ed.). New York: Springer.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th edn.). New York: Springer.
* Cressie and Wikle (2011) Cressie, N. A.C., & Wikle, C. K. (2011). _Statistics for spatio-temporal data_. New York: Wiley.
* Doob (1953) Doob, J. L. (1953). _Stochastic processes_. New York: Wiley.
* Fuller (1996) Fuller, W. A. (1996). _Introduction to statistical time series_ (2nd ed.). New York: Wiley.
* Hannan (1970) Hannan, E. J. (1970). _Multiple time series_. New York: Wiley.
* Koopmans (1974) Koopmans, L. H. (1974). _The spectral analysis of time series_. New York: Academic.
* Prado and West (2010) Prado, R., & West, M. (2010). _Time series: Modeling, computation, and inference_. Boca Raton: CRC Press.
* Shumway (1988) Shumway, R. H. (1988). _Applied statistical time series analysis_. Englewood Cliffs: Prentice-Hall.
* Shumway and Stoffer (2011) Shumway, R. H., & Stoffer, D. S. (2011). _Time series analysis and its applications: With R examples_ (3rd ed.). New York: Springer.
* Tukey (1977) Tukey, J. W. (1977). _Exploratory data analysis_. Reading: Addison-Wesley.
* Waldmeier (1960) Waldmeier, M. (1960-1978). _Monthly sunspot bulletin_. Zurich: Swiss Federal Observatory.
* Waldmeier (1961) Waldmeier, M. (1961). _The sunspot activity in the years 1610-1960_. Zurich: Swiss Federal Observatory.

## Chapter 7 Time Domain Analysis

**Abstract** This chapter develops Box-Jenkins models. These involve applying the linear filters of Chap. 6 to white noise. It also introduces state-space models and the Kalman filter.

In the previous chapter frequency domain analysis was introduced using a spectral approximation for an arbitrary second-order stationary time series \(\ldots,y_{-1},y_{0},y_{1},\ldots\). In the traditional approach, the variance components in the spectral approximation will be nonzero, thus there is little chance of developing a parsimonious model. (The model with measurement error is more promising in that regard.)

The time domain approach to time series analysis does not apply to arbitrary stationary processes. The time domain assumes that the process can be modeled as a simple recursive filter, a causal linear filter, or a general recursive filter of an uncorrelated stationary error process. Linear filters were discussed in Sect. 6.6. The uncorrelated stationary error process is often referred to as _white noise_. It has the same properties as measurement error. When the filtered process is stationary, a general recursive filter, and hence a simple recursive filter, can also be modeled as a causal linear filter. The particular choice of a model is determined by selecting a filter that does a good job of explaining the observed series and is relatively parsimonious (i.e., has few parameters).

In time domain analysis, the covariance function is again very important. In addition, partial correlations between observations are important. In this chapter, correlations and general prediction are discussed in Sect. 7.1. Section 7.2 introduces the various time domain models. These sections are followed by discussions of prediction (forecasting), estimation, model selection, and seasonal adjustment. The chapter closes with consideration of dynamic linear models also known as the state-space model. State-space methodology is closely tied to the Kalman filter. The reader needs to be familiar with best linear prediction, which is discussed thoroughly in _PA_ Sects. 6.3-6.5 and is reviewed in Appendix B. Example 9.3.1 relates partial correlations to multivariate linear models and best linear prediction is implicit in the review of best linear unbiased prediction in Sect. 4.1.

The time domain analysis of time series is discussed in many books and research articles. The classic text on the subject is Box, Jenkins, Reinsel, and Ljung (2015). Shorter discussions can be found in general texts on time series, e.g., Shumway and Stoffer (2011), Brockwell and Davis (1991, 2002), Prado and West (2010), Chatfield (2003), Diggle (1990), Fuller (1996).

### Correlations and Prediction

We have seen that the (auto)covariance function \(\sigma(k)\) determines the spectral distribution and the variance components of the spectral approximation. Although time domain analysis is very different in spirit from frequency analysis, the covariance function again plays an important role. Along with the covariance function, time domain analysis uses the correlation function, the partial covariance function, and the partial correlation function. The purpose of this section is to define these three additional functions and to relate partial correlation to best linear prediction. Assume a second-order stationary process \(\cdots,y_{-1},y_{0},y_{1},\cdots\) with mean \(\mu\) and covariance function \(\sigma(k)\).

The _correlation function_ is simply

\[\rho(k) \equiv {\rm Cov}(y_{t},y_{t+k})\Big{/}\sqrt{{\rm Var}(y_{t}){\rm Var}(y_ {t+k})}\] \[= \sigma(k)\Big{/}\sqrt{\sigma(0)\sigma(0)}\] \[= \sigma(k)/\sigma(0)\,.\]

The _partial correlation function_ is defined to be the partial correlation between \(y_{t}\) and \(y_{t+k}\) given \(y_{t+1},\ldots,y_{t+k-1}\), cf. _PA_ Sect. 6.5. Of course, this only makes sense for \(k\geq 2\). Using notation similar to that in Appendix B, let \(y=(y_{t},y_{t+k})^{\prime}\) and \(x=(y_{t+1},\ldots,y_{t+k-1})^{\prime}\). The _partial covariance_ is the off-diagonal element of

\[V_{yy}-V_{yx}V_{xx}^{-1}V_{xy},\]

where

\[V_{yy} = \left[\begin{array}{cc}\sigma(0)&\sigma(k)\\ \sigma(k)&\sigma(0)\end{array}\right],\] \[V_{yx} \equiv V_{xy}^{\prime} \equiv \left[\begin{array}{c}\sigma_{1}^{\prime}\\ \sigma_{2}^{\prime}\end{array}\right]=\left[\begin{array}{cc}\sigma(1)& \sigma(2)&\cdots&\sigma(k-1)\\ \sigma(k-1)&\sigma(k-2)&\cdots&\sigma(1)\end{array}\right], \tag{7.1.1}\]

and

\[V_{xx} = \left[\begin{array}{cccc}\sigma(0)&\sigma(1)&\sigma(2)&\cdots& \sigma(k-2)\\ \sigma(1)&\sigma(0)&\sigma(1)&\cdots&\sigma(k-3)\\ \sigma(2)&\sigma(1)&\sigma(0)&\cdots&\sigma(k-4)\\ \vdots&\vdots&\vdots&&\vdots\\ \sigma(k-2)&\sigma(k-3)&\sigma(k-4)&\cdots&\sigma(0)\end{array}\right]. \tag{7.1.2}\]

Note that none of these matrices depend on \(t\); all are functions of \(k\) alone.

The partial correlation function is denoted \(\phi(k)\). For \(k=1\), define \(\phi(1)\equiv\rho(1)\). For \(k\geq 2\), \(\phi(k)\) is the ratio of the partial covariance of \(y_{t}\) and \(y_{t+k}\) to the product of the partial standard deviations of \(y_{t}\) and \(y_{t+k}\). In particular, the partial covariance is

\[\sigma(k)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{2}\,.\]

The partial variances for \(y_{t}\) and \(y_{t+k}\) are, respectively,

\[\sigma(0)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}\]

and

\[\sigma(0)-\sigma_{2}^{\prime}V_{xx}^{-1}\sigma_{2}\,.\]

Thus,

\[\phi(k)\equiv\{\sigma(k)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{2}\}\bigg{/} \sqrt{\{\sigma(0)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}\}\{\sigma(0)- \sigma_{2}^{\prime}V_{xx}^{-1}\sigma_{2}\}}\,.\]

Partial correlations can also be defined recursively which may be a better way to compute them (see _PA_ Exercise 6.9.9).

**Exercise 7.1**.: Let \(H=[h_{ij}]\) be a \((k-1)\times(k-1)\) matrix, where

\[h_{ij}=\cases{1&if $i+j=k$\cr 0&otherwise.\cr}\]

\(H\) is of the form

\[H=\left[\matrix{0&\cdots&0&0&1\cr 0&\cdots&0&1&0\cr 0&\cdots&1&0&0\cr\vdots&& \vdots&\vdots&\vdots\cr 1&\cdots&0&0&0\cr}\right]\,.\]

Show that \(\sigma_{1}^{\prime}H=\sigma_{2}^{\prime}\), \(\sigma_{2}^{\prime}H=\sigma_{1}^{\prime}\), \(HV_{xx}H=V_{xx}\) and \(HH=I\).

Using the results of Exercise 7.1, we can show that

\[\sigma(0)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}=\sigma(0)-\sigma_{2}^{ \prime}V_{xx}^{-1}\sigma_{2}\]

(i.e., the partial variances are the same for \(y_{t}\) and \(y_{t+k}\)). To see this, observe that

\[\sigma_{2}^{\prime}V_{xx}^{-1}\sigma_{2} = \sigma_{2}^{\prime}HHV_{xx}^{-1}HH\sigma_{2}\] \[= \sigma_{1}^{\prime}HV_{xx}^{-1}H\sigma_{1}\] \[= \sigma_{1}^{\prime}(H^{-1}V_{xx}H^{-1})^{-1}\sigma_{1}\] \[= \sigma_{1}^{\prime}(HV_{xx}H)^{-1}\sigma_{1}\] \[= \sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}\,.\]It follows that

\[\phi(k)=\{\sigma(k)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{2}\}\Big{/}\{\sigma(0)- \sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}\}\,.\]

#### Partial Correlation and Best Linear Prediction

We now investigate the relationship between the coefficient of \(y_{t}\) in the best linear predictor of \(y_{t+k}\) based on \(y_{t},y_{t+1},\ldots,y_{t+k-1}\) and the partial correlation between \(y_{t+k}\) and \(y_{t}\). In particular, we will show that these are identical. From Appendix B, the best linear predictor is

\[\hat{E}(y_{t+k}|y_{t},x)=\mu+\delta^{\prime}\left[\begin{pmatrix}y_{t}\\ x\end{pmatrix}-J\mu\right],\]

where

\[\delta^{\prime}=[\sigma(k),\sigma_{2}^{\prime}]\left[\begin{matrix}\sigma(0)& \sigma_{1}^{\prime}\\ \sigma_{1}&V_{xx}\end{matrix}\right]^{-1}. \tag{7.1.3}\]

The coefficient of \(y_{t}\) is \(\delta_{k}\) in \(\delta^{\prime}=(\delta_{k},\ldots,\delta_{1})\). Let \(K=[\sigma(0)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}]^{-1}\), from the standard result on the inverse of a partitioned matrix, see Exercise B.4,

\[\left[\begin{matrix}\sigma(0)&\sigma_{1}^{\prime}\\ \sigma_{1}&V_{xx}\end{matrix}\right]^{-1}=\left[\begin{matrix}K&-K\sigma_{1}^{ \prime}V_{xx}^{-1}\\ -V_{xx}^{-1}\sigma_{1}K&V_{xx}^{-1}+V_{xx}^{-1}\sigma_{1}\sigma_{1}^{\prime}V_{ xx}^{-1}K\end{matrix}\right],\]

so

\[\delta_{k} = [\sigma(k),\sigma_{2}^{\prime}]\left[\begin{matrix}K\\ -V_{xx}\sigma_{1}K\end{matrix}\right]\] \[= (\sigma(k)-\sigma_{2}^{\prime}V_{xx}^{-1}\sigma_{1})K\] \[= \{\sigma(k)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{2}\}\Big{/}\{ \sigma(0)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}\}\] \[= \phi(k)\,.\]

Because this is a second-order property of the process \(y_{t}\) (i.e., involves only means, variances, and covariances), and because \(y_{t}\) is second-order stationary, it follows immediately that \(\phi(k)\) is also the coefficient of \(y_{t-k}\) in the best linear predictor of \(y_{t}\) from \(y_{t-1},\ldots,y_{t-k}\).

Recursive methods exist for finding BLPs based on \(Y=(y_{1},\ldots,y_{n})^{\prime}\). The difficulty of computing the predictors directly is that finding best linear predictors requires solving a system of equations based on an \(n\times n\) matrix. The obvious method of solution involves taking the inverse of the \(n\times n\) matrix. To be computationally practical for large sample sizes, more sophisticated methods of finding a solution are needed. We discuss two methods for finding \(\hat{E}(y_{n+1}|Y)\). A more detailed discussion of prediction methods based on \(Y\) is given by Brockwell and Davis (1991, Section 5.2).

#### The Durbin-Levinson Algorithm

The _Durbin-Levinson algorithm_ is a recursive procedure for predicting the future in a stationary process. Having observed \(Y\), to find the BLP of \(y_{n+1}\) Durbin-Levinson applies of the results in the previous subsection with \(t=1\) and \(k=n\), repeats them for \(t=2\), \(k=n-1\), then for \(t=3\), \(k=n-2\), etc.,

\[\hat{E}(y_{n+1}|Y)=\mu+\delta^{\prime}(Y-\mu J)=\mu+[\sigma(n),\sigma_{2}^{ \prime}]\left[\begin{array}{cc}\sigma(0)&\sigma_{1}^{\prime}\\ \sigma_{1}&V_{xx}\end{array}\right]^{-1}(Y-\mu J).\]

The BLP without including \(y_{1}\) is

\[\hat{E}(y_{n+1}|y_{2},\ldots,y_{n})=\mu+\sigma_{2}^{\prime}V_{xx}^{-1}\left[ \begin{array}{c}y_{2}-\mu\\ \vdots\\ y_{n}-\mu\end{array}\right].\]

The Durbin-Levinson algorithm establishes a simple formula for the BLP based on \(Y\) in terms of \(\sigma_{2}^{\prime}V_{xx}^{-1}\), which is the only difficult thing to find in \(\hat{E}(y_{n+1}|y_{2},\ldots,y_{n})\). But finding \(\sigma_{2}^{\prime}V_{xx}^{-1}\) is easier than finding \(\delta\), because \(\delta\) requires the inversion of an \(n\times n\) matrix but \(V_{xx}\) is only \((n-1)\times(n-1)\). The idea then is to apply the partitioning idea to \(\hat{E}(y_{n+1}|y_{2},\ldots,y_{n})\) and find it by inverting an \((n-2)\times(n-2)\) matrix and continue the process to where you only need to invert a \(1\times 1\) matrix.

Writing \(\delta=(\delta_{n},\ldots,\delta_{1})\), Sect. 7.1.1 shows that

\[\delta_{n}=\big{(}\sigma(n)-\sigma_{2}^{\prime}V_{xx}^{-1}\sigma_{1}\big{)}K,\]

where

\[K=[\sigma(0)-\sigma_{1}^{\prime}V_{xx}^{-1}\sigma_{1}]^{-1}.\]

Write the other elements of \(\delta\) as \(\delta_{\mathrm{s}}=(\delta_{n-1},\ldots,\delta_{1})\). Using the results of Sect. 7.1, it is easily seen that

\[\delta_{\mathrm{s}} = \sigma_{2}^{\prime}V_{xx}^{-1}-\delta_{n}\sigma_{1}^{\prime}V_{xx }^{-1}\] \[= \sigma_{2}^{\prime}V_{xx}^{-1}-\delta_{n}\sigma_{2}^{\prime}V_{xx }^{-1}H.\]

Recall that the vectors \(\sigma_{1}\) and \(\sigma_{2}\) have the same entries in reverse order. Thus, solving the reduced data problem to find \(\sigma_{2}^{\prime}V_{xx}^{-1}\) leads to simple formulae for the full data values \(K\), \(\delta_{n}\), and \(\delta_{\mathrm{s}}\).

#### Innovations Algorithm

The _innovations algorithm_ is based on repeated application of Proposition B.1.8 to obtain \(\hat{E}(y_{n+1}|Y)\) as a linear combination of the prediction errors, say, \(\varepsilon_{1}\equiv y_{1}\) and, for \(t>1\), \(\varepsilon_{t}\equiv e(y_{t}|y_{t-1},\ldots,y_{1})\equiv y_{t}-\hat{E}(y_{t}| y_{t-1},\ldots,y_{1})\); see Exercise 7.2. Write thebest linear predictor as

\[\hat{E}(y_{n+1}|Y)=\sum_{t=1}^{n}\eta_{t}\varepsilon_{t}. \tag{7.1.4}\]

To use this equation, one needs to know the \(\eta_{t}\)s and the \(\varepsilon_{t}\)s. The \(\eta_{t}\)s are determined by the repeated application of Proposition B.1.8. Note that the \(\eta_{t}\)s depend on \(n\). The \(\varepsilon_{t}\)s are found recursively. The value of \(\varepsilon_{1}=y_{1}\) is known. For \(t=2,\ldots,n\), \(\hat{E}(y_{t}|y_{t-1},\ldots y_{1})\), and thus \(\varepsilon_{t}\), can be found using exactly the same procedure as used for \(\hat{E}(y_{n+1}|Y)\).

#### Exercise 7.2.

1. Using Proposition B.1.8, find \(\hat{E}(y_{4}|y_{3},y_{2},y_{1})\) in terms of \(\sigma(\cdot)\), \(y_{1}\), \(e(y_{2}|y_{1})\), and \(e(y_{3}|y_{2},y_{1})\).
2. Use induction to show that \(y_{1},e(y_{2}|y_{1}),\cdots,e(y_{n}|y_{n-1},\cdots,y_{1})\) are uncorrelated.

To obtain \(\hat{E}(y_{n+k}|Y)\), first obtain \(\hat{E}(y_{n+k}|Y,y_{n+1},\ldots,y_{n+k-1})\) as a linear combination of prediction error terms. From Exercise B.5 and the equivalent of Eq. (7.1.4) for predicting \(y_{n+k}\),

\[\hat{E}(y_{n+k}|Y) =\hat{E}\big{[}\hat{E}(y_{n+k}|Y,y_{n+1},\ldots,y_{n+k-1})\big{|}Y \big{]}\] \[=\hat{E}\left[\sum_{t=1}^{n+k}\eta_{t}\varepsilon_{t}\right|Y\right].\]

By Proposition B.1.4, Exercise 7.2, Proposition B.1.5, and Corollary B.1.3,

\[\hat{E}\left[\sum_{t=1}^{n+k}\eta_{t}\varepsilon_{t}\big{|}Y\right] =\hat{E}\left[\sum_{t=1}^{n+k}\eta_{t}\varepsilon_{t}\Big{|} \varepsilon_{n},\ldots,\varepsilon_{1}\right]\] \[=\hat{E}\left[\sum_{t=1}^{n}\eta_{t}\varepsilon_{t}\Big{|} \varepsilon_{n},\ldots,\varepsilon_{1}\right]\] \[=\sum_{t=1}^{n}\eta_{t}\varepsilon_{t}.\]

Note that the \(\eta_{t}\)s depend on \(n+k\).

Methods related to the Durbin-Levinson and innovations algorithms can also be used to obtain the exact prediction variance.

### Time Domain Models

We now consider the various models used for analyzing time series data in the time domain. Generally, these are stationary models, so their frequency properties can be studied using the methods of Chap. 6. Time domain models are just linear filters of the white noise process. White noise is the name used for the uncorrelated error process \(e_{t}\), where

\[\mathrm{E}(e_{t})=0;\qquad\mathrm{Var}(e_{t})=\sigma^{2};\qquad\mathrm{Cov}(e_{t },e_{t^{\prime}})=0,\quad t\neq t^{\prime}.\]

In particular, because time domain models are linear filters, their frequency properties are easily derived from Sect. 6.6 together with the fact, established in Exercise 6.5, that the spectral density of white noise is \(f_{e}(\nu)=\sigma^{2}\).

#### Autoregressive Models: \(\mathbf{AR}(p)\)s

Let \(\ldots,e_{-1},e_{0},e_{1},\ldots\) be a stationary process of uncorrelated errors (i.e., white noise). An autoregressive model of order \(p\), denoted \(AR(p)\), is a model that states that the observable time series is a simple recursive filter of the error process involving \(p\) terms, namely

\[y_{t}=\sum_{s=1}^{p}\phi_{s}y_{t-s}+e_{t}\,. \tag{7.2.1}\]

Because the \(AR(p)\) model is just a simple recursive filter, the results of Exercises 6.2 and 6.6 (in Sects. 6.3 and 6.6 respectively) fully determine which frequencies are important in a stationary autoregressive process.

Whether an autoregressive process is stationary depends on another way of looking at the process. Let \(B\) be the _backshift operator_, namely

\[By_{t}\equiv y_{t-1}\,.\]

This is a very useful tool in writing models. For example, the \(\nabla\) operator of Example 6.6.2 is

\[\nabla y_{t}=(1-B)y_{t},\]

and

\[B^{2}y_{t}=B(By_{t})=By_{t-1}=y_{t-2}\,.\]

To examine the \(AR(p)\) model, let

\[\Phi(B)\equiv 1-\sum_{s=1}^{p}\phi_{s}B^{s}\,.\]

The \(AR(p)\) model (7.2.1) can be rewritten as

\[y_{t}-\sum_{s=1}^{p}\phi_{s}y_{t-s}=e_{t}\]or

\[\Phi(B)y_{t}=e_{t}\,. \tag{7.2.2}\]

Note that \(\Phi(B)\) is a polynomial in the backshift operator. If we substitute a scalar variable \(x\) for \(B\), we get a standard polynomial \(\Phi(x)\). The roots of \(\Phi(x)\) (i.e., the solutions to \(\Phi(x)=0\)) are generally complex numbers. Let \(x_{0}\) be an arbitrary root. If all of the roots satisfy \(|x_{0}|^{2}>1\), then the rational polynomial \(1/\Phi(x)\) can be written as an infinite polynomial. This follows from doing a Taylor expansion of the function \(1/\Phi(x)\) about \(0\); see Exercise 7.9.14. In particular, there exists

\[\Psi(B)\equiv 1+\sum_{s=1}^{\infty}\psi_{s}B^{s}\]

such that

\[\sum_{s=1}^{\infty}|\psi_{s}|<\infty \tag{7.2.3}\]

and

\[\Psi(B)\Phi(B)=1\,. \tag{7.2.4}\]

Equation (7.2.4) states that the product of the polynomials \(\Psi(B)\) and \(\Phi(B)\) is the constant polynomial that takes only the value \(1\).

Applying \(\Psi(B)\) to (7.2.2) yields

\[y_{t}=\Psi(B)\Phi(B)y_{t}=\Psi(B)e_{t}=e_{t}+\sum_{s=1}^{\infty}\psi_{s}e_{t- s}, \tag{7.2.5}\]

so the \(AR(p)\) model (7.2.1) can be written as a causal linear filter of the error process. Condition (7.2.3) implies that \(y_{t}\) is a mean zero second-order stationary process (see Sect. 6.6). The restriction that all roots \(x_{0}\) have \(|x_{0}|^{2}>1\) is needed to obtain stationarity.

The coefficients in \(\Psi(B)\) can be identified by solving an infinite system of equations. Note that (7.2.4) will occur if and only if the polynomials in the scalar \(x\) satisfy \(\Psi(x)\Phi(x)=1\). Because the constant terms in both \(\Psi(x)\) and \(\Phi(x)\) are \(1\), \(\Psi(x)\Phi(x)=1\) if and only if the coefficient of \(x^{k}\) in \(\Psi(x)\Phi(x)\) equals zero for every \(k\geq 1\).

**Exercise 7.3.**  Show that in an \(AR(1)\) model with \(|\phi_{1}|<1\),

\[\psi_{k}=\phi_{1}^{k}\,.\]

The forms (7.2.2) and (7.2.5) are useful in finding the covariance function \(\sigma_{y}(\cdot)\) of the process \(y_{t}\). Let \(k\) be nonnegative and note that

\[\text{Cov}(\Phi(B)y_{t},y_{t-k})=\text{Cov}(e_{t},y_{t-k}). \tag{7.2.6}\]Using the fact that \(\sigma_{y}(k)=\sigma_{y}(-k)\), the left-hand side is

\[\text{Cov}(\Phi(B)y_{t},y_{t-k}) = \text{Cov}\left(y_{t}-\sum_{s=1}^{p}\phi_{s}y_{t-s},y_{t-k}\right)\] \[= \sigma_{y}(-k)-\sum_{s=1}^{p}\phi_{s}\sigma_{y}(-k+s)\] \[= \sigma_{y}(k)-\sum_{s=1}^{p}\phi_{s}\sigma_{y}(k-s).\]

The right-hand side of (7.2.6) is

\[\text{Cov}(e_{t},y_{t-k}) = \text{Cov}(e_{t},\Psi(B)e_{t-k})\] \[= \text{Cov}(e_{t},e_{t-k})+\sum_{s=1}^{\infty}\psi_{s}\text{Cov}( e_{t},e_{t-k-s})\] \[= \left\{\begin{array}{ll}\sigma^{2}&\text{if $k=0$}\\ 0&\text{if $k\neq 0$}\end{array}\right..\]

Setting equal the reexpressions of the left- and right-sides of (7.2.6) gives

\[\sigma_{y}(0)-\sum_{s=1}^{p}\phi_{s}\sigma_{y}(s)=\sigma^{2}\]

and for \(k\geq 1\)

\[\sigma_{y}(k)-\sum_{s=1}^{p}\phi_{s}\sigma_{y}(k-s)=0\,.\]

These are known as the _Yule-Walker equations_ and for given \(\phi_{s}\)s are solved to find \(\sigma_{y}(k)\) for \(k=0,1,\ldots\). The symmetry of \(\sigma_{y}(\cdot)\) about zero completes its characterization.

The covariance function can also be characterized in terms of the \(\psi\)s. Details are given in Eq. (7.2.12) for autoregressive moving average (_ARMA_) models. Because autoregressive models are also _ARMA_ models, the later discussion applies to the current case. In particular, the covariance function for an \(AR(1)\) process is given later in Example 7.2.3.

We now establish that the partial correlation function has the properties that

\[\phi(p)=\phi_{p}\]

and

\[\phi(k)=0\qquad k=p+1,p+2,\ldots\,.\]

This is a key feature in identifying the order \(p\) of an \(AR\) model. One can estimate the partial correlations and if they obviously drop off after some point, that should indicate \(p\). The result follows from the fact that 

[MISSING_PAGE_EMPTY:9579]

To deal with time series that have a nonzero mean, write \(\mathrm{E}(y_{t})=\mu\) and use the \(AR(p)\) model

\[\Phi(B)[y_{t}-\mu]=e_{t}\,.\]

Equivalently, we can write

\[\Phi(B)y_{t} = \Phi(1)\mu+e_{t}\] \[= \left(1-\sum_{s=1}^{p}\phi_{s}\right)\mu+e_{t}\] \[= \alpha+e_{t},\]

where \(\alpha\equiv\left(1-\sum_{s=1}^{p}\phi_{s}\right)\mu\). Another equivalent expression is

\[y_{t}=\alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}+e_{t}\,.\]

Note that

\[\mathrm{E}(y_{t})\equiv\mu=\alpha\Big{/}\left(1-\sum_{s=1}^{p}\phi_{s}\right).\]

#### Moving Average Models: \(\mathrm{MA}(q)\)s

A moving average model of order \(q\), denoted \(MA(q)\), is just a causal linear filter of the error (white noise) process. In particular,

\[y_{t}=e_{t}-\sum_{s=1}^{q}\theta_{s}e_{t-s}\,. \tag{7.2.8}\]

The covariance function and spectral densities for linear filters were discussed in Sect. 6.6. Applying (6.6.4) gives

\[\sigma_{y}(k)=\left\{\begin{array}{ll}\sigma^{2}\left(1+\sum_{s=1}^{q}\theta _{s}^{2}\right)&k=0\\ \sigma^{2}\left(-\theta_{k}+\sum_{s=1}^{q-k}\theta_{s+k}\theta_{s}\right)&k=1, \ldots,q-1\\ -\sigma^{2}\theta_{q}&k=q\\ 0&k>q\.\end{array}\right.\]

For model selection, this result is analogous to the fact that an \(AR(p)\) model has \(\phi(k)=0\) for \(k>p\). If we estimate \(\sigma_{y}(k)\) or equivalently the correlation \(\rho_{y}(k)\), and if the estimated correlations are negligible for, say, \(k>5\), then an \(MA(5)\) model is suggested.

Using the backshift operator, (7.2.8) can be written as

\[y_{t}=\Theta(B)e_{t}\,, \tag{7.2.9}\]where

\[\Theta(B)\equiv 1-\sum_{s=1}^{q}\theta_{s}B^{s}.\]

Note that with only a finite number of nonzero coefficients in the filter, the process \(y_{t}\) is a mean zero second-order stationary process. If the roots of \(\Theta(x)\) are all outside the unit circle in the complex plane, the process defined by (7.2.9) is said to be _invertible_. In such a case, \(1/\Theta(B)\) can be written as an infinite polynomial in \(B\) and \(e_{t}\) can be written as a causal linear filter of \(y_{t}\).

Processes with nonzero mean are written

\[y_{t}-\mu=\Theta(B)e_{t}\,.\]

Example 7.2.2.: The \(MA(1)\) process, \(y_{t}=\mu+e_{t}-\theta_{1}e_{t-1}\), has

\[\mathrm{E}(y_{t}) = \mu,\] \[\sigma(0) = \sigma^{2}(1+\theta_{1}^{2}),\] \[\sigma(1) = -\sigma^{2}\theta_{1},\] \[\sigma(k) = 0,\qquad k>1,\]

and

\[\rho(1)=\frac{-\theta_{1}}{1+\theta_{1}^{2}}.\]

#### Autoregressive Moving Average Models: \(\mathbf{ARMA}(p,q)s\)

In many ways, the most important time domain models are the autoregressive moving average models. These are general recursive filters of the error process, namely

\[y_{t}=\sum_{s=1}^{p}\phi_{s}y_{t-s}+e_{t}-\sum_{s=1}^{q}\theta_{s}e_{t-s}\,. \tag{7.2.10}\]

This is an \(AR(p)\) with the addition of \(q\) moving average terms. It is also an \(MA(q)\) with the addition of \(p\) autoregressive terms. Note that both the \(AR(p)\) and \(MA(q)\) models are special cases of \(ARMA\) models. Model (7.2.10) can be rewritten as

\[y_{t}-\sum_{s=1}^{p}\phi_{s}y_{t-s}=e_{t}-\sum_{s=1}^{q}\theta_{s}e_{t-s}\,,\]

or, using the backshift operator,

\[\Phi(B)y_{t}=\Theta(B)e_{t}\,. \tag{7.2.11}\]Here, we assume that the roots of both \(\Phi(x)\) and \(\Theta(x)\) are outside the unit circle in the complex plane. This ensures that the process is both stationary and invertible. We also assume that the two polynomials have no roots in common. If the polynomials have \(s\) roots in common, then \(s\) common terms can be factored out of both sides of (7.2.11), thus creating an \(ARMA(p-s,q-s)\) model. To eliminate duplication, we impose the condition of no common roots. The spectral density of model (7.2.11) can be obtained using the results in Sect. 6.6.

To compute the covariance function for an \(ARMA(p,q)\), use the model in the form (7.2.11). Assuming the roots of the polynomial are outside the unit circle, the rational polynomial \(1/\Phi(B)\) is itself an infinite polynomial. Write

\[\Psi(B)=\Theta(B)/\Phi(B);\]

thus, dividing (7.2.11) by \(\Phi(B)\) gives

\[y_{t}=\Psi(B)e_{t}\,.\]

Computing the covariance function directly for \(k\geq 0\),

\[\sigma(k)=\text{Cov}(y_{t},y_{t+k}) = \text{Cov}(\Psi(B)e_{t},\Psi(B)e_{t+k}) \tag{7.2.12}\] \[= \text{Cov}\left(\sum_{s=0}^{\infty}\psi_{s}e_{t-s},\sum_{s^{ \prime}=0}^{\infty}\psi_{s^{\prime}}e_{t+k-s^{\prime}}\right)\] \[= \sum_{s=0}^{\infty}\sum_{s^{\prime}=0}^{\infty}\psi_{s}\psi_{s^{ \prime}}\text{Cov}(e_{t-s},e_{t+k-s^{\prime}})\] \[= \sum_{s=0}^{\infty}\psi_{s}\psi_{s+k}\text{Cov}(e_{t-s},e_{t-s})\] \[= \sigma^{2}\sum_{s=0}^{\infty}\psi_{s}\psi_{s+k}\,.\]

Equation (7.2.12) is just a special case of (6.6.4). Note that it can also be used to find the covariance function for an \(AR(p)\) process, where we consider an \(AR(p)\) as an \(ARMA(p,0)\) process.

As usual, to model a process \(y_{t}\) with \(\text{E}(y_{t})=\mu\), use

\[\Phi(B)[y_{t}-\mu]=\Theta(B)e_{t}\,.\]

An equivalent form is

\[y_{t}=\alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}+e_{t}-\sum_{s=1}^{q}\theta_{s}e_{t- s},\]

where \(\alpha\equiv\left(1-\sum_{s=1}^{p}\phi_{s}\right)\mu=\Phi(1)\mu\). Covariance properties are not affected by this change.

It is interesting to note that \(ARMA\) models can be used to approximate the covariance structure of any stationary process with \(\sigma(k)\to 0\) as \(k\to\infty\). In particular,for such a covariance function and any integer \(K\geq 0\), there exists an \(ARMA(p,q)\) process \(y_{t}\) with covariance function \(\sigma_{y}(k)\) such that

\[\sigma_{y}(k)=\sigma(k)\text{ for }k=0,1,\ldots,K.\]

Example 7.2.3. _The Covariance Function for an \(\operatorname{AR}(1)\)_.

Identify the \(AR(1)\) process with an \(ARMA(1,0)\); thus,

\[\Theta(B)=1\,.\]

Writing

\[\Phi(B)=1-\phi_{1}B,\]

it is easily seen that if \(|\phi_{1}|<1\),

\[\Psi(B)=1/\Phi(B)=\sum_{s=0}^{\infty}\phi_{1}^{s}B^{s}\,.\]

Applying (7.2.12) and using the fact that \(\sum_{s=0}^{\infty}v^{s}=\frac{1}{1-v}\) for \(v\in(0,1)\) yields

\[\sigma(k) = \sigma^{2}\sum_{s=0}^{\infty}\phi_{1}^{2s+k}\] \[= \sigma^{2}\phi_{1}^{k}\sum_{s=0}^{\infty}\left(\phi_{1}^{2}\right) ^{s}\] \[= \sigma^{2}\phi_{1}^{k}\Big{/}(1-\phi_{1}^{2})\]

for \(k\geq 0\). From the symmetry of the covariance function, the correlation function is

\[\rho(k)=\phi_{1}^{|k|}\,.\]

Example 7.2.4. _The \(ARMA(1,1)\) model is_

\[[1-\phi_{1}(B)](y_{t}-\mu)=e_{t}-\theta_{1}e_{t-1}\]

_or_

\[y_{t}=(1-\phi_{1})\mu+\phi_{1}y_{t-1}+e_{t}-\theta_{1}e_{t-1}\,. \tag{7.2.13}\]

_The process is stationary if \(|\phi_{1}|<1\),_

\[\operatorname{E}(y_{t}) = (1-\phi_{1})\mu+\phi_{1}\operatorname{E}(y_{t-1})+0\] \[= (1-\phi_{1})\mu+\phi_{1}\mu\] \[= \mu\,.\]The covariance function \(\sigma_{y}(\cdot)\) can be computed via application of equality (7.2.12). The autoregressive transformation is

\[\Phi(B)=1-\phi_{1}B,\]

so

\[1\big{/}\Phi(B) = 1+\phi_{1}B+\phi_{1}^{2}B^{2}+\phi_{1}^{3}B^{3}+\cdots\] \[= \sum_{s=0}^{\infty}\phi_{1}^{s}B^{s}\,.\]

The moving average polynomial is

\[\Theta(B)=1-\theta_{1}B;\]

thus,

\[\Psi(B) = \Theta(B)\big{/}\Phi(B) \tag{7.2.14}\] \[= \sum_{s=0}^{\infty}\phi_{1}^{s}B^{s}-\sum_{s=0}^{\infty}\theta_{1 }\phi_{1}^{s}B^{s+1}\] \[= 1+\sum_{s=1}^{\infty}(\phi_{1}^{s}-\theta_{1}\phi_{1}^{s-1})B^{s}\] \[= 1+\sum_{s=1}^{\infty}\phi_{1}^{s-1}(\phi_{1}-\theta_{1})B^{s}\,.\]

Applying (7.2.12),

\[\sigma(0) = \sigma^{2}\left[1+\sum_{s=1}^{\infty}\phi_{1}^{2(s-1)}(\phi_{1}- \theta_{1})^{2}\right]\] \[= \sigma^{2}[1+(\phi_{1}-\theta_{1})^{2}/(1-\phi_{1}^{2})]\]

and, for \(k>0\),

\[\sigma(k) = \sigma^{2}\left[\phi_{1}^{k-1}(\phi_{1}-\theta_{1})+\sum_{s=1}^{ \infty}\phi_{1}^{2s-2+k}(\phi_{1}-\theta_{1})^{2}\right]\] \[= \sigma^{2}\{(\phi_{1}-\theta_{1})[\phi_{1}^{k-1}+\phi_{1}^{k}( \phi_{1}-\theta_{1})\Big{/}(1-\phi_{1}^{2})]\}\] \[= \sigma^{2}(\phi_{1}-\theta_{1})\phi_{1}^{k-1}[1+\phi_{1}(\phi_{1} -\theta_{1})\Big{/}(1-\phi_{1}^{2})]\,.\]

Note that

\[\sigma(k)=\phi_{1}^{k-1}\sigma(1).\qed\]

#### Autoregressive Integrated Moving Average Models:

\(\mathbf{ARIMA}(p,d,q)\)s

The \(ARMA(p,q)\) model and its special cases, the \(AR(p)\) and \(MA(q)\) models, are used to model second-order stationary time series. Using \(ARIMA(p,d,q)\) models is the time domain method for dealing with nonstationarity. Recall that in Example 6.6.2 and Exercise 6.5, the difference operator was used to transform series that were nonstationary into stationary processes. The \(ARIMA(p,d,q)\) model assumes that the \(d\)th difference

\[\nabla^{d}y_{t}\equiv(1-B)^{d}y_{t}\]

is a stationary \(ARMA(p,q)\) process. The \(ARIMA(p,d,q)\) model is written

\[\Phi(B)\nabla^{d}y_{t}=\Theta(B)e_{t}\,.\]

Example 7.2.5: The \(ARIMA(1,1,1)\) model can be rewritten as

\[[1-\phi_{1}(B)](y_{t}-y_{t-1})=e_{t}-\theta_{1}e_{t-1}\]

or

\[(y_{t}-y_{t-1})-\phi_{1}(y_{t-1}-y_{t-2})=e_{t}-\theta_{1}e_{t-1}\]

or

\[y_{t}=(1+\phi_{1})y_{t-1}-\phi_{1}y_{t-2}+e_{t}-\theta_{1}e_{t-1}\,.\]

### Time Domain Prediction

One of the prime motivations in analyzing time series is to be able to predict (forecast) the future of the series. The spectral approximation has limited value as a forecasting tool. The spectral model fits the data perfectly, so the prediction of the future is that the past will reoccur. In particular, if \(n\) is even, the predictions for the next \(n\) observations will be precisely \(y_{1},\ldots,y_{n}\). If measurement error is included, reduced models can be fitted and more interesting predictions result.

These problems do not occur in the time domain. Time domain models are particularly well-suited for making predictions. We wish to examine the best linear predictor of, say, \(y_{n+k}\), based on the observations actually in hand, \(y_{1},\ldots,y_{n}\). For processes that are Gaussian, the best linear predictor is also the best predictor. The BLP methods of Sect. 7.1 continue to apply but some interesting twists arise.

Let \(Y=(y_{1},\ldots,y_{n})^{\prime}\). We begin our discussion of prediction with examples.

Example 7.3.1: _Prediction for an \(AR(1)\) Model._

Because the model

\[y_{t}=\alpha+\phi_{1}y_{t-1}+e_{t}\]is linear and \(\hat{E}(\cdot|Y)\) is a linear operator, we can develop, recursively, a formula for the BLP. For \(k=1,2,\ldots\),

\[\hat{E}(y_{n+k}|Y) = \hat{E}(\alpha+\phi_{1}y_{n+k-1}+e_{n+k}|Y)\] \[= \alpha+\phi_{1}\hat{E}(y_{n+k-1}|Y)+\hat{E}(e_{n+k}|Y)\] \[= \alpha+\phi_{1}\hat{E}(y_{n+k-1}|Y)\,.\]

The last equality holds by Proposition B.1.5 because \(\mathrm{E}(e_{n+k})=0\) and \(\mathrm{Cov}(e_{n+k},Y)=0\). Similarly, for \(k\geq 2\),

\[\hat{E}(y_{n+k-1}|Y)=\alpha+\phi_{1}\hat{E}(y_{n+k-2}|Y),\]

so

\[\hat{E}(y_{n+k}|Y) = \alpha+\phi_{1}[\alpha+\phi_{1}\hat{E}(y_{n+k-2}|Y)]\] \[= \alpha(1+\phi_{1})+\phi_{1}^{2}\hat{E}(y_{n+k-2}|Y)\,.\]

Continuing this procedure gives

\[\hat{E}(y_{n+k}|Y) = \alpha\sum_{s=0}^{k-1}\phi_{1}^{s}+\phi_{1}^{k}\hat{E}(y_{n}|Y)\] \[= \alpha\sum_{s=0}^{k-1}\phi_{1}^{s}+\phi_{1}^{k}y_{n},\]

where the last equality holds by Proposition B.1.2 because we are predicting \(y_{n}\) from a vector of observations that includes \(y_{n}\). 

Example 7.3.2. _Prediction for an MA\((1)\) Model_.

The model is

\[y_{t}=\mu+e_{t}-\theta_{1}e_{t-1}\,.\]

The BLP for \(k=2,3,\ldots\) is

\[\hat{E}(y_{n+k}|Y) = \hat{E}(\mu+e_{n+k}-\theta_{1}e_{n+k-1}|Y) \tag{7.3.1}\] \[= \mu+\hat{E}(e_{n+k}|Y)-\theta_{1}\hat{E}(e_{n+k-1}|Y)\] \[= \mu,\]

where the last equality is a result of

\[\mathrm{E}(e_{n+k})=\mathrm{E}(e_{n+k-1})=0,\]

\[\mathrm{Cov}(e_{n+k},Y)=\mathrm{Cov}(e_{n+k-1},Y)=0,\]

and Proposition B.1.5.

For \(k=1\), \(\text{Cov}(e_{n+k-1},Y)=\text{Cov}(e_{n},Y)\neq 0\), so (1) does not hold. For \(k=1\), note that \(\text{Cov}(y_{n+1},y_{n-s})=\sigma(s+1)=0\), \(s=1,\ldots,n\), so \(\text{Cov}(Y,y_{n+1})\equiv V_{Yy}=(0,0,\ldots,0,\sigma(1))^{\prime}\). The covariance matrix of \(Y\) is of the form (7.1.2) with \(k-1=n\). This can also be written

\[\text{Cov}(Y)=V_{YY}=\begin{bmatrix}V_{22}&\sigma_{1}\\ \sigma_{1}^{\prime}&\sigma(0)\end{bmatrix},\]

where \(V_{22}\) is defined by (7.1.2) with \(k=n\) and \(\sigma_{1}^{\prime}\) is defined by (7.1.1) with \(k=n\). Using Exercise B.4 on inverses of partitioned matrices, we can show that

\[V_{YY}^{-1}V_{Yy}=\begin{bmatrix}-\sigma(1)V_{22}^{-1}\sigma_{1}/[\sigma(0)- \sigma_{1}^{\prime}V_{22}^{-1}\sigma_{1}]\\ \sigma(1)/[\sigma(0)-\sigma_{1}^{\prime}V_{22}^{-1}\sigma_{1}]\end{bmatrix}.\]

Thus,

\[\hat{E}(y_{n+1}|Y)=\mu+[Y-\mu J]^{\prime}V_{YY}^{-1}V_{Yy}, \tag{7.3.2}\]

where \(V_{YY}^{-1}V_{Yy}\) is characterized as before. This is quite complicated and likely to get more so for an \(MA(q)\) with \(q>1\). It is also difficult to compute \(V_{22}^{-1}\) for large \(n\). 

#### Conditioning on \(Y_{\infty}\)

It is my impression that modern software is more likely to bite the bullet and perform the matrix computations necessary for making predictions. But in dealing with moving average processes, it is computationally convenient (although mathematically less precise) to condition not only on \(Y\) but on \(Y_{\infty}=(y_{n},\ldots,y_{1},y_{0},y_{-1},y_{-2},\ldots)^{\prime}\). The use of \(Y_{\infty}\) allows us to develop a simple recursive prediction method similar to that used for the \(AR(1)\) process in Example 7.3.1. For an invertible \(MA(q)\) process

\[[y_{t}-\mu]=\Theta(B)e_{t}\]

with \(\Psi(B)\Theta(B)=1\), we can write

\[\Psi(B)[y_{t}-\mu]=e_{t},\]

where

\[\Psi(B)[y_{t}-\mu]=\sum_{s=0}^{\infty}\psi_{s}[y_{t-s}-\mu]=\sum_{s=0}^{\infty }\psi_{s}y_{t-s}-\mu\sum_{s=0}^{\infty}\psi_{s}\,.\]

Thus, there is an invertible linear transformation between \((y_{t},y_{t-1},\ldots)^{\prime}\) and \((e_{t},e_{t-1},\ldots)^{\prime}\). In other words, there exists a nonsingular transformation between the two vectors for any value of \(t\). Let \(e_{\infty}=(e_{n},e_{n-1},\ldots)^{\prime}\). By Proposition B.1.4, for any random variable \(w\),

\[\hat{E}(w|Y_{\infty})=\hat{E}(w|e_{\infty})\,. \tag{7.3.3}\]Example 7.3.3. _MA_(1) _Prediction Using \(Y_{\infty}\)._

For \(k=2,3,\ldots\), just as before,

\[\hat{E}(y_{n+k}|Y_{\infty})=\mu\.\]

However, for \(k=1\),

\[\hat{E}(y_{n+1}|Y_{\infty}) = \mu+\hat{E}(e_{n+1}|Y_{\infty})-\theta_{1}\hat{E}(e_{n}|Y_{\infty})\] \[= \mu+0-\theta_{1}\hat{E}(e_{n}|Y_{\infty})\] \[= \mu-\theta_{1}e_{n},\]

where the last equality follows from either Corollary B.1.3 or Propositions B.1.4 and B.1.2. \(\Box\)

Comparing the preceeding result with (7.3.2), it becomes clear why predicting with \(Y_{\infty}\) is mathematically more convenient than predicting with \(Y\). In general, (7.3.3) and Proposition B.1.2 imply that

\[\hat{E}(e_{n+k}|Y_{\infty})=\left\{\begin{array}{ll}0&k\geq 1\\ e_{n+k}&k\leq 0\.\end{array}\right. \tag{7.3.4}\]

This characterization is very useful in dealing with moving averages. \(\Box\)

Exercise 7.4. Consider an \(AR(p)\) model with \(p<n\).

1. Show for \(k=0,1,2,\ldots\) that \(\hat{E}(y_{n+k}|Y)\) depends only on \(y_{n},y_{n-1},\ldots,y_{n-p+1}\).
2. Show for \(k=0,1,\ldots\) that \(\hat{E}(y_{n+k}|Y)=\hat{E}(y_{n+k}|Y_{\infty})\).

Hint: For (a), use induction on \(k\).

We now consider prediction for an \(ARMA(p,q)\) model. Predictions for \(ARIMA\) models are found in a similar fashion as will be illustrated later. The forecasting procedure consists of building the predictions recursively. We wish to find \(\hat{E}(y_{n+k}|Y_{\infty})\). The value depends on all of \(p\), \(q\), and \(k\). Moreover, to illustrate the recursive nature, assume that \(p\geq 2\) and \(q\geq 2\). The \(ARMA(p,q)\) model is

\[y_{t}=\alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}+e_{t}-\sum_{s=1}^{q}\theta_{s}e_{t- s},\]

so

\[\hat{E}(y_{n+1}|Y_{\infty}) = \alpha+\sum_{s=1}^{p}\phi_{s}\hat{E}(y_{n+1-s}|Y_{\infty})\] \[+\hat{E}(e_{n+1}|Y_{\infty})-\sum_{s=1}^{q}\theta_{s}\hat{E}(e_{n +1-s}|Y_{\infty})\] \[= \alpha+\sum_{s=0}^{p-1}\phi_{s+1}y_{n-s}+0-\sum_{s=0}^{q-1}\theta _{s}e_{n-s}\,.\]For \(k=2\),

\[\hat{E}(y_{n+2}|Y_{\infty}) = \alpha+\sum_{s=1}^{p}\phi_{s}\hat{E}(y_{n+2-s}|Y_{\infty})\] \[\quad+\hat{E}(e_{n+2}|Y_{\infty})-\sum_{s=1}^{q}\theta_{s}\hat{E}( e_{n+2-s}|Y_{\infty})\] \[= \alpha+\phi_{1}\hat{E}(y_{n+1}|Y_{\infty})\] \[\quad+\sum_{s=0}^{p-2}\phi_{s+2}y_{n-s}+0-0-\sum_{s=0}^{q-2}\theta _{s+2}e_{n-s},\]

where \(\hat{E}(y_{n+1}|Y_{\infty})\) was found earlier.

The general recursive pattern is clear, for \(k=1,2,\ldots\),

\[\hat{E}(y_{n+k}|Y_{\infty})=\alpha+\sum_{s=1}^{p}\phi_{s}\hat{E}(y_{n+k-s}|Y_{ \infty})+0-\sum_{s=1}^{q}\theta_{s}\hat{E}(e_{n+k-s}|Y_{\infty}), \tag{7.3.5}\]

where \(\hat{E}(y_{t}|Y_{\infty})=y_{t}\) if \(t\leq n\) and is found recursively for \(t>n\) and where \(\hat{E}(e_{t}|Y_{\infty})=e_{t}\) for \(t\leq n\) and \(\hat{E}(e_{t}|Y_{\infty})=0\) for \(t>n\).

There are two problems in using this result. First, we do not know the values of \(\alpha\), the \(\phi_{j}\)s, and the \(\theta_{j}\)s. Second, we have not actually observed \(Y_{\infty}\). In particular, because we do not know \(Y_{\infty}\), we also do not know \(e_{n},e_{n-1},\ldots,e_{n-q}\), and these error terms are explicitly involved in \(\hat{E}(y_{n+k}|Y_{\infty})\).

The first problem is generally handled by substituting estimates of \(\alpha\), the \(\phi_{j}\)s, and the \(\theta_{j}\)s for the actual parameters. Parameter estimation is discussed in Sect. 7.5.

One approach to the second problem is to assume that \(e_{t}=0\) for \(t=0,-1,-2,\ldots\). Note that, by the invertability of \(\Phi(\cdot)\), this also implies that \(y_{t}=\mu=\alpha\big{/}\left(1-\sum_{s=1}^{p}\phi_{s}\right)\) for \(t=0,-1,-2,\ldots\). With this assumption, we can simply solve for \(e_{1},\ldots,e_{n}\). The \(ARMA(p,q)\) model can be rearranged as

\[e_{t}=\sum_{s=1}^{q}\theta_{s}e_{t-s}+(y_{t}-\mu)-\sum_{s=1}^{p}\phi_{s}(y_{t-s }-\mu)\,. \tag{7.3.6}\]

For \(t=1\), the right-hand side involves only "observed" random variables, so

\[e_{1} = 0+(y_{1}-\mu)-0\] \[= y_{1}-\mu\] \[= y_{1}-\left\{\alpha\bigg{/}\left(1-\sum_{s=1}^{p}\phi_{s}\right) \right\}\,.\]

Similarly,

\[e_{2}=\theta_{1}e_{1}+(y_{2}-\mu)-\phi_{1}(y_{1}-\mu)\,.\]Equation (7.3.6) can be used recursively to obtain \(e_{3},e_{4},\ldots,e_{n}\). In practice, the parameters \(\alpha\), \(\phi_{s}\), \(s=1,\ldots,p\), and \(\theta_{s}\), \(s=1,\ldots,q\) must be estimated, so estimated errors \(\hat{e}_{t}\), \(t=n-q,\ldots,n\) are used in the BLP, Eq. (7.3.5).

Another, less appealing, approach to dealing with the problem that \(Y_{\infty}\) is not observed is to assume that \(e_{1}=\cdots=e_{q}=0\) and to otherwise ignore the problem. With this assumption about the errors, \(e_{n},\ldots,e_{n-q}\) can be computed using (7.3.6) and then substituted into (7.3.5) to give predictions. If \(n\) is much greater than \(p\) and \(q\), the two approaches should give similar results. Yet another approach to dealing with the "prehistoric" values in \(Y_{\infty}\) is a method called _backcasting_, which is discussed in Sect. 7.5.

Example 7.3.4. _Prediction for an ARMA\((1,1)\)_.

Using Eq. (7.3.5),

\[\hat{E}(y_{n+1}|Y_{\infty}) = \alpha+\phi_{1}y_{n}-\theta_{1}e_{n}\,,\] \[\hat{E}(y_{n+2}|Y_{\infty}) = \alpha+\phi_{1}\hat{E}(y_{n+1}|Y_{\infty})\] \[= \alpha+\phi_{1}[\alpha+\phi_{1}y_{n}-\theta_{1}e_{n}]\,,\] \[\hat{E}(y_{n+3}|Y_{\infty}) = \alpha+\phi_{1}\hat{E}(y_{n+2}|Y_{\infty})\] \[= \alpha+\phi_{1}\{\alpha+\phi_{1}[\alpha+\phi_{1}y_{n}-\theta_{1} e_{n}]\}\] \[= \alpha+\phi_{1}\alpha+\phi_{1}^{2}\alpha+\phi_{1}^{3}y_{n}-\phi_ {1}^{2}\theta_{1}e_{n}\,,\]

and in general, for \(k\geq 1\),

\[\hat{E}(y_{n+k}|Y_{\infty})=\alpha\sum_{s=0}^{k-1}\phi_{1}^{s}+\phi_{1}^{k}y_{ n}-\phi_{1}^{k-1}\theta_{1}e_{n}\,.\]

**Exercise 7.5.**

1. Show that with \(e_{t}=0\) for \(t=0,-1,-2,\ldots\), an \(ARMA(1,1)\) model has \[e_{t}=(y_{t}-\mu)+\sum_{s=1}^{t-1}(\theta_{1}-\phi_{1})\theta_{1}^{t-1-s}(y_{ s}-\mu)\] for \(t=1,\ldots,n\).
2. Show that the assumption \(e_{1}=0\) leads to \[e_{t}=(y_{t}-\mu)+\left[\sum_{s=1}^{t-1}(\theta_{1}-\phi_{1})\theta_{1}^{t-1- s}(y_{s}-\mu)\right]-\theta_{1}^{t-1}(y_{1}-\mu)\,.\]We complete our discussion of \(Y_{\infty}\) prediction in an \(ARMA(p,q)\) model by finding the mean squared error of prediction,

\[\mathrm{E}[y_{n+k}-\hat{E}(y_{n+k}|Y_{\infty})]^{2}\,.\]

Because the BLP is an unbiased estimate of \(y_{n+k}\), this is also called the _prediction variance_.

By definition,

\[\Phi(B)[y_{n+k}-\mu]=\Theta(B)e_{n+k}\,. \tag{7.3.7}\]

Also, (7.3.5) can be restated as

\[\Phi(B)[\hat{E}(y_{n+k}|Y_{\infty})-\mu]=\Theta(B)\hat{E}(e_{n+k}|Y_{\infty})\,. \tag{7.3.8}\]

Subtracting (7.3.8) from (7.3.7) gives

\[\Phi(B)\{y_{n+k}-\hat{E}(y_{n+k}|Y_{\infty})\}=\Theta(B)\{e_{n+k}-\hat{E}(e_{n +k}|Y_{\infty})\}\,. \tag{7.3.9}\]

By assumption, we can write

\[\Psi(B)=\Theta(B)\big{/}\Phi(B)\,.\]

Multiplying (7.3.9) by \(1/\Phi(B)\) gives

\[y_{n+k}-\hat{E}(y_{n+k}|Y_{\infty})=\Psi(B)\{e_{n+k}-\hat{E}(e_{n+k}|Y_{\infty })\}\,. \tag{7.3.10}\]

Recall that

\[\hat{E}(e_{t}|Y_{\infty})=\left\{\begin{array}{ll}e_{t}&t\leq n\\ 0&t>n\end{array}\right.\,,\]

so

\[e_{t}-\hat{E}(e_{t}|Y_{\infty})=\left\{\begin{array}{ll}0&t\leq n\\ e_{t}&t>n\end{array}\right..\]

Substituting into (7.3.10) gives

\[y_{n+k}-\hat{E}(y_{n+k}|Y_{\infty})=\sum_{s=0}^{k-1}\psi_{s}e_{n+k-s}\,. \tag{7.3.11}\]

From (7.3.11), the prediction variance is easily computed:

\[\mathrm{E}[y_{n+k}-\hat{E}(y_{n+k}|Y_{\infty})]^{2} = \mathrm{Var}\left(\sum_{s=0}^{k-1}\psi_{s}e_{n+k-s}\right)\] \[= \sum_{s=0}^{k-1}\psi_{s}^{2}\mathrm{Var}(e_{n+k-s})\] \[= \sigma^{2}\sum_{s=0}^{k-1}\psi_{s}^{2}\,.\]With estimated parameters, this probably underestimates the true prediction error. For a Gaussian process, we get a \((1-\alpha)100\%\) prediction interval for \(y_{n+k}\) with endpoints

\[\hat{E}(y_{n+k}|Y_{\infty})\pm z\Big{(}1-\frac{\alpha}{2}\Big{)}\sqrt{\sigma^{2} \sum_{s=0}^{k-1}\psi_{s}^{2}}\,.\]

**Example 7.3.5**: _Prediction Variance in an ARMA\((1,1)\)._

In (7.2.14), the polynomial transformation \(\Psi(B)\) was given for an _ARMA\((1,1)\)_. Applying this gives

\[\mathrm{E}[y_{n+1}-\hat{E}(y_{n+1}|Y_{\infty})]^{2}=\sigma^{2}\]

and, for \(k\geq 2\),

\[\mathrm{E}[y_{n+k}-\hat{E}(y_{n+k}|Y_{\infty})]^{2}\] \[= \sigma^{2}\left[1+\sum_{s=1}^{k-1}\phi_{1}^{2(s-1)}(\phi_{1}- \theta_{1})^{2}\right]\] \[= \sigma^{2}\left[1+(\phi_{1}-\theta_{1})^{2}\left\{(1-\phi_{1}^{2 (k-1)})/(1-\phi_{1}^{2})\right\}\right]\] \[= \sigma^{2}\left[1+(\phi_{1}-\theta_{1})^{2}(1-\phi_{1}^{2(k-1)}) /(1-\phi_{1}^{2})\right].\]

To predict for an _ARIMA\((p,d,q)\)_ model, let

\[\Phi(B)\nabla^{d}y_{t}=\Theta(B)e_{t}\]

and define

\[z_{t}=\nabla^{d}y_{t},\]

so that \(z_{t}\) is an _ARMA\((p,q)\)_. A new problem in prediction is that \(z_{1},\ldots,z_{d}\) are at most partially observed. We have to assume "prehistoric" (i.e., \(t=0,-1,-2,\ldots\)) values not only for the \(e_{t}\)s but also for the \(y_{t}\)s. With some reasonable assumption about the prehistoric \(y_{t}\)s (e.g., \(y_{t}=\alpha+\beta t\)) and again assuming that \(e_{t}=0\), \(t=0,-1,-2,\ldots\), predictions

\[\hat{E}(z_{n+k}|Z_{\infty})\]

can be made for any \(k>0\). Here, \(Z_{\infty}=(z_{n},z_{n-1},\ldots)^{\prime}\).

To predict \(y_{n+k}\), write

\[\hat{E}(z_{n+k}|Z_{\infty}) = \hat{E}(\nabla^{d}y_{n+k}|Z_{\infty})\] \[= \hat{E}\big{[}(1-B)^{d}y_{n+k}|Z_{\infty}\big{]}\]\[=\hat{E}\left[\sum_{s=0}^{d}{d\choose s}(-1)^{d}y_{n+k-s}\Big{|}Z_{ \infty}\right]\] \[=\sum_{s=0}^{d}{d\choose s}(-1)^{d}\hat{E}(y_{n+k-s}|Z_{\infty})\,. \tag{7.3.12}\]

Using the approximation

\[\hat{E}(y_{t}|Z_{\infty})\doteq y_{t} \tag{7.3.13}\]

for \(t\leq n\), Eq. (7.3.13) can be solved recursively to obtain \(\hat{E}(y_{n+1}|Z_{\infty}),\hat{E}(y_{n+2}|Z_{\infty}),\ldots\). If \(n\) is much greater than \(p\), \(d\), and \(q\), the assumptions about the prehistory should not have much effect on the predictions.

Example 7.3.6. _Prediction for an ARIMA\((p,1,q)\) model_.

From Eq. (7.3.12),

\[\hat{E}(z_{n+k}|Z_{\infty})=\hat{E}(y_{n+k}|Z_{\infty})-\hat{E}(y_{n+k-1}|Z_{ \infty})\]

so

\[\hat{E}(y_{n+k}|Z_{\infty})=\hat{E}(z_{n+k}|Z_{\infty})+\hat{E}(y_{n+k-1}|Z_{ \infty}).\]

In particular,

\[\hat{E}(y_{n+1}|Z_{\infty})=\hat{E}(z_{n+1}|Z_{\infty})+y_{n}.\]

The values \(\hat{E}(z_{n+k}|Z_{\infty})\) are found by applying the \(ARMA(p,q)\) prediction results to the first difference process \(z_{t}\). Values of \(\hat{E}(y_{n+k}|Z_{\infty})\) for \(k>1\) are found in a straightforward recursive manner. 

### Nonlinear Least Squares

The idea of using estimates that minimize the sum of squared errors is a geometric idea, not a statistical idea; it does not depend on the statistical properties of the observations. As was seen in _PA_ Chap. 2, for linear models the least squares estimates are not only intuitively appealing but are also BLUEs, MLEs, and minimum variance unbiased estimates under appropriate assumptions. These other properties of the estimates do depend on the statistical model.

The idea of least squares estimation can be extended to very general nonlinear situations. In particular, the idea can be used to obtain estimates for time domain models. In many such extensions, least squares estimates are also maximum likelihood estimates for normal data. In this section we discuss least squares in general and discuss the Gauss-Newton algorithm for computing the estimates. The section closes with a discussion of nonlinear regression. Although nonlinear regression is not directly applicable to time series analysis, it is important in its own right and illustrates one application of nonlinear least squares methodology. Estimation for time domain models is discussed in Sect. 7.5.

Consider a vector of observations \(v=(v_{1},v_{2},\ldots,v_{n})\) and a parameter \(\xi\in\mathbf{R}^{p}\). For known functions \(f_{i}(\cdot)\), we can write a model

\[v_{i}=f_{i}(\xi)+e_{i}.\]

Here the \(e_{i}\)s are errors. The least squares estimates are values of \(\xi\) that minimize the sum of squared errors

\[\mathrm{SSE}(\xi)=\sum_{i=1}^{n}[v_{i}-f_{i}(\xi)]^{2}\,.\]

In matrix, notation write

\[F(\xi)=\begin{bmatrix}f_{1}(\xi)\\ \vdots\\ f_{n}(\xi)\end{bmatrix}\]

and \(e=(e_{1},\ldots,e_{n})^{\prime}\), so

\[v=F(\xi)+e\,.\]

Note that \(F\) is a function from \(\mathbf{R}^{p}\) to \(\mathbf{R}^{n}\). The sum of squared errors can be rewritten as

\[\mathrm{SSE}(\xi)=[v-F(\xi)]^{\prime}[v-F(\xi)]\,. \tag{7.4.1}\]

The functions \(f_{i}\) can be very general. They can even depend on the \(v_{j}\)s, a situation that occurs in the time domain models discussed in Sect. 7.5. We now illustrate the application of this model to standard univariate linear models.

Example 7.4.1: Consider the linear function of \(\xi\), \(F(\xi)=Z\xi\) for a fixed \(n\times p\) matrix \(Z\) of rank \(p\). The model is \(v=Z\xi+e\) and the criterion to be minimized is

\[[v-Z\xi]^{\prime}[v-Z\xi]\,.\]

The minimizing value gives least squares estimates of \(\xi\) in the linear model. \(\Box\)

#### The Gauss-Newton Algorithm

The _Gauss-Newton algorithm_ is a method for finding least squares estimates in nonlinear problems. The algorithm consists of obtaining a sequence of linear least squares estimates that converge to the least squares estimate in the nonlinear problem.

The Gauss-Newton algorithm requires multivariate calculus. We use the notation set in Appendix A. The procedure also requires an initial guess (estimate) for \(\xi\), say \(\xi_{0}\), and defines a series of estimates \(\xi\), that converge to the least squares estimate \(\hat{\xi}\).

Given \(\xi_{r}\), we define \(\xi_{r+1}\). Using the first-order Taylor's approximation, for \(\xi\) in a neighborhood of \(\xi_{r}\),

\[F(\xi)\doteq F(\xi_{r})+\mathbf{d}_{\xi}F(\xi_{r})(\xi-\xi_{r})\,, \tag{7.4.2}\]where, because \(\xi_{r}\) is known, \(F(\xi_{r})\) and \(\mathbf{d}_{\xi}F(\xi_{r})\) are known. The derivative \(\mathbf{d}_{\xi}F(\xi_{r})\) is the \(n\times p\) matrix of partial derivatives evaluated at \(\xi_{r}\). We assume that \(\mathbf{d}_{\xi}F(\xi_{r})\) has full column rank.

Define

\[\text{SSE}_{r}(\xi)\equiv[v-F(\xi_{r})-\mathbf{d}_{\xi}F(\xi_{r})(\xi-\xi_{r})] ^{\prime}[v-F(\xi_{r})-\mathbf{d}_{\xi}F(\xi_{r})(\xi-\xi_{r})]\,. \tag{7.4.3}\]

Substituting the approximation (7.4.2) into Eq. (7.1.1), we see that \(\text{SSE}_{r}(\xi)\doteq\text{SSE}(\xi)\) when \(\xi\) is near \(\xi_{r}\). If \(\xi_{r}\) is near the least squares estimate \(\hat{\xi}\), the minimum of \(\text{SSE}_{r}(\xi)\) should be close to the minimum of \(\text{SSE}(\xi)\). Now, make the following identifications:

\[Y = v-F(\xi_{r}),\] \[X = \mathbf{d}_{\xi}F(\xi_{r}),\] \[\beta = (\xi-\xi_{r})\,.\]

With these identifications, minimizing \(\text{SSE}_{r}(\xi)\) is equivalent to minimizing

\[[Y-X\beta]^{\prime}[Y-X\beta]\,.\]

From standard linear model theory, this is minimized by

\[\beta_{r+1} = (X^{\prime}X)^{-1}X^{\prime}Y\] \[= ([\mathbf{d}_{\xi}F(\xi_{r})]^{\prime}[\mathbf{d}_{\xi}F(\xi_{r}) ])^{-1}[\mathbf{d}_{\xi}F(\xi_{r})]^{\prime}[v-F(\xi_{r})]\,.\]

Now, \(\beta=\xi-\xi_{r}\), so

\[\xi_{r+1}\equiv\xi_{r}+\beta_{r+1}\]

minimizes \(\text{SSE}_{r}(\xi)\). Although \(\xi_{r+1}\) minimizes \(\text{SSE}_{r}(\xi)\) exactly, \(\xi_{r+1}\) is only an approximation to the value \(\hat{\xi}\) that minimizes \(\text{SSE}(\xi)\). However, as \(\xi_{r}\) converges to \(\hat{\xi}\), the approximation (7.4.2) becomes increasingly better.

**Example 7.4.2**.: Again consider the linear function \(F(\xi)=Z\xi\). From standard linear model theory, we know that \(\hat{\xi}=(Z^{\prime}Z)^{-1}Z^{\prime}v\). Because \(\mathbf{d}_{\xi}F(\xi)=Z\), given any \(\xi_{0}\),

\[\beta_{1} = (Z^{\prime}Z)^{-1}Z^{\prime}(v-Z\xi_{0})\] \[= (Z^{\prime}Z)^{-1}Z^{\prime}v-\xi_{0}\]

and

\[\xi_{1}=\xi_{0}+\beta_{1}=(Z^{\prime}Z)^{-1}Z^{\prime}v=\hat{\xi}.\]

Thus, for a linear least squares problem, the Gauss-Newton method converges to \(\hat{\xi}\) in only one iteration. 

An alternative to the Gauss-Newton method of finding least squares estimates is the method of steepest descent, see Draper and Smith (1981). Marquardt (1963) haspresented a method that is a compromise between Gauss-Newton and the method of steepest descent. Marquardt's compromise is often used to improve the convergence properties of Gauss-Newton.

#### Nonlinear Regression

We present a very short account of the _nonlinear regression model_ and the corresponding least squares estimates. Christensen (2015, Chapter 23) and Draper and Smith (1981, Chapter 10) give more extensive introductions. Seber and Wild (1989, 2003) give a complete account.

Consider a situation in which there are observations \(y_{1},\ldots,y_{n}\) taken on a dependent variable and corresponding observations on independent variables denoted by the row vectors \(x^{\prime}_{1},\ldots,x^{\prime}_{n}\). A linear model is

\[y_{i}=x^{\prime}_{i}\beta+e_{i}\,.\]

Nonlinear regression is the model

\[y_{i}=f(x_{i};\beta)+e_{i}, \tag{7.4.4}\]

where \(f(\cdot;\cdot)\) is a known function. This is a special case of the general nonparametric model introduced in Chap. 1. Write \(Y=(y_{1},\ldots,y_{n})\) and \(e=(e_{1},\ldots,e_{n})\). In the notation of our discussion of the Gauss-Newton algorithm,

\[\begin{array}{rcl}v&=&Y,\\ \xi&=&\beta,\\ f_{i}(\cdot)&=&f(x_{i};\cdot),\end{array}\]

and

\[F(\beta)=\begin{bmatrix}f(x_{1};\beta)\\ \vdots\\ f(x_{n};\beta)\end{bmatrix}.\]

In this problem only one derivative vector needs to be determined, \(\mathbf{d}_{\beta}f(x;\beta)\). However, the vector is evaluated at \(n\) different \(x_{i}\) values. Note that \(x\) need not be a \(p\) vector. We can now apply Gauss-Newton to find least squares estimates.

Nonlinear regression is a problem in which least squares estimates are MLEs under suitable conditions. Assume

\[e\sim N(0,\sigma^{2}I);\]

then, because \(F(\beta)\) is a fixed vector,

\[Y\sim N[F(\beta),\sigma^{2}I]\]with density

\[f(Y|\beta,\sigma^{2})=(2\pi)^{-n/2}\sigma^{-n/2}\exp\left\{-\left[Y-F(\beta) \right]^{\prime}\left[Y-F(\beta)\right]\big{/}2\sigma^{2}\right\}\,.\]

The likelihood is the density taken as a function of \(\beta\) and \(\sigma^{2}\) for fixed \(Y\). The MLEs maximize the likelihood. For any \(\sigma^{2}\), the likelihood is maximized by minimizing \([Y-F(\beta)]^{\prime}[Y-F(\beta)]\). Thus, least squares estimates \(\hat{\beta}\) are also MLEs. The MLE of \(\sigma^{2}\) can be found by maximizing

\[L(\sigma^{2})=(2\pi)^{-n/2}(\sigma^{2})^{-n}\exp\left\{-\left[Y-F(\hat{\beta}) \right]^{\prime}\left[Y-F(\hat{\beta})\right]\big{/}2\sigma^{2}\right\}\,.\]

Differentiation leads to

\[\hat{\sigma}^{2}=[Y-F(\hat{\beta})]^{\prime}[Y-F(\hat{\beta})]/n\,.\]

Asymptotic results for maximum likelihood estimation allow statistical inferences to be made for nonlinear regression models.

Some nonlinear relationships can be transformed into linear relationships. The nonlinear regression model (7.4.4) indicates that

\[y_{i}\doteq f(x_{i};\beta)\,.\]

If \(f(\cdot;\cdot)\) can be written as

\[f(x_{i};\beta)=f(x_{i}^{\prime}\beta)\]

and \(f\) is invertible, the relationship is said to be linearizable. In that case,

\[f^{-1}(y_{i})\doteq x_{i}^{\prime}\beta\,.\]

An alternative to fitting model (4) is to fit

\[f^{-1}(y_{i})=x_{i}^{\prime}\beta+\epsilon_{i}\,. \tag{7.4.5}\]

The choice between analyzing the nonlinear model (7.4.4) and the linear model (7.4.5) is typically based on which model better approximates the assumption of independent identically distributed normal errors. Just as in linear regression, the nonlinear least squares fit of \(\beta\) in model (7.4.4) generates residuals

\[\hat{\epsilon}_{i}=y_{i}-\hat{y}_{i}=y_{i}-f(x_{i};\hat{\beta})\,.\]

These can be plotted to examine normality and heteroscedasticity.

### Estimation

In this section, we discuss empirical estimates of correlations and partial correlations. These are empirical in the sense that they do not depend on any particular time domain model for the second-order stationary time series. We also discuss least squares and maximum likelihood estimates for the parameters of time domain models.

#### Correlations

The empirical estimate of the variance of a stationary process is similar to the sample variance of a simple random sample:

\[s(0)\equiv\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\bar{y}.)^{2}\,.\]

This differs from the unbiased estimate for a random sample in that it uses the multiplier \(1/n\) instead of \(1/(n-1)\). An empirical estimate of \(\sigma(k)=\operatorname{Cov}(y_{t},y_{t+k})\) can be based on the sample covariance of \((y_{1},\ldots,y_{n-k})\) and \((y_{k+1},\ldots,y_{n})\). An estimate of \(\sigma(k)\) is

\[s(k)=\frac{1}{n-k}\sum_{i=1}^{n-k}(y_{i}-\bar{y}.)(y_{i+k}-\bar{y}.)\,.\]

This differs from the estimate for simple random samples in that (a) the multiplier is \(1/(n-k)\) rather than \(1/(n-k-1)\) and (b) \(\bar{y}.\) is used to estimate the mean of both samples rather than using \((y_{1}+\cdots+y_{n-k})/(n-k)\) and \((y_{k+1}+\cdots+y_{n})/(n-k)\), respectively.

Sometimes the alternative estimates

\[\hat{\sigma}(k)=\frac{n-k}{n}s(k)=\frac{1}{n}\sum_{i=1}^{n-k}(y_{i}-\bar{y}.)( y_{i+k}-\bar{y}.)\]

are used. The fact that the \(\hat{\sigma}(k)\)s all use the same multiplier is convenient for some purposes. In particular, it ensures that estimates of covariance matrices such as (7.1.2) are nonnegative definite; see Brockwell and Davis (1991, Section 7.2). Note that \(\hat{\sigma}(0)=s(0)\).

The correlation function \(\rho(k)=\sigma(k)/\sigma(0)\) is estimated with either

\[r(k)=s(k)/s(0)\]

or

\[\hat{\rho}(k)=\hat{\sigma}(k)/\hat{\sigma}(0)\,.\]Partial covariances and partial correlations as defined in Sect. 7.1 are functions of \(\sigma(k)\). Either estimate of \(\sigma(k)\) can be used to define estimated partial covariances and correlations. For example, substituting \(\hat{\sigma}(k)\) into (7.1.1), (7.1.2), and (7.1.3) leads to an estimated partial correlation

\[\hat{\phi}(k)=\hat{\delta}_{k},\]

where \(\hat{\delta}_{k}\) is the first component of \(\hat{\delta}=(\hat{\delta}_{k},\ldots,\hat{\delta}_{1})\) and \(\hat{\delta}\) is defined by replacing \(\sigma(\cdot)\) with \(\hat{\sigma}(\cdot)\) in (7.1.4). An alternative to using matrix operations for finding \(\hat{\phi}(k)\) is to use the results of Exercise 7.9.16.

Box, Jenkins, and Reinsel (1994, p. 323) suggest that estimated covariances are only useful when \(n\geq 50\) and \(k\leq\frac{n}{4}\). Inferences for correlations can be based on asymptotic normality.

#### Conditional Estimation for \(\mathbf{AR}(p)\) Models

Various aspects of estimation in an \(ARMA(p,q)\) model simplify when \(q=0\). This simplification can be quite useful, so we consider the special case first. In particular, three estimation methods are examined: empirical estimation, least squares, and maximum likelihood. The \(AR(p)\) model is

\[y_{t}=\alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}+e_{t},\]

where \(\mathrm{E}(e_{t})=0\), \(\mathrm{Var}(e_{t})=\sigma^{2}\). The parameters to be estimated are \(\alpha,\phi_{1},\phi_{2},\ldots,\phi_{p}\) and \(\sigma^{2}\).

As established in Sect. 7.2,

\[\hat{E}(y_{t}|y_{t-1},\ldots,y_{t-p})=\mu+\sum_{s=1}^{p}\phi_{s}(y_{t-s}-\mu)= \alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}.\]

Thus, modifying (7.2.7), we get estimates \(\hat{\phi}_{1},\ldots,\hat{\phi}_{p}\) that satisfy

\[\hat{\sigma}_{y}(k)=\sum_{s=1}^{p}\hat{\phi}_{s}\hat{\sigma}(s-k)\]

for \(k=1,\ldots,p\). In particular,

\[\left[\begin{array}{c}\hat{\phi}_{1}\\ \vdots\\ \hat{\phi}_{p}\end{array}\right]=\left[\begin{array}{cccc}\hat{\sigma}(0)& \hat{\sigma}(1)&\cdots&\hat{\sigma}(p-1)\\ \hat{\sigma}(1)&\hat{\sigma}(0)&\cdots&\hat{\sigma}(p-2)\\ \vdots&\vdots&&\vdots\\ \hat{\sigma}(p-1)&\hat{\sigma}(p-2)&\cdots&\hat{\sigma}(0)\end{array}\right]^ {-1}\left[\begin{array}{c}\hat{\sigma}(1)\\ \hat{\sigma}(2)\\ \vdots\\ \hat{\sigma}(p)\end{array}\right].\]Note that from our earlier discussion

\[\hat{\phi}(p)=\hat{\phi}_{p}\,.\]

Moreover, taking \(\hat{\mu}=\bar{y}\). gives

\[\hat{\alpha}=\left(1-\sum_{s=1}^{p}\hat{\phi}_{s}\right)\hat{\mu}\,.\]

The second method of estimating \(\alpha,\phi_{1},\ldots,\phi_{p}\) is least squares. The least squares estimates are the values that minimize \(\sum_{t=p+1}^{n}e_{t}^{2}\) or equivalently

\[\sum_{t=p+1}^{n}\left[y_{t}-\alpha-\sum_{s=1}^{p}\phi_{s}y_{t-s}\right]^{2}\,.\]

The sum is from \(t=p+1\) to \(n\) because not all of the \(y_{t-s}\)s are observed for \(t\leq p\). Conditional methods use precisely the cases for which we have complete data.

In applying the Gauss-Newton method to obtain least squares estimates, let \(v=(y_{n},\ldots,y_{p+1})^{\prime}\), \(\xi=(\alpha,\phi_{1},\ldots,\phi_{p})^{\prime}\), \(z_{t}^{\prime}=(1,y_{t-1},\ldots,y_{t-p})\), and for \(t=n,\ldots,p+1\)

\[\alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}=f_{t}(\xi)=z_{t}^{\prime}\xi\,.\]

Note that each \(f_{t}(\xi)\) is a linear function of \(\xi\), so \(F(\xi)\) is a linear function of \(\xi\). In particular,

\[Z=\left[\begin{array}{c}z_{n}^{\prime}\\ \vdots\\ z_{p+1}^{\prime}\end{array}\right]\]

and \(F(\xi)=Z\xi\). Example 7.4.2 applies, giving

\[\left[\begin{array}{c}\hat{\alpha}\\ \hat{\phi}_{1}\\ \vdots\\ \hat{\phi}_{p}\end{array}\right]=\hat{\xi}=(Z^{\prime}Z)^{-1}Z^{\prime}v,\]

which is obtained from fitting the linear model

\[v=Z\xi+e\,,\;\mathrm{E}(e)=0\,,\;\mathrm{Cov}(e)=\sigma^{2}I\,.\]

Thus, conditional least squares estimates for an \(AR(p)\) model are particularly easy to find.

The conditional least squares estimates are also conditional maximum likelihood estimates when the process is Gaussian. The \(AR(p)\) model is a special case of the \(ARMA(p,q)\) model and conditional least squares estimates are conditional MLEs in the more general model. The equivalence will be established later.

Finally, to estimate \(\sigma^{2}\), one can use either

\[\hat{\sigma}^{2}=\frac{1}{n-p}\sum_{t=p+1}^{n}\left[y_{t}-\hat{\alpha}-\sum_{s=1 }^{p}\hat{\phi}_{s}y_{t-s}\right]^{2}\]

or

\[\text{MSE}=\frac{1}{n-2p-1}\sum_{t=p+1}^{n}\left[y_{t}-\hat{\alpha}-\sum_{s=1}^ {p}\hat{\phi}_{s}y_{t-s}\right]^{2}.\]

#### Conditional Least Squares for \(\mathbf{ARMA}(p,q)\)s

We now consider conditional least squares estimates for the model

\[y_{t}=\alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}+e_{t}-\sum_{s=1}^{q}\theta_{s}e_{t- s}.\]

Let \(\xi=(\alpha,\phi_{1},\ldots,\phi_{p},\theta_{1},\ldots,\theta_{q})^{\prime}\). Conditional least squares estimates \(\hat{\xi}=(\hat{\alpha},\hat{\phi}_{1},\ldots,\hat{\phi}_{p},\hat{\theta}_{1}, \ldots,\hat{\theta}_{q})^{\prime}\) minimize \(\sum_{t=p+1}^{n}e_{t}^{2}\) or equivalently

\[\text{SSE}_{C}(\xi)\equiv\sum_{t=p+1}^{n}\left(y_{t}-\left[\alpha+\sum_{s=1}^{p }\phi_{s}y_{t-s}-\sum_{s=1}^{q}\theta_{s}e_{t-s}\right]\right)^{2}.\]

The difficulty in minimizing this is that the \(e_{t}\)s depend on the other parameters.

To apply the Gauss-Newton method, let \(v=(y_{n},\ldots,y_{p+1})^{\prime}\) and

\[f_{t}(\xi)=\alpha+\sum_{s=1}^{p}\phi_{s}y_{t-s}-\sum_{s=1}^{q}\theta_{s}e_{t-s} (\xi)\,.\]

Differentiating,

\[\mathbf{d}_{\xi}f_{t}(\xi)=\left[\mathbf{d}_{\alpha}f_{t}(\xi),\mathbf{d}_{ \phi_{1}}f_{t}(\xi),\ldots,\mathbf{d}_{\phi_{p}}f_{t}(\xi),\mathbf{d}_{\theta_ {1}}f_{t}(\xi),\ldots,\mathbf{d}_{\theta_{q}}f_{t}(\xi)\right],\]

where

\[\mathbf{d}_{\alpha}f_{t}(\xi) = 1,\] \[\mathbf{d}_{\phi_{s}}f_{t}(\xi) = y_{t-s}-\sum_{s=1}^{q}\theta_{s}\mathbf{d}_{\phi_{s}}e_{t-s}( \xi),\]

and\[\mathbf{d}_{\theta_{s}}e_{t}(\xi)=-\sum_{s=1}^{q}\left[e_{t-s}(\xi)+\theta_{s} \mathbf{d}_{\theta_{s}}e_{t-s}(\xi)\right].\]

Given a current estimate \(\xi_{r}=(\alpha^{(r)},\phi_{1}^{(r)},\ldots,\phi_{p}^{(r)},\theta_{1}^{(r)}, \ldots,\theta_{q}^{(r)})^{\prime}\), we need to be able to evaluate \(f_{t}(\xi_{r})\) and \(\mathbf{d}_{\xi}f_{t}(\xi_{r})\) for \(t=n,\ldots,p+1\). In particular, we need to be able to evaluate \(e_{t}(\xi_{r})\), \(\mathbf{d}_{\theta_{s}}e_{t}(\xi_{r})\), and \(\mathbf{d}_{\theta_{s}}e_{t}(\xi_{r})\). To do this, we repeat the assumption made in Sect. 7.3 that

\[e_{t}=0,\qquad t=0,-1,-2,\ldots, \tag{7.5.1}\]

where now we think of these statements as implying that \(e_{t}(\xi)=0\) for all \(\xi\). Thus, we have conditioned on the values of \(e_{t},\ t=0,-1,-2,\ldots\). Because the functions are constant the derivatives are zero; thus,

\[\mathbf{d}_{\alpha}e_{t}(\xi)=\mathbf{d}_{\phi_{t}}e_{t}(\xi)=\mathbf{d}_{ \theta_{s}}e_{t}(\xi)=0\qquad t=0,-1,-2,\ldots. \tag{7.5.2}\]

Recalling that

\[e_{t}(\xi)=\sum_{j=1}^{q}\theta_{j}e_{t-j}(\xi)+y_{t}-\sum_{j=1}^{p}\phi_{j}y_ {t-j}, \tag{7.5.3}\]

we have

\[\mathbf{d}_{\phi_{s}}e_{t}(\xi)=-y_{t-s}+\sum_{j=1}^{q}\theta_{j}\mathbf{d}_{ \phi_{s}}e_{t-j}(\xi) \tag{7.5.4}\]

and

\[\mathbf{d}_{\theta_{s}}e_{t}(\xi)=e_{t-s}(\xi)+\sum_{j=1}^{q}\theta_{j} \mathbf{d}_{\theta_{s}}e_{t-j}(\xi)\,. \tag{7.5.5}\]

Assumptions (7.5.1) and (7.5.2) along with (7.5.3), (7.5.4), and (7.5.5) allow us to compute \(e_{t}(\xi_{r})\) and the necessary partial derivatives. Computation of the \(e_{t}(\xi_{r})\)s assuming (7.5.1) was discussed in Sect. 7.3. Our discussion there assumed that \(\xi=(\alpha,\phi_{1},\ldots,\phi_{p},\theta_{1},\ldots,\theta_{q})^{\prime}\) was known. We simply use those results assuming that \(\xi=\xi_{r}\).

Computation of the partial derivatives is a bit more involved. We illustrate the method for \(\mathbf{d}_{\theta_{s}}e_{t}(\xi)\) with \(q=2\). Assume that for \(t=1,\ldots,n\), \(e_{t}=e_{t}(\xi_{r})\) has already been computed. We simplify notation by writing \(\theta_{s}^{(r)}=\theta_{s}\) and \(\mathbf{d}_{\theta_{s}}e_{t}(\xi)\) as \(\mathbf{d}_{\theta_{s}}e_{t}\). The method makes repeated use of (7.5.1), (7.5.2), and (7.5.5).

For \(t=1\),

\[\mathbf{d}_{\theta_{1}}e_{1} = e_{0}+\theta_{1}\mathbf{d}_{\theta_{1}}e_{0}+\theta_{2}\mathbf{d }_{\theta_{1}}e_{-1}\] \[= 0+\theta_{1}0+\theta_{2}0\] \[= 0,\]

\[\mathbf{d}_{\theta_{2}}e_{1} = e_{-1}+\theta_{1}\mathbf{d}_{\theta_{2}}e_{0}+\theta_{2}\mathbf{d }_{\theta_{2}}e_{-1}\] \[= 0+\theta_{1}0+\theta_{2}0\] \[= 0\,.\]For \(t=2\),

\[{\bf d}_{\theta_{1}}e_{2} = e_{1}+\theta_{1}{\bf d}_{\theta_{1}}e_{1}+\theta_{2}{\bf d}_{\theta_ {1}}e_{0}\] \[= e_{1}+\theta_{1}0+\theta_{2}0\] \[= e_{1},\]

\[{\bf d}_{\theta_{2}}e_{2} = e_{0}+\theta_{1}{\bf d}_{\theta_{2}}e_{1}+\theta_{2}{\bf d}_{ \theta_{2}}e_{0}\] \[= 0+\theta_{1}0+\theta_{2}0\] \[= 0\,.\]

For \(t=3\),

\[{\bf d}_{\theta_{1}}e_{3} = e_{2}+\theta_{1}{\bf d}_{\theta_{1}}e_{2}+\theta_{2}{\bf d}_{ \theta_{1}}e_{1}\] \[= e_{2}+\theta_{1}e_{1},\]

\[{\bf d}_{\theta_{2}}e_{3} = e_{1}+\theta_{1}{\bf d}_{\theta_{2}}e_{2}+\theta_{2}{\bf d}_{ \theta_{2}}e_{1}\] \[= e_{1}+\theta_{1}0+\theta_{2}0\] \[= e_{1}\,.\]

For \(t=4\),

\[{\bf d}_{\theta_{1}}e_{4} = e_{3}+\theta_{1}{\bf d}_{\theta_{1}}e_{3}+\theta_{2}{\bf d}_{ \theta_{1}}e_{2}\] \[= e_{3}+\theta_{1}[e_{2}+\theta_{1}e_{1}]+e_{1}\] \[= e_{3}+\theta_{1}e_{2}+(1+\theta_{1}^{2})e_{1},\]

\[{\bf d}_{\theta_{2}}e_{4} = e_{2}+\theta_{1}{\bf d}_{\theta_{2}}e_{3}+\theta_{2}{\bf d}_{ \theta_{2}}e_{2}\] \[= e_{2}+\theta_{1}e_{1}+\theta_{2}0\] \[= e_{2}+\theta_{1}e_{1}\,.\]

This procedure goes on recursively, thus allowing computation of \({\bf d}_{\theta_{j}}f_{t}(\xi_{r})\), which is necessary to execute the Gauss-Newton algorithm.

#### Conditional MLEs for \({\bf ARMA}(p,q)s\)

It remains to establish the equivalence of conditional least squares and conditional maximum likelihood for Gaussian processes. To do this, we need to find the joint distribution of \(y_{n},\ldots,y_{p+1}\). The joint distribution is conditional on the unknown parameters \(\xi\). We also condition on \(e_{p},e_{p-1},\ldots\) or equivalently \(y_{p},y_{p-1},\ldots\) Writing the stationary invertible \(ARMA(p,q)\) process as \[\Phi(B)[y_{t}-\mu]=\Theta(B)e_{t}\]

and letting

\[\Psi(B)=\Theta(B)/\Phi(B)\]

we have

\[[y_{t}-\mu]=\Psi(B)e_{t}\,. \tag{7.5.6}\]

From (7.5.6), with \(e_{p},e_{p-1},\ldots\) fixed, each \(y_{t}-\mu,t>p\) is a linear function of the random variables \(e_{t},\ldots,e_{p+1}\). In particular, write

\[\left(\begin{array}{c}y_{n}\\ \vdots\\ y_{p+1}\end{array}\right)=A\left(\begin{array}{c}e_{n}\\ \vdots\\ e_{p+1}\end{array}\right)+\eta.\]

The fixed vector \(\eta\) is \(\eta=(\eta_{n},\ldots,\eta_{p+1})^{\prime}\) with \(\eta_{t}=\mu+\sum_{s=0}^{\infty}\psi_{t-p+s}e_{p-s}\). Moreover,

\[A=\left[\begin{array}{c}a^{\prime}_{n}\\ \vdots\\ a^{\prime}_{p+1}\end{array}\right]\]

and \(a^{\prime}_{t}=(0,\ldots,0,\psi_{0},\psi_{1},\ldots,\psi_{t-p-1})\) with \(\psi_{0}=1\). (\(\psi_{0}=1\) because the first coefficients in both \(\Phi(B)\) and \(\Theta(B)\) are 1.) Observe that \(A\) is a nonsingular, upper triangular matrix with 1s down the diagonal. The distribution of \(y=(y_{n},\ldots,y_{p+1})^{\prime}\) is

\[y\sim N(\eta,\sigma^{2}AA^{\prime}),\]

and the density is

\[f(y) = (2\pi)^{-\left(\frac{n-p}{2}\right)}|\sigma^{2}AA^{\prime}|^{- \frac{1}{2}}\exp[-(y-\eta)^{\prime}(AA^{\prime})^{-1}(y-\eta)/2\sigma^{2}]\] \[= (2\pi)^{-\left(\frac{n-p}{2}\right)}(\sigma^{2})^{-\frac{n-p}{2} }|A|^{-1}\] \[\times\exp[-\{A^{-1}(y-\eta)\}^{\prime}\{A^{-1}(y-\eta)\}/2\sigma ^{2}]\] \[= (2\pi)^{-\left(\frac{n-p}{2}\right)}(\sigma^{2})^{-\frac{n-p}{2} }\exp[-\{A^{-1}(y-\eta)\}^{\prime}\{A^{-1}(y-\eta)\}/2\sigma^{2}],\]

where the last equality follows from the fact that \(A\) is upper triangular with 1s on the diagonal, so \(|A|=1\).

The problem with (7.5.7) is that it is a complicated function of our original parameters: \(\xi\) and \(\sigma^{2}\). Both \(A\) and \(\eta\) depend on the coefficients of \(\Psi(B)\). Note, however, that

\[A^{-1}(y-\eta)=(e_{n},\ldots,e_{p+1})^{\prime},\]

where each \(e_{t}\) is a function of both \(y\) and the parameters \(\xi\). Writing the \(ARMA(p,q)\) model as \[e_{t}(y,\xi)=\sum_{s=1}^{q}\theta_{s}e_{t-s}(y,\xi)+y_{t}-\sum_{s=1}^{p}\theta_{s}y_ {t-s}-\alpha,\]

we see that

\[\begin{array}{l}\{A^{-1}(y-\eta)\}^{\prime}\{A^{-1}(y-\eta)\}\\ =\sum_{t=p+1}^{n}\left[y_{t}-\alpha-\sum_{s=1}^{p}\phi_{s}y_{t-s}+\sum_{s=1}^{ q}\theta_{s}e_{t-s}(y,\xi)\right]^{2}\\ ={\rm SSE}_{C}(\xi),\end{array} \tag{7.5.8}\]

and the density can be rewritten as

\[f(y)=(2\pi)^{-\frac{(n-p)}{2}}(\sigma^{2})^{-\frac{(n-p)}{2}}\exp\left\{-\frac {1}{2\sigma^{2}}{\rm SSE}_{C}(\xi)\right\}.\]

Thinking of this as a function of \(\sigma^{2}\) and \(\xi\) with \(y\) fixed, we have the likelihood

\[L(\xi,\sigma^{2})=(2\pi)^{-\frac{(n-p)}{2}}(\sigma^{2})^{-\frac{(n-p)}{2}}\exp \left\{-\frac{1}{2\sigma^{2}}{\rm SSE}_{C}(\xi)\right\}. \tag{7.5.9}\]

Clearly, for any \(\sigma^{2}\), the value of \(\xi\) that minimizes (7.5.8) will maximize the likelihood. However, by definition, the minimizing value of (7.5.8) is given by the least squares estimates. Given the least squares estimate \(\hat{\xi}\), the MLE of \(\sigma^{2}\) can then be found by solving \({\bf d}_{\sigma^{2}}L(\hat{\xi},\sigma^{2})=0\) to get,

\[\hat{\sigma}^{2}=\frac{1}{n-p}\sum_{t=p+1}^{n}\left[y_{t}-\hat{\alpha}-\sum_{s= 1}^{p}\hat{\phi}_{s}y_{t-s}+\sum_{s=1}^{q}\hat{\theta}_{s}e_{t-s}(y,\hat{\xi}) \right]^{2}. \tag{7.5.10}\]

#### Unconditional Estimation for \({\rm ARMA}(p,q)\) Models

We begin with maximum likelihood estimation. This discussion will lead naturally into least squares estimation. For unconditional MLEs, we need the density of \(Y=(y_{n},\cdots,y_{1})^{\prime}\). Again consider the equality (7.5.6); however, we now redefine \(A\) as an \(n\times\infty\) matrix

\[A=\begin{bmatrix}a^{\prime}_{n}\\ \vdots\\ a^{\prime}_{1}\end{bmatrix},\]

where

\[a^{\prime}_{t}=(0,\cdots,0,\psi_{0},\psi_{1},\psi_{2},\cdots),\]

with the first \(n-t\) columns equal to \(0\) and \(\psi_{0}=1\). Writing the infinite vector \[e=(e_{n},e_{n-1},e_{n-2},\cdots)^{\prime},\]

Eq. (7.5.6) becomes

\[Y=Ae+\mu J, \tag{7.5.11}\]

where \(J\) is an \(n\) vector of 1s. Recalling that the \(e_{t}\)s are i.i.d. \(N(0,\sigma^{2})\), we see that

\[Y\sim N(\mu J,\sigma^{2}AA^{\prime}),\]

so the unconditional likelihood function is

\[f(Y|\xi,\sigma^{2}) = (2\pi)^{-\frac{n}{2}}(\sigma^{2})^{-\frac{n}{2}}|AA^{\prime}|^{- \frac{1}{2}} \tag{7.5.12}\] \[\times\exp\left[-(Y-\mu J)^{\prime}(AA^{\prime})^{-1}(Y-\mu J) \Big{/}2\sigma^{2}\right].\]

It is important to note that both \(\mu\) and \(A\) depend on the parameters \(\xi\). When convenient, we write \(A(\xi)\) for \(A\). Equation (7.2.12) can be written in a somewhat simpler form. Note that

\[\hat{E}(e|Y,\xi)\equiv\hat{E}(e|Y)=A^{\prime}(AA^{\prime})^{-1}(Y-\mu J).\]

Clearly,

\[(Y-\mu J)^{\prime}(AA^{\prime})^{-1}(Y-\mu J) = \{\hat{E}(e|Y,\xi)\}^{\prime}\{\hat{E}(e|Y,\xi)\}\] \[= \sum_{t=-\infty}^{n}\{\hat{E}(e_{t}|Y,\xi)\}^{2},\]

so substituting into (7.5.12) gives the likelihood

\[L(\xi,\sigma^{2})=(2\pi)^{-\frac{n}{2}}(\sigma^{2})^{-\frac{n}{2 }}|A(\xi)A^{\prime}(\xi)|^{-\frac{1}{2}} \tag{7.5.13}\] \[\times\exp\left\{-\hat{E}(e|Y,\xi)^{\prime}\hat{E}(e|Y,\xi)\Big{/} 2\sigma^{2}\right\}.\]

Maximum likelihood estimates maximize the function (7.5.13). Least squares estimates minimize

\[{\rm SSE}(\xi) \equiv (Y-\mu J)^{\prime}(AA^{\prime})^{-1}(Y-\mu J)\] \[= \hat{E}(e|Y,\xi)^{\prime}\hat{E}(e|Y,\xi)\] \[= \sum_{t=-\infty}^{n}\{\hat{E}(e_{t}|Y,\xi)\}^{2}.\]

In unconditional estimation, the least squares estimates need not equal the maximum likelihood estimates. However, for moderate to large sample sizes \(n\) and parameter values that are not near their boundaries, the least squares estimate of \(\xi\) gives a very good approximation to the MLE; see Box et al. (1994, Section 7.1.4) and McLeod (1977). This phenomenon occurs because the determinant of the covariance matrix usually plays a very minor role in determining the maximum of the likelihood.

To actually find estimates, an iterative procedure is required. The Gauss-Newton method can be used to find least squares estimates. Because the determinant \(|A(\xi)A^{\prime}(\xi)|\) depends on \(\xi\), some other method (e.g., Newton-Raphson) must be used to obtain MLEs. (Newton-Raphson was discussed in Christensen (1997, Section 12.4).) In addition, the values \(\hat{E}(e_{t}|Y,\xi)\) need to be evaluated. As in Sect. 7.3, we could use

\[\hat{E}(e_{t}|Y,\xi)\doteq\hat{E}(e_{t}|Y_{\infty},\xi),\]

where

\[\hat{E}(e_{t}|Y_{\infty},\xi)=\cases{0&$t>n$\cr e_{t}&$t\leq n$\cr}, \tag{7.5.14}\]

along with the assumption that \(e_{t}=0,\ t\leq 0\). This assumption implies that \(y_{t}=\mu,\ t\leq 0\), and thus we can compute \(e_{t}\) for \(t=1,\ldots,n\) using

\[e_{t}=\sum_{s=1}^{q}\theta_{s}e_{t-s}+y_{t}-\sum_{s=1}^{p}\phi_{s}y_{t-s}- \alpha.\]

These very strong assumptions can be improved upon in practice by incorporating Box and Jenkins' method of _back forecasting (backcasting)_. Backcasting is used to obtain values for \(\hat{E}(e_{t}|Y,\xi)\). It is based on the observation that if \(w_{t}\) and \(e_{t}\) are both white noise processes, the mean and covariance function of a stationary process defined by

\[\Phi(B)(y_{t}-\mu)=\Theta(B)e_{t}\]

must be the same as the mean and covariance function of

\[\Phi(F)(y_{t}-\mu)=\Theta(F)w_{t}, \tag{7.5.15}\]

where \(F\) is the _forward_ shift operator (i.e., \(F(w_{t})=w_{t+1}\)). Thus, model (7.5.15) is every bit as good a model for the data as the standard \(ARMA(p,q)\) model. Now, rather than assuming \(e_{t}=0,\ t=0,-1,\ldots\) we assume \(w_{t}=0\) for \(t=n+1,n+2,\ldots\). Rather than using (7.5.14), we use

\[\hat{E}(w_{t}|Y^{\infty},\xi)=\cases{w_{t}&$t\geq 1$\cr 0&$t<n$\cr}, \tag{7.5.16}\]

where \(Y^{\infty}=(y_{1},\cdots,y_{n},\mu,\mu,\mu,\cdots)^{\prime}\). Note that the assumption \(w_{t}=0\) for \(t\geq n+1\) implies that \(y_{t}=\mu\) for \(t\geq n+1\). Rewriting (7.5.15) as

\[w_{t}=\sum_{s=1}^{q}\theta_{s}w_{t+s}+y_{t}-\sum_{s=1}^{p}\phi_{s}y_{t+s}-\alpha, \tag{7.5.17}\]

we can compute \(w_{n},w_{n-1},\cdots,w_{1},\cdots\) recursively. This in turn allows us to backcast values \(\hat{E}(y_{t}|Y^{\infty},\xi)\) for \(t<1\), namely

\[\hat{E}(y_{t}|Y^{\infty},\xi)=\alpha+\sum_{s=1}^{p}\phi_{s}\hat{E}(y_{t+s}|Y^{ \infty},\xi)\]\[+\hat{E}(w_{t}|Y^{\infty},\xi)-\sum_{s=1}^{q}\theta_{s}\hat{E}(w_{t+s}|Y^{\infty}, \xi),\]

where the terms \(\hat{E}(w_{t}|Y^{\infty},\xi)\) are computed using (7.5.16) and (7.5.17) and the values \(\hat{E}(y_{t}|Y^{\infty},\xi)\) are computed recursively for \(t<1\) and for \(t\geq 1\), \(\hat{E}(y_{t}|Y^{\infty},\xi)=y_{t}\).

The backcasting values \(\hat{E}(y_{t}|Y^{\infty},\xi),t<1\) can be used in the standard _ARMA_ model to improve on the assumption \(e_{t}=0\), \(t<1\) and its consequence \(y_{t}=\mu\), \(t<1\). Instead of these assumptions, we choose a large value \(Q\) and assume that \(\hat{E}(y_{t}|Y,\xi)=\hat{E}(y_{t}|Y^{\infty},\xi)\) for \(t=0,1,\ldots,1-Q\) and that \(e_{t}=0\) for \(t<1-Q\). As before, the assumption on \(e_{t}\) implies that \(y_{t}=\mu\) for \(t<1-Q\).

The functions \(L(\xi,\sigma^{2})\) and \(\mathrm{SSE}(\xi)\) are now evaluated using

\[\hat{E}(e_{t}|Y,\xi)=\sum_{s=1}^{q}\theta_{s}\hat{E}(e_{t-s}|Y,\xi)+\hat{E}(y_{ t}|Y,\xi)-\sum_{s=1}^{p}\phi_{s}\hat{E}(y_{t-s}|Y,\xi)-\alpha,\]

where

\[\hat{E}(y_{t}|Y,\xi)\doteq\cases{y_{t}&$t=1,\ldots,n$\cr\hat{E}(y_{t}|Y^{\infty },\xi)&$t=1-Q,\ldots,0$\cr\mu&$t\leq-Q$\cr},\]

\[\hat{E}(e_{t}|Y,\xi)\doteq 0\qquad t\leq-Q,\]

and \(\hat{E}(e_{t}|Y,\xi)\) is computed recursively for \(t>-Q\).

One problem is that a specific choice of \(Q\) must be made. Without getting into details, we mention a basic consideration in the selection of \(Q\). For a stationary _ARMA_ process, the covariance function is given in (7.2.12) as \(\sigma(k)=\sigma^{2}\sum_{s=0}^{\infty}\psi_{s}\psi_{s+k}\). Because \(\sum_{s=0}^{\infty}\psi_{s}\) is finite, \(\lim_{s\to\infty}\psi_{s}=0\). It is intuitively obvious (and not hard to show) that if the correlation between \(y_{n+k}\) and the observed data is approaching zero, then \(\hat{E}(y_{n+k}|Y)\) must approach \(\mu\), the mean of the stationary process. If the correlation between the future and the present is zero, there is no basis for predicting the future as anything other than the mean of the process.

The same phenomenon occurs in back forecasting. For a stationary process, the back forecasts should settle down around \(\hat{\mu}\) for times \(t\) that are large and negative. Moreover, when the backcasts have approached \(\mu\), the predictions of \(e_{t}\) must be near zero. For a given \(Q\), by checking whether these phenomena occur for \(t\) near \(-Q\), one can decide whether \(Q\) is sufficiently large.

Brockwell and Davis (1991, Chapter 8) present methods for unconditional maximum likelihood and least squares estimation based on the innovations algorithm; see Exercises 7.2 and 7.6. These use the exact value of \(\hat{E}(e_{t}|Y,\xi)\) rather than the approximation \(\hat{E}(e_{t}|Y_{\infty},\xi)\). Brockwell and Davis also present asymptotic distributional and inferential results.

**Exercise 7.6.** Let \(e(y_{t}|y_{t-1},\cdots,y_{1};\xi)=y_{t}-\hat{E}(y_{t}|y_{t-1},\cdots,y_{1},\xi)\) from an _ARMA_\((p,q)\) model with parameters \((\xi,\sigma^{2})\), and let \(p_{t}(\xi,\sigma^{2})\) be the corresponding prediction variance. Use the results of Exercise 7.2 to show that \[L(\xi,\sigma^{2})=(2\pi)^{-\frac{n}{2}}\left[\prod_{t=1}^{n}p_{t}( \xi,\sigma^{2})\right]^{-\frac{1}{2}}\\ \times\exp\left\{-y_{1}^{2}/2\sigma(0)^{2}-\sum_{t=1}^{n}\left[e(y _{t}|y_{t-1},\cdots,y_{1};\xi)\right]^{2}\Big{/}2p_{t}(\xi,\sigma^{2})\right\}.\]

Hint: Show that there is a nonsingular linear transformation between the data vector \(Y\) and the vector of sequential prediction errors.

#### Estimation for ARIMA\((p,d,q)\) Models

As in Sect. 7.3, _ARIMA\((p,d,q)\)_ models are analyzed by considering the corresponding \(ARMA(p,q)\) model for

\[z_{t}=\nabla^{d}y_{t}\,.\]

### Model Selection

Box et al. (1994) suggest performing model selection by examination of the empirical correlations and partial correlations. Alternatively, general model selection criteria such as the Akaike Information Criterion (AIC) and Schwarz's asymptotic Bayesian modification of the AIC can be applied to time domain models; see Akaike (1973) and Schwarz (1978).

#### Box-Jenkins

The Box-Jenkins approach is based on two facts. First, for an \(AR(p)\) process, \(\phi(k)=0\) for \(k>p\). Second, for a \(MA(q)\) process, \(\rho(k)=0\) for \(k>q\). Empirical estimates \(\hat{\phi}(k)\) and \(\hat{\rho}(k)\) are obtained as in the previous section.

Recall that the stationary invertible \(ARMA(p,q)\) model

\[\Phi(B)[y_{t}-\mu]=\Theta(B)e_{t}\]

can be written as an infinite moving average

\[y_{t}-\mu=\frac{\Theta(B)}{\Phi(B)}e_{t}\]

and also as an infinite autoregressive process

\[\frac{\Phi(B)}{\Theta(B)}[y_{t}-\mu]=e_{t}\,.\]Moreover, the infinite sum \(\Theta(1)/\Phi(1)=1/\left[\Phi(1)/\Theta(1)\right]\) is a finite number, so the terms in each sequence converge to zero fairly quickly.

If \(q=0\), \(\hat{\rho}(k)\) should gradually approach zero while \(\hat{\phi}(k)\) drops off precipitously after \(k=p\). Similarly if \(p=0\), \(\hat{\phi}(k)\) should gradually approach zero while \(\hat{\rho}(k)\) drops off after \(k=p\). If both \(p\) and \(q\) are positive, then the first \(p\) terms of \(\Phi(B)/\Theta(B)\) will tend to dominate, so \(\hat{\phi}(k)\) should drop off substantially after \(k=p\). Similarly, the first \(q\) terms of \([\Phi(B)]^{-1}\Theta(B)\) should dominate, leading \(\hat{\rho}(k)\) to drop off substantially after \(k=q\).

For example, if \(|\hat{\phi}(k)|\) drops precipitously to near zero after \(k=3\), and if \(|\hat{\rho}(k)|\) decreases gradually to zero, an \(AR(3)\) model is suggested. If \(|\hat{\rho}(k)|\) drops quickly after \(k=2\) and \(|\hat{\phi}(k)|\) decreases gradually, a \(MA(2)\) model is suggested. If \(|\hat{\phi}(k)|\) drops after \(k=3\) and \(|\hat{\rho}(k)|\) drops after \(k=2\), an \(ARMA(3,2)\) model is suggested.

The standard errors of \(\hat{\rho}(k)\) and \(\hat{\phi}(k)\) can be used to help decide which correlations are important. For white noise (i.e., \(\rho(k)=0\) for all \(k\geq 1\)), the large sample standard deviation of \(\hat{\rho}(k)\) is estimated by

\[\mathrm{SE}(\hat{\rho}(k))=\frac{1}{\sqrt{n}}\left[1+2\sum_{j=1}^{k-1}\hat{ \rho}(j)^{2}\right]^{1/2};\]

see Bartlett (1946). Sometimes people take \(\mathrm{SE}(\hat{\rho}(k))=1/\sqrt{n}\) which comes from substituting \(\rho(j)=0\) for \(\hat{\rho}(j)\). Similarly, Quenille (1949) has shown that for large samples from an \(AR(p)\) the standard error of \(\hat{\phi}(k)\) for \(k>p\) is

\[\mathrm{SE}(\hat{\phi}(k))\doteq 1/\sqrt{n}.\]

To identify important correlations, it is useful to plot \((k,\hat{\rho}(k))\) and \((k,\hat{\phi}(k))\). Values with absolute values greater than, say, twice the standard error are likely to be nonzero.

Nonstationarity can sometimes be identified by graphing the time series. A stationary process should display constant variability about its mean value \(\mu\). If the variability in the plot seems to increase with time, a log transformation may alleviate the problem. If a trend appears in the plot, differencing (i.e., considering \(\nabla^{d}y_{t}\)) would be in order. Once \(d\) has been determined so that the plot looks stationary, \(p\) and \(q\) can be identified as earlier, yielding an \(ARIMA(p,d,q)\) model.

As a supplement to visual inspection of the graph, there are several statistics that give suggestions of nonstationarity. First, a value of \(|\hat{\phi}(1)|\) near 1 suggests possible nonstationarity. Also, \(\hat{\rho}(k)\) approaching zero very slowly suggests nonstationarity. Finally, nonstationarity is suggested in the frequency domain by the existence of one or more small frequencies that are very important. In practice, the differencing parameter \(d\) is rarely taken to be greater than 2.

Having identified a time domain model, we can attempt to check its appropriateness. In any time domain model, \(e_{t}\) is a white noise process. Assume the most general model, an \(ARIMA(p,d,q)\). The \(e_{t}\) process can be estimated by \[\hat{e}_{t}=\sum_{s=1}^{q}\hat{\theta}_{s}\hat{e}_{t-s}+\nabla^{d}(y_{t}-\hat{\mu} )-\sum_{s=1}^{p}\hat{\phi}_{s}\nabla^{d}(y_{t-s}-\hat{\mu}),\]

where it is assumed that \(\hat{e}_{t}=0\), \(t=0,-1,-2,\ldots\). Brockwell and Davis (1991, Section 9.4) suggest the use of standardized residuals based only on previous data,

\[r_{t}=\left[y_{t}-\hat{E}(y_{t}|y_{t-1},\cdots,y_{1};\hat{\xi})\right]\left/ \sqrt{p_{t}(\hat{\xi},\hat{\sigma}^{2})},\right.\]

where \(p_{t}(\xi,\sigma^{2})\) is the prediction variance as found in Sect. 7.3. These residuals arise naturally when using the innovations algorithm to obtain unconditional maximum likelihood estimates (see Exercise 7.6) and appear to be an improvement over the unstandardized residuals \(\hat{e}_{t}\).

The residuals can be plotted against time to detect evidence of nonstationarity. For example, the residuals may display a trend or increasing variability.

The correlation and partial correlation functions for white noise are

\[\begin{array}{c}\rho(0)\equiv\phi(0)=1,\\ \rho(k)=\phi(k)=0\qquad k=1,2,\ldots\.\end{array}\]

The estimated correlations and partial correlations for the residual process should be similar if the model is correct.

The spectral density of white noise is \(f(v)=\sigma^{2}\). If the model is correct, the residual process should not have any important frequencies.

Finally, to check whether the white noise is Gaussian, a normal plot of the residuals can be used; see _PA-V_ Sect. 12.2 (Christensen 2011, Section 13.2).

#### Testing Model Fit

The _Ljung-Box_ modification of the _Box-Pierce_ portmanteau test is used to evaluate whether the estimated autocorrelation function is consistent with the process being white noise. When evaluating the fit of an \(ARMA(p,q)\) model, the autocorrelation function should be computed from the estimated errors (residuals). Specifically, compare the test statistic

\[Q(h)\equiv n(n+2)\sum_{k=1}^{h}\frac{\beta(k)^{2}}{n-k}\]

to a \(\chi^{2}(h-df)\) distribution where, excluding \(\sigma^{2}\), \(df=p+q+1\) is the number of parameters in an \(ARMA(p,q)\) model with nonzero mean. Frequently, this test is performed for several values of \(h\). Minitab uses \(h=12,24,36,48\). At [https://robjhyndman.com/hyndsight/ljung-box-test/](https://robjhyndman.com/hyndsight/ljung-box-test/) (Last accessed on 13 August 2019) Rob Hyndman recommended \(h=\min(10,n/5)\) and that you should not be too fussy about choosing \(h\).

#### Model Selection Criteria

If one has identified several candidate models, a formal model selection criterion can be used to identify the best of these. The criteria to be considered require a maximum likelihood fit of the model, so computing the criteria for large numbers of models is expensive. (Using approximate fits for large numbers of models may be a reasonable practical procedure.)

The AIC and Schwarz's large sample Bayesian modification, the BIC, are discussed in Clayton, Geisser, and Jennings (1985). As applied to Gaussian ARMA models that include a mean and an unknown error variance, so that the number of fitted parameters is \(k=p+q+2\), and with a log-likelihood denoted \(\ell(\xi,\sigma^{2})\), these criteria can be estimated by

\[\text{AIC}=-2\ell(\hat{\xi},\hat{\sigma}^{2})+2k\]

and

\[\text{BIC}=-2\ell(\hat{\xi},\hat{\sigma}^{2})+k\log(n)\]

where the estimates are MLEs.

The model that minimizes the criterion is the best fitting model. In practice, one should use these criteria to identify a small group of best models. For each of these models, the residual process should be evaluated to determine whether a Gaussian white noise assumption appears reasonable. For multivariate autoregressions, a simulation by Lutkepohl (1985) suggests that the BIC outperforms the AIC and many other criteria in terms of correct model identification and minimization of prediction errors. Clayton et al. (1985) examine simulations for data that are not time series. Hurvich and Tsai (1989) discuss correcting the AIC for bias which leads to

\[\text{AICc}=AIC+2k(k+1)/[n-k-1]\]

_PA-V_ gives a brief discussion of these measures for linear models and some additional references.

The AIC is not asymptotically consistent in the sense that a consistent criterion asymptotically chooses the correct model. The BIC criterion is a modification of the AIC specifically developed to achieve consistency. It is not surprising that the BIC outperforms criteria that are not consistent for large samples when the true model is one of the models being considered. In practice, however, our models are only approximations to reality. The appropriate question is not "Which criterion selects the correct model most often?" but "Which criterion selects the best approximation most often?"

#### An Example

Example 7.6.1. Consider again the coal production data from Example 6.0.1. The data are given in Table 6.1 and are displayed in Fig. 6.1. Figure 7.1 gives theestimated correlation and partial correlation functions. Note that the correlation plot from R's acf function includes \(\hat{\rho}(0)\equiv 1\) but the partial correlation (pacf) plot does not include \(\hat{\phi}(0)\). (The forecast library's Acf and Pacf functions seem to be consistent.) Standard errors for the correlations can be based on Bartlett and Quennille's asymptotic formulae; the resulting 95% rejection regions for testing that the correlations are zero are displayed as dashed lines in the figures. Note that multiple tests of the correlations are being performed without controlling the overall error rate; thus, any marginally significant correlations are still questionable. The autocorrelations are dying out quite slowly, and the dominant partial autocorrelation is \(\hat{\phi}(1)=0.769\). These traits are consistent with a nonstationary process.

Figure 2 gives an old fashioned plot of the estimated correlations. It has two advantages. First, it actually gives the numerical correlation estimates \(\hat{\rho}(k)\). Second, one can more readily apply the "ink test." The fact that the plot contains a great deal of ink suggests that the process in nonstationary. It is a little more work to see that in Fig. 1.

In an attempt to eliminate nonstationarity, we consider the first difference process \(y_{i+1}-y_{i},\;i=1,\ldots,60\). The data are plotted in Fig. 3. The estimated autocorrela

Figure 1: Estimated autocorrelations and partial autocorrelations for the coal production data. The label â€œWtâ€ refers to the weight of coal produced

[MISSING_PAGE_EMPTY:9614]

The small correlation and partial correlation values at \(k=1\) and the relatively large values at \(k=2,3\) suggest that the _ARIMA_ models \((0,1,0)\), \((2,1,2)\), and \((3,1,3)\) should be examined. Summary statistics for the three models along with \((1,1,1)\) and several others are contained in Table 7.1. All of the models in Table 7.1 were fitted by maximum likelihood.

\begin{table}
\begin{tabular}{|l|c c c c c|} \hline MODEL & \(\hat{\sigma}^{2}\) & \(\ell(\hat{\xi},\hat{\sigma}^{2})\) & AIC & AICc & BIC \\ \hline \((0,1,0)\) & 3394 & \(-\)328.52 & 661.04 & 661.25 & 665.23 \\ \((1,1,1)\) & 3426 & \(-\)327.79 & 663.59 & 664.31 & 671.96 \\ \((2,1,2)\) & 2925 & \(-\)322.31 & 656.62 & 658.20 & 669.18 \\ \((3,1,3)\) & 3024 & \(-\)322.21 & 660.43 & 663.25 & 677.18 \\ \hline \((2,1,1)\) & 2873 & \(-\)322.32 & 654.63 & 655.74 & 665.11 \\ \((1,1,2)\) & 2952 & \(-\)323.09 & 656.18 & 657.29 & 666.65 \\ \((2,1,1)\)* & 2857 & \(-\)322.67 & 653.35 & 654.08 & 661.73 \\ \((1,1,2)\)* & 2934 & \(-\)323.43 & 654.86 & 655.59 & 663.24 \\ \hline \end{tabular}

* Indicates a model fitted without a constant

\end{table}
Table 7.1: Coal production ARIMA model selection statistics

Figure 7.4: Estimated autocorrelations and partial autocorrelations for the first difference of the coal production data

[MISSING_PAGE_FAIL:310]

Unlike previous editions of this book, I'm going to believe the maximum likelihood fit (R was not available at the time of the previous edition) and proceed with that analysis.

From the statistics in Table 7.1, the models \((2,1,1)\)*, and \((2,1,1)\) appear reasonable. The model \((2,1,1)\)* is an _ARIMA_ model in which the constant is assumed to be zero and looks better than \((2,1,1)\). The R library forecast has a procedure for automatically finding the best model. In this case auto.arima arrived at the same model: \(ARIMA(2,1,1)\) with no constant. (forecast also has testing procedures for determining how many differences are needed.)

Having had the benefit of looking at the 7 years of additional data displayed in Fig. 6.11, I am inclined to (cheat and) believe that the series has an increasing trend, so my choice for further consideration is \((2,1,1)\). Recall that a linear trend corresponds to a nonzero mean in the first difference process.

The asymptotic correlation matrix for the estimated parameters is

\[\begin{array}{ccccc}&&\hat{\phi}_{1}&&\hat{\phi}_{2}&&\hat{\theta}_{2}&&\hat{ \alpha}\\ \hat{\phi}_{1}&&1.000&&0.042&&-0.759&&-0.022\\ \hat{\phi}_{2}&&0.042&&1.000&&0.390&&-0.022\\ \hat{\theta}_{2}&&-0.759&&0.390&&1.000&&0.008\\ \hat{\alpha}&&-0.022&&-0.022&&0.008&&1.000\end{array}\]

The residuals from this fitted model should be evaluated to see if they are consistent with a white noise error process. The correlation and partial correlation functions are given in Fig. 7.5. Neither seems unreasonable. In fact, the \(P\) values for the Ljung-Box test at \(h=10,15,20\) are almost suspiciously high; all exceeding \(0.99\).

In results not shown, a rankit (normal scores) plot looks reasonably linear, and a frequency analysis of the residuals gives spectral estimates that are consistent with the error process being white noise. The primary problem with the residuals is that the variability seems to decrease after 1960. The plot of residuals versus year is given in Fig. 7.6.

Assuming that the possible heteroscedasticity is not a serious problem, what have we accomplished? Not a great deal. The model \((2,1,1)\) does not really explain the data very well. Fuller (1976, p. 364) suggests using \(F\) statistics for approximate tests of model adequacy. These are certainly reasonable statistics for model comparisons. The problem with their use is in determining an appropriate reference distribution. From the R output of a maximum likelihood fit to an \(ARIMA(p,d,q)\) with a constant, the sum of squares error for the model is

\[SSE=\hat{\sigma}^{2}\times(n-p-d-q-1)\]

Testing \((2,1,1)\) against \((0,1,0)\) gives

\[F=\frac{(200222.1-160901.9)/3}{2873.249}=4.56.\]If we compare the test statistic to an \(F(3,56)\) distribution, we obtain a rough \(P\)_value_ of 0.01. For comparing a model to the model of no \(ARMA\) structure, this is not particularly impressive. The \(R^{2}\) type measure

\[\frac{200222.1-160901.9}{200222.1}=0.196\]

indicates that the \((2,1,1)\) model accounts for only 20% of the variability in the data. This, by itself, does not mean that the fitted model is a poor one; we could have the perfect model but a process that is subject to a great deal of variability. However, whether the model is appropriate or not, we cannot expect to obtain accurate forecasts from it. The squared correlation between the data \(y_{t}\) and the fitted values \(\hat{y}_{t}\) is a respectable 75% but the justification of looking at this squared correlation when the \(y_{t}\)s are dependent is dubious.

One of the most important uses of time domain models is to predict the future. Table 2 contains the actual coal production figures for 1981 through 1987 along with the forecasts based on the \((2,1,1)\) model fitted both with and without a nonzero

Figure 5: Estimated autocorrelations and partial autocorrelations for the residuals of the \(ARIMA(2,1,1)\) model

mean. Ninety-five percent prediction intervals are given for both models. While neither model gives very accurate predictions, the model that includes a trend does a bit better. The model without a trend gives predictions that are settling down around 792. For both models, the prediction intervals are so large that, in spite of the poor point predictions, the actual coal production values fall inside them. This results from the large estimates of \(\sigma^{2}\) associated with the models.

\begin{table}
\begin{tabular}{|c c|c c c|c c c|} \hline  & & _ARIMA_ & 95\% Limits & _ARIMA_ & 95\% Limits \\ Year & Actual & \((2,1,1)\) & Lower & Upper & \((2,1,1)\)* & Lower & Upper \\ \hline
81 & 818.4 & 779.18 & 674.12 & 884.24 & 774.76 & 669.99 & 879.53 \\
82 & 833.5 & 803.50 & 654.93 & 952.06 & 792.71 & 643.66 & 941.77 \\
83 & 777.9 & 813.36 & 652.20 & 974.51 & 798.48 & 636.24 & 960.72 \\
84 & 891.8 & 806.17 & 619.37 & 992.97 & 787.15 & 599.20 & 975.10 \\
85 & 879.0 & 817.65 & 611.30 & 1024.01 & 793.34 & 585.68 & 1001.01 \\
86 & 886.1 & 822.02 & 600.77 & 1043.27 & 793.16 & 570.20 & 1016.12 \\
87 & 912.7 & 824.16 & 585.88 & 1062.45 & 790.87 & 550.78 & 1030.96 \\ \hline \end{tabular} The model \((2,1,1)\)* is fitted with a mean of zero

\end{table}
Table 2: _ARIMA_ model forecasts

Figure 6: Time plot for the residuals of the \((2,1,1)\) model

[MISSING_PAGE_FAIL:314]

The quality of the predictions goes down markedly after 1980. It is always harder to predict the future than the past, and our model is not particularly effective. Actually, both Figs. 7.6 and 7.7 suggest that the behavior of the series since 1960 may be inconsistent with the previous data. Thus, it is not surprising that the predictions for 1981 to 1987 are not terribly good. Perhaps a better approach would be to analyze only the data from 1960 forward. Unfortunately, 21, or even 28 observations make a rather inadequate database for time series analysis.

In general, our only hope for prediction is that the future will behave like the relevant past. If there is no relevant past or if we include the irrelevant past, as we may have in this example, we are left to swim in the oceans of uncertainty, or worse, to follow random paths in the deserts of decision making that lead nowhere but give the illusion of progress. (Ok, ok, I'll take my tongue out of my cheek now.) \(\Box\)

### Seasonal Adjustment

Consider a time series consisting of monthly flypaper sales. (If you don't know what flypaper is, think of bug zappers.) It is reasonable that sales may be related to the previous couple of months, but sales may very well be related to the corresponding values in the previous year. An appropriate autoregressive model might be

\[y_{t}=\alpha+\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\phi_{1,1}y_{t-12}-\phi_{1}\phi_{ 1,1}y_{t-13}-\phi_{2}\phi_{1,1}y_{t-14}+e_{t},\]

or equivalently,

\[y_{t}=\alpha+\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\phi_{1,1}[y_{t-12}-\phi_{1}y_{t- 13}-\phi_{2}y_{t-14}]+e_{t}\,.\]

Write

\[\Phi(B)=1-\phi_{1}B-\phi_{2}B^{2}\]

and

\[\Phi_{1}(B^{12})=1-\phi_{1,1}B^{12};\]

then, model (7.7.1) is

\[\Phi_{1}(B^{12})\Phi(B)[y_{t}-\mu]=e_{t}\,.\]

This is referred to as a _multiplicative seasonal autoregressive_ model and is written \(AR(2)\times(1)_{12}\).

In general, if the seasonal effects occur every \(T\) time units, we can define a _multiplicative seasonal autoregressive integrated moving average_ model \(ARIMA(p,d,q)\times(P,D,Q)_{T}\),

\[\Phi_{P}(B^{T})\Phi(B)\nabla_{T}^{D}\nabla^{d}[y_{t}-\mu]=\Theta_{Q}(B^{T}) \Theta(B)e_{t},\]where

\[\Phi_{P}(B^{T}) = 1-\sum_{s=1}^{P}\phi_{s,P}B^{Ts},\] \[\nabla_{T}^{D} = (1-B^{T})^{D},\]

and

\[\Theta_{Q}(B^{T})=1-\sum_{s=1}^{Q}\theta_{s,Q}B^{Ts}.\]

These models are discussed in detail in Box et al. (2015).

Example 7.7.1.: Table 7.3 consists of data from a daily census made of the inpatients at the University of Wisconsin Hospital between April 21 and July 29, 1974. These data were previously presented by Pandit and Wu (1983, Appendix A). Figure 7.8 presents the data graphically. There is a clear periodicity in the data. While this suggests that a frequency analysis of the data may be particularly enlightening, our object is to illustrate techniques used for identifying multiplicative seasonal ARIMA models. Figure 7.9 presents the estimated correlation function and the estimated partial correlation function for the series. Note that the correlation function displays periodic behavior with a period of 7. This makes sense in terms of a weekly cycle. Moreover, the correlations are dying out very slowly. Together, these suggest the appropriateness of a seasonal difference of order 7.

It should be noted that the need for seasonal differencing is often accompanied by the need for regular differencing. Moreover, the need for seasonal differencing is often hidden until the regular differencing is completed.

\begin{table}
\begin{tabular}{|c|c c c c c c c|} \hline  & \multicolumn{8}{c|}{Day} \\ Week & Su & M & Tu & W & Th & F & Sa \\ \hline I & 397 & 462 & 486 & 483 & 477 & 438 & 407 \\ II & 421 & 480 & 484 & 486 & 479 & 415 & 400 \\ III & 419 & 477 & 510 & 503 & 500 & 435 & 408 \\ IV & 417 & 478 & 497 & 500 & 512 & 450 & 421 \\ V & 423 & 471 & 496 & 478 & 463 & 413 & 396 \\ VI & 366 & 375 & 444 & 469 & 480 & 439 & 402 \\ VII & 442 & 492 & 507 & 518 & 493 & 439 & 399 \\ VIII & 428 & 476 & 499 & 488 & 460 & 419 & 380 \\ IX & 406 & 472 & 502 & 495 & 490 & 443 & 398 \\ X & 417 & 490 & 505 & 499 & 484 & 430 & 384 \\ XI & 392 & 452 & 455 & 426 & 414 & 405 & 379 \\ XII & 410 & 485 & 514 & 525 & 511 & 461 & 436 \\ XIII & 444 & 488 & 494 & 510 & 493 & 429 & 392 \\ XIV & 420 & 466 & 476 & 494 & 484 & 423 & 388 \\ XV & 411 & 472 & & & & & & \\ \hline \end{tabular}
\end{table}
Table 7.3: University of Wisconsin hospital data Figure 7.8: Plot of hospital data

Figure 7.9: Estimated autocorrelations and partial autocorrelations for the hospital data

Figure 10 gives the correlation function and partial correlation function for the series generated by taking differences of order 7. Figure 11 gives the seasonally differenced series. The correlations display the classic pattern of an autoregressive process with \(p>1\). The only problem is that they are dying out very slowly. The partial correlations show an extremely large value for \(\phi_{1}\). These conclusions are consistent with the need for a regular difference.

Figure 12 gives the correlation function and partial correlation function for the series generated by taking a seasonal difference of order 7 and a regular difference of order 1. The correlations suggest the need for a moving average term of order 7. Thus, the suggested moving average portion of the model is \(\Theta(B)=1\) and \(\Theta_{1}(B^{7})=1-\theta_{1,1}B^{7}\). The partial correlations are large for \(k=6,7,13\) and not small for \(k=1\). One possible way to model this would be to take \(\Phi(B)=1-\phi_{1}B-\phi_{6}B^{6}\) and \(\Phi_{1}(B^{7})=1-\phi_{1,1}B^{7}\). Altogether, the model is an \(ARIMA([1,6],1,0)\times(1,1,1)_{7}\), where the notation \([1,6]\) is used to indicate the peculiar nature of \(\Phi(B)\). Although this is a model that involves only four parameters in addition to \(\sigma^{2}\), it is really quite complex. The model for \(y_{t}\) involves \(y_{t-k}\) for \(k=1\), 2, 6, 7, 8, 9, 13, 14, 15, 16, 20, and 21.

Figure 10: Estimated autocorrelations and partial autocorrelations for the seasonally differenced hospital data

While we halt this example now, it should be recognized that this is only the beginning of an analysis of these data. The model needs to be fitted, the residuals checked, and other models need to be investigated. The plot in Fig. 13, of the regularly and seasonally differenced series, is also quite interesting. There is a strange increase in variability that occurs around 35 days and again around 70 days. This may just be an oddity of the data, or it may be an indication of some structure in the hospital that generates a 5 week seasonal pattern. More investigation of hospital practices or more data would be needed to verify a 5 week cycle. Of course, the odd behavior of the series could just mean that differencing has not achieved stationarity. \(\Box\)

### The Multivariate State-Space Model and the Kalman Filter

The _state-space model_ provides a quite general paradigm for modeling multivariate time series. At each time \(t\), a \(q_{t}\times 1\) vector of observations \(Y_{t}\) is observed. We assume that \(Y_{t}\) satisfies a linear model

\[Y_{t}=X_{t}\beta_{t}+e_{t} \tag{7.8.1}\]

Figure 11: Seasonally differenced hospital data

in which all of the components are allowed to vary with time but \(X_{t}\) always has \(p\) columns. The error vector \(e_{t}\) is assumed to satisfy

\[\mathrm{E}(e_{t})=0;\qquad\mathrm{Cov}(e_{t})\equiv V_{t}\,.\]

The linear model (7.8.1) is often called the _observation equation_.

Dependencies between times are modeled through the \(p\times 1\) vector \(\beta_{t}\). Rather than assuming that \(\beta_{t}\) is fixed, we assume that \(\beta_{t}\) satisfies a multivariate autoregressive model. Let \(\Phi\) be a \(p\times p\) matrix. We assume that

\[\beta_{t}=\Phi\beta_{t-1}+\varepsilon_{t}, \tag{7.8.2}\]

where \(\varepsilon_{t}\) is a \(p\times 1\) error vector with

\[\mathrm{E}(\varepsilon_{t})=0;\qquad\mathrm{Cov}(\varepsilon_{t})\equiv \Sigma_{t}.\]

The autoregressive model (7.8.2) is referred to as the _state_ equation.

Finally, for \(i=1,\ldots,t\) and \(j=1,\ldots,t\), assume that

\[\mathrm{Cov}(e_{i},e_{j})=0\ \ i\neq j\,,\]

Figure 7.12: Estimated autocorrelations and partial autocorrelations for the regularly and seasonally differenced hospital data

\[\text{Cov}(\varepsilon_{i},\varepsilon_{j})=0\ \ i\neq j\,,\]

and

\[\text{Cov}(e_{i},\varepsilon_{j})=0\qquad\text{all }i,j\,.\]

To start the sequence off, we assume that \(\beta_{0}\) has

\[\text{E}(\beta_{0}) \equiv \tilde{\beta}_{0},\] \[\text{Cov}(\beta_{0}) \equiv P_{0},\] \[\text{Cov}(\beta_{0},e_{i}) = 0\qquad i=1,\ldots,t,\] \[\text{Cov}(\beta_{0},\varepsilon_{i}) = 0\qquad i=1,\ldots,t\,.\]

Traditionally, prediction of the unobservable vector \(\beta_{t}\) has been viewed as the primary goal of state-space model analysis. The _Kalman filter_ is a recursive procedure for predicting \(\beta_{t}\) on the basis of \(Y_{t},Y_{t-1},\ldots,Y_{1}\).

At first glance, Eq. (7.8.2) may seem rather restrictive. It appears to be a matrix version of a first-order autoregressive process. The fact that it appears to be first or

Figure 7.13: Regularly and Seasonally differenced hospital data

der may seem restrictive. In fact, because (7.8.2) involves matrices, there is nothing intrinsically first-order about it.

Example 7.8.1.  A very basic model in many applications is that observations are the sum of a signal \(\mu_{t}\) plus some noise \(e_{t}\), namely

\[y_{t}=\mu_{t}+e_{t}\,.\]

If the signal is the result of an autoregressive process, a state-space model is appropriate. Suppose \(\mu_{t}\) is an \(AR(3)\) process, namely

\[\mu_{t}=\phi_{1}\mu_{t-1}+\phi_{2}\mu_{t-2}+\phi_{3}\mu_{t-3}+\epsilon_{t}\,.\]

The state equation is

\[\left(\begin{array}{c}\mu_{t}\\ \mu_{t-1}\\ \mu_{t-2}\end{array}\right)=\left[\begin{array}{ccc}\phi_{1}&\phi_{2}&\phi_ {3}\\ 1&0&0\\ 0&1&0\end{array}\right]\left(\begin{array}{c}\mu_{t-1}\\ \mu_{t-2}\\ \mu_{t-3}\end{array}\right)+\left(\begin{array}{c}\epsilon_{t}\\ 0\\ 0\end{array}\right).\]

The observation equation is

\[y_{t}=(1,0,0)\left(\begin{array}{c}\mu_{t}\\ \mu_{t-1}\\ \mu_{t-2}\end{array}\right)+e_{t}\,.\]

Thus, \(X_{t}=(1,0,0)\) and \(\beta_{t}=(\mu_{t},\mu_{t-1},\mu_{t-2})^{\prime}\). Note that, in practice, it would be unusual for the values \(\phi_{1}\), \(\phi_{2}\), and \(\phi_{3}\) to be known. In the development of the Kalman filter, these are assumed to be known. Typically, they will be estimated from the data, and the estimates will be substituted for the true parameters to make predictions. 

The state-space model was originally introduced to track missiles using satellite observations of their positions. The satellite observations \(Y_{t}\) are subject to error. Based on a first-order differential equation, the actual position of the missile is modeled by a first-order autoregressive process.

The following example illustrates a model in which the matrix \(\Phi\) is completely known.

Example 7.8.2.  Phadke (1981) and Meinhold and Singpurwalla (1983) present a model useful in quality control. The number of defective items in a process is transformed into a value \(y_{t}\). (The transformation is used to make the data distribution approximate a normal distribution.) The transformed number of defectives is modeled as a signal plus noise,

\[y_{t}=\mu_{t}+e_{t}\,.\]

However, the signal is generated in an unusual fashion. The signal is subject to a drift. The underlying signal for defectives is a parameter \(\theta_{t}\) determined by \[\theta_{t}=\theta_{t-1}+w_{t,1},\]

where \(w_{t,1}\) is an error term that is uncorrelated with other error terms. However, the signal for any particular observation is subject to additional error, namely

\[\mu_{t}=\theta_{t}+w_{t,2},\]

where \(w_{t,2}\) is an uncorrelated error term. Upon observing that

\[\mu_{t}=\theta_{t-1}+w_{t,1}+w_{t,2}\,,\]

we see that the state equation is

\[\left(\begin{array}{c}\theta_{t}\\ \mu_{t}\end{array}\right)=\left(\begin{array}{cc}1&0\\ 1&0\end{array}\right)\left(\begin{array}{c}\theta_{t-1}\\ \mu_{t-1}\end{array}\right)+\left(\begin{array}{c}w_{t,1}\\ w_{t,1}+w_{t,2}\end{array}\right)\]

and the observation equation is

\[y_{t}=(0,1)\left(\begin{array}{c}\theta_{t}\\ \mu_{t}\end{array}\right)+e_{t}\,.\]

The matrix \(\Phi\) in the state equation is completely known. 

Example 7.8.3.: We now present an \(ARMA(p,q)\) model as a state-space model.

Begin with an \(ARMA(2,1)\),

\[y_{t}=\phi_{1}y_{t-1}+\phi_{2}y_{t-2}+\eta_{t}-\theta_{1}\eta_{t-1}\,,\]

where \(\eta_{t}\) is a white noise process. To write this as a state-space model let \(q_{t}=1\), \(Y_{t}=y_{t}\), \(X_{t}=[1,0]\), \(e_{t}\equiv 0\),

\[\beta_{t}=\left[\begin{array}{c}y_{t}\\ \phi_{2}y_{t-1}-\theta_{1}\eta_{t}\end{array}\right],\quad\Phi=\left[\begin{array} []{cc}\phi_{1}&1\\ \phi_{2}&0\end{array}\right],\quad\varepsilon_{t}=\left[\begin{array}{c}1\\ -\theta_{1}\end{array}\right]\eta_{t}\,.\]

Note that

\[\beta_{t+1}=\Phi\beta_{t}+\varepsilon_{t+1}\]

becomes

\[\left[\begin{array}{c}y_{t+1}\\ \phi_{2}y_{t}-\theta_{1}\eta_{t+1}\end{array}\right]=\left[\begin{array}{cc} \phi_{1}&1\\ \phi_{2}&0\end{array}\right]\left[\begin{array}{c}y_{t}\\ \phi_{2}y_{t-1}-\theta_{1}\eta_{t}\end{array}\right]+\left[\begin{array}{c}1\\ -\theta_{1}\end{array}\right]\eta_{t+1}\]

or

\[\left[\begin{array}{c}y_{t+1}\\ \phi_{2}y_{t}-\theta_{1}\eta_{t+1}\end{array}\right] =\left[\begin{array}{cc}\phi_{1}y_{t}+\phi_{2}y_{t-1}-\theta_{1 }\eta_{t}\\ \phi_{2}y_{t}\end{array}\right]+\left[\begin{array}{c}\eta_{t+1}\\ -\theta_{1}\eta_{t+1}\end{array}\right]\] \[=\left[\begin{array}{cc}\phi_{1}y_{t}+\phi_{2}y_{t-1}+\eta_{t+1 }-\theta_{1}\eta_{t}\\ \phi_{2}y_{t}-\theta_{1}\eta_{t+1}\end{array}\right].\]The first row of the equation is the ARMA(2,1) model and the second row is tautological.

Now consider an \(ARMA(r,r-1)\) model,

\[y_{t}=\sum_{j=1}^{r}\phi_{j}y_{t-j}+\eta_{t}-\sum_{k=1}^{r-1}\theta_{k}\eta_{t-k}\,.\]

To write this as a state-space model, let

\[\beta_{t}=\left[\begin{array}{c}y_{t}\\ \sum_{j=1}^{r-1}\phi_{j+1}y_{t-j}-\sum_{k=1}^{r-1}\theta_{k}\eta_{t+1-k}\\ \sum_{j=2}^{r-1}\phi_{j+1}y_{t+1-j}-\sum_{k=2}^{r-1}\theta_{k}\eta_{t+2-k}\\ \vdots\\ \phi_{r}y_{t-1}-\theta_{r-1}\eta_{t}\end{array}\right]\]

\[\Phi=\left[\begin{array}{cccc}\phi_{1}&1&&0\\ \phi_{2}&&\ddots&\\ \vdots&0&&1\\ \phi_{r}&0&\cdots&0\end{array}\right],\qquad\varepsilon_{t}=\left[\begin{array} []{c}1\\ -\theta_{1}\\ -\theta_{2}\\ \vdots\\ -\theta_{r-1}\end{array}\right]\eta_{t}\]

To write a general \(ARMA(p,q)\) model as a state-space model, take \(r=\max(p,q+1)\) and set appropriate parameters equal to 0. \(\Box\)

Durbin and Koopman (2012) discuss how to write arbitrary ARIMA models as state-space models.

The state-space model and the Kalman filter were originally introduced by Kalman (1960) and Kalman and Bucy (1961). This was done in the engineering literature. Harrison and Stevens (1971, 1976) first presented the method as a Bayesian procedure based on \(\Phi\) being known and a multivariate normal distribution for \((\beta^{\prime}_{0},e^{\prime}_{1},\ldots,e^{\prime}_{t},\varepsilon^{\prime}_{ 1},\ldots,\varepsilon^{\prime}_{t})^{\prime}\). Meinhold and Singpurwalla (1983) give a nice exposition of this approach. The approach taken here is based on linear expectations. Because linear expectations are the conditional expectations for multivariate normals, the derivation given here is also valid for multivariate normals. Diderrich (1985) has examined the relationship between the Kalman filter and Goldberger-Theil estimators. Wegman (1982) relates the Kalman filter to stochastic differential equations. Shumway and Stoffer (2011) give a number of references to various applications of the Kalman filter.

Since the previous edition of this work, state-space models have become a more standard part of time series, cf. Prado and West (2010) and Durbin and Koopman (2012).

[MISSING_PAGE_EMPTY:9631]

to time \(t\); thus,

\[\text{Cov}(\beta_{t},e_{t})=0\,.\]

Second, \(\beta_{t-1}\) is a linear function of \(\beta_{0}\) and errors previous to time \(t\), so

\[\text{Cov}(\beta_{t-1},\varepsilon_{t})=0\,.\]

Third, by Proposition B.1.10,

\[\text{Cov}(\beta_{t-1},\beta_{t-1}-\hat{E}(\beta_{t-1}|x))=P_{t-1}\,.\]

Fourth, because \(\text{Cov}(\varepsilon_{t},x)=0\) and \(\hat{E}(\beta_{t-1}|x)\) is a linear function of \(x\),

\[\text{Cov}(\varepsilon_{t},\beta_{t-1}-\hat{E}(\beta_{t-1}|x))=0\,.\]

The computation goes as follows.

\[\text{Cov}[\beta_{t},Y_{t}-\hat{E}(Y_{t}|x)]\] \[=\text{Cov}[\beta_{t},X_{t}\Phi(\beta_{t-1}-\hat{E}(\beta_{t-1}|x ))+X_{t}\varepsilon_{t}+e_{t}]\] \[=\text{Cov}[\beta_{t},X_{t}\Phi(\beta_{t-1}-\hat{E}(\beta_{t-1}|x ))]+\text{Cov}[\beta_{t},X_{t}\varepsilon_{t}]\] \[=\text{Cov}[\Phi\beta_{t-1}+\varepsilon_{t},X_{t}\Phi(\beta_{t-1 }-\hat{E}(\beta_{t-1}|x))]\] \[\quad+\text{Cov}[\Phi\beta_{t-1}+\varepsilon_{t},X_{t}\varepsilon _{t}]\] \[=\text{Cov}[\Phi\beta_{t-1},X_{t}\Phi(\beta_{t-1}-\hat{E}(\beta_{t -1}|x))]+\text{Cov}[\varepsilon_{t},X_{t}\varepsilon_{t}]\] \[=\Phi\text{Cov}[\beta_{t-1},\beta_{t-1}-\hat{E}(\beta_{t-1}|x)] \Phi^{\prime}X_{t}^{\prime}+\Sigma_{t}X_{t}^{\prime}\] \[=(\Phi P_{t-1}\Phi^{\prime}+\Sigma_{t})X_{t}^{\prime}\,. \tag{7.8.8}\]

Substituting (7.8.4), (7.8.5), (7.8.6) and (7.8.8) into (7.8.3) gives the standard form for the Kalman filter. Let

\[R_{t}\equiv\Phi P_{t-1}\Phi^{\prime}+\Sigma_{t}; \tag{7.8.9}\]

then,

\[\hat{E}(\beta_{t}|Y_{t},x)=\] \[\Phi\hat{E}(\beta_{t-1}|x)+R_{t}X_{t}^{\prime}[X_{t}R_{t}X_{t}^{ \prime}+V_{t}]^{-1}[Y_{t}-X_{t}\Phi\hat{E}(\beta_{t-1}|x)]. \tag{7.8.10}\]

Equation (7.8.10) is a formula for predicting \(\beta_{t}\) given \(Y_{t}\), the prediction of \(\beta_{t-1}\), and the matrix \(P_{t-1}\) defined in (7.8.7). \(P_{t-1}\) enters through Eq. (7.8.9). Note that the only matrix assumed to be nonsingular is \(X_{t}R_{t}X_{t}^{\prime}+V_{t}\). Neither \(V_{t}\) nor \(\Sigma_{t}\) are assumed nonsingular. This is important in some applications (e.g., Example 7.8.1).

To use (7.8.10) recursively, we need a recursive formula for \(P_{t}\). From Proposition B.1.9,

\[P_{t} = \text{Cov}[\beta_{t}-\hat{E}(\beta_{t}|Y_{t},x)]\] \[= \text{Cov}[\beta_{t}-\hat{E}(\beta_{t}|x)]-\text{Cov}[\beta_{t}, Y_{t}-\hat{E}(Y_{t}|x)]\]\[\times[\text{Cov}(Y_{t}-\hat{E}(Y_{t}|x))]^{-1}\text{Cov}[Y_{t}-\hat{E}(Y_{t}|x), \beta_{t}]\,.\]

Parts of this have already been computed in (7.8.6) and (7.8.8). The only new computation is

\[\text{Cov}[\beta_{t}-\hat{E}(\beta_{t}|x)] = \text{Cov}[\Phi(\beta_{t-1}-\hat{E}(\beta_{t-1}|x))+\varepsilon_{t}]\] \[= \text{Cov}[\Phi(\beta_{t-1}-\hat{E}(\beta_{t-1}|x))]+\text{Cov}[ \varepsilon_{t}]\] \[= \Phi P_{t-1}\,\Phi^{\prime}+\Sigma_{t}\,.\]

Thus, using (7.8.9), we get the recursive formula

\[P_{t}=R_{t}-R_{t}X_{t}^{\prime}[X_{t}R_{t}X_{t}^{\prime}+V_{t}]^{-1}X_{t}R_{t}\,. \tag{7.8.11}\]

Note that the recursive nature of (7.8.11) lies in the fact that \(R_{t}\) is a function of \(P_{t-1}\). The matrix \(P_{t}\) is useful in updating predictions but also provides standard errors of prediction for linear combinations of \(\beta_{t}\).

To start the recursive process based on (7.8.9), (7.8.10), and (7.8.11), we need an initial prediction for \(\beta_{0}\) and the covariance matrix for the error of that prediction. The initial prediction is based on no data, so \(\hat{E}(\beta_{0})=\text{E}(\beta_{0})=\tilde{\beta}_{0}\) and \(\text{Cov}(\beta_{0}-\hat{E}(\beta_{0}))=\text{Cov}(\beta_{0})=P_{0}\).

Having developed a procedure for obtaining \(\hat{E}(\beta_{t}|Y_{t},x)\), prediction of the future given \(Y_{1},\ldots,Y_{t}\) is easily performed:

\[\hat{E}(\beta_{t+1}|Y_{t},x) = \hat{E}(\Phi\beta_{t}+\varepsilon_{t+1}|Y_{t},x)\] \[= \Phi\hat{E}(\beta_{t}|Y_{t},x).\]

The prediction covariance is

\[\text{Cov}[\beta_{t+1}-\hat{E}(\beta_{t+1}|Y_{t},x)] = \text{Cov}[\Phi\{\beta_{t}-\hat{E}(\beta_{t}|Y_{t},x)\}+\varepsilon _{t+1}]\] \[= \Phi P_{t}\Phi^{\prime}+\Sigma_{t+1}\] \[= R_{t+1}\,.\]

Similarly,

\[\hat{E}(Y_{t+1}|Y_{t},x) = \hat{E}(X_{t+1}\beta_{t+1}+e_{t+1}|Y_{t},x)\] \[= X_{t+1}\hat{E}(\beta_{t+1}|Y_{t},x)\] \[= X_{t+1}\Phi\hat{E}(\beta_{t}|Y_{t},x),\]

with prediction covariance matrix

\[\text{Cov}[Y_{t+1}-\hat{E}(Y_{t+1}|Y_{t},x)] = \text{Cov}[X_{t+1}\{\beta_{t+1}-\hat{E}(\beta_{t+1}|Y_{t},x)\}+e_ {t+1}]\] \[= X_{t+1}\text{Cov}[\beta_{t+1}-\hat{E}(\beta_{t+1}|Y_{t},x)]X_{t+1} ^{\prime}+V_{t+1}\] \[= X_{t+1}R_{t+1}X_{t+1}^{\prime}+V_{t+1}\,.\]

To predict events further in the future,\[\hat{E}(\beta_{t+r}|Y_{t},x)=\Phi^{r}\hat{E}(\beta_{t}|Y_{t},x),\]

with the prediction covariance matrix obtained recursively using

\[\text{Cov}[\beta_{t+r}-\hat{E}(\beta_{t+r}|Y_{t},x)]=\Phi\text{Cov}[\beta_{t+r-1 }-\hat{E}(\beta_{t+r-1}|Y_{t},x)]\Phi^{\prime}+\Sigma_{t+r}\,.\]

The covariance matrix depends only on \(P_{t}\), \(\Phi\), and \(\Sigma_{t+1},\ldots,\Sigma_{t+r}\). To predict future observations,

\[\hat{E}(Y_{t+r}|Y_{t},x) = X_{t+r}\hat{E}(\beta_{t+r}|Y_{t},x)\] \[= X_{t+r}\Phi^{r}\hat{E}(\beta_{t}|Y_{t},x)\,.\]

Again, the prediction covariance matrix is obtained recursively:

\[\text{Cov}[Y_{t+r}-\hat{E}(Y_{t+r}|Y_{t},x)]=X_{t+r}\text{Cov}[\beta_{t+r}-\hat {E}(\beta_{t+r}|Y_{t},x)]X_{t+r}^{\prime}+V_{t+r}\,.\]

Note that this covariance matrix does not involve any of \(X_{t+1},\ldots,X_{t+r-1}\).

If \(\beta_{0}\), \(e_{1},\ldots,e_{t}\), and \(\varepsilon_{1},\ldots,\varepsilon_{t}\) are taken to have a joint normal distribution, then the posterior distribution of \(\beta_{t}\) given the data \((Y_{t},x)\) is multivariate normal with mean \(\hat{E}(\beta_{t}|Y_{t},x)\) and covariance matrix \(P_{t}\). The subjective Bayesian aspect of this analysis lies entirely in the a priori assumption that

\[\beta_{0}\sim N(\beta_{0},P_{0})\,.\]

#### Parameter Estimation

For the purposes of prediction, the matrices \(\Phi\) and \(X_{i}\), \(V_{i}\), \(\Sigma_{i}\), \(i=1,\ldots,t\) were assumed to be known. This is similar to the approach taken for prediction in time domain models. In practice, as with time domain models, many of these parameters must be estimated. In particular, the matrix \(\Phi\) defining the multivariate autoregressive process and the covariances matrices \(V_{i}\) and \(\Sigma_{i}\) generally need to be estimated. The design matrices \(X_{i}\) are typically known. The estimation of the covariance matrices presents special problems in that they include many more parameters than there are observations. The covariance matrices must be modeled in some way if estimates are to be obtained. Suppose the data \(Y_{1},\ldots,Y_{n}\) have been observed. One commonly used and particularly simple model is that, for some unknown positive definite matrices \(V\) and \(\Sigma\),

\[V_{1}=V_{2}=\cdots=V_{n}=V\]

and

\[\Sigma_{1}=\Sigma_{2}=\cdots=\Sigma_{n}=\Sigma.\]

The assumption that the observation covariance matrices \(V_{i}\) are equal implies that the number of observations available at each time is the same. Although model (7.8.2)implies that the dimensions of \(\beta_{t}\) and \(\varepsilon_{t}\) remain constant, in general there is no such restriction on \(Y_{t}\), \(X_{t}\), and \(e_{t}\).

The standard estimation method seems to be maximum likelihood based on the assumption that \((\beta_{0},e_{1},\ldots,e_{n},\varepsilon_{1},\ldots,\varepsilon_{n})^{\prime}\) has a multivariate normal distribution. It follows that the data \(Y_{1},Y_{2},\ldots,Y_{t}\) have a multivariate normal distribution. The joint density can be written as the product of conditional densities, that is,

\[f(Y_{1},\ldots,Y_{t})=f(Y_{1})f(Y_{2}|Y_{1})f(Y_{3}|Y_{1},Y_{2})\cdots f(Y_{t} |Y_{1},Y_{2},\ldots,Y_{t-1})\,.\]

In fact, assuming that \(\beta_{0}\) is fixed, all of the conditional distributions are normals with means and covariances given by the Kalman filter. For example,

\[Y_{t}|Y_{1},\ldots,Y_{t-1}\sim N(\hat{E}(Y_{t}|x),\text{Cov}[Y_{t}-\hat{E}(Y_{ t}|x)])\,,\]

where \(\hat{E}(Y_{t}|x)\) is given before (7.8.5) and \(\text{Cov}[Y_{t}-\hat{E}(Y_{t}|x)]\) is given by (7.8.6). Write

\[\hat{e}_{t}\equiv Y_{t}-\hat{E}(Y_{t}|Y_{1},\ldots,Y_{t-1})\]

and

\[Q_{t}\equiv\text{Cov}[Y_{t}-\hat{E}(Y_{t}|Y_{1},\ldots,Y_{t-1})]\,.\]

The likelihood function is the product of the conditional normal densities, that is,

\[L(\Phi,V_{1},\ldots,V_{n},\Sigma_{1},\ldots,\Sigma_{n})=\\ \left(\prod_{t=1}^{n}(2\pi)^{-q_{t}/2}\right)\left(\prod_{t=1}^{n }|Q_{t}|^{-1/2}\right)\exp\left[-\frac{1}{2}\sum_{t=1}^{n}\hat{e}_{t}^{\prime }Q_{t}^{-1}\hat{e}_{t}\right].\]

As usual, it is easier to maximize the log-likelihood,

\[\ell(\Phi,V_{1},\ldots,V_{n},\Sigma_{1},\ldots,\Sigma_{n})=-\log(2\pi)\sum_{t= 1}^{n}\frac{q_{t}}{2}-\frac{1}{2}\sum_{t=1}^{n}\log(|Q_{t}|)-\frac{1}{2}\sum_ {t=1}^{n}\hat{e}_{t}^{\prime}Q_{t}\hat{e}_{t}\,.\]

Remembering that parametric models for \(V_{1},\ldots,V_{n}\) and \(\Sigma_{1},\ldots,\Sigma_{n}\) are mandatory, it is important to note that this log-likelihood function is valid for any such models. If \(V_{i}=V_{i}(\eta)\) and \(\Sigma_{i}=\Sigma_{i}(\zeta)\), \(i=1,\ldots,n\), then

\[\ell(\Phi,\eta,\zeta)=\ell(\Phi,V_{1}(\eta),\ldots,V_{n}(\eta),\Sigma_{1}( \zeta),\ldots,\Sigma_{n}(\zeta))\,.\]

Similarly, some or all of the components of \(\Phi\) can be known without materially changing the likelihood function.

In practice, with almost any parameterization, the log-likelihood will be a very complicated nonquadratic function of the parameters. (Quadratic functions are relatively easy to maximize.) Some sort of iterative maximization technique is generally required. Much of the work on this maximization problem has used the covariance model that assumes equal covariance matrices. Shumway and Stoffer (1982, 2011) discuss maximization of \(\ell(\Phi,V,\Sigma)\) using the EM algorithm. The EM algorithm is presented in Dempster, Laird, and Rubin (1977). The Newton-Raphson algorithm has also been used to maximize \(\ell(\Phi,V,\Sigma)\). This approach is discussed by Gupta and Mehra (1974), Jones (1980), and Ansley and Kohn (1984).

With a likelihood this complex, it is probably safer to use Bayesian methods rather than relying on a single point that maximizes the likelihood, cf. West and Harrison (1997), Prado and West (2010), Cressie and Wikle (2011).

#### Missing Values

The assumption of equal covariance matrices seems to be the standard way of modeling the \(V_{t}\)s and \(\Sigma_{t}\)s. As mentioned earlier, equal \(V_{t}\)s imply that \(q_{t}\), the dimension of \(Y_{t}\), is the same for all \(t\). This does not seem like a particularly harsh requirement. The vector \(Y_{t}\) will often consist of observations on \(q_{t}\) dependent variables. The assumption that, at each time \(t\), the same number of variables are available to be observed is not very restrictive. The only problem is that some observations may be missing at some times. The generality of the state-space model allows missing values to be handled easily.

Suppose the \(q\times 1\) vector \(Z_{t}\) is to be observed at time \(t\). Assume an observation equation

\[Z_{t}=W_{t}\beta_{t}+\xi_{t},\]

where

\[\text{Cov}(\xi_{t})=V.\]

However, the actual observation is \(Y_{t}\), which consists of some of the components of \(Z_{t}\). If no components are missing, \(Y_{t}=Z_{t}\), \(X_{t}=W_{t}\), and \(e_{t}=\xi_{t}\). If some components are lost, write

\[Z_{t} = \left[\begin{array}{c}Y_{t}\\ Y_{tM}\end{array}\right],\] \[W_{t} = \left[\begin{array}{c}X_{t}\\ X_{tM}\end{array}\right],\] \[\xi_{t} = \left[\begin{array}{c}e_{t}\\ e_{tM}\end{array}\right],\]

and

\[\text{Cov}(\xi_{t})=\left[\begin{array}{cc}V_{t}&V_{tM}\\ V_{Mt}&V_{MM}\end{array}\right].\]

There is no real loss of generality in assuming that the last components of \(Z_{t}\) are the ones that were not observed. The observation equation for \(Z_{t}\) implies an observation equation for the actual observations,

\[Y_{t}=X_{t}\beta_{t}+e_{t},\]

for which \[\text{Cov}(e_{t})=V_{t}.\]

In estimating the parameters \(\Phi,V\), and \(\Sigma\), the likelihood allows \(V_{t}\) to be a function of the parameters. Because \(V_{t}\) is a submatrix of \(V\), \(V_{t}\) is clearly a function of the parameter matrix \(V\). In fact, the likelihood function is not much more complicated than if none of the components of \(Z_{t}\) were lost.

Of course, the loss of components has no effect on the Kalman filter. The filter is based on \(Y_{t}\) and treats \(V_{t}\) as known. The lost components are simply ignored. The references given earlier for maximizing the likelihood also discuss the missing value problem.

### Additional Exercises

#### Exercise 7.9.1

Which of the following processes are stationary? Which are invertible?

1. \((1-1.5B+0.54B^{2})y_{t}=(1-0.5B)e_{t}\).
2. \((1-\frac{5}{8}B)y_{t}=(1+0.1B-0.56B^{2})e_{t}\).
3. \((1-1.6B+0.55B^{2})y_{t}=(1-0.6B)e_{t}\).
4. \((1-B+0.2475B^{2})y_{t}=(1-B-\frac{7}{36}B^{2})e_{t}\).
5. \((1-0.7B-0.66B^{2}+0.432B^{3})y_{t}=(1-0.8B+0.07B^{2})e_{t}\).
6. \((1-0.8B+0.07B^{2})y_{t}=(1-0.1B)e_{t}\).

#### Exercise 7.9.2

Consider the \(AR(1)\) model

\[y_{t}=10-0.7y_{t-1}+e_{t},\ \sigma^{2}=3.\]

1. Find \(\mu\), \(\sigma(0)\), \(\sigma(1)\), \(\sigma(2)\), and \(\sigma(3)\).
2. If \(y_{4}=12\), will \(y_{5}\) tend to be less than or greater than \(\mu\)?
3. Is the process stationary?
4. Give \(\dot{E}(y_{4+k}|Y)\) and the prediction variance for \(k=1,2,3\) and \(Y=(12,11,11,12)^{\prime}\).

#### Exercise 7.9.3

Consider the \(MA(1)\) model

\[y_{t}=10+e_{t}-0.4e_{t-1},\ \sigma^{2}=3.\]

1. Find \(\mu\), \(\sigma(0)\), \(\sigma(1)\), \(\sigma(2)\), and \(\sigma(3)\).
2. If \(y_{4}=12\), will \(y_{5}\) tend to be less than or greater than \(\mu\)?
3. Is the process stationary?4. Give \(\hat{E}(y_{4+k}|Y_{\infty})\) and the prediction variance for \(k=1,2,3\) and \(Y=(12,11,11,12)^{\prime}\).

##### Exercise 7.9.4

Consider the \(ARMA(1,1)\) model

\[y_{t}=10-0.7y_{t-1}+e_{t}-0.4e_{t-1},\ \sigma^{2}=3\]

1. Find \(\mu\), \(\sigma(0)\), \(\sigma(1)\), \(\sigma(2)\), and \(\sigma(3)\).
2. If \(y_{4}=12\), will \(y_{5}\) tend to be less than or greater than \(\mu\)?
3. Is the process stationary?
4. Give \(\hat{E}(y_{4+k}|Y_{\infty})\) and the prediction variance for \(k=1,2,3\) and \(Y=(12,11,11,12)^{\prime}\).

##### Exercise 7.9.5

Do a time domain analysis of the sunspot data in Exercise 6.9.5. This should include estimation, model fitting, and checking of assumptions.

##### Exercise 7.9.6

Use a multiplicative seasonal model to analyze the international air passenger data of Exercise 6.9.6. This should include estimation, model fitting, and checking of assumptions.

##### Exercise 7.9.7

For an \(MA(1)\) process, find an estimate of \(\theta_{1}\) in terms of \(\hat{\rho}(1)\). Are any restrictions on \(\hat{\rho}(1)\) needed?

##### Exercise 7.9.8

For an \(AR(2)\) process, use the Yule-Walker equations, with \(\hat{\sigma}_{y}(\cdot)\) replacing \(\sigma_{y}(\cdot)\) to obtain estimates

\[\hat{\phi}_{1}=\frac{\hat{\rho}(1)(1-\hat{\rho}(1))}{1-\hat{\rho}(1)^{2}}\]

and

\[\hat{\phi}_{2}=\frac{\hat{\rho}(2)-\hat{\rho}(1)^{2}}{1-\hat{\rho}(1)^{2}}.\]

##### Exercise 7.9.9

Show that the variance for predicting \(k\) steps ahead in an \(AR(1)\) process is

\[\sigma^{2}\frac{1-\phi_{1}^{2k}}{1-\phi_{1}^{2}}.\]

##### Exercise 7.9.10

Show, for an \(MA(1)\) process with parameter \(\theta_{1}\), that

\[\hat{E}(y_{n+1}|Y_{\infty})=-\sum_{s=0}^{\infty}\theta_{1}^{s}y_{n-s}.\]

**Exercise 7.9.11**.: Let \(e_{t}\) be a second-order stationary process. Show that

\[y_{t}=e_{t}-\theta_{1}e_{t-1}\]

and

\[w_{t}=e_{t}-\frac{1}{\theta_{1}}e_{t-1}\]

have the same correlation function.

**Exercise 7.9.12**.: What are the largest possible values of \(|\rho(1)|\) and \(|\rho(2)|\) for an \(MA(2)\) process?

**Exercise 7.9.13**.: Find the \(\psi_{i}\)s, \(\mu\), and \(\sigma(k)\) for the following processes.

1. The \(AR(3)\) process \[(1-0.9B)(1+0.8B)(1-0.6B)y_{t}=e_{t}.\]
2. The \(ARMA(2,1)\) process \[(1-0.9B)(1-0.6B)y_{t}=(1-0.5)e_{t}.\]

**Exercise 7.9.14**.: Show that if all the roots \(x_{0}\) of \(\Phi(x)\) have \(|x_{0}|>1\), then there exists a polynomial

\[\Psi(x)=\sum_{i=0}^{\infty}\psi_{i}x^{i}\]

with

\[\sum_{i=0}^{\infty}|\psi_{i}|<\infty\]

and, for \(x\in[-1,1]\),

\[[\Psi(x)]\,[\Phi(x)]=1.\]

Hint: Do a Taylor expansion of \(1/\Phi(x)\) about 0.

**Exercise 7.9.15**.: Generate 25 realizations of an \(ARMA(1,1)\) process for \(\phi_{1}=-0.8,-0.2,0,.2,.8\) and \(\theta_{1}=-0.8,-0.2,0,.2,.8\).

#### Exercise 7.9.16

Prove _PA_-_V_'s Exercise 6.9.9, that

1. \(\rho_{12\cdot 3}=\frac{\rho_{12}-\rho_{13}\rho_{23}}{\sqrt{1-\rho_{13}^{2}}\sqrt{1-\rho_{23}^{2}}}\)
2. \(\rho_{12\cdot 34}=\frac{\rho_{12\cdot 4}-\rho_{13\cdot 4}\rho_{23\cdot 4}}{\sqrt{1-\rho_{13\cdot 4}^{2}}\sqrt{1-\rho_{23\cdot 4}^{2}}}\).

## References

* Akaike (1973) Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In B.N. Petrov & F. Czaki (Eds.), _Proceedings of the 2nd International Symposium on Information_. Budapest: Akademiai Kiado.
* Ansley & Kohn (1984) Ansley, C. F., & Kohn, R. (1984). On the estimation of ARIMA models with missing values. In E. Parzen (Ed.) _Time series analysis of irregularly observed data_. New York: Springer.
* Bartlett (1946) Bartlett, M. S. (1946). On the theoretical specification of sampling properties of autocorrelated time series. _Journal of the Royal Statistical Society, Supplement, 8_, 27-41.
* Box et al. (1994) Box, G. E. P., Jenkins, G. M., & Reinsel, G. C. (1994). _Time series analysis: Forecasting and control_ (3rd ed.). New York: Wiley.
* Box et al. (2015) Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). _Time series analysis: Forecasting and control_ (5th ed.). New York: Wiley.
* Brockwell & Davis (1991) Brockwell, P. J., & Davis, R. A. (1991). _Time series: Theory and methods_ (2nd ed.). New York: Springer.
* Brockwell & Davis (2002) Brockwell, P. J., & Davis, R. A. (2002). _Introduction to time series and forecasting_ (2nd ed.). New York: Springer.
* Chatfield (2003) Chatfield, C. (2003). _The analysis of time series: An introduction_ (6th ed.). New York: Chapman and Hall.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York: Springer.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton, FL: Chapman and Hall/CRC Press.
* Clayton et al. (1985) Clayton, M. K., Geisser, S., & Jennings, D. E. (1985). A comparison of several model selection procedures. In P. Goel & A. Zellner (Eds.). _Bayesian inference and decision techniques_. Amsterdam: Elsevier Science Publishers B.V.
* Cressie & Wikle (2011) Cressie, N. A. C., & Wikle, C. K. (2011). _Statistics for spatio-temporal data_. New York: Wiley.
* Cressie et al. (1996)* Dempster et al. (1977) Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. _Journal of the Royal Statistical Society, Series B,__39_, 1-38.
* Diderrich (1985) Diderrich, G. T. (1985). The Kalman filter from the perspective of Goldberger-Theil estimators. _The American Statistician,__39_, 193-198.
* Diggle (1990) Diggle, P. J. (1990). _Time series: A biostatistical introduction_. New York: Oxford University Press.
* Draper & Smith (1981) Draper, N., & Smith, H. (1981). _Applied regression analysis_ (2nd ed.). New York: Wiley.
* Durbin & Koopman (2012) Durbin, J., & Koopman, S. J. (2012). _Time series analysis by state space methods_ (2nd ed.). Oxford: Oxford University Press.
* Fuller (1976) Fuller, W. A. (1976). _Introduction to statistical time series_. New York: Wiley.
* Fuller (1996) Fuller, W. A. (1996). _Introduction to statistical time series_ (2nd ed.). New York: Wiley.
* Gupta & Mehra (1974) Gupta, N. K., & Mehra, R. K. (1974). Computational aspects of maximum likelihood estimation and reduction in sensitivity function calculations. _IEEE Transactions on Automatic Control,__AC-19_, 774-783.
* Harrison & Stevens (1971) Harrison, P. J., & Stevens, C. F. (1971). A Bayesian approach to short-term forecasting. _Operations Research Quarterly,__22_, 341-362.
* Harrison & Stevens (1976) Harrison, P. J., & Stevens, C. F. (1976). Bayesian forecasting. _Journal of the Royal Statistical Society, Series B,__38_, 205-247.
* Hurvich & Tsai (1989) Hurvich, C. M., & Tsai, C.-L. (1989). Regression and time series model selection in small samples. _Biometrika,__76_, 297-308.
* Jones (1980) Jones, R. H. (1980). Maximum likelihood fitting of ARMA models to time series with missing observations. _Technometrics,__22_, 389-396.
* Kalman (1960) Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. _Journal of Basic Engineering,__82_, 34-45.
* Kalman & Bucy (1961) Kalman, R. E., & Bucy, R. S. (1961). New results in linear filtering and prediction theory. _Journal of Basic Engineering,__83_, 95-108.
* Lutkepohl (1985) Lutkepohl, H. (1985). Comparison of criteria for estimating the order of a vector autoregressive process. _Journal of Time Series Analysis,__65_, 297-303.
* Marquardt (1963) Marquardt, D. W. (1963). An algorithm for least squares estimation of non-linear parameters. _Journal of the Society for Industrial and Applied Mathematics,__2_, 431-441.
* McLeod (1977) McLeod, A. I. (1977). Improved Box-Jenkins estimators. _Biometrika,__64_, 531-534.
* Meinhold & Singpurwalla (1983) Meinhold, R. J., & Singpurwalla, N. D. (1983). Understanding the Kalman filter. _The American Statistician,__37_, 123-127.
* Pandit & Wu (1983) Pandit, S. M., & Wu, S. M. (1983). _Time series and system analysis with applications_. New York: Wiley.
* San Francisco_, 1045-1052.
* Prado & West (2010) Prado, R., & West, M. (2010). _Time series: Modeling, computation, and inference_. Boca Raton, FL: CRC Press.
* Quenille (1949) Quenille, M. H. (1949). Approximate tests of correlation in time-series. _Journal of the Royal Statistical Society, Series B,__11_, 68-84.

* [SchwarzSchwarz1978] Schwarz, G. 1978. Estimating the dimension of a model. Annals of Statistics, 16, 461-464.
* [Seber and WildSeber and Wild1989] Seber, G. A. F., & Wild, C. J. 1989, 2003. Nonlinear Regression. New York: Wiley (The 2003 version seems to be a reprint of the 1989 version.).
* [Shumway and StofferShumway and Stoffer1982] Shumway, R. H., & Stoffer, D. S. 1982. An approach to time-series smoothing and forecasting using the EM algorithm. Journal of Time Series Analysis, 3, 253-264.
* [Shumway and StofferShumway and Stoffer2011] Shumway, R. H., & Stoffer, D. S. 2011. Time Series Analysis and Its Applications: With R Examples (3rd ed.). New York: Springer.
* [WegmanWegman1982] Wegman, E. J. 1982. Kalman filtering. In N. Johnson & S. Kotz (Eds.) Encyclopedia of statistics. New York: Wiley.
* [West HarrisonWeg Harrison1997] West, M., & Harrison, P. J. 1997. Bayesian forecasting and dynamic models (2nd ed.). New York: Springer.

## Chapter 8 Linear Models for Spatial Data: Kriging

**Abstract** This chapter addresses linear models for spatial data. A key aspect is the introduction of models for the covariance between data points separated in space. The same ideas can be used to model time series but, unlike the methods in the previous two chapters, time is not required to be observed at regular intervals.

There are innumerable situations in which data are collected at various locations in space and thus innumerable potential applications for methods of analysis for spatial data. Data collected at known locations in space are often correlated. For example, deposits of high-quality copper are more likely to occur near other high-quality deposits. The levels of lead contamination in the soil around a smelter are likely to be correlated. The prevalences of AIDS (or any other communicable disease) viewed geographically are correlated. We make no attempt to cover the entire array of procedures developed for spatial data. We only examine the relation of linear models to spatial prediction.

One branch of statistics concerned with spatial prediction is known as _geostatistics_. The practical application of geostatistics was originally developed in relative isolation from the mainstream of statistics. Not surprisingly, it uses some terminology that is unfamiliar to classically trained statisticians. David (1977) and Journel and Huijbregts (1978) give details of the geostatistical approach using geostatistical terminology. Ripley (1981) takes a point of view that is probably more familiar to most statisticians. He uses ideas of prediction for stochastic processes that are closely related to time series methods. Cressie (1993) gives an excellent presentation of both the theory and application of statistics for spatial data. Stein (1999) gives an excellent account of the theory. Isaaks and Srivastava (1989) give a relatively elementary introduction; see also Cliff and Ord (1981) and Kitanidis (1997). In this chapter, we present both traditional and geostatistical terminologies.

Most often, spatial data are sampled at irregular locations. Occasionally, data are collected at all the locations on a uniformly spaced grid. Such data are called _regular lattice data_ and allow some models that are not available with irregular locations.

Contamination around a smelter is largely a two-dimensional issue. Mining minerals and metals is clearly a three-dimensional issue. To a large extent, time seriescan be viewed as a special case of spatial data in which the dimension of the space is \(d=1\). The time series models of the previous chapters assume that observations are taken at equally spaced time intervals. These observations are taken on a one-dimensional regular lattice and special models have been developed for such data. When time series observations are not equally spaced, you might as well view them as arbitrary one-dimensional spatial data.

The state-space model and associated Kalman filter of Sect. 7.8 provide a natural method for examining how spatial linear models change over time. Early work on spatio-temporal modeling includes Huang and Cressie (1996), Berke (1998), and Mardia, Goodall, Redfern, and Alonso (1998). Handcock and Wallis (1994) take an alternative approach to Bayesian spatio-temporal modeling.

While not covered here, Bayesian analysis of spatial data is popular both because of its flexibility and the fact that it naturally accounts for the effects of estimating covariances, cf. Sect. 4.7. In its simplest form, this involves extending Bayesian standard linear models (see _PA_ Sect. 2.9 and Christensen, Johnson, Branscum, & Hanson 2010) by using ideas similar to Chap. 4 and those introduced here to include positive covariances among observations and by incorporating a prior distribution on the additional parameters. Early discussions were given by Kitanidis (1986), Omre and Halvorsen (1989), and Handcock and Stein (1993). Diggle, Tawn, and Moyeed (1998) proposed an extension of generalized linear models to spatial problems along with a Bayesian analysis based on Markov Chain Monte Carlo (MCMC) methods.

Many recent books are devoted to spatial or spatio-temporal data. These include Gelfand, Diggle, Guttorp, Fuentes (2010), Cressie and Wikle (2011), Wikle, Zammit-Mangion, and Cressie (2019), Sherman (2011), and Banerjee, Carlin, and Gelfand (2015). See also Schabenberger and Gotway (2005) and Waller and Gotway (2004), the later focuses on public heath. Hodges (2014) also includes discussion of spatial models. _Many of these emphasize Bayesian methods_.

We begin by discussing the modeling of spatial data in terms of stochastic processes. In Sect. 8.2, linear models and best linear unbiased predictors for spatial data are presented. Best linear unbiased prediction is known in the geostatistics literature as _kriging_. The methods of kriging were developed in France by Matheron (1965, 1969). He was originally inspired by the contributions of D.G. Krige. The French work was performed independently of Goldberger (1962), who first derived general best linear unbiased predictors for linear models. The relationship between prediction based on covariances and prediction based on an alternative measure of variability, the semivariogram, is examined in Sect. 8.3. The role of measurement error is considered in Sect. 8.4. Section 8.5 looks at the effects of estimating the covariances when performing best linear unbiased prediction. Section 8.6 gives models for covariance functions and semivariograms. The special case of spatial lattice data is examined in Sect. 8.7 because of its close relationship to the analysis of time series data observed at regular intervals. Estimation of covariances and the semivariogram is considered in the final section.

### 8.1 Modeling Spatial Data

Spatial data can be considered as a realization of a stochastic process (_random field_)

\[y(u),\qquad u\in D\subset{\bf R}^{d}\,.\]

Here, \(u\) is a location in \(D\). Most often, \(d\), the dimension of the space, is 1, 2, or 3. (To some extent, time could be added to these problems by merely increasing the dimension.) For every value of \(u\), \(y(u)\) is a random variable. To analyze spatial data, we need to model \(y(u)\). We begin by assuming that, for any \(u\), \({\rm E}[y(u)]\) and \({\rm Var}[y(u)]\) exist. It follows that \(y(u)\) can be decomposed as

\[y(u)=m(u)+e(u),\]

where \(m(u)\) is the fixed mean function of \(y(u)\), namely

\[m(u)\equiv{\rm E}[y(u)]\,,\]

and \(e(u)\) is a stochastic error process with

\[{\rm E}[e(u)]=0\,.\]

In particular, \(e(u)\equiv y(u)-m(u)\). Our approach to modeling \(y(u)\) involves modeling both \(m(u)\) and \(e(u)\).

Begin by assuming a linear structure for \(m(u)\). This is known in geostatistics as the _universal kriging_ model. In particular, assume that there are, say, \(p\) known functions of \(u\), say, \(x_{1}(u),x_{2}(u),\ldots,x_{p}(u)\) so that the mean function satisfies

\[m(u)=\sum_{j=1}^{p}\beta_{j}x_{j}(u)\]

for some fixed unknown parameters \(\beta_{1},\ldots,\beta_{p}\). A special case of the universal kriging model is the _ordinary kriging_ model

\[m(u)=\mu\]

for an unknown parameter \(\mu\).

In practice, the \(x_{j}(\cdot)\)s are often just functions of the location coordinates (cf. Chap. 1), but they can be any variables that are available for every location. Satellite data often provide a variety of interesting predictor functions \(x_{j}(u)\).

A mathematically simpler, but typically unrealistic, model is to assume that \(m(u)\) is known. _Simple kriging_ is the special case

\[m(u)=\mu_{0},\]where \(\mu_{0}\) is a known value. The case with \(m(u)\) known will not be treated further. Simple modifications of the procedures outlined later provide data analysis for the case with \(m(u)\) known.

In modeling the error process \(e(u)\), we will be primarily interested in its second-order (second moment) properties. The _covariance function_ is our primary concern,

\[\sigma(u,w)\equiv\text{Cov}[e(u),e(w)]\;.\]

Note also that

\[\sigma(u,w)=\text{Cov}[y(u),y(w)]\,.\]

and

\[\sigma(u,w)=\sigma(w,u).\]

Often, the covariance function is modeled in terms of an unknown parameter vector \(\theta\). In that case, write \(\sigma(u,w;\theta)\). Some of the common assumptions made about \(e(u)\) are that it is (1) second-order stationary, (2) strictly stationary, (3) intrinsically stationary, (4) increment stationary, or (5) isotropic. These terms are defined in the next subsection.

#### Stationarity

We restrict attention to processes for which means and variances exist. A process \(y(u)\) is said to be _strictly stationary_ if for any value \(k\), any locations \(u_{1},\ldots,u_{k}\), any (Borel) sets \(C_{1},\ldots,C_{k}\), and any vector \(h\in\mathbf{R}^{d}\), we have

\[\Pr[y(u_{1})\in C_{1},\ldots,y(u_{k})\in C_{k}]=\Pr[y(u_{1}+h)\in C_{1},\ldots, y(u_{k}+h)\in C_{k}]. \tag{8.1.1}\]

The process is stationary in the sense that the joint distribution of the process evaluated at any set of points is not changed if all of the points are relocated in the same way. In particular,

\[m(u)=m(u+h)\]

for any vector \(h\), so \(m(u)\) must be a constant, namely

\[m(u)=\mu\;. \tag{8.1.2}\]

Also, for two locations \(u\) and \(w\) and any vector \(h\),

\[\sigma(u,w)=\sigma(u+h,w+h).\]

In particular, let \(h=-w\) so

\[\sigma(u,w)=\sigma(u-w,0)\]

and the covariance function can be thought of as a function of \(u-w\) alone. To indicate this, let \(h=u-w\) and write \[\sigma(u,w) = \sigma(u-w)\] \[= \sigma(h).\]

Since \(\sigma(u,w)=\sigma(w,u)\), for a stationary process,

\[\sigma(u-w)=\sigma(w-u),\]

and

\[\sigma(h)=\sigma(-h).\]

Note that for any location \(u\), equation (8.1.3) implies that

\[\sigma(0)=\sigma(u,u)=\operatorname{Var}[y(u)]\,.\]

If \(y(u)\) is strictly stationary and the joint distribution of all the random variables in (8.1.1) is multivariate normal for any \(k\), then the process is called a _Gaussian process_. To specify a particular Gaussian process it is enough to specify (8.1.2) and (8.1.3).

As we have discussed, when variances exist property (8.1.1) implies properties (8.1.2) and (8.1.3). A _second-order_ (weak) _stationary process_ is any process that satisfies (8.1.2) and (8.1.3). A second-order stationary process may be strictly stationary in the sense that (8.1.1) holds, but it need not be. Any second-order stationary process has a corresponding strictly stationary Gaussian process.

We define a process to be _increment stationary_ if it satisfies (8.1.2) and, for any integer \(k\), locations \(u_{1},\ldots,u_{k}\), sets \(C_{1},\ldots,C_{k-1}\), and vector \(h\),

\[\Pr[y(u_{2})-y(u_{1})\in C_{1},\ldots,y(u_{k})-y(u_{k-1})\in C_{k -1}]=\\ \Pr[y(u_{2}+h)-y(u_{1}+h)\in C_{1},\ldots,y(u_{k}+h)-y(u_{k-1}+h) \in C_{k-1}]\,.\]

It is immediate that stationary processes are increment stationary, but the converse need not be true. Brownian motion (see Breiman 1968) is a well-known process with \(u\in[0,\infty)\) that is increment stationary but not stationary.

In the geostatistics literature, second-order properties are not typically characterized using the covariance function. Instead, they are represented by either the variogram or the semivariogram. These functions are similar to the covariance function but are more appropriate for use with increment stationary processes because they are defined directly on the increments. For a process satisfying (8.1.2), the _semivariogram_ is defined as

\[\gamma(u,w) \equiv \frac{1}{2}\mathrm{E}[y(u)-y(w)]^{2}\] \[= \frac{1}{2}\operatorname{Var}[y(u)-y(w)]\] \[= \frac{1}{2}\bigl{\{}\operatorname{Var}\left[y(u)\right]+ \operatorname{Var}\left[y(w)\right]-2\mathrm{Cov}[y(w),y(u)]\bigr{\}}\] \[= \frac{1}{2}\left[\sigma(u,u)+\sigma(w,w)-2\sigma(w,u)\right].\]The _variogram_ is twice the semivariogram. Clearly, the two functions contain similar information. The variogram has a more natural definition but, as will be seen later, for second-order stationary processes the semivariogram has advantages. Our discussion will use the semivariogram exclusively.

For an increment stationary process, \(\gamma(u,w)=\gamma(u+h,w+h)\) for any \(h\). Letting \(h=-w\),

\[\gamma(u,w)=\gamma(u-w,0),\]

and we write

\[\gamma(u,w)=\gamma(u-w)\,. \tag{8.1.4}\]

A process is said to be _intrinsically stationary_ if it satisfies both (8.1.4) and (8.1.2). Intrinsic stationary processes need not be increment stationary, but increment stationary processes are intrinsically stationary. The relationship between increment stationarity and intrinsic stationarity is similar to the relationship between stationarity and second-order stationarity. Just as stationary processes are increment stationary, second-order stationary processes are intrinsically stationary. To whit, with second-order stationarity

\[\gamma(u,w) = \frac{1}{2}\left[\sigma(u,u)+\sigma(w,w)-2\sigma(w,u)\right].\] \[= \frac{1}{2}\big{[}\sigma(0)+\sigma(0)-2\sigma(w-u)\big{]}\] \[= \sigma(0)-\sigma(w-u)\] \[= \sigma(0)-\sigma(u-w)\,.\]

This is a function of \(u-w\), so second-order stationary processes are also intrinsically stationary. In particular, if \(h=u-w\),

\[\gamma(h)=\sigma(0)-\sigma(h)\,. \tag{8.1.5}\]

If variances do not exist, a process can be increment stationary without being intrinsically stationary.

It has been argued that methods based on the semivariogram are preferable to methods based on the covariance function because the semivariogram can exist in cases where the covariance function does not (e.g., processes with infinite variances). I have never heard of a measuring device that allows infinitely large observations, so I cannot imagine a need for modeling data with infinite variances. A more interesting rationale for preferring models based on intrinsic stationarity, rather than second-order stationarity, is that second-order stationarity implies a constant variance for all observations, whereas intrinsic stationarity allows different variances. However, the real goal is prediction and, as we will see in Sect. 8.3, predictions based on intrinsic stationarity are identical to predictions based on second-order stationarity.

Another generalization of stationarity is contained in the ideas of _generalized covariance functions_ and _intrinsic random functions of order \(k\)_. Just as an intrinsically stationary process with its stationary semivariogram can be used to model a process with a nonstationary covariance function, an intrinsic random function of order \(k\) with its stationary generalized covariance function can be used to model a process with a nonstationary covariance function. Knowledge of the generalized covariance function does not completely specify the covariance structure of a process, but it can be used to obtain best linear unbiased predictions for universal kriging models. The equivalence of predictions based on the covariance function and the generalized covariance function can be established by modifying the results given in Sect. 8.3; see Christensen (1990, 1993). Generalized covariance functions and intrinsic random functions were originally introduced by Matheron (1973) to avoid problems encountered with using residuals to estimate second-order properties in universal kriging. For data analysis, the fundamental idea is the same as in REML estimation, see Sect. 4.3, but the methods are not necessarily based on likelihood analysis. Delfiner (1976) gives an introduction to these topics; they will not be discussed further.

Often the second-order properties of a process can be assumed to depend only on the distance between two points and not on the direction between them. A second-order stationary process is _isotropic_ if

\[\sigma(u-w)=\sigma(\|u-w\|)\,.\]

An intrinsically stationary process is isotropic if

\[\gamma(u-w)=\gamma(\|u-w\|)\,.\]

A process that is not isotropic is said to be _anisotropic_.

Finally, a characterization of processes that leads to computational advantages is separability. Let \(h^{\prime}=(h_{1},\ldots,h_{d})\). A second-order stationary process is said to be _separable_ if its covariance function can be written as

\[\sigma(h)=\prod_{i=1}^{d}\sigma_{i}(h_{i})\]

for some one-dimensional covariance functions \(\sigma_{1}(\cdot),\ldots,\sigma_{d}(\cdot)\). A similar property for the semivariogram defines separability for intrinsically stationary processes. See Zimmerman (1989) and Zimmerman and Harville (1990) for applications of separability. More generally, if we partition \(h\) as \(h^{\prime}=(\tilde{h}_{1},\ldots,\tilde{h}_{r})\), we can define separability as

\[\sigma(h)=\prod_{k=1}^{r}\tilde{\sigma}_{k}(\tilde{h}_{k}).\]

For example, if time is used as a dimension, the time covariance may be multiplied by the spatial covariance to get the spatio-temporal covariance. In three dimensions characterized by longitude, latitude, and depth, sometimes depth is taken as a separable covariance term.

In this subsection, concepts of stationarity have been discussed for an arbitrary process \(y(u)\) for which variances exist. In the remainder of the chapter, these variants of stationarity will be incorporated into diverse models for the error process \(e(u)\). Inthe universal kriging model, \(\mathrm{E}[y(u)]\) is not constant, so the process \(y(u)\) fails to satisfy any of the definitions related to stationarity. On the other hand, the error process has a constant mean of zero, so it may be modeled with some sort of stationarity assumption. As will be seen in the next two sections, the actual process of best linear unbiased prediction does not require any version of stationarity. Nonetheless, modeling the covariance structure is vital to the analysis of spatial data, and stationarity assumptions are an important aspect of covariance modeling.

It seems to have been the case that most people either put a lot of effort into modeling \(m(\cdot)\) or \(\sigma(\cdot,\cdot)\) but not both. People who did ordinary kriging seemed to put a lot of work into modeling the covariance (or semivariogram) function. People who did sophisticated modeling of the mean function seemed to worry less about the covariance function they used.

### Best Linear Unbiased Prediction of Spatial Data: Kriging

The purpose of this section is to illustrate the relationship between kriging and best linear unbiased prediction. The object of kriging is the prediction of unobserved spatial random variables based on the values of observed random variables. There is little to do except establish that we are working with a linear model. Given that fact, the BLUP is well-known; see Sect. 4.1. Kriging is often presented as either point kriging or block kriging. We give a detailed discussion of point kriging and mention the variations needed for block kriging.

Assume that the universal kriging model

\[m(u)=\sum_{j=1}^{p}\beta_{j}x_{j}(u)\]

holds, that observations have been taken at locations \(u_{1},\ldots,u_{n}\), and that we wish to predict the value of \(y(u_{0})\). Set the following notation:

\[y_{i} = y(u_{i})\,,\quad i=0,\ldots,n\,,\] \[Y = (y_{1},\ldots,y_{n})^{\prime}\,,\] \[x_{ij} = x_{j}(u_{i})\,,\quad i=0,\ldots,n\,,\] \[x_{i}^{\prime} = (x_{i1},\ldots,x_{ip})\,,\quad i=0,\ldots,n\,,\] \[X = \begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix},\] \[\beta = (\beta_{1},\ldots,\beta_{p})^{\prime}\,,\] \[e_{i} = e(u_{i})\,,\quad i=0,\ldots,n\,,\] \[e = (e_{1},\ldots,e_{n})^{\prime}\,,\]\[\sigma_{ij}=\sigma(u_{i},u_{j})\,,\quad i,j=0,\ldots,n\,.\]

The universal kriging model, as applied to the observations, can be written as

\[Y=X\beta+e,\quad\mathrm{E}(e)=0,\quad\mathrm{Cov}(e)=\Sigma, \tag{8.2.1}\]

where

\[\Sigma=[\sigma_{ij}]\,,\quad i,j=1,\ldots,n\,.\]

Generally, \(\Sigma\) is nonsingular and the functions \(x_{j}(u)\) and locations are taken so that \(X\) has full column rank with \(J\in C(X)\). Under the full column rank assumption, \(\beta\) is estimable, so, for any location \(u_{0}\), \(x_{0}^{\prime}\beta\) is estimable. Let

\[\Sigma_{Y0}\equiv\begin{bmatrix}\sigma_{10}\\ \vdots\\ \sigma_{n0}\end{bmatrix}\,.\]

Applying the results of Sect. 4.1, the best linear unbiased predictor of \(y_{0}\) is

\[\hat{y}_{0}=x_{0}^{\prime}\hat{\beta}+\delta^{\prime}(Y-X\hat{\beta}), \tag{8.2.2}\]

where

\[\hat{\beta}=(X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}Y\]

and

\[\delta=\Sigma^{-1}\Sigma_{Y0}\,.\]

As in Sect. 4.1, we can write

\[\hat{y}_{0}=b^{\prime}Y,\]

where

\[b^{\prime}=x_{0}^{\prime}(X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}+ \delta^{\prime}\left(I-X(X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1} \right)\,. \tag{8.2.3}\]

The mean squared prediction error (prediction variance) is

\[\mathrm{Var}(y_{0}-\hat{y}_{0}) = \sigma_{00}-\Sigma_{Y0}^{\prime}\Sigma^{-1}\Sigma_{Y0}+[x_{0}-X^ {\prime}\delta]^{\prime}(X^{\prime}\Sigma^{-1}X)^{-1}[x_{0}-X^{\prime}\delta] \tag{8.2.4}\] \[= \sigma_{00}-2b^{\prime}\Sigma_{Y0}+b^{\prime}\Sigma b\,.\]

Often, the object of kriging is to produce a two- or three-dimensional map of the variable \(y(u)\) over the region \(D\). To do this, a large number of predictions for various locations \(u_{0}\) are needed. Rewrite (2) as

\[\hat{y}_{0}=x_{0}^{\prime}\hat{\beta}+\Sigma_{Y0}^{\prime}\Sigma^{-1}(Y-X\hat {\beta})\,.\]

Given \(\hat{\beta}\) and \(\Sigma^{-1}(Y-X\hat{\beta})\), computation of different predictions is very inexpensive. It requires only the computation of inner products between \(x_{0}\) and \(\hat{\beta}\) and between \(\Sigma_{Y0}\) and \(\Sigma^{-1}(Y-X\hat{\beta})\) for the various values of \(x_{0}\) and \(\Sigma_{Y0}\).

Kitanidis (1986) gives a Bayesian analysis of the universal kriging model. In particular, he relates Bayes point predictions to the best linear unbiased predictors and the variance of the predictive distribution to the prediction variance. Cressie (1986, 1993) presents prediction methods based on exploratory data analysis. Computations are discussed in the more modern books mentioned earlier.

#### Block Kriging

So far we have assumed that our observations have been taken at point locations \(u_{1},\ldots,u_{n}\). In fact, it is physically impossible to take observations at points. An observation taken at \(u_{i}\) must actually be an observation on some neighborhood of \(u_{i}\). If the neighborhood is small, the approximation to point observations should be good. However, in many applications (particularly mining), the neighborhood is sufficiently large that the properties of the neighborhood need to be incorporated into the analysis. In the geostatistical literature, the neighborhoods are referred to as blocks. The theory presented earlier carries through with almost no change. We need only to redefine the terms of the linear model.

Let \(B_{i}\) be the block associated with the \(i\)th observation. Let \(|B_{i}|\) be the volume (area) of the block. The observations and value to be predicted are now

\[y_{i}=\frac{1}{|B_{i}|}\int_{B_{i}}y(u)du\]

for \(i=0,1,\ldots,n\). The elements of the model matrix \(X\) are

\[x_{ij}=\frac{1}{|B_{i}|}\int_{B_{i}}x_{j}(u)du\,.\]

Note that under the universal kriging model

\[\mathrm{E}(y_{i}) = \frac{1}{|B_{i}|}\int_{B_{i}}\sum_{j=1}^{p}\beta_{j}x_{j}(u)du\] \[= \sum_{j=1}^{p}\beta_{j}\left\{\frac{1}{|B_{i}|}\int_{B_{i}}x_{j}( u)du\right\}\] \[= \sum_{j=1}^{p}\beta_{j}x_{ij}\,.\]

Thus, the \(y_{i}\)s follow a linear model.

Note that \(y_{i}-\mathrm{E}(y_{i})=(1/|B_{i}|)\int_{B_{i}}e(u)du\), so the covariance of two observations is

\[\mathrm{Cov}(y_{i},y_{j})=\mathrm{E}\left\{\frac{1}{|B_{i}|}\frac{1}{|B_{j}|} \int_{B_{i}}e(u)du\int_{B_{j}}e(v)dv\right\}\]\[= \frac{1}{|B_{i}|}\frac{1}{|B_{j}|}\int_{B_{i}}\int_{B_{j}}\operatorname{ E}[e(u)e(v)]dudv\] \[= \frac{1}{|B_{i}|}\frac{1}{|B_{j}|}\int_{B_{i}}\int_{B_{j}}\sigma(u, v)dudv\,.\]

Given this linear model with known covariance structure, the BLUP can be obtained as usual.

#### Gaussian Process Regression

If one chooses to think of \(u\in\mathbf{R}^{d}\) as a vector of predictor variables in a regression, rather than as a vector of locations, the ideas discussed in this chapter are sometimes referred to as _Gaussian process regression_. Many discussions of Gaussian process regression restrict themselves to the ordinary kriging model. In the machine learning community the most common reference seems to be Rasmussen and Williams (2006).

### Prediction Based on the Semivariogram: Geostatistical Kriging

In much of the geostatistics literature, kriging is presented as a procedure that uses the semivariogram of the error process to determine optimal predictions. In general, given the semivariogram, one cannot reproduce the covariance function, so it is not clear that kriging can be performed using only the semivariogram. A special case in which the semivariogram and the covariance function are equivalent is that of a second-order stationary process with

\[\lim_{\|u\|\to\infty}\sigma(u)=0\,. \tag{8.3.1}\]

In this case, by (8.1.5), \(\gamma(h)=\sigma(0)-\sigma(h)\), so

\[\lim_{\|u\|\to\infty}\gamma(u)=\sigma(0)\]

and

\[\sigma(h)=\lim_{\|u\|\to\infty}\gamma(u)-\gamma(h)\,.\]

But generally, for intrinsically stationary processes and for second-order stationary processes that do not have property (8.3.1), we cannot expect to reproduce the covariance function from the semivariogram.

In this section, we give a condition under which the BLUP can be found as a function of the semivariogram. Interestingly, the condition does not involve the vari ability of the error process; the condition is essentially that the linear model (8.2.1) contains an intercept and that for predicting \(y(u_{0})\), the mean \(m(u_{0})=x_{0}^{\prime}\beta\) also contains an intercept. In practice, this condition can be specified as

\[x_{1}(u)=1\qquad\text{all }u\,.\]

More generally, we can assume that

\[J_{n+1}\in C\left(\begin{bmatrix}X\\ x_{0}^{\prime}\end{bmatrix}\right),\]

or equivalently that, for some vector \(d\),

\[J=Xd\]

and

\[1=x_{0}^{\prime}d\,.\]

Our proof is based on the result of _PA-V_ Exercise 6.8c (Christensen 2011, Exercise 12.1c) that if \(b^{\prime}Y\) is the BLUP, there exists a vector, that we will call \(\lambda\), such that

\[\begin{bmatrix}\Sigma&X\\ X^{\prime}&0\end{bmatrix}\begin{bmatrix}b\\ \lambda\end{bmatrix}=\begin{bmatrix}\Sigma_{Y0}\\ x_{0}\end{bmatrix}. \tag{8.3.2}\]

There are two simple ways to prove this. First, \(b^{\prime}\) as given in (8.2.3) can be substituted into (8.3.2) and a vector \(\lambda\) can be identified that satisfies the equality. Alternatively, if the inverses exist, one can establish that

\[\begin{bmatrix}\Sigma&X\\ X^{\prime}&0\end{bmatrix}^{-1}=\begin{bmatrix}\Sigma^{-1}(I-A)&\Sigma^{-1}X(X ^{\prime}\Sigma^{-1}X)^{-1}\\ (X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}&-(X^{\prime}\Sigma^{-1}X)^{ -1}\end{bmatrix},\]

where \(A=X(X^{\prime}\Sigma^{-1}X)^{-1}X^{\prime}\Sigma^{-1}\); see _PA_ Exercise B.21. The solution of (8.3.2) is

\[\begin{bmatrix}b\\ \lambda\end{bmatrix}=\begin{bmatrix}\Sigma&X\\ X^{\prime}&0\end{bmatrix}^{-1}\begin{bmatrix}\Sigma_{Y0}\\ x_{0}\end{bmatrix},\]

and substituting for the inverse matrix it is easily seen that \(b\) is the same as (8.2.3). In any case, if (8.3.2) has a unique solution, \(b\) must be as in (8.2.3).

Having established (8.3.2), we show that \(b\) satisfies a similar equation that involves only the semivariogram. If this equation has a unique solution, the solution must determine the BLUP.

In particular, define semivariogram matrices similar to \(\Sigma\) and \(\Sigma_{Y0}\), say

\[\Gamma=[\gamma(u_{i},u_{j})]_{n\times n}\]

and

\[\Gamma_{Y0}=[\gamma(u_{i},u_{0})]_{n\times 1}\,.\]The equation analogous to (8.3.2) is

\[\left[\begin{array}{cc}-\Gamma&X\\ X^{\prime}&0\end{array}\right]\left[\begin{array}{c}b\\ \xi\end{array}\right]=\left[\begin{array}{c}-\varGamma_{Y0}\\ x_{0}\end{array}\right]. \tag{8.3.3}\]

We now establish the equivalence of Eqs. (8.3.2) and (8.3.3).

**Proposition 8.3.1**.: _Suppose there exists a vector \(d\) such that \(J=Xd\) and \(1=x_{0}^{\prime}d\). Any solution \([b^{\prime},\xi^{\prime}]^{\prime}\) to (8.3.3) determines a solution \([b^{\prime},\lambda^{\prime}]^{\prime}\) to (8.3.2) and conversely._

Proof.: Let \(\sigma_{y}^{\prime}=[\sigma_{11},\ldots,\sigma_{nn}]\). It is easily seen that

\[\Sigma=\frac{1}{2}[\sigma_{y}J^{\prime}+J\sigma_{y}^{\prime}]-\Gamma \tag{8.3.4}\]

and

\[\Sigma_{Y0}=\frac{1}{2}[\sigma_{y}+\sigma_{00}J]-\varGamma_{Y0}\,. \tag{8.3.5}\]

Both Eqs. (8.3.2) and (8.3.3) imply that \(X^{\prime}b=x_{0}\), so a solution to either equation satisfies

\[J^{\prime}b=d^{\prime}X^{\prime}b=d^{\prime}x_{0}=1.\]

Suppose \([b^{\prime},\xi^{\prime}]^{\prime}\) is a solution to (8.3.3). We need to show that this determines a solution \([b^{\prime},\lambda^{\prime}]^{\prime}\) for (8.3.2). From Eq. (8.3.3)

\[-\varGamma b+X\xi=-\varGamma_{Y0}\,.\]

Substituting for \(-\varGamma\) and \(-\varGamma_{Y0}\) from (8.3.4) and (8.3.5) gives

\[\Sigma b-\frac{1}{2}\sigma_{y}J^{\prime}b-\frac{1}{2}J\sigma_{y}^{\prime}b+X\xi =\Sigma_{Y0}-\frac{1}{2}\sigma_{y}-\frac{1}{2}\sigma_{00}J\,.\]

Using the fact that \(J^{\prime}b=1\) and rearranging terms gives

\[\Sigma b-\frac{1}{2}J\sigma_{y}^{\prime}b+X\xi+\frac{1}{2}\sigma_{00}J=\Sigma_ {Y0}\,.\]

However, \(J=Xd\), so

\[\Sigma b+X\left(-\frac{1}{2}d\sigma_{y}^{\prime}b+\xi+\frac{1}{2}\sigma_{00}d \right)=\Sigma_{Y0}\,.\]

Pick \(\lambda=-\frac{1}{2}d\sigma_{y}^{\prime}b+\xi+\frac{1}{2}\sigma_{00}d\), and \([b^{\prime},\lambda^{\prime}]^{\prime}\) is a solution to (8.3.2).

To show that any solution \([b^{\prime},\lambda^{\prime}]^{\prime}\) for (8.3.2) yields a solution to (8.3.3), use a similar argument. In particular, use (8.3.4) and (8.3.5) to substitute for \(\Sigma\) and \(\Sigma_{Y0}\) in (8.3.2).

Based on Proposition 8.3.1, if (8.3.2) and (8.3.3) have unique solutions, then a solution to (8.3.3) determines a solution to (8.3.2), which determines the BLUP. The traditional method for finding the BLUP in geostatistics is to find a solution to (8.3.3). We have established mild conditions for the validity of that approach.

The prediction variance can be written in terms of the semivariogram. By (8.2.4), the fact that \(J^{\prime}b=1\), and Eqs. (8.3.4) and (8.3.5),

\[\mathrm{E}(y_{0}-\hat{y}_{0})^{2} = \sigma_{00}-2b^{\prime}\Sigma_{Y0}+b^{\prime}\Sigma b \tag{8.3.6}\] \[= \sigma_{00}-2b^{\prime}\left\{\frac{1}{2}\sigma_{y}+\frac{1}{2} \sigma_{00}J-\Gamma_{Y0}\right\}+b^{\prime}\left\{\frac{1}{2}\sigma_{y}J^{ \prime}+\frac{1}{2}J\sigma_{y}^{\prime}-\Gamma\right\}b\] \[= \sigma_{00}-b^{\prime}\sigma_{y}-\sigma_{00}b^{\prime}J+2b^{ \prime}\Gamma_{Y0}+\frac{1}{2}b^{\prime}\sigma_{y}J^{\prime}b+\frac{1}{2}b^{ \prime}J\sigma_{y}^{\prime}b-b^{\prime}\Gamma b\] \[= 2b^{\prime}\Gamma_{Y0}-b^{\prime}\Gamma b\,.\]

### Measurement Error and the Nugget Effect

One aspect of best linear (unbiased) prediction that is unappealing for some purposes is the fact, illustrated in _PA-V_ Exercise 6.8b (Christensen 2011, Exercise 12.1b), that the predictor of a point that has been observed is just the point itself (i.e., \(\hat{y}_{i}=y_{i}\)). This phenomenon occurs because in predicting \(y_{0}\equiv y_{i}\),

\[\Sigma_{Y0}=\begin{bmatrix}\sigma_{1i}\\ \vdots\\ \sigma_{ni}\end{bmatrix},\]

which is just the \(i\)th column of \(\Sigma\). Clearly, a solution to \(\Sigma\delta=\Sigma_{Y0}\) is given by the vector \(\delta=(0,\ldots,0,1,0,\ldots,0)^{\prime}\), where the \(1\) is in the \(i\)th place, i.e., \(\delta\) is the \(i\)th column of \(I_{n}\). Thus,

\[\hat{y}_{i} = x_{i}^{\prime}\hat{\beta}+\delta^{\prime}(Y-X\hat{\beta})\] \[= x_{i}^{\prime}\hat{\beta}+(y_{i}-x_{i}^{\prime}\hat{\beta})\] \[= y_{i}\,.\]

Given our current model, this is only reasonable. We have available only one realization of the stochastic process, so there is only one value observable at \(u_{i}\) and we have observed it. If we wish to predict at \(u_{i}\), the predictor must be \(y_{i}\).

For many purposes, more smoothing is desired in the predictor. This can be accomplished by imagining that subsequent measurements taken at \(u_{i}\) could be different from our original observation \(y_{i}\). For this to happen, our observations must be subject to measurement error. Measurement errors are generally modeled as being uncorrelated with constant variance (white noise). In particular, we assume that observations follow the model \[y(u)=m(u)+e(u)+e_{M}(u),\]

where all terms are defined as in Sect. 8.1 except that \(e_{M}(u)\) is a second-order stationary measurement error process, namely

\[\eqalign{{\rm E}[e_{M}(u)]&=0,\cr{\rm Var}[e_{M}(u)]&=\sigma_{M}^{2},\cr{\rm Cov }[e_{M}(u),e_{M}(w)]&=0\qquad u\neq w,}\]

and

\[{\rm Cov}[e_{M}(u),e(w)]=0\qquad\hbox{any $u,w$}.\]

Define covariance functions \(\sigma_{e}(u,w)\) for \(e(u)\), \(\sigma_{M}(u,w)\) for \(e_{M}(u)\) and \(\sigma_{\epsilon}(u,w)\) for \(\epsilon(u)\equiv e(u)+e_{M}(u)\). Note that \(\sigma_{\epsilon}(u,w)=\sigma_{e}(u,w)+\sigma_{M}(u,w)\) and \(\sigma_{M}(u,w)=\sigma_{M}^{2}\delta_{u,w}\), where \(\delta_{u,w}\) is the Kronecker delta: 1 if \(u=w\) and 0 otherwise.

Letting \(e_{M}=\left(e_{M}(u_{1}),\ldots,e_{M}(u_{n})\right)^{\prime}\), the spatial linear model is

\[Y=X\beta+\epsilon,\]

where \(\epsilon=e+e_{M}\), \({\rm E}(\epsilon)=0\), and writing \({\rm Cov}(\epsilon)=V\) gives

\[V=\Sigma+\sigma_{M}^{2}I\,.\]

With measurement error, the covariance matrix of \(Y\) is now \(V\) but the covariance between \(Y\) and a future observation at \(u_{i}\) is still \(\Sigma_{Y0}\). The covariance is based entirely on \(e(u)\). The measurement error process \(e_{M}(u)\) contributes nothing. The prediction is

\[\hat{y}_{i}=x_{i}^{\prime}\hat{\beta}+\Sigma_{Y0}^{\prime}V^{-1}(Y-X\hat{\beta }),\]

where

\[\hat{\beta}=(X^{\prime}V^{-1}X)^{-1}X^{\prime}V^{-1}Y\,.\]

Typically, the prediction does not simplify to \(y_{i}\).

A well-known concept in geostatistics is that of the _nugget effect_. The terminology comes from mining in that a spatial process might not be smooth because one finds nuggets of gold etc. A nugget effect is said to occur for an intrinsically stationary process if

\[\lim_{\|h\|\to 0}\gamma(h)=\gamma_{0}>0\,.\]

Note that by definition, \(\gamma(0)=0\) (see Sect. 8.1). The existence of a nugget effect is in some sense equivalent to measurement error.

First, we show that adding measurement error to any intrinsically stationary process creates a nugget effect. Assume an arbitrary measurement error model. The total error process is

\[\epsilon(u)=e(u)+e_{M}(u)\,.\]

The measurement error process \(e_{M}(u)\) is second-order stationary by definition and it is easy to see that its semivariogram is \(\gamma_{M}(h)=\sigma_{M}^{2}(1-\delta_{h,0})\). Assume that \(e(u)\) is intrinsically stationary. Because the measurement error is assumed to be uncorrelated with \(e(u)\), it is not hard to show that \(\varepsilon(u)\) must also be intrinsically stationary with

\[\gamma_{e}(h)=\gamma_{e}(h)+\gamma_{M}(h).\]

If \(\lim_{\|h\|\to 0}\gamma_{e}(h)\) exists, it must be nonnegative, and

\[\lim_{\|h\|\to 0}\gamma_{e}(h)=\lim_{\|h\|\to 0}\gamma_{e}(h)+\lim_{\|h\|\to 0}\gamma_{M}(h)=\lim_{\|h\|\to 0}\gamma_{e}(h)+\sigma_{M}^{2}>0\,.\]

Conversely, if \(\varepsilon(u)\) is an intrinsically stationary error process with a nugget effect, then there exists an intrinsically stationary error process \(e(u)\) and an uncorrelated measurement error process \(e_{M}(u)\) such that the process \(\eta(u)=e(u)+e_{M}(u)\) has the same semivariogram as \(\varepsilon(u)\). Take \(\lim_{\|h\|\to 0}\gamma_{e}(h)\equiv\gamma_{0}\) and simply define \(e(u)\) to be a second-order stationary process with

\[\sigma_{e}(u,w)=\left\{\begin{array}{ll}\sigma_{\varepsilon}(u,w)&u\neq w\\ \sigma_{\varepsilon}(u,u)-\gamma_{0}&u=w\end{array}\right.\]

and \(e_{M}(h)\) to be the measurement error process with

\[\sigma_{M}(h)=\left\{\begin{array}{ll}0&h\neq 0\\ \gamma_{0}&h=0\end{array}\right..\]

Because the measurement error process is taken to be uncorrelated with \(e(h)\) and both are intrinsically stationary, \(\eta(u)\) is intrinsically stationary with

\[\gamma_{\eta}(h)=\gamma_{e}(h)+\gamma_{M}(h)=\gamma_{e}(h).\]

What remains to be shown is that \(\sigma_{e}(u,w)\) is a valid covariance function. To do that, one needs to show that for any \(n\) and locations \(u_{i}\), the matrix \(\Sigma_{e}\equiv[\sigma_{e}(u_{i},u_{j})]\) is nonnegative definite.

For prediction, the difference between measurement error and a nugget effect without measurement error exists only in \(\Sigma_{Y0}\) and then only when \(y_{0}=y_{i}\) for some \(i=1,\ldots,n\). With measurement error, a new observation at location \(u_{i}\) can be different from \(y_{i}\). The covariance between \(Y\) and a new observation at \(u_{i}\) is

\[\Sigma_{YiM}=\left[\begin{array}{c}\sigma_{e}(u_{1},u_{i})\\ \vdots\\ \sigma_{e}(u_{n},u_{i})\end{array}\right].\]

With a pure nugget effect, the only observation has already been taken, so

\[\Sigma_{YiN}=\left[\begin{array}{c}\sigma_{e}(u_{1},u_{i})\\ \vdots\\ \sigma_{e}(u_{n},u_{i})\end{array}\right]=\Sigma_{YiM}+\sigma_{M}^{2}\delta,\]where \(\delta^{\prime}=(0,\ldots,0,1,0,\ldots,0)\) with the 1 in the \(i\)th place. For predictions not at a data point, either model gives

\[\Sigma_{Y0}=\begin{bmatrix}\sigma_{e}(u_{1},u_{0})\\ \vdots\\ \sigma_{e}(u_{n},u_{0})\end{bmatrix}.\]

If \(\sigma_{e}(u,w)\) is continuous, as \(u_{0}\to u_{i}\), the measurement error model gives continuous predictions because \(\Sigma_{Y0}\to\Sigma_{YiM}\). In the pure nugget effect model, \(\Sigma_{Y0}\) does not converge to \(\Sigma_{YiM}\), so the predictions are discontinuous at the data points. The discontinuities exist so that data points will be predicted as themselves.

With only one observation at each location, the effects of measurement error and the nugget effect are statistically indistinguishable. Nonetheless, measurement error and the nugget effect are distinct concepts. The idea of a nugget effect is that the spatial process is fundamentally discontinuous. (One stumbles upon nuggets of gold, but gold is not continuously spread around the surface of the earth.) There is nothing one can do about such discontinuity. The corresponding spatial predictions are smooth everywhere except at the observed data locations, where the predicted value is the actual observation. In a sense, having a nugget effect simply means that the spatial correlation is weaker because no matter how close two locations are, the covariance between them remains smaller than the spatial variance, so there is no assurance that the observations will be close to each other. On the other hand, the effects of measurement error can be reduced and modeled by repeated sampling. As we take more observations at a given location, the variability should be entirely due to measurement error. We can then estimate both the mean value at that location, which should be the realization of the spatial process, and the variance at that location, which is the measurement error variance.

### The Effect of Estimated Covariances on Prediction

As the universal kriging model simply defines a spatial linear model, all of the results of Chap. 4 continue to apply. In particular, the covariance matrix \(\Sigma\) is rarely known, so must be estimated. Thus, under conditions presented in Sect. 4.7, point estimates of spatial mean values and point predictions at individual locations are unbiased and naive estimates of their variability often seriously underestimate the true variability. This is an excellent motivation for performing Bayesian analysis, which gives similar estimates and predictions without the tendency to underestimate variability.

Since predictions based on the semivariogram are identical to predictions based on the covariance function, such predictions also share the properties discussed in Chap. 4 including the tendency towards naively underestimating prediction error.

Cressie (1988) observes that to ensure consistency of estimators in ordinary kriging one must assume not only that the process is strictly stationary but also _ergodic_;see Adler (1981). Recall that strict stationarity (and the existence of second moments) implies second-order, increment, and intrinsic stationarities. None of these three imply stationarity. For Gaussian processes, ergodicity is implied by

\[\lim_{\|h\|\to\infty}\sigma(h)=0\,.\]

Because ordinary kriging is a special case of universal kriging, even stronger assumptions may be necessary to ensure consistency in the more general model. In particular, some assumptions about the asymptotic behavior of the model matrix \(X\) are probably needed.

It should be emphasized that we are not discussing consistency of predictors. Even if the joint distributions (first and second moments) were known, the best predictor (best linear predictor) would not give perfect predictions. Best linear unbiased estimates depend on estimating the mean function. Consistency is concerned with the estimated mean converging to the true mean so that the BLUP converges to the BLP. When the covariances are also estimated, we obtain only an estimated BLUP. In this case again, consistency refers to the estimated BLUP converging to the BLP. Stein (1988, 1999) discusses asymptotically efficient prediction. Diamond and Armstrong (1983) indicate that prediction is reasonably robust to the choice of different covariance functions. See also Zimmerman and Cressie (1992).

### Models for Covariance Functions and Semivariograms

In practice, the covariance function \(\sigma(u,w)\) is rarely known. To obtain an estimate of the covariance matrix of \(Y\) (i.e., \(\Sigma\)), some method of estimating \(\sigma(u,w)\) is needed. Recalling that \(\Sigma\) is an \(n\times n\) matrix with \(n(n+1)/2\) distinct elements, there is little hope of estimating \(\Sigma\) from the \(n\) observations in \(Y\) without making assumptions about \(\Sigma\) or, equivalently, assumptions about \(\sigma(\cdot,\cdot)\). In particular, we assume that the covariance function depends on a vector of parameters \(\theta\). Write the covariance function as

\[\sigma(u,w;\theta),\]

which is a known function for given \(\theta\). We can also write

\[\mbox{Cov}(Y)=\Sigma(\theta)=[\sigma(u_{i},u_{j};\theta)]\]

and

\[\mbox{Cov}(Y,y_{0})=\Sigma_{Y0}(\theta)=[\sigma(u_{i},u_{0};\theta)]\,.\]

Alternatively, we can assume that the semivariogram depends on a vector of parameters \(\theta\) and write

\[\gamma(u,w;\theta)\,,\]

\[\Gamma(\theta)=[\gamma(u_{i},u_{j};\theta)]\,,\]and

\[\varGamma_{Y0}(\theta)=\left[\gamma(u_{i},u_{0};\theta)\right].\]

In this section, we consider some of the standard models for \(\sigma(u,w;\theta)\) and \(\gamma(u,w;\theta)\). In Sect. 8.8, methods of estimating \(\theta\) are discussed. Section 11.6 discusses nonparameteric covariance functions for functional data. They can also be applied to spatial data.

#### The Linear Covariance Model

This model assumes that \(\sigma(u,w;\theta)\) is linear in the components of \(\theta\). In particular, write \(\theta=(\theta_{1},\ldots,\theta_{s})^{\prime}\) and, for \(k=1,\ldots,s\), let \(\sigma_{k}(u,w)\) be a known covariance function. The linear covariance model is

\[\sigma(u,w;\theta)=\sum_{k=1}^{s}\theta_{k}\sigma_{k}(u,w)\,.\]

To ensure that \(\sigma(u,w;\theta)\) is a legitimate covariance function, we assume that \(\theta_{k}\geq 0\) for all \(k\). Writing the \(n\times n\) matrix

\[\varSigma_{k}=\left[\sigma_{k}(u_{i},u_{j})\right],\]

we have

\[\varSigma(\theta)=\sum_{k=1}^{s}\theta_{k}\varSigma_{k}\,.\]

Because each \(\sigma_{k}\) is a covariance function, the matrices \(\varSigma_{k}\) are nonnegative definite. Frequently one takes, \(\varSigma_{1}=I_{n}\). Clearly, this is the linear covariance structure of Sect. 4.4 with \(V_{0}=0\) and \(V_{k}=\varSigma_{k}\).

The linear semivariogram model is defined similarly,

\[\gamma(u,w;\theta)=\sum_{k=0}^{r}\theta_{k}\gamma_{k}(u,w),\]

for known semivariograms \(\gamma_{k}(\cdot,\cdot)\).

A commonly used linear semivariogram is the isotropic function

\[\gamma(\|h\|\,;\theta)=\theta_{0}+\theta_{1}\|h\|\,, \tag{8.6.1}\]

where \(\theta_{0}\) and \(\theta_{1}\) are nonnegative and \(\theta_{0}=\sigma_{M}^{2}\). In fact, this is often referred to as _the_ linear semivariogram model. This semivariogram cannot correspond to a second-order stationary process because

\[\lim_{\|h\|\to\infty}\{\theta_{0}+\theta_{1}\|h\|\}=\infty\,.\]Recall that for a second-order stationary process \(\gamma(\left\|h\right\|)=\sigma(0)-\sigma(\left\|h\right\|)\) and, by Cauchy-Schwartz, \(\left|\sigma(\left\|h\right\|)\right|\leq\sigma(0)\); thus, the variance would have to be infinite. Brownian motion is a process in \(\mathbf{R}\) that has a linear semivariogram. The linear semivariogram model would seem to be most appropriate for data that have a logical origin (e.g., data collected around a smelter) and that resemble a random walk in that the variability increases as one gets further from the origin.

There is a temptation to modify (8.6.1) so that

\[\lim_{\left\|h\right\|\rightarrow\infty}\gamma(\left\|h\right\|;\theta)\neq \infty.\]

The idea is that the linear semivariogram may be a reasonable approximation up to a point but that the variability of real data would not go on increasing indefinitely. It is sometimes suggested that the function

\[g(\left\|h\right\|;\theta)=\left\{\begin{array}{ll}\theta_{0}+\theta_{1} \left\|h\right\|&\left\|h\right\|\leq\theta_{2}\\ \theta_{0}+\theta_{1}\theta_{2}&\left\|h\right\|>\theta_{2}\end{array}\right.\]

could be used. This would correspond to a second-order stationary process with measurement error \(\sigma_{M}^{2}=\theta_{0}\) and a variance for each observation of \(K=\theta_{0}+\theta_{1}\theta_{2}\). Unfortunately, the corresponding "covariance" function

\[s(\left\|h\right\|;\theta)=K-g(\left\|h\right\|;\theta)\]

is not a legitimate covariance function. It is not nonnegative definite. One can find locations \(u_{1},\ldots,u_{k}\) such that the \(k\times k\) matrix \(\left[s(\left\|u_{i}-u_{j}\right\|;\theta)\right]\) is not nonnegative definite. Moreover, \(g(\cdot;\theta)\) does not satisfy a property similar to nonnegative definiteness that is necessary for all semivariograms; see Journel and Huijbregts (1978).

#### Nonlinear Isotropic Covariance Models

We now present some of the standard isotropic covariance models that are nonlinear. In the next subsection, some methods of dealing with nonisotropic (anisotropic) covariances will be considered.

The _spherical covariance function_ is

\[\sigma(\left\|h\right\|;\theta)=\left\{\begin{array}{ll}\theta_{1}\left[1- \frac{3\left\|h\right\|}{2\theta_{2}}+\frac{\left\|h\right\|^{3}}{2\theta_{2}^ {3}}\right]&0<\left\|h\right\|\leq\theta_{2}\\ \theta_{0}+\theta_{1}&\left\|h\right\|=0\\ 0&\left\|h\right\|>\theta_{2}\end{array}\right.\]

for \(\theta_{0}\), \(\theta_{1}\), \(\theta_{2}\) nonnegative. This covariance function arises naturally on \(\mathbf{R}^{3}\) (see Matern 1986, Section 3.2) and also defines a covariance function on \(\mathbf{R}^{2}\). The measurement error variance is \(\sigma_{M}^{2}=\theta_{0}\). The total variance is \(\theta_{0}+\theta_{1}\). The _range_ of a covariance function is the distance after which observations become uncorrelated.

For the spherical model, observations more than \(\theta_{2}\) units apart are uncorrelated, so the range is \(\theta_{2}\).

Another class of covariance functions is

\[\sigma(\|h\|\,;\theta)=\left\{\begin{array}{ll}\theta_{1}\exp[-(\theta_{2}\|h\| )^{\nu}]&\|h\|>0\\ \theta_{0}+\theta_{1}&\|h\|=0\end{array}\right.\]

for \(\theta_{0}\), \(\theta_{1}\), \(\theta_{2}\) nonnegative and \(0<\nu\leq 2\). The measurement error variance is \(\theta_{0}\), the total variance is \(\theta_{0}+\theta_{1}\), and the range is infinite. For \(\nu=1\), this is called the _exponential covariance function_, and for \(\nu=2\) it is called the _Gaussian covariance function_. While the range is infinite, correlations decrease very rapidly as \(\|h\|\) increases. Of course, this phenomenon depends on the values of \(\nu\) and \(\theta_{2}\). For \(\nu=1\), the covariance structure is very similar to an _AR(1)_ process, see Exercise 8.2. Using this covariance function for \(\nu<2\), the spatial process \(e(u)\) is continuous but not differentiable. For \(\nu=2\), \(e(u)\) is infinitely differentiable. Moreover, with \(\nu=2\) and \(\theta_{0}=0\), the covariance matrix \(\Sigma\) is often nearly singular, hence making the resulting analysis quite unreliable. The use of \(\nu=2\) seems to be almost universally frowned upon.

Whittle (1954) has shown that a covariance function that depends on \(K_{1}(\cdot)\), the first-order modified Bessel function of the second kind, arises naturally in \({\bf R}^{2}\). In particular, for \(\theta_{1},\theta_{2}>0\), the function is

\[\sigma(\|h\|\,;\theta)=\theta_{1}\|h\|\theta_{2}K_{1}(\|h\|\theta_{2})\,.\]

This can be modified by adding a measurement error of variance \(\theta_{0}\) when \(\|h\|=0\). Whittle (1963) considers more general functions

\[\sigma(\|h\|\,;\theta)=[\theta_{1}/2^{\nu-1}\Gamma(\nu)](\theta_{2}\|h\|)^{\nu }K_{\nu}(\theta_{2}\|h\|),\]

where \(\nu>0\) and \(K_{\nu}(\cdot)\) is the \(\nu\) order modified Bessel function of the second kind. Ripley (1981, p. 56) gives some graphs of these functions and mentions that, for \(\nu=1/2\), one gets the exponential model (without measurement error). Also, as \(\nu\rightarrow\infty\), it approaches the Gaussian model. This class of covariance functions is often called the Matern class and seems to be increasingly popular--in part because it makes \(e(u)\) finitely differentiable.

In both of the last two families, \(\nu\) can either be fixed or it can be treated as a parameter.

#### Modeling Anisotropic Covariance Functions

Anisotropic covariance functions are simply covariance functions that are not isotropic. We mention only two possible approaches to modeling such functions. Suppose that \(h=(h_{1},h_{2},h_{3})^{\prime}\) and that we suspect the variability in the direction \((0,0,1)^{\prime}\) is causing the anisotropicity. (Isn't anisotropicity a wonderful word?) For example, \(h_{1}\) and \(h_{2}\) could determine a surface location (e.g., longitude and latitude), while \(h_{3}\) determines depth. For fixed \(h_{3}\), variability might very well be isotropic in \(h_{1}\) and \(h_{2}\); however, the variability in depth may not behave like that in the other two directions.

Ripley (1981) suggests modifying isotropic models. Rather than using \(\sigma(\|h\|)\), where \(\|h\|=\sqrt{h_{1}^{2}+h_{2}^{2}+h_{3}^{2}}\), use \(\sigma(\sqrt{h_{1}^{2}+h_{2}^{2}+\lambda h_{3}^{2}})\), where \(\lambda\) is an additional parameter to be estimated. For example, the exponential model becomes

\[\sigma(h;\theta,\lambda)=\left\{\begin{array}{ll}\theta_{1}\exp\left(-\theta_ {2}\sqrt{h_{1}^{2}+h_{2}^{2}+\lambda h_{3}^{2}}\right)&\|h\|>0\\ \theta_{0}+\theta_{1}&\|h\|=0\end{array}\right..\]

This is a special case of the elliptical covariance functions discussed by Matern (1986). Elliptical covariance functions are isotropic functions \(\sigma(\cdot)\) evaluated at \(\sqrt{h^{\prime}Ah}\). Here, \(A\) can be taken as any nonnegative definite matrix and may involve additional parameters.

Huijbregts (1975) suggests adding different isotropic models, for example,

\[\sigma(h;\theta)=\sigma_{1}(\|h\|\,;\theta_{1})+\sigma_{2}(|h_{3}|\,;\theta_{2 })\,,\]

where \(\sigma_{1}(\cdot;\theta_{1})\) is an isotropic covariance on the entire vector \(h\) that depends on a parameter vector \(\theta_{1}\). Similarly, \(\sigma_{2}(\cdot;\theta_{2})\) is isotropic in the component \(h_{3}\) and depends on the parameter vector \(\theta_{2}\).

#### Nonlinear Semivariograms

In geostatistics, if the semivariogram \(\gamma(\cdot)\) has the property that

\[\lim_{\|h\|\to\infty}\gamma(h)=\gamma_{\infty}<\infty\,,\]

then \(\gamma_{\infty}\) is called the _sill_ of the semivariogram. Any semivariogram with a sill can be obtained from a second-order stationary process with the property that

\[\lim_{\|h\|\to\infty}\sigma(h)=0\,.\]

In particular, the stationary covariance function is

\[\sigma(0) = \gamma_{\infty}\] \[\sigma(h) = \gamma_{\infty}-\gamma(h)\,.\]

Conversely, any stationary covariance function with

\[\lim_{\|h\|\to\infty}\sigma(h)=0 \tag{8.6.2}\]determines a semivariogram with a sill. This follows from the fact that for second-order stationary processes

\[\gamma(h)=\sigma(0)-\sigma(h)\,. \tag{8.6.3}\]

All of the nonlinear covariance functions we have considered satisfy (8.6.2). It is a simple matter to convert them to semivariogram models using (8.6.3).

### Models for Spatial Lattice Data

When there exist only a finite (or countable) number of locations at which data would be collected, models for the covariances can be created to exploit that special structure. Such data are referred to as lattice data. When the locations occur only on a regular grid, they are referred to as regular lattice data. Throughout this section, we will assume normal errors, thus our model is

\[Y=X\beta+e\,,\quad e\sim N(0,\Sigma). \tag{8.7.1}\]

We consider three covariance models: spatial covariance selection models, spatial autoregression models, and spatial autoregressive moving average models. In all three cases, parameter estimation is through maximum likelihood. Predictions are made using empirical BLUPs.

#### Spatial Covariance Selection Models

_Covariance selection_ is a graphical modeling device for specifying conditional independence between normally distributed random variables. Whittaker (1990, Chapter 6) and Edwards (2000, Chapter 3) discuss covariance selection in the context of _graphical Gaussian models_. Christensen (1997, Chapter 5) discusses graphical models for count data. The key point is that, for normal data, 0s in \(\Sigma^{-1}\) determine conditional independencies among the random variables. (Graphical representations help one interpret the conditional independencies.) In particular, an off-diagonal element of \(\Sigma^{-1}\) is zero if and only if the partial covariance between the two corresponding random variables given all of the remaining random variables is zero. For multivariate normal data, the partial covariance is zero (i.e., \(\sigma_{ij\cdot k\neq i,j}=0\)), if and only if the random variables \(y_{i}\) and \(y_{j}\) are conditionally independent given all of the other random variables \(y_{k}\) with \(k\neq i,j\); see Whittaker (1990, Chapter 5), Edwards (2000, Section 3.1), or Exercise B.3 and _PA_ Sect. 6.5 and Exercise B.21.

The idea in spatial covariance selection is to model \(\Sigma^{-1}\) directly by incorporating nonzero off-diagonal parameters only for locations that are close to one another. Often, the model is parameterized as \[\Sigma^{-1}=\frac{1}{\sigma^{2}}(I-C),\]

where \(\sigma^{2}\) is a scalar parameter and the parameter matrix \(C=[c_{ij}]\) has \(c_{ii}\equiv 0\). \(C\) must be a symmetric matrix such that \(I-C\) is positive definite. Typically, \(c_{ij}\equiv 0\) unless the locations \(u_{i}\) and \(u_{j}\) are close to one another.

Example 8.7.1. : For a regular lattice, we reindex the locations \(u_{i}\), \(i=1,\ldots,n\) into \(u_{g,h}\), \(g=1,\ldots,G\), \(h=1,\ldots,H\) with the idea that all the locations fall on a regular grid. We can then posit models for \(C\) such as \(0=c_{(g,h),(g^{\prime},h^{\prime})}\) except

\[\theta_{0,1} = c_{(g,h),(g,h-1)}=c_{(g,h),(g,h+1)},\] \[\theta_{1,0} = c_{(g,h),(g-1,h)}=c_{(g,h),(g+1,h)},\] \[\theta_{1,1} = c_{(g,h),(g-1,h-1)}=c_{(g,h),(g+1,h-1)}\] \[= c_{(g,h),(g-1,h+1)}=c_{(g,h),(g+1,h+1)}.\]

Using a general result from graphical models, this particular model can be interpreted as saying that an observation \(y_{g,h}\) is independent of all the other data given the values of the eight observations that immediately surround it. (This includes two horizontal neighbors, two vertical neighbors, and four diagonal neighbors.) The model reduces the covariance matrix to a four-parameter family (including \(\sigma^{2}\)). The \(\theta\) parameters model the spatial correlation. Note that some care must be taken about how to model covariances at the edges of the lattice. 

For nonregular lattices, one approach is to impose a regular lattice over the map of the nonregular lattice and then associate each point in the nonregular lattice with the closest point in the regular lattice.

See Cressie (1993, Section 7.2) for details of computing maximum likelihood estimates for covariance selection models.

#### Spatial Autoregression Models

One approach to producing a spatial autoregression is to model the error vector in (8.7.1). Let \(\xi\sim N(0,\sigma^{2}I)\) and for a parameter matrix \(\Phi\), define \(e\) in (8.7.1) through

\[(I-\Phi)e=\xi.\]

This is similar in spirit to the time series \(AR(p)\) model (7.2.2).

Example 8.7.2. : For a regular lattice, we could posit a model

\[e_{g,h}=\phi_{0,1}e_{g,h+1}+\phi_{0,-1}e_{g,h-1}+\phi_{1,0}e_{g+1,h}+\phi_{-1, 0}e_{g-1,h}+\xi_{g,h}\,,\]

or equivalently 

[MISSING_PAGE_FAIL:361]

Computations for the maximum likelihood and REML estimates seem to be complicated.

Covariance selection and spatial autoregression models are both naturally specified using the inverse covariance matrix. In Chap. 4 the likelihood and REML equations were developed for a parameterization like \(\Sigma(\theta)\), and used \(\mathbf{d}_{\theta_{j}}\Sigma(\theta)\). However, the use of an inverse covariance parameterization was also discussed and equations were given.

#### Exercise 8.2.

Show that the covariance function of an \(AR(1)\) time series process is a special case of the exponential covariance function on \(\mathbf{R}^{1}\).

### Estimation of Covariance Functions and Semivariograms

The spatial linear model

\[Y=X\beta+e,\quad\text{E}(e)=0,\quad\text{Cov}(e)=\Sigma(\theta), \tag{8.8.1}\]

lends itself immediately to the methods discussed in Chap. 4 for estimation of the covariance parameters. In this model any measurement error has been built into \(\Sigma(\theta)\). Unlike the mixed models of Chap. 5, there is very little simplification to be obtained from the specific models considered for spatial data except that the linear covariance model has the linear covariance structure of Sect. 4.4.

Assuming normality and writing \(\theta=(\theta_{1},\ldots,\theta_{s})^{\prime}\), the likelihood equations are

\[\operatorname{tr}\left\{\Sigma^{-1}(\theta)[\mathbf{d}_{\theta_{ j}}\Sigma(\theta)]\right\} =[Y-X\hat{\beta}(\theta)]^{\prime}\Sigma^{-1}(\theta)[\mathbf{d}_ {\theta_{j}}\Sigma(\theta)]\Sigma^{-1}(\theta)[Y-X\hat{\beta}(\theta)]\] \[=Y^{\prime}[I-A(\theta)]^{\prime}\Sigma^{-1}(\theta)[\mathbf{d}_ {\theta_{j}}\Sigma(\theta)]\Sigma^{-1}(\theta)[I-A(\theta)]Y\]

for \(j=1,\ldots,s\). The REML equations are

\[\operatorname{tr}\left\{\Sigma^{-1}(\theta)[I-A(\theta)][\mathbf{ d}_{\theta_{j}}\Sigma(\theta)]\right\}\\ =Y^{\prime}[I-A(\theta)]^{\prime}\Sigma^{-1}(\theta)[\mathbf{d}_{ \theta_{j}}\Sigma(\theta)]\Sigma^{-1}(\theta)[I-A(\theta)]Y,\]

\(j=1,\ldots,s\),

Based on Eq. (8.3.4), when \(J\in C(X)\) and \(C(B)=C(X)^{\perp}\), for any semivariogram model we have

\[B^{\prime}\Sigma(\theta)B=B^{\prime}\Gamma(\theta)B.\]

It is clear from their derivations that estimates of \(\theta\) from REML or, when appropriate, MINQUE are the same when based on either a covariance function \(\sigma(u,w;\theta)\) or a corresponding semivariogram \(\gamma(u,w;\theta)\). To apply the REML or MINQUE equations, replace all occurrences of \(\Sigma^{-1}(\theta)[I-A(\theta)]\) with \(B[B^{\prime}\Gamma(\theta)B]^{-1}B^{\prime}\) where \(B\) is chosen as a full rank matrix. Maximum likelihood is ill-defined based on the semi-variogram alone.

**Exercise 8.3**.: Using Eq. (8.3.4), show that if \(J\in C(X)\), then

\[\operatorname{tr}\left\{\Sigma^{-1}(\theta)[I-A(\theta)][\mathbf{d}_{\theta_{j}} \Sigma(\theta)]\right\}=\operatorname{tr}\left\{\Sigma^{-1}(\theta)[I-A( \theta)][\mathbf{d}_{\theta_{j}}\Gamma(\theta)]\right\}\]

and

\[[I-A(\theta)]^{\prime}\Sigma^{-1}(\theta)[\mathbf{d}_{\theta_{j}} \Sigma(\theta)] \Sigma^{-1}(\theta)[I-A(\theta)]\] \[=[I-A(\theta)]^{\prime}\Sigma^{-1}(\theta)[\mathbf{d}_{\theta_{j }}\Gamma(\theta)]\Sigma^{-1}(\theta)[I-A(\theta)].\]

Unfortunately, these facts cannot be used directly to solve the REML or MINQUE equations because \(\Gamma(\theta)\) does not determine either \(\Sigma^{-1}(\theta)\) or \(A(\theta)\), even though \(\Gamma(\theta)\) and \(B\) determine \([I-A(\theta)]^{\prime}\Sigma^{-1}(\theta)=\Sigma^{-1}(\theta)[I-A(\theta)]\).

The geostatistics literature also includes various ad hoc model fitting procedures; see Cressie (1985). These are usually based on empirical estimates \(\tilde{\sigma}(u_{i},u_{j})\) of the elements of \(\Sigma(\theta)\) combined with an ad hoc method of choosing \(\hat{\theta}\) so that the values \(\sigma(u_{i},u_{j};\hat{\theta})\) are in some sense close to the values \(\tilde{\sigma}(u_{i},u_{j})\). These are addressed briefly in the last subsection.

Of course, the observed data are \(Y\), so estimation must be based on \(Y\) and the model for \(Y\). It is a subtle point, but worth mentioning, that we must estimate the parameter \(\theta\) in \(\Sigma(\theta)\) rather than the parameter \(\theta\) in \(\sigma(u,w;\theta)\). Of course, given an estimate \(\tilde{\theta}\), not only does \(\Sigma(\tilde{\theta})\) estimate the covariance matrix but also \(\sigma(u,w;\tilde{\theta})\) estimates the covariance function. Nonetheless, our observations only give direct information about \(\Sigma(\theta)\).

#### Nonlinear Covariance Functions

For nonlinear covariance functions, Chap. 4 applies but there are not many simplifications available. The matrix

\[\mathbf{d}_{\theta_{i}}\Sigma(\theta)=[\mathbf{d}_{\theta_{i}}\sigma(u_{j},u_{ k};\theta)]\]

depends on the particular covariance model being used. For example, assuming the isotropic exponential model without measurement error (\(\theta_{0}=0\)) gives

\[\sigma(u,w;\theta)=\theta_{1}e^{-\theta_{2}\|u-w\|}.\]

Differentiation yields

\[\mathbf{d}_{\theta_{1}}\sigma(u,w;\theta)=e^{-\theta_{2}\|u-w\|},\]thus defining \({\bf d}_{\theta_{1}}\Sigma(\theta)\). Also,

\[{\bf d}_{\theta_{2}}\sigma(u,w;\theta)=-\theta_{1}\|u-w\|e^{-\theta_{2}\|u-w\|},\]

which defines \({\bf d}_{\theta_{2}}\Sigma(\theta)\).

The standard covariance models are discontinuous at \(\|h\|=0\) when measurement error occurs. This might give one doubts about whether the methods for obtaining MLEs and REML estimates can be executed. There is no problem. Derivatives are taken with respect to the \(\theta_{i}\)'s, and all of the standard models are continuous in \(\theta\). If measurement error is incorporated, rewrite the model as

\[Y=X\beta+e,\quad{\rm E}(e)=0,\quad{\rm Cov}(e)=V(\theta),\]

\[V(\theta)=\theta_{0}I_{n}+\Sigma(\theta_{s})\]

where \(\theta^{\prime}=(\theta_{0},\theta_{*}^{\prime})=(\theta_{0},\theta_{1}, \ldots,\theta_{s})^{\prime}\). Now \([{\bf d}_{\theta_{j}}V(\theta)]=[{\bf d}_{\theta_{j}}\Sigma(\theta)]\), \(j=1,\ldots,s\) but there is one more equation to consider, one that involves \([{\bf d}_{\theta_{0}}V(\theta)]=I_{n}\). The derivatives of \(\Sigma(\theta_{*})\) remain the same as without measurement error.

The maximum likelihood approach for spatial data was apparently first proposed by Kitanidis (1983) for linear covariance functions. Mardia and Marshal (1984) independently proposed using MLEs for general covariance models. Kitanidis and Lane (1985) also extended Kitanidis (1983) to general covariance functions. All of these articles discuss computational procedures. In their article on analyzing field-plot experiments, Zimmerman and Harville (1990) present a nice general discussion of maximum likelihood and residual maximum likelihood methods. They also point out that results in Zimmerman (1989) can be used to reduce the computational burden when many of the standard covariance models are used. Warnes and Ripley (1987) have pointed out that the likelihood function is often multimodal and that care must be taken to obtain the global rather than some local maximum of the likelihood function; see also Mardia and Watkins (1989). (Multimodality of the [restricted] likelihood is another good reason for doing a Bayesian analysis. A Bayesian analysis explores the likelihood rather than focusing on its maximum.) As always, high correlations between the parameter estimates can cause instability in the estimates.

#### Linear Covariance Functions

With

\[\Sigma(\theta)=\sum_{k=1}^{s}\theta_{k}\Sigma_{k}\]

the likelihood equations become

\[\sum_{k=1}^{s}\theta_{k}{\rm tr}\big{[}\Sigma_{k}\Sigma^{-1}(\theta)\Sigma_{j} \Sigma^{-1}(\theta)\big{]}=Y^{\prime}[I-A(\theta)]^{\prime}\Sigma^{-1}(\theta) \Sigma_{j}\Sigma^{-1}(\theta)[I-A(\theta)]Y\]\(j=1,\ldots,s\). The REML equations become

\[\sum_{k=1}^{s}\theta_{k}\text{tr}\big{\{}\Sigma_{k}[I-A(\theta)]^{ \prime}\Sigma^{-1}(\theta)\Sigma_{j}\Sigma^{-1}(\theta)[I-A(\theta)]\big{\}}\\ =Y^{\prime}[I-A(\theta)]^{\prime}\Sigma^{-1}(\theta)\Sigma_{j} \Sigma^{-1}(\theta)[I-A(\theta)]Y\]

\(j=1,\ldots,s\). The MINQUE equations are

\[\sum_{k=1}^{s}\theta_{k}\text{tr}\big{\{}\Sigma_{k}[I-A(w)]^{ \prime}\Sigma^{-1}(w)\Sigma_{j}\Sigma^{-1}(w)[I-A(w)]\big{\}}\\ =Y^{\prime}[I-A(w)]^{\prime}\Sigma^{-1}(w)\Sigma_{j}\Sigma^{-1}( w)[I-A(w)]Y\]

\(j=1,\ldots,s\).

Because the \(\Sigma_{k}\)s are all nonnegative definite, they can be written as \(\Sigma_{k}=Z_{k}Z_{k}^{\prime}\) for some matrix \(Z_{k}\) and as a result, the spatial model can be rewritten as a variance component model. The theory for linear covariance structures of Sect. 4.4 preempts the value of this observation. The equivalence of the linear covariance function model and variance component model was apparently first recognized by Kitanidis (1983, 1985). Marshall and Mardia (1985) also proposed MINQUE estimation. Stein (1987) gives asymptotic efficiency and consistency results for MINQUE estimates.

#### Traditional Geostatistical Estimation

The traditional approach to covariance function or semivariogram estimation (see Journel & Huijbregts 1978 or David 1977) is to obtain an "empirical" estimate and to fit a model to the empirical estimate. We concentrate on fitting covariance functions and discuss fitting semivariograms at the end of the subsection. We begin by discussing empirical estimation. In order to have enough data to perform estimation, we assume second-order stationarity (i.e., \(\sigma(u,w)=\sigma(u-w)\)). The empirical estimate is nonparametric in the sense that estimates are not based on any covariance model with a small number of parameters. To begin the procedure, choose a nonnegative definite weighting matrix, say \(\Sigma_{0}\), and fit the model

\[Y=X\beta+e,\quad\text{E}(e)=0,\quad\text{Cov}(e)=\Sigma_{0}\]

to obtain residuals

\[\hat{e}_{0}=(I-A_{0})Y=Y-X\hat{\beta}_{0},\]

where

\[A_{0}=X(X^{\prime}\Sigma_{0}^{-1}X)^{-}X^{\prime}\Sigma_{0}^{-1}\]

and

\[X\hat{\beta}_{0}=A_{0}Y\,.\]These residuals are the basis of empirical estimates of the covariance function. For any vector \(h\), there is a finite number \(N_{h}\) of pairs of observations \(y_{i}\) and \(y_{j}\) for which \(u_{i}-u_{j}=h\). For each of these pairs, list the corresponding residual pairs, say \((\hat{e}_{0i},\hat{e}_{0i(h)})\), \(i=1,\ldots,N_{h}\). If \(N_{h}\geq 1\), the traditional empirical estimator is

\[\hat{\sigma}(h)=\hat{\sigma}(-h)=\frac{1}{N_{h}}\sum_{i=1}^{N_{h}}\hat{e}_{i} \hat{e}_{i(h)}\,.\]

If \(N_{h}\) is zero, no empirical estimate is possible because no data have been collected with \(u_{i}-u_{j}=h\). Often, any vector \(u_{i}-u_{j}\) in a neighborhood of \(h\) is included in the computation. With a finite number of observations, there will be only a finite number of vectors, say \(h(1),h(2),\ldots,h(q)\), that have \(N_{h(k)}>0\). In practice, if \(N_{h(k)}\) is not substantially greater than 1, we may not wish to include \(h(k)\) as a vector for which the covariance function will be estimated.

Given a parametric stationary covariance function, say \(\sigma(h;\theta)\), a least squares estimate of \(\theta\) can be obtained by minimizing

\[\sum_{i=1}^{q}\left\{\hat{\sigma}[h(i)]-\sigma[h(i);\theta]\right\}^{2}\,.\]

Weighted least squares estimates can also be computed. If the covariances or asymptotic covariances of \(\hat{\sigma}\left[h(i)\right]\) and \(\hat{\sigma}\left[h(j)\right]\) can be computed, say \(\mbox{Cov}\left\{\hat{\sigma}[h(i)],\hat{\sigma}[h(j)]\right\}=v_{ij}\), write

\[V=[v_{ij}]\]

and choose \(\theta\) to minimize

\[S^{\prime}V^{-1}S,\]

where \(S^{\prime}=\left\{\hat{\sigma}[h(1)]-\sigma[h(1);\theta],\ldots,\hat{\sigma}[h (q)]-\sigma[h(q);\theta]\right\}\). In many cases, the covariances will be small relative to the variances, so a reasonable estimate can be obtained by minimizing

\[\sum_{i=1}^{q}\left\{\hat{\sigma}[h(i)]-\sigma[h(i);\theta]\right\}^{2}/v_{ii}\,.\]

The Gauss-Newton procedure described in Sect. 7.4.1 can be used to find the estimate, say \(\hat{\theta}_{0}\), using any of these criteria.

The best fit of the linear model and hence the best residuals is obtained by taking \(\Sigma_{0}=\Sigma\). Because \(\Sigma\) is unknown, it is reasonable to use \(\hat{\theta}_{0}\) to estimate it. Let

\[\Sigma_{1}=[\sigma(u_{i},u_{j};\hat{\theta}_{0})],\]

and find residuals

\[\hat{e}_{1}=(I-A_{1})Y,\]where

\[A_{1}=X(X^{\prime}\Sigma_{1}^{-1}X)^{-}X^{\prime}\Sigma_{1}^{-1}\,.\]

These residuals lead to pairs \((\hat{e}_{1i},\hat{e}_{1i(h)})\), \(i=1,\ldots,N_{h}\) and estimates \(\hat{\sigma}_{1}(h)=(1/N_{h})\sum_{i=1}^{N_{h}}\hat{e}_{1i}\hat{e}_{1i(h)}\). The estimates can then be used to obtain \(\hat{\theta}_{1}\) and define

\[\hat{\Sigma}_{2}=[\sigma(u_{i},u_{j};\hat{\theta}_{1})]\,.\]

This procedure can be iterated in the hope that the sequence \(\hat{\theta}_{t}\) converges to some value \(\hat{\theta}\). Armstrong (1984) presents criticisms of this method.

The idea of using weighted least squares as a criterion for fitting semivariograms was first presented by Cressie (1985). The preceeding presentation is a covariance function version of Cressie's ideas. Cressie's discussion was restricted to the ordinary kriging model. For this model, he computed the necessary variances and covariances. Cressie also suggested using robust empirical semivariogram estimates, in particular those proposed by Hawkins and Cressie (1984). Again, Cressie computed the variances and covariances necessary for weighted least squares. The traditional empirical semivariogram estimator in ordinary kriging is

\[\hat{\gamma}(h)=\frac{1}{2N_{h}}\sum_{i=1}^{N_{h}}(y_{i}-y_{i(h)})^{2},\]

where the pairs \((y_{i},y_{i(h)})\) are the \(N_{h}\) pairs whose locations differ by \(h\). The robust estimates from Hawkins and Cressie (1984) are

\[2\tilde{\gamma}(h)=\left[\frac{1}{N_{h}}\sum_{i=1}^{N_{h}}\left|y_{i}-y_{i(h)} \right|^{1/2}\right]^{4}\bigg{/}[0.457+0.494/N_{h}]\]

and

\[2\tilde{\gamma}(h)=\left[\text{median}\{\left|y_{i}-y_{i(h)}\right|^{1/2}\} \right]^{4}\bigg{/}B_{h},\]

where \(B_{h}\) is a bias correction factor.

Methods other than least squares and weighted least squares are often used to fit covariance functions \(\sigma(h;\theta)\) and semivariograms \(\gamma(h;\theta)\) to their empirical counterparts. Various methods have been devised for particular covariance and semivariogram models. Models have also frequently been fit by visual inspection.

If an isotropic covariance function or semivariogram is assumed, the empirical estimates change slightly. For covariance functions,

\[\hat{\sigma}(\|h\|)=\frac{1}{N_{h}}\sum_{i=1}^{N_{h}}\hat{e}_{i}\hat{e}_{i(\| h\|)}\,,\]

where the pairs \((\hat{e}_{i},\hat{e}_{i(\|h\|)})\), \(i=1,\ldots,N_{h}\) are all residual pairs with locations separated by the distance \(\|h\|\). For the semivariogram in ordinary kriging,\[2\hat{\gamma}(\|h\|)=\frac{1}{N_{h}}\sum_{i=1}^{N_{h}}[y_{i}-y_{i(\|h\|)}]^{2}.\]

The pairs \((y_{i},y_{i(\|h\|)})\), \(i=1,\ldots,N_{h}\) consist of all observations with locations separated by \(\|h\|\).

Zimmerman and Zimmerman (1991) present results from a Monte Carlo experiment comparing various techniques of estimating the variogram in ordinary kriging. Cressie (1989) gives a very complete illustration of traditional methods for semivariogram estimation in ordinary kriging.

## Bibliography

* Adler (1981) Adler, R. J. (1981). _The geometry of random fields_. New York: Wiley.
* Armstrong (1984) Armstrong, M. (1984). Problems with universal kriging. _Journal of the International Association for Mathematical Geology, 16_, 101-108.
* Banerjee et al. (2015) Banerjee, S., Carlin, B. P., Gelfand, A. E. (2015). _Hierarchical modeling and analysis for spatial data_ (2nd ed.). Boca Raton, FL: Chapman & Hall/CRC.
* Berke (1998) Berke, O. (1998). On spatio-temporal prediction for on-line monitoring data. _Communications in Statistics, Series A, 27_, 2343-2369.
* Breiman (1968) Breiman, L. (1968). _Probability_. Reading, MA: Addison-Wesley.
* Christensen (1990) Christensen, R. (1990). The equivalence of predictions from universal kriging and intrinsic random function kriging. _Mathematical Geology, 22_, 655-664.
* Christensen (1993) Christensen, R. (1993). Quadratic covariance estimation and equivalence of predictions. _Mathematical Geology, 25_, 541-558.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York: Springer.
* Christensen et al. (2010) Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton, FL: Chapman and Hall/CRC Press.
* Cliff & Ord (1981) Cliff, A., & Ord, J. K. (1981). _Spatial processes: Models and applications_. London: Pion.
* Cressie (1985) Cressie, N. (1985). Fitting variogram models by weighted least squares. _Journal of the International Association for Mathematical Geology, 17_, 563-586.
* Cressie (1986) Cressie, N. (1986). Kriging nonstationary data. _Journal of the American Statistical Association, 81_, 625-634.
* Cressie (1988) Cressie, N. (1988). Spatial prediction and ordinary kriging. _Mathematical Geology, 20_, 405-421.
* Cressie (1989) Cressie, N. (1989). Geostatistics. _The American Statistician, 43_, 197-202.
* Cressie (1993) Cressie, N. A. C. (1993). _Statistics for spatial data_ (Revised ed.). New York: Wiley.
* Cressie & Wikle (2011) Cressie, N. A. C., & Wikle, C. K. (2011). _Statistics for spatio-temporal data_. New York: Wiley.

* David (1977) David, M. (1977). _Geostatistical ore reserve estimations_. New York: Elsevier.
* Delfiner (1976) Delfiner, P. (1976). Linear estimation of nonstationary spatial phenomena. In M. Guarascia, M. David, & C. Huijbregts (Eds.) _Advanced geostatistics in the mining industry_. Dordrecht: Reidel.
* Diamond & Armstrong (1983) Diamond, P., & Armstrong, M. (1983). Robustness of variograms and conditioning of kriging matrices. _Journal of the International Association for Mathematical Geology, 16_, 809-822.
* Diggle et al. (1998) Diggle, P. J., Tawn, J. A., & Moyeed, R. A. (1998). Model-based geostatistics. _Applied Statistics, 47_, 299-326.
* Edwards (2000) Edwards, D. (2000). _Introduction to graphical modeling_ (2nd ed.). Berlin: Springer.
* Gelfand et al. (2010) Gelfand, A. E., Diggle, P., Guttorp, P., Fuentes, M. (2010). _Handbook of spatial statistics_. Boca Raton, FL: Chapman & Hall/CRC.
* Goldberger (1962) Goldberger, A. S. (1962). Best linear unbiased prediction in the generalized linear regression model. _Journal of the American Statistical Association, 57_, 369-375.
* Handcock & Stein (1993) Handcock, M. S., & Stein, M. L. (1993). A Bayesian analysis of kriging. _Technometrics, 35_, 403-410.
* Handcock & Wallis (1994) Handcock, M. S., & Wallis, J. R. (1994). An approach to statistical spatial-temporal modeling of meterological fields (with discussion). _Journal of the American Statistical Association, 89_, 368-378.
* a proposal. _Journal of the International Association for Mathematical Geology, 16_, 3-18.
* Hodges (2014) Hodges, J. S. (2014). _Richly parameterized linear models: Additive, time series, and spatial models using random effects_. Boca Raton, FL: Chapman and Hall/CRC.
* Huang & Cressie (1996) Huang, H.-C., & Cressie, N. (1996). Spatio-temporal prediciton of snow water equivalent using the Kalman filter. _Computational Statistics and Data Analysis, 22_, 159-175.
* Huijbregts (1975) Huijbregts, C. J. (1975). Regionalized variables and quantitative analysis of spatial data. In J. C. Davis & M. J. McCullagh (Eds.), _Display and analysis of spatial data_. New York: Wiley.
* Isaaks & Srivastava (1989) Isaaks, E. H., & Srivastava, R. M. (1989). _An introduction to applied geostatistics_. Oxford: Oxford University Press.
* Journel & Huijbregts (1978) Journel, A. G., & Huijbregts, Ch. J. (1978). _Mining geostatistics_. New York: Academic Press.
* Kitanidis (1983) Kitanidis, P. K. (1983). Statistical estimation of polynomial generalized covariance functions and hydrologic applications. _Water Resources Research, 19_, 909-921.
* Kitanidis (1985) Kitanidis, P. K. (1985). Minimum-variance unbiased quadratic estimation of covariances of regionalized variables. _Journal of the International Association for Mathematical Geology, 17_, 195-208.
* Kitanidis (1986) Kitanidis, P. K. (1986). Parameter uncertainty in estimation of spatial functions: Bayesian analysis. _Water Resources Research, 22_, 499-507.
* Kitanidis (1997) Kitanidis, P. K. (1997). _Introduction to geostatistics: Applications to hydrogeology_. Cambridge: Cambridge University Press.
* Kitanidis & Lane (1985) Kitanidis, P. K., & Lane, R. W. (1985). Maximum likelihood parameter estimation of hydrologic spatial processes by the Gauss-Newton method. _Journal of Hydrology, 79_, 53-71.
* Kitanidis & Lane (1985)* (1998) Mardia, K. V., Goodall, C., Redfern, E. J., & Alonso, F. J. (1998). The kriged Kalman filter (with discussion). _Test, 7_, 217-252.
* (1989) Mardia, K. V., & Watkins, A. J. (1989). On multimodality of the likelihood in the spatial linear model. _Biometrika, 76_, 289-295.
* (1984) Mardia, K. V., & Marshal, R. J. (1984). Maximum likelihood estimation of models for residual covariance in spatial regression. _Biometrika, 71_, 135-146.
* (1985) Marshall, R. J., & Mardia, K. V. (1985). Minimum norm quadratic estimation of components of spatial covariance. _Journal of the International Association for Mathematical Geology, 17_, 517-525.
* (1986) Matern, B. (1986). _Spatial variation_ (2nd ed.). New York: Springer.
* (1965) Matheron, G. (1965). Les variable regionalisees et leur estimation: Masson, Paris, xxxp.
* (1969) Matheron, G. (1969). Le krigeage universal: Fascicule 1, Cahiers du CMMM., 82p.
* (1973) Matheron, G. (1973). The intrinsic random functions and their applications. _Advances in Applied Probability, 5_, 439-468.
* (1989) Omre, H., & Halvorsen, K. B. (1989). The Bayesian bridge between simple and universal kriging. _Mathematical Geology, 21_, 767-786.
* (2006) Rasmussen, C. E., & Williams, C. K. I. (2006). _Gaussian processes for machine learning_. Cambridge: MIT Press.
* (1981) Ripley, B. D. (1981). _Spatial statistics_. New York: Wiley.
* (2005) Schabenberger, O., & Gotway, C. A. (2005). _Statistical methods for spatial data analysis_. Boca Raton, FL: Chapman & Hall/CRC.
* (2011) Sherman, M. (2011). _Spatial statistics and spatio-temporal data: Covariance functions and directional properties_. New York: Wiley.
* (1987) Stein, M. L. (1987). Minimum norm quadratic estimation of spatial variograms. _Journal of the American Statistical Association, 82_, 765-772.
* (1988) Stein, M. L. (1988). Asymptotically efficient prediction of a random field with misspecified covariance function. _The Annals of Statistics, 16_, 55-64.
* (1999) Stein, M. L. (1999). _Interpolation of spatial data: Some theory for kriging_. New York: Springer.
* (2004) Waller, L. A., & Gotway, C. A. (2004). _Applied spatial statistics for public health data._ New York: Wiley.
* (1987) Warnes, J. J., & Ripley, B. D. (1987). Problems with likelihood estimation of covariance functions of spatial Gaussian processes. _Biometrika, 74_, 640-642.
* (1990) Whittaker, J. (1990). _Graphical models in applied multivariate statistics_. New York: Wiley.
* (1954) Whittle, P. (1954). On stationary processes in the plane. _Biometrika, 41_, 434-449.
* (1963) Whittle, P. (1963). Stochastic processes in several dimensions. _Bulletin of the International Statistical Institute, 40_(1), 974-994.
* (2019) Wikle, C. K., Zammit-Mangion, A., Cressie, N. (2019). _Spatio-temporal statistics with R_. Boca Raton: Chapman and Hall/CRC.
* (1989) Zimmerman, D. L. (1989). Computationally exploitable structure of covariance matrices and generalized covariance matrices in spatial models. _Journal of Statistical Computation and Simulation, 32_, 1-15.

* Zimmerman & Cressie (1992) Zimmerman, D. L., & Cressie, N. (1992). On the stability of the geostatistical method. _Mathematical Geology, 24_, 45-59.
* Zimmerman & Harville (1990) Zimmerman, D. L., & Harville, D. A. (1990). A random field approach to the analysis of field-plot experiments and other spatial experiments. _Biometrics, 47_, 223-239.
* Zimmerman & Zimmerman (1991) Zimmerman, D. L., & Zimmerman, M. B. (1991). A Monte Carlo comparison of spatial variogram estimators and kriging predictors. _Technometrics, 33_, 77-91.

## Chapter 9 Multivariate Linear Models: General

**Abstract** This chapter introduces the basic theory for linear models with more than one dependent variable.

Chapters 9 through 14 examine topics in multivariate analysis. Specifically, they discuss the theory of multivariate linear models, applications of multivariate linear models, generalized multivariate linear models and associated longitudinal models, discriminant analysis (with binary regression), and principal components (with factor analysis and classical multidimensional scaling). The basic ideas behind these subjects are closely related to linear model theory. A multivariate linear model is a collection of linear models with more than one dependent variable that share the same model matrix \(X\). A _standard multivariate linear model_ has identical covariances among dependent variables for every individual but with data from different individuals uncorrelated. Generalized multivariate linear models extend the standard multivariate linear model by positing a mean structure that relates the dependent variables. Discriminant analysis is closely related to both Mahalanobis distance (_PA-V_ Sect. 12.1 or Christensen 2011, Sect. 13.1) and multivariate one-way analysis of variance. Principal components, cf. _PA-V_ Sect. 14.2.1 (Christensen 2011, Subsection 15.2.1), are user-constructed variables that are best linear predictors of the original data. Factor analysis has ties to both multivariate linear models and principal components.

These six chapters are introductory in nature. The discussions benefit from the advantage of being based on linear model theory. They suffer from the disadvantage of being relatively brief. More detailed discussions are available in numerous other sources, such as Anderson (2003), Arnold (1981), Dillon and Goldstein (1984), Eaton (1983), Gnanadesikan (1977), Johnson and Wichern (2007), Mardia, Kent and Bibby (1979), Morrison (2004), Muirhead (1982), Press (1982), and Seber (1984). More recent contributions to this literature include Everitt and Hothorn (2011), Rencher and Christensen (2012), and Marden (2015).

As mentioned earlier, the distinction between standard multivariate linear models and standard (univariate) linear models is simply that multivariate linear models involve more than one dependent variable. For multivariate data, let the dependentvariables be \(y_{1},\ldots,y_{q}\). The idea is that all \(q\) random variables will be observed on each of \(n\) individuals. The standard assumption is that the random variables have some unknown covariance matrix \(\Sigma\) that is the same for all individuals but different individuals are uncorrelated. If \(n\) observations are taken on each dependent variable, we have \(y_{i1},\ldots,y_{iq}\), \(i=1,\ldots,n\). Let \(Y_{1}=[y_{11},\ldots,y_{n1}]^{\prime}\) and, in general, \(Y_{h}=[y_{1h},\ldots,y_{nh}]^{\prime}\), \(h=1,\ldots,q\). For each \(h\), the vector \(Y_{h}\) is the vector of \(n\) responses on the variable \(y_{h}\) and can be used as the response vector for a linear model. For \(h=1,\ldots,q\), write the linear model

\[Y_{h}=X\beta_{h}+e_{h},\quad\mbox{E}(e_{h})=0,\quad\mbox{Cov}(e_{h})=\sigma_{ hh}I, \tag{9.0.1}\]

where \(X\) is a known \(n\times p\) matrix that is the same for all dependent variables (it depends on the individuals but not on the variable being measured), but \(\beta_{h}\) and the error vector \(e_{h}=[e_{1h},\ldots,e_{nh}]^{\prime}\) are peculiar to the dependent variable. Here we are using \(\sigma_{hh}\) (rather than \(\sigma_{h}^{2}\)) to denote the variance associated with \(y_{h}\). The assumed covariance structure determines the covariances between the \(Y_{h}\) vectors, see Exercise 9.1.

The multivariate linear model consists of fitting the \(q\) linear models simultaneously. Write the matrices

\[Y_{n\times q}=[Y_{1},\ldots,Y_{q}],\quad B_{p\times q}=[\beta_{1},\ldots,\beta _{q}],\quad e_{n\times q}=[e_{1},\ldots,e_{q}].\]

The multivariate linear model is

\[Y=XB+e\,. \tag{9.0.2}\]

The key to the analysis of the standard multivariate linear model is the random nature of the \(n\times q\) error matrix \(e=[e_{ih}]\). At a minimum, we assume that \(\mbox{E}(e)=0\) and that different individuals \(i\) are uncorrelated,

\[\mbox{Cov}(e_{ih},e_{i^{\prime}h^{\prime}})=\left\{\begin{array}{ll}\sigma_{ hh^{\prime}}&\mbox{if $i=i^{\prime}$}\\ 0&\mbox{if $i\neq i^{\prime}$}\end{array}\right..\]

Let

\[\delta_{ii^{\prime}}=\left\{\begin{array}{ll}1&\mbox{if $i=i^{\prime}$}\\ 0&\mbox{if $i\neq i^{\prime}$}\end{array}\right.,\]

then the covariances can be written simply as

\[\mbox{Cov}(e_{ih},e_{i^{\prime}h^{\prime}})=\sigma_{hh^{\prime}}\delta_{ii^{ \prime}}\,.\]

To construct tests and confidence regions, we assume that the \(e_{ij}\)s have a multivariate normal distribution with the previously indicated mean and covariances. Note that this covariance structure implies that the error vector in model 9.0.1 has \(\mbox{Cov}(e_{h})=\sigma_{hh}I\), as indicated previously.

**Exercise 9.1**.: For any two columns of \(Y\), say \(Y_{r}\) and \(Y_{s}\), show that \(\mbox{Cov}(Y_{r},Y_{s})=\sigma_{rs}I\).

An alternative but equivalent way to state the standard multivariate linear model is by examining the rows of model (9.0.2). Write

\[Y=\begin{bmatrix}y_{1}^{\prime}\\ \vdots\\ y_{n}^{\prime}\end{bmatrix},\quad X=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix},\quad\text{and}\quad e=\begin{bmatrix}\varepsilon_{ 1}^{\prime}\\ \vdots\\ \varepsilon_{n}^{\prime}\end{bmatrix}.\]

The standard multivariate linear model is also

\[y_{i}^{\prime}=x_{i}^{\prime}B+\varepsilon_{i}^{\prime}, \tag{9.0.3}\]

\(i=1,\ldots,n\). The error vector \(\varepsilon_{i}\) has the properties

\[\text{E}\left(\varepsilon_{i}\right)=0,\qquad\text{Cov}\left(\varepsilon_{i} \right)=\Sigma_{q\times q}=\left[\sigma_{hh^{\prime}}\right],\]

and, for \(i\neq j\),

\[\text{Cov}(\varepsilon_{i},\varepsilon_{j})=0.\]

To construct tests and confidence regions, the vectors \(\varepsilon_{i}\) are assumed to have independent multivariate normal distributions.

To reiterate, the multivariate model (9.0.2) holds if and only if the \(q\) univariate models (9.0.1) hold simultaneously and the multivariate model holds if and only if the \(n\) models (9.0.3) hold simultaneously. All of these models have errors with mean zero and all models determine the same covariance structure. The unknown covariance parameters are the unique parameters in \(\Sigma\). We assume throughout that \(\Sigma\) is positive definite.

### The Univariate Model

The key to estimation in the multivariate linear model is rewriting the model as a univariate linear model. The multivariate model is

\[Y=XB+e\,\quad\text{E}(e)=0\,\quad\text{Cov}(e_{ih},e_{i^{\prime}h^{\prime}}) =\sigma_{hh^{\prime}}\delta_{ii^{\prime}}. \tag{9.1.1}\]

Observing that

\[XB=[X\beta_{1},\ldots,X\beta_{1}],\]

the multivariate model holds if and only if each of the \(q\) univariate linear models hold. To have all \(q\) models hold simultaneously is to have

\[\begin{bmatrix}Y_{1}\\ Y_{2}\\ \vdots\\ Y_{q}\end{bmatrix}=\begin{bmatrix}X\beta_{1}\\ X\beta_{2}\\ \vdots\\ X\beta_{q}\end{bmatrix}+\begin{bmatrix}e_{1}\\ e_{2}\\ \vdots\\ e_{q}\end{bmatrix}\]which in turn can be rewritten as a single (univariate) linear model

\[\begin{bmatrix}Y_{1}\\ Y_{2}\\ \vdots\\ Y_{q}\end{bmatrix}=\begin{bmatrix}X&0&\cdots&0\\ 0&X&&\vdots\\ \vdots&&\ddots&0\\ 0&0&&X\end{bmatrix}\begin{bmatrix}\beta_{1}\\ \beta_{2}\\ \vdots\\ \beta_{q}\end{bmatrix}+\begin{bmatrix}e_{1}\\ e_{2}\\ \vdots\\ e_{q}\end{bmatrix}, \tag{9.1.2}\]

where the error vector has mean zero and the covariance matrix is

\[\begin{bmatrix}\sigma_{11}I_{n}&\sigma_{12}I_{n}&\cdots&\sigma_{1q}I_{n}\\ \sigma_{12}I_{n}&\sigma_{22}I_{n}&\cdots&\sigma_{2q}I_{n}\\ \vdots&\vdots&\ddots&\vdots\\ \sigma_{1q}I_{n}&\sigma_{2q}I_{n}&\cdots&\sigma_{qq}I_{n}\end{bmatrix}. \tag{9.1.3}\]

The Vec operator stacks the columns of a matrix, it is clear that we have rewritten model (9.1.1) as

\[\text{Vec}(Y)=\text{Vec}(XB)+\text{Vec}(e)\]

and that model (9.1.2) goes a step further in writing \(\text{Vec}(XB)\) as the product of the model matrix in (9.1.2) times the parameter vector \(\text{Vec}(B)\). _Vec operators, Kronecker products and their algebra are reviewed in Appendix A.2_ The model matrix in (9.1.2) is the Kronecker product \([I_{q}\otimes X]\). Model (9.1.2) can now be rewritten as

\[\text{Vec}(Y)=[I_{q}\otimes X]\text{Vec}(B)+\text{Vec}(e)\,. \tag{9.1.4}\]

The first two moments of \(\text{Vec}(e)\) are

\[\text{E}[\text{Vec}(e)]=0\]

and, rewriting (9.1.3) as a Kronecker product,

\[\text{Cov}[\text{Vec}(e)]=[\Sigma\otimes I_{n}]\,. \tag{9.1.5}\]

## 0.2 BLUEs

For estimation, the nice thing about the standard multivariate linear model (9.1.1) is that least squares estimates are optimal. In particular, it will be shown that optimal estimation is based on

\[\hat{Y}=X\hat{B}=MY,\]

where \(M=X(X^{\prime}X)^{-}X^{\prime}\) is, as always, the perpendicular projection operator onto the column space of \(X\), \(C(X)\). This is a simple generalization of the univariate linear model results of \(PA\) Chap. 2. To show that least squares estimates are best linear unbiased estimates (BLUEs) in model (9.1.4) with the covariance in (9.1.5), apply \(PA\) Proposition 2.7.5 or \(PA\) Theorem 10.4.5, both of which state that for a univariatelinear model \(Y_{n\times 1}=X\beta+e\), \(\mathrm{E}(e)=0\), \(\mathrm{Cov}(e)=\sigma^{2}V\), least squares estimates are BLUEs if \(C(VX)\subset C(X)\).

The model matrix in (9.1.4) is \([I_{q}\otimes X]\). The covariance matrix is \([\Sigma\otimes I_{n}]\). We need to show that \(C\left([\Sigma\otimes I_{n}]\,[I_{q}\otimes X]\right)\subset C\left([I_{q} \otimes X]\right)\). In particular,

\[[\Sigma\otimes I_{n}][I_{q}\otimes X] = [\Sigma\otimes X]\] \[= \left[\begin{matrix}\sigma_{11}X&\cdots&\sigma_{1q}X\\ \vdots&&\vdots\\ \sigma_{1q}X&\cdots&\sigma_{qq}X\end{matrix}\right]\] \[= [I_{q}\otimes X][\Sigma\otimes I_{p}]\,.\]

Recalling that \(C(RS)\subset C(R)\) for any conformable matrices \(R\) and \(S\), it is clear that

\[C([\Sigma\otimes I_{n}][I_{q}\otimes X])=C([I_{q}\otimes X][\Sigma\otimes I_{p }])\subset C([I_{q}\otimes X]).\]

Applying \(PA\) Proposition 2.7.5 or \(PA\) Theorem 10.4.5 establishes that least squares estimates are best linear unbiased estimates for model (9.1.4) with the covariance in (9.1.5).

To find least squares estimates in model (9.1.4), we need the perpendicular projection operator onto \(C\left([I_{q}\otimes X]\right)\). The ppo is

\[\mathcal{M}=[I_{q}\otimes X]\left([I_{q}\otimes X]^{\prime}[I_{q}\otimes X] \right)^{-}[I_{q}\otimes X]^{\prime}.\]

Because \([A\otimes B]^{\prime}=[A^{\prime}\otimes B^{\prime}]\), we have

\[[I_{q}\otimes X]^{\prime}[I_{q}\otimes X] = [I_{q}\otimes X^{\prime}][I_{q}\otimes X]\] \[= [I_{q}\otimes X^{\prime}X]\,.\]

It is easily seen from the definition of a generalized inverse that

\[\left([I_{q}\otimes X^{\prime}X]\right)^{-}=[I_{q}\otimes(X^{\prime}X)^{-}]\,.\]

It follows that

\[\mathcal{M} = [I_{q}\otimes X][I_{q}\otimes(X^{\prime}X)^{-}][I_{q}\otimes X]^{\prime}\] \[= [I_{q}\otimes X(X^{\prime}X)^{-}X^{\prime}]\] \[= [I_{q}\otimes M]\,.\]

By \(PA\) Theorem 2.2.1, in a univariate linear model \(Y_{n\times 1}=X\beta+e\), least squares estimates \(\hat{\beta}\) satisfy \(X\hat{\beta}=MY_{n\times 1}\); thus, for the univariate linear model (9.1.4), least squares estimates of \(\mathrm{Vec}(B)\), say \(\mathrm{Vec}(\hat{B})\), satisfy

\[[I_{q}\otimes X]\mathrm{Vec}(\hat{B})=[I_{q}\otimes M]\mathrm{Vec}(Y);\]that is,

\[\begin{bmatrix}X\hat{\beta}_{1}\\ \vdots\\ X\hat{\beta}_{q}\end{bmatrix}=\begin{bmatrix}MY_{1}\\ \vdots\\ MY_{q}\end{bmatrix}.\]

In terms of the multivariate linear model (9.1.1), this is equivalent to

\[X\hat{\beta}=MY.\]

### Unbiased Estimation of \(\Sigma\)

The usual unbiased estimate of \(\Sigma\) depends only on the mean and covariance assumptions that we have made. The result looks like the result for a standard univariate linear model, indeed the standard univariate linear model is just the special case where \(q=1\). An unbiased estimate of \(\Sigma\) is

\[S\equiv\frac{1}{n-r(X)}\,Y^{\prime}(I-M)Y.\]

The proof that \(S\) is unbiased is similar in spirit to the proof of \(PA\) Theorem 1.3.2, a result that gives the expected value of a quadratic form, cf. \(PA\) Exercise 1.10a. To establish the unbiasedness, consider the \(i,j\) element of \(Y^{\prime}(I-M)Y\).

\[\begin{array}{rl}\mathrm{E}[Y^{\prime}_{i}(I-M)Y_{j}]&=\,\mathrm{E}[(Y_{i}- X\beta_{i})^{\prime}(I-M)(Y_{j}-X\beta_{j})]\\ &=\,\mathrm{E}\{\mathrm{tr}[(Y_{i}-X\beta_{i})^{\prime}(I-M)(Y_{j}-X\beta_{j}) ]\}\\ &=\,\mathrm{E}\{\mathrm{tr}[(I-M)(Y_{j}-X\beta_{j})(Y_{i}-X\beta_{i})^{\prime }]\}\\ &=\,\mathrm{tr}\{\mathrm{E}[(I-M)(Y_{j}-X\beta_{j})(Y_{i}-X\beta_{i})^{\prime }]\}\\ &=\,\mathrm{tr}\{(I-M)\mathrm{E}[(Y_{j}-X\beta_{j})(Y_{i}-X\beta_{i})^{\prime }]\}\\ &=\,\mathrm{tr}\{(I-M)\mathrm{Cov}(Y_{j},Y_{i})\}\\ &=\,\mathrm{tr}\{(I-M)\sigma_{ji}I\}\\ &=\,\sigma_{ij}\big{[}n-r(X)\big{]}\,.\end{array}\]

It follows that each element of \(S\) is an unbiased estimate of the corresponding element of \(\Sigma\).

**Example 9.3.1**.: _Partial Correlation Coefficients_.

Partial correlations were discussed in \(PA\) Sect. 6.5. Suppose we have \(n\) observations on two dependent variables \(y_{1},y_{2}\) and \(p-1\) independent variables \(x_{1},\ldots,x_{p-1}\). Write

\[Y=\begin{bmatrix}y_{11}&y_{12}\\ \vdots&\vdots\\ y_{n1}&y_{n2}\end{bmatrix}=[Y_{1},Y_{2}]\]\[Z=\begin{bmatrix}x_{11}&\cdots&x_{1\,p-1}\\ \vdots&&\vdots\\ x_{n1}&\cdots&x_{n\,p-1}\end{bmatrix}.\]

Write a multivariate linear model as

\[Y=[J,Z]B+e,\]

where \(J\) is an \(n\times 1\) vector of 1's. As discussed earlier, the unbiased estimate of \(\Sigma\) is \(S=[s_{ij}]\), where

\[S = Y^{\prime}(I-M)Y/[n-r(X)]\] \[= \frac{1}{n-r(X)}\begin{bmatrix}Y^{\prime}_{1}(I-M)Y_{1}&Y^{ \prime}_{1}(I-M)Y_{2}\\ Y^{\prime}_{2}(I-M)Y_{1}&Y^{\prime}_{2}(I-M)Y_{2}\end{bmatrix}\,.\]

From \(PA\) Sect. 6.5, the sample partial correlation coefficient is

\[r_{y\cdot x} = \frac{Y^{\prime}_{1}(I-M)Y_{2}}{[Y^{\prime}_{1}(I-M)Y_{1}\,Y^{ \prime}_{2}(I-M)Y_{2}]^{1/2}}\] \[= \frac{s_{12}}{\sqrt{s_{11}s_{22}}}\,.\]

The sample partial correlation coefficient is just the sample correlation coefficient as estimated in a multivariate linear model in which the effects of the \(x\) variables have been eliminated. \(\Box\)

We assumed that \(\Sigma\) is positive definite and it is clear that \(S\) and \(E\equiv Y^{\prime}(I-M)Y\) are nonnegative definite. We will often need to know that \(S\) and \(E\) are also nonsingular, hence positive definite. \(S\) and \(E\) are random matrices, so it is by no means clear that they are nonsingular. Fortunately, the following theorem often establishes that \(S\) and \(E\) are nonsingular with probability one.

**Theorem 9.3.2**.: Let \(Y\) be an \(n\times q\) random matrix and let \(A\) be a fixed symmetric \(n\times n\) matrix. If the joint distribution of the \(nq\) elements of \(Y\) admits a density function with respect to Lesbesgue measure on \(\mathbf{R}^{nq}\), then

\[\Pr\big{\{}r(Y^{\prime}AY)=\min\,[q,r(A)]\big{\}}=1.\]

Proof. See Okamoto (1973). \(\Box\)

If the density condition of Theorem 9.3.2 holds, with probability one,

\[\begin{array}{rl}r(E)&=r\big{[}Y^{\prime}(I-M)Y\big{]}\\ &=\min\big{[}q,r(I-M)\big{]}\\ &=\min\big{[}q,n-r(X)\big{]}.\end{array}\]We will henceforth assume that the density conditions holds and that the number of observations \(n\) is large enough so that

\[q\leq n-r(X).\]

In this case, with probability one, \(E\) is a nonnegative definite \(q\times q\) matrix of rank \(q\), hence \(E\) is almost surely positive definite. In the discussion to follow, we will simply treat \(E\), and \(S\), as positive definite.

In particular, for maximum likelihood estimation and for testing in multivariate linear models we assume that the rows of \(Y\) are independent with

\[y_{i}\sim N(B^{\prime}x_{i},\Sigma).\]

This satisfies the density condition of Theorem 9.3.2.

### Maximum Likelihood Estimates

Write the matrices \(Y\) and \(X\) using their component rows,

\[Y=\begin{bmatrix}y_{1}^{\prime}\\ \vdots\\ y_{n}\end{bmatrix}\qquad\text{and}\qquad X=\begin{bmatrix}x_{1}^{\prime}\\ \vdots\\ x_{n}^{\prime}\end{bmatrix}.\]

We assume that the rows of \(Y\) are independent and \(y_{i}\sim N(B^{\prime}x_{i},\Sigma)\) with \(\Sigma\) positive definite. The likelihood function for \(Y\) is

\[L(B,\Sigma)=\prod_{i=1}^{n}(2\pi)^{-q/2}|\Sigma|^{-1/2}\exp\left[-(y_{i}-B^{ \prime}x_{i})^{\prime}\Sigma^{-1}(y_{i}-B^{\prime}x_{i})/2\right],\]

where \(|\Sigma|\) denotes the determinant of \(\Sigma\). The log of the likelihood function is

\[\ell(B,\Sigma)=-\frac{nq}{2}\log(2\pi)-\frac{n}{2}\log(|\Sigma|)-\frac{1}{2} \sum_{i=1}^{n}(y_{i}-B^{\prime}x_{i})^{\prime}\Sigma^{-1}(y_{i}-B^{\prime}x_{i }).\]

Maximum likelihood estimates (MLEs) maximize these functions.

Consider the multivariate model as the univariate model (9.1.2). As with any other univariate linear model, if the nonsingular covariance matrix is fixed, then the MLE of \([I_{q}\otimes X]\text{Vec}(B)\) is the same as the BLUE. As seen in Sect. 9.2, least squares estimates are BLUEs. The least squares estimate of \(XB\) does not depend on the covariance matrix; hence, for any value of \(\Sigma\), \(X\hat{B}=MY\) maximizes the likelihood function. It remains only to find the MLE of \(\Sigma\).

The log-likelihood, and thus the likelihood, are maximized for any \(\Sigma\) by substituting a least squares estimate for \(B\). Write \(\hat{B}=(X^{\prime}X)^{-}X^{\prime}Y\). We need to maximize \[\ell(\hat{B},\Sigma)=-\frac{nq}{2}\log(2\pi)-\frac{n}{2}\log(| \Sigma|)\\ -\frac{1}{2}\sum_{i=1}^{n}[y_{i}-Y^{\prime}X(X^{\prime}X)^{-}x_{i} ]^{\prime}\Sigma^{-1}[y_{i}-Y^{\prime}X(X^{\prime}X)^{-}x_{i}]\]

subject to the constraint that \(\Sigma\) is positive definite. The last term on the right-hand side can be simplified. Define the \(n\times 1\) vector

\[\rho_{i}=(0,\ldots,0,1,0,\ldots,0)^{\prime}\]

with the 1 in the \(i\)th place.

\[\sum_{i=1}^{n}[y_{i}-Y^{\prime}X(X^{\prime}X)^{-}x_{i}]^{\prime} \Sigma^{-1}[y_{i}-Y^{\prime}X(X^{\prime}X)^{-}x_{i}]\] \[=\sum_{i=1}^{n}[y^{\prime}_{i}-x^{\prime}_{i}(X^{\prime}X)^{-}X^{ \prime}Y]\Sigma^{-1}[y_{i}-Y^{\prime}X(X^{\prime}X)^{-}x_{i}]\] \[=\sum_{i=1}^{n}\rho^{\prime}_{i}[Y-X(X^{\prime}X)^{-}X^{\prime}Y] \Sigma^{-1}[Y^{\prime}-Y^{\prime}X(X^{\prime}X)^{-}X^{\prime}]\rho_{i}\] \[=\sum_{i=1}^{n}\rho^{\prime}_{i}(I-M)Y\Sigma^{-1}Y^{\prime}(I-M) \rho_{i}\] \[=\operatorname{tr}[(I-M)Y\Sigma^{-1}Y^{\prime}(I-M)]\] \[=\operatorname{tr}[\Sigma^{-1}Y^{\prime}(I-M)Y]\,.\]

Thus, our problem is to maximize

\[\ell(\hat{B},\Sigma)=-\frac{nq}{2}\log(2\pi)-\frac{n}{2}\log(|\Sigma|)-\frac{1} {2}\operatorname{tr}[\Sigma^{-1}Y^{\prime}(I-M)Y]\,. \tag{9.4.1}\]

We will find the maximizing value by setting all the partial derivatives (with respect to the \(\sigma_{ij}\)'s) equal to zero. To find the partial derivatives, we need the derivative results in Appendix A.1. In particular, we need

\[\mathbf{d}_{\sigma_{ij}}\log|\Sigma| =\operatorname{tr}\bigl{[}\Sigma^{-1}\mathbf{d}_{\sigma_{ij}} \Sigma\bigr{]} \tag{9.4.2}\] \[=\operatorname{tr}\bigl{[}\Sigma^{-1}T_{ij}\bigr{]}\,,\]

where the symmetric \(q\times q\) matrix \(T_{ij}\) has ones in row \(i\) column \(j\) and row \(j\) column \(i\) and zeros elsewhere. We need

\[\mathbf{d}_{\sigma_{ij}}\Sigma^{-1} =-\Sigma^{-1}[\mathbf{d}_{\sigma_{ij}}\Sigma]\Sigma^{-1}\] \[=-\Sigma^{-1}T_{ij}\Sigma^{-1}\,.\]Finally, we need a result involving the derivative of a trace. Using the chain rule,

\[\begin{array}{rl}{\bf d}_{\sigma_{ij}}\mbox{tr}[\Sigma^{-1}Y^{\prime}(I-M)Y]&= \mbox{tr}\big{[}{\bf d}_{\sigma_{ij}}\{\Sigma^{-1}Y^{\prime}(I-M)Y\}\big{]}\\ &=\mbox{tr}\big{[}\big{\{}{\bf d}_{\sigma_{ij}}\Sigma^{-1}\big{\}}\,Y^{\prime}( I-M)Y\big{]}\\ &=\mbox{tr}[-\Sigma^{-1}T_{ij}\Sigma^{-1}Y^{\prime}(I-M)Y]\,.\end{array} \tag{9.4.3}\]

Applying (9.4.2) and (9.4.3) to (9.4.1), we get

\[{\bf d}_{\sigma_{ij}}\ell(\hat{B},\Sigma)=-\frac{n}{2}\mbox{tr}[\Sigma^{-1}T_{ ij}]+\frac{1}{2}\mbox{tr}[\Sigma^{-1}T_{ij}\Sigma^{-1}Y^{\prime}(I-M)Y]\,.\]

Setting the partial derivatives equal to zero leads to finding a positive definite matrix \(\Sigma\) that solves

\[\mbox{tr}[\Sigma^{-1}T_{ij}]=\mbox{tr}[\Sigma^{-1}T_{ij}\Sigma^{-1}Y^{\prime}( I-M)Y/n] \tag{9.4.4}\]

for all \(i\) and \(j\).

Let \(\hat{\Sigma}=\frac{1}{n}Y^{\prime}(I-M)Y\). \(\hat{\Sigma}\) is positive definite whenever \(S\) and \(E\) are. \(\hat{\Sigma}\) provides our solution to the likelihood equations. Substituting \(\hat{\Sigma}\) for \(\Sigma\) in (4) gives

\[\begin{array}{rl}\mbox{tr}[\hat{\Sigma}^{-1}T_{ij}]&=\mbox{tr}[\hat{\Sigma}^{ -1}T_{ij}\hat{\Sigma}^{-1}Y^{\prime}(I-M)Y/n]\\ &=\mbox{tr}[\hat{\Sigma}^{-1}T_{ij}]\,.\end{array}\]

Obviously, this holds for all \(i\) and \(j\). (Readers should at least ponder Exercise 9.10.10.)

Of course we have not actually shown that \(\hat{\Sigma}\) maximizes the likelihood, only that it is a critical point. As usual, if \(q=1\), this reduces to the standard univariate result.

### Hypotheses and Statistics

Consider testing the multivariate model

\[Y=XB+e \tag{9.5.1}\]

against a reduced model

\[Y=X_{0}\Gamma+e, \tag{9.5.2}\]

where \(C(X_{0})\subset C(X)\) and the elements of \(e\) are multivariate normal. The covariance matrix \([\Sigma\otimes I_{n}]\) from the univariate model (4.1.2) is unknown, so standard univariate methods of testing do not apply. Let \(M_{0}=X_{0}(X_{0}^{\prime}X_{0})^{-}X_{0}^{\prime}\) be the perpendicular projection operator onto \(C(X_{0})\). Multivariate tests of model (9.5.2) versus model (9.5.1) are based on the _hypothesis statistic_

\[H\equiv Y^{\prime}(M-M_{0})Y\]

and the _error statistic_

\[E\equiv Y^{\prime}(I-M)Y.\]These statistics look identical to the sums of squares used in standard univariate linear models. The difference is that the univariate sums of squares are scalars, while in multivariate models these statistics are matrices. The matrices have diagonals that consist of sums of squares for the various dependent variables and off-diagonals that are sums of cross products of the different dependent variables.

For univariate models, the test statistic is proportional to the scalar \(Y^{\prime}(M-M_{0})Y[Y^{\prime}(I-M)Y]^{-1}\). For multivariate models, the test statistic is often taken as a function of the matrix \(Y^{\prime}(M-M_{0})Y[Y^{\prime}(I-M)Y]^{-1}\) or some closely related matrix. For multivariate models, there is no one test statistic that is the universal standard for performing tests. Various test statistics are discussed in the next section.

The existence of multiple test statistics is not that unusual. Even in a univariate balanced one-way ANOVA, the range test (that underlies Tukey's HSD multiple comparison procedure) is a reasonable alternative to the \(F\) test for no group differences. Ott's Analysis of Means is similarly based on an alternative to the \(F\) test. See Christensen (1996, Chapter 6.)

The procedure for testing an hypothesis about an estimable parametric function, say

\[H_{0} : \Lambda^{\prime}B=0\]

where \(\Lambda^{\prime}=P^{\prime}X\), follows from model testing exactly as in \(PA\) Sect. 3.3. The test is based on the hypothesis statistic

\[H \equiv Y^{\prime}M_{MP}Y\] \[= (\Lambda^{\prime}\hat{B})^{\prime}[\Lambda^{\prime}(X^{\prime}X) ^{-}\Lambda]^{-}(\Lambda^{\prime}\hat{B})\]

and the error statistic

\[E\equiv Y^{\prime}(I-M)Y\.\]

The projection operator in \(H\) is \(M_{MP}=MP(P^{\prime}MP)^{-}P^{\prime}M\). Its use follows from the fact that \(\Lambda^{\prime}B=0\) puts a constraint on the model that requires \(\mbox{E}(Y_{h})\in C[(I-M_{MP})X]=C(M-M_{MP})\) for each \(h\). Thus, the reduced model can be written

\[Y=(M-M_{MP})\Gamma+e,\]

and the hypothesis statistic is

\[Y^{\prime}[M-(M-M_{MP})]Y=Y^{\prime}M_{MP}Y\.\]

Just as in \(PA\) Sects. 3.1 and 3.2, both the reduced model hypothesis and the parametric function hypothesis can be generalized. The reduced model can be generalized to

\[Y=X_{0}\Gamma+XQ+e,\]

where \(XQ\) is a known \(n\times q\) matrix. As in \(PA\) Sect. 3.2, model (9.5.1) is rewritten as

\[(Y-XQ)=XB_{*}+e\]for some appropriate reparameterization \(B_{*}\). Model (9.5.4) is rewritten as

\[(Y-XQ)=X_{0}\Gamma+e\,. \tag{9.5.6}\]

The test of (9.5.4) versus (9.5.1) is performed by testing (9.5.6) against (9.5.5). The hypothesis statistic for the test is

\[H\equiv(Y-XQ)^{\prime}(M-M_{0})(Y-XQ)\,.\]

The error statistic is

\[E \equiv (Y-XQ)^{\prime}(I-M)(Y-XQ)\] \[= Y^{\prime}(I-M)Y\,.\]

Similarly, for a known matrix \(\tilde{W}\), a test can be performed of

\[H_{0}\!:\Lambda^{\prime}B=\tilde{W}\]

where \(\Lambda^{\prime}=P^{\prime}X\) and the equation \(\Lambda^{\prime}B=\tilde{W}\) has at least one solution. Let \(Q\) be a known solution \(\Lambda^{\prime}Q=\tilde{W}\); then, the hypothesis statistic is

\[H \equiv (Y-XQ)^{\prime}M_{MP}(Y-XQ)\] \[= (\Lambda^{\prime}\hat{B}-\tilde{W})^{\prime}[\Lambda^{\prime}(X^ {\prime}X)^{-}\Lambda]^{-}(\Lambda^{\prime}\hat{B}-\tilde{W})\]

and the error statistic is

\[E \equiv (Y-XQ)^{\prime}(I-M)(Y-XQ)\] \[= Y^{\prime}(I-M)Y\,.\]

An interesting variation on the hypothesis \(H_{0}\!:\Lambda^{\prime}B=0\) is

\[H_{0}\!:\Lambda^{\prime}B_{\xi}=0, \tag{9.5.7}\]

where \(\xi\) can be any \(q\times 1\) vector and again \(\Lambda^{\prime}=P^{\prime}X\). To test (9.5.7), transform model (9.5.1) into

\[Y\xi=XB\xi+e\xi\,.\]

This is a standard univariate linear model with dependent variable vector \(Y\xi\), parameter vector \(B\xi\), and error vector \(e\xi\sim N(0,\xi^{\prime}XB\xi I_{n})\). It is easily seen that the least squares estimate of \(B\xi\) in the univariate model is \(\hat{B}\xi\). From univariate theory, a test for (9.5.7) is based on the noncentral \(F\) distribution, in particular

\[\frac{(\Lambda^{\prime}\hat{B}\xi)^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda]^{-}(\Lambda^{\prime}\hat{B}\xi)/r(\Lambda)}{\xi^{\prime}Y^{\prime}(I- M)Y\xi/[n-r(X)]}\sim F[r(\Lambda),n-r(X),\pi],\]

where

\[\pi=\xi^{\prime}B^{\prime}\Lambda[\Lambda^{\prime}(X^{\prime}X)^{-}\Lambda]^{ -}\Lambda^{\prime}B\xi/2\xi^{\prime}\Sigma\xi\,.\]This test can also be generalized in several ways. For example, let \(W\) be a known \(q\times r\) matrix with \(r(W)=r<q\). A test of

\[H_{0}\,\colon\Lambda^{\prime}BW=0 \tag{9.5.8}\]

can be performed by examining the transformed multivariate linear model

\[YW=XBW+eW\,.\]

Here, the dependent variable matrix is \(YW\), the error matrix is \(eW\), and the parameter matrix is \(BW\). The test of (9.5.8) follows precisely the form of (9.5.3). The hypothesis statistic is

\[H_{*}\equiv W^{\prime}Y^{\prime}M_{MP}YW=(\Lambda^{\prime}\hat{B}W)^{\prime}( \Lambda^{\prime}(X^{\prime}X)^{-}\Lambda)^{-}(\Lambda^{\prime}\hat{B}W)\]

and the error statistic is

\[E_{*}\equiv W^{\prime}Y^{\prime}(I-M)YW\,.\]

It was convenient to assume that \(W\) has full column rank. If \(B\) can be any \(p\times q\) matrix and \(W\) has full column rank, then \(BW\) can be any \(p\times r\) matrix. Thus, \(BW\) can serve as the parameter matrix for a multivariate linear model. If \(W\) does not have full column rank, then \(YW\) has linear dependencies, \(\operatorname{Cov}(W^{\prime}\varepsilon_{i})=W^{\prime}\Sigma W\) is singular, and \(BW\) is not an arbitrary \(p\times r\) matrix. None of these problems is an insurmountable difficulty for conducting the analysis of the transformed model, but proving that the analysis works for a nonfull rank \(W\) is more trouble than it is worth.

**Exercise 9.2**.: Show that under the multivariate linear model

\[\operatorname{E}\left[Y^{\prime}\left(M-M_{0}\right)Y\right]=r(M-M_{0})\, \Sigma+B^{\prime}X^{\prime}(M-M_{0})XB.\]

### Test Statistics

Various functions of the hypothesis and error statistics have been proposed as test statistics. Four of the more commonly used are discussed here. A complete survey will not be attempted, and an exhaustive treatment of the related distribution theory will certainly not be given.

In this section, we consider testing only reduced models such as (9.5.2) or parametric hypotheses such as (9.5.3). Hence,

\[E=Y^{\prime}(I-M)Y\]

and, according to the context, either

\[H=Y^{\prime}(M-M_{0})Y\]or

\[H=Y^{\prime}M_{MP}Y\,.\]

Adjustments for other hypotheses are easily made.

The test statistics discussed in this section are all functions of \(H\) and \(E\). Under normality, the null distributions of these statistics depend on \(H\) and \(E\) only through the fact that they have independent central _Wishart distributions_.

**Definition 9.6.1**.: Let \(w_{1},w_{2},\cdots,w_{n}\) be independent \(N(\mu_{i},\Sigma)\); then

\[W=\sum_{i=1}^{n}w_{i}w_{i}^{\prime}\]

has a noncentral Wishart distribution with \(n\) degrees of freedom, covariance matrix \(\Sigma\), and noncentrality parameter matrix \(Q\), where

\[Q=\frac{1}{2}\Sigma^{-1}\sum_{i=1}^{n}\mu_{i}\mu_{i}^{\prime}\,.\]

If \(Q=0\), the distribution is a central Wishart. In general, write

\[W\sim W(n,\Sigma,Q)\,.\]

Under the full model and assuming normal distributions, \(H\) and \(E\) have independent Wishart distributions. In particular,

\[E\sim W\big{(}n-r(X),\Sigma,0\big{)}\]

and

\[H\sim W\left(r(X)-r(X_{0}),\Sigma,\frac{1}{2}\Sigma^{-1}B^{\prime}X^{\prime}( M-M_{0})XB\right).\]

The reduced model is true if and only if

\[H\sim W\big{(}r(X)-r(X_{0}),\Sigma,0\big{)}.\]

**Exercise 9.3**.:
1. Use Definition 9.6.1 to show that \(E\) and \(H\) have the distributions indicated earlier.
2. Show that \(H\) and \(E\) are independent.
3. Show that \(MY\) and \(E\) are independent.

Hint: For (b), show that \((I-M)Y\) and \((M-M_{0})Y\) are independent. This exercise assumes that you know basic properties of multivariate normal distributions.

The covariance matrix of a Wishart was previously introduced in the discussion of MIVQUE in Chap. 4. See Muirhead (1982) for a detailed discussion. Ad

[MISSING_PAGE_FAIL:386]

is too small. Noting that \(\hat{\Sigma}=E/n\) and \(\hat{\Sigma}_{H}=(E+H)/n\), we see that

\[U=\frac{|E|}{|E+H|}\,.\]

Multiplying the numerator and denominator of \(U\) by \(|E^{-1}|\), we get a slightly different form,

\[U=\frac{|I|}{|I+HE^{-1}|}=|I+HE^{-1}|^{-1}\,.\]

When \(H_{0}\) is true, \(U\) has some distribution, say

\[U\sim U(q,d,n-r(X)),\]

where \(d\) is either \(r(X)-r(X_{0})\) or \(r(\Lambda)\) depending on which kind of hypothesis is being tested. An \(\alpha\)-level test is rejected if the observed value of \(U\) is smaller than the \(\alpha\) percentile of the \(U\big{(}q,d,n-r(X)\big{)}\) distribution.

The likelihood ratio test statistic (LRTS) is often referred to as _Wilks's \(\Lambda\)_. The symbol \(\Lambda\) is used here for other purposes, so to minimize internal confusion, the LRTS is denoted \(U\). In reading applications and computer output, the reader needs to remember that references to Wilks's \(\Lambda\) are references to the LRTS.

Rao (1951) has established the following approximate distribution for \(U\) when \(H_{0}\) is true. Let

\[r = r(X),\] \[d = r(X)-r(X_{0})=r(\Lambda),\] \[s = \frac{qd}{2}+1,\] \[f = (n-r)+d-\frac{1}{2}(d+q+1),\]

and

\[t=\left\{\begin{array}{ll}\big{[}(q^{2}d^{2}-4)/(q^{2}+d^{2}-5)\big{]}^{ \frac{1}{2}}&\mbox{if }\min(q,d)\geq 2\\ 1&\mbox{if }\min(q,d)=1\end{array}\right.;\]

then, it is approximately true that

\[\frac{1-U^{1/t}}{U^{1/t}}\ \ \frac{ft-s}{qd}\sim F(qd,ft-s).\]

The test is rejected for large values of \((1-U^{1/t})/U^{1/t}\). If \(\min(q,d)\) is 1 or 2, the distribution is exact. For properties and tables of the \(U\) distribution, see a text on multivariate analysis, such as Seber (1984, p. 413).

An alternative to the likelihood ratio test statistic can be generated by _Roy's union-intersection principle_ (see Roy 1953). His method is based on the fact that the reduced model (9.5.2) holds if and only if, for every \(q\times 1\) vector \(\xi\), the univariate linear model \[Y\xi=X_{0}\Gamma\xi+e\xi\,\ e\xi\sim N(0,(\xi^{\prime}\Sigma\xi)I)\]

holds. If we denote \(H_{0\xi}\) as the hypothesis that model (9.6.1) holds and \(H_{0}\) as the hypothesis that model (9.5.2) holds, then

\[H_{0}=\bigcap_{\text{all }\xi}H_{0\xi}\.\]

The intersection is appropriate because the equivalence requires the validity of _every_ model of the form (9.6.1).

Similarly, the full model (9.5.1) holds if and only if every model

\[Y\xi=XB\xi+e\xi\]

holds. The alternative hypothesis \(H_{A}\) that (9.5.1) holds but (9.5.2) does not is simply that, for some \(\xi\), the univariate model (9.6.2) holds but model (9.6.1) does not. Let \(H_{A\xi}\) be the univariate alternative. Because \(H_{A}\) requires only one of the \(H_{A\xi}\) to be true, we have

\[H_{A}=\bigcup_{\text{all }\xi}H_{A\xi}\.\]

This reformulation of \(H_{A}\) and \(H_{0}\) in terms of unions and intersections is the genesis of the name "union-intersection principle." (Merely mentioning an alternative hypothesis makes me feel like an apostate.)

For testing model (9.6.1) against model (9.6.2), there is a standard univariate \(F\) statistic available, \(F_{\xi}=[\xi^{\prime}Y^{\prime}(M-M_{0})Y\xi/(r(X)-r(X_{0}))]/[\xi^{\prime}Y^{ \prime}(I-M)Y\xi/(n-r(X))]\). The union-intersection principle test statistic is simply

\[\psi_{\text{max}}\equiv\sup_{\xi}\{F_{\xi}\}\.\]

The test is rejected if \(\psi_{\text{max}}\) is too large. In practice, an alternative but equivalent test statistic is used. Because

\[F_{\xi}=\frac{n-r(X)}{r(X)-r(X_{0})}\frac{\xi^{\prime}H\xi}{\xi^{\prime}E\xi}\,\]

it is equivalent to reject \(H_{0}\) if

\[\phi_{\text{max}}\equiv\sup_{\xi}\left[\frac{\xi^{\prime}H\xi}{\xi^{\prime}E \xi}\right]\]

is too large.

We will show that \(\phi_{\text{max}}\) is distributed as the largest eigenvalue of \(HE^{-1}\). The argument is based on the following result.

**Lemma 9.6.2.** If \(E\) is a positive definite matrix, then there exists a nonsingular matrix \(E^{1/2}\) such that \[E=E^{1/2}E^{1/2}\]

and

\[E^{-1/2}E\,E^{-1/2}=I.\]

Proof.  As in the proof of _PA_ Theorem B.22, write

\[E=PD(\lambda_{i})P^{\prime},\]

where \(P\) is an orthonormal matrix, \(D(\lambda_{i})\) is diagonal, and the \(\lambda_{i}\)'s are all positive. Pick

\[E^{1/2}=PD(\sqrt{\lambda_{i}})P^{\prime},\]

and the results are easily shown. 

Note that any vector \(\xi\) can be written as \(E^{-1/2}\rho\) for some vector \(\rho\), and any vector \(\rho\) determines a vector \(\xi\), so

\[\phi_{\max}=\sup_{\rho}\left[\frac{\rho^{\prime}E^{-1/2}HE^{-1/2}\rho}{\rho^{ \prime}\rho}\right].\]

Writing \(E^{-1/2}HE^{-1/2}\) as \(P_{0}D(\phi_{i})P_{0}^{\prime}\), where \(P_{0}\) is an orthonormal matrix and the \(\phi_{i}\)'s are the eigenvalues of \(E^{-1/2}HE^{-1/2}\), we can replace \(\rho\) with \(v=P_{0}\rho\), giving

\[\phi_{\max}=\sup_{v}\left[\frac{v^{\prime}D(\phi_{i})v}{v^{\prime}v}\right].\]

Because

\[v^{\prime}D(\phi_{i})v/v^{\prime}v=\sum_{i=1}^{q}v_{i}^{2}\phi_{i}\Big{/}\sum _{i=1}^{q}v_{i}^{2}\]

is a weighted average of the \(\phi_{i}\)'s, the maximum value is attained when all of the weight is placed on the largest eigenvalue. Thus,

\[\phi_{\max}=\max_{i}\ \phi_{i}\.\]

Finally, we need to establish that \(E^{-1/2}HE^{-1/2}\) and \(HE^{-1}\) have the same eigenvalues. Let \(w\) be an eigenvector for \(E^{-1/2}HE^{-1/2}\) corresponding to the eigenvalue \(\phi\). Because \(E^{-1/2}HE^{-1/2}w=\phi w\), we have

\[E^{1/2}E^{-1/2}HE^{-1/2}E^{-1/2}E^{1/2}w=\phi E^{1/2}w\]

and

\[HE^{-1}E^{1/2}w=\phi E^{1/2}w\.\]

Clearly, \(E^{1/2}w\) is an eigenvector of \(HE^{-1}\) corresponding to \(\phi\). Thus, we have shown that \(\phi_{\max}\) is the largest eigenvalue of \(HE^{-1}\).

Rather than using \(\phi_{\max}\), tests are often performed using \(\theta_{\max}\), where \(\theta_{\max}\) is the maximum eigenvalue of \(H(E+H)^{-1}\). It is a simple matter to see that the eigenvalues of \(H(E+H)^{-1}\) and \(HE^{-1}\) are related by the equation \(\theta=\phi/(1+\phi)\). Thus, \(\theta_{\max}\) is a one-to-one increasing transformation of \(\phi_{\max}\) and the test based on \(\theta_{\max}\) is equivalent to the one based on \(\phi_{\max}\). Tables of the distribution of \(\theta_{\max}\) under \(H_{0}\) were worked out by Heck (1960). Derivations and tables for the distribution of \(\phi_{\max}\) or \(\theta_{\max}\) under the null hypothesis can be found in many standard multivariate analysis books, such as Seber (1984, Section 2.5).

Although the likelihood ratio and union-intersection test statistics are probably the best known, several other test statistics have also been proposed. Two of these are the _Lawley-Hotelling trace_

\[T^{2}=\big{[}n-r(X)\big{]}\mbox{tr}[HE^{-1}]=\mbox{tr}[HS^{-1}]\]

and _Pillai's trace_

\[V=\mbox{tr}\big{[}H(E+H)^{-1}\big{]}\,.\]

**Exercise 9.4.**  Show that \(\theta\) is an eigenvalue of \(H(E+H)^{-1}\) if and only if \(\phi\) is an eigenvalue of \(HE^{-1}\), where \(\theta=\phi/(1+\phi)\). Show that

\[U=\prod_{h=1}^{q}1/(1+\phi_{i}),\quad T^{2}=\sum_{h=1}^{q}\phi_{i}/[n-r(X)], \quad V=\sum_{h=1}^{q}\phi_{i}/(1+\phi_{i}).\]

Hints: Show that the eigenvalues of \(AB^{-1}\) and \(B^{-1}A\) are the same. Use the fact that

\[I=(E+H)^{-1}H+(E+H)^{-1}E\.\]

Though the exact distributions of \(T^{2}\) and \(V\) require special tables, their distributions when \(H_{0}\) is true can be approximated using standard tables. Let \(d=r(X)-r(X_{0})=r(\Lambda)\) and let \(n-r(X)=n-r\). A very good approximation to \(T^{2}\) has been suggested by McKeon (1974). He advocates using

\[GT^{2}\sim F(qd,D),\]

where

\[D=4+\frac{qd+2}{B-1},\]

\[B=\frac{(n-r+d-q-1)(n-r-1)}{(n-r-q-3)(n-r-q)},\]

and

\[G=(qd)^{-1}\left[\frac{D}{D-2}\right]\left[\frac{n-r-q-1}{n-r}\right].\]

This gives the exact distribution for \(\min(q,d)=1\). In particular, if \(d=1\)\[B=\frac{n-r-1}{n-r-q-3},\quad D=n-r-q+1,\quad G=\frac{1}{n-r}\frac{n-r-q+1}{q}.\]

It follows that when \(r(H)=1\),

\[F\equiv\frac{1}{dfE}\frac{dfE-q+1}{q}T^{2}\sim F(q,dfE-q+1).\]

Note that, for large \(n\), the term \(B-1\) gets very small so that \(D\), the denominator degrees of freedom, gets large. In particular, for large \(n\), the distribution of \(GT^{2}\) is approximately that of a \(\chi^{2}(qd)\) divided by its degrees of freedom. This result is equivalent to the standard asymptotic approximation

\[T^{2}\sim\chi^{2}(qd).\]

The null distribution of Pillai's trace can be approximated by

\[\frac{n-r-q+s}{|q-d|+s}\ \ \frac{V}{s-V}\sim F\left(s\left[|q-d|+s\right],s \left[n-r-q+s\right]\right),\]

where

\[s=\min(q,d).\]

Asymptotically,

\[(n-r)V\sim\chi^{2}(qd).\]

Seber (1984, p. 414) compares all four test statistics. He also provides tables and other details about the distributions under \(H_{0}\) (see Seber 1984, Section 2.5). Kres (1983) is another good source for tables related to multivariate linear models.

For standard linear models, the univariate \(F\) test should be about 1 if the null model is correct, regardless of whether the data are multivariate normal. Similarly, although formal tests based on these four statistics depend on the assumption of multivariate normality, these are reasonable test statistics even without the normality assumption as long as the assumed mean and covariance structures are reasonable. To evaluate the test statistics intuitively, we need some idea of the values of the test statistics when \(H_{0}\) is true. As we have seen, even without the assumption of normality

\[\mbox{E}(S)=\mbox{E}\left(E/\left[n-r(X)\right]\right)=\Sigma,\]

and from Exercise 9.3, under \(H_{0}\),

\[\mbox{E}\left(H/r(M-M_{0})\right)=\Sigma\.\]

Using these equalities as crude approximations, it follows that if \(H_{0}\) is true,

\[U\doteq\left|I+\left(\frac{r(X)-r(X_{0})}{n-r(X)}\right)I\right|^{-1}\]\[\begin{array}{l}\phi_{\max}=\left[\frac{n-r(X)}{n-r(X_{0})}\right]^{q},\\ \phi_{\max}\doteq\frac{r(X)-r(X_{0})}{n-r(X)},\\ \theta_{\max}\doteq\frac{n-r(X)}{n-r(X_{0})},\\ T^{2}\doteq\operatorname{tr}(I)\left[r(X)-r(X_{0})\right]\\ =q\left[r(X)-r(X_{0})\right],\end{array}\]

and

\[\begin{array}{l}V\doteq\operatorname{tr}(I)\left[r(X)-r(X_{0})\right]/\left[ n-r(X_{0})\right]\\ =q\left[r(X)-r(X_{0})\right]/\left[n-r(X_{0})\right].\end{array}\]

These _comparison values_ can be very useful in exploring the data. If the observed value of \(U\) is much smaller than \((\left[n-r(X)\right]/\left[n-r(X_{0})\right])^{q}\) or if the observed values of the other test statistics are much larger than the comparison values, the null hypothesis is called in question. In a formal test, the null distribution of the test statistic is used to quantify the meaning of the word "much" in the previous sentence.

An important parameter in the distributions of the test statistics is the rank of \(HE^{-1}\). With probability one, \(E\) is nonsingular, so

\[r\bigl{(}HE^{-1}\bigr{)}=r(H).\]

By definition,

\[H=Y^{\prime}(M-M_{0})Y\]

is a \(q\times q\) matrix. Because \(r(M-M_{0})=r(X)-r(X_{0})\), Theorem 9.3.2 implies that if \(Y\) has a density, \(r(HE^{-1})=r(H)=\min\left[q,r(X)-r(X_{0})\right]\) with probability one.

#### Equivalence of Test Statistics

Whenever \(H\) is a rank one matrix, all four of the test statistics discussed are equivalent. First, note that \(E\) is nonsingular (with probability one), so \(HE^{-1}\) has rank one. A rank one matrix has only one nonzero eigenvalue, hence the maximum eigenvalue equals the sum of the eigenvalues. In other symbols,

\[\phi_{\max}=\operatorname{tr}[HE^{-1}],\]

or equivalently

\[\phi_{\max}=\frac{1}{n-r(X)}T^{2}.\]Thus, the union-intersection test statistic is equivalent to Hotelling's \(T^{2}\).

The likelihood ratio test uses the statistic

\[U=|I+HE^{-1}|^{-1}\,.\]

It is easy to see that the eigenvalues of \(I+HE^{-1}\) are just one plus the eigenvalues of \(HE^{-1}\). The determinant is the product of the eigenvalues \(\prod_{h=1}^{q}(1+\phi_{i})\), and \(HE^{-1}\) has only one nonzero eigenvalue, so

\[|I+HE^{-1}|=1+\phi_{\max}\]

and

\[U=(1+\phi_{\max})^{-1}\,.\]

The statistic \(U\) is a strictly decreasing function of \(\phi_{\max}\), thus the likelihood ratio test is equivalent to the other tests.

The equivalence of Pillai's trace to Hotelling's \(T^{2}\) is established in Exercise 9.5.

**Exercise 9.5**.: Use _PA-V_ Proposition 12.5.1 (Christensen 2011, Proposition 13.5.1) to show that the test based on Pillai's trace is equivalent to the test based on Hotelling's \(T^{2}\). The proposition states that

\[(A+a^{\prime}b)^{-1}=A^{-1}-A^{-1}a^{\prime}(I+bA^{-1}a^{\prime})^{-1}bA^{-1}.\]

### Prediction and Confidence Regions

Suppose we wish to predict the value of a new observation vector \(y_{0}^{\prime}\) with \(\mathrm{E}(y_{0}^{\prime})=x_{0}^{\prime}B\), where \(x_{0}^{\prime}B\) is estimable (i.e., \(x_{0}^{\prime}B=\rho_{0}^{\prime}XB\) for some vector \(\rho_{0}\)). It is natural to assume that \(y_{0}\) is generated by the same process as \(Y\), thus \(\mathrm{Cov}(y_{0})=\Sigma\) and \(y_{0}\) is independent of \(Y\). As discussed in Chap. 4, it follows that the best linear unbiased predictor of \(y_{0}\) is \(\hat{y}_{0}\equiv\hat{B}^{\prime}x_{0}\).

A prediction region for \(y_{0}\) can be based on the distribution of \(y_{0}-\hat{y}_{0}\). Clearly, \(\mathrm{E}(y_{0}-\hat{y}_{0})=0\) and, recalling Exercise 9.1,

\[\mathrm{Cov}(y_{0}-\hat{y}_{0}) = \Sigma+\mathrm{Cov}(\hat{y}_{0})\] \[= \Sigma+\mathrm{Cov}(Y^{\prime}M\rho_{0})\] \[= \Sigma+\mathrm{Cov}\left(\begin{bmatrix}Y_{1}^{\prime}M\rho_{0} \\ \vdots\\ Y_{q}^{\prime}M\rho_{0}\end{bmatrix}\right)\] \[= \Sigma+\rho_{0}^{\prime}M\rho_{0}\,\Sigma\] \[= \left(1+x_{0}^{\prime}(X^{\prime}X)^{-}x_{0}\right)\Sigma.\]If \(y_{0}\) and \(Y\) are multivariate normal, then \(y_{0}-\hat{y}_{0}\) is also normal, independent of \(E\), and

\[\left(1+x_{0}^{\prime}(X^{\prime}X)^{-}x_{0}\right)^{-1}(y_{0}-\hat{y}_{0})(y_{0 }-\hat{y}_{0})^{\prime}\sim W(1,\Sigma,0).\]

It follows that

\[\frac{\operatorname{tr}\left[(y_{0}-\hat{y}_{0})(y_{0}-\hat{y}_{0})^{\prime}S^{ -1}\right]}{1+x_{0}^{\prime}(X^{\prime}X)^{-}x_{0}},\]

or equivalently

\[\frac{(y_{0}-\hat{y}_{0})^{\prime}S^{-1}(y_{0}-\hat{y}_{0})}{1+x_{0}^{\prime}( X^{\prime}X)^{-}x_{0}},\]

has the same distribution as the null distribution of the Lawley-Hotelling \(T^{2}\) statistic with \(d=1\). From our discussion of McKeon's approximation with \(dfE\equiv n-r(X)\), the exact distribution is

\[\frac{(y_{0}-\hat{y}_{0})^{\prime}S^{-1}(y_{0}-\hat{y}_{0})}{(dfE)\left(1+x_{0 }^{\prime}(X^{\prime}X)^{-}x_{0}\right)}\frac{dfE+1-q}{q}\sim F(q,dfE+1-q,0).\]

A \((1-\alpha)100\%\) prediction ellipsoid consists of all \(y_{0}\) vectors that satisfy

\[(y_{0}-\hat{y}_{0})^{\prime}S^{-1}(y_{0}-\hat{y}_{0})\leq\\ F(1-\alpha,q,dfE+1-q)\frac{q}{dfE+1-q}\left(dfE\right)\left(1+x_{0}^{\prime}(X^{\prime}X)^{-}x_{0}\right).\]

_The methods illustrated here can also be used to obtain confidence ellipsoids._ Examples of such ellipsoids are given in Sects. 10.1 and 10.2.

## Multiple Testing Methods

A number of multiple comparison methods for univariate linear models were discussed in _PA_ Chap. 5. _PA_ also discusses general issues of multiple testing in more detail than we do here. Three of the methods discussed in _PA_ are easily adapted for use with multivariate linear models.

Consider a collection of single degree of freedom estimable null hypotheses \(H_{0j}:\lambda_{j}^{\prime}B=0\), \(j=1,\ldots,s\). For example, the hypotheses could be contrasts in a one-way ANOVA or interaction contrasts in a balanced two-way ANOVA. Define \(\Lambda\equiv[\lambda_{1},\ldots,\lambda_{s}]\). Then \(\Lambda^{\prime}B=0\), for example, could be for testing equality of treatment groups in a one-way ANOVA or for testing no interaction in a balanced two-way ANOVA. The relevant \(\lambda\)s are determined by \(X\), and are appropriate for any \(q\). Clearly, \(\Lambda^{\prime}B=0\) if and only if \(\lambda_{j}^{\prime}B=0\), \(j=1,\ldots,s\). Moreover, both of these conditions are equivalent to \(\lambda^{\prime}B=0\) for every \(\lambda\in C(\Lambda)\). Equivalently, there exists a \(\lambda\in C(\Lambda)\) such that \(\lambda^{\prime}B\neq 0\) if and only if \(\Lambda^{\prime}B\neq 0\) which, since the \(\lambda_{j}\)s are a spanning set for \(C(\Lambda)\), occurs if and only if there exists a \(j\) such that \(\lambda_{j}^{\prime}B\neq 0\). Indeed, we do not even need to define \(\Lambda=[\lambda_{1},\ldots,\lambda_{s}]\) as long as \[C(\Lambda)=\mbox{span}\{\lambda_{1},\ldots,\lambda_{s}\}.\]

Henceforth, _any reference within this section to \(\lambda^{\prime}B\) presumes that \(\lambda\in C(\Lambda)\)_.

For testing \(\Lambda^{\prime}B=0\), \(H_{\Lambda}\equiv Y^{\prime}M_{MP}Y\). For testing \(\lambda^{\prime}B=0\) with \(\lambda\in C(\Lambda)\) the hypothesis matrix is \(H_{\lambda}\equiv Y^{\prime}M_{MP}Y\) where \(H_{\Lambda}-H_{\lambda}\) is nonnegative definite. Here \(P\) and \(\rho\) are defined by \(\Lambda^{\prime}=P^{\prime}X\) and \(\lambda^{\prime}=\rho^{\prime}X\) and have the property that \(M\rho\in C(MP)\). Finally, define \(H_{j}\equiv H_{\lambda_{j}}\) and more generally we may replace a subscript \(\lambda_{j}\) with \(j\) in this section. For our four test statistics based on \(H_{\Lambda}\), \(H_{\lambda}\), and \(E\),

\[U_{\Lambda} \leq U_{\lambda},\] \[\phi_{\max,\Lambda} \geq \phi_{\max,\lambda}.\] \[T_{\Lambda}^{2} \geq T_{\lambda}^{2},\] \[V_{\Lambda} \geq V_{\lambda}.\]

See Exercise 9.6 for the validity of these statements. Unlike univariate models, when \(q\geq 2\) only \(\phi_{\max}\) typically has a \(\lambda\) for which equality can always be achieved.

Let a generic \(\alpha\) level test of \(\Lambda^{\prime}B=0\) be to reject when

\[Q_{\Lambda}>Q[1-\alpha,q,r(\Lambda),n-r(X)]\]

and a generic \(\alpha\) level test of \(\lambda^{\prime}B=0\) be to reject when

\[Q_{\lambda}>Q[1-\alpha,q,1,n-r(X)].\]

Specifically, \(Q\) can be any of \(\phi_{\max}\), \(T^{2}\), \(V\), or \(1/U\).

The _Least Significant Difference (LSD)_ method often (I think incorrectly) associated with Fisher, amounts to

1. Check the truth of \[Q_{\Lambda}>Q[1-\alpha,q,r(\Lambda),n-r(X)]\]
2. If the inequality is not true, quit and go home. None of the \(\lambda^{\prime}B\)s are declared significant.
3. If the inequality is true, declare \(\lambda^{\prime}B\) significantly different from 0 if and only if \[Q_{\lambda}>Q[1-\alpha,q,1,n-r(X)].\]

Note that step 3 is just the usual \(\alpha\) level \(Q\) test for \(\lambda^{\prime}B=0\). Collective experience indicates that the method works reasonably well if you restrict yourself in step 3 to testing only a prespecified finite collection \(\lambda^{\prime}_{j}B=0\), \(j=1,\ldots,s\). Experience indicates that this method performs very badly, in the sense that it rejects too many hypotheses \(\lambda^{\prime}B=0\) that are true, if you use it to test arbitrarily chosen \(\lambda^{\prime}B\).

_Scheffe's method_ amounts to

1. Check the truth of \[Q_{\Lambda}>Q[1-\alpha,q,r(\Lambda),n-r(X)].\] (9.8.1)2. If the inequality is not true, quit and go home. None of the \(\lambda^{\prime}B\)s are declared significant.
3. If the inequality is true, declare \(\lambda^{\prime}B\) significantly different from 0 if and only if \[Q_{\lambda}>Q[1-\alpha,q,r(\Lambda),n-r(X)].\]

The basis of Scheffe's method is replacing \(Q_{\Lambda}\) in (9.8.1) with \(Q_{\lambda}\). The motivation is based on the earlier discussed fact that \[Q_{\Lambda}\geq Q_{\lambda},\] (9.8.2) so it is impossible to reject \(\lambda^{\prime}B=0\) unless you have previously rejected \(\Lambda^{\prime}B=0\). In fact, you can skip steps 1 and 2 of the algorithm because they take care of themselves. Unfortunately, unlike Scheffe univariate test, except for \(Q=\phi_{\max}\), there does not seem to exist a \(\lambda\in C(\Lambda)\) for which equality occurs in (9.8.2), so there is no assurance in step 3 that there exists a \(\lambda^{\prime}B=0\) that would be rejected, even though, based on step 1, we are confident that there exist \(\lambda^{\prime}B\neq 0\). In particular (and this agrees with the univariate case), there is no assurance that any of the specific \(\lambda^{\prime}_{j}B=0\) hypotheses will be rejected, even though we are reasonably confident that they cannot all be true.

Scheffe's procedure makes it very difficult to reject a \(\lambda^{\prime}B=0\). Collective experience indicates that this method performs well if you use it to test arbitrary \(\lambda^{\prime}B\) (including ones suggested by the data) but is too conservative if your interest is only in testing a prespecified finite collection \(\lambda^{\prime}_{j}B=0\), \(j=1,\ldots,s\). Too conservative means not finding enough of the \(\lambda^{\prime}_{j}B\)s that are different from 0.

Bonferroni's method simply involves declaring \(\lambda^{\prime}_{j}B\) significantly different from 0 if and only if

\[Q_{\lambda_{j}}>Q[1-\alpha_{j},q,1,n-r(X)],\]

where the \(\alpha_{j}\)s add up to \(\alpha\). Typically one takes \(\alpha_{j}=\alpha/s\).

When considering \(\lambda_{j}\)s that define orthogonal constraints, i.e., \(\lambda_{j}\)s for which \(\rho^{\prime}_{j}M\rho_{k}=0\) when \(j\neq k\), \(T_{\Lambda}^{2}=T_{1}^{2}+\cdots+T_{s}^{2}\), but no similar decompositions seem to work for the other statistics.

For any \(q\times r\) matrix \(W\), all of these ideas apply immediately to testing \(H_{0j}:\lambda^{\prime}_{j}BW=0\), \(j=1,\ldots,s\), \(H_{0}:\Lambda^{\prime}BW=0\), and \(H_{0}:\lambda^{\prime}BW=0\), \(\lambda\in C(\Lambda)\). You just apply the ideas to the multivariate linear model \(YW=XBW+eW\).

**Exercise 9.6**.: Let \(A\), \(A_{1}\), \(A_{2}\), and \(A_{1}-A_{2}\) be nonnegative definite and let \(B\) be positive definite.

1. Provide a simple induction proof that for nonnegative \(\phi_{h}\)s, \(\prod_{h=1}^{q}(1+\phi_{h})\geq 1+\prod_{h=1}^{q}\phi_{h}\).
2. Show that \[|A+B|\geq|A||I+A^{-1/2}BA^{-1/2}|\geq|A|[1+|A^{-1/2}BA^{-1/2}|]\geq|A|.\]
3. Show that \[|A+B|\geq|A||I+A^{-1/2}BA^{-1/2}|\geq|A|[1+|A^{-1/2}BA^{-1/2}|]\geq|A|.\]\[|I+E^{-1/2}H_{\Lambda}E^{-1/2}|=\] \[|I+E^{-1/2}H_{\lambda}E^{-1/2}+E^{-1/2}[H_{\Lambda}-H_{\lambda}]E^{-1/2}| \geq|I+E^{-1/2}H_{\lambda}E^{-1/2}|\] so that \(U_{\Lambda}\leq U_{\lambda}\).
4. Show that \(\xi^{\prime}Y^{\prime}M_{MP}Y\xi\geq\xi^{\prime}Y^{\prime}M_{M\rho}Y\xi\) and that \(\phi_{\max,\Lambda}\geq\phi_{\max,\lambda}\).
5. Show that \(T_{\Lambda}^{2}\geq T_{\lambda}^{2}\).
6. Show that \(\operatorname{tr}[(A_{2}+B)A_{1}]\geq\operatorname{tr}[A_{2}(A_{1}+B)]\).
7. Show that \(\operatorname{tr}[A_{1}(A_{1}+B)^{-1}]\geq\operatorname{tr}[(A_{2}+B)^{-1}A_ {2}]\).
8. Show that \(V_{\Lambda}\geq V_{\lambda}\).
9. Extend the multiple testing ideas to a collection of tests involving a known matrix \(\tilde{W}\) with \(C(\tilde{W})\subset C(\Lambda^{\prime})\) and hypotheses determined by \(H_{0}\colon\Lambda^{\prime}B=\tilde{W}\).

Perhaps a more interesting multiple testing problem examines null hypotheses \(H_{0jk}:\lambda^{\prime}_{j}B\xi_{k}=0\), \(j=1,\ldots,s\), \(k=1,\ldots,r\). The Bonferroni idea would be to test each hypothesis at the \(\alpha/rs\) level. Alternatively, similar to \(\Lambda\), write \(W=[\xi_{1},\ldots,\xi_{r}]\) so that the multiple testing problem is related to testing \(\Lambda^{\prime}BW=0\) but also to \(\lambda^{\prime}B\xi\) whenever \(\lambda\in C(\Lambda)\) and \(\xi\in C(W)\). An alternative to Bonferroni is to test \(\Lambda^{\prime}BW=0\) first and not declare any \(\lambda^{\prime}B\xi\neq 0\) unless the first test is rejected. Note that

\[\Lambda^{\prime}BW=0\]

iff

\[\lambda^{\prime}B\xi=0,\quad\forall\lambda\in C(\Lambda),\,\xi\in C(W)\]

iff

\[\lambda^{\prime}_{j}B\xi_{k}=0,\quad j=1,\ldots,s,\,k=1,\ldots,r.\]

Thus

\[\Lambda^{\prime}BW\neq 0\]

iff

\[\exists\lambda\in C(\Lambda),\,\xi\in C(W)\text{ such that }\lambda^{\prime}B \xi\neq 0\]

iff

\[\exists j,k\text{ such that }\lambda^{\prime}_{j}B\xi_{k}\neq 0.\]

The fact that no \(\lambda^{\prime}B\xi=0\) is rejected unless \(\Lambda^{\prime}BW=0\) is rejected immediately controls the weak experimentwise error rate. If \(\Lambda^{\prime}BW=0\) is rejected, LSD then just tests each \(H_{0jk}:\lambda^{\prime}_{j}B\xi_{k}=0\) at the \(\alpha\) level. (Recall LSD does not work well for arbitrary \(\lambda^{\prime}B\xi=0\).) Of course, if you perform a test of \(\Lambda^{\prime}BW=0\) first, and quit if it is not rejected, any scheme for declaring some of \(\lambda^{\prime}B\xi=0\) significant has controlled the weak experimentwise error rate. It is not clear how to apply the Scheffe idea except for Roy's test.

Roy's method of test construction for

\[H_{0}\colon\Lambda^{\prime}BW=0\]

is to maximize the \(F\) statistics for the univariate hypotheses \[H_{0}\colon\,\Lambda^{\prime}BW\zeta=0.\]

In particular, this is

\[\sup_{\zeta}\frac{\left(\Lambda^{\prime}\hat{B}W\zeta\right)^{\prime}\left[\Lambda ^{\prime}(X^{\prime}X)^{-}\Lambda\right]^{-1}\left(\Lambda^{\prime}\hat{B}W \zeta\right)\bigg{/}r(\Lambda)}{\zeta^{\prime}W^{\prime}SW\zeta}=\frac{n-r(X)} {r(\Lambda)}\phi_{\max},\]

where \(\phi_{\max}\) is the maximum eigenvalue of \((W^{\prime}Y^{\prime}M_{MP}YW)[W^{\prime}Y^{\prime}(I-M)YW]^{-1}\). It immediately follows that

\[\frac{\left(\Lambda^{\prime}\hat{B}W\zeta\right)^{\prime}\left[\Lambda^{\prime} (X^{\prime}X)^{-}\Lambda\right]^{-1}\left(\Lambda^{\prime}\hat{B}W\zeta\right) \bigg{/}r(\Lambda)}{\zeta^{\prime}W^{\prime}SW\zeta}\leq\frac{n-r(X)}{r(\Lambda )}\phi_{\max}, \tag{9.8.3}\]

with equality achieved for some \(\zeta\) (cf., Sect. 9.6).

Moreover, from our knowledge of univariate tests (cf., \(PA\) Sect. 5.1)

\[\left(\lambda^{\prime}\hat{B}W\zeta\right)^{\prime}\left[\lambda ^{\prime}(X^{\prime}X)^{-}\lambda\right]^{-1}\left(\lambda^{\prime}\hat{B}W \zeta\right)=\zeta^{\prime}W^{\prime}Y^{\prime}M_{M\rho}YW\zeta\\ \leq\zeta^{\prime}W^{\prime}Y^{\prime}M_{MP}YW\zeta=\left(\Lambda ^{\prime}\hat{B}W\zeta\right)^{\prime}\left[\Lambda^{\prime}(X^{\prime}X)^{-} \Lambda\right]^{-1}\left(\Lambda^{\prime}\hat{B}W\zeta\right)\]

and there exists a \(\lambda\) that achieves equality, namely \(\lambda^{\prime}=\rho^{\prime}X\) with \(\rho=M_{MP}YW\zeta\).

Taking \(\xi=W\zeta\), it follows that the multiple comparison procedure that rejects \(H_{0}\colon\lambda^{\prime}B\xi=0\) if and only if

\[\frac{\left(\lambda^{\prime}\hat{B}\xi\right)^{2}\bigg{/}r(\Lambda)}{\left[ \lambda^{\prime}(X^{\prime}X)^{-}\lambda\right]\xi S\xi}>\frac{n-r(X)}{r( \Lambda)}\phi_{\max}\left[1-\alpha,r,r(\Lambda),n-r(X)\right]\]

has an experimentwise error rate no greater than \(\alpha\) when applied to testing any or all hypotheses of the form \(\lambda^{\prime}B\xi=0\) and there exists a pair \(\lambda\) and \(\xi\) that will reject whenever the test of \(\Lambda^{\prime}BW=0\) is rejected, so the experimentwise error rate for testing all hypotheses of the form \(H_{0}\colon\lambda^{\prime}BW\xi=0\) is precisely \(\alpha\).

For testing something less than all hypotheses, such as a finite number \(H_{0jk}\colon\lambda^{\prime}_{j}B\xi_{k}=0\), this method, first developed in Roy and Bose (1953), has an experimentwise error rate that is actually something less that \(\alpha\). Typically for the methods discussed in this section the actual experimentwise error rate is something less that \(\alpha\).

As with Scheffe's method, the Roy-Bose procedure for controlling the simultaneous error rate of multiple tests can be adapted to providing simultaneous confidence intervals. With confidence coefficient \((1-\alpha)100\%\), the intervals

\[\lambda^{\prime}\hat{B}\xi\pm\sqrt{\xi^{\prime}E\xi\left[\lambda^{\prime}(X^{ \prime}X)^{-}\lambda\right]\phi_{\max}\left[1-\alpha,q,r(\Lambda),n-r(X) \right]}\]

contain all parameters of the form \(\lambda^{\prime}B\xi\).

Exercise 9.7. Show that the constraint imposed on a univariate linear model by any hypothesis \(H_{0}\) : \(\zeta^{\prime}\Lambda^{\prime}\beta=0\) is contained in the constraint subspace determined by the estimable hypothesis \(H_{0}\) : \(\Lambda^{\prime}\beta=0\). Hint: Review _PA_ Sect. 3.3.

### Multivariate Time Series and Spatial Models

Model (9.1.1) is easily generalized into a model that has common spatial-temporal relationships between the \(n\) observation units but an arbitrary correlation matrix associated with \(q\) measurements on each unit. Again, write the multivariate model \(Y=XB+e\), \(\mathrm{E}(e)=0\) as

\[\mathrm{Vec}(Y)=[I_{q}\otimes X]\mathrm{Vec}(B)+\mathrm{Vec}(e),\qquad\mathrm{ E}[\mathrm{Vec}(e)]=0 \tag{9.9.1}\]

but now we incorporate a common spatial-temporal relationship among the observational units by defining

\[\mathrm{Cov}[\mathrm{Vec}(e)]=[\Sigma\otimes V(\phi)]\,, \tag{9.9.2}\]

where \(V(\phi)\) can incorporate the correlation structure of any of our standard time series or spatial models.

When \(V(\phi)\) is known it is not difficult to show that the BLUEs for this model are the individual generalized least squares estimates discussed in _PA_ Sect. 2.7, i.e.,

\[X\hat{B}=AY\,;\qquad A\equiv X\left[X^{\prime}V^{-1}(\phi)X\right]^{-}X^{ \prime}V^{-1}(\phi) \tag{9.9.3}\]

and that an unbiased estimate of \(\Sigma\) is

\[S(\phi)\equiv\frac{1}{n-r(X)}Y^{\prime}(I-A)^{\prime}V^{-1}(\phi)(I-A)Y.\]

Unfortunately, \(\phi\) is never known, so these are not really estimates. Of course maximum likelihood and REML can be used to estimate \(\phi\) and \(\Sigma\), but depending on how \(V(\phi)\) is defined, there may be identifiability issues with the parameters, e.g. you might want to make \(V(\phi)\) a correlation matrix rather than a covariance matrix.

Exercise 9.8. Prove that for \(V(\phi)\) known, solutions to (9.9.3) are BLUEs and that \(S\) is unbiased for \(\Sigma\).

#### A Tensor Model

Consider a sequence of spatial multivariate linear models\[\mathcal{Y}_{t}=XB_{t}+e_{t}\qquad t=1,\ldots,T,\]

where each \(\mathcal{Y}_{t}\) is an \(n\times q\) matrix of observations. For each \(t\) turn the multivariate model into a univariate linear model

\[\text{Vec}(\mathcal{Y}_{t})=[I_{q}\otimes X]\text{Vec}(B_{t})+\text{Vec}(e_{t} ),\quad\text{E}[\text{Vec}(e_{t})]=0,\quad\text{Cov}[\text{Vec}(e_{t})]=[ \Sigma\otimes V].\]

Combine the univariate versions of the multivariate models into another spatial multivariate linear model,

\[[\text{Vec}(\mathcal{Y}_{1})\cdots\text{Vec}(\mathcal{Y}_{T})]=[I_{q}\otimes X ][\text{Vec}(B_{1})\cdots\text{Vec}(B_{T})]+[\text{Vec}(e_{1})\cdots\text{Vec }(e_{T})]\]

wherein the observation matrix is now \(nq\times T\) and the rows of this matrix are assumed to have a covariance structure that is a multiple of some matrix \(\Psi\). We can again turn this into a univariate linear model

\[\begin{bmatrix}\text{Vec}(\mathcal{Y}_{1})\\ \vdots\\ \text{Vec}(\mathcal{Y}_{T})\end{bmatrix}=[I_{T}\otimes I_{q}\otimes X] \begin{bmatrix}\text{Vec}(B_{1})\\ \vdots\\ \text{Vec}(B_{T})\end{bmatrix}+\begin{bmatrix}\text{Vec}(e_{1})\\ \vdots\\ \text{Vec}(e_{T})\end{bmatrix} \tag{9.9.4}\]

wherein the covariance matrix is

\[[\Psi\otimes\Sigma\otimes V].\]

If we think of \(\mathcal{Y}\) as a three-dimensional matrix with subscripts \(iht\), \(i=1,\ldots,n\), \(h=1,\ldots,q\), \(t=1,\ldots,T\) we can define \(\text{Vec}(\mathcal{Y})\) as the dependent variable vector in model (9.9.4). This is essentially a three-dimensional tensor notation and similar to the notation used for ANOVA models in \(PA\).

### Additional Exercises

#### Exercise 9.10.1

Show that if \(W\sim W(n,\Sigma,0)\), then

\[AWA^{\prime}\sim W(n,A\Sigma A^{\prime},0).\]

#### Exercise 9.10.2

Show that if \(W_{1},\cdots W_{r}\) are independent with \(W_{i}\sim W(n_{i},\Sigma,0)\), then

\[\sum_{i=1}^{r}W_{i}\sim W\left(\sum_{i=1}^{r}n_{i},\Sigma,0\right).\]

#### Exercise 9.10.3

Show that if \(W\sim W(n,\Sigma,0)\), then \[\frac{\lambda^{\prime}W\lambda}{\lambda^{\prime}\Sigma\lambda}\sim\chi^{2}(n,0).\]

**Exercise 9.10.4.**  For \(i=1,2,3\), let \(y_{i}\sim N\left(\mu+(i-2)\xi,\Sigma\right)\), where \(\Sigma\) is known and \(y_{1},y_{2}\), and \(y_{3}\) are independent. Find the maximum likelihood estimates of \(\mu\) and \(\xi\).

**Exercise 9.10.5.**  Based on the multivariate linear model

\[Y=XB+e,\,\text{E}(e)=0,\,\text{Cov}(\varepsilon_{i},\varepsilon_{j})=\delta_{ ij}\Sigma,\]

find a 99% prediction interval for \(y_{0}^{\prime}\xi\), where \(y_{0}\) is an independent observation that is distributed \(N(B^{\prime}x_{0},\Sigma)\).

**Exercise 9.10.6.**  Let \(y_{1},y_{2},\cdots,y_{n}\) be i.i.d. \(N(X\beta,\Sigma)\), where \(\Sigma_{n\times n}\) is unknown. Show that the maximum likelihood estimate of \(X\beta\) is

\[X\beta=X(X^{\prime}E^{-1}X)^{-}X^{\prime}E^{-1}\tilde{y}.\]

**Exercise 9.10.7.**  Consider the multivariate linear model \(Y=XB+e\) and the parametric function \(\Lambda^{\prime}BW\), where \(W\) is a \(q\times r\) matrix of rank \(r\). Find simultaneous confidence intervals for all parameters of the form \(\zeta^{\prime}\Lambda^{\prime}BW\xi\).

**Exercise 9.10.8.**  Use Lemma 9.6.2 to show that if \(A\) is nonnegative definite and \(B\) is positive definite, then \(AB^{-1}\) is nonnegative definite. Hint: Show that the nonnegative definite matrix \(B^{-1/2}AB^{-1/2}\) has the same eigenvalues as \(AB^{-1}\).

**Exercise 9.10.9.**  Rewrite the multivariate linear model in terms of \(\text{Vec}(Y^{\prime})\). Write it similarly to both (9.1.2) with covariance (9.1.3) and using the Vec operator with Kronecker products as in (9.1.4) with (9.1.5).

**Exercise 9.10.10.**  In the multivariate linear model \(Y=XB+e\) the likelihood equations reduce to

\[\text{tr}\left\{\Sigma^{-1}[\mathbf{d}_{\theta_{j}}\Sigma]\right\}=\text{tr} \left\{\Sigma^{-1}[\mathbf{d}_{\theta_{j}}\Sigma]\Sigma^{-1}\hat{\Sigma}\right\},\]

for \(j=1,\ldots,s\) where \(\hat{\Sigma}\equiv Y^{\prime}(I-M)Y/n\) and \(\theta_{1},\theta_{2},\ldots,\theta_{s}\) from Chap. 4 correspond to \(\sigma_{gh}\) for \(g\leq h\) so that \(s=q(q+1)/2\). We recognize that \(\Sigma=\hat{\Sigma}\) provides a solution to the likelihood equations. Now suppose that \(\Sigma\equiv\sigma^{2}[(1-\rho)I_{1}+\rho J_{q}^{q}]\), so that \(s=2\) and \((\theta_{1},\theta_{2})=(\sigma^{2},\rho)\). It is clear that \(\Sigma=\hat{\Sigma}\) still provides a solution to the likelihood equations, why is \(\hat{\Sigma}\) no longer the MLE? (The argument has almost nothing to do with the particular parametric form that we chose for \(\Sigma\).)

#### Exercise 9.10.11

Use the univariate linear model form of the multivariate linear model and the likelihood equations from Sect. 4.3 to obtain the MLEs for the multivariate linear model.

## References

* Anderson (2003) Anderson, T. W. (2003). _An introduction to multivariate statistical analysis_ (3rd ed.). New York: Wiley.
* Arnold (1981) Arnold, S. F. (1981). _The theory of linear models and multivariate analysis_. New York: Wiley.
* Christensen (1996) Christensen, R. (1996). _Analysis of variance, design, and regression: Applied statistical methods_. London: Chapman and Hall.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York: Springer.
* Dillon & Goldstein (1984) Dillon, W. R., & Goldstein, M. (1984). _Multivariate analysis: Methods and applications_. New York: Wiley.
* Eaton (1983) Eaton, M. L. (1983). _Multivariate statistics: A vector space approach_. New York: Wiley.
* Everitt & Hothorn (2011) Everitt, B., & Hothorn, T. (2011). _An introduction to applied multivariate analysis with R_. New York: Springer.
* Gnanadesikan (1977) Gnanadesikan, R. (1977). _Methods for statistical data analysis of multivariate observations_. New York: Wiley.
* Heck (1960) Heck, D. L. (1960). Charts of some upper percentage points of the distribution of the largest characteristic root. _Annals of Mathematical Statistics, 31_, 625-642.
* Johnson & Wichern (2007) Johnson, R. A., & Wichern, D. W. (2007). _Applied multivariate statistical analysis_ (6th ed.). Englewood Cliffs, NJ: Prentice-Hall
* Kres (1983) Kres, H. (1983). _Statistical tables for multivariate analysis_. New York: Springer.
* McKeon (1974) McKeon, J. J. (1974). \(F\) approximations to the distribution of Hotelling's \(T_{0}^{2}\). _Biometrika, 61_, 381-383.
* Marden (2015) Marden, J. I. (2015). _Multivariate statistics: Old school_. [http://stat.istics.net/Multivariate](http://stat.istics.net/Multivariate)
* Mardia et al. (1979) Mardia, K. V., Kent, J. T., & Bibby, J. M. (1979). _Multivariate analysis_. New York: Academic.
* Morrison (2004) Morrison, D. F. (2004). _Multivariate statistical methods_ (4th ed.). Pacific Grove, CA: Duxbury Press.
* Muirhead (1982) Muirhead, R. J. (1982). _Aspects of multivariate statistical theory_. New York: Wiley.

* Okamoto (1973) Okamoto, M. (1973). Distinctness of the eigenvalues of a quadratic form in a multivariate sample. _Annals of Statistics, 1_, 763-765.
* Press (1982) Press, S. J. (1982). _Applied multivariate analysis: Using Bayesian and frequentist methods of inference_ (2nd ed.). Malabar, FL: R.E. Krieger (Latest reprinting, Dover Press, 2005).
* Rao (1951) Rao, C. R. (1951). An asymptotic expansion of the distribution of Wilks' criterion. _Bulletin of the International Statistical Institute, 33_, 177-180.
* Rencher & Christensen (2012) Rencher, A. C., & Christensen, W. F. (2012). _Methods of multivariate analysis_ (3rd ed.). New York: Wiley.
* Roy (1953) Roy, S. N. (1953). On a heuristic method of test construction and its use in multivariate analysis. _Annals of Mathematical Statistics, 24_, 220-238.
* Roy & Bose (1953) Roy, S. N., & Bose, R. C. (1953). Simultaneous confidence interval estimation. _Annals of Mathematical Statistics, 24_, 513-536.
* Seber (1984) Seber, G. A. F. (1984). _Multivariate observations_. New York: Wiley.
* Tarpey et al. (2015) Tarpey, T., Ogden, R. T., Petkova, E., & Christensen, R. (2015). A paradoxical result in estimating regression coefficients. _The American Statistician, 68_, 271-276.

## Chapter 10 Multivariate Linear Models: Applications

**Abstract** This chapter applies the results of Chap. 9 to the one-sample, two-sample, and one-way ANOVA problems. A major tool in MANOVA is profile analysis, which is analogous to performing a split-plot analysis. Profile analysis leads us to the consideration of generalized multivariate linear models (growth curve models, GMANOVA models). Finally, we consider testing for whether a subset of the dependent variables actually provides us with additional information over and above the variables not considered in the subset. In Chap. 12 testing for additional information is seen as an important tool in linear discriminant analysis.

### 10.1 One-Sample Problems

The multivariate one-sample problem has the same linear structure as the univariate one-sample problem that was explored in _PA_ Exercises 2.3 and 3.3. Let \(y_{1},\ldots,y_{n}\) be i.i.d. \(N(\mu,\Sigma)\), where \(\mu\) is \(q\times 1\) and \(\Sigma\) is \(q\times q\). Write \(y^{\prime}_{i}=(y_{i1},\ldots,y_{iq})\),

\[Y=\begin{bmatrix}y^{\prime}_{1}\\ \vdots\\ y^{\prime}_{n}\end{bmatrix},\]

and

\[Y=J\mu^{\prime}+e,\]

where again \(J\) is an \(n\times 1\) vector of 1s. The ppo onto \(C(J)\) is \(\frac{1}{n}J^{n}_{n}\) so by the Fundamental Theorem of Least Squares Estimation, least squares estimates satisfy

\[J\beta^{\prime}=\frac{1}{n}J^{n}_{n}Y=J\bar{y^{\prime}},\]

where \(\bar{y^{\prime}}=\frac{1}{n}\sum_{i=1}^{n}y^{\prime}_{i}=(\bar{y}_{\cdot 1}, \ldots,\bar{y}_{\cdot q})\). The sample mean \(\bar{y}_{\cdot}\) is also the MLE of \(\mu\). The MLE of \(\Sigma\) is\[\hat{\Sigma}=\frac{n-1}{n}S = \frac{1}{n}Y^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Y\] \[= \frac{1}{n}\left[\left(I-\frac{1}{n}J_{n}^{n}\right)Y\right]^{ \prime}\left[\left(I-\frac{1}{n}J_{n}^{n}\right)Y\right]\] \[= \frac{1}{n}[(y_{1}-\bar{y}.),\ldots,(y_{n}-\bar{y}.)]\left[\begin{array}{c}(y_{1}-\bar{y}.)^{\prime}\\ \vdots\\ (y_{n}-\bar{y}.)^{\prime}\end{array}\right]\] \[= \frac{1}{n}\sum_{i=1}^{n}(y_{i}-\bar{y}.)(y_{i}-\bar{y}.)^{\prime}\,.\]

In particular, writing \(\hat{\Sigma}=[\hat{\sigma}_{jk}]\), we have

\[\hat{\sigma}_{jk} = \frac{1}{n}Y_{j}^{\prime}\left(I-\frac{1}{n}J_{n}^{n}\right)Y_{k}\] \[= \frac{1}{n}\left[\left(I-\frac{1}{n}J_{n}^{n}\right)Y_{j}\right]^{\prime}\left[\left(I-\frac{1}{n}J_{n}^{n}\right)Y_{k}\right]\] \[= \frac{1}{n}\sum_{i=1}^{n}(y_{ij}-\bar{y}._{j})(y_{ik}-\bar{y}._{k})\,.\]

The standard unbiased estimate of the covariance is \(s_{jk}=\hat{\sigma}_{jk}[n/(n-1)]\).

To test the hypothesis \(H_{0}:\mu=\mu_{0}\) we need to recognize that, because the one-sample model is a regression model, \(\mu\) is estimable, and \(\Lambda^{\prime}\) is just the scalar 1. The hypothesis and error statistics are

\[H = (\Lambda^{\prime}\hat{B}-W)^{\prime}[\Lambda^{\prime}(X^{\prime}X)^{-}\Lambda]^{-}(\Lambda^{\prime}\hat{B}-W)\] \[= (\bar{y}.-\mu_{0}^{\prime})^{\prime}[1(\Lambda^{\prime}J)^{-1}1]^{-1}(\bar{y}.-\mu_{0}^{\prime})\] \[= n(\bar{y}.-\mu_{0})(\bar{y}.-\mu_{0})^{\prime}\]

and

\[E=(n-1)S\,.\]

In the one-sample problem, the Lawley-Hotelling trace is the famous Hotelling \(T^{2}\) statistic.

\[T^{2} = (n-1)\,\mbox{tr}[HE^{-1}]\] \[= \mbox{tr}\big{[}n(\bar{y}.-\mu_{0})(\bar{y}.-\mu_{0})^{\prime}S^{-1}\big{]}\] \[= n\,\mbox{tr}[(\bar{y}.-\mu_{0})^{\prime}S^{-1}(\bar{y}.-\mu_{0})]\] \[= n(\bar{y}.-\mu_{0})^{\prime}S^{-1}(\bar{y}.-\mu_{0})\,.\]

The reason the test statistic simplifies so nicely is because \(H\) is a rank one matrix, which also implies that the other three test statistics are equivalent. Under \(H_{0}\),

\[\frac{T^{2}}{(n-1)}\frac{n-q}{q}\sim F(q,n-q).\]This follows from the exact part of McKeon's approximation, see also Seber (1984, Section 2.4). The test is rejected for large values of \(T^{2}\). There is no need for any unusual tables to perform the test.

Arguments similar to those given in Sect. 9.7 for the development of the prediction region yield a \((1-\alpha)100\%\) confidence ellipsoid for \(\mu\) consisting of all \(\mu\) vectors that satisfy

\[(\mu-\bar{y}.)^{\prime}S^{-1}(\mu-\bar{y}.)\leq F(1-\alpha,q,n-q)\frac{q}{n-q} \,\frac{n-1}{n}.\]

**Example 10.1.1**: Mosteller and Tukey (1977) consider data from _The Coleman Report_ on the relationships between several variables and mean verbal test scores for sixth graders at twenty schools in the New England and Mid-Atlantic regions of the United States. The data are also given in Christensen (2015, Table 6.14). In this example, we consider only two variables, \(x_{2}\), the percentage of sixth-graders' fathers employed in white-collar jobs and \(x_{5}\), one-half of the sixth-graders' mothers' mean number of years of schooling. The data are given in Table 10.1.

In particular, we will test the null hypothesis that the percentage of white-collar fathers is 50% and that the mothers are on average high-school graduates (i.e., have 12 years of schooling). To perform a formal test of this hypothesis, we need to establish that the data have a multivariate normal distribution. A normal plot (see _PA-V_ Sect. 12.2 or Christensen 2011, Sect. 13.2) of \(x_{2}\) is given in Fig. 10.1a. It is not very encouraging. It has a very noticeable shoulder at the low end. It also has a gap and a flat spot in the middle. The Shapiro-Francia statistic for the plot is \(W^{\prime}=0.904\), which has a \(P\) value of about 0.05. Figure 10.1b contains a normal plot of \(\sqrt{x_{2}}\). This is much better behaved and has a \(W^{\prime}\) value of 0.933. The variance stabilizing transformation \(\mathrm{Arcsin}(\sqrt{x_{2}/100})\) was also considered, but its behavior was actually worse than that of \(\sqrt{x_{2}}\). The normal plot for \(x_{5}\) in Fig. 10.1c has a minor shoulder near the top but a \(W^{\prime}\) value of 0.976; we leave \(x_{5}\) untransformed. Figure 10.2 contains a plot of \(\sqrt{x_{2}}\) versus \(x_{5}\). This should look elliptical. Except for the smallest value of \(x_{5}\), it is not too bad.

\begin{table}
\begin{tabular}{|c c c|c c c|} \hline \(x_{2}\) & \(\sqrt{x_{2}}\) & \(x_{5}\) & \(x_{2}\) & \(\sqrt{x_{2}}\) & \(x_{5}\) \\ \hline
28.87 & 5.37 & 6.19 & 12.20 & 3.49 & 5.62 \\
20.10 & 4.48 & 5.17 & 22.55 & 4.75 & 5.34 \\
69.05 & 8.31 & 7.04 & 14.30 & 3.78 & 5.80 \\
65.40 & 8.09 & 7.10 & 31.79 & 5.64 & 6.19 \\
29.59 & 5.44 & 6.15 & 11.60 & 3.41 & 5.62 \\
44.82 & 6.69 & 6.41 & 68.47 & 8.27 & 6.94 \\
77.37 & 8.80 & 6.86 & 42.64 & 6.53 & 6.33 \\
24.67 & 4.97 & 5.78 & 16.70 & 4.09 & 6.01 \\
65.01 & 8.06 & 6.51 & 86.27 & 9.29 & 7.51 \\
9.99 & 3.16 & 5.57 & 76.73 & 8.76 & 6.96 \\ \hline \end{tabular}
\end{table}
Table 10.1: Coleman report data To test the hypothesis, we need the statistics

\[S = \left[ {\matrix{ 4.288 & 1.244 \cr 1.244 & 0.428 \cr}} \right]\]

and

\[\vec{y}_{\cdot}^{\prime} = (6.069,6.255).\]

The hypothesized mean is

\[\mu_{0}^{\prime} = (\sqrt{50},12/2),\]

and Hotelling's statistic is

\[T^{2} = 20(\vec{y}_{\cdot}-\mu_{0})^{\prime}S^{-1}(\vec{y}_{\cdot}-\mu_{0}) = 93.45.\]

The comparison value for \(T^{2}\) is 2 so \(T^{2}\) seems to be highly significant. To perform a formal test based on multivariate normality, compute

\[{T^{2} \over n - 1}{n - q\over q} = {93.45 \over 19}{18 \over 2} = 44.27\]and compare it to an \(F(2,18)\) distribution. Again, the result is highly significant.

We have not yet sufficiently analyzed the question of multivariate normality. Not only must the marginal distributions of a multivariate normal be normal but all linear combinations of the variables must also be normal. Figure 10.1d contains a normal plot of \(\sqrt{x_{2}}+x_{5}\). While this has a not unrespectable \(W^{\prime}\) value of 0.925, the plot looks horrible. The normal plot for \(\sqrt{x_{2}}-x_{5}\) is also disturbing. Nevertheless, the null model is so clearly untrue that the lack of multivariate normality is probably not crucial. \(\Box\)

### Two-Sample Problems

_PA_ Exercises 2.2 and 3.2 examine the univariate two-sample problem. The multivariate two-sample problem has a similar linear structure. Let \(y_{11},\ldots,y_{1r}\) be i.i.d. \(N_{q}(\mu_{1},\Sigma)\), let \(y_{21},\ldots,y_{2t}\) be i.i.d. \(N_{q}(\mu_{2},\Sigma)\), and let the two samples be independent. Write

\[\hat{Y}_{1}=\begin{bmatrix}y_{11}^{\prime}\\ \vdots\\ \vdots\\ y_{1r}^{\prime}\end{bmatrix},\quad\hat{Y}_{2}=\begin{bmatrix}y_{21}^{\prime} \\ \vdots\\ y_{2t}^{\prime}\end{bmatrix}\]and

\[Y=\begin{bmatrix}\tilde{Y}_{1}\\ \tilde{Y}_{2}\end{bmatrix}.\]

The multivariate linear model is

\[Y=\begin{bmatrix}J_{r}&0\\ 0&J_{t}\end{bmatrix}\begin{bmatrix}\mu_{1}^{\prime}\\ \mu_{2}^{\prime}\end{bmatrix}+e.\]

The Fundamental Theorem of Least Squares Estimation gives

\[\begin{bmatrix}J_{r}\hat{\mu}_{1}^{\prime}\\ J_{t}\hat{\mu}_{2}^{\prime}\end{bmatrix}=\begin{bmatrix}J_{r}&0\\ 0&J_{t}\end{bmatrix}\begin{bmatrix}\hat{\mu}_{1}^{\prime}\\ \hat{\mu}_{2}^{\prime}\end{bmatrix}=\begin{bmatrix}\frac{1}{r}J_{r}^{r}&0\\ 0&\frac{1}{t}J_{t}^{t}\end{bmatrix}\begin{bmatrix}\tilde{Y}_{1}\\ \tilde{Y}_{2}\end{bmatrix}=\begin{bmatrix}J_{r}\tilde{y}_{1}^{\prime}.\\ J_{t}\tilde{y}_{2}^{\prime}.\end{bmatrix},\]

so \(\hat{\mu}_{1}=\tilde{y}_{1}.\) and \(\hat{\mu}_{2}=\tilde{y}_{2}.\) It follows that

\[(I-M)Y=\begin{bmatrix}\tilde{Y}_{1}-J_{r}\tilde{y}_{1}^{\prime}\\ \tilde{Y}_{2}-J_{t}\tilde{y}_{2}.\end{bmatrix},\]

hence

\[S=\frac{(r-1)S_{1}+(t-1)S_{2}}{r+t-2}, \tag{10.2.1}\]

where

\[S_{1}=\frac{1}{r-1}\begin{bmatrix}\tilde{Y}_{1}-J_{r}\tilde{y}_{1}^{\prime}. \end{bmatrix}^{\prime}\begin{bmatrix}\tilde{Y}_{1}-J_{r}\tilde{y}_{1}^{\prime}. \end{bmatrix}=\frac{1}{r-1}\sum_{j=1}^{r}(y_{1j}-\tilde{y}_{1}.)(y_{1j}-\tilde {y}_{1}.)^{\prime} \tag{10.2.2}\]

and

\[S_{2}=\frac{1}{t-1}\begin{bmatrix}\tilde{Y}_{2}-J_{t}\tilde{y}_{2}^{\prime}. \end{bmatrix}^{\prime}\begin{bmatrix}\tilde{Y}_{2}-J_{t}\tilde{y}_{2}^{\prime}. \end{bmatrix}=\frac{1}{t-1}\sum_{j=1}^{t}(y_{2j}-\tilde{y}_{2}.)(y_{2j}-\tilde {y}_{2}.)^{\prime}. \tag{10.2.3}\]

##### Exercise 10.1.

Prove formulas (10.2.1)-(10.2.3).

To test \(\mu_{1}=\mu_{2}\), write the hypothesis as \(H_{0}\colon\mu_{1}-\mu_{2}=0\). The null hypothesis is equivalent to assuming a reduced model

\[Y=J_{n}\mu^{\prime}+e,\]

where \(n=r+t\) and \(\mu\) is a \(q\times 1\) vector. The parametric form of the hypothesis statistic is

\[H =(\tilde{y}_{1}.-\tilde{y}_{2}.)\begin{bmatrix}(1,-1)\begin{pmatrix} r&0\\ 0&t\end{pmatrix}^{-1}\begin{pmatrix}1\\ -1\end{pmatrix}\end{bmatrix}^{-1}(\tilde{y}_{1}.-\tilde{y}_{2}.)^{\prime}\] \[=\left(\frac{1}{r}+\frac{1}{t}\right)^{-1}(\tilde{y}_{1}.-\tilde{ y}_{2}.)(\tilde{y}_{1}.-\tilde{y}_{2}.)^{\prime}.\]

The error statistic is

\[E=(r+t-2)S.\]\[\bar{y}_{1\cdot}=\begin{bmatrix}194.47\\ 267.05\\ 137.37\\ 185.95\end{bmatrix},\qquad\bar{y}_{2\cdot}=\begin{bmatrix}179.55\\ 290.80\\ 157.20\\ 209.25\end{bmatrix}.\]The individual estimated covariance matrices are

\[S_{1}=\left[\begin{array}{cccc}187.596&176.863&48.371&113.582\\ 176.863&345.386&75.980&118.781\\ 48.371&75.980&66.357&16.243\\ 113.582&118.781&16.243&239.941\end{array}\right]\]

and

\[S_{2}=\left[\begin{array}{cccc}101.839&128.063&36.989&32.592\\ 128.063&389.010&165.358&94.368\\ 36.989&165.358&167.537&66.526\\ 32.592&94.368&66.526&177.882\end{array}\right].\]

The pooled estimate of the covariance matrix is

\[S=\left[\begin{array}{cccc}143.559&151.803&42.527&71.993\\ 151.803&367.788&121.877&106.245\\ 42.527&121.877&118.314&42.064\\ 71.993&106.245&42.064&208.073\end{array}\right].\]

To test the hypothesis \(H_{0}\colon\mu_{1}-\mu_{2}=0\), we need the statistics

\[(\bar{y}_{1\cdot}-\bar{y}_{2\cdot})^{\prime}=(14.92,-23.75,-19.83,-23.30)\]

and

\[S^{-1}=\left[\begin{array}{cccc}0.0132580&-0.0053492&0.0015135&-0.0021618\\ -0.0053492&0.0066679&-0.0047338&-0.0005969\\ 0.0015135&-0.0047338&0.0130491&-0.0007445\\ -0.0021618&-0.0005969&-0.0007445&0.0060093\end{array}\right].\]

These yield

\[(\bar{y}_{1\cdot}-\bar{y}_{2\cdot})^{\prime}S^{-1}(\bar{y}_{1\cdot}-\bar{y}_{2 \cdot})=13.70\]

and Hotelling's test statistic

\[T^{2}=\left(\frac{19\cdot 20}{19+20}\right)\,13.70=133.5\,.\]

The test statistic is huge, so one can feel reasonably confident that the populations are different in spite of any doubts about the validity of the assumptions. Standardizing \(T^{2}\) so that it can be compared to an \(F\) distribution gives

\[\frac{T^{2}}{19+20-2}\,\frac{19+20-4-1}{4}=30.66.\]

The corresponding reference distribution is an \(F(4,34)\).

### 10.3 One-Way Analysis of Variance and Profile Analysis

Consider a multivariate one-way analysis of variance

\[y^{\prime}_{ij}=\mu^{\prime}_{i}+\varepsilon^{\prime}_{ij}, \tag{10.3.1}\]

where \(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\), the \(\varepsilon_{ij}\)s are independent \(N(0,\Sigma)\) random vectors, and \(\mu_{i}=(\mu_{i1},\cdots,\mu_{iq})^{\prime}\). One can do the standard test of \(H_{0}\): \(\mu_{1}=\cdots=\mu_{a}\) by testing the MANOVA model against the reduced model

\[y^{\prime}_{ij}=\mu^{\prime}+\varepsilon^{\prime}_{ij}\,. \tag{10.3.2}\]

Write the dependent variable matrix as

\[Y=[y_{ij,h}],\]

where the pair \((i,j)\) denote a row of \(Y\) and \(h=1,\ldots,q\) denotes a column. Let \(X\) be the model matrix for the one-way MANOVA. As in _PA_ Chap. 4, \(X=[x_{ij,k}]\), where \(x_{ij,k}=\delta_{ik}\), \(k=1,\ldots,a\). This is just the standard model matrix for a univariate one-way ANOVA that is parameterized without a grand mean. Let

\[B=\begin{bmatrix}\mu^{\prime}_{1}\\ \vdots\\ \mu^{\prime}_{a}\end{bmatrix}\]

and, in conformance with \(Y=[y_{ij,h}]\), write a matrix with \(\varepsilon^{\prime}_{ij}\) as the \(ij\) row, say

\[e=[\varepsilon^{\prime}_{ij}]\,.\]

The full model (10.3.1) is

\[Y=XB+e,\]

and the reduced model (10.3.2) is

\[Y=J\mu^{\prime}+e\,.\]

Because the linear structure of \(X\) is just that of a one-way ANOVA, the analysis is similar to that of _PA_ Chap. 4. Consider the estimation of \(\Sigma\). The error matrix is

\[E=Y^{\prime}(I-M)Y=\left[Y^{\prime}_{h}(I-M)Y_{k}\right],\]

where, from _PA_ Chap. 4, the elements of this matrix are

\[Y^{\prime}_{h}(I-M)Y_{k} = \left[(I-M)Y_{h}\right]^{\prime}\left[(I-M)Y_{k}\right]\] \[= \sum_{i=1}^{a}\sum_{j=1}^{N_{i}}\left(y_{ij,h}-\bar{y}_{i\cdot j \cdot h}\right)\left(y_{ij,k}-\bar{y}_{i\cdot k}\right).\]It follows easily that

\[E=Y^{\prime}(I-M)Y=\sum_{i=1}^{a}\sum_{j=1}^{N_{i}}\left(y_{ij}-\bar{y}_{i\cdot} \right)\left(y_{ij}-\bar{y}_{i\cdot}\right)^{\prime}.\]

Thinking of the observations as separate samples and using the results of Sect. 10.1, define

\[S_{i}=\sum_{j=1}^{N_{i}}\left(y_{ij}-\bar{y}_{i\cdot}\right)\left(y_{ij}-\bar{ y}_{i\cdot}\right)^{\prime}/(N_{i}-1).\]

From the fact that

\[S=E/(n-a),\]

we can write \(S\) as a weighted average of the \(S_{i}\)s,

\[S=\sum_{i=1}^{a}\left(\frac{N_{i}-1}{n-a}\right)S_{i},\]

with \(n\equiv N_{1}+\cdots+N_{a}\). This is the usual pooled estimate of \(\Sigma\).

Estimation of the parameter \(\mu_{ih}\) is performed exactly as in a one-way ANOVA based on the model

\[Y_{h}=X\beta_{h}+e_{h},\]

where \(\beta_{h}=(\mu_{1h},\cdots,\mu_{ah})^{\prime}\). The estimate is \(\hat{\mu}_{ih}=\bar{y}_{i\cdot,h}\) which implies that

\[\hat{\mu}_{i}=\bar{y}_{i\cdot}\]

for \(i=1,\ldots,a\) and

\[\hat{B}=\begin{bmatrix}\bar{y}_{1\cdot}^{\prime}\\ \vdots\\ \bar{y}_{a\cdot}^{\prime}\end{bmatrix}.\]

Finally, the hypothesis matrix for testing the reduced model of no treatment effects, i.e., \(H_{0}\): \(\mu_{1}=\cdots=\mu_{a}\), is

\[H=Y^{\prime}\left(M-\frac{1}{n}J_{n}^{n}\right)Y=\sum_{i=1}^{a}N_{i}\left(\bar {y}_{i\cdot}-\bar{y}_{\cdot\cdot}\right)\left(\bar{y}_{i\cdot}-\bar{y}_{\cdot \cdot}\right)^{\prime}.\]

Example 10.3.1. _One-Way Analysis of Variance with Repeated Measures_.

A study was conducted to examine the effects of two drugs on heart rates. Thirty women were randomly divided into three groups of ten. An injection was given to each person. Depending on their group, women received either a placebo, drug A, or drug B. Repeated measurements of their heart rates were taken beginning at 2 min after the injection and at 5 min intervals thereafter. Four measurements were taken on each individual. The data are given in Table 10.2.

Clearly, observations taken over time on the same individual are correlated. We can consider the heart rate measurements taken at the four times to be four depen

[MISSING_PAGE_FAIL:414]

\[S_{2}=\left[\begin{array}{cccc}10.54&13.57&12.93&6.77\\ 13.57&22.54&18.62&10.12\\ 12.93&18.62&21.82&12.82\\ 6.77&10.12&12.82&8.99\end{array}\right],\]

and

\[S_{3}=\left[\begin{array}{cccc}24.10&25.11&19.18&18.89\\ 25.11&28.22&23.11&22.33\\ 19.18&23.11&24.93&19.00\\ 18.89&22.33&19.00&22.00\end{array}\right].\]

In general, a weighted average of these gives \(S\). Because the sample sizes for the drugs are all equal, the weights are equal and a simple average gives

\[S=\left[\begin{array}{cccc}17.76&17.91&13.44&8.80\\ 17.91&22.61&17.61&11.84\\ 13.44&17.61&20.03&13.57\\ 8.80&11.84&13.57&14.11\end{array}\right].\]

The error matrix is

\[E=(30-3)S=\left[\begin{array}{cccc}479.4&483.7&363.0&237.5\\ 483.7&610.5&475.6&319.7\\ 363.0&475.6&540.8&366.4\\ 237.5&319.7&366.4&381.0\end{array}\right].\]

The correlation matrix is

\[R=\left[\begin{array}{cccc}1.000&0.894&0.713&0.556\\ 0.894&1.000&0.828&0.663\\ 0.713&0.828&1.000&0.807\\ 0.556&0.663&0.807&1.000\end{array}\right].\]

It consists of the correlations between all the variables, e.g.,

\[r_{12}=0.894=\frac{17.91}{\sqrt{17.76}\sqrt{22.61}}.\]

The reason for treating these data as a multivariate one-way ANOVA was our initial claim that the observations made on an individual are correlated. This certainly seems to be borne out by the large off-diagonal elements of the correlation matrix. In fact, for normal data, we could test whether the correlations are zero. In _PA_ Sect. 6.5, a \(t\) test for partial correlations was presented. While these are not partial correlations in the usual sense, the correlations are pooled over three groups so these correlations represent a special case of partial correlations. A test of \(H_{0}\): \(\rho_{34}=0\) can be based on comparing \(\sqrt{30-4}\,(0.807)/\sqrt{1-0.807^{2}}=6.97\) to a \(t(30-4)\) distribution. The correlation is highly significant.

Of course, there are methods available for analyzing correlated data other than the multivariate linear model. One alternative is to consider a split plot model, where individual women are whole plots, drugs are whole plot treatments, and the four times are subplot treatments. (There are no blocks in the whole plots.) As discussed in _PA_ Chap. 11, the split plot model assumes that observations in different whole plots are uncorrelated while all observations in the same whole plot have a common positive correlation. In the multivariate one-way, the covariance matrix for observations on the same person is denoted by \(\Sigma\). The split plot model assumes that

\[\Sigma=\sigma^{2}\begin{bmatrix}1&\rho&\cdots&\rho\\ \rho&1&\cdots&\rho\\ \vdots&\vdots&\ddots&\vdots\\ \rho&\rho&\cdots&1\end{bmatrix}. \tag{10.3.3}\]

If this assumption is correct, the split plot model is more appropriate than the multivariate one-way ANOVA. Although the correlation matrix \(R\) does not seem very supportive of the split plot model assumption, a detailed comparison of the two analyses will be made in Example 10.3.3.

Returning to the multivariate one-way analysis of these data, we might wish to test for differences in the treatment means. If we fit the reduced model

\[y^{\prime}_{ij}=\mu^{\prime}+e^{\prime}_{ij},\]

we obtain

\[S_{0}=\begin{bmatrix}37.64&31.52&28.02&33.72\\ 31.52&39.60&37.10&32.29\\ 28.02&37.10&41.89&35.39\\ 33.72&32.29&35.39&45.37\end{bmatrix}\]

and

\[E_{0}=(30-1)S_{0}=\begin{bmatrix}1091.5&914.2&812.7&977.9\\ 914.2&1148.3&1076.0&936.4\\ 812.7&1075.9&1214.7&1026.3\\ 977.9&936.4&1026.3&1315.9\end{bmatrix}.\]

The hypothesis matrix can be computed as

\[H =Y^{\prime}(M-M_{0})Y=Y^{\prime}(I-M_{0})Y-Y^{\prime}(I-M)Y\] \[=E_{0}-E\] \[=\begin{bmatrix}612.1&430.5&449.7&740.4\\ 430.5&537.8&600.4&616.7\\ 449.7&600.4&673.9&659.9\\ 740.4&616.7&659.9&934.9\end{bmatrix}.\]

If there are no differences in the drug means, then \(H\) divided by its degrees of freedom should estimate \(\Sigma\). Computing this gives\[\frac{1}{2}H=\begin{bmatrix}306&215&225&370\\ 215&269&300&308\\ 225&300&337&330\\ 370&308&330&467\end{bmatrix}.\]

Even though this estimate has only two degrees of freedom, it is clear that this is not estimating the same thing that \(S\) is estimating.

If the data have a multivariate normal distribution, formal tests can be performed. The various test statistics, comparison values, and \(\alpha=0.01\) normal theory critical values are as follows.

\begin{tabular}{c c c c} \hline Statistic & Observed value & Comparison value & Critical value \\ \hline \(U\) & 0.0628 & 0.75 & 0.440 \\ \(\phi_{\max}\) & 5.52 & 0.07 & Less than 1.19 \\ \(T^{2}\) & 188.0 & 8 & 26.66 \\ \(V\) & 1.44 & 0.276 & Less than 0.725 \\ \hline \end{tabular}

In particular,

\[T^{2}=187.99,\]

which is much larger than the intuitive comparison value \(q\left[r(X)-r(X_{0})\right]=4(2)=8\). Comparing \(T^{2}\) to the exact small sample distribution, McKeon's approximate distribution, or the asymptotic \(\chi^{2}\) distribution with \(q\left[r(X)-r(X_{0})\right]=8\) degrees of freedom leads to the clear conclusion that the drugs have different multivariate means.

The validity of multivariate linear model tests depends on the data having a multivariate normal distribution. In particular, each dependent variable must be normal and any linear combinations of the variables must also be normal. As in _PA-V_ Sect. 12.2 (Christensen 2011, Section 13.2), the normality of data can be evaluated using normal plots. If the sample size for each drug were large, it would be appropriate to check for normality within the treatment groups. Since it is difficult to draw distributional conclusions using only ten observations, the residual matrix \(\hat{e}=(I-M)Y\) was used. In general, the standardized residuals are more appropriate for normal plots, but for this model all cases have the same leverage, so plotting the residuals would be equivalent.

Normal plots of the standardized residuals were made for each dependent variable (Fig. 10.3) and also for one linear combination of the residuals (Fig. 10.4). The linear combination was the residuals for the sum of the four variables. All of the normal plots looked reasonably linear. The Shapiro-Francia statistic \(W^{\prime}\) was also computed for each plot (see _PA-V_ Sect. 12.2 or Christensen 2011, Sect. 13.2). The results are as follows.

\begin{tabular}{c c} \hline Variable & \(W^{\prime}\) \\ \hline Time 1 & 0.986 \\ Time 2 & 0.974 \\ Time 3 & 0.949 \\ Time 4 & 0.980 \\ Sum of variables 0.960 \\ \hline \end{tabular}

Comparing these values to those tabled in Christensen (2015) gives no cause for concern. Of course, to do a proper check for multivariate normality, one should inspect the normal plot for every linear combination of the columns of the residual matrix. Unfortunately, that can be rather time-consuming. The data analyst must usually be satisfied with evaluating some finite number of linear combinations.

Another assumption of the multivariate linear model is that the covariance matrix is the same for every observation vector \(y_{i}\). As in _PA-V_ Sect. 12.4, (Christensen 2011, Section 13.4), residual plots can be performed, checking for constant variance in each of the dependent variables. Residual plots for linear combinations of the columns of the residual matrix should also display constant variance. Figure 10.5 plots the standardized residuals from each variable against their predicted values. Nothing too untoward appears.

In a one-way ANOVA, the assumption of a constant covariance matrix can also be checked by comparing the estimated covariance matrices for each of the

Figure 10.3: Heart rate data: Normal plots

individual groups. The sample covariance matrices \(S_{1},S_{2}\), and \(S_{3}\) were given earlier. Although they display some differences that seem to be fairly substantial (the estimated covariances between Time 1 and Time 4 vary from 0.73 to 18.89), taken as a whole, the covariance matrices are reasonably consistent. In fact, Bartlett's modification to the likelihood ratio test for equality of the covariance matrices gives a \(P\) value greater than 0.05 (based on a \(\chi^{2}(20)\) approximation). This in spite of the fact that the test is so notoriously sensitive to nonnormality that it is rarely used.

A visual approach to evaluating the individual covariance matrices can be based on plotting pairs of variables. The 10 pairs of observations at times 1 and 2 can be plotted for each of the three drugs. Each of these three plots should be roughly elliptical and the ellipses should have the same orientation in two-dimensional space. A similar set of \(a=3\) plots can be made for each of the \(\left(\begin{matrix}q\\ 2\end{matrix}\right)=6\) pairs of dependent variables. Given the difficulties of evaluating plots based on only ten observations, there seems to be no reason to doubt the assumption of equal covariance matrices for this example. In particular, the orientations of the three plots in each set are consistent.

Figure 4: Heart rate data: normal plot for sum

#### Profile Analysis

Profile analysis seeks to examine possible similarities between the \(\mu_{i}\) vectors. The name derives from the plots of the \(a\) theoretical curves defined by \((h,\mu_{ih})\), \(h=1,\ldots,q\). These curves are referred to as profiles. Because the \(\mu_{i}\)s are not available, the estimated profiles \((h,\hat{\mu}_{ih})\) provide a valuable visual display. (The \(q\) points available for each \(i\) are connected by line segments to obtain the \(a\) different profiles.)

Figure 10.6 contains a plot of the estimated profiles from Example 10.3.1.

Three questions are commonly asked in profile analysis. First, whether the curves are parallel. Second, whether the curves have the same average level. The average level for each curve is defined as the average over different dependent variables. The third question is whether the average curve is horizontal. The average curve is

Figure 10.5: Heart rate data: standardized residual-fitted values plotsobtained by averaging over groups. Note that the hypotheses involving averages are of particular interest when the curves are parallel. In this case, if the average levels are the same, the profiles are the same, and if the average profile is horizontal, all profiles are horizontal.

To test these questions, it is convenient to define, for a general value \(r\), the \((r-1)\times r\) matrix

\[\Lambda^{\prime}_{r}=\left[J_{r-1},-I_{r-1}\right].\]

Note that

\[\Lambda^{\prime}_{a}B=\begin{bmatrix}\mu^{\prime}_{1}-\mu^{\prime}_{2}\\ \mu^{\prime}_{1}-\mu^{\prime}_{3}\\ \vdots\\ \mu^{\prime}_{1}-\mu^{\prime}_{a}\end{bmatrix},\]

so \(H_{0}\colon\Lambda^{\prime}_{a}B=0\) is precisely \(H_{0}\colon\mu_{1}=\mu_{2}=\cdots=\mu_{a}\).

The hypothesis that the profiles are parallel is that for every possible choice of \(i\) and \(i^{\prime}\)

\[\mu_{i1}-\mu_{i^{\prime}1}=\mu_{i2}-\mu_{i^{\prime}2}=\cdots=\mu_{iq}-\mu_{i^ {\prime}q}.\]

Figure 10: Heart rate profiles

Equivalently, the hypothesis is that

\[\mu_{11}-\mu_{i1}=\cdots=\mu_{1q}-\mu_{iq}\quad\mbox{for}\quad i=2,\ldots,a\,,\]

that is, the other profiles are all parallel to the group 1 profile. Finally, the hypothesis can be written \((\mu_{11}-\mu_{i1})-(\mu_{12}-\mu_{i2})=0\), \((\mu_{11}-\mu_{i1})-(\mu_{13}-\mu_{i3})=0\),..., \((\mu_{11}-\mu_{i1})-(\mu_{1q}-\mu_{iq})=0\) for \(i=2,\ldots,a\). With \(\Lambda^{\prime}_{a}B\) illustrated earlier, it is not difficult to see that the profiles are parallel if and only if \(\Lambda^{\prime}_{a}B\Lambda_{q}=0\). The test of parallel profiles is \(H_{0}:\Lambda^{\prime}_{a}B\Lambda_{q}=0\). This is a standard multivariate hypothesis and yields the hypothesis statistic

\[H_{*}=(\Lambda^{\prime}_{a}\beta\Lambda_{q})^{\prime}[\Lambda^{\prime}_{a}(X^{ \prime}X)^{-1}\Lambda_{a}]^{-1}(\Lambda^{\prime}_{a}\beta\Lambda_{q})\,.\]

For our one-way ANOVA model matrix, \(X^{\prime}X\) is \(\mbox{Diag}(N_{1},\ldots,N_{a})\) so it is easily inverted. The hypothesis statistic is compared to

\[E_{*}=\Lambda^{\prime}_{q}E\Lambda_{q}.\]

To test whether the average levels of the curves are the same, we need the average level for each curve. For the \(i\)th curve, the average level is \(\bar{\mu}_{i.}=\frac{1}{q}\sum_{h=1}^{q}\mu_{ih}\). The hypothesis \(\bar{\mu}_{1.}=\cdots=\bar{\mu}_{a.}\) can be written as \(\bar{\mu}_{1.}-\bar{\mu}_{2.}=0\), \(\bar{\mu}_{1.}-\bar{\mu}_{3.}=0\),..., \(\bar{\mu}_{1.}-\bar{\mu}_{a.}=0\). Equivalently, we can look for equality of the curve totals, where the \(i\)th curve total is \(\mu_{i.}=\mu^{\prime}_{i}J_{q}=\sum_{h=1}^{q}\mu_{ih}\). Recalling the form of \(\Lambda^{\prime}_{a}B\), it is easily seen that the null hypothesis is \(H_{0}:\Lambda^{\prime}_{a}BJ_{q}=0\) and

\[H_{*}=(\Lambda^{\prime}_{a}\beta J_{q})^{\prime}[\Lambda^{\prime}_{a}(X^{ \prime}X)^{-1}\Lambda_{a}]^{-1}(\Lambda^{\prime}_{a}\beta J_{q})\,.\]

The error statistic is

\[E_{*}=J^{\prime}_{q}EJ_{q}.\]

The third hypothesis is that the average curve is horizontal. The average curve is based on \(\bar{\mu}^{\prime}=\frac{1}{a}J^{\prime}_{a}B=\frac{1}{a}\sum_{i=1}^{a}\mu^{ \prime}_{i}\). Write \(\bar{\mu}^{\prime}=(\bar{\mu}_{1.},\ldots,\bar{\mu}_{.q})\). Testing whether the curve is horizontal amounts to testing that \(\bar{\mu}_{1.}-\bar{\mu}_{.2}=0\),..., \(\bar{\mu}_{.1}-\bar{\mu}_{.q}=0\). Equivalently, we can test whether the total curve is horizontal by examining \(\mu^{\prime}_{.}=J^{\prime}_{a}B\). Clearly, the test that the total curve is horizontal is \(H_{0}:J^{\prime}_{a}B\Lambda_{q}=0\) with

\[H_{*}=(J^{\prime}_{a}\beta\Lambda_{q})^{\prime}[J^{\prime}_{a}(X^{\prime}X)^{ -1}J_{a}]^{-1}(J^{\prime}_{a}\beta\Lambda_{q})\]

and

\[E_{*}=\Lambda^{\prime}_{q}E\Lambda_{q}.\]

Finally, another word about combining the hypotheses. The curves are both parallel and have the same average level if and only if the curves are identical (i.e., \(\mu_{1}=\cdots=\mu_{a}\)). In other notation, \(\Lambda^{\prime}_{a}B\Lambda_{q}=0\) and \(\Lambda^{\prime}_{a}BJ_{q}=0\) if and only if \(\Lambda^{\prime}_{a}B=0\). Also, the curves are parallel and the average curve is horizontal if and only if all of the curves are horizontal. Putting it another way, \(\Lambda^{\prime}_{a}B\Lambda_{q}=0\) and \(J^{\prime}_{a}B\Lambda_{q}=0\) if and only if \(B\Lambda_{q}=0\). The hypothesis that \(B\Lambda_{q}=0\) is simply that all of the curves are horizontal.

#### Comparison with Split Plot Analysis

Example 10.3.3. We again consider the heart rate data of Example 10.3.1. It was mentioned earlier that using a split plot model is an alternative method of analyzing these data. The nature of profile analysis will be clearer if we contrast profile analysis with the more familiar split plot analysis. We begin with the split plot analysis; see the theoretical discussion in _PA_ Chap. 11, the balanced examples in Christensen (1996, Chapter 12), and the unbalanced examples in Christensen (2015, Chapter 19).

The split plot model for this experimental design is

\[y_{ijk}=\mu+\delta_{i}+\eta_{ij}+\tau_{k}+(\delta\tau)_{ik}+e_{ijk},\]

\(i=1,2,3\); \(j=1,\ldots,10\); \(k=1,2,3,4\). Here \(\delta_{i}\) indicates the drug effect, \(\tau_{k}\) indicates the time effect, \(\eta_{ij}\) indicates a random error for the \(ij\) individual, and \(e_{ijk}\) is a random error specific to the observation on the \(ij\) individual at the \(k\)th time period. As usual, we assume \(\text{Var}(\eta_{ij})=\sigma_{w}^{2}\), \(\text{Var}(e_{ijk})=\sigma_{s}^{2}\), and that all of the random errors have zero covariance with all of the other random errors. For the construction of tests and confidence regions, it is assumed that the errors have a joint multivariate normal distribution. To construct the ANOVA table for a balanced split plot model, it is often convenient to compute the sums of squares treating the data as a complete factorial experiment and then combine terms to get the correct table. Considering the data as a complete factorial, there are three treatments, Drugs (\(D\)), Times (\(T\)), and Individuals (\(I\)). The corresponding ANOVA table is

\[\begin{array}{lcccc}\hline\text{Source}&df&SS&MS\\ \hline\text{Drugs}&2&2438.5&1219.2\\ \text{Individuals}&9&404.3&44.93\\ D\times I&18&1221.5&67.86\\ \text{Times}&3&222.3&74.10\\ T\times D&6&320.1&53.36\\ T\times I&27&64.0&2.37\\ T\times D\times I&54&321.9&5.96\\ \hline\end{array}\]

Since individuals do not constitute whole plot blocks, the whole plot error is found by pooling the Individuals and the \(D\times I\) terms. The subplot error contains the \(T\times I\) and \(T\times D\times I\) terms. The correct split plot ANOVA table is

\[\begin{array}{lcccc}\hline\text{Source}&df&SS&MS&F\\ \hline\text{Drugs}&2&2438.5&1219.2&20.25\\ \text{Whole plot error}&27&1625.8&60.22\\ \text{Times}&3&222.3&74.10&15.56\\ T\times D&6&320.1&53.36&11.20\\ \text{Subplot error}&81&385.9&4.76\\ \hline\end{array}\]

All of the effects are highly significant. The next step in the split plot analysis might be to examine contrasts in the interactions. This involves looking at contrasts in the \(3\times 4\) table of means \(\bar{y}_{i\cdot k}\).

The three basic tests in profile analysis are analogous to the tests for whole plot treatments, subplot treatments, and interaction. The test for parallelism is equivalent to testing for interaction. The test of whether the average levels of the curves are the same is equivalent to testing for whole plot treatments. The test of whether the average curve is horizontal is equivalent to testing for subplot treatments. Profile analysis is based on examining the structure of

\[\hat{B}=\left[\begin{array}{cccc}73.8&72.8&72.0&70.3\\ 82.9&83.1&83.4&82.9\\ 72.9&79.0&79.6&72.0\end{array}\right].\]

This is the \(3\times 4\) table of means, \(\bar{y}_{i\cdot k}\). The split plot analysis examines the structure of exactly the same means table. While not all interaction contrasts have obvious multivariate tests, any interaction contrast that can be written as \(\lambda^{\prime}B\xi\) can also be tested in the multivariate model.

Depending on the software one has available, to test the hypothesis of parallel profiles \(H_{0}\): \(\Lambda_{3}^{\prime}BA_{4}=0\), it may be convenient to do a one-way ANOVA on a new set of variables \((y_{ij1}^{*},y_{ij2}^{*},y_{ij3}^{*})=(y_{ij1}-y_{ij2},y_{ij1}-y_{ij3},y_{ij1} -y_{ij4})=y_{ij}^{\prime}\Lambda_{4}\). The one-way MANOVA on the transformed data yields

\[S_{*}=\Lambda_{4}^{\prime}SA_{4}=\left[\begin{array}{cccc}4.54&4.01&2.89\\ 4.01&10.90&9.09\\ 2.89&9.09&14.27\end{array}\right]\]

and

\[E_{*}=\Lambda_{4}^{\prime}E\Lambda_{4}=\left[\begin{array}{cccc}122.5&108.3& 77.9\\ 108.3&294.2&245.3\\ 77.9&245.3&385.4\end{array}\right]. \tag{10.3.4}\]

Fitting the model of no drug effects yields.

\[S_{0*}=\Lambda_{4}^{\prime}S_{0}\Lambda_{4}=\left[\begin{array}{cccc}14.19&15.19&4.68\\ 15.19&23.48&11.28\\ 4.68&11.28&15.57\end{array}\right]\]

and

\[E_{0*}=\Lambda_{4}^{\prime}E_{0}\Lambda_{4}=\left[\begin{array}{cccc}411.4&44 0.6&135.7\\ 440.6&680.8&327.2\\ 135.7&327.2&451.5\end{array}\right].\]

The hypothesis statistic is

\[H_{*}=E_{0*}-E_{*}=\left[\begin{array}{cccc}288.9&332.3&57.8\\ 332.3&386.6&81.9\\ 57.8&81.9&66.1\end{array}\right].\]

Dividing by the degrees of freedom gives\[\frac{1}{2}H_{*}=\left[\begin{array}{ccc}144.5&166.1&28.8\\ 166.1&193.3&40.8\\ 28.8&40.8&33.1\end{array}\right].\]

It is clear that \(\frac{1}{2}H_{*}\) and \(S_{*}\) are not estimating the same thing. The test statistics, comparison values, and normal theory critical values for an \(\alpha=0.05\) test follow.

\begin{tabular}{c c c c} \hline Statistic & Observed value & Comparison value & Critical value \\ \hline \(U\) & 0.204 & 0.807 & 0.510 \\ \(\phi_{\max}\) & 3.22 & 0.07 & less than 0.886 \\ \(T^{2}\) & 91.4 & 6 & 22.08 \\ \(V\) & 0.902 & 0.207 & less than 0.569 \\ \hline \end{tabular}

All of the values are far from their comparison values and are in the critical regions of the tests. The hypothesis that the profiles are parallel is rejected. In other words, the relationship between the time means depends on which drug you look at (i.e., there is interaction).

To explore further the lack of parallelism, consider the orthogonal contrasts in drugs determined by \(\lambda_{1}^{\prime}=(1,-1,0)\) and \(\lambda_{2}^{\prime}=(-1,-1,2)\). The test of \(H_{0}\): \(\lambda_{1}^{\prime}BA_{4}=0\) examines whether the placebo curve is parallel to the drug A curve. \(H_{0}\): \(\lambda_{2}^{\prime}B\Lambda_{4}=0\) hypothesizes that the drug B curve is parallel to the average of the others. Define the hypothesis matrices

\[H_{i}=(\lambda_{i}^{\prime}\hat{B}\Lambda_{4})^{\prime}\left[\lambda_{i}^{ \prime}(X^{\prime}X)^{-1}\lambda_{i}\right]^{-1}(\lambda_{i}^{\prime}\hat{B} \Lambda_{4}).\]

Thus,

\[H_{1} = \left[\begin{array}{c}1.2\\ 2.3\\ 3.5\end{array}\right](5)[1.2,2.3,3.5]\] \[= \left[\begin{array}{ccc}7.20&13.80&21.00\\ 13.80&26.42&40.25\\ 21.00&40.25&61.25\end{array}\right]\]

and

\[H_{2} = \left[\begin{array}{c}-13\\ -14.7\\ -1.7\end{array}\right](5/3)[-13,-14.7,-1.7]\] \[= \left[\begin{array}{ccc}281.67&318.50&36.83\\ 318.50&360.15&41.65\\ 36.83&41.65&4.82\end{array}\right].\]

Because the contrasts \(\lambda_{i}\) are orthogonal

\[H_{*}=H_{1}+H_{2}.\]The test statistics, comparison values, and \(\alpha=0.01\) critical values follow.

\begin{tabular}{c c c c c} \hline Statistic & \(\lambda_{1}^{\prime}B\Lambda_{4}=0\) & \(\lambda_{2}^{\prime}B\Lambda_{4}=0\) & Comp. value & Crit. value \\ \hline \(U\) & 0.853 & 0.237 & 0.900 & 0.617 \\ \(\phi_{\rm max}\) & 0.17 & 3.21 & 0.07 & 0.56 \\ \(T^{2}\) & 4.65 & 86.75 & 3.00 & 15.16 \\ \(V\) & 0.147 & 0.763 & 0.103 & 0.359 \\ \hline \end{tabular} Clearly, the predominant cause of nonparallelism is due to the fact that drug B is not parallel to the placebo and drug A. The placebo and drug A appear to be reasonably parallel.

A problem with doing formal tests for the orthogonal contrasts considered earlier is that they were chosen after looking at the sample profiles in Fig. 10.6. This invalidates the distributions used. Since \(H_{*}=H_{1}+H_{2}\), we have

\[T_{*}^{2}=T_{1}^{2}+T_{2}^{2},\]

where \(T_{*}^{2}={\rm tr}(H_{*}S_{*}^{-1})\), \(T_{1}^{2}={\rm tr}(H_{1}S_{*}^{-1})\), and \(T_{2}^{2}={\rm tr}(H_{2}S_{*}^{-1})\). Moreover,

\[T_{i}^{2}\leq T_{*}^{2}.\]

If we use the Scheffe idea and reject \(H_{0}\): \(\lambda_{i}^{\prime}B\Lambda_{4}=0\) only when \(T_{i}^{2}\) is greater than the critical value appropriate for an \(\alpha\)-level test based on \(T_{*}^{2}\), then the experimentwise error rate is no greater than \(\alpha\) (and probably much less). Since \(T_{2}^{2}=86.75\) for the second contrast and the critical value appropriate for \(T_{*}^{2}\) is 22.08, the profile for drug B and the average profile for the placebo and drug A display a significant lack of parallelism, regardless of the fact that the contrast was chosen after examining the data.

Although not every contrast in the drug-time interaction can be written as \(\lambda^{\prime}B\xi\) for some drug contrast vector \(\lambda\) and time contrast vector \(\xi\), these are often the most interpretable contrasts. Clearly, any such contrast can be tested in the multivariate model. As just illustrated, orthogonal contrasts in the drugs are a useful tool for balanced data, especially in relation to \(T^{2}\). Because of the correlation between times, orthogonal contrasts are of little interest relative to the times. For example, it is natural in the split plot model to examine orthogonal polynomial contrasts in the times. Unfortunately, the relationship between orthogonal polynomial contrasts and polynomial regression depends on the validity of the least squares analysis. Using the standard polynomial contrasts is not particularly appropriate for the multivariate model. Fitting a polynomial in the times requires the use of a growth curve model (see Sect. 10.4).

In univariate ANOVA, when there is interaction, the tests of main effects are difficult to interpret. This is precisely because they involve averaging over the interactions. In profile analysis, when the profiles are not parallel, the hypotheses that involve averages are also difficult to interpret. For example, if the curves are not parallel, testing whether the average curve is horizontal does not seem too interesting. If this null hypothesis were true, it would tell us very little. On the other hand, if the profiles are parallel and the average profile is horizontal, then every profile must be horizontal. _Because there is clear evidence that the profiles are not parallel, the other two standard tests in profile analysis are of little interest._ In spite of this fact, we now illustrate their computation and interpretation.

The test for equality of the average levels, \(H_{0}\): \(\Lambda_{3}^{\prime}BJ_{4}=0\), can be performed by doing a univariate one-way ANOVA on the dependent variable constructed by adding together the four time variables. The test corresponds to testing for drug main effects in the split plot model. The ANOVA table is as follows.

\begin{tabular}{l r r r r} \hline Source & \(df\) & \(SS\) & \(MS\) & \(F\) \\ \hline Drugs & 2 & 9754 & 4877 & 20.25 \\ Error & 27 & 6503 & 241 & \\ \hline Total & 29 & 16,257 & & \\ \hline \end{tabular} The \(F\) test is highly significant, thus indicating that the drugs affect the average level of the curves. In other words, averaging over times, there are differences in the drugs. Note that this is precisely the split plot model test for Drug main effects. The sums of squares for Drugs and Error are exactly four times those reported earlier for the split plot model. This is a result of the fact that \(q=4\).

The test of \(H_{0}\): \(J_{3}^{\prime}B\Lambda_{4}=0\) is a test of whether the average curve is horizontal. It tests for differences in Times averaging over Drugs. Using the dependent variables formed to test for parallelism, we get

\[\hat{B}\Lambda_{4}=\begin{bmatrix}1.0&1.8&3.5\\ -0.2&-0.5&0.0\\ -6.1&-6.7&0.9\end{bmatrix}\]

and

\[J_{3}^{\prime}\hat{B}\Lambda_{4}=(-5.3,-5.4,4.4)\.\]

From standard ANOVA theory,

\[(X^{\prime}X)=\begin{bmatrix}10&0&0\\ 0&10&0\\ 0&0&10\end{bmatrix}\]

and

\[\left[J_{3}^{\prime}(X^{\prime}X)^{-1}J_{3}\right]^{-1}=\frac{10}{3}\.\]

It follows that

\[H_{*} = \frac{10}{3}\begin{bmatrix}-5.3\\ -5.4\\ 4.4\end{bmatrix}[-5.3,-5.4,4.4]\] \[= \begin{bmatrix}93.6&95.4&-77.7\\ 95.4&97.2&-79.2\\ -77.7&-79.2&64.5\end{bmatrix}.\]Clearly, this matrix has rank one, hence one degree of freedom. The error matrix \(E_{*}\) is the same as used to test parallelism. It is given in Eq. (10.3.4). An ad hoc evaluation of the null hypothesis compares \(H_{*}/1\) with \(S_{*}\). The matrices are vastly different. Formal tests confirm the conclusion that the average curve is not horizontal. In particular,

\[T^{2}=56.3893,\qquad F=17.4,\qquad F(0.999,3,25)=7.451.\]

It is interesting to examine the relationship between the split plot model test for Time main effects and the multivariate test for average time effects. With one degree of freedom for \(H_{*}\), the standard multivariate test statistics are equivalent. In particular,

\[T^{2} = [n-r(X)]\,{\rm tr}\left[E_{*}^{-1}H_{*}\right]\] \[= {\rm tr}\left[(\Lambda_{4}^{\prime}S\Lambda_{4})^{-1}H_{*}\right].\]

If we require that \(S\) have the structure of a split plot covariance matrix, write

\[S = \hat{\sigma}^{2}\begin{bmatrix}1&\hat{\rho}&\hat{\rho}&\hat{\rho} \\ \hat{\rho}&1&\hat{\rho}&\hat{\rho}\\ \hat{\rho}&\hat{\rho}&1&\hat{\rho}\\ \hat{\rho}&\hat{\rho}&\hat{\rho}&1\end{bmatrix}\] \[= \hat{\sigma}^{2}(1-\hat{\rho})I_{4}+\hat{\sigma}^{2}\hat{\rho}J_{ 4}^{4},\]

then it is easily seen that

\[\Lambda_{4}^{\prime}S\Lambda_{4}=\hat{\sigma}^{2}(1-\hat{\rho})\Lambda_{4}^{ \prime}\Lambda_{4}\]

so

\[T^{2}={\rm tr}\left[(\Lambda_{4}^{\prime}\Lambda_{4})^{-1}H_{*}\right]/\left[ \hat{\sigma}^{2}(1-\hat{\rho})\right].\]

Recall from \(PA\) Chap. 11 that MS(Subplot Error) is the unbiased estimate of \(\sigma^{2}(1-\rho)\). Also note that \({\rm tr}\left[(\Lambda_{4}^{\prime}\Lambda_{4})^{-1}H_{*}\right]=\) SS(Times). Thus, restricting the form of \(\Sigma\) leads naturally to

\[T^{2}=\mbox{SS(Times)/MS(Subplot Error)}.\]

The split plot \(F\) statistic for testing main effects in Times is just \(T^{2}/3\). However, by imposing additional structure on \(\Sigma\), one gains degrees of freedom for the denominator of the \(F\) statistic, so one ends up comparing \(F=15.5\) to an \(F(3,81)\) distribution rather than the multivariate linear model comparison of \(F=17.4\) to an \(F(3,25)\) distribution.

A similar argument relates the \(T^{2}\) for parallelism to the \(F\) statistic for interaction.

#### Computations

There are a number of ways that one can approach the computational problems involved in one-way MANOVA and profile analysis. The three main computational approaches are through use of flexible interactive statistics packages, structured programs, and matrix manipulation programs. R and MINITAB are both flexible interactive statistical packages and matrix manipulation programs. SAS is a structured program with matrix manipulation. Code for the MANOVA and split plot analyses in Christensen (2015) is on my website for that book.

The primary computation involved is finding covariance matrices: one for each treatment, a pooled covariance matrix, and one ignoring treatments. These can be found using a flexible interactive package such as MINITAB that includes some matrix commands. More structured programs require less thought but more reading. For example, BMDP 4V automatically provides the univariate split plot analysis, the multivariate profile analysis, and compromise tests based on the Greenhouse and Geisser (1959) and Huynh and Feldt (1976) adjustments to the degrees of freedom of the univariate tests. The additional reading is needed to identify such things as the definition of \(\Lambda\) used in profile analysis. (The definition given earlier was just one of an infinite number of equally good choices. While the choice will not affect the standard test statistics, the matrices \(H_{s}\) and \(E_{s}\) do depend on \(\Lambda\).) Other things that need to be identified in structured programs are the exact definitions of the test statistics. For example, in BMDP 4V, the TRACE statistic is \(\text{tr}[HE^{-1}]\) rather than \(T^{2}\) or \(V\) and MXROOT is \(\theta_{\text{max}}\) rather than \(\phi_{\text{max}}\). Of course, with a good matrix-manipulation package, such as MATLAB or R, you can do any of these computations directly.

#### Covariance Matrix Modeling

The multivariate model makes no assumptions about the form of \(\Sigma\). The split plot model assumes \(\Sigma\) has the form of Eq. (10.3.3). Increasingly, other forms are being used. For example, with the heart rate data, one might use

\[\Sigma=\sigma^{2}\begin{bmatrix}1&\phi&\phi^{2}&\phi^{3}\\ \phi&1&\phi&\phi^{2}\\ \phi^{2}&\phi&1&\phi\\ \phi^{3}&\phi^{2}&\phi&1\end{bmatrix}.\]

This model would be appropriate if, for each \(ij\), the \(\epsilon_{ij,h}\)s \(h=1,2,3,4\) follow the same \(AR(1)\) time series process (see Sect. 7.2). When using covariance models other than those appropriate for the multivariate linear model or the split plot model, exact tests and BLUEs are typically not available. Typically, maximum likelihood estimation is used along with large sample likelihood ratio tests for these covariance models. The results of Chap. 4 all apply because multivariate linear models can be written as linear models. See also the discussion in Sect. 11.5.2 and the references given near its end.

### Growth Curves for One-Way MANOVA

A natural extension of profile analysis is to develop models for the profiles of the various groups. For the one-way MANOVA model \(Y=XB+e\) with the cell means parameterization

\[B=\begin{bmatrix}\mu_{1}^{\prime}\\ \vdots\\ \mu_{a}^{\prime}\end{bmatrix},\]

the \(q\) observations in each row of \(Y\) might be taken at times \(t_{1},\ldots,t_{q}\). We might then incorporate a growth curve model which posits that the components of \(\mu_{i}\), when plotted against time, form the parabola \(\mu_{ih}=\gamma_{0}+\gamma_{i1}t_{h}+\gamma_{i2}t_{h}^{2}\). Write this in matrix form as

\[\mu_{i}=\begin{bmatrix}\mu_{i1}\\ \vdots\\ \mu_{iq}\end{bmatrix} =\begin{bmatrix}1&t_{1}&t_{1}^{2}\\ \vdots&\vdots&\vdots\\ 1&t_{q}&t_{q}^{2}\end{bmatrix}\begin{bmatrix}\gamma_{0}\\ \gamma_{i1}\\ \gamma_{i2}\end{bmatrix}\] \[=Z\gamma_{i}\.\]

Note that the coefficients of the parabola are allowed to vary with the group \(i\) but that the model matrix of the growth curve is the same for each group.

In general, for a fixed \(q\times r\) matrix \(Z\) with \(r(Z)=r<q\), we assume a linear growth curve model for each \(\mu_{i}\), say

\[\mu_{i}=Z\gamma_{i}\,.\]

Incorporating the models for the growth curves into the one-way MANOVA gives

\[B=\begin{bmatrix}\mu_{1}^{\prime}\\ \vdots\\ \mu_{a}^{\prime}\end{bmatrix}=\begin{bmatrix}\gamma_{1}^{\prime}Z^{\prime}\\ \vdots\\ \gamma_{a}^{\prime}Z^{\prime}\end{bmatrix}=\Gamma Z^{\prime},\]

where

\[\Gamma=\begin{bmatrix}\gamma_{1}^{\prime}\\ \vdots\\ \gamma_{a}^{\prime}\end{bmatrix}.\]

The complete multivariate growth curve model is

\[Y=X\Gamma Z^{\prime}+e\,. \tag{10.4.1}\]

In the next chapter we examine the analysis of models having this general form. Such models are usually called growth curve models or generalized multivariate analysis of variance (GMANOVA) models. We refer to them as _generalized multivariate linear models_.

### Testing for Additional Information

In some multivariate linear model problems, it is of interest to examine whether all the information about \(XB\) can be obtained from a subset of the dependent variables. If so, the other variables provide no additional information beyond that available in the subset. Let \(y\) be the vector of dependent variables, and partition it as \(y^{\prime}=(y^{\prime}_{1},y^{\prime}_{2})\). Our interest is in determining whether \(y_{1}\) contains all the useful information or whether there is useful information in \(y_{2}\).

Assume a multivariate linear model

\[y^{\prime}_{i}=x^{\prime}_{i}B+\epsilon^{\prime}_{i}\]

\(i=1,\ldots,n\), with \(\epsilon_{i}\sim N(0,\Sigma)\), and for \(i\neq j\), \(\epsilon_{i}\) and \(\epsilon_{j}\) independent. Partition each \(y_{i}\) as \(y^{\prime}_{i}=(y^{\prime}_{i1},y^{\prime}_{i2})\) and partition \(B\) and \(\Sigma\) in conformance with the \(y_{i}\)s to give

\[(y^{\prime}_{i1},y^{\prime}_{i2})=x^{\prime}_{i}[B_{1},B_{2}]+(\epsilon^{ \prime}_{i1},\epsilon^{\prime}_{i2}),\]

where

\[\text{Cov}\left(\begin{bmatrix}\epsilon_{i1}\\ \epsilon_{i2}\end{bmatrix}\right)=\begin{bmatrix}\Sigma_{11}&\Sigma_{12}\\ \Sigma_{21}&\Sigma_{22}\end{bmatrix}.\]

Let

\[Y_{j}=\begin{bmatrix}y^{\prime}_{1j}\\ \vdots\\ y^{\prime}_{nj}\end{bmatrix}\]

\(j=1,2\). In matrix form, the linear model is

\[[Y_{1},Y_{2}]=X[B_{1},B_{2}]+[e_{1},e_{2}].\]

The procedure for evaluating additional information is based on the conditional distribution of \(y_{i2}\) given \(y_{i1}\) and is similar to the analysis of covariance method used in the next chapter for generalized multivariate linear models. The conditional distribution is normal with mean equal to the best linear predictor of \(y_{i2}\) based on \(y_{i1}\) and covariance matrix equal to the covariance of the prediction error. Thus,

\[y_{i2}\mid y_{i1}\sim N\left(B^{\prime}_{2}x_{i}+\Sigma_{21}\Sigma_{11}^{-1}(y _{i1}-B^{\prime}_{1}x_{i}),\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12} \right).\]

\(i=1,\ldots,n\). Conditioning on \(Y_{1}\), the random vectors \(y_{i2},i=1,\ldots n\) are still independent, so the conditional distribution also defines a multivariate linear model. Let

\[e_{2\cdot 1}=\begin{bmatrix}\xi^{\prime}_{1}\\ \vdots\\ \xi^{\prime}_{m}\end{bmatrix},\]

where the \(\xi_{i}\)s are i.i.d. \(N(0,\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})\). Then,\[\begin{split} Y_{2}&=XB_{2}+(Y_{1}-XB_{1})\Sigma_{11}^{-1} \Sigma_{12}+e_{2\cdot 1}\\ &=X(B_{2}-B_{1}\Sigma_{11}^{-1}\Sigma_{12})+Y_{1}(\Sigma_{11}^{-1} \Sigma_{12})+e_{2\cdot 1}\\ &\equiv X\Delta+Y_{1}\Gamma+e_{2\cdot 1}\,.\end{split} \tag{10.5.1}\]

Our interest is in whether \(Y_{2}\) contains any additional information about \(XB\) beyond that available in \(Y_{1}\). If \(X\Delta=0\), the conditional distribution of \(Y_{2}\) depends only on \(\Sigma\); \(XB\) is not involved. Thus, \(Y_{2}\) provides no additional information on \(XB\). In particular, if \(\Delta\equiv B_{2}-B_{1}\Sigma_{11}^{-1}\Sigma_{12}=0\), then knowledge of \(B_{1}\) determines \(XB\). The linear structure involved in the components \(Y_{2}=XB_{2}+e_{2}\) is determined entirely through the dependence between \(Y_{1}\) and \(Y_{2}\).

Example 10.5.1. Suppose \(Y=XB+e\) is a multivariate one-way analysis of variance. The matrix \(X\) merely identifies the treatment group for each observation. If the conditional distribution of \(Y_{2}\) does not involve \(X\), then the conditional distribution does not depend on the treatment groups. It follows that for the purpose of modeling treatment means, there is no additional information in \(Y_{2}\). 

The actual test procedure is straightforward. The hypothesis \(H_{0}\): \(X\Delta=0\) can be tested by comparing the fit of model (10.5.1) to that of

\[Y_{2}=Y_{1}\Gamma+e_{2\cdot 1}\,. \tag{10.5.2}\]

Alternatively, if \(\Delta\) is estimable, analysis of covariance methods can be used to obtain \(\hat{\Delta}\) and a test can be based on the estimate. In practice, the idea that \(\Delta\) could really be zero is rather far-fetched. The important thing is that the test provides a way of evaluating whether \(Y_{2}\) is worth bothering about. If the reduced model (10.5.2) fits nearly as well as (10.5.1), then the \(X\Delta\) structure is not really needed to explain \(Y_{2}\). If \(Y_{2}\) has almost no relationship to \(C(X)\) except through \(Y_{1}\), an analysis of \(Y_{1}\) should provide essentially all the information on the relationship between \((Y_{1},Y_{2})\) and \(C(X)\).

Example 10.5.2. Typically, the method of testing for additional information is of most value when the number of dependent variables \(q\) is quite large. It provides a method of _reducing the dimensionality_ of the problem. The current example is restricted to demonstrating some elementary computations using the heart rate data of Example 10.3.1. In particular, we test whether the first two measurements add any information beyond that contained in the last two measurements.

The error matrix for the test can be obtained from a multivariate analysis of covariance (ACOVA). The components of the ACOVA are available in the matrix \(E\) reported in Example 10.3.1. The error matrix for the analysis of covariance model is The reduced model involves only the covariates and not the analysis of variance structure. To obtain the reduced model error matrix \(E_{0}\), one can regress \(y_{1}\) on \(y_{3}\), \(y_{4}\) and \(y_{2}\) on \(y_{3}\), \(y_{4}\) without including intercepts. The diagonal elements of \(E_{0}\) are the sums of the squared residuals. The off-diagonal element is the sum of the crossproducts of the two sets of residuals. These computations yield

\[E_{0}=\begin{bmatrix}422.747&205.842\\ 205.842&206.801\end{bmatrix}.\]

The hypothesis matrix is

\[H=E_{0}-E=\begin{bmatrix}187.539&41.538\\ 41.538&14.610\end{bmatrix}\]

and

\[HE^{-1}=\begin{bmatrix}1.60460&-1.15564\\ 0.30659&-0.18608\end{bmatrix}.\]

Under the null hypothesis, \(HE^{-1}\) should approximate a scalar multiple of the identity matrix. The observed matrix \(HE^{-1}\) is nothing like that. In particular, \(T^{2}=25\operatorname{tr}\left[HE^{-1}\right]=35.46\), while the comparison value for \(T^{2}\), as introduced in Chap. 9, is only 6. Formal tests of \(H_{0}\colon\ \Delta=0\) are highly significant. The first two time measurements are _not_ extraneous. 

The analysis of additional information can be generalized to examine whether \(Y_{2}\) contains additional information for specific purposes. For example, if \(X\) is the model matrix for a one-way ANOVA, we can test whether \(Y_{2}\) contains additional information for the purpose of distinguishing treatment groups. (This is different from modeling treatment means.) Rather than testing model (10.5.1) against the reduced model (10.5.2), test model (10.5.1) against the analysis of covariance model without treatment groups,

\[Y_{2}=J\mu^{\prime}+Y_{1}\Gamma+e_{2.1}\,. \tag{10.5.3}\]

By assuming a parameterization of model (10.5.1) in which

\[X=\left[J,X_{\ast}\right]\]

and

\[B=\begin{bmatrix}B_{10}&B_{20}\\ B_{11}&B_{21}\end{bmatrix},\]

it is easily seen that this is a test of whether \(Y_{2}\) contains any additional information about the part of the model that distinguishes treatment groups,

\[X_{\ast}\left[B_{11},B_{21}\right]. \tag{10.5.4}\]

It is of interest to note that if \(Y_{2}\) consists of only one column, the test of model (10.5.3) versus model (10.5.1) is a standard univariate \(F\) test. In particular, se quential (stepwise) evaluation of the individual variables is quite simple. Moreover, the \(F\) test can be constructed from the diagonal elements of \(E^{-1}\) and \((H+E)^{-1}\) that correspond to \(Y_{2}\); here, \(E\) and \(H\) are defined for testing the multivariate linear models \(Y=XB+e\) and \(Y=J\mu^{\prime}+e\). This provides a simple way to compute all of the \(F\) statistics needed for a stepwise evaluation. Variable selection methods in discriminant analysis are based on testing the additional information available for distinguishing treatment groups; see Sect. 12.6.

##### Exercise 10.3.

Find the \(F\) test in terms of \(E^{-1}\) and \((H+E)^{-1}\).

##### Exercise 10.4.

Test whether the first two time variates in the heart rate data are needed for distinguishing the treatment groups.

### 10.6 Additional Exercises

##### Exercise 10.6.1.

Jolicoeur and Mosimann (1960) give data on the length, width, and height of painted turtle shells. The carapace dimensions of 24 females and 24 males are given in Table 10.3. Use Hotelling's \(T^{2}\) statistic to test whether there is a sex difference in shell dimensions. Is there a significant sex difference between any of the individual dimensions? Use plots to check the validity of your assumptions.

\begin{table}
\begin{tabular}{|r r r r|r r r||r r r|} \hline \multicolumn{3}{|c|}{Female} & \multicolumn{3}{|c||}{Female} & \multicolumn{3}{|c|}{Male} & \multicolumn{3}{|c|}{Male} \\ \hline Length & Width & Height & Length & Width & Height & Length & Width & Height & Length & Width & Height \\ \hline
98 & 81 & 38 & 138 & 98 & 51 & 93 & 74 & 37 & 116 & 90 & 43 \\
103 & 84 & 38 & 138 & 99 & 51 & 94 & 78 & 35 & 117 & 90 & 41 \\
103 & 86 & 42 & 141 & 105 & 53 & 96 & 80 & 35 & 117 & 91 & 41 \\
105 & 86 & 42 & 147 & 108 & 57 & 101 & 84 & 39 & 119 & 93 & 41 \\
109 & 88 & 44 & 149 & 107 & 55 & 102 & 85 & 38 & 120 & 89 & 40 \\
123 & 92 & 50 & 153 & 107 & 56 & 103 & 81 & 37 & 120 & 93 & 44 \\
123 & 95 & 46 & 155 & 115 & 63 & 104 & 83 & 39 & 121 & 95 & 42 \\
133 & 99 & 51 & 155 & 117 & 60 & 106 & 83 & 39 & 125 & 93 & 45 \\
133 & 102 & 51 & 158 & 115 & 62 & 107 & 82 & 38 & 127 & 96 & 45 \\
133 & 102 & 51 & 159 & 118 & 63 & 112 & 89 & 40 & 128 & 95 & 45 \\
134 & 100 & 48 & 162 & 124 & 61 & 113 & 88 & 40 & 131 & 95 & 46 \\
136 & 102 & 49 & 177 & 132 & 67 & 114 & 86 & 40 & 135 & 106 & 47 \\ \hline \end{tabular}
\end{table}
Table 10.3: Carapace dimensions

[MISSING_PAGE_FAIL:435]

[MISSING_PAGE_FAIL:436]

* Jolicoeur & Mosimann (1960) Jolicoeur, P., & Mosimann, J. E. (1960). Size and shape variation on the painted turtle: A principal component analysis. _Growth, 24_, 339-354.
* Lubischew (1962) Lubischew, A. A. (1962). On the use of discriminant functions in taxonomy. _Biometrics, 18_, 455-477.
* Mosteller & Tukey (1977) Mosteller, F., & Tukey, J. W. (1977). _Data analysis and regression_. Reading, MA: Addison-Wesley.
* Seber (1984) Seber, G. A. F. (1984). _Multivariate observations_. New York: Wiley.
* Smith et al. (1962) Smith, H., Gnanadesikan, R., & Hughes, J. B. (1962). Multivariate analysis of variance (MANOVA). _Biometrics, 18_, 22-41.

## Chapter 11 Generalized Multivariate Linear Models and Longitudinal Data

**Abstract** This chapter further develops the growth curve models introduced in Chap. 10. This broader class of models is shown to be intimately related to split plot models. We also illustrate how the models can be modified to deal with missing data which is a common problem in longitudinal studies. The models are also related to functional data analysis.

### 11.1 Generalized Multivariate Linear Models

In Sect. 10.4 we introduced the growth curve model as a modification of the one-way MANOVA model in order to relate the mean structure across the dependent variables. The same idea can be applied to arbitrary multivariate linear models. Consider the multivariate linear model

\[Y=XB+e\]

with the mean and covariance properties discussed in Chap. 9. If we assume that \(B=\Gamma Z^{\prime}\) for some known \(q\times r\) matrix \(Z\) and an unknown \(p\times r\) matrix of parameters \(\Gamma\), we get

\[Y=X\Gamma Z^{\prime}+e\,. \tag{11.1.1}\]

This is variously called a _growth curve model_, a _generalized multivariate analysis of variance (GMANOVA)_ model, or a _generalized multivariate linear model (GMLM)_.

Just as multivariate linear models are a special class of univariate linear models, the generalized multivariate linear model can also be written as a univariate linear model. Unfortunately, if \(Z\neq I_{q}\), the generalized multivariate linear model is not a member of the class that we have been calling multivariate linear models. Multivariate linear models are not allowed to specify structure between the means of the different variables. The results on optimal estimation and the distribution theory based on the Wishart distribution depend on applying the same projection operator to each dependent variable vector \(Y_{h}\). If, as in the growth curve model, one specifiesa linear structure for the means across the dependent variables, then the best estimate for any parameter will incorporate information available from the other variables. For example, in a multivariate linear model estimation of \(\rho^{\prime}X\beta_{h}\) comes from analyzing the \(Y_{h}\) variable separately, e.g., \(\rho^{\prime}X\hat{\beta}_{h}=\rho^{\prime}MY_{h}\), but in a GMANOVA model the estimate typically depends on the information in the other columns of \(Y\). This is enough to invalidate the usual estimation results and distribution theory. Some alternative form of analysis must be developed. When \(C(Z)=C(I_{q})\), the generalized multivariate linear model is equivalent to a multivariate linear model.

Rewrite model (11.1.1) as

\[\text{Vec}(Y)=\text{Vec}(X\Gamma Z^{\prime})+\text{Vec}(e)\]

and then as the linear model

\[\text{Vec}(Y)=[Z\otimes X]\text{Vec}(\Gamma)+\text{Vec}(e). \tag{11.1.2}\]

We continue to assume that

\[\text{E}(e)=0\qquad\text{and}\qquad\text{Cov}\left[\text{Vec}(e)\right]=[ \Sigma\otimes I_{n}].\]

For known \(\Sigma\), optimal estimates of \(\Gamma\) are generalized least squares estimates that satisfy

\[[Z\otimes X]\text{Vec}(\hat{\Gamma})=\mathcal{A}\text{Vec}(Y)\]

where \(\mathcal{A}\) is the appropriate oblique projection operator. It is not hard to see that

\[\mathcal{A}=[A_{Z}\otimes M]\]

where \(M\) is the ppo onto \(C(X)\) and \(A_{Z}\equiv Z[Z^{\prime}\Sigma^{-1}Z]^{-}Z^{\prime}\Sigma^{-1}\) is an oblique projection operator onto \(C(Z)\). Thus, the optimal estimate of \(\Gamma\), for known \(\Sigma\), satisfies

\[[Z\otimes X]\text{Vec}(\hat{\Gamma})=[A_{Z}\otimes M]\text{Vec}(Y) \tag{11.1.3}\]

or

\[X\hat{\Gamma}Z^{\prime}=MYA_{Z}^{\prime}.\]

##### Exercise 11.1

Prove that Eq. (11.1.3) gives the BLUEs when \(\Sigma\) is known.

In particular, when the inverses exist we can write generalized least squares estimates for each \(i\), say,

\[\tilde{\gamma}_{i}\equiv[Z^{\prime}\Sigma^{-1}Z]^{-1}Z^{\prime}\Sigma^{-1}y_{i}\]

so that we can write an \(n\times r\) matrix

\[\tilde{Y}_{1}\equiv\begin{bmatrix}\tilde{\gamma}_{1}^{\prime}\\ \vdots\\ \tilde{\gamma}_{n}^{\prime}\end{bmatrix}=Y\Sigma^{-1}Z[Z^{\prime}\Sigma^{-1}Z ]^{-1}\]Multiplying the GMLM (11.1.1) on the right by \(\Sigma^{-1}Z[Z^{\prime}\Sigma^{-1}Z]^{-1}\) gives the standard multivariate linear model

\[\tilde{Y}_{1}=X\Gamma+\tilde{e}_{1} \tag{11.1.4}\]

and we could then find \(\hat{\Gamma}\) by fitting the multivariate linear model (11.1.4) using least squares, i.e., \(X\hat{\Gamma}=M\tilde{Y}_{1}\).

Of course, \(A_{Z}\) and \(\hat{\Gamma}\) depend on knowing \(\Sigma\), which is an unrealistic assumption. As discussed in Chap. 4, an appropriate procedure is to estimate \(\Sigma\) with, say, \(\hat{\Sigma}\) and use that to compute empirical estimates \(\hat{\Gamma}\) using \(\tilde{A}_{Z}\). The standard procedures seem to be based on observing that the linear model (11.1.2) is a reduced model relative to the standard multivariate linear model

\[\operatorname{Vec}(Y)=[I_{q}\otimes X]\operatorname{Vec}(B)+\operatorname{Vec }(e). \tag{11.1.5}\]

We can use model (11.1.5) to estimate the covariance parameters. From the multivariate linear model we know that an unbiased estimate of \(\Sigma\) is \(S\equiv Y^{\prime}(I-M)Y/[n-r(X)]\). Identifying \(S\equiv\tilde{\Sigma}\) leads to an empirical estimate of \(\Gamma\). Relative to model (11.1.2), \(S\) should be a residual type estimate in the sense of Sect. 4.7, so it should lead to the unbiasedness and variance estimation properties discussed in that section. In fact, Khatri (1966) showed that the maximum likelihood estimates, \(\hat{\Gamma}_{ML}\) and \(\hat{\Sigma}\), are determined by

\[X\hat{\Gamma}_{ML}Z^{\prime}=MY\tilde{A}_{Z}^{\prime}\]

where \(\tilde{A}_{Z}\equiv Z(Z^{\prime}S^{-1}Z)^{-}Z^{\prime}S^{-1}\) and

\[n\hat{\Sigma}=(Y-X\hat{\Gamma}_{ML}Z^{\prime})^{\prime}(Y-X\hat{\Gamma}_{ML}Z^ {\prime})=Y^{\prime}(I-M)Y+(I-\tilde{A})Y^{\prime}MY(I-\tilde{A})^{\prime}.\]

Model (11.1.1) must not be confused with the transformation of \(Y=XB+e\) considered after Eq. (9.5.8):

\[YW=XBW+eW. \tag{11.1.6}\]

Model (11.1.1) has \(\operatorname{E}(Y)=XTZ^{\prime}\) whereas model (11.1.6) has \(\operatorname{E}(YW)=XBW\). The parameter matrices \(\Gamma\) and \(B\) seem interchangeable but in fact \(B\) is analogous to \(\Gamma Z^{\prime}\). The known \(q\times r\) matrices \(Z\) and \(W\) seem interchangeable but they are used differently. Model (11.1.1) examines \(\operatorname{E}(Y)\) whereas model (11.1.6) examines \(\operatorname{E}(YW)\). Model (11.1.6) is still a standard multivariate linear model, unlike the GMLM (11.1.1). Model (11.1.6) reduces the dimensionality of everything from \(q\) to \(r\) whereas model (11.1.1) has \(q\) dimensions for the dependent variable but only \(r\) for the parameter matrix. In the transformed model, \(B\) is \(p\times q\), whereas in model (11.1.1), \(\Gamma\) is \(p\times r\). Because \(B\) is completely unknown, in model (11.1.6) \(BW\) remains completely unknown, which allows us to treat model (11.1.6) as a standard multivariate linear model. These distinctions make all the difference in being able to derive optimal estimates and closed-form small sample distributions, something we can do for model (11.1.6) but not for model (11.1.1).

In the next two sections we discuss methods for analyzing generalized multivariate linear models. Both involve transforming the generalized multivariate linear model into a standard multivariate linear model. Section 11.4 discusses a class of covariance matrices for which least squares estimates are optimal, thus enabling an optimal analysis. Section 11.5 discusses the application of generalized multivariate linear models to longitudinal data and extends those results to data in which some of the \(q\) dependent variables are unobserved. Section 11.6 discusses the relationship between generalized multivariate linear models and generalized split plot models.

### Generalized Least Squares Analysis

The problem with Eq. (11.1.3) is that we do not know \(\Sigma\). One way to analyze growth curve models is, similar in spirit to MINQUE in Chap. 4, just to pick a weighting matrix \(G\) to use in place of \(\Sigma^{-1}\). The most common choice is \(G=I_{q}\). In this ad hoc approach, when the inverses exist, we multiply

\[Y=X\Gamma Z^{\prime}+e\]

on the right by \(GZ[Z^{\prime}GZ]^{-1}\) to obtain

\[YGZ[Z^{\prime}GZ]^{-1}=X\Gamma Z^{\prime}GZ[Z^{\prime}GZ]^{-1}+eGZ[Z^{\prime}GZ ]^{-1}.\]

Upon simplification we get the standard multivariate linear model

\[YGZ[Z^{\prime}GZ]^{-1}=X\Gamma+eGZ[Z^{\prime}GZ]^{-1}\,.\]

In particular, we can write generalized least squares estimates for each \(i\), say,

\[\tilde{\gamma}_{i}\equiv[Z^{\prime}GZ]^{-1}Z^{\prime}Gy_{i}\]

so that we can write an \(n\times r\) matrix

\[\tilde{Y}_{1}\equiv\begin{bmatrix}\tilde{\gamma}_{1}^{\prime}\\ \vdots\\ \tilde{\gamma}_{n}^{\prime}\end{bmatrix}=YGZ[Z^{\prime}GZ]^{-1}\]

We can then find \(\hat{\Gamma}_{G}\) by fitting the multivariate linear model

\[\tilde{Y}_{1}=X\Gamma+\tilde{e}_{1} \tag{11.2.1}\]

using least squares. If \(X\) is the model matrix for a one-way ANOVA, the rows of \(\hat{\Gamma}_{G}\) consist of the sample means of the \(\tilde{\gamma}_{i}\)s from each group.

While \(\hat{\Gamma}_{G}\) is the best linear unbiased estimate of \(\Gamma\) based on \(\tilde{Y}_{1}\), it is _not_ the best linear unbiased estimate of \(\Gamma\) based on \(Y\). This \(\hat{\Gamma}_{G}\) loses information due to \(\tilde{Y}_{1}\) being based on suboptimal estimators. None of the estimates \(\tilde{\gamma}_{i}\) are optimal because they are based on \(G\) rather than \(\Sigma^{-1}\). As discussed in Sect. 11.4, useful models for \(\Sigma\) exist in which the ordinary least squares estimates obtained with \(G=I\) do give optimal estimation. Such models for \(\Sigma\) provide good justification for the analysis discussed here with \(G=I\).

Tests of hypotheses for model (11.2.1) are performed as for any other standard multivariate linear model. The most commonly performed test for a multivariate linear model is against the model that replaces \(X\) with \(J_{n}\). It tests whether the regression coefficients of the growth curves are the same for every individual. One advantage of analyzing model (11.2.1) for some fixed choice of \(G\) is that the standard test statistics have known distributions under \(H_{0}\). Unfortunately, likelihood ratio tests for the original model (11.1.1) have thus far proved to have intractable distributions.

##### Exercise 11.2.

Show that if \(Z\) does not have full column rank that it suffices to pick any \((Z^{\prime}GZ)^{-}\) but that one should restrict hypotheses being tested to be of the form \(\Lambda^{\prime}TW\) where \(\Lambda^{\prime}\Gamma\) is estimable in the sense that \(\Lambda^{\prime}=P^{\prime}X\) for some matrix \(P\) but where \(\Gamma W\) is also estimable in the sense that \(W=Z^{\prime}W_{0}\) for some \(W_{0}\). What does it mean for \(\Lambda^{\prime}\Gamma W\) to be estimable in model (11.1.2)? (In a standard multivariate linear model there are no restrictions on \(W\) nor are there any in a GMLM when \(Z\) has full rank.)

##### Example 11.2.1.

Once again, consider the heart rate data of Example 10.3.1. We have seen that standard analysis of variance procedures can be used to examine relationships between drugs. Unfortunately, standard procedures are less applicable for comparing times because observations over time are correlated. If some contrast is of particular interest, it can be examined, but such tools as orthogonal contrasts and tabled polynomial contrasts do not retain their attractive properties. In lieu of using orthogonal polynomial contrasts, we fit a polynomial growth curve model. Specifically, we assume a quadratic growth curve model. Recall that heart rates are measured at 2, 7, 12, and 17 min after the injection. The model for the time means is

\[\begin{bmatrix}\mu_{i1}\\ \mu_{i2}\\ \mu_{i3}\\ \mu_{i4}\end{bmatrix} = \begin{bmatrix}1&2&4\\ 1&7&49\\ 1&12&144\\ 1&17&289\end{bmatrix}\begin{bmatrix}\gamma_{0}\\ \gamma_{1}\\ \gamma_{2}\end{bmatrix}\] \[= Z\gamma_{i},\]

\(i=1,2,3\).

The simplest choice of \(G\) is \(G=I\). (This choice of \(G\) actually _is_ equivalent to using a column of ones and tabled polynomial contrasts for \(W\) in model (11.1.6).) Let

\[W_{1} = Z(Z^{\prime}Z)^{-1}\] \[= \begin{bmatrix}1.41&-0.25&0.01\\ -0.15&0.17&-0.01\\ -0.53&0.21&-0.01\\ 0.27&-0.13&0.01\end{bmatrix}.\]

The transformed dependent variable matrix in model (11.2.1) becomes \[\tilde{Y}_{1}\equiv YW_{1},\]

so we fit the multivariate one-way MANOVA

\[\tilde{Y}_{1}=X\Gamma+\tilde{e}_{1}\.\]

The estimates obtained are

\[\hat{\Gamma}=\begin{bmatrix}73.96&-0.093&-0.007\\ 82.60&0.139&-0.007\\ 68.19&2.561&-0.137\end{bmatrix}\]

and

\[\tilde{S}_{1}=\begin{bmatrix}19.447&-0.701&-0.0025\\ -0.701&0.51570&-0.02485\\ -0.0025&-0.02485&0.001379\end{bmatrix}.\]

Computing the usual test statistics for no treatment effects in the multivariate one-way ANOVA model, we find substantial differences among the three injections. For example, \(T^{2}=184.81\), which is huge when compared to the asymptotic null distribution \(\chi^{2}(6)\).

Write

\[\tilde{Y}_{1}=[\tilde{Y}_{10},\tilde{Y}_{11},\tilde{Y}_{12}]\.\]

The vector \(\tilde{Y}_{10}=(\hat{\gamma}_{10},\ldots,\hat{\gamma}_{n0})^{\prime}\) is used for inferences about intercepts. The vector \(\tilde{Y}_{11}=(\hat{\gamma}_{11},\ldots,\hat{\gamma}_{n1})^{\prime}\) is used for slopes; \(\tilde{Y}_{12}\) is used to analyze the coefficients of the quadratic terms in the polynomials. Each of these vectors can be examined in a univariate one-way ANOVA. Pairs of them can be examined in a multivariate one-way.

The coefficient of the quadratic term for drug B is \(\gamma_{32}\). The least squares estimate \(\hat{\gamma}_{32}=-0.137\) has a standard error of \(0.01174\) and a \(t\) statistic of \(-11.67\). The quadratic term for drug B is clearly important. All of this can be obtained from the univariate one-way ANOVA on \(\tilde{Y}_{12}\).

Similarly, we can test whether parabolas are needed to model the placebo and drug A. This is a test of \(H_{0}\): \(\gamma_{12}=\gamma_{22}=0\) and depends only on \(\tilde{Y}_{12}\). Again, the test is just a univariate ANOVA test. The reduced model is

\[\tilde{y}_{12,ij}=\delta_{i3}\gamma_{i2}+\tilde{e}_{12,ij},\]

where \(\delta_{i3}\) is l if \(i=3\) and zero otherwise. From the matrix \(\tilde{S}_{1}\) reported earlier, we find that \(\text{SSE}(Full)=(27)(0.001379)=0.03723\). For the reduced model, \(\text{SSE}(Red.)=0.03821\), so the \(F\) statistic is

\[F=\frac{[0.03821-0.03723]/2}{0.001379}=0.355\]

and there is no evidence of the need for a parabola in either the placebo or drug A.

We can also test for equality of the slopes in the placebo and drug A. Because our interest is in the slopes, the dependent variable is \(\tilde{Y}_{11}\). The reduced model is again a one-way ANOVA, but now the placebo and drug A are considered as the same treatment. The error sums of squares for the full and reduced models take the values

\[\text{SSE}(\mathit{Full})=13.924\,,\]

\[\text{SSE}(\mathit{Red.})=14.193\,.\]

The \(F\) statistic is

\[F=\frac{14.193-13.924}{[13.924/27]}=0.522,\]

so there is no evidence of a difference between the placebo and drug A. We can also test whether both slopes are zero. The new reduced model is \(\tilde{y}_{11,ij}=\delta_{i3}\gamma_{i1}+\tilde{e}_{11,ij}\) with

\[\text{SSE}(\mathit{Red.})=14.203\]

and

\[F=\frac{[14.203-13.924]/2}{0.5157}=0.271\,.\]

There is no evidence of a nonzero slope for the placebo and drug A.

The procedure for testing whether the placebo and drug A act the same can be applied to \(\hat{Y}_{10}\) to determine if there is evidence of a difference in intercepts between the placebo and drug A. If no difference is found, we would have found no evidence of any difference between the placebo and drug A on any of the dependent variables. The statistics are

\[\text{SSE}(\mathit{Full})=525.07,\]

\[\text{SSE}(\mathit{Red.})=898.83,\]

\[F=\frac{898.83-525.07}{19.45}=19.22\,.\]

Comparing this to \(F(0.995,1,27)=9.34\) or even to the Scheffe critical point \(2\,F(0.995,2,27)=12.98\) establishes that there is a significant difference at the \(0.005\) level between the intercepts for the placebo and drug A.

The heart rates under the placebo seem to be relatively constant at about \(\hat{\gamma}_{10}\doteq 74\) beats per minute. Over the course of the \(17\min\) experiment, drug A yields approximately constant heart rates at \(\hat{\gamma}_{20}\doteq 82.5\) beats per minute. Over the course of the experiment, heart rates for drug B can be approximated by the parabola \(68.19+2.561\,t-0.137\,t^{2}\). Clearly, this is only an approximation. It is unlikely that heart rates would really become negative after thirty-three and a half minutes.

This entire analysis is essentially an exercise in quantifying and evaluating the visual impressions given by Fig. 10.6. For example, the downward trend seen in the placebo profile is not statistically significant from the current data and analysis. Both \(\hat{\gamma}_{11}\) and \(\hat{\gamma}_{12}\) are negative, so the downward trend is being modeled, but neither coefficient is significant, so we do not have firm evidence of a downward trend.

Although we only tested that these coefficients were zero in combination with the corresponding values for drug A, the same results occur for the individual tests. 

##### Exercise 11.3.

1. Using \(\tilde{Y}_{11}\) and \(\tilde{Y}_{12}\) as dependent variables, do a multivariate test of \(\gamma_{11}=\gamma_{12}=0\).
2. Perform an analysis similar to the one just given using a split plot model. Compare the results to those of the growth curve model.

### MACOVA Analysis

The use of model (11.2.1) to analyze growth curves was apparently first proposed by Potthoff and Roy (1964). An extension of this method based on multivariate analysis of covariance (MACOVA) and normality was proposed by Rao (1965, 1966, 1967) and Khatri (1966). Let \(Z\) have full rank, \(W_{1}=GZ(Z^{\prime}GZ)^{-1}\) and let \(W_{2}\) be a full column rank matrix with \(C(W_{2})=C(Z)^{\perp}\). As established later in Exercise 11.4, with these choices the matrix \(W=[W_{1},W_{2}]\) is nonsingular. Because \(W\) is a nonsingular matrix, the generalized multivariate linear model (11.1.1) is equivalent to

\[YW=X\Gamma Z^{\prime}W+eW\,. \tag{11.3.1}\]

Write \(\tilde{Y}_{1}=YW_{1}\) and \(\tilde{Y}_{2}=YW_{2}\). Note that \(\tilde{Y}_{1}\) is precisely the dependent variable matrix in model (11.2.1). Using the definition of \(W\), (11.3.1) can be rewritten as

\[[\tilde{Y}_{1},\tilde{Y}_{2}]=[X\Gamma,0]+[eW_{1},eW_{2}],\]

which is similar to a multivariate linear model. As will be seen later, under normality, the conditional distribution of \(\tilde{Y}_{1}\) given \(\tilde{Y}_{2}\) is determined by the multivariate linear model

\[\tilde{Y}_{1}=X\Gamma+\tilde{Y}_{2}\Psi+\tilde{e}_{1\cdot 2}\,. \tag{11.3.2}\]

In particular, this is a multivariate analysis of covariance model. Estimates and quadratic forms for tests are derived as in \(PA\) Chap. 9, so

\[\begin{array}{rcl}\hat{\Psi}&=&[\tilde{Y}_{2}^{\prime}(I-M)\tilde{Y}_{2}]^{- 1}\tilde{Y}_{2}^{\prime}(I-M)\tilde{Y}_{1}\,\\ X\hat{\Gamma}&=&M(\tilde{Y}_{1}-\tilde{Y}_{2}\hat{\Psi}),\end{array}\]

and, with \(\tilde{\Sigma}_{1\cdot 2}\) denoting the conditional covariance matrix of a row of \(\tilde{Y}_{1}\), the estimate of the covariance matrix is based on

\[[n-r(X,\tilde{Y}_{2})]\tilde{S}_{1\cdot 2}=\tilde{Y}_{1}^{\prime}\{(I-M)-(I-M) \tilde{Y}_{2}[\tilde{Y}_{2}^{\prime}(I-M)\tilde{Y}_{2}]^{-1}\tilde{Y}_{2}^{ \prime}(I-M)\}\tilde{Y}_{1}.\]

In general, \(\Gamma\) need not be estimable because \(X\) need not have full column rank. To test that the growth curves are the same for everybody (in one-way MANOVA\(H_{0}\): \(\gamma_{1}=\gamma_{2}=\cdots=\gamma_{a}\)), simply test the full model (11.3.2) against the reduced model

\[\tilde{Y}_{1}=J\mu^{\prime}+\tilde{Y}_{2}\Psi+\tilde{e}_{1\cdot 2}\,.\]

One substantial advantage of the Rao-Khatri modification is that, for inferences about \(\Gamma\), the method does not depend on the specific choice of the matrix \(W\). This can be shown for all aspects of the problem; we illustrate only that \(X\hat{\Gamma}\) does not depend on \(G\). Let \(E\equiv Y^{\prime}(I-M)Y=[n-r(X)]S\).

\[X\hat{\Gamma} = M(\tilde{Y}_{1}-\tilde{Y}_{2}\hat{\Psi})\] \[= M(YW_{1}-YW_{2}\hat{\Psi})\] \[= MY(W_{1}-W_{2}[\tilde{Y}_{2}^{\prime}(I-M)\tilde{Y}_{2}]^{-1} \tilde{Y}_{2}^{\prime}(I-M)\tilde{Y}_{1})\] \[= MY(W_{1}-W_{2}[W_{2}^{\prime}EW_{2}]^{-1}W_{2}^{\prime}EW_{1})\] \[= MY(I-A_{2})W_{1}\,\]

where \(A_{2}\equiv W_{2}(W_{2}^{\prime}EW_{2})^{-1}W_{2}^{\prime}E\). Because \(W_{2}^{\prime}Z=0\), Lemma 4.3.1 implies that \((I-A_{2})\) is the oblique projection operator onto \(C(E^{-1}Z)\) along \(C(W_{2})\). By Lemma 4.3.2, \(E^{-1}Z(Z^{\prime}E^{-1}Z)^{-1}Z^{\prime}\) is the projection operator onto \(C(E^{-1}Z)\) along \(C(W_{2})\), so

\[X\hat{\Gamma} = MYE^{-1}Z(Z^{\prime}E^{-1}Z)^{-1}Z^{\prime}W_{1}\] \[= MYE^{-1}Z(Z^{\prime}E^{-1}Z)^{-1}Z^{\prime}GZ(Z^{\prime}GZ)^{-1}\] \[= MYE^{-1}Z(Z^{\prime}E^{-1}Z)^{-1}\] \[= MYS^{-1}Z(Z^{\prime}S^{-1}Z)^{-1}\,\]

which does not depend on \(W\). Moreover, this is precisely the intuitively appealing estimator based on taking \(\Sigma=S\) in model (11.1.4) or \(G^{-1}=S\) in model (11.2.1).

It remains to show that (11.3.2) is a valid multivariate linear model. A typical row of \(Y\) is \(y_{i}^{\prime}\) so a row of \(YW\) is \(y_{i}^{\prime}W=[y_{i}^{\prime}W_{1},y_{i}^{\prime}W_{2}]\). Define \([\tilde{y}_{1i}^{\prime},\tilde{y}_{2i}^{\prime}]=[y_{i}^{\prime}W_{1},y_{i}^ {\prime}W_{2}]\). We will find the conditional distribution of \(\tilde{y}_{1i}\) given \(\tilde{y}_{2i}\). Because \([\tilde{y}_{1i}^{\prime},\tilde{y}_{2i}^{\prime}]\) is multivariate normal, the conditional distribution is also normal. The conditional mean is the best linear predictor of \(\tilde{y}_{1i}\) based on \(\tilde{y}_{2i}\). The conditional covariance matrix is the prediction covariance matrix. (These concepts were discussed in Chap. 4 and _PA_ Chap. 6.) The parameters of the conditional distribution are simple functions of the parameters of the joint distribution. We begin by finding the mean and covariance matrix of the joint distribution. The rows of \(X\) can be written as \(x_{i}^{\prime}\), so \(\mathrm{E}(y_{i}^{\prime})=x_{i}^{\prime}\Gamma Z^{\prime}\) and, as usual, \(\mathrm{Cov}(y_{i})=\Sigma\). It follows that

\[\mathrm{E}[\tilde{y}_{1i}^{\prime},\tilde{y}_{2i}^{\prime}] = \mathrm{E}(y_{i}^{\prime}W)=x_{i}^{\prime}\Gamma Z^{\prime}W\] \[= [x_{i}^{\prime}\Gamma ZW_{1},x_{i}^{\prime}\Gamma Z^{\prime}W_{2}]\] \[= [x_{i}^{\prime}\Gamma,0],\]

where the last equality follows from the choice of \(W\). Also,\[\begin{split}\text{Cov}\left(\begin{bmatrix}\tilde{y}_{1i}\\ \tilde{y}_{2i}\end{bmatrix}\right)&=&\text{Cov}(W^{\prime}y_{i})=W^{\prime} \Sigma W\\ &=&\begin{bmatrix}W_{1}^{\prime}\Sigma W_{1}&W_{1}^{\prime}\Sigma W_{2}\\ W_{2}^{\prime}\Sigma W_{1}&W_{2}^{\prime}\Sigma W_{2}\end{bmatrix}.\end{split}\]

From the theory of best linear predictors,

\[\tilde{y}_{1i}|\tilde{y}_{2i}\sim N(\Gamma^{\prime}x_{i}+\Psi^{\prime}\tilde{y }_{2i},\tilde{\Sigma}_{1\cdot 2}), \tag{11.3.3}\]

where \(\Psi^{\prime\prime}\equiv W_{1}^{\prime}\Sigma W_{2}(W_{2}^{\prime}\Sigma W_{2 })^{-1}\) and

\[\tilde{\Sigma}_{1\cdot 2}\equiv W_{1}^{\prime}\Sigma W_{1}-W_{1}^{\prime} \Sigma W_{2}(W_{2}^{\prime}\Sigma W_{2})^{-1}W_{2}^{\prime}\Sigma W_{1}\,.\]

Because the \(y_{i}\)s are independent, the \(W^{\prime}y_{i}\)s are also independent. In particular, \(\tilde{y}_{1i}\) is independent of \(\tilde{y}_{2^{\prime}}\) whenever \(i\neq i^{\prime}\). Thus, the distribution in (11.3.3) is also the distribution of \(\tilde{y}_{1i}\) given \(\tilde{Y}_{2}\). Moreover, because \(y_{i}\) and \(y_{i^{\prime}}\) are independent for \(i\neq i^{\prime}\), the random vector \(\tilde{y}_{1i}\) given \(\tilde{y}_{2i}\) is independent of any other observations, say \(\tilde{y}_{1i^{\prime}}\) given \(\tilde{y}_{2i^{\prime}}\). Because both \(\tilde{y}_{2i}\) and \(\tilde{y}_{2i^{\prime}}\) can be replaced by \(\tilde{Y}_{2}\) in the conditioning, it follows that, given \(\tilde{Y}_{2}\), the \(\tilde{y}_{1i}\)s are i.i.d. with the distribution in (11.3.3). This is precisely the definition of the multivariate linear model (11.3.2). Because (11.3.2) is a conditional normal theory multivariate linear model, the estimates \(\hat{\Psi}\) and \(X\hat{\Gamma}\) are conditional maximum likelihood estimates. In fact, since \(X\hat{\Gamma}\) does not depend on \(\Sigma\) yet maximizes the conditional likelihood for any \(\Sigma\), and since the marginal distribution of \(\tilde{Y}_{2}\) does not depend on \(\Gamma\), the conditional maximum likelihood estimate of \(X\Gamma\) is also the unconditional MLE.

We now reanalyze the heart rate data using the analysis of covariance growth curve model. Recall from Example 11.2.1 that

\[Z=\begin{bmatrix}1&2&4\\ 1&7&49\\ 1&12&144\\ 1&17&289\end{bmatrix},\]

and we took

\[W_{1}=Z(Z^{\prime}Z)^{-1}.\]

The measurements are taken at equally spaced time intervals, so a standard table of polynomial contrasts can be used to obtain the matrix \(W_{2}\). The cubic contrast coefficients with four equally spaced time periods is \((-1,3,-3,1)\) and \(W_{2}\) can be taken as

\[W_{2}=\begin{bmatrix}-1\\ 3\\ -3\\ 1\end{bmatrix}.\]

Computing \(\tilde{Y}_{1}=YW_{1}\) and \(\tilde{Y}_{2}=YW_{2}\) and fitting the multivariate one-way analysis of covariance model \[\tilde{Y}_{1}=X\Gamma+\tilde{Y}_{2}\Psi+\tilde{e}_{1.2}\]

gives

\[\tilde{\Gamma}=\left[\matrix{74.179&-0.1209&-0.00630\cr 82.785&0.1162&-0.00640 \cr 68.730&2.4926&-0.13528\cr}\right],\]

\[\tilde{\Psi}=[0.1998,-0.02533,0.000635],\]

and the error statistic

\[\tilde{E}\equiv\tilde{E}_{1.2}=\left[\begin{array}{ccc}477.88&-12.95&-0.2176 \\ -12.95&13.17&-0.6519\\ -0.2176&-0.6519&0.0368\end{array}\right]\]

with 26 degrees of freedom. Again, \(X\) is the full rank model matrix associated with the cell means parameterization, so \(\Gamma\) is estimable.

The reduced model for no differences between drugs in the regression coefficients is

\[\tilde{Y}_{1}=J\mu^{\prime}+\tilde{Y}_{2}\Psi+\tilde{e}_{1.2},\]

with an error statistic of

\[\tilde{E}_{0}=\left[\matrix{1470.31&-162.16&8.01\cr-162.16&54.29&-2.76\cr 8.01&-2.7 6&0.14\cr}\right].\]

For testing the reduced model, the hypothesis statistic is

\[\tilde{H}=\tilde{E}_{0}-\tilde{E}=\left[\matrix{992.43&-149.21&8.23\cr-149.21& 41.12&-2.11\cr 8.23&-2.11&0.11\cr}\right].\]

The standard test statistics are \(U=0.064\), \(\phi_{\max}=5.43\), \(T^{2}=185.3\), \(V=1.434\). There are clear differences due to drugs in the coefficients of the parabolas.

We can now repeat the detailed analysis of the parabolas given in Example 11.3.1. The full and reduced models used in the analysis are all the same except that all now include the covariate matrix \(\tilde{Y}_{2}\). The specific models used for various tests were discussed in the earlier example.

In looking at the coefficients of the quadratic terms,

\[\hat{\gamma}_{32}=-0.13528,\]

with

\[{\rm SE}(\hat{\gamma}_{32})=0.01225\]

and

\[t_{obs}=-11.04\.\]These results can be obtained by fitting the univariate linear model \(\tilde{Y}_{12}=X\gamma_{2}+\tilde{Y}_{2}\psi_{2}+e_{2}\), where \(\tilde{Y}_{12}\) is the third component of \(\tilde{Y}_{1}=[\tilde{Y}_{10},\tilde{Y}_{11},\tilde{Y}_{12}]\). The analogous results in Example 11.5.1 were obtained by analyzing the model without the covariate \(\tilde{Y}_{2}\). This analogy holds between all the models of Example 11.5.1 and the models needed here, so only summary statistics are given in the remainder of this example.

For testing \(H_{0}\): \(\gamma_{12}=\gamma_{22}=0\), the error sums of squares for the full and reduced models are

\[\text{SSE}(\mathit{Full})=0.036753;\qquad\text{SSE}(\mathit{Red.})=0.037550\]

and the \(F\) statistic is

\[F=0.282,\]

which is not significant.

For looking at equality of the slopes in the placebo and drug A,

\[\text{SSE}(\mathit{Full})=13.166;\qquad\text{SSE}(\mathit{Red.})=13.447\]

and

\[F=0.555.\]

For testing whether the slopes are both zero, to three decimal places we again happen to have

\[\text{SSE}(\mathit{Red.})=13.447,\]

but the numerator has two degrees of freedom, so the \(F\) statistic is half as large:

\[F=0.278.\]

Neither test is significant.

To test for differences in intercepts between the placebo and drug A

\[\text{SSE}(\mathit{Full})=477.88;\qquad\text{SSE}(\mathit{Red.})=848.13\]

and

\[F=20.14.\]

Clearly, the placebo and drug A have different intercepts.

As in Example 11.5.1, we have found that heart rates are fairly constant for the placebo and drug A. The rates are approximately 74 and 83 beats per minute. Drug B again follows a parabola. The coefficients are remarkably close in the two analyses. 

Rao (1965) has suggested that the analysis of covariance procedure be used only with columns of \(\tilde{Y}_{2}\) that are important contributors to model (11.3.2). Formal tests can be made for the various columns, but these depend on the choice of \(G\).

Example 11.3.2. To test whether \(\tilde{Y}_{2}\) contributes additional information to the model using the heart rate data with \(G=I\), the error matrix from Example 11.2.1is the appropriate \(\tilde{E}_{0}\), i.e., \(\tilde{E}_{0}=[n-r(X)]\tilde{S}_{1}\) from Example 11.2.1, and the error matrix from Example 11.3.1 is \(\tilde{E}\). Subtracting these gives

\[\tilde{H}=\left[\begin{array}{ccc}47.18844&-5.98145&0.15006\\ -5.98145&0.75819&-0.01902\\ 0.15006&-0.01902&0.00048\end{array}\right],\]

and multiplying by \(E^{-1}\) leads to

\[T^{2}=4.45.\]

The comparison value for \(T^{2}\) is 3, so there is no overwhelming evidence of the importance of \(\tilde{Y}_{2}\). The hypothesis matrix \(H\) has only one degree of freedom, so McKeon's approximate distribution for \(T^{2}\) is exact. The formal test is based on comparing

\[\frac{T^{2}}{dfE}\,\frac{dfE+1-q}{q}=\frac{4.45}{26}\,\frac{24}{3}=1.37\]

to an \(F(q,dfE+1-q)\) distribution. The result is far from significant. This is consistent with the fact that the results of our two analyses of these data are not very different. \(\Box\)

##### Exercise 11.4.

1. Show that the estimate of the covariance matrix \(\tilde{\Sigma}_{1\cdot 2}\) does not depend on the choice of the matrix \(G\) in \(W_{1}\).
2. Show that \(W\) is nonsingular if and only if \(C(W_{1})\cap C(W_{2})=\{0\}\).
3. Show that \(C(W_{1})\cap C(W_{2})=\{0\}\).

Hint: Since \(G\) is positive definite, so is \(G^{-1}\). Recall that \(v=0\) if and only if \(v^{\prime}Gv=0\).

### Rao's Simple Covariance Structure

Rao's _Simple Covariance Structure (SCS)_ is a class of covariance matrices \(\Sigma\) for which least squares estimates are optimal. Under a SCS, the least squares analysis illustrated in Sect. 11.2 is optimal. Under SCS the MACOVA analysis in Sect. 11.3 becomes unnecessary but that analysis provides a useful test of whether the SCS is an appropriate model for the data. Finally, the optimality of least squares also suggests a way of testing a reduced model \(XI_{0}Z_{0}^{\prime}\) in lieu of the full model \(XI\Gamma Z^{\prime}\).

As in the previous section, let \(C(W_{2})=C(Z)^{\perp}\). Rao's SCS is a model for the covariance matrix in which

\[\Sigma=Z\tilde{D}Z^{\prime}+W_{2}\Theta W_{2}^{\prime},\]see Geisser (1970). In the general SCS model, parts or all of \(\tilde{D}\) and \(\Theta\) can be unknown parameters. In particular, the covariance models

\[\Sigma=\sigma_{s}^{2}I_{q}+\sigma_{w}^{2}J_{q}^{q}\quad\text{and}\quad\Sigma= \sigma_{0}^{2}I_{q}+ZDZ^{\prime}\]

are special cases of the SCS. The first of these is known variously as the _intraclass correlation_ structure, the _compound symmetry_ structure, and the _equal covariance_ structure. For this to be of SCS we need \(J_{q}\in C(Z)\). The second covariance structure is just a mixed model but is often called the _random regression coefficients_ structure and most often involves an unstructured covariance matrix \(D\). Most often, compound symmetry is a special case of random regression coefficients. In particular, compound symmetry is determined by adding a random (intercept) effect for each individual. In the growth curve context, adding a random slope and intercept for each individual gives a random regression coefficient structure.

#### Exercise 11.5.

Show that compound symmetry and random regression coefficients are special cases of SCS. Hint: Write \(I=M_{Z}+(I-M_{Z})\).

With SCS and using \(PA\) Proposition 2.7.5, because \(C(\Sigma Z)\subset C(Z)\) we have \(A_{Z}=M_{Z}\) and least squares estimates become optimal in model (11.1.1). Potthoff and Roy's (1964) method for analyzing generalized multivariate linear models in Sect. 11.2 is typically suboptimal because it is based on suboptimal estimates of the parameters in \(\Gamma\), but knowing that least squares estimates are optimal provides a strong justification for using the least squares methods illustrated in Sect. 11.2.

The analysis of the multivariate linear model

\[\tilde{Y}_{1}=X\Gamma+\tilde{e}_{1}, \tag{11.4.1}\]

is based on having an unstructured covariance matrix for the rows of \(\tilde{Y}_{1}\equiv YZ(Z^{\prime}Z)^{-1}\). If we assume that \(\Sigma=\sigma_{s}^{2}I_{q}+\sigma_{w}^{2}J_{q}^{q}\) with \(J_{q}\in C(Z)\), it is possible to show that the univariate version of the GMLM (11.1.2) is a generalized split plot model as discussed in \(PA\) Chap. 11 and Sect. 11.6, and therefore can be analyzed with the more powerful methods available for split plot models rather than with multivariate methods for model (11.4.1).

On the other hand, if we assume that \(\Sigma=\sigma^{2}I+ZDZ^{\prime}\) where \(D\) is an unknown and unstructured covariance matrix, the covariance matrix associated with model (11.4.1) is

\[(Z^{\prime}Z)^{-1}Z^{\prime}\Sigma Z(Z^{\prime}Z)^{-1}=\sigma^{2}(Z^{\prime}Z) ^{-1}+D.\]

We will see later that unbiased estimates of the parameters \(\sigma^{2}\) and \(D\) exist. But observe that if \(D\) is an unstructured covariance matrix, there is no usable structure in the covariance of model (11.4.1) that could improve estimation or inference in that model.

In the general SCS case,

\[(Z^{\prime}Z)^{-1}Z^{\prime}\Sigma Z(Z^{\prime}Z)^{-1}=\tilde{D}\]and regardless of whether \(\tilde{D}\) contains known structure, the analysis based on the multivariate linear model (11.4.1) remains valid, even if it may be suboptimal when \(\tilde{D}\) is structured.

#### Reduced Models in \(Z\)

While standard multivariate linear model theory applied to model (11.4.1) allows testing hypotheses involving \(\Gamma\), another approach to testing reduced models relative to the \(Z\Gamma^{\prime}\) structure is available through the device of testing for additional information as in Sect. 10.5. The details of testing were described there. Here we illustrate how to partition the data into two parts so as to allow testing of whether one part contains useful information.

Suppose \(Z=[Z_{0},Z_{1}]\) and we want to test whether the reduced model

\[Y=XT_{0}Z_{0}^{\prime}+e\]

provides an adequate fit. Using ideas from analysis of covariance, without loss of generality we can replace \(Z\) in model (11.1.1) with \([Z_{0},(I-M_{Z0})Z_{1}]\) where \(M_{Z0}=Z_{0}(Z_{0}^{\prime}Z_{0})^{-1}Z_{0}^{\prime}\). Then, instead of examining model (11.1.1), we examine the equivalent growth curve model

\[Y=X\left[\begin{array}{cc}I_{0},&I_{1}\end{array}\right]\left[\begin{array}[ ]{c}Z_{0}^{\prime}\\ Z_{1}^{\prime}(I-M_{Z0})\end{array}\right]+e=X\Gamma_{0}Z_{0}^{\prime}+XT_{1}Z_ {1}^{\prime}(I-M_{Z0})+e. \tag{11.4.2}\]

With SCS, instead of transforming model (11.1.1) into model (11.4.1) we transform model (11.4.2) into

\[[\tilde{Y}_{10},\tilde{Y}_{11}]\equiv[YZ_{0}(Z_{0}^{\prime}Z_{0})^{-1},Y(I-M_{ Z0})Z_{1}\left\{Z_{1}^{\prime}(I-M_{Z0})Z_{1}\right\}^{-1}]=X\left[\begin{array}{cc }I_{0},&I_{1}\end{array}\right]+\tilde{e}. \tag{11.4.3}\]

We can now test the efficacy of \(XT_{1}Z_{1}^{\prime}(I-M_{Z0})\) in model (11.4.2) by using the standard test from Sect. 10.5 of whether \(\tilde{Y}_{11}\) adds additional information beyond that provided by \(\tilde{Y}_{10}\) in model (11.4.3).

#### Unbiased Covariance Parameter Estimation

As mentioned earlier and demonstrated in Sect. 11.6, with \(\Sigma=\sigma_{s}^{2}I_{q}+\sigma_{w}^{2}J_{q}^{q}\) and \(J_{q}\in C(Z)\), the growth curve model is a generalized split plot model, so unbiased estimates of \(\sigma_{s}^{2}\) and \(\sigma_{w}^{2}\) are available from that theory.

For general SCS, we mentioned earlier that

\[(Z^{\prime}Z)^{-1}Z^{\prime}\Sigma Z(Z^{\prime}Z)^{-1}=\tilde{D}.\]Model (11.4.1) then provides an unbiased estimate of an unstructured matrix \(\tilde{D}\) via

\[(Z^{\prime}Z)^{-1}Z^{\prime}Y^{\prime}(I-M)YZ(Z^{\prime}Z)^{-1}/[n-r(X)].\]

Of course if \(\tilde{D}\) contains structure, this estimate will not be particularly efficient. For example, under the assumptions associated with compound symmetry,

\[(Z^{\prime}Z)^{-1}Z^{\prime}\Sigma Z(Z^{\prime}Z)^{-1}=\sigma_{s}^{2}(Z^{ \prime}Z)^{-1}+\sigma_{w}^{2}bb^{\prime}\]

for a vector \(b\) with \(J_{q}=Zb\). We can get better estimates of this matrix by estimating the individual parameters from the generalized split plot model. Incidentally, it is not hard to see that an unbiased estimate of \(\Theta\) is available from

\[(W_{2}^{\prime}W_{2})^{-1}W_{2}^{\prime}Y^{\prime}(I-M)YW_{2}(W_{2}^{\prime}W_ {2})^{-1}/[n-r(X)].\]

We devote most of our attention to looking at unbiased estimation of the parameters in \(\Sigma=\sigma^{2}I_{q}+ZDZ^{\prime}\). In particular, we show how to write a growth curve model with this covariance structure as a mixed model as in Chap. 5. For the purpose of finding unbiased covariance parameter estimates, we assume a slightly more general covariance structure \(\Sigma=\sigma^{2}I_{q}+(ZU)D(U^{\prime}Z^{\prime})\) where \(D\) is an unstructured covariance matrix but \(U\) is a known matrix. Incorporating \(U\) allows the possibility of involving, say, quadratic growth curves or linear growth curves with harmonic effects in \(Z\), but restricting the random effects for individuals to be only, say, random slopes and intercepts.

On an individual level the mixed model generating such a covariance structure is

\[y_{i}=Z\Gamma^{\prime}x_{i}+ZU\xi_{i}+\epsilon_{i},\qquad\text{Cov}(\xi_{i})=D,\qquad\text{Cov}(\epsilon_{i})=\sigma^{2}I_{q},\]

which, as advertised, gives \(\Sigma=\sigma^{2}I_{q}+ZUDU^{\prime}Z^{\prime}\) under the usual assumptions. An unbiased estimate of

\[\sigma^{2}(U^{\prime}Z^{\prime}U)^{-1}+D\]

is

\[(U^{\prime}Z^{\prime}U)^{-1}U^{\prime}Z^{\prime}Y^{\prime}(I-M)YZU(U^{\prime}Z ^{\prime}U)^{-1}/[n-r(X)].\]

An unbiased estimate of \(\sigma^{2}\) will then provide us with an unbiased estimate of \(D\).

The mixed model for the complete data is

\[\text{Vec}(Y)=[Z\otimes X]\text{Vec}(\Gamma)+[ZU\otimes I_{n}]\text{Vec}( \Xi)+\text{Vec}(e),\]

\[\text{Cov}\left[\text{Vec}(\Xi)\right]=[D\otimes I_{n}],\quad\text{Cov}\left[ \text{Vec}(e)\right]=[\sigma^{2}I_{m}\otimes I_{n}],\]

\[\text{E}\left[\text{Vec}(\Xi)\right]=0,\quad\text{Cov}\left[\text{Vec}(\Xi), \text{Vec}(e)\right]=0,\quad\text{E}\left[\text{Vec}(e)\right]=0.\]

From Henderson's method 3, an unbiased estimate of \(\sigma^{2}\) is available from

\[\text{Vec}(Y)^{\prime}(I-\mathcal{P})\text{Vec}(Y)/[n-r(\mathcal{P})],\]where \(\mathcal{P}\) is the ppo onto \(C([Z\otimes X],[ZU\otimes I_{n}])\). In the special case that \(U=I_{r}\), this estimate is just the mean square for subplot error in the generalized split plot model associated with model (11.1.1).

### Testing the SCS Assumption

The virtue of the MACOVA method in Sect. 11.3 is that it automatically incorporates an estimate of \(\Sigma\) into the process of estimating \(\Gamma\) while retaining the ability to conduct inferences using results known for multivariate linear models. With the SCS, least squares estimates are optimal so there is no need to estimate \(\Sigma\) in order to obtain a good estimate of \(\Gamma\). However, the MACOVA model still provides a valuable test, namely a test of the validity of the SCS that we have assumed for \(\Sigma\).

For the SCS, the MACOVA method uses \(W_{1}=Z(Z^{\prime}Z)^{-1}\) and takes \(W_{2}\) to be a full column rank matrix with \(C(W_{2})=C(Z)^{\perp}\). With these choices the matrix \(W=[W_{1},W_{2}]\) is nonsingular so the GMLM (11.1.1) is equivalent to

\[YW=X\Gamma ZW+e.\]

Exploiting the partitioning in \(W\) we get

\[[\tilde{Y}_{1},\tilde{Y}_{2}]\equiv YW=[X\Gamma,0]+e.\]

This leads to the ACOVA model

\[\tilde{Y}_{1}=X\Gamma+\tilde{Y}_{2}\Psi+e_{1.2}\]

where

\[\Psi=W_{1}^{\prime}\Sigma W_{2}(W_{2}^{\prime}\Sigma W_{2})^{-1}.\]

However, under SCS,

\[W_{1}^{\prime}\Sigma W_{2}=(Z^{\prime}Z)^{-1}Z^{\prime}[Z\tilde{D}Z^{\prime}+W _{2}\Theta W_{2}^{\prime}]W_{2}=0.\]

Thus, rejecting a test of \(H_{0}:\Psi=0\), provides evidence that the SCS is incorrect. In particular, if one assumes either compound symmetry or random regression coefficients, rejecting this test is evidence that those models are incorrect. However, it is possible for those models to be incorrect even when SCS is true.

Example 11.3.2 provides a test for whether the heart rate data are consistent with a quadratic growth curve model with a SCS.

### Longitudinal Data

Models for the analysis of longitudinal (_repeated measures_) data are primarily concerned with the behavior of individuals over time. Both profile analysis and growth curve models can address the issue of behavior over time. In this section, we examine the structure of generalized multivariate linear models in more detail and generalize that structure to unbalanced data. We begin with the full data case. For simplicity we discuss modifications of the one-way MANOVA model.

#### Full Data

Consider the growth curve model (10.4.1) based on a one-way MANOVA. On an individual level, the model is a \(q\) dimensional linear model

\[y_{ij}=Z\gamma_{i}+\varepsilon_{ij},\quad\varepsilon_{ij}\sim N(0,\Sigma),\]

with \(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\) and different \(y_{ij}\)s independent. This model is appropriate when we have samples of individuals from each of \(a\) groups with exactly \(q\) observations on each individual, observations that are taken at exactly the same times. Typically, the columns of \(Z\) are functions of time. For growth curves, these functions of time can be polynomials as illustrated in Sect. 10.4, or other functions used for curve fitting like sines and cosines.

To create an overall linear model from the growth curve model (11.1.1) we find it convenient to transpose the model and write

\[\mathrm{Vec}(Y^{\prime})=\mathrm{Vec}(Z\Gamma^{\prime}X^{\prime})+\mathrm{Vec} (e^{\prime}),\quad\mathrm{E}[\mathrm{Vec}(e^{\prime})]=0_{nq},\quad\mathrm{Cov }[\mathrm{Vec}(e^{\prime})]=[I_{n}\otimes\Sigma].\]

Using result 8 from Appendix A.2, this can be written as the linear model

\[\mathrm{Vec}(Y^{\prime})=[X\otimes Z]\mathrm{Vec}(\Gamma^{\prime})+\mathrm{Vec }(e^{\prime}),\quad\mathrm{E}[\mathrm{Vec}(e^{\prime})]=0_{nq},\quad\mathrm{ Cov}[\mathrm{Vec}(e^{\prime})]=[I_{n}\otimes\Sigma].\]

With \(X\) a matrix of group indicators having \(N_{i}\) observations in group \(i\), the overall linear model is\[\left[\begin{array}{c}y_{11}\\ \vdots\\ y_{1N_{1}}\\ y_{21}\\ \vdots\\ y_{2N_{2}}\\ \vdots\\ y_{a1}\\ y_{a1}\\ \vdots\\ y_{aN_{a}}\end{array}\right]=\left[\begin{array}{ccc}Z&0&0\\ \vdots&\vdots&\vdots\\ Z&0&0\\ 0&Z&0\\ \vdots&\vdots&\vdots\\ 0&Z&0\\ \vdots&\ddots&\vdots\\ 0&0&Z\\ \vdots&\vdots&\vdots\\ 0&0&Z\end{array}\right]\left[\begin{array}{c}\gamma_{1}\\ \gamma_{2}\\ \vdots\\ \gamma_{a}\end{array}\right]+\left[\begin{array}{c}\varepsilon_{11}\\ \vdots\\ \varepsilon_{1N_{1}}\\ \varepsilon_{21}\\ \vdots\\ \varepsilon_{2N_{2}}\\ \vdots\\ \varepsilon_{a1}\\ \varepsilon_{aN_{a}}\end{array}\right].\]

This is a generalized multivariate linear model. None of the entries in this model is a scalar.

To explore the model further, partition

\[Z=[J_{q},\,Z_{*}]\quad\text{and}\quad\gamma_{i}=\left[\begin{array}{c}\gamma _{i0}\\ \gamma_{i*}\end{array}\right].\]

A model without group-time interaction would have \(\gamma_{1*}=\cdots=\gamma_{a*}\equiv\gamma_{*}\). The corresponding model is

\[\left[\begin{array}{c}y_{11}\\ \vdots\\ y_{1N_{1}}\\ \vdots\\ y_{a1}\\ \vdots\\ y_{aN_{a}}\end{array}\right]=\left[\begin{array}{ccc}J_{q}&0&Z_{*}\\ \vdots&\vdots&\vdots\\ J_{q}&0&Z_{*}\\ \ddots&&\vdots\\ 0&J_{q}&Z_{*}\\ \vdots&&\vdots\\ 0&J_{q}&Z_{*}\end{array}\right]\left[\begin{array}{c}\gamma_{10}\\ \vdots\\ \vdots\\ \gamma_{a0}\\ \gamma_{*}\end{array}\right]+\left[\begin{array}{c}\varepsilon_{11}\\ \vdots\\ \varepsilon_{1N_{1}}\\ \vdots\\ \varepsilon_{a1}\\ \vdots\\ \varepsilon_{aN_{a}}\end{array}\right].\]

This does not seem to be a generalized multivariate linear model but it can be written as \(y_{ij}=J_{q}\gamma_{i0}+Z_{*}\gamma_{*}+\varepsilon_{ij}\), \(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\).

If there is no interaction, we can consider a model without group effects,

\[\left[\begin{array}{c}y_{11}\\ \vdots\\ y_{aN_{a}}\end{array}\right]=\left[\begin{array}{cc}J_{q}&Z_{*}\\ \vdots&\vdots\\ J_{q}&Z_{*}\end{array}\right]\left[\begin{array}{c}\gamma_{0}\\ \gamma_{*}\end{array}\right]+\left[\begin{array}{c}\varepsilon_{11}\\ \vdots\\ \vdots\\ \varepsilon_{aN_{a}}\end{array}\right].\]

Equivalently, we can write this as \(y_{ij}=Z\gamma+\varepsilon_{ij}\), \(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\) or as \(\text{Vec}(Y^{\prime})=\text{Vec}(Z\gamma f^{\prime}_{n})+\text{Vec}(e^{\prime})\).

If there is no interaction, we can also consider a model without time effects:

\[\left[\begin{array}{c}y_{11}\\ \vdots\\ y_{1N_{1}}\\ \vdots\\ y_{a1}\\ \vdots\\ y_{aN_{a}}\end{array}\right]=\left[\begin{array}{ccc}J_{q}&&0\\ \vdots&&\vdots\\ J_{q}&&0\\ &\ddots&\\ 0&&J_{q}\\ \vdots&&\vdots\\ 0&&J_{q}\end{array}\right]\left[\begin{array}{c}\gamma_{10}\\ \vdots\\ \vdots\\ \gamma_{a0}\end{array}\right]+\left[\begin{array}{c}\varepsilon_{11}\\ \vdots\\ \varepsilon_{1N_{1}}\\ \vdots\\ \varepsilon_{a1}\\ \vdots\\ \varepsilon_{aN_{a}}\end{array}\right].\]

This model can be written as \(y_{ij}=J_{q}\gamma_{0}+\varepsilon_{ij}\), \(i=1,\ldots,a\), \(j=1,\ldots,N_{i}\) or \(\text{Vec}(Y^{\prime})=\text{Vec}(J_{q}\gamma_{0}^{\prime}X^{\prime})+\text{Vec }(e^{\prime})\).

Obviously, when the groups have factorial structure, higher order ANOVA models for the groups can be incorporated into the analysis.

An additional complication allows \(s\) covariates to be measured for each individual. If \(w_{ij}\) is an \(s\) vector of covariates for the \(ij\) individual, say, initial systolic and diastolic blood pressures and cholesterol, we can also incorporate such terms into the overall model as

\[\left[\begin{array}{c}y_{11}\\ \vdots\\ y_{1N_{1}}\\ y_{21}\\ \vdots\\ y_{2N_{2}}\\ \vdots\\ y_{a1}\\ \vdots\\ y_{aN_{a}}\end{array}\right]=\left[\begin{array}{ccc}Z&0&0&J_{q}w_{11}^{ \prime}\\ \vdots&\vdots&\vdots\\ Z&0&0&J_{q}w_{1N_{1}}^{\prime}\\ 0&Z&0&J_{q}w_{21}^{\prime}\\ \vdots&\vdots&\vdots\\ 0&Z&0&J_{q}w_{2N_{2}}^{\prime}\\ &\ddots&&\vdots\\ 0&0&Z&J_{q}w_{a1}^{\prime}\\ \vdots&\vdots&\vdots&\vdots\\ 0&0&Z&J_{q}w_{aN_{a}}^{\prime}\end{array}\right]\left[\begin{array}{c}\gamma _{1}\\ \gamma_{2}\\ \vdots\\ \gamma_{a}\\ \varepsilon_{a1}\\ \vdots\\ \varepsilon_{aN_{a}}\end{array}\right]+\left[\begin{array}{c}\varepsilon_{1 1}\\ \vdots\\ \varepsilon_{1N_{1}}\\ \varepsilon_{21}\\ \vdots\\ \varepsilon_{2N_{2}}\\ \varepsilon_{a1}\\ \vdots\\ \varepsilon_{aN_{a}}\end{array}\right].\]

Defining the \(n\times s\) matrix \(W\) to have an \(ij\) row of \(w_{ij}^{\prime}\), the model can be written

\[\text{Vec}(Y^{\prime})=[X\otimes Z]\text{Vec}(\Gamma^{\prime})+[W\otimes J_{q }]\xi+\text{Vec}(e^{\prime}),\]

\[\text{E}[\text{Vec}(e^{\prime})]=0_{nq},\qquad\text{Cov}[\text{Vec}(e^{\prime })]=[I_{n}\otimes\Sigma].\]

Similarly, covariates can be incorporated into the no interaction model, the time effects only model, and the group effects only model. Notice that these covariates are not time dependent, for example, blood pressure is only allowed to be measured once for each person, not measured at each time. It is not hard to see how this model could be modified to allow the covariate effects to depend on the group \(i\). However, when the data come from a randomized experiment, models that incorporate treatment by covariate interaction create problems with causal interpretations of the results, cf. Cox (1958) or Christensen (2015, Section 17.8).

All of these are linear models with a parametric covariance structure so all of them are subject to the results of Chap. 4. For models that are also generalized multivariate linear models, the methods of the previous section provide data analysis. The incorporation of time-dependent covariates into the analysis eliminates any simplifying structure, so models with time-dependent covariates are modeled as if they have incomplete data.

#### Incomplete Data

With longitudinal data, it is rare to observe every person at every time, but the modeling ideas that apply to complete data also apply to incomplete data.

A natural generalization of the growth curve model is to allow different numbers of observations on different individuals making \(y_{ij}\) a \(q(ij)\) vector. There are a total of \(q\) observations possible, but only \(q(ij)\leq q\) are observed. With different numbers of measurements, the times of the measurements depend on the individual; hence we generalize \(Z\) to matrices \(Z_{ij}\) that depend on the individual. Now, model the data as

\[y_{ij}=Z_{ij}\gamma_{i}+\varepsilon_{ij},\quad\varepsilon_{ij}\sim N(0, \Sigma_{ij}(\theta)). \tag{11.5.1}\]

The \(y_{ij}\)s remain independent, and \(\Sigma_{ij}(\theta)\) is a function of some parameters \(\theta\) that are the same regardless of the individual. The matrix \(Z_{ij}\) is \(q(ij)\times r\) for \(r\leq q\). It is permissible to have \(r>q(ij)\) for some \(ij\)s. If \(r=q\), typically no additional structure is being imposed related to the times, so \(C(Z_{ij})=C(I_{q(ij)})=\mathbf{R}^{q(ij)}\), and the model is similar to a two-way ANOVA with interaction, where groups and times are the factors but with missing data and correlated observations. In the original growth curve model, it is hard to imagine \(Z\) depending on anything other than the times of measurement because \(Z\) must be the same for every individual. However, with \(Z\) depending on \(i\) and \(j\), we can easily incorporate interesting additional covariates into \(Z_{ij}\). For example, these measurements could include such time-dependent covariates as the weight and blood pressure of an individual taken at each time.

For each \(ij\), the longitudinal model (11.5.1) is simply a linear model with a parametric covariance matrix. The individual models can be combined into a model for the total data \[\begin{bmatrix}y_{11}\\ \vdots\\ y_{1N_{1}}\\ y_{21}\\ \vdots\\ y_{2N_{2}}\\ \vdots\\ y_{a1}\\ \vdots\\ y_{aN_{a}}\end{bmatrix}=\begin{bmatrix}Z_{11}&0&0\\ \vdots&\vdots&\vdots\\ Z_{1N_{1}}&0&0\\ 0&Z_{21}&0\\ \vdots&\vdots&\vdots\\ 0&Z_{2N_{2}}&0\\ 0&0&Z_{a1}\\ \vdots&\vdots&\vdots\\ 0&0&Z_{aN_{a}}\end{bmatrix}\begin{bmatrix}\gamma_{1}\\ \gamma_{2}\\ \vdots\\ \vdots\\ \gamma_{a}\end{bmatrix}+\begin{bmatrix}\varepsilon_{11}\\ \varepsilon_{1N_{1}}\\ \varepsilon_{21}\\ \vdots\\ \varepsilon_{2N_{2}}\\ \vdots\\ \varepsilon_{a1}\\ \vdots\\ \varepsilon_{aN_{a}}\end{bmatrix}, \tag{11.5.2}\]

with

\[\text{E}\begin{bmatrix}\varepsilon_{11}\\ \vdots\\ \varepsilon_{aN_{a}}\end{bmatrix}=0\quad\text{and}\quad\text{Cov}\begin{bmatrix} \varepsilon_{11}\\ \vdots\\ \varepsilon_{aN_{a}}\end{bmatrix}=\text{Blk diag}[\Sigma_{ij}(\theta)].\]

This is _not_ a standard multivariate linear model or generalized multivariate linear model, so the estimation and testing results given earlier do not apply. The analysis depends only on the general results of Chap. 4.

To explore this model further, partition

\[Z_{ij}=[J_{q(ij)},\quad Z_{ij*}\,]\quad\text{and}\quad\gamma_{i}=\begin{bmatrix} \gamma_{0}\\ \gamma_{*}\end{bmatrix}.\]

We now present a model in which the time dependent covariates (including any simple functions of time that are the same for all individuals) do not interact with the treatment groups. A model without group-time interaction would have \(\gamma_{1*}=\cdots=\gamma_{a*}\equiv\gamma_{*}\). The corresponding model is

\[\begin{bmatrix}y_{11}\\ \vdots\\ y_{1N_{1}}\\ \vdots\\ y_{a1}\\ \vdots\\ y_{aN_{a}}\end{bmatrix}=\begin{bmatrix}J_{q(11)}&0&Z_{11*}\\ \vdots&\vdots&\vdots\\ J_{q(1N_{1})}&&0&Z_{1N_{1}*}\\ \ddots&&&\vdots\\ 0&J_{q(a1)}&Z_{a1*}\\ \vdots&&\vdots&\vdots\\ 0&J_{q(aN_{a})}&Z_{aN_{a}*}\end{bmatrix}\begin{bmatrix}\gamma_{10}\\ \vdots\\ \vdots\\ \gamma_{a0}\\ \gamma_{*}\end{bmatrix}+\begin{bmatrix}\varepsilon_{11}\\ \vdots\\ \varepsilon_{1N_{1}}\\ \vdots\\ \varepsilon_{a1}\\ \vdots\\ \varepsilon_{aN_{a}}\end{bmatrix}.\]

If there is no interaction, we can consider a model without group effects,

\[\begin{bmatrix}y_{11}\\ \vdots\\ y_{aN_{a}}\end{bmatrix}=\begin{bmatrix}J_{q(11)}&Z_{11*}\\ \vdots&\vdots\\ J_{q(aN_{a})}&Z_{aN_{a}*}\end{bmatrix}\begin{bmatrix}\gamma_{0}\\ \gamma_{*}\end{bmatrix}+\begin{bmatrix}\varepsilon_{11}\\ \vdots\\ \varepsilon_{aN_{a}}\end{bmatrix},\]

[MISSING_PAGE_FAIL:460]

To construct standard errors and tests of reduced models, use the general ideas discussed in Chap. 4, specifically in Sects. 4.1.2 and 4.7.

The biggest modeling concern is what to use for the covariance matrix \(\Sigma_{ij}(\theta)\). One possibility is to take \(\Sigma_{ij}(\theta)\) as a submatrix of a \(q\times q\) positive definite matrix of parameters \(\Sigma\). The submatrix is determined by which \(q(ij)\) of the \(q\) possible times are observed in \(y_{ij}\). Another possibility is to use a mixed model

\[y_{ij}=Z_{ij}\gamma_{i}+W_{ij}b_{ij}+\xi_{ij},\quad b_{ij}\sim N(0,D),\quad\xi_ {ij}\sim N(0,\sigma^{2}I_{q(ij)}),\]

with the \(b_{ij}\)s and \(\xi_{ij}\)s all independent. In this model,

\[\Sigma_{ij}(\theta)=W_{ij}DW^{\prime}_{ij}+\sigma^{2}I_{q(ij)},\]

where \(D\) is a matrix of parameters, all of which need to be estimated along with \(\sigma^{2}\). Alternatively, one could let \(D\) depend on some smaller number of parameters. Note that the size of \(D\) does not depend on \(ij\), so the number of columns in \(W_{ij}\) must not depend on \(ij\). In particular, \(W_{ij}=J_{q(ij)}\) leads to a random effect for each individual and \(W_{ij}\) can also model random slopes and intercepts for each individual.

Other models for \(\Sigma_{ij}(\theta)\) can be taken from time domain analysis (see Sect. 7.2), spatial data analysis (see Sect. 8.6), the next section on functional data, or linear combinations of the various possibilities. In particular, one can think of time as a one-dimensional space making the spatial data models immediately applicable.

Once a covariance structure is determined, the model can be fitted by maximum likelihood or a combination of REML and empirical generalized least squares. While these models look more complicated than spacial data models, they are in fact simpler. For each individual \(ij\), the longitudinal model (11.5.1) is fundamentally similar to the universal kriging models considered in Chap. 8 and to the mixed models considered in Chap. 5. However, this data structure should be the fondest wish of anyone doing either spatial data or mixed models. Longitudinal data provide the luxury of having independent replications providing extra information on the common parameters. In particular, the methods for fitting spatial data models discussed in Chap. 8 all apply to these models.

For more extensive discussions of longitudinal data analysis see Hand and Crowder (1996), Diggle, Heagerty, Liang, and Zeger (2002), or Fitzmaurice, Laird, and Ware (2011).

## 11.6 Functional Data Analysis

_Functional data analysis_ is really just longitudinal data analysis when \(q\) and the various \(q(ij)\)s are large. Unlike the previous section where we focused on one-way MANOVA type data, we here use \(h\) to identify individuals.

Consider two linear models for functional responses on each of \(n\) independent subjects, \(h=1,\ldots,n\). First, a model that concerns itself with relationships between subjects,

\[y_{h}(t)=x_{h}^{\prime}\beta(t)+\varepsilon_{h}(t), \tag{11.6.1}\]

where \(y_{h}(t)\) is the observed functional response, \(x_{h}\) is a known \(p\) vector, \(\beta(t)\) is a fixed unknown \(p\) dimensional parameter function and \(\varepsilon_{h}(t)\) is an unobservable random error function with mean \(0\) and covariance function \(\sigma(t,\bar{t})\).

Second is a model that treats all subjects the same but concerns itself with relationships over time

\[y_{h}(t)=\gamma^{\prime}z(t)+\varepsilon_{h}(t), \tag{11.6.2}\]

where \(z(t)\) is a known \(r\) dimensional function with \(r\leq q\) and \(\gamma\) is an unknown parameter vector common to all subjects.

Of course nobody has yet figured out how to actually record an infinite amount of data, so of necessity one must evaluate the functions at a finite number of times. For simplicity in introducing concepts, assume that all subjects are observed at the same times \(t_{j}\), \(j=1,\ldots,q\). Models (11.6.1) and (11.6.2) now correspond to standard longitudinal models. Restricting attention to the observed times, define

\[y_{h}=\begin{bmatrix}y_{h}(t_{1})\\ \vdots\\ y_{h}(t_{q})\end{bmatrix},\quad Y_{n\times q}=\begin{bmatrix}y_{1}^{\prime}\\ \vdots\\ y_{n}^{\prime}\end{bmatrix},\quad\varepsilon_{h}=\begin{bmatrix}\varepsilon_{ h}(t_{1})\\ \vdots\\ \varepsilon_{h}(t_{q})\end{bmatrix},\quad e=\begin{bmatrix}\varepsilon_{1}^{ \prime}\\ \vdots\\ \varepsilon_{n}^{\prime}\end{bmatrix},\]

and

\[\beta_{j}=\beta(t_{j}),\quad B_{p\times q}=\left[\beta_{1}\quad\cdots\quad \beta_{q}\right].\]

This allows us to write model (11.6.1) as the multivariate linear model

\[Y=XB+e. \tag{11.6.3}\]

If we further define

\[Z_{q\times r}=\begin{bmatrix}z(t_{1})^{\prime}\\ \vdots\\ z(t_{q})^{\prime}\end{bmatrix}\]

we can write model (11.6.2) as a one sample GMLM

\[Y=J\gamma^{\prime}Z^{\prime}+e \tag{11.6.4}\]

where \(J\) is an \(n\) vector of ones. Moreover, we can combine the modeling ideas of (11.6.1) and (11.6.2) into a GMLM

\[Y=XTZ^{\prime}+e \tag{11.6.5}\]

or

\[\text{Vec}(Y)=[Z\otimes X]\text{Vec}(\Gamma)+\text{Vec}(e).\]In all of these models,

\[\text{Cov}(y_{h})=\Sigma=[\sigma_{jj^{\prime}}],\quad\sigma_{jj^{\prime}}=\sigma(t _{j},t_{j^{\prime}})\]

so that

\[\text{Cov}[\text{Vec}(Y)]=[\Sigma\otimes I_{n}].\]

The fundamental assumption that subjects are independent is reflected by the \(I_{n}\) term of the covariance matrix. In the multivariate model (11.6.3), a standard assumption is that \(n\geq q+r(X)\). This provides an almost surely nonsingular (nonparametric) estimate of the covariance matrix \(\Sigma\).

As we did for longitudinal models, rewrite the GMLM as

\[\text{Vec}(Y^{\prime})=[X\otimes Z]\text{Vec}(\Gamma^{\prime})+\text{Vec}(e^{ \prime}),\quad\text{Cov}[\text{Vec}(Y^{\prime})]=[I_{n}\otimes\Sigma]\]

or

\[y_{h}=\sum_{j=1}^{p}x_{hj}Z\gamma_{j}+\varepsilon_{h},\quad i=1,\ldots,n. \tag{11.6.6}\]

In particular, if \(X\) corresponds to a one-way ANOVA, model (11.6.6) reduces to

\[y_{h}=Z\gamma_{j}+\varepsilon_{h},\]

where \(\gamma_{j}\) is the regression parameter vector unique to the group that contains subject \(h\). One advantage of this reexpression is that the covariance matrix becomes block diagonal.

A primary complication to these models viewed either as functional data or other longitudinal data is that frequently the observation times depend on the subject so that we observe subject \(h\) at \(t_{hj}\), \(j=1,\ldots,q(h)\). Without loss of generality, we consider all the \(t_{hj}\)s as contained in the set \(\{t_{j}|j=1,\ldots,q\}\) so that \(q(h)\leq q\). This leads to redefining

\[y_{h}=\begin{bmatrix}y_{h}(t_{h1})\\ \vdots\\ y_{h}(t_{hq(h)})\end{bmatrix}=\begin{bmatrix}y_{h1}\\ \vdots\\ y_{hq(h)}\end{bmatrix},\]

with a similar definition of \(\varepsilon_{h}\). Let \(W_{h}\) be a \(q(h)\times q\) matrix of indicators defined by

\[\begin{bmatrix}t_{h1}\\ \vdots\\ t_{hq(h)}\end{bmatrix}=W_{h}\begin{bmatrix}t_{1}\\ \vdots\\ t_{q}\end{bmatrix}\]

and define

\[Z_{h}=W_{h}Z\]

so that \(Z_{h}\) is a \(q(h)\times r\) matrix. The linear model (11.6.6) becomes

\[y_{h}=\sum_{j=1}^{p}x_{hj}Z_{h}\gamma_{j}+\varepsilon_{h},\quad i=1,\ldots,n. \tag{11.6.7}\]For model (11.6.7), denote

\[\text{Cov}(y_{h})=\Sigma_{h}=W_{h}\Sigma W_{h}^{\prime}\]

which is also determined by applying \(\sigma(t,\tilde{t})\) to the observation times associated with subject \(h\), that is,

\[\Sigma_{h}=[\sigma_{h,j,j^{\prime}}],\quad\sigma_{h,j,j^{\prime}}=\sigma(t_{hj },t_{h^{j^{\prime}}}).\]

A unique feature of functional data is that if one is willing to think of \(t\) as continuous, one should be prepared to deal with a large number of time observations, in particular, \(q>n\). To make progress in that situation, one must assume some parametric family for \(\sigma(t,\tilde{t})\). Wolfinger (1996) discussed a variety of covariance parameterizations for longitudinal models. Allowing for heteroscedasticity immediately involves \(q\) parameters. The most general models that Wolfinger considered were the antedependence, Toeplitz, and first order factor analytic models. For modeling covariances, the first two incorporate \(q-1\) additional parameters and the last adds \(q\) parameters. Thus these covariance models involve about \(2q\) parameters, whereas an unstructured covariance matrix involves \(q(q+1)/2\) free parameters. With \(q\) large, there is considerable room for developing "nonparametric" covariance estimates with more than \(2q\) but fewer than \(q(q+1)/2\) parameters.

One should also note the trade-off between fixed effects modeling of time effects and modeling the covariance structure. This trade-off is common wisdom in spatial data analysis. Typically, the more one invests in developing a complicated model for fixed effects, the less one needs to invest in developing a complicated covariance model. Relative to the models discussed here, a complicated fixed effects model for time corresponds to having \(Z_{h}\) matrices with a large number of columns \(r\).

The covariance function \(\sigma(t,\tilde{t})\) is a positive definite function. The theory of RKHSs relies on a result by Mercer (1909) that provides sufficient conditions on \(\sigma(t,\tilde{t})\) for being able to write

\[\sigma(t,\tilde{t})=\sum_{k=1}^{\infty}\theta_{k}\phi_{k}(t)\phi_{k}(\tilde{t})\]

for some functions \(\phi_{k}(t)\) and nonnegative \(\theta_{k}\)s. RKHS theory assumes that \(\sigma(t,\tilde{t})\) is known and avoids working with the functions \(\phi_{k}(t)\). Similar to Chap. 1, we instead use a collection of known spanning functions for the \(\phi_{k}(t)\)s and estimate the \(\theta_{k}\)s. Of course, we cannot handle an infinite sum so we use the approximation

\[\sigma(t,\tilde{t})\doteq\sum_{k=1}^{s}\theta_{k}\phi_{k}(t)\phi_{k}(\tilde{t}). \tag{11.6.8}\]

For our purposes, we would choose \(s\) between \(2q\) and \(q(q+1)/2\).

When applying (11.6.8)

\[\Sigma_{h}=[\sigma_{h,j,f^{\prime}}],\quad\sigma_{h,j,f^{\prime}}=\sigma(t_{hj},t_ {hj^{\prime}})=\sum_{k=1}^{s}\theta_{k}\phi_{k}(t_{hj})\phi_{k}(t_{hj^{\prime}}).\]

Incorporating this covariance structure into the models is quite simple via random effects. First define

\[\Phi_{hk}=[\,\phi_{k}(t_{h1})\quad\cdots\quad\phi_{k}(t_{hq(h)})\,]^{\prime}, \quad\text{and}\quad\Phi_{h}=[\,\Phi_{h1}\quad\cdots\quad\Phi_{hs}\,].\]

In any of the subjectwise models simply replace the term \(\varepsilon_{h}\) with a term \(\Phi_{h}b_{h}\) where \(b_{h}\) is a random \(s\) vector having mean \(0\), uncorrelated components, and variances \(\theta_{1},\ldots,\theta_{s}\). Thus, in model (11.6.7) fit

\[y_{h}=\sum_{j=1}^{p}x_{hj}Z_{h}\gamma_{j}+\Phi_{h}b_{h}.\]

Technically, this is very similar to the random coefficients models discussed by Wolfinger (1996). He discussed using both first order and second order polynomials, so \(s=1,2\). In addition, Wolfinger retained \(\varepsilon_{h}\) but assumed that \(\text{Cov}(\varepsilon_{h})=\sigma^{2}I_{q(h)}\), thus making a model

\[y_{h}=\sum_{j=1}^{p}x_{hj}Z_{h}\gamma_{j}+\Phi_{h}b_{h}+\varepsilon_{h}.\]

I can see no reason not to include such an error term. Relative to the trade-off between fixed effects and covariance modeling, using \(Z_{h}\)s with large \(r\) allow \(s\) to be smaller.

The use of sines and cosines for the basis functions is particularly appealing because of their close connection with frequency domain time series analysis. Let \(t_{q}\) be the last time at which any observation was taken, then

\[\phi_{2k-1}(t)=\cos\left(2\pi\frac{k}{t_{q}}t\right),\quad\text{and}\quad\phi _{2k}(t)=\sin\left(2\pi\frac{k}{t_{q}}t\right).\]

Note that if we assume \(\theta_{2k-1}=\theta_{2k}\) for all \(k\), both \(\sigma(t,\tilde{t})\) and its approximation will be homoscedastic.

Wolfinger also examined a factor analysis model, cf. Sect. 14.4, with

\[\Sigma=\begin{bmatrix}\theta_{01}&&\\ &\ddots&\\ &&\theta_{0q}\end{bmatrix}+\begin{bmatrix}\theta_{11}\\ \vdots\\ \theta_{1q}\end{bmatrix}\begin{bmatrix}\theta_{11}\\ \vdots\\ \theta_{1q}\end{bmatrix}^{\prime}.\]

He mentions that this can be generalized to

\[\Sigma=\begin{bmatrix}\theta_{01}&&\\ &\ddots&\\ &&\theta_{0q}\end{bmatrix}+\sum_{k=1}^{s}\begin{bmatrix}\theta_{k1}\\ \vdots\\ \theta_{kq}\end{bmatrix}\begin{bmatrix}\theta_{k1}\\ \vdots\\ \theta_{kq}\end{bmatrix}^{\prime}.\]This covariance model includes \(q+qs-s(s-1)/2\) free parameters.

If the functional data involve function domains with more than one dimension, the situation is analogous to fitting a spatial model to each individual. For example, if \(t\in{\bf R}^{2}\) so that \(t=(t_{1},t_{2})^{\prime}\), one can use two sets of functions \(\phi_{k}\) and \(\psi_{k}\) and model

\[\sigma(t,\tilde{t})\doteq\sum_{k=1}^{s}\theta_{k}\phi_{k}(t_{1})\psi_{k}(t_{2}) \phi_{k}(\tilde{t}_{1})\psi_{k}(\tilde{t}_{2}).\]

or

\[\sigma(t,\tilde{t})\doteq\sum_{k=1}^{s_{1}}\sum_{k^{\prime}=1}^{s_{2}}\theta_{ kk^{\prime}}\phi_{k}(t_{1})\psi_{k^{\prime}}(t_{2})\phi_{k}(\tilde{t}_{1}) \psi_{k^{\prime}}(\tilde{t}_{2}).\]

### 11.7 Generalized Split Plot Models

Christensen (1987) introduced generalized split plot (GSP) models as a broad family of linear models that display the salient characteristics of split plot models. For example, least squares estimates are optimal, exact \(F\) tests are available for whole plot effects where they are tested against a whole-plot error term, and subplot effects (including subplot treatment by whole-plot treatment interactions) have exact \(F\) tests based on a subplot error term.

In this section we relate generalized multivariate linear models (GMLMs) to GSP models following, with modifications, the notation of \(PA\) Chap. 11. Let \(y_{i}=(y_{i1},\ldots,y_{im})^{\prime}\) be the vector of observations associated with whole-plot \(i\). A fundamental assumption of split plot models is that

\[{\rm Cov}(y_{i})=\sigma_{s}^{2}I_{m}+\sigma_{w}^{2}J_{m}^{m}\]

where \(\sigma_{s}^{2}\) is the subplot variance and \(\sigma_{w}^{2}\) is the whole-plot variance. Different \(y_{i}\)s are assumed to be uncorrelated and for testing they are collectively assumed to be multivariate normal.

We denote the number of whole plots by \(\tilde{n}\) and, with \(m\) subplots in each whole plot, the total number of observations is \(n=\tilde{n}m\). GSP models do not require any form of balance among the whole-plots, but there is little hope of developing a simple whole-plot analysis without the assumption that the subplots are balanced. The whole plot analysis is based on averaging over the observations in the subplots, and if those averages are not balanced, the whole-plot means are no longer comparable to one another. Christensen (2015, Subsection 19.2.1) exploits results in Christensen (1984) to provide an analysis of split plot data with missing subplots but this is done at the expense of abandoning the whole-plot analysis.

In addition to subplot balance, GSP models assume a particular structure to the model matrix. Let \(\mathcal{Y}\) be the \(n\) vector of all observations. Write the GSP model as

\[\mathcal{Y}=\mathcal{X}_{*}\delta+\mathcal{X}_{2}\gamma+{\bf e},\]\[\mathrm{E}(\mathbf{e})=0,\quad\mathrm{Cov}(\mathbf{e})=\sigma_{s}^{2}I_{n}+\sigma_{w} ^{2}\mathcal{X}_{1}\mathcal{X}_{1}^{\prime}=\mathrm{Cov}(\mathcal{Y})\]

where \(\mathcal{X}_{1}\) is a matrix whose columns are indicator variables for the whole plots. Note that the ppo onto \(C(\mathcal{X}_{1})\), is

\[\mathcal{M}_{1}\equiv\frac{1}{m}\mathcal{X}_{1}\mathcal{X}_{1}^{\prime}.\]

The conditions required of GSP models are that

\[C\left(\mathcal{X}_{*}\right)\subset C\left(\mathcal{X}_{1}\right)\quad\text{ and}\quad C\left(\mathcal{X}_{*},\mathcal{X}_{2}\right)=C\left[\mathcal{X}_{*},\left(I-\mathcal{M}_{1}\right)\mathcal{X}_{2}\right].\]

To get this second condition satisfied, it is enough to show that

\[C\left(\mathcal{M}_{1}\mathcal{X}_{2}\right)\subset C\left(\mathcal{X}_{*} \right).\]

The ppo onto \(C\left(\mathcal{X}_{*},\mathcal{X}_{2}\right)\) is \(\mathcal{M}_{*}+\mathcal{M}_{2}\) where \(\mathcal{M}_{*}\) is the ppo onto \(C\left(\mathcal{X}_{*}\right)\) and \(\mathcal{M}_{2}\) is the ppo onto \(C\left[\left(I-\mathcal{M}_{1}\right)\mathcal{X}_{2}\right]\) and the two ppos are orthogonal. The sum of squares for whole plot error is \(\mathcal{Y}^{\prime}\left(\mathcal{M}_{1}-\mathcal{M}_{*}\right)\mathcal{Y}\) and the sum of squares for subplot error is \(\mathcal{Y}^{\prime}\left(I-\mathcal{M}_{1}-\mathcal{M}_{2}\right)\mathcal{Y}\).

#### GMLMs Are GSP Models

We now demonstrate how to associate the GMLM matrix \(\left[Z\otimes X\right]\) from (11.1.2) with the model matrix for a GSP. In this association, \(q\equiv m\), that is, the number of dependent variables in the GMLM is precisely the number of subplot units. While \(n=\tilde{n}m\) in a GSP is the total number of observations, the number of rows in \(Y\) and \(X\) of the GMLM is \(\tilde{n}\). (In all other discussions we have called this number \(n\).) The matrix \(X\) determines the model for the whole-plot analysis, e.g., in a split plot experimental design, \(X\) is determined by the whole-plot design. We assume that we can partition \(Z\) as

\[Z=[J_{m},Z_{2}].\]

Let \(\mathcal{Y}\equiv\mathrm{Vec}(Y)\) with

\[\mathcal{X}_{1}=[J_{m}\otimes I_{\tilde{n}}],\quad\mathcal{X}_{*}=[J_{m} \otimes X],\quad\mathcal{X}_{2}=[Z_{2}\otimes X].\]

This makes the first column of \(\Gamma\) into \(\delta\), the whole plot effect vector. Clearly,

\[C(\mathcal{X}_{*},\mathcal{X}_{2})=C([J_{m}\otimes X],[Z_{2}\otimes X])=C([Z \otimes X]).\]

Defining \(M_{2}\) as the ppo onto \(C\left[\left(I_{m}-\frac{1}{m}J_{m}^{m}\right)Z_{2}\right]\),we get

\[\mathcal{M}_{1}=\left[\frac{1}{m}J_{m}^{m}\otimes I_{\tilde{n}}\right],\qquad \mathcal{M}_{*}=\left[\frac{1}{m}J_{m}^{m}\otimes M\right],\qquad\mathcal{M}_ {2}=[M_{2}\otimes M]\]and also, by direct computation,

\[\mathcal{M}_{*}+\mathcal{M}_{2}=[M_{Z}\otimes M]\,.\]

To prove that model (11.1.2) satisfies the model matrix conditions for a GSP model, we need to show that \(C\left(\mathcal{X}_{*},\mathcal{X}_{2}\right)=C\left[\mathcal{X}_{*},(I- \mathcal{M}_{1})\,\mathcal{X}_{2}\right]\), which is equivalent to showing that \(C(\mathcal{M}_{1}\mathcal{X}_{2})\subset C(\mathcal{X}_{*})\). In the GMLM,

\[\mathcal{M}_{1}\mathcal{X}_{2}=\left[\frac{1}{m}J_{m}^{m}\otimes I_{\bar{n}} \right][Z_{2}\otimes X]=\left[\frac{1}{m}J_{m}^{m}Z_{2}\otimes X\right].\]

It follows that

\[C(\mathcal{M}_{1}\mathcal{X}_{2})=C\left(\left[\frac{1}{m}J_{m}^{m}Z_{2} \otimes X\right]\right)\subset C\left([J_{m}\otimes X]\right)=C(\mathcal{X}_{ *}).\]

GMLMs are not as flexible as GSP models. For example, the standard split plot model involves whole-plot blocks, whole-plot treatments, subplot treatments, and whole-plot treatment by subplot treatment interaction. A GMLM that contained all of those terms would also have a whole-plot blocks by subplot treatment interaction.

### 11.8 Additional Exercises

##### Exercise 11.8.1

Box (1950) gives data on the weights of three groups of rats. One group was given thyroxin in their drinking water, one thiouracil, and the third group was a control. Weights are measured in grams at weekly intervals. The data were given in Table 10.5. Analyze the data using the analysis of covariance method for growth curve analysis.

##### Exercise 11.8.2

Box (1950) presents data on the weight loss of a fabric due to abrasion. Two fillers were used in three proportions. Some of the fabric was given a surface treatment. Weight loss was recorded after 1000, 2000, and 3000 revolutions of a machine designed to test abrasion resistance. The data are given in Table 11.1. Perform a multivariate analysis of variance, a profile analysis, and fit a simple linear growth curve model. Check your assumptions.

[MISSING_PAGE_EMPTY:9775]

* Rao (1965) Rao, C. R. (1965). The theory of least squares when the parameters are stochastic and its application to the analysis of growth curves. _Biometrika, 52_, 447-458.
* II_. New York: Academic Press.
* Rao (1967) Rao, C. R. (1967). Least squares theory using an estimated dispersion matrix and its application to measurement signals. In _Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability_ (vol. 1, pp. 355-372).
* Wolfinger (1996) Wolfinger, R. D. (1996). Heterogeneous variance-covariance structures for repeated measures. _Journal of Agricultural, Biological, and Environmental Statistics, 1_, 205-230.

## Chapter 12 Discrimination and Allocation

**Abstract** This chapter discusses discrimination and allocation. Regression data are commonly the result of sampling a population, taking two or more measurements on each individual sampled, and then examining how those variables relate to one another. Discrimination problems have a very different sampling scheme. In discrimination problems data are obtained from multiple groups and we seek efficient means of telling the groups apart, i.e., discriminating between them. Discrimination is closely related to one-way multivariate analysis of variance in that we seek to tell groups apart. One-way multivariate analysis of variance addresses the question of whether the groups are different whereas discriminant analysis seeks to specify how the groups are different. Allocation is the problem of assigning new individuals to their appropriate group. Allocation procedures have immediate application to diagnosing medical conditions.

Consider the eight populations of people determined by all combinations of sex (male, female) and age (adult, adolescent, child, infant). These are commonly used distinctions, but the populations are not clearly defined. It is not obvious when infants become children, when children become adolescents, nor when adolescents become adults. On the other hand, most people can clearly be identified as members of one of these eight groups. It might be of interest to see whether one can _discriminate_ among these populations on the basis of, say, various aspects of their blood chemistry. The discrimination problem is sometimes referred to as the problem of _separation_. Another potentially interesting problem is trying to predict the population of a new individual given only the information on their blood chemistry. The problem of predicting the population of a new case is referred to as the problem of _allocation_. Other names for this problem are _identification_ and _classification_.

Example 12.0.1. : Aitchison and Dunsmore (1975) present data on Cushing's syndrome, a medical condition characterized by overproduction of cortisol by the adrenal cortex. Twenty-one individuals were identified as belonging to one of three types: _adenona_, _bilateral hyperplasia_, and _carcinoma_. The amounts of tetrahydro-cortisone and pregnanetriol excreted in the urine were measured. The data are givenin Table 1. We wish to discriminate among the three types of Cushing's syndrome based on the urinary excretion data. A quick glance at Table 1.1 establishes that none of the data groups seems to be from a bivariate normal distribution. The pregnanetriol value for case 4 looks out of place because it is the only value that is nonzero in the hundredths place but, from looking at similar data, it is apparently a valid observation. Following Aitchison and Dunsmore (1975), the analysis is performed on the logarithms of the data. The log data are plotted in Fig. 1.1. Ripley (1996, Section 2.4) discusses these data in the context of predictive allocation. 

Most books on multivariate analysis contain extensive discussions of discrimination and allocation. The author can particularly recommend the treatments in Anderson (2003), Johnson and Wichern (2007), and Seber (1984). In addition, Hand (1981) and Lachenbruch (1975) have written monographs on the subject. As is so often the case in statistics, the first modern treatment of these problems was by Sir Ronald A. Fisher; see Fisher (1936, 1938). The discussion in this chapter is closely related to methods associated with the multivariate normal distribution. One alternative approach is based on logistic regression; see also Christensen (1997), Press (1982), Press and Wilson (1978), or Seber (1984). There are also a variety of nonparametric methods available. More recently methods such as support vector machines and other "machine learning" tools have been applied to this problem, cf. Hastie, Tibshirani, and Friedman (2016). Logistic regression and discrimination, other similar generalized linear models, and support vector machines are examined in the next chapter.

Discrimination seems to be a purely descriptive endeavor. The observations are vectors in \(\mathbf{R}^{q}\). All observations come from known populations. Discriminate analysis uses the observations to partition \(\mathbf{R}^{q}\) into regions, each uniquely associated with a particular population. Given a partition, it is easy to allocate future observations. An observation \(y\) is allocated to population \(r\) if \(y\) falls into the region of \(\mathbf{R}^{q}\) associated with the \(r\)th population. The difficulty lies in developing a rational approach to partitioning \(\mathbf{R}^{q}\).

\begin{table}
\begin{tabular}{|c c c c|c c c c|} \hline Case & Type & TETRA & PREG & Case & Type & TETRA & PREG \\ \hline
1 & A & 3.1 & 11.70 & 12 & B & 15.4 & 3.60 \\
2 & A & 3.0 & 1.30 & 13 & B & 7.7 & 1.60 \\
3 & A & 1.9 & 0.10 & 14 & B & 6.5 & 0.40 \\
4 & A & 3.8 & 0.04 & 15 & B & 5.7 & 0.40 \\
5 & A & 4.1 & 1.10 & 16 & B & 13.6 & 1.60 \\
6 & A & 1.9 & 0.40 & 17 & C & 10.2 & 6.40 \\
7 & B & 8.3 & 1.00 & 18 & C & 9.2 & 7.90 \\
8 & B & 3.8 & 0.20 & 19 & C & 9.6 & 3.10 \\
9 & B & 3.9 & 0.60 & 20 & C & 53.8 & 2.50 \\
10 & B & 7.8 & 1.20 & 21 & C & 15.8 & 7.60 \\
11 & B & 9.1 & 0.60 & & & & \\ \hline \end{tabular}
\end{table}
Table 1.1: Cushingâ€™s syndrome data Just as a solution to the discrimination problem implicitly determines an allocation rule, a solution to the allocation problem implicitly solves the discrimination problem. The set of all \(y\) values to be allocated to population \(r\) determines the region associated with population \(r\).

Our discussion will be centered on the allocation problem. We present allocation rules based on Mahalanobis distance, maximum likelihood, and Bayes theorem. An advantage of the Mahalanobis distance method is that it is based solely on the means and covariances of the population distributions. The other methods require knowledge of the entire distribution in the form of a density. Not surprisingly, the Mahalanobis, the maximum likelihood, and the Bayes rules are similar for normally distributed populations.

In general, consider the situation in which there are \(t\) populations and \(q\) variables \(y_{1},\ldots,y_{q}\) with which to discriminate among them. In particular, if \(y=(y_{1},\ldots,y_{q})^{\prime}\) is an observation from the \(i\)th population, we assume that either the mean and covariance matrix of the population are known, say

\[\mathrm{E}(y)=\mu_{i}\]

and

\[\mathrm{Cov}(y)=\Sigma_{i},\]

Figure 12: Cushingâ€™s syndrome data

or that the density of the population distribution, say

\[f(y|i),\]

is known. In practice, neither the density, the mean, nor the covariance matrix will be known for any population. These must be estimated using data from the various populations.

An important special case is where the covariance matrix is the same for all populations, say

\[\Sigma\equiv\Sigma_{1}=\cdots=\Sigma_{t}\,.\]

In this case, samples from the populations constitute data for a standard one-way MANOVA.

Section 12.1 deals with the general allocation problem. Section 12.2 examines quadratic discriminant analysis (QDA). It applies the general ideas by estimating parameters and densities. Section 12.3 examines linear discriminant analysis (LDA). It is the special case of equal covariance matrices. Section 12.4 introduces ideas of cross-validation for estimating error rates. Section 12.5 contains some general discussion. Sections 12.6 and 12.7 examine the relationship between MANOVA and LDA and in particular discuss a method for selecting variables in LDA and introduce discrimination coordinates that are useful in visualizing the discrimination procedure. Finally, Sect. 12.8 discusses a broader idea of linear discrimination that includes both LDA and QDA and relates it to the ideas explored in the next chapter.

In recent years it has become quite common to refer to binary regression problems as classification problems. In olden times, classification was used as a synonym for what we are doing here. Discrimination tries to tell two or more groups apart. Binary regression also tries to tell two groups apart and generalizations of it seek to tell more than two groups apart. The big difference between binary regression and discrimination is in the nature of the data. In binary regression you sample a population and try to predict the subgroup in which a new observation will fall. In discrimination you separately sample from the subgroups but still try to predict where a new observation from the population will fall. The key difference is that with discrimination data the researcher determines the sample size from each group, whereas in binary regression the number observed from each group is itself a binomial random variable determined by the overall sample size and the population probability for that group. With discrimination data, allocating a new observation to a group requires knowledge from outside the data collection about the population probabilities of the groups.

It is tempting to use binary regression methods to analyze discrimination data but such methods always need to be adjusted for the group probabilities. Regression methods will incorrectly treat the group sample size proportions as estimates of the group probabilities, when in fact the group sample size proportions are determined by the researcher. In fact a frequent reason for collecting discrimination data is to obtain more information on groups that occur infrequently in the general population, e.g., to better diagnose rare diseases. Binary generalized linear models and related regression/discrimination methods are examined in the next chapter.

### The General Allocation Problem

In this section, we discuss allocation rules based on Mahalanobis distance, maximum likelihood, and Bayes theorem. These rules are based on populations with either known means and covariances or known densities.

#### Mahalanobis Distance

As discussed in _PA-V_ Sect. 12.1 (Christensen 2011, Section 13.1), the Mahalanobis distance

\[D^{2}=(y-\mu)^{\prime}\Sigma^{-1}(y-\mu)\]

is a frequently used measure of how far a random vector is from the center of its distribution. In the allocation problem, we have a random vector \(y\) and \(t\) possible distributions from which it could arise. A reasonable allocation procedure is to assign \(y\) to the population that minimizes the observed Mahalanobis distance. In other words, allocate \(y\) to population \(r\) if

\[(y-\mu_{r})^{\prime}\Sigma_{r}^{-1}(y-\mu_{r})=\min_{i}(y-\mu_{i})^{\prime} \Sigma_{i}^{-1}(y-\mu_{i})\,. \tag{12.1.1}\]

#### Maximum Likelihood

If the densities \(f(y|i)\) are known for each population, the population index \(i\) is the only unknown parameter. Given an observation \(y\), the likelihood function is

\[L(i)\equiv f(y|i),\]

which is defined for \(i=1,\ldots,t\). The maximum likelihood allocation rule assigns \(y\) to population \(r\) if

\[L(r)=\max_{i}L(i),\]

or equivalently if

\[f(y|r)=\max_{i}f(y|i).\]

If the observations have a multivariate normal distribution, the maximum likelihood rule is very similar to the Mahalanobis distance rule. From _PA_ Sect. 1.3, the likelihoods (densities) are

\[L(i)=f(y|i)=(2\pi)^{-q/2}|\Sigma_{i}|^{-1/2}\exp[-(y-\mu_{i})^{\prime}\Sigma_{i }^{-1}(y-\mu_{i})/2]\,,\]

\(i=1,\ldots,t\), where \(|\Sigma_{i}|\) is the determinant of the covariance matrix. The logarithm is a monotone increasing function, so maximizing the log-likelihood is equivalent to maximizing the likelihood. The log-likelihood is\[\ell(i)\equiv\log[L(i)]=-\frac{q}{2}\log(2\pi)-\frac{1}{2}\log(|\Sigma_{i}|)-\frac {1}{2}(y-\mu_{i})^{\prime}\Sigma_{i}^{-1}(y-\mu_{i})\,.\]

If we drop the constant term \(-\frac{q}{2}\log(2\pi)\) and minimize twice the negative of the log-likelihood rather than maximizing the log-likelihood, we see that the maximum likelihood rule for normally distributed populations is: assign \(y\) to population \(r\) if

\[\log(|\Sigma_{r}|)+(y-\mu_{r})^{\prime}\Sigma_{r}^{-1}(y-\mu_{r})=\min_{i}\left\{ \log(|\Sigma_{i}|)+(y-\mu_{i})^{\prime}\Sigma_{i}^{-1}(y-\mu_{i})\right\}\,. \tag{12.1.2}\]

The only difference between the maximum likelihood rule and the Mahalanobis rule is the inclusion of the term \(\log(|\Sigma_{i}|)\) which does not depend on \(y\), the case to be allocated. Both the Mahalanobis rule and the maximum likelihood rule involve quadratic functions of \(y\). Methods related to these rules are often referred to as _quadratic discrimination_ methods. (As discussed in Chap. 1, quadratic functions of the predictor variables define linear functions of an expanded set of predictor variables. More on this in the chapter's last section and in Exercise 13.4.)

Example 12.1.1. The simplest case of allocation is assigning a new observation \(y\) to one of two normal populations with the same variance. The top panel of Fig. 12.2 contains normal densities with variance 1. The one on the left has mean 2; the one on the right has mean 5. The solid dot is the point at which the two densities are equal. For \(y\) to the right of the dot, the maximum likelihood allocation is to the mean 5 population. To the left of the dot, the maximum likelihood allocation is to the the mean 2 population. The Mahalanobis distance in this one dimensional problem is \(|y-\mu_{i}|/1\). The black dot is also the solution to \(y-2=y-5\), so \(y\) values to the left of the dot are closer to the mean 2 population and those to the right are closer to the mean 5 population.

The bottom panel of Fig. 12.2 is far more complicated because it involves unequal variances. The population on the left is \(N(2,1)\) whereas the population on the right is now \(N(5,9)\). The two squares are where the densities from the two distributions are equal. To the left of the left square and to the right of the right square, the \(N(5,9)\) has a higher density, so a \(y\) in those regions would be assigned to that population. Between the two squares, the \(N(2,1)\) has a higher density, so a \(y\) between the squares is assigned to the mean 2 population. The squared Mahalanobis distances are \((y-2)^{2}/1\) and \((y-5)^{2}/9\). Setting these equal gives the two black dots. Again, \(y\) is assigned to the mean 2 population if and only if \(y\) is between the black dots. The normal theory maximum likelihood and Mahalanobis methods are similar but distinct. 

#### Bayesian Methods

We will discuss two procedures for Bayesian allocation. One is an intuitive rule. The other is a formal procedure based on costs of misclassification. It will also be shown that the intuitive rule can be arrived at by the formal procedure.

Bayesian allocation methods presuppose that for each population \(i\) there exists a prior probability, say \(\pi(i)\), that the new observation \(y\) comes from that population. Typically, these prior probabilities are arrived at either from previous knowledge of the problem or through the use of the maximum entropy principle (see Berger 1985, Section 3.4). The maximum entropy principle dictates that the \(\pi(i)\)s should be chosen to minimize the amount of information contained in them. This is achieved by selecting

\[\pi(i)=1/t\,, \tag{12.1.3}\]

\(i=1,\ldots,t\).

Given the prior probabilities and the data \(y\), the posterior probability that \(y\) came from population \(i\) can be computed using Bayes theorem (see Berger 1985, Section 4.2 or Christensen, Johnson, Branscum, & Hanson 2010, Chapter 2). The posterior probability is

\[\pi(i|y)=f(y|i)\pi(i)\bigg{/}\sum_{j=1}^{t}f(y|j)\pi(j)\,. \tag{12.1.4}\]

A simple intuitive allocation rule is to assign \(y\) to the population with the highest posterior probability. In other words, assign \(y\) to population \(r\) if

\[\pi(r|y)=\max_{i}\pi(i|y)\,.\]

The denominator in (12.1.4) does not depend on \(i\), so the allocation rule is equivalent to choosing \(r\) such that

Figure 12.2: One dimensional normal discrimination

\[f(y|r)\pi(r)=\max_{i}\{f(y|i)\pi(i)\}.\]

In the important special case in which the \(\pi(i)\) are all equal, this corresponds to maximizing \(f(y|i)\); that is, choosing \(r\) so that

\[f(y|r)=\max_{i}f(y|i)\,.\]

Thus, for equal initial probabilities, the intuitive Bayes allocation rule is the same as the maximum likelihood allocation rule. In particular, if the populations are normal, the Bayes rule with equal prior probabilities is based on (12.1.2).

Most methods of discrimination are based on estimating the density functions \(f(y|i)\). These include LDA, QDA, and such nonparametric methods as nearest neighbors and kernels (in the sense of Sect. 1.7.2). As discussed in the next chapter, logistic regression and some other binary regression methods give direct estimates of \(\pi(i|y)\), but those estimates are based on having implicitly estimated \(\pi(i)\) with \(N_{i}/\sum_{j}N_{j}\), where the \(N_{j}\)s are the numbers of observations in each group. This is rarely appropriate for discrimination data. Appropriate discrimination procedures must correct for this.

To develop a formal Bayesian analysis requires knowledge of the costs of correct classification (allocation) and of misclassification. Let \(c(j|i)\) be the cost of classifying \(y\) into population \(j\) when \(y\) is in fact from population \(i\). Note that \(c(i|i)\) is the cost of correct classification. For any \(j\neq i\), \(c(j|i)\) is a cost for misclassification. Typically, we would take \(c(i|i)\leq c(j|i)\) for all \(j\).

The expected cost of classifying \(y\) into population \(i\) (given the data) is simply

\[C(i|y)=\sum_{j=1}^{t}c(i|j)\pi(j|y)\,.\]

The formal Bayes allocation rule is to assign \(y\) to population \(r\) if

\[C(r|y)=\min_{i}C(i|y)\,.\]

Now, consider the special case where

\[c(i|i)=0\]

and

\[c(j|i)=c\qquad\mbox{for}\qquad j\neq i\,.\]

This cost structure leads to the intuitive Bayes rule discussed earlier. Clearly, the formal Bayes rule is equivalent to choosing \(r\) such that

\[c-C(r|y)=\max_{i}\{c-C(i|y)\}\,.\]

Note two things. First, by the definition of probability,\[\sum_{j=1}^{t}\pi(j|y)=1\,.\]

Second, from the assumed cost structure,

\[C(i|y)=c\sum_{j\neq i}\pi(j|y)\,.\]

Using these two facts,

\[c-C(i|y) = c\sum_{j=1}^{t}\pi(j|y)-c\sum_{j\neq i}\pi(j|y)\] \[= c\ \pi(i|y)\,.\]

Thus, the formal Bayes rule allocates \(y\) to population \(r\) if

\[c\,\pi(r|y)=\max_{i}c\,\pi(i|y),\]

or equivalently if

\[\pi(r|y)=\max_{i}\pi(i|y)\,.\]

This is precisely the intuitive rule given earlier.

Combining our Bayesian results we see that assuming equal prior probabilities as in (12.1.3) and the classification cost structure of (12.1.5) and (12.1.6), the formal Bayes rule is identical to the maximum likelihood rule. In particular, for normally distributed populations, the Bayes rule is given by (12.1.2).

### 12.2 Estimated Allocation and QDA

One serious problem with the allocation rules of the previous section is that typically the moments and the densities are unknown. In (12.1.1) and (12.1.2) typically the values \(\mu_{i}\) and \(\Sigma_{i}\) are unknown. In practice, allocation is often based on estimated means and covariances or estimated densities.

We assume that a random sample of \(N_{i}\equiv N(i)\) observations is available from the \(i\)th population. The \(j\)th observation from the \(i\)th population is denoted \(y_{ij}\equiv(y_{ij,1},\ldots,y_{ij,q})^{\prime}\). Note that

\[\mathrm{E}(y_{ij})=\mu_{i}\]

and

\[\mathrm{Cov}(y_{ij})=\Sigma_{i}\,.\]

It is important to recognize that in the special case of equal covariance matrices, the data follow a multivariate one-way ANOVA model with \(t\) groups. It will be convenient to write a matrix that contains the \(i\)th sample,\[Y_{i}\equiv\begin{bmatrix}y_{i1}^{\prime}\\ \vdots\\ y_{iN(i)}^{\prime}\end{bmatrix}.\]

The estimated Mahalanobis distance rule is that an observation \(y\) is allocated to population \(r\) if

\[(y-\bar{y}_{r}.)^{\prime}S_{r}^{-1}(y-\bar{y}_{r}.)=\min_{i}(y-\bar{y}_{i}.)^{ \prime}S_{i}^{-1}(y-\bar{y}_{i}.)\,\]

where

\[S_{i}=\sum_{j=1}^{N_{i}}(y_{ij}-\bar{y}_{i}.)(y_{ij}-\bar{y}_{i}.)^{\prime} \bigg{/}(N_{i}-1)=Y_{i}^{\prime}\left[I-\frac{1}{N_{i}}J_{N(i)}^{N(i)}\right]Y _{i}\bigg{/}(N_{i}-1)\,.\]

An estimated maximum likelihood allocation rule is to assign \(y\) to population \(r\) if

\[\hat{f}(y|r)=\max_{i}\hat{f}(y|i).\]

If \(q\) is not too large, the estimate \(\hat{f}(y|i)\) can be estimated nonparametrically using nearest neighbors or kernels (similar to those used in Sect. 1.7.3) or, if \(\hat{f}(y|i)\) depends on parameters \(\theta_{i},\hat{f}(y|i)\) can be obtained by estimating the parameters, which is what we do for multivariate normals.

For multivariate normal densities, an estimated maximum likelihood allocation rule is to assign \(y\) to population \(r\) if

\[\log(|S_{r}|)+(y-\bar{y}_{r}.)^{\prime}S_{r}^{-1}(y-\bar{y}_{r}.)=\min_{i}\{ \log(|S_{i}|)+(y-\bar{y}_{i}.)^{\prime}S_{i}^{-1}(y-\bar{y}_{i}.)\}\,.\]

Application of this estimated normal theory allocation rule is often referred to as _quadratic discriminant analysis (QDA)_.

Although the QDA allocation decision is based a quadratic function of the predictor variables in \(y\), the quadratic function in \(y\) can also be written as a linear combination of the \(y\) predictor variables, their squares, and their crossproducts. In one sense, QDA is not linear discrimination but in another sense it is. (In terms of the "training" data \(y_{ij}\), it is neither quadratic nor linear but rather a very complicated function.) This issue is discussed further in Sect. 12.8.

With estimated parameters, the Bayes allocation rule is really only a quasi-Bayesian rule. The allocation is Bayesian, but the estimation of \(f(y|i)\) is not. Geisser's (1971) suggestion of using the Bayesian predictive distribution as an estimate of \(f(y|i)\) has been shown to be optimal under frequentist criteria by Aitchison (1975), Murray (1977), and Levy and Perng (1986). It also provides the optimal Bayesian allocation. In particular, for normal data, treating the maximum likelihood estimates as the mean and covariance matrix of the normal gives an inferior estimate for the distribution of new observations. The appropriate distribution (for "noninformative" priors) is a multivariate \(t\) with the same location vector and a covariance matrix that is a multiple of the MLE. See Geisser (1977) for a discussion of these issues in relation to discrimination.

In any case, plugging in estimates of \(\mu_{i}\) and \(\Sigma_{i}\) requires that good estimates be available. Friedman (1989) has proposed an alternative estimation technique for use with small samples.

Finally, in examining the data, it is generally not enough to look just at the results of the allocation. Typically, one is interested not only in the population to which a case is allocated but also in the clarity of the allocation. It is desirable to know whether a case \(y\) is clearly in one population or whether it could have come from two or more populations. The posterior probabilities from the Bayesian method address these questions in the simplest fashion. Similar information can be gathered from examining the entire likelihood function or the entire set of Mahalanobis distances.

We now illustrate QDA on the Cushing's syndrome data. Clarity of allocation is addressed later in Example 12.5.1. An alternative analysis of these data based on assuming equal covariance matrices for the groups is presented in the next section.

**Example 12.2.1**.: _Cushing' Syndrome: Quadratic Discrimination Analysis._

Performing a QDA on the Cushing's syndrome data of Example 12.0.1 begins with estimating the means and covariance matrices for the three populations. These are as follows.

\begin{tabular}{c c c c c} \hline Variable & \multicolumn{3}{c|}{Group means} & Grand \\  & \(a\) & \(b\) & \(c\) & mean \\ \hline log(Tet) & 1.0433 & 2.0073 & 2.7097 & 1.8991 \\ log(Preg) & \(-\)0.60342 & \(-\)0.20604 & 1.5998 & 0.11038 \\ \hline \end{tabular}

\begin{tabular}{c c c} Covariance & matrix for adenoma \\  & log(Tet) & log(Preg) \\ log(Tet) & 0.1107 & 0.1239 \\ log(Preg) & 0.1239 & 4.0891 \\ \end{tabular}

\begin{tabular}{c c c} Covariance & matrix for bilateral hyperplasia \\  & log(Tet) & log(Preg) \\ log(Tet) & 0.2119 & 0.3241 \\ log(Preg) & 0.3241 & 0.7203 \\ \end{tabular}

\begin{tabular}{c c c} Covariance & matrix for carcinoma \\  & log(Tet) & log(Preg) \\ log(Tet) & 0.5552 & \(-\)0.2422 \\ log(Preg) & \(-\)0.2422 & 0.2885 \\ \end{tabular}

The results of QDA are given in Table 12. Group probabilities are computed in two ways. First, the data are used to estimate the parameters of the normal densities and the estimated densities are plugged into Bayes theorem. Second, the probabilities are estimated from cross-validation, the details of which will be discussed in Sect. 12.4. The analyses are based on _equal costs and prior probabilities_; hence, they are also maximum likelihood allocations. Using the resubstitution method, only two cases are misallocated: 9 and 12. Under cross-validation, cases 1, 8, 19, and 20 are also misclassified. 

### Linear Discrimination Analysis: LDA

If the populations are all multivariate normal with the same covariance matrix, say \(\Sigma\equiv\Sigma_{1}=\cdots=\Sigma_{t}\), then the Mahalanobis distance rule (12.1.1) and the maximum-likelihood/Bayes allocation rule (12.1.2) are identical. The maximum-likelihood/Bayes allocation rule assigns \(y\) to the population \(r\) that satisfies

\[\log(|\Sigma|)+(y-\mu_{r})^{\prime}\Sigma^{-1}(y-\mu_{r})=\min_{i}\left\{\log( |\Sigma|)+(y-\mu_{i})^{\prime}\Sigma^{-1}(y-\mu_{i})\right\}.\]

\begin{table}
\begin{tabular}{|c c|c c c|c c c|} \hline  & & \multicolumn{3}{c|}{Resubstitution} & \multicolumn{3}{c|}{Cross-validation} \\ Allocated & \multicolumn{3}{c|}{True group} & \multicolumn{3}{c|}{True group} \\ to group & \(a\) & \(b\) & \(c\) & \(a\) & \(b\) & \(c\) \\ \hline \(a\) & 6 & 1 & 0 & 5 & 2 & 0 \\ \(b\) & 0 & 8 & 0 & 0 & 7 & 2 \\ \(c\) & 0 & 1 & 5 & 1 & 1 & 3 \\ \hline \end{tabular}
\end{table}
Table 2: Quadratic discrimination analysis However, the term \(\log(|\Sigma|)\) is the same for all populations, so the rule is equivalent to choosing the population \(r\) that satisfies

\[(y-\mu_{r})^{\prime}\Sigma^{-1}(y-\mu_{r})=\min_{i}(y-\mu_{i})^{\prime}\Sigma^{- 1}(y-\mu_{i})\,.\]

This is precisely the Mahalanobis distance rule.

In practice, estimates must be substituted for \(\Sigma\) and the \(\mu_{i}\)s. With equal covariance matrices the data fit the multivariate one-way ANOVA model of Sect. 10.3, so the standard estimates \(\bar{y}_{i\cdot}\), \(i=1,\ldots,t\) and \(S=E/(n-t)\) are reasonable. The allocation rule is: assign \(y\) to population \(r\) if

\[(y-\bar{y}_{r\cdot})^{\prime}S^{-1}(y-\bar{y}_{r\cdot})=\min_{i}(y-\bar{y}_{i \cdot})^{\prime}S^{-1}(y-\bar{y}_{i\cdot})\,.\]

Recall that in a one-way ANOVA, the estimated covariance matrix is a weighted average of the individual estimates, namely

\[S=\sum_{i=1}^{t}(N_{i}-1)S_{i}/(n-t)\,.\]

Although \((y-\bar{y}_{i\cdot})^{\prime}S^{-1}(y-\bar{y}_{i\cdot})\) is a quadratic function of \(y\), the allocation only depends on a linear function of \(y\). Note that

\[(y-\bar{y}_{i\cdot})^{\prime}S^{-1}(y-\bar{y}_{i\cdot})=y^{\prime}S^{-1}y-2 \bar{y}_{i\cdot}^{\prime}S^{-1}y+\bar{y}_{i\cdot}^{\prime}S^{-1}\bar{y}_{i \cdot}\,.\]

The term \(y^{\prime}S^{-1}y\) is the same for all populations. Subtracting this constant and dividing by \(-2\), the allocation rule can be rewritten as: assign \(y\) to population \(r\) if

\[y^{\prime}S^{-1}\bar{y}_{r\cdot}-\frac{1}{2}\bar{y}_{r\cdot}^{\prime}S^{-1} \bar{y}_{r\cdot}=\max_{i}\left\{y^{\prime}S^{-1}\bar{y}_{i\cdot}-\frac{1}{2} \bar{y}_{i\cdot}S^{-1}\bar{y}_{i\cdot}\right\}\,.\]

This is based on a linear function of \(y\), so methods related to this allocation rule are often referred to as (traditional) _linear discriminant analysis (LDA)_.

Example 12.3.1. _Cushing's Syndrome data._

LDA results from estimating normal densities with equal covariance matrices. In this example we also _assume equal prior probabilities and costs_ so the Bayesian allocation corresponds to a maximum likelihood allocation. All results are based on pooling the covariance matrices from Example 12.2.1 into

\[\begin{array}{ll}&\mbox{Pooled Covariance Matrix}\\ &\mbox{log(Tet) log(Preg)}\\ \mbox{log(Tet) }&0.2601&0.1427\\ \mbox{log(Preg) }&0.1427&1.5601.\end{array}\]The results of the linear discriminant analysis are summarized in Table 2.3. Based on resubstitution, four cases are misallocated, all from group \(b\). Based on leave-one-out cross-validation, three additional cases are misallocated. Cases 8 and 9 are consistently classified as belonging to group \(a\), and cases 12 and 16 are classified as \(c\). In addition, when they are left out of the fitting process, cases 1 and 4 are allocated to groups \(c\) and \(b\), respectively, while case 19 is misallocated as \(b\). It is interesting to note that linear discrimination has a hard time deciding whether case 19 belongs to group \(b\) or \(c\). A more detailed discussion of these cases is given later.

Consider \(t=2\) populations. If \(q=2\) we can easily plot the data from each population as we did in Fig. 1. Linear discrimination seeks to find a line that best separates the two clouds of data points (as is illustrated in the next chapter). If \(q=3\), while it is harder to do, we can still plot the data from each population. Linear discrimination now seeks to find a plane in three dimensions that best separates the two clouds of data points. For \(q>3\), visualization becomes problematic but linear discrimination uses hyperplanes, i.e., translated (shifted) vector spaces (affine spaces) of dimension \(q-1\), to separate the populations. In \(q\) dimensions, quadratic discrimination uses quadratic, rather than linear functions to separate the data. (But quadratics are linear functions in higher dimensional spaces.)

In particular, if there are just two groups, it is easy to see that the traditional linear discrimination rule assigns \(y\) to group 1 if

\[y^{\prime}S^{-1}\bar{y}_{1\cdot}-\frac{1}{2}\bar{y}_{1\cdot}^{\prime}S^{-1} \bar{y}_{1\cdot}>y^{\prime}S^{-1}\bar{y}_{2\cdot}-\frac{1}{2}\bar{y}_{2\cdot} ^{\prime}S^{-1}\bar{y}_{2\cdot},\]

which occurs if and only if

\[y^{\prime}S^{-1}(\bar{y}_{1\cdot}-\bar{y}_{2\cdot})>\frac{1}{2}\bar{y}_{1\cdot }^{\prime}S^{-1}\bar{y}_{1\cdot}-\frac{1}{2}\bar{y}_{2\cdot}^{\prime}S^{-1} \bar{y}_{2\cdot},\]

iff

\[y^{\prime}S^{-1}(\bar{y}_{1\cdot}-\bar{y}_{2\cdot})>\frac{1}{2}(\bar{y}_{1\cdot }+\bar{y}_{2\cdot})^{\prime}S^{-1}(\bar{y}_{1\cdot}-\bar{y}_{2\cdot}),\]

iff, with \(\hat{\mu}\equiv(\bar{y}_{1\cdot}+\bar{y}_{2\cdot})/2\),

\[(y-\hat{\mu})^{\prime}S^{-1}(\bar{y}_{1\cdot}-\bar{y}_{2\cdot})>0. \tag{12.3.1}\]

The separating line (or plane or hyperplane) in a plot consists of the \(y\) values with \((y-\hat{\mu})^{\prime}S^{-1}(\bar{y}_{1\cdot}-\bar{y}_{2\cdot})=0\) or \(y^{\prime}S^{-1}(\bar{y}_{1\cdot}-\bar{y}_{2\cdot})=\hat{\mu}^{\prime}S^{-1}( \bar{y}_{1\cdot}-\bar{y}_{2\cdot})\).

As discussed in the next chapter, any linear function of \(y\) can be used to form a two group discrimination rule, say, assign \(y\) to a group based on whether \(y^{\prime}\beta_{*}>\beta_{0}\). The trick is to find a good vector \(\beta_{*}\) and a good constant \(\beta_{0}\). LDA uses obviously good estimates of what are the optimal choices for \(\beta_{*}\) and \(\beta_{0}\) when the data are multivariate normal with equal covariance matrices.

### 12.4 Cross-Validation

It is of considerable interest to be able to evaluate the performance of allocation rules. Depending on the populations involved, there is generally some level of misclassification that is unavoidable. (If the populations are easy to tell apart, why are you worrying about them?) If the distributions that determine the allocation rules are known, one can simply classify random samples from the various populations to see how often the data are misclassified. This provides simple yet valid estimates of the error rates. Unfortunately, things are rarely that straightforward. In practice, the data available are used to estimate the distributions of the populations. If the same data are also used to estimate the error rates, a bias is introduced. Typically, this double dipping in the data overestimates the performance of the allocation rules. The method of estimating error rates by reclassifying the data used to construct the classification rules is often called the _resolution method_.

\begin{table}
\begin{tabular}{|c c|c c c|c c c|} \hline  & & \multicolumn{3}{c|}{Resubstitution} & \multicolumn{3}{c|}{Cross-validation} \\ Allocated & \multicolumn{3}{c|}{True group} & \multicolumn{3}{c|}{True group} \\ to group & \(a\) & \(b\) & \(c\) & \(a\) & \(b\) & \(c\) \\ \hline \(a\) & 6 & 2 & 0 & 4 & 2 & 0 \\ \(b\) & 0 & 6 & 0 & 1 & 6 & 1 \\ \(c\) & 0 & 2 & 5 & 1 & 2 & 4 \\ \hline \end{tabular}
\end{table}
Table 12.3: Linear discriminationTo avoid the bias of the resubstitution method, _cross-validation_ is often used, cf. Geisser (1977) and Lachenbruch (1975). Cross-validation often involves leaving out one data point, estimating the allocation rule from the remaining data, and then classifying the deleted case using the estimated rule. Every data point is left out in turn. Error rates are estimated by the proportions of misclassified cases. This version of cross-validation is also known as the _jackknife_. (The jackknife was originally a tool for reducing bias in location estimates.) The computation of the cross-validation error rates can be simplified by the use of updating formulae similar to those discussed in _PA-V_ Sect. 12.5 (Christensen 2011, Section 13.5).

While resubstitution underestimates error rates, cross-validation may tend to overestimate them. In standard linear models, if one thinks of the variance \(\sigma^{2}\) as the error rate, resubstitution is analogous to estimating the variance with the _naive estimate_\(SSE/n\) (also the normal theory MLE) whereas leave-one-out cross-validation is analogous to estimating the variance with \(\text{PRESS}/n\) where PRESS is the _predicted residual sum of squares_ discussed in _PA-V_ Sect. 12.5 (Christensen 2011, Section 13.5). Using Jensen's inequality it is not too difficult to show that

\[\text{E}\left[\frac{SSE}{n}\right]<\text{E}\left[\frac{SSE}{n-r(X)}\right]= \sigma^{2}\leq\text{E}\left[\frac{PRESS}{n}\right],\]

and, indeed, that on average the cross-validation estimate \(\text{PRESS}/n\) over estimates \(\sigma^{2}\) by at least as much as the naive (resubstitution) method underestimates \(\sigma^{2}\). While this is not really an issue in linear models, because we know how to find an unbiased estimate of the error in linear models, this result calls in question the idea of blindly using leave-one-out cross-validation to estimate error rates in allocation problems and logistic regression. In fact, since the over-estimation and the underestimation seem to have similar orders of magnitude in standard linear models, one might consider averaging the two estimates.

For large data sets, \(K\) group cross-validation seems to be more popular than leave-one-out cross-validation. This involves (1) randomly dividing the data into \(K\) groups and (2) fitting the model on \(K-1\) of those groups. Evaluate the error by (3) using this fitted model to allocate the data for the one omitted group, and (4) comparing these allocations to the true group memberships for the omitted group. This is done \(K\) times, where each group is omitted one time. The overall estimates of error are averages from the \(K\) different estimates. This approach requires quite a bit of data to be effective. Leave-one-out cross-validation uses \(K=n\) but it seems popular to pick \(K\) considerably smaller than \(n\) and, indeed, this seems likely to reduce the bias problem of the previous paragraph.

When I first wrote this book (and contrary to the previous discussion), leave-one-out cross-validation was considered to have less bias than the resubstitution method but typically a considerably larger variance, cf. more recently (Hastie et al. 2016, Section 7.10). Hastie et al. also suggest that smaller values of \(K\) like \(K=5\) should have less variability but possibly more bias.

If the number of observations is much larger than the number of parameters to be estimated, resubstitution is often adequate for estimating the error rates. When the number of parameters is large relative to the number of observations, the bias becomes unacceptably high. Under normal theory, the parameters involved are simply the means and covariances for the populations. Thus, the key issue is the number of variables used in the discrimination relative to the number of observations. (While we have not discussed nonparametric discrimination, in the context of this discussion nonparametric methods should be considered as highly parametric methods.)

The bootstrap has also been suggested as a tool for estimating error rates. It often has both small bias and small variance, but it is computationally intensive and handles large biases poorly. The interested reader is referred to Efron (1983) and the report of the Panel on Discriminant Analysis, Classification, and Clustering in _Statistical Science_ (1989).

### 12.5 Discussion

Example 12.5.1. _Cushing's Syndrome Data._

Careful inspection of Table 12.1 and Fig. 12.1 sheds light on both the LDA and QDA procedures. From Fig. 12.1, there seems to be almost no evidence that the covariance matrices are equal. Adenoma displays large variability in log(pregnantriol), very small variability in log(tetrahydrocortisone), and almost no correlation between the variables. Carcinoma is almost the opposite. It has large variability in log(Tet) and small variability in log(Preg). Carcinoma seems to have a negative correlation. Bilateral hyperplasia displays substantial variability in both variables, with a positive correlation. These conclusions are also visible from the estimated covariance matrices. Given that the covariance structure seems to differ from group to group, linear discrimination does surprisingly well when evaluated by resubstitution. Recall that linear discriminant analysis has been found to be rather robust. Of course, quadratic discrimination does a much better job for these data.

The fact that the assessments based on cross-validation are much worse than those based on resubstitution is due largely to the existence of influential observations. The mean of group \(c\) and especially the covariance structure of group \(c\) are dominated by the large value of log(Tet) for case 20. Case 20 is not misclassified by the LDA because its effect on the covariance structure is minimized by the pooling of covariance estimates over groups. In cross-validated QDA, its effect on the covariance of group \(c\) is eliminated, so case 20 seems more consistent with group \(b\). The large log(Preg) value of case 1 is also highly influential. With case 1 dropped out and case 20 included, case 1 is more consistent with carcinoma than with adenoma. The reason that cases 8 and 9 are misclassified is simply that they tend to be consistent with group \(a\), see Fig. 12.1. In examining Table 12.1, a certain symmetry can be seen involving cases 12 and 19. Because of case 19, when case 12 is unassigned it looks more like group \(c\) than its original group. Similarly, because of case 12, when case 19 is unassigned it looks more like group \(b\) than group \(c\) under quadratic discrimination. Case 19 is essentially a toss-up under LDA. Cases 4 and 16 are misclassified under LDA because they involve very unusual data. Case 4 has an extremely small pregnanetriol value, and case 16 has a very large tetrahydrocortisone value for being part of the bilateral hyperplasia group.

In a data set this small, it seems unreasonable to drop influential observations. If we cannot believe the data, there is little hope of being able to arrive at a reasonable analysis. If further data bear out the covariance tendencies visible in Fig. 12.1, the better analysis is provided by quadratic discrimination. It must be acknowledged that the error rates obtained by resubstitution are unreliable. They are generally biased toward underestimating the true error rates and may be particularly bad for these data. QDA simply provides a good description of the data. There is probably insufficient data to produce really good predictions. 

The methods discussed explicitly in this chapter are all related to the normal distribution. If the true distributions \(f(y|i)\) are elliptically symmetric, both the quadratic and linear methods work well. Moreover, the linear discrimination method is generally quite robust; it even seems to work quite well for discrete data. See Lachenbruch, Sneeringeer, and Revo (1973), Lachenbruch (1975), and Hand (1983) for details.

The gold standard for discrimination seems to be, depending on one's philosophical bent, maximum likelihood or Bayesian discrimination. But they are only the gold standard if you know what the distributions are. If you know the densities, those are the only functions of the data that need concern you.

Linear and quadratic discrimination for nonnormal data can be based on Mahalanobis distances rather than on densities. Since they are not based on densities, they are ad hoc methods. Many of the binary regression methods discussed in the next chapter provide direct estimates of \(\pi(i|y)\) that are (typically) inappropriate for discrimination data but from which appropriate density estimates can be inferred. Often the regression methods implicitly or explicitly perform discrimination in higher dimensions. Instead of linear or quadratic discrimination on the basis of, say, \(y=(y_{1},y_{2},y_{3})^{\prime}\), they discriminate on the basis of some extended vector, for example, \(\tilde{y}=(y_{1},y_{2},y_{3},y_{1}^{2},y_{2}^{2},y_{3}^{2},y_{1}y_{2},y_{1}y_{ 3},y_{2}y_{3})^{\prime}\). _If you know the densities, there is little point in expanding the dimensionality, because the density is the only relevant function of the data._ But if you do not know the densities, expanding the dimensionality can be very useful. In particular, support vector machines typically use expanded data. Of course, one could also perform traditional linear or quadratic discrimination on the new \(\tilde{y}\) and I suspect that, when practical, linear and quadratic discrimination on \(\tilde{y}\) will often be competitive with the newer methods. Personally, I am more comfortable using expanded data in logistic (or log-linear) discrimination than in LDA or QDA.

For \(\tilde{y}\) as given above, any linear discrimination method based on \(\tilde{y}^{\prime}\beta_{\rm s}\) is equivalent to a quadratic discrimination based on \(y\). This is not to say that LDA applied to \(\tilde{y}\) is QDA, but merely that \(\tilde{y}^{\prime}\beta_{\rm s}\) is always a quadratic function of \(y\). If you know that the data \(y\) are normal, QDA on \(y\) is pretty nearly optimal. (For known mean vectors and covariance matrices it is optimal.) And if the data are normal with equal covariance matrices, those optimal quadratic discriminate functions reduce to linear functions of \(y\). But if \(y\) is normal, \(\tilde{y}\) is certainly _not_ normal and applying traditional LDA methods to \(\tilde{y}\) is unlikely to agree with QDA. Nonetheless, LDA on \(\tilde{y}\) is some form of quadratic discrimination.

### Stepwise LDA

One interesting problem in linear allocation is the choice of variables. Including variables that have no ability to discriminate among populations can only muddy the issues involved. By analogy with multiple regression, one might expect to find advantages to allocation procedures based solely on variables with high discriminatory power. In multiple regression, methods for eliminating independent variables are either directly based on, or closely related to, testing whether exclusion of the variables hurts the regression model. In other words, a test is performed of whether, given the included variables, the excluded variables contain any additional information for prediction. In discrimination and allocation, methods for eliminating discriminatory variables such as _stepwise discrimination_ are based on testing whether, given the included variables, the excluded variables contain any additional information for discrimination among the populations. We have noted that LDA is closely related to the multivariate one-way ANOVA model. Tests of additional information can be performed as in Sect. 10.5. In particular, they are typically performed by testing a one-way ACOVA model such as (10.5.1) against the no treatment effects ACOVA model (10.5.3).

Example 12.2.2.: We now illustrate the process of stepwise discrimination using data given by Lubischew (1962). He considered the problem of discriminating among three populations of flea-beetles within the genus _Chaetocnema_. Six variables were given: \(y_{1}\), the width, in microns, of the first joint of the first tarsus, \(y_{2}\), the same measurement for the second joint, \(y_{3}\), the maximum width, in microns, of the aedeagus in the fore part, \(y_{4}\), the front angle, in units of \(7.5^{\circ}\), of the aedeagus, \(y_{5}\), the maximum width of the head, in \(0.01\,\mathrm{mm}\) units, between the external edges of the eyes, and \(y_{6}\), the width of the aedeagus from the side, in microns. In addition, Lubischew mentions that \(r_{12}\equiv y_{1}/y_{2}\) is very good for discriminating between one of the species and the other two. The vector of dependent variables is taken as \(y^{\prime}=(y_{1},y_{2},y_{3},y_{4},y_{5},y_{6},r_{12})\). Stepwise discrimination is carried out by testing for additional information in the one-way MANOVA.

Evaluating the assumptions of a one-way MANOVA with three groups and seven dependent variables is a daunting task. There are three \(7\times 7\) covariance matrices that should be roughly similar. To wit, there are \({7\choose 2}=21\) bivariate scatter plots to check for elliptical patterns. If the capability exists for the user, there are \({7\choose 3}=35\) three-dimensional plots to check. There are \(3(7)=21\) normal plots to evaluate the marginal distributions and at least some linear combinations of the variables should be evaluated for normality. Of course, if \(y_{1}\) and \(y_{2}\) are multivariate normal, the constructed variable \(r_{12}\) cannot be. However, it may be close enough for our purposes.

If the assumptions break down, it is difficult to know how to proceed. After any transformation, everything needs to be reevaluated, with no guarantee that things will have improved. It seems like the best bet for a transformation is some model-based system similar to the Box and Cox (1964) method (see Andrews, Gnanadesikan, Warner 1971).

For the most part, in this example, we will cross our fingers and hope for the best. In other words, we will rely on the robustness of the procedure. While it is certainly true that the \(P\) values used in stepwise discriminant analysis should typically not be taken at face value (this is true for almost any statistical modeling technique), the \(P\) values can be viewed as simply a one-to-one transformation of the test statistics. Thus, decisions based on \(P\) values are based on the relative sizes of comparable test statistics. The test statistics are reasonable even without the assumption of multivariate normality so, from this point of view, multivariate normality is not a crucial issue.

The assumption of equal covariance matrices is a stickier issue. From _PA_ Sect. 3.2, univariate test statistics are based on the squared length of the vector of differences between the optimal fitted values under the full model and the optimal fitted values under the reduced model. If the reduced model is (nearly) correct, the difference vector should be near zero (but subject to random variation that affects its length). For the multivariate linear model, the hypothesis statistic \(H\) consists of the squared length for each variable and inner products between distinct variables. Small matrices \(H\) (if they are not _too_ small) are consistent with the reduced model, while large matrices \(H\) are inconsistent with it. Even with unequal covariance matrices for the groups, the least squares estimates are reasonable, so valid conclusions can be based on the size of \(H\). In fact, with unequal covariance matrices, least squares estimates are still optimal for the full one-way MANOVA model. To see this, write the model as a univariate linear model and, as in Sect. 9.2, use Theorem 10.4.5 from _PA_. The problem with unequal covariance matrices is twofold. First, it is difficult to evaluate explicitly what we mean by large and small matrices \(H\). Second, the inner products (and thus the lengths) are being evaluated with the Euclidean inner product, which is less than optimal.

Although the properties of formal tests can be greatly affected by the invalidity of the MANOVA assumptions, crude but valid evaluations can still be made based on the test statistics. This is often the most that we have any right to expect from multivariate procedures. For univariate models, Scheffe (1959, Chapter 10) gives an excellent discussion of the effects of invalid assumptions on formal tests.

The three species of flea-beetles considered will be referred to as simply A, B, and C and indexed as 1, 2, and 3, respectively. There are 21 observations on species A with

\[\vec{y}_{1}^{\prime}.=(183.1,129.6,51.2,146.2,14.1,104.9,1.41)\]

and

\[S_{1}=\left[\begin{array}{ccccccc}147.5&66.64&18.53&15.08&-5.21&14.21&0.406 \\ 66.64&51.25&11.55&2.48&-1.81&3.09&-0.044\\ 18.53&11.55&4.99&5.85&-0.524&5.49&0.017\\ 15.08&2.48&5.85&31.66&-0.969&15.63&0.090\\ -5.21&-1.81&-0.524&-0.969&0.791&-1.99&-0.021\\ 14.21&3.09&5.49&15.63&-1.99&38.23&0.078\\ 0.406&-0.044&0.017&0.090&-0.021&0.078&0.0036\end{array}\right].\]Species B has 31 observations with

\[\vec{y}_{2}^{\prime}.=(201.0,119.3,48.9,124.6,14.3,81.0,1.69)\]

and

\[S_{2}=\left[\begin{array}{cccccc}222.1&63.40&22.60&30.37&4.37&29.47&0.926\\ 63.40&44.16&7.91&11.82&0.337&11.47&-0.100\\ 22.60&7.91&5.52&5.69&0.005&4.23&0.075\\ 30.37&11.82&5.69&21.37&-0.327&11.70&0.088\\ 4.37&0.337&0.005&-0.327&1.21&1.27&0.029\\ 29.47&11.47&4.23&11.70&1.27&79.73&0.085\\ 0.926&-0.100&0.075&0.088&0.029&0.085&0.009\end{array}\right].\]

For species C, there are 22 observations with

\[\vec{y}_{3}^{\prime}.=(138.2,125.1,51.6,138.3,10.1,106.6,1.11)\]

and

\[S_{3}=\left[\begin{array}{cccccc}87.33&44.55&20.53&19.17&-0.736&15.29&0.301 \\ 44.55&73.04&15.71&14.02&-0.390&21.23&-0.267\\ 20.53&15.71&8.06&8.21&-0.294&4.97&0.027\\ 19.17&14.02&8.21&2.16&-0.502&7.93&0.027\\ 0.736&-0.390&-0.294&-0.502&0.944&0.277&-0.002\\ 15.29&21.23&4.97&7.93&0.277&34.25&-0.061\\ 0.301&-0.267&0.027&0.027&-0.002&-0.061&0.0046\end{array}\right].\]

The pooled estimate of the covariance is a weighted average of \(S_{1}\), \(S_{2}\), and \(S_{3}\), with approximately 50% more weight on \(S_{2}\) than on the other estimates.

Although, typically, backward elimination is to be preferred to forward selection in stepwise procedures, it is illustrative to demonstrate forward selection on these data. We will begin by making a very rigorous requirement for inclusion: variables will be included if the \(P\) value for adding them is 0.01 or less.

The first step in forward selection consists of performing the univariate one-way ANOVA \(F\) tests for each variable.

The \(P\) values are all sufficiently small to warrant inclusion of the variables. By far the largest \(F\) statistic, and thus the smallest \(P\) value, is for \(r_{12}\), so this is the first variable included for use in discrimination. Note that \(r_{12}\) is the variable constructed by Lubischew.

The second and all subsequent steps of the procedure involve performing a one-way analysis of covariance for each variable not yet included. For the second step, the sole covariate is \(r_{12}\), and a test is made for treatment effects in the analysis of covariance model. For the dependent variables \(y_{1}\) through \(y_{6}\), the results are as follows.

\begin{tabular}{c c c} \hline Step 2: & Statistics for entry, \(df=2,70\) \\ Variable & \(F_{\mbox{obs}}\) & \(\Pr[F>F_{\mbox{obs}}]\) \\ \hline \(y_{1}\) & 9.904 & 0.0002 \\ \(y_{2}\) & 8.642 & 0.0004 \\ \(y_{3}\) & 6.386 & 0.0028 \\ \(y_{4}\) & 87.926 & 0.0001 \\ \(y_{5}\) & 30.549 & 0.0001 \\ \(y_{6}\) & 28.679 & 0.0001 \\ \hline \end{tabular} The largest \(F\) statistic is for \(y_{4}\), and the corresponding \(P\) value is less than 0.01, so \(y_{4}\) is included for discrimination.

At the third step, both \(r_{12}\) and \(y_{4}\) are used as covariates in a one-way analysis of covariance. Again, the \(F\) tests for treatment differences are performed.

\begin{tabular}{c c c} \hline Step 3: & Statistics for entry, \(df=2,69\) \\ Variable & \(F_{\mbox{obs}}\) & \(\Pr[F>F_{\mbox{obs}}]\) \\ \hline \(y_{1}\) & 2.773 & 0.0694 \\ \(y_{2}\) & 3.281 & 0.0436 \\ \(y_{3}\) & 6.962 & 0.0018 \\ \(y_{5}\) & 24.779 & 0.0001 \\ \(y_{6}\) & 3.340 & 0.0412 \\ \hline \end{tabular} Variable \(y_{5}\) is included for discrimination. Note the large difference between the \(F\) statistic for \(y_{5}\) and that for the other variables. There is an order-of-magnitude difference between the abilities of the \(r_{12}\), \(y_{4}\), and \(y_{5}\) to discriminate and the abilities of the other variables. Considering the questionable validity of formal tests, this is an important point. It should also be mentioned that this conclusion is based on one sequence of models. There is a possibility that other sequences would lead to different conclusions about the relative importance of the variables. In fact, it would be desirable to check all models or, better yet, have an algorithm to identify the best models.

Step 4 simply adds weight to our conclusions of the previous paragraph. In performing the analysis of covariance with three covariates, none of the variables considered have the very large \(F\) statistics seen earlier.

\begin{tabular}{c c c} \hline Step 4: & Statistics for entry, \(df=2,68\) \\ Variable & \(F_{\mbox{obs}}\) & \(\Pr[F>F_{\mbox{obs}}]\) \\ \hline \(y_{1}\) & 1.985 & 0.1453 \\ \(y_{2}\) & 2.567 & 0.0842 \\ \(y_{3}\) & 3.455 & 0.0372 \\ \(y_{6}\) & 3.359 & 0.0406 \\ \hline \end{tabular}

Any rule that terminates forward selection when all \(P\) values exceed 0.0371 will stop the selection process at Step 4. In particular, our stringent stopping rule based on \(P\) values of 0.01 terminates here.

In practice, it is much more common to use a stopping rule based on \(P\) values of 0.05, 0.10, or 0.15. By any of these rules, we would add variable \(y_{3}\) and continue checking variables. This leads to Step 5 and the corresponding \(F\) statistics.

\begin{tabular}{c c c} \hline Step 5: & Statistics for entry, \(df=2,67\) \\ Variable & \(F_{\mbox{obs}}\) & \(\Pr[F>F_{\mbox{obs}}]\) \\ \hline \(y_{1}\) & 7.040 & 0.0017 \\ \(y_{2}\) & 8.836 & 0.0004 \\ \(y_{6}\) & 3.392 & 0.0395 \\ \hline \end{tabular}

Surprisingly, adding \(y_{3}\) has changed things dramatically. While the \(F\) statistic for \(y_{6}\) is essentially unchanged, the \(F\) values for \(y_{1}\) and \(y_{2}\) have more than tripled. Of course, we are still not seeing the huge \(F\) statistics that were encountered earlier, but apparently one can discriminate much better with \(y_{3}\) and either \(y_{1}\) or \(y_{2}\) than would be expected from the performance of any of these variables individually. This is precisely the sort of thing that is very easily missed by forward selection procedures and one of the main reasons why they are considered to be poor methods for model selection. Forward selection does have advantages. In particular, it is cheap and it is able to accommodate huge numbers of variables.

The stepwise procedure finishes off with two final steps. Variable \(y_{2}\) was added in the previous step. The results from Step 6 are as follows.

\begin{tabular}{c c c} \hline Step 6: & Statistics for entry, \(df=2,66\) \\ Variable & \(F_{\mbox{obs}}\) & \(\Pr[F>F_{\mbox{obs}}]\) \\ \hline \(y_{1}\) & 0.827 & 0.4418 \\ \(y_{6}\) & 3.758 & 0.0285 \\ \hline \end{tabular}

Variable \(y_{6}\) is added if our stopping rule is not extremely stringent. This leaves just \(y_{1}\) to be evaluated.

\begin{tabular}{c c c} \hline Step 7: & Statistics for entry, \(df=2,65\) \\ Variable & \(F_{\mbox{obs}}\) & \(\Pr[F>F_{\mbox{obs}}]\) \\ \hline \(y_{1}\) & 0.907 & 0.4088 \\ \hline \end{tabular}

By any standard \(y_{1}\) would not be included. Of course, \(r_{12}\) is the ratio of \(y_{1}\) and \(y_{2}\), so it is not surprising that there is no need for all three variables. A forward selection procedure that does not include \(r_{12}\) would simply include all of the variables.

We have learned that \(r_{12}\), by itself, is a powerful discriminator. The variables \(r_{12}\), \(y_{4}\), and \(y_{5}\), when taken together, have major discriminatory powers. Variable \(y_{3}\), taken together with either \(y_{1}\) or \(y_{2}\) and the previous three variables, may provide substantial help in discrimination.

Finally, \(y_{6}\) may also contribute to distinguishing among the populations. Most of these conclusions are visible from Table 12.4 that summarizes the results of the forward selection.

It is also of interest to see the results of a multivariate analysis of variance for all of the variables included at each step. For example, after Step 3, variables \(r_{12}\), \(y_{4}\), and \(y_{5}\) were included for discrimination. The likelihood ratio test statistic for no group effects in the one-way MANOVA is \(U=0.0152\). This is a very small, hence very significant, number. Table 12.5 lists the results of such tests for each step in the process. Based on their \(P\) values, all of the variables added had substantial discriminatory power. Thus, it is not surprising that the \(U\) statistics in Table 12.5 decrease as each variable is added.

In practice, decisions about the practical discriminatory power of variables should not rest solely on the \(P\) values. After all, the \(P\) values are often unreliable. Other methods, such as the graphical methods presented in the next section, should be used in determining the practical usefulness of results based on multivariate normal distribution theory.

\begin{table}
\begin{tabular}{|c c c c|} \hline  & Variable & LRTS & \\ Step & entered & \(U_{\mbox{obs}}\) & \(\Pr[U<U_{\mbox{obs}}]\) \\ \hline
1 & \(r_{12}\) & 0.09178070 & 0.0001 \\
2 & \(y_{4}\) & 0.02613227 & 0.0001 \\
3 & \(y_{5}\) & 0.01520881 & 0.0001 \\
4 & \(y_{3}\) & 0.01380601 & 0.0001 \\
5 & \(y_{2}\) & 0.01092445 & 0.0001 \\
6 & \(y_{6}\) & 0.00980745 & 0.0001 \\ \hline \end{tabular}
\end{table}
Table 12.5: Forward stepwise discrimination: MANOVA tests

\begin{table}
\begin{tabular}{|c c c c|} \hline  & Variable & & & \\ Step & entered & \(F_{\mbox{obs}}\) & \(\Pr[F>F_{\mbox{obs}}]\) \\ \hline
1 & \(r_{12}\) & 351.292 & 0.0001 \\
2 & \(y_{4}\) & 87.926 & 0.0001 \\
3 & \(y_{5}\) & 24.779 & 0.0001 \\
4 & \(y_{3}\) & 3.455 & 0.0372 \\
5 & \(y_{2}\) & 8.836 & 0.0004 \\
6 & \(y_{6}\) & 3.758 & 0.0285 \\ \hline \end{tabular}
\end{table}
Table 4: Summary of forward selection 

### 12.7 Linear Discrimination Coordinates

As mentioned earlier, one is typically interested in the clarity of classification. This can be investigated by examining the posterior probabilities, the entire likelihood function, or the entire set of Mahalanobis distances. It is done by computing the allocation measures for each element of the data set. The allocation measure can be estimated either by the entire data set or the data set having deleted the case currently being allocated. To many people, the second, cross-validatory, approach is more appealing.

An alternative approach to examining the clarity of discrimination is through the use of _linear discrimination coordinates_. This approach derives from the work of Fisher (1938) and Rao (1948, 1952). It consists of redefining the coordinate system in \({\bf R}^{q}\) in such a way that the different treatment groups in the one-way ANOVA have, in some sense, maximum separation in each coordinate. The clarity of discrimination can then be examined visually by inspecting one-, two-, or three-dimensional plots of the data. In these plots, cases are identified by their populations. If the new coordinate system is effective, observations from the same population should be clustered together and distinct populations should be well-separated.

It is standard practice to redefine the coordinate system by taking linear combinations of the original variables. It is also standard practice to define the new coordinate system sequentially. In particular, the first coordinate is chosen to maximize the separation between the groups. The second coordinate maximizes the separation between the groups given that the second linear combination is uncorrelated with the first. The third maximizes the separation given that the linear combination is uncorrelated with the first two. Subsequent coordinates are defined similarly. In the following discussion, we assume a constant covariance matrix for the \(t\) groups. It remains to define what precisely is meant by "maximum separation of the groups."

Recall that with equal covariance matrices, the data available in a discriminant analysis fit a multivariate one-way ANOVA,

\[Y=XB+e\,.\]

Thus,

\[E=\sum_{i=1}^{t}\,\sum_{j=1}^{N_{i}}(y_{ij}-\bar{y}_{i}.)(y_{ij}-\bar{y}_{i}.) ^{\prime}\]

and

\[H=\sum_{i=1}^{t}N_{i}(\bar{y}_{i}.-\bar{y}_{..})(\bar{y}_{i}.-\bar{y}_{..})^{ \prime}\,.\]

Also, define

\[H_{*}=\sum_{i=1}^{t}(\bar{y}_{i}.-\bar{y}_{..})(\bar{y}_{i}.-\bar{y}_{..})^{ \prime}\,.\]The linear discrimination coordinates are based on \(E\) and either \(H\) or \(H_{*}\). We will examine the use of \(H\) in detail. Some comments will also be made on the motivation for using \(H_{*}\).

For any vector \(y=(y_{1},\ldots,y_{q})^{\prime}\), the first linear discrimination coordinate is defined by

\[y^{\prime}a_{1},\]

where the vector \(a_{1}\) is chosen so that the univariate one-way ANOVA model

\[(Ya_{1})=X(Ba_{1})+(ea_{1})\]

has the largest possible \(F\) statistic for testing equality of group effects. Intuitively, the linear combination of the variables that maximizes the \(F\) statistic must have the greatest separation between groups. The degrees of freedom are not affected by the choice of \(a_{1}\), so we need to find \(a_{1}\) that maximizes

\[\frac{(Ya_{1})^{\prime}\left(M-\frac{1}{n}J_{n}^{n}\right)(Ya_{1})}{(Ya_{1})^{ \prime}(I-M)(Ya_{1})},\]

or equivalently

\[\frac{a_{1}^{\prime}Ha_{1}}{a_{1}^{\prime}Ea_{1}}\,.\]

A one-dimensional plot of the \(n\) elements of \(Ya_{1}\) shows the maximum separation between groups that can be achieved in a one-dimensional plot.

The second linear discrimination coordinate is

\[y^{\prime}a_{2}\]

such that

\[\frac{a_{2}^{\prime}Ha_{2}}{a_{2}^{\prime}Ea_{2}}\]

is maximized subject to the constraint that, for any \(i\) and \(j\), the estimated covariance between \(y_{ij}^{\prime}a_{1}\) and \(y_{ij}^{\prime}a_{2}\) is zero. The covariance condition can be rewritten as

\[a_{1}^{\prime}Sa_{2}=0,\]

or equivalently as

\[a_{1}^{\prime}Ea_{2}=0\,.\]

Another way of thinking of this condition is that \(a_{1}\) and \(a_{2}\) are orthogonal in the inner product space defined using the matrix \(E\).

A one-dimensional plot of \(Ya_{2}\) illustrates visually the separation in the groups. Even more productively, the \(n\) ordered pairs that are the rows of \(Y(a_{1},a_{2})\) can be plotted to illustrate the discrimination achieved by the first two linear discrimination coordinates.

For \(h=3,\ldots,r(H)\) the \(h\)th linear discriminant coordinate is

\[y^{\prime}a_{h},\]

where

\[a^{\prime}_{h}Ha_{h}/a^{\prime}_{h}Ea_{h}\]

is maximized subject to the covariance condition

\[a^{\prime}_{h}Ea_{i}=0\qquad i=1,2,\ldots,h-1\,.\]

Note that, using the inner product for \(\mathbf{R}^{q}\) based on \(E\), this defines an orthogonal system of coordinates (i.e., \(a_{1},\ldots,a_{q}\) define an orthogonal basis for \(\mathbf{R}^{q}\) using the inner product defined by \(E\)).

Unfortunately, the discrimination coordinates are not uniquely defined. Given a vector \(a_{h}\), any scalar multiple of \(a_{h}\) also satisfies the requirements listed earlier. One way to avoid the nonuniqueness is to impose another condition. The most commonly used extra condition is that \(a^{\prime}_{h}Ea_{h}=1\), so that \(a_{1},\ldots,a_{q}\) is an orthonormal basis for \(\mathbf{R}^{q}\) under the inner product defined by \(E\). Alas, even this does not quite solve the uniqueness problem because \(-a_{h}\) has the same properties as \(a_{h}\).

Before going into the details of actually finding the linear discrimination coordinates, we illustrate their use. It will be established later that the linear discrimination coordinate vectors \(a_{i}\), \(i=1,\ldots,q\) are eigenvectors of \(E^{-1}H\). Moreover, the appropriate metric for examining variables transformed into the linear discrimination coordinates is the standard Euclidean metric. This allows simple visual inspection of the transformed data. Writing \(A=[a_{1},\ldots,a_{q}]\), the mapping \(Y\) into \(YA\) gives the data matrix in the linear discrimination coordinates.

Consider again the heart rate data of Example 10.3.1. The data structure needed for development of linear discrimination coordinates is the same as for a one-way MANOVA. We have already examined these data for multivariate normality and equal covariance matrices. The data seem to satisfy the assumptions.

The linear discrimination coordinates are defined by a matrix of eigenvectors of \(E^{-1}H\). One is

\[A=\left[\begin{array}{rrrr}0.739&0.382&0.581&0.158\\ -0.586&-0.323&-0.741&0.543\\ -0.353&-0.234&0.792&-0.375\\ 0.627&-0.184&-0.531&-0.218\end{array}\right].\]

Recall that \(E\) and \(H\) were given in Example 10.3.1. The columns of \(A\) define four new data vectors \(Ya_{1}\), \(Ya_{2}\), \(Ya_{3}\), and \(Ya_{4}\) but remember that eigenvectors are not uniquely defined. Different software often give different eigenvectors but (when the eigenvalues are unique) they only vary by a scale factor, so the differences typically do not matter (unless you are trying to reproduce existing results). If we perform an analysis of variance on each variable, we get \(F\) statistics for discriminating between groups. All have 2 degrees of freedom in the numerator and 27 in the denominator.

As advertised, the \(F\) statistics are nonincreasing. The first two \(F\) statistics clearly establish that there are group differences in the first two coordinates. The last two \(F\) statistics are zero because with three groups there are 2 degrees of freedom for treatments and \(H\) is a \(4\times 4\) matrix of rank 2. Only two of the linear discrimination coordinates can have positive \(F\) statistics. This issue is discussed in more detail in the next subsection.

The big advantage of linear discrimination coordinates is that they allow us to plot the data in ways that let us visualize the separation in the groups. Figure 12.3 shows two plots that display the first discrimination coordinate values for each population. The software placed the populations in different positions. Note that the degree of separation is substantial and about the same for all three groups. The edges of the middle group are close to the edges of the other groups. The placebo has one observation that is consistent with drug A.

Figure 12.3: Plots of the heart rate data in the first linear discrimination coordinate

Figure 12.4 is similar to Fig. 12.3 except that it plots the data in the second discrimination coordinate. Note that in the second coordinate it is very difficult to distinguish between drugs A and B. The placebo is separated from the other groups, but there is more overlap around the edges than was present in the first coordinate.

Figure 12.5 is a scatter plot of the data in the first two discrimination coordinates. Together, the separation is much clearer than in either of the individual coordinates. There is still one observation from drug A that is difficult to distinguish from the placebo group but, other than that, the groups are very well-separated. That the one observation from drug A is similar to the placebo is a conclusion based on the Euclidean distance of the point from the centers of the groups for drug A and the placebo. It is not clear that Euclidean distances are appropriate, but that will be shown in the next subsection.

#### Finding Linear Discrimination Coordinates

The vectors \(a_{1},\ldots,a_{q}\) can be taken as eigenvectors of the matrix \(E^{-1}H\). Before showing this we prove a result similar to Theorem B.15 in _PA_. Theorem B.15 states that, given any symmetric matrix, say \(W\), there exists an orthonormal basis for \(\mathbf{R}^{n}\) consisting of eigenvectors of \(W\). Lemma 12.7.2 states that, relative to an inner product on \(\mathbf{R}^{q}\) defined by an arbitrary positive definite matrix \(E\), there exists an orthonormal basis for \(\mathbf{R}^{q}\) consisting of eigenvectors of \(E^{-1}H\), where \(H\) is an arbitrary symmetric matrix. Note that although we have continued to use the symbols \(H\) and \(E\) and our immediate interest is in application of these results to the specific matrices \(H\) and \(E\) defined earlier, the result does not depend on the choice of these matrices except as indicated in the lemma. The following series of results will also be used in Chap. 14 to derive principal components. In Chap. 14, \(E\) and \(H\) will not be the error and hypothesis matrices from a multivariate linear model.

**Lemma 12.7.2.** Let \(E\) be any \(q\times q\) positive definite matrix and let \(H\) be any symmetric \(q\times q\) matrix. Then there exists a \(q\times q\) diagonal matrix \(\Lambda\) and a matrix \(A\) such that

\[E^{-1}HA=AA\quad\text{and}\quad A^{\prime}EA=I.\]

Figure 12.5: Scatter plot of the heart rate data in the first two linear discrimination coordinatesObserve that the columns of \(A\) must be eigenvectors and the elements of \(\Lambda\) must be eigenvalues of \(E^{-1}H\).

Proof. Define \(E^{1/2}\) as in Lemma 9.6.2. By \(PA\) Theorem B.15 there exists \(B\) such that

\[E^{-1/2}HE^{-1/2}B=B\Lambda \tag{12.7.1}\]

with

\[I=BB^{\prime}=B^{\prime}B\,.\]

Let \(A=E^{-1/2}B\) then multiplying (12.7.1) on the left by \(E^{-1/2}\) gives

\[E^{-1}HA=A\Lambda\]

and

\[A^{\prime}EA=B^{\prime}E^{-1/2}EE^{-1/2}B=B^{\prime}B=I\,.\]

We will later need the following result.

**Corollary 12.7.3.**  If \(A\) is \(q\times q\), \(E^{-1}HA=A\Lambda\), and \(A^{\prime}EA=I\) then

\[E^{-1}=AA^{\prime}\,.\]

Proof. If \(A^{\prime}EA=I\) and both \(E\) and \(A\) are \(q\times q\), then \(A\) must be nonsingular. It follows that

\[I=(A^{\prime}EA)^{-1}=A^{-1}E^{-1}(A^{\prime})^{-1}\,.\]

Multiplying on the left by \(A\) and on the right by \(A^{\prime}\) gives

\[AA^{\prime}=AA^{-1}E^{-1}(A^{\prime})^{-1}A^{\prime}=E^{-1}\,.\]

The argument that the linear discrimination coordinates can be taken as eigenvectors of \(E^{-1}H\) has similarities to the proof of Lemma 12.7.2 and also to the argument in Sect. 9.6 that relates Roy's \(\phi_{\max}\) test statistic to the maximum eigenvalue of \(HE^{-1}\). Once again, the argument does not depend on the specific choices of \(H\) and \(E\).

**Proposition 12.7.4.**  Let \(E\) be any \(q\times q\) positive definite matrix and let \(H\) be any \(q\times q\) symmetric matrix. The vectors \(a_{1},\ldots,a_{q}\) satisfy

\[\frac{a_{1}^{\prime}Ha_{1}}{a_{1}^{\prime}Ea_{1}}=\sup_{a}\frac{a^{\prime}Ha}{ a^{\prime}Ea}\,,\]

and, for \(i>1\),

\[a_{i}^{\prime}Ea_{j}=0,\quad j=1,\ldots,i-1\,,\]

[MISSING_PAGE_EMPTY:9808]

if \(E^{-1}Ha=a\phi\), then \(\Lambda c=A^{-1}(A\Lambda)c=A^{-1}(E^{-1}HA)c=A^{-1}E^{-1}Ha=A^{-1}a\phi=A^{-1}Ac \phi=c\phi\). Conversely, if \(\Lambda c=c\phi\), then \(E^{-1}Ha=E^{-1}HAc=A\Lambda c=Ac\phi=a\phi\).

We begin by showing that \(a^{\prime}_{1}Ha_{1}/a^{\prime}_{1}Ea_{1}\) maximizes \(a^{\prime}Ha/a^{\prime}Ea\) if and only if \(a_{1}\) is an eigenvector of \(E^{-1}H\) corresponding to \(\phi_{1}\). A vector \(a_{1}\) maximizes \(a^{\prime}Ha/a^{\prime}Ea\) if and only if the corresponding vector \(c_{1}\) maximizes

\[\frac{c^{\prime}\Lambda c}{c^{\prime}c}=\sum_{j=1}^{q}c_{j}^{2}\phi_{j}\bigg{/} \sum_{j=1}^{q}c_{j}^{2}\,.\]

Because this is a weighted average of the \(\phi_{j}\)s and the \(\phi_{j}\)s are ordered from largest to smallest, the maximum value is \(\phi_{1}\). Given the previously demonstrated relationship between eigenvectors of \(E^{-1}H\) and \(\Lambda\), the proof is complete if we show that \(c_{1}\) maximizes \(c^{\prime}\Lambda c/c^{\prime}c\) if and only if \(c_{1}\) is an eigenvector of \(\Lambda\) corresponding to \(\phi_{1}\). If \(c_{1}\) is an eigenvector of \(\Lambda\) corresponding to \(\phi_{1}\), that is, if

\[\Lambda c_{1}=c_{1}\phi_{1}\,,\]

then

\[c^{\prime}_{1}\Lambda c_{1}/c^{\prime}_{1}c_{1}=\phi_{1},\]

so \(c_{1}\) maximizes \(c^{\prime}\Lambda c/c^{\prime}c\). Conversely, suppose \(\phi_{1}=\cdots=\phi_{s}>\phi_{s+1}\); then, to maximize \(c^{\prime}\Lambda c/c^{\prime}c\), a vector \(c=(k_{1},\ldots,k_{q})\) must have \(k_{s+1}=\cdots=k_{q}=0\). Such vectors clearly satisfy \(\Lambda c=c\phi_{1}\).

We complete the proof by induction. Suppose that for \(i=1,\ldots,h-1\)

\[\frac{a^{\prime}_{i}Ha_{i}}{a^{\prime}_{i}Ea_{i}}=\sup_{a}\bigg{\{}\frac{a^{ \prime}Ha}{a^{\prime}Ea}\bigg{|}a^{\prime}Ea_{j}=0\qquad j=1,\ldots,i-1\bigg{\}}\]

with

\[a^{\prime}_{i}Ea_{j}=0\qquad j=1,\ldots,i-1\]

if and only if \(a_{i}\) is an eigenvector of \(E^{-1}H\) corresponding to \(\phi_{i}\) with \(a^{\prime}_{i}Ha_{j}=0\), \(j=1,\ldots,i-1\). We need to prove that

\[\frac{a^{\prime}_{h}Ha_{h}}{a^{\prime}_{h}Ea_{h}}=\sup_{a}\bigg{\{}\frac{a^{ \prime}Ha}{a^{\prime}Ea}\bigg{|}a^{\prime}Ea_{j}=0\qquad j=1,\ldots,h-1\bigg{\}}\]

with

\[a^{\prime}_{h}Ea_{j}=0\qquad j=1,\ldots,h-1\]

if and only if \(a_{h}\) is an eigenvector of \(E^{-1}H\) corresponding to \(\phi_{h}\) with \(a^{\prime}_{h}Ea_{j}=0\), \(j=1,\ldots,h-1\). From the equivalence established earlier, \(a_{h}\) maximizes \(a^{\prime}Ha/a^{\prime}Ea\) subject to the conditions if and only if the corresponding vector \(c_{h}\) maximizes \(c^{\prime}\Lambda c/c^{\prime}c\) subject to the conditions \(c^{\prime}c_{j}=0\), \(j=1,\ldots,h-1\) and \(a_{h}\) is an eigenvector with \(a^{\prime}_{h}Ea_{j}=0\) if and only if \(c_{h}\) is an eigenvector with \(c^{\prime}_{h}c_{j}=0\).

Suppose \(\phi_{r}=\cdots=\phi_{h}=\cdots=\phi_{s}\) with either \(r=1\) or \(\phi_{r-1}>\phi_{r}\) and either \(s=q\) or \(\phi_{s}>\phi_{s+1}\). Write \(c=(k_{1},\ldots,k_{q})^{\prime}\) and \(c_{i}=(k_{i1},\ldots,k_{iq})^{\prime}\). Think of the element in the \(j\)th row of a vector as corresponding to \(\phi_{j}\). As additional parts of the induction hypothesis, assume that the terms in \(c_{i}\) corresponding to eigenvalues less than \(\phi_{i}\) must be zero and that if \(c\) is orthogonal to the \(c_{i}\)s the terms in \(c\) corresponding to eigenvalues greater than \(\phi_{h}\) must be zero. Specifically, for \(i=1,\ldots,h-1\) if \(\phi_{j}<\phi_{i}\), then \(k_{ij}=0\), and that if \(c^{\prime}c_{i}=0\), \(i=1,\ldots,h-1\) and \(\phi_{j}>\phi_{h}\), then \(k_{j}=0\). Note that the first of these conditions holds for \(c_{1}\) and that the second condition also holds because it does not apply to \(c_{1}\).

The second of the assumptions implies that for \(c\) orthogonal to \(c_{1},\ldots,c_{h-1}\),

\[c^{\prime}\Lambda c/c^{\prime}c=\sum_{j=r}^{q}k_{j}^{2}\phi_{j}\bigg{/}\sum_{j =r}^{q}k_{j}^{2}\,.\]

This is a weighted average of the values \(\phi_{j}\), \(j=r,\ldots,q\). The maximum value is \(\phi_{r}=\phi_{h}\). As before, \(c_{h}\) will attain the maximum if and only if \(c_{h}\) is an eigenvector. If \(c_{h}\) is an eigenvector (i.e., \(\Lambda c_{h}=c_{h}\phi_{h}\)), then the maximum is attained. Conversely, a maximum is only attained if \(k_{s+1}=\cdots=k_{q}=0\). Thus, an orthogonal maximizing vector \(c_{h}\) must have \(k_{h1}=\cdots=k_{hr-1}=k_{hs+1}=\cdots=k_{hq}=0\). Clearly, any such vector satisfies \(\Lambda c=c\phi_{h}\) and thus is an eigenvector of \(\phi_{h}\). In particular, \(c_{h}\) can be any eigenvector that satisfies \(c^{\prime}_{h}c_{j}=0\), \(j=1,\ldots,h-1\).

To complete the proof, we need to prove that our additional induction hypotheses hold for \(i=h\). We have already established that \(k_{h,s+1}=\cdots=k_{hq}=0\), which is precisely the condition that if \(\phi_{j}<\phi_{h}\), then \(k_{hj}=0\). We also need to show that if \(c^{\prime}c_{i}=0\), \(i=1,\ldots,h\), and \(\phi_{j}>\phi_{h+1}\), then \(k_{j}=0\). Equivalently, we need to show that if \(h<s\), \(k_{1}=\cdots=k_{r-1}=0\) and if \(h=s\), \(k_{1}=\cdots=k_{s}=0\). If \(h<s\), there is nothing to prove; the result follows from the induction hypothesis. For \(h=s\), the two induction hypotheses and the argument in the previous paragraph give: (1) for \(i=r,\ldots,s\), \(k_{is+1}=\cdots=k_{iq}=0\) and (2) \(k_{1}=\cdots=k_{r-1}=0\). Writing \(d_{i}=(k_{ir},\ldots,k_{is})\) and \(d=(k_{r},\ldots,k_{s})\), we see that for \(i=r,\ldots,s\), \(c^{\prime}c_{i}=0\) if and only if \(d^{\prime}d_{i}=0\). In particular, \(d_{r},d_{r+1},\ldots,d_{s}\) is an orthogonal basis for \({\bf R}^{s-r+1}\). Any other vector \(d\) that is orthogonal to \(d_{r},\ldots,d_{s}\) must be the zero vector. Thus, if \(c^{\prime}c_{i}=0\), \(i=r,\ldots,s\), then \(k_{r}=\cdots=k_{s}=0\). This is precisely what we needed to prove. \(\Box\)

#### Using Linear Discrimination Coordinates

In the previous subsection we established a practical procedure for finding linear discriminant coordinates and for transforming the original data into the discrimination coordinates. Simply find \(A\) such that

\[E^{-1}HA=A\Lambda\,,\]

where \(\Lambda=D(\phi_{1},\ldots,\phi_{q})\) and

\[A^{\prime}EA=I\,.\]This is not difficult to do using a good matrix manipulation computer program. The transformed data are

\[Z=YA\,.\]

As illustrated in Example 12.7.1, the \(n\) rows of \(Z\) can be plotted in a variety of ways to examine the efficacy of the different coordinates for discrimination. The columns of \(Z\) corresponding to the largest eigenvalues show the clearest discrimination because they maximize the one-way ANOVA \(F\) test. The estimated covariance matrix of a transformed vector \(A^{\prime}y\) is

\[\widehat{\text{Cov}}(A^{\prime}y)=A^{\prime}SA=\frac{1}{n-t}A^{\prime}EA=\frac {1}{n-t}I,\]

so the Euclidean metric is appropriate for evaluating relationships between data points. This is important in that it allows intuitive evaluation of plots.

With \(t\) groups, there are at most \(t-1\) coordinates that are valuable for discrimination. If \(t>q\), this is not of much interest, but if \(t\leq q\), this means that some coordinates have no discriminatory power. Recall that

\[H=Y^{\prime}\left(M-\frac{1}{n}J_{n}^{n}\right)Y\]

and that \(r\bigl{(}M-\frac{1}{n}J_{n}^{n}\bigr{)}=t-1\). It follows that for the \(q\times q\) matrix \(H\)

\[r(H)\leq\min(q,t-1)\,.\]

Any choice of linear discrimination coordinates corresponds to a set of eigenvectors for \(E^{-1}H\). Write \(A=(a_{1},\ldots,a_{q})\) and partition \(A\) as \(A=(A_{*},A_{0})\), where the columns of \(A_{*}\) correspond to the nonzero eigenvalues of \(E^{-1}H\) and the columns of \(A_{0}\) correspond to the eigenvalue zero. If \(a_{h}\) is a column of \(A_{0}\), then, as will be seen in later,

\[a_{h}^{\prime}Ha_{h}/a_{h}^{\prime}Ea_{h}=0\,. \tag{12.7.2}\]

In other words, if the data are transformed to the \(h\)th discrimination coordinate (i.e., \(Ya_{h}\)), then a one-way ANOVA applied to the transformed data gives an \(F\) statistic of zero for testing differences between groups. Thus, the coordinate is useless in discrimination. This result (12.7.2) is an immediate consequence of the following lemma.

**Lemma 12.7.5**: \(HA_{0}=0\)_._

Proof.  The columns of \(A_{0}\) are eigenvectors of \(E^{-1}H\) corresponding to the eigenvalue \(0\). Hence, \(E^{-1}HA_{0}=0\). Multiplying on the left by \(E\) gives the result.

#### Relationship to Mahalanobis Distance Allocation

Sometimes, the linear discrimination coordinates are used to allocate new observations. Generally, this is done when the coordinates are chosen as in Lemma 12.7.2. Suppose the first \(s\) coordinates are to be used. Let

\[A_{s}=[a_{1},\ldots,a_{s}]\,.\]

The standard allocation rule is to assign \(y\) to population \(r\) if

\[(y-\bar{y}_{r}.)^{\prime}[A_{s}A_{s}^{\prime}](y-\bar{y}_{r}.)=\min_{i}(y-\bar {y}_{i}.)^{\prime}[A_{s}A_{s}^{\prime}](y-\bar{y}_{i}.)\,. \tag{12.7.3}\]

Note that

\[(y-\bar{y}_{i}.)^{\prime}[A_{s}A_{s}^{\prime}](y-\bar{y}_{i}.)=\sum_{j=1}^{s}[( y-\bar{y}_{i}.)^{\prime}a_{j}]^{2},\]

so the allocation is based on the squared values of the first \(s\) linear discrimination coordinates of the vector \((y-\bar{y}_{i}.)\). Once again, we see that the Euclidean metric is appropriate for the transformed variables. If the coordinates are not chosen as in Lemma 12.7.2 (i.e., if the vectors \(a_{1},\ldots,a_{q}\) are not an orthonormal set in the appropriate inner product), then taking a simple sum of squares is not appropriate. Thus, the restriction on the choice of coordinates is imposed.

This allocation procedure is closely related to the Mahalanobis distance method. If \(s=q\), then \(A_{s}=A\) and by Corollary 12.7.3, \(AA^{\prime}=E^{-1}\). The allocation rule is based on the (squared) distances

\[(y-\bar{y}_{i}.)^{\prime}E^{-1}(y-\bar{y}_{i}.)\,.\]

The estimated covariance matrix is \(S^{-1}=(n-t)E^{-1}\), so these distances are simply a constant multiple of the estimated Mahalanobis distances used in Sect. 12.2. The distances differ by merely a constant multiple; therefore, the allocation rules are identical.

Recall that there are at most \(\min(q,t-1)\) useful linear discrimination coordinates. Eigenvectors of \(E^{-1}H\) that correspond to the eigenvalue zero are not useful for discrimination. It makes little sense to choose \(s\) greater than the number of nonzero eigenvalues of \(E^{-1}H\). For reasons of collinearity (see _PA-V_ Chap. 12 or Christensen 2011, Chapter 13), it might make sense to choose \(s\) less than the number of nonzero eigenvalues of \(E^{-1}H\) when some of those nonzero eigenvalues are very close to zero. In fact, choosing the number of linear discrimination coordinates is reminiscent of choosing the number of principal components to use in principal component regression. (Recall the similarities between variable selection in discrimination and variable selection in regression.)

We now show that if \(s\) is chosen to be exactly the number of nonzero eigenvalues (i.e., \(s=r(E^{-1}H)\)), then the linear discrimination coordinate allocation rule based on (12.7.3) is precisely the same as the Mahalanobis distance rule. If \(r(E^{-1}H)=q\), then \(A_{s}=A\) and we have already shown the result. We need consider only the casewhere \(r(E^{-1}H)\leq t-1<q\). Using notation from earlier in the section, \(A=[A_{*},A_{0}]\). With \(s=r(E^{-1}H)\), we have \(A_{s}=A_{*}\). Before proving the equivalence of allocation rules, we need the following lemma.

**Lemma 12.7.6**.: _If \(t-1<q\), then for any \(i=1,\ldots,t\),_

\[A_{0}^{\prime}(\bar{y}_{i\cdot}-\bar{y}_{\cdot\cdot\cdot})=0.\]

Proof.  Recall that

\[H = \sum_{j=1}^{t}N_{j}(\bar{y}_{j\cdot}-\bar{y}_{\cdot\cdot\cdot})( \bar{y}_{j\cdot}-\bar{y}_{\cdot\cdot\cdot})^{\prime}\] \[= [\sqrt{N_{1}}(\bar{y}_{1\cdot}-\bar{y}_{\cdot\cdot\cdot}),\ldots, \sqrt{N_{t}}(\bar{y}_{t\cdot}-\bar{y}_{\cdot\cdot\cdot})]\left[\begin{matrix} \sqrt{N_{1}}(\bar{y}_{1\cdot}-\bar{y}_{\cdot\cdot\cdot})^{\prime}\\ \vdots\\ \sqrt{N_{t}}(\bar{y}_{t\cdot}-\bar{y}_{\cdot\cdot\cdot})^{\prime}\end{matrix} \right].\]

By Proposition B.51 in \(PA\),

\[C(H)=C([\sqrt{N_{1}}(\bar{y}_{1\cdot}-\bar{y}_{\cdot\cdot\cdot}),\ldots,\sqrt{ N_{t}}(\bar{y}_{t\cdot}-\bar{y}_{\cdot\cdot\cdot})])\,.\]

It follows that for \(i=1,\ldots,t\),

\[\bar{y}_{i\cdot}-\bar{y}_{\cdot\cdot}\in C(H),\]

and for some vector \(d\),

\[\bar{y}_{i\cdot}-\bar{y}_{\cdot\cdot}=Hd\,.\]

By Lemma 12.7.5,

\[0=A_{0}^{\prime}Hd=A_{0}^{\prime}(\bar{y}_{i\cdot}-\bar{y}_{\cdot\cdot\cdot})\,.\]

The equivalence of the allocation rules is established in the following proposition.

**Proposition 12.7.7**.: _If \(t-1<q\), then for any \(y\),_

\[(y-\bar{y}_{i\cdot})^{\prime}E^{-1}(y-\bar{y}_{i\cdot})=(y-\bar{y}_{i\cdot})^ {\prime}[A_{*}A_{*}^{\prime}](y-\bar{y}_{i\cdot})+(y-\bar{y}_{\cdot\cdot})^{ \prime}[A_{0}A_{0}^{\prime}](y-\bar{y}_{\cdot\cdot}), \tag{12.7.4}\]

_and the Mahalanobis distance allocation rule is identical to the linear discrimination coordinate allocation rule with \(s=r(E^{-1}H)\)._

Proof.  We begin by arguing that if Eq. (12.7.4) holds, the two allocation rules are identical. As mentioned earlier, the Mahalanobis rule minimizes \((y-\bar{y}_{i\cdot})^{\prime}E^{-1}(y-\bar{y}_{i\cdot})\) with respect to \(i\). The term \((y-\bar{y}_{\cdot\cdot})^{\prime}[A_{0}A_{0}^{\prime}](y-\bar{y}_{\cdot\cdot})\), on the right of (12.7.4), does not depend on \(i\), so the Mahalanobis rule minimizes \((y-\bar{y}_{i\cdot})^{\prime}[A_{*}A_{*}^{\prime}](y-\bar{y}_{i\cdot})\) with respect to \(i\). However, this is simply the linear discrimination coordinate rule for \(s=r(E^{-1}H)\).

We now prove Eq. (12.7.4). From Corollary 12.7.3 and the partition of \(A\),

\[E^{-1}=AA^{\prime}=A_{*}A_{*}^{\prime}+A_{0}A_{0}^{\prime},\]

so

\[(y-\bar{y}_{i})^{\prime}E^{-1}(y-\bar{y}_{i}.)=(y-\bar{y}_{i}.)^{\prime}A_{*}A_ {*}^{\prime}(y-\bar{y}_{i}.)+(y-\bar{y}_{i}.)^{\prime}A_{0}A_{0}^{\prime}(y- \bar{y}_{i}.).\]

It suffices to show that

\[(y-\bar{y}_{i}.)^{\prime}A_{0}A_{0}^{\prime}(y-\bar{y}_{i}.)=(y-\bar{y}_{..})^{ \prime}A_{0}A_{0}^{\prime}(y-\bar{y}_{..})\]

or

\[A_{0}^{\prime}(y-\bar{y}_{i}.)=A_{0}^{\prime}(y-\bar{y}_{..}).\]

Clearly,

\[\begin{array}{ll}A_{0}^{\prime}(y-\bar{y}_{i}.)&=A_{0}^{\prime}(y-\bar{y}_{.. }-\bar{y}_{i}.+\bar{y}_{..})\\ &=A_{0}^{\prime}(y-\bar{y}_{..})-A_{0}^{\prime}(\bar{y}_{i}.-\bar{y}_{..}). \end{array}\]

By Lemma 12.7.6, \(A_{0}^{\prime}(\bar{y}_{i}.-\bar{y}_{..})=0\) and the proof is complete. 

#### Alternate Choice of Linear Discrimination Coordinates

Our motivation for the choice of linear discrimination coordinates has been based entirely on maximizing analysis of variance \(F\) statistics. An alternative motivation, based on population rather than sample values, leads to slightly different results. Consider a linear combination of the dependent variable vector \(y\), say \(a^{\prime}y\). It follows that \(\mbox{Var}(a^{\prime}y)=a^{\prime}\Sigma a\) and, depending on its population, \(\mbox{E}(a^{\prime}y)=a^{\prime}\mu_{i}\). Define

\[\Omega=\sum_{i=1}^{t}(\mu_{i}-\bar{\mu}.)(\mu_{i}-\bar{\mu}.)^{\prime}.\]

The value

\[\frac{a^{\prime}Qa}{a^{\prime}\Sigma a}\]

can be viewed as a measure of the variability between the population means \(a^{\prime}\mu_{i}\) relative to the variance of \(a^{\prime}y\). Choosing \(a\) to maximize this measure may be a reasonable way to choose linear discrimination coordinates. Both \(\Omega\) and \(\Sigma\) are unknown parameters and must be estimated. The covariance matrix can be estimated with \(S\), and \(\Omega\) can be estimated with \[H_{*}=\sum_{i=1}^{t}(\bar{y}_{i}-\bar{y}_{..})(\bar{y}_{i}-\bar{y}_{..})^{\prime}.\]

The sample version of \(a^{\prime}\Omega a/a^{\prime}\Sigma a\) is

\[\frac{a^{\prime}H_{*}a}{a^{\prime}Sa}.\]

The error statistic \(E\) is a constant multiple of \(S\), so it is equivalent to work with

\[\frac{a^{\prime}H_{*}a}{a^{\prime}Ea}.\]

The subsequent development of linear discrimination coordinates follows as in our discussion based on \(a^{\prime}Ha/a^{\prime}Ea\).

### 12.8 Linear Discrimination

In linear model theory we consider a vector of predictor variables \(x\) and linear models \(x^{\prime}\beta\). The key feature of a linear model is that \(x^{\prime}\beta\) is a linear function of the predictor variables, whatever the predictor variables may be. The predictor variables are _not_ restricted to be a set of measurements originally taken on a collection of observational units. Chapter 1 examined the large variety of transformations that can be applied to the original measurements that make linear models far more flexible.

In this chapter, our predictor variables have been denoted \(y\), rather than \(x\). We have examined the traditional linear and quadratic discrimination methods LDA and QDA. We have pointed out that both of these methods are linear in the sense that they involve linear combinations of predictor variables, it is just that quadratic discrimination includes squares and cross-products of the original measurements as additional predictors. _The key aspect of LDA and QDA is not that they involve linear or quadratic functions of the original measurements but that the methods assume that the original data have a multivariate normal distribution and involve estimating appropriate normal densities._ If the data really are multivariate normal, no other procedure will give much of an improvement on LDA or QDA. If the data are not multivariate normal, nor easily transformed to multivariate normal, alternative discrimination procedures should be able to improve on them.

Obviously one _could_ apply LDA to a \(y\) vector that includes not only the original measurements but also squares and cross-products (or other transformations) and LDA would probably give reasonable results, even though such a \(y\) vector could not possibly have a multivariate normal distribution. In the next chapter we focus on linear discrimination methods that do not assume multivariate normality. These methods include both logistic discrimination and support vector machines (SVMs). All such methods admit as predictor variables, transformations of the original measurements.

Example 12.8.1: Figure 12.6 contains data of a form that have often been used to sell support vector machines because neither linear nor quadratic discriminant analysis can distinguish the two populations whereas SVMs separate them easily. _Such a claim is comparing apples with oranges._ It is true that the most naive forms of LDA and QDA cannot separate them. But the most naive form of SVMs cannot separate them either. If you transform the data into polar coordinates, you get the data representation in Fig. 12.7. It is trivial to separate the data in Fig. 12.7 with a vertical line. The difference between LDA and SVMs is that they may pick different vertical lines to do the separation. And in this case, how much do you really care which vertical line you use?

Obviously, no line is going to separate the data in Fig. 12.6. To separate the groups with a line, you have to transform the data. The main difference is that computer programs for SVMs have simple transformation methods built into them by allowing the specification of an appropriate reproducing kernel. Logistic regression programs could also allow the specification of an appropriate reproducing kernel, but typically they do not. For the more traditional methods, like LDA and QDA, it seems that the transformations need to be specified ex

Figure 12.6: Doughnut data

can_ apply LDA and QDA to transformed data, it is rarely a good idea unless the transformation is designed to make the data more multivariate normal.) \(\Box\)

### 12.9 Additional Exercises

##### Exercise 12.9.1

Consider the data of Example 10.3.1. Suppose a person has heart rate measurements of \(y=(84,82,80,69)^{\prime}\).

(a) Using normal theory linear discrimination, what is the estimated maximum likelihood allocation for this person?

(b) Using normal theory quadratic discrimination, what is the estimated maximum likelihood allocation for this person?

(c) If the two drugs have equal prior probabilities but the placebo is twice as probable as the drugs, what is the estimated maximum posterior probability allocation?

(d) Suppose the costs of correct classification are zero, the costs of misclassification depend only on the true population, and the cost of misclassifying an individual who actually has taken drug B is twice that of the other populations. Using the prior

Figure 12.7: Doughnut data in polar coordinates

probabilities of (c), what is the estimated Bayesian allocation?

(e) What is the optimal allocation using only the first two linear discrimination coordinates?

##### Exercise 12.9.2

In the motion picture _Diary of a Mad Turtle_ the main character, played by Richard Benjamin Kingsley, claims to be able to tell a female turtle by a quick glance at her carapace. Based on the data of Exercise 10.6.1, do you believe that it is possible to accurately identify a turtle's sex based on its shell? Explain. Include graphical evaluation of the linear discrimination coordinates.

##### Exercise 12.9.3

Using the data of Exercise 10.6.3, do a stepwise discriminant analysis to distinguish among the thyroxin, thiouracil, and control rat populations based on their weights at various times. To which group is a rat with the following series of weights most likely to belong: \((56,75,104,114,138)\)?

##### Exercise 12.9.4

Lachenbruch (1975) presents information on four groups of junior technical college students from greater London. The information consists of summary statistics for the performance of the groups on arithmetic, English, and form relations tests that were given in the last year of secondary school. The four groups are Engineering, Building, Art, and Commerce students. The sample means are:

\[S_{p} = \left[ {\begin{array}{*{20}c} {55.58} & {33.77} & {11.66} \\ {33.77} & {360.04} & {14.53} \\ {11.66} & {14.53} & {69.21} \\ \end{array} } \right].\]

What advice could you give to a student planning to go to a junior technical college who just achieved scores of \((22,90,31)^{\prime}\)?

##### Exercise 12.9.5

Suppose the concern in Exercise 12.9.4 is minimizing the cost to society of allocating students to the various programs of study. The great bureaucrat in the sky, who works on the top floor of the tallest building in Whitehall, has determined that the costs of classification are as follows:
Evaluate the program of study that the bureaucrat thinks is appropriate for the student from Exercise 12.9.4.

##### Exercise 12.9.6.

Show that the Mahalanobis distance is invariant under affine transformations \(z=Ay+b\) of the random vector \(y\) when \(A\) is nonsingular.

##### Exercise 12.9.7.

Let \(y\) be an observation from one of two normal populations that have means of \(\mu_{1}\) and \(\mu_{2}\) and common covariance matrix \(\Sigma\). Define \(\lambda^{\prime}=(\mu_{1}-\mu_{2})^{\prime}\Sigma^{-1}\).

(a) Show that, under linear discrimination, \(y\) is allocated to population 1 if and only if

\[\lambda^{\prime}y-\lambda^{\prime}\frac{1}{2}(\mu_{1}+\mu_{2})>0.\]

(b) Show that if \(y\) is from population 1,

\[\mathrm{E}(\lambda^{\prime}y)-\lambda^{\prime}\frac{1}{2}(\mu_{1}+\mu_{2})>0\]

and if \(y\) is from population 2,

\[\mathrm{E}(\lambda^{\prime}y)-\lambda^{\prime}\frac{1}{2}(\mu_{1}+\mu_{2})<0.\]

##### Exercise 12.9.8.

Consider a two group allocation problem in which the prior probabilities are \(\pi(1)=\pi(2)=0.5\) and the sampling distributions are exponential, namely

\[f(y|i)=\theta_{i}e^{-\theta_{i}y},\quad y\geq 0.\]

Find the optimal allocation rule. Assume a cost structure where \(c(i|j)\) is zero for \(i=j\) and one otherwise. The _total probability of misclassification_ for an allocation rule is precisely the Bayes risk of the allocation rule under this cost structure. Let \(\delta(y)\) be an allocation rule. The frequentist risk for the true population \(j\) is \(R(j,\delta)=\int c(\delta(y)|j)f(y|j)dy\) and the Bayes risk is \(r(p,\delta)=\sum_{j=1}^{t}R(j,\delta)\pi(j)\). See Berger (1985, Section 1.3) for more on risk functions. Find the total probability of misclassification for the optimal rule.

**Exercise 12.9.9.**  Suppose that the distributions for two populations are bivariate normal with the same covariance matrix. For \(\pi(1)=\pi(2)=0.5\), find the value of the correlation coefficient that minimizes the total probability of misclassification. The total probability of misclassification is defined in Exercise 12.9.8.

## References

* Aitchison (1975) Aitchison, J. (1975). Goodness of prediction fit. _Biometrika,__62_, 547-554.
* Aitchison & Dunsmore (1975) Aitchison, J., & Dunsmore, I. R. (1975). _Statistical prediction analysis_. Cambridge: Cambridge University Press.
* Anderson (2003) Anderson, T. W. (2003). _An introduction to multivariate statistical analysis_ (3rd ed.). New York: Wiley.
* Andrews et al. (1971) Andrews, D. F., Gnanadesikan, R., & Warner, J. L. (1971). Transformations of multivariate data. _Biometrics,__27_, 825-840.
* Berger (1985) Berger, J. O. (1985). _Statistical decision theory and bayesian analysis_. New York: Springer.
* Box & Cox (1964) Box, G. E. P. & Cox, D. R. (1964). An analysis of transformations (with discussion). _Journal of the Royal Statistical Society, Series B,__26_, 211-246.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York: Springer.
* Christensen et al. (2010) Christensen, R., Johnson, W., Branscum, A., & Hanson, T. E. (2010). _Bayesian ideas and data analysis: An introduction for scientists and statisticians_. Boca Raton: Chapman and Hall/CRC Press.
* Efron (1983) Efron, B. (1983). Estimating the error rate of a prediction rule: Improvement on cross-validation. _Journal of the American Statistical Association,__78_, 316-331.
* Fisher (1936) Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. _Annals of Eugenics,__7_, 179-188.
* Fisher (1938) Fisher, R. A. (1938). The statistical utilization of multiple measurements. _Annals of Eugenics,__8_, 376-386.
* Friedman (1989) Friedman, J. H. (1989). Regularized discriminant analysis. _Journal of the American Statistical Association,__84_, 165-175.
* Geisser (1971) Geisser, S. (1971). The inferential use of predictive distributions. In V. P. Godambe & D. A. Sprott (Eds.), _Foundations of statistical inference_. Toronto: Holt, Rinehart, and Winston.
* Geisser (1977) Geisser, S. (1977). Discrimination, allocatory and separatory, linear aspects. In J. Van Ryzin (Ed.), _Classification and clustering_. New York: Academic Press.
* Hand (1981) Hand, D. J. (1981). _Discrimination and classification_. New York: Wiley.
* Hand (1983) Hand, D. J. (1983). A comparison of two methods of discriminant analysis applied to binary data. _Biometrics,__39_, 683-694.
* Hastie et al. (2016) Hastie, T., Tibshirani, R., & Friedman, J. (2016). _The elements of statistical learning: Data mining, inference, and prediction_ (2nd ed.). New York: Springer.

* Johnson & Wichern (2007) Johnson, R. A., & Wichern, D. W. (2007). _Applied multivariate statistical analysis_ (6th ed.). Englewood Cliffs: Prentice-Hall.
* Lachenbruch (1975) Lachenbruch, P. A. (1975). _Discriminate analysis_. New York: Hafner Press.
* Lachenbruch et al. (1973) Lachenbruch, P. A., Sneeringeer, C., & Revo, L. T. (1973). Robustness of the linear and quadratic discriminant function to certain types of non-normality. _Communications in Statistics, 1_, 39-57.
* Levy & Perng (1986) Levy, M. S., & Perng, S. K. (1986). An optimal prediction function for the normal linear model. _Journal of the American Statistical Association, 81_, 196-198.
* Lubischew (1962) Lubischew, A. A. (1962). On the use of discriminant functions in taxonomy. _Biometrics, 18_, 455-477.
* Murray (1977) Murray, G. D. (1977). A note on the estimation of probability density functions. _Biometrika, 64_, 150-152.
* Panel (1989) Panel on Discriminant Analysis, Classification, and Clustering. (1989). Discriminant analysis and clustering. _Statistical Science, 4_, 34-69.
* Press (1982) Press, S. J. (1982). _Applied multivariate analysis: Using Bayesian and frequentist methods of inference_ (2nd ed.). Malabar: R.E. Krieger. (Latest reprinting, Dover Press, 2005).
* Press & Wilson (1978) Press, S. J., & Wilson, S. (1978). Choosing between logistic regression and discriminant analysis. _Journal of the American Statistical Association, 73_, 699-705.
* Rao (1948) Rao, C. R. (1948). The utilization of multiple measurements in problems of biological classification. _Journal of the Royal Statistical Society, Series B, 10_, 159-203.
* Rao (1952) Rao, C. R. (1952). _Advanced statistical methods in biometric research_. New York: Wiley.
* Ripley (1996) Ripley, B. D. (1996). _Pattern recognition and neural networks_. Cambridge: Cambridge University Press.
* Scheffe (1959) Scheffe, H. (1959). _The analysis of variance_. New York: Wiley.
* Seber (1984) Seber, G. A. F. (1984). _Multivariate observations_. New York: Wiley.

## Chapter 13 Binary Discrimination and Regression

Regression analysis and discrimination involve different data collection schemes. Regression collects independent observations from the joint distribution of \((y,\mathbf{x}^{\prime})\). Aldrich (2005) suggests that it was Fisher who first argued that regression estimation should condition on the predictor variables \(\mathbf{x}\). The first part of this chapter examines how to estimate \(p(\mathbf{x})\) from the conditional distribution of \(y\) given \(\mathbf{x}\). In this part we need only assume that the observations are conditionally independent and that \(y\sim\text{Bin}[1,p(\mathbf{x})]\equiv\text{Bern}[p(\mathbf{x})]\). The last section of the chapter considers estimates of \(p(\mathbf{x})\) derived from sampling the conditional distribution of \(\mathbf{x}\) given \(y\). This is the binary (\(t=2\)) version of the discrimination problem considered in Chap. 12. The middle of the chapter examines Hamming prediction without explicitly estimating \(p(\mathbf{x})\).

Throughout we will explicitly incorporate ideas on penalized estimates similar to Chap. 2. Implicit throughout is that the models can exploit the nonparametric linear structures discussed in Chap. 1.

We begin with the binomial regression problem that most generalized linear model computer programs are written to handle.

### Binomial Regression

Suppose there are a number of independent observations with \(y_{h}\sim\text{Bin}[1,p(\mathbf{x}_{h})]\). Often such data get reported only as the total number of successes for each vector of predictor variables. In such cases, we implicitly reindex the original data as

\[(y_{ij},\mathbf{x}_{i}^{\prime}),\quad i=1,\ldots,n,\ j=1,\ldots,N_{i}\]

so that the reported data are

\[(y_{i\cdot},\mathbf{x}_{i}^{\prime}),\quad i=1,\ldots,n,\quad\text{where }y_{i\cdot}\equiv\sum_{j=1}^{N_{i}}y_{ij}\.\]

We now have independent binomial random variables

\[N_{i}\bar{y}_{i\cdot}\equiv y_{i\cdot}\sim\text{Bin}[N_{i},p(\mathbf{x}_{i})]; \qquad i=1,\ldots,n,\]

where the binomial proportions \(\bar{y}_{i\cdot}\) are between 0 and 1. It is common practice to write binomial generalized linear model computer programs using the binomial proportions as the input data and specifying the \(N_{i}\)s as weights. Obviously such programs can also handle the original binary data \((y_{h},\mathbf{x}_{h}^{\prime})\) by writing \(h=1,\ldots,n\) but with \(N_{h}=1\) for all \(h\). _In conformance with such programs, we write_

\[y_{i}\equiv\bar{y}_{i\cdot}\]

_for the rest of this section._

The likelihood function for independent data with \(N_{i}y_{i}\sim\text{Bin}[N_{i},p(\mathbf{x}_{i})]\) is

\[L[p(\cdot)]\equiv\prod_{i=1}^{n}\binom{N_{i}}{N_{i}y_{i}}[p(\mathbf{x}_{i})]^{ N_{i}y_{i}}[1-p(\mathbf{x}_{i})]^{N_{i}-N_{i}y_{i}}.\]The _deviance_ is defined as \(-2\) times the log-likelihood so

\[D[p(\cdot)]\] \[\equiv -2\sum_{i=1}^{n}\left\{N_{i}y_{i}\log\left[p(\mathbf{x}_{i})\right]+ \left(N_{i}-N_{i}y_{i}\right)\log\left[1-p(\mathbf{x}_{i})\right]\right\}-2\sum _{i=1}^{n}\log\left[\binom{N_{i}}{N_{i}y_{i}}\right]\] \[= \sum_{i=1}^{n}-2N_{i}\left\{y_{i}\log\left[p(\mathbf{x}_{i})\right] +\left(1-y_{i}\right)\log\left[1-p(\mathbf{x}_{i})\right]\right\}-2\sum_{i=1}^ {n}\log\left[\binom{N_{i}}{N_{i}y_{i}}\right].\]

A maximum likelihood estimate of \(p(\cdot)\) maximizes the likelihood or, equivalently, minimizes the deviance. To simplify notation denote the constant term in the deviance

\[K\equiv-2\sum_{i=1}^{n}\log\left[\binom{N_{i}}{N_{i}y_{i}}\right].\]

The constant term has no effect on estimation. For binary regression models in which \(N_{i}\equiv 1\) so that \(y_{i}\) is \(1\) or \(0\), the constant term in the deviance vanishes and only one of the two terms in the braces actually applies. Either \(y_{i}\) or \(1-y_{i}\) has to be zero, so one of the terms in the braces always gets multiplied by \(0\).

If the function \(p(\mathbf{x})\) is known except for some unknown parameter vector \(\theta\), write \(p(\mathbf{x};\theta)\). The maximum likelihood estimate of \(\theta\) maximizes the parameterized likelihood

\[L(\theta)\equiv\prod_{i=1}^{n}\binom{N_{i}}{N_{i}y_{i}}[p(\mathbf{x}_{i};\theta )]^{N_{i}y_{i}}[1-p(\mathbf{x}_{i};\theta)]^{N_{i}-N_{i}y_{i}}\]

or minimizes the parameterized deviance

\[D(\theta)\equiv\sum_{i=1}^{n}-2N_{i}\left\{y_{i}\log\left[p(\mathbf{x}_{i}; \theta)\right]+\left(1-y_{i}\right)\log\left[1-p(\mathbf{x}_{i};\theta)\right] \right\}+K.\]

_Henceforth, we take \(x\) to be a \(d\) vector that includes all explanatory variables. In most cases \(x^{\prime}=(1,\mathbf{x}^{\prime})\)._

Binomial generalized linear models typically specify that the conditional probability is a known function of \(x^{\prime}\beta\). In particular,

\[p(\mathbf{x})\equiv p(x)=F(x^{\prime}\beta)\]

for some known cumulative distribution function (cdf) \(F\) for which the inverse function \(F^{-1}\) exists. \(F\) is a cdf so that the real valued term \(x^{\prime}\beta\) is transformed into a number between \(0\) and \(1\). The inverse function is called a _link_ function and is used to isolate the linear structure \(x^{\prime}\beta\) of the model, i.e.,

\[F^{-1}[p(x)]=x^{\prime}\beta.\]

The most common choices for \(F\) are the standard versions of the _logistic_, normal, and _Gumbel (minimum)_ distributions. With \(\Phi(\cdot)\) denoting the cdf for a \(N(0,1)\) random variable,\[p(x)=F(x^{\prime}\beta)=\left\{\begin{array}{ll}e^{x^{\prime}\beta}\left/\left[1+e ^{x^{\prime}\beta}\right]&\text{Logistic}\\ \Phi(x^{\prime}\beta)&\text{Normal}\\ 1-\exp\left[-e^{x^{\prime}\beta}\right]&\text{Gumbel}.\end{array}\right.\]

Most often the procedures are referred to by the names of the inverse functions rather than the names of the original cdfs:

\[x^{\prime}\beta=F^{-1}[p(x)]=\left\{\begin{array}{ll}\log\left\{p(x)\right/ \left[1-p(x)\right]\right\}&\text{Logit}\\ \Phi^{-1}[p(x)]&\text{Probit}\\ \log\left\{-\log[1-p(x)]\right\}&\text{Complementary log-log}.\end{array}\right.\]

In the case of logit/logistic models, logit often refers to ANOVA type models and logistic is often used for regression models. I use the terms interchangeably but prefer calling them logit models. (In this chapter, all references to "generalized linear models" refer to the subclass of binomial generalized linear models defined using an inverse cdf link.)

In any case, the likelihood function for such data is

\[L_{F}(\beta)\equiv\prod_{i=1}^{n}\binom{N_{i}}{N_{i}y_{i}}[F(x_{i}^{\prime} \beta)]^{N_{i}y_{i}}[1-F(x_{i}^{\prime}\beta)]^{N_{i}-N_{i}y_{i}}\]

and the deviance is

\[D_{F}(\beta)\equiv\sum_{i=1}^{n}-2N_{i}\left\{y_{i}\log\left[F(x_{i}^{\prime} \beta)\right]+(1-y_{i})\log\left[1-F(x_{i}^{\prime}\beta)\right]\right\}+K. \tag{13.1.1}\]

As always, the constant term \(K\) in the deviance is irrelevant to the estimation of \(\beta\).

Note that minimum deviance (maximum likelihood) estimation fits into the pattern discussed in _PA-V_ Sect. 13.6 of estimating \(\beta\) by defining weights \(w_{i}>0\) and a loss function \(\mathcal{L}(y,u)\) and minimizing

\[\sum_{i=1}^{n}w_{i}\mathcal{L}(y_{i},x_{i}^{\prime}\beta).\]

Here the weights are \(w_{i}=N_{i}\) and the loss function is

\[\mathcal{L}(y,u)=-2\left\{y\log\left[F(u)\right]+(1-y)\log\left[1-F(u)\right] \right\}.\]

In later sections we will see that the loss function is easier to interpret for binary data and that _support vector machines_ use a very similar procedure when estimating \(\beta\).

In analogy to penalized least squares estimates, we can form _penalized minimum deviance (penalized maximum likelihood)_ estimates that minimize

\[D_{F}(\beta)+k\mathcal{P}(\beta).\]Typically, we use the same penalty functions \(\mathcal{P}(\beta)\) as discussed in Chap. 2 for penalized least squares. For a multiple regression with \(x_{i}^{\prime}\beta\equiv\beta_{0}+\sum_{j=1}^{d-1}\beta_{j}x_{ij}\), write \(\beta_{*}\equiv(\beta_{1},\ldots,\beta_{d-1})^{\prime}\). As with standard regression, we typically would not penalize the intercept. By choosing

\[\mathcal{P}_{L}(\beta)\equiv\sum_{j=1}^{d-1}|\beta_{j}|=\|\beta_{*}\|_{1}\]

we get lasso binomial regression. By choosing

\[\mathcal{P}_{R}(\beta)\equiv\beta_{*}^{\prime}\beta_{*}=\sum_{j=1}^{d-1}\beta_ {j}^{2}\]

we get one form of ridge binomial regression. Elastic net binomial regression is obtained by taking

\[\mathcal{P}_{E}(\beta)\equiv k_{1}\mathcal{P}_{R}(\beta)+k_{2}\mathcal{P}_{L}( \beta).\]

As mentioned in Chap. 2, these penalty functions penalize each coefficient the same amount, so typically one would _standardize the predictor variables to a common length_ before applying such a penalty. (The penalization ideas apply to all generalized linear models, not just these binomial generalized linear models, and are fundamental to support vector machines.)

As in Chap. 2, it is a simple matter to generalize the penalized estimation ideas to a partitioned model,

\[F^{-1}[p(x_{i},z_{i})]=x_{i}^{\prime}\beta+z_{i}^{\prime}\gamma\]

where \(x_{i}\) and \(z_{i}\) are known, \(\beta\) and \(\gamma\) are unknown parameters and where we only penalize \(\gamma\).

#### Data Augmentation Ridge Regression

For linear ridge regression we established in Sect. 2.2 that the ridge estimates could be obtained by fitting an augmented linear model. We now define an analogous augmented binomial regression model and infer the penalty function that it implicitly incorporates. (The penalty is not the traditional ridge penalty.) Although ridge regression requires no assumption of normality, the analogies between standard regression and binomial regression will be clearer making it.

Model (2.2.5) is an augmented, partitioned linear model that provides generalized ridge estimates. By specifying \(Q=I\), model (2.2.5) provides standard ridge estimates. With \(Q=I\), model (2.2.5) treats the augmented observations 0 as observations on independent random variables \(\tilde{y}_{j}\), \(j=1,\ldots,s\) with the distribution \(\tilde{y}_{j}\sim N(\gamma_{j},\sigma^{2}/k)\). The model involves finding the simplest form of generalized least squares estimates: weighted least squares. The vector of weights becomes \(w\equiv[J_{n}^{\prime},k_{s}^{\prime}]^{\prime}\). Relatively few computer programs for linear models incorporate the ability to perform generalized least squares but I cannot remember ever using a regression program that did not include the capability for weighted least squares.

Data augmentation binomial ridge regression takes \(s\) augmenting observations as \(\tilde{y}_{j}=F(0)\) and treats them as independent with \(k\tilde{y}_{j}\sim\text{Bin}[k,F(\gamma_{j})]\). For logit and probit models \(\tilde{y}_{j}=0.5\). To analyze such data you need software that is coded in enough generality that it permits analysis on binomials with non-integer numbers of trials. An augmented observation \(\tilde{y}_{j}=F(0)\) comes from a case with probability \(F(\gamma_{j})\) so it forces \(\gamma_{j}\) towards \(0\). The parameter \(k\) determines how many Bernoulli trials \(\tilde{y}_{j}\) corresponds to, so it determines how much \(\gamma_{j}\) gets forced towards \(0\). These augmented data define the same augmented model matrix as in (2.2.5). Model (2.2.5) augments the data \(Y\) with a string of \(0\)s but instead we augment \(Y\) into \([Y^{\prime},F(0)J^{\prime}_{s}]^{\prime}\). The weight vector we need for the augmented binomial model is exactly the same as the weight vector for model (2.2.5).

The penalty associated with this procedure is defined by what the augmenting observations add to the deviance function. Ignoring the constant term that the augmented data add to the deviance, it is not hard to see that the penalty function, say, \(\mathcal{P}_{R2}(\gamma)\) is defined via

\[k\mathcal{P}_{R2}(\gamma)\equiv k\sum_{j=1}^{s}-2\left\{F(0)\log\left[F(\gamma _{j})\right]+\left[1-F(0)\right]\log\left[1-F(\gamma_{j})\right]\right\}.\]

### Binary Prediction

Henceforth we use binary data (\(N_{i}=1\)) to make binary predictions. In the machine learning community, binary prediction is called _classification_, cf. Hastie, Tibshirani, and Friedman (2016). I cannot overemphasize that when performing this activity, there is an important distinction to be made over how the data were collected. In regression problems, a sample is taken from a population and individuals randomly fall into a group: \(0\) or \(1\). In the discrimination problems considered in Chap. 12, data are randomly sampled from each group separately. The term "classification" has traditionally been associated with discrimination problems, but maintaining that distinction is surely a losing battle.

#### Exercise 13.1

How does sampling from fixed groups, as associated with ANOVA models, fit into the regression/discrimination distinction?

One of the beauties of using the binomial/binary generalized linear models in Sect. 13.1 for binary prediction of regression data is that they provide estimated probabilities for belonging in the two groups. Having good estimates of \(p(x)\equiv\text{E}(y|x)\) helps in making good predictions for any prediction loss: squared error, absolute error, even Hamming. However, under Hamming prediction loss, all that matters is, for any \(x\), estimating whether \(p(x)>0.5\) or \(p(x)<0.5\). Hamming loss only cares whether cases get assigned to the correct group.

The estimate of \(p(x)\) leads to a linear prediction rule. A _linear prediction rule_ amounts to defining a hyperplane of \(x\) vectors and predicting that points on one side of the hyperplane will be a 1 and points on the other side will be a 0. In Sect. 13.4 we consider a wider class of linear prediction rules that merely assign cases to groups without actually estimating the probability function. The motivation for this will be by analogy to the estimation methods for generalized linear models discussed in Sect. 13.3. These linear prediction rules include the support vector machines considered in Sect. 13.5 and they seem to implicitly assume that the data are regression data rather than discrimination data. Section 13.6 looks at how to estimate \(p(x)\) from the best predictor associated with a specific loss function.

Consider again the Cushing's syndrome data from Table 12.1, To illustrate binary prediction, we restrict our attention to the 15 cases that are bilateral hyperplasia or carcinoma. Again, the analysis is performed on the logarithms of the predictor variables. Figure 13.1 plots the points and includes three linear prediction rules: logistic regression, probit regression and a support vector machine. Points above a line are classified as carcinoma and points below a line are identified as bilateral hyperplasia. To anthropomorphize, the generalized linear models seem to care more about not getting any point too badly wrong. The SVM almost seems like if it cannot get that one bilateral point correctly classified, it doesn't care how far it is from the line. (Indeed, even if the SVM could get that bilateral point correctly classified, if ignoring it will get the fitted line far enough away from all the other points, the SVM would still largely ignore the misclassified point. For more on this see the artificially simple example in the online R commands document.) Christensen (1997, Section 4.7) and Ripley (1996, Section 2.4) discuss the complete three group data.

It is not clear whether the Cushing Syndrome data are regression data or discrimination data. If regression data, someone would have sampled 21 Cushing's Syndrome patients who fell into the categories: 6 adenoma, 10 bilateral hyperplasia, 5 carcinoma. If discrimination data, someone decided to sample 6 adenoma patients, 10 bilateral hyperplasia patients, and 5 carcinoma patients. We assumed the latter in Chap. 12. For discrimination data, the generalized linear model methods of the next section require the adjustments discussed at the end of the chapter before they will make proper predictions. Linear prediction methods that are not closely associated with estimating \(p(x)\) have shaker justifications when used for discrimination data because they do not lend themselves to the adjustments that are clearly needed for generalized linear models. The discrimination methods of Chap. 12_are_ closely associated with estimating \(p(x)\) but they do it indirectly by estimating the density \(f(y|i)\) of the predictor variables given the group.

### Binary Generalized Linear Model Estimation

For binary data the deviance in (13.1.1) reduces to

\[D_{F}(\beta)\equiv\sum_{i=1}^{n}-2\left\{y_{i}\log\left[F(x_{i}^{\prime}\beta) \right]+(1-y_{i})\log\left[1-F(x_{i}^{\prime}\beta)\right]\right\}. \tag{13.3.1}\]

Again, minimum deviance (maximum likelihood) estimation fits into the pattern of estimating \(\beta\) by defining a loss function \(\mathcal{L}(y,u)\) and weights \(w_{i}>0\) and then minimizing

\[\sum_{i=1}^{n}w_{i}\mathcal{L}(y_{i},x_{i}^{\prime}\beta).\]

For binary generalized linear models the weights are all 1 and, as before, the loss function is

\[\mathcal{L}_{F}(y,u)=-2\left\{y\log\left[F(u)\right]+(1-y)\log\left[1-F(u) \right]\right\}\]

but now, because of the binary nature of the data, we can write

\[\mathcal{L}_{F}(y,u)=\left\{\begin{array}{ll}-2\log\left[F(u)\right]&\text{ if }y=1\\ -2\log\left[1-F(u)\right]&\text{ if }y=0\end{array}.\right. \tag{13.3.2}\]

Figure 13.1: Logistic regression, probit regression, and an SVM: Cushingâ€™s syndrome data (subset)The logit and probit loss functions are plotted in Fig. 13.2. The loss functions are quite similar, as were the prediction lines in Fig. 13.1.

A penalized minimum deviance (penalized maximum likelihood) estimate is defined as in Sect. 13.1. It can be viewed as minimizing

\[\sum_{i=1}^{n}\mathcal{L}_{F}(y_{i},x_{i}^{\prime}\beta)+k\mathcal{P}(\beta).\]

(The artificial example in the R code document includes a data augmentation ridge fit that is reasonably similar to the default SVM.)

### 13.4 Linear Prediction Rules

We now examine linear prediction rules in detail. First, that generalized linear models lead to linear prediction rules and then, that similar ideas can produce linear

Figure 13.2: Binary logistic regression and probit regression loss functions

prediction rules without an explicit probability model. In Sect. 13.6 we will try to relate such rules back to probability models.

For the generalized linear models, the optimal Hamming loss predictor is 1 when \(F(x^{\prime}\beta)>0.5\) and 0 when \(F(x^{\prime}\beta)<0.5\). These conditions are equivalent to predicting 1 when \(x^{\prime}\beta>F^{-1}(0.5)\) and 0 when \(x^{\prime}\beta<F^{-1}(0.5)\). Thus the hyperplane of \(x\) vectors that satisfy \(x^{\prime}\beta=F^{-1}(0.5)\) implicitly defines a linear prediction rule which is the optimal Hamming rule for the generalized linear model.

With \(\hat{\beta}\) the minimum deviance estimate and \(F(x^{\prime}\hat{\beta})\) the estimated probability for group 1, the logistic and probit lines in Fig. 13.1 were constructed by setting \(F(x^{\prime}\hat{\beta})=0.5\), i.e., \(x^{\prime}\hat{\beta}=F^{-1}(0.5)=0\). (The last equality only holds when 0 is a median of \(F\) and always holds when \(F\) is symmetric about 0.) When \(x^{\prime}\hat{\beta}>0\), the logistic and probit models have \(F(x^{\prime}\hat{\beta})>0.5\). When \(x^{\prime}\hat{\beta}<0\), they have \(F(x^{\prime}\hat{\beta})<0.5\).

In a regression setting we typically have

\[x^{\prime}\beta\equiv\beta_{0}+\sum_{j=1}^{d-1}\beta_{j}x_{j}=\beta_{0}+{\bf x ^{\prime}}\beta_{*}\]

where

\[{\bf x^{\prime}}\equiv(x_{1},\ldots,x_{d-1}).\]

The hyperplane \(x^{\prime}\hat{\beta}=F^{-1}(0.5)\) is the same creature as \(\big{[}\beta_{0}-F^{-1}(0.5)\big{]}+{\bf x^{\prime}}\beta_{*}=0\). As a function of the predictor variables in \({\bf x}\), the orientation of the hyperplane is determined by \(\beta_{*}\). Hyperplanes with \(\beta_{*}\) vectors that are multiples of one another are parallel in \({\bf R}^{d-1}\).

Any hyperplane \(x^{\prime}\beta=0\) can be used to predict binary outcomes using the rule: _if \(x^{\prime}\beta>0\) the case is predicted as group 1 and if \(x^{\prime}\beta<0\) the case is predicted as group 0_. Using the regression notation that means: group 1 if \(-\beta_{0}<{\bf x^{\prime}}\beta_{*}\) and group 0 if \(-\beta_{0}>{\bf x^{\prime}}\beta_{*}\). To use a hyperplane \(x^{\prime}\beta=C\) is simply to redefine \(\beta_{0}\) as \(\beta_{0}-C\). If \(\eta\) is a nonzero scalar multiple of \(\beta\), the vectors \(x\) with \(x^{\prime}\beta=0\) are precisely the same as the vectors \(x\) with \(x^{\prime}\eta=0\), so \(\beta\) and \(\eta\) define the same linear predictor. (Although if the constant of proportionality is negative, the codes for the two groups will be reversed.)

#### Exercise 13.2

Show that \(|x^{\prime}\beta|\) is \(\|\beta\|\) times the perpendicular distance from \(x\) to the prediction hyperplane (subspace) \(\{x|x^{\prime}\beta=0\}\) by finding the perpendicular distance using \(M_{\beta}\equiv\beta(\beta^{\prime}\beta)^{-1}\beta^{\prime}\).

Since any \(\beta\) determines a binary predictor, we can certainly pick one by minimizing

\[\sum_{i=1}^{n}{\cal L}(y_{i},x_{i}^{\prime}\beta)+k{\cal P}(\beta),\]

[MISSING_PAGE_FAIL:526]

for a symmetric matrix \(B\) and that this is also a linear function of the predictor row vector \((1,TL,PL)^{\prime}\otimes(1,TL,PL)^{\prime}\). (The \(B\) matrix will be the subject of some linear (equality) constraints.)

#### Loss Functions

We have discussed the loss functions associated with binomial/binary generalized linear models. The loss function for support vector machines is discussed in the next section. The use of squared error loss is related to the normal theory discriminant analysis of Chap. 12 and is also discussed in the next subsection. In the machine learning community the use of squared error loss together with a penalty function is sometimes called the _proximal support vector machine_. Another loss function that gets used as an approximation to AdaBoost is

\[\mathcal{L}_{Ada}(y,u)=\left\{\begin{array}{ll}e^{-u}&\mbox{if }y=1\\ e^{u}&\mbox{if }y=0\end{array}\right..\]

Figure 13: Quadratic model logistic regression, probit regression (indistinguishable from logistic), and an SVM: Cushingâ€™s syndrome data (subset)

[MISSING_PAGE_FAIL:528]

Figure 13.4: Logistic regression and SVM loss functions

Figure 13.5: Quadratic model logistic regression, probit regression, and an SVM with reduced tuning parameter: Cushingâ€™s syndrome data (subset)

the constraints imposed by needing to minimize the loss function. The issue is less about whether the loss is more important than the penalty function and more about the highest order polynomial involved in the minimization. Appendix A.3 discusses the general problem of minimizing quadratic functions subject to linear inequality constraints and a subsection applies the general results to the SVM problem. In this chapter, we merely cite the most important of those results. Hastie et al. (2016), Zhu (2008), and Moguerza and Munoz (2006) all present introductions to SVMs.

#### Probability Estimation

The value \(|x^{\prime}\beta|\), which is \(\|\beta\|\) times the perpendicular distance from \(x\) to the prediction hyperplane, should measure the assuredness of a classification. The bigger the value, the more sure we should be of the classification. Unfortunately, for SVMs this does not obviously convert to a classification probability. First, the loss function associated with SVMs is similar to the logit and probit losses, so SVMs might be generalized linear models for some \(F\). If so, that would give us a way to associate probabilities with the SVM predictor. We will show that SVMs cannot be generalized linear models. Second, the general equation for determining probabilities from a best predictor given in Eq. (13.6.2) does not apply because the SVM loss function is not differentiable everywhere.

From (13.3.2) it is easy to see that a generalized linear model has

\[1=F(u)+[1-F(u)]=\exp[-\mathcal{L}_{F}(1,u)/2]+\exp[-\mathcal{L}_{F}(0,u)/2].\]

Making a similar computation for SVMs,

\[\exp[-\mathcal{L}_{S}(1,u)/2]+\exp[-\mathcal{L}_{S}(0,u)/2]=e^{-(1-u)_{+}/2}+e ^{-(1+u)_{+}/2}.\]

Evaluating this at \(u=\pm 1\) gives \(1+e^{-1}\), which is not equal to 1 but even more importantly, evaluating this at \(u=0\) gives a different value, \(2e^{-1/2}\), so there is no hope of rescaling the SVM loss function into one that corresponds to a generalized linear model.

#### Parameter Estimation

Finding the SVM parameter estimates is generally performed by turning the estimation problem into a quadratic optimization problem, cf. Appendix A.3.

Write our binary data in vector form as

\[Y\equiv\begin{bmatrix}Y_{1}\\ Y_{0}\end{bmatrix}\]where \(N_{1}\) successes are in \(Y_{1}\equiv J_{N_{1}}\) and \(N_{0}=n-N_{1}\) failures are in \(Y_{0}\equiv 0^{1}_{N_{0}}\). For this discussion only

\[J_{1}\equiv J_{N_{1}};\qquad J_{0}\equiv J_{N_{0}}.\]

Similarly write the model matrix, which includes an intercept predictor, as

\[X\equiv\begin{bmatrix}X_{1}\\ X_{0}\end{bmatrix}\equiv\begin{bmatrix}J_{1}&\mathbf{X}_{1}\\ J_{0}&\mathbf{X}_{0}\end{bmatrix}.\]

Any \(n\) vector \(v\) may be written

\[v=\begin{bmatrix}v_{1}\\ v_{0}\end{bmatrix}\]

in conformance with \(Y_{1}\) and \(Y_{0}\).

Support vector machines pick \(\beta=(\beta_{0},\beta_{*}^{\prime})^{\prime}\) by minimizing

\[\sum_{h=1}^{n}\mathcal{L}_{S}(y_{h},x_{h}^{\prime}\beta)+k\beta_{*}^{\prime} \beta_{*} \tag{13.5.1}\]

where

\[\mathcal{L}_{S}(y,u)=\left\{\begin{array}{ll}(1-u)_{+}&\text{if }y=1\\ (1+u)_{+}&\text{if }y=0\.\end{array}\right.\]

The innovative idea is to introduce slack variables \(\xi=(\Xi_{1},\ldots,\Xi_{n})^{\prime}\) that serve as upper bounds for the contributions to the loss function. Because the loss function is nonnegative, the slack variables are also. Minimizing the sum of the slack variables, because they are unknown upper bounds, amounts to minimizing the sum of the losses, so minimizing (13.5.1) is equivalent to finding

\[\inf_{\beta,\xi}\left(k\beta_{*}^{\prime}\beta_{*}+\xi^{\prime}J\right) \tag{13.5.2}\]

subject to

\[\mathcal{L}_{S}(y_{h},x_{h}^{\prime}\beta)\leq\Xi_{h},\quad h=1,\ldots,n. \tag{13.5.3}\]

To establish this as a quadratic optimization problem, we need to replace the loss function constraints (13.5.3) with linear constraints. When \(y_{h}=1\) the loss is \(\mathcal{L}_{S}(1,x_{h}^{\prime}\beta)=(1-x_{h}^{\prime}\beta)_{+}=\max\{0,1- x_{h}^{\prime}\beta\}\). We place two linear constraints on the slack variables to force them to be upper bounds for the loss function: \(0\leq\Xi_{h}\) and \(1-x_{h}^{\prime}\beta\leq\Xi_{h}\), which gives us \(\mathcal{L}_{S}(1,x_{h}^{\prime}\beta)\leq\Xi_{h}\). In matrix form these constraints are

\[0^{1}_{N_{1}}\leq\xi_{1};\qquad J_{1}-X_{1}\beta\leq\xi_{1} \tag{13.5.3a}\]

where _an inequality applied to a matrix is understood to apply elementwise_. Similarly for \(y_{h}=0\) impose \(0\leq\Xi_{h}\) and \(1+x_{h}^{\prime}\beta\leq\Xi_{h}\) or

\[0^{1}_{N_{0}}\leq\xi_{0};\qquad J_{0}+X_{0}\beta\leq\xi_{0}. \tag{13.5.3b}\]In total there are \(2n\) linear inequality constraints being imposed on the criterion function (13.5.2).

In matrix notation, rewrite the penalized loss function in standard form for quadratic optimization as

\[k\beta_{*}^{\prime}\beta_{*}+\xi^{\prime}J=\frac{1}{2}\begin{bmatrix}\beta_{0}\\ \beta_{*}\\ \xi\end{bmatrix}^{\prime}\begin{bmatrix}0&0&0\\ 0&2kI&0\\ 0&0&0\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{*}\\ \xi\end{bmatrix}+\begin{bmatrix}0\\ 0\\ J_{n}\end{bmatrix}^{\prime}\begin{bmatrix}\beta_{0}\\ \beta_{*}\\ \xi\end{bmatrix}, \tag{13.5.4}\]

which is to be minimized subject to the constraints (13.5.3) rewritten in standard form as

\[\begin{bmatrix}-X_{1}&-I&0\\ X_{0}&0&-I\\ 0&-I&0\\ 0&0&-I\end{bmatrix}\begin{bmatrix}\beta\\ \xi_{1}\\ \xi_{0}\end{bmatrix}\leq\begin{bmatrix}-J_{1}\\ -J_{0}\\ 0\\ 0\end{bmatrix}\]

or

\[\begin{bmatrix}-J_{1}&-\mathbf{X}_{1}&-I&0\\ J_{0}&\mathbf{X}_{0}&0&-I\\ 0&0&-I&0\\ 0&0&0&-I\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{*}\\ \xi_{1}\\ \xi_{0}\end{bmatrix}\leq\begin{bmatrix}-J_{1}\\ -J_{0}\\ 0\\ 0\end{bmatrix}.\]

In the SVM literature the criterion function (13.5.4) often gets multiplied by \(\tilde{C}\equiv 1/2k\) which results in minor changes to the results, cf. Exercise A.2.

As discussed in Appendix A.3, typically one finds an \(n\) vector \(\lambda_{1}\) (_not_ an \(N_{1}\) vector like \(v_{1}\) in \(v^{\prime}=(v_{1}^{\prime},v_{0}^{\prime})^{\prime}\)) that _maximizes_ the dual criterion

\[\frac{-1}{2k}\lambda_{1}^{\prime}\begin{bmatrix}\mathbf{X}_{1}\mathbf{X}_{1}^{ \prime}&-\mathbf{X}_{1}\mathbf{X}_{0}^{\prime}\\ -\mathbf{X}_{0}\mathbf{X}_{1}^{\prime}&\mathbf{X}_{0}\mathbf{X}_{0}^{\prime} \end{bmatrix}\lambda_{1}+\lambda_{1}^{\prime}J_{n}.\]

(Appendix A.3 involves another \(n\) vector \(\lambda_{2}\) because there are \(2n\) linear inequality constraints.) The dual criterion has \(\lambda_{1}\) subject to the constraints

\[-J_{1}^{\prime}\lambda_{11}+J_{0}^{\prime}\lambda_{10}=0\]

and

\[0\leq\lambda_{1}\leq 1.\]

Actual solutions \(\beta\) and \(\xi\) need to incorporate the well-known KKT conditions.

Appendix A.3 establishes that

\[\hat{\beta}_{*}=\frac{1}{2k}\left(\mathbf{X}_{1}^{\prime}\lambda_{11}-\mathbf{X }_{0}^{\prime}\lambda_{10}\right).\]

Often many of the \(\lambda_{1}\) values are zero, so it makes sense to report only the values of \(\lambda_{1}\) that are nonzero and report the corresponding rows of \(\mathbf{X}_{1}\) and \(-\mathbf{X}_{0}\). The computer 

[MISSING_PAGE_FAIL:533]

##### Exercise 13.4.

Apply cubic logistic regression along with a cubic kernel SVM to the Cushing's syndrome data. Provide a two-dimensional plot of the separating curves.

##### Advantages of SVMs

To be honest, the whole point of writing this chapter was to address support vector machines. I had planned a subsection listing the advantages of SVMs but, after studying SVMs, I no longer see any advantages. I once thought that their ability to involve the kernel trick was an advantage. But we established in Sect. 1.8.2 that \(C(\Phi)=C(\hat{R})\), so the kernel trick applies to any linear structure \(X\beta\), whether it is applied to regular linear models, generalized linear models, proportional hazard models, or anything else. (I once gave a talk where I jokingly referred to \(C(X)=C(XX^{\prime})\) as the Fundamental Theorem of RKHSs for Statistics.) The other advantage I imagined for SVMs was computational, because the vector \(\lambda_{1}\) is often nonzero on only a relatively small subset of the data involving "support vectors." But in my review of the literature (which was far from complete but more extensive than the references I have given) I did not notice any such claims being made for SVMs; no more does the theory in Appendix A.3 suggest to me any such advantage. In fact, I found some discussion of the need to deal with the computational problems that SVMs have with big data (something that would be unlikely to arise if the computational complexity was being driven by a relatively small number of support vectors). This is _not_ to say that SVMs don't give reasonable answers; they do. I am just not aware of any advantages they have over using logistic regression with the kernel trick and penalized estimation.

##### Separating Hyper-Hogwash

SVMs are often sold as finding the optimal hyperplane that has all the data from one group above the hyperplane and all the data from the other group below the hyperplane. The "optimal" hyperplane is defined as the hyperplane that maximizes the distance from the plane to the points on either side that are closest to the hyperplane. While this technical argument is correct, as a reason for using SVMs I think it is quite simply hogwash. I am _not_ saying that SVMs are hogwash, only this argument for using them. The optimal separating hyperplane phenomenon is based almost entirely on the fact that SVMs involve minimizing the ridge regression penalty.

* The whole point of binomial generalized linear models is to find good ways of estimating probabilities for the cases that are not obviously from one group or the other. If a separating hyperplane exists, the problem is trivial! All of the MLEprobabilities can be pushed arbitrarily close to 1 or 0, cf. Exercise 13.5. _The important question for SVMs (like for all linear predictors) is not how to pick a separating hyperplane but how to pick a hyperplane when separation is not possible!_

In Fig. 13.1, using a linear function of \(TL\) and \(PL\), separation was not possible. In Fig. 13.3, using quadratic functions of \(TL\) and \(PL\), separation is possible, so the _reported_ maximum likelihood logistic and probit fits do that; they separate the cases. In fact, because it is possible to separate the cases, unique maximum likelihood fits to the linear predictors do not exist. The reported curves in Fig. 13.3 for logistic and probit regression are merely those reported when R's glm function stopped iterating. Anderson (1972) argued that any logistic regression program will find you a separating hyperplane when they exist. Essentially, when the program finds a separating hyperplane, that fact establishes that no unique maximum likelihood estimate will exist. Anderson (1972) and Albert and Anderson (1984) show that there are no unique maximum likelihood estimators for separable logistic regression.

* _Finding the **optimal** separating hyperplane is **largely** a waste of time._ Figure 13.5 illustrates three separating hyperplanes in the form of parabolas. What basis is there for picking one separating hyperplane over another one? In terms of maximizing the likelihood, they are all equally good. Why should you think there would be a best separating hyperplane? Do you really need to impose some artificial optimality criterion to find a "best" separating hyperplane? (I admit that maximizing the distance from the separating hyperplane to the closest points on either side is a nice choice, if you think it is worth the trouble to make a choice.)
* _If a separating hyperplane exists, and the procedure does not give you a separating hyperplane, then clearly the procedure is not about finding the optimal separating hyperplane._ Figure 13.3 shows that the default parabola fitted by svm does _not_ separate the two groups, even though the logit and probit fitted parabolas do separate the groups. I am not saying that the svm solution is bad, only that it is not finding a separating hyperplane when one clearly exists.

**Exercise 13.5**.: For a generalized linear model with \(F(0)=0.5\), suppose \(x^{\prime}\tilde{\beta}=0\) defines a separating hyperplane. Show that for \(C>1\), \(\hat{\beta}=C\tilde{\beta}\) has at least as high a likelihood because \(|x^{\prime}_{i}\hat{\beta}|\) is closer to \(\infty\) than \(|x^{\prime}_{i}\tilde{\beta}|\), hence \(F(x^{\prime}_{i}\hat{\beta})\) is closer to 0 or 1 than \(F(x^{\prime}_{i}\tilde{\beta})\).

By letting \(C\) go to infinity in the exercise, the associated likelihood will approach 1, which is its supremum. If \(\tilde{\beta}\) defines a separating hyperplane, typically _any_ sufficiently small modification of \(\tilde{\beta}\) will give another separating hyperplane, which will also lead to maximizing the likelihood.

### 13.6 Best Prediction and Probability Estimation

We began by assuming independent binary data \(y_{h}\sim\text{Bern}[p(x_{h})]\) and showed that fitting generalized linear models leads us to minimizing certain loss functions. In the last two sections we have ignored the distributional assumptions and discussed linear predictors based on minimizing different loss functions. We now go back and relate minimization of arbitrary loss functions to best prediction and to estimation of probabilities. Earlier we made the case that good estimation of probabilities was vital to estimating the best predictors for standard predictive loss functions such as squared error and Hamming.

The best predictor \(\hat{f}\) for an arbitrary predictive loss function \(\mathcal{L}(y,u)\) satisfies

\[\text{E}_{y,\mathbf{x}}\left\{\mathcal{L}[y,\hat{f}(\mathbf{x})]\right\}=\inf_ {f}\text{E}_{y,\mathbf{x}}\left\{\mathcal{L}[y,f(\mathbf{x})]\right\}. \tag{13.6.1}\]

The best predictor, if it can be found, is found by conditioning on \(\mathbf{x}\) and is the number \(\hat{u}\equiv\hat{f}(\mathbf{x})\) that achieves

\[\text{E}_{y|\mathbf{x}}\left[\mathcal{L}(y,\hat{u})\right]=\inf_{u}\text{E}_{ y|\mathbf{x}}\left[\mathcal{L}(y,u)\right].\]

If \(\mathcal{L}(y,u)\) is differentiable in \(u\) for all \(y\) and if the derivative can be taken under the integral of the conditional expectation, cf. Cramer (1946), the best prediction for a fixed \(\mathbf{x}\) should occur when

\[0=\mathbf{d}_{u}\text{E}_{y|\mathbf{x}}\left[\mathcal{L}(y,u)\right]=\text{E} _{y|\mathbf{x}}\left[\mathbf{d}_{u}\mathcal{L}(y,u)\right].\]

In the special case of binary prediction, this easily becomes

\[0=p(\mathbf{x})\left[\mathbf{d}_{u}\mathcal{L}(1,u)\right]+\left[1-p(\mathbf{x })\right]\left[\mathbf{d}_{u}\mathcal{L}(0,u)\right].\]

Typically, for known \(p(\mathbf{x})\), we would solve for \(\hat{u}\equiv\hat{f}(\mathbf{x})\) to find the best predictor for the loss function. As alluded to earlier, we can find the best predictor for square error, Hamming, and even absolute error loss functions.

In binary regression, sometimes people solve the equation for \(p(\mathbf{x})\),

\[p(\mathbf{x})=\frac{-\mathbf{d}_{u}\mathcal{L}[0,u]}{\left[\mathbf{d}_{u} \mathcal{L}(1,u)\right]-\left[\mathbf{d}_{u}\mathcal{L}(0,u)\right]}. \tag{13.6.2}\]

The original idea was to use the conditional distribution of \(y\) to find the best predictor (BP) under the loss function. Solving for \(p(\mathbf{x})\) is using the BP to find the conditional distribution. It presumes that you know the BP without knowing the conditional distribution. In practice, an estimated predictor \(u=\hat{f}(\mathbf{x})\) is sometimes plugged into (13.6.2) to obtain an estimate of \(p(\mathbf{x})\).

Predictive estimators \(\tilde{f}\) are often chosen to achieve

\[\inf_{f\in\mathscr{F}}\left\{\sum_{h=1}^{n}\mathscr{L}[y_{h},f(x_{h})]+k\mathscr{P }(f)\right\},\]

cf. Sect. 3.5.3. The fact that the sum puts the same weight on each observation pair \((y_{h},x_{h}^{\prime})\) is something that is (only?) appropriate when the data come from a simple random sample of some population, e.g., not discrimination data.

Standard limit theorems assure that \((1/n)\sum_{h=1}^{n}\mathscr{L}[y_{h},f(x_{h})]\) will be a reasonable estimate of \(\mathrm{E}_{y,\mathbf{x}}\left\{\mathscr{L}[y,f(\mathbf{x})]\right\}\) and if \(k\mathscr{P}(f)/n\to 0\), we should be able to evaluate the effectiveness of \(f\) for large samples. But the estimated predictor \(\tilde{f}\) is not generally an estimate of the best predictor \(\hat{f}\) as defined by (13.6.1), it is an estimate of the best predictor in \(\mathscr{F}\). Only if you are willing to assume that the best predictor is in \(\mathscr{F}\) does it make sense to use Eq. (13.6.2) to estimate the conditional probabilities. But, as discussed in Chap. 3, that is an assumption that we often make. Zhang, Liu, and Wu (2013) discuss these issues and argue that regularization, i.e., incorporating a penalty function when estimating the predictor \(\tilde{f}\), can have deleterious effects on using (13.6.2) for probability estimation.

It seems to be the case that people often define best prediction with one loss function, e.g., squared error or Hamming, but are willing to use a completely different predictive loss function to obtain an estimated predictor \(\tilde{f}\) and an estimate of \(p(x)\).

The following Exercise establishes that Eq. (13.6.2) can work very well as a method for estimating probabilities but that it can also work very poorly. It depends on the loss function being used.

##### Exercise 13.6.

(a) Show that, for linear models with squared error loss, Eq. (13.6.2) gives the rather unsatisfactory result \(p(\mathbf{x})=x^{\prime}\beta\). Why is this unsatisfactory?

(b) Show that for a generalized linear model based on a cdf \(F\) that is symmetric about 0, Eq. (13.6.2) returns the standard answer \(p(\mathbf{x})=F(x^{\prime}\beta)\).

(c) Show that if a loss function has the properties \(\mathscr{L}[0,-u]=\mathscr{L}[1,u]\) and \(\mathbf{d}_{u}\mathscr{L}[1,u]<0\), then Eq. (13.6.2) gives a number between 0 and 1 with \(p(0)=0.5\).

(d) When do the generalized linear models of the previous section have the properties in (c)?

Incidentally, Eq. (13.6.2) and fitting binomial generalized linear models are not the only ways to associate a linear predictor with group probabilities given the predictor variables. In Chap. 12 we used LDA and QDA (which are both linear predictors) to estimate group probabilities via estimation of the sampling distribution \(f(y|i)\). Doing that required knowledge of the prevalences (marginal group probabilities) as will be needed in the next section.

### Binary Discrimination

Binary discrimination shares the same predictive goal as binary regression but it involves using a different type of data and therefore requires outside information about the prevalences of the groups within the overall population. Instead of sampling from the joint distribution of the dependent and predictor variables (or the conditional distribution of the dependent variable given the predictor variables), discrimination involves sampling from the conditional distribution of the predictor variables given the dependent variable. If the reader has no previous experience with the difference between logistic regression and logistic discrimination, reading a more elementary treatment such as Christensen (2015, Section 21.9) is advisable.

To be consistent with the notation of Chap. 12, we need to change the binary regression notation. In Chap. 12, \(y\) is a \(q\) dimensional vector of predictor variables (rather than a \(d-1\) vector \(\mathbf{x}\)) and we will now use \(z\) (rather than \(y\)) to denote group membership. To predict \(z\) from \(y\) we need to envision a joint distribution for \((z,y^{\prime})\). Call the density (with respect to an appropriate dominating measure) \(f(z,y)\). Denote the marginal density (_prevalence_) of \(z\) as \(\pi(z)\), the conditional density of \(z\) given \(y\) as \(\pi(z|y)\), the marginal density of \(y\) as \(f(y)\), and the conditional density of \(y\) given \(z\) as \(f(y|z)\). In Chap. 12, \(z\) was fixed, not random, and we wrote \(i\) in place of \(z\). In binary regression and discrimination we denote the two \(z\) groups as 0 and 1. (In Chap. 12, two groups would have been labeled 1 and 2.)

Since we are focused on predicting \(z\), in both regression [sampling from either \(f(z,y)\) or \(\pi(z|y)\)] and discrimination [sampling from \(f(y|z)\)], our goal is to estimate \(\pi(z|y)\). (A sample from the joint distribution \(f(z,y)\) can be viewed as a sample from either conditional scheme.) Regression data gives direct information on \(\pi(z|y)\). Discrimination data gives direct information on \(f(y|z)\) but only indirect information on \(\pi(z|y)\). Using Bayes' Theorem with only two groups, the posterior probabilities in (12.1.4) become

\[\pi(1|y)=\frac{f(y|1)\pi(1)}{f(y|1)\pi(1)+f(y|0)\pi(0)};\qquad\pi(0|y)=1-\pi(1 |y).\]

If we know the prevalence distribution \(\pi(z)\), discrimination data allow us to estimate \(f(y|z)\) and, indirectly, \(\pi(z|y)\).

Bayes theorem also determines the posterior odds for seeing \(z=1\),

\[O(1|y)\equiv\frac{\pi(1|y)}{\pi(0|y)}=\frac{f(y|1)}{f(y|0)}\frac{\pi(1)}{\pi(0 )}. \tag{13.7.1}\]

We will see that binomial regression methods applied to discrimination data are easily adjusted to give appropriate posterior odds. The formulae for logistic regression is particularly nice.

Define the \(q+1\) dimensional row vectors \(x^{\prime}\equiv(1,y^{\prime})\) and \(\beta^{\prime}=(\beta_{0},\beta_{*}^{\prime})\) so that \(x^{\prime}\beta=\beta_{0}+y^{\prime}\beta_{*}\). Further combining the notation of Chap. 12 with the binary regression notation, the probability of seeing an observation in group 1 given the predictors is defined interchangeably as\[\pi(1|y)\equiv p(y)\equiv p(x).\]

Binary generalized linear models further assume \(p(y)=F(x^{\prime}\beta)\) for a known, invertible cdf \(F\). The densities \(f(z,y)\), \(f(y|z)\), and \(f(y)\) bear _no_ relationship to the cdf \(F\) used in specifying binary generalized linear models.

Binary regression assumes (conditionally) independent observations

\[z_{h}\sim\mbox{Bin}[1,p(y_{h})];\qquad h=1,\ldots,n\]

with the associated likelihood function, cf. Sect. 13.1. The likelihood function for discrimination data is

\[\prod_{h=1}^{n}f(y_{h}|z_{h})\equiv\prod_{i=0}^{1}\prod_{j=1}^{N_{i}}f(y_{ij}|i),\]

where there are \(N_{i}\) observations \(y_{ij}\) on group \(i\). Christensen (1997) argues that the logistic regression likelihood function can be viewed as a _partial likelihood_ function for discrimination data. (The argument is actually for the log-linear model that is equivalent to the logistic model.) This allows one to use binary regression methods to estimate the densities \(f(y|i)\) associated with discrimination data but it requires a correction for the prevalences implicitly assumed when treating discrimination data as if it were regression data.

Unconditional data constitute a random sample of \((z,y^{\prime})\) values. Clearly, one can estimate the marginal probabilities (prevalences) of the groups from unconditional data. The obvious estimate of \(\pi(1)\) is the number of observed values \(z=1\) divided by the sample size, \(N_{1}/(N_{1}+N_{0})\). If the \(y\)s are preselected and one samples from \(z|y\), i.e. the usual regression sampling scheme, there is no statistical basis for estimating \(\pi(1)\) without knowing the marginal density \(f(y)\). If the \(y_{h}\) were sampled from the appropriate distribution for \(y\), it would make \((z_{h},y^{\prime}_{h})\) a random sample from the appropriate joint distribution, and \(z_{h}\) a random sample with the prevalence probabilities, which makes \(N_{1}/(N_{1}+N_{0})\) the obvious estimate of \(\pi(1)\). Any estimation scheme that puts equal weight on the losses associated with each observation is implicitly treating the \(y_{h}\)s as a random sample and using \(N_{1}/(N_{1}+N_{0})\) as an estimate of \(\pi(1)\).

There is no possible way to estimate \(\pi(1)\) from discrimination data. A value for \(\pi(1)\) has to be obtained from some source outside the data before it becomes possible to estimate \(\pi(1|y)\). If we want to use regression estimates computed from discriminant data, we need to correct for the implicit use of \(N_{1}/(N_{1}+N_{0})\) as an estimate of \(\pi(1)\).

In discriminating between two groups, the maximum likelihood allocation can be based on the relative densities (likelihood ratio) \(f(y|1)/f(y|0)\). We now derive the likelihood ratio estimate, say, \(\hat{f}(y|1)/\hat{f}(y|0)\) from some arbitrary fitted binary regression estimates \(\tilde{\pi}(1|y)\equiv\bar{p}(y)\equiv\bar{p}(x)\) that incorporate the inappropriate prevalence \(\hat{\pi}(1)=N_{1}/(N_{1}+N_{0})\). From these we further obtain an estimate \(\hat{\pi}(1|y)\) of \(\pi(1|y)\) for an appropriate prior \(\pi(1)\).

The binary regression estimated posterior odds are \[\tilde{\mathcal{O}}(y)=\frac{\hat{\pi}(1|y)}{\hat{\pi}(0|y)}=\frac{\hat{f}(y|1)}{ \hat{f}(y|0)}\frac{\hat{\pi}(1)}{\hat{\pi}(0)}=\frac{\hat{f}(y|1)}{\hat{f}(y|0)} \frac{N_{1}}{N_{0}}.\]

These induce the estimated relative likelihoods

\[\frac{\hat{f}(y|1)}{\hat{f}(y|0)}=\frac{\hat{\pi}(1|y)}{\hat{\pi}(0|y)}\frac{N_ {0}}{N_{1}}.\]

To obtain the actual estimated posterior probabilities \(\hat{\pi}(i|y)\) for discrimination using the actual prevalences \(\pi(i)\), use the estimated odds

\[\hat{O}(y)\equiv\frac{\hat{\pi}(1|y)}{\hat{\pi}(0|y)}=\frac{\hat{f}(y|1)}{\hat {f}(y|0)}\frac{\pi(1)}{\pi(0)}=\frac{\hat{\pi}(1|y)}{\hat{\pi}(0|y)}\frac{N_{ 0}}{N_{1}}\frac{\pi(1)}{\pi(0)}.\]

The discrimination odds \(\hat{O}\) give discrimination probabilities \(\hat{\pi}\) through \(\hat{\pi}=\hat{O}/(1+\hat{O})\).

When fitting a logistic discrimination, the odds take a particularly nice form:

\[\log\left[\hat{O}(y)\right] \equiv x^{\prime}\tilde{\beta}-\log\left(\frac{N_{1}}{N_{0}}\right)+ \log\left(\frac{\pi(1)}{\pi(0)}\right)\] \[= y^{\prime}\tilde{\beta}_{*}+\left[\tilde{\beta}_{0}-\log\left( \frac{N_{1}}{N_{0}}\right)+\log\left(\frac{\pi(1)}{\pi(0)}\right)\right]\] \[= x^{\prime}\hat{\beta}\]

where

\[\hat{\beta}^{\prime}\equiv\left(\tilde{\beta}_{0}-\log\left(\frac{N_{1}}{N_{0} }\right)+\log\left(\frac{\pi(1)}{\pi(0)}\right),\;\tilde{\beta}_{*}^{\prime} \right).\]

This leads to

\[\hat{\pi}(1|y)=e^{x^{\prime}\hat{\beta}}\Big{/}\left[1+e^{x^{\prime}\hat{\beta }}\right].\]

Logistic discrimination defines a hyperplane of \(y\) values by \(x^{\prime}\hat{\beta}\equiv\hat{\beta}_{0}+y^{\prime}\hat{\beta}_{*}=0\) which corresponds to \(0.5=\hat{\pi}(1|y)\). If \(x^{\prime}\hat{\beta}>0\), \(\hat{\pi}(1|y)>0.5\). If \(x^{\prime}\hat{\beta}<0\), \(\hat{\pi}(1|y)<0.5\). The logistic regression model also defines a hyperplane of \(y\) values defined by \(0.5=\tilde{p}(x^{\prime}\tilde{\beta})\) which is equivalent to \(0=x^{\prime}\tilde{\beta}\) or \(-\tilde{\beta}_{0}=y^{\prime}\tilde{\beta}_{*}\). Because \(\hat{\beta}_{*}=\tilde{\beta}_{*}\), these hyperplanes are parallel in \(q\) dimensions, but unless the prevalences \(\pi(i)\) are proportional to the sample sizes \(N_{i}\), the regression and discrimination hyperplanes are distinct.

There are several ways of generalizing logistic discrimination to handle \(t>2\). Christensen (1997, 2015) focuses on the fact that logit/logistic models are actually log-linear models and that the appropriate log-linear model can easily be generalized to handle more than two populations. In particular, he illustrates for \(t=3\) how to turn estimated odds into allocations. Without probability estimates, SVMs often rely on performing all of the \({t\choose 2}\) binary discrimination problems and "voting" for a winning allocation. Voting could also be used when probability estimates exist, not that I would do that.

[MISSING_PAGE_FAIL:541]

In Fig. 13.3 the logistic and probit regression parabolas were almost on top of each other. The logistic discrimination parabolas in Fig. 13.7 should have the same shape as the logistic regression parabolas but move closer to the bilateral group. In fact, I cannot _see_ any difference between the logistic regression and logistic discrimination parabolas for these data. The QDA parabolas have a radically different shape than the logistic parabolas but, although they do not completely separate the two groups, they do not do a bad job. The SVM is unchanged and the curve that is visible on the plot has a different shape from both other methods. Figure 13.8 is the same as Fig. 13.7 except that it replaces the default SVM with the one from Fig. 13.5 that has a reduced tuning parameter (increased cost). 

The code on my website for constructing these figures also contains code for producing tables of estimated posterior probabilities for logistic discrimination that are similar to those presented in Chap. 12 and in Christensen (1997, 2015).

Figure 13.7: Quadratic model logistic discrimination, QDA, and an SVM: Cushingâ€™s syndrome data (subset)

## Bibliography

* Albert & Anderson (1984) Albert, A., & Anderson, J. A. (1984). On the existence of maximum likelihood estimates in logistic regression models. _Biometrika, 71_, 1-10.
* Aldrich (2005) Aldrich, J. (2005). Fisher and regression. _Statistical Science, 20_, 401-417.
* Anderson (1972) Anderson, J. A. (1972). Separate sample logistic discrimination. _Biometrika, 59_, 19-35.
* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer-Verlag.
* Christensen (2015) Christensen, R. (2015). _Analysis of variance, design, and regression: Linear modeling for unbalanced data_ (2nd ed.). Boca Raton: Chapman and Hall/CRC Press.
* Cramer (1946) Cramer, H. (1946). _Mathematical methods of statistics_. Princeton: Princeton University Press.
* Hastie et al. (2016) Hastie, T., Tibshirani, R., & Friedman, J. (2016). _The elements of statistical learning: Data mining, inference, and prediction_ (2nd ed.). New York: Springer.
* Moguerza & Munoz (2006) Moguerza, J. M., & Munoz, A. (2006). Support vector machines with applications. _Statistical Science, 21_, 322-336.
* Ripley (1996) Ripley, B. D. (1996). _Pattern recognition and neural networks_. Cambridge: Cambridge University Press.
* Riedl et al. (2016)

Figure 13.8: Quadratic model logistic discrimination, QDA, and an SVM with reduced tuning parameter: Cushingâ€™s syndrome data (subset)* [Williams1959] Williams, E. J. (1959). _Regression analysis_. New York: John Wiley and Sons.
* [Zhang, Liu, WuZhang et al.2013] Zhang, C., Liu, Y., & Wu, Z. (2013). On the effect and remedies of shrinkage on classification probability estimation. _The American Statistician_, _67_, 134-142.
* [Zhu2008] Zhu, M. (2008). Kernels and ensembles. _The American Statistician_, _62_, 97-109.

## Chapter Principal Components, Classical Multidimensional Scaling, and Factor Analysis

### Abstract

This chapter introduces the theory and application of principal components, classical multidimensional scaling, and factor analysis. Principal components seek to effectively summarize high dimensional data as lower dimensional scores. Multidimensional scaling gives a visual representation of points when all we know about the points are the distances separating them. Classical multidimensional scaling is seen to be an application of principal components when the distances are standard Euclidean distances. Principal components and factor analysis are often used for similar purposes but their theoretical background is quite different.

Suppose that observations are available on \(q\) variables. When \(q\) is quite large it can be very difficult to grasp the relationships among the many variables. It might be convenient if the variables could be reduced to a more manageable number. Clearly, it is easier to work with 4 or 5 variables than with, say, 25. (In the era of big data, perhaps I should be arguing that 400 or 500 variables are easier to work with than 2500.) Of course, one cannot reasonably expect to get a substantial reduction in dimensionality without some loss of information. We want to minimize that loss. Assuming that a reduction in dimensionality is desirable, how can it be performed efficiently? One reasonable method is to choose a small number of linear combinations of the variables based on their ability to reproduce the entire set of variables. In effect, we want to create a few new variables that are best able to predict the original variables. _Principal component analysis (PCA)_ finds linear combinations of the original variables that are best linear predictors of the full set of variables. This predictive approach to _dimensionality reduction_ seems intuitively reasonable. We emphasize this interpretation of principal component analysis rather than the traditional motivation of finding linear combinations that account for most of the variability in the data. The predictive approach is mentioned in Rao (1973, p. 591). Seber (1984) takes an approach that is essentially predictive. Seber's discussion is derived from Okamoto and Kanazawa (1968). Schervish (1986) gives an explicit derivation in terms of prediction. Other approaches, that are not restricted to linear combinations of the dependent variables, are discussed by Gnanadesikan (1977,Section 2.4) and Li and Chen (1985). Jolliffe (1986) gives a thorough discussion with many examples.

More recently, _independent component analysis (ICA)_ has become a popular method of data reduction. Hyvarinen, Karhunen, and Oja (2001) introduce the subject as both a generalization of principal components and as a generalization of factor analysis. (The latter seems more appropriate to me.) The R (package and) program fastICA begins by computing the principal components and obtains the "independent components" from them.

Principal components are similar in spirit to the linear discrimination coordinates discussed in Sect. 12.7. Principal components actually form a new coordinate system for \(\mathbf{R}^{q}\). These coordinates are defined sequentially so that they are mutually orthogonal in an appropriate inner product and have successively less ability to predict the original dependent variables. In practice, only the first few coordinates are used to represent the entire vector of dependent variables.

Section 14.1 presents several alternative derivations for theoretical principal components including both predictive and nonpredictive motivations. The predictive discussion depends heavily on concepts of best linear prediction which are reviewed in Appendix B. Section 14.2 examines the use of sample principal components. Section 14.3 introduces _classical multidimensional scaling (CMDS)_, which seeks to plot the locations of cases when one only knows the distances between the cases. The reason for examining CMDS here is their close relation to PCA. The final section examines _factor analysis_. Although many people consider principal component analysis a special case of factor analysis, in fact their theoretical bases are quite different.

### The Theory of Principal Components

In this section we give several derivations of principal components. First, principal components are derived as a sequence of orthogonal linear combinations of the variable vector \(y\). Each linear combination has maximum capability to predict the full set of variables subject to the condition that each combination is orthogonal to the previous linear combinations. In this sequence, orthogonality is defined using the inner product determined by \(\Sigma\), the covariance matrix of \(y\). Second, it is shown that the first \(r\) principal components have maximum capability to predict \(y\) among all sets of \(r\) linear combinations of \(y\). Thus, if \(r\) linear combinations of \(y\) are to be analyzed instead of the full vector \(y\), the first \(r\) principal components of \(y\) are linear combinations from which \(y\) can be most nearly reconstructed. The section closes with a discussion of alternate derivations of principal components and of principal components based on the correlation matrix.

#### Sequential Prediction

Let \(y=(y_{1},\ldots,y_{q})^{\prime}\) be a vector in \(\mathbf{R}^{q}\). We define new coordinates \(a_{1}^{\prime}y,a_{2}^{\prime}y,\ldots,a_{q}^{\prime}y\) having certain statistical properties. If we think of \(y\) as a random vector with 

[MISSING_PAGE_EMPTY:9853]

It suffices to find \(a_{1},\ldots,a_{q}\) such that

\[\frac{a_{1}^{\prime}\Sigma^{2}a_{1}}{a_{1}^{\prime}\Sigma a_{1}}=\sup_{a}\frac{a^{ \prime}\Sigma^{2}a}{a^{\prime}\Sigma a}\,,\]

and, for \(i=2,\ldots,q\),

\[a_{i}^{\prime}\Sigma a_{j}=0,\ j=1,\ldots,i-1\,,\]

with

\[\frac{a_{i}^{\prime}\Sigma^{2}a_{i}}{a_{i}^{\prime}\Sigma a_{i}}=\sup_{a}\left\{ \frac{a^{\prime}\Sigma^{2}a}{a^{\prime}\Sigma a}\bigg{|}a^{\prime}\Sigma a_{j}= 0;\ j=1,\ldots,i-1\right\}\,.\]

By taking \(H=\Sigma^{2}\) and \(E=\Sigma\), this is just a special case of the problem solved in Proposition 12.7.4. An optimal vector \(a_{i}\) is an eigenvector of \(E^{-1}H=\Sigma^{-1}\Sigma^{2}=\Sigma\) that corresponds to the \(i\)th largest eigenvalue \(\phi_{i}\) of \(\Sigma\).

To evaluate how well each coordinate predicts \(y\), observe that

\[\operatorname{tr}(V_{a_{i}}) = \operatorname{tr}(\Sigma)-a_{i}^{\prime}\Sigma^{2}a_{i}/a_{i}^{ \prime}\Sigma a_{i}\] \[= \sum_{j=1}^{q}\phi_{j}-\phi_{i}^{2}/\phi_{i}\] \[= \sum_{j=1}^{q}\phi_{j}-\phi_{i}\,.\]

Thus, because the \(\phi_{i}\)s are decreasing, each coordinate does no better at predicting \(y\) than the previous components.

#### Joint Prediction

It is also of interest to evaluate the overall predictive ability of a set of principal components, say \(a_{1}^{\prime}y,\ldots,a_{r}^{\prime}y\), that provide optimal sequential prediction. Using Propositions B.1.5 and B.1.9, a simple inductive proof yields

\[\operatorname{tr}\bigl{\{}\operatorname{Cov}[y-\hat{E}(y|a_{1}^{\prime}y, \ldots,a_{r}^{\prime}y)]\bigr{\}}=\sum_{j=r+1}^{q}\phi_{j}\,. \tag{14.1.2}\]

Together, the first \(r\) principal components do a good job of predicting \(y\) if the ratio

\[\frac{\operatorname{tr}\{\operatorname{Cov}[y-\hat{E}(y|a_{1}^{\prime}y, \ldots,a_{r}^{\prime}y)]\}}{\operatorname{tr}\{\operatorname{Cov}[y]\}}=\frac {\sum_{j=r+1}^{q}\phi_{j}}{\sum_{j=1}^{q}\phi_{j}}\]

is very small. This occurs if \(\phi_{r+1},\ldots,\phi_{q}\) are all very small relative to \(\sum_{j=1}^{q}\phi_{j}\). While the user must decide how much information can be sacrificed to the goal of dimensionality reduction, Johnson and Wichern (2007, Section 8.2) suggest that for many purposes the principal components form an effective substitute for the original variables when the ratio is 0.2 or less for large \(q\) and \(r=1\), 2, or 3.

##### Exercise 14.1.

Prove Eq. (14.1.2).

As a matter of fact, \(a^{\prime}_{1}y,\ldots,a^{\prime}_{r}y\) do as good or better at predicting \(y\) than any other \(r\) linear combinations of \(y\). Let \(B\) be a \(q\times r\) matrix of rank \(r\). Then, \(B^{\prime}y\) is a vector consisting of \(r\) linear combinations of \(y\). The best linear predictor of \(y\) based on \(B^{\prime}y\) has

\[\operatorname{Cov}[y-\hat{E}(y|B^{\prime}y)] = \Sigma-\Sigma B(B^{\prime}\Sigma B)^{-1}B^{\prime}\Sigma\] \[= \Sigma^{1/2}(I-\Sigma^{1/2}B(B^{\prime}\Sigma B)^{-1}B^{\prime} \Sigma^{1/2})\Sigma^{1/2}\] \[= \Sigma^{1/2}(I-M_{\Sigma^{1/2}B})\Sigma^{1/2},\]

where \(M_{\Sigma^{1/2}B}\) is the perpendicular projection operator onto the space \(C(\Sigma^{1/2}B)\) under the standard Euclidean inner product. It will be shown later (by the reader) that the prediction error satisfies

\[E\big{\{}\big{[}y-\hat{E}(y|B^{\prime}y)\big{]}^{\prime}\big{[}y -\hat{E}(y|B^{\prime}y)\big{]}\big{\}} = \operatorname{tr}\big{\{}\operatorname{Cov}\big{[}y-\hat{E}(y|B^ {\prime}y)\big{]}\big{\}}\] \[= \operatorname{tr}\big{\{}\Sigma^{1/2}(I-M_{\Sigma^{1/2}B})\Sigma^ {1/2}\big{\}}\] \[= \operatorname{tr}\{(I-M_{\Sigma^{1/2}B})\Sigma\}\] \[\geq \sum_{j=r+1}^{q}\phi_{j}\,. \tag{14.1.3}\]

By Eq. (14.1.2), the \(r\) linear combinations given by the principal components achieve the lower bound, so the first \(r\) principal components are not only sequentially optimal predictors but also jointly optimal predictors.

The inequality (14.1.3) can be established using the following lemma.

##### Lemma 14.2.1.

Let \(\phi_{1}\geq\cdots\geq\phi_{q}>0\) be the eigenvalues of \(\Sigma\). Let \(v_{1},\ldots,v_{r}\) be _any_ orthonormal vectors in \(\mathbf{R}^{q}\). Then,

1. \(\sum_{j=q-r+1}^{q}\phi_{j}\leq\sum_{j=1}^{r}v^{\prime}_{j}\Sigma v_{j}\leq \sum_{j=1}^{r}\phi_{j}\,.\)
2. If \(M\) is a perpendicular projection operator on \(\mathbf{R}^{q}\) (standard inner product) and \(r(M)=r\), then \[\sum_{j=q-r+1}^{q}\phi_{j}\leq\operatorname{tr}\{M\Sigma\}\leq\sum_{j=1}^{r} \phi_{j}\,.\]

##### Exercise 14.2.

Prove Lemma 14.2.1.

Hints: For (a), first consider the special case \(\Sigma\) diagonal and use some ideas from the proof of Proposition 12.7.4. Then,use the eigenvector-eigenvalue decomposition \(\Sigma=P\Lambda P^{\prime}\), where \(P^{\prime}P=I_{q}\) and \(\Lambda\) is diagonal. For (b), use the orthonormal basis decomposition for a perpendicular projection operator (i.e., \(M=OO^{\prime}\) with \(O^{\prime}O=I_{r}\)).

To see that (14.1.3) is a result of Lemma 14.2.1, note that \(I-M_{\Sigma^{1}/2B}\) is a perpendicular projection operator of rank \(q-r\) and that part (b) of the lemma leads immediately to (14.1.3).

In applications, the correlation between the \(i\)th principal component \(a^{\prime}_{i}y\) and the \(h\)th variable \(y_{h}\) is often cited. The correlations for \(h=1,\ldots,q\) are frequently used in trying to develop an interpretation for the \(i\)th principal component. The idea is to recognize some common characteristic(s) of the \(y_{h}\)s that have large correlations with \(a^{\prime}_{i}y\). The principal component is then interpreted as an underlying factor that measures this characteristic. This procedure is really a factor-analytic use of principal components and is open to criticisms similar to those made of factor analysis (see Sect. 14.4). In particular, such interpretations are not only subjective (subjectivity is unavoidable) but they are apparently unverifiable (a much more serious problem). Regardless of the appropriate use of these correlations, they have a simple mathematical form. Note that because \(a_{i}\) is an eigenvector of \(\Sigma\),

\[\mathrm{Cov}(y,a^{\prime}_{i}y)=\Sigma a_{i}=\phi_{i}a_{i}\,.\]

Thus, for the \(h\) component of \(y\),

\[\mathrm{Cov}(y_{h},a^{\prime}_{i}y)=\phi_{i}a_{ih},\]

where

\[a^{\prime}_{i}=(a_{i1},\ldots,a_{iq})\,.\]

In addition,

\[\mathrm{Var}(a^{\prime}_{i}y)=a^{\prime}_{i}\Sigma a_{i}=\phi_{i}a^{\prime}_{i }a_{i}\]

and

\[\mathrm{Var}(y_{h})=\sigma_{hh}\]

so

\[\mathrm{Corr}(y_{h},a^{\prime}_{i}y)=\phi_{i}a_{ih}/\sqrt{\sigma_{hh}\phi_{i} a^{\prime}_{i}a_{i}}\,.\]

Often, the vectors \(a_{i}\) are chosen so that \(a^{\prime}_{i}a_{i}=1\). This generates a simplification in the formula for the correlation.

#### Other Derivations of Principal Components

Although the methodology of principal components was first proposed by Pearson (1901), principal component analysis in its modern form was originated by Hotelling (1933). It is curious that, although Hotelling was originally interested in a prediction problem (actually a factor analysis problem), he transformed his prob lem into one of finding vectors \(a_{1},\ldots,a_{r}\) of fixed length such that \(a^{\prime}_{i}y\) has maximum variance subject to the condition that \(\operatorname{Cov}(a^{\prime}_{i}y,a^{\prime}_{j}y)=0\), \(j=1,\ldots,i-1\). This is the form in which principal component analysis is traditionally presented.

Apparently, it was intuitively clear to Hotelling that his linear combinations \(a^{\prime}_{i}y\) should have maximum predictive capability. In fact, the solution to Hotelling's problem is identical to the solution just given for the prediction problem. Unfortunately, that fact is not obvious to many of us who do not share Hotelling's keen insight into multivariate analysis. We now prove that this equivalence is true. To begin, we solve a slightly different problem and then show that the solution to Hotelling's problem and the alternative problem are the same.

Suppose we want to find vectors \(a_{1},\ldots,a_{q}\) that satisfy three properties: \(a^{\prime}_{i}a_{i}=K\) for all \(i\) and some constant \(K\), \(\operatorname{Var}(a^{\prime}_{1}y)\) is maximized, and given that \(a^{\prime}_{i}a_{j}=0\), \(j=1,\ldots,i-1\), \(\operatorname{Var}(a^{\prime}_{i}y)\) is maximized. The condition that \(a^{\prime}_{i}a_{i}=K\) is necessary because multiplying \(a_{i}\) by a constant changes the variance. If we allow different size vectors, we could never attain a maximum variance. Alternatively, we could allow different size vectors, but maximize the standardized variance. In other words, maximizing \(\operatorname{Var}(a^{\prime}y)\) for a fixed size vector is identical to maximizing \(\operatorname{Var}(a^{\prime}y)/a^{\prime}a=a^{\prime}\Sigma a/a^{\prime}a\). Using this equivalence, our problem is to find \(a_{1}\) such that

\[a^{\prime}_{1}\Sigma a_{1}/a^{\prime}_{1}a_{1}=\sup_{a}a^{\prime}\Sigma a/a^{ \prime}a\]

and for \(i=2,\ldots,q\) find \(a_{i}\) such that

\[\frac{a^{\prime}_{i}\Sigma a_{i}}{a^{\prime}_{i}a_{i}}=\sup_{a}\left\{\frac{a^ {\prime}\Sigma a}{a^{\prime}a}\bigg{|}a^{\prime}a_{j}=0;\ j=1,\ldots,i-1\right\}\]

and

\[a^{\prime}_{i}a_{j}=0;\ j=1,\ldots,i-1\,.\]

Using Proposition 12.7.4 with \(E=I\) and \(H=\Sigma\), just as in the sequential prediction problem, the vectors \(a_{i}\) are eigenvectors of \(\Sigma\) with respect to the eigenvalues \(\phi_{1}\geq\cdots\geq\phi_{q}\).

Hotelling's problem was slightly different. For \(i=2,\ldots,q\) and \(j=1,\ldots,i-1\), Hotelling wanted \(\operatorname{Cov}(a^{\prime}_{i}y,a^{\prime}_{j}y)=a^{\prime}_{i}\Sigma a_{j }=0\) instead of \(a^{\prime}_{i}a_{j}=0\). We want to show that the eigenvectors that solve our modified problem also solve Hotelling's problem. We can show this inductively. Clearly, \(a_{1}\) is the same for either problem. Now suppose the first \(i-1\) eigenvectors solve both problems. We need to show that if \(a_{i}\) solves our modified problem it will also solve Hotelling's problem. The key point is that for any vector \(a\) and \(j=1,\ldots,i-1\),

\[a^{\prime}\Sigma a_{j}=\phi_{j}a^{\prime}a_{j}\]

because \(a_{j}\) is an eigenvector of \(\Sigma\). It follows that with \(\Sigma\) positive definite,

\[a^{\prime}\Sigma a_{j}=0\quad\text{if and only if}\quad a^{\prime}a_{j}=0\,.\]Thus,

\[\left\{\frac{a^{\prime}\Sigma a}{a^{\prime}a}\bigg{|}a^{\prime}a_{j}=0;\ j=1, \ldots,i-1\right\}=\left\{\frac{a^{\prime}\Sigma a}{a^{\prime}a}\bigg{|}a^{ \prime}\Sigma a_{j}=0;\ j=1,\ldots,i-1\right\},\]

so any \(a_{i}\) that is a solution to our modified problem also satisfies

\[\frac{a_{i}^{\prime}\Sigma a_{i}}{a_{i}^{\prime}a_{i}}=\sup_{a}\left\{\frac{a^ {\prime}\Sigma a}{a^{\prime}a}\bigg{|}a^{\prime}\Sigma a_{j}=0;\ j=1,\ldots,i-1 \right\},\]

with

\[a_{i}^{\prime}\Sigma a_{j}=0;\ j=1,\ldots,i-1\,.\]

This establishes that eigenvectors of \(\Sigma\) provide solutions to Hotelling's problem just as they do for the modified problem and the prediction problems.

Finally, principal components can be related to ellipsoids. If \(y\sim N(\mu,\Sigma)\), the set of points that have constant likelihood (the same value of the density) fall on ellipsoids defined by \(\Sigma^{-1}\). Figure 14.1 illustrates a density isobar for a

\[N\left(\left[\begin{array}{cc}1\\ 2\end{array}\right],\left[\begin{array}{cc}1.0&0.9\\ 0.9&2.0\end{array}\right]\right)\]

random vector. The major and minor axes are denoted \(a_{1}\) and \(a_{2}\), respectively. We want to show that axes of the ellipse are determined by the eigenvectors of \(\Sigma\).

In general, an ellipsoid centered at zero defined by \(\Sigma^{-1}\) is

\[\{a|a^{\prime}\Sigma^{-1}a=c\},\]

where \(c\) is some constant. Let the vectors \(a_{1},\ldots,a_{q}\) denote the directions of the axes of the ellipsoid. The principal axis is the longest vector \(a\) on the ellipsoid. The subsequent axes are the longest vectors on the ellipsoid that are orthogonal (in the Euclidean inner product) to the previous axes. We want to show that these are determined by the eigenvectors of \(\Sigma\).

Any vector \(a\) can be made to fit on the ellipsoid by standardizing it, i.e., \((\sqrt{c}/\sqrt{a^{\prime}\Sigma^{-1}a})a\) is always on the ellipsoid because

\[(\sqrt{c}/\sqrt{a^{\prime}\Sigma^{-1}a})a^{\prime}\Sigma^{-1}(\sqrt{c}/\sqrt{a ^{\prime}\Sigma^{-1}a})a=c.\]

The principal axis is the longest vector of the form \((\sqrt{c}/\sqrt{a^{\prime}\Sigma^{-1}a})a\). Because \(c\) is a constant, a vector \(a_{1}\) is in the direction of the principal axis if and only if

\[\frac{a_{1}^{\prime}a_{1}}{a_{1}^{\prime}\Sigma^{-1}a_{1}}=\sup_{a}\frac{a^{ \prime}a}{a^{\prime}\Sigma^{-1}a}\,.\]Similarly, vectors in the directions of the other axes must satisfy

\[\frac{a_{i}^{\prime}a_{i}}{a_{i}^{\prime}\Sigma^{-1}a_{i}}=\sup_{a}\left\{\frac{a^ {\prime}a}{a^{\prime}\Sigma^{-1}a}\bigg{|}a^{\prime}a_{j}=0;\ j-1,\ldots,i-1\right\}\]

and

\[a_{i}^{\prime}a_{j}=0;\ j=1,\ldots,i-1\,.\]

Again, to show that the eigenvectors of \(\Sigma\) provide a solution to this problem, we solve a related problem in which we require

\[\frac{a_{i}^{\prime}a_{i}}{a_{i}^{\prime}\Sigma^{-1}a_{i}}=\sup_{a}\left\{\frac {a^{\prime}a}{a^{\prime}\Sigma^{-1}a}\bigg{|}a^{\prime}\Sigma^{-1}a_{j}=0;\ j=1,\ldots,i-1\right\}\]

and

\[a_{i}^{\prime}\Sigma^{-1}a_{j}=0\,.\]

The related problem is solved using Proposition 12.7.4 with \(E=\Sigma^{-1}\) and \(H=I\). The eigenvectors of \(E^{-1}H=\Sigma\) provide solutions. The equivalence of the solutions

Figure 14.1: Two-dimensional normal density isobar (\(\mu=(1,2)^{\prime}\), \(\sigma_{11}=1.0\), \(\sigma_{12}=0.9\), \(\sigma_{22}=2.0\)) with major and minor axes

to the ellipsoid problem and our related problem is based on the facts that \(\Sigma a=\phi a\) if and only if \(\frac{1}{\phi}a=\Sigma^{-1}a\) and that for an eigenvector \(a_{j}\) of \(\Sigma\)

\[a^{\prime}\Sigma^{-1}a_{j}=0\quad\text{if and only if}\quad a^{\prime}a_{j}=0\,.\]

Finally, the principal axis is actually \((\sqrt{c}/\sqrt{a_{1}^{\prime}\Sigma^{-1}a_{1}})a_{1}\) with the other axes, in order, being \((\sqrt{c}/\sqrt{a_{i}^{\prime}\Sigma^{-1}a_{i}})a_{i}\). Note that, not only are the original \(a_{i}\)s eigenvectors, but the axes are also eigenvectors of \(\Sigma\), just eigenvectors that have been given the appropriate length to be on the ellipsoid.

#### Principal Components Based on the Correlation Matrix

In our discussion of principal components, we have used minimization of the trace of the prediction error covariance matrix as a criterion for optimal prediction. This is precisely the sum of the prediction error variances for each component of \(y\), namely,

\[\sum_{h=1}^{q}\text{Var}\big{[}y_{h}-\hat{E}(y_{h}|a^{\prime}y)\big{]}\,.\]

If, for example, \(y\) is the length, width, and height of a randomly chosen dog house (or turtle shell) and all the measurements are taken in centimeters, this is probably a reasonable prediction error criterion. However, if the length and width are measured in centimeters and the height is measured in kilometers, the height numbers will all be small, so the height variances will be small, so getting good height predictions will be nearly irrelevant to the process of getting good overall predictions as determined by this criterion.

In general, if an individual variable, say \(y_{h}\), has a very small variance, then any linear combination \(a^{\prime}y\) generates a very small prediction error variance for \(y_{h}\) because the prediction error variance is never greater than the original variance. An optimal linear combination \(a^{\prime}y\) minimizes the sum of all the prediction variances, so prediction based on \(a^{\prime}y\) may lead to incongruities. Variables that happen to be measured on scales with small absolute variability are almost ignored in favor of predicting variables that are measured with large variability. Moreover, because the variance of any individual variable can be made arbitrarily large or small simply by multiplying the variable by a constant, the principal components are subject to the whims of measurement scale.

An obvious way to avoid this problem is to standardize the variance of the individual variables. Suppose \(\Sigma=[\sigma_{hh^{\prime}}]\), and let \(D=\text{Diag}(\sigma_{11},\ldots,\sigma_{qq})\); then, the variable \(z=D^{-1/2}y\) has

\[\text{Cov}(z)=D^{-1/2}\Sigma D^{-1/2},\]

which is the correlation matrix for \(y\). Because \(\text{Cov}(z)\) is a matrix that has 1's down the main diagonal, the principal components for predicting \(z=(z_{1},\ldots,z_{q})\) will give comparable weight to predicting each of the individual variables \(z_{h}\). Moreover, because \(z\) is a nonsingular linear transformation of \(y\), no information is lost in analyzing \(z\) in place of \(y\). The transformation of \(y\) to \(z\) is simply an implicit way of defining a new criterion for optimal prediction of \(y\). Schervish (1986) gives an explicit derivation of principal components based on the correlation matrix.

### 14.2 Sample Principal Components

In practice, the covariance matrix \(\Sigma\) is unknown, so the principal components cannot be computed. However, if a sample \(y_{1},\ldots,y_{n}\) of observations on \(y\) is available, sample principal components can be computed from either the sample covariance matrix

\[S=\sum_{i=1}^{n}(y_{i}-\bar{y}.)(y_{i}-\bar{y}.)^{\prime}/(n-1)\]

or the sample correlation matrix

\[R=D^{-1/2}SD^{-1/2},\]

where \(S=[s_{ij}]\) and \(D=\text{Diag}(s_{11},\ldots,s_{qq})\). Most often, the correlation matrix seems to be the appropriate choice.

It is convenient to use \(S\) or \(R\) to define an inner product and to choose \(a_{1},\ldots,a_{q}\) as an orthonormal set of eigenvectors corresponding to the eigenvalues \(\phi_{1}\geq\cdots\geq\phi_{q}\) of \(S\) or \(R\), respectively. Write \(A=[a_{1},\ldots,a_{q}]\) and, for \(r\leq q\), \(A_{r}=[a_{1},\ldots,a_{r}]\). A vector \(w\), rewritten in the principal component coordinate system, is \(A^{\prime}w\). Write the entire data set as

\[Y=\begin{bmatrix}y_{1}^{\prime}\\ \vdots\\ y_{n}^{\prime}\end{bmatrix}.\]

Using \(S\), the data in the principal component coordinate system are

\[YA.\]

If principal components are based on the correlation matrix \(R\), the rescaled data are

\[\begin{bmatrix}z_{1}^{\prime}\\ \vdots\\ z_{n}^{\prime}\end{bmatrix}=Z=YD^{-1/2}\]

and the data in the principal component coordinate system are The point of principal component analysis is to reduce dimensionality. If the smallest ordered eigenvalues of \(S\), \(\phi_{r+1},\ldots,\phi_{q}\) are small, a random vector \(y\) with covariance matrix \(S\) can be predicted well by \(A^{\prime}_{r}y\). If the entire data set is transformed in this way, a principal component observation matrix is obtained,

\[YA_{r}=[Ya_{1},\ldots,Ya_{r}], \tag{14.2.1}\]

where

\[Ya_{i}=\begin{bmatrix}a^{\prime}_{i}y_{1}\\ \vdots\\ a^{\prime}_{i}y_{n}\end{bmatrix}.\]

The elements of the vector \(Ya_{i}\) consist of the \(i\)th principal component applied to each of the \(n\) observation vectors. The principal component observation matrix combines these vectors for each of the first \(r\) principal components.

The analysis of the data can be performed on the principal component observations with a minimal loss of information. This includes various plots and formal statistical techniques for the analysis of a sample from one population.

#### The Sample Prediction Error

A question that arises immediately is just how much information is lost by using \(r\) principal components rather than the entire data set. Based on the substitutions \(\bar{y}.=\mu\) and \(S=\Sigma\), the total error of prediction from using the first \(r\) principal components is

\[\sum_{i=1}^{n}\big{[}y_{i}-\hat{E}(y|A^{\prime}_{r}y_{i})\big{]}^{\prime}\big{[} y_{i}-\hat{E}(y|A^{\prime}_{r}y_{i})\big{]},\]

where

\[\hat{E}(y|A^{\prime}_{r}y_{i})=\bar{y}.+SA_{r}(A^{\prime}_{r}SA_{r})^{-1}A^{ \prime}_{r}(y_{i}-\bar{y}.)\,.\]

The total error can be rewritten in terms of the eigenvalues of \(S\), \(\phi_{1}\geq\cdots\geq\phi_{q}\). Note that

\[\begin{array}{rl}y_{i}-\hat{E}(y|A^{\prime}_{r}y_{i})&=\,(y_{i}-\bar{y}.)- SA_{r}(A^{\prime}_{r}SA_{r})^{-1}A^{\prime}_{r}(y_{i}-\bar{y}.)\\ &=\,[I-SA_{r}(A^{\prime}_{r}SA_{r})^{-1}A^{\prime}_{r}](y_{i}-\bar{y}.)\,.\end{array}\]

Let \(\mathcal{A}_{r}\equiv A_{r}(A^{\prime}_{r}SA_{r})^{-1}A^{\prime}_{r}S\); this is an oblique projection operator. In particular, \(\mathcal{A}^{\prime}_{r}S=S\mathcal{A}_{r}\). Using this fact and basic results on traces, observe that

\[\begin{array}{l}\sum_{i=1}^{n}\big{[}y_{i}-\hat{E}(y|A^{\prime}_{r}y.)\big{]} ^{\prime}\big{[}y_{i}-\hat{E}(y|A^{\prime}_{r}y_{i})\big{]}\\ \quad=\,\text{tr}\left\{\sum_{i=1}^{n}\big{[}y_{i}-\hat{E}(y|A^{\prime}_{r}y_{i}) \big{]}\big{[}y_{i}-\hat{E}(y|A^{\prime}_{r}y_{i})\big{]}^{\prime}\right\}\\ \end{array}\]\[= \text{tr}\left\{\sum_{i=1}^{n}[I-\mathcal{A}_{r}^{\prime}](y_{i}- \bar{y}.)(y_{i}-\bar{y}.)^{\prime}[I-\mathcal{A}_{r}^{\prime}]^{\prime}\right\}\] \[= \text{tr}\left\{[I-\mathcal{A}_{r}^{\prime}]\left[\sum_{i=1}^{n}(y _{i}-\bar{y}.)(y_{i}-\bar{y}.)^{\prime}\right][I-\mathcal{A}_{r}]\right\}\] \[= \text{tr}\left\{[I-\mathcal{A}_{r}^{\prime}][(n-1)S][I-\mathcal{ A}_{r}]\right\}\] \[= \text{tr}\left\{(n-1)\left[S-SA_{r}(A_{r}^{\prime}SA_{r})^{-1}A_{ r}^{\prime}S\right]\right\}\] \[= (n-1)\text{tr}\left\{S-SA_{r}(A_{r}^{\prime}SA_{r})^{-1}A_{r}^{ \prime}S\right\}\,.\]

From the subsection on joint prediction,

\[\text{tr}\{S-SA_{r}(A_{r}^{\prime}SA_{r})^{-1}A_{r}^{\prime}S\}=\text{tr}\{ \text{Cov}[y-\hat{E}(y|A_{r}^{\prime}y)]\},\]

where \(\text{Cov}(y)=S\). By Eq. (14.2.2),

\[\sum_{i=1}^{n}\left[y_{i}-\hat{E}(y|A_{r}^{\prime}y_{i})\right]^{\prime}\left[ y_{i}-\hat{E}(y|A_{r}^{\prime}y_{i})\right]=(n-1)\sum_{j=r+1}^{q}\phi_{j}\,.\]

To evaluate the quality of prediction, the total error of prediction using \(r\) components can be compared to the maximum possible prediction error. The maximum possible prediction error can be viewed as using zero principal components. The maximum is

\[\sum_{i=1}^{n}(y_{i}-\bar{y}.)^{\prime}(y_{i}-\bar{y}.) = \text{tr}\left\{\sum_{i=1}^{n}(y_{i}-\bar{y}.)(y_{i}-\bar{y}.)^{ \prime}\right\}\] \[= (n-1)\text{tr}\{S\}\] \[= (n-1)\sum_{j=1}^{q}\phi_{j}\,.\]

The value

\[100\sum_{j=r+1}^{q}\phi_{j}\bigg{/}\sum_{j=1}^{q}\phi_{j}\]

is the percentage of the maximum prediction error left unexplained by \(\hat{E}(y_{i}|A_{r}^{\prime}y_{i})\), \(i=1,\ldots,n\). Alternatively,

\[100\sum_{j=1}^{r}\phi_{j}\bigg{/}\sum_{j=1}^{q}\phi_{j}\]

is the percentage of the maximum prediction error accounted for by \(A_{r}^{\prime}y\).

#### Using Principal Components

Principal components are designed to reduce dimensionality. They provide a number \(r<q\) of linear combinations \(a^{\prime}_{iY}\) that maximize the ability to linearly predict the original random \(q\)-vector \(y\). Thus they are appropriate to use when you are taking a random sample of \(y\)s.

Although the analysis of data can be performed on the principal component observations with a minimal loss of information, why accept any loss of information? Two possibilities come to mind. First, if \(q\) is very large, an analysis of all \(q\) variables may be untenable. If one must reduce the dimensionality before any work can proceed, principal components are a reasonable place to begin. However, it should be kept in mind that principal components are based on linear combinations of \(y\) and linear predictors of \(y\). If the important structure in the data is nonlinear, principal components can totally miss that structure.

A second reason for giving up information is when you do not trust all of the information. In prediction theory the underlying idea is that a vector \((y,x^{\prime})\) would be randomly sampled from some population and we would seek to predict \(y\) based on \(x\). Principal component regression (cf. _PA-V_, Chap. 13, or Christensen 2011, Chapter 15) can be used to reduce the dimensionality of \(x\). But _PA_ argues that using the principal components are an effective way to treat collinearity, even if you only have a sample from \(y|x\). The main idea was that, with errors in the model matrix, directions corresponding to small eigenvalues are untrustworthy. In the present context, we might say that any statistical relationships depending on linear combinations that do not provide substantial power of prediction are questionable.

As a general principle, to reduce the dimensionality of data successfully you need to know ahead of time that it can be reduced. Whether it can be reduced depends on the goal of the analysis. The work involved in figuring out whether data reduction can be accomplished often negates the value of doing it. The situation is similar to that associated with Simpson's paradox in contingency table analysis (see Christensen 1997, Section 3.1; Christensen 2014). Valid inferences cannot always be obtained from a collapsed contingency table. To know whether valid inferences can be obtained, one needs to analyze the full table first. Having analyzed the full table, there may be little point in collapsing to a smaller-dimensional table. Often, it would be convenient to reduce a data set using principal components and then do a MANOVA on the reduced data. Unfortunately, about the only way to find out if that approach is reasonable is to examine the results of a MANOVA on the entire data set.

Principal components are well designed for data reduction within a given population. If there are samples available from several populations with the same covariance matrix, then the optimal data reduction will be the same for every group and can be estimated using the pooled covariance matrix. Note that this essentially requires doing a one-way MANOVA prior to the principal component analysis. If an initial MANOVA is required, you may wonder why one would bother to reduce the data having already done a significant analysis on the unreduced set.

In particular, my friend Ed Bedrick has point out that if \(y\) is sampled from more than one population, reducing the dimensionality, without having first accounted for the different populations, can cause you to loose the ability to distinguish the populations. As illustrated in Fig. 14.2, with two normal populations having means \(\mu_{1}\) and \(\mu_{2}\), if the vector \(\mu_{1}-\mu_{2}\) is orthogonal to the eigenvectors that you are using to define your principal components, then there may be no information in your principal components capable of distinguishing the populations. In fact, even if \(\mu_{1}-\mu_{2}\) is merely close to \(C(a_{1},\ldots,a_{r})^{\perp}\) you may lose most of the information for distinguishing the populations. Basically, the only way to tell that this is not happening is to do the one-way MANOVA prior to doing the principal component analysis. Again, there may be no point in doing principal components after doing MANOVA. Jolliffe (1986, Section 9.1) discusses this problem in more detail.

**Exercise 14.3**.: Consider two \(q\)-vectors \(y_{j}\) with \(\operatorname{E}(y_{j})=\mu_{i}\) and \(\operatorname{Cov}(y_{j})=\Sigma\) and \(z\) independent of the \(y_{j}\)s with \(z\sim\operatorname{Bern}(p)\). Define the mixture random vector \(y\equiv zy_{1}+(1-z)y_{2}\). Assume that \(a_{1},\ldots,a_{q}\) are eigenvectors of \(\Sigma\) associated with eigenvalues \(\phi_{1}>\cdots>\phi_{q}>0\).

Figure 14.2: Two populations indistinguishable in the first principal component

1. Show that \(\operatorname{Cov}(y)=\Sigma+p(1-p)(\mu_{1}-\mu_{2})(\mu_{1}-\mu_{2})^{\prime}\).
2. Show that if \(\mu_{1}-\mu_{2}\) is an eigenvector for \(\phi_{k}\), then the \(a_{i}\)s are all eigenvectors of \(\operatorname{Cov}(y)\) with corresponding eigenvalues \(\phi_{i}\) for \(i\neq k\) and the eigenvalue \(\phi_{k}+p(1-p)\|\mu_{1}-\mu_{2}\|^{2}\) corresponding to \(a_{k}\).
3. Show that if \(\mu_{1}-\mu_{2}\) is proportional to \(a_{k}\) and \(\phi_{k}+p(1-p)\|\mu_{1}-\mu_{2}\|^{2}<\phi_{r}\), then the first \(r\) principal components of \(y_{i}\) agree with the first \(r\) principal components of \(y\) and that for \(i=1,\ldots,r\), \(\operatorname{E}(a_{i}^{\prime}y_{1})=\operatorname{E}(a_{i}^{\prime}y_{2})\).
4. With \(\mu_{1}-\mu_{2}\) proportional to \(a_{k}\), what happens to the first \(r\) principal components of \(y\) when \(\phi_{k}+p(1-p)\|\mu_{1}-\mu_{2}\|^{2}>\phi_{r}\)?
5. Show that if \((\mu_{1}-\mu_{2})\in C(a_{r+1},\ldots,a_{q})\) and \(\phi_{r-1}+p(1-p)\|\mu_{1}-\mu_{2}\|^{2}<\phi_{r}\), then the first \(r\) principal components of \(y_{i}\) agree with the first \(r\) principal components of \(y\).

Data reduction is also closely related to a more nebulous idea, the identification of underlying factors that determine the observed data. For example, the vector \(y\) may consist of a battery of tests on a variety of subjects. One may seek to explain scores on the entire set of tests using a few key factors such as general intelligence, quantitative reasoning, verbal reasoning, and so forth. It is common practice to examine the principal components and try to interpret them as measuring some sort of underlying factor. Such interpretations are based on examination of the relative sizes of the elements of \(a_{i}\). Although factor identification is commonly performed, it is, at least in some circles, quite controversial.

Example 14.2.1. One of the well-traveled data sets in multivariate analysis is from Jolicoeur and Mosimann (1960) on the shell (carapace) sizes of painted turtles. Aspects of these data have been examined by Morrison (2004) and Johnson and Wichern (2007). The data were given in Exercise 10.6.1 and Table 10.3. The analysis is based on \(10^{3/2}\) times the natural logs of the height, width, and length of the shells. Because all of the measurements are taken on a common scale, it may be reasonable to examine the sample covariance matrix rather than the sample correlation matrix. The point of this example is to illustrate the type of analysis commonly used in identifying factors. No claim is made that these procedures are reasonable.

For 24 males, the covariance matrix is

\[S=\begin{bmatrix}6.773&6.005&8.160\\ 6.005&6.417&8.019\\ 8.160&8.019&11.072\end{bmatrix}.\]

The eigenvalues and corresponding eigenvectors for \(S\) are as follows.

\[\begin{array}{cccc}\phi_{i}&23.303&0.598&0.360\\ \hline&a_{1}&a_{2}&a_{3}\\ 10^{3/2}\ln(\text{height})&0.523&0.788&-0.324\\ 10^{3/2}\ln(\text{width})&0.510&-0.594&-0.622\\ 10^{3/2}\ln(\text{length})&0.683&-0.159&0.713\end{array}\]Recall that eigenvectors are not uniquely defined. Eigenvectors of a matrix \(B\) corresponding to \(\phi\) (along with the zero vector) constitute the null space of \(B-\phi I\). Often, the null space has rank 1, in which case every eigenvector is a multiple of every other eigenvector. If we standardize the eigenvectors of \(S\) so that each has a maximum element of 1, we get the following eigenvectors.

\[\begin{array}{cccc}&&a_{1}&&a_{2}&&a_{3}\\ 10^{3/2}\ln(\text{height})&&0.764&&1&&-0.451\\ 10^{3/2}\ln(\text{width})&&0.747&&-0.748&&-0.876\\ 10^{3/2}\ln(\text{length})&&1&&-0.205&&1\\ \phi&&23.30&&0.60&&0.36\end{array}\]

The first principal component accounts for \(100(23.30)/(23.30+0.60+0.36)=96\%\) of the predictive capability (variance) of the variables. The first two components account for \(100(23.30+0.60)/(24.26)=98.5\%\) of the predictive capability (variance) of the variables. All the elements of \(a_{1}\) are positive and approximately equal, so \(a_{1}^{\prime}y\) can be interpreted as a measure of overall size. The elements of \(a_{2}\) are a large positive value for \(10^{3/2}\ln(\text{height})\), a large negative value for \(10^{3/2}\ln(\text{width})\), and a small value for \(10^{3/2}\ln(\text{length})\). The component \(a_{2}^{\prime}y\) can be interpreted as a comparison of the \(\ln(\text{height})\) and the \(\ln(\text{width})\). Finally, if one considers the value \(a_{31}=-0.451\) small relative to \(a_{32}=-0.876\) and \(a_{33}=1\), one can interpret \(a_{3}^{\prime}y\) as a comparison of width versus length.

Interpretations such as these necessarily involve rounding values to make them more interpretable. The interpretations just given are actually appropriate for the three linear combinations of \(y\), \(b_{1}^{\prime}y\), \(b_{2}^{\prime}y\), and \(b_{3}^{\prime}y\) that follow.

\[\begin{array}{cccc}&&b_{1}&&b_{2}&&b_{3}\\ 10^{3/2}\ln(\text{height})&&1&&1&&0\\ 10^{3/2}\ln(\text{width})&&1&&-1&&-1\\ 10^{3/2}\ln(\text{length})&&1&&0&&1\end{array}\]

The first interpreted component is

\[\begin{array}{ll}b_{1}^{\prime}y&=&10^{3/2}\ln\left[(\text{height})(\text{ width})(\text{length})\right]\\ &=&10^{3/2}\ln\left[\text{volume}\right],\end{array}\]

where the volume is that of a box. It is interesting to note that in this particular example, the first principal component can be interpreted without changing the coefficients of \(a_{1}\).

\[\begin{array}{ll}a_{1}^{\prime}y&=&10^{3/2}[0.764\ln\left(\text{height} \right)+0.747\ln\left(\text{width}\right)+\ln\left(\text{length}\right)]\\ &=&10^{3/2}\ln[(\text{height})^{0.764}(\text{width})^{0.747}(\text{length})] \,.\end{array}\]

The component \(a_{1}^{\prime}y\) can be thought of as measuring the log volume with adjustments made for the fact that painted turtle shells are somewhat curved and thus not a perfect box. Because the first principal component accounts for 96% of the predictive capability, to a very large extent, if you know this pseudovolume measurement, you know the height, length, and width.

In this example, we have sought to interpret the elements of the vectors \(a_{i}\). Alternatively, one could base interpretations on estimates of the correlations \(\text{Corr}(y_{h},a_{i}^{\prime}y)\) that were discussed in Sect. 14.2. The estimates of \(\text{Corr}(y_{h},a_{1}^{\prime}y)\) are very uniform, so they also suggest that \(a_{1}\) is an overall size factor. 

Linear combinations \(b_{i}^{\prime}y\) that are determined by the effort to interpret principal components will be called _interpreted components_. Although it does not seem to be common practice, it is interesting to examine how well interpreted components predict the original data and compare that to how well the corresponding principal components predict the original data. As long as the interpreted components are linearly independent, a full set of \(q\) components will predict the original data perfectly. _Any_ nonsingular transformation of \(y\) will predict \(y\) perfectly because it amounts to simply changing the coordinate system. If we restrict attention to \(r\) components, we know from the theoretical results on joint prediction that the interpreted components can predict no better than the actual principal components. In general, to evaluate the predictive capability of \(r\) interpreted components, write \(B_{r}=[b_{1},\ldots,b_{r}]\) and compute

\[\sum_{i=1}^{n}[y_{i}-\hat{E}(y|B_{r}^{\prime}y_{i})]^{\prime}[y_{i}-\hat{E}(y| B_{r}^{\prime}y_{i})]=(n-1)\text{tr}\{S-SB_{r}(B_{r}^{\prime}SB_{r})^{-1}B_{r}^{ \prime}S\}.\]

One hundred times this value divided by \((n-1)\sum_{i=1}^{q}\phi_{i}=(n-1)\text{tr}(S)\) gives the percentage of the predictive error unaccounted for by the \(r\) interpreted components. If this is not much greater than the corresponding percentage for the first \(r\) principal components, the interpretations are to some extent validated.

Using the first two interpreted components from Example 14.2.1,

\[\text{tr}[SB_{2}(B_{2}^{\prime}SB_{2})^{-1}B_{2}^{\prime}S]=23.88\]

and

\[\frac{100\text{tr}[S-SB_{2}(B_{2}^{\prime}SB_{2})^{-1}B_{2}^{ \prime}S]}{\text{tr}[S]} = \frac{100(24.26-23.88)}{24.26}\] \[= \frac{100(0.38)}{24.26}\] \[= 1.6.\]

Using the first two principal components,\[\frac{100\text{tr}[S-SA_{2}(A_{2}^{\prime}SA_{2})^{-1}A_{2}^{\prime}S]}{\text{tr}[S]} = \frac{100\sum_{j=1}^{3}\phi_{j}-\sum_{j=1}^{2}\phi_{j}}{\sum_{j=1}^{3}\phi_{j}}\] \[= \frac{100(0.36)}{24.26}\] \[= 1.5.\]

Thus, in this example, there is almost no loss of predictive capability by using the two interpreted components rather than the first two principal components. 

There is one aspect of principal component analysis that is often overlooked. It is possible that the most interesting components are those that have the least predictive power. Such components are taking on very similar values for all cases in the sample. It may be that these components can be used to characterize the population. Jolliffe (1986) has a fairly extensive discussion of uses for the _last few_ principal components.

Example 14.2.3.: The smallest eigenvalue of \(S\) is 0.36 and corresponds to the linear combination

\[a_{3}^{\prime}y=10^{3/2}\left[-0.451\ln\left(\text{height}\right)-0.876\ln \left(\text{width}\right)+\ln\left(\text{length}\right)\right].\]

This linear combination accounts for only 1.5% of the variability in the data. It is essentially a constant. All male painted turtles in the sample have about the same value for this combination. The linear combination is a comparison of the ln-length with the ln-width and ln-height. This might be considered as a measurement of the general shape of the carapace. One would certainly be very suspicious of any new data that were supposedly the shell dimensions of a male painted turtle but which had a substantially different value of \(a_{3}^{\prime}y\). On the other hand, this should not be thought of as a discrimination tool except in the sense of identifying whether data are or are not consistent with the male painted turtle data. We have no evidence that other species of turtles will produce substantially different values of \(a_{3}^{\prime}y\). 

### 14.3 Classical Multidimensional Scaling

Multidimensional Scaling starts with a matrix containing the squared distances between a set of objects and produces a plot of the objects that reflects those distances. There are a number of methods for doing this but we restrict our attention to _Classical Multidimensional Scaling (CMDS)_ because it reproduces the (mean corrected) sample principal component scores from only the squared distance matrix.

Consider a matrix \(\mathcal{D}\) that consists of the squared distances between \(n\) objects. To obtain an \(r\) dimensional graphical representation of the objects, find eigenvectors \(a_{1},\ldots,a_{r}\) of \([I-(1/n)JJ^{\prime}]\mathcal{D}[I-(1/n)JJ^{\prime}]\) corresponding to its \(r\) largest eigenvalues. Create the matrix

\[A_{r}\equiv[a_{1},\cdots,a_{r}]\equiv\begin{bmatrix}\mathbf{a}_{1}^{\prime}\\ \vdots\\ \mathbf{a}_{n}^{\prime}\end{bmatrix}\]

and in \(r\) dimensions, plot the \(n\) vectors \(\mathbf{a}_{i}\) to represent the \(n\) objects.

Example 14.3.1: I computed the distances between the 21 observations in the Cushing's syndrome data of Table 12.1 and applied CMDS to the squared distances. The result appears in Fig. 14.3. The plot is just a recentering and rotation of the data appearing in Fig. 12.1. (The data are rotated about 45\({}^{\circ}\) counterclockwise.) This occurs because, as we will show, the two-dimensional CMDS method is essentially just plotting the first two principal components of the data. Because the original data were two dimensional, the first two principal components contain all the information in the data, so we just get a recentered, rotated plot of the data. \(\Box\)

Figure 14.3: Classical multidimensional scaling: Cushing syndrome data

Starting with a data matrix

\[Y=\begin{bmatrix}y^{\prime}_{1}\\ \vdots\\ y^{\prime}_{n}\end{bmatrix}\]

that contains observations on \(n\) objects, we construct the squared Euclidian distance matrix \(\mathcal{D}\) for which the \(ij\) element \(d_{ij}\) is the squared distance between \(y_{i}\) and \(y_{j}\). In particular,

\[d_{ij}=(y_{i}-y_{j})^{\prime}(y_{i}-y_{j}).\]

We then establish that we can find the mean corrected sample principal components directly from \(\mathcal{D}\). CMDS consists of plotting the mean corrected sample principal components.

Multiplying out the squared distances gives

\[d_{ij}\equiv(y_{i}-y_{j})^{\prime}(y_{i}-y_{j})=y^{\prime}_{i}y_{i}+y^{\prime} _{j}y_{j}-2y^{\prime}_{i}y_{j}.\]

The squared distances are functions of the inner products and all of the inner products are given by

\[YY^{\prime}=\begin{bmatrix}y^{\prime}_{1}y_{1}&y^{\prime}_{1}y_{2}&\cdots&y^{ \prime}_{1}y_{n}\\ y^{\prime}_{2}y_{1}&y^{\prime}_{2}y_{2}&\cdots&y^{\prime}_{2}y_{n}\\ \vdots&\vdots&\ddots&\vdots\\ y^{\prime}_{n}y_{1}&y^{\prime}_{n}y_{2}&\cdots&y^{\prime}_{n}y_{n}\end{bmatrix}.\]

Create a vector consisting of the squared lengths of the vectors of observations,

\[\mathbf{d}\equiv(y^{\prime}_{1}y_{1},y^{\prime}_{2}y_{2},\cdots,y^{\prime}_{n }y_{n})^{\prime}.\]

It is not hard to see that the squared Euclidean distance matrix is

\[\mathcal{D}=\mathbf{d}J^{\prime}+J\mathbf{d}^{\prime}-2YY^{\prime}. \tag{14.3.1}\]

To relate this to principal components, we need to relate \(\mathcal{D}\) to the sample covariance matrix

\[S\equiv\frac{1}{n-1}Y^{\prime}[I-(1/n)JJ^{\prime}]Y=\frac{1}{n-1}\left\{[I-(1 /n)JJ^{\prime}]Y\right\}^{\prime}\left\{[I-(1/n)JJ^{\prime}]Y\right\}.\]

The mean of the row vectors \(y^{\prime}_{i}\) is

\[\vec{y}^{\prime}_{\cdot}=(1/n)J^{\prime}Y.\]

If we recenter the data, that does not change their orientation in space, it only shifts the data to being centered at \(0\). The centered data matrix is

\[Y-J\vec{y}^{\prime}_{\cdot}=[I-(1/n)JJ^{\prime}]Y.\]From (14.2.1) the principal component scores are \(YA_{r}\) where \(A_{r}\) has columns that are eigenvectors of \(S\) corresponding to the \(r\) largest eigenvalues. The mean corrected principal component scores are

\[(Y-J\bar{y^{\prime}_{\cdot}})A_{r}=[I-(1/n)JJ^{\prime}]YA_{r}.\]

The key mathematical fact in relating squared distances to principal components is that if \(\lambda\) and \(b\) are an eigenvalue and eigenvector for \(B^{\prime}B\), then \(\lambda\) and \(Bb\) are an eigenvalue and eigenvector for \(BB^{\prime}\). In particular, if \(\phi\) and \(a\) are an eigenvalue and eigenvector of \((n-1)S\), then \(\phi\) and

\[\left\{[I-(1/n)JJ^{\prime}]Y\right\}a\]

are an eigenvalue and eigenvector of

\[\left\{[I-(1/n)JJ^{\prime}]Y\right\}\left\{[I-(1/n)JJ^{\prime}]Y\right\}^{ \prime}=[I-(1/n)JJ^{\prime}]YY^{\prime}[I-(1/n)JJ^{\prime}].\]

The formula in (14.3.2) is just a mean corrected principal component.

We now show that (14.3.3) can be obtained directly from the squared distance matrix. Using (14.3.1) write

\[\begin{split}[I-(1/n)JJ^{\prime}]\mathcal{D}[I-(1/n)JJ^{\prime}] &=[I-(1/n)JJ^{\prime}]\left(\mathbf{d}J^{\prime}+J\mathbf{d}^{ \prime}-2YY^{\prime}\right)[I-(1/n)JJ^{\prime}]\\ &=[I-(1/n)JJ^{\prime}]\left(-2YY^{\prime}\right)[I-(1/n)JJ^{ \prime}]\end{split}\]

Thus

\[[I-(1/n)JJ^{\prime}]\left(YY^{\prime}\right)[I-(1/n)JJ^{\prime}]=\frac{-1}{2}[I -(1/n)JJ^{\prime}]\mathcal{D}[I-(1/n)JJ^{\prime}].\]

Since \(S\) and \((n-1)S\) have the same eigenvectors and the same ordering of the eigenvalues, we can compute the mean corrected principal components from the squared distance matrix.

While Example 14.3.1 is informative about what CMDS is doing, in practice multidimensional scaling is often used in situations where only a measure of distance between objects is available; not the raw data from which the distances were computed.

Example 14.3.2. Lawley and Maxwell (1971) and Johnson and Wichern (2007) examine data on the examination scores of 220 male students. The dependent variable vector consists of test scores on (Gaelic, English, history, arithmetic, algebra, geometry). The correlation matrix is We are going to treat the 6 tests as objects and use the correlation matrix as a measure of similarity between the objects. In particular, we used

\[\mathcal{D}=1-R\]

as our squared distance measure with results displayed in the top panel of Fig. 14.4. The bottom panel of Fig. 14.4 contains the CMDS representation when the distance is measured as 1 minus the squared correlation between the variables. (This does _not_ involve multiplying the matrix \(R\) times itself.) 

### 14.4 Factor Analysis

Principal components are often used in an attempt to identify factors underlying the observed data. There is also a formal modeling procedure called _factor analysis_ that is used to address this issue. The model looks similar to a multivariate linear model but several of the assumptions are changed and, most importantly, you don't get to see the matrix of predictor variables. It is assumed that the observation vectors

Figure 14.4: Classical multidimensional scaling: examination data

are uncorrelated and have \(\mathrm{E}(y_{i})=\mu\) and \(\mathrm{Cov}(y_{i})=\Sigma\). For \(n\) observations, the factor analysis model is

\[y_{i}^{\prime}=\mu^{\prime}+x_{i}^{\prime}B+\varepsilon_{i}^{\prime},\quad i=1, \ldots,n\]

or

\[Y=J\mu^{\prime}+XB+e, \tag{14.4.1}\]

where \(Y\) is \(n\times q\) and \(X\) is \(n\times r\). Most of the usual multivariate linear model assumptions about the rows of \(e\) are made,

\[\mathrm{E}(\varepsilon_{i}) = 0,\] \[\mathrm{Cov}(\varepsilon_{i},\varepsilon_{j}) = 0\qquad i\neq j\,,\]

and for \(\Psi\) nonnegative definite

\[\mathrm{Cov}(\varepsilon_{i})\equiv\Psi,\]

except \(\Psi\) is now assumed to be _diagonal_. The matrix \(B\) remains a fixed but unknown matrix of parameters. The entries in \(B\) are now called _factor loadings_.

The primary change in assumptions relates to \(X\) which is now random and _unobservable_. We have assumed that the mean of the observation vector on individual \(i\) is \(\mathrm{E}(y_{i}^{\prime})=\mu^{\prime}\), so the corresponding row \(x_{i}^{\prime}B\) better be random with mean zero. Each row of \(X\) is assumed to be an unobservable random vector with

\[\mathrm{E}(x_{i}) = 0\,,\] \[\mathrm{Cov}(x_{i},x_{j}) = 0\,,\quad i\neq j\,,\] \[\mathrm{Cov}(x_{i}) = I_{r}\,,\]

and

\[\mathrm{Cov}(x_{i},\varepsilon_{j})=0\,,\quad\text{any }i,j\,.\]

The idea behind the model is that the elements of \(X\) consist of \(r\) underlying _common factors_. For fixed individual \(i\), the different observations, \(y_{ih}\), \(h=1,\ldots,q\) are different linear combinations of the \(r\) (random) common factors for that individual \(x_{ik}\), \(k=1,\ldots,r\) plus some (white noise) error \(\varepsilon_{ih}\), \(h=1,\ldots,q\). The errors for the individual are allowed to have different variances but they are assumed to be uncorrelated.

Specifically, every individual \(i\) has observations \(y_{ih}\) that involve the same linear combinations (loadings) of the common factors, \(\sum_{k=1}^{r}\beta_{hk}x_{ik}\), where the \(\beta_{hk}\)s do not depend on the individual \(i\) but change depending on \(h\), so the linear combination depends on which variable \(y_{ih}\) is being considered for individual \(i\). Of course different individuals \(i\) have different realizations of the random common factors \(x_{ik}\). So, while all the common factors affect each column of \(Y\), different linear combinations of the common factors apply to different columns. The \(k\)th column of \(X\) consists of \(n\) realizations of the \(k\)th common factor. The row \(k\) of \(B\) therefore tells how the \(k\)th factor is incorporated into all of observations. It serves as the basis for trying to interpret what the \(k\)th factor contributes. The \(h\)th column of \(B\) are the coefficients that generate the \(h\)th dependent variable \(y_{ih}\).

For the model to be of interest, the number of factors \(r\) should be less than the number of variables \(q\). Based on this model, \(y_{i}=\mu+B^{\prime}x_{i}+\varepsilon_{i},\;i=1,\ldots,n\), so

\[\operatorname{Cov}(y_{i}) = \operatorname{Cov}(B^{\prime}x_{i}+\varepsilon_{i})\] \[= B^{\prime}B+\Psi\,.\]

In most of the discussion to follow, we will work directly with the matrix \(B^{\prime}B\). It is convenient to have a notation for this matrix. Write

\[\Lambda\equiv B^{\prime}B\,.\]

The matrix \(\Lambda\) is characterized by two properties: (1) \(\Lambda\) is nonnegative definite and (2) \(r(\Lambda)=r\). Recalling our initial assumption that \(\operatorname{Cov}(y_{i})=\Sigma\), the factor analysis model has imposed the restriction that

\[\Sigma=\Lambda+\Psi\,.\]

Clearly, one cannot have \(r(\Lambda)=r>q\). It is equally clear that if \(r=q\), one can always find matrices \(\Lambda\) and \(\Psi\) that satisfy (14.4.2). Just choose \(\Lambda=\Sigma\) and \(\Psi=0\). If \(r<q\), Eq. (14.4.2) may place a real restriction on \(\Sigma\), see Exercise 14.4b.

In practice, \(\Sigma\) is unknown and estimated by \(S\), so one seeks matrices \(\hat{B}\) and \(\hat{\Psi}\) such that

\[S\doteq\hat{B}^{\prime}\hat{B}+\hat{\Psi}\,.\]

The interesting questions now become (a) how many factors \(r\) are needed to get a good approximation and (b) which matrices \(\hat{B}\) and \(\hat{\Psi}\) give good approximations. The first question is certainly amenable to analysis. Clearly, \(r=q\) will always work, so there must be ways to decide when \(r<q\) is doing an adequate job. The second question ends up being tricky. The problem is that if \(U\) is any orthonormal matrix, \(U\hat{B}\) works just as well as \(\hat{B}\) because

\[\hat{B}^{\prime}\hat{B}=(U\hat{B})^{\prime}(U\hat{B})\,.\]

#### Additional Terminology and Applications

As may already be obvious, factor analysis uses quite a bit of unusual terminology.

The elements of a row of \(e\) are called _unique_ or _specific factors_. These are uncorrelated random variables that are added to the linear combinations of common factors to generate the observations. They are distinct random variables for each distinct observation (i.e., they are specific to the observation). The \(i\)th diagonal element of \(\Psi\equiv\operatorname{Cov}(\varepsilon_{i})\) is called the _uniqueness_, _specificity_, or _specific variance_ of the \(i\)th variable.

The diagonal elements of \(\Lambda\) are called _communalities_. Writing \(\Lambda=[\lambda_{ij}]\), the communality of the \(i\)th variable is generally denoted

\[h_{i}^{2}\equiv\lambda_{ii}\,.\]

Note that if \(B=[\beta_{ij}]\),

\[h_{i}^{2}=\sum_{k=1}^{r}\beta_{ki}^{2}\,.\]

The total variance is

\[\mbox{tr}[\Sigma]=\mbox{tr}[\Lambda]+\mbox{tr}[\Psi]\,.\]

The _total communality_ is

\[v\equiv\mbox{tr}[\Lambda]=\sum_{i=1}^{q}h_{i}^{2}=\sum_{i=1}^{q}\ \sum_{k=1}^{r }\beta_{ki}^{2}\,.\]

The matrix

\[\Lambda=\Sigma-\Psi=\mbox{Cov}(B^{\prime}x)\]

is called the _reduced covariance matrix_, for obvious reasons. Often the observations are standardized so that \(\Sigma\) is actually a correlation matrix. If this has been done, \(\Lambda\) is sometimes called the reduced correlation matrix (even though it need not be a correlation matrix).

In practice, factor analysis is used primarily to obtain estimates of \(B\). One then tries to interpret the estimated factor loadings in some way that makes sense relative to the subject matter of the data. As is discussed later, this is a fairly controversial procedure. One of the reasons for the controversy is that \(B\) is not uniquely defined. Given any orthonormal \(r\times r\) matrix \(U\), write \(X_{0}=XU^{\prime}\) and \(B_{0}=UB\); then,

\[XB=XU^{\prime}UB=X_{0}B_{0},\]

where \(X_{0}\) again satisfies the assumptions made about \(X\). Unlike standard linear models, \(X\) is not observed, so there is no way to tell \(X\) and \(X_{0}\) apart. There is also no way to tell \(B\) and \(B_{0}\) apart. Actually, this indeterminacy is used in factor analysis to increase the interpretability of \(B\). This will be discussed again later. At the moment, we examine ways in which the matrix \(B\) is interpreted.

One of the key points in interpreting \(B\) is recognizing that it is the rows of \(B\) that are important and not the columns. A column of \(B\) is used to explain one dependent variable. A row of \(B\) consists of all of the coefficients that affect a single common factor. The \(q\) elements in the \(j\)th row of \(B\) represent the contributions made by the \(j\)th common factor to the \(q\) dependent variables. Traditionally, if a factor has all of its large loadings with the same sign, the subject matter specialist tries to identify some common attribute of the dependent variables that correspond to the high loadings. This common attribute is then considered to be the underlying factor. A _bipolar_ factor involves high loadings that are both positive and negative; the user identifiescommon attributes for both the group of dependent variables with positive signs and the group with negative signs. The underlying factor is taken to be one that causes individuals who are high on some scores to be low on other scores. The following example involves estimated factor loadings. Estimation is discussed in the following two subsections.

##### Again consider the correlation matrix

\[R = \left[ {\matrix{1.000 &0.439 &0.410 &0.288 &0.329 &0.248 \cr 0.439 &1.000 &0.351 &0.354 &0.320 &0.329 \cr 0.410 &0.351 &1.000 &0.164 &0.190 &0.181 \cr 0.288 &0.354 &0.164 &1.000 &0.595 &0.470 \cr 0.329 &0.320 &0.190 &0.595 &1.000 &0.464 \cr 0.248 &0.329 &0.181 &0.470 &0.464 &1.000 \cr} \right]\]

from Lawley and Maxwell (1971) and Johnson and Wichern (2007) that was obtained from (Gaelic, English, history, arithmetic, algebra, geometry) examination scores on 220 male students.

For \(r=2\), maximum likelihood estimation gives one choice of estimates,

\[\hat{B} = \left[ {\matrix{0.553 &0.568 &0.392 &0.740 &0.724 &0.595 \cr 0.429 &0.288 &0.450 &-0.273 &-0.211 &-0.132 \cr}} \right]\]

and

\[(\hat{\psi}_{1},\ldots,\hat{\psi}_{6}) = \left( {0.510,0.594,0.644,0.377,0.431,0.628} \right).\]

Factor interpretation involves looking at the rows of \(\hat{B}\) and trying to interpret them. Write

\[\hat{B} = \left[ {\matrix{\hat{b}_{1}^{\prime} \cr\hat{b}_{2}^{\prime} \cr}} \right].\]

All of the elements of \(\hat{b}_{1}\) are large and fairly substantial. This suggests that the first factor is a factor that indicates general intelligence. The second factor is bipolar, with positive scores on math subjects and negative scores on nonmath subjects. The second factor might be classified as some sort of math-nonmath factor. This example will be examined again later with a slightly different slant.

Rather than taking the factor analysis model as a serious model for the behavior of data, it may be more appropriate to view factor analysis as a data analytic procedure that seeks to discover structure in the covariance matrix and may _suggest_ the presence of underlying factors. My son Fletcher (not to be confused with my imaginary son Basil) has convinced me that if you have a previous idea of the important factors it may be a worthwhile exercise to see whether the data are capable of being contorted into consistency with those previous factors.

#### Maximum Likelihood Theory

Maximum likelihood theory can be used for both estimation and testing. Maximum likelihood factor analysis is based on assuming that the random vectors in the factor model have a joint multivariate normal distribution and rewriting the factor analysis model as a standard multivariate linear model. (By contrast, ICA allows at most one of the factors to have a normal distribution.) To do this, the random terms are pooled together as, say

\[\xi_{i}=B^{\prime}x_{i}+\varepsilon_{i}\]

and

\[\xi=XB+e\,.\]

With \(\Lambda=B^{\prime}B\), the factor analysis model is a special case of the one-sample model of Sect. 10.1,

\[Y=J\mu^{\prime}+\xi\,, \tag{14.4.3}\]

where

\[E(\xi_{i}) = 0\,,\] \[\text{Cov}(\xi_{i},\xi_{j}) = 0\qquad i\neq j\,,\]

and

\[\text{Cov}(\xi_{i})=\Lambda+\Psi,\]

with \(\Psi\) diagonal, \(\Lambda\) nonnegative definite, and \(r(\Lambda)=r\).

In Sect. 10.1, the assumption was simply that

\[\text{Cov}(\xi_{i})=\Sigma\,.\]

The new model places the restriction on \(\Sigma\) that

\[\Sigma=\Lambda+\Psi, \tag{14.4.4}\]

where \(r(\Lambda)=r\) and \(\Psi\) is diagonal. For \(\xi_{i}\)s with a joint multivariate normal distribution, the likelihood function for an arbitrary \(\Sigma\) was discussed in Chap. 9. Clearly, the likelihood can be maximized subject to the restrictions that \(\Sigma=\Lambda+\Psi\), \(\Lambda\) is non-negative definite, \(r(\Lambda)=r\), and \(\Psi\) is diagonal. However Seber (1984, Exercise 5.4) argues that even these parameters are not identifiable without additional restrictions.

Because \(\Lambda+\Psi\) is just a particular choice of \(\Sigma\), as in Chap. 9 the maximum likelihood estimate of \(\mu\) is always the least squares estimate, \(\hat{\mu}=\bar{y}\). This simplifies the maximization problem. Unfortunately, with the additional restrictions on \(\Sigma\), closed-form estimates of the covariance matrix are no longer available. Computational methods for finding MLEs are discussed in Lawley and Maxwell (1971) and Joreskog (1975). They can be quite difficult.

**Exercise 14.4**.:
1. Show that the maximization problem reduces to finding a rank \(r\) matrix \(\hat{\Lambda}\) and a diagonal matrix \(\hat{\Psi}\) that minimize \[\log(|\Lambda+\Psi|)+\operatorname{tr}\{(\Lambda+\Psi)^{-1}\hat{\Sigma}_{q}\},\] where \(\hat{\Sigma}_{q}=\frac{n-1}{n}S\) and \(\hat{\Lambda}\) is nonnegative definite.
2. Show that any positive definite \(\Sigma\) can be written with a factor analysis structure having \(r=q-1\). Hint: Look at \(\Sigma=PD(\phi_{i})P^{\prime}=P\left[D(\phi_{i})-\phi_{q}I+\phi_{q}I\right]P^{\prime}\)

One advantage of the maximum likelihood method is that standard asymptotic results apply. Maximum likelihood estimates are asymptotically normal. Minus two times the likelihood ratio test statistic is asymptotically chi-squared under the null hypothesis. See Geweke and Singleton (1980) for a discussion of sample size requirements for the asymptotic test.

Of specific interest are tests for examining the rank of \(\Lambda\). If \(r<s\), the restriction \(r(\Lambda)=r\) is more stringent than the restriction \(r(\Lambda)=s\). To test \(H_{0}:r(\Lambda)=r\) versus \(H_{\Lambda}:r(\Lambda)=s\), one can use the likelihood ratio test statistic. This is just the maximum value of the likelihood under \(r(\Lambda)=r\) divided by the maximum value of the likelihood under \(r(\Lambda)=s\). Under \(H_{0}\), \(-2\) times the log of this ratio has an asymptotic chi-squared distribution. The degrees of freedom are the difference in the number of independent parameters for the models with \(r(\Lambda)=s\) and \(r(\Lambda)=r\). If we denote

\[\hat{\Sigma}_{r}=\hat{\Lambda}+\hat{\Psi}\]

when \(r(\Lambda)=r\) with a similar notation for \(r(\Lambda)=s\), \(-2\) times the log of the likelihood ratio test statistic is easily shown to be

\[n\left[\ln\left(\frac{|\hat{\Sigma}_{r}|}{|\hat{\Sigma}_{s}|}\right)+ \operatorname{tr}\{(\hat{\Sigma}_{r}^{-1}-\hat{\Sigma}_{s}^{-1})\hat{\Sigma}_ {q}\}\right],\]

where again

\[\hat{\Sigma}_{q}=\frac{n-1}{n}S.\]

As will be seen later, the degrees of freedom for the test are usually

if \(s=q,q-1\)\(df=q(q+1)/2-q-[qr-r(r-1)/2]\),

if \(s<q-1\)\(df=[qs-s(s-1)/2]-[qr-r(r-1)/2]\).

The formula for degrees of freedom is derived from the number of independent parameters in each model. If \(r(\Lambda)\equiv r=q,q-1\), the covariance matrix \(\Sigma\) is unrestricted. The independent parameters are the \(q\) elements of \(\mu\) and the \(q(q+1)/2\) distinct elements of \(\Sigma\). Recall that because \(\Sigma\) is symmetric, not all of its elements are distinct. Thus, for \(r=q,q-1\), the model has

\[q+q(q+1)/2\]

degrees of freedom.

Counting the degrees of freedom when \(r(\Lambda)=r<q-1\) is a bit more complicated. The model involves the restriction

\[\Sigma=\Lambda+\Psi,\]

where \(\Psi\) is diagonal and \(\Lambda\) is nonnegative definite with rank \(r\). Clearly, \(\Psi\) has \(q\) independent parameters, the diagonal elements. Because \(\Lambda\) is of rank \(r\), then \(q-r\) columns out of the \(q\times q\) matrix are linear combinations of the other \(r\) columns. Thus, the independent parameters are at most the elements of these \(r\) columns. There are \(qr\) of these parameters. However, \(\Lambda\) is also symmetric. All of the parameters above the diagonal are redundant. In the first \(r\) columns there are \(1+2+\cdots+(r-1)=r(r-1)/2\) of these redundant values. Thus, \(\Lambda\) has at most \(qr-r(r-1)/2\) parameters. Finally, \(\mu\) again involves \(q\) independent parameters. Adding the number of independent parameters in \(\mu\), \(\Psi\), and \(\Lambda\) gives the maximum model degrees of freedom as

\[q+q+[qr-r(r-1)/2]\,.\]

Taking differences in model degrees of freedom gives the test degrees of freedom indicated earlier. However, if \(r\) and \(q\) are both large, the number of factor model parameters can exceed the number of parameters for the unrestricted covariance matrix model, so an unrestricted covariance matrix should be used. In particular, if \(r=q-1\), the number of factor model parameters always exceeds the number of parameters in the unrestricted covariance matrix. Fortunately, \(r\) is usually taken to be small.

Thus far in the discussion, we have ignored \(B\) in favor of \(\Lambda=B^{\prime}B\). Given a function of \(\Lambda\), say \(B=f(\Lambda)\), and the maximum likelihood estimate \(\hat{\Lambda}\), the MLE of \(B\) is \(\hat{B}=f(\hat{\Lambda})\). The problem is in defining the function \(f\). There are an uncountably infinite number of ways to define \(f\). If \(f\) defines \(B\) and \(U\) is an orthonormal matrix, then

\[f_{1}(\Lambda)=Uf(\Lambda)\]

is just as good a definition of \(B\) because \(\Lambda=B^{\prime}B=B^{\prime}U^{\prime}UB\). As mentioned, this indeterminacy is used to make the results more interpretable. The matrix \(B\) is redefined until the user gets a pleasing \(\hat{B}\). The procedure starts with any \(\hat{B}\) and then \(\hat{B}\) is rotated (multiplied by an orthonormal matrix) until \(\hat{B}\) seems to be interpretable to the user. In fact, there are some standard rotations (e.g., varimax and quartimax), that are often used to increase interpretability. For a more complete discussion of rotations see Williams (1979).

Often, in an effort to make \(B\) well defined, it is taken to be \(D\left(\sqrt{\phi_{1}},\ldots,\sqrt{\phi_{r}}\right)A^{\prime}_{r}\), where \(A_{r}=[a_{1},\ldots,a_{r}]\) with \(a_{i}\) an eigenvector of \(\Lambda\) with length one corresponding to a positive eigenvalue \(\phi_{i}\). To accomplish the goal one would need to address the issues of eigenvalues not being unique and the nonidentifiability of \(\Lambda\).

Example 14.4.2. : For \(r=2\), the orthonormal matrices \(U\) used in rotations are \(2\times 2\) matrices. Thus, the effects of orthogonal rotations can be plotted. The plots consist of \(q\) points, one for each dependent variable. Each point consists of the two values in each column of \(\hat{B}\). Figure 14.5 gives a plot of the unrotated factor loadings presented

[MISSING_PAGE_FAIL:575]

The factor analysis model for maximum likelihood assumes that the matrix of common factors \(X\) has rows consisting of independent observations from a multivariate normal distribution with mean zero and covariance matrix \(I_{r}\). While \(X\) is not observable, it is possible to predict the rows of \(X\). In Exercise 14.5, it will be seen that \(\hat{E}(x_{i}|Y)=B(\Lambda+\Psi)^{-1}(y_{i}-\mu)\). Thus, estimated best linear predictors of the \(x_{i}\)s can be obtained. These _factor scores_ are frequently used to check the assumption of multivariate normality. Bivariate plots can be examined for elliptical shapes and outliers. Univariate plots can be checked for normality.

#### Principal Factor Estimation

It would be nice to have a method for estimating the parameters of model (14.4.3) that did not depend on the assumption of normality. Thurstone (1931) and Thompson (1934) have proposed _principal (axes) factor estimation_ as such a method. The parameters to be estimated are \(\mu\), \(\Lambda\), and \(\Psi\). As mentioned earlier, model (14.4.3) is just a standard multivariate linear model with a peculiar choice for \(\Sigma\). The results of Sect. 9.2 imply that \(\bar{y}\). is the best linear unbiased estimate of \(\mu\).

Figure 14.6: Varimax factor loadings

It remains to estimate \(\Lambda\) and \(\Psi\). If \(\Psi\) is known, estimation of \(\Lambda\) is easy. Using Eq. (14.4.4)

\[\Lambda=\Sigma-\Psi,\]

where \(\Lambda\) is assumed to be nonnegative definite of rank \(r\). If it were not for the rank condition, a natural estimate would be

\[\tilde{\Lambda}\equiv S-\Psi\,.\]

Incorporating the rank condition, one natural way to proceed is to choose a nonnegative definite matrix of rank \(r\), say \(\hat{\Lambda}\), that minimizes, say,

\[\mathop{\rm tr}\nolimits\{(S-\Psi)-\Lambda\}.\]

Although other functions of \((S-\Psi)-\Lambda\) might be reasonable, the trace is a convenient choice because we have already solved a version of this problem.

Let \(\phi_{1}\geq\cdots\geq\phi_{q}\) be the eigenvalues of \(S\), let \(a_{1},\ldots,a_{q}\) be the corresponding eigenvectors, let \(A_{r}=[a_{1},\ldots,a_{r}]\), and let \(G\) be a \(q\times r\) matrix of rank \(r\). In our discussion of principal components, we established that

\[\mathop{\rm tr}\nolimits[S-SA_{r}(A_{r}^{\prime}SA_{r})^{-1}A_{r}^{\prime}S]= \min_{G}\mathop{\rm tr}\nolimits[S-SG(G^{\prime}SG)^{-1}G^{\prime}S]\,.\]

Clearly, \(SG(G^{\prime}SG)^{-1}G^{\prime}S\) is nonnegative definite of rank \(r\). If we consider the problem of estimating \(\Sigma\) when \(r(\Sigma)=r\) and restrict ourselves to the class of estimates \(SG(G^{\prime}SG)^{-1}G^{\prime}S\), then the matrix \(SA_{r}(A_{r}^{\prime}SA_{r})^{-1}A_{r}^{\prime}S\) is an optimal rank \(r\) estimate of \(S\).

Applying this result in the factor analysis problem gives an optimal estimate

\[\hat{\Lambda}=\tilde{\Lambda}A_{r}(A_{r}^{\prime}\tilde{\Lambda}A_{r})^{-1}A_{ r}^{\prime}\tilde{\Lambda},\]

where \(A_{r}\) consists of eigenvectors of \(\tilde{\Lambda}=S-\Psi\). If we choose the eigenvectors so that \(A_{r}^{\prime}A_{r}=I_{r}\), \(\hat{\Lambda}\) simplifies to

\[\hat{\Lambda}=A_{r}D(\phi_{1},\ldots,\phi_{r})A_{r}^{\prime},\]

where \(\phi_{1},\ldots,\phi_{r}\) are the \(r\) largest eigenvalues of \(\tilde{\Lambda}\). An obvious estimate of \(B\) is

\[\hat{B}=D(\sqrt{\phi_{1}},\ldots,\sqrt{\phi_{r}})A_{r}^{\prime}.\]

Of course, any rotation of \(\hat{B}\) is an equally appropriate estimate.

All of this assumes that \(\Psi\) is known. In practice, one makes an initial guess \(\Psi_{0}\) that leads to initial estimates \(\tilde{\Lambda}_{0}=S-\Psi_{0}\) and \(\hat{\Lambda}_{0}\). Having computed \(\hat{\Lambda}_{0}\), compute \(\Psi_{1}\) from the diagonal elements of \(S-\hat{\Lambda}_{0}\) and repeat the process to obtain \(\hat{\Lambda}_{1}\). This iterative procedure can be repeated until convergence. A common choice for \(\Psi_{0}=D(\psi_{0})\) is

\[\psi_{i0}=1/s^{ii},\]

where \(s^{ii}\) is the \(i\)th diagonal element of \(S^{-1}\).

Another common choice for \(\Psi_{0}\) is taking \(\psi_{i0}=0\) for all \(i\). This choice yields \(\tilde{\Lambda}=S\), and the rows of \(\hat{B}=D(\sqrt{\phi_{i}})A_{r}^{\prime}\) are eigenvectors of \(S\). These are the same vectors as used to determine principal components. In fact, principal components are often used to address questions about underlying factors. The difference is that in a principal component analysis the elements of the eigenvector determine a linear combination of the dependent variable \(y\). In the factor analysis model, the elements of an eigenvector, say \(a_{1}\), are the \(q\) coefficients applied to the first hypothetical factor. Although factor interpretations are based on these \(q\) values, in the factor analysis model, data are generated using the \(r\) values \(a_{1h},\ldots,a_{rh}\) taken _across_ eigenvectors.

Some of the problems with principal factor estimation are that \(r\) is assumed to be known, there are no tests available for the value of \(r\), and the matrix \(S-\Psi\) may not be nonnegative definite.

In our examination of principal components, we found that the eigenvectors of \(\Sigma\) provided solutions to several different problems: sequential prediction, joint prediction, sequential variance maximization, and geometrical interpretation. The principal factor estimation method can also be motivated by a sequential optimization problem, see Gnanadesikan (1977).

#### Computing

For the previous edition, I think I did this analysis in an old version of Minitab. I repeated the analysis in Minitab 18, in R's factanal and in the R library psych's program fa. R's factanal fits using only maximum likelihood, Minitab allows both methods, and fa allows these two and several more. Between these various programs, the third digit of the loadings often differed by 1.

As for rotations, R's factanal allows none and varimax as well as promax which is a transformation but not a rotation. Minitab allows none, varimax, quartimax, equimax, and a family of rotations called orthomax. Psych's fa allows none, varimax, quartimax, equamax, and four others as well as promax and six more transformations that are not rotations.

#### Discussion

There is no question that model (14.4.3) is a reasonable model. There is considerable controversy about whether the factor analysis model (14.4.1) has any meaning beyond that of model (14.4.3).

Factor analysis is a frequently used methodology. Obviously, its users like it. Users like to rotate the estimated factor loadings \(\tilde{B}\) and interpret their results. On the other hand, many people, often of a more theoretical bent, are deeply disturbed by the indeterminacy of the factors and the factor loadings. Many people claim it is impossible to understand the nature of the underlying factors and the basis of their interpretation. Personally, I have always tried to straddle this particular fence. There are people on both sides that I respect. (OK, as I have gotten older, I've fallen on the "disturbed by the indeterminacy" side of the fence.)

An important criterion for evaluating models is that if a model is useful it should be useful for making predictions about future observables. The maximum likelihood model (14.4.3), like all linear models, satisfies this criterion. The prediction of a new case would be \(\bar{y}\). The peculiar covariance matrix of model (14.4.3) plays a key role in predicting the unobserved elements of a new case when some of the elements have been observed.

The factor analysis model (14.4.1) looks like it is more than the corresponding linear model. The interpretation of factor loadings depends on (14.4.1) being more than the linear model. If the factor analysis model really is more than the linear model, it should provide predictions that are distinct from the linear model. When the factor analysis model is correct, these predictions should be better than the linear model predictions.

Unfortunately, the factor analysis model does not seem to lend itself to prediction except through the corresponding linear model. One can predict the factor vectors \(x_{i}\) (assuming that \(\mu\), \(B\), and \(\Psi\) are known), but this does not affect prediction of \(y_{i}\).

##### Exercise 14.5.

Show that

(a) \(\hat{E}(x_{i}|Y)=\hat{E}(x_{i}|y_{i})=B(\Lambda+\Psi)^{-1}(y_{i}-\mu)\).

(b) \(\hat{E}(\mu+B^{\prime}x_{i}+\varepsilon_{i}|y_{i})=y_{i}\).

Do not use the fact that \(\hat{E}(y_{i}|y_{i})=y_{i}\).

Though the factor analysis model may not hold up to careful scrutiny, it does not follow that the data-analytic method known as factor analysis is a worthless endeavor. Rather than thinking of factor analysis as a theoretical method of estimating the loadings on some unspecified factors, it may be better to think of it as a data-analytic method for identifying structure in the covariance matrix. As a data-analytic method, it is neither surprising nor disconcerting that different people (using different rotations) obtain different results. It is more important whether, in practice, users working on similar problems often obtain similar results.

The factor analysis model is one motivation for this method of data analysis. We now will present a slightly different view. We begin by decomposing the covariance matrix into the sum of \(r\) different covariance matrices plus \(\Psi\). In other words, write

\[B=\begin{bmatrix}b_{1}^{\prime}\\ \vdots\\ b_{r}^{\prime}\end{bmatrix}\]

and

\[\Lambda_{i}=b_{i}b_{i}^{\prime}\]Thus,

\[\begin{split}\Sigma&=\Lambda+\Psi\\ &=B^{\prime}B+\Psi\\ &=\sum_{i=1}^{r}b_{i}b_{i}^{\prime}+\Psi\\ &=\sum_{i=1}^{r}\Lambda_{i}+\Psi\,.\end{split}\]

We can think of \(y\) as being a random observation vector and \(\Lambda_{i}\) as being the covariance matrix for some factor, say \(w_{i}\), where \(y=\mu+\sum_{i=1}^{r}w_{i}+\varepsilon\) with \(\text{Cov}(w_{i},w_{j})=0\), \(\text{Cov}(\varepsilon)=\Psi\), and \(\text{Cov}(w_{i},\varepsilon)=0\). In the usual factor analysis model with factors \(x=(x_{1},\ldots,x_{r})^{\prime}\) and \(B^{\prime}=[b_{1},\ldots,b_{r}]\), we have \(w_{i}=x_{i}b_{i}\). The question then becomes what kind of underlying factor \(w_{i}\) would generate a covariance matrix such as \(\Lambda_{i}\). The advantage to this point of view is that attention is directed towards explaining the observable correlations. In traditional factor analysis, attention is directed towards estimating the ill-defined factor loadings. Of course, the end result is the same.

Just as the matrix \(B\) is not unique, neither is the decomposition

\[\Lambda=\sum_{i=1}^{r}\Lambda_{i}\,.\]

In practice, one would rotate \(B\) to make the \(\Lambda_{i}\)s more interpretable. Moreover, as will be seen later, one need not actually compute \(\Lambda_{i}\) to discover its important structure. The key features of \(\Lambda_{i}\) are obvious from examination of \(b_{i}\).

**Example 14.4.3**.: Using \(\hat{B}\) from Example 14.4.1

\[\hat{\Lambda}_{1}=\hat{b}_{1}\hat{b}_{1}^{\prime}=\begin{bmatrix}0.31&0.31&0.22& 0.41&0.40&0.33\\ 0.31&0.32&0.22&0.42&0.41&0.34\\ 0.22&0.22&0.15&0.29&0.28&0.23\\ 0.41&0.42&0.29&0.55&0.54&0.44\\ 0.40&0.41&0.28&0.44&0.52&0.43\\ 0.33&0.34&0.23&0.44&0.43&0.35\end{bmatrix}\,.\]

All of the variances and covariances are uniformly high because all of the elements of \(b_{1}\) are uniformly high. The factor \(w_{1}\) must be some kind of overall measure--call it general intelligence.

The examination of the second covariance matrix

\[\hat{\Lambda}_{2}=\begin{bmatrix}0.18&0.12&0.19&-0.12&-0.09&-0.07\\ 0.12&0.08&0.13&-0.08&-0.06&-0.04\\ 0.19&0.13&0.20&-0.12&-0.09&-0.06\\ -0.12&-0.08&-0.12&0.07&0.06&0.04\\ -0.09&-0.06&-0.09&0.06&0.04&0.03\\ -0.07&-0.04&-0.06&0.04&0.03&0.02\end{bmatrix}\]is trickier. The factor \(w_{2}\) has two parts; there is positive correlation among the first three variables: Gaelic, English, and history. There is positive correlation among the last three variables: arithmetic, algebra, and geometry. However, the first three variables are negatively correlated with the last three variables. Thus, \(w_{2}\) can be interpreted as a math factor and a nonmath factor that are negatively correlated.

A totally different approach to dealing with \(\Lambda_{2}\) is to decide that any variable with a _variance_ less than, say, 0.09 is essentially constant. This leads to

\[\tilde{\Lambda}_{2}=\left[\begin{matrix}0.18&0&0.19&0&0&0\\ 0&0&0&0&0&0\\ 0.19&0&0.20&0&0&0\\ 0&0&0&0&0&0\\ 0&0&0&0&0&0\\ 0&0&0&0&0&0\end{matrix}\right].\]

Thus, the second factor puts weight on only Gaelic and history. The second factor would then be interpreted as some attribute that only Gaelic and history have in common.

Either analysis of \(\Lambda_{2}\) can be arrived at by direct examination of

\[b_{2}^{\prime}=(-0.429,-0.288,-0.450,0.273,0.211,0.132)\,.\]

The pattern of positives and negatives determines the corresponding pattern in \(\Lambda_{2}\). Similarly, the requirement that a variance be greater than 0.09 to be considered nonzero corresponds to a variable having an absolute factor loading greater than 0.3. Only Gaelic and history have factor loadings with absolute values greater than 0.3. Both are negative, so \(\Lambda_{i}\) will display the positive correlation between them. 

The examination of underlying factors is, of necessity, a very slippery enterprize. The parts of factor analysis that are consistent with traditional ideas of modeling are estimation of \(\Lambda\) and \(\Psi\) and the determination of the rank of \(\Lambda\). The rest is pure data analysis. It is impossible to prove that underlying factors actually exist. The argument in factor analysis is that if these factors existed they could help explain the data.

Factor analysis can only suggest that certain factors might exist. The appropriate question is not whether they really exist but whether their existence is a useful idea. For example, does the idea of a factor for general intelligence help people to understand the nature of test scores. A more stringent test of usefulness is whether the idea of a general intelligence factor leads to accurate predictions about future observable events. Recall from Exercise 14.5 that one can predict factor scores, so those predictions can be used as a tool in making predictions about future observables for the individuals in the study.

An interesting if unrelated example of these criteria for usefulness involves the force of gravity. For most of us, it is impossible to prove that such a force exists. However, the idea of this force allows one to both explain and predict the behavior of physical objects. The fact that accurate predictions can be made does not prove that gravity exists. If an idea explains current data in an intelligible manner and/or allows accurate prediction, it is a useful idea. For example, the usefulness of Newton's laws of motion cannot be disregarded just because they break down for speeds approaching that of light.

### Additional Exercises

##### Exercise 14.5.1

1. Find the vector \(b\) that minimizes \[\sum_{i=1}^{q}\left[y_{i}-\mu_{i}-b^{\prime}(x-\mu_{x})\right]^{2}.\]
2. For given weights \(w_{i}\), \(i=1,\ldots,q\), find the vector \(b\) that minimizes \[\sum_{i=1}^{q}w_{i}^{2}\left[y_{i}-\mu_{i}-b^{\prime}(x-\mu_{x})\right]^{2}.\]
3. Find the vectors \(b_{i}\) that minimize \[\sum_{i=1}^{q}w_{i}^{2}\left[y_{i}-\mu_{i}-b^{\prime}_{i}(x-\mu_{x})\right]^{2}.\]

##### Exercise 14.5.2

In a population of large industrial corporations, the covariance matrix for \(y_{1}=assets/10^{6}\) and \(y_{2}=net\;income/10^{6}\) is

\[\Sigma=\left[\begin{array}{cc}75&5\\ 5&1\end{array}\right].\]

1. Determine the principal components.
2. What proportion of the total prediction variance is explained by \(a^{\prime}_{1}y\)?
3. Interpret \(a^{\prime}_{1}y\).
4. Repeat (a), (b), and (c) for principal components based on the correlation matrix.

##### Exercise 14.5.3

What are the principal components associated with \[\Sigma=\left[\begin{array}{cccc}5&0&0&0\\ 0&3&0&0\\ 0&0&3&0\\ 0&0&0&2\end{array}\right]?\]

Discuss the problem of reducing the variables to a two-dimensional space.

[MISSING_PAGE_FAIL:583]

**Exercise 14.5.7**.: Assume a two-factor model with

\[\Sigma=\begin{bmatrix}0.15&0.00&0.05\\ 0.00&0.20&-0.01\\ 0.05&-0.01&0.05\end{bmatrix}\]

and

\[B=\begin{bmatrix}0.3&0.2&0.1\\ 0.2&-0.3&0.1\end{bmatrix}.\]

What is \(\Psi\)? What are the communalities?

**Exercise 14.5.8**.: Using the vectors \(v_{1}\) and \(v_{2}\) from Exercise 14.5.4, let

\[\Lambda=v_{1}v_{1}^{\prime}+v_{2}v_{2}^{\prime}.\]

Give the eigenvector solution for \(B\) and another set of loadings that generates \(\Lambda\).

**Exercise 14.5.9**.: Given that

\[\Sigma=\begin{bmatrix}1.00&0.30&0.09\\ 0.30&1.00&0.30\\ 0.09&0.30&1.00\end{bmatrix}\]

and

\[\Psi=D(0.1,0.2,0.3),\]

find \(\Lambda\) and two choices of \(B\).

**Exercise 14.5.10**.: Find definitions for the well-known factor loading matrix rotations varimax, direct quartimin, quartimax, equamax, and orthoblique. What is each rotation specifically designed to accomplish? Apply each rotation to the covariance matrices of Exercise 14.5.9.

**Exercise 14.5.11**.: Do a factor analysis of the female turtle carapace data of Exercise 10.6.1. Include tests for the numbers of factors and examine various factor-loading rotations.

**Exercise 14.5.12**.: Do a factor analysis of the Chapman data discussed in Exercise 14.5.6.

**Exercise 14.5.13**.: Show the following determinant equality.

\[|\Psi+BB^{\prime}|=|I+B^{\prime}\Psi^{-1}B||\Psi|.\]

**Exercise 14.5.14**.: Find the likelihood ratio test for

\[H_{0}:\ \Sigma=\sigma^{2}\left[(1-\rho)I+\rho JJ^{\prime}\right]\]

against the general alternative.

## References

* Christensen (1997) Christensen, R. (1997). _Log-linear models and logistic regression_ (2nd ed.). New York: Springer.
* Christensen (2011) Christensen, R. (2011). _Plane answers to complex questions: The theory of linear models_ (4th ed.). New York: Springer.
* Christensen (2014) Christensen, R. (2014). Comment. _The American Statistician, 68_, 13-17.
* Dixon & Massey Jr. (1983) Dixon, W. J., & Massey Jr., F. J. (1983). _Introduction to statistical analysis_. New York: McGraw-Hill.
* Geweke & Singleton (1980) Geweke, J. F., & Singleton, K. J. (1980). Interpreting the likelihood ratio statistic in factor models when sample size is small. _Journal of the American Statistical Association, 75_, 133-137.
* Gnanadesikan (1977) Gnanadesikan, R. (1977). _Methods for statistical data analysis of multivariate observations_. New York: Wiley.
* Hotelling (1933) Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. _Journal of Educational Psychology, 24_, 417-441, 498-520.
* Hyvarinen et al. (2001) Hyvarinen, A., Karhunen, J. & Oja, E. (2001). _Independent component analysis_. New York: Wiley.
* Johnson & Wichern (2007) Johnson, R. A., & Wichern, D. W. (2007). _Applied multivariate statistical analysis_ (6th ed.). Englewood Cliffs: Prentice-Hall.
* Jolicoeur & Mosimann (1960) Jolicoeur, P., & Mosimann, J. E. (1960). Size and shape variation on the painted turtle: A principal component analysis. _Growth, 24_, 339-354.
* Jolliffe (1986) Jolliffe, I. T. (1986). _Principal component analysis_. New York: Springer.
* Joreskog (1975) Joreskog, K. G. (1975). Factor analysis by least squares and maximum likelihood. In K. Enslein, A. Ralston, & H. S. Wilf (Eds.) _Statistical methods for digital computers_. New York: Wiley.
* Lawley & Maxwell (1971) Lawley, D. N., & Maxwell, A. E. (1971). _Factor analysis as a statistical methodology_ (2nd ed.). New York: American Elsevier.
* Li & Chen (1985) Li, G. & Chen, Z. (1985). Projection pursuit approach to robust dispersion matrices and principal components: Primary theory and Monte Carlo. _Journal of the American Statistical Association, 80_, 759-766.
* Morrison (2004) Morrison, D. F. (2004). _Multivariate statistical methods_ (4th ed.). Pacific Grove, CA: Duxbury Press.
* Okamoto & Kanazawa (1968) Okamoto, M. & Kanazawa, M. (1968). Minimization of eigenvalues of a matrix and optimality of principal components. _Annals of Mathematical Statistics, 39_, 859-863.
* O'Brien & Wichern (2004)Pearson, K. (1901). On lines and planes of closest fit to systems of points in space. _Philosophical Magazine, 6_(2), 559-572.
* Rao (1973) Rao, C. R. (1973). _Linear statistical inference and its applications_ (2nd ed.). New York: Wiley.
* Schervish (1986) Schervish, M. J. (1986). _A predictive derivation of principal components_. Technical Report 378, Department of Statistics, Carnegie-Mellon University, Pittsburgh, PA.
* Seber (1984) Seber, G. A. F. (1984). _Multivariate observations_. New York: Wiley.
* Thompson (1934) Thompson, G. H. (1934). Hotelling's method modified to give Spearman's _g. Journal of Educational Psychology, 25_, 366-374.
* Thurstone (1931) Thurstone, L. L. (1931). Multiple factor analysis. _Psychological Review, 38_, 406-427.
* Williams (1979) Williams, J. S. (1979). A synthetic basis for comprehensive factor-analysis theory. _Biometrics, 35_, 719-733.

## Appendix A Mathematical Background

**Abstract** This appendix reviews two areas of mathematics that arise in dealing with dependent data: differentiation and the use of Vec operators with Kronecker products. It also examines quadratic optimization problems and their connection to support vector machines.

## Appendix A.1 Differentiation

In _PA_ I managed to avoid using very much calculus. There is no avoiding calculus when dealing with dependent data.

If \(F\) is a function from \({\bf R}^{s}\) into \({\bf R}^{t}\) with \(F(x)=[f_{1}(x),\ldots,f_{t}(x)]^{t}\), then the derivative of \(F\) at \(c\) is the \(t\times s\) matrix of partial derivatives,

\[{\bf d}_{x}F(x)|_{x=c}\equiv[\partial f_{i}(x)/\partial x_{j}|_{x=c}].\]

When the context is clear, we often use simpler notations such as

\[{\bf d}_{x}F(x)|_{x=c}\equiv{\bf d}_{x}F(c)\equiv{\bf d}F(c).\]

Critical points are points \(c\) where \({\bf d}F(c)=0\).

A first order Taylor's expansion of \(F\) around the point \(c\) is

\[F(x)\doteq F(c)+[{\bf d}F(c)](x-c)\]

or, to be more mathematically precise,

\[F(x)=F(c)+[{\bf d}F(c)](x-c)+o(\|x-c\|),\]

where \(\|x-c\|^{2}=(x-c)^{\prime}(x-c)\) and for scalars \(a_{n}\to 0\), \(o(a_{n})\) has the property that \(o(a_{n})/a_{n}\to 0\).

In fact, the first order Taylor's expansion is essentially the mathematical definition of a derivative. The technical definition of a derivative, if it exists, is that it is some \(t\times s\) matrix \(dF(c)\) such that for any \(\varepsilon>0\), there exists a \(\delta>0\) for which any \(x\) with \(\|x-c\|<\delta\) has

\[\|F(x)-F(c)-[{\bf d}F(c)](x-c)\|<\varepsilon\|x-c\|.\]

In other words, the linear function of \(x\) defined by \([{\bf d}F(c)](x-c)\) is a good approximation to the curved function \(F(x)-F(c)\) in neighborhoods of \(c\). Under suitable conditions, the matrix \({\bf d}F(c)\) is unique and equals the matrix of partial derivatives of \(F\) evaluated at the vector \(c\).

In particular, if \(g\) maps \({\bf R}^{s}\) into \({\bf R}\), then \({\bf d}g(x)\) is a \(1\times s\)_row_ vector. In this case we can also define the second derivative matrix of \(g\) at \(c\) as

\[{\bf d}^{2}_{xx}g(c)\equiv{\bf d}^{2}_{xx}g(x)|_{x=c}\equiv{\bf d}_{x}\left\{[ {\bf d}_{x}g(x)]^{\prime}\right\}|_{x=c}=\left[\partial^{2}g(x)/\partial x_{i }\partial x_{j}|_{x=c}\right],\]

which is an \(s\times s\) matrix. Taylor's second order expansion can be written

\[g(x)\doteq g(c)+[{\bf d}g(c)](x-c)+(x-c)^{\prime}\big{[}{\bf d}^{2}g(c)\big{]} (x-c)/2\]

or, to be more mathematically precise,

\[g(x)=g(c)+[{\bf d}g(c)](x-c)+(x-c)^{\prime}\big{[}{\bf d}^{2}g(c)\big{]}(x-c)/ 2+o\big{(}\|x-c\|^{2}\big{)}.\]

First and second order Taylor's expansions are fundamental to the models used in response surface methodology, cf. [http://www.stat.unm.edu/~fletcher/TopicsInDesign](http://www.stat.unm.edu/~fletcher/TopicsInDesign) or _ALM-II_.

The chain rule can be written as a matrix product. If \(f:{\bf R}^{s}\to{\bf R}^{t}\) and \(g:{\bf R}^{t}\to{\bf R}^{n}\), then the composite function is defined by

\[(g\circ f)(x)\equiv g[f(x)]\]

and its derivative is an \(n\times s\) matrix that satisfies

\[{\bf d}(g\circ f)(c)=[{\bf d}_{v}g(v)|_{v=f(c)}][{\bf d}_{x}f(x)|_{x=c}] \equiv{\bf d}g[f(c)]{\bf d}f(c).\]

Since we are examining linear models, we will be particularly interested in the derivatives of linear functions and quadratic functions.

**Proposition A.1.** Let \(A\) be a fixed \(t\times s\) matrix with \(t=s\) in part (b).

(a) \({\bf d}_{x}[Ax]=A\).

(b) \({\bf d}_{x}[x^{\prime}Ax]=2x^{\prime}A\).

Proof. (a) is proven by writing each element of \(Ax\) as a sum and taking partial derivatives. (b) is proven by writing \(x^{\prime}Ax\) as a double sum and taking partial derivatives.

Let \(A(u)=[a_{ij}(u)]\) be an \(t\times s\) matrix that is a function of a scalar \(u\). We define the derivative of \(A(u)\) with respect to \(u\) as the matrix of derivatives for its individual entries, i.e.,

\[{\bf d}_{u}A(u)\equiv[{\bf d}_{u}a_{ij}(u)].\]

Functions like \(A(u)\), from \({\bf R}\) into a matrix, do not fit into our definition of derivatives, however \(A(u)x\) and \(x^{\prime}A(u)x\) are functions of \(u\) from \({\bf R}\) into \({\bf R}^{t}\) and \({\bf R}\), respectively, and it is easy to see that

\[{\bf d}_{u}[A(u)x]=[{\bf d}_{u}A(u)]x\quad\mbox{and that}\quad{\bf d}_{u}[x^{ \prime}A(u)x]=x^{\prime}[{\bf d}_{u}A(u)]x.\]

We now present some useful rules for matrix derivatives.

**Proposition A.2.**  For (c) and (d), \(t=s\).

1. A form of the product rule holds for conformable matrices \(A(u)\) and \(B(u)\), \[{\bf d}_{u}[A(u)B(u)]=[{\bf d}_{u}A(u)]B(u)+A(u)[{\bf d}_{u}B(u)].\]
2. When \(B\) and \(C\) are fixed matrices of conformable sizes, \[{\bf d}_{u}[CA(u)B]=C[{\bf d}_{u}A(u)]B.\]
3. The derivative of an inverse is \[{\bf d}_{u}A^{-1}(u)=-A^{-1}(u)[{\bf d}_{u}A(u)]A^{-1}(u).\]
4. The derivative of a trace is \[{\bf d}_{u}\{{\rm tr}[A(u)]\}={\rm tr}[{\bf d}_{u}A(u)].\]
5. If \(V(u)\) is positive definite for all \(u\), \[{\bf d}_{u}\log\left\{\det[V(u)]\right\}={\rm tr}\bigl{\{}V^{-1}(u)[{\bf d}_{u }V(u)]\bigr{\}}.\] The notations \(\det[V]\) and \(|V|\) are used interchangeably to indicate the determinant.

Proof. See Exercise A.1 for a proof of the proposition. \(\Box\)

**Exercise A.1.**  Prove Proposition A.2. Hints: For (a), consider \(A(u)B(u)\) elementwise. For (b), use (a) twice. For (c), use (a) and the fact that \(0={\bf d}_{u}I={\bf d}\left[A(u)A^{-1}(u)\right]\). For (d) use the fact that the trace is a linear function of the diagonal elements. For (e), write \(V=P{\rm Diag}(\phi_{i})\,P^{\prime}\) and show that both sides equal

\[\sum_{i=1}^{q}{\bf d}_{u}\phi_{i}(u)\,\frac{1}{\phi_{i}(u)}.\]

For the right-hand side, use (a) and the fact that \(0={\bf d}_{u}I={\bf d}_{u}PP^{\prime}\).

### Vec Operators and Kronecker Products

Kronecker products and Vec operators are extremely useful in multivariate analysis and some approaches to variance component estimation. They are also often used in writing balanced ANOVA models. We present their definitions and basic algebraic properties.

We begin with definitions. Let \(A\) be an \(r\times c\) matrix. Write \(A=[A_{1},A_{2},\ldots,A_{c}]\), where \(A_{i}\) is the \(i\)th column of \(A\). Then the _Vec_ operator stacks the columns of \(A\) into an \(rc\times 1\) vector; thus,

\[\left[\text{Vec}(A)\right]^{\prime}=[A_{1}^{\prime},A_{2}^{\prime},\ldots,A_{c }^{\prime}].\]

Let \(A=[a_{ij}]\) be an \(r\times c\) matrix and \(B=[b_{ij}]\) be an \(s\times d\) matrix. The _Kronecker product_ of \(A\) and \(B\), written \(A\otimes B\), is an \(r\times c\) matrix of \(s\times d\) matrices. The matrix in the \(i\)th row and \(j\)th column is \(a_{ij}B\). In total, \(A\otimes B\) is an \(rs\times cd\) matrix.

Example A.2.1.: If

\[A=\begin{bmatrix}1&4\\ 2&5\end{bmatrix},\quad B=\begin{bmatrix}1&3\\ 0&4\end{bmatrix},\]

then

\[\text{Vec}(A)=[1,2,4,5]^{\prime},\qquad\text{Vec}(B)=[1,0,3,4]^{\prime},\]

and

\[A\otimes B=\begin{bmatrix}1\begin{pmatrix}1&3\\ 0&4\end{pmatrix}&4\begin{pmatrix}1&3\\ 0&4\end{pmatrix}\\ 2\begin{pmatrix}1&3\\ 0&4\end{pmatrix}&5\begin{pmatrix}1&3\\ 0&4\end{pmatrix}\end{bmatrix}=\begin{bmatrix}1&3&4&12\\ 0&4&0&16\\ 2&6&5&15\\ 0&8&0&20\end{bmatrix}.\qquad\Box\]

The algebraic properties are as follows:

1. If the matrices are of conformable sizes, \([A\otimes(B+C)]=[A\otimes B]+[A\otimes C]\).
2. If the matrices are of conformable sizes, \([(A+B)\otimes C]=[A\otimes C]+[B\otimes C]\).
3. If \(a\) and \(b\) are scalars, \(ab[A\otimes B]=[aA\otimes bB]\).
4. If the matrices are of conformable sizes, \([A\otimes B][C\otimes D]=[AC\otimes BD]\).
5. The transpose of a Kronecker product matrix is \([A\otimes B]^{\prime}=[A^{\prime}\otimes B^{\prime}]\).
6. The generalized inverse of a Kronecker product matrix is \([A\otimes B]^{-}=[A^{-}\otimes B^{-}]\).
7. For two vectors \(v\) and \(w\), \(\text{Vec}(vw^{\prime})=w\otimes v\).
8. For a matrix \(W\) and conformable matrices \(A\) and \(B\), \(\text{Vec}(AWB^{\prime}){=}[B\otimes A]\text{Vec}(W)\).
9. For conformable matrices \(A\) and \(B\), \(\text{Vec}(A)^{\prime}\text{Vec}(B)=\text{tr}(A^{\prime}B)\).
10. The Vec operator commutes with any matrix operation that is performed elementwise. For example, \(E\{\text{Vec}(W)\}=\text{Vec}\{E(W)\}\) when \(W\) is a random matrix. Similarly, for conformable matrices \(A\) and \(B\) and scalar \(\phi\), \(\text{Vec}(A+B)=\text{Vec}(A)+\text{Vec}(B)\) and \(\text{Vec}(\phi A)=\phi\,\text{Vec}(A)\).
11. If \(A\) and \(B\) are positive definite, then \(A\otimes B\) is positive definite.

Most of these are well-known facts and easy to establish. Two of them are somewhat more unusual, and we present proofs.

**Item 8**.: We show that for a matrix \(W\) and conformable matrices \(A\) and \(B\), \(\operatorname{Vec}(AWB^{\prime})=[B\otimes A]\operatorname{Vec}(W)\). First note that if \(\operatorname{Vec}(AW)=[I\otimes A]\operatorname{Vec}(W)\) and \(\operatorname{Vec}(WB^{\prime})=[B\otimes I]\operatorname{Vec}(W)\), then \(\operatorname{Vec}(AWB^{\prime})=[I\otimes A]\operatorname{Vec}(WB^{\prime} )=[I\otimes A][B\otimes I]\operatorname{Vec}(W)=[B\otimes A]\operatorname{Vec }(W)\).

To see that \(\operatorname{Vec}(AW)=[I\otimes A]\operatorname{Vec}(W)\), let \(W\) be \(r\times s\) and write \(W\) in terms of its columns \(W=[w_{1},\ldots,w_{s}]\). Then \(AW=[Aw_{1},\ldots,Aw_{s}]\) and \(\operatorname{Vec}(AW)\) stacks the columns \(Aw_{1},\ldots,Aw_{s}\). On the other hand,

\[[I\otimes A]\operatorname{Vec}(W)=\begin{bmatrix}A&&0\\ &\ddots&\\ 0&&A\end{bmatrix}\begin{bmatrix}w_{1}\\ \vdots\\ w_{s}\end{bmatrix}=\begin{bmatrix}Aw_{1}\\ \vdots\\ Aw_{s}\end{bmatrix}.\]

To see that \(\operatorname{Vec}(WB^{\prime})=[B\otimes I]\operatorname{Vec}(W)\), take \(W\) as above and write \(B_{m\times s}=[b_{ij}]\) with _rows_\(b^{\prime}_{1},\ldots,b^{\prime}_{m}\). First note that \(WB^{\prime}=[Wb_{1},\ldots,Wb_{m}]\), so \(\operatorname{Vec}(WB^{\prime})\) stacks the columns \(Wb_{1},\ldots,Wb_{m}\). Now observe that

\[[B\otimes I_{r}]\operatorname{Vec}(W)=\begin{bmatrix}b_{11}I_{r}&\cdots&b_{1s} I_{r}\\ \vdots&\ddots&\vdots\\ b_{m1}I_{r}&\cdots&b_{ms}I_{r}\end{bmatrix}\begin{bmatrix}w_{1}\\ \vdots\\ w_{s}\end{bmatrix}=\begin{bmatrix}Wb_{1}\\ \vdots\\ Wb_{m}\end{bmatrix}.\]

**Item 11**.: To see that if \(A_{r\times r}\) and \(B_{s\times s}\) are positive definite, then \(A\otimes B\) is positive definite, consider the eigenvalues and eigenvectors of \(A\) and \(B\). Recall that a symmetric matrix is positive definite if and only if all of its eigenvalues are positive. Suppose that \(Av=\phi v\) and \(Bw=\theta w\). We now show that all of the eigenvalues of \(A\otimes B\) are positive. Observe that

\[[A\otimes B][v\otimes w] = [Av\otimes Bw]\] \[= [\phi v\otimes\theta w]\] \[= \phi\theta[v\otimes w].\]

This shows that \([v\otimes w]\) is an eigenvector of \([A\otimes B]\) corresponding to the eigenvalue \(\phi\theta\). As there are \(r\) choices for \(\phi\) and \(s\) choices for \(\theta\), this accounts for all \(rs\) of the eigenvalues in the \(rs\times rs\) matrix \([A\otimes B]\). Moreover, \(\phi\) and \(\theta\) are both positive, so all of the eigenvalues of \([A\otimes B]\) are positive.

## A.3 Quadratic Optimization

We give a brief introduction to quadratic optimization. The methods presented here can be used for fitting linear models and linear models subject to linear (equality) constraints but we have not needed them for fitting least squares or even for finding ridge regression estimates. The reason for discussing this topic is that both lasso regression and support vector machines involve minimizing quadratic functions subject to linear inequality constraints.

In general, a quadratic optimization problem seeks a vector \(\eta\) that minimizes

\[\frac{1}{2}\eta^{\prime}A\eta+b^{\prime}\eta\]

subject to the one dimensional constraints

\[\tilde{q}^{\prime}_{i}\eta=\tilde{d}_{i},\quad i=1,\ldots,s;\qquad\tilde{q}^{ \prime}_{i}\eta\leq\tilde{d}_{i},\quad i=s+1,\ldots,s+t.\]

As always with quadratic forms, without loss of generality we assume that \(A\) is symmetric. Rewrite the constraints in matrix notation as

\[Q_{1}\eta=d_{1};\qquad Q_{2}\eta\leq d_{2} \tag{1}\]

where matrix inequalities are understood to apply elementwise. Also write

\[Q=\left[\begin{array}{c}Q_{1}\\ Q_{2}\end{array}\right];\qquad d=\left[\begin{array}{c}d_{1}\\ d_{2}\end{array}\right].\]

If \(t=0\), i.e., if there are no inequality constraints, the minimizing values solve the linear system

\[\left[\begin{array}{cc}A&Q^{\prime}_{1}\\ Q_{1}&0\end{array}\right]\left[\begin{array}{c}\eta\\ \lambda\end{array}\right]=\left[\begin{array}{c}-b\\ d_{1}\end{array}\right].\]

This is a special case of the more general results that follow.

We now present Karush, Kuhn, and Tucker's (KKT's) four necessary conditions for a solution. First set up the Lagrange function associated with the inequality constraints being equalities:

\[\frac{1}{2}\eta^{\prime}A\eta+b^{\prime}\eta+\lambda^{\prime}(Q\eta-d). \tag{2}\]

If the constraints are all equalities, \(\lambda\) is a vector of Lagrange multipliers. With inequalities they are called KKT multipliers. The idea is that since \((Q\eta-d)\leq 0\), if we take \(\lambda\geq 0\), we have \(\lambda^{\prime}(Q\eta-d)\leq 0\). The answer to our problem should occur where we have \(\eta\) and \(\lambda\) achieving

\[\sup_{\lambda\geq 0}\inf_{\eta}\left[\frac{1}{2}\eta^{\prime}A\eta+b^{\prime} \eta+\lambda^{\prime}(Q\eta-d)\right].\]

The requirement that \(\lambda\geq 0\) is known as KKT _dual feasibility_.

For any fixed \(\lambda\) the infinum should occur as a solution to

\[0 = {\bf d}_{\eta}\left[\frac{1}{2}\eta^{\prime}A\eta+b^{\prime}\eta+ \lambda^{\prime}(Q\eta-d)\right] \tag{3}\] \[= \eta^{\prime}A+b^{\prime}+\lambda^{\prime}Q.\]

This is the KKT _stationarity condition_. For a solution of (3) to exist, we must have

\[b+Q^{\prime}\lambda\in C(A).\]We refer to this as the _KKT column space condition_ (although it is not one of the four KKT conditions).

Equality (3) implies that

\[0=\eta^{\prime}A\eta+b^{\prime}\eta+\lambda^{\prime}Q\eta.\]

This allows us to simplify (2) into

\[\frac{-1}{2}\eta^{\prime}A\eta-\lambda^{\prime}d\]

which we want to maximize relative to \(\lambda\geq 0\). Moreover, equation (3) also allows us to solve for \(\eta\),

\[\eta^{\prime}=-\left(b^{\prime}+\lambda^{\prime}Q\right)A^{-};\qquad\eta=-A^{- }\left(b+Q^{\prime}\lambda\right).\]

The choice of generalized inverse for \(A\) is crucial to satisfying KKT's conditions other than stationarity.

Having solved for \(\eta\), we can find (5) as a function of \(\lambda\) alone,

\[\frac{-1}{2}\left(b^{\prime}+\lambda^{\prime}Q\right)A^{-}AA^{-}\left(b+Q^{ \prime}\lambda\right)-\lambda^{\prime}d\]

or, due to (4),

\[\frac{-1}{2}\left(b^{\prime}+\lambda^{\prime}Q\right)A^{-}\left(b+Q^{\prime} \lambda\right)-\lambda^{\prime}d.\]

The maximizing value of the criterion function does not depend on the choice of generalized inverse, even though an appropriate solution for \(\eta\) that satisfies all of the KKT conditions does.

The KKT _primal feasibility_ condition is simply that any solution \(\eta\) must satisfy the original constraints in (1). In terms of \(\lambda\) that means picking \(\lambda\) and \(A^{-}\) so that

\[-QA^{-}\left(b+Q^{\prime}\lambda\right)-d\overset{=}{\leq}0,\]

where it is understood that some rows of the vectors are equalities and some are inequalities.

The final KKT condition is _complementary slackness_ which means that

\[0=Diag(\lambda)[Q\eta-d]=-Diag(\lambda)\left[A^{-}\left(b+Q^{\prime}\lambda \right)-d\right].\]

This condition is vacuous for the equality constraints for which \(Q_{1}\eta-d_{1}=0\).

If solving the stationarity condition gives \(\lambda\) elements that do not satisfy dual feasibility or complementary slackness, the solution is not a minimizer and some corrective action must be taken.

In (2) we did _not_ set the derivative with respect to \(\lambda\) equal to 0. Setting the derivative with respect to \(\lambda\) equal to 0 gives both \(Q_{1}\eta=d_{1}\) and \(Q_{2}\eta=d_{2}\). Given (1), it is appropriate to set \(Q_{1}\eta=d_{1}\) but it is not appropriate to set \(Q_{2}\eta=d_{2}\).

#### Application to Support Vector Machines

This discussion relies on the notation of Section 13.5. Any \(n\) vector \(v\) may be written as an \(N_{1}\) vector and an \(N_{0}\) vector, \(v^{\prime}=(v^{\prime}_{1},v^{\prime}_{0})\). For this discussion only, denote \(J_{1}\equiv J_{N_{1}}\) and \(J_{0}\equiv J_{N_{0}}\). The data are written as

\[Y=\begin{bmatrix}Y_{1}\\ Y_{0}\end{bmatrix}=\begin{bmatrix}J_{1}\\ 0\end{bmatrix};\qquad X=\begin{bmatrix}X_{1}\\ X_{0}\end{bmatrix}=\begin{bmatrix}J_{1}&\mathbf{X}_{1}\\ J_{0}&\mathbf{X}_{0}\end{bmatrix}.\]

The criterion function (13.5.4) is

\[\frac{1}{2}\eta^{\prime}A\eta+b^{\prime}\eta=\frac{1}{2}\begin{bmatrix}\beta_{ 0}\\ \beta_{*}\\ \xi\end{bmatrix}^{\prime}\begin{bmatrix}0&0&0\\ 0&2kI&0\\ 0&0&0\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{*}\\ \xi\end{bmatrix}+\begin{bmatrix}0\\ 0\\ J_{n}\end{bmatrix}^{\prime}\begin{bmatrix}\beta_{0}\\ \beta_{*}\\ \xi\end{bmatrix}. \tag{7}\]

For SVMs we only have inequality constraints:

\[Q\eta-d =\begin{bmatrix}-X_{1}&-I&0\\ X_{0}&0&-I\\ 0&-I&0\\ 0&0&-I\end{bmatrix}\begin{bmatrix}\beta\\ \xi_{1}\\ \xi_{0}\end{bmatrix}-\begin{bmatrix}-J_{1}\\ -J_{0}\\ 0\\ 0\end{bmatrix}\] \[=\begin{bmatrix}-J_{1}&-\mathbf{X}_{1}&-I&0\\ J_{0}&\mathbf{X}_{0}&0&-I\\ 0&0&-I&0\\ 0&0&0&-I\end{bmatrix}\begin{bmatrix}\beta_{0}\\ \beta_{*}\\ \xi_{1}\\ \xi_{0}\end{bmatrix}-\begin{bmatrix}-J_{1}\\ -J_{0}\\ 0\\ 0\end{bmatrix}\leq 0. \tag{8}\]

For stationary solutions to exist we need the KKT column space condition (4) to hold, i.e.,

\[\begin{bmatrix}0\\ 0\\ J_{1}\\ J_{0}\end{bmatrix}+\begin{bmatrix}-J^{\prime}_{1}&J^{\prime}_{0}&0&0\\ -\mathbf{X}^{\prime}_{1}&\mathbf{X}^{\prime}_{0}&0&0\\ -I&0&-I&0\\ 0&-I&0&-I\end{bmatrix}\begin{bmatrix}\lambda_{11}\\ \lambda_{10}\\ \lambda_{21}\\ \lambda_{20}\end{bmatrix}\\ =\begin{bmatrix}0\\ 0\\ J_{n}\end{bmatrix}+\begin{bmatrix}-J^{\prime}_{1}\lambda_{11}+J^{\prime}_{0} \lambda_{10}\\ -\mathbf{X}^{\prime}_{1}\lambda_{11}+\mathbf{X}^{\prime}_{0}\lambda_{10}\\ -\lambda_{1}-\lambda_{2}\end{bmatrix}=b+Q^{\prime}\lambda\in C(A)=C\left( \begin{bmatrix}0\\ I\\ 0\end{bmatrix}\right).\]

This requires that

\[-J^{\prime}_{1}\lambda_{11}+J^{\prime}_{0}\lambda_{10}=0\]

and \(J-\lambda_{1}-\lambda_{2}=0\) or

\[\lambda_{2}=J-\lambda_{1}.\]

The first of these displayed equalities is a standard SVM condition. The second eliminates our need to be concerned with \(\lambda_{2}\). Moreover, the dual feasibility condition \(\lambda\geq 0\) implies that both \(\lambda_{1}\geq 0\) and \(J-\lambda_{1}\geq 0\) so\[0\leq\lambda_{1}\leq 1,\]

which is another standard SVM condition. These two conditions will be referred to as the _column space conditions_.

##### Computing \(\beta_{*}\)

Using the column space conditions,

\[b+Q^{\prime}\lambda=\left[\begin{array}{c}0\\ -{\bf X}_{1}^{\prime}\lambda_{11}+{\bf X}_{0}^{\prime}\lambda_{10}\\ 0\end{array}\right],\]

so

\[\eta=-A^{-}(b+Q^{\prime}\lambda)=A^{-}\left[\begin{array}{c}0\\ {\bf X}_{1}^{\prime}\lambda_{11}-{\bf X}_{0}^{\prime}\lambda_{10}\\ 0\end{array}\right]\]

In particular, because \(AA^{-}A=A\), \((b+Q^{\prime}\lambda)\in C(A)\), and the specific form of \(A\),

\[\beta_{*} = \left[\begin{array}{c}0\\ I\\ 0\end{array}\right]^{\prime}\eta=\left[\begin{array}{c}0\\ I\\ 0\end{array}\right]^{\prime}\frac{1}{2k}A\eta\] \[= \frac{1}{2k}\left[\begin{array}{c}0\\ I\\ 0\end{array}\right]^{\prime}AA^{-}\left[\begin{array}{c}0\\ {\bf X}_{1}^{\prime}\lambda_{11}-{\bf X}_{0}^{\prime}\lambda_{10}\\ 0\end{array}\right]\] \[= \frac{1}{2k}\left({\bf X}_{1}^{\prime}\lambda_{11}-{\bf X}_{0}^{ \prime}\lambda_{10}\right).\]

As discussed in Section 13.5, often many of the \(\lambda_{1}\) values are 0 which is reflected in how computer programs report their results.

##### The Dual Criterion

We now evaluate the criterion function (7) in terms of \(\lambda\) by using the solution for \(\eta\) provided by the KKT stationarity and column space conditions. Maximizing this function is often known as the _dual problem_. Since \((b+Q^{\prime}\lambda)\in C(A)\), in (6) the choice of \(A^{-}\) does not matter when evaluating (7) so we take the simplest generalized inverse.

\[A^{-}=\left[\begin{array}{ccc}0&0&0\\ 0&(1/2k)I&0\\ 0&0&0\end{array}\right].\]Substitution gives

\[\frac{-1}{2}(b+Q^{\prime}\lambda)^{\prime}A^{-}(b+Q^{\prime}\lambda)- \lambda^{\prime}d\] \[=\left[\begin{array}{cc}0&0\\ {\bf X}_{1}^{\prime}\lambda_{11}-{\bf X}_{0}^{\prime}\lambda_{10}\\ 0&0\end{array}\right]^{\prime}\left[\begin{array}{cc}0&0\\ 0&(1/2k)I&0\\ 0&0&0\end{array}\right]\left[\begin{array}{cc}0\\ {\bf X}_{1}^{\prime}\lambda_{11}-{\bf X}_{0}^{\prime}\lambda_{10}\\ 0\end{array}\right]-\left[\begin{array}{cc}\lambda_{1}\\ \lambda_{2}\end{array}\right]^{\prime}\left[\begin{array}{cc}-J_{n}\\ 0\end{array}\right]\] \[=\frac{-1}{2k}\lambda_{1}^{\prime}\left[\begin{array}{cc}{\bf X} _{1}{\bf X}_{1}^{\prime}&-{\bf X}_{1}{\bf X}_{0}^{\prime}\\ -{\bf X}_{0}{\bf X}_{1}^{\prime}&{\bf X}_{0}{\bf X}_{0}^{\prime}\end{array} \right]\lambda_{1}+\lambda_{1}^{\prime}J_{n}.\]

The constraints on \(\lambda\) are typically cited as being the dual feasibility condition \(\lambda\geq 0\) and the column space conditions. Together these require \(0\leq\lambda_{1i}\leq 1\) and \(J_{1}^{\prime}\lambda_{11}-J_{0}^{\prime}\lambda_{10}=0\).

Exercise A.2. Show that if you multiply the original criterion function (7) by \(1/2k\equiv\tilde{C}\), the dual criterion function remains unchanged but the column space conditions change to \(0\leq\lambda_{1}\leq\tilde{C}\). How does this change the formula for \(\beta_{\rm s}\)? This form of the problem involving a "cost" \(\tilde{C}\), rather than a tuning parameter \(k\) for the penalty, seems quite popular.

An actual solution \(\eta\) needs to incorporate the two other KKT conditions: primal feasibility and complementary slackness. Primal feasibility is that the inequality constraints in (8) continue to hold. Complementary slackness is the key to determining \(\beta_{0}\).

##### Computing \(\beta_{0}\)

Complementary slackness is \(0=D(\lambda)[Q\eta-d]\). Together with a column space condition, it requires

\[0=\left[\begin{array}{cc}D(\lambda_{1})&0\\ 0&D(J-\lambda_{1})\end{array}\right]\left(\begin{array}{cccc}-J_{1}&-{\bf X }_{1}&-I&0\\ J_{0}&{\bf X}_{0}&0&-I\\ 0&0&-I&0\\ 0&0&0&-I\end{array}\right]\left[\begin{array}{c}\beta_{0}\\ \beta_{\rm s}\\ \xi\end{array}\right]-\left[\begin{array}{c}-J_{1}\\ -J_{0}\\ 0\\ 0\end{array}\right]\right).\]

If \(0<\lambda_{1h}<1\), the bottom half of the equation implies \(\xi_{h}=0\). Using that fact in the top half of the equation, depending on whether \(y_{h}\) is 1 or zero, we must have \(\beta_{0}+{\bf x}_{h}^{\prime}\beta_{\rm s}=1\) or \(\beta_{0}+{\bf x}_{h}^{\prime}\beta_{\rm s}=-1\). Changing notation a bit, think about \(\lambda_{1}^{\prime}=(\lambda_{11}^{\prime},\lambda_{10}^{\prime})\). If \(y_{1j}\) denotes an element of \(Y_{1}\) with \(0<\lambda_{11j}<1\), then \(\beta_{0}=1-{\bf x}_{1j}^{\prime}\beta_{\rm s}\) and similarly for \(y_{0j}\) an element of \(Y_{0}\) with \(0<\lambda_{10j}<1\), \(\beta_{0}=-1-{\bf x}_{1j}^{\prime}\beta_{\rm s}\).

It may seem curious that \(\beta_{0}\) should be so directly tied to the arbitrary number 1, but remember that any multiple of \(\beta\) defines the same hyperplane, so we have just chosen a multiple that defines \(\beta_{0}\) in terms of being 1 unit away from an appropriate value of \(\lambda_{h}^{\prime}\beta_{\rm s}\).

As alluded to in Section 13.5, computer programs often report a \(\lambda_{1}\) vector that does not satisfy all of the equalities implied in the paragraph above. As a result, computer programs, and even published works, make some ado about how to obtain \(\beta_{0}\) from the various cases that have \(0<\lambda_{1h}<1\).

## Appendix B Best Linear Predictors

**Abstract** Best linear prediction is a key concept in the development of many topics in statistics. It is introduced in Chap. 6 of _PA_ as an alternative to traditional regression theory. In this volume, it is introduced in Chap. 4 and used in the discussions of time series analysis, the Kalman filter, kriging, principal components, and factor analysis.

### B.1 Properties of Best Linear Predictors

We need to establish general properties of best linear predictors that are analogous to results for conditional expectations. Let \(y=(y_{1},\ldots,y_{q})^{\prime}\) and \(x=(x_{1},\ldots,x_{p-1})^{\prime}\). Denote

\[\begin{array}{rcl}\text{E}(y)&=&\mu_{y}\\ \text{Cov}(y)&=&V_{yy}\end{array}\qquad\qquad\qquad\begin{array}{rcl}\text{E}(x)&=&\mu_{x}\\ \text{Cov}(x)&=&V_{xx}\end{array}\]

and

\[\text{Cov}(y,x)=V_{yx}=V_{xy}^{\prime}.\]

The best linear predictor (BLP) of \(y\) is defined to be the linear function \(f(x)\) that minimizes

\[\text{E}\left\{\left[y-f(x)\right]^{\prime}\left[y-f(x)\right]\right\}.\]

The best linear predictor, also called the linear expectation, is

\[\hat{E}(y|x)\equiv\mu_{y}+B^{\prime}(x-\mu_{x}),\]

where \(B_{(p-1)\times q}\) is a solution

\[V_{xx}B=V_{xy}.\]

In general, the linear expectation \(\hat{E}(y|x)\) is neither the conditional expectation of \(y\) given \(x\) nor an estimate of the conditional expectation; it is a different concept. The conditional expectation \(\text{E}(y|x)\) is the best predictor of \(y\) based on \(x\). \(\hat{E}(y|x)\) isthe best _linear_ predictor. Conditional expectations require knowledge of the entire multivariate distribution. Linear expectations depend only on the mean vector and covariance matrix. For some families of multivariate distributions, of which the multivariate normal is the best known, the linear expectation and the conditional expectation happen to be the same. This is similar to the fact that for multivariate normals best linear unbiased estimates are also best within the broader class of (possibly nonlinear) unbiased estimates. Linear expectations have a number of properties that are similar to those of conditional expectations. Many of these properties will be explored in the current section. The notation \(\hat{E}(y|x)\) for the linear expectation has been used for at least sixty years, see Doob (1953).

Similar to _PA_ Section 6.3, \(\hat{E}(y|x)\) is a function of \(x\), \(\mathrm{E}(y)=\mathrm{E}[\hat{E}(y|x)]\), and the _prediction error covariance matrix_ is

\[\mathrm{Cov}[y-\hat{E}(y|x)]\] \[=\mathrm{E}\left\{[(y-\mu_{y})-V_{yx}V_{xx}^{-}(x-\mu_{x})][(y- \mu_{y})-V_{yx}V_{xx}^{-}(x-\mu_{x})]^{\prime}\right\}\] \[=V_{yy}-V_{yx}V_{xx}^{-}V_{xy}.\]

In particular, the _partial correlation_ between two components of \(y\) given \(x\) is defined as the correlation between those components obtained from the prediction error covariance matrix. To simplify the discussion, it is assumed in the following that appropriate inverses exist. In particular,

\[B=V_{xx}^{-1}V_{xy},\]

so that the linear expectation is unique. First, we establish that linear expectation is a linear operator.

Proposition 1: Let \(A\) be an \(r\times q\) matrix and let \(a\) be an \(r\times 1\) vector. The best linear predictor of \(Ay+a\) based on \(x\) is

\[\hat{E}(Ay+a|x)=A\hat{E}(y|x)+a\,.\]

#### Exercise 2.1

Prove Proposition 1:

If we predict a random variable \(y\) from a set of random variables that includes \(y\), then the prediction is just \(y\). It is convenient to state this result in terms of the \(x\) vector.

Proposition 2: For \(x=(x_{1},\ldots,x_{p-1})^{\prime}\),

\[\hat{E}(x_{i}|x)=x_{i}\,.\]

**Exercise B.2.**  Prove Proposition B.1.2. Hint: By definition, \(\hat{E}(x_{i}|x)=\mu_{xi}+B^{\prime}(x-\mu_{x})\), where \(V_{xx}B=V_{xx_{i}}\). In this case, \(V_{xx_{i}}\) is the \(i\)th column of \(V_{xx}\).

Propositions B.1.1 and B.1.2 lead to the following corollary.

**Corollary B.1.3.**  If \(\beta\in{\bf R}^{p-1}\), then

\[\hat{E}(x^{\prime}\beta|x)=x^{\prime}\beta\,.\]

The next result is that a nonsingular affine transformation of the predictors does not change the linear expectation.

**Proposition B.1.4.**  Let \(A\) be a \((p-1)\times(p-1)\) nonsingular matrix and let \(a\) be a vector in \({\bf R}^{p-1}\); then,

\[\hat{E}(y|Ax+a)=\hat{E}(y|x).\]

Proof. Note that \({\rm Cov}(y,Ax+a)={\rm Cov}(y,Ax),{\rm Cov}(Ax+a)={\rm Cov}(Ax)\) and \((Ax+a)-{\rm E}(Ax+a)=A(x-\mu_{x})\). Then,

\[\hat{E}(y|Ax+a) = \mu_{y}+{\rm Cov}(y,Ax)[{\rm Cov}(Ax)]^{-1}A(x-\mu_{x})\] \[= \mu_{y}+V_{yx}A^{\prime}[AV_{xx}A^{\prime}]^{-1}A(x-\mu_{x})\] \[= \mu_{y}+V_{yx}A^{\prime}A^{\prime-1}V_{xx}^{-1}A^{-1}A(x-\mu_{x})\] \[= \mu_{y}+V_{yx}V_{xx}^{-1}(x-\mu_{x})\] \[= \hat{E}(y|x)\,.\]

The next proposition involves predictors that are uncorrelated with the random vector to be predicted.

**Proposition B.1.5.**  If \({\rm Cov}(y,x)=0\), then

\[\hat{E}(y|x)=\mu_{y}\,.\]

Proof. \(\hat{E}(y|x)=\mu_{y}+B^{\prime}(x-\mu_{x})\), where \(V_{xx}B=V_{xy}\). If \(V_{xy}=0\), the matrix \(B=0\) is the solution, thus giving the result. \(\Box\)

Again, all of these results are analogous to results for conditional expectations; see _PA_ Appendix D. In Proposition B.1.5, the condition \({\rm Cov}(y,x)=0\) is analogous to the idea, for conditional expectations, that \(y\) and \(x\) are independent. In Proposition B.1.4, the idea that \(A\) is nonsingular in the transformation \(Ax+a\) corresponds to taking an invertible transformation of the conditioning variable. Because of theseanalogies, any proofs that depend only on the five results given earlier have corresponding proofs for conditional expectations. This observation generalizes results from best linear predictors to best predictors. As mentioned in _PA_ Section 6.3 and earlier in this section, the best predictor of \(y\) based on \(x\) is \(\mathrm{E}(y|x)\). The reason for not using best predictors is that they require knowledge of the joint distribution of the random variables. Best linear predictors require knowledge only of the first and second moments. For Gaussian processes, best linear predictors are also best predictors.

##### Exercise b.3.

Assume that

\[\begin{bmatrix}y\\ x\end{bmatrix}\sim N\left(\begin{bmatrix}\mu_{y}\\ \mu_{x}\end{bmatrix},\begin{bmatrix}V_{yy}&V_{yx}\\ V_{xy}&V_{xx}\end{bmatrix}\right)\]

and that the covariance matrix is nonsingular, so that a density exists for the joint distribution. Show that

\[y|x\sim N\big{\{}\hat{E}(y|x),\mathrm{Cov}\big{[}y-\hat{E}(y|x)\big{]}\big{\}}\,.\]

In applications to the time domain models of Chapter 7, it is occasionally convenient to allow \(x\) to be an infinite vector \(x=(x_{1},x_{2},\ldots)^{\prime}\). For infinite vectors, the condition \(V_{xx}\beta=V_{xy}\) can be written rigorously as \(\sum_{j=1}^{\infty}\sigma_{ij}\beta_{j}=\sigma_{ij}\) for \(i=1,2,\ldots\), where \(V_{xx}=[\sigma_{ij}]\) and \(V_{xy}=[\sigma_{ij}]\). It is not difficult to see that all of the preceeding propositions continue to hold.

Finally, to derive the Kalman filter and later the joint prediction properties of principal components, some additional results are required. The first involves linear expectations based on a predictor consisting of two vectors with zero correlation. (I am not aware of any corresponding result for conditional expectations given two independent vectors.)

To handle three vectors simultaneously, we need some additional notation. Consider a partition of \(y\), say \(y^{\prime}=(y^{\prime}_{1},y^{\prime}_{2})\). Denote

\[\mathrm{Cov}(y)=\mathrm{Cov}\begin{pmatrix}y_{1}\\ y_{2}\end{pmatrix}=\begin{bmatrix}V_{11}&V_{12}\\ V_{21}&V_{22}\end{bmatrix}\]

and

\[\mathrm{Cov}(y_{i},x)=V_{ix}\qquad i=1,2\,.\]

Also, let

\[\mathrm{E}(y_{i})=\mu_{i}\qquad i=1,2\,.\]

The main result follows.

##### Proposition b.1.6.

If \(\mathrm{Cov}(y_{1},x)=0\), then

\[\hat{E}(y_{2}|y_{1},x)=\hat{E}(y_{2}|x)+\hat{E}(y_{2}|y_{1})-\mu_{2}\,.\]Proof. By definition,

\[\hat{E}(y_{2}|y_{1},x)=\mu_{2}+(V_{21},V_{2x})\begin{bmatrix}V_{11}&V_{1x}\\ V_{x1}&V_{xx}\end{bmatrix}^{-1}\begin{pmatrix}y_{1}-\mu_{1}\\ x-\mu_{x}\end{pmatrix}\,. \tag{1}\]

By assumption, \(V_{1x}=0\), and it follows that

\[\begin{bmatrix}V_{11}&V_{1x}\\ V_{x1}&V_{xx}\end{bmatrix}^{-1}=\begin{bmatrix}V_{11}&0\\ 0&V_{xx}\end{bmatrix}^{-1}=\begin{bmatrix}V_{11}^{-1}&0\\ 0&V_{xx}^{-1}\end{bmatrix}\,. \tag{2}\]

Substituting (2) into (1) gives

\[\hat{E}(y_{2}|y_{1},x) =\mu_{2}+V_{21}V_{11}^{-1}(y_{1}-\mu_{1})+V_{2x}V_{xx}^{-1}(x-\mu_ {x})\] \[=\hat{E}(y_{2}|y_{1})+\hat{E}(y_{2}|x)-\mu_{2}\,.\]

To simplify notation, let

\[e(y_{1}|x)\equiv y_{1}-\hat{E}(y_{1}|x)\,.\]

This is the prediction error from predicting \(y_{1}\) using \(x\). Our primary application of Proposition B.1.6 will be through the following lemma.

**Lemma B.1.7**: \[\operatorname{Cov}\big{[}e(y_{1}|x),x\big{]}=0\,.\]

Proof.

\[\operatorname{Cov}\big{[}e(y_{1}|x),x\big{]} =\operatorname{Cov}[y_{1}-\hat{E}(y_{1}|x),x]\] \[=\operatorname{Cov}[(y_{1}-\mu_{1})-V_{1x}V_{xx}^{-1}(x-\mu_{x}),x]\] \[=\operatorname{Cov}(y_{1}-\mu_{1},x)-V_{1x}V_{xx}^{-1} \operatorname{Cov}(x-\mu_{x},x)\] \[=V_{1x}-V_{1x}V_{xx}^{-1}V_{xx}\] \[=0\,.\]

Lemma B.1.7 leads to the key result.

**Proposition B.1.8**: \[\hat{E}(y_{2}|y_{1},x)=\hat{E}(y_{2}|x)+\operatorname{Cov}\left[y_{2},e(y_{1} |x)\right]\left\{\operatorname{Cov}\left[e(y_{1}|x)\right]\right\}^{-1}e(y_{1 }|x)\,.\]

Proof. First, note that \(\hat{E}(y_{1}|x)\) is an affine transformation of \(x\). Thus, we can write

\[\begin{bmatrix}y_{1}-\hat{E}(y_{1}|x)\\ x\end{bmatrix}=\begin{bmatrix}I&B\\ 0&I\end{bmatrix}\begin{bmatrix}y_{1}\\ x\end{bmatrix}-\begin{bmatrix}-\mu_{1}\\ 0\end{bmatrix}\]

[MISSING_PAGE_EMPTY:9909]

**Proposition B.1.10**: \[\operatorname{Cov}[y,y-\hat{E}(y|x)]=\operatorname{Cov}[y-\hat{E}(y|x)]\,.\]

Proof.

\[\operatorname{Cov}[y-\hat{E}(y|x)] = V_{yy}-V_{yx}V_{xx}^{-1}V_{xy},\] \[\operatorname{Cov}[y,y-\hat{E}(y|x)] = \operatorname{Cov}[y]-\operatorname{Cov}[y,\hat{E}(y|x)]\] \[= V_{yy}-\operatorname{Cov}[y,\mu_{y}+V_{yx}V_{xx}^{-1}(x-\mu_{x})]\] \[= V_{yy}-V_{yx}V_{xx}^{-1}V_{xy}\,.\]

**Exercise B.4**: For \(A\) nonsingular, let

\[A=\begin{bmatrix}A_{11}&A_{12}\\ A_{21}&A_{22}\end{bmatrix},\]

and let \(A_{1\cdot 2}=A_{11}-A_{12}A_{22}^{-1}A_{21}\). Show that if all inverses exist,

\[A^{-1}=\begin{bmatrix}A_{1\cdot 2}^{-1}&-A_{1\cdot 2}^{-1}A_{12}A_{22}^{-1} \\ \\ -A_{22}^{-1}A_{21}A_{1\cdot 2}^{-1}&A_{22}^{-1}+A_{22}^{-1}A_{21}A_{1\cdot 2}^{-1} A_{12}A_{22}^{-1}\end{bmatrix}\]

and that

\[A_{22}^{-1}+A_{22}^{-1}A_{21}A_{1\cdot 2}^{-1}A_{12}A_{22}^{-1}=\begin{bmatrix}A_ {22}-A_{21}A_{11}^{-1}A_{12}\end{bmatrix}^{-1}.\]

**Exercise B.5**: Let \(Y=(y_{1},\ldots,y_{n})^{\prime}\) and \(Y_{k}=(y_{n+1},\ldots,y_{n+k})^{\prime}\). Show that

\[\hat{E}\begin{bmatrix}y_{n+k+1}\big{|}Y\end{bmatrix}=\hat{E}\begin{bmatrix} \hat{E}(y_{n+k+1}|Y,Y_{k})\big{|}Y\end{bmatrix}.\]

Hint: Use the definition of \(\hat{E}(\cdot|\cdot)\), Proposition B.1.1, and results on the inverse of a partitioned matrix.

### Irrelevance of Units in Best Multivariate Prediction

Suppose you want to predict the size of a dog house, \(y=(\text{height,length,width})^{\prime}\), from the size of the dog \(x\). It would make sense to measure the components of \(y\) in some common unit, like centimeters. If we do, our standard measure of prediction error \([y-f(x)]^{\prime}[y-f(x)]\) is measured in square centimeters. If, however, we measured height in kilometers but length and width in centimeters the numbers in \(y_{1}\) would be 5 orders of magnitude smaller than those for \(y_{2}\) and \(y_{3}\). You mightthink that would affect the best predictor or best linear predictor. In fact, based on the prediction criterion, we would expect it to be less important to predict \(y_{1}\) well, because the squared error that it would contribute to the overall prediction criterion will probably be tiny compared to the squared prediction errors associated with \(y_{2}\) and \(y_{3}\).

It would be reasonable, when the components of \(y\) involve different units, to minimize the prediction criterion

\[\mathrm{E}\left\{[y-f(x)]^{\prime}V_{yy}^{-1}[y-f(x)]\right\}.\]

One nice thing about this prediction criterion is that it is unitless. The units associated with \(V_{yy}^{-1}\) cancel with the units associated with \(y\) and \(f(x)\).

Quite remarkably, none of these considerations have any affect on the best predictor or the best linear predictor. Write \(V_{yy}^{-1}=Q^{\prime}Q\) where \(Q\) is nonsingular. Then

\[\mathrm{E}\left\{[y-f(x)]^{\prime}V_{yy}^{-1}[y-f(x)]\right\}\\ =\mathrm{E}\left(\left\{Q[y-f(x)]\right\}^{\prime}\left\{Q[y-f(x)]\right\} \right)=\mathrm{E}\left\{[Qy-\tilde{f}(x)]^{\prime}[Qy-\tilde{f}(x)]\right\}\]

where \(\tilde{f}(x)\equiv Qf(x)\). Because \(Q\) is invertible, it follows that prediction of \(y\) under the new \(V_{yy}^{-1}\) criterion is equivalent to prediction of \(Qy\) under the old criterion. In particular, the best predictor \(m_{V}(x)\) under the new criterion will be \(Q^{-1}\) times the best predictor of \(Qy\) under the old criterion. However, the best predictor of \(Qy\) under the old criterion is \(E(Qy|x)\) so the best predictor of \(y\) under the new criterion is \(Q^{-1}E(Qy|x)=Q^{-1}QE(y|x)=E(y|x)\) which was the best predictor of \(y\) under the old criterion. Similar arguments establish that the best linear predictor under the new criterion is also the same as the best linear predictor under the old criterion. Moreover, the same arguments hold if we replace \(V_{yy}^{-1}\) in the new criterion with any positive definite matrix.

## Appendix C Residual Maximum Likelihood

**Abstract** Residuals have singular distributions, so they do not have density/likelihood functions. This appendix addresses issues of how to maximize a likelihood function that does not exist.

### 1 Maximum Likelihood Estimation for Singular Normal Distributions

Maximum likelihood estimation involves maximizing the joint density of the observations over the possible values of the parameters. This assumes the existence of a density. A density cannot exist if the covariance matrix of the observations is singular, as is the case with residuals. We consider an approach that allows maximum likelihood estimation and show some uniqueness properties of the approach.

Suppose \(Y\) is a random vector in \(\mathbf{R}^{n}\), \(\operatorname{E}(Y)=\mu\), \(\operatorname{Cov}(Y)=V\). If \(r(V)=t<n\), then, as seen in \(PA\) Lemma 1.3.5, \(\Pr[(Y-\mu)\in C(V)]=1\) and \(Y-\mu\) is restricted to an \(t\)-dimensional subspace of \(\mathbf{R}^{n}\). It is this restriction of \(Y-\mu\) to a subspace of \(\mathbf{R}^{n}\) (with Lesbesgue measure zero) that causes the nonexistence of the density. We seek a linear transformation from \(\mathbf{R}^{n}\) to \(\mathbf{R}^{t}\) that will admit a density for the transformed random vector. The linear transformation should not lose any information and the MLEs should, in some sense, be the unique MLEs.

Suppose we pick an \(n\times t\) matrix \(Q\) with \(C(Q)=C(V)\). \(Q^{\prime}Y\) together with a nonrandom function of \(Y\) can reconstruct \(Y\) with probability 1. Let \(M_{V}\) be the perpendicular projection operator onto \(C(V)\), then \(Y=M_{V}Y+(I-M_{V})Y\). \(M_{V}Y=Q(Q^{\prime}Q)^{-1}Q^{\prime}Y\) is a function of \(Q^{\prime}Y\) while \((I-M_{V})Y=(I-M_{V})\mu\) with probability 1, because \(\operatorname{Cov}[(I-M_{V})Y]=(I-M_{V})V(I-M_{V})=0\).

We would also like to see that \(\operatorname{Cov}(Q^{\prime}Y)=Q^{\prime}VQ\) is nonsingular, so that a density can exist. Since \(C(Q)=C(V)\) and \(V\) is symmetric, \(V=QTQ^{\prime}\) for some symmetric matrix \(T\). If \(T\) is nonsingular, then \(Q^{\prime}VQ=(Q^{\prime}Q)T(Q^{\prime}Q)\) is nonsingular, because both \(T\) and \(Q^{\prime}Q\) are nonsingular. We now show that \(T\) is nonsingular. Suppose \(T\) is singular. Then there exists \(d\in\mathbf{R}^{t}\), \(d\neq 0\), so that \(Td=0\). Since \(r(Q)=t\), there exists\(b\neq 0\) such that \(d=Q^{\prime}b\). Because \(Q^{\prime}b=Q^{\prime}M_{V}b\), we can assume that \(b\in C(V)\). Now, \(Vb=QTQ^{\prime}b=QTd=0\). However, for \(b\in C(V)\), \(Vb=0\) can only happen if \(b=0\), which is a contradiction.

As mentioned earlier, there is little hope of estimating the entire matrix \(V\). A more manageable problem is to assume that \(V\) is a function of a parameter vector \(\theta\). It should also be clear that in order to transform to a nonsingular random variable, we will need to know \(C(V)\). This forces us to assume that \(C(V(\theta))\) does not depend on \(\theta\).

Suppose now that \(Y\sim N(\mu,V(\theta))\); then \(Q^{\prime}Y\sim N(Q^{\prime}\mu,Q^{\prime}V(\theta)Q)\). The density of \(Q^{\prime}Y\) is

\[f(Q^{\prime}Y|\mu,\theta)\] \[=\frac{1}{(2\pi)^{\frac{f}{2}}}\frac{1}{|Q^{\prime}V(\theta)Q|^{ \frac{1}{2}}}\exp[-(Q^{\prime}Y-Q^{\prime}\mu)^{\prime}[Q^{\prime}V(\theta)Q] ^{-1}(Q^{\prime}Y-Q^{\prime}\mu)/2]\] \[=(2\pi)^{-r/2}|Q^{\prime}V(\theta)Q|^{-1/2}\exp[-(Y-\mu)^{\prime} Q[Q^{\prime}V(\theta)Q]^{-1}Q^{\prime}(Y-\mu)/2].\]

The MLEs are obtained by maximizing this with respect to \(\mu\) and \(\theta\). A direct consequence of Proposition C.1.1 below is that maximization of \(f(Q^{\prime}Y)\) does not depend on the choice of \(Q\).

**Proposition C.1.1.** If \(Q\) and \(Q_{0}\) are two \(n\times t\) matrices of rank \(t\) and \(C(Q)=C(Q_{0})\), then

(1) for some scalar \(k\), \(k|Q^{\prime}VQ|=|Q^{\prime}_{0}VQ_{0}|\);

(2) \(Q[Q^{\prime}VQ]^{-1}Q^{\prime}=Q_{0}[Q^{\prime}_{0}VQ_{0}]^{-1}Q^{\prime}_{0}\) when the inverses exist.

Proof. Since \(C(Q)=C(Q_{0})\) and both are full rank, \(Q_{0}=QK\) for some nonsingular \(K\).

(1) \(|Q^{\prime}_{0}VQ_{0}|=|K^{\prime}Q^{\prime}VQK|=|K|^{2}|Q^{\prime}VQ|\). Take \(k=|K|^{2}\).

\[Q_{0}[Q^{\prime}_{0}VQ_{0}]^{-1}Q^{\prime}_{0} = QK[K^{\prime}Q^{\prime}VQK]^{-1}K^{\prime}Q^{\prime}\] \[= QKK^{-1}[Q^{\prime}VQ]^{-1}(K^{\prime})^{-1}K^{\prime}Q^{\prime}\] \[= Q[Q^{\prime}VQ]^{-1}Q^{\prime}. \tag{2}\]

**Corollary C.1.2.**\(f(Q^{\prime}Y|\mu,\theta)=k^{-1/2}f(Q^{\prime}_{0}Y|\mu,\theta)\).

### Residual Maximum Likelihood Estimation

Residual maximum likelihood (REML) estimation involves finding maximum likelihood estimates of covariance parameters from the distribution of the residuals. This allows for estimation of the covariance parameters without the complication of the fixed effects. We will see that this procedure is equivalent to the restricted maximum likelihood methods described in Section 4.3.

An apparent problem with this idea is that of defining the residuals when \(V(\theta)\) is unknown. We show that any reasonable definition of residuals gives the same answers.

Consider the model

\[Y=X\beta+e,\quad e\sim N(0,V(\theta)),\]

\(\theta=(\theta_{1},\ldots,\theta_{s})^{\prime}\), As discussed in _PA_ Section 10.2, the only reasonable linear unbiased estimates of \(X\beta\) are of the form \(AY\), where \(A\) is some projection operator onto \(C(X)\). The residuals can be defined as \((I-A)Y\). The distribution of the residuals is

\[(I-A)Y\sim N(0,(I-A)V(\theta)(I-A)^{\prime}).\]

\(V(\theta)\) is assumed nonsingular, so \(C((I-A)V(\theta)(I-A)^{\prime})=C(I-A)\). Let \(r(X)\equiv r\), so \(r(I-A)=n-r\). For an \(n\times(n-r)\) matrix \(Q\) with \(C(Q)=C(I-A)\), a residual MLE of \(\theta\) maximizes

\[f(Q^{\prime}(I-A)Y|\theta)=(2\pi)^{-(n-s)/2}|Q^{\prime}(I-A)V( \theta)(I-A)^{\prime}Q|^{-1/2}\\ \times\exp[-Y^{\prime}(I-A)^{\prime}Q[Q^{\prime}(I-A)V(\theta)(I- A)^{\prime}Q]^{-1}Q^{\prime}(I-A)Y/2].\]

We will show that this depends on neither \(A\) nor \(Q\) by showing that \(C[(I-A)^{\prime}Q]=C(X)^{\perp}\) and appealing to Section 1.

##### Proposition c.2.1.

\(C((I-A)^{\prime}Q)=C(X)^{\perp}\).

Proof. Clearly, \(Q^{\prime}(I-A)X=0\), so \(C((I-A)^{\prime}Q)\subset C(X)^{\perp}\). The rank of \(C(X)^{\perp}\) is \(n-r\), so it is enough to show that the rank of \((I-A)^{\prime}Q\) is \(n-r\). Since \((I-A)^{\prime}Q\) is an \(n\times(n-r)\) matrix it is enough to show that for any \(d\in{\bf R}^{n-r}\), \((I-A)^{\prime}Qd=0\) implies \(d=0\). Since \(C(I-A)=C(Q)\), \(Qd=(I-A)c\) for some \(c\). If \((I-A)^{\prime}Qd=0\), then \(c^{\prime}(I-A)^{\prime}Qd=d^{\prime}Q^{\prime}Qd=0\); so \(Qd=0\). Since \(Q\) is an \(n\times(n-r)\) matrix of full column rank, \(d=0\). 

In Section 4.3, REML is defined as maximum likelihood estimation from \(B^{\prime}Y\), where \(r(B)=n-r\) and \(B^{\prime}X=0\). That definition is equivalent to the one used here. Choose \(A=M\). Then \(I-M\) is the perpendicular projection operator onto \(C(X)^{\perp}\). Take \(Q=B\) so that \(Q^{\prime}(I-M)Y=B^{\prime}Y\).

## Reference

Doob, J.L. (1953). _Stochastic Processes_. John Wiley and Sons, New York.

## Author Index

**A**

Adler, R.J., 338

Aitchison, J., 457, 458, 466

Akaike, H., 286

Albert, A., 522

Aldrich, J., 504

Allen, G.I., 121

Alonso, F.J., 322

Anderson, J.A., 522

Anderson, T.W., 357, 458

Andrews, D.F., 475

Ansley, C., 121

Ansley, C.F., 313

Armstrong, M., 3, 351

Arnold, S.F., 357

**B**

Banerjee, S., 322

Bartlett, M.S., 287

Bedrick, E.J., 185, 191, 193

Berger, J.O., 463, 499

Berke, O., 322

Berlinet, A., 87, 103, 104

Bibby, J.M., 357

Bivand, R., 87

Bloomfield, P., 197

Bose, R.C., 383

Box, G.E.P., 241, 248, 276, 284, 420, 453, 475

Branscium, A., viii, 32, 71, 322, 463

Bray, A., 136

Breiman, L., 49, 206, 325

Brillinger, D.R., 197

Brockwell, P.J., 197,.248, 250, 275, 285, 288

Bucy, R.S., 307

Buhlmann, P., 69, 87

**C**

Carlin, B.P., 322

Carroll, R.J., 25, 124, 157

Casella, G., 145, 161

Chatfield, C., 248

Chen, Z., 534

Christensen, R., vii, viii, ix, xv, xvi, 1, 20, 25, 26, 29, 32, 39, 45, 55, 59, 61, 70, 71, 73, 87, 125, 127, 129, 137, 141, 143, 157, 166, 173, 185, 188, 191, 193, 217, 273, 284, 288, 322, 327, 332, 334, 343, 357, 367, 371, 378, 389, 391, 402, 403, 408, 414, 423, 443, 451, 458, 461, 472, 492, 503, 509, 525-527, 546

Christensen, W.F., 357

Clayton, M.K., 167, 289

Cliff, A., 321

Clyde, M., 73

Cox, D.R., xiv, 443, 475

Craven, P., 120

Cressie, N.A.C., 145, 150, 197, 313, 321, 322, 330, 337, 344, 347, 351, 352

Crowder, M.J., 446

**D**

Danford, M.B., 420

David, M., 321

Davis, R.A., 197,.248

de Barra, G., 96

Delfiner, P., 327

Dempster, A.P., 312

Diamond, P., 338

Diderich, G.T., 307

Diggle, P.J., 248, 322, 446

Dillon, W.R., 357Dixon, W.J., 571

Doob, J.L., 206, 207, 588

Draper, N., 272, 273

Draper, N.R., 60, 66

Dunsmore, I.R., 457

Durbin, J., 307

Eaton, M.L., 149, 154, 357

Edwards, D., 343

Efromovich, S., 8, 18, 19, 25, 38, 55

Efron, B., 473

Eilers, P.H.C., 32

El-Bassiouni, Y., 185

Eubank, R., 87

Eubank, R.L., 1

Everitt, B., 357

Fan, J., 83

Feldt, L.S., 414

Ferguson, T.S., 136, 154

Fisher, R.A., 458, 481

Fitzmaurice, G.M., 446

Friedman, J.H., 60, 87, 458, 467, 508

Fuller, W.A., 197, 248, 294

G
Geisser, S., 289, 414, 436, 466, 472

Gelfand, A.E., 322

George, E.I., 73

Geweke, J.F., 561

Gnanadesikan, R., 357, 420, 475, 533, 566

Goldberger, A.S., 322

Goldsmith, J., 18, 54

Goldstein, M., 18, 357

Golub, G., 121

Gomez-Rubio, V., 87

Goodall, C., 322

Gotway, C.A., 322

Green, P.J., 38, 65, 66

Greenhouse, S.W., 414

Gu, C., 87, 93, 103, 104, 110, 111, 114

Gupta, N.K., 313

Guttorp, P., 322

Halmos, P.R., 93

Halvorsen, K.B., 322

Hand, D.J., 446, 458, 474

Handcock, M.S., 322

Hannan, E.J., 197

Hanson, T.E., viii, 32, 72, 322, 463

Harrison, P.J., 307, 313

Hart, J.D., 1, 18, 20

Harville, D.A., 93, 149, 150, 191, 327

Hastie, T., 60, 69, 87, 458, 472, 508, 517

Hawkins, D.M., 351

Heagerty, P., 446

Heath, M., 121

Heck, D.L., 375

Heckman, N., 87

Helton, J., 87

Henderson, C.R., 179

Hodges, J.S., 136, 161, 167, 322

Hoerl, A.E., 60, 66, 109

Hotelling, H., 538

Hothorn,T., 357

Huang, H.-C., 322

Hughes, H.M., 420

Hughes, J.B., 420

Huijbregts, Ch.J., 321, 340, 342, 349

Hurvich, C.M., 20, 289

Huynh, H., 414

Hyvarinen, A., 534

I Isaaks, E.H., 321

Jenkins, G.M., 241, 248, 276

Jennings, D.E., 289

Jeske, D.R., 150

Jiang, J., 161

Johnson, D.E., 45

Johnson, R.A., 357, 458, 537, 548, 554

Johnson, W., viii, 32, 72, 322, 463

Jolicoeur, P., 419, 548

Jolliffe, I.T., 534, 547, 551

Jones, R.H., 313

Joreskog, K.G., 560

Journal, A.G., 321, 349

Kackar, R.N., 150

Kalbfleisch, J.D., xiv

Kalisch, M., 69, 87

Kalman, R.E., 307

Kanazawa, M., 533

Karhunen, J., 534

Kennard, R., 60, 66, 109

Kent, J.T., 357

Kenward, M.G., 129

Khatri, C.G., 425, 430

Khuri, A.I., 161

Kitanidis, P.K., 321, 322, 330, 348, 349

[MISSING_PAGE_FAIL:611]

Schwarz, G., 286

Searle, S.R., 145, 147, 161

Seber, G.A.F., 273, 357, 372, 375, 376, 391, 458, 533, 560

Seely, J.F., 185

Sell, G., 94, 100

Shang, H.L., 18, 54

Sherman, M., 322

Shumway, R.H., 197, 241, 248, 307, 312

Silverman, B.W., 38, 65, 66, 69, 87

Singleton, K.J., 561

Singpurwalla, N.D., 305, 307

Sinha, B.K., 161

Smith, A.F.M., 18

Smith, H., 272, 273, 420

Smith, M., 73

Sneeringeer, C., 474

Srivastava, R.M., 321

Stein, M.L., 321, 322, 338, 349

Stevens, C.F., 307

Stoffer, D.S., 197, 248, 307, 312

Storlie, C., 87, 109, 110

Sun, S.-K., 137

Swiler, L., 87

**T**

Tarpey, T., 127, 371

Tawn, J.A., 322

Thomas-Agnan, C., 87, 103, 104

Thompson, G.H., 564

Thurstone, L.L., 564

Tibshirani, R., 60, 69, 87, 109, 458, 508

Tsai, C.-L., 20, 289

Tukey, J.W., 49, 195, 391

**V**

van de Geer, S., 69, 87

van Houwelingen, J.C., 157

van Nostrand, R.C., 60

**W**

Wahba, G., 87, 93, 111, 120, 121

Wainwright, M.J., 69, 87

Waldmeier, M., 241

Waller, L.A., 322

Wallis, J.R., 322

Wand, M.P., 87, 111, 167

Ware, J.H., 446

Warner, J.L., 475

Warnes, J.J., 348

Watkins, A.J., 348

Wecker, W., 121

Wedderburn, R.W.M., xiv

Wegman, E.J., 307

West, M., 193, 248, 307, 313

Whittaker, J., 343

Whittle, P., 341

Wichern, D.W., 357, 458, 537, 548, 554

Wikle, C.K., 197, 313, 322

Wild, C.J., 273

Williams, C.K.I., 331

Williams, E.J., 515

Williams, J.S., 562

Wilson, S., 458

Wolfinger, R.D., 449, 450

Wood, S., 111

Wu, S.M., 299

**Y**

Yandell, B., 110

Young, N., 94

**Z**

Zammit-Mangion, A., 322

Zeger, S.L., 446

Zhang, H., 110

Zhu, M., 87, 517

Zimmerman, D.L., 150, 327, 338, 348, 352

Zimmerman, M.B., 352

Zou, H., 109

[MISSING_PAGE_EMPTY:9919]

Correlation coefficient, 317 partial, 317 Correlation function, 248 Covariance function, 42, 198, 207, 324 exponential, 341 Matern, 341 range, 341 spherical, 340 Covariance modeling, 414 Covariance selection, 343 Covariance stationary, 198 Cov(*), 3 Cross-validation, 60, 66, 472 Crossperiodogram, 238 Curse of dimensionality, 39 \(C(X)\), 3 \(C(X)^{\perp}\), 3

**D**

**d\({}_{x}\)**_F_(_c_), 575 Daniel

kernel, 202 spectral window, 202 Daniell kernel modified, 229 Derivative

definition, 576 Derivative notation, 575 Determinant, 461 Determinant notation, 134, 364, 577 Deviance, 505 Direct sum, 104 Discrimination, 457 Distance, 94 Dual feasibility, 580 Dual problem, 583 Durbin-Levinson algorithm, 251

**E**

\(\text{E}(\cdot)\), 3 Elastic net penalty, 83 Empirical estimate, 127, 148 Equal covariances, 436 Equally spaced data, 2 Equivalence of test statistics, 378 Ergodic, 338 Error statistic \(E\), 366 Estimating equations, 135, 139 Estimation Bayesian, 71 Bayesian, 166 best linear unbiased (BLUE), 131 estimating equations, 135, 139 Henderson's method, 179 maximum likelihood, 135 maximum likelihood, 134 method of moments, 135, 139 minimum norm quadratic unbiased (MINQUE), 143 minimum variance unbiased, 181 residual maximum likelihood (REML), 597 restricted maximum likelihood (REML), 137 restricted maximum likelihood (REML), 597 unbiased for variance, 179 uniformly minimum variance unbiased (UMVU), 181 variance unbiased, 179 Exponential covariance function, 341

**F**

Factor loadings, 556 Fast Fourier transform, 202 Father wavelet, 6 FFT, 202 Filter autoregressive, 233 causal linear, 230 general linear, 230 general recursive, 232 high pass, 231 low pass, 231 simple recursive, 233 First difference operator, 229 Fixed effects, 163 Fixed effects model, 161 Forecasting, 262 Forward shift operator, 284 Frequency response function, 230 Functional data analysis, 446 Functional predictors, 53 Fundamental Theorem of Least Squares Estimation, 3

**G**

Gaussian process, 199, 325 Gaussian process regression, 331 Generalized additive models, 40 Generalized covariance functions, 326 Generalized cross-validation, 60, 66 Generalized inverse, 3 Generalized least squares, 38 Generalized least squares estimate, 127 Generalized likelihood ratio test, 181 Generalized linear model, 506 Generalized multivariate analysis of variance, 415, 423 Generalized multivariate linear model, 415, 423

[MISSING_PAGE_FAIL:615]

Method of moments, 135, 139

Mexican hat wavelet, 6

Minimum norm quadratic unbiased (translation invariant) estimation, 143

Minimum variance quadratic unbiased translation invariant estimate, 146

Minimum variance unbiased estimate, 181

MINQUE, 143

MINQUE equations, 144

MIVQUE, 146

Mixed model, 161, 162

Mixed model equations, 165

MLE, 127

Models

fixed effects, 161

mixed, 161, 162

random effects, 161

Mother spline, 30

Mother wavelet, 6

Moving average, 229

Moving average models, 257

Multiple comparisons, 379

Multiple testing, 379

N

Nadaraya-Watson kernel estimate, 38

\(N(\cdot,\cdot)\), 3

Nested models, 183

Newton-Raphson, 134

Noisy, 59

Norm, 94, 95

Normed vector space, 94

Notation

basic, 3

Nugget effect, 335

O

Offset vector, 144

Ofversten's tests, 188

\(o\left( a_{n}\right)\), 575

Ordinary kriging, 323

Orthogonal, 95

polynomials, 63

Orthogonal complement, 3

Orthogonal series, 1, 6

Orthonormality condition, 18

Overfitting, 41, 53, 59

P

PA, vii

Partial correlation, 588

Partial correlation coefficient, 317

Partial correlation function, 249

Partial correlations, 362

Partial covariance, 248

Partial likelihood, 526

Partitioned linear model, 3

Partitioned matrices, 593

\(P(\cdot)\), 201

Penalized estimation, 60

Penalized least squares, 59

Penalized likelihood, 73

Penalized likelihood estimates, 83

Penalized maximum likelihood estimates, 506

Penalized minimum deviance estimates, 506

Penalty function, 59

Periodogram, 201

Perpendicular, 3

Perpendicular projection, 96

Perpendicular projection operator, 3, 98

Pillai's trace, 375

Plug-in estimate, 127, 148

Ppo, 3, 140

Predicted residual sum of squares, 472

Prediction

best linear predictor (BLP), 130

best linear unbiased predictor (BLUP), 131, 164

best predictor (BP), 130

Prediction regions, 378

Predictor, 131

PRESS, 472

Prevalence, 525

Primal feasibility, 581

Principal axes factor estimation, 564

Principal factor estimation, 564

Prediction operator, 127

Proximal support vector machine, 514

Pukelsheim, Friedrich, 145

Pure error, 41

QDA, 466

Quadratic discriminant analysis, 466

Quadratic discrimination, 462

Quadratic forms

variance, 147

Quadratic optimization, 580

R.k., 40

Radial basis function, 42

Random effects, 162

Random effects model, 161

Random field, 323

Random regression coefficients, 436

Rank, 3

Rao's simple covariance structure, 435\(r(\cdot)\), 3 Recursive partitioning, 46 Reduced covariance matrix, 558 Regularized estimation, 60 REML, 137, 327, 597 REML equations, 138, 142 degenerate, 184 Repeated measures, 440 Reproducing kernel, 40, 100 Reproducing kernel Hilbert space, 40, 100 Reproducing property, 100 Residual maximum likelihood, 137, 597 Restricted maximum likelihood, 137, 597 Resubstitution method, 471 Ricker wavelet, 6 Ridge regression, 63 Ridge trace, 60, 66 Riesz Representation Theorem, 99, 100 RKHSs, 40 Roy's union-intersection principle, 372

**S**

Sandwich estimators, 178 SCAD, 83 Scheffe's method, 411 SCS, 435 Second-order stationary, 198, 325 Seely, Justus, 145 Semivariogram, 325 sill, 342 Separability, 327 Separation, 457 Sill, 342 Simple covariance structure, 435 Simple kriging, 323 Simple recursive filter, 233 Singular normal distribution, 595 Singular value decomposition, 104 Smoothly clipped absolute deviation penalty, 83 Spanning functions, 4 Spatial prediction, 321 Specific factor, 557 Specificity, 558 Specific variance, 558 Spectral density, 202, 211 Spectral distribution function, 207, 209 Spectral representation theorem, 207, 232 Spherical covariance function, 340 Splines, 5 Squared sample coherence function, 237 Squared sample imaginary coherence function, 237 Squared sample real coherence function, 235 Standard linear model, 3 Stationarity second-order, 325 second order, 198 strict, 198 weak, 198, 325 Stationarity in the wide sense, 198 Stationary, 198, 324 Stepwise discrimination, 475 Stochastic process spatial, 323 time series, 195 Stone-Weierstrass theorem, 4 Strictly stationary, 198, 324 Support of a function, 25 Support vector machine, 506 Support vector machines, 495 Support vectors, 520 SVM, 495, 506 **T**

Taylor's approximation, 575, 576 Testing additional information, 416 Tests generalized likelihood ratio, 181 Ofversten's, 188 variance components, 185 Wald, 185 Test statistics equivalence, 378 Thresholding, 73 Time-dependent covariates, 443 Total communicability, 558 Total probability of misclassification, 499 Trace plot, 60, 66 tr(\(\cdot\)), 3 Tri-weight, 38 Triangular array, 5 Tuning parameter, 60

**U**

UMVU, 127, 181 Unbiased estimate, 179 Unbiased predictor, 131 Uniformly minimum variance unbiased estimate, 127, 181 Union-intersection principle, 372 Unique factor, 557 Uniqueness, 558 Universal kriging, 323

**V**

\(V^{-1}(\theta)\), 126 Variance component, 161 Variance component models, 134Variance components, 161

Variance estimation

variance component models, 134, 179

Variogram, 325, 326

Vec operator, 578

\(\mid\cdot\mid\)

determinant, 134, 577

\(\mid\cdot\mid\)

volume (area), 330

\(\mid\mid\cdot\mid\mid\), 3

\(\widetilde{V}\), 128

\(\widetilde{V}\)

**W**

Wavelet, 4, 6

Weak stationary, 198, 325

Weighted least squares, 38

Weighting function, 202

White noise, 211, 216, 247, 334

Wilks's \(\Lambda\), 372

Wishart distributions, 370

**X**

\(x_{i}^{\prime}\), 3

\(X_{j}\), 3

\(\mid\cdot\mid\mid\), 3

**Y**

Yule-Walker equations, 255

**Z**

\(z_{i}^{\prime}\), 3

\(Z_{j}\), 3