[MISSING_PAGE_EMPTY:1]

Springer Texts in Statistics

Series editors

R. DeVeaux

S.E. Fienberg

I. Olkin

More information about this series at [http://www.springer.com/series/417](http://www.springer.com/series/417)Galen R. Shorack

Probability for Statisticians

Second Edition

\begin{tabular}{l l} \multicolumn{2}{l}{Galen R. Shorack} & \multicolumn{1}{l}{Editorial Boards} \\ Department of Statistics & George Casella \\ University of Washington & Department of Biometrics \\ Seattle, WA & Cornell University \\ USA & Ithaca, NY \\ USA & USA \\ Stephen Fienberg \\ Department of Statistics & Carnegie Mellon University \\ Pittsburgh, PA \\ USA & Ingram Olkin \\ Department of Statistics & Stanford University \\ Stanford, CA & USA \\ \end{tabular}

\begin{tabular}{l l} ISSN 1431-875X & ISSN 2197-4136 (electronic) \\ Springer Texts in Statistics & ISBN 978-3-319-52206-7 & ISBN 978-3-319-52207-4 (eBook) \\ DOI 10.1007/978-3-319-52207-4 & Library of Congress Control Number: 2017941074 \\ \end{tabular}

\begin{tabular}{l l} Library of Congress Control Number: 2017941074 \\ \end{tabular}

1st edition: \(\copyright\) Springer-Verlag New York, Inc. 2000

2nd edition: \(\copyright\) Springer International Publishing AG 2017

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

### Printed on acid-free paper

This Springer imprint is published by Springer Nature

The registered company is Springer International Publishing AG

The registered company address is: Gewerbestrasse 11, 6330 Cham, SwitzerlandThis dedication to my parents is offered as a permanent gift to my family, so that my parents' basic history may not be lost.

_To my Father--who loved me_

\(\bullet\) Theodore James Shorack (August 20, 1904-July 31, 1983) Charleston, WV

\(\bullet\) With only a third-grade education, he taught me that mathematics is fun.

\(\bullet\) Effie, Minnesota; the Aleutian Islands; Eugene, Oregon. Homesteader, boxer, carpenter and contractor. He loved the mathematics of his carpenter's square.

\(\bullet\) He loved his children with all his being.

_To my Mother--who \(praised\) me_

\(\bullet\) Marcella (Blaha) Shorack (November 4, 1902-April 25, 1987) St. Paul, MN

\(\bullet\) "What you don't have in your head, son, you'll have to have in your feet"

\(\bullet\) Effie, Minnesota; Battle Creek, Michigan; Flagstaff, Arizona; Eugene, Oregon. Home-steader and dedicated teacher. She had a heart for her troubled students.

\(\bullet\) She cared dearly about whom and what her children would become.

Theodore J. Shorack and Marcella (Blaha) Shorack, wed 6/12/1929

We, their descendents, are entrusted with their memory.

1. Theodore James Shorack Jr. (1929-1966; Vietnam pilot, my boyhood hero) and Elva (Buehler) Shorack (1925)

Candace (1953).

Helen (1989)--Vietnam

Kathleen (1955), and Walter Petty (1953)

Elizabeth (1987), Angela (1990)

Theodore III (or Todd) (1957), and Karie (Lott) Shorack (1960)

Theodore IV (1985), Wesley (1988)

John (1960), and Birgit (Funck) Shorack (1958)

Johanna (1990), Marna (1993), John Mark (1995)

2. Charlene (or Chari) Rose Boehnke (1931-2012) and George Boehnke (1931-2014)

Michael (1956), and Betsy Foxman (1955)

David (1985), Kevin (1987), Richard (1989)

Richard (1958-1988)

Barbara (1961), and Terrance Aalund (1952)

Katherine (1989), Daniel (1991), Gary (1995)

3. Roger Allen Shorack (1934) daughter Stefani (1968)

and Heather (Cho) Shorack (1949)

4. Galen Richard Shorack (1939; the author) and Marianne (Crabtree) Shorack (1938); Sandra Ney Wood (1943)

Galen (or GR) (1964), and Lanet (Benson) Shorack (1967)

Nikolai (1999), Luca (2002), Nadia (2005)

Bart (1966), and Kerri (Winkenweder) Shorack (1968)

Landon (1995), Kyle (1998), Titus (2001)

Matthew (1969), and Julie (Mitchell) Shorack (1969)

(Isaac (1999), Thomas (2000), Alexia (2001))--Uganda, Tessa (2005)My grandparents' generation

* Peter Shorack and Anna Milici (immigrants from Milica Celo, near the Plitvice Lakes) (They married in 1890, and came to the US in 1898.) Nicholas died by age 10 Annie wed Ivan Harrington (Archie (wed Ellen; 5) +9) William wed Kate (William Jr. (wed Virginia; 2)) Amelia wed Charlie Lord Theodore James Shorack (my father) Jenny wed Godfrey Knight (5)
* Frank Blaha Jr. (Chicago 3/24/1893, cooper) and Marcella Nekola (immigrant from the Prague area as a child) Marcella Barbara Blaha (my mother) Marie (1904) wed Carlos Halstead (Carlos, Gilbert, Christine) George (1906) (my father's dear friend) wed Carmen Jirik Julia (1908) wed Lyle Dinnell Nan (1911) (first child born on the Effie, Minnesota homestead) Rose (1913) Helen (1916) Carol (twin) (1918) wed Ashley Morse (Leigh (wed Kent; 2), Laurel) Don (twin) (1918) wed Jean Dora Frank J. Blaha Sr. (1850, lumber mill and railroad) wed Rose Hrda (1852) 1/30/1872. They had immigrated separately to the USA in their 20s; he from the Prague area (Frank Jr. (my mother's father, homesteader), James (married Aunt Anna Nekola, printer), Joseph, Agnes.). Frank Jr. was an inept farmer, but he enjoyed his books and raised educated daughters. Thomas Nekola (wagon maker in the Prague area) married Mary Tomasek. (Barbara, Anna (married Uncle James), Marcella (my mother's mother), Albert, Pete, Frances) Peter Shorack (an only child) seems to have been "on the move" when he arrived in Milici, but his origin is unclear. Was he fleeing a purge in the east (he said) or Austro-Hungarian conscription (his wife said)? His parentage is unknown. He died from alcohol when my father was nine. Anna Milici was the fourth of five children of Maximo and Martha Milici. Maximo (appropriately 6'9") came to the USA, but fled home when two attackers did not survive. Later, his neighbors there banded together and killed him with pitchforks. Anna visited Milici with her children when my father was five, but had to leave her children there for one year. She had hidden enough to get herself home, thus thwarting Peter's efforts to strand his family. A hard woman in most ways, she used her gun to run off robbers (when linguistic Peter was running a railroad gang) and poachers (after she was alone on the homestead). My father worked in logging camps as a young kid; his mother and older brother would not allow him to go to school. He hopped a freight when he was bigger, but "hammer toes" allowed him to negate his mistake of an army enlistment. Back home, he trained religiously as a boxer and a fighter. He thus "survived" his older brother, boxed the county fairs with George, defended his interests in my mother, won two professional fights (but lost two teeth), fought (with some success) for his full winter logging earnings (each spring the same companies would go bankrupt, leaving their debts unpaid). WW II construction work on the Al-Can Highway and in the Aleutian Islands gave him the nestegg to get us out of that country. With boxing and the gym for entertainment, he lived entirely off a whiskey allotment sold late in each month, every full paycheck came home--and he learned carpentry. On to Oregon! After ten years, he was building his own houses on speculation, in spite of his financial raw fear common to so many of that depression generation. But that gave his sons jobs to go to college, and he sent his daughter. He took incredible pride in even the smallest of the accomplishments of any of us. Part of him died with my brother, flying cover on a pilot pickup. My mother provided the stability in our family, not an easy task. She provided the planning, tried to challenge us, watched for opportunities to expand our horizons. A shy woman, she defended the value of her son's life by following the anti-Vietnam circuit and challenging all speakers. Her's was the quiet consistency that I better appreciated after having a family of my own.

## Preface

This is a heavily reworked and considerably shortened version of the first edition of this text. Especially, considerable "extra" material and background material have now been either removed or moved to the appendices. Moreover, some important rearrangement of chapters has taken place to facilitate its intended use as a text.

Chapters 1-5 provide the measure-theoretic mathematical foundation for the rest of the text. Then Chapter 6 (Distribution and Quantile Functions) and Chapter 7 (Independence and Conditional Expectation) hone some tools geared to probability theory. Appendix A (see page 417) provides a brief introduction to elementary probability theory that could be useful for some mathematics students (This Appendix A begins on page 417). A very useful version of this text could end at this page; omit Sections 10.5-10.9, and greatly slim down or eliminate the later parts of Chapters 7 and 13 (intentionally paired), Chapters 12 and 15 (again, intentionally paired), Chapter 6, Chapter 11, and Chapter 14.

The classical weak law of large numbers (WLLN) and strong law of large numbers (SLLN) as presented in Sections 8.2-8.4 are particularly complete, and they also emphasize the important role played by the behavior of the maximal summand. Presentation of good inequalities is emphasized in the entire text, and Chapter 8 is a good example. (Also, there is a very general collection of characterizations of the WLLN in Section C.1, that is then specialized carefully to the context of the behavior of the sample variance as an estimator. This Appendix C will also be appealed to in the optional Sections 10.5 (for a very general CLT for sums of row independent rvs) and 10.6 (where the domain of attraction of the Normal distribution is characterized via many very different looking equivalent conditions)).

The classical central limit theorem (CLT) and its Lindeberg, Liapunov, and Berry-Esseen generalizations are presented in Chapter 10 using the characteristic function (chf) methods introduced in Chapter 9. Many statistical applications also appear in Chapter 10.

A form of the most general CLT for "negligible pieces" is found in the optional Section 10.5, along with a more statistical variant. This is specialized to the iid case in Section 10.6--where many versions of necessary and sufficient conditions are presented. These Section 10.6 variants are justified primarily in Sections C.1-C.3. Conditions for both the weak bootstrap and the strong bootstrap are also developed in Section 10.8, and a universal bootstrap CLT based on light trimming of the sample is presented in Section 10.9. This approach emphasizes a statistical perspective. Gamma and Edgeworth approximations appear at the end of Chapter 11. The early parts of Chapter 11 deal with infinitely divisible and stable rvs. One of the main objectives in this second edition is to make it easier for many instructors to pick and choose from such topics (All references to the Stein method in the first edition have been removed because of a basic problem with that presentation.)Both the distribution function (df\(F(.)\)) and the quantile function (qf \(K(\cdot)\equiv F^{-1}(\cdot)\)) are emphasized throughout (quantile functions are important to statisticians). In Chapter 6, much general information about both dfs, qfs, and the Winsorized variance is developed. The text includes presentations showing how to exploit the inverse transformation \(X\equiv K(\xi)\) with \(\xi\cong\) Uniform(0, 1). In particular, Section C.6 inequalities relating the qf and the Winsorized variance to some empirical process results of Chapter 12 are used in Chapter 15 of this text to treat trimmed means and \(L\)-statistics, rank and permutation tests, and sampling from finite populations (Even more of this appears in the first addition of this text).

Chapter 13 provides quite a strong set of results for martingales. (The first edition includes even more topics and examples. Especially, there is a nice treatment of predictable variation.)

Chapter 14 considers convergence in law on more general metric spaces.

I have learned much through my association with David Mason, and I would like to acknowledge that here. Especially (in the context of this text), Theorem 12.10.3 is a beautiful improvement on Theorem 12.10.2, in that it still has the potential for necessary and sufficient results. I really admire the work of Mason and his colleagues. It was while working with David that some of my present interests developed. In particular, a useful companion to Theorem 12.10.3 is knowledge of quantile functions. Section 6.6 and Appendix C owe a debt to what I have compiled and produced on that topic while working on various applications, partially with David.

Jon Wellner has taught from several versions of this text. In particular, he typed an earlier version and thus gave me a major critical boost. That head start is what turned my thoughts to writing a text for publication. The Hoffman-Jorgensen inequalities in Section 8.10 came from him. He has also formulated a number of exercises, suggested various improvements, and offered good suggestions and references regarding predictable processes. My thanks to Jon for all of these contributions (Obviously, whatever problems may remain lie solely with me.)

My thanks go to John Kimmel for his interest in the first version of this text, and for his help and guidance through the various steps and decisions.

This is intended as a textbook, not as a research manuscript. Accordingly, it is somewhat lightly referenced. There is a section at the end that contains some discussion of the literature.

Galen R. Shorack

Seattle, Washington

July 2015

## Acknowledgements

I would like to thank several publishers for allowing me to use material in this text that originated elsewhere in my body of work. The citations made here are to publications referenced below. Various parts of Chapter 12, a few small pieces throughout the text, and much of the current Section 13.11 appeared in Shorack and Wellner (1986) (published by John Wiley & Sons). [Regarding the 1st Edition: The trimmed means example of Section 16.1 owes a substantial debt to Shorack (1997a) (published by Gordon and Breach Publishers), the trimmed mean and bootstrap examples of Sections 16.2 and 16.3 are largely from Shorack (1998) (published by Elsevier Science), while the L-statistics example of Section 16.4 is largely from Shorack (1997b) (published by Elsevier Science).] The inequalities from these publications upon which the applications are based are found at various points in the current Chapter 6 and Appendix B. The author appreciates the generosity of these publishers.

[MISSING_PAGE_EMPTY:8798]

\(\begin{array}{ll}\mbox{\bf 7}&\mbox{\bf Independence and Conditional Distributions}\\ 1&\mbox{\bf Independence}\\ 2&\mbox{\bf The Tail $\sigma$-Field}\\ 3&\mbox{\bf Uncorrelated Random Variables}\\ 4&\mbox{\bf Basic Properties of Conditional Expectation}^{o}\\ 5&\mbox{\bf Regular Conditional Probability}^{*}\end{array}\)

\(\begin{array}{ll}\mbox{\bf WLLN, SLLN, LIL, and Series}\\ 0&\mbox{\bf Introduction}\\ 1&\mbox{\bf Borel-Cantelli and Kronecker lemmas}\\ 2&\mbox{\bf Truncation, WLLN, and Review of Inequalities}\\ 3&\mbox{\bf Maximal Inequalities and Symmetrization}^{o}\\ 4&\mbox{\bf The Classical Laws of Large Numbers, LLNs}\\ 5&\mbox{\bf Applications of the Laws of Large Numbers}\\ 6&\mbox{\bf Law of the Iterated Logarithm}\\ 7&\mbox{\bf Strong Markov Property for Sums of IID RVs}\\ 8&\mbox{\bf Convergence of Series of Independent RVs}\\ 9&\mbox{\bf Martingales}.\\ 10&\mbox{\bf Maximal Inequalities, Some with $\nearrow$ Boundaries}^{o}\end{array}\)

\(\begin{array}{ll}\mbox{\bf Characteristic Functions and Determining Classes}\\ 1&\mbox{\bf Classical Convergence in Distribution}\\ 2&\mbox{\bf Determining Classes of Functions}\\ 3&\mbox{\bf Characteristic Functions, with Basic Results}\\ 4&\mbox{\bf Uniqueness and Inversion}\\ 5&\mbox{\bf The Continuity Theorem}\\ 6&\mbox{\bf Elementary Complex and Fourier Analysis}\\ 7&\mbox{\bf Esseen's Lemma}\\ 8&\mbox{\bf Distributions on Grids}^{*}\\ 9&\mbox{\bf Conditions for $\phi$ to Be a Characteristic Function}^{*}\end{array}\)

\(\begin{array}{ll}\mbox{\bf CLTs via Characteristic Functions}\\ 0&\mbox{\bf Introduction}\\ 1&\mbox{\bf Basic Limit Theorems}\\ 2&\mbox{\bf Variations on the Classical CLT}\\ 3&\mbox{\bf Examples of Limiting Distributions}^{o}\\ 4&\mbox{\bf Local Limit Theorems}^{o}.\\ 5&\mbox{\bf Normality Via Winsorization and Truncation}^{*}\\ 6&\mbox{\bf Independent Identically Distributed RVs}^{*}\\ 7&\mbox{\bf A Converse of the Classical CLT}^{*}\\ 8&\mbox{\bf Bootstrapping}^{o}\\ 9&\mbox{\bf Bootstrapping with Slowly $\nearrow$ Winsorization}^{*}\end{array}\)

\(\begin{array}{ll}\mbox{\bf 11}&\mbox{\bf Infinitely Divisible and Stable Distributions}^{o}\\ 1&\mbox{\bf Infinitely Divisible Distributions}^{o}\\ 2&\mbox{\bf Stable Distributions}^{o}\\ 3&\mbox{\bf Characterizing Stable Laws}^{o}\\ 4&\mbox{\bf The Domain of Attraction of a Stable Law}^{o}\\ 5&\mbox{\bf Gamma Approximation}^{*}\\ 6&\mbox{Edgeworth Expansions}^{*}\end{array}\)Brownian Motion and Empirical Processes
* 1 Special Spaces
* 2 Existence of Processes on \((C,\mathcal{C})\) and \((D,\mathcal{D})\)
* 3 Brownian Motion and Brownian Bridge
* 4 Stopping Times
* 5 Strong Markov Property
* 6 Embedding a RV in Brownian Motion
* 7 Barrier Crossing Probabilities
* 8 Embedding the Partial Sum Process\({}^{o}\)
* 9 Other Properties of Brownian Motion\({}^{o}\)
* 10 Various Empirical Process
* 11 Inequalities for Various Empirical Processes\({}^{o}\)
* 12 Applications\({}^{o}\)
* 13 Martingales
* 1 Basic Technicalities for Martingales
* 2 Simple Optional Sampling Theorem
* 3 The Submartingale Convergence Theorem
* 4 Applications of the S-mg Convergence Theorem
* 5 Decomposition of a Submartingale Sequence
* 6 Optional Sampling.
* 7 Applications of Optional Sampling
* 8 Introduction to Counting Process Martingales\({}^{*}\)
* 9 CLTs for Dependent RVs\({}^{o}\)
* 14 Convergence in Law on Metric Spaces\({}^{o}\)
* 1 Convergence in Distribution on Metric Spaces\({}^{o}\)
* 2 Metrics for Convergence in Distribution\({}^{o}\)
* 15 Asymptotics via Empirical Processes
* 0 Introduction
* 1 Trimmed and Winsorized Means\({}^{*}\)
* 2 Linear Rank Statistics and Finite Sampling
* 3 L-Statistics\({}^{*}\)

## Appendix A Special Distributions

* 1 Elementary Probability
* 2 Distribution Theory for Statistics
* 3 Linear Algebra Applications
* 4 The Multivariate Normal Distribution
* 4 The Multivariate Normal Distribution
* 5 General Topology and Hilbert Space\({}^{*}\)
* 1 General Topology\({}^{*}\)
* 2 Metric Spaces\({}^{*}\)
* 3 Hilbert Space\({}^{*}\)
* 4.1 More WLLN and CLT\({}^{o}\)
* 4.2 Introduction
* 4.3 General Moment Estimation\({}^{o}\)
* 4.4 Slowly Varying Partial Variance When \(\sigma^{2}=\infty^{o}\)
* 4.4 Specific Tail Relationships\({}^{o}\)
* 4.5 Regularly Varying Functions\({}^{*}\)

[MISSING_PAGE_EMPTY:8801]

## Use of This Text

The University of Washington is on the quarter system, so my description will reflect this fact. My thoughts are offered as a potential guide to an instructor. They certainly do not comprise an essential recipe.

The reader will note that the exercises are interspersed within the text. It is important to read all of the exercises as they are encountered (even if they are not written up), as most of them contain some worthwhile contribution to the story.

Chapters 1-5 provide the measure-theoretic background that is necessary for the rest of the text. Many of our students have had at least some kind of an undergraduate exposure to part of this subject. Still, it is important that I present the key parts of this material rather carefully. I feel it is useful for all of them.

_Chapter_ 1 (measures; 5 lectures)

Emphasized in my presentation are generators, the monotone property of measures, the Caratheodory extension theorem, completions, the approximation lemma, and the correspondence theorem. Presenting the correspondence theorem carefully is important, as this allows one the luxury of merely highlighting some proofs in Chapter 5. [The minimal monotone class theorem of Section 1.1, claim 8 of the Carathedory extension theorem proof, and most of what follows the approximation lemma in Section 1.2 would never be presented in my lectures.] {I always assign Exercises 1.1.1 (generators), 1.2.1 (completions), and 1.2.3 (the approximation lemma). Other exercises are assigned, but they vary each time.}

_Chapter_ 2 (measurable functions and convergence; 4 lectures)

I present most of Sections 2.1, 2.2 and 2.3. Highlights are preservation of \(\sigma\)-fields, measurability of both common functions and limits of simple functions, induced measures, convergence and divergence sets (especially), and relating \(\rightarrow_{\mu}\) to \(\rightarrow_{a.s}\) (especially, reducing the first to the second by going to subsequences). I then assign Section 2.4 as outside reading and Section 2.5 for exploring. [I never lecture on either Section 2.4 or 2.5.] {I always assign Exercises 2.2.1 (specific \(\sigma\)-fields), 2.3.1 (concerning \(\rightarrow_{a.e.}\)), 2.3.3 (a substantial proof), and 2.4.1 (Slutsky's theorem).}

_Chapter_ 3 (integration; 7 lectures)

This is an important chapter. I present all of Sections 3.1 and 3.2 carefully, but Section 3.3 is left as reading, while some of the Section 3.4 inequalities (\(\mathcal{C}_{r}\), Holder, Liapunov, Markov, and Jensen) are done carefully. I do Section 3.5 carefully as far as Vitali's theorem, and then assign the rest as outside reading. {I always assign Exercises 3.2.1-3.2.2 (only the zero function), 3.3.3(differentiating under the integral sign), 3.5.1 (substantial theory), and 3.5.7 (the Scheffe theorem).}

_Chapter_ 4 (Radon-Nikodym; 2 lectures)

I present ideas from Section 4.1, sketch the Jordan-Hahn decomposition proof, and then give the proofs of the Lebesgue decomposition, the Radon-Nikodym theorem, and the change of variable theorem. These final two topics are highlighted. The fundamental theorem of calculus of Section 4.4 is briefly discussed. [I would present only bits of Section 4.3. (especially, an interpretation of (4.1.1)). {I always assign Exercises 4.2.1 (manipulating Radon-Nikodym derivatives), 4.2.7 (mathematically substantial), and 4.4.1, 4.4.2, and 4.4.4 (so that the students must do some outside reading in Section 4.4 on their own).}

_Chapter_ 5 (Fubini; 2 lectures)

The first lecture covers Sections 5.1 and 5.2. I always discuss/prove Theorem 5.1.2 (product measure) and Theorem 5.1.3 (Fubini, with the Tonelli corollary). Proving Proposition 5.2.1 is a must. The remaining time is spent on Section 5.3. [I rarely lecture from Section 5.4, but I do assign it for outside perusing.] {I always assign Exercises 5.3.1 (measurability in a countable number of dimensions) and 5.4.1 (the finite-dimensional field).}

The mathematical tools have now been developed. In Chapters 6 and 7, we learn about some specialized probabilistic tools. The presentation of one of the main classic topics of probability theory then commences in Chapter 8.

_Chapter_ 6 (distribution functions (dfs) and quantile functions (qfs); 4 lectures)

This chapter is quite important to this text. Sections 6.1-6.4 should be covered. Skorokhod's theorem in Section 6.3 must be done carefully. Section 6.5 should be left as outside reading. [Lecture from Section 6.6 is purely optional, and I would not exceed one lecture.] {I always assign Exercises 6.1.1 (it exhibits a simple continuous df in two dimensions that does not have a density), 6.3.1 (\(F^{-1}(\cdot)\) is left continuous), 6.3.3 (change of variable), and 6.4.2 (for practice working with \(X\equiv K(\xi)\)). Consider lecturing on Theorem 6.6.1 (the infinite variance case).}

_Chapter_ 7 (conditional expectation; 2 lectures)

The first lecture covers Sections 7.1 and 7.2. It highlights Proposition 7.1.1 (on the preservation of independence), Theorem 7.1.2 (extending independence from \(\pi\)-systems), and Kolmogorov's 0-1 law. The other lecture (which could be postponed until just prior to taking up martingales) provides some discussion of the definition of conditional probability in Section 7.4, includes proofs of several parts of Theorem 7.4.1 (properties of conditional expectation). It might also briefly discuss the Definition 7.5.1 of regular conditional probability. [I never lecture on Sections 7.3 or 7.5.] {I always assign Exercises 7.1.2 and 7.1.3 (they provide routine practice with the concepts), Exercise 7.4.1 (discrete conditional probability), Exercise 7.4.3 (repeated stepwise smoothing in a particular example), and part of Exercise 7.4.4 (proving additional parts of Theorem 7.4.1).}

_Chapter_ 8 (laws of large numbers (LLNs) and inequalities; 3 lectures for now)

Since we are on the quarter system at the University of Washington, this leaves me 3 lectures to spend on the law of large numbers in Chapter 8 before the Christmas break at the end of the autumn quarter. In these first 3 lectures, I do Sections 8.1 and 8.2 with Khinchine's weak law of large numbers (WLLN), Kolmogorov's inequality only from Section 8.3, and at this time I present only Kolmogorov's strong law of large numbers (SLLN) from Section 8.4. {I always assign Exercises 8.1.1 (Cesaro summability), 8.2.1 (it generates good ideas related to the proofs), 8.2.2 (as it practices the important \(O_{p}(\cdot)\) and \(o_{p}(\cdot)\) notation), 8.4.4 (the substantial result of Marcinkiewicz and Zygmund), 8.4.7 (random sample size), and at least one of the alternative SLLN proofs contained in 8.4.8-8.4.10.}

At this point at the beginning of the winter quarter, the instructor will have his/her own opinions about what to cover. I devote the winter quarter to the weak law of large numbers(WLLN), an introduction to the law of the iterated logarithm (LIL), and various central limit theorems (CLTs). That is, the second term treats material from Chapters 8-10, with others optional. I will now outline my choices.

_Chapter 8_ (LLNs, inequalities, LIL, and series; 6 lectures)

My lectures cover Section 8.3 (symmetrization inequalities and Levy's inequality for the WLLN, and the Ottovani-Skorokhod inequality for series), Feller's WLLN from Section 8.4, the Glivenko-Cantelli theorem from Section 8.5, the LIL for normal rvs in Proposition 8.6.1, the strong Markov property of Theorem 8.7.1, and the two series Theorem 8.8.2. [I do not lecture from either of Section 8.9 or 8.10 at this time.] {I always assign Exercise 8.6.1 (Mills' ratio).}

_Chapter 9_ (characteristic functions (chfs); 8 lectures)

Sections 9.1 and 9.2 contain the classic results that relate to deriving convergence in distribution from the convergence of various integrals. I also cover Sections 9.3-9.8. {I always assign Exercises 9.3.1 and 9.3.3(a) (deriving specific chfs) and 9.4.1 (Taylor series expansions of the chf).}

_Chapter 10_ (CLTs via chfs; 6 lectures)

The classical CLT, the Poisson limit theorem, and the multivariate CLT make a nice lecture. The chi-square goodness-of-fit Example 10.1.1 and/or the median Example 10.3.3 make a lecture of illustrations. Chf proofs of the usual CLTs are given in Section 10.2 (Section 9.7 on Esseen's theorem could have been left until now). Other examples from either Section 10.2 or 10.3 could now be chosen, and Example 10.3.4 (weighted sums of iid rvs) is my first choice. [The chi-square goodness-of-fit Example 10.1.1 could motivate a student to read from Sections A.3 and A.4.]

At this stage, I still have at least 7 optional lectures at the end of the winter quarter and about 12 more during the spring quarter. In the other 16 lectures of the spring quarter, I feel it appropriate to consider Brownian motion in Chapter 12 and then martingales in Chapter 13 (in a fashion to be described below). Let me first describe some possibilities for the optional lectures, assuming that the above core was covered.

Chapter 10 (bootstrap)

Both Sections 10.8 and 10.9 on the bootstrap require also Sects 6.5 and 6.6.

_Chapter 10_ (domain of normal attraction of the Normal df)

The converse in Theorem 10.6.1 of the classical CLT Theorem 10.1.1 requires the Gine-Zinn symmetrization inequality and the Khinchine inequality of Section 8.3, and the Paley-Zygmund inequality of Section 3.4.

_Chapter 11_ (infinitely divisible and stable laws)

First, Section 11.1 (infinitely divisible laws) is independent of the rest, including Section 11.2 (stable laws). The theorem stated in Section 11.4 (domain of attraction of stable laws) would require methods of Section C.4 to prove, but the interesting exercises are accessible without this.

_Chapters 8 and 10_, _and Appendix_\(C\) (domain of attraction of the Normal df)

Combining the Section 8.3 subsection on maximal inequalities of another ilk, Sections 6.6, Sections 10.5-10.6, and Sections C.1-C.3 makes a nice unit. Levy's asymptotic normality condition (ANC) of (10.6.19) for a rv \(X\) has some prominence. In Section C.2, purely geometric methods plus Cauchy-Schwarz are used to derive a multitude of equivalent conditions. In the process, quantile functions are carefully studied. In Theorem 10.6.1, the ANC is seen to be equivalent to conditions equivalent to having a WLLN for the rv \(X^{2}\), and in this context, many additional equivalent conditions are again presented.

_Chapter_10 (higher-order approximations)

The local limit theorem in Section 10.4 can be done immediately for continuous \(\mathsf{dfs}\), but it also requires Section 9.8 for discrete \(\mathsf{dfs}\). The expansions given in Section 11.5 (Gamma approximation) and 11.6 (Edgeworth approximation) also require (9.6.22).

_Assorted topics suitable for individual reading_

Suppose that Chapter 12 has been covered. Consider the Chapter 15 examples on Trimmed and Winsorized means, L-statistics, and linear rank \(\mathsf{R}\)-statistics (with finite sampling). Convergence in distribution on the line is presented in Sections 9.1 and 9.2, which could be extended to metric spaces in Chapter 14. Sections 13.8 (counting process martingales) and 13.9 (martingale CLTs) are possible once the earlier parts of this chapter are covered.

The primary topics for the spring quarter are Chapter 12 (Brownian motion and elementary empirical processes) and Chapter 13 (martingales).

_Chapter_12 (Brownian motion; 6 lectures)

I discuss Section 12.1, sketch the one proof of Section 12.2 and carefully apply that result in Section 12.3, and treat Section 12.4 carefully (as I believe that at some point a lecture should be devoted to a few of the more subtle difficulties regarding measurability). I am a bit cavalier regarding Section 12.5 (strong Markov property), but I apply it carefully in Sections 12.6, 12.7, and the Theorem 12.8.1. I assign Section 12.9 as outside reading. [I do not lecture on Theorem 12.8.2.] {I always assign Exercises 12.1.2 (on \((\mathcal{C},\mathcal{C})\)), 12.3.1 (various transforms of Brownian motion), 12.3.3 (integrals of Normal processes), 12.4.1 (properties of stopping times), 12.7.3(a) (related to embedding a rv in Brownian motion), and 12.8.2 (the LIL via embedding).}

_Chapter_13 (martingales; 10 lectures)

I cover much of the first seven sections. {I always assign Exercises 13.1.4 (a counting process martingale), 13.3.2 (a proof for continuous time mgs), and both 13.3.7 and 13.3.9 (on \(\mathcal{L}_{r}\)-convergence).}

_Appendix A_ (elementary probability; 0 lectures)

Sections A.1 and A.2 were written to provide background reading for those graduate students in mathematics who lack an elementary probability background. Sections A.3 and A.4 allow graduate students in statistics to read some of the basic multivariate results in appropriate matrix notation. [I do not lecture from this appendix.] {But I do assign Exercises A.1.8 (the Poisson process exists) and A.2.1 (ii) (so that the convolution formula is refreshed).}

Useful topics that appear only in the \(1\mathsf{st}\) edition of this text

_Appendix_ (maximum likelihood estimation (or, MLE))

I see MLE as being of considerable interest in conjunction with statistical pursuits, rather than as a direct part of a course on probability theory. This is available in Appendix A of the first edition. Other topics are also treated only in Appendix A of the first edition, such as careful development of the Gamma function and the derivative of its logarithm--with applications to maximum likelihood estimation. Chapter 16 in the first edition goes beyond the present Chapter 15 in the depth and breadth of its coverage of applications of the convergence of empirical processes.

[MISSING_PAGE_FAIL:19]

Integral signs and summation signs in important equations (or sufficiently complicated equations) are large, while those in less important equations are small. It is a matter of assigned prominence. The most important theorems, definitions, and examples have been given titles in boldface type to assign prominence to them. The titles of somewhat less important results are not in boldface type. Routine references to theorem 10.4.1 or definition 7.3.1 do not contain capitalized initial letters. The author very specifically wishes to downgrade the prominence given to this routine use of these words. Starting new sections on new pages allowed me to carefully control the field of vision as the most important results were presented.

## Chapter 1 Measures

### 1 Basic Properties of Measures

**Motivation 1.1** (The Lebesgue integral) The Riemann integral of a continuous function \(f\)(we will restrict attention to \(f(x)\geq 0\) on \(a\leq x\leq b\) for convenience) is formed by subdividing the domain of \(f\), forming approximating sums, and passing to the limit. Thus the \(m\)th Riemann sum for \(\int_{a}^{b}f(x)\,dx\) is defined as

\[RS_{m}\equiv\sum_{i=1}^{m}f(x_{mi}^{*})[x_{mi}-x_{m,i-1}], \tag{1}\]

where \(a\equiv x_{m0}<x_{m1}<\cdots<x_{mm}\equiv b\) (with \(x_{m,i-1}\leq x_{mi}^{*}\leq x_{mi}\) for all \(i\)) satisfy mesh\({}_{m}\equiv\max[x_{mi}-x_{m,i-1}]\to 0\). Note that \(x_{mi}-x_{m,i-1}\) is the measure (or length) of the interval \([x_{m,i-1},x_{mi}]\), while \(f(x_{mi}^{*})\) approximates the values of \(f(x)\) for all \(x_{m,i-1}\leq x\leq x_{mi}\) (at least it does if \(f\)is continuous on \([a,b]\)). Within the class \(\mathcal{C}^{+}\) of all nonnegative continuous functions, this definition works reasonably well. But it has one major shortcoming. The conclusion \(\int_{a}^{b}f_{n}(x)\,dx\to\int_{a}^{b}f(x)\,dx\) is one we often wish to make if \(f_{n}\) "converges" to \(f\). However, even when all \(f_{n}\) are in \(\mathcal{C}^{+}\) and \(f(x)\equiv\lim f_{n}(x)\) actually exists, it need not be that \(f\)is in \(\mathcal{C}^{+}\) (and thus \(\int_{a}^{b}f(x)\,dx\) may not even be well-defined) or that \(\int_{a}^{b}f_{n}(x)\,dx\to\int_{a}^{b}f(x)\,dx\) (even when it is well defined).

A different approach is needed. (Note figure 1.1.)

The Lebesgue integral of a bounded and nonnegative function is formed by subdividing the range. Thus the \(m\)th Lebesgue sum for \(\int_{a}^{b}f(x)\,dx\) is defined as

\[LS_{m}\equiv\sum_{k=1}^{m^{2}}\frac{k-1}{2^{m}}\times\text{measure}\left(\left\{ x:\frac{k-1}{2^{m}}\leq f(x)<\frac{k}{2^{m}}\right\}\right), \tag{2}\]

and \(\int_{a}^{b}f(x)\,dx\) is defined to be the limit of the \(LS_{m}\) sums as \(m\to\infty\). For what class \(\mathcal{M}\) of functions \(f\) can this approach succeed? The members \(f\) of the class \(\mathcal{M}\) will need to be such that the measure (or length) of all sets of the form

\[\left\{x:\frac{k-1}{2^{m}}\leq f(x)<\frac{k}{2^{m}}\right\} \tag{3}\]

can be specified. This approach leads to the concept of a \(\sigma\)-field \(\mathcal{A}\) of subsets of \([a,b]\) that are measurable (that is, we must be able to assign to these sets a number called their "length"),and this leads to the concept of the class \(\mathcal{M}\) of measurable functions. This class \(\mathcal{M}\) of measurable functions will be seen to be closed under passage to the limit and all the other operations that we are accustomed to performing on functions. Moreover, the desirable property \(\int_{a}^{b}f_{n}(x)\,dx\to\int_{a}^{b}f(x)\,dx\) for functions \(f_{n}\) "converging" to \(f\) will be broadly true. 

**Definition 1.1** (Set theory): Consider a nonvoid class \(\mathcal{A}\) of subsets \(A\) of a nonvoid set \(\Omega\). (For us, \(\Omega\) will be the sample space of an experiment.)

(a) Let \(A^{c}\) denote the _complement_ of \(A\), let \(A\cup B\) denote the _union_ of \(A\) and \(B\), let \(A\cap B\) and \(AB\) both denote the _intersection_, let \(A\setminus B\equiv AB^{c}\) denote the _set difference_, let \(A\triangle B\equiv(A^{c}B\cup AB^{c})\) denote the _symmetric difference_, and let \(\emptyset\) denote the _empty set_. The class of all subsets of \(\Omega\) will be denoted by \(2^{\Omega}\). Sets \(A\) and \(B\) are called _disjoint_ if \(AB=\emptyset\), and sequences of sets \(A_{n}\) or classes of sets \(\mathcal{A}_{t}\) are called disjoint if all pairs are disjoint. Writing \(A+B\) or \(\sum_{1}^{\infty}A_{n}\) will also denote a union, but will imply the disjointness of the sets in the union. As usual, \(A\subset B\) denotes that \(A\) is a _subset_ of \(B\). We call a sequence \(A_{n}\)_increasing_ (and we will nearly always denote this fact by writing \(A_{n}\nearrow\)) when \(A_{n}\subset A_{n+1}\) for all \(n\geq 1\). We call the sequence _decreasing_ (denoted by \(A_{n}\searrow\)) when \(A_{n}\supset A_{n+1}\) for all \(n\geq 1\). We call the sequence _monotone_ if it is either increasing or decreasing. Let \(\omega\) denote a generic element of \(\Omega\). We

Figure 1.1: Riemann sums and Lebesgue sums.

will use \(1_{A}(\cdot)\) to denote the _indicator function_ of \(A\), which equals 1 or 0 at \(\omega\) according as \(\omega\in A\) or \(\omega\notin A\).

(b) \({\cal A}\) will be called a _field_ if it is closed under complements and unions. (That is, \(A\) and \(B\) in \({\cal A}\) requires that \(A^{c}\) and \(A\cup B\) be in \({\cal A}\).) [Note that both \(\Omega\) and \(\emptyset\) are necessarily in \({\cal A}\), as \({\cal A}\) was assumed to be nonvoid, with \(\Omega=A\cup A^{c}\) and \(\emptyset=\Omega^{c}\).]

(c) \({\cal A}\) will be called a \(\sigma\)-_field_ if it is closed under complements and countable unions. (That is, \(A,A_{1},A_{2},\ldots\) in \({\cal A}\) requires that \(A^{c}\) and \(\cup_{1}^{\infty}\,A_{n}\) be in \({\cal A}\).)

(d) \({\cal A}\) will be called a _monotone class_ provided it contains \(\cup_{1}^{\infty}A_{n}\) for all increasing sequences \(A_{n}\) in \({\cal A}\) and contains \(\cap_{1}^{\infty}A_{n}\) for all decreasing sequences \(A_{n}\) in \({\cal A}\).

(e) \((\Omega,{\cal A})\) will be called a _measurable space_ provided \({\cal A}\) is a \(\sigma\)-field of subsets of \(\Omega\).

(f) \({\cal A}\) will be called a \(\pi\)-_system_ provided _AB_ is in \({\cal A}\) for all \(A\) and \(B\) in \({\cal A}\); and \({\cal A}\) will be called a \(\bar{\pi}\)-_system_ when \(\Omega\) in \({\cal A}\) is also guaranteed.

If \({\cal A}\) is a field (or a \(\sigma\)-field), then it is closed under intersections (under countable intersections); since \(AB=(A^{c}\cup B^{c})^{c}\) (since \(\cap_{1}^{\infty}A_{n}=(\cup_{1}^{\infty}A_{n}^{c})^{c}\)). Likewise, we could have used "intersection" instead of "union" in our definitions by making use of \(A\cup B=(A^{c}\cap B^{c})^{c}\) and \(\cup_{1}^{\infty}A_{n}=(\cap_{1}^{\infty}A_{n}^{c})^{c}\). (This used _De Morgan's laws_.)

**Proposition 1.1** (Closure under intersections)

(a) Arbitrary intersections of fields, \(\sigma\)-fields, or monotone classes are fields, \(\sigma\)-fields, or monotone classes, respectively.

[For example, \({\cal F}\equiv\cap\{{\cal F}_{\alpha}:\,{\cal F}_{\alpha}\) is a field under consideration\(\}\) is a field.]

(b) There is a minimal field, \(\sigma\)-field, or monotone class _generated by_ (or, containing) any specified class \({\cal C}\) of subsets of \(\Omega\). Call \({\cal C}\) the _generators_. For example,

\[\sigma[{\cal C}]\equiv\cap\{{\cal F}_{\alpha}:{\cal F}_{\alpha}\mbox{ is a $ \sigma$-field of subsets of $\Omega$ for which ${\cal C}\subset{\cal F}_{\alpha}$}\} \tag{4}\]

is the _minimal \(\sigma\)-field generated by_\({\cal C}\) (that is, containing \({\cal C}\)).

(c) A collection \({\cal A}\) of subsets of \(\Omega\) is a \(\sigma\)-field if and only if it is both a field and a monotone class.

**Proof.** (c) \((\Leftarrow)\cup_{1}^{\infty}\,A_{n}=\cup_{1}^{\infty}(\cup_{1}^{n}A_{k})\ )\equiv\cup_{1}^{\infty}B_{n}\in{\cal A}\) since the \(B_{n}\) are in \({\cal A}\) and are \(\nearrow\). Everything else is even more trivial. \(\square\)

**Exercise 1.1** (Generators)** Let \({\cal C}_{1}\) and \({\cal C}_{2}\) denote two collections of subsets of the set \(\Omega\). If \({\cal C}_{2}\subset\sigma[{\cal C}_{1}]\) and \({\cal C}_{1}\subset\sigma[{\cal C}_{2}]\), then \(\sigma[{\cal C}_{1}]=\sigma[{\cal C}_{2}]\). Prove this important fact.

**Definition 1.2** (Measures and events): Consider a measurable space \((\Omega,{\cal A})\) and a set function \(\mu:\,{\cal A}\rightarrow[0,\ \infty]\) (that is, \(\mu(A)\geq 0\) for each \(A\in{\cal A}\)) having \(\mu(\emptyset)=0\).

(a) Now \({\cal A}\) is a \(\sigma\)-field and if \(\mu\) is _countably additive_ (abbreviated \(c.a.\)) in that

\[\mu\left(\sum_{n=1}^{\infty}A_{n}\right)=\sum_{n=1}^{\infty}\mu(A_{n})\quad \mbox{for all disjoint sequences $A_{n}$ in ${\cal A}$}, \tag{5}\]

then \(\mu\) is called a _measure_ (or, equivalently, a _countably additive measure_) on \((\Omega,{\cal A})\). The triple \((\Omega,{\cal A},\mu)\) is then called a _measure space_. We call \(\mu\)_finite_ if \(\mu(\Omega)<\infty\). We call \(\mu\)_\(\sigma\)-finite_ if there exists a _measurable decomposition_ of \(\Omega\) as \(\Omega=\sum_{1}^{\infty}\Omega_{n}\) with \(\Omega_{n}\in{\cal A}\) and \(\mu(\Omega_{n})<\infty\) for all \(n\). The sets \(A\) in the \(\sigma\)-field \({\cal A}\) are called _events_.

[Even if \({\cal A}\) is not a \(\sigma\)-field, we will still call \(\mu\) a measure on \((\Omega,{\cal A})\), when (5) holds for all sequences \(A_{n}\in{\cal A}\) for which \(\sum_{1}^{\infty}A_{n}\) is in \({\cal A}\). We will not, however, use the term "measurespace" to describe such a triple. We will consider below measures on fields, on certain \(\bar{\pi}\)-systems, and on some other collections of sets. A useful property of a collection of sets is that along with any sets \(A_{1},\ldots,A_{k}\) it also includes all sets of the type \(B_{k}\equiv A_{k}A_{k-1}^{c}\cdots A_{2}^{c}A_{1}^{c}\); then \(\cup_{1}^{n}A_{k}=\sum_{1}^{n}B_{k}\) is easier to work with.]

(b) Of less interest, call \(\mu\) a _finitely additive measure_ (abbreviated \(f.a.\)) on \((\Omega,{\cal A})\) if

\[\mu\left(\sum\nolimits_{1}^{n}A_{k}\right)=\sum\nolimits_{1}^{n}\mu(A_{k}) \tag{6}\]

for all disjoint sequences \(A_{k}\) in \({\cal A}\) for which \(\sum_{1}^{n}A_{k}\) is also in \({\cal A}.\)

**Definition 1.3** (Outer measures) Consider a set function \(\mu^{*}:2^{\Omega}\to[0,\;\infty].\)

(a) Suppose that \(\mu^{*}\) also satisfies the following three properties.

Null: \(\mu^{*}(\emptyset)=0.\)

Monotone: \(\mu^{*}(A)\leq\mu^{*}(B)\) for all \(A\subset B.\)

Countable subadditivity: \(\mu^{*}(\bigcup_{1}^{\infty}A_{n})\leq\sum_{1}^{\infty}\mu^{*}(A_{n})\) for all sequences \(A_{n}.\)

Then \(\mu^{*}\) is called an _outer measure_.

(b) An arbitrary subset \(A\) of \(\Omega\) is called \(\mu^{*}\)-_measurable_ if

\[\mu^{*}(T)=\mu^{*}(TA)+\mu^{*}(TA^{c})\qquad\mbox{for all subsets $T\subset \Omega$.} \tag{7}\]

Sets \(T\) used in this capacity are called _test sets_.

(c) We let \({\cal A}^{*}\) denote the class of all \(\mu^{*}\)-_measurable sets_, that is,

\[{\cal A}^{*}\equiv\left\{A\in 2^{\Omega}:A\,\mbox{is $\mu^{*}$-measurable} \right\}. \tag{8}\]

[Note that \(A\in{\cal A}^{*}\) if and only if \(\mu^{*}(T)\geq\mu^{*}(TA)+\mu^{*}(TA^{c})\) for all \(T\subset\Omega,\) since the other inequality is trivial by the subadditivity of \(\mu^{*}.\)]

**Motivation 1.2** (Measure) In this paragraph we will consider only one possible measure \(\mu,\) namely the Lebesgue-measure generalization of length. Let \({\cal C}_{I}\) denote the set of all intervals of the types \((a,\;b],\;(-\infty,\;b],\;\mbox{and}\;(a,\;+\infty)\) on the real line \(R,\) and for each of these intervals \(I\) we assign a measure value \(\mu(I)\) equal to its length, thus \(b-a,\infty,\infty\) in the three special cases. All is well until we manipulate the sets in \({\cal C}_{I},\) as even the union of two elements in \({\cal C}_{I}\) need not be in \({\cal C}_{I}.\) Thus, \({\cal C}_{I}\) is not a very rich collection of sets. A natural extension is to let \({\cal C}_{F}\) denote the collection of all finite disjoint unions of sets in \({\cal C}_{I},\) where the measure \(\mu(A)\) we assign to each such set \(A\) is just the sum of the measures (lengths) of all its disjoint pieces. Now \({\cal C}_{F}\) is a field, and is thus closed under the elementary operations of union, intersection, and complementation. Much can be done using only \({\cal C}_{F}\) and letting "measure" be the "exact length." But \({\cal C}_{F}\) is not closed under passage to the limit, and it is thus insufficient for many of our needs. For this reason the concept of the smallest \(\sigma\)-field containing \({\cal C}_{F},\) labeled \({\cal B}\equiv\sigma[{\cal C}_{F}],\) is introduced. We call \({\cal B}\) the Borel sets. But let us work backwards. Let us assign an outer measure value \(\mu^{*}(A)\) to every subset \(A\) in the class \(2^{R}\) of all subsets of the real line \(R.\) In particular, to any subset \(A\) we assign the value \(\mu^{*}(A)\) that is the infimum of all possible numbers \(\sum_{n=1}^{\infty}\mu(A_{n}),\) in which each \(A_{n}\) is in the field \({\cal C}_{F}\) (so that we know its measure) and in which the \(A_{n}\)'s form a _cover_ of \(A\) (in that \(A\subset\cup_{1}^{\infty}A_{n})\). Thus each number \(\sum_{1}^{\infty}\mu(A_{n})\) is a natural upper bound to the measure (or generalized length) of the set \(A,\) and we will specify the infimum of such upper bounds to be the outer measure of \(A.\) Thus to each subset \(A\) of the real line we assign a value \(\mu^{*}(A)\) of generalized length. This value seems "reasonable," but does it "perform correctly"? Let us say that a particular set \(A\) is \(\mu^{*}\)-measurable (that is, it "performs correctly") if \(\mu^{*}(T)=\mu^{*}(TA)+\mu^{*}(TA^{c})\) for all subsets \(T\) of the real line \(R\)--that is, if the \(A\) versus \(A^{c}\) division of the line divides every subset \(T\) of the line into two pieces in a fashion that is \(\mu^{*}\)-additive. This is undoubtedly a combination of reasonableness and fine technicality that took some time to evolve in the mind of its creator, Caratheodory, while he searched for a condition that "worked." In what sense does it "work"? The collection \({\cal A}^{*}\) of all \(\mu^{*}\)-measurable sets turns out to be a \(\sigma\)-field. Thus the collection \({\cal A}^{*}\) is closed under all operations that we are likely to perform; and it is big enough, in that it is a \(\sigma\)-field that contains \({\cal C}_{F}\). Thus we will work with the restriction \(\mu^{*}|{\cal A}^{*}\) of \(\mu^{*}\) to the sets of \({\cal A}^{*}\) (here, the vertical line means "restricted to"). This is enough to meet our needs (and it turns out to be exactly the maximal possible stopping point).

There are many measures other than length. For an \(\nearrow\) and right-continuous function \(F\) on the real line (called a generalized df) we define the Stieltjes measure of an arbitrary interval \((a,\ b]\) (with \(-\infty\leq a<b\leq\infty\)) in \({\cal C}_{I}\) by \(\mu_{F}((a,\ b])=F(b)-F(a)\), and we extend it to sets in \({\cal C}_{F}\) by adding up the measure of the pieces. Reapplying the previous paragraph, we can extend \(\mu_{F}\) to the \(\mu^{*}_{F}\)-measurable sets. It is the important Caratheodory extension theorem that will establish that all Stieltjes measures (including the case of ordinary length, where \(F(x)=x\), as considered in the first paragraph) can be extended from \({\cal C}_{F}\) to the Borel sets \({\cal B}\). That is, all Borel sets are \(\mu^{*}\)-measurable for every Stieltjes measure. One further extension is possible, in that every measure can be completed" (see the end of section 1.2). We note here only that when the Stieltjes measure \(\mu_{F}\) associated with the generalized df \(F\) is "completed," its domain of definition is extended from the Borel sets \({\cal B}\) (which all Stieltjes measures have in common) to a larger collection \(\hat{{\cal B}}_{\mu_{F}}\) that depends on the particular \(F.\) It is left to section 1.2 to simply state that this is as far as we can go. That is, except in rather trivial special cases (especially, mass at only countably many points), we find that \(\hat{{\cal B}}_{\mu_{F}}\) is a proper subset of \(2^{R}\). (Otherwise, it is typically impossible to try to define the measure of all subsets of \(\Omega\) in a suitable fashion.) \(\Box\)

**Example 1.1** (Some examples of measures, informally)

(a) _Lebesgue measure_:

Let \(\lambda(A)\) denote the length of \(A\).

(b) _Counting measure_:

Let \(\#(A)\) denote the number of "points" in \(A\) (or the _cardinality_ of \(A\)).

(c) _Unit point mass_:

Let \(\delta_{\omega_{0}}(A)\equiv 1_{A}(\omega_{o})\), assigning measure 1 or 0 to \(A\) as \(\omega_{o}\in A\) or not.

(\(*\)) Rigorous proof that these are measures will follow from theorem 1.3.1 below. \(\Box\)

**Example 1.2** (Borel sets):

(a) Let \(\Omega=R\) and let \({\cal C}\) consist of all finite disjoint unions of intervals of the types \((a,\ b],\ (-\infty,\ b],\ \mbox{and}\ (a,\ +\infty)\). Clearly, \({\cal C}\) is a field. Then \({\cal B}\equiv\sigma[{\cal C}]\) will be called the _Borel sets_ (or the _Borel subsets_ of \(R\)). Let \(\mu(A)\) be defined to be the sum of the lengths of the intervals composing \(A\), for each \(A\in{\cal C}\). Then \(\mu\) is a (c.a.) measure on the field \({\cal C}\), as will be seen in the proof of theorem 1.3.1 below.

(b) If \((\Omega,\ d)\) is a metric space and \({\cal U}\equiv\{\mbox{all }d-\mbox{open subsets of }\Omega\}\), then \({\cal B}\equiv\sigma[{\cal U}]\) will be called the _Borel sets_ or the _Borel \(\sigma\)-field_.

(c) If \((\Omega,\ d)\) is \((R,\ |\cdot|)\) for absolute value \(|\cdot|\), then \(\sigma[{\cal C}]=\sigma[{\cal U}]\) even though \({\cal C}\neq{\cal U}.\) (This claim is true, since \({\cal C}\subset\sigma[{\cal U}]\) and \({\cal U}\subset\sigma[{\cal C}]\) are clear. Then, use exercise 1.1.)

(d) Let \(\bar{R}\equiv[-\infty,\ =\infty]\) denote the _extended real line_; let \(\bar{\cal B}\equiv\sigma[{\cal B},\ \{-\infty\},\ \{+\infty\}]\).

(e) For any interval \(I\subset[-\infty,\ \infty]\), let \(\bar{\cal B}_{I}\equiv\{B\cap I:B\in\bar{\cal B}\}=\bar{\cal B}\cap I\). \(\Box\)

**Proposition 1.2** (Monotone properties of measures): Let \((\Omega,\ {\cal A},\ \mu)\) denote a measure space. (Of course, \(\mu(A)\leq\mu(B)\) for \(A\subset B\) in \({\cal A}\).) Let \(A_{1},A_{2},\ldots\) be in \({\cal A}\).

(a) If \(A_{n}\subset A_{n+1}\) for all \(n\), then

\[\mu(\bigcup_{1}^{\infty}A_{n})=\lim_{n\to\infty}\mu(A_{n}). \tag{9}\]

(b) If \(\mu(A_{n_{0}})<\infty\) for some \(n_{0}\), and \(A_{n}\supset A_{n+1}\) for all \(n\), then

\[\mu(\bigcap_{n=1}^{\infty}A_{n})=\lim_{n\to\infty}\mu(A_{n}). \tag{10}\]

(Letting \(\Omega\) denote the real line \(R\), letting \(A_{n}=[n,\ \infty)\), and letting \(\mu\) denote either Lebesgue measure or counting measure, we see the need for some requirement.)

(c) (Countable subadditivity) Whenever \(A_{1},A_{2},\ldots\) and \(\cup_{1}^{\infty}A_{n}\) are all in \(\mathcal{A}\), then

\[\mu(\bigcup_{1}^{\infty}A_{k})\leq\sideset{}{{}_{1}^{\infty}}{\sum}\mu(A_{k})\ ;\]

(d) All this also holds true for a measure on a field (via the same proofs).

**Proof.** (a) Now,

\[\mu(\cup_{1}^{\infty}A_{n})=\mu(\sum_{1}^{\infty}(A_{n}\backslash A_{n-1})) \qquad\text{with}\,A_{0}\equiv\emptyset\] (p) \[= \sideset{}{{}_{1}^{\infty}}{\sum}\mu(A_{n}\backslash A_{n-1}) \qquad\text{by c.a.}\] \[= \lim_{n}\sideset{}{{}_{k=1}^{n}}{\sum}\mu(A_{k}\backslash A_{k-1})\] \[= \lim_{n}\mu(\sum_{k=1}^{n}(A_{k}\backslash A_{k-1})) \qquad\text{by f.a.}\] (q) \[= \lim_{n}\mu(A_{n}).\]

(b) Without loss of generality, redefine \(A_{1}=A_{2}=\cdots=A_{n_{0}}\). Let \(B_{n}\equiv A_{1}\backslash A_{n}\) so that the sets \(B_{n}\) are \(\nearrow\), with \(B_{1}=\emptyset\). Draw a picture of concentric circles of decreasing radaii to represent the general decreasing sets \(A_{n}\). Then locate the sets \(B_{n}\) in this picture, and note that they are indeed increasing. It is pictorially clear that \(A_{1}\cap(\cap_{n=1}^{\infty}A_{n})^{c}=\cup_{n=1}^{\infty}B_{n}\). Thus,

\[\lim_{n}\mu(B_{n})=\mu(\cup_{n=1}^{\infty}B_{n}) \qquad\text{by (a)}\] \[=\mu(A_{1}\cap(\cap_{1}^{\infty}A_{n})^{c})\] (r) \[=\mu(A_{1})-\mu(\cap_{n=1}^{\infty}A_{n})\]

On the other hand,

\[\lim_{n}\mu(B_{n})=\lim_{n}\{\mu(A_{1})-\mu(A_{n})\}\qquad\text{by f.a.}\] (s) \[=\mu(A_{1})-\lim_{n}\mu(A_{n}).\]

Equate the quantities in (r) and (s); since \(\mu(A_{1})<\infty\), we can cancel it to obtain the equality \(\mu(\cap_{1}^{\infty}A_{n})=\lim_{n}\mu(A_{n})\).

(c) Let \(B_{1}\equiv A_{1},B_{2}\equiv A_{2}A_{1}^{c},\ldots,B_{k}\equiv A_{k}A_{k-1}^ {c}\cdots A_{1}^{c}\). Then these newly defined sets \(B_{k}\) are disjoint, and \(\cup_{k=1}^{n}A_{k}=\sum_{k=1}^{n}B_{k}\). Hence [a technique worth remembering]

\[\mu(\bigcup_{k=1}^{n}A_{k})=\mu(\sum_{k=1}^{n}B_{k})=\sum_{k=1}^{n}\mu(B_{k}) \tag{11}\]\[\begin{array}{l}\mbox{where }\bigcup\nolimits_{k=1}^{n}A_{k}=\sum\nolimits_{k=1}^{n }B_{k}\mbox{ is }\nearrow\mbox{for}B_{k}\equiv A_{k}A_{k-1}^{c}\cdots A_{1}^{c}\\ \leq\sum\nolimits_{1}^{n}\!\!\mu(A_{k})\end{array}\]

(t) \(\leq\sum\nolimits_{1}^{\infty}\!\!\mu(A_{k})\) by monotonicity.

Let \(n\to\infty\) in (11), and use part (a) to get the result. 

**Definition 1.4** (liminf and limsup of sets): Let

\[\begin{array}{l}\underline{\lim}\,A_{n}\equiv\bigcup\nolimits_{n=1}^{ \infty}\bigcap\nolimits_{k=n}^{\infty}A_{k}=\bigcup\nolimits_{n=1}^{\infty}B_{ n}\quad(\mbox{where }\,B_{n}\equiv\bigcap\nolimits_{k=n}^{\infty}A_{k}\mbox{ is }\nearrow)\\ =\{\omega:\omega\mbox{ is in all but finitely many}A_{n}\mbox{'s}\}\equiv\{\omega: \omega\in A_{n}\mbox{ a.b.f.}\}\,,\end{array} \tag{12}\]

where we use _a.b.f._ to abbreviate in _all but finitely many cases_. Let

\[\begin{array}{l}\overline{\lim}\,A_{n}\equiv\bigcap\nolimits_{n=1}^{\infty} \bigcup\nolimits_{k=n}^{\infty}A_{k}=\bigcap\nolimits_{n=1}^{\infty}B_{n}\mbox { (where }\,B_{n}\equiv\bigcup\nolimits_{k=n}^{\infty}A_{k}\mbox{ is } \searrow)\\ =\{\omega:\omega\mbox{ is in an infinite number of }A_{n}\mbox{'s}\}\equiv\{\omega: \omega\in A_{n}\mbox{ i.o.}\},\end{array} \tag{13}\]

where we use _i.o._ to abbreviate _infinitely often_.

(It is important to learn to read these two mathematical equations in a way that makes it clear that the verbal description is correct.) It is verbally trivial that we always have \(\underline{\lim}\,A_{n}\subset\overline{\lim}\,A_{n}\). Define

\[\lim A_{n}\equiv\underline{\lim}\,A_{n}\quad\mbox{whenever }\underline{\lim}\,A_{n}=\overline{\lim}\,A_{n}. \tag{14}\]

We also let \(\liminf A_{n}\equiv\underline{\lim}\,A_{n}\) and \(\limsup A_{n}\equiv\overline{\lim}\,A_{n}\), giving us alternative notations.

**Proposition 1.3** Clearly, \(\lim A_{n}\) equals \(\cup_{1}^{\infty}A_{n}\) when \(A_{n}\) is an \(\nearrow\) sequence, and \(\lim A_{n}\) equals \(\cap_{1}^{\infty}A_{n}\) when \(A_{n}\) is a \(\searrow\) sequence.

**Exercise 1.2** (a) Now \(\mu(\liminf A_{n})\leq\liminf\!\mu(A_{n})\) is always true.

(b) Also, \(\limsup\mu(A_{n})\leq\mu(\limsup A_{n})\) holds if \(\mu(\Omega)<\infty\). (Why the condition?)

**Definition 1.5** (\(\lim\,\inf\) and \(\lim\,\sup\) of numbers): Recall that for real number sequences \(a_{n}\) one defines \(\underline{\lim}\,a_{n}\equiv\liminf a_{n}\) and \(\overline{\lim}\,a_{n}\equiv\limsup a_{n}\) by

\[\begin{array}{l}\liminf_{n\to\infty}a_{n}\equiv\lim_{n\to\infty}(\inf_{k\geq n }a_{k})=\sup_{n\geq 1}(\inf_{k\geq n}a_{k})\quad\mbox{and}\\ \limsup_{n\to\infty}a_{n}\equiv\lim_{n\to\infty}(\sup_{k\geq n}a_{k})=\inf_{n \geq 1}(\sup_{k\geq n}a_{k}),\end{array} \tag{15}\]

and these yield the smallest limit point and the largest limit point, respectively, of the sequence \(a_{n}\).

**Definition 1.6** (Continuity of set functions): A set function \(\mu\) defined on some class of subsets \(\mathcal{A}\) of a non-void set \(\Omega\) is _continuous from below_ (or, from _above_) if \(\mu(\lim A_{n})=\lim\mu(A_{n})\) for all sequences \(A_{n}\) in \(\Omega\) that are \(\nearrow\) to a set in \(\mathcal{A}\) (or, for all sequences \(A_{n}\) in \(\Omega\) that are \(\searrow\) to a set in \(\mathcal{A}\), with at least one \(\mu(A_{n})\) finite). Call \(\mu\)_continuous_ in case it is continuous both from below and from above. If \(\lim\mu(A_{n})=\mu(A)\) whenever \(A_{n}\searrow A\), then \(\mu\) is said to be _continuous from above at_ this particular \(A\) in \(\mathcal{A}\), etc.

The next result is often used in conjunction with the Caratheodory extension theorem of the next section. View it as a converse to the proposition 1.2.

**Proposition 1.4** (Continuity of measures): If a finitely additive measure \(\mu\) on either a field or \(\sigma\)-field is either continuous from below or has \(\mu(\Omega)<\infty\) and is continuous from above at \(\emptyset\), then it is a countably additive measure.

**Proof.** Suppose first that \(\mu\) is continuous from below. Then

\[\mu\left({\sum\nolimits_{1}}^{\infty}A_{k}\right)=\mu\left({\lim \sum\nolimits_{1}}^{n}A_{k}\right)\] (a) \[={\lim\mu(\sum\nolimits_{1}}^{n}A_{k})\qquad\mbox{by continuity from below}\] (b) \[={\lim\sum\nolimits_{1}}^{n}\mu(A_{k})\qquad\mbox{by f.a. (where we used only that ${\cal A}$ is a field)}\] (c) \[={\sum\nolimits_{1}}^{\infty}\mu(A_{k}),\]

giving the required countable additivity. Thus \(\mu\) is c.a. on \({\cal A}\).

Suppose next that \(\mu\) is finite and is also continuous from above at \(\emptyset\). Then f.a. (even if \({\cal A}\) is only a field) gives

\[\mu\left({\sum\nolimits_{1}}^{\infty}A_{k}\right)=\mu\left({\sum \nolimits_{1}}^{n}A_{k}\right)+\mu\left({\sum\nolimits_{n+1}}^{\infty}A_{k} \right)={\sum\nolimits_{1}}^{n}\mu(A_{k})+\mu\left({\sum\nolimits_{n+1}}^{ \infty}A_{k}\right)\] (d) \[\to{\sum\nolimits_{1}}^{\infty}\mu(A_{k})+0,\]

where \(\mu({\sum\nolimits_{n+1}}^{\infty}A_{k})\to\mu(\emptyset)=0\) by continuity from above at \(\emptyset\), since \({\sum\nolimits_{n+1}}^{\infty}A_{k}\searrow\emptyset\) and \(\mu\) is finite. That is, this f.a. measure is also c.a., and hence it is a measure. \(\square\)

**Definition 1.7** ("Little oh," "big oh," and "at most" \(\oplus\)): We write:

\[\begin{array}{ll}a_{n}\equiv o(r_{n})&\mbox{if}\quad a_{n}/r_{n}\to 0,\\ a_{n}\equiv O(r_{n})&\mbox{if}\quad\bar{\lim}|a_{n}/r_{n}|\leq(\mbox{some $M$})<\infty.\end{array} \tag{16}\]

We write

\[a_{n}=b_{n}\oplus c_{n}\quad\mbox{if}\quad|a_{n}-b_{n}|\leq c_{n}. \tag{17}\]

This last notation allows us to string inequalities together linearly, instead of having to start a new inequality on a new line. (I use it often.)

**Exercise 1.3** (\(\pi\)-systems and \(\lambda\)-systems): Consider a measurable space (\(\Omega\), \({\cal A}\)). A class \({\cal D}\) of subsets is called a \(\lambda\)-_system_ if it contains the space \(\Omega\) and all proper differences (\(A\setminus B\), when \(B\subset A\) with both \(A,\,B\in{\cal D}\)) and if it is closed under monotone increasing limits. (Recall that a class is called a \(\pi\)-_system_ if it is closed under finite intersections, while \(\bar{\pi}\)-systems are also required to contain \(\Omega\).)

(a) The minimal \(\lambda\)-system generated by a class \({\cal C}\) is denoted by \(\lambda[{\cal C}]\). Show that \(\lambda[{\cal C}]\) is equal to the intersection of all \(\lambda\)-systems containing \({\cal C}\).

(b) A collection \({\cal A}\) of subsets of \(\Omega\) is a \(\sigma\)-field if and only if it is both a \(\pi\)-system and a \(\lambda\)-system.

(c) Let \({\cal C}\) be a \(\pi\)-system and let \({\cal D}\) be a \(\lambda\)-system. Then \({\cal C}\subset{\cal D}\) implies that \(\sigma[{\cal C}]\subset{\cal D}\). Note (or, show) that this follows from (19) below.

**Proposition 1.5**\({}^{o}\) [Dynkin's \(\pi\)-\(\lambda\) theorem] Let \(\mu\) and \(\mu^{\prime}\) be two finite measures on the measurable space \((\Omega,\ {\cal A})\). Let \({\cal C}\) be a \(\bar{\pi}\)-system, where \({\cal C}\subset{\cal A}\). Then

\[\mu=\mu^{\prime}\,\mbox{on the $\bar{\pi}$-system ${\cal C}$ \ \ \ implies \ \ }\mu=\mu^{\prime}\,\mbox{on $\sigma[{\cal C}]$}. \tag{18}\]

**Proof.** (The author never includes this proof or the next one in lectures.) We first show that on _any_ measurable space \((\Omega,\ {\cal A})\) we have

\[\sigma[{\cal C}]=\lambda[{\cal C}]\ \ \ \mbox{when ${\cal C}$ is a $\pi$-system of subsets of ${\cal A}$}. \tag{19}\]

Let \({\cal D}\equiv\lambda[{\cal C}]\). By the easy exercise 1.3(a)(b), it suffices to show that \({\cal D}\) is a \(\pi\)-system (that is, that \(A\), \(B\in{\cal D}\) implies \(A\cap B\in{\cal D}\)). We first go just halfway; let

(a) \[{\cal E}_{C}\equiv\{A\in{\cal D}:AC\in{\cal D}\},\qquad\mbox{for any fixed}\,C \in{\cal C}.\]

Then \({\cal C}\subset{\cal E}_{C}\), and \(\Omega\in{\cal E}_{C}\). Also, for \(A,B\in{\cal E}_{C}\) with \(B\subset A\) and for \(C\in{\cal C}\) we have (since both \(AC\) and \(BC\) are in \({\cal D}\)) that \((A\backslash B)C=(AC\backslash BC)\in{\cal D}\), so that \(A\backslash B\in{\cal E}_{C}\). Finally, if \(A_{n}\) is \(\nearrow\) in \({\cal E}_{C}\), then \(A_{n}C\) is \(\nearrow\) in \({\cal D}\); so \(A\equiv\lim A_{n}\) has \(AC\in{\cal D}\), and \(A\in{\cal E}_{C}\). Thus \({\cal E}_{C}\) is a \(\lambda\)-system containing \({\cal C}\). Thus \({\cal E}_{C}={\cal D}\), since \({\cal D}\) was the smallest such class. We have thus learned of \({\cal D}\) that

(b) \[AC\in{\cal D}\,\,\mbox{for all}\,C\in{\cal C},\ \ \mbox{for each}\ A\in{\cal D}.\]

To go the rest of the way, we define

(c) \[{\cal F}_{D}\equiv\{A\in{\cal D}:AD\in{\cal D}\},\,\mbox{for any fixed}\ D\in{ \cal D}.\]

Then \({\cal C}\subset{\cal F}_{D}\), by (b), and \(\Omega\in{\cal F}_{D}\). Also, for \(A,B\in{\cal F}_{D}\) with \(B\subset A\) and for \(D\in{\cal D}\) we have (since both \(AD\) and \(BD\) are in \({\cal D}\)) that \((A\backslash B)D=(AD\backslash BD)\in{\cal D}\), so that \(A\backslash B\in{\cal F}_{D}\). Finally, if \(A_{n}\) is \(\nearrow\) in \({\cal F}_{D}\), then \(A_{n}D\) is \(\nearrow\) in \({\cal D}\); so \(A\equiv\lim A_{n}\) has \(AD\in{\cal D}\), and \(A\in{\cal F}_{D}\). Thus \({\cal F}_{D}\) is a \(\lambda\)-system containing \({\cal C}\). Thus \({\cal F}_{D}={\cal D}\), since \({\cal D}\) was the smallest such class. We have thus learned of \({\cal D}\) that

(d) \[AD\in{\cal D}\,\,\mbox{for all}\ A\in{\cal D},\,\,\mbox{for each}\ D\in{ \cal D}.\]

That is, \({\cal D}\) is closed under intersections; and thus \({\cal D}\) is a \(\pi\)-system. Thus (19) holds.

We will now demonstrate that \({\cal G}\equiv\{A\in{\cal A}:\mu(A)=\mu^{\prime}(A)\}\) is a \(\lambda\)-system on \(\Omega\). First, \(\Omega\in{\cal G}\), since \(\Omega\) is in the \(\bar{\pi}\)-system \({\cal C}\). Second, when \(A\subset B\) are both in \({\cal G}\) we have the equality (since \(\mu(A)\) and \(\mu^{\prime}(A)\) are finite)

(e) \[\mu(B\backslash A)=\mu(B)-\mu(A)=\mu^{\prime}(B)-\mu^{\prime}(A)=\mu^{\prime}( B\backslash A),\]

giving \(B\backslash A\in{\cal G}\). Finally, let \(A_{n}\nearrow A\) with all \(A_{n}\)'s in \({\cal G}\). Then proposition 1.2(i) yields the result

(f) \[\mu(A)=\lim\mu(A_{n})=\lim\mu^{\prime}(A_{n})=\mu^{\prime}(A),\]

so that \(A\in{\cal G}\). Thus \({\cal G}\) is a \(\lambda\)-system.

Thus the collection \({\cal G}\) on which \(\mu=\mu^{\prime}\) is a \(\lambda\)-system that contains the \(\bar{\pi}\)-system \({\cal C}\). Applying (19) shows that \(\sigma[{\cal C}]\subset{\cal G}\). \(\Box\)

The previous result is very useful in extending the verification of independence from small classes of sets to larger ones. The next proposition is used for both Fubini's theorem and the existence of a regular conditional probability distribution. It could also have been used below to give an alternate proof of uniqueness in the Caratheodory extension theorem.

**Proposition 1.6**: \({}^{*}\)(Minimal monotone class; Halmos)\({}^{*}\)The minimal monotone class \({\cal M}\equiv m[{\cal C}]\) containing the field \({\cal C}\) and the minimal \(\sigma\)-field \(\sigma[{\cal C}]\) generated by the same field \({\cal C}\) satisfy

\[m[{\cal C}]=\sigma[{\cal C}]\quad\mbox{when ${\cal C}$ is a field.} \tag{20}\]

**Proof.** Since \(\sigma\)-fields are monotone classes, we have that \(\sigma[{\cal C}]\supset{\cal M}\). If we now show that \({\cal M}\) is a field, then proposition 1.1(c) will imply that \(\sigma[{\cal C}]\subset{\cal M}\).

To show that \({\cal M}\) is a field, it suffices to show that

(a) \[A,B\mbox{ in ${\cal M}$ implies $AB,A^{c}B,AB^{c}$ are in ${\cal M}$.}\]

Suppose that (a) has been established. We will now show that (a) implies that \({\cal M}\) is a field. Complements: Let \(A\in{\cal M}\), and note that \(\Omega\in{\cal M}\), since \({\cal C}\subset{\cal M}\). Then \(A,\Omega\in{\cal M}\) implies that \(A^{c}=A^{c}\Omega\in{\cal M}\) by (a).

Unions: Let \(A,B\in{\cal M}\). Then \(A\cup B=(A^{c}\cap B^{c})^{c}\in{\cal M}\).

Thus \({\cal M}\) is indeed a field, provided that (a) is true. It thus suffices to prove (a).

For each \(A\in{\cal M}\), let \({\cal M}_{A}\equiv\{B\in{\cal M}\ :\ AB,\ A^{c}B,\ AB^{c}\in{\cal M}\}\). Note that it suffices to prove that

(b) \[{\cal M}_{A}={\cal M}\quad\mbox{ for each fixed $A\in{\cal M}$.}\]

We first show that

(c) \[{\cal M}_{A}\mbox{ is a monotone class.}\]

Let \(B_{n}\) be monotone in \({\cal M}_{A}\), with limit set \(B.\) Since \(B_{n}\) is monotone in \({\cal M}_{A}\), it is also monotone in \({\cal M}\), and thus \(B\equiv\lim_{n}B_{n}\in{\cal M}\). Since \(B_{n}\in{\cal M}_{A}\), we have \(AB_{n}\in{\cal M}\), and since \(AB_{n}\) is monotone in \({\cal M}\), we have \(AB=\lim_{n}AB_{n}\in{\cal M}\). In like fashion, \(A^{c}B\) and \(AB^{c}\) are in \({\cal M}\). Therefore, \(B\in{\cal M}_{A}\), by definition of \({\cal M}_{A}\). That is, (c) holds.

We next show that

(d) \[{\cal M}_{A}={\cal M}\quad\mbox{ for each fixed $A\in{\cal C}$.}\]

Let \(A\in{\cal C}\) and let \(C\in{\cal C}\). Then \(A\in{\cal M}_{C}\), since \({\cal C}\) is a field. But \(A\in{\cal M}_{C}\) if and only if \(C\in{\cal M}_{A}\), by the symmetry of the definition of \({\cal M}_{A}\). Thus \({\cal C}\subset{\cal M}_{A}\). That is, \({\cal C}\subset{\cal M}_{A}\subset{\cal M}\), and \({\cal M}_{A}\) is a monotone class by (c). But \({\cal M}\) is the minimal monotone class containing \({\cal C}\), by the definition of \({\cal M}\). Thus (d) holds. But in fact, we shall now strengthen (d) to

(e) \[{\cal M}_{B}={\cal M}\quad\mbox{ for each fixed $B\in{\cal M}$.}\]

The conditions for membership in \({\cal M}\) imposed on pairs \(A,B\) are symmetric. Thus for \(A\in{\cal C}\), the statement established above in (d) that \(B\in{\cal M}(={\cal M}_{A})\) is true if and only if \(A\in{\cal M}_{B}\). Thus \({\cal C}\subset{\cal M}_{B}\), where \({\cal M}_{B}\) is a monotone class. Thus \({\cal M}_{B}={\cal M}\), since (as was earlier noted) \({\cal M}\) is the smallest such monotone class. Thus (e) (and hence (a)) is established.

## 2 Construction and Extension of Measures

**Definition 2.1** (Outer extension): Let \(\Omega\) be arbitrary. Let \(\mu\) be a c.a. measure on a field \(\mathcal{C}\) of subsets \(\Omega\). For each \(A\in 2^{\Omega}\) define

\[\mu^{*}(A)\equiv\inf\left\{\sum_{n=1}^{\infty}\mu(A_{n}):A\subset\bigcup_{n=1} ^{\infty}A_{n}\mbox{ with all }A_{n}\in\mathcal{C}\right\}. \tag{1}\]

(This \(\mu^{*}\) will turn out to be an outer measure on the class \(2^{\Omega}\).) Now, \(\mu^{*}\) is called the _outer extension_ of \(\mu\). The sequences \(A_{1},A_{2},\ldots\) are called _Caratheodory coverings_. (There is always at least one covering of every subset \(A\), since \(\Omega\in\mathcal{C}\).)

**Theorem 2.1** (Caratheodory extension theorem): Let \(\mu\) be a c.a. measure on a field \(\mathcal{C}\). Then the \(\mu^{*}\) of (1) is a measure on the class \(\mathcal{A}^{*}\) of \(\mu^{*}\)-measurable sets (as defined in (1.1.7)), and \(\mathcal{A}^{*}\) is necessarily a \(\sigma\)-field. Moreover, \(\sigma[\mathcal{C}]\subset\mathcal{A}^{*}\) The c.a. measure \(\mu\) is extended from \(\mathcal{C}\) to be a c.a. measure on \(\mathcal{A}^{*}\) simply by defining

\[\mu(A)\equiv\mu^{*}(A)\quad\mbox{for each }A\mbox{ in }\mathcal{A}^{*}. \tag{2}\]

If \(\mu\) is \(\sigma\)-finite on \(\mathcal{C}\), then the extension (2) of \(\mu\) to \(\mathcal{A}^{*}\) is unique and is also \(\sigma\)-finite.

**Comment:** Later, exercise 2.1 will extend \(\mu\) to a "completed" \(\sigma\)-field \(\hat{\mathcal{A}}_{\mu}\). When \(\mu\) is \(\sigma\)-finite on \(\mathcal{C}\), corollary 3 and proposition 2.1 below will then imply that \(\mathcal{A}^{*}=\hat{\mathcal{A}}_{\mu}\) and that this is the largest possible \(\sigma\)-field to which \(\mu\) can be uniquely extended.

**Proof.** The proof proceeds by a series of claims.

Claim 1: \(\mu^{*}\) is an outer measure on \((\Omega,2^{\Omega})\).

Null: Now, \(\mu^{*}(\emptyset)=0\), since \(\emptyset,\emptyset,\ldots\) is a covering of \(\emptyset\).

Monotone: Let \(A\subset B\). Then every covering of \(B\) is also a covering of \(A\). Thus \(\mu^{*}(A)\leq\mu^{*}(B)\). Countably subadditive: Let all \(A_{n}\subset\Omega\) be arbitrary. Let \(\epsilon>0\). For each \(A_{n}\) there is a covering \(\{A_{nk}:k\geq 1\}\) such that

\[\sum\nolimits_{1}^{\infty}\mu(A_{nk})\leq\mu^{*}(A_{n})+\epsilon/2^{n},\quad \mbox{since }\mu^{*}(A_{n})\mbox{ is an infimum}. \tag{3}\]

[The choice of a convergent series (like \(\epsilon/2^{n}\)) that adds to \(\epsilon\) is an important technique for the reader to learn.] Now \(\cup_{n}A_{n}\subset\cup_{n}(\cup_{k}A_{nk})\). Thus (since \(\mu^{*}\) is monotone)

\[\mu^{*}(\bigcup_{n}A_{n})\leq\mu^{*}(\bigcup_{n}\bigcup_{k}A_{nk })\leq\sum\nolimits_{n}\sum\nolimits_{k}\mu(A_{nk})\] \[\qquad\mbox{since the }A_{nk}\mbox{'s form a covering of the set }\bigcup_{n}\bigcup_{k}A_{nk}\] \[\leq\sum\nolimits_{n}[\,\mu^{*}(A_{n})+\epsilon/2^{n}]=\sum \nolimits_{n}\mu^{*}(A_{n})+\epsilon.\qquad\qquad\mbox{by (\ref{eq:main})}\]

But \(\epsilon>0\) was arbitrary, and thus \(\mu^{*}(\bigcup_{n}A_{n})\leq\sum_{n}\mu^{*}(A_{n})\).

Claim 2: \(\mu^{*}|\mathcal{C}=\mu\) (that is, \(\mu^{*}(C)=\mu(C)\) for all \(C\in\mathcal{C}\)), and \(\mathcal{C}\subset\mathcal{A}^{*}\).

Let \(C\in\mathcal{C}\). Then \(\mu^{*}(C)\leq\mu(C)\), since \(C,\emptyset,\emptyset,\ldots\) is a covering of \(C.\) For the other direction, we let \(A_{1},A_{2},\ldots\) in \(\mathcal{C}\) be any covering of \(C.\) Since \(\mu\) is c.a. on \(\mathcal{C}\), and since \(\cup_{1}^{\infty}(A_{n}\cap C)=C\in\mathcal{C}\), we have from proposition 1.1.2(c) that

\[\mu(C)=\mu(\cup_{1}^{\infty}(A_{n}\cap C))\leq\sum\nolimits_{1}^{\infty}\mu(A _{n}\cap C)\leq\sum\nolimits_{1}^{\infty}\mu(A_{n}),\]

[MISSING_PAGE_FAIL:32]

Letting \(n\to\infty\) gives

(f) \[\mu^{*}(T)\geq\sum\nolimits_{1}^{\infty}\mu^{*}(TA_{k})+\mu^{*}(TA^{c})\] (g) \[\geq\mu^{*}(TA)+\mu^{*}(TA^{c})\quad\text{ since $\mu^{*}$ is countably subadditive.}\]

Thus \(A\in{\mathcal{A}}^{*}\).

Claim 6: \(\mu^{*}\) is c.a. on \({\mathcal{A}}^{*}\).

Replace \(T\) by \(A=\sum_{1}^{\infty}A_{n}\) in (f) to get \(\mu^{*}(A)\geq\sum_{1}^{\infty}\mu^{*}(A_{n})\), and then countable subadditivity gives the reverse inequality.

Claim 7: When \(\mu\) is a finite measure, its extension \(\mu^{*}\) to \({\mathcal{A}}^{*}\) is unique.

Let \(\nu\) denote any other extension of \(\mu\) to \({\mathcal{A}}^{*}\). Let \(A\) in \({\mathcal{A}}^{*}\). For any Caratheodory covering \(A_{1},A_{2},\ldots\) of \(A\) (with the \(A_{n}\)'s in \({\mathcal{C}}\)), countable subadditivity gives

\[\nu(A)\leq\nu(\cup_{1}^{\infty}A_{n})\leq\sum\nolimits_{1}^{\infty}\nu(A_{n})= \sum\nolimits_{1}^{\infty}\mu(A_{n}),\]

since \(\mu=\nu\) on \({\mathcal{A}}\). Thus (recall the definition of \(\mu^{*}\) in (1))

(h) \[\nu(A)\leq\mu^{*}(A)\quad\text{ for all $A\in{\mathcal{A}}^{*}$}.\]

Note that the measures \(\mu^{*}\) and \(\nu\) on \({\mathcal{A}}^{*}\) also satisfy

(i) \[\nu(A)+\nu(A^{c})=\nu(\Omega)=\mu^{*}(\Omega)=\mu^{*}(A)+\mu^{*}(A^{c})\]

for all \(A\) in \({\mathcal{A}}^{*}\) (using \(\Omega\in{\mathcal{A}}\) for \(\nu(\Omega)=\mu^{*}(\Omega)\)). Since (h) gives both

(j) \[\nu(A)\leq\mu^{*}(A)\quad\text{ and }\quad\nu(A^{c})\leq\mu^{*}(A^{c})\]

(where all four of these terms are finite), we can infer from (i) that

(k) \[\nu(A)=\mu^{*}(A)\quad\text{ for all $A\in{\mathcal{A}}^{*}$}.\]

This gives the uniqueness of \(\mu^{*}\) on \({\mathcal{A}}^{*}\)

Claim 8: Uniqueness of \(\mu^{*}\) on \({\mathcal{A}}^{*}\) also holds when \(\mu\) is a \(\sigma\)-finite measure on \({\mathcal{C}}\). Label the sets of the measurable partition as \(D_{n}\), and let \(\Omega_{n}\equiv\sum_{1}^{n}D_{k}\) so that \(\Omega_{n}\nearrow\Omega\). Claim 7 establishes that

(l) \[\nu(A\Omega_{n})=\mu^{*}(A\Omega_{n})\quad\text{ for all $A\in{\mathcal{A}}^{*}$}.\]

It follows that

(m) \[\nu(A)=\lim_{n}\nu(A\Omega_{n})\] by proposition 1.1.2 \[=\lim_{n}\mu^{*}(A\Omega_{n})\] by (1) (n) \[=\mu^{*}(A)\] by proposition 1.1.2,

completing the proof. In fact, the following corollary was established. 

**Corollary 1** The \(\mu^{*}\)-measurable sets \({\mathcal{A}}^{*}\) of (1.1.7) were shown to contain \(\sigma[{\mathcal{C}}].\) Thus we can now view the c.a. measure \(\mu\) on the field \({\mathcal{C}}\) has having been extended uniquely to the (useful and intelligible) \(\sigma\)-field \(\sigma[{\mathcal{C}}]\), or as having been extended to the (less intelligible, but at least as large) \(\sigma\)-field \({\mathcal{A}}^{*}\). (We seek clarity.)

**Questions** When we extended our measure \(\mu\) from the field \({\cal C}\) to the \(\sigma\)-field \({\cal A}^{*}\), did we actually go beyond \(\sigma[{\cal C}]\)? Can we go further? Corollary 2 and Corollary 3 will show that we can always "complete" \(\mu\) on \(\sigma[{\cal C}]\), and in so doing extend it to (an obviously useful) \(\sigma\)-field named \(\hat{\cal A}_{\mu}\), that is still contained in \({\cal A}^{*}\). Proposition 2 will show that there are definite limitations to extension (the most famous example being the "Lebesgue sets" of proposition 2). In doing so, proposition 2 will imply that \({\cal A}^{*}=\hat{\cal A}_{\mu}\), which now gives us a very useful interpretation of \({\cal A}^{*}\).

**Definition 2** **(Complete measures)** Let \((\Omega,\ {\cal A},\ \mu)\) denote a measure space. If \(\mu(A)=0\), then \(A\) is called a _null set_. We call \((\Omega,\ {\cal A},\ \mu)\)_complete_ if whenever we have \(A\subset\) (some \(B\)) \(\in{\cal A}\) with \(\mu(B)=0\), we necessarily also have \(A\in{\cal A}\). (That is, all subsets of sets of measure \(0\) are required to be measurable.)

**Exercise 2** **(Completion)** Let \((\Omega,\ {\cal A},\ \mu)\) denote a measure space. Show that

\[\hat{\cal A}_{\mu}\equiv\{A:A_{1}\subset A\subset A_{2}\ \mbox{with}\ A_{1},A_{2}\in{\cal A}\ \mbox{and}\ \mu(A_{2}\backslash A_{1})=0\}\]

\[=\{A\cup N:A\in{\cal A},\ \mbox{and}\ N\subset\,(\mbox{some}\ B)\in{\cal A}\ \mbox{having}\ \mu(B)=0\}\]

\[=\{A\triangle N:A\in{\cal A},\ \mbox{and}\ N\subset\,(\mbox{some}\ B)\in{\cal A}\,\mbox{having}\ \mu(B)=0\},\]

and that \(\hat{\cal A}_{\mu}\) is a \(\sigma\)-field. Define \(\hat{\mu}\) on \(\hat{\cal A}_{\mu}\) by

\[\hat{\mu}(A\cup N)=\mu(A)\]

for all \(A\in{\cal A}\) and for all \(N\subset\) (some \(B\)) \(\in{\cal A}\) having \(\mu(B)=0\). Show that \((\Omega,\hat{\cal A}_{\mu},\hat{\mu})\) is a complete measure space for which \(\hat{\mu}|{\cal A}=\mu\). (Note: A proof must include a demonstration that definition (7) leads to a well-defined \(\hat{\mu}\). That is, whenever \(A_{1}\cup N_{1}=A_{2}\cup N_{2}\) we must have \(\mu(A_{1})=\mu(A_{2})\), so that \(\hat{\mu}(A_{1}\cup N_{1})=\hat{\mu}(A_{2}\cup N_{2})\).)

**Definition 2** **(Lebesgue sets)** The completion of Lebesgue measure on \((R,\ {\cal B},\ \lambda)\) is still called Lebesgue measure, and it is still denoted by \(\lambda\). The resulting completed \(\sigma\)-field \(\hat{\cal B}_{\lambda}\) of the Borel sets \({\cal B}\) is called the _Lebesgue sets_.

**Corollary 2** When we complete a measure \(\mu\) on a \(\sigma\)-field \({\cal A}\), this completed measure \(\hat{\mu}\) is the unique extension of \(\mu\) to \(\hat{\cal A}_{\mu}\). (It is typical to denote such extensions by \(\mu\) also (rather than by \(\hat{\mu}\)), and to always make such extensions automatically.)

**Corollary 3** (Thus when we begin with a \(\sigma\)-finite measure \(\mu\) on a field \({\cal C}\), both the extension to \({\cal A}\equiv\sigma[{\cal C}]\) and the further extension to \(\hat{\cal A}_{\mu}\equiv\hat{\sigma}[{\cal C}]_{\mu}\) are unique.) Here, we note that all sets in \(\hat{\cal A}_{\mu}=\hat{\sigma}[{\cal C}]_{\mu}\) are in the class \({\cal A}^{*}\) of \(\mu^{*}\)-measurable sets. (Proposition 2 below will imply that \(\hat{\cal A}_{\mu}={\cal A}^{*}\).)

**Proof.** Consider corollary 2 first. Let \(\nu\) denote any extension to \(\hat{\cal A}_{\mu}\). We will demonstrate that

\[\mbox{(a)}\qquad\nu(A\cup N)=\mu(A)\quad\mbox{ for all}\ A\in{\cal A},\ \mbox{and all null sets}\ N\]

(that is, \(\nu=\hat{\mu}\)). Assume not. Then there exist sets \(A\in{\cal A}\) and \(N\subset\) (some \(B\)) in \({\cal A}\) with \(\mu(B)=0\) such that \(\nu(A\cup N)>\mu(A)\) (necessarily, \(\nu(A\cup N)\geq\nu(A)=\mu(A)\)). For this \(A\) and \(N\) we have\[\mu(A) =\nu(A)<\nu(A\cup N)=\nu(A\cup(A^{c}N))\quad\mbox{where $A^{c}N \subset A^{c}B=(\mbox{null})$}\] (b) \[=\nu(A)+\nu(A^{c}N)\leq\nu(A)+\nu(B)\] since \(\nu\) is a measure on the completion (c) \[=\mu(A)+\mu(B)\quad\mbox{ since $\nu$ is an extension of $\mu$}.\]

Hence \(\mu(B)>0\), which is a contradiction. Thus the extension is unique.

We now turn to corollary 3. Only the final claim needs demonstrating. Suppose \(A\) is in \(\hat{\sigma}[{\cal C}]_{\mu}\). Then \(A=A^{\prime}\cup N\) for some \(A^{\prime}\in{\cal A}\) and some \(N\) satisfying \(N\subset B\) with \(\mu(B)=0\). Since \({\cal A}^{*}\) is a \(\sigma\)-field, it suffices to show that any such \(N\) is in \({\cal A}^{*}\). Since \(\mu^{*}\) is subadditive and monotone, we have

\[\mu^{*}(T)\leq\mu^{*}(TN)+\mu^{*}(TN^{c})=\mu^{*}(TN^{c})\leq\mu^{*}(T),\] (d)

because \(\mu^{*}(TN)=0\) follows from using \(B,\emptyset\emptyset,\ldots\) to cover \(TN\). Thus equality holds in this last equation, showing that \(N\) is \(\mu^{*}\)-measurable. \(\Box\)

**Exercise 2.2** Let \(\mu\) and \(\nu\) be finite measures on \((\Omega,\ {\cal A})\).

(a) Show by example that \(\hat{\cal A}_{\mu}\) and \(\hat{\cal A}_{\nu}\) need not be equal.

(b) Prove or disprove each half: \(\hat{\cal A}_{\mu}=\hat{\cal A}_{\nu}\) iff \(\mu\) and \(\nu\) have identical null sets.

(c) Give an example of an LS-measure \(\mu\) on \(R\) (see section 1.3) for which \(\hat{\cal B}_{\mu}=2^{R}\).

**Exercise 2.3** (a) Replace the sets \(A_{n}\) in the field \({\cal C}\) used in definition (1) by the disjoint sets \(B_{n}\equiv A_{n}A_{n-1}^{c}\cdots A_{2}^{c}A_{1}^{c}\) (which are also in \({\cal C}\)), to show that this definition could have insisted on using disjoint sets \(A_{n}\).

(b) **(Approximation lemma; Halmos)** Let the \(\sigma\)-finite measure \(\mu\) on the field \({\cal C}\) be extended to \({\cal A}=\sigma[{\cal C}]\); also refer to this extension as \(\mu\). Show that for each \(A\in{\cal A}\) (or \(\hat{\cal A}_{\mu}\)) having \(\mu(A)<\infty\), and for each \(\epsilon>0\),

\[\mu(A\triangle C)<\epsilon\quad\mbox{for some set}\,C\in{\cal C}. \tag{8}\]

(Hint. Truncate the sum in (1.2.1) to define \(C\).)

(c) If \(\mu(A)=\infty\), then (8) can fail, even in a simple case--with a bad choice for \({\cal C}\). Let \(\mu\) denote counting measure on the integers. Now \({\cal C}\equiv\{C:C\mbox{ or }C^{c}\mbox{ is finite}\}\) is a field. Determine \(\sigma[{\cal C}]\). Show that (8) fails for the set \(A\) of even integers.

**Definition 2.4** (Regular measures on metric spaces) Let \(d\) denote a metric on \(\Omega\), let \({\cal A}\) denote the Borel sets, and let \(\mu\) be a measure on \((\Omega,\ {\cal A})\). Suppose that for each set \(A\) in \(\hat{\cal A}_{\mu}\), and for every \(\epsilon>0\), one can find an open set \(O_{\epsilon}\) and a closed set \(C_{\epsilon}\) for which both \(C_{\epsilon}\subset A\subset O_{\epsilon}\) and \(\mu(O_{\epsilon}\backslash C_{\epsilon})<\epsilon\). Suppose also that if \(\mu(A)<\infty\), one then requires that the set \(C_{\epsilon}\) be compact. Then \(\mu\) is called a _regular measure_. (Note exercise 1.3.1 below. Contrast its content with (8).)

**Exercise 2.4** (Nonmeasurable sets) Let \(\Omega\) consist of the sixteen values \(1\),..., \(16\). (Think of them arranged in four rows of four values.) Let

\[\begin{array}{ll}C_{1}=\{1,2,3,4,5,6,7,8\},&C_{2}=\{9,10,11,12,13,14,15,16\},\\ C_{3}=\{1,2,5,6,9,10,13,14\},&C_{4}=\{3,4,7,8,11,12,15,16\}.\end{array}\]

Let \({\cal C}\) denote the field generated by \(\{C_{1},\ C_{2},\ C_{3},\ C_{4}\}\), and let \({\cal A}=\sigma[{\cal C}]\).

(a) Show that \({\cal A}\equiv\sigma[{\cal C}]\neq 2^{\Omega}\). (Note that \(2^{\Omega}\) contains \(2^{16}=65\),\(536\) sets.)(b) Let \(\mu(C_{i})=\frac{1}{2},1\leq i\leq 4\), with \(\mu(C_{1}C_{3})=\frac{1}{4}\). Show \(\hat{\mathcal{A}}_{\mu}=\mathcal{A}\), with \(2^{4}=16\) sets.

(c) Let \(\mu(C_{i})=\frac{1}{2},i=2,3,4\), with \(\mu(C_{2}C_{4})=0\). Show that \(\hat{\mathcal{A}}_{\mu}\) has \(2^{10}=1024\) sets.

(d) Illustrate proposition 2.1 below in the context of this exercise.

**Proposition 2.1** (Not all sets need be measurable) Let \(\mu\) be a measure on \(\mathcal{A}\equiv\sigma[\mathcal{C}]\), with \(\mathcal{C}\) a field. If \(B\notin\hat{\mathcal{A}}_{\mu}\), then there are infinitely many measures on \(\sigma[\hat{\mathcal{A}}_{\mu}\cup\{B\}]\) that agree with \(\mu\) on \(\mathcal{C}\). (Thus the \(\sigma\)-field \(\hat{\mathcal{A}}_{\mu}\) is as far as we can go with the unique extension process. We merely state this observation for reference, without proof. Note exercise 2.4d.)

**Proposition 2.2** (Not all subsets are Lebesgue sets): There is a subset \(\mathit{D}\) of \(\mathit{R}\) that is not in the Lebesgue sets \(\hat{\mathcal{B}}_{\lambda}\). (This will require the axiom of choice.)

**Proof.** Define the equivalence relation \(\sim\) on elements of \([0,\,1)\) by \(x\sim y\) if \(x-y\) is a rational number. Use the axiom of choice to specify a set \(\mathit{D}\) that contains exactly one element from each equivalence class. Now define \(D_{z}\equiv\{z+x\ (\text{modulo }1):x\in D\}\) for each rational \(z\) in \([0,1)\), so that \([0,1)=\sum_{z}D_{z}\) represents \([0,\,1)\) as a countable union of disjoint sets. Moreover, all \(D_{z}\) must have the same outer measure; call it \(a\). Assume \(D=D_{0}\) is measurable. But then \(1=\lambda([0,1))=\sum_{z}\lambda(D_{z})=\sum_{z}a\) gives only \(\sum_{z}a=0\) (when \(a=0\)) and \(\sum_{z}a=\infty\) (when \(a>0\)) as possibilities. This is a contradiction. Thus \(D\notin\hat{\mathcal{B}}_{\lambda}\). \(\square\)

**Exercise 2.5**: Just understand the sketch above, noting that \(D=\sum_{z}D_{z}\) (with disjoint sets \(D_{z}\) for the rationals \(z\)).

**Proposition 2.3** (Not all Lebesgue sets are Borel sets): There necessarily exists a set \(A\in\hat{\mathcal{B}}_{\lambda}\backslash\mathcal{B}\) that is a Lebesgue set but not a Borel set.

**Proof.** This proof follows exercise 6.3.4 below; it requires the axiom of choice. \(\square\)

**Exercise 2.6**: Every subset \(\mathit{A}\) of \(\Omega\) having \(\mu^{*}(A)=0\) is a \(\mu^{*}\)-measurable set.

**Exercise 2.7**: \({}^{*}\): Show that the Caratheodory theorem can fail if \(\mu\) is not \(\sigma\)-finite.

**Coverings**

Earlier in this section we encountered Caratheodory coverings.

**Exercise 2.8**: \({}^{*}\) (Vitali covering): (a) We say that a family \(\mathcal{V}\) of intervals \(I\) is a _Vitali covering_ of a set \(\mathit{D}\) if for each \(x\in D\) and each \(\epsilon>0\) there exists an interval \(I\in\mathcal{V}\) for which \(x\in I\) and \(\lambda(I)<\epsilon\).

(b) (Vitali covering theorem) Let \(D\subset R\) have outer Lebesgue measure \(\lambda^{*}(D)<\infty\). Let \(\mathcal{V}\) be a collection of closed intervals that forms a Vitali covering of \(\mathit{D}.\) Then there exists a finite number of pairwise disjoint intervals \((I_{1},\ \dots,\ I_{m})\) in \(\mathcal{V}\) whose Lebesgue outer measure \(\lambda^{*}\) satisfies

\[\lambda^{*}\left(D\backslash\sum\nolimits_{j=1}^{m}I_{j}\right)<\epsilon. \tag{9}\]

(Compare this "nice approximation" of a set to the nice approximations given in exercise 2.3 and in definition 2.4.) [Lebesgue measure \(\lambda\) will be formally shown to exist in the next section, and \(\lambda^{*}\) will be discussed more fully.] [Result (9) will be useful in establishing the Lebesgue result that increasing functions on \(\mathit{R}\) necessarily have a derivative, except perhaps on a set having Lebesgue measure zero.]

**Exercise 2.9**: (Heine-Borel) If \(\{U_{t}:t\in T\}\) is an arbitrary collection of open sets that covers a compact subset \(D\) of \(R\), then there exists a finite number of them \(U_{1},\ldots,U_{m}\) that also covers \(D\).

The familiar Heine-Borel result will be frequently used. It is stated here only to contrast it with the important new ideas of Caratheodory and Vitali coverings.

## Chapter 1 Measures

### 1.1 Lebesgue-Stieltjes Measures

At the moment we know only a few measures informally. We now construct the large class of measures that lies at the heart of probability theory.

**Definition 1.1** (Lebesgue-Stieltjes measure): A measure \(\mu\) on the real line \(R\) assigning finite values to finite intervals is called a _Lebesgue-Stieltjes measure_. (The measure \(\mu\) on \((R,\ 2^{R})\) whose value \(\mu(A)\) for any set \(A\) equals the number of rationals in \(A\) is _not_ a Lebesgue-Stieltjes measure.)

**Definition 1.2** (gdf): A finite \(\nearrow\) function \(F\) on \(R\) that is right-continuous is called a _generalized \(df\)_ (to be abbreviated \(gdf\)). Then \(F\_(\cdot)\equiv\lim_{y\nearrow}.F(y)\) denotes the left-continuous version of \(F.\) The _mass function_ of \(F\) is defined by

\[\Delta F(\cdot)\equiv F(\cdot)-F\_(\cdot),\quad\mbox{while}\quad F(a,\ b]\ \equiv F(b)-F(a)\quad\mbox{for all}\ \ a\leq b\]

is called the _increment function_ of \(F.\) Identify gdfs having the same increment function. Only one member \(F\) of each equivalence class so obtained satisfies \(F_{-}(0)=0\), and this \(F\) can (and occasionally will) be used as the _representative member of the class_ (also to be called the _representative \(gdf\)_).

**Example 1.1**: We earlier defined three measures on \((R,\mathcal{B})\) informally.

(a) For Lebesgue measure \(\lambda\), a gdf is the identity function \(F(x)=x\).

(b) For counting measure, a gdf is the greatest integer function \(F(x)=[x]\).

(c) For unit point mass at \(x_{0}\), a gdf is \(F(x)=1_{[x_{0},\infty)}(x)\). \(\Box\)

**Theorem 1.2** (Correspondence theorem; Loeve): The relationship

\[\mu((a,\ b])\equiv F(a,\ b]\quad\mbox{for all}-\infty\leq a\leq b\leq+\infty \tag{1.1}\]

establishes a 1-to-1 correspondence between the Lebesgue-Stieltjes measures \(\mu\) on \(\mathcal{B}\) and the set of representative members of the equivalence classes of generalized dfs. (Each such \(\mu\) extends uniquely to \(\hat{\mathcal{B}}_{\mu}-\)but it is still labeled as \(\mu,\ \mbox{not}\ \hat{\mu}\).)

**Notation 1.3**: We formally establish some notation that will be used throughout. Important classes of sets include:

\[\mathcal{C}_{I}\equiv\{\mbox{all intervals}\ (a,\ b],\ (-\infty,\ b],\ \mbox{or}\ (a,\ +\infty):-\infty<a<b<+\infty\}. \tag{1.2}\]

\[\mathcal{C}_{F}\equiv\{\mbox{all finite disjoint unions of intervals}\ \mbox{in}\,\mathcal{C}_{I}\}=\mbox{(a field).} \tag{1.3}\]

\[\mathcal{B}\equiv\sigma[\mathcal{C}_{F}]\equiv\mbox{(the $\sigma$-field of Borel sets).} \tag{1.4}\]

\[\hat{\mathcal{B}}_{\mu}\equiv\mbox{(the $\sigma$-field $\mathcal{B}$ completed for the measure $\mu$).} \tag{1.5}\]

\[\bar{\mathcal{B}}\equiv\sigma[\mathcal{B},\ \{-\infty\},\ \{+\infty\}]\quad\mbox{and}\quad\bar{\mathcal{B}}_{\mu}\equiv\sigma[\hat{\mathcal{B}}_{\mu},\ \{-\infty\},\ \{+\infty\}]. \tag{1.6}\]

**Proof.** : Given a LS-measure \(\mu\), define the increment function \(F(a,\ b]\) via (1.1). We clearly have \(0\leq F(a,\ b]<\infty\) for all finite \(a,b,\) and \(F(a,\ b]\to 0\) as \(b\searrow a\), by proposition 1.1.2. Now specify \(F_{-}(0)\equiv 0\), \(F(0)\equiv\mu(\{0\})\), \(F(b)\equiv F(0)+F(0,\ b]\) for \(b>0\), and \(F(a)=F(0)-F(a,\ 0]\) for \(a<0\). This \(F(\cdot)\) is the representative gdf.

Given a representative gdf, we define \(\mu\) on the collection \(\mathcal{I}\) of all finite intervals \((a,\ b]\) via (1.1). We will now show that \(\mu\) is a well-defined and c.a. measure on this collection \(\mathcal{I}\) of finite intervals.

Nonnegative: \(\mu\geq 0\) for any \((a,\ b]\), since \(F\) is \(\nearrow\).

Null: \(\mu(\emptyset)=0\), since \(\emptyset=(a,\ a]\) and \(F(a,\ a]=0\).

Countably additive on \(\mathcal{I}\): Let \(I\equiv(a,\ b]=\sum_{1}^{\infty}I_{n}\equiv\sum_{1}^{\infty}(a_{n},\ b_{n}]\). We must show

(a) \[\mu(\sum_{1}^{\infty}I_{n})=\sum_{1}^{\infty}\mu(I_{n}).\]

First, we will show that \(\sum_{1}^{\infty}\mu(I_{k})\leq\mu(I)\). Fix \(n\). Then \(\sum_{1}^{n}I_{k}\subset I\), so that (relabel if necessary, so that \(I_{1},\ldots,I_{n}\) is a left-to-right ordering of these intervals)

(b) \[\sum_{1}^{n}\mu(I_{k})=\sum_{1}^{n}F(a_{k},\ b_{k}]\leq F(a,\ b]=\mu(I).\]

Letting \(n\to\infty\) in (b) gives the first claim.

Next, we will show that \(\mu(I)\leq\sum_{1}^{\infty}\mu(I_{k})\). Suppose \(b-a>\epsilon>0\) (the case \(b-a=0\) is trivial, as \(\mu(\emptyset)=0\)). Fix \(\theta>0\). For each \(k\geq 1\), use the right continuity of \(F\) to choose an \(\epsilon_{k}>0\) so small that

(c) \[F(b_{k},\ b_{k}+\epsilon_{k}]<\theta/2^{k},\ \mbox{and define}\ J_{k}\equiv(a_{k},\ c_{k})\equiv(a_{k},\ b_{k}+\epsilon_{k}).\]

These \(J_{k}\) form an open cover of the compact interval \([a+\epsilon,\ b]\), so that some finite number of them are known to cover \([a+\epsilon,\ b]\), by the Heine-Borel theorem. Sorting through these intervals one at a time, choose \((a_{1},\ c_{1})\) to contain \(b\), choose \((a_{2},\ c_{2})\) to contain \(a_{1}\), choose \((a_{3},\ c_{3})\) to contain \(a_{2},\ldots\) ; finally (for some \(K\)), choose \((a_{K},\ c_{K})\) to contain \(a+\epsilon\). Then (relabeling the subscripts, if necessary)

(d) \[\leq\sum_{1}^{\infty}\mu(I_{k})+\theta.\]

Let \(\theta\searrow 0\) and then \(\epsilon\searrow 0\) in (d) to obtain the second claim (and the truth of (a)) as

(e) \[\mu(I)=F(a,\ b]\leq\sum_{1}^{\infty}\mu(I_{k}).\]

We will now show that \(\mu\) is a well-defined c.a. measure on the given field \(\mathcal{C}_{F}\). If \(A=\sum_{n}I_{n}\in\mathcal{C}_{F}\) with each \(I_{n}\) of type \((a,\ b]\), then we define \(\mu(A)\equiv\sum_{n}\mu(I_{n})\). If we also have \(A=\sum_{m}I_{m}^{\prime}\), then we must show (where the subscripts \(m\) and \(n\) could take on either a finite or a countably infinite number of values) that

(f) \[\sum_{m}\mu(I_{m}^{\prime})=\sum_{n}\mu(I_{n})=\mu(A).\]

Now, \(I_{m}^{\prime}=A\cap I_{m}^{\prime}=\sum_{n}I_{n}I_{m}^{\prime}\) and \(I_{n}=AI_{n}=\sum_{m}I_{m}^{\prime}I_{n}\), so \(\mu\) is well defined by

(g) \[\sum_{m}\mu(I_{m}^{\prime})=\sum_{m}\sum_{n}\mu(I_{n}I_{m}^{\prime})=\sum_{n} \sum_{m}\mu(I_{n}I_{m}^{\prime})=\sum_{n}\mu(I_{n}I_{m}^{\prime})=\sum_{n}\mu (I_{n})=\mu(A).\]

The c.a. of \(\mu\) on \(\mathcal{C}_{F}\) is then trivial; if disjoint \(A_{n}=\sum_{m}I_{nm}\) for each \(n\), it then follows that \(A\equiv\sum_{n}A_{n}=\sum_{n}\sum_{m}I_{nm}\) with \(\mu(A)=\sum_{n}\sum_{m}\mu(I_{mn})=\sum_{n}\mu(A_{n})\).

Finally, a measure \(\mu\) on \(\mathcal{C}_{F}\) determines a unique measure on \(\mathcal{B}\), as is guaranteed by the Caratheodory extension of theorem 1.2.1.

**Exercise 3.1** (Calculating Lebesgue-Stieltjes measure): (i) Consider a gdf \(F\) on \(R\) and its associated Lebesgue-Stieltjes measure \(\mu_{F}\) on \((R,{\cal C}_{\cal F})\), for the field \({\cal C}_{\cal F}\) of (3). Show that we can replace the definition of the outer extension \(\mu_{F}^{*}\) to the Borel \(\sigma\)-field \({\cal B}\) in the Caretheodory fashion of (1.3.1) by

\[\mu_{F}^{*}(A)=\inf\Big{\{}{\sum}_{n=1}^{\infty}(F(d_{n})-F(c_{n}))\ :\ A\subset{ \sum}_{n=1}^{\infty}(c_{n},d_{n}]\Big{\}}\]

for countable unions of _disjoint subintervals_\((c_{n},\ d_{n}]\) of \(\bar{R}\). Show also that

\[\mu_{F}(A)=\inf\Big{\{}{\sum}_{n=1}^{\infty}(F(d_{n})-F(c_{n})):A\subset{\sum} _{n=1}^{\infty}(c_{n},d_{n}]\Big{\}}\ \mbox{for all}\ A\in\hat{\cal B}_{\mu_{F}}. \tag{7}\]

(This gives a very concrete visualization of how the \(\mu_{F}\) measure of an arbitrary subset of \(R\) is formed-and this approach is the same for every Lebesgue-Stieltjes measure \(\mu_{F}\) on \(R\).)

(ii) Let \(I\) denote _any_ subinterval of the real line \(R\). Let \(F\) denote any fixed gdf on \(I\). (Now \(F(x)=x\) is a gdf on \(R\) itself, \(F(x)=1-e^{-x}\) is a gdf on \([0,\ \infty)\), and \(F(x)=1/(1-x)-1\) is a gdf on [0,1).) Define \(\mu_{F}((a,\ b])=F(b)-F(a)\) for all \(a\leq b\) with \(a\) and \(b\) in \(\bar{I}\). Let \({\cal B}_{I}\equiv\{B\cap I:B\in{\cal B}\}\). It is still true that the right hand side of (7) gives the value of \(\mu_{F}(A)\) for every \(A\) in the completion of the \(\sigma\)-field \({\cal B}_{I}\). That is, taking the infimum over _disjoint intervals_ of the type \((a,\ b]\) is suffficient.

**Definition 3.3** (Absolutely continuous dfs): Say that a gdf \(F\) on an interval \(I\) is _absolutely continuous_ if for all \(\epsilon>0\) there exists a \(\delta_{\epsilon}>0\) for which

\[{\sum}_{k=1}^{n}|F(d_{k})-F(c_{k})|<\epsilon\quad\mbox{provided}\quad{\sum}_{k =1}^{n}(d_{k}-c_{k})<\delta_{\epsilon} \tag{8}\]

whenever \(n\geq 1\) and all of the intevals \((c_{k},\ d_{k}]\) are mutually disjoint subintervals of \(I\). (Note that the first and third dfs in part (ii) of the previous exercise are absolutely continuous, while the third one is continuous but not even uniformly continuous.)

**Exercise 3.2** (All LS-measures on \((R,{\cal B})\) are regular): Show that all Lebesgue-Stieltjes measures on \((R,{\cal B})\) are regular measures (see definition 1.2.4). (Use the open intervals \(J_{n}\) in the theorem 3.1 proof.)

**Probability Measures, Probability Spaces, and DFs**

**Definition 3.4** (**Probability distributions \(P(\cdot)\) and dfs \(F(\cdot)\)**):

(a) In probability theory we think of \(\Omega\) as the set of all possible outcomes of some experiment, and we refer to it as the _sample space_. The individual points \(\omega\) in \(\Omega\) are referred to as the _elementary outcomes_. The measurable subsets \(A\) in the collection \({\cal A}\) are referred to as _events_. A measure of interest is now denoted by \(P\); it is called a _probability measure_, and must satisfy \(P(\Omega)=1\). We refer to \(P(A)\) as the _probability_ of \(A\), for each event \(A\) in \(\hat{\cal A}_{P}\). The triple \((\Omega,{\cal A},P)\) (or \((\Omega,\hat{\cal A}_{P},\hat{P})\), if this is different) is referred to as a _probability space_.

(b) An \(\diagup\) right-continuous function \(F\) on \(R\) having \(F(-\infty)\equiv\lim_{x\to-\infty}F(x)=0\) and \(F(+\infty)\equiv\lim_{x\to+\infty}F(x)=1\) is called a _distribution function_ (which we will abbreviate as \(df\)). (For probability measures, setting \(F(-\infty)=0\) is used to specify the representative df.)

**Corollary 1** (The correspondence theorem for dfs): Defining \(P(\cdot)\) on all intervals \((a,b]\) via \(P((a,\ b])\equiv F(b)-F(a)\) for all \(-\infty\leq a<b\leq+\infty\) establishes a 1-to-1 correspondence between probability distributions \(P(\cdot)\) on \((R,{\cal B})\) and dfs \(F(\cdot)\) on \(R\).

**Exercise 3.3**  Prove this trivial corollary.

**Remark 3.1** (Density functions)  Those probability distributions that have an absolutely continuous df will turn out be be exactly those probablity distributions that have a "density function" \(f\). Moreover, \(F^{\prime}\) will always exist for _any_ df \(F\) (except perhaps on a set of Lebesgue measure 0), but it will serve as a density function only for the absolutely continuous dfs. Moreover, in chapter 4 we will also learn to think of a "probability mass function" as a density with respect to counting measure.

## Chapter 2 Measurable Functions and Convergence

### 1 Mappings and \(\sigma\)-Fields

**Notation 1.1** (Inverse images): Suppose \(X\) denotes a function mapping some set \(\Omega\) into the extended real line \(\bar{R}\equiv R\cup\{\pm\infty\}\); we denote this by \(X:\Omega\rightarrow\bar{R}\). Let \(X^{+}\) and \(X^{-}\) denote the _positive part_ and the _negative part_ of \(X\), respectively:

\[X^{+}(\omega)\equiv\left\{\begin{array}{ll}X(\omega)&\mbox{ if }X( \omega)\geq 0,\\ 0&\mbox{ else,}\end{array}\right. \tag{1}\]

\[X^{-}(\omega)\equiv\left\{\begin{array}{ll}-X(\omega)&\mbox{ if }X( \omega)\leq 0,\\ 0&\mbox{ else.}\end{array}\right. \tag{2}\]

Note that

\[X=X^{+}-X^{-}\quad\mbox{and}\quad|X|=X^{+}+X^{-}=X+2X^{-}=2X^{+}-X. \tag{3}\]

We also use the following notation:

\[[X=r]\equiv X^{-1}(r)\equiv\{\omega:X(\omega)=r\}\quad\mbox{ for all real }r, \tag{4}\]

\[[X\in B]\equiv X^{-1}(B)\equiv\{\omega:X(\omega)\in B\}\quad\mbox{for all Borel sets }B, \tag{5}\]

\[X^{-1}(\mathcal{B})\equiv\{X^{-1}(B):B\in\mathcal{B}\}. \tag{6}\]

We call these the _inverse images_ of \(r,B\), and \(\mathcal{B}\), respectively. We let

\[\bar{\mathcal{B}}\equiv\sigma[\mathcal{B},\{+\infty\},\{-\infty\}]. \tag{7}\]

Inverse images are also well-defined when \(X:\Omega\rightarrow\Omega^{\prime}\) for arbitrary sets \(\Omega\) and \(\Omega^{\prime}\). 

For \(A,B\in\Omega\) we define \(A\triangle B\equiv AB^{c}\cup A^{c}B\) and \(A\backslash B\equiv AB^{c}\). There is use for the notation\[\|X\|\equiv\sup_{\omega\in\Omega}|X(\omega)|, \tag{8}\]

and we will also reintroduce this _sup norm_ in other contexts below.

**Proposition 1.1** (Basics of inverse images).: Let \(X:\,\Omega\to\Omega^{\prime}\) and \(Y:\Omega^{\prime}\to\Omega^{\prime\prime}\). Let \(T\) denote an arbitrary index set. Then for all \(A,B,A_{t}\subset\Omega^{\prime}\) we have

\[X^{-1}(B^{c})=[X^{-1}(B)]^{c}, X^{-1}(A\backslash B)=X^{-1}(A)\backslash X^{-1}(B), \tag{10}\] \[X^{-1}\left(\bigcup_{t\in T}A_{t}\right)=\bigcup_{t\in T}X^{-1}(A_{t})\, X^{-1}\left(\bigcap_{t\in T}A_{t}\right)=\bigcap_{t\in T}X^{-1}(A_{t}). \tag{9}\]

For all sets \(A\subset\Omega^{\prime\prime}\), the composition \(Y\circ X\) satisfies

\[(Y\circ X)^{-1}(A)=X^{-1}(Y^{-1}(A))=X^{-1}\circ Y^{-1}(A). \tag{11}\]

Proof.: Trivial. 

**Proposition 1.2** (Preservation of \(\sigma\)-fields).: Let \(X:\Omega\to\Omega^{\prime}\). Then:

\[\mathcal{A}\equiv X^{-1}(\mbox{a $\sigma$-field $\mathcal{A}^{\prime}$ of subsets of $\Omega^{\prime}$})=(\mbox{a $\sigma$-field of subsets of $\Omega$}). \tag{13}\] \[X^{-1}(\sigma[\mathcal{C}^{\prime}])=\sigma[X^{-1}(\mathcal{C}^{ \prime})]\qquad\mbox{for any collection $\mathcal{C}^{\prime}$ of subsets of $\Omega^{\prime}$}.\] (14) \[\mathcal{A}^{\prime} \equiv\{A^{\prime}:X^{-1}(A^{\prime})\in(\mbox{a specific $\sigma$-field $\mathcal{A}$ of subsets of $\Omega$})\}\] \[=\mbox{(a $\sigma$-field of subsets of $\Omega^{\prime}$}). \tag{12}\]

Proof.: Now, (12) is trivial from proposition 1.1. Consider (14). Now:

(a) \(A^{\prime}\in\mathcal{A}^{\prime}\qquad\) implies \(X^{-1}(A^{\prime})\in\mathcal{A}\)

 implies \(X^{-1}(A^{\prime c})=[X^{-1}(A^{\prime})]^{c}\in\mathcal{A}\)

 implies \(A^{\prime c}\in\mathcal{A}^{\prime}\)

 implies \(X^{-1}(\bigcup_{n}A^{\prime}_{n})=\bigcup_{n}X^{-1}(A^{\prime}_{n})\in \mathcal{A}\)

 implies \(\bigcup_{n}A^{\prime}_{n}\in\mathcal{A}^{\prime}\).

This gives (14). Consider (13). Using (12) gives

(c) \(X^{-1}(\sigma[\mathcal{C}^{\prime}])=\)(a \(\sigma\)-field containing \(X^{-1}(\mathcal{C}^{\prime})\)) \(\supset\sigma[X^{-1}(\mathcal{C}^{\prime})]\).

Then (14) shows that

(d) \(\mathcal{A}^{\prime}\equiv\{A^{\prime}:X^{-1}(A^{\prime})\in\sigma[X^{-1}( \mathcal{C}^{\prime})]\}=\)(a \(\sigma\)-field containing \(\mathcal{C}^{\prime}\)) \(\supset\sigma[\mathcal{C}^{\prime}]\), so that (using first \(\sigma[\mathcal{C}^{\prime}]\subset\mathcal{A}^{\prime}\) from (d), and then the definition of \(\mathcal{A}^{\prime}\) in (d))

(e) \(X^{-1}(\sigma[\mathcal{C}^{\prime}])\subset X^{-1}(\mathcal{A}^{\prime})\subset \sigma[X^{-1}(\mathcal{C}^{\prime})]\).

Combining (c) and (e) gives (13). [We will apply (13) below to obtain (2.2.6).] 

Roughly, using (12) we will restrict \(X\) so that \(\mathcal{F}(X)\equiv X^{-1}(\vec{\mathcal{B}})\subset\mathcal{A}\) for our original \((\Omega,\mathcal{A},\mu)\), so that we can then "induce" a measure on \((\vec{R},\vec{\mathcal{B}})\). Or, (14) tells us that the collection \(\mathcal{A}^{\prime}\) is such that we can always induce a measure on \((\Omega^{\prime},\mathcal{A}^{\prime})\). We do this in the next section. First, we generalize our definition of Borel sets to \(n\) dimensions.

**Example 1.1** (Euclidean space): Let

\[R_{n}\equiv R\times\cdots\times R\equiv\{(r_{1},\ldots,r_{n}):\ \mbox{ each }r_{i}\,\mbox{ is in }R\}.\]

Let \(U_{n}\) denote all open subsets of \(R_{n}\), in the usual Euclidean metric. Then

\[{\cal B}_{n}\equiv\sigma[U_{n}]\mbox{ is called the class of }Borel\mbox{ }sets\mbox{ of }R_{n}. \tag{15}\]

Following the usual notation, \(B_{1}\times\cdots\times B_{n}\equiv\{(b_{1},\ldots,b_{n})\ :\ b_{1}\in B_{1},\ldots,b_{n}\in B_{n}\}\). Now let

\[\prod\nolimits_{i=1}^{n}{\cal B}\equiv{\cal B}\times\cdots\times{\cal B} \equiv\sigma[\{B_{1}\times\cdots\times B_{n}:\ \mbox{all }B_{i}\mbox{ are in }{\cal B}\}]. \tag{16}\]

Now consider

\[\sigma[\{(_{-\infty},r_{1}]\times\cdots\times(_{-\infty},r_{n}]:\ \mbox{all }r_{i}\mbox{ are in }R\}]. \tag{17}\]

Note that the three \(\sigma\)-fields of (15), (16), and (17) are equal. Just observe that each of these three classes generates the generators of the other two classes, and apply exercise 1.1.1. (Surely, we can define a generalization of area \(\lambda_{2}\) on \((R_{2},{\cal B}_{2})\) by beginning with \(\lambda_{2}(B_{1}\times B_{2})=\lambda(B_{1})\times\lambda(B_{2})\) for all \(B_{1}\) and \(B_{2}\) in \({\cal B}\), and then extending to all sets in \({\cal B}_{2}\). We will do this in theorem 5.1.1, and we will call it Lebesgue measure on two-dimensional Euclidean space. This clearly extends to \(\lambda_{n}\) on \((R_{n},{\cal B}_{n})\).) \(\Box\)

## 2 Measurable Functions

We seek a large usable class of functions that is closed under passage to the limit. This is the fundamental property of the class of measurable functions. Propositions 2.2 and 2.3 below will show that the class of measurable functions is also closed under all of the standard mathematical operations. Thus, this class is sufficient for our needs.

**Definition 2.1** (Simple functions, etc.): Let the measure space \((\Omega,\mathcal{A},\mu)\) be given and fixed throughout our discussion. Consider the following classes of functions. The _indicator function_\(1_{A}(\cdot)\) of the set \(A\subset\Omega\) is defined by

\[1_{A}(\omega)\equiv\cases{1&if $\omega\in A$,\cr 0&else.\cr} \tag{1}\]

A _simple function_ is of the form

\[X(\omega)\equiv\sum_{i=1}^{n}x_{i}1_{A_{i}}(\omega)\quad\mbox{for }\sum_{1}^{n}A_{i}=\Omega\quad\mbox{with all}\quad A_{i}\in\mathcal{A},\quad \mbox{and}\quad x_{i}\in R. \tag{2}\]

An _elementary function_ is of the form

\[X(\omega)\equiv\sum_{i=1}^{\infty}x_{i}1_{A_{i}}(\omega)\quad\mbox{for }\sum_{i=1}^{\infty}A_{i}=\Omega\mbox{ with all}\quad A_{i}\in\mathcal{A},\mbox{ and }x_{i}\in\bar{R}. \tag{3}\]

**Definition 2.2** (**Measurability**): Suppose that \(X:\Omega\to\Omega^{\prime}\), where \((\Omega,\mathcal{A})\) and \((\Omega^{\prime},\mathcal{A}^{\prime})\) are both measurable spaces. We then say that \(X\) is \(\mathcal{A}^{\prime}\)-\(\mathcal{A}\)-_measurable_ if \(X^{-1}(\mathcal{A}^{\prime})\subset\mathcal{A}\). We also denote this by writing either

\[X:(\Omega,\mathcal{A})\to(\Omega^{\prime},\mathcal{A}^{\prime})\quad\mbox{or} \quad X:(\Omega,\mathcal{A},\mu)\to(\Omega^{\prime},\mathcal{A}^{\prime}) \tag{4}\]

(or even \(X\ :\ (\Omega,\mathcal{A},\mu)\to(\Omega^{\prime},\mathcal{A}^{\prime},\mu^{ \prime})\) for the measure \(\mu^{\prime}\) "induced" on \((\Omega^{\prime},\mathcal{A}^{\prime})\) by the mapping \(X\), as will soon be defined). In the special case \(X:(\Omega,\mathcal{A})\to(\bar{R},\bar{\mathcal{B}})\), we simply call \(X\)_measurable_; and in this special case we let \(\mathcal{F}(X)\equiv X^{-1}(\bar{\mathcal{B}})\) denote the _sub \(\sigma\)-field_ of \(\mathcal{A}\)_generated by \(X\)_.

**Proposition 2.1** (Measurability criteria): Let \(X:\Omega\to\bar{R}\). Suppose \(\sigma[\mathcal{C}]=\bar{\mathcal{B}}\).

Then measurability can be characterized by either of the following:

\[X\mbox{ is measurable}\quad\mbox{if and only if}\quad X^{-1}(\mathcal{C})\subset \mathcal{A}. \tag{5}\]

\[X\mbox{ is measurable}\quad\mbox{if and only if}\quad X^{-1}([-\infty,x])\in \mathcal{A}\mbox{ for all }x\in\bar{R}. \tag{6}\]

Note that we could replace \([-\infty,x]\) by any one of \([-\infty,x)\), \([x,+\infty]\), or \((x,+\infty]\).

**Proof.** : Consider (5). Let \(X^{-1}(\mathcal{C})\subset\mathcal{A}\). Then

(a) \[X^{-1}(\bar{\mathcal{B}})=X^{-1}(\sigma[\mathcal{C}])=\sigma[X^{-1}( \mathcal{C})]\qquad\mbox{by proposition \ref{prop:1}}\] (b) \[\subset\mathcal{A}\quad\mbox{ since }X^{-1}(\mathcal{C})\subset \mathcal{A},\mbox{ and }\mathcal{A}\mbox{ is a }\sigma\mbox{-field}.\]

The other direction is trivial. Thus (5) holds. To demonstrate (6), we need to show that \(\mathcal{B}\) satisfies(c) \(\sigma[\{[-\infty,x]:x\in R\}]=\bar{\mathcal{B}}\equiv\sigma[\mathcal{B},\{-\infty \},\{+\infty\}]\).

Since \(\mathcal{B}=\sigma[\mathcal{C}_{I}]\) for \(\mathcal{C}_{I}\) as in (1.3.2) and since

(d) \((a,\ b]=[-\infty,b]\cap[-\infty,a]^{c},\quad[-\infty,b)=\bigcup_{1}^{\infty}[- \infty,b-1/n]\),

(e) \(\{-\infty\}=\cap_{n}[-\infty,-n],\quad\{+\infty\}=\bigcap_{n}[-\infty,n]^{c}\), etc.,

the equality (c) is obvious. The rest is trivial. \(\square\)

**Proposition 2.2** (Measurability of common functions): Let \(X,Y\), and \(X_{n}\)'s be measurable functions. Consider \(cX\) with \(c>0,-X\), \(\inf X_{n}\), \(\sup X_{n}\), \(\liminf X_{n}\), \(\limsup X_{n}\), \(\lim X_{n}\) if it exists, \(X^{2},X\pm Y\) if it is well-defined, \(XY\) where \(0\cdot\infty\equiv 0,X/Y\) if it is well-defined, \(X^{+},X^{-},|X|\), and the composite \(g(X)\) for a continuous \(g\) and for any measurable function \(g\). All of these are measurable functions.

**Proposition 2.3** (Measurability via simple functions):

(7) Simple and elementary functions are measurable.

(8) \(X:\Omega\to\bar{R}\) is measurable  if and only if

\(X\) is the limit of a sequence of simple functions.

Moreover:

(9) If \(X\geq 0\) is measurable, then \(X\) is

the limit of a sequence of simple functions that are \(\geq 0\) and \(\nearrow.\)

The \(X_{n}\)'s and \(Z_{n}\)'s that are defined in (10) and (12) below are important.

**Proof.** The functions in proposition 2.2 are measurable, as is now shown.

(a) \([cX<x]=[X<x/c]\), \([-X<x]=[X>-x]\).

(b) \([\inf X_{n}<x]=\cup[X_{n}<x]\), \(\sup X_{n}=-\inf(-X_{n})\).

(c) \(\liminf X_{n}=\sup\limits_{n}(\inf\limits_{k\geq n}X_{k})\), \(\limsup X_{n}=-\liminf(-X_{n})\).

(d) \(\lim X_{n}=\liminf X_{n}\), provided that \(\lim X_{n}(\omega)\) exists for all \(\omega\).

(e) \([X^{2}<x]=[-\sqrt{x}<X<\sqrt{x}]=[X<\sqrt{x}]\cap[X\leq-\sqrt{x}]^{c}\).

Each of the sets where \(X\) or \(Y\) equals \(0,\infty\), or \(-\infty\) is measurable; use this below.

(f) \([X>Y]=\bigcup_{r}\{\ X>r>Y:r\) is rational}, so \([X>Y]\) is a measurable set.

So, \([X+Y>z]=[X>z-Y]\in\mathcal{A}\) since \(z-Y\) is trivially measurable.

(Here \([X=\infty]\cap[Y=-\infty]=\emptyset\) is implied, as \(X+Y\) is well defined. Etc., below.)(g) \(X-Y=X+(-Y)\quad\mbox{and}\quad\)\(XY=[(X+Y)^{2}-(X-Y)^{2}]/4.\)

(h) \(X/Y=X\times(1/Y),\)

since \([1/Y<x]=[Y>1/x]\) for \(x>0\) in case \(Y>0\), and for general \(Y\) one can write \(\frac{1}{Y}=\frac{1}{Y}1_{[Y>0]}-\frac{1}{Y}1_{[Y<0]}\) with the two indicator functions measurable.

(i) \(X^{+}=X\lor 0\) and \(X^{-}=(-X)\lor 0.\)

For \(g\) measurable, \((g\circ X)^{-1}(\bar{\mathcal{B}})=X^{-1}(g^{-1}(\bar{\mathcal{B}}))\subset X^ {-1}(\bar{\mathcal{B}})\subset\mathcal{A}\). Then continuous \(g\) are measurable, since

(j) \(g^{-1}(\mathcal{B})=g^{-1}(\sigma[\mbox{open sets}])=\sigma[g^{-1}(\mbox{ open sets})]\subset\sigma[\mbox{open sets}]\subset\bar{\mathcal{B}},\) and both \(g^{-1}(\{+\infty\})\) and \(g^{-1}(\{-\infty\})\) are a (possibly void) subset of \(\{-\infty,+\infty\}.\) Now apply the result for measurable \(g\).

We now prove proposition 2.3. Claim (7) is trivial. Consider (8). Define simple functions \(X_{n}\) by

\[X_{n} \equiv \sum_{k=1}^{n2^{n}}\frac{k-1}{2^{n}}\times\Big{\{}1_{[\frac{k-1}{ 2^{n}}\leq X<\frac{k}{2^{n}}]}-1_{[\frac{k-1}{2^{n}}\leq-X<\frac{k}{2^{n}}]} \Big{\}}\] \[+\ n\times\{1_{[X\geq n]}-1_{[-X\geq n]}\}. \tag{10}\]

Since \(|X_{n}(\omega)-X(\omega)|\leq 2^{-n}\) for \(|X(\omega)|<n\), we have

(k) \(X_{n}(\omega)\to X(\omega)\)  as \(n\to\infty\)  for each \(\omega\in\Omega\).

Also, the nested subdivisions \(k/2^{n}\) cause \(X_{n}\) to satisfy

(l) \(X_{n}\nearrow\) when \(X\geq 0\).

We extend proposition 2.3 slightly by further observing that

(11) \(\|X_{n}-X\|\to 0\)  as \(n\to\infty\),  if \(X\) is bounded.

Also, the elementary functions

\[Z_{n}\equiv \sum_{k=1}^{\infty}\frac{k-1}{2^{n}}\times\Big{\{}1_{[\frac{k-1}{ 2^{n}}\leq X<\frac{k}{2^{n}}]}-1_{[\frac{k-1}{2^{n}}\leq-X<\frac{k}{2^{n}}]} \Big{\}}\] \[+\ \infty\times\{1_{[X=\infty]}-1_{[X=-\infty]}\} \tag{12}\]

are always such that

(13) \(\|(Z_{n}-X)\times 1_{[-\infty<X<\infty]}\|\leq 1/2^{n}\to 0\)  as \(n\to\infty\). 

**Proposition 2.4** (The discontinuity set is measurable; Billingsley)  If \((M,d)\) and \((M^{\prime},d^{\prime})\) are metric spaces and \(\psi:M\to M^{\prime}\) is any function (not necessarily a measurable function), then the _discontinuity set_ of \(\psi\) defined by \[D_{\psi}\equiv\{x\in M:\psi\mbox{ is not continuous at }x\} \tag{14}\]

is necessarily in the Borel \(\sigma\)-field \(B_{d}\) (that is, the \(\sigma\)-field generated by the \(d\)-open subsets of \(M\)).

**Proof.** Let

\[A_{\epsilon,\delta}\equiv\{x\in M:d(x,y)<\delta,d(x,z)<\delta\mbox{ and }\] (a) \[d^{\prime}(\psi(y),\psi(z))\geq\epsilon\mbox{ for distinct }y,z\in M\}.\]

Note that \(A_{\epsilon,\delta}\) is an open set, since \(\{u\in M:d(x,u)<\delta_{0}\}\subset A_{\epsilon,\delta}\) will necessarily occur if \(\delta_{0}\equiv\{\delta-[d(x,y)\lor d(x,z)]\}/2\); that is, the \(y\) and \(z\) that work for \(x\) also work for all \(u\) in \(M\) that are sufficiently close to \(x\). (Note: The \(y\) that worked for \(x\) may have been \(x\) itself.) Then (b) \(D_{\psi}=\bigcup_{i=1}^{\infty}\bigcap_{j=1}^{\infty}A_{\epsilon_{i},\delta_{ j}}\in{\cal B}_{d}\),

where \(\epsilon_{1},\epsilon_{2},\ldots\) and \(\delta_{1},\delta_{2},\ldots\) both denote the positive rationals, since each \(A_{\epsilon,\delta}\) is an open set. \(\Box\)

**Induced Measures**

**Example 2.1** (**Induced measures**) We now turn to the "induced measure" previewed above. Suppose \(X:(\Omega,{\cal A},\mu)\rightarrow(\Omega^{\prime},{\cal A}^{\prime})\), so that \(X\) is \({\cal A}^{\prime}\)-\({\cal A}\)-measurable. We define \(\mu_{X}\equiv\mu^{\prime}\) by

\[\mu_{X}(A^{\prime})\equiv\mu^{\prime}(A^{\prime})\equiv\mu(X^{-1}(A^{\prime})) \qquad\mbox{for each }A^{\prime}\in{\cal A}^{\prime}. \tag{15}\]

Then \(\mu_{X}\equiv\mu^{\prime}\) is a measure on \((\Omega^{\prime},{\cal A}^{\prime})\) called the _induced measure_. This is true, since we verify that

(a) \(\mu^{\prime}(\emptyset)=\mu(X^{-1}(\emptyset))=\mu(\emptyset)=0\), and

(b) \(=\sum_{1}^{\infty}\mu(X^{-1}(A^{\prime}_{n}))=\sum_{1}^{\infty}\mu^{\prime}(A^ {\prime}_{n})\).

Note also that

(c) \(\mu^{\prime}(\Omega^{\prime})=\mu(X^{-1}(\Omega^{\prime}))=\mu(\Omega)\).

Thus if \(\mu\) is a probability measure, then so is \(\mu_{X}\equiv\mu^{\prime}\). Note also that we could regard \(X\) as an \({\cal A}^{\prime}\)-\({\cal F}(X)\)-measurable transformation from the measure space \((\Omega,{\cal F}(X),\mu)\) to \((\Omega^{\prime},{\cal A}^{\prime},\mu_{X})\).

Suppose further that \(F\) is a generalized df on the real line \(R\), and that \(\mu_{F}(\cdot)\) is the associated measure on \((R,{\cal B})\) satisfying \(\mu_{F}((a,b])=F(b)-F(a)\) for all \(a\) and \(b\) (as was guaranteed by the correspondence theorem (theorem 1.3.1)). Thus \((R,B,\mu_{F})\) is a measure space. Define

\[X(\omega)=\omega\qquad\mbox{for all }\omega\in R. \tag{16}\]

Then \(X\) is a measurable transformation from \((R,{\cal B},\mu_{F})\) to \((R,{\cal B})\) whose induced measure \(\mu_{X}\) is equal to \(\mu_{F}\). Thus for any given df \(F\) we can always construct a measurable function \(X\) whose df is \(F\)

**Exercise 2.1**  Suppose \((\Omega,{\cal A})=(R_{2},{\cal B}_{2})\), where \({\cal B}_{2}\) denotes the \(\sigma\)-field generated by all open subsets of the plane. Recall that this \(\sigma\)-field contains all sets \(B\times R\) and \(R\times B\) for all \(B\in{\cal B}\); here \(B_{1}\times B_{2}\equiv\{(r_{1},r_{2}):r_{1}\in B_{1},r_{2}\in B_{2}\}\). Now define measurable transformations \(X_{1}((r_{1},r_{2}))=r_{1}\) and \(X_{2}(r_{1},r_{2}))=r_{2}\). Then define \(Z_{1}\equiv(X_{1}^{2}+X_{2}^{2})^{1/2}\) and \(Z_{2}\equiv{\rm sign}(X_{1}-X_{2})\), where \({\rm sign}(r)\) equals \(1\), \(0,-1\) according as \(r\) is \(>0,=0,<0\). The exercise is to give _geometric descriptions_ of the \(\sigma\)-fields \({\cal F}(Z_{1}),{\cal F}(Z_{2})\), and \({\cal F}(Z_{1},Z_{2})\).

**Proposition 2.5** (The form of an \({\cal F}({\bf Z})\)-measurable function): Suppose that \(Z\) is a measurable function on \((\Omega,{\cal A})\) and that \(Y\) is \({\cal F}(Z)\)-measurable. Then there must exist a measurable function \(g\) on \((\bar{R},\bar{\cal B})\) such that \(Y=g(Z)\).

**Proof.** (The approach of this proof is to consider indicator functions, simple functions, nonnegative functions, general functions. This approach will be used again and again. Learn it!) Suppose that \(Y=1_{D}\) for some set \(D\in{\cal F}(Z)\), so that \(Y\) is an indicator function that is \({\cal F}(Z)\)-measurable. Then we can rewrite \(Y\) as \(Y=1_{D}=1_{Z^{-1}(B)}=1_{B}(Z)\equiv g(Z)\), for some \(B\in\bar{\cal B}\) that depends on \(D\), where \(g(r)\equiv 1_{B}(r)\). Thus the proposition holds for indicator functions. It holds for simple functions, since when all \(B_{i}\in\bar{\cal B}\),

\[Y=\sum_{1}^{m}c_{i}1_{D_{i}}=\sum_{1}^{m}c_{i}1_{Z^{-1}(B_{i})}=\sum_{1}^{m}c_{ i}1_{B_{i}}(Z)\equiv g(Z).\]

Let \(Y\geq 0\) be \({\cal F}(Z)\)-measurable. Then there do exist \(\nearrow\)simple \({\cal F}(Z)\)-measurable functions \(Y_{n}\) such that \(Y\equiv\lim_{n}Y_{n}=\lim_{n}g_{n}(Z)\) for the \(\nearrow\)simple \(\bar{\cal B}\)-measurable functions \(g_{n}\). Now let \(g=\lim g_{n}\), which is \(\bar{B}\)-measurable, and note that \(Y=g(Z)\). For general \(Y=Y^{+}-Y^{-}\), use \(g=g^{+}-g^{-}\). \(\square\)

**Exercise 2.2** (Measurability criterion): Let \({\cal C}\) denote a \(\bar{\pi}\)-system of subsets of \(\Omega.\) Let \({\cal V}\) denote a vector space of functions; that is, \(X+Y\in{\cal V}\) and \(\alpha X\in{\cal V}\) for all \(X,Y\in{\cal V}\) and all \(\alpha\in R\)--and, all the usual elementary facts hold.

(a) Suppose that:

\[1_{C}\in{\cal V}\quad\mbox{for all $C\in{\cal C}$}. \tag{17}\]

(18) If \(A_{n}\nearrow A\quad\mbox{with $1_{A_{n}}\in{\cal V}$},\quad\mbox{then $1_{A}\in{\cal V}$}.\)

Show that \(1_{A}\in{\cal V}\) for every \(A\in\sigma[{\cal C}].\)

(b) It then follows trivially that every simple function

\[X_{n}\equiv\sum_{1}^{m}\alpha_{i}1_{A_{i}}\quad\mbox{is in ${\cal V}$}; \tag{19}\]

here \(m\geq 1\), all \(\alpha_{i}\in R\), and \(\sum_{1}^{m}A_{i}=\Omega\) with all \(A_{i}\in\sigma[{\cal C}]\).

(c) Now suppose further that \(X_{n}\nearrow X\) for \(X_{n}\)'s as in (19) implies that \(X\in{\cal V}.\) Show that \({\cal V}\) contains all \(\sigma[{\cal C}]\)-measurable functions.

## 3 Convergence

**Convergence Almost Everywhere**

**Definition 3.1** (\(\rightarrow_{a.e.}\)): Let \(X_{1},X_{2},\ldots\) denote measurable functions on \((\Omega,{\cal A},\mu)\) to \((\bar{R},\bar{\cal B})\). Say that the sequence \(X_{n}\)_converges almost everywhere_ to \(X\) (denoted by \(X_{n}\rightarrow_{a.e.}\)\(X\) as \(n\rightarrow\infty\)) if for some \(N\in{\cal A}\) for which \(\mu(N)=0\) we have \(X_{n}(\omega)\to X(\omega)\) as \(n\rightarrow\infty\) for all \(\omega\notin N\). If for all \(\omega\notin N\) the sequence \(X_{n}(\omega)\) is a Cauchy sequence, then we say that the sequence \(X_{n}\)_mutually converges_ _a.e._ and denote this by writing \(X_{n}-X_{m}\rightarrow_{a.e.}\)\(0\) as \(m\wedge n\rightarrow\infty\). (Here, \(m\wedge n\equiv\min(m,n)\).)

**Exercise 3.1**: Let \(X_{1},X_{2},\ldots\) be measurable functions from \((\Omega,{\cal A},\mu)\) to \((\bar{R},\bar{\cal B})\).

(a) If \(X_{n}\rightarrow_{a.e.}X\), then \(X=\tilde{X}\) a.e. for some measurable \(\tilde{X}\).

(b) If \(X_{n}\rightarrow_{a.e.}X\) and \(\mu\) is complete, then \(X\) itself is measurable.

**Proposition 3.1**: A sequence of measurable functions \(X_{n}\) that are a.e. finite converges a.e. to a measurable function \(X\) that is a.e. finite if and only if these functions \(X_{n}\) converges mutually a.e. (Thus we can redefine such functions on null sets and make them everywhere finite and everywhere convergent and/or follow the convention of corollary 2 to the Caretheodory theorem 1.2.1 and automatically complete every measure.)

**Proof.** The union of the countable number of null sets on which finiteness or convergence fails is again a null set \(N\). On \(N^{c}\), the claim is just a property of the real numbers. \(\Box\)

**Proposition 3.2** (The convergence and divergence sets are measurable): Consider the finite measurable functions \(X,X_{1},X_{2},\ldots\) (perhaps redefined on null sets to achieve this); thus, they are \({\cal B}\)-\({\cal A}\)-measurable. Then the convergence and mutual convergence sets are measurable. In fact, the _convergence set_ is given by

\[[X_{n}\to X]\equiv\bigcap_{k=1}^{\infty}\bigcup_{n=1}^{\infty}\bigcap_{m=n}^{ \infty}\left[|X_{m}-X|<\frac{1}{k}\right]\in{\cal A}, \tag{1}\]

and the _mutual convergence set_ is given by

\[[X_{n}-X_{m}\to 0]\equiv\bigcap_{k=1}^{\infty}\bigcup_{n=1}^{\infty} \bigcap_{m=n}^{\infty}\left[|X_{m}-X_{n}|<\frac{1}{k}\right]\in{\cal A}. \tag{2}\]

**Proof.** Just read the right-hand side of (1) as, for all \(\epsilon\equiv 1/k>0\) there exists an \(n\) such that for all \(m\geq n\) we have \(|X_{m}(\omega)-X(\omega)|<1/k\). (Practice saying this until it makes sense.) \(\Box\)

Taking complements in (1) allows the _divergence set_ to be expressed via

\[[X_{n}\to X]^{c}=\bigcup_{k=1}^{\infty}\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{ \infty}\left[|X_{m}-X|\geq\frac{1}{k}\right]\equiv\bigcup_{k=1}^{\infty}A_{k} \quad\mbox{with}\quad A_{k}\nearrow\mbox{in }k, \tag{3}\]

where

\[A_{k}=\bigcap_{n=1}^{\infty}D_{kn},\quad\mbox{and the}\quad D_{kn}\equiv\bigcup _{m=n}^{\infty}[|X_{m}-X|\geq 1/k]\quad\mbox{are }\searrow\mbox{ in }n. \tag{4}\]

**Proposition 3.3**: Consider finite measurable \(X_{n}\)'s and a finite measurable \(X\) on any \((\Omega,{\cal A},\mu)\). (i) We have

\[X_{n}\to_{a.e.}\mbox{ (such an $X$)}\qquad\qquad\mbox{iff}\qquad X_{n}-X_{m} \to_{a.e.}0\qquad\qquad\mbox{iff} \tag{5}\] \[\mu(\bigcap_{n=1}^{\infty}\bigcup\nolimits_{m=n}^{\infty}[|X_{m}- X_{n}|>\epsilon])=0,\quad\mbox{for all $\epsilon>0$}.\]

(A _finite_ limit \(X(\omega)\) exists if and only if the Cauchy criterion holds; and we want to be able to check for the existence of a finite limit \(X(\omega)\) without knowing its value.)

(ii)**(Most useful criterion for \(\to_{\bf a.e.}\))** On any \((\Omega,{\cal A},\mu)\), we have

\[X_{n}\to a.e.\mbox{ (some finite measurable $X$)}\qquad\

**Proposition 3.4** (a) If \(X_{n}\rightarrow_{\mu}X\) and \(X_{n}\rightarrow_{\mu}\tilde{X}\), then \(X=\tilde{X}\) a.e.

(b) On a complete measure space, \(X=\tilde{X}\) on \(N^{c}\), for a null set \(N\).

**Proof.** For all \(\epsilon>0\)

(a) \(\mu([|X-\tilde{X}|\geq 2\epsilon])\leq\mu([|X_{n}-X|\geq\epsilon])+\mu([|X_{n}- \tilde{X}|\geq\epsilon])\to 0\),

giving \(\mu([|X-\tilde{X}|\geq\epsilon])=0\) for all \(\epsilon>0\). Thus

(b) \(\mu([X\neq\tilde{X}])=\mu(\bigcup_{k}[|X-\tilde{X}|\geq 1/k])\leq\sum_{1}^{ \infty}\mu(|X-\tilde{X}|\geq 1/k)=\sum_{1}^{\infty}0\),

as claimed. \(\Box\)

**Exercise 3.2** (a) Show that in general \(\rightarrow_{\mu}\) does not imply \(\rightarrow_{a.e.}\).

(b) Give an example with \(\mu(\Omega)=\infty\) where \(\rightarrow_{a.e.}\) does not imply \(\rightarrow_{\mu}\).

**Theorem 3.1** (Relating \(\rightarrow_{\mu}\) to \(\rightarrow_{a.e.}\)): Let \(X\) and \(X_{1},X_{2},\ldots\) be measurable and finite a.e. functions. The following are true.

(11) \(X_{n}\rightarrow_{a.e.}\) (such an \(X\)) if and only if \(X_{n}-X_{m}\rightarrow_{a.e.}0\).

(12) \(X_{n}\rightarrow_{\mu}\) (such an \(X\)) if and only if \(X_{n}-X_{m}\rightarrow_{\mu}0\).

(13) Let \(\mu(\Omega)<\infty\). Then \(X_{n}\rightarrow_{a.e.}\) (such an \(X\)) implies \(X_{n}\rightarrow_{\mu}X\).

(14) (Riesz) If \(X_{n}\rightarrow_{\mu}X\), then for some \(n_{k}\) we have \(X_{n_{k}}\rightarrow_{a.e.}X\). (See (16)).

**(Reducing \(\rightarrow_{\mu}\) to \(\to a.e.\) by going to subsequences)** Suppose \(\mu(\Omega)<\infty\). Then

(15) \(X_{n}\rightarrow_{\mu}X\) if and only if

each subsequence \(n^{\prime}\) has a further \(n^{\prime\prime}\) on which \(X_{n^{\prime\prime}}\rightarrow_{a.e.}\) (such an \(X\)).

**Proof.** Now, (11) is proposition 3.1, and (12) is exercise 3.3 below. Result (13) comes from the elementary observation that

(a) \(\mu([|X_{n}-X|\geq\epsilon])\leq\mu(\bigcup_{m=n}^{\infty}[|X_{m}-X|\geq \epsilon])\to 0\), by (6).

To prove (14), choose \(n_{k}\uparrow\) such that

(b) \(\mu(A_{k})\equiv\mu([|X_{n_{k}}-X|>1/2^{k}])<1/2^{k}\),

with \(\mu([|X_{n}-X|>1/2^{k}])<1/2^{k}\) for all \(n\geq n_{k}\). Now let

(c) \(B_{m}\equiv\bigcup_{k=m}^{\infty}A_{k}\), so that \(\mu(B_{m})\leq\sum_{k=m}^{\infty}2^{-k}\leq 1/2^{m-1}\)

On \(B_{m}^{c}=\bigcap_{m}^{\infty}A_{k}^{c}\) we have \(|X_{n_{k}}-X|\leq 1/2^{k}\) for all \(k\geq m\), so that

(d) \(|X_{n_{k}}(\omega)-X(\omega)|\leq 1/2^{k}\to 0\) as \(k\rightarrow\infty\), for each \(\omega\in B_{m}^{c}\),

with \(\mu(B_{m})\leq 1/2^{m-1}\). Since convergence occurs on each \(B_{m}^{c}\), we have(e) \(X_{n_{k}}(\omega)\to X(\omega)\) as \(k\to\infty\) for each \(\omega\in C\equiv\bigcup_{m=1}^{\infty}B_{m}^{c}\),

where \(B_{m}=\bigcup_{k=m}^{\infty}A_{k}\) is \(\searrow\) with \((\bigcap_{m=1}^{\infty}B_{m})\subset\) (every \(B_{m}\)). So

(f) \(\mu(C^{c})=\mu(\bigcap_{m=1}^{\infty}B_{m})\leq\limsup\mu(B_{m})\leq\lim 1/2^{m-1}=0\),

completing the proof of (14).

(Comment on exercise 3.3: When \(X_{n}\to_{\mu}X\), analogy with (a) gives

(16) \(\mu(\{|X_{m}-X_{n}|\geq 1/2^{k}\})\leq 1/2^{k}\) for all \(m,n\geq(\)some \(n_{k})\).

Thus \(A_{k}\equiv\{\ |X_{n_{k}}-X_{n_{k+1}}|\geq 1/2^{k}\}\) has \(P(A_{k})\leq 1/2^{k}\) for all \(k\). In analogy with the first paragraph, prove the a.s. convergence of the \(X_{n_{k}}\) to some \(X\) on this subsequence by considering

(17) \(|X_{n_{k}}-X_{n_{\ell}}|\leq|(X_{n_{k}}-X_{n_{k+1}})+\cdots+(X_{n_{\ell-1}}-X_ {n_{\ell}})|\).

Then show that the whole sequence converges in measure to this \(X\).)

Consider the unproven half of (15). Suppose that every \(n^{\prime}\) contains a further \(n^{\prime\prime}\) as claimed (with a particular \(X\)). Assume that \(X_{n}\to_{\mu}X\) fails. Then for some \(\epsilon_{o}>0\) and some \(n^{\prime}\)

(g) \(\lim_{n^{\prime}}\mu([|X_{n^{\prime}}-X|>\epsilon_{o}])=(\)some \(a_{o})>0\).

But we are given that some further subsequence \(n^{\prime\prime}\) has \(X_{n^{\prime\prime}}\to_{a.e.}X\), and thus \(X_{n^{\prime\prime}}\to_{\mu}X\) by (13), using \(\mu(\Omega)<\infty\). Thus

(h) \(\lim_{n^{\prime\prime}}\mu([|X_{n^{\prime\prime}}-X|>\epsilon_{o}])=0\);

but this is a contradiction of (g). \(\square\)

**Exercise 3.3** As in (12), show that \(X_{n}\to_{\mu}X\) if and only if \(X_{m}-X_{n}\to_{\mu}0.\) (Hint. Adapt the proof of (16).)

**Exercise 3.4** (a) Suppose that \(\mu(\Omega)<\infty\) and \(g\) is continuous a.e. \(\mu_{X}\) (that is, \(g\) is continuous except perhaps on a set of \(\mu_{X}\) measure \(0\)). Then \(X_{n}\to_{\mu}X\) implies that \(g(X_{n})\to_{\mu}g(X)\).

(b) Let \(g\) be uniformly continuous on the real line. Then \(X_{n}\to_{\mu}X\) implies that \(g(X_{n})\to_{\mu}g(X)\). (Here, \(\mu(\Omega)=\infty\) is allowed.)

**Exercise 3.5** (a) **(Dini)** Consider continuous transformations \(X_{n}\) from a compact space \(\Omega\) to \(R\) for which \(X_{n}(\omega)\nearrow X(\omega)\) for each \(\omega\in\Omega\), where \(X\) is continuous. Then \(X_{n}\) converges uniformly to \(X\) on \(\Omega\).(Likewise, if \(X_{n}(\omega)\searrow X(\omega)\) for all \(\omega\).)

(b) In general, a uniform limit of bounded and continuous functions \(X_{n}\) is also bounded and continuous.

## 4 Probability, RVs, and Convergence in Law

**Definition 4.1** (**Random variable and df**): (a) A _probability space_\((\Omega,\mathcal{A},P)\) is just a measure space for which \(P(\Omega)=1\). Now, \(X:(\Omega,\mathcal{A},P)\rightarrow(R,\mathcal{B})\) will be called a _random variable_ (to be abbreviated \(rv\)); thus it is a \(\mathcal{B}\)-\(\mathcal{A}\)-measurable function. If \(X:(\Omega,\mathcal{A},P)\rightarrow(\bar{R},\bar{\mathcal{B}}),\) then we will call \(X\) an _extended_\(rv\).

(b) The _distribution function_ (to be abbreviated \(df\)) of a rv is defined by

\[F_{X}(x)\equiv P(X\leq x)\qquad\mbox{for all }-\infty<x<\infty. \tag{1}\]

We recall that \(F\equiv F_{X}\) satisfies

\[F\mbox{ is }\nearrow\mbox{ and right continuous, with }F(-\infty)=0\mbox{ and }F(+\infty)=1. \tag{2}\]

We let \(C_{F}\) denote the _continuity set_ of \(F\) that contains all points at which \(F\) is continuous. (That \(F\nearrow\) is trivial, and the other three properties all follow from the monotone property of measure, since \((\infty,x]=\bigcap_{n=1}^{\infty}(-\infty,x+a_{n}]\) for every possible sequence \(a_{n}\searrow 0,\bigcap_{n=1}^{\infty}(-\infty,-n]=\emptyset,\mbox{ and }\bigcup_{n=1}^{ \infty}(-\infty,n]=R.)\)

(c) If \(F\) is \(\nearrow\) and right continuous with \(F(-\infty)\geq 0\) and \(F(+\infty)\leq 1,\) then \(F\) will be called a _sub_\(df.\)

(d) The induced measure on \((R,\mathcal{B})\) (or \((\bar{R},\bar{\mathcal{B}})\)) will be denoted by \(P_{X}.\) It satisfies

\[P_{X}(B)=P(X^{-1}(B))=P(X\in B)\qquad\mbox{for all }B\in\mathcal{B} \tag{3}\]

(for all \(B\in\bar{\mathcal{B}}\) if \(X\) is an extended rv). We call this the _induced distribution_ of \(X.\) We use the notation \(X\cong F\) to denote that the induced distribution \(P_{X}(\cdot)\) of the rv \(X\) has df \(F.\)

(e) We say that rvs \(X_{n}\) (with dfs \(F_{n}\)) _converge in distribution_ or _converge in law_ to a rv \(X_{0}\) (with df \(F_{0}\)) if

\[F_{n}(x)=P(X_{n}\leq x)\to F_{0}(x)=P(X_{0}\leq x)\qquad\mbox{at each }x\in C_{F_{0}}. \tag{4}\]

We abbreviate this by writing either \(X_{n}\rightarrow_{d}X_{0},F_{n}\rightarrow_{d}F_{0},\) or \(\mathcal{L}(X_{n})\rightarrow\mathcal{L}(X_{0}).\)

**Notation 4.1**: Suppose now that \(\{X_{n}:n\geq 0\}\) are rvs on \((\Omega,\mathcal{A},P)\). Then it is customary to write \(X_{n}\rightarrow_{p}X_{0}\) (in place of \(X_{n}\rightarrow_{p}X_{0}\)) and \(X_{n}\rightarrow_{a.s.}X_{0}\) (as well as \(X_{n}\rightarrow_{a.e.}X_{0}\)). The "\(p\)" is an abbreviation for _in probability_, and the "a.s." is an abbreviation for _almost surely_.

Anticipating the next chapter, we let \(Eg(X)\) denote \(\int g(X)d\mu,\) or \(\int g(X)dP\) when \(\mu\) is a probability measure \(P\). We say that \(X_{n}\)_converges to \(X_{0}\) in \(rth\) mean_ if \(E|X_{n}-X_{0}|^{r}\to 0.\) We denote this by writing \(X_{n}\rightarrow_{r}X_{0}\) or \(X_{n}\rightarrow_{\mathcal{L}_{r}}X_{0}.\) \(\square\)

**Proposition 4.1**: Suppose that the rvs \(X\cong F\) and \(X_{n}\cong F_{n}\) satisfy \(X_{n}\rightarrow_{p}X.\) Then \(X_{n}\rightarrow_{d}X.\) (Thus, \(X_{n}\rightarrow_{a.s.}X\) implies that \(X_{n}\rightarrow_{d}X.\))

**Proof.** (This result has limited importance. But the technique introduced here is useful; see exercise 4.1 below.) Now,

(a) \[F_{n}(t)=P(X_{n}\leq t)\leq P(X\leq t+\epsilon)+P(|X_{n}-X|\geq\epsilon)\] (b) \[\leq F(t+\epsilon)+\epsilon\mbox{ for all }n\geq\mbox{some }n_{\epsilon}.\]

Also,

\[F_{n}(t)=P(X_{n}\leq t)\geq P(X\leq t-\epsilon\mbox{ and }|X_{n}-X| \leq\epsilon)\equiv P(AB)\] \[\geq P(A)-P(B^{c})=F(t-\epsilon)-P(|X_{n}-X|>\epsilon)\] \[\geq F(t-\epsilon)-\epsilon\qquad\mbox{for }n\geq(\mbox{some }n_{ \epsilon}^{\prime}).\]Thus for \(n\geq(n_{\epsilon}\lor n_{\epsilon}^{\prime})\) we have

(c) \(F(t-\epsilon)-\epsilon\leq\underline{\lim}\,F_{n}(t)\leq\overline{\lim}\,F_{n}(t) \leq F(t+\epsilon)+\epsilon\).

If \(t\) is a continuity point of \(F\), then letting \(\epsilon\to 0\) in (c) gives \(F_{n}(t)\to F(t)\). Thus \(F_{n}\to_{d}F\). \(\Box\)

The following elementary result is extremely useful. Often, one knows that \(X_{n}\to_{d}X\), but what one is really interested in is a slight variant of \(X_{n}\), rather than \(X_{n}\) itself. The next result was designed for just such situations.

**Definition 4.2** (Type): Two rvs \(X\) and \(Y\) are of the same _type_ if \(Y\cong aX+b\).

**Theorem 4.1** (Slutsky): Suppose that \(X_{n}\to_{d}X\), while the rvs \(Y_{n}\to_{p}a\) and \(Z_{n}\to_{p}b\) as \(n\to\infty\) (here \(X_{n},Y_{n}\), and \(Z_{n}\) are defined on a common probability space, but \(X\) need not be). Then

(5) \(U_{n}\equiv Y_{n}\times X_{n}+Z_{n}\to_{d}aX+b\) as \(n\to\infty\).

**Exercise 4.1**: Prove Slutsky's theorem. (Hint. Recall the proof of proposition 4.1. Then write \(U_{n}=(Y_{n}-a)X_{n}+(Z_{n}-b)+aX_{n}+b\) where \(Y_{n}-a\to_{p}0\) and \(Z_{n}-b\to_{p}0.\) Note also that \(P(|X_{n}|>(\mbox{some large $M_{\epsilon}$}))<\epsilon\) for all \(n\geq(\mbox{some $n_{\epsilon}$}).)\)

**Exercise 4.2**: Let \(c\) be a constant. Show that \(X_{n}\to_{d}c\) if and only if \(X_{n}\to_{p}c\).

**Remark 4.1**: Suppose \(X_{1},X_{2},\ldots\) are independent rvs with a common df \(F.\) Then \(X_{n}\to_{d}X_{0}\) for any rv \(X_{0}\) having df \(F\). However, there is no rv \(X\) for which \(X_{n}\) converges to \(X\) in the sense of \(\to_{a.s.},\to_{p}\), or \(\to_{r}\). (Of course, we are assuming that \(X\) is not a _degenerate_ rv (that is, that \(\mu_{F}\) is not a unit point mass).) \(\Box\)

## 5 Discussion of Sub \(\sigma\)-Fields \({}^{*}\)

Consider again a sequence of rvs \(X_{1},X_{2},\ldots\) where each quantity \(X_{n}\) is a measurable transformation \(X_{n}:(\Omega,{\cal A},P)\to(R,{\cal B},P_{X_{n}})\), and where \(P_{X_{n}}\) denotes the induced measure. Each rv \(X_{n}\) is \({\cal B}\)-\({\cal F}(X_{n})\)-measurable, with \({\cal F}(X_{n})\) a sub \(\sigma\)-field of \({\cal A}.\) Even though the intersection of any number of \(\sigma\)-fields is a \(\sigma\)-field, the union of even two \(\sigma\)-fields need not be a \(\sigma\)-field. We thus define the _sub \(\sigma\)-field generated by \(X_{1},\ldots,X_{n}\)_ as

\[{\cal F}(X_{1},\ldots,X_{n})\equiv\sigma[\bigcup_{k=1}^{n}{\cal F}(X_{k})]={ \sf X}^{-1}(B_{n})\qquad\mbox{for ${\sf X}_{n}\equiv(X_{1},\ldots,X_{n})^{ \prime}$}, \tag{1}\]

where the equality will be shown in the elementary proposition 5.2.1 below.

Note that \({\cal F}(X_{1},\ldots,X_{n})\subset{\cal F}(X_{1},\ldots,X_{n},X_{n+1})\), so that these necessarily form an increasing sequence of \(\sigma\)-fields of \({\cal A}\). Also, define

\[{\cal F}(X_{1},X_{2},\ldots)\equiv\sigma[\bigcup_{k=1}^{\infty}{\cal F}(X_{k})]. \tag{2}\]

It is natural to say that such \({\rm X}_{n}=(X_{1},\ldots,X_{n})^{\prime}\) are adapted to the \({\cal F}(X_{1},\ldots,X_{n})\). In fact, if \({\cal F}_{1}\subset{\cal F}_{2}\subset\cdots\) is any sequence of \(\sigma\)-fields for which \({\cal F}(X_{1},\ldots,X_{n})\subset{\cal F}_{n}\) for all \(n\), then we say that the \({\rm X}_{n}\)'s are _adapted_ to the \({\cal F}_{n}\)' s.

Think of \({\cal F}(X_{1},\ldots,X_{n})\) as the _amount of information_ available at time \(n\) from \(X_{1},\ldots,X_{n}\); that is, you have available for inspection all of the probabilities

\[P((X_{1},\ldots,X_{n})\in B_{n})=P((X_{1},\ldots,X_{n})^{-1}(B_{n}))=P_{(X_{1},\ldots,X_{n})}(B_{n}), \tag{3}\]

for all Borel sets \(B_{n}\in{\cal B}_{n}\). Rephrasing, you have available for inspection all of the probabilities

\[P(A),\qquad\mbox{for all $A$ }\in{\cal F}(X_{1},\ldots,X_{n}). \tag{4}\]

At stage \(n+1\) you have available \(P(A)\) for all \(A\in{\cal F}(X_{1},\ldots,X_{n},X_{n+1})\); that is, you have more information available. (Think of \({\cal F}_{n}\backslash{\cal F}(X_{1},\ldots,X_{n})\) as the amount of information available to you at time \(n\) that goes beyond the information available from \(X_{1},\ldots,X_{n}\); perhaps some of it comes from other rvs not yet mentioned, but it is available nonetheless.)

Suppose we are not given rvs, but rather (speaking informally now, based on your general feel for probability) we are given joint dfs \(F_{n}(x_{1},\ldots,x_{n})\) that we think ought to suffice to construct probability measures on \((R_{n},{\cal B}_{n})\). In (2.2.16) we saw that for \(n=1\) we could just let \((\Omega,{\cal A},\mu)=(R,{\cal B},\mu_{F})\) and use \(X(\omega)=\omega\) to define a rv that carried the information in the df \(F\). How do we define probability measures \(P_{n}\) on \((R_{n},{\cal B}_{n})\) so that the _coordinate_\(rvs\)

\[X_{k}(\omega_{1},\ldots,\omega_{n})=\omega_{k}\qquad\mbox{for all $(\omega_{1}, \ldots,\omega_{n})\in R_{n}$} \tag{5}\]

satisfy

\[P_{n}(X_{1}\leq x_{1},\ldots,X_{n}\leq x_{n})=F_{n}(x_{1},\ldots,x_{n})\qquad \mbox{for all $(x_{1},\ldots,x_{n})\in R_{n}$}, \tag{6}\]

and thus carry all the information in \(F_{n}\)? Chapter 5 will deal with this construction. But even now it is clear that for this to be possible, the \(F_{n}\)'s will have to satisfy some kind of consistency condition as we go from step \(n\) to \(n+1\). Moreover, the consistency problem should disappear if the resulting \(X_{n}\)'s are "independent."

But we need more. We will let \(R_{\infty}\) denote all infinite sequences \(\omega_{1},\omega_{2},\ldots\) for which each \(\omega_{i}\in R\). Now, the construction of (5) and (6) will determine probabilities on the collection \({\cal B}_{n}\times\prod_{k=n+1}^{\infty}R\) of all subsets of \(R_{\infty}\) of the form

\[\begin{array}{l}B_{n}\times\prod_{k=n+1}^{\infty}R\\ \equiv\{(\omega_{1},\ldots,\omega_{n},\omega_{n+1},\ldots):(\omega_{1},\ldots, \omega_{n})\in B_{n},\omega_{k}\in R\qquad\mbox{for $k\geq n+1$}\},\end{array} \tag{7}\]with \(B_{n}\in{\cal B}_{n}\). Each of these collections is a \(\sigma\)-field (which within this special probability space can be denoted by \({\cal F}(X_{1},\ldots,X_{n})\)) in this overall probability space \((R_{\infty},{\cal B}_{\infty},P_{\infty})\), for some appropriate \({\cal B}_{\infty}\). But what is an appropriate \(\sigma\)-field \(B_{\infty}\) for such a probability measure \(P_{\infty}\)? At a minimum, \({\cal B}_{\infty}\) must contain

\[\sigma\left[\bigcup\nolimits_{n=1}^{\infty}\Big{\{}B_{n}\times\coprod\nolimits_ {k=n+1}^{\infty}R\Big{\}}\right]=\sigma\left[\bigcup\nolimits_{n=1}^{\infty}{ \cal F}(X_{1},\ldots,X_{n})\right], \tag{8}\]

and indeed, this is what we will use for \({\cal B}_{\infty}\). Of course, we also want to construct the measure \(P_{\infty}\) on \((R_{\infty},{\cal B}_{\infty})\) in such a way that

\[P_{\infty}\left(\coprod\nolimits_{k=1}^{n}(-\infty,x_{k}]\times\coprod\nolimits _{k=n+1}^{\infty}R\right)=F_{n}(x_{1},\ldots,x_{n})\qquad\mbox{for all $n\geq 1$} \tag{9}\]

and for all \(x_{1},\ldots,x_{n}\) in \(R\). The details are given in chapter 5.

Until chapter 5 we will assume that we are _given_ the rvs \(X_{1},X_{2},\ldots\) on some \((\Omega,{\cal A},P)\), and we will need to deal only with the _known_ quantities \({\cal F}(X_{1},\ldots,X_{n})\) and \({\cal F}(X_{1},X_{2},\ldots)\) defined in (1) and (2). This is probability theory: Given \((\Omega,{\cal A},P)\), we study the behavior of rvs \(X_{1},X_{2},\ldots\) that are defined on this space. Now contrast this with statistics: Given a physical situation producing measurements \(X_{1},X_{2},\ldots\), we construct models \(\{(R_{\infty},{\cal B}_{\infty},P_{\infty}^{\theta})\;:\;\theta\in\Theta\}\) based on various plausible models for \(F_{n}^{\theta}(x_{1},\ldots,x_{n}),\;\theta\in\Theta\), and we then use the data \(X_{1},X_{2},\ldots\) and the laws of probability theory to decide which model \(\theta_{0}\in\Theta\) was most likely to have been correct and what action to take. In particular, the statistician must know that the models to be used are well-defined.

We also need to extend all this to uncountably many rvs \(\{X_{t}:t\in T\}\), for some interval \(T\) such as \([a,b]\), or \([a,\infty)\), or \([a,\infty]\), or \((-\infty,\infty),\ldots\). We say that rvs \(X_{t}:(\Omega,{\cal A},P)\to(R,{\cal B})\) for \(t\in T\) are _adapted_ to an \(\nearrow\)sequence of \(\sigma\)-fields \({\cal F}_{t}\) if \({\cal F}_{s}\subset{\cal F}_{t}\) for all \(s\leq t\) with both \(s,t\in T\) and if each \(X_{t}\) is \({\cal F}_{t}\)-measurable. In this situation we typically let \(R_{T}\equiv\prod_{t\in T}R_{t}\) and then let

\[{\cal F}_{t}\equiv{\cal F}(X_{s}:s\leq t)\equiv\sigma\left[\bigcup\nolimits_{ s}X_{s}^{-1}({\cal B}):s\leq t\mbox{ and }s\in T\right]\qquad\mbox{for all $t\in T$}. \tag{10}\]

This is also done in chapter 5 (where more general sets \(T\) are, in fact, considered).

The purpose in presenting this section here is to let the reader start now to become familiar and comfortable with these ideas before we meet them again in chapter 5 in a more substantial and rigorous presentation. (The author assigns this as reading at this point and presents only a very limited amount of chapter 5 in his lectures.)

**Exercise 5.1** (a) Show that the class \({\cal C}\equiv\{X_{1}^{-1}(B_{1})\cap\{X_{2}^{-1}(B_{2})\;:\;B_{1},B_{2}\in{ \cal B}\}\) is a \(\bar{\pi}\)-system that generates the \(\sigma\)-field \({\cal F}(X_{1},X_{2})\).

(b) Recall the Dynkin \(\pi\)-\(\lambda\) theorem, and state its implications in this context.

(c) State an extension of this part (a) to \({\cal F}(X_{1},\ldots,X_{n})\) and to \({\cal F}(X_{1},X_{2},\ldots)\).

## Chapter 3 Integration

### 1 The Lebesgue Integral

Let \((\Omega,\mathcal{A},\mu)\) be a fixed measure space and let \(X,Y,X_{n},\ldots\) denote measurable functions from \((\Omega,\mathcal{A},\mu)\) to \((\bar{R},\bar{\mathcal{B}})\). If \(\Omega=\sum_{i}^{n}A_{i}\) where \(A_{1},\ldots,A_{n}\) are in \(\mathcal{A}\), then \(A_{1},\ldots,A_{n}\) is called a _partition_ (or _measurable partition_) of \(\Omega\).

**Definition 1.1** (Lebesgue integral \(\int X\,d\mu\) or \(\int X\)): If \(X=\sum_{i=1}^{n}x_{i}1_{A_{i}}\geq 0\) is a simple function (where all \(x_{i}\geq 0\) and \(A_{1},\ldots,A_{n}\) is a partition of \(\Omega\)), then

\[\int X\,d\mu\equiv\sum_{i=1}^{n}x_{i}\mu(A_{i}). \tag{1}\]

(We must verify that this is well defined. That is, we must show that the value assigned to \(\int X\,d\mu\) in (1) is independent of the representation of \(X\) that is specified.) If \(X\geq 0\), then

\[\int X\,d\mu\equiv\sup\left\{\int Y\,d\mu:0\leq Y\leq X\text{ and }Y\text{ is such a simple function}\right\}. \tag{2}\]

For general measurable \(X\),

\[\int X\,d\mu\equiv\int X^{+}\,d\mu-\int X^{-}\,d\mu, \tag{3}\]

provided that at least one of \(\int X^{+}\,d\mu\) and \(\int X^{-}\,d\mu\) is finite. We let

\[\begin{array}{l}\mathcal{L}_{1}\,\equiv\mathcal{L}_{1}(\Omega,\mathcal{A}, \mu)\,\equiv\{X:\int|X|\,d\mu<\infty\},\\ \mathcal{L}_{1}^{+}\,\equiv\mathcal{L}_{1}^{+}(\Omega,\mathcal{A},\mu)\,\equiv \{X\in\mathcal{L}_{1}:X\geq 0\},\\ \mathcal{L}_{r}\,\equiv\mathcal{L}_{r}(\Omega,\mathcal{A},\mu)\,\,\equiv \mathcal{L}_{r}(\mu)\equiv\{X:\int|X|^{r}\,d\mu<\infty\},\quad\text{for each }r>0;\end{array} \tag{4}\]

in each of these definitions we agree to identify \(X\) and \(X^{\prime}\) whenever \(X=X^{\prime}\) a.e. \(\mu\). If \(X\) (which is not measurable) equals a measurable \(Y\) on a set \(A\) having \(\mu(A^{c})=0\), then \(\int X\,d\mu\equiv\int Y\,d\mu\). (Clearly, \(\int X\,d\mu\) is not affected by the choice of \(Y\) or \(A\).)

If \(X\) is measurable and \(\int X\,d\mu\) is finite, then \(X\) is called _integrable_. For any \(A\in\mathcal{A}\),

\[\int_{A}X\,d\mu\equiv\int X1_{A}\,d\mu. \tag{5}\]

We also use the notation (especially in proofs, to save space)

\[\int X\equiv\int X\,d\mu\equiv\ \text{(the }integral\text{ of }X)\ \equiv\text{E}X\equiv\ \text{(the }expectation\text{ of }X). \tag{6}\]For ordinary Lebesgue measure \(\mu\) on \(R\), we often write \(\int X\,d\mu=\int X(r)\,dr\).

It will now be demonstrated that the definition (1) makes sense and that \(\int X\,d\mu\) satisfies the following elementary properties.

**Proposition 1.1** (Elementary properties of the integral): It holds that definition 1.1 of the integral is unambiguous. Now suppose that the functions \(\ X\) and \(Y\) are measurable, that \(\int X\,d\mu\) and \(\int Y\,d\mu\) are well-defined, and that their sum (the number \(\int X\,d\mu+\int Y\,d\mu\)) is a well-defined number in \([-\infty,+\infty]\). Then

\[\int(X+Y)\,d\mu=\int X\,d\mu+\int Y\,d\mu\qquad\text{and}\qquad\int cX\,d\mu=c \int X\,d\mu, \tag{7}\] \[0\leq X\leq Y\qquad\text{ implies }\qquad 0\leq\int X\,d\mu\leq\int Y \,d\mu. \tag{8}\]

**Proof.** _Case 1_: Consider simple functions \(X\geq 0\) and \(Y\geq 0\).

Claim 1: Defining \(\int X\,d\mu=\sum_{1}^{m}x_{i}\mu(A_{i})\) for such simple functions \(X=\sum_{1}^{m}x_{i}1_{A_{i}}\) makes \(\int X\,d\mu\) well-defined for these simple functions.

Suppose that we also have \(X=\sum_{1}^{n}z_{j}1_{C_{j}}\). Then

\[\text{(a)}\qquad\sum\nolimits_{i=1}^{m}x_{i}\sideset{}{{}^{n}}{\sum}_{j=1}^{n} 1_{A_{i}C_{j}}=X=\sideset{}{{}^{n}}{\sum}_{j=1}^{n}z_{j}\sideset{}{{}^{n}}{ \sum}_{i=1}^{m}1_{A_{i}C_{j}},\]

so that \(x_{i}=z_{j}\) if \(A_{i}C_{j}\neq\emptyset\). Thus

\[\sideset{}{{}^{n}}{\sum}_{i=1}^{m}x_{i}\mu(A_{i})=\sideset{}{{}^{n} }{\sum}_{i=1}^{m}x_{i}\sideset{}{{}^{n}}{\sum}_{j=1}^{n}\mu(A_{i}C_{j})=\sideset {}{{}^{n}}{\sum}_{i=1}^{m}\sideset{}{{}^{n}}{\sum}_{j=1}^{n}x_{i}\mu(A_{i}C_{j})\] \[\quad=\sum\nolimits_{i=1}^{m}\sideset{}{{}^{n}}{\sum}_{j=1}^{n}z_{ j}\mu(A_{i}C_{j})\qquad\text{ since }x_{i}=z_{j}\quad\text{if }\mu(A_{i}C_{j})>0\] \[\quad=\sum\nolimits_{j=1}^{n}z_{j}\sideset{}{{}^{n}}{\sum}_{i=1}^{ m}\mu(A_{i}C_{j})=\sum\nolimits_{j=1}^{n}z_{j}\mu(C_{j});\]

and since the two extreme terms that represent the two different definitions of the quantity \(\int X\,d\mu\) are equal, we see that \(\int X\,d\mu\) is well-defined.

Claim 2: The integral behaves linearly for such simple functions.

Suppose \(X=\sum_{1}^{m}x_{i}1_{A_{i}}\) and \(Y=\sum_{1}^{n}y_{j}1_{B_{j}}\). Then \(X+Y=\sum_{1}^{m}\sum_{1}^{n}(x_{i}+y_{j})1_{A_{i}B_{j}}\). We thus have

\[\int(X+Y)\,d\mu=\sum_{1}^{m}\sum_{1}^{n}(x_{i}+y_{j})\mu(A_{i}B_{ j})\] \[\quad=\sideset{}{{}^{n}}{\sum}_{1}^{m}x_{i}\mu(A_{i}B_{j})+\sideset {}{{}^{n}}{\sum}_{1}^{m}\sum_{1}^{n}y_{j}\mu(A_{i}B_{j})\] \[\quad=\sideset{}{{}^{n}}{\sum}_{1}^{m}x_{i}\sideset{}{{}^{n}}{ \sum}_{1}^{n}\mu(A_{i}B_{j})+\sideset{}{{}^{n}}{\sum}_{1}^{n}y_{j}\sideset{}{{}^{n }}{\sum}_{1}^{m}\mu(A_{i}B_{j})\] \[\quad=\sideset{}{{}^{n}}{\sum}_{1}^{m}x_{i}\mu(A_{i})+\sideset{}{{ }^{n}}{\sum}_{1}^{n}y_{j}\mu(B_{j})=\int X\,d\mu+\int\!Y\,d\mu,\]

which establishes the additivity for simple functions.

Claim 3: For any measurable \(X\geq 0\), it is also trivial that \(\int cX=c\int X\) in (7). This holds since \(Y\leq X\) if and only if \(cY\leq cX\), with \(\int cY=c\int Y\) for simple \(Y\).

Claim 4: So too, the monotonicity in (8) is trivial for any measurable \(0\leq X\leq Y\).

The proof of linearity for general \(X\geq 0\) and \(Y\geq 0\) is included in the proof of the monotone convergence theorem (MCT) (that is, the first theorem of the next section). That is, we will prove the MCT using only Claims 1, 3, and 4. Then we will use the MCT and Claim 2 to obtain the linearity of the integral for any functions \(X\geq 0\) and \(Y\geq 0\).

_Case 2_: The final linearity step is then trivial. Just write \(X=X^{+}-X^{-}\) and \(Y=Y^{+}-Y^{-}\) and do algebra. \(\Box\)

**Notation 1.1** Let \(F\) denote a generalized df and let \(\mu_{F}\) denote the associated Lebesgue-Stieltjes measure. Suppose that \(g\) is an integrable function on \(R\). We will then freely use the notation

\[\int_{R}g(x)dF(x)\equiv\int_{R}g(x)\,d\mu_{F}(x). \tag{9}\]

## 3 Integration

**Theorem 2.1** (MCT, monotone convergence theorem): Suppose that \(X_{n}\nearrow X\) a.e. for measurable functions \(X_{n}\geq 0\) a.e. Then

\[0\leq\int X_{n}\,d\mu\nearrow\int X\,d\mu. \tag{1}\]

**Corollary 1** For \(X\geq 0\), the simple \(X_{n}\) in (2.2.10) satisfy \(\int X_{n}\,d\mu\nearrow\int X\,d\mu\).

**Proof.** By redefining on null sets if necessary, we may assume that \(X_{n}\nearrow X\) for all \(\omega\). Thus \(X\) is measurable, by proposition 2.2.2. Also, \(\int X_{n}\) is \(\nearrow\), and so \(a\equiv\lim\int X_{n}\) exists in \([0,\infty]\). Moreover, \(X_{n}\leq X\) implies \(\int X_{n}\leq\int X\); and so we conclude that \(a=\lim\int X_{n}\leq\int X\).

Let \(Y\equiv\sum_{1}^{m}c_{j}1_{D_{j}}\) be an arbitrary simple function satisfying \(0\leq Y\leq X\). Fix \(0<\theta<1\). Then note that \(A_{n}\equiv[X_{n}\geq\theta Y]\nearrow\Omega\) (since \(0\leq\theta Y\leq X\) on \([X=0]\) and \(0\leq\theta Y<X\) on \([X>0]\) are both trivial). Claims 3 and 4 of the proposition 3.1.1 proof give (for any simple function \(Y=\sum_{1}^{m}c_{j}1_{D_{j}}\) as above)

(a) \[\theta\int Y\times 1_{A_{n}}=\int\theta Y\times 1_{A_{n}}\leq\int X_{n} \times 1_{A_{n}}\leq\int X_{n}\leq a,\] while (b) \[\theta\int Y\times 1_{A_{n}}=\theta{\sum_{1}}_{1}^{m}c_{j}P(D_{j}A_{n}) \rightarrow\theta{\sum_{1}}_{1}^{m}c_{j}P(D_{j})=\theta\int Y\]

using \(P(D_{j}A_{n})\to P(D_{j})\) by proposition 1.1.2. Let \(\theta\to 1\) in (a) and (b) to obtain \(\int Y\leq a\). Since \(0\leq Y\leq X\) is arbitrary, this gives \(\int X\leq a=\lim\int X_{n}\). \(\square\)

**Proof.** We now return to the linearity of the integral for general measurable functions \(X\geq 0\) and \(Y\geq 0\). Let \(X_{n}\nearrow X\) and \(Y_{n}\nearrow Y\) for the measurable simple functions of (2.2.10). Then \(X_{n}+Y_{n}\nearrow X+Y\). Thus the MCT twice, the linearity of the integral for simple functions, and then the MCT again give the general linearity of the integral via

\[\int X+\int Y=\lim\int X_{n}+\lim\int Y_{n}=\lim(\int X_{n}+\int Y_{n})\]

(a) \[=\lim\int(X_{n}+Y_{n})\] by simple function linearity (b) \[=\int(X+Y)\] by the MCT.

In general, combine the integrals of \(X^{+},X^{-},Y^{+}\), and \(Y^{-}\) appropriately. \(\square\)

**Theorem 2.2** (Fatou's lemma): For \(X_{n}\)'s measurable,

\[\int\underline{\lim}\,X_{n}\,d\mu\leq\underline{\lim}\int X_{n}\,d\mu,\qquad \qquad\mbox{provided that $X_{n}\geq 0$ a.e. for all $n$}. \tag{2}\]

**Proof.** Redefine on null sets (if necessary) so that all \(X_{n}\geq 0\). Then

\[Y_{n}\equiv\inf_{k\geq n}X_{k}\nearrow\underline{\lim}\,X_{n},\qquad\mbox{or} \qquad\underline{\lim}\,X_{n}=\lim Y_{n}\mbox{ with $Y_{n}\nearrow,$} \tag{3}\]so that

(a) \[\int\underline{\lim}\,X_{n}=\int\lim Y_{n}=\lim\int Y_{n}\qquad\mbox{ by the MCT}\] (b) \[=\underline{\lim}\int Y_{n}\leq\underline{\lim}\int X_{n}\qquad\qquad \mbox{ since }Y_{n}\leq X_{n}.\qed\]

**Theorem 2.3** (DCT, dominated convergence theorem): Let \(|X_{n}|\leq Y\) a.e. for all \(n\), for some _dominating function_\(Y\in{\cal L}_{1}\); and suppose either (i) \(X_{n}\to_{a.e.}X\) or (ii) \(X_{n}\to_{\mu}X\). Then

\[\int|X_{n}-X|\,d\mu\to 0\quad\mbox{as}\;\;n\to\infty\qquad\mbox{(that is,}\quad X_{n}\to_{{\cal L}_{1}}X). \tag{4}\]

(If \((\sup_{n\geq 1}|X_{n}|)\) is integrable, then it is a suitable dominating function.)

**Corollary 1**: Note that (4) implies both

\[\int X_{n}\,d\mu\to\int X\,d\mu\qquad\qquad\qquad\mbox{(that is,}\quad\mbox{E}X_{n}\to\mbox{E}X)\qquad\qquad\mbox{ and} \tag{6}\] \[\sup_{A\in A}|\int_{A}X_{n}\,d\mu-\int_{A}X\,d\mu|\to 0. \tag{5}\]

**Proof.** (i) Suppose that \(X_{n}\to_{a.e.}X\). Then \(Z_{n}\equiv|X_{n}-X|\to_{a.e.}0\). (Here, \(0\leq Z_{n}\leq 2Y\) a.s., where both of the functions \(0\) and \(2Y\) are in \({\cal L}_{1}\).) Now apply Fatou's lemma to the rvs \(2Y-Z_{n}\geq 0\), and conclude that

(a) \[\int(2Y-0)=\int\underline{\lim}(2Y-Z_{n})\leq\underline{\lim}\int(2Y-Z_{n}) \qquad\mbox{by Fatou}\] \[=\underline{\lim}(\int 2Y-\int Z_{n})\]

\[=\int 2Y-\overline{\lim}\int Z_{n}. \tag{7}\]

Hence, \(\overline{\lim}\int Z_{n}\leq\int 0=0\) (as \(\int 2Y\) is finite). Combining the two results gives

(b) \[0\leq\underline{\lim}\int Z_{n}\leq\overline{\lim}\int Z_{n}\leq 0;\]

so \(\lim\int Z_{n}=0\), as claimed.

(ii) Suppose \(X_{n}\to_{\mu}X\). Let \(a\equiv\overline{\lim}\int Z_{n}\geq 0\). Let \(n^{\prime}\) be a subsequence such that \(\int Z_{n^{\prime}}\to a\). But \(Z_{n^{\prime}}\to_{\mu}0\), so theorem 2.3.1 gives a further subsequence \(n^{\prime\prime}\) such that \(Z_{n^{\prime\prime}}\to_{a.e.}0\), while we still have \(\int Z_{n^{\prime\prime}}\to a\). But \(\int Z_{n^{\prime\prime}}\to 0\) by case (i). Thus \(a=0\). Thus

(c) \[0\leq\overline{\lim}\int Z_{n}=a=0,\mbox{ or }\int Z_{n}\to 0.\]

(iii) Consider the corollary. We have

(d) \[|\int_{A}X_{n}-\int_{A}X|=|\int_{A}((X_{n}-X)^{+}-(X_{n}-X)^{-})|\leq\int|X_{n }-X|\to 0\]

uniformly in all \(A\in{\cal A}\).

**Theorem 2.4**: \(\int\sum_{1}^{\infty}X_{n}\,d\mu=\sum_{1}^{\infty}\int X_{n}\,d\mu\qquad\mbox{if $X_{n} \geq 0$ a.e., for all $n$}.\)__

**Proof.** Note that \(0\leq Z_{n}\equiv\sum_{1}^{n}X_{k}\nearrow Z\equiv\sum_{1}^{\infty}X_{k}\,\)a.e., and now apply the MCT to the \(Z_{n}\)'s. \(\Box\)

**Theorem 2.5** (Absolute continuity of the integral): Fix \(X\in{\cal L}_{1}\). Then

\[\int_{A}|X|\,d\mu\to 0\qquad\mbox{as}\qquad\mu(A)\to 0. \tag{8}\]

That is, \(\int_{A}|X|\,d\mu<\epsilon\), provided only that \(\mu(A)<\) (an appropriate \(\delta_{\epsilon}\)).

**Proof.** Now, \(\int|X|1_{[|X|\leq n]}\nearrow\int|X|\) by the MCT, so we may claim that

(a) \[\int|X|1_{[|X|>n]}\leq\epsilon/2\qquad\mbox{for $n\geq N\equiv\mbox{ (some $N_{\epsilon}$)}$}.\]

Thus

(b) \[\int_{A}|X|\leq\int_{A}|X|1_{[|X|\leq N]}+\int|X|1_{[|X|>N]}\leq N\times\mu(A)+ \epsilon/2\leq\epsilon,\]

provided that \(\mu(A)\leq\epsilon/(2N)\). \(\Box\)

**Exercise 2.1** (Only the zero function): Show that

\[X\geq 0\mbox{ and }\int X\,d\mu=0\qquad\mbox{implies}\qquad\mu([X>0])=0. \tag{9}\]

**Exercise 2.2** (Only the zero function): (a) Suppose \(\sigma[C]={\cal A}\), for a field \(C\). Show that

\[\int_{A}X\,d\mu=\cases{=0,\cr\geq 0\qquad\mbox{for all $A\in C$}\qquad\mbox{ implies}\qquad X=\cases{=0&a.e.,\cr\geq 0&a.e.}\cr} \tag{10}\]

On \((R,{\cal B},\mu_{F})\), for a generalized df \(F\), we only need (10) for all intervals \(A=(a,b]\).

**Exercise 2.3**: Consider a measure space \((\Omega,{\cal A},\mu)\). Let \(\mu_{0}\equiv\mu|{\cal A}_{0}\) for a sub \(\sigma\)-field \({\cal A}_{0}\) of \({\cal A}\). Starting with indicator functions, show that \(\int X\,d\mu=\int X\,d\mu_{0}\) for any \({\cal A}_{0}\)-measurable function \(X\). Hint: Consider four cases, as in the next proof.

**Definition 2.1** (Induced measure): Suppose that \(X:(\Omega,{\cal A},\mu)\to(\Omega^{\prime},{\cal A}^{\prime})\) is a measurable function. Recall from (2.2.15) that

\[\mu^{\prime}(A^{\prime})\equiv\mu_{X}(A^{\prime})=\mu(X^{-1}(A^{\prime}))\qquad \qquad\qquad\qquad\mbox{for all $A^{\prime}\in{\cal A}^{\prime}$}, \tag{11}\]

and \(\mu^{\prime}\) is a measure on \((\Omega^{\prime},{\cal A}^{\prime})\), called the _induced measure_ of \(X\).

**Theorem 2.6** (Theorem of the unconscious statistician): (i) The induced measure \(\mu_{X}(\cdot)\) of the measurable function \(X:\,(\Omega,{\cal A},\mu)\to(\Omega^{\prime},{\cal A}^{\prime},\mu_{X})\) determines the induced measure \(\mu_{g(X)}\) for all measurable functions \(g\): \(\Omega^{\prime},{\cal A}^{\prime})\to(\bar{R},\bar{\cal B})\).

(ii) (Change of variable) Then

\[\int_{X^{-1}(A^{\prime})}g(X(\omega))\,d\mu(\omega)=\int_{A^{\prime}}g(x)\,d\mu_ {X}(x)\qquad\qquad\mbox{for all $A^{\prime}\in{\cal A}^{\prime}$}, \tag{12}\]

in the sense that if either side exists then so does the other and they are equal. So,

\[\int_{X^{-1}(g^{-1}(B))}g(X(\omega))\,d\mu(\omega)=\int_{g^{-1}(B)}g(x)\,d\mu_{ X}(x)=\int_{B}y\,d\mu_{Y}(y)\quad\mbox{ for $B\in\bar{\cal B}$}. \tag{13}\]

**Proof.** (i) Now, \(Y\equiv g(X)\) is measurable. By (2.1.11) and (2.2.5) we see that

(a) \[\mu_{Y}(B)=\mu_{g(X)}(B)=\mu([g(X)\in B])=\mu(X^{-1}\circ g^{-1}(B))=\mu_{X}(g ^{-1}(B))\]

is well-defined, since \(g^{-1}(B)\in{\cal A}^{\prime}\). Thus the first claim holds.

(ii) We only prove the first equality in (12) when \(A^{\prime}=\Omega^{\prime}\) and \(X^{-1}(\Omega^{\prime})=\Omega\), since we can replace \(g\) by \(g\times 1_{A^{\prime}}\), noting that \(1_{A^{\prime}}(X(\omega))=1_{x^{-1}(A^{\prime})}(\omega)\).

Case 1. \(g=1_{A^{\prime}}\) : Then

(b) \[\int 1_{A^{\prime}}(X)\,d\mu=\int 1_{X^{-1}(A^{\prime})}\,d\mu=\mu(X^{-1}(A^{ \prime}))=\mu_{X}(A^{\prime})=\int 1_{A^{\prime}}\,d\mu_{X}.\]

Case 2. \(g=\sum_{i=1}^{n}c_{i}1_{A^{\prime}_{i}}\), where \(\sum_{i=1}^{n}A^{\prime}_{i}=\Omega^{\prime}\) with \(A^{\prime}_{i}\in{\cal A}^{\prime}\) and all \(c_{i}\geq 0\): Then

(c) \[=\sum_{i=1}^{n}c_{i}\int 1_{A^{\prime}_{i}}\,d\mu_{X}=\int g\,\,d\mu_{X}.\]

Case 3. \(g\geq 0\): Let \(g_{n}\geq 0\) be simple with \(g_{n}\nearrow g\): Then

(d) \[\begin{array}{ll}\int g(X)\,d\mu=\lim\int g_{n}(X)\,d\mu&\mbox{ by the MCT, since $g_{n}(X)\nearrow g(X)$}\\ =\lim\int g_{n}\,d\mu_{X}&\mbox{ by case 2}\end{array}\]

(e) \[=\int g\,\,d\mu_{X}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\mbox{by the MCT.}\]

In the arguments (b), (c), (d), (e) one should start from the end that is assumed to exist, in order to make a logically tight argument. (Note the next exercise.) \(\Box\)

**Exercise 2.4** Let \(Y\equiv g(X)\) in the context of the theorem 2.6. Verify the truth of the second equality in (13).

**Exercise 2.5** Let \(X\) equal \(-1,0,1\) with probability \(1/3\) for each possibility. Let \(g(x)=x^{2}\). Then evaluate both sides in (13), and see why such calculations were performed unconsciously for years.

**Exercise 2.6** (Integrals as measures): Let \(X\geq 0\) for some measurable function \(X\) on \((\Omega,\mathcal{A})\). Show that

\[\phi(A)\equiv\int_{A}X\,d\mu\qquad\qquad\text{for all }A\in\mathcal{A} \tag{14}\]

defines a measure \(\phi\) on \((\Omega,\mathcal{A})\). Note that \(\phi\) is a finite measure when \(X\in\mathcal{L}_{1}(\Omega,\mathcal{A},\mu)\).

**Exercise 2.7** (Absolutely continuous dfs): Let \(Z\geq 0\) on \(R\) with \(Z\in\mathcal{L}_{1}(R,\mathcal{B},\lambda)\) for the Lebesgue measure \(\lambda(\cdot)\) generalization of length. Then \(F(x)\equiv\int_{(-\infty,x]}Zd\lambda\) defines a generalized df on \(R\). Use the absolute continuity of the integral theorem 2.5 to show that this gdf is absolutely continuous on \(R\) (in the sense of definition 1.3.3).

## 3 Evaluating and Differentiating Integrals

Let \((R,\hat{\cal B}_{\mu},\mu)\) denote a Lebesgue-Stieltjes measure space that has been completed. If \(g\) is \(\hat{\cal B}_{\mu}\)-measurable, then \(\int_{(a,b]}g\,d\mu\) denotes a _Lebesgue-Stieltjes integral_ of \(g\) on \((a,b]\). If \(F\) is a generalized df corresponding to \(\mu\), then we also use the notation \(\int_{(a,b]}g\,d\mu_{F}\). Also, \(\int_{a}^{b}g\,dF\equiv\int_{(a,b]}g\,dF\) will denote the Riemann-Stieltjes integral.

**Theorem 3.1** (Equality of LS and RS integrals): Let \(g\) be continuous on \([a,b]\). Then the Lebesgue-Stieltjes integral and the Riemann-Stieltjes integral are equal. (Since the LS-integral and the RS-integral are equal, we can continue to evaluate most LS-integrals using the methods learned in a more elementary calculus.)

**Proof.** We first recall the classical setup associated with the definition of the RS-integral. Consider any sequence of partitions \(a\equiv x_{n0}<\cdots<x_{nn}\equiv b\) such that the partition \({\cal X}_{n}\equiv\{x_{n0},x_{n1},\ldots,x_{nn}\}\) is a _refinement_ of \({\cal X}_{n-1}\) in the sense that \({\cal X}_{n-1}\subset{\cal X}_{n}\). Then if \({\rm mesh}_{n}\equiv\max_{1\leq k\leq n}(x_{nk}-x_{n,k-1})\to 0\), and if \(x^{*}_{nk}\)'s are such that \(x_{n,k-1}<x^{*}_{nk}\leq x_{nk}\), we have

(a) \[g_{n}\equiv{\sum}_{k=1}^{n}g(x^{*}_{nk})1_{(x_{n,k-1},x_{nk}]}\to g \qquad\mbox{ uniformly on }[a,b],\]

since \(g\) is (necessarily) uniformly continuous on \([a,b]\). Thus for all such sequences the LS-integral of section 3.1 satisfies

\[\int_{(a,b]}g\,d\mu_{F}=\lim_{n}\int_{(a,b]}g_{n}\,d\mu_{F}\qquad \mbox{ by the DCT, bounded by a constant}\] \[=\lim_{n}{\sum}_{1}^{n}g(x^{*}_{nk})\mu((x_{n,k-1},x_{nk}])=\lim_{ n}{\sum}_{1}^{n}g(x^{*}_{nk})F(x_{n,k-1},x_{nk}]\] (b) \[=\lim_{n}\,\{\mbox{a Riemann--Stieltjes sum for the integral of }g\}=\lim_{n}\int_{a}^{b}g_{n}\,dF\] (c) \[=\{\mbox{the Riemann--Stieltjes integral of }g\}=\int_{a}^{b}g\,dF,\]

and this holds for all partitions and \(x^{*\prime}_{nk}\)s as above, provided only that \({\rm mesh}_{n}\to 0\). Thus the LS-integral \(\int_{a}^{b}g\,dF\) and the RS-integral are equal for continuous \(g\). \(\Box\)

**Exercise 3.1**: \({}^{*}\)(RS-integral compared to LS-integral) We state a few additional facts here, just for completeness (that are valid when \(g\) is more general):

(1) \(g\) is RS-integrable with respect to \(F\qquad\mbox{ if and only if}\)

(2) \(g\) is continuous a.e. \(\mu_{F}(\cdot)\).

(2) If \(g\) is RS-integrable with respect to \(F\),

then the RS and LS-integrals \(\int_{a}^{b}g\,dF\) are equal.

Let \(D(F)\) and \(D(g)\) denote the discontinuity sets of \(F\) and \(g\). Then

(3) \(g\) is not RS-integrable when \(D(F)\cap D(g)\neq\emptyset\).

(Consider \(g(\cdot)\equiv 1_{\{0\}}(\cdot)\) and \(F\equiv 1_{\{0,\infty\}}\) regarding (3).)

**Exercise 3.2**: Suppose that the improper RS-integral of a continuous function \(g\) on \(R\), defined by \(\mbox{RS}(\int g\,dF)\equiv\lim_{a\to-\infty,b\to\infty}(\mbox{RS}\ \int_{a}^{b}g\,dF)\) exists finitely. Then \(\lim_{a\to-\infty,b\to\infty}(\mbox{LS}\ \int_{a}^{b}|g|dF)\) need not be finite. Thus the fact that an improper RS-integral exists does not imply that the function is LS-integrable. Construct an example on \([0,\infty)\).

**Exercise 3.3** (**Differentiation under the integral sign)**: (a) Suppose that the function \(X(t,\cdot)\) is an integrable function on \((\Omega,\mu)\), for each \(t\in[a,b]\). Suppose also that for a.e. \(\omega\) the partial derivative \(\frac{\partial}{\partial t}X(t,\omega)\) exists for all \(t\) in the nondegenerate interval \([a,b]\) (use one-sided derivatives at the end points), and that

\[|\frac{\partial}{\partial t}X(t,\omega)|\leq Y(\omega)\qquad\mbox{for all }t \in[a,b],\quad\mbox{where }Y\in{\cal L}_{1}.\]

Then the derivative and integral may be interchanged, in that

\[\frac{d}{dt}\int_{\Omega}X(t,\omega)\,d\mu(\omega)=\int_{\Omega}\left[\frac{ \partial}{\partial t}X(t,\omega)\right]\,d\mu(\omega)\qquad\mbox{ for all }t\in[a,b]. \tag{4}\]

(b) Fix \(t\in(a,b)\). Formulate hypotheses that yield (4) at this fixed \(t\).

Exercise 3.4 and exercise 3.5 below combine to offer a more elementary problem that is still along the lines of exercise 3.1.

**Exercise 3.4** (Continuity on an interval [a, b] implies uniform continuity) Let \(g\) denote a continuous real valued function on a closed interval \([a,b]\). Show that \(g\) is uniformly continuous on \([a,b]\). That is, show that for every tiny number \(\epsilon>0\) there exists a number \(\delta_{\epsilon}>0\) for which

\[|f(y)-f(x)|<\epsilon\qquad\mbox{whenever}\qquad|x-y|<\delta_{\epsilon}. \tag{5}\]

**Exercise 3.5** (Riemann integrability) Let \(g\geq 0\) denote a continuous real valued function on a closed interval \([a,b]\). Let \(\epsilon\) and \(\delta_{\epsilon}\) be as in (5). Let \(\int_{a}^{b}g(x)\,dx\) denote the area under \(g\). Show that if \(\mbox{mesh}_{m}<\delta_{\epsilon}\) in the Riemann sum \(\mbox{RS}_{m}\) defined in (1.1.1), then \(\mbox{RS}_{m}\) is so close to the area \(\int_{a}^{b}g(x)\,dx\) that

\[|\mbox{RS}_{m}-\int_{a}^{b}g(x)\,dx|<\epsilon(b-a). \tag{6}\]

## 4 Inequalities

**Convexity**  We begin by briefly reviewing convexity. A real-valued function \(f\) defined on some interval \(I\) of real numbers is _convex_ if

\[\begin{array}{ll}f(\alpha x+(1-\alpha)y)\leq\alpha f(x)+(1-\alpha)f(y)\\ \mbox{for all $x,y$ in $I$ and all $0\leq\alpha\leq 1$.}\end{array} \tag{1}\]

We will use the following facts. If \(f\) is convex on an interval, then \(f\) is continuous on the interior \(I^{o}\) of the interval. Also, the left and right derivatives exist and satisfy \(D(x-)\leq D(x+)\) at each point in the interior \(I^{o}\) of the interval. The following is useful. Convexity on the interval \(I\) holds if and only if

\[\begin{array}{ll}f((x+y)/2)\leq[f(x)+f(y)]/2&\mbox{for all $x,y$ in $I$, provided that}\\ f\mbox{ is also assumed to be bounded (or continuous, or measurable) on $I$.}\end{array} \tag{2}\]

(There exist functions satisfying the inequality in (2) that are not continuous, but they are unbounded in every finite interval. Thus requiring (1) for all \(0\leq\alpha\leq 1\) is strictly stronger then requiring it to hold only for \(\alpha=1/2\).) We need a simple test for convexity (when \(f\) is 'nice'), and so note that \(f\) is convex if

\[f^{\prime}(x)\mbox{ is }\nearrow\mbox{ for all $x\in I$}\qquad\mbox{or if}\qquad f^{\prime\prime}(x)\geq 0\mbox{ for all $x\in I$.} \tag{3}\]

We call \(f\)_strictly convex_ if strict inequality holds in any of the above. If \(f\) is convex, then there exists a linear function \(\ell\) such that \(f(x)\geq\ell(x)\) with equality at any prespecified \(x_{0}\) in the interior \(I^{o}\) of the domain \(I\) of \(f\); this function is called the _supporting hyperplane_. (Call \(f\)_concave_ in \(-f\) is convex.) \(\Box\)

**Definition 4.1** (Moments): (The following definitions make sense on a general measure space \((\Omega,{\cal A},\mu)\), Recall, as in (3.1.6), that \({\rm E}h(X)=\int h(X(\omega))\,d\mu(\omega)=\int h(X)\,d\mu=\int h(X)\).) Let

\[\mu\equiv\mu_{X}\equiv\mbox{ (the $mean$ of $X$)}\equiv{\rm E}X.\qquad\mbox{(Note the two different uses of $\mu$)} \tag{4}\]

Now let:

\[{\rm E}X^{k} \equiv(kth\;moment\mbox{ of $X$)},\qquad\qquad\qquad\qquad\qquad \qquad\mbox{ for $k\geq 1$ an integer}, \tag{6}\] \[{\rm E}|X|^{r} \equiv(\mbox{\it rth absolute moment of $X$)},\qquad\qquad\qquad\qquad\mbox{ for $r>0$},\] (7) \[\|X\|_{r} \equiv(\mbox{\it rth norm of $X$})\equiv\{{\rm E}|X|^{r}\}^{1/r}\mbox{ for $r\geq 1$}\quad\mbox{(or,${\rm E}|X|^{r}$ for $r<1$)}. \tag{5}\]

The following notation is standard on a probability space \((\Omega,{\cal A},P)\), where \(\mu(\Omega)=1:\)

\[\begin{array}{ll}\sigma^{2}\equiv{\rm Var}[X]\equiv{\rm E}(X-\mu)^{2}=( \mbox{the $variance$ of $X$}),\\ \sigma\equiv{\rm StDev}[X]\equiv\mbox{ (the $standard$ deviation$ of $X$)}.\end{array} \tag{8}\]

We will write \(X\cong(\mu,\sigma^{2})\) (on a probability space) if \({\rm E}X=\mu\) and \({\rm Var}\;[X]=\sigma^{2}<\infty\). We will write \(X\cong F(\mu,\sigma^{2})\) if \(X\) also has df \(F(\cdot)\). Further (on a probability space)

\[\mu_{k}\equiv{\rm E}(X-\mu)^{k}\equiv(k\mbox{\it th central moment of $X$}),\quad\mbox{for $k\geq 1$}, \tag{10}\] \[{\rm Cov}[X,Y]\equiv{\rm E}[(X-\mu_{X})(Y-\mu_{Y})]=(\mbox{the $covariance$ of $X$ and $Y$}). \tag{9}\]

Note that \({\rm Cov}\;[{\rm X},X]={\rm Var}[X]\). (Probability theory has \(P(\Omega)\equiv\mu(\Omega)=1\).)Throughout this section \(X\) and \(Y\) will denote measurable functions.

**Proposition 4.1** (\({\cal L}_{s}\subset{\cal L}_{r}\)): Let \(\mu(\Omega)<\infty\). Then \({\cal L}_{s}\subset{\cal L}_{r}\) whenever \(0<r<s\). (So if \({\rm E}|X|^{s}<\infty\), then \({\rm E}|X|^{r}\) and \({\rm E}X^{k}\) are finite for all \(0\leq r,k\leq s\).)__

**Proof.** Now, \(|x|^{r}\leq 1+|x|^{s}\); and integrability is equivalent to absolute integrability. Note that \(\mu(\Omega)<\infty\) is used to claim \(1\in{\cal L}_{1}\). \(\square\)

**Proposition 4.2**: Let \(\mu(\Omega)<\infty\). Then \(\sigma^{2}<\infty\) holds if and only if \({\rm E}X^{2}<\infty\). From here on, we will only refer to \(\sigma^{2}\) on a probability space; then \(\sigma^{2}={\rm E}X^{2}-\mu^{2}\).__

**Proof.** Let \({\rm E}X^{2}<\infty\). Then \({\rm E}X^{2}-\mu^{2}\mu(\Omega)={\rm E}(X^{2})-{\rm E}(2\mu(X-\mu))-{\rm E}( \mu^{2})={\rm E}(X-\mu)^{2}={\rm Var}[X]\). Note that proposition 4.1 was used for \({\rm E}X.\) Thus \(\mu(\Omega)<\infty\) was used. Let \(\sigma^{2}<\infty\). Then \({\rm E}\{(X-\mu)^{2}+2\mu(X-\mu)+\mu^{2}\}={\rm E}X^{2}\). \(\square\)

**Inequality 4.1** (\({\rm C}_{r}\)-inequality): \({\rm E}|X+Y|^{r}\leq C_{r}{\rm E}|X|^{r}+C_{r}{\rm E}|Y|^{r}\),

where \(C_{r}=2^{r-1}\) for \(r\geq 1\) and \(C_{r}=1\) for \(0<r\leq 1\). (Note, \({\cal L}_{r}\) is a vector space.)__

**Proof.** There are no restrictions on \(\mu\). Note that \({\rm E}|X+Y|^{r}\leq{\rm E}(|X|+|Y|)^{r}\).

Case 1. \(r>1\): Then \(|x|^{r}\) is convex in \(x\) for \(x\geq 0\), since its derivative is \(\uparrow\). Thus \(|(x+y)/2|^{r}\leq[|x|^{r}+|y|^{r}]/2\); and now take expectations.

Case 2. \(0<r\leq 1\): Now, \(|x|^{r}\) is concave and \(\nearrow\) for \(x\geq 0\); just examine derivatives. Thus \(|x+y|^{r}-|x|^{r}\leq|0+y|^{r}-0^{r}\) since the increase from \(x\) to \(x+y\) can not exceed the increase from \(0\) to \(y\), and now take expectations. \(\square\)

**Inequality 4.2** (Holder's inequality): For \(r>1\), with \(1/r+1/s=1\),

\[|{\rm E}(XY)|\leq{\rm E}|XY|\leq{\rm E}^{1/r}|X|^{r}{\rm E}^{1/s}|Y|^{s}=\|X\| _{r}\|Y\|_{s}. \tag{11}\]

(ii) Suppose both \({\rm E}|X|^{r}\) and \({\rm E}|Y|^{s}\) fall in \((0,\infty)\). Equality holds in the second half if and only if \(|Y|^{s}=_{a.e.}\)\(c|X|^{r}\) for some \(c>0\); then, \(c={\rm E}|Y|^{s}/{\rm E}|X|^{r}\). Further, \(|{\rm E}XY|={\rm E}|XY|\) if and only if \(XY\geq_{a.e.}0\) or \(XY\leq_{a.e.}0\).

**Proof.** The result is trivial if \({\rm E}|X|^{r}=0\) or \(\infty\). Likewise for \({\rm E}|Y|^{s}\). So suppose that both expectations are in \((0,\infty)\). Since \(f(x)=e^{x}\) is convex by fact (3), it satisfies (1) with \(\alpha\equiv 1/r\) and \(1-\alpha=1/s,x\equiv r\log|a|\) and \(y\equiv s\log|b|\) for some \(a\) and \(b\); thus (1) becomes (with equality if and only if \(r\log|a|=x=y=s\log|b|\))

\[\exp(\frac{1}{r}x+\frac{1}{s}y)\leq\frac{1}{r}e^{x}+\frac{1}{s}e^{y},\mbox{ or:} \tag{12}\]

**Young's inequality** For all \(a,b\) we have

\[|ab|\leq\frac{|a|^{r}}{r}+\frac{|b|^{s}}{s}\mbox{ with equality iff }|a|^{r}=|b|^{s} \quad\mbox{iff}\quad|b|=|a|^{1/(s-1)}=|a|^{r-1}. \tag{13}\]

Now let \(a=|X|/\|X\|_{r}\) and \(b=|Y|/\|Y\|_{s}\), and take expectations. Equality holds if and only if \((|Y|/\|Y\|_{s})^{s}=_{a.e.}(|X|/\|X\|_{r})^{r}\) (that is, all mass is located at equality in (12)) if and only if

(a) \[(|Y|^{s}/{\rm E}|Y|^{s})=_{a.e.}(|X|^{r}/{\rm E}|X|^{r})\quad(\mbox{or }|Y|^{s}=_{a.e.}({\rm E}|Y|^{s})/{\rm E}|X|^{r})|X|^{r}).\]

**Exercise 4.1** (Convexity inequality): Show that

\[u^{\alpha}v^{1-\alpha}\leq\alpha u+(1-\alpha)v\quad\mbox{for all $0\leq\alpha\leq 1 $ and all $u,v\geq 0$.}\]

Use this to reprove Holder's inequality.

**Inequality 4.3** (Cauchy-Schwarz): \(\{{\rm E}(XY)\}^{2}\leq({\rm E}|XY|)^{2}\leq{\rm E}X^{2}{\rm E}Y^{2}\)_._

If both \({\rm E}X^{2}\) and \({\rm E}Y^{2}\) take values in \((0,\infty)\), then equality holds throughout both of the inequalities if and only if either \(Y=aX\) a.e. or \(Y=-aX\) a.e., for some \(a>0\); in fact, \(a^{2}={\rm E}Y^{2}/{\rm E}X^{2}\). (Only \(Y^{2}=cX^{2}\) a.e. for some \(c>0\) is required for equality in the rightmost inequality above.)

**Example 4.1** (Correlation inequality): For rvs \(X\) and \(Y\) (on a probability space) having positive and finite variances, it holds that

\[-1\leq\rho\leq 1, \tag{14}\]

for the _correlation_\(\rho\) of \(X\) and \(Y\) defined by

\[\rho\equiv\rho_{X,Y}\equiv\ {\rm Corr}[X,Y]\equiv\frac{{\rm Cov}[X,Y]}{\sqrt{{ \rm Var}[X]{\rm Var}[Y]}}.\qed \tag{15}\]

**Exercise 4.2** Consider rvs \(X\) and \(Y\) having \({\rm E}X^{2}\) and \({\rm E}Y^{2}\) in \((0,\infty)\). Show that

\[\rho =+1\ \mbox{if and only if $X-\mu_{X}=a(Y-\mu_{Y})$ a.e. for some $a>0$,}\] \[\rho =-1\ \mbox{if and only if $X-\mu_{X}=a(Y-\mu_{Y})$ a.e. for some $a<0$.}\]

In fact \(a^{2}=\sigma_{Y}^{2}/\sigma_{X}^{2}\). Thus \(\rho\) measures linear dependence, not general dependence.

**Inequality 4.4** (a) **(Liapunov's inequality)**: It holds that

\[{\rm E}^{1/r}|X|^{r}\ \mbox{is}\nearrow\mbox{in}\ r\ \mbox{for all}\ r\geq 0,\ \mbox{provided}\ \mu(\Omega)=1. \tag{16}\]

(b) Let \(\mu(\Omega)\) be finite. Then \(\|X\|_{r}\leq\|X\|_{s}\times\{\mu(\Omega)\}^{\frac{1}{r}-\frac{1}{s}}\) for all \(0<r<s\).

(c) \(h(r)\equiv\log{\rm E}|X|^{r}\) is convex on \([a,b]\) if \(X\in{\cal L}_{a}\cap{\cal L}_{b}(\ 0<a\) and any \(\mu(\Omega)\) value).

**Proof.** (c) Apply Holder to \(|X|^{\alpha a}\) and \(|X|^{(1-\alpha)b}\) with \(r=1/\alpha\) and \(s=1/(1-\alpha)\) and obtain the inequality

(p) \[{\rm E}|X|^{\alpha a+(1-\alpha)b}\leq({\rm E}|X|^{\alpha a\cdot\frac{1}{\alpha }})^{\alpha}({\rm E}|X|^{(1-\alpha)b\cdot\frac{1}{1-\alpha}})^{1-\alpha}\]

(q) \[=({\rm E}|X|^{a})^{\alpha}({\rm E}|X|^{b})^{1-\alpha}.\]

(All expectations are finite if \(X\in{\cal L}_{a}\cap{\cal L}_{b}\); since \(a\leq r\leq b\) and \(c>0\) implies that \(c^{r}\leq c^{b}\) or \(c^{r}\leq c^{a}\) as \(c\geq 1\) or \(c\leq 1\).) Taking logarithms gives the convexity

(r) \[h(\alpha a+(1-\alpha)b)\leq\alpha h(a)+(1-\alpha)h(b).\]

(b) Finiteness of \({\rm E}|X|^{b}\) gives the finiteness of \({\rm E}|X|^{r}\) on \((0,b]\) via proposition 4.1. Then apply Holder to \(|X|^{r}\cdot 1\) with \(a=\frac{s}{r}\) and \(\frac{1}{a}+\frac{1}{b}=1\) (and \({\rm E}1=\mu(\Omega)\)).

**Exercise 4.3** (Littlewood's inequality): Define \(m_{r}\equiv{\rm E}|X|^{r}\). Show that for \(0\leq r\leq s\leq t\) we have (write \(m_{s}={\rm E}(|X|^{\lambda s}\cdot|X|^{(1-\lambda)s})\), and apply Holder)

\[m_{s}^{t-r}\leq m_{r}^{t-s}m_{t}^{s-r}\qquad\qquad\qquad(\mbox{thus},\quad m_{ 2}^{3}\leq m_{1}^{2}m_{4}). \tag{17}\]

**Inequality 4.5** (Minkowski's inequality): \({\rm E}^{1/r}|X+Y|^{r}\leq{\rm E}^{1/r}|X|^{r}+{\rm E}^{1/r}|Y|^{r}\) for all \(r\geq 1\). That is, \(\|X+Y\|_{r}\leq\|X\|_{r}+\|Y\|_{r}\) for \(r\geq 1\).

(Recall that \(\|X+Y\|_{r}\leq\|X\|_{r}+\|Y\|_{r}\) for \(0<r\leq 1\), by the \(C_{r}\)-inequality and (7).)

Thus \(\|\cdot\|_{r}\) turns \({\cal L}_{r}\) into a metric space (if we identify \(X\) and \(X^{\prime}\) when \(X=_{a.e.}X^{\prime}\)).

**Proof.** This is trivial for \(r=1\). Suppose \(r>1\), and note that \(s=r/(r-1)\). Then for any measure \(\mu\) we have

(a) \[{\rm E}\{|X+Y|^{r}\}\leq{\rm E}\{|X||X+Y|^{r-1}\}+{\rm E}\{|Y||X+ Y|^{r-1}\}\] \[\qquad\leq(\|X\|_{r}+\|Y\|_{r})\ \|\|X+Y|^{r-1}\|_{s}\qquad\mbox{by H\"{o}lder's inequality twice}\] \[\qquad=(\|X\|_{r}+\|Y\|_{r})\ {\rm E}^{1/s}|X+Y|^{(r-1)s}=(\|X\|_{r}+ \|Y\|_{r}){\rm E}^{1/s}|X+Y|^{r}.\]

If \({\rm E}|X+Y|^{r}=0\), the result is trivial. If not, we divide to get the result. \(\Box\)

**Inequality 4.6** (Basic inequality): Let \(g\geq 0\) be \(\nearrow\) on \([0,\infty)\) and even. Then for all measurable \(X\) we have

\[\mu(|X|\geq\lambda)\leq{\rm E}g(X)/g(\lambda)\qquad\qquad\qquad\qquad\mbox{ for all }\lambda>0. \tag{18}\]

**Proof.** Now,

(a) \[{\rm E}g(X)=\int_{[|X|\geq\lambda]}g(X)\,d\mu+\int_{[|X|<\lambda]}g(X)\,d\mu \geq\int_{[|X|\geq\lambda]}g(X)\,d\mu\] (b) \[\qquad\geq g(\lambda)\int_{[|X|\geq\lambda]}1\,d\mu=g(\lambda)\mu(|X|\geq \lambda).\] \(\Box\)

The next two inequalities are immediate corollaries.

**Inequality 4.7** (Markov's inequality): \(\mu(|X|\geq\lambda)\leq{\rm E}|X|^{r}/\lambda^{r}\) for all \(\lambda>0\).

**Inequality 4.8** (Chebyshev's inequality): If \({\rm E}|X|<\infty\), then

\[\mu(|X-\mu|\geq\lambda)\leq{\rm Var}[X]/\lambda^{2}\qquad\qquad\qquad\mbox{ for all }\lambda>0. \tag{19}\]

**Inequality 4.9** (Paley-Zygmund): If \({\rm E}|X|<\infty\) for a rv \(X\geq 0\), then

\[P(X>\lambda)\geq[({\rm E}X-\lambda)^{+}]^{2}/{\rm E}X^{2}\qquad\qquad\qquad \mbox{ for each }\lambda>0. \tag{20}\]

**Proof.** Cauchy-Schwarz, and then rearrangement give the inequality

\[{\rm E}X={\rm E}(X1_{[X\leq\lambda]})+{\rm E}(X1_{[X>\lambda]})\leq\lambda+\sqrt{{ \rm E}(X^{2})P(X>\lambda)}\]

\(\Box\)

**Inequality 4.10** (Jensen's inequality): Let \(X:(\Omega,{\cal A},P)\to(I,B_{I},P_{X})\), where \(I\)_any_ interval subset of \([-\infty,\infty]\); thus \(P(X\in I)=1\). Suppose \({\rm E}X\) is in the interior \({\rm I}^{\rm o}\) of the interval \(I\). Let \(g\) be convex on \(I\). Then the rv \(X\) satisfies

\[g({\rm E}X)\leq{\rm E}g(X).\hskip 113.811024pt({\rm Again},\ P(\Omega)=1\ {\rm is\ required}.)\]

For strictly convex \(g\), equality holds if and only if \(X={\rm E}X\) a.e.

(Comment. Useful \(g\) include \(t^{r}\) on \([0,\infty)\) for any \(r\geq 1,|t|\), and \(-\log t\) on \((0,\infty)\).)

**Proof.** Let \(\ell(\cdot)\) be a supporting hyperplane to \(g(.)\) at \({\rm E}X.\) Then

(a) \[{\rm E}g(X)\geq{\rm E}\ell(X)\] (b) \[=\ell({\rm E}X)\qquad{\rm since}\ \ell(\cdot)\ {\rm is\ linear\ and}\ \mu(\Omega)=1\] (c) \[=g({\rm E}X)\qquad{\rm since}\ g(\cdot)=\ell(\cdot)\ {\rm at}\ {\rm E}X.\]

Now \(g(X)-\ell(X)\geq 0\). Thus \({\rm E}g(X)={\rm E}l(X)\) if and only if \(g(X)=\ell(X)\) a.e. \(\mu\) if and only if \(X={\rm E}X\) a.e. \(\mu\). \(\Box\)

**Inequality 4.11** (Bonferroni): For any events \(A_{k}\) on a probability space \((\Omega,{\cal A},P)\),

\[{\sum}_{i=1}^{n}P(A_{i})\geq P\bigl{(}{\bigcup}_{i=1}^{n}\,A_{i})\geq{\sum}_{i =1}^{n}P(A_{i})-{\sum}{\sum}_{i\neq j}P(A_{i}A_{j})\.\]

**Exercise 4.4** (Winsorized variance): (a) Let the rv \(X\) have finite mean \(\mu\). Fix \(c,d\) with \(c\leq\mu\leq d\). Let \(\tilde{X}\) equal \(c,X,d\) according as \([X\leq c],[c<X\leq d],[d<X]\), and set \(\tilde{\mu}\equiv{\rm E}\tilde{X}\). Show that \({\rm E}|\tilde{X}-\tilde{\mu}|^{2}\leq{\rm E}|\tilde{X}-\mu|^{2}\leq{\rm E}|X- \mu|^{2}\).

(b)\({}^{*}\) (Chow and Teicher) Given both a rv \(X\) with finite mean \(\mu\) and a number \(r\geq 1\), show how to choose \(c,d\) so that \({\rm E}|\tilde{X}-\tilde{\mu}|^{r}\leq{\rm E}|X-\mu|^{r}\).

**Exercise 4.5**\({}^{*}\)(Hardy): Let \(h\in{\cal L}_{2}(R^{+},{\cal B},\lambda)\) and define \(\bar{H}(u)=u^{-1}\int_{0}^{u}h(s)ds\) for \(u>0\). Let \(r>1\). Use the Holder inequality to show that

\[\int_{0}^{\infty}\bar{H}^{r}(u)du\leq(\ \frac{r}{r-1})^{r}\int_{0}^{\infty}h^{r }(u)du\ {\rm with\ equality\ if\ and\ only\ if\ }h=0\ {\rm a.e.} \tag{22}\]

Let \(r>1\). Then \(\sum_{1}^{\infty}(\frac{1}{n}{\sum_{1}^{n}}x_{k})^{r}\leq(\frac{r}{r-1})^{r}{ \sum_{1}^{\infty}}x_{n}^{r}\) when all \(x_{n}\geq 0\).

[Hint. Write \(\bar{H}(u)=u^{-1}\int_{0}^{u}h(s)s^{\alpha}s^{-\alpha}ds\) for some \(\alpha\). Also, first consider \(x_{n}\searrow\).]

**Exercise 4.6** (Wellner): Let \(T\cong{\rm Binomial}(n,p)\), so \(P(T=k)=\pmatrix{n\cr k}p^{k}(1-p)^{n-k}\) for \(0\leq k\leq n\). The measure associated with \(T\) has mean \(np\) and variance \(np(1-p)\). Then use inequality 4.6 with \(g(x)=\exp(rx)\) and \(r>0\), to show that

\[P(T/n\geq p\epsilon)\leq\exp(-nph(\epsilon)),\hskip 28.452756pt{\rm where}\ h( \epsilon)\equiv\epsilon(\log(\epsilon)-1)+1. \tag{23}\]

**Exercise 4.7** (Geometric mean): Show that \((x_{1}\times\cdots\times x_{n})^{1/n}\leq(x_{1}+\cdots+x_{n})/n\) whenever all \(x_{k}\geq 0\).

**Exercise 4.8**: \({}^{*}\): Let \(X,Y\geq 0\) with \(XY\geq 1\) and \(P(\Omega)=1\). Show that

\[\mu_{X}\times\mu_{Y}\geq 1\qquad\mbox{and}\qquad\{1+\mu_{X}^{2}\}^{1/2}\leq \mbox{E}\{(1+X^{2})^{1/2}\}\leq(1+\mu_{X}). \tag{24}\]

**Exercise 4.9**: \({}^{*}\)(Clarkson's inequality): Let \(X,Y\) in \({\cal L}_{r}(\Omega,{\cal A},\mu)\). Show that

\[\mbox{E}|X+Y|^{r}+\mbox{E}|X-Y|^{r}\leq 2^{r-1}\{\mbox{E}|X|^{r}+\mbox{E}|Y|^{ r}\}\qquad\mbox{provided $r\geq 2$}. \tag{25}\]

**Exercise 4.10**: Let \({\cal L}\) Log \({\cal L}\) denote all measurable \(X\) having \(\mbox{E}\{|X|\times\mbox{Log}(|X|)\}\) finite, with \(\mbox{Log}(x)\equiv(1\vee\log x)\). Show that \({\cal L}_{r}\subset{\cal L}\) Log \({\cal L}\subset{\cal L}_{1}\), for all \(r>1\).

**Exercise 4.11**: Show that for all \(a,b\) we have

\[||a|-|b||^{r}\leq||a|^{r}-|b|^{r}|\qquad\mbox{for $r\geq 1$}, \tag{26}\]

with the reverse inequality for \(0<r<1\).

**Exercise 4.12**: Let \((\Omega,{\cal A},\mu)\) have \(\mu(\Omega)<\infty\). Then \(P(A)\equiv\mu(A)/\mu(\Omega)\), for all \(A\in{\cal A}\), is a probability measure \(P\).

(a) Restate Jensen's inequality (26) in terms of \(\mu\).

(b) Restate Liapunov's inequality (16) in terms of \(\mu\).

## 5 Modes of Convergence

**Definition 5.1** (Modes of convergence): Let \(X\) and \(X_{n}\)'s be measurable and a.e. finite from the measure space \((\Omega,{\cal A},\mu)\) to \((\bar{R},\bar{B})\).

(a) Recall that \(X_{n}\)_converges_\(a.e.\) to \(X\) (denoted by \(X_{n}\to_{a.e.}X\)) if

\[X_{n}(\omega)\to X(\omega)\quad\mbox{for all }\omega\in A,\qquad\quad\mbox{ where }\mu(A^{c})=0. \tag{1}\]

(b) Also, recall that \(X_{n}\)_converges in measure_ to \(X\) (denoted by \(X_{n}\to_{\mu}X\)) if

\[\mu([\omega:|X_{n}(\omega)-X(\omega)|\geq\epsilon])\to 0\qquad\mbox{for each }\epsilon>0. \tag{2}\]

(c) Now (rigorously for the first time), \(X_{n}\)_converges in \(r\)th mean_ to \(X\) (denoted by \(X_{n}\to_{r}X\) or \(X_{n}\to_{{\cal L}_{r}}X\)) if

\[{\rm E}|X_{n}-X|^{r}\to 0\quad\mbox{for $X_{n}$'s and $X$ in ${\cal L}_{r}$}; \tag{3}\]

here, \(r>0\) is fixed. (Note from the \(C_{r}\)-inequality that if \(X_{n}-X\) and one of \(X\) or \(X_{n}\) is in \({\cal L}_{r}\), then the other of \(X_{n}\) or \(X\) is also in \({\cal L}_{r}\).)

Recall from chapter 2 that \(X_{n}\to a.e.\) (some a.e. finite \(X\)) holds if and only if \(X_{n}-X_{m}\to_{a.e.}0\) as \(m\wedge n\to\infty\). Likewise, in chapter 2 we had \(X_{n}\to_{\mu}\) (some \(X\)) if and only if \(X_{n}-X_{m}\to_{\mu}0\) as \(m\wedge n\to\infty\). Next, we will consider \(X_{n}\to_{r}X.\)(First, note that \(X_{n}\to_{r}X\) trivially implies \(X_{n}\to_{\mu}X\), using the Markov inequality.)

**Exercise 5.1** (Completeness of \({\cal L}_{r}\)): (I) Let \(X_{n}\)'s be in any \({\cal L}_{r}\), for \(r>0\).

(a) (Riesz-Fischer) \(X_{n}\to_{r}\) (some \(X\in{\cal L}_{r}\)) if and only if \(X_{n}-X_{m}\to_{r}0\).

That is, \({\cal L}_{r}\) is _complete_ with respect to \(\to_{r}\). Prove (a), using (2.3.14). (Show that \(({\cal L}_{r},\|\cdot\|_{r})\) is a complete metric space (when \(r>0\)), provided that we identify \(X\) and \(X^{\prime}\) whenever \(X=X^{\prime}\) a.e.) (Note theorem 5.8 below regarding separability.)

(II) Let \(\mu(\Omega)<\infty\). Then:

(b) If \(X_{n}\to_{s}X,\mbox{ then }X_{n}\to_{r}X\) for all \(0<r\leq s\).

(c) Show by example that \(X_{n}\to_{{\cal L}_{r}}X\) does not imply that \(X_{n}\to_{a.e.}X.\)

(d) Show by example that \(X_{n}\to_{a.e.}X\) does not imply that \(X_{n}\to_{{\cal L}_{1}}X.\)

(Hint: Use Fatou's lemma in (a) and Holder's inequality in (b).)

**Summary** Let \(X\) and \(X_{n}\)'s be measurable and a.e. finite (see definition 5.1). Then

\[\begin{array}{l}X_{n}\mbox{ converges a.e., in measure, or in ${\cal L}_{r}$ to such an $X$}\\ \mbox{ if and only if}\\ X_{n}\mbox{ is Cauchy a.e., Cauchy in measure, or Cauchy in ${\cal L}_{r}$}.\end{array} \tag{4}\]

**Consequences of Convergence in Distribution on \((\Omega,{\cal A},P)\)**

**Notation 5.1**: Suppose now that \(\mu\) really denotes a probability measure, and so we will label it \(P\). Recall that rvs \(X_{n}\)_converges in distribution_ to a rv \(X\) (denoted by \(X_{n}\to_{d}X\), \(F_{n}\to_{d}F\) or \({\cal L}(X_{n})\to{\cal L}(X)\) with \({\cal L}(\cdot)\) referring to "law") when the dfs \(F\) and \(F_{n}\) of the rvs \(X\) and \(X_{n}\) satisfy (recall (2.4.4))

\[F_{n}(x)\to F(x)\mbox{ as }n\to\infty\quad\mbox{for each continuity point $x\in C_{F}$ of $F(\cdot)$}. \tag{5}\][Note that \(F_{n}\equiv 1_{[1/n,\infty)}\to_{d}F\equiv 1_{[0,\infty)}\), even though \(F_{n}(0)=0\dashrightarrow\!\!\!\dash 1=F(0)\).] The statement \(\to_{d}\) will carry with it the implication that \(F\) corresponds to a probability measure \(P\), which can be viewed as the \(P_{X}=\mu_{X}\) of an appropriate rv \(X\). \(\square\)

**Theorem 5.1** (a) **(Helly-Bray)** Consider the rvs \(X\) and \(X_{n}\) on some \((\Omega,{\cal A},P)\). Suppose \(F_{n}\to_{d}F\), and suppose that \(g\) is bounded and is continuous a.s.\(\,F\). Then

\[\int gdF_{n}={\rm E}g(X_{n})=\int g(X_{n})dP\to\int g(X)dP={\rm E}g(X)=\int gdF. \tag{6}\]

(b) Conversely, \({\rm E}g(X_{n})\to{\rm E}g(X)\) for all bounded, continuous \(g\) implies \(F_{n}\to_{d}F\).

**Theorem 5.2** (Mann-Wald): Consider the rvs \(X\) and \(X_{n}\) on some \((\Omega,{\cal A},P)\). Suppose \(X_{n}\to_{d}X\), and let \(g\) be continuous a.s.\(\,F\). Then \(g(X_{n})\to_{d}g(X)\).

**Proof.** We ask for a proof for continuous \(g\) in the next exercise, but we give a "look-ahead" proof now. (See theorem 6.3.2 below for the Skorokhod proof.)

**Skorokhod** If \(X_{n}\to_{d}X_{o}\), then the rvs \(Y_{n}\equiv F_{n}^{-1}\) on \(([0,1],{\cal B}_{[0,\,1]}P\equiv\lambda)\) have

\[Y_{n}\cong X_{n}\mbox{ for all }n\geq 0\quad\mbox{and}\quad especially\ Y_{n}\to Y_{o}\ \mbox{a.s.}\ P(\cdot). \tag{7}\]

Thus \(A_{1}\equiv\{\omega:Y_{n}(\omega)\to Y_{o}(\omega)\}\) has \(P(A_{1})=1\). Also,

(a) \[P(A_{2})\equiv P(\{\omega:g\mbox{ is continuous at }Y_{o}(\omega)\})\]

(b) \[=P_{Y_{o}}(\{y:g\mbox{ is continuous at }y\})=P_{Y_{o}}(C_{g})=P_{X}(C_{g})=1.\]

Thus \(A\equiv A_{1}\cap A_{2}=A_{1}\cap Y_{o}^{-1}(C_{g})\) has \(P(A)=1\). Especially,

(c) \[g(Y_{n}(\omega))\to g(Y_{o}(\omega))\mbox{ for all }\omega\in A,\quad\mbox{ with }P(A)=1.\]

Since \(g\) is bounded, applying the DCT to (7) gives the Helly-Bray claim that

(d) \[\int g\,dF_{n}\equiv\int g\,d\mu_{F_{n}}=\int g(Y_{n})dP\to\int g(Y_{o})dP= \int g\,d\mu_{F_{o}}\equiv\int g\,dF_{o}.\]

We note additionally that since (7) implies \(g(Y_{n})\to_{a.s.}g(Y_{o})\), it also implies \(g(Y_{n})\to_{d}g(Y_{o})\). Since \(g(X_{n})\cong g(Y_{n})\) for all \(n\), we can also conclude that \(g(X_{n})\to_{d}g(X_{o})\). This argument did not use the boundedness of \(g\), and so proves the Mann-Wald theorem. Theorem 3.2.6 was used twice in (d). (Proving Helly-Bray as indicated in the next exercise would have been possible now, but the proof based on Skorokhod's theorem is more in keeping with the spirit of this book. The Helly-Bray theorem will be used later in this section, in proving Vitali's theorem.)

Consider the converse. Let \(g_{\epsilon}(\cdot)\) equal 1, be linear, equal 0 on \((-\infty,x-\epsilon]\), on \([x-\epsilon,x]\), on \([x,\infty)\); and let \(h_{\epsilon}(\cdot)\) equal 1, be linear, equal 0 on \((-\infty,x]\), on \([x,x+\epsilon]\), on \([x+\epsilon,\infty)\), with \(g_{\epsilon}\) and \(h_{\epsilon}\) both continuous. (Let \({\cal G}\equiv\{\mbox{all such }g_{\epsilon}\mbox{ and }h_{\epsilon}\}\).) Then

\[F_{o}(x-\epsilon)\leq{\rm E}\,g_{\epsilon}(X_{o})=\lim{\rm E}\,g_{\epsilon}(X _{n})\leq\underline{\lim}\,F_{n}(x)\]

(e) \[\leq\overline{\lim}\,F_{n}(x)\leq\lim{\rm E}\,h_{\epsilon}(X_{n})={\rm E}\,h_{ \epsilon}(X_{o})\leq F_{o}(x+\epsilon),\]

so that \(F_{n}(x)\to F_{o}(x)\) at all continuity points of \(F\). \(\square\)

**Definition 5.2** (Determining class): Let \({\cal G}\) denote a collection of bounded and continuous functions \(g\) on the real line \(R\). If for any rvs \(X\) and \(Y\) the condition

\[{\rm E}g(X)={\rm E}g(Y)\mbox{ for all }g\in{\cal G}\mbox{ implies }X\cong Y,\]

then call \({\cal G}\) a _determining class_. (The proof of the converse half of Helly-Bray exhibited one such class of particularly simple functions. See also section 9.1 for further examples which will prove particularly useful.)

**Exercise 5.2**: (a) Prove the Helly-Bray result \(\int g\,dF_{n}\to\int g\,dF\) for all bounded and continuous \(g\), without appeal to theorem 6.3.2 of Skorokhod. (Truncate the real line at large continuity points \(\pm M\) of \(F\), and then use the uniform continuity of \(g\) on the interval \([-M,M]\) to obtain a simple proof in this case. Note exercise 9.1.1.)

(b) Alter your proof to be valid when \(g\) is bounded and merely continuous a.s. \(\mu_{F}\).

**General Moment Convergence on \((\Omega,{\cal A},\mu)\)**

**Theorem 5.3** (Moment convergence under \(\to_{r}\)): Let \(X_{n}\to_{r}X,r>0\). Then

\[{\rm E}|X_{n}|^{r}\to{\rm E}|X|^{r}. \tag{8}\]

Moreover, \(X_{n}^{+}\to_{r}X^{+},X_{n}^{-}\to_{r}X^{-}\), and \(|X_{n}|\to_{r}|X|\). (See also exercise 9.1.1.)

**Proof.** Let \((\Omega,{\cal A},\mu)\) be arbitrary and \(0<r\leq 1\). The \(C_{r}\)-inequality gives

(a) \[{\rm E}|X_{n}|^{r}\leq{\rm E}|X_{n}-X|^{r}+{\rm E}|X|^{r}\mbox{ and }{\rm E}|X| ^{r}\leq{\rm E}|X-X_{n}|^{r}+{\rm E}|X_{n}|^{r},\]

so that

\[|{\rm E}|X_{n}|^{r}-{\rm E}|X|^{r}|\leq{\rm E}|X_{n}-X|^{r}\to 0\mbox{ when }0<r\leq 1. \tag{9}\]

Suppose \(r\geq 1\). Then using Minkowski's inequality twice (as in (a)) gives

\[|{\rm E}^{1/r}|X_{n}|^{r}-{\rm E}^{1/r}|X|^{r}|\leq{\rm E}^{1/r}|X_{n}-X|^{r} \to 0,\quad\mbox{when }\,r\geq 1. \tag{10}\]

Combining (9) and (10) shows that \({\rm E}|X_{n}|^{r}\to{\rm E}|X|^{r}\). (Recall exercise 5.1(b).)

Now, \(|X_{n}^{+}-X^{+}|\) equals \(|X_{n}-X|,|X_{n}-0|,|0-X|,|0-0|\) just as \([X_{n}\geq 0,X\geq 0][X_{n}\geq 0\), \(X<0],[X_{n}<0,X\geq 0],[X_{n}<0,X<0]\). Thus

\[|X_{n}^{+}-X^{+}|\leq|X_{n}-X|,\mbox{ and }|X_{n}^{-}-X^{-}|\leq|X_{n}-X| \tag{11}\]

also. Hence \(X_{n}^{+}\to_{r}X^{+}\), so that \({\rm E}(X_{n}^{+})^{r}\to{\rm E}(X^{+})^{r}\). Likewise for \(X_{n}^{-}\). Cross-product terms are \(0\), since \(X^{+}(\omega)X^{-}(\omega)=0\); so if \(r=k\) is integral, then

(b) \[{\rm E}X_{n}^{k}={\rm E}(X_{n}^{+})^{k}+(-1)^{k}{\rm E}(X_{n}^{-})^{k}\to{\rm E }(X^{+})^{k}+(-1)^{k}{\rm E}(X^{-})^{k}={\rm E}(X^{k}).\]

**Uniform Integrability and Vitali's Theorem**

**Definition 5.3** (Uniformly integrable): A collection of measurable \(X_{t}\)'s is called _integrable_ if \(\sup_{t}{\rm E}|X_{t}|<\infty\). Further, a collection of rvs \(\{X_{t}:t\in T\}\) is said to be _uniformly integrable_ (which is abbreviated \(u.i.\)) if

\[\sup_{t\in T}{\rm E}\{|X_{t}|\times 1_{[|X_{t}|\geq\lambda]}\}\to 0\quad\mbox{as }\lambda\to\infty. \tag{12}\]

The functions \(X_{n}(t)\equiv\frac{1}{n}1_{[-n^{2},n^{2}]}(t)\) on \((R,{\cal B},\lambda)\) are u.i. but they are not integrable. We will see below that u.i. implies 'integrable' when \(\mu(\Omega)<\infty\).

**Remark 5.1** (Dominated \(X_{t}\)'s are u.i.)  Suppose these \(|X_{t}|\leq Y\)a.s. for some \(Y\in{\cal L}_{1}\). Then these \(X_{t}\)'s are integrable, in that \(\sup_{t}{\rm E}|X_{t}|\leq{\rm E}Y<\infty\). But, more is true. For some null sets \(N_{t}\), we have \([|X_{t}|\geq\lambda]\subset[|Y|\geq\lambda]\cup N_{t}\). It follows that \(\mu(|X_{t}|\geq\lambda)\leq\mu(|Y|\geq\lambda)\to 0\) uniformly in \(t\) as \(\lambda\to\infty\) (use Markov's inequality). Then for each fixed \(t\),

\[\int_{[|X_{t}|\geq\lambda]}|X_{t}|\,d\mu\leq\int_{[|Y|\geq\lambda]}Y\,d\mu\to 0 \mbox{ uniformly in }t\,{\rm as}\,\lambda\to\infty\]

by the absolute continuity of the integral of \(Y\) in theorem 3.2.5. Thus:

(13)  If all \(|X_{t}|\leq Y\) for some \(Y\in{\cal L}_{1}\), then these \(X_{t}\)'s are uniformly integrable. \(\square\)

**Exercise 5.3**  (a) Now \({\rm E}Y=\int_{0}^{\infty}P(Y\geq y)\,dy=\int_{0}^{\infty}[1-F(y)]\,dy\) for any rv \(Y\geq 0\) with df \(F\) (as will follow from Fubini's theorem below). Sketch a proof.

(b) In fact, this formula can also be established rigorously now. Begin with simple functions \(Y\) and sum by parts. Then apply the MCT for the general result.

(c) Use the result of (a) to show that for \(Y\geq 0\) and \(\lambda\geq 0\) we have

\[\int_{[Y\geq\lambda]}YdP=\lambda P(Y\geq\lambda)+\int_{\lambda}^{\infty}P(Y \geq y)\,dy.\hskip 56.905512pt\mbox{(Draw pictures.)}\]

(d) Suppose there is a \(Y\in{\cal L}_{1}\) such that \(P(|X_{n}|\geq y)\leq P(Y\geq y)\) for all \(y>0\) and all \(n\geq 1\). Then use (c) to show that \(\{X_{n}:n\geq 1\}\) is uniformly integrable.

**Exercise 5.4** (Uniform integrability criterion)  If \(\sup_{t}{\rm E}|X_{t}|^{r}\leq M<\infty\) for some \(r>1\), then the \(X_{t}\)'s are uniformly integrable. (Compare this to theorem 5.6 of de la Vallee Poussin below, by letting \(G(x)=x^{r}\).)

**Theorem 5.4** (Uniform absolute continuity of integrals)  Let \(\mu(\Omega)<\infty\). A family of measurable \(X_{t}\)'s is uniformly integrable if and only if both

\[\sup_{t}{\rm E}|X_{t}|\leq\mbox{ (some }M)\,<\infty\hskip 56.905512pt \mbox{(the collection is integrable), and} \tag{14}\]

\[\mu(A)<\delta_{\epsilon}\mbox{ implies }\sup_{t}\int_{A}|X_{t}|\,d\mu<\epsilon \hskip 28.452756pt\mbox{(uniform absolute continuity).} \tag{15}\]

Note: \(\mu(\Omega)<\infty\) is needed only to show that u.i. implies (14).

**Proof.**  Suppose these conditions hold. Then Markov's inequality and (14) give

(a) \[\mu(|X_{t}|\geq\lambda)\leq{\rm E}|X_{t}|/\lambda\leq\sup_{t}{\rm E}|X_{t}|/ \lambda<\delta_{\epsilon}\qquad\mbox{uniformly in }t\]

for \(\lambda\) large enough. Then (15) applied to the sets \([|X_{t}|\geq\lambda]\) yields (12). (Note that \(\mu(\Omega)<\infty\) was not used.)

Suppose the u.i. condition (12) holds. If \(\mu(A)<\delta\), then

(b) \[\int_{A}|X_{t}|\,d\mu=\int_{A}|X_{t}|\times 1_{[|X_{t}|<\lambda]}\,d\mu+ \int_{A}|X_{t}|\times 1_{[|X_{t}|\geq\lambda]}\,d\mu\]

(c) \[\leq\lambda\times\mu(A)+\int|X_{t}|1_{[|X_{t}|\geq\lambda]}\,d\mu\leq\epsilon/ 2+\epsilon/2=\epsilon\qquad\mbox{using (12)}\]for a sufficiently large fixed \(\lambda\) and for \(\delta\leq\epsilon/(2\lambda)\). (We have not yet used \(\mu(\Omega)<\infty\).) Moreover, for \(\lambda\) large enough, (12) again gives

\[\mbox{\rm(d)}\qquad\mbox{\rm E}|X_{t}|\leq\lambda\mu(\Omega)+\int_{[|X_{t}|\geq \lambda]}|X_{t}|\,d\mu\leq\lambda\,\mu(\Omega)+1\qquad\quad\mbox{for all $t$;}\]

thus the collection is integrable. Thus (15) holds. \(\Box\)

**Theorem 5.5** (Vitali): (i) Let \(\mu(\Omega)<\infty\) and \(r>0\). Suppose that \(X_{n}\to_{\mu}X\). The following are equivalent (where only (16) implies (17) and (20) uses \(\mu(\Omega)<\infty\)):

(16) \[\{|X_{n}|^{r}:n\geq 1\}\mbox{ are uniformly integrable.}\] (17) \[\|X_{n}-X\|_{r}\to 0.\qquad\quad\mbox{(Only $X$ or $X_{n}$'s in ${\cal L}_{r}$ is required; by (\ref{eq:10}) or (\ref{eq:10})).}\] (18) \[\mbox{\rm E}|X_{n}|^{r}\to\mbox{\rm E}|X|^{r}<\infty.\] (19) \[\overline{\lim}_{n}\,\mbox{\rm E}|X_{n}|^{r}\leq\mbox{\rm E}|X|^{ r}<\infty.\] (20) The uniform absolute continuity of (15) holds for the \(|X_{n}|^{r},\mbox{ and }X\in{\cal L}_{r}\).

(ii) Let \(\mu(\Omega)=\infty\) and \(r\geq 1\). Suppose \(X_{n}\to_{\mu\,\mbox{\rm\scriptsize or}\,a.e.}X\) with all \(X_{n}\) in \({\cal L}_{r}\).

(a) Then (17), (18), and (19) are equivalent--and they imply (16).

(b) Suppose the uniform absolute continuity of (15) holds for the \(|X_{n}|^{r}\). Suppose for each \(\epsilon>0\) there exists a set \(A_{\epsilon}\) having \(\mu(A_{\epsilon})<\infty\) for which \(\sup_{n}\int_{A_{\epsilon}^{C}}|X_{n}|^{r}\,d\mu\leq\epsilon\) (compare this with (14)). These two hold true if and only if (17)-(19) hold.

**Corollary 1** (\({\cal L}_{r}\)-convergence): Let \(\mu(\Omega)<\infty\). Let \(r>0\). Let all \(X_{n}\in{\cal L}_{r}\). Then \(X_{n}\to_{r}X(\mbox{\rm or},\mbox{\rm E}|X_{n}-X|^{r}\to 0)\) if and only if both \(X_{n}\to_{\mu}X\) and one (hence both) of the two families of functions \(\{|X_{n}|^{r}\,:\,n\geq 1\}\) or \(\{|X_{n}-X|^{r}:n\geq 1\}\) is u.i.

**Remark 5.2** (Vitali's theorem): Let \(X_{n}\to_{\mu}X\) throughout, with \(\mu(\Omega)\) arbitrary. Fatou and \(X_{n^{\prime\prime}}\to_{a.e.}X\) on some further subsequence \(n^{\prime\prime}\) of any \(n^{\prime}\)_always_ yield

\[\mbox{\rm E}|X|^{r}=\mbox{\rm E}\lim|X_{n^{\prime\prime}}|^{r}\leq\underline{ \lim}\,\mbox{\rm E}|X_{n^{\prime\prime}}|^{r}\leq\overline{\lim}\,\mbox{\rm E }|X_{n^{\prime\prime}}|^{r}\leq\overline{\lim}\,\mbox{\rm E}|X_{n}|^{r}. \tag{21}\]

So Vitali (ii)(a) yields \(\mbox{\rm E}|X_{n}|^{r}\to\mbox{\rm E}|X|^{r}<\infty\), if \(r\geq 1\). Vitali thus gives (for \(r\geq 1\))

\[\begin{array}{l}\mbox{\rm E}|X_{n}-X|^{r}\to 0\quad\mbox{if and only if}\quad \mbox{\rm E}|X_{n}|^{r}\to\mbox{\rm E}|X|^{r}\qquad\mbox{(any $\mu(\Omega)$ value)}\\ \mbox{if and only if}\quad\mbox{(in case $\mu(\Omega)<\infty$) the rvs $\{|X_{n}|^{r}:n\geq 1\}$ are u.i.}\end{array} \tag{22}\]

**Exercise 5.5**: Consider Vitali's theorem. In the proof that follows we will show that (17) \(\Rightarrow\) (18) \(\Rightarrow\) (19) \(\Rightarrow\) (16) and (20) \(\Rightarrow\) (16) for \(r>0\) and any \(\mu(\Omega)\) value.

(p) Prove Vitali's (ii)(a) that (19) implies (17) when \(r\geq 1\). (See exercise 5.10.)

(q) Prove the "true" Vitali theorem in (ii)(b). (Find a hint in exercise 5.10.)

(r) Give an example to demonstrate the implication that just (16) can hold in (ii).

(s) Note that \(t\in[0,\infty)\) may replace \(n\in\{1,2,\ldots\}\) in all of Vitali's theorem.

(t) Let \(r\geq 1\). Let \(X_{n}\to_{\mu}X\), where \(|X_{n}|^{r}\leq Y_{n}\) with \(Y_{n}\to_{\mu}Y\) and \(\mbox{\rm E}|Y_{n}|\to\mbox{\rm E}|Y|\) provides a bound. Show that \(X_{n}\to_{r}X\).

**Proof.\({}^{*}\)** Suppose (16); show (17). Now, \(X_{n^{\prime}}\to_{a.s.}X\) for some subsequence by theorem 2.3.1. Thus \(\mbox{\rm E}|X|^{r}=\mbox{\rm E}(\underline{\lim}\,|X_{n^{\prime}}|^{r})\leq \underline{\lim}\,\mbox{\rm E}|X_{n^{\prime}}|^{r}\leq M<\infty\) using Fatou and (14). Thus \(X\in{\cal L}_{r}\).

The \(C_{r}\)-inequality gives \(|X_{n}-X|^{r}\leq C_{r}\{|X_{n}|^{r}+|X|^{r}\}\). The \(|X_{n}-X|^{r}\) are easily shown u.i. as in the theorem 5.4 proof. So for large \(n\),

(j) \[\operatorname{E}\!|X_{n}-X|^{r}=\operatorname{E}\!\{|X_{n}-X|^{r} \times 1_{[|X_{n}-X|>e]}\}+\operatorname{E}\!\{|X_{n}-X|^{r}\times 1_{[|X_{n}-X| \leq\in]}\}\] (k) \[\leq\epsilon+\epsilon^{r}\times\mu(\Omega)\] (only here (and for \[\operatorname{E}\!|X|^{r}\] finite) was \[\mu(\Omega)<\infty\] needed);

the \(\epsilon\) in (k) is from (15), since \(\mu(|X_{n}-X|\geq\epsilon)\to 0\) by hypothesis. Thus (17) holds.

Now (16) implies (20) by theorem 5.4, with \(X\in{\cal L}_{r}\) by Fatou (as in the previous paragraph) and using \(\mu(\Omega)<\infty\). We will not use \(\mu(\Omega)<\infty\) again. Next, (20) implies (16) since \(\mu(|X_{n}|\geq\lambda)\leq\mu(|X_{n}-X|\geq\lambda/2)\cup\mu(|X|\geq\lambda/2 )<\epsilon+\epsilon\), by first specifying \(\lambda\) large (as \(X\in{\cal L}_{r}\)) and then \(n\) large (as \(X_{n}\to_{\mu}X\)).

Now, (17) implies (18) by theorem 5.3. Also (18) trivially implies (19).

Suppose (19) holds. Define \(f_{\lambda}\) to be a continuous function on \([0,\infty)\) that equals \(|x|^{r},0\), or is linear, according as \(|x|^{r}\leq\lambda,|x|^{r}\geq\lambda+1\), or \(\lambda\leq|x|^{r}\leq\lambda+1\). Then (graphing \(f_{\lambda}(x)\) and \(x^{r}\) on \([0,\lambda+1]\)) we have \(Y_{n}\equiv f_{\lambda}(X_{n})\to_{\mu}Y\equiv f_{\lambda}(X)\) by the uniform continuity of each \(f_{\lambda}\). (See exercise 2.3.4(b.).) Let \(n^{\prime}\) denote any subsequence of \(n\), and let \(n^{\prime\prime}\) denote a further subsequence on which \(X_{n^{\prime\prime}}\to_{a.e.}X\) (see (2.3.14) in theorem 2.3.1 of Riesz). On the subsequence \(n^{\prime\prime}\) we then have

\[\overline{\lim}_{n^{\prime\prime}}\int_{[|X_{n^{\prime\prime}}|^{ r}>\lambda+1]}|X_{n^{\prime\prime}}|^{r}\,d\mu\] \[=\overline{\lim}_{n^{\prime\prime}}\{\int|X_{n^{\prime\prime}}|^{ r}\,d\mu-\int_{[|X_{n^{\prime\prime}}|^{r}\leq\lambda+1]}|X_{n^{\prime\prime}}|^{ r}\,d\mu\}\] (l) \[\leq\operatorname{E}\!|X|^{r}-\underline{\lim}_{n^{\prime\prime} }\int_{[|X_{n^{\prime\prime}}|^{r}\leq\lambda+1]}|X_{n^{\prime\prime}}|^{r}\,d\mu \qquad\qquad\text{by (\ref{eq:n-1})}\] (m) \[\leq\operatorname{E}\!|X|^{r}-\underline{\lim}_{n^{\prime\prime} }\operatorname{E}\!f_{\lambda}(X_{n^{\prime\prime}})\leq\operatorname{E}\!|X| ^{r}-\operatorname{E}\!f_{\lambda}(X)\quad\text{by Fatou}\] (n) \[\leq\int_{[|X|^{r}\geq\lambda]}|X|^{r}\,d\mu\to 0\text{ as } \lambda\to\infty,\quad\text{since }X\in{\cal L}_{r}.\]

Thus, \(\overline{\lim}_{n}\int_{[|X_{n}|^{r}>\lambda+1]}|X_{n}|^{r}\,d\mu\leq\int_{[|X |^{r}\geq\lambda]}|X|^{r}\,d\mu\to 0\), which implies (16). 

**Theorem 5.6**: \({}^{*}\) (de la Vallee Poussin)  Let \(\mu(\Omega)<\infty\). A family of \({\cal L}_{1}\)-integrable functions \(X_{t}\) is uniformly integrable if and only if there exists a convex function \(G\) on \([0,\infty)\) for which \(G(0)=0,G(x)/x\to\infty\) as \(x\to\infty\) and

\[\sup_{t}\operatorname{E}\!G(|X_{t}|)<\infty. \tag{23}\]

**Proof.**  For \(\lambda\) so large that \(G(x)/x\geq c\) for all \(x\geq\lambda\) we have

(a) \[\int_{[|X_{t}|\geq\lambda]}|X_{t}|\,d\mu\leq\tfrac{1}{c}\int_{[|X_{t}|\geq \lambda]}G(|X_{t}|)\,d\mu\leq\tfrac{1}{c}\sup_{t}\operatorname{E}\!G(|X_{t}|)<\epsilon\]

for \(c\) sufficiently large. Thus (23) implies \(\{X_{t}:t\in T\}\) is uniformly integrable.

Now we show that \(\{X_{t}:t\in T\}\) u.i. implies (23) for some \(G\). We define \(G(x)=\int_{0}^{x}g(y)\,dy\) where (with a sequence \(b_{n}\nearrow\) having \(b_{0}=0\), to be specified below) \(g(x)\equiv b_{n}\) for all \(n\leq x<n+1,n\geq 0\). Define \(a_{n}(t)\equiv\mu(|X_{t}|\geq n)\). Note,\[\mbox{\rm EG}(|X_{t}|)\leq b_{1}\mu(1\leq|X_{t}|<2)+(b_{1}+b_{2})\mu(2 \leq|X_{t}|<3)+\cdots\] (b) \[=\sum\nolimits_{n=1}^{\infty}b_{n}a_{n}(t).\]

Note that \(G(n+2)\geq[n/2]g([n/2])=[n/2]b_{[n/2]}\), so that \(G(x)/x\to\infty\) as \(x\to\infty\). It thus suffices to choose \(b_{n}\nearrow\infty\) such that \(\sup_{t}\sum_{1}^{\infty}b_{n}a_{n}(t)<\infty\). By the definition of uniform integrability, we can choose integers \(c_{n}\uparrow\infty\) such that

\[\sup_{t}\int_{[|X_{t}|\geq c_{n}]}|X_{t}|\,d\mu\leq 1/2^{n}.\]

Thus for all \(t\) we have

\[1/2^{n}\geq\int_{[|X_{t}|\geq c_{n}]}|X_{t}|\,d\mu\geq\sum_{i=c_{ n}}^{\infty}i\,\mu(i\leq|X_{t}|<i+1)\] \[=\sum\nolimits_{i=c_{n}}^{\infty}\sum\nolimits_{j=1}^{i}\mu(i\leq |X_{t}|<i+1)\] \[\geq\sum\nolimits_{j=c_{n}}^{\infty}\sum\nolimits_{i=j}^{\infty} \mu(i\leq|X_{t}|<i+1)=\sum\nolimits_{j=c_{n}}^{\infty}\mu(|X_{t}|\geq j)\] (d) \[=\sum\nolimits_{j=c_{n}}^{\infty}a_{j}(t).\]

Thus, interchanging the order of summation,

\[1=\sum\nolimits_{n=1}^{\infty}2^{-n}\geq\sup_{t}\sum\nolimits_{n=1}^{\infty} \sum\nolimits_{j=c_{n}}^{\infty}a_{j}(t)=\sup_{t}\sum\nolimits_{j=1}^{\infty}b_ {j}a_{j}(t)\]

for \(b_{j}\equiv\) (the number of integers \(n\) such that \(c_{n}\) is \(\leq j\)).

While u.i. yields a convex \(G\) in (23), u.i. follows from (23) without convexity. \(\Box\)

**Exercise 5.6**  Consider only the definition of u.i. Do not appeal to Vitali.

(a) Let \(\xi\cong\) Uniform(0, 1), and let \(X_{n}\equiv(n/\log n)1_{[0,1/n]}(\xi)\) for \(n\geq 3\). Show that these \(X_{n}\) are uniformly integrable and \(\int X_{n}dP\to 0\), even though these rvs are not dominated by any fixed integrable rv \(Y\).

(b) Let \(Y_{n}\equiv n1_{[0,1/n]}(\xi)-n1_{[1/n,2/n]}(\xi)\). Show that these \(Y_{n}\) are not uniformly integrable, but that \(\int Y_{n}dP\to 0\). However, \(\int|Y_{n}|dP\not\to 0\).

**Summary of Modes of Convergence Results**

**Theorem 5.7** (Convergence implications): Let \(X\) and \(X_{n}\)'s be measurable and a.e. finite. (Note figure 5.1.)

(i) If \(X_{n}\to_{a.e.}X\) and \(\mu(\Omega)<\infty\), then \(X_{n}\to_{\mu}X\).

(ii) If \(X_{n}\to_{\mu}X\), then \(X_{n^{\prime}}\to_{a.e.}X\) on some subsequence \(n^{\prime}\).

(iii) If \(X_{n}\to_{r}X\), then \(X_{n}\to_{\mu}X\) and \(\{|X_{n}|^{r}:\ n\geq 1\}\) are uniformly integrable.

(iv) Let \(r\geq 1\). If \(X_{n}\to_{\mu\,{\rm or}\,a.e.}X\) and \(\overline{\lim}\,\mbox{\rm E}|X_{n}|^{r}\leq\mbox{\rm E}|X|^{r}<\infty\), then \(X_{n}\to_{r}X\).

Let \(\mu(\Omega)<\infty\). Then \(X_{n}\to_{\mu}X\) and \(\{|X_{n}|^{r}:n\geq 1\}\) are u.i. iff \(X_{n}\to_{r}X\).

(v) If \(X_{n}\to_{r}X\) and \(\mu(\Omega)<\infty\), then \(X_{n}\to_{r^{\prime}}X\) for all \(0<r^{\prime}<r\).

(vi) If \(X_{n}\to_{p}X\), then \(X_{n}\to_{d}X\).

(vii) Let \(\mu(\Omega)<\infty\). Then \(X_{n}\to_{\mu}X\) if and only if every subsequence \(\{n^{\prime}\}\) contains a further subsequence \(\{n^{\prime\prime}\}\) for which \(X_{n^{\prime\prime}}\to_{a.e.}X\).

(viii) If \(X_{n}\to_{d}X\), then \(Y_{n}\to_{a.e.}Y\) for Skorokhod rvs with \(Y_{n}\cong X_{n}\) and \(Y\cong X\).

**Proof.** See theorem 2.3.1 for (i) and (ii). Markov's inequality gives (iii) via

\[\mu(|X_{n}-X|\geq\epsilon)\leq{\rm E}|X_{n}-X|^{r}/\epsilon^{r}\to 0.\]

Vitali's theorem includes both halves of (iv). Holder's inequality gives (v) via

(a) \[{\rm E}|X_{n}-X|^{r^{\prime}}\leq\{{\rm E}|X_{n}-X|^{r^{\prime}(r/r^{\prime})} \}^{r^{\prime}/r}({\rm E1})^{1-r^{\prime}/r},\mbox{ (with ${\rm E1}=\mu(\Omega)$)};\]

note also exercise 5.1(b) and the proof of inequality 3.4.4(b). Proposition 2.4.1 gives (vi). Theorem 2.3.1 then gives (vii). The Skorokhod construction (to appear more formally as theorem 6.3.2 below) was stated above in (7); (7) gives (viii). \(\Box\)

Exercise 5.7 (Scheffe's theorem)Let \(f_{0},f_{1},f_{2},\ldots\) be \(\geq 0\) on \((\Omega,{\cal A},\mu)\).

Prove the following without resort to Vitali. Then prove them via Vitali.

(a) Suppose \(\int_{\Omega}f_{n}\,d\mu=1\) for all \(n\geq 0\), and \(f_{n}\to_{a.e.}f_{o}\) with respect to \(\mu\), then

\[\sup_{A\in A}|\int_{A}f_{n}\,d\mu-\int_{A}f_{o}\,d\mu|\leq\int_{\Omega}|f_{n}-f _{o}|\,d\mu\to 0\mbox{ as }n\to\infty. \tag{24}\]

(Think of this as the uniform convergence of measures with densities \(f_{n}\).)

(Hint. Integrate \((f_{o}-f_{n})^{+}\) and \((f_{o}-f_{n})^{-}\) separately. Note that \((f_{o}-f_{n})^{+}\leq f_{o}\).)

(b) Show that \(\overline{\lim}\int_{\Omega}f_{n}\,d\mu\leq\int_{\Omega}f_{o}\,d\mu<\infty\) and \(f_{n}\to_{\mu\mbox{ or a.e.}}f_{o}\) is sufficient for (24).

**Approximation of Functions in \({\cal L}_{r}\) by Continuous Functions\({}^{*}\)**

Let \(C_{c}\) denote the class of continuous functions on \(R\) that vanish outside a compact set, and then let \(C_{c}^{(\infty)}\) denote the subclass that has an infinite number of continuous derivatives. Let \({\cal S}_{c}\) denote the class of all _step functions_ on \(R\), where such a step function is of the form \(\sum_{1}^{m}y_{j}1_{I_{j}}\) for disjoint finite intervals \(I_{j}\). Further, let \(F\) denote a generalized df, and let \(\mu(\cdot)\equiv\mu_{F}(\cdot)\) denote the associated Lebesgue-Stieltjes measure. Let \(X\) denote a measurable function on \((\Omega,{\cal A},\mu)=(R,{\cal B},\mu_{F})\).

**Theorem 5.8** (The continuous functions are dense in \({\cal L}_{r}(R,{\cal B},\mu_{F}),r\geq 1\)): \(\,\)

Suppose throughout that \(X\in{\cal L}_{r}\), for some fixed \(1\leq r<\infty\).

(a) (Continuous functions) Then for each \(\epsilon>0\) there is a bounded and continuous function \(Y_{\epsilon}\) in \(C_{c}\) for which \(\int|X-Y_{\epsilon}|^{r}\,d\mu_{F}<\epsilon\). Thus the class \(C_{c}\) is \(\epsilon\)-dense within the class \({\cal L}_{r}\) under the \(\|\cdot\|_{r}\)-norm.

(b) We may insist that \(Y_{\epsilon}\in C_{c}^{(\infty)}\). (The \(Y_{\epsilon}\) of exercise 5.17 has \(\sup|Y_{\epsilon}|\leq\sup|X|\).)

(c) (Step functions) Such a close approximation may also be found within the step functions \({\cal S}_{c}\), making them \(\epsilon\)-dense also.

(d) All this extends to rvs on \((R_{n},{\cal B}_{n})\) (or on locally compact Hausdorff spaces).

(e) All these spaces \({\cal L}_{r}\) are separable, provided \(\mu\) is \(\sigma\)-finite and \({\cal A}\) is _countably generated_ (that is, \({\cal A}=\sigma[{\cal C}]\) with \({\cal C}\) a countable collection of sets).

**Proof.** Let \(r=1\). Consider only \(X^{+}\). Approximate it by a simple function \(X_{\epsilon}=\sum_{1}^{\kappa}x_{i}1_{A_{i}}\) of (2.2.10) so closely that \(\int|X^{+}-X_{\epsilon}|\,d\mu_{F}<\epsilon/3\). (We can require that all \(A_{i}\subset\) (some \([-M_{\epsilon},M_{\epsilon}]\)) for which \(\int_{[|X^{+}|\geq M_{\epsilon}]}x+X^{+}\,d\mu_{F}<\epsilon/3\), and that each \(x_{i}>0\).) Now, the Halmos approximation lemma of exercise 1.2.3 guarantees sets \(B_{1},\ldots,B_{n}\) made up of a finite disjoint union of intervals of the form \((a,b]\) (with \(a\) and \(b\) finite continuity points of \(F\), as in (b) of the proof of theorem 1.3.1) for which

\[\mu_{F}(A_{i}\triangle B_{i})<\epsilon/(3\kappa|x_{i}|),\qquad \quad\mbox{and so}\qquad X^{\prime}_{\epsilon}\equiv\sum_{1}^{\kappa}x_{i}1_{ B_{i}}\qquad\mbox{satisfies}\] (p) \[\int|X_{\epsilon}-X^{\prime}_{\epsilon}|\,d\mu_{F}<\epsilon/3. \qquad\quad\mbox{(note that these $B_{i}$ need not be disjoint).}\]

This \(X^{\prime}_{\epsilon}\) is the step function called for in part (c). Rewrite this \(X^{\prime}_{\epsilon}=\sum_{1}^{m}y_{j}1_{C_{j}}\) with disjoint \(C_{j}=(a_{j},b_{j}]\). Now approximate \(1c_{j}\) by the continuous function \(W_{j}\) that equals \(0\), is linear, equals \(1\) according as \(x\in[a_{j},b_{j}]^{c}\), as \(x\in[a_{j},a_{j}+\delta]\cup[b_{j}-\delta,b_{j}]\), as \(x\in[a_{j}+\delta,b_{j}-\delta]\). (We require that \(\delta\) be specified so small that the combined \(\mu_{F}\) measure of all \(2m\) sets of the type \(x\in[a_{j},a_{j}+\delta]\) and \([b_{j}-\delta,b_{j}]\) is at most \(\theta\equiv\epsilon/(6\sum_{1}^{m}y_{j})\). Then let \(Y_{\epsilon}\equiv\sum_{1}^{m}c_{j}W_{j}\), which has \(\int|X^{\prime}_{\epsilon}-Y_{\epsilon}|\,d\mu_{F}<\epsilon/3\). Thus \(\int|X-Y_{\epsilon}|\,d\mu_{F}<\epsilon\), as called for in part (a). For (b), the function \(\psi(x/\delta)\) (where

\[\psi(x)\equiv\frac{\int_{x}^{1}\exp(-1/((s(1-s)))ds}{\int_{0}^{1}\exp(-1/((s(1- s)))ds}\quad\mbox{for $0\leq x\leq 1$}, \tag{25}\]

with \(\psi(x)\) equal to \(1\) or \(0\) according as \(x\leq 0\) or \(x\geq 1\)) is easily seen to have an infinite number of continuous derivatives on \(R\) (with all said derivatives equal to \(0\) when \(x\) equals \(0\) or \(1\)). Use \(\psi(-x/\delta)\) on \([a_{j},a_{j}+\delta]\) and \(\psi(x/\delta)\) on \([b_{j}-\delta,b_{j}]\) to connect values \(0\) to \(1\), instead of linear connections. The result is a function in \(C_{c}^{(\infty)}\).

For \(r>1\), write \(X=X^{+}-X^{-}\) and use the \(C_{r}\)-inequality and \(|a-b|^{r}\leq|a^{r}-b^{r}|\) for all \(a,b\geq 0\). For example, make \({\rm E}|X^{+}-Y_{\epsilon}^{+}|^{r}\leq{\rm E}|(X^{+})^{r}-(Y_{\epsilon}^{+})^ {r}|<\epsilon\) by the case \(r=1\). (Exercise 5.18 asks for a proof of (e).) \(\square\)

**Miscellaneous Results\({}^{*}\)**

**Exercise 5.8**\({}^{*}(\rightarrow_{a.u.},\) **Egorov; convergence "almost" implies uniform convergence)**

**(i) We define** \(X_{n}\rightarrow_{a.u.}X\) **(which is used as an abbreviation for** _almost uniform convergence_**) to mean that for all** \(\epsilon>0\) **there exists an** \(A_{\epsilon}\) **with** \(\mu(A_{\epsilon})<\epsilon\) **such that** \(X_{n}\rightarrow_{\rm uniformly}X\) **on** \(A_{\epsilon}^{c}\)**. Recall (**2.3.5**) to show**

(26) \(({\bf Egorov})If\)\(\mu(\Omega)<\infty\) **and** \(X_{n}\rightarrow_{a.e.}\) **(some** \(X),\) **then** \(X_{n}\rightarrow_{a.u.}X.\)****

**If** \(|X_{n}|\leq\)****\(Y\) **a.e. for all** \(n\) **where** \(Y\in{\cal L}_{r}\) **with** \(r>1\)**, then** \(\mu(\Omega)<\infty\) **is not needed.**

**(ii) (a) If** \(X_{n}\rightarrow_{a.u.}X\)**, then both** \(X_{n}\rightarrow_{a.e.}X\) **and** \(X_{n}\rightarrow_{\mu}X\)**.**

**(b) If** \(X_{n}\rightarrow_{\mu}X\)**, then** \(X_{n^{\prime}}\rightarrow_{a.u.}X\) **on some subsequence** \(n^{\prime}\)**.**

**Exercise 5.9**: (a) **(An integrable function "almost" equals a bounded function) Suppose** \(\mu(\Omega)\in[0,\infty]\) **and** \(\int_{\Omega}|X|\,d\mu<\infty\)**. Fix** \(\epsilon>0\)**. Show the existence of a set** \(A_{\epsilon}\) **with** \(\mu(A_{\epsilon})<\infty\) **for which both** \(|X|\leq\) **(some** \(M_{\epsilon}\)**) on** \(A_{\epsilon}\) **and** \(\int_{A_{\epsilon}^{c}}|X|\,d\mu<\epsilon\)**.**

**(b) Let** \(\mu(\Omega)<\infty\)**. Let** \(X\) **be measurable and finite a.e. For any** \(\epsilon>0\)**, specify a finite number** \(M_{\epsilon}\) **and a set** \(A_{\epsilon}\) **having** \(\mu(A_{\epsilon}^{c})<\epsilon\) **and** \(|X|\leq M_{\epsilon}\) **on** \(A_{\epsilon}\)**.**

**Exercise 5.10**: **Verify Vitali's theorem 5.5(ii)(a) when** \(\mu(\Omega)=\infty\) **is allowed. (Hint. Apply exercise 5.9, Scheffe's theorem, absolute continuity of the integral, Egorov's theorem, and exercise 3.4.12.)**

**Exercise 5.11 (\(\ell_{r}\)-Spaces)** **Let** \(\Omega\) **be an arbitrary set and consider the class of all subsets** \(A\)**. Let** \(\mu(A)\) **denote the cardinality of** \(A\) **when this is finite, and let it equal** \(\infty\) **otherwise. This is** _counting measure_ **on** \(\Omega\)**. Let** \(0<r<\infty\)**. Let** \(\ell_{r}(\Omega)\) **denote all functions** \(X:\Omega\rightarrow_{R}\) **for which** \(\sum_{\omega\in\Omega}|X(\omega)|^{r}<\infty\)**. Then**

(27) \(\|X\|_{r}=\{\sum_{\omega\in\Omega}|X(\omega)|^{r}\}^{1/r},\) **for all** \(r\geq 1\)**,**

**defines a norm on** \(\ell_{r}(\Omega)\) **(see (**3.4.7**) for** \(0<r<1\)**). This is just a special case of an** \({\cal L}_{r}\)**-space, and it is important enough to deserve its specialized notation. Show that**

(28) \(\ell_{r}\subset\ell_{s}\) **for all** \(0<r<s<\infty\)**.**

**This set inclusion is proper if** \(\Omega\) **has infinitely many points.**

**Exercise 5.12**: (An added touch) Let** \(a_{n}\geq 0\) **satisfy** \(\sum_{1}^{\infty}a_{n}<\infty\)**. Show that there necessarily exists a sequence** \(c_{n}\uparrow\infty\) **for which** \(\sum_{1}^{\infty}c_{n}a_{n}<\infty\)**. The exercises below are presented for "flavor" or as tools, rather than to be worked.**

**Exercise 5.13**\({}^{*}\)**(Weak** \({\cal L}_{r}\)**-convergence; and in** \({\cal L}_{\infty}\)**)  Let** \(X_{n},X\in{\cal L}_{r}\)**, with** \(r\geq 1\)**. Let** \(1/r+1/s=1\) **define** \(s\) **for** \(r>1\)**. Let** \(s=\infty\) **when** \(r=1\)**, and** \({\cal L}_{\infty}\) **denotes all bounded** \({\cal A}\)**-measurable** \(Y\) **on** \(\Omega\)**, and let** \(\|X\|_{\infty}\equiv\inf\{c:\mu(\{\omega:|X(\omega)|>c\})=0\}\) **denote the** _essential supremum_ **of such functions** \(X\)**. Let** \(s=1\) **when** \(r=\infty\)**. (The following results can be compared with Vitali's theorem.)**

**(A) (a) Fix** \(1\leq r<\infty\)**. Let** \(X_{n}\rightarrow_{r}X\) **on** \({\cal L}(\Omega,{\cal A},\mu)\)**. Show (via the Holder inequality) that** \(X_{n}\) **converges weakly in** \({\cal L}_{r}\) **(denoted by** \(X_{n}\rightarrow_{w-{\cal L}_{r}}X\)**) in that**

(29) \(\int_{\Omega}X_{n}Y\,d\mu\rightarrow\int_{\Omega}XY\,d\mu\qquad\mbox{for all }Y\in{\cal L}_{s}\)**.**That is, \(X_{n}\to_{{\cal C}_{r}}X\) implies \(X_{n}Y\to_{{\cal L}_{1}}XY\) for all \(Y\in{\cal L}_{s}\).

(b) (Radon-Reisz) Conversely, suppose that \(X_{n}\to_{w-{\cal L}_{r}}X\) and additionally that the moments satisfy \({\rm E}|X_{n}|^{r}\to{\rm E}|X|^{r}\), where \(1<r<\infty\). Show that \(X_{n}\to_{{\cal C}_{r}}X\).

(B) (c) Let \(X_{n}\to_{\mu\,{\rm or}\,a.e.}X\). Let \(1<r<\infty\) and \(\sup_{n}{\rm E}|X_{n}|^{r}<\infty\). Show (29). (Recall Scheffe's theorem regarding \(r=1\).)

(C) (d) (Lehmann) Fix \(M\). Let \({\cal F}_{M}\equiv\{X\ :\ \|X\|_{\infty}\leq M<\infty\}\). Let \(X,X_{1},X_{2},\ldots\) denote specific functions in \({\cal F}_{M}\). Then (29) holds for all \(Y\in{\cal L}_{1}\) if and only if (29) holds for all \(Y\) in the subclass \(\{1_{A}:\mu(A)<\infty\}\). (Note also exercise 5.21 below.)

**Exercise 5.14**: (a) \({\cal L}_{\infty}(\Omega,\sigma[\{\)a1 open sets\(\}]\), \(\mu\)) is a complete metric space under the essential sup norm \(\|\cdot\|_{\infty}\) whenever \(\Omega\) is a locally compact Hausdorff space.

(b) The set \({\cal S}_{c}\) of simple functions that vanish off of compact sets is dense in this complete metric space \(({\cal L}_{\infty},\|\cdot\|_{\infty})\). (Recall theorem 5.8.)

(c) No family of continuous functions is dense in \(({\cal L}_{\infty}([0,1],{\cal B},\lambda),\|\cdot\|_{\infty})\), and the space \({\cal L}_{\infty}\) is not separable under the norm \(\|\cdot\|_{\infty}\).

**Exercise 5.15**: \({}^{*}\)(Lusin) Suppose \(X_{n}\to_{a.e.}X\), with \(\mu\) being \(\sigma\)-finite on \((\Omega,{\cal A})\). Determine a measurable decomposition \(\Omega=A_{0}+A_{1}+A_{2}+\cdots\) for which \(\mu(A_{0})=0\) and \(X_{n}\to X\) uniformly on each of \(A_{1},A_{2},\ldots\).

**Exercise 5.16**: **(Lusin; any meaurable function is "almost" continuous)**

Let \(X\) be an \((R,B)\) measurable function on \(R\).

(a) Let \(\epsilon>0\). Show that there exists a continuous function \(Y_{\epsilon}\) on \(R\) and a closed set \(D_{\epsilon}\) such that \(\lambda(D_{\epsilon}^{c})<\epsilon\) and \(X=Y_{\epsilon}\) on \(D_{\epsilon}\).

(b) Show that a function \(X:\,R\to R\) is \({\cal B}\)-measurable if and only if there exists a sequence of continuous function \(Y_{n}:\,R\to R\) for which \(Y_{n}\to_{a.e.}X\).

(Hint. (a) Begin with simple functions like those in (2.2.10). Consider each \([n,n+1]\) separately. Apply Egorov's theorem.)

**Exercise 5.17**: \({}^{*}\)(Lusin) Let \(X\) be measurable on \((\Omega,{\cal A},\mu)\), where \(\Omega\) is a locally compact Hausdorff space (every point has a neighborhood whose closure is compact, such as the real line \(R\) with the usual Euclidean metric) and \({\cal A}=\sigma[\{\)open sets\(\}]\). Suppose \(X(\omega)=0\) for all \(\omega\in A^{c}\), where \(\mu(A)<\infty\). Let \(\epsilon>0\). Then there exists \(Y_{\epsilon}\), where \(Y_{\epsilon}(\omega)=0\) for all \(\omega\in B^{c}\), with the set \(B\) compact, and where \(Y_{\epsilon}\) is continuous, \(\sup|Y_{\epsilon}|\leq\sup|X|\), and \(\mu(\{\omega:X(\omega)\neq Y_{\epsilon}(\omega)\})<\epsilon\). (Again, a measurable function is "almost equal" to a continuous function.) (Note exercise B.1.14 below.)

**Exercise 5.18**: \({}^{*}\) Prove the separability of \({\cal L}_{r}\) in theorem 5.8(e).

**Exercise 5.19**: \({}^{*}\)(Halmos) Let \((\Omega,{\cal A},\mu)\) be \(\sigma\)-finite. For each \(A_{1},A_{2}\) in \({\cal A}\) define \(\rho(A_{1},A_{2})=\mu(A_{1}\triangle A_{2})\). We agree to identify all members of the equivalence classes of subsets \(A\equiv\{A^{\prime}\) : \(\rho(A,A^{\prime})=0\). Let \({\cal A}_{0}\) denote the collection of all of these equivalence classes \(A\) that satisfy \(\mu(A)<\infty\).

(a) Show that \(({\cal A}_{0},\rho)\) is a metric space.

(b) Show that the metric space \(({\cal A}_{0},\rho)\) is separable whenever \({\cal A}=\sigma[{\cal C}]\) for some countable collection \({\cal C}\) (that is, whenever \({\cal A}\) is countably generated).

**Definition 5.4** (Dominated families of measures) Suppose that \({\cal M}\) is a family of measures \(\mu\) on some \((\Omega,{\cal A})\) having \(\mu\ll\mu_{0}\) for some \(\sigma\)-finite measure \((\Omega,{\cal A},\mu_{0})\). Denote this by \({\cal M}\ll\mu_{0}\)and say that \({\cal M}\) is _dominated_ by \(\mu_{0}\). Show that there exists a probability distribution \(P_{0}\) on \((\Omega,{\cal A})\) for which \(\mu\ll P_{0}\) for all \(\mu\in{\cal M}\); that is, for which \({\cal M}\ll P_{0}\). (Note definition 4.1.3.)

**Exercise 5.20**\({}^{*}\)(Berger) Let \({\cal P}\) denote a collection of probability measures \(P\) on the measurable space \((\Omega,{\cal A})\). Suppose \({\cal A}=\sigma[{\cal C}]\) for some countable collection \({\cal C}\); that is, \({\cal A}\) is countably generated. Let \(d_{TV}\) denote the total variation metric on \({\cal P}\); see exercise 4.2.10 below. Show that

\[{\cal P}\mbox{ is dominated}\qquad\mbox{if and only if}\qquad({\cal P},d_{TV}) \mbox{ is a separable metric space.} \tag{30}\]

(For example, let \({\cal P}\) denote all Poisson(\(\lambda\)) distributions on \(0,1,2,\ldots\) having \(\lambda>0\). The countable collection of distributions with \(\lambda\) rational is dense in \(({\cal P},d_{TV})\).)

(Hint. Use the previous exercise.)

**Exercise 5.21**\({}^{*}\)(Lehmann) Suppose that \((\Omega,{\cal A},\mu)\) is \(\sigma\)-finite and \({\cal A}\) is countably generated. Let \(\Phi\) denote the set of all \({\cal A}\)-measurable \(\phi\) for which \(0\leq\phi(\omega)\leq 1\) for all \(\omega\in\Omega\). Consider an arbitrary sequence \(\phi_{n}\in\Phi\). Show that a subsequence \(n^{\prime}\) and a function \(\phi\in\Phi\) must exist for which

\[\int_{\Omega}\phi_{n^{\prime}}f\,d\mu\to\int_{\Omega}\phi f\,d\mu\mbox{ for all }f\in{\cal L}_{1}(\Omega,{\cal A},\mu). \tag{31}\]

(That is, \(\phi_{n^{\prime}}\to_{w-{\cal L}_{1}}\phi\), or \(\phi_{n^{\prime}}\) converges weakly in \({\cal L}_{1}\) to \(\phi\) in the sense of (29)). (Hint. By exercise 5.13(d) it suffices to verify (31) for all \(f=1_{A}\) with \(\mu(A)<\infty\).)

## Chapter 4 Derivatives via Signed Measures

### 0 Introduction

In a typical calculus class the derivative \(F^{\prime}(x)\) of a function \(F\) at \(x\) is defined as the limit of the difference quotients \([F(x+h)-F(x)]/h\) as \(h\to 0\). One of the major theorems encountered is then the Fundamental Theorem of Calculus that expresses \(F\) as the integral of its derivative (with this result formulated on some interval \([a,b]\) with respect to ordinary Lebesgue measure \(d\lambda=dx\)). We can thus write \(F(x)-F(a)=\int_{a}^{x}F^{\prime}(y)\,dy\) under appropriate hypothesis on \(F\). In the context of an elementary probability class we let \(f\equiv F^{\prime}\) and rewrite the fundamental result as \(P([a,x])=\int_{[a,x]}f(y)\,dy=\int_{a}^{x}f(y)\,dy\) for all \(a\leq x\leq b\), or even as

\[P(A)=\int_{A}f(y)\,dy\qquad\text{for all events $A$ of the form}\,[a,x]. \tag{1}\]

Let us now turn this order around and begin by defining one function \(\phi\) as the "indefinite integral" of another function \(X\), and do it on an arbitrary measure space \((\Omega,\mathcal{A},\mu)\). Thus for a fixed \(X\in\mathcal{L}_{1}(\Omega,\mathcal{A},\mu)\), define

\[\phi(A)=\int_{A}Xd\mu\qquad\text{ for all $A\in\mathcal{A}$}. \tag{2}\]

As in exercise 3.2.6 (and as example 1.1 will show), if \(X\geq 0\) then this \(\phi\) is a measure on \((\Omega,\mathcal{A})\). In general \(\phi(A)\equiv\int_{A}X\,d\mu=\int_{A}X^{+}\,d\mu-\int_{A}X^{-}\,d\mu\) is the difference of two measures, and is thus called a "signed measure." As (2) suggests, we can think of \(X\) as a derivative of the signed measure \(\phi\) with respect to the measure \(\mu\). This is the so called "Radon-Nikodym derivative." In this context it is possible to formulate important general questions that have clean conclusions via straight forward and/or clever proofs. This is done in section 4.1 and section 4.2, and this gives us most of what we need as we go forward. But before going on, in section 4.3 and section 4.4 we relate this new approach back to the more familar approach represented by (1). Of course, the \(f\) in (1) must equal the Radon-Nikodym derivative (viewed in the new context); but much is gained by this new perspective.

## 1 Decomposition of Signed Measures

**Definition 1.1** (Signed measure): A _signed measure_ on a \(\sigma\)-field (or a field) \(\mathcal{A}\) is a set function \(\phi:\,\mathcal{A}\to(-\infty,+\infty]\) for which \(\phi(\emptyset)=0\) and \(\phi(\sum A_{n})=\sum\phi(A_{n})\) for all countable disjoint sequences of \(A_{n}\)'s in \(\mathcal{A}\) (requiring \(\sum A_{n}\) in \(\mathcal{A}\) in the case of a field). When additivity is required only for finite unions, then \(\phi\) is called a _finitely additive_ (f.a.) _signed measure_. (If \(\phi\geq 0\), then \(\phi\) is a measure or f.a. measure.) If \(|\phi(\Omega)|<\infty\), then \(\phi\) is called _finite_. If \(\Omega=\sum_{1}^{\infty}\Omega_{n}\) with all components \(\Omega_{n}\in\mathcal{A}\) and all values \(|\phi(\Omega_{n})|<\infty\), then \(\phi\) is called \(\sigma\)-_finite_.

**Proposition 1.1** (Elementary properties): (i) If \(\phi(A)\) is finite and \(B\subset A\), then \(\phi(B)\) is finite. Thus \(\phi(\Omega)\) finite is equivalent to \(\phi(A)\) being finite for all \(A\in\mathcal{A}\).

(ii) If \(|\phi(\sum_{1}^{\infty}A_{n})|<\infty\), then \(\sum_{1}^{\infty}|\phi(A_{n})|<\infty\) (so, it is absolutely convergent).

(\(*\)) Theorem 1.1 below will show that \(\phi(A)\geq\) (some \(M)>-\infty\), for all \(A\in\mathcal{A}\).

**Proof.** (i) Now,

(a) (a finite number) \(=\phi(A)=\phi(B)+\phi(A\,\setminus\,B)\)

implies that \(\phi(B)\) and \(\phi(A\,\setminus\,B)\) are both finite numbers.

(ii) Let \(A_{n}^{+}\) equal \(A_{n}\) or \(\emptyset\) as \(\phi(A_{n})\) is \(\geq 0\) or \(<0\). And let \(A_{n}^{-}\) equal \(A_{n}\) or \(\emptyset\) as \(\phi(A_{n})\) is \(\leq 0\) or \(>0\). Then \(\sum\phi(A_{n}^{+})=\phi(\sum A_{n}^{+})<\infty\) by (i), since \(\sum A_{n}^{+}\subset\sum A_{n}.\) Likewise, \(\sum\phi(A_{n}^{-})=\phi(\sum A_{n}^{-})\). Now, convergent series of numbers in \([0,\infty)\) may be rearranged at will. Thus \(\sum|\phi(A_{n})|=\sum\phi(A_{n}^{+})-\sum\phi(A_{n}^{-})\) is finite. \(\square\)

**Example 1.1** (The prototypical example): Let \(X\) be measurable. Then

\[\phi(A)\equiv\int_{A}X\,d\mu\mbox{ is a signed measure if }X^{-}\in\mathcal{L}_{1}. \tag{1}\]

Note that \(\phi\) is finite if \(X\in\mathcal{L}_{1}\). Also, \(\phi\) is \(\sigma\)-finite if \(X\) is a.e. finite and \(\mu\) is \(\sigma\)-finite.

**Proof.** Now, \(\phi(\emptyset)=\int_{\emptyset}X\,d\mu=\int X\cdot 1_{\emptyset}\,d\mu= \int 0\,d\mu=0\). Also,

(a) \[\phi(A)=\int_{A}X^{+}\,d\mu-\int_{A}X^{-}\,d\mu\geq-\int_{A}X^{-}\,d\mu\geq- \int_{\Omega}X^{-}\,d\mu>-\infty\]

for all \(A\in\mathcal{A}\). Finally,

\[\phi(\,\sum_{1}^{\infty}A_{n})=\int_{\Sigma A_{n}}X=\int_{\Sigma A_{n}}X^{+}- \int_{\Sigma A_{n}}X^{-}\quad\mbox{ with the $X^{-}$-term finite}\]

\[=\sum\int_{A_{n}}X^{+}-\sum\int_{A_{n}}X^{-}\qquad\mbox{ by the MCT, twice}\]

(b) \[=\sum(\int_{A_{n}}X^{+}-\int_{A_{n}}X^{-})=\sum\int_{A_{n}}X=\sum_{1}^{\infty }\phi(A_{n}).\]

Thus \(\phi\) is a signed measure.

Note that \(|\phi(A)|=|\int_{A}X|\leq\int_{A}|X|\leq\int|X|<\infty\) for all \(A\), if \(X\in\mathcal{L}_{1}\).

Let \(\Omega\equiv\sum_{n}\Omega_{n}\) be a measurable decomposition for the \(\sigma\)-finite \(\mu\). Then the sets \(\Omega_{nm}\equiv\Omega_{n}\cap[m\leq X<m+1]\) and \(\Omega_{n,\pm\infty}\equiv\Omega_{n}\cap[X=\pm\infty]\), for \(n\geq 1\) and for all integers \(m\), is a decomposition showing \(\phi\) to be \(\sigma\)-finite. \(\square\)

[MISSING_PAGE_FAIL:88]

as many times as we can, then sets with \(\phi(A_{k})\geq\frac{1}{2}\) as many times as we can,... To this end, let

(b) \[\begin{array}{l}n_{1}\equiv\min\{i:\phi(A_{1})\geq 1/i\mbox{ for some }A_{1} \subset B,\mbox{ with }A_{1}\in{\cal A}\},\\ \cdots\\ n_{k}\equiv\min\{i:\phi(A_{k})\geq 1/i\mbox{ for some }A_{k}\subset B\backslash\sum_{j=1}^{k-1}A_{j},\mbox{ with }A_{k}\in{\cal A}\}\\ \cdots\end{array}\]

(If \((n_{1},n_{2},\ldots)=(1,1,1,1,2,..\) then some \(A_{k}\) has \(\phi(A_{k})\geq 1\) for \(1\leq k\leq 4,\frac{1}{2}\leq\phi(A_{k})<1\) for \(k=5\),...) Let \(C\equiv B\backslash\sum_{k}A_{k}\), where the union is infinite (unless the process of choosing \(n_{k}\)'s terminates); note that only finitely many \(A_{k}\) exist for each \(1/i\) [else proposition 1.1(i) would be violated]. The c.a. of \(\phi\) then gives

(c) \[0>\phi(B)=\phi(C)+\sum_{k}\phi(A_{k})\geq\phi(C)>-\infty.\]

Moreover, \(C\) is a negative set, since no subset can have measure exceeding \(1/i\) for any \(i\). Now we know that we have at least one negative set. So we let

(d) \[d\equiv\inf\{\phi(C):C\mbox{ is a negative set }\}<0,\qquad\mbox{ and define }\Omega^{-}\equiv\cup_{k}C_{k},\]

where \(C_{k}\) denotes a sequence of negative sets for which \(\phi(C_{k})\searrow d\). Replace these \(C_{k}\) by \(\tilde{C}_{k}\equiv\cup_{1}^{k}C_{j}\); these are \(\nearrow\) sets with \(\Omega^{-}=\cup_{k}\tilde{C}_{k}\) and with \(\phi(\tilde{C}_{k})\searrow d\). Now, \(\Omega^{-}\) is also a negative set (else one of the \(C_{k}\)'s would not be), and thus \(\phi(\Omega^{-})\geq d\), because it must exceed the infimum of such values. But \(\phi(\Omega^{-})\leq d\) also holds, since \(\phi(\Omega^{-})=\phi(\tilde{C}_{k})+\phi(\Omega^{-}\backslash\tilde{C}_{k}) \leq\phi(\tilde{C}_{k})\) for all \(k\) gives \(\phi(\Omega^{-})\leq d\). Thus \(\phi(\Omega^{-})=d\); so, \(d\) must be finite. Then \(\Omega^{+}\) is a positive set, since if \(\phi(A)<0\) for some \(A\subset\Omega^{+}\), then the set \(\Omega^{-}\cup A\) would have \(\phi(A\cup\Omega^{-})<d\) (which is a contradiction). 

**Exercise 1.3**  The set \(\Omega^{+}\) is essentially unique, in that if \(\Omega^{+}_{1}\) and \(\Omega^{+}_{2}\) both satisfy the theorem, then \(|\phi|(\Omega^{+}_{1}\triangle\Omega^{+}_{2})=0\).

**Lebesgue Decomposition**

**Definition 1.3** (Absolute continuity of measures): Let \(\mu\) and \(\phi\) denote a measure and a signed measure on a \(\sigma\)-field \({\cal A}\). Call \(\phi\)_absolutely continuous with respect to \(\mu\)_, denoted by \(\phi\ll\mu\), if \(\phi(A)=0\) for each \(A\in{\cal A}\) having \(\mu(A)=0\). We say \(\phi\) is _singular with respect to \(\mu\)_, denoted by \(\phi\perp\mu\), if there exists a set \(N\in{\cal A}\) for which \(\mu(N)=0\) while \(|\phi|(N^{c})=0\).

**Exercise 1.4**  Let \(\mu\) be a measure and let \(\phi\) be signed measures on \((\Omega,{\cal A})\). Show that the following are equivalent: (a) \(\phi\ll\mu\). (b) \(\phi^{+}\ll\mu\) and \(\phi^{-}\ll\mu\). (c) \(|\phi|\ll\mu\).

**Theorem 1.2** (Lebesgue decomposition): Let \(\mu\) denote any \(\sigma\)-finite measure on the measurable space \((\Omega,{\cal A})\). Let \(\phi\) be any other \(\sigma\)-finite signed measure on this space \((\Omega,{\cal A})\). Then there exists a unique decomposition of \(\phi\) with respect \(\mu\) as

\[\phi=\phi_{ac}+\phi_{s}\qquad\mbox{ where }\qquad\phi_{ac}\ll\mu\quad\mbox{ and }\phi_{s}\perp\mu, \tag{7}\]

with \(\phi_{ac}\) and \(\phi_{s}\) being \(\sigma\)-finite signed measures. Moreover,

\[\phi_{ac}(A)=\int_{A}Z_{0}\,d\mu\qquad\mbox{ for all }A\in{\cal A} \tag{8}\]

for some finite \({\cal A}\)-measurable function \(Z_{0}\), which is unique a.e. \(\mu\).

**Proof.** By \(\sigma\)-finiteness and the Jordan-Hahn decomposition, we need only give the proof if \(\mu\) and \(\phi\) are finite measures; just separately consider \(\phi^{+}_{\Omega_{n}}\) and \(\phi^{-}_{\Omega_{n}}(n=1,2,\ldots)\) for a joint \(\sigma\)-finite decomposition \(\Omega=\sum_{1}^{\infty}\Omega_{n}\) of \(\mu\) and \(|\phi|\). (To give the details would be pedantic.) We now establish the existence of the decomposition in the reduced problem when \(\phi\) and \(\mu\) are finite measures. Let

(a) \[{\cal Z}\equiv\{Z:Z\geq 0,Z\in{\cal L}_{1}\mbox{ and }\int_{A}Z\,d\mu\leq\phi(A) \mbox{ for all }A\in{\cal A}\}.\]

Now, \({\cal Z}\neq\emptyset\), since \(Z\equiv 0\) is in \({\cal Z}\).

Case 1. \(\phi\ll\mu\): The first step is to observe that

(b) \[Z_{1},Z_{2}\in{\cal Z}\mbox{ implies }Z_{1}\lor Z_{2}\in{\cal Z}.\]

With \(A_{1}\equiv\{\omega\in A:Z_{1}(\omega)>Z_{2}(\omega)\}\) and \(A_{2}\equiv AA^{c}_{1}\), we have

(c) \[\int_{A}(Z_{1}\lor Z_{2})\,d\mu=\int_{A_{1}}Z_{1}\,d\mu+\int_{A_{2}}Z_{2}\,d \mu\leq\phi(A_{1})+\phi(A_{2})=\phi(A).\]

Thus (b) holds. Now choose a sequence \(Z_{n}\in{\cal Z}\) such that

(d) \[\int_{\Omega}Z_{n}\,d\mu\to c\equiv\sup_{Z\in{\cal Z}}\int_{\Omega}Z\,d\mu \leq\sup_{Z\in{\cal Z}}\phi(\Omega)\leq\phi(\Omega)<\infty.\]

We may replace \(Z_{n}\) by \(\tilde{Z}_{n}\equiv Z_{1}\vee\cdots\lor Z_{n}\) in (d). These \(\tilde{Z}_{n}\) in (d) are an \(\diagup\) sequence of functions. Then let \(Z_{0}\equiv\lim\tilde{Z}_{n}\). The MCT then gives (for any \(A\in{\cal A}\))

(e) \[\int_{A}Z_{0}\,d\mu=\lim\int_{A}\tilde{Z}_{n}\,d\mu\leq\lim\phi(A)\leq\phi(A), \quad\mbox{ so that }\quad Z_{0}\in{\cal Z},\qquad\mbox{ and }\]

(f) \[\int_{\Omega}Z_{0}\,d\mu=\lim\int_{\Omega}\tilde{Z}_{n}\,d\mu=c<\infty,\qquad \mbox{ showing that }Z_{0}\geq 0\mbox{ is a.e. finite.}\]

(Redefine \(Z_{0}\) on a null set so that it is always finite.)

We now define

(g) \[\phi_{ac}(A)\equiv\int_{A}Z_{0}\,d\mu\quad\mbox{ and }\quad\phi_{s}(A)\equiv \phi(A)-\phi_{ac}(A)\qquad\mbox{ for all }A\in{\cal A}.\]

Then \(\phi_{ac}\) is a finite measure, which can be seen by applying example 1.1 with \(c\) finite; and \(\phi_{ac}\ll\mu\). Moreover,

(h) \[\phi_{s}\equiv\phi-\phi_{ac}\geq 0\]

(since \(Z_{0}\in{\cal Z}\)), so that \(\phi_{s}\) is a finite measure by exercise 1.1. If \(\phi_{s}(\Omega)=0\), then \(\phi=\phi_{ac}\) and we are done, with \(\phi_{s}\equiv 0\). (In the next paragraph we verify that \(\phi_{s}\equiv 0\) always holds in Case 1; that is, we will verify that \(\phi_{s}(\Omega)=0\).)

\({}^{o}\)Assume \(\phi_{s}(\Omega)>0\). Then (since \(\mu(\Omega)\) is finite) there is some \(\theta>0\) for which

(i) \[\phi_{s}(\Omega)>\theta\mu(\Omega).\]

Let \(\Omega^{+}\) and \(\Omega^{-}\) denote the Jordan-Hahn decomposition for \(\phi^{*}\equiv\phi_{s}-\theta\mu\). Then

(j) \[\mu(\Omega^{+})>0\qquad\mbox{ must follow (while (i) is being assumed).}\](Assume (j) is not true, so that \(\mu(\Omega^{+})=0\). This implies \(\phi_{ac}(\Omega^{+})=\int_{\Omega+}Z_{o}\,d\mu=0.\) It further implies that \(\phi_{s}(\Omega^{+})=0\) (since \(\phi_{s}=\phi-\phi_{ac}\ll\mu\), as \(\phi\ll\mu\) is assumed for Case 1 and as \(\phi_{ac}\ll\mu\) is obvious from example 4.1.1). But \(\phi_{s}(\Omega^{+})=0\) contradicts (i) by implying that

\[\phi_{s}(\Omega)-\theta\mu(\Omega)\equiv\phi^{*}(\Omega)=-\phi^{*}(\Omega^{-}) +\phi^{*}(\Omega^{+})\]

(k) \[=-\phi^{*}(\Omega^{-})+[\phi_{s}(\Omega^{+})-\theta\mu(\Omega^{+})]=-\phi^{*}( \Omega^{-})\leq 0.\]

Thus (j) must also hold, under the assumption made above that inequality (i) holds.) Now, \(\phi_{s}(A\Omega^{+})\geq\theta\mu(A\Omega^{+})\) (by the definition of \(\Omega^{+}\) below (i)). Thus (as \(\phi_{s}\geq 0\) by (h) gives the inequality \(\phi_{s}(A\Omega^{-})\geq 0\)),

\[\phi(A)=\phi_{ac}(A)+\phi_{s}(A)=\int_{A}Z_{0}\,d\mu+\phi_{s}(A \Omega^{+})+\phi_{s}(A\Omega^{-})\] \[\geq\int_{A}Z_{0}\,d\mu+\phi_{s}(A\Omega^{+})\] \[\geq\int_{A}Z_{0}\,d\mu+\theta\mu(A\Omega^{+})\qquad\mbox{ as $\Omega^{+}$ is a positive set for $\phi^{*}\equiv\phi_{s}-\theta\mu$}\] (l) \[=\int_{A}(Z_{0}+\theta 1_{\Omega+})\,d\mu\qquad\quad\mbox{ for all $A\in{\cal A}$}.\]

This implies both \(Z_{\theta}\equiv Z_{0}+\theta 1_{\Omega+}\in{\cal Z}\) and \(\int_{\Omega}Z_{\theta}\,d\mu=c+\theta\mu(\Omega^{+})>c\). But this is a contradiction. Thus \(\phi_{s}(\Omega)=0\). Thus \(\phi\) equals \(\phi_{ac}\) and satisfies (8), and the theorem holds in Case 1. The a.s. \(\mu\) uniqueness of \(Z_{0}\) follows from exercise 3.2.2. (This also establishes the Radon-Nikodym theorem below.)

\({}^{o}\)Case 2. General \(\phi\): Let \(\nu\equiv\phi+\mu\), and note that both \(\phi\ll\nu\) and \(\mu\ll\nu\). Then by Case 1 we can infer that

(m) \[\phi(A)=\int_{A}X\,d\nu\quad\mbox{ and }\quad\mu(A)=\int_{A}Y\,d\nu\qquad\mbox{ for all $A\in{\cal A}$}\]

for finite \(\nu\)-integrable functions \(X\geq 0\) and \(Y\geq 0\) that are unique a.e. \(\nu\). Let \(D\equiv\{\omega:Y(\omega)=0\}\), and then \(D^{c}=\{\omega:Y(\omega)>0\}\). Define

(n) \[\phi_{s}(A)\equiv\phi(AD)\qquad\mbox{ and }\qquad\phi_{ac}(A)=\phi(AD^{c}).\]

Now \(\mu(D)=\int_{D}Y\,d\nu=\int_{D}0\;d\nu=0\), and (n) gives \(\phi_{s}(D^{c})=\phi(D^{c}D)=\phi(\emptyset)=0\); thus \(\phi_{s}\perp\mu\). Is \(\phi_{ac}<<\mu\)? Let \(\mu(A)=0\). Then \(\mu(AD^{c})=0\). Then by (m), \(0=\mu(AD^{c})=\int_{AD^{c}}Y\,d\nu\); and thus \(Y=0\) a.e. \(\nu\) in \(AD^{c}\), by exercise 3.2.1. But \(Y>0\) on \(AD^{c}\), and so \(\nu(AD^{c})=0\). Then (n) and (m) give \(\phi_{ac}(A)=\phi(AD^{c})=\int_{AD^{c}}X\,d\nu=0\), since \(\nu(AD^{c})=0\). So, \(\mu(A)=0\) implies \(\phi_{ac}(A)=0\). Thus \(\phi_{ac}\ll\mu\).

Consider the uniqueness of the decomposition. If \(\phi=\phi_{ac}+\phi_{s}=\bar{\phi}_{ac}+\bar{\phi}_{s}\), then \(\psi\equiv\phi_{ac}-\bar{\phi}_{ac}=\bar{\phi}_{s}-\phi_{s}\) satisfies both \(\psi\perp\mu\) and \(\psi\ll\mu\). Thus \(\psi\equiv 0\). \(\Box\)

**Exercise 1.5**  Verify the following elementary facts for signed measures \(\phi_{1},\phi_{2},\phi\) and a measure \(\mu\) on some measurable space \((\Omega,{\cal A})\).

(a) If \(\phi_{1}\ll\mu\) and \(\phi_{2}\ll\mu\), then \(\phi_{1}+\phi_{2}\ll\mu\)

(b) If \(\phi_{1}\perp\mu\) and \(\phi_{2}\perp\mu\), then \(\phi_{1}+\phi_{2}\perp\mu\)

(c) If \(\phi\ll\mu\) and \(\phi\perp\mu\), then \(\phi\equiv 0\). (This was used in the previous proof.)

## 2 The Radon-Nikodym Theorem

Recall that the absolute continuity \(\phi\ll\mu\) means that

\[\phi(A)=0\quad\mbox{whenever}\quad\mu(A)=0\mbox{ with }A\in{\cal A}. \tag{1}\]

**Theorem 2.1** (Radon-Nikodym): _Suppose both the signed measure \(\phi\) and the measure \(\mu\) are \(\sigma\)-finite on a measurable space \((\Omega,{\cal A})\). Then \(\phi\ll\mu\) if and only if there exists uniquely a.e. \(\mu\) a finite-valued \({\cal A}\)-measurable function \(Z_{0}\) on \(\Omega\) for which_

\[\phi(A)=\int_{A}Z_{0}\,d\mu\quad\mbox{ for all }A\in{\cal A}. \tag{2}\]

_Moreover, \(\phi\) is finite if and only if \(Z_{0}\) is integrable._

The function \(Z_{0}\) of (2) is often denoted by \([\frac{d\phi}{d\mu}]\) (or, more commonly by \(\frac{d\phi}{d\mu}\)), so that we also have the following very suggestive notation:

\[\phi(A)=\int_{A}\left[\frac{d\phi}{d\mu}\right]d\mu\qquad\mbox{ for all }A\in{\cal A}.\]

We call \(Z_{0}\) the _Radon-Nikodym derivative_ (or the _density_) of \(\phi\) with respect to \(\mu\).

**Proof.** The Lebesgue decomposition theorem shows that such a \(Z_{0}\) necessarily exists. The sufficiency is just the trivial example 4.1.1. The "moreover" part is also a trivial result.

**Theorem 2.2** (Change of variable theorem): _Let \(\mu\ll\nu\) where \(\mu\) and \(\nu\) are \(\sigma\)-finite measures on \((\Omega,{\cal A})\). If \(\int X\,d\mu\) has a well-defined value in \([-\infty,\infty]\), then_

\[\int_{A}X\,d\mu=\int_{A}X\left[\frac{d\mu}{d\nu}\right]\,d\nu\quad\mbox{ for all }A\in{\cal A}, \tag{3}\]

_One useful special case results from_

\[\int_{a}^{b}f\ dG=\int_{a}^{b}fg\ dH,\quad\mbox{ with }G\equiv\int_{a}g\ dH\mbox{ for a generalized df }H, \tag{4}\]

_where \(g\geq 0\) on \((a,b]\) is measurable, and where we agree that \(\int_{a}^{b}\equiv\int_{(a,b]}\)._

**Proof.** Case 1. \(X=1_{B}\), for \(B\in{\cal A}\): Then the Radon-Nikodym theorem gives

(a) \[\int_{A}1_{B}\,d\mu=\mu(AB)=\int_{AB}[\frac{d\mu}{d\nu}]\,d\nu=\int_{A}1_{B}[ \frac{d\mu}{d\nu}]\,d\nu.\]

Case 2. \(X=\sum_{1}^{n}c_{i}1_{B_{i}}\), for a partition \(B_{i}\): Case 1 and linearity of the integral give

(b) \[\int_{A}X\,d\mu=\sum_{i=1}^{n}c_{i}\int_{A}1_{B_{i}}\,d\mu=\sum_{1}^{n}c_{i} \int_{A}1_{B_{i}}\,[\frac{d\mu}{d\nu}]\,d\nu=\int_{A}X[\frac{d\mu}{d\nu}]\,d\nu.\]

Case 3. \(X\geq 0\): Let \(X_{n}\geq 0\) be simple functions that \(\nearrow\) to \(X\). Then the MCT twice gives

(c) \[\int_{A}X\,d\mu=\lim\int_{A}X_{n}\,d\mu=\lim\int_{A}X_{n}[\frac{d\mu}{d\nu}]\, d\nu=\int_{A}X[\frac{d\mu}{d\nu}]\,d\nu.\]Case 4. \(X\) measurable and at least one of \(X^{+},X^{-}\) in \({\cal L}_{1}\): Then

\[\int_{A}X\,d\mu=\int_{A}X^{+}\,d\mu-\int_{A}X^{-}\,d\mu\]

(d) \[=\int_{A}X^{+}[\frac{d\mu}{d\nu}]\,d\nu-\int_{A}X^{-}[\frac{d\mu}{d\nu}]\,d\nu= \int_{A}X[\frac{d\mu}{d\nu}]\,d\nu,\]

so long as one of \(\int_{A}X^{+}\,d\mu\) and \(\int_{A}X^{-}\,d\mu\) is finite. \(\Box\)

**Exercise 2.1** (Derivative of a sum, and a chain rule): Let \(\mu\) and \(\nu\) be \(\sigma\)-finite measures on \((\Omega,{\cal A})\). Let \(\phi\) and \(\psi\) be \(\sigma\)-finite signed measures on \((\Omega,{\cal A})\). Then

\[\left[\frac{d(\phi+\psi)}{d\mu}\right]=\left[\frac{d\phi}{d\mu}\right]+\left[ \frac{d\psi}{d\mu}\right]\ \mbox{a.e. }\mu\qquad\mbox{ if }\phi\ll\mu\mbox{ and }\psi\ll\mu, \tag{5}\]

\[\left[\frac{d\phi}{d\nu}\right]=\left[\frac{d\phi}{d\mu}\right]\cdot\left[\frac {d\mu}{d\nu}\right]\ \mbox{a.e. }\nu\qquad\qquad\mbox{ if }\phi\ll\mu\mbox{ and }\mu\ll\nu. \tag{6}\]

Show that \([\frac{d\mu}{d\nu}]=1/[\frac{d\omega}{d\mu}]\) holds a.e. \(\mu\) and a.e. \(\nu\) if \(\mu\ll\nu\) and \(\nu\ll\mu\).

Note that theorem 3.2.6 (of the unconscious statistician) is another change of variable theorem. That is, if \(X:(\Omega,{\cal A})\to(\bar{\Omega},\bar{\cal A})\) and \(g:(\bar{\Omega},\bar{\cal A})\to(\bar{R},\bar{\cal B})\), then

\[\int_{(g\circ X)^{-1}(B)}g(X)d\,\mu=\int_{g^{-1}(B)}g\,d\mu_{X}=\int_{B}y\,d\mu _{g(X)}(y)\qquad\mbox{ for all }B\in{\cal B}, \tag{7}\]

when one of the these integrals is well-defined. (See also exercise 6.3.3 below.)

**Exercise 2.2**: Let \(P_{\mu,\sigma^{2}}\) denote the \(N(\mu,\sigma^{2})\) distribution. Let \(P\) have the density \(f\equiv[dP/d\lambda]\) with respect to Lebesgue measure \(\lambda\) for which \(f>0\).

(a) Show that \(\lambda\ll P\) with density \(1/f\).

(b) Show that \(P_{\mu,1}\ll P_{0,1}\) and compute \([dP_{\mu,1}/dP_{0,1}]\).

(c) Show that \(P_{0,\sigma^{2}}\ll P_{0,1}\) and compute \([dP_{0,\sigma^{2}}/dP_{0,1}]\).

(d) Compute \([dP/dP_{0,1}]\) and \([dP_{0,1}/dP]\) when \(P\) denotes the Cauchy distribution.

**Exercise 2.3**: Flip a coin. If heads results, let \(X\) be a Uniform(0, 1) outcome; but if tails results, let \(X\) be a Poisson (\(\lambda\)) outcome. The resulting distribution on \(R\) is labeled \(\phi\).

(a) Let \(\mu\) denote Lebesgue measure on \(R\). Find the Lebesgue decomposition of \(\phi\) with respect to this \(\mu\); that is, write \(\phi=\phi_{ac}+\phi_{s}\).

(b) Let \(\mu\) be counting measure on \(\{0,1,2,\ldots\}\). Find the Lebesgue decomposition of \(\phi\) with respect to this \(\mu\).

(If need be, see the definitions of various distributions in chapter 9.)

**Exercise 2.4**: Let \(\mu\) be a \(\sigma\)-finite measure on \((\Omega,{\cal A})\). Define \(\phi(A)\equiv\int_{A}X\,d\mu\) for all \(A\in{\cal A}\) for some \(\mu\)-integrable function \(X\). Show that

\[|\phi|(A)=\int_{A}|X|\,d\mu\quad\mbox{ for all }A\in{\cal A}.\]

**Exercise 2.5** (Alternative definition of absolute continuity) Let \(\phi\) be finite and let \(\mu\) be \(\sigma\)-finite, for measures on \((\Omega,{\cal A})\). Then \(\phi\ll\mu\) if and only if for every \(\epsilon>0\) there exists \(\delta_{\epsilon}>0\) such that \(\mu(A)<\delta_{\epsilon}\) implies \(|\phi|(A)<\epsilon\). Show that if \(\phi\) is not finite, then the claim could fail (even if \(\mu\) is a finite measure); give an example.

**Exercise 2.6** (Domination) If \(\mu_{1},\mu_{2},\ldots\) are finite measures on some \((\Omega,{\cal A})\), then there exists a finite measure \(\mu\) on \((\Omega,{\cal A})\) such that \(\mu_{k}\ll\mu\) for each \(k\geq 1\).

**Exercise 2.7** (Halmos) Suppose \(\mu_{1},\mu_{2},\ldots\) and \(\nu_{1},\nu_{2},\ldots\) are finite measures on some \((\Omega,{\cal A})\) for which \(\mu_{k}\ll\nu_{k}\) for each \(k\geq 1\). Suppose also that

\[\mu(A)=\lim_{n\to\infty}\sum_{k=1}^{n}\mu_{k}(A)\quad\mbox{ and }\quad\nu(A)= \lim_{n\to\infty}\sum_{k=1}^{n}\nu_{k}(A)\]

for all \(A\in{\cal A}\). Show that the following hold a.e. \(\nu\):

\[\begin{array}{l}[d(\sum_{k=1}^{n}\nu_{k})/d\nu]\nearrow 1\quad\mbox{ and }\quad[d(\sum_{k=1}^{n}\mu_{k})/d\nu]\nearrow[d\mu/d\nu],\qquad\mbox{ and }\\ [d\sum_{k=1}^{n}\mu_{k}/d\sum_{k=1}^{n}\nu_{k}]\to[d\mu/d\nu_{k}].\end{array} \tag{8}\]

These can be thought of as theorems about Radon-Nikodym derivatives, about absolute continuity of measures, or about change of variables.

**Exercise 2.8** Let \({\cal A}\) denote the collection of all subsets \(A\) of an uncountable set \(\Omega\) for which either \(A\) or \(A^{c}\) is countable. Let \(\mu(A)\) denote the cardinality of \(A.\) Define \(\phi(A)\) to equal \(0\) or \(\infty\) according as \(A\) is countable or uncountable. Show that \(\phi\ll\mu\). Then show that the Radon-Nikodym theorem fails in this non \(\sigma\)-finite case.

**Exercise 2.9** For a \(\sigma\)-finite measure \(\mu\) and a finite measure \(\nu\) on \((\Omega,{\cal A})\), let

\[\phi(A)\equiv\mu(A)-\nu(A)\qquad\mbox{ for all }A\in{\cal A}.\]

(a) Show that \(\phi\) is a signed measure. (b) Show that

\[\phi(A)=\int_{A}(f-g)\,d(\mu+\nu),\]

for measurable functions \(f\) and \(g\) with \(g\in{\cal L}_{1}^{+}(\mu+\nu)\). (Note example 4.1.1.)

(c) Determine \(\phi^{+},\phi^{-},\) and \(|\phi|\); and determine \(|\phi|(\Omega)\) in case \(\mu\) is also a finite measure.

**Exercise 2.10** (Total variation distance between probability measures) Define \(P\) and \(Q\) to be probability measures on \((\Omega,{\cal A})\).

(a) Show that the _total variation distance_\(d_{TV}(P,Q)\) between \(P\) and \(Q\) satisfies

\[d_{TV}(P,Q)\equiv\{\sup_{A\in A}|P(A)-Q(A)|\}=\frac{1}{2}\int|p-q|\,d\mu \tag{9}\]

for any \(\sigma\)-finite measure \(\mu\) dominating both \(P\) and \(Q\) (that is, \(P\ll\mu\) and \(Q\ll\mu\)).

(b) Use part (a) to show that \(d_{TV}(P,Q)=|P-Q|(\Omega)/2\).

(c) Note specifically that the choice of dominating measure \(\mu\) does not affect the value of \(d_{TV}(P,Q)\). (Note section 14.2 below.)

**Exercise 2.11** (Hellinger distance between probability measures): Let \(P\) and \(Q\) denote probability measures on \((\Omega,{\cal A})\). Define the _Hellinger distance_\(H(P,Q)\) by

\[H^{2}(P,Q)\equiv\frac{1}{2}\int[\sqrt{p}-\sqrt{q}]^{2}\,d\mu \tag{10}\]

for any measure \(\mu\) dominating both \(P\) and \(Q\). Show that the choice of dominating measure \(\mu\) does not affect the value of \(H(P,Q)\). (Note section 14.2 below.)

**Exercise 2.12**: Let \(\phi\) be a \(\sigma\)-finite signed measure. Define

\[\int Xd\phi=\int Xd\phi^{+}-\int Xd\phi^{-} \tag{11}\]

when this is finite. Show that \(|\int Xd\phi|\leq\int|X|d|\phi|\).

**Exercise 2.13**: Let \((\Omega,{\cal A})\) be a measurable space, and let \({\cal M}\) denote the collection of all finite signed measures \(\mu\) on \((\Omega,{\cal A})\). Let \(\|\mu\|\equiv|\mu|(\Omega)\). Thus \(\|\mu_{1}-\mu_{2}\|=|\mu_{1}-\mu_{2}|(\Omega)\). Show that \(({\cal M},\|\cdot\) is a complete metric space.

**Exercise 2.14** *(Loeve): Suppose \(X_{1},X_{2},\ldots\) are integrable on \((\Omega,{\cal A},\mu)\). Define \(\phi_{n}(A)\equiv\int_{A}X_{n}\,d\mu\) for all \(A\in{\cal A}\), and suppose \(\phi_{n}(A)\) converges to a finite number for all \(A\in{\cal A}\). Define \(|\phi|_{n}(A)\equiv\int_{A}|X_{n}|\,d\mu\). Then \(\sup_{n}|\phi|_{n}(\Omega)<\infty\). Moreover,

\[\sup_{n}|\phi|_{n}(A)\to 0\qquad\mbox{ as either }\mu(A)\to 0\mbox{ or }A \searrow\emptyset. \tag{12}\]

Finally, there exists an integrable function \(X\) (that is unique a.e. \(\mu\)) for which

\[\phi_{n}(A)\to\phi(A)\equiv\int_{A}X\,d\mu\qquad\mbox{ for all }A\in{\cal A}. \tag{13}\]

**Exercise 2.15** (Likelihood ratios): Let \(P\) and \(Q\) denote any two measures on some \(\sigma\)-finite measure space \((\Omega,{\cal A},\mu)\). Suppose that \(P<<\mu\) and \(Q<<\mu\). Show that

\[\frac{[dP/d\mu]}{[dQ/d\mu]}=\frac{[dP/d(P+Q)]}{[dQ/d(P+Q)} \tag{14}\]

except on a \(\mu\)-null set (which is also a \((P+Q)\)-null set). This means that the "likelihood ratio" on the right hand side of (14) (that appears in various statistical settings) can always be replaced by the one on the left hand side. (In a statistical situation where \(P\) and \(Q\) are probability measures, the right hand side is always defined--even if, say, \(P\) is absolutely continuous and \(Q\) is discrete.)

## 3 Lebesgue's Theorem

**Theorem 3.1** (Lebesgue): (a) Suppose \(F\) is any \(\nearrow\) function on \([a,b]\). Then \(F\) has an integrable derivative \(F^{\prime}\) that exists and is finite a.e. \(\lambda\) on \([a,b]\).

(b) If \(F\) is \(\nearrow\) on \(R\), then \(F^{\prime}\) exists a.e. \(\lambda\) on \(R\) and is integrable on any finite \([a,b]\).

**Proof.** : \({}^{*}\)Consider the _Dini derivates_

\[D^{+}F(x) \equiv\limsup_{h\to 0+}[F(x+h)-F(x)]/h,\] \[D^{-}F(x) \equiv\limsup_{h\to 0+}[F(x)-F(x-h)]/h,\] \[D_{+}F(x) \equiv\liminf_{h\to 0+}[F(x+h)-F(x)]/h,\] \[D_{-}F(x) \equiv\liminf_{h\to 0+}[F(x)-F(x-h)]/h.\]

Trivially, \(D^{+}F(x)\geq D_{+}F(x)\) and \(D^{-}F(x)\geq D_{-}F(x)\). All four derivates having the same finite value is (of course) the definition of \(F\) being _differentiable_ at \(x\), with the common value called the derivative of \(F\) at \(x\) and being denoted by \(F^{\prime}(x)\). Let

\[A \equiv\{x:D^{+}F(x)>D_{-}F(x)\}\] (a) \[\equiv\bigcup_{r,s}A_{rs}\equiv\bigcup_{r,s}\{x:D^{+}F(x)>s>r>D_{ -}F(x)\},\]

where the union is over all rational \(r\) and \(s\). To show that \(\lambda(A)=0\), it suffices to show that all \(A_{rs}\) have outer Lebesgue measure zero, in that \(\lambda^{*}(A_{rs})=0\). To this end, let \(U\) be an open set for which \(A_{rs}\subset U\) with \(\lambda(U)<\lambda^{*}(A_{rs})+\epsilon\). For each \(x\in A_{rs}\) we can specify infinitely many and arbitrarily small \(h\) for which \([x-h,x]\subset U\) and \([F(x)-F(x-h)]/h<r\). This collection of closed intervals covers \(A_{rs}\) in the sense of Vitali (see exercise 1.2.8). Thus some finite disjoint collection of them has interiors \(I_{1}\equiv(x_{1}-h_{1},x_{1}),\ldots,I_{m}\equiv(x_{m}-h_{m},x_{m})\) for which \(B_{rs}\equiv A_{rs}\cap(\sum_{i=1}^{m}I_{i})\) has \(\lambda^{*}(B_{rs})>\lambda^{*}(A_{rs})-\epsilon\). Then

(b) \[\sum_{i=1}^{m}[F(x_{i})-F(x_{i}-h_{i})]<r\sum_{i=1}^{m}h_{i}\leq r\lambda(U)<r[ \lambda^{*}(A_{rs})+\epsilon].\]

For each \(y\in B_{rs}\) we can specify infinitely many and arbitrarily small \(h\) for which \([y,y+h]\subset(\mbox{some }I_{i})\) and \([F(y+h)-F(y)]/h>s\). This collection covers \(B_{rs}\) in the sense of Vitali. Thus some finite disjoint collection of them has interiors \(J_{1}\equiv(y_{1},y_{1}+h_{1}),\ldots,J_{n}\equiv(y_{n},y_{n}+h_{n})\) for which \(C_{rs}\equiv B_{rs}\cap(\sum_{j=1}^{n}J_{j})\) has \(\lambda^{*}(C_{rs})>\lambda^{*}(B_{rs})-\epsilon\). Then

(c) \[\sum_{j=1}^{n}[F(y_{j}+h_{j})-F(y_{j})]>s\sum_{j=1}^{n}h_{j}\geq s[\lambda^{*}( B_{rs})-\epsilon]>s[\lambda^{*}(A_{rs})-2\epsilon].\]

Moreover, since the disjoint union of the \(J_{j}\)'s is a subset of the disjoint union of the \(I_{i}\)'s, results (b) and (c) yield

\[r[\lambda^{*}(A_{rs})+\epsilon]>\sum_{i=1}^{m}[F(x_{i})-F(x_{i}-h_{i})]\](d) \(\geq\sum_{j=1}^{n}[F(y_{j}+h_{j})-F(y_{j})]>s[\lambda^{*}(A_{rs})-2\epsilon]\)

for every \(\epsilon>0\). That is, \(r\lambda^{*}(A_{rs})\geq s\lambda^{*}(A_{rs})\). But \(r<s\). Thus \(\lambda^{*}(A_{rs})=0\) for all rational \(r\) and \(s\). Thus \(\lambda^{*}(A)=0\). Analogously, \(\lambda(\{x:D^{-}F(x)>D_{+}F(x)\})=0.\) So \(D^{-}F(x)\geq D_{-}F(x)\geq D^{+}F(x)\geq D_{+}F(x)\geq D^{-}F(x)\) a.e. \(\lambda\). Thus \(F^{\prime}\) exists a.e. \(\lambda\).

_Important_: _Read this paragraph_. The measurable function difference quotients

(e) \(D_{n}F(x)\equiv n[F((x+1/n)\wedge b)-F(x)]\)

on \([a,b]\) converge a.e. \(\lambda\) to \(F^{\prime}(x)\) on \([a,b]\), so that \(F^{\prime}(x)\) is measurable. Applying Fatou's lemma to the \(D_{n}F\) (which are \(\geq 0\), since \(F\) is \(\nearrow\)) gives

(f) \(\int_{a}^{b}F^{\prime}(x)d\lambda(x)=\int_{a}^{b}[\underline{\lim}\,D_{n}F(x)] d\lambda(x)\quad\leq\underline{\lim}\int_{a}^{b}D_{n}F(x)d\lambda(x)\)

(g) \(=\underline{\lim}\int_{a}^{b}n[F((x+1/n)\wedge b)-F(x)]d\lambda(x)\)

(h) \(=\underline{\lim}\,[\int_{b}^{b+1/n}nF(b)d\lambda(x)-\int_{a}^{a+1/n}nF(x)d \lambda(x)]\)

(i) \(\leq\underline{\lim}\{F(b)-F(a+1/n)\}\leq F(b)-F_{+}(a)\leq F(b)-F(a),\quad\) as is \(F\nearrow.\)

Thus \(F^{\prime}\) is integrable, and hence \(F^{\prime}\) is also finite a.e. \(\lambda\). (We now present this last fact as a corollary, since _situations with strict inequality are very revealing_.) \(\Box\)

**Corollary 1** (a) Suppose \(F\) is an \(\nearrow\) function on \([a,b]\), with \(-\infty\leq a\leq b\leq\infty.\) Then \(F^{\prime}\) exists a.e. \(\lambda\) and

(1) \(\int_{a}^{b}F^{\prime}(x)d\lambda(x)\leq F(b)-F(a).\)

So, \(F\) is differentiable a.e. \(\lambda\), and its derivative \(F^{\prime}\) is finite a.e. \(\lambda\) and satisfies (1).

(b) If \(F\) is any df on \(R\), then \(F^{\prime}\) exists a.s. \(\lambda\) on \(R\). So let \(a\) and \(b\) become infinite in (1) to obtain \(\int_{\infty}^{\infty}F^{\prime}(x)d\lambda(x)\leq F(\infty)-F(-\infty)=1-0=1.\)

The Lebesgue singular df in example 6.1.1 below will show that equality need not hold in equation (1); this continuous df is constant valued on a collection of disjoint intervals of total length 1. (An example in Hewitt and Stromberg (1965, p. 278) shows that \(F^{\prime}(x)=0\) is possible for a.e. \(x\), even with a \(\uparrow F\).) Equality also fails for any discrete distribution that places any mass in \((a,b]\). This is the point of the following exercise.

**Exercise 3.1** (Distributions can be discrete, singular, or absolutely continuous)

(a) Let \(F_{D}\) denote the discrete distribution on \([0,1]\) that puts mass \(1/(n+1)\) at each of the \(n+1\) points \(i/n\) for \(0\leq i\leq n\). Graph \(F_{D}\), and calculate \(F^{\prime}_{D}\) from your graph. Then show that \(\int_{0}^{1}F^{\prime}_{D}(x)d\lambda(x)=0\).

(b) Let \(F_{C}\) denote the Lebesgue Singular df associated with the Cantor set; see example 6.1.1 below. What is the value of \(F^{\prime}_{C}\) at all points interior to the "flat spots"? Now show that \(\int_{0}^{1}F^{\prime}_{C}(x)d\lambda(x)=0\).

(c) Show that \(\int_{-\infty}^{\infty}F^{\prime}(x)d\lambda(x)=1\) for the Uniform \((0,1)\) df--which has a density.

**Theorem 3.2**\({}^{o}\)(Term-by-term differentiation of series) Let \(g_{k}\) be \(\nearrow\) on \([a,b]\) for each \(k\geq 1\), and suppose that \(S_{n}(x)\equiv\sum_{k=1}^{n}g_{k}(x)\) converges at \(x=a\) and \(x=b\). Then \(S_{n}(x)\to S(x)\) for all \(x\) in \([a,b]\), for some finite-valued measurable function \(S(x)\). Mainly, \(S^{\prime}(\cdot)\) exists a.s. \(\lambda\) and is given by

\[S^{\prime}(x)=\sum_{k=1}^{\infty}g^{\prime}_{k}(x).\]

**Corollary 1** If the power series \(S(x)\equiv\sum_{n=1}^{\infty}a_{n}(x-a)^{n}\) converges absolutely for \(x=a+R\), then for all \(|x-a|<R\) we may differentiate \(S(x)\) term by term. Moreover, this is true for any number of derivatives of \(S\).

**Proof.** Note that \(S_{n}(a)\) is a convergent sum. Now write

\[S_{n}(x)=S_{n}(a)+[S_{n}(x)-S_{n}(a)]=S_{n}(a)+\sum_{k=1}^{n}[g_{k}(x)-g_{k}(a )].\]

Since \(\nearrow\) sequences bounded above converge, the convergence at \(x=a\) and \(x=b\) gives convergence at all \(x\) in the interval. We may replace \(g_{k}(x)-g_{k}(a)\) by \(g_{k}(x)\) and then assume all \(g_{k}\geq 0\) on \([a,b]\) with \(g_{k}(a)=0\). Since \(S\) and all \(S_{n}\) are \(\nearrow\), the derivatives \(S^{\prime}\) and all \(S^{\prime}_{n}\) exist a.e. \(\lambda\) by theorem 3.1 (of Lebesgue). Now,

(a) \[S^{\prime}_{n}(x)\leq S^{\prime}_{n+1}(x)\leq S^{\prime}(x)\mbox{ a.e. }\lambda;\]

both essentially follow from

(b) \[\frac{S(x+h)-S(x)}{h}=\frac{S_{n}(x+h)-S_{n}(x)}{h}+\sum_{n+1}^{\infty}\frac{g _{k}(x+h)-g_{k}(x)}{h}\]

\[\geq\frac{S_{n}(x+h)-S_{n}(x)}{h}.\]

From (a) we see (without having made use of \(g_{k}(a)=0\)) that

(c) \[S^{\prime}_{n}(\cdot)\mbox{ converges a.e. }\lambda\mbox{ with }\lim S^{\prime}_{ n}\leq S^{\prime}\mbox{ a.e. }\lambda.\]

Because \(S^{\prime}_{n}\nearrow\), it suffices to show that \(S^{\prime}_{n_{i}}\to_{a.e.}S^{\prime}\) for some subsequence \(n_{i}\). Since \(S_{n}(b)\nearrow S(b)\), we may specify \(n_{i}\) so large that \(0\leq S(b)-S_{n_{i}}(b)<2^{-i}\), and then

(d) \[0\leq S(x)-S_{n_{i}}(x)=\sum_{n_{i}+1}^{\infty}g_{k}(x)\leq\sum_{n_{i}+1}^{ \infty}g_{k}(b)=S(b)-S_{n_{i}}(b)<2^{-i},\]

for all \(x\in[a,b]\). Thus

(e) \[0\leq\sum_{i=1}^{\infty}[S(x)-S_{n_{i}}(x)]\leq\sum_{i=1}^{\infty}2^{-i}=1 \qquad\mbox{ for all }x\in[a,b],\]

where the series in (e) has summands

(f) \[h_{i}(x)\equiv S(x)-S_{n_{i}}(x)\qquad\mbox{ that are }\nearrow\mbox{ in }x.\]

Thus conclusion (c) also applies to these \(h_{i}\)'s (not just the \(g_{k}\)'s), and we conclude from (c) that the series

(g) \[T^{\prime}_{n}\equiv\sum_{i=1}^{n}h^{\prime}_{i}\qquad\mbox{ converges a.e. }\lambda.\]But a series of real numbers can converge only if its _n_th term goes to 0; that is,

(h) \[S^{\prime}(x)-S^{\prime}_{n_{i}}(x)=h^{\prime}_{i}(x)\to 0\qquad\text{ a.e. }\lambda.\]

As noted above, this suffices for the theorem. 

**Exercise 3.2**  Prove the corollary.

**Example 3.1** (Taylor's expansion)  Suppose \(g(\cdot)\) is defined in a neighborhood of \(a\). Let \(x^{*}\) denote a point somewhere between \(x\) and \(a\). Let

\[P_{1}(x) \equiv g(a)+g^{\prime}(a)(x-a), \tag{4}\] \[P_{2}(x) \equiv P_{1}(x)+g^{\prime\prime}(a)(x-a)^{2}/2!,\] (5) \[P_{3}(x) \equiv P_{2}(x)+g^{\prime\prime\prime}(a)(x-a)^{3}/3!,\ldots,\] (6) \[R_{1}(x) \equiv[g(x)-g(a)]/(x-a)\qquad\text{ or }\;\;g^{\prime}(a),\qquad \qquad\text{ as }\;\;\;x\neq a\text{ or }\;x=a,\] (7) \[R_{2}(x) \equiv 2![g(x)-P_{1}(x)]/(x-a)^{2}\;\;\;\text{ or }\;\;g^{\prime\prime}(a),\qquad\qquad\text{ as }\;\;\;x\neq a\text{ or }\;x=a,\] (8) \[R_{3}(x) \equiv 3![g(x)-P_{2}(x)]/(x-a)^{3}\;\;\;\text{ or }\;\;g^{\prime\prime\prime}(a),\qquad\qquad\text{ as }\;\;\;x\neq a\text{ or }\;x=a. \tag{3}\]

Then l'Hospital's rule gives (provided \(g^{\prime}(a),g^{\prime\prime}(a),g^{\prime\prime\prime}(a),\ldots\) exist, respectively)

\[\lim_{x\to a}R_{1}(x) =g^{\prime}(a)=R_{1}(a), \tag{10}\] \[\lim_{x\to a}R_{2}(x) =\lim_{x\to a}\frac{g^{\prime}(x)-P^{\prime}_{1}(x)}{x-a}=\lim_{x\to a }\frac{g^{\prime}(x)-g^{\prime}(a)}{x-a}=g^{\prime\prime}(a)=R_{2}(a),\] \[\lim_{x\to a}R_{3}(x) =\lim_{x\to a}\frac{2![g^{\prime}(x)-P^{\prime}_{2}(x)]}{(x-a)^{2}}= \lim_{x\to a}\frac{g^{\prime\prime}(x)-P^{\prime\prime}_{2}(x)}{x-a}\] (11) \[=\lim_{x\to a}\frac{g^{\prime\prime}(x)-g^{\prime\prime}(a)}{x-a}=g ^{\prime\prime\prime}(a)=R_{3}(a). \tag{9}\]

Thus we find it useful to use the representations (with \(g^{(k)}(a)\) abbreviating that \(g^{(k)}(\cdot)\) exists at \(a\), and with \(g^{(k)}(\cdot)\) abbreviating that \(g^{(k)}(x)\) exists for all \(x\) in a neighborhood of _a_)

\[g(x) =\left\{\begin{array}{ll}P_{1}(x)+[R_{1}(x)-g^{\prime}(a)](x-a)& \text{ if }g^{\prime}(a),\\ P_{1}(x)+[g^{\prime}(x^{*})-g^{\prime}(a)](x-a)&\text{ if }g^{\prime}(\cdot),\\ \end{array}\right. \tag{13}\] \[g(x) =\left\{\begin{array}{ll}P_{2}(x)+[R_{2}(x)-g^{\prime\prime}(a)] (x-a)/2!&\text{ if }g^{\prime\prime}(a),\\ P_{2}(x)+[g^{\prime\prime}(x^{*})-g^{\prime\prime}(a)](x-a)^{2}/2!&\\ =P_{1}(x)+g^{\prime\prime}(x^{*})(x-a)^{2}/2!&\text{ if }g^{\prime}(\cdot),\\ \end{array}\right.\] (14) \[g(x) =\left\{\begin{array}{ll}P_{3}(x)+[R_{2}(x)-g^{\prime\prime}(a )](x-a)^{3}/3!&\text{ if }g^{\prime\prime\prime}(a),\\ P_{3}(x)+[g^{\prime\prime\prime}(x^{*})-g^{\prime\prime\prime}(a)](x-a)^{3}/3!& \\ =P_{2}(x)+g^{\prime\prime\prime}(x^{*})(x-a)^{3}/3!&\text{ if }g^{\prime}(\cdot). \qed\end{array}\right. \tag{12}\]

**Exercise 3.3** (a) Show that if \(g^{\prime\prime}(x)\) exists, then

\[g^{\prime\prime}(x)=\lim_{h\to 0}\frac{1}{h^{2}}\{g(x+h)-2g(x)+g(x-h)\}. \tag{15}\]

(b) An analogous result holds for any \(g^{(2k)}(x)\).

**Exercise 3.4***  Prove the Vitali covering theorem. (See exercise 1.2.8.)

**Exercise 3.5** Let \(f(x)=\sum_{0}^{\infty}a_{k}x^{k}/\sum_{1}^{\infty}b_{k}x^{k}\) in some interval. Suppose that all \(a_{k},b_{k}>0\) and \(a_{k}/b_{k}\uparrow\). Then \(f^{\prime}(x)>0\) for all \(x\) in that interval. (This result is useful in conjunction with the monotone likelihood ratio principle.)

## 4 The Fundamental Theorem of Calculus

**Definition 4.1** (Bounded variation): Let \(F\) denote a real-valued function on \([a,b]\). The _total variation_ of \(F\) over \([a,b]\) is defined by

\[V_{a}^{b}F\equiv V_{[a,b]}F \tag{1}\] \[\equiv\sup\left\{\sum\limits_{k=1}^{n}|F(x_{k})-F(x_{k-1})|:a \equiv x_{0}<x_{1}<\cdots<x_{n}\equiv b,n\geq 1\right\}.\]

We say that \(F\) is of _bounded variation_\((BV)\) on \([a,b]\) if \(V_{a}^{b}F<\infty\). (Note the \(V_{a}^{b}F\) is a measure of the "total amount of wiggle" of \(F\) over \([a,b]\).)

It is clear that

\[V_{a}^{b}F=V_{a}^{c}F+V_{c}^{b}F\qquad\mbox{ for }a\leq c\leq b\mbox{ and }F\mbox{ of BV}. \tag{2}\]

**Definition 4.2** (Absolutely continuous functions): A real-valued function \(F\) on any subinterval \(I\) of the line \(R\) is said to be _absolutely continuous_ if for all \(\epsilon>0\) there exists a \(\delta_{\epsilon}>0\) such that

\[\sum_{k=1}^{n}|F(d_{k})-F(c_{k})|<\epsilon\qquad\mbox{ whenever }\qquad\sum_{k=1}^{n}(d_{k}-c_{k})<\delta_{\epsilon} \tag{3}\]

with \(n\geq 1\) and with disjoint subintervals \((c_{k},d_{k}]\) contained in \(I.\) (This implies that the "wiggle" of \(F\) over a combined length must be small if \(F\) is not given much combined length in which to wiggle.)

**Definition 4.3** (Lipschitz condition): A real-valued function \(F\) on any subinterval \(I\) of \(R\) is said to be _Lipschitz_ if for some finite constant \(M\) we have

\[|F(y)-F(x)|\leq M|y-x|\quad\mbox{ for all }x\mbox{ and }y\mbox{ in }I. \tag{4}\]

We first establish some elementary relationships among the Lipschitz condition, absolute continuity, bounded variation, and the familiar property of being \(\nearrow\). These concepts have proven to be important in the study of differentiation. We will soon proceed further in this direction, and we will also consider the relationship between ordinary derivatives and Radon-Nikodym derivatives. We first recall from theorem 1.3.1 (the correspondence theorem) that every generalized df \(F\) can be associated with a Lebesgue-Stieltjes measure \(\mu_{F}\) via \(\mu_{F}((a,b])\equiv F(b)-F(a)\).

**Proposition 4.1** (The basics): Let \(\lambda\) denote Lebesgue measure.

(i) If \(F\) is of BV on \([a,b]\), then

\[F(x)=F_{1}(x)-F_{2}(x)\quad\mbox{ with }F_{1}(x)\equiv V_{a}^{x}F\mbox{ and }F_{2}(x)\equiv V_{a}^{x}F-F(x) \tag{5}\]

both being \(\nearrow\) on \([a,b]\). Also, \(F^{\prime}=F^{\prime}_{1}-F^{\prime}_{2}\)  a.e. \(\lambda\), with \(F^{\prime}_{1}\) and \(F^{\prime}_{2}\) both integrable.

(ii) If \(F\) is absolutely continuous, then it is of BV. The \(F_{1}\) and \(F_{2}\) in (i) are both absolutely continuous and \(\nearrow\).

(iii) Lipschitz functions are absolutely continuous.

**Proof.** Consider (i). Now, \(F_{1}(x)=V_{a}^{x}F\) is obviously \(\nearrow\); use (2). Then \(F_{2}\) is also \(\nearrow\), since for \(x\leq y\) we have

\[F_{2}(y)-F_{2}(x)=[V_{a}^{y}F-F(y)]-[V_{a}^{x}F-F(x)]\](a) \[=V_{x}^{y}F-[F(y)-F(x)]\geq 0.\]

Since \(F_{1}\) and \(F_{2}\) are \(\nearrow\), their derivatives \(F_{1}^{\prime}\) and \(F_{2}^{\prime}\) exist a.e. \(\lambda\) and are integrable by theorem 4.3.1 (Lebesgue's theorem).

Consider (ii). Let \(F(\cdot)\) be absolutely continuous. We will first show that such an \(F\) is of BV. Let \(\epsilon=1\) with its \(\delta_{1}\), and choose \(n\) so large that the equally spaced values \(a\equiv x_{0}<x_{1}<\cdots<x_{n}\equiv b\) have mesh \(\equiv(b-a)/n<\delta_{1}\). Then (2) yields

(b) \[V_{a}^{b}F=\sum_{k=1}^{n}V_{[x_{k-1},x_{k}]}F\leq\sum_{k=1}^{n}1=n;\]

and thus \(F\) is of BV. We must still show that \(F_{1}\) is absolutely continuous if \(F\) is. So we suppose that \(F\) is absolutely continuous, and specify that \(\sum_{1}^{n}(d_{k}-c_{k})<\delta_{\epsilon/2}\) for some choice of \(n,c_{k}\)'s, and \(d_{k}\)'s. We now use these same \(n,c_{k},d_{k}\) to verify that \(F_{1}\) is absolutely continuous. Well, for each fixed \(k\) with \(1\leq k\leq n\) and the tiny number \(\epsilon/(2n)\), the definition of the BV of \(F\) gives

(c) \[F_{1}(d_{k})-F_{1}(c_{k})=V_{[c_{k},d_{k}]}F<\sum_{j=1}^{m_{k}}|F(a_{n,k,j})-F( a_{n,k,j-1})|+(\epsilon/2n)\]

for some choice of \(c_{k}\equiv a_{n,k,0}<\cdots<a_{n,k,m_{k}}\equiv d_{k}\). These add to give

\[\sum_{k=1}^{n}|F_{1}(d_{k})-F_{1}(c_{k})|=\sum_{k=1}^{n}|V_{[a,d_{k}]}F-V_{[a, c_{k}]}F|=\sum_{k=1}^{n}V_{[c_{k},d_{k}]}F\]

(d) \[\leq\sum_{k=1}^{n}(\sum_{j=1}^{m_{k}}|F(a_{n,k,j})-F(a_{n,k,j-1})|+(\epsilon/2n))\]

(e) \[\leq(\epsilon/2)+(\epsilon/2)=\epsilon\]

by absolute continuity of \(F\), since it follows from above that

(f) \[\sum_{k=1}^{n}\sum_{j=1}^{m_{k}}(a_{n,k,j}-a_{n,k,j-1})=\sum_{k=1}^{n}(d_{k}-c _{k})<\delta_{\epsilon/2}.\]

Consider (iii). Being Lipschitz implies absolute continuity with \(\delta_{\epsilon}=\epsilon/M\). \(\Box\)

**Exercise 4.1** (Jordan-Hahn type analog of proposition 4.1i) For \(F\) of BV on \([a,b]\), let \(F^{+}(x)\equiv V_{a}^{x}F^{+}\) and \(F^{-}(x)\equiv V_{a}^{x}F^{-}\), where

\[V_{a}^{x}F^{\pm}\equiv\sup\{\sum_{k=1}^{n}[F(x_{k})-F(x_{k-1})]^{\pm}:a\equiv x _{0}<\cdots<x_{n}\equiv x,n\geq 1\}.\]

Verify that \(F-F(a)=F^{+}-F^{-}\) with \(F^{+}\) and \(F^{-}\) both \(\nearrow\) (an alternative to (5)). (Note how the \(F_{1}\) of proposition 4.1i corresponds to the total variation measure.)

Example: Let \(F(x)\) equal \(x,2-x,x-4\) on \([0,1]\), [1, 3], [3, 4]. Determine the decomposition of (5) for this \(F\), as well as the decomposition of this exercise.

**Exercise 4.2** Let \(f\) be continuous on \([a,b]\), and define \(F(x)=\int_{a}^{x}f(y)\,dy\) for each \(a\leq x\leq b\). Then \(F\) is differentiable at each \(x\in(a,b)\) and \(F^{\prime}=f\) on \((a,b)\). (Since \(f\) is continuous, we need only the Riemann integral. Can we extend this to the Lebesgue integral? Can we reverse the order, and first differentiate and then integrate? The next theorem answers these important questions.)

**Theorem 4.1** (Fundamental theorem of calculus): (i) Let \(F\) be absolutely continuous on \([a,b]\), and let \(\lambda\) denote Lebesgue measure. Then \(F^{\prime}\) exists a.e. \(\lambda\) and

\[F(x)-F(a)=\int_{a}^{x}F^{\prime}d\lambda\qquad\mbox{ for all }x\in[a,b];\quad \mbox{ also, }\quad F^{\prime}=\left[\frac{d\mu_{F}}{d\lambda}\right]\mbox{ a.e. }\lambda. \tag{6}\]

(ii) If \(F(x)-F(a)=\int_{a}^{x}f\,d\lambda\) for some \(f\) that is integrable with respect to \(\lambda\) on \([a,b]\), then \(F\) is absolutely continuous on \([a,b]\). Moreover, \(f=F^{\prime}=[\frac{d\mu_{F}}{d\lambda}]\) a.e. \(\lambda\).

**Remark 4.1** (a) The fundamental theorem of calculus can be summarized by saying that \(F\) is absolutely continuous if and only if it is the integral of its derivative. The ordinary derivative \(F^{\prime}\) is, in fact, also a Radon-Nikodym derivative of the signed measure \(\mu_{F}\) naturally associated with \(F\); see the proof of theorem 4.2 below.

(b) If \(F\) is \(\diagup\) on \([a,b]\), then the derivative \(F^{\prime}\) exists a.e. \(\lambda\) on \([a,b]\) and is integrable with respect to Lebesgue measure \(\lambda\) and \(\int_{a}^{b}F^{\prime}(x)d\lambda(x)\leq F(b)-F(a)\); see (4.3.1). The Lebesgue singular df \(F\) of (6.1.9) below yields a strict inequality, as does any discrete dstribution on \((a,b]\); recall exercise 4.3.2.

(c) The Lipschitz condition represents "niceness with a vengeance," as it guarantees that all difference quotients are uniformly bounded. \(\Box\)

**Proof.** Consider the converse. If \(F(x)\equiv F(a)+\int_{a}^{x}f(y)\,dy\) for \(a\leq x\leq b\), then \(F\) is absolutely continuous by the absolute continuity of the integral theorem. Then \(F\) is of bounded variation on \([a,b]\) and \(F^{\prime}\) exists a.e. \(\lambda\) in \([a,b]\), by proposition 4.1(ii). Moreover, \(F^{\prime}\) is integrable, using (4.3.1). But does \(F^{\prime}=f\) a.e. \(\lambda\)?

Case 1: Suppose \(|f|\) is bounded by some finite \(M\) on \([a,b]\). We could consider \(f^{+}\) and \(f^{-}\) separately, but we will simply assume without loss of generality that \(f\geq 0\). Then the difference quotient \(D_{n}F(x)\equiv n\int_{x}^{x+1/n}f(y)\,dy\) of \(F\) also satisfies \(|D_{n}F|\leq M\) on \((a,b)\), and \(D_{n}F(x)\to F^{\prime}(x)\) a.e. Applying the DCT (with dominating function identically equal to \(M\)) once for each fixed \(x\in(a,b)\) gives

\[\int_{a}^{x}F^{\prime}(y)\,dy=\int_{a}^{x}\lim D_{n}F(y)\,dy=\lim\int_{a}^{x} n[F(y+1/n)-F(y)]dy\]

\[=\lim[n\int_{x}^{x+1/n}F(y)\,dy-n\int_{a}^{a+1/n}F(y)\,dy]\]

\[=F(x)-F(a)\qquad\mbox{ by continuity of }F\]

(a) \[=\int_{a}^{x}f(y)\,dy.\]

Thus \(F^{\prime}(y)=f(y)\) a.e. on \([a,b]\), by the a.e. uniqueness of the Radon-Nikodym derivative (which is ultimately exercise 3.2.2).

Case 2: Suppose \(f\) is integrable. Again, \(f\geq 0\) may be assumed. Let \(f_{n}(\cdot)\equiv n\wedge f(\cdot)\), with \(f-f_{n}\geq 0\). Now, \(\int_{a}^{x}f_{n}\) has derivative \(f_{n}\) a.e. on \([a,b]\), by case 1. Thus

(b) \[F^{\prime}(x)=\frac{d}{dx}\int_{a}^{x}f(y)\,dy=\tfrac{d}{dx}\int_{a}^{x}f_{n}(y )\,dy+\tfrac{d}{dx}\int_{a}^{x}[f(y)-f_{n}(y)]dy\geq f_{n}(x)+0\]

a.e. \(\lambda\) on \([a,b]\) (for each \(n\geq 1\)). Hence \(F^{\prime}(x)\geq f(x)\) a.e. on \([a,b]\). Thus

(c) \[\int_{a}^{b}F^{\prime}(x)\,dx\geq\int_{a}^{b}f(x)\,dx=F(b)-F(a),\mbox{ which is }\geq\int_{a}^{b}F^{\prime}(x)\,dx\]

by (4.3.1). The two inequalities in (c) combine to give

(d) \[\int_{a}^{b}[F^{\prime}(x)-f(x)]\,dx=0\qquad\mbox{ with }F^{\prime}(x)-f(x)\geq 0\mbox{ a.e.,}\]

so that \(F^{\prime}=f\) a.e. on \([a,b]\) by exercise 3.2.1.

Consider the direct half when \(F\) is absolutely continuous on \([a,b]\). Without loss, suppose that \(F\) is \(\nearrow\) (by proposition 4.1(ii)), so that \(F^{\prime}\) exists a.e. on \([a,b]\) (see theorem 4.3.1) and that \(F^{\prime}\) is integrable (see (4.3.1)). Use

(e) \[\mu_{F}((a,x])\equiv F(x)-F(a)\quad\mbox{ for all }x\in[a,b]\]

and the correspondence theorem to associate a Lebesgue-Stieltjes measure \(\mu_{F}\) with \(F\) (which is a generalized df). We will show that \(\mu_{F}<<\lambda\) in theorem 4.2 below. Then the Radon-Nikodym will give

(f) \[F(x)-F(a)=\int_{a}^{x}fd\lambda\quad\mbox{ for all }x\in[a,b],\qquad\mbox{ with }f\equiv[d\mu_{F}/d\lambda].\]

Now apply the converse half of the fundamental theorem of calculus to conclude that \(F^{\prime}=f\equiv[d\mu_{F}/d\lambda]\) a.e. on \([a,b]\). \(\Box\)

**Theorem 4.2** (Densities): (i) Let \(F\) be \(\nearrow\) and absolutely continuous on some subinterval \([a,b]\) of \(R\). Then the Lebesgue-Stieltjes measure \(\mu_{F}\) (as in (e) above) satisfies both \(\mu_{F}\ll\lambda\) (name its Radon-Nikodym derivative \([d\mu_{F}/d\lambda]\)), and also

\[F(x)-F(a)=\int_{a}^{x}fd\lambda\mbox{ for all }x\in[a,b],\qquad\qquad\mbox{ with }f\equiv[d\mu_{F}/d\lambda]=_{a.e.}F^{\prime}. \tag{7}\]

(ii) Mainly, let \(F\) be absolutely continuous on every finite subinterval of \(I\) (with \(I\) any fixed interval in \(R\)), and fix \(a\) anywhere in \(I\). Then (7) holds for all \(x\) in \(I\).

**Proof.** (i) Let \(\mu\equiv\mu_{F}\) and fix the finite interval \([a,b]\). Given \(\epsilon>0\), let \(\delta_{\epsilon}>0\) be as in the definition (3) of absolute continuity. Let \(A\in{\cal B}\) be a subset of \([a,b]\) having \(\lambda(A)<\delta_{\epsilon}/2\). Recalling our definition (1.2.1) of \(\lambda\) via Caratheodory coverings, we can claim that Lebesgue measure satisfies

(a) \[\lambda(A)=\inf\{\sum_{n=1}^{\infty}\lambda(B_{n}):A\subset\sum_{n=1}^{\infty }B_{n}\mbox{ for }B_{n}\mbox{'s in the field }C_{F}\}.\]

We replaced the \(A_{n}\)'s of (1.2.1) by the disjoint sets \(B_{n}\equiv A_{n}A_{n-1}^{c}\cdots A_{2}^{c}A_{1}^{c}\). Note that each \(B_{n}\) is in \(C_{\cal F}\), and thus equals a finite union of intervals of the type \((c,d]\), while \(B_{n+1}\) then adds at most a finite number of additional such intervals. Thus

(b) \[A\subset\sum_{1}^{\infty}(c_{n},d_{n}],\mbox{ where }\sum_{1}^{\infty}(d_{n}-c_{n})<\lambda(A)+\delta_{\epsilon}/2<\delta_{\epsilon}\]Thus (using absolutely continuity of \(F\) to obtain the first \(\epsilon\) in (d))

(c) \[\mu_{F}(A)\leq\mu_{F}(\sum_{1}^{\infty}(c_{n},d_{n}])=\sum_{1}^{\infty}\mu_{F}((c _{n},d_{n}])=\sum_{1}^{\infty}[F(d_{n})-F(c_{n})]\] (d) \[=\lim_{m}\sum_{1}^{m}[F(d_{n})-F(c_{n})]\leq\lim_{m}\epsilon=\epsilon,\]

since \(\sum_{1}^{m}(d_{n}-c_{n})<\delta_{\epsilon}\). Thus \(\mu_{F}(A)<\epsilon\) when \(\lambda(A)<\delta_{\epsilon}/2\), so that \(\mu_{F}(A)=0\) whenever \(\lambda(A)=0\). Now apply Radon-Nikodym to obtain the \(f\) exhibited in (7).

(ii) Now, let \(F\) be \(\nearrow\) and absolutely continuous on all finite subintervals of \(I\), and fix any \(a\) in the interval \(I\). Applying (7) to finite intervals \(I_{n}\equiv[a_{n},b_{n}]\nearrow I\), the MCT gives \(\mu_{F}(A)=0\) whenever \(\lambda(A)=0\), for any \(A\in{\cal B}\cap I\). Thus \(\mu_{F}\ll\lambda\). \(\Box\)

**Exercise 4.3** (Absolutely continuous dfs): Let \(F\) be \(\nearrow\), right continuous and bounded on \(R\), with \(F(-\infty)=0\). Define \(\mu_{F}\) via \(\mu_{F}((a,b])=F(b)-F(a)\) for all \(a<b\). Show that \(\mu_{F}\ll\lambda\) if and only if \(F\) is an absolutely continuous function on \(R\).

**Exercise 4.4**: (a) Show that the composition \(g(h)\) of two absolutely continuous functions is absolutely continuous when \(h\) is monotone.

(b) Show that \(g(h)\) need not be absolutely continuous without restrictions on \(h\).

(c) Define a continuous function on \([0,1]\) that is not absolutely continuous.

(d) The functions \(g+h\) and \(g\cdot h\) are absolutely continuous when both \(f\) and \(g\) are.

**Exercise 4.5***: Suppose that \(h:[a,b]\to(0,\infty)\) is absolutely continuous on \([a,b].\) Show that \(\log h\) is also absolutely continuous on \([a,b].\)

**Exercise 4.6***: (Another characterization of absolute continuity)

(a) \(F\) is Lipschitz on \([a,b]\) iff \(F\) is differentiable a.e. \(\lambda\) on \([a,b]\) with \(F^{\prime}\) bounded.

(b) Absolutely continuous functions on \(R\) map \({\cal B}\) into \({\cal B}\) and null sets into null sets.

(c) A continuous function of BV is absolutely continuous iff it maps \({\cal B}\) into \({\cal B}\).

**Example 4.1** (Change of variable; densities of transformed rvs): Let \(X\) be a rv on \((\Omega,{\cal A},P)\) with df \(F_{X}\ll\lambda\equiv\) (Lebesgue measure) and density \(f_{X}\). Let

\[Y\equiv g(X)\mbox{ where }g^{-1}\mbox{ is }\uparrow\mbox{ and absolutely continuous.} \tag{8}\]

Then

\[F_{Y}(y)\equiv P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_{X}(g^{-1}(y)),\]

where the composition \(F_{Y}=F_{X}(g^{-1})\) of these absolutely continuous functions is absolutely continuous (by exercise 4.4a). So the fundamental theorem of calculus tells us that \(F_{Y}\) is the integral of its derivative. We can then compute this derivative from the ordinary chain rule. Thus

\[F_{Y}(b)-F_{Y}(a)=\int_{a}^{b}F^{\prime}_{Y}(r)d\lambda(r)=\int_{a}^{b}[F^{ \prime}_{X}(g^{-1}(r))\frac{d}{dr}g^{-1}(r)]d\lambda(r)\]

for all \(a\leq b\). Thus \(F_{Y}\ll\lambda\) with density

\[f_{Y}(y)=f_{X}(g^{-1}(y))\frac{d}{dy}g^{-1}(y) \tag{9}\]

on the real line. Call \((d/dy)g^{-1}(y)\) the _Jacobian_ of the transformation.

**Exercise 4.7** Let \(\equiv\log 1/X\) where the rv \(X\) is distributed uniformly on \([0,1]\), with df \(F_{X}(x)\) equal to \(0,x,1\) according as \(x\) is in \((-\infty,x],(0,1],(1,\infty)\). We say that \(X\) has the _Uniform\((0,1)\)_ distribution. Determine the df and density of \(Y\equiv\log 1/X.\) It is called the Exponential\((1)\) distribution.

**Exercise 4.8** Use your exposure to a more elementary version of probability to come up with three more examples of elementary change of variable results of the type presented in the previous exercise.

**Exercise 4.9** (Specific step functions that are dense in \({\cal L}_{2}\)) Let \(h\in{\cal L}_{2}([0,1],{\cal B},\lambda)\). Consider the following two approximations to \(h(\cdot)\). Let

\[\bar{h}_{m}(t)\equiv m\int_{(i-1)/m}^{i/m}h(s)ds\mbox{ and }\check{h}_{m}(t) \equiv h(i/(m+1))\]

for \((i-1)/m<t\leq i/m\) and \(m\geq 1\). Show that:

\[\bar{h}_{m}\to h\mbox{ a.s. \ and \ \ }{\cal L}_{2}. \tag{10}\]

\[\check{h}_{m}\to h\mbox{ a.s. \ and }{\cal L}_{2}\mbox{ provided that }h\mbox{ is }\nearrow. \tag{11}\]

(Hint. Show that \(0\leq\int_{0}^{1}(\bar{h}_{m}-h)^{2}dt=\int_{0}^{1}(h^{2}-\bar{h}_{m}^{2})dt\), and then

\[\bar{h}_{m}(t) =m(i/m-t)\{\int_{t}^{i/m}h\,ds/(i/m-t)\}\] \[\quad+m(t-(i-1)/m)\{\int_{(i-1)/m}^{t}h\,ds/(t-(i-1)/m)\}\to h(t) \mbox{ a.s.}\]

Alternatively, use the fact that the continuous functions are dense in \({\cal L}_{2}\).)

## Chapter 5 Measures and Processes on Products

### 1 Finite-Dimensional Product Measures

**Definition 1.1** (Product spaces): Let \((\Omega,\mathcal{A})\) and \((\Omega^{\prime},\mathcal{A}^{\prime})\) be measurable spaces. Define

\[\mathcal{A}\times\mathcal{A}^{\prime}\equiv\sigma[\mathcal{F}]\text{ where } \mathcal{F}\equiv\left\{\sum_{i=1}^{m}(A_{i}\times A^{\prime}_{i}):m\geq 1,\ A_{i} \in\mathcal{A}\text{ and }A^{\prime}_{i}\in\mathcal{A}^{\prime}\right\}, \tag{1}\]

\[\mathcal{F}_{0}\equiv\{A\times A^{\prime}:A\in\mathcal{A}\text{ and}A^{\prime}\in\mathcal{A}^{\prime}\}. \tag{2}\]

Here \(A\times A^{\prime}\equiv\{(\omega,\omega^{\prime})\ :\ \omega\in A,\ \omega^{ \prime}\in A^{\prime}\}\), which is called a _measurable rectangle_. The \(\sigma\)-field \(\mathcal{A}\times\mathcal{A}^{\prime}\equiv\sigma[\mathcal{F}]\) is called the _product \(\sigma\)-field_. \((\Omega\times\Omega^{\prime},\mathcal{A}\times\mathcal{A}^{\prime})\) is called the _product measurable space_. The sets \(A\times\Omega^{\prime}\) and \(\Omega\times A^{\prime}\) are called _cylinder sets_.

**Proposition 1.1**: \(\mathcal{F}\) is a field.

See figure 1.1, and write the displayed union as a union of sets disjoint in \(\mathcal{F}_{0}\). Perhaps, start with decomposing \(\Omega\) into the \(2^{2}\) disjoint sets \(B_{1}\cap\cdots\cap B_{4}\), where each \(B_{i}\) equals \(A_{i}\) or \(A_{i}^{c}\) (and with \(2^{2}\) analogous \(B^{\prime}_{1}\cap\cdots\cap B^{\prime}_{4}\)). Then sum disjoint subsets from the \(2^{4}\) possible cross products.

**Theorem 1.1** (Existence of the product measure): Let \((\Omega,\mathcal{A},\mu)\) and \((\Omega^{\prime},\mathcal{A}^{\prime},\nu)\) be \(\sigma\)-finite measure spaces. Define \(\phi\) on the field \(\mathcal{F}\) via

\[\phi\left(\sum_{i=1}^{m}(A_{i}\times A^{\prime}_{i})\right)=\sum_{i=1}^{m}\mu( A_{i})\times\nu(A^{\prime}_{i})\text{ for disjoint sets }A_{i}\times A^{\prime}_{i}. \tag{3}\]Then \(\phi\) is a well-defined and \(\sigma\)-finite measure on the field \(\mathcal{F}\). Moreover, \(\phi\) extends uniquely to a \(\sigma\)-finite measure, called the _product measure_ and also denoted by \(\phi\), on \((\Omega\times\Omega^{\prime},\mathcal{A}\times\mathcal{A}^{\prime})\). Even when completed, this measure is still unique and is still referred to as the product measure \(\phi\).

**Proof.** (See the following exercise; it mimicks the proof of the correspondence theorem. Here, \(\mathcal{F}_{0}\) and \(\mathcal{F}\) play the roles of all finite intervals \(\mathcal{I}\) and the field \(\mathcal{C}_{F}.\) Although the proof asked for in exercise 1.1 below is "obvious," it still requires much tedious detail.) We will give a better proof herein very soon. \(\square\)

**Exercise 1.1**  Verify that \(\phi\) is well-defined on \(\mathcal{F}_{0}\), and that \(\phi\) is countably additive on \(\mathcal{F}_{0}\). Then verify that \(\phi\) is well-defined on \(\mathcal{F}\), and that \(\phi\) is countably additive on \(\mathcal{F}\). Thus \(\phi\) is a \(\sigma\)-finite measure on \(\mathcal{F}\), so that the conclusion of theorem 1.1 follows from the Caratheodory extension of theorem 1.2.1 and its corollary.

**Exercise 1.2** \({}^{*}\) Use induction to show that theorem 1.1 extends to \(n\)-fold products.

**Example 1.1** (Lebesgue measure in \(n\) dimensions, etc.): (a) We define

\[(R_{n},\mathcal{B}_{n})=\prod\nolimits_{i=1}^{n}(R,\mathcal{B})\qquad\text{ and}\qquad(\bar{R}_{n},\bar{\mathcal{B}}_{n})\equiv\prod\nolimits_{i=1}^{n}(\bar{R}, \bar{\mathcal{B}})\]

to be the \(n\)-fold products of the real line \(R\) with the Borel sets \(\mathcal{B}\) and of the extended real line \(\bar{R}\) with the \(\sigma\)-field \(\bar{\mathcal{B}}\equiv\sigma[\mathcal{B},\{+\infty\},\{-\infty\}]\), respectively. Recall from example 2.1.1 that \(\mathcal{B}_{n}=\sigma[\mathcal{U}_{n}]\), where \(\mathcal{U}_{n}\) denotes all open subsets of \(R_{n}\). We will refer to both \(\mathcal{B}_{n}\) and \(\bar{\mathcal{B}}_{n}\) as the _Borel sets_.

(b) Let \(\lambda\) denote Lebesgue measure on \((R,\mathcal{B})\), as usual. We extend \(\lambda\) to \((\bar{R},\bar{\mathcal{B}})\) by the convention that \(\lambda(\{+\infty\})=0\) and \(\lambda(\{-\infty\})=0\). Then

\[(R_{n},\mathcal{B}_{n},\lambda_{n})\equiv\prod\nolimits_{i=1}^{n}(R,\mathcal{ B},\lambda)\quad\text{and}\quad(\bar{R}_{n},\bar{\mathcal{B}}_{n},\lambda_{n}) \equiv\prod\nolimits_{i=1}^{n}(\bar{R},\bar{\mathcal{B}},\lambda)\]

provides us with a definition of \(n\)-_dimensional Lebesgue measure_\(\lambda_{n}\) as the natural generalization of the concept of volume. It is clear that

\[(R_{m}\times R_{n},\mathcal{B}_{m}\times\mathcal{B}_{n},\lambda_{m}\times \lambda_{n})=(R_{m+n},\mathcal{B}_{m+n},\lambda_{m+n}),\]

and that this holds on the extended Euclidean spaces as well. (It is usual not to add the \({}^{\wedge}\) symbol in dealing with the completions of these particular measures.)

(c) Now, \(\lambda\) is just a particular Lebesgue-Stieltjes measure on \((R,\mathcal{B})\). Any Lebesgue-Stieltjes measure \(\mu_{F}\) on \((R,\mathcal{B})\) or \((\bar{R},\bar{\mathcal{B}})\) yields an obvious \(n\)-fold product on either \((R_{n},\mathcal{B}_{n})\) or \((\bar{R}_{n},\bar{\mathcal{B}}_{n})\), which could appropriately be denoted by \(\mu_{F}\times\cdots\times\mu_{F}.\) Further, we will let \(\mathcal{F}_{n}\) denote the field consisting of all finite disjoint unions of sets of the form \(I_{1}\times\cdots\times I_{n}\) where each \(I_{k}\) is of the form \((a,b],\ (-\infty,b]\) or \((a,+\infty)\) when considering \((R_{n},\mathcal{B}_{n})\) (or of the form \((a,b],\ [-\infty,b],\) or \((a,+\infty]\) when considering \((\bar{R}_{n},\bar{\mathcal{B}}_{n})\)). (That is, in the case of \((R_{n},\mathcal{B}_{n})\) there is the alternative field \(\mathcal{F}_{n}\) that also generates the \(\sigma\)-field \(\mathcal{B}_{n}\); and this \(\mathcal{F}_{n}\) is made up of simpler sets than is the field \(\mathcal{B}\times\cdots\times\mathcal{B}\) used in definition 1.1.)

(d) The Halmos approximation lemma now shows that if \((\mu_{F}\times\cdots\times\mu_{F})(A)<\infty\) and if \(\epsilon>0\) is given, then \((\mu_{F}\times\cdots\times\mu_{F})(A\triangle C_{\epsilon})<\epsilon\) for some \(C_{\epsilon}\) in (the simpler field) \(\mathcal{F}_{n}\). That is, the simpler field gives us a nicer conclusion in this example, because its sets \(C\) are simpler. (Or, use \(A_{\epsilon}\) in the field \(\mathcal{F}\) of (1) in place of \(C_{\epsilon}\).) \(\square\)

**Exercise 1.3**  Any Lebesgue-Stieltjes measure \(\mu_{F_{1}}\times\cdots\times\mu_{F_{n}}\) on \((R_{n},\mathcal{B}_{n})\), in example 1.1, is a regular measure. Show this for \(n=2\) (appeal to theorem 1.3.1).

**Definition 1.2** (Sections): (a) Let \(X\) denote a function on \(\Omega\times\Omega^{\prime}\). For each \(\omega\) in \(\Omega\), the function \(X_{\omega}(\cdot)\) on \(\Omega^{\prime}\) defined by \(X_{\omega}(\omega^{\prime})\equiv X(\omega,\omega^{\prime})\) for each \(\omega^{\prime}\) in \(\Omega^{\prime}\) is called an \(\omega\)-_section_ of \(X(\cdot,\cdot)\). An \(\omega^{\prime}\)-_section_\(X_{\omega^{\prime}(\cdot)}\) of \(X(\cdot,\cdot)\) is defined analogously.

(b) Let \(C\) be a subset of \(\Omega\times\Omega^{\prime}\). For each \(\omega\) in \(\Omega\), the set \(C_{\omega}=\{\omega^{\prime}:(\omega,\omega^{\prime})\) is in \(C\}\) is called the \(\omega\)-_section_ of \(C\). An \(\omega^{\prime}\)-_section_ of \(C\) is defined analogously.

**Theorem 1.2** (Product measure): Let \((\Omega,\mathcal{A},\mu)\) and \((\Omega^{\prime},\mathcal{A}^{\prime},\nu)\) denote finite measure spaces. Let \(C\in\mathcal{A}\times\mathcal{A}^{\prime}\). Then:

(6) Every

\[C_{\omega^{\prime}}\in\mathcal{A}\]

 and every

\[C_{\omega}\in\mathcal{A}^{\prime}\]

 whenever

\[C\in\mathcal{A}\times\mathcal{A}^{\prime}\]

, (7) \[\phi(C)\equiv\int_{\Omega^{\prime}}\mu(C_{\omega^{\prime}})\,d\nu(\omega^{ \prime})=\int_{\Omega}\nu(C_{\omega})\,d\mu(\omega)\]

 for every

\[C\in\mathcal{A}\times\mathcal{A}^{\prime}\]

,

and this \(\phi\) is exactly the product measure \(\phi=\mu\times\nu\) of theorem 1.1.

**Exercise 1.4** (An elementary illustration): Let \(C\) denote the set in \(2\)-dimensional space \(R_{2}\) enclosed by the two curves \(y=x\) and \(y=x^{2}\) on \([0,1]\). Draw a picture of \(C\) on an ordinary \((x,y)\)-axes system that represents \(R_{2}\).

(I)(a) First, use ordinary introductory calculus to evaluate the area of \(C\).

(II) Answer the following in the context of the Product Measure Theorem.

(b) Evaluate (and describe--possibly, via a picture) the sections \(C_{x}\) and \(C_{y}\)--for the appropriate measures \(\mu\) and \(\nu\). Then valuate \(\nu(C_{x})\) and \(\mu(C_{y})\).

(c) Use the results of (b) to evaluate the area of \(C\) in two different ways.

**Proof.** We first show (6). This result is trivial for any \(C\) in \(\mathcal{F}_{0}\), or any \(C\) in \(\mathcal{F}\) (and for \(C^{\mathcal{C}}\)). Now let \(\mathcal{S}\) denote the class of all sets \(C\) in \(\mathcal{A}\times\mathcal{A}^{\prime}\) for which (6) is true. Then \(\mathcal{S}\) is trivially seen to be a \(\sigma\)-field, using

(a) \[(\cup_{n}C_{n})_{\omega^{\prime}}=\cup_{n}C_{n,\omega^{\prime}}\] and \[(C^{c})_{\omega^{\prime}}=(C_{\omega^{\prime}})^{c}\].

But since \(\mathcal{F}\subset\mathcal{S}\), we have that \(\mathcal{A}\times\mathcal{A}^{\prime}=\sigma[\mathcal{F}]\) equals \(\mathcal{S}\).

Consider (7). Note that if the sets \(C_{n}\) converge monotonically to some set \(C\), then \(1_{C_{n}}\) converges monotonically to \(1_{C}\) and

(b) every section of

\[1_{C_{n}}\]

 converges monotonically to the corresponding section of

\[1_{C}\]

Let \(\mathcal{M}\) denote the collection of all sets \(C\) in \(\mathcal{A}\times\mathcal{A}^{\prime}\) for which (7) holds. Clearly, \(\mathcal{M}\) contains \(\mathcal{F}_{0}\) and \(\mathcal{F}\). We now use (b) to show that \(\mathcal{M}\) is a monotone class; it will then follow by proposition 1.1.6 that \(\mathcal{M}=\sigma[\mathcal{F}]=\mathcal{A}\times\mathcal{A}^{\prime}\). Let \(C_{n}\) denote a sequence of sets in the class \(\mathcal{M}\) that converge monotonically (we will consider only the \(\nearrow\) case, since we only need to take complements in the \(\searrow\) case), and we give the name \(C\) to the limiting set. Since \(1_{C_{n}}\nearrow 1_{C}\), the function \(1_{C}\) is \((\mathcal{A}\times\mathcal{A}^{\prime})\)-measurable, and thus every section of \(1_{C}\) is measurable by (6). Now, for fixed \(\omega^{\prime}\) the number \(h(\omega^{\prime})\equiv\mu(C_{\omega^{\prime}})=\int_{\Omega}1_{C_{\omega^{ \prime}}}(\omega)\,d\mu(\omega)\) is (by the MCT and (b)) the \(\nearrow\) limit of the sequence of numbers \(h_{n}(\omega^{\prime})\equiv\mu(C_{n,\omega^{\prime}})=\int_{\Omega}1_{C_{n, \omega^{\prime}}}(\omega)\,d\mu(\omega)\), for each \(\omega^{\prime}\) in \(\Omega^{\prime}\). Thus the function \(h\) on \(\Omega^{\prime}\) is the limit of the functions \(h_{n}\) on \(\Omega^{\prime}\); and since \(C_{n}\) is in \(\mathcal{M}\), the functions \(h_{n}\) are \(\mathcal{A}^{\prime}\)-measurable by (7); thus \(h\) is \(\mathcal{A}^{\prime}\)-measurable by proposition 2.2.2. Moreover, the finite \(\nearrow\) numbers \(\phi(C_{n})\) are bounded above by \(\mu(\Omega)\nu(\Omega^{\prime})\), and thus converge to some number; call it \(\phi(C)\). That is,(c) \[\phi(C)\equiv\lim_{n}\phi(C_{n})=\lim_{n}\int_{\Omega^{\prime}}\{\int_{\Omega}1_{C_{ n,\omega^{\prime}}}(\omega)\,d\mu(\omega)\}\,d\nu(\omega^{\prime})\quad\mbox{since $C_{n}\in{\cal M}$}\] (d) \[=\lim_{n}\int_{\Omega^{\prime}}h_{n}d\nu=\int_{\Omega^{\prime}}\{ \lim_{n}h_{n}\}\,d\nu=\int_{\Omega^{\prime}}h\,d\nu\quad\mbox{ by the MCT and $h_{n}\nearrow h$}\] \[=\int_{\Omega^{\prime}}\{\int_{\Omega}1_{C_{\omega^{\prime}}}( \omega)\,d\mu(\omega)\}\,d\nu(\omega^{\prime})\quad\mbox{ by the definition of $h$}\] (e) \[=\int_{\Omega^{\prime}}\mu(C_{\omega^{\prime}})\,d\nu(\omega^{\prime}).\]

(Since \(\phi(C)\) is finite, we see that \(h\) is \(\nu\)-integrable. Thus \(h(\omega^{\prime})\) is finite for a.e. \([\nu]\omega^{\prime}\).) The argument for each fixed \(\omega\) is symmetric, and it gives the second equality in (7). Thus \(C\) is in \({\cal M}\), making \({\cal M}\) the monotone class \({\cal A}\times{\cal A}^{\prime}\); and (b) holds. (Thus the result (7) holds for the set function \(\phi\). But is \(\phi\) a measure?)

In this paragraph we will show that the product measure \(\phi\) of theorem 1.1 exists, and is defined by (e). To this end, let \(D_{1},D_{2},\ldots\) be pairwise disjoint sets in \({\cal A}\times{\cal A}^{\prime}\), and let \(C_{n}\equiv\sum_{1}^{n}D_{k}\nearrow C\equiv\sum_{1}^{\infty}D_{k}\). Then linearity of both single integrals shows (in the second equality) that

(f) \[\sum\nolimits_{1}^{\infty}\phi(D_{k})=\lim_{n}\sum\nolimits_{1}^{n}\phi(D_{k}) =\lim_{n}\phi(\sum\nolimits_{1}^{n}D_{k})=\lim_{n}\phi(C_{n})\] (g) \[=\phi(C)=\phi(\sum\nolimits_{1}^{\infty}D_{k}),\quad\mbox{by (c) through (e)}\]

so that \(\phi\) is c.a., and a measure on \({\cal A}\times{\cal A}^{\prime}\). We have just verified that the product measure of (3) exists on \({\cal A}\times{\cal A}^{\prime}\), and is given by (7). That is, we have just proven theorem 1.1 and given the representation (7) for \(\phi(C)\). Note that the product measure \(\phi\) also satisfies \(\phi(C)=\int_{\Omega\times\Omega^{\prime}}1_{C}(\omega,\omega^{\prime})d\phi( \omega,\omega^{\prime})\). \(\Box\)

**Exercise 1.5** : Give the details to verify that \(\sum_{1}^{n}\phi(D_{k})=\phi(\sum_{1}^{n}D_{k})\) in line (f) of the proof above of the product measure theorem.

**Theorem 1.3** (Fubini): Let \((\Omega,{\cal A},\mu)\) and \((\Omega^{\prime},{\cal A}^{\prime},\nu)\) be \(\sigma\)-finite measure spaces. Let \(\phi=\mu\times\nu\) on \((\Omega\times\Omega^{\prime},{\cal A}\times{\cal A}^{\prime}).\) Suppose that \(X(\omega,\omega^{\prime})\) is \(\phi\)-integrable (i.e., \(X^{-1}(\vec{\cal B})\subset{\cal A}\times{\cal A}^{\prime}\) and \(\int_{\Omega\times\Omega^{\prime}}X\,d\phi\) is finite). Then:

(8) All \(\omega^{\prime}\)-sections \(X_{\omega^{\prime}}(\cdot)\) of \(X\) are \({\cal A}\)-measurable functions on \(\Omega\). (9) For a.e. \([\nu]\) fixed \(\omega^{\prime}\),  the function \(X_{\omega^{\prime}}(\cdot)=X(\cdot,\omega^{\prime})\) is \(\mu\)-integrable. (10) The function \(h(\omega^{\prime})\equiv\int_{\Omega}X_{\omega^{\prime}}(\omega)\,d\mu(\omega)\) is a \(\nu\)-integrable function of \(\omega^{\prime}\). (11) \[\int_{\Omega\times\Omega^{\prime}}X(\omega,\omega^{\prime})d\phi(\omega,\omega^ {\prime})=\int_{\Omega^{\prime}}[\int_{\Omega}X(\omega,\omega^{\prime})\,d\mu( \omega)]\,d\nu(\omega^{\prime})=\int_{\Omega^{\prime}}h(\omega^{\prime})\,d\nu (\omega^{\prime}).\] (Setting \(X\) equal to \(1_{C}\) in (11) for \(C\in{\cal A}\times{\cal A}^{\prime}\) shows how the value \(\phi(C)\) of the product measure \(\phi\) at \(C\) was defined as an iterated integral; recall (7).)

**Corollary 1** (Tonelli): Let \(X\) be \({\cal A}\times{\cal A}^{\prime}\)-measurable and suppose either

\[\int\left[\int|X|\,d\mu\right]\,d\nu<\infty\quad\mbox{or}\quad\int\left[\int| X|\,d\nu\right]\,d\mu<\infty\quad\mbox{or}\quad X\geq 0. \tag{12}\]Then the claims of Fubini's theorem are true, including

\[\int X\,d\phi=\int\left[\int X\,d\mu\right]\,d\nu=\int\left[\int X\,d\nu\right]\,d\mu. \tag{13}\]

**Corollary 2** (\(\mu\times\nu\) null sets): A set \(C\) in \({\cal A}\times{\cal A}^{\prime}\) is (\(\mu\times\nu\))-null if and only if almost every \(\omega\)-section of \(C\) is a \(\nu\)-null set. That is, for \(C\in{\cal A}\times{\cal A}^{\prime}\) we have

\[\mu\times\nu(C)=0\quad\mbox{if and only if }\nu(C_{\omega})=0\quad\mbox{for a.e. }[\mu]\ \ \omega\mbox{ in }\Omega. \tag{14}\]

**Proof.** By using the \(\sigma\)-finiteness of the two measures to decompose both \(\Omega\) and \(\Omega^{\prime}\), we may assume in this proof that both \(\mu\) and \(\nu\) are finite measures. We begin by discussing only measurability questions.

We will first show that

(a) all

\[\omega^{\prime}\]

-sections of an

\[({\cal A}\times{\cal A}^{\prime})\]

-measurable function

\[X\]

 are

\[{\cal A}\]

-measurable.

The previous theorem shows that

(b) all

\[\omega^{\prime}\]

-sections

\[X_{\omega^{\prime}}\]

 of

\[X\]

 are

\[{\cal A}\]

-measurable

whenever

\[X=1_{C}\]

 for some

\[C\in{\cal A}\times{\cal A}^{\prime}\]

Now let \(X\) denote any (\({\cal A}\times{\cal A}^{\prime}\))-measurable function. Then for any \(B\) in \(\bar{\cal B}\),

(c) \[X_{\omega}^{-1}(B)=\{\omega:X(\omega,\omega^{\prime})\in B\}=\{\omega:(\omega, \omega^{\prime})\in X^{-1}(B)\}\]

is the \(\omega^{\prime}\)-section of the indicator function of the set \(C=X^{-1}(B)\); so (b) shows that any arbitrary \(\omega^{\prime}\)-section of this \(X\) is \({\cal A}\)-measurable, and so establishes (a) and (8).

We now turn to all the other claims of the Fubini and Tonelli theorems. By theorem 1.2 they hold for all (\({\cal A}\times{\cal A}^{\prime}\))-measurable indicator functions. Linearity of the various integrals shows that the theorems also hold for all simple functions. Applying the MCT to the various integrals shows that the theorems also hold for all (\({\cal A}\times{\cal A}^{\prime}\))-measurable \(X\geq 0\). Then linearity of the integral shows that the theorems also hold for all \(X\) for whichever of the three integrals exists finitely (the double integral or either iterated integral).

Corollary 2 follows immediately by applying (13) and exercise 3.2.2 (only the zero function) to the integral of the function \(1_{C}\). \(\Box\)

**Corollary 3**: All this extends naturally to \(n\) dimensions.

**Example 1.2** (Summing infinite series): (i) (One-dimensional infinite series) We can think of

(a) \[\sum\nolimits_{n=0}^{\infty}a_{n}\quad\mbox{ as }\quad\int_{\Omega}X\,d\mu= \mbox{E}(X)\quad\mbox{on }(\Omega,{\cal A},\mu),\]

where \(\Omega\equiv\{0,1,2,\ldots\}\), \({\cal A}\equiv\{\mbox{all subsets of }\Omega\}=2^{\Omega}\), \(\mu\) denotes counting measure, and \(X(n)=a_{n}\). Well,

(b) \[\mbox{E}|X|=\sum\nolimits_{n=0}^{\infty}|a_{n}|=\sum\nolimits_{n=0}^{\infty}a _{n}^{+}+\sum\nolimits_{n=0}^{\infty}a_{n}^{-}=\int_{\Omega}X^{+}\,d\mu+\int_ {\Omega}X^{-}\,d\mu.\]Now let \(\pi\) denote any one-to-one transform that maps \(\Omega\) onto \(\Omega\). Then an arbitrary rearrangement of the sequence \(a_{n}\) can be written as \(a_{\pi(n)}\). Appealing to the definition in (3.1.2) for the value of \(\int_{\Omega}X^{+}\,d\mu\) (which was given there as having the value \(\sup\{\int_{\Omega}Y\,d\mu:0\leq Y\leq X^{+},\) with \(Y\) a simple fuction\(\}\); and noting that \(0\leq Y\leq X^{+}\) if and only if \(0\leq Y_{\pi}\leq X^{+}_{\pi}\), where \(X_{\pi}(n)\equiv X(\pi(n))\)), we see that the values of

(c) \[\sum\nolimits_{n=0}^{\infty}a_{n}^{+}=\int_{\Omega}X^{+}\,d\mu=\int_{\Omega}X^{ +}_{\pi}\,d\mu=\sum\nolimits_{n=0}^{\infty}a_{\pi(n)}^{+}\ \mbox{for any 1-1 transform $\pi$}.\]

Thus the values of

(d) \[\sum\nolimits_{n=0}^{\infty}a_{\pi(n)}^{+}=\int_{\Omega}X^{+}\,d\mu\quad\mbox{ and}\quad\sum\nolimits_{n=0}^{\infty}a_{\pi(n)}^{-}=\int_{\Omega}X^{-}\,d\mu\]

have the same values for every \(\pi\). Thus, for every \(\pi\),

(e) \[\begin{array}{l}\sum\nolimits_{n=0}^{\infty}a_{\pi(n)}^{-}=\int_{\Omega}X_{ \pi}\,d\mu=\int_{\Omega}X\,d\mu=\int_{\Omega}X^{+}\,d\mu-\int_{\Omega}X^{-}\,d \mu\\ =\sum\limits_{n=0}^{\infty}a_{n}^{+}-\sum\limits_{n=0}^{\infty}a_{n}^{-}=\sum \limits_{n=0}^{\infty}a_{n}.\end{array}\]

That is, for every one-to-one transformation \(\pi\) of \(\{0,1,2,\ldots\}\) onto \(\{0,1,2,\ldots\}\),

\[\sum\limits_{n=1}^{\infty}a_{\pi(n)}=(\mbox{one fixed number})=\sum\limits_{n=0}^{\infty}a_{n}\quad(\mbox{provided }\sum\limits_{n=0}^{\infty}|a_{n}|<\infty). \tag{15}\]

(ii) (Two-dimensional infinite series) Likewise, for any 1-1 transform \(\pi\) of \(\Omega\times\Omega\) onto itself, we have

(16) \[\sum\nolimits_{\{\mbox{all }m,n\}}a_{\pi(m,n)}\] \[=\sum\nolimits_{\{\mbox{all }m,n\}}a_{\pi(m,n)}^{+}-\sum \nolimits_{\{\mbox{all }m,n\}}a_{\pi(m,n)}^{-}=(\mbox{one fixed number})\] \[=\sum\nolimits_{m=0}^{\infty}[\sum\nolimits_{n=0}^{\infty}a_{m,n }^{+}]\ -\sum\nolimits_{m=0}^{\infty}[\sum\nolimits_{n=0}^{\infty}a_{m,n}^{-}]\] \[=\sum\nolimits_{m=0}^{\infty}\left[\sum\nolimits_{n=0}^{\infty}a_ {m,n}\right]\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\(ii) Why does Fubini's theorem fail here? (Solve \(f(x,y)=0\), and use this to divide the domain of \(f\). Integrate over each of these two regions separately.)

(iii) Construct another example of this type.

**Example 1.3** (Application to conditional expectation in probability and statistics) Consider the measure space \((R_{2},{\cal B}_{2},\lambda_{2})=(R\times R,{\cal B}\times{\cal B},\lambda\times\lambda)\) in the product measure theorem context; thus \(\phi=\lambda_{2}\) is two-dimensional Lebesgue measure. Next, consider Fubini's theorem, replacing \(X(\omega,\omega^{\prime})\) in that result by

\[g(x,y)f_{Y|X=x}(y)f_{X}(x)\quad\mbox{for all }(x,y)\in R_{2}=\Omega \tag{18}\]

as in an elementary probability class. Continuing in the probabilistic context, think of \(f_{X}(\cdot)\) as the marginal density of \(X\), of \(f_{Y|X=x}(\cdot)\) as the conditional density of \(Y\) given that \(X=x\), and of \(g(X,Y)\) as a random function whose expectation you would like to evaluate. Tonelli gives

\[{\rm E}g(X,Y)=\int_{R_{2}}g(x,y)f_{Y|X=x}(y)f_{X}(x)d\lambda_{2}( x,y)\] \[\quad=\int_{R}\{\int_{R}g(x,y)f_{Y|X=x}(y)\lambda(y)\}f_{X}(x)d \lambda(x) \tag{19}\] \[\quad=\int_{R}\{{\rm E}(g(X,Y)|X=x)\}f_{X}(x)d\lambda(x)={\rm E} \{{\rm E}(g(X,Y)|X)\}\]

provided the iterated integral of \(|g(x,y)|\) is finite. This shows how an elementary conditional model with dependent rvs can fit into the Lebesgue integral context. (To treat an elementary version of discrete rvs, just replace two-dimensional Lebesgue measure \(\lambda_{2}\) by two-dimensional counting measure on the set of pairs \((m,n)\) with \(m,n\geq 0\).) Mainly, this example shows one way that some dependent models can be treated in the context of the product measure theorem using a "base measure" \(\phi=\mu\times\nu\) that seemingly can only be associated with independendent rvs. (The Lebesgue integral allows us to treat absolutely continuous, discrete, and singular distributions simultaneously via the expectation operation \({\rm E}\{\cdot\}\). This effect will be extended to a general treatment of conditional expectation in chapter 7.) \(\Box\)

## 2 Random Vectors on \((\Omega,\mathcal{A},P)\)

We will now treat measurable functions from a probability space \((\Omega,\mathcal{A},P)\) to a Euclidean space \((R_{n},\mathcal{B}_{n})\), with \(n\geq 1\). Let \(\mathsf{x}\equiv(x_{1},\ldots,x_{n})^{\prime}\) denote a generic vector in the Euclidean space \(R_{n}\).

**Definition 2.1** (Random vectors): Suppose \(\mathsf{X}\equiv(X_{1},\ldots,X_{n})^{\prime}\) is such that \(\mathsf{X}:\Omega\to R_{n}\) is \(\mathcal{B}_{n}\)-\(\mathcal{A}\)-measurable. Then \(\mathsf{X}\) is called a _random vector_ (which is also abbreviated \(rv\)). Define the _joint distribution function_ (or just \(df\)) of \(\mathsf{X}\) by

\[F(\mathsf{x})\equiv F_{X_{1},\ldots,X_{n}}(x_{1},\ldots,x_{n})=P(\bigcap\nolimits _{i=1}^{n}[X_{i}\leq x_{i}]).\]

Write \(\mathsf{x}\leq\mathsf{y}\) to denote that \(x_{i}\leq y_{i}\) for all \(1\leq i\leq n\); and now define the basic rectangles \((\mathsf{x},\mathsf{y}]\)\(\equiv\times_{i=1}^{n}(x_{i},y_{i}]\) whenever \(\mathsf{x}\leq\mathsf{y}\). Let

**Proposition 2.1** (Measurability): Now, \(\mathsf{X}\equiv(X_{1},\ldots,X_{n})^{\prime}:\,\Omega\to R_{n}\) or \(\bar{R}_{n}\) is such that

\[\mathsf{X}\text{ is }\left\{\begin{array}{l}\mathcal{B}_{n}\text{-} \mathcal{A}\text{-measurable}\\ \bar{\mathcal{B}}_{n}\text{-}\mathcal{A}\text{-measurable}\end{array}\right.\text { if and only if each }X_{i}\text{ is }\left\{\begin{array}{l}\mathcal{B}\text{-} \mathcal{A}\text{-measurable}\\ \bar{\mathcal{B}}\text{-}\mathcal{A}\text{-measurable}.\end{array}\right.\]

Thus, a random vector is measurable if and only if each coordinate rv is measurable.

**Proof.** We give the details for finite-valued functions. \((\Rightarrow)\) Now,

\[[X_{i}\leq x_{i}]=X_{i}^{-1}((-\infty,x_{i}])\] \[=\mathsf{X}^{-1}(R\times\cdots\times R\times(-\infty,x_{i}]\times R \times\cdots\times R)\in\mathcal{A}.\]

\((\Leftarrow)\) Also, \([\mathsf{X}\leq\mathsf{x}]=\bigcap\nolimits_{i=1}^{n}[X_{i}\leq x_{i}]\in \mathcal{A}\), since each \(X_{i}\) is measurable, where

\[\sigma[\mathcal{H}_{n}]\equiv\sigma[\{\text{all }(-\infty,x_{1}]\times\cdots \times(-\infty,x_{n}]\}]=\mathcal{B}_{n}.\]

Moreover, using (2.1.12) for the final equality,

\[\mathcal{F}(\mathsf{X}_{n})\equiv\mathsf{X}_{n}^{-1}(\mathcal{B}_{n})= \mathsf{X}_{n}^{-1}(\sigma[\mathcal{H}_{n}])=\sigma[\mathsf{X}_{n}^{-1}( \mathcal{H}_{n})]\subset\mathcal{A},\]

with the set inclusion shown in the first line. That is, \(\mathsf{X}^{-1}(\mathcal{B}_{n})\subset\mathcal{A}\). \(\square\)

**Exercise 2.1** (_Joint_ df_): A joint df \(F\) is \(\nearrow\) and right continuous and satisfies

\[\text{All }F(x_{1},\ldots,x_{i-1},-\infty,x_{i+1},\ldots,x_{n})=0 \quad\text{and}\quad F(\infty,\ldots,\infty)=1,\] \[F(x_{1},\ldots,x_{i-1},+\infty,x_{i+1},\ldots,x_{n}) \tag{3}\] \[=F_{X_{1},\ldots,X_{i-1},X_{i+1},\ldots,X_{n}}(x_{1},\ldots,x_{i- 1},x_{i+1},\ldots,x_{n}) \tag{2}\]

for all \(j=1,\ldots,n\) and \(x_{1},\ldots,x_{n}\).

**Exercise 2.2**: Suppose \(F:R_{n}\to R\) is \(\nearrow\) and right continuous and satisfies (2) and (3). Then there exists a unique probability measure \(P\equiv P_{F}\) on \(\mathcal{B}_{n}\) that satisfies

\[P((\mathsf{x},\mathsf{y}])=F(\mathsf{x},\mathsf{y}]\quad\text{for all } \mathsf{x}\leq\mathsf{y}. \tag{4}\]This is a generalization of the correspondence theorem to. Now note that the identity function, for each, is a random vector on that has as its joint df the function above. Thus, given any joint df, there is a random vector having  as its joint df. This is in the spirit of example 2.2.1.

**Definition 2.2** (Joint density of rvs): Let denote a rv. Define for all, so that defines the induced distribution of on. Let denote Lebesgue measure on. If, then a finite-valued Radon-Nikodym derivative exists (and is unique a.e. for which

(5)

When this is true, is called the _joint density_ (or, the _density_) of the rv. (For one-dimensional rvs, we often denote the distribution, df, and density of,, and.)

**Exercise 2.3** (Marginal densities): Suppose that has the induced distribution, and with joint density (as in the previous definition). Let denote denote the complementary indices. Show that the induced distribution of satisfies, and that its joint density is given by

(6)

on. We also call the _marginal density_ of.

**Exercise 2.4** (Lebesgue-Stieltjes measures on ): (i) Let be a gdf on. Using the notation of definition 2.1, let

(7)

in parallel with the correspondence theorem 1.3.1 and exercise 1.3.1. Show that

(8)

for countable unions of _disjoint subrectangles_ of. As in definition 1.3.3, call _absolutely continuous_ if for all there exists a for which

(9)

whenever and all of the subrectangles of are mutually disjoint. Here, is the Lebesgue-Stieltjes measure generalization of area with.

(ii) State and prove the two-dimensional analogs of exercise 3.2.6 (integrals as measures) and exercise 3.2.7 (absolutely continuous dfs).

## 3 Countably Infinite Product Probability Spaces\({}^{o}\)

We now begin to carry out the program discussed in section 2.5. That is, we will extend the notion of rvs and product probability measures to a countably infinite number of dimensions.

**Notation 3.1** (\(R_{\infty}\) and \({\cal B}_{\infty}\)): Let

\[R_{\infty}\equiv\prod_{n=1}^{\infty}R\equiv\{(x_{1},x_{2},\ldots):x_{n}\in R \mbox{ for all }n\geq 1\}. \tag{1}\]

Let \(I\) denote an interval of the type \((c,d],\ (-\infty,d],\ (c,+\infty),\) or \((-\infty,\infty)\). An \(n\)-_dimensional rectangle_ will mean any set of the form \(I_{1}\times\cdots\times I_{n}\times R\times R\times\cdots,\) where each interval \(I_{i}\) is of the type above. A _finite-dimensional rectangle_ is an \(n\)-dimensional rectangle, for some \(n\geq 1\). A _cylinder set_ is defined as a set of the form \(B_{n}\times R\times R\times\cdots\) with \(B_{n}\) in \({\cal B}_{n}\) for some \(n\geq 1\). Thus:

\[{\cal C}_{I} \equiv\{\mbox{all finite-dimensional rectangles}\}\] \[=\{I_{1}\times\cdots\times I_{n}\times R\times R\times\cdots:n \geq 1,\mbox{all }I_{i}\mbox{ as above}\}, \tag{3}\] \[{\cal C}_{F} \equiv\{\mbox{all finite disjoint unions of finite-dimensional rectangles}\},\] (4) \[{\cal C}_{\infty} \equiv\{\mbox{all cylinder sets}\}\equiv\{B_{n}\times R\times R \times\cdots\ :\ n\geq 1,\ B_{n}\in{\cal B}_{n}\}. \tag{2}\]

Both \({\cal C}_{F}\) and \({\cal C}_{\infty}\) are fields, and a trivial application of exercise 1.1.1 shows that

\[{\cal B}_{\infty}\equiv\sigma[{\cal C}_{I}]=\sigma[{\cal C}_{F}]=\sigma[{\cal C }_{\infty}]. \tag{5}\]

Thus, extending a measure from \({\cal C}_{I}\) to \({\cal B}_{\infty}\) will be of prime interest to us. We first extend the criterion for measurability from \(n\) dimensions to a countably infinite number of dimensions.

**Proposition 3.1** (Measurability on \({\cal B}_{\infty}\)): (a) Now, \({\sf X}\equiv(X_{1},X_{2},\ldots)^{\prime}:\Omega\to R_{\infty}\) is \({\cal B}_{\infty}\)-\({\cal A}\)-measurable if and only if each \(X_{n}\) is \({\cal B}\)-\({\cal A}\)-measurable.

(b) If \({\sf X}\) is \({\cal B}_{\infty}\)-\({\cal A}\)-measurable and if \((i_{1},i_{2},\ldots)\) is an arbitrary sequence of integers, then \({\sf Y}\equiv(X_{i_{1}},X_{i_{2}},\ldots)^{\prime}\) is \({\cal B}_{\infty}\)-\({\cal A}\)-measurable.

**Exercise 3.1**: Prove proposition 3.1.

**Notation 3.2**: We will use the notation

\[{\cal F}(X_{i})\equiv X_{i}^{-1}({\cal B})\quad\mbox{and}\quad{\cal F}(X_{i_{ 1}},X_{i_{2}},\ldots)\equiv{\sf Y}^{-1}({\cal B}_{\infty})=\sigma[\bigcup_{n= 1}^{\infty}X_{i_{n}}^{-1}({\cal B})] \tag{6}\]

to denote the minimal sub \(\sigma\)-fields of \({\cal A}\) relative to which the quantities \(X_{i}\) and \({\sf Y}\equiv(X_{i_{1}},X_{i_{2}},\ldots)\) are measurable.

Now suppose that \(P_{n}\) is a probability measure on \((R_{n},{\cal B}_{n})\), for each \(n\geq 1.\) The question is: When can we extend the collection \(\{P_{n}\ :\ n\geq 1\}\) to a measure on \((R_{\infty},{\cal B}_{\infty})?\) Reasoning backwards to see what conditions the family of finite-dimensional distributions should satisfy leads to the following definition.

**Definition 3.1** (Consistency) Finite-dimensional distributions \(\{(R_{n},{\cal B}_{n},P_{n})\}_{n=1}^{\infty}\) are _consistent_ if for every \(n\geq 1\), every \(B_{1},\ldots,B_{n}\in{\cal B}\), and every \(1\leq i\leq n\),

\[\begin{array}{l}P_{n-1}((X_{1},\ldots,X_{i-1},X_{i+1},\ldots,X_{n})\in B_{1} \times\cdots\times B_{i-1}\times B_{i+1}\cdots\times B_{n})\\ =P_{n}((X_{1},\ldots,X_{i-1},X_{i},X_{i+1},\ldots,X_{n})\in B_{1}\times\cdots \times B_{i-1}\times R\times B_{i+1}\times\cdots\times B_{n}).\end{array} \tag{7}\]Theorem 3.1 (Kolmogorov's extension theorem)An extension of any consistent family of probability measures \(\{(R_{n},\mathcal{B}_{n},P_{n})\}_{n=1}^{\infty}\) to a probability \(P(\cdot)\) on \((R_{\infty},\mathcal{B}_{\infty})\) necessarily exists, and it is unique.

We will first summarize the main part of this proof as a separately stated result that seems of interest in its own right.

Theorem 3.2 \({}^{o}\) (\((R_{\infty},\mathcal{B}_{\infty})\) extension theorem; Breiman)Let \(P\) on \(\mathcal{C}_{I}\) satisfy:

(a) \(P\geq 0\) and \(P(R_{\infty})=1\).

(b) If \(D=\sum_{j=1}^{m}D_{j}\) for \(n\)-fold rectangles \(D\) and \(D_{j}\), then \(P(D)=\sum_{1}^{m}P(D_{j})\).

(c) If \(D\) denotes any fixed \(n\)-dimensional rectangle, then there exists a sequence of compact \(n\)-dimensional rectangles \(D_{j}\) for which \(D_{j}\nearrow D\) and \(P(D_{j})\nearrow P(D)\). (That is, \(P\) is well-defined and additive on \(n\)-dimensional rectangles and satisfies something like continuity from below.) Then there exists a unique extension of \(P\) to \(\mathcal{B}_{\infty}\).

Proof.\({}^{*}\) (Recall the continuity result of proposition 1.1.3.) Now,

\[\begin{array}{l}\mathcal{C}_{F}\equiv\{\mbox{all finite disjoint unions of finite-dimensional rectangles}\}\\ =\{\mbox{a field generating $\mathcal{B}_{\infty}$}\}.\end{array}\]

For \(A=\sum_{1}^{m}D_{j}\in\mathcal{C}_{F}\), define \(P(A)\equiv\sum_{j=1}^{m}P(D_{j})\).

First, we will show that \(P\) is well-defined on \(\mathcal{C}_{F}\). Let \(A=\sum_{1}^{m}D_{j}=\sum_{1}^{m^{\prime}}D^{\prime}_{k}.\) Now, \(D^{\prime}_{k}=D^{\prime}_{k}A=\sum_{1}^{m}D^{\prime}_{k}D_{j}\) and \(D_{j}=D_{j}A=\sum_{1}^{m^{\prime}}D_{j}D^{\prime}_{k}\). Thus

\[P(A)=\sum_{1}^{m}P(D_{j})=\sum_{1}^{m}P(\sum_{1}^{m^{\prime}}D_{j}D^{\prime}_{ k})=\sum_{1}^{m}\sum_{1}^{m^{\prime}}P(D_{j}D^{\prime}_{k})\]

(r)

Next, we will show that \(P\) is f.a. on \(\mathcal{C}_{F}\). So we let \(A_{1},\ldots,A_{m}\in\mathcal{C}_{F}\) be such that \(A\equiv\sum_{1}^{m}A_{i}\in\mathcal{C}_{F}\) also. Then, writing \(A_{i}=\sum_{1}^{m_{i}}D_{ij}\) with \(D_{i1},\ldots,D_{i,m_{i}}\) disjoint,

\[P(A)=P(\sum_{1}^{m}A_{i})=P(\sum_{1}^{m}\sum_{1}^{m_{i}}D_{ij})=\sum_{1}^{m} \sum_{1}^{m_{i}}P(D_{ij})=\sum_{1}^{m}P(A_{i}),\]

(using condition (b) in each of the last two equalities), since \(P\) is well-defined.

We will now show that \(P\) is continuous from above at \(\emptyset\). Let \(A_{n}\)'s in \(\mathcal{C}_{F}\) be such that \(A_{n}\searrow\emptyset\). We must show that \(P(A_{n})\searrow 0\). Assume not. Then \(P(A_{n})\searrow\epsilon>0\); and by going to subsequences, we may assume that \(A_{n}=A_{n}^{*}\times\prod_{n+1}^{\infty}R\), where each \(A_{n}^{*}\) is a finite union of disjoint rectangles (repeat some members of the sequence if necessary in order to have \(A_{n}^{*}\subset R_{n}\)). By condition (c), choose \(B_{n}^{*}\subset A_{n}^{*}\) such that \(B_{n}^{*}\) is a finite union of compact disjoint rectangles in \(R_{n}\) with

\[P(A_{n}\backslash B_{n})<\epsilon/2^{n+1},\quad\mbox{where $B_{n}\equiv B_{n}^{*} \times\prod_{n+1}^{\infty}R$}.\]

Let \(C_{n}=\bigcap_{1}^{n}B_{k}\equiv C_{n}^{*}\times\prod_{n+1}^{\infty}R\), with \(C_{n}^{*}\) compact in \(R_{n}\) (the \(B_{n}\)'s need not be \(\searrow\), but the \(C_{n}\)'s are). Then we observe that \(C_{n}\searrow\emptyset\), since \(C_{n}\subset B_{n}\subset A_{n}\) with \(A_{n}\searrow\emptyset\); but we also have \(P(C_{n})\geq\epsilon/2\), since

\[P(A_{n}\backslash C_{n})\leq\sum_{k=1}^{n}P(A_{n}\backslash B_{k})\leq\sum_{k= 1}^{n}P(A_{k}\backslash B_{k})\leq\sum_{n+1}^{n}\epsilon/2^{k+1}\leq\epsilon/2.\]But \(C_{n}\searrow\)with \(P(C_{n})\geq\epsilon/2\) for all \(n\) is not compatible with the conclusion that \(C_{n}\searrow\emptyset\): Let \(\mathsf{x}^{(1)}\in C_{1},\ldots,\mathsf{x}^{(n)}\in C_{n},\ldots\), where \(\mathsf{x}^{(n)}\equiv(x_{1}^{(n)},x_{2}^{(n)},\ldots).\) Choose an initial subsequence \(N_{1}\) such that \(x_{1}^{(N_{1})}\rightarrow\) (some \(x_{1}\)) \(\in C_{1}^{*}\); then choose a further subsequence \(N_{2}\) such that \((x_{1}^{(N_{2})},x_{2}^{(N_{2})})\rightarrow\) (some \((x_{1},x_{2})\)) \(\in C_{2}^{*},\ldots\). Along the diagonal subsequence, say \(N\), we have \(x_{j}^{(N)}\to x_{j}\), for all \(j\). Now, \(\mathsf{x}=(x_{1},x_{2},\ldots)\in C_{n}\) for all \(n\). Hence \(C_{n}\searrow\emptyset\). But this is a contradiction, and thus allows us to claim that \(P(A_{n})\searrow 0\) for any \(A_{n}\)'s in \(\mathcal{C}_{F}\) that satisfy \(A_{n}\searrow\emptyset\).

Now apply the continuity of measures in proposition 1.1.3, and then apply the Caratheodory extension of theorem 1.2.1 to complete the proof. \(\square\)

Proof.*We now turn to the Kolmogorov extension theorem. The \(P\) defined by

(v) \[P(B_{1}\times\cdots\times B_{n}\times\cdots)\equiv P_{n}(B_{1}\times\cdots \times B_{n})=P_{m+n}(B_{1}\times\cdots\times B_{n}\times R\times\cdots\times R)\]

is a well-defined f.a. probability on \(\mathcal{C}_{I}=\{\)all finite-dimensional rectangles\(\}\); this follows from the consistency condition (7). Thus (a) and (b) of theorem 3.2 hold.

We will now verify (c). Fix \(n\). Let \(D_{n}\) be an arbitrary but fixed \(n\)-dimensional rectangle. It is clearly possible to specify compact \(n\)-dimensional rectangles \(D_{nj}\) for which \(D_{nj}\nearrow D_{n}\) as \(j\rightarrow\infty\). Write \(D_{j}=D_{nj}\times\prod_{n+1}^{\infty}R\) and \(D=D_{n}\times\prod_{n+1}^{\infty}R\), so that \(D_{j}\nearrow D\). Thus, by the continuity of signed measures in proposition 1.1.4,

(w) \[P(D_{j})=P_{n}(D_{nj})\nearrow P_{n}(D_{n})=P(D),\]

since \(P_{n}\) is a measure on \((R_{n},\mathcal{B}_{n})\). Thus (c) holds. The conclusion follows from theorem 3.2.

**Example 3.1** (Coordinate rvs) Once consistent probability measures \(P_{n}(\cdot)\) on \((R_{n},\mathcal{B}_{n})\) have been extended to a probability measure \(P(\cdot)\) on \((R_{\infty},\mathcal{B}_{\infty})\), it is appropriate then to define \(X_{n}(x_{1},x_{2},\ldots)=x_{n}\), for each \(n\geq 1\). These are rvs on the probability space \((\Omega,\mathcal{B},P)\equiv(R_{\infty},\mathcal{B}_{\infty},P)\). Moreover,

\[P((X_{1},\ldots,X_{n})\in B_{n})=P((X_{1},\ldots,X_{n})^{-1}(B_ {n}))=P(\mathsf{X}^{-1}(B_{n}\times\prod_{n+1}^{\infty}R)) \tag{8}\] \[=P(B_{n}\times\prod_{n+1}^{\infty}R)=P_{n}(B_{n})\]

for all \(B_{n}\in\mathcal{B}_{n}\). We thus have a realization of \(\mathsf{X}\equiv(X_{1},X_{2},\ldots)\): \(\Omega\to R_{\infty}\) that is \(\mathcal{B}_{\infty}\)-\(\mathcal{A}\)-measurable, and each \((X_{1},\ldots,X_{n})\) induces the distribution \(P_{n}\) on \((R_{n},\mathcal{B}_{n})\). This is the natural generalization of example 2.2.1 and the comment below exercise 5.2.2. \(\square\)

**Theorem 3.3** (The finite dimensional dfs define probability theory): Let \(\mathsf{X}=(X_{1},X_{2},\ldots)^{\prime}\) denote any random element on \((R_{\infty},\mathcal{B}_{\infty})\). Then \(P_{\mathsf{X}}\) can be determined solely by examination of the finite-dimensional distributions of \(\mathsf{X}.\) Also, whether or not there exists a finite rv \(X\) such that \(X_{n}\) converges to \(X\) in the sense of \(\rightarrow_{a.s.},\rightarrow_{p},\rightarrow_{r}\), or \(\rightarrow_{d}\) can be similarly determined.

Proof.: Let \(\mathcal{C}\) denote the \(\bar{\pi}\)-system consisting of \(R_{\infty}\) and of all sets of the form \(\prod_{1}^{n}(-\infty,x_{i}]\)\(\times\prod_{n+1}^{\infty}R\), for some \(n\geq 1\) and all \(x_{i}\in R\). The finite-dimensional distributions (even the finite-dimensional dfs) determine \(P_{\infty}\) on \(\mathcal{C}\), and hence on \(\mathcal{B}_{\infty}=\sigma[\mathcal{C}]\) (appeal to Dynkin's \(\pi\)-\(\lambda\) theorem of proposition 1.1.5). To emphasize the fact further, we now consider each convergence mode separately.

\(\rightarrow_{d}\): Obvious.

\(\rightarrow_{r}\): \(\mathrm{E}|X_{n}-X|^{r}\to 0\) if and only if \(\mathrm{E}|X_{n}-X_{m}|^{r}<\epsilon\) for all \(n,m\geq\) some \(N_{\epsilon}\).

\(\rightarrow_{p}\): \(X_{n}\rightarrow_{p}X\) if and only if \(P(|X_{n}-X_{m}|>\epsilon)<\epsilon\) for all \(n,m\geq\) some \(N_{\epsilon}\).

\(\rightarrow_{a.s.}\): \(X_{n}\rightarrow{a.s.}\): \(X\) if and only if

\[1 =P(\cup_{n=1}^{\infty}\cap_{m=n}^{\infty}[|X_{m}-X_{n}|\leq \epsilon])\quad\text{for all }\epsilon>0\] \[\quad=\lim_{n}\lim_{N}P(\cap_{m=n}^{N}[|X_{m}-X_{n}|\leq \epsilon])=\lim_{n}\lim_{N}\{\text{a function of }F_{X_{n},\ldots,X_{N}}\}.\]

The proof is complete 

**Example 3.2** (Equivalent experiments): Perhaps I roll an ordinary die \(n\) times with the appearance of an even number called "success." Perhaps I draw a card at random \(n\) times, each time from a freshly shuffled deck of standard playing cards, with "red" called "success." Perhaps I flip a fair coin \(n\) times with "heads" called "success." Note that \((X_{1},\ldots,X_{n})\) has the same distribution in all three cases. Thus, if I report only the data from one of these experiments, you can not hope to determine which of the three experiments was actually performed. These are called _equivalent experiments_.

## 4 Random Elements and Processes on \((\Omega,\mathcal{A},P)\)\({}^{o}\)

**Definition 4.1** (Projections and finite-dimensional subsets): Let \(M_{T}\) denote a collection of functions that associate with each \(t\) of some set \(T\) a real number denoted by either \(x_{t}\) or \(x(t)\). (\(T\) is usually a Euclidean set such as \([0,1],\ R,\) or \([0,1]\times R.\) The collection \(\mathcal{M}_{T}\) is often a collection of "nice" functions, such as the continuous functions on \(T.\)) For each integer \(k\) and all \((t_{1},\ldots,t_{k})\) in \(T\) we let \(\pi_{t_{1},\ldots,t_{k}}\) denote the _projection mapping_ of \(M_{T}\) into \(k\)-dimensional space \(R_{k}\) defined by

\[\pi_{t_{1},\ldots,t_{k}}(x)\equiv(x(t_{1}),\ldots,x(t_{k})). \tag{1}\]

Then for any \(B\) in the set of all \(k\)-dimensional Borel subsets \(\mathcal{B}_{k}\) of \(R_{k}\), the set \(\pi_{t_{1},\ldots,t_{k}}^{-1}(B)\) is called a _finite-dimensional subset_ of \(M_{T}\).

**Exercise 4.1**: Show that the collection \(\mathcal{M}_{T}^{0}\) of all finite-dimensional subsets of \(M_{T}\) is necessarily a field. (This is true no matter what collection \(M_{T}\) is used.)

**Definition 4.2** (Measurable function spaces, finite-dimensional distributions, random elements, and normal processes): We let \(\mathcal{M}_{T}\) denote the \(\sigma\)-field generated by the field \(\mathcal{M}_{T}^{0}\). We call \(\mathcal{M}_{T}^{0}\) and \(\mathcal{M}_{T}\) the _finite-dimensional field_ and the _finite- dimensional \(\sigma\)-field_, respectively. Call the measurable space \((M_{T},\mathcal{M}_{T})\) a _measurable function space_ over \(T.\)

Given any probability space \((\Omega,\mathcal{A},P)\) and any measurable space \((\Omega^{*},\mathcal{A}^{*})\), an \(\mathcal{A}^{*}\)-\(\mathcal{A}\)-measurable mapping \(X:\ \Omega\to\Omega^{*}\) will be called a _random element_. We denote this by \(X:\ (\Omega,\mathcal{A})\to(\Omega^{*},\mathcal{A}^{*})\) or by \(X:\ (\Omega,\mathcal{A},P)\to(\Omega^{*},\mathcal{A}^{*})\), or even by \(X:\ (\Omega,\mathcal{A},P)\to(\Omega^{*},\mathcal{A}^{*},P^{*})\), where \(P^{*}\) denotes the induced probability on the image space.

A random element \(X:\ (\Omega,\mathcal{A},P)\to(M_{T},\mathcal{M}_{T},P^{*})\) in which the image space is a measurable function space will be called a _process_. The _finite-dimensional distributions_ of a process are the distributions induced on the \((R_{k},\mathcal{B}_{k})\) by the projection mappings \(\pi_{t_{1},\ldots,t_{k}}\): \((M_{T},\mathcal{M}_{T},P^{*})\to(R_{k},\mathcal{B}_{k})\). If all of the finite-dimensional distributions of a process \(X\) are multivariate normal (see section 9.3 below), then we call \(X\) a _normal process_.

**Definition 4.3** (Realizations and versions): If two random elements \(X\) and \(Y\) (possibly from different probability spaces to different measurable function spaces) have identical induced finite-dimensional distributions, then we refer to \(X\) and \(Y\) as different _realizations_ of the same random element and we call them _equivalent_ random elements. We denote this by agreeing that

\[X\cong Y\qquad\text{means that $X$ and $Y$ are equivalent random elements.}\]

(We will see in chapter 12 that a process called Brownian motion can be realized on both the \((R_{[0,1]},\mathcal{B}_{[0,1]})\) of (3) and \((C,\mathcal{C})\), where \(C\equiv C_{[0,1]}\) denotes the space of all continuous functions on \([0,1]\) and \(\mathcal{C}\equiv\mathcal{C}_{[0,1]}\) denotes its finite-dimensional \(\sigma\)-field.)

If \(X\) and \(Y\) are defined on the same probability space and \(P(X_{t}=Y_{t})=1\) for all \(t\in T\), then \(X\) and \(Y\) are called _versions_ of each other. (In chapter 12 we will see versions \(X\) and \(Y\) of Brownian motion where \(X:\ (\Omega,\mathcal{A},P)\to(R_{[0,1]},\mathcal{B}_{[0,1]})\) and \(Y:\ (\Omega,\mathcal{A},P)\to(C_{[0,\ 1]}\mathcal{C}_{[0,1]})\). Of course, this \(X\) and \(Y\) are also different realizations of Brownian motion.)

**Definition 4.4** (Finite-dimensional convergence, \(\ \to_{fd}\)): Suppose \(X,X_{1},X_{2},\ldots\) denote processes with image space \((M_{T},\mathcal{M}_{T})\). If the convergence in distribution

\[\pi_{t_{1},\ldots,t_{k}}(X_{n})=(X_{n}(t_{1}),\ldots,X_{n}(t_{k}))\to_{d}(X(t_{ 1}),\ldots,X(t_{k}))=\pi_{t_{1},\ldots,t_{k}}(X) \tag{2}\]

holds for all \(k\geq 1\) and all \(t_{1},\ldots,t_{k}\) in \(T\), then we write \(X_{n}\to_{fd}X\) as \(n\to\infty\), and we say that the _finite-dimensional distributions_ of \(X_{n}\)_converge_ to those of \(X\).

**The General Stochastic Process**

**Notation 4.1** (\((R_{T},{\cal B}_{T})\)): We now adopt the convention that

\[(R_{T},{\cal B}_{T}) \tag{3}\]

 denotes the measurable function space with

\[R_{T}\equiv\prod\nolimits_{t\in T}R_{t}\]

,

where each

\[R_{t}\]

 is a copy of the real line. Thus

\[R_{T}\]

 consists of all possible real-valued functions on

\[T\]

, and

\[{\cal B}_{T}\]

 is the smallest

\[\sigma\]

-field with respect to which all

\[\pi_{t}\]

 are measurable. We call a process

\[X:\;(\Omega,{\cal A},P)\to(R_{T},{\cal B}_{T})\]

 a _general stochastic process_. We note that a general stochastic process is also a process. But we do not yet know what

\[{\cal B}_{T}\]

 looks like.

A set

\[B_{T}\in{\cal B}_{T}\]

 is said to have _countable base_

\[t_{1},t_{2},\ldots\]

 if (4) \[B_{T}=\pi_{t_{1},t_{2},\ldots}^{-1}(B_{\infty})\quad\mbox{for some $B_{\infty}\in{\cal B}_{\infty}$}\]

;

here \({\cal B}_{\infty}\) is the countably infinite-dimensional

\[\sigma\]

-field of section 5.3. Let

\[{\cal B}_{\cal C}\]

 denote the _class of countable base sets_ defined by (5) \[{\cal B}_{\cal C}\equiv\{\mbox{ $B_{T}\in{\cal B}_{T}:B_{T}$ has a countable base}\}.\]

(Recall

\[{\cal F}(X_{1},X_{2},\ldots)\]

 and

\[{\cal F}(X_{s}:\,s\leq t)\]

 measurability from section 2.5.) \(\Box\)

**Proposition 4.1** (Measurability in

\[(R_{T},{\cal B}_{T})\]

): Now,

\[{\cal B}_{\cal C}\]

 is a

\[\sigma\]

-field. In fact,

\[{\cal B}_{\cal C}\]

 is the smallest

\[\sigma\]

-field relative to which all

\[\pi_{t}\]

 are measurable; that is, (6) \[{\cal B}_{T}={\cal B}_{\cal C}\]

Also (generalizing proposition 5.2.1),

\[X \tag{7}\]

 is

\[{\cal B}_{T}\]

-

-measurable if and only if

\[X_{t}\]

 is

\[{\cal B}\]

-measurable for each

\[t\in T\]

**Proof.** Clearly,

\[{\cal B}_{T}\]

 is the smallest

\[\sigma\]

-field containing

\[{\cal B}_{\cal C}\]

; so (6) will follow from showing that

\[{\cal B}_{\cal C}\]

 is a

\[\sigma\]

-field. Now,

\[{\cal C}\]

 is closed under complements, since

\[\pi_{t_{1},t_{2},\ldots}^{-1}(B_{\infty})^{\cal C}=\pi_{t_{1},t_{2},\ldots}^{- 1}(B_{\infty}^{\cal C})\]

. Suppose that

\[B_{1},B_{2},\ldots\]

 in

\[{\cal B}_{\cal C}\]

 have countable bases

\[T_{1},T_{2},\ldots\]

, and let

\[T_{0}=\cup_{m=1}^{\infty}T_{m}\]

. Then

\[\mbox{using the countable set of distinct coordinates in $T_{0}$, reexpress each $B_{m}$ as $B_{m}=\pi_{T_{0}}^{-1}(B_{m}^{\infty})$ for some $B_{m}^{\infty}\in{\cal B}_{\infty}$. Then $\cup_{m=1}^{\infty}B_{m}=\pi_{T_{0}}^{-1}(\cup_{m=1}^{\infty}B_{m}^{\infty})$ is in ${\cal B}_{\cal C}$. Thus ${\cal B}_{\cal C}$ is closed under countable unions. Thus ${\cal B}_{C}$ is a $\sigma$-field.}\]

Now to establish (7): Suppose \(X\) is

\[B_{T}\]

-

-measurable. Then

\[X_{t}^{-1}(B)=X^{-1}(\pi_{t}^{-1}(B))\in{\cal A}\quad\mbox{for $B\in{\cal B}$},\]

so that each

\[X_{t}\]

 is

\[{\cal B}\]

-measurable. Suppose that each

\[X_{t}\]

 is

\[{\cal B}\]

-

-measurable. Then exercise 5.3.1 shows that

\[(X_{t_{1}},X_{t_{2}},\ldots)\]

 is

\[{\cal B}_{\infty}\]

-

-measurable for all sequences

\[t_{1},t_{2},\ldots\]

 of elements of

\[T\]

. That is,

\[X^{-1}({\cal B}_{\cal C})\subset{\cal A}\]

. Since

\[{\cal B}_{T}={\cal B}_{\cal C}\]

, we thus have

\[X^{-1}({\cal B}_{T})\subset{\cal A}\]

, and hence

\[X\]

 is

\[{\cal B}_{T}\]

-

-measurable. \(\Box\)

**Remark 4.1** (Consistency of induced distributions in

\[(R_{T},{\cal B}_{T})\]

): Any general stochastic process

\[X:\;(\Omega,{\cal A},P)\to(R_{T},{\cal B}_{T})\]

 has a _family of induced distributions_ (8) \[P_{t_{1},\ldots,t_{k}}^{*}(B_{k})=P(X^{-1}\circ\pi_{t_{1},\ldots,t_{k}}^{-1}(B_ {k}))\quad\mbox{for all $B_{k}\in{\cal B}_{k}$}\]

for all

\[k\geq 1\]

 and all

\[t_{1},\ldots,t_{k}\in T\]

. These distributions are necessarily _consistent_ in the sense that

\[P_{t_{1},\ldots,t_{k}}^{*}(B_{1}\times\cdots\times B_{i-1}\times R \times B_{i+1}\times\cdots\times B_{k})\] \[\qquad=P_{t_{1},\ldots,t_{i-1},t_{i+1},\ldots,t_{k}}^{*}(B_{1} \times\cdots\times B_{i-1}\times B_{i+1}\times\cdots\times B_{k}) \tag{9}\]for all \(k\geq 1\), all \(B_{1},\ldots,B_{k}\in{\cal B}\), all \(1\leq i\leq k\), and all \(t_{1},\ldots,t_{k}\in T\). (The next result gives a converse. It is our fundamental result on the existence of stochastic processes with specified distributions.) \(\square\)

**Theorem 4.1** (Kolmogorov's consistency theorem): Given a consistent set of distributions as in (9), there exists a distribution \(P\) on \((R_{T},{\cal B}_{T})\) such that the identity map \(X(\omega)=\omega\), for all \(\omega\in R_{T}\), is a general stochastic process \(X:(R_{T},{\cal B}_{T},P)\to(R_{T},{\cal B}_{T})\) whose family of induced distributions is the \(P^{*}_{t_{1},\ldots,t_{k}}\) of (9).

**Exercise 4.2**: Prove theorem 4.1. (Define \(P^{*}(B)=P(\pi_{T_{i}}^{-1}(B))\) for \(B\in{\cal B}_{\cal C}\) and each countable subset \(T_{i}\) of \(T\). Use notational ideas from the proof of proposition 4.1 to show easily that \(P^{*}(\cdot)\) is well-defined and countably additive.)

**Example 4.1** (Comment on \((R_{[0,1]},{\cal B}_{[0,1]})\)): The typical function \(x\) in \(R_{T}\) has no smoothness properties. Let \(T=[0,1]\) and let \({\cal C}\) denote the subset of \(R_{[0,1]}\) that consists of all functions that are continuous on \([0,1]\). We now show that

\[{\cal C}\notin{\cal B}_{[0,1]}. \tag{10}\]

Let \((\Omega,{\cal A},P)\) denote Lebesgue measure on the Borel subsets of \([0,1]\). Let \(\xi(\omega)=\omega\). Now let \(X:(\Omega,{\cal A},P)\to(R_{[0,1]},{\cal B}_{[0,1]})\) via \(X_{t}(\omega)=0\) for all \(\omega\in\Omega\) and for all \(t\in T\). Let \(Y:(\Omega,{\cal A},P)\to(R_{[0,1]},{\cal B}_{[0,1]})\) via \(Y_{t}(\omega)=1_{\{t\}}(\xi(\omega))\). Now, all finite-dimensional distributions of \(X\) and \(Y\) are identical. Note, however, that \([\omega:X(\omega)\in{\cal C}]=\Omega\), while \([\omega\ :\ Y(\omega)\in{\cal C}]=\emptyset\). Thus \({\cal C}\) cannot be in \({\cal B}_{[0,1]}\). \(\square\)

**Smoother Realizations of General Stochastic Processes**

Suppose now that \(X\) is a process of the type \(X:\,(\Omega,{\cal A},P)\to(R_{T},{\cal B}_{T},P^{*})\). As the previous example shows, \(X\) is not the unique process from \((\Omega,{\cal A},P)\) that induces the distribution \(P^{*}\) on \((R_{T},{\cal B}_{T})\). We now let \(M_{T}\) denote a proper subset of \(R_{T}\) and agree that \({\cal M}_{T}\) denotes the \(\sigma\)-field generated by the finite-dimensional subsets of \(M_{T}\). Suppose now that \(X(\omega)\in M_{T}\) for all \(\omega\in\Omega\). Can \(X\) be viewed as a process \(X:\,(\Omega,{\cal A},P)\to(M_{T},{\cal M}_{T},\tilde{P})\) such that \((M_{T},{\cal M}_{T},\tilde{P})\) has the same finite-dimensional distributions as does \((R_{T},{\cal B}_{T},P^{*})\)? We now show that the answer is necessarily yes. Interesting cases arise when the functions of the \(M_{T}\) above have smoothness properties such as continuity. The next result is very important and useful.

**Theorem 4.2** (Smoother realizations of processes): Consider an arbitrary measurable mapping \(X:(\Omega,{\cal A},P)\to(R_{T},{\cal B}_{T},P^{*})\).

(i) Let \(M_{T}\subset R_{T}\). Then we can view \(X\) as a process \(X:(\Omega,{\cal A})\to(M_{T},{\cal M}_{T})\) if and only if every sample path \(X.(\omega)=X(\cdot,\omega)\) is in \(M_{T}\) and every \(X_{t}(\cdot)\equiv X(t,\cdot)\) is a random variable.

(ii) Let \(X(\Omega)\subset M_{T}\subset R_{T}\). Then \(X:(\Omega,{\cal A},P)\to(M_{T},{\cal M}_{T},\tilde{P})\). where the finite- dimensional distributions of \((M_{T},{\cal M}_{T},\tilde{P})\) are the same as those of \((R_{T},{\cal B}_{T},P^{*})\).

(iii) Comment: All this is true even when \(M_{T}\) is not in the class \({\cal B}_{T}\).

**Proof.** : (i) (\(\Leftarrow\)) Note first that \(M_{T}\cap{\cal B}_{T}={\cal M}_{T}\) (recall definition 4.2). Moreover, when \(X(\Omega)\subset M_{T}\), it necessarily follows that

(a) \[\begin{array}{l}X^{-1}({\cal M}_{T})=X^{-1}(M_{T}\cap{\cal B}_{T})=X^{-1}(M_ {T})\cap X^{-1}({\cal B}_{T})=\Omega\cap X^{-1}({\cal B}_{T})\\ =X^{-1}({\cal B}_{T}).\end{array}\]Since each \(X_{t}\) is a rv, we have \(X^{-1}({\cal B})\subset{\cal A}\) by (7). Thus \(X^{-1}({\cal M}_{T})\subset{\cal A}\), and we see that \(X\) is indeed an \({\cal M}_{T}-{\cal A}-\) measurable mapping from \(\Omega\) to \(M_{T}\). Note further that the natural pairs of generator sets \((\pi_{t_{1},\ldots,t_{k}}^{-1}((-\infty,r_{1}]\ \times\cdots\times(-\infty,r_{k}])\) in \({\cal B}_{T}\), or \(\pi_{t_{1},\ldots,t_{k}}^{-1}((-\infty,r_{1}]\times\cdots\times(-\infty,r_{k}]) \cap M_{T}\) in \({\cal M}_{T})\) have the same inverse images under \(X\); thus the finite dimensional distributions induced from \((\Omega,{\cal A})\) to \((R_{T},{\cal B}_{T})\) and to \((M_{T},{\cal M}_{T})\) are identical.

(\(\Rightarrow\)) Clearly, \(X\) : \(\Omega\to M_{T}\) implies \(X\) : \(\Omega\to R_{T}\). Also, for any \(t\in T\) and any \(B\in{\cal B}\),

\[X^{-1}(\pi_{t}^{-1}(B))=X^{-1}(M_{T}\cap\pi_{t}^{-1}(B))\qquad \mbox{since $X:\Omega\to M_{T}$}\] \[\qquad\in X^{-1}({\cal M}_{T})\qquad\mbox{since ${\cal M}_{T}=M_{T }\cap{\cal B}_{T}$}\]

(b) \[\in{\cal A}\qquad\mbox{since $X$ is ${\cal M}_{T}-{\cal A}-$measurable}.\]

Thus each \(X_{t}\) is a rv, and so \(X\) is \({\cal B}_{T}-{\cal A}-\)measurable by (7).

(ii) This is now clear, and it summarizes the most useful part of this theorem. \(\Box\)

**Exercise 4.3**  Let \({\cal M}\) denote any non-void class of subsets of \(\Omega\), and let \(M\) denote any non-void subset. Show that

\[\sigma[{\cal M}]\cap M=\sigma[{\cal M}\cap M]. \tag{11}\]

**Remark 4.2**  It is interesting to consider the case where \(M_{T}\) is a countable or finite set. The resulting \((M_{T},{\cal M}_{T},\bar{P})\) is the natural probability space. \(\Box\)

## Chapter 6 Distribution and Quantile Functions

### 1 Character of Distribution Functions

Let \(X:(\Omega,\mathcal{A},P)\rightarrow(R,\mathcal{B},P_{X})\) be a rv with _distribution function_\((df)F_{X}\), where

\[F_{X}(x)\equiv P(X\leq x)=P_{X}((-\infty,x])\qquad\mbox{for}-\infty<x<\infty. \tag{1}\]

Then \(F\equiv F_{X}\) was seen earlier to satisfy

\[F\mbox{ is }\nearrow\mbox{ and right continuous, with }F(-\infty)=0\mbox{ and }F(+\infty)=1. \tag{2}\]

Because of the proposition below, any function \(F\) satisfying (2) will be called a df. (If \(F\) is \(\nearrow\), right continuous, \(0\leq F(-\infty)\), and \(F(+\infty)\leq 1\), we earlier agreed to call \(F\) a _sub-df_. As usual, \(F(a,b]\ \equiv F(b)-F(a)\) denotes the increments of \(F\), and \(\triangle F(x)\equiv F(x)-F_{-}(x)=F(x)-F(x-)\) is the mass of \(F\) at \(x\).)

(a) Call \(F\)_discrete_ if \(F\) is of the form \(F(\cdot)=\sum_{j}b_{j}1_{[a_{j},\infty)}(\cdot)\) with \(\sum_{j}b_{j}=1\), where the \(a_{j}\) form a non-void finite or countable set. Such measures \(\mu_{F}\) have Radon-Nikodym derivative \(\sum_{j}b_{j}1_{\{a_{j}\}}(\cdot)\) with respect to counting measure on the \(a_{j}\)'s.

(b) A df \(F\) is called _absolutely continuous_ if \(F(\cdot)=\int_{\infty}^{\cdot}f(y)\,d\lambda(y)\) for some \(f\geq 0\) that integrates to \(1\) over \(R\). The corresponding measure has Radon-Nikodym derivative \(f\) with respect to Lebesgue measure \(\lambda\); this \(f\) is also called a _probability density_. Moreover, \(F\) is an absolutely continuous function and the ordinary derivative \(F^{\prime}\) of the df \(F\) exists a.e. \(\lambda\) and satisfies \(F^{\prime}=f\) as \(\lambda\).

(c) A df \(F\) is called _singular_ if \(\mu_{F}(N^{c})=0\) for a \(\lambda\)-null set \(N\).

**Proposition 1.1** (There exists an \(X\) with df \(F\)) If \(F\) satisfies (2), then there exists a probability space \((\Omega,\mathcal{A},P)\) and a rv \(X:(\Omega,\mathcal{A},P)\rightarrow(R,\mathcal{B})\) for which the df of \(X\) is \(F\). We write \(X\cong F\).

**Proof**.  Example 2.2.1 shows that \(X(r)=r\) on \((R,{\cal B},\mu_{F})\) is one example. \(\Box\)

**Theorem 1.1** (Decomposition of a df) Any df \(F\) can be decomposed as

\[F=F_{d}+F_{c}=F_{d}+F_{s}+F_{ac}=(F_{d}+F_{s})+F_{ac},\]

where \(F_{d},F_{c},F_{s},\) and \(F_{ac}\) are the unique sub-dfs of the following types (unique among those sub-dfs equal to \(0\) at \(-\infty\)):

\[\begin{array}{ll}(4)&F_{d}\mbox{ is a step function of the form }\sum_{j}b_{j}1_{[a_{j},\infty)}\mbox{ (with all}\,b_{j}>0).\\ (5)&F_{c}\mbox{ is continuous}.\\ (6)&F_{s}\mbox{ and }F_{s}+F_{d}\mbox{ are both singular with respect to Lebesgue measure }\lambda.\\ &F_{ac}(\cdot)=\int_{-\infty}^{\cdot}f_{ac}(y)\,d\lambda(y)\\ (7)&\mbox{ for some }f_{ac}\geq 0\mbox{ that is finite, measurable, and unique a.e. }\lambda,\\ &\mbox{ and this }F_{ac}(\cdot)\mbox{ is absolutely continuous on the whole real line}.\end{array}\]

**Proof**.  Let \(\{a_{j}\}\) denote the set of all discontinuities of \(F\), which can only be jumps; and let \(b_{j}\equiv F(a_{j})-F_{-}(a_{j})\). There can be only a countable number of jumps, since the number of jumps of size exceeding size \(1/n\) is certainly bounded by \(n\). Now define \(F_{d}\equiv\sum_{j}b_{j}1_{[a_{j},\infty)},\) which is obviously \(\nearrow\) and right continuous, since \(F_{d}(x,y]\leq F(x,y]\searrow 0\) as \(y\searrow x\) (the inequality holds, since the sum of jump sizes over every finite number of jumps between \(a\) and \(b\) is clearly bounded by \(F(x,y]\), and then just pass to the limit). Define \(F_{c}=F-F_{d}\). Now, \(F_{c}\) is \(\nearrow\), since for \(x\leq y\) we have \(F_{c}(x,y]=F(x,y]-F_{d}(x,y]\geq 0\). Now, \(F_{c}\) is the difference of right-continuous functions, and hence is right continuous; it is left continuous, since for \(x\nearrow y\) we have

\[\mbox{(a)}\qquad F_{c}(x,\ y]=F(x,y]-\sum_{x<a_{j}\leq y}b_{j}=F_{-}(y)-F(x)- \sum_{x<a_{j}<y}b_{j}\to 0-0=0.\]

We turn to the uniqueness of \(F_{d}.\) Assume that \(F_{c}+F_{d}=F=G_{c}+G_{d}\) for some other \(G_{d}\equiv\sum_{j}\bar{b}_{j}1_{[\bar{a}_{j},\infty)}\) with distinct \(\bar{a}_{j}\)'s and \(\sum_{j}\bar{b}_{j}\leq 1\). Then \(F_{d}-G_{d}=G_{c}-F_{c}\) is continuous. If \(G_{d}\neq F_{d}\), then either some jump point or some jump size disagrees. No matter which disagrees, at some \(a\) we must have

\[\mbox{(b)}\qquad\triangle F_{d}(a)-\triangle G_{d}(a)\neq 0,\]

contradicting the continuity of \(G_{c}-F_{c}=F_{d}-G_{d}\). Thus \(G_{d}=F_{d}\), and hence \(F_{c}=G_{c}\). This completes the first decomposition.

We now turn to the further decomposition of \(F_{c}\). Associate a measure \(\mu_{c}\) with \(F_{c}\) via \(\mu_{c}((-\infty,x])=F_{c}(x)\). Then the Lebesgue decomposition theorem shows that \(\mu_{c}=\mu_{s}+\mu_{ac}\), where \(\mu_{s}(B)=0\) and \(\mu_{ac}(B^{c})=0\) for some \(B\in{\cal B}\); we say that \(\mu_{s}\) and \(\mu_{ac}\) are orthogonal. Moreover, this same Lebesgue theorem implies the claimed uniqueness and shows that \(f_{ac}\) exists with the uniqueness claimed. Now, \(F_{ac}(x)\equiv\mu_{ac}((-\infty,x])=\int_{-\infty}^{x}f_{ac}(y)\,dy\) is continuous by \(F_{ac}(x,y]\leq\mu_{ac}(x,y]\to 0\) as \(y\to x\) or as \(x\to y\). Thus \(F_{s}\equiv F_{c}-F_{ac}\) is continuous, and \(F_{s}(x)=\mu_{s}((-\infty,x])\). In fact, \(F_{ac}\) is absolutely continuous on \(R\) by the absolute continuity of the integral. (Now \(F_{c}=F_{s}+F_{ac}\) decomposes \(F_{c}\) with respect to \(\lambda\), while \(F=(F_{d}+F_{s})+F_{ac}\) decomposes \(F\) with respect to \(\lambda\).) \(\Box\)

**Example 1.1** (Lebesgue singular df) Define the _Cantor set_\(C\) by

\[C\equiv\{x\in[0,1]:x=\sum_{n=1}^{\infty}2a_{n}/3^{n},\mbox{ with all }a_{n}\mbox{ equal to }0\mbox{ or }1\}.\](Thus the Cantor set is obtained by removing from \([0,1]\) the open interval \((\frac{1}{3},\frac{2}{3})\) at stage one, then the open intervals \((\frac{1}{9},\frac{2}{9})\) and \((\frac{7}{9},\frac{8}{9})\) at stage two,....) Finally, we define \(F\) on \(C\) by

(9) \(F(\sum_{n=1}^{\infty}2a_{n}/3^{n})=\sum_{n=1}^{\infty}a_{n}/2^{n}\).

Figure 1.1 Lebesgue singular function.

Now note that \(\{F(x):x\in C\}=[0,1]\), since the right-hand side of (9) represents all of \([0,1]\) via dyadic expansion. We now define \(F\) "linearly" on \(C^{c}\) (the first three "components" are shown in figure 1.1 above). Since the resulting \(F\) is \(\nearrow\) and achieves every value in \([0,1]\), it must be that \(F\) is continuous. Now, \(F\) assigns no mass to the "flat spots" whose lengths sums to \(1\) since \(\frac{1}{3}+\frac{2}{9}+\frac{4}{27}+\cdots=\frac{1/3}{1-2/3}=1\). Thus \(F\) is singular with respect to Lebesgue measure \(\lambda\), using \(\lambda(C^{c})=1\) and \(\mu_{F}(C^{c})=0\). Call this \(F\) the _Lebesgue singular_\(df\). (The theorem in the next section shows that removing the flat spots does, even for a general df \(F\), leave only the essentials.) We have, in fact, shown that

(10) \(F:C\rightarrow[0,1]\) is 1-1, is \(\uparrow,\)and is continuous; so \(F^{-1}:[0,1]\to C\) is \(1-1\). \(\Box\)

**Exercise 1.1**  Let \(X\cong N(0,1)\) (as in (A.1.22) below), and let \(Y\equiv 2X\).

(a) Is the df \(F(\cdot,\cdot)\) of \((X,Y)\) continuous?

(b) Does the measure \(\mu_{F}\) on \(R_{2}\) have a density with respect to two-dimensional Lebesgue measure? (Hint. Appeal to corollary 2 to Fubini's theorem.)

**Exercise 1.2**  Show that the Cantor set \(C\) is _perfect_ (thus, each \(x\in C\) is an accumulation point of \(C\)) and _totally disconnected_ (between any \(c_{1}<c_{2}\) in \(C\) there is an interval that lies entirely in \(C^{c}\)). (Note that the cardinality of \(C\) equals that of \([0,1]\). At which points is \(1_{C}(\cdot)\) continuous?

**Definition 1.1**  Two rvs \(X\) and \(Y\) are said to be of the same _type_ if \(Y\cong aX+b\) for some \(a>0\). Their dfs are also said to be of the same type.

## 2 Properties of Distribution Functions

**Definition 2.1** The _support_ of a given df \(F\equiv F_{X}\) is defined to be the minimal closed set \(C\) having \(P(X\in C)=1\). A point \(x\) is a _point of increase_ of \(F\) if every open interval \(U\) containing \(x\) has \(P(X\in U)>0\). A _realizable \(t\)-quantile of \(F\)_, for \(0<t<1\), is any value \(z\) for which \(F(z)=t\). (Such a \(z\) need not exist.) Define \(U_{t}\) to be the maximal open interval of \(x\)'s for which \(F(x)=t\) (this flat spot will be an interval, since \(F\) is \(\nearrow\)).

**Theorem 2.1** (Jumps and flat spots): Let \(C\) denote the support of \(F\). Then:

(a) \(C\equiv(\bigcup_{0\leq t\leq 1}U_{t})^{c}\) is a closed set having \(P(C)=1\).

(b) \(C\) is equal to the set of all points of increase.

(c) \(C\) is the support of \(F\).

(d) \(F\) has at most a countable number of discontinuities, and these discontinuities are all discontinuities of the jump type.

(e) \(F\) has an at most countable number of flat spots (the nonvoid \(U_{t}\)'s). These are exactly those \(t\)'s that have more than one realizable \(t\)-quantile.

(We will denote jump points and jump sizes of \(F\) by \(a_{i}\)'s and \(b_{i}\)'s. The \(t\) values and the \(\lambda(U_{t})\) values of the multiply realizable \(t\)-quantiles will be seen in the proof of proposition 6.3.1 below to correspond to the jump points \(c_{j}\) and the jump values \(d_{j}\) of the function \(K(\cdot)\equiv F^{-1}(\cdot)\), and there are at most countably many of them.)

**Proof.*** (a) For each \(t\) there is a maximal open interval \(U_{t}\) (possibly void) on which \(F\) equals \(t\), and it is bounded for each \(0<t<1\). Now, \(P(X\in U_{t})=0\) using proposition 1.1.2. Note that \(C\equiv(\cup_{t}U_{t})^{c}\) is closed (since the union of an arbitrary collection of open sets is open). Hence \(C^{c}=\cup_{0\leq t\leq 1}U_{t}=\cup(a_{n},b_{n})\), where \((a_{1},b_{1}),\ldots\) are (at most countably many) disjoint open intervals, and all those with \(0<t<1\) must be finite. Now, by proposition 1.1.2, for the finite intervals we have \(P(X\in(a_{n},b_{n}))=\lim_{\epsilon\to 0}P(X\in[a_{n}+\epsilon,b_{n}- \epsilon])=\lim_{\epsilon\to 0}0=0\), where \(P(X\in[a_{n}+\epsilon,b_{n}-\epsilon])=0\) holds since this finite closed interval must have a finite subcover by \(U_{t}\) sets. If \((a_{n},b_{n})=(-\infty,b_{n})\), then \(P(X\in(-\infty,b_{n}))=0\), since \(P(X\in[-1/\epsilon,b_{n}-\epsilon])=0\) as before. An analogous argument works if \((a_{n},b_{n})=(a_{n},\infty)\). Thus \(P(X\in C^{c})=0\) and \(P(X\in C)=1\). Note that the \(U_{t}\)'s are just the \((a_{n},b_{n})\)'s in disguise; each \(U_{t}\subset\) some \((a_{n},b_{n})\), and hence \(U_{t}=\) that \((a_{n},b_{n})\). Thus \(U_{t}\) is nonvoid for at most countably many \(t\)'s.

(b) Let \(x\in C\). We will now show that it is a point of increase. Let \(U\) denote a neighborhood of \(x\), and let \(t\equiv F(x)\). Assume \(P(U)=0\). Then \(x\in U\subset U_{t}\subset C^{c}\), which is a contradiction of \(x\in C\). Thus all points \(x\in C\) are points of increase. Now suppose conversely that \(x\) is a point of increase. Assume \(x\notin C\). Then \(x\in\) some \((a_{n},b_{n})\) having \(P(X\in(a_{n},b_{n}))=0\), which is a contradiction. Thus \(x\in C\). Thus the closed set \(C\) is exactly the set of points of increase.

(c) Assume that \(C\) is not the minimal closed set having probability 1. Then \(P(\tilde{C})=1\) for some closed \(\tilde{C}\subsetneq C\). Let \(x\in C\backslash\tilde{C}\) and let \(t=F(x)\). Since \(\tilde{C}^{c}\) is open, there is an open interval \(V_{x}\) with \(x\in V_{x}\subset\tilde{C}^{c}\) and \(P(X\in V_{x})=0\). Thus \(x\in V_{x}\subset(\)some \(U_{t})\subset C^{c}\). So \(x\notin C\), which is a contradiction. Thus \(C\) is minimal.

So, (d) and (e) follow. See also the summary following proposition 6.3.1.

## 3 The Quantile Transformation

**Definition 3.1** (Quantile function): For any df \(F(\cdot)\) we define the _quantile function_\((qf)\) (which is the inverse of the df) by

\[K(t)\equiv F^{-1}(t)\equiv\inf\{x:F(x)\geq t\}\qquad\mbox{ for }0<t<1. \tag{1}\]

**Theorem 3.1** (The inverse transformation): Let

\[X\equiv K(\xi)\equiv F^{-1}(\xi),\qquad\mbox{ where }\xi\cong\mbox{Uniform}(0,1). \tag{2}\]

The following are all true.

\[[X\leq x]=[\xi\leq F(x)]\qquad\mbox{ for every real }x. \tag{3}\]

\[1_{[X\leq.]}=1_{[\xi\leq F(\cdot)]}\qquad\qquad\mbox{ on }R,\mbox{ for every }\omega. \tag{4}\]

\[X\equiv K(\xi)\equiv F^{-1}(\xi)\qquad\mbox{ has df }F. \tag{5}\]

\[1_{[X<(\cdot)]}=1_{[\xi<F_{-}(\cdot)]}\qquad\mbox{ on }R,\mbox{ for a.e. }\omega; \tag{6}\]

failure occurs if and only if \(\xi(\omega)\) equals the height of a flat spot of \(F\).

Proof.: Fix an arbitrary \(x\). Now, \(\xi\leq F(x)\) implies \(X=F^{-1}(\xi)\leq x\) by (1). If \(X=F^{-1}(\xi)\leq x\), then \(F(x+\epsilon)\geq\xi\) for all \(\epsilon>0\); so right continuity of \(F\) implies \(F(x)\geq\xi\). Thus (3) holds; (4) and (5) are then immediate.

If \(\xi(\omega)=t\) where \(t\) is not in the range of \(F\), then (6) holds. If \(\xi(\omega)=t\) where \(F(x)=t\) for exactly one \(x\), then (6) holds. If \(\xi(\omega)=t\) where \(F(x)=t\) for at least two distinct \(x\)'s, then (6) fails; theorem 6.2.1 shows that this can happen for at most a countable number of \(t\)'s. (Or: Graph a df \(F\) that exhibits the three types of points \(t\), and the rest is trivial with respect to (6), since the value of \(F\) at any other point is immaterial. Specifically, (6) holds for \(\omega\) unless \(F\) has a flat spot at height \(t\equiv\xi(\omega)\). Note figure 3.1.)

**Definition 3.2** (Convergence in quantile) Let \(K_{n}\) denote the qf associated with df \(F_{n}\), for each \(n\geq 0\). We write \(K_{n}\to_{d}K_{0}\) to mean that \(K_{n}(t)\to K_{0}(t)\) at each continuity point \(t\) of \(K_{0}\) in \((0,1)\). We then say that \(K_{n}\)_converges in quantile_ to \(K_{0}\), or \(K_{n}\)_converges in distribution_ to \(K_{0}\).

**Proposition 3.1** (Convergence in distribution equals convergence in quantile)

\[F_{n}\to_{d}F\qquad\mbox{ if and only if}\qquad K_{n}\to_{d}K. \tag{7}\]

**Proof**. Suppose \(F_{n}\to_{d}F\). Let \(t\in(0,1)\) be such that there is at most one value \(x\) having \(F(x)=t\) (that is, this is not a multiply realizable \(t\)-quantile). Let \(z\equiv F^{-1}(t)\).

First: We have \(F(x)<t\) for \(x<z\), where \(x\) will always denote a continuity point of \(F\). Thus \(F_{n}(x)<t\) for \(n\geq\) (some \(N_{x}\)). Thus \(F_{n}^{-1}(t)\geq x\) for \(n\geq N_{x}\). Thus \(\liminf F_{n}^{-1}(t)\geq x\), when \(x<z\) is a continuity point of \(F\). Thus \(\liminf F_{n}^{-1}(t)\geq z\), since there are continuity points \(x\) that \(\nearrow z\). Second: We also have \(F(x)>t\) for \(z<x\), with \(x\) a continuity point. Thus \(F_{n}(x)>t\), and hence \(F_{n}^{-1}(t)\leq x\) for \(n\geq\) (some \(N_{x}\)). Thus \(\limsup F_{n}^{-1}(t)\leq x\). Thus \(\limsup F_{n}^{-1}(t)\leq z\), since there are continuity points \(x\) that \(\searrow z\). Thus \(K_{n}(t)=F_{n}^{-1}(t)\to z=K(t)\). The proof of the converse is virtually identical. \(\Box\)

**Exercise 3.0** (Give the proof of the converse for the previous proposition.

**Summary** \(F_{n}^{-1}(t)\to F^{-1}(t)\) for all but at most a countably infinite number of \(t\)'s (namely, for all but those \(t\)'s that have multiply realizable \(t\)-quantiles; these correspond to the heights of flat spots of \(F\), and these flat spot heights \(t\) are exactly the discontinuity points of \(K\)).

**Exercise 3.1** (Left continuity of \(K\)) Show that \(K(t)=F^{-1}(t)\) is left continuous on \((0,1)\). (Note that \(K\) is discontinuous at \(t\in(0,1)\) if and only if the corresponding \(U_{t}\) is nonvoid (see theorem 6.2.1). Likewise, the jump points \(c_{j}\) and the jump sizes \(d_{j}\) of \(K(.)\) are equal to the \(t\) values and the \(\lambda(U_{t})\) values of the multiply realizable \(t\)-quantiles. We earlier agreed to use \(a_{i}\) and \(b_{i}\) for the jump points and jump sizes of the associated df \(F\).)

**Exercise 3.2** (Properties of dfs) (i) For any df \(F\) we have

\[F\circ F^{-1}(t)\geq t\qquad\mbox{ for all }0\leq t\leq 1,\]

and equality fails if and only if \(t\in(0,1)\) is not in the range of \(F\) on \([-\infty,\infty]\).

(ii) (The probability integral transformation) If \(X\) has a continuous df \(F\), then \(F(X)\cong\mbox{Uniform}(0,1)\). In fact, for any df \(F\),

\[P(F(X)\leq t)\leq t\qquad\mbox{ for all }0\leq t\leq 1,\]

with equality failing if and only if \(t\) is not in the closure of the range of \(F\).

(iii) For any df \(F\) we have

\[F^{-1}\circ F(x)\leq x\qquad\mbox{ for all }-\infty<x<\infty,\]

and equality fails if and only if \(F(y)=F(x)\) for some \(y<x\). Thus

\[P(F^{-1}\circ F(X)\neq X)=0\qquad\mbox{ whenever }X\cong F.\]

(iv) If \(F\) is a continuous df and \(F(X)\cong\mbox{Uniform}(0,\,1)\), then \(X\cong F\).

(v) Graph \(F\circ F^{-1}\) and \(F^{-1}\circ F\) for the df \(F\) in figure 6.3.1.

**Proposition 3.2**: **(The randomized probability integral transformation)** Let \(X\) denote an arbitrary rv. Let \(F\) denote its df, and let \((a_{j},b_{j})\)'s denote an enumeration of whatever pairs (jump point, jump size) the df \(F\) possesses. Let \(\eta_{1},\eta_{2},\ldots\) denote iid Uniform(0, 1) rvs (that are also independent of \(X\)). Then both

\[\xi^{o}\equiv F(X)-\sum_{j}b_{j}\eta_{j}1_{[X=a_{j}]}\cong\mbox{ Uniform}(0,1)\qquad\mbox{ and} \tag{8}\] \[X=F^{-1}(\xi^{o})=K(\xi^{o}). \tag{9}\]

(We have reproduced the original \(X\) from a Uniform(0, 1) rv that was defined using both \(X\) and some independent extraneous variation. Note figure 3.1.)

**Proof**. We have merely smoothed out the mass \(b_{j}\) that \(F(X)\) placed at \(F(a_{j})\) by subtracting the random fractional amount \(\eta_{j}b_{j}\) of the mass \(b_{j}\). \(\Box\)

**Exercise 3.3** (Change of variable)** Let \(Y\cong G\) and \(X=H^{-1}(Y)\cong F\), where \(H\) is \(\nearrow\) and right continuous on the real line with left-continuous inverse \(H^{-1}\).

(a) Use the theorem of the unconscious statistician to conclude that

\[\int_{(-\infty,H(x)]}m(H^{-1})\,dG=\int_{(-\infty,x]}m\ dF\quad\mbox{ for measurable functions $m\geq 0$}, \tag{10}\]

since \((H^{-1})^{-1}((-\infty,x])=\{t:H^{-1}(t)\leq x\}=\{t:t\leq H(x)\}\) as in (3). Check it.

(b) Let \(G=I,H=F\), and \(Y=\xi\cong\mbox{Uniform}(0,\,1)\) above, for any df \(F\). Let \(g\) denote any measurable function. Then (via part (a), or via (2) and (3)), show that

\[\int_{0}^{F(x)}g(F^{-1}(t))\,dt=\int_{(-\infty,x]}g\ dF,\quad\mbox{and}\quad \int_{F_{-}(x)}^{1}g(F^{-1}(t))\,dt=\int_{[x,\infty)}g\ dF. \tag{11}\]

(c) Exercise 3.5.3(b) established (13). Still, now use (6.4.1) below to verify both

\[\int_{[0,x]}y\ dF_{|X|}(y)=\int_{0}^{x}P(|X|>y)\,dy-xP(|X|>x)\quad\mbox{and} \tag{12}\] \[\int_{[x,\infty)}y\ dF_{|X|}(y)=xP(|X|\geq x)+\int_{x}^{\infty}P( |X|\geq y)\,dy\quad\mbox{even if infinite}\]

Interpret formulas (12)-(13) in terms of various shaded areas shown in figure 8.4.1. Do the same for (11) when \(g(y)=y\) and \(X\geq 0\).

**Exercise 3.4** Let \(h\) be measurable on \([0,1]\). Then

\[\int_{(-\infty,x]}h(F_{-})\,dF\leq\int_{0}^{F(x)}h(t)\,dt\leq\int_{(-\infty,x] }h(F)\,dF\quad\mbox{if $h\nearrow$}\,. \tag{14}\]

Reverse the inequalities if \(h\searrow\).

**Proof**. We finally prove proposition 1.2.3. Let \(D\) be a subset of \([0,1]\) that is not Lebesgue measurable; its existence is guaranteed by proposition 1.2.2. Let \(B\equiv F^{-1}(D)\) for the Lebesgue singular df \(F\). Then (6.1.10) shows that \(B\) is a subset of the Cantor set \(C\). Since \(\lambda(C)=0\) and \(B\subset C\), then \(B\) is a Lebesgue set with \(\lambda(B)=0\); that is, \(B\in\hat{\cal B}_{\lambda}\). We now assume that \(B\) is Borel set (and seek a contradiction). Now \(F^{-1}\) is measurable by (6.1.10), and so \((F^{-1})^{-1}(B)\in{\cal B}\). But

\[\mbox{(a)}\qquad(F^{-1})^{-1}(B)=\{r:F^{-1}(r)\in B\}=\{r:F^{-1}(r)\in F^{-1}( D)\}=D\notin{\cal B}, \tag{15}\]

since \(F^{-1}\) is one-to-one on \([0,1]\). This is a contradiction. Thus \(B\in\hat{\cal B}_{\lambda}\backslash{\cal B}\). \(\Box\)

**The Elementary Skorokhod Construction Theorem**

Let \(X_{0},X_{1},X_{2},\ldots\) be iid \(F\). Then \(X_{n}\to_{d}X_{0}\), but the \(X_{n}\) do not converge to \(X_{0}\) in the sense of \(\to_{a.s.},\to_{p}\), or \(\to r\). However, whenever \(X_{n}\to_{d}X_{0}\), it is possible to replace the \(X_{n}\)'s by rvs \(Y_{n}\) having the same (marginal) dfs, for which the stronger result \(Y_{n}\to_{a.s.}Y_{0}\) holds.

**Theorem 3.2** (Skorokhod): Suppose that \(X_{n}\to_{d}X_{0}\). Define \(\xi(\omega)=\omega\) for each \(\omega\in[0,1]\) so that \(\xi\cong\) Uniform \((0,1)\) on \((\Omega,{\cal A},P)\equiv([0,1],{\cal B}\cap[0,1],\lambda)\), for Lebesgue measure \(\lambda\). Let \(F_{n}\) denote the df of \(X_{n}\), and define \(Y_{n}\equiv F_{n}^{-1}(\xi)\) for all \(n\geq 0\). Let \(D_{K_{0}}\) denote the at most countable discontinuity set of \(K_{0}\). Then both

\[\begin{array}{l}Y_{n}\equiv K_{n}(\xi)\equiv F_{n}^{-1}(\xi)\cong X_{n}\cong F _{n}\quad\mbox{ for all }n\geq 0\qquad\mbox{ and }\\ Y_{n}(\omega)\to Y_{0}(\omega)\quad\mbox{ for all }\omega\notin D_{K_{0}}. \end{array} \tag{15}\]

**Proof**. This follows trivially from Proposition 3.1. \(\Box\)

**Exercise 3.5** (Wasserstein distance) Let \(k=1\) or \(2\). Define

\[\begin{array}{l}{\cal F}_{k}\equiv\{F:F\mbox{ is a df},\mbox{ and }\int|x|^{k}\,dF(x)<\infty\},\qquad\mbox{ and }\\ d_{k}(F_{1},\ F_{2})\equiv\{\int_{0}^{1}|F_{1}^{-1}(t)-F_{2}^{-1}(t)|^{k}\,dt\}^{1/k}\qquad\mbox{ for all }F_{1},F_{2}\in{\cal F}_{k}.\end{array}\]

(a) Show that all such \(({\cal F}_{k},d_{k})\) spaces are complete metric spaces, and that

\[\begin{array}{l}d_{k}(F_{n},F_{0})\to 0\qquad\mbox{ (with all }\{F_{n}\}_{0}^{\infty}\in{\cal F}_{k})\qquad\mbox{ if and only if }\\ F_{n}\to_{d}F_{0}\quad\mbox{ and }\quad m_{nk}\equiv\int|x|^{k}\,dF_{n}(x)\to m_{0k}\equiv\int|x|^{k}\,dF_{0}(x)\in(0,\infty).\end{array} \tag{16}\]

(The rvs \(Y_{n}\equiv F_{n}^{-1}(\xi)\) of (15) satisfy \(Y_{n}\to_{{\cal L}_{k}}Y_{0}\) if \({\rm E}|Y_{n}|^{k}\to{\rm E}|Y_{0}|^{k}\in(0,\infty)\).)

(b) Apply this to the empirical df \({\mathbb{F}}_{n}\) and qf of iid \(F\) rvs \(X_{1},X_{2},\ldots\) to conclude

\[\int_{0}^{1}[{\mathbb{F}}_{n}^{-1}(t)-F^{-1}(t)]^{k}\,dt\to_{a.e}.0\qquad\mbox{ iff }\qquad\frac{1}{n}\sum_{1}^{n}|X_{i}|^{k}\to_{a.e.}{\rm E}|X|^{k}<\infty; \tag{17}\]

note (3.5.22). (Refer this to the SLLN later.)

## 4 Integration by Parts Applied to Moments

Integration by Fubini's theorem or "integration by parts" formulas are useful in many contexts. Here we record a few of the most useful ones.

### Integration by Parts

**Proposition 4.1** (Integration by parts formulas): Suppose that both the left-continuous function \(U\) and the right-continuous function \(V\) are monotone functions. Then for any \(a\leq b\) we have both

\[U_{+}(b)V(b)-U(a)V_{-}(a)=\int_{[a,b]}U\,dV+\int_{[a,b]}V\,dU\qquad \text{ and} \tag{2}\] \[U(b)V(b)-U(a)V(a)=\int_{(a,b]}U\,dV+\int_{[a,b)}V\,dU, \tag{1}\]

where \(U_{+}(x)\equiv\lim_{y\searrow x}U(y)\) and \(V_{-}(x)\equiv\lim_{y\nearrow x}V(y)\). (Symbolically, written as \(d(UV)=U_{-}dV+V_{+}dU\), it implies also that \(\int h\,d(UV)=\int h[U_{-}dV+V_{+}dU]\) for any measurable \(h\geq 0\).)

Figure 4.1 Integration by parts.

**Proof**. We can apply Fubini's theorem at steps (a) and (b) to obtain

\[[U_{+}(b)-U(a)][V(b)-V_{-}(a)]=\int_{[a,b]}\{\int_{[a,b]}dU\}\,dV\] (a) \[=\int_{[a,b]}\int_{[a,b]}[1_{[x^{\prime}<y^{\prime}]}(x,y)+1_{[x^{ \prime}\geq y^{\prime}]}(x,y)]\,dU(x)\,dV(y)\] (b) \[=\int_{[a,b]}[U(y)-U(a)]\,dV(y)+\int_{[a,b]}[V(x)-V_{-}(a)]\,dU(x)\] \[=\int_{[a,b]}UdV-U(a)[V(b)-V_{-}(a)]+\int_{[a,b]}VdU-V_{-}(a)[U_{+}(b)-U( a)].\]

Algebra now gives (1). Add \(U(a)[V(b)-V_{-}(a)]+V_{-}(a)[U_{+}(b)-U(a)]\) to each side of (1) to obtain (2). \(\Box\)

**Exercise 4.1**: How should the left side of (1) be altered if we replace \([a,b]\) in both places on the right side of (1) by \((a,b)\), or by \((a,b],\text{or by }[a,b)\)? (Just plug in \(a_{+}\) or \(a_{-}\) as well as \(b+\text{ or }b_{-}\) on both sides of the equation \(d(UV)=U_{-}dV+V_{+}dU\) so as to include or exclude that endpoint; this will give the proper formulation.)

### Useful Formulas for Means, Variances, and Covariances

If \(\xi\cong\text{Uniform}(0,\,1)\) and \(F\) is an arbitrary df, then the rv \(X\equiv F^{-1}(\xi)\) has df \(F\). **Thinking of \(X\) as \(F^{-1}(\xi)\) presents alternative ways to approach problems.**_We will do this often_! Note that this \(X=F^{-1}(\xi)\) satisfies both

\[X=\int_{(0,1)}F^{-1}(t)\,d1_{[\xi\leq t]}\qquad\mbox{ and }\qquad X=\int_{(-\infty, \infty)}x\ d1_{[X\leq x]}, \tag{3}\]

where \(1_{[\xi\leq t]}\) is a random df that puts mass 1 at the point \(\xi(\omega)\) and \(1_{[X\leq x]}\) is a random df that puts mass 1 at the point \(x\). If \(X\) has a finite mean \(\mu\), then (depending on which representation of \(X\) we use)

\[\mu=\int_{(0,1)}F^{-1}(t)\,dt\qquad\mbox{ and }\qquad\mu=\int_{(-\infty,\infty)}x \ dF(x) \tag{4}\]

Moreover, when \(\mu\) is finite we can combine the two previous formulas to write

\[X-\mu=\int_{(0,1)}F^{-1}(t)\,d(1_{[\xi\leq t]}-t)=-\int_{(0,1)}(1_{[\xi\leq t]} -t)\,dF^{-1}(t) \tag{5}\]

or

\[X-\mu=\int_{(-\infty,\infty)}x\ d(1_{[X\leq x]}-F(x))=-\int_{(-\infty,\infty)} (1_{[X\leq x]}-F(x))\,dx. \tag{6}\]

The first formula in each of (5) and (6) is trivial; the second follows from integration by parts. For example, (5) is justified by \(|tF^{-1}(t)|\leq|\int_{0}^{t}F^{-1}(s)\,ds|\to 0\) as \(t\to 0\) when \({\rm E}|X|=\int_{0}^{1}|F^{-1}(t)|\,dt<\infty\), and the analogous result \((1-t)F^{-1}(t)\to 0\) as \(t\to 1\). For (6), we note that \(x[1-F(x)]\leq\int_{(x,\infty)}y\ dF(y)\to 0\) as \(x\to\infty\) if \({\rm E}|X|<\infty\). So Fubini's theorem would seem to give (using \({\rm E}|X|<\infty\) for (5)) that,

\[{\rm Var}\ [X]={\rm E}\left\{\int_{(0,1)}(1_{[\xi\leq s]}-s)\,dF^{-1}( s)\int_{(0,1)}(1_{[\xi\leq t]}-t)\,dF^{-1}(t)\right\} \tag{7}\] \[=\int_{(0,1)}\int_{(0,1)}{\rm E}\{(1_{[\xi\leq s]}-s)(1_{[\xi\leq t ]}-t)\}\,dF^{-1}(s)\,dF^{-1}(t)\] (8) \[=\int_{(0,1)}\int_{(0,1)}[s\wedge t-st]\,dF^{-1}(s)\,dF^{-1}(t) \qquad\mbox{(true, even if ${\rm E}|X|=\infty$)}.\]

(In fact, see exercise 4.2 and (6.6.2) to rigorize the steps of the proof of (7)\(-\)(8), even when \({\rm E}|X|=\infty\).) The parallel formula (via the same type of argument) is

\[{\rm Var}[X]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}[F(x\wedge y)-F(x)F( y)]\,dx\ dy\qquad\mbox{(even if ${\rm E}|X|=\infty$)}, \tag{9}\]

starting from (6). Of course, we already know, when \({\rm E}(X^{2})<\infty\), that

\[{\rm Var}[X]=\int_{0}^{1}[F^{-1}(t)-\mu]^{2}\,dt\quad\mbox{ and }\quad{\rm Var}[X]=\int_{-\infty}^{\infty}(x-\mu)^{2}\,dF(x). \tag{10}\]

**Proposition 4.2** (Other formulas for means, variances, and covariances)

(i) If \(X\geq 0\) has df \(F\), then

\[\int_{0}^{\infty}P(X>x)\,dx={\rm E}X=\int_{0}^{\infty}(1-F(x))\,dx\quad\mbox{ and }\quad{\rm E}X=\int_{0}^{1}F^{-1}(t)\,dt.\]

(ii) If \({\rm E}|X|<\infty\), then

\[{\rm E}(X)=-\int_{-\infty}^{0}F(x)\,dx+\int_{0}^{\infty}(1-F(x))\,dx=\int_{0}^{ 1}F^{-1}(t)\,dt.\]

(iii) Let \(r>0\). If \(X\geq 0\), then

\[\int_{0}^{\infty}P(X^{r}>x)\,dx={\rm E}(X^{r})=\int_{0}^{\infty}rx^{r-1}(1-F(x) )\,dx=\int_{0}^{1}[F^{-1}(t)]^{r}dt\]

In fact, one of the two integrals is finite if and only if the other is finite.

(iv) Let \((X,Y)\) have joint df \(F\) with marginal dfs \(F_{X}\) and \(F_{Y}\). Let \(G\) and \(H\) be \(\nearrow\) and left continuous. Then

\[{\rm Cov}\;[G(X),H(Y)]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}[F(x,y)-F _{X}(x)F_{Y}(y)]\,dG(x)\,dH(y)\]

whenever this covariance is finite. Note the special case \(G=H=I\) for Cov \([X,Y]\). Hint. Without loss, \(G_{-}(0)=G_{+}(0)=H_{-}(0)=H_{+}(0)\). Make use of the fact that \(G(x)=\int_{[0,\infty)}1_{[0,x)}(s)\,dG_{-}(s)\) in the first quadrant, etc.

(v) Let \(K\) be \(\nearrow\) and left continuous and \(\xi\cong{\rm Uniform}(0,\,1)\) (perhaps \(K=h(F^{-1})\) for an \(\nearrow\) left-continuous function \(h\), and for \(X\equiv F^{-1}(\xi)\) for a df \(F\)). When finite,

\[{\rm Var}\;[K(\xi)]=\int_{0}^{1}\int_{0}^{1}[s\wedge t-st]\,dK(s)\, dK(t)\qquad\mbox{ and } \tag{15}\] \[{\rm Var}\;[K(\xi)]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty }[F(x\wedge y)-F(x)F(y)]\,dh(x)\,dh(y)={\rm Var}[h(X)] \tag{16}\]

follow from (8) and (14).

(vi) If \(X\geq 0\) is integer-valued, then

\[{\rm E}X=\sum_{k=1}^{\infty}P(X\geq k)\qquad\mbox{ and }\qquad{\rm E}X^{2}= \sum_{k=1}^{\infty}(2k-1)P(X\geq k). \tag{17}\]

**Exercise 4.2** (Winsorized X)  Let \(\tilde{X}_{a,a^{\prime}}\equiv\tilde{K}_{a,a^{\prime}}(\xi)\), where \(\xi\cong{\rm Uniform}(0,1)\). Here, \(\tilde{K}_{a,a^{\prime}}\) equals \(K_{+}(a),K(t),K(1-a^{\prime})\) according as \(0<t\leq a,a<t<1-a^{\prime},1-a^{\prime}\leq t<1\). We say that \(X\) has been Winsorized outside \((a,1-a^{\prime})\)

(a) Use the Fubini/Tonelli combination (as above) to check that

\[{\rm E}(\tilde{K}_{a,a^{\prime}}^{2}(\xi))-({\rm E}(\tilde{K}_{a,a^{\prime}}(\xi))^{2}={\rm Var}[\tilde{K}_{a,a^{\prime}}(\xi)]\] \[\qquad=\int_{0}^{1}\int_{0}^{1}1_{(a,1-a^{\prime})}(s)1_{(a,1-a^{ \prime})}(t)(s\wedge t-st)\,dK(s)\,dK(t);\]

essentially, obtain (8) for \(\tilde{X}_{a,a^{\prime}}\). Then let \((a\lor a^{\prime})\to 0\), and apply the MCT, to obtain (7) for general \(X.\) (Use (6.6.2) to see that (8) holds even if \({\rm E}|X|=\infty\).)

(b) Establish (9) using similar methods.

**Exercise 4.3**: (a) Prove formulas (11)-(13). [Hint. Use integration by parts.]

(b) Prove the formula (14).

**Exercise 4.4**: Prove the formulas in (17).

**Exercise 4.5**: Give an extension of (13) to arbitrary rvs.

**Exercise 4.6**: (a) Use Fubini and use integration by parts to show twice that for arbitrary \(F\) and for every \(x\geq 0\) we have

\[\int_{[0,x]}y^{2}\,dF(y)=2\int_{0}^{x}tP(X>t)\,dt-x^{2}P(X>x). \tag{18}\]

(b) Verify (6.3.12) and (6.3.13) once again, with the current methods.

**Exercise 4.7** (Integration by parts formulas) We showed in proposition 4.1 earlier that \(d(UV)=U_{-}dV+V_{+}dU\) (with left continuous \(U\) and right continuous \(V\)).

(i) Now show (noting that \(dU_{-}=dU_{+}\)) that

\[dU^{2}=d(U_{-}U_{+})=U_{-}dU+U_{+}dU=(2U+\triangle U)\,dU\qquad\mbox{ for } \triangle U\equiv U-U_{-}. \tag{19}\]

(ii) Apply proposition 4.1 to \(1=U\cdot(1/U)\) to obtain

\[d(1/U)=-\{1/(U_{+}U_{-})\}\,dU=-\{1/(U(U+\triangle U))\}\,\,dU. \tag{20}\]

(iii) Show by induction that for \(k=1,2,\ldots\) we have

\[dU^{k}=(\sum_{i=0}^{k-1}U_{+}^{i}U_{-}^{k-i-1})\,dU. \tag{21}\]

**Exercise 4.8**: Show that for an arbitrary df \(F\) we have

\[d(F/(1-F))=\{1/((1-F)(1-F_{-}))\}\,dF. \tag{22}\]

**Exercise 4.9**: For any df \(F\) we have

\[\int[F(x+\theta)-F(x)]\,dx=\theta\qquad\mbox{ for each }\theta\geq 0.\]

**Exercise 4.10** (Stein) Suppose \(X\cong(0,\sigma^{2})\) with df \(F\). Then \(g(x)\equiv\int_{x}^{\infty}y\,dF(y)/\sigma^{2}\) is a density. (And \(g(x)=-\int_{-\infty}^{x}y\,dF(y)/\sigma^{2}\) is also true.)

**Exercise 4.11**: (a) Show that \(\int_{0}^{\infty}\{P(|X|>x)\}^{1/2}dx<\infty\) implies E\(X^{2}<\infty\).

(b) Show that \(\{\int_{0}^{\infty}\{P(|X|>x)\}^{1/2}\,dx\leq\frac{r}{r-2}\|X\|_{r}\) for any \(r>2\), so that the integral on the left is finite whenever \(X\in{\cal L}_{r}\) for any \(r>2\).

Hint. Verify (a) when \(X\) is bounded, via (13) and Markov. Then apply the MCT. Consider (b). Bound \(\int_{0}^{\infty}=\int_{0}^{c}+\int_{c}^{\infty}\leq c+\int_{c}^{\infty}\) via Markov, and then choose "\(c\)" to minimize the bound.

## 5 Important Statistical Quantities\({}^{o}\)

**Notation 5.1** (**Trimming, Winsorizing, and Truncating, and dom(\(a,a^{\prime}\))**)** First, let dom(\(a,a^{\prime}\)) denote \([0,1-a^{\prime})\) if \(X\geq 0\), or \((a,1]\) if \(X\leq 0,\)or \((a,1-a^{\prime})\) otherwise. Let \(\tilde{K}_{a,a^{\prime}}(\cdot)\) denote \(K(\cdot)\) Winsorized outside the domain of Winsorization dom(\(a,a^{\prime}\)). Thus when \(X\) takes both positive and negative values and we suppose that "\(a\)" and "\(a^{\prime}\)" are specified so small that \(K_{+}(a)<0<K(1-a^{\prime})\), it follows that

\[\tilde{K}_{a,a^{\prime}}(t)\mbox{ equals }K_{+}(a),K(t),K(1-a^{ \prime})\] \[\mbox{ as }0<t\leq a,a<t<1-a^{\prime},1-a^{\prime}\leq t<1\]

(while \(a\equiv 0\) and \(\tilde{K}(a)\equiv K(0)\) if \(X\geq 0\), etc). Let \(\xi\) denote a Uniform(0, 1) rv. Let

\[\tilde{\mu}(a,a^{\prime})\equiv\tilde{\mu}_{K}(a,a^{\prime})\equiv\mbox{E} \tilde{K}_{a,a^{\prime}}(\xi)\equiv\int_{0}^{1}\tilde{K}_{a,a^{\prime}}(t)\,dt,\]

which is the \((a,a^{\prime})\)-Winsorized mean of the rv \(K(\xi)\), and let

\[\tilde{\sigma}^{2}(a,a^{\prime})\equiv\tilde{\sigma}_{K}^{2}(a,a^ {\prime})\equiv\mbox{Var}[\tilde{K}_{a,a^{\prime}}(\xi)]=\int_{0}^{1}\tilde{K }_{a,a^{\prime}}^{2}(t)\,dt-\tilde{\mu}(a,a^{\prime})^{2}\] \[\qquad=\int_{0}^{1}\int_{0}^{1}[s\wedge t-st]\,d\tilde{K}_{a,a^{ \prime}}(s)\,d\tilde{K}_{a,a^{\prime}}(t)\]

denote the \((a,a^{\prime})\)-Winsorized variance (recall (6.4.8)). For general \(X\), let

\[\tilde{\mu}(a)\equiv\tilde{\mu}(a,a),\qquad\tilde{\sigma}^{2}(a)\equiv\tilde{ \sigma}^{2}(a,a),\qquad\mbox{ and }\qquad\tilde{K}_{a}(\cdot)\equiv\tilde{K}_{a,a}(\cdot);\]

but \(\tilde{\mu}(a)\equiv\tilde{\mu}_{0,a}\) if \(X\geq 0\), etc.

We now let \(0\leq k_{n}<n-k^{\prime}_{n}\leq n\) denote integers, and then let

\[a_{n}\equiv k_{n}/n\quad\mbox{ and }\quad a^{\prime}_{n}\equiv k^{\prime}_{n}/n,\qquad\mbox{ so that }0\leq a_{n}<1-a^{\prime}_{n}\leq 1.\]

Let \(\tilde{K}_{n}(\cdot)\) denote \(K(\cdot)\) Winsorized outside dom(\(a_{n},a^{\prime}_{n}\)). Let

\[\tilde{\mu}_{n}\equiv\tilde{\mu}_{K}(a_{n},a^{\prime}_{n})\equiv \int_{a_{n}}^{1-a_{i}}K(t)\,dt,\quad\tilde{\mu}_{n}\equiv\tilde{\mu}_{K}(a_{n },a^{\prime}_{n})\equiv\tilde{\mu}_{n}/(1-a_{n}-a^{\prime}_{n})\,\] \[\tilde{\mu}_{n}\equiv\tilde{\mu}_{K}(a_{n},a^{\prime}_{n})\equiv \mu_{\tilde{K}_{n}}\equiv\mbox{E}\tilde{K}_{n}(\xi)\equiv\int_{0}^{1}\tilde{K }_{n}(t)\,dt,\]

so that \(\tilde{\mu}_{n}\) is the \((a_{n},a^{\prime}_{n})\)-Trimmed mean, \(\tilde{\mu}_{n}\) is the \((a_{n},a^{\prime}_{n})\)-\(\tilde{W}insorized\,mean\), and \(\tilde{\mu}_{n}\) is herein called the \((a_{n},a^{\prime}_{n})\)-Truncated mean of the rv \(K(\xi)\). Then let

\[\tilde{\sigma}_{n}^{2}\equiv\tilde{\sigma}_{K}^{2}(a_{n},a^{\prime}_{n})\equiv \sigma_{\tilde{K}_{n}}^{2}\equiv\mbox{Var}[\tilde{K}_{n}(\xi)]=\int_{0}^{1} \int_{0}^{1}[s\wedge t-st]\,d\tilde{K}_{n}(s)\,d\tilde{K}_{n}(t)\]

denote the \((a_{n},a^{\prime}_{n})\)-Winsorized variance. When they are finite, the _mean_\(\mu\) and _variance_\(\sigma^{2}\) satisfy

\[\mu\equiv\mu_{K}=\int_{0}^{1}K(t)=\int x\,dF(x)=\mbox{E}(X)=\mbox {E}K(\xi)\,\] \[\sigma^{2}\equiv\sigma_{K}^{2}=\int_{0}^{1}\int_{0}^{1}[s\wedge t -st]\,dK(s)\,dK(t)=\mbox{E}X^{2}-\mu^{2}=\mbox{E}K^{2}(\xi)-\mu^{2}.\]

Let \(a.\equiv\inf\{t:K(t)\geq 0\}\), and let \(a_{\rm o}\equiv a.\wedge(1-a.)\). (But \(a.\equiv 0\) if \(X\geq 0\), and \(a.\equiv 1\) if \(X\leq 0.\)) Now, \((K-\tilde{\mu}_{n})^{+}\) and \((K-\tilde{\mu}_{n})^{-}\) denote the positive and negative parts of \(K-\tilde{\mu}_{n}\), and let

\[\bar{K}_{n}\equiv[K-\tilde{\mu}_{n}]\quad\mbox{ and }\quad\bar{K}_{n}^{2}\equiv-[(K- \tilde{\mu}_{n})^{-}]^{2}+[(K-\tilde{\mu}_{n})^{+}]^{2}\quad\mbox{ on }(0,1).\]In this context, we may wish to assume that both

\[(k_{n}\wedge k^{\prime}_{n})\to\infty\qquad\text{ and }\qquad(a_{n}\lor a^{\prime}_{n}) \to 0; \tag{10}\]

and perhaps we will also assume

\[a^{\prime}_{n}/a_{n}\to 1\qquad\text{ and/or }\qquad(k_{n}-k^{\prime}_{n})/(k_{n} \wedge k^{\prime}_{n})\to 0. \tag{11}\]

We will refer to \(k_{n},k^{\prime}_{n}\) as the \(trimming/\)_Winsorizing numbers_ and \(a_{n},a^{\prime}_{n}\) as the _trimming/_

_Winsorizing fractions_. Describe the case of (10) as _slowly growing to \(\infty\)_.

Suppose \(X_{n1},\ldots,X_{nn}\) is an iid sample with df \(F\) and qf \(K\). Let \(X_{n:1}\leq\cdots\leq X_{n:n}\) denote the _order statistics_ (that is, they are the ordered values of \(X_{n1},\ldots,X_{nn}\)). Let \(\mathbb{K}_{n}(.)\) on \([0,1]\) denote the _empirical \(qf\)_ that equals \(X_{n:i}\) on \(((i-1)/n,i/n]\), for \(1\leq i\leq n\), and that is right continuous at zero. Now let

\[\bar{X}_{n}\equiv\frac{1}{n}\sum_{k=1}^{n}X_{nk}=\mu_{\mathbb{K}_{n}}(0,0) \quad\text{and}\quad S_{n}^{2}\equiv\frac{1}{n}\sum_{k=1}^{n}(X_{nk}-\bar{X}_{ n})^{2}=\sigma_{\mathbb{K}_{n}}^{2}(0,0) \tag{12}\]

denote the _sample mean_ and the "_sample variance_." We also let

\[\tilde{X}_{n}\equiv\frac{1}{n}\sum_{i=k_{n}+1}^{n-k^{\prime}_{n}} X_{n:i}=\tilde{\mu}_{\mathbb{K}_{n}}(a_{n},a^{\prime}_{n})\,\quad\breve{X}_{n} \equiv\frac{1}{n-k_{n}-k^{\prime}_{n}}\sum_{i=k_{n}+1}^{n-k^{\prime}_{n}}X_{n:i}, \tag{14}\] \[\tilde{X}_{n}\equiv\frac{1}{n}\left[k_{n}X_{n:k_{n}+1}+\sum_{i=k_ {n}+1}^{n-k^{\prime}_{n}}X_{n:i}+k^{\prime}_{n}X_{n-k^{\prime}_{n}}\right]= \tilde{\mu}_{\mathbb{K}_{n}}(a_{n},a^{\prime}_{n}) \tag{13}\]

denote the _sample \((a_{n},a^{\prime}_{n})\)-Ytruncated mean_, the _sample \((a_{n},a^{\prime}_{n})\)-Yrimmed mean_, and the _sample \((a_{n},a^{\prime}_{n})\)-Winsorized mean. Let \(\tilde{X}_{n:1},\ldots,\tilde{X}_{n:n}\) denote the \((a_{n},a^{\prime}_{n})\)-Winsorized order \(statistics\), whose empirical qf is \(\tilde{\mathbb{K}}_{n}\). Now note that

\[\tilde{X}_{n}=\frac{1}{n}\sum_{i=1}^{n}\tilde{X}_{n:i}=\mu_{\tilde{\mathbb{K} }_{n}};\qquad\text{ let }\qquad\tilde{S}_{n}^{2}\equiv\frac{1}{n}\sum_{i=1}^{n}( \tilde{X}_{n:i}-\tilde{X}_{n})^{2}=\sigma_{\tilde{\mathbb{K}}_{n}}^{2} \tag{15}\]

denote the _sample \((a_{n},a^{\prime}_{n})\)-_Winsorized variance_. Let

\[\tilde{\sigma}_{n}^{2}\equiv\tilde{\sigma}_{n}^{2}/(1-a_{n}-a^{\prime}_{n})^{ 2}\qquad\text{ and }\qquad\tilde{S}_{n}^{2}\equiv\tilde{S}_{n}^{2}/(1-a_{n}-a^{ \prime}_{n})^{2}. \tag{16}\]

Of course, \(\bar{X}_{n},S_{n},\breve{X}_{n},\tilde{S}_{n}\) estimate \(\mu,\sigma,\breve{\mu}_{n},\tilde{\sigma}_{n}\). We also define the standardized forms of these estimators as

\[Z_{n}\equiv\frac{\sqrt{n}(\bar{X}_{n}-\mu)}{\sigma}\qquad\text{ and }\qquad\breve{Z}_{n}\equiv\frac{\sqrt{n}(\breve{X}_{n}-\breve{\mu}_{n})}{ \breve{\sigma}_{n}}=\frac{\sqrt{n}(\breve{X}_{n}-\breve{\mu}_{n})}{\tilde{ \sigma}_{n}}, \tag{17}\]

and the Studentized forms of these estimators as

\[T_{n}\equiv\frac{\sqrt{n}(\bar{X}_{n}-\mu)}{S_{n}}\qquad\text{ and }\qquad\breve{T}_{n}\equiv\frac{\sqrt{n}(\breve{X}_{n}-\breve{\mu}_{n})}{ \tilde{S}_{n}}=\frac{\sqrt{n}(\breve{X}_{n}-\breve{\mu}_{n})}{\tilde{S}_{n}}. \tag{18}\]

(The first formula for \(\breve{T}_{n}\) is for _statistical application_, while the second formula is for _probabilistic theory_.)We define the _sample median_\(\ddot{X}_{n}\) to equal \(X_{n:(n+1)/2}\) or \((X_{n:n/2}+X_{n:n/2+1})/2\), according as \(n\) is odd or even.

##### Representing RVs

We will often assume that the independent rvs \(X_{n1},\ldots,X_{nn}\) having df \(F\) and qf \(K\) are defined in terms of iid Uniform\((0,1)\) rvs \(\xi_{n1},\ldots,\xi_{nn}\) (as above (6.4.3)) via

\[X_{nk}\equiv K(\xi_{nk})=F^{-1}(\xi_{nk})\qquad\mbox{ for }1\leq k\leq n. \tag{19}\]

As one alternative, we start with data rvs \(X_{1},\ldots,X_{n}\) that are iid \(K\) and then define Uniform\((0,1)\) rvs \(\xi_{1}^{o},\ldots\xi_{n}^{o}\) via (6.3.8). Then the rvs

\[X_{k}\equiv F^{-1}(\xi_{k}^{o})\qquad\mbox{ (as in (\ref{eq:10}.9)) are just the original $X_{k}$'s}. \tag{20}\]

This device in (20) can be useful. In this context, let \(\xi_{n:1}^{o}<\cdots<\xi_{n:n}^{o}\) denote the order statistics of the iid Uniform\((0,\,1)\) rvs \(\xi_{k}^{0}\) of (20). Let \(\mathsf{R}_{n}\equiv(R_{n1},\ldots,R_{nn})^{\prime}\) denote the _ranks_ of these \(\xi_{1}^{o},\ldots,\xi_{n}^{o}\), and let \(\mathsf{D}_{n}\equiv(D_{n1},\ldots,D_{nn})^{\prime}\) denote their _antiranks_. Thus the rank vector \(\mathsf{R}_{n}\) is a random permutation of the vector \((1,\,2,\,\ldots,n)\)' while \(\mathsf{D}_{n}\) is the inverse permutation. These satisfy

\[\xi_{D_{nk}}^{o}=\xi_{n:k}^{o}\qquad\mbox{ and }\qquad\xi_{k}^{o}=\xi_{n:R_{nk}}^{o}. \tag{21}\]

We will learn later that

\[(\xi_{n:1}^{o},\ldots,\xi_{n:n}^{o})\quad\mbox{and}\quad(R_{n1},\ldots,R_{nn}) \mbox{ are independent random vectors}. \tag{22}\]

Using the rvs \(\xi_{k}^{o}\) of this paragraph corresponds (when forming the rank vector of data \(X_{k}\) coming from a discontinuous df) to breaking ties at random in forming the ranks. Such notation is sometimes used throughout the remainder of this book. \(\square\)

##### The Empirical DF

**Notation 5.2** (Empirical dfs and processes): Let \(X_{1},X_{2},\ldots\) be iid with \(\mbox{df}\,F\) and qf \(K\). The _empirical df_\(\mathbb{F}_{n}\) of \((X_{1},\ldots,X_{n})\) is defined by

\[\mathbb{F}_{n}(x)\equiv\frac{1}{n}\sum_{k=1}^{n}1_{(-\infty,x]}(X_{k})=\frac{1 }{n}\sum_{k=1}^{n}1_{[X_{k}\leq x]}\quad\mbox{ for }-\infty<x<\infty. \tag{23}\]

This is a step function on the real line \(R\) that starts at height \(0\) and jumps by height \(1/n\) each time the argument reaches another observation as it moves from left to right along the line. We can think of \(\mathbb{F}_{n}\) as an estimate of \(F\). The important study of the _empirical process_

\[\mathbb{E}_{n}(x)\equiv\sqrt{n}[\mathbb{F}_{n}(x)-F(x)]\qquad\mbox{ for }x\in R \tag{24}\]

will allow us to determine how this estimator \(\mathbb{F}_{n}\) of \(F\) performs.

We also let \(\xi_{1},\xi_{2},\ldots\) be iid Uniform(0, 1), with true df the identity function \(I\) on \([0,1]\) and with _uniform empirical df_

\[\mathbb{G}_{n}(t)\equiv\frac{1}{n}\sum_{k=1}^{n}1_{[0,t]}(\xi_{k})=\frac{1}{n} \sum_{k=1}^{n}1_{[\xi_{k}\leq t]}\qquad\text{ for }0\leq t\leq 1. \tag{25}\]

The corresponding _uniform empirical process_ is given by

\[\mathbb{U}_{n}(t)\equiv\sqrt{n}[\mathbb{G}_{n}(t)-t]\qquad\text{ for }t\in[0,1]. \tag{26}\]

If we now define an iid \(F\) sequence \(X_{1},X_{2},\ldots\) via \(X_{k}\equiv F^{-1}(\xi_{k})=K(\xi_{k})\), then the empirical df and empirical process of these \((X_{1},\ldots,X_{n})\) satisfy

\[\begin{array}{l}(\mathbb{F}_{n}-F)=[\mathbb{G}_{n}(F)-I(F)]\text{ on }R \quad\text{ and }\quad\mathbb{E}_{n}=\mathbb{U}_{n}(F)\text{ on }R,\\ \text{ valid for every }\omega,\end{array} \tag{27}\]

as follows by (6.3.3). (If we use the \(\xi_{k}^{o}\)'s of (6.3.8), then the \(\mathbb{F}_{n}\) on the left in (22) is everywhere equal to the \(\mathbb{F}_{n}\) of the original \(X_{k}\)'s.) Thus our study of properties of \(\mathbb{E}_{n}\) can proceed via a study of the simpler \(\mathbb{U}_{n}\), which is then evaluated at a deterministic \(F\). (Recall also in this regard theorem 5.3.3 about probability being determined by the finite dimensional distributions.)

## 6 Infinite Variance\({}^{o}\)

Whenever the variance is infinite, (1) will show that the Truncated variance \(\check{\sigma}^{2}\) dominates the square \(\check{\mu}^{2}\) of the Truncated mean. The same is true in the Winsorized case, as in (2). Let \(\tilde{K}_{a,a^{\prime}}\) denote \(K\) Winsorized outside \((a,1-a^{\prime})\).

**Theorem 6.1** (Gnedenko-Kolmogorov): Every nondegenerate qf \(K\) satisfies

\[\limsup_{a\lor a^{\prime}\to 0}\{\int_{a}^{1-a^{\prime}}|K(t)|\,dt\}^{2}/ \int_{a}^{1-a^{\prime}}K^{2}(t)\,dt=0\qquad\mbox{ whenever }\mathrm{E}K^{2}(\xi)=\infty, \tag{1}\] \[\mathrm{Var}[\tilde{K}_{a,a^{\prime}}(\xi)]/\mathrm{E}\tilde{K}_{ a,a^{\prime}}^{2}(\xi)\to 1\mbox{ as }(a\lor a^{\prime})\to 0\qquad\mbox{ whenever }\mathrm{E}K^{2}(\xi)=\infty. \tag{2}\]

**Proof**.  Let \(h>0\) be continuous, symmetric about \(t=1/2,\uparrow\) to \(\infty\) on \([1/2,\,1)\), and suppose it satisfies \(C_{h}\equiv\int_{0}^{1}h^{2}(t)\,dt<\infty\). Let \(b\equiv 1-a^{\prime}\). Then Cauchy-Schwarz provides the bound

\[\{\int_{a}^{b}|K(t)|\,dt\}^{2}=\{\int_{a}^{b}h(t)|K(t)/h(t)|\,dt\}^{2}\leq\int _{a}^{b}h^{2}(t)\,dt\int_{a}^{b}[K^{2}(t)/h^{2}(t)]\,dt.\] (a)

Fix \(c\equiv c_{\epsilon}\) so close to zero that \(C_{h}/h^{2}(c)<\epsilon\). Fix \(c_{\epsilon}\), and let \(a\lor a^{\prime}\to 0\). Then

\[\{\int_{a}^{b}|K(t)|\,dt\}^{2}/\int_{a}^{b}K^{2}(t)\,dt\leq C_{h}\int _{a}^{b}[K^{2}(t)/h^{2}(t)]\,dt/\int_{a}^{b}K^{2}(t)\,dt\] (b) \[\leq C_{h}\left\{\frac{\int_{c}^{1-c}K^{2}(t)\,dt}{h^{2}(1/2)}+ \frac{(\int_{a}^{c}+\int_{1-c}^{b})K^{2}(t)\,dt}{h^{2}(c)}\right\}/\int_{a}^{b }K^{2}(t)\,dt\] (c) \[\leq C_{h}\{\int_{c}^{1-c}K^{2}(t)\,dt/h^{2}(1/2)\}/\int_{a}^{b}K^ {2}(t)\,dt+\epsilon\] (d) \[<2\epsilon\qquad\mbox{ for $a$ and $b$ near enough to $0$ and $1$},\quad\mbox{since }\mathrm{E}K^{2}(\xi)=\infty.\]

Then (2) follows from \([aK_{+}(a)+a^{\prime}K(1-a^{\prime})]^{2}/[aK_{+}^{2}(a)+a^{\prime}K^{2}(1-a^ {\prime})]\to 0\). \(\square\)

**Exercise 6.1** (_Comparing contributions to the variance_): Let \(K(.)\) be arbitrary. Establish the following elementary properties of qfs (of non-trivial rvs):

\[\begin{array}{ll}\mbox{(a)}&\limsup_{a\lor a^{\prime}\to 0}[aK_{+}^{2}(a)+a^{\prime}K^{2}(1-a^{ \prime})]/\tilde{\sigma}^{2}&\qquad\left\{\begin{array}{ll}=0&\mbox{ if }\mathrm{E}K^{2}(\xi)<\infty,\\ \leq 1&\mbox{ if }\mathrm{E}K^{2}(\xi)=\infty.\end{array}\right.\\ \mbox{(b)}&\limsup_{a\lor a^{\prime}\to 0}\int_{a}^{1-a^{\prime}}K^{2}(t)\,dt/\tilde{\sigma}^{2}&\qquad\left\{\begin{array}{ ll}<\infty&\mbox{ always,}\\ \leq 1&\mbox{ if }\mathrm{E}K^{2}(\xi)=\infty.\end{array}\right.\\ \mbox{(c)}&\limsup_{a\lor a^{\prime}\to 0}\{a|K_{+}(a)|+a^{\prime}|K(1-a^{\prime})|\}/\tilde{\sigma}&\qquad\left\{ \begin{array}{ll}=0&\mbox{ always,}\\ =0&\mbox{ if }\mathrm{E}K^{2}(\xi)=\infty.\end{array}\right.\\ \mbox{(d)}&\limsup_{a\lor a^{\prime}\to 0}\int_{a}^{1-a^{\prime}}|K(t)|\,dt/\tilde{\sigma}&\qquad\left\{\begin{array}{ ll}<\infty&\mbox{ always,}\\ =0&\mbox{ if }\mathrm{E}K^{2}(\xi)=\infty.\end{array}\right.\\ \mbox{(e)}&\limsup_{a\lor a^{\prime}\to 0}\mathrm{E}|\tilde{K}_{a,a^{\prime}}(\xi)|/\tilde{\sigma}&\qquad\left\{ \begin{array}{ll}<\infty&\mbox{ always,}\\ =0&\mbox{ if }\mathrm{E}K^{2}(\xi)=\infty.\end{array}\right.\\ \end{array}\]

**Exercise 6.2**: Let \(0<r<s\). Show that for every nondegenerate qf \(K\) we have

\[\lim_{a\lor a^{\prime}}\sup_{\to 0}\{\int_{a}^{1-a^{\prime}}|K(t)|^{r}\,dt\}^{s/r}/\int_{a}^{1-a^{\prime}}|K(t)|^{s}\,dt=0\qquad\mbox{ if }\mathrm{E}|K(\xi)|^{s}=\infty.\]

**Exercise 6.3** (_An added touch_) Let \(K\geq 0\) be \(\searrow\) on \((0,1)\) with \(\int_{0}^{1}K(t)\,dt<\infty\). There exists a positive \(\searrow\) function \(h(\cdot)\) on \((0,1)\) with \(h(t)\to\infty\) as \(t\to 0\) for which \(\int_{0}^{1}h(t)K(t)\,dt<\infty\). (Note exercise 3.5.12.)

**Proposition 6.1** Let \(X\) have \(s\)th absolute moment \({\rm E}|X|^{s}=\infty\), and let \(0<r<s\). Let \(\overline{|X|_{n}^{s}}\equiv\frac{1}{n}\sum_{1}^{n}|X_{k}|^{s}\) for iid rvs \(X_{1},X_{2},\ldots\) distributed as \(X\), etc. Then

\[[\ \overline{|X|_{n}^{r}}]^{s/r}/\,\overline{|X|_{n}^{s}}\to_{\rm a.s.}0\qquad \mbox{(using the following remark 6.1).} \tag{4}\]

**Remark 6.1** It is useful to give this proof now even though it will not be until the SLLN (theorem 8.4.2) that we prove \({\rm E}|X|^{s}<\infty\) yields \(\overline{|X|_{n}^{s}}\to_{a.s.}{\rm E}|X|^{s}\). Likewise, \({\rm E}|X|^{s}=\infty\) yields \(\overline{|X|_{n}^{s}}\to_{a.s.}\infty\). These will be used in the proofs below.

**Proof**. We follow the proof of Gnedenko-Kolmogorov's theorem 6.6.1, as we give the proof of (4) for \(s=2\) and \(r=1\). Let \({\mathbb{K}}_{n}\) denote the empirical qf. Let \(h\) be positive, continuous, symmetric about \(t=\frac{1}{2},\uparrow\) to \(\infty\) on \([\frac{1}{2},1)\) and suppose it satisfies \(C_{h}\equiv\int_{0}^{1}h^{2}(t)\,dt<\infty\). Define \(a=1-b=1/(2n)\). Then Cauchy-Schwarz gives the bound

\[(\int_{a}^{b}|{\mathbb{K}}_{n}(t)|\,dt)^{2}=(\int_{a}^{b}h(t)|{ \mathbb{K}}_{n}(t)/h(t)|\,dt)^{2}\leq\int_{a}^{b}h^{2}(t)\,dt\int_{a}^{b}[{ \mathbb{K}}_{n}^{2}(t)/h^{2}(t)]\,dt\] (a) \[=C_{h}\int_{a}^{b}[{\mathbb{K}}_{n}^{2}(t)/h^{2}(t)]\,dt.\]

Let \(c\equiv c_{\epsilon}\) be fixed so close to zero that \(C_{h}/h^{2}(c)<\epsilon/8\). Then

\[\{\frac{1}{n}\sum_{1}^{n}|X_{k}|\}^{2}/\overline{X_{n}^{2}}\leq 4 \{\int_{a}^{b}|{\mathbb{K}}_{n}(t)|\,dt\}^{2}/\overline{X_{n}^{2}}\] (b) (the "4" comes from the definition of \[a\] and \[b\], which gives only half of the two end intervals) \[\leq 4C_{h}\int_{a}^{b}[{\mathbb{K}}_{n}^{2}(t)/h^{2}(t)]\,dt/ \overline{X_{n}^{2}}\qquad\mbox{ by (a)}\] (b) \[\leq 4C_{h}\left\{\frac{\int_{c}^{1-c}{\mathbb{K}}_{n}^{2}(t)\, dt}{h^{2}(1/2)}+\frac{(\int_{a}^{c}+\int_{1-c}^{b}){\mathbb{K}}_{n}^{2}(t)\,dt}{h^{2} (c)}\right\}/\overline{X_{n}^{2}}\] (c) \[\leq\frac{4C_{h}}{h^{2}(1/2)}\{\int_{c}^{1-c}{\mathbb{K}}_{n}^{2} (t)\,dt/\overline{X_{n}^{2}}\}+\frac{4C_{h}}{h^{2}(c)}\cdot\{\overline{X_{n}^ {2}}/\overline{X_{n}^{2}}\}\] (d) \[<\frac{\epsilon}{2}+\frac{\epsilon}{2}\cdot 1=\epsilon\qquad \mbox{ for all $n$ exceeding some $n_{\epsilon}$},\]

using remark 6.1 for \(\overline{X_{n}^{2}}\to_{a.s.}\infty\) in the final step. (Thus the numerator of the leading term in (c) converges a.s. to a finite number, while the denominator has an a.s. limit exceeding \(2/\epsilon\) times the numerator.) \(\Box\)

**Exercise 6.4** (a) Prove proposition 6.1 for general \(0<r<s\).

(b) Now for iid rvs, (4) implies that \(\to_{p}0\) as well. Hence, \(\to_{p}0\) is immediate if \(X_{n1},\ldots,X_{nn}\) are iid as \(X\), for each \(n\). Be clear on this.

**Proposition 6.2** (Equivalent versions of negligibility): _For any vector \({\rm X}\equiv(X_{1},\ldots,X_{n})\), let_

\[{\cal D}_{n}^{2}\equiv[{\rm max}_{1\leq k\leq n}\,\frac{1}{n}|X_{k}-\bar{X}_{n}| ^{2}]/S_{n}^{2}\qquad\mbox{ where }S_{n}^{2}\equiv\overline{X_{n}^{2}}-(\bar{X}_{n})^{2} \tag{5}\]

_Let \(X_{1},X_{2},\ldots\) be iid as \(X\), and set X= \((X_{1},\ldots,X_{n})^{\prime}\). Then_

\[{\cal D}_{n}^{2}\to_{a.s.}0\qquad\mbox{ if and only if }\qquad[{\rm max}_{1\leq k \leq n}\,\frac{1}{n}X_{k}^{2}]/\overline{X_{n}^{2}}\to_{a.s.}0. \tag{6}\]

_Let \(X_{n1},\ldots,X_{nn}\) be iid as \(X\), for each \(n\geq 1\). Set X= \((X_{n1},\ldots,X_{nn})^{\prime}\). Then_

\[{\cal D}_{n}^{2}\to_{p}0\qquad\mbox{ if and only if }\qquad[{\rm max}_{1\leq k \leq n}\,\frac{1}{n}X_{nk}^{2}]/\overline{X_{n}^{2}}\to_{p}0. \tag{7}\]

**Proof**. Consider (6). Note that

(a) \[{\cal D}_{n}^{2}=\left\{{\rm max}\,|\frac{X_{k}}{\sqrt{n\overline{X_{n}^{2}}}} -\frac{\bar{X}_{n}}{\sqrt{n\overline{X_{n}^{2}}}}|\right\}^{2}/\{1-(\bar{X}_{ n})^{2}/\overline{X_{n}^{2}}\}.\]

Since \(0\leq(\bar{X}_{n})^{2}/\overline{X_{n}^{2}}\leq 1\) always holds by the Liapunov inequality, we have

(b) \[|\bar{X}_{n}|/\sqrt{n\overline{X_{n}^{2}}}\leq 1/\sqrt{n}\to_{a.s.}0\mbox{ always holds for }\mbox{\it all }{\rm rvs}.\]

Thus the second term in the numerator always goes to zero for all rvs (independent, or not). Consult remark 6.1 for the following two claims. The denominator of (a) converges a.s. to \(1-0=1\) if \({\rm E}X^{2}=\infty\) (by (4)), while the denominator of (a) converges a.s. to \((1-{\rm E}^{2}X/{\rm E}X^{2})<1\) if \({\rm E}X^{2}<\infty\). Thus \({\cal D}_{n}^{2}(\omega)\to 0\) for a.e. fixed \((X_{1}(\omega),X_{2}(\omega),\ldots)\) if and only if the lead term in the numerator of (a) goes to zero for a.e. fixed \(\omega\); that is, if and only if \([{\rm max}\,|X_{k}|]/\{n\overline{X_{n}^{2}}\}^{1/2}\to 0\) for a.e. fixed \(\omega\). This gives (6). Then (7) follows analogously using exercise 6.4b. \(\square\)

**Proposition 6.3** Let \(X\geq 0\) with \({\rm E}X^{s}=\infty\) have df \(F\) and qf \(K\). The \(r\)th _partial absolute moment_\(M_{r}\) is defined on \([0,1]\) by

\[M_{r}(t)\equiv\int_{0}^{1-t}K^{r}(u)\,du=\int_{t}^{1}m_{r}(u)\,du,\quad\mbox{ where}\quad m_{r}(t)\equiv[K(1-t)]^{r}. \tag{8}\]

Then, for \(0<r<s\),

\[tm_{s}(t)/M_{s}(t)\to 0\ \mbox{ implies }\ tm_{r}(t)/M_{r}(t)\to 0. \tag{9}\]

(Sections C.2-C.3) contain a good deal more in the spirit of (1)-(3) and (9).)

**Proof**. Raising to a power increases a maximum more than an average, and so

\[\frac{tm_{r}(t)}{M_{r}(t)}=\frac{tK^{r}(1-t)}{\int_{(t,1]}K^{r}(1-u)\,du}\leq \frac{t}{1-t}\,\frac{K^{s}(1-t)}{\frac{1}{1-t}\int_{(t,1]}K^{s}(1-u)\,du}=\frac {tm_{s}(t)}{M_{s}(t)} \tag{10}\]

for all \(t\) so close to \(1\) that \(K(1-t)>1\). This establishes (9). \(\square\)

[MISSING_PAGE_FAIL:143]

## Chapter 7 Independence and Conditional Distributions

### 1 Independence

The idea of independence of events \(A\) and \(B\) is that the occurrence or nonoccurrence of \(A\) has absolutely nothing to do with the occurrence or nonoccurrence of \(B\). It is customary to say that \(A\) and \(B\) are _independent events_ if

(1) \(P(AB)=P(A)P(B)\).

Classes \(\mathcal{C}\) and \(\mathcal{D}\) of events are called _independent classes_ if (1) holds for all \(\mathcal{A}\in\mathcal{C}\) and all \(\mathcal{B}\in\mathcal{D}\). We need to define the independence of more complicated collections.

**Definition 1.1** (Independence): Consider a fixed probability space \((\Omega,\mathcal{A},P)\).

(a) Consider various sub \(\sigma\)-fields of \(\mathcal{A}\). Call such \(\sigma\)-fields \(\mathcal{A}_{1},\ldots,\mathcal{A}_{n}\)_independent \(\sigma\)-fields_ if they satisfy

(2) \(P(A_{1}\cap\cdots\cap A_{n})=\prod_{1}^{n}P(A_{i})\) whenever \(A_{i}\in\mathcal{A}_{i}\) for \(1\leq i\leq n\).

The \(\sigma\)-fields \(\mathcal{A}_{1},\mathcal{A}_{2},\ldots\) are called _independent \(\sigma\)-fields_ if \(\mathcal{A}_{1},\ldots,\mathcal{A}_{n}\) are independent for each \(n\geq 2\). (Use this definition for arbitrary classes \(\mathcal{A}_{1},\ldots,\mathcal{A}_{n}\), too.)

(b) Rvs \(X_{1},\ldots,X_{n}\) are called _independent \(rvs\)_ if the \(\sigma\)-fields \(\mathcal{F}(X_{i})\equiv X_{i}^{-1}(B)\), for \(1\leq i\leq n\), are independent. Rvs \(X_{1},X_{2},\ldots\) are called _independent \(rvs\)_ if \(X_{1},\ldots,X_{n}\) are independent for each \(n\geq 2\).

(c) Events \(A_{1},\ldots,A_{n}\) are called _independent events_ if the \(\sigma\)-fields \(\sigma[A_{1}],\ldots,\sigma[A_{n}]\) are independent \(\sigma\)-fields; here note that

(3) \(\sigma[A_{i}]=\{\phi,A_{i},A_{i}^{c},\Omega\}\).

The next exercise is helpful because it will relate the rather formidable definition of independent events in (3) back to the simple definition (1).

**Exercise 1.1**: (a) Show that \(P(AB)=P(A)P(B)\) if and only if \(\{\emptyset,A,A^{c},\ \Omega\}\) and \(\{\emptyset,B,B^{c},\Omega\}\) are independent \(\sigma\)-fields. [Thus we maintain the familiar (1).]

(b) Show that \(A_{1},\ldots,A_{n}\) are independent if and only if

(4) \(P(A_{i_{1}}\cdots A_{i_{k}})=\prod_{j=1}^{k}P(A_{i_{j}})\) whenever \(1\leq i_{1}<\cdots<i_{k}\leq n\) with \(1\leq k\leq n\).

**Remark 1.1**: When discussing a pair of possibly independent events, one should draw the Venn diagram as a square representing \(\Omega\) divided into half vertically (with respect to \(A,A^{c}\)) and into half horizontally (with respect to \(B,B^{c}\)) creating four cells (rather than as the familiar two-circle picture). Also, if one writes on the table the probability of each of the four combinations \(AB\), \(AB^{c},A^{c}B,A^{c}B^{c}\), one has the contingency table superimposed on the picture. (See figure 1.1) [This extends to two partitions \((A_{1},\ldots,A_{m})\) and \((B_{1},\ldots,B_{n})\), but not to three events.] \(\square\)__

**Theorem 1.1** (Expectation of products): Suppose \(X\) and \(Y\) are independent rvs for which \(g(X)\) and \(h(Y)\) are integrable. Then \(g(X)h(Y)\) is integrable, and

\[\mathrm{E}[g(X)h(Y)]=\mathrm{E}g(X)\mathrm{E}h(Y). \tag{5}\]

**Proof.** The assertion is obvious for \(g=1_{A}\) and \(h=1_{B}\). Now hold \(g=1_{A}\) fixed, and proceed through simple and nonnegative \(h\). Then with \(h\) held fixed, proceed through simple, nonnegative, and integrable \(g\). Then extend to integrable \(h\). \(\square\)__

**Proposition 1.1** (Extending independence on \(\pi\)-systems):

(a) Suppose the \(\pi\)-system \(\mathcal{C}\) and a class \(\mathcal{D}\) are independent. Then

\[\sigma[\mathcal{C}]\quad\mbox{ and }\quad\mathcal{D}\quad\mbox{ are independent.}\]

(b) Suppose the \(\pi\)-systems \(\mathcal{C}\) and \(\mathcal{D}\) are independent. Then

\[\sigma[\mathcal{C}]\quad\mbox{ and }\quad\sigma[\mathcal{D}]\quad\mbox{ are independent $\sigma$-fields.}\]

(c) If \(\mathcal{C}_{1},\ldots,\mathcal{C}_{n}\) are independent \(\pi\)-systems (see (2)), then

\[\sigma[\mathcal{C}_{1}],\ldots,\sigma[\mathcal{C}_{n}]\quad\mbox{ are independent $\sigma$-fields.}\]

**Proof.** (a) Fix \(D\in\mathcal{D}\), and define

\[\mathcal{C}_{D}\equiv\{A\in\sigma[\mathcal{C}]:P(AD)=P(A)P(D)\}.\]

We now demonstrate that \(\mathcal{C}_{D}\) is a \(\lambda\)-system (that trivially contains \(\mathcal{C}\)). Trivially, \(\Omega\in\mathcal{C}_{D}\). If \(A,B\in\mathcal{C}_{D}\) with \(A\subset B\), then

\[P((A\setminus B)D)=P(AD\setminus BD)=P(AD)-P(BD)\]

Figure 1.1: The \(2\times 2\) table.

(j) \[=P(A)P(D)-P(B)P(D)=P(A\setminus B)P(D);\]

and this implies that \(A\setminus B\in{\cal C}_{D}\). If \(A_{n}\nearrow A\) with all \(A_{n}\in{\cal C}_{D}\), then

(k) \[P(AD)=P(\lim A_{n}D)=\lim P(A_{n}D)=\lim P(A_{n})P(D)=P(A)P(D);\]

and this implies that \(A\in{\cal C}_{D}\). Thus \({\cal C}_{D}\) is a \(\lambda\)-system, and it trivially contains the \(\pi\)-system \({\cal C}\). Thus \({\cal C}_{D}\supset\lambda[{\cal C}]=\sigma[{\cal C}]\), using (1.1.19) for the equality. Finally, this is true for every \(D\in{\cal D}\).

Just apply part (a) to the \(\pi\)-system \({\cal D}\) and the arbitrary class \(\sigma[C]\) to obtain (b). Part (c) is left to exercise 1.3. \(\Box\)

**Theorem 1.2**: Let \(X_{1},X_{2},\ldots\) be independent rvs on \((\Omega,{\cal A},P)\). Let \({\rm i}\equiv(i_{1},i_{2},\ \ldots)\) and \({\rm j}\equiv(j_{1},j_{2},\ldots)\) be disjoint sets of integers. (a) Then

\[{\cal F}(X_{i_{1}},X_{i_{2}},\ldots)\ {\rm and}\ {\cal F}(X_{j_{1}},X_{j_{2}}, \ldots)\quad\ {\rm are\ independent\ }\sigma\hbox{-fields}. \tag{6}\]

(b) This extends immediately to countably many disjoint sets of integers.

**Corollary 1** (Preservation of independence): Any rvs \(h_{1}(X_{i_{1}},X_{i_{2}},\ldots)\), \(h_{2}(X_{j_{1}},X_{j_{2}}\ldots),\ldots\) (that are based on disjoint sets of the underlying independent rvs \(X_{k}\)) are themselves independent rvs, for any choice of the \({\cal B}\)-\({\cal B}_{\infty}\)-measurable functions \(h_{1},h_{2},\ldots\).

**Proof.** Let \({\cal C}\) denote all sets of the form \(C\equiv[X_{i_{1}}\in B_{1},\ldots,X_{i_{m}}\in B_{m}]\), for some \(m\geq 1\) and for \(B_{1},\ldots,B_{m}\) in \({\cal B}\). Let \({\cal D}\) denote all sets of the form \(D\equiv[X_{j_{1}}\in B^{\prime}_{1},\ldots,X_{j_{n}}\in B^{\prime}_{n}]\) for some \(n\geq 1\) and sets \(B^{\prime}_{1},\ldots,B^{\prime}_{n}\) in \({\cal B}\). Both \({\cal C}\) and \({\cal D}\) are \(\pi\)-systems, while \(\sigma[{\cal C}]={\cal F}(X_{i_{1}},X_{i_{2}},\ldots)\) and \(\sigma[{\cal D}]={\cal F}(X_{j_{1}},X_{j_{2}},\ldots)\). In fact, \({\cal F}(X_{i_{1}},X_{i_{2}},\ldots)={\rm X}_{i}^{-1}({\cal B}_{\infty})={\rm X }_{i}^{-1}\)\((\sigma[C_{\infty}])=\sigma[{\rm X}_{i}^{-1}(C_{\infty})]=\sigma[C]\). Thus

(p) \[P(C\cap D)=P(\{\bigcap_{k=1}^{m}[X_{i_{k}}\in B_{k}]\}\cap\{ \bigcap_{l=1}^{n}[X_{j_{l}}\in B^{\prime}_{l}]\})\] \[\qquad=\prod_{k=1}^{m}P(X_{i_{k}}\in B_{k})\prod_{l=1}^{n}P(X_{j_ {l}}\in B^{\prime}_{l})\qquad\ {\rm by\ independence}\] \[\qquad=P(\bigcap_{k=1}^{m}[X_{i_{k}}\in B_{k}])P(\bigcap_{l=1}^{ n}[X_{j_{l}}\in B^{\prime}_{l}])\qquad\ {\rm by\ independence}\] (q) \[\qquad=P(C)P(D),\]

so that \({\cal C}\) and \({\cal D}\) are independent classes. Thus \(\sigma[{\cal C}]\) and \(\sigma[{\cal D}]\) are independent by proposition 1.1(b), as is required for (6). The extension to countably many disjoint sets of indices is done by induction using proposition 1.1(c), and is left to the exercises. (The corollary is immediate.) \(\Box\)

**Exercise 1.2**: Prove theorem 1.2(b).
**Exercise 1.3**: Prove proposition 1.1(c).

**Criteria for Independence**

**Theorem 1.3**: The rvs \((X_{1},\ldots,X_{n})\) are independent rvs if and only if

\[F_{X_{1},\ldots,X_{n}}(x_{1},\ldots,x_{n})=F_{X_{1}}(x_{1})\cdots F_{X_{n}}(x_{ n})\quad{\rm for\ all}\ x_{1},\ldots,x_{n}. \tag{7}\]

**Proof.** Clearly, independence implies that the joint df factors. For the converse we suppose that the joint df factors. Then for all \(x_{1},\ldots,x_{n}\) we have

\[P(X_{1}\leq x_{1},\ldots,X_{n}\leq x_{n})=P(X_{1}\leq x_{1})\cdots P(X_{n}\leq x _{n})\.\]

That is, the classes \(\mathcal{C}_{i}\equiv\{[X_{i}\leq x_{i}]:x_{i}\in R\}\) are independent, and they are \(\pi\)-systems, with \(\sigma[\mathcal{C}_{i}]=\mathcal{F}(X_{i})\). Independence of \(X_{1},\ldots,X_{n}\) then follows from proposition 1.1(c). \(\Box\)

**Exercise 1.4**: Rvs \(X,Y\) that take on only a countable number of values are independent if and only if \(P([X=a_{i}][Y=b_{j}])=P(X=a_{i})P(Y=b_{j})\) for all \(i,j\).

**Exercise 1.5**: Show that rvs \(X,Y\) having a joint density \(f(.,\,.)\) are independent if and only if the joint density factors to give \(f(x,y)=f_{X}(x)f_{Y}(y)\) for a.e. \(x,y\).

**Remark 1.2**: That rv's \(X,Y\) are independent if and only if their characteristic function factors appears as theorem 9.5.3 of Section 9. \(\Box\)

## 2 The Tail \(\sigma\)-Field

**Definition 2.1** (The tail \(\sigma\)-field): Consider an arbitrary random element \({\sf X}\equiv(X_{1},\,X_{2},\ldots)\) from \((\Omega,{\cal A},P)\) to \((R_{\infty},{\cal B}_{\infty})\). Then \({\cal T}\equiv\bigcap_{n=1}^{\infty}{\cal F}(X_{n},X_{n+1},\ldots)\) is called the _tail \(\sigma\)-field_, and any event \(D\in{\cal T}\) is called a _tail event_.

**Theorem 2.1** (Kolmogorov's 0-1 law): If \(X_{1},X_{2},\ldots\) are independent rvs, then \(P(D)\) equals 0 or 1 for all tail events \(D\) in the tail \(\sigma\)-field \({\cal T}\).

**Proof.** Fix a set \(D\in{\cal T}\), and then note that \(D\in{\cal F}({\sf X})={\sf X}^{-1}({\cal B}_{\infty})\). By the Halmos approximation lemma of exercise 1.2.3 and the introduction to section 2.5, there exists an integer \(n\) and a set \(D_{n}\) in the \(n\)th member of \(\cup_{m}{\cal F}(X_{1},\ldots,X_{m})=\cup_{m}{\sf X}^{-1}_{m}(B_{m})=\) (a field) such that \(P(D_{n}\triangle D)\to 0\). Thus both \(P(D\cap D_{n})\to P(D)\) and \(P(D_{n})\to P(D)\) occur. Happily, \(D\in{\cal T}\subset{\cal F}(X_{n+1},\ldots)\). so that \(D\) and \(D_{n}\in{\cal F}(X_{1},\ldots,X_{n})\) are independent. Hence

\[P^{2}(D)\gets P(D)P(D_{n})=P(D\cap D_{n})\to P(D),\]

yielding \(P^{2}(D)=P(D)\). Thus \(P(D)=0\) or 1. \(\square\)

**Remark 2.1** (Sequences and series of independent rvs converge a.s., or almost never): Note that for any Borel sets \(B_{1},B_{2},\ldots\) in \({\cal B}\),

\[[X_{n}\in B_{n}\mbox{ i.o.}]\mbox{ equals }\overline{\lim}[X_{n}\in B_{n}]= \bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}[X_{m}\in B_{m}]\in{\cal T}, \tag{1}\]

since \(\bigcup_{m=n}^{\infty}[X_{m}\in B_{m}]\in{\cal F}(X_{n},\ldots)\). Also,

\[\underline{\lim}[X_{n}\in B_{n}]=\bigcup_{n=1}^{\infty}\bigcap_{ m=n}^{\infty}[X_{m}\in B_{m}]=(\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}[X_{ m}\in B_{m}^{c}])^{c} \tag{2}\] \[=(\overline{\lim}[X_{n}\in B_{n}^{c}])^{c}\in{\cal T}.\]

Note also that

\[[\omega\ :\ X_{n}(\omega)\to\ (\mbox{some finite }X(\omega))]^{c}=[ \omega\ :\ X_{n}(\omega)\nrightarrow(\mbox{some finite }X(\omega))] \tag{3}\] \[=\bigcup_{k=1}^{\infty}\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty }[\omega:|X_{m}(\omega)-X_{n}(\omega)|>1/k]\in(\bigcup_{k=1}^{\infty}{\cal T} )={\cal T}.\]

Likewise, if \(S_{n}=\sum_{i=1}^{n}X_{i}\), then

\[[\omega\ :\ S_{n}(\omega)\rightarrow(\mbox{some finite }S(\omega))]^{c} \tag{4}\] \[=\bigcup_{k=1}^{\infty}\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty }[\omega:|\sum_{i=n+1}^{m}X_{i}(\omega)|>1/k]\in{\cal T}.\]

The following result has thus been established. \(\square\)

**Theorem 2.2**: Sequences and series of independent rvs can only converge either a.s. or almost never.

**The Symmetric \(\sigma\)-Field**

**Definition 2.2** (Symmetric sets): Let \(\pi\) denote any mapping of the integers onto themselves that (for some finite \(n\)) merely permutes the first \(n\) integers. Let \({\sf X}\equiv(X_{1},X_{2},\ldots)\) be \({\cal B}_{\infty}\)-measurable, and set \({\sf X}_{\pi}\equiv(X_{\pi(1)},X_{\pi(2)},\ldots)\). Then \(A\equiv{\sf X}^{-1}(B)\) for some \(B\in{\cal B}_{\infty}\) is called a _symmetric set_ if \(A={\sf X}_{\pi}^{-1}(B)\) for all such \(\pi\). Let \({\cal S}\) denote the collection of all symmetric sets.

**Exercise 2.1** (Hewitt-Savage 0-1 law)

Let \({\sf X}\equiv(X_{1},X_{2},\ldots)\) have iid coordinates \(X_{k}\).

(a) Show that \(P(A)\) equals 0 or 1 for every \(A\) in \({\cal S}\).

(b) Show that \({\cal S}\) is a \(\sigma\)-field, called the _symmetric_\(\sigma\)-field.

(c) Show that the tail \(\sigma\)-field \({\cal T}\) is a subset of the symmetric \(\sigma\)-field \({\cal S}\).

(d) Give an example where \({\cal T}\) is a proper subset of \({\cal S}\).

(Hint. Use the approximation lemma of exercise 1.2.3 as it was used above.)

## 3 Uncorrelated Random Variables

Recall from definition 3.4.1 that \(X\cong(\mu,\sigma^{2})\) denotes that the rv \(X\) on a probability space \((\Omega,{\cal A},P)\) has mean \(\mu\) and a variance \(\sigma^{2}\) that is assumed to be finite.

**Definition 3.1** (Correlation): If \(X_{1},\ldots,X_{n}\) have finite variances, then we call them _uncorrelated_ if \({\rm Cov}[X_{i},X_{j}]\equiv{\rm E}\{(X_{i}-{\rm E}X_{i})(X_{j}-{\rm E}X_{j}) \}=0\) for all \(i\neq j\). Define the dimensionless quantity

\[{\rm Corr}[X_{i},X_{j}]\equiv\frac{{\rm Cov}[X_{i},X_{j}]}{\sqrt{{\rm Var}[X_{ i}]{\rm Var}[X_{j}]}}\]

to be the _correlation_ between \(X_{i}\) and \(X_{j}\). If \({\rm X}\equiv(X_{1},\ldots,X_{n})\), then the \(n\times n\)_covariance matrix_ of X is defined to be the matrix \(\Sigma\equiv|[\sigma_{ij}]|\) whose \((i,j)\)th element is \(\sigma_{ij}\equiv{\rm Cov}[X_{i},\ X_{j}]\).

**Proposition 3.1**: Independent rvs with finite variances are uncorrelated.

**Proof.** Now,

(a) \[{\rm Cov}\ [X,Y]={\rm E}[(X-\mu_{X})(Y-\mu_{Y})]={\rm E}(X-\mu_{X}){\rm E}(Y- \mu_{Y})=0\cdot 0=0,\]

where

(b) \[0\leq\{{\rm Cov}\ [X,Y]\}^{2}\leq{\rm Var}[X]\ {\rm Var}\ [Y]<\infty\]

by Cauchy-Schwarz. \(\Box\)

Note from (b) (or recall from (3.4.14)) that

\[\mid{\rm Corr}\ [X,Y]\mid\leq 1\qquad\mbox{ for any $X$ and $Y$ having finite variances.} \tag{1}\]

**Proposition 3.2**: If \((X_{1},\ldots,X_{n})\) are uncorrelated and \(X_{i}\cong(\mu_{i},\sigma_{i}^{2})\), then

\[\sum_{1}^{n}a_{i}X_{i}\cong\left(\sum_{1}^{n}a_{i}\mu_{i},\sum_{1}^{n}a_{i}^{2 }\sigma_{i}^{2}\right). \tag{2}\]

In particular, suppose \(X_{1},\ldots,X_{n}\) are uncorrelated \((\mu,\sigma^{2})\). Then

\[\bar{X}_{n}\equiv\frac{1}{n}\sum_{1}^{n}X_{i}\cong\left(\mu,\frac{\sigma^{2}}{ n}\right),\qquad\mbox{ while }\qquad\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\cong(0,1), \tag{3}\]

provided that \(0<\sigma<\infty\). Moreover

\[{\rm Cov}\ \left[\sum_{i=1}^{m}a_{i}X_{i},\sum_{j=1}^{n}b_{j}Y_{j} \right]={\rm Cov}\left[\sum_{i=1}^{m}a_{i}(X_{i}-\mu_{X_{i}}),\sum_{j=1}^{n}b_ {j}(Y_{j}-\mu_{Y_{j}})\right] \tag{4}\] \[=\sum_{i=1}^{m}\sum_{j=1}^{n}a_{i}b_{j}\ {\rm Cov}\ [X_{i},Y_{j}].\]

Note that if

\[Y=AX,\ \mbox{then}\ \Sigma_{Y}=A\Sigma_{X}A^{\prime}. \tag{5}\]

**Proof.** This is all trivial. \(\Box\)

## 4 Basic Properties of Conditional Expectation \({}^{0}\)

The Lebesgue integral is a widely applicable tool that extended the value of the Reimann approach. It allows more general "heavy duty results." So too, we now need to extend and rigorize our elementary approach to conditional expectation, in a way that keeps the useful results intact. (Illustrations follow the definitions.)

**Definition 4.1** (Conditional expectation): Let \((\Omega,{\cal A},P)\) denote a probability space. Let \({\cal D}\) denote a sub \(\sigma\)-field of \({\cal A}\). Let \(Y\) be a rv on \((\Omega,{\cal A},P)\) for which \({\rm E}|Y|<\infty\). By \({\rm E}(Y|{\cal D})(\cdot)\) we mean any \({\cal D}\)-measurable function on \(\Omega\) such that

\[\int_{D}{\rm E}(Y|{\cal D})(\omega)\,dP(\omega)=\int_{D}Y(\omega)\,dP(\omega) \qquad\qquad\qquad\mbox{for all }\,\,D\in{\cal D}. \tag{1}\]

Such a function exists and is unique a.e. \(P\), as is seen below; we call this the _conditional expectation of \(Y\) given \({\cal D}\)_. If \(X\) is another rv on \((\Omega,{\cal A},P)\), then

\[{\rm E}(Y|X)(\omega)\equiv{\rm E}(Y|{\cal F}(X))(\omega); \tag{2}\]

we recall that \({\cal F}(X)\equiv X^{-1}({\cal B})\) for the Borel subsets \({\cal B}\) of the real line \(R\).

**Justification** of definition 4.1. Let \({\rm E}|Y|<\infty\). Define a signed measure \(\nu\) on \({\cal D}\) by

(a) \[\nu(D)\equiv\int_{D}Y\,dP\qquad\qquad\qquad\mbox{for all }D\in{\cal D}.\]

Now, \(\nu\) is a signed measure on \((\Omega,{\cal D})\) by example 4.1.1, and the restriction of \(P\) to \({\cal D}\) (denoted by \(P|{\cal D}\)) is another signed measure on \((\Omega,{\cal D})\). Moreover, that \(\nu\ll P|{\cal D}\) is trivial. Thus the Radon-Nikodym theorem guarantees, uniquely a.e. \(P|{\cal D}\), a \({\cal D}\)-measurable function \(h\) such that (recall exercise 3.2.3 for the second equality)

(b) \[\nu(D)=\int_{D}hd(P|{\cal D})=\int_{D}h\,dP\qquad\mbox{ for all }D\in{\cal D}.\]

Now, being \({\cal D}\)-measurable and unique a.s. \(P|{\cal D}\) implies that the function \(h\) is unique a.s. \(P\). Define \({\rm E}(Y|{\cal D})\equiv h\). Radon-Nikodym derivatives are only unique a.e., and any function that works is called a _version_ of the Radon-Nikodym derivative. \(\Box\)

**Proposition 4.1**: Suppose that \(Z\) is a rv on \((\Omega,{\cal A})\) that is \({\cal F}(X)\)-measurable. Then there exists a measurable function \(g\) on \((R,{\cal B})\) such that \(Z=g(X)\).

**Proof.** This is just proposition 2.2.5 again. \(\Box\)

**Notation 4.1**: Since \({\rm E}(Y|X)={\rm E}(Y|{\cal F}(X))\) is \({\cal F}(X)\)-measurable, the previous proposition shows that \(h\equiv{\rm E}(Y|X)=g(X)\) for some measurable function \(g\) on \((R,{\cal B})\). The theorem of the unconscious statistician gives \(\int_{x^{-1}(B)}g(X)\,dP=\int_{B}g\,dP_{X}\), where we have written the general set \(D\in{\cal F}(X)\) as \(D=X^{-1}(B)\) for some \(B\in{\cal B}\). Thus we may define \({\rm E}(Y|X=x)=g(x)\) to be a \({\cal B}\)-measurable function on \(R\) for which

\[\int_{B}{\rm E}(Y|X=x)\,dP_{X}(x)=\int_{X^{-1}(B)}Y(\omega)\,dP(\omega)\qquad \mbox{ for all }B\in{\cal B}. \tag{3}\]

This function \({\rm E}(Y\,|\,X=x)\) exists and is unique a.s. \(P_{X}\), as above. In summary:

\[\mbox{If }g(x)\equiv{\rm E}(Y\,|\,X=x),\quad\mbox{then }h(\omega)\equiv{\rm E}(Y \,|\,X)(\omega)=g(X(\omega)). \tag{4}\]Definition 4.2 (Conditional probability): Since \(P(A)=\mathrm{E1}_{A}\) for standard probability, define the _conditional probability of A given \(\mathcal{D}\)_, denoted by \(P(A|\mathcal{D})\), by

\[P(A|\mathcal{D})\equiv\mathrm{E}(1_{A}|\mathcal{D}). \tag{5}\]

Equivalently, \(P(A|\mathcal{D})\) is a \(\mathcal{D}\)-measurable function on \(\Omega\) satisfying

\[P(A\cap D)=\int_{D}P(A|\mathcal{D})\,dP\] for all

\[D\in\mathcal{D}\]

, and it exists and is unique a.s. \(P\). Also,

\[P(A|X)\equiv P(A|\mathcal{F}(X))\]

Thus \(P(A|X)(\omega)=g(X(\omega))\), where \(g(x)\equiv P(A|X=x)\) is a \(\mathcal{B}\)-measurable function satisfying

\[P(A\cap X^{-1}(B))=\int_{B}P(A|X=x)\,dP_{X}(x)\qquad\text{ for all }B\in \mathcal{B}. \tag{7}\]

This function exists and is unique a.s. \(P_{X}\).

**Discussion 4.1** (Discrete case; the elementary treatment): Given that the event \(B\) has occurred (with \(P(B)>0\)), how likely is it now for the event \(A\) to occur. The classic elementary approach defines the _conditional probability_ of _A given \(B\)_ by \(P(A|B)\equiv P(AB)/P(B)\). Thus we have taken a revisualized view of things, while regarding \(B\) as the updated sample space. For any event \(B\), only that portion \(AB\) of the event \(A\) is relevant (as \(B\) was known to have occurred). Thus all that matters is the probabilistic size \(P(AB)\) of \(AB\) relative to the probabilistic size \(P(B)\) of \(B\). The resulting \(P(\cdot|B)\) is a probability distribution over \(\mathcal{A}_{B}\equiv\{AB:A\in\mathcal{A}\}\).

For discrete rvs \(X\) and \(Y\) with mass function \(p(\cdot,\cdot)\) this leads to

\[p_{Y|X=x}(y)\equiv p(x,y)/p_{X}(x),\quad\text{for each $x$ for which $p_{X}(x) \neq 0$,}\]

for the conditional mass function. It is then natural to define

\[\mathrm{E}(\psi(Y)|X=x)\equiv\sum_{\text{all $y$}}\psi(y)p_{Y|X=x}(y),\text{ for each $x$ with $p_{X}(x)\neq 0$}\]

when \(\mathrm{E}|\psi(Y)|<\infty\). It is then elementary to show that \(\mathrm{E}(Y)=\mathrm{E}(\mathrm{E}(\psi(Y)|X))\). \(\square\)

**Exercise 4.1 ("Discrete" conditional probability; the general treatment): Suppose \(\Omega=\sum_{i}D_{i}\), and then define \(\mathcal{D}=\sigma[D_{1},D_{2},\ldots]\). Show that (whether the summation is finite or countable) the different expressions needed on the different sets \(D_{i}\) of the partition \(\mathcal{D}\) can be combined together via**

\[P(A|\mathcal{D})=\sum_{i}\frac{P(AD_{i})}{P(D_{i})}1_{D_{i}}, \tag{8}\]

where (just for definiteness) \(\frac{P(AD_{i})}{P(D_{i})}\equiv P(A)\) if \(P(D_{i})=0\). For general \(Y\in\mathcal{L}_{1}\) show that the function \(\mathrm{E}(Y|\mathcal{D})\) takes the form

\[\mathrm{E}(Y|\mathcal{D})=\sum_{i}\left\{\frac{1}{P(D_{i})}\int_{D_{i}}Y\,dP \right\}1_{D_{i}}, \tag{9}\]

with the term in braces defined to be \(0\) if \(P(D_{i})=0\) (just for definiteness). (We note that the standard elementary approach to conditional probability is, in the discrete case, embedded within (8) and (9)-but it sits in there "differently.")

**Example 4.1**  Let an urn consist of six balls identical except for the numbers 1, 2, 2, 3, 3. Let \(X_{1}\) and \(X_{2}\) represent a sample of size two drawn with replacement, and set \(Y=X_{2}\) and \(S=X_{1}+X_{2}\). Consider figure 4.1 above. In the figure (a) we see the sample space \(\Omega\) of \((X_{1},X_{2})\) with the values of \(S\) superimposed, while the figure (b) superimposes the probability function on the same representation of \(\Omega\). In the figure (c) we picture the five "diagonal sets" that generate \(\mathcal{D}\equiv S^{-1}(B)\). The three-part figure (d) depicts \(P(Y=i|\mathcal{D})(\cdot)\) as a \(\mathcal{D}\)-measurable function on \(\Omega\) for each of the three choices \([Y=1],[Y=2]\), and \([Y=3]\) for \(A\), while the figure (e) depicts \(\mathrm{E}(Y|\mathcal{D})(\omega)\) as a \(\mathcal{D}\)-measurable function. (Had we used the elementary definition of \(P(Y=\cdot|S=k)\) as a function of \(y\) for each fixed \(k\), then the conditional distributions would have been those shown along the five diagonals in the figure (f), while \(\mathrm{E}(Y|S=k)\) is shown at the end of each diagonal.) \(\square\)

Figure 4.1: Conditional probability and conditional expectation

**Remark 4.1**: It will be seen that (with \(A\) and \(A_{i}\)'s regarded as fixed)

\[0\leq P(A|{\cal D})\leq 1\qquad\mbox{a.s.}\;P, \tag{11}\] \[P(\sum_{1}^{\infty}A_{i}|{\cal D})=\sum_{1}^{\infty}P(A_{i}|{\cal D })\qquad\mbox{a.s.}\;P,\] (12) \[P(\emptyset|{\cal D})=0\qquad\mbox{a.s.}\;P,\] (13) \[A_{1}\subset A_{2}\qquad\mbox{implies}\qquad P(A_{1}|{\cal D}) \leq P(A_{2}|{\cal D})\quad\mbox{a.s.}\;P. \tag{10}\]

(To see these, just apply parts (16)(monotonicity) and (17)(MCT) of theorem 4.1 appearing below.) These properties remind us of a probability distribution. \(\square\)

**Discussion 4.2** (Continuous case; the elementary treatment) For discrete rvs, the conditional distribution is specified by

\[g(y|x)\equiv P(Y=y|X=x)\equiv P(Y=y\mbox{ and }X=x)/P(X=x).\]

(This is in line with discussion 4.1.) One "natural approximation" of this approach for continuous rvs considers

\[g(y|x)=\lim_{h\to 0}\int_{x-h}^{x+h}f_{X,Y}(r,y)\,dr/\int_{x-h}^{x+h}f_{X}(r) \;dr.\]

But making this approach rigorous fails without sufficient smoothness, and leads to a tedious and limited theory. So elementary texts just suggest the even more blatant and "less rigorous" imitation of the discrete result via

\[g(y|x)\triangle y\doteq\frac{f_{X,Y}(x,y)\triangle x\triangle y}{f_{X}(x) \triangle x}\doteq\frac{f_{X,Y}(x,y)\triangle y}{f_{X}(x)}.\]

Discussion 4.3 suggests that the general approach of this section should ultimately lead to this same elementary result in the case when densities do exist.

Moreover, if \((x(t),y(t)),a\leq t\leq b\), parametrizes a smooth curve (imagine a circle about the origin, or a line of slope \(135^{0}\)), it is definition 4.2 that leads rigorously to formulas of the type

\[\frac{f(x(t),y(t))\sqrt{(dx/dt)^{2}+(dy/dt)^{2}}}{\int_{a}^{b}f(x(t^{\prime}),y (t^{\prime}))\sqrt{(dx/dt^{\prime})^{2}+(dy/dt^{\prime})^{2}}dt^{\prime}}\qquad \mbox{ for }a\leq t\leq b\]

for the conditional density at the point \(t\) given that one is on the curve. \(\square\)

**Discussion 4.3** (Continuous case; the general treatment) Let us consider the current approach to conditional probability. We will illustrate it in a special case. Let \(A\in{\cal B}_{2}\) denote a two-dimensional Borel set. Let \(T\equiv T({\rm X})\equiv(X_{1}^{2}+X_{2}^{2})^{1/2}\), so that \(T=t\) defines (in the plane \(\Omega=R_{2}\)) the circle \(C_{t}\equiv\{(x_{1},x_{2})\;:\;x_{1}^{2}+x_{2}^{2}=t^{2}\}\). Let \(B\in{\cal B}\) denote a one-dimensional Borel set of \(t\)'s, and then let \(D\equiv T^{-1}(B)=\cup\{C_{t}:t\in B\}\). Requirements (6) and (7) (in a manner similar to exercise 4.1, but from a different point of view than used in discussion 4.1) become

\[P(AD)=P(A\cap(\cup\{C_{t}:t\in B\}))=\int_{\cup\{C_{t}:t\in B\}} P(A|T)({\sf x})\,dP_{\sf x}({\sf x})\] \[\qquad\equiv\int_{\cup\{C_{t}:t\in B\}}h_{A}({\sf x})\,dP_{\sf x} ({\sf x})=\int_{B}g_{A}(t)\,dP_{T}(t)\equiv\int_{B}P(A|T=t)\,dP_{T}(t).\]

So if \(g_{A(\cdot)}\) is given a value at \(t\) indicating the probabilistic proportion of \(A\cap C_{t}\) that belongs to \(A\) (or \(h({\sf x})\) is given this same value at all \({\sf x}\in C_{t}\)), then the above equation ought to be satisfied.

(When densities exist, such a value would seem to be \(g_{A}(t)=\int_{C_{t}}1_{A}(\mathrm{x})p_{\mathrm{X}}(\mathrm{x})d\mathrm{x}/\int_{ C_{t}}p_{\mathrm{X}}(\mathrm{x})d\mathrm{x}\), while \(h_{A}(\mathrm{x})=g_{A}(T(\mathrm{x}))\) would assign this same value at each \(\mathrm{x}\in C_{t}\).) The requirements (1) and (2) become

\[\int_{\cup\{C_{t}:t\in B\}}Y\,dP=\int_{\cup\{C_{t}:t\in B\}}h(\mathrm{x})\,dP( \mathrm{x})=\int_{B}g(t)\,dP_{T}(t).\]

(When densities exist, the value \(\mathrm{E}(Y|T=t)=g(t)=\int_{C_{t}}Y(\mathrm{x})p(\mathrm{x})\,d\mathrm{x}/\int _{C_{t}}p(\mathrm{x})\,d\mathrm{x}\) seems appropriate, with \(h(\mathrm{x})\) assigning this same value for all \(\mathrm{x}\) in \(C_{t}\).)

The reader is urged to draw an \((x_{1},x_{2})\)-plane to serve as the sample space for \(\mathsf{X}\equiv(X_{1},X_{2})\) and a half line \([0,\infty)\) to serve as a sample space for \(T\equiv(X_{1}^{2}+X_{2}^{2})^{1/2}\). Then any \(\mathcal{D}\equiv\mathcal{F}(T)\) measurable function, such as \(g_{A}(t)\equiv P(A|T=t)\), leads to the function \(h_{A}(\mathrm{x})=g_{A}(T(\mathrm{x}))=g_{A}(t)\) that is a constant on every circle \(C_{t}\) in the \((x_{1},x_{2})\)-plane. Picture this on your diagram. We need to guarantee that our specific guess for the function \(h_{A}(\mathrm{x})=g_{A}(t)=P(A|T=t)\) works--that is, (6) and (7) hold. Happily, theorems in section 7.5 can be summarized by saying that in all the old standard problems we need not bother. When we fix \(t\), the function \(P(A|T=t)=g_{A}(t)\) behaves just like the old probability distributions over all of the sets \(A\) in \(\mathcal{A}\). Great! So why are we doing this? The answer is by analogy. We already knew how to take the expectation of a rv that has a density (we used an integral), or a mass function (we used a summation), but by this point we have learned how to rigorously do all cases at once using only the expectation sign format \(\mathrm{E}(\cdot)\) (whether the distribution is absolutely continuous, discrete, singular, or a mixture of these). It can be very useful to learn how do general cases for conditional expectation as well. To this end we will now call on John Wayne and the cavalry (and this has the distinct look of the Halmos-Savage approach via a Radon-Nikodym derivative) and the bullets can be found in the upcoming theorem 4.1. 

**Exercise 4.2** (A) (i) Mimic discussion 4.2 in case \(T\equiv X_{1}+X_{2}\), instead.

(ii) Make up another interesting example.

(B) (iii) Repeat example 4.1 and the accompanying figure, but now in the context of sampling without replacement.

(iv) Make up another interesting example.

**Exercise 4.3** Let \(Y\) be a rv on some \((\Omega,\mathcal{A},P)\) that takes on the eight values \(1,\ldots,8\) with probabilities \(1/32\), \(2/32\), \(3/32\), \(4/32\), \(15/32\), \(4/32\), \(1/32\), \(2/32\), respectively. Let \(C\equiv\mathcal{F}(Y)\), and let \(C_{i}\equiv[Y=i]\) and \(p_{i}\equiv P(C_{i})\) for \(1\leq i\leq 8\). Let \(\mathcal{D}\equiv\sigma[\{C_{1}+C_{5},C_{2}+C_{6},C_{3}+C_{7},C_{4}+C_{8}\}], \mathcal{E}\equiv\sigma[\{C_{1}+C_{2}+C_{5}+C_{6},C_{3}+C_{4}+C_{7}+C_{8}\}]\) and \(\mathcal{F}\equiv\{\Omega,\emptyset\}\).

(a) Represent \(\Omega\) as a \(2\times 4\) rectangle having eight \(1\times 1\) cells representing \(C_{1},\ldots,C_{4}\) in the first row and \(C_{5},\ldots,C_{8}\) in the second row. Enter the appropriate values of \(Y(\omega)\) and \(p_{i}\) in each cell, forming a table. Evaluate \(\mathrm{E}(Y)\).

(b) Evaluate \(\mathrm{E}(Y|\mathcal{D})\). Present this function in a similar table. Evaluate \(\mathrm{E}(\mathrm{E}(Y|\mathcal{D}))\).

(c) Evaluate \(\mathrm{E}(Y|\mathcal{E})\). Present this function in a similar table. Evaluate \(\mathrm{E}(\mathrm{E}(Y|\mathcal{E}))\).

(d) Evaluate \(\mathrm{E}(Y|\mathcal{F})\). Present this function in a similar table. Evaluate \(\mathrm{E}(\mathrm{E}(Y|\mathcal{F}))\).

**Theorem 4.1** (Properties of conditional expectation): Let \(X,Y,Y_{n}\) be integrable rvs on \((\Omega,\mathcal{A},P)\). Let \(\mathcal{D}\) be a sub \(\sigma\)-field of \(\mathcal{A}\). Let \(g\) be measurable. Then for _any_ versions of the conditional expectations, the following hold:

(14) \[\text{(Linearity)}\ \mathrm{E}(aX+bY|\mathcal{D})=a\mathrm{E}(X| \mathcal{D})+b\mathrm{E}(Y|\mathcal{D})\ \text{a.s.}\ P\,(\text{or,a.s.}\ (P| \mathcal{D})).\] (15) \[\mathrm{E}Y=\mathrm{E}[\mathrm{E}(Y|\mathcal{D})].\] (16) \[\text{(Monotonicity)}\ X\leq Y\,\text{a.s.}\ P\ \text{implies}\ \mathrm{E}(X|\mathcal{D})\leq\mathrm{E}(Y|\mathcal{D})\ \text{a.s.}\ P.\] (17) (MCT) If \(0\leq Y_{n}\nearrow Y\,\text{a.s.}\ P\), then \(\mathrm{E}(Y_{n}|\mathcal{D})\nearrow\mathrm{E}(Y|\mathcal{D})\) a.s. \(P\).

(18) (Fatou) If \(0\leq Y_{n}\,\mbox{a.s.}\,P,\mbox{ then }\mbox{E}(\underline{\lim}\,Y_{n}|{\cal D}) \leq\underline{\lim}\,\mbox{E}(Y_{n}|{\cal D})\) a.s. \(P.\) (19) (DCT) If all \(|Y_{n}|\leq X\) and \(Y_{n}\to_{a.s.}Y,\mbox{ then }\mbox{E}(Y_{n}|{\cal D})\to_{\mbox{\tiny a.s.}} \mbox{E}(Y|{\cal D}).\) (20) If \(Y\) is \({\cal D}\)-measurable and \(XY\in{\cal L}_{1}(P),\mbox{ then }\mbox{E}(XY|{\cal D})=_{a.s.}Y\mbox{E}(X|{\cal D}).\) (21) If \({\cal F}(Y)\) and \({\cal D}\) are independent, then \(\mbox{E}(Y|{\cal D})=\mbox{E}Y\) a.s. \(P.\) (22) (Stepwise smoothing). If \({\cal D}\subset{\cal E}\subset{\cal A},\) then \(\mbox{E}[\mbox{E}(Y|{\cal E})|{\cal D}]=\mbox{E}(Y|{\cal D})\) a.s. \(P.\) (23) If \({\cal F}(Y,X_{1})\) is independent of \({\cal F}(X_{2}),\mbox{ then }\mbox{E}(Y|X_{1},X_{2})=\mbox{E}(Y|X_{1})\) a.s. \(P.\) (24) \(C_{r},\mbox{H\"{o}lder},\mbox{ Liapunov},\mbox{ Minkowski},\mbox{ and Jensen inequalities hold for }\mbox{E}(\cdot|{\cal D}).\) Jensen: \(g(\mbox{E}(Y|{\cal D}))\leq_{a.s.}\mbox{E}[g(Y)|{\cal D}]\) for \(g\) convex with \(g(Y)\) integrable. (25) Let \(r\geq 1.\) If \(Y_{n}\to_{{\cal L}_{r}}Y,\mbox{ then }\mbox{E}(Y_{n}|{\cal D})\to_{{\cal L} \nabla}\mbox{E}(Y|{\cal D}).\) In fact, \(\mbox{E}|\mbox{E}(X|{\cal D})-\mbox{E}(Y|{\cal D})|^{r}\leq\mbox{E}|X-Y|^{r}.\) (26) \(h_{D}(\cdot)\) is a determination of \(\mbox{E}(Y|{\cal D})\) if and only if \(\mbox{E}(XY)=\mbox{E}(Xh_{D})\) for all \({\cal D}\)-measurable rvs \(X.\) (27) If \(P(D)=0\mbox{ or }1\mbox{ for all }D\in{\cal D},\mbox{ then }\mbox{E}(Y|{\cal D})=\mbox{E}Y\,\mbox{a.s.}\,P.\)

**Proof.\({}^{o}\)** We first prove (14). Now, by linearity of expectation,

(a) 

To prove (15), simply note that

(b) 

For (16), use (14) for the first step of

(c) 

Then \(\mbox{E}(Y|{\cal D})-\mbox{E}(X|{\cal D})\) is a Radon-Nikodym derivative of \(\nu(\cdot)\) and so is \(\geq 0\) a.s.

Statement (17) follows easily from (16), since we have

(d) 

Thus \(\lim_{n}\mbox{E}(Y_{n}|{\cal D})\) exists a.s., and

(e) 

and we can appeal to the uniqueness of Radon-Nikodym derivatives, or apply exercise 3.2.2. Now we use (16) and (17) to prove (18). Thus

\[\mbox{E}(\underline{\lim}\,Y_{n}|{\cal D})=\mbox{E}(\underline{\lim}\,\inf_{k \geq n}Y_{k}|{\cal D})\qquad\mbox{ by the definition of }\underline{\lim}\](f) \[=\lim_{n}\operatorname{E}(\inf_{k\geq n}Y_{k}|\mathcal{D}),\qquad\text{a.s., by the MCT of (17)}\] \[\leq\lim_{n}\inf_{k\geq n}\operatorname{E}(Y_{k}|\mathcal{D})\qquad \text{by the monotonicity of (16)}\] \[=\varliminf\operatorname{E}(Y_{n}|\mathcal{D})\qquad\qquad\text{by the definition of }\varliminf.\]

To prove (19), apply the Fatou of (18) to \(Y_{n}+X\) to get

\[\operatorname{E}(Y|\mathcal{D})+\operatorname{E}(X|\mathcal{D})= \operatorname{E}(X+Y|\mathcal{D})=\operatorname{E}(\varliminf(X+Y_{n})| \mathcal{D})\] (g) \[\leq\varliminf\operatorname{E}(X+Y_{n}|\mathcal{D})=\varliminf \operatorname{E}(Y_{n}|\mathcal{D})+\operatorname{E}(X|\mathcal{D}).\]

Canceling the a.e. finite \(\operatorname{E}(X|\mathcal{D})\) from both ends of (g) gives

\[\operatorname{E}(Y|\mathcal{D})\leq \varliminf\operatorname{E}(Y_{n}|\mathcal{D})\leq \varlimsup\operatorname{E}(Y_{n}|\mathcal{D})\ ;\] (h) \[\leq\operatorname{E}(Y|\mathcal{D})\qquad\qquad\qquad\text{by applying the Fatou of (18) again, to $X-Y_{n}$.}\]

To prove (20) we proceed through indicator, simple, nonnegative, and then general functions, and each time we apply exercise 3.2.2 at the final step.

Case 1: \(Y=1_{D^{*}}\). Then

\[\int_{D}Y\operatorname{E}(X|\mathcal{D})\,dP=\int_{D}1_{D^{*}} \operatorname{E}(X|\mathcal{D})\,dP=\int_{D\cap D^{*}}\operatorname{E}(X| \mathcal{D})\,dP\] \[=\int_{D\cap D^{*}}X\,dP=\int_{D}1_{D^{*}}X\,dP=\int_{D}YX\,dP= \int_{D}\operatorname{E}(YX|\mathcal{D})\,dP.\]

Case 2: \(Y=\sum_{1}^{n}a_{i}1_{D_{i}}\). Then

\[\int_{D}Y\operatorname{E}(X|\mathcal{D})\,dP=\sum_{1}^{n}a_{i} \int_{D}1_{D_{i}}\operatorname{E}(X|\mathcal{D})\,dP\] \[=\sum_{1}^{n}a_{i}\int_{D}1_{D_{i}}X\,dP\qquad\text{by case 1}\] \[=\int_{D}YX\,dP=\int_{D}\operatorname{E}(YX|\mathcal{D})\,dP.\]

Case 3: \(Y\geq 0\). Let simple functions \(Y_{n}\nearrow Y\) where \(Y_{n}\geq 0\). Suppose first that \(X\geq 0\). Then we have

\[\int_{D}\operatorname{E}(YX|\mathcal{D})\,dP=\int_{D} \operatorname{E}(\lim_{n}Y_{n}X|\mathcal{D})\,dP\] \[=\int_{D}\lim_{n}\operatorname{E}(Y_{n}X|\mathcal{D})\,dP\qquad \text{by the MCT of (17)}\] \[=\int_{D}\lim_{n}Y_{n}\operatorname{E}(X|\mathcal{D})\,dP\qquad \text{by case 2}\] \[=\int_{D}Y\operatorname{E}(X|\mathcal{D})\,dP\qquad\qquad\text{ by the MCT.}\]

For general \(X\), use \(X=X^{+}-X^{-}\) and the linearity of (14).

Case 4: General \(Y\). Just write \(Y=Y^{+}-Y^{-}\)

To prove (21), simply note that for each \(D\in\mathcal{D}\) one has

\[\int_{D}\operatorname{E}(Y|\mathcal{D})\,dP=\int_{D}Y\,dP=\int 1_{D}Y\,dP=\operatorname{E}(1_{D}) \operatorname{E}(Y)\] \[=P(D)\operatorname{E}(Y)=\int_{D}\operatorname{E}(Y)\,dP;\]

and apply exercise 3.2.2. Assertion (22) is proved by noting that

\[\int_{D}\operatorname{E}[\operatorname{E}(Y|\mathcal{E})|\mathcal{D}]\,dP= \int_{D}\operatorname{E}[Y|\mathcal{E}]\,dP=\int_{D}Y\,dP\quad\text{since }D\in\mathcal{D}\subset\mathcal{E}\] \[=\int_{D}\operatorname{E}(Y|\mathcal{D})\,dP.\]The integrands of the two extreme terms must be equal a.s. by the exercise 3.2.2.

Consider (23). Now,

\[\mathcal{F}(X_{1},X_{2})=\sigma[D\equiv D_{1}\cap D_{2}\equiv X_{1}^{-1}(B_{1}) \cap X_{2}^{-1}(B_{2}):B_{1},B_{2}\in\mathcal{B}].\]

Let \(D=D_{1}\cap D_{2}\) be any one of the generators of \(\mathcal{F}(X_{1},X_{2})\). Then

\[\nu_{1}(D)\equiv\int_{D}\mathrm{E}(Y|X_{1},X_{2})\,dP=\int_{D}Y\,dP =\int 1_{D_{1}}1_{D_{2}}Y\,dP\] \[\quad=\int 1_{D_{2}}\int_{D_{1}}Y\,dP=\int 1_{D_{2}}\,dP\int_{D_{ 1}}\mathrm{E}(Y|X_{1})\,dP=\int 1_{D_{2}}1_{D_{1}}\mathrm{E}(Y|X_{1})\,dP\] \[\quad=\int_{D}\mathrm{E}(Y|X_{1})\,dP\equiv\nu_{2}(D)\.\]

Since \(\nu_{1}\) and \(\nu_{2}\) are measures on \(\mathcal{F}(X_{1},X_{2})\) that agree on all sets in the \(\bar{\pi}\)-system consisting of all sets of the form \(D=D_{1}\cap D_{2}\), they agree on the \(\sigma\)-field \(\mathcal{F}(X_{1},X_{2})\) by the Dynkin \(\pi\)-\(\lambda\) theorem. Thus the integrands satisfy \(\mathrm{E}(Y|X_{1},X_{2})=\mathrm{E}(Y|X_{1})\,\)a.s.

We next prove (25), leaving most of (24) and (26) to the exercises. We have

\[\mathrm{E}| \mathrm{E}(Y_{n}|\mathcal{D})-\mathrm{E}(Y|\mathcal{D})|^{r}= \mathrm{E}|\mathrm{E}(Y_{n}-Y|\mathcal{D})|^{r}\] (i) \[\leq\mathrm{E}[\mathrm{E}(|Y_{n}-Y|^{r}|\mathcal{D})]\] by the conditional Jensen inequality of (24) \[=\mathrm{E}|Y_{n}-Y|^{r}\] by (15) \[\to 0.\]

To prove (27), note that for all \(D\in\mathcal{D}\) we have

\[\int_{D}\mathrm{E}(Y|\mathcal{D})\,dP=\int_{D}Y\,dP=\left\{\begin{array}{ll} \mathrm{E}(Y)&\mathrm{if}\,P(D)=1\\ 0&\mathrm{if}\,P(D)=0\end{array}\right\}=\int_{D}\mathrm{E}(Y)\,dP.\]

(Durrett) We now turn to the Jensen inequality of (24). The result is trivial for linear \(g\). Otherwise, we define

\[\mathrm{(k)}\qquad C\equiv\{(c,d):c,d\text{ are rational, and }\ell(x)\equiv cx +d\leq g(x)\text{ for all }x\text{ in }I\},\]

and observe that

\[\mathrm{(l)}\qquad g(x)=\sup_{\mathrm{all}\;c,d\in C}(cx+d)\qquad\text{for all }x\in I^{o};\]

this follows from the supporting hyperplane result (below (3.4.3)). For any fixed \(cx+d\) for which \((c,d)\in C\) with \(cx+d\leq g(x)\) on all \((a,b)\) we have

\[\mathrm{(m)}\qquad\mathrm{E}(g(X)|\mathcal{D})\geq\mathrm{E}(cX+d|\mathcal{D} )=c\mathrm{E}(X|\mathcal{D})+d\;\;\mathrm{a.s.}\qquad\text{by (\ref{eq:L1}).}\] (m)

Hence (as the union of a countable number of null sets is null), (l) and (m) give

\[\mathrm{(28)}\qquad\mathrm{E}(g(X)|\mathcal{D})\geq\sup_{\mathrm{all}\;c,d\in C }\{c\mathrm{E}(X|\mathcal{D})+d\}=g(\mathrm{E}(X|\mathcal{D}))\;\mathrm{a.s.} \tag{29}\]

since \((\inf I)<\mathrm{E}(X|\mathcal{D})<(\sup I)\) a.s. (since \(\mathrm{E}(X)\in I^{0}\) was asumed in (3.4.21)). 

**Exercise 4.4**: Prove (26) and the rest of (24), in theorem 4.1.

**Exercise 4.5**: (Dispersion inequality)  Suppose that \(X\) and \(Y\) are independent rvs with \(\mu_{Y}=0\). Let \(r\geq 1\). Show that \(|X+Y|\) is more dispersed than \(X\) in that

\[\mathrm{E}|X|^{r}\leq\mathrm{E}|X+Y|^{r}\qquad(\mathrm{or},\mathrm{E}|X+\mu_{Y }|^{r}\leq\mathrm{E}|X+Y|^{r}\text{ more generally}). \tag{30}\]

(Hint. Use Fubini on the induced distribution in \((R_{2},\mathcal{B}_{2})\) and then apply Jensen's inequality to \(g_{x}(y)=|x+y|^{r}\) to the inner integral. Note also exercise 8.2.3 below.)

**Exercise 4.6**: (a) Let \(P\) denote the Uniform \((-1,1)\) distribution on the Borel subsets \({\cal B}\) of \(\Omega=[-1,1]\). Let \(W(\omega)\equiv|\omega|,X(\omega)\equiv\omega^{2},Y(\omega)\equiv\omega^{3}\), and \(Z(\omega)\equiv\omega^{4}\). Fix \(A\in{\cal B}\). Show that versions of various conditional probabilities are given by

\[P(A|W)(\omega)=P(A|X)(\omega)=P(A|Z)(\omega)={{1\over 2}}\{1_{A}( \omega)+1_{A}(-\omega)\}\mbox{ on }\Omega,\]

while \(P(A|Y)(\omega)=1_{A}(\omega)\) on \(\Omega\).

**Exercise 4.7**: Determine \(P(A|W)(\omega),\,P(A|X)(\omega),\,P(A|Y)(\omega)\), and \(P(A|Z)(\omega)\) for \(W,X,Y\), and \(Z\) as in exercise 4.6 when \(P\) has the density \(1-|x|\) on \(([-1,1],{\cal B})\).

**Exercise 4.8**: Determine \(P(A|W)(\omega),P(A|X)(\omega)\), and \(P(A|Y)(\omega)\) for \(W,X,\) and \(Y\) as in exercise 4.6 when \(P\) has density \(1-x\) on \((0,1]\) and \({{3\over 2}}(1-x^{2})\) on \([-1,0]\).

## 5 Regular Conditional Probability \({}^{*}\)

For fixed \(A\), the function \(P(A|{\cal D})(\cdot)\) is a \({\cal D}\)-measurable function on \(\Omega\) that is only a.s. unique. We wish that for each fixed \(\omega\) the set function \(P(\cdot|{\cal D})(\omega)\) were a probability measure. But for each disjoint sequence of \(A_{i}\)'s there is a null set where (7.4.10)-(7.4.12) may fail, and there typically are uncountably many such sets. The union of all such null sets need not be null. In the most important special cases, though, we may assume that \(P(\cdot|{\cal D})(\omega)\) behaves as we would like, where the nonuniqueness of \(P(A|{\cal D})(\cdot)\) also provides the key, by allowing us to make whatever negligible changes are required. (For added useful generality, we will work on a sub \(\sigma\)-field \(\tilde{\cal A}\) of the basic \(\sigma\)-field \({\cal A}\).)

**Definition 5.1**  We will call \(P(A|{\cal D})(\omega)\) a _regular conditional probability_ on a sub \(\sigma\)-field \(\tilde{\cal A}\) of the \(\sigma\)-field \({\cal A}\), given the \(\sigma\)-field \({\cal D}\), if

(1) for each fixed

\[A\in\tilde{\cal A}\]

, the function

\[P(A|{\cal D})(\cdot)\]

 of

\[\omega\]

 satisfies definition 4.2, (2) for each fixed

\[\omega,P^{\omega}(\cdot|{\cal D})\equiv P(\cdot|{\cal D})(\omega)\]

 is a probability measure on

\[\tilde{\cal A}\]

**Exercise 5.1**  Verify that the discrete conditional probability of exercise 7.4.1 is a regular conditional probability.

When a regular conditional probability exists, conditional expectation can be computed by integrating with respect to conditional probability, and we first show this general theorem 5.1. In theorem 5.2 and beyond we shall show specifically how to construct such conditional probabilities in some of the most important examples.

**Theorem 5.1**  Let \(P(A|{\cal D})(\omega)\) be a regular conditional probability on \(\tilde{\cal A}\), and let \(Y\in{\cal L}_{1}(\Omega,\tilde{A},P)\). Then a version of the conditional expectation of \(Y\) given \({\cal D}\) is formed by setting

\[{\rm E}\{Y|{\cal D}\}(\omega)=\int Y(\omega^{\prime})\,dP^{\omega}(\omega^{ \prime}|{\cal D}),\qquad\mbox{for each fixed $\omega$.} \tag{3}\]

**Proof.**  If \(Y=1_{A}\), then (3) follows from

(a) \[\int Y\,dP^{\omega}(\cdot|{\cal D})=\int 1_{A}\,dP^{\omega}(\cdot|{\cal D })=\int_{A}\,dP^{\omega}(\omega^{\prime}|{\cal D})=P^{\omega}(A|{\cal D})=P(A |{\cal D})(\omega)\] (b) \[=_{a.s.}{\rm E}\{Y|{\cal D}\}(\omega),\qquad\mbox{no matter which version of the latter is used,}\]

with the various steps true by definition. Thus (3) is trivial for simple functions \(Y\). If \(Y\geq 0\) and \(Y_{n}\) are simple functions for which \(Y_{n}\nearrow Y\), then for any version of the conditional expectation function \({\rm E}(Y|{\cal D})(\cdot)\) we have

\[\int Y(\omega^{\prime})\,dP^{\omega}(\omega^{\prime}|{\cal D})=\lim\int Y_{n}( \omega^{\prime})\,dP^{\omega}(\omega^{\prime}|{\cal D})\]

(c) \[=_{a.s.}\ \lim{\rm E}\{Y_{n}|{\cal D}\}(\omega)=_{a.s.}{\rm E}\{Y|{\cal D }\}(\omega)\]

using the MCT (ordinary, and of (17)) in the first and last steps. Finally, let \(Y=Y^{+}-Y^{-}\).

Regular conditional probabilities need not exist; the null sets on which things fail may have a nonnull union. However, if \(Y:(\Omega,{\cal A})\rightarrow(R,{\cal B})\) is a rv, then things necessarily work out on \((R,{\cal B})\), and this will be generalized to any "Borel space." We will now start from scratch with regular conditional probability, and will choose to regard it as a measure over the image \(\sigma\)-field.

**Definition 5.2** (Borel space): If a 1-to-1 bimeasurable mapping \(\phi\) from \((M,{\cal G})\) to a measurable subset \(B_{o}\) of \((R,{\cal B})\) exists, then \((M,{\cal G})\) is called a _Borel space_.

**Exercise 5.2**: (a) Show that \((R_{n},{\cal B}_{n})\) is a Borel space.

(b) Show that \((R_{\infty},{\cal B}_{\infty})\) is a Borel space.

(c) The spaces \((C,{\cal C})\) and \((D,{\cal D})\) to be encountered below are also Borel spaces.

(d) Let \((M,d)\) be a complete and separable metric space having Borel sets \({\cal M}\), and let \(M_{0}\in{\cal M}\). Then \((M_{0},M_{0}\cap{\cal M})\) is a Borel space.

(This exercise is the only mathematically difficult part of the chapter that we have encountered so far.)

**Definition 5.3** (Regular conditional distribution): Suppose that \(Z\): \((\Omega,{\cal A})\rightarrow(M,{\cal G})\). Let \(\tilde{\cal A}\equiv Z^{-1}({\cal G})\), and let \({\cal D}\) be a sub \(\sigma\)-field of \({\cal A}\). Then \(P_{Z}(G|{\cal D})(\omega)\) will be called a _regular conditional distribution for \(Z\) given \({\cal D}\)_ if

\[\begin{array}{l}\mbox{for each fixed $G\in{\cal G}$,}\\ \mbox{the function $P_{Z}(G|{\cal D})(\cdot)$ is a version of $P(Z\in G|{\cal D})(\cdot)$ on $\Omega$,}\end{array} \tag{4}\]

\[\begin{array}{l}\mbox{for each fixed $\omega\in\Omega$,}\\ \mbox{the set function $P_{Z}(\cdot|{\cal D})(\omega)$ is a probability distribution on $(M,{\cal G})$;}\end{array} \tag{5}\]

and \(P_{Z}^{\omega}(\cdot|{\cal D})\equiv P_{Z}(\cdot|{\cal D})(\omega)\) will be used to denote this probability distribution.

**Theorem 5.2** (Existence of a regular conditional distribution): Suppose \(Z:(\Omega,{\cal A})\rightarrow(M,{\cal G})\) with \((M,{\cal G})\) a Borel space. Then the existence of a regular conditional probability distribution \(P_{Z}^{\omega}(G|{\cal D})\equiv P_{Z}(G|{\cal D})(\omega)\) is guaranteed.

**Proof.** : Case 1: Suppose first that \(Z:(\Omega,{\cal A})\rightarrow(R,{\cal B})\). Let \(r_{1},r_{2},\ldots\) denote the set of rational numbers. Consider \(P(Z\leq r_{i}|{\cal D})\) and note that except on a null set \(N\), all of the following hold:

(a) \[r_{i}\leq r_{j}\ \ \mbox{implies}\ \ P(Z\leq r_{i}|{\cal D})\leq P(Z\leq r_{j}|{ \cal D}).\] (b) \[\lim_{r_{j}\searrow r_{i}}P(Z\leq r_{j}|{\cal D})=P(Z\leq r_{i}|{\cal D}).\] (c) \[\lim_{r_{j}\nearrow\infty}P(Z\leq r_{j}|{\cal D})=P(\Omega|{\cal D})=1.\] (d) \[\lim_{r_{j}\searrow-\infty}\ P(Z\leq r_{j}|{\cal D})=P(\emptyset|{\cal D})=0.\]

Now define, for an arbitrary but fixed df \(F_{0}\),

(e)Then for every \(\omega\), the function \(F(\cdot|{\cal D})(\omega)\) is a df. Also, (e) and the DCT of theorem 7.4.1 show that \(F(z|{\cal D})(\cdot)\) is a version of \(P(Z\leq z|{\cal D})(\cdot)\).

Now extend \(P(Z\leq\cdot|{\cal D})(\omega)\) to a distribution (labeled \(P_{Z}(B|{\cal D})(\omega)\)) over all \(B\in{\cal B}\) via the correspondence theorem. We now define

\[{\cal M}\equiv\{C\in{\cal B}:P_{Z}(C|{\cal D})\mbox{ is a version of }P(Z\in C|{ \cal D})\}.\]

Now, \({\cal M}\) contains all \((a,b]\) and all \(\sum_{1}^{m}(a_{i},b_{i}]\), and \({\cal M}\) is closed under monotone limits. Thus \({\cal M}={\cal B}\), by the minimal monotone class result of proposition 1.1.6, completing the proof in this case.

Case 2: Let \(Y\equiv\phi(Z)\), so \(Y\) is a rv. Thus a regular conditional distribution \(P_{Y}(B|{\cal D})\) exists by case 1. Then for \(G\in{\cal G}\), define \(P_{Z}(G|{\cal D})\equiv P_{Y}(\phi(G)|{\cal D})\). \(\Box\)

**Example 5.1** (Elementary conditional densities): Suppose that

\[P((X,Y)\in B_{2})=\int_{B_{2}}\int f(x,y)\,dx\,dy\qquad\mbox{ for all }B_{2}\in{\cal B}_{2},\]

where \(f\geq 0\) is measurable; and then \(f(x,y)\) is called the _joint density_ of \(X,Y\) (or, the Radon-Nikodym derivative \(dP/d\lambda_{2}\)). Let \(B_{2}\equiv B\times R\), for all \(B\in{\cal B}\). We can conclude that \(P_{X}\ll\lambda\equiv\) (Lebesgue measure), with

\[\frac{dP_{X}(x)}{d\lambda}=f_{X}(x)\equiv\int_{R}f(x,y)\,dy;\]

we call \(f_{X}(x)\) the _marginal density_ of \(X\). We first define

\[g(y|x)\equiv\cases{f(x,y)/f_{X}(x)&for all $x$ with $f_{X}(x)\neq 0$,\cr\mbox{ an arbitrary density $f_{0}(y)$ for all $x$ with $f_{X}(x)=0$,}\cr}\]

and then define

\[P(Y\in A|X=x)=\int_{A}g(y|x)\,dy\quad\mbox{for $A\in{\cal B}$.}\]

Call \(g(y|x)\) the _conditional density_ of \(Y\)_given_\(X=x\), and this \(P(Y\in A|X=x)\) will now be shown to be a regular conditional distribution (if modified appropriately on the set where \(f_{X}(x)=0\)). Moreover, if \({\rm E}|h(Y)|<\infty\), then

\[{\rm E}\{h(Y)|X=x\}=\int_{-\infty}^{\infty}h(y)g(y|x)\,dy\quad\mbox{a.s. }P_{X}.\]

Thus (8) (also written as (10)) fulfills theorem 5.2, and (9) will be seen to fulfill theorem 5.3. (Note that this example also holds for vectors \(x\in R_{m}\) and \(y\in R_{n}\).) \(\Box\)

**Proof.** By Fubini's theorem,

\[P(X\in B)=\int\int_{B\times R}f(x,y)\,dx\times dy=\int_{B}[\int_{R}f(x,y)\,dy] \,dx=\int_{B}f_{X}(x)\,dx.\]

Moreover, Fubini's theorem tells us that \(f_{X}(x)\) is \({\cal B}\)-measurable.

Let \(S\equiv\{x:f_{X}(x)\neq 0\}\). We may assume that \((\Omega,{\cal A})=(R_{2},B_{2})\). Let \(\tilde{{\cal A}}=Y^{-1}({\cal B})\). We will verify that (7.4.6) holds. For \(A\in{\cal B}\) and for \([Y\in A]\in\tilde{{\cal A}}\equiv R\times{\cal B}\), we note that for all \(B\in{\cal B}\) we have (writing \(f(x)\) for \(f_{X}(x)\))

\[\int_{B}[\int_{A}g(y|x)\,dy]\,dP_{X}(x)\] \[\quad=\int_{B}[\int_{A}g(y|x)\,dy]f(x)\,dx=\int_{B\cap S}[\int_{A} \frac{f(x,y)}{f(x)}\,dy]f(x)\,dx\] \[\quad=\int_{B\cap S}[\int_{A}f(x,y)\,dy]\,dx=\int\int_{B\times A}f (x,y)(dx\times dy)\] \[\quad=P((X,Y)\in B\times A)=P([Y\in A]\cap X^{-1}(B))\] (b) \[\quad=\int_{B}P(Y\in A|X=x)\,dP_{X}(x).\]

Thus

(c) \[P(Y\in A|X=x)=\int_{A}g(y|x)\,dy\qquad\mbox{a.s. }P_{X},\]

and so \(g(y|x)\) works as a version. Now, for any fixed set \(A\in B\) we note that

\[\int_{A}g(y|x)\,dy=1_{S}(x)\times[\int_{A}f(x,y)\,dy\,/\,f(x)]+1_{S^{c}}(x) \times\int_{A}f_{0}(y)\,dy \tag{10}\]

is a measurable function on \((R,{\cal B})\). It is clear that for each fixed \(x\) the function of (10) acts like a probability distribution. Thus (10) defines completely a regular conditional probability distribution.

Suppose that \({\rm E}|h(Y)|<\infty\). Then (9) holds since

\[\int_{B}[\int_{R}h(y)g(y|x)\,dy]\,dP_{X}(x)=\int_{B\cap S}[\int_{R }h(y)g(y|x)\,dy]f(x)\,dx\] \[\quad=\int_{B\cap S}[\int_{R}h(y)f(x,y)\,dy]\,dx=\int_{B}[\int_{R} h(y)f(x,y)\,dy]\,dx\] \[\quad=\int\int_{B\times R}h(y)f(x,y)(dx\times dy)\] (d) \[\quad=\int\int_{x^{-1}(B)}h(y)\,dP_{X,Y}(x,y)=\int_{B}{\rm E}(h(Y )|X=x)\,dP_{X}(x).\qed\]

**Theorem 5.3** (Conditional expectation exists as an expectation): Given a measurable mapping \(Z:\,(\Omega,{\cal A})\to(M,{\cal S})\), where \((M,{\cal S})\) is a Borel space, consider a transformation \(\phi:\,(M,S)\to(R,{\cal B})\) with \({\rm E}|\phi(Z)|<\infty\). Then a version of the conditional expectation of \(\phi(Z)\) given \({\cal D}\) is formed by setting

\[{\rm E}\{\phi(Z)|{\cal D}\}(\omega)=\int_{M}\phi(z)\,dP_{Z}^{\omega}(z|{\cal D })\qquad\mbox{for all }\omega. \tag{11}\]

**Proof.** Apply theorem 5.1 to the regular conditional distribution of theorem 5.2. \(\Box\)

**Theorem 5.4** (A most useful format for conditional expectation): Suppose that \(X:\,(\Omega,{\cal A},P)\to(M_{1},{\cal G}_{1})\) and \(Y:\,(\Omega,{\cal A},P)\to(M_{2},{\cal G}_{2})\) (with Borel space images). Then \((X,Y):\,(\Omega,{\cal A},P)\to(M_{1}\times M_{2},{\cal G}_{1}\times{\cal G}_ {2})\). Also (as above)

(12) a regular conditional probability \(P(A|X=x)\) exists,

for sets \(A\in\tilde{A}\equiv Y^{-1}({\cal G}_{2})\subset{\cal A}\) and for \(x\in M_{1}\). Let \({\rm E}|h(X,Y)|<\infty\). (a) Then

\[{\rm E}(h(X,Y)|X=x)=\int_{M_{2}}h(x,y)\,dP(y|X=x)\quad\mbox{a.s.} \tag{13}\]

(b) If \(X\) and \(Y\) are independent, then

\[{\rm E}(h(X,Y)|X=x)={\rm E}(h(x,Y))\quad\mbox{a.s.} \tag{14}\]

**Exercise 5.3**  Prove theorem 5.4 above. (Give a separate trivial proof of (14).) Hint. Begin with indicator functions \(h=1_{G_{1}}1_{G_{2}}\).

**Example 5.2** (Sufficiency of the order statistics): Let \(X_{1},\ldots,X_{n}\) be iid with \(\mbox{df}\,F\) in the class \({\cal F}_{c}\) of all continuous dfs. Let \(T({\sf x})\equiv(x_{n:1},\ldots,x_{n:n})\) denote the vector of ordered values of \({\sf x}\), and let \({\cal T}\equiv\{{\sf x}\,:\,\,x_{n:1}<\cdots<x_{n:n}\}\). Exercise 5.5 below asks the reader to verify that \(P_{F}(T({\sf X})\in{\cal T})=1\) for all \(F\in{\cal F}_{c}\). Let \({\cal X}\) denote those \({\sf x}\in R_{n}\) having distinct coordinates. Let \({\cal A}\) and \({\cal B}\) denote all Borel subsets of \({\cal X}\) and \({\cal T}\), respectively. Then \({\cal D}\equiv T^{-1}({\cal B})\) denotes all symmetric subsets of \({\cal A}\) (in that \({\sf x}\in D\subset{\cal D}\) implies \(\pi({\sf x})\in D\) for all \(n!\) permutations \(\pi({\sf x})\) of \({\sf x}\)). Let \(dP^{(n)}_{F}(\cdot)\) denote the \(n\)-fold product measure \(dF(\cdot)\times\cdots\times dF(\cdot)\) Suppose \(\phi(\cdot)\) is \(P^{(n)}_{F}\)-integrable. Then define

\[\phi_{0}({\sf x})\equiv{1\over n!}\sum_{\mbox{\scriptsize all $n!$ permutations }}\,\,\phi(\pi({\sf x})), \tag{15}\]

which is a \({\cal D}\)-measurable function. Since \(P^{(n)}_{F}\) is symmetric, for any symmetric set \(D\in{\cal D}\) we have

\[\int_{D}\phi({\sf x})\,dP^{(n)}_{F}({\sf x})=\int_{D}\phi(\pi({ \sf x}))\,dP^{(n)}_{F}({\sf x})\qquad\mbox{ for all $n!$ permutations $\pi(\cdot)$}\] \[\qquad=\int_{D}\phi_{0}({\sf x})\,dP^{(n)}_{F}({\sf x})\qquad \mbox{ for every $D\in{\cal D}$}.\]

But this means that

\[{\sf E}_{F}\{\phi(\times)|T\}({\sf x})=\phi_{0}({\sf x})\qquad\mbox{a.s. $P^{(n)(\cdot)}_{F}$}. \tag{16}\]

Now, for any \(A\in{\cal A}\), the function \(1_{A}(\cdot)\) is \(P^{(n)}_{F}\)-integrable for all \(F\in{\cal F}_{c}\). (Thus conclusion (14) can be applied.) Fix \(F\in{\cal F}_{c}\). For any fixed \(A\in{\cal A}\) we have

\[P^{(n)}_{F}(A|T)({\sf x})={\rm E}_{F}\{1_{A}({\sf x})|T\}({\sf x})={1\over n!} \sum_{\mbox{\scriptsize all $\pi$}}1_{A}(\pi({\sf x})) \tag{17}\] \[\qquad={\mbox{(the $\#$ of times $\pi({\sf x})$ is in $A$)}\over n!}.\]

Note that the right-hand side of (15) does not depend on the particular \(F\in{\cal F}_{c}\), and so \(T\) is said to be a _sufficient statistic_ for the family of distributions \({\cal F}_{c}\). (Note discussion 4.3 once again.) \(\Box\)

**Example 5.3** (Ranks): Consider the ranks \({\sf R}_{n}=(R_{n1},\ldots,R_{nn})^{\prime}\) and the antiranks \({\sf D}_{n}=(D_{n1},\ldots,D_{nn})^{\prime}\) in a sample from some \(F\in{\cal F}_{c}\) (see the previous example, and (6.5.22)). Let \(\prod_{n}\) denote the set of all \(n!\) permutations of \((1,\ldots,n)\). Now, \({\sf R}_{n}\) takes on values in \(\prod_{n}\) when \(F\in{\cal F}_{c}\) (since ties occur with probability 0). By symmetry, for every \(F\in{\cal F}_{c}\) we have

\[P^{(n)}_{F}({\sf R}_{n}={\sf r})=1/n!\qquad\mbox{for each ${\sf r}\in\prod_{n}$}\,. \tag{18}\]

Note that \({\sf X}\) is equivalent to \((T,{\sf R}_{n})\), in the sense that each determines the other. Note also that

\[T\mbox{ and ${\sf R}_{n}$ are independent rvs (for each fixed $F\in{\cal F}_{c}$)}, \tag{19}\]

in that for all \(B\in{\cal B}\equiv\) (the Borel subsets of \({\cal T}\)) and for all \(\pi\in\prod_{n}\) we have

\[P^{(n)}_{F}([T\in B]\mbox{ and $[{\sf R}_{n}={\sf r}]$}) \tag{20}\] \[=\ \{\ {1\over n!}\}\times\int_{T^{-1}(B)}n!\,dP^{(n)}_{F}({ \sf x})=\{P^{(n)}_{F}({\sf R}_{n}={\sf r})\}\!\times\!P^{(n)}_{F}(T\in B)\] \[=P({\sf R}_{n}={\sf r})\times P^{(n)}_{F}(T\in B),\]since \(P({\sf R}_{n}={\rm r})\) does not depend on the particular \(F\in{\cal F}_{c}\). Since the ranks are independent of the sufficient statistic, they are called _ancillary_ rvs. (Note that \({\rm D}_{n}\) is also equally likely distributed over \(\prod_{n}\), and that it is distributed independent of the order statistics \(T\).) \(\Box\)

**Exercise 5.4**: Suppose that the \(n\) observations are sampled from a continuous distribution \(F\). Verify that with probability one all observations are distinct. (Hint. Use corollary 2 to theorem 5.1.3.)

**Exercise 5.5**: Suppose \(X_{1},\ldots,X_{n}\) are iid Bernoulli (\(p\)) rvs, for some \(p\in(0,1)\). Let \(T\equiv\sum_{1}^{n}X_{k}\) denote the total number of successes. Show that this rv \(T\) is sufficient for this family of probability distributions (that is, \(T\) is "sufficient for \(p\)").

**Exercise 5.6**: Let \(\xi_{1}\) and \(\xi_{2}\) be independent Uniform(0, 1) rvs. Let \(\Theta\equiv 2\pi\xi_{1}\) and \(Y\equiv-\log\xi_{2}\). Let \(R\equiv(2Y)^{1/2}\). Now let \(Z_{1}\equiv R\cos\Theta\) and \(Z_{2}\equiv R\sin\Theta\). Determine the joint distribution of \((Y,\Theta)\) and of \((Z_{1},Z_{2})\).

## Chapter 8 Wlln, Slln, Lll, and Series

### 0 Introduction

This is one of the classically important chapters of this text. The first three sections of it are devoted to developing the specific tools we will need. In the second section we also present Khinchin's weak law of large numbers (WLLN), which can be viewed as anticipating both of the classical laws of large numbers (LLNs). Both the classical weak law of large numbers (Feller's WLLN) and classical strong law of large numbers (Kolmogorov's SLLN) are presented in section 8.4, where appropriate negligibility of the summands is also emphasized. This section is the main focus of the chapter. Some applications of these LLNs are given in the following section 8.5 Then we branch out. The law of the iterated logarithm (LIL), the strong Markov property, and convergence of infinite series are treated in sections 8.6 - 8.8. The choice was made to be rather specific in section 8.4, with easy generalizations in section 8.8. The usual choice is to begin more generally, and then specialize. Martingales (mgs) are introduced briefly in section 8.9, both for limited use in chapter 12 and so that the inequalities in the following section 8.10 can be presented in appropriate generality.

## 1 Borel-Cantelli and Kronecker lemmas

The first three sections will develop the required tools, while applications will begin with the LLNs (the first of which appears in section 8.2). We use the notation

\[[A_{n}\text{ i.o.}]=[\omega:\omega\in A_{n}\text{ infinitely often}]=\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}A_{m}=\overline{\lim}_{n}\,A_{n}. \tag{1}\]

This concept is important in dealing with convergence of various random elements. The following lemmas exhibit a nice dichotomy relative to sequences of independent events.

**Lemma 1.1** (Borel-Cantelli lemma): For any events \(A_{n}\),

\[\sum_{n=1}^{\infty}P(A_{n})<\infty\qquad\text{ implies }\qquad P(A_{n}\text{ i.o.})=0. \tag{2}\]

**Lemma 1.2** (Second Borel-Cantelli lemma): For a sequence of independent events \(A_{1},A_{2},\ldots\), we have the converse that

\[\sum_{n=1}^{\infty}P(A_{n})=\infty\qquad\text{ implies }\qquad P(A_{n}\text{ i.o.})=1. \tag{3}\]

Thus independent events \(A_{1},A_{2},\ldots\) have \(P(A_{n}\text{ i.o.})\) equal to 0 or 1 according as \(\sum_{1}^{\infty}P(A_{n})\) is finite or infinite.

**Proof.** We use proposition 1.1.2 freely. Now,

(a) \[P(A_{n}\text{ i.o.})=P(\lim_{n}\bigcup_{n}^{\infty}A_{m})=\lim_{n}P(\bigcup_{n}^{ \infty}A_{m})\leq\lim_{n}\sum_{n}^{\infty}P(A_{m})=0\]

whenever \(\sum_{1}^{\infty}P(A_{m})<\infty\). Also,

\[P((\overline{\lim}\,A_{n}]^{c})=P(\bigcup_{n=1}^{\infty}\bigcap_{m=n}^{\infty} A_{m}^{c})=\lim_{n}P(\bigcap_{m=n}^{\infty}A_{m}^{c})\]

\[=\lim_{n}\lim_{N}P(\bigcap_{m=n}^{N}A_{m}^{c})\]

(b) \[=\lim_{n}\lim_{N}\prod_{m=n}^{N}[1-P(A_{m})]\quad\text{ by independence }\]

(c) \[\leq\lim_{n}\lim_{N}\exp(-\sum_{m=n}^{N}P(A_{m}))\qquad\text{ since }1-x\leq\exp(-x)\]

(d) \[=\lim_{n}\exp(-\sum_{m=n}^{\infty}P(A_{m}))=\lim_{n}\exp(-\infty)=\lim_{n}0=0,\]

using \(\sum_{1}^{\infty}P(A_{n})=\infty\). \(\square\)

**Remark 1.1** (Kolmogorov's 0-1 law): In theorem 7.2.1 we considered the tail \(\sigma\)-field \(\mathcal{T}\equiv\bigcap_{n=1}^{\infty}\mathcal{F}(X_{n},\ X_{n+1},\ \ldots)\) of an arbitrary sequence of independent rvs \(X_{1},X_{2},\ldots\). We learned that \(P(D)=0\) or 1 for all \(D\in\mathcal{T}\). (Here, let \(X_{n}\equiv 1_{A_{n}}\) and obtain the characterization via the finiteness of \(\sum_{1}^{\infty}P(A_{n})\) at the end of lemma 1.2. The tail event in question is \([X_{n}=1\) i.o.].) \(\square\)

**Lemmas About Real Numbers**

An important bridge going from the convergence of series to the convergence of averages is provided by Kronecker's lemma. (An alternative bridge is provided by the monotone inequality (8.10.1) (note also inequality 8.4.10).)

**Lemma 1.3** (Kronecker's lemma): (a) Let \(b_{n}\geq 0\) and \(\nearrow\infty\). For \(x_{1},x_{2},\ldots\) real,

\[\sum_{k=1}^{n}x_{k}\to(\mbox{some real $r$})\qquad\mbox{implies}\quad\frac{1}{b_{n}} \sum_{k=1}^{n}b_{k}x_{k}\to 0.\]

(b) So, \(\sum_{k=1}^{n}x_{k}/k\to(\mbox{some real $r$})\quad\mbox{implies}\quad\frac{1}{n} \mbox{$\sum_{k=1}^{n}x_{k}\to 0$}\).

**Proof.**  Let \(s_{k}\equiv\sum_{1}^{k}x_{j}\) with \(s_{0}\equiv 0\) and \(b_{0}\equiv 0\). Summing by parts gives

(a) \(\frac{1}{b_{n}}\mbox{$\sum_{1}^{n}b_{k}x_{k}$}=\frac{1}{b_{n}}\mbox{$\sum_{1}^ {n}b_{k}(s_{k}-s_{k-1})$}=\frac{1}{b_{n}}\mbox{$\sum_{0}^{n-1}(b_{k}-b_{k+1}) s_{k}$}+\frac{1}{b_{n}}b_{n}s_{n}\) (b) \(=-\mbox{$\sum_{1}^{n}a_{k}s_{k-1}$}+s_{n}\quad\mbox{where}\quad a_{k}\equiv \frac{b_{k}-b_{k-1}}{b_{n}}\geq 0\,\mbox{with $\sum_{1}^{n}a_{k}=1$}\) (c) \(=-\mbox{$\sum_{1}^{n}a_{k}(s_{k-1}-r)$}+(s_{n}-r)\).

Since \(|s_{k}-r|\leq\epsilon\) for all \(k\geq(\mbox{some $N_{\epsilon}$})\), we have

(d) \(\left|\frac{1}{b_{n}}\mbox{$\sum_{1}^{n}b_{k}x_{k}$}\right|\leq\mbox{$\sum_{1} ^{N_{\epsilon}}$}|a_{k}(s_{k-1}-r)|+\mbox{$\sum_{N_{\epsilon}+1}^{n}$}|a_{k}(s_ {k-1}-r)|+|s_{n}-r|\) (e) \(\leq\frac{\mbox{$\sum_{1}^{N_{\epsilon}}(b_{k}-b_{k-1})|s_{k-1}-r|$}}{b_{n}}+ \epsilon(\mbox{$\sum_{N_{\epsilon}+1}^{n}$}a_{k})+\epsilon\qquad\mbox{for $n\geq N_{ \epsilon}$}\)

\(\leq 3\epsilon\qquad\mbox{for $n$ sufficiently larger than $N_{\epsilon}$}\).

[Since \(\sum_{1}^{n}x_{k}\to r\), we must have \(x_{k}\to 0\). Note that \(\sum_{1}^{n}b_{k}x_{k}/b_{n}\) puts large weight only on the later terms.] \(\Box\)

**Lemma 1.4** (Convergence of sums and products): Suppose \(a\in[0,\ \infty]\), all constants \(c_{nk}\geq 0\), and \(m_{n}\equiv[\max_{1\leq k\leq n}c_{nk}]\to 0\). Then

\[\prod_{k=1}^{n}(1-c_{nk})\to e^{-a}\qquad\mbox{if and only if}\qquad\sum_{k=1}^{n}c_{nk}\to a.\]

**Proof.**  We will write \(a=b\oplus c\) to mean that \(|a-b|\leq c\). For \(m_{n}\leq 1/2\),

\[\log\prod_{1}^{n}(1-c_{nk})=\sum_{1}^{n}\log(1-c_{nk})=-\mbox{$\sum_{1}^{n}c_{ nk}$}\oplus\mbox{$\sum_{1}^{n}c_{nk}^{2}$}=-(1\oplus m_{n})\mbox{$\sum_{1}^{n}c_{nk}$}\]

via an expansion of \(\log(1+x)\). This yields the result, as \(m_{n}\to 0\). \(\Box\)

**Exercise 1.1** (Cesaro summability): If \(s_{n}\equiv\sum_{k=1}^{n}x_{k}\to r\), then \(\frac{1}{n}\mbox{$\sum_{k=1}^{n}s_{k}\to r$}\).

**Exercise 1.2**: Let all \(a_{n}\geq 0\). Suppose \(\sum_{1}^{\infty}a_{n}b_{n}<\infty\) holds whenever \(\sum_{1}^{\infty}b_{n}^{2}<\infty\) with all \(b_{n}\geq 0\). Show that \(\sum_{1}^{\infty}a_{n}^{2}<\infty\).

**Exercise 1.3** (Toeplitz) Let \(a_{nk}\) (for \(1\leq k\leq k_{n}\), with \(k_{n}\to\infty\)) be such that: (i) For every fixed \(k\), we have \(a_{nk}\to 0\). (ii) \(\sum_{k=1}^{k_{n}}\lvert a_{nk}\rvert\leq c<\infty\), for every \(n\). Let \(x^{\prime}_{n}\equiv\sum_{k=1}^{k_{n}}a_{nk}x_{k}\). Then

(a) \(x_{n}\to 0\) implies \(x^{\prime}_{n}\to 0\).

If \(\sum_{k=1}^{k_{n}}a_{nk}\to 1\), then

(b) \(x_{n}\to x\) implies \(x^{\prime}_{n}\to x\).

In particular, if \(b_{n}\equiv\sum_{k=1}^{n}a_{k}\nearrow\infty\), then

(c) \(x_{n}\to x\) finite entails \(\sum_{k=1}^{n}a_{k}x_{k}/b_{n}\to x\).

[This exercise will not be employed anywhere in this text.]

**Exercise 1.4** Show that

(7) \(1-x\leq e^{-x}\leq 1-x/(1+x)\) for all \(x\geq 0\).

**Exercise 1.5** Let \(X_{1},\ldots,X_{n}\) be independent rvs. (i) Show that

(8) \(\begin{array}{l}\sum_{1}^{n}P(\lvert X_{k}\rvert>x)/[1+\sum_{1}^{n}P(\lvert X _{k}\rvert>x)]\\ \qquad\leq p_{\max}(x)\equiv P(\max_{1\leq k\leq n}\left\lvert X_{n}\right\rvert >x)\leq\sum P(\lvert X_{k}\rvert>x)\text{for all }x\geq 0.\end{array}\)

(ii) So whenever \(p_{\max}(x)\equiv P(\max_{1\leq k\leq n}\left\lvert X_{n}\right\rvert>x)\leq \frac{1}{2}\), then

(9) \(\begin{array}{l}\frac{1}{2}\sum_{1}^{n}P(\lvert X_{k}\rvert>x)\leq p_{\max} \leq\sum_{1}^{n}P(\lvert X_{k}\rvert>x).\end{array}\)

## 2 Truncation, WLLN, and Review of Inequalities

Truncated rvs necessarily have moments, and this makes them easier to work with. But it is crucial not to lose anything in the truncation.

**Definition 2.1** (Khinchin equivalence): Two sequences of rvs \(X_{1},X_{2},\ldots\) and \(Y_{1},Y_{2},\ldots\) for which \(\sum_{n=1}^{\infty}P(X_{n}\neq Y_{n})<\infty\) are called _Khinchin equivalent_.

**Proposition 2.1**: (i) Let \(X_{1},X_{2},\ldots\) and \(Y_{1},Y_{2},\ldots\) be Khinchin equivalent rvs.

(a) If \(X_{n}\to_{a.s.}\) (some rv \(X\)), then \(Y_{n}\to_{a.s.}\) (the same rv \(X\)).

(b) If \(S_{n}\equiv\sum_{1}^{n}X_{k}\to_{a.s.}\) (some rv \(S\)), then \(T_{n}\equiv\sum_{1}^{n}Y_{k}\to_{a.s.}\) (some rv \(T\)).

(c) If \(S_{n}/b_{n}\to_{a.s.}\) (some rv \(U\)) and \(b_{n}\to\infty\), then \(T_{n}/b_{n}\to_{a.s.}\) (the same rv \(U\)).

(ii) Of less interest, \(\to_{p}\) may replace \(\to_{a.s.}\) in (a), (b), and (c).

**Proof.** The Borel-Cantelli lemma gives \(P(X_{n}\neq Y_{n}\) i.o.) \(=0\); or

(p) \[X_{n}(\omega)=Y_{n}(\omega)\mbox{ for all }n\geq\mbox{ (some }n(\omega))\mbox{ \ \ holds true for a.e. }\omega.\]

Thus the a.s. statements for \(X_{n}\) and \(S_{n}\) are trivial. Moreover, since \(X_{n}(\omega)=Y_{n}(\omega)\) for all \(n\geq\) (some fixed \(n(\omega)\) ), we have

(q) \[\frac{S_{n}}{b_{n}}=\frac{S_{n(\omega)}+S_{n}-S_{n(\omega)}}{b_{n}}=\frac{S_{n( \omega)}+T_{n}-T_{n(\omega)}}{b_{n}}=\frac{S_{n(\omega)}-T_{n(\omega)}}{b_{n}}+ \frac{T_{n}}{b_{n}}\] (r) \[=o(1)+T_{n}/b_{n}\]

using \(b_{n}\to\infty\).

Since a sequence (such as \(X_{n},\ S_{n}\) or \(S_{n}/b_{n}\)) converges in probability if and only if each subsequence \(n^{\prime}\) contains a further subsequence \(n^{\prime\prime}\) on which the convergence is a.s., the in probability statements follow directly from the a.s. statements. \(\Box\)

**Inequality 2.1** (Sandwiching E\(|X|\)): For any rv \(X\) we have

\[\sum_{n=1}^{\infty}P(|X|\geq n)\leq\mbox{E}|X|=\int_{0}^{\infty}P(|X|>x)\,dx \leq\sum_{n=0}^{\infty}P(|X|\geq n). \tag{1}\]

If \(X\) is a rv with values \(0,1,2,\ldots\), then

\[\mbox{E}(X)=\sum_{n=1}^{\infty}P(X\geq n). \tag{2}\]

**Proof.** If \(X\geq 0\), then \(\mbox{E}X=\int_{0}^{\infty}[1-F(x)]\,dx\) by (6.4.11); consult figure 2.1. If \(X\geq 0\) is integer valued, then

\[\begin{array}{l}\mbox{E}(X)=\sum_{k=0}^{\infty}k\,P(X=k)=\sum_{k=1}^{\infty }\sum_{n=1}^{k}P(X=k)\\ \quad\quad=\sum_{n=1}^{\infty}\sum_{k=n}^{\infty}P(X=k)\\ \quad\quad=\sum_{n=1}^{\infty}P(X\geq n).\end{array}\] (a)

For the greatest integer function \([\cdot]\), an arbitrary rv satisfies

(b) \[[|X|]\leq|X|\leq[|X|]+1.\]Moreover, (a) shows that

(c) \[\mathrm{E}[|X|]=\sum_{n=1}^{\infty}P([|X|]\geq n)=\sum_{n=1}^{\infty}P(|X|\geq n),\]

while (consult figure 2.1 again)

(d) \[\mathrm{E}\{[|X|]+1\}=\sum_{n=1}^{\infty}P(|X|\geq n)+1=\sum_{n=1}^{\infty}P(|X |\geq n)+P(|X|\geq 0).\]

**Example 2.1** (Truncating and Winsorizing): Let \(X_{1},X_{2},\ldots\) be iid as \(X\). Let us truncate and Winsorize the rv \(X_{n}\) by defining

\[\tilde{X}_{n}=X_{n}\times 1_{[|X_{n}|<n]}\quad\text{and}\quad\tilde{X}_{n}=-n \times 1_{[X_{n}\leq-n]}+X_{n}\times 1_{[|X_{n}|<n]}+n\times 1_{[X_{n}\geq n]}. \tag{3}\]

From (2) we see that

\[\begin{array}{l}\mathrm{E}|X|<\infty\qquad\qquad\qquad\qquad\qquad\text{if and only if}\quad\int_{0}^{\infty}P(|X|>t)\,dt<\infty\qquad\text{ iff}\\ \sum_{1}^{\infty}P(X_{n}\neq\tilde{X}_{n})<\infty\qquad\text{if and only if} \quad\sum_{1}^{\infty}P(X_{n}\neq\tilde{X}_{n})<\infty,\end{array} \tag{4}\]

so that these \(\tilde{X}_{n}\)'s and \(\tilde{X}_{n}\)'s are Khinchin equivalent to the \(X_{n}\)'s if and only if the absolute moment \(\mathrm{E}|X|<\infty\). (Do not lose sight of this during the SLLN.) \(\square\)

Proof.: Using inequality 2.1, then iid, and then the Borel-Cantelli lemmas, we obtain that \(\mathrm{E}|X|<\infty\) if and only if \(\sum_{1}^{\infty}P(|X|\geq n)<\infty\) if and only if \(\sum_{1}^{\infty}P(|X_{n}|\geq n)<\infty\) if and only if \(P(|X_{n}|\geq n\,\mathrm{i.o.})=0\). This gives (4), as well as the additional fact that

\[\mathrm{E}|X|<\infty\qquad\text{if and only if}\qquad P(|X_{n}|\geq n\,\mathrm{i.o.})=0\quad\text{for}\quad X_{n}\text{'s iid as}\,X. \tag{5}\]

This final fact (5) is a useful supplementary result. Recall (6.4.11). \(\square\)

**Exercise 2.1** (a): Show \(\mathrm{E}X^{2}=2\int_{0}^{\infty}xP(|X|>x)\,dx=\sum_{k=1}^{\infty}(2k-1)P(|X| \geq k)\) for an integer valued rv \(X\geq 0\). (Let \(\tau(x)\equiv xP(|X|>x)\).) Thus, for any rv \(X\),

\[\mathrm{E}X^{2}<\infty\quad\text{iff}\quad\int_{0}^{\infty}xP(|X|>x)\,dx< \infty\quad\text{iff}\quad\sum_{n=1}^{\infty}nP(|X|\geq n)<\infty. \tag{6}\]

(b) Now let \(X_{1},X_{2},\ldots\) be iid as \(X\). Let \(\tilde{X}_{n}\) or \(\tilde{X}_{n}\) result when \(X_{n}\) is truncated or Winsorized outside either \([-\sqrt{n},\ \sqrt{n}]\) (or, \((-\sqrt{n},\ \sqrt{n})\)). Show

\[\begin{array}{l}\mathrm{E}X^{2}<\infty\quad\text{if and only if}\\ \text{these new }\tilde{X}_{n}\text{'s and }\tilde{X}_{n}\text{'s are Khinchin equivalent to the }X_{n}\text{'s}.\end{array} \tag{7}\]

#### Khinchin's WLLN

We begin with an easy result that illustrates our path rather clearly.

**Theorem 2.1** (i) **(WLLN; Khinchin)** Let \(X_{1},\ldots,X_{n}\) be iid with mean \(\mu\). Then

\[\bar{X}_{n}\to_{p}\mu\qquad\mbox{and}\qquad\bar{X}_{n}\to_{\mathcal{L}_{1}}\mu. \tag{8}\]

(In fact, \(\bar{X}_{n}-\int_{|x|\leq n}dF_{X}(x)\to_{p}0\) if and only if \(xP(|X|>x)\to 0\) as \(x\to\infty\). Note that \(xP(|X|>x)\to 0\) as \(x\to\infty\) can hold even if \(\mathrm{E}(X)\) fails to exist.)

(ii) (More general WLLN) Suppose that \(X_{n1},\ldots,X_{nn}\) are independent. Then

\[\bar{X}_{n}-\mathrm{E}\bar{X}_{n}\to_{p}0\quad\mbox{holds if}\quad\tfrac{1}{n} {\sum_{k=1}^{n}}\mathrm{E}\{|X_{nk}|1_{[\epsilon\sqrt{n}<|X_{nk}|]}\}\to 0\quad \mbox{for all}\quad\epsilon>0. \tag{9}\]

This condition holds if the rvs \(\{X_{nk}:n\geq 1\mbox{ and }1\leq k\leq n\}\) are uniformly integrable.

**Proof.** (ii) Truncate these rvs via \(Y_{nk}\equiv X_{nk}\times 1_{[-n\leq X_{nk}\leq n]}\), and define the means \(\mu_{nk}\equiv\mathrm{E}(Y_{nk})\). Then define \(\bar{\mu}_{n}\equiv{\sum_{1}^{n}}\mu_{nk}/n\). We will show that

(a) \(\bar{Y}_{n}-\bar{\mu}_{n}\to_{p}0\), provided \(\frac{1}{n}{\sum_{1}^{n}}\mathrm{E}\{|X_{nk}|\ 1_{[\epsilon\sqrt{n}<|X_{nk}| \leq n]}\}\to 0\) for all \(\epsilon>0\).

Chebyshev gives

\[P(|\bar{Y}_{n}-\bar{\mu}_{n}|\geq\epsilon)\leq\frac{1}{\epsilon^{2 }n^{2}}{\sum_{k=1}^{n}}\mathrm{Var}[Y_{nk}]\leq\tfrac{1}{\epsilon^{2}n^{2}}{ \sum_{k=1}^{n}}\mathrm{E}(Y_{nk}^{2})\] \[\quad=\frac{1}{\epsilon^{2}n^{2}}{\sum_{k=1}^{n}}\mathrm{E}\{Y_{ nk}^{2}(1_{|X_{k}|\leq\epsilon^{2}\sqrt{n}})+1_{[\epsilon^{2}\sqrt{n}<|X_{nk}| \leq n]}\}\}\] (b) \[\quad\leq\frac{1}{\epsilon^{2}n^{2}}{\sum_{k=1}^{n}}\epsilon^{4}n +\tfrac{1}{\epsilon^{4}n^{2}}{\sum_{k=1}^{n}}\mathrm{E}\{Y_{nk}^{2}1_{[ \epsilon^{2}\sqrt{n}<|X_{nk}|\leq n]}\}\] (c) \[\quad\leq\epsilon^{2}+\frac{1}{\epsilon^{2}n}{\sum_{k=1}^{n}} \mathrm{E}\{|X_{nk}|1_{[\epsilon^{2}\sqrt{n}<|X_{nk}|\leq n]}\}\] (d) \[\quad<2\epsilon^{2}\qquad\mbox{for all }n\geq\mbox{ (some $n_{ \epsilon}$)},\mbox{by (\ref{eq:khint})}.\]

So, \(\bar{Y}_{n}-\bar{\mu}_{n}\to_{p}\) when the average in (b) converges to \(0\) (which is slightly weaker than requiring the condition in (9)); that is, (a) holds. Then \(\bar{X}_{n}-\bar{\mu}_{n}\to_{p}0\) since

(e) \[P(\bar{X}_{n}\neq\bar{Y}_{n})\leq P(\max_{1\leq k\leq n}|X_{nk}| >n)\leq{\sum_{1}^{n}}P(|X_{nk}|>n)\] (f) \[\quad\leq\frac{1}{n}{\sum_{1}^{n}}\mathrm{E}\{|X_{nk}|1_{[n<|X_{nk }|]}\}\] (g) \[\quad\to 0\quad\mbox{by (\ref{eq:kint})}.\]

Finally, \(\mathrm{E}(\bar{X}_{n})-\bar{\mu}_{n}\to 0\) is exactly the conclusion in (g); so \(\bar{X}_{n}-\mathrm{E}(\bar{X}_{n})\to_{p}0\).

(i) In this iid case with finite mean, the condition (9) is satisfied and \(\mathrm{E}(\bar{X}_{n})=\mu\). In fact, \(\bar{X}_{n}\to_{\mathcal{L}_{1}\mu}\); use Vitali, since \(\bar{X}_{n}\to_{p}\mu\) and the \(\bar{X}_{n}\)'s are uniformly integrable by exercise 8.4.16 below. The rest of (i) is justified in theorem 8.4.1 below. \(\Box\)

**Remark 2.1** There are two natural ways to proceed to improve Khinchin's WLLN in the iid case. One way is to obtain the conclusion \(\bar{X}_{n}\to_{a.s.}\mu\); and this is done in Kolmogorov's SLLN (theorem 8.4.2 below). Another way is to continue to relax the assumption of a finite mean and center differently; and this is done in Feller's WLLN (theorem 8.4.1 below). (Other possibilities and other approaches will be outlined in the exercises of section 8.4.)

In section 8.3 we will develop a number of inequalities (so called "maximal inequalities") to help us to the stated goal. (At the end of this section the reader could go directly to section 8.4, and then go back to section 8.3 for the inequalities as they are needed.) (Also, at the end of section 8.3 we improve on the technique used in the proof of the Khintchine WLLN to obtain the _truncation inequality_ to be used in the WLLN in section 8.4 below.)

[MISSING_PAGE_FAIL:173]

## 3 Maximal Inequalities and Symmetrization\({}^{o}\)

Sums of independent random variables play an important role in probability and statistics. Our goal initially in this section is to develop probability bounds for the maximum of the first \(n\) partial sums. Such inequalities are called _maximal inequalities_. The most famous of these is Kolmogorov's inequality. For symmetric rvs, Levy's inequality is an extremely clean and powerful version of such a maximal inequality; it does not require the underlying rvs to have any moments. Neither does the Ottavani-Skorokhod inequality, which is true for arbitrary rvs, though it is not nearly as clean. (Recall (2.3.7) which shows that \(S_{n}\to_{a.s.}\) (some rv \(S\)) if and only if \(P(\max_{n\leq m\leq N}|S_{m}-S_{n}|\geq\epsilon)\leq\epsilon\) for all \(N\geq n\geq\) (some \(n_{\epsilon}\)).)

**Inequality 3.1** (Kolmogorov): Let \(X_{1},X_{2},\ldots\) be independent, with \(X_{k}\cong(0,\ \sigma_{k}^{2})\). Let \(S_{k}\equiv X_{1}+\cdots+X_{k}\). Then

\[P\left(\max_{1\leq k\leq n}|S_{k}|\geq\lambda\right)\leq\mbox{Var}[S_{n}]/ \lambda^{2}=\sum_{k=1}^{n}\sigma_{k}^{2}/\lambda^{2}\quad\mbox{for all }\lambda>0. \tag{1}\]

[This contains Chebyshev's inequality that \(P(|S_{n}|\geq\lambda)\leq\mbox{Var}[S_{n}]/\lambda^{2}\) for all \(\lambda>0\).]

**Proof.** Let \(A_{k}\equiv[\max_{1\leq j<k}|S_{j}|<\lambda\leq|S_{k}|]\), so that \(A\equiv\sum_{1}^{n}A_{k}=[\max_{1\leq k\leq n}|S_{k}|\geq\lambda]\). Thus \(k\) is the first index for which \(|S_{k}|\) exceeds \(\lambda\); call \(k\) the _first passage time_. Then

\[\mbox{Var}[S_{n}]=\int S_{n}^{2}dP\geq\int_{A}S_{n}^{2}dP=\sum_{1} ^{n}\int_{A_{k}}[(S_{n}-S_{k})+S_{k}]^{2}dP\] \[=\sum_{1}^{n}\int[(S_{n}-S_{k})^{2}1_{A_{k}}+(S_{n}-S_{k})2S_{k}1 _{A_{k}}+S_{k}^{2}1_{A_{k}}]dP\] (a) \[\geq\sum_{1}^{n}[\int 0\,dP+\mbox{E}(S_{n}-S_{k})\mbox{E}(2S_{k}1 _{A_{k}})+\int_{A_{k}}S_{k}^{2}dP]\quad\mbox{by independence}\] (b) \[\geq\sum_{1}^{n}[0+0\cdot\mbox{(a number)}+\int_{A_{k}}\lambda^{2 }dP]=\sum_{1}^{n}\lambda^{2}P(A_{k})=\lambda^{2}P(A).\]

**Definition 3.1** (Symmetric rvs): A rv \(X\) is called _symmetric_ if \(X\cong-X\). Note that this is equivalent to its df satisfying \(F(-x)=1-F_{-}(x)\) for all \(x\geq 0\). Suppose \(X\cong X^{\prime}\) are independent rvs; then \(X^{s}\equiv X-X^{\prime}\) is called the _symmetrization_ of the initial rv \(X\).

**Definition 3.2** (Medians): Let \(X\) be an arbitrary rv. Then \(m\equiv\mbox{median(X)}\) is any number for which \(P(X\geq m)\geq\frac{1}{2}\) and \(P(X\leq m)\geq\frac{1}{2}\). [One median of the symmetrization \(X^{s}\) of any rv \(X\) is always \(0\). And (2) below shows that the tails of \(X^{s}\) behave roughly the same as do those of \(X\).]

**Inequality 3.2** (Symmetrization inequality): Let \(X^{s}\equiv X-X^{\prime}\) where \(X\cong X^{\prime}\) with \(X\) and \(X^{\prime}\) independent. Let \(r>0\) and let \(a\) be any real number. Then both

\[\begin{array}{l}2^{-1}P(|X-\mbox{median}(X)|\geq\lambda)\leq P(|X^{s}|\geq \lambda)\leq 2P(|X-a|\geq\lambda/2)\mbox{ and}\\ 2^{-1}\mbox{E}|X-\mbox{median}(X)|^{r}\leq\mbox{E}|X^{s}|^{r}\leq 2^{1+r} \mbox{E}|X-a|^{r}.\end{array} \tag{2}\]

We may replace \(\geq\) by \(>\) in the three events in the upper half of (2).

**Proof.** Let \(m\equiv\mbox{median}(X)\). Now, the first inequality comes from

\[P(X^{s}\geq\lambda)=P[(X-m)-(X^{\prime}-m)\geq\lambda]\] (a) \[\geq P(X-m\geq\lambda)P(X^{\prime}-m\leq 0)\geq P(X-m\geq\lambda)/2.\]The second inequality holds, since for any real \(a\),

\[P(|X^{s}|\geq\lambda)=P(|(X-a)-(X^{\prime}-a)|\geq\lambda)\] (b) \[\leq P(|X-a|\geq\lambda/2)+P(|X^{\prime}-a|\geq\lambda/2)=2P(|X-a| \geq\lambda/2).\]

Plug (2) into (6.4.13) for the moment inequalities. \(\Box\)

**Inequality 3.3** (Levy): Let \(X_{1},\ldots,X_{n}\) be independent and symmetric rvs. Now define, \(S_{n}\equiv X_{1}+\cdots+X_{n}\). Then both

\[P\left(\max_{1\leq k\leq n}|S_{k}|\geq\lambda\right)\leq 2P(|S_{n}|\geq \lambda)\quad\mbox{for all }\lambda>0\qquad\qquad\mbox{and} \tag{3}\] \[P\left(\max_{1\leq k\leq n}|X_{k}|\geq\lambda\right)\leq 2P(|S_{n} |\geq\lambda)\quad\mbox{for all }\lambda>0. \tag{4}\]

Thus, \(2\mbox{E}|S_{n}|^{r}\geq\{\mbox{E}(\max_{1\leq k\leq n}|S_{k}|^{r})\vee\mbox{E }(\max_{1\leq k\leq n}|X_{k}|^{r})\}\), for each \(r>0\).

**Proof.** Let \(A_{k}\equiv[\max_{1\leq j<k}S_{j}<\lambda\leq S_{k}]\) for \(1\leq k\leq n\), so that \(k\) is the smallest index for which \(S_{k}\) exceeds \(\lambda\). Then

\[\mbox{(a)} P(S_{n}\geq\lambda)=\sum_{k=1}^{n}P(A_{k}\cap[S_{n}\geq\lambda]) \geq\sum_{1}^{n}P(A_{k}\cap[S_{n}\geq S_{k}])\] \[=\sum_{1}^{n}P(A_{k})P(S_{n}-S_{k}\geq 0)\] \[\mbox{by independence of }X_{1},\ldots,X_{k}\mbox{ from }X_{k+1}, \ldots,X_{n}\] \[\mbox{(b)} \geq\sum_{1}^{n}P(A_{k})/2\qquad\mbox{by symmetry}\] \[\mbox{(c)} =P(\max_{1\leq k\leq n}S_{k}\geq\lambda)/2.\]

Combine this with the symmetric result, and achieve the first claim.

Now let \(A_{k}\equiv[\max_{1\leq j<k}|X_{j}|<\lambda\leq|X_{k}|]\) for \(1\leq k\leq n\). Fix \(k\). Let \(S_{n}^{o}\equiv 2X_{k}-S_{n}\cong S_{n}\), and note that \(2\lambda\leq 2|X_{k}|\leq|S_{n}|+|S_{n}^{o}|\) on \(A_{k}\). Moreover,

\[\mbox{(d)} P(A_{k})\leq P(A_{k}\cap[|S_{n}|\geq\lambda])+P(A_{k}\cap[|S_{n}^{o}| \geq\lambda])=2P(A_{k}\cap[|S_{n}|\geq\lambda]).\]

So summing on \(k\) gives \(P(A)\leq 2P(A\cap[|S_{n}|\geq\lambda])\leq 2P(|S_{n}|\geq\lambda)\).

See Feller(1966) proof: Let \(M\equiv X_{K}\), where \(K\equiv\min\{k:|X_{k}|=\max_{1\leq j\leq n}|X_{j}|\}\). Let \(T\equiv S_{n}-X_{K}\). Then, for all four choices of \(+\) or \(-\) signs, the rvs \((\pm M,\ \pm T)\) have the same distribution. Then we require both

\[P(M\geq\lambda)\leq P(M\geq\lambda\mbox{ and }T\geq 0)+P(M\geq \lambda\mbox{ and }T\leq 0)\] \[\mbox{(e)} =2P(M\geq\lambda\mbox{ and }T\geq 0)\] \[\mbox{(f)} \leq 2P(M+T\geq\lambda)=2P(S_{n}\geq\lambda)\]

and the symmetric result. [See exercise 3.2 below for more on (4).] \(\Box\)

**Remark 3.1**: Kolmogorov's inequality is a moment inequality. Since the \(\mbox{rv}\ S_{n}/\mbox{StDev}[S_{n}]\cong(0,1)\) is often approximately normal \((2\pi)^{-1/2}\exp(-x^{2}/2)\) on the line, and since

\[P(|N(0,1)|\geq\lambda)=\int_{[|x|\geq\lambda]}\frac{1}{\sqrt{2 \pi}}\exp(-x^{2}/2)\,dx\leq\sqrt{\frac{2}{\pi}}\int_{\lambda}^{\infty}\frac{x} {\lambda}\exp(-x^{2}/2)\,dx \tag{6}\] \[\leq\sqrt{\frac{2}{\pi}}\frac{1}{\lambda}\exp(-\lambda^{2}/2)\mbox { for all }\lambda>0,\]

both Levy's inequality and the Ottaviani-Skorokhod inequality to follow offer the hope of a much better bound. \(\Box\)

**Inequality 3.4** Let \(S_{k}\equiv X_{1}+\cdots+X_{k}\) for independent rvs \(X_{k}\).

**(Ottaviani-Skorokhod)** For all \(0<c<1\) we have

\[P\left(\max_{1\leq k\leq n}|S_{k}|\geq\lambda\right)\leq\frac{P(|S _{n}|\geq c\lambda)}{[1-\max_{1\leq k\leq n}P(|S_{n}-S_{k}|>(1-c)\lambda)]} \quad\mbox{ for }\lambda>0\] \[\quad\leq 2P(|S_{n}|\geq c\lambda)\qquad\mbox{for all }\lambda\geq \sqrt{2}\ \mbox{StDev}[S_{n}]/(1-c).\]

**(Etemadi)** Alternatively,

\[P(\max_{1\leq k\leq n}|S_{k}|\geq 4\lambda)\leq 4\max_{1\leq k\leq n}P(|S_{k}| \geq\lambda)\quad\mbox{for all }\lambda>0.\]

Hence, \(\mbox{E}(\max_{1\leq k\leq n}|S_{k}|^{r})\leq 4^{1+r}\max_{1\leq k\leq n} \mbox{E}|S_{k}|^{r}\) for each \(r>0\). (See (8.2.13).)

**Proof.** Let \(A_{k}\equiv[S_{1}<\lambda,\ \dots,\ S_{k-1}<\lambda,\ S_{k}\geq\lambda]\), so that \(\sum_{k=1}^{n}A_{k}=[\max_{1\leq k\leq n}S_{k}\geq\lambda]\). Thus \(k\) is the smallest index for which \(S_{k}\) exceeds \(\lambda\). (This is now the third time we have used this same trick.) Note that

(a) \[a \equiv\min_{1\leq k\leq n}P(|S_{n}-S_{k}|\leq(1-c)\lambda)\] (b) \[=1-\max_{1\leq k\leq n}P(|S_{n}-S_{k}|>(1-c)\lambda)\] \[\geq 1-\max_{1\leq k\leq n}\ \mbox{Var}[S_{n}-S_{k}]/[(1-c) \lambda]^{2}\qquad\mbox{by Chebyshev's inequality}\] \[\geq 1\mbox{--Var}[S_{n}]/[(1-c)\lambda]^{2}\] (c) \[\geq\tfrac{1}{2}\qquad\mbox{if }\lambda\geq\sqrt{2}\ \mbox{StDev}[S_{n}]/ (1-c)\]

allows us to "improve" (7) to (8). Meanwhile, (7) comes from

(d) \[a\times P(\max_{1\leq k\leq n}S_{k}\geq\lambda)\leq\sum_{k=1}^{n} P(|S_{n}-S_{k}|\leq(1-c)\lambda)P(A_{k})\] (e) \[=\sum_{k=1}^{n}P(A_{k}\cap[|S_{n}-S_{k}|\leq(1-c)\lambda])\qquad \mbox{by independence}\] (f) \[\leq P(S_{n}\geq c\lambda).\]

Combining (f) and (b) with the analogous result for \(-S_{n}\) completes the proof. \(\Box\)

**Exercise 3.1** Prove Etemadi's inequality.

**Exercise 3.2** Consider independent rvs \(X_{k}^{s}\equiv X_{k}-X_{k}^{\prime}\), for \(1\leq k\leq n\), with all \(X_{k}\) and \(X_{k}^{\prime}\) independent, and with each \(X_{k}^{\prime}\cong X_{k}\). Let \(m_{k}\) denote a median of \(X_{k}\), and let \(a\) denote any real number. Let \(\lambda>0\) and \(r>0\). Show that both:

\[2^{-1}P(\max|X_{k}-m_{k}|\geq\lambda)\leq P(\max|X_{k}^{s}|\geq \lambda)\leq 2P(\max|X_{k}-a|\geq\lambda/2). \tag{9}\] \[2^{-1}\mbox{E}(\max|X_{k}-m_{k}|^{r})\leq\mbox{E}(\max|X_{k}^{s }|^{r})\leq 2^{1+r}\mbox{E}(\max|X_{k}-a|^{r}).\]

[Also, \(2^{-1}P(\max|X_{k}^{s}|\geq\lambda)\leq P(|S_{n}^{s}|\geq\lambda)\leq 2P(|S_{n} -a|\geq\lambda/2)\) (for any real \(a\)), by inequality 3.3 and inequality 3.2.]

**Inequalities for Rademacher RVs\({}^{*}\)****Inequality 3.5** (Symmetrization; Gine-Zinn)** Let \(X_{1},\dots,X_{n}\) be iid rvs, and let \(\epsilon_{1},\dots,\)\(\epsilon_{n}\) denote an independent sample of iid Radamacher rvs (that satisfy \(P(\epsilon_{k}=\pm 1)=\frac{1}{2}\)). Then

\[P\left(\frac{1}{\sqrt{n}}\left|\sum_{k=1}^{n}\epsilon_{k}X_{k}\right|>2\lambda \right)\leq\sup_{1\leq m\leq n}2P\left(\left|\frac{1}{\sqrt{m}}\sum_{k=1}^{m}X _{k}\right|>\lambda\right)\quad\mbox{for all }\lambda>0. \tag{10}\]

**Proof.** By conditioning on the Rademacher rvs we obtain

(a) \[P(n^{-1/2}|\sum_{1}^{n}\epsilon_{k}X_{k}|>2\lambda)\] \[\qquad\leq P(n^{-1/2}|\sum_{k:\epsilon_{k}=1}\epsilon_{k}X_{k}|> \lambda)+P(n^{-1/2}|\sum_{k:\epsilon_{k}=-1}\epsilon_{k}X_{k}|>\lambda)\] \[\qquad\leq\mathrm{E}_{\epsilon}P(n^{-1/2}|\sum_{k:\epsilon_{k}=1}X _{k}|>\lambda)+\mathrm{E}_{\epsilon}P(n^{-1/2}|\sum_{k:\epsilon_{k}=-1}X_{k}|>\lambda)\] (b) \[\qquad\leq 2\sup_{m\leq n}P(n^{-1/2}|\sum_{1}^{m}X_{k}|>\lambda)\] \[\qquad\leq 2\sup_{m\leq n}P(m^{-1/2}|\sum_{1}^{m}X_{k}|>\lambda)\] (c) \[\qquad\leq 2\sup_{m\geq 1}P(m^{-1/2}|\sum_{1}^{m}X_{k}|>\lambda), \qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{as required.}\quad\square\]

**Exercise 3.3** (a) (Khinchin inequality) Suppose \(\epsilon_{1},\ldots,\epsilon_{n}\) are iid Rademacher rvs. Let \(a_{1},\ldots,a_{n}\) be real constants. Then

\[A_{r}(\sum_{1}^{n}a_{k}^{2})^{1/2}\leq(\mathrm{E}|\sum_{1}^{n}a_{k}\epsilon_{ k}|^{r})^{1/r}\leq B_{r}(\sum_{1}^{n}a_{k}^{2})^{1/2},\text{ for each }\quad r\geq 1, \tag{11}\]

for some constants \(A_{r}\) and \(B_{r}\). Establish this for \(r=1\), with \(A_{1}=1/\sqrt{3}\) and \(B_{1}=1\). [Hint. Use Littlewood's inequality with \(r,s,t\) equal to 1, 2, 4.]

(b) (Marcinkiewicz-Zygmund inequality) For \(X_{1},\ldots,X_{n}\) independent 0 mean rvs,

\[(\tfrac{1}{2}A_{r})^{r}\mathrm{E}(\sum_{1}^{n}X_{k}^{2})^{r/2}\leq\mathrm{E}| \sum_{1}^{n}X_{k}|^{r}\leq(2B_{r})^{r}\mathrm{E}(\sum_{1}^{n}X_{k}^{2})^{r/2},\text{ for each }\quad r\geq 1. \tag{12}\]

**Exercise 3.4** Let \(X_{1},\ldots,X_{n}\) be independent with 0 means, and independent of the iid Rademacher rvs \(\epsilon_{1},\ldots,\epsilon_{n}\). Let \(\phi\) be \(\nearrow\) and convex on \(R\). Then

\[\mathrm{E}\phi(|\sum_{1}^{n}\epsilon_{k}X_{k}|/2)\leq\mathrm{E}\phi(|\sum_{1}^ {n}X_{k}|)\leq\mathrm{E}\phi(2|\sum_{1}^{n}\epsilon_{k}X_{k}|).\]

[Hint. The left side is an average of terms like \(\mathrm{E}\phi(|\sum_{1}^{n}e_{k}(X_{k}-\mathrm{E}X_{k}^{\prime})|/2)\), for independent \(X_{k}^{\prime}\cong X_{k}\) and with each \(e_{k}\) equal to \(\pm 1\).]

**Weak Negligibility, or Maximal Inequalities of Another IIk\({}^{*}\)****Discussion 3.1 (Weak negligibility)** Let \(Y_{n1},\ldots,Y_{nn}\) be independent rvs having dfs \(F_{n1},\ldots,F_{nn}\). Let \(\theta>0\) be given. For any \(\epsilon>0\), let \(p_{nk}^{\epsilon}\equiv P(|Y_{nk}|>\epsilon)\). Now, the maximum \(\mathrm{Max}_{n}\equiv[\max_{1\leq k\leq n}|Y_{nk}|]\) satisfies

\[1-\exp(-\sum_{1}^{n}p_{nk}^{\epsilon})\leq 1-\prod_{1}^{n}(1-p_{nk}^{\epsilon})=P (\mathrm{Max}_{n}>\epsilon)\leq\sum_{1}^{n}p_{nk}^{\epsilon}. \tag{13}\]

[The equality uses \(\bigcap_{1}^{n}A_{k}^{c}=[\bigcup_{1}^{n}A_{k}]^{c}\), and the first bound follows from the inequality \(1-x\leq\exp(-x)\).] This gives (so does exercise 8.1.5) the standard result that

\[\mathrm{Max}_{n}\rightarrow_{p}0\quad\text{ if and only if }\quad n\bar{P}_{n}( \epsilon)\equiv\sum_{1}^{n}p_{nk}^{\epsilon}\to 0\text{ for all }\epsilon>0. \tag{14}\]

Define \(x_{\theta n}\) by requiring \([-x_{\theta n},\ x_{\theta n}]\) to be the smallest interval that is both closed and symmetric to which \(\bar{F}_{n}\equiv\sum_{1}^{n}F_{nk}/n\) assigns probability at least \(1-\theta/n\). Let \(\bar{P}_{n}(x)\equiv\frac{1}{n}\sum_{1}^{n}P(|Y_{nk}|>x)\) denote the average tail probability, and then let \(\overline{K}_{n}\) denote the qf of the df \(1-\bar{P}_{n}(\cdot)\). Note the quantile relationship \(x_{\theta n}=\overline{K}_{n}(1-\theta/n)\). Note that \(\overline{K}_{n}(1-\theta/n)=\inf\{x:1-\bar{P}_{n}(x)\geq 1-\theta/n\}=\inf\{x:\bar{P}_{n}(x) \leq\theta/n\}\). Thus

\[n\bar{P}_{n}(\epsilon)\leq\theta\qquad\text{ if and only if }\qquad\overline{K}_{n}(1-\theta/n)\leq\epsilon. \tag{15}\]Fix \(0<\epsilon\leq 1\) and \(0<\theta\leq 1\), and suppose that we are considering all \(n\) exceeding some \(n_{\epsilon,\theta}\). Conclusions (14) and (15) give (the seemingly new emphasis)

\[\mbox{Max}_{n}\to_{p}0\quad\mbox{ if and only if }\quad x_{\theta n}=\overline{K}_{n}(1- \theta/n)\to 0\mbox{ for all }0<\theta\leq 1.\qed \tag{16}\]

**Discussion 3.2** (Weak negligibility in the LLN context): Let \(\nu_{n}>0\) be constants. Applying the previous paragraph to the rvs \(|Y_{nk}|/n\nu_{n}\) (whose average df has the \((1-\theta/n)\)th quantile \(x_{\theta n}/n\nu_{n}\)) gives the equivalencies

\[M_{n}/\nu_{n}\equiv\Big{[}\max_{k}\frac{1}{n}|Y_{nk}|\Big{]}/\nu _{n}\to_{p}0, \tag{18}\] \[x_{\theta n}/n\nu_{n}\to 0\qquad\qquad\mbox{ for all }0<\theta\leq 1,\] (19) \[\sum_{1}^{n}P(|Y_{nk}|/n\nu_{n}>\epsilon)\to 0\quad\mbox{ for all }0<\epsilon\leq 1. \tag{17}\]

The _truncated absolute moment_\(u_{1n}\equiv\int_{[|y|\leq x_{1n}]}|y|d\bar{F}_{n}(y)\) as well as the _Winsorized absolute moment_\(\tilde{u}_{1n}\equiv u_{1n}+x_{1n}\bar{P}_{n}(x_{1n})\) are among the potential choices for \(\nu_{n}\) that could lead to \(\bar{X}_{n}/\nu_{n}\to_{p}1\) for independent arrays \(X_{n1},\ldots,X_{nn}\) on \([0,\ \infty)\). (Here \(x_{1n}\) means the quantile \(x_{\theta n}\) with \(\theta=1\).) \(\qed\)

**Inequality 3.6** (Daniels' equality): With high probability there is an upper linear bound on the uniform empirical df \(\mathbb{G}_{n}\). That is, for each \(0<\lambda<1\),

\[P(\mathbb{G}_{n}(t)\leq t/\lambda\mbox{ for all }0\leq t\leq 1)=P(\xi_{n:k}\geq \lambda k/n\mbox{ for }1\leq k\leq n)=1-\lambda. \tag{20}\]

**Proof.** (Robbins) The vector of Uniform(0, 1) order statistics (\(\xi_{n:1},\ \ldots\xi_{n:n}\)) has joint density \(n!\) on its domain \(0<t_{1}<\cdots<t_{n}<1\). Thus

(a) \[P(\mathbb{G}_{n}(t)\leq t/\lambda\mbox{ for }0\leq t\leq 1)=P(\xi_{n:k} \geq\lambda k/n\mbox{ for }1\leq k\leq n)\] (b) \[=\int_{\lambda}^{1}\int_{\lambda(n-1)/n}^{t_{n}}\cdots\int_{ \lambda 2/n}^{t_{3}}\int_{\lambda/n}^{t_{2}}n!\,dt_{1}\cdots dt_{n}=\cdots=\] (c) \[=n!(\frac{t^{n}}{n!}-\frac{\lambda t^{n-1}}{n!})|_{\lambda}^{1}= 1-\lambda.\qed\]

**Inequality 3.7** (Chang's inequality): With high probability there is a lower linear bound on the uniform empirical df \(\mathbb{G}_{n}\). That is, for all \(\lambda\geq 1\), we have

\[P(\|I/\mathbb{G}_{n}^{-1}\|_{\xi_{n1}}^{1}\leq\lambda)=P(\mathbb{G}_{n}(t) \geq t/\lambda\mbox{ on all of }[\xi_{n:1},1])\geq 1-2\lambda^{2}e^{-\lambda}. \tag{21}\]

(This provides a nice symmetry with the previous inequality; see Chapter 12.)

**Truncation Inequalities**

Let \(X_{n1},\ldots,X_{nn}\) be independent \(F_{n1},\ldots,F_{nn}\), with \(\bar{P}_{n}(x)\equiv\frac{1}{n}{\sum_{1}^{n}}P(|X_{nk}|>x)\). Note that \(1-\bar{P}_{n}(\cdot)\) is the average df of the rvs \(|X_{n1}|,\ldots,|X_{nn}|\), and recall from above (15) that \(x_{\theta n}\) denotes the \(1-\theta/n\) quantile of this df (for any \(0<\theta\leq 1\)). Let \(\bar{\tau}_{n}(x)\equiv x\bar{P}_{n}(x)\). Let \(\overline{|X|}_{n}\equiv\frac{1}{n}{\sum_{1}^{n}}|X_{nk}|,\ W_{n}\) be a rv with df \(\bar{F}_{n}\), and let

\[\mu_{\theta n}\equiv\int_{[|y|\leq x_{\theta n}]}y\,d\bar{F}_{n}(y)\qquad\mbox {and}\qquad u_{\theta n}\equiv\int_{[|y|\leq x_{\theta n}]}|y|\,d\bar{F}_{n}( y). \tag{22}\]

**Inequality 3.8** (Truncation inequality I): For row independent \(X_{n1},\ldots,X_{nn}\),

\[P(|\bar{X}_{n}-\mu_{\theta n}|\geq\epsilon u_{\theta n})\leq\frac{1}{\epsilon^{2} }(\frac{x_{\theta n}}{nu_{\theta n}})+\theta\qquad\text{for all }\epsilon>0. \tag{24}\] \[P(||\bar{X}|_{n}-u_{\theta n}|\geq\epsilon u_{\theta n})\leq\frac {1}{\epsilon^{2}}(\frac{x_{\theta n}}{nu_{\theta n}})+\theta\quad\text{ for all }\epsilon>0. \tag{23}\]

**Proof.** Let \(Y_{nk}\equiv X_{nk}\times 1_{[|X_{nk}|\leq c_{n}]}\) for some \(c_{n}>0\). Then for any \(\nu_{n}>0\),

(a) \[P(|\bar{X}_{n}-\mathrm{E}(\bar{Y}_{n})|\geq\epsilon\nu_{n})\leq \tfrac{1}{\epsilon^{2}\nu_{n}^{2}}\text{ Var }[\bar{Y}_{n}]+\sum_{1}^{n}P(Y_{nk}\neq X_{nk})\] (25) \[\leq\frac{1}{\epsilon^{2}\nu/_{n}^{2}}\frac{1}{n^{2}}\!\sum_{1}^ {n}\!\mathrm{E}(Y_{nk}^{2})+n\bar{P}_{n}(c_{n})\leq\tfrac{1}{\epsilon^{2}\nu/ _{n}^{2}}\tfrac{1}{n}\int_{[|y|\leq c_{n}]}y^{2}\,d\bar{F}_{n}(y)+n\bar{P}_{n} (c_{n})\] (b) \[\leq\frac{c_{n}}{\epsilon^{2}\nu_{n}^{2}}\frac{1}{n}\int_{[|y|\leq c_{n}]}| y|\,d\bar{F}_{n}(y)+n\bar{P}_{n}(c_{n}).\]

To obtain (23), set \(c_{n}=x_{\theta n}\) and \(\nu_{n}=u_{\theta n}\) in (b) (which uses \(\mathrm{E}(\bar{Y}_{n})=\mu_{\theta n}\)), and then observe that \(\int_{[|y|\leq x_{\theta n}]}y^{2}\,d\bar{F}_{n}(y)\leq x_{\theta n}u_{\theta n}\). \(\square\)

**Inequality 3.9** (Truncation inequality II): For row independent \(X_{n1},\ldots,X_{nn}\), define \(Y_{nk}\equiv X_{nk}\times 1_{[|X_{nk}|\leq n]}\), and then let \(\tilde{\mu}_{n}\equiv\mathrm{E}(\bar{Y}_{n})\). Then (for any \(0<\epsilon\leq 1\))

\[P(|\bar{X}_{n}-\tilde{\mu}_{n}|\geq\epsilon)\leq\tfrac{2}{\epsilon^{2}n}\int_{ [0,n]}x\bar{P}_{n}(x)\,dx=\tfrac{2}{\epsilon^{2}n}\int_{[0,n]}\bar{\tau}_{n}( x)\,dx. \tag{26}\]

**Proof.** From line (25) in the previous proof (with \(c_{n}=n\) and \(\nu_{n}=1\)) one gets

(a) \[P(|\bar{X}_{n}-\bar{\mu}_{n}|\geq\epsilon)\leq\tfrac{-1}{\epsilon^ {2}}\tfrac{1}{n}\int_{[0,n]}y^{2}d\bar{P}_{n}(y)+n\bar{P}_{n}(n)\] (b) \[=\tfrac{2}{\epsilon^{2}n}\int_{[0,n]}y\bar{P}_{n}(y)\,dy-\tfrac{1}{ \epsilon^{2}}\tfrac{n^{2}}{n}\bar{P}_{n}(n)+n\bar{P}_{n}(n)\quad\text{(integrating by parts)}\] (c) \[=\tfrac{2}{\epsilon^{2}n}\int_{[0,n]}\bar{P}_{n}(y)\,d(y^{2}/2)-( \tfrac{1}{\epsilon^{2}}-1)n\bar{P}_{n}(n)\leq\tfrac{2}{\epsilon^{2}n}\int_{[0, n]}y\bar{P}_{n}(y)\,dy.\]

\(\square\)

**Exercise 3.5** Write out the (nearly identical) details for the proof of (24).

**Exercise 3.6** Show that \(\bar{X}_{n}-\bar{\mu}_{n}\to_{p}0\) in the context of the previous inequality provided \(\sup\{\bar{\tau}_{n}(x):\epsilon^{2}\sqrt{n}\leq x\leq n\}\to 0\).

## 4 The Classical Laws of Large Numbers, LLNs

It is now time to present versions of the laws of large numbers under minimal hypotheses. The _weak_ law of large numbers (WLLN) will establish \(\to_{p}\), while the _strong_ law of large numbers (SLLN) will establish \(\to_{a.e.}\) of a sample average \(\bar{X}_{n}\).

**Theorem 4.1** (Wlln; Feller): Let \(X_{n1},\ldots,X_{nn}\) be iid with df \(F\) and qf \(K\), for each \(n\). Let \(\bar{X}_{n}\equiv(X_{n1}+\cdots+X_{nn})/n\). The following are equivalent:

\[\begin{array}{llll}(1)&\bar{X}_{n}-\mu_{n}\to_{p}0&\mbox{for some choice of constants $\mu_{n}$.}\\ (2)&\tau(x)\equiv xP(|X|>x)\to 0&\mbox{(true, iff $\tau^{\pm}(x)\equiv xP(X^{\pm}>x)\to 0$).}\\ (3)&t\{|F^{+}_{1}(t)|+|F^{-}(1-t)|\}\to 0&\mbox{(true, iff $tF^{-}_{|X|}(1-t)\to 0$).}\\ (4)&M_{n}\equiv\ [\ \frac{1}{n}\max_{1\leq k\leq n}|X_{nk}|]\to_{p}0.\end{array}\]

When (1) holds, possible choices include \(\mu_{n}\equiv\int_{[-n,n]}x\,dF(x),\nu_{n}\equiv\int_{1/n}^{1-1/n}K(t)\,dt\), and \(m_{n}\equiv\mbox{median}(\bar{X}_{n})\). If \(E|X|<\infty\), then (1) holds with \(\mu_{n}\equiv\mu\equiv\mbox{E}X\).

**Theorem 4.2** (SLLN; Kolmogorov): Let \(X,X_{1},X_{2},\ldots\) be iid rvs. Then:

\[\begin{array}{llll}(5)&\mbox{E}|X|<\infty&\mbox{implies}&\bar{X}_{n}\to_{a.s }\mu\equiv\mbox{E}X.\\ (6)&\mbox{E}|X|=\infty&\mbox{implies}&\overline{\lim}\,|X^{-}_{n}|=\infty\mbox { a.s.}\\ (7)&\mbox{E}|X|<\infty&\mbox{iff}&\overline{\lim}\,|X^{-}_{n}|<\infty\mbox{ a.s.} &\mbox{iff}&\bar{X}_{n}\to_{\mathcal{L}_{1}}\mbox{ (some rv).}\\ (8)&\mbox{E}|X|<\infty&\mbox{iff}&M_{n}\equiv[\frac{1}{n}\max_{1\leq k\leq n}|X _{k}|]\to_{a.s.}0&\mbox{iff}&\frac{X_{n}}{n}\to_{a.s.}0&\mbox{iff}&M_{n}\to_{ \mathcal{L}_{1}}0.\end{array}\]

Conditions (4) and (8) show the sense in which these LLNs are tied to the size of the maximal summand. This is an important theme, do not lose sight of it. We now give a symmetric version of condition (4). (See also exercises 4.18-4.21.)

**Theorem 4.3** (The maximal summand): Let \(X_{n1},\ldots,X_{nn},n\geq 1\), be iid row independent rvs with df \(F\). We then let \(X^{s}_{nk}\equiv X_{nk}-X^{\prime}_{nk}\) denote their symmetrized versions. Fix \(r>0\). [Most important is \(r=1\).] Then

\[\begin{array}{llll}(9)&\tau(x)\equiv x^{r}P(|X|>x)\to 0&\mbox{iff}&\tau^{s}(x) \equiv x^{r}P(|X^{s}|>x)\to 0&\mbox{iff}\\ (10)&[\max_{1\leq k\leq n}\frac{1}{n}|X_{nk}-a|^{r}]\to_{p}0\mbox{ for all/some $a$}&\mbox{iff}&[\max_{1\leq k\leq n}\frac{1}{n}|X^{s}_{nk}|^{r}]\to_{p}0. \end{array}\]

**Proof.** We first consider the SLLN. Let \(Y_{n}\equiv X_{n}\times 1_{[|X_{n}|<n]}\), for the iid \(X_{n}\)'s.

Suppose \(E|X|<\infty\). Using inequality 8.2.1 in the second step and iid in the third, we obtain

\[\begin{array}{llll}(\mbox{a})&\mu=\mbox{E}X\mbox{ is finite}&\mbox{iff}& \mbox{E}|X|<\infty&\mbox{iff}&\sum_{n=1}^{\infty}P(|X|\geq n)<\infty\\ &\mbox{iff}&\sum_{n=1}^{\infty}P(|X_{n}|\geq n)<\infty&\mbox{iff}&\sum_{n=1}^{ \infty}P(Y_{n}\neq X_{n})<\infty\end{array}\]

\[\begin{array}{llll}(\mbox{b})&\mbox{iff}&\mbox{the $X_{n}$'s and $Y_{n}$'s are Khinchin equivalent rvs.}\\ \end{array}\]

**Comment**: Recall that \(\mbox{E}|X|=\int_{0}^{\infty}P(|X|>y)\,dy\) and (see (6.3.12))

\[\begin{array}{llll}(i)&\int_{(x,\infty)}\,y\,dF_{|X|}(y)=xP(|X|>x)+\int_{x}^{ \infty}P(|X|>y)\,dy;&\mbox{so}\\ (ii)&\int_{[0,x]}\,y\,dF_{|X|}(y)=\int_{0}^{x}P(|X|>y)\,dy-xP(|X|>x).\end{array}\]Thus \(\bar{X}_{n}\to_{a.s.}\mu\) wherever \(\bar{Y}_{n}\to_{a.s.}\mu\), by proposition 8.2.1(c). So we now make the definitions \(\mu_{n}\equiv\mbox{EY}_{n}\) and \(\sigma_{n}^{2}\equiv\mbox{Var}[Y_{n}]\). Then we can write

(c) \[\bar{Y}_{n}=[\sum_{1}^{n}(Y_{i}-\mu_{i})/n]+\bar{\mu}_{n},\qquad\mbox{where}\, \bar{\mu}_{n}\equiv(\mu_{1}+\cdots+\mu_{n})/n\to\mu\]

by the Cesaro summability exercise 8.1.1 (since \(\mu_{n}=\mbox{EY}_{n}=\int_{(-n,n)}\,x\,dF(x)\to\mu\) by the DCT with dominating function given by \(|x|\)). It thus suffices to show that \(\sum_{1}^{n}(Y_{i}-\mu_{i})/n\to_{a.s.}0\). By Kronecker's lemma it thus suffices to show that

(d) \[Z_{n}\equiv\sum_{i=1}^{n}(Y_{i}-\mu_{i})/i\to_{a.s.}\mbox{ (some rv $Z$)}.\]

But \(Z_{n}\to_{a.s.}\mbox{ (some $Z$)}\), by proposition 2.3.3, if for all \(\epsilon>0\) we have

\[p_{nN}^{\epsilon}\equiv P(\max_{n\leq m\leq N}|Z_{m}-Z_{n}|\geq\epsilon)\] (e) \[=P(\max_{n\leq m\leq N}|\sum_{i=n+1}^{m}[(Y_{i}-\mu_{i})/i]|\geq \epsilon)\to 0.\]

Kolmogorov's inequality yields (e) via

\[p_{nN}^{\epsilon}\leq\epsilon^{-2}{\sum_{n+1}^{N}}\mbox{Var}[(Y_{i}-\mu_{i})/i ]=\epsilon^{-2}{\sum_{n+1}^{N}}\sigma_{i}^{2}/i^{2}\] (f) \[\leq\epsilon^{-2}{\sum_{n+1}^{\infty}}\sigma_{i}^{2}/i^{2}\qquad \mbox{for all $N$}\] \[\to 0\qquad\mbox{as $n\to\infty$},\]

provided that

(g) \[\sum_{n=1}^{\infty}\sigma_{n}^{2}/n^{2}=\sum_{n=1}^{\infty}\mbox{Var}[Y_{n}-\mu _{n}]/n^{2}<\infty.\]

Now, this last is seen to be true via the following Kolmogorov argument that

\[{\sum_{1}^{\infty}}\mbox{Var}[Y_{n}-\mu_{n}]/n^{2}\leq{\sum_{1}^{ \infty}}\mbox{EY}_{n}^{2}/n^{2}={\sum_{1}^{\infty}}\int_{[|x|<n]}x^{2}\,dF(x)/n ^{2}\] \[={\sum_{1}^{\infty}}{\sum_{k=1}^{n}}\int_{[k-1\leq|x|<k]}x^{2}\,dF (x)/n^{2}\] (h) \[={\sum_{k=1}^{\infty}}{\sum_{n=k}^{\infty}}{\frac{1}{n^{2}}}\int_{[ k-1\leq|x|<k]}x^{2}\,dF(x)\] (i) \[\leq{\sum_{k=1}^{\infty}}2\int_{[k-1\leq|x|<k]}x^{2}\,dF(x)/k\] \[\mbox{since }\quad{\sum_{n=k}^{\infty}}1/n^{2}\leq\int_{k}^{ \infty}(2/x^{2})\,dx=2/k\] (j) \[\leq 2{\sum_{k=1}^{\infty}}\int_{[k-1\leq|x|<k]}|x|\,dF(x)\] (k) \[=2\mbox{E}|X|<\infty.\]

Thus we do have \(Z_{n}\to_{a.s.}\mbox{ (some rv $Z$)}\), and so \(\bar{X}_{n}\to_{a.s.}\mu\).

Suppose E\(|X|=\infty\). Then the "sandwich" inequality 8.2.1 gives

(l) \[{\sum_{n=0}^{\infty}}P(|X_{n}|\geq nC)={\sum_{n=0}^{\infty}}P(|X|/C\geq n)\geq \mbox{E}|X/C|=\infty\mbox{ for all $C>0$},\]

so that applying the second Borel-Cantelli lemma to (l) gives

(m) \[P(|X_{n}|\geq nC\mbox{ i.o.})=1\qquad\mbox{for all}\quad C>0\mbox{ (and hence for all large $C>0$)}.\]

Since \(S_{n}=S_{n-1}+X_{n},(\mbox{m})\) implies (using the fact that \(|S_{n}|<nC/2\) and \(|X_{n}|\geq nC\) yields \(|S_{n-1}|>nC/2>(n-1)C/2\)) that

(n) \[P(|S_{n}|\geq nC/2\mbox{ i.o.})=1\quad\mbox{for all $C>0$}.\]That is, \(\overline{\lim}\,|S_{n}|/n\geq C/2\) a.s. for all \(C\). That is, \(\overline{\lim}\,|S_{n}|/n=\infty\) a.s.

Thus (5)-(7) hold. [Apply Vitali with exercise 4.16 below for \(\bar{X}_{n}\to_{\mathcal{L}_{1}}\mu\) in (7). If \(\bar{X}_{n}\to\mathcal{L}_{1}\) (some rv \(W\)), then \(\bar{X}_{n}\to_{p}\) W--and the averages \(\bar{X}_{n}\) are u.i. by Vitali. Thus E\(X\) is finite.]

Consider (8). Suppose \(M_{n}\to_{a.s.}0\). Then a.s. for all \(n\geq\) (some \(n_{\omega}\)) we have

(o) \[[\max_{1\leq k\leq n}|X_{k}|]/n<\epsilon,\mbox{ and hence }|X_{n}|/n<\epsilon.\]

We merely repeat this last statement, writing

(p) \[A_{n}\equiv[|X_{n}|/n\geq\epsilon]\quad\mbox{satisfies}\quad P(A_{n}\,\mbox{i.o.})=P(|X_{n}|/n\geq\epsilon\mbox{ i.o.})=0.\]

Thus inequality 8.2.1 (by applying iid, and then the second Borel-Cantelli) gives

(q) \[\mbox{E}|X|/\epsilon=\mbox{E}|X/\epsilon|\leq\sum_{n=0}^{\infty}P(|X/\epsilon| \geq n)=\sum_{n=0}^{\infty}P(|X_{n}|/n\geq\epsilon)<\infty.\]

Conversely, suppose E\(|X|<\infty\). Then \(S_{n}/n\to_{a.s.}\mu\) by the SLLN. Since

(r) \[\frac{X_{n}}{n}=\frac{S_{n}-n\mu}{n}-\frac{n-1}{n}\left[\frac{S_{n-1}-(n-1)\mu} {n-1}\right]+\frac{\mu}{n}\to_{a.s.}0-1\cdot 0+0=0,\]

we have a.s. that

(s) \[|X_{n}|/n\leq\epsilon\qquad\mbox{for all }n\geq(\mbox{some }n_{\omega}).\]

Thus for all \(n\) exceeding some even larger \(n^{\prime}_{\omega}\) we have

\[\left[\max_{1\leq k\leq n}\frac{|X_{k}|}{n}\right]=\left[\max_{1\leq k\leq n} \frac{k}{n}\cdot\frac{|X_{k}|}{k}\right]\leq\left[\max_{1\leq k\leq n_{\omega}} \frac{|X_{k}|}{n}\right]\vee\left[\max_{k\geq n_{\omega}}\left|\frac{|X_{k}|}{ k}\right|\right] \tag{11}\]

(t) \[\leq n^{-1}[\mbox{ a fixed number depending on }\omega]+\epsilon\leq 2\epsilon\quad\mbox{ using (s)},\]

where we will have to increase the specification on \(n^{\prime}_{\omega}\) for (t). Thus \(M_{n}\to_{a.s.}0\).

Finally, note exercise 4.17 for \(M_{n}\to_{\mathcal{L}_{1}}0\) if and only if E\(|X|<\infty\).

From (6.4.11) we see (note figure 4.1) that

\[\mbox{E}|X|<\infty\quad\mbox{iff}\quad\int_{0}^{\infty}P(|X|>x)\,dx<\infty\quad \mbox{iff}\quad\int_{0}^{1}|F^{-1}(t)|dt<\infty.\qed \tag{12}\]

**Remark 4.1**: Suppose \(X_{1},\ldots,X_{n}\) are independent, with \(X_{i}\cong(0,\sigma_{i}^{2})\). Then

\[S_{n}\equiv X_{1}+\cdots+X_{n}\cong(0,\ \sum_{1}^{n}\sigma_{i}^{2}),\quad\mbox{ while }\quad\bar{X}_{n}\equiv S_{n}/n\cong(0,\ \sum_{1}^{n}\sigma_{i}^{2}/n^{2}).\]

Chebyshev's equality and Kolmogorov's inequality give, respectively,

\[\begin{array}{ll}(a)&P(|S_{n}|\geq\lambda)\leq\mbox{Var}[S_{n}]/\lambda^{2}= \sum_{i=1}^{n}\sigma_{i}^{2}/\lambda^{2}&\mbox{for all }\lambda>0,\\ (b)&P(\max_{1\leq k\leq n}|S_{k}|\geq\lambda)\leq\mbox{Var}[S_{n}]/\lambda^{2}& \mbox{for all }\lambda>0.\end{array} \tag{13}\]

For \(X_{1},X_{2},\ldots\) iid \((\mu,\ \sigma^{2})\), the inequality (13)(a) gives \(\bar{X}_{n}\to_{p}\mu\), by Chebyshev's inequality. But the WLLN conclusion \(\bar{X}_{n}\to_{p}\mu\) should not require the variance \(\sigma^{2}\) to be finite, as this cheap proof based on (13)(a) requires. Indeed, Khintchine's WLLN of theorem 8.2.1 didn't. Exercise 4.8 below outlines one very cheap proof of the SLLN using "only" the Borel-Cantelli lemma, and exercise 4.9 outlines a slightly improved version that also uses Kolmogorov's inequality. Kolmogorov's proof of the full SLLN made the key step of incorporating truncation. Exercise 4.10 describes an elementary way to avoid use of Kronecker's lemma.

**Proof.** Consider the WLLN. Suppose (2) holds. Define \(Y_{nk}\equiv X_{nk}\times 1_{[|X_{nk}|\leq n]}\) and \(\mu_{n}\equiv\mathrm{E}Y_{nk}=\int_{[-n,n]}x\,dF(x)\). Now (using integration by parts for (b)),

\[P(|\bar{X}_{n}-\mu_{n}|\geq\epsilon)=P(|\bar{X}_{n}-\mathrm{E} \bar{Y}_{n}|\geq\epsilon) \tag{14}\] \[\qquad\leq\frac{2}{n\epsilon^{2}}\int_{[0,n]}\tau(x)\,dx=\frac{2} {n\epsilon^{2}}\int_{[0,n]}xP(|X|>x)\,dx\mbox{ for all }0<\epsilon\leq 1,\]

by the truncation inequality (8.3.26). (Note (6.4.18).) Note that \(\tau(x)\leq x\), and choose \(M>0\) so large that \(\tau(x)<\epsilon^{3}/4\) for \(x>M\). Apply this to (14) for

(a) \[\tfrac{2}{n\epsilon^{2}}\int_{0}^{n}\tau(x)\,dx\leq\tfrac{2}{n\epsilon^{2}} \{\int_{0}^{M}xdx+\int_{M}^{n}\tfrac{\epsilon^{3}}{4}\,dx\}\leq\tfrac{M^{2}}{ n\epsilon^{2}}+\tfrac{\epsilon}{2}\leq\epsilon\]

for all \(n\geq 2M^{2}/\epsilon^{3}\). Combining (14) and (a), it follows that

(b) \[P(|\bar{X}_{n}-\mu_{n}|\geq\epsilon)\leq\epsilon\qquad\mbox{for }n\geq\mbox{ ( some }N_{\epsilon}).\]

Thus \(\bar{X}_{n}-\mu_{n}\to_{p}0\). We also have \(\bar{X}_{n}-\mathrm{median}(\bar{X}_{n})\to_{p}0\), since the symmetrization inequality 8.3.2 gives

\[P(|\bar{X}_{n}-\mathrm{median}(\bar{X}_{n})|\geq\epsilon)\leq 4P(|\bar{X}_{n}-\mu_ {n}|\geq\epsilon/2)\to 0. \tag{15}\]

[The acceptability of the third exhibited choice for the centering constant is left to exercise 4.1 below.] In any case, we have shown that (2) implies (1).

The equivalence of (2) and (3) follows. Note figure 4.1, bearing in mind that \((a\lor b)\leq a+b\leq 2(a\lor b)\) for the definitions \(a\equiv\tau^{+}(x)\equiv xP(X^{+}>x)\) and \(b\equiv\tau^{-}(x)\equiv xP(X^{-}>x)\). Figure 4.1 thus shows that \(\tau^{-}(x)\to 0\) holds if and only if \(t|F^{-1}(t)|\to 0\), that \(\tau^{+}(x)\to 0\) holds if and only if \(t|F^{-1}(1-t)|\to 0\), and that \(\tau(x)\to 0\) holds if and only if \(tF^{-1}_{|X|}(1-t)\to 0\).

Consider the equivalence of (4) and (2). We know from (8.3.14) that \(M_{n}\to_{p}0\) if and only if \(nP(|X|>\epsilon n)\to 0\) for all \(\epsilon>0\), that is, if and only if \(\tau(\epsilon n)=\epsilon nP(|X|>\epsilon n)\to 0\) for all \(\epsilon>0\). Thus \(M_{n}\to_{p}0\) if and only if \(\tau(x)\to 0\) as \(x\to\infty\).

We still need to show that (1) implies (2), but we'll wait a paragraph for this.

Consider next theorem 4.3. We will only provide a proof with \(r=1\) (we may just replace \(|X|\) by \(|Y|\equiv|X|^{r}\), after raising \(|X|\) to the power \(r\)). Now, \(\tau_{X}(x)\to 0\) implies \(\tau^{s}(x)\to 0\) by the right-hand side of inequality 8.3.2 with \(a=0\), while the left-hand side then gives \(\tau_{X-\mathrm{med}}(x)\to 0\). The equivalence of (4) and (2) then gives \(\max|X_{nk}-\mathrm{med}|/n\to_{p}0\), which trivially gives \(\max|X_{nk}|/n\to_{p}0\), which gives \(\tau_{X}(x)\to 0\) by the equivalence of (4) and (2). This completes theorem 4.3.

Finally, we prove that (1) implies (2). Suppose that there exist some constants \(\mu_{n}\) such that \(S_{n}/n-\mu_{n}=\bar{X}_{n}-\mu_{n}\to_{p}0\). Let \(S_{n}^{s}=S_{n}-S_{n}^{\prime}\), where \(S_{n}^{\prime}\equiv X_{n1}^{\prime}+\cdots+X_{nn}^{\prime}\) with \(X_{nk}^{\prime}\cong X_{nk}\) and with \(X_{nk}\)'s and \(X_{nk}^{\prime}\) independent. Thus \(S_{n}^{s}/n\to_{p}0\). Then \(M_{n}^{s}\equiv\max_{1\leq k\leq n}|X_{nk}^{s}|/n\to_{p}0\) by the (8.3.4) Levy inequality. Thus \(\tau^{s}(x)\to 0\) by theorem 4.3, and hence \(\tau(x)\to 0\) by theorem 4.3. \(\Box\)

**Exercise 4.1**  Verify that the choice \(\nu_{\theta,n}\equiv\int_{\theta/n}^{1-\theta/n}K(t)\,dt\) (for any \(0<\theta\leq 1\)) also works in the WLLN as a centering constant in (1). The most natural choice is

\[\nu_{n}\equiv\int_{1/n}^{1-1/n}K(t)\,dt. \tag{16}\]

We have just seen that _good inequalities lead to good theorems_! In sections 8.9 and 12.11 we will add to our growing collection of good inequalities. Some will be used in this text, and some will not. But the author thinks it important to illustrate these possibilities.

**Exercise 4.2*** When \({\rm E}|X|=\infty\), the SLLN above showed that \(\lim_{n}|\bar{X}_{n}|=\infty\) a.s. Show the following stronger result. If \(X_{1},X_{2},\ldots\) are iid with \({\rm E}|X|=\infty\), then

\[\overline{\lim}_{n\to\infty}\,|\bar{X}_{n}-c_{n}|=_{a.s.}\ \infty\]

for every sequence of constants \(c_{n}\). (Note exercise 4.23 below.)

**Exercise 4.3** (Erickson) (a) If \({\rm E}X^{-}<\infty\) but \({\rm E}X^{+}=\infty\), then \(\lim S_{n}/n=_{a.s.}+\infty\). (b) (Kesten)\({}^{*}\) If both \({\rm E}X^{+}=\infty\) and \({\rm E}X^{-}=\infty\), then either \(S_{n}/n\to_{as}\infty,S_{n}/n\to_{as}\ -\infty\), or both \(\overline{\lim}\,S_{n}/n=\infty\) and \(\underline{\lim}\,S_{n}/n=-\infty\).

**Exercise 4.4** (Marcinkiewicz-Zygmund) Let \(X_{1},X_{2},\ldots\) be iid. Let \(0<r<2\). Establish the equivalence

\[{\rm E}|X|^{r}<\infty\qquad\hbox{if and only if}\qquad\frac{1}{n^{1/r}}\sum_{k= 1}^{n}(X_{k}-c)\to_{a.s.}0\quad\hbox{for some $c$.}\]

If so, then \(c={\rm E}X\) when \(1\leq r<2\), while \(c\) is arbitrary (so \(c=0\) works) when \(0<r<1\). [Hint. Truncate via \(Y_{n}\equiv X_{n}\times 1_{[|X_{n}|<n^{1/r}]}\) in a SLLN type proof.]

**Exercise 4.5*** (Feller) Let \(X_{1},X_{2},\ldots\) be iid with \({\rm E}|X|=\infty\). If \(a_{n}/n\uparrow\), then

\[\overline{\lim}\,|S_{n}|/a_{n}=\cases{=0\quad\hbox{a.s.},\cr=\infty\ \hbox{a.s.},\cr}\quad\hbox{according as}\quad\sum_{n=1}^{\infty}P(|X_{n}|\geq a_{n})= \cases{<\infty,\cr=\infty.}\]

[Note that \(P(|X_{n}|\geq a_{n}\) i.o.) equals \(0\) or \(1\) as \(\sum_{1}^{\infty}P(|X_{n}|\geq a_{n})\) is finite or infinite.]

**Exercise 4.6** (Fallify the overlap between (17) and (18).

**Exercise 4.7** (Random sample size) (a) Let \(X_{1},X_{2},\ldots\) be iid with \(\tau(x)\to 0\) as \(x\to\infty\). Let \(N_{n}\geq 0\) be any integer-valued rv satisfying \(N_{n}/n\to_{p}c\in(0,\infty)\). Then

\[S_{N_{n}}/N_{n}-\mu_{n}\to_{p}0,\quad\hbox{for}\,\mu_{n}\equiv\int_{[-n,n]}x\, dF(x).\]

(b) Suppose \(X_{1},X_{2},\ldots\) are iid and \(\mu\equiv{\rm E}X\) is finite. Let \(N_{n}\geq 0\) be any positive integer-valued rv satisfying \(N_{n}/n\to_{a.s.}c\in(0,\infty)\). Then

\[S_{N_{n}}/N_{n}\to_{a.s.}\mu.\]

**Exercise 4.8** (A weak SLLN) (a) For \(X_{n1},\ldots,X_{nn}\) independent (or, uncorrelated) with \(X_{nk}\cong(0,\ \sigma_{nk}^{2})\) and all \(\sigma_{nk}^{2}\leq(\hbox{some $M$})<\infty\), we have \(\bar{X}_{n}\to_{a.s.}0\).

Hint. Show that \(P(|S_{n}|\geq n\epsilon)\leq M/(n\epsilon^{2})\), so that \(P(|\bar{X}_{n^{2}}|=|S_{n^{2}}/n^{2}|>0\) i.o.) = 0. Then show that the "block maximum"

\[\Delta_{n}\equiv\max_{n^{2}<k<(n+1)^{2}}|S_{k}-S_{n^{2}}|\]

has \({\rm E}\Delta_{n}^{2}\leq 2n{\rm E}\{|S_{(n+1)^{2}-1}-S_{n^{2}}|^{2}\}\leq 4n^{2}M\), so that \(P(\Delta_{n}/n^{2}>\epsilon\ {\rm i.o.})=0\).

(b) Use Kolmogorov's inequality to obtain \({\rm E}\triangle_{n}^{2}\leq 2nM\), under independence.

**Exercise 4.9** Let \(X_{n1},\ldots,X_{nn}\) be row independent rvs (here, merely uncorrelated is much harder to consider) with means \(0\) and having all \({\rm E}X_{nk}^{4}\leq\) (some \(M\)) \(<\infty\).

(a) (Cantelli's inequality) Verify that \(\bar{X}_{n}\equiv S_{n}/n\equiv(X_{n1}+\cdots+X_{nn})/n\) satisfies

\[P(|S_{n}|\geq\lambda)\leq 3Mn^{2}/\lambda^{4}\quad\mbox{for all $\lambda>0$.}\]

(b) (A very weak SLLN) Show that under these circumstances \(\bar{X}_{n}\to_{a.s.}0\).

**Exercise 4.10** (Alternative proof of the SLLN) Apply either the Hajek-Renyi inequality (inequality 8.10.3) or the monotone inequality (inequality 8.10.1) as a replacement for the use of the Kronecker lemma in the SLLN proof.

**Exercise 4.11** (St. Petersburg paradox) Let \(X_{1},X_{2},\ldots\) be iid rvs for which \(P(X=2^{m})=1/2^{m}\) for \(m\geq 1\). Show that \(S_{n}/a_{n}-1=(S_{n}-b_{n})/a_{n}\to_{p}0\) for \(b_{n}\equiv n\log_{{\rm Base}\,2}n\) and \(a_{n}\equiv n\log_{{\rm Base}\,2}n\) also. Hint. Let \(Y_{nk}\equiv X_{k}1_{[X_{k}\leq n\,\log_{{\rm Base}\,2}n]}\). (While \(S_{n}/a_{n}\to_{p}1\) was just called for, it can also be shown that \(S_{n}/a_{n}\to_{a.s.}\infty\).)

**Exercise 4.12*** (Spitzer) Let \(X,X_{1},X_{2},\ldots\) be iid. Establish the following claim.

\[{\rm E}(X)=0\mbox{ for }X\in{\cal L}_{1}\qquad\mbox{iff}\qquad\sum_{n=1}^{ \infty}\tfrac{1}{n}P(|S_{n}|\geq n\epsilon)<\infty\qquad\mbox{for all $\epsilon>0$.} \tag{21}\]

**Exercise 4.13** If \(X_{1},X_{2},\ldots\) are iid Exponential(l), then \(\overline{\lim}\,X_{n}/\log n=1\) a.s. and \(X_{n:n}/\log n\to 1\) a.s.

**Exercise 4.14** If \(X_{1},X_{2},\ldots\) are iid N(0, 1), then \(X_{n:n}/\sqrt{2\log n}\to_{p}1\).

**Exercise 4.15** (a) Does the WLLN hold for the Cauchy distribution?

(b) Does the WLLN hold if \(P(|X|>x)=e/[2x\log x]\) for \(x\geq e,X\) symmetric?

(c) Make up one more example of each of these two types.

**Exercise 4.16** (Uniform integrability of sample averages) Let \(X_{1},X_{2},\ldots\) be iid, and let \(\bar{X}_{n}\equiv(X_{1}+\cdots+X_{n})/n\). Then the rvs \(\{X_{n}:n\geq 1\}\) are uniformly integrable if and only if the rvs \(\{\bar{X}_{n}:n\geq 1\}\) are uniformly integrable. (Relate this to the SLLN result in (7).) (We only need independence for u.i. \(X_{k}\)'s to yield u.i. \(\bar{X}_{n}\)'s.)

**Exercise 4.17** (a) Let row independent rvs \(X_{n1},\ldots,X_{nn}\) be iid with the df \(F(\cdot)\). Let \(F\) have finite mean \(\mu\equiv{\rm E}X\). We know \(M_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}|/n]\to_{p}0\) by the WLLN. Trivially, \({\rm E}M_{n}\leq{\rm E}|X|\). Show that

\[{\rm E}M_{n}={\rm E}[\max_{1\leq k\leq n}\frac{1}{n}|X_{nk}|]\to 0\qquad( \mbox{that is, $M_{n}\to_{{\cal L}_{1}}0$}). \tag{22}\]

(b) Let \(X_{1},X_{2},\ldots\) be iid. Show that \({\rm E}|X|<\infty\quad\mbox{if and only if}\quad M_{n}\to_{{\cal L}_{1}}0\).

**Exercise 4.18** (Negligibility for \(r=1\), a.s.) (i) Let \(X,X_{1},X_{2},\ldots\) be iid rvs. Let \(r>0\) (with \(r=1\) the most important case). Prove that the following are equivalent:

\[{\rm E}|X|^{r}<\infty. \tag{24}\] \[M_{rn}\equiv\ [\tfrac{1}{n}\max_{1\leq k\leq n}|X_{k}|^{r}]\to_{a.s.}0.\] (25) \[{\rm E}M_{rn}\to 0. \tag{23}\](ii) Since \({\rm E}|X|^{r}<\infty\) if and only if the symmetrized rv \(X-X^{\prime}\) has \({\rm E}|X-X^{\prime}|^{r}<\infty\) (by exercise 8.2.3), we can add three analogous equivalences for iid symmetric rvs distributed as \(X-X^{\prime}\).

**Exercise 4.19*** (Maller-Resnick; Kesten) For a sequence of iid rvs \(X,X_{1},X_{2},\ldots\) let \(\bar{X}_{n}\equiv(X_{1}+\cdots+X_{n})/n\) and let \(M_{1n}\equiv[\frac{1}{n}\max_{1\leq k\leq n}|X_{k}|]\). Then (difficult)

\[M_{1n}/|\bar{X}_{n}|\rightarrow_{a.s.}0\quad\mbox{if and only if}\quad 0<|{\rm E} X|<\infty. \tag{26}\]

**Exercise 4.20** (Negligibility for \(r=2\), a.s.) (i) Let \(X,X_{1},X_{2},\ldots\) be iid rvs (that are not identically equal to \(0\)). Let \(r>0\) (with \(r=2\) the most important case). Prove that the following are equivalent (you should use the difficult (26) for (30)):

\[{\rm E}|X|^{r}<\infty. \tag{27}\]

\[M_{rn}\equiv\ [\frac{1}{n}\max_{1\leq k\leq n}|X_{k}|^{r}]\rightarrow_{a. s.}0. \tag{28}\]

\[{\rm E}M_{rn}\to 0. \tag{29}\]

\[M_{rn}/[\frac{1}{n}{\sum_{1}}^{n}|X_{k}|^{r}]\rightarrow_{a.s.}0. \tag{30}\]

(ii) When \(r=2\), we may add the equivalent condition (by (6.6.6))

\[{\cal D}_{n}^{2}\equiv\ [\frac{1}{n}\max_{1\leq k\leq n}(X_{k}-\bar{X}_{n})^{2} ]/[\frac{1}{n}{\sum_{k=1}}^{n}(X_{k}-\bar{X}_{n})^{2}]\rightarrow_{a.s.}0. \tag{31}\]

(iii) Again (by (8.2.18)), \({\rm E}|X|^{r}<\infty\) if and only if \({\rm E}|X-X^{\prime}|^{r}<\infty\).

**Exercise 4.21** (Negibility, in probability) Let \(X_{n1},\ldots,X_{nn}\) be iid \(F\), for \(n\geq 1\). Let \(X\cong F\). Let \(r>0\). Prove that the following are equivalent:

\[yP(|X|^{r}>y)\to 0\] as \[y\rightarrow\infty. \tag{32}\]

\[x^{r}P(|X|>x)\to 0\] as \[x\rightarrow\infty. \tag{33}\]

\[x^{r}P(|X-X^{\prime}|>x)\to 0.\] as \[x\rightarrow\infty,\mbox{ here $X$ and$X^{\prime}$ are iid $F$}. \tag{34}\]

(35) \[x^{r}P(|X-(\mbox{some `$a$'})|>x)\to 0.\] Then any '\[a\] 'works; so \[{\rm med}(X)\] works.

(36) \[M_{rn}\equiv[\frac{1}{n}\max_{1\leq k\leq n}|X_{nk}|^{r}]\rightarrow_{p}0.\] (Any \[|X_{nk}-a|^{r}\] may replace \[|X_{nk}|^{r}\].)

\[{\rm E}M_{rn}^{\alpha}={\rm E}[\frac{1}{n^{\alpha}}\max_{1\leq k\leq n}|X_{nk} |^{r\alpha}]\to 0\qquad\qquad\mbox{ for all $0<\alpha<1$}. \tag{37}\]

In case \(r>1\) (and especially for \(r=2\)) add to this list the equivalent condition

\[{\rm E}M_{rn}^{1/r}={\rm E}[\frac{1}{n^{1/r}}\max_{1\leq k\leq n}|X_{nk}|] \to 0. \tag{38}\]

Because (34) is included in this list, the iid \(X_{nk}\)'s may be replaced by iid symmetrized \(X_{nk}^{s}\)'s in (32), (33), and (36)-(38). Moreover

\[{\rm E}|X|^{p}<\infty\qquad\mbox{for all $0<p<r$ whenever (\ref{eq:M_rn}) holds.} \tag{39}\]

The remaining problems in this subsection are mainly quite substantial. They are here for "flavor." Some also make good exercises for section 8.8. (Some of the martingale inequalities found in section 8.10 should prove useful (here and below).)

**Exercise 4.22*** (Kesten) Let \(X_{1},X_{2},\ldots\) be iid as \(X\geq 0\), with \({\rm E}|X|=\infty\). Then

\[\overline{\lim}\,\frac{X_{n}}{S_{n-1}}=\infty\mbox{ a.s.} \tag{40}\]

[MISSING_PAGE_FAIL:188]

**Exercise 4.29*** (More general WLLN) Let \(X_{n1},\ldots,X_{nn}\) be independent, and set \(S_{n}\equiv X_{n1}+\cdots+X_{nn}\). Truncate via \(Y_{nk}\equiv X_{nk}\times 1_{[|X_{nk}|\leq b_{n}]}\), for some \(b_{n}>0\) having \(b_{n}\nearrow\infty\). Let \(\mu_{nk}\) and \(\sigma_{nk}^{2}\) denote the mean and variance of \(Y_{nk}\). Then

\[S_{n}/b_{n}\to_{p}0 \tag{50}\]

if and only if we have all three of

\[\sum_{1}^{n}P(|X_{nk}|>b_{n})\to 0,\qquad\sum_{1}^{n}\mu_{nk}/b_{n}\to 0,\qquad\mbox{and}\qquad\sum_{1}^{n}\sigma_{nk}^{2}/b_{n}^{2}\to 0. \tag{51}\]

[The converse is a substantial problem.]

## 5 Applications of the Laws of Large Numbers

Let \(X_{1},X_{2},\ldots\) be iid \(F\). Let \(\mathbb{F}_{n}\) denote the empirical df of \(X_{1},\ldots,X_{n}\), given by

\[\mathbb{F}_{n}(x)\equiv\mathbb{F}_{n}(x,\omega)\equiv\frac{1}{n}\sum_{k=1}^{n}1_ {(-\infty,x]}(X_{k}(\omega))=\frac{1}{n}\sum_{k=1}^{n}1_{[X_{k}\leq x]}\quad \mbox{for all real $x$.} \tag{1}\]

**Theorem 5.1** (Glivenko-Cantelli): We have

\[\|\mathbb{F}_{n}-F\|\equiv\sup_{-\infty<x<\infty}\|\mathbb{F}_{n}(x)-F(x)| \to_{a.s.}\ 0\qquad\mbox{as $n\to\infty$.} \tag{2}\]

[This is a uniform SLLN for the random function \(\mathbb{F}_{n}(\cdot)\).]

**Proof.** Let \(X_{k}=F^{-1}(\dot{\xi}_{k})\) for \(k\geq 1\) be iid \(F\), with the \(\dot{\xi}_{k}\)'s iid Uniform (0, 1). Let \(\mathbb{G}_{n}\) denote the empirical df of the first \(n\) of these \(\dot{\xi}_{k}\)'s, and let \(\mathbb{F}_{n}\) denote the empirical df of the first \(n\) of these \(X_{k}\)'s. Let \(I\) denote the identity function. Then

\[(\mathbb{F}_{n}-F)=[\mathbb{G}_{n}(F)-I(F)]\qquad\mbox{on $(-\infty,\ \infty)$}\qquad\mbox{for every}\,\omega \tag{3}\]

by (6.3.4). Thus by theorem 5.3.3, it will suffice to prove the result in the special case of uniform empirical df's \(\mathbb{G}_{n}\)'s. (Recall the remark in bold above (6.4.3) that the representation of \(X\) as \(F^{-1}(\xi)\) allows alternative ways to approach problems. Moreover, using the \(\dot{\xi}_{k}\)'s of (6.3.8) gives us back the original \(X_{k}\)'s.)

Now, \(\mathbb{G}_{n}(k/M)-k/M\to_{a.s.}0\) as \(n\to\infty\) for \(0<k\leq M\) by the SLLN applied to the iid Bernoulli \((k/M)\) rv's \(1_{[0,k/M]}(\dot{\xi}_{i})\). We now assume that \(M\) is so large that \(1/M<\epsilon\). Then for \((k-1)/M\leq t\leq k/M\), with \(1\leq k\leq M\), we have both

\[\mbox{(a)}\qquad\mathbb{G}_{n}(t)-t\leq\mathbb{G}_{n}(\frac{k}{M})-\frac{k-1}{ M}\leq\mathbb{G}_{n}(\frac{k}{M})-\frac{k}{M}+\frac{1}{M}\qquad\mbox{and}\] \[\mbox{(b)}\qquad\mathbb{G}_{n}(t)-t\geq\mathbb{G}_{n}(\frac{k-1}{ M})-\frac{k}{M}\geq\mathbb{G}_{n}(\frac{k-1}{M})-\frac{k-1}{M}-\frac{1}{M}.\]

These combine to give

\[\mbox{(c)}\qquad\sup_{0\leq t\leq 1}|\mathbb{G}_{n}(t)-t|\leq[\max_{0 \leq k\leq M}|\mathbb{G}_{n}(\frac{k}{M})-\frac{k}{M}|]+\frac{1}{M}\] \[\qquad\to_{a.s.}\ 0+1/M<\epsilon.\]

Since \(\epsilon>0\) is arbitrary, we have shown that \(\sup_{0\leq t\leq 1}|\mathbb{G}_{n}(t)-t|\to_{a.s.}0\). That is,

\[\|\mathbb{F}_{n}-F\|=\|\mathbb{G}_{n}(F)-F\|\leq\|\mathbb{G}_{n}-I\|\to_{a.s.} \ 0, \tag{4}\]

as claimed. \(\Box\)

**Exercise 5.1** Let \(\xi_{n1},\ldots,\xi_{nn}\) denote any row independent Uniform \((0,1)\) rvs, and let all \(X_{nk}=F^{-1}(\xi_{nk})\) for a fixed df \(F\). Let \(\mbox{F}_{n}\) and \(\mathbb{G}_{n}\) denote the empirical dfs of the \(n\)th rows of these two arrays. Show that (2) still holds.

**Example 5.1** (Weierstrass approximation theorem): If \(f\) is continuous on \([0,1]\), then there exist polynomials \(B_{n}\) such that \(\|B_{n}-f\|=\sup_{0\leq t\leq 1}|B_{n}(t)-f(t)|\to 0\) as \(n\to\infty\).

**Proof.** (Bernstein) Define the _Bernoulli polynomials_

\[B_{n}(t)=\sum_{k=0}^{n}f\left(\frac{k}{n}\right)\left(\begin{array}{c}n\\ k\end{array}\right)t^{k}(1-t)^{n-k}\qquad\mbox{for $0\leq t\leq 1$} \tag{5}\](a) \(={\rm E}f(T/n)\) where \(T\cong{\rm Binomial}(n,t)\).

Since \(f\) is continuous, \(f\) is bounded by some \(M\), and \(f\) is uniformly continuous on \([0,1]\) having \(|f(x)-f(y)|<\epsilon\) whenever \(|x-y|<\delta_{\epsilon}\). Then

\[|f(t)-B_{n}(t)|=|\sum_{k=0}^{n}[f(t)-f(k/n)]{n\choose k}t^{k}(1-t)^ {n-k}|\] \[\leq|\sum_{\{k:|k/n-t|<\delta_{\epsilon}\}}\ {\rm same}|+|\sum_{\{k: |k/n-t|\geq\delta_{\epsilon}\}}\ {\rm same}|\] (b) \[<\epsilon+2MP(|T/n-t|\geq\delta_{\epsilon})\] by uniform continuity of \[f\] \[\leq\epsilon+2Mt(1-t)/n\delta_{\epsilon}^{2}\] for all

\[t\]

 (by Chebyshev) \[\leq\epsilon+2M/4n\delta_{\epsilon}^{2}\leq 2\epsilon\] for

\[n\geq\]

 some

\[N_{\epsilon}\]

, \[\qquad\hbox{ for all }0\leq t\leq 1\]

As the choice of \(N_{\epsilon}\) does not depend on \(t\), the convergence is uniform. Note that this is just an application of a weak form of the WLLN (that is, of the Chebyshev inequality). \(\Box\)

**Example 5.2** (Borel's normal numbers) : A number \(x\) in \([0,1]\) is called _normal to base \(d\)_ if when expanded to base \(d\), the fraction of each of the digits \(0,\ldots,d-1\) converges to \(1/d\). The number is _normal_ if it is normal to base \(d\) for each \(d>1.\) We are able to conclude that

a.e. number in \([0,1]\) is normal with respect to Lebesgue measure \(\lambda\).

[Comment: \(\frac{1}{3}=0.010101\ldots\) in base 2 is normal in base 2, but \(\frac{1}{3}=0.1000\ldots\) in base 3 is not normal in base 3.] [This was a historically important example, which spurred some of the original development.]

**Proof.** : Let \((\Omega,\ {\cal A},\ P)=([0,1],\ {\cal B}\cap[0,\ 1],\ \lambda)\). Let

(a) \(\omega=\sum_{n=1}^{\infty}\beta_{n}(\omega)/d^{n}\) define rvs

\[\beta_{1},\beta_{2},\ldots\]

Note that the \(\beta_{n}\)'s are iid discrete uniform on \(0,1,\ldots,d-1\). Thus, letting \(\eta_{nk}=1\) or \(0\) according as \(\beta_{n}=k\) or \(\beta_{n}\neq k\), we have

(b) \(\lambda(A_{d,k})\equiv\lambda(\{\omega:n^{-1}\mbox{$\sum_{j=1}^{n}\eta_{jk} \to 1/d\})=1\)

by the SLLN. Thus \(A_{d}\equiv\bigcap_{k=0}^{d-1}A_{d,k}\) has \(\lambda(A_{d})=1\); that is, a.e. \(\omega\) in \([0,1]\) is normal to base \(d\). Then trivially, \(A\equiv\bigcap_{d=1}^{\infty}A_{d}\) has \(\lambda(A)=1\). And so, a.e. \(\omega\) in \([0,1]\) is normal. \(\Box\)

**Example 5.3** : (SLLN for random sample size) Let \(N_{n}\) be positive-integer valued rvs for which \(N_{n}/n\rightarrow_{a.s.}c\in(0,\ \infty)\), and let \(X_{1},X_{2},\ldots\) be iid with mean \(\mu\).

(a) Then

(b) If \(X_{1},X_{2},\ldots\) are iid Bernoulli\((p)\) and \(N_{n}(\omega)\equiv\min\{k:S_{k}(\omega)=n\}\), then the waiting times \(N_{n}\) satisfy \(N_{n}/n\rightarrow_{a.s.}1/p\).

**Proof.** : (a) Now, \(S_{n}/n\rightarrow_{a.s}\mu\) by the SLLN, and thus \(N_{n}\rightarrow_{a.s.}\infty\) implies \(S_{N_{n}}/N_{n}\rightarrow_{a.s.}\mu\). Thus

\[S_{N_{n}}/n=(S_{N_{n}}/N_{n})(N_{n}/n)\rightarrow_{a.s.}\ \mu\cdot c,\]

using \(N_{n}\rightarrow_{a.s.}\infty\) by \(c>0\).

(b) We also have (since \(\mu=p\))

\[1=S_{N_{n}}/n=(S_{N_{n}}/N_{n})(N_{n}/n),\quad\mbox{so}\quad N_{n}/n=1/(S_{N_{ n}}/N_{n})\rightarrow_{a.s.}1/p,\]completing the proof. Note that we could also view \(N_{n}\) as the sum of \(n\) iid Geometric(\(p\)) rvs, and then apply the SLLN. \(\Box\)

**Exercise 5.2** (Monte Carlo estimation)  Let \(h:[0,1]\to[0,1]\) be continuous.

(i) Let \(X_{k}\equiv 1_{[h(\xi_{k})\geq\Theta_{k}]}\), where \(\xi_{1},\xi_{2},\ldots\Theta_{1},\Theta_{2},\ldots\) are iid Uniform \((0,1)\) rvs. Show that this sample average is a strongly consistent estimator of the integral; that is, show that \(\bar{X}_{n}\to_{a.s.}\int_{0}^{1}h(t)\,dt\).

(ii) Let \(Y_{k}\equiv h(\xi_{k})\). Show that \(\bar{Y}_{n}\to_{a.s.}\int_{0}^{1}h(t)\,dt\).

(iii) Evaluate \(\mbox{Var}[\bar{X}_{n}]\) and \(\mbox{Var}[\bar{Y}_{n}]\), and compare them.

## 6 Law of the Iterated Logarithm

**Theorem 6.1** (LIL; Hartman-Wintner; Strassen): Let \(X_{1},X_{2},\ldots\) be iid rvs. Consider the partial sums \(S_{n}\equiv X_{1}+\cdots+X_{n}\).

(a) If \(\mbox{\rm E}X=0\) and \(\sigma^{2}\equiv\mbox{\rm Var}[X]<\infty\), then

\[\lim\sup_{n\to\infty}\frac{S_{n}}{\sqrt{2n\log\log n}}=\sigma\mbox{ a.s.},\quad\mbox{while }\lim\inf_{n\to\infty}\frac{S_{n}}{\sqrt{2n\log\log n}}=-\sigma\mbox{ a.s.}\]

(b) In fact,

\[\frac{S_{n}}{\sqrt{2n\log\log n}}\leadsto_{a.s.}[-\sigma,\ \sigma].\]

[That is, for a.e. \(\omega\) the limit set of \(S_{n}/\sqrt{2n\log\log n}\) is exactly \([-\sigma,\sigma]\) ].

(c) Conversely, if

\[\overline{\lim}\frac{|S_{n}|}{\sqrt{2n\log\log n}}<\infty\mbox{ a.s.},\qquad \mbox{then}\qquad\mbox{\rm E}X=0\mbox{ and }\sigma^{2}<\infty.\]

**Theorem 6.2** (The other LIL; Chung): If \(X_{1},X_{2},\ldots\) are iid \((0,\ \sigma^{2})\), then

\[\underline{\lim}\max_{1\leq k\leq}\sqrt{2\log\log n}\frac{|S_{k}|}{\sqrt{n} \sigma}=\pi/2\mbox{ a.s.}\]

[We state this for fun only, as it has seen little application.]

Versions of both theorems are also known for cases other than iid. The classical proof of theorem 6.1 in full generality begins with truncation, and then carefully uses exponential bounds for bounded rvs. A more modern proof relies upon Skorokhod embedding of the partial sum process in Brownian motion. This general proof is outlined in the straightforward exercise 12.8.2, after embedding is introduced. But the proof below for the special case of normal rvs contains several of the techniques used in the classical proof of the general case (and in other related problems). And it is also a crucial component of the general case in exercise 12.8.2.

**Proposition 6.1**: Let \(Z_{1},Z_{2},\ldots\) be iid \(N(0,1)\) rvs. Let \(S_{n}\equiv Z_{1}+\cdots+Z_{n}\) and \(b_{n}\equiv\sqrt{2\log\log n}\). Then \(\limsup_{n\to\infty}S_{n}/\sqrt{n}b_{n}=1\) a.s.

**Proof.** Let \(\epsilon>0\). We will use the exponential bound

\[\exp[-(1+\epsilon)\lambda^{2}/2]\leq P(S_{n}/\sqrt{n}\geq\lambda)\leq\exp[-(1 -\epsilon)\lambda^{2}/2]\qquad\mbox{for all}\quad\lambda>\lambda_{\epsilon}\]

(for some \(\lambda_{\epsilon}\)) [see Mills' ratio exercise 6.1 below], and the Levy maximal inequality

\[\mbox{(a)}\qquad P(\max_{1\leq k\leq n}S_{k}\geq\lambda)\leq 2P(S_{n}\geq \lambda)\qquad\mbox{for all}\quad\lambda>0.\]

Let \(n_{k}\equiv[a^{k}]\) for \(a>1\); a sufficiently _small_\(a\) will be specified below. Now,

\[A_{k} \equiv\bigcup_{n_{k-1}\leq m\leq n_{k}}[S_{m}\geq\sqrt{m}(1+2 \epsilon)b_{m}]\] \[\subset\left[\max_{n_{k-1}\leq m\leq n_{k}}S_{m}\geq(1+2\epsilon )\sqrt{\frac{n_{k-1}}{n_{k}}}b_{n_{k-1}}\sqrt{n_{k}}\right],\]since \(\sqrt{n}\) is \(\nearrow\) and \(b_{n}\) is \(\nearrow\); so that for \(k\) sufficiently large,

(c) \[P(A_{k})\leq 2P\left(S_{n_{k}}/\sqrt{n_{k}}\geq(1+2\epsilon)\sqrt{ \frac{n_{k-1}}{n_{k}}}b_{n_{k-1}}\right)\qquad\mbox{by (a)}\] \[\qquad\leq 2\exp\left(-\frac{1}{2}(1-\epsilon)(1+2\epsilon)^{2} \frac{1-\epsilon}{a}2\log k\right)\qquad\qquad\mbox{by (5)}\] \[\qquad\leq 2\exp(-(1+\epsilon)\log k)=2/k^{1+\epsilon}\qquad\qquad \mbox{for $a$ sufficiently close to $1$}\] (d) \[=(k\mbox{th term of a convergent series}).\]

Thus \(P(A_{k}\mbox{ i.o.})=0\) by Borel-Cantelli. Since \(\epsilon>0\) is arbitrary, we thus have

(e) \[\limsup_{n\to\infty}\ \frac{S_{n}}{\sqrt{n}b_{n}}\leq 1\mbox{ a.s.}\]

Since (e) is true, on any subsequence \(n_{k}\to\infty\) we can claim that

(f) \[P(A_{k}\mbox{ i.o.})=0,\qquad\mbox{including when $n_{k}\equiv[a^{k}]$ for $a$ huge}.\]

We must now show that the \(\overline{\lim}\) in (e) is also \(\geq 1\) a.s. We will still use \(n_{k}\equiv[a^{k}]\), but \(a\) will be specified sufficiently _large_ below. We write \(S_{n_{k}}=S_{n_{k-1}}+(S_{n_{k}}-S_{n_{k-1}})\), so that

(g) \[\frac{S_{n_{k}}}{\sqrt{n_{k}}b_{n_{k}}}=\sqrt{\frac{n_{k-1}}{n_{k} }}\left(\frac{b_{n_{k-1}}}{b_{n_{k}}}\right)\frac{S_{n_{k-1}}}{\sqrt{n_{k-1}}b _{n_{k-1}}}+\frac{S_{n_{k}}-S_{n_{k-1}}}{\sqrt{n_{k}}b_{n_{k}}}\] (h) \[\qquad\sim\frac{1}{\sqrt{a}}\cdot 1\cdot\frac{S_{n_{k-1}}}{\sqrt{n_{ k-1}}b_{n_{k-1}}}+\frac{S_{n_{k}}-S_{n_{k-1}}}{\sqrt{n_{k}}b_{n_{k}}}.\]

Now, the independent events

(i) \[B_{k}\equiv[S_{n_{k}}-S_{n_{k-1}}\geq(1-2\epsilon)\sqrt{n_{k}}b_{n_{k}}]= \left[\frac{S_{n_{k}}-S_{n_{k-1}}}{\sqrt{n_{k}}-n_{k-1}}\geq\frac{(1-2 \epsilon)\sqrt{n_{k}}b_{n_{k}}}{\sqrt{n_{k}}-n_{k-1}}\right]\]

have

(j) \[P(B_{k})\geq\exp\left(-\frac{1}{2}(1+\epsilon)(1-2\epsilon)^{2} \frac{n_{k}}{n_{k}-n_{k-1}}b_{n_{k}}^{2}\right)\qquad\qquad\mbox{by (5)}\] \[\qquad\geq\exp(-\frac{1}{2}(1+\epsilon)(1-2\epsilon)^{2}\frac{(1+ \epsilon)a}{a-1}2\log k)\] \[\qquad\geq\exp(-(1-\epsilon)\log k)\qquad\qquad\mbox{ for $a$ sufficiently large}\]

(k) \[=1/k^{1-\epsilon}=(k\mbox{th term of a series with an infinite sum}),\]

so that \(P(B_{k}\mbox{ i.o.})=1\) by the second Borel-Cantelli lemma. But \(P(A_{k}\mbox{ i.o.})=0\) and \(P(B_{k}\mbox{ i.o.})=1\) means that

(l) \[P(A_{k}^{c}\cap B_{k}\mbox{ i.o.})=1.\]

Moreover, on \(A_{k}^{c}\cap B_{k}\) we have, using (h), (i), and the symmetric version of (f),

(m) \[\frac{S_{n_{k}}}{\sqrt{n_{k}}b_{n_{k}}}\geq-\frac{(1+2\epsilon)(1+\epsilon)}{ \sqrt{a}}+(1-2\epsilon)\geq(1-3\epsilon)\]

for the constant \(a\) specified sufficiently large. Thus, even focusing only on the subsequence \(n_{k}\) in (f) with this large \(a\equiv a_{\epsilon}\), since \(\epsilon>0\) was arbitrary,

(n) \[\limsup_{n\to\infty}\frac{S_{n_{k}}}{\sqrt{n_{k}}b_{n_{k}}}\geq 1\mbox{ a.s.}\]

Combining (e) and (n) gives the proposition.

**Exercise 6.1** (Mills' ratio): Show that for all \(\lambda>0\)

\[\frac{\lambda}{\lambda^{2}+1}\frac{1}{\sqrt{2\pi}}\exp(-\lambda^{2}/2)<P(N(0,1)> \lambda)<\frac{1}{\lambda}\frac{1}{\sqrt{2\pi}}\exp(-\lambda^{2}/2), \tag{6}\]

which can be rewritten as

\[\frac{\lambda}{\lambda^{2}+1}\phi(\lambda)<1-\Phi(\lambda)<\frac{1}{\lambda} \phi(\lambda) \tag{7}\]

where \(\phi\) and \(\Phi\) denote the standard normal \(N(0,1)\) density and df, respectively. Show (5) follows from this. This is the end of this exercise.

\((*)\) For a standardized rv \(Z_{n}\), one might then _hope_ that as \(\lambda_{n}\to\infty\)

\[\exp(-(1+\epsilon_{n})\lambda_{n}^{2}/2)\leq P(Z_{n}\geq\lambda_{n})\leq\exp(- (1-\epsilon_{n})\lambda_{n}^{2}/2), \tag{8}\]

as was applied in (5). [This clean exponential bound for normal rvs was the key to the simple LIL proof in proposition 6.1. The classic Hartman-Wintner proof uses truncation to achieve a reasonable facsimile of this in other cases.]

\((*)\) (Ito-McKean) It is even true that for all \(\lambda>0\) there are the tighter bounds

\[\frac{2}{\sqrt{\lambda^{2}+4}+\lambda}\phi(\lambda)<1-\Phi(\lambda)<\frac{2}{ \sqrt{\lambda^{2}+2}+\lambda}\phi(\lambda). \tag{9}\]

**Exercise 6.2**: In place of (c) in the LIL proof of proposition 6.1, use Mills' ratio to bound \(P(A_{n})\equiv P(S_{n}/\sqrt{n}\geq(1+2\epsilon)\sqrt{2\log n})\). Use that bound directly to show that \(\limsup|S_{n}|/\)\((\sqrt{n}\sqrt{2\log n})\leq 1\,\)a.s. [This "poor" result will show the value of using the "block of indices" in the definition of \(A_{k}\) in the proof we gave.]

**Exercise 6.3**: Let arbitrary events \(A_{n}\) and \(B_{n}\) satisfy \(P(A_{n}\mbox{ i.o.})=0\) and \(P(B_{n}\mbox{ i.o.})=1\). Show that \(P(A_{n}^{c}\cap B_{n}\mbox{ i.o.})=1\) (as in (1) above).

**Summary**: Suppose \(X,X_{1},X_{2},\ldots\) are iid \((\mu,\ 1)\). Then:

\[\frac{\sum_{k=1}^{n}(X_{k}-\mu)}{n}\to_{a.s.}0\qquad\qquad\mbox{ by the SLLN}. \tag{11}\] \[\frac{\sum_{k=1}^{n}(X_{k}-\mu)}{n^{1/r}}\to_{a.s.}0\quad\mbox{ for all }1\leq r<2,\mbox{ by Marcinkiewicz-Zygmund}.\] (12) \[\frac{\sum_{k=1}^{n}(X_{k}-\mu)}{\sqrt{2n\log\log n}}\rightsquigarrow_{a.s.}[-1,1]\qquad\mbox{by the LIL}. \tag{10}\]

Suppose we go all the way to \(\sqrt{n}\) in the denominator. Then the classical CLT gives

\[\frac{\sum_{k=1}^{n}(X_{k}-\mu)}{\sqrt{n}}\to_{d}N(0,1)\qquad\mbox{ by the CLT}, \tag{13}\]

even though we have divergence to \(\pm\infty\) for a.e. \(\omega\) (by the LIL). \(\Box\)

**Exercise 6.4** (\(r\)th mean convergence theorem): Let \(X,X_{1},X_{2},\ldots\) be iid, and consider the partial sums \(S_{n}\equiv X_{1}+\cdots+X_{n}\). Let \(0<r<2\) (and suppose \(\mbox{E}X=0\) in case \(1\leq r<2\)). The following are equivalent:

(a) \(\mbox{E}|X|^{r}<\infty\).

(b) \(S_{n}/n^{1/r}\to_{a.s.}0\).

(c) \(\mbox{E}|S_{n}|^{r}=o(n)\).

(d) \(\mbox{E}(\max_{1\leq k\leq n}|S_{k}|^{r})=o(n)\).

Hint. For (a) and (b) imply (c), use the Hoffmann-Jorgensen (8.10.6) below.

## 7 Strong Markov Property for Sums of IID RVs

Let \(X_{1},X_{2},\ldots\) be iid and let \(S_{n}\equiv X_{1}+\cdots+X_{n}\). Let \({\rm S}\equiv(S_{1},S_{2},\!\!\ldots)\).

**Definition 7.1**: The integer valued rv \(N\) is a _stopping time_ for the sequence of rvs \(S_{1},S_{2},\ldots\) if \([N=k]\in{\cal F}(S_{1},\ \ldots,\ S_{k})\) for all \(k\geq 1\). It is elementary that

\[\mathcal{F}_{N}\equiv\mathcal{F}(S_{k}:k\leq N) \tag{1}\] \[\equiv\{A\in\mathcal{F}({\rm S}):A\cap[N=k]\in\mathcal{F}(S_{1}, \ldots,S_{k})\mbox{ for all }k\geq 1\}=({\rm a}\,\sigma\mbox{-field}), \tag{2}\]

since it is clearly closed under complements and countable intersections. (Clearly, \([N=k]\) can be replaced by \([N\leq k]\) in the definition of \(\mathcal{F}_{N}\) in (2).)

**Proposition 7.1**: Both \(N\) and \(S_{N}\) are \(\mathcal{F}_{N}\)-measurable.

**Proof.** Now, to show that \([N\leq m]\in\mathcal{F}_{N}\) we consider \([N\leq m]\cap[N=k]\) equals \([N=k]\) or \(\emptyset\), both of which are in \(\mathcal{F}({\rm S})\); this implies \([N\leq m]\in\mathcal{F}_{N}\). Likewise,

\[({\rm a})\qquad[S_{N}\leq x]\cap[N=k]=[S_{k}\leq x]\cap[N=k]\in\mathcal{F}(S_{ 1},\ \ldots,\ S_{k}),\]

implying that \([S_{N}\leq x]\in\mathcal{F}_{N}\). \(\Box\)

**Theorem 7.1** (The strong Markov property): If \(N\) is a stopping time, then the increments continuing from the random time

\[\tilde{S}_{k}\equiv S_{N+k}-S_{N},\ k\geq 1, \tag{3}\]

have the same distribution on \((R_{\infty},\ \mathcal{B}_{\infty})\) as does \(S_{k},k\geq 1\). Moreover, defining \(\tilde{S}\equiv(\tilde{S}_{1},\tilde{S}_{2},\ \ldots)\),

\[\mathcal{F}(\tilde{S})\equiv\mathcal{F}(\tilde{S}_{1},\tilde{S}_{2},\ \ldots) \mbox{ is independent of }\mathcal{F}_{N}\mbox{ (hence of }N\mbox{ and }S_{N}). \tag{4}\]

**Proof.** Let \(B\in B_{\infty}\) and \(A\in\mathcal{F}_{N}\). Now,

\[({\rm a}) P([\tilde{S}\in B]\cap A)=\sum_{n=1}^{\infty}P([\tilde{S}\in B]\cap A \cap[N=n])\] \[=\sum_{n=1}^{\infty}P([(S_{n+1}-S_{n},\ S_{n+2}-S_{n},\ \ldots)\in B] \cap(A\cap[N=n]))\] \[\mbox{ with }A\cap[N=n]\in\mathcal{F}(S_{1},\ \ldots,\ S_{n})\] \[=\sum_{n=1}^{\infty}P([(S_{n+1}-S_{n},\ S_{n+2}-S_{n},\ \ldots) \in B])P(A\cap[N=n])\] \[=P({\rm S}\in B)\!\sum_{n=1}^{\infty}\!P(A\cap[N=n])\] \[({\rm b}) =P({\rm S}\in B)P(A).\]

Set \(A=\Omega\) in (b) to conclude that \(\tilde{S}\cong{\rm S}\). Then use \(P(\tilde{S}\in B)=P({\rm S}\in B)\) to rewrite (b) as

\[P([\tilde{S}\in B]\cap A)=P(\tilde{S}\in B)P(A),\] (c)

which is the statement of independence. \(\Box\)

**Exercise 7.1** (Manipulating stopping times): Let \(N_{1}\) and \(N_{2}\) denote stopping times relative to an \(\nearrow\) sequence of \(\sigma\)-fields \(\mathcal{A}_{1}\subset\mathcal{A}_{2}\subset\cdots\). Show that \(N_{1}\wedge N_{2},N_{1}\lor N_{2},N_{1}+N_{2}\), and \(N_{o}\equiv i\) are all stopping times.

**Definition 7.2** Define _waiting times_ for return to the origin by

\[\begin{array}{lcl}W_{1}\equiv\min\{n:S_{n}=0\}&\mbox{ with }W_{1}=+\infty&\mbox{ if the set is empty,}\\ \vdots&\vdots&\vdots\\ W_{k}\equiv\min\{n>W_{k-1}:S_{n}=0\}&\mbox{ with }W_{k}=+\infty&\mbox{ if the set is empty.}\end{array}\]

Then define \(T_{k}\equiv W_{k}-W_{k-1}\), with \(W_{0}\equiv 0\), to be the _interarrival times_ for return to the origin.

**Proposition 7.2** If \(P(S_{n}=0\mbox{ i.o.})=1\), then \(T_{1}\,T_{2},\ldots\) are well-defined rvs and are, in fact, iid.

**Proof.** Clearly, each \(W_{k}\) is always an extended-valued rv, and the requirement \(P(S_{n}=0\mbox{ i.o.})=1\) guarantees that \(W_{k}(\omega)\) is well-defined for all \(k\geq 1\) for a.e. \(\omega\).

Now, \(T_{1}=W_{1}\) is clearly a stopping time. Thus, by the strong Markov property, \(T_{1}\) is independent of the rv \(\tilde{S}^{(1)}\equiv\tilde{S}\) with \(k\)th coordinate \(\tilde{S}^{(1)}_{k}\equiv\tilde{S}_{k}\equiv S_{T_{1}+k}-S_{T_{1}}\) and \(\tilde{S}^{(1)}\equiv\tilde{S}\cong\tilde{S}\). Thus \(T_{2}\) is independent of the rv \(\tilde{S}^{(2)}\) with \(k\)th coordinate \(\tilde{S}^{(2)}_{k}\equiv\tilde{S}^{(1)}_{T_{2}+k}-\tilde{S}^{(1)}_{T_{2}}=S_{ T_{1}+T_{2}+k}-S_{T_{1}+T_{2}}\) and \(\tilde{S}^{(2)}\cong\tilde{S}^{(1)}\cong\tilde{S}\). Continue with \(\tilde{S}^{(3)}\), etc. [Note the relationship to interarrival times of a Bernoulli process.] \(\Box\)

**Exercise 7.2** (Wald's identity) (a) Suppose \(X_{1},X_{2},\ldots\) are iid with mean \(\mu\), and \(N\) is a stopping time with finite mean. Show that \(S_{n}\equiv X_{1}+\cdots+X_{n}\) satisfies

\[\mbox{E}S_{N}=\mu\mbox{E}N. \tag{6}\]

(b) Suppose each \(X_{k}\) equals \(1\) or \(-1\) with probability \(p\) or \(1-p\) for some \(0<p<1.\) Then define the rv \(N\equiv\min\{n:S_{n}\mbox{ equals }-a\mbox{ or }b\}\), where \(a\) and \(b\) are strictly positive integers. Show that \(N\) is a stopping time that is a.s. finite. Then evaluate the mean E_N_. [Hint. \([N\geq k]\in{\cal F}(S_{1},\ldots,S_{k-1})\), and is thus independent of \(X_{k}\), while \(S_{N}=\sum_{k=1}^{\infty}X_{k}1_{[N\geq k]}\)

## 8 Convergence of Series of Independent RVs

In section 8.4 we proved the SLLN after recasting it (via Kronecker's lemma) as a theorem about a.s. convergence of infinite series. In this section we consider the convergence of infinite series directly. Since the convergence set of a series is a tail event (recall remark 7.2.1), convergence can happen only with probability \(0\) or \(1\). Moreover, the first theorem below seems to both limit the possibilities and broaden the possible approaches to them. All proofs are given at the end of this section.

**Theorem 8.1**  Let \(X_{1},X_{2},\ldots\) be independent. Then, for some rv \(S\), we have

\[S_{n}\equiv\sum_{k=1}^{n}X_{k}\rightarrow_{a.s.}S\quad\mbox{iff}\quad\sum_{k=1 }^{n}X_{k}\rightarrow_{p}S\quad\mbox{iff}\quad\sum_{k=1}^{n}X_{k}\rightarrow_{ d}S. \tag{1}\]

[We will show the first equivalence now, and leave the second until exercise 10.2.10.]

**Theorem 8.2** (The 2-series theorem): Let \(X_{1},X_{2},\ldots\) be independent rvs for which \(X_{k}\cong(\mu_{k},\ \sigma_{k}^{2})\). Let \(S_{n}\equiv\sum_{k=1}^{n}X_{k}\) and \(S_{0,n}\equiv\sum_{k=1}^{n}(X_{k}-\mu_{k})\). (a) Then

\[\sum_{k=1}^{n}\mu_{k}\rightarrow\mu\mbox{ and }\sum_{k=1}^{\infty}\sigma_{k}^{ 2}<\infty\quad\mbox{imply}\quad S_{n}\equiv\sum_{k=1}^{n}X_{k}\rightarrow_{a. s.}\mbox{ (some rv }S). \tag{2}\]

Of course, in this situation \(S_{0,n}\rightarrow_{a.s.}S_{0}\equiv S-\mu\). Moreover,

\[\mbox{E}S=\mu\equiv\sum_{k=1}^{\infty}\mu_{k}\quad\mbox{Var}[S]=\sigma^{2} \equiv\sum_{k=1}^{\infty}\sigma_{k}^{2},\quad\mbox{ and }\quad S_{n} \rightarrow_{\mathcal{L}_{2}}S. \tag{3}\]

(b) Further, if all \(|X_{k}|\leq\) (some \(c\)), then (including converses) both:

\[S_{0,n}\equiv\sum_{k=1}^{n}(X_{k}-\mu_{k})\rightarrow_{a.s.}\mbox{ (some rv }S_{0})\qquad\mbox{if and only if}\qquad\sum_{k=1}^{\infty}\sigma_{k}^{2}<\infty. \tag{5}\] \[S_{n}\equiv\sum_{k=1}^{n}X_{k}\rightarrow_{a.s.}\mbox{ (some rv }S)\qquad\mbox{iff}\qquad\sum_{k=1}^{n}\mu_{k}\rightarrow\mu\quad\mbox{and }\quad\sum_{k=1}^{\infty}\sigma_{k}^{2}<\infty. \tag{4}\]

If a series is to converge, the size of its individual terms must be approaching zero. Thus the rvs must be effectively bounded. Thus truncation should be particularly effective for demonstrating the convergence of series.

**Theorem 8.3** (The 3-series theorem): Let \(X_{1},X_{2},\ldots\) be independent rvs.

(a) Define \(X_{k}^{(c)}\) to be the trimmed \(X_{k}\) that equals \(X_{k}\) or \(0\) as \(|X_{k}|\leq c\) or as \(|X_{k}|>c.\) Then the series

\[S_{n}\equiv\sum_{k=1}^{n}X_{k}\rightarrow_{a.s.}\mbox{ (some rv }S) \tag{6}\]

if and only if for some \(c>0\) the following three series _all_ converge:

\[\mbox{I}_{c}\equiv\sum_{k=1}^{\infty}P(|X_{k}|>c),\qquad\Pi_{c}\equiv\sum_{k=1 }^{\infty}\mbox{Var}[X_{k}^{(c)}],\qquad\mbox{III}_{c}\equiv\sum_{k=1}^{ \infty}\mbox{E}X_{k}^{(c)}. \tag{7}\]

(b) The condition (7) holds for some \(c>0\) if and only if it holds for all \(c>0\).

(c) If either \(\mbox{I}_{c},\Pi_{c},\) or \(\mbox{III}_{c}\) diverges for any \(c>0\), then \(\sum_{k=1}^{n}X_{k}\) diverges a.s.

**Example 8.1** Suppose \(X_{1},X_{2},\ldots\) are independent and are uniformly bounded. They are assumed to be independent of the iid Rademacher rvs \(\epsilon_{1},\ldots,\epsilon_{n}\). Then

\[\sum_{k=1}^{n}\epsilon_{k}X_{k}\to_{a.s.}\mbox{ (some rv }S)\qquad\mbox{if and only if}\qquad\sum_{k=1}^{\infty}\sigma_{k}^{2}<\infty. \tag{8}\]

Moreover, \(S\cong(0,\ \sum_{k=1}^{\infty}\sigma_{k}^{2})\). [This is immediate from the 2-series theorem.] \(\square\)

**Exercise 8.1** Suppose \(X_{1},X_{2},\ldots\) are iid with \(P(X_{k}=0)=P(X_{k}=2)=\frac{1}{2}.\) Show that \(\sum_{k=1}^{n}X_{k}/3^{k}\to_{a.s.}\) (some \(S\)), and determine the mean, variance, and the name of the df \(F_{S}\) of \(S\). Also determine the characteristic function of \(S\) (at some point after chapter 9).

**Exercise 8.2** (a) Show that \(\sum_{k=1}^{n}a^{k}X_{k}\to_{a.s.}\) (some \(S\)) when \(X_{1},X_{2},\ldots\) are independent with \(X_{k}\cong\mbox{Uniform}(-k,k)\) for \(k\geq 1\), and where \(0<a<1\).

(b) Evaluate the mean and the variance (give a simple expression) of \(S\).

**Exercise 8.3** Let \(X_{1},X_{2},\ldots\) be arbitrary rvs with all \(X_{k}\geq 0\) a.s. Let \(c>0\) be arbitrary. Then \(\sum_{k=1}^{\infty}\mbox{E}(X_{k}\wedge c)<\infty\) implies that \(\sum_{k=1}^{n}X_{k}\to_{a.s.}\) (some rv \(S\)). The converse holds for independent rvs.

**Exercise 8.4** (a) Let \(Z_{1},Z_{2},\ldots\) be iid \(N(0,1)\) rvs. Show that

\(\sum_{k=1}^{\infty}[Z_{2k-1}^{2}+Z_{2k}^{2}]/2^{k}\to_{a.s.}\) (some rv),

\(\sum_{n=1}^{\infty}[\sum_{k=1}^{n}Z_{k}/2^{n+k}]\to_{a.s.}\) (some rv),

and determine (if possible) the mean, variance, and distribution of the limiting rvs.

(b) Let \(Y_{1},Y_{2},\ldots\) be iid Cauchy(0,1) rvs. Does \(\sum_{k=1}^{\infty}Y_{k}/2^{k}\to_{a.s.}\) (some rv)? If so, what is the distribution of the limit?

**Proofs \({}^{\mbox{\footnotesize o}}\)**

**Proof.** Consider theorem 8.1. Now, \(\to_{a.s.}\) always implies \(\to_{p}\). So we turn to the converse. Suppose \(S_{n}\to_{p}S\) (which is equivalent to \(S_{m}-S_{n}\to_{p}0\)). To establish \(S_{n}\to_{a.s.}\), it is enough to verify (2.3.7) that for all \(\epsilon>0\) and \(\theta>0\) we have

\[P(\max_{n\leq m\leq N}|S_{m}-S_{n}|\geq\epsilon)<\theta\qquad\mbox{ for all }n\geq\mbox{ (some }n_{\epsilon,\theta}). \tag{9}\]

But Ottaviani-Skorokhod's inequality 10.3.4 gives

(a) \[P(\max_{n\leq m\leq N}|S_{m}-S_{n}|\geq\epsilon)=P(\max_{n<m \leq N}|\sum_{n+1}^{m}X_{k}|\geq\epsilon)\] (b) \[\leq P(|\sum_{n+1}^{N}X_{k}|\geq\epsilon/2)/[1-\max_{n<m\leq N}P(| \sum_{n+1}^{m}X_{k}|>\epsilon/2)]\] (c) \[\leq P(|S_{N}-S_{n}|\geq\epsilon/2)/[1-\max_{n\leq m\leq N}P(|S_{ m}-S_{n}|>\epsilon/2)]\] (d) \[=o(1)/[1-o(1)]<\theta\qquad\mbox{for all }n,N\geq\mbox{ (some }n_{\epsilon,\theta}),\]

using \(S_{N}-S_{n}\to_{p}0\) for (d). Thus (9) holds, and \(S_{n}\to_{a.s.}\) (some rv \(S^{\prime}\)). The a.s. limit \(S^{\prime}\) equals \(S\) a.s. by proposition 2.3.4. \(\square\)

**Proof.** Consider theorem 8.2, part (a): We first verify (2). By theorem 8.1, to establish that \(S_{0,n}\to_{a.s.}\) (some \(S\)) we need only show that \(S_{0,m}-S_{0,n}\to_{p}0\). But this follows immediately from Chebyshev's inequality, since

\[P(|S_{0,m}-S_{0,n}|\geq\epsilon)\leq\mbox{Var}[S_{0,m}-S_{0,n}]/\epsilon^{2} \leq\sum_{n+1}^{\infty}\sigma_{k}^{2}/\epsilon^{2}<\epsilon\]for all sufficiently large \(n\). Thus (2) holds, since \(S_{n}=S_{0,n}+\mu_{n}\).

We next verify (3) that \(\mbox{Var}[S]=\mbox{Var}[S_{0}]=\sigma^{2}\) and \(\mbox{E}S=\mu\). Fatou gives

(e) \[\mbox{E}(S_{0}^{2})=\mbox{E}(\lim S_{0,n}^{2})=\mbox{E}(\varunderline{\lim}\,S_ {0,n}^{2})\leq\varunderline{\lim}\,\mbox{E}(S_{0,n}^{2})=\varunderline{\lim} \,\sum_{1}^{n}\sigma_{k}^{2}=\sigma^{2},\]

and \(\mbox{E}(S_{0}^{2})\geq\sigma^{2}\) since

(f) \[\mbox{E}(S_{0}^{2})=\mbox{E}\{(\sum_{1}^{n}(X_{k}-\mu_{k}))^{2}\}+\mbox{E}\{( \sum_{n+1}^{\infty}(X_{k}-\mu_{k}))^{2}\}\geq\mbox{E}(S_{0,n}^{2})\to\sigma^{2}\]

(as the two rvs are independent, the first has mean \(0\), and both have finite variance (as follows from (e)). Thus \(\mbox{E}(S_{0}^{2})=\sigma^{2}\). Inasmuch as both \(S_{0,n}\to_{a.s.}S_{0}\) and \(\mbox{E}(S_{0,n}^{2})\to\mbox{E}(S_{0}^{2})\), the Vitali theorem gives \(S_{0,n}\to_{\mathcal{L}_{2}}S_{0}\). Then exercise 3.5.1b and Vitali show that \(\mbox{E}(S_{0})=\lim\mbox{E}(S_{0,n})=\lim 0=0\). As \(S=S_{0}+\mu\), we have \(\mbox{E}S=\mbox{E}(S_{0}+\mu)=\mu\) and \(\mbox{Var}[S]=\mbox{Var}[S_{0}]=\mbox{E}(S_{0}^{2})=\sigma^{2}=\sum_{1}^{\infty }\sigma_{k}^{2}\). \(\Box\)

The proof of part (b) of the 2-series theorem above will require a converse of Kolmogorov's inequality that is valid for bounded rvs.

**Inequality 8.1** (Kolmogorov's other inequality): Consider independent zero-mean rvs \(X_{k}\), and set \(S_{k}\equiv X_{1}+\cdots+X_{k}\) for \(1\leq k\leq n\). Suppose \(|X_{k}|\leq\) (some \(M\)) \(<\infty\) for all \(k\). Then

\[P(\max_{\leq k\leq n}|S_{k}|\leq\lambda)\leq(\lambda+M)^{2}/{\sum_{k=1}^{n}} \sigma_{k}^{2}\qquad\mbox{for all $\lambda>0$}. \tag{10}\]

**Proof.** Let \(A_{k}\equiv[\max_{1\leq j<k}|S_{j}|\leq\lambda<|S_{k}|]\). Let \(M_{n}\equiv[\max_{1\leq k\leq n}|S_{k}|]\). We give another first passage argument. Thus

(a) \[\mbox{E}\bigg{\{}S_{n}^{2}1{\sum_{1}^{n}}A_{k}\bigg{\}}={\sum_{1}^{n}} \mbox{E}\{S_{n}^{2}1_{A_{k}}\}={\sum_{1}^{n}}\mbox{E}\{[S_{k}+(S_{n}-S_{k})]^{ 2}1_{A_{k}}\}\] \[\qquad={\sum_{1}^{n}}\{\mbox{E}(S_{k}^{2}1_{A_{k}})+2\cdot 0+P(A_{k}) \mbox{E}(S_{n}-S_{k})^{2}\}\quad\mbox{by independence}\] (b) \[\qquad\leq(\lambda+M)^{2}{\sum_{1}^{n}}P(A_{k})+{\sum_{1}^{n}}P(A_{k}){ \sum_{j=k+1}^{n}}\sigma_{j}^{2}\] (c) \[\qquad\leq\{(\lambda+M)^{2}+\mbox{Var}[S_{n}]\}(1-P(M_{n}\leq \lambda)),\]

where in step (b) we take advantage of \(|S_{k}|\leq|S_{k-1}|+|X_{k}|\leq\lambda+M\) on \(A_{k}\). We also note that

(d) \[\mbox{E}\{S_{n}^{2}1{\sum_{1}^{n}}A_{k}\}=\mbox{E}S_{n}^{2}-\mbox{E}\{S_{n}^{2} 1_{[M_{n}\leq\lambda]}\}\ \geq\mbox{Var}[S_{n}]-\lambda^{2}P(M_{n}\leq\lambda)\]

using \(|S_{n}|\leq\lambda\) on the event \([M_{n}\leq\lambda]\) to obtain (d). Combining (c) and (d) and doing algebra gives

(e) \[P(M_{n}\leq\lambda)=\frac{(\lambda+M)^{2}}{(\lambda+M)^{2}+\mbox{Var}[S_{n}]- \lambda^{2}}\ \leq\frac{(\lambda+M)^{2}}{\mbox{Var}[S_{n}]}.\]

**Proof.** Consider Theorem 8.2, part(b): Consider first the forward half of (4). Since \(S_{0,n}\to_{a.s.}\) (some rv \(S_{0}\)), for some sufficiently large \(\lambda\) we have

(a) \[0<P(\sup_{n}|S_{0,n}|\leq\lambda)\] \[\qquad=\lim_{N}P(\max_{1\leq n\leq N}|S_{0,n}|\leq\lambda)\quad \mbox{since measures are monotone}\](b) \(\leq\lim_{N}(\lambda+c)^{2}/{\sum_{1}^{N}\sigma_{k}^{2}}\) by Kolmogorov's other inequality 8.1 (c) \(=(\lambda+c)^{2}/{\sum_{1}^{\infty}\sigma_{k}^{2}}\).

Then (c) implies that \({\sum_{1}^{\infty}\sigma_{k}^{2}}<\infty\). Conversely, \({\sum_{1}^{\infty}\sigma_{k}^{2}}<\infty\) implies by (2) that \(S_{0,n}\rightarrow_{a.s.}\) (some rv \(S_{0}\)). So, both halves of (4) hold.

Consider (5). Again, (2) gives the converse half. Consider the forward half. Suppose that \(S_{n}\rightarrow_{a.s.}S\). The plan is first to symmetrize, so that we can use (4) to prove (5). Let \(X_{n}\)'s be independent, and independent of the \(X_{n}\)'s with \(X_{n}^{\prime}\cong X_{n}\); then \(X_{n}^{s}\equiv X_{n}-X_{n}^{\prime}\) denotes the symmetrized rv. Since \(\to a.s.\) depends only on the finite-dimensional distributions, the given fact that \(S_{n}\rightarrow_{a.s.}S\) implies that the rv \(S_{n}^{\prime}\equiv{\sum_{1}^{n}X_{k}^{\prime}}\rightarrow_{a.s.}\) (some rv \(S^{\prime}\)) \(\cong S\). We can thus claim that

(d) \(S_{n}^{s}\equiv{\sum_{1}^{n}}X_{k}^{s}\rightarrow_{a.s.}S^{s}\equiv S-S^{\prime}\).

Now, \(|X_{n}^{s}|\leq 2c\); thus (d) and (4) imply that \({\sum_{1}^{\infty}}\)Var\([X_{n}^{s}]<\infty\). Thus

(e) \({\sum_{1}^{\infty}\sigma_{n}^{2}}={\sum_{1}^{\infty}}\)Var\([X_{n}^{s}]/2<\infty\).

Now, (e) and (2) imply that \({\sum_{1}^{n}}(X_{k}-\mu_{k})\to a.s.\) (some rv \(S_{0}\)) with mean 0. Thus

(f) \({\sum_{1}^{n}}\mu_{k}=[{\sum_{1}^{n}}X_{k}]-[{\sum_{1}^{n}}(X_{k}-\mu_{k})] \rightarrow_{a.s.}S-S_{0}\).

Thus \(S=S_{0}+\mu\) with \(\mu\equiv{\sum_{1}^{\infty}}\mu_{k}\) convergent, and the forward half of (5) holds. \(\Box\)

**Proof.** Consider the 3-series theorem. Consider (a) and (b) in its statement: Suppose that the 3 series converge for at least one value of \(c\). Then II and III imply that \({\sum_{1}^{n}}X_{k}^{(c)}\rightarrow_{a.s.}\) by (2). Thus \({\sum_{1}^{n}}X_{k}\rightarrow_{a.s.}\) by proposition 8.2.1, since I \(<\infty\) implies that \(X_{1},X_{2},\ldots\) and \(X_{1}^{(c)},X_{2}^{(c)},\ldots\) are Khinchin equivalent sequences.

Suppose that \({\sum_{1}^{n}}X_{k}\rightarrow_{a.s.}\cdot\) Then for all \(c>0\) we have \(P(|X_{n}|>c\) i.o.) \(=0\), so that \(I<\infty\) holds for all \(c>0\) by the second Borel-Cantelli lemma. Thus \({\sum_{1}^{n}}X_{k}^{(c)}\rightarrow_{a.s.}\) for all \(c\), since I \(<\infty\) implies that \(X_{1}^{(c)},X_{2}^{(c)},\ldots\) and \(X_{1},X_{2},\ldots\) are Khinchine equivalent sequences. Thus II \(<\infty\) and III \(<\infty\) for all \(c\) by the 2-series theorem result (4).

Consider (c). Kolmogorov's 0-l law shows that \(S_{n}\) either converges a.s. or else diverges a.s.; and it is not convergent if one of the three series fails to converge. \(\Box\)

**\(\mathcal{L}_{2}\)-Convergence of Infinite Series, and A.S. Convergence\({}^{*}\)**

**Exercise 8.5** (\(\mathcal{L}_{2}\)-convergence of series) Let \(X_{1},X_{2},\ldots\) be independent rvs in \(\mathcal{L}_{2}\), where \(X_{k}\) has mean \(\mu_{k}\) and variance \(\sigma_{k}^{2}\). Then the sum \(S_{n}\equiv X_{1}+\cdots+X_{n}\) has mean \(m_{n}\equiv{\sum_{k=1}^{n}}\mu_{k}\) and variance \(v_{n}^{2}\equiv{\sum_{k=1}^{n}}\sigma_{k}^{2}\). Show that

(11) \(S_{n}\rightarrow_{\mathcal{L}_{2}}(\mbox{ some rv }S)\) if and only if \(m_{n}\rightarrow\mbox{ (some }\mu)\) and \(v_{n}^{2}\rightarrow\mbox{(some }\sigma^{2})\).

If \(S_{n}\rightarrow_{\mathcal{L}_{2}}S\), then E\(S=\mu\) and Var\([S]=\sigma^{2}\).

**Exercise 8.6** (Chow-Teicher) Let \(X_{1},X_{2},\ldots\) be iid with finite mean. Suppose the series of real numbers \({\sum_{1}^{n}}a_{k}\) converges, where the \(|a_{k}|\) are uniformly bounded. Show that \({\sum_{1}^{n}}a_{k}X_{k}\rightarrow_{a.s.}\) (some rv \(S\)).

**Other Generalizations of the LLNs\({}^{*}\)**

**Exercise 8.7** The following (with \(r=1\)) can be compared to theorem 8.4.4. If \(X_{1},X_{2},\ldots\) are independent with \(0\) means, then

\[\sum_{1}^{\infty}\mbox{E}|X_{n}|^{2r}/n^{r+1}<\infty\quad\mbox{for some}\quad r \geq 1\quad\mbox{implies}\quad S_{n}/n\to_{a.s.}0. \tag{12}\]

**Exercise 8.8** (Chung) Here is an even more general variation on theorem 8.??. Suppose that \(\phi>0\) is even and continuous, and either \(\phi(x)/x\nearrow\) but \(\phi(x)/x^{2}\searrow\) or else \(\phi(x)\nearrow\) but \(\phi(x)/x\searrow\). Let \(b_{n}\nearrow\infty\). Let \(X_{1},X_{2},\ldots\) be independent with \(0\) means. Then

\[\begin{array}{l}\sum_{n=1}^{\infty}\mbox{E}\phi(X_{n})/\phi(b_{n})<\infty \qquad\qquad\mbox{implies both}\\ \sum_{n=1}^{\infty}X_{n}/b_{n}\to_{a.s.}\mbox{ (some rv)}\quad\mbox{and}\quad\sum_{1}^{n}X_{k}/b_{n}\to_{a.s.}0.\end{array} \tag{13}\]

The WLLN is taken up again in sections 10.1 and 10.2, after the characteristic function tool has been introduced in chapter 9.

## 9 Martingales

**Definition 9.1** (Martingales): (a) Consider the sequence of rvs \(S_{1},S_{2},\ldots\) defined on a probability space \((\Omega,{\cal A},P)\) and adapted to an \(\nearrow\) sequence of \(\sigma\)-fields \({\cal A}_{1}\subset{\cal A}_{2}\subset\cdots\). Call it a _martingale_ (abbreviated \(mg\)) if \({\rm E}|S_{k}|<\infty\) for all \(k\), and

\[{\rm E}(S_{k}|{\cal A}_{i})=_{a.s.}S_{i}\quad\mbox{for all $i\leq k$ in the index set}. \tag{1}\]

If \((S_{k},\ {\cal A}_{k}),k\geq 1\) is a mg, then the increments \(X_{k}\equiv S_{k}-S_{k-1}\) are called the _martingale differences_.

(b) Let \(I\) denote a subinterval of the extended real line \(\bar{R}\). A collection \(\{S_{t}:t\in I\}\) of rvs on some \((\Omega,{\cal A},P)\) that is adapted to an \(\nearrow\) family of \(\sigma\)-fields \(\{{\cal A}_{t}:t\in I\}\) is called a _martingale_ if \({\rm E}|S_{t}|<\infty\) for all \(t\in I\), and

\[{\rm E}(S_{t}|{\cal A}_{r})=_{a.s.}S_{r}\quad\mbox{for all $r\leq t$ in $I$}. \tag{2}\]

(c) If "\(=\)"is replaced by " \(\geq\)" in either of (1) or (2), then either of \(\{S_{k}:k\geq 1\}\) or \(\{S_{t}:t\in I\}\) is called a _submartingale_ (or _submg_).

**Example 9.1** (The prototypical example): Let \(X_{1},\ldots,X_{n}\) denote independent rvs with 0 means, and set \(S_{k}\equiv X_{1}+\cdots+X_{k}\) and \({\cal A}_{k}\equiv\sigma[X_{1},\ \ldots,\ X_{k}]\) for \(1\leq k\leq n.\) Then the sequence of partial sums satisfies

\[(S_{k},\ {\cal A}_{k}),1\leq k\leq n,\mbox{ is a mg}, \tag{3}\]

while (provided \(X_{k}\) also has finite variance \(\sigma_{k}^{2}\))

\[(S_{k}^{2},\ {\cal A}_{k}),1\leq k\leq n,\mbox{ is a submg}. \tag{4}\]

The first claim is trivial, and the second holds, since

\[{\rm E}(S_{k}^{2}|{\cal A}_{i})={\rm E}\{S_{i}^{2}+2S_{i}(S_{k}-S_{i})+(S_{k}- S_{i})^{2}|{\cal A}_{i}\}\]

(a) \[\geq S_{i}^{2}+2S_{i}{\rm E}\{S_{k}-S_{i}|{\cal A}_{i}\}+0=S_{i}^{2}+0+0=S_{i} ^{2},\]

using (7.4.20) and (7.4.16). \(\square\)

**Exercise 9.1** (Equivalence): (a) Show that \((S_{t},\ {\cal A}_{t}),t\in I,\) is a martingale if and only if all \({\rm E}|S_{t}|<\infty\) and

\[\int_{{\cal A}_{r}}S_{t}\,dP=\int_{A_{r}}S_{r}\,dP\qquad\mbox{for all $A_{r} \in{\cal A}_{r}$ and all $r\leq t$ with $r,t\in I$}. \tag{5}\]

(b) For a submartingale, just replace "\(=\)" by "\(\geq\)" in (5).

**Notation 9.1**: We will use the following notational system:

\[\left\{\begin{array}{l@{\quad\quad}l}{\rm mg}\qquad&{\rm and}\quad=\quad&{ \rm for\ a\ martingale}.\\ {\rm submg}\qquad&{\rm and}\quad\geq\quad&{\rm for\ a\ submartingale}.\\ {\rm s\mbox{-}mg}\qquad&{\rm and}\quad\geq\quad&{\rm for\ a\ s\mbox{-}mg\,( \mbox{mg\,or\ submg},\ as\ the\ case\ may\ be)}.\end{array}\right. \tag{6}\]

Thus \((S_{t},{\cal A}_{t}),t\in I,\) is a s-mg if and only if all \({\rm E}|S_{t}|<\infty\) and

\[\int_{A_{r}}S_{t}dP\geqq\int_{A_{r}}S_{r}dP\quad\mbox{ for all $A_{r}\in{\cal A }_{r}$, \ and\ for all $r\leq t$ with }\quad r,\ t\in I. \tag{7}\]

**Exercise 9.2** Turn \((S_{k}^{2},\ {\cal A}_{k}),1\leq k\leq n\) into a martingale in (4) by centering it appropriately.

## 10 Maximal Inequalities, Some with \(\nearrow\) Boundaries \({}^{o}\)

**Inequality 10.1 (Monotone inequality)** For arbitrary rvs \(X_{1},\ldots,X_{n}\) and for constants \(0<b_{1}\leq\cdots\leq b_{n}\) we let \(S_{k}\equiv X_{1}+\cdots+X_{k}\) and obtain

\[\left(\max_{1\leq k\leq n}\frac{|S_{k}|}{b_{k}}\right)\leq 2\max_{1\leq k\leq n} \left|\sum_{i=1}^{k}\frac{X_{i}}{b_{i}}\right|.\]

If all \(X_{i}\geq 0\), then we may replace 2 by 1. [This also holds in higher dimensions, when properly formulated. See Shorack and Smythe(1976).]

**Proof.** Define \(b_{0}=0,X_{0}=0,Y_{j}=X_{j}/b_{j}\), and \(T_{k}=\sum_{j=1}^{k}Y_{j}\). Then

\[\mbox{(a)}\qquad S_{k}=\sum_{j=1}^{k}b_{j}\triangle T_{j}=\sum_{j=1}^{k} \triangle T_{j}\!\sum_{i=1}^{j}\!\triangle b_{i}=\sum_{i=1}^{k}T_{ik}\triangle b _{i},\]

where \(\triangle b_{j}\equiv b_{j}-b_{j-1},\triangle T_{j}\equiv T_{j}-T_{j-1}\), and \(T_{ik}\equiv\sum_{j=i}^{k}Y_{j}\). As \(\sum_{i=1}^{k}(\triangle b_{i}/b_{k})=1\) with each \(\triangle b_{i}/b_{k}\geq 0\), we have

\[\mbox{(b)}\qquad\left(\max_{1\leq k\leq n}|S_{k}|/b_{k}\right)\leq \left(\max_{1\leq k\leq n}\{\sum_{i=1}^{k}|T_{ik}|(\triangle b_{i}/b_{k})\}\right)\] \[\mbox{(c)}\qquad\leq\max_{1\leq k\leq n}\left(\max_{1\leq i\leq k }|T_{ik}|\right)\qquad\mbox{since an average does not exceed the maximum}\] \[\mbox{(d)}\qquad\leq 2\left(\max_{1\leq k\leq n}|T_{k}|\right).\]

Note that 1 can replace 2 in step (d) if all \(X_{i}\geq 0\). \(\Box\)

**Martingale Maximal Inequalities**

**Inequality 10.2 (Doob)** Let \((S_{k},\ {\cal A}_{k}),1\leq k\leq n\), be a submg and define the maximum \(M_{n}\equiv\max_{1\leq k\leq n}S_{k}\). Then

\[\lambda P(M_{n}\geq\lambda)\leq\int_{[M_{n}\geq\lambda]}S_{n}\,dP\leq{\rm E}S _{n}^{+}\leq{\rm E}|S_{n}|\quad\mbox{ for all }\lambda>0,\] \[\mbox{(3)}\qquad P(M_{n}\geq\lambda)\leq\inf_{r>0}{\rm E}(e^{rS_{ n}})/e^{r\lambda}\quad\mbox{ for all }\lambda>0.\]

If \((S_{k},{\cal A}_{k}),1\leq k\leq n\) is a zero-mean mg with all of the variances \({\rm E}S_{k}^{2}<\infty\), then we can conclude that \((S_{k}^{2},{\cal A}_{k}),1\leq k\leq n\) is a submg. This allows the maximum to be bounded by

\[P(M_{n}\geq\lambda)\leq{\rm Var}[S_{n}]/\lambda^{2}\qquad\qquad\mbox{ for all }\lambda>0.\]

[This last is Kolmogorov's inequality, valid for zero-mean mgs.]

**Proof.** Since \({\rm E}(S_{n}|{\cal A}_{k})\geq S_{k}\) a.s. by the definition of a submg, we have

\[\mbox{(a)}\qquad\int_{A_{k}}S_{n}\,dP=\int_{A_{k}}{\rm E}(S_{n}|{\cal A}_{k}) \,dP\geq\int_{A_{k}}S_{k}\,dP\quad\mbox{for all }A_{k}\in{\cal A}_{k}\]

by (7.4.1) in the definition of conditional expectation. Now let

\[\mbox{(b)}\qquad A_{k}\equiv[\max_{1\leq j<k}S_{j}<\lambda\leq S_{k}],\]so that \(k\) is the first index for which \(S_{k}\) is \(\geq\lambda\). Then

\[\lambda P(M_{n}\geq\lambda)=\lambda{\sum_{1}^{n}}P(A_{k})\leq{\sum_{1}^{n}}\int_{ A_{k}}S_{k}\,dP\]

(c) \[\leq{\sum_{1}^{n}}\int_{A_{k}}S_{n}\,dP\quad\mbox{using (a)}\]

(d) \[=\int_{[M_{n}\geq\lambda]}S_{n}\ dP\leq\int_{[M_{n}\geq\lambda]}S_{n}^{+}dP\leq \int S_{n}^{+}dP\leq\int|S_{n}|\,dP,\]

as claimed. In a s-mg context, these are called "first passage time" proofs. To this end, set \(\tau\) equal to \(k\) on \(A_{k}\) for \(1\leq k\leq n\), and set \(\tau\) equal to \(n+1\) on \(({\sum_{1}^{n}}A_{k})^{c}.\) Then \(\tau\) is the first passage time to the level \(\lambda\).

That \(\{(\exp(rS_{k}),{\cal A}_{k}),\ 1\leq k\leq n\}\) is also a submg for any \(r>0\) follows from Jensen's inequality for conditional expectation with an \(\nearrow g(\cdot)\) via

(e) \[{\rm E}(e^{rS_{k}}|{\cal A}_{j})\equiv{\rm E}(g_{r}(S_{k})|{\cal A}_{j})\geq_{ a.s}.\ g_{r}({\rm E}(S_{k}|{\cal A}_{j}))\geq g_{r}(S_{j})=e^{rS_{j}}.\]

Applying Doob's first inequality (2) to (e) gives (3). [This is often sharper than (2), though it requires the existence of the moment generating function \({\rm E}\exp(rS_{n})\).] When \((S_{k},\ {\cal A}_{k})\) is a mg, then \((S_{k}^{2},\ {\cal A}_{k})\) is also a submg (by another application of the same Jensen's inequality), so applying (2) to the latter submg gives (4). \(\Box\)

**Inequality 10.3** (Hajek-Renyi): Let \((S_{k},{\cal A}_{k}),1\leq k\leq N,\) be a mg with all \({\rm E}S_{k}=0\). Let \(X_{k}\equiv S_{k}-S_{k-1}\) have variance \(\sigma_{k}^{2}\). Let \(0<b_{1}\leq\cdots\leq b_{N}\). Then

\[P\left(\max_{n\leq k\leq N}|S_{k}|/b_{k}\geq\lambda\right)\leq\frac{4}{\lambda ^{2}}\left\{\sum_{k=1}^{n}\sigma_{k}^{2}/b_{n}^{2}+\sum_{k=n+1}^{N}\sigma_{k} ^{2}/b_{k}^{2}\right\}\mbox{ for all }\lambda>0. \tag{5}\]

**Proof.** (We give the proof for independent rvs.) The monotone inequality bounds the maximum partial sum via

\[\left(\max_{n\leq k\leq N}|S_{k}|/b_{k}\right)\leq 2\left(\max_{n\leq k\leq N} \left|\frac{S_{n}}{b_{n}}+\sum_{i=n+1}^{k}\frac{X_{i}}{b_{i}}\right|\right). \tag{6}\]

Applying Kolmogorov's inequality (4) to (6) gives

(a) \[P\left(\max_{n\leq k\leq N}|S_{k}|/b_{k}\geq\lambda\right)\leq(\lambda/2)^{-2} \{{\rm Var}[S_{n}/b_{n}]+\sum_{k=n+1}^{N}{\rm Var}[X_{k}]/b_{k}^{2}\}\]

(b) \[=(4/\lambda^{2})\{{\sum_{1}^{n}}\sigma_{k}^{2}/b_{n}^{2}+\sum_{n+1}^{N}\sigma_ {k}^{2}/b_{k}^{2}\}.\]

(A more complicated proof can eliminate the factor 4.) \(\Box\)

**Exercise 10.1**: To complete the proof of the Hajek-Renyi inequality for mgs, one can show that \(T_{k}\equiv S_{n}/b_{n}+\sum_{n+1}^{k}X_{i}/b_{i}\) is such that \((T_{k},\ {\cal A}_{k}),n\leq k\leq N\), is also a mg, and that \({\rm Var}[T_{N}]\) is equal to the right-hand side of (b). Do it.

**Inequality 10.4** (Birnbaum-Marshall): Let \((S(t),\ {\cal A}(t)),0\leq t\leq\theta\), be a mg having \(S(0)=0,{\rm E}S(t)=0\), and \(\nu/(t)={\rm E}S^{2}(t)\) finite and continuous on \([0,\ \theta].\) Suppose that paths of \(S\) are right (or left) continuous. Let \(q(\cdot)>0\) on \((0,\ \theta]\) be \(\nearrow\) and right (or left) continuous. Then

\[P(\|S/q\|_{0}^{\theta}>\lambda)\leq 4\lambda^{-2}\int_{0}^{\theta}[q(t)]^{-2}d \nu/(t)\qquad\mbox{for all }\lambda>0. \tag{7}\]

**Proof.** Because of right (or left) continuity and \(S(0)=0\), we have

(a) \[P(\|S/q\|_{0}^{\theta}\leq\lambda)=P\Big{(}\max_{0\leq i\leq 2^{n}}|S( \theta i/2^{n})|/q(\theta i/2^{n})\leq\lambda\mbox{ for all }n\geq 1\Big{)}\] \[\quad=\lim P\left(\max_{0\leq i\leq 2^{n}}|S(\theta i/2^{n})|/q( \theta i/2^{n})\leq\lambda\right)\qquad\mbox{by proposition 1.2.2}\] \[\quad\geq\lim\{1-4\lambda^{-2}{\sum_{1}^{2^{n}}}{\rm E}[S^{2}( \theta i/2^{n})-S^{2}(\theta(i-1)/2^{n})]/q^{2}(\theta i/2^{n})\}\quad\mbox{by (\ref{eq:S0})}\] \[\quad=1-4\lambda^{-2}\lim{\sum_{1}^{2^{n}}}q^{-2}(\theta i/2^{n}) [\nu/(\theta i/2^{n})-\nu/(\theta(i-1)/2^{n})]\] (b) \[\quad\to 1-4\lambda^{-2}\int_{0}^{\theta}[q(t)]^{-2}d\nu/(t)\qquad\mbox{ using the MCT.}\]

**Inequality 10.5 (Doob's \({\cal L}_{r}\)-inequality)** (i) Let \((S_{k},\ {\cal A}_{k})\), for \(1\leq k\leq n\), be a submg. Consider \(M_{n}\equiv\max_{1\leq k\leq n}S_{k}^{+}\). Let \(r>1\). Then

\[{\rm E}M_{n}^{r}\leq(\tfrac{r}{r-1})^{r}{\rm E}\{(S_{n}^{+})^{r}\}. \tag{8}\]

(ii) Let \((S_{k},\ {\cal A}_{k}),1\leq k\leq n\), be a mg. Let \(M_{n}\equiv\max_{1\leq k\leq n}|S_{k}|\). Let \(r>1\). Then

\[{\rm E}M_{n}^{r}\leq(\tfrac{r}{r-1})^{r}{\rm E}\{|S_{n}|^{r}\}. \tag{9}\]

**Proof.** Now, \((S_{k}^{+},{\cal A}_{k})\), for \(1\leq k\leq n\), is also a submg, by the conditional version of Jensen's inequality. (Or, refer to (13.1.7) below.) [Refer to (13.1.5) for case (ii).] Thus in case (i) we have

(a) \[{\rm E}M_{n}^{r}=\int_{0}^{\infty}r\lambda^{r-1}P(M_{n}>\lambda)\,d\lambda \qquad\mbox{by (\ref{eq:M_n})}\] (b) \[\quad\leq\int_{0}^{\infty}r\lambda^{r-1}\lambda^{-1}{\rm E}\{S_{n}^{+}1_{[ M_{n}\geq\lambda]}\}\,d\lambda\qquad\mbox{by Doob's inequality 10.2}\] (c) \[\quad={\rm E}\{S_{n}^{+}\int_{0}^{M_{n}}r\lambda^{r-2}d\lambda\} \qquad\mbox{by Fubini}\] \[\quad={\rm E}\{S_{n}^{+}(\tfrac{r}{r-1})M_{n}^{r-1}\}\] (d) \[\quad\leq(\tfrac{r}{r-1})({\rm E}\{(S_{n}^{+})^{r}\})^{1/r}({\rm E }\{M_{n}^{r}\})^{(r-1)/r}\qquad\mbox{by H\"{o}lder's inequality,}\]

where \(r^{-1}+s^{-1}=1\) implies that \(s=r/(r-1)\). So

(e) \[({\rm E}M_{n}^{r})^{1-(r-1)/r}\leq(\frac{r}{r-1})({\rm E}(S_{n}^{+})^{r})^{1/r},\]

which gives the results. (Just change \(S_{n}^{+}\) to \(|S_{n}|\) for case (ii).) \(\Box\)

**Hoffmann-Jorgensen Inequalities\({}^{*}\)**

The following inequalities show that "in probability" control of the overall sum and of the maximal summand actually gives control of moments of sums of independent rvs.

**Inequality 10.6** (Hoffmann-Jorgensen, probability form). Let \(X_{1},\ldots,X_{n}\) be independent rvs, and let \(S_{k}\equiv X_{1}+\cdots+X_{k}\) for \(1\leq k\leq n\). Let \(\lambda,\eta>0\). Then

\[P(\max_{1\leq k\leq n}|S_{k}|>3\lambda+\eta)\leq\{P(\max_{1\leq k\leq n}|S_{k}| >\lambda)\}^{2}+P(\max_{1\leq i\leq n}|X_{i}|>\eta). \tag{10}\]

If the \(X_{i}\)'s are also symmetric, then both

\[P(\max_{1\leq k\leq n}|S_{k}|>3\lambda+\eta)\leq\{2P(|S_{n}|> \lambda)\}^{2}+P(\max_{1\leq i\leq n}|X_{i}|>\eta)\qquad\mbox{and} \tag{12}\] \[P(|S_{n}|>2\lambda+\eta)\leq\{2P(|S_{n}|>\lambda)\}^{2}+P(\max_ {1\leq i\leq n}|X_{i}|>\eta). \tag{11}\]

**Inequality 10.7** (Hoffmann-Jorgensen, moment form). Let the rvs \(X_{1},\ldots,X_{n}\) be independent, and let \(S_{k}\equiv X_{1}+\cdots+X_{k}\) for \(1\leq k\leq n\). Suppose that each \(X_{i}\in{\cal L}_{r}(P)\) for some \(r>0\). Then

\[E\left(\max_{1\leq k\leq n}|S_{k}|^{r}\right)\leq 2(4t_{0})^{r}+2\cdot 4^{4} \mbox{E}\left(\max_{1\leq i\leq n}|X_{i}|^{r}\right), \tag{13}\]

where \(t_{0}\equiv\inf\{t>0:P(\max_{1\leq k\leq n}|S_{k}|>t)\leq 1/(2\cdot 4^{r})\}\).

If the \(X_{i}\)'s are also symmetric, then

\[\mbox{E}|S_{n}|^{r}\leq 2(3t_{0})^{r}+2\cdot 3^{r}\mbox{E}\left(\max_{1\leq i \leq n}|X_{i}|^{r}\right), \tag{14}\]

where \(t_{0}\equiv\inf\{t>0:P(|S_{n}|>t)\leq 1/(8\cdot 3^{r})\}\).

**Proof.** Consider inequality 10.6. Let \(\tau\equiv\inf\{k\leq n:|S_{k}|>\lambda\}\). Then \([\tau=k]\) depends only on \(X_{1},\ldots,X_{k}\), and \([\max_{k\leq n}|S_{k}|>\lambda]=\sum_{k=1}^{n}[\tau=k]\). On \([\tau=k],|S_{j}|\leq\lambda]\) if \(j<k\), and for \(j\geq k\),

(a) \[|S_{j}|=|S_{j}-S_{k}+X_{k}+S_{k-1}|\leq\lambda+|X_{k}|+|S_{j}-S_{k}|;\]

hence

(b) \[\max_{1\leq j\leq n}|S_{j}|\leq\lambda+\max_{1\leq i\leq n}|X_{i}|+\max_{k<j \leq n}|S_{j}-S_{k}|.\]

Therefore, by independence,

\[P(\tau=k,\ \max_{1\leq k\leq n}|S_{k}|>3\lambda+\eta)\]

(c) \[\leq P(\tau=k,\ \max_{1\leq i\leq n}|X_{i}|>\eta)+P(\tau=k)P(\max_{k<j\leq n }|S_{j}-S_{k}|>2\lambda).\]

But \(\max_{k<j\leq n}|S_{j}-S_{k}|\leq 2\max_{1\leq k\leq n}|S_{k}|\), and hence summing over \(k\) on both sides yields

(d) \[P(\max_{k\leq n}|S_{k}|>3\lambda+\eta)\leq P(\max_{i\leq n}|X_{i}|>\eta)+\{P( \max_{k\leq n}|S_{k}|>\lambda)^{2}\}.\]

The second inequality follows from the first by Levy's inequality 8.3.3.

For the symmetric case, first note that

(e) \[|S_{n}|\leq|S_{k-1}|+|X_{k}|+|S_{n}-S_{k}|,\]so that

\[P(\tau=k,\ |S_{n}|>2\lambda+\eta)\]

(f) \[\leq P(\tau=k,\ \max_{1\leq i\leq n}|X_{i}|>\eta)+P(\tau=k)P(|S_{n}-S_{k}|> \lambda);\]

and hence summing over \(k\) then yields

\[P(|S_{n}|>2\lambda+\eta)\]

(g) \[\leq P(\max_{i\leq n}|X_{i}|>\eta)+P(\max_{k\leq n}|S_{k}|>\lambda)P(\max_{k \leq n}|S_{n}-S_{k}|>\lambda).\]

The third inequality again follows from Levy's inequality. \(\Box\)

**Proof.** Consider inequality 10.7. Here is the proof of (14); the proof of (13) is similar. Let \(u>t_{0}\). Then, using (12) for (i),

(h) \[\begin{array}{ll}{\rm E}|S_{n}|^{r}=3^{r}(\int_{0}^{u}+\int_{u}^{\infty})P(| S_{n}|>3t)d(t^{r})\qquad\mbox{by (\ref{eq:13}.13).}\\ {\rm(i)}&\leq(3u)^{r}+4\cdot 3^{r}\int_{u}^{\infty}P(|S_{n}|>t)^{2}d(t^{r})+3^{r }\int_{u}^{\infty}P(\max_{1\leq i\leq n}|X_{i}|>t)d(t^{r})\\ {\rm(j)}&\leq(3u)^{r}+4\cdot 3^{r}P(|S_{n}|>u)\int_{u}^{\infty}P(|S_{n}|>t)d(t^{r })+3^{r}{\rm E}(\max_{1\leq i\leq n}|X_{i}|^{r}).\end{array}\]

Since \(4\cdot 3^{r}P(|S_{n}|>u)\leq\frac{1}{2}\) by our choice of \(u\), applying (6.4.13) again (to (j)) gives

(k) \[{\rm E}|S_{n}|^{r}\leq(3u)^{r}+\frac{1}{2}{\rm E}|S_{n}|^{r}+3^{r}{\rm E}(\max_ {1\leq i\leq n}|X_{i}|^{r}).\]

Simple algebra now gives (14). \(\Box\)

**Exercise 10.2**  Provide the details in the case of (13)

## Chapter 9 Characteristic Functions and Determining Classes

### 1 Classical Convergence in Distribution

**Definition 1.1** (Sub-dfs): (a) Suppose we have rvs \(X_{n}\cong F_{n}\) and \(X\). We now wish to allow the possibility that \(X\) is an extended rv. In this case, we assume that \(H\) is a _sub-df_ (we will not use the notation \(F\) in this context), and we will write \(X\cong H.\) The interpretation in the case of an extended rv \(X\) is that \(H(-\infty)=P(X=-\infty)\), \(H(x)=P(-\infty\leq X\leq x)\) for all \(-\infty<x<\infty\), and \(1-H(+\infty)=P(X=+\infty)\). The set \(C_{H}\) of all points at which \(H\) is continuous is called the _continuity set_ of \(H.\)

(b) If \(F_{n}(x)\to H(x)\) as \(n\to\infty\) at each \(x\in C_{H}\) of a sub-df \(H\), then we say that \(X_{n}\) (or \(F_{n}\)) _converges in sub-df_ to \(X\) (or \(H\)), and we write \(X_{n}\to_{sd}X\)(or \(F_{n}\to_{sd}H\)) as \(n\to\infty\). [What has happened in the case of sub-df convergence is that amounts \(H(-\infty)\) and \(1-H(+\infty)\) of mass have escaped to \(-\infty\) and \(+\infty\), respectively.]

(c) We have agreed that \(F_{n},F\), etc. denote a bona fide df, while \(H_{n},H\), etc. may denote a sub-df. Thus \(F_{n}\to_{d}F\) (with letter \(F\) rather than letter \(H\)) will still imply that the limit is necessarily a bona fide df. [The next definition provides a condition that guarantees (in a totally obvious way, on \(R\) at least) that any possible limit is a bona fide df.]

**Definition 1.2** (Tightness): A family \(\mathcal{P}\) of distributions \(P\) on \(R\) is called _tight_ if for each \(\epsilon>0\) there is a compact set (which for one-dimensional rvs is just a closed and bounded set) \(K_{\epsilon}\) with

\[P(K_{\epsilon})=P(X\in K_{\epsilon})\geq 1-\epsilon\qquad\mbox{for all dfs $P\in\mathcal{P}$.} \tag{1}\]

**Theorem 1.1** (Helly-Bray): If \(F_{n}\to_{d}F\) and \(g\) is bounded and continuous a.s.\(\,F\), then the expectations satisfy

\[\int g\,dF_{n}=\mbox{E}g(X_{n})\to\mbox{E}g(X)=\int g\,dF. \tag{2}\]

Conversely, if (2) holds for all bounded continuous \(g\), then \(F_{n}\to_{d}F.\)

[Thus \(F_{n}\to_{d}F\) if and only if \(\int g\,dF_{n}\to\int g\,dF\) for all bounded and continuous \(g\).]

**Theorem 1.2** (Continuous mapping theorem; Mann-Wald): Suppose that \(X_{n}\to_{d}X\) and suppose that \(g\) is continuous a.s. \(F\). Then \(g(X_{n})\to_{d}g(X)\).

How do we establish that \(F_{n}\to_{d}F\)? We have the necessary and sufficient condition of the Helly-Bray theorem 1.1 (presented earlier as theorem 3.5.1). (We should now recall our definition of the determining class used in the context of the proof of theorem 3.5.1.) We can also show convergence in distribution of more complicated functions of rvs via Mann-Wald's continuous mapping theorem 1.2 (presented earlier as theorem 3.5.2); an example is given by

\[Z_{n}\to_{d}Z\quad\text{implies that}\quad Z_{n}^{2}\to_{d}Z^{2}\cong\chi_{1}^{2}\]

where \(g(x)=x^{2}\). The concept of tightness was introduced above to guarantee that any possible limit is necessarily a bona fide df. This becomes more important in light of the next theorem.

**Theorem 1.3** (Helly's selection theorem): Let \(F_{1},F_{2},\ldots\) be any sequence of dfs. There necessarily exists a subsequence \(F_{n^{\prime}}\) and a sub-df \(H\) for which \(F_{n^{\prime}}\to_{sd}H.\) If the subsequence of dfs is tight, then the limit is necessarily a bona fide df.

**Corollary 1**: Let \(F_{1},F_{2},\ldots\) be any sequence of dfs. Let \(H\) be a fixed sub-df. Suppose every sd-convergent subsequence \(\{F_{n^{\prime}}\}\) satisfies \(F_{n^{\prime}}\to_{sd}\) (this same \(H\)). Then the whole sequence satisfies \(F_{n}\to_{sd}H.\) (Here is an alternative phrasing. Suppose every subsequence \(n^{\prime}\) contains a further subsequence \(n^{\prime\prime}\) for which \(F_{n^{\prime\prime}}\) converges in distribution to this _one fixed_ sub-df \(H\). Then the whole sequence satisfies \(F_{n}\to_{sd}H.\))

**Proof.** Let \(r_{1},r_{2},\ldots\) denote a sequence which is dense in \(R\). Using Bolzano-Weierstrass, choose a subsequence \(n_{1j}\) such that \(F_{n_{1j}}(r_{1})\to\) (some \(a_{1}\)). A further subsequence \(n_{2j}\) also satisfies \(F_{n_{2j}}(r_{2})\to\) (some \(a_{2}\)). Continue in this fashion. The diagonal subsequence \(n_{jj}\) converges to \(a_{i}\) at \(r_{i}\) for all \(i\geq 1\). [This _Cantor diagonalization technique_ is important. Learn it!] Define \(H_{o}\) on the \(r_{i}\)'s via \(H_{o}(r_{i})=a_{i}.\) Now define \(H\) on all real values via

\[H(x)=\inf\{H_{o}(r_{i})\ :\ r_{i}>x\};\]

this \(H\) is clearly is \(\nearrow\) and takes values in \([0,1]\). We must now verify that \(H\) is also right-continuous, and that \(F_{n_{jj}}\to_{sd}H\). That is, the diagonal subsequence, which we will now refer to as \(n^{\prime}\), is such that \(F_{n^{\prime}}=F_{n^{\prime}_{j}}=F_{n_{jj}}\to_{sd}H.\)

The monotonicity of \(H_{o}\) trivially gives \(\inf_{y\searrow x}H(y)\geq H(x)\). Meanwhile,

\[H(x)>H_{o}(r_{\kappa_{\epsilon}})-\epsilon\qquad\text{for some $x<r_{\kappa_{ \epsilon}}$ that is sufficiently close to}x\]

\[\geq H(y)-\epsilon\qquad\qquad\qquad\text{for any $x<y<r_{\kappa_{\epsilon}}$}\]

yields \(\inf_{y\searrow x}H(y)\leq H(x)\). Hence \(\inf_{y\searrow x}H(y)=H(x)\), and \(H\) is right continuous.

We next show that \(F_{n^{\prime}_{j}}(x)=F_{n_{jj}}(x)\to H(x)\) for any \(x\in\mathcal{C}_{H}\). Well,

\[F_{n^{\prime}_{j}}(r_{k})\leq F_{n^{\prime}_{j}}(x)\leq F_{n^{\prime}_{j}}(r_{ \ell})\qquad\text{for all}\,r_{k}<x<r_{\ell}.\]Passing to the limit on \(j\) gives

(e) \[H_{o}(r_{k})\leq\varliminf F_{n^{\prime}_{j}}(x)\leq\varlimsup F_{n^{\prime}_{j}}( x)\leq H_{o}(r_{\ell})\qquad\qquad\mbox{for all $r_{k}<x<r_{\ell}$.}\]

Now let \(r_{k}\nearrow x\) and \(r_{\ell}\searrow x\) in (e) to get

(f) \[\lim_{j}F_{n^{\prime}_{j}}(x)=H(x)\qquad\mbox{for all $x\in{\cal C}_{H}$} \qquad\qquad\qquad\mbox{(that is,$F_{n^{\prime}_{j}}\to_{sd}H$).}\]

Consider the corollary. Fact: Any bounded sequence of real numbers contains a convergent subsequence; and the whole original sequence converges if and only if all subsequential limit points are the same. Or, if every subsequence \(a_{n^{\prime}}\) contains a further subsequence \(a_{n^{\prime\prime}}\) that converges to the one fixed number \(a_{o}\), then we have \(a_{n}\to a_{o}\). We effectively showed above that every subsequence \(F_{n^{\prime}}\) contains a further subsequence \(F_{n^{\prime\prime}}\) for which

\[F_{n^{\prime\prime}}(x)\to\mbox{(the same $H(x)$)}\qquad\qquad\qquad\qquad \mbox{for each fixed $x\in C_{H}$.}\]

Thus the whole sequence has \(F_{n}(x)\to H(x)\) for each \(x\in C_{H}\). So, \(F_{n}\to_{sd}H\). \(\square\)

**Exercise 1.1** (Convergence of expectations and moments):

(a) Suppose \(F_{n}\to_{sd}H\) and that both \(F_{n-}(a)\to H_{-}(a)\) and \(F_{n}(b)\to H(b)\) for some constants \(-\infty<a<b<\infty\) in \(C_{H}\) having \(H(a)<H(b)\). Then

\[\int_{[a,b]}g\,dF_{n}\to\int_{[a,b]}g\;dH\quad\mbox{for all $g\in C_{[a,b]} \equiv\{g:g\mbox{ is continuous on $[a,b]$}\}$.} \tag{3}\]

Moreover, if \(F_{n}\to_{s.d.}H\), then

\[\int g\,dF_{n}\to\int g\,dH\qquad\qquad\mbox{for all $g\in C_{0}$,} \tag{4}\]

where \(C_{0}\equiv\{g:g\mbox{ is continuous on $R$ and $g(x)\to 0$ as $|x|\to\infty$}\}\).

(b) Suppose \(F_{n}\to_{d}F\) and \(g\) is continuous on the line. Suppose \(|g(x)|/\psi(x)\to 0\) as \(|x|\to\infty\), where \(\psi\geq 0\) has \(\int\psi dF_{n}\leq K<\infty\) for all \(n\). Then \(\int g\,dF_{n}\to\int g\,dF\).

(c) If \({\rm E}|X_{n}|^{r_{0}}<\mbox{(some $M$)}<\infty\) for all large \(n\), then \(F_{n}\to_{d}F\) implies that

\[{\rm E}|X_{n}|^{r}\to{\rm E}|X|^{r}\quad\mbox{and}\quad{\rm E}X_{n}^{k}\to{\rm E }X^{k}\mbox{ for $0<r<r_{0}$ and $0<k<r_{0}$.} \tag{5}\]

(d) Let \(g\) be continuous. If \(F_{n}\to_{sd}H\), then \(\liminf\int|g|dF_{n}\geq\int|g|\,dH\).

[Actually, \(g\) continuous a.s. \(H\) suffices in (a), (b), and (d) above.]

**Exercise 1.2** (Polya's lemma): If \(F_{n}\to_{d}F\) for a continuous df \(F\), then

\[\|F_{n}-F\|\to 0. \tag{6}\]

Thus if \(F_{n}\to_{d}F\) with \(F\) continuous and \(x_{n}\to x\), then \(F_{n}(x_{n})\to F(x)\).

**Exercise 1.3** (Verifying tightness): Suppose \(X_{n}\cong F_{n}\). Show that \(\{F_{n}:n\geq 1\}\) is tight if either

(a) \[\varlimsup{\rm E}|X_{n}|^{r}<\infty\qquad\qquad\qquad\mbox{for some $r>0$,}\qquad\qquad\qquad\qquad\mbox{or}\]

(b) \[F_{n}\to_{d}F.\]

### Equivalent Definitions of Convergence in Distribution

The condition \(F_{n}(x)\to F(x)\) can be rewritten both as \(P_{n}((-\infty,x])\to P((-\infty,x])\) as well as \(\mbox{E1}_{(-\infty,x]}(X_{n})\to\mbox{E1}_{(-\infty,x]}(X)\). Thus \(\to_{d}\) is reduced to computing expectations of the particularly simple function \(1_{(-\infty,x]}\); but these simple functions have the disadvantage of being discontinuous.

**Definition 1.3** (Closure, interior, and boundary): The _closure_ of \(B\) is defined to be \(\bar{B}\equiv\cap\{C:B\subset C\) and \(C\) is closed\(\}\), while \(B^{0}\equiv\cup\{\ U:U\subset B\) and \(U\) is open\(\}\) is called the _interior_ of \(B\). These have the property that \(\bar{B}\) is the smallest closed set containing \(B\), while \(B^{0}\) is the largest open set contained within \(B\). The _boundary_ of \(B\) is defined to be \(\partial B\equiv\bar{B}\backslash B^{0}\). A set \(B\) is called a \(P\)-_continuity set_ if \(P(\partial B)=0.\) (These definitions are valid on a general metric space, not just on \(R.\))

**Theorem 1.4** (\(\to_{d}\) equivalencies): Let \(F,F_{1},F_{2},\ldots\) be the dfs associated with the probability distributions \(P_{1},P_{2},\ldots\). Let \(C_{b}\) denote all bounded, continuous functions \(g\) on \(R\), and then let \(C_{bu}\) denote all bounded and uniformly continuous functions \(g\) on \(R\). The following are equivalent:

\[F_{n}\to_{d}F. \tag{7}\]

\[F_{n}(x)\to F(x)\qquad\qquad\mbox{for all $x$ in a dense set}. \tag{8}\]

\[\mbox{E}g(X_{n})=\int g\,dF_{n}\to\int g\,dF=\mbox{E}g(X)\quad\mbox{for all $g$ in $C_{b}$}. \tag{9}\]

\[\int g\,dF_{n}\to\int g\,dF\qquad\mbox{for all $g$ in $C_{bu}$}. \tag{10}\]

\[\overline{\lim}\,P_{n}(B)\leq P(B)\quad\mbox{ for all closed sets $B$}. \tag{11}\]

\[\underline{\lim}\,P_{n}(B)\geq P(B)\quad\mbox{ for all open sets $B$}. \tag{12}\]

\[\lim P_{n}(B)=P(B)\quad\mbox{ for all $P$-continuity sets $B$}. \tag{13}\]

\[\lim P_{n}(I)=P(I)\qquad\mbox{ for all (even unbounded) $P$-continuity intervals $I$}. \tag{14}\]

\[L(F_{n},F)\to 0\qquad\qquad\mbox{ for the Levy metric $L$ (see below)}. \tag{15}\]

**Exercise 1.4**: That (7)-(10) are equivalent is either trivial, or done previously. Cite the various reasons. Then show that (11)-(15) are also equivalent to \(\to_{d}.\)

**Exercise 1.5** (Levy's metric): For any dfs \(F\) and \(G\) define (the \(45^{\rm o}\) distance between \(F\) and \(G\))

\[L(F,\ G)\equiv\inf\{\epsilon>0:F(x-\epsilon)-\epsilon\leq G(x)\leq F(x+ \epsilon)+\epsilon\quad\mbox{for all}\,x\}. \tag{16}\]

Show that \(L\) is a metric and that the set of all dfs under \(L\) forms a complete and separable metric space. Also show that \(F_{n}\to_{d}F\) is equivalent to \(L(F_{n},F)\to 0.\)

**Convergence of Types**

**Definition 1.4** (Type): When \(Y\cong(X-b)/a\) for some \(a\neq 0\), we say that \(X\) and \(Y\) are of the same _type_. [Suppose that \(X_{n}\rightarrow_{d}X\) where \(X\) is not degenerate. Then if \(a_{n}\to a\neq 0\) and \(b_{n}\to b\), we know from Slutsky's theorem that \((X_{n}-b_{n})/a_{n}\rightarrow_{d}Y\cong(X-b)/a\).]

**Theorem 1.5** (Convergence of types): Suppose \((X_{n}-b_{n})/a_{n}\rightarrow_{d}X\cong F,\) and \((X_{n}-\beta_{n})/\)\(\alpha_{n}\rightarrow_{d}Y\cong G\), where \(a_{n}>0,\alpha_{n}>0\), and both \(X\) and \(Y\) are nondegenerate. Then there exists \(a>0\) and a real \(b\) such that

\[a_{n}/\alpha_{n}\rightarrow\ (\mbox{some positive }a)\qquad\mbox{and}\qquad( \beta_{n}-b_{n})/a_{n}\rightarrow\ (\mbox{some real }b) \tag{17}\]

and \(Y\cong aX+b\) (or, equivalently, \(F(x)=G(ax+b)\) for all \(x\)).

**Remark 1.1**: The classical CLT implies that if \(X_{1},X_{2},\ldots\) are iid \((0,\sigma^{2})\), then \(S_{n}/\sqrt{n}\rightarrow_{d}N(0,1)\). The above theorem tells us that no matter how we normalize \(S_{n}\), the only possible nondegenerate limits in distribution are normal distributions. Moreover, if \(S_{n}/a_{n}\rightarrow_{d}\) (some rv), the limiting distribution can be nondegenerate only if \(a_{n}/\sqrt{n}\rightarrow\) (some constant) \(\in(0,\infty)\).

**Exercise 1.6** (Proof of the convergence of types theorem): Prove theorem 1.5 on the convergence of types.

[Hint. Start with continuity points \(x<x^{\prime}\) of the df \(G\) and then continuity points \(y,y^{\prime}\) of the df \(F\) for which \(F(y)<G(x)\leq G(x^{\prime})<F(y^{\prime})\). Then for all \(n\) large enough one will have \(a_{n}y+b_{n}\leq\alpha_{n}x+\beta_{n}\leq\alpha_{n}x^{\prime}+\beta_{n}\leq a_ {n}y^{\prime}+b_{n}\).]

**Higher Dimensions**

If \(X,X_{1},X_{2},\ldots\) are \(k\)-dimensional random vectors with dfs \(F,F_{1},F_{2},\ldots\), then we say that \(X_{n}\)_converges in distribution_ to \(X\) if

\[F_{n}(x)\to F(x)\qquad\mbox{for all }x\in C_{F}, \tag{18}\]

just as in one dimension.

The Helly-Bray theorem, the Mann-Wald theorem, Helly's selection theorem, and Polya's lemma all hold in \(k\) dimensions; generalizations of the other results also hold. Moreover, if \(X^{\prime}_{n}\) denotes the first \(j\) coordinates of \(X_{n}\), with \(1\leq j<k\), then \(X_{n}\rightarrow_{d}X\) implies \(X^{\prime}_{n}\rightarrow_{d}X^{\prime}\).

**Exercise 1.7**: Prove the \(k\)-dimensional Helly-Bray theorem (along the lines of exercise 3.5.2) using Helly's selection theorem and Polya's lemma. Prove that \(X_{n}\rightarrow_{d}X\) implies \(X^{\prime}_{n}\rightarrow_{d}X^{\prime}\). After reading Section 2, prove the \(k\)-dimensional version of the Mann-Wald theorem.

**Exercise 1.8**: Prove that theorem 1.4 holds in \(k\) dimensions.

See also theorem 9.5.2 and theorem 10.1.3 below.

## 2 Determining Classes of Functions

We can approximate the functions \(1_{(-\infty,z]}(\cdot)\) to an arbitrary degree of accuracy within various classes of particularly smooth functions. Within these classes of functions we do not have to worry about the continuity of the limiting measure at \(z\), and this will make these classes more convenient. Indeed, the specialized class \({\cal H}^{0}\) below is of this type.

**Definition 2.1** (Determining class): A collection \({\cal G}\) of bounded and continuous functions \(g\) is called a _determining class_ if for any choice of dfs \(\tilde{F}\) and \(F\), the requirement that \(\int g\,d\tilde{F}=\int g\,dF\) for all \(g\in{\cal G}\) implies \(\tilde{F}=F.\)

**Definition 2.2** (Various classes of smooth functions): (i) Let \(C\) (let \(C_{b}\)) [let \(C_{bu}\)] denote the class of continuous (bounded and continuous) [bounded and also uniformly continuous] functions on \(R\). Let \(C_{b}^{(k)}\) (let \(C_{b}^{(\infty)}\)) denote the subclasses with \(k\) (with all) derivatives bounded and continuous.

(ii) An extra \(c\) on these classes will indicate that all functions vanish outside some compact subset of \(R\).

(iii) Let \(C_{0}\) denote the subclass of \(C\) that converge to \(0\) as \(|x|\to\infty\).

(iv) Let \({\cal H}^{0}\) denote the class of all \(h_{z,\epsilon}\) with \(z\) real and \(\epsilon>0\); here \(h_{z,\epsilon}(x)\) equals \(1\), is linear, equals \(0\) according as \(x\) is in \((-\infty,z],\)is in \([z,z+\epsilon],\)is in \([z+\epsilon,\infty)\) (this class was introduced in the proof of the Helly-Bray theorem 3.5.1).

(v) Let \({\cal G}^{0}\) denote the class of all continuous functions \(g_{a,b,\epsilon}\) with \(a<b\) and \(\epsilon>0\); here \(g_{a,b,\epsilon}(x)\) equals \(0\), is linear, equals \(1\) according as \(x\) is in \((-\infty,a-\epsilon]\,\cup[b+\epsilon,\infty)\), is in \([a-\epsilon,a]\cup[b,b+\epsilon]\), is in \([a,b]\).

**Theorem 2.1** (Criteria for \(\to_{d}\); a **kinder and gentler Helly-Bray)**

(i) Let \(F_{1},F_{2},\ldots\) be tight. Let \({\cal G}\) be a determining class.

(a) If \(\int g\,dF_{n}\to(\)some \(\#_{g})\) for each \(g\in{\cal G}\), then \(F_{n}\to_{d}F\). Further, \(\#_{g}=\int g\,dF.\)

(b) Conversely: If \(F_{n}\to_{d}F\), then \(\int g\,dF_{n}\to\int g\,dF\) for each \(g\in{\cal G}\).

(ii) Each of the various classes \(C_{0},C_{b},C_{bu},C_{b}^{(k)}\) with \(k\geq 1,C_{b}^{(\infty)},{\cal H}^{0}\), and \({\cal G}^{0}\) is a determining class.

(iii) So, too, if we add an extra subscript \(c\) to the various \(C\)-classes in (ii). (That is, we require they take on the value \(0\) outside some compact subset of \(R\).)

[For some proofs in the literature, functions \(g\) with sharp corners are unhandy.]

**Exercise 2.1**: Prove the previous theorem.

**Exercise 2.2** (Higher dimensions): Show that the natural extension of each of the results of this section to \(R_{k}\) is valid.

**Exercise 2.3**: Exhibit at least one more determining class.

[MISSING_PAGE_FAIL:215]

Comment. A condition for convergence due to Carleman \(\sum_{1}^{\infty}\mu_{2k}^{-1/2k}=\infty\) has often been claimed to be necessary and sufficient. It is not. See Stoyanov (1977; p. 113).

**Exercise 2.6**  Show that the \(N(0,1)\) distribution is uniquely determined by its moments.

**Summary** The methods of this section that establish \(\to_{d}\) by verifying the moment condition that \(\mathrm{E}g(X_{n})\to\mathrm{E}g(X)\) for all functions \(g\) in a given determining class \(\mathcal{G}\) can be extended from the present setting of the real line to more general settings; note Chapter 15. This chapter now turns to the development of results associated with the particular determining class \(\mathcal{G}\equiv\{g_{t}(\cdot)\equiv e^{it}:t\in R\}\). The resulting function is called the _characteristic function_ of the rv \(X\). The rest of Chapter 9 includes a specialized study of the characteristic function. Chapter 10 will apply this characteristic function tool to the CLT.

## 3 Characteristic Functions, with Basic Results

Elementary Facts

**Definition 3.1** (Characteristic function): Let \(X\) be an arbitrary rv, and let \(F\) denote its df. The _characteristic function_ of \(X\) (abbreviated \(chf\)) is defined (for all \(t\in R\)) by

\[\begin{array}{l}\phi(t)\equiv\phi_{X}(t)\equiv Ee^{itX}=\int_{-\infty}^{\infty }e^{itx}\,d\mu_{F}(x)=\int_{-\infty}^{\infty}e^{itx}\,dF(x)\\ \quad\equiv\int_{-\infty}^{\infty}\cos(tx)\,dF(x)+i\int_{-\infty}^{\infty}\sin (tx)\,dF(x).\end{array} \tag{1}\]

With \(dF\) replaced by \(h\,d\mu\), we call this the _Fourier transform_ of the signed measure \(hd\,\mu\). (We note that the chf \(\phi_{X}(t)\) exists for \(-\infty<t<\infty\) for _all_ rvs \(X\), since \(|e^{itX(\omega)}|\leq 1\) for all \(t\) and all \(\omega\).)

**Proposition 3.1** (Elementary properties): Let \(\phi\) denote an arbitrary chf.

(a) \[\phi(0)=1\mbox{ and }|\phi(t)|\leq 1\] for all

\[t\in R\]

. (b) \[\phi_{aX+b}(t)=e^{itb}\phi_{X}\,\,(at)\] for all

\[t\in R\]

. (c) \[\phi_{\Sigma_{1}^{n}X_{i}}(\cdot)=\prod\nolimits_{i=1}^{n}\!\phi_{X_{i}}(\cdot) \qquad\mbox{when }X_{1},\ldots,X_{n}\mbox{ are independent.}\]

. (d) \[\bar{\phi}_{X}(t)=\phi_{X}(-t)=\phi_{-X}(t)=\mbox{E}\cos tX-i\mbox{E}\sin tX \qquad\mbox{for all }t\in R.\]

. (e) \[\phi\mbox{ is real-valued if and only if }X\cong-X.\]

. (f) \[|\phi(\cdot)|^{2}\mbox{ is a chf }\qquad\mbox{[of the rv }X^{s}\equiv X-X^{\prime},\mbox{ with }X\mbox{ and }X^{\prime}\mbox{ iid with chf }\phi].\]

. (g) \[\phi(\cdot)\mbox{ is uniformly continuous on }R.\]

**Proof.** : Now, (a), (b), (c), and (d) are trivial. (e) If \(X\cong-X\), then \(\phi_{X}=\bar{\phi}_{X}\); so \(\phi_{X}\) is real. If \(\phi\) is real, then \(\phi_{X}=\bar{\phi}_{X}=\phi_{-X}\); so \(X\cong-X\) by the uniqueness theorem below. (f) If \(X\) and \(X^{\prime}\) are independent with characteristic function \(\phi\), then \(\phi_{X-X^{\prime}}=\phi_{X}\phi_{-X}=\phi\bar{\phi}=|\phi|^{2}\). For (g), we note that for all \(t\),

\[|\phi(t+h)-\phi(t)|=|\int[\exp^{i(t+h)x}-e^{itx}]\,dF(x)|\]

(a) \[\leq\int|e^{itx}||e^{ihx}-1|\,dF(x)\leq\int|e^{ihx}-1|\,dF(x)\to 0\]

as \(h\to 0\), by the DCT with dominating function 2.

The converse of (c) is false. Let \(X_{1}\equiv X_{2}\) and \(X_{3}\) be two iid Cauchy(0, 1) rvs. We will see below that \(\phi_{\rm Cauchy}(t)=\exp(-|t|)\), giving \(\phi_{2X_{1}}(t)=\phi_{X_{1}+X_{2}}(t)=\phi_{X_{1}+X_{3}}(t)\) for all \(t\). \(\Box\)

**Motivation 3.1** (Proving the CLT via chfs): In this chapter we present an alternative method for establishing \(F_{n}\to_{d}F\). It is based on the fact (to be demonstrated below) that the complex exponential functions \(e^{it.}\) on \(R\), indexed by \(t\in R\), form a limit determining class. Saying this another way, the chf \(\phi\) determines the distribution \(P\), or the df \(F\)(or the density \(f,\text{if there is one}\)). Thus (as is shown in the continuity theorem below) we can establish that \(F_{n}\to_{d}F\) by showing that \(\phi_{n}(\cdot)\to\phi(\cdot)\) on \(R\). Indeed, using just the elementary properties listed above, it is trivial to give an informal "proof" of the classical CLT. Thus, we begin by expanding the chf of one rv \(X\) as

(a) \[\phi_{(X-\mu)/\sqrt{n}}(t)=\phi_{X-\mu}(t/\sqrt{n})=\text{E}e^{it( X-\mu)/\sqrt{n}}\] (b) \[=\text{E}\{1+\frac{it}{\sqrt{n}}(X-\mu)+\frac{(it)^{2}}{n}(X-\mu)^{2}/2+o(t ^{2}/n)\}\] (c) \[=1+0-t^{2}\sigma^{2}/2n+o(t^{2}/n)=1-t^{2}[\sigma^{2}+o(1)]/2n.\] (In section 6 we will make such expansions rigorous, and in section 7 we will estimate more carefully the size of the errors that were made.)

Then the standardized sum of the iid rvs \(X_{1},\ldots,X_{n}\) is

(d) \(Z_{n}\equiv\sqrt{n}(\bar{X}_{n}-\mu)=\sum_{1}^{n}(X_{k}-\mu)/\sqrt{n}\),

and it has chf

(e) \(\phi_{Z_{n}}(t)=\prod_{k=1}^{n}\phi_{(X_{k}-\mu)/\sqrt{n}}(t)=[\phi_{(X-\mu)/ \sqrt{n}}(t)]^{n}\)

(f) \[=\{1-\frac{t^{2}[\sigma^{2}+o(1)]}{2n}\}^{n}\to e^{-t^{2}\sigma^{2}/2}\] (g) \[=\phi_{N(0,\sigma^{2})}(t)\qquad\text{as will be shown below}.\]

Since \(\phi_{Z_{n}}(\cdot)\to\phi_{Z}(\cdot)\) on \(R\), where \(Z\cong N(0,1)\), the uniqueness theorem and the continuity theorem combine to guarantee that \(Z_{n}\to_{d}Z\). In principle, this is a rather elementary way to prove the CLT.

Think of it this way. To have all the information on the distribution of \(X\), we must know \(P(X\in B)\) for all \(B\in\mathcal{B}\). We have seen that the df \(F\) also contains all this information, but it is presented in a different format; a statistician may well regard this \(F\) format as the "tabular probability calculating format." When a density \(f\) exists, it also contains all the information about \(P\); but it is again presented in a different format, which the statistician may regard as the "distribution visualization format." We will see that the chf presents all the information about \(P\) too. It is just one more format, which we may well come to regard as the "theorem proving format".

**Table** 3.1 Some Important Characteristic Functions

\begin{tabular}{l l l}
**Distribution** & **Density** & **Chf** \\ Binomial(\(n,p\)) & \(\left(\begin{array}{c}n\\ k\end{array}\right)p^{k}(1-p)^{n-k}\); for \(0\leq k\leq n\) & \([1+p(e^{it}-1)]^{n}\) \\ Poisson(\(\lambda\)) & \(e^{-\lambda}\lambda^{k}/k!\); for \(k\geq 0\) & \(\exp(\lambda(e^{it}-1))\) \\ GeometricF(\(p\)) & \(pq^{k}\); for \(k\geq 0\) & \(p(1-qe^{it})^{-1}\) \\ Normal(\(\mu,\sigma^{2}\)) & \(e^{-(x-\mu)^{2}/2\sigma^{2}}/\sqrt{2\pi}\sigma\) on R & \(\exp(it\mu-\sigma^{2}t^{2}/2)\) \\ Exponential(\(\theta\)) & \(e^{-x/\theta}/\theta\) on \(R^{+}\) & \((1-it\theta)^{-1}\) \\ Chisquare(\(n\)) & \(x^{(n/2)-1}e^{-x/2}/[2^{n/2}\Gamma(n/2)]\) & \((1-2it)^{-n/2}\) \\ Gamma(\(r,\theta\)) & \(x^{r-1}e^{-x/\theta}/[\theta^{r}\Gamma(r)]\) on \(R^{+}\) & \((1-it\theta)^{-r}\) \\ Uniform(0, 1) & \(1_{[0,1]}(x)\) & \([\exp(it)-1]/it\) \\ Double Exp(\(\theta\)) & \(e^{-[x]/\theta}/2\theta\) & \(1/(1+\theta^{2}t^{2})\) \\ Cauchy(0, 1) & \(1/[\pi(1+x^{2})]\) & \(e^{-|t|}\) \\ de la Vallee Poussin & \((1-\cos x)/(\pi x^{2})\) on \(R\) & \([1-|t|]\times 1_{[-1,1]}(t)\) \\ Triangular(\(0,1\)) & \([1-|x|]\times 1_{[-1,1]}(x)\) & \(2(1-\cos t)/(t^{2})\) on \(R\) \\ \end{tabular}

**Review of Some Useful Complex Analysis**

A function \(f\) is called _analytic_ on a _region_ (a connected open subset of the complex plane) if it has a derivative at each point of the region; if it does, then it necessarily has derivatives of all orders at each point in the region. If \(z_{0}\) is an isolated singularity of \(f\) and \(f(z)=\sum_{n=0}^{\infty}a_{n}(z-z_{0})^{n}+\sum_{n=1}^{m}b_{n}(z-z_{0})^{-n}\), then \(k\equiv\) (the _residue_ of \(f\) at \(z_{0}\)) = \(b_{1}\). Thus if \(f\) has a pole of order \(m\) at \(z_{0}\) (that is, \(b_{n}=0\) for \(n>m\) in the expansion above), then \(g(z)\equiv(z-z_{0})^{m}f(z)=b_{m}+\cdots+b_{1}(z-z_{0})^{m-1}+\sum_{0}^{\infty }a_{n}(z-z_{0})^{m+n}\) has \(b_{1}=g^{(m-1)}(z_{0})/(m-1)!\). Thus

\[b_{1}=k=(\mbox{residue of $f$ at $z_{0}$})\quad\{=\lim_{z\to z_{0}}(z-z_{0})f(z)\, \mbox{ for a simple pole at $z_{0}$}\}.\]

We also note that a _smooth arc_ is described via equations \(x=\phi(t)\) and \(y=\psi(t)\) for \(a\leq t\leq b\) when \(\phi^{\prime}\) and \(\psi^{\prime}\) are continuous and not simultaneously zero. A _contour_ is a continuous chain of a finite number of smooth arcs that do not cross the same point twice. _Closed_ means that the starting and ending points are identical. (See Ahlfors (1953, pp. 102, 123) for what follows.)

**Lemma 3.1** (Residue theorem) If \(f\) is analytic on a region containing a closed contour \(C\), except for a finite number of singularities \(z_{1},\ldots,z_{n}\) interior to \(C\) at which \(f\) has residues \(k_{1},\ldots,k_{n}\), then (for counterclockwise integration over \(C\))

\[\int_{C}f(z)dz=2\pi i\sum_{j=1}^{n}k_{j}\ \left\{\begin{array}{l l}=0& \mbox{if $f$ is analytic,}\\ =2\pi i(z-z_{o})f(z_{0})&\mbox{for one simple pole at $z_{o}$}.\end{array}\right.\]

**Lemma 3.2** Let \(f\) and \(g\) be functions analytic in a regions \(\Omega\). Suppose that \(f(z)=g(z)\) for all \(z\) on a set \(S\) that has an accumulation point in \(\Omega\). We then have the equality \(f(z)=g(z)\) for all \(z\in\Omega\). (That is, \(f\) is determined on \(\Omega\) by its values on \(S\). So if there is a Taylor series representation \(f(z)=\sum_{j=0}^{\infty}a_{n}(z-z_{o})^{j}\) valid on some disk interior to \(\Omega\), then the coefficients \(a_{1},a_{2},\ldots\) determine \(f\) on all of \(\Omega\).)

### Evaluating Various Characteristic Functions

**Example 3.1** (Derivation of the Cauchy(0, 1)chf) Let \(C\) denote the upper semicircle centered at the origin with radius \(R\) parametrized counterclockwise; and let \(A\) (for arc) denote \(C\) without its base. Let \(t>0\). The Cauchy chf is approached via

(a) \[\int_{C}\frac{e^{itz}}{\pi(1+z^{2})}\,dz\equiv\int_{C}f(z)\,dz=2\pi i \cdot(z-z_{o})f(z_{o})\] (with \[z_{o}=i\] ) (b) \[=2\pi i\cdot(z-i)\frac{e^{itz}}{\pi(1+iz)(1-iz)}\Big{|}_{z=i}=e^{-t}\] for \[t>0\].

It further holds that

(c) \[\int_{C}\frac{e^{itz}}{\pi(1+z^{2})}\,dz=\int_{-R}^{R}\frac{e^{itz} }{\pi(1+x^{2})}\,dx+\int_{A}\frac{e^{itz}}{\pi(1+z^{2})}\,dz\] (d) \[\to\int_{-\infty}^{\infty}\frac{e^{itz}}{\pi(1+x^{2})}\,dx+0= \phi(t)\] as \[R\to\infty\],

since the second integral in (b) is bounded in absolute value by

\[\tfrac{1}{\pi}\int_{A}\tfrac{1}{R^{2}-1}dz=\tfrac{1}{\pi}\tfrac{1}{R^{2}-1} \pi R\to 0\] as \[R\to\infty\].

Since the Cauchy is symmetric, \(\phi(-t)=\phi(t)=\exp(-|t|)\); or, integrate the contour clockwise when \(t<0\). The tabular entry has been verified. That is,

\[\phi(t)=\exp(-|t|),\qquad\text{for all $t$},\qquad\text{gives the Cauchy(0,1) chf}. \tag{4}\]

**Example 3.2** (Derivation of the \(N(0,1)\) chf) Let \(X\) be \(N(0,1)\). Then

\[\phi(t)=\int_{-\infty}^{\infty}e^{itz}\tfrac{1}{\sqrt{2\pi}}e^{-x^{2}/2}dx.\]

Let us instead think of \(\phi\) as a function of a complex variable \(z\). That is,

\[\phi(z)=\int_{-\infty}^{\infty}e^{izx}\tfrac{1}{\sqrt{2\pi}}e^{-x^{2}/2}dx.\]

Let us define a second function \(\psi\) on the complex plane by

(b) \[\psi(z)\equiv e^{-z^{2}/2}.\]

Now \(\phi\) and \(\psi\) are analytic on the whole complex plane. Let us now consider the purely imaginary line \(z=iy\). On this line it is clear that

\[\psi(iy)=e^{y^{2}/2},\]

and since elementary calculations show that

\[\phi(iy)=\int_{-\infty}^{\infty}e^{-yx}\tfrac{1}{\sqrt{2\pi}}e^{-x^{2}/2}=e^{ y^{2}/2}\int_{-\infty}^{\infty}\tfrac{1}{\sqrt{2\pi}}e^{-(x+y)^{2}/2}dx=e^{y^{2} /2},\]

we have \(\psi=\phi\) on the line \(z=iy\). Thus lemma 3.2 implies that \(\psi(z)=\phi(z)\) for all \(z\) in the plane. Thus \(\phi(t)=\psi(t)\) for all real \(z=t\). That is,

\[\phi(t)=\exp(-t^{2}/2),\qquad\text{for all real $t$},\qquad\text{gives the $N(0,1)$ chf}. \tag{5}\]

(A similar approach works for the gamma distribution in exercise 3.3 below.)

**Exercise 3.1**  Derive the \(N(0,1)\) chf via the residue theorem. Then extend to \(N(\mu,\sigma^{2})\). [Hint. Let \(C\) denote a closed rectangle of height \(t\) with base \([-R,R]\) on the \(x\)-axis.]

**Exercise 3.2** (a) Derive the Poisson \((\lambda)\) chf (by summing power series).

(b) Derive the GeometricT\((p)\)chf.

(c) Derive the Bernoulli\((p)\), Binomial\((n,p)\), and NegBiT\((m,p)\) chfs.

**Exercise 3.3** (a) Derive the Gamma\((r,\theta)\)chf. [Hint. Note example 3.2.]

(b) Derive the Exponential\((\theta)\) and Chisquare(n), and Double Exponential\((\theta)\) chfs.

**Exercise 3.4**  Derive the Logistic(0, 1) chf.

Hint. Use the lemma 3.2 approach.

**Exercise 3.5**  Show that the real part of a chf (or Re \(\phi(\cdot)\)) is itself a chf.

**Exercise 3.6**  Let \(\phi\) be a chf. Show that \(\frac{1}{c}\int_{0}^{c}\phi(tu)\,du\) is a chf.

## 4 Uniqueness and Inversion

For the chf to be a useful tool, there must be a l-to-l correspondence between dfs and chfs. The fact that this is so is called the _uniqueness theorem_. We give a simple proof of the uniqueness theorem at the end of this subsection. But the simple proof does not establish an _inversion formula_ that expresses the df as a function of the chf. In order to establish an inversion formula, we will need some notation, and an inversion formula useful for other purposes will require a hypothesis on the chf that is strong enough to allow some useful simplification.

Let \(U\) denote a rv with continuous density \(f_{U}(\cdot)\) and let \(W\) denote a rv with a bounded and continuous density \(f_{W}(\cdot)\) and with chf \(\phi_{W}(\cdot)\); and suppose we are lucky enough to determine a _complementary pair_ that (for some constant \(c\)) satisfy the relationship

\[f_{U}(t)=c\,\phi_{W}(-t)\qquad\mbox{for all real }t.\qquad\qquad\qquad\mbox{ (Complementary pair)} \tag{1}\]

We give three examples of such pairs. Let \(Z\cong N(0,1),T\cong\mbox{Triangular}(0,\ 1)\), and let \(D\) have the de la Vallee Poussin density. Then examples of (1) are

\[U=Z\qquad\mbox{and}\qquad W=Z,\qquad\mbox{with }c=1/\sqrt{2\pi}, \tag{2}\]

\[U=T\qquad\mbox{and}\qquad W=D,\qquad\mbox{with }c=1, \tag{3}\]

\[U=D\qquad\mbox{and}\qquad W=T,\qquad\mbox{with }c=1/2\pi. \tag{4}\]

(The Cauchy(0, 1) and the Double Exponential(0, 1) then lead to two additional complementary pairs.) (The beauty of this is that we can nearly eliminate the use of complex analysis.) (In all such examples we have \(2\pi cf_{W}(0)=1\).)

An arbitrary rv \(X\), having df \(F_{X}(\cdot)\) and chf \(\phi_{X}(\cdot)\) may not have a density. Let us recall from the convolution formula (A.2.2) that (if \(U\) has a density) a slightly perturbed version \(X_{a}\) of \(X\) is smoother than \(X\), in that

\[X_{a}\equiv X+aU\mbox{ always has a density }f_{a}(\cdot);\mbox{ and }X_{a} \to_{d}X\mbox{ as }a\to 0 \tag{5}\]

by Slutsky's theorem, since \(aU\to_{p}0\) as \(a\to 0\). Thus \(F(\cdot)=\lim F_{a}(\cdot)\) at each point in the continuity set \(C_{F}\) of \(F\). This is the key to the approach we will follow to establish an inversion formula.

**Theorem 4.1** (Uniqueness theorem): Every df on the line has a unique chf.

**Theorem 4.2** (Inversion formula): If an arbitrary rv \(X\) has df \(F_{X}(\cdot)\) and chf \(\phi_{X}(\cdot)\) we can always write

\[F_{X}(r_{2})-F_{X}(r_{1})=\lim_{a\to 0}\int_{r_{1}}^{r_{2}}f_{a}(y)\,dy\qquad \mbox{for all }r_{1}<r_{2}\mbox{ in }C_{F_{X}}, \tag{6}\]

where the density \(f_{a}(\cdot)\) of the rv \(X_{a}\equiv X+aU\) of (5) [with \(U\) as in (1)] is given by

\[f_{a}(y)=\int_{-\infty}^{\infty}e^{-iyv}\,\phi_{X}(v)cf_{W}(av)\,dv\qquad\mbox{ for all }y\in R. \tag{7}\]Theorem 4.3 (Inversion formula for densities)If a rv \(X\) has a chf \(\phi_{X}(\cdot)\) that satisfies the integrability condition

\[\int_{-\infty}^{\infty}|\phi_{X}(t)|\,dt<\infty,\]

then \(X\) has a uniformly continuous density \(f_{X}(\cdot)\) given by

\[f_{X}(x)=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{-itx}\phi_{X}(t)\,dt.\]

**Remark 4.1**: The uniqueness theorem can be restated as follows: The set of complex exponentials \({\cal G}\equiv\{e^{itx}\) for \(x\in R:t\in R\}\) is a determining class. This is so because knowing all values of \(\phi_{X}(t)={\rm E}e^{itX}\) allows the df \(F\) to be determined, via the inversion formula. \(\Box\)

Proof.: From the convolution formula (A.2.2) and \(X_{a}\equiv X+aU\) we have

\[\begin{array}{ll}f_{a}(y)=\int_{-\infty}^{\infty}\frac{1}{a}f_{U}(\frac{y-x} {a})\,dF_{X}(x)&\\ \mbox{(a)}&=c\int_{-\infty}^{\infty}\frac{1}{a}\phi_{W}(\frac{x-y}{a})\,dF_{X }(x)&\mbox{by (\ref{eq:1})}\\ &=(c/a)\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{i(x-y)w/a}f_{W}(w)\,dw \,dF_{X}(x)&\\ &=(c/a)\int_{-\infty}^{\infty}e^{-iyw/a}f_{W}(w)\int_{-\infty}^{\infty}e^{i(w/a )x}\,dF_{X}(x)\,dw&\mbox{by Fubini}\\ \mbox{(b)}&=(c/a)\int_{-\infty}^{\infty}e^{-iyw/a}\phi_{X}(w/a)f_{W}(w)\,dw=c \int_{-\infty}^{\infty}e^{-iyv}\phi_{X}(v)f_{W}(av)\,dv.\end{array}\]

Since \(X_{a}\to_{d}X\), at continuity points \(r_{1}<r_{2}\) of \(F\) we have (with \(X_{a}\cong F_{a}(\cdot)\))

\[F_{X}(r_{2})-F_{X}(r_{1})=\lim_{a\to 0}\{F_{a}(r_{2})-F_{a}(r_{1})\}=\lim_{a \to 0}\int_{r_{1}}^{r_{2}}f_{a}(y)\,dy.\]

This establishes theorems 4.1 and 4.2.

The particular formula given in (c) might look useless, but the mere fact that one can recover \(F_{X}\) from \(\phi_{X}\) via _some_ formula is enough to establish the important property of uniqueness. (See exercise 4.3 for some utility for (7).) We now turn to theorem 4.3, in which we have added a hypothesis that allows the previous formula to be manipulated into a simple and useful form.

Suppose that (8) holds, so that applying the DCT to (b) (using a constant times \(|\phi_{X}(\cdot)|\) as a dominating function) gives [recall the hypothesis on the \(f_{W}(\cdot)\) of (1)] as \(a\to 0\) that

\[f_{a}(y)\to f(y)\equiv[cf_{W}(0)]\int_{-\infty}^{\infty}e^{-iyv}\phi_{X}(v)\,dv,\]

since \(f_{W}\) is bounded and is continuous at \(0\). Note that uniform continuity of \(f\) follows from the bound

\[|f(y+h)-f(y)|=[cf_{W}(0)]|\int_{-\infty}^{\infty}[e^{-i(y+h)v}-e^{-iyv}]\phi_{ X}(v)\,dv|\]

\[\leq[cf_{W}(0)]\int_{-\infty}^{\infty}|e^{-ihv}-1||\phi_{X}(v)|\,dv\to 0\qquad\qquad \qquad\mbox{as}\,h\to 0,\]by applying the DCT (with dominating function \(2c\|f_{W}\|\phi_{X}(\cdot)|\)). The uniform convergence of \(f_{a}\) to \(f\) on any finite interval involves only an \(|f_{W}(0)-f_{W}(av)|\) term under the integral sign. That \(f\) really is the density of \(F_{X}\) follows from applying this uniform convergence in (c) to obtain

\[F_{X}(r_{2})-F_{X}(r_{1})=\int_{r_{1}}^{r_{2}}f(y)\,dy.\] (f)

The conclusion (9) holds since specifying \(U=W=Z\) gives

\[[cf_{W}(0)]=1/(2\pi)\] (as it always must). \[\square\] (g)

Esseen's inequality 9.7.1 below provides an important extension of theorem 4.2 by showing that if two chfs are sufficiently close over most of their domain, then the corresponding dfs will be uniformly close over their entire domain.

**Exercise 4.1**  Show that setting \(W=Z\) in line (c) of the previous proof leads, for any rv \(X\), to the alternative inversion formula

\[F_{X}(r_{2})-F_{X}(r_{1})=\lim_{a\to 0}\frac{1}{2\pi}\int_{-\infty}^{\infty} \frac{e^{-itr_{2}}-e^{-itr_{1}}}{-it}\phi_{X}(t)e^{-a^{2}t^{2}/2}\,dt \tag{10}\]

at all continuity points \(r_{1}<r_{2}\) of \(F_{X}(\cdot)\). [This is one possible alternative to (6).]

**Exercise 4.2**  Derive the chf of the Triangular(0, 1) density on the interval \([-1,1]\) (perhaps, add two appropriate uniform rvs). Then use theorem 4.3 to derive the chf of the de la Vallee Poussin density, while simultaneously verifying that the non-negative and real integrable function \((1-\cos x)/(\pi x^{2})\) really is a density. Following section 6, determine \(\mathrm{E}|X|\) when \(X\) has the de la Vallee Poussin density.

**Exercise 4.3** (Kernel density estimator): Since the rv \(X\) having df \(F_{X}(\cdot)\) and chf \(\phi_{X}(\cdot)\) may not have a density, we choose instead to estimate the density \(f_{a}(\cdot)\) of (5) and (7) using

\[\hat{f}_{a}(x)\equiv c\int_{-\infty}^{\infty}e^{-itx}\hat{\phi}_{X}(t)f_{W}\ \mbox{(at)}\ dt \tag{11}\]

[where \(f_{U}(\cdot)=c\phi_{W}(-\cdot)\) and where we now insist that \(\mu_{U}=0\) and \(\sigma_{U}^{2}\) is finite] with the _empirical_\(chf\,\hat{\phi}_{X}(\cdot)\) defined by

\[\hat{\phi}_{X}(t)\equiv\int_{-\infty}^{\infty}e^{itx}d\,\mathbb{F}_{n}(x)= \frac{1}{n}\sum_{j=1}^{n}e^{itX_{j}}\qquad\mbox{for}\ -\infty<t<\infty. \tag{12}\]

(a) Verify that \(\hat{f}_{a}(\cdot)\) is actually a kernel density estimator, meaning that it can be expressed as

\[\hat{f}_{a}(x)=\frac{1}{a}\int_{-\infty}^{\infty}f_{U}\left(\frac{x-y}{a} \right)d\,\mathbb{F}_{n}(y)=\frac{1}{n}\sum_{j=1}^{n}\frac{1}{a}f_{U}\left( \frac{x-X_{j}}{a}\right). \tag{13}\]

[This has statistical meaning, since we are averaging densities centered at each of the observations.](b) Show that \(\hat{f}_{a}(x)\) is always _unbiased_ (in that it has mean \(f_{a}(x)\)) and has a finite variance we can calculate; thus for all \(x\in R\) we can show that

\[\mathrm{E}\hat{f}_{a}(x)=f_{a}(x), \tag{15}\] \[\mathrm{Var}[\hat{f}_{a}(x)]=\frac{1}{n}\left\{\frac{1}{a^{2}}\int_{- \infty}^{\infty}f_{U}^{2}(\frac{x-y}{a})\,dF(y)-[f_{a}(x)]^{2}\right\}. \tag{14}\]

(c) Supposing that \(F_{X}(\cdot)\) has a density \(f(\cdot)\in C_{b}^{(2)}\), determine the order of the mean squared error

\[\mathrm{MSE}\{\hat{f}_{a}(x)\}\equiv\mathrm{Bias}^{2}\{\hat{f}_{a}(x)\}+ \mathrm{Var}[\hat{f}_{a}(x)]\equiv\{\mathrm{E}(\hat{f}_{a}(x))-f(x)\}^{2}+ \mathrm{Var}[\hat{f}_{a}(x)] \tag{16}\]

of \(\hat{f}_{a}(x)\), viewed as an estimator of \(f(x)\). (It is intended that you rewrite (16) by expanding \(f_{a}(x)\) in a Taylor series in "\(a\)" (valid for \(f(\cdot)\in C_{b}^{(2)}\)), and then analyze the magnitude of (16) for values of "\(a\)" near 0. It might also be useful to relabel \(f_{U}(\cdot)\) by \(\psi\) now so that your work refers to any kernel density estimator, right from the beginning. This will avoid "starting over" in part (f).) Show that this MSE expression is of order \(n^{-4/5}\) for \(f(\cdot)\in C_{b}^{(2)}\) when \(a\) is of order \(n^{-1/5}\), and that this is the minimal attainable order.

(d) Note that the choice \(U=Z\) (or \(U=T\)) leads to an \(\hat{f}_{a}(\cdot)\) that is the sum of \(n\) normal (or triangular) densities that are centered at the \(n\) data points and that have a scale parameter directly proportional to \(a\).

(e) Obtain an expression for \(\lim_{a\to 0}a^{4/5}\) MSE \(\{\hat{f}_{a}(x)\}\) in terms of \(f(x),f^{\prime}(x)\), and \(f^{\prime\prime}(x)\) when \(a=n^{-1/5}\) (and obtain it for both of the choices \(U=Z\) and \(U=T\)).

(f) We could also motivate the idea of a kernel density estimator based on (13) alone. How much of what we have done still carries over for a general kernel? What properties should a good kernel exhibit? What can you prove in this more general setting? (Now, for sure, replace \(f_{U}\) by a function labeled \(\psi\). A simple sentence that specifies the requirements on \(\psi\) should suffice.)

**Exercise 4.4**  Use the table of chfs above to show in what sense the sums of independent Binomial, Poisson, NegBiT, Normal, Cauchy, Chisquare and Gamma rvs have distributions that again belong to the same family. (Recall section A.2, noting that chfs have allowed the complicated operation of convolution of dfs or densities to be replaced by the simple operation of multiplication of chfs.)

## 5 The Continuity Theorem

**Theorem 5.1** (Continuity theorem for chfs; Cramer-Levy): (i) If \(\phi_{n}\to\phi\) where \(\phi\) is continuous at \(0\), then \(\phi\) is the chf of a bona fide df \(F\) and \(F_{n}\to_{d}F\).

(ii) \(F_{n}\to_{d}F\) implies \(\phi_{n}\to\phi\) uniformly on any finite interval \(|t|\leq T\).

**Inequality 5.1** (Chf bound on the tails of a df): For any df \(F\) we have

\[P(|X|\geq\lambda)\leq 7\lambda\int_{0}^{1/\lambda}[1-\operatorname{Real}\!\phi(t) ]\,dt\qquad\text{for all }\lambda>0.\]

**Proof.** Now,

(a) \[\lambda\int_{0}^{1/\lambda}[1-\operatorname{Real}\!\phi(t)]\,dt= \lambda\int_{0}^{1/\lambda}\int_{-\infty}^{\infty}[1-\cos(tx)]\,dF(x)\,dt\] \[\qquad=\int_{-\infty}^{\infty}\lambda\int_{0}^{1/\lambda}[1- \cos(tx)]\,dt\,dF(x)\] \[\qquad=\int_{-\infty}^{\infty}\{\lambda t[1-\frac{\sin(xt)}{xt}] \}]_{0}^{1/\lambda}\,dF(x)\] \[\qquad=\int_{-\infty}^{\infty}[1-\frac{\sin(x/\lambda)}{(x/ \lambda)}]\,dF(x)\] \[\qquad\geq\int_{[|x|/\lambda\geq 1]}[1-\sin(x/\lambda)/(x/\lambda)] \,dF(x)\] (b) \[\qquad=\inf_{[|y|\geq 1]}[1-\sin(y)/y]P(|X|\geq\lambda)=[1-\sin(1)]P (|X|\geq\lambda)\] (c) \[\qquad=(.1585\ldots)P(|X|\geq\lambda)\geq P(|X|\geq\lambda)/7,\]

as claimed. (It may be interesting to compare this to the Chebyshev inequality.) [This idea will be carried further in (10.5.9) and (10.5.10).] \(\square\)

**Proof.** Consider theorem 5.1. (i) The uniqueness theorem for chfs shows that the collection \(\mathcal{G}\) of complex exponential functions form a determining class, and the expectations of these are hypothesized to converge. It thus suffices (by the kinder and gentler Helly-Bray theorem (theorem 9.2.1(i)(a))) to show that \(\{F_{n}:n\geq 1\}\) is tight. Now,

(a) \[\overline{\lim}_{n\to\infty}\,P(|X_{n}|\geq\lambda)\leq\overline{\lim}_{n\to \infty}\,7\lambda\int_{0}^{1/\lambda}[1-\operatorname{Real}\,\phi_{n}(t)]\,dt\] \[\qquad=7\lambda\int_{0}^{1/\lambda}[1-\operatorname{Real}\,\phi(t)]\,dt\] \[\qquad\text{by the DCT, with dominating function 2}\] (b) \[\qquad\to 0\quad\text{ as }\lambda\to\infty,\]

so that \(\{F_{n}:n\geq 1\}\) is tight.

(ii) Now replacing \(X_{n}\to_{d}X\) by versions \(Y_{n}\to_{a.s.}Y\) (and using Skorokhod's construction) gives for \(|t|\leq T\) that

(c) \[|\phi_{n}(t)-\phi(t)|\leq\int|e^{itY_{n}}-e^{itY}|\,dP\] \[\qquad\leq\int|e^{it(Y_{n}-Y)}-1|\,dP\] \[\qquad\leq\int\sup_{|t|\leq T}|e^{it(Y_{n}-Y)}-1|\,dP\] (d) \[\qquad\to 0\qquad\qquad\text{ as }\qquad\sup\{|it(Y_{n}-Y)|:|t|\leq T\}\leq T|Y_{n}-Y|\to 0\]

by the DCT, with dominating function 2. \(\square\)

#### Higher Dimensions

If \(X_{1},\ldots,X_{k}\) are rvs on \((\Omega,{\cal A},P)\), then the \(B_{k}\)-\({\cal A}\)-mapping \({\sf X}\equiv(X_{1},\ \ldots,X_{k})^{\prime}\) from \(\Omega\) to \(R_{k}\) induces a measure \(P_{\sf X}\) on \((R_{k},B_{k})\). The characteristic function of \({\sf X}\) is

\[\phi_{\sf X}({\sf t})\equiv{\rm E}\,e^{it^{\prime}{\sf X}}={\rm E}\,e^{i[t_{1} X_{1}+\cdots+t_{k}X_{k}]}\quad\mbox{for ${\sf t}\equiv(t_{1},\ldots,t_{k})^{\prime}\in R_{k}$}. \tag{2}\]

Without further explanation, we state simply that the uniqueness theorem (that \(\{g_{\sf t}\equiv\exp(i{\sf t^{\prime}{\sf x}})\) for all \({\sf x}\in R_{n}:\,{\sf t}\in R_{n}\}\) is a determining class) and the Cramer-Levy continuity theorem still hold, based on minor modifications of the previous proof. We also remark that all equivalences of \(\to_{d}\) in theorem 1.1 are still valid. But we now take up an extremely useful approach to showing convergence in distribution in higher dimensions.

The characteristic function of the one-dimensional linear combination \(\vec{\lambda}^{\prime}{\sf X}\) is

\[\phi_{\vec{\lambda}^{\prime}{\sf X}}(t)={\rm E}\,e^{i[t\lambda_{1}X_{1}+\cdots +t\lambda_{k}X_{k}]}\mbox{ for $t\in R$}. \tag{3}\]

Comparison of this with (2) shows that knowing the joint chf \(\phi_{\vec{\lambda}}({\sf t})\) for all \({\sf t}\in R_{k}\) is equivalent to knowing the one-dimensional chf \(\phi_{\vec{\lambda}^{\prime}{\sf X}}(t)\) for all \(t\in R\) and \(\vec{\lambda}\in R_{k}\) for which \(|\vec{\lambda}|=1\). This immediately yields the following useful result.

**Theorem 5.2** (Cramer-Wold device): If \({\sf X}_{n}\equiv(X_{n1},\ldots,X_{nk})^{\prime}\) satisfy

\[\phi_{\vec{\lambda}^{\prime}{\sf X}_{n}}(t)\to\phi_{\vec{\lambda}^{\prime}{\sf X }}(t)\qquad\mbox{for all $t\in R$}\qquad\mbox{and for each $\vec{\lambda}\in R _{k}$}, \tag{4}\]

then \({\sf X}_{n}\to_{d}{\sf X}\). (It suffices to show (4) for all unit vectors \(\vec{\lambda}\) in \(R_{k}\).) [In fact, we only require that \(\vec{\lambda}^{\prime}{\sf X}_{n}\to_{d}\vec{\lambda}^{\prime}{\sf X}\) for all such \(\vec{\lambda}\) (no matter what method we use to show it), as such a result implies (4).]

**Theorem 5.3**: The rvs \(X_{1},\ldots,X_{k}\) are independent if and only if the joint chfs satisfy \(\phi_{\sf X}(t_{1},\ldots,\ t_{k})=\prod_{1}^{k}\phi_{X_{i}}(t_{i})\).

**Exercise 5.1\({}^{*}\)**: Prove the claims made below (2) for the \(n\)-dimensional chf \(\phi_{\sf X}\).
**Exercise 5.2**: Prove theorem 5.3.

## 6 Elementary Complex and Fourier Analysis

**Lemma 6.1** (Taylor expansions of \(\log(1+z)\) and \(e^{z}\)): [Note that \(\log z\) is a many-valued function of a complex \(z=re^{i\theta}\); any of \((\log r)+i[\theta+2\pi m]\) for \(m=0,\pm 1,\pm 2,\ldots\) will work for \(\log z\). However, when we write \(\log z=\log r+i\theta\), we will always suppose that \(-\pi<\theta\leq\pi\). Moreover, we denote this unique determination by \(\mathrm{Log}\;z\); this is the _principal branch_.] The Taylor series expansion of \(\mathrm{Log}\,(1+z)\) gives

\[\begin{split}&|\mathrm{Log}(1+z)-\sum\nolimits_{k=1}^{m-1}(-1)^{ k-1}z^{k}/k|=|\sum\nolimits_{k=m}^{\infty}(-1)^{k-1}z^{k}/k|\\ &\quad\leq\frac{|z|^{m}}{m}(1+|z|+|z|^{2}+\cdots)\leq\frac{|z|^{m }}{m(1-|z|)}\end{split} \tag{1}\]

for \(|z|<1\). Thus

\[\mathrm{Log}\,(1+z)-z|\leq|z|^{2}/(2(1-\theta))\qquad\text{for}\quad|z|\leq \theta<1. \tag{2}\]

From another Taylor series expansion we have for all \(z\) that

\[|e^{z}-\sum\nolimits_{k=0}^{m-1}z^{k}/k!|=|\sum\nolimits_{k=m}^{\infty}z^{k}/k!|\leq|z|^{m}\sum\nolimits_{j=0}^{\infty}\frac{|z|^{j}}{j!}\frac{j!}{(j+m)!} \leq\frac{|z|^{m}e^{|z|}}{m!}. \tag{3}\]

**Lemma 6.2** (Taylor expansion of \(e^{it}\)): [Let \(m\geq 0\) and \(0\leq\delta\leq 1\) (and set the constant \(K_{0,0}=2\), below). Then for all real \(t\) we have

\[\left|e^{it}-\sum\limits_{k=0}^{m}\frac{(it)^{k}}{k!}\right|\leq\frac{\delta 2 ^{1-\delta}}{(m+\delta)\cdots(2+\delta)(1+\delta)(0+\delta)}|t|^{m+\delta} \equiv K_{m,\delta}|t|^{m+\delta}. \tag{4}\]

**Proof.** : The proof is by induction. For \(m=0\) we have both \(|e^{it}-1|\leq 2\leq 2|t/2|^{\delta}\) for \(|t/2|\geq 1\), and (since \(\int_{0}^{t}ie^{is}ds=\int_{0}^{t}(i\cos s-\sin s)\,ds=e^{it}-1\))

(a) \[|e^{it}-1|\leq|\int_{0}^{t}ie^{is}ds|\leq\int_{0}^{|t|}ds=|t|\leq 2|t/2|^{\delta }\qquad\text{for $|t/2|\leq 1$;}\]

so that (4) holds for \(m=0\). We now assume that (4) holds for \(m-1\), and we will verify that it thus holds for \(m\). We again use \(e^{it}-1=i\int_{0}^{t}e^{is}\,ds\) and further note that \(i\sum\nolimits_{k=0}^{m-1}\int_{0}^{t}[(is)^{k}/k!]\,ds=\sum\nolimits_{k=0}^{m -1}(i^{k+1}/k!)\int_{0}^{t}s^{k}ds=\sum\nolimits_{1}^{m}(it)^{k}/k!\) to obtain

(b) \[\leq K_{m-1,\delta}\int_{0}^{|t|}s^{m-1+\delta}\,ds\qquad\qquad\text{by the induction step}\]

(c) \[=K_{m,\delta}|t|^{m+\delta}.\]

[See Chow and Teicher (1997).] (The next inequality is immediate.) 

**Inequality 6.1** (Moment expansion inequality): [Suppose \(\mathrm{E}|X|^{m+\delta}<\infty\) for some \(m\geq 0\) and \(0\leq\delta\leq 1\). Then

\[\left|\phi(t)-\sum\limits_{k=0}^{m}\frac{(it)^{k}}{k!}\mathrm{E}X^{k}\right| \leq K_{m,\delta}\,|t|^{m+\delta}\mathrm{E}|X|^{m+\delta}\qquad\text{for all $t$.} \tag{5}\]

**Some Alternative Tools**

**Lemma 6.3** (The first product lemma) For all \(n\geq 1\), let complex \(\beta_{n1},\ldots,\beta_{nn}\) satisfy the following conditions:

(a) \(\beta_{n}\equiv\sum_{1}^{n}\beta_{nk}\to\beta\quad\mbox{as $n\to\infty$}\).

(b) \(\delta_{n}\equiv[\max_{1\leq k\leq n}\lvert\beta_{nk}\rvert]\to 0\).

(c) \(M_{n}\equiv\sum_{k=1}^{n}\lvert\beta_{nk}\rvert\quad\mbox{satisfies}\quad \delta_{n}M_{n}\to 0\).

Then (compare this with the stronger Lemma 8.1.4, which requires all \(\beta_{nk}\geq 0\))

\[\prod_{k=1}^{n}(1+\beta_{nk})\to e^{\beta}\quad\mbox{as $n\to\infty$}. \tag{6}\]

**Proof.** When \(0<\delta_{n}\leq\frac{1}{2}\) (and we are on the principal branch), (2) gives

(p) \(\lvert\sum_{k=1}^{n}\mbox{Log}\,(1+\beta_{nk})-\sum_{k=1}^{n}\beta_{nk}\rvert \leq\sum_{k=1}^{n}\lvert\beta_{nk}\rvert^{2}\leq\delta_{n}M_{n}\to 0\).

Thus

(q) \(\sum_{k=1}^{n}\mbox{Log}(1+\beta_{nk})\to\beta\qquad\mbox{as $n\to\infty$}\).

Moreover, (q) shows that

(r) \(\prod_{k=1}^{n}(1+\beta_{nk})=\exp(\mbox{Log}(\prod_{k=1}^{n}(1+\beta_{nk}))) \ =\exp(\sum_{k=1}^{n}\mbox{Log}(1+\beta_{nk}))\)

\(\to\exp(\beta)\),

and this gives (6). [See Chung (1974).] (Recall lemma 8.1.4.) \(\Box\)

**Lemma 6.4** (The second product lemma) If \(z_{1},\ldots,z_{n}\) and \(w_{1},\ldots,w_{n}\) denote complex numbers with modulus at most \(1\), then

\[\left\lvert\prod_{k=1}^{n}z_{k}-\prod_{k=1}^{n}w_{k}\right\rvert\leq\sum_{k=1} ^{n}\lvert z_{k}-w_{k}\rvert. \tag{7}\]

**Proof.** This is trivial for \(n=1\). We will use induction. Now,

(a) \(\lvert\prod_{k=1}^{n}z_{k}-\prod_{k=1}^{n}w_{k}\rvert\leq\lvert z_{n}\rvert \left\lvert\prod_{k=1}^{n-1}z_{k}-\prod_{k=1}^{n-1}w_{k}\right\rvert+\lvert z _{n}-w_{n}\rvert\left\lvert\prod_{k=1}^{n-1}w_{k}\right\rvert\)

(b) \(\leq\left\lvert\prod_{k=1}^{n-1}z_{k}-\prod_{k=1}^{n-1}w_{k}\right\rvert+\lvert z _{n}-w_{n}\rvert\cdot\prod_{k=1}^{n-1}1\leq\sum_{k=1}^{n-1}\lvert z_{k}-w_{k} \rvert+\lvert z_{n}-w_{n}\rvert\)

by the induction step. [See most newer texts.] \(\Box\)

**Inequality 6.2** (Moment expansions of chfs) Suppose \(0<E|X|^{m}<\infty\) for some \(m\geq 0\). Then (for some \(0\leq g(t)\leq 1\)) the chf \(\phi\) of \(X\) satisfies

\[\left\lvert\phi(t)-\sum_{k=0}^{m}\frac{(it)^{k}}{k!}\mbox{E}X^{k}\right\rvert \leq\frac{3}{m!}\lvert t\rvert^{m}\mbox{E}|X|^{m}g(t)\qquad\mbox{where $g(t) \to 0$ as $t\to 0$}. \tag{8}\]

**Proof.** Use the real expansions for \(\sin\) and \(\cos\) to obtain

(a) \[e^{itx}=\cos(tx)+i\sin(tx)=\sum_{k=0}^{m-1}\frac{(itx)^{k}}{k!}+\frac{(itx)^{m}}{m! }[\cos(\theta_{1}tx)+i\sin(\theta_{2}tx)]\] (b) \[=\sum_{k=0}^{m}\frac{(itx)^{k}}{k!}+\frac{(itx)^{m}}{m!}[\cos(\theta_{1}tx)+i \sin(\theta_{2}tx)-1].\]

Here, we have some \(\theta_{1},\theta_{2}\) with \(0\leq|\theta_{1}|\vee|\theta_{2}|\leq 1\). Then (8) follows from (b) via

\[\lim_{t\to 0}\!\mbox{E}|X^{m}[\cos(\theta_{1}tX)-1+i\sin(\theta_{2}tX)]|=0, \tag{9}\]

by the DCT with dominating function \(3|X|^{m}\). [See Breiman (1968).] \(\Box\)

**Inequality 6.3** (Summary of useful facts): Let \(X\cong(0,\sigma^{2})\). Result (8) then gives the highly useful

\[|\phi(t)-(1-\frac{1}{2}\sigma^{2}t^{2})|\leq\frac{3}{2}\sigma^{2}t^{2}g(t) \mbox{ where }g(t)\to 0\,\mbox{as}\,t\to 0,\]

with \(0\leq g(t)\leq 1\). Applying this and (5) gives (since \(K_{2,1}=\frac{1}{6}\), and since \(K_{1,1}=\frac{1}{2}\) allows \(\frac{1}{2}\,\sigma^{2}t^{2}\) to replace \(\frac{3}{2}\,\sigma^{2}t^{2}g(t)\))

\[|\phi(t)-(1-\frac{1}{2}\sigma^{2}t^{2})|\leq\frac{1}{2}\sigma^{2}t^{2}\wedge \frac{1}{6}\mbox{E}|X|^{3}|t|^{3}\qquad\mbox{for all }t\in R. \tag{10}\]

**Exercise 6.1** (Distributions determined by their moments):

(a) Suppose that \(\mbox{E}|X|^{n}<\infty\). Then the \(n\)th derivative \(\phi^{(n)}(\cdot)\) is a continuous function given by \(\phi^{(n)}(t)=i^{n}\mbox{E}(X^{n}e^{itX})\), so that \(\mbox{E}X^{n}=i^{-n}\phi^{(n)}(0)\).

(b) The series \(\psi(t)=\sum_{0}^{\infty}(it\ )^{k}\mbox{E}(X^{k})/k!\) has radius of convergence \(R\equiv 1/(eL)\), where \(L=\overline{\lim}_{k}\,|\mu_{k}|^{1/k}/k=\overline{\lim}_{k}(\mbox{E}|X|^{k}) ^{1/k}/k=\overline{\lim}_{\mu_{2}}t^{1/2k}/(2k)\); use the root test.

(c) The series in (b) has the same \(R>0\) if \(\sum_{k=0}^{\infty}\mu_{2k}\ t^{2k}/(2k)!<\infty\) for some \(t>0\).

(d) The series (b) converges for \(|t|<r\) if and only if \(\mbox{E}\exp(t|X|)<\infty\) for \(|t|<r\) if and only if \(\mbox{E}\exp(tX)<\infty\) for \(|t|<r\)

(e) If the radius of convergence in (b) is strictly positive, then the distribution having the stated moments is uniquely determined by its moments \(\mu_{k}\).

(f) Show that the Normal(0, 1) distribution is uniquely determined by its moments.

(g) Show that any Gamma(\(r,1\)) distribution is uniquely determined by its moments.

(h) Show that it is valid to expand the mgs of Normal(0, 1) and Gamma(\(r,1\)) to compute their moments. Do it.

**Exercise 6.2** (a) If \(\phi^{\prime\prime}(0)\) is finite, then \(\sigma^{2}\) is finite. Prove this.

(b) In fact, if \(\phi^{(2k)}(0)\) is finite, then \(\mbox{E}X^{2k}<\infty\). Prove this.

(*) Appeal to Exercise 3.3.

**Exercise 6.3** (Bounds on \((1-x/n)^{n}\)): (i) Use \((1+t)\leq e^{t}\leq 1/(1-t)\) for \(0\leq t<1\) at \(t=x/n\) to show that

\[0\leq e^{-x}-(1-\frac{x}{n})^{n}\leq\frac{x^{2}}{n}e^{-x} \mbox{ for }\,0\leq x<n.\]

(ii) (Hall and Wellner) Show that

\[2e^{-2}\leq n\sup_{x\geq 0}|e^{-x}-(1-x/n)^{n}1_{[0,n]}(x)|\leq(2+n^{-1})e^{-2} \,\mbox{ for all}\,n\geq 1.\]

**Results from Fourier Analysis**

On some occasions we will need to know the behavior of \(\phi(t)\) for \(|t|\) large.

**Lemma 6.5** (Riemann-Lebesgue lemma): If \(\int_{-\infty}^{\infty}\!|g(x)|\,dx<\infty\), then

\[\int_{-\infty}^{\infty}\!e^{itx}g(x)\,dx\to 0\quad\mbox{as $t\to\infty$}. \tag{11}\]

**Proof.** Now, \(\Psi\equiv\{\psi\equiv\sum_{1}^{m}c_{i}1_{(a_{i},b_{i}]}:\,a_{i},b_{i},c_{i}\in R\) and \(m\geq 1\}\) is dense in \({\cal L}_{1}\) by theorem 3.5.8; that is, if \(\int_{-\infty}^{\infty}\!|g(x)|\,dx<\infty\), then there exists \(\psi\in\Psi\) such that \(\int_{-\infty}^{\infty}|g-\psi|\,dx<\epsilon\). Thus \(\gamma(t)\equiv|\int_{-\infty}^{\infty}e^{itx}g(x)\,dx|\) satisfies

\[\gamma(t)\leq\int_{-\infty}^{\infty}|e^{itx}||g(x)-\psi(x)|\,dx+|\int_{-\infty }^{\infty}e^{itx}\psi(x)\,dx|\]

(a) \[\leq\epsilon+\sum_{1}^{m}\!|c_{i}||\int_{a_{i}}^{b_{i}}e^{itx}\,dx|.\]

It thus suffices to show that for any \(a,b\) in \(R\) we have

\[\int_{a}^{b}\!\!e^{itx}\,dx\to 0\qquad\mbox{as}\quad|t|\to\infty. \tag{12}\]

A quick picture of sines and cosines oscillating very fast (and canceling out over the interval) shows that (12) is trivial. (Or write \(e^{itx}=\cos(tx)+i\sin(tx)\) and compute the integrals.) \(\Box\)

**Lemma 6.6** (Tail behavior of chfs)

(i) If \(F\) has density \(f\) with respect to Lebesgue measure, then \(|\phi(t)|\to 0\) as \(|t|\to\infty\).

(ii) If \(F\) has \(n+1\) integrable derivatives \(f,f^{\prime},\ldots,f^{(n)}\) on \(R\), then

\[|t|^{n}|\phi(t)|\to 0\qquad\mbox{as}\quad|t|\to\infty. \tag{13}\]

**Proof.** The fact that \(|\phi(t)|\to 0\) as \(|t|\to\infty\) follows from the Reimann-Lebesgue lemma, since \(f\) is integrable. Since \(f\) is absolutely continuous and a density, it follows that that \(f(x)\to 0\) as \(|x|\to\infty\). Then use

\[\phi(t)=\int e^{itx}f(x)\,dx=\int f(x)d(e^{itx}/it)\]

(a) \[=(e^{itx}/it)f(x)|_{-\infty}^{\infty}-\int e^{itx}f^{\prime}(x)\,dx/(it)\]

(b) \[=-\int e^{itx}f^{\prime}(x)\,dx/(it)\quad\mbox{ with }\quad f^{\prime}( \cdot)\in{\cal L}_{1},\]

using \(f(x)\to 0\) as \(|x|\to\infty\) in going from (a) to (b). (Note exercise 6.4 below.) Applying the Riemann-Lebesgue lemma to (b) gives \(|t||\phi(t)|\to 0\) as \(|t|\to\infty\). Keep on integrating by parts and applying the Riemann-Lebesgue lemma. \(\Box\)

**Exercise 6.4**: Verify lemma 6.2(ii) when \(n=1\).

#### Other Alternative Tools\({}^{o}\)

Then chf always exists, so it can always be used. However, if \(X\geq 0\) or if \(X\) is integer valued, _then_ Laplace transforms or probability generating functions offer more elementary tools.

**Exercise 6.5\({}^{*}\)** (Laplace transform) Let \({\cal F}^{+}\) denote the class of all dfs \(F\) having \(F_{-}(0)=0\). For any df \(F\in{\cal F}^{+}\) we define the _Laplace transform_\(L\) of \(F\) by

\[L(\lambda)={\rm E}e^{-\lambda X}=\int_{0}^{\infty}e^{-\lambda x}\,dF(x)\mbox{ for }\lambda\geq 0. \tag{14}\]

(a) Establish an analogue of proposition 3.1(a), (b), (c), and (g).

(b) (Uniqueness) Show that each df in \({\cal F}^{+}\) has a unique Laplace transform.

(c) (Continuity) Let \(X_{n}\cong F_{n}\in{\cal F}^{+}\). If \(L_{n}(\lambda)\to(\mbox{some }L(\lambda))\) for all \(\lambda\geq 0\) with \(L(\cdot)\) right continuous at \(0\), then \(L\) is the Laplace transform of a df \(F\in{\cal F}^{+}\) for which the convergence in distribution \(F_{n}\to_{d}F\) holds.

(d) Establish analogues of inequality 6.1 on moment expansions.

**Exercise 6.6\({}^{*}\)** (Probability generating function) Let \({\cal F}^{I}\) denote the class of all dfs \(F\) assigning mass \(1\) to the integers \(0,1,2,\ldots\). For any df \(F\in{\cal F}^{I}\) we define the _probability generating function_\(g\) of \(F\) by

\[g(z)={\rm E}\,z^{X}=\sum\nolimits_{k=0}^{\infty}\!p_{k}\,z^{k}\qquad\mbox{for all complex $z$ having $|z|\leq 1$}. \tag{15}\]

(a) Establish an analogue of proposition 3.1.

(b) (Uniqueness) Show that each df \(F\) in \({\cal F}^{I}\) has a unique generating function.

(c) (Continuity) Let \(X_{n}\cong F_{n}\in{\cal F}^{I}\). If \(g_{n}(z)\to(\mbox{some }g(z))\) for all \(|z|\leq 1\) with \(g(\cdot)\) continuous at \(1\), then \(g\) is the generating function of a df \(F\) in \({\cal F}^{I}\) for which \(F_{n}\to_{d}F\).

**Exercise 6.7\({}^{o}\)** (Cumulant generating function) The _cumulant generating function_\(\psi_{X}(\cdot)\) of a rv \(X\) is defined by

\[\psi_{X}(t)\equiv{\rm Log}\,\phi_{X}(t)={\rm Log}\,{\rm E}(e^{itX}), \tag{16}\]

and is necessarily finite for \(t\)-values in some neighborhood of the origin.

(a) Temporarily suppose that all moments of \(X\) are finite. Let \(\mu_{k}\equiv{\rm E}(X-\mu)^{k}\) denote the \(k\)-th central moment, for \(k\geq 1\). Then when \(\mu={\rm E}X=0\) and with \(\sigma^{2}\equiv\mu_{2}\), we have the formal expansion

\[\phi_{X}(t)=1-t^{2}\sigma^{2}/2+(it)^{3}\mu_{3}/3!+(it)^{4}\mu_{4}/4!+\cdots \equiv 1+z.\]

Verify that further _formal_ calculations based on this yield

\[\psi_{X}(t)={\rm Log}\,\phi_{X}(t)={\rm Log}\,(1+z)=z-z^{2}/2+z^{3}/3+\cdots \tag{17}\]

\[=(it)^{2}\mu_{2}/2!+(it)^{3}\mu_{3}/3!+(it)^{4}(\mu_{4}-3\mu_{2}^{2})/4!+\cdots \tag{18}\]

\[=(it)^{2}\sigma^{2}/2!+(it)^{3}\mu_{3}/3!+t^{4}(\mu_{4}-3\sigma^{4})/4!+\cdots \tag{19}\]

\[\equiv\sum_{j=2}^{\infty}(it)^{j}\kappa_{j}/j!, \tag{20}\]where \(\kappa_{j}\) is called the \(j\)th _cumulant_ of \(X\). Note that for independent rvs \(X_{1},\ldots,X_{n}\),

(19) (the \(j\)th cumulant of \(\sum_{k=1}^{n}X_{k}\)) = \(\sum_{k=1}^{n}\) (the \(j\)th cumulant of \(X_{k}\)),

which is nice. Then verify that in the iid case, the third and fourth cumulants of the standardized rv \(Z_{n}\equiv\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\) are

\[\gamma_{1}/\sqrt{n}\equiv(\mu_{3}/\sigma^{3})/\sqrt{n}\qquad\mbox{and}\qquad \gamma_{2}/n\equiv(\mu_{4}/\sigma^{4}-3)/n, \tag{20}\]

where \(\gamma_{1}\) measures skewness and \(\gamma_{2}\) measures tail heaviness. [This is particularly nice; it shows that the effect (on the distribution of \(\bar{X}_{n}\)) of skewness disappears at rate \(1/\sqrt{n}\), while the effect of tail heaviness disappears at rate \(1/n\).]

(b) Finally, if only E\(|X|^{m}<\infty\) for some \(m\geq 1\) is known, show that in a sufficiently small neighborhood of the origin

\[|\psi(t)-\sum_{j=2}^{m}\kappa_{j}(it)^{j}/j!|\leq c_{m}|t|^{m}{\rm E}|X|^{m} \delta_{m}(t), \tag{21}\]

where \(\delta_{m}(t)\searrow 0\) as \(t\searrow 0\), and

\[|\psi(t)-\sum_{j=2}^{m-1}\kappa_{j}(it)^{j}/j!|\leq\bar{c}_{m}|t|^{m}{\rm E}|X| ^{m} \tag{22}\]

for some universal constant \(\bar{c}_{m}\). The exercise is to establish carefully that all of this is true.

## 7 Esseen's Lemma

Let \(G\) denote a fixed function having \(G(-\infty)=0,G(+\infty)=1\), having derivative \(g\) on the real line for which \(|g(\cdot)|\) is bounded by some constant \(M\), having \(\int_{-\infty}^{\infty}xg(x)\ dx=0\), and then let \(\psi(t)\equiv\int_{-\infty}^{\infty}e^{itx}g(x)\ dx\). Let \(F\) denote a general df having mean \(0\), and let \(\phi\) denote its characteristic function. We wish to estimate \(\|F-G\|=\sup_{-\infty<x<\infty}|F(x)-G(x)|\) in terms of the distance between \(\phi\) and \(\psi.\) Roughly speaking, the next inequality says that if \(\phi\) and \(\psi\) are sufficiently close over most of their domain, then \(\|F-G\|\) will be small. [In the initial application of this Esseen's lemma, we will take \(G,g,\psi\) to be the \(N(0,1)\)df, density, and chf. In this context, the constant of (1) is \(24\|g\|/\pi=24M/\pi=24/(\sqrt{2\pi}\pi)=3.047695\ldots\).]

##### Inequality 7.1 (Esseen's lemma)

Let \(F\) and \(G\) be as above. For any \(a>0\) we have the uniform bound

\[\|F-G\|\leq\frac{1}{\pi}\int_{-a}^{a}|\frac{\phi(t)-\psi(t)}{t}|\,dt+\frac{24 ||g\|}{\pi a}. \tag{1}\]

##### Proof.

The key to the technique is to smooth by convolving \(F\) and \(G\) with the df \(H_{a}\) whose density \(h_{a}\) and characteristic function \(\gamma_{a}\) are given by

\[h_{a}(x)=\frac{1-\cos(ax)}{\pi ax^{2}}\quad\mbox{on $R$}\quad\mbox{and}\qquad \gamma_{a}(t)=\cases{1-|t|/a&if $|t|\leq a$,\cr 0&if $|t|>a$.\cr} \tag{2}\]

This \(h_{a}\) is the density of \(V/a\), when \(V\) has the de la Vallee density. Let \(F_{a}\) and \(G_{a}\) denote the convolutions of \(F\) and \(G\) with \(H_{a}\), for "\(a\)" large. We will now show that

\[\|F-G\|\leq 2\|F_{a}-G_{a}\|+24\|g\|/(\pi a). \tag{3}\]

Let \(\Delta\equiv F-G\). Now, \(\Delta(x)=\Delta_{+}(x)\) and \(\Delta_{-}(x)\) exist for all \(x\); thus there exists \(x_{o}\) such that either \(D\equiv\|F-G\|=|\Delta(x_{o})|\) or \(D=|\Delta_{-}(x_{o})|\). Without loss of generality, we suppose that \(D=|\Delta(x_{o})|\)(just replace \(X,Y\) by \(-X,-Y\) if not). Note figure 7.1. Without loss of generality, we act below as though \(\Delta(x_{o})>0\), and we let \(z_{o}>x_{o}\). (If \(\Delta(x_{o})<0\), then let \(z_{o}<x_{o}\)). Now, since \(F\) is \(\nearrow\) and \(g\) is bounded by \(M\), we have

(a) \[\Delta(z_{o}-x)\geq D/2+Mx\qquad\mbox{for $|x|\leq\epsilon=\frac{D}{2M}$,}\]

where \(\epsilon\equiv D/2M\) and \(z_{o}\equiv x_{o}+\epsilon\). Trivially (since \(D\) was the supremum),

(b) \[\Delta(z_{o}-x)\geq-D\qquad\mbox{for $|x|>\epsilon$.}\]

Thus, with \(\Delta_{a}\equiv F_{a}-G_{a}\), using (a) and (b) gives

(c) \[\|F_{a}-G_{a}\|\geq\Delta_{a}(z_{o})=\int_{-\infty}^{\infty}\Delta(z_{o}-x)h_ {a}(x)\ dx\qquad\mbox{by the convolution formula}\]

(d) \[\geq\int_{-\epsilon}^{\epsilon}[D/2+Mx]h_{a}(x)\ dx-D\int_{[|x|> \epsilon]}h_{a}(x)\ dx\] \[=(D/2)[1-\int_{[|x|>\epsilon]}h_{a}(x)\ dx]+M\cdot 0-D\int_{[|x|> \epsilon]}h_{a}(x)\,dx\] \[\qquad\mbox{since $xh_{a}(x)$ is odd}\]\[=(D/2)-(3D/2)\int_{[|x|>\epsilon]}h_{a}(x)\,dx\geq(D/2)-(12M/\pi a)\]

(e) \[=\|F-G\|/2-(12M/\pi a),\]

(which is (3)), since

(f) \[\int_{[|x|>\epsilon]}h_{a}(x)\ dx\leq 2\int_{\epsilon}^{\infty}(2/\pi ax^{2})\ dx=4/(\pi a \epsilon)=8M/(\pi aD).\]

Figure 7.1 Bounds for Esseen's lemma

We now bound \(\|F_{a}-G_{a}\|\). By the Fourier inversion formula, \(F_{a}\) and \(G_{a}\) have bounded continuous "densities" that satisfy

\[f_{a}(x)-g_{a}(x)=\int_{-a}^{a}e^{-itx}[\phi(t)-\psi(t)]\gamma_{a}(t)\,dt/(2\pi). \tag{4}\]

From this we suspect that

\[\Delta_{a}(x)=\frac{1}{2\pi}\int_{-a}^{a}e^{-itx}\frac{\phi(t)-\psi(t)}{-it} \gamma_{a}(t)\,dt. \tag{5}\]

That the integrand is a continuous function that equals \(0\) at \(t=0\) (since \(F\) and \(G\) have \(0\) "means," inequality 6.1 gives this) makes the right-hand side well-defined, and we may differentiate under the integral sign by the DCT [with dominating function \(\gamma_{a}(\cdot)\)] to get the previous equation (4). Thus \(\Delta_{a}(x)\) can differ from the right-hand side of (5) by at most a constant; but this constant is \(0\), since obviously \(\Delta_{a}(x)\to 0\) as \(|x|\to\infty\), while the right-hand side does the same by the Riemann-Lebesgue lemma. Equation (5) gives

\[|\Delta_{a}(x)|\leq\tfrac{1}{2\pi}\int_{-a}^{a}|\tfrac{\phi(t)-\psi(t)}{t}|\,dt \qquad\text{ for all }x. \tag{6}\]

Combining (3) and (6) gives (1).

**Corollary 1** (Stein).: _Suppose that instead of convolving \(F\) and \(G\) with the \(H_{a}\) of (2), we convolve with an arbitrary df \(H\) instead. In this situation we obtain_

\[\|F-G\|\leq 2\|F*H-G*H\|+8\|g\|{\rm E}|H^{-1}(\xi)|. \tag{7}\]

Proof.: Picking up at line (d) of the previous proof (with \(Y\cong H\)), we obtain

(d) \[\|F*H-G*H\|\geq\int_{[-\epsilon,\epsilon]}[D/2+My]\,dH(y)-DP(|Y|>\epsilon)\]

\[\geq(D/2)[1-P(|Y|>\epsilon)]-M{\rm E}|Y|-DP(|Y|>\epsilon)\]

\[\geq(D/2)-(3D/2)P(|Y|>\epsilon)-M{\rm E}|Y|\]

(e) \[\geq(D/2)-4M{\rm E}|Y|\]

using Markov's inequality and \(\epsilon\equiv D/2M\) in the last step.

## 8 Distributions on Grids\({}^{*}\)

**Definition 8.1**  We say that a rv \(X\) is _distributed on a grid_ if there exist real numbers \(a,d\) such that the probabilities \(p_{n}\equiv P(X=a+nd)\) satisfy \(\sum_{-\infty}^{\infty}p_{n}=1.\) We call \(d\) the _span_ of the grid. The _maximal span_ is \(\sup\{|d|:|d|\) is a span\(\}.\)

**Proposition 8.1**  If \(t_{0}\neq 0\), the following are equivalent:

(a) \(|\phi(t_{0})|=1.\)

(b) \(|\phi|\) has period \(t_{0}\); that is, \(|\phi(t+nt_{0})|=|\phi(t)|\) for all \(n\) and \(t\).

(c) The rv \(X\) is distributed on a grid of span \(d=2\pi/t_{0}\).

**Proof.**  Suppose that (a) holds. Then \(\phi(t_{0})=e^{i\alpha}\) for some real \(\alpha\). That is, \(\int e^{it_{0}x}\,dF(x)=e^{i\alpha},\) or \(\int e^{i(t_{0}x-\alpha)}\,dF(x)=1\). Taking real parts gives

(p) \(\int_{-\infty}^{\infty}[1-\cos(t_{0}x-\alpha)]\,\,dF(x)=0.\)

Since the integrand is nonnegative for all \(X\), this means that

(q) \(1-\cos(t_{0}x-\alpha)=0\quad\mbox{a.s.}\,\,F;\)

that is,

(r) \(t_{0}X-\alpha\in\{2\pi m:m=0,\pm 1,\pm 2,\ldots\}\qquad\mbox{a.s.}\)

That is, \(X\in\{\alpha/t_{0}+(2\pi/t_{0})m:m=0,\pm 1,\pm 2,\,\ldots\}\) a.s.; so (c) holds.

Suppose (c) holds. Then (b) holds, since

\(|\phi(t+nt_{0})|=|\sum_{m=-\infty}^{\infty}p_{m}e^{i(t+nt_{0})(a+dm)}|\)

\(=|e^{i(t+nt_{0})a}||\sum_{m=-\infty}^{\infty}p_{m}e^{i(t+nt_{0})dm}|\)

\(=|\sum_{m=-\infty}^{\infty}p_{m}e^{i(t+2\pi n/d)dm}|\)

(s) \(=|\sum_{m=-\infty}^{\infty}p_{m}e^{itdm}||e^{i2\pi nm}|=|\sum_{m=-\infty}^{ \infty}p_{m}e^{itdm}e^{ita}|=|\phi(t)|.\)

Suppose that (b) holds. Then

(t) \(1=|\phi(0)|=|\phi(0+t_{0}n)|=|\phi(0+t_{0}1)|=|\phi(t_{0})|,\)

so that (a) holds. \(\Box\)

**Corollary 2**  If \(a=0\) in (c), then we may replace \(|\phi|\) by \(\phi\) in (a) and (b), and proposition 8.1 will still hold.

**Proposition 8.2**  One of the following possibilities must hold:

(d) \(|\phi(t)|<1\) for all \(t\neq 0\).

(e) \(|\phi(t)|<1\) for \(0<t<2\pi/d\) and \(|\phi(2\pi/d)|=1.\,\mbox{Thus},X\) has maximal span \(d\).

(f) \(|\phi(t)|=1\) for all \(t.\) And so \(\phi(t)=e^{iat}\) for all \(t\) and \(P(X=a)=1,\mbox{ for some }a\).

**Proof.** Clearly, either (d), (e), or (f) holds, or else \(|\phi(t_{n})|=1\) for some sequence \(t_{n}\to 0\). In this latter case, \(|\phi(mt_{n})|=1\) for all \(m\), for each \(n\) by proposition 8.1. Since \(\{mt_{n}:n\geq 1,m=0,\pm 1,\pm 2,\ldots\}\) is dense in \(R\) and since \(\phi\), and thus \(|\phi|\), is continuous, we must have case (f) again. It remains to establish the consequences of (e) and (f).

Consider (e). Proposition 8.1 shows that (e) holds if and only if both \(d\) is a span and no number exceeding \(d\) is a span.

In the case of (f), we have \(|\phi(t_{1})|=1=|\phi(t_{2})|\) for some \(t_{1}\) and \(t_{2}\) having \(t_{1}/t_{2}=\) (an irrational number). But \(|\phi(t_{1})|=1\) and \(|\phi(t_{2})|=1\) imply that both \(2\pi/t_{1}\) and \(2\pi/t_{2}\) are spans. Thus if at least two points have positive mass, then the distance between them must equal \(m_{1}2\pi/t_{1}\) for some integer \(m_{1}\) and it must equal \(m_{2}2\pi/t_{2}\) for some integer \(m_{2}\). That is, \(2\pi m_{1}/t_{1}=2\pi m_{2}/t_{2}\), or \(t_{1}/t_{2}=m_{1}/m_{2}=\) (a rational number). This contradiction shows that there can be at most one mass point \(a\). \(\Box\)

**Exercise 8.1** (Inversion formula for distributions on a grid).: Let \(X\) be distributed on a grid with \(p_{n}=P(X=a+dn)\). Then \(\phi(t)=\sum_{-\infty}^{\infty}p_{n}e^{it(a+dn)}\). Show that

\[p_{m}=\frac{d}{2\pi}\int_{-\pi/d}^{\pi/d}\phi(t)e^{-it(a+dm)}\,dt. \tag{1}\]

## 9 Conditions for \(\phi\) to Be a Characteristic Function\({}^{*}\)

**Example 9.1**  Now, for \(a>0\),

\[F_{a}^{\prime}(x)=[1-\cos(ax)]/(\pi ax^{2})\qquad\mbox{for $x\in R$}\]

is a de la Vallee Poussin density function with chf

\[\phi_{a}(t)=(1-|t|/a)1_{[-a,a]}(t)\qquad\mbox{for $t\in R$}.\]

Let \(F_{a}\) denote the df. Then

\[F\equiv\sum_{1}^{n}p_{i}F_{a_{i}}\qquad\mbox{with}\quad p_{i}\geq 0,\quad\sum _{1}^{n}\!p_{i}=1,\quad\mbox{and}\quad 0<a_{1}<\cdots<a_{n}\]

is a df with characteristic function

\[\phi=\sum_{1}^{n}\!p_{i}\phi_{a_{i}}.\]

Thus any even function \(\phi\geq 0\) with \(\phi(0)=1\) whose graph on \([0,\infty)\) is a convex polygon is a chf. \(\Box\)

**Proposition 9.1** (Polya)  Let \(\phi\geq 0\) be an even function with \(\phi(0)=1\) whose graph on \([0,\infty)\) is convex and \(\downarrow\). Then \(\phi\) is a chf.

**Proof.** Pass to the limit in the obvious picture, using the continuity theorem to complete the proof. \(\Box\)

Bochner's theorem below gives necessary and sufficient conditions for a function to be a chf. We merely state it, as a background fact. Its proof can be found in a number of the standard texts.

**Definition 9.1**  A complex-valued function \(\phi(\cdot)\) on \(R\) is _nonnegative definite_ if for any finite set \(T\) and any complex-valued function \(h(\cdot)\) we have

\[\sum\nolimits_{s,t\in T}\!\phi(s-t)h(s)\bar{h}(t)\geq 0. \tag{1}\]

**Theorem 9.1** (Bochner)  A complex-valued function \(\phi(\cdot)\) is a chf if and only if it is non-negative definite and continuous.

## Chapter 10 CIIs via Characteristic Functions

### 10 Introduction

The classical CLT states that if \(X_{1},X_{2},\ldots\) are iid \((\mu,\sigma^{2})\), then

\[\sqrt{n}(\bar{X}_{n}-\mu)\to_{d}N(0,\sigma^{2})\qquad\text{ as }n\to\infty. \tag{1}\]

This chapter will also consider the following generalizations.

1. Triangular arrays of row-independent non-iid rvs \(X_{n1},\ldots,X_{nn}\) for \(n\geq 1\). Liapunov, Lindeberg-Feller (and the optional General CLT theorem 10.5.1).
2. The rate of convergence of such dfs to the limiting df, via Berry-Esseen.
3. The multidimensional CLT.
4. Random sample sizes, sample quantiles, and many other examples.
5. Non-normal limits (both the degenerate WLLN and a Poisson limit).
6. Convergence of the density functions as well (the local CLT 10.4.1).
7. Necessary and sufficient conditions for the optional statistically formulated CLT 10.5.2 for triangular arrays of row independent rvs.
8. See also the optional theorems 10.6.1, 10.7.1, 10.8.1, 10.9.1, and 10.9.2 for some best possible results for iid rvs and for bootstrap samples. Theorem 10.6.1 develops the domain of attraction \(\mathcal{D}\)(Normal) of the Normal distribution.

In chapter 11 we consider situations that lead to stable and infinitely divisible rvs as limits. Edgeworth and other approximations are also considered there. Section 13.9 includes a discussion of martingale CLTs. Chapter 15 has sections on asymptotic normality of trimmed means, of \(L\)-statistics, and of \(R\)-statistics (the latter includes a finite sampling CLT). The proofs of some optional results above require knowledge of Sections C.1-C.4. The chapter 15 examples require an inequality in Sections C.6.

## Chapter 10 Clts via characteristic functions

### 10 Basic Limit Theorems

The goal of this section is to use a chf approach to present the classical central limit theorems for sums of iid random variables in \(R\) and in \(R_{k}\). We also compare and contrast the central limit theorem with the Poisson limit theorem.

#### The Classical CLT

**Theorem 1.1** (Classical CLT): For each \(n\geq 1\), let \(X_{n1},\ldots,X_{nn}\) be iid \(F(\mu,\sigma^{2})\); this denotes that the df \(F(.)\) of the \(X_{nk}\)'s has mean \(\mu\) and finite variance \(\sigma^{2}\). Define the total \(T_{n}\equiv X_{n1}+\cdots+X_{nn}\) and the average \(\bar{X}_{n}\equiv T_{n}/n\). Then as \(n\to\infty\),

\[\sqrt{n}(\bar{X}_{n}-\mu)=\tfrac{1}{\sqrt{n}}(T_{n}-n\mu)=\tfrac{1}{\sqrt{n}} \sum_{k=1}^{n}(X_{nk}-\mu)\to_{d}N(0,\ \sigma^{2}). \tag{1}\]

**Proof.** Now, for fixed \(t\) we have (with \(0\leq g(t)\leq 3/2\) and \(g(t)\to 0\) as \(t\to 0\))

(a) \[\phi_{\sqrt{n}(\bar{X}_{n}-\mu)}(t)=\prod_{k=1}^{n}\phi_{(X_{nk}- \mu)/\sqrt{n}}(t)=\prod_{k=1}^{n}[\phi_{X_{nk}-\mu}(t/\sqrt{n})]^{n}\] \[\qquad=\left[1-\frac{\sigma^{2}}{2}\left(\frac{t}{\sqrt{n}}\right) ^{2}+\left(\frac{t}{\sqrt{n}}\right)^{2}\sigma^{2}g\left(\frac{t}{\sqrt{n}} \right)\right]^{n}\qquad\text{by inequality \ref{ineq:2}}\] (b) \[\qquad=\left[1-\frac{\sigma^{2}t^{2}}{2n}+\frac{t^{2}}{n}\times \sigma^{2}g\left(\frac{t}{\sqrt{n}}\right)\right]^{n}.\]

The first product lemma 9.6.3 with \(\theta=-\sigma^{2}t^{2}/2\) trivially applies. Thus

(c) \[\phi_{\sqrt{n}(\bar{X}_{n}-\mu)}(t)\to e^{-\sigma^{2}t^{2}/2}=\phi_{N(0,\sigma^ {2})}(t),\]

using table 9.1.1. Thus \(\sqrt{n}(\bar{X}_{n}-\mu)\to_{d}N(0,\sigma^{2})\) by the Cramer-Levy continuity theorem 9.5.1 and the uniqueness theorem 9.4.1.

Had we chosen to appeal to the second product lemma 9.6.4 instead, we would have instead claimed that

\[|\phi_{\sqrt{n}(\bar{X}_{n}-\mu)}(t)-(1-\sigma^{2}t^{2}/2n)^{n}|\] (d) \[\qquad=|\prod_{k=1}^{n}\phi_{(X_{nk}-\mu)/\sqrt{n}}(t)-\prod_{k=1 }^{n}(1-\sigma^{2}t^{2}/2n)|\] (e) \[\qquad\leq\sum_{k=1}^{n}|\phi_{(X_{nk}-\mu)/\sqrt{n}}(t)-(1-\sigma^{2}t^{2}/2 n)|\] (f) \[\qquad\leq\sum_{k=1}^{n}(t^{2}/n)\sigma^{2}g(t/\sqrt{n})=t^{2} \sigma^{2}g(t/\sqrt{n})\to 0.\]

But \((1-\sigma^{2}t^{2}/2n)^{n}\to\exp(-\sigma^{2}t^{2}/2)=\phi_{N(0,\sigma^{2})}(t)\), so the continuity theorem and the uniqueness theorem again complete the proof. \(\square\)

#### Degenerate Limits

**Exercise 1.1** (WLLN, or classical degenerate convergence theorem): For each \(n\geq 1\), let \(X_{n1},\ldots,X_{nn}\) be iid with finite mean \(\mu\). Use chfs to show the WLLN result that \(\bar{X}_{n}\to_{p}\mu\) as \(n\to\infty\). Equivalently,

\[\bar{X}_{n}\to_{d}\ \ \text{(the degenerate distribution with mass 1 at $\mu$)}. \tag{2}\]

**The Classical PLT**

**Theorem 1.2** (Classical Poisson limit theorem; the PLT): For each \(n\geq 1\), suppose that \(X_{n1},\ldots,X_{nn}\) are independent Bernoulli \((\lambda_{nk})\) rvs for which the values of the parameters satisfy \(\lambda_{n}\equiv\sum_{k=1}^{n}\lambda_{nk}\to\lambda\in(0,\ \infty)\) while \([\max_{1\leq k\leq n}\lambda_{nk}]\to 0.\) (This is true if \(\lambda_{n1}=\cdots=\lambda_{nn}=\lambda_{n}/n\) for all \(n\), with \(\lambda_{n}\to\lambda\)). Then

\[T_{n}\equiv X_{n1}+\cdots+X_{nn}\to_{d}\ \ \mbox{Poisson}(\lambda)\qquad\mbox{ as }n\to\infty. \tag{3}\]

**Proof.** From table 9.1.1 we have \(\phi_{X_{nk}}(t)=1+\lambda_{nk}(e^{it}-1)\). Thus

(a) \(\phi_{T_{n}}(t)=\prod_{k=1}^{n}\phi_{X_{nk}}(t)=\prod_{k=1}^{n}[1+\lambda_{nk }(e^{it}-1)]\)

(b) \(\to\exp(\lambda(e^{it}-1))\qquad\mbox{ by the first product lemma \ref{thm:Poisson}}\)

(c) \(=\phi_{\mbox{\scriptsize Poisson}}(\lambda)(t)\qquad\mbox{ by table \ref{thm:Poisson}}.1\)

Now apply the Cramer-Levy continuity theorem and the uniqueness theorem. \(\Box\)

**Exercise 1.2** (Poisson local limit theorem): Show that

\[P(T_{n}=k)\to P(\ \mbox{Poisson}(\lambda)=k)\qquad\mbox{ as }n\to\infty,\ \mbox{for }k=0,1,\ldots, \tag{4}\]

when \(\lambda_{n1}=\cdots=\lambda_{nn}\) in the PLT. Show that this implies

\[d_{TV}(P_{n},\ P)\equiv\sup\{|P_{n}(A)-P(A)|\ :\ A\in{\cal B}\}\to 0, \tag{5}\]

where \(T_{n}\cong P_{n}\) and \(\mbox{Poisson}(\lambda)\cong P\). [Exercise 12.5.4 will improve this.]

**Exercise 1.3: Show that if \(T_{\lambda}\cong\mbox{Poisson}(\lambda)\), then \((T_{\lambda}-\lambda)/\sqrt{\lambda}\to_{d}N(0,1)\) as the parameter \(\lambda\to\infty\).**

**A Comparison of Normal and Poisson Convergence**

**Exercise 1.4: (a) Suppose the hypotheses of the classical CLT hold. Show that

\[M_{n}\equiv\left[\max_{1\leq k\leq n}\frac{1}{\sqrt{n}}|X_{nk}-\mu|\right] \to_{p}0. \tag{6}\]

(b) Suppose the hypotheses of the classical PLT hold. Show that

\[M_{n}\equiv\left[\max_{1\leq k\leq n}|X_{nk}|\right]\to_{d}\ \ \mbox{ Bernoulli}(1-e^{-\lambda}). \tag{7}\]

(\(*\)) There is something fundamentally different regarding the negligibility of the corresponding terms in these two cases! The CLT involves summing many tiny pieces, but the PLT arises from very occasionally having a "large" piece.

**Remark 1.1**: Let \(Y_{n1},\ldots,Y_{nn}\) be independent. Let \(p_{nk}^{\epsilon}\equiv P(|Y_{nk}|>\epsilon)\). Recall equation (8.3.14) for the conclusion

\[M_{n}\equiv\left[\max_{1\leq k\leq n}|Y_{nk}|\right]\to_{p}0\qquad\mbox{ if and only if}\qquad\sum_{k=1}^{n}p_{nk}^{\epsilon}\to 0\quad\mbox{ for all }\epsilon>0. \tag{8}\]This was proved via the (8.3.13) inequality

\[1-\exp(-\sum_{k=1}^{n}p_{nk}^{\epsilon})\leq P(M_{n}\geq\epsilon)\leq\sum_{k=1}^{n} p_{nk}^{\epsilon}. \tag{9}\]

##### The Multivariate CLT

**Theorem 1.3** (Classical multivariate CLT): Let \({\sf X}_{n}\equiv(X_{n1},\ldots,X_{nk})^{\prime},n\geq 1\), be a sequence of iid \((\vec{\mu},\Sigma)\) random vectors. Then

\[\frac{1}{n^{1/2}}\sum_{j=1}^{n}({\sf X}_{j}-\vec{\mu})\rightarrow_{d}N_{k}({ \rm O},\Sigma)\qquad\mbox{ as }{\rm n}\rightarrow\infty. \tag{10}\]

**Proof.**  For any \(\vec{\lambda}\in R_{k}\) the rvs

(a) \[Y_{j}\equiv\vec{\lambda}^{\prime}({\sf X}_{j}-\vec{\mu})\cong(0,\vec{\lambda}^ {\prime}\Sigma\vec{\lambda})\qquad\mbox{ are iid for }j=1,\ldots,n.\]

Thus the classic CLT gives

(b) \[\sqrt{n}\bar{Y}_{n}\rightarrow_{d}N(0,\vec{\lambda}^{\prime}\Sigma\vec{\lambda }).\]

That is, \({\sf Z}_{n}\equiv n^{-1/2}\sum_{1}^{n}({\sf X}_{j}-\vec{\mu})\) satisfies

(c) \[\phi_{\vec{\lambda}^{\prime}{\sf Z}_{n}}(t)=\phi_{\sqrt{n}\bar{Y}_{n}}(t) \rightarrow\exp(-\vec{\lambda}^{\prime}\Sigma\vec{\lambda}\,t^{2}/2).\]

Now, if \({\sf Z}\cong N_{k}({\rm O},\Sigma)\), then \(\vec{\lambda}^{\prime}{\sf Z}\cong N(0,\vec{\lambda}^{\prime}\Sigma\vec{ \lambda})\) ; and hence

(d) \[\phi_{\vec{\lambda}^{\prime}{\sf Z}}(t)=\exp(-\vec{\lambda}^{\prime}\Sigma\vec{ \lambda}\,t^{2}/2).\]

Thus (c) and (d) give \(\phi_{\vec{\lambda}^{\prime}{\sf Z}_{n}}(t)\rightarrow\phi_{\vec{\lambda}^{ \prime}{\sf Z}}(t)\) for all \(t\in R\), for each \(\vec{\lambda}\in R_{k}\). Thus the Cramer-Wold theorem (theorem 9.5.2) shows that \({\sf Z}_{n}\rightarrow_{d}\vec{Z}\).

**Exercise 1.5** (Empirical process; Doob): Let \({\mathbb{U}}_{n}\equiv\sqrt{n}[{\mathbb{G}}_{n}-I]\) be the uniform empirical process of sections 6.5 and 12.10, and let \({\mathbb{U}}\) denote the Brownian bridge of (A.4.13). Show that \({\mathbb{U}}_{n}\rightarrow_{fd}{\mathbb{U}}\) as \(n\rightarrow\infty\); that is, show that for any set of points \(0<t_{1}<\cdots<t_{k}<1\) we have

\[({\mathbb{U}}_{n}(t_{1}),\ldots,{\mathbb{U}}_{n}(t_{k}))\rightarrow_{d}({ \mathbb{U}}(t_{1}),\ldots,{\mathbb{U}}(t_{k}))\qquad\mbox{ as }n\rightarrow\infty.\]

(Essentially, all results in chapter 12 derive from this example--via a suggestion of Doob (1949).)

**Exercise 1.6** (Partial sum process of iid rvs): Let \({\mathbb{S}}_{n}\) denote the partial sum process of iid \((0,1)\) rvs (see (11) below) and let \({\mathbb{S}}\) denote Brownian motion (as in (A.4.12)) Show that \({\mathbb{S}}_{n}\rightarrow_{fd}{\mathbb{S}}\) as \(n\rightarrow\infty\). [Hint. Set things up cumulating from the left, and then transform. Or note that the random element you must consider can be written in a form equivalent to something simpler. Or use the Cramer-Wold device. One of these methods is much simpler then the others.]

**Exercise 1.7** (Partial sum process) Suppose that \(X_{n1},\ldots,X_{nn}\) are independent \((0,\sigma_{nk}^{2})\) and satisfy Lindeberg's condition (10.2.11) below. Define \(\mathbb{S}_{n}\) on \([0,1]\) via

\[\mathbb{S}_{n}(t)=\sum_{i=1}^{k}X_{ni}/s_{nn}\qquad\text{ for }\frac{s_{nk}^{2}} {s_{nn}^{2}}\leq t<\frac{s_{n,k+1}^{2}}{s_{nn}^{2}},\quad 0\leq k\leq n, \tag{11}\]

with \(s_{nk}^{2}\equiv\sum_{i=1}^{k}\sigma_{ni}^{2}\) and \(s_{n0}^{2}\equiv 0\). Show that \(\mathbb{S}_{n}\to_{fd}\mathbb{S}\), where \(\mathbb{S}\) denotes Brownian motion. (Only attempt this problem following theorem 10.2.2.)

**Example 1.1** (**Chisquare goodness of fit statistic)** Suppose \(\Omega=\sum_{i=1}^{k}A_{i}\). Now let \(X_{1},\ldots,X_{n}\) be iid on \((\Omega,\mathcal{A})\) with all \(p_{i}\equiv P(X\in A_{i})>0\). Let

\[N_{ni}\equiv\sum_{j=1}^{n}1_{A_{i}}(X_{j})\equiv\text{(the number of }X_{j}\text{'s that fall in }A_{i})\qquad\text{ for }1\leq i\leq k. \tag{12}\]

(a) Now, \((Z_{1j},\ldots,Z_{kj})^{\prime}\), with \(Z_{ij}\equiv(1_{A_{i}}(X_{j})-p_{i})/\sqrt{p_{i}}\), has mean vector \(\mathsf{O}\) and covariance matrix \(\Sigma=|[\sigma_{i^{\prime}}]|\) with \(\sigma_{ii}=1-p_{i}\) and \(\sigma_{ii^{\prime}}=-\sqrt{p_{i}p_{i^{\prime}}}\) for \(i\neq i^{\prime}\).

(b) Thus \(\mathsf{W}_{n}\equiv\sum_{1}^{n}Z_{j}/\sqrt{n}\to_{d}\mathsf{W}\cong N_{k}( \mathsf{O},\Sigma)\) as \(n\to\infty\), by theorem 1.3.

(c) The usual chisquare goodness of fit statistic is

\[Q_{n}(\mathsf{p})\equiv\sum_{i=1}^{k}\frac{(N_{ni}-np_{i})^{2}}{ np_{i}}=\sum_{i=1}^{k}\frac{(\text{Observed}_{i}-\text{Expected}_{i})^{2}}{ \text{Expected}_{i}} \tag{13}\] \[=\mathsf{W}_{n}^{\prime}\mathsf{W}_{n}\to_{d}\mathsf{W}^{\prime} \mathsf{W}\qquad\text{ by the Mann--Wald theorem}\] (14) \[=(G\mathsf{W})^{\prime}(G\mathsf{W})\cong\text{Chisquare}(k-1);\]

here \(G\) is \(k\times k\) and orthogonal with first row \(\sqrt{\mathsf{p}^{\prime}}\), so that \(G\,\Sigma\,G^{\prime}=G[I-\sqrt{\mathsf{p}}\sqrt{\mathsf{p}^{\prime}}\,]G^{ \prime}=I-(1,0,\ldots,0)^{\prime}(1,0,\ldots,0)\). This has diagonal elements \((0,1,1,\ldots,1)\) with all off-diagonal elements \(0\), and then \(G\mathsf{W}\cong N(\mathsf{O},G\Sigma G^{\prime})\) (by (7.3.5) and (A.3.6)). We also use (A.1.29) for (16). [If a value of Expected is unknown, it should be replaced by an appropriate estimator Expected.] (See exercise 10.3.26 below.) (This statistic is just a quadratic form.) \(\square\)

**Exercise 1.8*** [Independence in an \(I\times J\) table] Suppose both \(\Omega=\sum_{i=1}^{I}A_{i}\) and \(\Omega=\sum_{j=1}^{J}B_{j}\) represent partitions of \(\Omega\).

(a) Let \(p_{ij}\equiv P(A_{i}B_{j})=p_{i}.p._{j}\), where \(p_{i}.\equiv P(A_{i})\) and \(p._{j}\equiv P(B_{j})\). Let

\[N_{ij}\equiv\text{(the number of iid observations }X_{1},\ldots,X_{n}\text{ that fall in }A_{i}B_{j}).\]

Let \(\hat{p}_{i}.\equiv\sum_{j=1}^{J}N_{ij}/n\) and \(\hat{p}._{j}\equiv\sum_{i=1}^{I}N_{ij}/n\). Show that

\[Q_{n}^{a}\equiv\sum_{i=1}^{I}\sum_{j=1}^{J}(N_{ij}-n\hat{p}_{i}.\hat{p}._{j}) ^{2}/(n\hat{p}_{i}.\hat{p}._{j})\to_{d}\text{Chisquare}((I-1)(J-1)). \tag{15}\]

(b) Let \(p_{i|j}\equiv P(A_{i}|B_{j})\). Let \(n\equiv n_{\cdot 1}+\cdots+n_{\cdot J}\). For each \(1\leq j\leq J\), let \(N_{ij}\equiv\text{(the number of iid }P(\cdot|B_{j})\text{ observations }X_{1}^{(j)},\ldots,X_{n_{j}}^{(j)}\text{ that fall in }A_{i}B_{j})\). Let \(\hat{p}_{i|j}\equiv\sum_{j=1}^{J}N_{ij}/n_{\cdot j}\). Show that when \(\sigma[A_{1},\ldots,A_{I}]\) and \(\sigma[B_{1},\ldots,\ B_{J}]\) are independent, the chisquare statistic satisfies

\[Q_{n}^{b}\equiv\sum_{i=1}^{I}\sum_{j=1}^{J}(N_{ij}-n_{j}\hat{p}_{i|j})^{2}/(n_ {j}\hat{p}_{i|j})\to_{d}\text{Chisquare}((I-1)(J-1)) \tag{16}\]

as \(n_{1}\wedge\cdots\wedge n_{J}\to\infty\).

\((*)\) Suppose that both sets of marginal totals \(n_{1},.,\ldots,n_{I}\). and \(n_{\cdot 1},\ldots,n_{\cdot J}\) are fixed, and that both sum to \(n\). Suppose that \(n\) balls are assigned to the \(IJ\) cells at random without replacement, subject to the side conditions on the marginal totals stated above. Let \(N_{ij}\) denote the number assigned to the \((i,j)\)-th cell. It holds that

\[Q_{n}^{c}\equiv\sum_{i=1}^{I}\sum_{j=1}^{J}(N_{ij}-n_{i\cdot}n_{\cdot j}/n)^{2} /(n_{i\cdot}n_{j\cdot}/n)\to_{d}\mbox{Chisquare}((I-1)(J-1)) \tag{17}\]

as \((n_{1}\). \(\wedge\cdots\wedge n_{I}.)\wedge(n_{\cdot 1}\wedge\cdots\wedge n_{\cdot J})\to\infty\). [Suppose \(I=5\) different social groups are at work in broadcasting, where the sum of the \(I\) group sizes \(n_{i}\). of our data is \(n=250\). The number whose salaries fall in each decile (thus \(J=10\)) of the observed salaries is necessarily \(n_{\cdot j}=n/J=25\). The statistic in (17) can be used to test for independence of group and salary level.] 

##### Limiting Distributions of Extremes

**Exercise 1.9**:

(a) Let \(\xi_{n1},\ldots,\xi_{nn}\) be iid Uniform(0, 1) rvs. Then the sample minimum \(\xi_{n:n}\) satisfies \(n\xi_{n:n}\to\mbox{Exp}(1)\).

(b) Now, \(\xi_{n:n}\) is the sample maximum. Determine the joint asymptotic distribution of \(n\xi_{n:1}\) and \(n(1-\xi_{n:n})\).

**Exercise 1.10**: (Special cases of Gnedenko's theorem)  Let \(X_{n:n}\) be the maximum of an iid sample \(X_{1},\ldots,X_{n}\) from \(F(\cdot)\cdot\) Then:

(a) \[\begin{array}{l}P(X_{n:n}-\log n\leq y)\to\exp(-e^{-y})\qquad\mbox{ for all }y\in R,\\ \mbox{ when }1-F(x)=e^{-x}\mbox{ for }x\geq 0.\end{array}\]

(b) \[\begin{array}{l}P(n^{1/b}X_{n:n}\leq y)\to\exp(-|y|^{b})\qquad\qquad\mbox{ for all }y<0,\\ \mbox{ when }1-F(x)=|x|^{b}\quad\mbox{ for }-1\leq x\leq 0,\quad\mbox{ with }b>0.\end{array}\]

(c) \[\begin{array}{l}P(X_{n:n}/n^{1/a}\leq y)\to\exp(-y^{-a})\qquad\qquad\mbox{ for all }y>0,\\ \mbox{ when }1-F(x)=1/x^{a}\mbox{ for }x\geq 1,\qquad\qquad\mbox{ with }a>0.\end{array}\]

[Distributions that are "suitably similar" to these prototypes yield the same limiting results, and these limits are the only possible limits.]

## 2 Variations on the Classical CLT

**Notation 2.1** Let \(X_{nk},1\leq k\leq n\) for each \(n\geq 1\), be row-independent rvs having means \(\mu_{nk}\) and variances \(\sigma_{nk}^{2}\), and let \(\gamma_{nk}\equiv{\rm E}|X_{nk}-\mu_{nk}|^{3}<\infty\) denote the third absolute central moments. Let

\[{\rm sd}_{n}\equiv\sqrt{\sum_{k=1}^{n}\sigma_{nk}^{2}}\qquad{\rm and }\qquad\gamma_{n}\equiv\sum_{k=1}^{n}\gamma_{nk},\qquad{\rm and\ let} \tag{1}\] \[Z_{n}\equiv\frac{1}{{\rm sd}_{n}}\sum_{k=1}^{n}[X_{nk}-\mu_{nk}]. \tag{2}\]

Let \(\phi_{nk}(\cdot)\) denote the chf of \((X_{nk}-\mu_{nk})/{\rm sd}_{n}\). \(\Box\)

**Theorem 2.1** (Rate of convergence in the CLT): Consider the rvs above. The df \(F_{Z_{n}}\) of the standardized \(Z_{n}\) is uniformly close to the \(N(0,1)\) df \(\Phi\), in that

\[\|F_{Z_{n}}-\Phi\|\leq 13\ \gamma_{n}/{\rm sd}_{n}^{3}. \tag{3}\]

**Corollary 1** (Liapunov CLT): (4) \[Z_{n}\to_{d}N(0,1)\qquad{\rm whenever}\qquad\gamma_{n}/{\rm sd}_{n}^{3}\to 0.\] (5)

**Corollary 2** (Berry-Esseen for iid rvs): Let \(X_{n1},\ldots,X_{nn}\) be iid rvs with df \(F(\mu,\sigma^{2})\) having \(\gamma\equiv{\rm E}|X-\mu|^{3}<\infty\). Then

\[\|F_{Z_{n}}-\Phi\|\leq\frac{8\,\gamma}{\sigma^{3}\sqrt{n}}. \tag{6}\]

**Proof.** Here we first give a delicate proof of the rate of convergence to normality in (3) based on Esseen's lemma (with (4) shown to be a corollary to this proof). [A rather simple proof of (4) is asked for in exercise 2.4 below.] Without loss, we assume that all \(\mu_{nk}=0\). Now, let \(a\equiv{\rm sd}_{n}^{3}/\gamma_{n}\); and assume throughout that \(a\geq 9\) (note that (3) is meaningless unless \(a>13\)). (Recall that \(a=b\oplus c\) means \(|a-b|\leq c\).) Note that

\[\left|\phi_{Z_{n}}(t)-e^{-t^{2}/2}\right|=\left|\prod_{k=1}^{n} \phi_{nk}(t)-e^{-t^{2}/2}\right|\] \[\leq e^{-t^{2}/2}\left|e^{\{\sum_{k=1}^{n}{\rm Log\ }\phi_{nk}(t) \}+t^{2}/2}-1\right|\] \[\equiv e^{-t^{2}/2}|e^{z}-1|\leq e^{-t^{2}/2}|z|e^{|z|}\qquad{\rm by \ (\ref{eq:f1})}\]

for all \(z\), where

\[{\rm(a)} |z|=\left|\sum_{k=1}^{n}{\rm Log\ }\phi_{nk}(t)+t^{2}/2\right|= \left|\sum_{k=1}^{n}\left\{{\rm Log\ }(1+\left[\phi_{nk}(t)-1\right])-\frac{i^{2}t^{2}\sigma_{nk}^{2}}{2\,{\rm sd }_{n}^{2}}\right\}\right|\] \[\leq \left|\sum_{k=1}^{n}\left\{\left[\phi_{nk}(t)-\left(1+\frac{i^{2 }t^{2}\sigma_{nk}^{2}}{2\,{\rm sd}_{n}^{2}}\right)\right]\oplus\left|\phi_{nk}( t)-1\right|^{2}\right\}\right|\] \[{\rm provided\ that\ }|\phi_{nk}(t)-1|\leq\frac{1}{2},\ {\rm using\ (\ref{eq:f1})}\](b) \[\leq\frac{|t|^{3}}{6}\frac{\gamma_{n}}{\mathrm{sd}_{n}^{3}}+\sum_{k=1}^{n}\left( \frac{|t|^{3/2}\mathrm{E}|X_{nk}|^{3/2}}{\mathrm{sd}_{n}^{3/2}}K_{1,1/2}\right)^ {2}\qquad\mbox{where $K_{1,1/2}^{2}=\frac{8}{9}$}\] using (9.6.4) [with \(m=2\) and \(\delta=1,\mbox{ then with $m=1$ and $\delta=\frac{1}{2}$}\)] (c) \[\leq\frac{1}{2a}|t|^{3}(\frac{19}{9})\quad\mbox{ with }\quad a\equiv\mathrm{sd}_{n}^{3}/\gamma_{n}\] using the Liapunov inequality for \((\mathrm{E}|X_{nk}|^{3/2})^{2}\leq\gamma_{nk}\). But validity of (7) required that all \(|\phi_{nk}(t)-1|\leq\frac{1}{2}\). However, (9.6.4) with \(m=1\) and \(\delta=1\) gives (d) \[|\phi_{nk}(t)-1|\leq\frac{1}{2}t^{2}\sigma_{nk}^{2}/\mathrm{sd}_{n}^{2}\leq \frac{1}{2}a^{2/3}[\gamma_{n}/\mathrm{sd}_{n}^{3}]^{2/3}\qquad\mbox{ on $|t|\leq a^{1/3}$}\] (e) \[\leq\frac{1}{2}\qquad\qquad\qquad\mbox{ on $|t|\leq a^{1/3}$}.\]

Consider for a moment the Liapunov CLT corollary 1 For any fixed \(t\), the bound on \(|z|\) in (c) goes to \(0\) whenever \(1/a=\gamma_{n}/\mathrm{sd}_{n}^{3}\to 0\). Moreover, (e) always holds when \(\gamma_{n}/\mathrm{sd}_{n}^{3}\to 0\), since \((\max\sigma_{nk}^{2}/\mathrm{sd}_{n}^{2})^{3/2}\leq(\max\gamma_{nk}/\mathrm{sd }_{n}^{3})\leq(\gamma_{n}/\mathrm{sd}_{n}^{3})\to 0\). Thus \(\phi_{n}(t)\to\exp(-t^{2}/2)\) and \(Z_{n}\to_{d}\,N(0,1)\) by (6), whenever \(\gamma_{n}/\mathrm{sd}_{n}^{3}\to 0\). That is, the Liapunov CLT corollary 1 holds. [This is already a good CLT!]

We now turn back to theorem 2.1 itself. Since the bound of (c) gives

(f) \[|z|\leq\frac{1}{2a}|t|^{3}\frac{19}{9}\leq\frac{1}{4}t^{2}\quad\mbox{ when $|t|\leq\frac{9}{38}a$}\qquad\mbox{(and as $a^{1/3}\leq\frac{9}{38}a$, if $a\geq 9$)},\]

we can claim from(6), (e) and (c) that

(g) \[|\phi_{Z_{n}}(t)-e^{-t^{2}/2}|\leq\frac{19}{18a}|t|^{3}e^{-t^{2}/4}\leq\frac{2 }{a}|t|^{3}e^{-t^{2}/4}\mbox{ for $|t|\leq a^{1/3}$ (when $a\geq 9$)}.\]

(Having the bound in (g) only over the range \(|t|\leq a^{1/3}\) is not sufficient for what is too come; we extend it in the next paragraph.)

Now, \(|\phi_{n}(t)|^{2}\) is the chf of the symmetrized rv \(Z_{n}^{s}\equiv Z_{n}-Z_{n}^{\prime}\) (and this rv has mean \(0\), variance \(2\), and third absolute moment bounded above by \(8\gamma_{n}/\mathrm{sd}_{n}^{3}\) (via the \(C_{r}\)-inequality)). Thus

\[|\phi_{Z_{n}}(t)|\leq[|\phi_{n}(t)|^{2}]^{1/2}\leq[1+0-\frac{2t^{2}}{2!}+\frac {|t|^{3}}{3!}8\gamma_{n}/\mathrm{sd}_{n}^{3}]^{1/2}\]

(h) \[\leq\exp(-t^{2}[\frac{1}{2}-\frac{2|t|}{3}\gamma_{n}/\mathrm{sd}_ {n}^{3}]) \mbox{ using }\,1-x\leq e^{-x}\] (i) \[\leq\exp(-t^{2}/4) \mbox{ for $|t|\leq(3/8)(\mathrm{sd}_{n}^{3}/\gamma_{n})$},\]

as was desired. This leads to

(j) \[|\phi_{Z_{n}}(t)-e^{-t^{2}/2}|\leq 2e^{-t^{2}/4}\leq\frac{2}{a}|t|^{3}e^{-t^{2}/4}\quad\mbox{ for $a^{1/3}\leq|t|\leq\frac{3}{8}a$}.\]

##### Key chf inequality

Combining (6), (e), (f) and (h) gives (provided \(a\geq 9\))

\[|\phi_{Z_{n}}(t)-e^{-t^{2}/2}|\leq(2|t|^{3}\gamma_{n}/\mathrm{sd}_{n}^{3})e^{-t^{2}/4}\qquad\mbox{ for $0\leq|t|\leq\frac{3}{8}\mathrm{sd}_{n}^{3}/\gamma_{n}=\frac{3}{8}a$}. \tag{8}\]

We next apply Esseen's lemma to (8) and get (3). Since we know the variance of a normal distribution,

(k) \[\|F_{Z_{n}}-\Phi\|\leq\int_{-(3/8)a}^{(3/8)a}\frac{1}{\pi|t|}\frac{2|t|^{3}}{a}e^{-t^{2}/4}dt+\frac{3.04769}{(3/8)a}\] \[\qquad\leq\frac{1}{a}\left[\frac{2}{\pi}\int_{-\infty}^{\infty}t^{2}e^{-t^{2}/4}dt+\frac{3.04769}{3/8}\right]\]

(l) \[=[8/\sqrt{\pi}+(8/3)3.04769]/a\doteq 12.641/a\leq 13/a.\]In the iid case use \(K_{1,1}=\frac{1}{2}\) and \(\beta\equiv{\rm E}|X|^{3}/\sigma^{3}=\gamma/\sigma^{3}\geq 1\) in (b), and obtain

\[|z| \leq\frac{|t|^{3}\beta}{6\sqrt{n}}+n\left(\frac{t^{2}\sigma^{2}}{2 n\sigma^{2}}\right)^{2}\leq\frac{|t|^{3}\beta}{6\sqrt{n}}+\frac{t^{4}\beta^{2}}{4n}\] (m) \[\leq\frac{5}{12}\frac{\beta}{\sqrt{n}}|t|^{3}\leq\frac{5}{12}|t|^{ 3}\qquad\qquad\qquad\qquad\mbox{ for all }|t|\leq\sqrt{n}/\beta,\]

with (e) necessarily valid. Thus (8) can be replaced in the iid case by

\[|\phi_{Z_{n}}(t)-e^{-t^{2}/2}|\leq\tfrac{5}{12}\tfrac{\gamma}{\sigma^{3}\sqrt {n}}|t|^{3}e^{-t^{2}/12}\qquad\mbox{ on }\ 0\leq|t|\leq\sqrt{n}\sigma^{3}/\gamma; \tag{9}\]

this yields \(8\gamma/\sqrt{n}\sigma^{3}\) when the steps leading to (l) are repeated. \(\square\)

**Theorem 2.2** (Lindeberg-Feller): Let \(X_{n1},\ldots,X_{nn}\) be row independent, with \(X_{nk}\cong(\mu_{nk},\sigma_{nk}^{2})\). Let \({\rm sd}_{n}^{2}\equiv\sum_{1}^{n}\sigma_{nk}^{2}\). The following statements are equivalent:

\[Z_{n}\to_{d}N(0,1)\qquad\mbox{ and }\qquad[\max_{1\leq k\leq n}P(|X_{nk}- \mu_{nk}|/{\rm sd}_{n}>\epsilon)]\to 0. \tag{10}\] \[LF_{n}^{\epsilon}\equiv\sum_{k=1}^{n}\int_{[|x-\mu_{nk}|\geq \epsilon\,{\rm sd}_{n}]}\left[\frac{x-\mu_{nk}}{{\rm sd}_{n}}\right]^{2}dF_{nk} (x)\to 0\qquad\mbox{ for all }\epsilon>0. \tag{11}\]

[Condition (11) implies that \(M_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}-\mu_{nk}|/{\rm sd}_{n}]\to_{p}0\), via (10.1.9).]

**Proof.** (Lindeberg) We prove the sufficiency here, with the necessity considered in the following separate proof. We note that the moment expansion inequality (found in inequality 9.6.1) gives bounds on \(\beta_{nk}(t)\), where

\[\phi_{nk}(t)\equiv 1+\theta_{nk}(t)\equiv 1-\frac{\sigma_{nk}^{2}}{{\rm sd}_{ n}^{2}}\frac{t^{2}}{2}+\beta_{nk}(t) \tag{12}\]

defines \(\theta_{nk}(t)\) and \(\beta_{nk}(t)\). Moreover (in preparation for the product lemma)

\[\phi_{Z_{n}}(t)=\prod_{k=1}^{n}\phi_{nk}(t)=\prod_{k=1}^{n}[1+\theta_{nk}(t)]= \prod_{k=1}^{n}\left[1-\frac{\sigma_{nk}^{2}}{{\rm sd}_{n}^{2}}\tfrac{t^{2}}{ 2}+\beta_{nk}(t)\right] \tag{13}\]

where

\[\theta_{n}(t)\equiv\sum_{k=1}^{n}\theta_{nk}(t)=-\frac{t^{2}}{2}\sum_{k=1}^{n} \frac{\sigma_{nk}^{2}}{{\rm sd}_{n}^{2}}+\sum_{k=1}^{n}\beta_{nk}(t)=-t^{2}/2 +\sum_{k=1}^{n}\beta_{nk}(t). \tag{14}\]

The inequality of (9.6.4) (compare this with (9.6.10), and with (7)) gives

\[\left|\sum_{k=1}^{n}\beta_{nk}(t)\right|=\left|\sum_{k=1}^{n}\left[\phi_{nk}(t )-1-0+\frac{\sigma_{nk}^{2}}{{\rm sd}_{n}^{2}}\frac{t^{2}}{2}\right]\right| \tag{15}\] \[\leq\sum_{k=1}^{n}\left|\int\left\{e^{it(x-\mu_{nk})/{\rm sd}_{n} }-\left[1+\frac{it(x-\mu_{nk})}{{\rm sd}_{n}}+\frac{[it(x-\mu_{nk})]^{2}}{2 \,{\rm sd}_{n}^{2}}\right]\right\}dF_{nk}(x)\right|\] (16) \[\leq\sum_{k=1}^{n}\int_{[|(x-\mu_{nk})|<\epsilon\,{\rm sd}_{n}]} \frac{1}{6}\left|\frac{it(x-\mu_{nk})}{{\rm sd}_{n}}\right|^{3}dF_{nk}(x)\] (17) \[\qquad+\sum_{k=1}^{n}\int_{[|(x-\mu_{nk})|\geq\epsilon\,{\rm sd} _{n}]}\frac{1}{2}\left|\frac{it(x-\mu_{nk})}{{\rm sd}_{n}}\right|^{2}dF_{nk}(x) \tag{18}\]\[\leq\epsilon\frac{|t|^{3}}{6}\left[\frac{\sum_{k=1}^{n}\sigma_{nk}^{2}}{\mathrm{ sd}_{n}^{2}}\right]+\frac{t^{2}}{2}\sum_{k=1}^{n}\int_{|(x-\mu_{nk})|\geq\epsilon\, \mathrm{sd}_{n}]}\left[\frac{x-\mu_{nk}}{\mathrm{sd}_{n}}\right]^{2}dF_{nk}(x). \tag{16}\]

Thus normality holds, since the integral in (16) goes to \(0\) for all \(\epsilon>0\) by (11).

Note that \([\max_{1\leq k\leq n}|\theta_{nk}(t)|]\to 0\) as required by the product lemma, since we can use inequality 9.6.1 on the \(\theta_{nk}(t)\) in (12) to claim that \(|\theta_{nk}(t)|\leq\frac{t^{2}}{2}\sigma_{nk}^{2}/\mathrm{sd}_{n}^{2}\), and then use (11) on the second term below to claim that

(d) \[\sigma_{nk}^{2}/\mathrm{sd}_{n}^{2} \leq[\int_{[|x-\mu_{nk}|\leq\epsilon\,\mathrm{sd}_{n}]}(x-\mu_{nk })^{2}dF_{nk}(x)/\mathrm{sd}_{n}^{2}\] \[\qquad+\int_{[|x-\mu_{nk}|>\epsilon\,\mathrm{sd}_{n}]}(x-\mu_{nk })^{2}dF_{nk}(x)/\mathrm{sd}_{n}^{2}\] (e) \[\leq\epsilon^{2}+o(1)\leq\epsilon,\qquad\qquad\qquad\qquad\qquad \qquad\mbox{ for }n\geq\mbox{ (some }n_{\epsilon}).\qed\]

**Proof.** (Feller) We proved sufficiency in the previous proof; we now turn to necessity. Suppose that condition (10) holds. Applying (9.6.2) [since the easy exercise 10.2.9 below applied to our uan rvs shows that the terms \(z_{nk}=\phi_{nk}(t)-1\) converge uniformly to \(0\) on any finite interval] gives

\[|\sum_{k=1}^{n}\mathrm{Log}\,\phi_{nk}(t)-\sum_{k=1}^{n}[\phi_{nk }(t)-1]|\leq\sum_{k=1}^{n}|\phi_{nk}(t)-1|^{2} \tag{18}\] \[\qquad\leq[\max_{1\leq k\leq n}|\phi_{nk}(t)-1|]\times(t^{2}/2) \times[\sum_{k=1}^{n}\sigma_{nk}^{2}/\mathrm{sd}_{n}^{2}]\qquad\mbox{ by (\ref{eq:1})} \tag{17}\]

(a) \[\leq o(1)\times(t^{2}/2)\times 1\to 0,\qquad\qquad\qquad\qquad\qquad \mbox{ using (\ref{eq:1}) via exercise 2.1.}\]

We thus have (for any finite \(M\))

(b) \[\mathrm{Log}\prod_{1}^{n}\phi_{nk}(t)=\sum_{1}^{n}[\phi_{nk}(t)-1]+o(1),\qquad \qquad\mbox{ uniformly on any }|t|\leq M.\]

But we also know that

(c) \[\mathrm{Log}\prod_{1}^{n}\phi_{nk}(t)\to-t^{2}/2,\]

since we have assumed asymptotic normality. [Recall that \(a=b\oplus c\) means that \(|a-b|\leq c\).] Combining (b) and (c) shows that for every tiny \(\epsilon>0\) and every huge \(M>0\) we have

\[-t^{2}/2=\mathrm{Real}(-t^{2}/2)=\mathrm{Real}\{\sum_{1}^{n}[\phi_{nk}(t)-1] \}\oplus\epsilon\qquad\mbox{ for }|t|\leq M \tag{19}\]

for all large \(n\); that is, for \(n\geq\) (some \(n_{\epsilon M}\)) we have

\[t^{2}/2=\sum_{1}^{n}\int[1-\cos(t(x-\mu_{nk})/\mathrm{sd}_{n})]dF_{nk}(x) \oplus\epsilon\qquad\mbox{ on }|t|\leq M. \tag{20}\]

Define \(y_{k}\equiv(x-\mu_{nk})\). We further define \(I_{nk}\equiv[|x-\mu_{nk}|<\epsilon\,\mathrm{sd}_{n}]\). Note that

(d) \[0\leq 1-\cos(ty/\mathrm{sd}_{n})\leq(t^{2}y^{2}/2\,\mathrm{sd}_{n}^{2}).\]

Thus for all \(|t|\leq M\) we have for all \(n\geq n_{\epsilon M}\) that

(e) \[(t^{2}/2)\sum_{k=1}^{n}\int_{I_{nk}^{c}}(y_{k}^{2}/\mathrm{sd}_{ n}^{2})dF_{nk}(x)=(t^{2}/2)[1-\sum_{k=1}^{n}\int_{I_{nk}}(y_{k}^{2}/\mathrm{sd}_{ n}^{2})dF_{nk}(x)]\] \[\qquad=(t^{2}/2)-\sum_{k=1}^{n}\int_{I_{nk}}(t^{2}y_{k}^{2}/2\, \mathrm{sd}_{n}^{2})dF_{nk}(x)\] (f) \[\qquad\leq(t^{2}/2)-\sum_{k=1}^{n}\int_{I_{nk}}[1-\cos(ty_{k}/ \mathrm{sd}_{n})]dF_{nk}(x)\qquad\mbox{ by (d)}\]

[MISSING_PAGE_EMPTY:9037]

the nonzero contribution shown in the last step is due to the \(U_{k}\)'s, whereas we do already know that the contribution due to the \(Y_{k}\)'s is \(o(1)\). This example shows that it is possible to have \(X_{n}\to_{d}X\) without having \({\rm Var}[X_{n}]\to{\rm Var}[X]\). Note that \({\rm Var}[N(0,1)/(1+c^{2})]=1/(1+c^{2})<1=\lim 1=\underline{\lim}\,{\rm Var}[S_{n}/{ \rm sd}_{n}]\) (via the Fatou lemma and Skorokhod's theorem).

(ii) Note that if \(X_{n1}\cong N(0,pn)\), for some \(0<p<1,X_{nk}\equiv 0\) for \(2\leq k\leq[pn]\), and \(X_{nk}\cong N(0,1)\) for \(pn<k\leq n\) for independent rvs \(X_{nk}\), then \(S_{n}/{\rm sd}_{n}\to_{d}N(0,1)\), while Lindeberg's condition fails and \([\max_{1\leq k\leq n}\sigma_{nk}^{2}/{\rm sd}_{n}^{2}]\to p>0\). \(\Box\)

**Remark 2.2**: It is known that the constant \(8\) in (5) can be replaced by \(0.7975\). It is also known in the iid case with \({\rm E}|X|^{3}<\infty\) that the "limiting distribution measure" \(d(F,\Phi)\equiv\lim_{n\to\infty}\sqrt{n}|F_{Z_{n}}-\Phi|\) exists, and that this measure achieves the bound \(\sup_{F}(\sigma^{3}/\gamma)d(F,\ \Phi)=(\sqrt{10}+3)/(6\sqrt{2\pi})=0.409\). This sup is achieved by \(c[{\rm Bernoulli}(a)-a]\), where \(c=(\sqrt{10}-3)/2\) and \(a=(4-\sqrt{10})/2\). Thus the constant \(0.7975\) cannot be greatly improved. Many other improvements and refinements of the Berry-Esseen theorem are possible. The books by Bhattacharya and Rao (1976, pp. 110, 240) and Petrov (1977) both give many. We list three as "exercises" in exercise 2.15 below. \(\Box\)

**Exercise 2.4** (Liapunov's \((2+\delta)\)-Clt) Define \(\gamma_{nk}^{\delta}={\rm E}|X_{nk}-\mu_{nk}|^{2+\delta}\) for every value \(0<\delta\leq 1\). Suppose we have _Liapunov's \((2+\delta)\)-condition_ that

\[\sum_{k=1}^{n}{\rm E}|X_{nk}-\mu_{nk}|^{2+\delta}/{\rm sd}_{n}^{2+\delta} \to 0\qquad\mbox{ for some }0<\delta\leq 1. \tag{25}\]

Show that \(\frac{1}{{\rm sd}_{n}}\sum_{1}^{n}(X_{nk}-\mu_{nk})\to_{d}N(0,\ 1)\). (Appeal first to (9.6.5). Alternatively, verify that all \(LF_{n}^{\epsilon}\to 0\).)

**Exercise 2.5**: Construct an example with iid \(X_{1},X_{2},\ldots\) for which the Lindeberg condition holds, but for which Liapunov's \((2+\delta)\)-condition fails for each \(0<\delta\leq 1\).

**Exercise 2.6** (Liapunov-type WLLN) Let \(X_{n1},\ldots,X_{nn},n\geq 1\), be a triangular array of row-independent rvs with \(0\) means. Then

\[\sum_{k=1}^{n}{\rm E}|X_{nk}|^{1+\delta}/n^{1+\delta}\to 0\qquad\mbox{ for some }0<\delta\leq 1\]

implies that \(\bar{X}_{n}\to_{p}0\) as \(n\to\infty\). (Or, mimic the WLLN proof.)

**Exercise 2.7** (Pitman) For iid \({\rm rvs},\bar{X}_{n}\to_{p}a\) holds if and only if \(\phi^{\prime}(0)=ai\).

**Exercise 2.8** (i) Show that Lindeberg's condition that all \(LF_{n}^{\epsilon}\to 0\) implies _Feller's condition_ (which is not strong enough to guarantee asymptotic normality) that

\[[\max_{1\leq k\leq n}\sigma_{nk}^{2}]/{\rm sd}_{n}^{2}\to 0. \tag{26}\]

(ii) Let \(X_{n1},\ldots,X_{nn}\) be row independent Poisson \((\lambda/n)\,{\rm rvs}\), with \(\lambda>0\). Discuss which of Lindeberg-Feller, Liapunov, and Feller conditions holds in this context.

(iii) Repeat part (ii) when \(X_{n1},\ldots,X_{nn}\) are row independent and all have the probability density \(cx^{-3}(\log x)^{-2}\) on \(x\geq e\) (for some constant \(c>0\)).

(iv) Repeat part (ii) when \(P(X_{nk}=a_{k})=P(X_{nk}=-a_{k})=1/2\) for the row independent rvs. Discuss this for general \(a_{k}\geq 0\), and present interesting examples.

**Exercise 2.9** Let \(X_{n1},\ldots,X_{nn}\) be row independent, with \(X_{nk}\cong(\mu_{nk},\sigma_{nk}^{2})\). Let \(T_{n}\equiv Z_{n1}+\cdots+Z_{nn}\), and set \(\mu_{n}\equiv\sum_{1}^{n}\mu_{nk}={\rm E}T_{n}\) and \({\rm sd}_{n}^{2}\equiv\sum_{1}^{n}\sigma_{nk}^{2}={\rm Var}[T_{n}].\) The following statements are equivalent:

\[Z_{n}\equiv(T_{n}-\mu_{n})/{\rm sd}_{n}\to_{d}N(0,1)\qquad\mbox{and}\qquad[ \max_{1\leq k\leq n}\sigma_{nk}^{2}]/{\rm sd}_{n}^{2}\to 0. \tag{28}\] \[LF_{n}^{\epsilon}\equiv\sum_{k=1}^{n}\int_{[|x-\mu_{nk}|\geq{\rm ed }_{n}]}[{x-\mu_{nk}\over{\rm sd}_{n}}]^{2}dF_{nk}(x)\to 0\qquad\mbox{ for all }\epsilon>0. \tag{27}\]

(Example 11.1.2 below treats the current exercise by a different method.)

**Exercise 2.10**  Complete the proof of theorem 8.8.1 regarding the equivalence of \(\to_{d},\to_{p}\), and \(\to_{a.s.}\) for sums of independent rvs.

**Exercise 2.11**  Formulate a WLLN in the spirit of the Lindeberg-Feller theorem.

**Exercise 2.12**  Establish the \((2+\delta)\)-analogue of theorem 2.1. [Hint. Use both \(2+\delta\) and \(1+\delta/2\) moments in line (b) of the theorem 2.1 proof, via lemma 9.6.2.]

**Exercise 2.13**  Construct a second example satisfying the key property of remark 2.1 (i), that the limiting variance is not the limit of the variances.

**Exercise 2.14** (Large deviations; Cramer))  Let \(X_{n1},\ldots,X_{nn}\) be iid \(F\). Suppose \(X\cong F\) has a moment generation function \(M(t)\equiv{\rm E}\,e^{tX}\) that is finite in some neighborhood of the origin. Let \(Z_{n}\equiv\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\), and let \(Z\cong N(0,1)\), Then

\[P(Z_{n}>x_{n})/P(Z>x_{n})\to 1\qquad\mbox{ provided }\qquad x_{n}/n^{1/6}\to 0. \tag{29}\]

(This exercise is repeated again later as exercise 11.6.6.)

**Exercise 2.15*** (a) (Petrov) Suppose \(X_{n1},\ldots X_{nk}\) are row independent rvs for which \(X_{nk}\cong(0,\sigma_{nk}^{2})\), and set \(\sigma_{n}^{2}\equiv\sum_{k=1}^{n}\sigma_{nk}^{2}\) and \(F_{n}(x)\equiv P(S_{n}/\sigma_{n}\leq x)\). Then for some universal constant \(C\) we have

\[\|F_{n}-\Phi\|\leq C[\sigma_{n}^{-3}\sum_{k=1}^{n}{\rm E}X_{nk}^{3}1_{[|X_{nk }|<\epsilon\sigma_{n}]}+\sigma_{n}^{-2}\sum_{k=1}^{n}{\rm E}X_{nk}^{2}1_{[|X_ {nk}|\geq\epsilon\sigma_{n}]}]. \tag{30}\]

(b) (Petrov) If \(E[X_{nk}^{2}g(X_{nk})]<\infty\) for \(1\leq k\leq n\) where

\[g\geq 0\mbox{ is even and }\nearrow\mbox{ for }x>0,\mbox{ and }x/g(x)\mbox{ is }\nearrow\mbox{ for }x>0, \tag{31}\]

then for some absolute constant \(C\) we have the very nice result

\[\|F_{n}-\Phi\|\leq C\sum_{1}^{n}{\rm E}[X_{nk}^{2}g(X_{nk})]/\sigma_{n}^{2}g( \sigma_{n}). \tag{32}\]

**(c) (Nagaev)** Bounds on \(|F_{n}(x)-\Phi(x)|\) that decrease as \(|x|\to\infty\) are given (in the iid case) in the expression

\[|F_{n}(x)-\Phi(x)|\leq C({\rm E}|X|^{3}/\sigma^{3})/(\sqrt{n}(1+|x|^{3})) \qquad\mbox{ for all real }x. \tag{33}\]

**(d) (Bernstein)** Let \(r>2\). Consider row-independent rvs \(X_{n1},\ldots,X_{nn}\) for which we have \(X_{nk}\cong(0,\sigma_{nk}^{2})\). Let \(Z_{n}\equiv\sum_{k=1}^{n}(X_{nk}-\mu_{nk})/\sigma_{n}\).

(\(\alpha\)) Let \(\sum_{1}^{n}{\rm E}|X_{nk}|^{r}/\sigma_{n}^{r/2}\to 0\). Then \(Z_{n}\to_{d}N(0,1)\) and \({\rm E}|Z_{n}|^{r}\to{\rm E}|N(0,1)|^{r}\).

(\(\beta\)) The converse holds if \([\max_{1\leq k\leq n}\sigma_{nk}^{2}/{\rm sd}_{n}^{2}]\to 0\) is also true.

(\(*\)) See Petrov (1975, pp. 118, 113, 125, 103) for (a), (b), (c), and (d).

**Exercise 2.16**  Beginning with (15), try to obtain the Berry-Esseen bound (but with a different constant) by appeal to the second product lemma.

## Chapter 10 Clts via characteristic functions

### 10 Examples of Limiting Distributions

\({}^{o}\)

**Example 10** (Delta method): (a) Suppose \(c_{n}[W_{n}-a]\to_{d}V\) where \(c_{n}\to\infty\), and suppose \(g(\cdot)\) is differentiable at \(a\) (recall (4.3.6) and (4.3.12)). Then (as in the chain rule proof of calculus) immediately

\[c_{n}[g(W_{n})-g(a)]=_{a}\{g^{\prime}(a)\}\cdot c_{n}[W_{n}-a]\to d\{g^{\prime} (a)\}\cdot V.\]

[Recall that \(U_{n}=_{a}V_{n}\) means that \(U_{n}-V_{n}\to_{p}0\).]

(b) The obvious vector version of this has the conclusion

\[c_{n}[g(\mathrm{W}_{n})-g(\mathrm{a})]=_{a}\{\nabla g(a)\}\cdot c_{n}[\mathrm{ W}_{n}-\mathrm{a}]. \tag{1}\]

**Example 11** (Asymptotic normality of the sample variance): Suppose the rvs \(X_{1},\ldots,\)\(X_{n}\) are iid \((\mu,\sigma^{2})\) with \(\mu_{4}\equiv\mathrm{E}X^{4}<\infty\) and \(\sigma^{2}>0\). Then

\[S_{n}^{2}\equiv\frac{1}{n-1}\sum_{k=1}^{n}(X_{k}-\bar{X})^{2}=(\mathrm{the} \ sample\ variance). \tag{2}\]

For a useful phrasing of conclusions, define

\[\begin{array}{l}Z_{k}\equiv(X_{k}-\mu)/\sigma\cong(0,1)\,\\ Y_{k}\equiv Z_{k}^{2}=[(X_{k}-\mu)/\sigma]^{2}\cong(1,\frac{\mu_{4}}{\sigma^{4 }}-1)=(1,2(1+\gamma_{2}/2))\,\end{array} \tag{3}\]

where \(\gamma_{2}\equiv(\mu_{4}-3\sigma^{4})/\sigma^{4}\equiv\) (the _kurtosis_) measures the tail heaviness of the distribution of \(X\). We will show that as \(n\to\infty\) both

\[\sqrt{n}[S_{n}^{2}-\sigma^{2}]_{\frac{1}{\sqrt{2}\sigma^{2}}}=_{a}\frac{1}{ \sqrt{2}}\sqrt{n}[\bar{Y}_{n}-1]\to_{d}N(0,1+\gamma_{2}/2)\qquad\mbox{ and } \tag{4}\]

\[\sqrt{n}[S_{n}-\sigma]_{\sigma}^{\frac{2}{2}}=_{a}\sqrt{n}[\bar{Y}_{n}-1]\to_{d }\sqrt{2}N(0,1+\gamma_{2}/2). \tag{5}\]

**Proof.** Now,

\[\begin{array}{l}\frac{S_{n}^{2}}{\sigma^{2}}=\frac{1}{n-1}\sum_{k=1}^{n} \frac{(X_{k}-\bar{X}_{n})^{2}}{\sigma^{2}}=\frac{n}{n-1}\frac{1}{n}\sum_{k=1}^ {n}[(\frac{X_{k}-\mu}{\sigma})-(\frac{\bar{X}_{n}-\mu}{\sigma})]^{2}\\ \hskip 14.226378pt=\frac{n}{n-1}\frac{1}{n}\sum_{k=1}^{n}[Z_{k}-\bar{Z}_{n}]^{ 2}.\end{array}\] (a)

Then note from (a) that

\[\frac{\sqrt{n}(S_{n}^{2}-\sigma^{2})}{\sqrt{2}\sigma^{2}}=\frac{n}{n-1}\frac{1 }{\sqrt{2}}\{\sqrt{n}(\bar{Y}_{n}-1)-\sqrt{n}\bar{Z}_{n}^{2}\}-\frac{\sqrt{n}} {\sqrt{2}(n-1)\] (c)

\[=_{a}\frac{1}{\sqrt{2}}\sqrt{n}(\bar{Y}_{n}-1)\to_{d}N(0,\ \mathrm{Var}[Y]/2).\] (a)

**Exercise 11**: (a) Determine the joint limiting distribution of \(\sqrt{n}(\bar{X}_{n}-\mu)\) and \(\sqrt{n}(S_{n}-\sigma)\) in the iid case, where \(S_{n}^{2}\equiv[\sum_{1}^{n}(X_{k}-\bar{X}_{n})^{2}/(n-1)]\) (Consider the representation of \(S_{n}\) in (6) as a normed sum of the rvs \(Y_{k}\).) What condition on the moments is required for the result?(b) Find the asymptotic distribution of the (appropriately normalized) _coefficient of variation_\(S_{n}/\bar{X}_{n}\) in this iid case; that is, consider \(\sqrt{n}(S_{n}/\bar{X}_{n}-\sigma/\mu)\). Obtain a useful representation by appealing to part (a). (Suppose now that all \(X_{k}\geq 0\).)

(c) Note that (6) provides a stronger conclusion than just asymptotic normality, in that it forms a superb starting point for the further asymptotic work in (a) and (b). Note also (13) below.

**Exercise 3.2** (Moments of \(\bar{X}_{n}\) and \(S_{n}^{2}\)): Let \(X_{1},\ldots,X_{n}\) be iid. Note/show that (provided that \(\mu\) or \(\sigma^{2}\) is well-defined) \(\bar{X}_{n}\cong(\mu,\sigma^{2})\) and \(\mbox{\rm E}S_{n}^{2}=\sigma^{2}\). Show that (provided that \(\mu_{3}\) or \(\mu_{4}\) is well-defined):

\[\mbox{\rm E}(\bar{X}_{n}-\mu)^{3}=\frac{\mu_{3}}{n^{3}}. \tag{8}\] \[\mbox{\rm E}(\bar{X}_{n}-\mu)^{4}=\frac{3\sigma^{4}}{n^{2}}+ \frac{\mu_{4}-3\sigma^{4}}{n^{3}}.\] (9) \[\mbox{\rm Var }[S_{n}^{2}]=\frac{1}{n}\{\mu_{4}-\frac{n-3}{n-1} \sigma^{4}\}\qquad\mbox{ and }\qquad\mbox{\rm Cov}[\bar{X}_{n},S_{n}^{2}]= \frac{1}{n}\mu_{3}. \tag{7}\]

**Exercise 3.3**: If \(X_{1},\ldots,X_{n}\) are iid \((\mu,\sigma^{2})\), then \(\sqrt{n}[X_{n}^{2}-\mu^{2}]\to_{d}2\mu\times N(0,\sigma^{2})\) (by the delta method). What is the asymptotic distribution of \(n\bar{X}_{n}^{2}\) when \(\mu=0\)?

**Exercise 3.4** (Two samples): If \(\sqrt{m}(S_{m}-\theta)\to_{d}N(0,1)\) as \(m\to\infty\) and \(\sqrt{n}(T_{n}-\theta)\to_{d}N(0,1)\) as \(n\to\infty\) for independent rvs \(S_{m}\) and \(T_{n}\), then \(\sqrt{\frac{mn}{m+n}}(S_{m}-T_{n})\to_{d}N(0,1)\) holds as \(m\wedge n\to\infty\). [Hint: Suppose initially that \(\lambda_{mn}\equiv m/(m+n)\to\lambda\in[0,1]\). Use Skorohod (or, use convolution or chfs) to extend it.] This is useful for the two-sample \(t\)-test and \(F\)-test.

**Exercise 3.5** (Simple linear rank statistics): Let \(T_{N}\equiv\frac{1}{\sqrt{N}}\sum_{1}^{N}c_{i}a_{\pi(i)}\), where \((\pi(1),\ldots,\)\(\pi(N))\) achieves each of the \(N!\) permutations of \((1,\ldots,N)\) with probability \(1/N!\). Here, the \(c_{i}\) and \(a_{i}\) are constants. Show that:

\[\mbox{\rm E}a_{\pi(i)}=\bar{a}_{N},\quad\mbox{\rm Var}[a_{\pi(i)} ]=\sigma_{a}^{2}\equiv\frac{1}{N}\sum_{1}^{N}(a_{i}-\bar{a}_{N})^{2},\] \[\mbox{\rm Cov }[a_{\pi(i)},\ a_{\pi(j)}]=-\frac{1}{N-1}\sigma_{a}^{2}\quad\mbox{ for all }i\neq j. \tag{10}\]

\[\frac{1}{\sqrt{N}}\mbox{\rm E}T_{N}=\bar{c}_{N}\cdot\bar{a}_{N},\qquad\mbox{ \rm Var}[T_{N}]=\frac{N}{N-1}\sigma_{c}^{2}\sigma_{a}^{2}. \tag{11}\]

[Hint. Var \([\sum_{1}^{N}a_{\pi(i)}]=0\), as in (A. 1.8).]

**Example 3.3** (The median \(\ddot{X}_{n}\)): The population _median_ of the distribution of a rv \(X\)'s is any value \(\theta\) satisfying \(P(X\leq\theta)\geq\frac{1}{2}\) and \(P(X\geq\theta)\geq\frac{1}{2}\). Let \(X_{1},\ldots,X_{n}\) be iid with df \(F(\cdot-\theta)\), for some \(\theta\in R\), where \(F(0)=\frac{1}{2}\) and \(F^{\prime}(0)>0\) exists and exceeds zero. (Thus \(X_{i}\cong\theta+\epsilon_{i}\), for \(\epsilon_{i}\)'s that are iid \(F(\cdot)\) with a unique median at \(0\).) The ordered values of the \(X_{k}\)'s are denoted by \(X_{n:1}\leq\cdots\leq X_{n:n}\), and are called the _order statistics_. The _sample median_\(\ddot{X}_{n}\) is defined to be \(X_{n:m}\) or _any_ point in the interval \([X_{n:m},X_{n:m+1}]\) according as \(n\) equals \(2m+1\) or \(2m\) is odd or even. Let \(X_{n}^{l}\) and \(X_{n}^{r}\) denote the left and right endpoints of the interval of possible sample medians (of course, \(X_{n}^{l}=X_{n}^{r}=X_{n:m+1}\) if \(n=2m+1\) is odd). Let \(\ddot{X}_{n}\) denote any sample median. (a) Then

\[Z_{1n}\equiv\sqrt{n}[\ddot{X}_{n}-\theta]\to_{d}Z_{1}\cong N(0,\ \frac{1}{4[F^{ \prime}(0)]^{2}}). \tag{12}\]

(b) If \(F(\cdot)\cong(\mu,\sigma^{2})\) also, then \(Z_{2n}\equiv\sqrt{n}[\ddot{X}_{n}-(\mu+\theta)]\to_{d}Z_{2}\cong N(0,\sigma^{2})\).

(c) In fact, the limiting normal distribution is given by \((Z_{1n},Z_{2n})\to d(Z_{1},Z_{2})\), where the covariance of the limiting normal distribution is given by

\[\mbox{E}\{[\epsilon-\mu]\times[1_{[\epsilon>0]}-1/2]/F^{\prime}(0)\}. \tag{13}\]

**Proof.** By the event equality \([X_{n}^{r}-\theta\leq y/\sqrt{n}]=[\sum_{1}^{n}1_{[X_{i}-\theta\leq y/\sqrt{n}]}> n/2]\), we have

\[P(\sqrt{n}[X_{n}^{r}-\theta]\leq y)=P(X_{n}^{r}-\theta\leq y/ \sqrt{n})=P(\sum_{1}^{n}1_{[X_{i}-\theta\leq y/\sqrt{n}]}>n/2) \tag{14}\] \[=P(\tfrac{1}{n}\sum_{1}^{n}1_{[\epsilon_{i}\leq y/\sqrt{n}]}>1/2) \qquad\mbox{ since }\epsilon_{i}\equiv X_{i}-\theta\cong F(\cdot)\] \[=P(\tfrac{1}{\sqrt{n}}\sum_{1}^{n}\{1_{[\epsilon_{i}>y/\sqrt{n}]} -\tfrac{1}{2}\}<0)\] (a) \[=P(W_{n}+A_{n}<0).\]

Here

\[W_{n}\equiv\tfrac{1}{\sqrt{n}}\sum_{1}^{n}\{1_{[\epsilon_{i}>0]}-P(\epsilon_{i} >0)\}\to_{d}Z_{1}\cong N(0,\,p(1-p))\]

with \(p\equiv P(\epsilon_{i}>0)=\tfrac{1}{2}\), and (as we will now show)

\[A_{n}\equiv\tfrac{1}{\sqrt{n}}\sum_{1}^{n}\{1_{[\epsilon_{i}>y/\sqrt{n}]}-1_{[ \epsilon_{i}>0]}\}\to_{p}-yF^{\prime}(0). \tag{16}\]

Note that all terms in the summation in \(A_{n}\) are of the same sign. Then

(b) \[A_{n}\to_{p}-yF^{\prime}(0\pm)\qquad\mbox{ according as }y>0\mbox{ or }y<0,\]

since

\[\mbox{E}A_{n}=\sqrt{n}[P(\epsilon>y/\sqrt{n})-P(\epsilon>0)]=-y[F( y/\sqrt{n})-F(0)]/[y/\sqrt{n}]\] (c) \[\to-yF^{\prime}(0\pm)\qquad\mbox{ [ provided only that both }F^{\prime}(0\pm)\mbox{ exist]}\]

and

(d) \[\mbox{Var}[A_{n}]=[F(y/\sqrt{n})-F(0)]\{1-[F(y/\sqrt{n})-F(0)]\}\to 0.\]

Thus \(W_{n}+A_{n}\to_{d}Z_{1}-yF^{\prime}(0)\) via (15), (16), and Slutsky. By \(\to_{d}\) we then have

\[P(\sqrt{n}[X_{n}^{r}-\theta]\leq y)=P(W_{n}+A_{n}<0)\] (e) \[\to P(Z_{1}-yF^{\prime}(0)\leq 0)=P(Z_{1}/F^{\prime}(0)\leq y)\mbox{ for each }y.\]

That is,

(f) \[\sqrt{n}[X_{n}^{r}-\theta]\to_{d}Z_{1}/F^{\prime}(0)\cong N(0,p(1-p)/[F^{ \prime}(0)]^{2}).\]

In like fashion, \([\sqrt{n}[X_{n}^{l}-\theta]\leq y]=[\sum_{1}^{n}1_{[\epsilon_{i}\leq y/\sqrt{n} ]}\geq n/2]\), so that

(g) \[P(\sqrt{n}[X_{n}^{l}-\theta]\leq y)=P(\sum_{1}^{n}1_{[\epsilon_{i}\leq y/\sqrt {n}]}\geq n/2)=P(W_{n}+A_{n}\leq 0).\]

Thus the same argument as before gives

(h) \[\sqrt{n}[X_{n}^{l}-\theta]\to_{d}Z_{1}/F^{\prime}(0).\]Now we squeeze the general \(\tilde{X}_{n}\) in between, via

(i) \[P(\sqrt{n}[X_{n}^{r}-\theta]\leq y)\leq P(\sqrt{n}[\breve{X}_{n}-\theta]\leq y) \leq P(\sqrt{n}[X_{n}^{l}-\theta]\leq y),]\]

where both ends converge to \(P(Z_{1}/F^{\prime}(0)\leq y)\). This completes the proof.

Summary   It has been demonstrated that the events (note (15))

\[\begin{array}{l}[\omega\ :\ \sqrt{n}(\breve{X}_{n}(\omega)-\theta)\leq y]\ \ \mbox{and}\ \ [\omega\ :\ W_{n}(\omega)\leq yF^{\prime}(0)]\\ \mbox{differ by a probabilistically negligible amount.}\end{array} \tag{17}\]

For the joint result, apply (17) and the multivariate CLT to \((W_{n},Z_{2n})\). \(\Box\)

**Exercise 3.6** (Joint asymptotic normality of quantiles): For \(0<p<1\), the \(p\)th _quantile_\(x_{p}\) of \(F\) is now defined as \(x_{p}\equiv F^{-1}(p)\). (a) Show that if \(F\) has a derivative \(F^{\prime}(x_{p})>0\) at \(x_{p}\), then

\[\sqrt{n}[X_{n:[np]}-x_{p}]\rightarrow_{d}N(0,p(1-p)/[F^{\prime}(x_{p})]^{2}) \qquad\mbox{ as }n\rightarrow\infty. \tag{18}\]

(b) Establish joint normality for \(p_{i}\) and \(p_{j}\) quantiles, where the covariance matrix of the asymptotic distribution has \((i,j)\)th entry

\[\sigma_{ij}\equiv[(p_{i}\wedge p_{j})-p_{i}p_{j}]/[F^{\prime}(x_{p_{i}})F^{ \prime}(x_{p_{j}})].\]

Write out the analogue of (17), and use it.

**Exercise 3.7**: What happens when you try to apply (12) to:

\[(a)\,F^{\prime}(x)=\exp(-|x|)/2?\qquad\mbox{ or }\qquad\mbox{ (b) }F^{\prime}(x)=\mbox{$\frac{1}{2}$}1_{[-1,0)}(x)+\mbox{$\frac{1}{4}$}1_{[0,2]}(x)?\]

Show that \(\sqrt{n}[\breve{X}_{n}-\theta]\rightarrow_{d}\) (a rv) in both cases. (In case (b) it is not normal.)

**Exercise 3.8**: Verify (12) both for \(n=2m+1\) odd and for \(n=2m\) even.

Hint. Since \(X_{n:m}\leq\breve{X}_{n}\leq X_{n:m+1}\),

\[P(\sum_{1}^{n}1_{[\epsilon_{i}\leq y/\sqrt{n}]}>\mbox{$\frac{n}{2}$}])\leq P( \sqrt{n}(\breve{X}_{n}-\theta)\leq z)\leq P(\sum_{1}^{n}1_{[\epsilon_{i}\leq y /\sqrt{n}]}\geq\mbox{$\frac{n}{2}$}\ ])\.\]

(The right side is an equality when \(n\) is odd.)

**Exercise 3.9**: Consider (with hypothesis as weak as possible) the asymptotic distribution of (appropriately normalized forms of) both

\[\mbox{$\frac{1}{n}$}\sum_{1}^{n}|X_{k}-\breve{X}_{n}|\qquad\mbox{ and }\qquad\mbox{$\frac{1}{n}$}\sum_{1}^{n}|X_{k}-\breve{X}_{n}| \tag{19}\]

for iid samples \(X_{1},\ldots,X_{n}\) from a df \(F(\mu,\sigma^{2})\) having median \(\nu\).

**Exercise 3.10**: Let \(X_{1},X_{2},\ldots\) be independent with \(X_{k}\cong\mbox{Uniform}(-k,k)\). Then establish that \(S_{n}/\sigma_{n}\rightarrow_{d}N(0,1)\).

**Exercise 3.11**: Determine the limiting distribution of

\[\sum_{k=1}^{n}(X_{k}-X_{2n+1-k})/\{\sum_{k=1}^{n}(X_{k}-X_{2n+1-k})^{2}\}^{1/2},\]

where \(X_{1},X_{2},\ldots\) are iid \((\mu,\sigma^{2})\,\mbox{rvs.}\) Hint. Think "Slutsky."

**Exercise 3.12**: Determine the.95-quantile of the limiting distribution of

\[\prod_{k=1}^{n}U_{k}^{-X_{k/\sqrt{n}}},\]

for independent rvs with \(X_{k}\cong\) Double Exponential(0, 1) and \(U_{k}\cong\) Uniform(0,1).

**Example 3.4** (Weighted sums of iid rvs): Suppose that rvs \(X_{n1},\ldots,X_{nn}\) are row independent and iid \((\mu,\sigma^{2})\). Let \(\mbox{c}_{n}\equiv(c_{n1},\ldots,c_{nn})^{\prime}\) for \(n\geq 1\), and set

\[\bar{c}_{n}\equiv\sum_{k=1}^{n}c_{nk}/n\qquad\mbox{ and }\qquad\sigma_{cn}^{2} \equiv\sum_{k=1}^{n}(c_{nk}-\bar{c}_{n})^{2}/n\equiv\mbox{SS}_{cc}/n.\]

Suppose we have the uan condition

\[{\cal D}_{c}\equiv{\cal D}(\mbox{c}_{n})\equiv\frac{\max_{1\leq k\leq n}(c_{ nk}-\bar{c}_{n})^{2}}{\sum_{k=1}^{n}(c_{nk}-\bar{c}_{n})^{2}}=\frac{[\max_{1 \leq k\leq n}(c_{nk}-\bar{c}_{n})^{2}/n]}{\sigma_{cn}^{2}}\to 0 \tag{20}\]

as \(n\to\infty\). Then

\[\sum_{k=1}^{n}\frac{(c_{nk}-\bar{c}_{n})}{\sqrt{n}\sigma_{cn}}\frac{X_{nk}-\mu }{\sigma}\to_{d}N(0,1). \tag{21}\]

[We need not center the \(c_{nk}\)'s if the \(X_{nk}\)'s have mean 0.]

**Proof.** Without loss of generality, set \(\mu=0\). Now, Lindeberg's condition holds, as we demonstrate via

\[\left|\sum_{k=1}^{n}\int_{[|c_{nk}-\bar{c}_{n}||x|\geq\epsilon\sigma \sqrt{n}\sigma_{cn}]}\left[\frac{(c_{nk}-\bar{c}_{n})}{\sigma\sqrt{n}\sigma_{ cn}}\right]^{2}x^{2}dF(x)\right|\] (a) \[\leq\sigma^{-2}\cdot 1\cdot\int_{[|x|\geq\epsilon\sigma/\sqrt{D_{c}} ]}x^{2}dF(x)\to 0,\]

since \({\cal D}_{c}\to 0\) and \(\int x^{2}dF(x)<\infty\). \(\Box\)

The preceding example is useful in regression situations.

**Exercise 3.13** (Monte Carlo estimation): Let \(h:\,[0,1]\to[0,1]\) be a measurable function, and let \(\theta=\int_{0}^{1}h(t)dt\). Let \(X_{1},Y_{1},X_{2},Y_{2},\ldots\) be iid Uniform(0,1) rvs. Define two different estimators of \(\theta\) by

\[T_{1n}\equiv\sum_{k=1}^{n}h(X_{k})/n\qquad\mbox{ and }\qquad T_{2n}\equiv\sum_{k=1}^{n}1_{[X_{k}\leq h(Y_{k})]}/n.\]

(a) Show that both \(T_{1n}\) and \(T_{2n}\) are unbiased estimators of \(\theta\), and determine which estimator has the smaller variance. Indicate how the variance of each estimator could be estimated.

(b) Determine the joint asymptotic distribution of appropriately normalized forms of \(T_{1n}\) and \(T_{2n}\).

**Exercise 3.14** (An analogue of the student-\(t\) statistic based on quartiles): Let \(X_{1},\ldots,X_{n}\) be iid with df \(F(\cdot).\) Let \(m\equiv[n/4]\), for the greatest integer \([\cdot]\). Let

\[U_{n}\equiv X_{n:m},\quad V_{n}\equiv\ddot{X}_{n}\equiv\mbox{ (the median)},\quad W_{n}\equiv X_{n:n+1-m}\]denote the quartiles and median of the sample. Make appropriate assumptions regarding \(F(\cdot)\).

(a) Determine the joint asymptotic distribution of

\[(\sqrt{n}[V_{n}-\nu],-\sqrt{n}[W_{n}-U_{n}-\mu])\]

for appropriately defined \(\mu\) and \(\nu\).

(b) Simplify this if the \(X_{i}\) are symmetrically distributed about \(0\).

(c) Determine the asymptotic distribution under symmetry of the (student-\(t\) like) statistic (formed from three sample quantiles)

\[T_{n}\equiv\sqrt{n}[V_{n}-\nu]/[W_{n}-U_{n}].\]

**Exercise 3.15** Let the \(X_{k}\)'s be iid Cauchy(0, 1) in the previous exercise.

(d) Evaluate \(F(x)=P(X\leq x)\) for \(x\in R\).

(e) Solve \(p=F(x)\) for \(x_{p}\equiv F^{-1}(p)\), when \(0<p<1\).

(f) Express your answers to (b) and (c) of the previous exercise in the present context.

**Exercise 3.16** (Poisson estimation) Let \(X_{1},\ldots,X_{n}\) be iid Poisson (\(\theta\)).

(a) Reasonable estimators of \(\theta\) include the sample mean \(T_{1n}\equiv\bar{X}_{n}\), the sample variance \(T_{2n}\equiv S_{n}^{2}\), and \(T_{3n}\equiv\sum_{1}^{n}kX_{k}/\sum_{1}^{n}k\) (which puts more emphasis on the more recent observations). Evaluate lim Var\([T_{in}]\) for \(i=1,2,3\).

(b) Verify that \(T_{4n}\equiv X_{n}^{2}-\bar{X}_{n}/n\) and \(T_{5n}\equiv\overline{X_{n}^{2}}-\bar{X}_{n}\) are both unbiased estimators of \(\theta^{2}\). Evaluate lim Var \([T_{in}]\) for \(i=4,5\).

(c) Determine the asymptotic distribution of \(D_{n}\equiv\sqrt{n}[\bar{X}_{n}-S_{n}^{2}]/\bar{X}_{n}\) when the observations really do follow a Poisson distribution.

(d) What is the asymptotic distribution of \(D_{n}\) when the observations \(X_{k}\) actually follow a NegBiT (r,\(p\)) distribution?

**Theorem 3.1 (Doeblin's CLT for a random number of rvs)** Consider iid \((0,\sigma^{2})\) rvs \(X_{1},X_{2},\ldots\). Let \(\{\nu_{n}\}_{n=1}^{\infty}\) be integer-valued rvs such that the proportion \(\nu_{n}/n\to_{p}c\in(0,\infty)\) as \(n\to\infty\). Let \(T_{n}\equiv X_{1}+\cdots+X_{n}\) denote the total. Then

\[T_{\nu_{n}}/\sqrt{\nu_{n}}\to_{d}N(0,\sigma^{2}).\]

[Note that \(\nu_{n}\) and \(X_{1},X_{2},\ldots\) need _not_ be independent.]

**Proof.** Now,

\[\frac{T_{\nu_{n}}}{\sqrt{\nu_{n}}}=\sqrt{\frac{[cn]}{\nu_{n}}}\left\{\frac{T_{ [cn]}}{\sqrt{[cn]}}+\frac{T_{\nu_{n}}-T_{[cn]}}{\sqrt{[cn]}}\right\}.\]

Note that \(T_{[cn]}/\sqrt{[cn]}\to_{d}N(0,1)\) and \([cn]/\nu_{n}=\frac{[cn]}{cn}\frac{c}{\nu_{n}/n}\to_{p}1\). In the next paragraph we will show that

(a) \[(T_{\nu_{n}}-T_{[cn]})/\sqrt{[cn]}\to_{p}0.\]

The theorem then follows from Slutsky's theorem.

We now let \(A_{n}\equiv[|T_{\nu_{n}}-T_{[cn]}|/\sqrt{[cn]}>\epsilon]\), and note that

(b) \[P(A_{n})=\sum_{k=1}^{\infty}P(A_{n}\cap[\nu_{n}=k])\] \[\qquad=\sum_{\{k:|k-[cn]|\leq\epsilon^{3}cn\}}P(A_{n}\cap[\nu_{n}=k ])+\sum_{\{k:|k-[cn]|>\epsilon^{3}cn\}}P(A_{n}\cap[\nu_{n}=k])\] (c) \[\qquad\equiv\sum_{1}+\sum_{2}.\]

Since \(\nu_{n}/[cn]\to_{p}1\), for \(n\) sufficiently large we have

(d) \[\sum {}_{2}\leq P(|\nu_{n}-[cn]|>\epsilon^{3}cn)\leq P\left(|\frac{\nu_ {n}}{[cn]}-1|>\epsilon^{3}\frac{cn}{[cn]}\right)\] \[\qquad\leq P\left(|\frac{\nu_{n}}{[cn]}-1|>\epsilon^{3}\right)<\epsilon.\]

Also, applying Kolmogorov's inequality twice,

\[\sum {}_{1}\leq P\left(\max_{|k-[cn]|\leq\epsilon^{3}cn}|T_{k}-T_{[cn] }|>\epsilon\sqrt{[cn]}\right)\] \[\qquad\leq P\left(\max_{[cn]\leq k\leq\epsilon^{3}cn}|T_{k}-T_{[ cn]}|>\epsilon\sqrt{[cn]}\right)\] \[\qquad+P\left(\max_{[cn]-\epsilon^{3}cn\leq k\leq[cn]}|T_{k}-T_{[ cn]}|>\epsilon\sqrt{[cn]}\right)\] \[\qquad\leq 2\sum_{k=[cn]+1}\mbox{Var }[X_{k}]/\epsilon^{2}[cn]\leq 2 \epsilon^{3}cn\sigma^{2}/\epsilon^{2}[cn]\] (e) \[\qquad\leq 4\sigma^{2}\epsilon\qquad\mbox{for all $n$ sufficiently large.}\]

Combining (d) and (e) into (c) shows \(P(A_{n})\to 0\), as required. 

**Exercise 3.17**: Let \(V_{n}^{2}\) now denote the sample variance. Show, in the context of Doeblin's CLT, that \(T_{\nu_{n}}/V_{\nu_{n}}\to_{d}N(0,1)\) as \(n\to\infty\).

**Exercise 3.18**: Prove a version of Doeblin's theorem for \(X_{nk}\)'s independent but not iid; assume the Lindeberg condition and \(\nu_{n}/n\to_{p}c\in(0,\infty)\). [Revert to the Liapunov condition, if necessary.]

**Exercise 3.19** (Sample correlation coefficient \(R_{n}\); Cramer and Anderson) Let us suppose that

\[\left[\begin{array}{c}X_{i}\\ Y_{i}\end{array}\right]\mbox{ are iid }\left[\left[\begin{array}{c}0\\ 0\end{array}\right],\left[\begin{array}{c}1&\rho\\ \rho&1\end{array}\right]\right]\qquad\mbox{ for $1\leq i\leq n$},\]

and that the \(\Sigma\) below has finite entries. Consider \(\sqrt{n}[R_{n}-\rho]\), where \(R_{n}\) is the _sample correlation coefficient_. Thus \(R_{n}\equiv\mbox{SS}_{XY}/\{\mbox{SS}_{XX}\mbox{SS}_{YY}\}^{1/2}\) for the sums of squares \(\mbox{SS}_{XY}\equiv\sum_{1}^{n}(X_{i}-\bar{X}_{n})(Y_{i}-\bar{Y}_{n})\), etc.

(a) Reduce the case of general means, variances and covariances to this case.

(b) Note that

\[\left[\begin{array}{c}\frac{1}{\sqrt{n}}\sum_{1}^{n}X_{i}Y_{i}-\rho\\ \frac{1}{\sqrt{n}}\sum_{1}^{n}(X_{i}^{2}-1)\\ \frac{1}{\sqrt{n}}\sum_{1}^{n}(Y_{i}^{2}-1)\end{array}\right]\to_{d}\left[ \begin{array}{c}Z_{1}\\ Z_{2}\\ Z_{3}\end{array}\right]\cong N(0,\Sigma) \tag{24}\]with

\[\Sigma\equiv\left[\begin{array}{ccc}\mbox{E}(X^{2}Y^{2})-\rho^{2}&\mbox{E}(X^{3}Y) -\rho&\mbox{E}(XY^{3})-\rho\\ \mbox{E}(X^{3}Y)-\rho&\mbox{E}X^{4}-1&\mbox{E}(X^{2}Y^{2}-1\\ \mbox{E}(XY^{3})-\rho&\mbox{E}(X^{2}Y^{2})-1&\mbox{E}Y^{4}-1\end{array}\right]. \tag{25}\]

(c) Then show that \(\sqrt{n}[R_{n}-\rho]\rightarrow_{d}Z_{1}-\frac{\rho}{2}Z_{2}-\frac{\rho}{2}Z_{ 3}\cong N(0,\tau^{2})\), and evaluate \(\tau^{2}\).

(d) Show that when \(X\) and \(Y\) are independent, then \(\sqrt{n}[R_{n}-\rho]\rightarrow_{d}N(0,1)\).

(e) If the \((X_{i},Y_{i})^{\prime}\) are jointly normal, show that

\[\Sigma=\left[\begin{array}{ccc}1+\rho^{2}&2\rho&2\rho\\ 2\rho&2&2\rho^{2}\\ 2\rho&2\rho^{2}&2\end{array}\right]. \tag{26}\]

Then simplify the expression for \(\tau^{2}\) and obtain

\[\sqrt{n}[R_{n}-\rho]\rightarrow_{d}N(0,(1-\rho^{2})^{2}). \tag{27}\]

(f) Show that \(\sqrt{n}[g(R_{n})-g(\rho)]\rightarrow_{d}N(0,1)\) for \(g(t)\equiv\frac{1}{2}\log(\frac{1\pm t}{1-t})\).

(g) Approximating the distribution of \(\sqrt{n-3}[g(R_{n})-g(\rho)-\frac{\rho}{2(n-1)}]\) by \(N(0,1)\) yields an excellent result.

(h) Show that \(\mbox{Cov}[X_{i}-\bar{X}_{n},\ Y_{i}-\bar{Y}_{n}]=(1-\frac{1}{n})\mbox{Cov}[X_{ i},\ Y_{i}]\).

**Exercise 3.20** (Extreme Value quantiles): Let \(X\) and \(X_{1},\ldots,X_{n}\) be iid with the _Weibull_\((\alpha,\beta)\) density \(f(x)=(\beta x^{\beta-1}/\alpha^{\beta})\)\(\exp(-(x/\alpha)^{\beta})\) on \(x\geq 0\). Now, \((X/\alpha)^{\beta}\cong\) Exponential(l), and thus \(Y\equiv\log X\) satisfies

\[Y\cong\nu+\tau W\qquad\mbox{ where }\nu\equiv\log\alpha\mbox{ and }\tau\equiv 1/\beta\]

and \(W\) has the _Extreme Value_ density for minima given by \(\exp(w-e^{w})\) on \((-\infty,\infty)\). Let \(Y_{n:1}\leq\cdots\leq Y_{n:n}\) denote the order statistics of the rvs \(Y_{k}\equiv\log X_{k}\). First, let \(0<p_{1}<p_{2}<1\), and then define \(U_{n}\equiv Y_{n:[np_{1}]}\) and \(V_{n}\equiv Y_{n:[np_{2}]}\). We seek values \(p_{1}\) and \(p_{2}\) such that

\[\left[\begin{array}{c}\sqrt{n}(V_{n}-\nu)\\ \sqrt{n}(V_{n}-U_{n}-\tau)\end{array}\right]\rightarrow_{d}N(\mbox{O},\Sigma)\.\]

(a) Let \(0<p<1\). Evaluate \(y_{p}\equiv F_{Y}^{-1}(p),f_{Y}(y_{p})\) and \(p(1-p)/f_{Y}^{2}(y_{p})\).

(b) Determine values of \(p_{1}\) and \(p_{2}\) that achieve the objective.

(c) Establish the claimed asymptotic normality, and evaluate \(\Sigma\) both symbolically and numerically.

**Exercise 3.21** (Estimating a common normal mean): Consider independent rvs \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) from \(N(\theta,\sigma^{2})\) and \(N(\theta,\tau^{2})\). When \(\gamma\equiv\sigma^{2}/\tau^{2}\) is known, the unbiased estimator of \(\theta\) that has minimum variance (for all possible values of the parameters within this model) is known to be

\[\hat{\theta}_{o}\equiv\hat{\theta}_{o,mn}\equiv\frac{m\bar{X}_{m}+\gamma\cdot n \bar{Y}_{n}}{m+\gamma\cdot n}.\]

Define \(\alpha\equiv\alpha_{mn}\equiv m/(m+\gamma\cdot n)\). Let \(\hat{\alpha}\equiv\hat{\alpha}_{mn}(S_{X}^{2},S_{Y}^{2})\) depend only on the two sample variances \(S_{X}^{2}\equiv\sum_{1}^{m}(X_{i}-\bar{X}_{m})^{2}/(m-1)=\mbox{SS}_{XX}/(m-1)\) and \(S_{Y}^{2}\equiv\sum_{1}^{n}(Y_{j}-\bar{Y}_{n})^{2}/(n-1)=\mbox{SS}_{YY}/(n-1)\), and suppose that \(\hat{\alpha}\) is a rv with values in \([0,1]\). We hypothesize that

\[\hat{\alpha}^{2}/\alpha^{2}\rightarrow_{p}1\qquad\mbox{ as }m\wedge n\rightarrow\infty.\](All limits below are to be taken as \(m\wedge n\to\infty.\)) Then define

\[\hat{\theta}\equiv\hat{\theta}_{mn}=\hat{\alpha}\bar{X}_{n}+(1- \hat{\alpha})\bar{Y}_{n},\] \[v_{o}^{2}\equiv v_{omn}^{2}\equiv\frac{1}{m}\alpha^{2}\sigma^{2}+ \frac{1}{n}(1-\alpha)^{2}\tau^{2},\] \[\hat{v}^{2}\equiv\hat{v}_{mn}^{2}\equiv\frac{1}{m}\alpha^{2}S_{X} ^{2}+\frac{1}{n}(1-\alpha)^{2}S_{Y}^{2},\] \[\hat{V}^{2}\equiv\hat{V}_{mn}^{2}\equiv\frac{1}{m}\hat{\alpha}^{2 }S_{X}^{2}+\frac{1}{n}(1-\hat{\alpha})^{2}S_{Y}^{2},\] \[\tilde{\alpha}\equiv\tilde{\alpha}_{mn}\equiv\frac{(m-1)}{(m-1)+ (n-1)S_{X}^{2}/S_{Y}^{2}}=\mbox{SS}_{YY}/(\mbox{SS}_{XX}+\mbox{SS}_{YY}).\]

Note that \(\tilde{\gamma}\equiv\tilde{\gamma}_{mn}\equiv S_{X}^{2}/S_{Y}^{2}\to_{p}\gamma\), and \(\tilde{\alpha}^{2}/\alpha^{2}\to_{p}1\) is indeed true.

(a) Show that \(\mbox{E}\hat{\theta}=\theta\).

(b) Show that \((\hat{\theta}-\theta)/v_{o}\to_{d}N(0,1)\).

(c) Show that \(|\hat{v}^{2}-v_{o}^{2}|/v_{o}^{2}\to_{p}0\) and \(|\hat{V}^{2}-\hat{v}^{2}|/\hat{v}^{2}\to_{p}0\).

(d) Thus \((\hat{\theta}-\theta)/\hat{V}\to_{d}N(0,1)\).

(e) Evaluate \(v^{2}\equiv v_{mn}^{2}\equiv\mbox{Var}[\hat{\theta}]\) in terms of \(\mbox{E}\hat{\alpha}^{2}\) and \(\mbox{E}(1-\hat{\alpha})^{2}\).

(f) Determine the distribution of \(\tilde{\alpha}\). Does \(\tilde{\alpha}/\alpha\to_{{\cal L}_{2}}1\)?

**Exercise 3.22** (Exponential estimation) Let \(X_{1},\ldots,X_{n}\) be iid Exponential (\(\theta\)). The minimum variance estimator of \(\theta\) is known to be the sample mean \(\bar{X}_{n}\). Another unbiased estimator of \(\theta\) is \(T_{n}\equiv\bar{G}_{n}/\Gamma^{n}(1+1/n)\), where \(\bar{G}_{n}\equiv(\prod_{1}^{n}X_{k})^{1/n}\) denotes the geometric mean of the observations. Evaluate the limiting ratio of the variances \(\lim\mbox{Var}[\bar{X}_{n}]/\mbox{Var}[T_{n}]\).

**Exercise 3.23** Let \(X_{1},\ldots,X_{n}\) be iid Poisson(\(\lambda\)). Show the moment convergence \(\mbox{E}|\bar{X}_{n}-\lambda|^{3}\to\mbox{E}|N(0,1)|^{3}\).

**Statistical Applications**

**Exercise 3.24** (Simple linear regression) Consider the simple linear regression model of (A.3.25); thus we are assuming that

\[Y_{nk}=\gamma+\beta x_{nk}+\epsilon_{k}\equiv\alpha+\beta(x_{nk}-\bar{x}_{n})+ \epsilon_{k}\qquad\mbox{ for iid rvs }\epsilon_{k}\cong(0,\sigma^{2}) \tag{28}\]

and for known constants \(x_{nk}\) for \(1\leq k\leq n\). The least squares estimators (LSEs) \(\hat{\alpha}_{n}\) and \(\hat{\beta}_{n}\) of \(\alpha\) and \(\beta\) are defined to be those values of \(a\) and \(b\) that minimize the sum of squares \(\sum_{1}^{n}[Y_{nk}-(a+b(x_{nk}-\bar{x}_{n})]^{2}\).

(a) Show that the LSEs are given by

\[\hat{\alpha}_{n} =\bar{Y}_{n}\] and \[\hat{\beta}_{n} =\frac{\Sigma_{1}^{n}(x_{nk}-\bar{x}_{n})Y_{nk}}{\Sigma_{1}^{n}(x_ {nk}-\bar{x}_{n})^{2}}\equiv\sum_{k=1}^{n}d_{nk}Y_{nk}. \tag{29}\]

(b) Let \(\mbox{SS}_{xx}\equiv\sum_{1}^{n}(x_{nk}-\bar{x}_{n})^{2},\mbox{d}_{n}=(d_{n1}, \ldots,d_{nn})^{\prime},\mbox{x}_{n}\equiv(x_{n1},\ldots,x_{nn})^{\prime}\), and

\[{\cal D}(\mbox{x}_{n})\equiv\Bigl{[}\max_{1\leq k\leq n}|x_{nk}-\bar{x}_{n}|^{2 }\Bigr{]}/\mbox{SS}_{xx}=\Bigl{[}\max_{1\leq k\leq n}d_{nk}\Bigr{]}={\cal D}( \mbox{d}_{n}). \tag{30}\]Use the Cramer-Wold device and the weighted sums of example 10.3.4 to show that

\[\left[\begin{array}{c}\sqrt{n}\,[\hat{\alpha}_{n}-\alpha]\\ \sqrt{\mbox{SS}_{xx}}\,[\hat{\beta}_{n}-\beta]\end{array}\right]\to_{d}N({\sf O},\sigma^{2}I),\quad\mbox{ provided that}\quad{\cal D}({\sf d}_{n})={\cal D}({\sf x }_{n})\to 0 \tag{31}\]

(recall (10.3.20)). [Note also that the LSE \(\hat{\beta}_{n}\) of \(\beta\) is given by \(\hat{\beta}_{n}=\mbox{SS}_{xY}/\mbox{SS}_{xx}\).]

**Definition 3.1** (Noncentral distributions): (a) Let \(X_{1},\ldots,X_{m}\) be independent, and suppose that \(X_{i}\cong N(\theta_{i},\;\sigma^{2})\). Let \(\theta^{2}\equiv\sum_{1}^{m}\theta_{i}^{2}\), and define \(\delta\) via \(\delta^{2}\equiv\theta^{2}/\sigma^{2}.\) Show that the quadratic form

\[U\equiv\sum_{1}^{m}X_{i}^{2}/\sigma^{2}\cong(Z_{1}+\theta)^{2}+\sum_{i=2}^{m} Z_{i}^{2}, \tag{32}\]

where \(Z_{1},\ldots,Z_{m}\) are iid N\((0,1)\) rvs. Denote this distribution by

\[U\cong\chi_{m}^{2}(\delta^{2}/2), \tag{33}\]

and say that \(U\) is distributed as _noncentral chisquare_ with \(m\) degrees of freedom and noncentrality parameter \(\delta\).

(b) Let \(Y\cong N(\theta,1),U\cong\chi_{m}^{2}(\delta^{2}/2)\) and \(V\cong\chi_{n}^{2}\) be independent rvs. We define the noncentral Student-\(t_{n}(\theta)\) distribution and the noncentral Snedecor-\(F_{m,n}(\delta^{2}/2)\) via

\[T_{n}(\theta)\equiv\frac{Y}{V/n}\cong\mbox{Student-}t_{n}(\theta)\mbox{ and }\frac{n}{m}\frac{U}{V}=\frac{U/m}{V/n}\cong\mbox{Snedecor-}F_{m,n}(\delta^{2}/2). \tag{34}\]

(Note that \(T_{n}^{2}(\theta)\cong F_{1,n}(\theta^{2}/2)\).)

**Proposition 3.1** (Form of the noncentral distributions): Consider the rvs \(U,V,\) and \(Y\) of the previous definition. Let \(y>0.\) (a) The rv \(U\) of (32) satisfies

\[P(\chi_{m}^{2}(\delta^{2}/2)>y)=\sum_{k=0}^{\infty}P(\mbox{Poisson}(\delta^{2} /2)=j)\times P(\chi_{m+2j}^{2}>y). \tag{35}\]

Here, Poisson(\(\lambda\)) denotes a Poisson rv with mean \(\lambda\), and \(\chi_{r}^{2}\) denotes an ordinary chisquare rv with \(r\) degrees of freedom.

(b) It is thus trivial that

\[P(F_{m,n}(\delta^{2}/2)>y)=\sum_{k=0}^{\infty}P(\mbox{Poisson}(\delta^{2}/2)=j )\times P(F_{m+2j,n}>y). \tag{36}\]

(c) For \(C_{n}\equiv 2^{(n+1)/2}\Gamma(n/2)\sqrt{\pi n}\) we have

\[P(T_{n}(\delta)>y) \tag{37}\] \[=\frac{1}{c_{n}}\int_{y}^{\infty}\int_{0}^{\infty}u^{(n-1)/2}e^{- u/2}\exp(-\frac{1}{2}(v(\frac{u}{n})^{1/2}-\delta)^{2})du\,dv.\]

**Exercise 3.25**: Prove proposition 3.1.

**Exercise 3.26** (Chisquare goodness of fit, again): (a) (Local alternatives) We suppose that the statistic \(Q_{n}\equiv Q_{n}({\sf p}_{0})\) of (10.1.13) is computed, but that in reality the true parameter vector is now \({\sf p}_{n}\equiv{\sf p}_{0}+{\sf a}/\sqrt{n}\) (with \(\sum_{1}^{k}a_{i}=0\), so that the coordinates \(p_{ni}\) add to 1). Let \(\hat{p}_{ni}\equiv N_{ni}/n\) estimate \(p_{ni}\) for \(1\leq i\leq k\). Show that the vector

\[{\sf W}_{n}^{k\times 1}\equiv\left|[\sqrt{n}(\hat{p}_{ni}-p_{0i})/\sqrt{p_{0i}} \,]\right|\to_{d}{\sf W}+{\sf d}, \tag{38}\]where \({\sf W}\cong N({\sf O},I-\sqrt{\sf p}\sqrt{\sf p^{\prime}})\) and \(d_{i}\equiv a_{i}/\sqrt{p_{0i}}\) for \(1\leq i\leq k\). Thus

\[Q_{n}={\sf W}^{\prime}_{n}{\sf W}_{n}\to_{d}Q\equiv({\sf W}+{\sf d})^{\prime}({ \sf W}+{\sf d})\cong\chi_{k-1}^{2}({\sf d}^{\prime}{\sf d}/2). \tag{39}\]

(b) (Fixed alternatives) Suppose that \(Q_{n}\equiv Q_{n}({\sf p}_{0})\) is computed, but a fixed \({\sf p}\) is true. Show that

\[\frac{1}{n}Q_{n}\to_{a.s.}\ \ \sum_{i=1}^{k}(p_{i}-p_{0i})^{2}/p_{0i}. \tag{40}\]

**Exercise 3.27**  Suppose \({\sf X}\cong N(\vec{\theta},\Sigma)\), with rank \((\Sigma)=r\). Show that

\[{\sf X}^{\prime}\Sigma^{-}{\sf X}=Y^{\prime}Y\cong\chi_{r}^{2}(\vec{\theta}^ {\prime}\Sigma^{-}\vec{\theta}/2),\qquad\mbox{ where} \tag{42}\] \[Y\equiv\Sigma^{-1/2}X=(\Gamma D^{-1/2}\Gamma^{\prime})X\cong N \left(\Sigma^{-1/2}\theta,\left[\begin{array}{cc}I_{r}&0\\ 0&0\end{array}\right]\right). \tag{41}\]

## 4 Local Limit Theorems \({}^{o}\)

Recall from Scheffe's theorem that if \(f_{n}\) and \(f\) are densities with respect to some dominating measure \(\mu\), then

\[f_{n}(x)\to f(x)\quad\mbox{a.e. }\mu \tag{1}\]

implies that

\[d_{TV}(P_{n},\ P)\equiv\sup_{B\in{\cal B}}|P(X_{n}\in B)-P(X\in B)|\to 0 \qquad\mbox{ as }n\to\infty. \tag{2}\]

Thus convergence of densities implies convergence in total variation distance, which is stronger than convergence in distribution. We will now establish (1) in a CLT context, for summands that are either suitably continuous or else are distributed on a grid.

**Theorem 4.1** (Local limit theorem, continuous case): Let \(X,X_{1},X_{2},\ldots\) be iid \((0,\sigma^{2})\) with \(\int_{-\infty}^{\infty}|\phi_{X}(t)|dt<\infty\). Then \(S_{n}/\sqrt{n}\) has a density \(f_{n}(\cdot)\) for which

\[\sup_{-\infty<x<\infty}|f_{n}(x)-(1/\sigma)f_{Z}(x/\sigma)|\to 0\qquad\mbox{ as }n\to\infty, \tag{3}\]

for the \(N(0,1)\) density \(f_{Z}(\cdot)\)

**Theorem 4.2** (Local limit theorem, discrete case): Let \(X_{1},X_{2},\ldots\) be iid \((0,\sigma^{2})\) rvs that take values on the grid \(a\pm md\) for \(m=0,\pm 1,\pm 2,\ldots\). Now let \(x^{*}=(na+md)/(\sigma\sqrt{n}),\)_and_\(m=0,\pm 1,\pm 2,\ldots\) denote a possible value of \(S_{n}/(\sigma\sqrt{n})\), and let \(p_{n}(x)\equiv P(S_{n}/(\sigma\sqrt{n})=x)\). Then

\[\sup_{-\infty<x^{*}<\infty}|(\sigma\sqrt{n}/d)p_{n}(x)-f_{Z}(x)|\to 0\qquad \mbox{ as }n\to\infty, \tag{4}\]

for the \(N(0,1)\) density \(f_{Z}(\cdot)\).

**Example 4.1**: Let \(X_{1},\ldots,X_{n}\) be iid Bernoulli\((\theta)\) rvs. Then (by (4))

\[\sup_{0\leq m\leq n}\sqrt{n}\left|P(S_{n}=m)-\frac{1}{\sqrt{n\theta(1-\theta)} }f_{Z}\left(\frac{m-n\theta}{\sqrt{n\theta(1-\theta)}}\right)\right|\to 0.\qed \tag{5}\]

**Exercise 4.1***: Verify (5) by direct computation.

**Exercise 4.2**: Give an example where \(X_{n}\to_{d}X\), but (2) fails.

**Proof.** Consider theorem 4.1. Without loss of generality we may suppose that \(\sigma=1\). Notice that

(a) \[\int|\phi_{S_{n}/\sqrt{n}}(t)|dt<\infty,\quad\mbox{ since }\quad|\phi_{S_{n}/ \sqrt{n}}(t)|=|\phi_{X}(t/\sqrt{n})|^{n}\leq|\phi_{X}(t/\sqrt{n}|).\]

Thus the Fourier inversion formula of (9.4.9) gives

(b) \[f_{n}(y)\equiv f_{S_{n}/\sqrt{n}}(y)=(1/2\pi)\int_{-\infty}^{\infty}e^{-ity} \phi_{S_{n}/\sqrt{n}}(t)dt.\]

This same formula also holds for the distribution of a N\((0,1)\) rv \(Z\). Thus

(c) \[2\pi|f_{n}(x)-f_{Z}(x)|\leq\int_{-\infty}^{\infty}|\phi_{X}^{n} (t/\sqrt{n})-e^{-t^{2}/2}|dt\] \[\quad=(\int_{[|t|\leq a]}+\int_{[a<|t|<\delta\sqrt{n}]}+\int_{[|t |\geq\delta\sqrt{n}]})|\phi_{X}^{n}(t/\sqrt{n})-e^{-t^{2}/2}|dt\] (d) \[\quad\equiv I_{1n}+I_{2n}+I_{3n}.\]We first specify \(\delta>0\) so small that \(|\phi_{X}(t)|\leq\exp(-t^{2}/4)\) for \(|t|\leq\delta\). This is possible, since \(|\phi(t)|=|1-0-t^{2}/2|+|o(t^{2})|\leq 1-t^{2}/4\leq\exp(-t^{2}/4)\) in some neighborhood \(|t|\leq\delta\), by inequality 9.6.2.

Thus for \(a\) specified large enough (since \(|\phi_{X}(t/\sqrt{n})|^{n}\leq e^{-t^{2}/4}\) for \(|t|<\delta\sqrt{n}\)) we have

(e) \[I_{2n}\leq\int_{|[t]>a]}2e^{-t^{2}/4}dt<\epsilon.\]

For this fixed large \(a\) we have

(f) \[I_{1n}<\epsilon\qquad\mbox{ for }n\geq\mbox{ (some }n_{1}),\]

since the Cramer-Levy theorem implies that the convergence of these chfs is uniform on any \(|t|\leq a\).

Now, \(X\) is not distributed on a grid (we have a formula for the density). Thus \(|\phi_{X}(t)|<1\) for all \(t\neq 0\), by proposition 9.8.2. Moreover, \(|\phi_{X}(t)|\to 0\) as \(|t|\to\infty\), by the Riemann-Lebesgue lemma, giving \(|\phi_{X}(t)|<(\mbox{some }\theta)<1\) for \(|t|>\) (some \(\lambda\)). Thus, since \(\theta<1\),

\[I_{3n}<\theta^{n-1}\int_{-\infty}^{\infty}|\phi_{X}(t/\sqrt{n}) |dt+\int_{[|t|>\delta\sqrt{n}]}e^{-t^{2}/2}dt\] \[\qquad=\sqrt{n}\theta^{n-1}\int_{-\infty}^{\infty}|\phi_{X}(t)|dt +2(\delta\sqrt{n})^{-1}e^{-n\delta^{2}/2} \tag{6}\] \[\qquad=o(n^{-r})\qquad\mbox{ for any }r>0\qquad\mbox{ whenever }\qquad\overline{\lim}_{|t|\to 0}\,|\phi_{X}(t)|<1.\] (g) \[<\epsilon\qquad\mbox{ for }n\geq\mbox{ (some }n_{2}).\]

Combining (e), (f), and (g) into (d) establishes the claim made in the theorem. 

**Proof.** Consider theorem 4.2. By the inversion formula (9.8.1) given for distributions on grids,

(h) \[\frac{\sigma\sqrt{n}}{d}p_{n}(x)=\frac{\sigma\sqrt{n}}{d}\frac{d}{\sigma\sqrt{n }2\pi}\int_{-\pi\sigma\sqrt{n}/d}^{\pi\sigma\sqrt{n}/d}\phi_{X}^{n}(t/\sqrt{n })e^{-itx}dt.\]

By the inversion formula (9.4.9) given for densities,

(i) \[f_{Z}(x)=(1/2\pi)\int_{-\infty}^{\infty}e^{-t^{2}/2}e^{-itx}dt.\]

Thus

(j) \[|(\sigma\sqrt{n}/d)p_{n}(x)-f_{Z}(x)|\] \[\qquad\leq(1/2\pi)\int_{-\pi\sigma\sqrt{n}/d}^{\pi\sigma\sqrt{n} /d}|\phi_{X}^{n}(t/\sqrt{n})-e^{-t^{2}/2}|dt\] \[\qquad+(1/2\pi)\int_{|[t|>\pi\sigma\sqrt{n}/d]}e^{-t^{2}/2}dt\] (k) \[\qquad=(\int_{|[t|\leq a]}+\int_{[a<|t|<\delta\sqrt{n}]}+\int_{[ \delta\sqrt{n}<|t|<\pi\sigma\sqrt{n}/d]})|\phi_{X}^{n}(t/\sqrt{n})-e^{-t^{2}/ 2}|dt\] \[\qquad+o(n^{-r})\] (l) \[\qquad\equiv I_{1n}+I_{2n}+I_{3n}+o(n^{-r}).\]

The proof of theorem 4.1 applies, virtually verbatim; the only thing worthy of note is that \(0<\theta<1\) now holds, since \(\pi\sigma\sqrt{n}/d\) is only \(\frac{1}{2}\) of the period.

## 5 Normality Via Winsorization and Truncation\({}^{*}\)

**Definition 5.1** (a) Call \(X_{n1},\ldots,X_{nn}\)_weakly negligible_ (or, _strongly negligible_) if

\[M_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}|]\to_{p}0\qquad(\mbox{or, if }M_{n}\to_{a.s.} 0). \tag{1}\]

(b) Call them _uniformly asymptotically negligible_ (or \(uan\)) under the weaker

\[\max_{1\leq k\leq n}P(|X_{nk}|\geq\epsilon)\to 0\qquad\mbox{for all }\epsilon>0. \tag{2}\]

The objective is to now investigate when some version of the CLT holds for uan pieces, as in (2). (That is, there are to be no monsters among the many, no giants among the peons, etc., and the law of the mob is to hold sway. These are cute descriptive phrases others have used to summarize the problem very cleverly.) The results in theorem 5.1 are classical, while theorem 5.2 and its iid consequences were developed with statistical intent in Shorack (2000)--and are streamlined herein.

**Theorem 5.1** (Asymptotic normality): Let \(X_{n1},\ldots,X_{nn}\) be independent rvs. Let \(a>0\) and \(b\) be arbitrary. As above, let \(M_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}|]\). Fix a truncation constant; let it be any \(c>0\). Let \(\check{X}_{nk}\) be the Truncated rv that equals \(X_{nk}\) or \(0\) according as \(|X_{nk}|\leq c\) or as \(|X_{nk}|>c\). The following are equivalent:

\[\sum_{k=1}^{n}X_{nk}\to_{d}N(b,a^{2})\qquad\mbox{ for uan rvs }X_{n1},\ldots,X_{nn}. \tag{3}\]

\[\begin{array}{ll}\sum_{k=1}^{n}P(|X_{nk}|\geq\epsilon)\to 0&\mbox{ for all }\epsilon>0\qquad(\mbox{equivalently, }M_{n}\to_{p}0),\mbox{ while}\\ \check{\mu}_{n}\equiv\sum_{k=1}^{n}\mbox{E}\check{X}_{nk}\to b&\mbox{ and }&\check{\sigma}_{n}^{2}\equiv\sum_{k=1}^{n}\mbox{Var}[\check{X}_{nk}]\to a^{2}\end{array} \tag{4}\]

If (4) holds for one \(c>0\), then it holds for each \(c>0\). These results also hold with Winsorized quantities \(\check{X}_{nk},\check{\mu}_{n},\check{\sigma}_{n}^{2}\) replacing the Truncated quantities \(\check{X}_{nk},\check{\mu}_{n},\check{\sigma}_{n}^{2}.\) Thus (3) and the theorem of types imply both \(\check{\sigma}_{n}^{2}/\check{\sigma}_{n}^{2}\to 1\) and \((\check{\mu}_{n}-\check{\mu}_{n})/\check{\sigma}_{n}\to 0.\) Nor would it have mattered if we had truncated outside \((-c,c)\) instead of \([-c,c]\).

**Corollary 1** Suppose that \(Y_{n}\equiv\sum_{1}^{n}X_{nk}\to_{d}\) (some rv \(Y\)) for rvs \(X_{nk}\) that are row independent and uan. Then

\[Y\mbox{ has a Normal distribution}\qquad\mbox{ iff }\qquad M_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}|]\to_{p}0. \tag{5}\]

**Corollary 2** Let the \(X_{nk}\) above be symmetric. Then (3) (with \(b=0\)) holds iff

\[\sum_{k=1}^{n}X_{nk}^{2}\to_{p}a^{2}\qquad\qquad\qquad\mbox{ for uan rvs }X_{n1},\ldots,X_{nn}. \tag{6}\]

**Proof.** Suppose (4). Then all \(P(\sum_{1}^{n}X_{nk}\neq\sum_{1}^{n}\check{X}_{nk})\leq\sum_{1}^{n}P(|X_{nk}| \geq c)\to 0\), so that \(\sum_{1}^{n}X_{nk}=_{a}\sum_{1}^{n}\check{X}_{nk}\). Thus we need only show that the normalized rv \(\check{Z}_{n}\equiv(\sum_{1}^{n}\check{X}_{nk}-\check{\mu}_{n})/\check{\sigma }_{n}\to_{d}N(0,1)\). It suffices to verify that Lindeberg's \(\check{L}F_{n}^{*}\) of (10.2.11) satisfies \(\check{L}F_{n}^{*}\to 0\). We will do so presently. First, note that \(M_{n}\to_{p}0\), by (8.3.14). Thus \([\max|\check{X}_{nk}|]\leq M_{n}\to_{p}0\) (and \([\max|\check{X}_{nk}|]\leq M_{n}\to_{p}0\)). Thus

(a) \[\check{m}_{n}\equiv\max_{k}|\check{\mu}_{nk}|\leq\mbox{E}\max_{k}|\check{X}_{nk }|\to 0,\qquad(\mbox{and}\quad\check{m}_{n}\equiv\max_{k}|\check{\mu}_{nk}|\to 0),\]by the DCT with dominating function "\(c\)." Thus for \(n\geq\) (some \(n_{\epsilon}\)) we have

(b) \[\tilde{m}_{n}\leq\epsilon\,a/8\mbox{ and }\check{\sigma}_{n}\geq a/2\qquad(\mbox{ and}\quad\tilde{m}_{n}\leq\epsilon\,a/8,\quad\mbox{and}\quad\tilde{\sigma}_{n}\geq a/2).\]

Then (3) must hold, since the Lindeberg-Feller quantity \(\tilde{L}F_{n}^{\epsilon}\) of (10.2.11) satisfies

(c) \[\tilde{L}F_{n}^{\epsilon}\equiv\frac{1}{\check{\sigma}_{n}^{2}} \sum_{k=1}^{n}\int_{[|x-\check{\mu}_{nk}|\geq\epsilon\check{\sigma}_{n}]\cap[|x |\leq c]}[x-\check{\mu}_{nk}]^{2}dF_{nk}(x)\] (d) \[\leq[\frac{c+\epsilon a/8}{a/2}]^{2}\sum_{k=1}^{n}\int_{[|x-\check {\mu}_{nk}|\geq\epsilon\check{\sigma}_{n}]\cap[|x|\leq c]}dF_{nk}(x)\qquad \mbox{for }n\geq n_{\epsilon},\mbox{ by (b)}\] (e) \[\leq[\frac{c+\epsilon a/8}{a/2}]^{2}\sum_{k=1}^{n}P(|X_{nk}|\geq \epsilon a/4)\qquad\mbox{ by (b) again}\] (f) \[\leq[\frac{c+\epsilon a/8}{a/2}]^{2}P(M_{n}\geq\epsilon a/4)\to 0,\qquad\quad\mbox{ since }\quad M_{n}\to_{p}0\mbox{ by (\ref{eq:d})}.\]

(The exact same bounds for \(\tilde{L}F_{n}^{\epsilon}\) in the Winsorized case hold in (c)-(f), showing that \(\tilde{L}F_{n}^{\epsilon}\to 0\).) Thus (4) (for either \(\check{X}_{nk}\)'s or \(\tilde{X}_{nk}^{\prime}\)s) implies (3).

Suppose that (3) holds. Then

\[\sum_{1}^{n}X_{nk}^{s}\to_{d}N(0,2a^{2})\qquad\mbox{ for the symmetrized un rvs }X_{nk}^{s}\equiv X_{nk}-X_{nk}^{\prime} \tag{7}\]

discussed in section 8.3. Applying the continuity theorem 9.5.1 for chf's then yields

(g) \[\prod_{1}^{n}\{1-[1-\phi_{nk}^{s}(t)]\}=\phi_{n}^{s}(t)\to\exp(-a^{2}t^{2}), \qquad\mbox{ uniformly on any }|t|\leq M,\]

as \(n\to\infty\). Since the \(X_{nk}^{s}\)s are symmetric, their chf's are real valued. Moreover, all \((1-\phi_{nk}^{s}(t))=\mbox{E}(1-\cos tX_{nk}^{s})\geq 0\), and note (10.2.23). Thus lemma 8.1.4 (more powerful than lemma 9.6.3, for positive numbers) shows that (g) holds if and only if

\[\sum_{1}^{n}\mbox{E}(1-\cos tX_{nk}^{s})=\sum_{1}^{n}(1-\phi_{nk}^{s}(t))\to a ^{2}t^{2},\quad\mbox{ uniformly on any }|t|\leq M. \tag{8}\]

Thus for all \(n\geq\) (some \(n_{\epsilon,M}\)) we have (recalling that \(a=b\oplus c\) means that \(|a-b|\leq c\))

(h) \[\frac{1}{3}a^{2}t^{3}=\int_{0}^{t}a^{2}u^{2}du=\int_{0}^{t}\sum_ {1}^{n}\mbox{E}(1-\cos uX_{nk}^{s})du\oplus\epsilon^{5}\] from (8) \[=\int_{0}^{t}\{\sum_{1}^{n}\int_{-\infty}^{\infty}(1-\cos ux)dF_{ nk}^{s}(x)\}du\oplus\epsilon^{5}\] (i) \[=\int_{-\infty}^{\infty}\{\sum_{1}^{n}\int_{0}^{t}(1-\cos ux)du \}dF_{nk}^{s}(x)\oplus\epsilon^{5}\] by Tonelli \[=\sum_{1}^{n}\int_{-\infty}^{\infty}\{u(1-\frac{\sin ux}{ux})|_{0 }^{t}\}dF_{nk}^{s}(x)\oplus\epsilon^{5}\] (9) \[=\sum_{1}^{n}\int_{-\infty}^{\infty}t\left(1-\frac{\sin tx}{tx} \right)dF_{nk}^{s}(x)\oplus\epsilon^{5}\] uniformly on any \[|t|\leq M\]

whenever (8) holds. (Recall the inequality 9.5.1 for "comparison.") Let \(t_{1}=1\) and \(t_{2}=2\). Since \(\frac{1}{2}(8t_{1}^{3}-t_{2}^{3})=0\), subtracting the \(t_{2}\)-values of expressions (h) and (9) from 8 times the \(t_{1}\)-values of expressions (h) and (9) gives (for any \(\epsilon>0\))

(j) \[0=\frac{1}{2}\sum_{1}^{n}\int_{-\infty}^{\infty}[8(1-\frac{\sin x }{x})-2(1-\frac{\sin 2x}{2x})]dF_{nk}^{s}(x)\oplus\epsilon^{5}\] (10) \[=\sum_{1}^{n}\int_{-\infty}^{\infty}\left[3-\frac{4\sin x}{x}+ \frac{\sin 2x}{2x}\right]dF_{nk}^{s}(x)\oplus\epsilon^{5}\equiv\sum_{1}^{n} \int_{-\infty}^{\infty}h(x)dF_{nk}^{s}(x)\oplus\epsilon^{5}.\]

Consider the function \(h(x)\equiv(3-\frac{4\sin x}{x}+\frac{\sin 2x}{2x})\). (On \([0,4]\) the function \(h\) increases from 0 to about 4, it then oscillates ever smaller about 3, and eventually converges to 3 at \(\infty\).) The basic inequality (3.4.18) gives (for all _tiny_\(\epsilon>0\))

(k) \[P(\max_{k}|X_{nk}^{s}|\geq\epsilon)\leq\sum_{1}^{n}P(|X_{nk}^{s}|\geq\epsilon) \leq\frac{1}{h(\epsilon)}(\sum_{1}^{n}\mbox{E}(h(X_{nk}^{s}))\leq\frac{ \epsilon^{5}}{h(\epsilon)},\]since \(\epsilon^{5}/h(\epsilon)\sim 10\epsilon\) as \(\epsilon\downarrow 0\) (simply expand \(\sin x\) and \(\sin 2x\) in the formula for the function \(h(\cdot)\) in (10)). Thus, (8.3.14) and (k) yield

\[M_{n}^{s}\equiv[\max_{1\leq k\leq n}|X_{nk}^{s}|]\to_{p}0.\qquad\mbox{Also,} \qquad M_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}|]\to_{p}0 \tag{11}\]

holds true since \([\max_{1\leq k\leq n}|X_{nk}-\tilde{m}_{nk}|]\to_{p}0\) follows from \(M_{n}^{s}\to_{p}0\) and the symmetrization inequality (8.3.9), and the uan condition then guarantees that \([\max_{1\leq k\leq n}|\tilde{m}_{nk}|]\to 0\). (See Kallenberg (1997) regarding this vital (11) via (9) and (10). Very nice!) So far, we have shown that (3) implies (7) and (11). (Note that the "only if" implication in Corollary 1 is now established as well.)

We now truncate these symmetrized rvs by letting \(X_{nk}^{s}(c)\equiv X_{nk}^{s}\cdot 1_{[|X_{nk}^{s}|\leq c]}\) denote \(X_{nk}^{s}\) truncated at \(\pm c\). Then (for each \(c>0\)) (11) yields

(l) \[|P(\sum_{1}^{n}X_{nk}^{s}(c)\leq x)-P(\sum_{1}^{n}X_{nk}^{s}\leq x)|\leq\sum_{ 1}^{n}P(|X_{nk}^{s}|\geq c)\to 0.\]

Together with (7) and (11) this implies (for each \(c>0\)) that

\[\sum_{1}^{n}X_{nk}^{s}(c)\to_{d}N(0,2a)\quad\mbox{ and }\quad M_{n}^{s}=[\max_{1 \leq k\leq n}|X_{nk}^{s}|]\to_{p}0, \tag{12}\]

(Note that \((\ell)\) also shows that the \(X_{nk}^{s}\)'s can replace the \(X_{nk}^{s}\)'s in (12)). Apply the continuity theorem to (12) to get

(m) \[\begin{array}{ll}a^{2}t^{2}\leftarrow\sum_{1}^{n}\mbox{E}(1-\cos tX_{nk}^{s} (c))=\sum_{1}^{n}\mbox{$\frac{1}{2}$}t^{2}\mbox{E}(X_{nk}^{s}(c))^{2}\oplus \sum_{1}^{n}t^{4}\mbox{E}(X_{nk}^{s}(c))^{4}\\ \hskip 14.226378pt=\mbox{$\frac{1}{2}$}t^{2}\sum_{1}^{n}\mbox{E}(X_{nk}^{s}(c) )^{2}\oplus c^{2}t^{4}\sum_{1}^{n}\mbox{E}(X_{nk}^{s}(c))^{2}\end{array}\]

(n) \[\hskip 14.226378pt=\mbox{$\frac{1}{2}$}t^{2}\sum_{1}^{n}\mbox{E}(X_{nk}^{s}(c ))^{2}(1\oplus 2c^{2}M^{2})\qquad\qquad\mbox{(uniformly on $|t|\leq M$)}.\]

Let \(\epsilon>0\) be given. Define \(c_{\epsilon}=\sqrt{\epsilon}/2M^{2}\). From (m) and (n) (after dividing both sides by \(t^{2}/2\)) the terms \(\tilde{V_{n}^{s}}(c)\equiv\sum_{1}^{n}\mbox{E}(X_{nk}^{s}(c))^{2}\) satisfy

(o) \[\frac{2a^{2}}{1+\epsilon}=2a^{2}/(1+2c_{\epsilon}^{2}M^{2})\leq\tilde{V_{n}^{s }}(c_{\epsilon})\leq 2a^{2}/(1-2c_{\epsilon}^{2}M^{2})=\frac{2a^{2}}{(1- \epsilon)}\]

for all \(n\geq\) (some \(n_{\epsilon,M}^{(1)}\)). From \(M_{n}^{s}\to_{p}0\) we also have for \(n\geq\) (some \(n_{\epsilon}^{(2)}\)) that

(p) \[\begin{array}{l}\tilde{V_{n}^{s}}(1)=\tilde{V_{n}^{s}}(c_{\epsilon})\oplus \sum_{1}^{n}\mbox{E}\{(X_{nk}^{s})^{2}\wedge 1)\cdot 1_{[c_{\epsilon}\leq|X_{nk}^{s}|]} \}\\ \hskip 14.226378pt=\tilde{V_{n}^{s}}(c_{\epsilon})\oplus\sum_{1}^{n}P(|X_{nk}^{ s}|\geq c_{\epsilon})=\tilde{V_{n}^{s}}(c_{\epsilon})\oplus P(M_{n}^{s}\geq c_{ \epsilon})\end{array}\]

(q) \[\hskip 14.226378pt=\tilde{V_{n}^{s}}(c_{\epsilon})\oplus\epsilon.\]

Combining (o) and (q) with (12) gives (with truncation at \(c=1\))

\[\sum_{1}^{n}\tilde{X_{nk}^{s}}(1)\to_{d}N(0,2a^{2}),M_{n}^{s}=[\max_{k}|X_{nk} ^{s}|]\to_{p}0,\;\;\mbox{and}\;\;\tilde{V_{n}^{s}}(1)\to 2a^{2}. \tag{13}\]

That is, the \(X_{nk}^{s}(1)\) satisfy (4). (Since \(\tilde{V_{n}^{s}}(1)\) can replace \(\tilde{V_{n}^{s}}(1)\) on the left hand side of (p) (with no other changes in (p)\(-\)(q)), "tildes" can replace "checks" in (13).)

Note: The symbol \(\tilde{X_{nk}^{s}}(1)\) (symmetrize, and then truncate at \(c=1\)) as well as the symbol \(\tilde{X_{nk}^{s}}(1)\) (truncate at \(c=1\), and then symmetrize) are henceforth replaced by the symbols \(\tilde{X_{nk}^{s}}\) and \(\tilde{X_{nk}^{s}}\). It is a consequence of (11) that

\[\sum_{1}^{n}X_{nk}^{s}=_{a}\sum_{1}^{n}X_{nk}^{s}=_{1}(X_{nk}-X_{nk}^{\prime})= _{a}\sum_{1}^{n}(\tilde{X_{nk}}-\tilde{X_{nk}^{\prime}})=_{1}^{n}\tilde{X_{nk} ^{s}}. \tag{14}\]From (14) and (13), and (11), we claim that

\[\sum_{1}^{n}\check{X}_{nk}^{s}\to_{d}N(0,2a^{2})\ \ \mbox{and}\ \ M_{n}^{s}\to_{p}0.\ \mbox{So}\ \mbox{E}\{\sum_{1}^{n}(\check{X}_{nk}^{s})^{2}\}\to 2a^{2} \tag{15}\]

by the argument (begin at (12)) made in previous paragraph--that led to the third claim made in (13). That is (parallel to (13)), the \(\check{X}_{nk}^{s}\) also satisfy (4). (So do the \(\check{X}_{nk}^{s\prime}\)s.)

Since all \(\mbox{Var}[\check{X}_{nk}^{s}]=2\mbox{Var}[\check{X}_{nk}]\), the third claim in (15) plus (11) imply that

\[\check{\sigma}_{n}^{2}\equiv\sum_{1}^{n}\check{\sigma}_{nk}^{2}\equiv\mbox{Var }[\sum_{1}^{n}\check{X}_{nk}]=\tfrac{1}{2}\ \mbox{Var}\ [\sum_{1}^{n}\check{X}_{nk}^{s}]\to a^{2}\ \ \mbox{and}\ \ M_{n}\to_{p}0, \tag{16}\]

while hypothesis (3) gives

(r) \[(\sum_{1}^{n}\check{X}_{nk}-\sum_{1}^{n}\check{\mu}_{nk})+(\sum_{1}^{n}\check {\mu}_{nk})=\sum_{1}^{n}\check{X}_{nk}=_{a}\sum_{1}^{n}X_{nk}\to_{d}N(b,\ a^{ 2}).\]

Using only \(M_{n}\to_{p}0\) (see (11)) we now have

\[\check{m}_{n}\equiv\max_{1\leq k\leq n}|\check{\mu}_{nk}|\leq\epsilon+\max_{1 \leq k\leq n}1\cdot P(|X_{nk}|\geq\epsilon)\]

(s) \[\leq\epsilon+\sum_{1}^{n}P(|X_{nk}|\geq\epsilon)\leq 2\epsilon\qquad\mbox{ for all }n\geq\ (\mbox{some }n_{\epsilon}),\ \mbox{by (11)},\]

so \(\check{m}_{n}\to 0\). Thus \(M_{n}\to_{p}0\) implies that \(\check{M}_{n}\equiv[\max_{1\leq k\leq n}|\check{X}_{nk}-\check{\mu}_{nk}|]\) satisfies

\[\check{M}_{n}\to_{p}0\quad\mbox{ with }\quad\check{m}_{n}\to 0.\qquad\mbox{Moreover,}\qquad\tilde{M}_{n}\to_{p}0\quad\mbox{ with}\quad\tilde{m}_{n}\to 0 \tag{17}\]

follows from the analogous proof. From hypothesis (3) (and using \(M_{n}\to_{p}0\) in (11) to replace \(X_{nk}\)'s by \(\check{X}_{nk}\)'s for the \(=_{a}\) in (t)) we have

(t) \[(\sum_{1}^{n}\check{X}_{nk}-\sum_{1}^{n}\check{\mu}_{nk})+(\sum_{1}^{n}\check {\mu}_{nk})=_{a}\sum_{1}^{n}X_{nk}\to_{d}N(b,\ a^{2}).\]

Note that the hypotheses of (4) hold for rvs \(\check{X}_{nk}-\check{\mu}_{nk}\) (since they have mean 0, since \(\check{\sigma}_{n}^{2}\to a^{2}\) by (16), and since \(\check{M}_{n}\to_{p}0\) by (17)). That is, these adjusted rvs

\[(\check{X}_{nk}-\check{\mu}_{nk})\ (\mbox{for }1\leq k\leq n)\ \mbox{also satisfy (4)}, \tag{18}\]

and hence (3)--note that

they are "nicer" than the general rvs to which the Lindeberg-Feller theorem 10.2.2 applies in that their most extreme values have been truncated. Thus

(u) \[(\sum_{1}^{n}\check{X}_{nk}-\sum_{1}^{n}\check{\mu}_{nk})=\sum_{1}^{n}(\check {X}_{nk}-\check{\mu}_{nk})\to_{d}N(0,\ a^{2}).\]

Combining (t) and (u) gives

(v) \[\check{\mu}_{n}\equiv\sum_{1}^{n}\check{\mu}_{nk}\to b.\qquad\mbox{Moreover,} \qquad\check{\sigma}_{n}^{2}=\sum_{1}^{n}\check{\sigma}_{nk}^{2}\to a^{2}\]

has already been shown in (16). Thus (3) implies (4) is established with \(c=1\) for the rvs \(X_{nk}\) using (v) and \(M_{n}\to_{p}0\) from (11). The arguments in (q) and (r)\(-\)(v) allow any \(c>0\) to replace \(c=1\). This gives (3) implies (4) for any \(c>0.\) The arguments in (r)-(v) also hold if Winsorized \(X_{nk}\)'s replace Truncated \(X_{nk}\)'s Thus (3) also implies (4) for Winsorized \(X_{nk}\)'s. 

**Proof.**  Consider Corollary 1. In light of the remark below (11), the "only if part" of Corollary 1 has already been established, So suppose that \(M_{n}\to_{p}0.\) Since \(Y_{n}\to_{d}Y\), the symmetrized versions satisfy \(Y_{n}^{s}\to_{d}Y^{s}\equiv Y-Y^{\prime}\). That \(M_{n}\to_{p}0\) (and hence \(M_{n}^{s}\to_{p}0\)) means that the versions truncated at \(c=1\) also satisfy \(\check{Y}_{n}\equiv\sum_{1}^{n}\check{X}_{nk}=_{a}Y_{n}\to_{d}Y\) and \(\check{Y}_{n}^{s}\equiv\check{Y}_{n}-\check{Y}_{n}^{\prime}\to_{d}Y^{s}\). Assume a subsequence \(n^{\prime}\) exsits along which \(\check{\sigma}_{n}^{2}\equiv\mbox{Var}[\sum_\(\frac{1}{2}\mbox{Var}[\sum_{1}^{n}\check{X}_{nk}^{s}]\to\infty.\) Then on this subsequence \(\check{Y}_{n}^{s}/\check{\sigma}_{n}\to_{p}0\). But a trivial application of the Liapunov CLT in (10.2.4) (using \(\gamma_{n}^{s}/(\mbox{sd}_{n}^{s})^{3}\leq 2/2\check{\sigma}_{n}\to 0\)) yields the contradictory result that \(\check{Y}_{n}^{s}/\check{\sigma}_{n}\to_{d}N(0,2)\) on the specified subsequence. Thus \(\limsup_{n}\check{\sigma}_{n}<\infty\) over the whole sequence. So now, pick a subsequence \(n^{\prime}\) on which \(\check{\sigma}_{n}\to\) (some _a_). Just as in (18), on this subsequence \(n^{\prime}\) we can claim that the rvs

\[(\mbox{w})\qquad(\check{X}_{nk}-\check{\mu}_{nk})\quad(\mbox{for }1\leq k\leq n) \mbox{ also satisfy (\ref{eq:1}), and hence (\ref{eq:1}).}\]

So on \(n^{\prime}\) (using \(M_{n}\to_{p}0\) for the \(=_{a}\) step),

\[(\mbox{x})\qquad\sum_{1}^{n}X_{nk}-\check{\mu}_{n}=_{a}\sum_{1}^{n}\check{X}_{ nk}-\check{\mu}_{n}=\sum_{1}^{n}(\check{X}_{nk}-\check{\mu}_{nk})\to_{d}N(0,\ a^{2}).\]

But, \(\sum_{1}^{n}(X_{nk}-\check{\mu}_{nk})+\check{\mu}_{n}=\sum_{1}^{n}X_{nk}\to_{d}Y\) is given. Thus \(\check{\mu}_{n}\) must converge to some finite number--name it \(b\), and so the limiting rv \(Y\) is \(N(b,a^{2})\). \(\Box\)

**Proof.** Consider Corollary 2. Let the rvs \(\check{X}_{nk}\) represent the symmetric uan rvs \(X_{nk}\) truncated at \(c=a\). Define the rvs \(W_{nk}\) via

\[X_{nk}^{2}\equiv\frac{a^{2}}{n}W_{nk}\qquad\mbox{ for }1\leq k\leq n.\]

First, suppose (3) and verify (6). Well, (3) implies (4), giving

\[\begin{array}{ll}(a):M_{n}^{2}=[\max_{1\leq k\leq n}X_{nk}^{2}]\to_{p}0,& \check{\mu}_{n}\to b,\,\check{\sigma}_{n}^{2}\to a^{2},\mbox{ and hence}\\ (b):\check{V}_{n}\equiv\sum_{1}^{n}\mbox{E}(\check{X}_{nk}^{2})=\check{\sigma} _{n}^{2}\to a^{2}&\mbox{(since all }\check{\mu}_{nk}=0\mbox{ gives }b=0).\end{array}\]

Rewriting (20) in terms of the \(W_{nk}\)'s gives

\[\begin{array}{ll}(a):\ M_{n}^{W}\equiv[\max_{1\leq k\leq n}\frac{1}{n}W_{ nk}]\to_{p}0,&\\ (b):\check{V}_{n}^{W}\equiv\mbox{E}(\frac{1}{n}\sum_{1}^{n}W_{nk}\cdot 1_{[W_{ nk}/n\leq 1})\to 1&\mbox{(since $c=a$ was used)}.\end{array}\]

In the context of theorem C.1.1 (with \(r=1\) and \(\nu_{n}=1\)), the conclusions of (21) become \(M_{n}^{W}/\nu_{n}=M_{n}^{W}\to_{p}0\) and

\[(\mbox{y})\qquad\check{u}_{n}^{W}/\nu_{n}\equiv U_{n}^{W}(n\,\nu_{n})/\nu_{n}= U_{n}^{W}(n)=\frac{1}{n}\sum_{1}^{n}\mbox{E}(W_{nk}\cdot 1_{[W_{nk}\leq n]})\to 1.\]

That is, condition (C.1.9) holds for the \(W_{nk}\). Hence the equivalent (C.1.3) gives

\[(\mbox{z})\qquad\sum_{1}^{n}X_{nk}^{2}/a^{2}=\bar{W}_{n}=\bar{W}_{n}/\nu_{n} \to_{p}1\mbox{ (for the uan rvs }W_{nk}/(n\,\nu_{n})=X_{nk}^{2}/a^{2}.)\]

Thus (6) holds. Next, suppose (6) = (z) holds and verify (4)--or its equivalent (3). Basically, just read (20)-(z) backward. (Still, can we make useful observations on the case of non-symmetric rvs \(X_{nk}\)? More on this later.) \(\Box\)

**Corollary 3** (i) Condition (3) that \(\sum_{1}^{n}X_{nk}\to_{d}N(b,a^{2})\) for row independent uan rvs \(X_{n1},\ldots,\)\(X_{nn}\) implies (see the proof of theorem 5.1, with \(\check{\mu}_{n}\equiv\sum_{1}^{n}\check{\mu}_{nk}\) and \(\check{\sigma}_{n}^{2}\equiv\sum_{1}^{n}\check{\sigma}_{nk}^{2}\) for Truncation at any fixed ('\(c\)' (say, \(c=1\)) that

\[\begin{array}{ll}\check{\mu}_{n}\to b,\check{\sigma}_{n}^{2}\to a^{2},& \mbox{and}&M_{n}\equiv\{\max_{1\leq k\leq n}|X_{nk}|\}/\check{\sigma}_{n}\to_ {p}0,\\ \check{m}_{n}\equiv\{\max_{k}|\check{\mu}_{nk}|\}/\check{\sigma}_{n}\to 0&\mbox{and}&\check{M}_{n}\equiv\{\max_{k}|\check{X}_{nk}-\check{\mu}_{nk}|\}/ \check{\sigma}_{n}\to_{p}0,\\ Z_{n}\equiv(\sum_{1}^{n}X_{nk}-\check{\mu}_{n})/\check{\sigma}_{n}=_{a}\check{Z}_{n} \equiv(\sum_{1}^{n}\check{X}_{nk}-\check{\mu}_{n})/\check{\sigma}_{n}\to N(0,1). \end{array}\](ii) Conversely (and trivially from (4) implies (3)),

\[\tilde{M}_{n}\to_{p}0\qquad\mbox{for some `$c$'}\qquad\mbox{implies}\qquad\tilde{Z}_{n}=(\sum_{1}^{n}\tilde{X}_{nk}-\check{\mu}_{n})/\check{\sigma}_{n}\to_{d}N(0,1). \tag{23}\]

(iii) The Winsorized constants

\[\tilde{\mu}_{n}\equiv\sum_{1}^{n}\tilde{\mu}_{nk}\quad\mbox{ and }\quad\tilde{\sigma}_{n}^{2}\equiv\sum_{1}^{n}\tilde{\sigma}_{nk}^{2}\quad\mbox{ can replace $\check{\mu}_{n}$ and $\check{\sigma}_{n}^{2}$ in (\ref{eq:Winsorized})-(\ref{eq:Winsorized}).} \tag{24}\]

(iv) The theorem of types then yields both \(\tilde{\sigma}_{n}^{2}/\tilde{\sigma}_{n}^{2}\to 1\) and \((\tilde{\mu}_{n}-\check{\mu}_{n})/\check{\sigma}_{n}\to 0.\) All results hold for any choice of the truncation point "\(c\)" of the given rvs \(X_{nk}\).

##### Statistical Reformulation of the General CLTs

##### Notation 5.1 (Weak negligibility in the CLT context)

Let \(X_{n1},\ldots,X_{nn}\) be independent rvs with dfs \(F_{n1},\ldots,F_{nn}\). Fix \(\theta>0\). Define \(x_{\theta n}\) by requiring \([-x_{\theta n},\;x_{\theta n}]\) to be the shortest closed, symmetric interval to which \(\bar{F}_{n}\equiv\frac{1}{n}\sum_{1}^{n}F_{nk}\) assigns probability at least \(1-\theta/n\). Let \(\bar{P}_{n}(x)\equiv\frac{1}{n}\sum_{1}^{n}P(|X_{nk}|>x)\) denote the average tail probability, and then let \(\overline{K}_{n}\) denote the qf of the df \(1-\bar{P}_{n}(\cdot)\). Note the quantile relationship \(x_{\theta n}=\overline{K}_{n}(1-\theta/n)\). Let \(W_{n}\) denote a rv with the df \(1-\bar{P}_{n}(\cdot)\).

Let \(\tilde{X}_{nk}\) denote \(X_{nk}\tilde{\mbox{Winsorized outside $[-x_{\theta_{o}n},x_{\theta_{o}n}]$ (commonly, use $\theta_{o}\equiv 1$)}}\). Let \(\tilde{\mu}_{nk},\tilde{\sigma}_{nk}^{2}\), and \(\tilde{v}_{nk}\) be the Winsorized mean, variance and second moment of \(\tilde{X}_{nk}\). Let \(\tilde{X}_{n}\equiv\frac{1}{n}\sum_{1}^{n}\tilde{X}_{nk}\). Its average means, variances, and \(2\)_nd_ moments are

\[\begin{array}{l}\tilde{\mu}_{n}\equiv\frac{1}{n}\sum_{1}^{n}\tilde{\mu}_{ nk},\tilde{\sigma}_{n}^{2}\equiv\frac{1}{n}\sum_{1}^{n}\tilde{\sigma}_{nk}^{2}, \mbox{ and }\tilde{v}_{n}=\frac{1}{n}\sum_{1}^{n}\tilde{v}_{nk}=\tilde{\sigma}_{n}^{2}+ \frac{1}{n}\sum_{1}^{n}\tilde{\mu}_{nk}^{2}\\ \mbox{ when Winsorizing outside $[-x_{\theta_{o}n},\;x_{\theta_{o}n}]$},\qquad\mbox{ and then let }\qquad\tilde{v}_{n}=\frac{1}{n}\sum_{1}^{n}\tilde{v}_{nk}\end{array} \tag{25}\]

for Truncated \(2\)_nd_ moments instead, etc. Such \(\check{\mu}_{n}\) and \(\check{\sigma}_{n}\) may replace the \(\tilde{\mu}_{n}\) and \(\tilde{\sigma}_{n}\) in the obvious fashion--see corollary 1 below. Applying discussion 8.3.1 to the rvs \(|X_{nk}|/\sqrt{n}\tilde{\sigma}_{n}\) (whose average df has \((1-\theta/n)\)th quantile \(x_{\theta n}/\sqrt{n}\tilde{\sigma}_{n}\)) shows the following conditions for the weak negligibility of the rvs \(X_{nk}/\sqrt{n}\tilde{\sigma}_{n}\) are equivalent:

\[M_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}|]/\sqrt{n}\,\tilde{\sigma}_{n}\to_{p}0.\] (weak negligibility) (27) \[x_{\theta n}/\sqrt{n}\,\tilde{\sigma}_{n}\to 0\qquad\mbox{ for all }0<\theta\leq\theta_{o}. \tag{28}\] \[\sum_{1}^{n}P(|X_{nk}|>\epsilon\sqrt{n}\,\tilde{\sigma}_{n})\to 0\qquad\mbox{ for all }0<\epsilon\leq 1. \tag{26}\]

(These differ from the \(\tilde{\mu}_{nk},\tilde{\mu}_{n},\tilde{\sigma}_{nk},\tilde{\sigma}_{n}\) in the previous subsection. For example, the truncation point \(x_{\theta n}\) now depends on \(n\) and \(\sqrt{n}\) is part of a renorming.) 

**Theorem 5.2** (Notation for Winsorization): Consider both \(\overline{X}_{n}\equiv\frac{1}{n}\sum_{k=1}^{n}X_{nk}\) and \(\overline{X_{n}^{2}}\equiv\frac{1}{n}\sum_{1}^{n}X_{nk}^{2}\) for row independent rvs \(X_{nk}(1\leq k\leq n)\). Let (note (25))

\[\begin{array}{l}Y_{nk}\equiv X_{nk}/\sqrt{n}\,\tilde{\sigma}_{n}\qquad\qquad \qquad\mbox{and}\quad\;M_{n}\equiv[\max_{1\leq k\leq n}|Y_{nk}|],\\ \tilde{Y}_{nk}\equiv(X_{nk}-\tilde{\mu}_{nk})/\sqrt{n}\,\tilde{\sigma}_{n} \qquad\mbox{ and }\quad\tilde{M}_{n}\equiv[\max_{1\leq k\leq n}|\check{Y}_{nk}|],\\ \tilde{m}_{n}\equiv[\max_{1\leq k\leq n}|\tilde{\mu}_{nk}|]/\sqrt{n}\,\tilde{ \sigma}_{n}\qquad\mbox{ and }\quad\tilde{M}_{n}^{v}\equiv[\max_{1\leq k\leq n}\frac{1}{n}X_{nk}^{2}]/\check{v}_{n}.\end{array}\]

##### (i) (CLT for the Winsorized mean)

First,

\[M_{n}\to_{p}0\qquad\mbox{ implies }\qquad\tilde{\overline{Z}}_{n}\sim\equiv\sqrt{n}(\overline{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}\to_{d}N(0,1), \tag{30}\]

\(\tilde{m}_{n}\to 0\) and \(\tilde{M}_{n}\to_{p}0\). (It need not be that \(\overline{\lim}\,\tilde{\sigma}_{n}<\infty\).) Conversely,

(31) \[\tilde{\overline{Z}}_{n}=\sqrt{n}(\overline{X}_{n}-\tilde{\mu}_{n})/\tilde{ \sigma}_{n}\to_{d}N(0,1)\qquad\mbox{ for }\mbox{}\If \(\tilde{m}_{n}\to 0\) also holds (hence the \(Y_{nk}\) are uan), then \(M_{n}\to_{p}0\) also holds. Especially,

\[\tilde{\overline{Z}}_{n}\equiv\sqrt{n}[\overline{X}_{n}-\tilde{\mu}_{n}]/\tilde {\sigma}_{n}\to_{d}N(0,1)\qquad\mbox{ and }\qquad\tilde{m}_{n}\to 0\qquad\mbox{ iff }\qquad M_{n}\to_{p}0. \tag{32}\]

**(ii) (Moment estimation)** The following claims hold for \(\overline{X_{n}^{2}}\).

\[\tilde{\overline{Z}}_{n}\to_{d}N(0,1)\mbox{ and }\tilde{m}_{n}\to 0 \qquad\mbox{ implies }\overline{X_{n}^{2}}/\tilde{v}_{n}\to_{p}1\mbox{ for uan }X_{nk}^{2}/n\,\tilde{v}_{n}. \tag{33}\]

\[\overline{X_{n}^{2}}/\tilde{v}_{n}\to_{p}1\mbox{ for uan }X_{nk}^{2}/n\,\tilde{v}_{n} \qquad\qquad\qquad\qquad\mbox{ iff }\qquad\tilde{M}_{n}^{v}\to_{p}0. \tag{34}\]

(Other results in theorem C.1.1 (with \(r=2\)) can also be recast in this context.)

**(iii) (Statistically useful)** Let \(S_{n}^{2}\equiv\overline{X_{n}^{2}}-\tilde{X}_{n}^{2}\). Suppose

\[\bar{X_{n}^{2}}/\tilde{v}_{n}\to_{p}1\mbox{ for uan }X_{nk}^{2}/n\,\tilde{v}_{n} \qquad\mbox{ with }\qquad\overline{\lim}\,\tilde{\mu}_{n}^{2}/\tilde{v}_{n}<1. \tag{35}\]

Then all of the following hold.

\[M_{n}\to_{p}0,\qquad\mbox{(which is equivalent to }\qquad\tilde{ \overline{Z}}_{n}\to_{d}N(0,1)\mbox{ and }\tilde{m}_{n}\to_{p}0). \tag{36}\]

\[S_{n}^{2}/\tilde{\sigma}_{n}^{2}\to_{p}1,\qquad\qquad\mbox{ and }\qquad\tilde{ \overline{Z}}_{n}\sim\equiv\sqrt{n}(\bar{X}_{n}-\tilde{\mu}_{n})/S_{n}\to_{d}N( 0,1). \tag{37}\]

**Corollary 1** (CLT for the Truncated Mean)  All of Theorem 5.2 still holds true when \(\tilde{\mu}_{n}\) and \(\tilde{\sigma}_{n}\) replace \(\tilde{\mu}_{n}\) and \(\tilde{\sigma}_{n}\) throughout. Especially,

\[\tilde{\overline{Z}}_{n}\equiv\sqrt{n}[\bar{X}_{n}-\tilde{\mu}_{n}]/\tilde{ \sigma}_{n}\to_{d}N(0,1)\quad\mbox{ and }\quad\tilde{m}_{m}\to 0\qquad\qquad\mbox{ iff }\qquad M_{n}\to_{p}0. \tag{38}\]

Here \(\tilde{M}_{n}\equiv[\max_{1\leq k\leq n}|X_{nk}-\tilde{\mu}_{nk}|]/\sqrt{n}\, \tilde{\sigma}_{n}\) and \(\tilde{m}_{n}\equiv[\max_{1\leq k\leq n}|\tilde{\mu}_{nk}|]/\sqrt{n}\,\tilde{ \sigma}_{n}\). (Recall (25) for \(\tilde{\mu}_{n}\) and \(\tilde{\sigma}_{n}\).) The theorem of types (compare (32) and (38)) yields

\[\tilde{\sigma}_{n}/\tilde{\sigma}_{n}\to 1\mbox{ and }\sqrt{n}(\tilde{\mu}_{n}- \tilde{\mu}_{n})/\tilde{\sigma}_{n}\to 0\qquad\mbox{ whenever }\qquad M_{n}\to_{p}0. \tag{39}\]

(Condition (35) also yields both of the conclusions stated in (38)--as well as

\[S_{n}^{2}/\tilde{\sigma}_{n}^{2}\to_{p}1\qquad\mbox{ and }\qquad\tilde{ \overline{Z}}_{n}\equiv\sqrt{n}(\overline{X}_{n}-\tilde{\mu}_{n})/S_{n}\to_{d} N(0,1). \tag{40}\]

**Proof.** For the proof of theorem 5.2, we will need notation similar to that above, for any \(\theta\). For \(0<\theta\leq\theta_{0}\) let \(\tilde{X}_{nk}^{\theta}\) denote \(X_{nk}\) Winsorized outside \([-x_{\theta n},x_{\theta n}]\); then let \(\tilde{\mu}_{nk}^{\theta}\) and \(\tilde{\sigma}_{nk}^{\theta}\) denote the obvious, and define both \(\tilde{\mu}_{\theta n}\equiv\frac{1}{n}\sum_{1}^{n}\tilde{\mu}_{nk}^{\theta}\) and \(\tilde{\sigma}_{\theta n}^{2}\equiv\frac{1}{n}\sum_{1}^{n}(\tilde{\sigma}_{nk }^{\theta})^{2}\). Let \(\tilde{X}_{\theta n}\equiv\frac{1}{n}\sum_{1}^{n}\tilde{X}_{nk}^{\theta}\), and let \(\tilde{Z}_{\theta n}\equiv\sqrt{n}[\tilde{X}_{\theta n}-\tilde{\mu}_{\theta n }]/\tilde{\sigma}_{\theta n}\). Define third central moments \(\tilde{\gamma}_{nk}^{\theta}\equiv\mbox{E}|\tilde{X}_{nk}^{\theta}-\tilde{ \mu}_{nk}^{\theta}|^{3}\), and let \(\tilde{\gamma}_{\theta n}\equiv\sum_{1}^{n}\tilde{\gamma}_{nk}^{\theta}\). Call the rvs \(\tilde{Y}_{nk}^{\theta}\equiv(\tilde{X}_{nk}^{\theta}-\tilde{\mu}_{nk}^{ \theta})/\sqrt{n}\,\tilde{\sigma}_{\theta n}\) the _associated summands_, and then let \(\tilde{M}_{\theta n}\equiv[\max_{k}|\tilde{Y}_{nk}^{\theta}|]\) denote their maximal summand.

Suppose \(M_{n}\to_{p}0\) (along with its equivalent (27)). Since all \(\tilde{\mu}_{nk}\in[-x_{\theta_{0}n},x_{\theta_{0}n}]\) when Winsorizing, we have \(\tilde{m}_{n}\leq x_{\theta_{0}n}/(\sqrt{n}\,\tilde{\sigma}_{n})\to 0\) by (27). Further,

(a) \[\tilde{M}_{n}=[\max_{1\leq k\leq n}|X_{nk}-\tilde{\mu}_{nk}|]/\sqrt{n}\,\tilde {\sigma}_{n}\leq M_{n}+x_{\theta_{o}n}/(\sqrt{n}\,\tilde{\sigma}_{n})\to_{p}0.\]

(Analogously, when Truncating, \(M_{n}\to_{p}0\) implies both \(\tilde{m}_{n}\to 0\) and \(\tilde{M}_{n}\to_{p}0\).)

Consider (30). So, suppose that \(M_{n}\to_{p}0\). Let a tiny \(0<\epsilon<1\) be given. Fix \(0<\theta<\epsilon<\theta_{0}\). Observe that

(b) \[\delta_{n}\equiv P(\sum_{1}^{n}X_{nk}\neq\sum_{1}^{n}\tilde{X}_{nk}^{\theta}) )\leq\sum_{1}^{n}P(|X_{nk}|>x_{\theta,n})\leq n(\theta/n)=\theta<\epsilon\]for all \(n\geq\) (some \(n_{\theta,\epsilon}\)). Thus for all \(n\geq n_{\theta,\epsilon}\)) we have

\[P(\tilde{\overline{Z}}_{n}\leq z)=P(\tilde{Z}_{\theta n}\times(\tilde{\sigma}_{ \theta n}/\tilde{\sigma}_{n})+\sqrt{n}(\tilde{\mu}_{\theta n}-\tilde{\mu}_{n})/ \tilde{\sigma}_{n}\leq z)\oplus\epsilon. \tag{41}\]

(Recall that \(a=b\oplus c\) means \(|a-b|\leq c.\)) Since \(\|F_{\tilde{Z}_{\theta n}}-\Phi\|\leq 13\tilde{\gamma}_{\theta n}/\sqrt{n}\tilde{ \sigma}_{\theta n}^{3}\) by the Berry-Esseen result (10.2.3), conclusion (30) will follow at once from (41) and the Slutsky theorem 2.4.1 by verifying (42) below. \(\square\)

**Inequality 5.1 (Applying the Liapunov CLT from a Quantile Viewpoint)**

Suppose only that \(M_{n}\to_{p}0\) (hence (27) holds as well). Fix any \(0<\theta\leq\theta_{0}\). Then

\[(\mathrm{i})\frac{\tilde{\gamma}_{\theta n}}{\sqrt{n}\tilde{\sigma}_{\theta n }^{3}}\to 0,\qquad(\mathrm{j})\frac{\tilde{\sigma}_{\theta n}^{2}}{\tilde{ \sigma}_{n}^{2}}\to 1,\qquad(\mathrm{k})\frac{\sqrt{n}|\tilde{\mu}_{\theta n}- \tilde{\mu}_{n}|}{\tilde{\sigma}_{n}}\to 0. \tag{42}\]

Thus \(\tilde{\overline{Z}}_{n}=\sqrt{n}(\bar{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma} _{n}\to_{d}N(0,1)\), as claimed in (30).

Proof.: For convenience, set \(\theta_{0}=1\). Note that \(\tilde{\sigma}_{n}=\tilde{\sigma}_{1n}=\tilde{\sigma}_{\theta_{o},n}\) now holds. Bounding one power of \(|\tilde{X}_{nk}^{\theta}-\tilde{\mu}_{nk}^{\theta}|^{3}\) by \(2\)\(x_{\theta n}\) in the integrand of each \(\tilde{\gamma}_{nk}^{\theta}\) gives

\[\frac{\tilde{\gamma}_{\theta n}}{\sqrt{n}\,\tilde{\sigma}_{\theta n}^{3}}\leq 2 \frac{x_{\theta n}}{\sqrt{n}\,\tilde{\sigma}_{\theta n}}\cdot\frac{\tilde{ \sigma}_{\theta n}^{2}}{\tilde{\sigma}_{\theta n}^{2}}\leq 2\frac{x_{\theta n}}{ \sqrt{n}\,\tilde{\sigma}_{n}}\to 0, \tag{43}\]

using (27). Since the probability outside \([-x_{1n},x_{1n}]\) is at most \(1/n\),

\[\sqrt{n}|\tilde{\mu}_{\theta n}-\tilde{\mu}_{n}|/\tilde{\sigma}_{n}\leq x_{ \theta n}/\sqrt{n}\,\tilde{\sigma}_{n}\to 0 \tag{44}\]

by (27). We need some notation before turning to (42)(j). Let \(\tilde{v}_{\theta n}\) denote the average of the second moments of the \(\tilde{X}_{nk}^{\theta}\), and set \(\tilde{v}_{n}\equiv\tilde{v}_{1n}\). Now, \(\bar{F}_{n}\) assigns at most \(1/n\) probability to the complement of the interval \([-x_{1n},x_{1n}]\), and on \([-x_{\theta n},x_{\theta n}]\) the integrand of \(\tilde{v}_{\theta n}\) never exceeds \(x_{\theta n}^{2}\). And so

\[0\leq[\tilde{\sigma}_{\theta n}^{2}-\tilde{\sigma}_{n}^{2}]/ \tilde{\sigma}_{\theta n}^{2}=\{\tilde{v}_{\theta n}-\tilde{v}_{n}\}/\tilde{ \sigma}_{\theta n}^{2}-\tfrac{1}{n}\sum_{1}^{n}\{(\tilde{\mu}_{nk}^{\theta})^{2 }-\tilde{\mu}_{nk}^{2}\}/\tilde{\sigma}_{\theta n}^{2} \tag{46}\] \[=\{\tilde{v}_{n}\oplus\tfrac{1}{n}x_{\theta n}^{2}-\tilde{v}_{n} \}/\tilde{\sigma}_{\theta n}^{2}-\tfrac{1}{n}\sum_{1}^{n}\{[\tilde{\mu}_{nk} \oplus x_{\theta n}P(|X_{nk}|>x_{1n})]^{2}-\tilde{\mu}_{nk}^{2}\}/\tilde{\sigma }_{\theta n}^{2}\] \[\leq x_{\theta n}^{2}/[n\,\tilde{\sigma}_{\theta n}^{2}]+2\frac{1 }{n}\sum_{1}^{n}\{[|\tilde{\mu}_{nk}|x_{\theta n}P(|X_{nk}|>x_{1n})]/\tilde{ \sigma}_{\theta n}^{2}\}\] \[\qquad+x_{\theta n\,\frac{1}{n}\sum_{1}^{n}P^{2}(|X_{nk}|>x_{1n} )/\tilde{\sigma}_{\theta n}^{2}\qquad(\text{where all }P^{2}(\cdot)\leq P(\cdot))\] \[\leq(1+2+1)\{x_{\theta n}^{2}/n\,\tilde{\sigma}_{\theta n}^{2}\} =4\{x_{\theta n}^{2}/n\,\tilde{\sigma}_{n}^{2}\}\to 0 \tag{45}\]

by (27). Thus (42) holds, giving the asymptotic normality of \(\bar{Z}_{n}\) in (41) and (30).

This proof of (42)(j) (ending in (46)) also verifies that both

\[\{\tilde{v}_{\theta n}-\tilde{v}_{n}\}/\tilde{\sigma}_{n}^{2}\to 0\qquad\text{ and }\qquad\tfrac{1}{n}\sum_{1}^{n}\{(\tilde{\mu}_{nk}^{\theta})^{2}-\tilde{\mu}_{nk}^{2}\}/\tilde{\sigma}_{n}^{2}\to 0.\qed \tag{47}\]

Proof.: We now consider (31) and (32). Suppose

(f) \[\tilde{\overline{Z}}_{n}=\sqrt{n}(\bar{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma} _{n}\to_{d}N(0,1)\qquad\text{ for }\text{uan }\tilde{Y}_{nk}\equiv(X_{nk}-\tilde{\mu}_{n})/\sqrt{n}\tilde{\sigma}.\]

This (f) is equivalent to \(\sum_{1}^{n}\tilde{Y}_{nk}\to_{d}N(0,1)\) for \(\text{uan }\tilde{Y}_{nk}\)'s. So (31) holds, since

(g) \[\tilde{M}_{n}=[\max_{1\leq k\leq n}|\tilde{Y}_{nk}|]=[\max_{1\leq k\leq n}|X_{ nk}-\tilde{\mu}_{nk}|/\sqrt{n}\tilde{\sigma}\to_{p}0\]

by either theorem 5.1 or its corollary 1. Then (30) and (31) give (32).

**Proof.** Consider (33). As \(\widetilde{\underline{\tilde{Z}}}_{n}\to_{d}N(0,1)\) for uan summands \(\tilde{Y}_{nk}\), corollary 1 to theorem 5.1 implies \(\tilde{M}_{n}=[\max_{1\leq k\leq n}|\tilde{Y}_{nk}|]\to 0\). Combine this with \(\tilde{m}_{n}\to 0\) to obtain \(M_{n}\to_{p}0\), and hence \([\max_{1\leq k\leq n}X_{nk}^{2}]/n\,\tilde{\sigma}_{n}^{2}=M_{n}^{2}\to_{p}0\). Equivalently (by (27)), \(\frac{1}{n}x_{\theta n}^{2}/\tilde{\sigma}_{n}^{2}\to_{p}0\) for every \(0<\theta\leq\theta_{o}\). Thus every \(\frac{1}{n}x_{\theta n}^{2}/\tilde{v}_{n}\to 0\). Since

\[0\leftarrow\frac{1}{n}x_{\theta n}^{2}/\tilde{v}_{n}=\{\ \frac{1}{n}x_{ \theta n}^{2}/\tilde{v}_{n}\}/(1\oplus\{\frac{1}{n}x_{1n}^{2}/\tilde{v}_{n}\} ),\qquad\mbox{ for }\qquad\theta\in\{1\}\cup(0,1),\]

every \(\{\ \frac{1}{n}x_{\theta n}^{2}/\tilde{v}_{n}\}\to 0\). This result that every \(\{\ \frac{1}{n}x_{\theta n}^{2}/\tilde{v}_{n}\}\to 0\) is conclusion (C.1.6) of theorem C.1.1 (with \(r=2\) and \(u_{1n}=\tilde{v}_{n}\)). Thus we can claim conclusion (C.1.4) that \(\tilde{X_{n}^{2}}/\tilde{v}_{n}\to_{p}1\) with the \(X_{nk}^{2}/n\tilde{v}_{n}\) being uan. This establishes (33). (All of the results (31), (32), and (33) hold in the context of corollary 1.) Conclusion (34) holds since (C.1.4) and (C.1.8) are equivalent conditions in theorem C.1.1. \(\square\)

**Proof.** Let us prove (36). Suppose first that _only_\(\frac{1}{n}\sum_{1}^{n}X_{nk}^{2}/\tilde{v}_{n}\to_{p}1\) for uan rvs \(X_{nk}^{2}/n\tilde{v}_{n}\), in (35). That is, the conclusion (C.1.4) holds (in theorem C.1.1 (with \(r=2\), and the \(u_{1n}\) of that theorem equal to our present \(\tilde{v}_{n}\))). Thus

\[\mbox{(m)}\qquad\tilde{M}_{n}^{v}\equiv[\max_{1\leq k\leq n}X_{nk}^{2}]/n \tilde{v}_{n}\to_{p}0,\]

by the equivalence of (C.1.4) with (C.1.8) (see (34)). We next wish to replace \(\tilde{v}_{n}\) in (m) by \(\tilde{\sigma}_{n}^{2}\) in order to claim \(M_{n}^{2}\to_{p}0\) under the hypothesis (35). To do the bookkeeping, we claim from the equivalence of (C.1.6) with (m) that

\[\mbox{(n)}\qquad x_{\theta n}^{2}/n\,\tilde{v}_{n}\to 0\qquad\mbox{ for all }0<\theta\leq 1.\]

Using (n), we will show that \(\tilde{\sigma}_{n}^{2}\) may replace \(\tilde{v}_{n}\) in (m) _provided_ that \(\underline{\lim}\,\tilde{\sigma}_{n}^{2}/\tilde{v}_{n}>0.\) (This added stipulation makes complete sense. If all rvs \(X_{nk}\) only take on one fixed value '\(c\)', then the average value of the \(X_{nk}^{2}\)'s does indeed "converge" to '\(c\)2'--but there is no hope of a CLT as the variance is \(0\). Still, we did not use this for (m).)

Analogous to (45), we now write

\[\frac{\tilde{\sigma}_{n}^{2}}{\tilde{v}_{n}}=\frac{\tilde{v}_{n} \oplus\frac{1}{n}x_{1n}^{2}}{\tilde{v}_{n}}-\frac{(\tilde{\mu}_{n}\oplus\frac {1}{n}x_{1n})^{2}}{\tilde{v}_{n}} \tag{48}\] \[\mbox{(o)} =\frac{\tilde{\sigma}_{n}^{2}}{\tilde{v}_{n}}\oplus\frac{1}{n}x_{ 1n}^{2}\oplus 2\frac{x_{1n}^{2}}{\tilde{v}_{n}}\oplus\frac{1}{n}\frac{1}{\tilde{v}_{n}} \mbox{(using }|\tilde{\mu}_{n}|\leq x_{1n})\] (50) \[\mbox{(p)} =\frac{\tilde{\sigma}_{n}^{2}}{\tilde{v}_{n}}+o(1)=1-\frac{\tilde {\mu}_{n}^{2}}{\tilde{v}_{n}}+o(1) \mbox{(using (n), with }\theta=1)\] (49) \[>0 \mbox{using }\,\overline{\lim}\,\tilde{\mu}_{n}^{2}/\tilde{v}_{n}<1 \mbox{ for the first time in this proof.}\]

Thus \(M_{n}\to_{p}0\) (by applying (49) to (m), since \(\overline{\lim}\,\tilde{\mu}_{n}^{2}/\tilde{v}_{n}<1\) was assumed in (35)). Thus all of (36) follows. (Since \(\tilde{\sigma}_{n}^{2}/\tilde{v}_{n}^{2}=1-\tilde{\mu}_{n}^{2}/\tilde{v}_{n}\), we can also replace \(\tilde{v}_{n}\) by \(\tilde{\sigma}_{n}^{2}\) in (m) when \(\overline{\lim}\,\tilde{\mu}_{n}^{2}/\tilde{v}_{n}<1\). Thus (35) implies both conclusions in (38).) \(\square\)

**Proof.** Consider (37), when (35) holds. Conclusions from (36) and (33) include

\[\mbox{(r)}\qquad M_{n}\to_{p}0,\ Z_{n}\to_{d}N(0,1),\ \tilde{m}_{n}\to 0,\mbox{ and }\tilde{X_{n}^{2}}/\tilde{v}_{n}\to_{p}1\mbox{ for uan}X_{nk}^{2}/n\,\tilde{v}_{n}.\]

Let \(\epsilon>0\) be given, and set \(0<\theta<\epsilon/2\). Then for all \(n\geq\) (some \(n_{\epsilon}\)), the Truncation inequality (8.3.25) gives

\[\mbox{(s)}\qquad P(|\bar{X}_{n}-\tilde{\mu}_{n}|\geq\epsilon\tilde{v}_{n}^{1/2}) \leq\frac{1}{\epsilon^{2}n\tilde{v}_{n}}\int_{[[x]\leq x_{\theta n}]}x^{2}d \bar{F}_{n}(x)+\theta\leq\frac{x_{\theta n}^{2}}{\epsilon^{2}n\tilde{v}_{n}}+ \frac{\epsilon}{2}<\epsilon\]for \(n\geq n_{\epsilon}\). By the equivalence of (C.1.4) that \(\overline{X_{n}^{2}}/\check{v}_{n}\to_{p}1\) for uan \(X_{nk}^{2}/n\check{v}_{n}\) and (C.1.5)) that \(\frac{x_{bn}^{2}}{n\check{v}_{n}}\to 0\) for each \(0<\theta\leq 1\)), we thus have

\[(\bar{X}_{n}-\check{\mu}_{n})/\check{v}_{n}^{1/2}\to_{p}0\qquad\text{ whenever}\qquad\overline{X_{n}^{2}}/\check{v}_{n}\to_{p}1\text{ for uan }X_{nk}^{2}/n\check{v}_{n}. \tag{50}\]

Then (50) and the final claim in (r) yield

(t) \[\frac{S_{n}^{2}}{\check{\sigma}_{n}^{2}}=\frac{\check{v}_{n}}{ \check{\sigma}_{n}^{2}}\{(\frac{\overline{X_{n}^{2}}}{\check{v}_{n}}-1)+\frac{ \check{v}_{n}}{\check{v}_{n}}-[(\frac{\bar{X}_{n}-\check{\mu}_{n}}{\check{v}_{ n}^{1/2}})^{2}-2\frac{\check{\mu}_{n}}{\check{v}_{n}^{1/2}}(\frac{\bar{X}_{n}- \check{\mu}_{n}}{\check{v}_{n}^{1/2}})+\frac{\check{\mu}_{n}^{2}}{\check{v}_{n }}]\}\] (u) \[=\frac{\check{v}_{n}}{\check{\sigma}_{n}^{2}}\{(o_{p}(1))+\left( \frac{\check{v}_{n}-\check{\mu}^{2}}{\check{v}_{n}}\right)-(o_{p}(1))^{2}-2 \frac{\check{\mu}_{n}}{\check{v}_{n}^{1/2}}o_{p}(1)\}\] (v) \[=1+\frac{1}{1-\check{\mu}_{n}^{2}/\check{v}_{n}}o_{p}(1)(1+o_{p}( 1)+2(\check{\mu}_{n}^{2}/\check{v}_{n})^{1/2})\] (w) \[\to_{p}1,\qquad\qquad\qquad\qquad\text{ using }\,\overline{\lim} \check{\mu}_{n}^{2}/\check{v}_{n}<1\text{ for the first time in this proof.}\]

Thus (37) (and (40)) follow from (n) and \(\overline{\lim}\check{\mu}_{2}/\check{v}_{n}<1\) via

(x) \[\frac{\check{\sigma}_{n}^{2}}{\check{\sigma}_{n}^{2}}=\frac{(\check{v}_{n} \oplus\frac{1}{n}x_{1n}^{2})-(\check{\mu}_{n}\oplus\frac{1}{n}x_{1n})^{2}}{ \check{v}_{n}(1-\check{\mu}_{n}^{2}/\check{v}_{n})}=1+\frac{o_{p}(1)}{1-\check {\mu}_{n}^{2}/\check{v}_{n}}\to 1.\qed\]

**Remark 10.5.1** (Simplification for iid rvs): Any non-degenerate rv \(X\) satisfies

\[\overline{\lim}\,\check{\mu}_{n}^{2}/\check{v}_{n}\text{ is less than 1 or equals 0}\qquad\text{ as}\qquad\sigma_{X}^{2}\text{ is finite or is infinite;} \tag{51}\]

use either \(\sigma_{X}^{2}>0\) or (6.6.1). When \(\sigma_{X}^{2}=\infty\), theorem 6.6.1 gives both

\[\check{\sigma}_{n}^{2}/\check{v}_{n}\to 1\text{ and }\check{\sigma}_{n}^{2}/\check{v}_{n}\to 1,\qquad\text{ and so }\check{\sigma}_{n}^{2}/\check{\sigma}_{n}^{2}\sim\check{v}_{n}/\check{v}_{n}=1-\check{\mu}_{n}^{2}/\check{v}_{n}\to 1.\qed \tag{52}\]

## 6 Independent Identically Distributed RVs\({}^{*}\)

**Notation 6.1** (Magnitude and Percentage Winsorization): Let \(F\) and \(K\) be the df and qf of a non-degenerate rv \(X.\) (There is no need for the following theorem if \(\mbox{Var}[X]\) is finite. However, it does apply to this finite variance case as well.)

**Magnitude Winsorization:** Let \([-x_{\theta n},x_{\theta n}]\) be the shortest interval symmetric about \(0\) that contains at least the proportion \((1-\theta/n)\) of the \(X\) probability. When \(|X|\) has df \(F_{|X|}(\cdot)\) and qf \(K_{|X|}(\cdot)\) then \(x_{\theta n}\equiv K_{|X|}(1-\theta/n)\). We now define the magnitude modified parameters. The _Winsorized mean_ and _variance_ notation is

\[\tilde{X}_{1n}\cong(\tilde{\mu}_{1n},\tilde{\sigma}_{1n}^{2}),\quad\mbox{ where }\tilde{X}_{1n}\mbox{ denotes }X\mbox{ Winsorized outside }[-x_{1n},x_{1n}]. \tag{1}\]

Define _Truncated second moments_ in terms of dfs on \([0,\infty)\) and qfs on \((0,1]\) via

\[U(x)\equiv\int_{[|y|\leq x]}y^{2}dF(y)\qquad\mbox{ and }\qquad V_{|X|}(t)\equiv\int_{[0,1-t]}K_{|X|}^{2}(s)ds. \tag{2}\]

Then, let \(u_{1n}\equiv U(x_{1n})\) and \(v_{1n}\equiv\check{v}_{1n}\equiv V_{|X|}(1/n)\) and \(\check{\mu}_{1n}=\int_{[|y|\leq x_{1n}]}ydF(y)\). The _Winsorized second moment_ in the present context is \(\tilde{u}_{1n}\), where

\[\tilde{u}_{1n}\equiv u_{1n}+x_{1n}^{2}P(|X|>x_{1n});\qquad\mbox{ this equals }\qquad\check{v}_{1n}\equiv v_{1n}+\tfrac{1}{n}K_{|X|}^{2}(1-1/n). \tag{3}\]

Equality holds in (3) even though \(u_{1n}\geq v_{1n}\) always holds (see theorem C.1.1 (C)). (The _magnitude Truncated second moment_ is denoted by both \(v_{1n}=\check{v}_{1n}\), so as to connect it more easily with theorem C.1.1. Let \(\tilde{\sigma}_{1n}^{2}\equiv v_{1n}-\check{\mu}_{1n}^{2}=\check{v}_{1n}- \check{\mu}_{1n}^{2}\).)

**Percentage Winsorization:** Winsorize an equal proportion "\(a\)" from each tail. Let \(a>0\) be tiny. We agree that \(\mbox{dom}(a,a)\) denotes \([0,1-a)\), \((a,1]\), or \((a,1-a)\) according as \(X\geq 0,X\leq 0\), or otherwise, and that \(\tilde{K}_{a,a}(\cdot)\) denotes \(K\) Winsorized outside dom(a, a). (For example, when \(X\) takes on both positive and negative values, then \(\tilde{K}_{a,a}(\cdot)\) equals \(K_{+}(a),K(t),K(1-a)\) according as \(t\leq a,a<t<1-a,1-a\leq t\). However, there is no need to Winsorize the smallest values of \(X\) if \(X\geq 0\), say.) Now (using the representation (6.3.2)), let

\[\tilde{X}(a)\equiv\tilde{K}_{a,a}(\xi)\cong(\check{\mu}(a),\tilde{\sigma}^{2} (a)),\ \ \mbox{with}\ \ \check{\mu}_{n}\equiv\check{\mu}(1/n)\mbox{ and }\tilde{\sigma}_{n}^{2}\equiv\tilde{\sigma}^{2}(1/n). \tag{4}\]

Thus the _percentage Winsorized second moment, mean, and variance_ (when we Winsorize outside dom(\(1/n,1/n)\) ) can also be written as \(\check{v}_{n}=\mbox{E}\tilde{K}_{1/n,1/n}^{2}(\xi)\), \(\tilde{\mu}_{n}=\mbox{E}\tilde{K}_{1/n,1/n}(\xi)\), and \(\tilde{\sigma}_{n}^{2}\equiv\check{v}_{n}-\check{\mu}_{n}^{2}\). Also, let \(\check{v}_{n}\equiv\int_{1/n}^{1-1/n}K^{2}(s)ds\equiv V(1/n)\), \(\check{\mu}_{n}=\int_{1/n}^{1-1/n}K(s)ds\), and \(\tilde{\sigma}_{n}^{2}\equiv\check{v}_{n}-\check{\mu}_{n}^{2}\) denote the Truncated moments in this case, where

\[V(t)\equiv\int_{\mbox{\scriptsize dom}(t,t)}K^{2}(s)ds\qquad\mbox{ and }\qquad m(t)\equiv\int_{\mbox{\scriptsize dom}(t,t)}|K(s)|ds \tag{5}\]

(for \(0\leq t\leq 1/2\)). Further, define (for \(0\leq t\leq 1\))

\[v(t)\equiv[K^{+}(1-t)]^{2}+[K^{-}_{+}(t)]^{2}\qquad\mbox{ and }\qquad q(t)\equiv K^{+}(1-t)+K^{-}_{+}(t). \tag{6}\]

Note that \(v(t)\leq q^{2}(t)\leq 2v(t)\).

**Normed CLT rvs** Now define the rvs \((X_{n1},\ldots,X_{nn})\equiv(K(\xi_{n1}),\ldots,K(\xi_{nn}))\) for row-independent Uniform(0, 1) rvs \(\xi_{n1},\ldots,\xi_{nn}\). They are row-independent rvs with df \(F\) and qf \(K\) (as shown in (6.3.2)). Our interest is in both

\[\bar{Z}_{n}\equiv\sqrt{n}\{\check{X}_{n}-\check{\mu}_{n}\}/\tilde{\sigma}_{n} \qquad\mbox{ and }\qquad\bar{Z}_{n}\equiv\sqrt{n}\{\check{X}_{n}-\check{\mu}_{1n}\}/\tilde{ \sigma}_{1n}\qquad \tag{7}\]

**Theorem 6.1** (CLT for general iid rvs): Let \(X_{n1},\ldots,X_{nn}\) be row independent iid rvs with variance in \((0,\infty]\). Possible choices for the constant \(\nu_{n}\) herein include

\[u_{1n},\tilde{u}_{1n},\tilde{\sigma}_{1n}^{2},\mbox{ and }v_{1n}\equiv\tilde{v}_{1n},\tilde{\sigma}_{1n}^{2},\mbox{ and }v_{n},\tilde{v}_{n},\tilde{\sigma}_{n}^{2},\mbox{ and }\tilde{v}_{n},\tilde{\sigma}_{n}^{2}. \tag{8}\]

The following conclusions are equivalent.

\[\begin{array}{ll}\mbox{$\bar{Z}$}_{n}\to_{d}N(0,1)]&\mbox{ for either version of $\tilde{\mbox{W}}$sinorizing in (\ref{W})}.\\ \\ (10)\mbox{$S_{n}^{2}/\tilde{\sigma}_{n}^{2}\equiv\ (\mbox{$\overline{X_{n}^{2}}$}-\mbox{$\bar{X}_{n}^{2}$})/\tilde{\sigma}_{n}^{2}\to_{p}1.\\ \\ (11)\mbox{$\mathcal{D}_{n}^{2}\equiv\ [\frac{1}{n}\max_{k}(X_{nk}-\mbox{$\bar{X}_{n}$})^{2}]/S_{n}^{2}\to_{p}0.\\ \\ (12)\mbox{$M_{n}/\mbox{$\overline{X_{n}^{2}}$}\equiv[\max_{1\leq k\leq n}\frac{1}{n}X_{ nk}^{2}]/\mbox{$\overline{X_{n}^{2}}$}\to_{p}0.\\ \\ (13)\mbox{$\overline{X_{n}^{2}}$}/\nu_{n}=[\frac{1}{n}\sum_{1}^{n}X_{nk}^{2}]/ \nu_{n}\to_{p}1&\mbox{ if $\nu_{n}$ is one of $u_{1n},\tilde{u}_{1n},v_{1n}$ or $v_{n},\tilde{v}_{n},\tilde{v}_{n}$}.\\ \\ (14)\mbox{$\overline{X_{n}^{2}}$}/\nu_{n}\to_{p}1&\mbox{ for at least one specific $\nu_{n}>0$ in (\ref{W})}.\\ \\ (15)\mbox{$M_{n}/\nu_{n}\to_{p}0$},&\mbox{ for any $\nu_{n}$ as in (\ref{W})}.\\ \\ (16)\mbox{$nP(X^{2}>cn\nu_{n})\to 0$}&\mbox{ for all $c>0$}&\mbox{ for any $\nu_{n}$\ as in (\ref{W})}.\\ \\ (17)\mbox{$M_{n}/\nu_{n}^{\prime}\to_{p}0$}&\mbox{ for any $\nu_{n}>0$ with $U(n\,\nu_{n})/\nu_{n}\to 1$}.\\ \\ (18)\mbox{$x_{\theta n}^{2}/(n\nu_{n})\to 0$}&\mbox{ for all $0<\theta\leq 1$},&\mbox{ for any $\nu_{n}$ as in (\ref{W})}.\\ \\ (19)\mbox{$R(x)\equiv x^{2}P(|X|>x)/U(x)\to 0$}&\mbox{ or }\quad R(x-)=x^{2}P(|X|\geq x)/U(x-)\to 0.\\ \\ (20)\mbox{$r(t)\equiv tK_{|X|}^{2}(1-t)/V_{|X|}(t)\to 0$}&\mbox{ or }\quad r(t+)=tK_{|X|}^{2}((1-t)+)/V_{|X|}(t)\to 0.\\ \end{array}\]

Moreover, \(\tilde{\mu}_{n}\) or \(\tilde{\mu}_{1n}\) can replace \(\tilde{\mu}_{n}\) or \(\tilde{\mu}_{1n}\) in the definition of \(\bar{Z}_{n}\) in (7). Then, by the theorem of types,

\[\tilde{\sigma}_{n}/\tilde{\sigma}_{1n}\to 1,\quad\sqrt{n}(\tilde{\mu}_{n}-\tilde{\mu}_{n})/ \tilde{\sigma}_{n}\to 0,\qquad\mbox{and}\qquad\sqrt{n}(\tilde{\mu}_{1n}-\tilde{\mu}_{1n})/ \tilde{\sigma}_{1n}\to 0. \tag{21}\]

**Definition 6.1** (Slowly varying functions): (a) Call \(L(\cdot)>0\)_slowly varying at \(0\)_ (written \(L\in\mbox{$\mathcal{R}$}_{0}\) or \(L\in\mbox{$\mathcal{L}$}\)) provided \(L(ct)/L(t)\to 1\) as \(t\to 0\), for each \(c>0\).

(b) The function \(l(\cdot)>0\) on \((0,\infty)\) is called _slowly varying at \(\infty\)_ (written as \(l\in\mbox{$\mathcal{U}$}_{0}\)) if it satisfies \(l(cx)/l(x)\to 1\) as \(x\to\infty\), for each positive number \(c>0\).

(\(*\)) The functions \(L(t)=\log(1/t)\) and \(l(x)=\log(x)\) are the prototypes.

**Proposition 6.1** (Equivalent conditions that give a CLT for iid rvs): The following contains further "best" items from a long list of equivalents. When any one (hence all) hold, we write either \(F\in\mbox{$\mathcal{D}$}(\mbox{Normal})\) or \(K\in\mbox{$\mathcal{D}$}(\mbox{Normal})\) and say that \(F(\mbox{or},K)\) is in the _domain of attraction_ of the normal distribution. (We require that specific \(a_{n}\searrow 0\) have \(\overline{\lim}(a_{n}/a_{n+1})<\infty\)) in (28), and \(a_{n}=\theta/n\) is one such.) The following are also equivalent to the conclusions listed in theorem 6.1.

\[\begin{array}{ll}x[M(cx)-M(x)]/U(x)\to 0,\mbox{ all $c>1$};&\mbox{ where $M(x)\equiv\int_{[|y|\leq x]}|y|dF(y)$}.\\ \\ (23)\mbox{$U(\cdot)$}&\mbox{ is slowly varying at infinity},\\ \\ (24)\mbox{$V(\cdot)$}&\mbox{ is slowly varying at zero},\\ \\ (25)\mbox{$V_{|X|}(\cdot)$}&\mbox{ is slowly varying at zero},\\ \\ (26)\mbox{$\tilde{\sigma}^{2}(\cdot)$}&\mbox{ is slowly varying at zero},\\ \\ (27)\mbox{$\tilde{r}^{2}(t)\equiv tq^{2}(t)/\mbox{$\tilde{\sigma}^{2}(t)\to 0$} $}&\mbox{ as $t\to 0$}.\end{array}\]

(See (6) for \(q(\cdot)\)).

\[\tilde{r}(a_{n})=\sqrt{a_{n}}q(a_{n})/\tilde{\sigma}(a_{n})\to 0\] for one specific \[a_{n}\searrow 0,\quad\mbox{as above}. \tag{29}\] \[tK^{2}_{|X|}(1-t)/V_{|X|}(t)\to 0\] where \[V_{|X|}(t)\equiv\int_{[0,1-t)}K^{2}_{|X|}(s)ds.\] (30) \[t[K^{2}_{+}(t)\lor K^{2}(1-t)]/V(t)\to 0\] where \[V(t)\equiv\int_{\mathrm{dom}(t,t)}K^{2}(s)ds.\] (31) \[t[K^{2}_{+}(ct)\lor K^{2}(1-ct)]/\tilde{\sigma}^{2}(t)\to 0\] for all \[0<c\leq 1.\,(\mathrm{Or},K^{2}_{|X|}(ct),\mbox{ etc.})\] (32) \[q(\theta/n)/[\sqrt{n}\tilde{\sigma}(1/n)]\to 0\] for all \[\theta>0.\] (See (6) for \[q(\cdot)\] ). (33) \[[q(\theta/n)-q(1/n)]/[\sqrt{n}\tilde{\sigma}(1/n)]\to 0\] for all \[\theta>0.\] (34) \[\sqrt{n}[m(\theta/n)-m(1/n)]/\tilde{\sigma}(1/n)\to 0\] for all \[\theta>0.\,(\mbox{See (3) for $m(\cdot)$}).\] (35) \[[v(\theta/n)-v(1/n)]/[n\tilde{\sigma}^{2}_{n}(1/n)]\to 0\] for all \[\theta>0.\,(\mbox{See near (6) for $v(\cdot)$}.) \tag{28}\]

(Recall the Gnedenko-Kolmogorov theorem 6.6.1, (6.6.7), and all of that section.) (For symmetric \(X^{s}\equiv X-X^{\prime}\) rvs, see definition 8.3.1.)

**Corollary 1 (Asymptotic normality of the Student-_T_ statistic)** If any one (hence, all) of (9)-(20) and (23)-(35) holds, then

\[T_{n}\equiv\sqrt{n}\{\bar{X}_{n}-\tilde{\mu}_{n}\}/S_{n}=_{a}\sqrt{n}\{\bar{X}_ {n}-\tilde{\mu}_{1n}\}/S_{n}\to_{d}N(0,1). \tag{36}\]

Thus, we have a confidence interval available for any of \(\tilde{\mu}_{n},\tilde{\mu}_{1n}\tilde{\mu}_{n}\), or \(\tilde{\mu}_{1n}\) that is asymptotically valid for any df \(F\in\mathcal{D}\)(Normal).

**Proof.** Consider theorem 6.1 The equivalence of conditions (10)-(20) and (23)-(35) is specifically developed in section C.1-section C.3, where much longer lists of equivalents are given. Theorem 6.1 needs only to tie the CLT into this other combined list. This is accomplished via comparing (C.1.24)(a) (note also (C.1.24)(c)) with (10.5.32) and (10.5.38) in theorem 10.5.2. Specifically, (10) \(\leftrightarrow\) (C.1.60) denotes the correspondence. Likewise, (11) \(\leftrightarrow\) (C.1.59), (12) \(\leftrightarrow\) (C.1.58), (13) \(\leftrightarrow\) ((C.1.51)-(C.1.54), (C.1.32), (C.1.42)), (14) \(\leftrightarrow\) (C.1.55), (15) \(\leftrightarrow\) ((C.1.57) and (C.1.38)), (16) \(\leftrightarrow\) ((C.1.28) and (C.1.32)), (17) \(\leftrightarrow\) (C.1.56), (18) \(\leftrightarrow\) (C.1.43) with \(r=2\), (19) \(\leftrightarrow\) (C.1.25), (20) \(\leftrightarrow\) (C.1.26).

The results in proposition 6.1 correspond to results in section C.2. In section C.2, theorem C.2.1 has parts (A)\(-\)(D) for magnitude Winsorizing as in (1)-(3), while theorem C.2.2 repeats much of this for for percentage Winsorizing as in (4)-(6). We will list A-D and F after an equation number (with the added symbol "F" referring to the use of the theorem C.2.2 version of a result (and not the theorem C.2.1 version). Thus (22) \(\leftrightarrow\) (C.2.15), (23) \(\leftrightarrow\) (C.2.13), (24) \(\leftrightarrow\) (C.2.7)F, (25) \(\leftrightarrow\) (C.2.7), (26) \(\leftrightarrow\) (C.2.4)F, (27) \(\leftrightarrow\) (C.2.12)F, (28) \(\leftrightarrow\) (C.2.12)BF, (29) \(\leftrightarrow\) (C.2.6), (30) \(\leftrightarrow\) (C.2.6)F, (31) \(\leftrightarrow\) (C.2.10)CF, (32) \(\leftrightarrow\) (C.2.10)BCF, (33) \(\leftrightarrow\) (C.2.9)BCF, (34) \(\leftrightarrow\) (C.2.11)BCF, (35) \(\leftrightarrow\) (C.2.7)BCF.

As earlier pointed out, many of these equivalences are found in the literature. Having a different denominator in a condition, or being required to verify it only on some sequence of values, could be useful. (The author finds all of the conditions in theorem 6.1 and proposition 6.1 to be interesting in their own right.)

## 7 A Converse of the Classical CLT\({}^{*}\)

**Theorem 7.1** (Domain of normal attraction of the normal df): Consider iid rvs \(X_{1},X_{2},\ldots\), and set \(Z_{n}=\sum_{1}^{n}X_{k}/\sqrt{n}\). Then:

\[\begin{array}{lclcl}(1)&Z_{n}={\rm O}_{p}(1)&\qquad\mbox{\rm implies}&{\rm E }X_{1}=0&\qquad\mbox{\rm and}&{\rm E}(X_{1}^{2})<\infty.\\ (2)&{\rm E}X_{1}=0\mbox{\rm\ and\ }{\rm Var}[X_{1}]<\infty&\qquad\mbox{\rm imply}&Z_{n} \to_{d}N(0,{\rm Var}[X_{1}]).\end{array}\]

**Proof.** (Gine and Zinn) Now, (2) was established previously. Consider (1). Fix \(t>0\). Let \(Z_{n}^{\epsilon}\equiv\sum_{1}^{n}\epsilon_{k}X_{k}/\sqrt{n}\) for iid Rademacher rvs \(\epsilon_{1},\epsilon_{2},\ldots\) that are independent of the \(X_{k}\)'s. By Gine-Zinn symmetrization of (8.3.10), we have

\[P(Z_{n}^{\epsilon}>2\lambda)\leq 2\sup_{n\geq 1}P(Z_{n}>\lambda);\]

and thus \(P(Z_{n}^{\epsilon}>\lambda)={\rm O}_{p}(1)\) by our hypotheses. Also, Khinchin's inequality in exercise 8.3.3 (regarding the \(X_{k}\)'s as fixed constants, and with \(r=1\)) gives

\[\mbox{\rm E}_{\epsilon}|Z_{n}^{\epsilon}|\geq\frac{1}{3}\overline{X_{n}^{2}}^ {1/2}=cS_{n},\qquad\mbox{\rm now with }S_{n}^{2}\equiv\overline{X_{n}^{2}} \quad\mbox{\rm and}\quad\ c=\frac{1}{3}.\]

Applying Paley-Zygmund's inequality 3.4.9 to \(Z_{n}^{\epsilon}\) (conditioned on fixed values of the \(X_{k}\)'s) for the first inequality and (b) for the second yields

\[\begin{array}{lcl}(\mbox{\rm c)}&P_{\epsilon}(|Z_{n}^{\epsilon}|>t)\geq\left( \frac{({\rm E}_{\epsilon}|Z_{n}^{\epsilon}|-t)^{+}}{({\rm E}_{\epsilon}\{(Z_{n }^{\epsilon})^{2}\})^{1/2}}\right)^{2}\geq\left(\frac{(cS_{n}-t)^{+}}{S_{n}} \right)^{2}\\ (\mbox{\rm d)}&=c^{2}(1-t/cS_{n})^{2}1_{[S_{n}>t/c]}\geq(c^{2}/4)1_{[S_{n}>2t/c] }.\end{array}\]

Taking expectations across the extremes of this inequality with respect to the \(X_{k}\)'s gives the bound

\[\mbox{\rm(e)}\qquad P(|Z_{n}^{\epsilon}|>t)\geq(c^{2}/4)P(S_{n}>2t/c).\]

Thus \(S_{n}={\rm O}_{p}(1)\), by combining (e), (a), and the hypothesis.

Fix \(M>0\). The SLLN gives

\[\begin{array}{lcl}(\mbox{\rm f)}&\frac{1}{n}\sum_{1}^{n}X_{k}^{2}1_{[X_{k}^{ 2}\leq M]}\to_{a.s.}&{\rm E}\{X_{1}^{2}1_{[X_{1}^{2}\leq M]}\}.\end{array}\]

But \(\to_{a.s.}\) implies \(\to_{d}\). Thus, applying (9.1.12) to the open set \((t,\infty)\) gives

\[\begin{array}{lcl}(\mbox{\rm g)}&1_{(0,{\rm E}(X_{1}^{2}1_{[X_{1}^{2}\leq M] }))}(t)\leq\underline{\lim}\,_{n}P(\frac{1}{n}\sum_{1}^{n}X_{k}^{2}1_{[X_{k}^{ 2}\leq M]}>t)\\ (\mbox{\rm h)}&\leq\sup_{n}P(\frac{1}{n}\sum_{1}^{n}X_{k}^{2}1_{[X_{k}^{2}\leq M ]}>t),\mbox{\rm\ for each }t>0.\end{array}\]

It follows that for each \(t>0\) we have

\[\begin{array}{lcl}(\mbox{\rm i)}&\sup_{M>0}1_{(0,{\rm E}(X_{1}^{2}1_{[X_{1} ^{2}\leq M]}))}(t)\leq\sup_{M>0}\sup_{n}P(\frac{1}{n}\sum_{1}^{n}X_{k}^{2}1_{[X_ {k}^{2}\leq M]}>t).\\ (\mbox{\rm j)}&\leq\sup_{n}P(\frac{1}{n}\sum_{1}^{n}X_{k}^{2}>t)\leq\sup_{n}P( S_{n}^{2}>t).\end{array}\]

Since \(S_{n}={\rm O}_{p}(1)\), we have \(S_{n}^{2}={\rm O}_{p}(1)\); and this implies that we can specify a \(t\) value of \(t_{0}\) in (j) so large that the right-hand side of (j) at \(t_{0}\) is less than \(1/2\). But this implies that for this \(t_{0}\) the indicator function in (i) must equal zero uniformly in \(M\). This means that

\[\mbox{\rm(k)}\qquad\sup_{M>0}{\rm E}(X_{1}^{2}1_{[X_{1}^{2}\leq M]})\leq t_{0}.\]But this last supremum equals \({\rm E}(X_{1}^{2})\), and hence we must have \({\rm E}(X_{1}^{2})\leq t_{0}<\infty\).

To complete the proof, we must now show that \({\rm E}(X_{1})=0\). Since \({\rm E}X_{1}^{2}<\infty\), the WLLN gives \(\bar{X}_{n}\to_{p}{\rm E}X_{1}\). But the hypothesis that \(Z_{n}={\rm O}_{p}(1)\) implies that \(\bar{X}_{n}=Z_{n}/\sqrt{n}\to_{p}0\). Combining these gives \({\rm E}X_{1}=0\).

## 8 Bootstrapping \({}^{o}\)

Suppose \(X_{1},X_{2},\ldots\) are an iid sample from \(F\). Denote the empirical df of the sample \({\sf X}_{n}\equiv(X_{1},\ldots,X_{n})^{\prime}\) by \({\mathbb{F}}_{n}(\cdot)\). This empirical df \({\mathbb{F}}_{n}\) has mean \(\bar{X}_{n}\) and variance \(S_{n}^{2}.\) Let \({\sf X}_{n}^{*}=(X_{n1}^{*},\ldots,X_{nn}^{*})\) denote an iid sample from \({\mathbb{F}}_{n}\), called the _bootstrap sample_. Let \(\bar{X}_{n}^{*}\) and \(S_{n}^{*}\) denote the mean and the standard deviation of the bootstrap sample. Since the moments of \({\mathbb{F}}_{n}\) exist, we will work with normed summands. Note that the normed summands of a bootstrap sample _always_ constitute a uan array, since

\[\max_{k}P^{*}(|X_{nk}^{*}-\bar{X}_{n}|/\sqrt{n}S_{n}\geq\epsilon)\leq\mbox{Var} [X_{1n}^{*}]/(\epsilon^{2}nS_{n}^{2})=1/n\epsilon^{2}\to 0, \tag{1}\]

all \(\epsilon>0\). The maximum normed summand (when forming the bootstrap mean) is

\[M_{n}^{*}\equiv[\max_{k}|X_{nk}^{*}-\bar{X}_{n}|]/\sqrt{n}S_{n}]. \tag{2}\]

Now (by random sampling \(X_{i}-\bar{X}_{n}\) values), we can view \(M_{n}^{*}\) as the value of either of \(M_{n}=\bar{M}_{n}\) of (10.5.30) and (10.5.31), and note that (for each \(0<\theta\leq\theta_{0}=1/2\)) the quantity \(x_{\theta n}/\sqrt{n}\tilde{\sigma}_{n}\) in (10.6.18) now has a value of \({\cal D}_{n}\), where

\[{\cal D}_{n}\equiv{\cal D}({\sf X}_{n})\equiv[\max_{k}|X_{k}-\bar{X}_{n}|]/ \sqrt{n}S_{n}. \tag{3}\]

Now, \({\cal D}_{n}\) is formed from the original sample, while \(M_{n}^{*}\) is formed from the bootstrap sample. The following theorem is in the spirit of result (10.6.9). (Note, moreover, that \(0\leq M_{n}^{*}\leq{\cal D}_{n}\), while \(P(M_{n}^{*}={\cal D}_{n})\geq 1-(1-1/n)^{n}\to 1-1/e>0\) also holds.) The "standardized" rv of (10.6.9) is now equal to

\[\bar{Z}_{n}^{*}\equiv\sqrt{n}[\bar{X}_{n}^{*}-\bar{X}_{n}]/S_{n},\qquad\mbox{ and also define}\qquad T_{n}^{*}\equiv\sqrt{n}[\bar{X}_{n}^{*}-\bar{X}_{n}]/S_{n}^{*}. \tag{4}\]

Agree that the _weak bootstrap_ holds if

\[\begin{array}{l}\bar{Z}_{n}^{*}\to_{d}N(0,1)\\ \mbox{for the joint probability on $\Omega\times\Omega_{n}^{*}$}.\end{array} \tag{5}\]

Agree that the _strong bootstrap_ holds if

\[\begin{array}{l}P(\bar{Z}_{n}^{*}\leq z|X_{1},\ldots,X_{n})\to P(N(0,1)\leq z )\\ \mbox{for a.e. given sequence of values of $X_{1},X_{2},\ldots$}.\end{array} \tag{6}\]

**Theorem 8.1** (Bootstrapping): Consider \(\bar{Z}_{n}^{*}\) in the iid case.

(i) The weak bootstrap for \(\bar{Z}_{n}^{*}\) is equivalent to both

\[{\cal D}_{n}\to_{p}0\qquad\mbox{and/or}\qquad\tilde{\sigma}^{2}(\cdot)\mbox{ is slowly varying at zero} \tag{7}\]

and/or any one (hence, all) of the equivalents in (10.6.9)-(10.6.35).

(ii) The strong bootstrap for \(\bar{Z}_{n}^{*}\) is equivalent to both

\[{\cal D}_{n}\to\mbox{a.s.}\,0\qquad\mbox{and/or}\qquad\mbox{Var}[X_{1}]=\sigma _{F}^{2}\in(0,\infty). \tag{8}\]

**Corollary 1** (i) The weak bootstrap holds for \(T_{n}^{*}\) whenever \({\cal D}_{n}\to_{p}0\).

(ii) The strong bootstrap holds for \(T_{n}^{*}\) whenever \({\cal D}_{n}\to a.s.\)\(0\).

**Proof.** Now (10.6.26) and (10.6.11) show that \({\cal D}_{n}\to_{p}0\) is equivalent to \(\tilde{\sigma}^{2}(\cdot)\) being slowly varying. (Thus, (7) is a true statement.) Additionally, it is known from the SLLN of theorem 8.4.1 (or, from exercise 8.4.20(ii)) that \({\cal D}_{n}\to a.s.\,0\) is equivalent to \({\rm Var}[X_{1}]<\infty\). (Thus, (8) is a true statement.)

We next verify normality. Consider (ii). The Liapunov bound of (10.2.5) is

(a) \[\|F_{\tilde{Z}_{n}^{*}}-\Phi\|\leq 8\gamma_{n}/\sqrt{n}S_{n}^{3}\leq 8[\max_{1 \leq k\leq n}|X_{k}-\bar{X}_{n}|]/S_{n}=8{\cal D}_{n}\to 0\]

for a.e. sample value of \(X_{1},X_{2},\ldots\). Thus (6) holds. We next consider (i). Well, \(P({\cal D}_{n}>\epsilon/8)<\epsilon\) for all \(n\geq\) (some \(n_{\epsilon}\)) means that \(P(\|F_{\tilde{Z}_{n}^{*}}-\Phi\|>\epsilon)<\epsilon\) for all \(n\geq n_{\epsilon}\). That is, (5) holds.

Consider the converse of the normality statements. Suppose \(\bar{Z}_{n}^{*}\to_{d}N(0,1)\) for a fixed array \((x_{1},\ldots,x_{n},\ldots)\). The summands are necessarily uan by (1). Thus (10.6.18) (for any \(\theta<1\)) is equivalent to \({\cal D}_{n}\to 0\) for this same fixed array (as already noted just above (3)). Thus \({\cal D}_{n}\to a.s.\,0\) is implied by the strong bootstrap, and \({\cal D}_{n}\to_{p}0\) is implied (by going to subsequences) by the weak bootstrap.

Consider the corollary next. Use both (10.6.13) with \(\tilde{v}_{n}^{2}\), and (6.6.4) to conclude that both (go to subsequences for the second)

\[S_{n}^{*}/S_{n}\to_{p^{*}}1\mbox{ for a.e. value of }(X_{1},X_{2},\ldots) \mbox{ if }{\cal D}_{n}\to a.s.\,0\qquad\mbox{ and } \tag{10}\] \[S_{n}^{*}/S_{n}\to_{p\times p^{*}}1\mbox{ if }{\cal D}_{n}\to_{p}0. \tag{9}\]

These last two results are useful in their own right. \(\Box\)

**Exercise 8.1** Establish the details.

## 9 Bootstrapping with Slowly \(\nearrow\) Winsorization\({}^{*}\)

Let \(k_{n}\) and \(k^{\prime}_{n}\) denote _Winsorization numbers_, with \(k_{n}\wedge k^{\prime}_{n}\to\infty\). But suppose the _Winsorization fractions_\((a_{n}\lor a^{\prime}_{n})\equiv(k_{n}\lor k^{\prime}_{n})/n\to 0\). The CLT _necessarily applies_ to any such Winsorized mean from any iid sample! Unfortunately the rvs \(\tilde{X}_{n}\) and \(S^{2}_{n}\) below are _not_ statistics-that is, they are _not computable_ from the data. However, the bootstrap version of this theorem is useful.

**Notation 9.1** Let \(K_{n}\equiv F_{n}^{-1}\) denote the qf associated with some unknown but fixed nondegenerate df \(F_{n}\). We can always specify that dom \((a_{n},\ a^{\prime}_{n})\equiv(a_{n},1-a^{\prime}_{n})\) for any \(n(a_{n}\wedge a^{\prime}_{n})\to\infty\). However, if \(F_{n-}(0)=0\), then specifying that dom \((0,\ a^{\prime}_{n})\equiv[0,1-a^{\prime}_{n})\) is preferable; and if \(F_{n}(0)=1\), then specifying that dom \((a_{n},\ 0)\equiv(a_{n},1]\) is preferable. So we agree to let \(a_{n}\wedge a^{\prime}_{n}\) denote \(a_{n}\wedge a^{\prime}_{n}\), or \(a^{\prime}_{n}\), or \(a_{n}\) according to the scheme used. Let \(\tilde{K}_{n}(\cdot)\) denote \(K_{n}\tilde{\mbox{Winsorized outside of dom $(a_{n},a^{\prime}_{n})$}}\), and let \(\tilde{\mu}_{n}\) and \(\tilde{\sigma}^{2}_{n}\) denote the mean and variance of \(\tilde{K}_{n}(\xi)\). Then the rvs \(\tilde{X}_{nk}\equiv\tilde{K}_{n}(\xi_{nk})\) are row-independent with qf \(\tilde{K}_{n}\) provided the \(\xi_{nk}\) are row-independent Uniform(0, 1) rvs (as in (6.3.5)). Let \(\tilde{X}_{n}\equiv(\tilde{X}_{n1}+\cdots+\tilde{X}_{nn})/n\) and \(\tilde{S}^{2}_{n}\equiv\sum_{1}^{n}(\tilde{X}_{nk}-\tilde{X}_{n})^{2}/n\) denote the sample mean and variance. The quantities of primary interest here are

\[\tilde{Z}_{n}\equiv\sqrt{n}[\tilde{X}_{n}-\tilde{\mu}_{n}]/\tilde{\sigma}_{n} \cong(0,1)\qquad\mbox{ and }\qquad\hat{Z}_{n}\equiv\sqrt{n}[\tilde{X}_{n}- \tilde{\mu}_{n}]/\tilde{S}_{n}. \tag{1}\]

Define \(\tilde{\gamma}_{n}\equiv\mbox{E}|\tilde{X}_{n1}-\tilde{\mu}_{n}|^{3}\). Suppose these \(\tilde{X}_{nk}\equiv\tilde{K}_{n}(\xi_{nk})\), with either known or unknown df \(F_{n}\), denote our model for our data. Also, let \({\cal F}_{n}\) denote the collection of all dfs \(F_{n}\) having \(\tilde{\sigma}_{n}>0\). \(\Box\)

**Theorem 9.1** (Universal CLT for Winsorized rvs) Suppose that the Winsorization fractions satisfy \(n(a_{n}\wedge a^{\prime}_{n})=(k_{n}\wedge k^{\prime}_{n})\to\infty\). Then uniformly in \({\cal F}_{n}\):

\[\|F_{\tilde{Z}_{n}}-\Phi\|\leq 8\tilde{\gamma}_{n}/\sqrt{n}\tilde{ \sigma}^{3}_{n}\leq 8/\sqrt{n(a_{n}\wedge a^{\prime}_{n})}=8/\sqrt{k_{n}\wedge k ^{\prime}_{n}}\to 0. \tag{2}\] \[P(|\tilde{S}_{n}/\tilde{\sigma}_{n}-1|\geq\epsilon)\to 0.\] (3) \[\|F_{\tilde{Z}_{n}}-\Phi\|\to 0. \tag{4}\]

Requiring further that \((a_{n}\lor a^{\prime}_{n})\to 0\) guarantees that every fixed nondegenerate df \(F\) is eventually in all further collections \({\cal F}_{n}\).

**Example 9.1** If \(F_{n}\) is Bernoulli (\(10^{-10}\)), then \(n\) must be huge before \(\tilde{\sigma}_{n}(a_{n})>0\). \(\Box\)

**Proof.** That \(\|F_{\tilde{Z}_{n}}-\Phi\|\leq 8\tilde{\gamma}_{n}/\sqrt{n}\tilde{\sigma}^{3}_{n}\) is immediate from the Berry-Esseen theorem for iid rvs in (10.2.5). Maximizing one power \(|\tilde{K}_{n}(t)-\tilde{\mu}_{n}|^{1}\) in the integrand of \(\tilde{\gamma}_{n}\) (but leaving \(|\tilde{K}_{n}(t)-\tilde{\mu}_{n}|^{2}\) to integrate) gives

\[\tilde{\gamma}_{n}/\sqrt{n}\tilde{\sigma}^{3}_{n}\leq[|K^{+}(1-a ^{\prime}_{n})-\tilde{\mu}_{n}|\vee|K^{-}_{+}(a_{n})-\tilde{\mu}_{n}|]/\sqrt{n }\tilde{\sigma}_{n} \tag{5}\] \[\qquad\leq 1/\sqrt{n(a_{n}\wedge a^{\prime}_{n})}=1/\sqrt{k_{n} \wedge k^{\prime}_{n}}\to 0,\]

as claimed. Thus (2) holds. Let \(q_{n}\equiv K(1-a^{\prime}_{n})-K_{+}(a_{n})\), as usual. Moreover,

\[\tilde{S}^{2}_{n}/\tilde{\sigma}^{2}_{n}=\{[\sum_{1}^{n}(\tilde{X}_{nk}-\tilde {\mu}_{n})^{2}/n]/\tilde{\sigma}^{2}_{n}\}-\{(\tilde{X}_{n}-\tilde{\mu}_{n})/ \tilde{\sigma}_{n}\}^{2}\equiv\{I_{2n}\}-\{I_{1n}\}^{2},\]where Chebyshev's inequality gives both \(P(|I_{1n}|\geq\epsilon)\leq 1/(\epsilon^{2}n)\to 0\) and

(b) \[P(|I_{2n}-1|\geq\epsilon)\leq\frac{1}{n^{2}\epsilon^{2}}\sum_{1}^{ n}\ \operatorname{Var}\,[\left(\frac{\tilde{X}_{nk}-\tilde{\mu}_{n}}{ \tilde{\sigma}_{n}}\right)^{2}-1]\leq\frac{\sum_{1}^{n}\operatorname{E}[( \tilde{X}_{nk}-\tilde{\mu}_{n})^{4}]}{\epsilon^{2}n^{2}\tilde{\sigma}_{n}^{4}}\] (c) \[\leq\frac{1}{\epsilon^{2}n(a_{n}\wedge a_{n}^{\prime})}\cdot\frac{( a_{n}\wedge a_{n}^{\prime})q_{n}^{2}}{\tilde{\sigma}_{n}^{2}}\leq\frac{2}{ \epsilon^{2}n(a_{n}\wedge a_{n}^{\prime})}\cdot 1=\frac{2}{\epsilon^{2}(k_{n} \wedge k_{n}^{\prime})}\to 0\]

uniformly in all dfs \(F\in\mathcal{F}_{n}\). Thus (3) holds. Write

(d) \[\hat{Z}_{n}=\sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{S}_{n}=\tilde{Z}_{n }+\tilde{Z}_{n}(\tilde{\sigma}_{n}/\tilde{S}_{n}-1).\]

Note that \(P(A_{ne})\equiv P([|\tilde{Z}_{n}|\leq M_{\epsilon}])\) can be made uniformly small via Chebyshev (as just below (a) ), even though this set depends on \(F\). Also, \(P(|\tilde{\sigma}_{n}/\tilde{S}_{n}-1|\geq\epsilon)\to 0\) uniformly in \(F\in\mathcal{F}_{n}\) also (as exhibited in (c) ). Thus (4) follows. \(\square\)

**Notation 9.2**: Let \(\mathsf{X}_{n}\equiv(X_{n1},\ldots,X_{nn})^{\prime}\) denote an iid sample from the qf \(K_{n}\), and let \(\tilde{X}_{n},S_{n}^{2},G_{n}\), and \(\mathbb{K}_{n}(\cdot)\) denote its sample mean, sample variance, sample third absolute central moment, and sample qf. Let \(\tilde{\mathsf{X}}_{n}\equiv(\tilde{X}_{n1},\ldots,\tilde{X}_{nn})^{\prime}\) denote the \((k_{n},k_{n}^{\prime})\)-Winsorized sample, for integers \(k_{n}\) and \(k_{n}^{\prime}\) having \(k_{n}\wedge k_{n}^{\prime}\) going to \(\infty\) (here \(k_{n}\wedge k_{n}^{\prime}\) will denote either \(k_{n}\wedge k_{n}^{\prime}\), or \(k_{n}^{\prime}\) or \(k_{n}\) as in the scheme of notation 9.1). Let \(a_{n}\equiv k_{n}/n\) and \(a_{n}^{\prime}\equiv k_{n}^{\prime}/n\). Let \(\tilde{X}_{n},\tilde{S}_{n},\tilde{G}_{n}\), and \(\tilde{\mathbb{K}}_{n}\) denote the sample mean, sample variance, sample third central moment, and sample qf of the population \(\tilde{\mathsf{X}}_{n}\). Let \(\widetilde{X}_{n}^{*}\equiv(X_{n1}^{*},\ldots,X_{nn}^{*})^{\prime}\) denote the iid bootstrap sample from \(\tilde{\mathbb{K}}_{n}(\cdot)\) and let \(\mathsf{X}_{n}^{*}\) and \(\widetilde{S}_{n}^{*2}\) be the sample mean and sample variance of the bootstrap sample \(\mathsf{X}_{n}^{*}\). Let \(\mathbb{P}_{n}^{*}\) denote the bootstrap probability distribution. Our rvs of interest are

\[\widetilde{Z}_{n}^{*}\equiv\sqrt{n}[\widetilde{X}_{n}^{*}-\tilde{X}_{n}]/ \tilde{S}_{n}\qquad\text{ and }\qquad\widehat{Z}_{n}^{*}\equiv\sqrt{n}[ \widetilde{X}_{n}^{*}-\tilde{X}_{n}]/\widetilde{S}_{n}^{*}. \tag{6}\]

[We saw in the previous section that the sample mean and sample variance \(\overline{X}_{n}^{*}\,\text{and}\,S_{n}^{*2}\) of an iid bootstrap sample from \(\mathbb{K}_{n}\) are such that \(\tilde{Z}_{n}\equiv\sqrt{n}[\,\overline{X_{n}^{*}}-\tilde{X}_{n}]/S_{n}\) satisfies the strong (or the weak) bootstrap if and only if \(\operatorname{Var}[\mathrm{X}]\in(0,\infty)\) (or \(F\in\mathcal{D}(\text{Normal})\)). But next we see the glories of Winsorizing! Winsorizing _does do_ what Winsorizing was supposed to do-in a Winsorized sample. The bootstrap _always works_, provided that we Winsorize at least a slowly increasing number.] \(\square\)

**Theorem 9.2** (Universal strong bootstrap CLT): Suppose the Winsorization fractions are such that \((k_{n}\wedge k_{n}^{\prime})=n(a_{n}\wedge a_{n}^{\prime})\to\infty\) (as in notation 9.2). Then uniformly in all \(\mathsf{X}_{n}\) for which \(\tilde{S}_{n}>0\), we have that for a.e. \(\mathsf{X}_{n}\), conditional on \(\mathsf{X}_{n}\),

\[\|F_{\widetilde{Z}_{n}^{*}}-\Phi\|\leq 8\,\tilde{G}_{n}/\sqrt{n} \tilde{S}_{n}^{3}\leq 8/\sqrt{n(a_{n}\wedge a_{n}^{\prime})}=8/\sqrt{k_{n} \wedge k_{n}^{\prime}}\to 0, \tag{8}\] \[\mathbb{P}_{n}^{*}(|\tilde{S}_{n}^{*}-\tilde{S}_{n}|/\tilde{S}_ {n}\geq\epsilon|\mathrm{X}_{n})\to 0,\] (9) \[\|F_{\widetilde{Z}_{n}^{*}}-\Phi\|\to 0,\] and (10) \[\underline{\lim}\,\tilde{S}_{n}>0\text{ if we also specify that }(a_{n}\lor a_{n}^{\prime})\to 0,\text{with }F\text{ nondegenerate.} \tag{7}\]

**Proof.** : This is immediate from the previous theorem. \(\square\)

**Remark 9.1**: If we knew how to Winsorize correctly in theorem 9.1, it would be a useful theorem. The point is, we do always know how to Winsorize correctly in the bootstrap of theorem 9.2.

But should we instead do bootstrap sampling from the empirical qf \(\mathbb{K}_{n}\) itself, rather than \(\tilde{\mathbb{K}}_{n}\), and then Winsorize this sample? No! Sampling from \(\tilde{\mathbb{K}}_{n}\) gives us the analog of theorem 9.1, while sampling from \(\mathbb{K}_{n}\) (it can be shown) does not. (Sampling from \(\mathbb{K}_{n}\) could, however, be shown to work for any qf \(K\) in a very large class of distributions.) \(\square\)

**Exercise 9.1**  Let \(\xi_{1},\xi_{2},\ldots\) be iid Uniform(0, 1) rvs. Let \(X_{nk}\equiv\check{F}_{n}^{-1}(\xi_{k})\) with

\[\check{F}_{n}^{-1}(t)\equiv-\sqrt{t\vee(1/n)}^{\,-1}1_{(0,1/2)}(t)+\sqrt{(1-t) \vee(1/n)}^{\,-1}1_{[1/2,1)}(t). \tag{11}\]

Let \(V_{n}\equiv\mbox{Var}[X_{nk}]\), and let \(\bar{X}_{n}\equiv\sum_{k=1}^{n}X_{nk}/n\). Compute \(V_{n}\), as well as the higher moments E\(|X_{nk}|^{3}\) and E\(X_{nk}^{4}\).

(a) Show that \(Z_{n}\equiv\sqrt{n}\bar{X}_{n}/\sqrt{V_{n}}\to_{d}N(0,1)\)_by verifying_ the Lindeberg condition.

(b) What conclusion does the Berry-Esseen theorem imply for \(Z_{n}\)?

(c) Show that \(\overline{X_{n}^{2}}\equiv\sum_{k=1}^{n}X_{nk}^{2}/n\) satisfies \(\bar{X}_{n}^{2}/V_{n}\to_{p}1\).

(d) Of course, this immediately implies that \(T_{n}\equiv\sqrt{n}\bar{X}_{n}/(\bar{X}_{n}^{2})^{1/2}\to_{d}N(0,1)\).

(e) Show that \((\mbox{E}|X_{nk}|)^{2}/V_{n}\to 0\).

**Exercise 9.2**  Formulate and solve another example in the spirit of exercise 9.1.

**Exercise 9.3**  Verify that \(S_{n}^{2}\equiv\sum_{k=1}^{n}(X_{nk}-\bar{X}_{n})^{2}/(n-1)\) satisfies \(S_{n}^{2}/\tilde{\sigma}_{n}^{2}\to_{p}1\), and do it _by verifying_ a Lindeberg type condition in the context of theorem 10.5.1.

## Chapter 11 Infinitely Divisible and Stable Distributions \({}^{o}\)

### 1 Infinitely Divisible Distributions \({}^{o}\)

**Definition 1.1** (Triangular arrays, and the uan condition): A _triangular array_ is just a collection of rvs \(X_{n1},\ldots,X_{nn},n\geq 1\), such that the rvs in the \(n\)th row are independent. Call it a _uan array_ if the _uniform asymptotic negligibility_ condition holds, that is

\[\max_{1\leq k\leq n}P(|X_{nk}|\geq\epsilon)\to 0\quad\text{as $n\to\infty$ for all $\epsilon>0$.} \tag{1}\]

The uan condition is a natural one for preventing one term from dominating the whole sum.

**The Problem:** Let \(S_{n}\equiv X_{n1}+\cdots+X_{nn}\) denote the _n_th row sum of a uan array.

(i) Find the family of all possible limit laws of \(S_{n}\).

(ii) Find conditions for convergence to a specified law of this form.

Find specialized results for further restrictions on the uan array.

(a) Suppose variances exist.

(b) Suppose the limit law is normal or Poisson.

(c) Consider \(S_{n}=[(X_{1}+\cdots+X_{n})-B_{n}]/A_{n}\) for a singly subscripted sequence of iid rvs \(X_{1},\ldots,X_{n},\ldots\).

Some of the results in this chapter are stated with only indications of the proofs. The goal in this chapter is simply to develop some rough understanding of the subject. We will see in this section that the set of all possible limit laws of row sums \(S_{n}\equiv X_{n1}+\cdots+X_{nn}\) of a uan array of \(X_{nk}\)'s is exactly the class of infinitely divisible laws, which we now define. 

**Definition 1.2** (Infinitely divisible): Call both the rv \(Y\) and its distribution _infinitely divisible_ (\(id\)) if for every value of \(n\) it is possible to decompose \(Y\) into \(n\) iid components as

\[Y\cong Y_{n1}+\cdots+Y_{nn}\quad\text{for some iid rvs $Y_{n1},\cdots,Y_{nn}$.} \tag{2}\]

We denote the class of all id distributions by \(\mathcal{I}\); the subclass with finite variance is denoted by \(\mathcal{I}_{2}\). (We remark that the \(Y_{ni}\)'s of this definition form a uan array, but this needs to be shown; note exercise 1.2.)

[MISSING_PAGE_FAIL:287]

It is natural to hope that \(S_{n}\) will converge in distribution to a rv \(Y\) whose Log \(\,\) chf is of the form

\[\text{Log }\phi_{Y}(t)\equiv\int_{-\infty}^{\infty}\phi(x,t)dK(t), \tag{9}\]

provided that \(K_{n}\to_{d}K.\)

When means and variances need not exist, we define

\[\alpha(x)\equiv x^{2}\wedge 1\text{ and }\beta(x)\equiv(|x|\wedge 1)\text{sign}(x). \tag{10}\]

We then note that (5) can also be manipulated to give

\[\text{Log }\phi_{S_{n}}(t)= \int_{-\infty}^{\infty}\left[\frac{(e^{itx}-1-it\beta(x))}{\alpha (x)}\right]\ \left[\alpha(x)\,d\sum_{k=1}^{n}F_{nk}(x)\right]\] \[+it\left[\int_{-\infty}^{\infty}\beta(x)d\sum_{k=1}^{n}F_{nk}(x) \right]\oplus\epsilon_{n}(x)\]

\[\equiv\int_{-\infty}^{\infty}\phi(x,t)dH_{n}(x)+it\beta_{n}\oplus\epsilon_{n}( x), \tag{11}\]

where this new \(\phi(x,t)\) still satisfies (7), and where we now define

\[H_{n}(x)=\int_{-\infty}^{x}\alpha(u)d{\sum_{k=1}^{n}}F_{nk}(u) \doteq(\text{contribution to variance up to }x), \tag{12}\]

at least for \(x\) near \(0,\) which is where all the action is in any uan array. It is natural to hope that \(S_{n}\) will now converge in distribution to a rv \(Y\) whose chf is of the form

\[\text{Log }\phi_{Y}(t)\equiv it\beta+\int_{-\infty}^{\infty}\phi(x,t)dH(x), \tag{13}\]

provided that \(H_{n}\to_{d}H\) and \(\beta_{n}\to\beta.\)

We are thus particularly interested in the behavior of \(K_{n}\) and/or \(H_{n}\) (and \(\epsilon_{n}(\cdot)\)), both for the general uan \(X_{nk}\)'s of (1) and for the special uan \(Y_{nk}\)'s of (2). 

The next example enables us to show that any chf of the form (9) or (13) is the chf of some id distribution. The details are left to the easy exercise 1.3.

**Example 1.1** (Generalized Poisson and compound Poisson distributions): We suppose that the rvs \(X_{n1},\ldots,X_{nn}\) are iid with

\[X_{nk}=\left\{\begin{array}{ll}a_{j}&\text{with probability }p_{nj},\quad \text{for }1\leq j\leq J,\\ 0&\text{otherwise},\end{array}\right.\]

where \(np_{nj}\to\lambda_{j}\in(0,\infty)\) as \(n\to\infty\). Then

\[\phi_{S_{n}}(t)\to\phi_{Y}(t)\equiv\exp({\sum_{j=1}^{J}}\lambda_{j}(e^{ita_{j} }-1))=\prod_{j=1}^{J}\exp(\lambda_{j}(e^{ita_{j}}-1)). \tag{14}\]

Thus the limiting distribution is that of \(Y\equiv\sum_{j=1}^{J}a_{j}Y_{j}\) for independent Poisson(\(\lambda_{j}\)) rvs \(Y_{j}\). This is called the _generalized Poisson distribution_.

Note also that the chf on the right-hand side of (14) satisfies

\[\phi_{Y}(t)=\exp(\lambda{\sum_{j=1}^{J}}p_{j}\big{(}e^{ita_{j}}-1))=\exp( \lambda(\phi_{W}(t)-1))=\text{E}e^{it(W_{1}+\ldots+W_{N})}, \tag{15}\]

where \(\lambda\equiv\sum_{j=1}^{J}\lambda_{j},p_{j}\equiv P(W=a_{j})=\lambda_{j}/\lambda\) for \(1\leq j\leq J,\,\phi_{W}\) is the chf of \(W,\) and \(W_{1},W_{2},\ldots\) are iid as \(W\) and \(N\cong\text{Poisson}(\lambda)\). The distribution of the rv \(Y\) is called the _compound Poisson distribution_, and is distributed as a Poisson sum of independent Multinomial \((k;\lambda_{1}/\lambda,\ldots,\lambda_{k}/\lambda)\,\text{rvs}\).

The compound Poisson distribution of (15) is obviously id, as is clearly seen by using \(\lambda/n\) in place of \(\lambda\) in (15) for the iid \(Y_{nk}\)'s of (2). Thus the generalized Poisson distribution in (14) is also id. It is in the compound Poisson format that we recognize that this distribution is id, but it will be in its generalized Poisson format that we will put it to work for us. \(\Box\)

**Exercise 1.3** Heuristics have suggested that if \(Y\) is id, then its \(\mbox{\rm chf }\phi\) is of the form (13) (the reader should also note (29), where the normal component of the limit is removed). They have not yet suggested the converse. However,

\[\begin{array}{l}i\beta t+\int_{-\infty}^{\infty}\phi(x,t)\,d\nu/(x)\leftarrow \sum_{j=1}^{m_{n}}\{[e^{itx_{j}}-1-it\beta(x_{j})]/\alpha(x_{j})\}\nu/(I_{j})+ i\beta t\\ =\sum_{j=1}^{m_{n}}(e^{itx_{j}}-1)\lambda_{j}+it\beta_{n}\\ \mbox{with }\lambda_{j}\equiv[\nu/(I_{j})/\alpha(x_{j})]\quad\mbox{ and }\quad \beta_{n}\equiv\beta-\sum_{j=1}^{m_{n}}\nu/(I_{j})\beta(x_{j})/\alpha(x_{j})\\ \mbox{and appropriate intervals }I_{j}\mbox{ (with }x_{j},I_{j}\mbox{ and }\lambda_{j}\mbox{ all depending on }n)\\ \rightarrow(\mbox{a limit of generalized Poisson rvs})\\ =(\mbox{a limit of id rvs})=\mbox{id},\end{array} \tag{16}\]

since the limit under \(\rightarrow_{d}\) of id rvs is also an id rv (as was stated in exercise 1.2(c)). [The present exercise is to make all this rigorous.]

**Theorem 1.1 (Kolmogorov's representation theorem)** Let the rv \(Y\) have chf \(\phi\) and finite variance. We will use the symbol \(K\) to denote a generalized df with \(0=K(-\infty)<K(+\infty)<\infty\). Now, \(Y\) is id if and only if

\[\mbox{Log }\phi(t)=i\beta t+\int_{-\infty}^{\infty}\phi(x,t)dK(x)\quad\mbox{ for some such gdf }K\mbox{ and some real }\beta, \tag{18}\]

with \(\phi(x,t)\) as in (6) (and (7)). The representation is unique. Moreover, it holds that \(\beta=\mbox{E}Y\) and \(K(+\infty)=\mbox{Var}[Y]\).

**Theorem 1.2 (Bounded variances limit theorem for \({\cal I}_{2}\))** (a) We start with a triangular array of row-independent rvs \(X_{nk}\cong(\mu_{nk},\sigma_{nk}^{2})\) having

\[\max_{1\leq k\leq n}\sigma_{nk}^{2}\to 0\quad\mbox{ and }\quad\sigma_{n}^{2}\equiv\sum_{k=1}^{n}\sigma_{nk}^{2}\leq M<\infty \tag{19}\]

(such a triangular array is necessarily uan). Then

\[S_{n}\equiv\sum_{k=1}^{n}X_{nk}\rightarrow_{d}Y, \tag{20}\]

where the limiting rv necessarily satisfies

\[\mbox{Log }\phi_{Y}(t)=i\beta t+\int_{-\infty}^{\infty}\left[\frac{e^{itx}-1- itx}{x^{2}}\right]dK(x)=it\beta+\int_{-\infty}^{\infty}\phi(x,t)dK(t) \tag{21}\]

with \(K(+\infty)\leq\limsup\sigma_{n}^{2}\), if and only if

\[\mu_{n}\equiv\sum_{k=1}^{n}\mu_{nk}\rightarrow\beta\quad\mbox{ and }\quad K_{n}(\cdot)\equiv\sum_{k=1}^{n}\int_{-\infty}^{\cdot}y^{2}dF_{nk}(y+ \mu_{nk})\rightarrow_{sd}K(\cdot). \tag{22}\]

If \(K_{n}(\infty)=\sigma_{n}^{2}\leq M<\infty\) is replaced by \(K_{n}(+\infty)=\sigma_{n}^{2}\rightarrow\mbox{Var}[Y]<\infty\), then we can claim that \(\mbox{Var}[Y]=K(+\infty)=\lim\sigma_{n}^{2}\), and we will write \(K_{n}\rightarrow_{d}K\).

(b) The family of all possible limit laws of such \(S_{n}\) is the family \({\cal I}_{2}\) of all possible infinitely divisible laws that have finite variance.

**Proof.** (If one grants exercises 1.1 and 1.3, then the proof we give will be complete. These exercises are straightforward.) Any chf of the form (18) or (21) is id, by exercise 1.3. Differentiating twice in (21) shows that this chf has mean \(\beta\) and variance \(\int_{-\infty}^{\infty}1dK=K(+\infty)\). For the uniqueness of the representation, just differentiate (21) twice, and thus obtain \(-(\mbox{Log}\,\phi(t))^{\prime\prime}=\int e^{itx}dK(x)\) subject to \(0=K(-\infty)<K(+\infty)=\mbox{Var}[Y]\); then applying the ordinary inversion formula to \(\int e^{itx}dK(x)\) gives \(K\) in terms of \(-(\mbox{Log}\,\phi)^{\prime\prime}\).

It remains to show that any id \(Y\) with mean \(0\) and finite variance has a chf of the form (18) with \(\beta=0\). Reconsider (6). For the special uan \(Y_{nk}\)'s of (2) we have \(\mbox{Var}[Y_{nk}]=\mbox{Var}[Y]/n\), so that exercise 1.1(b) implies that \(\epsilon_{n}(t)\to 0\) uniformly on all finite intervals. Moreover, the family of functions \(\phi(\cdot,t)\) in (7) are bounded and continuous functions that converge to \(0\) as \(|x|\to\infty\). Applying the Helly-Bray exercise 11.1.1 to each \(\phi(\cdot,t)\) in (6) shows (for the first equality we need only recall that \(Y\cong T_{n}\equiv Y_{n1}+\cdots+Y_{nn}\)) that

\[\mbox{Log}\,\,\phi_{Y}(t)=\mbox{Log}\,\,\phi_{T_{n}}(t)\to\int\phi(x,t)dK(x), \quad\mbox{provided that }K_{n}\to_{sd}K.\]

So we must show that \(K_{n}\to_{sd}K\). Now, every subsequence \(n^{\prime}\) has a further \(n^{\prime\prime}\) on which \(K_{n^{\prime\prime}}\to_{sd}\) (some \(K\)). But every resulting such \(\int\phi(x,t)dK(x)\) with a limiting \(K\) inserted must equal \(\mbox{Log}\,\phi_{Y}\), and thus the first paragraph of this proof implies that \(K\) has \(0=K(-\infty)<K(+\infty)=\mbox{Var}[Y]\). In fact, all possible subsequential limits \(K\) must be equal, by the uniqueness of the representation in equation (21). Thus \(K_{n}\to_{sd}K\) on the whole sequence \(n\). Thus \(\mbox{Log}\,\phi_{Y}=\int\phi(x,t)dK(t)\). This completes the proof of theorem 1.1.

We now turn to the proof of theorem 1.2. Under the basic hypothesis (19), we have in (6) that \(\epsilon_{n}(t)\to 0\) uniformly on all finite intervals (by exercise 1.1(b)). Thus whenever \(K_{n}\to_{sd}\) (some \(K\)) and \(\mu_{n}\to\mu\), we have by applying Helly-Bray to each \(\phi(\cdot,t)\) in (7) that \(\mbox{Log}\,\phi_{S_{n}}(t)\to\mbox{Log}\,\phi(t)\) for each \(t\), for the \(\phi\) of (21) with \(\beta=\mu\). Thus \(S_{n}\to_{d}Y\) for the id \(Y\) with chf given by (21).

Suppose \(S_{n}\to_{d}Y\). We argue (as in the proof of theorem 1.1) that each subsequence \(n^{\prime}\) has a further \(n^{\prime\prime}\) on which the \(K_{n}\) of (6) satisfies \(K_{n^{\prime\prime}}\to_{sd}\) (the same \(K\)) and \(\mu_{n^{\prime\prime}}\to(\)the same \(\mu\)). Thus \(K_{n}\to_{sd}K\) and \(\mu_{n}\to\mu\), using theorem 1.1 for this uniqueness. That is, \(\mbox{Log}\,\phi_{S_{n}}(t)\to\mbox{Log}\,\phi(t)\) for the \(\phi\) of (21) having this \(K\) and \(\mu\). But \(\mbox{Log}\,\phi_{S_{n}}(t)\to\mbox{Log}\,\phi_{Y}(t)\) also. Thus \(\mbox{Log}\,\phi_{Y}=\mbox{Log}\,\phi\). Moreover, \(K(+\infty)=\mbox{Var}[Y]\leq\liminf\mbox{Var}[S_{n}]=\liminf\sum_{1}^{n} \sigma_{nk}^{2}\leq M\) (using Fatou and a Skorokhod representation for which \(S_{n}^{*}\to_{a.s.}Y^{*}\)). If

\[\mbox{(b)}\qquad K_{n}(+\infty)=\mbox{Var}[S_{n}]=\sigma_{n}^{2}\to\mbox{Var}[ Y]=K(+\infty),\]

then \(K_{n}\to_{sd}K\) reasonably becomes \(K_{n}\to_{d}K\). \(\Box\)

**Example 1.2** (Normal convergence):

(i) (Representation) The \(N(0,1)\) chf \(\phi\) has

\[\mbox{Log}\,\phi(t)=-t^{2}/2.\]

Thus \(\mu=0\) and \(K=1_{[0,\infty)}\).

(ii) (Lindeberg-Feller theorem) Suppose the triangular array with \(X_{nk}\cong(\mu_{nk},\sigma_{nk}^{2})\) satisfies \(\mu_{nk}=0\) and \(\sigma_{n}^{2}\equiv\sum\sigma_{nk}^{2}=1\) for all \(n\). Then

\[\mbox{(23)}\qquad S_{n}\equiv\sum_{k=1}^{n}X_{nk}\to_{d}N(0,1)\qquad\mbox{ and}\qquad[\mbox{max}_{1\leq k\leq n}\sigma_{nk}^{2}]\to 0\]

if and only if

\[\sum_{k=1}^{n}\int_{[|x|\geq\epsilon]}x^{2}dF_{nk}(x)\to 0\qquad\mbox{for all }\epsilon>0.\]

**Exercise 1.4**  Verify example 1.2.

**Example 1.3** (Poisson convergence)

(i) (Representation) The Poisson(\(\lambda\)) chf \(\phi\) has

\[\mathop{\rm Log}\nolimits\phi(t)=\lambda(e^{it}-1)=it\lambda+\lambda(e^{it}-1-it).\]

Thus \(\mu=\lambda\) and \(K=\lambda\mathbb{1}_{[1,\infty)}\).

(ii) (Convergence) Suppose the triangular array with \(X_{nk}\cong(\mu_{nk},\sigma_{nk}^{2})\) satisfies \([\max_{1\leq k\leq n}\sigma_{nk}^{2}]\to 0\), and \(\sum_{1}^{n}\sigma_{nk}^{2}\to\lambda\in(0,\infty)\). Then

\[S_{n}\equiv\sum_{1}^{n}X_{nk}\to_{d}\mathop{\rm Poisson}\nolimits(\lambda) \tag{25}\]

if and only if

\[\sum_{k=1}^{n}\mu_{nk}\to\lambda\quad\mbox{ and }\quad\sum_{k=1}^{n}\int_{[|x-1| \geq\varepsilon]}x^{2}dF_{nk}(x+\mu_{nk})\to 0. \tag{26}\]

**Exercise 1.5**  Verify example 1.3.

**Exercise 1.6** (Decomposition of normal and Poisson distributions)  Suppose that \(X\cong X_{1}+X_{2}\), where \(X_{1}\) and \(X_{2}\) are independent \(\mathcal{I}_{2}\mathop{\rm rvs}\nolimits\). Then:

\[X\mbox{ normal implies that }X_{1}\mbox{ and }X_{2}\mbox{ are both normal.} \tag{27}\]

\[X\mbox{ Poisson implies that }X_{1}\mbox{ and }X_{2}\mbox{ are both Poisson.} \tag{28}\]

[This is also true if \(\mathcal{I}\) replaces \(\mathcal{I}_{2}\).]

**Exercise 1.7**  If \(\phi\) is a chf, then \(\exp(c(\phi-1))\) is an id chf for all \(c>0\). Thus any chf such as the one in (15) represents the log of an id chf. (We do not make explicit use of this anywhere.)

From here to the end of this section we mainly just state results, mostly by analogy.

**Theorem 1.3** (Levy-Khinchin representation theorem): Let \(Y\) have chf \(\phi.\) Then \(Y\) is infinitely divisible (id) if and only if

\[\mathop{\rm Log}\nolimits\phi(t)=i\beta t+\int_{-\infty}^{\infty}\phi(x,t)dH(x )=i\beta t-\sigma^{2}t^{2}/2+\int_{-\infty}^{\infty}\phi(x,t)\,d\nu(x), \tag{29}\]

where \(\sigma^{2}=\triangle H(0)\) and \(\nu\equiv H-\sigma^{2}1_{[0,\infty)}\),

for \(\phi(x,t)\) as in (11) (and (7)). The representation is unique. (We write \(Y=_{r}(\beta,H)\) to denote this representation. We will think of \(i\beta t-\sigma^{2}t^{2}/2\) as the normal component of the limit law.)

**Theorem 1.4** (General limit theorem for \(\mathcal{I}\))  Let the rv's \(X_{nk}\) form a uan triangular array. Then

\[S_{n}\equiv\sum_{k=1}^{n}X_{nk}=\sum_{k=1}^{n}(X_{nk}-b_{nk})+\sum_{k=1}^{n}b_ {nk}\to_{d}Y, \tag{30}\]

where necessarily

\[\mathop{\rm Log}\nolimits\phi_{Y}(t)=i\beta t+\int_{-\infty}^{\infty}[(e^{itx} -1-it\beta(x))/\alpha(x)]dH(x), \tag{31}\]if and only if for some finite-measure generalized df \(H\) and for some real \(\beta\) we have

\[\beta_{n}\to\beta\qquad\mbox{ and }\qquad H_{n}\to_{d}H \tag{32}\]

where

\[\begin{array}{l}\beta_{n}\equiv\sum_{k=1}^{n}[b_{nk}+\int_{-\infty}^{\infty} \beta(x)dF_{nk}(x+b_{nk})]\quad\mbox{and}\\ H_{n}(\cdot)=\sum_{k=1}^{n}\int_{-\infty}^{\cdot}\alpha(y)dF_{nk}(y+b_{nk}), \end{array} \tag{33}\]

and where \(b_{nk}\equiv{\rm E}X_{nk}1_{(-\delta,\delta)}(X_{nk})\), with \(\delta>0\) arbitrary but fixed. The family of possible limit laws of such \(S_{n}\) is the family \({\cal I}\) of all possible infinitely divisible distributions.

**Theorem 1.5** (Normal limits of uan arrays):

(a) Let \(S_{n}\equiv X_{n1}+\cdots+X_{nn}\) for iid \(X_{nk}\)'s, and suppose that \(S_{n}\to_{d}S\). Then

\[M_{n}\to_{p}0\quad\mbox{ if and only if }\quad S\mbox{ is normal.} \tag{34}\]

(b) Let \(S_{n}\equiv X_{n1}+\cdots+X_{nn}\) for independent \(X_{nk}\)'s, and suppose \(S_{n}\to_{d}S\). Then

\[M_{n}\to_{p}0\qquad\mbox{ if and only if }\qquad S\mbox{ is normal and the $X_{nk}$'s are uan.} \tag{35}\]

(Compare this to (10.5.5).)

**Exercise 1.8**: (a) Show that all Gamma\((r,\nu)\) rvs are infinitely divisible.

(b) Show that an infinitely divisible rv can not be concentrated on a finite interval.

**Exercise 1.9**: Use theorem 1.4 to prove the asymptotic normality condition of theorem 10.5.1.

**Exercise 1.10**: Use theorem 1.4 to prove theorem 1.5.

**Exercise 1.11**: Use chfs to determine necessary and sufficient conditions under which the WLLN will hold.

## 2 Stable Distributions \({}^{o}\)

**Definition 2.1** (Domain of attraction): Let \(X,X_{1},X_{2},\ldots\) be iid \(F\). Suppose an \(A_{n}>0\) and a \(B_{n}\) exist such that \(S_{n}\equiv X_{1}+\cdots+X_{n}\) satisfies

\[(S_{n}-B_{n})/A_{n}\rightarrow_{d}Y\cong G. \tag{1}\]

Then \(F\) is said to belong to the _domain of attraction_ of \(G\), and we write \(F\in{\cal D}(G)\). We also say that \(G\) possesses a domain of attraction. [If \({\rm Var}[X]<\infty\) above, then necessarily \((S_{n}-n{\rm E}X)/\sqrt{n}\rightarrow_{d}N(0,{\rm Var}[X])\) by the ordinary CLT; thus the only new and interesting cases have \({\rm E}X^{2}=\infty\).]

**Definition 2.2** (Stable law): Call a df \(G\)_stable_ if for all \(n\) there exist constants \(a_{n}>0\) and \(b_{n}\) with

\[S_{n}\cong a_{n}X+b_{n},\,{\rm where}\ X,X_{1},\ldots,X_{n}\ {\rm are\ iid\ as}\ G. \tag{2}\]

We call \(G\)_strictly stable_ if we may take all \(b_{n}=0\) in (2).

**Theorem 2.1** (Only stable dfs have domains of attraction): A df \(G\) will possess a domain of attraction if and only if \(G\) has a stable distribution. Moreover, the \(a_{n}\) of (2) must satisfy

\[a_{n}=n^{1/\alpha}\qquad{\rm for\ some}\ 0<\alpha\leq 2. \tag{3}\]

We call \(\alpha\) the _characteristic exponent_ of \(G\). [To compare with section C.4 results for \({\cal R}_{-\beta}\) with \(\beta>0\), we define

\[\beta\equiv(2/\alpha)-1\quad{\rm or}\quad\alpha=2/(\beta+1), \tag{4}\]

where \(0<\alpha\leq 2\) and \(0\leq\beta<\infty\).]

**Definition 2.3** (Basic domain of attraction): The df \(F\) is said to belong to the _basic domain of attraction_ of \(G\) (or the _domain of normal attraction_ of \(G\)), which is denoted by writing \(F\in{\cal D}_{N}(G)\), provided \(A_{n}=({\rm constant}){\times}n^{1/\alpha}\) works in (1).

**Remark 2.1**: (a) Suppose that \(G_{1}\) and \(G_{2}\) are both of the same type (in the sense of definition 9.1.4). Then:

\[\begin{array}{l}{\cal D}(G_{1})={\cal D}(G_{2})\qquad{\rm and}\qquad{\cal D }_{N}(G_{1})={\cal D}_{N}(G_{2}),\\ {G\in{\cal D}(G)\quad{\rm if}\ G\ {\rm is\ stable}.}\end{array} \tag{5}\]

(Thus there is a domain of attraction for the normal type, but this is not so for a particular normal df.) \(\Box\)

**Proof.** : Suppose \(G\) is stable. Then \(S_{n}\cong a_{n}X+b_{n}\), or \((S_{n}-b_{n})/a_{n}\cong X\cong G\) for all \(n\). Thus \(G\in{\cal D}(G)\).

Suppose \(G\) possesses a domain of attraction. Thus there exists \(X_{1},X_{2},\ldots\) iid \(F\) where \(F\in{\cal D}(G)\). Hence for some \(A_{n}>0\) and some \(B_{n}\) we have

(a) \[T_{n}\equiv(X_{1}+\cdots+X_{n}-B_{n})/A_{n}\rightarrow_{d}Y\cong G.\]Replace \(n\) by \(nk\) in (a) and obtain

\[T_{nk}=\frac{X_{1}+\cdots+X_{nk}-B_{nk}}{A_{nk}}\]

(b) \[=\left[\frac{(X_{1}+\cdots+X_{n})-B_{n}}{A_{n}}+\cdots+\frac{(X_{n(k-1)+1}+ \cdots+X_{nk})-B_{n}}{A_{n}}\right]\frac{A_{n}}{A_{nk}}\] \[\quad-(B_{nk}-kB_{n})/A_{nk},\]

which can be rewritten in the more useful format

\[\frac{A_{nk}}{A_{n}}T_{nk}+\left[\frac{B_{nk}-kB_{n}}{A_{n}}\right]\] \[=\left[\frac{X_{1}+\cdots+X_{n}-B_{n}}{A_{n}}\right]+\cdots+\left[ \frac{X_{n(k-1)+1}+\cdots+X_{nk}-B_{n}}{A_{n}}\right]\]

(c) \[\to Y_{1}+\cdots+Y_{k}\qquad\text{for $Y_{1},\ldots,Y_{k}$ iid as $G$}.\]

Let \(k\) be fixed. Recall that the convergence of types theorem states that if

(d) \[\begin{array}{l}T_{nk}\to_{d}Y\text{ (true, from (a))}\qquad\text{ and also}\\ a_{nk}T_{nk}+b_{nk}\to_{d}Y_{1}+\cdots+Y_{k}\quad\text{(true, from (c)),}\end{array}\]

with \(a_{nk}\equiv A_{nk}/A_{n}\) and \(b_{nk}\equiv(B_{nk}-kB_{n})/A_{n}\), then it must be that

(e) \[a_{nk}\to(\text{some $a_{k}$})\in(0,\infty),\qquad b_{nk}\to(\text{some $b_{k}$})\in(-\infty,\infty),\qquad\text{where}\]

(f) \[Y_{1}+\cdots+Y_{k}\cong a_{k}Y+b_{k}.\]

From (f), we see that \(G\) is stable. This completes the proof of the first statement.

Now we further exploit equation (e). From it we determine that

(g) \[a_{mk}=\lim_{n\to\infty}\frac{A_{nmk}}{A_{n}}=\lim_{n\to\infty}\frac{A_{nmk}}{A _{nm}}\frac{A_{nm}}{A_{n}}=a_{k}a_{m}\quad\text{ for all $m,k\geq 1$}.\]

We now let \(Z\equiv Y-Y^{\prime}\) and \(Z_{n}\equiv Y_{n}-Y^{\prime}_{n}\), where \(Y,Y^{\prime},Y_{1},Y^{\prime}_{1},\ldots\) are iid as \(G.\) Then the rvs \(Z\) and \(Z_{n}\) are symmetric, and (1) shows that

(h) \[Z_{1}+\cdots+Z_{n}\cong a_{n}Z\,\text{ for the same $a_{n}$'s as in (\ref{eq:Z_n})}.\]

Thus for some \(x>0\) fixed we have

\[P(Z>(a_{m}/a_{m+n})x)=P(a_{m+n}Z>a_{m}x)\] \[\quad=P(Z_{1}+\cdots+Z_{m+n}>a_{m}x)\qquad\text{by (h)}\] \[\quad=P((Z_{1}+\cdots+Z_{m})+(Z_{m+1}+\cdots+Z_{m+n})>a_{m}x)\] \[\quad=P(a_{m}Z_{1}+a_{n}Z_{2}>a_{m}x)\qquad\text{ by (h)}\] \[\quad\geq P(a_{n}Z_{2}\geq 0\,\text{ and $a_{m}Z_{1}>a_{m}x$})=P(Z_{2}\geq 0)P(Z_{1}>x)\](i) \(\geq\) (some \(\delta\)) \(>0\) for all \(m\) and \(n\) (by choice of \(x\)).

Thus \(a_{m}/a_{m+n}\) stays bounded away from \(\infty\) for all \(m\) and \(n\). Letting \(m=k^{I}\) and \(m+n=(k+1)^{I}\), we have from (h) that

\[(a_{k}/a_{k+1})^{I}=a_{k^{I}}/a_{(k+1)^{I}}=a_{m}/a_{m+n}\]

(j) \(\leq\) (some \(M\)) \(<\infty\) for all \(k\) and \(I\).

Thus \((a_{k}/a_{k+1})\leq 1\) for all \(k\); that is,

(k) \(a_{n}\) is \(\nearrow\), where \(a_{mk}=a_{m}a_{k}\) for all \(m,k\geq 1\),

was shown in (a). Thus exercise 2.1 below shows that \(a_{n}=n^{1/\alpha}\) for some \(\alpha>0.\) Suppose that \(\alpha>2\). Then \(\operatorname{Var}[Y]<\infty\) by exercise 2.2 below. We can thus claim that \(\sqrt{n}(\tilde{Y}-\mu)\to_{d}N(0,1)\) by the ordinary CLT. Thus \(A_{n}=\sqrt{n}\) and \(a_{k}=\sqrt{k}\) work above. By the convergence of types theorem, there are no other choices. That is, when \(\alpha>2\), then only \(\alpha=2\) works. (See Breiman (1968, p. 202).) \(\square\)

**Exercise 2.1** Suppose that \(a_{n}\nearrow\) with \(a=1\), and suppose that \(a_{mk}=a_{m}a_{k}\) for all \(k,m\geq 1\). Then show that necessarily \(a_{n}=n^{1/\alpha}\) for some \(\alpha\geq 0\).

**Exercise 2.2** (Moments) Suppose that \(Y\cong G\) is stable with characteristic exponent \(\alpha\). Then

(6) \(\operatorname{E}\lvert Y\rvert^{r}<\infty\) for all \(0<r<\alpha\).

[Hint. Use the inequalities of section 8.3 to show that \(nP(\lvert X\rvert>a_{n}x)\) is bounded in \(n\), where \(a_{n}\equiv n^{1/\alpha}\). Then bound the appropriate integral.]

**Exercise 2.3** (Strictly stable dfs) Suppose that \(G\) is stable with characteristic exponent \(\alpha\neq 1\). Then there is a number \(b\) such that \(G(\cdot+b)\) is strictly stable. [Hint. Show that \(b\) must satisfy \(b^{\prime}_{n}\equiv b_{n}+(a_{n}-n)b=0\) for all \(n\), and specify \(b\) such that \(b^{\prime}_{2}=0\). Or else \(b^{\prime}_{2}=b=0\) immediately.]

**Example 2.1** (Hitting time as a stable law) Watch Brownian motion \(\mathbb{S}\) until it first attains height \(a\). The time this takes is denoted by \(T_{a}\). According to the strong Markov property,

\[T_{na}\cong T_{a}^{(1)}+\cdots+T_{a}^{(n)}\text{ with }T_{a}^{(1)},\ldots,T_{a }^{(n)}\text{ iid as }T_{a}. \tag{7}\]

Checking the covariance functions, we see that \(\mathbb{S}(a^{2}\cdot)/a\cong\mathbb{S}\) on \([0,\infty)\), and thus \(T_{a}\cong a^{2}T_{1}\). Putting these last two equations together gives

\[T_{a}^{(1)}+\cdots+T_{a}^{(n)}\cong T_{na}\cong n^{2}a^{2}T_{1}\cong n^{2}T_{ a}; \tag{8}\]

thus \(T_{a}\) is strictly stable with \(\alpha=\frac{1}{2}\). From (12.7.3), \(P(T_{a}<t)=2[1-\Phi(a/\sqrt{t})]\) for the \(N(0,1)\) df \(\Phi\). Thus the df \(F_{a}\) and the density \(f_{a}\) of \(T_{a}\) are given by

\[F_{a}(x)=2[1-\Phi(a/\sqrt{x})]\text{ and }f_{a}(x)=\frac{a}{\sqrt{2\pi x^{3/2} }}e^{-a^{2}/(2x)}\text{ for }x>0. \tag{9}\]

## 3 Characterizing Stable Laws \({}^{o}\)

**Theorem 3.1** (General stable chfs): Suppose that \(Y\cong G\) is stable. Then either \(Y\) is a normal rv or there is a number \(0<\alpha<2\) and constants \(m_{1},m_{2}\geq 0\) and \(\beta\) for which

\[\begin{array}{ll}\mathop{\rm Log}\nolimits\phi(t)=i\beta t&+m_{1}\int_{0}^{ \infty}[e^{itx}-1-it\alpha(x)]\frac{1}{x^{1+\alpha}}dx\\ &+m_{2}\int_{-\infty}^{0}[e^{itx}-1-it\alpha(x)]\frac{1}{|x|^{1+\alpha}}dx. \end{array} \tag{1}\]

For \(0<\alpha<1\) or \(1<\alpha<2\) this can be put in the form

\[\mathop{\rm Log}\nolimits\phi(t)=idt-c|t|^{\alpha}\times(1+i\theta\,C_{\alpha} \mathop{\rm sign}\nolimits(t)), \tag{2}\]

where \(d\in(-\infty,\infty),c>0,|\theta|\leq 1\), and \(C_{\alpha}\equiv\tan(\pi\alpha/2)\). For \(\alpha=1\) the form is

\[\mathop{\rm Log}\nolimits\phi(t)=idt-c|t|\times(1+i\theta\mathop{\rm sign} \nolimits(t)\,\log|t|) \tag{3}\]

with \(c,d,\theta\) as above. In fact, \(\theta=(m_{1}-m_{2})/(m_{1}+m_{2})\) measures skewness, while the constants \(d\) and \((1/c)^{(1/\alpha)}\) are just location and scale parameters. Then let \(p\equiv m_{1}/(m_{1}+m_{2})=(1+\theta)/2\).

**Corollary 1** (Symmetric stable chfs): \(\phi(\cdot)\) is the chf of a nondegenerate and symmetric stable distribution with characteristic exponent \(\alpha\) if and only if

\[\phi(t)=\exp(-c|t|^{\alpha})\quad\mbox{for some $0<\alpha\leq 2$ and some $c>0$}. \tag{4}\]

**Proof.** We give only a direct proof of the corollary. This keeps things simple. Let \(Y\cong G\) be strictly stable with \(\mathop{\rm chf}\nolimits\phi\) and let \(\psi\equiv\mathop{\rm Log}\nolimits\phi\). Since \(S_{n}\cong a_{n}Y\), we have \(\phi^{n}(t)=\phi(a_{n}t)\). Thus (modulo \(2\pi i\))

\[n\psi(t)=\psi(a_{n}t)=\psi(a_{m}(a_{n}/a_{m})t)=m\psi((a_{n}/a_{m})t)\] (a) \[=m\psi((n/m)^{1/\alpha}t)\qquad\mbox{ by (\ref{eq:m_1}.3)}.\]

Thus for all rationals \(r>0\) we have shown that

\[r\psi(t)=\psi(r^{1/\alpha}t)\qquad\mbox{ modulo $2\pi i$}, \tag{5}\]

and by continuity, (5) also holds for all real \(r>0\). Set \(t=1\) and \(r=\tau^{\alpha}\) in (5) for

\[\psi(\tau)=c\tau^{\alpha}\qquad\mbox{ for all $\tau>0$, with $c\equiv\psi(1)$}.\] (b) \[\equiv(-c_{1}+ic_{2})\tau^{\alpha}.\]

It must be true that \(c_{1}>0\); if \(c_{1}<0\) were true, then we would have the impossible situation that \(\phi(\tau)\to\infty\) as \(\tau\to\infty\), while \(c_{1}=0\) would imply that \(|\phi(\tau)|=1\) for all \(\tau\), and that \(Y\) is degenerate by proposition 9.8.2. Thus for some \(c_{1}>0\), for \(t>0\) we must have

\[\phi(t)=\exp[(-c_{1}+ic_{2})t^{\alpha}],\qquad\mbox{ with $\phi(-t)=\overline{ \phi(t)}$}.\] (c)

We can summarize the two equations in (c) as

\[\phi(t)=\exp(-c_{1}|t|^{\alpha}\times[1-i(c_{2}/c_{1})\mathop{\rm sign} \nolimits(t)]).\] (d)

Since \(G\) is symmetric, \(\phi\) is real, and so \(c_{2}=0\). Thus (5) holds. All that remains is to be sure that \(\phi\) is a valid chf. This follows from the next two exercises.

If \(\alpha\neq 1\), then exercise 11.2.3 shows that \(\phi(t)\exp(idt)\) is a strictly stable chf for some real \(d\). Thus it satisfies (d), which is almost (2).

**Exercise 3.1**  Suppose that \(X\cong F\) with chf \(\phi\) that satisfies

\[\phi(t)=1-c|t|^{\alpha}+O(t^{2})\qquad\mbox{ as }t\to 0 \tag{6}\]

for some fixed \(\alpha\in(0,2)\). Suppose now that \(X_{1},\ldots,X_{n}\) are iid as \(F\). Show that the properly normed sum satisfies \(S_{n}/n^{1/\alpha}\to_{d}Z\) where \(\phi_{Z}(t)=\exp(-c|t|^{\alpha})\).

**Exercise 3.2**  Suppose \(X\) has density \(f(x)=(\alpha/2)|x|^{-(\alpha+1)}\times 1_{(1,\infty)}(|x|)\) with the constant \(\alpha\in(0,2)\). Show that \(\phi\) satisfies (6).

**Exercise 3.3** (Holtzmark-Chandrasekar) Let \(X_{n1},\ldots,X_{nn}\) be iid Uniform\((-n,n)\). We now let \(0<1/p<2\) and \(M>0\) be fixed. Let

\[Z_{n}\equiv\sum\nolimits_{k=1}^{n}Z_{nk}\equiv\sum\nolimits_{k=1}^{n}M\,{\rm sign }(X_{nk})/|X_{nk}|^{p}.\]

Regard \(Z_{n}\) as the sum of forces exerted on a unit mass at the origin by \(n\) stars of mass \(M\) that are uniformly distributed on \((-n,n)\) in a universe where an inverse \(p\)th power of attraction is operating. Show that \(Z_{n}\to_{d}Z\), where the chf of \(Z\) is given by \(\phi_{Z}(t)=\exp(-c|t|^{\alpha})\) for appropriate \(c\) and \(\alpha\).

**Exercise 3.4**  Show that (1) can be put into the form (2) or (3).

## 4 The Domain of Attraction of a Stable Law \({}^{o}\)

We now merely state some results that assert when convergence in distribution to a general stable law takes place.

**Theorem 4.1** (Stable domain of attraction with \(0<\alpha<2\)): (a) Now, \(F\in{\cal D}(G)\) for some stable \(G\) with characteristic exponent \(\alpha\in(0,2)\) if and only if (as \(x\to\infty\)) both

\[U(x)\equiv\int_{[|y|\leq x]}y^{2}\,dF(y)\in{\cal U}_{2-\alpha}\qquad[\mbox{for $\alpha=2/(\beta+1)$}] \tag{1}\]

(or equivalently)

\[V(t)\equiv\int_{t}^{1-t}K^{2}(s)ds\in{\cal R}_{-\beta}\qquad\quad[\mbox{for $\beta\equiv(2-\alpha)/\alpha$}]) \tag{2}\]

and also

\[P(X>x)/P(|X|>x)\to(\mbox{some $p$})\in[0,1]. \tag{3}\]

Moreover, \(\alpha\) and \(p\) determine \(G\) up to type, as follows from the theorem of types.

(b) The constants \(A_{n}\) of \((S_{n}-B_{n})/A_{n}\to_{d}Y\cong G\) necessarily satisfy (according to the theorem of types)

\[nU(A_{n})/A_{n}^{2}\sim nA_{n}^{-\alpha}L(A_{n})\to 1\quad\mbox{ for $L(\cdot)$ slowly varying at $\infty$}. \tag{4}\]

(c) The following are equivalent (for some constant \(0<\alpha<2\), and then for some \(c>0\) and some \(0\leq p\leq 1\)):

\[F\in{\cal D}_{N}(G)\qquad\qquad\mbox{ for some stable $G$ with characteristic exponent $\alpha$}. \tag{5}\]

\[x^{\alpha}P(X>x)\to cp\qquad\mbox{ and }\qquad x^{\alpha}P(X<-x)\to c(1-p)\qquad\mbox{ as $x\to\infty$}. \tag{6}\]

\[t|K(1-t)|^{\alpha}\to cp\qquad\mbox{ and }\qquad t|K(t)|^{\alpha}\to c(1-p)\qquad\mbox{ as $t\to 0$}. \tag{7}\]

**Theorem 4.2** (Domain of attraction of the normal): \(F\in{\cal D}(\mbox{Normal})\) if and only if either (hence, both) \(U(\cdot)\) is slowly varying at \(\infty\) or \(V(\cdot)\) is slowly varying at \(0\). (Theorem 10.6.1 and proposition 10.6.1 give a myriad of other equivalences.)

**Exercise 4.1**: Use section C.4 to show that for \(0<\alpha<2\) the following are equivalent conditions:

\[U\in{\cal U}_{2-\alpha}. \tag{9}\] \[P(|X|>x)\in{\cal U}_{\alpha}.\] (10) \[[x^{2}P(|X|>x)/\int_{[|y|\leq x]}y^{2}\,dF(y)]\to\mbox{$\frac{2- \alpha}{\alpha}$}\qquad\quad\mbox{ as $x\to\infty$}. \tag{8}\]

Other characterizations in terms of \(K\) can be found in or derived from theorem C.4.2. The theorems and this remark can also be proved via the Levy-Khinchin theorem and results about regularly varying functions.

**Exercise 4.2** (a) State necessary and sufficient conditions on \(F\) for \(F\in{\cal D}(\mbox{Cauchy})\).

(b) Do the same for \(F\in{\cal D}_{N}\) (Cauchy).

(c) Show by example that \({\cal D}_{N}\) (Cauchy) is a proper subset of \({\cal D}(\mbox{Cauchy})\).

(d) Observe that a symmetric df \(F(\cdot)\) is in \({\cal D}(\mbox{Cauchy})\) if and only if the tail function defined by \(xP(X>x)=x(1-F(x))\) is slowly varying. (Recall the tail function \(\tau(.)\) of Feller used in the WLLN in (8.4.2).)

**Exercise 4.3**: (a) Show by example that the domain of normal attraction of the normal law \({\cal D}_{N}(N(0,1))\) is a proper subset of the domain of attraction of the normal law \({\cal D}(N(0,1))\). To this end, let \(X_{1},X_{2},\ldots\) be iid with density

\[f(x)=|x|^{-3}\times(2\log|x|)\times 1_{[1,\infty)}(|x|)\]

and consider \(S_{n}/(\sqrt{n}\log n)\).

(b) Give a second example that works.

(c) For both examples, determine an \(A_{n}\) that works.

**Exercise 4.4**: (a) Consider the context of theorem 11.4.2. The constants \(A_{n}\) used for \((S_{n}-B_{n})/A_{n}\to_{d}Y\cong N(0,1)\) must satisfy \(n\,\tilde{\sigma}_{n}^{2}/A_{n}^{2}\to 1\) (equivalently, it must hold that \(nU(A_{n})/A_{n}^{2}\to 1\) as \(n\to\infty\)) as follows from the theorem of types.

(b) It is also true that

\[F\in{\cal D}_{N}\ ({\rm Normal})\qquad{\rm if\ and\ only\ if}\qquad\sigma^{2}<\infty. \tag{11}\]

(When \(\sigma^{2}\in(0,\infty)\), we know already that \(\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\to N(0,1)\).)

[MISSING_PAGE_FAIL:300]

**Proof.** We initially approximate the distribution of \(Z_{n}\) by that of

(a) \[\bar{Z}_{n}\equiv\frac{1}{\sqrt{2}}N(0,1)+\frac{1}{\sqrt{2}}[{\rm Gamma }(\bar{r})-\bar{r}]/\sqrt{\bar{r}}\qquad\mbox{ with }\bar{r}\equiv n/(2\mu_{3}^{2})\] \[\quad\cong\frac{1}{\sqrt{2}}(0,1;0,0)+\frac{1}{\sqrt{2}}(0,1;2/ \sqrt{\bar{r}},6/\bar{r})=(0,1;1/\sqrt{2\bar{r}},3/2\bar{r})\] (b) \[\quad\cong(0,1;\mu_{3}/\sqrt{n},3\mu_{3}^{2}/n)\qquad\mbox{ matching (\ref{eq:2}) to three cumulants}\] (c) \[\quad\equiv\frac{1}{\sqrt{n}}{\sum_{1}}[\frac{1}{\sqrt{2}}N_{k}+ \frac{1}{\sqrt{2}}W_{k}]\equiv\frac{1}{\sqrt{n}}{\sum_{1}}^{n}Y_{k},\]

where the \(N_{k}\cong N(0,1)\) and the \(W_{k}\cong[{\rm Gamma}(a)-a]/\sqrt{a}\) with \(a=1/(2\mu_{3}^{2})\) are independent. Let \(\phi_{Y}(t)\equiv{\rm E}e^{itY}\) and \(\psi_{Y}\equiv\log\phi_{Y}\), with \(\phi_{X}\) and \(\psi_{X}\) defined analogously. Then

\[|\phi_{Z_{n}}(t)-\phi_{\bar{Z}_{n}}(t)|=|\phi_{\bar{Z}_{n}}(t)| \times|e^{n[\psi_{X}(t/\sqrt{n})-\psi_{Y}(t/\sqrt{n})]}-1|\] \[\quad\equiv|\phi_{\bar{Z}_{n}}(t)|\times|e^{z}-1|\] (d) \[\quad\leq|\phi_{N(0,1)}(t/\sqrt{2})|\times|\phi_{\Sigma W_{k/ \sqrt{n}}}(t/\sqrt{2})|\times|z|e^{|z|}\] (e) \[\quad\leq e^{-t^{2}/4}\times 1\times|z|e^{|z|}.\]

Here (provided that \(|t|/\sqrt{n}\) is sufficiently small for the expansion of (9.6.22) to be valid) the inequality (9.6.22) then gives (since the first three cumulants of \(X\) and \(Y\) match)

(f) \[|z|=n|\psi_{X}(t/\sqrt{n})-\psi_{Y}(t/\sqrt{n})|\leq t^{4}\,\bar{ c}_{4}\,[{\rm E}X^{4}+{\rm E}Y^{4}]/n\] \[\quad\leq t^{4}\bar{c}_{4}[(3+(\mu_{4}-3))+(3+3\mu_{3}^{2})]/n \leq t^{4}7\bar{c}_{4}\mu_{4}^{3/2}/n\] (g) \[\quad\equiv t^{4}c^{2}/n\] (h) \[\quad\leq t^{2}/9\,\mbox{ for }|t|\leq\sqrt{n}/3c.\]

Plugging both (g) and (h) into (e) gives

(i) \[|\phi_{Z_{n}}(t)-\phi_{\bar{Z}_{n}}(t)|\leq(c^{2}t^{4}/n)\exp(-(5/36)t^{2}) \qquad\mbox{ for }|t|\leq\mbox{ (some $d$) }\sqrt{n},\]

where \(c\) and \(d\) depend on the \(\beta_{2}\) value of \((X-\mu)/\sigma\). [The specification in (a) that \(\bar{Z}_{n}\) has a normal component is not natural or practically useful, but it delivers the technically lovely exponential bound component \(\exp(-t^{2}/4)\) in both (d) and (e). Since (a) is not useful for a practical approximation, we will overcome this objection by doing the approximation (a) again--to difference it out. (I believe this whole approach may be new.)]

Let \(G_{n1},\ldots,G_{nn}\) be iid as \([{\rm Gamma}\ (b)-b]\ /\sqrt{b}\) for \(b\equiv 4/\mu_{3}^{2}\). Then

(j) \[G_{r}\equiv\frac{1}{\sqrt{n}}{\sum_{1}}^{n}G_{nk}\cong[{\rm Gamma}(r)-r]/ \sqrt{r}\,\mbox{ with }\,r\equiv 4n/\mu_{3}^{2}\]

(k) \[\cong(0,1;\mu_{3}/\sqrt{n},(3/2)\mu_{3}^{2}/n),\quad\mbox{ matching (\ref{eq:2}) to three cumulants.}\]

We now approximate \(G_{r}\) by the \(\bar{Z}_{n}\) of (a) [just as earlier we approximated \(Z_{n}\) by the \(\bar{Z}_{n}\) of (a)]. This gives (with generic constants \(c\) and _d_)

(l) \[|\phi_{G_{r}}(t)-\phi_{\bar{Z}_{n}}(t)|\leq(c^{2}t^{4}/n)\exp(-(5/36)t^{2}) \qquad\mbox{ for }\quad|t|\leq d\sqrt{n}.\]Combining (i) and (1) gives

\[|\phi_{Z_{n}}(t)-\phi_{G_{r}}(t)|\leq(ct^{4}/n)\exp(-(5/36)t^{2})\,\,\,{\rm for}\,| t|\leq d\sqrt{n},\]

where the generic \(c\) and \(d\) may depend on the df of \((X-\mu)/\sigma\).

Consider (5)(b). The inversion formula (using (9.6.22) to expand) gives

\[|f_{Z_{n}}(x)-f_{G_{r}}(x)|=\frac{1}{2\pi}|\int_{-\infty}^{\infty}e^{-itx}[\phi _{Z_{n}}(t)-\phi_{G_{r}}(t)]dt|\leq I_{1n}+I_{2n}+I_{3n}\]

(m) \[\equiv\int_{[|t|\leq d\sqrt{n}]}|\phi_{Z_{n}}(t)-\phi_{G_{r}}(t)|dt\] \[+\int_{[|t|>d\sqrt{n}]}|\phi_{Z_{n}}(t)|dt+\int_{[|t|>d\sqrt{n}]}| \phi_{G_{r}}(t)|dt.\]

Now (7) bounds the integrand of the lead term to give

(n) \[I_{1n}\leq\int(ct^{4}/n)\exp(-(5/36)t^{2})dt\leq c/n.\]

Since \(\int|\phi_{X}(t)|dt<\infty\) by (4)(b), the density inversion formula (9.4.9) shows that \(X\) has a density \(f_{X}(\cdot)\). Since \(X\) is thus not distributed on a grid (and likewise \(Y\)), proposition 9.8.2 gives

\[\theta\equiv\|\phi_{X}\|_{d}^{\infty}\vee\|\phi_{W}\|_{d}^{\infty}<1.\]

Thus the second term in (m) satisfies

\[I_{2n}=\int_{[|t|>d\sqrt{n}]}|\phi_{X}(t/\sqrt{n})|^{n}dt\leq\sqrt{n}\int_{[|s |>d]}|\phi_{X}(s)|^{n}ds\]

(o) \[\leq\theta^{n-1}\sqrt{n}\int_{-\infty}^{\infty}|\phi_{X}(s)|ds=o(n^{-r}),\qquad \mbox{for any $r>0$,}\]

since the \(\theta^{n}\) term goes to 0 geometrically. Likewise, \(I_{3n}=o(n^{-r})\), for any \(r>0\), since \(|\phi_{G}|^{k}\) satisfies (4)(b), for some \(k\). Combine (n) and (o) into (m) to get (5)(b).

Consider (5)(a). We will apply Esseen's lemma. Thus

\[\begin{array}{l}\|F_{Z_{n}}(x)-F_{G_{r}}(x)\|\equiv I_{1n}^{\prime}+I_{2n}^{ \prime}+I_{3n}^{\prime}\\ \qquad\leq\{\int_{[|t|\leq d\sqrt{n}]}+\int_{[d\sqrt{n}<|t|\leq dn]}\}\frac{1} {|t|}|\phi_{Z_{n}}(t)-\phi_{G_{r}}(t)|dt+24\|g_{r}\|/\pi dn\\ \qquad\leq\int(c|t|^{3}/n)\exp(-(5/36)t^{2})dt+2(dn/d\sqrt{n})\theta^{n}+24\|g _{r}\|/(\pi dn)\\ \qquad=O(1/n),\end{array}\]

where (4)(a) is now used to obtain \(\theta^{n}\).

Consider (6), when \(\mu_{4}\) is not assumed finite. Use of (9.6.22) at line (f) must be replaced by use of (9.6.21). The \(|t|^{3}{\rm E}|X|^{3}\) in (9.6.22) would give a bound of only \(C_{F}/\sqrt{n}\) at line (m) of the current proof; but the added \(\delta_{3}(t/\sqrt{n})\) term in (9.6.21) that is valid on \(|t|\leq d\sqrt{n}\) (now with a tiny \(d\)) allows a \(C_{F}\) to be replaced by a \(C_{F,n}\to 0\). Dominated convergence is used for this, with dominating function guaranteed by \({\rm E}|X|^{3}<\infty\).

If we knew any appropriate two-parameter family closed under convolution, we could choose those two parameters to match both third and fourth cumulants. Then \(C_{F}/n^{3/2}\) under \(\mu_{5}<\infty\) and \(C_{F,n}/n\) under \(\mu_{4}<\infty\) would be possible. The proof is essentially unchanged, and needs no further comment. The difference of two gammas can be specified in several different ways. All work. The only question is which has the greatest coverage of the (skewness, kurtosis)-plane. Using gammas, we seem stuck with positive kurtosis, which leaves out some of the least important situations. [Edgeworth expansions allow us to cover the whole (skewness, kurtosis)-plane, but they have some other deficiencies. For instance, the Edgeworth approximation to a df or density is not necessarily a df or density itself.]

#### Poisson Approximation

Most discrete distributions we care about live on the integers, and the write-up here will reflect that fact and make this case fit our notation with the least effort. Rather than approximating sums of such rvs \(X\) by an appropriate gamma with a continuity correction, we will use a nice discrete analogue of the gamma.

For the standardized Poisson, the first four cumulants are

\[G_{r}\equiv\frac{\mbox{Poisson}(r)-r}{\sqrt{r}}\cong\left(0,1;\frac{1}{\sqrt{r} },\frac{1}{r}\right). \tag{8}\]

For the difference of two Poissons we let \(p+q=1\) and \(c,d>0\) and set \(u\equiv p/c\) and \(v\equiv q/d\), and then define \(r=c^{2}n\) and \(s=d^{2}n\) and set

\[\begin{array}{l}G_{r,s}\equiv-\sqrt{p}\,G_{c^{2}n}+\sqrt{q}G_{d^{2}n}\cong(0, 1;\frac{1}{\sqrt{n}}(-\frac{p^{3/2}}{c}+\frac{q^{3/2}}{d}),\frac{1}{n}(\frac{p ^{2}}{c^{2}}+\frac{q^{2}}{d^{2}}))\\ \qquad=(0,1;\frac{1}{\sqrt{n}}(-\sqrt{p}\,u+\sqrt{q}\,v),\frac{1}{n}(u^{2}+v^{ 2})).\end{array} \tag{9}\]

[This approach can (multiply) match all \((\mu_{3},\mu_{4})\) pairs for which \(\mu_{3}^{2}\leq(\mu_{4}-3)\).]

**Theorem 5.2** (Poisson approximation): _Consider a rv \(X\) on the integers and let \(Z_{n}\) be as in (3). Let \(r\equiv n/\mu_{3}^{2}\), so that_

\[G_{r}\equiv[\mbox{Poisson}(r)-r]/\sqrt{r}\cong(0,1;\mu_{3}/\sqrt{n},\mu_{3}^{2} /n).\]

_(i) Then for some constants \(C_{F}\) and \(C_{F,n}\to 0\) (that may depend on the df of the standardized rv \((X-\mu)/\sigma\)):_

\[\|p_{Z_{n}}-p_{G_{r}}\|\leq C_{F}/n^{3/2}\qquad\mbox{ when }\mu_{4}<\infty. \tag{11}\] \[\|p_{Z_{n}}-pc_{r}\|\leq C_{F,n}/n\qquad\qquad\mbox{ when only }\mu_{3}\in(0,\infty). \tag{10}\]

_[Most probabilities that one computes involve summing over the appropriate \(M\sqrt{n}\) number of terms that are each of the type \(p_{Z_{n}}(\cdot)\).]_

_(ii) Suppose \(r\) and \(s\) can be specified so that \(G_{r,s}\cong(0,1;\mu_{3}/\sqrt{n},(\mu_{4}-3)/n)\). Then \(n^{2}\) can replace \(n^{3/2}\) in (10), provided that \(\mu_{5}<\infty\). And \(n^{3/2}\) can replace \(n\) in (11), provided that \(\mu_{4}<\infty\)._

**Proof.** The appropriate inversion formula now (for a distribution on the grid \(am+b\)) is given by

\[p_{m}\equiv P(X=m)=\frac{a}{2\pi}\int_{[|t|\leq\pi/a]}\exp(-it(am+b))\phi_{X}( t)dt. \tag{12}\]

By the previous proof (including the previous step (a) normal component, but now appearing in step (u)) yields

(u) \[|\phi_{Z_{n}}(t)-\phi_{G_{r}}(t)|\leq(c^{2}t^{4}/n)\exp(-(5/36)t^{2})\mbox{ for }|t|\leq d\sqrt{n},\]

for \(c\) and \(d\) that may depend on the df of \((X-\mu)/\sigma\). Applying the inversion formula in (12) now gives

(v) \[\sqrt{n}|P(Z_{n}=m/\sqrt{n})-P(G_{r}=m/\sqrt{n})|\] \[\qquad=\frac{\sqrt{n}}{2\pi\sqrt{n}}|\int_{[|t|\leq\pi\sqrt{n}]} e^{-itm/\sqrt{n}}[\phi_{Z_{n}}(t/\sqrt{n})-\phi_{G_{r}}(t/\sqrt{n})]dt|\](w) \[\leq\int_{|[t|\leq d\sqrt{n}]}|\phi_{Z_{n}}(t/\sqrt{n})-\phi_{G_{r}}(t/ \sqrt{n})|dt\] \[\quad+\int_{[d\sqrt{n}<|t|\leq\pi\sqrt{n}]}|\phi_{Z_{n}}(t/\sqrt{n}) |dt+\int_{[d\sqrt{n}<|t|\leq\pi\sqrt{n}]}|\phi_{G_{r}}(t/\sqrt{n})|dt\] (x) \[\leq c/n+o(n^{-r})+o(n^{-r}),\]

as is now easily shown with the same arguments as before (because \(\theta<1\), since \(\pi\sqrt{n}\) never reaches a full period of \(\phi_{X}\); recall proposition 9.8.2). 

**Exercise 5.1**: Verify part (iii) of theorem 5.1.

**Exercise 5.2**: Show that the chf \(\phi\) of the \(G_{r}\) of (1) satisfies \(\int|\phi(t)|^{k}\,dt<\infty\) for some \(k>0\).

**Exercise 5.3**: We can replace the Poisson by the NegBiT \((\mathrm{r},p)\) distribution with the moment structure

\[G_{r}\equiv\frac{\mathrm{NegBiT}(r,p)-r/p}{\sqrt{rq/p^{2}}}\cong\Big{(}0,1; \frac{1}{\sqrt{r}}\frac{1+q}{\sqrt{q}},\frac{1}{r}\frac{1+4q+q^{2}}{q}\Big{)}. \tag{13}\]

This is probably more useful than the previous theorem. (a) Verify the claim.

(b) Provide some numerical work to compare Poisson and NegBiT approximations to a situation of interest.

**Remark 5.1** (Gamma approximation or Edgeworth approximation?): In the next section we will derive the classical Edgeworth approximations. The first-order Gamma (or Poisson, or NegBiT) approximations of the current section are of the same order as the first-order Edgeworth approximations. Moreover, approximation by the \(G_{r}\)-distribution is an approximation by a probability distribution; but this is not true of the Edgeworth approximation. Happily, gamma approximations are easily and accurately implemented in S-plus, say.

The situation is similar regarding the two second-order approximations, provided that the first two cumulants of the underlying rv can be matched within the family of \(G_{rs}\)-distributions. However, the \(G_{rs}\)-distributions are not available within any set of computer-generated routines I know, so that this would be hard to implement at present. However, this would seem to make a _nice project_ for a computer-oriented statistician. 

**Examples**

**Example 5.1** (Sampling distribution of \(\bar{\mathbf{X}}_{\mathbf{n}}\) and \(\mathbf{S}_{\mathbf{n}}^{2}\)): Suppose \(X_{1},\ldots,X_{n}\) is a random sample from a population whose first four cumulants are \((\mu,\kappa_{2};\kappa_{3},\kappa_{4})\). [Let K&S denote Kendall and Stuart (1977, Vol. I).] How do we apply a gamma approximation?

(a) Consider first an infinite population, in which \(\kappa_{2}=\sigma^{2},\kappa_{3}=\mu_{3}\), and \(\kappa_{4}=\mu_{4}-3\). Then (9.6.20) gives the first four cumulants of \(\sqrt{n}(\bar{X}_{n}-\mu)\) as

\[\sqrt{n}(\bar{X}_{n}-\mu)\cong(0,\sigma^{2};\frac{1}{\sqrt{n}}\kappa_{3},\frac {1}{n}\kappa_{4}). \tag{14}\]

Now, unbiased estimators \(\hat{\kappa}_{j}\) of these \(\kappa_{j}\) are given (see K&S (p. 297, 300)) by

\[\begin{array}{l}\hat{\kappa}_{2}\equiv\frac{n}{n-1}\hat{m}_{2},\quad\hat{ \kappa}_{3}=\frac{n^{2}}{(n-1)(n-2)}\hat{m}_{3},\\ \hat{\kappa}_{4}\equiv\frac{n^{2}}{(n-1)(n-2)(n-3)}\{(n+1)\hat{m}_{4}-3(n-1) \hat{m}_{2}^{2}\},\end{array} \tag{15}\]where \(\hat{m}_{j}\equiv\sum_{1}^{n}(X_{j}-\bar{X}_{n})^{j}/n\). We will combine these facts with the theorems of the previous sections to approximate the distribution of \(\sqrt{n}(\bar{X}_{n}-\mu)\). Additionally (by K&S (p. 306-307),

\[\sqrt{n}(S_{n}^{2}-\sigma^{2})\cong(0,\kappa_{4}+\frac{2n}{n-1}\kappa_{2}^{2}; \frac{1}{n}\kappa_{6}+\frac{12}{n-1}\kappa_{4}\,\kappa_{2}+\frac{4(n-2)}{(n-1) ^{2}}\kappa_{3}^{2}+\frac{8n}{(n-1)^{2}}\kappa_{2}^{3}), \tag{16}\]

where correcting for skewness in (16) should probably be ignored. An unbiased estimator of the variance in (16) (unbiasedness verifiable from K&S (p. 296)) is

\[\frac{n-1}{n+1}[\hat{\kappa}_{4}+2\frac{n}{n-1}\hat{\kappa}_{2}^{2}]. \tag{17}\]

(b) Finally (by K&S (p. 327)),

\[\mbox{Corr}[\hat{\kappa}_{1},\hat{\kappa}_{2}]=\gamma_{1}/\left\{\gamma_{2}+ \frac{2n}{n-1}\right\}^{1/2}. \tag{18}\]

(c) In approximating the bootstrap distribution of \(\overline{X_{n}^{*}}\), it is exactly true that

\[\sqrt{n}(\overline{X_{n}^{*}}-\bar{X}_{n})\cong(0,\hat{\kappa}_{2};\frac{1}{ \sqrt{n}}\hat{\kappa}_{3},\frac{1}{n}\hat{\kappa}_{4}). \tag{19}\]

(d) Now consider a finite population \(X_{1},\ldots,X_{N}\) whose second, third, and fourth true cumulants \(K_{j}\) are given by (15), with \(N\) replacing \(n\). Unbiased estimators \(\hat{K}_{j}\) are also given by (15), now with \(n\) again (see K&S (p. 320)). It is also true (by K&S (p. 321-322)) that

\[\sqrt{n}(\bar{X}_{n}-\bar{X}_{N}) \cong\Big{(}0,\frac{N-n}{N}K_{2};\frac{N^{2}-3nN+2n^{2}}{n^{2}N^ {2}}K_{3},\] \[\{\alpha_{3}-4\alpha_{2}/N+6\alpha_{1}/N^{2}-\frac{3(N-1)}{N(N+1 )}\alpha_{1}^{2}\}K_{4}+3\frac{N-1}{N+1}\alpha_{1}^{2}K_{2}^{2}\Big{)}, \tag{20}\]

where \(\alpha_{r}\equiv(n^{-r}-N^{-r})\). Finally (by K&S (p. 323)),

\[\sqrt{n}(S_{n}^{2}-S_{N}^{2})\cong(0,K_{4}+2\frac{n}{n-1}K_{2}^{2}). \tag{21}\]

Then (by K&S (p. 323)) an unbiased estimator of this last variance is given by (17) (with \(K_{j}\) replacing \(\hat{\kappa}_{j}\)). (Though straightforward, the results cited from K&S are somewhat cumbersome.) 

**Example 5.2** (Hall). A noncentral chisquare rv \(\chi_{n}^{2}(\delta)\) satisfies

\[T\equiv\frac{[\chi_{n}^{2}(\delta)-(n+\delta)]}{\sqrt{2(n+2\delta)}}\cong\frac {1}{\sqrt{n}}\sum_{1}^{n}X_{k}\cong\Bigg{(}0,1;\mu_{3}\equiv\frac{2^{3/2}(1+3 \delta/n)}{\sqrt{n}(1+2\delta/n)^{3/2}}\Bigg{)}. \tag{22}\]

So we approximate this distribution by \(G_{r}\) with \(r\equiv(n+2\delta)^{3}/(2(n+3\delta)^{2})\). Then

\[P(\chi_{n}^{2}(\delta)\leq x)=P(\mbox{Gamma}(r)\leq r+[x-(n+ \delta)](n+2\delta)/2(n+3\delta)) \tag{24}\] \[=(1-\alpha)\,\,\,\mbox{if}\,\,x\equiv(n+\delta)+(\gamma^{\alpha}- r)2(n+3\delta)/(n+2\delta), \tag{23}\]

where \(\gamma^{\alpha}\) denotes the upper \(1-\alpha\) percentage point of Gamma\((r)\). This is easy to implement in Splus, for example. (Hall found that the accuracy seemed quite good, especially in relation to previous proposals.)

**Exercise 5.4** (Poisson approximation of the generalized binomial) We suppose \(X_{n1},\ldots,X_{nn}\) are independent rvs with \(X_{nk}\cong\mbox{Bernoulli}(p_{nk})\). Suppose further that \(Y_{nk}\cong\mbox{Poisson}(p_{nk})\) are independent for \(1\leq k\leq n\). Let \(P_{n}\) and \(Q_{n}\) denote the distributions of \(X_{n}\equiv\sum_{1}^{n}X_{nk}\) and \(Y_{n}\equiv\sum_{1}^{n}Y_{nk}\). Show that the total variation distance between \(P_{n}\) and \(Q_{n}\) satisfies

\[d_{TV}(P_{n},Q_{n})\equiv\sup\{|P_{n}(A)-Q_{n}(A)|:A\in B\}\leq\sum_{k=1}^{n}p _{nk}^{2}. \tag{25}\]

If \(p_{nk}=\lambda_{k}/n\) for \(1\leq k\leq n\), then the bound becomes \(\overline{\lambda^{2}}/n\).

[Hint. The first step is to replace the original Bernoulli(\(p_{nk}\)) rvs by different Bernoulli (\(p_{nk}\)) rvs, to be denoted by \(X_{nk}\) also. To this end we now define the new \(Z_{nk}\cong\mbox{Bernoulli}(1-(1-p_{nk})e^{p_{nk}})\) rvs that are independent for \(1\leq k\leq n\) (and they are also independent of the \(Y_{nk}\)'s). Now define

\[X_{nk}\equiv 1_{[Y_{nk}\geq 1]}+1_{[Y_{nk}=0]}1_{[Z_{nk}=1]}, \tag{26}\]

and verify that it is indeed a Bernoulli(\(p_{nk}\)) rv. (This choice of the jointly distributed pair \((X_{nk},Y_{nk})\) maximizes the mass on the diagonal \(x=y\) of an \((x,y)\)-coordinate system.) Now verify that

\[d_{TV}(P_{n},\ Q_{n})\leq P(X_{n}\neq Y_{n})\leq\sum_{1}^{n}P(X_{nk}\neq Y_{nk} )\leq\sum_{1}^{n}p_{nk}^{2}. \tag{27}\]

This type of proof is called a _coupling_ proof, in that the \((X_{nk},Y_{nk})\) pairs are coupled together as closely as possible.]

**Exercise 5.5** (a)-(d) Derive the claims made in example 5.1(a)-(d).

## Chapter 11 Infinitely divisible and stable distributions \({}^{o}\)

### 11.1 Edgeworth expansions \({}^{*}\)

#### The Setup

Let \(F_{0},f_{0}\), and \(\phi_{0}\) denote the \(N(0,1)\)df, density, and chf. Thus

\[f_{0}(x)\equiv e^{-x^{2}/2}/\sqrt{2\pi}\ \ \ \mbox{and}\ \ \phi_{0}(t)\equiv e^{- t^{2}/2} \tag{1}\]

on the real line. These are related via the inversion formula for chfs as

\[f_{0}(x)=\int_{-\infty}^{\infty}e^{-itx}\phi_{0}(t)dt/(2\pi). \tag{2}\]

Differentiating \(f_{0}\) gives

\[\begin{array}{ll}f_{0}^{\prime}(x)=-xf_{0}(x),&f_{0}^{\prime\prime}(x)=(x^{2 }-1)f_{0}(x),\\ f_{0}^{\prime\prime\prime}(x)=-(x^{3}-3x)f_{0}(x)\,&f_{0}^{(iv)}(x)=(x^{4}-6x^{2}+3 )f_{0}(x),\\ f_{0}^{(v)}(x)=-(x^{5}-10x^{3}+15x)f_{0}(x),&\\ f_{0}^{(vi)}(x)=(x^{6}-15x^{4}+45x^{2}-15)f_{0}(x);&\end{array} \tag{3}\]

and in general,

\[f_{0}^{(k)}(x)=(-1)^{k}H_{k}(x)f_{0}(x) \tag{4}\]

defines what we will call the \(k\)th _Hermite orthogonal polynomial_\(H_{k}\) (see exercise 11). Equating the derivatives in (3) to derivatives of the right-hand side of (2) gives

\[(-1)^{k}H_{k}(x)f_{0}(x)=\int_{-\infty}^{\infty}e^{-itx}(-it)^{k}\phi_{0}(t)dt /(2\pi), \tag{5}\]

which expresses \(H_{k}f_{0}\) as the inverse Fourier transform of \((\mbox{it}\,)^{k}\phi_{0}(t)\). This gives the _key result_ that

\[(it)^{k}\phi_{0}(t)\qquad\mbox{ is the Fourier transform of}\qquad H_{k}(\cdot)f_{0}(\cdot) \tag{6}\]

Now suppose that \(X_{1},\ldots,X_{n}\) are iid where

\[X\cong(0,\sigma^{2})\qquad\mbox{has chf }\phi(\cdot) \tag{7}\]

We let \(S_{n}\equiv\sum_{k=1}^{n}X_{k}\), and agree that

\[F_{n}(\cdot)\qquad\mbox{ denotes the df of}\qquad S_{n}/(\sigma\sqrt{n}). \tag{8}\]

The idea is to expand \(F_{n}\) in terms of the orthogonal polynomials \(H_{k}\). However, we choose instead to obtain a first-order or second-order approximation, together with an error analysis. Also,

\[f_{n}(\cdot)\qquad\mbox{ denotes the density of}\qquad S_{n}/(\sigma\sqrt{n}), \qquad\mbox{if it exists.} \tag{9}\]

In this latter case we will also seek to expand \(f_{n}\). The expansions we will derive for \(f_{n}\) and \(F_{n}\) are known as _Edgeworth expansions_.

#### Edgeworth Expansions for Densities

Instead of just assuming that \(f_{n}\) exists, we assume instead that the chf \(\phi\) of the rv \(X\cong(0,\sigma^{2})\) satisfies

\[\int_{-\infty}^{\infty}|\phi(t)|^{m}dt<\infty,\mbox{ for some positive integer }m. \tag{10}\]

This guarantees both that \(f_{n}\) exists for all \(n\geq m\), and that it can be found from the Fourier inversion formula (9.4.9).

**Theorem 6.1**  Suppose condition (10) holds. Let \(\gamma_{1}\equiv{\rm E}(X/\sigma)^{3}\) denote the _skewness_, and let \(\gamma_{2}\equiv{\rm E}(X/\sigma)^{4}-3\) denote the _tail heaviness_ of \(X\cong F(0,\sigma^{2})\).

(a) Then

\[\left\|f_{n}(\cdot)-f_{0}(\cdot)\ \{1+\frac{\gamma_{1}}{3!\sqrt{n}}H_{3}( \cdot)\}\right\| \tag{12}\] \[=o(1/\sqrt{n})\qquad\mbox{ (as a function of ${\rm E}|X/\sigma|^{3}$ and $F_{X/\sigma})\quad\mbox{ [if ${\rm E}|X|^{3}<\infty$]}$}\] or (13) \[=O(1/n)\qquad\mbox{ (as a function of ${\rm E}|X/\sigma|^{4}$ and $F_{X/\sigma})\quad\mbox{ [if ${\rm E}X^{4}<\infty$]}$}. \tag{11}\]

(b) Moreover,

\[\left\|f_{n}(\cdot)-f_{0}(\cdot)\left\{1+\left[\frac{1}{\sqrt{n}}\frac{ \gamma_{1}}{3!}H_{3}(\cdot)\right]+\frac{1}{n}\left[\frac{\gamma_{2}}{4!}H_{4} (\cdot)+\frac{\gamma_{1}^{2}}{2(3!)^{2}}H_{6}(\cdot)\right]\right\}\right\| \tag{15}\] \[=o(1/n)\qquad\mbox{ (as a function of ${\rm E}|X/\sigma|^{4}$ and $F_{X/\sigma})\quad\mbox{ [if ${\rm E}X^{4}<\infty$]}$}\] or (16) \[=O(1/n^{3/2})\quad\mbox{ (as a function of ${\rm E}|X/\sigma|^{5}$ and $F_{X/\sigma})\quad\mbox{ [if ${\rm E}|X|^{5}<\infty$]}$}. \tag{14}\]

We specifically write out that \(H_{0}(x)\equiv 1\) and

\[\begin{array}{ll}H_{1}(x)\equiv x,&H_{2}(x)\equiv x^{2}-1,\\ H_{3}(x)\equiv x^{3}-3x,&H_{4}(x)\equiv x^{4}-6x^{2}+3,\\ H_{5}(x)\equiv x^{5}-10x^{3}+15x,&H_{6}(x)\equiv x^{6}-15x^{4}+45x^{2}-15,\end{array} \tag{17}\]

for use in the current set of theorems. The previous theorem was for densities. The next is for dfs. Condition (10) is used to control the extreme tails in the Fourier inversion formulas for densities. In proving analogues of (11) and (12) for dfs, we will be able to use Esseen's lemma to control these tails instead. However, the analogues of (13) and (14) run into other problems, and these are again overcome via a (now weaker) restriction on \(\phi\). All proofs are at the end of this section.

**Exercise 6.1**  Find \(H_{7}(\cdot)\) Show that \(\int_{-\infty}^{\infty}H_{m}H_{n}f_{0}\,d\lambda=n!\) if \(m=n\), and \(0\) else.

**Edgeworth Expansions for Distribution Functions**

Consider the potential hypothesis

\[\limsup_{|t|\to\infty}|\phi(t)|<1. \tag{18}\]

(This is weaker than (10). The Riemann-Lebesgue lemma shows that (18) holds if \(F\) has an absolutely continuous component, a la theorem 6.1.1.)

**Theorem 6.2**  Suppose that \(X\) is not distributed on a grid. Let \(\gamma_{1}\equiv{\rm E}(X/\sigma)^{3}\) denote the _skewness_, and let \(\gamma_{2}\equiv{\rm E}(X/\sigma)^{4}-3\) denote the _tail heaviness_ of the rv \(X\cong F(0,\sigma^{2})\).

(a) Then

\[\left\|F_{n}(\cdot)-F_{0}(\cdot)\ +f_{0}(\cdot)\ \left\{\frac{\gamma_{1}}{3!\sqrt{n}}H_{2}(\cdot)\right\}\right\| \tag{20}\] \[=o(1/\sqrt{n})\ \mbox{(as a function of ${\rm E}|X/\sigma|^{3}$ and $F_{X/\sigma})\quad\mbox{ [if ${\rm E}|X|^{3}<\infty$]}$}\] or (additionally requiring (18) for (19)) \[=O(1/n)\quad\mbox{ (as a function of ${\rm E}|X/\sigma|^{4}$ and $F_{X/\sigma})\quad\mbox{ [if ${\rm E}X^{4}<\infty$]}$}. \tag{19}\](b) Moreover, when (16) holds,

\[\begin{array}{l}\left\|F_{n}(\cdot)-F_{0}(\cdot)+f_{0}(\cdot)\left\{\frac{1}{ \sqrt{n}}\frac{\gamma_{1}}{3!}H_{2}(\cdot)+\frac{1}{n}\left[\frac{\gamma_{2}}{4!}H_{3}(\cdot)+\frac{\gamma_{1}^{2}}{2\cdot(3!)^{2}}H_{5}(\cdot)\right]\right\} \right\|\\ =o(1/n)\qquad\qquad\mbox{ (as a function of E$|X/\sigma|^{4}$ and $F_{X/\sigma})$ \ \ \ \ [if EX^{4}<\infty]}\\ \qquad\mbox{ or }\\ =O(1/n^{3/2})\qquad\mbox{ (as a function of E$|X/\sigma|^{5}$ \ \ and \ $F_{X/\sigma})$ \ \ \ \ [if E|X|^{5}<\infty]}.\end{array} \tag{19}\]

**Exercise 6.2**: Let \(Z_{n}\equiv S_{n}/\sigma\sqrt{n}\) as above, and let \(f_{n}\) denote its density under (10). Let \(Z_{n}^{*}\equiv[\mbox{Gamma}(r)-r]/\sqrt{r}\), with \(r\equiv 4n/\gamma_{1}^{2}\), and let \(g_{n}(\cdot)\) denote its density. Show that \(\|f_{n}-g_{n}\|=O(1/n)\) as a function of E\(|X/\sigma|^{4}\) and \(F_{X/\sigma}\) when \(E|X|^{4}<\infty\).

**The Proofs**

We defined and expanded the cumulant generating function \(\psi(\cdot)\equiv\mbox{Log}\,\phi(\cdot)\) in exercise 9.6.6. The first few cumulants of the standardized rv \(X/\sigma\) were seen to be \(0,1,\gamma_{1}\), and \(\gamma_{2}\).

**Proof.** Consider theorem 6.1(a). Without loss of generality we suppose that \(\sigma=1\). We now agree that

(a) \[d_{n}\equiv f_{n}-\left[1+\frac{1}{\sqrt{n}}\frac{\gamma_{1}}{3!}H_{3}\right]f_ {0}\]

denotes the difference between the true \(f_{n}\) and our first approximation to it. Note from (6) that \(d_{n}\) has Fourier transform

\[\phi_{n}(t)\equiv[\phi(t/\sqrt{n})]^{n}-\phi_{0}(t)\Bigl{[}1+\frac {1}{\sqrt{n}}\frac{\gamma_{1}}{3!}(it)^{3}\Bigr{]}\] (b) \[=e^{-t^{2}/2}\{e^{t^{2}/2+n\psi(t/\sqrt{n})}-[1+(\gamma_{1}/3!)(it) ^{3}/\sqrt{n}]\}\] (c) \[\equiv e^{-t^{2}/2}[e^{z+\epsilon}-(1+z)]. \tag{21}\]

Thus the Fourier inversion formula (9.4.9) gives

(d) \[d_{n}(x)=\int_{-\infty}^{\infty}e^{itx}\phi_{n}(t)dt/(2\pi),\]

since (10) implies that \(\int_{-\infty}^{\infty}|\phi_{n}(t)|dt<\infty\). Thus for any fixed \(\theta>0\) and all \(x>0\) we have

(e) \[\begin{array}{l}|d_{n}(x)|\leq\int_{-\infty}^{\infty}|\phi_{n}(t)|dt=\int_{ -\infty}^{\infty}e^{-t^{2}/2}|e^{z+\epsilon}-(1+z)|dt\\ =\int_{[|t|\leq\theta\sqrt{n}/{\rm E}|X|^{3}]}e^{-t^{2}/2}|e^{z+\epsilon}-(1+ z)|dt+o(n^{-r})\end{array}\]

for any \(r>0\), as in (9.6.3). Now,

(f) \[\begin{array}{l}|\phi_{n}(t)|=e^{-t^{2}/2}|e^{n\psi(t/\sqrt{n})-(-t^{2}/2)}- [1+(\gamma_{1}/3!)(it)^{3}/\sqrt{n}]|\\ \equiv e^{-t^{2}/2}|e^{z+\epsilon}-(1+z)|=e^{-t^{2}/2}|e^{z}(e^{\epsilon}-1)+( e^{z}-(1+z))|\\ \leq e^{-t^{2}/2}[|\epsilon|e^{|\epsilon|}e^{|z|}+z^{2}e^{|z|}/2]\end{array}\]using (9.6.3). Note that for all \(|t|\leq\theta\sqrt{n}/{\rm E}|X|^{3}\) we have

(h) \[|z|\leq(|t|^{3}/6){\rm E}|X|^{3}/\sqrt{n}\leq\theta t^{2}/6\leq t^{2 }/8\qquad\mbox{if $\theta\leq\frac{3}{4}$},\] \[|\epsilon|=|n\psi(t/\sqrt{n})-[-t^{2}/2+(\gamma_{1}/6)(it)^{3}/ \sqrt{n}]|\] (i) \[\leq c_{3}|t|^{3}{\rm E}|X|^{3}\delta(\theta)/\sqrt{n}\leq c_{3}t^{2} \theta\delta(\theta)\leq t^{2}/8\qquad\mbox{if $\theta$ is small enough},\]

where \(\delta(\cdot)\) denotes the function \(\delta_{3}(\cdot)\) function of (9.6.21) associated with the rv \(X/\sigma\). Using (h) and (i), the bound in (e) becomes (for some \(\theta\) small enough)

\[|d_{n}(x)|\leq\int_{[|t|\leq\theta\sqrt{n}/{\rm E}|X|^{3}]}e^{-t^{2}/2}\{| \epsilon|e^{|\epsilon|}e^{|z|}+z^{2}e^{|z|}/2\}dt+o(n^{-r})\] (j) \[\leq c_{3}\delta(\theta)[{\rm E}|X|^{3}/\sqrt{n}]\int_{-\infty}^{ \infty}|t|^{3}e^{-t^{2}/4}dt\] \[\quad+[({\rm E}|X|^{3})^{2}/(72n)]\int_{-\infty}^{\infty}|t|^{6}e ^{-3t^{2}/8}dt+o(n^{-r})\] (k) \[=o(n^{-1/2})\mbox{ uniformly in $x$},\]

since a tiny \(\delta(\theta)\) results from a sufficiently tiny \(\theta\). Thus (11) holds. For (12), we replace the bound in line (i) above by

\[|\epsilon|\leq\bar{c}_{4}\,t^{4}{\rm E}X^{4}/n\leq t^{2}/8,\quad\mbox{which is valid for $|t|\leq\sqrt{n}/\sqrt{8\bar{c}_{4}{\rm E}|X|^{4}}$}, \tag{1}\]

as (9.6.22) guarantees. We then use (1) instead of (i) during (j) (now integrated over the interval \([|t|\leq\sqrt{n}/\sqrt{8/\bar{c}_{4}{\rm E}X^{4}}]\)).

We now turn to (13), and then (14). We first redefine

(m) \[d_{n}\equiv f_{n}-f_{0}\left\{1+\frac{1}{\sqrt{n}}\frac{\gamma_{1}}{3!}H_{3}+ \frac{1}{n}\left[\frac{\gamma_{2}}{4!}H_{4}+\frac{\gamma_{1}^{2}}{2\cdot(3!)^ {2}}H_{6}\right]\right\}.\]

Taking the inverse of its Fourier transform \(\phi_{n}(\cdot)\) gives (as in (e)) that for any fixed value of \(\theta>0\) and all \(x\),

\[|d_{n}(x)|\leq\int_{-\infty}^{\infty}|\phi_{n}(t)|dt\] (n) \[=\int_{[|t|\leq\theta\sqrt{n}/{\rm E}x^{4}]}e^{-t^{2}/2}|e^{t^{2} /2+n\psi(t/\sqrt{n})}-\{1+z+z^{2}/2\}|dt+o(n^{-r})\] (o) \[=\int_{[|t|\leq\theta\sqrt{n}/{\rm E}x^{4}]}e^{-t^{2}/2}|e^{z+ \epsilon}-(1+z+z^{2}/2)|dt+o(n^{-r})\]

for each fixed \(r>0\), with

(p) \[z\equiv\frac{1}{\sqrt{n}}\frac{\gamma_{1}}{3!}(it)^{3}+\frac{1}{n}\frac{\gamma _{2}}{4!}(it)^{4}.\]

The final details are nearly the same as before. \(\Box\)

**Exercise 6.3** : Finish the details of the previous proof of theorem 6.1(b).

**Proof.** Consider theorem 6.2(a). We note that

(q) \[D_{n}\equiv F_{n}-F_{0}+f_{0}\biggl{[}\frac{1}{\sqrt{n}}\frac{\gamma_{1}}{3!}H_{2} \biggr{]}\mbox{ has }D^{\prime}_{n}=d_{n}\equiv f_{n}-f_{0}\biggl{[}1+\frac{1}{ \sqrt{n}}\frac{\gamma_{1}}{3!}H_{3}\biggr{]},\]

where \(d_{n}\) is as in line (a) of the previous proof (just use \(xH_{2}(x)-2x=H_{3}(x)\) to verify this). Esseen's lemma then gives

(r) \[\|D_{n}\|\leq\frac{1}{\pi}\int_{[|t|\leq a\sqrt{n}/{\rm E}|X|^{3}]}\frac{|\phi _{n}(t)|}{|t|}dt+\frac{24\|f_{0}[1+(\gamma_{1}/3!)H_{3}/\sqrt{n}]\|}{\pi a\sqrt {n}/{\rm E}|X|^{3}},\]

where \(\phi_{n}\) is the same \(\phi_{n}\) appearing in (21). Since the norm in the second term on the right of (r) is bounded, the second term in (r) is less that \(\epsilon/\sqrt{n}\) whenever \(a\equiv a(\epsilon,F_{X/\sigma})\) is chosen large enough. Fix this \(a\) in the limits of integration of (r), and then break this integral into two pieces: the integral over \([|t|\leq\theta\sqrt{n}/{\rm E}|X|^{3}]\) with \(\theta\) as in (i), and the integral over \([|t|>\theta\sqrt{n}/{\rm E}|X|^{3}]\). The integral over the set \([|t|>\theta\sqrt{n}/{\rm E}|X|^{3}]\) is \(o(n^{-r})\), for any \(r>0\) (a la (11.5.6), as before at line (e)). Finally, the value of the integral over the set \([|t|\leq\theta\sqrt{n}/{\rm E}|X|^{3}]\) is bounded by a term like the right-hand side of (j) (in which \(|t|^{3}\) and \(t^{6}\) are replaced in those integrals by \(t^{2}\) and \(|t|^{5}\), to account for division by \(|t|\) in the integrand of (r)). This completes the proof of (17) when \(X\) is not distributed on a grid. For (18), the initial region of integration in (r) must be \([|t|\leq an/{\rm E}X^{4}]\), and then \(an/{\rm E}X^{4}\) will also appear below the norm term. Moreover, we will now use \(\theta\) for \(a\), since only \(O(1/n)\) is required.

Consider theorem 6.2(b). We note that

(s) \[D_{n}(\cdot)\equiv F_{n}(\cdot)-F_{0}(\cdot)+f_{0}(\cdot)\left\{\frac{1}{\sqrt {n}}\frac{\gamma_{1}}{3!}H_{2}(\cdot)+\frac{1}{n}\left[\frac{\gamma_{2}}{4!}H _{3}(\cdot)+\frac{\gamma_{1}^{2}}{2\cdot(3!)^{2}}H_{5}(\cdot)\right]\right\}\]

has derivative

(t) \[D^{\prime}_{n}(x)=d_{n}\] (u) \[=f_{n}(x)-f_{0}(x)+f_{0}(x)\left[\frac{1}{\sqrt{n}}\frac{\gamma_{1 }}{3!}H_{3}(x)\right]+\frac{1}{n}\left[\frac{\gamma_{2}}{4!}H_{4}(x)+\frac{ \gamma_{1}^{2}}{2(3!)^{2}}H_{6}(x)\right]\right\},\]

and this is the same \(d_{n}\) as in (m) of the previous proof. Thus the final details are nearly the same as before. \(\Box\)

**Exercise 6.4**  Complete the details in the previous proof of theorem 6.2(b).

**Exercise 6.5**  Consider a non-iid case in which all dfs \(F_{nk}\) have third and/or fourth moments that are of the same order. Then all of the previous results still obtain.

**Exercise 6.6** (Large deviations) Suppose the _moment generating function_ (or \(mgf\)) \(M_{X}(t)\equiv{\rm E}e^{tX}\) of the rv \(X\) is finite for \(0\leq|t|<\epsilon\). Let \(X_{1},X_{2},\ldots\) be iid \((0,\sigma^{2})\). Let \(F_{n}(\cdot)\) denote the df of \(\sqrt{n}(\bar{X}_{n}-\mu)\) and let \(F_{0}(\cdot)\) denote the \(N(0,1)\) df. Show that

\[[1-F_{n}(x_{n})]/[1-F_{0}(x_{n})]\to 1,\qquad\mbox{provided that}\qquad x_{n}=o(n^{1 /6}). \tag{22}\]

## Chapter 12 Brownian Motion and Empirical Processes

### 1 Special Spaces

#### General Metric Spaces

Let \((M,d)\) denote an arbitrary metric space and let \({\cal M}_{d}\) denote its _Borel_\(\sigma\)-field (that is, the \(\sigma\)-field generated by the collection of all \(d\)-open subsets of \(M\)). Let \({\cal M}_{d}^{B}\) denote the \(\sigma\)-field generated by the collection of all open balls, where a _ball_ is a subset of \(M\) of the form \(\{y\;:\;d(y,x)<r\}\) for some \(x\in M\) and some \(r>0\); call this the _Baire \(\sigma\)-field_. [The important concept of weak convergence is best described in the context of metric spaces.]

**Exercise 1.1** Now, \({\cal M}_{d}^{B}\subset{\cal M}_{d}\), while

\[{\cal M}_{d}^{B}={\cal M}_{d}\quad\mbox{if $(M,d)$}\quad\mbox{is a separable metric space.} \tag{1}\]

#### The Special Spaces \((C,{\cal C})\) and \((D,{\cal D})\)

For functions \(x,y\) on \([0,1]\), define the _uniform metric_ (or _supremum metric_) by

\[\|x-y\|\equiv\sup_{0\leq t\leq 1}|x(t)-y(t)|. \tag{2}\]

Let \(C\) denote the set of all continuous functions on \([0,1]\). Then

\[(C,\|\;\|)\quad\mbox{is a complete and separable metric space.} \tag{3}\]

Here \({\cal C}_{\|\;\|}\) will denote the \(\sigma\)-field of Borel subsets of \(C\); then \({\cal C}_{\|\;\|}^{B}\) will denote the \(\sigma\)-field of subsets of \(C\) generated by the open balls, and \({\cal C}\) will denote the \(\sigma\)-field generated by the finite-dimensional subsets of \(C\)(that is,all \(\pi_{\rm t}^{-1}(B_{k})\) for which \(0\leq t_{1}\leq\cdots\leq t_{k}\leq 1\) and \(B_{k}\in{\cal B}_{k}\)). It can be shown that

\[{\cal C}_{\|\;\|}={\cal C}_{\|\;\|}^{B}={\cal C}. \tag{4}\]

Let \(D\) denote the set of all functions on \([0,1]\) that are right continuous and possess left-hand limits at each point. (In some applications below it will be noted that \(D\) is also used to denote the set of all left-continuous functions on \([0,1]\) that have right-hand limits at each point. This point will receive no further mention. In some cases we will admit to \(D\), and/or to \(C\), onlyfunctions \(X\) having \(X(0)=0\), etc. This, too, will receive little, if any, further mention.) In any case

\[(D,\parallel\parallel)\quad\mbox{is a complete metric space that is not separable.} \tag{5}\]

Here \({\cal D}_{\parallel\parallel}\) will denote the Borel \(\sigma\)-field of subsets of \(D\), then \({\cal D}^{B}_{\parallel\parallel}\) will denote the \(\sigma\)-field of subsets of \(D\) generated by the open balls, and \({\cal D}\) will denote the \(\sigma\)-field generated by the finite-dimensional subsets of \(D\). It can be shown that

\[{\cal D}={\cal D}^{B}_{\parallel\parallel},\quad\mbox{and both are proper subsets of }{\cal D}_{\parallel\parallel}, \tag{6}\]

and moreover,

\[C\in{\cal D}\quad\mbox{and}\quad{\cal C}=C\cap{\cal D}. \tag{7}\]

We now digress briefly. The proper set inclusion of (6) caused difficulties in the historical development of the theory of empirical processes (note that the uniform empirical process \({\mathbb{U}}_{n}=\sqrt{n}({\mathbb{G}}_{n}-I)\) takes values in \(D\)). To circumvent these difficulties, various authors showed that it is possible to define a metric \(d\) on \(D\) that has nice properties (see exercise 1.4 below); thus there is a \(d(\cdot,\cdot)\) for which

\[(D,d)\mbox{ is a complete and separable metric space} \tag{8}\]

whose Borel \(\sigma\)-field \({\cal D}_{d}\) satisfies

\[{\cal D}_{d}={\cal D}. \tag{9}\]

Moreover, for all \(x,x_{n}\) in \(D\) the metric \(d\) satisfies

\[\|x_{n}-x\|\to 0\quad\mbox{implies}\quad d(x_{n},x)\to 0, \tag{10}\]

while

\[d(x_{n},x)\to 0\quad\mbox{with}\quad x\in C\quad\mbox{implies}\quad\|x_{n}-x\| \to 0. \tag{11}\]

The metric \(d\) will not be important to us. We are able to replace \(d\) by \(\parallel\parallel\) in our theorems; however, we include some information on \(d\) as an aid to the reader who wishes to consult the original literature.

**Exercise 1.2** Verify (3) and (4).

**Exercise 1.3** (i) Verify (5). [Hint. For each \(0\leq t\leq 1\) define a function \(x_{t}\) in \(D\) by letting \(x_{t}(s)\) equal \(0\) or \(1\) according as \(0\leq s\leq t\) or \(t\leq s\leq 1\).] (ii) Verify (6). [Hint. Consider \(\cup\{O_{t}\ :\ 0\leq t\leq 1\}\) where \(O_{t}\) is the open ball of radius \(\frac{1}{3}\) centered at \(x_{t}\).] (iii) Verify (7).

**Exercise 1.4\({}^{*}\)** Consult Billingsley (1968, pp. 112-115), and verify (8)-(11) for

\[d(x,\ y)\equiv\inf\{\|x-y\circ\lambda\|\vee(\sup_{s\neq t}|\log\frac{\lambda(t )-\lambda(s)}{t-s}|)\ :\ \lambda\in\Lambda\}, \tag{12}\]

where \(\Lambda\) denotes all \(\uparrow\) continuous maps of \([0,1]\) onto itself. [Roughly, this metric measures how closely \(x\) and a slightly perturbed (via \(\lambda\)) \(y\) line up, where too much perturbation is penalized. The ("log" bounds all \(\lambda\)-slopes away from both \(0\) and \(\infty\).]

**Exercise 1.5** Verify that

(13) \(C\) is both \(\|\)\(\|\)-separable and \(d\)-separable, viewed as a subset of \(D\).

[We will require the \(\|\)\(\|\)-separability below.]

Let \(q\geq 0\) be positive on \((0,1)\). For functions \(x,y\) on \([0,1]\) we agree that

(14) \(\|(x-y)/q\|\) is the \(\|\cdot/q\|\)-distance between \(x\,\)and\(\,y\),

when this is well-defined (that is, when \(\|x/q\|\) and \(\|y/q\|\) are finite).

**Exercise 1.6** It is useful to be able to view \(C_{\infty}\equiv C_{[0,\infty)}\) as a metric space; of course, this denotes the class of all continuous functions on \([0,\infty)\). (We may sometimes require a subclass, such as the one consisting of functions that equal zero at zero; and we will make no further mention of this.) Let \({\cal C}_{\infty}\equiv{\cal C}_{[0,\infty)}\) denote the finite-dimensional \(\sigma\)-field. Consider \((C_{\infty},{\cal C}_{\infty})=(C_{[0,\infty)}{\cal C}_{[0,\infty)})\).

(a) For functions \(x\) and \(y\) on \([0,\infty)\), define

(15) \(\rho_{\infty}(x,y)\equiv\sum_{k=1}^{\infty}2^{-k}\,\frac{\rho_{k}(x,y)}{1+\rho _{k}(x,y)}\),

where \(\rho_{k}(x,y)\equiv\sup_{0\leq t\leq k}|x(t)-y(t)|\). Show that \((C_{[0,\infty)},\rho_{\infty})\) is a metric space.

(b) Show that \(\rho_{\infty}(x,y)\to 0\) if and only if \(\rho_{k}(x,y)\to 0\) for each \(0<k<\infty\).

(c) Show that \((C_{[0,\infty)},\rho_{\infty})\) is a complete and separable metric space. Moreover, the \(\sigma\)-field \({\cal C}_{\rho_{\infty}}\) of Borel subsets is the same as the \(\sigma\)-field \({\cal C}_{[0,\infty)}\) of finite-dimensional subsets, as is the \(\sigma\)-field \({\cal C}_{\infty}^{B}\).

(d) Verify that \((D_{[0,\infty)},\rho_{\infty})\) is a complete metric space, and that the Borel \(\sigma\)-field \({\cal D}_{\rho_{\infty}}\) satisfies \(C_{[0,\infty)}\in{\cal D}_{\rho_{\infty}}\) and \({\cal C}_{\rho_{\infty}}={\cal D}_{\rho_{\infty}}\cap C_{[0,\infty)}\). Also, \({\cal D}_{[0,\infty)}={\cal D}_{\rho_{\infty}}^{B}\) is a proper subset of \({\cal D}_{\rho_{\infty}}\).

(e) Other spaces of continuous and right-continuous functions are analogously treated. They will receive no specific mention.

**Independent Increments and Stationarity**

If \(T\) is an interval in \((-\infty,\infty)\), then we will write

(16) \({\mathbb{X}}(s,t]\equiv{\mathbb{X}}(t)-{\mathbb{X}}(s)\) for any \(s,t\in T\),

and we will refer to this as an _increment_ of \({\mathbb{X}}\). If \({\mathbb{X}}(t_{0}),{\mathbb{X}}(t_{0},t_{1}],\ \ldots,{\mathbb{X}}(t_{k-1},t_{k}]\) are independent rvs for all \(k\geq 1\) and all \(t_{0}\leq\cdots\leq t_{k}\) in \(T\), then we say that \({\mathbb{X}}\) has _independent increments_. If \({\mathbb{X}}(s,t]\cong{\mathbb{X}}(s+h,t+h]\) for all \(s,t,s+h,t+h\) in \(T\) with \(h\geq 0\), then \({\mathbb{X}}\) is said to have _stationary increments_. If \(({\mathbb{X}}(t_{1}+h),\ldots,{\mathbb{X}}(t_{k}+h))\cong({\mathbb{X}}(t_{1}),\ldots,{\mathbb{X}}(t_{k}))\) for all \(k\geq 1,h\geq 0\), and all time points in \(T\), then \({\mathbb{X}}\) is said to be _a stationary process_.

## 2 Existence of Processes on \((C,\mathcal{C})\) and \((D,\mathcal{D})\)

When dealing with processes, we would like to work with the smoothest version possible. This is the version that best models physical reality. It is important at this point to recall theorem 5.4.2 on the existence of smoother versions of processes. Roughly, if all of the sample paths of a process are shown to lie in a (useful) subset of the current image space, then we can restrict ourselves to that subset.

**Theorem 2.1** (Existence of processes on \((\,\boldsymbol{C},\mathcal{C})\)): Begin with a process

\[\mathbb{X}:(\Omega,\mathcal{A},P)\to(R_{[0,1]},\mathcal{B}_{[0\,,\,1]}P_{X}).\]

Suppose that for some \(a,b>0\) the increments of \(\mathbb{X}\) satisfy

\[\mathrm{E}|\mathbb{X}(s,t]|^{b}\leq K\cdot F(s,t]^{1+a}\quad\text{for all }0 \leq s,t\leq 1, \tag{1}\]

where \(F\) is a continuous df concentrated on \([0,1]\) and \(F(s,t]\ \equiv F(t)-F(s)\). Then there exists an equivalent version \(\mathbb{Z}:(\Omega,\mathcal{A},P)\to(R_{[0,1]},\mathcal{B}_{[0\,,\,1]}P_{Z})\) for which

\[\mathbb{Z}:(\Omega,\mathcal{A},P)\to(C,\mathcal{C},P_{Z}),\quad\text{with} \quad\mathbb{Z}(t)=\mathbb{X}(t)\,\text{a.s.},\quad\text{for each }t\text{ in }[0,1]. \tag{2}\]

**Corollary 1** (Sample path properties): For any \(0<\delta<a/b\) and any \(\epsilon>0\), there exists a constant \(K_{\epsilon}\equiv K_{\epsilon,\delta,a,b}\) for which the process \(\mathbb{Z}\) of (2) satisfies

\[P(|\mathbb{Z}(s,t]|\leq K_{\epsilon}\cdot F(s,t]^{\delta}\text{ for all }0 \leq s\leq t\leq 1)\ \geq 1-\epsilon. \tag{3}\]

**Proof.** Case 1. Suppose that the df \(F\) of (1) is \(F(t)=t\) on \([0,1]\). Let \(0<\delta<a/b\) be fixed. Let \(\lambda\equiv(a/b-\delta)/2\). Define \(t_{ni}\equiv i/2^{n}\) for \(0\leq i\leq 2^{n}\quad\text{and}\quad n\geq 1\). For \(n\geq 0\) define processes \(\mathbb{Z}_{n}:\,(\Omega,\mathcal{A},P)\to(C,\mathcal{C})\) by letting

(a) \[\mathbb{Z}_{n}(t)\equiv X(t_{ni})+2^{n}(t-t_{ni})[\mathbb{X}(t_{n,i+1})- \mathbb{X}(t_{ni})]\qquad\text{for }t_{ni}\leq t\leq t_{n,i+1},\]

for each \(0\leq i\leq 2^{n}-1\); thus \(\mathbb{Z}_{n}(\cdot)\) equals \(\mathbb{X}_{n}(\cdot)\) at each \(t_{ni}\) and \(\mathbb{Z}_{n}(\cdot)\) is linear on the intervals between these points. Define

(b) \[U_{ni}\equiv|\mathbb{X}(t_{n,i-1},t_{ni}]|\quad\text{ for }1\leq i\leq 2^{n}.\]

If we define

(c) \[\Delta_{n}(t)\equiv\mathbb{Z}_{n}(t)-\mathbb{Z}_{n-1}(t)\quad\text{ for }0\leq t\leq 1,\]

then for \(t_{n-1,i}\leq t\leq t_{n-1,i+1}\) we have

(d) \[=|\mathbb{X}(t_{n,2i},t_{n,2i+1}]-\mathbb{X}(t_{n,2i+1},t_{n,2i+2} ]|/2\leq[U_{n,2i+1}+U_{n,2i+2}]/2\] (e) \[\leq[U_{n,2i+1}\lor U_{n,2i+2}]\]

for all \(n\geq 1\). Thus for all \(n\geq 1\) we have

(f) \[\|\Delta_{n}\|\leq V_{n}\equiv[\max_{1\leq i\leq 2^{n}}U_{ni}].\]Let \(\theta>0\) be arbitrary but fixed, and define

\[p_{n} \equiv P(\|\Delta_{n}\|_{\delta}>2\,\theta\,2^{-n\lambda})\] \[\equiv P(|\Delta_{n}(s,t]|/(t-s)^{\delta}>2\,\theta\,2^{-n\lambda} \mbox{ for some }0\leq s\leq t\leq 1).\]

Recalling (f) shows that

\[|\Delta_{n}(s,t]|\leq 2V_{n}\quad\mbox{ for all }0\leq s\leq t\leq 1.\]

Thus

\[|\Delta_{n}(s,t]|/(t-s)^{\delta}\leq 2V_{n}2^{n\delta}\quad\mbox{ for }2^{-n} \leq t-s,\]

while

\[|\Delta_{n}(s,t]|/(t-s)^{\delta}\leq[|\Delta_{n}(s,t]|/(t-s)|(t-s)^{1-\delta} \leq[V_{n}2^{n}]2^{-n(1-\delta)}\] \[=V_{n}2^{n\delta}\qquad\mbox{ for}\quad 0\leq t-s\leq 2^{-n}\]

(to see this, consider \(|\Delta_{n}(s,t]/(t-s)|\) when \(s\) and \(t\) are both points in some \([t_{n,i-1},t_{ni}]\)). Thus for all \(n\geq 1\), we have

\[p_{n} \leq P(2V_{n}2^{n\delta}>2\,\theta\,2^{-n\lambda})\leq P(V_{n}> \theta\,2^{-n(\delta+\lambda)})\quad\mbox{ by (g)}\] \[\leq\sum_{i=1}^{2^{n}}P(U_{ni}>\theta\,2^{-n(\delta+\lambda)}) \qquad\mbox{ by (f)}\] \[\leq\sum_{i=1}^{2^{n}}\mathds{E}U_{ni}^{b}/[\theta\,2^{-n(\delta+ \lambda)}]^{b}\qquad\mbox{ by Markov's inequality}\] \[\leq 2^{n}[K2^{-n(1+a)}]/[\theta\,2^{-n(\delta+\lambda)}]^{b} \qquad\mbox{ by (1)}\] \[=K\theta^{-b}2^{-n(a-b(\delta+\lambda))}=K\theta^{-b}2^{-nb(a/b- \delta-\lambda)}\] \[=K\theta^{-b}2^{-n\lambda b}\qquad\mbox{since }a/b-\delta=2\lambda>0.\]

Since \(0\leq t-s\leq 1\), we also have

\[p_{0} \equiv P(|\mathbb{Z}_{0}(s,t]|>2\theta(t-s)^{\delta}\quad\mbox{ for some }0\leq s\leq t\leq 1)\] \[\leq P(|\mathbb{X}(0,1]|>2\theta)\leq\mathds{E}|\mathbb{X}(0,1]|^ {b}/(2\theta)^{b}\] \[\leq K\theta^{-b}=K\theta^{-b}2^{-0\cdot\lambda b}.\]

Now, \(\lambda b=(a-\delta b)/2>0\), and so \(2^{-\lambda b}<1\); hence \(\sum_{n=0}^{\infty}p_{n}<\infty\). Thus for each arbitrarily small \(\theta\), we have for \(m\) sufficiently large (recall (g) for \(\|\cdot\|_{\delta}\)) that

\[P(\max_{m\leq k<\infty}\|\mathbb{Z}_{k}-\mathbb{Z}_{m}\|_{\delta }>\theta)=\lim_{n\to\infty}P(\max_{m\leq k\leq n}\|\mathbb{Z}_{k}-\mathbb{Z}_{ m}\|_{\delta}>\theta)\] \[\leq\lim_{n\to\infty}P(\max_{m\leq k\leq n}\|\mathbb{Z}_{k}- \mathbb{Z}_{m}\|_{\delta}>2\theta\,\mbox{$\sum_{m+1}^{n}2^{-k\lambda}$})\] \[\quad\mbox{ for }\sum\nolimits_{m+1}^{\infty}2^{-k\lambda}<1/2\] \[\leq\lim_{n\to\infty}P(\max_{m\leq k\leq n}\|\sum\nolimits_{i=m+ 1}^{k}\Delta_{i}\|_{\delta}>2\theta\,\mbox{$\sum_{i=m+1}^{n}2^{-i\lambda}$})\] \[\leq\lim_{n\to\infty}\sum\nolimits_{i=m+1}^{n}P(\|\Delta_{i}\|_{ \delta}>2\theta 2^{-i\lambda})\] \[\leq\sum\nolimits_{i=m+1}^{\infty}p_{i}\leq\sum\nolimits_{i=m+1}^ {\infty}K\theta^{-b}2^{-i\lambda b}\to 0\qquad\mbox{as }m\to\infty,\]so that

(n) \[\mathbb{Z}_{n}(t)\equiv\mathbb{Z}_{0}(t)+\sum\nolimits_{k=1}^{n}\Delta_{k}(t)\]

converges uniformly on \([0,1]\) for a.e. \(\omega\); call the limit function \(\mathbb{Z}(t)\). Since the uniform limit of continuous functions is continuous,

(o) \[\mathbb{Z}=\sum\nolimits_{n=0}^{\infty}\Delta_{n}=\lim\mathbb{Z}_{n}\qquad\mbox { is a continuous function on $[0,1]$ for a.e.\,$}\omega.\]

Now, \(\mathbb{Z}=\lim\mathbb{Z}_{n}\), and since \(\mathbb{Z}_{n}\) equals \(\mathbb{X}\) at each \(t_{ni}\), we have

\[\mathbb{Z}(t_{ni})=\mathbb{X}(t_{ni})\qquad\mbox{at each $t_{ni}=i/2^{n}$}\quad \mbox{with $0\leq i\leq 2^{n}$}\quad\mbox{and}\quad n\geq 0. \tag{4}\]

Thus all finite-dimensional distributions with diadic rational coordinates are equal. For other \(t\), we pick diadic rationals \(t_{1},t_{2},\ldots\) such that \(t_{m}\to t\). Then \(\mathbb{X}(t_{m})\to_{p}\mathbb{X}(t)\) as \(m\to\infty\) by (1) and Markov, while \(\mathbb{Z}(t_{m})\to_{a.s.}\mathbb{Z}(t)\) as \(m\to\infty\), since \(\mathbb{Z}\) has continuous sample paths. Thus \(\mathbb{Z}(t)=\mathbb{X}(t)\) a.s. by proposition 2.3.4. By redefining \(\mathbb{Z}\equiv 0\) on the null set of (n), we may assume

(p) \[\mathbb{Z}:(\Omega,\mathcal{A})\to(C,\mathcal{C})\]

by theorem 5.4.2. So all of the finite-dimensional distributions agree: In particular, we have \(P_{Z}([x\in C\ :\ x_{\rm t}\in B])=P_{X}([x\in R_{[0,1]}\ :\ x_{\rm t}\in B])\) for all sets \(B\in\mathcal{B}_{k}\) and for all \({\rm t}\in[0,1]^{k}\) for any \(k\geq 1\).

Case 2. General \(F\). Define

(q) \[\mathbb{Y}(t)\equiv\mathbb{X}(F^{-1}(t))\qquad\mbox{for $0\leq t\leq 1$},\]

where \(F^{-1}(t)\equiv\inf\{x\in[0,1]\ :\ F(x)\geq t\}\). Then for \(0\leq s\leq t\leq 1\),

(r) \[\begin{split}&\mbox{E}|\mathbb{Y}(s,t]|^{b}=\mbox{E}|\mathbb{X}(F^{-1} (s),F^{-1}(t)|^{b}\leq K[F\circ F^{-1}(t)-F\circ F^{-1}(s)]^{1+a}\\ &\qquad=K(t-s)^{1+a},\end{split}\]

since \(F\circ F^{-1}=I\) for continuous \(F\) by exercise 6.3.2. Now use case 1 to replace \(Y\) by an equivalent process \(\bar{\mathbb{Y}}:(\Omega,\mathcal{A})\to(C,\mathcal{C})\). Then define

(s) \[\mathbb{Z}=\bar{\mathbb{Y}}(F)\cong\mathbb{X}(F^{-1}\circ F)\qquad\mbox{by (q)}.\]

Now, \(F^{-1}\circ F(t)=t\), unless \(F(t-\epsilon)=F(t)\) for some \(\epsilon>0\); see exercise 6.3.2. But in this case equation (1) shows that \(\Delta\mathbb{X}\) is \(0\) across that same interval. Thus \(\mathbb{X}(F^{-1}\circ F)\cong\mathbb{X}\).

For the corollary, in case 1 we have (using (o) in line 2, (g) and (k) in line 3)

\[\begin{split}& P(|\mathbb{Z}(s,t]|/(t-s)^{\delta}>2\theta/(1-2^{- \lambda})\quad\mbox{for some $0\leq s\leq t\leq 1$})\\ &\quad=P(|\sum\nolimits_{k=0}^{\infty}\Delta_{k}(s,t]|/(t-s)^{ \delta}>2\theta\sum\nolimits_{n=0}^{\infty}2^{-n\lambda}\quad\mbox{for some $0\leq s\leq t\leq 1$})\\ &\quad\leq\sum\nolimits_{n=0}^{\infty}p_{n}\leq\sum\nolimits_{n=0} ^{\infty}K\theta^{-b}/2^{n\lambda b}=K\theta^{-b}/(1-2^{-\lambda b})\\ &\quad\to 0\qquad\mbox{as $\theta\to\infty$}.\end{split}\]

Take \(K_{\epsilon}\) to be an appropriately large value of \(\theta\). Use the transformation \(F^{-1}\) again in case 2.

**Exercise 2.1**  Prove (2), by simplifying the proof of theorem 2.1 as much as possible with this simpler goal in mind.

We merely state an analogous result for the existence of processes on \((D,{\cal D})\).

**Theorem 2.2** (**Existence of processes on \((D,{\cal D})\); Chentsov)** Let

\[{\mathbb{X}}:(\Omega,{\cal A},P)\to(R_{[0,1]},{\cal B}_{[0,\,1]}P_{X})\]

be a general process. Suppose that for some \(K>0,b>0\), and \(a>\frac{1}{2}\) we have

\[{\rm E}|{\mathbb{X}}(r,s]{\mathbb{X}}(s,t]|^{b}\leq K\cdot F(r,s]^{a}F(s,t]^{a} \quad\mbox{for all }0\leq r\leq s\leq t\leq 1, \tag{5}\]

where \(F\) is any df concentrated on \([0,1]\). Then there exists an equivalent version \({\mathbb{Z}}:(\Omega,{\cal A},P)\to(R_{[0,1]},{\cal B}_{[0,\,1]}P_{Z})\), which in fact satisfies

\[{\mathbb{Z}}:(\Omega,{\cal A},P)\to(D,{\cal D},P_{Z}),\quad\mbox{with}\quad{ \mathbb{Z}}(t)={\mathbb{X}}(t)\,\mbox{a.s.},\quad\mbox{for each }t\in[0,1]. \tag{6}\]

[See Billingsley (1968, pp. 130, 134), for example.]

**Exercise 2.2**  Verify the existence of the Poisson process on \((D,{\cal D})\).

**Exercise 2.3**  Let \({\mathbb{X}}:\,(\Omega,{\cal A},P)\to(R_{T},{\cal B}_{T})\) on some subinterval \(T\) of the line. Let \(T_{o}\) denote a countable dense subset of \(T\), and suppose \(P(\omega\;:\;{\mathbb{X}}(\cdot,\omega)\) is uniformly continuous on \(T_{o}\cap I)=1\) for every finite interval subinterval \(I\) of \(T\). Then there exists a version \({\mathbb{Z}}\) of \({\mathbb{X}}\) such that every sample path \({\mathbb{Z}}(\cdot,\omega)\) of \({\mathbb{Z}}\) is continuous.

## 3 Brownian Motion and Brownian Bridge

**Brownian Motion \(\mathbb{S}\) on \([0,1]\)**  We define \(\{\mathbb{S}(t)\ :\ 0\leq t\leq 1\}\) to be a _Brownian motion_ on \([0,1]\) if \(\mathbb{S}\) is a normal process having the moment functions

\[\mbox{E}\mathbb{S}(t)=0\quad\mbox{and}\quad\mbox{Cov}[\mathbb{S}(s),\mathbb{S} (t)]=s\wedge t\quad\mbox{for all}\quad 0\leq s,t\leq 1. \tag{1}\]

This covariance function is nonnegative definite (in the sense of (A.4.12)) and these distributions are consistent; thus Kolmogorov's consistency theorem shows that the process \(\mathbb{S}\) exists as a random element on \((R_{[0,1]},\mathcal{B}_{[0,1]})\). Modifying this \(\mathbb{S}\) on a set of measure zero (as in theorem 12.2.1), we may may create a version of \(\mathbb{S}\) that satisfies

(2) all sample paths of \(\mathbb{S}\) are continuous functions on \([0,1]\) that equal \(0\) at \(0\).

Thus (as with the smoother realizations of theorem 5.4.2) there is a nice realization of \(\mathbb{S}\) having smoother paths; that is,

\[\mathbb{S} \tag{3}\]

 exists as a process on

\[(C,\mathcal{C})\]

So, Brownian motion exists as the coordinate map \(S_{t}(\omega)\equiv\omega_{t}\) for some distribution \(P\) on \((\Omega,\mathcal{A})=(C,\mathcal{C})\). This is a more convenient realization of \(\mathbb{S}\) (than is the one guaranteed by Kolmogorov's consistency theorem). For either realization

\[\mathbb{S} \tag{4}\]

 has stationary and independent increments.

In fact, its sample paths satisfy

\[P(|\mathbb{S}(s,t]|\leq K_{\epsilon}(t-s)^{\delta}\quad\mbox{for all }0\leq s \leq t\leq 1)\geq 1-\epsilon \tag{5}\]

for some \(K_{\epsilon,\delta}\), for any fixed \(\epsilon>0\) and for any fixed \(0<\delta<\frac{1}{2}\). This follows from theorem 12.2.1 and its corollary, since for any \(k\geq 1\),

\[\mbox{E}\mathbb{S}(s,t]^{2k}=[1\cdot 3\cdots(2k-1)](t-s)^{k}\qquad\mbox{for all }0\leq s\leq t\leq 1, \tag{6}\]

and since \(a/b=(k-1)/(2k)\nearrow\frac{1}{2}\) as \(k\to\infty\). (Note that (5) would allow a further application of the smoother realizations theorem using just this smaller subset of such functions in \(\mathcal{C}\).) [No appeal has been made to section 1.]

**Brownian Bridge \(\mathbb{U}\) on \([0,1]\)**  Let us now define

\[\mathbb{V}(t)\equiv\mathbb{S}(t)-t\mathbb{S}(1)\qquad\mbox{and}\qquad\mathbb{ U}(t)\equiv-\mathbb{V}(t)\qquad\mbox{for }0\leq t\leq 1. \tag{7}\]

Then both \(\mathbb{U}\) and \(\mathbb{V}\) are obviously normal processes on \((C,\mathcal{C})\) and satisfy (5); just observe that \(\mathbb{V}(t)\) is a simple linear combination of two normal rvs. Moreover, trivial calculations give

\[\mbox{E}\mathbb{U}(t)=0\qquad\mbox{and}\qquad\mbox{Cov}[\mathbb{U}(s),\mathbb{ U}(t)]=s\wedge t-st\qquad\mbox{for all }0\leq s,t\leq 1. \tag{8}\]

Call \(\mathbb{U}\) a _Brownian bridge_. And \(\mathbb{V}\) is also a Brownian bridge.

**Brownian Motion \(\mathbb{S}\) on \([0,\infty)\)**  Similarly, we establish the existence of Brownian motion on \((C_{\infty},\mathcal{C}_{\infty})\). In particular, a Brownian motion on \((C_{\infty},\mathcal{C}_{\infty})\) is given by

\[\mathbb{S}(t)=(1+t)\mathbb{U}(t/(1+t)),\quad 0\leq t<\infty. \tag{9}\]

Recall the proposition 8.6.1 LIL result. In section 12.8 we will establish the companion LIL result for Brownian motion that

\[\limsup_{t\to\infty}|\mathbb{S}(t)|/[\sqrt{t}\,b(t)]=1\,\mbox{a.s.}\qquad\mbox {(the LIL for $\mathbb{S}$ at infinity)}, \tag{10}\]where \(b(t)\equiv\sqrt{2(1\vee\log\log t)}\)). (We will use it in very minor ways in the meantime.) The next exercise similarly defines some additional normal processes. These may provide a useful revisualization device that enables calculation.

**Exercise 3.1** (Transformations of Brownian motion): Let \(Z\cong\mathrm{N}(0,1)\) and the Brownian bridges \(\mathbb{V}\), \(\mathbb{U}^{(1)}\), and \(\mathbb{U}^{(2)}\) be independent. Fix \(a>0\). Show that:

\[\mathbb{S}(t)=\mathbb{V}(t)+tZ,\quad 0\leq t\leq 1,\qquad\mbox{is a Brownian motion.} \tag{11}\]

\[\mathbb{S}(at)/\sqrt{a},\quad\ 0\leq t<\infty,\qquad\qquad\qquad\mbox{is a Brownian motion.} \tag{12}\]

\[\mathbb{S}(a+t)-\mathbb{S}(a)\,\quad t\geq 0,\qquad\qquad\qquad\mbox{is a Brownian motion.} \tag{13}\]

\[\sqrt{1-a}\,\mathbb{U}^{(1)}\pm\sqrt{a}\,\mathbb{U}^{(2)}\qquad\qquad\qquad \mbox{is a Brownian bridge,}\qquad\mbox{if}\quad 0\leq a\leq 1. \tag{14}\]

\[\mathbb{Z}(t)\equiv[\mathbb{U}^{(1)}(t)+\mathbb{U}^{(2)}(1-t)]/\sqrt{2},\quad 0 \leq t\leq\tfrac{1}{2},\qquad\qquad\mbox{is a Brownian bridge.} \tag{15}\]

\[\mathbb{U}(t)=(1-t)\mathbb{S}(t/(1-t)),\quad 0\leq t\leq 1,\qquad\qquad\qquad\qquad \mbox{is a Brownian bridge;} \tag{16}\]

use the LIL at infinity of (10) to show that this \(\mathbb{U}(\cdot)\) converges to \(0\) at \(t=1\).

\[t\,\mathbb{S}(1/t),\quad 0\leq t<\infty,\qquad\qquad\mbox{is a Brownian motion;} \tag{17}\]

apply the LIL of (10) to verify that these sample paths converge to \(0\) at \(t=0\).

**Exercise 3.2** (LIL for \(\mathbb{S}\) and \(\mathbb{U}\) at \(0\)) Use (10), (17), and then (7) to show that

\[\limsup_{t\to 0}|\mathbb{S}(t)|/[\sqrt{t}\,b(1/t)]=1\,\mbox{a.s.};\quad\mbox{and} \quad\limsup_{t\to 0}|\mathbb{U}(t)|/[\sqrt{t}\,b(1/t)]=1\,\mbox{a.s.} \tag{18}\]

**Exercise 3.3** (Integrals of normal processes are normal rvs):

(a) Suppose \(\mathbb{X}\) is a normal process on \((C,\mathcal{C})\). Let \(\mathbb{X}\) have mean function \(m(\cdot)\) continuous on \(I\equiv[0,1]\) and covariance function \(\mathrm{Cov}(\cdot,\cdot)\) continuous on \(I\times I\). Let \(g(\cdot)\geq 0\) on \(I\quad\mbox{and}\quad q>0\) on \(I\) both be continuous on \(I\). Let \(K(\cdot)\) be an \(\nearrow\) and left continuous function for which \(\int_{0}^{1}q|g|dK<\infty\). Show that the integrated process

\[\int_{0}^{1}\mathbb{X}(t)g(t)dK(t)\] \[\qquad\cong N(\int_{0}^{1}m(t)g(t)dK(t),\int_{0}^{1}\int_{0}^{1} \mathrm{Cov}(s,t)g(s)g(t)dK(s)dK(t))\,, \tag{19}\]

provided that both \(m(s)/q(s)\quad\mbox{and}\quad\mathrm{Cov}[s,s]/q^{2}(s)\) are continuous for \(s\in I\).

(b) Determine the distribution of \(\int_{0}^{1}\mathbb{U}(t)dt\).

(c) Develop results for \(\int_{0}^{1}\mathbb{S}\,g\,d\,K\), for appropriate functions \(g\quad\mbox{and}\quad K\).

[Hint. (a) The Riemann-Stieltjes sums are normally distributed.]

**Exercise 3.4**  Let \(Z_{0},Z_{1},Z_{2},\ldots\) be iid \(N(0,1)\). Let \(f_{j}(t)\equiv\sqrt{2}\sin(j\pi t)\), for \(j\geq 1\); these are orthogonal functions. Verify that

\[\mathbb{U}(t)\equiv\sum_{j=1}^{\infty}Z_{j}f_{j}(t)/j\pi,\quad 0\leq t\leq 1,\quad \mbox{is a Brownian bridge.} \tag{20}\]

Thus the process \(\mathbb{S}(t)\equiv\mathbb{U}(t)+t\,Z_{0}\) is a Brownian motion on \([0,1]\). Moreover,

\[W^{2}\equiv\int_{0}^{1}\mathbb{U}^{2}(t)dt\cong\sum_{j=1}^{\infty}Z_{j}^{2}/(j ^{2}\pi^{2}). \tag{21}\]

This rv has a distribution that is well tabled (the asymptotic null distribution of the Cramer-von Mises statistic).

**Exercise 3.5**  Show that \(\mathbb{Z}\) is a Brownian motion on \([0,1]\), where

\[\mathbb{Z}(t)\equiv\mathbb{U}(t)+\int_{0}^{t}[\mathbb{U}(s)/(1-s)]ds\quad \mbox{for }0\leq t\leq 1. \tag{22}\]

Hint. Since the Reimann sums of normal rvs that define the integral are necessarily normal, the process \(\{\mathbb{Z}(t)\ :\ 0\leq t\leq 1\}\) will be a normal process. Then, its mean and covariance function will determine which normal process.

**Exercise 3.6** (White noise) (a) Suppose that \(h\quad\mbox{and}\quad\tilde{h}\) on \([0,1]\) are in \(\mathcal{L}_{2}\). View _white noise_ as an operator \(d\mathbb{S}\) that takes the function \(h\) into a rv \(\int_{[0,1]}h(t)d\mathbb{S}(t)\) in the sense of \(\to_{\mathcal{L}}\). Define this integral first for step functions, and then use exercise 4.4.5 to define it in general. Then show that \(\int_{[0,1]}h(t)d\mathbb{S}(t)\) exists as such an \(\to_{\mathcal{L}}\) limit for all \(h\) in \(\mathcal{L}_{2}\).

(b) In case \(h\) has a bounded continuous derivative \(h^{\prime}\) on \([0,1]\), show that

\[\int_{[0,1]}h(t)d\mathbb{S}(t)\equiv h\mathbb{S}|_{0-}^{1+}-\int_{[0,1]}\mathbb{ S}(t)h^{\prime}(t)dt. \tag{23}\]

(c) Determine the joint distribution of \(\int_{[0,1]}h(t)d\mathbb{S}(t)\quad\mbox{and}\quad\int_{[0,1]}\tilde{h}(t)d \mathbb{S}(t)\).

(d) Define \(\int_{[0,1]}h(t)d\mathbb{U}(t)\) (appeal first to (7) for the definition), and obtain the marginal and joint distributions of all three of the rvs in (c) and (d).

**Exercise 3.7** (Conditional Brownian motion)  Let \(0\leq r<s<t\). Determine the conditional distribution of \(\mathbb{S}(s)\) given that \(\mathbb{S}(r)=y\quad\mbox{and}\quad\mathbb{S}(t)=z\). Draw a figure for this situation, and then put your answer in a format that allows some insight to be offered as to an interpretation.

**Exercise 3.8**  Find the solution \(V(t)\) of the stochastic differential equation with \(V^{\prime}(t)=-kV(t)+\sigma\mathbb{S}^{\prime}(t)\). Determine its covariance function. (Think of a tiny particle suspended in a liquid whose velocity is impeded by the viscosity of the liquid and is additionally subjected to random changes from collisions with particles in the medium.) [Hint. Rewrite the equation first as \(e^{kt}[V^{\prime}(t)+kV(t)]=\sigma e^{kt}\mathbb{S}^{\prime}(t)\), then transform it to

\[V(t)=V(0)e^{-t}+\sigma\int_{0}^{t}e^{-\sigma(t-s)}d\mathbb{S}(s)\,\]

and then use integration by parts to give meaning to \(d\mathbb{S}(\cdot)\).]

**Exercise 3.9**  Verify Chentsov's condition (12.2.5) for Brownian bridge, that

\[{\rm E}\{{\mathbb{U}}_{n}(r,s]^{2}{\mathbb{U}}_{n}(s,t]^{2}\}\leq\ ({\rm some} \ K)(s-r)(t-s)\quad{\rm for\ all}\ 0\leq r\leq s\leq t\leq 1. \tag{24}\]

Specify a suitable specific \(K.\)

**Exercise 3.10**  The partial sum process \(\{{\mathbb{S}}_{n}(t):0\leq t<\infty\}\) is defined below in (12.2.5). Verify Chentsov's condition for the case of iid \((0,\sigma^{2})\,{\rm rvs},\) that

\[{\rm E}\{{\mathbb{S}}_{n}(r,s]^{2}{\mathbb{S}}_{n}(s,t]^{2}\}\leq\ ({\rm some} \ K)\sigma^{4}(s-r)(t-s) \tag{25}\]

on the grid with \(r=i/n,s=j/n,t=k/n\quad{\rm and}\quad 0\leq i<j<k<\infty\). Specify a suitable specific \(K.\) (Then note theorem 14.1.6.)

## 4 Stopping Times

We first paraphrase the main result of this section. If we observe a right-continuous process at a random time that depends on the process only through its past, then the result is a rv (that is, it is measurable).

**Notation 4.1** Let \((\Omega,\mathcal{A},P)\) denote our basic probability space. We suppose that our time set is a linearly ordered set such as \([0,1],[0,\infty)\), \([0,\infty],\{0,1,2,\dots\}\), \(\{0,1,2,\dots,\infty\}\). Let \(X\) denote a process with such an index set, defined on \((\Omega,\mathcal{A},P)\). We now suppose that the \(\mathcal{A}_{t}\)'s are an \(\diagup\) collection of sub \(\sigma\)-fields of \(\mathcal{A}\), in that \(\mathcal{A}_{s}\subset\mathcal{A}_{t}\) whenever \(s<t\). Call such a collection of \(\mathcal{A}_{t}\)'s a _filtration_. If it further holds that each \(X_{t}\) is an \(\mathcal{A}_{t}\)-measurable rv, then we say that the \(X\)-process is _adapted_ to the \(\mathcal{A}_{t}\)'s. The minimal such collection of \(\diagup\)\(\sigma\)-fields is the _histories_\(\sigma_{t}\equiv\sigma[X_{s}^{-1}(\mathcal{B})\ :\ s\leq t]\). Roughly, \(\sigma_{t}\) denotes all events for the process up to time \(t\). We let \(\mathcal{A}_{t+}\equiv\cap_{n=1}^{\infty}\mathcal{A}_{t+1/n}\); and if \(\mathcal{A}_{t+}=\mathcal{A}_{t}\) for all \(t\geq 0\), then we call the \(\sigma\)-fields \(\mathcal{A}_{t}\)_right continuous_. Let \(\mathcal{A}_{t-}\equiv\sigma[\mathcal{A}_{s}\ :\ s<t]\). Then let \(\mathcal{A}_{\infty}\equiv\sigma[\cup_{t<\infty}\mathcal{A}_{t}]\). \(\square\)

**Definition 4.1** (Stopping times): An (extended) rv \(\tau\geq 0\) will be called an (_extended_) _stopping time_ with respect to the \(\mathcal{A}_{t}\)'s if \([\tau\leq t]\in\mathcal{A}_{t}\) for all \(t\geq 0\). (That is, one can determine whether \(\tau\leq t\) using only current knowledge \(\mathcal{A}_{t}\).)

Roughly, whether \(\tau\) "stops" or "occurs" by time \(t\) or not depends only on those events \(\mathcal{A}_{t}\) with probabilities within our knowledge base up through time \(t\). We define the \(pre\)-\(\tau\)\(\sigma\)-\(field\)

\[\mathcal{A}_{\tau}\equiv\{A\in\mathcal{A}:A\cap[\tau\leq t]\in\mathcal{A}_{t} \quad\text{for all }t\geq 0\}; \tag{1}\]

roughly, at any instant we can decide whether or not \(A\) has occurred yet. Note that if \(\tau(\omega)\equiv t\) for all \(\omega\), then \(\mathcal{A}_{\tau}=\mathcal{A}_{t}\); that is, the fixed time \(t\) is a stopping time whose \(\sigma\)-field is \(\mathcal{A}_{t}\). We now develop some other technical properties.

**Proposition 4.1** (Preservation of stopping times): Suppose the rvs \(T_{1},T_{2},\dots\) are stopping times. Then:

\[T_{1}\lor T_{2}\quad\text{ and }\quad T_{1}\wedge T_{2}\quad\text{are stopping times}. \tag{2}\]

(3) If \(T_{n}\diagup\), \(\text{ then}\,T\equiv\lim T_{n}\quad\text{is a stopping time}\).

(4) If \(T_{n}\searrow\quad\text{and }\mathcal{A}_{t}\)'s are right continuous, then \(T\equiv\lim T_{n}\) is a stopping time.

This proposition is also true for extended stopping times.

**Proof.** Note that these four rvs satisfy

(a) \[[T_{1}\lor T_{2}\leq u]=[T_{1}\leq u]\cap[T_{2}\leq u]\in\mathcal{A}_{u},\] (b) \[[T_{1}\wedge T_{2}\leq u]=[T_{1}\leq u]\cup[T_{2}\leq u]\in\mathcal{A}_{u},\] (c) \[[T\leq u]=\cap_{n=1}^{\infty}[T_{n}\leq u]=\cap_{n=1}^{\infty}\ \text{( events in }\mathcal{A}_{u})\in\mathcal{A}_{u},\] (d) \[[T\leq u]=\cap_{m=1}^{\infty}\cup_{n=1}^{\infty}\,[T_{n}\leq u+1/m]= \cap_{m=1}^{\infty}(\mathcal{A}_{u+1/m}\ \text{events})\ \in\mathcal{A}_{u+}=\mathcal{A}_{u}.\]

No change is needed for extended stopping times. \(\square\)

**Proposition 4.2** (Integral stopping times): Integer-valued \(T\geq 0\) is a stopping time if and only if \([T=n]\in{\cal A}_{n}\) for all \(0\leq n<\infty\). This result is also true for extended stopping times.

**Proof.** If \(T\) is a stopping time, then \([T=n]=[T\leq n]\cap[T\leq n-1]^{c}\) is in \({\cal A}_{n}\), since \([T\leq n-1]^{c}\in{\cal A}_{n-1}\subset{\cal A}_{n}\). Also, \(T=\infty]=(\cup_{n=1}^{\infty}[T\leq n])^{c}\in{\cal A}_{\infty}\). Going the other way, \([T\leq n]=\cup_{m\leq n}[T=m]\in{\cal A}_{n}\), since \([T=m]\in{\cal A}_{m}\subset{\cal A}_{n}\). Also, \([T\leq\infty]=\Omega\in{\cal A}_{\infty}\). \(\Box\)

**Exercise 4.1** (Properties of stopping times): Let \(T_{1},T_{2},\ldots\) be (extended) stopping times; no ordering is assumed. Then (using (9) and/or (10) below is ok):

(5) \(T_{1}+T_{2}\) is an (extended) stopping time if the \({\cal A}_{t}\)'s are right continuous.

(6) \(A\in{\cal A}_{T_{1}}\) implies \(A\cap[T_{1}\leq T_{2}]\in{\cal A}_{T_{2}}\). [Hint. \([T_{1}\wedge t\leq T_{2}\wedge t]\in{\cal A}_{t}\).]

\([T_{1}<T_{2}],[T_{1}=T_{2}],[T_{1}>T_{2}]\) are all in both \({\cal A}_{T_{1}}\) and \({\cal A}_{T_{2}}\).

(7) \(T_{1}\leq T_{2}\) implies \({\cal A}_{T_{1}}\subset{\cal A}_{T_{2}}\). Also, \({\cal A}_{T_{1}}\cap[T_{1}\leq T_{2}]\subset{\cal A}_{T_{1}\wedge T_{2}}={\cal A }_{T_{1}}\cap{\cal A}_{T_{2}}\).

(8) If \(T_{n}\searrow T_{0}\) and the \({\cal A}_{t}\)'s are right continuous, then \(\,{\cal A}_{T_{0}}=\cap_{n=1}^{\infty}{\cal A}_{T_{n}}\).

[Hint. \([T_{1}+T_{2}\leq u]=\cap_{m=1}^{\infty}\{\cup[\tau_{1}\leq a+\frac{1}{m}]\cap[ T_{2}\leq b+\frac{1}{m}]:\,a+b\leq u\), rational \(a,b\}\).]

**Proposition 4.3** (Stopping time measurability): Suppose \(\tau\) is a stopping time with respect to the \({\cal A}_{t}\)'s. Then:

(9) \({\cal A}_{\tau}\) is a \(\sigma\)-field.

(10) \(\tau\) is \({\cal A}_{\tau}\)-measurable.

**Proof.** Consider (10). For example, for each real number \(r\) we have \([\tau\leq r]\in{\cal A}_{\tau}\), since \([\tau\leq r]\cap[\tau\leq t]=[\tau\leq r\wedge t]\in{\cal A}_{r\wedge t}\subset {\cal A}_{t}\) for each \(t\geq 0\). Thus (10) holds.

Consider (9). Let \(A_{1},A_{2},\ldots\) be in \({\cal A}_{\tau}\). Then

(a) \((\cup A_{k})\cap[\tau\leq t]=\cup(A_{k})\cap[\tau\leq t])=\cup\,(\mbox{events \ in ${\cal A}_{t}$})\in{\cal A}_{t}\).

Also, \(A_{1}^{c}\cap[\tau\leq t]=[\tau\leq t]\backslash(A_{1}\cap[\tau\leq t])\in{ \cal A}_{t}\). Thus, (9) holds. \(\Box\)

**Definition 4.2** (Progressively measurable): Let \(\{X(t):t\geq 0\}\) be a process. Let \({\cal B}_{t}\) denote the Borel subsets of \([0,t]\). Call \(X\)_progressively measurable_, and denote this type of measurability by writing

\[X:([0,t]\times\Omega,{\cal B}_{t}\times{\cal A}_{t})\to(R,{\cal B})\qquad\mbox{ for each $t\geq 0$},\]

provided that for each \(t\) in the index set we have

(11) \(\{(s,\omega):0\leq s\leq t\quad\mbox{and}\quad X(s,\omega)\in B\}\quad\mbox{is in ${\cal B}_{t}\times{\cal A}_{t}$}\).

**Proposition 4.4** (Measurability of stopped right-continuous processes): Let \(\tau\) be a stopping time with respect to the \({\cal A}_{t}\)'s. Also, let \(X:\,(\Omega,{\cal A},P)\to(D_{[0,\infty)},{\cal D}_{[0,\infty)})\) be a process adapted to the \({\cal A}_{t}\)'s. Then:

(12) \(X\) is progressively measurable.

(13) \(X(\tau)\) is \({\cal A}_{\tau}\)-measurable.

We may replace \([0,\infty)\) by any \([0,\theta)\) or \([0,\theta]\) with \(0<\theta<\infty\).

(Essentially, one can work nicely with right-continuous processes.)

**Proof.**  That \({\cal A}_{\tau}\) is a \(\sigma\)-field and that \(\tau\) is \({\cal A}_{\tau}\)-measurable are trivial for any image space. (For example, for each real number \(r\) we have \([\tau\leq r]\in{\cal A}_{\tau}\), and then \([\tau\leq r]\cap[\tau\leq t]=[\tau\leq r\wedge t]\in{\cal A}_{\tau\wedge t} \subset{\cal A}_{t}\) for each \(t\geq 0\).)

Fix \(t>0\). Now, (12) holds, since \(X=\lim X_{n}\) (by right continuity), where

\[X_{n}(s,\omega)\equiv\cases{X((k+1)/2^{n}\wedge t,\omega)&for $k/2^{n}\leq s<(k+1)/2^ {n},k\geq 0,s<t$,\cr X(t,\omega)&for $s=t$\cr} \tag{14}\]

clearly satisfies \(X_{n}\) : \(([0,t]\times\Omega,{\cal B}_{t}\times{\cal A}_{t})\to(R,{\cal B})\) is measurable. That is, \(X_{n}\) is progressively measurable. Thus the process \(X\) is also, by proposition 2.2.2, since \(X_{n}(\tilde{\omega})\to X(\tilde{\omega})\) for each \(\tilde{\omega}\equiv(s,\omega)\). That is, \(X\) : \(([0,t]\times\Omega,{\cal B}_{t}\times{\cal A}_{t})\to(R,{\cal B})\) is measurable (or, \(X\) is progressively measurable).

The following type of truncation argument with stopping times is common; learn it. We must show that \([X(\tau)\in B]\cap[\tau\leq t]\in{\cal A}_{t}\), for all Borel sets \(B\in{\cal B}\). But setting \(\tau^{*}\equiv\tau\wedge t\), we see that

\[[X(\tau)\in B]\cap[\tau\leq t]=[X(\tau^{*})\in B]\cap[\tau\leq t]\quad\mbox{ for }\;\tau^{*}\equiv\tau\wedge t, \tag{15}\]

and hence it suffices to show that \([X(\tau^{*})\in B]\in{\cal A}_{t}\). Note that the mapping \(\omega\to(\tau^{*}(\omega),\omega)\) is a measurable mapping from \((\Omega,{\cal A}_{t})\) to \(([0,t]\times\Omega,{\cal B}_{t}\times{\cal A}_{t})\), since for \(A\in{\cal A}_{t}\) and the identity function \(I\) we have (for sets \([0,s]\times A\) generating \({\cal B}_{t}\times{\cal A}_{t}\)) that

(a) \[[(\tau^{*},I)\in[0,s]\times A]=[\tau^{*}\leq s]\cap A=[\tau\wedge t\leq s]\cap A \in{\cal A}_{t}.\]

Combining this \((\tau^{*},I)\) : \((\Omega,{\cal A}_{t})\to([0,t]\times\Omega,{\cal B}_{t}\times{\cal A}_{t})\) measurability with the progressive measurability \(X\) : \(([0,t]\times\Omega,{\cal B}_{t}\times{\cal A}_{t})\to(R,{\cal B})\) shown above, we see that the composition map \(\omega\to X(\tau^{*}(\omega),\omega)\) is \({\cal B}\)-\({\cal A}_{t}\)-measurable. We express this by writing \([X(\tau^{*})\in B]\in{\cal A}_{t}\). (Both (12) and (13) have been established.) \(\Box\)

**Exercise 4.2**  Let \(T\geq 0\) be a rv and let \(\{{\cal A}_{t}:t\geq 0\}\) be an increasing sequence of \(\sigma\)-fields. Establish the following facts.

(a) If \(T\) is a stopping time, then \([T<t]\in{\cal A}_{t}\) for all \(t\geq 0\).

(b) If \([T<t]\in{\cal A}_{t}\) for all \(t\geq 0\) and the \({\cal A}_{t}\)'s are right continuous, then \(T\) is a stopping time.

**Exercise 4.3**  Let \(T_{1}\) be a stopping time, and suppose \(T_{2}\geq T_{1}\) where \(T_{2}\) is an \({\cal A}_{T_{1}}\)-measurable rv, then \(T_{2}\) is a stopping time.

**Exercise 4.4**  Let \(0<a<b<1\). Let \(T_{a}\equiv\inf\{t\ :\ \mathbb{S}(t)=a\}\) for a Brownian motion \(\mathbb{S}\) on \((C,{\cal C})\). Then \(T_{a}\) is a stopping time, but \(\frac{1}{2}T_{a}\) is not. Also, \(A\equiv[\|\mathbb{S}\|\geq a]\) is in \({\cal A}_{T_{a}}\), but \(B\equiv[\|\mathbb{S}\|\geq b]\) is not.

**Definition 4.3** (Augmented filtration)  Let \((\Omega,\widehat{A},P)\) denote the completion of the probability space \((\Omega,{\cal A},P)\). Let \({\cal N}\equiv\{N\in{\cal A}:P(N)=0\}\) denote all null sets. Let \(\{{\cal A}_{t}\ :\ t\geq 0\}\) be an \(\nearrow\) sequence of \(\sigma\)-fields; that is, the \({\cal A}_{t}\)'s form a filtration. If all \({\cal A}_{t}={\cal A}_{t+}\), the \({\cal A}_{t}\)'s are called _right-continuous_. If all \({\cal A}_{t}=\sigma[{\cal A}_{t},{\cal N}]\), then they are said to be a _complete filtration_. If a filtration \(\{{\cal A}_{t}\ :\ t\geq 0\}\) is both complete and right-continuous, such a collection of \(\sigma\)-fields is called an _augmented filtration_.

**Proposition 4.5** (Kallenberg) Let \(\{{\cal A}_{t}\ :\ t\geq 0\}\) denote an \(\nearrow\) sequence of \(\sigma\)-fields on the probability space \((\Omega,{\cal A},P)\); that is, the \({\cal A}_{t}\)'s form a filtration. Consider the null sets \({\cal N}\equiv\{N\in{\cal A}:P(N)=0\}\). Define \(\widehat{{\cal A}}_{t}\equiv\sigma[{\cal A}_{t},{\cal N}]\).

(a) Then the \(\nearrow\)collection of \(\sigma\)-fields \(\widehat{{\cal A}}_{t+}\) necessarily equals the completion \(\widehat{{\cal A}_{t+}}\) of the right-continuous filtration \({\cal A}_{t+}\), and so forms an augmented filtration for \((\Omega,\widehat{{\cal A}},P)\). Moreover, this is the minimal augmented filtration.

(b) If the \({\cal A}_{t}\equiv\sigma_{t}\) denote the histories of a right-continuous process \(X:\ (\Omega,{\cal A},P)\to(D_{[0,\infty)},{\cal D}_{[0,\infty)})\), then the completion of the right-continuized histories necessarily forms the minimal augmented filtration. (That is, complete each \(\sigma_{t+}\equiv\cap_{n=1}^{\infty}\sigma_{t+1/n}\).)

(c) If \(S\leq T\) a.s. then \({\cal A}_{S}\subset{\cal A}_{T}\) relative to the augmented filtration \(\widehat{\sigma_{t+}}=\widehat{\sigma}_{t+}\).

**Proof.** It is trivial that \(\widehat{{\cal A}_{t+}}\subset\widehat{\widehat{{\cal A}}_{t+}}=\widehat{{ \cal A}}_{t+}\). To show the converse, consider a set \(A\in\widehat{A}_{t+}\). Then for each \(n\geq 1\) we have \(A\in\widehat{A}_{t+1/n}\), so \(P(A\Delta A_{n})=0\) for some set \(A_{n}\in{\cal A}_{t+1/n}\). Note that \(A^{*}\equiv\overline{\lim}_{n}\,A_{n}\) is in \({\cal A}_{t+}\), while \(P(A\Delta A^{*})=0\) since \(A\Delta A^{*}\subset\cup_{1}^{\infty}(A\Delta A_{n})=\cup\{\mbox{null}\}_{n}=\{ \mbox{null}\}\); thus \(A\in\widehat{{\cal A}_{t+}}\). Thus the main claim in (a) is established. Let \({\cal F}_{t}\) denote any other augmented filtration for which all \({\cal F}_{t}\supset{\cal A}_{t}\). Then \(\widehat{{\cal A}_{t+}}=\widehat{{\cal A}}_{t+}\subset\widehat{{\cal F}}_{t+} ={\cal F}_{t+}={\cal F}_{t}\), as claimed. Part (b) follows at once. Part (c) follows from exercise 12.4.1 on properties of stopping times. \(\Box\)

**Example 4.1** (Haeusler) Let both \(A\quad\mbox{and}\quad A^{c}\) be measurable subsets of some \((\Omega,{\cal A},P)\) that have probability exceeding \(0\). Define \(X_{t}(\omega)\) on \(0\leq t\leq 1\) to be identically \(0\) if \(\omega\in A\) and to equal \((t-1/2)\cdot 1_{[1/2,1]}(t)\) if \(\omega\in A^{c}\). All paths of this \(X\)-process are continuous. Since \(X_{t}\) is always \(0\) for \(0\leq t\leq 1/2\), we have \(\sigma_{t}=\{\emptyset,\Omega\}\) for \(0\leq t\leq 1/2\). However, \(\sigma_{t}=\{\emptyset,A,A^{c},\Omega\}\) for \(1/2<t\leq 1\). These histories \(\sigma_{t}\) are not right continuous at \(t=1/2\). The right continuized histories \(\sigma_{t+}\) equal \(\{\emptyset,\Omega\}\) for \(0\leq t<1/2\) and equal \(\{\emptyset,A,A^{c},\Omega\}\) for \(1/2\leq t\leq 1\). They are already complete, so \(\widehat{\sigma}_{t+}=\widehat{\sigma_{t+}}=\sigma_{t+}\). Now, proposition 4.5(c) could be applied. \(\Box\)

## 5 Strong Markov Property

We now extend the strong Markov property (which was proved for discrete-time processes in section 8.6) to processes with stationary and independent increments.

**Theorem 5.1** (Strong Markov property): Consider the stochastic process \(X:(\Omega,\mathcal{A},P)\rightarrow(D_{[0,\infty)},\mathcal{D}_{[0,\infty)})\) adapted to right-continuous \(\mathcal{A}_{t}\)'s. Suppose that \(X(0)=0,X\) has stationary and independent increments, and suppose that the increment \(X(t+s)-X(t)\) is independent of \(\mathcal{A}_{t}\) for all \(s\geq 0\). Let \(\tau\) be an extended stopping time for the \(\mathcal{A}_{t}\)'s, and suppose \(P(\tau<\infty)>0\). For some \(t\geq 0\) we define

\[Y(t)\equiv\cases{X(\tau+t)-X(\tau)\mbox{ on }$[\tau<\infty]$,\cr 0\mbox{ on }$[\tau=\infty]$.\cr} \tag{1}\]

Then \(Y:([\tau<\infty]\cap\Omega,\ [\tau<\infty]\cap\mathcal{A},\,P(\cdot|[\tau< \infty]))\ \rightarrow(D_{[0,\infty)},\mathcal{D}_{[0,\infty)})\) and

\[P(Y\in F|[\tau<\infty])=P(X\in F)\quad\mbox{for all }F\in\mathcal{D}_{[0,\infty)}. \tag{2}\]

Moreover, for all \(F\in\mathcal{D}_{[0,\infty)}\) and for all \(A\in\mathcal{A}_{\tau}\), we have

\[P([Y\in F]\cap A|[\tau<\infty])=P([X\in F])\times P(A|[\tau<\infty]). \tag{3}\]

Thus if \(P(\tau<\infty)=1\), then \(X\quad\mbox{and}\quad Y\) are equivalent processes and the process \(Y\) is independent of the \(\sigma\)-field \(\mathcal{A}_{\tau}\).

**Proof.**  That \(Y\): \((\Omega\cap[\tau<\infty],\mathcal{A}\cap[\tau<\infty],P(\cdot|[\tau<\infty]) \rightarrow(D_{[0,\infty)},\mathcal{D}_{[0,\infty)})\) follows from proposition 12.4.3. This proposition and exercise 12.4.1 show that

\[\mathcal{A}_{t}^{\prime}\equiv\mathcal{A}_{\tau+t}\,\mbox{ are $\nearrow$ and right continuous, with $Y$ adapted to the $\mathcal{A}_{t}^{\prime}$'s.} \tag{4}\]

Case 1. Suppose the finite part of the range of \(\tau\) is a countable subset \(\{s_{1},s_{2},\ldots\}\) of \([0,\infty)\). Let \(t_{1},\ldots,t_{m}\geq 0\), let \(B_{1},\ldots,B_{m}\) be Borel subsets of the real line, and let \(A\in\mathcal{A}_{\tau}\). Then

\[P([Y(t_{1})\in B_{1},\ldots,Y(t_{m})\in B_{m}]\cap A\cap[\tau< \infty])\] \[\quad=\sum\nolimits_{k}P([Y(t_{1})\in B_{1},\ldots]\cap A\cap[ \tau=s_{k}])\] \[\quad=\sum\nolimits_{k}P([X(t_{1}+s_{k})-X(s_{k})\in B_{1}, \ldots]\cap A\cap[\tau=s_{k}])\] \[\quad=\sum\nolimits_{k}P(X(t_{1}+s_{k})-X(s_{k})\in B_{1},\ldots )P(A\cap[\tau=s_{k}])\] \[\quad=P(X(t_{1})\in B_{1},\ldots)\sum\nolimits_{k}P(A\cap[\tau=s _{k}])\]

(a) \[=P(X(t_{1})\in B_{1},\ldots,X(t_{m})\in B_{m})P(A\cap[\tau<\infty]),\]

where the third equality holds as \(A\cap[\tau=s_{k}]=(A\cap[\tau\leq s_{k}])\cap[\tau=s_{k}]\) is in \(\mathcal{A}_{s_{k}}\), and is thus independent of the other event by the independent increments of \(X\).

Putting \(A=[\tau<\infty]\) in (a) yields

\[P(Y(t_{1})\in B_{1},\ldots,Y(t_{m})\in B_{m}|[\tau<\infty])\] (b) \[=P(X(t_{1})\in B_{1},\ldots,X(t_{m})\in B_{m});\]substituting (b) into (a) and dividing by \(P(\tau<\infty)\) yields

\[P([Y(t_{1})\in B_{1},\ldots,Y(t_{m})\in B_{m}]\cap A|[\tau<\infty])\] (c) \[=P(Y(t_{1})\in B_{1},\ldots,Y(t_{m})\in B_{m}|[\tau<\infty])P(A|[\tau< \infty]).\]

Thus (b) and (c) hold for the class \({\cal G}\) of sets of the form \([Y(t_{1})\in B_{1},\ldots,Y(t_{m})\in B_{m}]\) and for all sets \(A\) in \({\cal A}_{\tau}\). Since \({\cal G}\) generates \(Y^{-1}({\cal D}_{[0,\infty)})\), equation (b) implies (2). Since \({\cal G}\) is also closed under finite intersections (that is, it is a \(\bar{\pi}-\)system), (c) and proposition 7.1.1 imply the truth of (3).

Case 2. Now consider a general stopping time \(\tau\). For \(n\geq 1\), define

\[\tau_{n}\equiv\cases{k/n\,\,\mbox{for}\,\,(k-1)/n<\tau\leq k/n\,\,\mbox{and} \,\,k\geq 1,\cr 1/n\,\,\mbox{for}\,\,\tau=0,\cr\infty\,\,\,\mbox{for}\,\,\tau= \infty.}\] (d)

Note that \(\tau_{n}(\omega)\searrow\tau(\omega)\) for \(\omega\in[\tau<\infty]\). For \(k/n\leq t<(k+1)/n\) we have

\[[\tau_{n}\leq t]=[\tau\leq k/n]\in{\cal A}_{k/n}\subset{\cal A}_{t}\]

(so that \(\tau_{n}\) is a stopping time), and also for \(A\) in \({\cal A}_{\tau}\) that

\[A\cap[\tau_{n}\leq t]=A\cap[\tau\leq k/n]\in{\cal A}_{k/n}\subset{\cal A}_{t}\]

(so that \({\cal A}_{\tau}\subset{\cal A}_{\tau_{n}}\)). Define

\[Y_{n}(t)=X(\tau_{n}+t)-X(\tau_{n})\quad\mbox{on}\,\,[\tau_{n}<\infty]=[\tau< \infty],\]

and let it equal 0 elsewhere. By case 1 results (b) and (c), both

\[P(Y_{n}\in F|[\tau<\infty])=P(X\in F)\quad\mbox{and}\] (f) \[P([Y_{n}\in F]\cap A|[\tau<\infty])=P(Y_{n}\in F|[\tau<\infty])P(A|[\tau<\infty])\] (g)

hold for all \(F\) in \({\cal D}_{[0,\infty)}\) and all \(A\) in \({\cal A}_{\tau}\) (recall that \({\cal A}_{\tau}\subset{\cal A}_{\tau_{n}}\) as shown above, and \([\tau<\infty]=[\tau_{n}<\infty]\)). Let \((r_{1},\ldots,r_{m})\) denote any continuity point of the joint df of the finite dimensional random vector \((Y(t_{1}),\ldots,Y(t_{m}))\), and define

\[\begin{array}{rl}G_{n}&\equiv[Y_{n}(t_{1})<r_{1},\ldots,Y_{n}(t_{m})<r_{m}, \tau<\infty],\\ G&\equiv[Y(t_{1})<r_{1},\ldots,Y(t_{m})<r_{m},\tau<\infty],\\ G^{*}&\equiv[Y(t_{1})\leq r_{1},\ldots,Y(t_{m})\leq r_{m},\tau<\infty],\\ H&\equiv[X(t_{1})<r_{1},\ldots,X(t_{m})<r_{m}].\end{array}\] (h)

By the right continuity of the sample paths, \(Y_{n}(t)\to Y(t)\) for every \(t\) and every \(\omega\) in \([\tau<\infty]\); thus

\[\mbox{(i)}\qquad G\subset\underline{\lim}\,G_{n}\subset\overline{\lim}\,G_{n }\subset G^{*}\]

Thus

\[\begin{array}{l}P(G|\tau<\infty)\leq P(\underline{\lim}\,G_{n}|[\tau<\infty] )\leq\underline{\lim}\,P(G_{n}|\tau<\infty)\quad\mbox{by (i), then DCT}\\ \quad=P(H)=\overline{\lim}\,P(G_{n}|\tau<\infty)\qquad\mbox{by using (f) twice}\\ \quad\leq P(\overline{\lim}\,G_{n}|\tau<\infty)\leq P(G^{*}|\tau<\infty)\qquad \mbox{by the DCT and (i)}\\ \quad\leq P(G|\tau<\infty)+\sum_{i=1}^{m}P(Y(t_{i})=r_{i}|\tau<\infty)\end{array}\](j) \[=P(G|\tau<\infty),\]

since \((r_{1},\ldots,r_{m})\) is a continuity point. Thus (j) implies

(k) \[P(G|\tau<\infty)=P(H),\]

and this is sufficient to imply (2). Likewise, for \(A\in\mathcal{A}_{\tau}\subset\mathcal{A}_{\tau_{n}}\),

\[P(G\cap A|[\tau<\infty])\leq P(\underline{\lim}\,G_{n}\cap A| \tau<\infty)\qquad\text{by (i)}\] \[\leq\underline{\lim}\,P(G_{n}\cap A|\tau<\infty)\qquad\text{by the DCT}\] \[=\underline{\lim}\,P(G_{n}|\tau<\infty)P(A|\tau<\infty)\qquad \text{by (c), with }[\tau<\infty]=[\tau_{n}<\infty]\] \[=P(G|\tau<\infty)P(A|\tau<\infty)\qquad\text{by (j)}\] \[=\overline{\lim}\,P(G_{n}|\tau<\infty)P(A|\tau<\infty)\qquad \text{by (j)}\] \[=\overline{\lim}\,P(G_{n}\cap A|\tau<\infty)\qquad\text{by (c), with }[\tau<\infty]=[\tau_{n}<\infty]\] \[\leq P(\overline{\lim}\,G_{n}\cap A|\tau<\infty)\leq P(G^{*} \cap A|\tau<\infty)\qquad\text{by the DCT, then (i)}\] \[\leq P(G\cap A|\tau<\infty)+\sum\nolimits_{i=1}^{m}P(Y(t_{i})=r_{i }|\tau<\infty)\] \[=P(G\cap A|\tau<\infty),\]

since \((r_{1},\ldots,r_{m})\) is a continuity point. Thus (1) implies

(m) \[P(G\cap A|\tau<\infty)=P(G|\tau<\infty)P(A|\tau<\infty);\]

and using proposition 7.1.1 again, we see that this is sufficient to imply (3).

The final statement is immediate, since when \(P(\tau<\infty)=1\) we must have \(P(A|\tau<\infty)=P(A)\) for all \(A\in\mathcal{A}\).

## 6 Embedding a RV in Brownian Motion

Let \(a,b>0\). For a Brownian motion \(\mathbb{S}\) on \((C_{\infty},\mathcal{C}_{\infty})\), we define

\[\tau\equiv\tau_{ab}\equiv\inf\{t\ :\ \mathbb{S}(t)\in(-a,b)^{c}\} \tag{1}\]

to be the first time \(\mathbb{S}\) hits either \(-a\) or \(b\). Call \(\tau\) a _hitting time_. [Show that \(\tau\) is a stopping time.] Note figure 6.1.

**Theorem 6.1** (Embedding via \(\tau_{ab}\)): Let \(\tau\equiv\tau_{ab}\). Then:

\[\mathrm{ES}(\tau)=0. \tag{2}\] \[P(\mathbb{S}(\tau)=-a)=b/(a+b)\qquad\quad\text{and}\qquad P( \mathbb{S}(\tau)=b)=a/(a+b).\] (3) \[\mathrm{E}\tau=ab=\mathrm{ES}^{2}(\tau)\qquad\qquad\qquad\text{ and}\qquad\mathrm{E}\tau^{2}\leq 4ab(a+b).\] (4) \[\mathrm{E}\tau^{\tau}\leq r\Gamma(r)2^{2r}\mathrm{ES}^{2r}(\tau) \leq r\Gamma(r)2^{2r}ab(a+b)^{2r-2}\qquad\text{for all $r\geq 1$}. \tag{5}\]

**Definition 6.1** (Martingale): A process \(\{M(t)\ :\ t\geq 0\}\) is a continuous parameter _martingale_\((mg)\) if \(\mathrm{E}|M(t)|<\infty\) for all \(t\), \(M\) is adapted to the \(\mathcal{A}_{t}\)'s, and

\[\mathrm{E}\{M(t)|\mathcal{A}_{s}\}=_{a.s.}M(s)\qquad\text{for all $0\leq s\leq t$}. \tag{6}\]

**Definition 6.2** (Stopping time): If \(\tau\) is a random time (just a rv that is \(\geq 0\)) for which the event \([\tau\leq t]\in\mathcal{A}_{t}\) for all \(t\), then we call \(\tau\) a _stopping time_.

**Future theorem** Let \(\tau\) be a stopping time. With appropriate regularity conditions on a mg \(M\), we can claim that

\[\mathrm{E}M(\tau)=\mathrm{E}M(0). \tag{7}\]

Our present applications are simple special cases of a result called the _optional sampling theorem_ for mgs. The general version will be proven in chapter 18. We will use it for such simple special cases now.

Figure 6.1: The stopping time \(\tau_{ab}\).

**Proof.** The independent increments of \(\mathbb{S}\) lead to satisfaction of the mg property stated in (6). Also, \(\mathbb{S}\) is suitably integrable (we will see later) for (7) to hold (note (13.6.9) and (13.6.16)). Thus, with \(p\equiv P(\mathbb{S}(\tau)=b)\), we have

(a) \[0=\mbox{ES}(\tau)=bp-a(1-p),\qquad\mbox{or}\qquad p=a/(a+b).\]

Also, the process

\[\{\mathbb{S}^{2}(t)-t:t\geq 0\}\qquad\mbox{is a mg adapted to the $\sigma$-fields ${\cal A}_{t}\equiv\sigma_{t}$}, \tag{8}\]

since

\[\mbox{E}\{\mathbb{S}^{2}(t)-t|{\cal A}_{s}\}=\mbox{E}\{[\mathbb{S }(t)-\mathbb{S}(s)+\mathbb{S}(s)]^{2}-t|{\cal A}_{s}\}\] \[=\mbox{E}\{[\mathbb{S}(t)-\mathbb{S}(s)]^{2}+2\mathbb{S}(s)[ \mathbb{S}(t)-\mathbb{S}(s)]+\mathbb{S}^{2}(s)-t|{\cal A}_{s}\}\] \[=\mbox{E}\{[\mathbb{S}(t)-\mathbb{S}(s)]^{2}\}+2\mathbb{S}(s) \mbox{E}\{\mathbb{S}(t)-\mathbb{S}(s)\}+\mathbb{S}^{2}(s)-t\] \[=t-s+2\mathbb{S}(s)\cdot 0+\mathbb{S}^{2}(s)-t\] (b) \[=\mathbb{S}^{2}(s)-s.\]

This process is also suitably integrable, so that optional sampling can be used to imply \(\mbox{E}[\mathbb{S}(\tau)^{2}-\tau]=0\). Thus

(c) \[\mbox{E}\tau=\mbox{ES}^{2}(\tau)=(-a)^{2}\cdot b/(a+b)+b^{2}\cdot a/(a+b)=ab.\]

We leave (5) to exercise 12.7.3 below. \(\Box\)

**Theorem 6.2** (Skorokhod embedding of a zero-mean rv): Suppose \(X\) is a rv with df \(F\) having mean \(0\) and variance \(0\leq\sigma^{2}\leq\infty\). Then there is a stopping time \(\tau\) such that the stopped rv \(\mathbb{S}(\tau)\) is distributed as \(X\); that is,

\[\mathbb{S}(\tau)\cong X. \tag{9}\]

Moreover,

\[\mbox{E}\tau=\mbox{Var}[X]\qquad\quad\mbox{and}\qquad\mbox{E}\tau^{2}\leq 1 6\mbox{E}X^{4}, \tag{10}\]

and for any \(r\geq 1\) we have

\[\mbox{E}\tau^{r}\leq K_{r}\mbox{E}|X|^{2r}\quad\mbox{with}\qquad K_{r}\equiv r \Gamma(r)2^{4r-2}. \tag{11}\]

**Proof.** For degenerate \(F\), just let \(\tau\equiv 0\). Thus suppose \(F\) is nondegenerate. Let \((A,B)\) be independent of \(\mathbb{S}\), with joint df \(H\) having

\[dH(a,b)=(a+b)dF(-a)dF(b)/EX^{+}\qquad\mbox{for $a\geq 0,b>0$}. \tag{12}\]

The procedure is to observe \((A,B)=(a,b)\) according to \(H\), and then to observe \(\tau_{ab}\), calling the result \(\tau.\) (Clearly, \(\tau_{ab}=0\) if \(a=0\) is chosen.) Note that \([\tau\leq t]\) can be determined by \((A,B)\quad\mbox{and}\quad\{\mathbb{S}(s)\ :\ 0\leq s\leq t\}\), and hence is an event in \({\cal A}_{t}\equiv\sigma[A,B,\mathbb{S}(s)\) : \(0\leq s\leq t\}\). For \(t\geq 0\),

\[P(\mathbb{S}(\tau)>t)=\mbox{E}(P\{\mathbb{S}(\tau)>t|A=a,B=b\})\] (a) \[=\int_{[0,\infty)}\int_{(0,t]}0\cdot dH(a,b)+\int_{[0,\infty)}\int_{(t, \infty)}(a/(a+b))dH(a,b)\quad\mbox{ by (3)}\](b) \[=\int_{(t,\infty)}\int_{[0,\infty)}\ a\ dF(-a)dF(b)/\mathrm{E}X^{+}=\int_{(t,\infty) }dF(b)\mathrm{E}X^{-}/\mathrm{E}X^{+}\] (c) \[=1-F(t),\]

since \(EX=0\) with \(X\) nondegenerate implies \(\mathrm{E}X^{+}=\mathrm{E}X^{-}\). Likewise, for \(t\geq 0\),

(d) \[P(\mathbb{S}(\tau)\leq-t)=\int_{[0,t)}\int_{(0,\infty)}0\cdot dH(a,b)+\int_{[t, \infty)}\int_{(0,\infty)}(b/(a+b))dH(a,b)\] (e) \[=\int_{[t,\infty)}\int_{(0,\infty)}bdF(b)dF(-a)/\mathrm{E}X^{+}=\int_{[t,\infty )}dF(-a)\] (f) \[=F(-t).\]

Thus \(\mathbb{S}(\tau)\cong X\). Moreover,

\[\mathrm{E}\tau=\mathrm{E}(\mathrm{E}\{\tau|A=a,B=b\})=\mathrm{E}(\mathrm{E}\{ \mathbb{S}^{2}(\tau)|A=a,B=b\})=\mathrm{E}\mathrm{S}^{2}(\tau)\]

(g) \[=\mathrm{E}X^{2}=\mathrm{Var}[X].\]

Note that \((a+b)^{2r-1}\leq 2^{2r-2}[a^{2r-1}+b^{2r-1}]\) by the \(C_{r}\)-inequality. Thus

\[\mathrm{E}\tau^{r}=\mathrm{E}(\mathrm{E}\{\tau^{r}|A=a,B=b\})\] (h) \[\leq 2^{2r}r\Gamma(r)\mathrm{E}(AB(A+B)^{2r-2})\quad\text{by (\ref{eq:E})}\] \[\leq 2^{2r}r\Gamma(r)\mathrm{E}(AB(A+B)^{2r-1}/(A+B))\] (i) \[\leq r\Gamma(r)2^{4r-2}\mathrm{E}\left(\frac{B}{A+B}A^{2r}+\frac{A}{A+B}B^{2r}\right)\]

(j) \[=K_{r}\mathrm{E}(\mathrm{E}\{\mathbb{S}^{2r}(\tau)|A=a,B=b\})=K_{r}\mathrm{E}( \mathbb{S}^{2r}(\tau))=K_{r}\mathrm{E}X^{2r},\]

as claimed.

## 7 Barrier Crossing Probabilities

For \(-a<0<b\) we defined the _hitting time_

\[\tau_{ab}\equiv\inf\{t\ :\ \mathbb{S}(t)\in(-a,b)^{c}\}, \tag{1}\]

where \(\mathbb{S}\) denotes Brownian motion on \((C_{\infty},C_{\infty})\). We also considered the rv \(\mathbb{S}(\tau_{ab})\), which is called Brownian motion _stopped_ at \(\tau_{ab}\). We saw that it took on the two values \(b\) and \(-a\) with the probabilities \(p\equiv a/(a+b)\quad\mbox{and}\quad q\equiv 1-p=b/(a+b)\).

For \(a>0\) we define the stopping time (the _hitting time_ of \(a\))

\[\tau_{a}\equiv\inf\{t:\mathbb{S}(t)\geq a\}. \tag{2}\]

[Now, \([\tau_{a}<c]=\cap_{q<a}\cup_{r<c}[\mathbb{S}(r)>q]\) (over rational \(p\) and \(q\)) shows that \(\tau_{a}\) is a stopping time.] The LIL of (8.6.1) shows that both \(\tau_{ab}\quad\mbox{and}\quad\tau_{a}\) are finite a.s.

**Theorem 7.1** (The reflection principle; Bachelier): Both

\[P(\sup_{0\leq t\leq c}\mathbb{S}(t)>a)=P(\tau_{a}<c)=2P(\mathbb{ S}(c)>a)\] \[\quad=2P(N(0,1)\geq a/\sqrt{c})\qquad\mbox{for $a>0$}\qquad\mbox{and} \tag{3}\]

\[P(\|\mathbb{S}\|_{0}^{1}>a)=4\sum\nolimits_{k=1}^{\infty}P((4k-3)a<N(0,1)<(4k- 1)a) \tag{5}\] \[\quad=1-\frac{4}{\pi}\sum_{k=0}^{\infty}\frac{(-1)^{k}}{2k+1}\exp \left(-\frac{(2k+1)^{2}\pi^{2}}{8a^{2}}\right)\quad\mbox{for $a>0$}. \tag{4}\]

**Proof.** Define the stopping time \(\tau_{a}^{\prime}\equiv\tau_{a}\wedge c\), and note that \(\tau_{a}=\tau_{a}^{\prime}\) on the event \([\mathbb{S}(c)>a]\). Now, \([\tau_{a}^{\prime}<c]\in\mathcal{A}_{\tau_{a}^{\prime}}\) is independent of the Brownian motion \(\{\mathbb{Y}(t)\equiv\mathbb{S}(\tau_{a}^{\prime}+t)-\mathbb{S}(\tau_{a}^{ \prime})\ :\ t\geq 0\}\), by strong Markov, with \(P(\tau_{a}^{\prime}<\infty)=1\). In (b) below we will use that \(\mathbb{S}(\tau_{a}^{\prime})=a\) on \([\mathbb{S}(c)>a]\). We have

\[P(\tau_{a}<c)=P(\tau_{a}^{\prime}<c)\] (a) \[=P([\tau_{a}^{\prime}<c]\cap[\mathbb{S}(c)>a])+P([\tau_{a}^{\prime}<c] \cap[\mathbb{S}(c)<a])+\mbox{O}\] (b) \[=P([\tau_{a}^{\prime}<c]\cap[\mathbb{S}(c)-\mathbb{S}(\tau_{a}^{ \prime})>0])+P([\tau_{a}^{\prime}<c]\cap[\mathbb{S}(c)-\mathbb{S}(\tau_{a}^{ \prime})<0])\] (c) \[=2P([\tau_{a}^{\prime}<c]\cap[\mathbb{S}(c)-\mathbb{S}(\tau_{a})>0]) \qquad\mbox{using the strong Markov property}\] (d) \[=2P(\mathbb{S}(c)>a),\]

since the events in (c) and (d) are identical.

The two-sided boundary of formula (4) follows from a more complicated reflec- tion principle. Let \(A+\equiv[\|\mathbb{S}^{+}\|>a]=][\mathbb{S}\) exceeds \(a\) somewhere on \([0,1]]\) and \(A_{-}\equiv[\|\mathbb{S}^{-}\|>a]=[\mathbb{S}\) falls below \(-a\) somewhere on \([0,1]]\). Though \([\|\mathbb{S}\|>a]=A_{+}\cup A_{-}\), we have \(P(\|\mathbb{S}\|>a)<P(A_{+})+P(A_{-})\), since we included paths that go above \(a\) and then below \(-a\) (or vice versa) twice. By making the first reflection in figure 7.1, we see that the probability of the former event equals that of \(A+-=[\|\mathbb{S}^{+}\|>3a]\), while that of the latter equals that of \(A_{-+}=[\|\mathbb{S}^{-}\|>3a]\). But subtracting out these probabilities from \(P(A^{+})+P(A^{-})\) subtracts out too much, since the path may then have recrossed the other boundary; we compensate for this by adding back in the probabilities of \(A+-+\equiv[\|\mathbb{S}^{+}\|>5a]\quad\mbox{and}\quad A_{-+-}\equiv[\|\mathbb{ S}^{-}\|>5a]\), which a secondreflection shows to be equal to the appropriate probability. But we must continue this process ad infinitum. Thus

(e) \[P(\|\mathbb{S}\|_{0}^{1}>a)=\cases{P(A_{+})-P(A_{+-})+P(A_{+-+})-\cdots+\cr P(A_{- })-P(A_{-+})+P(A_{-+-})-\cdots\cr}\] (f) \[=2[P(A_{+})-P(A_{+-})+P(A_{+-+})-\cdots]\qquad\hbox{by symmetry}\] \[=2\sum_{k=1}^{\infty}(-1)^{k+1}2P(N(0,1)>(2k-1)a)\qquad\hbox{by }(3)\] (g) \[=4\sum_{k=1}^{\infty}P((4k-3)a<N(0,1)<(4k-1)a)\]

as claimed. The final expression (5) is left for the reader; it is reputed to converge more quickly. \(\Box\)

**Exercise 7.1**: Prove (5). (See Chung (1974, p. 223).)

**Theorem 7.2** (The reflection principle for linear boundaries; Doob): Consider the line \(ct+d\) with \(c\geq 0,d>0\). Then:

\[P(\mathbb{S}(t)\geq ct+d\quad\hbox{for some }t\geq 0)=\exp(-2\,c\,d). \tag{7}\] \[P(|\mathbb{S}(t)|\geq ct+d\quad\hbox{for some }t\geq 0)=2\sum_{k=1}^{ \infty}(-1)^{k+1}\exp(-2k^{2}cd). \tag{6}\]

**Proof.** Now, for any \(\theta\neq 0\) the process

\[\{V(t)\equiv\exp(\theta[\mathbb{S}(t)-\theta t/2])\ :\ t\geq 0\}\qquad\hbox{is a mg}\quad(\hbox{with }V(0)\equiv 1). \tag{8}\]

This holds with \(\sigma_{t}\equiv\sigma[\mathbb{S}(s)\ :\ s\leq t]\) (using the mgf of a normal rv), since

Figure 7.1: The reflection principle for Brownian motion.

\[\mathrm{E}\{V(t)|\sigma_{s}\}=\mathrm{E}\{\exp(\theta[\mathbb{S}(s)-\theta s/2]+ \theta[\mathbb{S}(s,t]-\theta(t-s)/2])|\sigma_{s}\}\]

(a) \[=V(s)\mathrm{E}\{\exp(\theta N(0,t-s))\}\ \exp(-\theta^{2}(t-s)/2)\]

(b) \[=V(s).\]

Thus if we now redefine \(\tau_{ab}\) as \(\tau_{ab}\equiv\inf\{t\ :\ \mathbb{X}(t)\equiv\mathbb{S}(t)-\theta t/2\in(-a,b)^{c}\}\), where we have \(a>0,b>0\), then \(V(t)=e^{\theta\mathbb{X}(t)}\). Hence the "future theorem" gives

(c) \[1=\mathrm{E}V(\tau_{ab})=P(\mathbb{X}(\tau_{ab})=-a)e^{-\theta a}+P(\mathbb{X}( \tau_{ab})=b)e^{\theta b},\]

so that

\[P(\mathbb{X}(\tau_{ab})=b)=(1-e^{-\theta a})/(e^{\theta b}-e^{-\theta a}) \tag{9}\]

(d) \[\to e^{-\theta b}\qquad\text{if $\theta>0$ and }\quad a\to\infty\]

(e) \[=e^{-2cd}\qquad\text{if $\theta=2c$ and $b=d$}.\]

But this same quantity also satisfies (by proposition 1.1.2)

(f) \[P(\mathbb{X}(\tau_{ab})=b)\to P(\mathbb{X}(t)\geq b\quad\text{for some $t$})\qquad\text{as $a\to\infty$}\] \[\quad=P(\mathbb{S}(t)-\theta t/2\geq b\quad\text{for some $t$})=P(\mathbb{S}(t)\geq\theta t/2+b\text{ for some $t$})\]

(g) \[=P(\mathbb{S}(t)\geq ct+d\ \text{ for some $t$})\qquad\text{if $c=\theta/2$ and $d=b$}.\]

Equating (g) to (e) (via (f) and (9)) gives (6). 

**Exercise 7.2** : Prove (7). (See Doob (1949).)

**Theorem 7.3** (Kolmogorov-Smirnov distributions): Both

\[P(\|\mathbb{U}^{\pm}\|>b)=\exp(-2b^{2})\qquad\text{for all $b>0$}\qquad\text{and} \tag{10}\]

\[P(\|\mathbb{U}\|>b)=2\sum_{k=1}^{\infty}(-1)^{k+1}\exp(-2k^{2}b^{2})\qquad \text{for all $b>0$}. \tag{11}\]

**Proof.** : Now, \(\|\mathbb{U}^{-}\|\cong\|\mathbb{U}^{+}\|\) and

\[P(\|\mathbb{U}^{+}\|>b)=P(\mathbb{U}(t)>b\quad\text{for some $0<t<1$}) \tag{13}\] \[=P((1-t)\mathbb{S}(t/(1-t))>b)\quad\text{for some $0\leq t\leq 1$},\quad\text{by \ \eqref{eq:12}}\] \[=P(\mathbb{S}(r)>b+rb\quad\text{for some $r\geq 0$})\qquad\text{ letting $r=t/(1-t)$} \tag{12}\]

(a) \[=\exp(-2b^{2})\qquad\text{by theorem \ref{theorem:12}}.\]

Likewise,

(b) \[P(\|\mathbb{U}\|>b)=P(|\mathbb{S}(r)|>b+rb\text{ for some $r\geq 0$})\]

(c) \[=2\sum_{k=1}^{\infty}(-1)^{k+1}\exp(-2k^{2}b^{2})\]

by theorem 7.2.

**Exercise 7.3**: (a) Prove (12.6.5) for \(r=2\).

(b) Prove (12.6.5) for integral \(r\). (This is unimportant.)

[Hint. The \(V_{\theta}\equiv\exp(\theta[{\mathbb{S}}(t)-\theta t^{2}/2]),t\geq 0\) of (8) are martingales on \([0,\infty)\). Differentiate formally under the integral sign in the martingale equality

\[\int_{A}V_{\theta}(t)dP=\int_{A}V_{\theta}(s)dP\qquad\mbox{for all $A\in{\cal A}_{s}$}. \tag{13}\]

Then conclude that \([\partial^{k}/\partial\theta^{k}V_{\theta}(t)]|_{\theta=0}\) is a martingale for each \(k\geq 1\). For \(k=4\) this leads to \({\mathbb{S}}^{4}(t)-6t{\mathbb{S}}^{2}(t)+3t^{2}=t^{2}H_{4}({\mathbb{S}}(t)/ \sqrt{t})\) being a martingale on \([0,\infty)\); here \(H_{4}(t)=t^{4}-6t^{2}+3\) is the "fourth Hermite polynomial." The reader needs to work only with the single specific martingale in part (a); the rest of this hint is simply an intuitive explanation of how this martingale arises.]

## 8 Embedding the Partial Sum Process \({}^{o}\)

**The Partial Sum Process**

Let \(X_{n1},\ldots,X_{nn}\) be row-independent rvs having a common \(F(0,1)\) distribution, and let \(X_{n0}\equiv 0\). We define the _partial sum process_\(\mathbb{S}_{n}\) on \((D,\mathcal{D})\) by

\[\mathbb{S}_{n}(t)\equiv\frac{1}{\sqrt{n}}\sum_{i=0}^{[nt]}X_{ni}=\frac{1}{ \sqrt{n}}\sum_{i=0}^{k}X_{ni}\qquad\text{for }\frac{k}{n}\leq t<\frac{k+1}{n},0 \leq k\leq n \tag{1}\]

(or for all \(k\geq 0\), in case the \(n\)th row is \(X_{n1},X_{n2},...\)). Note that

\[\text{Cov}[\mathbb{S}_{n}(s),\mathbb{S}_{n}(t)]=\sum\nolimits_{i= 1}^{[ns]}\sum\nolimits_{j=1}^{[nt]}\text{Cov}[X_{ni},X_{nj}]/n\] \[\qquad=[n(s\wedge t)]/n\qquad\text{for }0\leq s,t\leq 1 \tag{2}\]

for the greatest integer function \([\cdot]\). We suspect that \(\mathbb{S}_{n}\)"converges" to \(\mathbb{S}\). We will establish this shortly.

**Embedding the Partial Sum Process**

**Notation 8.1** Let \(\{\mathbb{S}(t)\ :\ t\geq 0\}\) denote a Brownian motion on \((C_{\infty},\mathcal{C}_{\infty})\). Then

\[\mathbb{Z}_{n}(t)\equiv\sqrt{n}\mathbb{S}(t/n)\quad\text{for }t\geq 0\qquad \text{ is also such a Brownian motion}. \tag{3}\]

By using the Skorokhod embedding technique of the previous section repeatedly on the Brownian motion \(\mathbb{Z}_{n}\), we may guarantee that for appropriate stopping times \(\tau_{n1},\ldots,\tau_{nn}\) (with all \(\tau_{n0}\equiv 0\)) we obtain that

\[X_{nk}\equiv\mathbb{Z}_{n}(\tau_{n,k-1},\tau_{nk}],\quad\text{for }1\leq k\leq n,\quad\text{are iid }F(0,1)\text{ rvs}. \tag{4}\]

Let \(\mathbb{S}_{n}\) denote the partial sum process of these \(X_{nk}\)'s. Then, for \(t\geq 0\) we have

\[\begin{array}{l}\mathbb{S}_{n}\ (t)=\frac{1}{\sqrt{n}}\sum_{k=1}^{[nt]}X_{ nk}=\frac{1}{\sqrt{n}}\mathbb{Z}_{n}(\tau_{n,[nt]})=\mathbb{S}\left(\frac{\tau_{n [nt]}}{n}\right)\\ \ \ \ \ \ \ \ \ =\mathbb{S}\left(\frac{1}{n}\sum_{k=1}^{[nt]}T_{nk}\right)= \mathbb{S}(I_{n}(t))\end{array} \tag{5}\]

with \(T_{nk}\equiv(\tau_{nk}-\tau_{n,k-1})\quad\text{and}\quad I_{n}(t)\equiv\frac{1 }{n}\tau_{n,[nt]}\). Observe that:

\[X_{n1},\ldots,X_{nn}\quad\text{are iid }F(0,1),\text{in each row}. \tag{6}\]

\[T_{n1},\ldots,T_{nn}\quad\text{are iid with means }=1=\text{Var}[X],\text{in each row}. \tag{7}\]

\[\text{E}T_{nk}^{r}\leq K_{r}\cdot\text{E}|X_{nk}|^{2r},\quad\text{with}\quad K _{r}\equiv r\Gamma(r)2^{4r-2}.\qed \tag{8}\]

**Theorem 8.1** (Skorokhod's embedding theorem): The partial sum process \(\mathbb{S}_{n}\) on \((D,\mathcal{D})\) of row-independent \(F(0,1)\) rvs formed as above satisfies

\[\|\mathbb{S}_{n}-\mathbb{S}\|\to_{p}0\text{ as }n\to\infty. \tag{9}\]

**Notice**: The joint distributions of any \(\mathbb{S}_{m},\mathbb{S}_{n}\) in theorem 8.1 are not the same as they would be if formed from a single sequence of iid rvs. In fact, we have no idea of what these joint distributions may be. However, the partial sums of an iid sequence do not generally converge to their limit in the sense of \(\to_{p}\), so we have gained a great deal via the embedding.

**Theorem 8.2** (Embedding at a rate): Suppose that \(\mathrm{E}X^{4}<\infty\). Let \(I\) denote the identity function. Then for each \(0\leq\nu<\frac{1}{4}\), the process \(\mathbb{S}_{n}\) of (5) satisfies

\[n^{\nu}\|(\mathbb{S}_{n}-\mathbb{S})/I^{1/2-\nu}\|_{1/n}^{1}=O_{p}(1). \tag{10}\]

Proof.: Consider theorem 8.1. Let \(I\) denote the identity function. Suppose we now show that

(a) \[\|I_{n}-I\|_{0}^{1}=\sup_{0\leq t\leq 1}|\tau_{n,[nt]}/n-t|\to_{p}0.\]

Then on any subsequence \(n^{\prime}\) where \(\to_{p}0\) in (a) may be replaced by \(\to_{a.s.}0\), the continuity of the paths of \(\mathbb{S}\) will yield

(b) \[\|\mathbb{S}_{n^{\prime}}(\cdot)\ -\mathbb{S}(\cdot)\|=\|\mathbb{S}(I_{n^{ \prime}})-\mathbb{S}\|\to_{a.s.}0,\]

and thus (9) will follow. This is a useful argument; learn it. It therefore remains to prove (a). The WLLN gives

(c) \[I_{n}(t)=\tau_{n,[nt]}/n\to_{p}t\qquad\text{ for any fixed }t.\]

Using the diagonalization technique, we can extract from any subsequence a further subsequence \(n^{\prime}\) on which

(d) \[I_{n^{\prime}}(t)\to_{a.s.}t\qquad\text{ for all rational }t.\]

But since all functions involved are monotone, and since the limit function is continuous, this implies that a.s.

(e) \[I_{n^{\prime}}(t)\to t\qquad\text{ uniformly on }F[0,1].\]

Thus (a) follows from (e), since every \(n\) has a further \(n^{\prime}\) with the same limit. Thus the conclusion (9) holds.

In the proof just given, the conclusion (9) can trivially be replaced by

(f) \[\sup_{0\leq t\leq m}|\mathbb{S}_{n}(t)-\mathbb{S}(t)|\to_{p}0.\]

Appealing to exercise 12.1.6(b) for the definition of \(\|\cdot\|_{\infty}\), we thus obtain

\[\rho_{\infty}(\mathbb{S}_{n},\mathbb{S})\to_{p}0\quad\text{on}\quad(C_{\infty},C_{\infty}), \tag{11}\]

provided that the rvs \(X_{n1},X_{n2},\ldots\) are appropriately iid \((0,\sigma^{2})\). [We consider the proof of theorem 8.2 at the end of this section.] 

Let \(g:\,(D,\mathcal{D})\to(R,\mathcal{B})\) and let \(\Delta_{g}\) denote the set of all \(x\in D\) for which \(g\) is not \(\|\cdot\|\)-continuous at \(x\). If there exists a set \(\Delta\in\mathcal{D}\) having \(\Delta_{g}\subset\Delta\quad\text{and}\quad P(\mathbb{S}\in\Delta)=0\), then we say that \(g\) is \(a.s.\|\cdot\|\)-_continuous_ with respect to the process \(\mathbb{S}\).

**Theorem 8.3** (Donsker): Let \(g:\,(D,\mathcal{D})\to(R,\mathcal{B})\) denote an a.s. \(\|\cdot\|\)-continuous mapping that is \(\mathcal{D}\)-measurable. Then \(g(\mathbb{S}_{n}):\,(\Omega,\mathcal{A},P)\to(R,\mathcal{B})\), and both

\[g(\mathbb{S}_{n})\to_{p}g(\mathbb{S})\quad\text{ as }\ n\to\infty\qquad\text{ for the }\mathbb{S}_{n}\quad\text{of }(\ref{eq:g})\quad\text{and} \tag{12}\]

\[g(\mathbb{S}_{n})\to_{d}g(\mathbb{S})\quad\text{ as }\quad n\to\infty\qquad\text{for any }\mathbb{S}_{n}\,\text{ having the same distribution}. \tag{13}\]

(\(\mathcal{D}\)-measurability is typically trivial, and hypothesizing it avoids the measurability difficulties discussed in section 12.1.) [Theorem 8.2 allows (13) for \(\mathcal{D}\)-measurable functionals \(g\) that are continuous in other \(\|\cdot/q\|\)-metrics.]

**Proof.** Now, \(\|{\mathbb{S}}_{n}-{\mathbb{S}}\|\) is a \({\cal D}\)-measurable rv, and \(\|{\mathbb{S}}_{n}-{\mathbb{S}}\|\to_{p}0\) for the \({\mathbb{S}}_{n}\) of (5). Thus any subsequence \(n^{\prime}\) has a further subsequence \(n^{\prime\prime}\) for which \(\|{\mathbb{S}}_{n^{\prime\prime}}-{\mathbb{S}}\|\to 0\) for all \(\omega\notin A^{\prime\prime}\), where \(P(A^{\prime\prime})=0\). Moreover,

(a) \[P(A^{\prime\prime}\cup[{\mathbb{S}}\in\Delta])\leq P(A^{\prime\prime})+P({ \mathbb{S}}\in\Delta)=0,\]

and if \(\omega\notin A^{\prime\prime}\cup[{\mathbb{S}}\in\Delta]\), then \(g({\mathbb{S}}_{n^{\prime\prime}}(\omega))\to g({\mathbb{S}}(\omega))\) holds, as \(\|{\mathbb{S}}_{n^{\prime\prime}}(\omega)-{\mathbb{S}}(\omega)\|\to 0\) and \(g\) is \(\|\)\(\|\)-continuous at \({\mathbb{S}}(\omega)\). Thus \(g({\mathbb{S}}_{n})\to_{p}g({\mathbb{S}})\) as \(n\to\infty\) for the \({\mathbb{S}}_{n}\) of (5). Thus \(g({\mathbb{S}}_{n})\to_{d}g({\mathbb{S}})\) for the \({\mathbb{S}}_{n}\) of (5), and hence of (13) also. [Note that we are dealing only with functionals for which the compositions \(g({\cal S}_{n})\) and \(g({\cal S})\) are \((\Omega,{\cal A},P)\to(R,{\cal B})\) measurable.] \(\Box\)

**Example 8.1** : Since the functionals \(\|\cdot\|\) and \(\|\cdot^{+}\|\) are a.s. \(\|\cdot\|\)-continuous,

\[\|{\mathbb{S}}_{n}^{+}\|\to_{d}\|{\mathbb{S}}^{+}\|\quad\mbox{and}\quad\|{ \mathbb{S}}_{n}\|\to_{d}\|{\mathbb{S}}\|. \tag{14}\]

The limiting distributions are those given in theorem 7.1. \(\Box\)

**Exercise 8.1** : Let \(X_{0}\equiv 0\) and \(X_{1},X_{2},\ldots\) be iid \((0,\sigma^{2})\). Define \(S_{k}\equiv X_{1}+\cdots+X_{k}\) for each integer \(k\geq 0\).

(a) Find the asymptotic distribution of \((S_{1}+\cdots+S_{n})/c_{n}\) for an appropriate \(c_{n}\).

(b) Determine a representation for the asymptotic distribution of the "absolute area" under the partial sum process, as given by \((|S_{1}|+\cdots+|S_{n}|)/c_{n}\).

**The LIL**

Recall the (8.6.1) LIL for a single sequence of iid \(F(0,1)\) rvs \(X_{1},X_{2},\ldots\) with partial sums \(S_{n}\equiv X_{1}+\cdots+X_{n}\); that is

\[\overline{\lim}_{n\to\infty}\,|S_{n}|/\sqrt{2n\log\log n}=1\,\,\,\mbox{a.s.} \tag{15}\]

The two LILs for Brownian motion (recall (12.3.7) and (12.3.18)) are

\[\overline{\lim}_{t\to\infty}\,|{\mathbb{S}}(t)|/\sqrt{2t\log\log t}=1\,\,\, \mbox{a.s.}\,\,\mbox{and} \tag{16}\]

\[\overline{\lim}_{t\to 0}\,|{\mathbb{S}}(t)|/\sqrt{2t\log\log(1}/t)=1\,\,\,\mbox{a.s.} \tag{17}\]

**Notation 8.2** : Define stopping times \(T_{1},T_{2},\ldots\) (with \(T_{0}=0\)) having mean 1 for which the rvs

\[X_{k}\equiv{\mathbb{S}}(\tau_{k-1},\tau_{k}]\quad\mbox{are iid as $F$}. \tag{18}\]

Let \(\tau_{k}\equiv T_{0}+T_{1}+\cdots+T_{k}\) for \(k\geq 0\), and define the partial sums

\[S_{n}\equiv\sum\nolimits_{k=1}^{n}X_{k}={\mathbb{S}}(\tau_{n})={\mathbb{S}}(n)+ [{\mathbb{S}}(\tau_{n})-{\mathbb{S}}(n)]. \tag{19}\][Note that _this embedding differs_ from that in notation 8.1. This one is based on a single sequence of rvs \(X_{1},X_{2},\ldots\).] \(\Box\)

**Exercise 8.2** (The LIL): (a) First prove (15), while assuming that (16) is true. [Hint. By proposition 8.6.1, we want to show (roughly) that

\[\begin{array}{l}|\mathbb{S}(\tau_{n})-\mathbb{S}(n)|/\sqrt{2n\log\log n} \rightarrow_{a.s.}0\quad\mbox{or that}\\ |\mathbb{S}(\tau_{[t]})-\mathbb{S}(t)|/\sqrt{2t\log\log t}\rightarrow_{a.s.}0. \end{array} \tag{20}\]

We will now make rigorous this approach to the problem. First apply the SLLN to \(\tau[t]/t\) as \(t\rightarrow\infty\). Then define \(\Delta_{k}\equiv\sup\{|\mathbb{S}(t)-\mathbb{S}(t_{k})|\ :\ t_{k}\leq t\leq t_{k+1}\}\), with \(t_{k}\equiv(1+a)^{k}\) for some suitably tiny \(a>0\). Use a reflection principle and Mills' ratio to show that \(P\) (\(\Delta_{k}\geq\) (an appropriate \(c_{k}\))) \(<\infty\). Complete the proof using Borel-Cantelli.]

(b) Now that you know how to deal with the "blocks" \(\Delta_{k}\), model a proof of (16) on the proof of proposition 8.6.1.

**Proof for Embedding at a Rate\({}^{*}\)**

**Proof.** Consider theorem 8.2. Let \(d^{2}\equiv\mbox{Var}[T]\). Let \(\mbox{Log}k\equiv 1\vee(\log k)\). Let \(M\equiv M_{\epsilon}\) be specified below, and define

(a) \[A_{n}^{c}\equiv[\max_{1\leq k\leq n}|\sum\nolimits_{i=1}^{k}(T_{ni}-1)|/(d \sqrt{k}\mbox{Log}k)\geq 2M/d].\]

Then the monotone inequality gives

(b) \[\equiv[\max_{1\leq k\leq n}|\sum\nolimits_{i=1}^{k}Y_{ni}|\geq M/d],\]

where the \(Y_{ni}\)'s are independent with mean \(0\) and variance \((i\mbox{Log}^{2}i)^{-1}\). Thus the Kolmogorov inequality gives

\[\begin{array}{l}P(A_{n}^{c})\leq(d/M)^{2}\mbox{Var}[\sum\nolimits_{1}^{n}Y_ {ni}]=(d/M)^{2}\sum\nolimits_{1}^{n}(i\mbox{Log}^{2}i)^{-1}\\ \qquad\leq(d/M)^{2}\sum\nolimits_{1}^{\infty}(i\mbox{Log}^{2}i)^{-1}\equiv(d/M )^{2}v^{2}<\epsilon^{2}\quad\mbox{if}\quad M>dv/\epsilon\end{array} \tag{21}\]

(c) \[<\epsilon.\]

Thus

(d) \[P(B_{n})\equiv P\left(\max_{1\leq k\leq n}\frac{n^{\nu}|\mathbb{S}(\sum \nolimits_{1}^{k_{r}}T_{ni}/n)-\mathbb{S}(k/n)|}{(k/n)^{1/2-\nu}}\geq\frac{2M} {\sqrt{dv}}\right)\]\[\leq P(B_{n}\cap A_{n})+P(A_{n}^{c})\] \[\leq\sum\nolimits_{k=1}^{n}P\left(\left[\frac{|\mathbb{S}(\Sigma_{1} ^{k}T_{ni}/n)-\mathbb{S}(k/n)|}{[2M\sqrt{k}(\text{Log}k)/n]^{1/2}}\ \geq\right.\right.\] \[\qquad\qquad\qquad\left.\left.\frac{2M}{\sqrt{d}v}\frac{k^{1/2-\nu }}{\sqrt{n}}\frac{1}{[2M\sqrt{k}(\text{Log}k)/n]^{1/2}}\right]\cap A_{n}\right)+\epsilon\] \[\leq\sum\nolimits_{k=1}^{n}P(\sup\nolimits_{0\leq|r|\leq a}| \mathbb{S}(r+k/n)-\mathbb{S}(k/n)|/\sqrt{a}\geq b)+\epsilon\] with \(a\equiv 2M\sqrt{k}(\text{Log}k)/n\) (as in \(A_{n}\) in (a)), and with \(\geq b\) as on the right in (e) \[\leq 3\sum\nolimits_{k=1}^{n}P(\sup\nolimits_{0\leq r\leq a}|\mathbb{S}(t,t+r |/\sqrt{a}\geq b/3)+\epsilon\qquad\text{using (k) below}\] \[\leq 12\sum\nolimits_{k=1}^{n}P(N(0,1)\geq b/3)+\epsilon\qquad \text{by the reflection principle}\] \[\text{(i)} \leq 12\sum\nolimits_{k=1}^{n}\exp(-(b/3)^{2}/2)+\epsilon\qquad \text{by Mills' ratio}\] \[\leq 12\sum\nolimits_{k=1}^{n}\exp\left(-\frac{M}{9dv}\frac{k^{1/2 -2\nu}}{\text{Log}k}\right)+\epsilon\] \[\text{(j)} <2\epsilon,\]

if \(M\equiv M_{\epsilon}\) is large enough and if \(0\leq\nu<\frac{1}{4}\) (this final step holds, since \(\int_{0}^{\infty}\exp(-cx^{\delta})\)\(dx\to 0\) as \(c\to\infty\)). The inequality (g) used

\[\sup\nolimits_{0\leq|r|\leq a}|\mathbb{S}(r+k/n)-\mathbb{S}(k/n)|\] \[\leq\sup\nolimits_{0\leq r\leq a}|\mathbb{S}(r+k/n)-\mathbb{S}(k/ n)|+2\sup\nolimits_{0\leq r\leq a}|\mathbb{S}(r+k/n-a)-\mathbb{S}(k/n-a)|\]

[with \(t\) in (g) equal to \(k/n\) or \(k/n-a\), and with \(a\) as above (see (f))].

Now, \(P(B_{n})\leq 2\epsilon\) shows that (10) is true, provided that the sup over all of \([1/n,1]\) is replaced by the max over the points \(k/n\) with \(1\leq k\leq n\). We now "fill in the gaps". Thus (even a crude argument works here)

\[P(\sqrt{n}\max\nolimits_{1\leq k\leq n-1}\sup\nolimits_{0\leq t \leq 1/n}|\mathbb{S}(t+k/n)-\mathbb{S}(k/n)|/k^{1/2-\nu}\geq M)\] \[\leq\sum\nolimits_{k=1}^{n-1}P\left(\mathbb{S}\|_{0}^{1/n}\geq Mk ^{1/2-\nu}/\sqrt{n}\right)\] \[\leq 4\sum\nolimits_{k=1}^{n-1}P(N(0,1)\geq Mk^{1/2-\nu})\qquad \text{by the reflection principle}\] \[\leq 4\sum\nolimits_{k=1}^{n-1}\exp(-M^{2}k^{1-2\nu}/2)\qquad \qquad\text{by Mills' ratio}\] \[\text{(l)} <\epsilon,\]

if \(M\equiv M_{\epsilon}\) is large enough and if \(0\leq\nu<\frac{1}{2}\) (even). 

**Exercise 8.3** : Suppose \(\text{E}X^{4}<\infty\). Show that the process \(\mathbb{S}_{n}\) of (5) satisfies

\[(n^{1/4}/\log n)\|\mathbb{S}_{n}-\mathbb{S}\|=O_{p}(1). \tag{24}\]

[Hint. Replace \(n^{\nu}/(k/n)^{1/2-\nu}\) by \(n^{1/4}/\log n\) in the definition of \(B_{n}\) in (d). Now determine the new form of the bounds in (20) and (21).] [While interesting and often quoted in the literature, this formulation has little value for us.]

## 9 Other Properties of Brownian Motion \({}^{o}\)

Here we collect some selected sample path properties of Brownian motion, just to illustrate a sample of what is known. Some proofs are outlined in the exercises.

**Definition 9.1** (Variation)  For a sequence of _partitions_

\({\cal P}_{n}\equiv\{(t_{n,k-1},t_{nk}]\ :\ k=1,\ldots,n\}\) of \([0,1]\) (with \(0\equiv t_{n0}<\cdots<t_{nn}\equiv 1\)), define the \(r\)th _variation_ of \({\mathbb{S}}\) corresponding to \({\cal P}_{n}\) by

\[V_{n}(r)\equiv\sum\nolimits_{k=1}^{n}|{\mathbb{S}}(t_{nk})-{\mathbb{S}}(t_{n,k -1})|^{r}. \tag{1}\]

We call these partitions _nested_ if \({\cal P}_{n}\subset{\cal P}_{n+1}\) for all \(n\geq 1\). We further define the _mesh_ of the partitions to be \(\|{\cal P}_{n}\|\equiv\sup_{1\leq k\leq n}|t_{nk}-t_{n,k-1}|\).

**Theorem 9.1** (Nondifferentiability)

(a) Almost every Brownian path is nowhere differentiable.

(b) In fact, \(V_{n}(1)\to\infty\)  a.s. if \(\|{\cal P}_{n}\|\to 0\).

(c) (Finite squared variation) \(V_{n}(2)\to_{{\cal L}_{2}}1\) if \(\|{\cal P}_{n}\|\to 0\).

(d) (Finite squared variation) \(V_{n}(2)\to_{a.s.}1\)

if either (i) \(\sum_{n=1}^{\infty}\|{\cal P}_{n}\|<\infty\) or (ii) The \({\cal P}_{n}\) are nested with mesh approaching \(0\).

(e) (Dudley) \(V_{n}(2)\to_{a.s.}1\) if and only if \((\log n)\|{\cal P}_{n}\|\to 0\).

**Theorem 9.2** (Levy)  The Holder condition is true:

\[\limsup_{\{0\leq s<t\leq 1\ {\rm and}\ t-s=a\searrow 0\}}\frac{|{\mathbb{S}}(t)-{ \mathbb{S}}(s)|}{\sqrt{2a\log(1/a)}}=1\ \ {\rm a.s.} \tag{2}\]

**Theorem 9.3** (The zeros of \({\mathbb{S}}\) in \([0,1]\))  Define

\[{\rm Zeros}(\omega)\equiv\{t\in[0,1]\ :\ {\mathbb{S}}(t,\omega)=0\}.\]

For almost all \(\omega\), the set \({\rm Zeros}(\omega)\) is a closed and perfect set of Lebesgue measure zero. [A set \(A\) is called _dense in itself_ if every point \(x\) of \(A\) is such that every neighborhood of \(x\) contains another point of \(A\) beyond \(x\). A compact set that is dense in itself is called _perfect_.]

**Theorem 9.4** (Strassen)  Let \({\mathbb{Z}}_{n}(t)\equiv{\mathbb{S}}(nt)/\sqrt{2n\log\log n}\) for \(0\leq t\leq 1\). Let \({\cal K}\) be the set of absolutely continuous functions \(f\) on \([0,1]\) with \(f(0)=0\) and \(\int_{0}^{1}[f^{\prime}(t)]^{2}dt\leq 1\); equivalently,

\[{\cal K}\equiv\left\{f\in C[0,1]\ :\ f(0)=0,f(t)=\int_{0}^{t}f^{\prime}(s)ds, \int_{0}^{1}[f^{\prime}(s)]^{2}ds\leq 1\right\}.\]

For almost all \(\omega\) the sequence \(\{{\mathbb{Z}}_{n}(\cdot,\omega)\ :\ n=3,4,\ldots\}\) visualized within \(C[0,1]\) is relatively compact with limit set \({\cal K}\). That is,

\[P(\overline{\lim}_{n}\,\|{\mathbb{Z}}_{n}-{\cal K}\|=0)=1\ \ \ {\rm and}\ \ \ P(\cap_{f\in{\cal K}}[\underline{\lim}_{n}\,\|{\mathbb{Z}}_{n}-f\|=0])=1. \tag{3}\]

We will write this conclusion symbolically as \({\mathbb{Z}}\rightsquigarrow{\cal K}\). [This can be used to establish a LIL for various functionals \(g\) of \({\mathbb{S}}_{n}\), by determining the extreme values of \(g({\cal K})\).]

**Exercise 9.1** : Prove theorem 9.1 (a).

**Exercise 9.2** : (\(\alpha\)) Let \(Z\cong{\rm N}(0,1)\). Let \(r>0\). Show that:

(a) \(C_{r}\equiv{\rm E}|Z|^{r}=2^{r/2}\Gamma(\frac{r+1}{2})/\sqrt{\pi}\).

(b) \(|{\mathbb{S}}(t_{n,k-1},t_{nk}]|^{r}\cong(C_{r}|t_{nk}-t_{n,k-1}|^{r/2},(C_{2r} -C_{r}^{2})(t_{nk}-t_{n,k-1})^{r})\).

(\(\beta\)) Now show that \({\rm E}V_{n}(2)=1\quad\mbox{and}\quad{\rm Var}[V_{n}(2)]\leq(C_{2r}-C_{r}^{2}) \|{\cal P}_{n}\|\), giving

(c) \(\sum_{1}^{\infty}P(|V_{n}(2)-1|\geq\epsilon)\leq(C_{2r}-C_{r}^{2})\epsilon^{-2 }\sum_{1}^{\infty}\|{\cal P}_{n}\|<\infty\).

(\(\gamma\)) Finally, demonstrate the truth of theorem 9.1(d), case (i).

**Exercise 9.3** : Prove theorem 9.1(b) when all \(t_{nk}=k/2^{n}\).

[Hint. Let \(0<\lambda<1\). The Paley-Zygmund inequality gives

\[P(V_{n}(1)>\lambda{\rm E}V_{n}(1))\geq(1-\lambda)^{2}{\rm E}^{2}V_{n}(1)/{\rm E }(V_{n}^{2}(1))\to(1-\lambda)^{2},\]

where \({\rm E}V_{n}(1)\to\infty\).]

## 10 Various Empirical Process

Suppose that \(\xi_{n1},\ldots,\xi_{nn}\) are iid Uniform\((0,\,1)\). Their _empirical df_\(\mathbb{G}_{n}\) is defined by

\[\mathbb{G}_{n}(t)\equiv\frac{1}{n}\sum_{k=1}^{n}1_{[0,t]}(\xi_{nk}) \qquad\qquad\mbox{ for }0\leq t\leq 1 \tag{1}\]

\[=k/n\qquad\mbox{for}\quad\xi_{n:k}\leq t<\xi_{n:k+1}\quad\mbox{and}\quad 0 \leq k\leq n, \tag{2}\]

where \(0\equiv\xi_{n:0}\leq\xi_{n:1}\leq\cdots\leq\xi_{n:n}\leq\xi_{n:n+1}\equiv 1\) are the order statistics; see figure 10.1. Note that \(n\mathbb{G}_{n}(t)\cong\mbox{Binomial }(n,t)\cong(t,t(1-t))\). The Glivenko-Cantelli theorem shows that \(\mathbb{G}_{n}\) converges uniformly to the true df \(I\); that is,

\[\|\mathbb{G}_{n}-I\|\to_{a.s.}0\quad\mbox{(even for the present triangular array of $\xi_{nk}$'s)}. \tag{3}\]

(The Cantelli proof of the SLLN based on fourth moments shows that \(\mathbb{G}_{n}(t)\to_{a.s.}t\) for each fixed \(t\); even for triangular arrays. The rest of the proof is identical.) The _uniform empirical process_\(\mathbb{U}_{n}\) is defined by

\[\mathbb{U}_{n}(t)\equiv\sqrt{n}[\mathbb{G}_{n}(t)-t]=\frac{1}{\sqrt{n}}\sum_{k =1}^{n}[1_{[\xi_{nk}\leq t]}-t]\qquad\qquad\mbox{ for }0\leq t\leq 1. \tag{4}\]

This process is also pictured in figure 10.1. The means and covariances of \(\mathbb{U}_{n}\) are the same as those of Brownian bridge \(\mathbb{U}\), in that

\[\mbox{EU}_{n}(t)=0\quad\mbox{and}\quad\mbox{Cov}[\mathbb{U}_{n}(s),\mathbb{U}_{ n}(t)]=s\wedge t-st\qquad\mbox{for all }0\leq s,t\leq 1; \tag{5}\]

this follows easily from

\[\mbox{Cov}[1_{[0,s]}(\xi_{nk}),1_{[0,t]}(\xi_{nk})]=s\wedge t-st \qquad\qquad\qquad\qquad\mbox{ for}\quad 0\leq s,t\leq 1. \tag{6}\]

{Moreover, for any \(d_{k}\)'s and \(e_{k}\)'s we have immediately from this that

\[\mbox{Cov}[\sum\nolimits_{1}^{n}d_{k}1_{[0,s]}(\xi_{nk}),\sum\nolimits_{1}^{n }e_{k}1_{[0,t]}(\xi_{nk}))]=(\sum\nolimits_{1}^{n}d_{k}e_{k})\times[s\wedge t -st]; \tag{7}\]

we would have \(\sum\nolimits_{1}^{n}\mbox{E}[d_{k},e_{k}]\) instead, if these were rvs independent of the \(\xi_{nk}\)'s.} We note that \(\mathbb{G}_{n}^{-1}(t)\equiv\inf\{x\in[0,1]\ :\ \mathbb{G}_{n}(x)\geq t\}\) is left continuous, with

\[\mathbb{G}_{n}^{-1}(t)=\xi_{n:k}\quad\mbox{for }(k-1)/n<t\leq k/n \tag{8}\]

and \(\mathbb{G}_{n}^{-1}(0)=0\), as in figure 10.1. The _uniform quantile process_\(\mathbb{V}_{n}\) is defined by

\[\mathbb{V}_{n}(t)\equiv\sqrt{n}[\mathbb{G}_{n}^{-1}(t)-t]\quad\mbox{for}\quad 0 \leq t\leq 1. \tag{9}\]

The key identities relating \(\mathbb{U}_{n}\quad\mbox{and}\quad\mathbb{V}_{n}\) are (with \(I\) the identity function) the trivial

\[\mathbb{U}_{n}=-\mathbb{V}_{n}(\mathbb{G}_{n})+\sqrt{n}[\mathbb{G}_{n}^{-1} \circ\mathbb{G}_{n}-I]\quad\mbox{on}\quad[0,1], \tag{10}\]

\[\mathbb{V}_{n}=-\mathbb{U}_{n}(\mathbb{G}_{n}^{-1})+\sqrt{n}[\mathbb{G}_{n} \circ\mathbb{G}_{n}^{-1}-I]\quad\mbox{on}\quad[0,1]. \tag{11}\]

Note that

\[\|\mathbb{G}_{n}\circ\mathbb{G}_{n}^{-1}-I\|=1/n\quad\mbox{and}\quad\|\mathbb{ G}_{n}^{-1}\circ\mathbb{G}_{n}-I\|=[\max\nolimits_{1\leq k\leq n+1}\delta_{ nk}]; \tag{12}\]here \(\delta_{nk}\equiv(\xi_{n:k}-\xi_{n:k-1})\), for \(1\leq k\leq n+1\), denotes the \(k\)th of the \(n+1\)_uniform spacings_.

It is sometimes convenient to use the smoothed versions \(\check{\mathbb{G}}_{n}\quad\text{and}\quad\check{\mathbb{G}}_{n}^{-1}\) defined by

\[\check{\mathbb{G}}_{n}(\xi_{n:k})=k/(n+1)\quad\text{and}\quad\check{\mathbb{G}} _{n}^{-1}(k/(n+1))=\xi_{n:k}\qquad\text{for }0\leq k\leq n+1, \tag{13}\]

connected linearly between points. Upon occasion the _smoothed uniform quantile process_\(\check{\mathbb{V}}_{n}(t)\equiv\sqrt{n}[\check{\mathbb{G}}_{n}^{-1}(t)-t]\) is a useful variation on \(\mathbb{V}_{n}\). The Glivenko-Cantelli theorem implies that

\[\|\mathbb{G}_{n}-I\|\to_{a.s.}\ 0,\ \|\mathbb{G}_{n}^{-1}-I\|\to_{a.s.}\ 0,\ \|\check{\mathbb{G}}_{n}-I\|\to_{a.s.}\ 0,\ \|\check{\mathbb{G}}_{n}^{-1}-I\|\to_{a.s.}\ 0; \tag{14}\]

see figure 10.1. Coupling these with the identities (10) and (11) shows that

\[\|\mathbb{U}_{n}-\mathbb{U}\|\to_{p}0\quad\text{if and only if }\quad\| \mathbb{V}_{n}-\mathbb{V}\|\to_{p}0. \tag{15}\]

Let \(\mathfrak{c}_{n}\equiv(c_{n1},\ldots,c_{nn})^{\prime}\) denote a vector of known constants normalized so that

\[\frac{c_{n}.}{c_{n}.}\equiv\frac{1}{n}\sum_{k=1}^{n}c_{nk}=0\quad\text{and} \quad\sigma_{c,n}^{2}\equiv\frac{1}{n}\sum_{k=1}^{n}(c_{nk}-c_{n}.)^{2}=1, \qquad\text{and let} \tag{16}\]

We suppose that these constants also satisfy the _uan condition_

\[\max_{1\leq k\leq n}|c_{nk}-\bar{c}_{n}|/[\sqrt{n}\sigma_{c,n}]=\Big{[}\max_{1 \leq k\leq n}|c_{nk}|/\sqrt{n}\Big{]}\to 0\quad\text{as }n\to\infty. \tag{17}\]

The _weighted uniform empirical process_ is defined by

\[\mathbb{W}_{n}(t)\equiv\frac{1}{\sqrt{n}}\sum_{k=1}^{n}c_{nk}[1_{[\xi_{nk}\leq t ]}-t]\qquad\text{for }0\leq t\leq 1. \tag{18}\]

The \(\mathbb{W}_{n}\) process is pictured in figure 10.1. It is trivial from (7) that

\[\text{Cov}[\mathbb{W}_{n}(s),\mathbb{W}_{n}(t)]=s\wedge t-st\qquad\text{for }0 \leq s,t\leq 1. \tag{19}\]

It is easy to show that \(\mathbb{W}_{n}\to_{fd}\mathbb{W}\), where \(\mathbb{W}\) denotes another Brownian bridge, one that is independent of \(\mathbb{U}\).

Let \(\mathsf{R}_{n}\equiv(R_{n1},\ldots R_{nn})^{\prime}\) denote the _ranks_ of \(\xi_{n1},\ldots,\xi_{nn}\); and then denote the _antiranks_ by \(\mathsf{D}_{n}\equiv(D_{n1},\ldots,D_{nn})^{\prime}\). Then \(\mathsf{R}_{n}\) is a random permutation of the vector \((1,\ldots,n)^{\prime}\), while \(\mathsf{D}_{n}\) is the inverse permutation. These satisfy

\[\xi_{nD_{nk}}=\xi_{n:k}\quad\text{and}\qquad\xi_{nk}=\xi_{n:R_{nk}}. \tag{20}\]

As observed in example 7.5.3,

\[(\xi_{n:1},\ldots,\xi_{n:n})\quad\text{and}\qquad(R_{n1},\ldots,R_{nn})\qquad \text{are independent rvs.} \tag{21}\]

The empirical _finite sampling process_\(\mathbb{R}_{n}\) is defined by

\[\mathbb{R}_{n}(t)\equiv\frac{1}{\sqrt{n}}\sum_{k=1}^{[(n+1)t]}c_{nD_{nk}} \qquad\text{for }0\leq t\leq 1. \tag{22}\]

The \(\mathbb{R}_{n}\) process is also pictured in figure 10.1. The key identities are

\[\mathbb{W}_{n}=\mathbb{R}_{n}(\mathbb{G}_{n})\qquad\text{or}\qquad\mathbb{R}_{ n}=\mathbb{W}_{n}(\check{\mathbb{G}}_{n}^{-1})\quad\text{on }[0,1]. \tag{23}\]Figure 10.1 \(\mathbb{G}_{n},\mathbb{G}_{n}^{1},\mathbb{U}_{n},\mathbb{W}_{n}\) and \(\mathbb{R}_{n}\).

These identities give

\[\|\mathbb{W}_{n}-\mathbb{W}\|\to_{p}0\qquad\text{if and only if}\quad\|\mathbb{R}_{n}- \mathbb{R}\|\to_{p}0, \tag{24}\]

as with (15). Because of (21), we see that

\[\mathbb{R}_{n}\qquad\text{and}\qquad\mathbb{V}_{n}\qquad\text{are independent processes}. \tag{25}\]

We reiterate that

\[\mathbb{W}\qquad\text{and}\qquad\mathbb{V}=-\mathbb{U}\quad\text{are independent Brownian bridges}; \tag{26}\]

this is further corroborated, since (7) with \(\sum_{1}^{n}d_{k}e_{k}=\sum_{1}^{n}c_{nk}/n=0\) imply that the cross covariance

\[\operatorname{Cov}[\mathbb{U}_{n}(s),\mathbb{W}_{n}(t)]=0\qquad\text{for all }0\leq s,t\leq 1. \tag{27}\]

We will prove only part of theorems 10.1 and 10.3 (namely, that (28) holds). For the believability of the rest, we will rely on (28), our earlier proof that \(\mathbb{S}_{n}\) can be embedded at a rate, and the proof of theorem 10.2. (Shorack(1991) contains these proofs, written in the current style and notation.) See section 12.11 for proofs of theorems 10.2 and 10.4.

**Theorem 10.1** (Convergence of the uniform processes): We can define independent Brownian bridges \(\mathbb{U}=-\mathbb{V}\quad\text{and}\quad\mathbb{W}\) and row-independent Uniform(0, 1) rvs \(\xi_{n1},\ldots,\xi_{nn}\) on a common probability space \((\Omega,\mathcal{A},P)\) in such a way that

\[\|\mathbb{U}_{n}-\mathbb{U}\|\to_{p}0\qquad\text{and}\qquad\|\mathbb{V}_{n}- \mathbb{V}\|\to_{p}0, \tag{28}\]

\[\|\mathbb{W}_{n}-\mathbb{W}\|\to_{p}0\qquad\text{and}\qquad\|\mathbb{R}_{n}- \mathbb{R}\|\to_{p}0, \tag{29}\]

provided that the \(c_{nk}\)'s are uan with \(\bar{c}_{n}=0,\sigma_{c}^{2}=1\), and \(\bar{c}_{n}^{4}<\infty\).

**Theorem 10.2** (Pyke-Shorack): Let \(q>0\) on \((0,1)\,\text{be }\nearrow\) on \([0,\frac{1}{2}],\searrow\) on \([\frac{1}{2},1]\), and have \(\int_{0}^{1}[q(t)]^{-2}dt<\infty\). Then:

\[\|(\mathbb{U}_{n}-\mathbb{U})/q\|\to_{p}0\qquad\text{and}\qquad\|(\mathbb{V}_{ n}-\mathbb{V})/q\|\to_{p}0. \tag{30}\]

\[\|(\mathbb{W}_{n}-\mathbb{W})/q\|\to_{p}0\qquad\text{and}\qquad\|(\mathbb{R}_{ n}-\mathbb{W})/q\|\to_{p}0. \tag{31}\]

**Corollary 1** (Csorgo-Revesz): We may replace \(1/q\) in the previous theorem by \(K\), for any qf \(K\) having Var \([K(\xi)]<\infty\).

**Theorem 10.3** (Weighted approximation of the uniform processes): The embeddings of the previous theorem are such that for any \(0\leq\nu<\frac{1}{4}\) we have

(a) (M. Csorgo, S. Csorgo, Horvath, Mason)

\[\Delta_{\nu n}\equiv\|n^{\nu}(\mathbb{U}_{n}-\mathbb{U})/[I\wedge(1-I)]^{1/2- \nu}\|_{1/n}^{1-1/n}\ =O_{p}(1), \tag{32}\]

\[\bar{\Delta}_{\nu n}-\equiv\|n^{\nu}(\mathbb{V}_{n}-\mathbb{V})/[I\wedge(1-I)] ^{1/2-\nu}\|_{1/2n}^{1-1/2n}=O_{p}(1). \tag{33}\](b) (Shorack) Suppose \(\overline{\lim}\,c_{n}^{4}<\infty\). Then

\[\dot{\Delta}_{\nu n} \equiv\|n^{\nu}(\mathbb{W}_{n}-\mathbb{W})/[I\wedge(1-I)]^{1/2-\nu} \|_{1/n}^{1-1/n}=O_{p}(1), \tag{35}\] \[\ddot{\Delta}_{\nu n} \equiv\|n^{\nu}(\mathbb{R}_{n}-\mathbb{W})/[I\wedge(1-I)]^{1/2- \nu}\|_{1/2n}^{1-1/2n}=O_{p}(1). \tag{34}\]

[The supremum limits in (32) and (34) may be changed to \(c/n\) and \(1-c/n\) for any constant \(c>0\). This relates to exercise 10.3 below.]

**Theorem 10.4** (Weighted approximation of \(\mathbb{G}_{n}\); Mason): For _any_ realization of \(\mathbb{G}_{n}\), any \(n\geq 1\), any \(0<\nu<\frac{1}{2}\), and all \(\lambda>0\) we have

\[\Delta_{\nu n}^{0}\equiv\left\|\frac{n^{\nu}(\mathbb{G}_{n}-I)}{[I\wedge(1-I) ]^{1-\nu}}\right\|=O_{p}(1). \tag{36}\]

We may replace \(\mathbb{G}_{n}\) by \(\ddot{\mathbb{G}}_{n}^{-1}\) in (36).

**Example 10.1** (\(R\)-statistics): Consider the _simple linear rank statistics_

\[T_{n}\equiv\frac{1}{\sqrt{n}}\sum_{k=1}^{n}c_{nk}K\left(\frac{R_{nk}}{n+1} \right)=\frac{1}{\sqrt{n}}\sum_{k=1}^{n}K\left(\frac{i}{n+1}\right)c_{nD_{nk}} \tag{37}\]

\[=\int_{0}^{1}Kd\mathbb{R}_{n}=-\int_{0}^{1}\mathbb{R}_{n}dK, \tag{38}\]

where the last step holds if \(K=K_{1}-K_{2}\) with each \(K_{i}\nearrow\) and left continuous on \((0,1)\). As in (12.3.19), this suggests that

\[T_{n}\to_{p}\int_{0}^{1}\mathbb{W}dK\cong N(0,\mbox{Var}[K(\xi)]), \tag{39}\]

provided that the uan condition holds and provided that Var \([K_{i}(\xi)]<\infty\) for \(i=1,2.\) Indeed, this can be shown to be true. (Writing

\[T_{n}=-\int_{0}^{1}\mathbb{W}dK\oplus\|(\mathbb{R}_{n}-\mathbb{W})/q\|\int_{0 }^{1}[q(t)]^{-2}d|K|\]

provides a simple proof in case this integral is finite for some square integrable function \(q\) and for total variation measure \(d|K|.\)) We will return to this in section 15.2 below. \(\square\)

**Proof.** Consider \(\mathbb{V}_{n}\). We will represent our uniforms rvs as a normed sum of Exponential(1) rvs. Thus we really begin with a Skorokhod embedding of iid Exponential(1) rvs.

Let \(F(x)=1-\exp(-(x+1))\) for \(x\geq-1\), so that \(F\) is a \((0,1)\) df; and if \(X\cong F,\) then \(X+1\cong\mbox{Exponentil}(1)\). According to Skorokhod's embedding theorem, there exist row-independent rvs \(X_{n1},\ldots,X_{nn}\) with df \(F\) such that the partial sum process \(\mathbb{S}_{n}\) of the \(n\) th row satisfies \(\|\mathbb{S}_{n}-\mathbb{S}\|\to_{p}0\) for some Brownian motion \(\mathbb{S}\). We now define

\[\eta_{nk}\equiv k+X_{n1}+\cdots+X_{nk}\quad\mbox{and}\quad\xi_{n:k}\equiv\eta _{nk}/\eta_{n,n+1}\qquad\mbox{for $1\leq k\leq n+1$}. \tag{40}\]

It is an elementary exercise below to show that these \(\xi_{nk}\)'s are distributed as \(n\) row-independent Uniform(0, 1) order statistics. Let \(\mathbb{G}_{n}\) denote their empirical df and \(\mathbb{U}_{n}\) their uniform empirical process. The key identity relating \(\mathbb{V}_{n}\) to \(\mathbb{S}_{n}\) is

(a) \[\mathbb{V}_{n-1}\left(\frac{k}{n-1}\right)=\sqrt{n-1}\left[\frac{ \eta_{k}}{\eta_{n}}-\frac{k}{n-1}\right]\] \[=\frac{n}{\eta_{n}}\sqrt{\frac{n-1}{n}}\left[\frac{\eta_{k}-k}{ \sqrt{n}}-\frac{k}{n}\frac{\eta_{n}-n}{\sqrt{n}}\right]-\sqrt{n-1}\left[\frac{ k}{n-1}-\frac{k}{n}\right]\]\[=\frac{n}{\eta_{n}}\sqrt{\frac{n-1}{n}}[\mathbb{S}_{n}(k/n)-(k/n)\mathbb{S}_{n}(1) ]-\frac{1}{\sqrt{n-1}}\frac{k}{n}, \tag{41}\]

so that for \(0\leq t\leq 1\),

\[\mathbb{V}_{n-1}(t)=\frac{n}{\eta_{n}}\sqrt{\frac{n-1}{n}}[\mathbb{S}_{n}(I_{n }(t))-I_{n}(t)\mathbb{S}_{n}(1)]-\frac{1}{\sqrt{n-1}}I_{n}(t), \tag{42}\]

where \(I_{n}(t)\equiv k/n\) for \((k-1)/(n-1)<t\leq k/(n-1)\) and \(1\leq k\leq n-1\) with \(I_{n}(0)\equiv 0\) satisfies \(\|I_{n}-I\|\to 0\). Note that \(\eta_{n}/n\to_{p}1\) by the WLLN and that \(\|\mathbb{G}_{n}-I\|\to_{p}0\). Thus

\[\|\mathbb{V}_{n}-\mathbb{V}\|\to_{p}0\text{ for }\mathbb{V}\equiv\mathbb{S}-I \,\mathbb{S}(1) \tag{43}\]

follows from the identity (42), \(\|I_{n}-I\|\to 0\), and the fact that

(b) \[\|\mathbb{S}(I_{n})-\mathbb{S}\|\leq\|\mathbb{S}(I_{n})-\mathbb{S}(I_{n})\|+ \|\mathbb{S}(I_{n})-\mathbb{S}\|\leq\|\mathbb{S}_{n}-\mathbb{S}\|+\|\mathbb{S} (I)-\mathbb{S}\|\to_{p}0,\]

by continuity of all sample paths of the \(\mathbb{S}\) process.

All sample paths of \(\mathbb{V}\) are continuous, and the maximum jump size of \(|\mathbb{V}_{n}-\mathbb{V}|\) is bounded above by \([\sqrt{n}\max_{1\leq t\leq n+1}\delta_{ni}]\); so \(\|\mathbb{V}_{n}-\mathbb{V}\|\to_{p}0\) and (12) imply

\[[\sqrt{n}\max_{1\leq k\leq n+1}\delta_{ni}]=\sqrt{n}\|\mathbb{G}_{n}^{-1}\circ \mathbb{G}_{n}-I\|\to_{p}0\quad\text{as }n\to\infty. \tag{44}\]

Thus

\[\|\mathbb{U}_{n}-\mathbb{U}\|=\|-\mathbb{V}_{n}(\mathbb{G}_{n})+ \sqrt{n}[\mathbb{G}_{n}\circ\mathbb{G}_{n}^{-1}-I]+\mathbb{V}\|\] (c) \[\leq\|\mathbb{V}_{n}(\mathbb{G}_{n})-\mathbb{V}(\mathbb{G}_{n})\|+ \|\mathbb{V}(\mathbb{G}_{n})-\mathbb{V}\|+\sqrt{n}\|\mathbb{G}_{n}\circ\mathbb{ G}_{n}^{-1}-I\|\] (d) \[\leq\|\mathbb{V}_{n}-\mathbb{V}\|+\|\mathbb{V}(\mathbb{G}_{n})- \mathbb{V}\|+o_{p}(1)\] \[=\|\mathbb{V}(\mathbb{G}_{n})-\mathbb{V}\|+o_{p}(1)\] (e) \[=o_{p}(1),\]

using \(\|\mathbb{G}_{n}-I\|\to_{p}0\) and uniform continuity of the sample paths of V.

We will prove Mason's theorem in the next section. \(\square\)

**Exercise 10.1**: Establish the claim made just below (40).

**Example 10.2** (The supremum functionals): Suppose \(g:(D,\mathcal{D})\to(R,\mathcal{B})\) is an a.s. \(\|\cdot\|\)-continuous function. Then

\[g(\mathbb{U}_{n})\to_{p}g(\mathbb{U})\quad\text{and}\quad g(\mathbb{V}_{n}) \to_{p}g(\mathbb{V}) \tag{45}\]

for the special constructions of theorem 10.1. Moreover, convergence in distribution holds for any versions of these processes. Letting \(\#\) denote \(+,-\), or \(|\cdot|\), we can thus claim the convergence in distribution

\[\|\mathbb{U}_{n}^{\#}\|\to_{d}\|\mathbb{U}^{\#}\|\quad\text{and}\quad\int_{0}^ {1}\mathbb{U}_{n}^{2}(t)dt\to_{d}\int_{0}^{1}\mathbb{U}^{2}(t)dt \tag{46}\]

for any versions of these processes. These limiting distributions of \(\|\mathbb{U}^{\pm}\|\) were given in theorem 12.7.3, while that of \(\int_{0}^{1}\mathbb{U}^{2}(t)dt\) will be given in exercise 12.2. \(\square\)

**Exercise 10.2** (The two-sample uniform process)  (i) Let \(\mathbb{G}_{m}\)   and  \(\mathbb{H}_{n}\) be the empirical dfs of two independent Uniform(0, 1) special constructions. Let

\[\mathbb{U}_{m}=\sqrt{m}(\mathbb{G}_{m}-I)\quad\mbox{and}\quad\mathbb{V}_{n}\equiv \sqrt{n}(\mathbb{H}_{n}-I)\]

denote the corresponding empirical process, and let \(\lambda_{mn}\equiv n/(m+n)\). Then

\[\mathbb{W}_{mn}\equiv\sqrt{\frac{mn}{m+n}}(\mathbb{G}_{m}-\mathbb{H}_{n})=( \sqrt{\lambda_{mn}}\mathbb{U}_{m}-\sqrt{1-\lambda_{mn}}\mathbb{V}_{n})\]

has

\[\begin{array}{l}\|\mathbb{W}_{mn}-\mathbb{W}_{mn}^{0}\|\to_{p}0\mbox{ as }m \wedge n\to\infty,\quad\mbox{where}\\ \mathbb{W}_{mn}^{0}\equiv(\sqrt{\lambda_{mn}}\mathbb{U}-\sqrt{1-\lambda_{mn}} \mathbb{V})\mbox{ is a Brownian bridge.}\end{array}\]

We thus have \(\|\mathbb{W}_{mn}^{\#}\|\to_{d}\|\mathbb{W}^{\#}\|\) for Brownian bridge \(\mathbb{W}\). Write out the details. (ii) Now use a discrete reflection principle to compute the _exact_ distribution of \(P(\|\mathbb{W}_{mn}^{+}\|\geq a)\), and pass to the limit in the resulting expression to obtain (12.7.10). (This provides an alternative to the earlier method.) [Hint. Go through the order statistics of the combined sample from smallest to largest. If it is from sample 2, step up one unit as you go to the right one unit. If it is from sample 2, step down one unit as you go to the right one unit. In this way, perform a random walk from \((0,0)\) to \((2n,0)\). What is the chance you ever cross a barrier of height \(a\)?]

**Example 10.3** (The Kolmogorov-Smirnov and Cramer-von Mises statistics)   Let \(\xi_{n1},\ldots,\)\(\xi_{nn}\) be the iid Uniform \((0,1)\) rvs of the special construction, and let \(F\) denote an arbitrary df. Then \(X_{nk}\equiv F^{-1}(\xi_{nk}),1\leq k\leq n,\) are iid \(F\). Let \(\mathbb{F}_{n}\) denote the empirical df of \(X_{n1},\ldots,X_{nn}\) and let \(\mathbb{E}_{n}\) denote the empirical process defined by \(\mathbb{E}_{n}(x)\equiv\sqrt{n}[\mathbb{F}_{n}(x)-F(x)]\). Now, \(\mathbb{E}_{n}=\mathbb{U}_{n}(F)\). Thus (28) implies \(\|\mathbb{E}_{n}-\mathbb{U}(F)\|\leq\|\mathbb{U}_{n}-\mathbb{U}\|\to_{p}0\), where equality holds if \(F\) is continuous. Thus

\[\sqrt{n}D_{n}^{\#}\equiv\sqrt{n}\|(\mathbb{F}_{n}-F)^{\#}\|=\|\mathbb{U}_{n}^{ \#}\|\to d\|\mathbb{U}^{\#}\|\]

 Likewise, a change of variable allows elimination of \(F\), and gives

\[W_{n}^{2}\equiv\int n(\mathbb{F}_{n}-F)^{2}dF=\int_{0}^{1}\mathbb{U}_{n}^{2}(t )dt\to_{d}\int_{0}^{1}\mathbb{U}^{2}(t)dt\qquad\mbox{if $F$ is continuous.}\]

These statistics are used to test whether \(F\) is really the true df, and \(\sqrt{n}D_{n}^{\#}\)   and  \(W_{n}^{2}\) all measure how far the estimate \(\mathbb{F}_{n}\) of the true df differs from the hypothesized df \(F.\) [The percentage points of the asymptotic distributions of \(\sqrt{n}D_{n}^{\#}\)   and  \(W_{n}^{2}\), under the null hypothesis when \(F\) is really the true df, are available.]

Consider now the two-sample problem in which the rvs \(X_{nj}^{(i)}\equiv F^{-1}(\xi_{nj}^{(i)})\), for \(i=1,2\)   and \(1\leq j\leq n_{i}\), of independent special constructions have empirical dfs \(\mbox{F}_{n_{1}}^{(1)}\)   and  \(\mbox{F}_{n_{2}}^{(2)}\). Note that for independent uniform empirical processes

\[\sqrt{\frac{n_{1}n_{2}}{n_{1}+n_{2}}}[\mathbb{F}_{n_{1}}^{(1)}-\mbox{F}_{n_{2} }^{(2)}]=\sqrt{\frac{n_{2}}{n_{1}+n_{2}}}\mathbb{U}_{n_{1}}^{(1)}(F)-\sqrt{ \frac{n_{1}}{n_{1}+n_{2}}}\mathbb{U}_{n_{2}}^{(2)}(F)\equiv\mathbb{W}_{n_{1},n _{2}}(F)\]

\[=_{a}\mathbb{W}_{n_{1},n_{2}}(F)\quad\mbox{if $F$ is continuous,}\]where

\[\mathbb{W}_{n_{1},n_{2}}\equiv\sqrt{\frac{n_{1}}{n_{1}+n_{2}}}\mathbb{U}^{(1)}- \sqrt{\frac{n_{2}}{n_{1}+n_{2}}}\mathbb{U}^{(2)}\cong\mathbb{U}\quad\text{for all $n_{1}$ and $n_{2}$.} \tag{52}\]

This gives the asymptotic null distribution for the various supremum and integral functionals with which we have dealt, no matter which version of these processes is considered. \(\square\)

**Exercise 10.3**  Show that \(n\,\xi_{n:n}\to d\) Exponential(1).

**Exercise 10.4** (\(\int_{0}^{1}g\,d\mathbb{U}_{n}\))  Suppose \(\operatorname{Var}[g(\xi)]\quad\text{and}\quad\operatorname{Var}[h(\xi)]\) are finite.

(a) Show that there exist rvs (to be labeled \(\int_{0}^{1}g\,d\mathbb{U}\quad\text{and}\quad\int_{0}^{1}h\,d\mathbb{W}\)) for which

\[\int_{0}^{1}g\,d\mathbb{U}_{n}\to_{p}\int_{0}^{1}g\,d\mathbb{U}\quad\text{and} \quad\int_{0}^{1}h\,d\mathbb{W}_{n}\to_{p}\int_{0}^{1}h\,d\mathbb{W}. \tag{53}\]

(b) Show also that

\[\int_{0}^{1}g\,d\mathbb{V}_{n}\to_{p}-\int_{0}^{1}g\,d\mathbb{U}\quad\text{and} \quad\int_{0}^{1}h\,d\mathbb{R}_{n}\to_{p}\int_{0}^{1}h\,d\mathbb{W}. \tag{54}\]

**Exercise 10.5** (Mason)  Consider the \(\Delta_{n\nu}\) of (32). For some \(a>0\),

\[\sup_{n\geq 2}\operatorname{E}\exp(a\Delta_{n\nu})<\infty. \tag{55}\]

[Hint. This is too hard to be an "exercise," but it is a very nice bound.]

## 11 Inequalities for Various Empirical Processes \({}^{o}\)

We wish to apply the Birnbaum-Marshall and Hajek-Renyi inequalities to various martingales (mgs) associated with the processes of the previous section.

**Proposition 11.1** (Various martingales): \[\{\mathbb{U}_{n}(t)/(1-t):0\leq t<1\}\qquad\mbox{is a mg.}\] (1) \[\{\mathbb{W}_{n}(t)/(1-t):0\leq t<1\}\qquad\mbox{is a mg.}\] (2) \[\{\mathbb{U}(t)/(1-t):0\leq t<1\}\qquad\mbox{is a mg.}\] (3) \[\{\mathbb{V}_{n}(k/(n+1))\ :\ (1-k/(n+1))\ :\ 0\leq k\leq n\}\qquad \mbox{is a mg.}\] (4) \[\{\mathbb{R}_{n}(k/(n+1))/(1-k/n)\ :\ 0\leq k\leq n-1\}\qquad\qquad \mbox{is a mg.}\] (5)

**Proof.**  Let \({\cal A}_{t}\equiv\sigma[1_{[\xi\leq s]}\ :\ 0\leq s\leq t]\). Then

(a) \[\mbox{E}\{1_{[\xi\leq t]}-t|{\cal A}_{s}\}=1_{[\xi\leq s]}+ \frac{t-s}{1-s}1_{[\xi>s]}-t\] \[=1_{[\xi\leq s]}+\frac{t-s}{1-s}\{1-1_{[\xi\leq s]}\}-t\] (b) \[=\frac{1-t}{1-s}\{1_{[\xi\leq s]}-s\},\]

so that

\[[1_{[\xi\leq t]}-t]/(1-t),\qquad 0\leq t\leq 1,\quad\mbox{is a mg.} \tag{6}\]

Noting (12.10.7), summing (6) shows that (1) and (2) hold.

Let \({\cal A}_{t}\equiv\sigma[\mathbb{U}(s)\ :\ 0\leq s\leq t]\). Letting \(\Sigma_{st}\equiv\mbox{Cov}[\mathbb{U}(s),\mathbb{U}(t)]=s\wedge t-st\),

(c) \[\mbox{E}(\mathbb{U}(t)|{\cal A}_{s})=\{[s(1-t)]/[s(1-s)]\}\mathbb{U}(s),\]

since \(\mathbb{U}(t)|\mathbb{U}(s)\) is normal with mean \(\mu_{t}+\Sigma_{ts}\Sigma_{ss}^{-1}[\mathbb{U}(s)-\mu_{s}]\). Thus (3) holds.

Consider (5). Let \(Z_{nk}\equiv\mathbb{R}_{n}(k/(n+1))/(1-k/n)\), and set \(\Delta Z_{nk}\equiv Z_{nk}-Z_{n,k-1}\) for integers \(1\leq k\leq n-1\). Then

\[\Delta Z_{nk}=\frac{n}{n-k}\mathbb{R}\left(\frac{k}{n+1}\right)- \frac{n}{n-k}\frac{n-k}{n-k+1}\mathbb{R}\left(\frac{k-1}{n+1}\right)\] (d) \[=\frac{1}{\sqrt{n}}\frac{n}{n-k}\left[c_{nD_{nk}}+\frac{1}{n-k+1}{\sum_{j=1} ^{k-1}}c_{nD_{nj}}\right].\]

Let \({\cal A}_{k}\equiv\sigma[D_{n1},\ldots,D_{nk}]\). Then

\[\mbox{E}(\Delta Z_{nk}|{\cal A}_{k-1})=\frac{1}{\sqrt{n}}\frac{n} {n-k}[\mbox{E}(c_{nD_{nk}}|{\cal A}_{k-1})+\frac{1}{n-k+1}{\sum_{j=1}^{k-1}}c_ {nD_{nj}}]\] (e) \[=\frac{1}{\sqrt{n}}\frac{n}{n-k}\Big{[}\frac{1}{n-k+1}{\sum_{j=k} ^{n}}c_{nD_{nj}}+\frac{1}{n-k+1}{\sum_{j=1}^{k-1}}c_{nD_{nj}}\Big{]}\] \[=0,\quad\mbox{since }\bar{c}_{n}=0.\]Apply the finite sampling results (A.1.8) and (A.1.9) to (d) to conclude that

(f) \[\text{Var}\;[\Delta Z_{nk}]=\frac{\sigma_{c,n}^{2}}{n}\Big{(}\frac{n} {n-k}\Big{)}^{2}\Big{\{}1-\frac{2}{n-k+1}\frac{k-1}{n-1}+\frac{k-1}{(n-k+1)^{2} }\Big{[}1-\frac{(k-1)-1}{n-1}\Big{]}\Big{\}}\] (7) \[=\frac{\sigma_{c,n}^{2}}{n-1}\frac{n^{2}}{(n-k)(n-k+1)}.\]

Thus (5) holds. Consider (4).

Let \(\mathcal{A}_{k}\equiv\sigma[\xi_{n:1},\ldots\xi_{n:k}]\). Then

(g) \[\text{E}(\xi_{n:k}|\mathcal{A}_{i})-\frac{k}{n+1}=\xi_{n:i}+\frac{k-i}{n-i+1}[ 1-\xi_{n:i}]-\frac{k}{n+1}=\frac{n-k+1}{n-i+1}\Big{[}\xi_{n:i}-\frac{i}{n+1} \Big{]},\]

since the conditional distribution of \(\xi_{n:k}\) given \(\xi_{n:i}\) is that of the \((k-i)\)th order statistic in a sample of size \(n-i\) from Uniform \((\xi_{n:i},1)\), and (A.1.32) can be applied. Thus (4) holds. 

##### Inequality 11.1 (Pyke-Shorack)

Let \(\mathbb{X}\) denote one of the processes \(\mathbb{U}_{n},\breve{\mathbb{V}}_{n},\mathbb{W}_{n},\mathbb{R}_{n}\), or \(\mathbb{U}\). Let \(q>0\) on \([0,\theta]\) be \(\diagup\) and right continuous. Then for all \(\lambda>0\) we have the probability bound

\[P(\|\mathbb{X}/q\|_{0}^{\theta}\geq\lambda)\leq(16/\lambda^{2})\int_{0}^{ \theta}[q(t)]^{-2}dt. \tag{8}\]

**Proof.** Let \(\mathbb{X}\) denote any one of \(\mathbb{U}_{n},\mathbb{W}_{n}\), or \(\mathbb{U}\). Then \(\mathbb{X}(t)/(1-t)\) is a mg with mean \(0\) and variance \(\nu(t)\equiv t/(1-t)\). Thus the Birnbaum-Marshall inequality gives

\[P(\|\mathbb{X}(t)/q(t)\|_{0}^{\theta}\geq\lambda)=P(\|[\mathbb{X}(t)/(1-t)]/[q (t)/(1-t)]\|_{0}^{\theta}\geq\lambda)\]

(a) \[\leq(4/\lambda)^{2}\int_{0}^{\theta}[(1-t)/q(t)]^{2}d[t/(1-t)]=(4/\lambda)^{2} \int_{0}^{\theta}[q(t)]^{2}dt.\]

Let \(\mathbb{X}\) denote \(\mathbb{R}_{n}\). Then, with \(b_{k}\equiv q(k/(n+1))\quad\text{and}\quad m\equiv[(n+1)\theta]\),

\[P(\|\mathbb{R}_{n}(t)/q(t)\|_{0}^{\theta}\geq\lambda)=P(\max_{1\leq k\leq m}| \mathbb{R}_{n}(k/(n+1))|/b_{k}\geq\lambda)\]

(b) \[\leq P\Big{(}\max_{1\leq k\leq m}\frac{|\mathbb{R}_{n}(k/(n+1))/(1-k/n)|}{b_{ k}/(1-k/n)}\geq\lambda\Big{)}\]

(c) \[\leq\frac{4}{\lambda^{2}}\sum_{k=1}^{m}b_{k}^{-2}\leq\frac{16}{\lambda^{2}}\int _{0}^{\theta}[q(t)]^{-2}dt.\]

(We can improve (a) and (c) by a factor of \(4\), as stated in the Hajek-Renyi inequality, but there is no real point to this.) \(\square\)

##### Exercise 11.1

Verify (8) for \(\breve{\mathbb{V}}_{n}\).

**Inequality 11.2** (In probability linear bounds on \({\mathbb{G}}_{n}\) and \({\mathbb{G}}_{n}^{-1}\)): For all \(\epsilon>0\) there exists \(\lambda\equiv\lambda_{\epsilon}\) so small that the event \(A_{n\epsilon}\) on which

\[{\mathbb{G}}_{n}(t)\leq t/\lambda\quad\mbox{on }[0,1], {\mathbb{G}}_{n}(t)\geq\lambda t\quad\mbox{on }[\xi_{n:1},1], \tag{10}\] \[{\mathbb{G}}_{n}(1-t)\leq 1-\lambda(1-t)\quad\mbox{on }[0,\xi_{n:n}), \quad{\mathbb{G}}_{n}(1-t)\geq 1-(1-t)/\lambda\quad\mbox{on }[0,1],\] (11) \[|{\mathbb{G}}_{n}(t)-t|\leq 1/(\lambda\sqrt{n})\quad\mbox{on }[0,1] \tag{9}\]

has \(P(A_{n\epsilon})\geq 1-\epsilon\) for all \(n\geq 1\). Let \(1_{n\epsilon}\) denote the indicator function of \(A_{n\epsilon}\). (These conclusions hold for _any_ realization of \({\mathbb{G}}_{n}\quad\mbox{and}\quad{\mathbb{G}}_{n}^{-1}\).) (Note that linear bounds on \({\mathbb{G}}_{n}^{-1}\) are also established by this result.)

**Proof.** Now, \((\xi_{n:1},\ldots\xi_{n:n})\) has joint density \(n!\) on its domain. Thus

(a) \[\begin{array}{l}P({\mathbb{G}}_{n}(t)\leq t/\lambda\mbox{ for }0\leq t \leq 1)=P(\xi_{n:k}\geq\lambda k/n\mbox{ for }1\leq k\leq n)\\ =\int_{\lambda}^{1}\int_{\lambda(n-1)/n}^{t_{n}}\cdots\int_{\lambda 2/n}^{t_{3}}\int_{ \lambda/n}^{t_{2}}n!\,dt_{1}\cdots dt_{n}=\cdots\end{array}\] (b) \[\begin{array}{l}=n!\left[\frac{t^{n}}{n!}-\frac{\lambda t^{n-1}}{n!} \right]\bigg{|}_{\lambda}^{1}\ =1-\lambda\\ \geq 1-\epsilon/3\quad\mbox{for all }\lambda\leq\lambda_{\epsilon}\equiv \epsilon/3,\end{array}\] (c)

and for all \(n\). This gives the upper bound of (9). And (8.3.20) gives

**Daniels' equality**

\[\begin{array}{l}P(\|{\mathbb{G}}_{n}/I\|\leq\lambda)=P({\mathbb{G}}_{n}(t) \leq t/\lambda\quad\mbox{for }0\leq t\leq 1)\\ =P(\xi_{n:k}\geq\lambda k/n\quad\mbox{for }1\leq k\leq n)=P({\mathbb{G}}_{n}^{-1} (t)\geq\lambda t\quad\mbox{for all }0\leq t\leq 1)\\ =1-\lambda\quad\mbox{for all }0\leq\lambda\leq 1.\end{array} \tag{12}\]

We now turn to the lower bound of (9). Now,

\[S_{k}\equiv n\,\xi_{n:k+1}/k,\qquad 1\leq k\leq n-1,\quad\mbox{is a reversed mg}, \tag{13}\]

as a rearrangement of \({\rm E}(\xi_{n:k}|\xi_{n:k+1})=[k/(k+1)]\xi_{n:k+1}\) shows. So, immediately,

(d) \[\tilde{S}_{k}\equiv S_{(n-1)-k+1}=S_{n-k}=n\,\xi_{n:n-k+1}/(n-k)\quad\mbox{is a mg for }1\leq k\leq n-1.\]

Now calculate

\[\begin{array}{l}1-P({\mathbb{G}}_{n}(t)\geq\lambda t\,\mbox{everywhere on }[\xi_{n:1},1])\ [\mbox{or }=P(\|I/{\mathbb{G}}_{n}\|_{\xi_{n1}}^{1}>1/\lambda)]\\ =P(\xi_{n:k+1}>(k/n)/\lambda\quad\mbox{for some }1\leq k\leq n-1)\\ =P(\max_{1\leq k\leq n-1}S_{k}>1/\lambda)=P(\max_{1\leq k\leq n-1}\tilde{S}_{k }>1/\lambda)\\ =P(\max_{1\leq k\leq n-1}\exp(r\tilde{S}_{k})>\exp(r/\lambda))\end{array}\] (e)

[MISSING_PAGE_FAIL:355]

## 12 Applications \({}^{o}\)

**Theorem 12.1 (Donsker)** Let \(g:(D,{\cal D})\to(R,{\cal B})\) denote an a.s.\(\|\cdot\|\)-continuous mapping that is \({\cal D}\)-measurable. Then \(g({\mathbb{U}}_{n})\) : \((\Omega,{\cal A},P)\to(R,{\cal B})\), and

\[g({\mathbb{U}}_{n})\to_{p}g({\mathbb{U}})\quad\mbox{as}\quad n\to\infty\quad \mbox{for the ${\mathbb{U}}_{n}$}\quad\mbox{of $(\ref{eq:12.10.28})$}, \tag{1}\]

\[g({\mathbb{U}}_{n})\to_{d}g({\mathbb{U}})\quad\mbox{as $n\to\infty$}\quad\mbox{ for an arbitrary ${\mathbb{U}}_{n}$}. \tag{2}\]

[These conclusions hold for \({\cal D}\)-measurable functionals \(g\) that are continuous in other \(\|\cdot/q\|\)-metrics as well.]

**Exercise 12.1**  Write out the easy details to prove this Donsker theorem.

**Example 12.1 (Tests of fit)** (i) Call \(F\)_stochastically larger_ than \(F_{0}\) when \(P_{F}(X>x)\geq P_{F_{0}}(X>x)\) for all \(x\) (with strict inequality for at least one \(x\)), and write \(F\geq_{s}F_{0}\). Then to test the null hypothesis \(H_{0}\) that \(F=F_{0}\) is true against the alternative hypothesis \(H_{a}\) that \(F\geq_{s}F_{0}\) it is reasonable to reject the truth of the \(H_{0}\) claim for large values of _Birnbaum's statistic_\(Z_{n}\equiv\int_{-\infty}^{\infty}\sqrt{n}[{\mathbb{F}}_{n}(x)-F_{0}(x)]dF_{0}(x)\). Now suppose that \(H_{0}\) is true, with a continuous df \(F_{0}\). Then

\[Z_{n}\cong\int_{-\infty}^{\infty}{\mathbb{U}}_{n}(F_{0})dF_{0}=\int_{0}^{1}{ \mathbb{U}}_{n}(t)dt\to_{d}Z\equiv\int_{0}^{1}{\mathbb{U}}(t)dt\cong N\left(0, \tfrac{1}{12}\right). \tag{3}\]

(ii) Alternatively, one could form the _Cramer-von Mises statistic_

\[W_{n}\equiv\int_{-\infty}^{\infty}\{\sqrt{n}[{\mathbb{F}}_{n}(x) -F_{0}(x)]\}^{2}dF_{0}(x)\] \[\cong\int_{-\infty}^{\infty}{\mathbb{U}}_{n}^{2}(F_{0})dF_{0}\quad \mbox{by (\ref{eq:12.22})}\] \[=\int_{0}^{1}{\mathbb{U}}_{n}^{2}(t)dt\quad\mbox{when $F_{0}$ is continuous, by (\ref{eq:12.10})} \tag{4}\]

\[\to_{d}\int_{0}^{1}{\mathbb{U}}^{2}(t)dt\] \[=\int_{0}^{1}\{\sum_{k=1}^{\infty}\phi_{k}(t)\tfrac{1}{\pi k}Z_{ k}\}\{\sum_{j=1}^{\infty}\phi_{j}(t)\tfrac{1}{\pi j}Z_{j}\}dt\quad\mbox{(see below)}\] \[\quad\mbox{for the orthonormal functions $\phi_{k}(t)\equiv\sqrt{2}\sin(\pi kt)$ on $[0,1]$}\] \[\quad\mbox{and iid N(0,1) rvs $Z_{k}$}\] \[=\sum\nolimits_{k=1}^{\infty}\sum\nolimits_{j=1}^{\infty}\frac{1 }{\pi^{2}jk}Z_{j}Z_{k}\int_{0}^{1}\phi_{k}(t)\phi_{j}(t)dt \tag{6}\] \[=\sum\nolimits_{k=1}^{\infty}\frac{1}{\pi^{2}k^{2}}Z_{k}^{2}. \tag{5}\]

This shows that \(W_{n}\) is asymptotically distributed as an infinite weighted sum of independent \(\chi_{17}^{2}\)rvs. This representation of the limiting distribution has been used to provide tables. If \(W_{n}\equiv W_{n}(F_{0})\) is computed but a different df \(F\) is true, then

\[\frac{1}{n}W_{n}=\int_{-\infty}^{\infty}[{\mathbb{F}}_{n}-F_{0}]^{2}dF_{0}\to_ {a.s.}\ \int_{-\infty}^{\infty}[F-F_{0}]^{2}dF_{0}>0. \tag{7}\]

[In statistical parlance, this shows that the \(W_{n}\)-test is consistent against any df alternative having \(F\neq F_{0}\).](iii) A third possibility is the _Anderson-Darling statistic_

\[A_{n}\equiv\int_{-\infty}^{\infty}\frac{\{\sqrt{n}[\mathbb{F}_{n}-F_{0}] \}^{2}}{F_{0}(1-F_{0})}dF_{0}=\int_{0}^{1}\frac{\mathbb{U}_{n}^{2}(t)}{t(1-t)}dt\] \[\quad\mbox{for $F_{0}$ continuous} \tag{9}\] \[\to d\int_{0}^{1}\frac{\mathbb{U}^{2}(t)}{t(1-t)}dt\cong\sum_{k=1}^{\infty} \frac{1}{k(k+1)}Z_{k}^{2}. \tag{8}\]

**Proof.** (i) Consider Birnbaum's \(Z_{n}\).

Method 1: By (6.5.27) and then the change of variable theorem of (6.3.10) (with identity function \(H\)) one obtains the first two steps of (3). Apply Donsker for the third step. Appeal to (12.3.19) for the \(\to d\) to a normal rv \(Z\). Finally, appeal to Fubini's theorem for both

(a) \[\mbox{E}Z=\mbox{E}\int_{0}^{1}\mathbb{U}(t)dt=\int_{0}^{1}\mbox{E}( \mathbb{U}(t))dt=\int_{0}^{1}0\,dt=0\qquad\mbox{and}\] \[\mbox{E}(Z^{2})=\mbox{E}\{\int_{0}^{1}\int_{0}^{1}\mathbb{U}(s) \mathbb{U}(t)ds\,dt\}\] \[=\int_{0}^{1}\int_{0}^{1}\mbox{E}\{\mathbb{U}(s)\mathbb{U}(t)\}ds \,dt=\int_{0}^{1}\int_{0}^{1}[s\wedge t-st]ds\,dt\] (b) \[=\int_{0}^{1}\int_{0}^{t}s(1-t)ds\,dt=1/12.\]

Method 2: Apply (12.10.28) for

(c) \[\int_{0}^{1}|\mathbb{U}_{n}(t)-\mathbb{U}(t)|dt\leq\int_{0}^{1}1\,dt\times\| \mathbb{U}_{n}-\mathbb{U}\|\to_{p}0.\]

Thus \(Z_{n}\to_{d}Z\).

The rest of the justification of example 12.1 is outlined in exercises 12.2 and 12.3. \(\square\)

**Exercise 12.2** Consider the Cramer-von Mises statistic \(W_{n}\).

(I) Verify step (5). Use (12.10.28).

(II) We now seek to justify the step representing \(\mathbb{U}\) as an infinite series. To this end formally write

(p) \[\mathbb{U}(t)=\sum_{1}^{\infty}\phi_{k}(t)\frac{1}{\pi k}Z_{k}\]

for iid N\((0,1)\) rvs \(Z_{k}\) and the orthonormal functions \(\phi_{k}(\cdot)\). First recall the group of trigonometric identities

(q) \[\begin{array}{l}\sin(A+B)=\sin\;A\;\cos B+\cos\;A\;\sin B,\\ 2\sin A\cos B=\sin(A+B)+\sin(A-B),\\ \cos(A+B)=\cos\;A\;\cos B-\sin\;A\;\sin B,\\ 2\cos A\sin B=\sin(A+B)-\sin(A+B)\end{array}\]

Use these to verify that \(\int_{0}^{1}\phi_{j}(t)\phi_{k}(t)dt\) equals 0 or 1 according as \(j\neq k\) or \(j=k\). [Think of this formal \(\mathbb{U}(t)\) as an odd function on [-1, 1], and thus only these \(\phi_{k}(\cdot)\) are needed.] Then note that the Fourier coefficients and the Fourier series are

(r) \[\langle\mathbb{U},\phi_{k}\rangle=\int_{0}^{1}\mathbb{U}(t)\phi_{k}(t)dt=\frac {1}{\pi k}Z_{k},\](s) \(\qquad\mathbb{U}=\sum_{k=1}^{\infty}\langle\mathbb{U},\phi_{k}\}\phi_{k}=\sum_{k=1 }^{\infty}\phi_{k}\frac{1}{\pi k}Z_{k}.\)

So, verify that the series in (p) converges a.s. and then everything so far for the formal \(\mathbb{U}\) is rigorous. Then Parseval's identity (note theorem B.3.3) gives

(t) \(\qquad\int_{0}^{1}\mathbb{U}^{2}(t)dt=\|\mathbb{U}\|^{2}=\sum_{1}^{\infty}| \langle\mathbb{U},\phi_{k}\}|^{2}=\sum_{1}^{\infty}\frac{1}{\pi^{2}k^{2}}Z_{k} ^{2}.\)

Finally, one needs to verify the step (u) in the identity

\(\begin{array}{l}\mathrm{E}\left\{\sum_{j=1}^{\infty}\phi_{j}(s)\frac{1}{\pi j }Z_{j}\times\sum_{k=1}^{\infty}\phi_{k}(t)\frac{1}{\pi k}Z_{k}\right\}=\sum_{1 }^{\infty}\frac{1}{\pi^{2}k^{2}}\phi_{k}(s)\phi_{k}(t)\\ \quad=\frac{2}{\pi^{2}}\sum_{1}^{\infty}\frac{1}{k^{2}}\sin(\pi ks)\sin(\pi kt )\\ \quad=s\wedge t-st,\end{array}\)

and thus the (originally formal) process \(\mathbb{U}\) is in fact a Brownian bridge. Where did this idea come from? Verifying that

(v) \(\qquad\int_{0}^{1}\mathrm{Cov}\ [s,\ t]\phi_{k}(s)ds=\frac{1}{\pi k}\phi_{k}(t)\) on \([0,1]\)

shows that \(\mathrm{Cov}[s,t]\equiv\mathrm{Cov}[\mathbb{U}(s),\mathbb{U}(t)]=s\wedge t-st\) has eigenvalues \(\frac{1}{\pi k}\) with associated eigenfunctions \(\phi_{k}(\cdot)\) for \(k=1,2,\ldots\). [Recall the spectral decomposition of matrices in (A.3.2)-(A.3.4).]

**Exercise 12.3**  Verify the results claimed for the Anderson-Darling statistic \(A_{n}\). [Verifying \(\to_{d}\) will be a little trickier this time, since (12.10.30) will now be needed in place of (12.10.28).] The rest is roughly similar in spirit, but the details are now a geat deal more complicated. Fundamentally, one must now represent the covariance function

\[\mathrm{Cov}[s,t]=(s\wedge t-st)/\sqrt{s(1-s)t(1-t)}\]

as a convergent infinite series of orthonormal functions. (Hopefully, at least the approach is now clear. Providing the details is hard work.)

## Chapter 13 Martingales

### 1 Basic Technicalities for Martingales

**Notation 1.1** We will work with processes on the following time sets \(I:\{0,\ldots,n\},\{0,1,\ldots\},\)\(\{0,1,\ldots,\infty\},\{\ldots,-1,0\},\{-\infty,\ldots,-1,0\}\) in the discrete case and \([0,t],[0,\infty),[0,\infty],\)\((-\infty,0],[-\infty,0]\) in the continuous case. In the continuous cases we will consider only processes of the type \(X:\,(\Omega,\mathcal{A},P)\to(D_{I},\mathcal{D}_{I})\) that are adapted to an \(\nearrow\) sequence of sub \(\sigma\)-fields \(\mathcal{A}_{t}\) of \(\mathcal{A}\). We will use the notation \(\{a_{n}\}_{n=0}^{\infty},\{a_{n}\}_{n=0}^{\overline{\infty}},\{a_{n}\}_{n=- \infty}^{0},\{a_{n}\}_{n=-\infty}^{0}\) to denote sequences over \(\{0,1,\ldots\},\{0,1,\ldots,\infty\},\{\). \(\ldots\,,-1,0\},\{-\infty,\ldots,-1,0\},\) respectively. \(\Box\)

**Definition 1.1** (Martingale and submartingale): Suppose \(\mathrm{E}|X_{t}|<\infty\) for all \(t.\) Call the rvs \(\{X_{t},\,\mathcal{A}_{t}\}_{t\in I}\) a _martingale_ (abbreviated \(mg\)) if

\[E(X_{t}|\mathcal{A}_{s})=X_{s}\quad\mbox{a.s.}\quad\mbox{for each pair $s\leq t $ in $I$.} \tag{1}\]

Call \(\{X_{t},\mathcal{A}_{t}\}_{t\in I}\) a _submartingale_ (abbreviated _submg_) if

\[\mathrm{E}(X_{t}|\mathcal{A}_{s})\geq X_{s}\quad\mbox{a.s.}\quad\mbox{for each pair $s\leq t$ in $I$.} \tag{2}\]

(If the inequality in (2) is reversed, the process \(\{X_{t},\mathcal{A}_{t}\}_{t\in T}\) is then called a _super-martingale_.) When the index set \(I\) is a subset of the negative numbers \([-\infty,0]\), we refer to such a process as a _reversed martingale_ or _reversed submartingale_.

[Most results in the first seven sections of this chapter are due to Doob.]

##### Basic Technicalities

**Proposition 1.1** (Equivalence): Now, \(\{X_{t},\mathcal{A}_{t}\}_{t\in I}\) is a submg if and only if the moments \(\mathrm{E}|X_{t}|<\infty\) for all \(t\in I\) and for every pair \(s\leq t\) we have

\[\int_{A}(X_{t}-X_{s})dP\geq 0\quad\mbox{ for all }\ A\in\mathcal{A}_{s}. \tag{3}\]

Similarly, \(\{X_{t},\mathcal{A}_{t}\}_{t\in I}\) is a mg if and only if equality holds in (3).

**Notation**: As in section 8.9, we combine these two statements by writing

\[\{X_{t},\mathcal{A}_{t}\}_{t\in I}\mbox{ is a s-mg}\quad\mbox{iff}\quad\int_{A}(X_{t}-X_{s})dP\geqq 0\quad\mbox{for all }A\in\mathcal{A}_{s}. \tag{4}\]

**Proof.** As in section 8.9, for each pair \(s\leq t,\)

(a) \(\int_{A}(X_{t}-X_{s})dP=\int_{A}\mathrm{E}(X_{t}-X_{s}|\mathcal{A}_{s})dP\geqq 0\) for all \(A\in\mathcal{A}_{s}\)

if and only if \(\mathrm{E}(X_{t}|\mathcal{A}_{s})-X_{s}=\mathrm{E}(X_{t}-X_{s}|\mathcal{A}_{s})\geqq 0\) a.s. \(\Box\)

**Definition 1.2**:
1. Call \(\{X_{t},{\cal A}_{t}\}_{t\in I}\)_integrable_ if \(\sup\{{\rm E}|X_{t}|\ :\ t\in I\}<\infty\).
2. If \(\{X_{t}^{c},{\cal A}_{t}\}_{t\in I}\) is integrable, then \(\{X_{t},{\cal A}_{t}\}_{t\in I}\) is called _square-integrable_.

**Proposition 1.2**:
1. Let \(\phi:\)\((R,{\cal B})\to(R,{\cal B})\) have \({\rm E}|\phi(X_{t})|<\infty\) for all \(t\in I\).
2. If \(\phi\) is convex and \(\{X_{t},{\cal A}_{t}\}_{t\in I}\) is a mg, then \(\{\phi(X_{t}),{\cal A}_{t}\}_{t\in I}\) is a submg.
3. If \(\phi\) is convex and \(\nearrow\) and\(\{X_{t},{\cal A}_{t}\}_{t\in I}\) is a submg, then \(\{\phi(X_{t}),{\cal A}_{t}\}_{t\in I}\) is a submg.

**Proof.** Clearly, \(\phi(X_{t})\) is adapted to \({\cal A}_{t}\). Let \(s\leq t\). For the mg case,

1. \({\rm E}[\phi(X_{t})|{\cal A}_{s}]\geq\phi({\rm E}(X_{t}|{\cal A}_{s}))\) by the conditional Jensen inequality
2. \(=\phi(X_{s})\) a.s. (since \(\{X_{t},{\cal A}_{t}\}_{t\in I}\) is a mg)

For the submg case,

1. \({\rm E}[\phi(X_{t})|{\cal A}_{s}]\geq\phi({\rm E}(X_{t}|{\cal A}_{s}))\) by the conditional Jensen inequality
2. \(\geq\phi(X_{s})\) a.s.,

since \(\phi\) is \(\nearrow\) and \({\rm E}(X_{t}|{\cal A}_{s})\geq X_{s}\) a.s. \(\Box\)

**Example 1.1**: Let \(\{X_{t},{\cal A}_{t}\}_{t\in I}\) be a martingale. Then:

1. \(\{|X_{t}|^{r},{\cal A}_{t}\}_{t\in I}\) is a submg, for any \(r\geq 1\) having \({\rm E}|X_{t}|^{r}<\infty\) for all \(t\in I\),
2. \(\{X_{t}^{-},{\cal A}_{t}\}_{t\in I}\) is a submg,
3. \(\{X_{t}^{+},{\cal A}_{t}\}_{t\in I}\) is a submg (even if \(\{X_{t},{\cal A}_{t}\}_{t\in I}\) is only a submg).

[Note that \(\phi(x)=|x|^{r}\) and \(\phi(x)=x^{-}\) are convex, while \(\phi(x)=x^{+}\) is also \(\nearrow.\)] \(\Box\)

**Proposition 1.3**: If \(\{X_{t},{\cal A}_{t}\}_{t\in I}\) and \(\{Y_{t},{\cal A}_{t}\}_{t\in I}\) are s-mgs, then (trivially)

1. \(\{X_{t}+Y_{t},{\cal A}_{t}\}_{t\in I}\) is a s-mg.

**Exercise 1.1**: If \(\{X_{t}^{c},{\cal A}_{t}\}_{t\in I}\) is a submg for all \(c\) in some index set \(C\), then the maximum \(\{X_{t}^{c_{1}}\lor X_{t}^{c_{2}},{\cal A}_{t}\}_{t\in I}\) is necessarily a submg for any \(c_{1},c_{2}\in C\). Likewise, \(\{\sup_{c\in C}X_{t}^{c},{\cal A}_{t}\}_{t\in I}\) is a submg, provided that \({\rm E}|\sup_{c\in C}X_{t}^{c}|<\infty\) for all \(t\in I\).

**Definition 1.3** (Augmented filtration): Let \((\Omega,{\cal A},P)\) be a complete probability space. Let \({\cal N}\equiv\{N\in{\cal A}:P(N)=0\}\). Let \(\{{\cal A}_{t}\ :\ t\geq 0\}\) be such that the \({\cal A}_{t}\)'s are an \(\nearrow\) sequence of \(\sigma\)-fields with \({\cal A}_{t}=\hat{\cal A}_{t}={\cal A}_{t+}\) for all \(t\geq 0\) (here \(\hat{\cal A}_{t}\equiv\sigma[{\cal A}_{t},{\cal N}]\) and \({\cal A}_{t+}\equiv\cap\{{\cal A}_{r}\ :\ r>t\}\)). [That is, they are complete and right continuous.] Such a collection of \(\sigma\)-fields is called an _augmented filtration_.

**Notation 1.2** (Completeness assumption): In this chapter we will assume that the \(\sigma\)-fields \({\cal A}_{t}\) form an augmented filtration in that completion has already been performed on the \(\sigma\)-fields labeled \({\cal A}_{t}\). Thus, from proposition 12.4.5(c), we see that \(S\leq T\) a.s. implies \({\cal A}_{S}\subset{\cal A}_{T}\). For right-continuous processes on \((D_{[0,\infty)},{\cal D}_{[0,\infty)})\) this effectively comes for free; see proposition 12.4.5(b). If \(\{X_{t},{\cal A}_{t}\}_{t\in I}\) is a s-mg, then \(\{X_{t},\hat{\cal A}_{t}\}_{t\in I}\) is also a s-mg; note exercise 1.2 below.

**Exercise 1.2**  Verify the claim made in the previous assumption.

**Exercise 1.3**  If \(X\) is a process on \((D,{\cal D})\) or \((D_{[0,\infty)},{\cal D}_{[0,\infty)})\), then the histories \(\sigma_{t}\equiv\sigma[X_{s}\ :\ s\leq t]\) are right continuous, as are the \(\dot{\sigma}_{t}\equiv\sigma[\sigma_{t}\cup{\cal N}]\). (Recall (12.4.13) of proposition 12.4.4, proposition 12.4.5, and exercise 1.2.1.)

**Remark 1.1**  All definitions and results in this section make sense for processes on the measurable space \((R_{I},{\cal B}_{I})\). \(\Box\)

**Some Examples**

**Example 1.2** (Sums of iids)  Let \(X_{1},X_{2},\ldots\) be iid with means \({\rm E}(X_{i})=0\), and then define \(S_{n}\equiv X_{1}+\cdots+X_{n}\) and \({\cal A}_{n}\equiv\sigma[S_{1},\ldots,S_{n}]\). Then \({\rm E}|S_{n}|\leq\sum_{i=1}^{n}{\rm E}|X_{i}|<\infty\) and so \(\{S_{n},{\cal A}_{n}\}_{n=1}^{\infty}\) is a mg. \(\Box\)

**Example 1.3**  As in example 1.2, but now assume that \({\rm E}(X_{k}^{2})\equiv\sigma^{2}<\infty\). Let \(Y_{n}\equiv S_{n}^{2}-n\sigma^{2}\). Then \(\{Y_{n},{\cal A}_{n}\}_{n=1}^{\infty}\) is a mg. Note also that \(S_{n}^{2}\) is a submg by proposition 1.2, and that we have written

\[S_{n}^{2}=(S_{n}^{2}-n\sigma^{2})+n\sigma^{2}=(\mbox{martingale})+(\mbox{ increasing process}).\]

This is an example of the Doob decomposition of a submartingale, which we will establish in section 5. \(\Box\)

**Example 1.4**  Suppose \(\mu\equiv{\rm E}(X_{i})>0\) in example 1.2. Then the partial sums \(\{S_{n},{\cal A}_{n}\}_{n=1}^{\infty}\) form a submg. \(\Box\)

**Example 1.5** (Wald's mg)  Consider again example 1.2, but now suppose that the \(X_{k}\)'s have a mgf \(\phi(t)={\rm E}\exp(tX)\). Let \(Y_{n}\equiv\exp(cS_{n})/\phi(c)^{n}\). Then \(\{Y_{n},{\cal A}_{n}\}_{n=1}^{\infty}\) is a mg. Note that the mg of example 1.2 is recovered by differentiating once with respect to \(c\) and setting \(c=0\); the mg of example 1.4 is recovered by differentiating twice with respect to \(c\) and setting \(c=0\). \(\Box\)

**Example 1.6** (Brownian motion)  Let \(\{\mathbb{S}(t):t\geq 0\}\) be standardized Brownian motion, and let \({\cal A}_{t}\equiv\sigma[\mathbb{S}(s)\ :\ s\leq t]\). Then \(\{\mathbb{S}(t),{\cal A}_{t}\ :\ t\geq 0\}\) is a mg. \(\Box\)

**Example 1.7**  Let \(Y(t)\equiv\mathbb{S}(t)^{2}-t\) in example 1.6. Then the Brownian motion transform \(\{Y(t),{\cal A}_{t}\ :\ t\geq 0\}\) is a mg. \(\Box\)

**Example 1.8** (The exponential mg for Brownian motion)  As in example 1.6, let \(\mathbb{S}\) denote standard Brownian motion, and much as in example 1.5, set

\[Y(t)=\exp(c\mathbb{S}(t))/\exp(c^{2}t/2)=\exp(c\mathbb{S}(t)-c^{2}t/2).\]

Differentiating once with respect to \(c\) and setting \(c=0\) yields the mg of example 1.6; differentiating twice with respect to \(c\) and setting \(c=0\) yields the mg that appears in example 1.7; higher-order derivatives yield mgs based on the _Hermite polynomials_. (Recall (11.5.15) and (12.7.13).) \(\Box\)

**Example 1.9**  Let \(X\in{\cal L}_{1}\) and \({\cal A}_{n}\) be \(\nearrow\sigma\)-fields. Then \(Y_{n}\equiv{\rm E}(X|{\cal A}_{n})\) is a mg. \(\Box\)

**Example 1.10** (**Cumulative hazard \(\Lambda(\cdot)\), and a simple counting process)**: (a) Let \(X\) have df \(F\) on the reals \(R\). Then

\[\Lambda(t)\equiv\int_{(-\infty,t]}[1-F_{-}(r)]^{-1}\,dF(r)\quad\mbox{for all }t\in R \tag{9}\]

is called the _cumulative hazard function_. Note that

\[0\leq\Lambda(t)<\infty\mbox{ for all }t<\tau_{o}\equiv F^{-1}(1),\mbox{ while }\Delta\Lambda(\tau_{o})=\frac{\Delta F(\tau_{O})}{1-F_{-}(\tau_{O})}<\infty. \tag{10}\]

Moreover, \(\Lambda\) is a generalized df on \(R\) that assign measure \(\Lambda(a,b]\) to \((a,b]\) whenever \(-\infty\leq a<b<\tau_{o}\), that assign measure \(\Delta\Lambda(\tau_{o})\) to \(\{\tau_{o}\}\), and that assigns measure \(0\) to any \((a,b]\) for which \(\tau_{o}\leq a<b\leq\infty.\) It is common that \(\Lambda(t)\nearrow\infty\) as \(t\nearrow\tau_{o}.\) This holds when \(X\cong\mbox{Uniform}(0,\,1)\), and for all dfs for which \(F_{-}(F^{-1}(t))=t\) in some neighborhood with right endpoint \(\tau_{o}\). Still, note that when \(X\cong\mbox{Bernoulli}(p)\) with \(0<p<1,\mbox{ then }\Lambda(t)=(1-p)1_{[0,\infty)}(t)+1_{[1,\infty)}(t).\) Roughly, \(1-F_{-}(t)\) is the probability that Y still "lives" just prior to time \(t\). Given this, \(dF(t)\) is the "instantaneous probability" of a failure at time \(t\). Thus \(d\Lambda(t)=dF(t)/[1-F_{-}(t)]\) represents the _instantaneous hazard_ at time \(t\).

(b) Define the counting process

\[\mathbb{N}_{t}\equiv\mathbb{N}(t)\equiv 1_{[X\leq t]}\quad\mbox{ and let }\quad\mathcal{A}_{t}\equiv\sigma[\mathbb{N}(r):r\leq t],\quad\mbox{ for all }\quad t\in R. \tag{11}\]

Note that \(\mathbb{N}\) is an \(\nearrow\) and right-continuous process on \(R\) that is adapted to the history \(\sigma\)-fields \(\mathcal{A}_{t}\), and hence is a submg.

(c) The class \(C_{s}\equiv\{[X>r]\ :\ -\infty\leq r\leq s\}\) is a \(\overline{\pi}\)-system that generates \(\mathcal{A}_{s}\). So any two finite measures that agree on \(C_{s}\) also agree on \(\mathcal{A}_{s}\) by the Dynkin \(\pi\)-\(\lambda\) theorem.

(d) We start with a bit of practice. The reader is to show in exercise 1.4 below that

\[\mbox{E}\{\mathbb{N}_{t}|\mathcal{A}_{s}\}=_{a.s.}\,\,1_{[X\leq s]}+1_{[X>s]} \frac{F(s,t]}{1-F(s)}\quad\mbox{for all }-\infty<s<t<\infty \tag{12}\]

(where \(\frac{0}{0}\) is interpreted as \(0\) and \(F(s,t]\ \equiv F(t)-F(s)\) ) by verifying that for every set \(A\) in the \(\overline{\pi}\)-system \(C_{s}\) the relationship \(\int_{A}(\mathbb{N}_{t}-\mbox{E}\{\mathbb{N}_{t}|\mathcal{A}_{s}\})dP=0\) holds for the candidate for \(\mbox{E}\{\mathbb{N}_{t}|\mathcal{A}_{s}\}\) that is specified in (12). In like fashion verify that

\[\mbox{E}\{1_{[X\geq u]}|\mathcal{A}_{s}\}=_{a.s.}\,\,1_{[X>s]}\frac{1-F_{-}(u) }{1-F(s)}\qquad\qquad\mbox{ for all }-\infty<s<u\leq t<\infty. \tag{13}\]

(e) Next, the process

\[\mathbb{M}_{t}\equiv\mathbb{M}[(t)\equiv\mathbb{N}(t)-\int_{(-\infty,t]}1_{[X \geq r]}\,d\Lambda(r)\equiv\mathbb{N}(t)-\mbox{A}(t)\qquad\mbox{is a mg on }R \tag{14}\]

adapted to the \(\mathcal{A}_{t}\)'s. Note first that for all \(t\in R\),

\[\mbox{E}|\mathbb{M}_{t}|\leq\mbox{E}|\mathbb{N}_{t}|+\int_{-\infty}^{t}\mbox{E} 1_{[X\geq r]}\,d\Lambda(r)\leq F(t)+\int_{-\infty}^{t}\frac{1-F-}{1-F-}\,dF \leq 2F(t)\leq 2. \tag{15}\]

Then verify \(\int_{A}(\mathbb{N}_{t}-\mathbb{N}_{s})dP\) is equal to \(\int_{A}(\mathbb{A}_{t}-\mathbb{A}_{s})dP\) for all sets \(A\in\mathcal{C}_{s}.\) Statement (12) gives the so called "Doob-Meyer decomposition" of the submg \(\mathbb{N}\) into the mg \(\mathbb{M}\) and the \(\nearrow\) process \(\mathbb{A}\). The motivation for the definition of \(\mathbb{A}\) is found in (13.5.3) and (13.8.3). \(\square\)

**Example 1.11** (Another counting process)  Suppose that the rvs \(\xi_{1},\xi_{2},\ldots\) are iid Uniform(0, 1), and let \(\mathbb{N}_{n}(t)\equiv n\mathbb{G}_{n}(t)\equiv\) (the number of \(\xi_{i}\)'s \(\leq t\)). Then \(\mathbb{N}_{n}\) is a _counting process_, since it is \(\nearrow\), and it increases only by jumps upward of size \(+1.\) Hence it is a submartingale. The reader will be asked to show (giving another Doob-Meyer decomposition) that the uniform empirical process \(\mathbb{U}_{n}\) satisfies

\[\begin{array}{l}\mathbb{M}_{n}(t)\equiv\mathbb{N}_{n}(t)-\int_{0}^{t}\{n[1- \mathbb{G}_{n-}(r)]/(1-r)\}dr\\ \qquad=\sqrt{n}\{\mathbb{U}_{n}(t)+\int_{0}^{t}[\mathbb{U}_{n-}(r)/(1-r)]dr \}\qquad\mbox{ is a martingale}.\end{array} \tag{16}\]

The covariance function of this process is \(s\wedge t\) for all \(0\leq s,t\leq 1\)

**Example 1.12** (Poisson process): Suppose \(\mathbb{N}(t)\) is a Poisson process with rate \(\lambda>0\). It is a counting process and hence a submartingale. Moreover, the process \(\mathbb{M}[(t)\equiv\mathbb{N}(t)-\lambda t\) is a martingale, and the process \(\mathbb{M}^{2}(t)-\lambda t\) is also a martingale. \(\square\)

**Example 1.13** (Likelihood ratios): Let \((\Omega,\mathcal{A},P)\) and \((\Omega,\mathcal{A},Q)\) be probability spaces for \(Q\) and \(P\). Suppose that \(\mathcal{A}_{n}\) is an \(\nearrow\) sequence of sub \(\sigma\)-fields of \(\mathcal{A}.\) Suppose \(Q_{n}\) and \(P_{n}\) denote the measures \(Q\) and \(P\), respectively, restricted to \(\mathcal{A}_{n}\), and suppose that \(Q_{n}\ll P_{n}\). Let \(X_{n}\equiv dQ_{n}/dP_{n}\). Then for \(A\in\mathcal{A}_{m}\) and \(n>m\) we have \(\int_{A}X_{n}\,dP=Q_{n}(A)=Q_{m}(A)=\int_{A}X_{m}\,dP\), so that \(\int_{A}(X_{n}-X_{m})dP=0.\) This shows that

\[\{X_{n},\mathcal{A}_{n}\}_{n=1}^{\infty}\text{ is a mg of likelihood ratios.} \tag{17}\]

**Example 1.14** (Kakutani's mg): Let \(X_{1},X_{2},\ldots\) be independent rvs with each \(X_{k}\geq 0\) and \(\text{E}X_{k}=1\). Let \(M_{n}\equiv\prod_{1}^{n}X_{k}\), for \(1\leq k\leq n\), with \(M_{0}\equiv 1\). Then \(M_{n}\) is a mg with all \(\text{E}M_{n}=1\). \(\square\)

**Exercise 1.4**: Verify the claims made in example 1.10.

**Exercise 1.5**: Verify the claims made in example 1.11.

**Exercise 1.6**: Find the exponential martingale that corresponds to the mg \(\mathbb{M}(t)\) in example 1.12. Then differentiate this twice with respect to \(c\), set \(c=0\) each time, and obtain the two mgs given in the example.

[MISSING_PAGE_EMPTY:9151]

## 3 The Submartingale Convergence Theorem

**Theorem 3.1**: **(S-mg convergence theorem)**  Let \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) be a s-mg.

(A) Suppose \({\rm E}X_{n}^{+}\nearrow M<\infty\) (i.e., the \(X_{n}^{+}\)-submg is integrable). Then

(1) \(X_{n}\to X_{\infty}\) a.s.   for some  \(X_{\infty}\in{\cal L}_{1}\).

(B) For uniformly integrable \(X_{n}\)'s, this \(X_{\infty}\)_closes the s-mg_ in that

(2) \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a s-mg with \({\rm E}X_{n}\nearrow{\rm E}X_{\infty}\),   and with \({\cal A}_{\infty}\equiv\sigma[\cup_{n=1}^{\infty}{\cal A}_{n}]\).

In fact, supposing \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a submg, the conclusions (aa), (bb), and (ee) of part (C) below are equivalent. If all \(X_{n}\geq 0\), then (cc) and (dd) are also equivalent. (Closing the s-mg means that \(\int_{A}X_{n}\,dP\leqq\int_{A}X_{\infty}\,dP\) for all \(A\in{\cal A}_{n}\) and all \(n\), with the terminal rv \(X_{\infty}\in{\cal L}_{1}\).)

(C) If the \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) of (A) is actually a mg, then the following are equivalent.

(aa) \(X_{n}\)'s are uniformly integrable.  (bb)  \(X_{n}\to_{{\cal L}_{1}}X_{\infty}\).

(3) (cc)  Some rv \(Y\) closes the mg.  (dd)  \(X_{\infty}\) closes the mg.

(ee) \(X_{n}\)'s are integrable   and  \(\overline{\lim}_{n}{\rm E}|X_{n}|\leq{\rm E}|X_{\infty}|<\infty\).

(D) In all the above, if \(\{X_{t},{\cal A}_{t}\}_{t\in[0,\infty)}\) is a process on \((D_{[0,\infty)},{\cal D}_{[0,\infty)})\), then

(4) \(n,X_{n},{\cal A}_{n},\{0,1,\ldots\}\)   may be replaced by  \(t,X_{t},{\cal A}_{t},[0,\infty)\).

[Closing an \(X\)-martingale on \([0,\infty)\) by \(X_{\infty}\) is the same as closing an \(X\)-martingale on \([0,\theta)\) by the limiting rv \(X_{\theta}\).]

**Notation 3.1**  If a sequence does not converge to an extended real value, then it must necessarily be oscillating. If it does so oscillate, then some interval must be "upcrossed" infinitely often. We seek to take advantage of this. Let \(X_{1},X_{2},\ldots\) be a sequence of rvs. Let \(a<b\). Then:

\(U_{[a,b]}^{(n)}(\omega)\equiv\) (the number of upcrossings of \([a,b]\) in the first \(n\) steps)

(5) \(\equiv\left[\begin{array}{l}\mbox{number of integer pairs $(i,j)$ with $0\leq i<j\leq n$ having}\\ x_{i}(\omega)\leq a,\ a<X_{k}(\omega)<b\ \mbox{ for $i<k<j$, and $X_{j}(\omega)\geq b$}\end{array}\right],\)

(6) \(U_{[a,b]}^{(\infty)}(\omega)\equiv\lim_{n\to\infty}U_{[a,b]}^{(n)}(\omega)\). \(\Box\)

**Inequality 3.1** (Upcrossing inequality; Doob)   If \(\{X_{k},{\cal A}_{k}\}_{k=0}^{n}\) is a submg, then

(7) \({\rm E}U_{[a,b]}^{(n)}\leq\frac{1}{b-a}\{{\rm E}(X_{n}-a)^{+}-{\rm E}(X_{0}-a )^{+}\}\leq\frac{1}{b-a}\{{\rm E}X_{n}^{+}+|a|\}\).

If \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a submg, then

(8) \({\rm E}U_{[a,b]}^{(\infty)}\leq\frac{1}{b-a}\{{\rm E}(X_{\infty}-a)^{+}-E(X_{ 0}-a)^{+}\}\leq\frac{1}{b-a}\{{\rm E}X_{\infty}^{+}+|a|\}\).

**Proof.**  The number of upcrossings of \([a,b]\) made by \(X_{k}\) and the number of upcrossings of \([0,b-a]\) made by \((X_{k}-a)^{+}\) are identical; since \((X_{k}-a)^{+}\) is also a submg, we may assume that \(X_{k}\geq 0\) and \(a=0\) in this proof. Define

(a) \(\begin{array}{l}T_{0}\equiv 0,\quad T_{1}\equiv\min\{n\geq 0:X_{n}=0\},\quad T _{2}\equiv\min\{n>T_{1}:X_{n}\geq b\},\\ T_{3}\equiv\min\{n>T_{2}\ :\ X_{n}=0\},\ldots,T_{n+2}\equiv n;\end{array}\)here we use the convention that \(\min\emptyset\equiv n\). Clearly, these are stopping times that do satisfy \(0=T_{0}\leq T_{1}\leq\cdots\leq T_{n+2}=n\). Thus proposition 13.2.1 shows that the process \(\{X_{T_{i}},{\cal A}_{T_{i}}\}_{i=0}^{n+2}\) is a submg. Now,

\[X_{n}-X_{0}=\sum_{i=1}^{n+2}(X_{T_{i}}-X_{T_{i-1}})\]

(b) \[=\sum_{i\,\rm odd}\,(\mbox{same})+\sum_{i\,\rm even}\,(\mbox{same})\equiv I_{1} +I_{2},\]

and since \(X_{T_{i}}\) is a submg, we have

(c) \[{\rm E}I_{1}\geq 0\quad\mbox{ and }\quad{\rm E}I_{2}\geq 0.\]

Now suppose that \(U^{(n)}_{[0,b]}(\omega)=k\); then

\[I_{2}(\omega)=[X_{T_{2}(\omega)}(\omega)-X_{T_{1}(\omega)}(\omega)]+\cdots+[X_{ T_{2k}(\omega)}(\omega)-X_{T_{2k-1}(\omega)}(\omega)]+\cdots\]

(d) \[\geq[b]+\cdots+[b]+0=bU^{(n)}_{[0,b]}(\omega).\]

Thus (recall (2.1.3))

(e) \[{\rm E}(X_{n}-X_{0})={\rm E}I_{1}+{\rm E}I_{2}\geq{\rm E}I_{2}\geq b{\rm E}U^{ (n)}_{[0,b]}\]

and this is (7) in disguise (since \(X_{k}\) really denotes \((X_{k}-a)^{+}\)). Finally, note the positive part inequality \((X_{n}-a)^{+}\leq X_{n}^{+}+|a|\).

Now, \(0\leq U^{(n)}_{[a,b]}\nearrow U^{(\infty)}_{[a,b]}\). Thus,

(f) \[{\rm E}U^{(\infty)}_{[a,b]}=\lim{\rm E}U^{(n)}_{[a,b]}\qquad\mbox{ by the MCT}\] (g) \[\leq\lim\frac{1}{b-a}\{{\rm E}(X_{n}-a)^{+}-{\rm E}(X_{0}-a)^{+}\} \qquad\mbox{by (7)}\] (h) \[\leq\frac{1}{b-a}\{{\rm E}(X_{\infty}-a)^{+}-{\rm E}(X_{0}-a)^{+}\},\]

since \({\rm E}(X_{n}-a)^{+}\leq{\rm E}(X_{\infty}-a)^{+}\) follows from \(\{(X_{n}-a)^{+},{\cal A}_{n}\}_{n=0}^{\overline{\infty}}\) being a submg. 

**Proof.** (Proof of theorem 3.1) Now,

\[[\omega:\,\lim X_{n}(\omega)\mbox{ exists as a number in }[-\infty,\infty]]^{c}=[\underline{\lim}\,X_{n}<\overline{\lim}\,X_{n}]\]

(a) \[=\cup_{\{r<s\ {\rm rational}\}}[\underline{\lim}\,X_{n}<r<s<\overline{\lim}\,X_{n}] \equiv\cup_{\{r<s\ {\rm rational}\}}A_{rs}.\]

It suffices to prove that \(P(A_{rs})=0\) for all \(r,s\). Now,

(b) \[A_{rs}\subset B_{rs}\equiv[\omega:U^{(\infty)}_{[r,s]}=\infty].\]

It thus suffices to show that \(P(B_{rs})=0\) for all \(r,s\). But

(c) \[{\rm E}U^{(\infty)}_{[r,s]}={\rm E}\lim U^{(n)}_{[r,s]}=\lim{\rm E }U^{(n)}_{[r,s]}\qquad\mbox{ by the MCT}\] (d) \[\leq\lim\frac{1}{s-r}\{{\rm E}X_{n}^{+}+|r|\}\qquad\mbox{by (7)}\] (e) \[\leq(M+|r|)/(s-r)<\infty,\qquad\mbox{ since the $X_{n}^{+}$'s are integrable;}\]

hence we must have \(P(B_{rs})=0\). Thus \(X_{\infty}\equiv\lim X_{n}\) exists a.s. with values in \([-\infty,\infty]\), and \(X_{\infty}\) is \({\cal A}_{\infty}\)-measurable, since all \(X_{n}\)'s are. Now (recall (2.1.3)),

(f) \[{\rm E}|X_{n}|=2{\rm E}X_{n}^{+}-{\rm E}X_{n}\leq 2{\rm E}X_{n}^{+}-{\rm E }X_{0}\leq M<\infty.\]Thus Fatou's lemma implies

\[\mathrm{E}|X_{\infty}|=\mathrm{E}(\varliminf|X_{n}|)\leq\varliminf\mathrm{E}|X_{n} |\leq M<\infty;\]

thus \(X_{\infty}\in\mathcal{L}_{1}\) with its values in \((-\infty,\infty)\) a.s. Thus (A) holds.

Consider (B). Now \(X_{n}\to\) (some \(X_{\infty}\)) a.s. by (A) under any of (aa), (bb), or (ee). Vitali shows that (aa) is equivalent to (ee) and the \(\mathcal{L}_{1}\)-convergence of (bb) with the rv \(X_{\infty}\). (If \(X_{n}\to_{\mathcal{L}_{1}}Y\) in (bb), then this \(Y\) must equal \(X_{\infty}\) a.s. by going to subsequences.) Thus for \(n\geq m\), we have from (13.1.4) and \(\mathcal{L}_{1}\)-convergence that

\[\mathrm{(h)}\qquad\int_{A}X_{m}\,dP\leqq\int_{A}X_{n}\,dP\to\int_{A}X_{\infty} \,dP\qquad\text{ for all }A\in\mathcal{A}_{\infty}.\]

Thus \(\{X_{n},\mathcal{A}_{n}\}_{n=0}^{\overline{\infty}}\) is a s-mg by condition (13.1.4). That is, the rv \(X_{\infty}\) closes the s-mg (and thus (B) holds, except for the equivalence of (cc) and (dd)).

Consider (C). For (aa)-(ee), we lack only that (cc) implies (aa). For a mg with \(X_{n}\) (or for submg with \(X_{n}\geq 0\)), the \(|X_{n}|\) form a submg. Thus

\[\mathrm{(i)}\qquad\mathrm{E}\{|X_{n}|\times 1_{[|X_{n}|\geq\lambda]}\}\leq \mathrm{E}\{|Y|\times 1_{[|X_{n}|\geq\lambda]}\}\qquad\text{ since }|Y|\text{ closes the submg}\] \[\mathrm{(j)}\qquad\to 0\qquad\text{ by absolute continuity of the integral,}\]

and since the sets satisfy

\[\mathrm{(k)}\qquad P(|X_{n}|\geq\lambda)\leq\mathrm{E}|X_{n}|/\lambda\leq \mathrm{E}|Y|/\lambda\to 0\qquad\text{ uniformly in }n\text{ as }\lambda\to\infty.\]

This completes the entire proof in the case of discrete time.

Consider (D). (Continuous time) Our preliminaries will not assume the s-mg structure. Now, \(\varlimsup X_{t}(\omega)\) could be \(+\infty\) for some \(\omega\)'s, and this will cause difficulties with the present approach. Thus (following Doob) define

\[\mathrm{(l)}\qquad Y_{t}(\omega)=(2/\pi)\tan^{-1}(X_{t}(\omega))\]

to transform the range space from \([-\infty,\infty]\) to \([-1,\,1]\). For each \(m\) choose rational numbers \(t_{m1},\ldots,t_{m_{k_{m}}}\) in \([m,\infty)\) so that (remember, \(X\ :\ (\Omega,\mathcal{A},P)\to(D,\mathcal{D})\))

\[\mathrm{(m)}\qquad P(\sup_{t\in[m,\infty)}Y_{t}-\sup_{t_{mj}\in[m,\infty)}Y_{t _{mj}}>\tfrac{1}{m})<\tfrac{1}{2^{m}}.\]

This is possible, since the sup over all rationals \(r\) in \([m,\infty)\) equals the sup over all reals \(t\) in \([m,\infty)\), and the rationals are the limit of \(\{r_{1},\ldots,r_{k}\}\) for any ordering \(\{r_{1},r_{2},\ldots\}\) of the rationals; thus,

\[\mathrm{(n)}\qquad\begin{array}{l}0=P(\sup_{t\in[m,\infty)}Y_{t}-\sup_{r_{j }\in[m,\infty)}Y_{r_{j}}>\tfrac{1}{m})\\ =\lim_{k\to\infty}P(\sup_{t\in[m,\infty)}Y_{t}-\sup_{r_{j}\in[m,\infty),j\leq k }Y_{r_{j}}>\tfrac{1}{m}).\end{array}\]

We may assume that the \(t_{mj}\)'s were chosen so as to simultaneously satisfy

\[\mathrm{(o)}\qquad P(\inf_{t\in[m,\infty)}Y_{t}-\inf_{t_{mj}\in[m,\infty)}Y_{t _{mj}}<-\tfrac{1}{m})<\tfrac{1}{2^{m}}.\]Thus, if we

(p)   let \(t_{1}<t_{2}<\cdots\) denote an ordering of \(\cup_{m=1}^{\infty}\{t_{m1},\ldots,t_{mk_{m}}\}\) (which does exist, since all \(t_{mj}\geq m\)), then

(q)  \(P(\sup_{t\in[m,\infty)}Y_{t}-\sup_{t_{i}\in[m,\infty)}Y_{t_{i}}>\frac{1}{m})< \frac{1}{2^{m}}\)   and

\(P(\inf_{t\in[m,\infty)}Y_{t}-\inf_{t_{i}\in[m,\infty)}Y_{t_{i}}<-\frac{1}{m})< \frac{1}{2^{m}}\).

Letting \(A_{m}\) and \(B_{m}\) denote the events in (q) we see that

\(\sum_{1}^{\infty}P(A_{m}\cup B_{m})\leq\sum_{1}^{\infty}2/2^{m}<\infty\),

so that \(P(A_{m}\cup B_{m}\) i.o.\()=0\). Thus

(r)  \(\overline{\lim}_{t\to\infty}Y_{t}=\overline{\lim}_{i\to\infty}Y_{t_{i}}\) a.s.   and  \(\underline{\lim}_{t\to\infty}Y_{t}=\underline{\lim}_{i\to\infty}Y_{t_{i}}\) a.s.

Now, transforming back via \(X_{t}(\omega)=\tan((\pi/2)Y_{t}(\omega)),(\mbox{r})\) implies the next lemma:

**Lemma 3.1**   For every \(X:\,(\Omega,{\cal A},P)\to(D_{[0,\infty)},{\cal D}_{[0,\infty)})\) there exist rational numbers \(t_{1}<t_{2}<\cdots\) such that

(9)  \(\overline{\lim}_{t\to\infty}X_{t}=\overline{\lim}_{i\to\infty}X_{t_{i}}\) a.s.   and  \(\underline{\lim}_{t\to\infty}X_{t}=\underline{\lim}_{i\to\infty}X_{t_{i}}\) a.s.

[Note that \(t_{i}\to\infty\) in (9) could be replaced by \(t_{i}\nearrow\theta\) for any finite \(\theta\).]

Armed with the (9) "lemma," it is now easy to use the discrete version of this theorem to prove the continuous version. We will refer to the continuous versions of conclusions (1)-(3) as \((1^{\prime})\)-\((3^{\prime})\). We return to the proof of the theorem. (Now, we again assume s-mg structure in what follows.)

Let \(Y_{i}\equiv X_{t_{i}}\) and \(\tilde{\cal A}_{i}\equiv{\cal A}_{t_{i}}\) for the \(t_{i}\)'s in (9). Then \((Y_{i},\tilde{\cal A}_{i})_{i=0}^{\infty}\) is a s-mg to which the discrete theorems can be applied. Thus,

(s)  \(\overline{\lim}_{t\to\infty}X_{t}=\overline{\lim}_{i\to\infty}X_{t_{i}}\) a.s.   by (9)

(t)  \(=\) (a.s., some \(X_{\infty}\) in \({\cal L}_{1}\))   by (1)   applied to \((Y_{i},\tilde{\cal A}_{i})_{i=1}^{\infty}\)

(u)  \(=\underline{\lim}_{t\to\infty}X_{t}\)   by (9),

so that

(v)  \(X_{t}\to X_{\infty}\) a.s.   where \(X_{\infty}\in{\cal L}_{1}\).

That is, \((1^{\prime})\) holds. The rest is left to the reader in exercise 3.1. \(\Box\)

**Exercise 3.1**   Complete the proof of the continuous part of theorem 3.1.

**Exercise 3.2**   Let \(Y_{t}\equiv\mbox{E}(X|{\cal D}_{t})\), for \(X\in{\cal L}_{1}(\Omega,{\cal A},P)\) and for an arbitrary collection of sub \(\sigma\)-fields \({\cal D}_{t}\). Show that these \(Y_{t}\)'s are uniformly integrable.

**Exercise 3.3**   Let \({\cal A}_{-\infty}\subset\cdots\subset{\cal A}_{-1}\subset{\cal A}_{0}\subset{ \cal A}_{1}\subset\cdots\subset{\cal A}_{\infty}\) be sub \(\sigma\)-fields of the basic \(\sigma\)-field \({\cal A}\). Suppose the rv \(X\in{\cal L}_{1}(\Omega,{\cal A},P)\). Let \(Y_{n}\equiv\mbox{E}(X|{\cal A}_{n})\). Then the process \((Y_{n},{\cal A}_{n})_{n=-\underline{\infty}}^{\infty}\) is necessarily a uniformly integrable mg.

**Exercise 3.4**   Let \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) be a submg for which \(X_{n}\leq 0\). Then (1) holds, and \(Y\equiv 0\) closes the submg.

**Exercise 3.5** Let \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) be a submg. The following are equivalent:

(a): The \(X_{n}^{+}\)'s are uniformly integrable.

(b): There exists a rv \(Y\) that closes the submg.

(c): When these hold, then \(X_{\infty}\)(which necessarily exists a.s., and is in \({\cal L}_{1}\)) closes the submg.

[Hint. Do what you can with \(X_{n}^{+}\). Then apply it to \(Y_{n}^{(a)}\equiv(X_{n}\lor a)+|a|\), and let the constant \(a\to-\infty\).]

**Exercise 3.6** Let \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) be a submg with \(X_{n}\geq 0\). Let \(r>1\). Then the \(X_{n}^{r}\)'s are uniformly integrable if and only if the \(X_{n}^{r}\)-process is integrable.

**Exercise 3.7** (Martingale \({\cal L}_{r}\)-convergence theorem): (i) Let \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) be a mg sequence. Let \(r>1\). Then the following are equivalent:

(10) The \(|X_{n}|^{r}\)-process is integrable.

(11) \(X_{n}\to_{{\cal L}_{r}}X_{\infty}\).

(12) The \(X_{n}\)'s are uniformly integrable

(thus \(X_{n}\to_{a.s.}\) (some \(X_{\infty}\))) and \(X_{\infty}\in{\cal L}_{r}\).

(13) The \(|X_{n}|^{r}\)'s are uniformly integrable.

(14) \(\{|X_{n}|^{r},{\cal A}_{n}\}_{n=0}^{\overline{\infty}}\) is a submg and \({\rm E}|X_{n}|^{r}\nearrow{\rm E}|X_{\infty}|^{r}<\infty\).

(14*) \(M^{*}\equiv\sup\{|X_{n}|\ :\ 0\leq n\leq\infty\}\in{\cal L}_{r}\) (via Doob's \({\cal L}_{r}\)-inequality).

(ii) This theorem also holds for a submg when all \(X_{n}\geq 0\) a.s.

**Exercise 3.8** (a) Show that \(t,X_{t},{\cal A}_{t},[0,\infty)\) may replace \(n,X_{n},{\cal A}_{n},\{0,1,\ldots\}\) in all of exercise 3.3-exercise 3.7. [Also, \([0,\theta)\) may replace \([0,\infty)\).]

(b) Prove (D) of theorem 3.2 below.

**Definition 3.1** (Reversed s-mg): Let\(X_{n}\) be adapted to \({\cal A}_{n}\) with \(n\in\{\ldots,-1,0\}\), and define the \(\sigma\)-field \({\cal A}_{-\infty}\equiv\cap_{n=-\infty}^{0}{\cal A}_{n}\). The process \(\{X_{n},{\cal A}_{n}\}_{n=-\infty}^{0}\) is a _reversed s-mg_ (as defined earlier) if all \({\rm E}|X_{n}|<\infty\) and

(15) \(X_{n}\equiv{\rm E}(X_{m}|{\cal A}_{n})\) a.s. for all \(n\leq m\).

(This is like the second law of thermodynamics run backward in time, since \({\cal A}_{n}\) brings more stability as \(n\searrow.\))

**Theorem 3.2** (Reversed s-mg convergence theorem): Let \(\{X_{n},{\cal A}_{n}\}_{n=-\infty}^{0}\) be a s-mg sequence, or a reversed s-mg.

(A) It necessarily holds that

(16) \(X_{n}\to X_{-\infty}\) a.s. as \(n\to-\infty\)

for some \(X_{-\infty}\in[-\infty,\infty)\) a.s. that is \({\cal A}_{-\infty}\)-measurable.

(B)-(C) Furthermore, the following are equivalent (and all yield an \(X_{\infty}\in{\cal L}_{1}\)):

(17) \({\rm E}X_{n}\searrow M>-\infty\) as \(n\to-\infty\). (This is trivial if \(X_{n}\) is a mg.)

(18) \(X_{n}\)'s are uniformly integrable.

(19) \(X_{n}\to_{{\cal L}_{\infty}}X_{-\infty}\).

(20) \(\{X_{n},{\cal A}_{n}\}_{n=-\infty}^{0}\) is a s-mg, where \({\cal A}_{-\infty}\equiv\cap_{n=-\infty}^{0}{\cal A}_{n}\).

(D) In all the above, if \(\{X_{t},{\cal A}_{t}\}_{t\in(-\infty,0]}\) is a process on \((D_{(-\infty,0]},{\cal D}_{(-\infty,0]})\), then

\[n,X_{n},{\cal A}_{n},\{\ldots,-1,0\}\qquad\mbox{ may be replaced by }\qquad t,X_{t},{\cal A}_{t},(-\infty,0]. \tag{21}\]

**Proof.** Consider (16). Let \(U^{(n)}_{[r,s]}\) now denote the upcrossings of \([r,s]\) by the process \(X_{-n},\ldots,X_{-1},X_{0}\). Replace line (d) of the proof of theorem 3.1 by

(a) \[{\rm E}U^{(\infty)}_{[r,s]}\leq{1\over s-r}({\rm E}X^{+}_{0}+|r|)\leq\mbox{ (some constant) }<\infty,\]

and conclude that \(X_{-\infty}\equiv\lim X_{n}\) exists a.s. with values in \([-\infty,+\infty]\). Since the sequence \(\{X^{+}_{n},{\cal A}_{n}\}^{0}_{n=-\infty}\) is necessarily a submg, we obtain from Fatou that

(b) \[{\rm E}X^{+}_{-\infty}={\rm E}(\lim X^{+}_{n})\leq\underline{\lim}\,{\rm E}X^ {+}_{n}\leq\underline{\lim}\,{\rm E}X^{+}_{0}={\rm E}X^{+}_{0}<\infty.\]

Thus \(X_{-\infty}\) takes values in \([-\infty,+\infty)\) a.s., and is \({\cal A}_{-\infty}\)-measurable. Thus (16) does hold.

Suppose that (17) holds. Then (b) and (17) give

(c) \[\leq 2{\rm E}X^{+}_{0}-M<\infty;\]

thus \(X_{-\infty}\) takes values in \((-\infty,\infty)\) a.s., and \(X_{-\infty}\in{\cal L}_{1}\) (using only Fatou on the right hand side). Also, from (c),

(d) \[P(|X_{n}|\geq\lambda)\leq{\rm E}|X_{n}|/\lambda\leq[2{\rm E}X^{+}_{0}-M]/ \lambda\to 0\]

uniformly in \(n\in\{-\infty,\ldots,-1,0\}\) as \(\lambda\to\infty\). Thus (an analogous \(X_{\overline{n}}\) proof works only in the case of a mg),

(e) \[\int_{[X^{+}_{n}\geq\lambda]}X^{+}_{n}\,dP\leqq\int_{[X^{+}_{n}\geq\lambda]}X^{ +}_{0}\,dP\qquad\mbox{ by (\ref{eq:13.1.3})}\]

implies that the \(X^{+}_{n}\)'s are uniformly integrable. Now, for \(n<m\) we have

\[0 \geq-\int_{[X_{n}\leq-\lambda]}X^{-}_{n}dP={\rm E}(X_{n}-X_{m})+{ \rm E}X_{m}-\int_{[X_{n}>-\lambda]}X_{n}\,dP\] \[\geq{\rm E}(X_{n}-X_{m})+{\rm E}X_{m}-\int_{[X_{n}>-\lambda]}X_{m} \,dP\] \[={\rm E}(X_{n}-X_{m})+\int_{[X_{n}\leq-\lambda]}X_{m}\,dP\] (f) \[\geq-\epsilon+\int_{[X_{n}\leq-\lambda]}X_{m}\,dP\] \[\mbox{ for all }n\leq\mbox{ (a fixed m that is large enough), since }{\rm E}X_{n}\searrow M\] \[\geq-\epsilon-\int_{[X_{n}\leq-\lambda]}|X_{m}|dP\] (g) \[\geq-2\epsilon\mbox{ for }\lambda\mbox{ large enough, as in (d), with }m\mbox{ now fixed.}\]

Thus, the \(X^{-}_{n}\) are uniformly integrable. Thus, the \(X_{n}\) are uniformly integrable; that is, (18) holds.

Then (18) implies (19) by Vitali.

Suppose (19) holds. For any \(n\leq m\) we have from (18.1.4) that

(h) \[\int_{A}X_{m}\,dP\geqq\int_{A}X_{n}\,dP\to\int_{A}X_{-\infty}\,dP\qquad\mbox{ for all }A\in{\cal A}_{-\infty},\]

[MISSING_PAGE_FAIL:371]

## 4 Applications of the S-mg Convergence Theorem

The following examples give just a few selected applications to show the power of the various s-mg convergence theorems.

**Example 4.1** (Slln): Let \(X_{1},X_{2},\ldots\) be iid \(\mu\). Then the partial sum process \(S_{n}\equiv X_{1}+\cdots+X_{n}\) satisfies

(1) \(\overline{X}_{n}\equiv S_{n}/n\to\mu\) a.s. and \({\cal L}_{1}\) as \(n\to\infty\). \(\square\)

**Proof.** Let

(a) \({\cal A}_{-n}\equiv\sigma[S_{n},S_{n+1},\ldots]=\sigma[S_{n},X_{n+1},X_{n+2},.]\)

Now, \(Y_{-n}\equiv\mbox{E}(X_{1}|{\cal A}_{-n})\) is a reversed mg on...,\(-2,-1\) and

(b) \(\mbox{E}(X_{1}|{\cal A}_{-n})\to\mbox{E}(X_{1}|{\cal A}_{-\infty})\) a.s. and \({\cal L}_{1}\)

as \(n\to-\infty\), by theorem 13.3.1. Now,

\[\mbox{E}(X_{1}|{\cal A}_{-n})=\mbox{E}(X_{1}|S_{n},X_{n+1},\cdots)\] \[\quad=\mbox{E}(X_{1}|S_{n})\qquad\mbox{by (\ref{eq:L1})}\] \[\quad=\sum_{k=1}^{n}\mbox{E}(X_{k}|S_{n})/n\qquad\mbox{by symmetry}\] \[\quad=\mbox{E}(S_{n}|S_{n})/n\] (c) \[\quad=S_{n}/n.\]

Combining (b) and (c) gives

(d) \(S_{n}/n=\mbox{E}(X_{1}|{\cal A}_{-n})\to\mbox{E}(X_{1}|{\cal A}_{-\infty})\) a.s. and \({\cal L}_{1}\) as \(n\to\infty\).

But \(\lim(S_{n}/n)\) is measurable with respect to the symmetric \(\sigma\)-field, and so it is a.s. a constant by the Hewitt-Savage 0-1 law of exercise 7.2.1; hence \(\mbox{E}(X_{1}|{\cal A}_{-\infty})\) is a.s. a constant, by (d). But \(\mbox{E}[\mbox{E}(X|{\cal A}_{-\infty})]=\mu\), so that the constant must be \(\mu\); that is \(\mbox{E}(X_{1}|{\cal A}_{-\infty})=\mu\) a.s. Thus (d) implies \(S_{n}/n\to\mu\) a.s. and \({\cal L}_{1}\). \(\square\)

**Exercise 4.1** (SLLN for \(U\)-statistics) Let \(Y_{-n}\equiv U_{n}\) be a \(U\)-statistic based on \(X_{1},X_{2},\ldots\), with a symmetric kernel \(H\) for which \(\mbox{E}H(X_{1},X_{2})\) is finite. (Thus, \(H(x,y)=H(y,x)\) for all \(x,y\).) Consider the \(\sigma\)-field \({\cal A}_{-n}\equiv\sigma[\mbox{X}_{n:n},X_{n+1},X_{n+2},\,\ldots]\), for the vector \(\vec{X}_{n:n}\) of the first \(n\) order statistics of the sequence. [Hint. As with the SLLN above, the proof will again be based of the Hewitt-Savage 0-1 law for the symmetric \(\sigma\)-field.].

(a) Show that \(\{Y_{n},{\cal A}_{n}\}_{n=-\infty}^{-2}\) is a reversed mg.

(b) Use this to show that \(U_{n}\to_{a.s.\mbox{ and }{\cal L}_{1}}\)\(\mbox{E}H(X_{1},X_{2})\).

(c) Extend this to higher-dimensional kernels.

**Example 4.2** (Kolmogorov's 0-1 law) Suppose that \(Y_{1},Y_{2},\ldots\) are iid rvs and let \({\cal A}_{n}\equiv\sigma[Y_{1},\ldots,Y_{n}]\). Suppose that \(A\in{\cal T}\equiv\) (tail \(\sigma\)-field) \(=\cap_{n=1}^{\infty}\sigma[Y_{n+1},Y_{n+2},\ldots]\). Since \({\cal A}_{n}\) is independent of \({\cal T}\),

\[P(A)=_{a.s.}P(A|{\cal A}_{n})=\mbox{E}(1_{A}|{\cal A}_{n})\qquad\mbox{for every $n$.}\]But by theorem 13.3.3 we have

\[\mathrm{E}(1_{A}|\mathcal{A}_{n})\to_{a.s.}\ \ \mathrm{E}(1_{A}|\mathcal{A}_{ \infty})=_{a.s.}\ 1_{A}.\]

Thus \(P(A)\) must equal \(0\) or \(1\) (as \(P(A)=_{a.s.}1_{A}\) implies). 

**Example 4.3** (Approximation of \(\mathcal{L}_{1}\) and \(\mathcal{L}_{2}\) functions): Fix the function \(f\in\mathcal{L}_{1}([0,1]\), \(\mathcal{B}\), Lebesgue) ; thus \(\int_{0}^{1}|f(u)|du<\infty\). Let

\[\mathcal{A}_{n}\equiv\sigma[((i-1)/2^{n},i/2^{n}]\ :\ i=1,\ldots,2^{n}\}\nearrow \mathcal{B}[0,1].\]

Define \(X\equiv f(\xi)\), where \(\xi\cong\) Uniform \((0,\,1)\). Now let

\[X_{n}\equiv\mathrm{E}(X|\mathcal{A}_{n})=\mathrm{E}(f(\xi)|\mathcal{A}_{n})= \sum_{k=1}^{2^{n}}C_{nk}(f)1_{[(k-1)/2^{n}<\xi\leq k/2^{n}]},\]

with \(C_{nk}(f)\equiv 2^{n}\int_{(k-1)/2^{n}}^{k/2^{n}}f(u)du\). Since \(\mathrm{E}|X|<\infty\), theorem 13.3.3 gives

\[X_{n}\to\mathrm{E}(X|\mathcal{A}_{\infty})=\mathrm{E}(X|\mathcal{B})=X\quad \text{a.s. and}\ \ \mathcal{L}_{1}.\]

**Summary**  Let \(f\in\mathcal{L}_{1}\) and define the step function \(f_{n}^{s}(\cdot)\) by

\[f_{n}^{s}(t)\equiv 2^{n}\int_{(k-1)/2^{n}}^{k/2^{n}}f(u)du\quad\text{for }\tfrac{(k-1)}{2^{n}}<t\leq\tfrac{k}{2^{n}}\quad\text{and }1\leq k\leq 2^{n} \tag{2}\]

Then for every \(f\in\mathcal{L}_{1}\),

\[f_{n}^{s}(\cdot)\to f(\cdot)\quad\text{a.s. Lebesgue}\qquad\text{and}\qquad\int_{0}^{1}|f_{n}^{s}(t)-f(t)|dt\to 0. \tag{3}\]

Now suppose that \(f\in\mathcal{L}_{2}\). Then \(f\in\mathcal{L}_{1}\) also, and so (3) still holds; and this implies (13.3.12). Thus (the equivalent) (13.3.11) gives

\[f_{n}^{s}\to_{\mathcal{L}_{2}}f\quad\text{for every }f\in\mathcal{L}_{2}. \tag{4}\]

[So in both cases \(f_{n}^{s}(\cdot)\) can be thought of as approximating the derivative of the indefinite integral \(F(x)\equiv\int_{0}^{x}f(t)dt\).] 

**Example 4.4** (Kakutani's mg): Let \(X_{1},X_{2},\ldots\) be independent with each \(X_{k}\geq 0\) and \(\mathrm{E}X_{k}=1\). Define

\[M_{n}\equiv\prod_{1}^{n}X_{k},\quad\text{for }1\leq k\leq n, \tag{5}\]

with \(M_{0}\equiv 1\). Then \(\{M_{n},\mathcal{A}_{n}\}_{1}^{\infty}\) is a mg for which all \(\mathrm{E}M_{n}=1\), where \(\mathcal{A}_{n}\) is an appropriate \(\nearrow\) sequence of \(\sigma\)-fields (such as the histories). Since \(M_{n}\) is bounded in the space \(\mathcal{L}_{1}\), the sm-g convergence theorem of (13.3.1) shows that

\[M_{n}\to_{a.s.}M_{\infty}\in\mathcal{L}_{1}\quad\text{is always true}, \tag{6}\]

for the appropriate rv \(M_{\infty}\). We now show that the following are equivalent:

\[c_{\infty}\equiv\mathrm{E}M_{\infty}=1, \tag{7}\]

\[M_{n}\text{'s are uniformly integrable}, \tag{8}\]

\[M_{n}\to_{\mathcal{L}_{1}}M_{\infty}, \tag{9}\]

\[\prod_{1}^{\infty}a_{n}>0,\qquad\qquad\text{where}\quad a_{n}\equiv\mathrm{E}(X_{n}^{1/2})\leq 1, \tag{10}\]

\[\sum_{1}^{\infty}(1-a_{n})<\infty,\qquad\text{where}\quad a_{n}\equiv\mathrm{E}(X_{n}^{1/2})\leq 1. \tag{11}\]

Whenever one (hence all) of these equivalent statements fails, then necessarily

\[P(M_{\infty}=0)=1\quad\text{and}\quad c_{\infty}=0. \tag{12}\]

**Proof.** Because of (6), equivalence of (7)-(9) follows from Vitali's theorem (or from the submartingale convergence theorem). Equivalence of (10) and (11) is called for in the easy exercise 4.2 below. We first show that (10) implies (8). Suppose (10) holds. Define the normalized product

\[\begin{array}{l}N_{n}\equiv\prod_{1}^{n}X_{k}^{1/2}/{\prod_{1}^{n}}a_{k},\quad \mbox{with all}\quad\mbox{E}N_{n}=1\quad\mbox{and with}\\ \\ \mbox{E}(N_{n}^{2})=1/({\prod_{1}^{n}}a_{k})^{2}\leq 1/({\prod_{1}^{\infty}}a_{k})^{2}<\infty \quad\mbox{for all $n$.}\end{array} \tag{13}\]

Thus \(\{N_{n},{\cal A}_{1}\}_{1}^{\infty}\) is a mean-1 mg that is bounded in \({\cal L}_{2}\). Since all \({\prod_{1}^{n}}a_{k}\leq 1\), Doob's \({\cal L}_{2}\)-inequality (inequality 8.10.5) and the MCT give

(a) \[\begin{array}{l}\mbox{E}(\sup_{n}M_{n})=\lim_{n}\mbox{E}(\sup_{1\leq k\leq n }M_{k})\qquad\mbox{by the MCT}\\ \\ \leq\lim_{n}\mbox{E}(\sup_{1\leq k\leq n}N_{k}^{2})\times 1\\ \\ \leq({\frac{2}{2-1}})^{2}\mbox{E}(N_{n}^{2})<\infty\qquad\mbox{by Doob's ${\cal L}_{r}$-inequality}.\end{array}\]

Thus \(M^{*}\equiv\sup_{n}M_{n}\) is a rv in \({\cal L}_{1}\) for which \(0\leq M_{n}\leq M^{*}\) Hence the rvs \(\{M_{n}\ :\ 1\leq n\leq\infty\}\) are uniformly integrable. That is, (8) holds.

We next show that when (10) fails (that is, when \({\prod_{1}^{\infty}}a_{n}=0\)), then (7) fails (and that, in fact, (12) holds). Now (13) notes that the \(N_{n}\) all have mean 1, and hence they form an integrable mg. Thus \(N_{n}\to_{a.s.}\) (some \(N_{\infty})\in{\cal L}_{1}\) by the submartingale convergence theorem. Hence,

(d) \[M_{n}^{1/2}=({\prod_{1}^{n}}a_{k})N_{n}\to 0\quad\mbox{a.s.},\]

impling that \(M_{\infty}=0\) a.s. and thus that \(c_{\infty}=0\). This contradicts (7), and implies (12). \(\square\)

**Exercise 4.2**  Show the equivalence of (10) and (11). (Recall lemma 8.1.4.)

**Exercise 4.3** (Borel-Cantelli) Let \({\cal A}_{n}\) be an \(\nearrow\) sequence of \(\sigma\)-fields in \({\cal A}\). Show that \([A_{n}\) i.o.] \(=[\omega\ :\ {\sum_{n=1}^{\infty}}P(A_{n}|{\cal A}_{n-1})=\infty]\) a.s.

**A Branching Process Model**

**Example 4.5** (Branching Processes)  Let \(X\) denote the number of offspring of a particular type of individual, and let \(p_{k}\equiv P(X=k)\) for \(k=0,1,\ldots\). We start at generation zero with a single individual \(Z_{0}=1\), and it produces the individuals in a first generation of size \(Z_{1}\). These in turn produce a second generation of size \(Z_{2}\), and so forth. Thus,

\[Z_{n+1}\equiv\sum_{j=1}^{Z_{n}}X_{nj}\quad\mbox{for}\quad n\geq 0,\quad\mbox{ with $Z_{0}\equiv 1$,} \tag{14}\]

where \(X_{nj}\) denotes the number of offspring of the \(j\)th individual present in the \(n\)th generation. We assume that all \(X_{nj}\)'s are iid as the \(X\) above. Also, we suppose

\[m\equiv\mbox{E}X={\sum_{k=0}^{\infty}}kp_{k}<\infty,\quad\mbox{with}\quad p_{0 }>0,\quad\mbox{and}\quad p_{0}+p_{1}<1. \tag{15}\]

We call this a simple _branching process_ model. Let

\[W_{n}\equiv Z_{n}/m^{n}\quad\mbox{and}\quad{\cal A}_{n}\equiv\sigma[W_{1}, \ldots,W_{n}]. \tag{16}\]

**Proposition 4.1**: The process

\[\{W_{n},{\cal A}_{n}\}_{n=0}^{\infty}\quad\mbox{ is a mg}\quad\mbox{ with mean }\quad{\rm E}W_{n}=1, \tag{17}\]

and

\[{\rm Var}\;[W_{n}]=\left\{\begin{array}{ll}n\sigma^{2}&\quad\mbox{if }\;m=1,\\ \sigma^{2}\frac{1-m-n}{m(m-1)}&\quad\mbox{if }\;m\neq 1,\end{array}\right. \tag{18}\]

provided that \(\sigma^{2}\equiv{\rm Var}[X]<\infty\).

**Proof.** We note that

(a) \[{\rm E}Z_{n+1}={\rm E}[{\rm E}(Z_{n+1}|Z_{n})]=\sum_{k=0}^{\infty }{\rm E}(Z_{n+1}|Z_{n}=k)P(Z_{n}=k)\] \[=\sum_{k=0}^{\infty}{\rm E}(\sum_{j=1}^{k}X_{nj})P(Z_{n}=k)=\sum_ {k=0}^{\infty}mkP(Z_{n}=k)\] (b) \[=m{\rm E}(Z_{n})=\cdots=m^{n+1},\]

while the mg property follows from

(c) \[{\rm E}(W_{n+1}|{\cal A}_{n})=m^{-(n+1)}{\rm E}(Z_{n+1}|Z_{n})=m^{-(n+1)}mZ_{n }=W_{n}.\]

We leave (18) to the following exercise. \(\Box\)

**Exercise 4.4**: Verify the variance formula (18). Verify (20) below.

**Notation 4.1**: We define the generating functions \(f\) and \(f_{n}\) of \(X\) and \(Z_{n}\) by

\[f(s)\equiv\sum_{k=0}^{\infty}s^{k}p_{k}\quad\mbox{and}\quad f_{n}(s)\equiv\sum _{k=0}^{\infty}s^{k}P(Z_{n}=k). \tag{19}\]

It is easy to verify that

\[f_{n+1}(s)=f_{n}(f(s))=f(f_{n}(s))\qquad\mbox{for }|s|\leq 1. \tag{20}\]

**Theorem 4.1** (Branching process) (i) Suppose that \(m={\rm E}X>1\) and also \(\sigma^{2}\equiv{\rm Var}[X]<\infty\). Then

\[W_{n}\rightarrow_{a.s.\mbox{ and }}\;{\cal L}_{2}W_{\infty}\cong(1,\sigma^{2}/[m (m-1)]), \tag{21}\]

where \((W_{\infty},{\cal A}_{\infty})\) closes the mg. Also,

\[P(W_{\infty}=0)=\mbox{(the probability of ultimate extinction)}=\pi, \tag{22}\]

where

\[\pi\in(0,1)\quad\mbox{is the unique solution of}\qquad f(\pi)=\pi. \tag{23}\]

Moreover, the chf \(\phi\) of \(W_{\infty}\) is characterized as the unique solution of

\[\phi(mt)=f(\phi(t))\quad\mbox{for }t\in R\mbox{ subject }\phi(0)=1\quad\mbox{and}\quad\phi^{\prime}(0)=im. \tag{24}\]

(ii) If \(m\leq 1\), then \(W_{n}\rightarrow_{a.s.}0\) as \(n\rightarrow\infty\).

**Proof.** (i) Now, \({\rm E}W_{n}^{2}\leq 1+\sigma^{2}/[m(m-1)]\) for all \(n\), so that the mg \(\{W_{n},{\cal A}_{n}\}_{n=1}^{\infty}\) is square-integrable. Thus the mg \({\cal L}_{r}\) convergence of exercise 13.3.7 gives (21).

We let \(\pi^{*}\equiv P(W_{\infty}=0)\). Then

(a) \(\pi^{*}\)\(=\sum_{k=0}^{\infty}P(W_{\infty}=0|Z_{1}=k)P(Z_{1}=k)=\sum_{k=0}^{\infty}P(W_{ \infty}=0)^{k}p_{k}\) (b) \(=\sum_{k=0}^{\infty}\pi^{*k}p_{k}=f(\pi^{*})\).

Now, \(f(0)=p_{0}>0,f(1)=1,f^{\prime}(1-)=m>1\), and \(f^{\prime}(s)\) is \(\nearrow\) in \(s\) for \(0<s<1\); draw a figure. Thus \(f(\pi)=\pi\) has a unique solution in \((0,1)\). The solution \(\pi=1\) is ruled out by \({\rm Var}[W_{\infty}]>0\), since \(\pi=1\) would imply \(W_{\infty}\equiv 0\). (Note that (22) also follows from (20).)

We now turn to (24). Now,

(c) \(\phi_{n+1}(t)\equiv{\rm E}e^{itW_{n+1}}=\sum_{j=0}^{\infty}{\rm E}(\exp(itZ_{n +1}/m^{n+1})|Z_{1}=j)P(Z_{1}=j)\) (d) \(=\sum_{j=0}^{\infty}\phi_{n}(t/m)^{j}p_{j}\)

Since \(W_{n}\to_{a.s.}W_{\infty}\) implies \(W_{n}\to_{d}W_{\infty}\), we have \(\phi_{n}\to\phi\) on \(R\). Applying this to the identity in (c) and (d) gives

\(\phi(t)=\lim\phi_{n+1}(t)=\lim f(\phi_{n}(t/m))\) \(=f(\lim\phi_{n}(t/m))\) \(=f(\phi(t/m))\) since \(f\) is continuous on \(|r|\leq 1\) (e) \(=f(\phi(t/m))\).

Suppose now that \(\psi\) is any chf that satisfies \(\psi(t)=f(\psi(t/m))\). Then

\(\gamma(t)\equiv[\psi(t)-\phi(t)]/t=[(\psi(t)-1)-(\phi(t)-1)]/t\) \(\to\psi^{\prime}(0)-\phi^{\prime}(0)\) if \(\psi(0)=1\) and \(\psi^{\prime}(0)\) exists (f) \(=0\) if \(\psi^{\prime}(0)=im\)

as \(t\to 0\). Also,

\(|tm|\times|\gamma(tm)|=|\psi(tm)-\phi(tm)|=|f(\psi(t))-f(\phi(t))|\) \(\leq|f^{\prime}(t^{*})|\times|\psi(t)-\phi(t)|\) for \(t^{*}\in(0,1)\) by the mean value theorem \(\leq m|\psi(t)-\phi(t)|\) (g) \(=|tm|\times|\gamma(t)|\),

and iterating (e) gives

(h) \(|\gamma(t)|\leq|\gamma(t/m)|\leq\cdots\leq|\gamma(t/m^{n})|\to 0\),

so that \(\gamma(t)=0\) for all \(t\neq 0\). Trivially, \(\gamma(t)=0\) for \(t=0\). Thus, (24) holds.

(ii) Set \(s=0\) in (20) to get \(P(Z_{n+1}=0)=f(P(Z_{n}=0))\) here \(P(Z_{n}=0)\) is necessarily \(\nearrow\). Passing to the limit gives \(\pi=\lim P(Z_{n}=0)=f(\pi)\). But if \(m\leq 1\), then \(\pi=1\) is the only solution of \(\pi=f(\pi)\).

## 5 Decomposition of a Submartingale Sequence

**Definition 5.1** (Predictable process): A _predictable_ process \(\{A_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is one in which each \(A_{n}\) is \({\cal A}_{n-1}\)-measurable for each \(n\geq 0\); here \(A_{0}\) is a constant (or \(A_{0}\) is \(\{\emptyset,\Omega\}\)-measurable). [Especially interesting are processes that are both \(\nearrow\) and predictable, since any submg can be decomposed as the sum of a mg and such a predictable process.]

**Theorem 5.1** (Decomposition of a submg): Let \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) be a submg. Then \(X_{n}\) can be decomposed as

\[X_{n}=Y_{n}+A_{n}=[\mbox{a mg}]+[\mbox{an $\nearrow$}\quad\mbox{and predictable process}],\]

where \(\{Y_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a 0-mean mg and \(A_{n}\) is a predictable process satisfying

\[A_{0}\equiv\mbox{E}X_{0}\leq A_{1}\leq\cdots\leq A_{n}\leq\cdots\qquad\mbox{a.s.}\]

This decomposition is a.s. unique. Conversely, if \(X_{n}=Y_{n}+A_{n}\) as above, then \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a submg. [Call \(A_{n}\) the _compensator_.]

**Proof.** (Doob) Suppose that \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a submg. Let \(A_{0}\equiv\mbox{E}X_{0}\), and for \(n\geq 1\) define (the compensator candidate)

\[A_{n}\equiv\sum\limits_{k=1}^{n}[\mbox{E}(X_{k}|{\cal A}_{k-1})-X_{k-1}]+\mbox {E}X_{0}=\sum\limits_{k=1}^{n}\mbox{E}(\Delta X_{k}|{\cal A}_{k-1})+\mbox{E}X_{ 0},\]

with \(\Delta X_{k}\equiv X_{k}-X_{k-1}\). Clearly, \(A_{n}\) is an \(\nearrow\) process and each of the \(A_{n}\) is \({\cal A}_{n-1}\)-measurable. So, it remains only to show that \(Y_{n}\equiv X_{n}-A_{n}\) is a mg. Now,

\[\mbox{E}(Y_{n}|{\cal A}_{n-1})=\mbox{E}(X_{n}|{\cal A}_{n-1})-\mbox{E}(A_{n}|{ \cal A}_{n-1})=\mbox{E}(X_{n}|{\cal A}_{n-1})-A_{n}\]

(a) \[=\mbox{E}(X_{n}|{\cal A}_{n-1})-[\mbox{E}(X_{n}|{\cal A}_{n-1})-X_{n-1}]-A_{n-1}\]

(b) \[=X_{n-1}-A_{n-1}=Y_{n-1},\]

so \(\{Y_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is indeed a mg. Consider the uniqueness. Suppose \(X_{n}=Y_{n}+A_{n}\) is one such decomposition that works. Elementary computations give

\[\mbox{E}(X_{n}|{\cal A}_{n-1})=\mbox{E}(Y_{n}|{\cal A}_{n-1})+A_{n}=Y_{n-1}+A_ {n}\qquad\mbox{a.s.};\]

but the specification of the decomposition also states that

\[X_{n-1}=Y_{n-1}+A_{n-1}\qquad\mbox{a.s.}\]

Subtracting (d) from (c) gives uniqueness via

\[A_{n}-A_{n-1}=\mbox{E}(X_{n}|{\cal A}_{n-1})-X_{n-1}\qquad\mbox{a.s.}\]

The converse holds, since

\[\mbox{E}(Y_{n}+A_{n}|{\cal A}_{n-1})=Y_{n-1}+A_{n}\geq Y_{n-1}+A_{n-1}\qquad \mbox{a.s., for }n\geq 1\]

as required. \(\Box\)

**Exercise 5.1**: If the \(X\)-process is integrable, then the \(A\)-process is uniformly integrable (in either theorem 5.1 above or theorem 5.2 below).

**Theorem 5.2** (**Decomposition of a reversed submg**): Let \(\{X_{n},{\cal A}_{n}\}_{n=-\infty}^{0}\) be a submg such that \({\rm E}(X_{n})\searrow M>-\infty\) as \(n\searrow-\infty\) (thus, the \(X_{n}\)-process is uniformly integrable with \(X_{n}\to\) (some \(X\)) a.s. and \({\cal L}_{1}\) where \({\rm E}X_{-\infty}=M\)). Then \(X_{n}\) can be decomposed as

\[X_{n}=Y_{n}+A_{n}=[{\rm a\ mg}]+[{\rm an\ }\nearrow\ \mbox{ and predictable process that is }\geq 0], \tag{4}\]

where \(\{Y_{n},{\cal A}_{n}\}_{n=-\infty}^{0}\) is a mean-\(M\) mg and \(A_{n}\) is an \({\cal A}_{n-1}\)-measurable function with

\[0=A_{-\infty}\equiv\ \lim_{n\to-\infty}\ A_{n}\leq\cdots\leq A_{n}\leq\cdots\leq A_{0}\quad{\rm a.s.} \tag{5}\]

This decomposition is a.s. unique. Conversely, if \(X_{n}=Y_{n}+A_{n}\) as above, then \(\{X_{n},{\cal A}_{n}\}_{n=-\infty}^{0}\) is a submg. [Call \(A_{n}\) the _compensator_.]

**Proof.** (Doob) We define

\[A_{n}\equiv\sum_{k=-\infty}^{n}[{\rm E}(X_{k}|{\cal A}_{k-1})-X_{k-1}]\quad{\rm for }\ n\leq 0; \tag{6}\]

then \(A_{n}\) is clearly \(\geq 0,\nearrow\), and \({\cal A}_{n-1}\)-measurable, provided that it can be shown to be well-defined (that is, provided the sum converges a.s.). Now, with \(n\leq m\),

(a) \[{\rm E}(A_{m}-A_{n})=\sum_{n+1}^{m}{\rm E}(X_{k}-X_{k-1})={\rm E}X_{n}-{\rm E}X_{m} \leq{\rm E}X_{0}-{\rm E}X_{m}\] (b) \[\leq({\rm E}X_{0}-M)<\infty,\]

by hypothesis. Also,

(c) \[\tilde{A}_{m}\equiv\ \lim_{n\to-\infty}\ (A_{m}-A_{n})=\ \lim_{n\to-\infty}\ \sum_{n+1}^{m}[{\rm E}(X_{k}|{\cal A}_{k-1})-X_{k-1}]\]

is \(\geq 0\) and \(\nearrow\), so that the MCT gives

(d) \[{\rm E}\tilde{A}_{m}=\ \lim_{n\to-\infty}\ {\rm E}(A_{m}-A_{n})={\rm E}X_{m}-\ \lim_{n\to-\infty}\ {\rm E}X_{n}={\rm E}X_{m}-M<\infty\]

with a well-defined finite limit. Since \(\tilde{A}_{m}\geq 0\) and \({\rm E}\tilde{A}_{m}<\infty\), we know that \(\tilde{A}_{m}\) is finite a.s.; so (6) is well-defined. The \(\tilde{A}_{m}\)'s are \(\nearrow\) and bounded below by \(0\). Thus \(A_{-\infty}\equiv\lim_{m\to-\infty}\tilde{A}_{m}\) exists a.s., and it is \(\geq 0\). Moreover, the equalities in (d) show that \({\rm E}\tilde{A}_{m}\to 0\) as \(m\to-\infty\); just use the MCT via

(e) \[{\rm E}A_{-\infty}=\lim_{m\to-\infty}{\rm E}\tilde{A}_{m}=\lim_{m}{\rm E}X_{m} -M=M-M=0.\]

Thus \(A_{-\infty}=0\) a.s., and each \(\tilde{A}_{m}=A_{m}\) a.s.

Let \(Y_{n}\equiv X_{n}\)-\(A_{n}\). Lines (a)-(f) of the previous proof complete this proof, using (13.3.20) about mgs for the existence of \(Y_{-\infty}\). \(\Box\)

**Example 5.1** (**Predictable variation, or conditional variance of a mg**): Let \(\{X_{n},\)\({\cal A}_{n}\}_{n=0}^{\infty}\) be a mg with each \({\rm E}X_{n}^{2}<\infty\). Then \(\{X_{n}^{2},{\cal A}_{n}\}_{n=0}^{\infty}\) is a submg by proposition 13.1.2. By theorem 5.1, there is a decomposition for which

\[Z_{n}\equiv X_{n}^{2}-A_{n}\quad{\rm is\ a\ 0\mbox{-mean\ mg\ adapted\ to\ the\ }}\ {\cal A}_{n}\mbox{'s\ for\ }n\geq 0. \tag{7}\]

Here \(A_{n}\) is the predictable process (with \(A_{0}\equiv{\rm E}X_{0}^{2}\geq 0\)) defined by

\[A_{n}\equiv\sum_{k=1}^{n}\{{\rm E}(X_{k}^{2}|{\cal A}_{k-1})-X_{k-1}^{2}\}+{ \rm E}X_{0}^{2} \tag{8}\] \[=\sum_{k=1}^{n}{\rm E}\{X_{k}^{2}-X_{k-1}^{2}|{\cal A}_{k-1}\}+{ \rm E}X_{0}^{2}\] (9) \[=\sum_{k=1}^{n}{\rm E}\{(\Delta X_{k})^{2}|{\cal A}_{k-1}\}+{\rm E }X_{0}^{2},\quad{\rm for\ }n\geq 1,\]where \(\Delta X_{k}\equiv X_{k}-X_{k-1}\) and \(\Delta X_{0}=X_{0}\). The compensator term \(A_{n}\) (of the \(X_{n^{-}}^{2}\)-process) given in (9) is called the _conditional variance_ or the _predictable variation_ of the \(X_{n}\)-process. Note that (for \(\langle X\rangle_{n}\equiv A_{n}\)),

\[{\rm E}X_{n}^{2}={\rm E}\langle X\rangle_{n}={\rm E}A_{n}=\sum_{k=1}^{n}{\rm Var }\;[\Delta X_{k}]+{\rm E}X_{0}^{2}, \tag{10}\]

since we agree to also use the notation \(\langle X\rangle_{n}\) to denote the predictable variation process \(A_{n}\) that corresponds to the mg \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\).

**Summary**  For any mg \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) having all \({\rm E}X_{n}^{2}\) finite,

\[\langle X\rangle_{n}\equiv A_{n}=\sum_{k=1}^{n}{\rm E}\{(\Delta X_{k})^{2}|{ \cal A}_{k-1}\}+{\rm E}X_{0}^{2} \tag{11}\]

is always the _predictable variation_ (or _conditional variance_, or _compensator_), and the conditionally centered process

\[Z_{n}\equiv X_{n}^{2}-\langle X\rangle_{n}\quad\mbox{is a 0-mean mg with respect to the ${\cal A}_{n}$'s},\mbox{ for }n\geq 0. \tag{12}\]

**Martingale Transforms**

**Definition 5.2** (H-transforms)  Let \(\{H_{n}\}_{n=0}^{\infty}\) be a predictable process with respect to the filtration \(\{{\cal A}_{n}\}_{n=0}^{\infty}\). [Think of \(H_{n}\) being the amount a gambler will wager at stage \(n\), based only on complete knowledge of the outcomes of the game up through time \(n-1\) (but not, of course, through time \(n\)).] For some other process \(\{X_{n}\}_{n=0}^{\infty}\), define the H-_transform_ of \(X\) (to be denoted by \(\{(H\cdot X)_{n}\}_{n=0}^{\infty}\)) by

\[(H\cdot X)_{n}\equiv\sum_{k=1}^{n}H_{k}(X_{k}-X_{k-1})+H_{0}X_{0}=\sum_{k=0}^{ n}H_{k}\Delta X_{k}. \tag{13}\]

(We agree that \(\Delta X_{0}\equiv X_{0},\)and that \(H_{0}\) is a constant.)

**Theorem 5.3** (S-mg transforms) (i) Let \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) be a s-mg (or supermg). If \(\{H_{n}\}_{n=0}^{\infty}\) is predictable with each \(H_{n}\geq 0\) and bounded, then \(\{(H\cdot X)_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a s-mg (or supermg). (The supermartingale case shows that there is no system for beating the house in an unfavorable game.)

(ii) If \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a mg and \(\{H_{n}\}_{n=0}^{\infty}\) is predictable and bounded, then note that the process \(\{(H\cdot X)_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a mg with mean \(H_{0}{\rm E}X_{0}\).

**Proof.**  We compute

\[\mbox{(a)}\qquad{\rm E}[(H\cdot X)_{n+1}|{\cal A}_{n}]=(H\cdot X)_{n}+{\rm E} [H_{n+1}(X_{n+1}-X_{n})|{\cal A}_{n}]\] (b) \[=(H\cdot X)_{n}+H_{n+1}{\rm E}[\Delta X_{n+1}|{\cal A}_{n}]\] (c) \[\geqq(H\cdot X)_{n},\]

since \(H_{n+1}\geq 0\) and \({\rm E}(\Delta X_{n+1}|{\cal A}_{n})\geqq 0\). Note that \({\rm E}\Delta X_{0}={\rm E}X_{0}\) in the mg case. (The supermg case just reverses the inequality.) \(\Box\)

**Corollary 1**  If \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a s-mg and \(T\) is a stopping time, then:

\[\mbox{(a)}\qquad H_{n}\equiv 1_{[T\geq n]}\qquad\mbox{is}\geq 0,\mbox{ bounded, and predictable.} \tag{2}\] \[(H\cdot X)_{n}=X_{T\wedge n}=(\mbox{the stopped process})\qquad\mbox{is a s-mg.}\]

**Proof.** Now, \(H_{n}\) is predictable, since \([T\geq n]=[T\leq n-1]^{c}\in{\cal A}_{n-1}\) for a stopping time \(T\). Furthermore,

(a) \[(H\cdot X)_{n}=\sum_{k=0}^{n}1_{[T\geq k]}(X_{k}-X_{k-1})=\sum_{k=0}^{n}1_{[T \geq k]}\Delta X_{k}=X_{T\wedge n}\]

(the sum ends at \(m\) if \(T(\omega)=m\in[0,n]\); else, at \(n\)). Then, apply theorem 5.3. \(\Box\)

**Notation 5.1** Suppose \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a mg with \({\rm E}X_{n}^{2}<\infty\), so that \(\{X_{n}^{2}\}_{n=0}^{\infty}\) is a submg with predictable variation process \(\{\langle X\rangle_{n}\}_{n=0}^{\infty}\). Let \(\{H_{n}\}_{n=0}^{\infty}\) denote a predictable, bounded, and \(\geq 0\) process. Then we know that \(\{(H\cdot X)_{n}^{2}\}\) is a submg. We will now give the form taken by its predictable variation process \(\langle H\cdot X\rangle_{n}\). Also, we will summarize everything so far in one place. \(\Box\)

**Theorem 5.4** (Martingale transforms): Suppose \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) is a mg with \({\rm E}X_{n}^{2}<\infty\) for each \(n\), and let \(\{H_{n}\}_{n=0}^{\infty}\) be bounded, predictable, and \(\geq 0\). Then the predictable variation process \(\langle X\rangle_{n}\) is given by

\[\langle X\rangle_{n}\equiv A_{n}=\sum_{k=1}^{n}{\rm E}\{(\Delta X_{k})^{2}|{ \cal A}_{k-1}\}+{\rm E}X_{0}^{2}. \tag{14}\]

Then the conditionally centered process

\[Z_{n}\equiv X_{n}^{2}-\langle X\rangle_{n}\quad\mbox{is a 0-mean mg with respect to the ${\cal A}_{n}$'s}, \tag{15}\]

for \(n\geq 0\). The martingale transform

\[W_{n}\equiv(H\cdot X)_{n}\equiv\sum_{k=0}^{n}H_{k}\Delta X_{k} \tag{16}\]

is a mg with mean \(H_{0}{\rm E}X_{0}\) with respect to the \({\cal A}_{n}\)'s,

for \(n\geq 0\). Its _predictable variation process_\(\langle W\rangle_{n}\) is

\[\langle W\rangle_{n}\equiv\langle H\cdot X\rangle_{n}=\sum_{k=1}^{n}H_{k}^{2}{ \rm E}\{(\Delta X_{k})^{2}|{\cal A}_{k-1}\}+H_{0}^{2}{\rm E}X_{0}^{2}. \tag{17}\]

Moreover, for \(n\geq 0\), the sequence

\[L_{n}\equiv W_{n}^{2}-\langle W\rangle_{n}\equiv\{(H\cdot X)_{n}\}^{2}-\langle (H\cdot X)\rangle_{n} \tag{18}\]

is a 0-mean mg with respect to the \({\cal A}_{n}\)'s (with \(L_{0}=H_{0}^{2}(X_{0}^{2}-{\rm E}X_{0}^{2})\)).

**Proof.** Recall example 5.1 and theorem 5.3 for the first parts. Then by straightforward calculation from (13), we have

(a) \[\langle H\cdot X\rangle_{n}=\sum_{k=1}^{n}{\rm E}\{[\Delta(H\cdot X )_{k}]^{2}|{\cal A}_{k-1}\}+H_{0}^{2}{\rm E}X_{0}^{2}\] \[\qquad\mbox{since $(H\cdot X)_{0}=H_{0}X_{0}$}\] \[=\sum_{k=1}^{n}{\rm E}\;\{[H_{k}(X_{k}-X_{k-1})]^{2}|{\cal A}_{k- 1}\}+H_{0}^{2}{\rm E}X_{0}^{2}\] \[=\sum_{k=1}^{n}H_{k}^{2}\times{\rm E}\{(\Delta X_{k})^{2}|{\cal A }_{k-1}\}+H_{0}^{2}{\rm E}X_{0}^{2}\] \[\qquad\mbox{since $H_{k}$ is ${\cal A}_{k-1}$-measurable}\] (b) \[=\sum_{k=0}^{n}H_{k}^{2}\,\Delta\langle X\rangle_{k}.\]

Note that \(L_{0}=W_{0}^{2}-\langle W_{0}\rangle=[H_{0}\Delta X_{0}]^{2}-H_{0}^{2}{\rm E}(X _{0}^{2})=H_{0}^{2}(X_{0}^{2}-{\rm E}X_{0}^{2})\) has mean 0, while \(L_{n}\) is a mg by example 5.1. \(\Box\)

**Exercise 5.2** Let \((X_{n},{\cal A}_{n})_{n=0}^{\infty}\) be a submg. Let \(M_{n}\equiv\sup\{|X_{n}|\ :\ n\geq 1\}\). Use the Doob-Meyer decomposition and Doob's inequality 8.10.2 to show that

\[P(M_{n}\geq\lambda)\leq\frac{3}{3}\sup_{n}{\rm E}|X_{n}|\quad\mbox{for all $\lambda>0$}. \tag{19}\]

## 6 Optional Sampling

We now extend the simple optional sampling theorem of section 13.2. Our aim will be to relax the restrictive assumption used there, that the stopping times are bounded.

##### Discrete Time

**Notation 6.1**  Suppose that

(1) \(\{X_{n},\mathcal{A}_{n}\}_{n=0}^{\infty}\) is a s-mg

and that

(2) \(0\leq_{a.s.}T_{0}\leq_{a.s.}T_{1}\leq_{a.s.}\cdots<_{a.s.}\infty\) for stopping times \(T_{0},T_{1},\dots\).

We define

(3) \(\tilde{X}_{n}\equiv X_{T_{n}}\) and \(\tilde{\mathcal{A}}_{n}\equiv\mathcal{A}_{T_{n}}\),

so that the \(\tilde{X}_{n}\)'s are adapted to the \(\tilde{\mathcal{A}}_{n}\)'s. We would like to prove that \(\{\tilde{X}_{n},\tilde{\mathcal{A}}_{n}\}_{n=0}^{\infty}\) is a s-mg, but this requires hypotheses. The weakest such hypotheses presented are

(4) \(\mathrm{E}|\tilde{X}_{n}|<\infty\) for all \(n\), and

(5) \(\liminf_{k\to\infty}\int_{[T_{n}>k]}X_{k}^{+}\,dP=0\) for each \(n\).

(But these conditions (4) and (5) need to be replaced by _useful_ conditions that are more easily verifiable.) \(\square\)

**Theorem 6.1** (Optional sampling theorem): Let (1)-(3) define the sequence \(\{\tilde{X}_{n},\break\tilde{\mathcal{A}}_{n}\}_{n=0}^{\infty}\) with respect to the s-mg \(\{X_{n},\mathcal{A}_{n}\}_{n=0}^{\infty}\). Suppose (4) and (5) hold. Then the optionally sampled process

(6) \(\{\tilde{X}_{n},\tilde{\mathcal{A}}_{n}\}_{n=0}^{\infty}\) is a s-mg

for which

(7) \(\mathrm{E}X_{0}\leqq\mathrm{E}\tilde{X}_{0}\leqq\dots\leqq\mathrm{E}\tilde{X}_ {n}\leqq\dots\leqq\sup_{k}\mathrm{E}X_{k}\leq\infty\).

##### Corollary 1

(A) Condition (4) holds if \(\{X_{n},\mathcal{A}_{n}\}_{n=0}^{\infty}\) is integrable.

(B) Conditions (4) and (5) both hold if any of the following conditions holds:

(8) Each \(T_{n}\) is a.s. bounded by some fixed integer \(N_{n}\).

(9) The \(X_{n}\)'s are uniformly integrable.

(10) \(X_{n}\leq_{a.s.}\) (some \(M\)) \(<\infty\) for all \(n\).

(11) \(\mathrm{E}T_{j}<\infty\) for all \(j\), and there exists a constant \(K\) such that for all \(j\),

(12) \(\mathrm{E}(|X_{n}-X_{n-1}||\mathcal{A}_{n-1})(\omega)\leq K\) for all \(n\leq T_{j}(\omega)\) holds a.s.

**Notation 6.2** The theorem becomes much cleaner if our s-mg includes an entry at \(\infty\) that closes the s-mg. Suppose

\[\{X_{n},\mathcal{A}_{n}\}_{n=0}^{\infty}\qquad\mbox{ is a s-mg} \tag{12}\]

and

\[0\leq_{a.s.}\ T_{0}\leq_{a.s.}\ T_{1}\leq_{a.s.}\ \cdots\leq T_{\infty}\leq_{a.s.}\ \infty \tag{13}\]

for extended stopping times \(T_{0},T_{1},\ldots,T_{\infty}\). We again define

\[\tilde{X}_{n}\equiv X_{T_{n}}\quad\mbox{and}\quad\tilde{\mathcal{A}}_{n}\equiv \mathcal{A}_{T_{n}}, \tag{14}\]

so that the \(\tilde{X}_{n}\)'s are adapted to the \(\tilde{\mathcal{A}}_{n}\)'s (for \(0\leq n\leq\infty\)). \(\square\)

**Theorem 6.2** (Optional sampling theorem): Suppose (12)-(14) hold. Then

\[\{\tilde{X}_{n},\tilde{\mathcal{A}}_{n}\}_{n=0}^{\overline{\infty}}\quad\mbox {is a s-mg} \tag{15}\]

with \(\mbox{E}X_{0}\leqq\mbox{E}X_{T_{0}}\leqq\mbox{E}X_{T_{1}}\leqq\ldots\leqq \mbox{E}X_{T_{\infty}}\leqq\mbox{E}X_{\infty}<\infty\).

**Continuous Time**

**Theorem 6.3** (Optional sampling theorem): Suppose that the \(X\)-process is integrable and satisfies \(X\colon\ (\Omega,\mathcal{A},P)\to(D_{[0,\infty)},\mathcal{D}_{[0,\infty)})\), and that it is adapted to some filtration \(\{\mathcal{A}_{t}\}_{t\in[0,\infty)}\). Then

\[\begin{array}{l}n,X_{n},\mathcal{A}_{n},\{0,1,...\}\{0,1,\ldots,\infty\},k \quad\mbox{may be replaced by}\\ t,X_{t},\mathcal{A}_{t},[0,\infty),[0,\infty],s\end{array} \tag{16}\]

in theorem 6.1, corollary 1 (only (11) must be omitted from the list of things that carry over with no change), and theorem 6.2.

**Proofs\({}^{o}\)**

**Proof.** Consider theorem 6.1. Let \(A\in\mathcal{A}_{T_{n-1}}\). It suffices to show that

(a) \[\int_{A}X_{T_{n-1}}\,dP\leqq\int_{A}X_{T_{n}}\,dP.\]

Basically, we wish to use proposition 13.2.1 and the DCT. To this end we define

(b) \[T_{n}^{(k)}\equiv T_{n}\wedge k=(\mbox{a bounded stopping time})\nearrow T_{n}.\]

Now \(A\cap[T_{n-1}\leq k]\in\mathcal{A}_{T_{n-1}^{(k)}}\), since for each \(m\geq 0\) we have

\[\begin{array}{l}A\cap[T_{n-1}\leq k]\cap[T_{n-1}^{(k)}\leq m]=A\cap[T_{n-1} \leq k]\cap[T_{n-1}\wedge k\leq m\wedge k]\\ \qquad=A\cap[T_{n-1}\leq m\wedge k]\in\mathcal{A}_{m\wedge k}\subset\mathcal{A }_{m}.\end{array}\]

Thus, for \(n\) fixed we have

(c) \[\begin{array}{l}\int_{A\cap[T_{n-1}\leq k]}X_{T_{n-1}}\,dP\\ \qquad=\int_{A\cap[T_{n-1}\leq k]}X_{T_{n-1}^{(k)}}\,dP\qquad\mbox{ as the integrands are equal on the set}\\ \mbox{(d)}\leqq\int_{A\cap[T_{n-1}\leq k]}X_{T_{n}^{(k)}}\,dP\qquad\mbox{ by proposition \ref{prop:13.2.1}}\\ \mbox{(e)}\leqq\int_{A\cap[T_{n-1}\leq k]\cap[T_{n}\leq k]}X_{T_{n}}\,dP+\int_{A \cap[T_{n-1}\leq k]\cap[T_{n}>k]}X_{k}\,dP,\end{array}\]since \(X_{T_{n}^{(k)}}=X_{k}\) on \([T_{n}>k]\). Let \(k\to\infty\) in (c) and (e); and since \(T_{n}<\infty\) a.s. and since \({\rm E}|X_{T_{n-1}}|\) and \({\rm E}|X_{T_{n}}|\) are finite, the DCT implies (recall that \(a=b\oplus c\) means that \(|a-b|\leq c\))

(f) \(\qquad\int_{A}X_{T_{n-1}}\,dP\)

(g) \(\qquad\leqq\int_{A}X_{T_{n}}\,dP\oplus\underline{\lim}\int_{[T_{n}>k]}X_{k}^{+}\,dP\)

(h) \(\qquad\equiv\int_{A}X_{T_{n}}\,dP\oplus\underline{\lim}\,a_{k},\qquad\mbox{ where }\,\underline{\lim}_{k\to\infty}\,a_{k}=0\) by (5).

Equate the terms in (f) and (h) on a subsequence \(k^{\prime}\) having \(a_{k^{\prime}}\to 0\) to obtain

(i) \(\qquad\int_{A}X_{T_{n-1}}\,dP\leqq\int_{A}X_{T_{n}}\,dP\qquad\mbox{ for all }\qquad A\in{\cal A}_{T_{n-1}}\).

This is equivalent to (6) by (13.1.4).

Letting \(A=\Omega\) in (i) shows that \({\rm E}X_{T_{n}}\)is \(\nearrow\). Introducing the new stopping time \(\tau\equiv 0\leq T_{0}\) and applying this result shows that \({\rm E}X_{0}\equiv{\rm E}X_{\tau}\leqq{\rm E}X_{T_{n}}\).

It remains to show that \({\rm E}X_{T_{n}}\leqq\sup_{k}{\rm E}X_{k}\). Now,

(j) \(\qquad{\rm E}X_{T_{n}}=\int_{[T_{n}\leq k]}X_{T_{n}}\,dP+\int_{[T_{n}>k]}X_{T_ {n}}\,dP\)

(k) \(\qquad=\int_{[T_{n}\leq k]}X_{T_{n}^{(k)}}\,dP+\int_{[T_{n}>k]}X_{T_{n}}\,dP\)

(k) \(\qquad\leqq\int_{[T_{n}\leq k]}X_{k}\,dP+\int_{[T_{n}>k]}X_{T_{n}}\,dP\pm\int_{ [T_{n}>k]}X_{k}\,dP\)

by proposition 13.2.1, since \([T_{n}\leq k]\in{\cal A}_{T_{n}^{(k)}}\)

as \([T_{n}\leq k]\cap[T_{n}\wedge k\leq m]=[T_{n}\leq m\wedge k]\in{\cal A}_{k \wedge m}\in{\cal A}_{m}\), for \(m\geq 0\)

(l) \(\qquad\leqq{\rm E}X_{k}\oplus\int_{[T_{n}>k]}X_{k}^{+}\,dP\oplus\int_{[T_{n}>k ]}X_{T_{n}}\,dP\).

For the second term in (1) we recall (5). For the third term in (1) we note that \({\rm E}|X_{T_{n}}|<\infty\) and that \(T_{n}<\infty\) a.s. We thus conclude from (1) that

(m) \(\qquad{\rm E}X_{T_{n}}\leqq\overline{\lim}_{k\to\infty}\,EX_{k}+0+0\)

(n) \(\qquad\leqq\sup_{k}{\rm E}X_{k}\).

This gives (7). 

**Proof.** Consider the first claim made in the corollary. That is, we verify that (4) holds if \(\sup_{k}{\rm E}|X_{k}|<\infty\). Let \(T_{n}^{(k)}\equiv T_{n\wedge k}\). Now, both \(\{X_{n},{\cal A}_{n}\}_{n=0}^{\infty}\) and \(\{X_{n}^{+},{\cal A}_{n}\}_{n=0}^{\infty}\) are submgs. Since \(0\leq T_{n}^{(k)}\leq k\), proposition 13.2.1 implies that both

(a) \(\qquad X_{0},X_{T_{n}^{(k)}},X_{k}\) and \(X_{0}^{+},X_{T_{n}^{(k)}}^{+},X_{k}^{+}\) are submgs.

Thus,

(b) \(\qquad{\rm E}|X_{T_{n}^{(k)}}|={\rm E}[2X_{T_{n}^{(k)}}^{+}-X_{T_{n}^{(k)}}] \qquad\mbox{since }|x|=2x^{+}-x\)

(c) \(\qquad\leq 2{\rm E}X_{k}^{+}-{\rm E}X_{0}\qquad\mbox{using (a)}\)

\(\qquad\leq 2{\rm E}|X_{k}|-{\rm E}X_{0}\)

\(\qquad\leq 2\sup_{k}{\rm E}|X_{k}|-{\rm E}X_{0}\)

(d) \(\qquad\leq(\mbox{some M})<\infty\qquad\mbox{by hypothesis.}\)

[MISSING_PAGE_FAIL:384]

Let \(A\in{\cal A}_{T_{n-1}}\); recall that \(T_{n-1}\) could now equal \(+\infty\). Even so, \(A\cap[T_{n-1}\leq k]\in{\cal A}_{T_{n-1}^{(k)}}\), as shown at the start of the proof of theorem 6.1 Thus,

(b) \[\int_{A\cap[T_{n-1}\leq k]}X_{T_{n-1}}\,dP\] \[=\int_{A\cap[T_{n-1}\leq k]}X_{T_{n-1}^{(k)}}\,dP\quad\mbox{ since $T_{n-1}^{(k)}=T_{n-1}$ on $[T_{n-1}\leq k]$}\] \[\leq\int_{A\cap[T_{n-1}\leq k]}X_{T_{n}^{(k)}}\,dP\quad\mbox{ by proposition \ref{prop:13.2.1}}\] (c) \[=\int_{A\cap[T_{n-1}\leq k]\cap[T_{n}>k]}X_{k}\,dP+\int_{A\cap[T_{n-1} \leq k]\cap[T_{n}\leq k]}X_{T_{n}}\,dP,\]

since \(X_{T_{n}^{(k)}}=X_{k}\) on \([T_{n}>k]\). Let \(k\to\infty\) in (b) and (c); since (a) shows that \({\rm E}|X_{T_{n-1}}|\) and \({\rm E}|X_{T_{n}}|\) are finite, the DCT gives

(d) \[\int_{A\cap[T_{n-1}<\infty]}X_{T_{n-1}}\,dP\] \[\leq\underline{\rm lim}_{k\to\infty}\int_{A\cap[T_{n-1}\leq k] \cap[T_{n}>k]}X_{k}\,dP+\int_{A\cap[T_{n}<\infty]}X_{T_{n}}\,dP\] (e) \[=\int_{A\cap[T_{n-1}<\infty]\cap[T_{n}=+\infty]}X_{\infty}\,dP+\int_{A \cap[T_{n}<\infty]\cap[T_{n-1}<\infty]}X_{T_{n}}\,dP\] \[\mbox{ using $X_{k}\to_{\cal L}$}\,X_{\infty}\] (f) \[=\int_{A\cap[T_{n-1}<\infty]}X_{T_{n}}\,dP.\]

We add to each of (d) and (f) the equal terms of the equation

(g) \[\int_{A\cap[T_{n-1}=\infty]}X_{T_{n-1}}\,dP=\int_{A\cap[T_{n-1}=\infty]}X_{T_{ n}}\,dP,\]

and obtain

(h) \[\int_{A}X_{T_{n-1}}\,dP\leqq\int_{A}X_{T_{n}}\,dP\mbox{ for all $A\in{\cal A}_{T_{n-1}}$}.\]

Replace \(T_{n}\) by \(T_{\infty}\) in the previous paragraph to see that

(i) \[\int_{A}X_{T_{n-1}}\,dP\leqq\int_{A}X_{T_{\infty}}\,dP\mbox{ for all $A\in{\cal A}_{T_{n-1}}$}.\]

Finally, (h), (i), and (13.1.4) show that \(\{X_{T_{n}},{\cal A}_{T_{n}}\}_{n=0}^{\overline{\infty}}\) is a s-mg. Add in \(T_{a}\equiv 0\) and \(T_{b}\equiv\infty\) for the expectation claim, with \(A=\Omega\) in (h) and (i). \(\Box\)

**Proof.** Consider theorem 6.3. We must consider the extension of theorem 6.2. It suffices to consider the stopping times a pair at a time; we will do so, relabeling them so that \(S\leq T\) a.s. Let \(D_{n}\equiv\{k/2^{n}:k=0,1,\dots,\}\), and note that

(a) \[\{X_{t},{\cal A}_{t}\}_{t\in D_{n}}\qquad\mbox{ is a s-mg.}\]

Define extended stopping times \(T^{(n)}\) by

(b) \[T^{(n)}(\omega)=\cases{k/2^{n}&\mbox{ whenever $(k-1)/2^{n}<T(\omega)\leq k/2^{n}$ and $k\geq 0$},\cr\infty&\mbox{ whenever $T(\omega)=\infty$},\cr}\]

and make an analogous definition for \(S^{(n)}\); it is trivial that these rvs are extended stopping times. Note that a.s.

(c) \[S^{(n)}\leq T^{(n)},\quad S\leq S^{(n)},\quad T\leq T^{(n)}\ S^{(n)}\searrow S,\quad T^{(n)}\searrow T.\]

We can apply theorem 6.2 to \(S^{(n)}\) and \(T^{(n)}\) to conclude (as in (h), just above) that

(d) \[\int_{A}X_{S^{(n)}}\,dP\leqq\int_{A}X_{T^{(n)}}\,dP\qquad\mbox{ for all $A\in{\cal A}_{S}\subset{\cal A}_{S^{(n)}}$}.\]Now, by right continuity of the paths,

(e) \[X_{S^{(n)}}\to_{a.s.}X_{S}\qquad\text{ and }\qquad X_{T^{(n)}}\to_{a.s.}X_{T} \qquad\text{ as }n\to\infty.\]

Thus Vitali's theorem allows us to pass to the limit in (d) and obtain

(f) \[\int_{A}X_{S}\,dP\leqq\int_{A}X_{T}\,dP\qquad\text{ for all }A\in\mathcal{A}_{S},\]

provided that we show that

(g) the \(X_{T^{(n)}}\)'s (and analogously the \(X_{S^{(n)}}\)'s) are uniformly integrable.

Since \(0\leq T^{(n)}\leq T^{(n-1)}\) with both taking values in \(D_{n}\), theorem 6.2 gives

(h) \[X_{T^{(n)}}\leqq\operatorname{E}(X_{T^{(n-1)}}|\mathcal{A}_{T^{(n)}})\text{ a.s.}\]

Thus

\[\{Y_{n},\mathcal{B}_{n}\}_{n=-\infty}^{0}\quad\text{is a reversed s-mg, where }Y_{n}\equiv X_{T^{(-n)}}\text{ and }\mathcal{B}_{n}\equiv\mathcal{A}_{T^{(-n)}}. \tag{17}\]

From this we need only the rather minor fact (since \(\operatorname{E}X_{0}\leqq\operatorname{E}X_{T^{(n)}}\) for this s-mg pair) that

(i) \[\lim_{n\to-\infty}\,\operatorname{E}Y_{n}=\,\lim_{n\to-\infty}\,\operatorname{ E}(X_{T^{(-n)}})\geqq\operatorname{E}X_{0}>-\infty.\]

Thus the reversed s-mg theorem implies (g). Let \(A=\Omega\) in (f) for

(j) \[-\infty\leq\operatorname{E}X_{0}\leqq\operatorname{E}X_{S}\leqq\operatorname{ E}X_{T}\leqq\operatorname{E}X_{\infty}<\infty\]

(we also apply (f) to \(T_{0}\equiv 0\) and \(T_{\infty}\equiv\infty\)). Then (f) and (j) finish the proof. 

**Exercise 6.1**: Prove theorem 6.3 (for the case of integrable \(X_{t}\) in theorem 6.1).

**Exercise 6.2**: Prove theorem 6.3 (for the corollary to theorem 6.1 case).

**Exercise 6.3**: Write out all the details of step (h), in the context ot theorem 6.2.

## 7 Applications of Optional Sampling

**Example 7.1** (Gambler's ruin): Suppose \(Y_{1},Y_{2},\ldots\) are iid with \(p\!\equiv\!P(Y_{1}\!\!=\!\!1)\) and with \(q\!\equiv\!P(Y_{1}\!\!=\!\!-1)\). Let \(S_{n}\!\equiv\!Y_{1}+\cdots+Y_{n}\). Let \(-a\!<\!0\!<\!b\) be integers, and define the stopping time

\[\tau\equiv\inf\{n:S_{n}=-a\mbox{ or }b\}. \tag{1}\]

Define \({\cal A}_{n}\equiv\sigma[X_{1},\ldots,X_{n}]\). Let \(\phi(t)\!=\!pe^{t}+qe^{-t}\) denote the mgf of \(Y\). Let \(c_{0}\!\equiv\!\log(q/p)\), and note that \(\phi(c_{0})=1\). We now apply the examples of section 13.1. When \(p=q=\frac{1}{2}\) :

\[S_{n} \mbox{ is a mean-0 mg}, \tag{3}\] \[Z_{n}^{(1)}\equiv S_{n}^{2}-n \mbox{ is a mean-0 mg}. \tag{2}\]

For general \(p\) and \(q\) with \(p\in(0,1)\) :

\[Z_{n}^{(2)}\equiv S_{n}-n(p-q) \mbox{ is a mean-0 mg}, \tag{5}\] \[Z_{n}\equiv(q/p)^{S_{n}}=\exp(c_{0}S_{n})=\exp(c_{0}S_{n})/\phi^ {n}(c_{0}) \mbox{ is a mean-1 mg}. \tag{4}\]

We now make the claim (see exercise 7.1 below)

\[{\rm E}S_{\tau}=0,\quad{\rm E}Z_{\tau}^{(1)}=0,\quad{\rm E}Z_{\tau}^{(2)}=0, \quad{\rm E}Z_{\tau}=1. \tag{6}\]

With probability 1, the rv \(S_{\tau}\) takes on one of the values \(-a\) or \(b\). Now, \(\tau\wedge m\nearrow\tau\) a.s. and \(S_{\tau\wedge m}\to S_{\tau}\) a.s., while \((\tau\wedge m)\) is a bounded stopping time to which proposition 13.2.1 or theorem 6.1 necessarily applies. Thus, for \(p=q=\frac{1}{2}\) we can conclude that

\[0=\lim_{m}0=\lim_{m}{\rm E}S_{\tau\wedge m}={\rm E}S_{\tau}=-aP(S_{\tau}=-a)+b [1-P(S_{\tau}=-a)]\]

by proposition 13.2.1 and the DCT with dominating function \(a+b\); and

\[{\rm E}\tau=\lim{\rm E}(\tau\wedge m)={\rm E}S_{\tau\wedge m}^{2}\to{\rm E}S_{ \tau}^{2}=a^{2}P(S_{\tau}=-a)+b^{2}P(S_{\tau}=b)\]

by the MCT, proposition 13.2.1, and the DCT. Solving these gives

\[P(S_{\tau}=-a)=b/(a+b)\quad\mbox{and}\quad{\rm E}\tau=ab\quad\mbox{when}\quad p =q=\frac{1}{2}. \tag{7}\]

Justifying the other two equations in (6) (\(Z_{n}^{(2)}\) is analogous to \(Z_{n}^{(1)}\), while \(Z_{n}\) uses condition (13.6.11)),

\[P(S_{\tau}=-a)=\frac{1-(p/q)^{b}}{1-(p/q)^{a+b}}\qquad\mbox{if }p\neq q \tag{8}\]

and, with \(\mu\equiv p-q\),

\[{\rm E}\tau=\frac{b}{\mu}-\frac{b+a}{\mu}\frac{1-(p/q)^{b}}{1-(p/q)^{a+b}} \qquad\mbox{if }p\neq q. \tag{9}\]

Note that if \(\mu\equiv p-q<0\), then \([\max_{0\leq n<\infty}S_{n}]\cong{\rm Geometric}(p/q)\). That is,

\[P(\max_{0\leq n<\infty}S_{n}\geq b)=(p/q)^{b}\quad\mbox{for all integers }b,\quad\mbox{when }p<q. \tag{10}\]

(Just let \(a\!\to\!\infty\) in the formula for \(P(S_{\tau}\!=\!-a)\) to obtain the complementary probability.)

**Example 7.2** (Gambler's ruin for Brownian motion): Suppose that \(\mathbb{S}_{\mu}\) is Brownian motion with drift: \(\mathbb{S}_{\mu}(t)=\mathbb{S}(t)+\mu t\) for \(t\geq 0\). Define the stopping time \(\tau_{ab}\equiv\tau\equiv\inf\{t\geq 0:\mathbb{S}_{\mu}(t)=-a\) or \(b\}\), where \(-a<0<b\). An easy argument will show that \(\mathrm{E}\tau<\infty\). Observe first that

\[\mathbb{S}_{0}(t),\,\mathbb{S}_{0}^{2}(t)-t,\ \mathbb{S}_{\mu}(t)-\mu t\qquad\text{are 0- mean mgs.} \tag{11}\]

Then set \(\theta=-2\mu\), and recall (12.7.8) to conclude that

\[\exp(\theta[\mathbb{S}_{\mu}(t)-\mu t]-\theta^{2}t/2)=\exp(-2\mu[\mathbb{S}(t) +\mu t])\qquad\text{is a mean-1 mg.} \tag{12}\]

Applying the optional sampling theorem to (11) and (12), we obtain

\[P(\mathbb{S}(\tau)=-a)=b/(a+b) \text{if }\mu=0, \tag{14}\] \[\mathrm{E}\tau=ab \text{if }\mu=0,\] (15) \[P(\mathbb{S}_{\mu}(\tau)=-a)=\frac{1-e^{2\mu b}}{1-e^{2\mu(a+b) }} \text{if }\mu\neq 0,\] (16) \[\mathrm{E}\tau=\frac{b}{\mu}-\frac{a+b}{\mu}\frac{1-e^{2\mu b}}{ 1-e^{2\mu(a+b)}} \text{if }\mu\neq 0. \tag{13}\]

Let \(\|\mathbb{S}_{\mu}^{+}\|_{0}^{\infty}\equiv\sup_{0\leq t<\infty}\mathbb{S}_{ \mu}(t)\). Note that if \(\mu<0\), then \(\|\mathbb{S}_{\mu}^{+}\|_{0}^{\infty}\cong\mathrm{Exponential}(2|\mu|)\). The Exponential \((2|\mu|)\) tail probability gives

\[P(\|\mathbb{S}_{\mu}^{+}\|_{0}^{\infty}\geq b)=\exp(-2|\mu|b)=\exp(-\theta b) \qquad\text{for all }b>0. \tag{17}\]

Note the complete analogy with example 7.1. \(\square\)

**Exercise 7.1**: Give all details in justifying the final two equalities in (6).

**Exercise 7.2**: Verify completely the claims of example 7.2. (recall theorem 12.7.1 and theorem 12.7.2.)

**Exercise 7.3**: Derive an analogue of the previous example 7.2 that is based on the Poisson process \(\{\mathbb{N}(t):\ t\geq 0\}\).

## 8 Introduction to Counting Process Martingales\({}^{*}\)

**Heuristic Treatment of Counting Process Martingales**

Suppose now that the process \(\{M(x),{\cal A}_{x}\}_{x\in R}\) is a martingale. Then for every increment \(M(x+h)-M(x)\) we have \({\rm E}\{M(x+h)-M(x)|{\cal A}_{x}\}=0\). Operating _heuristically_, this _suggests_ that

\[{\rm E}\{dM(x)|{\cal A}_{x-}\}=0\qquad{\rm a.s.}\quad\mbox{for any martingale }\{M(x),{\cal A}_{x}\}_{x\in R}, \tag{1}\]

where \({\cal A}_{x-}\) is the \(\sigma\)-field generated by everything up to (but not including) time \(x\). With this background, we now turn to our problem.

Suppose now that

\[N(x)\qquad\mbox{is a counting process;} \tag{2}\]

a _counting process_ is (informally) an \(\nearrow\) process that can increase only by taking jumps of size \(+1\). Incremental change is modeled via

\[{\rm E}\{dN(x)|{\cal A}_{x-}\}=dA(x)\ {\rm a.s.};\qquad\mbox{here }dA(x)\ {\rm is}\ {\cal A}_{x-}\mbox{-measurable.} \tag{3}\]

It then seems that

\[M(x)\equiv N(x)-A(x)\quad\mbox{for }x\in R\qquad\mbox{is a martingale;} \tag{4}\]

we call \(A(.)=\int_{-\infty}^{x}dA(y)\equiv\int_{(-\infty,x]}dA(y)\) the _compensator_ of \(N\). Note that \(A(.)\) is an \(\nearrow\) and \({\cal A}_{x-}\)-measurable process.

We compute the _predictable variation process_\(\langle M\rangle\) of the martingale \(M\) (as suggested by (13.5.3) or (13.5.8), and using integration by parts) via

\[d\langle M\rangle(x)\equiv{\rm E}\{dM^{2}(x)|{\cal A}_{x-}\}={\rm E }\{M_{-}(x)dM(x)+M(x)dM(x)|{\cal A}_{x-}\} \tag{6}\] \[={\rm E}\{2M_{-}(x)dM(x)+[dM(x)]^{2}|{\cal A}_{x-}\}\] \[=2M_{-}(x){\rm E}\{dM(x)|{\cal A}_{x-}\}+{\rm E}\{[dM(x)]^{2}|{ \cal A}_{x-}\}\] \[=2M_{-}(x)\cdot 0+{\rm E}\{[dM(x)]^{2}|{\cal A}_{x-}\}\quad\mbox{by (1)}\] \[={\rm E}\{[dM(x)]^{2}|{\cal A}_{x-}\}\quad\mbox{as is also suggested immediately by (13.5.11)}\] \[\quad\mbox{(so, the heuristics of both (13.5.8) and (13.5.11) give the same thing)}\] \[={\rm E}\{[dN(x)-dA(x)]^{2}|{\cal A}_{x-}\}\] \[={\rm E}\{[dN(x)]^{2}-2[dA(x)][dN(x)]+[dA(x)]^{2}|{\cal A}_{x-}\}\] \[={\rm E}\{[dN(x)]^{2}|{\cal A}_{x-}\}-2dA(x){\rm E}\{dN(x)|{\cal A} _{x-}\}+{\rm E}\{[dA(x)]^{2}|{\cal A}_{x-}\}\] \[={\rm E}\{dN(x)|{\cal A}_{x-}\}-2[dA(x)]^{2}+[dA(x)]^{2}\ \mbox{by (3) and }[dN(x)]^{2}=dN(x)\] (7) \[=dA(x)-[dA(x)]^{2}\quad\mbox{by (3)}\] (8) \[=[1-\Delta A(x)]dA(x)\quad\mbox{where }\Delta A(x)\equiv A(x)-A_{-}(x)\equiv A(x)-A(x-);\]

note that \([dN(x)]^{2}=dN(x)\), since \(dN(x)\) takes on only the values \(0\) and \(1\). When we combine (5) = (8), it suggests that

\[\langle M\rangle(x)=\int_{-\infty}^{x}[1-\Delta A(y)]dA(y). \tag{9}\]

Thus (note (13.5.12) and (5)), the process

\[M^{2}(x)-\langle M\rangle(x)\quad\mbox{has}\quad{\rm E}\{d[M^{2}(x)-\langle M \rangle(x)]|{\cal A}_{x-}\}=0,\]which suggests that, provided that each \({\rm E}M^{2}(x)<\infty\),

\[M^{2}(x)-\langle M\rangle(x)\quad\mbox{for $x\in R$}\quad\mbox{is a $0$-mean mg with respect to the ${\cal A}_{x}$.} \tag{10}\]

**Summary**  Starting with a martingale \(M(.)\) having all \({\rm E}M^{2}(x)<\infty\), it seems that

\[\langle M\rangle(\cdot)\quad\mbox{is the predictable variation of the submartingale }\{M^{2}(x),{\cal A}_{x}\}_{t\in R}. \tag{11}\]

That is, \(\langle M\rangle\) (\(\cdot\)) is the \(\nearrow,\,\geq 0\), and \({\cal A}_{x-}\)-measurable process whose existence is guaranteed by the Doob-Meyer decomposition (see 13.9.2 in Shorack (2000)). [Note that we ("discovered" this without using said Doob-Meyer theorem; that theorem will merely guarantee that our heuristic guess-and-verify approach (assuming that we can make it rigorous) gives us the "right answer" This is typical.] (Note Fleming and Harrington (1991).)

Consider the martingale transform

\[W(x)\equiv\int_{-\infty}^{x}H(y)dM(y),\quad\mbox{where $H(x,\cdot)$is ${\cal A}_{x-}$-measurable for all $x$.} \tag{12}\]

Then \({\rm E}\{dW(x)|{\cal A}_{x}\}={\rm E}\{H(x)dM(x)|{\cal A}_{x-}\}=H(x){\rm E}\{ dM(x)|{\cal A}_{x-}\}=0\) by (1), so

\[\{W(x),{\cal A}_{x}\}_{x\in R}\quad\mbox{is a martingale, provided that each ${\rm E}|W(x)|<\infty$.} \tag{13}\]

Moreover, its predictable variation is given by

\[d\langle W\rangle(x)={\rm E}\{[dW(x)]^{2}|{\cal A}_{x-}\}\quad \mbox{appealing directly to (6) this time}\] \[={\rm E}\{[H(x)dM(x)]^{2}|{\cal A}_{x-}\}\] \[=H^{2}(x){\rm E}\{[dM(x)]^{2}|{\cal A}_{x-}\}\qquad\mbox{since $H(x)$ is ${\cal A}_{x-}$-measurable}\] \[=H^{2}(x)d\langle M\rangle(x)\qquad\mbox{by (6),}\]

suggesting that

\[\langle W\rangle(x)=\int_{-\infty}^{x}H^{2}d\langle M\rangle=\int_{-\infty}^{x }H^{2}(y)[1-\Delta A(y)]dA(y). \tag{14}\]

This also suggests, provided that each \({\rm E}W^{2}(x)<\infty\), that

\[\{L(x)\equiv W^{2}(x)-\langle W\rangle(x),{\cal A}_{x}\}_{x\in R},\qquad\mbox{is a $0$-mean mg.} \tag{15}\]

Processes \(H(.)\) that are \({\cal A}_{x-}\)-measurable satisfy \(H(x)={\rm E}\{H(x)|{\cal A}_{x-}\}\), and so \(H(x)\) can be determined by averaging \(H(.)\) over the past; such an \(H\) is thus called _predictable_. The martingale transform statement (12) can be summarized as

\[\int_{-\infty}^{x}\,\,[\mbox{predictable}]\,\,d\,\,[\mbox{martingale}]=[\mbox{ martingale}], \tag{16}\]

provided that expectations exist.

Suppose now that we have a sequence of martingales \(M_{n}\) whose increments satisfy a type of Lindeberg condition; this suggests that any limiting process \(M\) ought to be a normal process. From the martingale condition we hope that

\[\mbox{Cov}[M(y)-M(x),\ M(x)]=\lim_{n}{\rm E}\{[M_{n}(y)-M_{n}(x)]M_{n}(x)\}\] \[\quad=\lim_{n}{\rm E}\{M_{n}(x){\rm E}\{M_{n}(x,y|{\cal A}_{x}\} \})=\lim_{n}{\rm E}\{M_{n}(x)\cdot 0\}=0;\]

and for a normal process \(M(.)\) uncorrelated increments also mean independent increments. The variance process of \(M(.)\) should be \({\rm E}M^{2}(x)=\lim_{n}{\rm E}M_{n}^{2}(x)=\lim_{n}{\rm E}\langle M_{n}\rangle(x)\) by (6). So it seems reasonable to hope that [recall (12.1.15)]

\[M_{n}\to_{d}M\cong{\mathbb{S}}(V)\quad\mbox{ on $(D_{R},{\cal D}_{R},\rho_{ \infty})$}\quad\mbox{as}\quad n\to\infty \tag{17}\]for a Brownian motion \(\mathbb{S}\), provided that

(18) the increments of

\[M_{n}\]

 satisfy a type of Lindeberg condition,

and provided that (note (9))

\[\langle M_{n}\rangle(x)\to_{p}[\mbox{some }V(x)]\qquad\mbox{as }n\to\infty,\mbox{ for each }x\in R, \tag{19}\]

where

\[V \tag{20}\]

 is

\[\nearrow\]

 and right continuous with

\[V(-\infty)=0.\]

As noted above,

(21) it often holds that

\[V(x)=\lim_{n}\mbox{E}\langle M_{n}\rangle(x)=\lim_{n}\mbox{E}M_{n}^{2}(x)= \mbox{E}M^{2}(x).\]

Of course, the original martingales \(M_{n}\) need to be square integrable. This "quasi theorem" is roughly Rebolledo's CLT.

One other bit of heuristics seems in order. Suppose now that we have several counting processes \(N_{i}(x)\) and that we perform the above calculations and determine martingales \(M_{i}(x)=N_{i}(x)-A_{i}(x)\) with \(\langle M_{i}\rangle\)\((x)=\int_{-\infty}^{x}[1-\Delta A_{i}]\,dA_{i}\). Now, for \({\cal A}_{x-}\)-measurable functions \(c_{i}(.)\)

\[\mathbb{M}_{n}(x)\equiv\sum_{i=1}^{n}c_{i}(x)M_{i}(x)\qquad\mbox{is also a martingale.} \tag{22}\]

We note from (6) that

\[d\langle\mathbb{M}_{n}\rangle(x)=\mbox{E}\{[d\mathbb{M}_{n}(x)] ^{2}|{\cal A}_{x-}\}\] \[\qquad=\sum_{i=1}^{n}c_{i}^{2}(x)\mbox{E}\{[dM_{i}(x)]^{2}|{\cal A }_{x-}\}\] \[\qquad+\sum\sum_{i\neq j}c_{i}(x)c_{j}(x)\mbox{E}\{[dM_{i}(x)][ dM_{j}(x)]|{\cal A}_{x-}\} \tag{23}\] \[\qquad=\sum_{i=1}^{n}c_{i}^{2}(x)\langle M_{i}\rangle(x),\]

provided that the

\[M_{i}(x,y)\quad\mbox{and}\quad M_{j}(x,y]\mbox{ are uncorrelated, given }{\cal A}_{x-}. \tag{24}\]

In fact, conditions under which all of the previous heuristics are actually true are given below. Even without these, we can use these heuristics as the first step in a guess-and-verify approach.

**The Guess-and-Verify Approach in a Single Sample IID as \(F\)**

**Example 8.1** (Single-sample martingale): Let \(\tau_{o}\equiv F^{-1}(1)\). Suppose that

\[N_{i}(x)\equiv 1_{[X_{i}\leq x]}\qquad\qquad\qquad\qquad\qquad\qquad\qquad \mbox{ for all real }x, \tag{25}\]

for \(X_{1},\ldots,X_{n}\) iid \(F\) on \(R\). Let

\[{\cal A}_{x}\equiv\sigma[1_{[X_{i}\leq y]}\ :\ y\leq x,1\leq i\leq n]=\sigma[1_{[X_{i}>y]}\ :\ -\infty\leq y\leq x,1\leq i\leq n].\]

Then \(N_{i}\) is a counting process with

\[\mbox{E}\{ dN_{i}(x)|{\cal A}_{x-}\}=P(dN_{i}(x)=1|N_{i}(x-)=0)\] \[\qquad=dA_{i}(x)\equiv 1_{[X_{i}\geq x]}\,d\Lambda(x)\,\]

[MISSING_PAGE_FAIL:392]

[MISSING_PAGE_FAIL:393]

where we have defined

\[\mathbb{E}_{n}^{uc}\equiv\sqrt{n}[\mathbb{H}_{n}^{uc}-H^{uc}]\qquad\text{and}\quad \mathbb{E}_{n}\equiv\sqrt{n}[\mathbb{H}_{n}-H]\qquad\qquad\qquad\qquad\qquad \text{on}\;[0,\infty). \tag{48}\]

We would expect that a Skorokhod version \(\tilde{\mathbb{M}}_{n}\) of \(\mathbb{M}_{n}\) would satisfy

\[\|\tilde{\mathbb{M}}_{n}-\mathbb{M}[\|\to_{p}0\quad\text{as}\quad n\to\infty, \tag{49}\]

where for appropriately defined \(\mathbb{E}^{uc}\) and \(\mathbb{E}\),

\[\mathbb{M}[\equiv\mathbb{E}^{uc}+\int_{0}\tfrac{\mathbb{E}}{1-H_{-}}\,dH^{uc} \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{on}\quad[0,\infty). \tag{50}\]

Also,

\[\langle\mathbb{M}_{n}\rangle(t)=\int_{0}^{t}[1-\mathbb{H}_{n-}(s)]\times[1- \Delta\Lambda(s)]\,d\Lambda(s)\qquad\qquad\qquad\qquad\qquad\qquad\qquad \text{on}\quad[0,\infty), \tag{51}\]

and

\[V(t)\equiv\mathrm{E}\langle\mathbb{M}_{n}\rangle(t)=\int_{0}^{t}(1-G_{-})(1- \Delta\Lambda)\,dF=\int_{0}^{t}(1-H_{-})\times[1-\Delta\Lambda]\,d\Lambda. \tag{52}\]

As would now be expected, it can be shown (see (12.1.15) for \(\rho_{\infty}\)) that

\[\mathbb{M}\;\cong\;\mathbb{S}(V)\qquad\text{on}\quad(D_{[0,\infty)},\mathcal{D} _{[0,\infty)},\rho_{\infty}). \tag{53}\]

The _cumulative hazard function_\(\Lambda(.)\) and the Aalen-Nelson _cumulative hazard function estimator_\(\hat{\Lambda}_{n}(.)\) are defined by

\[\Lambda(t)\equiv\int_{0}^{\infty}\tfrac{1}{1-F_{-}}\,dF=\int_{0}^{t}\tfrac{1-G _{-}}{1-H_{-}}\,dF\quad\text{and}\quad\hat{\Lambda}_{n}(t)\equiv\int_{0}^{t} \tfrac{1}{1-\mathbb{H}_{n-}}d\mathbb{H}_{n}^{uc} \tag{54}\]

for all \(t\in[0,\infty)\). This is motivated by the deterministic halves of (39) and (40). We use this to form an estimator of the df \(F\), that is, a type of instantaneous life table. The Kaplan-Meier _product-limit estimator_ of the survival function \(1-F\) is defined by

\[1-\hat{\mathbb{F}}_{n}(t)\equiv\prod_{s\leq t}[1-\Delta\hat{\Lambda}_{n}(s)]= \prod_{Z_{n:i}\leq t}[1-1/(n-i+1)]^{\delta_{n:i}}\qquad\qquad\qquad\text{on} \quad[0,\infty). \tag{55}\]

Fundamental Processes We define

\[\mathbb{X}_{n}\equiv\sqrt{n}[\hat{\mathbb{F}}_{n}-F],\qquad\mathbb{B}_{n} \equiv\sqrt{n}[\hat{\Lambda}_{n}-\Lambda],\qquad\mathbb{Z}_{n}\equiv\sqrt{n} \tfrac{[\hat{\mathbb{F}}_{n}-F]}{1-F} \tag{56}\]

on \([0,\infty)\) ; and, with

\[T\equiv Z_{n:n}\quad\text{and}\quad J_{n}(.)\equiv 1_{[0,T]}=1_{[0,Z_{n:n}]}(.)=1_ {[\mathbb{H}_{n-(.)}(.)<1]} \tag{57}\]

(where \(J_{n}(.)\) is predictable), we further define \(\tau_{o}\equiv H^{-1}(1)\) and

\[\mathbb{X}_{n}^{T}\equiv\mathbb{X}_{n}(T\wedge\cdot),\qquad\mathbb{B}_{n}^{T} \equiv\mathbb{B}_{n}(T\wedge\cdot),\qquad\mathbb{Z}_{n}^{T}\equiv\mathbb{Z}_{n }(T\wedge\cdot)\qquad\qquad\qquad\text{on}\;[0,\infty). \tag{58}\]

Note that for \(t\in[0,\infty)\), (54) and (46) give

\[\mathbb{B}_{n}^{T}(t)=\sqrt{n}[\int_{0}^{T\wedge t}\frac{1}{1- \mathbb{H}_{n-}}d\mathbb{H}_{n}^{uc}-\int_{0}^{t}\,d\Lambda]\] \[\quad=\sqrt{n}\int_{0}^{T\wedge t}\tfrac{1}{1-\mathbb{H}_{n-}}d[ \mathbb{H}_{n}^{uc}-\int_{0}^{t}(1-\mathbb{H}_{n-})\,d\Lambda] \tag{59}\] \[\quad=\int_{0}^{t}\tfrac{J_{n}}{1-\mathbb{H}_{n-}}d\mathbb{M}_{n}= \int_{0}^{t}\quad[\text{predictable}]\;d\;[\text{martingale}]=[\text{ martingale}](t).\]

**Exercise 8.2**  Suggest that on \([0,\infty)\) we have

\[\langle B_{n}^{T}\rangle=\int_{0}^{t}\frac{J_{n}}{[1-\mathbb{H}_{N-}]}[1-\Delta \Lambda]\,d\Lambda\to C(t)\equiv\int_{0}^{t}\frac{1}{1-H_{-}}[1-\Delta\Lambda] \,d\Lambda. \tag{60}\]

**Exercise 8.3** (a) Verify (43).

(b) For \(0\leq s,t<\infty\), evaluate Cov \([\mathbb{E}_{n}^{uc}(s),\mathbb{E}_{n}(t)]\).

**Exercise 8.4**  Use integration by parts to show that

\[\mathbb{Z}_{n}^{T}(t)=\tfrac{\mathbb{X}_{n}(T\Lambda t)}{1-F(T\Lambda t)}=\int _{0}^{t}1_{[0,Z_{nn}]}\tfrac{1-\hat{\mathbb{F}}_{n-}}{1-F}\tfrac{1}{1-\mathbb{H }_{n-}}d\mathbb{M}_{n}\] on \[[0,\infty) \tag{62}\] \[=\int_{0}^{t}[\text{predictable}]\;d\;[\text{martingale}]=[\text{ martingale}](t). \tag{61}\]

Use the above heuristics to suggest (it can be proved by either mg or empirical process methods) that

\[\langle\mathbb{Z}_{n}\rangle^{T}(t)=\int_{0}^{t}1_{[0,Z_{n:n}]}[ \frac{1-\hat{\mathbb{F}}_{n-}}{1-F}]^{2}\frac{1}{(1-\mathbb{H}_{n-})^{2}}d \langle\mathbb{M}\rangle_{n}\] on \[[0,\infty)\] \[=\int_{0}^{t}1_{[0,Z_{n:n}]}[\tfrac{1-\hat{\mathbb{F}}_{n-}}{1-F}]^{2} \tfrac{1}{1-\mathbb{H}_{n-}}\times[1-\Delta\Lambda]\,d\Lambda \tag{63}\]

\[\to D(t)\equiv\int_{0}^{t}\tfrac{1}{1-H_{-}}\tfrac{1}{1-\Delta\Lambda}\;d\Lambda \text{ on }[0,\infty) \tag{64}\]

(which is a \(\geq 0\), right continuous, and \(\nearrow\) function), since

\[[(1-F_{-})/(1-F)]^{2}=[1/(1-\Delta\Lambda)]^{2}. \tag{65}\]

These facts suggest that (for the \(D(.)\) of (64))

\[\mathbb{Z}_{n}^{T}\to_{d}\mathbb{S}(D)\quad\text{and}\quad\mathbb{X}_{n}^{T} \to d(1-F)\mathbb{S}(D)\quad\text{on}\quad(D_{[0,\infty)},\mathcal{D}_{[0, \infty)},\rho_{[\infty)}). \tag{66}\]

We now prove the corresponding result for \(\mathbb{B}_{n}^{T}\).

**Theorem 8.1**  Let \(F\) denote an arbitrary df \(F\) on \([0,\infty)\). Then for \(C\) as in (60),

\[\mathbb{B}_{n}^{T}\to_{d}\mathbb{S}(C)\qquad\text{on}\,(D_{[0,\infty)}, \mathcal{D}_{[0,\infty)},\rho_{[\infty)}). \tag{67}\]

**Proof.**  We will show that for all \(0\leq r\leq s\leq t<\tau_{o}\), we have

\[\text{E}\{\mathbb{B}_{n}^{T}(r,s]^{3/2}\mathbb{B}_{n}^{T}(s,t]^{3/2}\}\leq 3 5C(r,s]^{3/4}C(s,t]^{3/4}\qquad\text{for $C$ as in (\ref{eq:C1})}, \tag{68}\]

so Chentsov's theorem gives relative compactness. We leave convergence of the finite-dimensional distributions to an exercise below. Define \(Y_{t}\equiv\mathbb{B}_{n}^{T}\), and note that \(Y_{t}\) is of the form

(a) \[Y_{t}=\int_{[0,t]}HdM\qquad\text{ for $H$ predictable and $M$ a mg};\]

in fact,

(b) \[H\equiv J_{n}/(1-\mathbb{H}_{n-})\quad\text{and}\quad M\equiv\mathbb{M}_{n}.\]

We will require two basic results. It holds that

(c) \[\text{E}\{Y(s,t]^{2}|\mathcal{A}_{s}\}\leq 2\beta_{n}C(s,t]\qquad\qquad\qquad \qquad\qquad\text{ for all $0\leq s\leq t<\infty$},\]where \(C(.)\) as defined in (60) is \(\geq 0\), right continuous, and \(\nearrow\), and where

(d) \[\beta_{n}\equiv\|(1-H_{-})/(1-\mathbb{H}_{n-})\|_{0}^{Z_{n:n}}\qquad\text{has} \qquad\text{E}\beta_{n}^{k}\leq 1+2k\Gamma(k+2).\]

These will be established below. However, we first use them to give a proof of (67).

Now, for all \(0\leq r\leq s\leq t<\tau_{o}\) we have

\[\text{E}\{ |Y(r,s]|^{3/2}|Y(s,t]|^{3/2}\}=\text{E}\{|Y(r,s]|^{3/2}\text{E}\{|Y(s,t] |^{3/2}|\mathcal{A}_{s}\}\}\] (e) \[\leq\text{E}\{|Y(r,s]|^{3/2}\text{E}\{Y(s,t]^{2}|\mathcal{A}_{s}\} ^{3/4}\}\qquad\text{ by conditional Liapunov}\] (f) \[\leq\text{E}\{|Y(r,s]|^{3/2}(2\beta_{n}C(s,t])^{3/4}\}\qquad\text{ by (c)}\] (g) \[\leq 2^{3/4}C(s,t]^{3/4}\text{E}\{(Y(r,s]^{2})^{3/4}\beta_{n}^{3/4}\}\] \[\leq 2^{3/4}(\text{E}\beta_{n}^{3})^{1/4}C(s,t]^{3/4}(\text{E}\{Y(r, s]^{2}\})^{3/4}\qquad\text{ by Holder's inequality}\] \[\leq 2^{3/4}(\text{E}\beta_{n}^{3})^{1/4}C(s,t]^{3/4}(\text{E}\{ \text{E}\{Y(r,s]^{2}|\mathcal{A}_{r}\}\})^{3/4}\] (h) \[\leq 2^{3/4}(\text{E}\beta_{n}^{3})^{1/4}C(s,t]^{3/4}(\text{E}\{ 2\beta_{n}C(r,s]\})^{3/4}\qquad\text{ by (c)}\] \[=2^{3/2}(\text{E}\beta_{n}^{3})^{1/4}(\text{E}\beta_{n})^{3/4}C(r, s]^{3/4}C(s,t]^{3/4}\] (i) \[\leq 2^{3/2}(\text{E}\beta_{n}^{3})^{1/2}C(r,s]^{3/4}C(s,t]^{3/4}\qquad\text{ by Liapunov's inequality}\] (j) \[\leq 35C(r,s]^{3/4}C(s,t]^{3/4}\qquad\text{ by (d)},\]

as claimed in (66).

We now establish (c). Now,

\[\text{E}\{ Y(s,t]^{2}|\mathcal{A}_{s}\}=\text{E}\{Y_{t}^{2}-2Y_{s}Y_{t}+Y_{s}^{2}|\mathcal{A}_{s}\}\] (k) \[=\text{E}\{Y_{t}^{2}-Y_{s}^{2}|\mathcal{A}_{s}\}\qquad\text{since }\{Y_{t},\mathcal{A}_{t}\}_{\{t\geq 0\}}\text{ is a mg}\] \[=\text{E}\{Y_{t}^{2}-\langle Y\rangle_{t}|\mathcal{A}_{s}\}-(Y_{s}^{2}- \langle Y\rangle_{s})+\text{E}\{Y(Y)_{t}-\langle Y\rangle_{s}|\mathcal{A}_{s}\}\] \[=\text{E}\{\langle Y\rangle_{t}-\langle Y\rangle_{s}|\mathcal{A}_{s}\}\qquad\text{since }\{Y_{t}^{2}-\langle Y\rangle_{t},\mathcal{A}_{t}\}_{\{t\geq 0\}}\text{ is a mg}\] (l) \[=\text{E}\{\int_{(s,t]}H^{2}d\langle\mathbb{M}_{n}\rangle|\mathcal{A}_{s}\}\qquad\text{ so far, holding very generally}\] (m) \[=\text{E}\{\int_{(st]}\frac{J_{n}}{(1-\mathbb{H}_{n-})^{2}}(1- \mathbb{H}_{n-})\times[1-\Delta\Lambda]\,d\Lambda|\mathcal{A}_{s}\}\qquad\text{ using (\ref{(r) \(\leq 1/((m+1)p)\leq 1/(mp)\).

We now turn to (d). Now, by (6.4.13) and (12.11.14) we have

\[\begin{array}{l}\mbox{E}\beta_{n}^{k}=\int_{0}^{\infty}P(\beta_{n}\geq x)kx^{k -1}dx\leq 1+\int_{1}^{\infty}2x^{2}e^{-x}kx^{k-1}dx\\ \qquad\leq 1+2k\Gamma(k+2)\qquad\mbox{ for general }k\\ \mbox{(t)}\qquad\leq 145\qquad\mbox{ for }k=3,\end{array}\]

as claimed in (d). 

**Exercise 8.5**: Complete the proof of theorem 8.1, by showing convergence of the finite-dimensional distributions.

**Exercise 8.6**: Show that (69) may be extended to give

\[\mbox{E}\{\frac{1}{Z^{k}}1_{[Z>0]}\}\leq k(k+1)/(mp)^{k}\quad\mbox{when} \quad Z\cong\mbox{Binomial}(m,p),\quad\mbox{and }k\geq 1. \tag{70}\]

**Remark 8.1**: At this point, three sections of the \(1st\) Edition have been entirely omitted. The first two contained a general form of the Doob-Meyer decomposition for continuous parameter martingales followed by a treatment of martingales of the form \(\int Hd\mathbb{M}=\int[\mbox{predictable}]\mbox{d}[\mbox{martingale}]\). The final omitted section treated the basic censored data martingale. Together, these three sections, form the continuous analog to the current section 13.5 and section 13.8.

## 9 CLTs for Dependent RVs \({}^{o}\)

Let \(\{X_{nk}:\,k=1,2,\ldots\) and \(n=1,2,\ldots\}\) be an array of rvs on a basic probability space \((\Omega,{\cal A},P)\). For each \(n\) we suppose that \(X_{n1},X_{n2},\ldots\) are adapted to an \(\nearrow\) sequence of \(\sigma\)-fields \({\cal A}_{n0}\subset{\cal A}_{n1}\subset{\cal A}_{n2}\subset\cdots\subset{\cal A}\). For each \(n\) we suppose that \(\kappa_{n}\) in an integer-valued stopping time with respect to these \(({\cal A}_{nk})_{k=0}^{\infty}\). We now introduce

\[\begin{array}{l}P_{k-1}(\cdot)\equiv P(\cdot|{\cal A}_{n,k-1}),\\ {\rm E}_{k-1}(.)\equiv{\rm E}(\cdot|{\cal A}_{n,k-1}),\qquad\mbox{and}\qquad \mbox{Var}_{k-1}[\cdot]\equiv[\cdot|{\cal A}_{n,k-1}].\end{array} \tag{1}\]

Our interest is in the sum

\[S_{n}\equiv\sum_{k=1}^{\kappa_{n}}X_{nk}. \tag{2}\]

We will have reason to consider

\[X^{\prime}_{nk}\equiv X_{nk}1_{[|X_{nk}|\leq 1]}\quad\mbox{and}\quad X^{\prime \prime}_{nk}\equiv X_{nk}1_{[|X_{nk}|>1]}. \tag{3}\]

What follows is the most basic CLT in this monograph. For row-independent rvs and \(\kappa_{n}\equiv n\) it reduces to the asymptotic normality conclusion in (10.5.30), with \(c=1\). The second theorem implies the first, and is very much in the spirit of the Lindeberg theorem.

**Theorem 9.1** (Basic dependent CLT)  Conclude that \(S_{n}\to_{d}N(0,1)\) if

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}P_{k-1}(|X_{nk}|\geq\epsilon)\to_{p}0 \quad\mbox{for all}\ \ \epsilon>0\\ \mbox{(equivalently,}\ \max_{1\leq k\leq\kappa_{n}}|X_{nk}|\to_{p}0),\end{array} \tag{4}\]

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}{\rm E}_{k-1}(X^{\prime}_{nk})\to_{p}0 \qquad\mbox{(i.e.,}\quad\mbox{partial sums of the $X_{nk}$ are nearly a mg)},\\ \sum_{k=1}^{\kappa_{n}}\mbox{Var}_{k-1}[X^{\prime}_{nk}]\to_{p}1.\end{array} \tag{5}\]

**Theorem 9.2**  Conclude that \(S_{n}\to_{d}N(0,1)\) if

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}{\rm E}_{k-1}(X^{2}_{nk}1_{[|X_{nk}| \geq\epsilon]})\to_{p}0\quad\mbox{for all}\ \ \epsilon>0,\\ \sum_{k=1}^{\kappa_{n}}{\rm E}_{k-1}(X_{nk})\to_{p}0,\end{array} \tag{7}\]

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}\mbox{Var}_{k-1}[X_{nk}]\to_{p}1.\end{array} \tag{9}\]

**Comments**  If one replaces (5) by

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}|{\rm E}_{k-1}(X^{\prime}_{nk})|^{2} \to_{p}1,\end{array} \tag{10}\]

then (6) may be replaced by

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}{\rm E}_{k-1}({X^{\prime}_{nk}}^{2}) \to_{p}1.\end{array} \tag{11}\]

If (4) holds, then (11) is equivalent to

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}X^{2}_{nk}\to_{p}1.\end{array} \tag{12}\]

If \(X_{n1},X_{n2},\ldots\) are mg differences, then (10) and (5) are implied by

\[\begin{array}{l}\sum_{k=1}^{\kappa_{n}}|{\rm E}_{k-1}(X_{nk}1_{[|X_{nk}|>1] })|\to_{p}0.\end{array} \tag{13}\]

Moreover, (13) is implied by either (7) or

\[{\rm E}(\max_{1\leq k\leq\kappa_{n}}|X_{nk}|)\to_{p}0. \tag{14}\]

We now summarize these last claims.

**Theorem 9.3** (MG CLT): _Suppose the \(X_{n1},X_{n2},\ldots\) are mg differences, for each \(n\). Then \(S_{n}\to_{d}N(0,1)\), provided that any of the following occurs:_

(15) _Conditions (7) and (9) hold,_

(16) _Conditions (12) and (14) hold,_

(17) _Conditions (4), (12), and (13) hold._

Note Hall and Heyde (1980) for such results.

## Chapter 14 Convergence in Law on Metric Spaces \({}^{o}\)

### 1 Convergence in Distribution on Metric Spaces \({}^{o}\)

Many results for convergence in distribution generalize to probability measures on a general metric space \((M,d)\) equipped with its Borel \(\sigma\)-field \({\cal M}_{d}\). We call such measures _Borel measures_. Instead of using convergence of dfs to define convergence in distribution (or law), we use directly the Helly-Bray idea embodied in the \(\to_{d}\) equivalences of theorem 9.1.4, and that theorem is here extended to general metric spaces. Skorokhod's construction is also generalized to complete and separable metric spaces. Section 12.1 gave specific information on the two very important metric spaces that gave rise to \((C,{\cal C})\) and \((D,{\cal D})\). In section 2 the dual bounded Lipschitz metric will be introduced, along with Hellinger, Prohorov, and total variation metrics. These are useful on function spaces.

**Definition 1.1** (Convergence in distribution): If \(\{P_{n}:n\geq 1\}\) and \(P\) are probability measures on \((M,d,{\cal M}_{d})\) satisfying

\[\int g\,dP_{n}\to\int g\,dP\qquad\mbox{for all $g\in C_{b}(M)$} \tag{1}\]

[where \(C_{b}(M)\equiv\{\mbox{all bounded and $d$-continuous functions $g$ from $M$ to $R$}\}\), and \(C_{bu}(M)\) denotes those functions that are additionally \(d\)-uniformly continuous], then we say that \(P_{n}\)_converges in distribution_ (or _law_) to \(P\), or that \(P_{n}\)_converges weakly_ to \(P\); and we write \(P_{n}\to_{d}P\) or \(P_{n}\to_{L}P\). Similarly, if \(X_{n}\), \(X\) are random elements in \(M\) for which

\[\mbox{E}\,g(X_{n})\to\mbox{E}\,g(X)\qquad\mbox{for all $g\in C_{b}(M)$}, \tag{2}\]

then we write \(X_{n}\to_{d}X\), \(X_{n}\to_{\cal L}X\) or \({\cal L}(X_{n})\to{\cal L}(X)\).

**Theorem 1.1** (Portmanteau theorem; Billingsley): For probability measures \(\{P_{n}:n\geq 1\}\) and \(P\) on any metric space \((M,d,{\cal M}_{d})\) the following are equivalent:

\[P_{n}\to_{d}P\qquad\qquad\qquad\mbox{[i.e.,}\quad\int g\,dP_{n}\to\int g\,dP \quad\mbox{for all $g\in C_{b}(M)$}]. \tag{3}\]

\[\int g\,dP_{n}\to\int g\,dP\qquad\mbox{for all $g\in C_{bu}(M)$}. \tag{4}\]

\[\overline{\lim}\,P_{n}(B)\leq P(B)\qquad\mbox{for all closed sets $B\in{\cal M}_{d}$}. \tag{5}\]

\[\underline{\lim}\,P_{n}(B)\geq P(B)\qquad\mbox{for all open sets $B\in{\cal M}_{d}$}. \tag{6}\]

\[\lim P_{n}(B)=P(B)\qquad\mbox{for all $P$-continuity sets $B\in{\cal M}_{d}$}. \tag{7}\]

**Proof.** Clearly, (3) implies (4).

Consider (4) implies (5): Suppose that (4) holds and that \(B\) is closed. Let \(\epsilon>0\). Then for integer \(m\) large enough, the set \(B_{m}\equiv\{x\,:\,d(x,B)<1/m\}\) satisfies

(a) \[P(B_{m})\leq P(B)+\epsilon,\]

since \(B_{m}\searrow B\) as \(m\to\infty\). Let \(g_{m}(x)\equiv\psi(md(x,B))=\max\{0,(1-md(x,B))\}\), where \(\psi(t)\) is equal to \(1\), \(1-t,0\) according as \(t\) has \(t\leq 0\), \(0\leq t\leq 1\), \(1\leq t\). Then

(b) \[1_{B}\leq g_{m}\leq 1_{B_{m}},\]

and for each \(m\geq 1\), \(g_{m}\) is Lipschitz and uniformly continuous. Hence, by (4) and also (a) and (b),

(c) \[\overline{\lim}_{n}\,P_{n}(B)\leq\lim_{n}\int g_{m}\,dP_{n}=\int g_{m}\,dP \leq P(B_{m})\leq P(B)+\epsilon.\]

Since \(\epsilon>0\) was arbitrary, (5) follows.

Equivalence of (5) and (6) follows easily by taking complements.

Consider (5) implies (3): Suppose that \(g\in C_{b}(M)\) and that (5) holds. Now, transform \(g\) linearly so that \(0\leq g(x)\leq 1\). Fix \(k\geq 1\), and define the closed set

(d) \[B_{j}\equiv\{x\in M:j/k\leq g(x)\}\qquad\mbox{for}\quad j=0,\ldots,k+1.\]

Then it follows that

(e) \[\begin{array}{l}\sum_{j=1}^{k+1}\frac{j-1}{k}P(x:\frac{j-1}{k}\leq g(x)< \frac{j}{k})\\ \qquad\leq\int g\,dP<\sum_{j=1}^{k+1}\frac{j}{k}P(x:\frac{j-1}{k}\leq g(x)< \frac{j}{k}).\end{array}\]

Rewriting the sum on the right side and summing by parts gives

(f) \[\sum_{j=1}^{k}(j/k)[P(B_{j-1})-P(B_{j})]=(1/k)+(1/k)\sum_{j=1}^{k}P(B_{j}),\]

which together with a similar summation by parts on the left side yields

(g) \[(1/k)\sum_{j=1}^{k}P(B_{j})\leq\int g\,dP\leq(1/k)+(1/k)\sum_{j=1}^{k}P(B_{j}).\]

Applying the right side of (g) to \(P_{n}\), and then using (5) for the closed sets \(B_{j}\), and then applying the left side of (g) to \(P\) gives

(h) \[\begin{array}{l}\overline{\lim}_{n}\int g\,dP_{n}\leq\overline{\lim}_{n} \left[\frac{1}{k}+\frac{1}{k}\sum_{j=1}^{k}P_{n}(B_{j})\right]\\ \qquad\leq\left[\frac{1}{k}+\frac{1}{k}\sum_{1}^{k}P(B_{j})\right]\leq\frac{1} {k}+\int g\,dP.\end{array}\]

Letting \(k\to\infty\) in (h) yields

(i) \[\overline{\lim}_{n}\int g\,dP_{n}\leq\int g\,dP.\]

Applying (i) to (the nontransformed) \(-g\) yields

(j) \[\underline{\lim}_{n}\int g\,dP_{n}\geq\int g\,dP.\]

Combining (i) and (j) gives (3).

Consider (5) implies (7): With \(B^{0}\) the interior of any set \(B\in{\cal M}\) and \(\bar{B}\) its closure, (5) and (6) give

\[P(B^{0})\leq\varliminf P_{n}(B^{0})\leq\varliminf P_{n}(B)\leq\varlimsup P_{n}(B )\leq\varlimsup P_{n}(\bar{B})\leq P(\bar{B}).\]

If \(B\) is a \(P\)-continuity set, then \(P(\partial B)=0\) and \(P(\bar{B})=P(B^{0})\), so the extreme terms in (k) are equal; thus \(\lim P_{n}(B)=P(B)\), as required by (7).

Consider (7) implies (5): Since \(\partial\{x:d(x,B)\leq\delta\}\subset\{x:d(x,B)=\delta\}\), the boundaries are disjoint for different \(\delta>0\), and hence at most countably many of them can have positive \(P\)-measure. Therefore, for some sequence \(\delta_{k}\to 0\), the sets \(B_{k}\equiv\{x:d(x,B)<\delta_{k}\}\) are \(P\)-continuity sets and \(B_{k}\searrow B\) if \(B\) is closed. It follows from \(B\subset B_{k}\) and then (7)

\[\varlimsup P_{n}(B)\leq\varlimsup P_{n}(B_{k})=P(B_{k}).\]

Then (5) follows from the monotone property of \(P\), since \(B_{k}\searrow B\) as \(k\to\infty\). \(\Box\)

**Proposition 1.1**: \(P_{n}\to_{d}P\) if and only if each subsequence \(\{P_{n^{\prime}}\}\) contains a further subsequence \(\{P_{n^{\prime\prime}}\}\) such that \(P_{n^{\prime\prime}}\to_{d}P\).

**Proof.**  This is easy from definition 1.1 (and the fact that a sequence of real numbers has \(x_{n}\to x\) if and only if each \(\{x_{n^{\prime}}\}\) contains a further subsequence \(\{x_{n^{\prime\prime}}\}\) such that \(x_{n^{\prime\prime}}\to x\)), as in the corollary to Helly's selection theorem 9.1.3. \(\Box\)

**Theorem 1.2** (Slutsky's theorem): Suppose that \(X_{n},Y_{n}\) are random elements taking values in a separable metric space \((M,d,{\cal M}_{d})\), both defined on some \(\Omega_{n}\).

(a) Show that \(d(X_{n},Y_{n})\) is a rv whenever \((M,d)\) is separable.

(b) If \(X_{n}\to_{d}X\) and \(d(X_{n},Y_{n})\to_{p}0\), then \(Y_{n}\to_{d}X\).

**Proof.**  For a closed set \(B\) and \(\delta>0\) let \(B_{\delta}\equiv\{x:d(x,B)<\delta\}\). Then

\[\begin{array}{ll}\mbox{(a)}&P(Y_{n}\in B)=P(Y_{n}\in B,d(X_{n},Y_{n})<\delta )+P(Y_{n}\in B,d(X_{n},Y_{n})\geq\delta)\\ \mbox{(b)}&\leq P(X_{n}\in\bar{B}_{\delta})+P(d(X_{n},Y_{n})\geq\delta).\end{array}\]

The second term on the right side of (a) goes to zero, since \(d(X_{n},Y_{n})\to_{p}0\) (note the following exercise). Then \(X_{n}\to_{d}X\) gives

\[\varlimsup P(Y_{n}\in B)\leq\varlimsup P(X_{n}\in\bar{B}_{\delta})\leq P(X\in \bar{B}_{\delta}),\qquad\mbox{for every $\delta>0$},\]

via the portmanteau theorem (5) applied to the \(X_{n}\)'s. Then \(\bar{B}_{\delta}\searrow B\) as \(\delta\searrow 0\), since \(B\) is closed, so \(P(X\in\bar{B}_{\delta})\searrow P(X\in B)\). Thus \(\varlimsup P(Y_{n}\in B)\leq P(X\in B)\) ; thus \(Y_{n}\to_{d}X\) follows from applying the portmanteau theorem to the \(Y_{n}\)'s. \(\Box\)

**Exercise 1.1**: Prove theorem 1.2(a). (Recall proposition 2.2.4.)

**Theorem 1.3** (Continuous mapping theorem): Let \(X_{n}\to_{d}X\) on \((M,d,{\cal M}_{d})\), and suppose \(g:\,M\to\bar{M}\) (where \((\bar{M},\bar{d})\) is another metric space) is continuous a.s. with respect to \(P\equiv P_{X}\) (that is, \(P(X\in C_{g})=1\) for the continuity set \(C_{g}\) of \(g\)). Then, necessarily, \(g(X_{n})\to_{d}g(X)\).

**Proof.** We simply note that this is essentially no different from the proof of the Mann-Wald theorem. (Only now we apply the general Skorokhod construction of the following theorem, instead of the elementary Skorokhod theorem.) [This proof, however, requires that the metric space be complete and separable. The next exercise asks the reader to provide a general proof.] \(\Box\)

**Exercise 1.2** : Prove the continuous mapping theorem above by appeal to (5) and then proposition 2.2.4 (that the discontinuity set of a transformation between metric spaces is always measurable). This proof requires neither completeness nor separability! (And this is actually a much more elementary proof.)

**Skorokhod's Construction**

We earlier established the elementary form of Skorokhod's theorem: If random variables \(X_{n}\to_{d}X_{0}\), then there exist random variables \(\{Y_{n}\,:\,n\geq 0\}\) defined on a common probability space satisfying \(Y_{n}\cong X_{n}\) for all \(n\geq 0\) and \(Y_{n}\to_{a.s.}Y_{0}\). That proof relied on the inverse transformation. We now turn to the extension of the elementary Skorokhod theorem from \(R\) to a complete and separable metric space \((M,d)\), whose open sets generate the Borel \(\sigma\)-field. [The first step in the proof will be to establish a preliminary result for just one \(P\) on \((M,d,{\cal M}_{d})\).]

**Theorem 1.4** (Skorokhod construction): Suppose \((M,d,{\cal M}_{d})\) is a complete and separable metric space and the measures \(\{P_{n}\,:\,n\geq 0\}\) satisfy \(P_{n}\to_{d}P_{0}\). Then there exist random elements \(\{X_{n}\,:\,n\geq 0\}\) taking values in the space \(M\) (thus \(X_{n}(\omega)\in M\) for all \(\omega\in\Omega\)) and all defined on the common probability space \((\Omega,{\cal A},P)\equiv([0,1],{\cal B}([0,1])\) Lebesgue), with \(X_{n}\cong P_{n}\) and satisfying

\[d(X_{n}(\omega),\ X_{0}(\omega))\to 0\qquad\mbox{for each $\omega\in\Omega$}. \tag{8}\]

**Proposition 1.2** : Suppose \(P\) is a probability measure on \((M,d,{\cal M}_{d})\). Then there is a random element \(X\) defined on \((\Omega,{\cal A},P)=([0,1],{\cal B}[0,1]\), Lebesgue) and taking values in \(M\) that has distribution \(P\).

**Proof.** For each \(k\), decompose \(M\) into a countable number of disjoint sets \(A_{k1},A_{k2},\ldots\) whose diameter is less than \(1/k\). Then arrange it so that \({\cal A}_{k+1}\) refines \({\cal A}_{k}\equiv\{A_{k1},A_{k2},\ldots\}\). Make a corresponding decomposition of the unit interval as \({\cal I}_{k}\equiv\{I_{k1},I_{k2},\ldots\}\) where the subintervals \(I_{kj}\) satisfy \(P(A_{kj})=\lambda(I_{kj})\) and where the decomposition \({\cal I}_{k+1}\) refines \({\cal I}_{k}\).

Let \(x_{kj}\) be a point in \(A_{kj}\), and define

\[\mbox{(a)}\qquad X_{k}(\omega)=x_{kj}\qquad\mbox{if $\omega\in I_{kj}\subset[0,1]$}.\]

Since \(\{X_{k}(\omega),X_{k+1}(\omega),\ldots\}\subset\) (some one \(A_{kj}\)), its diameter is bounded by \(1/k\). Thus, \(\{X_{k}(\omega)\}\) is Cauchy for each \(\omega,\lim_{k}X_{k}(\omega)\equiv X(\omega)\) exists, and

\[d(X(\omega),X_{k}(\omega))\leq 1/k\to 0\qquad\mbox{as $k\to\infty$}. \tag{9}\]

For a given set \(B\), write \(\sum^{*}\equiv\sum_{\{j:A_{jk}\cap B\neq\emptyset\}}\), and similarly for unions of sets. Then

\[P(X_{k}\in B)\leq P(X_{k}\in\bigcup^{*}A_{kj})=\sum^{*}P(X_{k} \in A_{kj})\] \[=\sum^{*}\lambda(I_{kj})=\sum^{*}P(A_{kj})\] \[\leq P(\overline{B^{1/k}}),\]where \(B^{\delta}\equiv\{x\,:\,d(x,B)\equiv\inf_{y\in B}d(x,y)<\delta\}\). For a closed set \(B\) we have \(\bigcap_{k=1}^{\infty}\overline{B^{1/k}}=B\), and so

(c) \[\overline{\lim}_{k}\,P(X_{k}\in B)\leq\overline{\lim}_{k}\,P(X_{k}\in\overline {B^{1/k}})\leq P(B)\qquad\mbox{for closed sets $B$},\]

and hence the distribution of \(X_{k}\) converges to \(P\) by (5) of the portmanteau theorem. It follows from Slutsky's theorem (with \(Y_{k}=X\) for all \(k\)) that \(X\cong P\). \(\Box\)

**Proof.** Consider Skorokhod's theorem. First construct the decompositions \({\cal A}_{k}\) of the proof of the previous proposition, but now do it in a way that makes each \(A_{kj}\) a \(P\)-continuity set. Because \(\partial\{y:d(x,y)<\delta\}\subset\{y:d(y,x)=\delta\}\), the spheres about \(x\) are \(P\)-continuity sets for all but countably many radii; hence \(M\) can be covered by countably many \(P\)-continuity sets all with diameter at most \(1/k\). The usual disjointification procedure preserves \(P\)-continuity because \(\partial(B\cap C)\subset(\partial B)\cup(\partial C)\).

Consider the decompositions \({\cal I}_{k}\) as before, and, for each \(n\), construct successively finer partitions \({\cal I}_{k}^{(n)}=\{I_{k1}^{(n)},I_{k2}^{(n)},\ldots\}\) with \(\lambda(I_{kj}^{(n)})=P_{n}(A_{kj})\). Inductively arrange the indexing so that \(I_{ki}^{(n)}<I_{kj}^{(n)}\) if and only if \(I_{ki}<I_{kj}\); here \(I<J\) for intervals \(I\) and \(J\) means that the right endpoint of \(I\) does not exceed the left endpoint of \(J\). In other words, we ensure that for each \(k\) the families \({\cal I}_{k},{\cal I}_{k}^{(1)},{\cal I}_{k}^{(2)},\ldots\) are ordered similarly.

Define \(X_{k}\) as before, where \(x_{kj}\in A_{kj}\), and define

(a) \[X_{k}^{(n)}(\omega)=x_{kj}\qquad\mbox{if $\omega\in I_{kj}^{(n)}$}.\]

Again \(X_{k}(\omega)\) converges to an \(X(\omega)\) satisfying (9), and \(X_{k}^{(n)}(\omega)\) converges to an \(X^{(n)}(\omega)\) satisfying

(b) \[d(X^{(n)}(\omega),X_{k}^{(n)}(\omega))\leq 1/k\to 0\qquad\mbox{as $k\to\infty$}.\]

And again \(X\) has distribution \(P\) and \(X^{(n)}\) has distribution \(P_{n}\).

Since \(\sum_{j}[P(A_{kj})-P_{n}(A_{kj})]=0\), it follows that

(c) \[\sum_{j}|\lambda(I_{kj})-\lambda(I_{kj}^{(n)})|=\sum_{j}|P(A_{kj}) -P_{n}(A_{kj})|\] \[=2\sum_{j}^{\prime\prime}[P(A_{kj})-P_{n}(A_{kj})]\] (d) \[=2\sum_{j}[P(A_{kj})-P_{n}(A_{kj})]^{+},\]

where the next to the last sum extends over those \(j\) for which the summand is positive. Each summand goes to \(0\) as \(n\to\infty\) because the \(A_{kj}\) are \(P\)-continuity sets, and it follows by the DCT (with dominating function identically equal to the constant function with value \(1\)) that

(e) \[\lim_{n}\sum_{j}|\lambda(I_{kj})-\lambda(I_{kj}^{(n)})|=0.\]

Fix \(k\) and \(j_{0}\), let \(\alpha\) and \(\alpha_{n}\) be the left endpoints of \(I_{kj_{0}}\) and \(I_{kj_{0}}^{(n)}\), respectively, and let \(\sum^{\prime}\) indicate summation over the set for which \(I_{kj}<I_{kj_{0}}\), which is the same as the set for which \(I_{kj}^{(n)}<I_{kj_{0}}^{(n)}\). Then (d) implies

(f) \[\alpha=\sum_{j}^{\prime}\lambda(I_{kj})=\lim_{n}\sum_{j}^{\prime}\lambda(I_{kj }^{(n)})=\lim_{n}\alpha_{n}.\]Similarly, the right endpoint of the interval \(I_{kj}^{(n)}\) converges as \(n\to\infty\) to the right endpoint of the interval \(I_{kj}\).

Hence, if \(\omega\) is interior to \(I_{kj}\) (which now fixes \(k\) and \(j\)), then \(\omega\) lies in \(I_{kj}^{(n)}\) for all \(n\) large enough, so that \(X_{k}(n)(\omega)=x_{kj}=X_{k}(\omega)\) for all \(n\geq\) (some \(n_{k,j,\omega}\)), and the conclusions (9) and (b) give

\[d(X(\omega),X^{(n)}(\omega))\leq 2/k\qquad\mbox{ for all }m\geq n_{k,j,\omega}.\]

Thus, if \(\omega\) is not an endpoint of any \(I_{kj}\), then for each \(k\) we have that (g) holds for all sufficiently large \(n\). In other words, \(\lim_{n}X^{(n)}(\omega)=X(\omega)\) if \(\omega\) is not in the set of endpoints of the \(I_{kj}\). This last set, being countable, has Lebesgue measure \(0\); thus if \(X^{(n)}(\omega)\) is redefined as \(X(\omega)\) on this set, \(X^{(n)}\) still has distribution \(P_{n}\) and there is now convergence for all \(\omega\). This completes the proof, with \(X^{(n)}\) denoting \(X_{n}\) and with \(X\) denoting \(X_{0}\). 

**Exercise 1.3** Recall the partial sum process \(\mathbb{S}_{n}=\{\mathbb{S}_{n}(t):t\geq 0\}\) defined earlier. Now, \(\mathbb{S}_{n}\to_{d}\mathbb{S}\) by Donsker's theorem (theorem 12.8.3), where \(\mathbb{S}\) is a Brownian motion process in \(M=C[0,1]\). Consider the following four functions:

(a) \(g(x)=\sup_{0\leq t\leq 1}x(t)\),

(b) \(g(x)=\int_{0}^{1}x(t)\,dt\),

(c) \(g(x)=\lambda(\{t\in[0,1]\,:\,x(t)>0\})\) where \(\lambda\) denotes Lebesgue measure,

(d) \(g(x)=\inf\{t>0:x(t)=b\}\), with \(b>0\) fixed.

For each of these real-valued functionals \(g\) of \(x\in C[0,1]\), find the discontinuity set \(D_{g}\) of \(g\). [If we can show that the \(P\) measure of these discontinuity sets is zero, where \(P\) denotes the measure of \(\mathbb{S}\) on \(C[0,1]\), then it follows immediately from the continuous mapping theorem that \(g(\mathbb{S}_{n})\to_{d}g(\mathbb{S})\).]

**Exercise 1.4** Let \(S_{0}\equiv 0,S_{1}\equiv X_{1},S_{k}\equiv X_{1}+\cdots+X_{k}\) for \(k\geq 1\) be the partial sums of the iid \((0,1)\) rvs that go into the definition of \(\mathbb{S}_{n}\). Represent \(n^{-3/2}\sum_{k=1}^{n}|S_{k}|\) as a functional \(g(\mathbb{S}_{n})\). Is the resulting \(g\) continuous?

**Tightness and Relative Compactness**

The notion of tightness comes into play in a crucial way in the general theory of convergence in distribution on a metric space \((M,d)\), since there now are more ways to "leave" the space than simply for mass to drift off to infinity.

**Definition 1.2** (Tightness) Let \(\mathcal{P}_{0}\) denote a collection of probability measures on some \((M,d,\,\mathcal{M}_{d})\). Then \(\mathcal{P}_{0}\) is _tight_ (or _uniformly tight_) if and only if for every \(\epsilon>0\) there is a compact set \(K_{\epsilon}\subset M\) with

\[P(K_{\epsilon})>1-\epsilon\qquad\mbox{for all }P\in\mathcal{P}_{0}.\]

**Definition 1.3** (Sequential compactness, or relative compactness) Let \(\mathcal{P}_{0}\) be a family of probability measures on \((M,d,\mathcal{M}_{d})\). We call \(\mathcal{P}_{0}\)_relatively compact_ (or _sequentially compact_) if every sequence \(\{P_{n}\}\subset\mathcal{P}_{0}\) contains a weakly convergent subsequence. That is, every sequence \(\{P_{n}\}\subset\mathcal{P}_{0}\) contains a subsequence \(\{P_{n^{\prime}}\}\) with \(P_{n^{\prime}}\to_{d}\) (some probability \(Q\)) (not necessarily in \(\mathcal{P}_{0}\)).

**Proposition 1.3** Let \((M,d,{\cal M}_{d})\) be a separable metric space.

(a) If \(\{P_{n}\}_{n=1}^{\infty}\) is relatively compact with limit set \(\{P\}\), then \(P_{n}\to_{d}P\).

(b) If \(P_{n}\to_{d}P\), then \(\{P_{n}\}_{n=1}^{\infty}\) is relatively compact.

(c) We have thus related convergence in distribution to relative compactness.

**Proof.** See proposition 1.1 for both (a) and (b). (That is, we have merely rephrased things we already know.) \(\Box\)

**Theorem 1.5** (Prohorov) Let \({\cal P}_{0}\) denote a collection of probability measures on the metric space \((M,d,{\cal M}_{d})\).

(a) If \({\cal P}_{0}\) is tight, then it is relatively compact.

(b) Suppose that \((M,d,{\cal M}_{d})\) is separable and complete. If the collection \({\cal P}_{0}\) is relatively compact, then it is tight.

(c) We have thus related relative compactness to tightness, at least on complete and separable metric spaces.

**Proof.** A full proof progresses from \(M=R_{k}\) to \(R_{\infty}\), to sigma compact \(M\), and finally to general \(M\), at each step using the next simpler case. We present only the proof of (a) for the case \(M=R_{k}\).

If \(\{P_{n}\}\) is any sequence in \({\cal P}_{0}\), then Helly's selection theorem implies that the corresponding sequence of dfs \(\{F_{n}\}\) defined by \(F_{n}(\times)\equiv P_{n}(-\infty,\times]\) contains a subsequence \(\{F_{n^{\prime}}\}\) satisfying

(p) \[F_{n^{\prime}}(\times)\to F(\times)\qquad\mbox{for all $\times\in C_{F}$},\]

where the sub df \(F\) is continuous from above. Now, there is a measure \(P\) on \(R_{k}\) such that \(P({\tt a},{\tt b}]\) equals \(F\) differenced around the vertices of the \(k\)-dimensional rectangle \(({\tt a},{\tt b}]\). Now \(P_{n^{\prime}}\to_{d}P\) will follow if we can show that \(P(R_{k})=1\).

Given \(\epsilon>0\), choose \(K\subset R_{k}\) compact with \(P_{n^{\prime}}(K)>1-\epsilon\) for all \(n^{\prime}\); this is possible by tightness of \({\cal P}_{0}\). Now choose \({\tt a},{\tt b}\) in \(R_{k}\) such that \(K\subset({\tt a},{\tt b}]\) and all \(2^{k}\) vertices of \(({\tt a},{\tt b}]\) are continuity points of \(F\) (we can do this because at most a countable number of parallel \((k-1)\)-dimensional hyperplanes can possibly have positive \(P\)-measure. Since \(P_{n^{\prime}}({\tt a},{\tt b}]\) equals \(F_{n^{\prime}}\) differenced around \(({\tt a},{\tt b}]\), (a) yields \(P_{n^{\prime}}({\tt a},{\tt b}]\geq P_{n^{\prime}}(K)\geq 1-\epsilon\), so \(P({\tt a},{\tt b}]\geq 1-\epsilon\). Since \(\epsilon\) was arbitrary, \(P(R_{k})=1\). Hence \(P\) is a probability measure, \(P_{n^{\prime}}\to_{d}P\), and \({\cal P}_{0}\) is relatively compact. \(\Box\)

**Convergence in Distribution on \((D,{\cal D})\)**

We phrase results carefully in this subsection, so as to mention primarily the familiar metric \(\|\cdot\|\) on \(D\) (while limiting mention of the contrived metric \(d\) of exercise 12.1.4 for which \((D,d)\) is complete and separable with \({\cal D}_{d}={\cal D}\)). Recall, \({\cal D}\) denotes the finite-dimensional \(\sigma\)-field.

**Theorem 1.6** (Criterion for \(\to_{d}\) on \((D,{\cal D})\); Chentsov) Let each \(X_{n}\) denote a process on \((D,{\cal D})\). Suppose that for some \(a>\frac{1}{2}\) and \(b>0\) the increments of the \(X_{n}\) processes satisfy

\[{\rm E}|X_{n}(r,s]X_{n}(s,t]|^{b}\leq[\mu_{n}(r,s]\times\mu_{n}(s,t]]^{a} \qquad\mbox{for all $0\leq r\leq s\leq t\leq 1$} \tag{11}\]

for some finite measure \(\mu_{n}\) on the Borel subsets of \([0,1]\). (Often, \(a=1\) and \(b=2\), and note the relaxation of this condition in the remark below.) Suppose that \(\mu\) is a continuous measureon the Borel subsets of \([0,1]\), and that either

\[\mu_{n}(s,t]\leq\mu(s,t]\qquad\mbox{for all $0\leq s\leq t\leq 1$ \ and for all $n\geq 1$}\qquad\mbox{or} \tag{12}\]

\[\mu_{n}/\mu_{n}([0,1])\to_{d}\mu/\mu([0,1])\qquad\mbox{as $n\to\infty$}. \tag{13}\]

(a) Then for the metric \(d\) of exercise 12.1.4 we have that

\[\{X_{n}\,:\,n\geq 1\}\mbox{ is relatively compact on $(D,d)$}. \tag{14}\]

(b) Especially, if we further have \(X_{n}\to_{f\cdot d}\). (some \(X\)) and if \(P(X\in C)=1\)), then

\[g(X_{n})\to_{d}g(X)\mbox{ for all $\mathcal{D}$-measurable and a.s. $\|\cdot\|$-continuous $g:D\to R$}. \tag{15}\]

**Remark 1.1**: For processes like partial sum process \(\mathbb{S}_{n}\) condition (11) is troublesome; but since \(\mathbb{S}_{n}\) is constant on the intervals \([(i-1)/n,\ i/n)\), it should be enough to verify (11) for \(r,s,t\) restricted to be of the form \(i/n\). We now make this rough idea precise.

For \(m\geq 1\) we let \(T_{m}\equiv\{0\equiv t_{m0}<t_{m1}<\cdots<t_{mk_{m}}\equiv 1\}\), and measure the coarseness of this partition by defining \(\mbox{mesh}(T_{m})\equiv\max\{t_{mi}-t_{m,i-1}\,:\,1\leq i\leq k_{m}\}\). Let \(x\in D\) and let \(A_{m}(x)\) denote the function in \(D\) that equals \(x(t_{mi})\) at \(t_{mi}\) for each \(0\leq i\leq k_{m}\) and that is constant in between these points. We agree to call \(A_{m}(x)\) the \(T_{m}\)-_approximation_ of \(x\). Suppose now that one of the two conditions (12) or (13) holds, that \(X_{n}=A_{n}(X_{n})\) with \(\mbox{mesh}(T_{n})\to 0\) as \(n\to\infty\), and that (11) holds for all \(r,s,t\) in \(T_{n}\). Then both (14) and (15) hold. [That \(X_{n}=A_{n}(X_{n})\) means that \(X_{n}\) is equal to its own \(T_{n}\)-approximation; and this clearly holds for \(\mathbb{S}_{n}\) when all \(t_{ni}=i/n\).] \(\Box\)

**"Proof."** By analogy with Helly's selection theorem, it should suffice to show the tightness. Thus, for each \(\epsilon>0\) we must exhibit a compact set \(K_{\epsilon}\) of functions on \([0,1]\) with \(P(X_{n}\in K_{\epsilon})>1-\epsilon\) for all \(n\). According to Arzela's theorem (see exercise B.2.11), a compact set of functions consists of a uniformly bounded set of functions that have a uniform bound on their "wiggliness." A complex and delicate argument (slightly in the spirit of Kolmogorov's inequality) based on (6) can be given to bound this "wiggliness." Since the details are long and hard and are only used for the present theorem, they will be skipped. See Billingsley (1968). \(\Box\)

**Exercise 1.5** (Prohorov): For any real-valued function on \([0,1]\) we now define the _modulus of continuity_\(\omega_{\delta}(x)\) of \(x\) by \(\omega_{\delta}(x)\equiv\max\{|x_{t}-x_{s}|\,:|t-s|\leq\delta\}\) for each \(\delta>0\). Let \(X,X_{1},X_{2},\ldots\) denote processes on \((C,{\cal C})\). Then \(X_{n}\to_{d}X\) on \((C,\|\cdot\|)\) if and only if both \(X_{n}\to_{f\cdot d}X\) and \(\lim_{\delta\to 0}\limsup_{n\to\infty}P(\omega_{\delta}(X_{n})>\epsilon)=0\) for all \(\epsilon>0\). (The modulus of continuity also measures the "wiggliness" of the process, and Prohorov's condition implies that the processes are "not too wiggly.")

**Exercise 1.6** (Doob): Use theorem 1.6 to establish that \(g(\mathds{U}_{n})\to_{d}g(\mathds{U})\) for all \(\mathcal{D}\)-measurable and a.s. \(\|\cdot\|\)-continuous functionals \(g\) on \(D\). See (12.10.4) for \(\mathds{U}_{n}\).

**Exercise 1.7** (Donsker): Use theorem 1.6 to establish that \(g(\mathbb{S}_{n})\to_{d}g(\mathbb{S})\) for all \(\mathcal{D}\)-measurable and a.s. \(\|\cdot\|\)-continuous functionals \(g\) on \(D\).

**Exercise 1.8** (Prohorov): Linearize \(\mathbb{S}_{n}\) between the \(i/n\)-points so as to make it a process on \((C,\|\cdot\|)\), and then use exercise 1.3 to show that this linearized process converges in distribution to Brownian motion.

## 2 Metrics for Convergence in Distribution \({}^{o}\)

**Definition 2.1** (Prohorov metric): [The Levy metric of exercise 9.1.5 extends in a nice way to give a metric for \(\to_{d}\) more generally.] For any Borel set \(B\in{\cal M}_{d}\) and \(\epsilon>0\), define

\[B^{\epsilon}\equiv\{y\in M:d(x,y)<\epsilon\quad\mbox{for some }x\in B\}.\]

Let \(P,Q\) be two probability measures on \((M,d,{\cal M}_{d})\). If we set

\[\rho(P,Q)\equiv\inf\{\epsilon:Q(B)\leq P(B^{\epsilon})+\epsilon\quad\mbox{for all }B\in{\cal M}_{d}\}, \tag{1}\]

then \(\rho\) is the _Prohorov metric_ (see exercise 2.1). (We note that this definition is not formed in a symmetric fashion.)

**Definition 2.2** (Dudley metric): (i) Label as \(BL(M,d)\) the set of all real-valued functions \(g\) on the metric space \((M,d)\) that are bounded and Lipschitz (in the sense that both of the quantities

\[\|g\|_{\infty}\equiv\sup_{x\in M}|g(x)|\qquad\mbox{and}\qquad\|g\|_{L}\equiv \sup_{x\neq y}[|f(x)-f(y)|/d(x,y)] \tag{2}\]

are finite). For functions \(g\) in \(BL(M,d)\) we define

\[\|g\|_{BL}\equiv\|g\|_{L}+\|g\|_{\infty}, \tag{3}\]

and so \(BL(M,d)=\{g\,:\,\|g\|_{BL}<\infty\}\).

(ii) Now let \(P,Q\) be two probability measures on \((M,{\cal M}_{d})\), and set

\[\beta(P,Q)\equiv\sup\{|\int g\,dP-\int g\,dQ|\,:\,\|g\|_{BL}\leq 1\}. \tag{4}\]

Then \(\beta\) is called the _dual bounded Lipschitz distance_ (or _Dudley distance_) between the probability distributions \(P\) and \(Q\).

**Proposition 2.1**: Let \({\cal P}\equiv\{\mbox{all probability distributions }P\mbox{ on }(M,{\cal M})\ \}\). Then both \(\rho\) and \(\beta\) are metrics on \({\cal P}\). (Equation (12) below will show that \({\cal M}_{\rho}={\cal M}_{\beta}\), which we abbreviate here as \({\cal M}\).)

**Exercise 2.1**: Prove the previous proposition.

The following theorem says that both \(\rho\) and \(\beta\) metrize \(\to_{d}\) on \((M,d,{\cal M}_{d})\) just as the Levy distance \(L\) metrized the convergence in distribution \(\to_{d}\) of dfs on \(R_{1}\).

**Theorem 2.1** (Metrizing \(\to_{d}\); Dudley): For any separable metric space \((M,d)\) and probability measures \(\{P_{n}\,:\,n\geq 1\}\) and \(P\) on the Borel \(\sigma\)-field \({\cal M}_{d}\), the following are equivalent conditions:

\[P_{n}\to_{d}P, \tag{6}\] \[\int g\,dP_{n}\to\int g\,dp\qquad\mbox{for all }g\in BL(M,d),\] (7) \[\beta(P_{n},P)\to 0,\] (8) \[\rho(P_{n},P)\to 0. \tag{5}\]

**Theorem 2.2** (Ulam): For \((M,d)\) complete and separable, each single \(P\) on \((M,{\cal M}_{d})\) is tight.

**Proof.** Let \(\epsilon>0\). By the separability of \(M\), for each \(m\geq 1\) there is a sequence \(A_{m1}\), \(A_{m2},\ldots\) of open \(1/m\) spheres covering \(M\). Choose \(i_{m}\) such that \(P(\bigcup_{i\leq i_{m}}A_{mi})>1-\epsilon/2^{m}\). Now, the set \(B\equiv\bigcap_{m=1}^{\infty}\bigcup_{i\leq i_{m}}A_{mi}\) is totally bounded in \(M\), meaning that for each \(\epsilon>0\) it has a finite \(\epsilon\)-net (that is, a set of points \(x_{k}\) with \(d(x,x_{k})<\epsilon\) for some \(x_{k}\), for each \(x\in B\)). [Nice trick!] By completeness of \(M,K\equiv\bar{B}\) is complete and is also compact; see exercises B.2.3(c) and B.2.4(e). Since

(a) \[P(K^{c})=P(\bar{B}^{c})\leq P(B^{c})\leq\sum_{m=1}^{\infty}P([\bigcup_{i\leq i _{m}}A_{mi}]^{c})<\sum_{1}^{\infty}\epsilon/2^{m}=\epsilon,\]

the conclusion follows. \(\square\)

**Proof.** We now prove theorem 2.1, since Ulam's theorem is in hand, but only under the additional assumption that \(M\) is complete. Clearly, (5) implies (6). We will now show that (6) implies (7). By Ulam's theorem, for any \(\epsilon>0\) we can choose \(K\) compact so that \(P(K)>1-\epsilon\). Let \(f|K\) denote \(f\) restricted to \(K\). Now, the set of functions \({\cal F}\equiv\{f|K\ :\ \|f\|_{BL}\leq 1\}\) forms a \(\|\cdot\|\)-totally bounded subset of the functions \({\cal C}_{b}(K)\) (by the Arzela theorem of exercise B.2.11(a)). Thus, for every \(\epsilon>0\) there is some finite \(k\equiv k_{\epsilon}\) and functions \(f_{1},\ldots,f_{k}\in{\cal F}\) such that for any \(f\in{\cal F}\) there is an \(f_{j}\) with

(a) \[\sup_{x\in K}|f(x)-f_{j}(x)|\leq\epsilon;\qquad\mbox{moreover,}\qquad\sup_{x \in K^{*}}|f(x)-f_{j}(x)|\leq 3\epsilon,\]

since \(f\) and \(f_{j}\) are in \({\cal F}\) (note the second half of (2)). Let

\[g(x)\equiv\max\{0,\ (1-d(x,\ K)/\epsilon)\};\]

then \(g\in BL(M,d)\) and \(1_{K}\leq g\leq 1_{K^{*}}\). Thus, \(\int g\,dP_{n}\to\int g\,dP\), so that for \(n\) large enough we have

(b) \[P_{n}(K^{\epsilon})\geq\int g\,dP_{n}>\int g\,dP-\epsilon\geq\int 1_{K}\,dP- \epsilon=P(K)-\epsilon>1-2\epsilon.\]

Hence, for any \(f\in{\cal F}\) we have from (a), (2), (b), and \(P(K)>1-\epsilon\) that

\[|\int f\,d(P_{n}-P)|=|\int(f-f_{j})\,d(P_{n}-P)+\int f_{j}\,d(P_{n}-P)|\] \[\leq|\int(f-f_{j})\,dP_{n}|+|\int(f-f_{j})\,dP|+|\int f_{j}\,d(P_ {n}-P)|\] (c) \[\leq(3\epsilon+2\times 2\epsilon)+(\epsilon+2\times\epsilon)+|\int f _{j}\,d(P_{n}-P)|\leq 11\epsilon\]

for \(n\) chosen large enough. Hence (7) holds.

We next show that (7) implies (8). Suppose a Borel set \(B\) and an \(\epsilon>0\) are given. Let \(f_{\epsilon}(x)\equiv\max\{0,\ (1-d(x,\ B)/\epsilon)\}\). Then \(f_{\epsilon}\in BL(M,d),\|f_{\epsilon}\|_{BL}\leq 1+\epsilon^{-1}\), and \(1_{B}\leq f_{\epsilon}\leq 1_{B^{*}}\). Therefore, for any \(P\) and \(Q\) on \(M\) we have from (4) that

(d) \[Q(B)\leq\int f_{\epsilon}\,dQ\leq\int f_{\epsilon}\,dP+(1+\epsilon^{-1})\, \beta(P,Q)\leq P(B^{\epsilon})+(1+\epsilon^{-1})\,\beta(P,Q),\]

and it follows that

(e) \[\rho(P,Q)\leq\max\{\epsilon,\ (1+\epsilon^{-1})\,\beta(P,Q)\}.\]

Hence, if \(\beta(P,Q)<\epsilon^{2}\), then \(\rho(P,Q)<\epsilon+\epsilon^{2}<2\epsilon\). Hence, for all \(P,Q\) we have \(\rho(P,Q)\leq 2\sqrt{\beta(P,Q)}\). Thus, (7) implies (8). [It is also possible to establish the inequality \(\beta(P,Q)/2\leq\rho(P,Q)\). This would give

(f) \[\beta(P,Q)/2\leq\rho(P,Q)\leq 2\sqrt{\beta(P,Q)},\]showing that \(\rho\) and \(\beta\) are equivalent metrics (see (12) and exercise 2.3 below).]

Finally, we will show that (8) implies (5). Suppose (8) holds. Let \(B\) denote a \(P\)-continuity set, and let \(\epsilon>0\). Then for \(0<\delta<\epsilon\) with \(\delta\) small enough, we have \(P(B^{\delta}\setminus B)<\epsilon\) and \(P((B^{c})^{\delta}\setminus B^{c})<\epsilon\). Then for \(n\) large enough we have

(g) \[P_{n}(B)\leq P(B^{\delta})+\delta\leq P(B)+2\epsilon\]

and also

(h) \[P_{n}(B^{c})\leq P((B^{c})^{\delta})+\delta\leq P(B^{c})+2\epsilon.\]

Combining these yields

(i) \[|P_{n}(B)-P(B)|\leq 2\epsilon,\]

and hence \(P_{n}(B)\to P(B)\). By the portmanteau theorem, this yields (5). \(\Box\)

**Strassen's Coupling Theorem**

Suppose \(P_{n}\to_{d}P_{0}\) on a separable metric space \((M,d)\). Then theorem 2.1 gives \(\rho(P_{n},P_{0})\to 0\), for Prohorov's metric \(\rho\), while Skorokhod's theorem gives existence of random elements \(X_{n}\) on a common \((\Omega,{\cal A},P)\) satisfying \(d(X_{n},X_{0})\to_{a.s.}0\). Claiming less than is true, \(d(X_{n},X_{0})\to_{p}0\), or \(P(d(X_{n},X_{0})\geq\epsilon)\to 0\) as \(n\to\infty\). We are then naturally led to ask how rapidly this convergence occurs. It turns out that this is essentially determined by \(\rho(P_{n},P)\). Alternatively, the following theorem can be used to bound \(\rho(P_{n},P)\), provided that a rate is available regarding Skorokhod.

**Theorem 2.3** (Strassen):Suppose that \(P\) and \(Q\) are measures on the Borel sets of a separable metric space \((M,d)\). Then \(\rho(P,Q)<\epsilon\) if and only if there exist \(X\) and \(Y\) defined on a common probability space with \(X\cong P\) and \(Y\cong Q\) and coupled closely enough that \(P(d(X,Y)\geq\epsilon)\leq\epsilon\).

**Proof.** See Dudley (1976, section 18). \(\Box\)

**Some Additional Metrics**

As shown in theorem 2.1, both the Prohorov metric \(\rho\) and also the dual-bounded Lipschitz metric \(\beta\) metrize weak convergence \((\to_{d})\). Other stronger metrics are also available and are often useful.

**Definition 2.3** (Total variation metric): For probability measures \(P\) and \(Q\) on the measurable space \((M,{\cal M}_{d})\), let

\[d_{TV}(P,Q)\equiv 2\sup\{|P(A)-Q(A)|\,:\,A\in{\cal M}_{d}\}; \tag{9}\]

\(d_{TV}\) is called the _total variation metric_.

**Proposition 2.2** The total variation metric \(d_{TV}\) is equal to

\[d_{TV}(P,Q)=\int|f-g|\,d\mu=2[1-\int f\wedge gd\mu], \tag{10}\]

where \(f=dP/d\mu\), \(g=dQ/d\mu\), and \(\mu\) is any measure dominating both \(P\) and \(Q\) (for example, \(P+Q\)).

**Proof.** Note that \(|f-g|=(f\lor g)-(f\wedge g)=(f+g)-2(f\wedge g)\). \(\Box\)

**Definition 2.4** (Hellinger metric) For probabilities \(P\) and \(Q\) on \((M,{\cal M}_{d})\), let

\[d_{H}^{2}(P,Q)\equiv\int[f^{1/2}-g^{1/2}]^{2}\,d\mu=2[1-\int\sqrt{fg}\,d\mu], \tag{11}\]

where \(f=dP/d\mu,g=dQ/d\mu\), and \(\mu\) is any measure dominating both \(P\) and \(Q\) (for example, \(P+Q\)); then \(d_{H}\) is called the _Hellinger metric_.

**Exercise 2.2** \(d_{H}\) does not depend on the choice of \(\mu\).

Here is a theorem relating these metrics and the Prohorov and bounded Lipschitz metrics. The inequalities in (12) show that \(\rho\) and \(\beta\) induce identical topologies. The inequalities (13) show that the total variation metric \(d_{TV}\) and the Hellinger metric \(d_{H}\) induce identical topologies. Moreover, (14) shows that the identical \(\beta\) and \(\rho\) topologies are finer than the \(d_{TV}\) topology (with more open sets and admitting more continuous functions). [Note exercise B.2.1 dealing with equivalent metrics.]

**Theorem 2.4** (Inequalities among the metrics) (a) For \(P\) and \(Q\) probability measures on \((M,{\cal M}_{d})\), the following inequalities necessarily hold:

\[\beta(P,Q)/2\leq\rho(P,Q)\leq 2\sqrt{\beta(P,Q)}, \tag{13}\] \[d_{H}^{2}(P,Q)\leq d_{TV}(P,Q)\leq d_{H}(P,Q)\{4-d_{H}^{2}(P,Q) \}^{1/2},\] (14) \[\rho(P,Q)\leq d_{TV}(P,Q). \tag{12}\]

(b) For dfs \(F,G\) on \(R\) (or \(R_{k}\)) we have the following:

\[L(F,G)\leq\rho(F,G)\leq d_{TV}(F,G), \tag{16}\] \[L(F,G)\leq d_{K}(F,G)\leq d_{TV}(F,G), \tag{15}\]

where \(d_{K}(F,G)\equiv\|F-G\|_{\infty}\equiv\sup_{x}|F(x)-G(x)|\) is the Kolmogorov distance.

**Exercise 2.3** Prove the first inequality in (12).

**Exercise 2.4** Prove (13). [Hint. To prove the left inequality, establish the inequality \(\int\sqrt{fg}\,d\mu\geq\int f\wedge gd\mu\) and use the second equality in (11). To show the right inequality, write \(|f-g|=|\sqrt{f}-\sqrt{g}||\sqrt{f}+\sqrt{g}|\).

**Exercise 2.5** Prove (14).

**Exercise 2.6** (Statistical interpretation of the \(d_{TV}\) metric): Consider testing \(P\) versus \(Q\). Find the test that minimizes the sum of the error probabilities, and show that the minimum sum of errors is \(\left\|P\wedge Q\right\|\equiv\int f\wedge gd\mu\). Note that \(P\) and \(Q\) are orthogonal if and only if \(\left\|P-Q\right\|\equiv d_{TV}(P,Q)=2\) if and only if \(\left\|P\wedge Q\right\|=0\) if and only if \(\int\sqrt{fg}\,d\mu\equiv\int\sqrt{dP\,dQ}=0\).

## Chapter 15 Asymptotics via Empirical Processes

### 0 Introduction

In section 15.1 we rederive the usual CLT for iid samples from any distribution in the domain of attraction of the normal distribution, but now using empirical process methods. We then obtain a corresponding result for the trimmed mean that is valid for samples from any df in _any_ domain of attraction, provided only that the number of observations trimmed from each tail grows to infinity so slowly that the fraction of observations trimmed from each tail goes to zero. When the qfs of all distributions in a class are bounded by an appropriate envelope qf, then all of these convergence results are uniform across the class of qfs. This uniformity allows random trimming. In section 15.2 similar results are derived for linear rank tests and permutation tests, and a uniform studentized CLT is given for sampling from a finite population. Also, two very interesting ways of creating normality are discussed in this section. In section 15.3 results are presented for L-statistics.

All of the results are based on the empirical process construction of section 12.10 combined with the quantile method inequalities from sections C.5-C.6. We will frequently obtain conclusions of the form \(T_{n}\rightarrow_{p}Z\) for a special construction version of an interesting statistic \(T_{n}\) and its limiting normal rv \(Z\). Or, we may even obtain \(\sup_{\mathcal{K}}|T_{n}-Z(K)|\rightarrow_{p}0\) for a family of normal rvs \(Z(K)\) indexed by the qf \(K\) in a class \(\mathcal{K}\). These strong \(\rightarrow_{p}\) conclusions are only true for the special construction versions of the statistic \(T_{n}\). However, \(T_{n}\rightarrow_{p}Z\) for the special construction \(T_{n}\) implies that \(T_{n}\rightarrow_{d}Z\) for _any_ version of \(T_{n}\). In like fashion, \(\sup_{\mathcal{K}}|T_{n}-Z(K)|\rightarrow_{p}0\) for a special construction version of \(T_{n}\) implies that the rate at which \(T_{n}\rightarrow_{d}Z\) is uniform across the entire class of qfs in \(\mathcal{K}\) for _any_ version of the statistic \(T_{n}\).

## Chapter 1 Trimmed and Winsorized Means \({}^{*}\)

**Notation 1.1** Let \(X_{n1},\ldots,X_{nn}\) be iid with df \(F\) and df \(K\). Let \(X_{n:1}\leq\cdots\leq X_{n:n}\) denote their order statistics, with empirical qf \(\mathbb{K}_{n}\). Define \(\mu\equiv\mu_{K}\equiv\int_{0}^{1}K(t)dt=\mathrm{E}K(\xi)\) and \(\sigma^{2}\equiv\sigma_{K}^{2}\equiv\mathrm{Var}[K(\xi)]\), when these exist. We also let \(\tilde{X}_{n}=\sum_{1}^{n}X_{ni}/n\) denote the sample mean and \(S_{n}^{2}=\sum_{1}^{n}(X_{ni}-\tilde{X}_{n})^{2}/n\) denote the "sample variance" For trimming numbers \(k_{n}\wedge k_{n}^{\prime}\geq 1\) and for \(a_{n}\equiv k_{n}/n\) and \(a_{n}^{\prime}\equiv k_{n}^{\prime}/n\), we let \(\tilde{K}_{n}(\cdot)\) denote \(K(\cdot)\) Winsorized outside \((a_{n},\ a_{n}^{\prime})\) (see notation 6.5.1) and define

\[\begin{array}{l}\tilde{\mu}_{n}\equiv\tilde{\mu}_{K}(a_{n},\ a_{n}^{\prime} )\equiv\int_{a_{n}}^{1-a_{n}}K(t)dt=\mathrm{E}\{K(\xi)\times 1_{(a_{n},1-a_{n}^{ \prime})}(\xi)\},\\ \tilde{\sigma}_{n}^{2}\equiv\tilde{\sigma}_{K}^{2}(a_{n},\ a_{n}^{\prime}) \equiv\int_{0}^{1}\int_{0}^{1}[s\wedge t-st]d\tilde{K}_{n}(s)d\tilde{K}_{n}(t) =\mathrm{Var}[\tilde{K}_{n}(\xi)],\\ \tilde{X}_{n}\equiv\tilde{X}_{n}(a_{n},\ a_{n}^{\prime})\equiv\frac{1}{n}\sum_{ i=k_{n}+1}^{n-k_{n}^{\prime}}X_{n:i}=\tilde{\mu}_{\mathbb{K}_{n}}(a_{n},\ a_{n}^{ \prime}),\\ \tilde{S}_{n}^{2}\equiv\tilde{\sigma}_{\mathbb{K}_{n}}^{2}(a_{n},\ a_{n}^{ \prime}).\end{array} \tag{0}\]

Here \(\tilde{S}_{n}\equiv\tilde{\sigma}_{\mathbb{K}_{n}}(a_{n},\ a_{n}^{\prime})\) denotes the _sample \((a_{n},\ a_{n}^{\prime})\)-Winsorized standard deviation_, and the rv \(\tilde{X}_{n}=\tilde{\mu}_{\mathbb{K}_{n}}(a_{n},\ a_{n}^{\prime})\) is being called the _sample \((a_{n},\ a_{n}^{\prime})\)-truncated mean_, while \(\tilde{X}_{n}\equiv\tilde{X}_{n}/(1-a_{n}-a_{n}^{\prime})\) denotes the _sample \((a_{n},\ a_{n}^{\prime})\)-trimmed mean_. Also, \(\tilde{\mu}_{n}\equiv\tilde{\mu}_{n}/(1-a_{n}-a_{n}^{\prime})\) is the population trimmed mean, and \(\tilde{\sigma}_{n}\equiv\tilde{\sigma}_{n}/(1-a_{n}-a_{n}^{\prime})\). Now, \(\tilde{X}_{n}\) is our vehicle of convenience for studying the trimmed mean \(\tilde{X}_{n}\), since

\[\sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}=\tilde{Z}_{n}\equiv \sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}. \tag{1}\]

Summary: \(\tilde{X}_{n}\) has statistical meaning, while \(\tilde{X}_{n}\) does not; but \(\tilde{X}_{n}\) is much more convenient to work with notationally and probabilistically.

So long as \(k_{n}\wedge k_{n}^{\prime}\geq 1\), we _always_ have (integrating Brownian bridge \(\mathbb{U}\), which for each fixed \(\omega\) is just a continuous function)

\[Z(\tilde{K}_{n})\equiv-\int_{a_{n}}^{1-a^{\prime}_{n}}\mathbb{U}dK/\tilde{ \sigma}_{n}=-\int_{0}^{1}\mathbb{U}d\tilde{K}_{n}/\tilde{\sigma}_{n}\cong N(0, 1). \tag{2}\]

Generally, \(\tilde{K}_{t,t}(\cdot)\) denotes \(K(\cdot)\) Winsorized outside \((t,\ 1-t)\), and \(\tilde{\sigma}^{2}(t)\equiv\mathrm{Var}[\tilde{K}_{t,t}(\xi)].\) Recall that \(R_{n}=_{a}S_{n}\) denotes that \(R_{n}-S_{n}\to_{p}0\). \(\square\)

**Convention** We now specify that _throughout this chapter \(X_{ni}\equiv K(\xi_{ni})=F^{-1}(\xi_{ni})\)_, for \(1\leq i\leq n\), for the \(\xi_{ni}\) described in notation 1.3 below. (Recall the sentence above (6.4.3) noting that this representation of rvs allows alternative methods of proof.)

**Note** The conclusions of being "\(=_{a}\)" in (4) and (6) below are true _only_ for these particular rvs \(F^{-1}(\xi_{ni})\) that are iid \(F(\cdot)\) but the implied \(\to_{d}\) conclusion is true for _any_ iid rvs \(X_{n1},\ldots,X_{nn}\) having df \(F(\cdot)\).

**Theorem 1.1** (The CLT for \(\tilde{X}_{n}\)): Let \(K\in\mathcal{D}\)(Normal), which is equivalent to

\[t[K_{+}^{2}(ct)\lor K^{2}(1-ct)]/\tilde{\sigma}^{2}(t)\to 0\qquad\text{as $t\to 0$, \quad for each fixed $c>0$} \tag{3}\]

by (10.6.31). Define \(\tilde{\mu}_{n}\equiv\mu_{K}(1/n,\ 1/n)\) and \(\tilde{\sigma}_{n}\equiv\tilde{\sigma}_{K}(1/n,\ 1/n)\), and let \(\tilde{K}_{n}(\cdot)\) denote \(K(\cdot)\) Winsorized outside \((1/n,\ 1-1/n)\). Then the mean \(\tilde{X}_{n}\) of the iid rvs \(X_{n1},\ldots,X_{nn}\) above satisfies (as is also shown in theorem 10.6.1)

\[Z_{n}=\sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}=_{a}Z(\tilde{ K}_{n})\cong N(0,1)\quad\text{and}\quad S_{n}/\tilde{\sigma}_{n}\to_{p}1. \tag{4}\]

**Notation 1.2** For the following theorem, suppose the integers \(k_{n}\) and \(k^{\prime}_{n}\) satisfy

\[(k_{n}\wedge k^{\prime}_{n})\to\infty,\quad(a_{n}\lor a^{\prime}_{n})\to 0,\quad \mbox{and}\quad\ a^{\prime}_{n}/a_{n}\to 1.\]

Of course, \(\tilde{K}_{n}(\cdot)\) now denotes \(K(\cdot)\) Winsorized outside \((a_{n},\ 1-a^{\prime}_{n})\). \(\Box\)

**Theorem 1.2** (The CLT for trimmed means): Suppose the qf \(K(\cdot)\) is such that the partial variance \(\tilde{\sigma}^{2}(t)\equiv\mbox{Var}[\tilde{K}_{t,t}(\xi)]\) is in one of the regularly varying classes \({\cal R}_{-\beta}\), for some \(\beta\geq 0\). [This union of all the \({\cal R}_{-\beta}\) classes was labeled as \(\tilde{\cal D}\) in definition (C.5.33), and \({\cal R}_{0}\equiv{\cal D}(\mbox{Normal})\). Note inequality C.5.4.] If (5) holds, then

\[\tilde{Z}_{n}=\sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}=_{a}Z (\tilde{K}_{n})\cong N(0,1)\quad\mbox{and}\quad\tilde{S}_{n}/\tilde{\sigma}_{n }\to_{p}1.\]

If \(\beta=0\), we can weaken (5) to \(a_{n}\lor a^{\prime}_{n}\to 0\) when \(0<\underline{\lim}\,a_{n}/a^{\prime}_{n}\leq\overline{\lim}\,a_{n}/a^{\prime}_ {n}<\infty\).

**Corollary 1** (Trimming fixed fractions): Suppose \(a_{n}\to a\) and \(a^{\prime}_{n}\to a^{\prime}\) for \(0<a<1-a^{\prime}<1\). Then (4) holds for any qf \(K(\cdot)\) continuous at both \(a\) and \(1-a^{\prime}\).

**Notation 1.3**: Be clear that we are working on the specific probability space \((\Omega,{\cal A},P)\) of theorem 12.10.3 on which are defined a fixed Brownian bridge \(\mathbb{U}\) and a triangular array of rvs whose \(n\)th row members \(\xi_{n1},\ldots,\xi_{nn}\) are iid Uniform(0, 1) with order statistics \(0\leq\xi_{n:1}\leq\cdots\leq\xi_{n:n}\leq 1\), empirical df \(\mathbb{G}_{n}\), and empirical process \(\mathbb{U}_{n}=\sqrt{n}[\mathbb{G}_{n}-I]\) that not only satisfies \(\|\mathbb{U}_{n}=\mathbb{U}\|\to_{p}0\), but in fact, for each fixed \(0\leq\nu<\frac{1}{4}\), satisfies

\[\triangle_{\nu n}\equiv\left\|\frac{n^{\nu}(\mathbb{U}_{n}-\mathbb{U})}{[I \wedge(1-I)]^{(1/2)-\nu}}\right\|_{1/n}^{1-1/n}=O_{p}(1). \tag{7}\]

For any \(\xi_{n1},\ldots,\xi_{nn}\) (and hence for this realization also), for each fixed \(0<\nu<1\),

\[\triangle^{0}_{\nu/n}\equiv\left\|\frac{n^{\nu}(\mathbb{G}_{n}-I)}{[I\wedge(1- I)]^{1-\nu}}\right\|_{0}^{1}=O_{p}(1). \tag{8}\]

**Remark 1.1** We prove these theorems in such a way that the uniformity available is apparent. To see the full range of uniformity possible, consult the \(1st\) Edition of this text, where this topic is pursued much more completely. Random \(k_{n}\) and \(k^{\prime}_{n}\), (useful to applied statisticians) are also considered therein. \(\Box\)

Proofs.: Integration by parts (with \(\int_{(d,c]}\equiv-\int_{(c,d]}\) if \(c<d\) in (b), (c), etc. below) [a.s., \(dK\) puts no mass at any \(\xi_{n:}\)] yields

\[\tilde{Z}_{n}=\sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}= \sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}\] as in (1) \[\mbox{(a)} =\sqrt{n}\{\int_{[\xi_{n}k_{n}+1,\xi_{nn-k^{\prime}_{n}}]}Kd \mathbb{G}_{n}-\int_{(a_{n},1-a^{\prime}_{n})}K(t)dt\}/\tilde{\sigma}_{n}\] \[\mbox{(b)} =\sqrt{n}\{-K\mathbb{G}_{n}|_{\xi_{n:k_{n}+1}-}-\int_{[\xi_{n:k_{ n}+1},\xi_{n:n-k^{\prime}_{n}}]}\mathbb{G}_{n}dK+K\mathbb{G}_{n}|_{\xi_{n:n-k^{ \prime}_{n}}}\}/\tilde{\sigma}_{n}\] \[\mbox{(c)} =_{a.s.}\ -\int_{(a_{n},1-a^{\prime}_{n})}\mathbb{U}d\tilde{K}_{n}/ \tilde{\sigma}_{n}-\int_{(a_{n},1-a^{\prime}_{n})}[\mathbb{U}_{n}-\mathbb{U}]d \tilde{K}_{n}/\tilde{\sigma}_{n}\] \[\mbox{}-\int_{(\xi_{n:k_{n}+1},a_{n}]}\sqrt{n}[\mathbb{G}_{n}(t) -a_{n}]dK/\tilde{\sigma}_{n}\] \[\mbox{}-\int_{[1-a^{\prime}_{n},\xi_{n:n-k^{\prime}_{n}})}\sqrt{n }[\mathbb{G}_{n}(t)-(1-a^{\prime}_{n})]dK/\tilde{\sigma}_{n}\]

[MISSING_PAGE_EMPTY:9203]

Thus the CLT claim of (4) holds, provided only (adding the symmetric condition in the right tail)

\[\{\sqrt{a_{n}}|K_{+}(\lambda a_{n})|+\sqrt{a^{\prime}_{n}}|K((1-\lambda a^{\prime }_{n}\ \}/\tilde{\sigma}_{n}\to 0 \tag{12}\]

for each \(0<\lambda\leq 1\), and it holds uniformly in any class of qfs in which (12) holds uniformly. But (12) follows from (3) (or any of the equivalent conditions like (10.6.32), or (10.6.35), or (10.6.13), or (10.6.12), or (C.2.6), or (C.2.13), or (C.2.15)-for example.) Thus the normality in theorem 1.1 is established again--using a fundamentally different proof from that in section 10.6. [Note that (the crude upper bound) condition (12) implies (11), (g), and (h).]

Consider theorem 1.2. The first term in (d) converges to \(0\), and the other two terms that appear in (d) equal zero. Thus, again \(\gamma_{1n}\to_{p}0\) whenever \(k_{n}\wedge k^{\prime}_{n}\to\infty\)(but now, it converges _uniformly_ in _all_ qfs). In the present context the \(\gamma_{3n}\) term always equals \(0\). Thus, _only_\(\gamma_{2n}\) must be shown to be negligible; but we must now be much more careful than the crude upper bound (12). Now, using \((f)(ii)\) in the definition of \(1_{n\epsilon}\) in (f), we obtain

(i) \[|\gamma_{2n}|\times 1_{n\epsilon}\leq E_{n}\times|\sqrt{a_{n}}\int_{I_{ \lambda n}}dK/\tilde{\sigma}_{n}|,\]

where \(I_{\lambda n}\equiv(a_{n}(1-\lambda/\sqrt{k_{n}}),\ a_{n}(1+\lambda/\sqrt{k_{n }}))\). Thus the CLT claim of (6) holds, provided that (with symmetric requirements in the right tail)

\[\sqrt{a_{n}}\,|K(a_{n}(1\mp\lambda/\sqrt{k_{n}}))-K_{+}(a_{n})|/\tilde{\sigma} _{n}\equiv\sqrt{a_{n}}\int_{I_{\lambda n}^{\mp}}dK/\tilde{\sigma}_{n}\to 0 \tag{13}\]

for each \(\lambda>0\). Thus, normality holds uniformly in any class \({\cal K}_{u}\) in which both (13) and its right tail analogue hold uniformly; call such a class \({\cal K}_{u}\) a _uniformity class_.

**Summary** (so far) Whenever \(k_{n}\wedge k^{\prime}_{n}\to\infty\),

\[\sup\nolimits_{K\in{\cal K}_{u}}|\sqrt{n}[\tilde{X}_{n}(a_{n},\ a^{\prime}_{ n})-\check{\mu}_{n}]/\tilde{\sigma}_{n}-Z(\tilde{K}_{n})|\to_{p}0 \tag{14}\]

for any class \({\cal K}_{u}\) in which both (13) and its right tail analogue hold uniformly. (Also, we may replace \(\tilde{\sigma}_{n}\) by \(\tilde{S}_{n}\) in (14) under this same requirement (as was shown in the variance estimation proof given in the \(1st\) Edition)).

Now, (13) does hold for a fixed \(K\) whenever both \(K\) is in any \({\cal R}_{-\beta}\) and the trimming numbers of (5) are employed (appeal to theorem C.5.1), and this gives theorem 1.2. (Two uniformity classes \({\cal K}_{u}\) are exhibited in theorem C.5.2.) Aside from variance estimation, the proofs of theorems 1.1 and 1.2 are complete. The \(1st\) Edition carefully considers both variance estimation and the uniformity of the asymptotic normality, via making (12) hold uniformly. \(\Box\)

**Remark 1.2** The class \(\tilde{\cal D}\) of qfs \(K\) having \(\tilde{\sigma}^{2}(\cdot)\in{\cal R}_{-\beta}\) for some \(\beta\geq 0\) is strictly bigger than the stable laws; the class of stable laws also require that

\[v^{+}(t)/v^{-}(t)\to(\mbox{some }(1-p)/p)\in[0,1],\]

so that both the contributions from the two extreme tails \(\{\)(from \(1\) to \(k_{n}\)) and (from \(n-k^{\prime}_{n}+1\) to \(n\))\(\}\) can be balanced. (Recall the De Haan result in exercise C.4.2.) But we do not need to balance them; we threw them away. \(\Box\)

**Exercise 1.1** Verify that \(\sup\{\mathbb{U}_{n}(t)|/\sqrt{a_{n}}\ :\ ca_{n}\leq t\leq a_{n}/c\}=O_{p}(1)\).

**Exercise 1.2*** (Rossberg) Suppose \(0\leq\xi_{n:1}\leq\cdots\leq\xi_{n:n}\) are the order statistics of the first \(n\) of the infinite sequence of independent Uniform(0, 1) rvs \(\xi_{1},\xi_{2},\ldots\). Let \(\alpha_{n}\equiv\{j:1\leq j\leq k_{n}\},\delta_{n}\equiv\{j:k_{n}+1\leq j\leq l _{n}\},\beta_{n}\equiv\{j:l_{n}+1\leq j\leq n-l^{\prime}_{n}\},\delta^{\prime}_ {n}\equiv\{j:n-l^{\prime}_{n}+1\leq j\leq n-k^{\prime}_{n}\}\), and \(\alpha^{\prime}_{n}\equiv\{j:n-k^{\prime}_{n}+1\leq j\leq n\}\). Show that the collections of rvs \(\{\xi_{n:j}\ :\ j\in\alpha_{n}\},\{\xi_{n:j}\ :\ j\in\beta_{n}\}\), and \(\{\xi_{n:j}\ :\ j\in\alpha^{\prime}_{n}\}\) are asymptotically independent when \(k_{n}\nearrow\infty\) with \(a_{n}\equiv k_{n}/n\to 0\) and \(l_{n}\equiv k_{n}^{1+2\iota^{/}}\) for \(0<\nu<\frac{1}{4}\) (with analogous conditions in the right tail).

**Remark 1.3** Consider asymptotic normality of the sample mean. Let \(U_{n},V_{n},W_{n},V^{\prime}_{n}\), and \(U^{\prime}_{n}\) denote the sum of those \(X_{n:j}=K(\xi_{n:j})\) for which \(j\) is in \(\alpha_{n},\delta_{n},\beta_{n},\delta^{\prime}_{n}\), and \(\alpha^{\prime}_{n}\), respectively. The previous exercise shows that \(W_{n},U_{n}\), and \(U^{\prime}_{n}\) are asymptotically independent. Exercise C.6.1 shows that \(V_{n}\) and \(V^{\prime}_{n}\) are _always_ asymptotically negligible. We saw in appendix C that the condition \(\tilde{\sigma}^{2}(\cdot)\in{\cal L}\) is also necessary for (4). Since the vast middle is "nearly always" normal, one needs to determine what is happening only in the extreme tails (as the mid-tails were always negligible). This asymptotic independence is at the heart of the very general asymptotics for \(\bar{X}_{n}\) found in S. Csorgo, Haeusler, and Mason (1989). \(\Box\)

**Exercise 1.3** (Winsorized mean) Let \(\tilde{Z}_{n}\equiv\sqrt{n}(\tilde{X}_{n}-\tilde{\mu}_{n})/\tilde{\sigma}_{n}\) for the sample mean \(\tilde{X}_{n}\) of the Winsorized sample \(\tilde{X}_{n1},\ldots,\tilde{X}_{nn}\). For the \(N(0,\ 1)\) rv \(Z(\tilde{K}_{n})\) of (2), the identity (9) becomes

\[\tilde{Z}_{n}=Z(\tilde{K}_{n})+\gamma_{1n}+\tilde{\gamma}_{2n}+ \tilde{\gamma}^{\prime}_{2n},\] \[\mbox{where}\qquad\tilde{\gamma}_{2n}\equiv\sqrt{n}\int_{I_{n}} \mathbb{G}_{n}dK/\tilde{\sigma}_{n}\quad\mbox{and}\quad\tilde{\gamma}^{\prime} _{2n}\equiv\sqrt{n}\int_{I^{\prime}_{n}}\mathbb{G}_{n}dK/\tilde{\sigma}_{n}\]

and \(\gamma_{1n}\) is as before. Here \(I_{n}\) is equal to \([\xi_{n:k_{n}+1},\ a_{n}]\) or \((a_{n},\ \xi_{n:k_{n}+1})\) according as \(\xi_{n:k_{n}+1}<a_{n}\) or \(\xi_{n:k_{n}+1}\geq a_{n}\), and \(I^{\prime}_{n}\) is equal to \([1-a^{\prime}_{n},\ \xi_{n:n-k^{\prime}_{n}}]\) or \((\xi_{n:n-k^{\prime}_{n}},\ 1-a^{\prime}_{n})\) according as \(1-\xi_{n:n-k^{\prime}_{n}}<a^{\prime}_{n}\) or \(1-\xi_{n:n-k^{\prime}_{n}}\geq a^{\prime}_{n}\). Show that this quantity \(\tilde{\gamma}_{2n}\) for the Winsorized mean essentially exceeds the \(\gamma_{2n}\) of the trimmed mean proof by the factor \(\sqrt{k_{n}}\). This is just enough to prevent analogues of the theorems for trimmed means that we proved in this chapter.

(a) Prove what you can for the Winsorized mean.

(b) Graph a typical \(K\) on \((0,1)\). Locate \(a_{n}\) and \(\xi_{n:k_{n}+1}\) near the 0 endpoint (and suppose \(K(0)<0\)). Obtain the natural graphical upper bounds on the magnitudes of \(\gamma_{2n}\) and \(\tilde{\gamma}_{2n}\), and note how the second quantity is inherently larger than the first (pictorially, a "trapezoid" versus a "triangle").

**Exercise 1.4** (Uniform WLLN) Suppose the qf \(K_{0}(\cdot)\) is of order one, in that

\[t(|K_{0}(t)|+|K_{0}(1-t)|)\to 0\quad\mbox{as}\quad t\to 0. \tag{15}\]

Then one can claim the uniform WLLN

\[\sup_{K\in{\cal K}_{0}}\left|\bar{X}_{n}-\int_{1/n}^{1-1/n}K(t)dt\right|\to_{ p}0, \tag{16}\]

when \({\cal K}_{0}\equiv\{K:\,|K|\leq|K_{0}|\) on \((0,\ a_{0}]\cup[1-a_{0},1]\}\), for some \(0<a_{0}\leq\frac{1}{2}\).

## 2 Linear Rank Statistics and Finite Sampling

**Example 2.1** (Linear rank statistics): Consider the \(\mathbb{R}_{N}\) process of section 12.10, with the same notation and assumptions as were made there. Thus (for \(c_{Ni}\)'s with mean \(\bar{c}_{N}=0\), standard deviation \(\sigma_{cN}^{2}=1\), and standardized fourth central moment \(\overline{c_{N}^{4}}/\sigma_{cN}^{4}\leq M<\infty\) for all \(N\)) we define (for the antiranks \(\mathrm{D}\equiv(D_{N1},\ \ldots,\ D_{NN})\))

\[\mathbb{R}_{N}(t)\equiv\frac{1}{\sqrt{N}}\sum_{i=1}^{[(N+1)t]}\frac{c_{ND_{Ni }}-c_{N}}{\sigma_{cN}}=\frac{1}{\sqrt{N}}\sum_{i=1}^{[(N+1)t]}c_{ND_{Ni}}\quad \text{on $[0,1]$}, \tag{1}\]

and this process satisfies \(\breve{\Delta}_{\nu N}=O_{p}(1)\), as in (12.10.35). The known constants \(\mathrm{c}_{N}\equiv(c_{N1},\ldots,c_{NN})^{\prime}\) are called _regression constants_. Let \(\mathfrak{a}_{N}\equiv(a_{N1},\ldots,a_{NN})^{\prime}\) specify known _scores_. Let \(a_{N}.,\sigma_{aN}^{2}>0\), and \(\mu_{4}(\mathfrak{a}_{N})\) denote their mean, variance, and fourth central moment. The class of _simple linear rank statistics_ is defined by

\[T_{N}\equiv T_{N}(\mathfrak{a}_{N})\equiv\frac{1}{\sqrt{N}}\sum_{i=1}^{N}\frac {a_{Ni}-a_{N}}{\sigma_{aN}}c_{ND_{Ni}}. \tag{2}\]

Now, \(\mathrm{E}T_{N}=0\) and \(\mathrm{Var}[T_{N}]=\frac{N}{N-1}\) by exercise 2.1 below. Assume (for convenience only) that the scores are ordered as

\[a_{N1}\leq\cdots\leq a_{NN},\]

and we define an \(\nearrow\) left-continuous qf \(K_{N}\) on \([0,1]\) by

\[K_{N}(t)=a_{Ni}-a_{N}.\qquad\text{for}\quad(i-1)/N<t\leq i/N,\quad\text{and} \quad 1\leq i\leq N, \tag{3}\]

with \(K_{N}(0)\equiv a_{N1}\). Note that

\[T_{N}\equiv\int_{0}^{1}K_{N}d\,\mathbb{R}_{N}/\sigma_{aN}=-\int_{0}^{1}\mathbb{ R}_{N}dK_{N}/\sigma_{aN}. \tag{4}\]

The basic probability space and rvs are as defined in notation 15.1.3 and in (12.10.35). Let

\[Z_{N}\equiv Z(\mathfrak{a}_{N})\equiv-\int_{0}^{1}\mathbb{W}dK_{N}/\sigma_{aN}. \tag{5}\]

Clearly, \(Z_{N}\) is a normal rv with mean \(0\), since it is just a linear combination of the jointly normal rvs \(\mathbb{W}(1/N),\ldots,\mathbb{W}(1-1/N)\). Fubini's theorem gives

\[\mathrm{Var}\left[\mathrm{Z_{N}}\right]=\mathrm{Var}\left[-\int_{ 0}^{1}\mathbb{W}\mathrm{dK_{N}}/\sigma_{\mathrm{aN}}\right]=\int_{0}^{1}\int_{ 0}^{1}\left[\mathrm{s}\wedge\mathrm{t}-\mathrm{st}\right]\,\mathrm{dK_{N}(s) \,\mathrm{dK_{N}(t)}}/\sigma_{\mathrm{aN}}^{2} \tag{6}\] \[=\mathrm{Var}[K_{N}(\xi)]/\sigma_{aN}^{2}=1.\]

We will also consider the sum of independent rvs given by

\[T_{N}^{0}\equiv T_{N}^{0}(\mathfrak{a}_{N})\equiv\tfrac{-1}{ \sigma_{aN}}\int_{0}^{1}\mathbb{W}dK_{N}=\tfrac{1}{\sigma_{aN}}\int_{0}^{1}K_{ N}\,d\mathbb{W}_{N}\] \[=\tfrac{1}{\sqrt{N}}\sum_{i=1}^{n}[c_{Ni}K_{N}(\xi_{Ni})]/\sigma_ {aN}\cong(0,1). \tag{7}\]

We next show that \(T_{N}=a_{N}\,T_{N}^{0}=a_{N}\,Z_{N}\cong N(0,1)\) under rather mild conditions.

**Exercise 2.1**  Show that \({\rm E}T_{N}=0\) and \({\rm Var}[T_{N}]=N/(N-1)\).

**Definition 2.1** (\({\cal D}({\tt a}_{N})\)-negligibility): Call such \({\tt a}_{N}\) a _negligible_ array if

\[{\cal D}({\tt a}_{N})\equiv\frac{\max_{1\leq i\leq N}|a_{Ni}-a_{N \cdot}|}{\sqrt{N}\sigma_{aN}}\leq\quad({\rm some}\ \epsilon_{N})\searrow 0. \tag{8}\]

We let \({\cal A}\) denote any collection of such arrays that uses the same \(\epsilon_{N}\)'s. When the \(a_{Ni}\) are random, we call them

\[p\mbox{-}negligible\ \ {\rm if}\ \ {\cal D}({\tt a}_{N})\to_{p}0\quad{\rm and} \quad a.s.\mbox{-}negligible\ \ {\rm if}\ \ {\cal D}({\tt a}_{N})\to_{{\rm a.s.}}0.\]

**Theorem 2.1** (Uniform CLT for linear rank statistics): Suppose that the regression constants satisfy \(\overline{c_{N}^{4}}\leq M<\infty\) for all \(N\). Let \({\cal A}\) be a collection of uniformly negligible arrays, in that \(\epsilon_{N}\equiv\sup_{A}{\cal D}({\tt a}_{N})\to 0\). Then

\[\sup_{A}|T_{N}({\tt a}_{N})-Z({\tt a}_{N})|\to_{p}0\quad{\rm and}\quad\sup_{A} |T_{N}^{0}({\tt a}_{N})-Z({\tt a}_{N})|\to_{p}0. \tag{9}\]

**Proof.**  Fix \(0\leq\nu<\frac{1}{4}\). Now, \(\gamma({\tt a}_{N})\equiv T_{N}-Z_{N}\) satisfies

(a) \[|\gamma({\tt a}N)|=|T_{N}-Z_{N}|=|-\int_{1/(N+1)}^{N/(N+1)}({\mathbb{ R}}_{N}-{\mathbb{W}})dK_{N}/\sigma_{aN}|\] (b) \[\leq\left\|\frac{N^{\nu}({\mathbb{R}}_{N}-{\mathbb{W}})}{[I(1-I)]^{1/2- \nu}}\right\|_{1/(N+1)}^{N/(N+1)}\times\int_{1/(N+1)}^{N/(N+1)}N^{-\nu}[t(1-t) ]^{1/2-\nu}dK_{N}(t)/\sigma_{aN}\] (10) \[\equiv\tilde{\triangle}_{\nu N}\times M_{\nu N}(\tilde{K}_{N}),\]

where \(\tilde{\triangle}_{\nu N}=O_{p}(1)\) by (12.10.35). Thus (C.6.3) gives

\[|T_{N}-Z_{N}|\leq\tilde{\triangle}_{\nu N}\] \[\times\ \left\{\frac{\sqrt{9/\nu}}{r^{\nu}}+\sqrt{r}\frac{|K_{N}(1/ (N+1))-a_{N\cdot}|}{\sqrt{N}\sigma_{aN}}+\sqrt{r}\frac{|K_{N}(N/(N+1))-a_{N \cdot}|}{\sqrt{N}\sigma_{aN}}\right\}\] (c) \[\leq\tilde{\triangle}_{\nu N}\times\{\sqrt{9/\nu}/r^{\nu}+2\sqrt{r}[ \max_{1\leq i\leq N}|a_{Ni}-a_{N\cdot}|/\sqrt{N}\sigma_{aN}]\}\]

\[\leq\tilde{\triangle}_{\nu N}\times\{\sqrt{9/\nu}/r^{\nu}+2\sqrt{r}{\cal D}({ \tt a}_{N})\}. \tag{11}\]

By first choosing \(r\) large and then letting \(N\to\infty\), we see that

(d) \[|T_{N}-Z_{N}|=O_{p}(1)\times o(1)\to_{p}0.\]

Note that we have separated the randomness from properties of the \(a_{Ni}\)'s, so that the convergence is uniform over \({\cal A}\). [The rate for \(T_{N}\) in (9) depends on the sequence \(\epsilon_{N}\to 0\), and on the \(c_{N^{i}}\)'s only through the \(M_{\epsilon}\)'s in the statement \(P(\hat{\triangle}_{\nu N}\geq M_{\epsilon})\leq\epsilon\) for all \(N\).] Since (see (12.10.34))

(e) \[|T_{N}^{0}-Z_{N}|=|\int_{1/(N+1)}^{N/(N+1)}({\mathbb{W}}_{N}-{\mathbb{W}})dK_{ N}/\sigma_{aN}|\leq\hat{\triangle}_{\nu N}\times M_{\nu N}(\tilde{K}_{N}),\]

comparing (e) with (a) and (10) shows that the proof is the same in this case.

[One can also allow random regression constants \({\tt c}_{N}\) and scores \({\tt a}_{N}\) that are independent of the antiranks for which \(\overline{c_{N}^{4}}=O_{p}(1)\) and \(\sup_{A}{\cal D}({\tt a}_{N})\to_{p}0\).]

**Example 2.2** (Creating normality): The following are known, and intuitive.

**(A)**_(Using normal regression constants)_ When \(\overline{\lim}\,\overline{c_{N}^{4}}\leq\infty\), present methods give

\[T_{N}\to_{d}N(0,1)\quad\mbox{if and only if either }{\cal D}({\tt a}_{N})\to 0 \mbox{ or }c_{ND_{N1}}\to_{d}N(0,1). \tag{12}\]

**Result 1** Thus, with absolutely no hypotheses,

the choice

\[c_{Ni}\equiv\Phi^{-1}(i/(N+1))\]

\[always \tag{13}\]

gives

\[T_{N}\to_{d}N(0,1)\]

 for

\[\it every\]

choice of

\[a_{Ni}\]

's.

(B) _(Winsorizing a finite sample)_ Let \(\tilde{a}_{N}.,\tilde{\sigma}_{aN},\tilde{T}_{N}\), and \(\tilde{Z}_{N}\) be defined as before, but now based on the \((k_{N},\ k_{N}^{\prime})\)-_Winsorized population_\(\tilde{\tilde{a}}_{N}\) consisting of

\[a_{N,k_{N}+1},\ldots,a_{N,k_{N}+1};a_{N,k_{N}+1},\ldots,a_{N,N-k_{N}^{\prime}} ;a_{N,N-k_{N}^{\prime}},\ldots,a_{N,N-k_{N}^{\prime}}. \tag{14}\]

Of course, theorem 2.1 also applies to \(\tilde{T}_{N}\). But note that now

\[{\cal D}(\tilde{\tt a}_{N})=\max_{1\leq i\leq N}|\tilde{a}_{Ni}- \tilde{a}_{N}.|/\sqrt{N}\tilde{\sigma}_{aN}\] \[\quad=[|a_{N,k_{N}+1}-\tilde{a}_{N}.|\vee|a_{N,N-k_{N}^{\prime}}- \tilde{a}_{N}.|]/(\sqrt{N}\tilde{\sigma}_{aN})\] \[\quad\leq\frac{|a_{N,k_{N}+1}-\tilde{a}_{N}.|\vee|a_{N,N-k_{N}^{ \prime}}-\tilde{a}_{N}.|}{\{(k_{N}+1)(a_{N,k_{N}+1}-\tilde{a}_{N}.)^{2}+(k_{N}^ {\prime}+1)(a_{N,N-k_{N}^{\prime}}-\tilde{a}_{N}.)^{2}\}^{1/2}} \tag{15}\] \[\quad\leq 1/\sqrt{(k_{N}\wedge k_{N}^{\prime})+1},\quad\mbox{ provided only that }a_{N,k_{N}+1}<a_{N,N-k_{N}^{\prime}}.\]

Thus \({\cal D}(\tilde{\tt a}_{N})\to 0\) whenever

\[k_{N}\wedge k_{N}^{\prime}\to\infty,\quad\mbox{and}\quad a_{N,k_{N}+1}<a_{N,N- k_{N}^{\prime}}\quad\mbox{for all $N$ sufficiently large.} \tag{16}\]

**Result 2** Suppose \(\overline{c_{N}^{4}}\leq M<\infty\) for all \(N\). For fixed \(k_{N}\wedge k_{N}^{\prime}\to\infty\), we have

\[\sup_{A}\{|\tilde{T}_{N}({\tt a}_{N})-\tilde{Z}({\tt a}_{N})|:\mbox{ all arrays in }{\cal A}\mbox{ with }a_{N,k_{N}+1}<a_{N,N-k_{N}^{\prime}}\}\to_{p}0. \tag{17}\]

**Summary** Asymptotic normality is guaranteed by Winsorizing a number that slowly increases to infinity, provided only that we do not collapse the whole sample. \(\square\)

**Exercise 2.2**: Argue heuristically why (13) should be true. [See Shorack(1996).]

**Example 2.3** (Permutation statistics): Suppose \(X_{1},\ldots,X_{N}\) are iid rvs with nondegenerate df \(F\) on \((\Omega,{\cal A},P)\). Then let \({\tt X}_{N}\equiv(X_{1},\ \ldots,\ X_{N})^{\prime}\) denote the full population of observed values, having order statistics \(X_{N:1}\leq\cdots\leq X_{N:N}\), antiranks \((D_{N1},\ \ldots,\ D_{NN})\), sample mean \(\tilde{X}_{N}\), sample variance \(S_{N}^{2}\), empirical df \(\mathbb{F}_{N}\), and empirical qf \(\mathbb{K}_{N}\equiv\mathbb{F}_{N}^{-1}\). Let \(0\leq k_{N}<N-k_{N}^{\prime}\leq N\), and let \(\tilde{X}_{N}\) denote the \((k_{N},\ k_{N}^{\prime})\)-Winsorized population \(\tilde{X}_{N:1}\leq\cdots\leq\tilde{X}_{N:N}\) (as in (14)) whose parameters are the Winsorized mean \(\tilde{T}_{N}\), the Winsorized variance \(\tilde{S}_{N}^{2}\), and empirical qf \(\tilde{\mathbb{K}}_{N}\). We note that

\[\begin{array}{l}X_{N:1}\leq\cdots\leq X_{N:N}\quad\mbox{and}\quad(D_{N1},\ \ldots,\ D_{NN})\,\mbox{are independent rvs,}\\ \mbox{if tied}\,X_{i}\,\mbox{'s are randomly assigned their ranks.}\end{array} \tag{18}\]We also recall (from theorem 10.7.1, or from exercise 8.4.20) that

\[\mathcal{D}(\mathsf{X}_{N})\to_{a.s.}0\quad\text{if and only if }0<\operatorname{Var}[X]<\infty. \tag{20}\] \[\mathcal{D}(\mathsf{X}_{N})\to_{p}0\qquad\text{if and only if }F\in D(\text{Norma1}). \tag{19}\]

Moreover, from (16),

\[\mathcal{D}(\tilde{\mathsf{X}}_{N})\to_{a.s.}0\qquad\text{[making }\tilde{\mathsf{X}}_{N}\text{ a.s. negligible] for a.e. }X_{1},X_{2},\ldots, \tag{21}\]

provided only that

\[k_{N}\wedge k^{\prime}_{N}\to_{a.s.}\infty\quad\text{and}\quad\varliminf(X_{N:N -k^{\prime}_{N}}-X_{N:k_{N}+1})>0\text{ a.s.} \tag{22}\]

for \(k_{N}\) and \(k^{\prime}_{N}\) that are either fixed integer sequences or integer-valued rvs that are independent of the antiranks \(\mathsf{D}_{N}\). Condition (22) _necessarily holds_ if

\[F\text{ is }any\text{ nondegenerate }df,\text{ and if }k_{N}\wedge k^{\prime}_{N}\to\infty\text{ while }(k_{N}\lor k^{\prime}_{N})/N\to 0. \tag{23}\]

[We shall maintain the order statistics (which are on \((\Omega,\mathcal{A},P)\)), but we can replace the independent antiranks by a realization (on some \((\Omega^{*},\mathcal{A}^{*},P^{*})\) independent of \((\Omega,\mathcal{A},P)\)) for which \(\tilde{\triangle}^{*}_{\nu N}=O_{p}(1)\) on \((\Omega^{*},\ \mathcal{A}^{*},\ P^{*})\) (from (12.10.25)) for some Brownian bridge \(\mathbb{W}\). This is possible whenever \(\overline{c_{N}^{4}}\leq M<\infty\) for all \(N\) (see theorem 12.10.3).]

By a _permutation statistic_ we mean a rv of the form

\[T_{N}\equiv T_{N}(\mathsf{X}_{N})\equiv\frac{1}{\sqrt{N}}\sum_{i=1}^{N}c_{Ni} X_{i}/S_{N}=\frac{1}{\sqrt{N}}\sum_{i=1}^{N}c_{ND_{Ni}}X_{N:i}/S_{N}, \tag{24}\]

with \(S_{N}^{2}=\sigma_{XN}^{2}=\sum_{1}^{N}(X_{k}-\bar{X}_{N})^{2}/N\) and with a \(\mathsf{c}_{N}\equiv(c_{N1},\ \ldots,\ c_{NN})^{\prime}\) population that is standardized. (Note that the distribution of \(T_{N}\) is unaltered by using this different realization of the antiranks.) 

**Theorem 2.2** (Permutation tests): If \(\tilde{\triangle}^{*}_{\nu N}=O_{p}(1)\) (as when \(\varlimsup\overline{c_{N}^{4}}<\infty\)), then the asymptotic normality

\[T_{N}=_{a}Z(\mathsf{X}_{N})\cong N(0,1)\] \[\text{holds }\left\{\begin{array}{ll}\text{on }(\Omega^{*}, \mathcal{A}^{*},P^{*})&\text{for a.e. }X_{1},\ X_{2},\ \ldots&\text{if }c_{Ni}=\Phi^{-1}(i/N+1)\,\\ \text{on }(\Omega^{*},\mathcal{A}^{*},P^{*})&\text{for a.e. }X_{1},\ X_{2},\ \ldots&\text{if }0<\operatorname{Var}[X]<\infty,\\ \text{on }(\Omega\times\Omega^{*},\mathcal{A}\times\mathcal{A}^{*},P\times P^{*})& \text{if }F\in D\ (\text{Normal}).\end{array}\right. \tag{25}\]

[The convergence is uniform over classes \(\mathcal{F}\) of dfs \(F\) in which \(\mathcal{D}(\mathsf{X}_{N})\to_{p}\) or \({}_{a.s.}0\) uniformly.] Also, whenever (22) (or (23)) holds a.e. we have

\[\tilde{T}_{N}=_{a}Z(\tilde{\mathsf{X}}_{N})\cong N(0,1)\qquad\text{on }(\Omega^{*},\mathcal{A}^{*},P^{*})\text{ for a.e. }X_{1},X_{2},\ldots \tag{26}\]

Similar results hold for \(T_{N}(\mathsf{Y}_{N})\) and \(\tilde{T}_{N}(\mathsf{Y}_{N})\), where \(\mathsf{Y}_{N}\equiv(Y_{N1},\ \ldots,\ Y_{NN})^{\prime}\) with

\[Y_{Ni}\equiv\hat{g}_{N}(X_{i})\quad\text{for any function }\hat{g}_{N}(\cdot)\text{ independent of the antiranks }\mathsf{D}_{N}. \tag{27}\]

**Proof.** Equation (11) now becomes (recall (10) for \(M_{\nu N}(\cdot)\))

(a) \[|\gamma({\sf X}_{N})|\equiv|T_{N}-Z({\sf X}_{N})|=|-\int_{0}^{1}( {\mathbb{R}}_{N}-{\mathbb{W}})d{\mathbb{K}}_{N}/S_{N}|\leq\breve{\triangle}_{ \nu N}^{*}\times M_{\nu N}({\mathbb{K}}_{N})\] (28) \[\leq\breve{\triangle}_{\nu N}^{*}\times\{\sqrt{9/\nu}/r^{\nu}+2 \sqrt{r}{\cal D}({\sf X}_{N})\}\] (b) \[=O_{p}(1)\times o(1)\to_{p}0,\quad\mbox{as in (\ref{eq:Z1})},\]

using either (19), (20) on subsequences, or the proof of (13) on \({\cal D}({\sf X}_{N})\). So, (25) holds. Likewise, conclusion (26) holds by using (22) to apply (15) to \({\cal D}(\breve{\sf X}_{N})\). \(\Box\)

**Sampling from Finite Populations**

**Example 2.4** (Simple random sampling): Let \(X_{1},\ldots,X_{n}\) be a random sample without replacement from an \({\sf a}_{N}\equiv(a_{N1},\ \ldots,\ a_{NN})^{\prime}\) population. As usual, let \(\bar{X}_{n}\) and \(S_{n}^{2}\equiv\overline{X_{n}^{2}}-\overline{X_{n}^{2}}\) denote the sample mean and "sample variance." Suppose that \(a_{N1}\leq\cdots\leq a_{NN}\), that \(n\equiv n_{N}\), and that the \({\cal D}({\sf a}_{N})\) of (8) satisfy both

\[0<\underline{\lim}\,n/N\leq\overline{\lim}\,n/N<1\quad\mbox{and} \tag{30}\] \[\sup_{A}{\cal D}({\sf a}_{N})=\sup_{A}\{\max_{1\leq i\leq N}|a_{Ni }-a_{N}.|/\sqrt{N}\sigma_{aN}\}\to 0. \tag{29}\]

Prior to normalizing, the \(c_{Ni}\)'s consist of \(n\) values of \(1\) and \(m\equiv N-n\) values of \(0\), with \(c_{N}.=-n/N\) and \(\sigma_{cN}^{2}=mn/N^{2}\). After normalizing,

\[\overline{c_{N}^{4}}=(m^{3}+n^{3})/(mnN)\leq 2(m\lor n)/(m\wedge n).\]

Thus (29) implies that all \(\overline{c_{N}^{4}}\leq\) (some \(M\)) \(<\infty\). Since \(c_{ND_{N1}}\to_{d}N(0,1)\) clearly fails, (12) shows that \(T_{N}\to_{d}N(0,1)\) if and only if \({\cal D}({\sf a}_{N})\to 0\). The limiting rv (as in (5)) will be \(Z_{N}\equiv Z({\sf a}_{N})\equiv-\int_{0}^{1}{\mathbb{W}}dK_{N}/\sigma_{aN} \cong N(0,1)\). Now define

\[T_{N}\equiv T_{N}({\sf a}_{N})\equiv\frac{\sqrt{n}(\bar{X}_{n}-a_{N}.)}{\sigma _{aN}\sqrt{1-n/N}}=-\int_{0}^{1}{\mathbb{R}}_{N}dK_{N}/\sigma_{aN}\cong(0,\ N/ (N-1)), \tag{31}\]

\[\hat{T}_{N}\equiv\hat{T}_{N}({\sf a}_{N})\equiv\frac{\sqrt{n}(\bar{X}_{n}-a_{N }.)}{S_{n}\sqrt{1-n/N}}.\hskip 14.226378pt\Box \tag{32}\]

**Theorem 2.3** (Simple random sampling): Suppose (29) holds, and suppose the arrays \({\cal A}\) are uniformly negligible with \(\sup_{A}{\cal D}({\sf a}_{N})\to 0\) (as in (30)). Then

\[\sup_{A}|T_{N}({\sf a}_{N})-Z({\sf a}_{N})|\to_{p}0, \tag{33}\]

\[\sup_{A}|\frac{1}{\sigma_{aN}}S_{n}-1|\to_{p}0, \tag{34}\]

\[\hat{T}_{N}({\sf a}_{N})-Z({\sf a}_{N})\to_{p}0.\quad\mbox{In fact,}\qquad \sup_{A}|\hat{T}_{N}({\sf a}_{N})-Z({\sf a}_{N})|\to_{p}0 \tag{35}\]

if (36) also holds. That is, the uniform convergence conclusion in (35) holds if

\[\sup_{N}\int_{1/(N+1)}^{N/(N+1)}g(t)d\frac{K_{N}(t)-a_{N}}{\sigma_{aN}}<\infty \tag{36}\]

for \(g(t)=b(t)[t\wedge(1-t)]^{1/2}\) and \(b(t)=b(1-t)=1\vee[2\log_{2}1/t]^{1/2}\) for \(t\in[0,1/2]\).

**Exercise 2.3** (a) Show that (30) and (36) both hold whenever the qfs \(K_{N}(\cdot)\) have a uniformly bounded \(2+\delta\) moment for any \(\delta>0\).

(b) Devise a "logarithmic moment" that will suffice.

**Proof.** Now, (33) follows from (9). Consider (34). Let \(d_{N}\equiv\sqrt{m/nN}\). Simple algebra (start on the right) gives

(a) \[\frac{S_{n}^{2}-\sigma_{aN}^{2}}{\sigma_{aN}^{2}}=\left\{\frac{\frac{1}{n}\sum_ {1}^{n}(X_{i}-a_{N}.)^{2}-\sigma_{aN}^{2}}{\sigma_{aN}^{2}}\right\}-\left\{ \frac{\bar{X}_{n}-a_{N}.}{\sigma_{aN}}\right\}^{2}\equiv I_{2n}-I_{1n}^{2}.\]

Using Chebyshev's inequality with the finite sampling variance of (A.1.9) yields

(b) \[P(|I_{1n}|\geq\epsilon)\leq\mbox{Var}[\bar{X}_{n}]/\epsilon^{2}\sigma_{aN}^{2} =[1-\tfrac{n-1}{N-1}]/n\epsilon^{2}\to 0.\]

Letting \(Y_{i}\equiv(X_{i}-a_{N}.)^{2}\), we use (A. 1.9) again for

(c) \[P(|I_{2n}|\geq\epsilon)=P(|\bar{Y}_{n}-\mu_{Y}|\geq\epsilon\sigma_ {aN}^{2})\leq\mbox{Var}[\bar{Y}_{n}]/\epsilon^{2}\sigma_{aN}^{4}\] (d) \[\leq\frac{\sigma_{Y}^{2}}{n\epsilon^{2}\sigma_{aN}^{4}}[1-\frac{n-1 }{N-1}]\leq\frac{\mbox{E}(X-a_{N}.)^{4}}{n\epsilon^{2}\sigma_{aN}^{4}}\frac{m}{ N-1}\] \[=\frac{\sum_{1}^{N}(X_{i}-a_{N}.)^{4}/N}{n\epsilon^{2}\sigma_{aN}^ {4}}\frac{m}{N-1}\leq{\cal D}^{2}\ (\mbox{a}N)\frac{\sigma_{aN}^{2}}{ \sigma_{aN}^{2}}\frac{2m}{\epsilon^{2}n}\] (e) \[\leq{\cal D}^{2}(\mbox{a}_{N})\frac{2m}{\epsilon^{2}n}\to 0\mbox{ by (\ref{eq:d1}).}\]

Thus (34) holds. Then (33) and (34) gives the first claim in (35) via Slutsky's theorem. (Note that (29) uniformly bounds the ratio \(\sqrt{m/n}.)\) The second claim made in (35) will now follow from the identity

(f) \[(\hat{T}_{N}-Z_{N})=(T_{N}-Z_{N})\times\{[(\sigma_{aN}/S_{n})-1]+1\}+Z_{N} \times[(\sigma_{aN}/S_{n})-1],\]

provided we show that

\[\sup_{N}|Z(\mbox{a}_{N})|=O_{p}(1). \tag{37}\]

The proof that (36) implies (37) is found in the \(1st\) Edition. \(\Box\)

**Remark 2.1** At this point in the \(1st\) Edition, the next section was used to rederive the bootstrap results of Chapter 10 using the present methods instead. \(\Box\)

## 3 L-Statistics \({}^{*}\)

Let \(K\equiv F^{-1}\), and define \(X_{ni}\equiv K(\xi_{ni})\), for \(1\leq i\leq n\), in terms of the Uniform \((0,1)\) rvs \(\xi_{n1},\ldots,\xi_{nn}\) of notation 15.1.3. Then \(X_{n1},\ldots,X_{nn}\) are iid \(F\), and we let \(X_{n:1}\leq\cdots\leq X_{n:n}\) denote the order statistics. Suppose the statistician specifies a known \(\nearrow\) and left-continuous function \(h\), known constants \(c_{n1},\ldots,c_{nn}\), and known integers \(0\leq k_{n}<n-k^{\prime}_{n}\leq n\). We wish to establish the asymptotic normality of the _trimmed_ L-_statistic_

\[L_{n}\equiv L_{n}(k_{n},\ k^{\prime}_{n})\equiv\frac{1}{n}\sum_{i=k_{n}+1}^{n- k^{\prime}_{n}}c_{ni}\,h(X_{n:i})=\frac{1}{n}\sum_{i=k_{n}+1}^{n-k^{\prime}_{ n}}c_{ni}\,H(\xi_{n:i}), \tag{1}\]

where \(H\equiv h(F^{-1})=h(K)\) is also \(\nearrow\) and left continuous. [Other useful cases such as \(h(x)=x^{2}\) are dealt with by considering \((H^{-})^{2}\) and \((H^{+})^{2}\) separately, and then adding the results. Here \(H^{-}\equiv-H\cdot{\bf 1}_{[H\leq 0]}\) and \(H^{+}\equiv H\cdot{\bf 1}_{[H\geq 0]}\) denote the negative and positive parts of \(H\). Thus there is no theoretical loss in now assuming that \(h(X)=X\) and \(H=F^{-1}\).]

Now, \({\mathbb{G}}_{n}\) and \({\mathbb{U}}_{n}\) denote the empirical df and the empirical process of those specially constructed \(\xi_{ni}\)'s of notation 15.1.2, whose empirical process \({\mathbb{U}}_{n}\) converges pathwise to the Brownian bridge \({\mathbb{U}}\) in the manner described. This will figure heavily in our proofs and in \(=_{a}\) claims, but not in any \(\to_{d}\) claims.

We need a centering constant \(\mu_{n}\) for \(L_{n}\). We define

\[J_{n}(t)=c_{ni}\qquad\mbox{for $(i-1)/n<t<i/n$}\quad\mbox{and}\quad 1\leq i \leq n, \tag{2}\]

where the value of \(J_{n}\) at the \(i/n\) points is totally inconsequential. Suppose that \(J_{n}\) "converges" to \(J\) in some sense. Define \(a_{n}\equiv k_{n}/n,\ a^{\prime}_{n}\equiv k^{\prime}_{n}/n\) as before, and then define centering constants

\[(\mu_{n}\equiv\int_{a_{n}}^{1-a_{\bar{a}}}J_{n}(t)H(t)dt\quad\mbox{and}\quad \mu_{n}^{0}\equiv\int_{a_{n}}^{1-a_{\bar{a}}}J(t)H(t)dt \tag{3}\]

with \(\mu^{0}\equiv\mu_{n}^{0}(0,0)=\int_{0}^{1}J(t)H(t)dt\). Note that \(\mu_{n}=\sum_{i=k_{n}+1}^{n-k^{\prime}_{n}}c_{ni}\int_{(i-1)/n}^{i/n}H(t)dt\), which means that \(k_{n}=0\) and \(c_{n1}>0\) (that \(k^{\prime}_{n}=0\) and \(c_{nn}>0\)) entails the added requirement that E\(H^{-}(\xi)\) be finite (that E\(H^{+}(\xi)\) be finite), for \(\xi\cong\)Uniform \((0,1)\). [Our main interest is in \(\mu_{n}\), while \(\mu_{n}^{0}\) is secondary; \(\mu_{n}\) is _the_ data analysis constant, while \(\mu_{n}^{0}\) is just a constant for theory.]

It is convenient to assume that on \((0,1)\)

\[J_{n}\geq 0,\quad J\geq 0\quad\mbox{is continuous},\quad\mbox{and $H$ is }\ \nearrow\ \mbox{and left continuous}. \tag{4}\]

[More generally, we can apply our results for two separate \(J\) functions and then just subtract the results.] Now specify \(a.\in(0,1)\) to satisfy \(H_{+}(a.)=0\), and define

\[K(t)\equiv\int_{(a,t)}J(s)dH(s), \tag{5}\]

where \(\int_{(a,t)}\equiv-\int_{[t,a]}\).(But set \(a.\ =0\) if \(H(\cdot)\ \geq 0,\ \mbox{and use}\ \int_{[0,t)}\) in (5) ; and set \(a.=1\) if \(H(\cdot)\leq 0\), and use \(\int_{[t,1]}\) in (5).) Thus (in case (5))

\[K\ \mbox{is}\ \nearrow\ \mbox{and left continuous on $(0,1)$ with $K_{+}(a.)=0$}, \tag{6}\]

where \(K+\mbox{is the right-continuous version, and $\triangle K\equiv K_{+}-K$}\). [Since \(L_{n}-\mu_{n}\) is invariant under vertical shift, there is actually no _theoretical_ loss in also assuming as we did above that \(H\) satisfies \(H_{+}(a.)=0.\)] Since \(K\) is a qf, the _unobservable_ rvs

\[Y_{ni}\equiv K(\xi_{ni}),\ \mbox{for $1\leq i\leq n$},\ \mbox{are iid with qf $K$},\ \mbox{and let}\ \bar{Y}_{n}\equiv\frac{1}{n}\sum_{i=1}^{n}Y_{ni}. \tag{7}\]The most historically important case obtains when

\[\sigma^{2}\equiv\mbox{Var}[K(\xi)]\in(0,\ \infty),\quad\mu\equiv\mbox{E}K(\xi), \quad\mbox{and}\quad k_{n}=k^{\prime}_{n}=0. \tag{8}\]

In this case we would _desire to show_ that (on the same probability space where the special \(\xi_{ni}\)'s above are defined) for some \(N(0,1)\) rv that we will denote by \(Z_{K}\) (or alternatively, and suggestively, we will also denote by \(\int_{0}^{1}Kd\mathbb{U}/\sigma\)) we have

\[\sqrt{n}[L_{n}(0,0)-\mu_{n}(0,0)]/\sigma=_{a}\sqrt{n}(\tilde{Y}_{n}-\mu)/ \sigma=_{a}Z_{K}\equiv\int_{0}^{1}Kd\mathbb{U}/\sigma\cong N(0,1). \tag{9}\]

We would also like a consistent estimator of \(\sigma\), and we might want to be able to replace \(\mu_{n}(0,0)\) by \(\mu^{0}=\int_{0}^{1}J(t)H(t)dt\). Of course, \(J_{n}\) will have to approximate \(J\) sufficiently closely.

Let \(\tilde{K}_{n}\) denote \(K\) Winsorized outside \((a_{n},\ 1-a^{\prime}_{n})\), and define the _unobservable_ Winsorized rvs

\[\tilde{Y}_{ni}\equiv\tilde{K}_{n}(\xi_{ni})\quad\mbox{for $1\leq i\leq n$}. \tag{10}\]

Then \(\tilde{Y}_{n1},\ldots,\tilde{Y}_{nn}\) are iid with qf \(\tilde{K}_{n}\) and mean \(\tilde{\mu}_{n}\) and variance \(\tilde{\sigma}_{n}^{2}\) given by

\[\tilde{\mu}_{n}\equiv\mbox{E}\tilde{Y}_{ni}=\int_{0}^{1}\tilde{K}_{n}(t)dt \quad\mbox{and}\quad\tilde{\sigma}_{n}^{2}\equiv\mbox{Var}[\tilde{Y}_{ni}]= \mbox{Var}[\tilde{K}_{n}(\xi)]. \tag{11}\]

(We can allow only \(k_{n}=0\) or \(k^{\prime}_{n}=0\) if the variance double integral is finite.) Let

\[\tilde{Y}_{n}.\ \equiv\tilde{Y}_{n}(a_{n},\ a^{\prime}_{n})\equiv\frac{1}{n} \sum_{i=1}^{n}\tilde{Y}_{ni}=\int_{0}^{1}\tilde{K}_{n}d\mathbb{G}_{n}. \tag{12}\]

In this case it is our _desire to show_ that

\[\sqrt{n}(L_{n}-\mu_{n})/\tilde{\sigma}_{n}=_{a}\sqrt{n}(\tilde{Y}_ {n}.\ -\tilde{\mu}_{n})/\tilde{\sigma}_{n}=\int_{0}^{1}\tilde{K}_{n}d\mathbb{ U}_{n}/\tilde{\sigma}_{n}=-\int_{0}^{1}\mathbb{U}_{n}d\tilde{K}_{n}/\tilde{ \sigma}_{n} \tag{14}\] \[=_{a}Z_{K}(a_{n},\ a^{\prime}_{n})\equiv\int_{0}^{1}\tilde{K}_{n} d\mathbb{U}/\tilde{\sigma}_{n}\cong N(0,1). \tag{13}\]

We also seek an appropriate estimator of \(\tilde{\sigma}_{n}\), and we may want to be able to replace \(\mu_{n}\) by \(\mu_{n}^{0}\). Whenever \(k_{n}\wedge k^{\prime}_{n}\geq 1\), we always define the symbol \(\int_{0}^{1}\tilde{K}_{n}d\mathbb{U}\) to mean \(-\int_{0}^{1}\mathbb{U}d\tilde{K}_{n}\)(that is, pathwise integration for each \(\omega\)) for _all_ qfs.

Make _throughout_ without further comment the rather modest assumptions that \(a_{n}\) and \(a^{\prime}_{n}\) satisfy \(\liminf(1-a_{n}-a^{\prime}_{n})>0\) and \(\liminf\tilde{\sigma}_{n}=\liminf\sigma_{K}(a_{n},\ a^{\prime}_{n})>0\) for the df \(F\) or \(F_{0}\) under consideration at the particular moment. The first says that we will deal with averaging, rather than just "quantiles" (though we could easily have added in fixed quantiles had we chosen to do so). The second says that the statistician has at least enough insight to avoid removing all the variation.

We now state the two most elementary theorems about \(L\)-statistics found in the \(1\)_st_ Edition. The results found there establish uniform convergence to normality over large classes of dfs, and they present studentized versions of such results. This is a very complete treatment of \(L\)-statistics. Roughly, suppose the finite sample score function \(J_{n}\) function is sufficiently close to a limiting score function \(J\), as defined in the \(1\)_st_ Edition. Then any asymptotic normality theorem for the mean (whether trimmed or untrimmed) of a sample from the df \(K\) defined in (5) is also true for the corresponding \(L\)-statistic of (1) based on samples from the df \(F\). Consult the \(1\)_st_ Edition for proofs of the following results.

**Theorem 3.1** (CLT for L-Statistics): Suppose the score function \(J(\cdot)\) of (4) is approximated "sufficiently closely" by a sequence \(J_{n}(\cdot)\). Let the statistic \(L_{n}\) in (1) be untrimmed. Suppose also that \(Y\equiv K(\xi)\cong(\mu,\ \sigma^{2})\) with \(\sigma^{2}\in(0,\ \infty)\) for the \(K\) of (5). Let \(\tilde{\mu}_{n}\) and \(\tilde{\sigma}_{n}\) be as in (11) (with \(a_{n}=a^{\prime}_{n}=1/n,\) for the sake of the proof). Let \(\mu_{n}\) be as in (3). Then (in the context of notation 15.1.3)

\[\sqrt{n}(L_{n}-\mu_{n})/\sigma=_{a}\sqrt{n}(\bar{Y}_{n}-\mu)/\sigma=_{a}Z_{n}(K) \equiv\int_{1/n}^{1-1/n}{\mathbb{U}}\,dK/\tilde{\sigma}_{n}\cong N(0,1). \tag{15}\]

(Moreover, \(V_{n}/\sigma\to_{p}1\) for an estimator \(V_{n}^{2}\) of \(\sigma^{2}\) presented in the \(1st\) Edition.)

**Theorem 3.2** (CLT for trimmed L-statistics): Suppose that \(J(\cdot)\) as in (4) is approximated "sufficiently closely" by a sequence \(J_{n}(\cdot)\). Suppose the statistician protects himself by specifying trimming numbers \(k_{n}\) and \(k^{\prime}_{n}\) for which \(k_{n}\wedge k^{\prime}_{n}\to\infty\), while \(a_{n}\lor a^{\prime}_{n}\to 0\) with \(a_{n}/a^{\prime}_{n}\to 1\), and suppose that \(K\) is in the statistical domain of attraction \(\tilde{\cal D}\) (recall (C.5.33), (15.1.3), and proposition 10.6.1). Then (in the context of notation 15.1.3)

\[\sqrt{n}(L_{n}-\mu_{n})/\tilde{\sigma}_{n}=_{a}\sqrt{n}(\tilde{Y}_{n}.\ - \tilde{\mu}_{n})/\tilde{\sigma}_{n} \tag{16}\]

\[=-\int_{a_{n}}^{1-a_{n}}{\mathbb{U}}_{n}dK/\tilde{\sigma}_{n}=-\int_{0}^{1}{ \mathbb{U}}_{n}d\tilde{K}_{n}/\tilde{\sigma}_{n}, \tag{17}\]

(Moreover, \(\tilde{V}_{n}/\tilde{\sigma}_{n}\to_{p}1\) for an estimator \(\tilde{V}_{n}^{2}\) of \(\tilde{\sigma}_{n}^{2}\) presented in the \(1st\) Edition.) [If \(K\in{\cal D}\)(Normal), only \((k_{n}\wedge k^{\prime}_{n})\geq 1\) is required and \(a_{n}/a^{\prime}_{n}\to 1\) may be omitted.]

## Appendix A Special Distributions

### 1 Elementary Probability

#### Independent Bernoulli Trials

If \(P(X=1)=p=1-P(X=0)\), then \(X\) is said to be a _Bernoulli_(\(p\)) rv. We refer to the event \([X=1]\) as "success," and \([X=0]\) as "failure." Let \(X_{1},\ldots,X_{n}\) be iid Bernoulli(\(p\)), and let \(T_{n}\equiv X_{1}+\cdots+X_{n}\) denote the number of successes in \(n\) independent Bernoulli(\(p\)) trials. Now,

\[P(X_{i}=x_{i}\text{ for }1\leq i\leq n)=p^{\Sigma_{1}^{n}x_{i}}(1-p)^{n-\Sigma_ {1}^{n}x_{i}}\qquad\text{ if all }x_{i}\text{ equal }0\text{ or }1;\]

this formula gives the joint distribution of \(X_{1},\ldots,X_{n}\). From this we obtain

\[P(T_{n}=k)=\left(\begin{array}{c}n\\ k\end{array}\right)p^{k}(1-p)^{n-k}\qquad\text{for }0\leq k\leq n, \tag{1}\]

since each of the \(\left(\begin{array}{c}n\\ k\end{array}\right)\) different possibilities that place \(k\) of the 1's in specific positions in an \(n\)-vector containing \(k\) outcomes 1 and \(n-k\) outcomes 0 has probability \(p^{k}(1-p)^{n-k}\), from the earlier display. We denote this by writing \(T_{n}\cong Binomial(n,p)\) when (1) holds. Note that Binomial(\(1,p\)) is the same as Bernoulli(\(p\)).

Let \(X_{1},X_{2},\ldots\) be iid Bernoulli(\(p\)); call this a _Bernoulli_(\(p\)) _process_. Interesting rvs include \(Y_{1}\equiv W_{1}\equiv\min\{n:T_{n}=1\}\). Since we can rewrite the event \([Y_{1}=k]=[X_{1}=\cdots=X_{k-1}=0,X_{k}=1]\), we have

\[P(Y_{1}=k)=(1-p)^{k-1}p\qquad\text{for }k=1,2,\ldots. \tag{2}\]

We write \(Y_{1}\cong\)_Geometric T_(\(p\)). Now let \(W_{m}\equiv\min\{n:T_{n}=m\}\). We call \(W_{m}\) the _waiting time_ to the \(m\)th success; \(W_{m}\) counts the number of _turns_ until the \(m\)th success. We let \(Y_{m}\equiv W_{m}-W_{m-1}\) for \(m\geq 1\), with \(W_{0}\equiv 0\), and we call the \(Y_{m}\)'s the _interarrival times_. Note that \([W_{m}=k]=[T_{k-1}=m-1\) and \(X_{k}=1]\). Hence

\[P(W_{m}=k)=\left(\begin{array}{c}k-1\\ m-1\end{array}\right)p^{m}(1-p)^{k-m}\qquad\text{for }k=m,m+1,\ldots. \tag{3}\]

We write \(W_{m}\cong\)_Negative Binomial Turns_(\(m,p\)) \(\equiv\)_NegBiT_(\(m,p\)). [We agree that _NegBiF_(\(m,p\)) denotes the distribution of \(W_{m}-m\), and that this "F" connotes "failures"; the rv \(W_{m}-m\) counts the number of _failures_ prior to the \(m\)th success.]

**Exercise 1.1**  Explain why \(Y_{1},Y_{2},\ldots\) are iid GeometricT(\(p\)).

Since the number of successes in the first \(n_{1}+n_{2}\) trials is the same as the number of successes in the first \(n_{1}\) trials plus the number of successes in the next \(n_{2}\) trials, it is clear that

\[T_{1}+T_{2}\cong\mbox{Binomial}(n_{1}+n_{2},p)\qquad\mbox{for independent rvs }T_{i}\cong\mbox{Binomial}(n_{i},p). \tag{4}\]

Likewise, waiting for \(m_{1}\) successes and then waiting for \(m_{2}\) more successes is the same as waiting for \(m_{1}+m_{2}\) successes in the first place. Hence,

\[W_{1}+W_{2}\cong\mbox{NegBiT}(m_{1}+m_{2},p)\qquad\mbox{for independent rvs }W_{i}\cong\mbox{NegBiT}(m_{i},p). \tag{5}\]

**Urn Models**

Suppose an urn contains \(N\) balls that are identical, except that \(M\) bear the number 1 and \(N-M\) bear the number 0. Thoroughly mix the balls in the urn. Draw one ball at random. Let \(X_{1}\) denote the number on the ball drawn. Then \(X_{1}\cong\mbox{Bernoulli}(p)\) with \(p\equiv M/N\). Now replace the ball in the urn, thoroughly mix, and draw at random a second ball with number \(X_{2}\). Continue the process. This is the _sampling with replacement_ scheme. Then \(T_{n}\equiv X_{1}+\cdots+X_{n}\cong\mbox{Binomial }(n,p)\), where \(p=M/N\) represents the probability of success in \(n\) independent Bernoulli(\(p\)) trials.

Suppose now that the same scheme is repeated, except that the balls are not replaced. In this _sampling without replacement_ scheme \(X_{1},\ldots,X_{n}\) are dependent Bernoulli(\(p\)) rvs with \(p=M/N\). Also,

\[P(T_{n}=k)=\frac{\left(\begin{array}{c}M\\ k\end{array}\right)\left(\begin{array}{c}N-M\\ n-k\end{array}\right)}{\left(\begin{array}{c}N\\ n\end{array}\right)},\qquad\mbox{provided that the value $k$ is possible.} \tag{6}\]

We write \(T_{n}\cong Hypergeometric(M,N-M;n)\).

Suppose now that sampling is done without replacement, but the \(N\) balls in the urn bear the numbers \(a_{1},\ldots,a_{N}\). Let \(X_{1},\ldots,X_{n}\) denote the numbers on the first \(n\) balls drawn, and let \(T_{n}\equiv X_{1}+\cdots+X_{n}\). We call this the general _finite sampling model_. Call \(\bar{a}\equiv\sum_{1}^{N}a_{i}/N\) the _population mean_ and \(\sigma_{a}^{2}\equiv\sum_{1}^{N}(a_{i}-\bar{a})^{2}/N\) the _population variance_. Note that \(X_{i}\cong(\bar{a},\sigma_{a}^{2})\) for all \(1\leq i\leq n\), since we now assume \(n\leq N\). From (7.3.4), we have

\[0=\mbox{Var}[\sum_{1}^{N}X_{i}]=N\mbox{Var}[X_{1}]+N(N-1)\mbox{Cov}[X_{1},X_{2 }], \tag{7}\]

with the 0 valid, since \(\sum_{1}^{N}X_{i}\) is a constant. Solving (7) yields

\[\mbox{Cov}[X_{1},X_{2}]=-\sigma_{a}^{2}/(N-1). \tag{8}\]

As in (7), and using (8), \(\mbox{Var}[T_{n}]=n\sigma_{a}^{2}-n(n-1)\sigma_{a}^{2}/(N-1)\). Thus

\[\mbox{Var}[T_{n}/n]=\frac{1}{n}\sigma_{a}^{2}\left[1-\frac{n-1}{N-1}\right], \tag{9}\]

where \([1-(n-1)/(N-1)]\) is called the _correction factor for finite population sampling_.

**Exercise 1.2**  Verify (8) and (9).

**Exercise 1.3**  Suppose that \(T_{1}\cong\mbox{Binomial}\;(m,p)\) and \(T_{2}\cong\mbox{Binomial}(n,p)\) are independent. Then the conditional distribution of the first component \(T_{1}\) given the total \(T_{1}+T_{2}=k\) is Hypergeometric\((k,m+n-k;m)\).

**The Poisson Process**

Suppose now that \(X_{n1},\ldots,X_{nn}\) are iid Bernoulli\((p_{n})\), where \(np_{n}\to\lambda\) as \(n\to\infty\). Let \(T_{n}\equiv X_{n1}+\cdots+X_{nn}\), so that \(T_{n}\cong\mbox{Binomial}(n,p_{n})\). Simple calculations give

\[P(T_{n}=k)\to\lambda^{k}e^{-\lambda}/k!\qquad\mbox{for $k=0,1,\ldots$}. \tag{10}\]

When \(P(T=k)=\lambda^{k}e^{-\lambda}/k!\) for \(k=0,1,\ldots\), we write \(T\cong Poisson(\lambda)\).

This is now used to model a Geiger counter experiment. A radioactive source with large half-life is placed near a Geiger counter. Let \(\mathbb{N}(t)\) denote the number of particles registered by time \(t\). We will say that \(\{\mathbb{N}(t):t\geq 0\}\) is a _Poisson process_. (Do note that our treatment is purely informal.) Physical considerations lead us to believe that the increments \(\mathbb{N}(t_{1}),\mathbb{N}(t_{1},t_{2}],\ldots,\mathbb{N}(t_{k-1},t_{k}]\) should be independent rvs; here, the _increment_\(\mathbb{N}(t_{i-1},t_{i}]\;\equiv\mathbb{N}(t_{i})-\mathbb{N}(t_{i-1})\) is the number of particle counts across the interval \((t_{i-1},t_{i}]\). We say that \(\mathbb{N}\) has _independent increments_. Let us now define

\[\nu\equiv E\mathbb{N}(1)\equiv\;[\mbox{the \emph{intensity} of the process}]. \tag{11}\]

Let \(M\) denote the number of radioactive particles in our source, and let \(X_{i}\) equal 1 or 0 depending on whether or not the \(i\)th particle registers by time \(t=1\). It seems possible to assume that \(X_{1},\ldots,X_{M}\) are iid Bernoulli. Since \(\mathbb{N}(1)=X_{1}+\cdots+X_{M}\) has mean \(\nu=E\mathbb{N}(1)=MEX_{1}\), this leads to \(\mathbb{N}(1)\cong\mbox{Binomial}(M,\nu/M)\). By the first paragraph of this section, \(\mathbb{N}(1)\) is thus approximately a Poisson (\(\nu\)) rv. We now alter our point of view slightly, and agree that we will _use this approximation as our model_. Thus \(\mathbb{N}(1)\) is a Poisson\((\nu)\) rv. Since \(M\) is huge, the accuracy should be superb. Because of the stationary and independent increments we thus have

\[\mathbb{N}(s,t]\equiv\mathbb{N}(t)-\mathbb{N}(s)\cong\mbox{Poisson}(\nu(t-s)) \qquad\mbox{for all $0\leq s\leq t$}, \tag{12}\]

\[\mathbb{N}\]

 has independent increments.

We agree also that \(\mathbb{N}(0)\equiv 0\). (This is actually enough to rigorously specify a Poisson process.) Let \(Y_{1}\equiv W_{1}\equiv\inf\{t:\mathbb{N}(t)=1\}\). Since

\[[Y_{1}>t]=[\mathbb{N}(t)<1]=[\mathbb{N}(t)=0], \tag{13}\]

we see that \(1-F_{Y_{1}}(t)=P(Y_{1}>t)=P(\mathbb{N}(t)=0)=e^{-\nu t}\) by (12). Thus \(Y_{1}\) has \(\mbox{df}\,1-\exp(-\nu t)\) for \(t\geq 0\) and density

\[f_{Y_{1}}(t)=\nu e^{-\nu t}\qquad\mbox{for $t\geq 0$}; \tag{14}\]

we write \(Y_{1}\cong Exponential(\nu).\) Now let \(W_{m}\equiv\inf\{t:N(t)=m\}\); we call \(W_{m}\) the \(m\)th _waiting time_. We call \(Y_{m}\equiv W_{m}-W_{m-1},m\geq 1\), the \(m\)th _interarrival time_. In light of the physical properties of our Geiger counter model, and using (13), it seems reasonable that

\[Y_{1},Y_{2},\ldots\mbox{ are iid Exponential}(\nu)\,\mbox{rvs.} \tag{15}\]

Our assumption of the previous sentence could be expressed as follows:

\[Y_{1}\mbox{ and }\mathbb{N}_{1}(t)\equiv\mathbb{N}(Y_{1},Y_{1}+t]=\mathbb{N}(Y_{1 }+t)-\mathbb{N}(Y_{1})\mbox{ are independent,} \tag{16}\] \[\mathbb{N}_{1}\mbox{is again a Poisson process, with intensity $\nu$}. \tag{17}\]We will call this the _strong Markov property_ of the Poisson process. Additionally,

\[[W_{m}>t]=[\mathbb{N}(t)<m], \tag{18}\]

so that \(1-F_{W_{m}}(t)=P(W_{m}>t)=\sum_{k=0}^{m-1}(\nu t)^{k}e^{-\nu t}/k!\); the derivate of this expression telescopes, and shows that \(W_{m}\) has density

\[f_{W_{m}}(t)=\nu^{m}t^{m-1}e^{-\nu t}/\Gamma(m)\qquad\mbox{for $t\geq 0$}. \tag{19}\]

We write \(W_{m}\cong Gamma(m,\nu)\). Since waiting for \(m_{1}\) counts and then waiting for \(m_{2}\) more counts is the same as waiting for \(m_{1}+m_{2}\) counts in the first place,

\[Z_{1}+Z_{2}\cong\mbox{Gamma}(m_{1}+m_{2},\nu)\qquad\mbox{for independent $Z_{i} \cong\mbox{Gamma}(m_{i},\nu)$}. \tag{20}\]

It is true that (19) is a density for any real number \(m>0\), and the property (20) still holds for all positive \(m_{i}\)'s.

**Exercise 1.4**: Verify (10), that Binomial \((n,p_{n})\to\mbox{Poisson}(\lambda)\) as \(np_{n}\to\lambda\).
**Exercise 1.5**: Verify (19), that \(F_{W_{m}}\) has derivative \(f_{W_{m}}\).
**Exercise 1.6**: Verify that (20) holds for arbitrary real \(m_{i}>0\).
**Exercise 1.7**: If \(X\cong\mbox{Poisson}(\nu_{1})\) and \(Y\cong\mbox{Poisson}(\nu_{2})\), then the conditional distribution of \(X\) given that \(X+Y=n\) is Binomial\((n,\nu_{1}/(\nu_{1}+\nu_{2}))\).
**Exercise 1.8**: Use Kolmogorov's extension theorem to show that a Poisson process \(\mathbb{N}\) exists on \((R_{[0,\infty)},{\cal B}_{[0,\infty)}).\) Then apply the smoother realizations theorem 5.4.2 to claim that a.e. sample path is right continuous with integral jumps.
**Location and Scale**

If \(a>0\), then

\[F_{aZ+b}(x)=P(aZ+b\leq x)=P(Z\leq(x-b)/a)=F_{Z}((x-b)/a)\]

holds for any \(F_{Z}(\cdot).\) Thus for any density \(f_{Z}(\cdot)\) the rv \(aZ+b\) has density

\[f_{aZ+b}(x)=\frac{1}{a}f_{Z}\left(\frac{x-b}{a}\right)\qquad\mbox{for $-\infty<x<\infty$}. \tag{21}\]
**Normal Distributions**

Suppose the rv \(Z\) has density

\[f_{Z}(x)=\frac{1}{\sqrt{2\pi}}\exp(-\frac{x^{2}}{2})\qquad\mbox{for $-\infty<x<\infty$}. \tag{22}\]

Then \(Z\) is said to be a _Standard Normal_\(rv\). Thus the rv \(X\equiv\mu+\sigma Z\cong(\mu,\sigma^{2})\) has density (by (21))

\[\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2}\ (\frac{x-\mu}{\sigma})^{2}) \qquad\mbox{for $-\infty<x<\infty$}, \tag{23}\]

and we write \(X\cong Normal(\mu,\sigma^{2})\), or just \(X\cong N(\mu,\sigma^{2})\).

**Exercise 1.9**  Show that the formula \(f_{Z}(\cdot)\) of (22) is a density. Then show that this density has mean 0 and variance 1. [Transform to polar coordinates to compute \((\int\!f_{Z}(x)\,dx)^{2}=1\).]

The importance of the Normal distribution derives from the following theorem. Recall that if \(X_{1},\ldots,X_{n}\) are iid \((\mu,\sigma^{2})\) with \(0<\sigma<\infty\), then \(\sqrt{n}(\bar{X}-\mu)/\sigma\cong(0,1)\) for the sample average \(\bar{X}_{n}\equiv(X_{1}+\cdots+X_{n})/n\). This is only a statement about moments. But much more is true. The proof of the powerful result we now state is found in chapter 10. We will use it here for motivational purposes.

**Theorem 1.1 (Classical CLT)** Let \(X_{1},\ldots,X_{n}\) be iid \((\mu,\sigma^{2})\) with \(\sigma<\infty\). Then

\[\sqrt{n}(\bar{X}_{n}-\mu)\to_{d}N(0,\sigma^{2})\quad\mbox{as $n\to\infty$}. \tag{24}\]

Let \(0<\sigma<\infty\). Then the rv \(Z_{n}\) below is _asymptotically normal_, in that

\[Z_{n}\equiv\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\to_{d}N(0,1)\qquad\mbox{as $n\to\infty$}. \tag{25}\]

Suppose now that \(Z\) is \(N(0,1)\). Then

\[F_{Z^{2}}(x)=P(Z^{2}\leq x)=P(-\sqrt{x}\leq Z\leq\sqrt{x})=F_{Z}(\sqrt{x})-F_{Z }(-\sqrt{x}); \tag{26}\]

thus \(Z^{2}\) has density

\[f_{Z^{2}}(x)=\frac{1}{2\sqrt{x}}[f_{Z}(\sqrt{x})-f_{Z}(-\sqrt{x})]\qquad\mbox{ for $x\geq 0$}. \tag{27}\]

[Note that formula (27) is true for any density \(f_{Z}(\cdot)\).] Plugging into (27) for this \(Z\) shows that

\[f_{Z^{2}}(x)=(2\pi x)^{-1/2}\exp(-x/2)\quad\mbox{for $x\geq 0$}; \tag{28}\]

this is called the Chisquare(1) distribution. Note that Chisquare(1) is the same as Gamma \((\frac{1}{2},\frac{1}{2})\). Thus (20) establishes that

\[\mbox{if $X_{1},\ldots,X_{m}$ are iid $N(0,1)$},\quad\mbox{then $\sum_{i=1}^{m}X_{i}^{2}\cong\mbox{Chisquare}(m)$}, \tag{29}\]

where _Chisquare_(_m_) \(\equiv\) Gamma\((\frac{m}{2},\frac{1}{2})\).

**Uniform and Related Distributions**

Write \(X\cong\)_Uniform(a, b)_ if

\[f_{X}(x)=\frac{1}{(b-a)}1_{[a,b]}(x)=\frac{1}{(b-a)}\qquad\mbox{on $[a,b]$}. \tag{30}\]

By far the most important special case is Uniform(0, 1). A generalization of this is the Beta\((c,d)\) family. We write \(X\cong Beta(c,d)\) if

\[f_{X}(x)=\frac{1}{\beta(c,d)}x^{c-1}(1-x)^{d-1}1_{[0,1]}(x)=\frac{1}{\beta(c,d )}x^{c-1}(1-x)^{d-1}\qquad\mbox{on $[0,1]$}, \tag{31}\]

where \(\beta(c,d)\equiv\Gamma(c)\Gamma(d)/\Gamma(c+d)\). Here, \(b>0\) and \(c>0\) are required.

Suppose that \(\xi_{1},\ldots,\xi_{n}\) are iid Uniform\((0,1)\). Let \(0\leq\xi_{n:1}\leq\cdots\leq\xi_{n:n}\leq 1\) denote the ordered values of the \(\xi_{i}\)'s; we call the \(\xi_{n:i}\)'s the _uniform order statistics_. It seems intuitive that \(\xi_{n:i}\) equals \(x\) if \((i-1)\) of the \(\xi_{i}\)'s fall in \([0,x)\), 1 of the \(\xi_{i}\)'s is equal to \(x\), and \(n-i\) of the\(\xi_{i}\)'s fall in \((x,1]\). There are \(n!/[(i-1)!(n-i)!]\) such designations of the \(\xi_{ni}\)'s, and for each such designation the "chance" of the rv's falling in the correct parts of \([0,1]\) is \(x^{i-1}(1\cdot dx)(1-x)^{n-i}\). Thus

\[f_{\xi_{ni}}(x)=\frac{n!}{(i-1)!(n-i)!}x^{i-1}(1-x)^{n-i}1_{[0,1]}(x),\quad \mbox{or }\xi_{n:i}\cong\mbox{Beta}(i,n-i+1). \tag{32}\]

**Exercise 1.10** : Give a rigorous derivation of (32) by computing \(1-F_{\xi_{ni}}(x)\) and then differentiating it.

**Exercise 1.11** : Choose a point at random on the surface of the unit sphere (with probability proportional to area). Let \(\Theta\) denote the longitude and \(\Phi\) denote the latitude (relative to some fixed axes) of the point so chosen. Determine the joint density of \(\Theta\) and \(\Phi\).

**The Cauchy Distribution**

Write \(X\cong\)_Cauchy_(_b, a_) if

\[f_{X}(x)=1/\{a\pi[1+(x-b)^{2}/a^{2}]\}\qquad\mbox{on }(-\infty,\infty). \tag{33}\]

By far the most important special case is Cauchy(0, 1); we then say simply that \(X\cong\)_Cauchy_, and its density is given by \(1/[\pi(1+x^{2})]\) on \((-\infty,\infty)\). Verify that \(E|X|=\infty\). We will see below that if \(X_{1},\ldots,X_{n}\) are iid Cauchy, then the sample average \(\bar{X}_{n}\equiv(X_{1}+\cdots+X_{n})/n\cong\) Cauchy. These two facts make the Cauchy ideal for many counterexamples.

**Double Exponential and Logistic Distributions**

We say \(X\cong\)_Double Exponential_(_b, a_) when \((X-b)/a\) has density \(\frac{1}{2}\exp(-|x|)\) on the line. We say \(X\cong\)_Logistic_(_b, a_) when \((X-b)/a\) has density \(e^{x}/(1+e^{x})^{2}=1/(e^{-x/2}+e^{x/2})^{2}\) on the line.

**Exercise 1.12** : Now, \(X\equiv F^{-1}(\xi)\) has df \(F\) by the inverse transformation. So, compute \(F^{-1}\) for the Logistic(0, 1) and the Double Exponential(0, 1) distributions.

**Rademacher Random Variables and Symmetrization**

Many problems become simpler if the problem is symmetrized. One way of accomplishing this is by the appropriate introduction of Rademacher rvs. We say that \(\epsilon\) is a _Rademacher_\(rv\) if \(P(\epsilon=1)=P(\epsilon=-1)=\frac{1}{2}\). Thus \(\epsilon\cong 2\) Bernoulli(\(\frac{1}{2}\)) \(-1\).

We say that \(X\) is a _symmetric_\(rv\) if \(X\cong-X\). If \(X\) and \(X^{\prime}\) are iid, then \(X^{s}\equiv(X-X^{\prime})\cong(X^{\prime}-X)=-(X-X^{\prime})=-X^{s}\); hence \(X^{s}\) is a symmetric rv.

**Exercise 1.13** : If \(X\) is a symmetric rv independent of the Rademacher rv \(\epsilon\), then \(X\cong\epsilon X\) always holds.

**The Multinomial Distribution**

Suppose that \(B_{1}+\cdots+B_{k}=R\) for Borel sets \(B_{i}\in{\cal B}\); recall that we call this a _partition_ of \(R\). Let \(Y_{1},\ldots,Y_{n}\) be iid rvs on \((\Omega,{\cal A},P)\). Let \({\sf X}_{i}\equiv(X_{i1},\ldots,X_{ik})\equiv(1_{B_{1}}(Y_{i}),\ldots,1_{B_{k }}(Y_{i}))\) for \(1\leq i\leq n\), and set

\[{\sf T}\equiv(T_{1},\ldots,T_{k})^{\prime}=(\sum_{i=1}^{n}X_{i1},\ldots,\sum_{ i=1}^{n}X_{ik})=(\sum_{i=1}^{n}1_{B_{1}}(Y_{i}),\ldots,\sum_{i=1}^{n}1_{B_{k}}(Y_{i} )). \tag{34}\]Note that \(X_{1j},\ldots,X_{nj}\) are iid Bernoulli(\(p_{j}\)) with \(p_{j}\equiv P(Y_{i}\in B_{j})\), and thus \(T_{j}\cong\mbox{Binomial}(n,p_{j})\) (marginally). However, \(T_{1},\ldots,T_{n}\) are dependent rvs. The joint distribution of \((T_{1},\ldots,T_{n})^{\prime}\) is called the _Multinomial_(\(n\), p) distribution. We now derive it. The number of ways to designate \(n_{1}\) of the \(Y_{i}\)'s to fall in \(B_{1},\ldots\), and \(n_{k}\) of the \(Y_{i}\)'s to fall in \(B_{k}\) is the _multinomial coefficient_

\[\left(\begin{array}{c}n\\ n_{1}\ldots n_{k}\end{array}\right)\equiv\frac{n!}{n_{1}!\cdots n_{k}!},\qquad \mbox{where }n_{1}+\cdots+n_{k}=n. \tag{35}\]

Each such designation occurs with probability \(\prod_{1}^{k}p_{i^{n_{i}}}\). Hence for each possible n,

\[P({\sf T}={\sf n})\equiv P(T_{1}=n_{1},\ldots,T_{k}=n_{k})=\left(\begin{array} []{c}n\\ n_{1}\ldots n_{k}\end{array}\right)p_{1}^{n_{1}}\cdots p_{k}^{n_{k}}. \tag{36}\]

It is now a trivial calculation that

\[{\rm Cov}[X_{ij},X_{il}]={\rm E1}_{B_{j}}(Y_{i})1_{B_{l}}(Y_{i})-{\rm E1}_{B_{j }}(Y_{i}){\rm E1}_{B_{l}}(Y_{i})=-p_{j}p_{l}\qquad\mbox{if }j\neq l. \tag{37}\]

Thus

\[{\rm Cov}[T_{j},T_{l}]=-n\,p_{j}\,p_{l}\qquad\mbox{for all }j\neq l. \tag{38}\]

Thus (with \(D_{\sf P}\) a diagonal matrix having each \(d_{ii}=p_{i}\))

\[\left(\begin{array}{c}T_{1}\\ \vdots\\ T_{k}\end{array}\right)\cong n\left(\left(\begin{array}{c}p_{1}\\ \vdots\\ p_{k}\end{array}\right),\left(\begin{array}{ccc}p_{1}(1-p_{1})&-p_{1}p_{k}\\ \vdots&\ddots&\vdots\\ -p_{k}p_{1}&p_{k}(1-p_{k})\end{array}\right)\right)=n({\sf p},[D_{\sf P}-{\sf pp }^{\prime}]). \tag{39}\]

**Assorted Facts**

**Stirling's Formula for \(n!\)** For all \(n>1\) we have

\[n!=e^{a_{n}}n^{n+1/2}e^{-n}\sqrt{2\pi},\qquad\mbox{where }1/(12n+1)<a_{n}<1/(1 2n). \tag{40}\]

**Eulers's Constant \(\gamma\)**

\[\sum_{i=1}^{n}1/i-\log n\uparrow\gamma\equiv 0.577215664901533\ldots. \tag{41}\]

**Exercise 1.14** (An added touch)  If \(\sum_{1}^{\infty}a_{n}<\infty\), there exists a sequence \(c_{n}\uparrow\infty\) such that \(\sum_{1}^{\infty}c_{n}a_{n}<\infty\).

**Elementary Conditional Probability**

One defines the _conditional probability_ of the event \(A\) given that the event \(B\) has occurred via \(P(A|B)\equiv P(AB)/P(B)\) when \(P(B)\neq 0\). One then calls \(A\) and \(B\)_independent_ if \(P(A|B)=P(A)\), because the probability of \(A\) is then unaffected by whether or not \(B\) occurred. Thus both of the following statements hold:

\[\begin{array}{l}\mbox{Definition: }P(A|B)\equiv P(AB)/P(B)\\ \mbox{leads to Theorem: }P(AB)=P(B)P(A|B),\quad\mbox{always.}\end{array} \tag{42}\](43) Definition: Independence means \(P(A|B)=P(A),\) and leads to Theorem: \(P(AB)=P(A)P(B)\) if \(A\) and \(B\) are independent.

The big advantage of computation of \(P(A|B)\) via the theorem of (42) is that one can often _revisualize_\(P(A|B)\) in the context of a much simpler problem. Thus the probability of drawing two Reds when drawing at random without replacement from an urn containing 6 Reds and 4 Whites is \(P(R_{1}R_{2})=P(R_{1})P(R_{2}|R_{1})=(6/10)\times(5/9)\), where we revisualized to an urn containing 5 Reds and 4 Whites to compute \(P(R_{2}|R_{1})=5/9\). [Had we used sampling with replacement, our answer would have been (6/10) \(\times(6/10)\) via (43).] [In the next exercise, revisualization works superbly to trivialize the problem.]

**Exercise 1.15** (Craps, according to Hoyle) (a) The "shooter" rolls two dice, and obtains a total (called the "point"). If "point" equals "seven" or "eleven," the game is over and "shooter" wins. If point equals "two" or "twelve," the game is over and "shooter" loses. Otherwise, the game continues. It is now a race between "point" and "seven." If "point" comes first, the "shooter" wins; otherwise, he loses. Determine the probability that the "shooter" wins in the game of craps.

[When trying to "convert" a "point" of "ten" (say), we can revisualize and say that on the turn on which the game ends the dice will be showing either one of the 3 tens or one of the 6 sevens, and the probability of this conversion is clearly \(3/(3+6)\).]

(b) (The Las Vegas game) The above game is favorable to the "shooter." Thus the version played in Las Vegas has different rules. Specifically, a "three" on the first roll of the two dice is also an immediate loss for "shooter." Determine the probability that "shooter" wins the Las Vegas version of craps.

## 2 Distribution Theory for Statistics

### Convolution

If \(X\) and \(Y\) are independent rvs on \((\Omega,{\cal A},P)\), then

\[F_{X+Y}(z)=P(X+Y\leq z)=\int\!\!\!\int_{x+y\leq z}\,dF_{X}(x)\,dF_{ Y}(y)\] \[\quad=\int_{-\infty}^{\infty}\int_{-\infty}^{z-x}\,dF_{Y}(y)\,dF_{ X}(x) \tag{1}\] \[\quad=\int_{-\infty}^{\infty}F_{Y}(z-x)\,dF_{X}(x)\equiv F_{X}*F_ {Y}(z)\]

is a formula, called the _convolution formula_, for \(F_{X+Y}\) in terms of \(F_{X}\) and \(F_{Y}\) (the symbol \(*\) defined here stands for "convolution"). In case \(Y\) has density \(f_{Y}\) with respect to Lebesgue measure, then so does \(X+Y\). In fact, since

\[\int_{-\infty}^{z}\int_{-\infty}^{\infty}f_{Y}(y-x)\,dF_{X}(x)\, dy=\int_{-\infty}^{\infty}[\int_{-\infty}^{z}f_{Y}(y-x)\,dy]\,dF_{X}(x)\] \[\quad=\int_{-\infty}^{\infty}F_{Y}(z-x)\,dF_{X}(x)=F_{X+Y}(z)\,\]

we see that \(X+Y\) has a density given by

\[f_{X+Y}(z)=\int_{-\infty}^{\infty}f_{Y}(z-x)\,dF_{X}(x). \tag{2}\]

In case both \(X\) and \(Y\) have densities, we further note that

\[f_{X+Y}(z)=\int_{-\infty}^{\infty}f_{Y}(z-x)f_{X}(x)\,dx\equiv f_{Y}*f_{X}(z). \tag{3}\]

**Exercise 2.1**  Use (2) to show that for \(X\) and \(Y\) independent:

(i) \(X\cong N(\mu_{1},\sigma_{1}^{2})\) and \(Y\cong N(\mu_{2},\sigma_{2}^{2})\) implies \(X+Y\cong N(\mu_{1}+\mu_{2},\sigma_{1}^{2}+\sigma_{2}^{2})\).

(ii) \(X\cong\mbox{Cauchy}(0,a_{1})\) and \(Y\cong\mbox{Cauchy}(0,a_{2})\) has \(X+Y\cong\mbox{Cauchy}(0,a_{1}+a_{2})\).

(iii) \(X\cong\mbox{Gamma}(r_{1},\theta)\) and \(Y\cong\mbox{Gamma}(r_{2},\theta)\) has \(X+Y\cong\mbox{Gamma}(r_{1}+r_{2},\theta)\).

**Exercise 2.2** (i) Let \(X_{1},\ldots,X_{n}\) be iid \(N(0,1)\). Show that the normed sample average necessarily satisfies \((X_{1}+\cdots+X_{n})/\sqrt{n}\cong N(0,1)\).

(ii) Let \(X_{1},\ldots,X_{n}\) be iid Cauchy\((0,1)\). Show \((X_{1}+\cdots+X_{n})/n\cong\mbox{Cauchy}(0,1)\).

If \(X\) and \(Y\) are independent rvs taking values in \(0,1,2,\ldots\), then clearly

\[P(X+Y=k)=\sum_{i=0}^{k}P(X=i)P(Y=k-i)\qquad\mbox{for $k=0,1,2,\ldots$}. \tag{4}\]

**Exercise 2.3**  Use (3) to show that for \(X\) and \(Y\) independent:

\[X\cong\mbox{Poisson}(\lambda_{1})\mbox{ and }Y\cong\mbox{Poisson}(\lambda_{2}) \mbox{ has }X+Y\cong\mbox{Poisson}(\lambda_{1}+\lambda_{2}).\]

A fundamental problem in probability theory is to determine constants \(b_{n}\) and \(a_{n}>0\) for which iid rvs \(X_{1},\ldots,X_{n},\ldots\) satisfy

\[(X_{1}+\cdots+X_{n}-b_{n})/a_{n}\to_{d}G,\quad\mbox{as $n\to\infty$} \tag{5}\]

for some nondegenerate df \(G\). Exercise 2.2 gives us two examples of such convergence; each was derived via the convolution formula. Except in certain special cases, such as exercises2.1 - 2.3, the various convolution formulas are too difficult to deal with directly. For this reason we need to develop a more oblique, but ultimately more convenient, approach if we are to solve problems of the form (5). This is taken up in chapters 9, 10, and 11.

##### Other Formulas

**Exercise 2.4**  Suppose that \(X\) and \(Y\) are independent with \(P(Y>0)=1\). Show that products and quotients of these rvs satisfy

\[F_{XY}(z)\equiv P(XY\leq z)=\int_{0}^{\infty}F_{X}(z/y)\,dF_{Y}(y) \qquad\mbox{ for all }z, \tag{7}\] \[F_{X/Y}(z)\equiv P(X/Y\leq z)=\int_{0}^{\infty}F_{X}(zy)\,dF_{Y}( y) \qquad\mbox{ for all }z. \tag{6}\]

If \(F_{X}\) has a density \(f_{X}\), then changing the order of integration above shows that \(F_{XY}\) and \(F_{X/Y}\) have densities given by

\[f_{XY}(z)=\int_{0}^{\infty}y^{-1}f_{X}(z/y)\,dF_{Y}(y) \qquad\mbox{ for all }z, \tag{9}\] \[f_{X/Y}(z)=\int_{0}^{\infty}y\,f_{X}(yz)\,dF_{Y}(y) \qquad\mbox{ for all }z. \tag{8}\]

**Exercise 2.5**  Let \(Z\cong N(0,1),U\cong\chi_{m}^{2}\), and \(V\cong\chi_{n}^{2}\) be independent.

(a) Establish these classically important results:

\[\frac{Z}{\sqrt{U/m}}\cong\mbox{ Student's }t_{m}. \tag{11}\] \[\frac{U/m}{V/n}\cong\mbox{ Snedecor's }F_{m,n}.\] (12) \[\frac{U}{U+V}\cong\mbox{Beta}(m/2,n/2). \tag{10}\]

Here

\[f_{t_{m}}(x)\equiv\frac{\Gamma((m+n)/2)}{\sqrt{\pi m}\Gamma(m/2) }\frac{1}{(1+x^{2}/m)^{(m+n)/2}} \qquad\mbox{for }-\infty<x<\infty, \tag{14}\] \[f_{F_{m,n}}(x)\equiv\frac{\Gamma((m+n)/2)}{\Gamma(m/2)\Gamma(n/2 )}\frac{(m/n)^{m/2}x^{m/2-1}}{(1+mx/n)^{(m+n)/2}} \qquad\mbox{for }0<x<\infty. \tag{13}\]

(b) Compute the \(k\)th moment of each of these three distributions.

**Exercise 2.6**  If \(Y_{1},\ldots,Y_{n+1}\) are iid Exponential(\(\theta\)), then

\[(Y_{1}+\cdots+Y_{i})/(Y_{1}+\cdots+Y_{n+1})\cong\mbox{Beta}(i,n-i+1). \tag{15}\]

**Exercise 2.7**  Let \(X_{1},\ldots,X_{n}\) be iid N(\(\mu,\sigma^{2}\)).

(a) Show that \(W_{n}\equiv\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\cong\mbox{N}(0,1)\).

(b) Show that \((n-1)S_{n}^{2}/\sigma^{2}\equiv\sum_{1}^{n}(X_{k}-\bar{X}_{n})^{2}/\sigma^{2} \cong\chi_{n-1}^{2}\).

(c) Show that \(W_{n}\) and \(S_{n}^{2}\) are independent rvs.

(d) Show that \(T_{n}\equiv\sqrt{n}(\bar{X}_{n}-\mu)/S_{n}\cong\) Student's \(t_{n-1}\).

[Hint. Let \(\Gamma\equiv|[\gamma_{ij}]|\) be an orthogonal matrix with all \(\gamma_{1j}=1/\sqrt{n}\). Now let \(\vec{Z}\equiv\Gamma(\vec{X}-\mu\vec{1})/\sigma\). This yields iid N(0,1) rvs \(Z_{1},\ldots,Z_{n}\), with \(W_{n}=Z_{1}\cong\mbox{N}(0,1)\) and \((n-1)S_{n}^{2}=\sum_{2}^{n}Z_{k}^{2}\cong\chi_{n-1}^{2}\). Apply exercise 2.5.]

**Statistical Confidence Intervals**

**Example 2.1**  Suppose we model the performances of \(n\) independent repetitions \(X_{1},\ldots,X_{n}\) of an experiment as iid \(N(\mu,\sigma^{2})\)rvs. The previous exercise shows that \(\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\) is a \(N(0,1)\) rv independent of the sample variance estimator \(S_{n}^{2}\equiv\sum_{1}^{n}(X_{k}-\bar{X}_{n})^{2}/(n-1)\) of \(\sigma^{2}\), and that \(S_{n}/\sigma\cong\{\chi_{n-1}^{2}/(n-1)\}^{1/2}\). Thus

\[T_{n}\equiv\sqrt{n}[\bar{X}_{n}-\mu]/S_{n}\cong{\cal T}_{n-1}\equiv\mbox{ Student }t_{n-1}. \tag{16}\]

Specify \(t_{p/2}\) such that \(P(-t_{p/2}\leq{\cal T}_{n-1}\leq t_{p/2})=1-p\); perhaps, with \(p=.05.\) Then with the "large" probability of \(1-p=.95\) we have

\[1-p=P(-t_{p/2}\leq T_{n}\leq t_{p/2})=P(-t_{p/2}\leq\sqrt{n}[ \bar{X}_{n}-\mu]/S_{n}\leq t_{p/2}) \tag{18}\] \[=P(\mu-t_{p/2}S_{n}/\sqrt{n}\leq\bar{X}_{n}\leq\mu+t_{p/2}S_{n}/ \sqrt{n}\leq\bar{X}_{n})\] (19) \[=P(\bar{X}_{n}-t_{p/2}S_{n}/\sqrt{n}\leq\mu\leq\bar{X}_{n}+t_{p/2 }S_{n}/\sqrt{n}). \tag{17}\]

That is:

The random interval \(\bar{X}_{n}\pm t_{p/2}S_{n}/\sqrt{n}\)

will contain the unknown value of \(\mu\)

an average of \((1-p)\times 100\%\) of the time.

So when we apply this to the data values \(x_{1},\ldots,x_{n}\), we can have \((1-p)\times 100\%\) confidence that the interval \(\bar{x}_{n}\pm t_{p/2}\,s_{n}/\sqrt{n}\) did enclose the true (but unknown) value of \(\mu\). We say that

\[\bar{X}_{n}\pm t_{p/2}\,S_{n}/\sqrt{n}\mbox{ provides a}\,(1-p)\times 100\% \mbox{ confidence interval} \tag{21}\]

for the unknown mean \(\mu\). Or we say that

\[\bar{x}_{n}\pm t_{p/2}\,s_{n}/\sqrt{n}\mbox{ provides a}\,(1-p)\times 100\% \mbox{ numerical confidence interval} \tag{22}\]

for the unknown mean \(\mu\). There is a probability of \(1-p\) (or a \((1-p)\times 100\%\) chance) that the former _will contain_ the unknown value of \(\mu\) when the \(X\)-experiment is repeated \(n\) times. There is a \((1-p)\times 100\%\) confidence (or degree of belief) that the latter _did contain_ the unknown value of \(\mu\) after the \(X\)-experiment was repeated \(n\) times giving the actual data values \(x_{1},\ldots,x_{n}\). We call \(t_{p/2}\,s_{n}/\sqrt{n}\) the _numerical margin for error_ exhibited by our experiment. \(\Box\)

**Transformations of Random Variables**

**Exercise 2.8**  Suppose \(X\) has density \(f_{X}(\cdot)\) with respect to Lebesgue measure \(\lambda_{n}(\cdot)\) on \(n\)-dimensional Euclidean space \(R_{n}\).

(a) Let \(Y\equiv AX\) denote a linear transformation with \(A\) a nonsingular matrix. The Jacobian of this linear transformation is

\[J\equiv\left|\left[\frac{\partial(\mbox{old})}{\partial(\mbox{new})}\right] \right|\equiv\left|\left[\frac{\partial x_{i}}{\partial y_{j}}\right]\right|= A^{-1},\qquad\mbox{with}\qquad|J|^{+}=|A^{-1}|^{+}=1/|A|^{+}. \tag{23}\]

Verify that the rv \(Y\) has a density \(f_{Y}(\cdot)\) with respect to Lebesgue measure that is given by \(f_{Y}(y)=f_{X}(A^{-1}y)/|A|^{+}\) on \(R_{n}\).

(b) Suppose now that \(X\) has density \(f_{X}(\cdot)\) with respect to Lebesgue measure on a region \(R_{X}\) in \(R_{n}\). Suppose the 1-to-1 transformation \(Y\equiv g(X)\) from \(R_{X}\) to the region \(R_{Y}\equiv g(R_{X})\) has a nonsingular Jacobian with continuous elements at each point of the region. Show that \(Y\) has a density given by

\[f_{Y}(y)=f_{X}(g^{-1}(y))\times|[\partial(\mbox{old})/\partial(\mbox{new})]|^{ +}\qquad\mbox{for $y$ in the region $R_{Y}$.} \tag{24}\]

(That is, any "nice" transformation is locally linear.)

**Exercise 2.9**  Suppose that \(U\equiv XY\) and \(V\equiv X/Y\) for rvs having joint density \(f_{XY}(\cdot,\cdot)\) on the region where \(x>0\) and \(y>0\). The inverse transformation is \(X=\sqrt{UV}\) and \(Y=\sqrt{U/V}\) with a "nice" Jacobian that is equal to \(2v\). Thus the joint density of \(U,V\) is

\[f_{UV}(u,v)=\frac{1}{2v}f_{XY}(\sqrt{uv},\sqrt{u/v}))\ \ \mbox{on the appropriate $(u,v)$-region}, \tag{25}\]

provided that the transformation is 1-to-1. (Obtaining the appropriate region is often the hardest part.) Now evaluate \(f_{UV}(\cdot,\cdot)\) and \(f_{V}(\cdot)\) in the following cases.

(a) \(X\) and \(Y\) are independent Exponential(1).

(b) \(X\) and \(Y\) are independent with density \(1/(xy)^{2}\) on \(x,y\geq 1\). Evaluate \(f_{U}(\cdot)\).

(c) \(X\) and \(Y\) are independent N(0, 1). [Note that this transformation is not 1-1.]

(d) \(X\cong\mbox{N}(0,1)\) and \(Y\cong\mbox{Uniform}(0,1)\) are independent.

[This exercise demonstrates vividly the important role played by the regions \(R_{X}\) and \(R_{Y}\).]

## 3 Linear Algebra Applications

**Notation 3.1** (Mean vector and covariance matrix)  Let \(X\equiv(X_{1},\ldots,X_{n})^{\prime}\) be a rv. Then \(\mbox{E}(X)\equiv\mu\equiv(\mu_{1},\ldots,\mu_{n})^{\prime}\), where \(\mu_{i}\equiv\mbox{E}(X_{i})\) is called the _mean vector_. And \(\Sigma\equiv|[\sigma_{ij}]|\equiv|[\mbox{Cov}[X_{i},X_{j}]]|\) is called the _covariance matrix_. (By the Cauchy-Schwarz inequality, both of \(\mu\) and \(\Sigma\) are well-defined provided that each of \(\sigma_{ii}\equiv\mbox{Var}[X_{i}]\equiv\mbox{Cov}[X_{i},X_{i}]\) is finite.) \(\Box\)

**Definition 3.1** (Linear algebra)  We will operate on \(n\)-dimensional space \(R_{n}\) with \(n\times n\) matrices and \(n\times 1\) vectors.

(i) A matrix \(\Gamma\) with column vectors \(\gamma_{i}\) (that is, \(\Gamma=[\gamma_{1},\ldots,\gamma_{n}]\)) is called _orthogonal_ if \(\Gamma^{\prime}\Gamma=I\). [Thus \(\gamma_{i}^{\prime}\gamma_{j}\) equals 1 or 0 according as \(i=j\) or \(i\neq j\); when \(\gamma_{i}^{\prime}\gamma_{j}=0\) we say that these vectors are orthogonal, and we write \(\gamma_{i}\perp\gamma_{j}\).] Under the orthogonal transformation of \(R_{n}\) onto itself defined by \(y=\Gamma x\), the image of each \(\gamma_{i}\) is the standardized basis vector \(e_{i}\equiv(0,\ldots,0,1,0,\ldots,0)^{\prime}\) with the 1 in the \(i\)th slot.

(ii) Call a symmetric matrix _A positive definite_ (written \(A>0\)) if \(x^{\prime}Ax>0\) for all vectors \(x\neq 0\). Call it _nonnegative definite_ (written \(A\geq 0\)) if \(x^{\prime}Ax\geq 0\) for all vectors \(x\neq 0\).

(iii) If \(A\) is symmetric and _idempotent_ (that is,if \(AA=A\)), then \(A\) is called a _projection matrix_ (the symbol \(P\) is often used for a projection matrix).

(iv) Let \(D_{a}\) be the _diagonal matrix_ with \(d_{ii}=a_{i}\) (and \(d_{ij}=0\) for all \(i\neq j\)).

(v) Let \({\cal R}[A]\) denote the _column space_ of \(A\); that is, it is the set of all vectors that can be written as linear combinations of the column vectors of \(A\).

(vi) Call \(x^{\prime}Ax=\sum_{j=1}^{n}\sum_{i=1}^{n}x_{i}a_{ij}x_{j}\) a _quadratic form_ in the vector \(x\).

What follows is the statistician's main result from linear algebra. We simply state it, then interpret it geometrically in discussion 3.1, and then put it into a very useful format in discussion 3.2.

**Theorem 3.1** (Principal axes theorem)  Let \(A\) denote an arbitrary real and symmetric matrix of rank \(r\).

(a) There exists an orthogonal matrix \(\Gamma\equiv[\gamma_{1},\ldots,\gamma_{n}]\) and a diagonal matrix \(D\) for which we have the representation

\[A=\Gamma D\Gamma^{\prime}\qquad\mbox{and/or}\qquad\Gamma^{\prime}A\Gamma=D \qquad\mbox{with rank}(D)=r. \tag{1}\]

The \(\gamma_{i}\) are called _eigenvectors_, while the corresponding \(d_{ii}\) are called _eigenvalues_. (See (39) below for further comments.)

(b) If \(A>0(A\geq 0)\), then all \(d_{ii}>0\,(d_{ii}\geq 0)\).

We can specify \(\Gamma\) such that \(d_{11}\geq\cdots\geq d_{rr}>0=d_{r+1,r+1}=\cdots=d_{nn}\).

(c) If \(P\) is a projection matrix, then all \(d_{ii}=1\) or 0. Moreover, we must have \(r\equiv\mbox{rank}(A)=\mbox{tr}(D)=\mbox{tr}(A)=\sum_{1}^{n}a_{ii}\).

**Discussion 3.1** (Spectral decomposition)  Consider a projection matrix \(P\) of rank \(r\). Then the transformation \(y=Px\) can be broken down as

\[Px=\Gamma D\Gamma^{\prime}x=[\gamma_{1},\ldots,\gamma_{n}]D[\gamma_{1},\ldots,\gamma_{n}]^{\prime}x=\sum_{1}^{n}d_{ii}(\gamma_{i}^{\prime}x)\gamma_{i}, \tag{2}\]

where \((\gamma_{i}^{\prime}x)\gamma_{i}\) is the projection of \(x\) onto \(\gamma_{i}\) in the direction of \(\gamma_{i}\), and where this term is present when \(d_{ii}=1\) and is absent when \(d_{ii}=0\). Also, \(Px\perp(I-P)x\), where the transformation

\[(I-P)x=\sum_{i=1}^{n}[1-d_{ii}](\gamma_{i}^{\prime}x)\gamma_{i} \tag{3}\]

projects onto \({\cal R}[\gamma_{r+1},\ldots,\gamma_{n}]={\cal R}^{\perp}[\gamma_{1},\ldots, \gamma_{r}]\). Finally,

\[Px=\sum_{i=1}^{r}d_{ii}(\gamma_{i}\gamma_{i}^{\prime})x=[\sum_{i=1}^{r}P_{i}]x \tag{4}\]with \(P_{i}\equiv\gamma_{i}\gamma_{i}^{\prime}\). This is called the _spectral decomposition_ of the transformation \(y=Px\). \(\square\)

**Exercise 3.1** (a) Show that for compatible matrices \(B\) and \(C\),

\[\operatorname{tr}(BC)=\operatorname{tr}(CB)\quad\text{and}\quad\text{rank} \quad(BC)\leq\text{rank}(B)\wedge\text{rank}(C),\]

giving \(\text{rank}(A\Gamma)=\text{rank}(A)\) above.

(b) Prove theorem 3.1(b)(c) using theorem 3.1 (a).

(c) Show that \(\mathcal{R}[A]=\mathcal{R}[AA^{\prime}]\) and \(\mathcal{R}[A^{\prime}]=\mathcal{R}[A^{\prime}A]\).

**Proposition 3.1** (Properties of \(\text{E}(\cdot)\))  (a) It holds that

\[\text{E}(AXB+C)=A\text{E}(X)B+C\quad\text{and}\quad\text{Cov}[AX,BY]=A\text{ Cov}[X,Y]B^{\prime}.\]

(b) Any covariance matrix \(\Sigma_{X}\equiv|[\text{Cov}[X_{i},X_{j}]]|\) satisfies \(\Sigma_{X}\geq 0\).

**Exercise 3.2**  Prove proposition 3.1.

**Discussion 3.2** (**Versions of \(\Sigma^{-}\) and \(\Sigma^{-1/2}\))   Let \(X\cong(\mu,\Sigma)\). According to the principal axes theorem, we can make the decomposition (for any orthogonal matrix \(\Delta\) whatsoever)

\[\begin{split}\Sigma&=\Gamma D\Gamma^{\prime}=\left[ \Gamma\left[\begin{array}{cc}D^{1/2}&0\\ 0&0\end{array}\right]\Delta^{\prime}\right]\ \left[\Delta\left[\begin{array}{cc}D^{1/2}&0\\ 0&0\end{array}\right]\Gamma^{\prime}\right]\\ &=\left[\Gamma\left[\begin{array}{cc}D^{1/2}&0\\ 0&0\end{array}\right]\right]\ \left[\left[\begin{array}{cc}D^{1/2}&0\\ 0&0\end{array}\right]\Gamma^{\prime}\right]=(\Gamma D^{1/2})(D^{1/2}\Gamma^{ \prime})\equiv AA^{\prime},\end{split} \tag{7}\]

where \(D^{1/2}\) has the numbers \(d_{ii}^{1/2}\) on its diagonal and where \(A\) is \(n\times k\). The presence of \(\Delta^{\prime}\Delta\) (which equals \(I\)) shows that this decomposition is not unique. Continuing on gives

\[\Sigma=\Gamma D\Gamma^{\prime}=(\Gamma D\Gamma^{\prime})(\Gamma D \Gamma^{\prime})\equiv\Sigma^{1/2}\Sigma^{1/2}, \tag{8}\] \[\Sigma^{-1/2}\equiv\Gamma D^{-1/2}\Gamma^{\prime},\qquad\text{ where}\qquad d_{ii}^{-1/2}\equiv\left\{\begin{array}{cc}0&\text{if}\ d_{ii}=0,\\ \frac{1}{\sqrt{d_{ii}}}\text{ if }d_{ii}>0,\end{array}\right.\] (9) \[\Sigma^{-}\equiv\Gamma D^{-}\Gamma^{\prime},\qquad\qquad\text{ where}\qquad D^{-}\equiv D^{-1/2}D^{-1/2}. \tag{10}\]

Note that

\[\Sigma\Sigma^{-}\Sigma=\Sigma\qquad\text{and}\qquad\Sigma^{1/2}\Sigma^{-1/2} \Sigma^{1/2}=\Sigma^{1/2}, \tag{11}\] \[\Sigma^{1/2}\Sigma^{-1/2}=\left[\begin{array}{cc}I_{k}&0\\ 0&0\end{array}\right]=\Sigma^{-1/2}\Sigma^{1/2}\qquad\text{and}\qquad\Sigma \Sigma^{-}=\left[\begin{array}{cc}I_{k}&0\\ 0&0\end{array}\right]=\Sigma^{-}\Sigma.\]

These last two results are in keeping with the definition of generalized inverses. \(\square\)

Recall that the _generalized inverse_\(B^{-}\) of the matrix \(B\) is defined to be any matrix \(B^{-}\) that satisfies \(BB^{-}B=B\). A generalized inverse always exists. It has the following interpretation. Fix the matrix \(B\) and the vector \(c\). Then

\[B\beta=c\quad\text{(with any $c\in\mathcal{R}[B]$)}\qquad\text{has the solution}\quad\hat{\beta}=B^{-}c. \tag{12}\]

(It is clear that such a solution does always exist, for a fixed \(c\).) Suppose such a \(B^{-}\) exists, in general; which we accept, and will use freely. Then replace \(c\) in (12) by each column of \(B\), and see that such a \(B^{-}\) must necessarily satisfy \(BB^{-}B=B\).

**Theorem 3.2** (Properties of covariance matrices):

(a) The following results are equivalent for real matrices:

(14) \(\Sigma\) is the covariance matrix of some rv \(Y\).

(15) \(\Sigma\) is symmetric and nonnegative definite.

(16) There exists an \(n\times n\) matrix \(A\) such that \(\Sigma=AA^{\prime}\). (Recall (7) for A.)

(b) The matrix \(\Sigma_{X}\) is positive definite (that is, \(\Sigma_{X}>0\)) if and only a vector \(c\neq 0\) and a constant \(b\) do not exist for which \(c^{\prime}X=b\) a.s.

**Proof.** Now, (14) implies (15): \(\Sigma\) is symmetric, since \({\rm E}Y_{i}Y_{j}={\rm E}Y_{j}Y_{i}\). Also, \(a^{\prime}\Sigma a={\rm Var}[a^{\prime}Y]\geq 0\) for all vectors \(a\), so that \(\Sigma\geq 0\).

Also, (15) implies (16): Just recall (17).

Also, (16) implies (14): Let \(X\equiv(X_{1},\ldots,X_{n})^{\prime}\), where \(X_{1},\ldots,X_{n}\) are independent \(N(0,1)\). Let \(Y\equiv AX\). Then \(Y\) has covariance matrix \(\Sigma=AA^{\prime}\) by (6). \(\Box\)

**Exercise 3.3**  Prove theorem 3.2(b).

**Exercise 3.4**  Let \(X\cong(\theta,\Sigma)\) and let \(B\) be symmetric.

(a) \({\rm E}\{(X-b)^{\prime}B(X-b)\}={\rm tr}(B\Sigma)+(\theta-b)^{\prime}B(\theta- b)\).

(b) If \(\Sigma=\sigma^{2}I\), then \({\rm tr}(B\Sigma)=\sigma^{2}{\rm tr}(B)=\sigma^{2}{\sum_{i=1}^{n}}b_{ii}\).

(c) If \(\Sigma=\sigma^{2}I\) and \(B\) is idempotent, then \({\rm tr}(B\Sigma)=\sigma^{2}{\rm tr}(B)=\sigma^{2}{\rm rank}(B)\).

**Exercise 3.5**  For symmetric \(A\) there exists an upper (or lower) triangular matrix \(H\) for which \(A=HH^{\prime}\). If \(A>0\,({\rm or}\ A\geq 0)\), we may suppose that all \(h_{ii}>0\) (or that all \(h_{ii}\geq 0\)).

**Discussion 3.3** (Best linear predictor and multiple correlation): Consider the partitioned random vector

\[\left[\matrix{Y_{0}\cr Y}\right]\cong\left[\matrix{0\cr 0}\right],\left[\matrix{ \sigma_{00}&\sigma_{0}^{\prime}\cr\sigma_{0}&\Sigma}\right]\qquad{\rm with}\ |\Sigma|\neq 0. \tag{17}\]

The _best linear predictor_ of \(Y_{0}\) based on \(Y\) is

\[\alpha_{0}^{\prime}Y\equiv\sigma_{0}^{\prime}\Sigma^{-1}Y\qquad({\rm or}\ \alpha_{0}\equiv\Sigma^{-1}\sigma_{0}), \tag{18}\]

where "best" is in the sense that

\[{\rm Var}[Y_{0}-\beta^{\prime}Y]\geq{\rm Var}[Y_{0}-\alpha_{0}^{\prime}Y]= \sigma_{00}-\sigma_{0}^{\prime}\Sigma^{-1}\sigma_{0}\qquad{\rm for\ all}\ \beta. \tag{19}\]

In parallel with this,

\[{\rm Corr}[Y_{0},\beta^{\prime}Y]\geq{\rm Corr}[Y_{0},\alpha_{0}^{\prime}Y] \qquad{\rm for\ all}\ \beta. \tag{20}\]

The maximized value of the correlation (that is, the _multiple correlation coefficient_) is given by

\[\rho_{0\cdot 1,\ldots,n}\equiv{\rm Corr}[Y_{0},\alpha_{0}^{\prime}Y]=\sqrt{ \frac{\sigma_{0}^{\prime}\Sigma^{-1}\sigma_{0}}{\sigma_{00}}}, \tag{21}\]

and the variance of the best linear predictor is also easily seen to equal

\[{\rm Var}[Y_{0}-\alpha_{0}^{\prime}Y]=\sigma_{00}-\sigma_{0}^{\prime}\Sigma^{-1 }\sigma_{0}=\sigma_{00}(1-\rho_{0\cdot 1,\ldots,n}^{2}). \tag{22}\][Proof. The first holds, since

\[\begin{split}&\text{Var}[Y_{0}-\beta^{\prime}Y]=\text{Var}[(Y_{0}- \alpha_{0}^{\prime}Y)+(\alpha_{0}^{\prime}-\beta)Y]\\ &\quad=\text{Var}[Y_{0}-\alpha_{0}^{\prime}Y]+2\cdot 0+(\alpha_{0}- \beta)^{\prime}\Sigma(\alpha_{0}-\beta)\\ &\quad\geq\text{Var}[Y_{0}-\alpha_{0}^{\prime}Y].\end{split}\]

The second holds, since

\[\text{Corr}^{2}[Y_{0},\beta Y]=\frac{(\beta^{\prime}\sigma_{0})^{2}}{\sigma_{00 }\ \beta^{\prime}\Sigma\beta}\leq\frac{\sigma_{0}^{\prime}\Sigma^{-1}\sigma_{0}}{ \sigma_{00}},\]

with equality only at \(\beta=c\Sigma^{-1}\sigma_{0}\) (as follows from application of Cauchy-Schwarz).]

**Simple linear regression model** We now want the best linear predictor of \(Y\) based on \(X\). The conditional distribution of \(Y\) given that \(X=x\) is given by

\[Y\big{|}X=x\cong\left(\mu_{Y}+\frac{\sigma_{XY}}{\sigma_{X}^{2}}(x-\mu_{X}), \sigma_{Y}^{2}-\frac{\sigma_{XY}^{2}}{\sigma_{X}^{2}}\right)=(\alpha+\beta x, \sigma_{\epsilon}^{2}),\]

expressing the moments in terms of

\[\rho\equiv\frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}},\quad\beta\equiv\rho\frac{ \sigma_{Y}}{\sigma_{X}},\quad\alpha\equiv\mu_{Y}-\beta\mu_{X},\quad\sigma_{ \epsilon}^{2}\equiv\sigma_{Y}^{2}(1-\rho^{2}).\]

This leads directly to the _simple linear regression model_ that conditionally on \(X=x\) the observations \(Y_{i}\) satisfy

\[Y_{i}=\alpha+\beta\,x_{i}+\epsilon_{i}\quad\text{where}\quad\epsilon_{i}\cong( 0,\sigma_{\epsilon}^{2})\text{ are iid}\]

with

\[\beta=\rho\frac{\sigma_{Y}}{\sigma_{X}},\sigma_{\epsilon}^{2}\equiv\sigma_{Y}^{ 2}(1-\rho^{2}),\quad\alpha\equiv\mu_{Y}-\mu_{X}\beta.\qed\]

**Discussion 3.4** (Conditional moments and projections): Suppose that

\[Y=\left[\begin{array}{c}Y^{(1)}\\ Y^{(2)}\end{array}\right]\cong\left[\begin{bmatrix}\mu^{(1)}\\ \mu^{(2)}\end{bmatrix},\left[\begin{array}{c}\Sigma_{11}&\Sigma_{12}\\ \Sigma_{21}&\Sigma_{22}\end{array}\right]\right].\]

Then the moments of the conditional distribution of \(Y^{(1)}\) given that \(Y^{(2)}=y^{(2)}\) are summarized in

\[Y^{(1)}\big{|}Y^{(2)}=y^{(2)}\ \cong\ (\mu^{(1)}+\Sigma_{12}\Sigma_{22}^{-1}(y^ {(2)}-\mu^{(2)}),\,\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}).\]

To see this, just define

\[Z\equiv\left[\begin{array}{c}Z^{(1)}\\ Z^{(2)}\end{array}\right]\equiv\left[\begin{array}{c}(Y^{(1)}-\mu^{(1)})- \Sigma_{12}\Sigma_{22}^{-1}(Y^{(2)}-\mu^{(2)})\\ Y^{(2)}-\mu^{(2)}\end{array}\right].\]

It is a minor calculation that

\[Z\cong[\vec{0},\Sigma_{Z}]\equiv\left[\left[\begin{array}{c}0\\ 0\end{array}\right],\left[\begin{array}{cc}\Sigma_{Z,11}&\Sigma_{Z,12}\\ \Sigma_{Z,21}&\Sigma_{Z,22}\end{array}\right]\right]=\left[\left[\begin{array} []{cc}0\\ 0\end{array}\right],\left[\begin{array}{cc}\Sigma_{11}-\Sigma_{12}\Sigma_{22}^ {-1}\Sigma_{21}&0\\ 0&\Sigma_{22}\end{array}\right]\right].\]The exercises will show that

\[|\Sigma_{Z}|=|\Sigma_{22}||\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}|. \tag{31}\]

[Proof. It is straightforward to compute \(\Sigma_{Z,12}=\Sigma_{12}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22}=0.\) Trivially, we have \(\Sigma_{Z,22}=\Sigma_{22}\). Then

(a) \[\Sigma_{Z,11}=\Sigma_{12}-2\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22}+\Sigma_{12} \Sigma_{22}^{-1}\Sigma_{22}\Sigma_{22}^{-1}\Sigma_{21}\]

(b) \[=\Sigma_{12}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}.\]

Since \(Y^{(1)}=\mu^{(1)}+Z^{(1)}+\Sigma_{12}\Sigma_{22}^{-1}Z^{(2)}\) with \(Y^{(2)}-\mu^{(2)}=Z^{(2)}\), conditionally

(c) \[Y^{(1)}\Big{|}Y^{(2)}=y^{(2)}\ \cong\ \mu^{(1)}+Z^{(1)}+\Sigma_{12}\Sigma_{22 }^{-1}z^{(2)}\]

\[\cong(\mu^{(1)}+\Sigma_{12}\Sigma_{22}^{-1}z^{(2)},\Sigma_{Z,11})\]

\[=(\mu^{(1)}+\Sigma_{12}\Sigma_{22}^{-1}(y^{(2)}-\mu^{(2)}),\Sigma_{Z,11}),\]

as required.] [See exercise 3.7 below for (31).] 

**Exercise 3.6**  Consider the rvs \(Z^{(1)}\) and \(Z^{(2)}\) in (29). Suppose \(\mu^{(1)}=0\) and \(\mu^{(2)}=0\). Let \({\cal H}^{0}_{1}\) and \({\cal H}^{0}_{2}\) denote the Hilbert spaces generated by the rv subsets \(Y^{(1)}\equiv(Y_{1},\ldots,Y_{k})\) and \(Y^{(2)}\equiv(Y_{k+1},\ldots,Y_{n})\), respectively. Show that \(Z^{(1)}\) is the projection of \(Y^{(1)}\) into the Hilbert space \(({\cal H}^{0}_{2})^{\perp}\). (See (B.3.1).)

**Discussion 3.5** (Partitioned matrices): Let

\[A\equiv\left[\begin{array}{cc}A_{11}&A_{12}\\ A_{21}&A_{22}\end{array}\right],\quad\mbox{and write}\qquad A^{-1}\equiv\left[ \begin{array}{cc}A^{11}A^{12}\\ A^{21}A^{22}\end{array}\right] \tag{32}\]

when the inverse exists. We agree that \(A_{11}\) is \(k\times k\).

**Exercise 3.7** (a) If \(|A_{22}|\neq 0\), show that \(|A|=|A_{22}||A_{11}-A_{12}A_{22}^{-1}A_{21}|\).

(b) If \(|A|\neq 0\), show that \(|A+xy^{\prime}|=|A|(1+y^{\prime}A^{-1}x)\) for all vectors \(x,y.\) [Hint. Appeal to

\[\left[\begin{array}{cc}C&0\\ E&D\end{array}\right]=|C||D|\qquad\mbox{and work with}\qquad B\equiv\left[ \begin{array}{cc}I&0\\ -A_{12}A_{22}^{-1}&I\end{array}\right]\]

for appropriate choices.]

**Exercise 3.8** (a) Show that for a symmetric \(A\) having \(|A_{11}|\neq 0\) and \(|A_{22}|\neq 0\):

\[A^{11}=(A_{11}-A_{12}A_{22}^{-1}A_{21})^{-1}\quad\mbox{and}\quad A ^{12}=-A_{11}^{-1}A_{12}A^{22}. \tag{34}\] \[A^{22}=(A_{22}-A_{21}A_{11}^{-1}A_{12})^{-1}\quad\mbox{and}\quad A ^{21}=-A_{22}^{-1}A_{21}A^{11}. \tag{33}\]

[Hint. Start multiplying the partitioned form of \(A\)\(A^{-1}=I\).]

(b) Obtain analogous formulas from \(A^{-1}A=I\).

(c) Show that

\[A^{11}A_{11}+A^{12}A_{21}=I\quad\mbox{and}\quad A_{11}A^{12}+A_{12}A^{22}=0. \tag{35}\]

**Exercise 3.9**  Show that for symmetric \(A\),

\[\frac{\partial}{\partial\beta}[\beta^{\prime}\ A\ \beta]=2A\beta. \tag{36}\]

**Discussion 3.6** (Simultaneous decomposition)  For a real symmetric matrix \(A\) that is non-negative definite (that is, \(A\geq 0\)) we wrote

\[A=\Gamma D\Gamma^{\prime}\qquad\mbox{and}\qquad\Gamma^{\prime}A\Gamma=D \tag{37}\]

with \(d_{11}\geq\cdots\geq d_{rr}>0\).

(A) We note that

\[|A-\lambda I|=|\Gamma||D-\lambda I||\Gamma^{\prime}|=|D-\lambda I|=0 \tag{38}\]

all have the same solutions \(d_{11},\ldots,d_{rr},0\), and thus \(d_{11},\ldots,d_{rr}\) are indeed the nonzero eigenvalues of \(A\). Moreover, (37) gives

\[A\Gamma=\Gamma D\quad\mbox{or}\quad A\gamma_{i}=d_{ii}\gamma_{i}\quad\mbox{ for}\quad 1\leq i\leq r, \tag{39}\]

so that \(\gamma_{1},\ldots,\gamma_{r}\) are the corresponding eigenvectors.

(B) Suppose \(A>0\) and \(B\geq 0\). Then

\[\begin{array}{l}|B-\lambda A|=0,\quad|A^{-1/2}BA^{-1/2}-\lambda I|=0,\quad |A^{-1}B-\lambda I|=0\\ \mbox{all have the same solutions}\,\lambda.\end{array} \tag{40}\]

[Just note that \(|B-\lambda A|=|A^{1/2}||A^{-1/2}BA^{-1/2}-\lambda I||A^{1/2}|\) and \(|A^{-1}||B-\lambda A|=|A^{-1}B-\lambda I|\).] Writing \(A^{-1/2}BA^{-1/2}=\Delta D_{\theta}\Delta^{\prime}\) with \(\Delta\) orthogonal gives

\[B=(A^{1/A}\Delta)D_{\theta}(\Delta^{\prime}A^{1/2})\quad\mbox{and}\quad A=(A^{1 /2}\Delta)(\Delta^{\prime}A^{1/2}). \tag{41}\]

This last formula is called the _simultaneous decomposition_ of \(A\) and \(B\). \(\square\)

**Discussion 3.7** (a) (Cauchy-Schwarz)  For all vectors \(x,y\):

\[(x^{\prime}y)^{2}\leq\|x\|^{2}\|y\|^{2}, \tag{42}\]

with equality (for \(y\neq 0\)) if and only if \(x=cy\) for some constant \(c\).

(b) For any real symmetric matrix \(A>0\)

\[\max_{a\neq 0}\frac{a^{\prime}Aa}{a^{\prime}a}=d_{11}(A) \tag{43}\]

(as follows immediately from (1), with \(d_{11}(A)\) the largest eigenvalue of \(A\)).

(c) Let \(A>0\), and fix \(C\geq 0\) and \(b\neq 0\). Then

\[(x^{\prime}y)^{2}\leq(x^{\prime}\ A\ x)(y^{\prime}A^{-1}y), \tag{44}\]

with equality (when \(y\neq 0\)) if and only if \(x=c\,A^{-1}y\) for some \(c\). Also,

\[\min_{a^{\prime}1=1}\{a^{\prime}\ A\ a\}=1/(1^{\prime}A^{-1}1), \tag{45}\]with equality only at \(a_{0}\equiv A^{-1}1/(1^{\prime}A^{-1}1)\). Also,

\[\max_{a\neq 0}\frac{(a^{\prime}b)^{2}}{a^{\prime}Aa}=b^{\prime}A^{-1}b, \tag{46}\]

with equality only when \(a=(\mbox{some}\,c)A^{-1}b\). Also,

\[\max_{a\neq 0}\frac{a^{\prime}Ca}{a^{\prime}Aa}=\max_{a\neq 0} \frac{(a^{\prime}A^{1/2})(A^{-1/2}CA^{-1/2})(A^{1/2}a)}{(a^{\prime}A^{1/2})(A^ {1/2}a)} \tag{47}\] \[=d_{11}(A^{-1/2}CA^{-1/2})=d_{11}(CA^{-1}).\]

Here, \((a^{\prime}Ca)=(a^{\prime}b)^{2}\) is an important special case (already solved via (46)).

(d) Let \(A>0\), let \(B^{k\times n}\) have rank \((B)=k\), and let \(b^{k\times 1}\neq 0\). Then

\[\min_{Ba=b}\{a^{\prime}Aa\}=b^{\prime}[BA^{-1}B^{\prime}]^{-1}b,\mbox{ is achieved at }a_{0}\equiv B^{\prime}[BA^{-1}B^{\prime}]^{-1}b. \tag{48}\]

**Exercise 3.10** : Prove (42)-(47) (the equality in (42) needs some attention). [The harder (48) is proven below.]

**Proof.** : Consider (48). Now,

\[a^{\prime}Aa\geq\frac{(a^{\prime}y)^{2}}{y^{\prime}A^{-1}y}\qquad\mbox{for all }y\neq 0\]

(a) \[=\frac{[a^{\prime}B^{\prime}[BA^{-1}B^{\prime}]^{-1}b]^{2}}{b^{\prime}[BA^{-1} B^{\prime}]^{-1}(BA^{-1}B^{\prime})[BA^{-1}B^{\prime}]^{-1}b}\qquad\mbox{if }y\equiv B^{\prime}[BA^{-1}B^{\prime}]^{-1}b\] (b) \[=\frac{[b^{\prime}[BA^{-1}B^{\prime}]^{-1}b]^{2}}{b^{\prime}[BA^{-1}B^{ \prime}]^{-1}b}\qquad\mbox{for all }a,\mbox{ using }Ba=b\]

\[=b^{\prime}[BA^{-1}B^{\prime}]^{-1}b\]

yielding a bound not depending on \(a\), which proves (48). \(\Box\)

**Discussion 3.8** (General Linear Model): Consider the general linear model

\[Y=X\beta+\epsilon\equiv\theta+\epsilon, \tag{49}\]

where \(X^{n\times p}\) is a matrix of known constants, where \(\beta^{p\times 1}\) is a vector of unknown parameters, and where the rv \(\epsilon^{n\times 1}\cong(\vec{0},\sigma^{2}I)\) with \(\sigma^{2}\) unknown. Recall that

\[{\cal R}[X]\equiv\mbox{ (the \emph{column space} of $X$)}=\{y:y=Xa\mbox{ with any vector }a\} \tag{50}\]

is a vector space (of rank \(r\), say). Noting that \(\theta=X\beta\in{\cal R}[X]\), the _least squares estimator_ (or LSE) of \(\theta\) is defined to be that value \(\hat{\theta}\) in \({\cal R}[X]\) that minimizes

\[\|\epsilon\|^{2}=\|Y-X\beta\|^{2}=\|Y-\theta\|^{2}. \tag{51}\]

This minimization clearly occurs when \(\hat{\theta}\) is the projection of \(Y\) onto \({\cal R}[X]\); so

\[\hat{\theta}=(\mbox{the unique projection of $Y$ onto ${\cal R}[X]$})=(\mbox{the unique LSE $\hat{\theta}$ of $\theta$}). \tag{52}\]We note that \(\hat{\beta}\) need not be unique, since

(53) any

\[\hat{\beta}\]

 for which

\[X\hat{\beta}=\hat{\theta}\]

 gives this same LSE

\[\hat{\theta}\]

Since

\[(Y-\hat{\theta})\]

 is

\[\bot\]

 to

\[\mathcal{R}[X]\]

, it must be that

\[\hat{\theta}\]

 and

\[\hat{\beta}\]

 satisfies the _normal equations_ (54) \[X^{\prime}(Y-\hat{\theta})=\vec{0},\qquad\mbox{or equivalently}\qquad X^{ \prime}X\hat{\beta}=X^{\prime}Y.\]

Conversely, suppose \(\hat{\beta}\) satisfies the normal equations. Then \(X\hat{\beta}\in\mathcal{R}[X]\) with \(X^{\prime}(Y-X\hat{\beta})=\vec{0}\), showing that \((Y-X\hat{\beta})\in\mathcal{R}[X]^{\bot}\); and thus \(X\hat{\beta}\) and \((Y-X\hat{\beta})\) must be the projections of \(Y\) onto the spaces \(\mathcal{R}[X]\) and \(\mathcal{R}[X]^{\bot}\), respectively. Thus

\[X\hat{\beta}=\hat{\theta}\equiv\mbox{ (the unique projection of $Y$ onto $\mathcal{R}[X]$)}\quad\mbox{iff}\quad X^{\prime}X\hat{\beta}=X^{\prime}Y. \tag{55}\]

Let \(\Omega\equiv\mathcal{R}[X]\), let \(r\equiv\mbox{rank}(X)\), and let \(P_{\Omega}\equiv\) (the projection matrix onto \(\Omega\)). We will next prove that:

\[P_{\Omega}=X(X^{\prime}X)^{-}X^{\prime}\equiv\mbox{ (the {\it hat\ matrix}), with $P_{\Omega}$ and $(I-P_{\Omega})$ idempotent.} \tag{57}\] \[\hat{\theta}\equiv\hat{Y}\equiv\mbox{ ( the {\it fitted\ value})=[X(X^{\prime}X)^{-}X^{\prime}]Y=P_{\Omega}Y.}\] (58) \[\hat{\epsilon}\equiv\mbox{(the {\it residuals})}\equiv(Y-\hat{Y})=[I-P_{\Omega}]Y=P_{\Omega^{\bot}}Y.\] (59) \[\mbox{E}\hat{\theta}=\theta,\mbox{E}\|\hat{\epsilon}\|^{2}=\mbox{E }\|Y-\hat{Y}\|^{2}=(n-r)\sigma^{2}\quad\mbox{and}\quad\mbox{Cov}[\hat{\theta}-\theta,Y-\hat{\theta}]=\vec{0}. \tag{56}\]

Call \(RSS\equiv\|\hat{\epsilon}\|^{2}=\hat{\epsilon}^{\prime}\hat{\epsilon}=\|Y-\hat{Y}\|^{2}=Y^{\prime}[I-P_{\Omega}]Y\) the _residual sum of squares_.

Proof. Let \(B\equiv X^{\prime}X\), with \(\mathcal{R}[B]=\mathcal{R}[X^{\prime}]\) (by exercise 3.1(c)); and then define \(c\equiv X^{\prime}Y\in\mathcal{R}[X^{\prime}]\). Then (by (13)) the projection of \(Y\) onto \(\Omega\) is given by \(P_{\Omega}Y=\hat{\theta}=X\hat{\beta}=XB^{-}c=X(X^{\prime}X)^{-}X^{\prime}Y\); so \(P_{\Omega}=X(X^{\prime}X)^{-}X^{\prime}\). Also, \(\mbox{E}(\hat{\theta})=\mbox{E}(P_{\Omega}Y)=P_{\Omega}\mbox{E}Y=P_{\Omega} \theta=\theta\). Finally, the residuals satisfy \(\|Y-\hat{Y}\|^{2}=(Y-\hat{Y})^{\prime}(Y-\hat{Y})=Y^{\prime}(I-P_{\Omega})^{ \prime}(I-P_{\Omega})Y=Y^{\prime}(I-P_{\Omega})Y\), with expectation (given by exercise 3.4) \(\sigma^{2}\mbox{tr}(I-P_{\Omega})+\theta^{\prime}(I-P_{\Omega})\theta=\sigma^{2} \mbox{tr}(I-P_{\Omega})+\theta^{\prime}\vec{0}=(n-r)\sigma^{2}\).

When \(\mbox{rank}(\mbox{X})=p\), then \(Xb=\hat{\theta}\) has a _unique_ solution \(\hat{\beta}\). Moreover,

\[\hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}Y \tag{60}\]

is the unique solution of the normal equations (\(\hat{\beta}\) is now called the LSE of \(\beta\)), and

\[\mbox{E}\hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}X\beta=\beta,\mbox{ so that }\hat{\beta}\mbox{ is an {\it unbiased\ estimator} of $\beta$, and} \tag{62}\] \[\Sigma_{\hat{\beta}}=(X^{\prime}X)^{-1}X^{\prime}(\sigma^{2}I)X(X^{ \prime}X)^{-1}=\sigma^{2}(X^{\prime}X)^{-1},\] with (63) \[\mbox{E}S^{2}=\sigma^{2}\mbox{ for }S^{2}\equiv\|Y-\hat{Y}\|^{2}/(n-p),\mbox{ so that }S^{2}\mbox{ is unbiased for }\sigma^{2}. \tag{61}\]

We thus say that \(\beta\) is _identifiable_ (and _estimable_) in this _full rank case_ when \(\mbox{rank}(X)=p\) (that is, when \(X\) is non-singular, or \(|X|\neq 0\)). \(\Box\)

**Exercise 3.11** (Gauss-Markov) Let \(Y=X\beta+\epsilon=\theta+\epsilon\) (as in (49)) with the rv \(\epsilon\cong(0,\sigma^{2}I)\) and with \(\mbox{rank}(X)=r\). Consider some \(c^{\prime}\theta(=c^{\prime}X\beta)\). Show that among the class of all linear unbiased estimators of \(c^{\prime}\theta\), the estimator \(c^{\prime}\hat{\theta}\) is the unique one having minimum variance (so, it is _best_). Determine its variance.

**Exercise 3.12** (Distribution theory under normality): (a) Let \(Y=X\beta+\epsilon=\theta+\epsilon\) (as in equation (49)) with \(\epsilon\cong N(0,\sigma^{2}I)\), and with \(r\equiv\operatorname{rank}(X)\). Show that

\[(\hat{\beta}-\beta)^{\prime}(X^{\prime}X)(\hat{\beta}-\beta)=\|X( \hat{\beta}-\beta)\|^{2}=\|\hat{\theta}-\theta\|^{2}\cong\sigma^{2}\text{ Chisquare}_{r}. \tag{65}\] \[\|\hat{\epsilon}\|^{2}=\|Y-\hat{Y}\|^{2}=\|Y-\hat{\theta}\|^{2} \cong\sigma^{2}\text{Chisquare}_{n-r},\] or (66) \[RSS/(n-r)=\|Y-\hat{Y}\|^{2}/(n-r)\cong\sigma^{2}\text{Chisquare}_{n-r}/(n-r),\] and (67) \[\hat{\theta}\text{ and }\hat{\epsilon}=(Y-\hat{Y})=(Y-\hat{\theta})\text{ are uncorrelated.} \tag{64}\]

When \(r\equiv\operatorname{rank}(X)=p\) (so that \(\hat{\beta}\) is unique, identifiable, and estimable), show that

(68) \[\hat{\beta}\cong N(\beta,\sigma^{2}(X^{\prime}X)^{-1})\] and (69) \[\hat{\beta}\text{ is independent of }\hat{\epsilon}=(Y-\hat{Y})=(Y-\hat{\theta}),\text{ and hence of }\|Y-\hat{\theta}\|^{2}\] also.

(b) Suppose instead that \(\epsilon\cong(\vec{0},V)\), with \(\operatorname{rank}(\text{V})=n\). Define \(Z\equiv V^{-1/2}Y,\) and show that this \(Z\) satisfies the linear model equation \(Z=X^{*}\beta+\epsilon^{*}\) where \(X^{*}\equiv V^{-1/2}X\) and \(\epsilon^{*}\cong(0,\sigma^{2}I)\). So, analogs of all the formulas in (a) are trivial.

**Exercise 3.13** (Alternative minimization in the general linear model): In the context of the model \(Y=X\beta+\epsilon=\theta+\epsilon\) (as in (49)) we now let \(\tilde{\theta}\) denote that \(\theta\in\mathcal{R}[X]\) that minimizes (for some positive definite covariance matrix \(M\))

\[\|\epsilon\|_{M}^{2}\equiv\epsilon^{\prime}M\epsilon=\|Y-\theta\|_{M}^{2}=\|Y-X \beta\|_{M}^{2}=(Y-X\beta)^{\prime}M(Y-X\beta) \tag{70}\]

(instead of minimizing \(\|\epsilon\|^{2}\) (as in (49))). Show that \((Y-\tilde{\theta})\perp_{M}\mathcal{R}[X]\), and so this resulting weighted LSE \(\tilde{\theta}=X\hat{\beta}\) must satisfy the _weighted normal equations_

\[X^{\prime}M\tilde{\theta}=X^{\prime}MY,\qquad\text{(equivalently,}X^{\prime} MX\tilde{\beta}=X^{\prime}MY). \tag{71}\]

Summary: \(X\tilde{\beta}=\tilde{\theta}\) if and only if \(\tilde{\beta}\) satisfies the weighted normal equations. Also,

\[\tilde{\beta}=(X^{\prime}MX)^{-}(X^{\prime}MY)\text{ does satisfy the weighted normal equations and} \tag{73}\] \[\tilde{\theta}=X\tilde{\beta}=[X(X^{\prime}MX)^{-}X^{\prime}M]Y \equiv P_{\Omega}Y\text{ projects }Y\text{ onto }\Omega\equiv\mathcal{R}[X]. \tag{72}\]

**Exercise 3.14** (Minimum variance unbiased linear estimators): (a) Let \(X_{1},\ldots,X_{n}\) be uncorrelated with common mean \(\mu\) and common finite variance \(\sigma^{2}\). All linear estimators \(T\equiv\sum_{1}^{n}a_{i}X_{i}\) having \(\sum_{1}^{n}a_{i}=1\) are unbiased estimators of \(\mu\) (that is, \(\text{E}\hskip 1.0ptT\hskip-1.0pt=\mu\)). Show that the choice with all \(a_{i}=1/n\) has minimum variance within this class of linear unbiased estimators. (b) Determine the minimum variance unbiased linear estimator of the common mean \(\mu\) when the variances are \(\sigma^{2}/c_{1},\ldots,\sigma_{n}^{2}/c_{n}\), with the \(c_{k}\) being known constants.

## 4 The Multivariate Normal Distribution

**Definition 4.1** (Jointly normal): Call \(Y=(Y_{1},\ldots,Y_{n})^{\prime}\)_jointly normal with \(0\) means_ if there exist iid \(N(0,1)\) rvs \(X_{1},\ldots,X_{k}\) and an \(n\times k\) matrix \(A\) of known constants for which \(Y=AX\). [We again write \(Y\) in this section, rather than \(\vec{Y}\), when the context seems clear.] Note that the \(n\times n\)_covariance matrix_\(\Sigma_{Y}\equiv\Sigma\) of the random vector \(Y\) is

\[\Sigma\equiv\Sigma_{Y}=\mathrm{E}YY^{\prime}=\mathrm{E}AXX^{\prime}A^{\prime}= AA^{\prime}. \tag{1}\]

The covariance matrix of \(X\) is the \(k\times k\) identity matrix \(I_{k}\). We will write \(X\cong N(0,I_{k})\), and we will write \(Y\cong N(0,\Sigma)\). Then write \(Y\cong N(\mu,\Sigma)\) if \(Y-\mu\cong N(0,\Sigma)\). Call \(Y\)_Multivariate Normal_ with mean vector \(\mu\) and covariances matrix \(\Sigma\), or just _Normal_ with mean vector \(\mu\) and covariance matrix \(\Sigma\). Call \(Y\)_nondegenerate_ when \(|\Sigma|\neq 0\)(that is, the determinant of \(\Sigma\) is not equal to \(0\)). Say that \(Y_{1},\ldots,Y_{n}\) are _linearly independent_ if (rank \(\Sigma\)) \(=\)\(n\). Of course, this means that

\[Y\]

 is nondegenerate if and only if \[\mathrm{rank}(A)=n.\] (2)

Now, \(\Sigma\) is symmetric. Also \(a\Sigma a^{\prime}=\mathrm{Var}[aY]\geq 0\) for all vectors \(a\). When \(a\Sigma a^{\prime}\geq 0\) for all vectors \(a\), the symmetric matrix \(\Sigma\) is called _nonnegative definite_, and one writes \(\Sigma\geq 0\).

**Theorem 4.1** (Densities): If \(Y\cong N(0,\Sigma)\) is nondegenerate, then \(Y\) has density (with respect to Lebesgue measure on \(R_{n}\)) given by

\[f_{Y}(y)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp(-y^{\prime}\Sigma^{-1}y/2) \quad\text{for all $y\in R_{n}$}. \tag{3}\]

[Note that each possible Normal distribution is completely determined by \(\mu\) and \(\Sigma\).]

**Proof.** Now, \(Y=XA\), where \(AA^{\prime}=\Sigma\), \((\mathrm{rank}A)=n\), \(|A|\neq 0,X\cong N(0,I_{k})\). It is trivial that

\[P(X\in B_{n})=\int 1_{B_{n}}f_{X}(x)\,dx\qquad\text{with}\qquad f_{X}(x)\equiv (2\pi)^{-n/2}\exp(-x^{\prime}x/2).\] (a)

Thus \(X=A^{-1}Y\) gives

\[P(Y\in B_{n})=P(AX\in B_{n})=P(X\in A^{-1}B_{n})=\int 1_{A^{-1}B_{n}}(x)f_{X}(x)\,dx\] \[\quad=\int 1_{A^{-1}B_{n}}(A^{-1}y)f_{X}(A^{-1}y)|\tfrac{ \partial x}{\partial y}|^{+}\,dy\] \[\quad=\int 1_{B_{n}}(y)(2\pi)^{-n/2}\exp(-(A^{-1}y)^{\prime}(A^{-1}y)/2)|\frac{\partial x}{\partial y}|^{+}\,dy\] (b) \[\quad=\int_{B_{n}}(2\pi)^{-n/2}|\Sigma|^{-1/2}\exp(-y^{\prime} \Sigma^{-1}y/2)\,dy,\]

since \((A^{-1})^{\prime}(A^{-1})=(AA^{\prime})^{-1}=\Sigma^{-1}\) and

\[|\frac{\partial x}{\partial y}|^{+}=|A^{-1}|^{+}=\sqrt{|A^{\prime-1}||A^{-1}|}=\sqrt{|\Sigma^{-1}|}=1/\sqrt{|\Sigma|}.\] (c)

This is the required statement.

**Theorem 4.2** (Characteristic functions and representations): (a) If we are given a random vector \(Y=A^{n\times k}X^{k\times 1}\) where \(X\cong N(0,I_{k})\), we have

\[\phi_{Y}(t)\equiv{\rm E}e^{it^{\prime}Y}=\exp(-t^{\prime}\Sigma t/2) \tag{4}\]

with \(\Sigma\equiv AA^{\prime}\) and \({\rm rank}(\Sigma)={\rm rank}(A)\).

(b) If \(Y\) has characteristic function \(\phi_{Y}(t)\equiv{\rm E}e^{itY}=\exp(-t^{\prime}\Sigma t/2)\) with \(\Sigma\geq 0\) of rank \(k\), then

\[Y\cong A^{n\times k}X^{k\times 1}\qquad\mbox{with (rank $A$)}=k\mbox{ and }X \cong N(0,I). \tag{5}\]

(Thus the number of independent rvs \(X_{i}\)'s needed is equal to the rank of \(A\).)

**Proof.** Our proof will use the fact that the characteristic function \(\phi_{Y}\) of any rv \(Y\) is unique (this is shown in theorem 9.4.1.) [When a density function does not exist, one can use this characteristic function for many of the same purposes.] We observe that

\[\phi_{Y}(t)={\rm E}\exp(it^{\prime}AX)={\rm E}\exp(i(A^{\prime}t)^{\prime}X)\]

(a) \[=\exp(-(A^{\prime}t)^{\prime}(A^{\prime}t)/2)\]

\[\mbox{since ${\rm E}e^{iux_{j}}=\exp(-u^{2}/2)$ for any $u$, by (\ref{eq:1})}\]

(b) \[=\exp(-t^{\prime}(AA^{\prime})t/2).\]

The converse follows from (A.3.7). \(\Box\)

Even when a Multivariate Normal rv\(Y\) does not have a density, the characteristic function can often be manipulated to establish a desired result.

**Theorem 4.3** (Marginals, independence, and linear combinations): Suppose that

\[Y=(Y_{1},\ldots,Y_{k},Y_{k+1},\ldots,Y_{n})^{\prime}\cong N(0,\Sigma)\qquad \mbox{with}\qquad\Sigma\equiv\pmatrix{\Sigma_{11}&\Sigma_{12}\cr\Sigma_{21}& \Sigma_{22}}\]

(i) The marginal covariance matrix of \((Y_{1},\ldots,Y_{k})^{\prime}\) is the \(k\times k\) matrix \(\Sigma_{11}\), and

\[(Y_{1},\ldots,Y_{k})^{\prime}\cong N(0,\Sigma_{11}). \tag{6}\]

(ii) If \(\Sigma_{12}=0\), then \((Y_{1},\ldots,Y_{k})^{\prime}\) and \((Y_{k+1},\ldots,Y_{n})^{\prime}\) are independent.

(iii) If \((Y_{1},Y_{2})\) is a jointly normal rv, then \(Y_{1}\) and \(Y_{2}\) are independent if and only if they have the zero covariance \({\rm Cov}[Y_{1},Y_{2}]=0\).

(iv) Linear combinations of Multivariate Normals are Multivariate Normal.

**Proof.** (i) Use the first \(k\) coordinates of the representation \(Y=AX\).

(ii) Use the fact that one can factor

\[\phi_{Y}(t)=\exp(-\frac{1}{2}t^{\prime}\pmatrix{\Sigma_{11}&0\cr 0&\Sigma_{22}}t).\]

(iii) Just apply (ii), as the other direction is trivial.

(iv) \(Z^{m\times 1}\equiv B^{m\times n}Y^{n\times 1}=B(AX)=(BA)X\). \(\Box\)

**Theorem 4.4** (Conditional distributions): If

\[Y=\left[\matrix{Y^{(1)}\cr Y^{(2)}\cr}\right]\cong N\left[\left[\matrix{\mu^{(1)} \cr\mu^{(2)}\cr}\right],\left[\matrix{\Sigma_{11}&\Sigma_{12}\cr\Sigma_{21}& \Sigma_{22}\cr}\right]\right], \tag{7}\]

then

\[Y^{(1)}\big{|}Y^{(2)}=y^{(2)}\cong N(\mu^{(1)}+\Sigma_{12}\Sigma_{22}^{-1}(y^{ (2)}-\mu^{(2)}),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}). \tag{8}\]

Note that

\[|\Sigma|=|\Sigma_{22}||\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}|. \tag{9}\]

Proof.: The vector

\[Z\equiv\left[\matrix{Z^{(1)}\cr Z^{(2)}\cr}\right]\equiv\left[\matrix{(Y^{(1 )}-\mu^{(1)})-\Sigma_{12}\Sigma_{22}^{-1}(Y^{(2)}-\mu^{(2)})\cr(Y^{(2)}-\mu^{ (2)})\cr}\right] \tag{10}\]

is just a linear combination of the \(Y_{i}\)'s, and so it is Normal. We need only verify the means and variances. But we did this in discussion A.3.4. 

**Exercise 4.1**: Show that \((Y_{1},Y_{2})\) can have Normal marginals without being jointly normal. [Hint. Consider starting with a joint \(N(0,I)\) density on \(R_{2}\) and move mass in a symmetric fashion to make the joint distribution nonnormal, but still keeping the marginals Normal.]

**Quadratic Forms**

**Exercise 4.2**: Let \(Y^{n\times 1}\cong N(0,I)\), and suppose that \(A\) is symmetric and of rank \(r\). Then \(Y^{\prime}AY\cong\chi_{r}^{2}\) if and only if \(A\) is a projection matrix (that is, \(A^{2}=A\)).

**Exercise 4.3**: Let \(Y^{n\times 1}\cong N(0,I)\). Suppose that \(A\) and \(B\) are symmetric and both \(Y^{\prime}AY\) and \(Y^{\prime}BY\) have chisquare distributions. Show that \(Y^{\prime}AY\) and \(Y^{\prime}BY\) are independent if and only if \(AB=0\).

**Exercise 4.4**: Suppose \(A\) and \(B\) are \(n\times n\) projection matrices with ranks \(r_{A}\) and \(r_{B}\), and suppose \(AB=0\) and \(I-A-B\geq 0\). Then:

(a) \(I-A\) is a projection matrix of rank \(n-r_{A}\).

(b) \(I-A-B\) is a projection matrix of rank \(n-r_{A}-r_{B}\).

**Exercise 4.5**: Suppose \(Y^{n\times 1}\cong N(0,\Sigma)\), and let \(A\) be an arbitrary symmetric matrix of rank \(r\). Show that \(Y^{\prime}AY\cong\chi_{r}^{2}\) if and only if \(A\Sigma A=A\).

**The Multivariate CLT**

The following result is theorem 14.1.3, but we also list it here for convenient referral.

**Theorem 4.5**: Suppose that the random vectors \(X_{1},\ldots,X_{n}\) are iid \((\mu,\Sigma)\). Then

\[\sqrt{n}(\bar{X}_{n}-\mu)\rightarrow_{d}N(0,\Sigma)\qquad\hbox{as $n\rightarrow\infty$}. \tag{11}\]

#### Normal Processes

To specify a normal process, we must specify consistent distributions (in the sense of Kolmogorov's consistency theorem). But \(\mu\) and \(\Sigma\) completely specify \(N(\mu,\Sigma)\), while the marginals of \(N(\mu,\Sigma)\) are \(N(\mu^{(1)},\Sigma_{11})\). Thus a normal process exists, provided only that the mean value function \(\mu(\cdot)\) on \(I\) and the covariance function \(\operatorname{Cov}(\cdot,\cdot)\) on \(I\times I\) are well-defined and are such that \(\operatorname{Cov}(\cdot,\cdot)\) is _nonnegative definite_ (meaning that every \(n\)-dimensional covariance matrix formed from it is nonnegative definite).

We call \(\{\mathbb{S}(t):0\leq t<\infty\}\) a _Brownian motion_ if \(\mathbb{S}\) is a normal process having

\[\operatorname{ES}(t)=0\qquad\text{and}\qquad\operatorname{Cov}[\mathbb{S}(s), \mathbb{S}(t)]=s\wedge t\qquad\text{for all }s,t\geq 0. \tag{12}\]

Since this covariance function is nonnegative definite, a version of the process \(\mathbb{S}\) exists on \((R_{[0,\infty)},\mathcal{B}_{[0,\infty)})\) by the Kolmogorov consistency condition. Then

\[\mathbb{U}(t)\equiv-[\mathbb{S}(t)-t\,\mathbb{S}(1)]\quad\text{for all }0\leq t \leq 1\text{ is called \emph{Brownian bridge}}. \tag{13}\]

It is a normal process on \((R_{[0,1]},\mathcal{B}_{[0,1]})\) for which

\[\operatorname{EU}(t)=0\qquad\text{and}\qquad\operatorname{Cov}[\mathbb{U}(s), \mathbb{U}(t)]=s\wedge t-st\qquad\text{for all }0\leq s,t\leq 1. \tag{14}\]

## Appendix B General Topology and Hilbert Space 1
Footnote 1: For a discussion of the results of [10], see also [11].

## 1 General Topology 2
Footnote 2: For a discussion of the results of [10], see also [11].

Only the definitions and the statements of the major results are presented. (Only a small amount will be referenced; so see this as a handy summary.)

**Definition 1.1** (Topology, open set, neighborhood, and boundary):

(a) A _topological space_ (or a _space_) is a pair \((M,\mathcal{U})\) where \(M\) is a set and \(\mathcal{U}\) is a family of subsets that satisfies

\[\begin{array}{l}\cup\mathcal{V}\equiv\bigcup_{V\in\mathcal{V}}\!V\in\mathcal{ U},\quad\text{for any $\mathcal{V}\subset\mathcal{U}$},\\ \cap\mathcal{V}\in\mathcal{U}\qquad\text{for any finite $\mathcal{V}\subset \mathcal{U}$},\qquad\text{and}\qquad\emptyset\text{ and $M$ are in $\mathcal{U}$}.\end{array} \tag{1}\]

\(\mathcal{U}\) is called a _topology_, and the sets in \(\mathcal{U}\) are called _open sets_.

The complements of all open sets are called _closed sets_.

An open set \(U\) containing a point \(x\in M\) is called a _neighborhood_ of \(x\).

(b) The _interior_\(A^{\circ}\) of \(A\) is int \((A)\equiv A^{\circ}\equiv\cup\{\ U:U\subset A\) with \(U\) open\(\}\).

The _closure_\(\bar{A}\) of \(A\) is defined by \(\mathrm{c1}(A)\equiv\bar{A}\equiv\cap\{\ F:F\supset A\) with \(F\) closed\(\}\).

The _boundary_\(\partial A\) of \(A\) is \(\partial A\equiv\bar{A}\backslash A^{\circ}\).

(c) Call \(x\) an _accumulation point_ of \(A\) if \(\overline{A\backslash\{x\}}\) contains \(x\).

**Exercise 1.1**: (a) Show that \(\bar{A}=\{x:\) every neighborhood \(U\) of \(x\) has \(U\cap A\neq\ \emptyset\}\).

(b) Show that \(\overline{A\cup B}=\bar{A}\cup\bar{B}\), while \(A\cap B\subset\bar{A}\cap\bar{B}\).

(c) Show that \(A^{\circ}=M\backslash\overline{A^{c}}\), and that \((A\cap B)^{\circ}=A^{\circ}\cap B^{\circ}\), while \((A\cup B)^{\circ}\supset A^{\circ}\cup B^{\circ}\).

(d) Show that \(\partial A=\bar{A}\cap\overline{A^{c}},M\backslash\partial A=A^{\circ}\cup(A ^{c})^{\circ},\bar{A}=A\cup\partial A\), and \(A^{\circ}=A\backslash\partial A\).

**Definition 1.2** (Bases and subbases): (a) Call \(\mathcal{B}\) a _base_ for the topology \(\mathcal{U}\) if \(\mathcal{B}\subset\mathcal{U}\) and if each nonvoid \(U\in\mathcal{U}\) satisfies \(U=\cup\mathcal{V}\) for some \(\mathcal{V}\subset\mathcal{B}\). We then say that \(\mathcal{B}\)_generates_\(\mathcal{U}\). (All the open intervals \((a,b)\) generate the open subsets of \(R\).)

(b) A family of sets \(\mathcal{S}\) is a _subbase_ for \(\mathcal{U}\) if \(\cup\mathcal{S}=M\) and the family of all finite intersections of members of \(\mathcal{S}\) is a base for \(\mathcal{U}\). (All \((a,b)\times R\) and \(R\times(a,b)\) serve as a subbase for all the open sets in \(R_{2}\).)

**Exercise 1.2** (Bases and subbases): (a) \(\mathcal{B}\) is a base for a topology \(\mathcal{U}\) on \(M\equiv\cup\mathcal{B}\) if and only if for each \(x\in M\) and each neighborhood \(U\in\mathcal{U}\) of \(x\) there exists a \(B\in\mathcal{B}\) such that \(x\in B\subset U\).

(b) \({\cal B}\) is a base for some topology on \(M\equiv\cup{\cal B}\) if and only if whenever \(B_{1}\) and \(B_{2}\) are members of \({\cal B}\) and \(x\in B_{1}\cap B_{2}\), there is a \(B_{3}\in B\) such that \(x\in B_{3}\subset B_{1}\cap B_{2}\).

(c) Any collection \({\cal S}\) of subsets of \(M\) having \(\cup{\cal S}=M\) is a subbase for a unique topology on \(M\).

**Exercise 1.3** (Relative topology)  Let \((M,{\cal U})\) be a topological space. First let \(N\subset M\), and then let \({\cal V}\equiv\{U\cap N:U\in{\cal U}\}\).

(a) Then \({\cal V}\) is a topology (the _relative topology_) for \(N\).

(b) Moreover, \(B\subset N\) is open (closed) with respect to \({\cal V}\) if and only if there is an \(A\subset M\) that is open (closed) with respect to \({\cal U}\) for which \(B=A\cap N\).

(c) The closure \(cl(B|N)\) of an arbitrary \(B\) in \((N,{\cal V})\) equals \(\bar{B}\cap N\), while the interior \(\mbox{\it int}\,(B|N)\) of an arbitrary \(B\) in \((N,{\cal V})\) equals \((\overline{B^{c}})^{c}\cap N\).

**Convergence**

**Definition 1.3** (Sequences)  A _sequence_ is a function \(x\) whose domain is the natural numbers {1, 2,...} and we let \(x_{n}\equiv x(n)\). Suppose \(x\) takes values \(x_{n}\) in some space \((M,{\cal U})\). Then the sequence is said to _converge_ to some \(x_{0}\in M\) if for each neighborhood \(U\) of \(x_{0}\) there exists a natural number \(n_{U}\) such that \(x_{n}\in U\) for all \(n\geq n_{U}\). We write \(x_{n}\to x_{0}\) or \(\lim_{n}x_{n}=x_{0}\) to denote that the sequence \(x_{n}\) converges to \(x_{0}\).

**Definition 1.4** (Nets)  A _partial ordering_ is a relationship \(R\) that is _reflexive_\((xRx)\), _antisymmetric_\((xRy\) and \(yRx\) implies \(x=y)\), and _transitive_\((xRy\) and \(yRz\) implies \(xRz)\). A _directed set_ is a set \(I\) under a relationship \(\preceq\) for which, for any \(i,j\in I\) there is a \(k\in I\) satisfying \(i\preceq k\) and \(j\preceq k\). A _net_ is any function whose domain is such a directed set \(I\) (with \(x_{i}\equiv x(i)\)) taking values in some topological space \((M,{\cal U})\). The net \(\{x_{i}\}_{i\in I}\) is said to _converge_ to a value \(x_{0}\) in \(M\) if for each neighborhood \(U\) of \(x_{0}\) there is an \(i_{U}\in I\) for which \(x_{i}\in U\) for all \(i\geq i_{U}\). [The set of all neighborhoods of a point constitutes a directed set. So does the set of all partitions of an interval.]

**Compactness**

**Definition 1.5** (Compactness) (a) A collection \({\cal A}\) of subsets of \(M\)_covers_ the subset \({\cal B}\) of \(M\) if \(B\subset\cup{\cal A}\); and \({\cal A}\) is called an _open cover_ of \(B\) if all sets \(A\) in \({\cal A}\) are open. A set \(A\) is called _compact_ if every open cover \({\cal A}\) of \(A\) has a finite subcover \({\cal V}\) (that is, \(A\subset\cup{\cal V}\), where \({\cal V}\subset{\cal A}\) contains only finitely many subsets of \({\cal A}\)). [This is called the _Heine-Borel property_.]

(b) \(A\subset M\) is _compact_ (_relatively compact_) if it (its closure) is compact in the relative topology.

(c) The collection \({\cal A}\) has the _finite intersection property_ if \(\cap{\cal F}\neq\emptyset\) for each finite subcollection \({\cal F}\) of subsets of \({\cal A}\).

**Exercise 1.4** (a) A closed subset of a compact space \((M,{\cal U})\) is compact.

(b) Let \({\cal B}\) be a base for \((M,{\cal U})\), and let \(A\subset M\). Then \(A\) is compact if and only if every cover of \(A\) by members of the base \({\cal B}\) admits a finite subcover.

(c) (Alexander's lemma) Let \({\cal S}\) be a subbase for the topological space \((M,{\cal U})\). Then \(M\) is a compact space if and only if each cover of \(M\) by members of the subbase \(S\) admits a finite subcover.

(d) A space \((M,{\cal U})\) is compact if and only if each family of closed sets with the finite intersection property has a nonempty intersection.

(e) \((M,{\cal U})\) is compact if and only if every net on \(M\) has at least one accumulation point in 

**Separation**

**Definition 1.6** (Separation) (a) (\(M,{\cal U}\)) is called a _Hausdorff space_ if for each \(x\neq y\) in \(M\) there are disjoint open sets \(U\) and \(V\) such that \(x\in U\) and \(y\in V.\)

(b) (\(M,{\cal U}\)) is called a _normal_ space if for each pair of disjoint closed sets \(A\) and \(B\) there are disjoint open sets \(U\) and \(V\) having \(A\subset U\) and \(B\subset V.\)

(c) (\(M,{\cal U}\)) is called _separable_ if it contains a countable subset \(D\) that is dense in \(M;\) here \(D\) is called _dense_ if \(\bar{D}=M.\)

(d) (\(M,{\cal U}\)) is called _first countable_ if for each \(x\in M\) there is a family \({\cal N}_{x}\) of neighborhoods \(N_{x}\) of \(x\) such that for each neighborhood \(U\) of \(x\) we have \(x\in N_{x}\subset U\) for some neighborhood \(N_{x}\) in \({\cal N}_{x}.\)

(e) (\(M,{\cal U}\)) is called _second countable_ or _perfectly separable_ if the topology \({\cal U}\) admits a countable base \(B.\)

**Exercise 1.5**

(a) A sequence with values in a Hausdorff space converges to at most one point.

(b) If all values \(x_{n}\) are in \(A\) and \(x_{n}\to x_{0},\) then \(x_{0}\in\bar{A}.\)

**Exercise 1.6** (a) A perfectly separable \((M,{\cal U})\) is separable.

(b) (Lindelof) An open cover of a perfectly separable space necessarily admits a countable subcover.

**Exercise 1.7** (a) A compact subset of a Hausdorff space \((M,{\cal U})\) is closed.

(b) A compact Hausdorff space is normal.

(c) A space \((M,{\cal U})\) is normal if and only if for each closed \(C\) and open \(U\) with \(C\subset U\) there exists an open \(V\) with \(C\subset V\subset\bar{V}\subset U.\)

(d) A subset of a Hausdorff space \((M,{\cal U})\) is Hausdorff in its relative topology.

(e) A subset of a normal space \((M,{\cal U})\) is normal in its relative topology.

**Continuity**

**Definition 1.7** (Continuous functions and homeomorphisms)

(a) Let \(f:M\to N\) for topological spaces \((M,{\cal U})\) and \((N,{\cal V})\). Call the function (or _mapping_) \(f\)_continuous at_\(x\) if for each neighborhood \(V\) of \(f(x)\) there is a neighborhood \(U\) of \(x\) for which \(f(U)\subset V.\)

(b) A 1-to-1 function that maps onto the image space is called a _bijection_.

(c) A 1-to-1 _bicontinuous function_\(f\) (that is, both \(f\) and \(f^{-1}\) are continuous and onto) is called a _homeomorphism_.

**Exercise 1.8** (Conditions for continuity) (a) \(f\) is a continuous function.

(b) Inverse images of all open sets are open.

(c) Inverse images of all closed sets are closed.

(d) Inverse images of all subbasic open sets are open.

(e) For each \(x\in M\) and each net \(\{x_{i}\}_{i\in I}\) converging to this \(x,\)\(\{f(x_{i})\}_{i\in I}\to f(x)\) converges.

(f) \(\underline{f(\bar{A})\subset\overline{f(A)}}\) for all subsets \(A\) of \(M.\)

(g) \(\overline{f^{-1}(B)}\subset f^{-1}(B^{-})\) for all subsets \(B\) of \(N.\)

(h) \(f^{-1}(B^{o})\subset[f^{-1}(B)]^{o}\) for all subsets \(B\) of \(N.\)

**Exercise 1.9** (a) **(Dini)** If \(f_{n}\) are continuous real-valued functions on a compact \((M,\Cal{U})\) for which \(f_{n}(x)\searrow f(x)\) for each \(x\in M\), then \(f_{n}\) converges uniformly to \(f\).

(b) A uniform limit of bounded and continuous functions \(f_{n}\) is also bounded and continuous.

**Exercise 1.10** (Continuity and compactness):

(a) If \(f:M\to N\) is continuous and \(M\) is compact, then \(f(M)\) is compact.

[Thus \([0,1]\) cannot be mapped continuously onto \(R\).]

(b) The composition of continuous functions is continuous.

(c) If \(f:M\to N\) is 1-to-1 and continuous (that is, it is a bijection) where \(M\) is compact and \(N\) is Hausdorff, then \(f\) is a homeomorphism.

**Exercise 1.11** (Urysohn's lemma) (\(M,\Cal{U}\)) is normal if and only if for each pair of disjoint closed sets \(A\) and \(B\) there is a continuous function \(f:M\to[0,1]\) having \(f(x)=0\) for all \(x\in A\) and \(f(x)=1\) for all \(x\in B\).

**Exercise 1.12** (Tietze) Let \((M,\Cal{U})\) be a normal space, and let \(A\) be a closed subset. If \(f:A\to[-1,1]\) is continuous, then \(f\) admits a continuous extension from \(M\) to [-1, 1].

**Product Spaces**

**Definition 1.8** (Product topology) Let \(P\{M_{a}\}\equiv\prod_{a\in A}M_{a}\) denote the _product set_ associated with the individual topological spaces \((M_{a},\Cal{U}_{a})\) over the index set \(A\); this product set is defined to be \(\{\ x:x\) is a function on \(A\) with \(x_{a}\equiv x(a)\in M_{a}\}\). The \(a_{0}\)th _projection function_\(\pi_{a_{0}}:P\{M_{a}\}\to M_{a_{0}}\) is defined by \(\pi_{a_{0}}(x)=x_{a_{0}}\). Let \(\Cal{S}\equiv\{\pi_{a}^{-1}(U):U\in\Cal{U}_{a}\) and \(a\in A\}\). Then \(\Cal{S}\) is the subbase for a topology called the _product topology_.

**Exercise 1.13** (a) Projection functions are continuous in the product topology. (Recall definition 5.4.1 for the projection maps \(\pi_{t_{1},\dots,t_{k}}(\cdot)\).)

(b) (Tychonoff) \(P\{M_{a}\}\) with the product topology is compact if and only if each \((M_{a},\Cal{U}_{a})\) is compact.

(c) The product of countably many separable topological spaces is a separable space in the product topology.

(d) \(\overline{P\{B_{a}\}}=P\{\bar{B}_{a}\}\) for all choices \(B_{a}\subset M_{a}\), and thus the product of closed sets is closed in the product topology.

(e) \((P\{B_{a}\})^{\rm o}=P\{B_{a}^{\rm o}\}\) for all choices \(B_{a}\subset M_{a}\), provided that \(A\) is finite.

(f) If each \((M_{a},\Cal{U}_{a})\) is Hausdorff, then the product topology is Hausdorff.

(g) Let \((M,\Cal{U})\) be a topological space and suppose \(f_{a}:M\to M_{a}\) is a continuous function for each \(a\in A\). Define \(f:M\to P\{M_{a}\}\) by \(\pi_{a}f(x)=f_{a}(x)\) for each \(x\in M\). Then \(f\) is continuous in the product topology.

**Local Compactness**

**Definition 1.9** (Locally compact) Let \((M,\Cal{U})\) be a topological space. Let \(x\in M.\) The space is _locally compact at \(x\)_ if \(x\) has a neighborhood with compact closure (that is, there exists a \(V\in\Cal{U}\) with \(x\in V\) for which \(\bar{V}\) is compact). The space is _locally compact_ if it is locally compact at each point.

[\(R_{n}\) and \((0,1)\) are locally compact.]

**Exercise 1.14** (Locally compact Hausdorff spaces are nice): Let \(M\) be a locally compact Hausdorff space. Let \(C\subset U\subset M\), with \(C\) compact and \(U\) open.

(a) Then there is an open set \(V\) whose closure is compact having \(C\subset V\subset\bar{V}\subset\ U.\)

(b) (Urysohn's lemma) There is a continuous function \(f:M\to[0,1]\) for which \(f(x)\) equals \(1\) for \(x\in C\) and \(0\) for \(x\in U^{c}.\)

(c) There is an open set \(V\) and a compact set \(D\) where \(C\subset V\subset D\subset U\) with \(V\) a countable union of compact sets and \(D\) a countable intersection of open sets. Sets of the type \(V\) form a base for the topology.

(d) Let \(C_{c}\) denote the set of all continuous functions \(f\) on \(M\) that _vanish outside compact sets_ (that is, \(f(x)=0\) for all \(x\notin\) (some compact \(K_{f}\))). Let \(C_{o}\) be the set of all continuous functions \(f\) on \(M\) that _vanish at infinity_ (that is, \(|f(x)|<\epsilon\) for all \(x\notin\) (some compact \(K_{f,\in)}\)), for each \(\epsilon>0\)). Show that the closure (in the sup norm metric on \(M\)) of \(C_{c}\) equals \(C_{o}.\)

**Exercise 1.15** (One-point compactification): The one-point compactification of any locally compact Hausdorff space \((M,{\cal U})\) makes it a compact Hausdorff space. That is, let \(\bar{M}\equiv M\cup\{p\}\), and designate the open sets \(\bar{U}\) to be all open sets \({\cal U}\) plus all sets \(\{p\}\cup C^{c}\) with \(C\) compact and \(\{p\}\) the one new point. Then \((\bar{M},\bar{\cal U})\) is a compact Hausdorff space.

**Category**

**Definition 1.10** (Category): A subset \(A\) of \((M,{\cal U})\) is _nowhere dense_ if \((\bar{A})^{\rm o}=\emptyset.\) If \(A\) can be written as a countable union of nowhere dense sets, then it is said to be of the _first category_; otherwise, it is said to be of the _second category_.

**Exercise 1.16** (Baire category theorem): (a) A locally compact Hausdorff space is second category. (b) Let \(C\) denote the class on all continuous functions on \([0,1]\), and let \(C^{(1)}\) denote the subclass of those that have continuous derivatives on \([0,1].\) Show that \((C,\|\cdot\|\) is a complete and separable metric space of category two, and that \(C^{(1)}\) is a subset of category one. That is, "most" continuous functions on \([0,1]\) are not differentiable on \([0,1].\)

**Topological Properties**

**Definition 1.11** (Topologically equivalent): If there exists a _homeomorphism_ between the two topological spaces \((M,{\cal U})\) and \((N,{\cal V})\) (that is, there exists a \(1\)-to-\(1\) bicontinuous mapping between them), then there exist \(1\)-to-\(1\) correspondences both between points and between open sets. We then say that these two topological spaces are _topologically equivalent_. Thus any properties that are defined solely in terms of the open sets either hold or fail simultaneously in the two spaces. Such concepts are called _topological concepts_. [These include closed set, closure, interior, boundary, accumulation point, compactness, separability, perfect separability, local compactness, category, continuity, and being either Hausdorff or normal.]

We are about to introduce metric spaces. We will soon see that two different metrics can induce the same topology. Thus distance in a metric space is not a topological property. A homeomorphism \(f\) that leaves distance unchanged (thus, \(d_{2}(f(x),f(y))=d_{1}(x,y)\)) is called an _isometry_.

## 2 Metric Spaces \({}^{*}\)

**Definition 2.1** (Metric space): (a) A _semimetric space_\((M,d)\) consists of a set \(M\) together with a _semimetric_\(d\) that satisfies

\[d(x,y)=d(y,x)\geq 0,\quad d(x,z)\leq d(x,y)+d(y,z),\quad\mbox{and}\quad d(x,x)=0\]

for all \(x,y,z\in M\). If \(d(x,y)=0\) implies \(x=y\), then \(d\) is called a _metric_ and \((M,d)\) is called a _metric space_. [If all \(d(x,y)\leq\) (some finite \(c\)), then \(d\) is called a _bounded metric_.] Call the set

\[S_{r}(x)\equiv\{y\in M\ :\ d(x,y)<r\}\quad\mbox{an open \emph{sphere} or \emph{ball} about $x$ of radius $r$}.\]

(b) The collection of all such spheres is the base for a topology, called the _metric topology_, and is denoted by \({\cal U}_{d}\). The abbreviation \((M,d,{\cal U}_{d})\) refers to the metric space with its metric topology, and we call it a _topological metric space_.

(c) Let \((M,{\cal U})\) be a topological space. Suppose there exists a metric \(d\) on \(M\) whose open balls form a base for the topology \({\cal U}\). Then the topology \({\cal U}\) is said to be _metrizable_.

**Exercise 2.1** (Equivalent metrics): (a) Two metrics \(d\) and \(\rho\) on a set \(M\) lead to equivalent topologies if and only if for each \(x\in M\) and each \(\epsilon_{1},\epsilon_{2}>0\) there are \(\delta_{1},\delta_{2}>0\) such that for all \(y\in M\),

\[d(x,y)<\delta_{1}\mbox{ implies }\rho(x,y)<\epsilon_{1}\qquad\mbox{and}\qquad \rho(x,y)<\delta_{2}\mbox{ implies }d(x,y)<\epsilon_{2}.\]

[If only the first holds, then \({\cal U}_{d}\subset{\cal U}_{\rho}\), meaning that the \(\rho\) topology is finer.] Such _equivalent metrics_ lead to the same continuous functions. If both sets of \(\delta\) values do not depend on \(x\), then the metrics are called _uniformly equivalent metrics_ and lead to the same uniformly continuous functions.

(b) Define four equivalent metrics on \(R_{n}\).

(c) \(\rho\equiv d/(1+d)\) defines a metric equivalent to the \(d\) metric on \((M,d)\); and \(\rho\) is a bounded metric.

**Exercise 2.2**: (a) The closed sphere \(\{y:d(x,y)\leq r\}\) equals \(\overline{S_{r}(x)}\) with respect to the metric topology \({\cal U}_{d}\).

(b) Any metric space is homeomorphic to a metric space with a bounded metric.

(c) A metric space is perfectly separable if and only if it is separable.

(d) A compact metric space is separable and perfectly separable.

(e) Any metric space is Hausdorff, normal, and first countable.

(f) The product of countably many metrizable spaces is metrizable in the product topology, with the bounded metric \(d(\mbox{x},\mbox{y})\equiv\sum_{1}^{\infty}\psi(d_{n}(x_{n},y_{n}))/2^{n}\), where we define \(\psi(t)\equiv t/(1+t)\). If each is separable, then so is the product.

(g) The relative topology on a subset \(A\) of the metric space \((M,d,{\cal U}_{d})\) is the same as the metric topology for \((A,d)\).

(h) Every subspace of a separable metric space is separable in its relative topology.

(i) A separable metric space is locally compact if and only if it is the union of \(U_{1}\subset U_{2}\subset\cdots\) with each \(\bar{U}_{n}\) a compact subset of \(U_{n+1}\).

**Completeness**

**Definition 2.2** (Complete): (a) We will call a sequence \(x\) in a metric space \((M,d)\)_a Cauchy sequence_ if \(d(x_{m},x_{n})\to 0\) as \(m\wedge n\to\infty\).

(b) The topological metric space \((M,d,{\cal U}_{d})\) is called _complete_ when every Cauchy sequence \(x\) having values \(x_{n}\) in \(M\) converges to a member of \(M.\)

**Exercise 2.3**: (a) A sequence \(x\) taking values in a metric space \((M,d)\) converges to a member \(x_{o}\) of \(M\) if and only if \(d(x_{n},x_{o})\to 0.\)

(b) Every compact metric space is complete, separable, and perfectly separable.

(c) Every closed subspace of a complete metric space is complete.

(d) A metric space in which every sequence has a convergent subsequence is perfectly separable.

**Compactness**

**Definition 2.3**: (a) A metric space \((M,d)\) is _totally bounded_ if for every \(\epsilon>0\) there is a finite subset \(F_{\epsilon}\) of \(M\) such that for each \(x\in M\) there exists a \(y\in F_{\epsilon}\) sufficiently close to \(x\) to have \(d(x,y)<\epsilon.\)

(b) Let \({\rm diam}(A)\equiv\sup\{d(x,y):x,y\in A\}\) denote the _diameter_ of \(A\). The metric space is called _bounded_ if \({\rm diam}(M)\) is finite.

**Exercise 2.4** (Compactness): The following conditions are equivalent to the compactness of a subset \(K\) of the metric space \((M,d,{\cal U}_{d})\).

(a) (Heine-Borel property = compactness, by definition):

Every open cover of \(K\) has a finite subcover.

(b) (Bolzano-Weierstrass property):

Every infinite subset of \(K\) has a limit point in \(K\).

(c) (Sequential compactness (or relative compactness)):

Every sequence in \(K\) has a subsequence converging to a point in \(K.\)

(d) (Countable compactness):

Every countable open cover of \(K\) has a finite subcover.

(e) \(K\) is totally bounded and complete.

\(K\) is totally bounded, and

(f) any \(\searrow\) sequence of closed spheres \(S_{n}\) in \(K\) whose diameters \(\to 0\) has

\(\cap_{1}^{\infty}S_{n}=\{\)some singleton \(x\}\in K\).

**Exercise 2.5**: A subset of \(R_{n}\) is compact if and only if it is closed and bounded.

**Exercise 2.6**: If the closed subset \(A\) and the compact subset \(B\) are disjoint, then we have \(d(A,\ B)\equiv\inf\{d(x,y)\ :\ x\in A,y\in B\}>0\).

**Definition 2.4** (Covering numbers): Let \((M,d)\) be a totally bounded metric space. Let \(\epsilon>0\). Let \(N(\epsilon,M)\) denote the minimum \(n\) for which \(M=\cup_{1}^{n}A_{k}\) for sets \(A_{k}\) all having diam \((A_{k})\leq 2\epsilon\). Let \(D(\epsilon,M)\) be the largest number \(m\) of points \(x_{k}\) in \(M\) having \(d(x_{i},x_{j})>\epsilon\) for all \(i\neq j\). These are called _covering numbers_.

**Exercise 2.7**: Show that \(N(\epsilon,M)\leq D(\epsilon,M)\leq N(\epsilon/2,M).\)

#### Spaces of Functions

**Exercise 2.8** (Uniform norm): Let \(C_{b}(M)\) denote the collection of all bounded and continuous functions from the topological space \((M,\Cal{U})\) to \(R,\) and define the _uniform norm_\(\|\cdot\|\) on the functions of \(C_{b}(M)\) by \(\|f\|\equiv\sup\{|f(x)|\ :\ x\in M\}.\)

(a) A uniform limit of bounded, continuous functions is bounded and continuous.

(b) Moreover, \((C_{b}(M),\|\cdot\|\) defines a complete metric space.

(c) Let \((M,\Cal{U})\) be a compact Hausdorff space. Then \((C_{b}(M),\|\cdot\|\) is separable if and only if \((M,\Cal{U})\) is metrizable.

**Definition 2.5** (Uniform continuity): A mapping \(f\) from a metric space \((M_{1},d_{1})\) to a metric space \((M_{2},d_{2})\) is _uniformly continuous_ if for all \(\epsilon>0\) there exists \(\delta_{\epsilon}>0\) for which \(d_{2}(f(x),f(y))<\epsilon\) whenever \(d_{1}(x,y)<\delta_{\epsilon}.\)

**Definition 2.6** (Equicontinuity): (a) A collection \(\Cal{F}\) of bounded and continuous functions from a topological space \((M,\Cal{U})\) to a metric space \((N,d)\) is _equicontinuous_ if for every \(\epsilon>0\) and each \(x\in M\) there is a neighborhood \(U\) of \(x\) for which \(d(f(x),f(y))<\epsilon\) for all \(y\in U\) and all \(f\in\Cal{F}.\)

(b) In fact, if \((M,\rho,\Cal{U}=\Cal{U}_{\rho})\) is a metric space for which \(d(f(x),f(y))<\epsilon\) for all \(f\in\Cal{F}\) whenever \(\rho(x,y)<\delta_{\epsilon},\) then the functions of \(\Cal{F}\) are called _uniformly equicontinuous_.

**Exercise 2.9**: A continuous function (an equicontinuous family of functions) from a compact metric space into another metric space is uniformly continuous (is uniformly equicontinuous).

**Exercise 2.10** (Ascoli's theorem): Let \(\Cal{F}\) be an equicontinuous class of functions from a separable metric space \((M,d)\) to a metric space \((N,\rho)\). Let \(f_{n}\) denote a sequence of such functions for which c1\((\{f_{n}(x):n\geq 1\})\) is compact for each \(x\in M.\) Then there is a subsequence \(f_{n^{\prime}}\) that converges pointwise to an \(f\) that is continuous on \(M\), and this convergence is uniform on any compact subset \(K\) of \(M.\)

**Exercise 2.11** (Arzela's theorem): (a) A collection \(\Cal{F}\) of real-valued, bounded, and continuous functions \(f\) on a compact topological space \((M,\Cal{U})\) is a totally bounded subset of \((C_{b}(M),\|\cdot\|\) if and only if it is a collection of uniformly bounded and uniformly equicontinuous functions.

(b) A subset \(\Cal{F}\) of \((C_{b}([a,b]),\|\cdot\|)\). is compact if and only if \(\Cal{F}\) is closed, bounded, and equicontinuous with respect to \(\|\cdot\|\).

## 3 Hilbert Space \({}^{*}\)

**Definition 3.1** (A) **(Vector space)** Let \({\cal V}\) be a real (or a complex) _vector space_; that is, \(x+y\in{\cal V}\) and \(cx\in{\cal V}\) for all \(x,y\) in the commutative group \({\cal V}\) and for all real scalars \(c\in R\) (or all complex scalars \(c\in C\)). Moreover, \(c(x+y)=cx+cy,(c+d)x=cx+\,dx,c(dx)=(cd)x\) and \(1x=x\) must be satisfied.

(B) **(Inner product space)** A vector space \({\cal H}\) is an _inner product space_ if for each pair \(x,y\in{\cal H}\) there is a real (or a complex) number \(\langle x,y\rangle\equiv\) (the _inner product_ of \(x\) and \(y\)) satisfying:

(a) \(\langle cx,y\rangle=c\langle x,y\rangle\) for all \(x,y\in{\cal H},\) and all \(c\in R\) (or all \(c\in C\)),

(b) \(\langle y,x\rangle=\langle x,y\rangle\) (or satisfying, \(\langle y,x\rangle=\overline{\langle x,y\rangle}\)),

(c) \(\langle x+y,z\rangle=\langle x,z\rangle+\langle y,z\rangle\) for all \(x,y,z\in{\cal H}\),

(d) \(\langle x,x\rangle\geq 0\) for all \(x\in{\cal H}\), with equality if and only if \(x=0\).

(C) **(Norm)** We use (d) to define the _norm_ (denoted by \(\|\cdot\|\)) of \(x\); thus

(e) \(\|x\|^{2}\equiv\langle x,x\rangle\), so that \(\|x\|=\sqrt{\langle x,x\rangle}\).

(D) **(Normed linear space)** The pair \(({\cal H},\|\cdot\|\) has the following three properties, and these are the three properties that define a _normed linear space_. Thus

(f) \(\|x+y\|\leq\|x\|+\|y\|\) for all \(x,y\in{\cal H}\).

(g) \(\|cx\|=|c|\cdot\|x\|\) for all \(x\in{\cal H}\) and all scalars \(c\).

(h) \(\|x\|=0\) implies that \(x=0\).

A complete normed linear space is called a _Banach space_.

(E) **(Orthogonal, and orthogonal complement)** We agree that

(i) \(x\perp y\) means that \(\langle x,y\rangle=0\),

and we then say that \(x\) is _orthogonal_ to \(y\). We also let

(j) \(M^{\perp}\equiv\{x:x\perp y\) for all \(y\in M\}\),

and we call it the _orthogonal complement_ of \(M\).

(F) **(Hilbert space)** If \({\cal H}\) is an inner product space that is complete (with respect to the metric defined by the norm \(\|x-y\|\)), then \({\cal H}\) is called a _Hilbert space_.

(G) **(Bounded linear map)** A _linear map_\(L:{\cal H}\to R\) satisfies

(k) \(L(cx+dy)=cL(x)+dL(y)\) for all \(x,y\in{\cal H}\) and all scalars \(c,d\).

The linear map \(L\) is said to be _bounded_ if \(\|L\|\) is finite, where

(l) \[\|L\|\equiv\sup_{x\in{\cal H}}|L(x)|/\|x\|;\]

call \(\|L\|\) the _norm_ of \(L\). The collection of all linear maps on \({\cal H}\) is denoted by \({\cal H}^{*}\).

**Example 3.1** (\({\cal L}_{2}\) is a Hilbert space): Let \(\mu\) denote a fixed positive measure. Then let \({\cal L}_{2}(\mu)\equiv\{X:\int X^{2}\,d\mu<\infty\}\), which is a Hilbert space with inner product \(\langle X,\,Y\rangle=\int XYd\mu={\rm E}(XY)\). Recall the completeness presented in exercise 3.5.1. (Note part (g) below to identify the bounded linear functionals on this \({\cal H}\). This is called the Reisz representation theorem.) \(\Box\)

**Exercise 3.1**:
1. Show that a linear functional \(L\) on \(({\cal H},\|\cdot\|\) is bounded if and only if it is continuous if and only if it is continuous at any one point.
2. Show that \(\|L\|=\sup_{\{x:\|x\|=1\}}|L(x)|/\|x\|\) is also valid.

**Proposition 3.1** (Elementary properties): Let \(x,y\in{\cal H}\), an inner product space.

1. (Cauchy-Schwarz) \(|\langle x,y\rangle\)\(|\leq\|x\|\|y\|\).
2. (Triangle inequality) \(\|x+y\|\leq\|x\|+\|y\|\).
3. (Pythagoras theorem) \(\|x+y\|^{2}=\|x\|^{2}+\|y\|^{2}\) if and only if \(x\perp y\).
4. (Parallelogram law) \(\|x+y\|^{2}+\|x-y\|^{2}\leq 2[\|x\|^{2}+\|y\|^{2}]\).
5. (Polarization identity) \(4\langle x,y\rangle=\|x+y\|^{2}-\|x-y\|^{2}\) when \(c\)'s are real.
6. (Linear functionals) \(x\to\langle x,y\rangle\) and \(x\to\langle y,x\rangle\) are uniformly continuous linear functionals on \({\cal H}\). The norms equal \(\|y\|\).
7. (Reisz) The only bounded linear functionals on \({\cal H}\) are those in (f).
8. \(x\to\|x\|\) is a bounded and uniformly continuous functional on \({\cal H}\).

**Exercise 3.2**: Prove proposition 3.1.

**Exercise 3.3**: An inner product space over the reals is a normed linear space iff the inner product satisfies the parallelogram law. [Hint. Use proposition 3.1(e).]

**Definition 3.2** (Subspaces): We say that \({\cal M}\subset{\cal H}\) is a _subspace_ of the vector space \({\cal H}\) if \({\cal M}\) is also a vector space. [It is enough if \(x+y\) and \(cx\) are in \({\cal M}\) for all \(x,y\in{\cal M}\) and all scalars \(c\).] [Recall that a subset \({\cal M}\) of the metric space \({\cal H}\) is _closed_ if and only if it is complete (that is, it contains all of its own limit points).]

**Exercise 3.4**: Let \({\cal M}\) be a subset of the Hilbert space \({\cal H}\).

(a) Then \({\cal M}^{\perp}\) is a closed subspace of \({\cal H}\). In fact, \({\cal M}^{\perp}=\cap_{\{x:x\in{\cal M}\}}x^{\perp}\).

(b) \(({\cal M}^{\perp})^{\perp}=\bar{\cal M}\supset{\cal M}\).

(c) If \({\cal M}_{1}\) and \({\cal M}_{2}\) are closed subspaces of \({\cal H}\) with \({\cal M}_{1}\perp{\cal M}_{2}\), then the _sum space_\({\cal M}_{1}+{\cal M}_{2}\equiv\{x+y:x\in{\cal M}_{1}\) and \(y\in{\cal M}_{2}\}\) is also a closed subspace of \({\cal H}\).

(d) If \({\cal M}_{1}\) and \({\cal M}_{2}\) denote subspaces of \({\cal H}\), then \(({\cal M}_{1}+{\cal M}_{2})^{\perp}={\cal M}_{1}^{\perp}\cap{\cal M}_{2}^{\perp}\). Thus, \(\overline{{\cal M}_{1}+{\cal M}_{2}}=({\cal M}_{1}^{\perp}\cap{\cal M}_{2}^{ \perp})^{\perp}\).

(e) Let \(L\) denote a continuous linear functional on \({\cal H}\). Then \({\cal M}=\{x:L(x)=0\}\) is a closed subspace, and either \({\cal M}^{\perp}\) has dimension 1 (or else \({\cal M}^{\perp}={\cal H}\)).

**Theorem 3.1** (Orthogonal projections): Let \({\cal M}\) be a closed subspace of \({\cal H}\).

(i) Unique mappings \(P\) and \(Q\) on \({\cal H}\) necessarily exist such that

\[x=Px+Qx\qquad\mbox{for all }x\in{\cal H},\qquad\mbox{with }Px\in{\cal M}\quad \mbox{and}\quad Qx\in{\cal M}^{\perp}.\]

This \(P\) and \(Q\) are called the _orthogonal projections_ of \({\cal H}\) onto \({\cal M}\) and onto \({\cal M}^{\perp}\). That is, \({\cal H}={\cal M}+{\cal M}^{\perp}\) with \({\cal M}\cap{\cal M}^{\perp}=\{0\}\) when \({\cal M}\) is a closed subspace.

(ii) Specifically and additionally, this \(P\equiv P_{\cal M}\) and \(Q\equiv P_{\cal M}\perp\) satisfy:

\[\|x\|^{2}=\|Px\|^{2}+\|Qx\|^{2},\] \[\|x-Px\|=\inf\{\|x-y\|\ :\ y\in{\cal M}\}\qquad\mbox{for each }x\in{ \cal H},\] \[\mbox{$P$ and $Q$ are bounded, uniformly continuous, and idempotent linear maps.}\]

(Call P _idempotent_ if \(P(P(x))=P(x)\) for all \(x\in{\cal H}\).)

(iii) Moreover, \(P_{\cal M}x\) may be uniquely determined by solving \(x-P_{\cal M}x\perp{\cal M}\). Or, \(P_{\cal M}\) is the unique linear map for which \(\langle P_{\cal M}x,y\rangle=\langle P_{\cal M}x,P_{\cal M}y\rangle\) for all \(x,y\in{\cal H}\).

(iv) If \({\cal M}\) is a proper subset of \({\cal H}\), then there exists a \(y\neq 0\) in \({\cal H}\) such that \(y\perp{\cal M}\). Moreover, the space spanned by \({\cal M}\) and \(y\) is closed.

**Example 3.2** (\({\cal H}_{D}\) **and \({\cal H}_{D}^{0}\) are closed subspaces of \({\cal H}\))** For \({\cal L}_{2}(\Omega,{\cal A},\mu)\), let

\[{\cal H}_{D}\equiv\{X\in{\cal L}_{2}(\Omega,{\cal A},\mu)\ :\ {\cal F}(X) \subset{\cal D}\}\quad\mbox{and}\quad{\cal H}_{D}^{0}\equiv\{X\in{\cal H}_{D} \ :\ \mbox{E}X=0\},\]

where \({\cal D}\subset{\cal A}\) denotes a fixed sub \(\sigma\)-field of \({\cal A}\). Then exercise 3.5.1 and theorem 3.5.3 show that both \({\cal H}_{D}\) and \({\cal H}_{D}^{0}\) are closed subspaces of \({\cal H}\equiv{\cal L}_{2}(\Omega,{\cal A},\mu)\). \(\Box\)

**Exercise 3.5**: (a) Prove theorem 3.1. (b) Show that

\[|\|x\|-\|y\|\leq\|x-y\|\quad\mbox{and}\quad\|Px-Py\|\leq\|x-y\|\quad\mbox{for all }x,y\in{\cal H}.\]

(c) Show \(\|x-Px\|=\max\{|\langle x,y\rangle|:y\in{\cal M}^{\perp}\) and \(\|y\|=1\}\) for closed subspaces \({\cal M}\).

**Exercise 3.6**: Let \({\cal M},{\cal M}_{1}\), and \({\cal M}_{2}\) denote non-trivial subspaces of \({\cal H}\). Verify:

(a) \(\|P_{\cal M}\|=1\).

(b) \(P_{{\cal M}_{2}}=P_{{\cal M}_{2}}(P_{{\cal M}_{1}})\qquad\mbox{when }{\cal M}_{2}\subset{\cal M}_{1}\).

(c) \(P_{{\cal M}_{1}+{\cal M}_{2}}=P_{{\cal M}_{1}}+P_{{\cal M}_{2}}\qquad\mbox{when }{\cal M}_{1}\perp{\cal M}_{2}\).

(d) \(P_{{\cal M}_{1}}=P_{\overline{P_{{\cal M}_{1}}({\cal M}_{2})}}+P_{{\cal M}_{1} \cap{\cal M}_{2}^{\perp}}\).

**Definition 3.3** (Independent, span, orthonormal, Fourier coefficient, and basis)

(i) Call vectors \(\{x_{1},\ldots,x_{n}\}\) in \({\cal V}\)_linearly independent_ if \(c_{1}x_{1}+\cdots+c_{n}x_{n}=0\) implies \(c_{1}=\cdots=c_{n}=0\). Call the set \({\cal Z}\)_linearly independent_ if all of its finite subsets are linearly independent. Let the _span_ of \({\cal Z}\) (denoted by \({\cal S}[{\cal Z}]\)) denote the set of all finite linear combinations of elements of the set \({\cal Z}\).

(ii) If \({\cal Z}\equiv\{z_{a}:a\in A\}\) is a subset of \({\cal H}\) indexed by \({\cal A}\), then the elements of \({\cal Z}\) are called _orthonormal_ if \(\langle z_{a},z_{b}\rangle=0\) for all \(a,b\in A\), while \(\|z_{a}\|=1\) for all \(a\in A\). Moreover, the values

\[x_{A}(a)\equiv\langle x,z_{a}\rangle,\mbox{ for all }a\in{\cal A},\mbox{ are called theFourier coefficients of }x \tag{6}\]

relative to \({\cal Z}\). A maximal orthonormal set is called an _orthonormal basis_.

**Theorem 3.2** (Coordinates)

Let vectors \(\{z_{1},\ldots,z_{n}\}\) be orthonormal in the Hilbert space \({\cal H}\).

(i) If \(x=\sum_{1}^{n}c_{k}z_{k}\), then for each \(1\leq k\leq n\),

\[c_{k}=\langle x,z_{k}\rangle,\mbox{ the }z_{1},\ldots,z_{n}\mbox{ are linearly independent, and }\|x\|^{2}=\sum_{1}^{n}|c_{k}|^{2}. \tag{7}\]

(ii) Let \(x\in{\cal H}\). Define \(s_{n}\equiv\sum_{1}^{n}\langle x,z_{k}\rangle z_{k}\). For all scalars \(c_{1},\ldots,c_{n}\) we have

\[\|x-s_{n}\|^{2}=\|x-\sum_{1}^{n}\langle x,z_{k}\rangle z_{k}\|^{2}\leq\|x- \sum_{1}^{n}c_{k}z_{k}\|^{2}, \tag{8}\]

with equality if and only if \(c_{k}=\langle x,z_{k}\rangle\) for all \(1\leq k\leq n\). Moreover,

\[\sum_{1}^{n}\langle x,z_{k}\rangle z_{k}\mbox{ is the orthogonal projection of }x\mbox{ onto the span }{\cal S}[z_{1},\ldots,z_{n}]. \tag{9}\]

(Bessel's identity) Call \(d(x,\,S[z_{1},\ldots,z_{n}])\equiv\inf\{\,\|x-\sum_{1}^{n}c_{k}z_{k}\|:c_{k}\)'s are scalars\(\}\) the _distance from_\(x\) to the span \(S[z_{1},\ldots,z_{n}]\); then (with \(s_{n}\equiv\sum_{1}^{n}\langle x,z_{k}\rangle z_{k}\))

\[d^{2}(x,\,\,{\cal S}[z_{1},\ldots,z_{n}])=\|x-s_{n}\|^{2}=\|x\|^{2}-\|s_{n}\|^{ 2}=\|x\|^{2}-\sum_{1}^{n}|\langle x,z_{k}\rangle|^{2}. \tag{10}\]

(iii) (Bessel's inequality) For any orthonormal set \(\{z_{a}:a\in A\}\),

\[\sum_{a\in A}\langle x,z_{a}\rangle|^{2}\leq\|x\|^{2}\quad\mbox{with equality if and only if }x=_{\ell_{2}}\sum_{a\in A}x_{A}(a)z_{a}. \tag{11}\]

(iv) Thus, each \(x\in{\cal H}\) has at most countably many nonzero Fourier coefficients.

(v) Let \(y_{1},y_{2},\ldots\) be orthogonal vectors in \({\cal H}\). Then

\[\sum_{1}^{n}y_{k}\to_{{\cal L}_{2}}\mbox{ \ (some }y)\in{\cal H}\mbox{ if and only if }\sum_{1}^{\infty}\|y_{k}\|^{2}<\infty, \tag{12}\]

and \(\|y\|^{2}\) then equals this sum. Also,

(13) if \(x\in{\cal H}\) and \(\sum_{1}^{n}\langle x,y_{k}\rangle y_{k}\to_{{\cal L}_{2}}\mbox{ \ (some }y)\in{\cal H}\), then \(x-y\perp y_{k}\) for all \(k\).

(vi) The collection \(\{\sum_{1}^{\infty}c_{n}z_{n}\ :\ |c_{n}|\leq d_{n}\}\) is compact if and only if \(\sum_{1}^{\infty}d_{n}^{2}<\infty\). Here, we let \(z_{1},z_{2},\ldots\) be any orthonormal set, and distance is the \(\|\cdot\|_{\ell_{2}}\) below.

(vii) Let \(\ell_{2}(A)\) denote \({\cal L}_{2}(A,2^{A},\) counting measure), as in exercise 3.5.11. Bessel's inequality can be rewritten. For any orthonormal set \(\{z_{a}:a\in A\}\) we have

\[\|x_{A}\|_{\ell_{2}}\equiv\sum_{a\in A}x_{A}(a)\overline{y_{A}(a)}\leq\|x\| \quad\mbox{(so,}\quad\|x_{A}-y_{A}\|_{\ell_{2}}\leq\|x-y\|). \tag{14}\]

(viii) An orthonormal set \(z_{1},z_{2},\ldots\) is closed and bounded, but it is not compact. Thus \({\cal H}\) is not locally compact.

**Theorem 3.3** (Orthonormal bases): Let \(\{z_{a}:a\in A\}\) be an orthonormal set. The following are equivalent:

(15) \[\{z_{a}\ :\ a\in A\}\mbox{ is an orthonormal basis.}\] (16) The span of these \(z_{a}\)'s is \(\|\cdot\|\)-dense in \({\cal H}\). (17) (Fourier series) \(x=\sum_{a\in A}x_{A}(a)z_{a}\) for all \(x\in{\cal H}\). (18) (Parseval's identity) \(\langle x,y\rangle=\sum_{a\in A}x_{A}(a)\overline{y_{A}(a)}\) for all \(x,y\in{\cal H}\). (19) (Bessel's identity) \(\|x\|^{2}=\sum_{a\in A}|x_{A}(a)|^{2}\) for all \(x\in{\cal H}\). (20) Every \(x\in{\cal H}\) is uniquely determined by its Fourier coefficients

**Theorem 3.4** (\({\cal H}\) **is isomorphic to \(\ell_{2}\)): (a) Suppose the Hilbert space \({\cal H}\) has an orthonormal basis \(\{z_{a}\ :\ a\in A\}\). Then the mapping \(x\to x_{A}\) maps \({\cal H}\) onto \(\ell_{2}(A)\). And it is an _isomorphism_ in that it preserves inner products via the correspondence

\[\langle x,y\rangle=\langle x_{A},y_{A}\rangle_{\ell_{2}}\equiv\sum_{a\in A}x_{ A}(a)\overline{y_{A}(a)}\qquad\mbox{for all }x,y\in{\cal H}. \tag{21}\]

(b) Every Hilbert space possesses an orthonormal basis. Thus every Hilbert space \({\cal H}\) is isomorphic to some \(\ell_{2}(A)\). The cardinality of \(A\) equals that of the basis.

(c) Every orthonormal basis for the Hilbert space \({\cal H}\) has the same cardinality.

(d) Every orthonormal set can be extended to an orthonormal basis.

(e) \({\cal H}\) is separable if and only if \({\cal H}\) contains an orthonormal basis that is finite or countably infinite.

**Exercise 3.7**: Prove: (a) theorem 3.2. (b) theorem 3.3. (c) theorem 3.4.

**Exercise 3.8** (Gram-Schmidt): Show that from any \(n\) vectors that are linearly independent one can define \(n\) orthonormal vectors that have the same span.

**Trigonometric Series**

Consider Fourier series representations defined on \([-\pi,\pi]\). In this case the functions

\[\phi_{0}(t)\equiv\frac{1}{\sqrt{2\pi}},\ \ \phi_{n}(t)\equiv\frac{1}{\sqrt{\pi}} \cos nt\,(\mbox{for }n\geq 0),\ \ \psi_{n}(t)\equiv\frac{1}{\sqrt{\pi}}\sin nt \,(\mbox{for }n\geq 1) \tag{22}\]

form a set (denote it by \({\cal Z}\)) of orthonormal functions. The corresponding Fourier coefficients for a function \(f\) in \({\cal L}_{1}\equiv{\cal L}_{1}([-\pi,\pi],{\cal B},\lambda)\) are given by

\[\begin{array}{ll}a_{n}\equiv\langle f,\,\phi_{n}\rangle=\int_{-\pi}^{\pi}f(t )\,\phi_{n}(t)\,dt&\mbox{(for }n\geq 0)\qquad\mbox{and}\\ b_{n}\equiv\langle f,\psi_{n}\rangle=\int_{-\pi}^{\pi}f(t)\psi_{n}(t)\,dt& \mbox{(for }n\geq 1).\end{array} \tag{23}\]

The partial sums of the _Fourier series representation_ of \(f\) are defined on \([-\pi,\pi]\) by

\[g_{n}(t)\equiv a_{o}\phi_{0}(t)+\sum_{j=1}^{n}\{a_{n}\,\phi_{n}( t)+b_{n}\psi_{n}(t)\} \tag{25}\] \[=\sum_{j=-n}^{n}c_{j}e^{int}\qquad\mbox{where}\quad c_{n}\equiv \frac{1}{2\pi}\int_{-\pi}^{\pi}f(t)e^{-int}\,dt\quad\mbox{for all }n. \tag{24}\]

When does \(g_{n}\)"converges" to \(f\)? Let \(\Omega\equiv[-\pi,\pi]\), and let \({\cal L}_{p}\equiv{\cal L}_{p}([-\pi,\pi],{\cal B},\lambda)\).

In case \(f\in{\cal L}_{1}([0,1],{\cal B},\lambda)\), we define Fourier coefficients

\[a_{n}\equiv\int_{0}^{1}f(t)\,\phi_{n}(t)\,dt\qquad\mbox{with}\quad\phi_{0}(t) \equiv 1\quad\mbox{and}\quad\phi_{n}(t)\equiv\sqrt{2}\cos n\pi t\quad\mbox{on }[0,1], \tag{26}\]for \(n\geq 1\), with the Fourier series representation \(g_{n}(t)\equiv\sum_{0}^{n}a_{j}\phi_{j}(t)\) on \([0,1]\) in terms of the set (denote it by \(\mathcal{Z}\)) of orthonormal functions \(\phi_{n},n\geq 0\). Let \(\Omega\equiv[0,1]\) and let \(\mathcal{L}_{p}\equiv\mathcal{L}_{p}([0,1],\mathcal{B},\lambda)\).

In case \(f\in\mathcal{L}_{1}([0,1],\mathcal{B},\lambda)\), we also define Fourier coefficients

\[a_{n}\equiv\int_{0}^{1}f(t)\,\phi_{n}(t)\,dt\qquad\text{with}\quad\phi_{n}(t) \equiv\sqrt{2}\sin n\pi t\quad\text{on $[0,1]$, for $n\geq 1$}, \tag{27}\]

with the Fourier series representation \(g_{n}(t)\equiv\sum_{1}^{n}a_{j}\phi_{j}(t)\) on \([0,1]\) in terms of this set (denote it by \(\mathcal{Z}\)) of orthonormal functions \(\phi_{n},n\geq 1\). Let \(\Omega\equiv[0,1]\) and let \(\mathcal{L}_{p}\equiv\mathcal{L}_{p}([0,1],\mathcal{B},\lambda)\).

We are interested in when each of these three Fourier series representations \(g_{n}\) "converges" to \(f\). We will consider various modes of convergence on these \([c,d]\).

**Theorem 3.5** (Reisz-Fischer): Consider any \(f\in\mathcal{L}_{2}\equiv\mathcal{L}_{2}(\Omega,\mathcal{B},\lambda)\), where we define \(\Omega\equiv[c,d]\). Let \(g_{n}\) denote the Fourier series representation of \(f\) based on the first \(n\) orthonormal functions \(\mathcal{Z}\) (in any one of the three cases considered above). Let \(\phi_{n}\) generically denote the full set of orthonormal functions in \(\mathcal{Z}\), and then let \(a_{n}\equiv\int_{\Omega}f(t)\,\phi_{n}(t)\,dt\) denote the corresponding Fouier coefficients.

(a) The class \(\mathcal{Z}\) is a complete orthonormal basis for \(\mathcal{L}_{2}\). (b) Moreover:

\[a_{n}\to 0 \text{for every $f\in\mathcal{L}_{2}$}. \tag{29}\] \[g_{n}\to_{\mathcal{L}_{2}}\ f \text{for every $f\in\mathcal{L}_{2}$}.\] (30) \[\int_{\Omega}f^{2}(t)\,dt=\sum_{\text{all}\,n}a_{n}^{2} \text{for every $f\in\mathcal{L}_{2}$}.\] (31) \[\int_{\Omega}f(t)\tilde{f}(t)\,dt=\sum_{\text{all}\,n}a_{n}\tilde {a}_{n} \text{for every $f,\tilde{f}\in\mathcal{L}_{2}$}, \tag{28}\]

where \(\tilde{a}_{n}\equiv\int_{\Omega}\tilde{f}(t)\,\phi_{n}(t)\,dt\) denotes the Fourier coefficients of the other \(\tilde{f}\in\mathcal{L}_{2}\).

**Theorem 3.6** (Carleson-Hunt): Let \(1<r\leq\infty\). Then

\[g_{n}\to_{a.e.}f\quad\text{for every $f\in\mathcal{L}_{r}([c,d],B,\lambda)$}. \tag{32}\]

**Definition 3.4** (Piecewise continuous): Suppose \(f\) is continuous at all but a finite number of points in \([c,d]\), while right and left hand limits of \(f\) do exist at each of these exceptional points. Then call \(f\)_piecewise continuous_ on \([c,d]\).

**Theorem 3.7**: (a) Let \(f\) be piecewise continuous on \(\Omega=[c,d]\), and suppose the two one-sided derivatives \(f^{\prime}_{+}(t)\) and \(f^{\prime}_{-}(t)\) both exist at a fixed \(t\in(c,d)\). Then

\[g_{n}(t)\to g(t)\equiv\frac{1}{2}[f_{+}(t)+f_{-}(t)]=\frac{1}{2}[f(t+)+f(t-)]. \tag{33}\]

**(Jordan condition)** (b) Let \(f\in\mathcal{L}_{1}([c,d],\mathcal{B},\lambda)\), and suppose that \(f\) is of bounded variation in some neighborhood of the point \(t\in(c,d)\). Then

\[g_{n}(t)\to g(t)\equiv\frac{1}{2}[f_{+}(t)+f_{-}(t)]=\frac{1}{2}[f(t+)+f(t-)]. \tag{34}\]

(c) Let \(f\) be continuous and of bounded variation on \([c,d]\). Then \(g_{n}\to f\) uniformly on every subinterval \([c^{o},d^{o}]\) having \(c<c^{o}<d^{o}<d\). (Uniform convergence of \(g_{n}\) to \(f\) on any interval containing a discontinuity of \(f\) is impossible. _Gibb's phenomenon_ is the name given to what goes wrong. To prevent it, endpoint conditions are assumed in the next theorem.)

**Theorem 3.8** Let \(f\) be continuous on \(\Omega=[c,d]\). We require \(f(-\pi)=f(\pi)\) in case (23) on \([-\pi,\pi],f(0)=f(1)=0\) in (27) on \([0,1]\), nothing in (26) on \([0,1]\).

Suppose that \(f^{\prime}\) is piecewise continuous on \(\Omega\). Then:

\[g_{n}(t)\to f(t) \mbox{converges absolutely for every }t\in\Omega=[c,d]. \tag{36}\] \[g_{n}\to f \mbox{uniformly on }\Omega=[c,d].\] (37) \[g^{\prime}_{n}(t)\to f^{\prime}(t) \mbox{for every }t\in(c,d)\quad\mbox{where }f^{\prime\prime}(t) \mbox{ exists.} \tag{35}\]

If \(f\) has \(m-1\) continuous derivatives and a piecewise continuous \(m\)th derivative, then \(g_{n}^{(k)}\to f^{(k)}\) as in (35)-(36) for \(1\leq k\leq m-1\), and as in (37) for \(k=m\).

**Theorem 3.9** Let \(f\) be piecewise continuous on \(\Omega=[c,d]\). Then

\[\int_{c}^{t}g_{n}(s)\,ds\to\int_{c}^{t}f(s)\,ds\quad\mbox{for all }t\in\Omega=[c,d] \tag{38}\]

(no matter how badly the \(g_{n}\) may fail to converge to \(f\)).

**Proposition 3.2** Let \(f\in{\cal L}_{1}\). If all \(a_{n}=0\), then \(f(t)\equiv 0\) on \(\Omega\) (in each case).

**Theorem 3.10** (Cesaro summability of Fourier series) Let \({\cal L}_{r}([-\pi,\pi],{\cal B},\lambda)\), for some fixed \(r\geq 1\). For the Fourier series representations \(g_{n}\) above, let

\[h_{n}(t)\equiv\frac{1}{n+1}{\sum_{j=0}^{n}g_{j}(t)}\qquad\mbox{on}\quad[0,1] \quad(\mbox{traditionally},h_{n}\mbox{ is denoted as }\sigma_{n}f). \tag{39}\]

Then the _Cesaro sums_\(h_{n}\) converge to \(f\) in the sense:

\[h_{n}\to_{{\cal L}_{r}}f \mbox{for every }f\in{\cal L}_{r}. \tag{41}\] \[h_{n}\to_{a.e.}f \mbox{for every }f\in{\cal L}_{r}. \tag{40}\]

**Exercise 3.9** Obtain the Fourier series representation of \(f(t)=t\) on \([-\pi,\pi].\) Use the previous theorems to claim that \({\sum_{1}^{\infty}n^{-2}=\pi^{2}/6}\).

**Exercise 3.10** (a) Show that \(f(t)\equiv(\pi-|t|)^{2}=\frac{\pi^{2}}{3}+{\sum_{n=1}^{\infty}\frac{4}{n^{2}} \cos nt}\) on \([-\pi,\pi]\) (with convergence in the sense of \({\cal L}_{2}\) and of (35)-(37)). In particular, now show that \(a_{0}=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(t)\,dt=\pi^{2}/3\) and \(a_{n}=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(t)\cos ntdt=2/n^{2}\) for \(n\geq 1\).

(b) Show that for all \(0\leq s,t\leq 1\) we have

\[C(s,t)\equiv{\sum_{n=1}^{\infty}\frac{\phi_{n}(s)\,\phi_{n}(t)}{n^{2}\pi^{2}} }\equiv{\sum_{n=1}^{\infty}\frac{2\sin n\pi s\,\sin n\pi t}{n^{2}\pi^{2}}}=s \wedge t-st. \tag{42}\]

**Other Orthonormal Systems**

**Exercise 3.11** (Legender polynomials) Consider \({\cal L}_{2}([-1,1],{\cal B},\lambda)\). Define

\[\phi_{n}(t)\equiv\frac{\sqrt{2n+1}}{2^{n}\sqrt{2}n!}\,\frac{d^{n}}{dt^{n}}(t^{ 2}-1)^{n}\qquad\mbox{for }n\geq 1. \tag{43}\]

These are the _Legender polynomials_.

(a) Show that by beginning with \(1\), \(t,t^{2},\ldots\) and then applying the Gram-Schmidt orthogonalization process, one arrives at these orthogonal polynomials.

(b) Also, verify that these polynomials are orthonormal.

(c) Verify that they form a basis.

**Exercise 3.12** (Hermite polynomials) See (11.6.4), (11.6.15), and exercise 11.6.1 for the orthonormal set of polynomials \(H_{n}/\sqrt{n}\) (for \(n\geq 0\)). (a) Show that they are orthonormal relative to \((R,{\cal B},P)\), with \(P\) denoting the \(N(0,1)\) measure.

(b) Verify that they form a basis.

**Bounded Linear Functionals on \({\cal L}_{r}\)**

**Exercise 3.13** (Reisz representation theorem) Fix \(1<r<\infty\), and define \(s\) via \(1/r+1/s=1\). A linear functional \(g\) on \({\cal L}_{r}\equiv{\cal L}_{r}(\Omega,{\cal A},\mu)\) is bounded if and only if there is a measurable function \(Y\in{\cal L}_{s}\equiv{\cal L}_{s}(\Omega,{\cal A},\mu)\) for which \(g(X)={\rm E}(XY)\) for all \(X\in{\cal L}_{r}\). The norm of this bounded linear functional \(g\) is given by \(\|g\|=\|Y\|_{s}.\) Thus the collection \({\cal L}_{r}^{*}\) of all bounded linear functionals on \({\cal L}_{r}\) is isomorphic to \({\cal L}_{s}\) (in that norms are preserved).

## Appendix C More WLLN and CLT \({}^{o}\)

### 0 Introduction

The beginning section C.1 can be viewed as a continuation of the WLLN theme, that now allows the case of a possibly infinite moment. To do so, we choose to estimate a moment when trimming a vanishingly small proportion from the tail of the moment. This leads to what could be referred to as the _ratio_\(LLN\). Such estimation of a second moment is intimately connected to determining the most general possible setting for a CLT for sums of independent rvs that are each individually rather negligible. (A version of this material appeared in the 1_st_ Edition as the sixth section of the LLNs chapter ten, though it has now been streamlined and improved.)

Such results depend on a careful understanding of the contributions of the tails of a df to its moments. This topic is also developed in sections C.2-C.3 and some of that is used in section C.1. (Sections 6.5 and 6.6 derive inequalities capable of yielding necessary and sufficient conditions for asymptotic normality for untrimmed, trimmed, and Winsorized averages, as well as the extension of such results to linear combinations of functions of order statistics and linear rank statistics.) These six sections could be viewed as a continuation of chapters 6 and 8 of the present text. A short treatment of the applications referred to above appears in chapter 15 of this text. These six sections on the df appeared in the 1_st_ Edition as sections seven through eleven of chapter seven, section six of chapter 10, and a more complete treatment of the applications that carefully treated uniform convergence to normality of the various statistics under very general conditions appeared in a chapter sixteen. Roughly then, nearly all this theoretical material has been moved to appendix C, with a greatly reduced version of the applications moved to chapter 15. The exception is chapter 11 where some of this remaining material is cited in developing the domain of attraction of the Normal distribution.

[MISSING_PAGE_FAIL:470]

[MISSING_PAGE_FAIL:471]

We now show that (6) implies (4) (and we are entitled to use (12) to do so). Fix \(0<\theta<\epsilon\). The truncation inequality (8.3.23) (the proof is repeated below) gives

(d) \[P(|\bar{X}_{n}/u_{\theta n}-1|>\epsilon)=P(|\bar{X}_{n}-u_{\theta n }|>\epsilon\,u_{\theta n})\] \[\leq P(|\overline{Z_{n}^{\theta}}-u_{\theta n}|>\epsilon\,u_{ \theta n})+\sum_{1}^{n}P(X_{nk}\neq Z_{nk}^{\theta})\qquad\qquad\mbox{(recall (a) for $Z_{nk}^{\theta}$)}\] (e) \[\leq\sum_{1}^{n}\!\mbox{E}((Z_{nk}^{\theta})^{2})/(\epsilon^{2}u_{ \theta n}^{2}n^{2})+n\bar{P}_{n}(x_{\theta n})\] (f) \[=\int_{[|y|\leq x_{\theta n}]}y^{2}d\bar{F}_{n}(y)/(\epsilon^{2}n \,u_{\theta n}^{2})+n\bar{P}_{n}(x_{\theta n})\] (g) \[\leq x_{\theta n}\int_{[|y|\leq x_{\theta n}]}|y|d\bar{F}(y)/( \epsilon^{2}n\,u_{\theta n}^{2})+\theta\] (h) \[=\epsilon^{-2}\{x_{\theta n}/n\,u_{\theta n}\}+\theta\leq\epsilon ^{-2}\{x_{\theta n}/nu_{1n}\}+\theta\] (i) \[\leq 2\epsilon\qquad\qquad\mbox{ for all $n$ exceeding some even larger $n_{\epsilon\,\theta}$}\qquad\qquad\mbox{(using (\ref{eq:P})).}\]

Thus for \(n\) large enough, (12) and (i) show that

(j) \[\frac{|\overline{X_{n}^{r}}-u_{1n}|}{u_{1n}}=\frac{u_{\theta n}}{u_{1n}}\cdot \left|\frac{\overline{X_{n}^{r}}-u_{\theta n}}{u_{\theta n}}\right|+\left| \frac{u_{\theta n}}{u_{1n}}-1\right|\leq(1+\epsilon)\epsilon+\epsilon\leq 3\epsilon\]

with probability at least \(1-2\epsilon\). Thus (4) holds. That is, (6) implies (4).

We next show that (4) implies (8). Now (4) implies that

(k) \[W_{n}^{s}\equiv\sum_{1}^{n}\!W_{nk}^{s}\equiv\sum_{1}^{n}\![(X_{nk}/n\,u_{1n} )-(X_{nk}^{\prime}/n\,u_{1n})]\to_{p}1-1=0,\]

for the symmetrized rvs \(W_{nk}^{s}\). Then Levy's inequality (8.3.4) and (k) give

(l) \[P(\max_{k}|W_{nk}^{s}|>\epsilon)\leq 2P(|W_{n}^{s}|>\epsilon)\to 0,\qquad\qquad\mbox{ for all $\epsilon>0$}.\]

It further holds that

\[P(\max_{k}|(X_{nk}/n\,u_{1n})-\mbox{median}(X_{nk}/n\,u_{1n})|>\epsilon)\] \[\leq\sum_{1}^{n}\!P(|(X_{nk}/n\,u_{1n})-\mbox{median}(X_{nk}/n\,u_ {1n})|>\epsilon)\] \[\leq 2{\sum_{1}}^{n}\!P(|W_{nk}^{s}|>\epsilon)\qquad\mbox{by the symmetrization inequality (\ref{eq:W_n})}\] (m) \[\to 0\quad\mbox{by (\ref{eq:W_n})},\qquad\qquad\mbox{since $\max_{k}|W_{nk}|\to_{p}0$ by $(\ell)$}. \tag{13}\]

That is,

(n) \[\max_{k}|(X_{nk}/n\,u_{1n})-\mbox{median}(X_{nk}/n\,u_{1n})|\to_{p}0.\]

Since the un condition in (4) yields \(\{\max_{k}\,|\mbox{median}(X_{nk})/n\,u_{1n})|\}\to 0\), we have shown from (n) that \(M_{n}=[\max\frac{1}{n}|X_{nk}|^{r}]\) (for \(r=1\)) satisfies

(o) \[\frac{1}{u_{1n}}M_{n}\to_{p}0,\qquad\mbox{or equivalently that}\quad n\bar{P}_{n}(\epsilon\,nu_{1n})\to 0\quad\mbox{for all $\epsilon>0$}.\]

Thus (4) implies (8), so (4) is added. Thus (4)-(8) are equivalent. \(\Box\)

**Exercise 1.2**  Prove corollary 1 to theorem 1.1 by adapting (d)-(j) in the previous proof in light of the truncation equality (8.3.23).

**Proof.** We now add (3) and (9). Consider (3) implies (9) first, and then (3) implies (4). Steps (k)-(o) in the previous proof (with \(u_{1n}\) replaced by \(\nu_{n}\)) yield

(a) \[\frac{1}{\nu_{n}}M_{n}\to_{p}0,\qquad\mbox{or equivalently that}\qquad n\bar{P}_{n}(\epsilon\,n\nu_{n})\to 0\quad\mbox{for all }\epsilon>0\]

(using (8.3.17)-(8.3.19) for the equivalence). Let

\[\check{X}_{n}\equiv\tfrac{1}{n}{\sum_{1}^{n}}X_{nk}{\mathbb{1}}_{[X_{nk}\leq n \nu_{n}]}\quad\mbox{and let}\quad\check{\mu}_{n}\equiv{\rm E}\check{X}_{n}\equiv{\int_{[|y|\leq n\nu_{n}]}|y|d\bar{F}_{n}(y)}. \tag{14}\]

Use (3) with \(P(\check{X}_{n}\neq\check{X}_{n})\leq{\sum_{1}^{n}}P(X_{nk}>n\nu_{n})=n\bar{P}_{n}(n\nu_{n})\to 0\) (by (a)) to get

\[\tfrac{1}{\nu_{n}}\check{X}_{n}\to_{p}1. \tag{15}\]

Use (8.3.25) and \(c_{n}\equiv n\,\nu_{n}\) and the second half of (a) to obtain the bound

(b) \[P({\lfloor\tfrac{1}{\nu_{n}}\check{X}_{n}-\tfrac{1}{\nu_{n}}\check{\mu}_{n}| \geq\epsilon})\leq\tfrac{1}{\epsilon^{2}}\check{\mu}_{n}/\nu_{n}+n\bar{P}_{n}(n\,\nu_{n})=\tfrac{1}{\epsilon^{2}}\check{\mu}_{n}/\nu_{n}+o(1).\]

Assume that \(\check{\mu}_{n}/\nu_{n}\to 0\) on some subsequence \(n^{\prime}\). Then (b) implies that

(c) \[\tfrac{\check{X}_{n^{\prime}}}{\nu_{n^{\prime}}}=\tfrac{\check{X}_{n^{\prime}}- \check{\mu}_{n^{\prime}}}{\nu_{n^{\prime}}}+\tfrac{\check{\mu}_{n^{\prime}}}{ \nu_{n^{\prime}}}=o_{p}(1)+o(1)\to_{p}0;\]

but this contradicts (15). Thus

(d) \[\overline{\lim}_{n}\nu_{n}/\check{\mu}_{n}\leq(\mbox{some }M)<\infty.\]

Combined with (a), this gives

(e) \[\tfrac{1}{\check{\mu}_{n}}M_{n}\to_{p}0,\qquad\mbox{or equivalently that}\qquad n\bar{P}_{n}(\epsilon\,n\check{\mu}_{n})\to 0\quad\mbox{for all }\epsilon>0,\]

by (8.3.17)-(8.3.19) again. With \(\check{\mu}_{n}={\rm E}\check{X}_{n}={\int_{[|y|\leq n\nu_{n}]}|y|d\bar{F}_{n}(y)}\) as in (14), let

\[\check{Z}_{n}\equiv\tfrac{1}{n}{\sum_{1}^{n}}X_{nk}{\mathbb{1}}_{[X_{nk}\leq n \check{\mu}_{n}]}\quad\mbox{and let}\quad\check{\mu}_{n}\equiv{\rm E}\check{Z}_{n}={ \int_{[|y|\leq n\check{\mu}_{n}]}|y|d\bar{F}_{n}(y)}. \tag{16}\]

It follows that

\[\tfrac{\check{\mu}_{n}}{\check{\mu}_{n}}=\tfrac{1}{\check{\mu}_{n}} \int_{[|y|\leq n\check{\mu}_{n}]}|y|d\bar{F}_{n}(y)\] \[\quad=1\oplus{\mathbb{1}}_{[\check{\mu}_{n}\leq\nu_{n}]}\cdot \tfrac{\nu_{n}}{\check{\mu}_{n}}\ \cdot n\bar{P}_{n}(n\check{\mu}_{n})\oplus{\mathbb{1}}_{[\check{\mu}_{n}>\nu_{n}]} \cdot\tfrac{\check{\mu}_{n}}{\check{\mu}_{n}}\cdot n\bar{P}_{n}(n\nu_{n})\] \[\quad=1+O(1)\cdot o(1)+1\cdot o(1)\qquad\mbox{using (d) and (e), and then (a)}\] (f) \[\to 1.\]

We will now use (e) and (f) to show (with \(\check{\mu}_{n}={\rm E}\check{X}_{n}={\int_{[|y|\leq n\nu n]}}x\,d\bar{F}_{n}(x)\)) that

\[\tfrac{1}{\check{\mu}_{n}}\check{Z}_{n}=\tfrac{1}{\check{\mu}_{n}}\tfrac{1}{n}{ \sum_{1}^{n}}X_{nk}{\mathbb{1}}_{[X_{nk}\leq n\check{\mu}_{n}]}\to_{p}1. \tag{17}\]

Let \(Z^{\epsilon}_{nk}\equiv X_{nk}{\mathbb{1}}_{[X_{nk}\leq\epsilon^{3}n\check{\mu}_{n}]}\), and let \(\check{\mu}_{en}\equiv{\rm E}\overline{Z^{\epsilon}_{n}}\) with \(\check{\mu}_{1n}=\check{\mu}_{n}\). Then (e) gives

(g) \[0\leq\tfrac{\check{\mu}_{1n}-\check{\mu}_{en}}{\check{\mu}_{n}}\leq\tfrac{1}{n \check{\mu}_{n}}{\sum_{1}^{n}}n\check{\mu}_{n}P(X_{nk}>\epsilon^{3}n\check{\mu}_{n})=n \bar{P}_{n}(\epsilon^{3}n\check{\mu}_{n})\to 0.\]

Then for \(n\geq(\mbox{some }n_{\epsilon})\) the truncation inequality (8.3.25) gives

\[P(|\check{Z}_{n}-\check{\mu}_{en}|>\epsilon\check{\mu}_{en})\leq P (|\check{Z}^{\epsilon}_{n}-\check{\mu}_{en}|>\epsilon\check{\mu}_{en})+n\bar{P}_{n}(\epsilon^{3}n\check{\mu}_{n})\] \[\quad\leq\tfrac{{\sum_{1}^{n}}{\rm E}(Z^{\epsilon}_{nk})^{2}}{ \epsilon^{2}\check{\mu}_{en}^{2}n^{2}}+n\bar{P}_{n}(\epsilon^{3}n\check{\mu}_{n}) \leq\tfrac{\epsilon^{3}n\check{\mu}_{n}}{\epsilon^{2}\check{\mu}_{en}^{2}n^{2}}{ \sum_{1}^{n}}{\rm E}Z^{\epsilon}_{nk}+n\bar{P}_{n}(\epsilon^{3}n\check{\mu}_{n})\] \[\quad=\epsilon\tfrac{\check{\mu}_{n}}{\check{\mu}_{en}}+n\bar{P}_{n }(\epsilon^{3}n\check{\mu}_{n})\]

[MISSING_PAGE_EMPTY:9261]

versions of (4)-(6) are each equivalent to each of the "\(u\)" versions. Then (8.3.17)-(8.3.19) show that the \(\tilde{u}\) versions of (6)-(8) are equivalent to each other. Thus the "\(\tilde{u}\)" claims in (D) hold. (Comment: Exercise 1.3 above holds (with \(r=1\)) by applying the final remark in (C).) \(\Box\)

**Proof.** We will now show that (7) and (10) are equivalent. Suppose (7), which we already know implies (4). Then

(a) \[[\max X_{nk}/n]/\bar{X}_{n}=\{\max X_{nk}/n\,u_{1n}\}\times\{u_{1n}/\bar{X}_{n}\}\] (b) \[=o_{p}(1)[1/(1+o_{p}(1))]\to_{p}0,\]

using (7) and then (4) in the last step. The uan condition in (10) is implied by the stronger (7). This shows that (7) implies (10); that is, it establishes that

(c) \[M_{n}/\bar{X}_{n}\to_{p}0.\]

Suppose, alternatively, that (7) fails for some uan array of row independent \(X_{nk}\). (Note O'Brien (1980) for the iid analog.) Then for a fixed tiny \(\epsilon>0\),

(d) \[\sum_{1}^{n}P(X_{nk}>\epsilon\,n\,u_{1n})\geq(\mbox{some $\alpha\in(0,1]$}),\quad\mbox{on some subsequence $m\to\infty$}.\]

Let \(A_{mk}\equiv[X_{mk}>\epsilon\,m\,u_{1m}]\), let \(C_{m}\equiv[\mbox{exactly 1 of the $X_{mk}$'s exceed $\epsilon\,m\,u_{1m}$}]\), and let \(D_{km}\equiv[\mbox{at least k of the $X_{mk}$'s exceed $\epsilon\,m\,u_{1m}$}]\). Bounding \(P(D_{1m})\) below by inclusion/exclusion gives

(e) \[P(C_{m})=P(D_{1m})-P(D_{2m})\] \[\geq\{\sum_{k=1}^{m}P(A_{mk})-\sum_{k=1}^{m}\!\sum_{i\neq k}\!P(A_ {mi})P(A_{mk})\}-P(D_{2m})\] (f) \[\geq\mbox{$\frac{1}{2}$}\!\sum_{k=1}^{m}\!P(A_{mk})-\{\max_{1\leq k \leq m}\!P(A_{mk})\}\{\sum_{k=1}^{m}\!P(A_{mk})\}\mbox{ (see (19))}\] (18) \[\geq\mbox{$\frac{1}{4}$}\!\sum_{k=1}^{m}\!P(A_{mk})\qquad\qquad \qquad\mbox{for all $n\geq\mbox{ (some $n_{3\epsilon}$)},\mbox{ by the uan condition,}$}\] (g) \[>\alpha/4\qquad\qquad\qquad\qquad\mbox{for all $m,\mbox{ by (d)}$},\]

since uan gives (18) once the Markov inequality gives (f) via

\[P(D_{2m})=P(1_{A_{m1}}+\cdots+1_{A_{mm}}\geq 2)\leq\mbox{$\frac{1}{2}$} \mbox{E}(\sum_{1}^{m}\!1_{A_{mk}})\leq\mbox{$\frac{1}{2}$}\!\sum_{1}^{m}\!P(A_ {mk}). \tag{19}\]

Now let \(S_{m}\equiv\sum_{1}^{m}\!X_{mk}\),

\[X^{\prime}_{mk}\equiv X_{mk}\cdot 1_{[X_{mk}\leq\epsilon\,m\,u_{1m}]},S^{ \prime}_{m}\equiv\sum_{1}^{m}\!X^{\prime}_{mk},\quad\mbox{and}\quad\bar{X}^{ \prime}_{m}\equiv\mbox{$\frac{1}{m}$}S^{\prime}_{m}\]

and note that

(h) \[\mu^{\prime}_{m}\equiv\mbox{E}(\overline{X^{\prime}}_{m})=\mbox{$\frac{1}{m} $}\!\sum_{1}^{m}\int_{[0,\epsilon\,m\,u_{1m}]}xdF_{mk}(x)=U_{m}(\epsilon\,m\,u_ {1m}).\]

Fix \(\beta\equiv\alpha\epsilon^{3}/2\) (note that \(1/\beta\geq 2\)). Then the Markov inequality gives

(i) \[P(G_{m})\equiv P(S^{\prime}_{m}\geq\mbox{$\frac{1}{\beta}$}\epsilon\,m\,u_{1m} )\leq\mbox{$\frac{\beta\,m}{\epsilon\,m}\frac{U_{m}(\epsilon\,m\,u_{1m})}{u_{1 m}}$}=\mbox{$\frac{\beta}{\epsilon}\frac{U_{m}(\epsilon\,m\,u_{1m})}{U_{m}(x_{1m})}$}\]

\[\leq\mbox{$\frac{\beta}{\epsilon}\{[u_{1m}+\int_{(x_{1m},\epsilon\,m\,u_{1m} ]}yd\bar{F}_{m}(y)]1_{[x_{1m}\leq\epsilon mu_{1m}]}$}\]

\[+u_{1m}1_{[x_{1m}>\epsilon uu_{1m}]\}/u_{1m}\]\[\leq\frac{\beta}{\epsilon}\{1+\epsilon\,m\,u_{1m}\frac{\bar{P}_{m}(x_{1m})}{u_{1m}}\}\] \[\leq\frac{\beta}{\epsilon}\{1+\epsilon{\sum_{1}^{m}}P(X_{mk}>x_{1m}) \}\leq\frac{2\beta}{\epsilon}\] by definition of

\[x_{1m}\] (j) \[<\epsilon^{2}\alpha,\] for

\[m\geq\]

(some

\[m_{\epsilon}\]

).

Combining (g) and (j) will give

\[P(M_{m}/\bar{X}_{m}>\frac{1}{2}\beta)\geq P([X_{m:m}/S_{m}>\frac{1}{2}\beta] \cap C_{m})\]

\[=P([S_{m}<\frac{2}{\beta}X_{m:m}]\cap C_{m})\]

\[=P([S^{\prime}_{m}+(X_{m:m}-X^{\prime}_{m:m})<\frac{2}{\beta}X_{m:m}]\cap C_{m})\]

\[=P([S^{\prime}_{m}<(\frac{1}{\beta}+(\frac{1}{\beta}-1))X_{m:m}+X^{\prime}_{m: m}]\cap C_{m})\]

\[\geq P([S^{\prime}_{m}<\frac{1}{\beta}X_{m:m}]\cap C_{m})=P([S^{\prime}_{m}/X_ {m:m}<\frac{1}{\beta}]\cap C_{m})\mbox{ as }1/\beta\geq 2\]

\[\geq P([S^{\prime}_{m}/\epsilon\,m\,u_{1m}<\frac{1}{\beta}]\cap C_{m})=P(G^{c} _{m}\cap C_{m})\qquad\qquad\mbox{using (i)}\]

\[\geq P(G^{c}_{m})+P(C_{m})-1\mbox{ as any }P(G^{c})+P(C)=P(G^{c}\cap C)+P(G^{c} \cup C)\]

\[\geq(1-\epsilon^{2}\alpha)+\alpha/4-1=\alpha(\frac{1}{4}-\epsilon^{2})>0\qquad \mbox{on the subsequence }m,\]

using (j) and (g) in (m). Thus \(M_{n}/\bar{X}_{n}\neq 0\) on the subsequence \(m\). Thus

\[M_{n}/\bar{X}_{n}\nto 0.\]

So, the failure of (7) implies the failure of (10); or (10) implies (7). Thus (c) and (n) give the equivalence of (7) and (10). (This present proof replaces the proof that added (10) to the list in the first edition of this text.) \(\Box\)

**Independent Identically Distributed RVs (or, the IID Case)**

**Notation 1.2** (**Absolute moments in the id case**): Let \(Y\) and all rvs \(Y_{nk}\) have a common df \(F(\cdot)\equiv F_{Y}(\cdot)\) and qf \(K(\cdot)\equiv K_{Y(\cdot)}\). (Let \(X\equiv|Y|\). The rvs \(X_{nk}\equiv|Y_{nk}|\) may be represented as \(K_{X}(\xi_{nk})\) for row-independent Uniform(0, 1) rvs \(\xi_{n1},\ldots,\xi_{nn}\), as in (6.3.5).) Then \(P(\cdot)\equiv 1-F_{X(\cdot)}\). The _truncated \(rth\) absolute moment functions_\(M(\cdot)\) and \(U(\cdot)\) (as defined in discussion 1.1) are defined by

\[M(t)\equiv\int_{[0,1-t]}K^{r}_{X}(s)\,ds\mbox{ on }(0,1],\ U(x)\equiv\int_{[y \leq x]}y^{r}dF_{X}(y)\mbox{ on }[0,\ \infty).\]

Let \(m_{\theta n}\equiv M(\theta/n)\) and \(u_{\theta n}\equiv U(x_{\theta n})\), where \(x_{\theta n}\equiv K_{X}(1-\theta/n)\) is defined to be the \((1-\theta/n)\)th quantile of \(F_{X}\). The _Winsorized absolute moment_

\[\tilde{m}_{1n}\equiv m_{1n}+\frac{1}{n}m(1/n)\quad\mbox{equals}\quad\tilde{u}_ {1n}\equiv u_{1n}+x^{r}_{1n}P(x_{1n}),\]

even though \(\frac{1}{n}m(\frac{1}{n})\geq x^{r}_{1n}P(x_{1n})\) as in theorem 1.1. \(\Box\)

**Comment** Section C.3 is devoted to showing that the four conditions in (25) and (26) are equivalent. (This section could be read now.) Note also exercise 6.3.3.

**Theorem 1.2** (Consistency of absolute moments for iid rvs; estimation): For iid rvs \(X_{nk}\geq 0\) as above, conditions (3)-(10), and (23)-(32) are equivalent.

(23) \[\overline{X_{n}^{r}}/\nu_{n}\to_{p}1\] for some sequence

\[\nu_{n}>0\]

. (24) \[\begin{array}{ll}\mbox{(a)}\ \overline{\frac{X_{n}^{r}}{u_{1n}}}\to_{p}1,&\mbox{(b)}\frac{\overline{X_{n}^{r}}}{\tilde{u}_{1n}}=\frac{\overline{X_{n}^{r}}}{\tilde{m}_{1n}}\to_{p}1,&\mbox{(c)}\ \frac{\overline{X_{n}^{r}}}{\tilde{m}_{1n}}\to_{p}1.\\ \\ R(x)\equiv\frac{x^{r}P(x)}{U(x)}=\frac{x^{r}P(X>x)}{\int_{[\nu_{x}\leq\iota]}w^{r} dF_{X}(w)}\to 0,&\mbox{or}\quad R(x-)\to 0.\\ \\ r(t)\equiv\frac{tK_{X}^{r}(1-t)}{\int_{[0,1-\iota)}K_{X}^{r}(s)\,ds}\to 0,&\mbox{or}\quad r(t+)\to 0.\\ \\ r(\epsilon/n)\to 0&\mbox{for every/any }0<\epsilon\leq 1.\\ \\ n\,P(X^{r}>\epsilon\,n\,m_{1n})\to 0&\mbox{for all }\epsilon>0.\\ \\ M_{n}/m_{1n}\equiv[\max_{1\leq k\leq n}\frac{1}{n}X_{nk}^{r}]/m_{1n}\to_{p}0. \\ \\ M_{n}/\bar{X_{n}^{r}}\to_{p}0.&\mbox{(31)}\quad M_{n}/\nu_{n}\to_{p}0\quad \mbox{and}\quad\tilde{\mu}_{n}/\nu_{n}\to 1,&\mbox{where }\tilde{\mu}_{n}\equiv U(n\,\nu_{n}).\end{array}\] (25)

Of course, \(u_{1n}/\nu_{n}\to 1,\tilde{u}_{1n}/\nu_{n}=\tilde{m}_{1n}/\nu_{n}\to 1\), and \(m_{1n}/\nu_{n}\to 1\) under (23). Moreover, \(\tilde{\mu}_{n}/\nu_{n}\to 1\) with \(\tilde{\mu}_{n}\equiv U(n\,\nu_{n})\), for any \(\nu_{n}\) used in (23) or (31). Also,

(26) \[\begin{array}{ll}u_{1n},\tilde{u}_{1n}=\tilde{m}_{1n},m_{1n}\mbox{ can replace }\nu_{n}\mbox{ in (23) or (31)},\\ \mbox{or they can replace }m_{1n}\mbox{ in (28) or (29)}.\end{array}\] (27)

**Proof.** Of course, all of (3)-(10) are still equivalent in this iid case. (However, the un condition is now redundant, since the claim just below (g) in the proof of theorem 1.1 that implied \(\{\max_{k}\mbox{median}(X_{nk})/n\,u_{1n}\}\to 0\) is now trivial in this iid case. The un consequence in (18) is also trivial.) Part (D) of theorem 1.1 showed that \(\tilde{u}_{1n}\) can everywhere replace \(u_{1n}\), and this justifies the inclusion of (24)(b). Note that (23), (24)(a), (30), and (31) are identical to (3), (4), (9), and (10).

We first show that (27) implies (24)(c). For \(X_{nk}\equiv K_{X}(\xi_{nk})\), define

(a) \[Z_{nk}^{\theta}\equiv K_{X}(\xi_{nk})1_{[\xi_{nk}<1-\theta/n]}\quad\mbox{and} \quad m_{\theta n}\equiv\mbox{E}Z_{nk}^{\theta}=\int_{[0,1-\theta/n)}K_{X}(s) \,ds.\]

Fix \(\epsilon>0\). Then for a fixed tiny \(\theta\equiv\theta_{\epsilon}<\epsilon\), the truncation inequality (8.4.25) gives

(b) \[\begin{array}{ll}P(|\bar{X}_{n}-m_{\theta n}|>\epsilon\,m_{\theta n})\leq \sum_{1}^{n}\mbox{E}(Z_{nk}^{\theta})^{2}/(\epsilon\,n\,m_{\theta n})^{2}+n \cdot(\theta/n)\\ \\ \leq\frac{\int_{[0,1-\theta/n)}K_{X}^{2}(s)\,ds}{\epsilon^{2}nm_{\theta n}^{2} }+\theta\leq\frac{(\theta/n)K_{X}(1-\theta/n)}{\theta\epsilon^{2}m_{\theta n }}+\theta\\ \\ \mbox{(d)}\quad=\frac{1}{\theta\epsilon^{2}}\frac{(\theta/n)K_{X}(1-\theta/n) }{M(\theta/n)}+\theta=\frac{1}{\theta\epsilon^{2}}r(\theta/n)+\theta<2 \epsilon\end{array}\] (b)

for \(n\) sufficiently large, by (27). In like fashion

(e) \[1\geq\frac{m_{1n}}{m_{\theta n}}=\frac{\int_{[0,1-1/n)}K_{X}(s)\,ds}{\int_{[0, 1-\theta/n)}K_{X}(s)\,ds}\geq 1-\frac{(1-\theta)}{\theta}\frac{(\theta/n)K_{X}(1- \theta/n)}{\int_{[0,1-\theta/n)}K_{X}(s)\,ds}\geq 1-\epsilon\]for all \(n\) sufficiently large, by (27). Thus, for \(n\geq\) (some \(n_{\epsilon}\)) we have

(f) \[\tfrac{|\bar{X}_{n}-m_{1n}|}{m_{1n}}=\tfrac{m_{\theta n}}{m_{1n}}|\tfrac{\bar{X} _{n}-m_{\theta n}}{m_{\theta n}}+1-\tfrac{m_{1n}}{m_{\theta n}}|\leq\tfrac{1}{ 1-\epsilon}[\epsilon+\epsilon]\leq\tfrac{2\epsilon}{1-\epsilon}\]

with probability at least \(1-2\epsilon\). That is, \(\bar{X}_{n}/m_{1n}\to_{p}1\), giving (24)(c).

Next, (24)(c) implies (29) (and its equivalent (28)); just replace \(u_{1n}\) by \(m_{1n}\) in that part of the proof (i.e., (k)-(o)) of theorem 1.1 that yields (4) implies (8).

We now prove that (28) implies (27), for the specific choice \(a_{n}=\theta/n\), for any fixed \(0<\theta\leq 1\). State (28) (recall (8.3.19)) as

(g) \[P(K_{X}(1-\xi)/(n\,m_{1n})>\epsilon)\leq\theta/n\qquad\text{for all $n\geq\ ( \text{some $n_{\epsilon\theta}$})$.}\]

This implies that \(K(1-\theta/n)\leq\epsilon\,n\,m_{1n}\), and so

(h) \[r(\theta/n)\equiv(\theta/n)K_{X}(1-\theta/n)/m_{\theta n}\leq(\theta/n\,m_{ \theta n})\epsilon\,n\,m_{1n}\leq\epsilon\,\theta\,m_{1n}/m_{\theta n}\leq\epsilon\]

for all \(n\geq n_{\epsilon\theta}\); that is, (27) holds with \(t=a_{n}=\theta/n\). Thus (27) holds.

That each of the two parts of (25) is equivalent to each part of (26) follows from (C.3.6), which is established simply by appealing to pictures of the graphs of \(F\) and \(K\). Then (26) trivially implies (27), while the proof that (27) implies (26) just mimics (m)-(n) on page 490. Finally, (24)(c) implies (23), then (24)(a), then (5), which then implies (27). 

**Comment** At this point the reader should peruse sections C.2\(-\)C.3, and only then come back to complete the reading of this section.

**Notation 1.3** (Moments in the iid case): Let \(Y_{n1},\ldots,Y_{nn}\) be a triangular array of row independent rvs, all with common df \(F\) and qf \(K\). Now define the rvs \(Y_{nk}^{+}\equiv Y_{nk}\times 1_{[Y_{nk}\geq 0]}\) and \(Y_{nk}^{-}\equiv-Y_{nk}\times 1_{[Y_{nk}<0]}\). Fix \(r>0\). Let

\[\begin{array}{ll}&m^{-}(t)\equiv|K_{+}^{-}(t)|^{r},&m^{+}(t)\equiv|K^{+}(1- t)|^{r},\\ &\bar{M}(t)\equiv\int_{(t,1]^{n(r)\,ds}}&\text{where}\qquad\bar{m}(\cdot)\equiv m ^{-}(\cdot)+m^{+}(\cdot),\\ &\bar{m}_{1n}\equiv\bar{M}(1/n)\,\qquad m_{1n}^{-}\equiv\int_{(1/n,1]}m^{-}(s) \,ds,\qquad m_{1n}^{+}\equiv\int_{(1/n,1]}m^{+}(s)\,ds,\\ &m_{1n}^{\pm}\quad\text{simultaneously denote any one of $m_{1n}^{-},m_{1n}^{+}$, or $m_{1n}^{+}\pm m_{1n}^{-}$,}\\ &\bar{Y}_{rn}^{-}\equiv\tfrac{1}{n}\!\sum_{k=1}^{n}[Y_{nk}^{-}]^{r},&\bar{Y}_ {rn}^{+}\equiv\tfrac{1}{n}\!\sum_{k=1}^{n}[Y_{nk}^{+}]^{r},&\bar{Y}_{rn}\equiv \bar{Y}_{rn}^{-}+\bar{Y}_{rn}^{+},\\ &Y_{rn}^{\pm}\quad\text{simultaneously denote any one of $\bar{Y}_{rn}^{-},\bar{Y}_{rn}^{+}$ or $\bar{Y}_{rn}^{+}\pm\bar{Y}_{rn}^{-}$.}\end{array} \tag{33}\]

Make crude plots of \(m^{-}(\cdot)\), \(m^{+}(\cdot)\), \(\bar{m}(\cdot)\) and \(m(\cdot)\equiv K_{[X]}^{r}(1-\cdot)\) before going on.

Let \(K_{X}\) denote the qf of the rv \(X\equiv|Y|\), and then (as in theorem 1.2) let

\[M(t)\equiv\int_{[0,1-t)}K_{|X|}^{r}(s)\,ds=\int_{(t,1]}K_{|X|}^{r}(1-s)\,ds= \int_{(t,1]}m(s)\,ds.\qed \tag{34}\]

**Theorem 1.3** (Consistency of moments in the iid case; as estimators): Fix an \(r>0\). Just some of the possible equivalent conditions are as follows:

\[[\tfrac{1}{n}\!\sum_{k=1}^{n}\!|Y_{nk}|^{r}]/\bar{m}_{1n}\to_{p}1,\qquad\qquad \text{where}\quad\bar{m}_{1n}\equiv\bar{M}(1-1/n). \tag{35}\]

\[\bar{r}(t)\equiv t\bar{m}(t)/\bar{M}(t)=t\bar{m}(t)/\int_{(t,1]^{m(s)\,ds}}\to 0\qquad\text{as $t\to 0$.} \tag{36}\]

\[\bar{r}(a_{n})\to 0\qquad\text{for every choice $a_{n}\equiv\epsilon/n$}\qquad\qquad\text{with $0<\epsilon\leq 1$.} \tag{37}\]

\[M_{n}/\bar{m}_{1n}=[\max_{1\leq k\leq n}\tfrac{1}{n}|K(\xi_{nk})|^{r}]/\bar{m}_{1n}\to_{p}0. \tag{38}\]

\[M_{n}/\overline{|X|_{n}^{r}}=[\max_{1\leq k\leq n}\tfrac{1}{n}|K(\xi_{nk})|^{r}]/[ \tfrac{1}{n}\!\sum_{k=1}^{n}\!|K(\xi_{nk})|^{r}]\to_{p}0. \tag{39}\]

\[M_{n}/\nu_{n}\to_{p}0\qquad\text{and}\qquad\check{\mu}_{n}/\nu_{n}\to 1,\qquad\text{where}\quad\check{\mu}_{n}\equiv U(n\bar{m}_{1n}). \tag{40}\]

(41) \(\bar{m}_{1n}\quad\text{can be added to the choices in (32)}\).

Moreover, these are also equivalent to each of the conditions in the lists of theorem 1.1 and theorem 1.2 such as

(42) \[[\tfrac{1}{n}{\sum_{k=1}^{n}}|Y_{nk}|^{\tau}]/m_{1n}\to_{p}1,\] where \[m_{1n}\equiv M(1/n)\]. (43) \[x_{\theta n}^{\tau}/n\,u_{1n}\to 0\] for all \[0<\theta\leq 1\], where \[u_{1n}\equiv U(x_{1n})\]. (44) \[nP(X^{r}>\epsilon\,n\,u_{n1})\to 0,\] for all \[\epsilon>0\]. (45) \[R(x)=x^{r}P(|X|>x)/\int_{[|y|\leq x]}|y|^{r}dF(x)\to 0\] as \[x\to\infty\].

Fix \(r\geq 1\). Let one (hence, all) of (3)-(10), (23)-(31), or (35)-(45), etc. hold. Then

\[[\bar{Y}_{rn}^{\pm}-m_{1n}^{\pm}]/\bar{m}_{1n}\to_{p}0\qquad\mbox{and}\qquad[ \bar{Y}_{rn}-(m_{1n}^{+}-m_{1n}^{-})]/\bar{m}_{1n}\to_{p}0. \tag{46}\]

**Proof.** Let \(r=1\). That (36) implies (35) is virtually the same as (b)-(f) in the proof of theorem 1.2, but now with \(Z_{nk}^{\theta}\equiv|K(\xi_{nk})|1_{[\theta/n<\xi_{nk}<1-\theta/n]}\). Trivially, (35) implies (23). From theorem 1.1 we know that (23) is equivalent to (10). However, (10) = (30) = (39), as they are identical in content. Now, (39) = (30) is equivalent to (26) (by theorem 1.2) and which is equivalent to (36) and to (37) (by theorem 2.2). Having the condition (10) = (30) = (39) on this list allowed us to add all previous equivalent conditions given in (23)-(31) in theorem 1.2, and this is what all four of the conditions given in the list (42) = (24)(c), (43) = (6), (44) = (7), and (45) = (25) were chosen to exemplify.

Consider (46). With the \(Z_{nk}^{\theta}\) of the previous paragraph, we now consider

(a) \[Y_{nk}^{\pm}\equiv m^{\pm}(\xi_{nk})\,\quad m_{\theta n}^{+}\equiv{\rm E}(Y_{ nk}^{+}1_{[\xi_{nk}<1-\theta/n]}),\quad m_{\theta n}^{-}\equiv{\rm E}(Y_{nk}^{-}1_ {[\theta/n<\xi_{nk}]}).\]

Using the truncation inequality (8.4.15) yet again (with \(m_{\theta n}\equiv{\rm E}(Z_{nk}^{\theta})\)) gives

(b) \[P(|Y_{n}^{\pm}-m_{\theta n}^{\pm}|\geq\epsilon\,m_{\theta n})\leq{\rm E}((Z_{n 1}^{\theta})^{2}/n(\epsilon\,m_{\theta n})^{2}+2\theta.\]

Now proceed to establish (46), as in (a)-(f) of the proof of theorem 1.2. Finally, combining (35) with (42) gives (46). \(\Box\)

**Theorem 1.4** (Equivalencies for consistent estimation of the variance \(\tilde{\sigma}_{n}^{2}\)): Let \(Y\) and \(Y_{n1},\ldots,Y_{nn}\) be iid with df \(F(\cdot)\) and qf \(K(\cdot)\) Let \(F_{|Y|}(\cdot)\) and \(K_{|Y|}(\cdot)\) denote the df and qf of \(|Y|\). Let \({\rm dom}(a,\ a)\) denote one of \((a,\ 1],\ [0,1-a)\), or \((a,\ 1-a)\) according as \(Y\leq 0,Y\geq 0\), or \(Y\) is general. Define \(\bar{v}(t)\equiv[K_{+}^{-}(t)]^{2}+[K^{+}(1-t)]^{2}\) and \(v(t)\equiv K_{|Y|}^{2}(t)\). The following are just some of the possible equivalent conditions:

\[x^{2}P(|Y|>x)/U(x)\to 0,\] where \[U(x)\equiv\int_{[|y|\leq x]}y^{2}dF(y). \tag{48}\] \[tK_{|Y|}^{2}(1-t)/V_{|Y|}(t)\to 0,\] where \[V_{|Y|}(t)\equiv\int_{[0,1-t)}K_{|Y|}^{2}(s)\,ds.\] (49) \[t[K_{+}^{2}(t)\lor K^{2}(1-t)]/V(t)\to 0,\] where \[V(t)\equiv\int_{{\rm dom}(t,t)}K^{2}(s)\,ds.\] (50) \[a_{n}\bar{v}(a_{n})/V(a_{n})\to 0\] for any one \[a_{n}\searrow 0\] with \[\overline{\lim}(a_{n}/a_{n+1})<\infty\]. (51) \[\overline{Y_{n}^{2}}/u_{1n}\to_{p}1,\] where \[u_{1n}\equiv U(x_{1n})\]. (52) \[\overline{Y_{n}^{2}}/\tilde{u}_{1n}\to_{p}1,\] where \[\tilde{u}_{1n}\equiv U(x_{1n})+x_{1n}^{2}P(|Y|>x_{1n}).\] (53) \[\overline{Y_{n}^{2}}/v_{1n}\to_{p}1,\] where \[v_{1n}\equiv\int_{[0,1-1/n)}K_{|Y|}^{2}(s)\,ds=V_{|Y|}(1/n).\] (54) \[\overline{Y_{n}^{2}}/\bar{v}_{1n}\to_{p}1,\] where \[\tilde{v}_{1n}\equiv V(1/n)=\int_{{\rm dom}(1/n,1/n)}K^{2}(s)\,ds. \tag{47}\](55) \(\overline{Y_{n}^{2}}/\nu_{n}\to_{p}1\) for at least one sequence \(\nu_{n}>0\). (56) \([\max_{k}\frac{1}{n}Y_{nk}^{2}]/\nu_{n}\to_{p}0\) and \(\check{\mu}_{n}/\nu_{n}\to 1\), where \(\check{\mu}_{n}\equiv U(nu_{1n})\). (57) \([\max_{k}\frac{1}{n}Y_{nk}^{2}]/\nu_{n}\to_{p}0\) where \(\nu_{n}\) is any one of \(v_{1n},\bar{v}_{1n},\check{u}_{1n}\) or \(u_{1n}\). (58) \([\max_{k}\frac{1}{n}Y_{nk}^{2}]/\bar{Y_{n}^{2}}\to_{p}0\). (59) \(\mathcal{D}_{n}^{2}\equiv[\max_{k}\frac{1}{n}|Y_{nk}-\bar{Y}_{n}|^{2}]/S_{n}^ {2}\to_{p}0\), where \(S_{n}^{2}\equiv\bar{Y_{n}^{2}}-(\bar{Y}_{n})^{2}\). (60) \(S_{n}^{2}/\tilde{\sigma}^{2}(\frac{1}{n})\to_{p}1\) where \(\tilde{\sigma}^{2}(\frac{1}{n})\) is the variance of \(K_{Y}(\xi)\) Winsorized outside \(\mathrm{dom}(\frac{1}{n},\ \frac{1}{n})\). (61) \(U(x)=\int_{[|y|\leq x]}y^{2}dF(y)\quad\mbox{is slowly varying at $\infty$}\). (See (C.2.13)). Now, (51)-(54) imply \(u_{1n}/v_{1n}\to 1,\tilde{u}_{1n}/v_{1n}=\tilde{v}_{1n}/v_{1n}\to 1\), and \(\bar{v}_{1n}/v_{1n}\to 1\). (If (50) holds for one such sequence \(a_{n}\), then it holds for all such sequences \(a_{n}\).) [Theorems C.2.1 and C.2.2 present many more equivalencies similar to (47)-(50) and (61), all of which follow from using just geometrical considerations and Cauchy-Schwarz.] (See theorems 1.1-1.3 for additional equivalencies.)

**Proof.** The equivalence of (47) through (59) (and of many other conditions from theorems 1.1-1.3, (with \(r=2\) for iid rvs), like (3)-(10), (23)-(32), and (35)-(45)) has already been established. We were able combine the lists of theorems 1.2-1.3 since (58) appeared on all of these lists. We added (59) via propositions 6.6.1 and 6.6.2. We added (61) via theorems C.2.1 and C.2.2. \(\square\)

**Remark 1.1** (Alternatively Winsorized variances) Let \(\tilde{\sigma}^{2}(x)\) denote the variance of \(Y\) Winsorized outside \([-x,\ x]\). Let \(\tilde{\sigma}^{2}(t)\) denote the variance of \(K_{Y}(\xi)\) Winsorized outside \(\mathrm{dom}(t,\ t)\). Note from theorem 1.4 that \(\tilde{u}_{1n}=\tilde{v}_{1n}\). Note also that

(62) \(\tilde{\sigma}^{2}(x_{1n})/\tilde{\sigma}^{2}(1/n)\to 1\)

when either \(\mathrm{Var}[Y]<\infty\) or when \(\mathrm{Var}[Y]=\infty\). \(\square\)

**Exercise 1.3** : Prove (27) implies (26) for \(r=1\) (based on the natural \((r=2)\) modification of the proof that (C. 2.6)(B) implies (C. 2.6)(A)--which is written out in the next section).

**Exercise 1.4** : (a) See (51) and (52) for \(u_{1n}\) and \(\tilde{u}_{1n}\), (53) for \(v_{1n}\), and (54) for \(\bar{v}_{1n}\) when \(r=2\). Show that both

(63) \(u_{1n}\geq v_{1n}\qquad\mbox{and}\qquad\tilde{v}_{1n}=\tilde{u}_{1n}\). (b) State the analogous results for \(u_{1n},m_{1n},\tilde{u}_{1n},\tilde{m}_{1n}\), and \(\tilde{\tilde{m}}_{1n}\) when \(r=1\). [Hint. Note figure 8.4.1 and exercise 6.3.3.]

**Theorem 1.5** (Strong negligibility) Let \(Y,Y_{1},Y_{2},\ldots\) be iid rvs (that are not identically equal to 0). Then

(64) \(\begin{array}{l}\mathcal{D}_{n}\to_{a.s.}0\quad\mbox{iff}\quad\mathrm{E}Y^{2 }<\infty\\ \mbox{iff}\quad M_{2n}\to_{a.s.}0\quad\mbox{iff}\quad\mathrm{E}M_{2n}\to 0\quad \mbox{iff}\quad M_{2n}/\bar{Y_{n}^{2}}\to_{a.s.}0.\end{array}\)

for \(M_{2n}\equiv[\frac{1}{n}\max_{1\leq k\leq n}Y_{k}^{2}]\). (Recall exercise 8.4.20.)

## 2 Slowly Varying Partial Variance When \(\sigma^{2}=\infty\,^{o}\)

Many facts about slowly varying functions can be learned from simple pictures. We concentrate here on just such facts. Levy's condition (14) has emerged as the necessary and sufficient condition of choice for the CLT; and we will now derive many equivalent ways to demonstrate it. (Necessary and sufficient conditions for consistent estimation of the variance parameters \(V(1/n)\) and \(\tilde{\sigma}^{2}(1/n)\) as defined below are also equivalent to the conditions on the current list.) (Note (21) below.)

**Notation 2.1** Let \(Y\) denote an arbitrary rv (with df \(F\) and qf \(K\)); let \(X\equiv|Y|\). Let \(F_{X}\) and \(K_{X}\) denote the df and qf of \(X\). For \(0<t<1\), let \(x_{t}\equiv K_{X}(1-t)\). Let \(\tilde{Y}_{t}\) denote \(Y\) Winsorized) outside \([-x_{t},x_{t}]\). Define various _partial moments_, via

\[v(t)\equiv K_{X}^{2}(1-t), V(t)\equiv\int_{(t,1]}v(s)\,ds, \tilde{\sigma}^{2}(t)\equiv\mbox{Var}[\tilde{Y}_{t}],\] (1) \[q(t)\equiv K_{X}(1-t), m(t)\equiv\int_{(t,1]}q(s)\,ds, \tilde{\mu}(t)\equiv\mbox{E}\tilde{Y}_{t},\] (2) \[U(x)\equiv\int_{[0,x]}y^{2}dF_{X}(y), M(x)\equiv\int_{[0,x]}y\,dF_{X}(y)\, \tilde{V}(t)\equiv\mbox{E}\tilde{Y}_{t}^{2}.\] \(\Box\)

**Theorem 2.1** (Partial variance, with symmetric \(\tilde{\bf W}\)**W**sinorizing): Only the case \(\sigma^{2}=\infty\) has interest; all conclusions below are trivial if \(\sigma^{2}<\infty\).

(A): The following (also referred to as (4)(A)-(12)(A)) are equivalent (as \(t\to 0\)) :

\[[\tilde{\sigma}^{2}(ct)-\tilde{\sigma}^{2}(t)]/\tilde{\sigma}^{2} (t)\to 0 \mbox{for all }0<c<1. \tag{3}\] \[[\tilde{V}(ct)-\tilde{V}(t)]/\tilde{V}(t)\to 0 \mbox{for all }0<c<1.\] (4) \[r(t)\equiv tv(t)/V(t)\to 0.\] (5) \[[V(ct)-V(t)]/V(t)\to 0 \mbox{for all }0<c<1.\] (6) \[t[v(ct)-v(t)]/V(t)\to 0 \mbox{for all }0<c<1.\] (7) \[t[v(ct)-v(t)]/V(t)\to 0 \mbox{for all }0<c<1.\] (8) \[d(t)\equiv\sqrt{t}[q(ct)-q(t)]/\sqrt{V(t)}\to 0 \mbox{for all }0<c<1.\] (9) \[tv(ct)/V(t)\to 0 \mbox{for all }0<c<1.\] (10) \[[m(ct)-m(t)]/\sqrt{tV(t)}\to 0 \mbox{for all }0<c<1.\] (11) \[\tilde{r}(t)\equiv tv(t)/\tilde{\sigma}^{2}(t)\to 0. \tag{12}\]

(B): Specify a sequence \(a_{n}\searrow 0\) as \(n\to\infty\) that satisfies \(\limsup a_{n}/a_{n+1}<\infty\). Conditions (4)(B)-(12)(B) are obtained by replacing \(t\) by \(a_{n}\) in (4)(A)-(12)(A). These conditions (4)(B)-(12)(B) are also equivalent to condition (4)(A).

(C): Conditions (5)(C)-(11)(C) are obtained by replacing \(V(a_{n})\) or \(\tilde{V}(a_{n})\) by \(\tilde{\sigma}^{2}(a_{n})\) in the denominators of (5)(B)-(11)(B). Then (5)(C)-(11)(C) are also equivalent to the condition (4)(A). (Also, (21) below is an equivalent; (20) and (22) are useful.)

(D): The most useful choices are \(a_{n}\equiv\epsilon/n\)(equivalently, \(a_{n}\equiv 1/n\) with \(c\equiv\epsilon\)), or the alternative \(a_{n}\equiv\epsilon_{n}/n\) with \(\epsilon_{n}\searrow 0\) subject to \(\limsup\epsilon_{n}/\epsilon_{n+1}<\infty\).

(E): The following are equivalent (as \(x\to\infty\)) to the previous conditions.

[Any sequence \(x_{n}\) to be specified below is assumed to satisfy \(\overline{\lim}(x_{n+1}/x_{n})<\infty\)).]

\[U\in{\cal U}_{0} \mbox{(that is, }U\mbox{ is slowly varying at }\infty\mbox{).} \tag{13}\] \[R(x)\equiv x^{2}P(X>x)/U(x)\to 0 \mbox{(equivalently, }R(x_{n})\to 0\mbox{).}\] (14) \[x[M(cx)-M(x)]/U(x)\to 0 \mbox{for any one (or, all) fixed }c>1.\] (15) \[\tilde{U}(x)\equiv U(x)+x^{2}P(X>x) \mbox{defines a function in }{\cal U}_{0}.\]

**Theorem 2.2** (Partial variance, equal fractions): Consider an arbitrary rv \(Y\) with df \(F\) and qf \(K\). Let \(\tilde{K}_{t,t}(\cdot)\) denote \(K(\cdot)\) Winsorized outside \(\mbox{dom}(t,\ t)\) (recall notation 6.5.1), and now redefine so that

\[v(t) \equiv\{[K_{+}^{-}(t)]^{2}+[K^{+}(1-t)]^{2}\}\] and \[V(t) \equiv\int_{(t,1]}v(s)\,ds, \tag{18}\] \[q(t) \equiv[K_{+}^{-}(t)+K^{+}(1-t)]\] and \[m(t) \equiv\int_{(t,1]}q(s)\,ds,\] (19) \[\tilde{\mu}(t) \equiv\mbox{E}[\tilde{K}_{t,t}(\xi)]\] and \[\tilde{\sigma}^{2}(t) \equiv\mbox{Var}[\tilde{K}_{t,t}(\xi)]. \tag{17}\]

All equivalences stated in parts (A), (B), and (C) of the previous theorem are still valid for the new definitions of \(q,v,\tilde{\sigma}^{2}\), and \(\tilde{V}(t)\equiv\mbox{E}[\tilde{K}_{t,t}^{2}(\xi)]\). (In the previous theorem we had \(q^{2}(t)=v(t)\). In the present theorem we have \(v(t)\leq q^{2}(t)\leq 2v(t)\), which is still convenient and meets all our needs. What is crucial is that \(m(\cdot)\), \(q(\cdot)\), \(v(\cdot)\) and \(V(\cdot)\) have an appropriate meaning. Note that this \(V(t)\) is the Truncated second moment (with respect to \(\mbox{dom}(t,t)\)).) (Also, (21) below is equivalent.)

**Note (to Winsorize the absolute value, or to Winsorize symmetrically)**

Theorem 2.1 Winsorizes symmetrically about zero, while the companion theorem 2.2 Winsorizes equal fractions from each tail. Statisticians use both strategies in the modification of their data, and these theorems are prepared to deal with both.

**Remark 2.1** (Why develop all these equivalent conditions?): Quite a number of the useful conditions that appear in the literature are developed herein. These have application in estimation of moments and partial moments and in the determination of necessary and sufficient conditions for asymptotic normality of iid rvs. Some of these conditions are best developed in the context of a careful examination of the df and the qf of the underlying distribution. These are considered in this section and the next. Some are best developed in the context of the _ratio_\(LLN\), and this was just done in the previous section in theorem C.1.4 for the estimation of a partial second moment and a partial variance when the underlying variance may be infinite. In sections 10.5-10.6 all of these results are connected to necessary and conditions for asymptotic normality of the mean of iid rvs. In sections 10.8-10.9 they are tied into the bootstrap via the quantity \({\cal D}_{n}\) appearing in (C.1.60). (Once started, it was just fun to see how completely it could be done.) \(\Box\)

**Proof.** Clearly, all of (4)(A)-(12)(C) (in the context of both theorem 2.1 and theorem 2.2), hold when \(\sigma^{2}<\infty\). So from here on we always assume that \(\sigma^{2}=\infty\).

(A): Now, (5) implies (6), since for each \(d>1\) figure 2.1(a) demonstrates that

(a) \[\frac{tv(t)}{V(t)}\leq\frac{tv(dt)+t[v(t)-v(dt)]}{V(dt)+(d-1)tv(dt)}\frac{d}{d}\] (b) \[\leq\frac{1}{d-1}\left\{\frac{dtv(dt)}{\tilde{V}(dt)}+\frac{dt[v(t)-v( dt)]}{1\cdot\tilde{V}(dt)}\right\}\leq\frac{1}{d-1}\left\{1+d\frac{t(v(t)-v(dt))}{ \tilde{V}(dt)}\right\}\] (c) \[\leq\frac{1}{d-1}+\frac{d}{d-1}\frac{\tilde{V}(\frac{1}{d}\,dt)-\tilde{V}( dt)}{\tilde{V}(dt)}\] (d) \[\to 1/(d-1)+0\qquad\mbox{by (\ref{eq:v1}).}\]

Since this holds for any \(d>1\), it gives (6).

Next, (6) implies (7), since for each fixed \(0<c<1\) figure 2.1(b) shows that

(e) \[[V(ct)-V(t)]/V(ct)\leq[(1-c)/c][ctv(ct)/V(ct)]\to 0.\]

Suppose (7), that \(V\in\mathcal{R}_{0}\). This means that \([V(ct/2)-V(t)]/V(t)\to 0\), and figure 2.1(c) then demonstrates that

(f) \[(c/2)t[v(ct)-v(t)]/V(t)\leq[V(ct/2)-V(t)]/V(t)\to 0;\]

this shows that (8) holds.

Supposing (8) about \(v(\cdot)\), we will establish (9) about \(d(\cdot).\) Now,

(g) \[t[v(ct)-v(t)]/V(t)=d(t)\times[d(t)+2\sqrt{t}q(t)/\sqrt{V(t)}]\geq d^{2}(t);\]

and so \(d(t)\to 0\) is implied, as required for (9).

Then (9) implies (5), since figure 2.1(b) also shows that

\[\frac{\tilde{V}(ct)-\tilde{V}(t)}{\tilde{V}(t)}\leq\frac{t[v(ct)-v(t)]}{V(t)+ tv(t)}\]

\[=\frac{\sqrt{t}[q(ct)-q(t)]}{\sqrt{\tilde{V}(t)}}\times\left\{\frac{\sqrt{t}[q( ct)-q(t)]}{\sqrt{\tilde{V}(t)}}+\frac{2\sqrt{t}q(t)}{\sqrt{\tilde{V}(t)}}\right\}\]

(h) \[\leq\frac{\sqrt{t}[q(ct)-q(t)]}{\sqrt{V(t)}}\times\left\{\frac{\sqrt{t}[q(ct)-q (t)]}{\sqrt{V(t)}}+\frac{2\sqrt{t}q(t)}{\sqrt{tv(t)}}\right\}\]

\[=d(t)\{d(t)+2\}\to 0.\]

We have closed the (5)-(9) circle. It is now trivial that (6) and (8) imply (10), while (10) even more trivially implies (6).

We obtain (11) implies (6) via

(i) \[(1-c)\left\{\frac{tv(t)}{V(t)}\right\}^{1/2}=(1-c)\frac{tq(t)}{\sqrt{tV(t)}} \leq\frac{\int_{ct}^{t}q(s)\,ds}{\sqrt{tV(t)}}=\frac{m(ct)-m(t)}{\sqrt{tV(t)}} \to 0.\]

Then (7) implies (11) via Cauchy-Schwarz in

\[\frac{m(ct)-m(t)}{\sqrt{tV(t)}}=\frac{\int_{ct}^{t}q(s)\,ds}{\sqrt{tV(t)}}\leq \left\{\frac{(1-c)t\!\!\int_{ct}^{t}\!\!q^{2}(s)\,ds}{tV(t)}\right\}^{1/2}\]

Figure 7.1: Comparison of areas.

\[\leq\left\{\frac{V(ct)-V(t)}{V(t)}\right\}^{1/2}\to 0.\]

So far, (5)-(11) are equivalent.

We next show that (4) is equivalent to the simpler (5). Suppose (4) holds, so that \(\tilde{\sigma}^{2}(\cdot)\in{\cal R}_{0}\). We use Gnedenko and Kolmogorov's theorem 6.6.1 to write

\[\frac{\tilde{V}(ct)-\tilde{V}(t)}{\tilde{V}(t)}=\left[\frac{\tilde{\sigma}^{2}( ct)-\tilde{\sigma}^{2}(t)}{\tilde{\sigma}^{2}(ct)}+\frac{\tilde{\mu}^{2}(ct)- \tilde{\mu}^{2}(t)}{\tilde{\sigma}^{2}(ct)}\right]\times\frac{\tilde{\sigma}^ {2}(ct)}{\tilde{\sigma}^{2}(t)}\times\frac{\tilde{\sigma}^{2}(t)}{\tilde{V}(t)}\]

(k) \[=\left[\frac{\tilde{\sigma}^{2}(ct)-\tilde{\sigma}^{2}(t)}{\tilde{\sigma}^{2}( ct)}+o(1)+o(1)\right]\times\frac{\tilde{\sigma}^{2}(ct)}{\tilde{\sigma}^{2}(t)} \times[1+o(1)]=o(1);\]

and this implies \(\tilde{V}(\cdot)\in{\cal R}_{0}\), which is (5). Now suppose that (5) holds, so that \(\tilde{V}(\cdot)\in{\cal R}_{0}\). The same Gnedenko-Kolmogorov theorem now gives

\[\frac{\tilde{\sigma}^{2}(ct)-\tilde{\sigma}^{2}(t)}{\tilde{\sigma}^{2}(t)}= \frac{\tilde{V}(ct)-\tilde{V}(t)}{\tilde{V}(t)[1+o(1)]}+\left[\frac{\tilde{\mu }^{2}(t)-\tilde{\mu}^{2}(ct)}{\tilde{V}(ct)}\right]\left[\frac{\tilde{V}(ct)}{ \tilde{V}(t)[1+o(1)]}\right]\]

(l) \[=\frac{\tilde{V}(ct)-\tilde{V}(t)}{\tilde{V}(t)}[1+o(1)]+[o(1)+o(1)][1+o(1)]=o(1);\]

and this implies \(\tilde{\sigma}^{2}(\cdot)\in{\cal R}_{0}\), which is (4).

That (6) and (12) are equivalent is immediate from (6.6.2), which gives

\[\tilde{r}(t)=\frac{tv(t)}{\tilde{\sigma}^{2}(t)}=\frac{tv(t)}{\tilde{V}(t)- \tilde{\mu}^{2}(t)}\sim\frac{tv(t)}{\tilde{V}(t)}=\frac{r(t)}{1+r(t)}. \tag{20}\]

(B): We next show that (6)(B) implies (6)(A). Suppose that (6)(B) holds for even one sequence \(a_{n}\searrow 0\) having \(\overline{\lim}\,a_{n}/a_{n+1}<\infty\). We are thus given that \(r_{n}\equiv a_{n}v(a_{n})/V(a_{n})\to 0\). So (6)(A) holds via

(m) \[\sup_{a_{n+1}\leq t\leq a_{n}}\frac{tv(t)}{V(t)}\leq\frac{a_{n}}{a_{n+1}}\frac{ a_{n+1}v(a_{n+1})}{V(a_{n+1})}\frac{V(a_{n+1})}{V(a_{n})}=\frac{a_{n}}{a_{n+1}}r_ {n+1}\frac{V(a_{n+1})}{V(a_{n})}\]

\[\leq\frac{a_{n}}{a_{n+1}}r_{n+1}/\left[1-\left(\frac{a_{n}}{a_{n+1}}-1\right) r_{n+1}\right]\to 0,\]

since

(n) \[1\geq\frac{V(a_{n})}{V(a_{n+1})}=1-\frac{\int_{(a_{n+1},a_{n}]}v(t)\,dt}{V(a_{n +1})}\geq 1-\frac{a_{n+1}v(a_{n+1})}{V(a_{n+1})}\left[\frac{a_{n}}{a_{n+1}}-1 \right]\to 1.\]

Since(6)(A) trivially implies (6)(B), the condition (6)(B) is now on the list.

That (5)(B) implies (5)(A) follows from

(o) \[1\leq\sup_{a_{n+1}\leq t\leq a_{n}}\frac{\tilde{V}(ct)}{\tilde{V}(t)}\leq\frac {\tilde{V}(c\,a_{n+1})}{\tilde{V}(a_{n})}\leq\frac{\tilde{V}((c/M)a_{n})}{ \tilde{V}(a_{n})}\to 1\times 1=1,\]

since \(\overline{\lim}(a_{n}/a_{n+1})<(\)some \(M)<\infty\). The trivial converse puts (5)(A) on the list.

We next show that (6)(B) implies (7)(B). Now, (6)(B) (since it implies (6)(A)) allows use of (e) with \(t=a_{n}\) to conclude (7)(B). The arguments of (f)-(h) can be repeated with to show that (7)(B) implies (8)(B), which implies (9)(B), which implies (5)(B). Now, (5)(B) was shown in (o) to imply (5)(A), and thus it also implies (6)(A), which trivially implies (6)(B). Also, (10)(B) can be added by the same argument used for (10)(A). The arguments of (i) and (j) with \(t=a_{n}\) allow us to add (11)(B). Finally, the equivalence of (4)(B) and (5)(B) still follows from arguments (k) and (1) with \(t=a_{n}\).

Next observe that (12)(B) is equivalent to (6)(B), since (m) gives

\[\tilde{r}(a_{n})=\frac{a_{n}v(a_{n})}{\tilde{\sigma}^{2}(a_{n})}\sim\frac{a_{n} v(a_{n})}{V(a_{n})+a_{n}v(a_{n})}=\frac{r(a_{n})}{1+r(a_{n})}.\]

Thus (12)(B) is on our list.

(C): Note that (6)(C) is exactly (12)(B), and so (6)(C) is on our list, and thus it implies (12)(A). We may reread (e) (with denominator \(V(ca_{n})\) replaced by \(\tilde{\sigma}^{2}(a_{n})\) throughout) to see that (6)(C) (that is, its (12)(A) consequences) implies (7)(C). Then rereading (f)-(h) (with the same denominator replacement) shows that (7)(C) implies (8)(C), which implies (9)(C), which implies (5)(C). We now close the circle on (5)(C)-(9)(C) by noting that (5)(C) implies (5)(B), again using the Gnedenko and Kolmogorov result (6.6.2). We can add (10)(C) by the same trivial argument as before. Rereading (i)-(j) (with the new denominator) then allows us to add (11)(C).

The proof of theorem 2.2 is essentially identical. All but lines (h) and (j) are identical; line (i) is identical because \(m\) is still the integral of \(q\). But (h) and (j) are not identical because we no longer have \(q^{2}=v\). But we do have \(v^{2}=[q^{+}]^{2}+[q^{-}]^{2}\), where \(q=q^{+}+q^{-}\), and that is enough to complete the proof. Just factor the two pieces separately in (h) and (j), and apply the trivial inequalities \((a+b)^{2}\leq 2(a^{2}+b^{2})\) and \(a\lor b\leq a+b\leq 2(a\lor b)\). \(\Box\)

**Proof.** (E): We prove this part separately. We first show that (14) implies (13). So suppose that (14) holds, so that \(R(x)\equiv x^{2}P(X>x)/U(x)\to 0\) as \(x\to\infty\). If \(c<1\), then

\[[U(x)-U(cx)]/U(cx)=\int_{(cx,x]}y^{2}dF_{X}(y)/U(cx)\]

(a) \[\leq c^{-2}[(cx)^{2}P(X>cx)/U(cx)]\to 0;\]

and for \(c>1\) it is analogous that \([U(cx)-U(x)]/U(x)\to 0\). Thus, \(U\) is slowly varying, as in (13).

Suppose (13) holds. Then for all \(x\geq\) (some \(x_{\epsilon}\)),

\[\frac{x^{2}P(x<X\leq 2x)}{U(x)}\leq\frac{\int_{x}^{2x}y^{2}dF_{X}(y)}{U(x)} \leq\frac{U(2x)-U(x)}{U(x)}<\epsilon,\]

(b) \[\frac{x^{2}P(2x<X\leq 4x)}{U(x)}\leq\frac{(2x)^{2}}{4}\frac{P(2x<X\leq 2\cdot 2x) }{U(2x)}\cdot\frac{U(2\cdot x)}{U(x)}\leq\frac{\epsilon\cdot(1+\epsilon)}{4},\]

(d) \[\frac{x^{2}P(4x<X\leq 8x)}{U(x)}<\frac{\epsilon}{4^{2}}\frac{U(4x)}{U(2x)} \frac{U(2x)}{U(x)}\leq\epsilon[(1+\epsilon)/4]^{2},\ldots\]

So for \(x\geq x_{\epsilon}\) we add these to get

\[R(x)=x^{2}P(X>x)/U(x)\leq\epsilon{\sum_{k=0}^{\infty}}[(1+\epsilon)/4]^{k} \leq\epsilon/(1-(1+\epsilon)/4)\leq 2\epsilon.\]

Thus \(R(x)\to 0\) as \(x\to\infty\). That is, (13) implies (14).

We will obtain (15) implies (14) analogously to \((\mathrm{b})-(\mathrm{d})\). (To keep notation nearly identical to \((\mathrm{b})-(\mathrm{d})\), we shall pretend that \(c=2\); but really, \(2\), \(4\), \(8\), \(\cdots\), are just surrogates for \(c,c^{2},c^{3},\cdots\).) Now, for all \(x\geq(some\;x_{\epsilon})\), (15) gives

\[\frac{x^{2}P(x<X\leq 2x)}{U(x)}\leq\frac{x[M(2x)-M(x)]}{U(x)}<\epsilon,\]

\[\frac{x^{2}P(2x<X\leq 4x)}{U(x)}\leq\frac{1}{2^{2}}\frac{2x[M(2\cdot 2x)-M(2x)] }{U(2x)}.\;(1+\frac{U(2x)-U(x)}{U(x)})\]

\[\leq\frac{1}{4}\epsilon(1+\frac{2x[M(2x)-M(x)]}{U(x)})\leq\epsilon\frac{1+ \epsilon}{4},\]

\[\frac{x^{2}P(4x<X\leq 8x)}{U(x)}\leq\frac{1}{4^{2}}\frac{4x[M(2\cdot 4x)-M(4x)] }{U(4x)}.\;\frac{U(2\cdot 2x)}{U(2x)}\cdot\frac{U(2x)}{U(x)}\]

\[\leq\frac{1}{4^{2}}\epsilon(1+\epsilon)^{2}\leq\epsilon(\frac{1+\epsilon}{4})^{ 2}\]

Add these to get \(R(x)\to 0\) as \(x\to\infty\), as in (e). Thus (15) implies (14).

We obtain (13) implies (15) via

\[\frac{x[M(cx)-M(x)]}{U(x)}\leq\frac{\int_{x}^{cx}y^{2}dF_{X}(y)}{U(x)}\leq \frac{[U(cx)-U(x)]}{U(x)}\to 0.\]

Now, (14) implies (16) via

\[\frac{\tilde{U}(cx)-\tilde{U}(x)}{\tilde{U}(x)}\leq\frac{U(cx)-U(x)}{U(x)}+ \frac{(c^{2}+1)x^{2}P(X>x)}{U(x)}\to 0.\]

Then (16) implies (14), since for \(c>1\) we have

\[\frac{\tilde{U}(cx)-\tilde{U}(x)}{\tilde{U}(x)}=\frac{[\int_{x}^{cx}y^{2}dF_{X} (y)+(cx)^{2}P(X>cx)-x^{2}P(X>x)]}{\tilde{U}(x)}\]

\[\geq\{[x^{2}[P(X>x)-P(X>cx)]+c^{2}x^{2}P(X>cx)-x^{2}P(X>x)\}/\tilde{U}(x)\]

\[\geq\frac{(c^{2}-1)x^{2}P(X>cx)}{\tilde{U}(cx)}\geq\frac{c^{2}-1}{c^{2}}\Bigg{/} \{1+\frac{U(cx)}{(cx)^{2}P(X>cx)}\},\]

and the extreme left term going to \(0\) forces \(R(cx)=(cx)^{2}P(X>cx)/U(cx)\to 0\). Thus (13)-(16) are equivalent. In fact, the second condition in (14) suffices, since

\[\sup_{x_{n}\leq x\leq x_{n+1}}\frac{x^{2}P(X>x)}{U(x)}\leq\left[\overline{ \lim}\,\frac{x_{n+1}^{2}}{x_{n}^{2}}\right]\frac{x_{n}^{2}P(X>x_{n})}{U(x_{n}) }\leq O(1)\times R(x_{n})\to 0.\]

The equivalence of (14) and (6)(A) is shown in the next section. Conditions (13)-(16) will then be added to the big list with the rest in theorems 2.1-2.2. 

Though already established, we will still give a simple proof that \(V(\cdot)\in\mathcal{L}\) does indeed imply \(\tilde{V}(\cdot)\in\mathcal{L}\). If \(V\in\mathcal{L}\), figure 2.1 (b) shows that

\[0\leq(1-c)\ a\ v(a)/V(a)\leq[V(ca)-V(a)]/V(a)\to 0,\]

[MISSING_PAGE_FAIL:487]

## 3 Specific Tail Relationships \({}^{o}\)

We list two relationships that could prove important. The first concerns the WLLN and the second concerns the CLT and variance estimation. They compare the height of the qf with the magnitude of a partial moment.

**Definition 3.1** (Order-\(r\) qfs): A qf \(K\) is of _order_\(r\) (with \(r>0\)) if

\[t\{|K_{+}(t)|^{r}+|K(1-t)|^{r}\}\to 0\qquad\mbox{as $t\to 0$}. \tag{1}\]

**Theorem 3.1** (The \((x,t)\)-order equivalence; WLLN) Let \(X\geq 0\) have qf \(K\). Then

\[\limsup_{t\to 0}t|K(1-t)|^{r}=\limsup_{x\to\infty}x^{r}P(X>x)=\limsup_{x\to \infty}xP(X^{r}>x). \tag{2}\]

for each \(r>0\). The same is true for \(\liminf\), and \(\lim\) (if it exists).

**Proof.** Consider figure 8.4.1. When \(r=1\), the two quantities in (2) correspond to areas; so we will use words appropriate to \(r=1\).

At any point \((x,F(x))\) the quantity \(xP(X>x)\) is just the area above and to the left of this point (in the leftmost half of following figure). At any point \((t,\ K(1-t))\) the quantity \(tK(1-t)\) is just the area below and to the right (in the rightmost half of the following figure). Just trace out these two areas in the figures, and note that all the local extreme points of these two sets of area values (that are associated with jumps and flatspots) are identical. \(\Box\)

**Remark 3.1**: Clearly, if \({\rm E}|X|^{r}<\infty\) for \(X\cong K\), then \(K\) is of order \(r\), since for some finite \(M\)

\[\begin{array}{ll}g(x)\equiv|x|^{r}F(x)[1-F(x)]\leq M\quad\mbox{on $R$},& \mbox{and}\\ \mbox{while}\qquad g(x)\to 0\quad\mbox{as $|x|\to\infty$},&\\ h(t)\equiv t[|K(t)|^{r}+|K(1-t)|^{r}]\leq M\quad\mbox{on $(0,1)$},&\\ \mbox{while}\quad h(t)\to 0\quad\mbox{as $t\to 0$}.&\Box\end{array} \tag{3}\]

**Theorem 3.2** (The \((x,t)\)-tail equivalence; CLT): Let \(X\geq 0\) have df \(F\) and qf \(K\), and fix a constant \(r>0\). Define \(R(\cdot)\) and \(r(\cdot)\) (as in theorem C.1.2) by

\[R(x)\equiv\frac{x^{r}P(X>x)}{\int_{[0,x]}y^{r}dF(y)}\mbox{ on $[0,\ \infty)$} \quad\mbox{and}\quad r(t)\equiv\frac{tK^{r}(1-t)}{\int_{[0,1-t]}K^{r}(s)\,ds} \mbox{ on $(0,1]$}. \tag{5}\]

Then

\[\overline{\lim}_{x\to\infty}\,R(x)=\overline{\lim}_{x\to\infty}\,R(x-)= \overline{\lim}_{t\to 0}\,r(t+)=\overline{\lim}_{t\to 0}\,r(t). \tag{6}\]

The same is true for the \(\underline{\lim}\), and for the \(\lim\) (if it exists).

**Comment** Recall that \(E|X|=\int_{0}^{\infty}\!P(|X|>y)\,dy\) and (see (6.3.12))

\[\begin{array}{ll}(\mbox{\it i})&\int_{(x,\infty)}ydF_{|X|}(y)=xP(|X|>x)+ \int_{x}^{\infty}\!P(|X|>y)\,dy;&\mbox{so}\\ (\mbox{\it ii})&\int_{[0,x]}ydF_{|X|}(y)=\int_{0}^{x}\!P(|X|>y)\,dy-xP(|X|>x). \end{array}\]

**Proof.** Suppose that \([x_{1},\ x_{2})\) is a maximal flat spot of \(F(\cdot).\) Then

(a) \[U(x_{1})=U(x)=U(x_{2}-)\leq U(x_{2})\qquad\text{and}\qquad R(x)\leq R(x_{2}-)\]

for all \(x\in[x_{1},\ x_{2}]\). Thus the supremum of \(R(\cdot)\) across \([x_{1},\ x_{2}]\) is \(R(x_{2}-)\). The three values \(R(x_{1}),R(x_{2}-)\), and \(R(x_{2})\) are numerically equal to the three values \(r(F(x_{1}))\), \(r(F(x_{1})+)\), and \(r(F(x_{2}))\); use (a) and picture the claims in terms of the flat spots in Figure 4.1. Suppose next that \(x\) is a discontinuity point of \(F(\cdot)\) with jump size \(F(x)-F(x-)\). Then \(R(x-)>R(x)\). Moreover the maximum of \(r(t)\) across \([F(x-),\ F(x)]\) is equal to \(r(F(x)+)=R(x-)\). Finally, at all other points there are unique pairs that solve \(R(x)=r(t)\). Thus (5) holds. This establishes the equivalence of the four different conditions that are stated in (25) and (26). \(\Box\)

## 4 Regularly Varying Functions \({}^{*}\)

**Definition 4.1** (Regularly varying functions, at 0): Call \(V(\cdot)>0\)_regularly varying at \(0\)_with_characteristic exponent \(r\)_(written \(V\in{\cal R}_{r}\)) if \(L(t)\equiv t^{-r}V(t)\) satisfies \(L(ct)/L(t)\to 1\) for each \(c>0\) (such a function \(L\) is called _slowly varying_, and we agree to write \(L\in{\cal L}\equiv{\cal R}_{0}\)). [Clearly, shifting a qf up or down has absolutely no effect on whether or not it is varies regularly.]

**Theorem 4.1**: Let \(V(t)\equiv\int_{t}^{1}\!v(s)\,ds\), where \(v\geq 0\) and \(v\searrow\) on some \(0<t<a_{\rm o}\). Consider also \(\tilde{V}(t)\equiv V(t)+tv(t)\). Let \(\beta\geq 0\).

**(Monotone density theorem)**

\[\begin{array}{llll}(1)&\quad V\in{\cal R}_{-\beta}&\quad\mbox{iff}&\quad tv (t)/V(t)\to\beta\mbox{ as }t\to 0.\\ (2)&\quad V\in{\cal R}_{-\beta}&\quad\mbox{implies}&\quad v\in{\cal R}_{-( \beta+1)},\qquad\mbox{when }\beta>0.\end{array}\]

**(Karamata theorem)**

\[\begin{array}{llll}(3)&\quad v\in{\cal R}_{-(\beta+1)}&\quad\mbox{implies}& \quad V\in{\cal R}_{-\beta},\quad\mbox{when}&\quad V(0)=\infty.\\ (4)&\quad V\in{\cal R}_{-\beta}&\quad\mbox{implies}&\quad\tilde{V}(t)\in{\cal R }_{-\beta}\quad\mbox{and}&\quad tv(t)/\tilde{V}(t)\to\beta/(1+\beta).\end{array}\]

**Proof.** Consider the "only if" half of (1). For \(0<b<a<1\) we note that

\[\begin{array}{llll}(a)&\quad(a-b)tv(at)\leq\int_{bt}^{at}\!v(s)\,ds=V(bt)-V( at)\leq(a-b)tv(bt).\end{array}\]

Also,

\[\begin{array}{ll}\frac{V(bt)-V(at)}{t^{-\beta}L(t)}=\frac{V(bt)}{(bt)^{- \beta}L(bt)}\times\frac{b^{-\beta}L(bt)}{L(t)}-\frac{V(at)}{(at)^{-\beta}L(at )}\times\frac{a^{-\beta}L(at)}{L(t)}\end{array}\]

\[\begin{array}{llll}\to(b^{-\beta}-a^{-\beta}).\end{array}\]

The left side of (a) thus gives

\[\begin{array}{llll}(c)&\quad\limsup_{t\searrow 0}\frac{tv(at)}{t^{-\beta}L(t)} \leq-\frac{(b^{-\beta}-a^{-\beta})}{b-a},\mbox{ and is also }\left.\leq-\frac{d}{ db}b^{-\beta}\right|_{b=1}=\beta\end{array}\]

by setting \(a=1\) and letting \(b\nearrow 1\). The right side of (a) analogously gives

\[\begin{array}{llll}(d)&\quad\liminf_{t\searrow 0}\frac{tv(bt)}{t^{-\beta}L(t)} \geq-\frac{(b^{-\beta}-a^{-\beta})}{b-a},\mbox{ and is also }\left.\geq-\frac{d}{ db}b^{-\beta}\right|_{b=1}=\beta\end{array}\]

by setting \(b=1\) and letting \(a\searrow 1\). Combining (c) and (d) gives \(tv(t)/V(t)\to\beta\), so the "only if" half of (1) holds. Then (2) and (4) are immediate.

Consider the converse part of (1). Let \(r(t)\equiv tv(t)/V(t)\), so that the ratio \(r(t)\in[\beta-\epsilon,\ \beta+\epsilon]\) for all \(0<t\leq\) (some \(t_{\epsilon}\)). Now, \(V\) is absolutely continuous on every closed subinterval of \((0,t_{\epsilon}]\) by the fundamental theorem of calculus. And thus \(\log V\) is absolutely continuous on the same closed subintervals (see exercise 4.4.5), and so we may claim that

\[\begin{array}{llll}(\log V)^{\prime}(s)=-r(s)/s&\quad\mbox{a.e.\ on }(0,t_{ \epsilon}].\end{array}\]

Thus for any \(0<t\leq t_{\epsilon}\), we can integrate to get

\[\begin{array}{llll}(\mbox{f)}&\quad\log V(t)-\log V(t_{\epsilon})=\int_{t}^{ t_{\epsilon}}[r(s)/s]ds,\end{array}\]and this gives

\[[(ct)^{\beta}V(ct)]/[t^{\beta}V(t)]=c^{\beta}e^{\int_{ct}^{t}[r(s)/s]ds}=e^{\beta \log c}e^{\int_{ct}^{t}[r(s)/s]ds}\]

(g) \[=e^{\int_{ct}^{t}[(r(s)-\beta)/s]ds}\]

(h) \[=(\mbox{some value between $c^{-\epsilon}$ and $c^{\epsilon}$})\quad\mbox{for all $0<t\leq t_{\epsilon}$}.\]

But \(\epsilon>0\) is arbitrary. Thus \(V\in{\cal R}_{-\beta}\). Moreover, (f) shows that \(V\) may be written in the form

\[V(t)=ct^{-\beta}\exp(\int_{t}^{d}[r(s)-\beta]/s\,ds) \tag{5}\]

for some constants \(c,d\) and for \(r(t)\equiv tv(t)/V(t)\to\beta\) as \(t\to 0\).

Now, (4) is immediate from (1), (2), and \(\tilde{V}(t)=V(t)+tv(t)\).

Consider (3). Fix \(0<r<1\). Let \(L(\cdot)\in{\cal L}\). Then for all \(w\leq(\mbox{some $t_{0}\equiv t_{re}$})\),

(i) \[(1-\epsilon)L(w)\leq L(rw)\leq(1+\epsilon)L(w).\]

(Recall that \(a=b\oplus c\) means that \(|a-b|\leq c\).) We write

\[V(rt)-V(rt_{0})=\int_{rt}^{rt_{0}}v(u)du=r{\int_{t}^{t_{0}}}v(rw)dw \qquad\mbox{with $u\equiv rw$}\] \[=r^{-\beta}{\int_{t}^{t_{0}}}w^{-(\beta+1)}L(rw)dw\] (j) \[=r^{-\beta}[1\oplus\epsilon]{\int_{t}^{t_{0}}}w^{-(\beta+1)}L(w) dw\qquad\mbox{using (i)}\] (k) \[=r^{-\beta}[1\oplus\epsilon]{\int_{t}^{t_{0}}}v(w)dw=r^{-\beta}[1 \oplus\epsilon][V(t)-V(t_{0})].\]

Thus

(l) \[(1-\epsilon)r^{-\beta}\leq\frac{V(rt)-V(rt_{0})}{V(t)-V(t_{0})}\leq(1+\epsilon )r^{-\beta}.\]

Since \(V(t)\to\infty\) as \(t\to 0\), we have from (1) that \(V\in R_{-\beta}\). \(\Box\)

**Definition 4.2** (Regularly varying functions, at \(\infty\)): A function \(U>0\) on \((0,\ \infty)\) is called _regularly varying_ with _exponent_\(\rho\) (written \(U\in{\cal U}_{\rho}\)) when \(l(x)\equiv x^{-\rho}U(x)\) satisfies \(l(cx)/l(x)\to 1\) for each \(c>0\) (such a function \(l(\cdot)\) is called _slowly varying_ at \(\infty\)).

**Theorem 4.2** (Partial variance, \(\beta>0\)): [Let \(\alpha\equiv 2/(\beta+1)\) and \(\beta=(2-\alpha)/\alpha\).]

(i) Based on the definitions in (C.2.1) and (C.2.3), the following are equivalent:

\[U\in{\cal U}_{2\beta/(1+\beta)}={\cal U}_{2-\alpha}. \tag{7}\] \[x^{2}P(X>x)/U(x)\to\beta\qquad\mbox{as $x\ \to\infty$}.\] (8) \[tv(t)/V(t)\to\beta\qquad\qquad\mbox{as $t\to 0$}.\] (9) \[V\in{\cal R}_{-\beta}.\] (10) \[v\in{\cal R}_{-(\beta+1)}. \tag{6}\]

(ii) Based on definition (C.2.17), the conditions (6)-(10) are equivalent.

**Proof.** The equivalence of (8), (9), and (10) follows from theorem 4.1. Then add (7) using theorem 4.1. That (6) is also equivalent is the subject of the next exercise. \(\Box\)

**Exercise 4.1**  Establish the initial equivalence (6) in theorem 4.2. [Hint. Use the integration by parts result \(P(X>x)=\int_{(x,\infty)}x^{-2}dU(x)\).]

**Exercise 4.2** (De Haan)  Let \(v^{-}(t)\equiv[K^{-}_{+}(t)]^{2}\) and \(v^{+}(t)\equiv[K^{+}(1-t)]^{2}\). Let \(V^{-}(t)\equiv\int_{t}^{1}v^{-}(s)\,ds\) and \(V^{+}(t)\equiv\int_{t}^{1}v^{+}(s)\,ds\). If \(v^{-}(t)/v^{+}(t)\to c\in[0,\ \infty]\) as \(t\to 0\), then \(V^{-}(t)/V^{+}(t)\to c\) as \(t\to 0\) when both \(V^{-}(0)\) and \(V^{+}(0)\) are infinite.

The following exercises summarize some standard general regular variation results. [That is, \(h\) is not assumed to be monotone. Karamata's result is now harder.]

**Exercise 4.3** (Regular variation holds uniformly) (a) Let \(h\) denote any function that is regularly varying at 0, of order \(r\in R\). Then for \(0<a<b\leq 1\) we have the uniform convergence of both:

\[\sup_{a\leq c\leq b}|h(ct)/h(t)-c^{r}|\to 0\qquad\mbox{as $t\to 0$}, \tag{12}\] \[\sup_{a\leq u\leq v\leq b}|\int_{u}^{v}h(ct)dc/h(vt)-\int_{u}^{v} c^{r}dc/v^{r}|\to 0\qquad\mbox{as $t\to 0$}. \tag{11}\]

(b) Prove (11) assuming \(h\searrow\).

[Hint. Use (5) for (b). That (12) follows from (11) is easy, even in general.]

**Exercise 4.4** (Karamata)  Let \(h\) be regularly varying at 0, of order \(r\in R\).

(i) Suppose \(r\leq-1\). Then

\[\int_{t}^{1}h(s)\,ds/[th(t)]\to-1/(r+1)\qquad\mbox{as $t\to 0$}. \tag{13}\]

(ii) Suppose \(r>-1\) or both \(r=-1\) and \(\int_{0}^{1/2}h(s)\,ds<\infty\). Then

\[\int_{0}^{t}h(s)\,ds/[th(t)]\to 1/(r+1)\qquad\mbox{as $t\to 0$}. \tag{14}\]

**Question** Theorem 4.1 shows that when \(\beta>0\), the collection of quantile functions \(\{K:V(\cdot)\in{\cal R}_{-\beta}\}\) form a subset of the collection \(\{K:\tilde{V}(\cdot)\in{\cal R}_{-\beta}\}\). Are these two collections actually the same collection?

## 5 Some Winsorized Variance Comparisons \({}^{*}\)

**Notation 5.1**  We agreed in notation 6.5.1 that for \(0\leq a\leq 1-a^{\prime}\leq 1\),

\[\begin{array}{l}\tilde{K}(\cdot)\equiv\tilde{K}_{a,a^{\prime}}(\cdot)\mbox{ denotes }K(\cdot)\mbox{ Winsorized outside dom}(a,a^{\prime}),\\ \tilde{\mu}\equiv\mbox{E}\tilde{K}(\xi),\quad\tilde{\sigma}^{2}\equiv\mbox{Var }[\tilde{K}(\xi)],\quad\tilde{V}\equiv\mbox{E}\tilde{K}^{2}(\xi),\quad\mbox{ with}\quad\xi\cong\mbox{Uniform}(0,1),\\ \tilde{q}\equiv K^{+}(1-a^{\prime})+K_{+}^{-}(a)\quad\mbox{and}\quad V\equiv \int_{\mbox{\scriptsize dom}(a,a)}\!K^{2}(t)\,dt.\end{array}\]

Thus whenever \(K_{+}(a)<0<K(1-a^{\prime})\) we note that \(\tilde{K}(t)=\tilde{K}_{a,a^{\prime}}(t)\) equals \(K_{+}(a)\), \(K(t),K(1-a^{\prime})\) according as \(t\) is in \((0,a],(a,1-a^{\prime}),[1-a^{\prime},1)\), while only the right tail of \(K(\cdot)\) is modified if \(X\geq 0\), and only the left tail of \(K(\cdot)\) is modified if \(X\leq 0\). (Recall also that \(a_{o}\equiv a\wedge(1-a)\) for \(a.\equiv\inf\{t:K(t)\geq 0\}\), from just below (6.5.8).) In any case,

\[\tilde{\sigma}^{2}=\tilde{\sigma}^{2}(a,a^{\prime})\equiv\int_{0}^{1}\!\!\int_{ 0}^{1}\!\!(s\wedge t-st)d\tilde{K}_{a,a^{\prime}}(s)d\tilde{K}_{a,a^{\prime}}(t)\]

for \(0\leq a<1-a^{\prime}\leq 1\), which shows that \(\tilde{\sigma}^{2}(a,\ a^{\prime})\)_always_\(\nearrow\mbox{Var}[K(\xi)]\) as \(a\searrow 0\) and \(a^{\prime}\searrow 0\). Setting \(a=a^{\prime}=0\) gives a valid representation of the variance in \([0,\ \infty]\) without mention of \(\mu\).

Figure 10.1 If \(K\in{\cal K}\), then the graph of \((K-\mu_{K})/\sigma_{\rm K}\) lies entirely in the shaded region.

Fix a qf \(K_{0}\) having mean \(0\) and finite variance \(\sigma_{0}^{2}\), and (for some \(0<a_{0}<\frac{1}{2}\) fixed) define the class of qfs

\[{\cal K}\equiv\{K(\cdot):|K-\mu_{K}|/\sigma_{K}\leq|K_{0}|\mbox{ on }(0,a_{0}]\cup[1-a_{0},1)\}\]

to be all qfs whose standardized form is bounded in the tails by the fixed qf \(K_{0}\). (See the figure above.) Let \(\mu\equiv\mu_{K}\) and \(\sigma\equiv\sigma_{K}\) for each \(K\in{\cal K}\). \(\Box\)

**Inequality 5.1** (Uniform Winsorized variance comparisons): Note that as \((a\lor a^{\prime})\to 0\),

\[\sup_{K\in{\cal K}}|\tilde{\sigma}^{2}-\sigma^{2}|/\tilde{\sigma}^{2}\to 0, \tag{4}\] \[\sup_{K\in{\cal K}}|\tilde{\mu}-\mu|/(\tilde{\sigma}\sqrt{a\lor a ^{\prime}})\to 0\qquad\qquad\mbox{(and $\tilde{\mu}$ may replace $\tilde{\mu}$)},\] and (5) \[\sup_{K\in{\cal K}}\{\sqrt{a}|K_{+}(ca)-\tilde{\mu}|+\sqrt{a^{\prime}}|K(1- c^{\prime}a^{\prime})-\tilde{\mu}|\}/\tilde{\sigma}\to 0 \tag{6}\]

for every fixed \(0<c,c^{\prime}<1\). [Thus \(\sup_{K\in{\cal K}}\sqrt{n}|\tilde{\mu}-\mu|/\tilde{\sigma}_{n}\to 0\) when \(a_{n}=a^{\prime}_{n}=1/n\).]

**Proof.** Let \(K\cong(0,1)\) with \(K\in{\cal K}\). The following bounds are uniform over \(K\in{\cal K}\) and all \(0\leq(a\lor a^{\prime})\leq\) (some \(a_{\epsilon}\)). For the first moment comparison we use

\[|\mu-\tilde{\mu}|/\sigma\leq\int_{0}^{a}\!\!|K_{0}(t)|dt+\int_{1-a}^{1},\ |K_ {0}(t)|dt+a|K_{0+}(a)|+a^{\prime}|K_{0}(1-a^{\prime})| \tag{7}\]

(a) \[\leq\sqrt{a\lor a^{\prime}}\times\epsilon,\]

since

\[\int_{0}^{a}\!\!|K_{0}(t)|dt\leq||\sqrt{t}K_{0}(t)||_{0}^{a}\times\int_{0}^{a}\! \!t^{-1/2}\,dt=o(\sqrt{a}) \tag{8}\]

follows from \(\sqrt{t}K_{0}(t)\to 0\) as \(t\to 0\) whenever \(\sigma^{2}<\infty\). Comparing second moments shows that for all \(0\leq(a\lor a^{\prime})\leq\) (some \(a_{\epsilon}\)) we have

\[|\!\int_{0}^{1}\!\!K^{2}(t)\,dt-\int_{0}^{1}\!\tilde{K}^{2}(t)\, dt|/\sigma^{2} \tag{9}\] \[\leq\left(\int_{0}^{a^{\prime}}+\int_{1-a}^{1}\right),K_{0}^{2}( t)\,dt+aK_{0+}^{2}(a)+a^{\prime}K_{0}^{2}(1-a^{\prime})\leq\epsilon.\]

Finally,

\[\{\sqrt{a}|K_{+}(ca)-\tilde{\mu}|+\sqrt{a^{\prime}}|K(1-c^{\prime} a^{\prime})-\tilde{\mu}|\}/\sigma \tag{10}\] \[\leq 2\sqrt{ca}|K_{0+}(ca)|/\sqrt{c}+2\sqrt{c^{\prime}a^{\prime}}| K_{0}(1-c^{\prime}a^{\prime})|/\sqrt{c^{\prime}}\leq\epsilon.\]

Simple algebra completes the proof. \(\Box\)

**Inequality 5.2** (Basic Winsorized variance inequality): We suppose that \(0\leq a\leq 1-a^{\prime}\leq 1\) (with \(a=0\) allowed only if \(X\geq 0\), and \(a^{\prime}=0\) allowed only if \(X\leq 0\)). Fix \(0<c,c^{\prime}<1\). Let \(\tilde{K}_{c}(\cdot)\equiv\tilde{K}_{ca,c^{\prime}a^{\prime}}(\cdot)\) and let \(\tilde{\mu}_{c},\tilde{\sigma}_{c}^{2},\tilde{V}_{c}\), and \(\tilde{q}_{c}\) denote the same quantities for this new qf. It is immediate from the next figure (using \(|\tilde{\mu}|\leq\tilde{q}\)) that for \(a\lor a^{\prime}\) sufficiently small,

\[0 \leq\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2}=(\tilde{V}_{c}- \tilde{V})-(\tilde{\mu}_{c}^{2}-\tilde{\mu}^{2})\] \[=\{\tilde{V}\oplus(a+a^{\prime})\tilde{q}_{c}^{2}-\tilde{V}\}+\{( \tilde{\mu}\oplus(a+a^{\prime})\tilde{q}_{c})^{2}-\tilde{\mu}^{2}\} \tag{12}\] \[\leq 5(a+a^{\prime})\tilde{q}_{c}^{2}. \tag{11}\]

**Inequalities Aimed at the Infinite-Variance Case**

Suppose the qf \(K(\cdot)\) satisfies \(K_{+}(a)<0<K(1-a^{\prime}).\) Fix \(0<c,c^{\prime}<1\). Define

\[d \equiv\frac{\sqrt{a}}{\tilde{\sigma}}[\tilde{\mu}-K_{+}(a)]\qquad \qquad\mbox{and}\qquad d^{\prime}\equiv\frac{\sqrt{a^{\prime}}}{\tilde{ \sigma}}[K(1-a^{\prime})-\tilde{\mu}], \tag{14}\] \[D \equiv\frac{\sqrt{a}}{\tilde{\sigma}}[K_{+}(a)-K_{+}(ca)|\quad \mbox{and}\qquad D^{\prime}\equiv\frac{\sqrt{a^{\prime}}}{\tilde{\sigma}}[K(1 -c^{\prime}a^{\prime})-K(1-a^{\prime})],\] (15) \[\tilde{D} \equiv\frac{a}{\tilde{\sigma}^{2}}[K_{+}^{2}(ca)-K_{+}^{2}(a)] \quad\mbox{and}\qquad\tilde{D}^{\prime}\equiv\frac{a^{\prime}}{\tilde{ \sigma}^{2}}[K^{2}(1-c^{\prime}a^{\prime})-K^{2}(1-a^{\prime})]. \tag{13}\]

All of these are then \(\geq 0\). (Note the figure below.) Recall that \(\tilde{q}\equiv K^{+}(1-a^{\prime})+K_{+}^{-}(a)\).

**Inequality 5.3** (Tail relationships)  Suppose \(K_{+}(a)<0<K(1-a)\). Then

\[0\leq d\lor d^{\prime}\leq 1, \tag{17}\] \[(\tilde{\mu}_{c}-\tilde{\mu})^{2}/\tilde{\sigma}^{2}\leq(\sqrt{a}D+ \sqrt{a^{\prime}}D^{\prime})^{2}\leq 2\ (a\ D^{2}+a^{\prime}D^{\prime^{2}}),\] (18) \[c\tilde{D}+c^{\prime}\tilde{D}^{\prime}\leq(\tilde{V}_{c}-\tilde{ V})/\tilde{\sigma}^{2}\leq\tilde{D}+\tilde{D}^{\prime}\] (19) \[\tilde{D}=D^{2}+2D\sqrt{a}|K_{+}(a)|/\tilde{\sigma}\quad\mbox{and} \quad\tilde{D}^{\prime}=(D^{\prime})^{2}+2D^{\prime}\sqrt{a^{\prime}}K(1-a^{ \prime})/\tilde{\sigma},\] (20) \[(\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2})/\tilde{\sigma}^{2}\leq D (D+2d)+D^{\prime}(D^{\prime}+2d^{\prime})\leq D(D+2)+D^{\prime}(D^{\prime}+2),\] (21) \[\tilde{q}\leq|K_{+}(a)-\tilde{\mu}|+|K(1-a^{\prime})-\tilde{\mu}| \leq 2\tilde{q},\] (22) \[\sqrt{a}|K_{+}(ca)|/\tilde{\sigma}\leq D+\sqrt{a}|K_{+}(a)|/\tilde {\sigma}\leq D+d+\sqrt{a}|\tilde{\mu}|/\tilde{\sigma},\] (23) \[D+D^{\prime}\leq 4[(a\lor a^{\prime})/(a\wedge a^{\prime})]^{1/2} \times(\tilde{q}_{c}-\tilde{q})/\tilde{q}. \tag{16}\]

**Proof.**  The figure immediately gives all but (20) and (23). For (23) observe that \(\tilde{\sigma}^{2}\geq\int_{0}^{1}(\tilde{K}(t)-\tilde{\mu})^{2}\,dt\geq(a \wedge a^{\prime})(\tilde{q})^{2}\). Since \(\nu=\mu_{Y}\) minimizes \(\mathrm{E}[(Y-\nu)^{2}]\),

(a) \[(\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2})/\tilde{\sigma}^{2}\leq(\int_{0}^{1 }(\tilde{K}_{c}(t)-\tilde{\mu})^{2}\,dt-\int_{0}^{1}(\tilde{K}(t)-\tilde{\mu}) ^{2}\,dt)/\tilde{\sigma}^{2}\] (b) \[\leq(\int_{0}^{a}+\int_{1-a^{\prime}}^{1})(\mathrm{same})dt/\tilde{\sigma}^ {2}\leq D^{2}+2Dd+(D^{\prime})^{2}+2D^{\prime}d^{\prime}.\]

There are many equivalent ways of expressing that a df or qf is in the domain of attraction of a normal or stable distribution. The next inequality enables us to go back and forth betweenvarious of these equivalent conditions and to establish new ones. We now prove that (24)-(28) below are equivalent. [We write (24)\({}_{u}\)-(28)\({}_{u}\) to denote uniformity of the inequalities in the qfs over some collection \({\cal K}_{u}\) of qfs \(K(\cdot)\).] It matters not here whether the variance of \(K(\cdot)\) is finite or infinite.

**Inequality 5.4** (Tail equivalencies): (A) Let the qf \(K\) be arbitrary. With fixed \(0<c,c^{\prime}<\infty\) (bounded from \(0\) and \(\infty\)), as \((a\lor a^{\prime})\to 0\) the following are equivalent:

\[D\lor D^{\prime}\to 0, \tag{25}\] \[[\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2}]/\tilde{\sigma}^{2}\to 0,\] (26) \[\tilde{D}\vee\tilde{D}^{\prime}\to 0,\] (27) \[[\tilde{V}_{c}-\tilde{V}]/\tilde{\sigma}^{2}\to 0,\] (28) \[[\tilde{V}_{c}-\tilde{V}]/\tilde{V}\to 0. \tag{24}\]

(B) These are also equivalent for a specific \(c\equiv c_{a}\to 1\) and \(c^{\prime}\equiv c_{a^{\prime}}\to 1\) as the maximum \((a\lor a^{\prime})\to 0\).

(C) The condition \(a\lor a^{\prime}\to 0\) may be replaced by a specific \(a_{n}\lor a^{\prime}_{n}\to 0\) as \(n\to\infty\).

(D) If any one of (24)-(27) holds uniformly over a class \({\cal K}_{u}\) of qfs \(K\), then all of them hold uniformly over the same class \({\cal K}_{u}\).

(E) Everywhere in (24)-(27) that a \(\tilde{\sigma}^{2}\) appears in a _denominator_ it may be replaced everywhere simultaneously by \(\tilde{V}\).

(F) Suppose \(X\geq 0\). We may let \(a\equiv 0\), and claim everything above with respect only to \(a^{\prime}\).

**Proof.** [The proofs are written assuming \(0<c,c^{\prime}<1\), with only minor adjustments needed otherwise] Now, (27) is equivalent to (26) by (18), and (26) implies (24) by (19) (all implications holding with the claimed uniformity). Then (24) implies (25) by (20) (also with the claimed uniformity). We will show in the next paragraph that (25) implies (27) and (24) (also with the claimed uniformity).

From the definition of \(\tilde{K}\) we have (for rectangular regions \(R_{1},R_{2},R_{3},R_{4}\) and corner regions \(R_{5},R_{6},R_{7},R_{8}\)) (noting that \({\rm E}(\tilde{K}(\xi)-\tilde{\mu})^{+}={\rm E}(\tilde{K}(\xi)-\tilde{\mu})^{+}\))

(a) \[[\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2}]/\tilde{\sigma}^{2}\] \[=(\int_{ca}^{1-c^{\prime}a^{\prime}}\int_{ca}^{1-c^{\prime}a^{ \prime}}-\int_{a}^{1-a^{\prime}}\int_{a}^{1-a^{\prime}})(s\wedge t-st)d[K(s)- \tilde{\mu}]d[K(t)-\tilde{\mu}]/\tilde{\sigma}^{2}\] (b) \[=\Sigma_{1}^{8}\int_{R_{i}}\int(s\wedge t-st)d[K(s)-\mu\ d[K(t)- \tilde{\mu}]/\tilde{\sigma}^{2}\] \[\geq\{\int_{ca}^{a}sd[K(s)-\tilde{\mu}]/\tilde{\sigma}\}\{\int_{a }^{1-a^{\prime}}(1-t)d[K(t)-\tilde{\mu}]/\tilde{\sigma}\}\] (c) \[\qquad+\int_{ca}^{a}\int_{ca}^{a}(s\wedge t-st)d[K(s)-\tilde{\mu}] d[K(t)-\tilde{\mu}]/\tilde{\sigma}^{2}+\{\mbox{(six\,terms}\}\] (d) \[\geq\{c\sqrt{a}D\}\{{\rm E}(\tilde{K}(\xi)-\tilde{\mu})^{+}/ \tilde{\sigma}\}+\{(c/2)D^{2}\}+\{\mbox{(six\, analogous\, terms)}\}\]

for \(a\lor a^{\prime}\) small enough (integrate the \({\rm E}\tilde{K}^{+}(\xi)\) term by parts).

Suppose (25) holds. From (d) (along with the other six analogous conclusions) we learn that the (25) conclusion \([\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2}]/\tilde{\sigma}^{2}\to 0\) (uniformly) implies both

\[D\sqrt{a}{\rm E}|\tilde{K}(\xi)-\tilde{\mu}|/\tilde{\sigma}\to 0\mbox{ (uniformly) and }D\to 0\,\mbox{(uniformly).} \tag{29}\]

Thus when \([\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2}]/\tilde{\sigma}^{2}\to 0\) (uniformly) this gives (recall (20), also)(e) \[\left|\frac{\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2}}{\tilde{\sigma}^{2}}-\frac{ \tilde{V}_{c}-\tilde{V}}{\tilde{\sigma}^{2}}\right|=\frac{|\tilde{\mu}_{c}^{2}- \tilde{\mu}^{2}|}{\tilde{\sigma}^{2}}\leq\frac{|\tilde{\mu}_{c}-\tilde{\mu}|}{ \tilde{\sigma}}\left(\frac{|\tilde{\mu}_{c}-\tilde{\mu}|}{\tilde{\sigma}}+2 \frac{|\tilde{\mu}|}{\tilde{\sigma}}\right)\]

\[\leq[\sqrt{a}D+\sqrt{a^{\prime}}D^{\prime}]\{[\sqrt{a}D+\sqrt{a^{\prime}}D^{ \prime}]+2E|\tilde{K}(\xi)|/\tilde{\sigma}\}\to 0 \tag{30}\]

(uniformly), and so assuming \([\tilde{\sigma}_{c}^{2}-\tilde{\sigma}^{2}]/\tilde{\sigma}^{2}\to 0\) (uniformly) implies \(|\tilde{V}_{c}-\tilde{V}|/\tilde{\sigma}^{2}\to 0\) (uniformly). That is, \((25)_{u}\) implies \((27)_{u}\). Thus \((24)_{u}\)-\((27)_{u}\) are equivalent.

Since \(|(\tilde{V}_{c}-\tilde{V})/\tilde{V}|\leq|\tilde{V}_{c}-\tilde{V}|/\tilde{ \sigma}^{2}\), we have \((27)_{u}\) implies \((28)_{u}\). Finally,

(f) \[\left|\frac{\tilde{V}_{c}-\tilde{V}}{\tilde{\sigma}^{2}}-\frac{\tilde{V}_{c}- \tilde{V}}{\tilde{V}}\right|=\frac{|\tilde{V}_{c}-\tilde{V}|}{\tilde{V}}\times \frac{\tilde{\mu}^{2}}{\tilde{\sigma}^{2}};\]

but \(\tilde{\mu}^{2}/\tilde{\sigma}^{2}\) is bounded as \(a\lor a^{\prime}\to 0\) whenever \(\sigma^{2}\in(0,\ \infty)\), and it likewise goes to zero whenever \(\sigma^{2}=\infty\) by (6.6.2). Thus (f) shows that \((28)\) implies \((27)\). Note also that \((28)_{u}\) implies \((27)_{u}\) in any class \({\cal K}_{u}\) in which

\[\overline{\lim}_{a\lor a^{\prime}\to 0}\ \sup_{K\in{\cal K}_{u}}\tilde{\mu}^{2}/ \tilde{\sigma}^{2}<\infty. \tag{31}\]

Claim (E) also follows from (6.6.2) when \({\rm E}K^{2}(\xi)^{2}=\infty\), and is trivial when this is finite. Claims (B), (C), and (G) are now trivial. \(\Box\)

**The Statistical Domain of Attraction**

**Definition 5.1** (Domains of attraction)  Let \(K\) be a qf, and define \(\tilde{V}(t)\equiv\tilde{V}(t,\ t)={\rm Var}[\tilde{K}_{t,t}(\xi)]\) as in (1). Let \(v^{-}(t)\equiv[K^{-}_{+}(t)]^{2}\) and \(v^{+}(t)\equiv[K^{+}(1-t)]^{2}\). Call \(K(\cdot)\)_balanced_ if \(v^{-}(t)/v^{+}(t)\to(1-p)/p\) for some \(p\in[0,1]\). Now define

\[{\cal D}\equiv\bigcup_{\beta\geq 0}\ \{\mbox{all $K:V(\cdot)\in\mbox{ some ${\cal R}_{-\beta}$},\mbox{ and $K(\cdot)$ is balanced if $\beta>0$}}\}, \tag{32}\]

and call \({\cal D}\) the classical _total domain of attraction_. [It is customary to focus instead on the value \(\alpha\) related to \(\beta\) by \(\alpha\equiv 2/(\beta+1)\).] Let

\[\tilde{\cal D}\equiv{\rm U}\{\mbox{all qfs $K:\tilde{V}\in{\cal R}_{-\beta}$ for some $\beta\geq 0$}\}, \tag{33}\]

and we call \(\tilde{\cal D}\) the _statistical domain of attraction_. [In trimming observations the two extreme tails do not have to be in balance, as they will be thrown away. That is, the natural CLT for the applied statistician can apply more generally than the probabilist's natural CLT. We are preparing for this.]

**Theorem 5.1** (Tail comparisons for regularly varying qfs)  Suppose the qf \(K\) is in the class \(\tilde{\cal D}\equiv\{\mbox{all qfs $K:\tilde{V}(\cdot)\in{\cal R}_{-\beta}$ for some $\beta\geq 0$}\}\). Suppose further that the following two conditions hold:

(34) (i): Fix \(c,c^{\prime}\) (if \(\beta=0\)) or (ii): Both \(c\equiv c_{a}\to 1\) and \(c^{\prime}\equiv c^{\prime}_{a}\to 1\) as \((a\lor a^{\prime})\to 0\) (if \(\beta>0\)), (i): \(0<\underline{\lim}\,a^{\prime}/a\leq\overline{\lim}\,a^{\prime}/a<\infty\) (if \(\beta=0\)) or (35) (ii): \(a^{\prime}/a\to 1\) (if \(\beta>0\)).

Then all of the conclusions \((24)\)-\((28)\) hold.

**Proof.** Because of inequality 5.4, we need only establish (28). We suppose that \(0<a,a^{\prime}<a_{\rm o}\equiv(a.\wedge(1-a.))\). We assume \(a^{\prime}<a\) in deriving an inequality (the other case of \(a<a^{\prime}\) is symmetric). Let \(\tilde{V}(a)\equiv\tilde{V}(a,a)\), etc. Now,

\[\frac{[\tilde{V}(ca,c^{\prime}a^{\prime})-\tilde{V}(a,a^{\prime})]}{\tilde{V}(a,a^{\prime})}\]

(a) \[\leq\frac{[\tilde{V}(ca)-\tilde{V}(a)]}{\tilde{V}(a)}+\frac{[\tilde{V}(c^{ \prime}a^{\prime})-\tilde{V}(a^{\prime})]}{\tilde{V}(a^{\prime})}\times\left\{1 +\frac{[\tilde{V}(a^{\prime})-\tilde{V}(a)]}{\tilde{V}(a)}\right\}.\]

Suppose \(\tilde{V}\in R_{-\beta}\), with \(\beta\geq 0\). Then \(L(a)\equiv a^{\beta}\tilde{V}(a)\in{\cal L}\) by definition of \({\cal R}_{-\beta}\). For \(\beta>0\) we have

\[\frac{[\tilde{V}(ca)-\tilde{V}(a)]}{\tilde{V}(a)}=\frac{(ca)^{-\beta}L(ca)-a^{ -\beta}L(a)}{a^{-\beta}L(a)}\]

(b) \[=c^{-\beta}\left(\frac{L(ca)}{L(a)}-1\right)+(c^{-\beta}-1)\to 0,\]

since \(L\in{\cal L},a\to 0\), and \(c\to 1\) as \((a\lor a^{\prime})\to 0\), while \([\tilde{V}(ca)-\tilde{V}(a)]/\tilde{V}(a)\to 0\) directly when \(\beta=0\) and \(\tilde{V}\in{\cal R}_{0}\). Of course, \([\tilde{V}(c^{\prime}a^{\prime})-\tilde{V}(a^{\prime})]/\tilde{V}(a^{\prime})\) is identical to the previous claim, since \(c^{\prime}\to 1\). For \(\beta>0\) (with \(1-\theta\equiv a^{\prime}/a\to 1\) as \((a\lor a^{\prime})\to 0\)) the other term in (a) satisfies

(c) \[\frac{[\tilde{V}(a^{\prime})-\tilde{V}(a)]}{\tilde{V}(a)}=\frac{[a(1-\theta)]^{ -\beta}L(a(1-\theta))-a^{-\beta}L(a)]}{a^{-\beta}L(a)}\]

(d) \[=(1-\theta)^{-\beta}\left(\frac{L(a(1-\theta))}{L(a)}-1\right)+[(1-\theta)^{- \beta}-1]\to 0,\]

while \([\tilde{V}(a^{\prime})-\tilde{V}(a)]/\tilde{V}(a)\to 0\) directly for \(\beta=0\). Note that if \(\beta=0\), we needed only \(0<a/M<a^{\prime}<Ma<\infty\) to conclude that \(\tilde{V}(a^{\prime})/\tilde{V}(a)\to 1\). \(\Box\)

**Definition 5.2** (Uniformity class \({\cal K}_{u}\))  Consider the _uniformity classes_\({\cal K}_{u}\) of qfs defined by

\[{\cal K}_{u}\equiv\{K:K(1-t)=-K(t)=t^{-\beta}\mbox{ for }0<t<1/2,\mbox{ for all }0<\beta\leq\beta_{0}\}, \tag{37}\] \[{\cal K}_{u}\equiv\{K(t)\equiv[-t^{-\beta}\cdot 1_{[t<a]}+(1-t)^{- \beta}\cdot 1_{[t>a]}]Q(t):0<\beta\leq\beta_{0},\ Q\in{\cal Q}\}, \tag{36}\]

where \({\cal Q}\) is any class of qfs \(Q\) contained in \({\cal L}={\cal R}_{0}\) that are uniformly slowly varying at \(0\) and at \(1\).

**Theorem 5.2** (Uniformity class \({\cal K}_{u}\))  As \(a\lor a^{\prime}\to 0\), we have for either of the classes \({\cal K}_{u}\) above that

\[\sup_{K\in{\cal K}_{u}}D\lor D^{\prime}\to 0, \tag{38}\]

provided that either

\[0<\liminf(c\wedge c^{\prime})\leq\limsup(c\wedge c^{\prime})< \infty\quad(\mbox{if }\beta=0)\qquad\mbox{or} \tag{40}\] \[\mbox{Both }c\equiv c_{a}\to 1\quad\mbox{and}\quad c^{ \prime}\equiv c^{\prime}_{a}\to 1\mbox{ as }(a\lor a^{\prime})\to 0\quad(\mbox{if }\beta>0). \tag{39}\]

Thus all of (24)-(28) hold uniformly over such classes \({\cal K}_{u}\).

[MISSING_PAGE_EMPTY:9286]

## 6 Inequalities for Winsorized Quantile Functions \({}^{*}\)

The key to the smoothly functioning power of the following inequalities is the formulation of the tail terms in (3) and (7) below. The inequalities look clumsy, but they work extremely efficiently. Note that the upper bounds that appear in equations (1) and (6) do not depend at all on the unknown qf \(K\). Roughly, parts (i), (ii), and (iii) prepare for the central limit theorem (CLT), the law of large numbers (LLN) (whether for second moments or for first moments), and general integral variance estimators. [If \(\tilde{\sigma}_{n}=0\), then just multiply through by this symbol. We agree that \(\int_{a}^{b}\equiv\int_{(a,b)}\) throughout the course of this section.] Use the notation of section 7.5. All qfs are assumed to be nondegenerate.

**Inequality 6.1 (Winsorized variance inequalities)** Let \(K(\cdot)\) be arbitrary. Let \(0<c\leq a_{n}<1-a_{n}^{\prime}\leq 1-c^{\prime}<1\). Let \(1/n\leq r/n\leq\frac{1}{2}\leq 1-r^{\prime}/n\leq 1-1/n\).

(i) **For the CLT** Fix \(\nu\in(0,\ \frac{1}{2})\). The following statements hold:

\[\int_{a_{n}}^{1-a_{\hat{n}}}n^{-\nu}[t(1-t)]^{1/2-\nu}dK(t)/ \tilde{\sigma}_{n}\leq\frac{\sqrt{9/\nu}}{(k_{n}\wedge k_{n}^{\prime})^{\nu}}. \tag{2}\] \[\int_{c}^{a_{n}}n^{-\nu}[t(1-t)]^{1/2-\nu}dK(t)/\tilde{\sigma}_{n }\leq\sqrt{a_{n}}|K_{+}(c)-K_{+}(a_{n})|/\tilde{\sigma}_{n}.\] (3) \[\int_{a_{n}}^{1-a_{\hat{n}}}n^{-\nu}[t(1-t)]^{1/2-\nu}dK(t)/ \tilde{\sigma}_{n}\leq\frac{\sqrt{9/\nu}}{[(k_{n}\lor r)\wedge(k_{n}^{\prime} \lor r^{\prime})]^{\nu}}+\gamma_{n}+\gamma_{n}^{\prime}, \tag{1}\]

where

\[\gamma_{n} \equiv\frac{\sqrt{r}|K_{+}(a_{n})-K_{+}(r/n)|}{\sqrt{n}\tilde{ \sigma}_{n}}\times 1_{[a_{n}<r/n]}\qquad\qquad\qquad\mbox{and}\] \[\gamma_{n}^{\prime} \equiv\frac{\sqrt{r^{\prime}}|K(1-a_{n}^{\prime})-K(1-r^{\prime}/ n)|}{\sqrt{n}\tilde{\sigma}_{n}}\times 1_{[a_{n}^{\prime}<r^{\prime}/n]}.\]

**(ii) For the WLLN and variance estimation** Fix \(\nu\in(0,1)\). Recall (6.5.7) for the distribution function \(\bar{K}_{n}^{2}\). Then

\[\int_{a_{n}}^{1-a_{\hat{n}}}n^{-\nu}[t(1-t)]^{1-\nu}dK_{n}^{-2}(t)/ \tilde{\sigma}_{n}^{2}\leq\frac{2}{[(k_{n}\lor r)\wedge(k_{n}^{\prime}\lor r^{ \prime})]^{\nu}}+\tilde{\gamma}_{n}+\tilde{\gamma}_{n}^{\prime}, \tag{4}\]

where

\[\tilde{\gamma}_{n} \equiv\frac{r|K_{+}^{2}(a_{n})-K_{+}^{2}(r/n)|}{n\tilde{\sigma}_ {n}^{2}}\times 1_{[a_{n}<r/n]}\qquad\qquad\mbox{and}\] \[\tilde{\gamma}_{n}^{\prime} \equiv\frac{r^{\prime}|K^{2}(1-a_{n}^{\prime})-K^{2}(1-r^{\prime}/ n)|}{n\tilde{\sigma}_{n}^{2}}\times 1_{[a_{n}^{\prime}<r^{\prime}/n]}.\]

Additionally,

\[\int_{a_{n}}^{1-a_{\hat{n}}}n^{-\nu}[t(1-t)]^{1-\nu}dK(t)\leq\frac{8}{\nu} \frac{||t(1-t)K(t)||_{a_{n}}^{1-a_{n}^{\prime}}}{[(k_{n}\lor r)\wedge(k_{n}^{ \prime}\lor r^{\prime})]^{\nu}}+\frac{\sqrt{r}}{\sqrt{n}}\gamma_{n}+\frac{ \sqrt{r^{\prime}}}{\sqrt{n}}\gamma_{n}^{\prime}. \tag{5}\]

**(iii) For variance estimation involving double integrals** Let

\[g_{n}(s,t)\equiv(ns(1-s))^{-\nu_{0}}(nt(1-t))^{-\nu_{1}}(s\wedge t-st)\qquad \mbox{for }s,t\in(0,1),\]where we set \(0\leq\nu_{0},\nu_{1}<\frac{1}{2}\) with \(0<\nu_{0}+\nu_{1}<1\) (to "dominate" the difference between the ("limiting" and estimated variances). Then

\[\int_{a_{n}}^{1-a_{\tilde{a}}}\!\int_{a_{n}}^{1-a_{\tilde{a}}}\!g_{n}(s,t)dK(s) dK(t)/\tilde{\sigma}_{n}^{2}\leq 2/(k_{n}\wedge k_{n}^{\prime})^{\nu_{0}+\nu_{1}}.\]

Since we also have \(1/n\leq r/n\leq\frac{1}{2}\leq 1-r^{\prime}/n\leq 1-1/n\), then

\[\int_{a_{n}}^{1-a_{\tilde{a}}}\int_{a_{n}}^{1-a_{\tilde{a}}}g_{n}( s,t)dK(s)dK(t)/\tilde{\sigma}_{n}^{2}\] \[\leq\frac{2}{[(k_{n}\lor r)\wedge(k_{n}^{\prime}\lor r^{\prime})] ^{\nu_{0}+\nu_{1}}}+8\sqrt{r}\gamma_{n}+8\sqrt{r^{\prime}}\gamma_{n}^{\prime}.\]

**Proof.** Let \(\tilde{K}\equiv\tilde{K}_{a,a^{\prime}}\) denote \(K\) Winsorized outside \((a,\ 1-a^{\prime}),\ \tilde{\mu}\equiv{\rm E}\tilde{K}(\xi)\), and \(\tilde{\sigma}^{2}\equiv{\rm Var}[\tilde{K}(\xi)]\). Integration by parts, then Cauchy-Schwarz, and the elementary inequality (\(|a|\ +\ |b|\ +\ |c|)^{2}\leq(3\max(|a|,|b|,|c|))^{2}\leq 9(a^{2}+b^{2}+c^{2})\) give (with \(\tilde{a}.\equiv\inf\{t:K(t)-\tilde{\mu}\geq 0\}\)) that

\[\int_{a}^{1-a^{\prime}}[t(1-t)]^{1/2-\nu}dK(t)=\int_{a}^{1-a^{ \prime}}[t(1-t)]^{1/2-\nu}d[K(t)-\tilde{\mu}]\] \[\leq a^{1/2-\nu}|K_{+}(a)-\tilde{\mu}|+a^{\prime 1/2-\nu}|K(1-a^{ \prime})-\tilde{\mu}|\] \[\quad+\int_{a}^{1-a^{\prime}}|K(t)-\tilde{\mu}|(1/2-\nu)|1-2t|[t( 1-t)]^{-(1/2+\nu)}\,dt\] \[\leq a^{-\nu}\{\sqrt{a}|K_{+}(a)-\tilde{\mu}|\}+(a^{\prime})^{-\nu }\{\sqrt{a^{\prime}}|K(1-a^{\prime})-\tilde{\mu}|\}\] \[\quad+(1/2-\nu)\{\int_{a}^{1-a^{\prime}}[K(t)-\tilde{\mu}]^{2}\, dt\}^{1/2}(a\wedge a^{\prime})^{-\nu}2^{\nu}/\sqrt{\nu}\] \[\leq(3/\sqrt{\nu})(a\wedge a^{\prime})^{-\nu}\{\int_{a}^{1-a^{ \prime}}[K(t)-\tilde{\mu}]^{2}\,dt\] \[\quad+a[K_{+}(a)-\tilde{\mu}]^{2}+a^{\prime}[K(1-a^{\prime})- \tilde{\mu}]^{2}\}^{1/2}\} \tag{8}\] \[\leq(3/\sqrt{\nu})(a\wedge a^{\prime})^{-\nu}\tilde{\sigma}_{K}(a,a^{\prime})\qquad\mbox{for any }0<\nu<\frac{1}{2}.\]

This gives (1). Also,

\[\int_{a}^{1-a^{\prime}}[t(1-t)]^{1-\nu}dK(t)\] \[\quad\leq a^{1-\nu}|K_{+}(a)|+(a^{\prime})^{1-\nu}|K(1-a^{\prime })+\int_{a}^{1-a^{\prime}}|K(t)/[t(1-t)]^{\nu}|dt\] \[\quad\leq(a\wedge a^{\prime})^{-\nu}\{a|K_{+}(a)|+2\int_{a}^{1-a^ {\prime}}|K(t)|dt+a^{\prime}|K(1-a^{\prime})|\} \tag{9}\] \[\quad\leq 2{\rm E}|\tilde{K}(\xi)|/(a\wedge a^{\prime})^{\nu} \qquad\mbox{for any }0<\nu<1.\]

Apply (9) to \(\bar{K}^{2}\equiv-[(K-\tilde{\mu})^{-}]^{2}+[(K-\tilde{\mu})^{+}]^{2}\) to get (4) via

\[\int_{a}^{1-a^{\prime}}[t(1-t)]^{1-\nu}d\bar{K}^{2}(t)/\tilde{\sigma}^{2}\leq 2 (a\wedge a^{\prime})^{-\nu}\qquad\mbox{for any }0<\nu<1. \tag{10}\]

[MISSING_PAGE_EMPTY:9289]

and the integral over \((a_{n},\ 1-a^{\prime}_{n})\times(a_{n},\ r/n]\) gives the same value. Whenever \(1-r^{\prime}/n\leq t\leq 1-a^{\prime}_{n}\), then both the integral over \((a_{n},\ 1-a^{\prime}_{n})\times[1-r^{\prime}/n,\ 1-a^{\prime}_{n})\) and that over \([1-r^{\prime}/n,\ 1-a^{\prime}_{n})\ \times(a_{n},\ 1-a^{\prime}_{n})\) give equal values symmetric to (e). Adding the four bounds, together with (6), then gives (7). 

**Exercise 6.1** : Suppose \(k_{n}\nearrow\infty\), but \(a_{n}\equiv k_{n}/n\to 0\). Let \(0<\nu<\frac{1}{4}\) be given. Define \(l_{n}\equiv k_{n}^{1+2\nu}\), and let \(e_{n}\equiv l_{n}/n\). Show that for _every_ nondegenerate qf \(K\) we necessarily have

\[\left(\int_{a_{n}}^{e_{n}}+\int_{1-e_{n}}^{1-a_{n}}\right)n^{-\nu}[t(1-t)]^{1/ 2-\nu}dK/\tilde{\sigma}_{n}\leq\sqrt{2}/k_{n}^{\nu}. \tag{14}\]

(Thus the tails will be separated from the vast middle by a vanishingly small piece that is inconsequential.)

## Remarks

I would like to use this discussion of the literature to say a very heartfelt "Thank you!" to a number of people who have figured prominently in my professional life. Especially, I want to thank my professors Fred Andrews (University of Oregon), Donald Truax (University of Oregon), and Lincoln Moses (Stanford University), whose voluntary efforts on my behalf had far-reaching consequences on most aspects of my life. I shall offer some thoughts on my own personal history as well as the subject matter of this book. My view is strongly affected by how I came to learn about these things. Others have undoubtedly had different experiences.

**Measure theory**

This text begins with five chapters devoted to measure theory. Halmos (1950) has had a major influence on what future books on measure theory and real analysis would contain and how they would present the subject. Other books on measure theory and real analysis that I have found to be especially useful include Royden (1963), Hewitt and Stromberg (1965), Rudin (1966), and the nicely simplified presentation of Bartle (1966). Many theorems in this introductory part are to some degree recognizable from several of these sources (and/or from the other sources listed in the probability section below). Certainly, Halmos's book was a popular one while I was getting my M.S. degree in mathematics at the University of Oregon, 1960-1962. My own introduction to "real and abstract analysis" came from a beautiful course taught by Karl Stromberg. Later, Edwin Hewitt was a colleague at the University of Washington. So it is a real pleasure for me to cite their work at various points. Lou Ward taught the topology course that I took at Oregon. He gave us a list of theorems, and we had to come up with proofs and present them. That was the most fun I ever had in the classroom. A good deal of appendix B reflects what I learned in his course. Kelly (1955), Copson (1968), and Housain (1977) are useful published sources. Watching over the Oregon graduate students in mathematics was Andrew Moursand, chairman. He really cared about all of us, and I owe him my thanks.

**Probability**

Loeve's (1977-78, originally 1955) presentation has been a very influential work on probability, certainly from the pedagogical point of view. To me, it refines and specializes much general analysis to probability theory, and then treats a broad part of this subject. Clearly, many learned probability from his text. Also, many seem to follow notational conventions used in his book. But I was rather late in learning from it. My original training was at Stanford from lectures that became Chung (1974), and those lectures also reflected Chung's efforts regarding translation of Gnedenko and Kolmogorov (1954). I truly enjoyed Chung's course, and his book. Breiman's (1968) style coincided most closely with my own. I particularly liked his treatment of partial sum and empirical processes, as one would suspect from my own research. I have sometimes used his text as a "permanent reference" to stand beside my own notes in my courses on probability theory. My choice of notation has been most influenced by Loeve and Breiman. Feller (1966) has a different flavor from most probability texts, and it includes various interesting approaches not found elsewhere. And it is informative on ratesof approximation. Billingsley (1968) created some excitement and spawned much interesting work, and a bit of that is included here. Doob's (1954) work on martingales has had a huge influence on the subject. I had the privilege of sitting in on a course on martingales taught at the University of Washington about 1968 by visiting Professor Ito. I find Meyer (1966) and Hall and Heyde (1980) particularly significant. Lectures by Tom Fleming that led to Fleming and Harrington (1991) sparked part of my martingale presentation here. Whittaker and Watson (1963) is still a superb source for the gamma function. Lehmann (1959) has greatly influenced my view of conditional probability and expectation. This brings me back to the University of Oregon, and to Fred Andrews. Fred "recruited me to statistics" and then taught a year-long course out of Lehmann's book (even though I was the only student), and he was one of those who lined me up for a National Science Foundation fellowship that made it possible for me to go to Stanford University. Don Truax also figured heavily in this. He cared about me, and I learned a lot from him. Thank you both!

The scene shifts southward. My years at Stanford were very fruitful, and I met some fine people. Ingram Olkin is fun and a good teacher, and he went out of his way to be helpful to me. The multivariate topics in appendix A represent things I learned from him. Lincoln Moses was my thesis advisor. This relationship grew out of a course in nonparametric statistics that I took from him. One of the topics in his course was Charles Stein's approach to the central limit theorem. Lin spoke on it for three days, even though he had to leave a couple of well-acknowledged gaps in his presentation--because he believed it was good work. That gave me a profound respect for him as a teacher. Lin was also my assigned advisor when I arrived at Stanford. His second sentence to me was, "OK, Shorack, what's important to you in life"? My answer had a lot to do with the geography of the Pacific Northwest. Two months before I graduated he responded on my behalf to a University of Washington opening. Wow!

At Washington I had a chance to teach courses in probability and statistics. And I learned a lot from my association with Ron Pyke, and later with Jon Wellner. The presentations in parts of chapters 12 and 14 reflect this to varying degrees. Fritz Scholz got me started on gamma approximations in the central limit theorem. Likewise, work with David Mason on quantile functions, embedding, and trimmed means is reflected in parts of chapters 6 and appendix C. I offer them all my thanks.

Obviously, I also owe a huge debt to "the literature" in regard to all these topics, and I will list some of those sources below. However, this is a textbook. It is not a research monograph. My emphasis is on presentation, not attribution. Often, my citation concerns where I learned something rather than who did it originally. And in some areas (especially, chapters 6 and 12) I have offered only a sampling of the citations that I could have given. Moreover, I have often chosen to cite a book instead of an original paper. My own work is cited "too heavily" because it is written in the same style and notation as this book.

The bibliography contains quite a number of other books on probability theory, and many are very good books. But it is the ones listed above that have had the most influence on me. I hope that the reader will find that my book also has a somewhat different flavor--a statistical flavor. That flavor will be enhanced if you think of chapters 16 and 17 and the first appendix of the original 2000 Edition as part of the total package.

Special thanks to Chari Boehnke, Roger and Heather Shorack, the Michael Boehnke family, the Barbara Aalund family, Kathleen Shorack, David Mason, Michael Perlman, Fritz and Roberta Scholz, Jon Wellner, the Jan Beirlant family, Piet Groeneboom, Frits Ruymgaart, Derek Dohn, and Pauline Reed for enabling me to write this book.

## References

* [1]**CHAPTERS 1-5 and APPENDIX B Ash**, R. (1972) _Real Analysis and Probability_. Academic Press, New York.
* [2]**Bartle**, R. (1966) _The Elements of Integration_. John Wiley & Sons, New York.
* [3]**Breiman**, L. (1968) _Probability_. Addision-Wesley, Reading, MA.
* [4]**Chow**, Y. and H. Teicher (1997) _Probability Theory: Independence, Interchangeability, Martingales_ 3rd ed. Springer-Verlag, New York.
* [5]**Cohn**, D. (1980) _Measure Theory_. Birkhauser, Boston.
* [6]**Copson**, E. (1968) _Metric Spaces_. Cambridge University Press, Cambridge.
* [7]**Halmos**, P. (1950) _Measure Theory_. Van Nostrand, Princeton, NJ.
* [8]**Hewitt**, E. and K. Stromberg (1965) _Real and Abstract Analysis_. Springer-Verlag, New York.
* [9]**Husain**, T. (1977) _Topology and Maps_. Plenum Press, New York.
* [10]**Loeve**, M. (1977-78) _Probability Theory_. Springer-Verlag, New York.
* [11]**Meyer**, P. (1966) _Probability and Potentials_. Blaisdell, Waltham, MA.
* [12]**Royden**, H. (1967) _Real Analysis_. Macmillan, New York.
* [13]**Rudin**, W. (1966) _Real and Complex Analysis_. McGraw-Hill, New York.
* [14]**CHAPTERS 6, 12, 15 and Appendix C Billingsley**, P. (1968) _Convergence of Probability Measures_. John Wiley & Sons, New York.
* [15]**Breiman**, L. (1968) _Probability_. Addision-Wesley, Reading, MA.
* [16]**Chung**, K. (1974) _A Course in Probability Theory_ 2nd ed. Academic Press, New York.
* [17]**Cramer**, H. and M. Leadbetter (1967) _Stationary and Related Stochastic Processes_. John Wiley & Sons, New York.
* [18]**Csorgo**, M., S. Csorgo, L. Horvath, and D. Mason (1986) What portion of the sample makes a partial sum asymptotically stable or normal? _Probab. Theory Related Fields_**72**, 1-16.
* [19]**Csorgo** M., S. Csorgo, L. Horvath, and D. Mason (1986) Weighted empirical and quantile processes. _Ann. Probab._**14**, 31-85.
* [20]**Csorgo**, S., E. Haeusler, and D. Mason (1988) The asymptotic distribution of trimmed sums. _Ann. Probab._**16**, 672-699.
* [21]**Csorgo** S., E. Haeusler, and D. Mason (1989) A probabilistic approach to the asymptotic distribution of sums of independent identically distributed random variables. _Adv. in Appl. Math._**9**, 259-333.
* [22]**Doob**, J. (1949) Heuristic approach to the Kolmogorov-Smirnov theorems. _Ann. Math. Statist_. **20**, 393-403.
* [23]**Freedman**, D. (1971) _Brownian Motion and Diffusion_. Holden-Day, San Fran.
* [24]**Haeusler**, E. and D. Mason (1989) A law of the iterated logarithm for modulus trimming. _Colloquia Math. Soc. J. Bolyai_, **57**, _Limit Theorems in Probability and Statistics_.
* [25]**Mason**, D. (2001) An exponential inequality for a weighted approximation to the uniform empirical process with applications. _Institute of Mathematical Statistics_: _Lecture Notes-Monograph Series_**36**, 477-498.
* [26]**Mason**, D. and G. Shorack (1990) Necessary and sufficient conditions for asymptotic normality of trimmed \(L\)-statistics. _J. Statist. Plan. Inf._**25**, 111-139.
* [27]

**Mason**, D. and G. Shorack (1992) Necessary and sufficient conditions for asymptotic normality of \(L\)-statistics. _Ann. Probab._**20**, 1779-1804.

**Meyer**, P. (1966) _Probability and Potentials_. Blaisdell, Waltham, MA.

**O'Brien**, R. (1980) A limit theorem for sample maxima and heavy branches in Galton-Watson trees. _J. Appl. Prob._ 17539-545.

**Ross**, S. (1997) _An Introduction to Probability Models_, 6th ed., Academic Press, San Diego.

**Shorack**, G. (1972) Functions of order statistics. _Ann. Math. Statist._**43**, 412-427.

**Shorack**, G. (1998) The asymptotic normality condition. Univ. Washington Dept. of Statistics Technical Reports 323, 336, 337, and 338.

**Shorack**, G. (1991) Embedding the finite sampling process at a rate. _Ann. Probability_**19**, 826-842.

**Shorack**, G. (1996) Linear rank statistics, finite sampling, permutation tests and Winsorizing. _Ann. Statist_. **24**, 1371-1385.

**Shorack**, G. (1997a) Inequalities for quantile functions with a uniform studentized CLT that includes trimming. _Nonparametric Statist._**8**, 307-335.

**Shorack**, G. (1997b) Uniform CLT, WLLN, LIL and bootstrapping in a data analytic approach to trimmed \(L\)-statistics. _J. Statist. Plan. Inf._**60**, 1-44.

**Shorack**, G. (1998) Applications of weighted approximations via quantile inequalities. _Asymptotic Methods in Probability and Statistics, A Volume in Honor of Miklos_ Csorgo. B. Szyszkowicz, ed., 151-167, Elsevier, Amsterdam.

**Shorack**, G. and J. Wellner (1986) _Empirical Processes with Applications to Statistics_. John Wiley & Sons, New York.

**CHAPTER 7 Breiman**, L. (1968) _Probability_. Addision-Wesley, Reading, MA.

**Lehmann**, E. (1959) _Testing Statistical Hypotheses_. John Wiley& Sons, New York.

**CHAPTER 8 Breiman**, L. (1968) _Probability_. Addision-Wesley, Reading, MA.

**Chow**, y. and H. Teicher (1997) _Probability Theory. Independence, Interchangeability, Martingales_ 3rd ed. Springer-Verlag, New York.

**Chung**, K. (1974) _A Course in Probability Theory_ 2nd ed. Academic Press, New York.

**Feller**, W. (1966) _An Introduction to Probability Theory and Its Applications_. Vol. 2. John Wiley & Sons, New York.

**Gine**, E., F. Gotze, and D. Mason (1998) When is the student \(t\)-statistic asymptotically standard normal? _Ann. Probab_. **25**, 1514-1531.

**Gnedenko**, B. and A. Kolmogorov (1954) _Limit Distributions for Sums of Independent Random Variables_. Addison-Wesley, Cambridge, Mass.

**Hoffman-Jorgensen**, J. (1974) Sums of independent Banach space valued random variables. _Studia Math_. **52**, 159-186.

**Kallenberg**, O. (1997) _Foundations of Modern Probability_. Springer-Verlag, New York.

**Loeve**, M. (1977-78) _Probability Theory_. Springer-Verlag, New York.

**Resnick**, S. (1999) _A Probability Path_. Birkhauser, Boston.

**Strassen**, V. (1967). Almost sure behavior of sums of independent random variables and martingales. _Proceeding of the Fifth Berkeley Symposium on Mathematical Statistics and Probability_**2**, 315-343, University of California Press, Berkeley, CA.

**CHAPTERS 9-11 and Appendix A Ahlfors**, L. (1979) _Complex Analysis_ McGraw-Hill, Inc., New York.

**Anderson**, T. (1984) _Introduction to Multivariate Statistical Analysis_. John Wiley & Sons., New York.

**Bickel**, P. and K. Doksum (1977) _Mathematical Statistics_: _Basic Ideas and Special Topics_. Holden-Day, San Francisco.

**Bhattacharya**, R. and R. Rao (1976) _Normal Approximation and Asymptotic Expansions_. John Wiley & Sons., New York.

**Billingsley**, P. (1986) _Probability and Measure_. John Wiley & Sons, New York. Breiman, L. (1968) _Probability_. Addision-Wesley, Reading, MA.

**Breiman**, L. (1968) _Probability_. Addision-Wesley, Reading, MA.

**Csorgo**, S. and D. Mason (1989) Bootstrapping empirical functions. _Ann. Statist_. **17**, 1447-1471.

**Durrett**, R. (1996). _Probability_: _Theory and Examples_. Wadsworth, Belmont, CA.

**Feller**, W. (1966) _An Introduction to Probability Theory and Its Applications_. Vol. 2. John Wiley & Sons, New York.

**Galambos**, J. (1995) _Advanced Probability Theory_ 2nd ed. Marcel Dekker, New York.

**Gine**, E., F. Gotze, and D. Mason (1997) When is the student t-statistic asymptotically standard normal? _Ann. Probab_. **25**, 1514-1531.

**Gine**, E. and J. Zinn (1990) Necessary conditions for the bootstrap of the mean. _Ann. Statist_. **17**, 684-691.

**Gnedenko**, B. and A. Kolmogorov (1954) _Limit Distributions for Sums of Independent Random Variables_. Addison-Wesley, Cambridge, Mass.

**Hall**, P. (1983) Chi squared approximations to the distribution of a sum of independent random variables. _Ann. Statist_. **11**, 1028-1036.

**Hall**, P. (1990) Asymptotic properties of the bootstrap for heavy tailed distributions. _Ann. Statist_. **18**, 1342-1360.

**Johnson**, N., A. Kotz, and N. Balakrishnan (1994) _Continuous Univariate Distributions_ Volumes 1 and 2. John Wiley & Sons, New York.

**Kendall**, M. and A. Stuart (1977) _The Advanced Theory of Statistics_ Vol. 2. Macmillan, New York.

**Lehmann**, E. and G. Casella (1998) _Theory of Point Estimation_ 3rd ed. Springer-Verlag, New York.

**Levy**, P. (1937) _Theorie de l'Addition des Variables Aleatoires_. Bautier-Villars, Paris.

**Loeve**, M. (1977-78) _Probability Theory_. Springer-Verlag, New York.

**Mason**, D. and G. Shorack (1992) Necessary and sufficient conditions for asymptotic normality of \(L\)-statistics. _Ann. Probab_. **20**, 1799-1804.

**Petrov**, V. (1975) _Sums of Independent Random Variables_. Springer-Verlag, New York.

**Rao**, C. (1965) _Linear Statistical Inference and Its Applications_ 2nd ed. John Wiley & Sons, New York.

**Shiryayev**, A. (1984) _Probability_. Springer-Verlag, New York.

**CHAPTER 13 Andersen**, P., O. Borgan, R. Gill, and N. Keiding (1993) _Statistical Models Based on Counting Processes_. Springer-Verlag, New York.

**Breiman**, L. (1968) _Probability_. Addision-Wesley, Reading, MA.

**Doob**, J. (1953) _Stochastic Processes_. John Wiley & Sons, New York.

**Fleming**, T. and D. Harrington (1991) _Counting Processes and Survival Analysis_. John Wiley & Sons, New York.

**Hall**, P. and C. Heyde (1980) _Martingale Limit Theory and Its Application_. Academic Press, New York.

**Liptser**, R. and A. Shiryaev (1977) _Statistics of Random Processes I_. Springer-Verlag, New York.

**Meyer**, P. (1966) _Probability and Potentials_. Blaisdell, Waltham, MA.

**Williams**, D. (1991) _Probability with Martingales_. Cambridge University Press, Cambridge.

**Chapter 14**

**Billingsley**, P. (1968) _Convergence of Probability Measures_. John Wiley & Sons, New York.

**Dudley**, R. (1976) _Convergence of laws on metric spaces, with a view to statistical testing_. Matematisk Institut Aarhus Universitet.

**Dudley**, R. (1989) _Real Analysis and Probability._ Wadsworth & Brooks/Cole, Pacific Grove, CA.

**Skorokhod**, A. (1956) Limit theorems for stochastic processes. _Theor. Probab. Appls._**1**, 261-290.

## Index

a.b.f., 7

Absolute continuity

fundamental theorem of calculus, 484

of functions, 20, 46, 82, 84, 86, 97, 327

of log \(f\), 484

of measures, 20, 46, 70, 73-76, 85, 86,

97, 108

of the integral, 44, 58, 73, 84, 357

Radon-Nikodym, 73

Adapted, 37, 38, 308

Added touch, 124

Ancillary statistic, 148

Approximation

by continuous functions, 63, 65

by elementary functions, 28

by simple functions, 28

by step functions, 63, 87

Approximation lemma, for sets, 15, 90, 131,

132

Associated rvs, 257

Best linear predictor, 433

Bootstrap, 269, 290

Borel sets, 4, 5, 16, 18

\(\mathcal{B},\mathcal{\hat{B}}_{\mu},\mathcal{\bar{B}}\), 18

countable base \(\mathcal{B}_{\mathcal{C}}\) of \(\mathcal{B}_{T}\), 102

generators of \(\mathcal{B}\), 25

generators of \(\mathcal{B}_{\infty}\), 38, 97

generators of \(\mathcal{B}_{n}\), 25, 37, 90

Borel-Cantelli, 150, 165, 166, 177, 184, 235,

325, 329

Bounded variation, 82-84

total variation measure, 69

Branching processes, 360

Brownian bridge, 304, 330

Brownian motion, 176, 304, 315, 322, 327,

374, 392

integrals of, 305

LIL, 304, 324

path properties, 327

reflection principle, 280, 318, 319, 325,

326, 335

transformations, 305, 322

Cantor set, 108, 113

Caratheodory

covering, 11, 17, 85

extension theorem, 5, 9, 11, 90, 99,

357

Chain rule, 74, 86

Change of variable, 86, 113

Radon-Nikodym, 73-75

unconscious statistician, 44, 74, 113

chf, 182, 200, 201 211

complementary pair, 206

continuity theorem, 226, 250

distributions on grids, 221

empirical, 208

Esseen's lemma, 231, 232, 287, 293,

296

examples, 203-205, 209

expansion for uan array, 272

Fourier, Laplace, etc., 201, 216

inversion formula, 249, 250, 287, 288,

294

moment expansions, 231

multivariate normal, 441

tail behavior, 215

uniqueness theorem, 226

Chisquare tests, 229, 247, 290

CLT, 126

asymptotic normality condition, 251,

277, 384

basic dependent, 384Berry-Esseen, 231, 236, 237, 268, 270 bootstrap, 266 classical, 178, 226, 423 connection with variance estimation, 126, 262, 474 creating normality, 409 delta method, 238 Doeblin, 243, 244 Edgeworth (_see also_), 292 examples, 228, 229, 238, 239, 242, 244, 245 finite sampling, 289, 411 for general iid uan arrays, 277 FrechetShohat via moments, 199 gamma approximation, 285, 289, 290 Liapunov, 231, 236 Lindeberg-Feller, 233, 236, 251, 275, 384 local limit (_see also_), 249 L-statistics, 412 martingale, 384 multivariate, 228, 442 negligibility, 124, 227, 474 Poisson approximation, 288 random sample size, 243 Rebolledo, 377, 378 R-statistics, 408 slowly varying \(\tilde{\sigma}^{2}(\cdot)\), 126, 262, 474 Studentized CLT for \(\tilde{\sigma}^{2}(\cdot)\in\mathcal{L}\), 262 trimmed means, 402, 403 universal bootstrap CLT, 269 universal studentized CLT, 268 Winsorized, 256, 262 Compensator, 363, 364, 375 Complete measure, 14, 31 space \(\mathcal{L}_{r}\), 55 Completeness assumption, 346 Conditional expectation, 134, 136, 143, 146, 187 properties of, 134 Conditional probability, 135, 136, 425, 442 regular, 9, 143-145 Convergence a.e., 31-33, 55 almost uniform (a.u.), 64 a.s., 35, 87, 100, 114, 181 Cesaro summability, 151, 165, 460 DCT, 43 fd convergence \(\to_{fd}\), 101 in distribution, 35, 55, 102, 112, 193, 196, 197, 211, 395 in \(\mathcal{L}_{2}\), 87 in measure, 32, 33, 55 in probability, 35, 55, 100, 181 in quantile, 112 in \(r\)th mean, 35, 55, 100 in sub-df, 193 in distribution, 35, 55, 100, 112, 113, 181, 193, 196, 387 MCT, 42 modes, 61 mutual, 31-33, 55 set, 31, 165, 182 uniform, 76 weakly in \(\mathcal{L}_{r}\), 64, 66 Convex, 49, 51, 53, 60, 346 Convolution formula, 427 Correlation coefficient, 244 multiple, 433 Counting process, 348, 375 Coupling, 291, 397 Cramer Cramer-Levy continuity theorem, 210 Cramer-von Mises, 306 Cramer-Wold device, 228, 247 Cumulant gf, 228, 286, 289, 294 Cumulative hazard function, 348, 379 **D** Decomposition of normals, 276 Decomposition of Poissons, 276 Dense Bernoulli polynomials, 173 continuous functions, 63, 87 in Hilbert space, 457 step functions, 63, 87, 359 Density estimation, 208 Derivative differentiable, 77, 84 Dini derivates, 77 Lebesgue theorem, 77, 79, 86 Radon-Nikodym, 73-75, 82, 84, 85, 134 series, 79 Taylor's theorem, 80 under integral sign, 48, 321 Determining class, 57, 198 \(C_{0},C_{b},C_{bu},C_{b}^{(k)},C_{b}^{(\infty)}\), 198 chf, 202\(\mathcal{G},\ \mathcal{G}_{0},\ \mathcal{G}_{1},\ \mathcal{G}_{2},\) 198

moments, 199, 200, 214

Df, 20, 35, 107

absolutely continuous, 20, 78, 108

decomposition of, 107

generalized, 18, 29

joint, 95, 96, 130

jumps and flatspots, 110

Lebesgue singular, 78, 108, 113

properties, 112

singular, 78, 108

sub, 35, 107, 108, 193

support, 110

Diagonalization, 194

Discontinuity set, 28

Distribution

Bernoulli, 419

beta, 423

Binomial, 419

Cauchy, 182, 243, 283, 424, 427

chisquare, 423

compound Poisson, 273

de la Vallee Poussin, 203, 206, 208,

223

double exponential, 242, 424

exponential, 87, 246, 421

extreme value, 245

gamma, 277, 285, 294, 422

geometric, 419

hitting time, 280

hypergeometric, 420

logistic, 424

multinomial, 273, 424

multivariate normal, 440

NegBiT, 289, 419

noncentral, 247, 290

normal, 245, 276, 422

Poisson, 243, 246, 276, 288, 421

sampling without replacement, 420

Snedecor's \(F_{m,n}\), 428

stable laws (_see also_), 281

Student's \(t_{m}\), 428

triangular, 203, 206, 208

uniform, 87, 423

Divergence set, 31

Domain of attraction, 491

\(\mathcal{D}\)(Normal), 262, 263, 269, 283

\(\mathcal{D}_{N}\)(Normal), 284

\(\mathcal{D}(G)\) of the stable law \(G\), 278

\(\mathcal{D}_{N}(G)\) of the stable law, 278

\(\mathcal{D}_{N}(G)\) of the stable law \(G\), 283

statistical \(\tilde{\mathcal{D}}\), 403, 413, 491

total \(\mathcal{D}\), 491

**E**

Edgeworth expansions, 287, 292, 293

Eigenvalues, 343, 431, 436

Embedding

Csorgo, M., Csorgo, S., Horvath, Mason, 332

Shorack, 332

Skorokhod, 176, 316, 322, 333

Empirical

chf, 208

df \(\mathbb{G}_{n}\), 121, 173, 329, 403

df \(\mathbb{F}_{n}\), 121, 173, 335

finite sampling process \(\mathbb{R}_{n}\), 330

process \(\mathbb{E}_{n}\), 122

process \(\mathbb{U}_{n}\), 121, 228, 329, 403

qf \(\mathbb{K}_{n}\), 120

quantile process \(\mathbb{V}_{n}\), 329

two-sample process \(\mathbb{W}_{m,n}\), 335

weighted process \(\mathbb{W}_{n}\), 330

Euclidean space, 25

Euler's constant, 425

Events, 3

Expectation, 40

of products, 128

Extremes, 230

**F**

Filtration, 310, 365, 368

augmented, 310, 346

Finite-dimensional

\(\sigma\)-field, 101

convergence \(\rightarrow_{fd}\), 101

distributions, 101

projection mappings \(\pi_{t_{1},\ldots,t_{k}}\), 101

rectangle, 97, 100

subsets, 101

Function

\(\psi\)-function, 63

elementary, 26-28

indicator, 3, 26

measurable, 26

simple, 26-28, 30, 39

step, 63

**G**

Gambler's ruin, 373, 374Gamma approximation; the GLT, 285 Generalized inverse, 432 \(\Sigma^{-}\) and \(\Sigma^{-1/2}\), 247, 432 Generators, 3 \(\sigma\)-field, 10, 26 of the induced \({\cal F}(X)\equiv X^{-1}(\bar{\cal B})\), 26 of the induced \({\cal F}(X_{1},\ldots,X_{n})\), 37 of the induced \({\cal F}(X_{1},X_{2},\ldots)\), 37 regarding \(X^{-1}\), 24 regarding independence, 8 various for Borel sets \({\cal B}\), 26 various for Borel sets \({\cal B}_{\infty}\), 38, 97 various for Borel sets \({\cal B}_{n}\), 25, 37, 90 Glivenko-Cantelli, 173, 330

**H**

Hermite polynomials, 292, 321, 347 Hewitt-Savage zero-one law, 132 Hilbert space, 454 \({\cal L}_{2}\) is a Hilbert space, 455 \(\ell_{2}\)-isomorphism, 458 Fourier coefficients, 342, 457 Gram-Schmidt, 458 inequalities, 455, 457 orthogonal projections, 456 orthonormal basis, 457-459 subspaces, 455 Hitting time, 280, 315, 318

**I**

Increment function, 18 Independence tests, 229 Independent, 139 \(\sigma\)-fields, 127, 128 rvs, 127, 129, 133, 428, 441 Indicator function proof, 30, 40, 44, 73, 90, 93, 140 Induced distribution, 24, 26, 29, 35, 56, 96, 102 distribution consistency, 98 Inequality basic, 52 Birnbaum-Marshall, 188, 337 Bonferroni, 53 bounds on \((1-x/n)^{n}\), 214 Cantelli, 169 Cauchy-Schwarz, 51, 436 Chang, 161, 340 Chebyshev, 52, 166 chf bound on tails, 210 chf key inequality, 232 convexity, 51 correlation, 51 \(C_{r}\)-inequality, 50, 139, 156, 232, 317 Daniels' equality, 161, 339 dispersion, 141, 156 Doob, 187, 188, 339 Etemadi, 159 geometric mean, 54 Gine-Zinn symmetrization, 159, 264 Hajek-Renyi, 169, 188, 337 Hardy, 53 Hilbert space inequalities, 455, 457 Hoffman-Jorgensen, 189 Holder, 50, 51, 139, 155 Jensen, 53, 54, 139, 141, 156, 188, 346 Khinchin, 155, 160, 264 Kolmogorov, 155, 165, 166, 187, 188, 244, 325 Kolmogorov's other, 183, 184 Levy, 158, 167, 176 Liapunov, 51, 54, 139, 156, 231 Littlewood, 52, 156 Marcinkiewicz-Zygmund, 160 Markov, 52, 156 Mills' ratio, 176, 178, 325, 326 Minkowski, 52, 57, 139, 156 moment expansions of chfs, 231, 234 monotone, 151, 169, 187, 188, 325 Ottaviani-Skorokhod, 158, 182 Paley-Zygmund, 52, 264, 328 Pyke-Shorack, 332, 338, 340 sandwiching the mean, 153, 154, 163, 165, 166 Shorack, 160, 487, 489, 494 Shorack-Smythe, 187 symmetrization, 157, 167 truncation, 161, 167, 466, 471, 473 upcrossing, 351 Wellner, 53 Winsorized variance, 487, 490, 494 Young, 50 Infinitely divisible, 271 log chf is never zero, 273 limits of, 272 subclass \({\cal I}_{2}\), 271 Information, 37 Integrable, 39 \({\cal L}_{r}^{+}\), 39 \({\cal L}_{1}\), 39\(\mathcal{L}_{2}\), 87

\(\mathcal{L}_{r}\), 39, 50

\(\ell_{r}\), 64

collection, 57

product, 92

uniformly, 57, 59, 60, 169, 351, 355,

364, 367

Integral, 40

improper, 48

Lebesgue, 39

Lebesgue-Stieltjes, 47

linearity, 41, 138

Riemann, 1, 84

Riemann-Stieltjes, 47, 48

Integration by parts formulas, 115, 118

Inverse image, 23, 24

of \(\sigma\)-fields, 24

Inverse transformation, 111

I.o., 7, 150

**J**

Jacobian, 86, 429

**K**

Khinchin equivalent rvs, 153, 154, 163, 184

Kolmogorov

consistency theorem, 103, 304

extension theorem, 98

Gnedenko-Kolmogorov theorem, 123,

124

inequality (_see also_), 157

Kolmogorov-Smirnov, 320, 335

representation theorem for \(\mathcal{I}\), 274

SLLN (_see also_), 163, 267

zero-one law, 131, 150, 358

**L**

Large deviations, 296

Lebesgue

decomposition theorem, 70, 73, 108

integral, 39

Lebesgue-Stieltjes measure, 18, 20, 47,

85, 90, 97

measure, 5, 25, 40, 74

measure \(\lambda_{n}\), 90

sets, 14, 16

singular df, 78, 108

sums, 1

theorem re derivatives, 77, 79, 86

Likelihood ratios, 349

LIL, 176, 178, 325, 327

Brownian motion (_see also_), 304

Limit theorem

general uan terms, 276

uan terms with negligible variances,

274

Lindeberg's \(LF_{n}^{\epsilon}\), 235, 236, 242, 251

Linear algebra, 431

Lipschitz condition, 82, 395

LLN, 171, 494

U-statistics, 358

Glivenko-Cantelli (_see also_), 173

negligibility, 161, 169, 170

random sample size, 168

ratio, 463, 476

SLLN of Kolmogorov, 155, 163, 168

strong, 168, 175, 179

weak, 169, 174, 241

WLLN of Feller, 155, 163, 168

WLLN of Khinchin, 155

Local limit theorem, 249

**M**

Martingale, 186, 315, 321, 345

\(\geq\)notation, 186

closes, 351, 353, 355, 368

CLT, 385

convergence theorem, 351, 355

counting process, 348

decomposition, 363, 364, 376

equivalence, 186, 345

examples re empiricals, 337

exponential, 347, 374

integrable, 346

Kakutani, 349, 359

optional sampling theorem, 315, 350,

367, 368

reversed, 339, 355, 358, 372

s-mg, 186, 345

square-integrable, 346

submg, 186, 187, 345

transform, 365, 366, 376

Wald, 347

Mason theorem, 333, 336, 340

Measurability criterion, 26, 27

Measurable, 26

\(\mathcal{A}^{\prime}\)-\(\mathcal{A}\)-measurable, 26

\(\mathcal{A}_{\tau}\)-measurable, 309

\(\mathcal{B}_{\infty}\)-measurable, 98

\(\mathcal{B}_{n}\)-measurable, 95\(\sigma[\mathcal{C}]\)-measurable, 26, 30, 300

\(\mathcal{D}\)-measurable, 303, 323

\(\mathcal{F}(\tilde{S})\)-measurable, 179

\(\mathcal{F}_{t}\)-measurable, 38

\(\mathcal{F}(X)\)-measurable, 26, 98, 134

\(\mathcal{F}(X_{1},\ldots,X_{n})\)-measurable, 37

\(\mathcal{F}(X_{s}:s\leq t)\)-measurable, 38, 102

\(\mathcal{F}(X_{1},X_{2},\ldots)\)-measurable, 37, 98, 102

\(\mathcal{F}(Z)\)-measurable, 30

as a limit, 27, 32

common functions are, 27

measurability criterion, 26

non, 15, 103

partition, 39

progressively, 309

set, 28, 31

space, 2

Measurable function spaces, 101

\((C,\mathcal{C})\), 101, 297, 300

\((R_{T},\mathcal{B}_{T})\), 102

\((R_{[0,1]},\mathcal{B}_{[0,1]})\), 101

\((C_{[0,\infty)},\mathcal{C}_{[0,\infty)})\), 299

\((D,\mathcal{D})\), 297, 300, 347

\((M,d)\) with \(\mathcal{M}_{d}\) or \(\mathcal{M}_{d}^{B}\), 297

general space \((M_{T},\mathcal{M}_{T})\), 101

Measure, 3

\(\sigma\)-finite, 11, 13, 68, 70, 73, 89

absolute continuity, 20, 46, 70, 71, 73-

75, 85, 86, 97, 108

Borel, 387

complete, 14, 18, 31

continuous, 69, 99

countably generated, 63, 66

counting, 5, 64, 74

finite, 68

induced, 24, 26, 29, 35, 44, 56, 102

Lebesgue, 4, 6, 25, 40, 74

Lebesgue \(\lambda_{n}\), 90

Lebesgue-Stieltjes, 4, 18, 20, 47, 85,

86, 90

monotone property, 5, 69

motivation, 4

outer, 4

outer extension, 11, 20

positive part, 69, 70

positive set, 69

probability, 20

product, 89, 91

regular, 15, 20

signed, 68, 70, 73, 84

singular, 70, 78, 108, 109

space, 3

total variation, 69

uniform convergence, 43, 57, 62, 76

Mesh, 47

Metric space, 297, 451

Arzela theorem, 394, 396, 453

Ascoli theorem, 453

compact, 452

compactness equivalents, 452

complete, 393, 451

covering numbers, 452

discontinuity set, 28

equicontinuity, 453

equivalent metrics, 451

properties, 454

regular measure, 15

relatively compact, 392

separable, 297, 393

sup norm, 453

totally bounded, 452

uniform continuity, 453

Metrics

\(d\) on \((D,\mathcal{D})\), 300

Dudley, 395

Hellinger, 76, 398

Kolmogorov, 398

Levy, 196

Prohorov, 395

total variation, 75, 249, 397, 398

Modulus of continuity, 394

Moment, 49

conditional, 434

consistent estimation, 464, 471, 473

convergence of, 57, 184, 195

correlation, 51, 133

covariance, 49, 116, 133, 440

cumulant, 289

generating function, 296

mean, 49, 115, 116, 119

moments determine the normal, 199

moments of stable laws, 280

Monte Carlo, 175

partial, 464, 470, 475, 476, 482

skewness \(\gamma_{1}\) and tail heaviness \(\gamma_{2}\), 285,

287, 290, 293

standard deviation, 49

truncated, 251, 261

variance, 49, 115, 116, 119, 474

**N**

Negligibility, 160, 169, 227, 251, 256, 271,

408, 411, 463, 474

nonnegative definite, 433, 443

uan, 235, 242, 257, 271, 330

Norm

\(\mathcal{L}_{2}\)-norm, 454

\(q\)-norm, 333

\(r\)th mean, 55

sup, 24, 297, 334, 453

Null set, 14, 93

**O**

Oh

big O, 8

big O\({}_{p}\), 156

little o\({}_{p}\), 156

little o, 8

_o_-plus \(\oplus\), or "at most", 8

Optional sampling, 350, 367, 373

Order statistics, 120, 147, 148, 239, 329,

423

Orthogonal, 229, 292, 428, 454, 457, 460

**P**

\(P_{\infty}\), 38

Partial sum process \(\mathbb{S}_{n}\), 228, 322, 324, 392,

394

Partition, 39

Poisson

compound, 273

generalized, 273

Poisson approximation, 288, 291

Poisson Limit Theorem (PLT), 227, 276

negligibility, 227

Poisson process, 349, 374, 421

Positive definite, 431

Positive part, 23, 119, 352

Predictable, 363, 364, 376

variation, 364, 366, 375, 376

Probability integral transformation, 112

inverse, 111

Process, 101

convergence on \((D,\mathcal{D})\), 393

counting, 348

empirical process (_see also_), 121

existence of on \((C,\mathcal{C})\), 300

existence of on \((D,\mathcal{D})\), 303

general, 102

independent increments, 299

normal, 101

realizations (equivalent), 101

realizations (smoother), 103

stationary, 299

stationary increments, 299

versions, 101

Product

countable, 97

cylinder set, 97

\(\mathcal{F}_{0},\ \mathcal{F},\ \mathcal{A}\times\mathcal{A}^{\prime}=\sigma[ \mathcal{F}],\ \mu\times\nu,\ 89\)

Fubini theorem, 92

integrable, 92

measurable rectangle, 89

measure, 89, 91

measure existence, 89, 92

null sets, 93

sections, 90, 92

\(\sigma\)-field, 89

space, 89

topology, 448

Product lemma, 226, 233

Product limit estimator, 380

Projection, 434, 442, 456

mappings \(\pi_{t_{1},\ldots,t_{k}}\), 101

**Q**

Qf, 111, 123, 472, 491, 494

order-\(r\), 482

standardized class \(\mathcal{K}\) bounded by \(K_{0}\),

487

Winsorized qf \(\tilde{K}\), 117, 123, 487, 494,

495

Quadratic form, 229, 431

Quantile, 110, 112, 241, 245

median, 157, 239

**R**

Random censorship, 379

Random sample size, 174, 225

Random variable (_see also_ rv), 35

Ranks and antiranks, 121, 147, 330

Rationals, 144, 281, 302, 353

Regression, 246, 434

Regularly varying, 403, 491

definition: at infinity, 485

definition: at zero, 484De Haan, 486

Karamata theorem, 484, 486

monotone density theorem, 484

qf (_see also_), 484

uniformly, 486

Representation theorem

Kolmogorov for \(\mathcal{I}_{2}\), 274

Levy-Khinchin, 276, 283

Revisualization, 305, 426

rv, 35

existence of, 32, 89, 103, 113

extended, 35

Rademacher, 159, 182, 424

random element, 101

random process, 101

random vector, 95

**S**

Sample

mean, 120, 243

median, 120, 239

quantiles, 241, 243

space, 20

trimmed mean, 120

truncated mean, 120

variance, 120, 238, 244, 262, 269, 429

Winsorized mean, 120

Winsorized variance, 120

Series, 79

three-series theorem, 181, 184

two-series theorem, 181

Set theory, 2

\(\bar{\pi}\)-system, 3, 4, 8, 30, 129, 141, 313

\(\lambda\)-system, 8, 129

\(\sigma\)-field, 3

De Morgan's laws, 3

limsup (liminf), 7, 131, 150

monotone class, 2, 10, 91, 145

pi system, 128

set difference \(A\backslash B\), 2

symmetric difference \(A\Delta B\), 2

\(\sigma\)-field, 3

\(\hat{\mathcal{A}}_{\mu}\), the completed \(\sigma\)-field, 14

\(\mathcal{A}=\sigma[\mathcal{C}]\), 11

\(\mu^{*}\)-measurable sets \(\mathcal{A}^{*}\), 3

histories, 308, 347

induced \(\mathcal{F}(X)\equiv X^{-1}(\bar{\mathcal{B}})\), 26

preservation of, 24

symmetric, 131

tail, 131

Skewness \(\gamma_{1}\) and tail heaviness \(\gamma_{2}\), 285

Skorokhod

elementary theorem, 56, 114, 275

embedding (_see also_), 333

theorem, 236, 390, 397

Slowly varying, 126, 262, 474, 475, 484

Space

Borel, 144

Hilbert, 454

inner product, 454

measurable, 3

measure, 3

probability, 20, 35

sample, 20

vector, 30, 454

Spectral decomposition, 343, 431

Stable laws, 278, 280, 281

strictly stable, 280, 281

symmetric, 281

Statistics

L-statistics, 412, 413

R-statistics, 239, 333, 407

U-statistics, 358

permutation, 409

Stirling's formula, 425

Stopping time, 179, 308, 315, 322, 324, 350, 365, 367

St. Petersburg paradox, 169

Strong Markov property, 179, 180, 280, 312, 422

Subsequences, 59, 61, 194, 275, 389, 393

criterion relating \(\to_{a.e.}\) to \(\to_{\mu}\), 33

Sufficient statistic, 147, 148

Symmetrized rv, 157, 159, 167, 184, 232

**T**

Tail equivalence, 490, 491

CLT, 482

WLLN, 167, 482

Theorem

absolute continuity of the integral, 44,

58

Arzela, 394, 396, 453

Ascoli, 453

Bochner, 223

Caratheodory extension (_see also_), 9,

11

Carleman, 200

Carleson-Hunt, 459

Chentsov, 393

[MISSING_PAGE_FAIL:518]

**W**

Waiting time, 180, 419

Wald's identity, 180

Wasserstein distance, 114

Winsorized, 123, 154, 269

fraction, 120

mean, 119, 464, 470

moment, 261

outside, 119, 123, 257, 261, 268, 402, 475, 476, 487, 495

variance, 119, 262, 266, 268, 487, 490, 491, 494