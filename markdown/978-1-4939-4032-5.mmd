[MISSING_PAGE_EMPTY:1]

Springer Texts in Statistics

_Series Editors:_

R. DeVeaux

S. Fienberg

I. Olkin

More information about this series at [http://www.springer.com/series/417](http://www.springer.com/series/417)

[MISSING_PAGE_EMPTY:12414]

Rabi Bhattacharya \(\bullet\) Lizhen Lin

Victor Patrangenaru

A Course in Mathematical

Statistics and Large Sample TheoryRabi Bhattacharya

Department of Mathematics

The University of Arizona

Tucson, AZ, USA

Victor Patrangenaru

Department of Statistics

Florida State University

Tallahssee, FL, USA

ISSN 1431-875X

ISSN 2197-4136 (electronic)

Springer Texts in Statistics

ISBN 978-1-4939-4030-1

ISBN 978-1-4939-4032-5 (eBook)

DOI 10.1007/978-1-4939-4032-5

Library of Congress Control Number: 2016941742

(c) Springer-Verlag New York 2016

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made.

### Printed on acid-free paper

This Springer imprint is published by Springer Nature

The registered company is Springer Science+Business Media LLC New York

## Preface

This book is primarily aimed at graduate students of statistics, mathematics, science and engineering who have had an undergraduate course in statistics, an upper division course in analysis and some acquaintance with measure theoretic probability. We have often taught courses based on it with very little emphasis on measure theory. Part I is designed as a one-semester course on basic parametric mathematical statistics whose presentation owes a great deal to the classic texts by Lehmann (1959) and Ferguson (1967). Part II deals with the large sample theory of statistics--parametric and nonparametric--and received a somewhat greater emphasis than Part I. But its main contents may be covered in a semester as well. Part III provides brief accounts of a number of topics of current interest. We expect the book to be used also as a reference by practitioners in other disciplines whose work involves the use of statistical procedures.

The Appendices at the end of the book provide a ready access to a number of standard results, with many proofs. Also, solutions are given to a number of selected exercises from Part I. For Part II, instead, exercises with a certain level of difficulty appear with detailed hints.

Statistics is a very big discipline and is growing fast in even new directions. The present book attempts to provide a rigorous presentation of what we consider to be the core of mathematical statistics.

It took us a long time to write this book which began with a set of class notes used over many years at Indiana University for a two-semester course in theoretical statistics. Its present incarnation, however, is quite different--much expanded and with many changes from the original.

We would like to take this opportunity to thank the NSF for its support over the years which enabled us to spend time on this project. In particular, we would like to acknowledge support from NSF grants DMS 1406872, IIS 1546331 and DMS 1106935. In addition, a UT-Austin start-up grant helped defray some of the expenses in the preparation of the manuscript. The Springer editors dealing with this book project deserve our sincere appreciation for their patience and for their counsel. Finally, we are extremely grateful to Virginia Jones, or Ginny, for her meticulous conversion of often badly handwritten material into beautiful LaTeX; her expertise with the alternate formats acceptable to the publisher and her judgement on which of them to adopt have also helped us greatly.

Tucson, AZ, USA Rabi Bhattacharya Austin, TX, USA Lizhen Lin Tallahassee, FL, USA Victor Patrangenaru October 2015

[MISSING_PAGE_EMPTY:12418]

[MISSING_PAGE_EMPTY:12419]

A Project for Students.

Appendix for Project: The Nonparametric Percentile Bootstrap of Efron

References

5 Testing Hypotheses

5.1 Introduction

5.2 Simple Hypotheses and the Neyman-Pearson Lemma

5.3 Examples

5.4 The Generalized N-P Lemma and UMP Unbiased Tests

5.5 UMP Unbiased Tests in the Presence of Nuisance Parameters

5.5.1 UMPU Tests in \(k\)-Parameter Exponential Families

5.6 Basu's Theorem

5.7 Duality Between Tests and Confidence Regions

5.8 Invariant Tests, the Two-Sample Problem and Rank Tests

5.8.1 The Two-Sample Problem

5.9 Linear Models

5.9.1 The Gauss-Markov Theorem

5.9.2 Testing in Linear Models

5.10 Notes and References

Exercises

112

Part II Mathematical Statistics: Large Sample Theory

6 Consistency and Asymptotic Distributions of Statistics

6.1 Introduction

6.2 Almost Sure Convergence, Convergence in Probability

and Consistency of Estimators

6.3 Consistency of Sample Moments and Regression Coefficients

6.4 Consistency of Sample Quantiles

6.5 Convergence in Distribution or in Law (or Weak Convergence):

The Central Limit Theorem

6.6 Asymptotics of Linear Regression

6.7 Asymptotic Distribution of Sample Quantiles, Order Statistics

6.8 Asymptotics of Semiparametric Multiple Regression

6.9 Asymptotic Relative Efficiency (ARE) of Estimators

6.10 Constructing (Nonparametric) Confidence Intervals

6.11 Errors in Variables Models

6.12 Notes and References

Exercises

158 References

164

7 Large Sample Theory of Estimation in Parametric Models

7.1 Introduction

7.2 The Cramer-Rao Bound

7.3 Maximum Likelihood: The One Parameter Case

7.4 The Multi-Parameter Case

7.5 Method of Moments

7.6 Asymptotic Efficiency of Bayes Estimators

7.7 Asymptotic Normality of \(M\)-estimators

[MISSING_PAGE_FAIL:10]

[MISSING_PAGE_EMPTY:12422]

[MISSING_PAGE_EMPTY:12423]

## Part I Mathematical Statistics: Basic (Nonsymptotic) Theory

## Chapter 1 Introduction

### 1.1 What is Statistical Inference?

Mathematics, including probability theory, is mostly concerned with _deductive inference:_ derivation of consequences of a given mathematical model. The goal of Statistics, on the other hand, is the inverse problem of _inductive inference,_ i.e., to figure out the (probability) model, or at least some features of the model, from some of its consequences (or observations). Since the model cannot be completely recovered from a finite set of observations from it, this inverse problem can only be solved approximately. The present course deals with the problem of finding an optimal approximation or, at least, a "good" approximation.

The underlying probability _model,_ usually referred to as the _population_ or _population distribution,_ is said to be _parametric,_ if it can be identified by the value of a finite-dimensional parameter. One then engages in _parametric statistical inference._ If the model can not be so identified, or indexed, by a finite-dimensional parameter, the model and the corresponding inference are said to be _nonparametric._ A special class of the nonparametric models are the so-called _semi-parametric_ models, whose structure usually involves a finite-dimensional parameter of interest, as well as a non-parametric family of distributions. If a member of the latter family is specified (as "true"), then the model is parametric with a finite-dimensional unknown parameter.

Often the interest lies only in knowing some features of the model, and not the model itself. This is generally the case with non-parametric and semi-parametric models. But even in the case of a parametric model one may only want to know the value of a subset of the parameter vector. The remaining parameters are then called _nuisance parameters._A simple illustration of the role of uncertainty in statistical analysis is provided by the problem of estimation of unknown parameters involved in the description of a deterministic physical law. If there are \(k\) unknown parameters, it would only require \(k\) observations of appropriate characteristics to compute the parameters, provided the observations are without error. In practice, however, there are always random _measuremental errors,_ and no matter how many observations are taken, one can only obtain _estimates_ of the parameters, and the estimates are subject to random errors. More commonly, the object of interest itself is random. For example, the life length of an electric bulb, the yearly rainfall in Tucson, an opinion poll. One may be interested in estimating the mean or a population proportion in such cases. Statistics analyzes the random variability inherent in the estimates and makes inferences based on them.

### Sampling Schemes

_(a) Simple Random Samples_ As mentioned above, the underlying probability model in a statistical inference problem is often referred to as the _population._ Sometimes this term is used broadly to also denote an actual finite (but generally large) group of individuals or objects whose characteristics are being explored. It is to be distinguished from the term _sample_ which usually comprises a relatively small number of units, chosen from the population (distribution) at _random,_ i.e., by a suitable probability mechanism, to ensure that the mathematical laws of probability may be applied for a scientific analysis of the sample.

We do not deal here much with the theory of _sample surveys_ which is concerned with the design of efficient and cost effective sampling schemes, taking into account the practical difficulties that arise in the implementation of such schemes. For although this is a subject of great importance in many problems such as the early projection of the results of an election, or constructing cost of living indices, or even conducting an opinion poll, for the most part in this course we consider either (1) a simple random sample of observations which may be taken to be _i.i.d._ or independent and identically distributed (each having the population distribution), or (2) a set of observations provided a priori by nature or some agency, and which may be reasonably assumed to be governed by a certain probability model. We will below briefly consider two types of simple random sampling from a finite population, as well as a stratified random sample from it, in order to estimate the mean of a population characteristic.

From a population of size \(N\) a _simple random sample_ of size \(n\) is drawn _with replacement,_ if (1) each of the \(n\) observations is drawn such as to give each of the \(N\) members of the population the same chance (namely, \(1/N\)) to be selected, and (2) the \(n\) observations are statistically independent. One may think of implementing this by having cards bearing numbers \(1,\ldots,N\), identifying the \(N\) members of the population, (1) mixing these up thoroughly in a hat, and then (2) picking one from the hat blindfolded, and observing or measuring the characteristic of the individual so chosen. Next, (3) return the chosen card back to the hat, and (4) repeat the procedures (1), (2) and (3), until \(n\) cards have been picked. For a _simple random sample without replacement,_ the step (3) is skipped. That is, the \(r\)-th card is picked from the group of \(N-(r-1)\) cards remaining in the hat after the first \(r-1\) cards are drawn and put away, giving each of these remaining cards the same chance of selection, namely, \(1/(N-r+1)\) (\(r=1,2,\ldots,n\)). For sampling without replacement, one requires \(n<N\). Various statistical softwares are available, which use sophisticated random number generators for obtaining a random sample.

A simple random sample without replacement is more efficient than a simple random sample of the same size taken with replacement. For the latter allows the wasteful possibility of observing the same individual in the population more than once. However, since the population size is in most cases enormously large compared to the sample (i.e., \(n/N\) is extremely small), these two methods are virtually the same. Since it is always easier to analyze independent and identically distributed (i.i.d.) observations, we will assume that the sample observations are independent (i.e., sampling is with replacement), unless stated otherwise. When random data are not provided by sampling, e.g., repeated measurements of length of an object using the same instrument, or amount of yearly rainfall in Tucson over the past 50 years, etc., one may consider these as independent observations or simple random samples with replacement.

_(b) Stratified Random Samples_ A perfectly homogeneous population (distribution), namely, one without variability, can be estimated without error by just one observation. For good statistical inference about a population with a great deal of heterogeneity, on the other hand, one needs a sufficiently large random sample. Thus it would make sense to divide a highly heterogeneous population into a small number of relatively homogeneous subpopulations, or _strata,_ and draw randomly and independently from each of these. Such a sampling scheme is called _stratified random sampling._ One popular way to draw a stratified random sample of size \(n\) from a population divided into \(k\) strata \(1,2,\ldots,k\), with given sizes \(N_{1},N_{2}\ldots,N_{k}\) (whose sum is \(N\)) is to draw \(k\) independent simple random samples, one from each stratum and with sample of size \(n(N_{i}/N)=nw_{i}\), say, drawn from the \(i\)-th stratum (\(i=1,\ldots,k\)). For simplicity, we assume \(nw_{i}\) to be an integer. This is the type of stratified sampling most commonly used. When good estimates of the variances \(v_{i}\) (\(i=1,\ldots,k\)) of the \(k\) strata are available, the mean of the population is more effectively estimated by letting the size \(n_{i}\) of the sample from the \(i\)-th stratum be proportional (approximately) to \(v_{i}\) as well as \(N_{i}\) (See Exercise 1.2(d)). This procedure is used instead only when reasonably reliable past information is available on the variances \(v_{i}\). On the other hand, stratum sizes are often available from census data, e.g., in the case of opinion polls, surveys of cost of living indices, etc. It may be a little surprising from the intuitive point of view that one can have arbitrarily chosen strata, without any reference to their homogeneity, and still have smaller expected squared error in estimating the mean \(m\) than one would have using a simple random sample of the same size, unless the means of all the strata are the same (Exercise 1.2(c)). Two different explanations one may advance to explain this seeming puzzle are the following. First, the statistician is using additional information here, namely, the knowledge of the sizes of sub-populations. Consider drawing a random observation \(X\) from the population in two steps. In the first step, choose a stratum at random, with the probability \(N_{i}/N\) of choosing the \(i\)-th stratum. At the second step, choose an observation at random from the chosen stratum giving equal chance of selection to every member of the stratum. Repeat these two steps independently \(n\) times to have a simple random sample of size \(n\). Then note that the conditional expectation \(v=E\{(X-m))^{2}|\)given the outcome of the first step}, is larger than \(v_{i}\equiv E\{(X-m_{i})^{2}|\) given the outcome of the first step}.

step}, if the first step outcome is the choice of the \(i\)-th stratum. Taking expectations on both sides of the inequality, one gets the desired result (Exercise 1.3). For the second intuitive explanation, note that differences among means of strata may be attributed to having on the whole less within stratum variability than the overall variability in the whole population. [However, it is easy to construct a stratum with a larger variance than in the population as a whole, for example by putting a stratum together by taking extreme observations from the right and left tails of the population. But that will reduce variability in the remaining population.]

We emphasize again that there are many practical issues that arise (1) in appropriately (and efficiently) _designing statistical experiments_ for data collection, and (2) in finding corrective methods for _non-response, missing data, dishonest reporting of data,_ etc., even in a properly designed experiment. Unfortunately, we would have little time in this course to deal with these important problems. A good classical reference on some of these matters is the book by W.G. Cochran (1977) entitled _Sampling Techniques_ (Wiley).

## 0.3 Some Simple Examples of Inference

The most common inference problems are those of (1) _estimation_ of the model or of some features of it, and (2) _testing hypotheses_ concerning the model. Among other somewhat different problems, we mention the one of (3) _classification_ of some observation as belonging to one of several populations. One may think of these as special cases of the general _statistical decision problem_ as described in the next chapter.

**Convention**  Henceforth we use the term _random sample_ to mean a simple random sample with replacement, unless stated otherwise.

Example 0.1: The Normal distribution \(N(\mu,\sigma^{2})\) provides a parametric model for measurements of lengths, etc., of some physical object. One may seek to estimate the mean length (or the "true" length) \(\mu\) and the standard deviation \(\sigma\) (a measure of imprecision of the measuring device), based on a random sample of size \(n\). If the sole interest is in \(\mu\), and not in \(\sigma\), then the parameter \(\sigma\) may be viewed as a "nuisance parameter". This is somewhat of a misnomer in the present example, since to judge the precision of any reasonable estimator of \(\mu\), one must estimate \(\sigma\). Note that (i) the sample mean \(\overline{X}\) is an unbiased estimator of \(\mu\), i.e., \(E(\overline{X})=\mu\); (ii) \(E(\overline{X}-\mu)^{2}=\sigma^{2}/n\), and (iii) \(s^{2}\equiv\sum_{1\leq i\leq n}(X_{i}-\overline{X})^{2}/(n-1)\) is an unbiased estimator of \(\sigma^{2}\).

Example 0.2: One wishes to compare two brands of automobile tires by observing the life times \(X_{1},\ldots,X_{m}\) of \(m\) tires of the first brand and those of \(n\) tires of the second brand, namely, \(Y_{1},\ldots,Y_{n}\). There is a claim that the second brand has a greater longevity than the first one. One may assume a parametric model with the two distributions \(Q_{1}\), \(Q_{2}\) on \([0,\infty)\) (of life lengths of tires of brands 1 and 2) being exponential with means \(\mu_{1}\) and \(\mu_{2}\). The claim may be stated as a hypothesis \(H:\mu_{2}>\mu_{1}\). One may test \(H\) by comparing the means \(\overline{X}\) and \(\overline{Y}\) of the two sets of observations, and accept the claim if \(\overline{Y}\) is much larger than \(\overline{X}\), and reject the claim, or \(H\), otherwise.

One may also consider the nonparametric model comprising all pairs \((Q_{1},Q_{2})\) of distributions with finite means \(\mu_{1}\), \(\mu_{2}\), respectively, and follow a similar procedure for testing \(H\) as above, based on the observed means.

Another interesting nonparametric approach to the problem in this example is the following. Instead of comparing the means of the two distributions of life lengths, it may be more appropriate to compare the two distribution functions \(F_{1}\) and \(F_{2}\). The claim may be stated as a test of the hypothesis involving random variables \(X\) and \(Y\) with distribution function \(F_{1}\) and \(F_{2}\), respectively: \(H:1-F_{2}(t)\equiv\mbox{Prob}(Y>t)\geq\mbox{Prob}(X>t)\equiv 1-F_{1}(t)\) for all positive \(t\), with a strict inequality for a least some \(t\). Suppose one only assumes that \(F_{1},F_{2}\) are continuous. Then the underlying model is the set of all pairs of continuous distributions on \([0,\infty)\). This is a nonparametric model. An appropriate and popular test of this hypothesis \(H\) is based on first ranking all the \(m+n\) observations from 1 through \(m+n\) in increasing order of their magnitudes, and then computing the mean rank of the \(Y\) observations among these. If this mean is sufficiently large compared to the mean of all the ranks, namely \((m+n+1)/2\), then one would accept the claim, and otherwise reject it. This test is known as the _Wilcoxon rank test_.

_Example 1.3_.: Consider the linear regression equation

\[Y=\alpha+\beta X+\varepsilon \tag{1}\]

where \(X\) is a non-stochastic "predictor variable", \(Y\) is the "response variable", and \(\varepsilon\) is Normal \(N(0,\sigma_{e}^{2})\). If \(n\) independent observations \((X_{i},Y_{i})\), \(i=1,\ldots,n\), follow this relation, then the underlying model is parametric as it is specified by the parameter \(\theta=(\alpha,\beta,\sigma_{e}^{2})\). One is generally interested in estimating the values of \(\alpha\) and \(\beta\), as these allow one to make a prediction of the value of \(Y\) based on (a value of) \(X\) by the formula: \(Y=\hat{\alpha}+\hat{\beta}X\), where \(\hat{\alpha}\) and \(\hat{\beta}\) are _least squares estimates_ of the corresponding parameters derived from the observations. If \(X\) is stochastic one assumes \((X_{i},\varepsilon_{i})\), \(1\leq i\leq n\), i.i.d. two-dimensional Normal, with \(E(X_{i})=\mu_{x}\), \(E\varepsilon_{i}=0\), \(\mbox{var}(X_{i})=\sigma_{x}^{2}\), \(\mbox{var}(\varepsilon_{i})=\sigma_{\varepsilon}^{2}\), \(\mbox{cov}(X_{i},\varepsilon_{i})=0\), and carries out the same analysis.

If one drops the assumption of Normality of \(\varepsilon\) and, instead, simply assumes that it has finite second moments and \(E(\varepsilon)=0\), then the model is semi-parametric.

### Notes and References

The present book is not concerned with the methodology for obtaining sample data; it simply assumes that the data conform to the hypothesis of randomness. There are many instances where nature provides data automatically (monthly rainfall, daily temperature, number of traffic accidents per week at a city cross section, etc.). In contrast, proper designing of sampling schemes are very important in the sampling of items for testing for defects or other features, public opinion polls, etc. Cochran (1977) is a classic text on the pitfalls involved in taking samples and sample surveys (bias arising from non-response and other forms of missing data, deliberate falsification of data, etc.), and on how to deal with them. It also describes optimal sampling schemes taking both accuracy and costs into consideration. In view of the advent of the computer and the internet, newer methods of taking samples are being developed. Still, some of the basic issues of sampling remain the same.

### Exercises for Chap. 1

**Ex. 1.1**.: From a population of size \(N\) a simple random sample of size \(n\) is drawn _without replacement,_ and a real-valued characteristic \(X\) measured to yield observations \(X_{j}\) (\(j=1,2,\ldots,n\)). Show that

1. the sample mean \(\overline{X}\) is an unbiased estimator of the population mean \(m\) (i.e., \(E(\overline{X})=m\)).
2. the expected squared error of \(\overline{X}\) as an estimator of \(m\), i.e., the variance of \(\overline{X}\), is smaller than that of the mean of a simple random sample of the same size \(n\) drawn _with replacement,_ and
3. the difference between the expected squared errors of the two estimators is \(O(n/N)\), as \(n/N\) goes to zero.

**Ex. 1.2**.: Suppose a population of size \(N\) is divided into \(k\) strata of sizes \(N_{i}\) (\(i=1,2,\ldots,k\)). Let \(m_{i}\) be the mean of the \(i\)-th stratum and \(\nu_{i}\) its variance (\(i=1,2,\ldots,k\)). For each \(i\), a simple random sample \(\{X_{ij}:j=1,\ldots,n_{i}\}\) of size \(n_{i}\) is drawn with replacement from the \(i\)-th stratum, and let \(\overline{X}_{i}\) denote the corresponding sample mean. Let \(\{x_{ij}:j=1,\ldots,N_{i}\}\) be the values of the \(N_{i}\) units in the \(i^{th}\) population (stratum), and \(\{x_{i}:i=1,\ldots,N\}\) be an enumeration of the values of the \(N\) units in the population overall. For the following statements, assume \(\nu_{i}>0\) and \(n_{i}>0\) for all \(i\), to avoid trivialities.

1. Show that (1) \(\overline{Y}:=\sum w_{i}\overline{X}_{i}\) is an unbiased estimator of the population mean \(m\) (i.e., \(E(\overline{Y})=m\)), where \(w_{i}=N_{i}/N\), and (2) \(E(\overline{Y}-m)^{2}=\sum w_{i}^{2}(\nu_{i}/n_{i})\).
2. Let \(v\) denote the population variance: \(v=(\frac{1}{N})\sum_{1\leq i\leq N}(x_{i}-m)^{2}=E(X-m)^{2}\), where \(x_{i}\) is the characteristic of the \(i\)-th member of the population, and \(X\) is a random observation from the population, i.e., \(P(X=x_{i})=1/N\) for all \(i\). Show that \(\nu=\sum_{1\leq i\leq k}w_{i}v_{i}+\sum_{1\leq i\leq k}w_{i}(m_{i}-m)^{2}\).
3. Let \(\overline{X}=(\frac{1}{n})\sum_{1\leq j\leq n}X_{j}\) be the mean of a simple random sample with replacement. Show that if \(n_{i}=nw_{i}\) for all \(i\), then \(E(\overline{Y}-m)^{2}<E(\overline{X}-m)^{2}\), unless \(m_{i}\)'s are all equal, in which case equality holds.
4. Suppose you know \(\nu_{i}\) for all \(i\). Show that the optimal choices of \(n_{i}\) (for minimizing the expected squared error of estimation of \(m\) by \(\overline{Y}\)) are \[n_{i}=\frac{nw_{i}\sqrt{\nu_{i}}}{\sum_{1\leq r\leq k}w_{r}\sqrt{\nu_{r}}} \qquad(1\leq i\leq k),\] (1.2) assuming the right sides are integers. [Hint: Use calculus and Lagrange multipliers for the constraint \(\sum n_{i}=n\).]

**Ex. 1.3**.:
1. Justify the inequality \(V\equiv E((X-m)^{2}\mid I)\geq v_{I}\equiv E((X-m_{I})^{2}\mid I)\) in Sect. 1.2(b) on stratified random samples, where \(I\) is the index of the stratum picked at random, and \(m_{I}=E(X\mid I)\).
2. Use this to show that \(v\equiv E(X-m)^{2}\geq\sum_{i=1}^{k}\frac{N_{i}}{N}\;v_{i}\), with equality if and only if \(m_{i}\)'s are all the same.

**Ex. 1.4** (**A Problem of Non-response).**: In a large random sample of size \(n\) from a very big population of size \(N\), there were a sizeable number \(n^{\char 126}\) of non-responses. To avoid the possibility of a systematic bias in the estimation of the population mean \(m\) of a certain variable \(X\), a random sub-sample of size \(s\) is drawn from the \(n^{\char 126}\) non-respondents and their \(X\)-values obtained, with additional efforts and costs. Let \(X_{j}\), \(1\leq j\leq n-n^{\char 126}\) denote the (observed) \(X\)-values of the original respondents, and \(X_{j}\), \(n-n^{\char 126}+1\leq j\leq n\) those of the (unobserved) respondents. Let \(Y_{j}\), \(1\leq j\leq s\), be the sub-sample observations.

Assume that the population comprises two groups--one (of size \(N-N^{\char 126}\), say,) from which the responses came and the other from which non-responses occurred. The two groups have possibly different means \(m_{R}\) and \(m^{\char 126}\), respectively, and variances \(\sigma_{R}^{2}\) and \(\sigma^{2\char 126}\). Let \(\overline{X}_{R}=\sum_{j=1}^{n-n^{\char 126}}X_{j}/(n-n^{\char 126})\) denote the mean of the responses, \(\overline{Y}=\sum_{j=1}^{s}Y_{j}/s\), \(\overline{X}^{\char 126}=\sum_{j=n-n^{\char 126}+1}^{n}X_{j}/n^{\char 126}\).

1. Show that \(\overline{Z}=[(n-n^{\char 126})\overline{X}_{R}+n^{\char 126}\overline{Y}]/n\) is an unbiased estimate of the population mean \(m\). [Hint: \(m=[(N-N^{\char 126})m_{R}+N^{\char 126}m^{\char 126}]/N\), \(E\overline{Z}=E(E[\overline{Z}\mid\xi n^{\char 126};\)\(X_{j}\), \(n-n^{\char 126}+1\leq j\leq n])=E((n-n^{\char 126})_{m_{R}}+n^{\char 126}\overline{X}^{ \char 126})/n\).]
2. Compute the variance of \(\overline{Z}\). [Hint: \(E(\overline{Z}-m)^{2}=E[\overline{Z}-E(\overline{Z}\mid\mathscr{F})+E( \overline{Z}\mid\mathscr{F})-m]^{2}=E(E((\overline{Z}-E)\overline{Z}\mid \mathscr{F})^{2}\mid\mathscr{F})+E(E(\overline{Z}\mid\mathscr{F})-m)^{2}\), where \(\mathscr{F}=\sigma\{n^{\char 126};X_{j},n-n^{\char 126}+1\leq j\leq n\}\).]

## Reference

Cochran, G. W. (1977). _Sampling techniques_ (3rd ed.). New York: Wiley.

## Chapter 2 Decision Theory

### 2.1 Decision Rules and Risk Functions

For a substantial part of this course our main interest will be in parametric models. The theory here is well developed and it helps one understand the issues that arise in the analysis of more complex models.

In the following the _observation_\(\mathbf{X}\) comprises all that is observed. For example, it may be given as a vector of i.i.d. random variables constituting the sample from a population: \(\mathbf{X}=(X_{1},\ldots,X_{n})\), as in the case of Example 1.1. In Example 1.2, \(\mathbf{X}=(X_{1},\ldots,X_{m},Y_{1},\ldots,Y_{n})\) where the \(X_{i}\)'s are i.i.d. observations from one population while the \(Y_{j}\)'s are i.i.d. observations from a second population, the two sets being independent of each other. In Example 1.3, \(\mathbf{X}=((X_{1},Y_{1}),\ldots,(X_{n},Y_{n}))\), obeying (1.1) as specified. Sometimes, with a slight abuse of this terminology, we will also describe the _individual components of \(\mathbf{X}\) as observations,_ when the context is clear.

We begin informally. Let \(\boldsymbol{\varTheta}\) be the parameter space, \(\mathbf{X}\) the observation (vector), and \(\mathscr{A}\) the set of all possible decisions or actions the statistician can take. A decision rule is a function \(d(\cdot)\) of the observation, taking values in \(\mathscr{A}\). A loss function \(L(\theta,a)\) is prescribed, measuring the loss incurred when an action \(a\) is taken while \(\theta\) is the true parameter value. The risk function \(R(\theta,d)\) associated with a decision rule \(d\) is defined by

\[R(\theta,d)=E_{\theta}L(\theta,d(\mathbf{X})), \tag{2.1}\]

where \(E_{\theta}\) denotes expectation under \(\theta\) (i.e., when the true parameter value is \(\theta\)).

Suppose \((S_{1},\mathscr{B}(S_{1}))\), \((S_{2},\mathscr{B}(S_{2}))\) are two measurable spaces, i.e., \(\mathscr{B}(S_{i})\) is a sigma-field on \(S_{i}\)\((i=1,2)\). A function \(f:S_{1}\to S_{2}\) is _measurable_ if \(f^{-1}(B_{2})\in\mathscr{B}(S_{1})\)\(\forall\)\(B_{2}\in\mathscr{B}(S_{2})\). We will often express this by saying \(f\) is measurable on \((S_{1},\mathscr{B}(S_{1}))\) into \((S_{2},\mathscr{B}(S_{2}))\).

**Definition 2.1**.: The _parameter space, action space_, and _observation space_ are three measurable spaces \((\boldsymbol{\Theta},\mathscr{B}(\boldsymbol{\Theta}))\), \((\mathscr{A},\mathscr{B}(\mathscr{A}))\), \((\mathscr{X},\mathscr{B}(\mathscr{X}))\), respectively. In the case of a metric space \(S\), \(\mathscr{B}(S)\) denotes the Borel sigma-field on \(S\). A _loss function_ is a real-valued measurable function on \((\boldsymbol{\Theta}\times\mathscr{A},\mathscr{B}(\boldsymbol{\Theta})\otimes \mathscr{B}(\mathscr{A}))\) (into \(\mathbb{R},\mathscr{B}(\mathbb{R})\)). Here \(\otimes\) is used to denote the product sigma-field of its factors. For each parameter value \(\boldsymbol{\theta}\), there exists a (specified) probability measure \(P_{\boldsymbol{\theta}}\) on a measurable space \((\boldsymbol{\Omega},\mathscr{F})\), with the corresponding probability space \((\boldsymbol{\Omega},\mathscr{F},P_{\boldsymbol{\theta}})\). An _observation_\(\mathbf{X}\) is a measurable map on \((\boldsymbol{\Omega},\mathscr{F})\) into the observation space \((\mathscr{X},\mathscr{B}(\mathscr{X}))\). A (non-randomized) _decision rule_\(d\) is a measurable map on the observation space \((\mathscr{X},\mathscr{B}(\mathscr{X}))\) into the action space \((\mathscr{A},\mathscr{B}(\mathscr{A}))\). The _risk function_\(R\) of a decision rule \(d\) is given by (2.1), where \(E_{\boldsymbol{\theta}}\) denotes expectation w.r.t. \(P_{\boldsymbol{\theta}}\).

In most problems that we deal with, the spaces \(\boldsymbol{\Theta}\), \(\mathscr{A}\), and \(\mathscr{X}\) are either countable or separable metric spaces. In the case a space is countable one uses the discrete topology on it, so that the sigma-field on it comprises all its subsets.

We consider four examples, the first two concern the problem of _estimation_, while the other two deal with the problem of _testing hypotheses_.

_Example 2.1_.: For the estimation of the mean of a Normal distribution based on a random sample \(\mathbf{X}=(X_{1},X_{2},\ldots,X_{n})\) of size \(n\), considered in Example 1.1 in Chap. 1, the parameter space is \(\boldsymbol{\Theta}=(-\infty,\infty)\times(\mathbf{0},\infty)=\mathbb{R} \times\mathbb{R}_{++}\), and the action space is \(\mathscr{A}=\mathbb{R}\). Here the observation space is \(\mathscr{X}=\mathbb{R}^{n}\). The most commonly used loss function is _squared error loss_

\[L(\boldsymbol{\theta},\boldsymbol{a})=|\mu-a|^{2},\qquad(\boldsymbol{\theta}=( \mu,\sigma^{2})). \tag{2.2}\]

One may take \(\Omega=\mathbb{R}^{\infty}\), the space of all infinite sequences of reals \((x_{1},x_{2},\ldots)\). Then the sigma-field \(\mathscr{F}\) is the Kolmogorov product sigma-field \(\mathscr{B}(\mathbb{R}^{\infty})\), and \(P_{\boldsymbol{\theta}}\) is the product probability measure with all factors being the Normal distribution \(N(\mu,\sigma^{2})\). That is, \(P_{\boldsymbol{\theta}}\) is the distribution of a sequence of i.i.d. \(N(\mu,\sigma^{2})\) random variables. The observation \(\mathbf{X}\) is the projection map on \(\Omega\) into its first \(n\) coordinates. Alternatively, one may take \(\Omega=\mathscr{X}=\mathbb{R}^{n}\), \(\mathbf{X}\) as the _identity map:_\(\mathbf{X}(\omega)=\omega=(x_{1},\ldots,x_{n})\) and \(P_{\boldsymbol{\theta}}\) as the product probability measure with all \(n\) factors the same, namely, \(N(\mu,\sigma^{2})\).

For the decision rule \(d(\mathbf{x})=\overline{x}\equiv(x_{1}+x_{2}+\cdots+x_{n})/n\) (\(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\in\mathscr{X}=\mathbb{R}^{n}\)), \(d(\mathbf{X})=\overline{X}\), and the risk function is given by

\[R(\boldsymbol{\theta},d)=E_{\boldsymbol{\theta}}(\mu-\overline{X})^{2}=\frac{ \sigma^{2}}{n}\,,\qquad\boldsymbol{\theta}=(\mu,\sigma^{2})\in\boldsymbol{ \Theta}. \tag{2.3}\]

If, instead of \(\mu\), the parameter of interest is \(\sigma^{2}\), then the action space is \(\mathbb{R}_{++}\). One may use the decision rule \(d(\mathbf{x})=\sum_{1\leq i\leq n}(x_{i}-\overline{x})^{2}/(n-1)\). Then \(d(\mathbf{X})=s^{2}\), as defined in Example 1.1 in Chap. 1. One may let \(\mathscr{A}=[0,\infty)\), and show (Exercise 2.1) that with squared error loss function \(L(\boldsymbol{\theta},a)=(\sigma^{2}-a)^{2}\), the risk function of this decision rule \(d\) is given by

\[R(\boldsymbol{\theta},d)=E_{\boldsymbol{\theta}}(\sigma^{2}-s^{2})^{2}=2\sigma^ {4}/(n-1). \tag{2.4}\]Finally, suppose one wishes to estimate the vector parameter \(\boldsymbol{\theta}\), then the action space is \(\mathbb{R}\times[0,\infty)=\mathbb{R}\times\mathbb{R}_{+}\). If the loss function is squared error (in Euclidean distance),

\[L(\boldsymbol{\theta},\mathbf{a})=|(\mu,\sigma^{2})-\mathbf{a}|^{2}\qquad \boldsymbol{\theta}=(\mu,\sigma^{2}),\quad\mathbf{a}=(a_{1},a_{2})\in\mathbb{ R}\times\mathbb{R}_{+}, \tag{2.5}\]

and one uses the _estimator_\(d(\mathbf{X})=(\overline{X},s^{2})\), then the risk function is given by the sum of the right sides of (2.3) and (2.4),

\[R(\boldsymbol{\theta},d)=\frac{\sigma^{2}}{n}+\frac{2\sigma^{4}}{n-1},\qquad \boldsymbol{\theta}=(\mu,\sigma^{2}). \tag{2.6}\]

Since the units of \(\mu\) and \(\sigma^{2}\) are different, it may be more appropriate to define the loss function as \((\mu-a_{1})^{2}+(\sigma-\sqrt{a_{2}})^{2}\). Then the risk function of \(d\) above is \(c(n)\sigma^{2}\), where \(c(n)\) only depends on \(n\) and is of the order \(O(n^{-1})\), as \(n\to\infty\) (Exercise 2.1).

For computations related to this example, the following proposition is useful. It is also an important property of samples from a Normal distribution. We will write \(U\stackrel{{ d}}{{\sim}}G\) to indicate that the random variable \(U\) has distribution \(G\). Also \(U\stackrel{{ d}}{{=}}V\) means \(U\) has the same distribution as \(V\).

**Proposition 2.1**.: _Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. \(N(\mu,\sigma^{2})\). Then \(\overline{X}\) and \(s^{2}\) are independent, with \(\overline{X}\stackrel{{ d}}{{\sim}}N(\mu,\frac{\sigma^{2}}{n})\) and \((n-1)s^{2}/\sigma^{2}\stackrel{{ d}}{{=}}\sum_{i=2}^{n}Y_{i}^{2}\), where \(Y_{2},\ldots,Y_{n}\) are i.i.d. standard Normal \(N(0,1)\)._

Proof.: First let \(\mu=0\), \(\sigma^{2}=1\). Write \(\mathbf{X}\) as the column vector \(\mathbf{X}=(X_{1},\ldots,X_{n})^{\prime}\). Let \(O\) be an \(n\times n\) orthogonal matrix whose first row is \(\left(\frac{1}{\sqrt{n}},\frac{1}{\sqrt{n}},\ldots,\frac{1}{\sqrt{n}}\right)\) [For example, take the \((k+1)\)-th row of \(O\) as \(\underbrace{(a,a,\ldots,a}_{k\text{ elements}}-ka,0,0,\ldots,0)\), with \(a=(k(k+1))^{-\frac{1}{2}}\)\((k=1,2,\ldots,n-1)\)]. Define \(\mathbf{Y}=O\mathbf{X}\). Then the probability density function of \(\mathbf{Y}=(Y_{1},Y_{2},\ldots,Y_{n})^{\prime}\) is the same as that of \(\mathbf{X}\), i.e., \(Y_{1},Y_{2},\ldots,Y_{n}\) are i.i.d. \(N(0,1)\) [The Jacobian of the transformation has determinant \(|O^{\prime}|=\pm 1\); \(|\mathbf{X}|^{2}=|\mathbf{Y}|^{2}\)]. But \(Y_{1}=\sqrt{n}\,\overline{X}\), and \(Y_{2}^{2}+Y_{3}^{2}+\cdots+Y_{n}^{2}\equiv\sum_{i=1}^{n}Y_{i}^{2}-Y_{1}^{2}= \sum_{i=1}^{n}X_{i}^{2}-n\overline{X}^{2}\equiv(n-1)s^{2}\) are independent.

To complete the proof in the general case \(X_{i}\stackrel{{ d}}{{\sim}}N(\mu,\sigma^{2})\), write \(Z_{i}=(X_{i}-\mu)/\sigma\), and apply the above argument to \(\mathbf{Z}=(Z_{1},Z_{2},\ldots,Z_{n})^{\prime}\). 

_Example 2.2_.: One of the most common statistical problems is that of estimation of the proportion \(\theta\) of members of a population possessing some characteristic. For example, \(\theta\) may be the proportion of adult Americans who are supportive of a pending gun control legislation, or it may be the proportion of defective items among all items of some kind manufactured by a company. A random sample \(\mathbf{X}=(X_{1},\ldots,X_{n})\) is observed, where \(X_{i}\) is 1 or 0 according as the \(i\)-th observation possesses the characteristic ("support gun control", "defective") or it does not. The most common estimate is the _sample proportion_\(d_{1}(\mathbf{X})=(X_{1}+\cdots+X_{n})/n\) of those in the sample possessing the characteristic. Its risk function is (Exercise 2.2)

\[R(\theta,d_{1}(\mathbf{X}))=\theta(1-\theta)/n,\qquad\theta\in\Theta=[0,1]. \tag{2.7}\]Here the observation space is \(\mathscr{X}=\{0,1\}^{n}\), and one may take \(\Omega=\mathscr{X}\), and \(P_{\theta}\) as the product probability: \(P_{\theta}(\{\mathbf{x}\})=\theta^{\sum x_{i}}(1-\theta)^{\sum(1-x_{i})}\), for all \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})\in\{0,1\}^{n}\).

If, instead of the sample proportion, one uses the (seemingly bad) estimator \(d_{2}(\mathbf{X})\equiv 1/3\), then the risk function is

\[R(\theta,d_{2}(\mathbf{X}))=(\theta-1/3)^{2},\quad\text{which is }0\text{ for } \theta=1/3. \tag{2.8}\]

No other estimator has a risk as low as that of \(d_{2}(\cdot)\) at the value \(\theta=1/3\). Although for values of \(\theta\) a bit away from \(1/3\), this estimator has a large risk, one can not get an estimator as good as \(d_{2}\) everywhere on the parameter space! Thus one can not hope for a decision rule which is better (or, at least, as good as) every other estimator uniformly over the entire parameter space. We will introduce later less stringent notions of optimality.

We next consider the problem of testing hypotheses about the underlying model, which may be expressed in terms of a _null hypothesis_\(H_{0}:\theta\in\Theta_{0}\) where \(\Theta_{0}\) is a subset of \(\Theta\), and an _alternative hypothesis_\(H_{1}:\theta\in\Theta_{1}=\Theta\backslash\Theta_{0}\). Here the action space is \(\mathscr{A}=\{a_{0},a_{1}\}\), where \(a_{i}\) accepts the hypothesis \(H_{i}\) (\(i=0,1\)). The most commonly used loss function in this case is the \(0-1\)_loss function,_

\[L(\theta,a_{i})=1-\delta_{ij}\quad\text{if }\theta\in\Theta_{j},\qquad(i,j\in\{0,1\}), \tag{2.9}\]

where \(\delta_{ij}\) is Kronecker's delta, which equals \(1\) or \(0\) according as \(i=j\) or \(i\neq j\). Thus the loss incurred is \(0\) if \(a_{i}\) is the correct decision (to accept \(H_{i}\)), and \(1\) if it is the wrong decision.

**Definition 2.2**.: The error of accepting the alternative hypothesis \(H_{1}\) when the null hypothesis \(H_{0}\) is correct, is called a _Type I Error_. Its probability is \(P_{\theta}(d(\mathbf{X})=a_{1})=P_{\theta}(\text{Accept the alternative hypothesis})\), when \(\theta\in\Theta_{0}\). The error of accepting the null hypothesis \(H_{0}\) when the alternative hypothesis \(H_{1}\) is correct is called a _Type II Error_. Its probability is \(P_{\theta}(d(\mathbf{X})=a_{0})=P_{\theta}(\text{Accept the null hypothesis})\), when \(\theta\in\Theta_{1}\).

_Example 2.3_.: In quality control experiments, an inspector is often confronted with the problem of deciding whether to "pass" or "fail" a product. For example, a product or a large batch of it may be considered good only if no more than \(5\%\) of the items are defective. The inspector's decision is to be based on a random sample \(\mathbf{X}=(X_{1},X_{2},\ldots,X_{n})\), where \(X_{i}=0\) or \(1\) according as the \(i\)-th item picked is defective or not. The parameter space \(\boldsymbol{\Theta}\) here is the set of all possible proportions \(\theta\) of defective items, conveniently taken as the unit interval \([0,1]\). The spaces \(\mathscr{X}\), \(\Omega\), \(\Theta\), and the probability distribution \(P_{\theta}\) are as in Example 2.2. Let \(\Theta_{0}=[0,0.05]\), while \(\Theta_{1}=(0.05,1.0]\). The action space is \(\{a_{0},a_{1}\}\), where the action \(a_{0}\) means the product is passed and \(a_{1}\) means it is failed. Suppose one uses the decision rule: \(d(\mathbf{x})=a_{0}\) if the sample proportion \(\hat{p}\) of defectives is \(0.03\) or less, and \(d(\mathbf{x})=a_{1}\) otherwise. Under the loss function (2.9), the risk function of the decision rule \(d(\cdot)\) is (Exercise 2.3)\[R(\theta,d)\!\!=\!\!E_{\theta}L(\theta,d(\mathbf{X}))=\left\{\begin{array}{ll}P_{ \theta}(\hat{p}\!\!>\!\!0.03)=\sum\limits_{r>0.03n}^{n}C_{r}\theta^{r}(1\!-\! \theta)^{n-r}\text{ for }0<\theta\leq 0.05,\\ \\ P_{\theta}(\hat{p}\leq 0.03)=\sum\limits_{r\leq 0.03n}^{n}C_{r}\theta^{r}(1\!-\! \theta)^{n-r}\text{ for }0.05<\theta\leq 1.\end{array}\right. \tag{10}\]

The top probability in (10) is the probability of a Type I Error, while the bottom probability is that of a Type II Error.

Consider Example 1.2 of Chap. 1, assuming that the lifetime distributions are exponential with means \(\mu_{1}\) and \(\mu_{2}\). We let \(H_{1}\) denote the claim: \(\mu_{2}>\mu_{1}\), so that \(\Theta_{1}=\{(\mu_{1},\mu_{2})\in(0,\infty)^{2}:\mu_{2}>\mu_{1}\}\), while \(\Theta_{0}=\{(\mu_{1},\mu_{2})\in(0,\infty)^{2}:\mu_{2}\leq\mu_{1}\}\). The observation space is \(\mathscr{X}=\{(\mathbf{x},\mathbf{y}):\mathbf{x}=(x_{1},\ldots,x_{m})\in[0, \infty)^{m},\,\mathbf{y}=(y_{1},\ldots,y_{n})\in[0,\infty)^{n}\}\). Let us denote the observation vector by \(\mathbf{X}=(\{X_{j}:j=1,\ldots,m\},\,\{Y_{j}:j=1,\ldots,n\})\). Write \(\overline{x}=\sum x_{j}/m\), \(\overline{y}=\sum y_{j}/n\), and \(\overline{X}\), \(\overline{Y}\) for the means of the corresponding sample observations.

Suppose one uses the decision rule \(d(\mathbf{x},\mathbf{y})=a_{1}\) if \(\overline{y}>(1+c)\overline{x}\) for some constant \(c>0\), and \(d(\mathbf{x},\mathbf{y})=a_{0}\) if \(\overline{y}\leq(1+c)\overline{x}\). Then the risk function of \(d\) is \(E_{\theta}L(\theta,d(\mathbf{X}))\), which equals \(E_{\theta}(\mathbf{1}_{\left\lceil\overline{Y}\leq(1+c)\overline{X}\right\rceil})\) for \(\theta\) in \(\Theta_{1}\), and \(E_{\theta}(\mathbf{1}_{\left\lfloor\overline{Y}>(1+c)\overline{X}\right\rfloor})\) for \(\theta\) in \(\Theta_{0}\). That is,

\[R(\theta,d)\!=\!\!\begin{cases}P_{\theta}(\overline{Y}\leq(1\!+\!c)\overline{ X})\text{ if }\theta\in\Theta_{1}\text{ (i.e., if }\mu_{2}>\mu_{1}),\,\text{[Type II Error Probability]}\\ P_{\theta}(\overline{Y}>(1\!+\!c)\overline{X})\text{ if }\theta\in\Theta_{0}\text{ (i.e., if }\mu_{2}\leq\mu_{1}),\,\text{[Type I Error Probability]}.\end{cases} \tag{11}\]

### Randomized Decision Rules, Admissibility

Sometimes for a given observation one may wish to assign probabilities among different (sets of) actions, instead of choosing a single action. We will later see that such allowance for randomization among several actions is particularly important in the case of hypothesis testing. Thus a general decision rule \(\delta\) (allowing randomization in the action space) may be defined as a measurable map from the observation space \((\mathscr{X},\mathscr{B}(\mathscr{X}))\) into the space \(\mathscr{P}(S)\) of all probability measures on the action space \((\mathscr{A},\mathscr{B}(\mathscr{A}))\). The sigma-field on \(\mathscr{P}(S)\) is generally taken to be the Borel sigma-field under the topology of weak convergence (assuming \(\mathscr{A}\) is a metric space).

To avoid confusing notation, we will continue to denote a decision rule by the symbol \(d\), with or without subscripts or superscripts, whether the rule is a randomized one or not.

We have seen in Example 2.2 that no estimator of the population proportion \(\theta\), based on a random sample of size \(n\), has the smallest risk function on the whole parameter space. Indeed, the smallest risk at every point \(\theta\) is zero. This is true not just for the squared error loss function, but for any loss function \(L\) such that \(L(\theta,a)>0\) for all \(\theta\neq a\), and \(=0\) for \(\theta=a\) (Exercise 2.2). We must then relax our requirement for optimality. One reasonable requirement would seem to be the following. From now on we will assume that the loss function \(L\) is given, so that all comparisons among decision rules are based on the risk function \(R(\theta,d)=E_{\theta}L(\theta,d(\mathbf{X}))\) (\(\theta\in\Theta\)).

**Definition 2.3**.: A decision rule \(d\) is said to be _inadmissible_ if there exists a decision rule \(d_{1}\) such that

\[R(\theta,d_{1})\leq R(\theta,d)\qquad\text{for all }\theta\in\Theta, \tag{2.12}\]

_with strict inequality for a least one \(\theta\). A decision rule which is not inadmissible is said to be admissible._

It turns out, unfortunately, that although it is easy to establish the admissibility of such estimators as \(d_{2}\) (\(\mathbf{X}\)) \(\equiv 1/3\) in Example 2.2 (Exercise 2.4), it is not so easy to prove the admissibility of time honored estimators such as the sample proportion \(d_{1}\)! We will later introduce other notions of optimality, or restrictions which will rule out frivolous estimators such as \(d_{2}\).

### Notes and References

For basic notions of loss and risk functions in testing (probabilities of Type 1 and Type 2 errors) and estimation (expected squared error) and admissibility one may refer to Bickel and Doksum (2001), Sect. 1.3, or Ferguson (1967), Sects. 1.3, 2.1.

### Ex. 2.1.

In Example 2.1, prove (2.4) and (2.6). Also with the loss function as \(L(\theta\),\(\mathbf{a})=(\mu-a_{1})^{2}+(\sigma-\sqrt{a_{2}}\,)^{2}\), instead of (2.5), compute the risk function of the estimator \(d(\mathbf{X})=(\overline{X},s^{2})\).

(a) Prove (2.7).

(b) In Example 2.2, with the loss function as specified, show that \(\min_{d}\{R(\theta,\)\(d(\mathbf{X}))\}=0\)\(\forall\,\theta\), where the minimum is taken over all decision rules.

(c) Justify the corresponding statement for any loss function for which \(L(\theta,a)>0\) if \(\theta\neq a\), \(L(\theta,\theta)=0\).

(a) Prove (2.10) in Example 2.3.

(a) Show that (a) the estimator \(d_{2}(\mathbf{X})\equiv\frac{1}{3}\) in Example 2.2 is admissible, and

(b) the estimator \(d_{3}(\mathbf{X})=\frac{X_{1}+\cdots+X_{n-1}}{n-1}\) is inadmissible (\(n>1\)).

(c) Find the range of values of \(\theta\) over which \(R(\theta,d_{2})\leq R(\theta,d_{3})\), and show that as \(n\) increases this interval converges to the singleton \(\{\frac{1}{3}\}\).

(a) Graph the two error probabilities in (2.10) in the case \(n=20\).

(a) Express the two error probabilities in (2.11) in terms of the parameters (and the sample sizes \(m,n\)).

## References

* Bickel & Doksum (2001) Bickel, P. J., & Doksum, K. (2001). _Mathematical statistics_ (2nd ed.). Englewood Cliffs, NJ: Prentice Hall.
* Ferguson (1967) Ferguson, T. (1967). _Mathematical statistics: A decision theoretic approach._ Boston: Academic.

## Chapter 3 Introduction to General Methods of Estimation

### 3.1 The Maximum Likelihood Estimator

Perhaps the most important estimator is statistics is the maximum likelihood estimator, originally used by Gauss for estimating the parameters of a Normal distribution \(N(\mu,\sigma^{2})\) in connection with his astronomical observations. The method in its general form is due to the British statistician R.A. Fisher who introduced it in the early part of the twentieth century. Fisher is widely regarded as the father of modern statistical theory.

**Definition 3.1**.: Let \(f(\mathbf{x}|\theta)\) denote the density of the distribution of the observation (vector) \(\mathbf{X}\), with respect to some sigma-finite measure \(\nu\). For example, \(\nu\) may be Lebesgue measure on \(\mathscr{X}=R^{n}\), in which case \(f\) is the _classical density_ of \(\mathbf{X}\), or \(\nu\) may be the counting measure on a countable observation space \(\mathscr{X}\) such as \(\{0,1\}^{n}\), in which case \(f\) is called a _probability mass function (pmf)_. The function \(\ell(\theta)=f(\mathbf{X}|\theta)\), \(\theta\in\Theta\), is called the _likelihood function_. The _maximum likelihood estimator_ (**MLE**) of \(\theta\) is the value of \(\theta\) where the likelihood function attains its maximum, assuming the existence of a unique such point in \(\Theta\).

One may argue (and we will make this precise in connection with the notion of _sufficiency_ later) that all the statistical information about the true parameter value in the (sample) observation \(\mathbf{X}\) is contained in the likelihood function \(\ell(\theta)\). The magnificent intuition of Gauss and Fisher is that the proper estimator of the true parameter value is the one that maximizes the likelihood--it is the value of the parameter which makes the given observation \(\mathbf{X}\) as the most likely to occur [You may think of this as a rather vain point of view!]_Example 3.1 (\(N(\mu,\sigma^{2})\))._ In the Normal example, Example 1.1, the density is with respect to Lebesgue measure \(\nu\) on \(R^{n}\). The _likelihood function_ is given by (with \(\theta=(\mu,\sigma^{2})\in\Theta=R\times(0,\infty)\))

\[\ell(\theta) =(2\pi\sigma^{2})^{-n/2}\exp\bigg{\{}-\frac{1}{2}\sum(X_{i}-\mu)^ {2}/\sigma^{2}\bigg{\}}\] \[=(2\pi\sigma^{2})^{-n/2}\exp\bigg{\{}-\bigg{(}\frac{1}{2\sigma^{2 }}\bigg{)}\left[\sum(X_{i}-\overline{X})^{2}+n(\overline{X}-\mu)^{2}\right] \bigg{\}}. \tag{3.1}\]

The maximizer of \(\ell\) maximizes the strictly increasing function of \(\ell\) given by the _log-likelihood function_

\[\ln\,\ell(\mu,\sigma^{2})=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\,\ln(\sigma^{2})- \bigg{(}\frac{1}{2\sigma^{2}}\bigg{)}\left[\sum(X_{i}-\overline{X})^{2}+n( \overline{X}-\mu)^{2}\right]. \tag{3.2}\]

Setting the derivative of this with respect to \(\mu\) to zero yields the MLE \(\widehat{\mu}\) of \(\mu\):

\[0=\frac{\partial}{\partial\mu}\ln\ell=\left(\frac{n}{\sigma^{2}}\right)( \overline{X}-\mu),\qquad\text{or }\widehat{\mu}=\overline{X}. \tag{3.3}\]

Also, differentiation with respect to \(\sigma^{2}\) (using the solution for \(\mu\)) yields

\[0=\left(-\frac{n}{2\sigma^{2}}\right)+\left(\frac{1}{2\sigma^{4}}\right)\sum(X _{i}-\overline{X})^{2},\qquad\text{or }\widehat{\sigma}^{2}=\sum\frac{(X_{i}- \overline{X})^{2}}{n}. \tag{3.4}\]

Thus the MLE of \(\theta=(\mu,\sigma^{2})\) is given by \((\overline{X},\sum(X_{i}-\overline{X})^{2}/n)\). One may easily check that the matrix of second derivatives of \(\ell(\theta)\) is negative-definite (Exercise 3.1), so that the above solution is the unique maximizer of the likelihood function. Equations (3.3), (3.4) yielding this solution are called _likelihood equations_.

_Example 3.2 (Bernoulli \((\theta)\))._ Consider Example 2.2 of Chap. 2.1, on estimating the proportion \(\theta\), based on a random sample of \(n\) i.i.d. Bernoulli random variables with probabilities \(\theta\) and \(1-\theta\) for values \(1\) and \(0\), respectively. Here we take \(\Theta=(0,1)\) and consider the density \(f(\mathbf{x}|\theta)\) with respect to the counting measure \(\nu\) on \(\{0,1\}^{n}\). The likelihood and the log-likelihood functions are

\[\ell(\theta)=\theta^{\sum X_{i}}(1-\theta)^{\sum(1-X_{i})},\quad\ln\ell( \theta)=\left(\sum X_{i}\right)\ln\theta+\left(n-\sum X_{i}\right)\ln(1-\theta). \tag{3.5}\]

The likelihood equation is then

\[0=\frac{\partial\ln\ell(\theta)}{\partial\theta}=\frac{1}{\theta }\,\sum X_{i}-\frac{n-\sum X_{i}}{1-\theta}, \text{or }(1-\theta)\sum X_{i}-\theta(n-\sum X_{i})=0,\] \[\text{or }\sum X_{i}-n\theta=0, \tag{3.6}\]

which has the solution \(\widehat{\theta}=\sum X_{i}/n\). Hence the MLE of the population proportion \(\theta\) is the sample proportion of \(1\)'s. You may check that the second derivative of the log-likelihood function is negative (Exercise 3.1).

_Example 3.3 (\(U(0,\theta)\))._ Let \(\mathbf{X}=(X_{1},\ldots,X_{n})\) be a random sample from the uniform distribution on the interval \((0,\theta]\) where \(\theta\in\Theta=(0,\infty)\). Here the density \(f(\mathbf{x}|\theta)=\prod_{1\leq i\leq n}(1/\theta)\mathbf{1}\)\(\{0<x_{i}\leq\theta\}\), \(\mathbf{x}=(x_{1},\ldots,x_{n})\), is with respect to Lebesgue measure \(\nu\) on \((0,\infty)^{n}\). The likelihood function is

\[\ell(\theta) =\frac{1}{\theta^{n}}\mathbf{1}_{\{X_{i}\leq\theta,\,1\leq i\leq n \}}, \tag{3.7}\] \[\quad\text{or}\;\;\ell(\theta)=\theta^{-n}\mathbf{1}\left\{\theta \geq M_{n}\equiv\max(X_{1},\ldots,X_{n})\right\},\quad\theta\in(0,\infty).\]

Here \(\mathbf{1}\{\ldots\}\) denotes the indicator function of the set \(\{\ldots\}\). Since the likelihood function has the value zero for \(\theta<M_{n}\), and decreases monotonically as \(\theta\) increases from \(M_{n}\) to infinity, its maximum is attained at \(\theta=M_{n}\). Thus the MLE of \(\theta\) is \(M_{n}=\max(X_{i}:i=1,\ldots,n)\). Note that the maximum here occurs at a point where \(\ell(\theta)\) is discontinuous (Exercise 3.2).

### Method of Moments

Classically, in order to estimate an \(r\)-dimensional parameter \(\theta=(\theta_{1},\ldots,\theta_{r})\) by the method of moments, one equates the first \(r\) population moments with the corresponding \(r\) sample moments. More generally, one may equate \(r\) functionally independent population averages, say, \(E_{\theta}g_{j}(X_{1})\), \(1\leq j\leq r\), with the corresponding sample averages \((1/n)\sum_{1\leq i\leq n}g_{j}(X_{i})\), \(1\leq j\leq r\), to solve for \(\theta_{j}\), \(j=1,\ldots,r\). Although the MLE can be generally shown to be superior to these estimates, the latter are sometimes a lot easier to compute and, may be used as initial solutions in an iterative process to numerically compute the MLE (Exercise 3.11). In Examples 3.1, 3.2, however, the classical method of moments, of equating as many sample moments with population moments as there are parameters, yield the same estimates as the MLE.

_Example 3.4 (Gamma \(\mathscr{G}(\alpha,\beta)\))._ Suppose the observation vector \(\mathbf{X}\) comprises \(n\) i.i.d. random variables \(X_{i}\) (\(i=1,\ldots,n\)) each with the _gamma density_ (with respect to Lebesgue measure on \((0,\infty)\)) given by

\[\gamma(x;\alpha,\beta)=(\alpha^{\beta}\varGamma(\beta))^{-1}x^{\beta-1}e^{-x/ \alpha},\qquad x>0,\;(\alpha>0,\beta>0). \tag{3.8}\]

The first two moments of the gamma distribution are easily computed as (Exercise 3.3)

\[E(X_{1})=\alpha\beta,\qquad E(X_{1}^{2})=\alpha^{2}\beta(\beta+1), \tag{3.9}\]

which, when equated with the corresponding sample moments \(m_{1}=\sum X_{i}/n\) and \(m_{2}=\sum X_{i}^{2}/n\), yield the solutions

\[\widetilde{\alpha}=\frac{(m_{2}-m_{1}^{2})}{m_{1}},\qquad\widetilde{\beta}= \frac{m_{1}}{\alpha}=\frac{m_{1}^{2}}{(m_{2}-m_{1}^{2})}, \tag{3.10}\]

that is, the right sides of the two equations in (3.10) are the method-of-moment estimates of \(\alpha\) and \(\beta\).

### Bayes Rules and Bayes Estimators

Sometimes the statistician has prior information about the true parameter value. This information does not generally specify a particular value, but, instead, is given as a probability distribution \(\tau\) on \(\Theta\), called _the prior distribution,_ or simply the _prior._ This distribution gives more weight to those values which, according to the statistician, are more likely to be true compared to other values. Following the same line of thought, if the statistician does not have any particular preference for any value, then a prior to use may be the uniform distribution (provided the parameter space admits one). We will not here get into the philosophical viewpoint of a _Bayesian._ Indeed, in this course our approach may appear closer to that of the so-called _frequentist,_ although that is not entirely intentional. Hopefully this apparent bias will be corrected in a follow up course. We remind the reader once again that we fix an arbitrarily chosen loss function \(L(\theta,a)\), which may satisfy certain general properties to be specified as needed.

Given a prior \(\tau\), the statistician computes the _Bayes risk_\(r(\tau,d)\) of a decision rule \(d\) given by

\[r(\tau,d)=\int_{\Theta}R(\theta,d)d\tau(\theta). \tag{3.11}\]

**Definition 3.2**.: A _Bayes rule_\(d_{0}\) is a decision rule, if one exists, which has the smallest Bayes risk among all decision rules:

\[r(\tau,d_{0})=\inf r(\tau,d), \tag{3.12}\]

where the infimum is over all decision rules \(d\). In the case of an estimation problem, a Bayes rule is called a _Bayes estimator._

Observe that the Bayes risk of a decision rule \(d\) (with respect to a prior \(\tau\)) may be expressed as

\[r(\tau,d)=\int_{\Theta}R(\theta,d)d\tau(\theta)=\int_{\Theta}\left\{\int_{ \mathscr{X}}L(\theta,d(\mathbf{x}))dP_{\theta}(\mathbf{x})\right\}d\tau( \theta). \tag{3.13}\]

Here \(P_{\theta}\) is defined canonically as the distribution of the observation \(\mathbf{X}\) on \(\mathscr{X}\) when \(\theta\) is the true parameter value. Suppose we define on (a possibly enlarged probability space \((\Omega,\mathscr{F},P)\)) a random variable \(\vartheta\) whose distribution is \(\tau\), and a random variable \(X\) (with values in \(\mathscr{X}\)) _whose conditional distribution, given \(\vartheta=\theta\), is \(P_{\theta}\)._ Indeed, one can define this space canonically as \(\Omega=\mathscr{X}\times\Theta\) with the product sigma-field, and with the probability measure \(P\) specified by

\[P(C\times D)=\int_{D}P_{\theta}(C)d\tau(\theta)\quad\text{for all }C\in\mathscr{B }(\mathscr{X})\text{ and }D\in\mathscr{B}(\Theta). \tag{3.14}\]

Denoting expectation with respect to \(P\) as \(E\), (3.13) then may be expressed as

\[r(\tau,d_{0})=EL(\vartheta,d(\mathbf{X})). \tag{3.15}\]

Note that (3.13) computes this expectation by (1) first taking the conditional expectation given \(\vartheta=\theta\) (the inner integral in (3.13)) and (2) then integrating thisconditional expectation with respect to the (marginal) distribution of \(\vartheta\) (i.e., with respect to the prior \(\tau\)). That is, (3.13) says

\[r(\tau,d)=EL(\vartheta,d(\mathbf{X}))=E[E(L(\vartheta,d(\mathbf{X}))\mid\vartheta )]. \tag{3.16}\]

One may reverse the order of integration, by first taking the conditional expectation, given \(\mathbf{X}\), and then integrating this conditional expectation over the (marginal) distribution of \(\mathbf{X}\):

\[r(\tau,d)=EL(\vartheta,d(\mathbf{X}))=E[E(L(\vartheta,d(\mathbf{X}))\mid \mathbf{X})]. \tag{3.17}\]

The conditional expectation in (3.17) is obtained by integrating \(L(\theta,d(\mathbf{x}))\) with respect to the conditional distribution of \(\vartheta\), given \(\mathbf{X}=\mathbf{x}\). This is the so-called _posterior distribution of \(\vartheta\)_, denoted \(d\tau(\theta|\mathbf{x})\). Next this conditional expectation is integrated with respect to the _marginal distribution_\(G\) of \(\mathbf{X}\), which has the density (with respect to \(\nu\)) given by

\[g(\mathbf{x})=\int_{\Theta}f(\mathbf{x}|\theta)d\tau(\theta),\qquad[dG(\mathbf{ x})=g(\mathbf{x})d\nu(\mathbf{x})]. \tag{3.18}\]

Hence

\[r(\tau,d)=\int\left[\int L(\theta,d(\mathbf{x}))d\tau(\theta\mid x)\right]g( \mathbf{x})d\nu(\mathbf{x}) \tag{3.19}\]

If the prior \(\tau\) has a density \(t(\theta)\) with respect to some sigma-finite measure \(\lambda\) (for example, \(\lambda\) may be Lebesgue measure on \(\Theta\)), then the _conditional density_ of \(\vartheta\), given \(\mathbf{X}=\mathbf{x}\), that is, the posterior density, is given by

\[t(\theta\mid\mathbf{x})=\frac{t(\theta)f(\mathbf{x}\mid\theta)}{g(\mathbf{x})},\qquad[d\tau(\theta\mid\mathbf{x})=t(\theta\mid\mathbf{x})d\lambda(\mathbf{x })]. \tag{3.20}\]

One may think of the posterior distribution as the updating of the prior given the observed data \(\mathbf{X}\). In the Bayesian paradigm, all inference is to be based on the posterior.

**Theorem 3.1**.: _Let \(\tau\) be a given prior with a finite second moment. In the problem of estimating the parameter \(\boldsymbol{\theta}\) belonging to a (measurable) set \(\Theta\subset R^{k}\), with the action space \(\mathscr{A}\) a (measurable) convex subset of \(\mathbb{R}\) containing \(\Theta\), and under squared error loss, the posterior mean of \(\vartheta\) is a Bayes estimator of \(\boldsymbol{\theta}\)._

To prove this we first need a simple lemma.

**Lemma 3.1**.: _Let \(\mathbf{Z}\) be a random vector with mean \(\boldsymbol{\mu}\) and a finite second moment. Then_

\[E|\mathbf{Z}-\boldsymbol{\mu}|^{2}<E|\mathbf{Z}-\mathbf{c}|^{2}\qquad\text{ for every }\mathbf{c}\neq\boldsymbol{\mu}. \tag{3.21}\]

Proof.: Let \(\mathbf{Z}=(Z_{1},\ldots,Z_{k})\), \(\boldsymbol{\mu}=(\mu_{1},\ldots,u_{k})\). Then (3.21) follows from the relations

\[E(Z_{i}-c_{i})^{2}\equiv E(Z_{i}-\mu_{i}+\mu_{i}-c_{i})^{2} =E(Z_{i}-\mu_{i})^{2}+(\mu_{i}-c_{i})^{2}+2(\mu_{i}-c_{i})E(Z_{i}- \mu_{i})\] \[=E(Z_{i}-\mu_{i})^{2}+(\mu_{i}-c_{i})^{2}\qquad(i=1,\ldots,k).\]Proof of Theorem 3.1.: The posterior mean of \(\vartheta\) is \(E(\vartheta\mid{\bf X})=d_{0}({\bf X})\), say. If \(d\) is any other decision rule (estimator), then one has, by applying the Lemma to the conditional distribution of \(\vartheta\), given \({\bf X}\),

\[E(L(\vartheta,d({\bf X}))\mid{\bf X}) \equiv E(|\vartheta-d({\bf X})|^{2}\mid{\bf X})\geq E(|\vartheta-d_{0}({ \bf X})|^{2}\mid{\bf X})\] \[\equiv E(L(\vartheta,d_{0}({\bf X}))\mid{\bf X}). \tag{3.22}\]

Hence

\[r(\tau,d) = E(L(\vartheta,d({\bf X}))=E[E(L(\vartheta,d({\bf X}))\mid{\bf X} ]\geq E[E(L(\vartheta,d_{0}({\bf X}))\mid{\bf X})] \tag{3.23}\] \[= E(L(\vartheta,d_{0}({\bf X}))=r(\tau,d_{0}).\]

_Remark 3.1_.: The convexity of \(\mathscr{A}\) ensures that \(d_{0}(X)\in\mathscr{A}\) a.s. The conclusion of Theorem 3.1 and its proof apply to any (measurable) real or vector valued function \(g(\theta)\) of \(\theta\) having a finite second moment under the given prior \(\tau\):

**Proposition 3.1**.: _If the action space \(\mathscr{A}\) is a (measurable) convex set \(C\), containing the range of \(g\), then under squared error loss \(L(\theta,{\bf a})=|g(\theta)-{\bf a}|^{2}\), \(E(g(\vartheta)\mid{\bf X})\) is a Bayes estimator of \(g(\theta)\)._

**Theorem 3.2**.: _Let \(g(\theta)\) be a real-valued measurable function on \(\Theta\) having a finite absolute first moment under the prior \(\tau\). Let the action space \(\mathscr{A}\) be an interval containing the range of \(g\). Under absolute error loss \(L(\theta,a)=|g(\theta)-a|\), a (the) median of the posterior distribution of \(\vartheta\) (i.e., the conditional distribution of \(\vartheta\), given \({\bf X}\)) is a Bayes estimator of \(\theta\)._

_Remark 3.2_.: Recall that a _median_ of a probability measure \(Q\) on \(R\) is a number \(M\) such that

\[Q((-\infty,M])\equiv Q(\{x\leq M\})\geq\frac{1}{2},\quad\text{and }Q([M,\infty))\equiv Q(\{x\geq M\})\geq\frac{1}{2}. \tag{3.24}\]

Unlike the mean, a median of a distribution need not be unique. There are two cases in which the median is unique: (1) There is a _unique_\(M\) such that \(F_{Q}(M)=\frac{1}{2}\), where \(F_{Q}\) is the (right-continuous) distribution function of \(Q\), and (2) there is a (necessarily unique) point \(M\) such that \(F_{Q}(M)>\frac{1}{2}\), and \(F_{Q}(M-)<\frac{1}{2}\). In the remaining case, \(F_{Q}\) has a flat stretch of \(x\)-values where it has the value \(\frac{1}{2}\). This may be either of the form (iii) \([M_{0},M_{1})\) (with a jump discontinuity at \(M_{1}\)), or of the form (iv) \([M_{0},M_{1}]\), \(M_{0}<M_{1}\). Such an interval comprises the set of medians of \(Q\), illustrating the case of _non-uniqueness_.

We will need the following Lemma to prove Theorem 3.2.

**Lemma 3.2**.: _Let \(Z\) be a real-valued random variable with a finite mean. If \(M\) is a median of the distribution \(Q\) of \(Z\), then_

\[\inf\{E|Z-c|:c\in\mathbb{R}\}=E|Z-M|. \tag{3.25}\]Proof: Let \(M\) be a median of \(Q\), and \(a<M\). Then

\[E|Z-a| = E\left[(Z-a){\bf 1}_{\{Z\geq a\}}\right]+E\left[(a-Z){\bf 1}_{\{Z<a\}}\right]\] \[= E\left[(Z-M){\bf 1}_{\{Z\geq a\}}+(M-a){\bf 1}_{\{Z\geq a\}}+(a-M) {\bf 1}_{\{Z<a\}}+(M-Z){\bf 1}_{\{Z<a\}}\right]\] \[= E\left[(Z-M)({\bf 1}_{\{Z\geq M\}}+{\bf 1}_{\{a\leq Z<M\}})+(M-Z)({ \bf 1}_{\{Z<M\}}-{\bf 1}_{\{a\leq Z<M\}})\right]\] \[\qquad\qquad\qquad\qquad+(M-a)[P(Z\geq a)-P(Z<a)]\] \[= E|Z-M|-2E[(M-Z){\bf 1}_{\{a\leq Z<M\}}]+(M-a)[2P(Z\geq a)-1]\] \[\geq E|Z-M|-2(M-a)P(a\leq Z<M)+(M-a)[2P(Z\geq a)-1]\] \[= E|Z-M|+2(M-a)[P(Z\geq a)-P(a\leq Z<M)]-(M-a)\] \[= E|Z-M|+2(M-a)P(Z\geq M)-(M-a)\geq E|Z-M|\]

using \(P(Z\geq M)\geq\frac{1}{2}\) in the last step. Similarly, one can show that for \(a>M\) one has \(E|Z-a|\geq E|Z-M|\). Else one may use the fact that \(-M\) is a median of the distribution of \(-Z\). Hence if \(a>M\), \(-a<-M\), so that the above argument shows that \(E|-Z-(-a)|\geq E|-Z-(-M)|\). 

Proof of Theorem 3.2.: This follows by first taking conditional expectation, given \({\bf X}:E[L(\vartheta,d({\bf X}))\mid{\bf X}]\equiv E[(|g(\vartheta)-d({\bf X} )|)|{\bf X}]\geq E[(|g(\vartheta)-d_{0}({\bf X})|)\mid{\bf X}]\equiv E[L( \vartheta,d_{0}({\bf X}))\mid{\bf X}]\), where \(d_{0}({\bf X})\) is a (the) median of the conditional distribution of \(g(\vartheta)\), given \({\bf X}\). Integrating this over the marginal distribution of \({\bf X}\), the proof is completed. 

Remark 3.3: In the case of a vector parameter \(\boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{k})\), if one uses the additive loss function, \(L(\boldsymbol{\theta},{\bf a})=|\theta_{1}-a_{1}|+\cdots+|\theta_{k}-a_{k}|\), then it follows by Lemma 3.1 that the vector of medians of the coordinates of \(\vartheta\), given \({\bf X}\), is a Bayes estimator of \(\boldsymbol{\theta}\).

Example 3.5: Let \(\Theta=[0,1]=\mathscr{A}\), \(L(\theta,a)=c(\theta-a)^{2}\) (where \(c>0\) does not depend on \(\theta\)), \(\mathscr{X}=\{0,1\}^{n}\equiv\) set of all \(n\)-tuples of \(0\)'s and \(1\)'s (the observation space for a random sample of size \(n\) from a Bernoulli distribution \(\mathscr{B}(\theta)\)), and

\[f({\bf x}\mid\theta) = P_{\theta}(\{{\bf X}={\bf x}\})=\theta^{r}(1-\theta)^{n-r},\quad {\bf x}=(x_{1},\ldots,x_{n})\in\mathscr{X},\] \[r := \sum_{i=1}^{n}x_{i}\quad\mbox{is the number of $1$'s in the sample.} \tag{3.26}\]

We wish to compute the Bayes estimator of \(\theta\) for the prior \(\tau\) with density \(t\) (with respect to Lebesgue measure on \(\Theta=[0,1]\)) given by

\[d\tau(\theta)=t(\theta)d\theta,\qquad t(\theta)=\frac{\Gamma(\alpha+\beta)}{ \Gamma(\alpha)\Gamma(\beta)}\ \theta^{\alpha-1}(1-\theta)^{\beta-1} \tag{3.27}\]

where \(\alpha>0\), \(\beta>0\) are parameters of this _beta distribution_\(\mathscr{B}_{e}(\alpha,\beta)\). Note that (See the _Appendix on Univariate Distributions_)

\[\int_{0}^{1}\theta^{\alpha-1}(1-\theta)^{\beta-1}d\theta=\frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)}\,. \tag{3.28}\]

The mean of the \(\mathscr{B}_{e}(\alpha,\beta)\) distribution is

\[\int_{0}^{1}\theta t(\theta)d\theta=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)}\int_{0}^{1}\theta^{\alpha}(1-\theta)^{\beta-1}d\theta\]\[=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\cdot\frac{ \Gamma(\alpha+1)\Gamma(\beta)}{\Gamma(\alpha+\beta+1)}=\frac{\alpha}{\alpha+ \beta}\,, \tag{3.29}\]

using (3.28) and the _identity:_\(\Gamma(\alpha+1)=\alpha\Gamma(\alpha)\) for all \(\alpha>0\). The _posterior density_ or \(\vartheta\) (i.e., _the conditional density of \(\vartheta\), given_ (\(\mathbf{X}=\mathbf{x}\)) is computed as in (3.20),

\[t(\theta\mid\mathbf{x}) =\frac{t(\theta)f(\mathbf{x}\mid\theta)}{g(\mathbf{x})}=\frac{c( \alpha,\beta)}{g(\mathbf{x})}\,\theta^{\alpha+r-1}(1-\theta)^{\beta+n-r-1}\] \[\left[c(\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)}\right]\qquad(r=\sum_{i}^{n}x_{i}), \tag{3.30}\]

where the marginal density (w.r.t. counting measure on \(\mathscr{X}\)) \(g(\mathbf{x})\) is given by (see (3.18))

\[g(\mathbf{x})=\int_{\boldsymbol{\Theta}}f(\mathbf{x}\mid\theta)t(\theta)d\theta. \tag{3.31}\]

Although we can surely compute \(g(\mathbf{x})\) from (3.31), note that a simpler way is to recognize that (3.30) gives a probability density function (in \(\theta\)) for every given \(\mathbf{x}\) and, therefore, must integrate out to \(1\). Then

\[g(\mathbf{x})=c(\alpha,\beta)\int_{0}^{1}\!\!\theta^{\alpha+r-1}(1-\theta)^{ \beta+n-r-1}d\theta=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, \frac{\Gamma(\alpha+\beta+n)}{\Gamma(\alpha+r)\Gamma(\beta+n-r)}. \tag{3.32}\]

But even this computation is unnecessary! For the functional form (3.30), as a density in \(\theta\), shows that the _posterior distribution of \(\vartheta\) is \(\mathscr{B}_{\epsilon}(\alpha+r,\beta+n-r)\)_. Hence the _Bayes estimator of \(\theta\),_ namely, the mean of its (or \(\vartheta\)'s) posterior distribution is, by (3.29),

\[d_{0}(\mathbf{x})=E(\mathscr{O}\mid\mathbf{X}=\mathbf{x})=\frac{r+\alpha}{n+ \alpha+\beta}\qquad\bigg{(}r=\sum_{i=1}^{n}x_{i}\bigg{)}. \tag{3.33}\]

If \(n\) is large (and \(\alpha\), \(\beta\) are relatively small), then this is not significantly different from the traditional (maximum likelihood) estimator \(\hat{\theta}=\overline{x}\equiv\sum_{i=1}^{n}x_{i}/n\) (the _sample proportion_ of \(1\)'s).

To compute the Bayes risks of \(d_{0}\) and \(\hat{\theta}\) (Exercise 3.4), first compute the second moment of a \(\mathscr{B}_{\epsilon}(\alpha,\beta)\) distribution as

\[\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\,\int_{0 }^{1}\theta^{2}\theta^{\alpha-1}(1-\theta)^{\beta-1}d\theta=\frac{\Gamma( \alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\cdot\frac{\Gamma(\alpha+2)\Gamma( \beta)}{\Gamma(\alpha+\beta+2)}\] \[= \frac{\alpha(\alpha+1)}{(\alpha+\beta)(\alpha+\beta+1)}. \tag{3.34}\]

_Example 3.6_.: Let \(\Theta=\mathbb{R}=\mathscr{A}\), \(\mathscr{X}=\mathbb{R}^{n}=\Omega\), \(P_{\theta}\) has density \(f(\mathbf{x}\mid\theta)\) (w.r.t. Lebesgue measure on \(\mathbb{R}^{n}\)), given by

\[f(\mathbf{x}\mid\theta) =(2\pi\sigma^{2})^{-n/2}\exp\left\{-\sum_{i=1}^{n}(x_{i}-\theta )^{2}/2\sigma^{2}\right\},\] \[\mathbf{x}=(x_{1},\ldots,x_{n})\in\mathbb{R}^{n}. \tag{3.35}\]Here \(\sigma^{2}>0\)_is assumed to be known_. Finally, let \(L(\theta,a)=c(\theta-a)^{2}\), with \(c>0\) not dependent on \(\theta\). We will construct the Bayes estimator of \(\theta\) for the Normal prior \(\tau\) with density (w.r.t. Lebesgue measure on \(\Theta=\mathbb{R}\)) \(t(\cdot)\) given by

\[t(\theta)=(2\pi\beta^{2})^{-\frac{1}{2}}\exp\left\{-\frac{\theta^{2}}{2\beta^{ 2}}\right\},\qquad\theta\in\mathbb{R}, \tag{3.36}\]

where \(\beta>0\) is a given constant. The _posterior density_ (w.r.t. Lebesgue measure) of \(\vartheta\) given \(\mathbf{X}\) is

\[t(\theta\mid\mathbf{x}) =\frac{t(\theta)f(\mathbf{x}\mid\theta)}{g(\mathbf{x})}=\frac{(2 \pi\beta^{2})^{-\frac{1}{2}}(2\pi\sigma^{2})^{-\frac{n}{2}}\exp\left\{-\frac{ \theta^{2}}{2\beta^{2}}-\frac{1}{2\sigma^{2}}\sum(x_{i}-\theta)^{2}\right\}}{ g(\mathbf{x})}\] \[=c_{1}(\beta,\sigma^{2},\mathbf{x})\exp\left\{-\frac{\theta^{2}} {2\beta^{2}}-\frac{n(\theta-\overline{x})^{2}}{2\sigma^{2}}\right\}\] \[=c_{2}(\beta,\sigma^{2},\mathbf{x})\exp\left\{-\left(\frac{1}{2 \beta^{2}}+\frac{n}{2\sigma^{2}}\right)\theta^{2}+\frac{n\overline{x}}{\sigma ^{2}}\,\theta\right\} \tag{3.37}\] \[=c_{3}(\beta,\sigma^{2},\mathbf{x})\exp\left\{-\frac{1}{2}\left( \frac{n\beta^{2}+\sigma^{2}}{\beta^{2}\sigma^{2}}\right)\left(\theta-\frac{n \beta^{2}}{n\beta^{2}+\sigma^{2}}\,\overline{x}\right)^{2}\right\}\quad\text{ on }\mathbf{X}=\mathbf{x},\]

where \(c_{i}(\beta,\sigma^{2},\mathbf{x})\) (\(i=1,2,3\)) do not involve \(\theta\). Hence the posterior distribution \(d\tau(\theta\mid\mathbf{x})\) is Normal \(N\left(\frac{n\beta^{2}\,\overline{x}}{n\beta^{2}+\sigma^{2}}\,,\,\frac{\beta ^{2}\sigma^{2}}{n\beta^{2}+\sigma^{2}}\right)\), with mean \([n\beta^{2}/(n\beta^{2}+\sigma^{2})]\overline{x}\). Therefore, the Bayes estimator of \(\theta\) is

\[d_{0}(\mathbf{x})=\frac{n\beta^{2}}{n\beta^{2}+\sigma^{2}}\,\,\overline{x}. \tag{3.38}\]

In this example \(d_{0}(\mathbf{X})\) is also the _median_ of the posterior, in view of the symmetry of the Normal distribution. Hence \(d_{0}\) in (3.38) is also the Bayes estimator under the loss function \(L(\theta,a)=|\theta-a|\).

This example is easily extended to \(k\)-dimensional i.i.d. \(N(\boldsymbol{\theta},I_{k})\) observations, where \(\boldsymbol{\theta}\in\mathbb{R}^{k}\), and \(I_{k}\) is the \(k\times k\) identity matrix. The prior distribution of \(\boldsymbol{\theta}\) is taken to be that of independent \(N(0,\beta_{i}^{2})\) random variables (\(i=1,\ldots,k\)) (Exercise 3.10).

A parametric family of priors is said to be _conjugate_ if, for every prior \(\tau\) in the family, the posterior distribution belongs to the same family (\(\forall\,\mathbf{x}\in\mathscr{X}\)). Examples 3.5, 3.6 and Exercise 3.5 above provide examples of such priors.

Bayes estimators are, under mild restrictions, admissible. Our next couple of results make this precise.

**Definition 3.3**.: A Bayes rule \(d_{0}\) (w.r.t. a prior \(\tau\), and a given loss function) is said to be _unique up to equivalence_ if for any other Bayes rule \(d_{1}\) (w.r.t. \(\tau\)) one has \(R(\theta,d_{0})=R(\theta,d_{1})\)\(\forall\,\theta\in\Theta\).

**Theorem 3.3**.: _If, for a given prior \(\tau\), a Bayes rule is unique up to equivalence, then it is admissible._

Proof.: Let a Bayes rule \(d_{0}\) be unique up to equivalence, and suppose, if possible, \(d_{0}\) is inadmissible. Then there exists a decision rule \(d_{1}\) satisfying (i) \(R(\theta,d_{1})\leq R(\theta,d_{0})\)\(\forall\,\theta\in\Theta\), and (ii) \(R(\theta_{1},d_{1})<R(\theta_{1},d_{0})\) for some \(\theta_{1}\in\Theta\). By integrating the first inequality (i) w.r.t. \(\tau\), one gets \(r(\tau,d_{1})\leq r(\tau,d_{0})\). But \(d_{0}\) has the smallestpossible Bayes risk among all decision rules. Hence \(r(\tau,d_{1})=r(\tau,d_{0})\), implying \(d_{1}\) is a Bayes rule. By the hypothesis of uniqueness, one then has \(R(\theta,d_{1})=R(\theta,d_{0})\ \forall\,\theta\in\Theta\), contradicting inequality (ii). 

Often the following stronger uniqueness holds for Bayes rules.

**Definition 3.4**.: A Bayes rule \(d_{0}\) is said to be _unique_ if for every Bayes rule \(d_{1}\) (w.r.t. the same prior \(\tau\)) one has

\[P_{\theta}(d_{1}(\mathbf{X})=d_{0}(\mathbf{X}))=1\qquad\forall\ \theta\in\Theta. \tag{3.39}\]

It is clear that (3.39) implies \(R(\theta,d_{1})=R(\theta,d_{0})\ \forall\ \theta\in\Theta\).

_Example 3.7_.: Let \(\Theta=[0,1]=\mathscr{A}\), \(L(\theta,a)=c(\theta-a)^{2}\) (\(c>0\) does not depend on \(\theta\)), \(\mathscr{X}=\{0,1\}^{n}\),

\[f(\mathbf{x}\mid\theta)\equiv P_{\theta}(\{\mathbf{x}\})_{=}\theta^{r}(1- \theta)^{n-r}\qquad(r=\sum_{1}^{n}x_{i},\ \mathbf{x}=(x_{1},\ldots,x_{n})).\]

In this example, let \(\tau\) be a prior assigning all its mass to \(\{0,1\}\) (\(\tau(\{0,1\})=1\)), say, \(\tau(\{0\})=p\), \(\tau(\{1\})=q=1-p\). Assume \(n\geq 2\). Consider the estimators

\[d_{0}(\mathbf{X})\equiv\overline{X},\qquad d_{1}(\mathbf{X})\equiv X_{1}. \tag{3.40}\]

Then

\[R(\theta,d_{0}) =\frac{c\theta(1-\theta)}{n}\,,\quad R(\theta,d_{1})=c\theta(1- \theta),\] \[r(\tau,d_{0}) =\int_{\Theta}R(\theta,d_{0})d\tau(\theta)=R(0,d_{0})p+R(1,d_{0}) q=0,\] \[r(\tau,d_{1}) =R(0,d_{1})p+R(1,d_{1})q=0.\]

For \(P_{0}(X_{i}=0)=1\), \(P_{1}(X_{i}=1)=1\ \forall\,i=1,2,\ldots,n\), and \(P_{0}(\overline{X}_{i}=0)=1\), \(P_{1}(\overline{X}=1)=1\). Thus \(d_{0}\) and \(d_{1}\) are both Bayes estimators. But \(d_{1}\) is clearly inadmissible, since for all \(0<\theta<1\), \(R(\theta,d_{0})<R(\theta,d_{1})\), while \(R(\theta,d_{0})=R(\theta,d_{1})\) for \(\theta=0,1\).

To rule out situations like this, one may require a property such as \(P_{1}\) below.

_Property \(P_{1}\)_ There exists a subset \(\widetilde{\Theta}\) of \(\Theta\) such that (1) \(\tau(\widetilde{\Theta})>0\) and (2) if \(P_{\theta_{0}}(A)=0\) for some \(\theta_{0}\in\widetilde{\Theta}\) (and for some event \(A\subset\mathscr{X}\)), then \(P_{\theta}(A)=0\) for all \(\theta\in\Theta\).

Note that the prior \(\tau\) in Example 3.7 does not satisfy \(P_{1}\).

**Theorem 3.4**.: _In a decision problem with loss \(c(\theta-a)^{2}\) (\(c>0\) independent of \(\theta\)), suppose a prior \(\tau\) has the property \(P_{1}\). Then a Bayes estimator w.r.t. \(\tau\) is unique, assuming \(r(\tau,d_{0})<\infty\)._

Proof.: A Bayes rule w.r.t. to \(\tau\) is the posterior mean \(d_{0}(\mathbf{X})=E(\vartheta\mid\mathbf{X})\). Write

\[r(\tau,d_{0})=c\int_{\mathscr{X}}v(\mathbf{x})dG(\mathbf{x}),\qquad v(\mathbf{ x}):=E[(\vartheta-d_{0}(\mathbf{x}))^{2}\mid\mathbf{X}=\mathbf{x}],\]where \(G\) is the (marginal) distribution of \(\mathbf{X}\). Now suppose \(d_{1}\) is another Bayes estimator w.r.t. \(\tau\), and

\[r(\tau,d_{1})=c\int_{\mathscr{K}}v_{1}(\mathbf{x})dG(\mathbf{x}),\qquad v_{1}( \mathbf{x}):=E\left[(\vartheta-d_{1}(\mathbf{x}))^{2}\mid\mathbf{X}=\mathbf{x }\right].\]

Let \(A=\{\mathbf{x}\in\mathscr{K}:d_{0}(\mathbf{x})\neq d_{1}(\mathbf{x})\}\). Then

\[r(\tau,d_{1}) =c\int_{A}v_{1}(\mathbf{x})dG(\mathbf{x})+c\int_{A^{c}}v_{1}( \mathbf{x})dG(\mathbf{x}),\] \[r(\tau,d_{0}) =c\int_{A}v(\mathbf{x})dG(\mathbf{x})+c\int_{A^{c}}v(\mathbf{x})dG (\mathbf{x}).\]

Since \(r(\tau,d_{1})=r(\tau,d_{0})\), it follows that \(\int_{A}(v_{1}(\mathbf{x})-v(\mathbf{x}))dG(\mathbf{x})=0\). But on \(A\), \(v_{1}(\mathbf{x})>v(\mathbf{x})\). Therefore, \(G(A)=0\). This implies

\[0=G(A)=\int_{\Theta}P_{\theta}(A)d\tau(\theta)\geq\int_{\widetilde{\Theta}}P_ {\theta}(A)d\tau(\theta),\]

so that the last integral must be zero. Hence \(P_{\theta}(A)=0\) a.e. (w.r.t. \(\tau\)) on \(\widetilde{\Theta}\). But \(\tau(\widetilde{\Theta})>0\). Therefore, there exists \(\theta_{0}\in\widetilde{\Theta}\) such that \(P_{\theta_{0}}(A)=0\). By property \(P_{1}\), one now gets \(P_{\theta}(A)=0\)\(\forall\,\theta\in\Theta\). Thus uniqueness holds. 

Remark 4: The above argument extends to the absolute error loss \(|\theta-a|\), provided the posterior distribution has a unique median a.s. for all \(x\). This is the case, e.g., when the prior has a strictly positive density on \(\Theta\).

Corollary 1: _Under the hypothesis of Theorem 3.1, a Bayes estimator is admissible._

We next turn to the (uneasy) relationship between Bayes estimators and unbiased estimators. Recall \(d(\mathbf{X})\) is an _unbiased estimator of a parametric function_\(g(\theta)\) if

\[E_{\theta}d(\mathbf{X})=g(\theta)\qquad\forall\ \theta\in\Theta.\]

Theorem 3.2: _Let loss be proportional to squared error. Then the Bayes estimator \(d(\mathbf{X})=E(\theta\mid\mathbf{X})\) (with a finite Bayes risk) is not unbiased, if \(r(\tau,d)>0\)._

Proof: Suppose, if possible, that \(d(\mathbf{X})\) is unbiased. Then

\[E\vartheta d(\mathbf{X}) =E[\vartheta E(d(\mathbf{X})\mid\vartheta)]=E\vartheta\vartheta= E\vartheta^{2},\quad(\text{by unbiasedness of }d),\] \[E\vartheta d(\mathbf{X}) =E[d(\mathbf{X})E(\vartheta\mid\mathbf{X})]=Ed(\mathbf{X})d( \mathbf{X})=Ed^{2}(\mathbf{X}).\]

On the other hand,

\[0<r(\tau,d)=E[\vartheta-d(\mathbf{X})]^{2}=E\vartheta^{2}+Ed^{2}(\mathbf{X})-2 E\vartheta d(\mathbf{X})=0,\]

a contradiction. 

Remark 5: In the proof of Theorem 3.1 the nature of the loss function is only used to require that \(\nu_{1}(\mathbf{x})>\nu(\mathbf{x})\)\(\forall\ \mathbf{x}\in\{d_{0}(\mathbf{x})\neq d_{1}(\mathbf{x})\}\). Thus Theorem 3.1 and Corollary 1 hold under every loss function such that \(E[L(\mathscr{O},\mathbf{X})\mid\mathbf{X}=\mathbf{x}]\) has a unique minimizer for every \(\mathbf{x}\).

### Minimax Decision Rules

A conservative statistician may try to avoid maximum penalty by choosing a decision rule \(d^{*}\) whose risk function \(R(\theta,d^{*})\) has the _smallest maximum value_.

Definition 3.5: A decision rule \(d^{*}\) is _minimax_ for a decision problem (specified by \(\Theta\), \(\mathscr{A}\), \(L(\theta,a)\), \(\mathscr{X}\), and \(P_{\theta}\)\(\forall\,\theta\in\Theta\)), if for every decision rule \(d\) one has

\[\sup_{\theta\in\Theta}R(\theta,d^{*})\leq\sup_{\theta\in\Theta}R(\theta,d) \tag{3.41}\]

or, equivalently, if

\[\sup_{\theta\in\Theta}R(\theta,d^{*})=\inf_{d}\sup_{\theta\in\Theta}R(\theta,d), \tag{3.42}\]

where the infimum on the right side is over the class of all decision rules \(d\).

Theorem 3.6: _Suppose \(\tau_{N}\)\((N=1,2,\dots)\) is a sequence of priors with corresponding Bayes rules \(d_{N}\) such that \(r(\tau_{N},d_{N})\to C<\infty\), as \(N\to\infty\). If there exists a decision rule \(d^{*}\) such that_

\[R(\theta,d^{*})\leq C\qquad\forall\ \theta\in\Theta, \tag{3.43}\]

_then \(d^{*}\) is minimax._

Proof: If \(d^{*}\) is not minimax, there exists \(d\) such that \(\sup_{\theta\in\Theta}R(\theta,d)<\sup_{\theta\in\Theta}R(\theta,d^{*})\ \leq C\). Let \(\epsilon>0\) be such that \(R(\theta,d)\leq C-\epsilon\)\(\forall\,\theta\in\Theta\). Then \(r(\tau_{N},d)\equiv\ \int_{\Theta}R(\theta,d)d\tau_{N}(\theta)\ \leq\ C-\epsilon\) for all \(N\), and \(r(\tau_{N},d)<r(\tau_{N},d_{N})\) for all sufficiently large \(N\) (since \(r(\tau_{N},d_{N})\to C\) as \(N\to\infty\)). This contradicts the fact that \(d_{N}\) is Bayes (w.r.t. \(\tau_{N}\)) for every \(N\). 

Example 3.8: Let \(\Theta=\mathbb{R}=\mathscr{A}\), \(\mathscr{X}=\mathbb{R}^{n}\), \(P_{\theta}\) (on \(\Omega=\mathscr{X}\)) has the density

\[f(\mathbf{x}\mid\theta)=(2\pi\sigma^{2})^{-n/2}\exp\left\{-\frac{1}{2\sigma^{ 2}}\sum_{i=1}^{n}(x_{i}-\theta)^{2}\right\}.\]

Let the loss function be \(L(\theta,a)=c(\theta-a)^{2}\)\((c>0)\). Consider the rule \(d^{*}(\mathbf{x})=\overline{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\). For each \(N=1,2,\dots\), consider the prior \(\tau_{N}\) which is \(N(0,N)\) (Normal with mean \(0\) and variance \(N\)). Then the Bayes estimator \(d_{N}\) for the prior \(\tau_{N}\) is (See Example 3.6) is \(d_{N}(\mathbf{x})=\frac{nN\overline{x}}{nN+\sigma^{2}}\) with the corresponding Bayes risk

\[r(\tau_{N},d_{N}) =cE(\vartheta-d_{N}(\mathbf{X}))^{2}=cE[E(\vartheta-d_{N}( \mathbf{X}))^{2}\mid\mathbf{X})]\] \[=cE\left(\frac{N\sigma^{2}}{Nn+\sigma^{2}}\right)=c\frac{N\sigma ^{2}}{Nn+\sigma^{2}}\longrightarrow\frac{c\sigma^{2}}{n}\quad\text{as }N \to\infty.\]

Since \(R(\theta,d^{*})=cE_{\theta}(\overline{X}-\theta)^{2}=c\frac{\sigma^{2}}{n}\), Theorem 3.6 applies, and \(d^{*}\) is minimax.

Theorem 3.7: _Suppose \(d^{*}\) is a decision rule whose risk function is a constant \(c^{\prime}\),_

\[R(\theta,d^{*})=c^{\prime}\qquad\forall\ \theta\in\Theta. \tag{3.44}\]

_If, in addition, (i) there exists some prior \(\tau\) such that \(d^{*}\) is Bayes w.r.t. to \(\tau\), or (ii) \(d^{*}\) is admissible, then \(d^{*}\) is minimax._Proof: Let \(d\) be any other decision rule. Then, if (i) holds,

\[\sup_{\theta\in\Theta}R(\theta,d) \geq\int_{\Theta}R(\theta,d)d\tau(\theta)\equiv r(\tau,d)\geq r( \tau,d^{*})\] \[=\int_{\Theta}R(\theta,d^{*})d\tau(\theta)=c^{\prime}=\sup_{ \theta\in\Theta}R(\theta,d^{*}).\]

Suppose now that (ii) holds, and \(d^{*}\) is not minimax. Then there exists \(d_{1}\) such that

\[\sup_{\theta\in\Theta}R(\theta,d_{1})<R(\theta,d^{*})=c^{\prime}\ \forall\ \theta.\]

This means \(d^{*}\) is not admissible, a contradiction. 

Note that condition (1) in Theorem 3.7 does not necessarily imply condition (2) without some condition such as \(P_{1}\) ensuring the uniqueness of the Bayes rule for \(\tau\).

Example 3.9 (Admissibility of the Sample Proportion): Consider Example 3.7 (or Exercise 3.4), but with \(\Theta=(0,1)\), \(\mathscr{A}=[0,1]\), and \(L(\theta,a)=(\theta-a)^{2}/\theta(1-\theta)\). The decision rule \(d^{*}=\overline{x}\) has constant risk

\[R(\theta,d^{*})=E_{\theta}(\theta-\overline{X})^{2}/\theta(1-\theta)=\frac{1}{ n}\qquad\forall\ \theta\in\Theta. \tag{3.45}\]

We will show that \(d^{*}\) is also Bayes with respect to the beta prior \(\tau=\mathscr{B}_{e}(1,1)\). For any decision rule \(d\), the Bayes risk w.r.t. the uniform prior \(\tau\) is

\[r(\tau,d) =\int_{0}^{1}\frac{E_{\theta}(\theta-d(\mathbf{X}))^{2}}{\theta(1 -\theta)}\,d\theta=\int_{0}^{1}\sum_{\mathbf{x}\in\{0,1\}^{n}}\frac{(\theta-d( \mathbf{x}))^{2}}{\theta(1-\theta)}\,\theta^{r}(1-\theta)^{n-r}d\theta\] \[=\sum_{\mathbf{x}\in\{0,1\}^{n}}\int_{0}^{1}(\theta-d(\mathbf{x}) )^{2}\theta^{r-1}(1-\theta)^{n-r-1}d\theta. \tag{3.46}\]

If \(r=0\), the integral is infinite, unless \(d(\mathbf{x})=0\), and if \(r=n\) the integral is infinite unless \(d(\mathbf{x})=1\). Hence a Bayes rule \(d_{0}\) must have the values

\[d_{0}(\mathbf{x})=\cases{0\ \text{if}\ $\mathbf{x}=(0,0,\ldots,0)$,\cr 1\ \text{if}\ $\mathbf{x}=(1,1,\ldots,1)$.} \tag{3.47}\]

For every other \(\mathbf{x}\), the summand in (3.46) may be expressed as

\[\frac{\Gamma(r)\Gamma(n-r)}{\Gamma(n)}\int_{0}^{1}(\theta-d(\mathbf{x}))^{2}b_ {r,n-r}(\theta)d\theta, \tag{3.48}\]

where \(b_{r,n-r}\) is the density of the beta distribution \(\mathscr{B}_{e}(r,n-r)\). The integral in (3.48) is then \(E(\vartheta-d(\mathbf{X}))^{2}\), where \(\vartheta\) has the \(\mathscr{B}_{e}(r,n-r)\) distribution, and is therefore minimum when \(d(\mathbf{x})=E\vartheta=\frac{r}{n}=\overline{x}\). Thus the (unique) Bayes estimator is \(d^{*}=\overline{x}\). In particular, \(d^{*}\) is _minimax_ (w.r.t. the loss \(\frac{(\theta-a)^{2}}{\theta(1-\theta)}\)), by Theorem 3.7 and admissible (by Theorem 3.3). Admissibility w.r.t. the loss function \(L(\theta,a)=(\theta-a)^{2}/[\theta(1-\theta)]\) means that there does not exist any decision rule \(d\) such that

\[R(\theta,d)\equiv\frac{E_{\theta}(\theta-d(\mathbf{X}))^{2}}{\theta(1-\theta) }\leq\frac{E_{\theta}(\theta-\overline{X})^{2}}{\theta(1-\theta)}\equiv R( \theta,d^{*})\quad\forall\ \theta\in(0,1), \tag{3.49}\]with strict inequality for some \(\theta\in(0,1)\). Canceling out \(\theta(1-\theta)\) from both sides, this implies that \(d^{*}(\mathbf{x})\equiv\overline{x}\)_is admissible w.r.t. squared error loss \(L(\theta,a)=(\theta-a)^{2}\)_, as well as w.r.t. the loss \([\theta(1-\theta)]^{-1}(\theta-a)^{2}\).

### Generalized Bayes Rules and the James-Stein Estimator

1. _Improper Priors and Generalized Bayes Rules._ If the (prior) weight measure \(\tau\) is allowed to be an arbitrary non-zero sigma-finite measure on \(\Theta\), it is called an _improper prior_ in case \(\tau(\Theta)=\infty\). Whether \(\tau\) is finite or not, a decision rule \(d_{0}(\mathbf{x})\) which minimizes \[a\longrightarrow\int_{\Theta}L(\theta,a)f(\mathbf{x}\mid\theta)d\tau(\theta)\] (3.50) for every \(\mathbf{x}\) (over all \(a\in\mathscr{A}\)) is called a _generalized Bayes rule_ for the possibly improper prior \(\tau\). If \(\tau\) is finite, then this minimizer is the same as that w.r.t. the (normalized) prior \(\tau/\tau(\Theta)\).

_Example 3.10_ (\(\mathbf{N}(\theta,\sigma^{2})\)).: Consider the problem of estimating the Normal mean \(\theta\), with \(\sigma^{2}\) known, discussed in Examples 3.6, 3.8. If \(\tau\) is the Lebesgue measure on \(\Theta=\mathbb{R}\), then the integral (3.50) equals

\[(2\pi\sigma^{2})^{-n/2}\int_{-\infty}^{\infty}(\theta-a)^{2}e^{- \frac{1}{2\sigma^{2}}\sum_{1}^{n}(x_{i}-\theta)^{2}}d\theta\] \[=\frac{(2\pi\sigma^{2})^{-(n-1)/2}}{\sqrt{n}}\,e^{-\frac{1}{2 \sigma^{2}}\sum_{1}^{n}(x_{i}-\overline{x})^{2}}\int_{-\infty}^{\infty}( \theta-a)^{2}\frac{\sqrt{n}}{\sqrt{2\pi\sigma^{2}}}\,e^{-\frac{n}{2\sigma^{2} }(\overline{x}-\theta)^{2}}d\theta \tag{3.51}\]

The last integral is the expected value of \((\vartheta-a)^{2}\) where \(\vartheta\) is \(N(\overline{x},\frac{\sigma^{2}}{n})\). Hence the minimum is attained at \(a=\overline{x}\), and \(d_{0}(\mathbf{x})=\overline{x}\) is the generalized Bayes estimator for \(\tau\).

We will apply the following theorem to prove that \(d_{0}(\mathbf{x})=\overline{x}\) is admissible as an estimator of \(\theta\) in this example.

We continue to use the notation \(r(\tau,d)\) as in (3.11), for improper priors as well as proper priors.

**Theorem 3.8** (Blyth's Method).: _Suppose that \(\theta\to R(\theta,d)\) is finite and continuous for every decision rule \(d\) for which \(R(\theta^{\prime},d)<\infty\) for some \(\theta^{\prime}\in\Theta\). Let \(d_{0}\) be a decision rule with a finite risk function. Assume there exists a sequence of proper or improper priors \(\tau_{N}\)\((N=1,2,\dots)\) with the following properties: (i) for every nonempty open subset \(\Theta_{0}\) of \(\Theta\) there exist \(N_{0}\) and \(b_{0}>0\) such that \(\tau_{N}(\Theta_{0})\geq b_{0}\)\(\forall\)\(N\geq N_{0}\), and (ii) \(r(\tau_{N},d_{0})-r(\tau_{N},d_{N})\to 0\) as \(N\to\infty\), where \(d_{N}\) is a generalized Bayes rule for \(\tau_{N}\)._

_Then \(d_{0}\) is admissible._

Proof.: Suppose \(d_{0}\) is inadmissible. Then there exists a decision rule \(d\), \(\theta_{0}\in\Theta\) and \(\varepsilon>0\) such that \(R(\theta,d)\leq R(\theta,d_{0})\)\(\forall\)\(\theta\), \(R(\theta_{0},d)<R(\theta_{0},d_{0})-\varepsilon\). By continuity of risk functions, there exists an open subset \(\Theta_{0}\) of \(\Theta\), with \(\theta_{0}\in\Theta_{0}\), for which \(R(\theta,d)<R(\theta,d_{0})-\varepsilon\;\forall\,\theta\in\Theta_{0}\). By (i) there exist \(N_{0}\) and \(b_{0}>0\) so that \(\tau_{N}(\Theta_{0})\geq b_{0}\;\forall\,N\geq N_{0}\). Hence

\[r(\tau_{N},d_{0})-r(\tau_{N},d)>\varepsilon\tau_{N}(\Theta_{0})\geq\varepsilon b _{0}>0\qquad\forall\,\,N\geq N_{0}.\]

This contradicts (ii), since \(r(\tau_{N},d)\geq r(\tau_{N},d_{N})\). 

_Example 3.11_ (Admissibility of the Mean of a Sample from \(N(\theta,\sigma^{2})\)).: In context of the Normal example above, let \(\tau_{N}=\sqrt{N}\,\mathbf{N}(0,N)\). To check condition (i) in Theorem 3.8, let \(\Theta_{0}\) be a nonempty open subset of \(\Theta=\mathbb{R}\). There exist \(\theta_{0}<\theta_{1}\) such that \((\theta_{0},\theta_{1})\subset\Theta_{0}\). Now

\[\tau_{N}(\Theta_{0}) \geq\tau_{N}((\theta_{0},\theta_{1}))=\sqrt{N}\int_{\theta_{0}}^{ \theta_{1}}\frac{1}{\sqrt{2\pi N}}\,e^{-\theta^{2}/2N}d\theta\] \[=\frac{1}{\sqrt{2\pi}}\int_{\theta_{0}}^{\theta_{1}}e^{-\theta^{2 }/2N}d\theta\longrightarrow\frac{1}{\sqrt{2\pi}}(\theta_{1}-\theta_{0})>0\quad \text{as }N\to\infty, \tag{3.52}\]

from which (i) follows. Also, \(r(\tau_{N},\overline{x})=\int R(\theta,\overline{x})d\tau_{N}(\theta)=\sqrt{N} \,\frac{\sigma^{2}}{n}\), \(r(\tau_{N},d_{N})=\sqrt{N}\left(\frac{N\sigma^{2}}{nN+\sigma^{2}}\right)\) (See Example 3.6, with \(\beta^{2}=N\)), so that

\[r(\tau_{N},\overline{x})-r(\tau_{N},d_{N})=\sqrt{N}\left(\frac{nN+\sigma^{2}- Nn}{n(nN+\sigma^{2})}\right)\sigma^{2}=\frac{\sqrt{N}}{n(nN+\sigma^{2})}\, \sigma^{4}\longrightarrow 0, \tag{3.53}\]

as \(N\to\infty\), proving condition (ii) of Theorem 3.8, and establishing the admissibility of \(\overline{x}\).

_Remark 3.6_.: If one tries to extend the proof to the \(k\)-dimensional Normal distribution \(\mathbf{N}(\boldsymbol{\theta},I)\) (where \(\boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{k})\in\Theta=\mathbb{R}^{k}\), and \(I\) the \(k\times k\) identity matrix), by letting \(\tau_{N}\) be the product measure \(\sqrt{n}\,\mathbf{N}(0,N)\times\cdots\times\sqrt{n}\,\mathbf{N}(0,N)\), then condition (i) of Theorem 3.8 holds as in (3.52) (with \((\sqrt{N}\,)^{k}\) canceling out from the numerator and denominator). However, condition (ii) breaks down, since \(r(\tau_{N},\overline{\mathbf{x}})-r(\tau_{N},d_{N})=(\sqrt{N}\,)^{k/2}k/\{n(nN +1)\}\), which does not go to zero as \(N\to\infty\), if \(k\geq 2\). It may be shown that \(\overline{\mathbf{x}}\) is admissible for the case \(k=2\). _However, \(\overline{\mathbf{x}}\) is inadmissible for \(k\geq 3\)_, a fact first discovered by Charles Stein in 1956 and which came as a big shock to most statisticians. We give below a proof of this inadmissibility due to James and Stein (1961) by showing that the so-called _James-Stein estimator_

\[d^{JS}(\mathbf{x}):=\left(1-\frac{(k-2)\sigma^{2}}{n|\overline{\mathbf{x}}|^{2 }}\right)\overline{\mathbf{x}} \tag{3.54}\]

is uniformly better than \(\overline{\mathbf{x}}\) for \(k\geq 3\), when the underlying distribution \(P_{\boldsymbol{\theta}}\) is \(\mathbf{N}(\boldsymbol{\theta},\sigma^{2}\mathbf{I})\) for some known \(\sigma^{2}>0\), and the loss function is squared error \(L(\boldsymbol{\theta},\mathbf{a})=|\boldsymbol{\theta}-\mathbf{a}|^{2}=\sum_{j =1}^{k}|\theta_{j}-a_{j}|^{2}\) (\(\boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{k})\), \(\mathbf{a}=(a_{1},\ldots,a_{k})\in\mathbb{R}^{k}\)).

**Theorem 3.9**.: _One has_

\[R(\theta,d^{JS})=k\,\frac{\sigma^{2}}{n}-(k-2)^{2}\,\frac{\sigma^{2}}{n}\,E_{ \boldsymbol{\theta}}\left(\frac{1}{|Y|^{2}}\right),\qquad(k\geq 3), \tag{3.55}\]

_where \(Y\) is \(N(\sqrt{n}\;\boldsymbol{\theta}/\sigma,I)\)._We need two auxiliary properties of the Normal distribution.

Lemma 3.3: Let \(g\) be a real-valued differentiable function on \(\mathbb{R}\) such that \(E|g^{\prime}(X)|<\infty\) where \(X\) is \(N(\theta,1)\), and assume \(g(x)\varphi(x-\theta)\to 0\) as \(|x|\to\infty\). Then \(Eg^{\prime}(X)=\text{cov}(X,g(X))\).

Demonstration Proof: Integrating by parts, and denoting the standard normal density by \(\varphi\),

\[Eg^{\prime}(X)=\int_{-\infty}^{\infty}g^{\prime}(x)\varphi(x-\theta)dx=\int_{ -\infty}^{\infty}g(x)(x-\theta)\varphi(x-\theta)dx\]

Lemma 3.4: Let \(\mathbf{g}=(g_{1},g_{2},\ldots,g_{k})\) be differentiable on \(\mathbb{R}^{k}\) into \(\mathbb{R}^{k}\). Let \(\mathbf{X}=(X_{1}\), \(\ldots\), \(X_{k})\) have the distribution \(N(\boldsymbol{\theta},I)\), \(\boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{k})\in\mathbb{R}^{k}\), \(I\)\(k\times k\) identity matrix. Assume that \(E|\mathbf{g}(\mathbf{X})|^{2}<\infty\) and define \(h_{j}(y)=E(g_{j}(\mathbf{X})|X_{j})_{X_{j}=y}=Eg_{j}(X_{1},\ldots,X_{j-1},y,X_ {j+1}\), \(\ldots\), \(X_{k})\). Assume that \(h_{j}\) satisfies the hypothesis of Lemma 3.3 (in place of \(g\) there), \(1\leq j\leq k\). Then

\[E|\mathbf{X}+\mathbf{g}(\mathbf{X})-\boldsymbol{\theta}|^{2}=k+E\left(| \mathbf{g}(\mathbf{X})|^{2}+2\sum_{j=1}^{k}\frac{\partial}{\partial x_{j}}\,g_ {j}(\mathbf{x})\mid_{\mathbf{x}=\mathbf{X}}\right).\]

Demonstration Proof: The left side equals

\[E|\mathbf{X}-\boldsymbol{\theta}|^{2}+E|\mathbf{g}(\mathbf{X})|^{2}+2E( \mathbf{X}-\boldsymbol{\theta})\cdot\mathbf{g}(\mathbf{X})=k+E|\mathbf{g}( \mathbf{X})|^{2}+2\sum_{j=1}^{k}E(X_{j}-\theta_{j})g_{j}(\mathbf{X}).\]

Now \(E(X_{j}-\theta_{j})g_{j}(\mathbf{X})=E[(X_{j}-\theta_{j})\cdot E(g_{j}( \mathbf{X})|X_{j})]=E(X_{j}-\theta_{j})h_{j}(X_{j})\). Apply Lemma 3.3 (with \(g=h_{j}\)) to get \(E(X_{j}-\theta_{j})h_{j}(X_{j})=Eh_{j}^{\prime}(X_{j})=E\left[\left(\frac{ \partial}{\partial x_{j}}\,g_{j}(\mathbf{x})\right)_{\mathbf{x}=\mathbf{X}}\right]\). 

Demonstration Proof of Theorem 3.9: Since \(\overline{\mathbf{X}}\) is distributed as \(\mathbf{N}\left(\boldsymbol{\theta},\frac{\sigma^{2}}{n}\,\mathbf{I}\right)\), by rescaling it as \(\frac{\sqrt{n}}{\sigma}\,\overline{\mathbf{X}}\) (which is distributed as \(\mathbf{N}(\boldsymbol{\gamma},\mathbf{I})\), with \(\boldsymbol{\gamma}=\frac{\sqrt{n}}{\sigma}\,\boldsymbol{\theta}\)), one may take \(n=1\), \(\sigma^{2}=1\) in the Theorem and write \(\mathbf{X}\) for \(\overline{\mathbf{X}}\). In this case write \(\mathbf{g}(\mathbf{x})=-\frac{(k-2)}{|\mathbf{x}|^{2}}\,\mathbf{x}\) to have (by (3.56))

\[E_{\theta}\left|d^{JS}(\mathbf{X})-\boldsymbol{\theta}\right|^{2}=E|\mathbf{X} +\mathbf{g}(\mathbf{X})-\boldsymbol{\theta}|^{2}=k+(k-2)^{2}E\left(\frac{1}{ |\mathbf{X}|^{2}}\right)+2E\left[\sum_{j=1}^{k}\frac{\partial}{\partial x_{j }}\,g_{j}(\mathbf{x})\mid_{\mathbf{x}=\mathbf{X}}\right].\]

Now \(\sum(\partial/\partial x_{j})g_{j}(\mathbf{x})=-(k-2)^{2}/|\mathbf{x}|^{2}\), so that

\[E_{\theta}\left|d^{JS}(\mathbf{X})-\boldsymbol{\theta}\right|^{2}=k-(k-2)^{2} E\left(\frac{1}{|\mathbf{X}|^{2}}\right),\qquad(k\geq 3).\]

Remark Remark 3.7: It has been shown that even \(d^{JS}\) is not admissible (James and Stein, 1961).

We conclude this chapter with a result which implies, in particular, that the sample mean \(\overline{X}\) is an admissible estimator of the population mean \(\theta\) of a Normal distribution \(N(\theta,\sigma^{2})\) when both \(\theta\) and \(\sigma^{2}>0\) are unknown parameters.

Theorem 3.10: _Let \(\Theta=\Theta_{1}\times\Theta_{2}\) and suppose a decision rule \(d\) is admissible for the parameter space\(\{(\theta_{1},\theta_{2}):\theta_{1}\in\Theta_{1}\}\) for every given value of \(\theta_{2}\in\Theta_{2}\). Then \(d\) is admissible when the parameter space is \(\Theta\)._

Proof: Suppose \(d\) is inadmissible when the parameter space is \(\Theta=\Theta_{1}\times\Theta_{2}\). Then there exists a decision rule \(d_{1}\) and a point \(\theta^{0}=(\theta^{0}_{1},\theta^{0}_{2})\) such that \(R(\theta,d_{1})\leq R(\theta,d)\ \forall\ \theta\in\Theta\) and \(R(\theta^{0},d_{1})<R(\theta^{0},d)\). But this implies \(R((\theta_{1},\theta^{0}_{2}),d_{1})\leq R((\theta_{1},\theta^{0}_{2}),d)\ \forall\ \theta_{1}\in\Theta_{1}\), \(R((\theta^{0}_{1},\theta^{0}_{2}),d_{1})<R((\theta^{0}_{1},\theta^{0}_{2}),d)\), contradicting the fact that \(d\) is admissible when the parameter space is \(\Theta_{1}\times\{\theta^{0}_{2}\}\). 

### Notes and References

For Bayes estimation we refer to Ferguson (1967, Sects. 1.8, 2.1-2.3), and Lehmann and Casella (1998, Chaps. 4 and 5).

### Exercises for Chap. 3

**Ex. 3.1.**:
* In Example 3.1, show that the solution \((\widehat{\mu},\widehat{\sigma}^{2})\) of the likelihood equations (3.3), (3.4) is the unique value of the parameter \(\theta=(\mu,\sigma^{2})\) which maximizes the likelihood function \(\ell(\theta)\).
* In Example 3.2, show that \(\hat{\theta}=\sum X_{i}/n\) is the unique maximizer of \(\ell(\theta)\).
* In Example 3.3,
* plot \(\ell(\theta)\) for \(n=4\), \(M_{n}=2\), and
* schematically draw the graph of \(\ell(\theta)\) for a general \(n\geq 2\).

**Ex. 3.3.**:
* Write down the likelihood function and the likelihood equation in Example 3.4. Note that no explicit solution is available for the MLE.
* Verify (3.9).

**Ex. 3.4.**: Calculate the Bayes risk \(r(\tau,d_{0})\) in Example 3.5, and compare this with the Bayes risk of \(\hat{\theta}\), namely,

\[r(\tau,\hat{\theta})=\int_{\Theta}R(\theta,\hat{\theta})t(\theta)d\theta=\int_ {0}^{1}\frac{\theta(1-\theta)}{n}\,t(\theta)d\theta. \tag{3.57}\]

**Ex. 3.5.**: Let the observation space be \(\mathscr{X}=\{0,1,\dots\}^{n}\equiv\mathbb{Z}_{+}^{n}\),

\[f(\mathbf{x}\mid\theta)=\prod_{i=1}^{n}e^{-\theta}\frac{\theta^{x_{i}}}{x_{i}!}\equiv e^{-n\theta}\frac{\theta^{\sum_{1}^{n}x_{i}}}{\prod_{i=1}^{n}x_{i}!} \qquad(\mathbf{x}\in\mathscr{X}) \tag{3.58}\]

where \(\theta\in\Theta=(0,\infty)\), \(\mathscr{A}=[0,\infty)\). Find the Bayes estimator of \(\theta\) (with squared error loss) for the prior \(\mathscr{G}(\alpha,\beta)\) (See definition in Example 3.4).

Ex. 3.6. Let \(\Theta=[0,1]=\mathscr{A}\), \(\mathscr{X}=\{0,1\}^{n}\), \(P_{\theta}(\{\mathbf{x}\})=\theta^{r}(1-\theta)^{n-r}\) (\(r=\sum_{1}^{n}x_{i}\)), \(L(\theta,a)=c(\theta-a)^{2}\) (\(c>0\)). By Example 3.5, the Bayes rule for the beta prior \(\mathscr{B}_{e}(\alpha,\beta)\) is \((r+\alpha)/(n+\alpha+\beta)\equiv d_{0}(\mathbf{X})\). Show that for \(\alpha=\beta=\sqrt{n}/2\), the risk function of the Bayes rule of \(d_{0}(\mathbf{X})\) is a constant (\(R(\theta,d_{0})=c/[4(\sqrt{n}+1)^{2}]\)\(\forall\)\(\theta\in\Theta\)), and conclude that

\[d^{*}(\mathbf{X}):=\frac{\sum_{i}^{n}X_{i}+\frac{\sqrt{n}}{2}}{n+\sqrt{n}} \equiv\frac{\overline{X}+\frac{1}{2\sqrt{n}}}{1+\frac{1}{\sqrt{n}}} \tag{3.59}\]

is minimax, as well as admissible.

Ex. 3.7. _Admissibility of the Sample Mean from a Poisson Distribution._ Let \(\Theta=(0,\infty)\), \(\mathscr{A}=[0,\infty)\), \(\mathscr{X}=\mathbb{Z}_{+}^{n}\equiv\{0,1,2,\dots\}^{n}=\Omega\),

\[P_{\theta}(\{\mathbf{x}\})=\prod_{i=1}^{n}e^{-\theta}\frac{\theta^{x_{i}}}{x_{ i}!}=e^{-n\theta}\frac{\theta\sum_{1}^{n}x_{i}}{\prod_{1}^{n}x_{i}!}\equiv f( \mathbf{x}\mid\theta).\]

Let \(L(\theta,a)=\frac{e^{\theta}}{\theta}(\theta-a)^{2}\) for (a), (b) below.

1. Find the Bayes estimator w.r.t. the prior \(\mathscr{G}(\alpha,\beta)\)(Gamma).
2. Show that \(\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\) is Bayes w.r.t. some prior \(\tau\), and admissible.
3. Show that \(\overline{X}\) is admissible _under squared error loss:_\(L(\theta,a)=(\theta-a)^{2}\).
4. Show that \(\overline{X}\) is minimax, w.r.t. loss function \(\frac{(\theta-a)^{2}}{\theta}\).

Ex. 3.8. Show that, under squared error loss, (a) \(\overline{\mathbf{X}}\) is an admissible estimator of \(\boldsymbol{\mu}\in\Theta_{1}=\mathbb{R}^{k}\) when the sample is from \(\mathbf{N}(\boldsymbol{\mu},\sigma^{2}\mathbf{I})\) with \(\boldsymbol{\mu}\), \(\sigma^{2}\) both unknown and \(k=1,2\), and that (b) \(\overline{\mathbf{X}}\) is inadmissible if \(k\geq 3\) (\(\Theta=\mathbb{R}^{k}\times(0,\infty)\)). [Hint: Assume the admissibility of the sample mean for \(k=1,2\), when \(\sigma^{2}\) is known, and use Theorem 3.10.]

Ex. 3.9. Let \(\overline{\mathbf{X}}\) be the mean of a random sample from \(N(\boldsymbol{\mu},\Sigma)\) when \(\boldsymbol{\mu}\in\mathbb{R}^{k}\equiv\Theta_{1}\), \(\Sigma\in\Theta_{2}\equiv\) set of all symmetric positive definite \(k\times k\) matrices. Let \(\Theta=\Theta_{1}\times\Theta_{2}\), \(\mathscr{A}=\Theta_{1}\), and let the loss function be squared error \(L(\theta,a)=|\boldsymbol{\mu}-\mathbf{a}|^{2}\).

1. Show that \(\overline{\mathbf{X}}\) is an admissible estimator of \(\boldsymbol{\mu}\) when the parameter space is restricted to \(\Theta_{1}\times\{\Sigma\}\) for any given \(\Sigma\in\Theta_{2}\), and \(k=1\) or \(2\).
2. Show that \(\overline{\mathbf{X}}\) is an admissible estimator of \(\boldsymbol{\mu}\) if \(k=1\) or \(2\) and when \(\boldsymbol{\mu}\) and \(\Sigma\) are both unknown, i.e., the parameter space is \(\mathbb{R}^{k}\times\Theta_{2}\).
3. _(Optional.)_ In the cases (a), (b) show that \(\overline{\mathbf{X}}\) is inadmissible if \(k\geq 3\). [Hint: Brownian motion with zero drift and arbitrary non-singular diffusion matrix \(\Gamma\) is recurrent iff \(k\leq 2\).]

Ex. 3.10. Extend Example 3.6 to obtain

1. the posterior distribution of the mean \(\boldsymbol{\theta}\) of the Normal distribution \(N(\boldsymbol{\theta},I_{k})\), based on a random sample \(\mathbf{X}_{1},\dots,\mathbf{X}_{n}\), when the prior distribution of \(\boldsymbol{\theta}\) is taken to be that of \(k\) independent Normal random variables, \(\theta_{i}\stackrel{{ d}}{{\sim}}N(0,\beta_{i}^{2})\) (\(i=1,\dots,k\)). Also,
2. compute the Bayes estimator of \(\boldsymbol{\theta}\) under squared error loss.

3. Compute the Bayes estimator of \(\boldsymbol{\theta}\) when the prior distribution is that of \(k\) independent Normal random variables \(\theta_{i}\stackrel{{ d}}{{\sim}}N(C_{i},\beta_{i}^{2})\) for some \(c_{i}\in\mathbb{R}\) and \(\beta_{i}^{2}>0\ \forall\ i\).

**Ex. 3.11**.: Consider a random sample \((X_{1},\ldots,X_{n})\) of size \(n=50\) from a gamma distribution \(\mathscr{G}(\alpha,\beta)\), with \(\overline{X}=4.5\), \(\frac{1}{n}\sum_{j=1}^{n}X_{j}^{2}=41\), and \(\frac{1}{n}\sum_{j=1}^{n}\log X_{j}=0.95\).

1. Find the method-of-moments estimates of \(\alpha,\beta\).
2. Use the estimates in (a) as the initial trial solution of the likelihood equations, and apply the Newton-Raphson, or the gradient method, to compute the MLEs \(\widehat{\alpha},\widehat{\beta}\), by iteration.

**Ex. 3.12**.: Consider \(\mathbf{X}=(X_{1},\ldots,X_{n})\) where \(X_{i}\)'s are i.i.d. \(N(\mu,\sigma^{2})\) with \(\mu\) known and \(\theta=\sigma^{2}>0\) is the unknown parameter. Let the prior \(\tau\) for \(\sigma^{2}\) be the _inverse gamma_\(\mathscr{G}(\alpha,\beta)\), i.e., \(1/\sigma^{2}\) has the gamma distribution \(\mathscr{G}(\alpha,\beta)\).

1. Compute the posterior distribution of \(\sigma^{2}\). [Hint: First compute the posterior distribution of \(1/\sigma^{2}\).]
2. Find the Bayes estimator of \(\sigma^{2}\) under squared error loss \(L(\sigma^{2},a)=(\sigma^{2}-a)^{2}\).

## References

* Ferguson (1967) Ferguson, T. (1967). _Mathematical statistics: A decision theoretic approach_. Boston: Academic.
* James & Stein (1961) James, W., & Stein, C. (1961). Estimation with quadratic loss. In _Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Berkeley Symposium on Mathematical Statistics and Probability_ (Vol. 1, pp. 361-379). University of California Press
* Lehmann & Casella (1998) Lehmann, E. L., & Casella, G. (1998). _Theory of point estimation_ (2nd ed.). New York: Springer.

## Chapter 4 Sufficient Statistics, Exponential Families, and Estimation

### 4.1 Sufficient Statistics and Unbiased Estimation

For simplicity, we assume \(\Omega=\mathscr{X}\), \(\mathbf{X}:\mathscr{X}\rightarrow\mathscr{X}\) the _identity map_. Let (\(\mathscr{T}\), \(\mathscr{B}(\mathscr{T})\)) be a measurable space and \(T:\Omega\rightarrow\mathscr{T}\equiv\mathscr{R}_{T}\) (range space of \(T\)) a measurable map. Then \(T\) is said to be a _statistic_. Generally one requires that \(T\) does not depend on unknown population parameters (i.e., it can be computed entirely based on the observation \(X\)). We will often write \(\mathscr{B}_{T}\) for \(\mathscr{B}(\mathscr{T})\) and \(\sigma(T)=T^{-1}(\mathscr{B}_{T})\equiv\{T^{-1}(B):B\in\mathscr{B}_{T}\}\).

**Definition 4.1**.: A statistic \(T\) is said to be _sufficient_ for a family \(\mathscr{P}\) of probability measures \(P\) on \(\mathscr{X}\) if the conditional distribution of the observation (vector) \(\mathbf{X}\), given \(T\), is the same for all \(P\in\mathscr{P}\). If we index \(\mathscr{P}\) as \(\{P_{\theta}:\theta\in\Theta\}\) where \(\Theta\) is an index (or parameter- ) set, one says \(T\) is _sufficient for \(\theta\)_ if the conditional distribution of \(\mathbf{X}\), given \(T\), does not depend on \(\theta\).

_Example 4.1_.: \(\Omega=\mathscr{X}=\{0,1\}^{n}\), \(P_{\theta}(\{\mathbf{x}\})\equiv f(\mathbf{x}\mid\theta)=\theta^{\sum x_{i}}(1 -\theta)^{n-\sum x_{i}}\), \(\theta\in\Theta=(0,1)\). Then \(T\equiv\sum_{1}^{n}X_{i}\) (i.e., \(T(\mathbf{x})=\sum_{i=1}^{n}x_{i}\ \forall\ \mathbf{x}\)) is a sufficient statistic for \(\theta\). To see this note that, for any _given_\(\mathbf{x}=\mathscr{X}\) and \(t\in\mathscr{B}_{T}=\{0,1,\ldots,n\}\),

\[P_{\theta}(\mathbf{X}=(x_{1},\ldots x_{n})\equiv\mathbf{x}\mid T =t)=\frac{P_{\theta}(X_{1}=x_{1},\ldots,X_{n}=x_{n}\ \&\ T=t)}{P_{\theta}(T=t)}\] \[= \left\{\begin{array}{ll}0&\mbox{if $T(\mathbf{x})\neq t$,}\\ \frac{\theta^{t}(1-\theta)^{n-t}}{\binom{n}{t}\theta^{t}(1-\theta)^{n-t}}\equiv \frac{1}{\binom{n}{t}}\ \mbox{if $T(\mathbf{x})=t$.}\end{array}\right. \tag{4.1}\]

For, \(T\) has the binomial distribution \(B(n,\theta)\), and (i) if \(T(\mathbf{x})\neq t\), the set \(\{X_{1}=x_{1},\ldots,X_{n}=x_{n},\)\(\&\)\(T(\mathbf{X})=t\}=\emptyset\), (ii) if \(T(\mathbf{x})=t\), then the set \(\{T=t\}\equiv\{y\in\mathscr{X}:T(y)=t\}\supset\{\mathbf{X}=\mathbf{x}\}\equiv\{ \mathbf{x}\}\), so that \(\{\mathbf{X}=\mathbf{x},T=t\}=\{\mathbf{X}=\mathbf{x}\}\).

Note that, equivalently, \(\overline{X}\)_is sufficient for \(\theta\)._

_Example 4.2_.: Let\(\mathscr{X}=\mathbb{R}^{n}=\Omega\), \(\theta\in\Theta=\mathbb{R}\), and \(P_{\theta}\) has density (with respect to Lebesgue measure on \(\mathbb{R}^{n}\))

\[f(\mathbf{x}\mid\theta)=(2\pi)^{-n/2}e^{-\frac{1}{2}\sum_{i=1}^{n}(x_{i}- \theta)^{2}},\qquad(\mathbf{x}\in\mathbb{R}^{n}). \tag{4.2}\]

In this case \(T=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\) is sufficient for \(\theta\). To see this consider the orthogonal transformation \(U:\mathbf{x}\rightarrow\mathbf{y}\) given by

\[y_{1}=\sum_{i=1}^{n}\frac{1}{\sqrt{n}}\,x_{i},\quad y_{j}=\sum_{i=1}^{n}c_{ij }x_{i}\qquad(2\leq j\leq n), \tag{4.3}\]

where the vectors \((c_{1j},c_{2j},\ldots,c_{nj})\), \(2\leq j\leq n\), are of unit length, orthogonal to \((1/\sqrt{n},1/\sqrt{n},\ldots,\)\(1/\sqrt{n}\,)\), and orthogonal to each other. Then \(\mathbf{Y}=U(\mathbf{X})\equiv(Y_{1},\ldots,Y_{n})\) has the distribution of \(n\) independent Normal random variables, with \(Y_{1}\) having the distribution \(N(\sqrt{n}\,\theta,1)\), while the distribution of each \(Y_{j}\) (\(2\leq j\leq n\)) is \(N(0,1)\). Therefore, the conditional distribution of \(\mathbf{Y}\), given \(Y_{1}=y_{1}\) is the distribution of \((y_{1},Y_{2},\ldots,Y_{n})\) with \(Y_{j}\)'s i.i.d. \(N(0,1)\), \(2\leq j\leq n\). Now note that \(\overline{X}=\frac{1}{\sqrt{n}}\,Y_{1}\). Hence the conditional distribution of \(\mathbf{Y}\), given \(\overline{X}=z\) is the distribution of \((\sqrt{n}\,z,Y_{2},\ldots,Y_{n})\). But \(\mathbf{X}=U^{-1}(\mathbf{Y})\) (\(=U^{\prime}(\mathbf{Y})\) if \(U\) is identified with the matrix of orthogonal rows in (4.3)). Hence the conditional distribution of \(\mathbf{X}\) given \(\overline{X}=z\) is the distribution of \(U^{-1}(\sqrt{n}\,z,Y_{2},\ldots,Y_{n})\) which does not depend on \(\theta\). Hence \(\overline{X}\) is sufficient for \(\theta\).

_Remark 4.1_.: _For statistical inference about \(\theta\)_ (or, \(P_{\theta}\)), \(\theta\in\Theta\), _it is enough to know the value of a sufficient statistic \(T\) for \(\theta\)_. For, given \(T=t\), one can _simulate_ the random variable \(\mathbf{X}^{0}=(X_{1}^{0},X_{2}^{0},\ldots,X_{n}^{0})\), say, whose distribution is the same as the conditional distribution, given \(T=t\). The (unconditional, or) marginal distribution of \(\mathbf{X}^{0}\) is then the same as the distribution \(P_{\theta}\) of \(\mathbf{X}\). In other words, given \(T\) the rest of the data contain no additional information about \(\theta\) that can not be gleaned from \(T\). (Exercise 4.1)

Since the conditional distribution of \(\mathbf{X}\), given a statistic \(T\), is not generally very easy to calculate the following criterion is very useful.

**Theorem 4.1** (The Factorization Theorem).: _Let \(\mathscr{P}=\{P_{\theta}:\theta\in\Theta\}\) be such that each \(P_{\theta}\) is absolutely continuous with respect to a sigma-finite measure \(\mu\) on \((\mathscr{X},\mathscr{B}(\mathscr{X}))\) with density \(f(\mathbf{x}\mid\theta)=(dP_{\theta}/d\mu)(\mathbf{x})\). Then \(T\) is sufficient for \(\mathscr{P}\) if and only if one has a factorization of \(f\) in the form_

\[f(\mathbf{x}\mid\theta)=g(T(\mathbf{x}),\theta)h(\mathbf{x})\qquad\text{ a.e. }(\mu), \tag{4.4}\]

_where, for each \(\theta\in\Theta\), \(t\to g(t,\theta)\) is a measurable nonnegative function on \(\mathscr{T}\), and \(h\) is a nonnegative measurable function on \(\mathscr{X}\) which does not depend on \(\theta\)._

Proof.: For a complete proof, see _Testing Statistical Hypothesis_ (2005), by E. Lehmann and J. Romano, pp. 43-44. We will give a proof for the discrete case (i.e., \(\mathscr{X}\) countable). Let \(f(\mathbf{x}\mid\theta)\) denote \(P_{\theta}(\{\mathbf{x}\})\). Suppose (4.4) holds. Withoutloss of generality, assume \(h(\mathbf{x})>0\)\(\forall\)\(\mathbf{x}\in\mathscr{X}\). For, if \(h(\mathbf{x})=0\), then (4.4) implies \(f(\mathbf{x}\mid\theta)=0\)\(\forall\)\(\theta\). Hence the set \(\{\mathbf{x}\in\mathscr{X}:h(\mathbf{x})=0\}\) may be removed from \(\mathscr{X}\). Then the distribution of \(T\) (under \(P_{\theta}\)) is given by

\[P_{\theta}(T=t)=\sum_{\{\mathbf{x}\in\mathscr{X},T(\mathbf{x})=t\}}f(\mathbf{x} \mid\theta)=g(t,\theta)h_{1}(t), \tag{4.5}\]

where \(h_{1}(t)=\sum_{\{\mathbf{x}\in\mathscr{X}:T(\mathbf{x})=t\}}h(\mathbf{x})\). Then

\[P_{\theta}(\mathbf{X}=\mathbf{x},T=t)=\cases{0&if $T(\mathbf{x})\neq t$\cr P_{ \theta}(\mathbf{X}=\mathbf{x})&if $T(\mathbf{x})=t$,\cr}\]

i.e.,

\[P_{\theta}(\mathbf{X}=\mathbf{x},T=t)=g(t,\theta)h(\mathbf{x})\mathbf{1}_{\{ T(\mathbf{x})=t\}}. \tag{4.6}\]

If \(P_{\theta}(T=t)>0\), then dividing (4.6) by (4.5) one gets

\[P_{\theta}(\mathbf{X}=\mathbf{x}\mid T=t)=\frac{h(\mathbf{x})}{h_{1}(t)}\, \mathbf{1}_{\{T(\mathbf{x})=t\}}. \tag{4.7}\]

Now note that \(P_{\theta^{\prime}}\) (\(T=t\)) = 0, for some \(\theta\) if and only if \(\{\mathbf{x}:T(\mathbf{x})=t\}=\emptyset\). Hence (4.7) holds for all \(\theta\), \(\mathbf{x}\) and \(dT\), and \(T\) is sufficient.

Conversely, suppose \(T\) is sufficient for \(\theta\). Then, writing \(P_{\theta}(T=t)=g(t,\theta)\) and \(P_{\theta}(\mathbf{X}=\mathbf{x}\mid T=t)=h_{2}(\mathbf{x},t)\), one obtains, on the set \(\{T(\mathbf{x})=t\}\subset\mathscr{X}\), for a given \(t\),

\[f(\mathbf{x}\mid\theta) \equiv P_{\theta}(\mathbf{X}=\mathbf{x})=P_{\theta}(\mathbf{X}= \mathbf{x},T=t)=P_{\theta}(T=t)h_{2}(\mathbf{x},t)\] \[= g(t,\theta)h_{2}(\mathbf{x},t)=g(T(\mathbf{x}),\theta)h_{2}( \mathbf{x},T(\mathbf{x}))=g(T(\mathbf{x}),\theta)h(\mathbf{x}),\]

say. 

Example 4.3: Let \(\mathbf{X}=(X_{1},\ldots,X_{n})\) where \(X_{i}\)'s are independent uniform random variables on the interval \([\alpha,\beta]\) (\(\alpha<\beta\)). That is, \(\mathscr{X}=\mathbb{R}^{n}\), \(\theta=(\alpha,\beta)\in\Theta=\{(\alpha,\beta):\alpha<\beta\) real}, \(P_{\theta}\) has density (w.r.t. Lebesgue measure on \(\mathbb{R}^{n}\))

\[f(\mathbf{x}\mid\theta) = \frac{1}{(\beta-\alpha)^{n}}\ \prod_{i=1}^{n}\mathbf{1}_{[\alpha, \beta]}(x_{i})\] \[\equiv \frac{1}{(\beta-\alpha)^{n}}\ \mathbf{1}_{\{\alpha\leq\min(x_{1}, \ldots,x_{n}),\beta\geq\max(x_{1},\ldots,x_{n})\}}\] \[= g(T(\mathbf{x}),\theta),\ \text{say,}\]

where \(T(\mathbf{x})=(\min(x_{1},\ldots,x_{n}),\max(x_{1},\ldots,x_{n}))\) (a measurable map on \(\mathscr{X}\) into \(\mathscr{T}=\{(m,M):m,M\) real numbers \(m\leq M\}\)), and \(g((m,M),(\alpha,\beta))=(\beta-\alpha)^{-n}\mathbf{1}_{\{m\geq\alpha\}} \mathbf{1}_{\{M\leq\beta\}}\). Here one may take \(h(\mathbf{x})\equiv 1\) in (4.4). Hence \(T\) is sufficient for \(\theta=(\alpha,\beta)\).

Example 4.4: Let \(\mathbf{X}=(X_{1},\ldots,X_{n})\) be i.i.d. \(N(\mu,\sigma^{2})\), \(\mu\in\mathbb{R}\) and \(\sigma^{2}>0\) being both unknown parameters. That is, \(\mathscr{X}=\mathbb{R}^{n}\), \(\Theta=\mathbb{R}\times(0,\infty)\), \(P_{\theta}\) has density (w.r.t. Lebesgue measure on \(\mathbb{R}^{n}\)), given by

\[f(\mathbf{x}\mid(\mu,\sigma^{2}))=(2\pi\sigma^{2})^{-n/2}\exp\left\{-\frac{1}{ 2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right\}\]which may be expressed as

\[f(\mathbf{x}\mid(\mu,\sigma^{2})) =(2\pi\sigma^{2})^{-n/2}\exp\left\{-\sum_{i=1}^{n}(x_{i}-\overline{x }+\overline{x}-\mu)^{2}/2\sigma^{2}\right\}\] \[=(2\pi\sigma^{2})^{-n/2}\exp\left\{-\left[\sum_{i=1}^{n}(x_{i}- \overline{x})^{2}+n(\overline{x}-\mu)^{2}\right]\Big{/}2\sigma^{2}\right\}\] \[=g(T(\mathbf{x}),\theta),\quad\text{say, }(\theta=(\mu,\sigma^{2})),\]

with \(T(\mathbf{x})=(\sum_{i=1}^{n}(x_{i}-\overline{x})^{2},\overline{x}).\) Hence taking \(h(\mathbf{x})\equiv 1\) in (4.4), it follows that \(T\) is sufficient for \((\mu,\sigma^{2})\).

The next theorem describes the important role sufficient statistics play in improving decision rules. Before we state and prove it, let us prove

Lemma 4.1 (Jensen's Inequality for Convex Functions in Multidimension): Let \(C\) be a (measurable) convex subset of \(\mathbb{R}^{k}\) and \(f\) a real-valued convex function on \(C\). If \(Y\) is a random variable with values in \(C\) such that \(EY\) is finite, then \(EY\in C\) and

\[f(EY)\leq Ef(Y).\]

If \(f\) is strictly convex, then this inequality is strict unless \(P(Y=EY)=1\).

Demonstration Proof: Convexity of \(f\) means that, for every \(z\in C\), there exists \(m\in\mathbb{R}^{k}\) such that

\[f(y)\geq f(z)+m\cdot(y-z)\qquad y\in C,\]

with a strict inequality for \(y\neq z\) if \(f\) is strictly convex. [Note: In the case \(C\) is an open convex set and \(f\) is twice continuously differentiable with the Hessian matrix (\((D_{i}D_{j}f(\mathbf{x}))\) positive definite for every \(\mathbf{x}\), (4.8) follows by a Taylor expansion of \(f\) around \(z\), with \(m(z)=(\operatorname{grad}f)(z)\)].

To prove the desired inequality, let \(z=EY\), \(y=Y\) in (4.8) to get

\[f(Y)\geq f(EY)+m(EY)\cdot(Y-EY).\]

Taking expectations on both sides, the inequality in the Lemma is obtained. In case \(f\) is strictly convex, the inequality (*) is strict for every \(Y(\omega)\neq EY\), and hence the inequality in the Lemma is strict unless \(Y\) is a constant (\(=EY\)) a.s. 

Theorem 4.2 (Rao-Blackwell Theorem): Let \(\mathscr{A}\) be a (measurable) convex subset of \(\mathbb{R}^{k}\), \(a\to L(\theta,a)\) a convex function for each \(\theta\in\Theta\), and \(T\) a sufficient statistic for \(\theta\). If \(d\) is a (non-randomized) decision rule, then the decision rule

\[\hat{d}(T)=E_{\theta}(d(\mathbf{X})\mid T)=E(d(\mathbf{X})\mid T)\]

is at least as good as \(d:R(\theta,\hat{d})\leq R(\theta,d)\) for all \(\theta\in\Theta\).

Demonstration Proof: Fix \(\theta\in\Theta\), and use (4.8) with \(f=L(\theta,\cdot)\), \(z=\hat{d}(T)\), \(y=d(\mathbf{X})\) to get

\[L(\theta,d(\mathbf{X}))\geq L(\theta,\hat{d}(T))+(d(\mathbf{X})-\hat{d}(T)) \cdot m(\hat{d}(T)).\]Now take conditional expectation, given \(T\), on both sides to get

\[E(L(\theta,d(\mathbf{X}))\mid T)\geq L(\theta,\hat{d}(T)),\]

and then taking expectation complete the proof. 

Remark 2: It is important to note that \(\hat{d}(T)\) as defined by the first equation in (4.9) does not depend on \(\theta\), in view of the sufficiency of \(T\). In other words, \(\hat{d}(T)\)_is an estimator._ The inequality (4.10) holds whether \(T\) is sufficient or not.

Corollary 1: _Let \(c(\theta)\) be a real-valued parametric function of \(\theta\), and \(d(\mathbf{X})\) an estimator of \(c(\theta)\), with \(E_{\theta}d^{2}(\mathbf{X})\) finite for all \(\theta\). Let \(T\) be a sufficient statistic for \(\theta\). (a) Then, for every \(\theta\in\Theta\),_

\[E_{\theta}(\hat{d}(T)-c(\theta))^{2}\leq E_{\theta}(d(\mathbf{X})-c(\theta))^{ 2}, \tag{4.11}\]

_with a strict inequality unless \(P_{\theta}(d(\mathbf{X})=\hat{d}(T))=1\)._

_(b) If \(d(\mathbf{X})\) is an unbiased estimator of \(c(\theta)\) (i.e., \(E_{\theta}d(\mathbf{X})=c(\theta)\ \forall\ \theta\)), then so is \(\hat{d}(T)\)._

Proof: (a) One can derive this from Theorem 4.2, by letting \(\mathscr{A}=\mathbb{R}\) and noting that \(a\to L(\theta,a)\equiv(c(\theta)-a)^{2}\) is strictly convex. _Alternatively,_ one has

\[E_{\theta}(d(\mathbf{X})-c(\theta))^{2} = E_{\theta}(d(\mathbf{X})-\hat{d}(T)+\hat{d}(T)-c(\theta))^{2}\] \[= E_{\theta}(\hat{d}(T)-c(\theta))^{2}+E_{\theta}(d(\mathbf{X})- \hat{d}(T))^{2},\]

since

\[E_{\theta}[(d(\mathbf{X})-\hat{d}(T))(\hat{d}(T)-c(\theta))]=E_{\theta}[(\hat{ d}(T)-c(\theta))\cdot E_{\theta}(d(\mathbf{X})-\hat{d}(T)\mid T)]=0.\]

(b)

\[E_{\theta}(\hat{d}(T))\,=\,E_{\theta}[E_{\theta}(d(\mathbf{X})\ \mid\ T)]\,=\,E_{ \theta}d(\mathbf{X})\,=\,c(\theta)\quad\forall\ \theta.\]

Corollary 2: _In the hypothesis of Corollary 1 assume \(E_{\theta}|d(\mathbf{X})|<\infty\) for all \(\theta\). Then (a) one has_

\[E_{\theta}\mid\hat{d}(T)-c(\theta)|\leq E_{\theta}|d(\mathbf{X})-c(\theta)|.\]

_Also, (b) \(\hat{d}(T)\) is an unbiased estimator of \(c(\theta)\) if \(d(\mathbf{X})\) is._

Proof: (a) One may apply Theorem 4.2 here, noting that the function \(a\to|a-c(\theta)|\) is convex. But, more simply, \(|\hat{d}(T)-c(\theta)|=E(|d(\mathbf{X})-c(\theta)\mid T)|\leq E(|d(\mathbf{X}) -c(\theta)|\mid T)\), and taking expectation with respect to \(P_{\theta}\) one obtains the desired result. (b) The proof of part (b) is the same as that of Corollary 1(b). 

Remark 3: As the statement of Theorem 4.2 indicates, Corollaries 1, 2 extend to the case of estimation of vector valued parametric functions \(\mathbf{c}(\theta)=(c_{1}(\theta),\dots,\)\(c_{k}(\theta))\) for additive loss functions such as \(L(\theta,\mathbf{a})=\sum_{1\leq i\leq k}(a_{i}-c_{i}(\theta))^{2}\) and \(L(\theta,\mathbf{a})=\sum_{1\leq i\leq k}|a_{i}-c_{i}(\theta)|\), respectively.

Remark 4: In general there are many sufficient statistics for \(\theta\). Which one should you use to improve on a given decision rule or estimator in the manner of Theorem 4.2 or its Corollary above? To answer this, consider two sufficient statistics \(T_{1},T_{2}\) (for \(\theta\)) such that \(T_{1}\) is a function of \(T_{2}\), i.e., \(T_{1}=f(T_{2})\), where \(f(T_{2})\) is a measurable function of \(T_{2}\). Then given an estimator \(d_{2}(T_{2})\) of a parametric function \(c(\theta)\), the estimator \(\hat{d}_{1}(T_{1})\equiv E_{\theta}(d_{2}(T_{2})\mid T_{1})\) is at least as good as \(d_{2}(T_{2})\) (say, under squared error loss \(L(\theta,a)=(c(\theta)-a)^{2}\)). One of course can reverse the argument and begin with an estimator \(d_{1}(T_{1})\) and have \(\hat{d}_{2}(T_{2})\equiv E_{\theta}(d_{1}(T_{1})\mid T_{2})\), which would be at least as good as \(d_{1}(T_{1})\). But, in this case, \(\hat{d}_{2}(T_{2})=E_{\theta}(d_{1}(f(T_{2}))\mid T_{2})=d_{1}(f(T_{2}))=d_{1} (T_{1})\). That is, \(d_{1}(T_{1})\) being already a function of \(T_{2}\), the _Rao-Blackwellization_ (of taking conditional expectation of a decision rule, given a sufficient statistic) does not alter the estimator! On the other hand, \(\hat{d}_{1}(T_{1})\) is a _strict improvement_ over \(d_{2}(T_{2})\) unless \(P_{\theta}(\hat{d}_{1}(T_{1})=d_{2}(T_{2}))=1\)\(\forall\)\(\theta\in\Theta\) (i.e., unless \(d_{2}(T_{2})\) is essentially a function of \(T_{1}\) already). _Thus the "smaller" the sufficient statistic \(T\) the better._

_Remark 4.5_.: A statistic \(T\) may be identified with the sigma-field

\[\sigma(T)\equiv T^{-1}(\mathscr{B}_{T})\equiv\{T^{-1}(C):C\in\mathscr{B}_{T}\}.\]

If \(T_{1}\) and \(T_{2}\) are statistics (with possibly different range spaces \(\mathscr{R}_{T_{1}}\) and corresponding \(\sigma\)-fields \(\mathscr{B}_{T_{i}}\) (\(i=1,2\))) are such that there is a bi-measurable one-to-one map \(g\) on \(\mathscr{R}_{T_{1}}\) onto \(\mathscr{R}_{T_{2}}\) with \(T_{2}=g(T_{1})\) (so that \(T_{1}=g^{-1}(T_{2})\)), then \(\sigma(T_{1})=\sigma(T_{2})\) and the statistics may be viewed as the _same, or_ to _contain the same information_ about \(\theta\), since knowing one means knowing the other.

It is technically more convenient to say that \(T_{1}\)_is a smaller statistic than_\(T_{2}\) if \(\sigma(T_{1})\subset\sigma(T_{2})\), and that _they are equivalent_ if \(\sigma(T_{1})=\sigma(T_{2})\). In a statistical decision problem (with a family of distributions \(P_{\theta}\), \(\theta\in\Theta\)), we say \(\sigma(T_{1})\subset\sigma(T_{2})\)_with \(P_{\theta}\)-probability one_\(\forall\)\(\theta\in\Theta\), if for every \(B_{1}\in\sigma(T_{1})\) there exists \(B_{2}\in\sigma(T_{2})\) such that \(P_{\theta}(B_{1}\Delta B_{2})=0\)\(\forall\)\(\theta\in\Theta\). Here \(\Delta\) denotes _symmetric difference_ between sets: \(B_{1}\Delta B_{2}=(B_{1}\cap B_{2}^{c})\cup(B_{1}^{c}\cap B_{2})\).

**Definition 4.2**.: In a statistical decision problem a sufficient statistic \(T^{*}\) is said to be _minimal sufficient_ (for \(\theta\in\Theta\)) if given any other sufficient statistic \(T\), \(\sigma(T^{*})\subset\sigma(T)\) with \(P_{\theta}\)-probability one \(\forall\)\(\theta\in\Theta\).

It can be shown that minimal sufficient statistics exist under fairly weak assumptions. But one can find examples when a minimal sufficient statistic does not exist.

For the purpose of unbiased estimation the following generally stronger property than minimality is very useful.

**Definition 4.3**.: A sufficient statistic \(T\) is said to be _complete_ if for any real-valued function \(g(T)\) of \(T\), integrable w.r.t. \(P_{\theta}\)\(\forall\)\(\theta\in\Theta\),

\[E_{\theta}g(T)=0\quad\forall\)\(\theta\in\Theta\quad\Longrightarrow\quad P_{ \theta}(g(T)=0)=1\quad\forall\)\(\theta\in\Theta. \tag{4.12}\]

A sufficient statistic \(T\) is said to be _boundedly complete_ if (4.12) holds for all bounded measurable functions \(g(T)\) of \(T\). A complete sufficient statistic is obviously boundedly complete.

**Proposition 4.1**.: _Suppose a minimal sufficient statistic \(S\) exists. Then if a sufficient statistic \(T\) exists which is boundedly complete then \(T\) is minimal._

[MISSING_PAGE_FAIL:53]

less). Hence the set \(C_{n}\) of all estimable functions is a subset of the set \(\Gamma_{n}\) of all polynomials (in \(\theta\)) of degree \(n\) or less. To show that \(\Gamma_{n}\) is precisely \(C_{n}\), note that \(\theta^{k}\) is estimable for \(k=0,1,\ldots,n\), with an estimator \(d(\mathbf{X})=X_{1}X_{2}\cdots X_{k}\) for \(k=1,\ldots,n\), and \(d(\mathbf{X})\equiv 1\) for \(k=0\). Hence all polynomials of degree \(n\) or less are estimable. The UMVU estimator of \(\theta^{k}\) is given by

\[\hat{d}_{k}(T)=E(X_{1}\ldots X_{k}\mid T), \tag{4.16}\]

(Exercise 4.3).

_Example 4.6_.: Let \(X_{1},\ldots,X_{n}\) be a random sample from the _uniform distribution_\(\mathscr{U}(0,\theta)\), \(\theta>0\), with p.d.f. \(f_{1}(x\mid\theta)=(1/\theta)\mathbf{1}_{(0,\theta]}(x)\), so that the (joint) density of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) is

\[f(\mathbf{x}\mid\theta) =\frac{1}{\theta^{n}}\prod_{j=1}^{n}\mathbf{1}_{[0<x_{j}\leq \theta]}=\frac{1}{\theta^{n}}\,\mathbf{1}_{[0<x_{j}\leq\theta\ \forall j=1,\ldots,n]}\] \[=\frac{1}{\theta^{n}}\,\mathbf{1}_{[0<M(\mathbf{x})\leq\theta]}, \qquad\mathbf{x}\in\mathbf{X}=(0,\infty)^{n}\]

where \(M(\mathbf{x})=\max\{x_{j}:1\leq j\leq n\}\). By the Factorization Theorem, \(M\) is a sufficient statistic for \(\theta\). We will show that \(M\) is a complete sufficient statistic for \(\theta\). For this note that the distribution function of \(M\) is

\[F_{M}(t)\equiv P(M\leq t)=P(X_{j}\leq t\ \forall\ j=1,\ldots,n)=\left\{\begin{array}{ ll}0&\mbox{for $t\leq 0$,}\\ \left(\frac{t}{\theta}\right)^{n}&\mbox{for $0<t\leq\theta$,}\\ 1&\mbox{for $t>\theta$,}\end{array}\right.\]

so that its p.d.f. is \(f_{M}(t\mid\theta)=\frac{1}{\theta^{n}}nt^{n-1}\mathbf{1}_{[0<t\leq\theta]}\). Now let \(g(t)\) be such that \(E_{\theta}g(M)=0\ \forall\,\theta\in\Theta=(0,\infty)\). This says

\[n\theta^{n}\int_{0}^{\theta}g(t)t^{n-1}dt=0\quad\forall\ \theta>0,\]

implying \(g(t)=0\) a.e. (with respect to Lebesgue measure on \(\mathbb{R}\)). (Exercise 4.4). From the expression for \(E_{\theta}g(M)\) given by the last integral, it follows that every function (of \(\theta\)) of this form (i.e., with \(g(t)t^{n-1}\) integrable on every interval \([0,a]\), \(a>0\)) is estimable. In particular, with \(g(t)=t\), one gets

\[E_{\theta}M=\frac{n}{\theta^{n}}\cdot\frac{\theta^{n+1}}{n+1}=\frac{n}{n+1}\,\theta,\]

so that \(\frac{n+1}{n}\,M\) is an unbiased estimator of \(\theta\) (and \(M\) is an unbiased estimator of \(\frac{n}{n+1}\,\theta\)). By the Lehmann-Scheffe Theorem, \(\left(\frac{n+1}{n}\right)M\) is the uniformly minimum variance unbiased estimator (UMVU) of \(\theta\). Note that \(E_{\theta}M^{2}=\frac{n}{n+2}\theta^{2}\), so that

\[E_{\theta}(M-\frac{n}{n+1}\theta)^{2} =(\frac{n}{n+2}\theta^{2})-(\frac{n}{n+1}\,\theta)^{2}=\theta^{2 }\,n\left[\frac{1}{n+2}-\frac{n}{(n+1)^{2}}\right]\] \[=\frac{n}{(n+2)(n+1)^{2}}\,\theta^{2}\sim\frac{\theta^{2}}{n^{2}};\] \[E_{\theta}\left(\frac{n+1}{n}\,M-\theta\right)^{2} =\left(\frac{n+1}{n}\right)^{2}\mathrm{var}_{\theta}(M)=\frac{1} {n(n+2)}\,\theta^{2}.\]We now turn to a large class of parametric families where complete sufficient statistics exist, so that every estimable parametric function has an UMVU estimator (unique, in case its risk function, or variance, is finite). Examples 4.2, 4.4 and 4.5 are such families, although Example 4.6 is not.

### Exponential Families

We begin with the one-parameter case.

**Definition 4.5**.: A _one-parameter exponential family of distributions_\(\{P_{\theta}:\theta\in\Theta\}\), \(\Theta\) an interval, is such that the probability measure \(P_{\theta}\) on a state space \(S\) has a density \(p(x\mid\theta)\), with respect to a reference measure \(v\) on \(S\), of the form

\[p(x\mid\theta)=c(\theta)h(x)e^{\pi(\theta)t(x)},\ x\in S,\quad\left[c(\theta)= 1/\int_{s}h(x)e^{\pi(\theta)t(x)}v(dx)\right], \tag{4.17}\]

where \(h(x)>0\) for all \(x\) in \(S\), and \(\theta\to\pi(\theta)\) is one-to-one, and \(t(x)\) is a real-valued (measurable) function on \(S\). Reparametrizing \(\theta\to\pi(\theta)=\pi\), and writing \(\tilde{p}(x\mid\pi)=p(x\mid\theta)\) for \(\theta\) such that \(\pi(\theta)=\pi\), one has

\[\tilde{p}(x\mid\pi)=\tilde{c}(\pi)h(x)e^{\pi t(x)},\quad\left(\tilde{c}(\pi)= \left[\int_{s}h(x)e^{\pi t(x)}v(dx)\right]^{-1}\right). \tag{4.18}\]

The new parameter is called the _natural parameter_ and the _natural parameter space_ is

\[\Pi=\left\{\pi\in R:\int h(x)e^{\pi t(x)}v(dx)<\infty\right\}. \tag{4.19}\]

In the case \(S\) is an interval, finite or infinite, and \(v(dx)\) is Lebesgue measure \(dx\), \(p\) or \(\tilde{p}\) is the usual density on \(S\), while on a countable state space \(S\) the measure \(v\) is the _counting measure_ with \(v(\{x\})=1\) for every \(x\in S\). The distribution with density \(\tilde{p}(x\mid\pi)\) (with respect to \(v\)) will be written as \(P_{\pi}\), or even \(P_{\theta}\).

Consider now a random sample \(\mathbf{X}=(X_{1},\ldots,X_{n})\) from \(P_{\theta}\), with \(X_{1},\ldots,X_{n}\) independent having the common distribution \(P_{\theta}\) (or \(P_{\pi}\)). The (joint) density of \(\mathbf{X}\) (with respect to the product measure \(\mu=v\times v\times\cdots\times v\)) on the observation space \(\mathscr{X}=S^{n}\) is written as

\[f(\mathbf{x}\mid\theta)=c^{n}(\theta)\prod h(x_{j})e^{\pi(\theta)\sum t(x_{j} )},\ \text{or}\ \tilde{f}(\mathbf{x}\mid\pi)=\tilde{c}^{n}(\pi)\prod h(x_{j})e^{ \pi\sum t(x_{j})}, \tag{4.20}\]

where the product \(\prod\) and the sum \(\sum\) are both over the indices \(j=1,\ldots,n\). Note that if \(v(dx)=dx\) is the Lebesgue measure then \(\mu(d\mathbf{x})=dx_{1}\ldots dx_{n}\) is the usual Lebesgue measure in \(n\)-dimension. In the case \(v\) is the counting measure on \(S\), \(\mu\) is the counting measure on \(\mathscr{X}=S^{n}:\mu(\{\mathbf{x}\})=1\) for every \(\mathbf{x}\in\mathscr{X}\). Note that, by the Factorization Theorem, \(T(\mathbf{x})=\sum_{1\leq j\leq n}t(x_{j})\) is a sufficient statistic.

As an example, let \(X_{1},\ldots,X_{n}\) be independent normal \(N(\theta,\sigma^{2})\) random variables (each with mean \(\theta\) and variance \(\sigma^{2}\)). Assume \(\sigma^{2}\) is known. Then with \(S=R\), \(v(dx)=dx\), the common pdf of the \(X_{i}\)'s is \[p(x\mid\theta) =(2\pi\sigma^{2})^{-\frac{1}{2}}\text{exp}\left\{-\frac{(x-\theta)^{2} }{2\sigma^{2}}\right\}=c(\theta)h(x)\text{exp}\left\{\left(\frac{\theta}{\sigma^ {2}}\right)x\right\}\quad(\theta\in\Theta=R),\] \[\left[c(\theta)=(2\pi\sigma^{2})^{-1/2}\exp\left\{\frac{-\theta^{ 2}}{2\sigma^{2}}\right\},\quad h(x)=\exp\left\{\frac{-x^{2}}{2\sigma^{2}} \right\}\right]. \tag{4.21}\]

Hence the natural parameter is \(\pi=\theta/\sigma^{2}\) and \(t(x)=x.\) The natural parameter space is \(\prod=R.\) Also, the distribution of the sufficient statistic \(T=\sum_{1\leq j\leq n}X_{j}\) is \(N(n\theta,n\sigma^{2})\) is a one-parameter exponential family, with density (with respect to Lebesgue measure on \(R\)) given by

\[f_{T}(t\mid\theta) =(2\pi n\sigma^{2})^{-1/2}\exp\left\{-\frac{(t-n\theta)^{2}}{2n \sigma^{2}}\right\}\] \[=c(\theta)h(t)\exp\left\{\left(\frac{\theta}{\sigma^{2}}\right)t \right\}=\tilde{c_{1}}(\pi)h(t)\exp\{\pi t\}, \tag{4.22}\]

where \(c(\theta)=(2\pi n\sigma^{2})^{-1/2}\exp\{-n\theta^{2}/2\sigma^{2}\},\)\(h(t)=\exp\{-t^{2}/2n\sigma^{2}\},\) and \(\pi=\theta/\sigma^{2}.\)

An example of a one-parameter family of discrete distributions is the Bernoulli family considered in Example 4.8.

We next consider the general case of \(k\)-parameter exponential families.

Let \(\{G_{\boldsymbol{\theta}}:\boldsymbol{\theta}\in\Theta\}\) be a family of probability measures on a measurable space \((S,\mathscr{S})\) which are absolutely continuous with respect to a sigma-finite measure \(\nu.\) If the density \(p(x\mid\boldsymbol{\theta})\) of \(G_{\boldsymbol{\theta}}\) (w.r.t. \(\nu\)) is of the form

\[p(x\mid\boldsymbol{\theta})=C(\boldsymbol{\theta})h(x)\exp\left\{\sum_{i=1}^{k }\pi_{i}(\boldsymbol{\theta})T_{i}(x)\right\}\qquad(x\in S,\boldsymbol{\theta }\in\Theta), \tag{4.23}\]

where \(h\) is a nonnegative measurable function on \(S,\)\(T_{i},\)\(1\leq i\leq k,\) are real-valued measurable functions on \(S,\) then \(\{G_{\boldsymbol{\theta}}:\boldsymbol{\theta}\in\Theta\}\) is said to be a \(k\)_-parameter exponential family._ Here \(\pi_{i}\) are real-valued functions on \(\Theta,\) and \(C(\boldsymbol{\theta})\) is a normalizing constant,

\[C(\boldsymbol{\theta})=\left(\int_{S}h(x)\exp\left\{\sum_{i=1}^{k}\pi_{i}( \boldsymbol{\theta})T_{i}(x)\right\}\nu(dx)\right)^{-1}. \tag{4.24}\]

Let \(\nu_{T}\) denote the image of the measure \(h(x)d\nu(x)\) on \((\mathbb{R}^{k},\mathscr{B}(\mathbb{R}^{k}))\) under the map \(x\to T(x)\equiv(T_{1}(x),\ldots,T_{k}(x)).\) That is,

\[\nu_{T}(B)=\int_{T^{-1}(B)}h(x)d\nu(x),\quad B\in\mathscr{B}(\mathbb{R}^{k}) \tag{4.25}\]

Then the _distributions_\(G_{\boldsymbol{\theta}}^{T},\) say, _of \(T\) under \(G_{\boldsymbol{\theta}}\)_ have densities with respect to \(\nu_{\mathbf{T}}\) given by

\[p_{{}_{T}}(\mathbf{t}\mid\boldsymbol{\theta})=C(\boldsymbol{\theta})\exp\left\{ \sum_{i=1}^{k}\pi_{i}(\boldsymbol{\theta})t_{i}\right\}\quad(\mathbf{t}=(t_{1},\ldots,t_{k})\in\mathbb{R}^{k},\ \theta\in\Theta), \tag{4.26}\]

so that \(\{G_{\boldsymbol{\theta}}^{T}:\boldsymbol{\theta}\in\Theta\}\) is a \(k\)-parameter exponential family. Note that [see (4.23)], by the Factorization Theorem, \(T\) is a _sufficient statistic for \(\{G_{\boldsymbol{\theta}}:\boldsymbol{\theta}\in\Theta\}.\)_ One may reparametrize \(\{G_{\mathbf{\theta}}:\mathbf{\theta}\in\Theta\}\) with the new parameter \(\mathbf{\pi}=(\pi_{1},\pi_{2},\ldots,\pi_{k})\in\mathbb{R}^{k}\), noting that if \(\pi(\mathbf{\theta})=\pi(\mathbf{\theta}^{\prime})\) then \(G_{\theta}=G_{\theta^{\prime}}\). One then writes the density (4.23) in the form

\[\tilde{p}(x\mid\mathbf{\pi})=\widetilde{C}(\mathbf{\pi})h(x)\exp\left\{\sum_{i=1}^{k} \pi_{i}T_{i}(x)\right\}\qquad(\widetilde{C}(\mathbf{\pi}))=(\int h(x)e^{\sum_{1}^ {k}\pi_{i}T_{i}(x)}d\nu(x))^{-1}. \tag{4.27}\]

\(\mathbf{\pi}\) is called a _natural parameter,_ and the density (4.26) (w.r.t. \(\nu_{T}\)) becomes

\[\tilde{p}_{T}(\mathbf{t}\mid\mathbf{\pi})=\widetilde{C}(\mathbf{\pi})\exp\left\{\sum_ {i=1}^{k}\pi_{i}t_{i}\right\}\qquad(\mathbf{t}\in\mathbb{R}^{k}). \tag{4.28}\]

The _natural parameter space_\(\Pi\) is taken to be the set of all \(\mathbf{\pi}\in\mathbb{R}^{k}\) for which the integral within parentheses in (4.27) is finite:

\[\Pi=\left\{\mathbf{\pi}\in\mathbb{R}^{k}:\int_{S}h(x)e^{\sum_{1}^{k}\pi_{i}T_{i}( x)}d\nu(x)<\infty\right\}\subset\mathbb{R}^{k}. \tag{4.29}\]

Suppose \(X_{1},X_{2},\ldots,X_{n}\) are i.i.d. random variables having a common density (w.r.t. \(\nu\)) of the form (4.23) [or, of the form (4.27)]. Then the (joint) distribution \(P_{\mathbf{\theta}}\) of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) has a density w.r.t. the product measure \(d\mu(\mathbf{x})=d\nu(x_{1})\times d\nu(x_{2})\times\cdots\times d\nu(x_{n})\) (on \((S^{n},\mathscr{S}^{\otimes n})\)), given by

\[f(\mathbf{x}\mid\mathbf{\theta})=C^{n}(\mathbf{\theta})\left(\prod_{j=1}^{n}h(x_{j}) \right)\exp\left\{\sum_{i=1}^{k}\pi_{i}(\mathbf{\theta})\left(\sum_{j=1}^{n}T_{i} (x_{j})\right)\right\},\quad(\mathbf{x}=(x_{1},x_{2},\ldots,x_{n})), \tag{4.30}\]

or, in terms of the natural parameter,

\[\tilde{f}(\mathbf{x}\mid\mathbf{\pi})=\widetilde{C}^{n}(\mathbf{\pi})\left(\prod_{j=1 }^{n}h(x_{j})\right)\exp\left\{\sum_{i=1}^{k}\pi_{i}\left(\sum_{j=1}^{n}T_{i} (x_{j})\right)\right\}. \tag{4.31}\]

Thus the family of distributions \(\{P_{\mathbf{\theta}}:\mathbf{\theta}\in\Theta\}\) on \((\mathscr{X}^{-}=S^{n},\mathscr{S}^{\otimes n})\) is a \(k\)-parameter exponential family, and \(\mathbf{T}(\mathbf{x})=(\sum_{j=1}^{n}T_{1}(x_{j}),\ldots,\sum_{j=1}^{n}T_{k}( x_{j}))\) is a _sufficient statistic_ for \(\{P_{\mathbf{\theta}}:\mathbf{\theta}\in\Theta\}\) (or, \(\{\widetilde{P}_{\mathbf{\pi}}:\mathbf{\pi}\in\Pi\}\)). The same argument as above shows that the distributions \(P_{\mathbf{\theta}}^{\mathbf{T}}\) (\(\mathbf{\theta}\in\Theta\)) (or \(\{\widetilde{P}_{\mathbf{\pi}}^{T}:\mathbf{\pi}:\Pi\}\)) of \(\mathbf{T}\) form a \(k\)-parameter exponential family with density

\[f_{\mathbf{T}}(\mathbf{t}\mid\mathbf{\theta})=C^{n}(\mathbf{\theta})\exp\left\{\sum_{i =1}^{k}\pi_{i}(\mathbf{\theta})t_{i}\right\}\qquad(\mathbf{\theta}\in\Theta,\, \mathbf{t}\in\mathbb{R}^{k})\]

or,

\[\tilde{f}_{\mathbf{T}}(\mathbf{t}\mid\mathbf{\pi})=\widetilde{C}^{n}(\mathbf{\pi}) \exp\left\{\sum_{i=1}^{k}\pi_{i}t_{i}\right\}\qquad(\mathbf{\pi}\in\Pi,\,\mathbf{t }\in\mathbb{R}^{k})\]

w.r.t. the measure (on \((\mathbb{R}^{k},\mathscr{B}(\mathbb{R}^{k}))\) given by

\[\mu_{\mathbf{T}}(B)=\int_{\mathbf{T}^{-1}(B)}\left(\prod_{j=1}^{n}h(x_{j}) \right)d\mu(\mathbf{x})\qquad(B\in\mathscr{B}(\mathbb{R}^{k})). \tag{4.32}\]_Example 4.7 (Normal)._ Let \(X_{1},\ldots,X_{n}\) be i.i.d. with common distribution \(G_{\boldsymbol{\theta}}=N(\mu,\sigma^{2})\), \((\boldsymbol{\theta}=(\mu,\sigma^{2})\in\mathbb{R}\times(0,\infty)\equiv\Theta)\), whose density w.r.t. Lebesgue measure \(\nu\) on \(\mathbb{R}\) is

\[p(x\mid\boldsymbol{\theta}) =\frac{1}{\sqrt{2\pi\sigma^{2}}}\,\exp\left\{-\frac{(x-\mu)^{2}}{ 2\sigma^{2}}\right\}\] \[=\frac{1}{\sqrt{2\pi\sigma^{2}}}\,\exp\left\{-\frac{\mu^{2}}{2 \sigma^{2}}\right\}\exp\left\{\frac{\mu}{\sigma^{2}}\,x-\frac{1}{2\sigma^{2}} \,x^{2}\right\}.\]

Thus \(\{G_{\boldsymbol{\theta}}:\boldsymbol{\theta}\in\Theta\}\) is a \(2\)_-parameter exponential family,_ with natural parameters \(\pi_{1}=\frac{\mu}{\sigma^{2}}\), \(\pi_{2}=-\frac{1}{2\sigma^{2}}\), and (sufficient statistic) \(T(x)=(x,x^{2})\). The natural parameter space is \(\Pi=\mathbb{R}\times(-\infty,0)\). The (joint) distribution \(P_{\boldsymbol{\theta}}\) of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) (on \(\mathscr{X}=\mathbb{R}^{n}\)) has the density (w.r.t. \(\mu\equiv\) Lebesgue measure on \(\mathbb{R}^{n}\))

\[f(\mathbf{x}\mid\boldsymbol{\theta})=(2\pi\sigma^{2})^{-\frac{n}{2}}\exp\left\{ -\frac{n\mu^{2}}{2\sigma^{2}}\right\}\exp\left\{\frac{\mu}{\sigma^{2}}\sum_{j= 1}^{n}x_{j}-\frac{1}{2\sigma^{2}}\sum_{j=1}^{n}x_{j}^{2}\right\},\]

or

\[\tilde{f}(\mathbf{x}\mid\boldsymbol{\pi})=\widetilde{C}^{n}(\pi)\exp\left\{\pi_ {1}\sum_{j=1}^{n}x_{j}+\pi_{2}\sum_{j=1}^{n}x_{j}^{2}\right\},\]

and \(\mathbf{T}=(\sum_{j=1}^{n}X_{j}\), \(\sum_{j=1}^{n}X_{j}^{2})\) is a sufficient statistic for \(\{P_{\boldsymbol{\theta}}:\boldsymbol{\theta}\in\Theta\}\).

_Example 4.8 (Bernoulli)._ Let \(X_{1},\ldots,X_{n}\) be i.i.d. Bernoulli \(\mathscr{B}(\theta)\), i.e., \(\mathrm{Prob}_{\theta}(X_{j}=1)=\theta\), \(\mathrm{Prob}_{\theta}(X_{j}=0)=1-\theta\), \(\theta\in\Theta=(0,1)\). Then (with \(\nu(\{0\})=\nu(\{1\})=1\), as the counting measure on \(S=\{0,1\}\)),

\[p(x\mid\theta) =\theta^{x}(1-\theta)^{1-x}\qquad[x\in\{0,1\},\,\theta\in(0,1)= \Theta]\] \[=(1-\theta)\left(\frac{\theta}{1-\theta}\right)^{x}=(1-\theta)e^{ x\log(\frac{\theta}{1-\theta})},\]

so that \(\{G_{\theta}:\theta\in(0,1)\}\) is a _one-parameter exponential family,_ with \(\pi(\theta)\equiv\pi_{1}(\theta)=\log(\frac{\theta}{1-\theta})\), \(T(x)=x\). Also, the distribution \(P_{\theta}\) of \(\mathbf{X}=(X_{1},\ldots,\mathbf{X}_{n})\) has the density (w.r.t. counting measure \(\mu\) on \(S^{n}=\{0,1\}^{n}\)) given by

\[f(\mathbf{x}\mid\theta) \equiv P_{\theta}(\{\mathbf{x}\})=\theta^{\sum_{1}^{n}x_{j}}(1- \theta)^{n-\sum_{1}^{n}x_{j}}\] \[=(1-\theta)^{n}e^{(\sum_{1}^{n}x_{j})\log(\frac{\theta}{1-\theta })}=\widetilde{C}^{n}(\pi)e^{\pi\sum_{j=1}^{n}x_{j}}\qquad(\mathbf{x}\in\{0,1 \}^{n}).\]

The natural parameter space is \(\Pi=(-\infty,\infty)=\mathbb{R}\). The distribution of the sufficient statistic \(\mathbf{T}=\sum_{j=1}^{n}X_{j}\) is Binomial \(\mathscr{B}(n,\theta)\):

\[f_{\mathbf{T}}(t\mid\theta) =\binom{n}{t}\theta^{t}(1-\theta)^{n-t}=(1-\theta)^{n}\binom{n}{ t}e^{\pi t}\] \[\big{(}t\in\{0,1,\ldots,n\},\quad\pi=\log\left(\frac{\theta}{1- \theta}\right)\in\mathbb{R}\big{)}.\]_Example 4.9 (Poisson)._ Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. Poisson \(\mathscr{P}(\theta)\), \(\theta\in\Theta=(0,\infty)\). That is,

\[G_{\theta}(\{x\})=p(x\mid\theta) =e^{-\theta}\frac{\theta^{x}}{x!}\quad(x\in S=\{0,1,2,\ldots\})\] \[=e^{-\theta}\frac{1}{x!}\,e^{x\log\theta}.\]

The (joint) distribution of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) is given by the density (w.r.t. the counting measure on \(S^{n}\))

\[f(\mathbf{x}\mid\theta)=e^{-n\theta}\frac{1}{\prod_{j=1}^{n}x_{j}!}\,e^{(\log \theta)\sum_{1}^{n}x_{j}}\qquad\mathbf{x}=(x_{1},\ldots,x_{n})\in S^{n}.\]

The natural parameter is \(\pi=\log\theta\in\prod=(-\infty,\infty)\), and \(\mathbf{T}=X_{1}+\cdots+X_{n}\) is a sufficient statistic for \(\{P_{\theta}:\theta\in(0,\infty)\}\) (\(P_{\theta}\) being the distribution of \(\mathbf{X}\)). This is a _one-parameter exponential family_.

_Example 4.10 (Gamma)._\(X_{1},\ldots,X_{n}\) are i.i.d. gamma \(\mathscr{G}(\alpha,\beta)\) with common pdf

\[p(x\mid\boldsymbol{\theta}) =\frac{1}{\alpha^{\beta}\Gamma(\beta)}\,e^{-\frac{x}{\alpha}}x^{ \beta-1}\mathbf{1}_{(0,\infty)}(x),\quad(\boldsymbol{\theta}=(\alpha,\beta)\in (0,\infty)\times(0,\infty)=\Theta),\] \[=\frac{1}{\alpha^{\beta}\Gamma(\beta)}\,\frac{1}{x}\,e^{-\frac{1} {\alpha}\,x+\beta\log x}.\]

This is a _two-parameter exponential family,_ with natural parameters \(\pi_{1}=-\frac{1}{\alpha}\), \(\pi_{2}=\beta\), \(\Pi=(-\infty,0)\times(0,\infty)\). The (joint) distribution \(P_{\boldsymbol{\theta}}\) of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) has pdf (w.r.t Lebesgue measure on \(\mathbb{R}^{n}\))

\[f(\mathbf{x}\mid\boldsymbol{\theta})=\left(\frac{1}{\alpha^{\beta}\Gamma(\beta )}\right)^{n}\frac{1}{\prod_{j=1}^{n}x_{j}}\,e^{-\frac{1}{\alpha}\sum_{1}^{n} x_{j}+\beta\sum_{j=1}^{n}\log x_{j}}.\]

A sufficient statistic for \(\{P_{\boldsymbol{\theta}}:\boldsymbol{\theta}\in\Theta\}\) is \(\mathbf{T}=(\sum_{1}^{n}X_{j},\sum_{1}^{n}\log X_{j})\).

_Example 4.11 (Beta)._\(X_{1},\ldots,X_{n}\) are i.i.d. (Beta \(\mathscr{B}_{e}(\alpha,\beta)\)) with common pdf

\[p(x\mid\boldsymbol{\theta}) =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\,x^{ \alpha-1}(1-x)^{\beta-1},\quad 0<x<1\ (\boldsymbol{\theta}=(\alpha,\beta)\in \Theta=(0,\infty)^{2})\] \[=\frac{C(\alpha,\beta)}{x(1-x)}\,e^{\alpha\log x+\beta\log(1-x)},\]

with natural parameters \(\pi_{1}=\alpha\), \(\pi_{2}=\beta\). This is a two-parameter exponential family. The (joint) pdf of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) is

\[f(\mathbf{x}\mid\boldsymbol{\theta}) =\widetilde{C}^{n}(\boldsymbol{\pi})\frac{1}{\prod_{j=1}^{n}x_{ j}(1-x_{j})}\exp\Bigg{\{}\pi_{1}\sum_{1}^{n}\log x_{j}+\pi_{2}\sum_{1}^{n} \log(1-x_{j})\Bigg{\}},\] \[(\mathbf{x}=(x_{1},\ldots,x_{j})\in(0,1)^{n}),\]

with \((\pi_{1},\pi_{2})\in(0,\infty)^{2}=\Pi\). \(\mathbf{T}(\mathbf{x})=(\sum_{1}^{n}\log x_{j},\sum_{1}^{n}\log(1-x_{j}))\) is a sufficient statistic for this family of distributions \(\widetilde{P}_{\boldsymbol{\pi}}\) (with pdf \(\bar{f}(\mathbf{x}\mid\boldsymbol{\pi})\)).

[MISSING_PAGE_FAIL:60]

\[f(\mathbf{x}\mid\boldsymbol{\theta}) =\theta_{1}^{\sum_{1}^{n}x_{1j}}\theta_{2}^{\sum_{1}^{n}x_{2j}} \ldots\theta_{k+1}^{\sum_{1}^{n}x_{k+1,j}}=C^{n}(\boldsymbol{\theta})\exp\left\{ \sum_{i=1}^{k}\log\left(\frac{\theta_{i}}{1-\theta_{1}-\ldots-\theta_{k}}\right) \hskip-1.422638pt\sum_{j=1}^{n}x_{ij}\right\}\] \[=\tilde{f}(\mathbf{x}\mid\boldsymbol{\pi})=\widetilde{C}^{n}( \boldsymbol{\pi})\exp\left\{\sum_{i=1}^{k}\pi_{i}T_{i}(\mathbf{x})\right\} \qquad[T_{i}(\mathbf{x})=(\sum_{j=1}^{n}x_{ij})_{1\leq i\leq k}]\]

where \((x_{1j},x_{2j},\ldots,x_{kj},\ x_{k+1,j})\in S\), \(1\leq j\leq n\). \(\mathbf{T}(\mathbf{x})=(T_{1}(\mathbf{x}),\ldots,T_{k}(\mathbf{x}))\) is a sufficient statistic for \(\{\widetilde{P}_{\boldsymbol{\pi}}:\boldsymbol{\pi}\in\Pi\}\). This is the so-called _multinomial model_.

The following result shows that the sufficient statistic \(\mathbf{T}\) in Examples 4.7-4.13 is _complete_. Before we state it, it is worthwhile to note that one may consider (4.27) to be the general form of a \(k\)-parameter exponential family with respect to a natural parameter. The joint distribution of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) based on a random sample from such a family happens to be of the same form. One may have similar joint distributions of random variables which are not independent, and the joint distribution may still belong to the exponential family (as is the case, for example, for Gaussian time series or Gaussian random fields). Thus we may regard (4.26), (4.27) to represent a general exponential family (Exercise 4.5).

**Theorem 4.4**.: _Let \(\Pi\) denote the natural parameter space of a \(k\)-parameter exponential family. Then the following hold._

1. \(\Pi\) _is convex._
2. _If_ \(\Pi\) _has a nonempty interior, then the sufficient statistic_ \(T\) _is complete for_ \(\{\widetilde{P}_{\boldsymbol{\pi}}:\pi\in\Pi\}\) _where_ \(\widetilde{P}_{\boldsymbol{\pi}}\) _has the density (_4.27_) with respect to a sigma-finite measure_ \(\nu\) _on an observation space_ \(\mathscr{X}=S\)_, say._

Proof.:
1. Let \(\boldsymbol{\pi}\), \(\boldsymbol{\pi}^{\prime}\in\Pi\), and \(0<\alpha<1\). Then, writing \(d\tilde{\nu}=hd\nu\), \[\int_{S}h(x)e^{(\alpha\pi+(1-\alpha)\pi^{\prime})\cdot T(x)}d\nu(x)= \int_{S}e^{\alpha\pi\cdot T(x)}\cdot e^{(1-\alpha)\pi^{\prime}\cdot T(x)}d \tilde{\nu}(x)\] \[\leq\left[\int_{S}\left(e^{\alpha\boldsymbol{\pi}\cdot T(x)} \right)^{\frac{1}{\alpha}}d\tilde{\nu}(x)\right]^{\alpha}\left[\int_{S}\left( e^{(1-\alpha)\boldsymbol{\pi}^{\prime}\cdot T(x)}\right)^{\frac{1}{1-\alpha}}d\tilde{ \nu}(x)\right]^{1-\alpha}<\infty,\] by Holder's inequality.
2. Let \(\boldsymbol{\pi}_{0}\) be an interior point of \(\Pi\). There exists \(\delta>0\) such that the open ball \(B(\boldsymbol{\pi}_{0},\delta)=\{\boldsymbol{\pi}:|\boldsymbol{\pi}- \boldsymbol{\pi}_{0}|<\delta\}\) is contained in \(\Pi\). Let \(g\) be a \(\widetilde{P}_{\boldsymbol{\pi}}\)-integrable function (\(\forall\,\boldsymbol{\pi}\in\Pi\)) on \(\mathbb{R}^{k}\) such that \(\widetilde{E}_{\boldsymbol{\pi}}g(T)=0\)\(\forall\,\boldsymbol{\pi}\in\Pi\). Writing \(g=g^{+}-g^{-}\) (\(g^{+}(\mathbf{t})=g(\mathbf{t})\mathbf{1}_{[0,\infty)}g(\mathbf{t})\)), \(g^{-}(\mathbf{t})=-g(\mathbf{t})\mathbf{1}_{(-\infty,0)}(g(\mathbf{t}))\) one then has \[\widetilde{E}_{\boldsymbol{\pi}}g^{+}(T)=\widetilde{E}_{\boldsymbol{\pi}}g^{ -}(T)\qquad\forall\,\boldsymbol{\pi}\in\Pi.\] (4.34) In particular, \[\widetilde{E}_{\boldsymbol{\pi}_{0}}g^{+}(T)=\widetilde{E}_{\boldsymbol{\pi}_{ 0}}g^{-}(T).\] (4.35) Suppose, if possible, \(\widetilde{P}_{\boldsymbol{\pi}_{0}}(g(T)=0)<1\). This means that the expectations in (4.35) are positive. Write \(\tilde{\nu}_{T}^{+}\) and \(\tilde{\nu}_{T}^{-}\) for the probability measures \[d\tilde{\nu}_{T}^{+}(\mathbf{t})=\frac{g^{+}(\mathbf{t})d\widetilde{P}_{ \boldsymbol{\pi}_{0}}(\mathbf{t})}{\widetilde{E}_{\boldsymbol{\pi}_{0}}g^{+}(T )}\,,\quad d\tilde{\nu}_{T}^{-}(\mathbf{t})=\frac{g^{-}(\mathbf{t})d \widetilde{P}_{\boldsymbol{\pi}_{0}}(\mathbf{t})}{\widetilde{E}_{\boldsymbol{ \pi}_{0}}g^{-}(T)}\,.\] (4.36)The moment generating function _(mgf)_ of \(\tilde{\nu}_{T}^{+}\) in \(B(0,\delta)\) is given by

\[\frac{\widetilde{C}(\boldsymbol{\pi}_{0})}{\widetilde{E}_{\boldsymbol{ \pi}_{0}}g^{+}(T)}\int\!\!e^{\boldsymbol{\xi}\cdot\mathbf{t}}g^{+}(\mathbf{t}) e^{\boldsymbol{\pi}_{0}\cdot\mathbf{t}}d\nu_{T}(\mathbf{t}) =\frac{\widetilde{C}(\boldsymbol{\pi}_{0})}{\widetilde{E}_{ \boldsymbol{\pi}_{0}}g^{+}(T)}\int\!\!e^{(\boldsymbol{\pi}_{0}+\boldsymbol{\xi })\cdot\mathbf{t}}g^{+}(\mathbf{t})d\nu_{T}(\mathbf{t})\] \[<\infty\ \forall|\boldsymbol{\xi}|<\delta.\]

But, by (4.34), the last integral equals \(\int e^{(\boldsymbol{\pi}_{0}+\boldsymbol{\xi})\cdot\mathbf{t}}g^{-}(\mathbf{t })d\nu_{T}(\mathbf{t})\ \forall\,|\boldsymbol{\xi}|<\delta.\) Also, \(\widetilde{E}_{\boldsymbol{\pi}_{0}}g^{+}(T)=\widetilde{E}_{\boldsymbol{\pi}_ {0}}g^{-}(T).\) Hence, the mgf's of \(\tilde{\nu}_{T}^{+}\) and \(\tilde{\nu}_{T}^{-}\) are equal in a neighborhood of the origin, namely, in \(B(0,\delta).\) By the proposition below it follows that the two probability measures \(\tilde{\nu}_{T}^{+}\) and \(\tilde{\nu}_{T}^{-}\) are identical. This means \(\tilde{P}_{\boldsymbol{\pi}_{0}}(g^{+}(T)=g^{-}(T))=1,\) or, \(\tilde{P}_{\boldsymbol{\pi}_{0}}(g(T)=0)=1,\) a contradiction. 

_Remark 4.6 (General Structure of Exponential Families)_.: On a probability space \((S,\mathscr{S},Q)\) let \(\mathbf{T}=(T_{1},\ldots,T_{k})\) be a random vector having a finite mgf \(\varphi\) on a set \(\Theta\ (\subset\mathbb{R}^{k})\) with a non-empty interior. Then the family of probability measures \(G_{\boldsymbol{\theta}}\) (\(\boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{k})\in\mathbb{R}^{k}\)) with density \(f(\mathbf{t}\ |\ \boldsymbol{\theta})=\frac{\exp\{\sum_{i=1}^{k}\theta_{i}t_{i}\}}{ \varphi(\boldsymbol{\theta})}\) with respect to the distribution \(v\) of \(\mathbf{T}\) in a \(k\)-parameter exponential family with natural parameter \(\boldsymbol{\theta}.\) Every exponential family has this form if the parameter space has a non-empty interior.

**Proposition 4.2**.: _If two probability measures \(Q_{1}\) and \(Q_{2}\) on \(\mathbb{R}^{k}\) have the same mgf in a neighborhood of the origin, then \(Q_{1}=Q_{2}.\)_

Proof.: First consider the case \(k=1.\) Suppose there exists \(u_{0}>0\) such that

\[\varphi_{1}(u)\equiv\int_{\mathbb{R}}e^{ux}dQ_{1}(x)=\varphi_{2}(u)\equiv\int_ {\mathbb{R}}e^{ux}dQ_{2}(x)\quad\text{for all $u$ in $(-u_{0},u_{0}).$} \tag{4.37}\]

Since

\[e^{|ux|}\leq e^{ux}+e^{-ux},\]

one has, on integrating both sides with respect to \(Q_{j},\)

\[\sum_{n=0}^{\infty}\beta_{n,j}\frac{|u|^{n}}{n!}\leq\varphi_{j}(u)+\varphi_{j }(-u)<\infty\quad\text{for $-u_{0}<u<u_{0},$} \tag{4.38}\]

where \(\beta_{n,j}=\int|x|^{n}dQ_{j}(x).\) Since \(e^{ux}\leq e^{|ux|},\) it follows that

\[\sum_{n=0}^{\infty}m_{n,j}\frac{u^{n}}{n!}\quad\text{converges absolutely in $-u_{0}<u<u_{0},\ (i=1,2)$}\]

and

\[\varphi_{j}(u)=\sum_{n=0}^{\infty}\frac{m_{n,j}}{n!}\,u^{n}\quad(j=1,2),\ -u_{0}<u<u_{0}.\]

Here \(m_{n,j}=\int x^{n}dQ_{j}(x)\ (j=1,2).\) Since a power series with a positive radius of convergence is infinitely differentiable within its radius of convergence, and can be differentiated term by term there, it follows in particular, \(m_{n,1}=m_{n,2},\)\(\forall\,n.\) To prove the proposition, we will show that the characteristic functions

\[f_{j}(v)=\int_{\mathbb{R}}e^{ivx}dQ_{j}(x)\qquad(v\in\mathbb{R}),\,(j=1,2),\]are identical. Now \(f_{j}\) is infinitely differentiable on \(\mathbb{R}\), and the \(n\)-th derivative is

\[(D^{n}f_{j})(\nu)=\int_{\mathbb{R}}(ix)^{n}e^{ivx}dQ_{j}(x)\quad(v\in\mathbb{R}),\,j=1,2. \tag{4.39}\]

By a Taylor expansion, using

\[\left|e^{i(v+h)x}-\left(\sum_{n=0}^{N}\frac{(ihx)^{n}}{n!}\right)e^{ivx}\right| \leq\frac{|hx|^{N+1}}{(N+1)!} \tag{4.40}\]

one obtains, using the convergence of the series in (4.38),

\[\left|f_{j}(v+h)-\sum_{n=0}^{N}\frac{h^{n}}{n!}(D^{n}f_{j})(v) \right|\leq\frac{\beta_{N+1,j}}{(N+1)!}\,|h|^{n}\quad(j=1,2)\] \[\longrightarrow 0\quad\text{as }N\rightarrow\infty\quad\ \forall \ |h|<u_{0}.\]

Therefore,

\[f_{j}(v+h)=\sum_{n=0}^{\infty}\frac{h^{n}}{n!}D^{n}f_{j}(v),\quad(j=1,2)\ \ \forall\ |h|<u_{0}. \tag{4.41}\]

Now letting \(v=0\), and using \(D^{n}f_{j}(0)=i^{n}m_{n,j}\)\((j=1,2)\), one gets

\[f_{1}(h)=f_{2}(h)\qquad\forall\ |h|<u_{0}, \tag{4.42}\]

which also implies that \(D^{n}f_{1}(v)=D^{n}f_{2}(v)\ \forall\ n\), if \(|v|<u_{0}\). Thus \(f_{1}(v+h)=f_{2}(v+h)\ \forall\ |v|<u_{0}\) and \(\forall\ |h|<u_{0}\). In other words, \(f_{1}(v)=f_{2}(v)\ \forall\ v\in(-2u_{0},2u_{0})\ \forall\ \varepsilon>0\). Thus \(f_{1}(v)=f_{2}(v)\ \forall\ |v|<2u_{0}\). Continuing in this manner, it follows that \(f_{1}(v)=f_{2}(v)\ \forall\ v\in\mathbb{R}\).

Now let \(k>1\). Let \(X,Y\) have distribution \(Q_{1},Q_{2}\), respectively. _Fix_\(v\in\mathbb{R}^{k}\backslash\{0\}\). The mgf's of the distributions of the random variables \(\nu.X\) and \(\nu.Y\) have finite and equal mgf's in a neighborhood of the origin (namely, \(Ee^{\tau v.X}=Ee^{\tau v.Y}\) for \(-\frac{u_{0}}{|v|}<\tau<\frac{u_{0}}{|v|}\), if the mgf's of \(Q_{1},Q_{2}\) are finite and equal in \(B(0,u_{0})\)). Hence \(v.X\) and \(v.Y\) have the same distribution. This being true for all \(v\in\mathbb{R}^{k}\), \(f_{1}(v)\equiv Ee^{iv.X}=Ee^{iv.Y}=f_{2}(v)\ \forall\ v\in\mathbb{R}^{k}\). Hence \(Q_{1}=Q_{2}\).

_Remark 4.7_.: Theorem 4.4, together with the Lehmann-Scheffe Theorem (Theorem 4.3), provides UMVU estimators of all estimable parametric functions.

### The Cramer-Rao Inequality

We have seen that uniformly minimum variance unbiased (UMVU) estimators exist for estimable parametric functions if a complete sufficient statistic exists. In particular, this is the case with \(k\)-parameter exponential families if the natural parameter space has a non-empty interior in \(\mathbb{R}^{k}\). We now derive a lower bound for the variance (expected squared error) of unbiased estimators of parametric functions under a set of regularity conditions which are satisfied by exponential families (if \(\Pi\) is an open subset of \(\mathbb{R}^{k}\)) and many other families. Although this lower bound is rarely attained, it is approached in the large sample limit by maximum likelihood estimators under the regularity conditions.

Theorem 4.5 (Cramer-Rao Information Inequality): _Suppose \({\bf X}\) has a density \(f({\bf x}\mid\theta)\) (with respect to a sigma-finite measure \(\mu\) on \((\mathscr{X},\mathscr{B}(\mathscr{X}))\) satisfying the following conditions:_

1. \(\Theta\) _is an open interval;_
2. \(\exists\) \(a\) \(\mu\)_-null set_ \(N\) _such that_ \(f({\bf x}\mid\theta)>0\)__\(\forall\)__\({\bf x}\in\mathscr{X}\backslash N\)_,_ \(\forall\)__\(\theta\in\Theta\)_;_
3. \(\int\frac{d}{d\theta}\,f({\bf x}\mid\theta)d\mu({\bf x})=\frac{d}{d\theta}\int_ {\mathscr{X}\backslash N}f({\bf x}\mid\theta)d\mu({\bf x})\equiv 0\)_,_ \(\forall\)__\(\theta\in\Theta\)_;_
4. _if_ \(T({\bf X})\) _is a real-valued statistic such that_ \(E_{\theta}T^{2}<\infty\)__\(\forall\)__\(\theta\in\Theta\)_, then writing_ \(a(\theta)=E_{\theta}T\)_, one has_ \[\frac{d}{d\theta}\,a(\theta)\equiv\frac{d}{d\theta}\int_{\mathscr{X}}T({\bf x })f({\bf x}\mid\theta)d\mu({\bf x})=\int_{\mathscr{X}\backslash N}T({\bf x}) \frac{d}{d\theta}\,f({\bf x}\mid\theta)d\mu(x).\] _Then_ \[{\rm var}_{\theta}T\equiv E_{\theta}(T-a(\theta))^{2}\geq\frac{(a^{\prime}( \theta))^{2}}{E_{\theta}\left(\frac{d\log f({\bf X}\mid\theta)}{d\theta}\right) ^{2}}.\] (4.43)

Proof: Condition (iii) may be restated as

\[0=E_{\theta}\,\frac{d\log f({\bf X}\mid\theta)}{d\theta}\qquad\left(=\int_{ \mathscr{X}\backslash N}\frac{\frac{d}{d\theta}f({\bf x}\mid\theta)}{f({\bf x} \mid\theta)}\,f({\bf x}\mid\theta)d\mu({\bf x})\right). \tag{4.44}\]

Similarly, condition (iv) says (writing \({\rm cov}_{\theta}\) for _covariance_ under \(P_{\theta}\))

\[{\rm cov}_{\theta}(T,\frac{d}{d\theta}\,\log f({\bf X}\mid\theta))=a^{\prime}( \theta), \tag{4.45}\]

since \({\rm cov}_{\theta}(T,\frac{d\log f({\bf X}\mid\theta)}{d\theta})=E_{\theta}T \frac{d\log f({\bf X}\mid\theta)}{d\theta}\), as \(E_{\theta}(\frac{d\log f({\bf X}\mid\theta)}{d\theta})=0\). The inequality (4.43) now follows from the Cauchy-Schwarz inequality applied to (4.45). 

Remark Remark 4.8: The most common case covered in this course is \({\bf X}=(X_{1},\ldots,X_{n})\), \(X_{j}\)'s being i.i.d. with a (common) pdf \(f_{1}(x\mid\theta)\) (w.r.t. a sigma-finite measure \(\nu\) on a measurable space \((S,\mathscr{B}(S))\). In this case, assumption (ii) may be replaced by \(({\rm ii})_{1}:f_{1}(x_{1}\mid\theta)>0\)\(\forall\)\(x_{1}\in S\backslash N_{1}\), _where_\(\nu(N_{1})=0\). Note that, in this case

\[f({\bf x}\mid\theta)\!\!=\!\prod_{j=1}^{n}f_{1}(x_{j}\mid\theta),\ \log f({\bf x}\mid\theta)\!\!=\!\sum_{j=1}^{n}\log f_{1}(x_{j}\mid\theta)\ ({\bf x}\!\!=\!\!(x_{1},\ldots,x_{n})\,\in S^{n}\!\!=\!\!\mathscr{X}),\]

so that (iii) [or, (4.44)] is equivalent to

\[\left({\rm iii}\right)_{1}:\quad E_{\theta}\frac{d}{d\theta}\,\log f_{1}(X_{1} \mid\theta)=0. \tag{4.46}\]

Also,

\[E_{\theta}\left(\frac{d\log f({\bf X}\mid\theta)}{d\theta}\right)^{2} ={\rm var}_{\theta}\left(\frac{d\log f({\bf X}\mid\theta)}{d \theta}\right)=nE_{\theta}\left(\frac{d\log f_{1}(X_{1}\mid\theta)}{d\theta} \right)^{2}\] \[=n\,{\rm var}_{\theta}\left(\frac{d\log f_{1}(X_{1}\mid\theta)}{ d\theta}\right), \tag{4.47}\]

[MISSING_PAGE_EMPTY:12476]

Now, under regularity conditions (allowing the interchange of the order of differentiation and integration below),

\[E_{\theta}\frac{d^{2}\log f_{1}(X_{1}\mid\theta)}{d\theta^{2}}=\int _{\mathscr{X}}\frac{d}{d\theta}\left(\frac{\frac{d}{d\theta}f_{1}(x\mid\theta)}{ f_{1}(x\mid\theta)}\right)f_{1}(x\mid\theta)d\nu(x)\] \[=\int_{\mathscr{X}}\frac{f_{1}^{\prime\prime}(x\mid\theta)f_{1}(x \mid\theta)-(f_{1}^{\prime}(x\mid\theta))^{2}}{f_{1}^{2}(x\mid\theta)}\,f_{1}(x \mid\theta)dv(x)\] \[=\int_{\mathscr{X}}f_{1}^{\prime\prime}(x\mid\theta)dv(x)-\int_{ \mathscr{X}}\left(\frac{d\log f_{1}(x\mid\theta)}{d\theta}\right)^{2}f_{1}(x \mid\theta)dv(x)\] \[=\frac{d^{2}}{d\theta^{2}}\int_{\mathscr{X}}f_{1}(x\mid\theta)dv(x )-E_{\theta}\left(\frac{d\log f_{1}(X_{1}\mid\theta)}{d\theta}\right)^{2}\] \[=-E_{\theta}\left(\frac{d\log f_{1}(X_{1}\mid\theta)}{d\theta} \right)^{2}. \tag{4.53}\]

Substituting this in (4.52), we get

\[\sigma^{2}(\theta_{0})=\left[\frac{1}{E_{\theta_{0}}}\left(\frac{d\log f_{1}(X _{1}\mid\theta)}{d\theta}\right)_{\theta=\theta_{0}}^{2}\right]^{-1}=\frac{1}{I (\theta_{0})}, \tag{4.54}\]

say. The quantity \(I(\theta)\) is referred to as the _Fisher information_. Thus \(\hat{\theta}_{n}\) is _asymptotically Normal with mean \(\theta_{0}\) and variance \(\frac{1}{nI(\theta_{0})}\)_, the latter being the lower bound in Theorem 4.5 for the variance of unbiased estimators of the parametric function \(a(\theta)=\theta\) [see (4.48)].

Consider next a \(k\)-parameter exponential family (4.17) or (4.21), \(k>1\), but with \(q\) restrictions on the parameters given by \(q<k\) smooth functionally independent relations among the parameters. The reduced model is expressed in terms of \(d=k-q\) independent parameters and is called a _curved exponential family_ since the new parameter space is a \(d\)-dimensional surface in the original \(k\)-dimensional parameter space. Generally, this term is restricted to those cases where the new model is not a \(d\)-dimensional exponential family. The following examples are taken from Bickel and Doksum (2001, pp. 126, 405).

_Example 4.14_ (_Estimation of Mean of a Gaussian with a Fixed Signal-to-Noise Ratio_).: Here \(X_{1},\ldots,X_{n}\) (\(n\geq 2\)) are i.i.d. \(N(\mu,\sigma^{2})\), \(\mu>0\) with the ratio \(\frac{\mu}{\sigma}=\lambda>0\) known. The common density may then be expressed as

\[f(x;\mu) =\frac{1}{\sqrt{2\pi\mu^{2}/\lambda^{2}}}\,\exp\left\{-\frac{ \lambda^{2}}{2\mu^{2}}\,x^{2}+\frac{\lambda^{2}}{\mu}\,x-\frac{\lambda^{2}}{2}\right\}\] \[=\frac{\lambda/u}{\sqrt{2\pi}}\,e^{-x^{2}/2}\exp\left\{\pi_{1}( \mu)x+\pi_{2}(\mu)x^{2}\right\}, \tag{4.55}\]

where \(\pi_{1}(\mu)=\lambda^{2}/\mu\), \(\pi_{2}(\mu)=-\lambda^{2}/2\mu^{2}\). If \(\lambda\) was not known, this would be a two-parameter exponential family in natural parameters \(\pi_{1}=-1/2\sigma^{2}\,\in(-\infty,0)\), \(\pi_{2}=\mu/\sigma^{2}\in(0,\infty)\). But with the given relation \(\mu/\sigma=\lambda\) known, it is a curved exponential family with \(k-q=2-1=d\). The log-likelihood function is

\[\ell=\log f_{n}(\mathbf{X};\mu)=-\frac{n}{2}\log 2\pi+n\log \lambda-n\log\mu-\frac{\lambda^{2}}{2\mu^{2}}T_{2}+\frac{\lambda^{2}}{\mu}T_ {1}-\frac{n\lambda^{2}}{2}\] \[\left(T_{2}=\sum_{1}^{n}X_{j}^{2},\,\,\,T_{1}=\sum_{1}^{n}X_{j} \right),\]and the likelihood equation is

\[0=-\frac{n}{\mu}+\frac{\lambda^{2}T_{2}}{\mu^{3}}-\frac{\lambda^{2}}{\mu^{2}}T_{1},\ \text{or,}\ \mu^{2}=\lambda^{2}m_{2}-\lambda^{2}\overline{X}\mu,\]

where \(m_{2}=\sum_{1}^{n}X_{j}^{2}/n\). The solutions of this quadratic equation are

\[\mu=-\frac{1}{2}\lambda^{2}\overline{X}\pm\sqrt{\frac{1}{4}\lambda^{4} \overline{X}^{2}+\lambda^{2}m_{2}}.\]

Since \(\mu>0\), the MLE of \(\mu\) is therefore given by

\[\hat{\mu}=-\frac{1}{2}\lambda^{2}\overline{X}+\frac{1}{2}\sqrt{\lambda^{4} \overline{X}^{2}+4\lambda^{2}m_{2}}\,. \tag{4.56}\]

_Example 4.15 (The Fisher Linkage Model)._ The following genetic model was considered by Fisher (1958, p. 301). Self-crossing of maize heterozygous on two alleles yield four types of offspring: sugary-white, sugary-green, starchy-white, starchy-green. If \(\theta_{1}\), \(\theta_{2}\), \(\theta_{3}\), \(\theta_{4}\) are the probabilities of having an offspring of these four types and if \(N_{1}\), \(N_{2}\), \(N_{3}\), \(N_{4}\) are the numbers of offspring of these types among \(n\) offspring, then \((N_{1},N_{2},N_{3},N_{4})\) has a multinomial distribution (See Example 4.13)

\[P_{\boldsymbol{\theta}}(N_{i}=n_{i};i=1,\ldots,4)=\frac{n!}{n_{1}!n_{2}!n_{3}! n_{4}!}\theta_{1}^{n_{1}}\theta_{2}^{n_{2}}\theta_{3}^{n_{3}}(1-\theta_{1}- \theta_{2}-\theta_{3})^{n_{4}},\]

a \(k=3\)-parameter exponential family. According to a linkage model, \(\theta_{1}=\frac{1}{4}(2+\eta)\), \(\theta_{2}=\theta_{3}=\frac{1}{4}(1-\eta)\) and, consequently, \(\theta_{4}=\frac{1}{4}\eta\), where the unknown parameter \(\eta\) lies in \([0,1]\). One then arrives at a curved exponential family with \(d=1\). The likelihood equation is

\[\frac{n_{1}}{2+\eta}-\frac{n_{2}+n_{3}}{1-\eta}+\frac{n_{4}}{\eta }=0,\ \text{or},\] \[-n\eta^{2}+[n_{1}-2(n_{2}+n_{3})-n_{4}]\eta+2n_{4}=0,\]

whose solutions are

\[\eta=-\frac{1}{2}\left[1-2\hat{p}_{1}+\hat{p}_{2}+\hat{p}_{3}\pm\sqrt{(1-2 \hat{p}_{1}+\hat{p}_{2}+\hat{p}_{3})^{2}+4\hat{p}_{4}}\right],\]

where \(\hat{p}_{i}=n_{i}/n\) (\(i=1,2,3,4\)). The positive solution is the MLE:

\[\hat{\eta}=-\frac{1}{2}\left(1-2\hat{p}_{1}+\hat{p}_{2}+\hat{p}_{3}\right)+ \frac{1}{2}\sqrt{(1-2\hat{p}_{1}+\hat{p}_{2}+\hat{p}_{3})^{2}+4\hat{p}_{4}}\,. \tag{4.57}\]

### 4.4 Notes and References

Our presentation is influenced by Ferguson (1967, Chap. 3), Lehmann (1959, Chap. 2), and Bickel and Doksum (2001, Sect. 3.4).

The notion of sufficiency is due to R.A. Fisher (1922), who also stated the factorization criterion. A rigorous derivation of the criterion for general dominated families is due to Halmos and Savage (1949), and further generalized by Bahadur (1954). The Rao-Blackwell theorem is due to Rao (1945) and Blackwell (1947).

The Lehmann-Scheffe theorem is due to Lehmann and Scheffe (1947, 1950, 1955), where the notion of minimal sufficiency is also introduced. Exponential families were introduced by R.A. Fisher (1934) in one dimension and extended to higher dimensions by G. Darmois, B.O. Koopman and E.J.G. Pitman. Barndorff-Nielsen (1978) and Brown (1986) provide rigorous accounts of general exponential families.

Diaconis and Ylvisaker (1979) explore the existence of conjugate priors for Bayes estimation in exponential families. Generally, the computation of the posterior distribution poses numerical challenges if the prior is not conjugate. See chap. 14 in the context.

### Exercises for Chap. 4

**Ex. 4.1**.: Show that, irrespective of the hypothesis of convexity of the loss function in Theorem 4.2, given any decision rule \(d(\mathbf{X})\) one can construct \(\mathbf{X}^{0}\) based entirely on the value of the sufficient statistic \(T\) such that \(\mathbf{X}\) and \(\mathbf{X}^{0}\) have the same distribution \(P_{\theta}\), \(\forall\ \theta\in\Theta\). Hence \(d(\mathbf{X}_{0})\) has the same risk function as \(d(\mathbf{X})\). In this sense, no information is lost by recording only the value of \(T\) (and nothing else) from the observation \(\mathbf{X}\),

**Ex. 4.2**.: Consider a family of distributions \(P_{\theta}\), \(\theta\in\Theta\), of the observation \(\mathbf{X}\), and let \(T_{1}\), \(T_{2}\) be two statistics. Show that \(\sigma(T_{1})\subset\sigma(T_{2})\) with \(P_{\theta}\)-probability one \(\forall\ \theta\in\Theta\), if and only if for every bounded \(\sigma(T_{1})\)-measurable real-valued function \(f(T_{1})\) there exists a \(\sigma(T_{2})\)-measurable bounded \(g(T_{2})\) such that \(P_{\theta}(f(T_{1})=g(T_{2}))=1\ \forall\ \theta\in\Theta\).

**Ex. 4.3**.:
* In Example 4.5, find the UMVU estimator of \(\theta^{k}\) (\(k=1,\ldots,n\)).
* In Example 4.6, find the UMVU estimators of (i) \(\sin\theta\), (ii) \(e^{\theta}\).

**Ex. 4.4**.: In Example 4.6, show that '\(\int_{0}^{\theta}g(t)t^{n-1}dt=0\ \forall\ \theta>0\)' implies \(g(t)=0\) a.e. (w.r.t. Lebesgue measure on \((0,\infty)\)). [Hint: Write \(g(t)=g^{+}(t)-g^{-}(t)\), where \(g^{+}(t)=\max\{0,g(t)\}\), \(g^{-}(t)=-\min\{0,g(t)\}\). Then \(F^{+}(\theta)\equiv\int_{0}^{\theta}g^{+}(t)t^{n-1}dt\) is the distribution function of the Lebesgue-Stieltjes (L-S) measure \(\mu^{+}\) on \((0,\infty)\), which equals \(F^{-}(\theta)\equiv\int_{0}^{\theta}g^{-}(t)t^{n-1}dt\)--the distribution function of an L-S measure \(\mu^{-}\), say. Hence \(\mu^{+}=\mu^{-}\). In particular, \(g^{+}(t)=g^{-}(t)\) a.e. on \((0,\infty)\)].

**Ex. 4.5** (Ornstein-Uhlenbeck Process).: Let \(\{V_{t}:0\leq t<\infty\}\) be a stationary Gaussian process with \(V_{t}\) having distribution \(N(0,\sigma^{2}/2\gamma)\), and \(\operatorname{cov}(V_{s},V_{s+t})=\frac{\sigma^{2}}{2\gamma}\,e^{-\gamma t}\) for some \(s\geq 0\), \(t\geq 0\) (\(\sigma^{2}>0\), \(\gamma>0\)). Consider observations \(X_{j}=V_{t_{j}}\) (\(j=1,\ldots,n\)), where \(0=t_{1}<t_{2}<\cdots<t_{n}\). Find UMVU estimators of \(\sigma^{2}\) and \(\gamma\).

**Ex. 4.6**.: Show that the assumptions of Theorem 4.5 hold for one-parameter exponential families where the natural parameter space is an open interval.

**Ex. 4.7**.: In Example 4.12, find the UMVU estimators of (a) \(\boldsymbol{\mu}\) and (b) \(\Sigma\). That is, find \(\hat{\boldsymbol{\mu}}\) and \(\widehat{\Sigma}\) such that (ia) \(E\hat{\boldsymbol{\mu}}=\boldsymbol{\mu}\), (ib) \(E\ \widehat{\Sigma}=\Sigma\) and (iia) \(\hat{\boldsymbol{\mu}}=\arg\min E|d_{1}(\mathbf{X})-\boldsymbol{\mu}|^{2}\) and (iib) \(\widehat{\Sigma}=\arg\min E|d_{2}(\mathbf{X})-\Sigma|^{2}\), over the class of all unbiased estimators \(d_{1}(\mathbf{X})\) of \(\boldsymbol{\mu}\), and the class of all unbiased estimators \(d_{2}(\mathbf{X})\) of \(\Sigma\). Here the distances in (iia,b) are Euclidean distances between vectors.

**Ex. 4.8**.: Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(N(\mu,\sigma^{2})\), \(\theta=(\mu,\sigma^{2})\in\mathbb{R}\times(0,\infty)\). Find the UMVU estimator of \(\mu/\sigma\). [Hint: Use the fact that \(\overline{X}\), and \(s^{2}\) are independent, unbiased estimators of \(\mu\), and of \(\sigma^{2}\). Also, it is known that \(U=\frac{(n-1)s^{2}}{\sigma^{2}}\) has a \(\chi^{2}_{n-1}\) distribution. Therefore, \(s^{-2}=\frac{n-1}{\sigma^{2}}\,U^{-1}\), where \(U\sim\chi^{2}_{n-1}\), and \(s^{-1}=\frac{\sqrt{n-1}}{\sigma}\,U^{-\frac{1}{2}}\), where \(U\sim\chi^{2}_{n-1}\). We get \(E(s^{-1})=\frac{\sqrt{n-1}}{\sigma}\,E(U^{-\frac{1}{2}})\), where \(U\sim\chi^{2}_{n-1}\). Note that for \(n\geq 3\), since \(U\sim\chi^{2}_{n-1}\), we have

\[c_{n}=E(U^{-\frac{1}{2}})=\int_{0}^{\infty}u^{\frac{n}{2}-2}\exp\left(-\frac{u }{2}\right),\]

a finite integral that can be expressed in terms of the Gamma function. Thus \(\frac{1}{\sigma}=E(\frac{1}{c_{n}\sqrt{n-1}s_{n}})\), and \(d(X)=\frac{\tilde{X}}{c_{n}\sqrt{n-1}s_{n}}\) is an unbiased estimator of \(\frac{\mu}{\sigma}\), and one may apply Lehmann-Scheffe's theorem.]

**Ex. 4.9**.: Let \(X_{j}\) (\(j=1,2,\ldots,n\)) be i.i.d. real-valued observations from an unknown distribution \(P\).

1. Let \(\Theta\) be the (infinite dimensional) set of all _continuous distributions_ on \((\mathbb{R},\mathscr{B}(\mathbb{R}))\) (i.e., \(P(\{x\})=0\)\(\forall\)\(x\in\mathbb{R}\)). Show that the _order statistic_\(T=(X_{(1)},X_{(2)},\ldots,X_{(n)})\) is sufficient for \(\{P:P\in\Theta\}\). [Hint: Compute the conditional distribution of \(\mathbf{X}\) given \(T\).]
2. Let \(\Theta\) be the set of all _discrete distributions_ on \(\{0,1,2,\ldots\}\) (i.e., \(\sum_{j}P(\{j\})=1\)). Let \(n_{j}=\#\{i:X_{i}=j\}\) (\(j=0,1,\ldots\)). Show that \(T=\{n_{j}:j=0,1,\ldots\}\) is a sufficient statistic for \(\{P:P\in\Theta\}\).

**Ex. 4.10** (Negative Binomial Distribution).: Let \(X_{j}\) (\(j=1,2,\ldots,n\)) be i.i.d. observations, with \(P_{\theta}(X_{1}=x)=\binom{r+x-1}{x}(1-\theta)^{r}\theta^{x}\) (\(x=0,1,\ldots\)). Here \(r\) is a positive integer (known) and \(\theta\in\Theta=(0,1)\). In a coin tossing experiment, \(r+X_{1}\) may be the first time the \(r\)-th tail shows up.

1. Calculate \(E_{\theta}X_{1}\) [Hint: Think of \(X_{1}\) as the sum of \(r\) i.i.d. random variables each of which has the \(P_{\theta}\)-distribution above, but with \(r=1\)].
2. Find the UMVU estimator of \(\frac{\theta}{1-\theta}\), and calculate its variance.
3. Find the MLE of \(\theta\) and compute its asymptotic distribution as \(n\to\infty\).

**Ex. 4.11**.: Let \(X_{1},\ldots,X_{n}\) be i.i.d. observations with the common density (w.r.t. Lebesgue measure on \((0,1)\)) \(f(x\mid\theta)=\theta x^{\theta-1}\) (\(0<x<1\)), \(\theta\in\Theta=(0,\infty)\).

1. Find the UMVU estimator of \(\theta\).
2. Find the UMVU estimator of \(1/\theta\).
3. Find the MLE of \(\theta\). [Hint: Look at \(Y_{j}=-\ln X_{j}\).]

**Ex. 4.12**.:
1. Derive the asymptotic distribution of the MLE \(\hat{\mu}\) in (4.56).
2. Derive the asymptotic distribution of the MLE \(\hat{\eta}\) in (4.57).

**Ex. 4.13**.: Assume \(\theta\in(0,1)\), and let \(X_{1},\ldots,X_{n}\) be i.i.d. from a geometric distribution \(P_{\theta}(X=x)=\theta(1-\theta)^{x-1}\), \(x=1,2,\ldots\). Show that \(T=\sum_{i=1}^{n}X_{i}\) is sufficient for \(\theta\). Is \(T\) a complete sufficient statistic?

## A Project for Students

#### Project: Space Shuttle Disaster

In 1986, the space shuttle Challenger exploded during take off, killing the seven astronauts aboard. It was determined that the explosion was the result of an O-ring failure, a splitting of a ring of rubber that seals different parts of the ship together. The flight accident was believed to be caused by the unusually cold weather (31 \({}^{\circ}\)F) at the time of the launch. The past O-ring failure data along with temperature at launch time are given below (in increasing order of temperature) for 23 prior flights. The flight numbers denote the (unimportant) time order of launch. The numbers 0 and 1 indicate "no O-ring failure" and "O-ring failure", respectively.

#### Project Objective

Estimate the probability of O-ring failure at temperature 31 \({}^{\circ}\)F and at 65 \({}^{\circ}\)F.

#### Suggested Model

Let \(Y\) denote the failure status (response variable), and \(X\) the temperature in degrees F at launch time (explanatory variable). Use the _logistic regression model,_

\[P(Y=1\mid X=x)=\exp\frac{\{\alpha+\beta x\}}{[1+\exp\{\alpha+ \beta x\}]}=P(X),\mbox{ say, and}\] \[P(Y=0\mid X=x)=1-p(x). \tag{4.58}\]

Note that one may express the model as

\[\log\left[\frac{p(x)}{(1-p(x))}\right]=\alpha+\beta x. \tag{4.59}\]

Hence the name logistic regression.

Assume that the regressor \(x\) is _stochastic_ and \((X_{i},Y_{i})\) are i.i.d. random vectors.

For 23 independent \(Y\) observations \((y_{1},\ldots,y_{23})\) the conditional likelihood function (i.e., the conditional p.d.f. of \(Y_{i}\), given \(X_{i}=x_{i}\) (\(i=1,\ldots,23\))), is

\[\ell(\mathbf{y}\mid\mathbf{x};\alpha,\beta)=\prod_{i=1,\ldots,23} \left[p(x_{i})^{y_{i}}(1-p(x_{i}))^{1-y_{i}}\right], \tag{4.60}\]

and the (conditional) log likelihood is

\[\log\ell=\sum_{i}[y_{i}(\alpha+\beta x_{i})]-\sum_{i}\log[1+\exp \{\alpha+\beta x_{i}\}]. \tag{4.61}\]

Assume that the distribution of \(X_{i}\) does not involve \(\alpha,\beta\).

1. (i)Find the maximum likelihood estimates \(\hat{\alpha},\hat{\beta}\) of \(\alpha\) and \(\beta\), and (ii) use these to estimate the desired failure probabilities at \(x=31^{0}\)F, and at \(x=65\,^{\circ}\)F, using (4.58). [Hint: You will need to compute the estimates numerically e.g., by the gradient method, as solutions of the likelihood equations \(\partial\log\ell/\partial\alpha=0\), \(\partial\log\ell/\partial\beta=0\), or using direct maximizing algorithms (e.g., as is available on Matlab). There is also a fast algorithm for this called a Re-weighted Least Squares Algorithm (See Wasserman 2003, pp. 223-224).]
2. By _bootstrapping_ from the i.i.d. observations \(\{(X_{i},Y_{i});\,1\leq i\leq 23\}\), (i)find a lower \(90\,\%\) confidence bound for the probability of failure at launch temperature \(x=31\,^{\circ}\)F. In other words, you are to find a number \(U\) such that Prob (\(p(31)\geq U\)) = 0.90. [Hint: Find the lower tenth percentile \(q_{0.10}\) of the bootstrap values \(\hat{\alpha}^{*}+\hat{\beta}^{*}(31)\), using estimates \(\hat{\alpha}\), \(\hat{\beta}\) of \(\alpha,\beta\) as in (a), but from each bootstrap resample from \(\{(X_{i},Y_{i}):i=1,\ldots,23\}\) instead of the original sample. Now use \(\exp\{q_{0.10}\}/(1+\exp\{q_{0.10}\})\) as the desired lower bound for the probability of failure (noting that \(e^{q}/(1+e^{q})\) is a strictly increasing function of \(q\)). Observe that this says that you are (approximately) \(90\,\%\) sure that the probability of failure at \(31\,^{\circ}\)F is at least \(\exp\{q_{0.10}\}/(1+\exp\{q_{0.10}\})\).]
3. Also find an upper \(90\,\%\) bound for the probability of O-ring failure at the temperature \(x=65\,^{\circ}\)F. That is, find a value \(q\) such that the probability of O-ring failure is less than \(e^{q}/(1+e^{q})\), with a probability \(0.90\).

[Note: **Bootstrapping** means taking repeated samples of size 23 (with replacement) from the observations \(\{(y_{i},x_{i}):i=1,\ldots,23\}\). Between 500 and 1000 such re-samples from the observed data should be enough. Each bootstrap sample (of size 23) is used to compute \((\hat{\alpha}^{*},\hat{\beta}^{*})\) as in (a)].

## Appendix for Project: The Nonparametric Percentile Bootstrap of Efron

Let \(\hat{\sigma}_{n}\) be the standard error of \(\hat{\theta}_{n}\) (That is, \(\hat{\sigma}_{n}\) is an estimate of the standard deviation of \(\hat{\theta}_{n}\)). An asymptotic confidence interval of confidence level \(1-\alpha\) for \(\theta\) would follow from the relation \(P(z_{\alpha/2}\hat{\sigma}_{n}\leq\hat{\theta}_{n}-\theta\leq z_{1-\alpha/2} \hat{\sigma}_{n})\approx 1-\alpha\), namely, it is the interval \([\hat{\theta}_{n}-z_{1-\alpha/2}\hat{\sigma}_{n},\hat{\theta}_{n}-z_{\alpha/2 }\hat{\sigma}_{n}]=[\hat{\theta}_{n}-z_{\alpha/2}\hat{\sigma}_{n},\hat{\theta }_{n}+z_{1-\alpha/2}\hat{\sigma}_{n}]=[l,u]\), say. Now the bootstrap version \(\hat{\theta}_{n}^{*}\) of \(\hat{\theta}_{n}\) is, under the empirical \(\mathbf{P}^{*}=\widehat{P}_{n}\), asymptotically Normal \(N(\hat{\theta}_{n},\hat{\sigma}_{n}^{2})\), so that the \(\alpha/2\)-th and \((1-\alpha/2)\)-th quantiles of \(\hat{\theta}_{n}^{*}\), \(q_{\alpha/2}^{*}\) and \(q_{1-\alpha/2}^{*}\) say, are asymptotically equal to \(\hat{\theta}_{n}+z_{\alpha/2}\hat{\sigma}_{n}=l\) and \(\hat{\theta}_{n}+z_{1-\alpha/2}\hat{\sigma}_{n}=u\), respectively.

Hence the _percentile bootstrap_ based confidence interval for \(\theta\) is given by

\[\left[q_{\alpha/2}^{*},q_{1-\alpha/2}^{*}\right]. \tag{4.62}\]

Note that the construction of this interval only involves resampling from the data repeatedly to construct bootstrap versions \(\hat{\theta}_{n}^{*}\) of \(\hat{\theta}_{n}\); it does not involve the computation of the standard error \(\hat{\theta}_{n}\).

Although (4.62) does not involve computing the standard error \(\hat{\sigma}_{n}\), the latter is an important object in statistical analysis. It follows from the above that the variance \(\hat{\sigma}_{n}^{*2}\) of the \(\hat{\theta}_{n}^{*}\) values from the repeated resamplings provide an estimate of \(\hat{\sigma}_{n}^{2}\) [A rough estimate of \(\hat{\sigma}_{n}\) is also provided by \(([q_{1-\alpha/2}^{*}-q_{\alpha/2}^{*}]/2z_{1-\alpha/2})^{\frac{1}{2}}\)].

When the standard error \(\hat{\sigma}_{n}\) of \(\hat{\theta}_{n}\) is known in closed form, one may use the studentized or pivoted statistic \(T_{n}=(\hat{\theta}_{n}-\theta)/\hat{\sigma}_{n}\) which is asymptotically standard Normal \(N(0,1)\). The usual CLT-based symmetric confidence interval for \(\theta\) is given by

\[\left[\hat{\theta}_{n}+z_{\alpha/2}\hat{\sigma}_{n},\hat{\theta}_{n}+z_{1- \alpha/2}\hat{\sigma}_{n}\right]=\left[\hat{\theta}_{n}-z_{1-\alpha/2}\hat{ \sigma}_{n},\hat{\theta}_{n}-z_{\alpha/2}\hat{\sigma}_{n}\right], \tag{4.63}\]

using \(P(|T_{n}|\leq z_{1-\alpha/2})=1-\alpha\). The corresponding pivotal bootstrap confidence interval is based on the resampled values of \(T_{n}^{*}=(\hat{\theta}_{n}^{*}-\hat{\theta}_{n})/\hat{\sigma}_{n}^{*}\), where \(\hat{\sigma}_{n}^{*}\) is the bootstrap estimate of the standard error as described in the preceding paragraph. Let \(c_{\alpha/2}^{*}\) be such that \(P^{*}(|T_{n}^{*}|\leq c_{\alpha/2}^{*})=1-\alpha\). The bootstrap pivotal confidence interval for \(\theta\) is then

\[\left[\hat{\theta}_{n}-c_{\alpha/2}^{*}\hat{\sigma}_{n}^{*},\hat{\theta}_{n}+c_ {\alpha/2}^{*}\hat{\sigma}_{n}^{*}\right]. \tag{4.64}\]

Suppose \(\hat{\theta}_{n}\) is based on i.i.d. observations \(X_{1},\ldots,X_{n}\), whose common distribution has a density (or a nonzero density component), and that it is a smooth function of sample means of a finite number of characteristics of \(X\), or has a stochastic expansion (Taylor expansion) in terms of these sample means (such as the MLE in regular cases). It may then be shown that the _coverage error_ of the CLT-based interval (4.63) is \(O(n^{-1})\), while that based on (4.64) is \(O(n^{-3/2})\), a major advantage of the bootstrap procedure. The coverage error of the percentile interval (4.62) is \(O(n^{-1/2})\), irrespective of whether the distribution of \(X\) is continuous or discrete.

**Definition 4.6**.: The _coverage error_ of a confidence interval for a parameter \(\theta\) is the (absolute) difference between the actual probability that the true parameter value belongs to the interval and the target level \(1-\alpha\).

## References

* Bahadur (1954) Bahadur, R. R. (1954). Sufficiency and statistical decision functions. _The Annals of Mathematical Statistics, 25_, 423-462.
* Barndorff-Nielsen (1978) Barndorff-Nielsen, O. (1978). _Information and exponential families in statistical theory_. New York: Wiley.
* Bickel & Doksum (2001) Bickel, P. J., & Doksum, K. (2001). _Mathematical statistics_ (2nd ed.). Englewood Cliffs, NJ: Prentice Hall.
* Blackwell (1947) Blackwell, D. (1947). Conditional expectation and unbiased sequential estimation. _The Annals of Mathematical Statistics, 18_(1), 105-110.
* Brown (1986) Brown, L. (1986). _Fundamentals of statistical exponential families: With applications in statistical decision theory_ (Vol. 9). Hayward: Institute of Mathematical Statistics.
* Diaconis & Ylvisaker (1979) Diaconis, P. and Ylvisaker, D. (1979). Conjugate priors for exponential families. _Ann. Statist. 7_, 269-281.
* Fisher (1922) Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. _Philosophical Transactions of the Royal Society of London, 222_, 310-366.
* Fisher (1934) Fisher, R. A. (1934). Two new properties of mathematical likelihood _Proceedings of the Royal Society A, 144_, 285-307.

Fisher, R. A. (1958). _Statistical methods for research workers_ (13th ed.). New York: Hafner
* Halmos & Savage (1949) Halmos, P., & Savage, J. (1949). Application of the Radon-Nikodym theorem to the theory of sufficient statistics. _The Annals of Mathematical Statistics, 20_, 225-241.
* Lehmann (1959) Lehmann, E. (1959). _Testing statistical hypothesis_. New York: Wiley.
* Lehmann & Scheffe (1947) Lehmann, E. L., & Scheffe, H. (1947). On the problem of similar regions. _Proceedings of the National Academy of Sciences of the United States of America, 33_, 382-386.
* Lehmann & Scheffe (1950) Lehmann, E. L., & Scheffe, H. (1950). Completeness, similar regions, and unbiased estimation. I. _Sankhya, 10_, 305-340.
* Lehmann & Scheffe (1955) Lehmann, E. L., & Scheffe, H. (1955). Completeness, similar regions, and unbiased estimation. II. _Sankhya, 15_, 219-236.
* Rao (1945) Rao, C. R. (1945). Information and accuracy attainable in the estimation of statistical parameters. _Bulletin of the Calcutta Mathematical Society, 37_(3), 81-91.
* Wasserman (2003) Wasserman, L. (2003). _All of statistics: A concise course in statistical inference_. New York: Springer.

## Chapter 5 Testing Hypotheses

**Abstract** This chapter develops the theory of optimal parametric tests. The Neyman-Pearson Lemma provides the most powerful test of any given size for a simple null hypothesis \(H_{0}\) against a simple alternative hypothesis \(H_{1}\). For one parameter exponential models such tests are uniformly most powerful (UMP) against one-sided alternatives. For two-sided alternatives here one obtains a UMP test among all unbiased tests of a given size. In multiparameter exponential models one may similarly obtain UMP unbiased tests in the presence of nuisance parameters. For statistical models which are invariant under a group of transformations all reasonable tests should be invariant under the group. The theory of UMP tests among all invariant tests is developed for linear models.

### 5.1 Introduction

An observation (vector) \(\mathbf{X}\) is distributed according to \(P_{\theta}\) on the observation space \(\mathscr{X}\). Here \(\theta\) is an unknown parameter lying in a set \(\Theta\). Suppose there are two competing hypotheses for \(\theta\):

\[\begin{array}{ll}\mbox{\it Null Hypothesis H}_{0}:&\theta\in\Theta_{0},\\ \mbox{\it Alternative Hypothesis H}_{1}:&\theta\in\Theta_{1}\end{array} \tag{5.1}\]

where \(\Theta_{0}\) and \(\Theta_{1}\) are nonempty and \(\Theta_{0}\cup\Theta_{1}=\Theta\). On the basis of the observation \(\mathbf{X}\), the statistician must decide whether to _accept_\(H_{0}\) or to _accept_\(H_{1}\)_(reject_\(H_{0}\)_). Generally, \(H_{0}\) is such that one can not afford to reject it unless the evidence against it is very compelling. This creates an asymmetry in the problem.

As discussed in Chap. 2, on page 2, one may take the action space here as \(\mathscr{A}=\{a_{0},a_{1}\}\), where \(a_{0}=\) "accept \(H_{0}\)", \(a_{1}=\) "accept \(H_{1}\)". The usual loss function is

\[L(\theta,a_{i})=\left\{\begin{array}{ll}0\mbox{ if }\theta\in\Theta_{i},\\ 1\mbox{ if }\theta\not\in\Theta_{i}\ \ (i=0,1).\end{array}\right. \tag{5.2}\]

**Definition 5.1**.: A _nonrandomized test_\(d\) is of the form

\[d(\mathbf{x})=\left\{\begin{array}{ll}a_{0}\mbox{ if }\mathbf{x}\in A,\\ a_{1}\mbox{ if }\mathbf{x}\in C=\mathscr{X}\backslash A,\end{array}\right. \tag{5.3}\]where \(A\) (the _acceptance region_ for \(H_{0}\)), or \(C\) (the _rejection region for \(H_{0}\),_ or _critical_ region), is a measurable subset of \(\mathscr{X}\). The _risk function_ is given by

\[R(\theta,d)=E_{\theta}L(\theta,d(\mathbf{X}))=\cases{\alpha_{d}(\theta)=P_{ \theta}(\mathbf{X}\in C)\mbox{ if }\theta\in\Theta_{0},\cr\beta_{d}(\theta)=P_{ \theta}(\mathbf{X}\in A)\mbox{ if }\theta\in\Theta_{1}.\cr} \tag{5.4}\]

The quantity \(\alpha_{d}(\theta)\) is called the probability of a _Type I Error,_ or the _level of significance_ of the test, while \(\beta_{d}(\theta)\) is the probability of a _Type II Error,_ and \(\gamma_{d}(\theta)=1-\beta_{d}(\theta)\equiv P_{\theta}(\mathbf{X}\in C)\), \(\theta\in\Theta_{1}\), is called the _power of the test._

**Definition 5.2**.: More generally, _a (randomized) test \(\delta\)_ is a measurable assignment of probabilities \(\mathbf{x}\rightarrow(1-\varphi(\mathbf{x}),\varphi(\mathbf{x}))\), \(0\leq\varphi(\mathbf{x})\leq 1\), so that, given \(\mathbf{X}=\mathbf{x}\), one takes the action \(a_{1}\) (accept \(H_{1}\) or, equivalently, reject \(H_{0}\)) with probability \(\varphi(\mathbf{x})\) and takes the action \(a_{0}\) (accept \(H_{0}\)) with probability \(1-\varphi(\mathbf{x})\). We will generally refer to \(\varphi\) as the _test,_ since \(\delta\) is determined by it. Then

\[R(\theta,\delta)=\cases{\alpha_{\delta}(\theta)=E_{\theta}\varphi(\mathbf{X}) \qquad\mbox{if }\theta\in\Theta_{0},\cr\beta_{\delta}(\theta)=1-E_{\theta} \varphi(\mathbf{X})\mbox{ if }\theta\in\Theta_{1}.\cr} \tag{5.5}\]

The power of the test is \(\gamma_{\delta}(\theta)=E_{\theta}\varphi(\mathbf{X})\) (for \(\theta\in\Theta_{1}\)).

In the case of a non-randomized test, \(\varphi(\mathbf{x})=1_{C}(\mathbf{x})\).

The maximum value (or, the supremum) of \(\alpha_{\delta}(\theta)\) over \(\Theta_{0}\) is referred to as the _size of the test_ and is also sometimes called the _level of significance of the test:_

\[\alpha_{\delta}\equiv\mbox{size of the test }\delta\ =\sup_{\theta\in\Theta_{0} }\alpha_{\delta}(\theta). \tag{5.6}\]

In view of the asymmetry of the nature of the hypotheses mentioned above, the classical testing procedure aims at keeping the size of the test small (say \(\alpha_{\delta}=0.05\) or \(0.01\)) while trying to minimize \(\beta_{\delta}(\theta)\), or maximize the power \(\gamma_{\delta}(\theta)\), \(\theta\in\Theta_{1}\), as far as possible.

**Definition 5.3**.: A test \(\delta^{*}\) is said to be _uniformly most powerful (UMP) of size \(\alpha\)_ if

\[\alpha_{\delta^{*}}\equiv\sup_{\theta\in\Theta_{0}}\alpha_{\delta}(\theta)=\alpha, \tag{5.7}\]

and

\[\gamma_{\delta^{*}}(\theta)\geq\gamma_{\delta}(\theta)\qquad\forall\ \theta \in\Theta_{1}, \tag{5.8}\]

for all tests \(\delta\) of size \(\alpha\) or less.

As we will see UMP tests exist only under special circumstances.

_Choice of \(H_{0}\) and \(H_{1}\) in One-Sided Tests_ To appreciate the importance of the choice of \(H_{0}\) and \(H_{1}\) in practical situations, consider the problem of a retailer deciding whether to buy a large consignment of a manufactured item. He would be happy if the proportion \(p\) of defectives did not exceed \(5\,\%\). One may then consider \(H_{0}:p\leq 0.05\), \(H_{1}:p>0.05\). Suppose a random sample of size \(n\) yields a proportion of defectives \(4\,\%\). At any reasonable level of significance, say \(\alpha=0.05\), the optimal test will "accept" \(H_{0}\). But accepting \(H_{0}\) is not in general a strong endorsement of \(H_{0}\), since it is given so much protection, and the statistician declares that \(H_{0}\) is "not rejected". On the other hand, let \(H_{0}:p\geq 0.05\), \(H_{1}:p<0.05\). It may very well happen that the corresponding optimal test will reject \(H_{0}\). This would be a rather strong indictment against \(H_{0}\), since it got rejected in spite of such a strong protection (namely, \(\alpha=0.05\)). The retailer may now feel confident that the quality of the product meets his criterion.

### Simple Hypotheses and the Neyman-Pearson Lemma

Consider the case where \(\Theta_{0}=\{\theta_{0}\}\) is a singleton. Then \(H_{0}\) is called a _simple null hypothesis_ (Else it is a _composite null hypothesis_). Similarly, if \(\Theta_{1}=\{\theta_{1}\}\), then \(H_{1}\) is called a _simple alternative hypothesis_ (Else it is a _composite alternative hypothesis_). We first consider the case of a simple null hypothesis \(H_{0}:\theta=\theta_{0}\) and a simple alternative hypothesis \(H_{1}:\theta=\theta_{1}\) (so that \(\Theta=\{\theta_{0},\theta_{1}\}\)). We will show that there exists a most powerful test \(\delta^{*}\) for a given size \(\alpha\) (\(0\leq\alpha\leq 1\)). Let \(P_{\theta}\) have density \(f(\mathbf{x}\mid\theta)\) (w.r.t. a \(\sigma\)-finite measure \(\mu\)).

**Theorem 5.1** (The Neyman-Pearson Lemma): _For \(H_{0}:\theta=\theta_{0}\), \(H_{1}:\theta=\theta_{1}\), consider the test \(\varphi^{*}(\mathbf{x})\) of the form_

\[\varphi^{*}(\mathbf{x})=\begin{cases}1&\text{if $f(\mathbf{x}\mid\theta_{1}) >kf(\mathbf{x}\mid\theta_{0})$},\\ \gamma&\text{if $f(\mathbf{x}\mid\theta_{1})=kf(\mathbf{x}\mid\theta_{0})$},\\ 0&\text{if $f(\mathbf{x}\mid\theta_{1})<kf(\mathbf{x}\mid\theta_{0})$},\end{cases} \tag{5.9}\]

_where \(0\leq k<\infty\) and \(0\leq\gamma\leq 1\) are constants._

1. _Then_ \(\varphi^{*}\) _is a most powerful test of its size._
2. _The test_ \[\varphi^{*}(\mathbf{x})=\begin{cases}1&\text{if $f(\mathbf{x}\mid\theta_{0})=0$,}\\ 0&\text{if $f(\mathbf{x}\mid\theta_{0})>0$,}\end{cases}\] (5.10) _is most powerful of size_ \(0\)_._
3. _For every_ \(\alpha\)_,_ \(0\leq\alpha\leq 1\)_, there exists a test of the above form._

Proof: (a) Let \(\varphi^{*}\) be as in (5.9), and consider a test \(\varphi\) of size no more than that of \(\varphi^{*}\). Then, writing \(\beta^{*}=\beta_{\delta^{*}}(\theta_{1})\), \(\beta=\beta_{\delta}(\theta_{1})\),

\[\beta-\beta^{*} =E_{\theta_{1}}\left[(1-\varphi(\mathbf{X}))-(1-\varphi^{*}( \mathbf{X}))\right]=E_{\theta_{1}}(\varphi^{*}(\mathbf{X})-\varphi(\mathbf{X}))\] \[=\int_{A_{1}=\{\mathbf{x}:f(\mathbf{x}|\theta_{1})>kf(\mathbf{x}| \theta_{0})\}}(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x}))f(\mathbf{x}\mid \theta_{1})d\mu(\mathbf{x})\] \[\quad+\int_{A_{2}=\{\mathbf{x}:f(\mathbf{x}|\theta_{1})=kf( \mathbf{x}|\theta_{0})\}}(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x}))f( \mathbf{x}\mid\theta_{1})d\mu(\mathbf{x})\] \[\quad+\int_{A_{3}=\{\mathbf{x}:f(\mathbf{x}|\theta_{1})<kf(\mathbf{ x}|\theta_{0})\}}(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x}))f(\mathbf{x}\mid \theta_{1})d\mu(\mathbf{x})\] \[\geq\int_{A_{1}}(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x}))kf( \mathbf{x}\mid\theta_{0})d\mu(\mathbf{x})\] \[\quad+\int_{A_{2}}(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x}))kf( \mathbf{x}\mid\theta_{0})d\mu(\mathbf{x})+\int_{A_{3}}(\varphi^{*}(\mathbf{x}) -\varphi(\mathbf{x}))kf(\mathbf{x}\mid\theta_{0})d\mu(\mathbf{x}), \tag{5.11}\]

since (1) on \(A_{1}\), \(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x})\geq 0\), (2) on \(A_{2}\), \(f(\mathbf{x}\mid\theta_{1})=kf(\mathbf{x}\mid\theta_{0})\), and (3) on \(A_{3}\), \(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x})\leq 0\) (and the factor \(f(\mathbf{x}\mid\theta_{1})\) is replaced by a smaller quantity on \(A_{3}\)). But the (extreme) right side of (5.11) equals \(k(E_{\theta_{0}}\varphi^{*}({\bf X})-E_{\theta_{0}}\varphi({\bf X}))=k(\alpha_{ \delta^{*}}-\alpha_{\delta})\geq 0\), since by hypothesis \(\alpha_{\delta}\leq\alpha_{\delta^{*}}\).
2. The size of \(\varphi^{*}\), given by (5.10), is \(\alpha_{\delta^{*}}=E_{\theta_{0}}\varphi^{*}({\bf X})=\int_{\{{\bf x}:f({\bf x }|\theta_{0})=0\}}f({\bf x}\mid\theta_{0})d\mu({\bf x})+\int_{\{{\bf x}:f({\bf x }|\theta_{0})>0\}}0\cdot f({\bf x}\mid\theta_{0})d\mu({\bf x})=0\). If \(\varphi\) is a test of size \(0\), then \(E_{\theta_{0}}\varphi({\bf X})=0\), so that \(\varphi({\bf x})=0\) a.e. \(\mu\) on \(\{{\bf x}:f({\bf x}\mid\theta_{0})>0\}\). Clearly, among all such tests a (the) most powerful test assigns \(\varphi({\bf x})=1\) on the rest of the observation space \(\{{\bf x}:f({\bf x}\mid\theta_{0})=0\}\).
3. Let \(0<\alpha\leq 1\). Write \(Y=f({\bf X}\mid\theta_{1})/f({\bf X}\mid\theta_{0})\). Note that \(P_{\theta_{0}}(Y=\infty)=P_{\theta_{0}}(\{{\bf x}:f({\bf x}\mid\theta_{0})=0\})=0\). Hence, under \(P_{\theta_{0}}\), \(0\leq Y<\infty\), a.s. We need to determine \(k\) and \(\gamma\) such that \(P_{\theta_{0}}(Y>k)+\gamma P_{\theta_{0}}(Y=k)=\alpha\), or \[P_{\theta_{0}}(Y\leq k)-\gamma P_{\theta_{0}}(Y=k)=1-\alpha.\] (5.12) If there exists \(k\) such that \(P_{\theta_{0}}(Y\leq k)=1-\alpha\), then use this \(k\) and take \(\gamma=0\). If not, there exists \(k_{0}\) such that \(P_{\theta_{0}}(Y<k_{0})<1-\alpha\) and \(P_{\theta_{0}}(Y\leq k_{0})>1-\alpha\). In this case, (5.12) is solved by taking \(k=k_{0}\) and \[\gamma=\frac{P_{\theta_{0}}(Y\leq k_{0})-(1-\alpha)}{P_{\theta_{0}}(Y=k_{0})}= \frac{\alpha-P_{\theta_{0}}(Y>k_{0})}{P_{\theta_{0}}(Y=k_{0})}\,.\] That is, \(\alpha=P_{\theta_{0}}(Y>k_{0})+\gamma P_{\theta_{0}}(Y=k_{0})\). 

_Remark 5.1_.: The use of densities \(f({\bf x}\mid\theta_{0})\), \(f({\bf x}\mid\theta_{1})\) of the distributions \(P_{\theta_{0}}\), \(P_{\theta_{1}}\) on \(\mathscr{X}\) is not a restriction. For, one may always take the dominating measure \(\mu=P_{\theta_{0}}+P_{\theta_{1}}\). In most of the examples that we deal with in this course, \(P_{\theta}\) has density w.r.t. Lebesgue measure (on \(\mathbb{R}^{n}\)), or w.r.t. the counting measure (on \(S^{n}\) where \(S\) is countable).

### Examples

Consider a one-parameter exponential family of distributions with density (w.r.t. a \(\sigma\)-finite measure \(\mu\) on the observation space \(\mathscr{X}\))

\[f({\bf x}\mid\theta)=\widetilde{C}(\theta)h({\bf x})\exp\{\pi(\theta)T({\bf x} )\}, \tag{5.13}\]

which may be written in natural parameter form as

\[\tilde{f}({\bf x}\mid\pi)=\widetilde{C}(\pi)h({\bf x})\exp\{\pi T({\bf x})\}. \tag{5.14}\]

Assume the natural parameter space \(\Pi\) is a nonempty open interval, \(\Pi=(a,b)\). We want to test

\[H_{0}:\pi=\pi_{0}\quad\mbox{against}\quad H_{1}:\pi=\pi_{1}. \tag{5.15}\]

The most powerful test \(\varphi^{*}\) of a given size \(\alpha\) is of the form given by the N-P Lemma:

\[\varphi^{*}({\bf x})=\left\{\begin{array}{ll}1&\mbox{if }\tilde{f}({\bf x}\mid \pi_{1})/\tilde{f}({\bf x}\mid\pi_{0})>k\\ \gamma&\mbox{if }\tilde{f}({\bf x}\mid\pi_{1})/\tilde{f}({\bf x}\mid\pi_{0})=k\\ 0&\mbox{if }\tilde{f}({\bf x}\mid\pi_{1})/\tilde{f}({\bf x}\mid\pi_{0})<k. \end{array}\right. \tag{5.16}\]That is,

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\text{ if }\exp\{(\pi_{1}-\pi_{0}) T(\mathbf{x})\}>k_{1}\equiv k\widetilde{C}(\pi_{0})/\widetilde{C}(\pi_{1})\\ \gamma&\text{ if }\exp\{(\pi_{1}-\pi_{0})T(\mathbf{x})\}=k_{1}\\ 0&\text{ if }\exp\{(\pi_{1}-\pi_{0})T(\mathbf{x})\}<k_{1}.\end{array}\right.\]

Suppose \(\pi_{1}>\pi_{0}\). Then

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\text{ if }T(\mathbf{x})>k_{2}=( \ln\,k_{1})/(\pi_{1}-\pi_{0})\\ \gamma&\text{ if }T(\mathbf{x})=k_{2}\\ 0&\text{ if }T(\mathbf{x})<k_{2}.\end{array}\right. \tag{5.17}\]

If, on the other hand, \(\pi_{1}<\pi_{0}\), then the most powerful test is

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\text{ if }T(\mathbf{x})<k_{2} \\ \gamma&\text{ if }T(\mathbf{x})=k_{2}\\ 0&\text{ if }T(\mathbf{x})>k_{2}.\end{array}\right. \tag{5.18}\]

Note that \(k_{2}\) and \(\gamma\) are determined _only by the size of the test_. For example, in the case \(\pi_{1}>\pi_{0}\), one has

\[\alpha=P_{\pi_{0}}(T>k_{2})+\gamma P_{\pi_{0}}(T=k_{2}). \tag{5.19}\]

_Example 5.1_.: Let \(\mathscr{X}=\mathbb{R}^{n}\), \(P_{\theta}\) the joint distribution of \(n\) i.i.d. variables, each with distribution \(N(\theta,1)\), so that

\[f(\mathbf{x}\mid\theta) =(2\pi)^{-n/2}\exp\left\{-\sum_{j=1}^{n}(x_{j}-\theta)^{2}/2\right\}\] \[\tilde{f}(\mathbf{x}\mid\pi) =(2\pi)^{-n/2}e^{-n\theta^{2}/2}\exp\left\{-\sum_{j=1}^{n}x_{j}^{ 2}/2\right\}\exp\left\{\theta\sum_{j=1}^{n}x_{j}\right\}\] \[=\widetilde{C}(\pi)h(\mathbf{x})\exp\{\pi T(\mathbf{x})\},\qquad( \mathbf{x}\in\mathscr{X})\]

where \(\pi=\theta\in\Pi=\mathbb{R}\), and \(T(\mathbf{x})=\sum_{j=1}^{n}x_{j}\). The most powerful test for

\[H_{0}:\theta=\theta_{0}\quad(\text{or }\pi=\pi_{0})\quad\text{ against }\quad H_{1}:\theta=\theta_{1}(\pi=\pi_{1}),\]

with \(\theta_{1}>\theta_{0}\), is given by

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\text{ if }T(\mathbf{x})>k_{2}\\ \gamma&\text{ if }T(\mathbf{x})=k_{2}\\ 0&\text{ if }T(\mathbf{x})<k_{2}.\end{array}\right.\]

To determine \(k_{2}\) and \(\gamma\), using (5.19), note first that \(P_{\pi_{0}}(T=k_{2})=0\), since the \(P_{\pi_{0}}\)-distribution of \(T=\sum_{1}^{n}X_{j}\) is \(\mathbf{N}(n\theta_{0},n)\) (Normal with mean \(n\theta_{0}\) and variance \(n\)), which assigns probability \(0\) to every singleton \(\{k_{2}\}\). Hence we can take the test to be nonrandomized,

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\text{ if }T(\mathbf{x})>k_{2},\\ 0&\text{ if }T(\mathbf{x})\leq k_{2},\end{array}\right. \tag{5.20}\]where, noting that \((T-n\theta_{0})/\sqrt{n}\) is \(\mathbf{N}(0,1)\), one has

\[\alpha=P_{\theta_{0}}(T>k_{2})=P_{\theta_{0}}\left(\frac{T-n\theta_{0}}{\sqrt{n} }>\frac{k_{2}-n\theta_{0}}{\sqrt{n}}\right)=1-\Phi\left(\frac{(k_{2}-n\theta_{0 })}{\sqrt{n}}\right) \tag{5.21}\]

i.e.,

\[\frac{k_{2}-n\theta_{0}}{\sqrt{n}}=\Phi^{-1}(1-\alpha)\quad(k_{2}=n\theta_{0}+ \sqrt{n}\,\Phi^{-1}(1-\alpha)), \tag{5.22}\]

\(\Phi\) being the (cumulative) distribution function of \(\mathbf{N}(0,1)\). A standard way of expressing (5.21)-(5.22) is:

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\mbox{if $\sqrt{n}(\overline {x}-\theta_{0})>z_{1-\alpha}\equiv\Phi^{-1}(1-\alpha)$}\\ 0&\mbox{if $\sqrt{n}(\overline{x}-\theta_{0})\leq z_{1-\alpha}$.}\end{array}\right. \tag{5.23}\]

Note that (5.23) is of the form (5.20), and this test has size \(\alpha\) (since \(\sqrt{n}(\overline{x}-\theta_{0})\) has the distribution \(\mathbf{N}(0,1)\) if \(\theta_{0}\) is the true parameter value).

_Example 5.2_.: Let \(\mathscr{X}=\{0,1\}^{n}\), and the probability mass function (i.e., density w.r.t. counting measure \(\mu\) on \(\mathscr{X}\)) is

\[p(\mathbf{x}\mid\theta) =\theta^{\sum_{1}^{n}x_{j}}(1-\theta)^{n-\sum_{1}^{n}x_{j}}=(1- \theta)^{n}\left(\frac{\theta}{1-\theta}\right)^{\sum_{1}^{n}x_{j}}\] \[=\widetilde{C}(\pi)e^{\pi\sum_{1}^{n}x_{j}}=\widetilde{C}(\pi)e^{ \pi T(\mathbf{x})},\quad\mbox{ say, }(T=\sum_{j=1}^{n}X_{j}),\]

with the natural parameter \(\pi=\ln\left(\frac{\theta}{1-\theta}\right)\in\Pi=\mathbb{R}\) (corresponding to \(\theta\in\Theta=(0,1)\)). Suppose we wish to test

\[H_{0}:\theta=\theta_{0}\quad\mbox{against}\quad H_{1}:\theta=\theta_{1}\quad( \theta_{1}>\theta_{0}). \tag{5.24}\]

Since \(\theta\to\pi(\theta)\) is strictly increasing, one has \(\pi_{0}\equiv\ln\left(\frac{\theta_{0}}{1-\theta_{0}}\right)<\pi_{1}=\ln \left(\frac{\theta_{1}}{1-\theta_{1}}\right)\), since \(\theta_{1}>\theta_{0}\). The best test \(\varphi^{*}\) of a given size \(\alpha\in(0,1)\) is then of the form

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\mbox{if $\sum_{1}^{n}x_{j}>k_{2}$}\\ \gamma&\mbox{if $\sum_{1}^{n}x_{j}=k_{2}$}\\ 0&\mbox{if $\sum_{1}^{n}x_{j}<k_{2}$}\end{array}\right.\]

where \(k_{2}\) (integer) and \(\gamma\) are determined by

\[\alpha =P_{\theta_{0}}\left(\sum_{1}^{n}X_{j}>k_{2}\right)+\gamma P_{ \theta_{0}}\left(\sum_{1}^{n}X_{j}=k_{2}\right)\] \[=\sum_{r=k_{2}+1}^{n}\binom{n}{r}\theta_{0}^{r}(1-\theta_{0})^{n- r}+\gamma\binom{n}{k_{2}}\theta_{0}^{k_{2}}(1-\theta_{0})^{n-k_{2}}. \tag{5.25}\]

For example, let \(n=20\), \(\theta_{0}=0.20\), \(\alpha=0.10\). Then the solution of (5.25) is given by (Look up Binomial tables with \(n=20\), \(\theta=0.20\))

\[k_{2} =6,\] \[\gamma =\frac{\alpha-P_{\theta_{0}}\left(\sum_{1}^{n}X_{j}>6\right)}{P_{ \theta_{0}}\left(\sum X_{j}=6\right)}=\frac{0.10-0.0867}{0.1091}=0.1219.\]Here, \(P_{\theta_{0}}(T>6)=0.0867(P_{\theta_{0}}(T>5)>0.10)\), and \(P_{\theta_{0}}(T=6)=0.1091\). Hence

\[\begin{split}& P_{\theta_{0}}(T>6)+\gamma P_{\theta_{0}}(T=6)=0.0867 +\left(\frac{0.10-0.0867}{0.1091}\right)0.1091\\ &=0.0867+0.10-0.0867=0.10=\alpha.\end{split}\]

Three important remarks may now be made.

_Remark 5.2_ (_Uniformly Most Powerful Tests_).: It follows from (5.13)-(5.19) (as illustrated by Examples 5.1 and 5.2 above) that the test (5.18) for \(H_{0}:\pi=\pi_{0}\) satisfying the size restriction (5.19), is _most powerful of size \(\alpha\) for every alternative \(H_{1}:\pi=\pi_{1}\), as long as \(\pi_{1}>\pi_{0}\)_. Hence this test is _uniformly most powerful of size \(\alpha\) for testing \(H_{0}:\pi=\pi_{0}\) against \(H_{1}:\pi>\pi_{0}\)_. Now compare this test \(\varphi^{*}\) against the _test \(\varphi_{\alpha}\)_ given by \(\varphi_{\alpha}(\mathbf{x})=\alpha\ \forall\ \mathbf{x}\in\mathscr{X}\) (i.e., whatever be the observation \(\mathbf{x}\), this test rejects \(H_{0}\) with probability \(\alpha\), and accepts it with probability \(1-\alpha\)). Since this test is of size \(\alpha\), it follows that \(E_{\pi}\varphi^{*}(\mathbf{X})\geq E_{\pi}\varphi_{\alpha}(\mathbf{X})=\alpha \ \forall\,\pi>\pi_{0}\). This property of a test, namely, that its power is at least as large as its size (i.e., the probability of rejecting \(H_{0}\) when it is false is at least as large as the probability of rejecting \(H_{0}\) when \(H_{0}\) is true), is called _unbiasedness_ (of the test). We now argue that \(\varphi^{*}\) is _uniformly most powerful (UMP)_ of its size, say \(\alpha\), for testing \(H_{0}:\pi\leq\pi_{0}\) against \(H_{1}:\pi>\pi_{0}\). Since it is UMP of size \(\alpha\) among all tests of size \(\alpha\) or less for testing \(H_{0}:\pi=\pi_{0}\) (against \(H_{1}:\pi>\pi_{0}\)), and since every test of size \(\alpha\) or less for \(H_{0}:\pi\leq\pi_{0}\) is also of size \(\alpha\) or less for testing \(H_{0}:\pi=\pi_{0}\), we only need to show that \(\varphi^{*}\) is of size \(\alpha\) for testing \(H_{0}:\pi\leq\pi_{0}\). Fix \(\pi_{-}<\pi_{0}\). The test \(\varphi^{*}\) is most powerful of its size, say \(\alpha_{-}\), for testing \(H_{0}:\pi=\pi_{-}\) against \(H_{1}:\pi=\pi_{0}\) (since \(\pi_{0}>\pi_{-}\)). In particular, it is at least as powerful as the test \(\varphi_{\alpha_{-}}\) (of the same size \(\alpha_{-}\)). But the power of \(\varphi^{*}\) (for testing \(H_{0}:\pi=\pi_{-}\), \(H_{1}:\pi=\pi_{0}\)) is \(E_{\pi_{0}}\varphi^{*}(\mathbf{X})=\alpha\), and that of \(\varphi_{\alpha_{-}}\) is \(E_{\pi_{0}}\varphi_{\alpha_{-}}(\mathbf{X})=\alpha_{-}\). Therefore, \(\alpha_{-}\leq\alpha\). This shows that \(E_{\pi}\varphi^{*}(\mathbf{X})\leq\alpha\ \forall\,\pi<\pi_{0}\). Hence, for testing \(H_{0}:\pi\leq\pi_{0}\) against \(H_{1}:\pi>\pi_{0}\), the size of \(\varphi^{*}\) is \(\alpha\). Incidentally, we have shown that \(E_{\pi}\varphi^{*}(\mathbf{X})\)_is monotone increasing with \(\pi\)._

Finally, reversing the inequalities in the proof of the N-P Lemma, one shows that _for testing \(H_{0}:\pi=\pi_{0}\), against \(H_{1}:\pi<\pi_{0}\), \(\varphi^{*}\) is the least powerful of all tests of its size._

One may also consider the problem of testing \(H_{0}:\sigma^{2}\leq\sigma_{0}^{2}\) against \(H_{1}:\sigma^{2}>\sigma_{0}^{2}\) based on i.i.d. observations \(X_{1},\ldots,X_{n}\) from \(N(\mu_{0},\sigma^{2})\), with \(\mu_{0}\) known (Exercise 5.9(a)). One may think of this as a statistical test to determine if a new equipment for measuring length is at least as accurate as a standard equipment, by taking measurements with the new equipment. Here \(\sigma_{0}^{2}\) is the known variance for the standard equipment.

_Remark 5.3_.: Note that for testing \(H_{0}:\pi=\pi_{0}\) against \(H_{1}:\pi=\pi_{1}\), where \(\pi_{1}<\pi_{0}\), the most powerful test, given by the N-P Lemma in the case of a one-parameter exponential family (5.13)-(5.16) becomes instead of (5.17))

\[\varphi^{*}(\mathbf{x})=\begin{cases}1&\text{ if }T(\mathbf{x})<k_{2}\ (=(\ell_{n}k _{1})/(\pi_{1}-\pi_{0}))\\ \gamma&\text{ if }T(\mathbf{x})=k_{2}\\ 0&\text{ if }T(\mathbf{x})>k_{2}.\end{cases} \tag{5.26}\]

Arguments entirely analogous to those made for the case \(\pi_{1}>\pi_{0}\) show that the test (5.26) is UMP of its size for testing \(H_{0}:\pi\geq\pi_{0}\) (for any given \(\pi_{0}\)) against \(H_{1}:\pi<\pi_{0}\). One may also derive this fact by changing the parameter \(\pi\to-\pi\) and the statistic \(T\to-T\) (so that \(\pi_{1}<\pi_{0}\) becomes \(-\pi_{1}>-\pi_{0}\)). (Exercise 5.1.)

Remark Remark 5.4: Although one-parameter exponential families constitute the most important examples for the existence of uniformly most powerful tests for hypotheses of the kind \(H_{0}:\theta\leq\theta_{0}\) against \(H_{1}:\theta>\theta_{0}\), the property that is actually used is that of the _monotone likelihood ratio_\(f(\mathbf{x}\mid\theta_{1})/f(\mathbf{x}\mid\theta_{0})\) as a function of a (sufficient) statistic \(T:\) If \(\theta_{1}>\theta_{0}\), \(f(\mathbf{x}\mid\theta_{1})/f(\mathbf{x}\mid\theta_{0})\) is a monotone increasing (or, monotone decreasing) function of \(T\) (Exercise 5.4).

Finally, in practice, statisticians generally do not rigidly fix the size \(\alpha\) of a test. Instead, they compute the _\(P\)-value_ of the test, namely, given the observation, the _smallest value of \(\alpha\) for which \(H_{0}\) would be rejected_ (in favor of \(H_{1}\)). In Example 5.2 above, if \(n=20\), \(\theta_{0}=0.20\), (\(H_{0}:\theta=\theta_{0}\), \(H_{1}:\theta>\theta_{0}\)) and if the observed \(T(\mathbf{x})\equiv\sum_{1}^{n}x_{j}=7\), then the \(P\)-value of the test is \(P_{\theta_{0}}(T>6)=P_{\theta_{0}}(T\geq 7)=0.0867\). One would reject \(H_{0}\) here if the size, or level of significance, \(\alpha\) is larger than \(0.0867\) (e.g., \(\alpha=0.10\)). In general, a test would reject \(H_{0}\) for every \(\alpha\geq P\)-value. Hence the smaller the \(P\)-value, the stronger is the evidence against \(H_{0}\).

### The Generalized N-P Lemma and UMP Unbiased Tests

In this section we consider the problem of testing of a null hypothesis against a _two-sided alternative_

\[H_{0}:\theta=\theta_{0},\quad H_{1}:\theta\neq\theta_{0},\]

where \(\theta\) is a real parameter. It is clear that for this, a UMP test of any given size \(\alpha\in(0,1)\) does not exist in general. As pointed out in the preceding section, even in the case of a one-parameter exponential family, a UMP test for \(H_{0}:\theta=\theta_{0}\) against \(H_{1}:\theta>\theta_{0}\) is _uniformly least powerful_ (among all tests of the same size) against alternatives \(\theta<\theta_{0}\) (and vice versa). The following diagram schematically illustrates the situation (Fig. 5.1): We will, therefore, restrict our attention to the class \(\mathscr{G}\) of _unbiased tests_\(\varphi\), i.e., _power of \(\varphi\geq\) size of \(\varphi\)_, or

\[E_{\theta}\varphi(\mathbf{X})\geq\alpha=\sup_{\theta\in\Theta_{0}}E_{\theta}( \mathbf{X})\quad\forall\;\theta\in\Theta_{1}. \tag{5.27}\]

[\(\inf_{\theta\in\Theta_{1}}E_{\theta}\varphi(\mathbf{X})\geq\sup_{\theta\in \Theta_{0}}E_{\theta}\varphi(\mathbf{X})\).] Note that there always exist unbiased tests of any given size. For example, _the test_\(\varphi_{\alpha}:\varphi_{\alpha}(\mathbf{x})=\alpha\;\forall\,\mathbf{x}\in \mathscr{X}\), is of size \(\alpha\) and is unbiased (since \(E_{\theta}\varphi_{\alpha}(\mathbf{X})=\alpha\;\forall\,\theta\)). The following theorem shows that, for one-parameter exponential families, a uniformly most powerful test exists in the class of all unbiased tests of any specified size. Such tests are said to be _uniformly most powerful unbiased_ or _UMPU_.

**Theorem 5.2**.: _Consider the one-parameter exponential family (5.14), in natural parameter form, with \(\Pi\) an open interval. A UMP unbiased test \(\varphi^{*}\) of size \(\alpha\), for testing \(H_{0}:\pi=\pi_{0}\) against \(H_{1}:\pi\neq\pi_{0}\) is given by_

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\mbox{ if }T(\mathbf{x})<t_{1} \mbox{ or }>t_{2},\\ \gamma_{i}&\mbox{ if }T(\mathbf{x})=t_{i}\ (i=1,2)\\ 0&\mbox{ if }t_{1}<T(\mathbf{x})<t_{2},\end{array}\right. \tag{5.28}\]

_where \(t_{1}<t_{2}\) and \(\gamma_{i}\)\((i=1,2)\) satisfy the equations_

\[\alpha=E_{\pi_{0}}\varphi^{*}(\mathbf{X})=P_{\pi_{0}}(T<t_{1})+P_{\pi_{0}}(T>t _{2})+\gamma_{1}P_{\pi_{0}}(T=t_{1})+\gamma_{2}P_{\pi_{0}}(T=t_{2}), \tag{5.29}\]

_and all tests \(\varphi\) (including \(\varphi^{*}\)) in \(\mathscr{G}\) satisfy_

\[0=\left(\frac{d}{d\pi}\,E_{\pi}\varphi(\mathbf{X})\right)_{\pi=\pi_{0}}\qquad \mbox{ (Unbiasedness),} \tag{5.30}\]

_or, equivalently, the relation (5.36) below, namely._

\[E_{\pi_{0}}\varphi(\mathbf{X})T(\mathbf{X})=\alpha E_{\pi_{0}}T(\mathbf{X}).\]

The following generalized version of the N-P Lemma is needed for the proof.

**Theorem 5.3** (Generalized N-P Lemma).: _Let \(f_{i}\), \(1\leq i\leq m+1\), be functions on \(\mathscr{X}\), and \(\mathscr{G}\) the class of tests \(\varphi\), \(0\leq\varphi\leq 1\), satisfying_

\[\int_{\mathscr{X}}f_{i}(\mathbf{x})\varphi(\mathbf{x})d\mu(\mathbf{x})=c_{i} \qquad(1\leq i\leq m). \tag{5.31}\]

_If \(\varphi^{*}\) is in \(\mathscr{G}\) and is of the form_

\[\varphi^{*}(\mathbf{x})=\left\{\begin{array}{ll}1&\mbox{ if }f_{m+1}(\mathbf{x})> \sum_{i=1}^{m}k_{i}f_{i}(\mathbf{x})\mbox{,}\\ \gamma(\mathbf{x})&\mbox{ if }f_{m+1}(\mathbf{x})=\sum_{i=1}^{m}k_{i}f_{i}( \mathbf{x})\mbox{,}\\ 0&\mbox{ if }f_{m+1}(\mathbf{x})<\sum_{i=1}^{m}k_{i}f_{i}(\mathbf{x})\mbox{,} \end{array}\right. \tag{5.32}\]

_for some constants \(k_{i}\)\((1\leq i\leq m)\) and some measurable \(\gamma(\mathbf{x})\), \(0\leq\gamma(\mathbf{x})\leq 1\), then_

\[\int_{\mathscr{X}}\varphi^{*}(\mathbf{x})f_{m+1}(\mathbf{x})d\mu(\mathbf{x})= \sup_{\varphi\in\mathscr{G}}\int_{\mathscr{X}}\phi(\mathbf{x})f_{m+1}(\mathbf{ x})d\mu(\mathbf{x}). \tag{5.33}\]

Proof.: Let \(\varphi\in\mathscr{G}\). Then the function \((\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x}))(f_{m+1}(\mathbf{x})-\sum_{i=1}^ {m}k_{i}f(\mathbf{x}))\) is nonnegative on \(\mathscr{X}\). Therefore, by (5.31),

\[0 \leq\int_{\mathscr{X}}(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x }))(f_{m+1}(\mathbf{x})-\sum_{i=1}^{m}k_{i}f_{i}(\mathbf{x}))d\mu(\mathbf{x})\] \[=\int_{\mathscr{X}}(\varphi^{*}(\mathbf{x})-\varphi(\mathbf{x}))( f_{m+1}(\mathbf{x}))d\mu(\mathbf{x})\] \[=\int_{\mathscr{X}}\varphi^{*}(\mathbf{x})f_{m+1}(\mathbf{x})d \mu(\mathbf{x})-\int_{\mathscr{X}}\varphi(\mathbf{x})f_{m+1}(\mathbf{x})d\mu( \mathbf{x}).\]Next note that, for the case of a one-parameter exponential family with \(\Pi\) an open interval, one can differentiate under the integral sign (Exercise 5.4(a)):

\[\frac{d}{d\pi}\,E_{\pi}\varphi({\bf X}) = \frac{d}{d\pi}\int_{\mathscr{X}}\varphi({\bf x}){\bf C}(\pi)h({\bf x })e^{\pi T({\bf x})}d\mu({\bf x}) \tag{5.34}\] \[= {\bf C}^{\prime}(\pi)\int_{\mathscr{X}}\varphi({\bf x})h({\bf x}) e^{\pi T({\bf x})}d\mu({\bf x})\] \[+\int_{\mathscr{X}}\varphi({\bf x}){\bf C}(\pi)h({\bf x})T({\bf x })e^{\pi T({\bf x})}d\mu({\bf x})\] \[= \frac{C^{\prime}(\pi)}{C(\pi)}\,E_{\pi}\varphi({\bf X})+E_{\pi} \varphi({\bf X})T({\bf X}).\]

Now

\[0 = \frac{d}{d\pi}\int_{\mathscr{X}}{\bf C}(\pi)h({\bf x})e^{\pi T({ \bf x})}d\mu({\bf x})\] \[= {\bf C}^{\prime}(\pi)\int_{\mathscr{X}}h({\bf x})e^{\pi T({\bf x })}d\mu({\bf x})+\int_{\mathscr{X}}{\bf C}(\pi)h({\bf x})T({\bf x})e^{\pi T({ \bf x})}d\mu({\bf x})\] \[= \frac{{\bf C}^{\prime}(\pi)}{{\bf C}(\pi)}+E_{\pi}T({\bf X}),\]

or,

\[E_{\pi}T({\bf X})=-\frac{{\bf C}^{\prime}(\pi)}{{\bf C}(\pi)}\,. \tag{5.35}\]

Hence the condition (5.30) may be expressed as

\[E_{\pi_{0}}\varphi({\bf X})T({\bf X}) = E_{\pi_{0}}\varphi({\bf X})\cdot E_{\pi_{0}}T({\bf X}) \tag{5.36}\] \[= \alpha E_{\pi_{0}}T({\bf X}).\]

To prove Theorem 5.2, use the Generalized N-P Lemma with \(f_{1}({\bf x})=f({\bf x}\mid\pi_{0})\), \(f_{2}({\bf x})=T({\bf x})f({\bf x}\mid\pi_{0})\), \(c_{1}=\alpha\), \(c_{2}=\alpha\,E_{\pi_{0}}T({\bf X})\), and \(f_{3}({\bf x})=f({\bf x}\mid\pi)\) for some \(\pi\neq\pi_{0}\).

_Case A. Fix \(\pi\), \(\pi_{0}\) with \(\pi>\pi_{0}\)._ The inequality \(f_{3}({\bf x})>k_{1}f_{1}({\bf x})+k_{2}f_{2}({\bf x})\) may be written as \({\bf C}(\pi)\exp\{\pi T({\bf x})\}h({\bf x})\ >\ k_{1}{\bf C}(\pi_{0})\exp\{\pi_ {0}T({\bf x})\}h({\bf x})\ +\ k_{2}{\bf C}(\pi_{0})\ \exp\{\pi_{0}T({\bf x}) \}h({\bf x})T({\bf x})\), or, using different constants, \(\exp\{\pi T({\bf x})\}>k^{\prime}_{1}\exp\{\pi_{0}T({\bf x})\}\ +k^{\prime}_{2} \exp\{\pi_{0}T({\bf x})\}T({\bf x})\), or

\[\exp\{(\pi-\pi_{0})T({\bf x})\}>k^{\prime}_{1}+k^{\prime}_{2}T({\bf x}),\quad[ k^{\prime}_{i}=k_{i}{\bf C}(\pi_{0})/{\bf C}(\pi)]. \tag{5.37}\]

For a given pair \(t_{1}<t_{2}\), one may find \(k^{\prime}_{1}\), \(k^{\prime}_{2}>0\) such that the function \(g(t):=\exp\{(\pi-\pi_{0})t\}\) satisfies \(g(t)=k^{\prime}_{1}+k^{\prime}_{2}t\) for the values \(t=t_{1}\), \(t=t_{2}\) [i.e., \(t\to k^{\prime}_{1}+k^{\prime}_{2}t\) is the line passing through the points \((t_{1},g(t_{1}))\) and \((t_{2},g(t_{2}))\).] See Fig. 5.2a.

Then "\(t<t_{1}\) or \(t>t_{2}\)" is equivalent to "\(g(t)>k^{\prime}_{1}+k^{\prime}_{2}t\)", and "\(t_{1}<t<t_{2}\)" means "\(g(t)<k^{\prime}_{1}+k^{\prime}_{2}t\)", "\(t=t_{i}\) for \(i=1\) or 2" means "\(g(t)=k^{\prime}_{1}+k^{\prime}_{2}t\)". Hence the test \(\varphi^{*}\) given by (5.28) may be expressed in the form (5.32) which, by the Generalized N-P Lemma, is uniformly most powerful in the class of all tests \(\varphi\) satisfying (1) \(E_{\pi_{0}}\varphi({\bf X})\equiv\int_{\mathscr{X}}\varphi({\bf x})f_{1}({\bf x })d\mu({\bf x})=\alpha\), (2) \(\alpha E_{\pi_{0}}T({\bf X})\equiv\int_{\mathscr{X}}\varphi({\bf x})f_{2}({\bf x })d\mu({\bf x})\). Since (5.30) is equivalent to (5.36), this class of tests is precisely the class of all tests of \(H_{0}:\pi=\pi_{0}\), \(H_{1}:\pi\neq\pi_{0}\) of size \(\alpha\), satisfying (5.30). Since this last class includes the class of all unbiased tests of size \(\alpha\), the proof of Theorem 5.2 is complete in the Case A. The _Case B:_\(\pi<\pi_{0}\), is entirely analogous. In this case one takes \(k^{\prime}_{2}<0\), and Fig. 5.2b applies.

It remains to show that for any \(\alpha\in(0,1)\) there exists a test of the form (5.28) with \(t_{1}<t_{2}\), \(\gamma_{i}\) (\(i=1,2\)) satisfying (5.29) and (5.30). The proof in the present generality is long1 and is omitted. Instead, Examples 5.3-5.5 and a number of Exercises illustrate the construction of UMPU tests for arbitrary sizes \(\alpha\), \(0<\alpha<1\). 

Footnote 1: See Ferguson (1967, pp. 215221).

_Example 5.3._ Let \(\mathscr{X}=\mathbb{R}^{n}\), \(f(\mathbf{x}\mid\theta)=f(\mathbf{x}\mid\pi)=(2\pi)^{-n/2}\exp\{-\frac{1}{2} \sum_{1}^{n}(x_{j}-\pi)^{2}\}=\mathbf{C}(\pi)h(\mathbf{x})\exp\{\pi\sum_{j=1}^{ n}x_{j}\}\), where \(h(\mathbf{x})=\exp\{-\frac{1}{2}\sum_{j=1}^{n}x_{j}^{2}\}\). That is, \(\mathbf{X}=(X_{1},\ldots,X_{n})\), where \(X_{i}\) are i.i.d. Normal \(\mathbf{N}(\theta,1)\equiv\mathbf{N}(\pi,1)\). For a given \(\pi_{0}\in\mathbb{R}\), we will construct the UMPU test of size \(\alpha\) for

\[H_{0}:\theta=0,\quad H_{1}:\theta\neq 0. \tag{5.38}\]

By Theorem 5.2, the desired test is of the form

\[\varphi^{*}(\mathbf{X})=\left\{\begin{array}{ll}1&\mbox{ if }T\equiv\sum_{1}^{ n}X_{j}<t_{1}\mbox{ or }>t_{2}\\ 0&\mbox{ if }t_{1}\leq T\leq t_{2},\end{array}\right. \tag{5.39}\]

where

\[P_{0}(t_{1}\leq T\leq t_{2})=1-\alpha, \tag{5.40}\]

and

\[E_{0}\varphi^{*}(\mathbf{X})T=\alpha E_{0}T=0. \tag{5.41}\]

Now (5.40) is equivalent to

\[P_{0}\left(c_{1}\leq\frac{T}{\sqrt{n}}\leq c_{2}\right)=1-\alpha\quad\left(c_ {i}=\tfrac{t_{i}}{\sqrt{n}},\ i=1,2\right), \tag{5.42}\]

and (5.41) is equivalent to \(E_{0}(1-\varphi^{*}(\mathbf{X}))T=0\), or \(E_{0}(1-\varphi^{*}(\mathbf{X}))\frac{T}{\sqrt{n}}=0\), or, noting that \(\frac{T}{\sqrt{n}}\) is \(\mathbf{N}(0,1)\) under \(P_{0}\),

\[\int_{c_{1}}^{c_{2}}z\varphi(z)dz=0\qquad\left(z=\tfrac{t}{\sqrt{n}}\right). \tag{5.43}\]

Since \(z\varphi(z)\) is an _odd function,_ the last condition implies \(c_{1}=-c_{2}\) (\(c_{2}>0\)). Then (5.42) fixes \(c_{2}=\Phi^{-1}\left(1-\frac{\alpha}{2}\right)\), where \(\Phi(x)\) is the cumulative distribution function of the standard Normal distribution. Hence the UMPU test of size \(\alpha\) is given by

\[\mbox{Reject }H_{0}\mbox{ iff }\left|\frac{\sum_{j=1}^{n}X_{j}}{\sqrt{n}} \right|>\Phi^{-1}\left(1-\frac{\alpha}{2}\right)=z_{1-\frac{\alpha}{2}},\ \mbox{say}. \tag{5.44}\]_Example 5.4_.: Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. gamma \(\mathscr{G}(\theta,\beta),\)_with \(\beta>0\) known,_ i.e., with density (w.r.t. Lebesgue measure on \((0,\infty)\)) given by

\[f(x\mid\theta)=\frac{1}{\Gamma(\beta)\theta^{\beta}}\,e^{-x/\theta}x^{\beta-1},\quad 0<x<\infty.\]

The (joint) density of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) (w.r.t. Lebesgue measure on \((0,\infty)^{n}=\mathscr{X}\)) is

\[\mathbf{f}(\mathbf{x}\mid\theta) =\left(\frac{1}{\Gamma(\beta)\theta^{\beta}}\right)^{n}e^{-\frac {1}{\theta}\sum_{j=1}^{n}x_{j}}\prod_{j=1}^{n}x_{j}^{\beta-1}\] \[=C(\theta)h(\mathbf{x})\exp\{\pi T\}\qquad\left(\pi=-\tfrac{1}{ \theta},\ T(\mathbf{x})=\sum_{1}^{n}x_{j}\right)\]

with the natural parameter \(\pi=-\frac{1}{\theta},\) and the complete sufficient statistic \(T=\sum_{j=1}^{n}X_{j}\) (for the one-parameter exponential family of probability measures \(\{P_{\pi}:\pi=-\frac{1}{\theta}\in(-\infty,0)=\Pi\}\)). By Theorem 5.2, the UMPU test of size \(\alpha\) for

\[H_{0}:\theta=1\,\text{ i.e., }\pi=-1,\ \ H_{1}:\theta\neq 1 \tag{5.45}\]

is given by

\[\varphi^{*}(\mathbf{X})=\left\{\begin{array}{ll}1&\text{ if }T<t_{1}\text{ or }T>t_{2},\\ 0&\text{ if }t_{1}\leq T\leq t_{2},\end{array}\right.\]

where, using the fact that, under \(P_{0},\)\(T\) has the gamma distribution \(\mathscr{G}(1,n\beta)\) with density

\[g(t)=\frac{1}{\Gamma(n\beta)}\,e^{-t}t^{n\beta-1},\qquad 0<t<\infty, \tag{5.46}\]

one has

\[\int_{t_{1}}^{t_{2}}g(t)dt=1-\alpha,\quad\int_{t_{1}}^{t_{2}}tg(t)dt=(1-\alpha )E_{0}T=(1-\alpha)(n\beta). \tag{5.47}\]

The second integral above is

\[(1-\alpha)n\beta =\int_{t_{1}}^{t_{2}}\frac{1}{\Gamma(n\beta)}\,t^{n\beta}d(-e^{-t})\] \[=\frac{1}{\Gamma(n\beta)}\left\{t_{1}^{n\beta}e^{-t_{1}}-t_{2}^{n \beta}e^{-t_{2}}\right\}+\frac{n\beta}{\Gamma(n\beta)}\int_{t_{1}}^{t_{2}}t^{n \beta-1}e^{-t}dt\] \[=\frac{1}{\Gamma(n\beta)}\left\{t_{1}^{n\beta}e^{-t_{1}}-t_{2}^{n \beta}e^{-t_{2}}\right\}+n\beta(1-\alpha),\]

that is, one has \(t_{1}^{n\beta}e^{-t_{1}}-t_{2}^{n\beta}e^{-t_{2}}=0,\) which may be expressed as

\[\left(\frac{t_{2}}{t_{1}}\right)^{n\beta}=e^{t_{2}-t_{1}}. \tag{5.48}\]

Thus \(t_{1}<t_{2}\) are determined by the size restriction (or, the first integral in (5.47) and (5.48)), usually by trial and error, using interpolation and a table of gamma probabilities.

_Example 5.5_.: Let \(X_{j}\)'s (\(1\leq j\leq n\)) be i.i.d. \(\mathbf{N}(0,\theta),\)\(\theta\in\Theta=(0,\infty).\) Then the (joint) density of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) (w.r.t. Lebesgue measure on \(\mathbb{R}^{n}\)) is \[{\bf f}({\bf x}\mid\theta)=(2\pi\theta)^{-n/2}\exp\left\{-\frac{1}{2\theta}\sum_{j= 1}^{n}x_{j}^{2}\right\},\]

which is the density of a one-parameter exponential family with natural parameter \(\pi=-\frac{1}{\theta}\in\Pi=(-\infty,0)\), and a complete sufficient statistic \(T=\sum_{j=1}^{n}X_{j}^{2}/2\). The distribution of \(T\) is gamma \(\mathscr{G}(\theta,n/2)\) (See the Appendix on Standard Distributions) with density (w.r.t. Lebesgue measure on \((0,\infty)\)) given by

\[g(t)=\frac{1}{\Gamma\left(\frac{n}{2}\right)\theta^{n/2}}\,e^{-t/\theta}t^{n/2 -1},\quad 0<t<\infty, \tag{5.49}\]

constituting a one-parameter exponential family, the same as in Example 5.4, with \(\beta=1/2\). Hence the UMPU test for \(H_{0}:\theta=1\), \(H_{1}:\theta\neq 1\), is given by

\[\mbox{Reject }H_{0}\mbox{ iff }\quad\frac{\sum_{1}^{n}X_{j}^{2}}{2}<t_{1}\quad \mbox{or}\quad>t_{2}, \tag{5.50}\]

where \(t_{1}\) and \(t_{2}\) are determined by the first (size) condition in (5.47), and (5.48) with \(\beta=\frac{1}{2}\).

Note that statistical tables are more readily available for the _chi-square distributions_\(\mbox{Chi}(n)=\mathscr{G}(2,n/2)\) than for general gamma distributions. Hence the test is generally expressed in terms of the chi-square _random variable_\(U=\sum_{j=1}^{n}X_{j}^{2}\)_with_\(n\)_degrees of freedom,_ so that (5.50) becomes

\[\mbox{Reject }H_{0}\mbox{ iff: }\quad U<u_{1}\quad\mbox{or}\quad>u_{2}, \tag{5.51}\]

where \(u_{i}=2t_{i}\) (\(i=1,2\)). That is,

\[P(u_{1}\leq U\leq u_{2})=1-\alpha,\quad\left(\frac{u_{2}}{u_{1}}\right)^{\frac {n}{2}}=e^{\frac{u_{2}}{2}-\frac{u_{1}}{2}}. \tag{5.52}\]

### UMP Unbiased Tests in the Presence of Nuisance Parameters

Implicitly or explicitly, all optimal statistical procedures so far have been shown to be based on sufficient statistics. Since their considerations are particularly important in this section, we begin with a simple affirmation that it is enough to restrict attention to functions of sufficient statistics.

**Proposition 5.1**.: _Let \(T=t({\bf X})\) be a sufficient statistic for \(\{P_{\theta}:\theta\in\Theta\}\). Given a test \(\varphi=\varphi({\bf X})\) for \(H_{0}:\theta\in\Theta_{0}\), \(H_{1}:\theta\in\Theta_{1}=\Theta\backslash\Theta_{0}\), there exists a test \(\psi=\psi(T)\) which has the same performance as \(\varphi\), in the sense \(E_{\theta}\psi(T)=E_{\theta}\varphi({\bf X})\)\(\forall\,\theta\in\Theta\)._

Proof.: Let \(\psi(T)=E_{\theta}(\varphi({\bf X})\mid T)\). Then \(\psi\) does not depend on \(\theta\) and is (hence) a statistic, \(0\leq\psi(T)\leq 1\) (almost surely \((P_{\theta})\)\(\forall\,\theta\)), and it satisfies \(E_{\theta}\psi(T)=E_{\theta}[E_{\theta}(\varphi({\bf X})\mid T)]=E_{\theta}\varphi ({\bf X})\)\(\forall\,\theta\in\Theta\). 

Given a parameter set \(\Theta\) which is a metric space, and nonempty subsets \(\Theta_{0}\), \(\Theta_{1}=\Theta\backslash\Theta_{0}\), let \(\Theta_{B}\) be the _boundary_ of \(\Theta_{0}\) (or, of \(\Theta_{1}\)), i.e., \(\Theta_{B}=\overline{\Theta}_{0}\cap\overline{\Theta}_{1}\).

**Proposition 5.2**.: _If the power function \(\theta\to E_{\theta}\varphi({\bf X})\) of a test is continuous on \(\Theta\), then every unbiased test \(\varphi\) of size \(\alpha\) has the property_

\[E_{\theta}\varphi({\bf X})=\alpha\qquad\forall\ \theta\in\Theta_{B}. \tag{5.53}\]

Proof.: Let \(\theta\in\Theta_{B}\). Then \(\exists\) sequences \(\{\theta_{n}^{(i)}:n=1,2,\ldots\}\subset\Theta_{i}\)\((i=0,1)\) such that \(\theta_{n}^{(i)}\to\theta\) as \(n\to\infty\)\((i=1,2)\). If \(i=0\), then the size restriction of \(\varphi\) implies \(E_{\theta_{n}^{(0)}}\varphi({\bf X})\leq\alpha\)\(\forall\,n\), so that, by continuity of the power function, \(E_{\theta}\varphi({\bf X})\leq\alpha\). Similarly, unbiasedness of \(\varphi\) implies \(E_{\theta_{n}^{(1)}}\varphi({\bf X})\geq\alpha\)\(\forall\,n\), and therefore \(E_{\theta}\varphi({\bf X})\geq\alpha\). 

**Definition 5.4**.: A test \(\varphi\) satisfying (5.53) is called \(\alpha\)_-similar._

**Proposition 5.3**.: _Suppose the power function \(\theta\to E_{\theta}\varphi({\bf X})\) is continuous on \(\Theta\) for all tests \(\varphi\). If \(\varphi^{*}\) is UMP in the class of all \(\alpha\)-similar tests, and if \(\varphi^{*}\) is of size \(\alpha\), then \(\varphi^{*}\) is UMP in the class of all unbiased tests of size \(\alpha\) of \(H_{0}:\theta\in\Theta_{0}\), \(H_{1}:\theta\in\Theta_{1}\)._

Proof.: The test \(\varphi_{\alpha}({\bf x})=\alpha\)\(\forall\)\({\bf x}\) is \(\alpha\)-similar. Therefore, \(E_{\theta}\varphi^{*}({\bf X})\geq E_{\theta}\varphi_{\alpha}({\bf X})=\alpha\)\(\forall\,\theta\in\Theta_{1}\). Thus \(\varphi^{*}\) is unbiased (and of size \(\alpha\), by assumption). On the other hand, by Proposition 5.2, every unbiased test of size \(\alpha\) is \(\alpha\)-similar. Since \(\varphi^{*}\) is UMP in the (bigger) class of all \(\alpha\)-similar tests, \(\varphi^{*}\) is UMP in the class of all unbiased tests of size \(\alpha\). 

Consider now the problem of testing

\[H_{0}:\theta_{1}\leq\theta_{1}^{0},\qquad H_{1}:\theta_{1}>\theta_{1}^{0}, \tag{5.54}\]

where \(\theta_{1}\) is the first coordinate of the parameter \(\theta=(\theta_{1},\theta_{2},\ldots,\theta_{k})\), parametrizing the family of probability measures \(\{P_{\theta}:\theta\in\Theta\}\), \(\Theta\subset\mathbb{R}^{k}\). Here \(\Theta_{0}=\{\theta\in\Theta:\theta_{1}\leq\theta_{1}^{0}\}\), \(\Theta_{1}=\{\theta\in\Theta:\theta_{1}>\theta_{1}^{0}\}\), and \(\Theta_{B}=\{\theta\in\Theta:\theta_{1}=\theta_{1}^{0}\}\). Suppose there exists a _sufficient statistic_\(T_{B}\) for the family \(\{P_{\theta}:\theta\in\Theta_{B}\}\). Given any \(\alpha\)-similar test \(\varphi({\bf X})\), the random variable

\[\psi(T_{B})=E_{\theta}[\varphi({\bf X})\mid T_{B}]\qquad(\theta\in\Theta_{B}) \tag{5.55}\]

does not involve any unknown parameter and is, therefore, a test, which is \(\alpha\)-similar.

**Definition 5.5**.: An \(\alpha\)-similar test \(\varphi({\bf X})\) is said to have the _Neyman structure_ if \(\psi(T_{B})\equiv E_{\theta}[\varphi({\bf X})\mid T_{B}]=\alpha\) a.s. \(P_{\theta}\)\(\forall\,\theta\in\Theta_{B}\).

**Theorem 5.4**.: _Suppose \(T_{B}\) is a complete sufficient statistic for the family \(\{P_{\theta}:\theta\in\Theta_{B}\}\). Then every \(\alpha\)-similar test has the Neyman structure._

Proof.: Let \(\varphi({\bf X})\) be \(\alpha\)-similar. Then, with \(\psi(T_{B})=E_{\theta}[\varphi({\bf x})\mid T_{B}]\)\((\theta\in\Theta_{B})\), one has \(E_{\theta}(\psi(T_{B})-\alpha)=0\)\(\forall\,\theta\in\Theta_{B}\). By the hypothesis of completeness, \(P_{\theta}(\psi(T_{B})=\alpha)=1\)\(\forall\,\theta\in\Theta_{B}\). 

_Remark 5.5_.: Note that the proof only required that the _sufficient statistic_\(T\) (for \(\{P_{\theta}:\theta\in\Theta_{B}\}\)) be _boundedly complete,_ i.e., if _for a bounded measurable function_\(g\), \(E_{\theta}g(T_{B})=0\)\(\forall\,\theta\in\Theta_{B}\), then \(P_{\theta}(g(T_{B})=0)=1\)\(\forall\,\theta\in\Theta_{B}\).

#### Umpu Tests in \(k\)-Parameter Exponential Families

For the problem of testing (5.54) in the case of a \(k\)-parameter exponential family with a complete sufficient statistic \(T=(T_{1},T_{2},\ldots,T_{k})\) with density (w.r.t. Lebesgue measure on an open subset \(\mathscr{T}\) of \(\mathbb{R}^{k}\))

\[g_{T}(\mathbf{t}\mid\theta)=C(\theta)h(\mathbf{t})\exp\left\{\sum_{i=1}^{k} \theta_{i}t_{i}\right\},\quad\mathbf{t}\in\mathscr{T}\subset\mathbb{R}^{k},\ (h(\mathbf{t})>0\ \forall\,\mathbf{t}\in\mathscr{T}), \tag{5.56}\]

with natural parameter \(\theta=\pi\in\Theta=\varPi\subset\mathbb{R}^{k}\), the (marginal) density of \((T_{2},\ldots,T_{k})\) is

\[\tilde{g}(t_{2},\ldots,t_{k}\mid\theta) =C(\theta)\exp\left\{\sum_{i=2}^{k}\theta_{i}t_{i}\right\}\int_{ \mathbb{R}}h(t_{1},t_{2},\ldots,t_{k})e^{\theta_{1}t_{1}}dt_{1}\] \[=C(\theta)h_{1}(t_{2},\ldots,t_{k},\theta_{1})\exp\left\{\sum_{i =2}^{k}\theta_{i}t_{i}\right\}, \tag{5.57}\] \[\quad(t_{2},\ldots,t_{k})\in\widetilde{\mathscr{T}}=\left\{(t_{2 },\ldots,t_{k})\colon h(t_{1},t_{2},\ldots,t_{k})>0\ \text{for some}\ t_{1}\right\}.\]

Dividing \(g_{T}\) by \(\tilde{g}\) we obtain the conditional density of \(T_{1}\), given \(T_{2}=t_{2},\ldots,T_{k}=t_{k}\), as

\[g_{1}(t_{1}\mid t_{2},\ldots,t_{k};\theta_{1}) =C_{1}(\theta_{1};t_{2},\ldots,t_{k})h(t_{1},t_{2},\ldots,t_{k})e^ {\theta_{1}t_{1}},\] \[\quad t_{1}\in\mathscr{T}_{1}(t_{2},\ldots,t_{k})=\{t_{1}:(t_{1}, t_{2},\ldots,t_{k})\in\mathscr{T}\}, \tag{5.58}\]

which is a one-parameter exponential family on \(\mathscr{T}_{1}(t_{2},\ldots,t_{k})\). One may consider the problem of testing (5.54) for this one-parameter exponential family (conditionally given \(T_{2}=t_{2},\ldots,T_{k}=t_{k}\)). For this family a UMP test of size \(\alpha\) is of the form

\[\varphi_{1}^{*}(T_{1}\mid T_{2}=t_{2},\ldots,T_{k}=t_{k})=\left\{\begin{aligned} 1&\text{if}\ T_{1}>t_{1}(t_{2},\ldots,t_{k}),\\ 0&\text{if}\ T_{1}\leq t_{1}(t_{2},\ldots,t_{k}), \end{aligned}\right. \tag{5.59}\]

where \(t_{1}(t_{2},\ldots,t_{k})\) is determined by

\[\int_{t_{1}(t_{2},\ldots,t_{k})}^{\infty}g_{1}(t_{1}\mid t_{2},\ldots,t_{k}; \theta_{1}^{0})dt_{1}=\alpha. \tag{5.60}\]

Not that \(T_{B}=(T_{2},\ldots,T_{k})\) is a complete sufficient statistic for the \((k-1)\)-parameter exponential family \(\{P_{\theta}:\theta\in\Theta_{B}\}\) with density [see (5.57)]

\[g_{3}(t_{2},\ldots,t_{k}\mid\theta_{2},\ldots,\theta_{k}) \equiv\tilde{g}(t_{2},\ldots,t_{k}\mid\theta_{1}^{0},\theta_{2}, \ldots,\theta_{k})\] \[=C(\theta_{1}^{0},\theta_{2},\ldots,\theta_{k})h_{1}(t_{2},\ldots,t_{k},\theta_{1}^{0})\exp\left\{\sum_{i=2}^{k}\theta_{i}t_{i}\right\}. \tag{5.61}\]

Hence, by Theorem 5.4, every \(\alpha\)-similar test for (5.54) has the Neyman structure. That is, such a test \(\varphi(T)=\varphi(T_{1},T_{2},\ldots,T_{k})\) satisfies

\[\psi(T_{B})\equiv E_{\theta}(\varphi(T)\mid T_{2},\ldots,T_{k})=\alpha\quad \text{a.s.}\ P_{\theta}\ \forall\ \theta\in\Theta_{B}. \tag{5.62}\]But among all such tests \(\varphi(T)\), \(\varphi^{*}(T)\equiv\varphi_{1}^{*}(T_{1}\mid T_{2},\ldots,T_{k})\) has the maximum conditional power:

\[E_{\theta}(\varphi^{*}(T)\mid T_{2},\ldots,T_{k})\geq E_{\theta}(\varphi(T)\mid T _{2},\ldots,T_{k})\quad(\text{a.s.}\,(P_{\theta}))\ \forall\,\theta\in\Theta_{1},\]

for all unbiased \(\varphi=\varphi(T)\) of size \(\alpha\). Taking expectation \((E_{\theta})\) on both sides, one concludes that \(\varphi^{*}\) is UMP unbiased among all unbiased tests of size \(\alpha\).

Consider now a \(k\)-parameter exponential family on a countable observation space, and let (5.56) denote the probability mass function _(pmf)_ of the complete sufficient statistic \(T=(T_{1},T_{2},\ldots,T_{k})\), with values in a (countable) set \(\mathscr{T}\). Replacing integrations by summations in (5.57), one arrives at (5.58) as the pmf of the conditional distribution of \(T_{1}\), given \(T_{2}=t_{2},\ldots,T_{k}=t_{k}\), which is a one-parameter exponential family. Reasoning as above, the UMPU test of size \(\alpha\) for (5.54) is given by

\[\varphi_{1}^{*}(T_{1}\mid T_{2}=t_{2},\ldots,T_{k}=t_{k})=\left\{\begin{array}[ ]{l}1\ \text{if}\ T_{1}>t_{1}(t_{2},\ldots,t_{k}),\\ \gamma\ \text{if}\ T_{1}=t_{1}(t_{2},\ldots,t_{k}),\\ 0\ \text{if}\ T_{1}<t_{1}(t_{2},\ldots,t_{k}),\end{array}\right. \tag{5.63}\]

where \(t_{1}(t_{2},\ldots,t_{k})\) and \(\gamma\), \(0\leq\gamma<1\), are determined by

\[\sum_{t_{1}>t_{1}(t_{2},\ldots,t_{k})}g(t_{1}\mid t_{2},\ldots,t_{k};\theta_{1} ^{0})+\gamma g(t_{1}(t_{2},\ldots,t_{k})\mid t_{2},\ldots,t_{k};\theta_{1}^{0} )=\alpha. \tag{5.64}\]

_Example 5.6_.: Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(\mathbf{N}(\mu,\sigma^{2})\), \(n\geq 2\), with joint density

\[(2\pi\sigma^{2})^{-n/2}\exp\left\{-\sum_{j=1}^{n}\frac{(x_{j}- \mu)^{2}}{2\sigma^{2}}\right\}=C(\boldsymbol{\theta})e^{\theta_{1}T_{1}( \mathbf{x})+\theta_{2}T_{2}(\mathbf{x})},\] \[\theta_{1}=\frac{\mu}{\sigma^{2}},\qquad\theta_{2}=-\frac{1}{2 \sigma^{2}},\] \[T_{1}(\mathbf{x})=\sum_{j=1}^{n}x_{j},\quad\ T_{2}(\mathbf{x})= \sum_{j=1}^{n}x_{j}^{2},\,C(\boldsymbol{\theta})=(2\pi\sigma^{2})^{-n/2}e^{n \mu^{2}/2\sigma^{2}}. \tag{5.65}\]

Here the natural parameter is \(\theta=(\theta_{1},\theta_{2})\in\mathbb{R}\times(-\infty,0)=\varTheta\). We wish to test

\[H_{0}:\mu\leq 0,\quad H_{1}:\mu>0,\quad\text{or}\] \[H_{0}:\theta_{1}\leq 0,\quad H_{1}:\theta_{1}>0. \tag{5.66}\]

By the preceding theory, the UMP unbiased test of size \(\alpha\in(0,1)\) is given by

\[\text{Reject }H_{0}\ \text{iff}\quad\sum_{1}^{n}X_{j}>\eta_{1}(T_{2}), \tag{5.67}\]

where \(\eta_{1}(t_{2})\) is determined for each value \(T_{2}=t_{2}\) so that

\[P_{\theta}\left(\sum_{1}^{n}X_{j}>\eta_{1}(t_{2})\mid T_{2}=t_{ 2}\right)=\alpha\ \text{a.s.}\ (P_{\theta}) \tag{5.68}\] \[\forall\ \theta\in\varTheta_{B}=\{(0,\theta_{2}):\theta_{2}<0\}=\{( 0,\sigma^{2}):\sigma^{2}>0\}.\]Since the conditional distribution in (5.68) (w.r.t. \(P_{\theta}\), \(\theta\in\Theta_{B}\)) depends only on \(\theta_{1}^{0}=0\), and not on \(\theta_{2}\) (or \(\sigma^{2}\)), one can find \(t_{1}\) (\(t_{2}\)), for every \(t_{2}>0\), so that (5.68) holds. However, to use standard statistical tables express the test (5.67) as

\[\text{Reject }H_{0}\text{ iff }\quad\frac{\sqrt{n}\ \overline{X}}{\sqrt{\left[ \frac{1}{n-1}\left(\sum X_{j}^{2}-n\overline{X}^{2}\right)\right]}}\equiv\frac {\sqrt{n}\,\overline{X}}{s}>c_{1}(T_{2}), \tag{5.69}\]

where \(c_{1}(t_{2})\) is determined, for \(T_{2}=t_{2}\), to satisfy

\[P_{\theta}\left(\frac{\sqrt{n}\ \overline{X}}{s}>c_{1}(t_{2})\mid T_{2}=t_{2} \right)=\alpha\quad\text{a.s.}\ P_{\theta}\ \forall\,\theta=(0,\theta_{2}). \tag{5.70}\]

Note that, for each \(t_{2}\), \(y\equiv\sum_{1}^{n}x_{j}\to\frac{\sqrt{n}\,\overline{x}}{s}=\sqrt{\frac{n-1}{ n}}\,\frac{y}{(t_{2}-y^{2}/n)^{1/2}}=h(y)\), say, is a strictly increasing function of \(y\) (for \(y^{2}/n<t_{2}\)), since the derivative of the function \(h(y)\) w.r.t. \(y\) is positive. Hence, one can find a unique \(\eta_{1}(t_{2})\) such that \(\{y>\eta_{1}(t_{2})\}\) is equivalent to \(\{h(y)>c_{1}(t_{2})\}\).

A second important fact about the \(t\)-_statistic_\(\frac{\sqrt{n}\,\overline{X}}{s}\) is that, under \(H_{0}\), i.e., w.r.t. \(P_{\theta}\), \(\theta\in\Theta_{B}\), its distribution does not depend on \(\theta\in\Theta_{B}\). From Basu's Theorem below, it then follows that \(\sqrt{n}\,\overline{X}/s\) is, under \(H_{0}\), independent of \(T_{2}\). Hence (5.70) becomes

\[P_{(0,\sigma^{2})}\left(\frac{\sqrt{n}\ \overline{X}}{s}>c_{1}(t_{2})\right)=\alpha,\]

so that \(c_{1}(t_{2})\) does not depend on \(t_{2}\) and is obtained from the standard \(t\)-table (with \(n-1\) d.f.). Then, finally, the UMP test among all unbiased tests of size \(\alpha\) is given by

\[\text{Reject }H_{0}\text{ iff }\quad\frac{\sqrt{n}\ \overline{X}}{s}>t_{1- \alpha}(n-1), \tag{5.71}\]

where \(t_{1-\alpha}(n-1)\) is the \((1-\alpha)\)th quantile of the \(t\)-distribution with \(n-1\) d.f.

_Example 5.7_.: Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. \(\mathbf{N}(\mu,\sigma^{2})\), \(n\geq 2\), \((\mu,\sigma^{2})\in\mathbb{R}\times(0,\infty)\). We wish to test, for some given \(\sigma_{0}^{2}>0\),

\[H_{0}:\sigma^{2}\leq\sigma_{0}^{2},\quad H_{1}:\sigma^{2}>\sigma_{0}^{2}. \tag{5.72}\]

As in Example 5.6, the UMP unbiased test of size \(\alpha\) is given by

\[\text{Reject }H_{0}\text{ iff }\quad\sum_{1}^{n}X_{j}^{2}>\eta_{2}(T_{1}),\]

where \(\eta_{2}(t_{1})\) is determined by

\[P_{(\mu,\sigma_{0}^{2})}\left(\sum_{1}^{n}X_{j}^{2}>\eta_{2}(t_{1})\mid T_{1} =t_{1}\right)=\alpha\quad\forall\,\mu\in\mathbb{R}\quad\text{(i.e., }\ \forall\,\theta\in\Theta_{B}).\]Since \(y=\sum_{1}^{n}x_{j}^{2}\to\sum_{1}^{n}(x_{j}-\overline{x})^{2}/\sigma_{0}^{2}=[ \sum_{1}^{n}x_{j}^{2}-n\overline{x}^{2}]/\sigma_{0}^{2}=(y-t_{1}^{2}/n)\sigma_{ 0}^{2}\) is strictly increasing (for \(y>0\), \(T_{1}=t_{1}\)), this test reduces to

\[\text{Reject }H_{0}\text{ iff }\quad\sum_{1}^{n}(X_{j}-\overline{X})^{2}/\sigma_{ 0}^{2}>c_{1}(t_{1}), \tag{5.73}\]

where \(c_{1}(t_{1})\) is determined so that

\[P_{(\mu,\sigma_{0}^{2})}\left(\sum_{1}^{n}(X_{j}-\overline{X})^{2}/\sigma_{0}^ {2}>c_{1}(t_{1})\mid T_{1}=t_{1}\right)=\alpha\quad\forall\,\mu\in\mathbb{R}. \tag{5.74}\]

But, as is well known, \(T_{1}=n\overline{X}\) and \(\sum_{1}^{n}(X_{j}-\overline{X})^{2}/\sigma_{0}^{2}\) are independent random variables (\(\forall\,(\mu,\sigma_{0}^{2})\)), the latter having a chi-square distribution with \(n-1\) degrees of freedom (See Proposition 2.1). Hence \(c_{1}(t_{1})\) does not depend on \(t_{1}\) and is given by \(\chi_{1-\alpha}^{2}(n-1)\)--the \((1-\alpha)\)th quantile of the chi-square distribution with \(n-1\) d.f.:

\[\text{Reject }H_{0}\text{ iff }\quad\sum_{1}^{n}(X_{j}-\overline{X})^{2}/ \sigma_{0}^{2}>\chi_{1-\alpha}^{2}(n-1). \tag{5.75}\]

### Basu's Theorem

In this section we use a useful result of Basu (1959) to compute certain UMPU tests.

**Theorem 5.5** (Basu's Theorem): _Let \(\mathbf{X}\) have distribution \(P_{\theta}\), \(\theta\in\widetilde{\Theta}\). Suppose \(T\) is a (boundedly) complete sufficient statistic for \(\{P_{\theta}:\theta\in\widetilde{\Theta}\}\). If \(Y\) is a statistic whose distribution does not depend on \(\theta\in\widetilde{\Theta}\) (i.e., it is the same for all \(\theta\in\widetilde{\Theta}\)), then \(T\) and \(Y\) are independent under \(P_{\theta}\)\(\forall\,\theta\in\widetilde{\Theta}\)._

Proof: We need to show that for every pair of bounded real-valued measurable statistics \(g(T)\) and \(h(Y)\), depending only on \(T\) and \(Y\), respectively, one has

\[E_{\theta}(g(T)h(Y))=E_{\theta}(g(T))\cdot E_{\theta}(h(Y)),\quad\forall\, \theta\in\widetilde{\Theta}. \tag{5.76}\]

Since \(c\equiv E_{\theta}h(Y)\) does not depend on \(\theta\), and neither does \(V(T)=E_{\theta}(h(Y)\mid T)\), we have \(E_{\theta}(V(T)-c)=0\)\(\forall\,\theta\in\widetilde{\Theta}\), so that, by (bounded) completeness of \(T\), \(V(T)=c\) a.s. \((P_{\theta})\)\(\forall\,\theta\in\widetilde{\Theta}\). Hence

\[E_{\theta}(g(T)(h(Y)-c)) =E_{\theta}[g(T)E_{\theta}(h(Y)-c\mid T)]\] \[=E_{\theta}[g(T)(V(T)-c)] =0\qquad\forall\,\,\theta\in\widetilde{\Theta}.\]

Therefore, (5.76) holds. 

We now apply the theory of this section to the so-called _two-sample problems_.

Example 5.8: Let \(X_{1},\ldots,X_{m}\), and \(Y_{1},Y_{2},\ldots,Y_{n}\) be independent samples from \(\mathbf{N}(\mu_{1},\sigma_{0}^{2})\) and \(\mathbf{N}(\mu_{2},\sigma_{0}^{2})\), respectively, where \(\sigma_{0}^{2}>0\) is given, and \(\mu_{1}\), \(\mu_{2}\) are unknown means. The (joint) density of these \(m+n\) observations is \[f({\bf x},{\bf y}\mid\mu_{1},\mu_{2}) =(2\pi\sigma_{0}^{2})^{-(m+n)/2}\exp\left\{-\frac{1}{2\sigma_{0}^{2} }\sum_{1}^{m}x_{j}^{2}+\frac{m\mu_{1}\overline{x}}{\sigma_{0}^{2}}-\frac{m\mu_{ 1}^{2}}{2\sigma_{0}^{2}}\right\}\] \[\quad\cdot\exp\left\{-\frac{1}{2\sigma_{0}^{2}}\sum_{1}^{n}y_{j}^ {2}+\frac{n\mu_{2}\overline{y}}{\sigma_{0}^{2}}-\frac{n\mu_{2}^{2}}{2\sigma_{0 }^{2}}\right\}\] \[=C(\mu_{1},\mu_{2})h({\bf x},{\bf y})e^{(\mu_{1}/\sigma_{0}^{2}) \sum_{1}^{m}x_{j}+(\mu_{2}/\sigma_{0}^{2})\sum_{1}^{n}y_{j}}\] \[=C(\mu_{1},\mu_{2})h({\bf x},{\bf y})\exp\left\{\left(\frac{\mu_{ 1}}{\sigma_{0}^{2}}-\frac{\mu_{2}}{\sigma_{0}^{2}}\right)\sum_{1}^{m}x_{j}+ \frac{\mu_{2}}{\sigma_{0}^{2}}\left(\sum_{1}^{m}x_{j}+\sum_{1}^{n}y_{j}\right)\right\}\] \[=\widetilde{C}(\theta_{1},\theta_{2})h({\bf x},{\bf y})\exp\{ \theta_{1}T_{1}({\bf x})+\theta_{2}T_{2}({\bf x},{\bf y})\}, \tag{5.77}\]

where \(\theta_{1}=(\mu_{1}-\mu_{2})/\sigma_{0}^{2}\), \(\theta_{2}=\mu_{2}/\sigma_{0}^{2}\), \(T_{1}({\bf x})=\sum_{1}^{m}x_{j}\), \(T_{2}({\bf x},{\bf y})=\sum_{1}^{m}x_{j}+\sum_{1}^{n}y_{j}\). Thus \(\{P_{\theta}:\theta\in\Theta\}\), with \(\Theta=\mathbb{R}^{2}\), is a two-parameter exponential family, with natural parameter \(\theta=(\theta_{1},\theta_{2})\). We wish to test

\[H_{0}:\mu_{1}\leq\mu_{2},\quad H_{1}:\mu_{1}>\mu_{2}, \tag{5.78}\]

which may be cast as

\[H_{0}:\theta_{1}\leq 0,\quad H_{1}:\theta_{1}>0.\]

By the preceding theory, a UMP unbiased test of size \(\alpha\) is given by

\[\mbox{Reject }H_{0}\mbox{ iff }\quad T_{1}>\eta_{1}(T_{2}), \tag{5.79}\]

where \(\eta_{1}(t_{2})\) is determined, for every value \(t_{2}\) of \(T_{2}\), such that

\[P_{\theta}(T_{1}>\eta_{1}(t_{2})\mid T_{2}=t_{2})=\alpha\quad\forall\,\theta \in\Theta_{B}=\{(0,\theta_{2}):\theta_{2}\in\mathbb{R}\}.\]

Since \(\overline{X}-\overline{Y}=\left(\frac{1}{m}+\frac{1}{n}\right)m\overline{X}- \frac{1}{n}(m\overline{X}+n\overline{Y})=\left(\frac{1}{m}+\frac{1}{n}\right) T_{1}-\frac{1}{n}\,T_{2}\), which is a strictly increasing function of \(T_{1}\), for any given value of \(T_{2}\), the test may be expressed as

\[\mbox{\it Reject }H_{0}\mbox{ iff }\quad\frac{\overline{X}-\overline{Y}}{\sigma_{0 }\sqrt{\frac{1}{n}+\frac{1}{n}}}>c(T_{2}), \tag{5.80}\]

where

\[P_{\theta}\left(\frac{\overline{X}-\overline{Y}}{\sigma_{0}\sqrt{\frac{1}{m }+\frac{1}{n}}}>c(T_{2})\mid T_{2}\right)=\alpha\quad\forall\,\theta=(0, \theta_{2})\in\Theta_{B}.\]

But the \(P_{\theta}\)-distribution of \(\frac{\overline{X}-\overline{Y}}{\sigma_{0}\sqrt{\frac{1}{m}+\frac{1}{n}}}\) is \({\bf N}(0,1)\), which does not involve \(\theta\), \(\forall\,\theta\in\Theta_{B}\). Hence, by Basu's Theorem (or, checking that this Normal random variable and the Normal random variable \(T_{2}\) are uncorrelated \(\forall\,\theta\)), \(\frac{\overline{X}-\overline{Y}}{\sigma_{0}\sqrt{\frac{1}{m}+\frac{1}{n}}}\) is independent of \(T_{2}\). Therefore, \(c(T_{2})=z_{1-\alpha}=\Phi^{-1}(1-\alpha)\), and the UMP unbiased test is given by

\[\mbox{Reject }H_{0}\mbox{ iff }\quad\frac{\overline{X}-\overline{Y}}{\sigma_{0} \sqrt{\frac{1}{m}+\frac{1}{n}}}>z_{1-\alpha}. \tag{5.81}\]

_Example 5.9_.: Let \(X_{1},X_{2},\ldots,X_{m}\) and \(Y_{1},Y_{2},\ldots,Y_{n}\) be independent random samples from \({\bf N}(\mu_{1},\sigma^{2})\) and \({\bf N}(\mu_{2},\sigma^{2})\), respectively, where \(\mu_{1}\), \(\mu_{2}\), \(\sigma^{2}\) are all unknown. We wish to find the UMP unbiased test of size \(\alpha\) for

\[H_{0}:\mu_{1}\leq\mu_{2},\quad H_{1}:\mu_{1}>\mu_{2}. \tag{5.82}\]The joint density of the observations maybe expressed as [see (5.77)]

\[f({\bf x},\mathbf{\mu}\mid\mu_{1},\mu_{2},\sigma^{2})=C(\mu_{1},\mu_{2}, \sigma^{2})\exp\{\theta_{1}T_{1}({\bf x})+\theta_{2}T_{2}({\bf x})+\theta_{3}T_ {3}({\bf x})\}, \tag{5.83}\]

where \(\theta_{1}=\frac{\mu_{1}}{\sigma^{2}}-\frac{\mu_{2}}{\sigma^{2}}\), \(\theta_{2}=\frac{\mu_{2}}{\sigma^{2}}\), \(\theta_{3}=-\frac{1}{2\sigma^{2}}\), and \(T_{1}({\bf x})=\sum_{1}^{m}x_{j}\), \(T_{2}({\bf x})=\sum_{1}^{m}x_{j}+\sum_{1}^{n}y_{j}\), \(T_{3}({\bf x})=\sum_{1}^{m}x_{j}^{2}+\sum_{1}^{n}y_{j}^{2}\). \(\theta=(\theta_{1},\theta_{2},\theta_{3})\) is the natural parameter, \(\theta\in\Theta=\mathbb{R}\times\mathbb{R}\times(-\infty,0)\), and \((T_{1},T_{2},T_{3})\) is a complete sufficient statistic for \(\{P_{\theta}:\theta\in\Theta\}\), while \((T_{2},T_{3})\) is a complete sufficient statistic for \(\{P_{\theta}:\theta\in\Theta_{B}\}\) with \(\Theta_{B}=\{(\theta_{1},\theta_{2},\theta_{3})\in\Theta:\theta_{1}=0\}\). Hence the UMP unbiased test of size \(\alpha\) for (5.82) (i.e., for \(H_{0}:\theta_{1}\leq 0\), \(H_{1}:\theta_{1}>0\)) is given by

\[\mbox{Reject $H_{0}$ iff}\quad T_{1}>\eta_{1}(T_{2},T_{3}), \tag{5.84}\]

where \(\eta_{1}(T_{2},T_{3})\) is determined by

\[P_{\theta}(T_{1}>\eta_{1}(T_{2},T_{3})\mid T_{2},T_{3})=\alpha\quad\forall\ \theta\in\Theta_{B}. \tag{5.85}\]

We will show that this is equivalent to the classical test which replaces the \(\sigma_{0}\) in (5.80) (which is here unknown) by its _pooled estimate_

\[s_{p}=\sqrt{\frac{\sum(X_{j}-\overline{X})^{2}+\sum(Y_{j}- \overline{Y})^{2}}{m+n-2}} =\sqrt{\frac{\sum X_{j}^{2}-\frac{(\sum X_{j})^{2}}{m}+\sum Y_{j}^{2}- \frac{(\sum Y_{j})^{2}}{n}}{m+n-2}}\] \[=\sqrt{\frac{T_{3}-\frac{T_{1}^{2}}{m}-\frac{(T_{2}-T_{1})^{2}}{n }}{m+n-2}}\,. \tag{5.86}\]

We have shown in Example 5.8 that \((\overline{X}-\overline{Y})/\sqrt{\frac{1}{m}+\frac{1}{n}}\) is an increasing function of \(T_{1}\), for any given \(T_{2}\). It is easy to check, using (5.86), that the derivative of \(s_{p}^{2}\) w.r.t. \(T_{1}\) (for fixed \(T_{2}\),\(T_{3}\)) is a decreasing function of \(T_{1}\). Therefore,

\[T_{1}\longrightarrow\tau=\frac{\overline{X}-\overline{Y}}{s_{p}\sqrt{\frac{1} {m}+\frac{1}{n}}}\mbox{ is an increasing function of $T_{1}$, for given $T_{2}$ and $T_{3}$.}\]

Hence the test (5.84) is equivalent to

\[\mbox{Reject $H_{0}$ iff}\quad\tau=\frac{\overline{X}-\overline{Y}}{s_{p}\sqrt{\frac {1}{m}+\frac{1}{n}}}>c(T_{2},T_{3})\]

where \(c(T_{2},T_{3})\) is determined from

\[P_{\theta}(\tau>c(T_{2},T_{3})\mid T_{2},T_{3})=\alpha\quad\forall\,\theta\in \Theta_{B}. \tag{5.87}\]

But for \(\theta\in\Theta_{B}\) (i.e., for \(\mu_{1}-\mu_{2}=0\)), the distribution of \(\tau\) is that of a Student's \(t\) with \(m+n-2\) d.f.,2 and is independent of \(\theta\in\Theta_{B}\). Hence, by Basu's Theorem, \(\tau\) is independent of \((T_{2},T_{3})\) under \(P_{\theta}\), for all \(\theta\in\Theta_{B}\). Thus \(c(T_{2},T_{3})\) does not depend on \((T_{2},T_{3})\) and is equal to \(t_{1-\alpha}(m+n-2)\)--the \(1-\alpha\)th quantile of the \(t\)-distribution with \(m+n-2\) d.f. The UMP unbiased test of size \(\alpha\) for (5.82) is given by

\[\text{Reject }H_{0}\text{ iff }\quad\frac{\overline{X}-\overline{Y}}{s_{p}\sqrt {\frac{1}{m}+\frac{1}{n}}}>t_{1-\alpha}(m+n-2). \tag{5.88}\]

There does not exist a UMP unbiased test of size \(\alpha\in(0,1)\) for \(H_{0}:\mu_{1}\leq\mu_{2}\) against \(H_{1}:\mu_{1}>\mu_{2}\) if the unknown variance \(\sigma_{1}^{2}\) of \(X_{i}\)'s is not equal to the unknown variance \(\sigma_{2}^{2}\) of \(Y_{j}\)'s, in Example 5. The problem of obtaining a "good" test in this case is known as the _Behrens-Fisher problem_.

Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) (\(m\geq 2\), \(n\geq 2\)) be independent random samples from \(\mathbf{N}(\mu_{1},\sigma_{1}^{2})\) and \(\mathbf{N}(\mu_{2},\sigma_{2}^{2})\) with all the parameters \(\mu_{i}\in\mathbb{R}\) (\(i=1,2\)), \(\sigma_{i}^{2}>0\) (\(i=1,2\)) unknown. We wish to test, at a level of significance \(\alpha\in(0,1)\),

\[H_{0}:\sigma_{1}^{2}\leq\sigma_{2}^{2}\quad\text{against}\quad H_{1}:\sigma_{ 1}^{2}>\sigma_{2}^{2}. \tag{5.89}\]

The (joint) density of the observation vector \((X_{1},X_{2},\ldots,X_{m},Y_{1},Y_{2},\ldots,Y_{n})\) may be expressed as [see (5.83)]

\[c(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2})\exp\left\{-\frac {1}{2\sigma_{1}^{2}}\sum_{1}^{m}x_{i}^{2}-\frac{1}{2\sigma_{2}^{2}}\sum_{1}^{n }y_{j}^{2}+\frac{\mu_{1}}{\sigma_{1}^{2}}\Bigl{(}\sum x_{i}\Bigr{)}+\frac{\mu_ {2}}{\sigma_{2}^{2}}\Bigl{(}\sum y_{j}\Bigr{)}\right\}\] \[=c_{1}(\theta_{1},\theta_{2},\theta_{3},\theta_{4})\exp\{\theta_ {1}T_{1}(\mathbf{x})+\theta_{2}T_{2}(\mathbf{x},\mathbf{y})+\theta_{3}T_{3}( \mathbf{x})+\theta_{4}T_{4}(\mathbf{y})\}, \tag{5.90}\]

where

\[\theta_{2} =-\frac{1}{2\sigma_{2}^{2}},\quad T_{2}(\mathbf{x},\mathbf{y})= \sum x_{i}^{2}+\sum y_{j}^{2},\quad\theta_{1}=\frac{1}{2\sigma_{2}^{2}}\left( 1-\frac{\sigma_{2}^{2}}{\sigma_{1}^{2}}\right),\quad T_{1}(\mathbf{x})=\sum x _{i}^{2},\] \[\theta_{3} =\frac{\mu_{1}}{\sigma_{1}^{2}},\quad T_{3}(\mathbf{x})=\sum x_{i },\quad\theta_{4}=\frac{\mu_{2}}{\sigma_{2}^{2}},\quad T_{4}(\mathbf{y})=\sum y _{j}. \tag{5.91}\]

Then (5.89) may be expressed as

\[H_{0}:\theta_{1}\leq 0,\quad H_{1}:\theta_{1}>0. \tag{5.92}\]

Hence the UMP unbiased test is of the form

\[\text{Reject }H_{0}:\text{iff }\quad T_{1}>\eta_{1}(T_{2},T_{3},T_{4}), \tag{5.93}\]

where \(\eta(t_{2},t_{3},t_{4})\) satisfies

\[P_{\theta}(T_{1} >\eta_{1}(t_{2},t_{3},t_{4})\mid T_{2}=t_{2},T_{3}=t_{3},T_{4}=t _{4})=\alpha\quad\forall\,\theta\in\Theta_{B},\] \[\Theta_{B} =\{(0,\theta_{2},\theta_{3},\theta_{4}):\theta_{2}<0,\ \theta_{3} \in\mathbb{R},\ \theta_{4}\in\mathbb{R}\}. \tag{5.94}\]

This probability does not depend on \(\theta\in\Theta_{B}\). Consider the statistic

\[F=\frac{\sum(X_{i}-\overline{X})^{2}/(m-1)}{\sum(Y_{j}-\overline{Y})^{2}/(n- 1)}\,=\,\frac{\left(T_{1}-\frac{T_{2}^{2}}{m}\right)/(m-1)}{\left(T_{2}-T_{1}- \frac{T_{4}^{2}}{n}\right)/(n-1)}\,, \tag{5.95}\]

which is easily seen to be a strictly increasing function of \(T_{1}\), for every set of given values of \(T_{2}\), \(T_{3}\), \(T_{4}\) that may occur a.s. (\(P_{\theta}\)), for all \(\theta\in\Theta_{B}\). Hence (5.93) is equivalent to

\[\text{Reject }H_{0}\text{ iff }\quad F>\eta_{2}(T_{2},T_{3},T_{4}),\]for an appropriate \(\eta_{2}\). But

\[F=\frac{\sum(X_{i}-\overline{X})^{2}/\sigma_{1}^{2}}{\sum(Y_{j}-\overline{Y})^{2}/ \sigma_{1}^{2}}\cdot\frac{n-1}{m-1}=\frac{U}{V}\cdot\frac{n-1}{m-1}\enspace, \tag{5.96}\]

where, \(\forall\ \theta\in\Theta_{B}\), \(U\) and \(V\) are two independent chi-square random variables. Hence the distribution of \(F\) does not depend on \(\theta\in\Theta_{B}\). It follows from Basu's Theorem that \(F\) is independent of \(T_{2}\), \(T_{3}\), \(T_{4}\), under \(P_{\theta}\), \(\theta\in\Theta_{B}\). Therefore, the UMP unbiased test is given by

\[\text{Reject }H_{0}\text{ iff }\quad F>F_{1-\alpha}, \tag{5.97}\]

where \(F_{1-\alpha}\) is the \(1-\alpha\)th quantile of the so-called _\(F\)-distribution with d.f._\((m-1,\)\(n-1)\). The distribution of \(F\) is computed in Appendix A, Example A.VIII.

Example (Match Pair Experiments): In order to test the effectiveness of an exercise regimen in reducing systolic blood pressures of people with moderately elevated blood pressure belonging to a certain age group, a random sample of \(n\) individuals is chosen from this population. For each individual the (systolic) blood pressure \(x\) before the start of the exercise regimen is recorded, as well as the blood pressure \(y\) after the completion of the exercise regimen. The model assumed is that \((X_{i},Y_{i})\), \(i=1,\ldots,n\), are i.i.d. Normal \(N((\mu_{x},\mu_{y})^{t}\sum)\) where \((\mu_{x},\mu_{y})\in\mathbb{R}^{2}\), and \(\sum\) is a positive definite covariance matrix with \(\sigma_{x}^{2}=\text{var}(X_{i})\), \(\sigma_{y}^{2}=\text{var}(Y_{i})\), \(\sigma_{x,y}=\text{covar}(X_{i},Y_{i})=p\sigma_{x}\sigma_{y}\). All the parameters are unknown. One wishes to test \(H_{0}:\mu_{x}=\mu_{y}\) against \(H_{1}:\mu_{x}>\mu_{y}\). Just as in the Behrens-Fisher problem, there is no uniformly most powerful unbiased (UMPU) test in this generality. However, if one assumes that \(\sigma_{x}^{2}=\sigma_{y}^{2}\), the UMPU test of size \(\alpha\) exists by the theory of Sects. 5.5, 5.6, and it is given by (Exercise 5.14(a)): Reject \(H_{0}\) iff \(\overline{d}>t_{n-1}(1-\alpha)(s_{d}/\sqrt{n})\) where, writing \(d_{i}=X_{i}-Y_{i}\), \(\overline{d}=\sum_{1\leq i\leq n}d_{i}/n\), \(s_{d}^{2}=\sum_{1\leq i\leq n}(d_{i}-\overline{d})^{2}/(n-1)\), and \(t_{n-1}(1-\alpha)\) is the \((1-\alpha)\)-th quantile of the \(t\)-distribution with \(n-1\) degrees of freedom.

Consider now an alternative design where a random sample of size \(n\) is chosen from the target population, and their blood pressures \(X_{i}\) are recorded (\(i=1,\ldots,n\)); this is the so-called _control group_. Another random sample of \(n\) individuals is drawn independently from this population and subjected to the exercise regimen. This is the _treatment group_. Let \(Y_{i}\) (\(i=1,\ldots,n\)) be the blood pressures of these individuals measured after the exercise regimen is completed. Suppose \(X_{i}\)'s are \(N(\mu_{x},\sigma_{x}^{2})\) and \(Y_{i}\)'s are \(N(\mu_{y},\sigma_{y}^{2})\), with \(\mu_{x}\), \(\sigma_{x}^{2}\), \(\mu_{y}\), \(\sigma_{y}^{2}\), the same as in the preceding paragraph. Assume that \(\sigma_{x}^{2}=\sigma_{y}^{2}\), and consider the UMPU test provided by Example 5.8 for \(H_{0}:\mu_{x}=\mu_{y}\) against \(H_{1}:\mu_{x}>\mu_{y}\). Give an argument to show that the match pair design is more efficient than this independent samples design if \(p>0\) and \(n\) is sufficiently large (Exercise 5.14(b)).

### Duality Between Tests and Confidence Regions

Given a family of non-randomized tests \(\{\varphi_{\theta_{0}}:\theta_{0}\in\Theta\}\) for testing \(H_{0}:\theta=\theta_{0}\) against \(H_{1}:\theta\in\Theta_{1}^{\theta_{0}}\subset\Theta\backslash\{\theta_{0}\}\), there exists a 'confidence region' for the unknown parameter \(\theta\) given by

\[S(\mathbf{x}):=\left\{\theta_{0}\in\Theta:\varphi_{\theta_{0}}(\mathbf{x})=0 \right\},\quad\mathbf{x}\in\mathscr{X}. \tag{5.98}\]That is, \(S({\bf x})\) is the set of values of \(\theta=\theta_{0}\) which are accepted by the test when presented as the null hypothesis \(H_{0}:\theta=\theta_{0}.\) If the family of tests is of size \(\alpha,\) then

\[P_{\theta_{0}}\left(\theta_{0}\in S({\bf X})\right)=P_{\theta_{0}}\left(\varphi_ {\theta_{0}}({\bf X})=0\right)=1-\alpha\;\;\forall\;\theta_{0}\in\Theta. \tag{5.99}\]

One expresses this as: '\(S({\bf X})\) is a confidence region for \(\theta\) of confidence level \(1-\alpha\)'. Conversely, suppose one is given a confidence region \(S\) of confidence level \(1-\alpha,\) that is, \({\bf x}\to S({\bf x})\) is a map on \(\mathscr{X}\) into the class of subsets of \(\Theta\) such that

\[P_{\theta_{0}}(\theta_{0}\in S({\bf X}))=1-\alpha\;\;\forall\;\theta_{0}\in \Theta. \tag{5.100}\]

Then the family of tests \(\varphi_{\theta_{0}}\) given by

\[\varphi_{\theta_{0}}({\bf x})=\left\{\begin{array}{ll}1\;\mbox{if $\theta_{0} \not\in S({\bf x})$},&\;{\bf x}\in\mathscr{X},\,\theta_{0}\in\Theta,\\ 0\;\mbox{if $\theta_{0}\in S({\bf x})$},&\end{array}\right. \tag{5.101}\]

is of size \(\alpha.\)

We now consider the more general situation of the estimation of, and confidence regions for, functions \(f(\theta)\) of \(\theta.\)

**Definition 5.6**.: Let \(\{P_{\theta}:\theta\in\Theta\}\) be a family of distributions on the observation space \(\mathscr{X}.\) Let \(f(\theta)\) be a function on \(\Theta\) into some space \(\Gamma.\) Suppose \({\bf x}\to S({\bf x})\) is a map on \(\mathscr{X}\) into the class of subsets of \(\Gamma\) such that, (i) for each \(\theta\in\Theta,\) the set \(\{{\bf x}:f(\theta)\in S({\bf x})\}\) is a measurable subset of \(\mathscr{X},\) and

\[\mbox{(ii)}\quad P_{\theta}(\{{\bf x}:f(\theta)\in S({\bf x}))=1-\alpha\quad \forall\;\theta\in\Theta.\]

Then \(S(\cdot)\) (or, \(S({\bf X})\)), where \({\bf X}\) has distribution \(P_{\theta},\)\(\theta\) being the unknown 'true' parameter value) is called a _confidence region_ for \(f(\theta)\) having a _confidence level_\(1-\alpha.\)

_Remark 5.7_.: To avoid confusion, \({\bf X}\) may be taken to be the identity map \({\bf x}\to{\bf x}\) on \(\mathscr{X}.\)

**Notation**: Let \(\overline{H}(f(\theta_{0}))\) be the space of alternatives against \(H_{0}:f(\theta)=f(\theta_{0}).\)

**Definition 5.7**.: A confidence region \(S^{*}(\cdot)\) of confidence level \(1-\alpha\) is said to be _uniformly most accurate (UMA)_ of level \(1-\alpha,\) if, \(\forall\;\theta_{0}\in\Theta\) and \(\forall\;\theta\in\overline{H}(f(\theta_{0})),\)

\[P_{\theta}(\{{\bf x}\in\mathscr{X}:f(\theta_{0})\in S^{*}({\bf x})\})\leq P_{ \theta}(\{{\bf x}\in\mathscr{X};f(\theta_{0})\in S({\bf x})\}), \tag{5.102}\]

holds for every confidence region \(S(\cdot)\) for \(f(\theta)\) of confidence level \(1-\alpha.\) A confidence region \(S(\cdot)\) for \(f(\theta)\) is said to be _unbiased of level \(1-\alpha\)_ if, \(\forall\,\theta_{0}\in\Theta,\)

\[P_{\theta}(\{{\bf x}\in\mathscr{X}:f(\theta_{0})\in S({\bf x})\})\leq 1- \alpha\quad\forall\;\theta\in\overline{H}(f(\theta_{0})). \tag{5.103}\]

A confidence region \(S^{*}(\cdot)\) is said to be _UMA unbiased of level \(1-\alpha\),_ if among all unbiased confidence regions \(S(\cdot)\) of level \(1-\alpha,\) it is the most accurate, i.e., (5.102) holds.

**Theorem 5.6** (Duality Between Confidence Regions and Tests).:
1. _Given a family of non-randomized tests_ \(\{\varphi_{\theta}:\theta\in\Theta\}\) _of size_ \(\alpha\)_, where_ \(\varphi_{\theta_{0}}\) _is a test (of size_ \(\alpha\)_) for_ \(H_{0}:f(\theta)=f(\theta_{0})\)__\((\theta_{0}\in\Theta)\)_,_ \[S(\mathbf{x}):=\{f(\theta):\theta\in\Theta,\,\varphi_{\theta}(\mathbf{x})=0\}, \quad\mathbf{x}\in\mathscr{X},\] (5.104) _is a confidence region for_ \(f(\theta)\) _of confidence level_ \(1-\alpha\)_._
2. _Conversely, given a confidence region_ \(S(\cdot)\) _for_ \(f(\theta)\) _of confidence level_ \(1-\alpha\)_, consider the family of tests_ \(\{\varphi_{\theta}:\theta\in\Theta\}\) _defined by_ \[\varphi_{\theta}(\mathbf{x}):=\left\{\begin{array}{ll}1&\mbox{ if }f(\theta)\not \in S(\mathbf{x}),\\ 0&\mbox{ if }f(\theta)\in S(\mathbf{x})\end{array}\right.\quad(\mathbf{x}\in \mathscr{X}).\] (5.105) _Then, for each_ \(\theta_{0}\in\Theta\)_,_ \(\varphi_{\theta_{0}}\) _is a test of size_ \(\alpha\) _for_ \(H_{0}:f(\theta)=f(\theta_{0})\)_._
3. _If a family of tests_ \(\{\varphi^{*}_{\theta}:\theta\in\Theta\}\) _is given such that, for each_ \(\theta_{0}\in\Theta\)_,_ \(\varphi^{*}_{\theta_{0}}\) _is UMP of size_ \(\alpha\) _for testing_ \(H_{0}:f(\theta)=f(\theta_{0})\)_, against_ \(H_{1}:\theta\in\overline{H}(f(\theta_{0}))\)_, then_ \(S^{*}(\cdot)\) _defined by (_5.104_), with_ \(\varphi_{\theta}\) _replaced by_ \(\varphi^{*}_{\theta}\)_, is UMA of size_ \(1-\alpha\)_._
4. _If_ \(\varphi^{*}_{\theta_{0}}\) _is UMA unbiased of size_ \(\alpha\) _for testing_ \(H_{0}:f(\theta)=f(\theta_{0})\)_, against_ \(H_{1}:\theta\in\overline{H}(f(\theta_{0}))\)__\((\forall\)__\(\theta_{0}\in\Theta)\)_, then the corresponding_ \(S^{*}(\cdot)\) _is UMA unbiased of level_ \(1-\alpha\)_._

Proof.:
1. By (5.104), we have equality of the events \[\{\mathbf{x}\in\mathscr{X}:f(\theta)\in S(\mathbf{x})\}=\{\mathbf{x}\in \mathscr{X}:\varphi_{\theta}(\mathbf{x})=0\},\quad\forall\,\theta\in\Theta.\] (5.106) Hence \[P_{\theta}(\{\mathbf{x}\in\mathscr{X}:f(\theta)\in S(\mathbf{x})\})=P_{\theta }(\{\mathbf{x}\in\mathscr{X}:\varphi_{\theta}(\mathbf{x})=0\})=1-\alpha\quad \forall\,\,\theta\in\Theta.\] (5.107)
2. if \(S(\cdot)\) is a confidence region for \(f(\theta)\) of confidence level \(1-\alpha\), then for the family of tests \(\{\varphi_{\theta}:\theta\in\Theta\}\) defined by (5.105), again (5.106) holds. Hence \(P_{\theta}(\{\mathbf{x}\in\mathscr{X}:\varphi_{\theta}(\mathbf{x})=1\})=1-P_{ \theta}(\{\mathbf{x}\in\mathscr{X}:\varphi_{\theta}(\mathbf{x})=0\})=1-P_{ \theta}(\{\mathbf{x}\in\mathscr{X}:f(\theta)\in S(\mathbf{x})\})=1-(1-\alpha )=\alpha\,\,\,(\forall\,\theta\in\Theta)\).
3. Suppose \(\varphi^{*}_{\theta_{0}}\) is UMP of size \(\alpha\) for testing \(H_{0}:f(\theta)=f(\theta_{0})\), against \(H_{1}:\theta\in\overline{H}(f(\theta_{0}))\), for every \(\theta_{0}\in\Theta\). Then if \(S^{*}(\cdot)\) is the corresponding confidence region for \(f(\theta)\) and \(S(\cdot)\) is any other confidence region of level \(1-\alpha\) for \(f(\theta)\), with corresponding family of tests defined by (5.105), then, \(\forall\,\,\theta\in\overline{H}(f(\theta_{0}))\), \[P_{\theta}(\{\mathbf{x}\in\mathscr{X}:f(\theta_{0})\in S^{*}( \mathbf{x})\})\] \[=P_{\theta}(\{\mathbf{x}\in\mathscr{X}:\varphi^{*}_{\theta_{0}}( \mathbf{x})=0\})=1-P_{\theta}(\{\mathbf{x}:\varphi^{*}_{\theta_{0}}(\mathbf{x })=1\})\] \[=1-(\text{Power of the test }\varphi^{*}_{\theta_{0}}\text{ at } \theta)\leq 1-(\text{Power of the test }\varphi_{\theta_{0}}\text{ at }\theta)\] \[=1-P_{\theta}(\{\mathbf{x}\in\mathscr{X}:\varphi_{\theta_{0}}( \mathbf{x})=1\})=P_{\theta}(\{\mathbf{x}\in\mathscr{X}:\varphi_{\theta_{0}}( \mathbf{x})=0\})\] \[=P_{\theta}(\{\mathbf{x}\in\mathscr{X}:f(\theta_{0})\in S( \mathbf{x})\}).\] (5.108)
4. This follows from (5.108), since the unbiasedness of the family of tests \(\{\varphi_{\theta}:\theta\in\Theta\}\) means, \(P_{\theta}(\{\mathbf{x}\in\mathscr{X}:\varphi_{\theta_{0}}(\mathbf{x})=1\})\equiv (\text{Power of }\varphi_{\theta_{0}}\text{ at }\theta)\geq\alpha\,\,\forall\,\theta\in\overline{H}(f(\theta_{0}))\). Therefore, for \(S(\cdot)\) defined by (5.103),\[P_{\theta}(\{{\bf x}\in\mathscr{X}:f(\theta_{0})\in S({\bf x})\})=P_{ \theta}(\{{\bf x}\in\mathscr{X}:\varphi_{\theta_{0}}({\bf x})=0\})\] \[=1-P_{\theta}(\{{\bf x}\in\mathscr{X}:\varphi_{\theta_{0}}({\bf x}) =1\})\leq 1-\alpha,\quad\forall\ \theta\in\overline{H}(f(\theta_{0}).\]

Same is true for \(S^{*}(\cdot)\). 

_Example 5.12_.: Let \(X_{1},\ldots,X_{n}\) be i.i.d. exponential with p.d.f.

\[f(x\mid\theta)=\frac{e^{-x/\theta}}{\theta}\,,\quad 0\leq x<\infty,\ \theta\in(0, \infty). \tag{5.109}\]

The uniformly most powerful test of size \(\alpha\) for testing \(H_{0}:\theta\leq\theta_{0}\), \(H_{1}:\theta>\theta_{0}\) is (See Example 5.4, with \(\beta=1\); but consider one-sided alternatives; or, see Exercise 5.2)

\[\varphi_{\theta_{0}}^{*}({\bf x})=\left\{\begin{array}{ll}1&\mbox{ if }\sum_{1}^{n}x_{i}>\theta_{0}c_{1-\alpha}(n),\\ 0&\mbox{ if }\sum_{1}^{n}x_{i}\leq\theta_{0}c_{1-\alpha}(n),\end{array}\right.\]

where \(c_{1-\alpha}(n)\) is the \((1-\alpha)\)th quantile of the gamma distribution \(\mathscr{G}(1,n)\). The _UMA confidence region_\(S^{*}(\cdot)\)_for_\(\theta\)_of level_\(1-\alpha\) is given by

\[S^{*}({\bf x}) =\left\{\theta_{0}>0:\sum_{1}^{n}x_{i}\leq\theta_{0}c_{1-\alpha}( n)\right\}=\left\{\theta_{0}:\theta_{0}\geq\frac{\sum_{1}^{n}x_{i}}{c_{1- \alpha}(n)}\right\}\] \[=\left[\frac{\sum_{1}^{n}x_{i}}{c_{1-\alpha}(n)}\,,\infty\right).\]

In this example, \(f(\theta)=\theta\), \(\overline{H}(\theta_{0})=\Theta\backslash\{\theta_{0}\}\).

_Example 5.13_.: Consider Exercise 5.7(c). Here \(X_{1},\ldots,X_{n}\) are i.i.d. \({\bf N}(\mu,\sigma^{2})\) (\(\mu\in\mathbb{R}\), \(\sigma^{2}>0\) are unknown). The UMPU test, of size \(\alpha\in(0,1)\), for \(H_{0}:\mu=\mu_{0}\), \(H_{1}:\mu\neq\mu_{0}\) is: \(\varphi^{*}({\bf x})=0\) if \(\left|\frac{\sqrt{n}\left(\overline{x}-\mu_{0}\right)}{s}\right|\leq t_{1- \frac{\alpha}{2}}(n-1)\), and \(\varphi^{*}({\bf x})=1\) otherwise. Hence the UMA unbiased confidence region \(S^{*}\) of level \(1-\alpha\) is given by \(S^{*}({\bf x})=\left\{\mu\in\mathbb{R}:\left|\frac{\sqrt{n}\left(\overline{x}- \mu\right)}{s}\right|\leq t_{1-\frac{\alpha}{2}}(n-1)\right\}\ =\ \left\{\mu\in \mathbb{R}:\left|\overline{x}-\mu\right|\leq t_{1-\frac{\alpha}{2}}(n-1)\frac{ s}{\sqrt{n-1}}\right\}\ =\ \left[\overline{x}-t_{1-\frac{\alpha}{2}}(n-1)\frac{s}{\sqrt{n}},\ \overline{x}+t_{1-\frac{\alpha}{2}}(n-1)\frac{s}{\sqrt{n}}\right]\).

If \(\sigma^{2}\) is known, \(\sigma^{2}=\sigma_{0}^{2}\), say, then the UMPU test \(\varphi^{*}\) of size \(\alpha\) for the above hypotheses is given by \(\varphi^{*}({\bf x})=0\) iff \(\left|\frac{\sqrt{n}\left(\overline{x}-\mu\right)}{\sigma_{0}}\right|\leq z_{1 -\frac{\alpha}{2}}\) (See Example 5.3). Hence the UMA unbiased confidence region (interval) for \(\mu\) is \(S^{*}({\bf x})=\left[\overline{x}-z_{1-\frac{\alpha}{2}}\frac{\sigma_{0}}{ \sqrt{n}},\right.\)\(\left.\overline{x}+z_{1-\frac{\alpha}{2}}\frac{\sigma_{0}}{\sqrt{n}}\right]\).

Note that here \(f(\theta)=\theta_{1}\), where \(\theta=(\theta_{1},\theta_{2})=(\mu,\sigma^{2})\). Here \(f(\theta_{0})=\mu_{0}\), \(\overline{H}(f(\theta_{0}))=\{(\mu,\sigma^{2}):\mu\neq\mu_{0}\}\). Also, unbiasedness of a confidence region \(S(\cdot)\) of level \((1-\alpha)\) here means

\[P_{\mu,\sigma^{2}}(\{{\bf x}:\mu_{0}\in S({\bf x})\})\leq 1-\alpha\quad\forall\ \mu_{0}\neq\mu,\ \sigma^{2}>0. \tag{5.110}\]

_Example 5.14_.: (See Example 5.7, and Exercise 5.8(b)). Let \(X_{1},\ldots,X_{n}\) be i.i.d. \({\bf N}(\mu,\sigma^{2})\) with \(\mu\in\mathbb{R}\), \(\sigma^{2}>0\) both unknown. Consider, for each \(\sigma_{0}^{2}>0\), the test \(H_{0}:\sigma^{2}=\sigma_{0}^{2}\) against \(H_{1}:\sigma^{2}\neq\sigma_{0}^{2}\). The _UMPU test_ of size \(\alpha\) is given by \(\varphi^{*}({\bf x})=0\) if \(c_{1}\sigma_{0}^{2}\leq(n-1)s^{2}\leq c_{2}\sigma_{0}^{2}\), and \(\varphi^{*}({\bf x})=1\) otherwise, where \(c_{1}<c_{2}\) are determined by the Eq. (5.179). Hence \(S^{*}({\bf x})=\{\sigma^{2}:c_{1}\sigma^{2}\leq(n-1)s^{2}\leq c_{2}\sigma^{2} \}=\{\sigma^{2}:c_{1}\leq\frac{(n-1)s^{2}}{\sigma^{2}}\leq c_{2}\}=\{\sigma^{2}: \frac{1}{c_{2}}\leq\frac{\sigma^{2}}{(n-1)s^{2}}\leq\frac{1}{c_{1}}\}=\left[ \frac{(n-1)s^{2}}{c_{2}},\,\frac{(n-1)s^{2}}{c_{1}}\right]\), is the UMA unbiased confidence region.

_Remark 5.8_.: To avoid computation of \(c_{1}\), \(c_{2}\) numerically, one sometimes uses an equal-tailed test, or confidence region, choosing \(c_{1}=\chi^{2}_{\frac{\alpha}{2}}<c_{2}=\chi^{2}_{1-\frac{\alpha}{2}}\), where \(\chi^{2}_{p}\) is the \(p\)th quantile of the chi-square distribution with \(n-1\) d.f.

### Invariant Tests, the Two-Sample Problem and Rank Tests

Consider a testing problem with observation \(\mathbf{X}\) having distribution \(P_{\theta}\), \(\theta\in\Theta\), and \(H_{0}:\theta\in\Theta_{0}\), \(H_{1}:\theta\in\Theta_{1}=\Theta\backslash\Theta_{0}\). Denote by \(\mathscr{X}\) the observation space in which \(\mathbf{X}\) takes values. Assume \(P_{\theta_{1}}\neq P_{\theta_{2}}\) if \(\theta_{1}\neq\theta_{2}\)_(Identifiability)_.

Let \(g\) be a (bi-measurable) _transformation_ on \(\mathscr{X}\), i.e., \(g\) is a one-to-one map on \(\mathscr{X}\) onto \(\mathscr{X}\), and \(g\) and \(g^{-1}\) are both measurable.

Suppose \(\mathbf{Y}:=g\mathbf{X}\) (\(:=g(\mathbf{X})\)) has a distribution \(P_{\theta^{\prime}}\) (in the family \(\{P_{\theta}:\theta\in\Theta\}\)) when \(\mathbf{X}\) has distribution \(P_{\theta}\). Writing \(\theta^{\prime}=\overline{g}_{\theta}\) in this case, one has a map \(\overline{g}\) (associated with \(g\)) on \(\Theta\) into \(\Theta\). This map is _one to one,_ i.e., if \(\theta_{1}\neq\theta_{2}\) then \(\overline{g}\theta_{1}\neq\overline{g}\theta_{2}\) (Exercise 5.15). Assume that this map is also _onto_\(\Theta\):

\[\overline{g}\Theta=\Theta. \tag{5.111}\]

Assume also that \(g\) leaves \(\Theta_{0}\) (and, therefore, also \(\Theta_{1}\)) invariant:

\[\overline{g}\Theta_{0}=\Theta_{0},\quad\overline{g}\Theta_{1}=\Theta_{1}. \tag{5.112}\]

We then say that the statistical testing problem is _invariant under \(g\)_. The reason for the nomenclature is that the testing problem stated for \(\mathbf{X}\) is exactly the same when stated for \(\mathbf{Y}\), namely, with the same observation space \(\mathscr{X}\), the same parameter space \(\Theta\) or family of distributions \(\{P_{\theta}:\theta\in\Theta\}\), the same null hypothesis \(H_{0}:\theta\in\Theta_{0}\) and the same alternative hypothesis \(H_{1}:\theta\in\Theta_{1}\). Therefore, if \(\varphi\) is a reasonable test, one should have

\[\varphi(\mathbf{x})=\varphi(g^{-1}\mathbf{x})\qquad\mathbf{x}\in\mathscr{X}. \tag{5.113}\]

For, given any \(\mathbf{x}\in\mathscr{X}\), the decision based on \(\mathbf{X}=\mathbf{x}\) (namely, \(\varphi(\mathbf{x})\)) should be the same as that based on \(\mathbf{Y}=\mathbf{x}\) (where \(\mathbf{Y}=g\mathbf{X}\)), i.e., based on \(\mathbf{x}=g^{-1}\mathbf{x}\). If one replaces \(\mathbf{x}\) by \(g\mathbf{x}\) in (5.113) then one gets

\[\varphi(g\mathbf{x})=\varphi(\mathbf{x})\quad\forall\ \ \mathbf{x}\in\mathscr{X}. \tag{5.114}\]

Note that (5.113) and (5.114) are equivalent.

Let now \(\mathscr{G}\) be a _group of transformations_ with each \(g\in\mathscr{G}\) of the above kind (The group operations are (1) \(g\to g^{-1}\), in the usual sense of the inverse of a function, and (2) \(g_{1}g_{2}=g_{1}(g_{2})\), the composition of functions). This also gives rise to the corresponding group \(\overline{\mathscr{G}}\) on the parameter space \(\Theta\). Then (5.113), (5.114) should hold for all \(g\in\mathscr{G}\) for a reasonable test \(\varphi\). In other words, _for each \(\mathbf{x}\in\mathscr{X}\), \(\varphi\) should be constant on the orbit_ of \(\mathbf{x}\),

\[O(\mathbf{x}):=\{g\mathbf{x}:g\in\mathscr{G}\}. \tag{5.115}\]

The invariant tests \(\varphi\) are therefore _functions on the space_ of orbits. In this sense the map \(\mathbf{x}\to O(\mathbf{x})\), say, \(T(\mathbf{x})=O(\mathbf{x})\), is a _maximal invariant,_

* _(Invariance)._ \(T(g\mathbf{x})=T(\mathbf{x})\quad\forall\ g\in\mathscr{G},\,\forall\ \mathbf{x}\in\mathscr{X}\),
* _(Maximality)._ If \(T(\mathbf{x}_{1})=T(\mathbf{x}_{2})\), then \(\exists\,g\in\mathscr{G}\) such that \(g\mathbf{x}_{1}=\mathbf{x}_{2}\). (5.116)

[MISSING_PAGE_FAIL:100]

Lehmann 1959, p. 299, for \(k>1\)). Under \(H_{0}\) the distribution of \(W\) does not involve \(\Sigma\), so that \(c\) can be computed. The statistic \(W\), or some multiple of it, is referred to as _Hotelling's_\(T^{2}\). It can be shown that \(\frac{n-k}{k(n-1)}W\) has the \(F\) distribution \(F_{k,n-k}\) (Lehmann, 1959, pp. 296-300).

Finally, consider the two-sample problem: \(H_{0}:\boldsymbol{\mu}_{1}=\boldsymbol{\mu}_{2}\), \(H_{1}:\boldsymbol{\mu}_{1}\neq\boldsymbol{\mu}_{2}\), where \(\boldsymbol{\mu}_{1}\), \(\boldsymbol{\mu}_{2}\) are the means of the \(k\)-dimensional Normal distributions \(N(\boldsymbol{\mu}_{1},\Sigma)\), \(N(\boldsymbol{\mu}_{2},\Sigma)\). Arguing as above one can show (See Lehmann, loc. cit.) that the UMP unbiased invariant test based on independent samples \(\mathbf{X}_{i}=(X_{i1},\ldots,X_{ik})^{\prime}\), \(1\leq i\leq n_{1}\), from \(N(\boldsymbol{\mu}_{1},\Sigma)\), and \(\mathbf{Y}_{i^{\prime}}=(Y_{i^{\prime}1},\ldots,Y_{i^{\prime}k})^{\prime}\), \(1\leq i^{\prime}\leq n_{2}\), from \(N(\boldsymbol{\mu}_{2},\Sigma)\), is to reject \(H_{0}\) if

\[T^{2}\equiv\frac{n-k-1}{k(n-2)}(\overline{\mathbf{X}}-\overline{\mathbf{Y}})^{ \prime}\left[S\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)\right]^{-1}( \overline{\mathbf{X}}-\overline{\mathbf{Y}})>F_{k,n-k-1}(1-\alpha),\]

where \(n=n_{1}+n_{2}\), \(S=\frac{(n_{1}-1)S_{1}+(n_{2}-1)S_{2}}{n_{1}+n_{2}-2}\), \(S_{1}\) and \(S_{2}\) being the sample covariance matrices of \(\mathbf{X}\) and \(\mathbf{Y}\). This is the two-sample Hotelling's \(T^{2}\) test. This provides the motivation for the large sample nonparametric \(t\) tests used in Chap. 8.

#### The Two-Sample Problem

In many statistical investigations one inquires whether one commodity (or brand) is better than another, whether, e.g., one brand of tires has a greater life length than another, or whether one diet reduces cholesterol level more than another diet, etc. Among the most common parametric models of this kind is that of two normal distributions \(\mathbf{N}(\mu,\sigma^{2})\) and \(\mathbf{N}(\eta,\sigma^{2})\), and one wishes to test \(H_{0}:\mu=\eta\) (or, \(\mu\leq\eta\)) against \(H_{1}:\mu>\eta\). Here \(\mu,\eta,\sigma^{2}\) are unknown. The uniformly most powerful unbiased test of size \(\alpha\) for this model, based on two independent random samples \(X_{i}\) (\(1\leq i\leq m\)) and \(Y_{j}\) (\(1\leq j\leq n\)) from \(\mathbf{N}(\mu,\sigma^{2})\) and \(\mathbf{N}(\eta,\sigma^{2})\), respectively, is the \(t\)-test: reject \(H_{0}\) iff \(T\equiv\frac{\overline{X}-\overline{Y}}{s_{p}\sqrt{\frac{1}{n}+\frac{1}{n}}}>t _{1-\alpha}(m+n-2)\) (See Example 5.9). One could also have a model in which \(X_{i}\)'s are i.i.d. exponential with parameter (mean) \(\theta_{1}\) and \(Y_{j}\)'s are i.i.d. exponential with parameter \(\theta_{0}\), and one wishes to test \(H_{0}:\theta_{1}\leq\theta_{2}\) against \(H_{1}:\theta_{1}>\theta_{2}\) (See Exercise 5.11). In the absence of a reliable parametric model one may use a nonparametric model based on the following notion of stochastic order.

**Definition 5.8**.: A random variable \(X\) is said to be _stochastically larger_ than a random variable \(Y\) if

\[P(X>x)\geq P(Y>x)\quad x\in\mathbb{R}, \tag{5.117}\]

with strict inequality for some \(x\). Note that, if \(F\) and \(G\) are the cumulative distribution functions of \(X\) and \(Y\), respectively, then (5.117) is equivalent to

\[F(X)\leq G(x)\qquad\forall\;x\in\mathbb{R},\] \[\text{with strict inequality for at least one }x. \tag{5.118}\]

Thus we may talk about a _stochastic order_ (a partial order, given by the first line in (5.118) among distributions on \(\mathbb{R}\). In both parametric examples given above,(the distribution of) \(X\) is stochastically larger than (that of) \(Y\) under \(H_{1}\). Since the interest in the problems mentioned earlier in these examples seems to concern stochastic order, we may formulate the testing _nonparametrically_ as follows. Based on independent random samples \(X_{i}\) (\(1\leq i\leq m\)) from a distribution with d.f. _(distribution function)_\(F\) and \(Y_{j}\) (\(1\leq j\leq n\)) from a distribution with d.f. \(G\), consider the testing problem

\[H_{0}:\theta\in\Theta_{0} = \{(F,G):F(x)=G(x)\ \forall\,x,\,F\text{ and }G\text{ continuous}\}\] \[= \{(F,F):F\text{ continuous}\}\] \[H_{1}:\theta\not\in\Theta_{0} = \{(F,G):F(x)\leq G(x)\ \forall\,x,\,F\neq G,\ F\text{ and }G\text{ continuous}\}\] \[\Theta=\Theta_{0}\cup\Theta_{1} = \{(F,G):F,G\text{ are continuous, }F(x)\leq G(x)\ \forall\,x\}. \tag{5.119}\]

_Remark 5.9_.: To simplify the discussion we have chosen \(H_{0}\) as above, instead of \(\{(F,G):F(x)\geq G(x)\ \forall\ x,\,F\text{ and }G\text{ continuous}\}\). The tests we derive, are valid for this more reasonable version of \(H_{0}\).

We will _first reduce the data by sufficiency._ The (pair of) ordered statistics \(T=((X_{(1)},X_{(2)},\ldots,X_{(m)}),\)\((Y_{(1)},Y_{(2)},\ldots,Y_{(n)}))\)_are sufficient for \(\theta=(F,G)\in\Theta\)_._ Here \(X_{(1)}<X_{(2)}<\cdots<X_{(m)}\) are the ordering of the \(X_{i}\)'s among themselves, and similarly for \(Y_{j}\)'s. To prove sufficiency of \(T\), note that (because \(X_{i}\)'s are i.i.d.) the \(m!\) different orderings \(X_{i_{1}}<X_{i_{2}}<\cdots<X_{i_{m}}\) (with \((i_{1},i_{2},\ldots,i_{m})\) an arbitrary permutation of \((1,2,\ldots,m)\)) are all equally likely, so that each has probability \(\frac{1}{m!}\). Similarly, the \(n!\) different orderings \(Y_{j_{1}}<Y_{j_{2}}<\cdots<Y_{j_{n}}\) all have the same probability \(\frac{1}{n!}\). Thus, for each value of \(T=(u_{1},u_{2},\ldots,u_{m},v_{1},v_{2},\ldots,v_{n})\) with \(u_{1}<u_{2},\cdots,<u_{m},\,v_{1}<v_{2}<\cdots<v_{n}\), one has

\[P_{\theta}(X_{1}=x_{1},X_{2}=x_{2},\ldots,X_{m}=x_{m},Y_{1}=y_{1 },Y_{2}=y_{2},\ldots,Y_{n}=y_{n})\mid\] \[\quad\quad T=(u_{1},u_{2},\ldots,u_{m},v_{1},v_{2},\ldots,v_{n}))\] \[= \left\{\begin{array}{rl}\frac{1}{m!n!}&\text{if }(x_{1},x_{2}, \ldots,x_{m})\text{ is a permutation of }(u_{1},u_{2},\ldots,u_{m})\text{ and}\\ (y_{1},y_{2},\ldots,y_{n})\text{ is a permutation of }(v_{1},v_{2},\ldots,v_{n}),\\ 0&\text{otherwise.}\end{array}\right. \tag{5.120}\]

Thus the conditional distribution of \((X_{1},X_{2},\ldots,X_{m},Y_{1},\ldots,Y_{n})\), given \(T\), does not depend on \(\theta\in\Theta\). This establishes the desired sufficiency of \(T\). Since \(F\) and \(G\) are continuous and the \(m+n\) random variables \(X_{i}\)'s and \(Y_{j}\)'s are independent, we may take the _observation space_ (of the sufficient statistic \(T\)) to be

\[\mathscr{X} = \left\{(\mathbf{u},\mathbf{v})\in\mathbb{R}^{m+n}:u_{1}<u_{2}, \cdots<u_{m},v_{1}<v_{2}<\cdots<v_{n},\ u_{i}\neq v_{j}\ \forall\,i,j\right\}, \tag{5.121}\] \[(\mathbf{u}=(u_{1},\ldots,u_{m}),\mathbf{v}=(v_{1},\ldots,v_{n})).\]

Let \(P_{\theta}\) denote the distribution of \(T\), when \(\theta=(F,G)\) is the true parameter value, \(\theta\in\Theta\).

That is, \(P_{\theta}(B)=\text{Prob}_{\theta}(T\in B)\), where \(\text{Prob}_{\theta}\) is the probability measure on the underlying probability space on which \(X_{i}\)'s and \(Y_{j}\)'s are defined.

Let \(\psi\) denote a _strictly increasing continuous_ function on \(\mathbb{R}\) onto \(\mathbb{R}\). Define, \(\forall\ \mathbf{x}=(\mathbf{u},\mathbf{v})\in\mathscr{X}\), \(g=g_{\psi}\) by

\[g_{\psi}\equiv g_{\psi}(\mathbf{u},\mathbf{v})=(\psi(u_{1}),\ldots,\psi(u_{m}),\psi(v_{1}),\ldots,\psi(v_{n})). \tag{5.122}\]Then \(g_{\psi}\) is a one-to-one map on \(\mathscr{X}\) onto \(\mathscr{X}\), and \(g_{\psi}\) and \(g_{\psi}^{-1}\) are both continuous. Indeed, if \(\psi^{-1}\) is the inverse of \(\psi\) on \(\mathbb{R}\) onto \(\mathbb{R}\), then \(g_{\psi}^{-1}\equiv g_{\psi^{-1}}\):

\[g_{\psi}^{-1}\mathbf{x}=(\psi^{-1}(u_{1}),\psi^{-1}(u_{2}),\dots,\psi^{-1}(u_{ m}),\psi^{-1}(v_{1}),\dots,\psi^{-1}(v_{n})).\]

Let \(\mathscr{G}\) denote the group of all such transformations on \(\mathscr{X}\) (corresponding to the group \(\Psi\) of all transformations \(\psi\) on \(\mathbb{R}\) onto \(\mathbb{R}\) such that \(\psi\) is strictly increasing and continuous). Then one obtains the following.

**Proposition 5.4**.:
1. _The testing problem (_5.119_) based on_ \(T=T(X_{1},\dots,X_{m},Y_{1},\dots,\ Y_{n})=(X_{(1)}\)_,_ \(\dots\)_,_ \(X_{(m)}\)_,_ \(Y_{(1)}\)_,_ \(\dots\)_,_ \(Y_{(n)})\) _is invariant under_ \(\mathscr{G}\)_._
2. _Every invariant test is a function of_ \[\mathbf{R}=(R_{1},R_{2},\dots,R_{m})\] (5.123) _where_ \(R_{i}\) _is the rank of_ \(X_{(i)}\) _among the_ \(m+n\) _values_ \(X_{(1)},\dots,X_{(m)}\)_,_ \(Y_{(1)},\dots,Y_{(n)}\) _(or, equivalently, the rank of_ \(X_{(i)}\) _among_ \(\{X_{1},\dots,X_{m},Y_{1},\)__\(\dots,Y_{n}\}\)_). That is,_ \(\mathbf{R}\) _is a maximal invariant under_ \(\mathscr{G}\)_._

Proof.: (a) Suppose \(\theta=(F,F)\in\Theta_{0}\). That is, \(X_{1},\dots,X_{m}\) are i.i.d. with (common) d.f. \(F\) and \(Y_{1},\dots,Y_{n}\) are i.i.d with (common) d.f. \(F\). Then, for any given \(\psi\in\Psi\), \(g_{\psi}X_{1},\dots,g_{\psi}X_{m}\) are i.i.d. with d.f. \(F\circ\psi^{-1}\). In particular, the distribution of \(T\) under this transformation (i.e., the distribution of \(T\circ\psi(X_{1},\dots,X_{m},Y_{1},\dots,Y_{n})=T(\psi X_{1},\dots,\psi X_{m},\psi Y_{1},\dots,\psi Y_{n})\)) is \(P_{\overline{g}_{\psi}\theta}\), with \(\overline{g}_{\psi}\theta=(F\circ\psi^{-1},F\circ\psi^{-1})\in\Theta_{0}\), when the distribution of \(T(X_{1},\dots,X_{m},Y_{1},\dots,Y_{n})\) is \(P_{\theta}\) with \(\theta=(F,F)\in\Theta_{0}\). Also, if \(F(x)\leq G(x)\ \forall\,x\), with strict inequality for some \(x=x_{0}\), say, then \(F\circ\psi^{-1}(x)\leq G\circ\psi^{-1}(x)\ \forall\,x\), with strict inequality for \(x=\psi(x_{0})\). Thus the distribution \(P_{\theta}\) of \(T(X_{1},\dots,X_{m},Y_{1},\dots,Y_{n})\) under \(\theta=(F,G)\in\Theta\) is transformed to the distribution \(P_{\overline{g}_{\psi}\theta}\) of \(T(\psi X_{1},\dots,\psi X_{m},\psi Y_{1},\dots,\psi Y_{n})\) under \(\overline{g}_{\psi^{\theta}}=(F\circ\psi^{-1},G\circ\psi^{-1})\in\Theta_{1}\).
2. It is clear that \(\mathbf{R}\) is invariant under \(\mathscr{G}\), since a strictly increasing transformation \(\psi\) does not change the relative orders of numbers on the line. To prove that \(\mathbf{R}\) is a maximal invariant, let \((\mathbf{u},\mathbf{v})\), \((\mathbf{u}^{\prime},\mathbf{v}^{\prime})\in\mathscr{X}\) be such that \(\mathbf{R}(\mathbf{u},\mathbf{v})=(r_{1},\dots,r_{m})=\mathbf{R}(\mathbf{u}^{ \prime},\mathbf{v}^{\prime})\). This means that the relative orders of \(u_{i}\)'s and \(v_{j}\)'s are the same as those of the \(u_{i}^{\prime}\)'s and \(v_{j}^{\prime}\)'s. For example, the number of \(v_{j}\)'s smaller than \(u_{1}\) is \(r_{1}-1\), the same is true of the number of \(v_{j}^{\prime}\)'s smaller than \(u_{1}^{\prime}\). In general, the number of \(v_{j}\)'s lying between \(u_{i}\) and \(u_{i+1}\) is \(r_{i+1}-r_{i}-1\), and the same is true of \(u_{i}^{\prime}\)'s and \(v_{j}^{\prime}\)'s. Now order \(m+n\) numbers \(u_{i}\)'s and \(v_{j}\)'s as \(w_{1}<w_{2}<\dots<w_{m+n}\), and similarly order the \(u_{i}^{\prime}\)'s and \(v_{j}^{\prime}\)'s as \(w_{1}^{\prime}<w_{2}^{\prime}<\dots<w_{m+n}^{\prime}\). From the above argument it follows that if \(w_{k}=u_{i}\) then \(w_{k}^{\prime}=u_{i}^{\prime}\), and if \(w_{k}=v_{j}\) then \(w_{k}^{\prime}=v_{j}^{\prime}\). Define the piecewise linear strictly increasing map \(\psi\) by \[\psi(w_{i})=w_{i}^{\prime},\qquad 1\leq i\leq m+n,\] with linear interpolation in \((w_{i},w_{i+1})\), \(1\leq i\leq m+n-1\). For \(x<w_{1}\), let \(\psi(x)\) be defined by extending the line segment joining \((w_{1},w_{1}^{\prime})\) and \((w_{2},w_{2}^{\prime})\). Similarly, for \(x>w_{m+n}\), define \(\psi(x)\) by extending the line segment joining \((w_{m+n-1},w_{m+n-1}^{\prime})\) and \((w_{m+n},w_{m+n}^{\prime})\). Then \(\psi(u_{i})=u_{i}^{\prime}\) and \(\psi(v_{j})=v_{j}^{\prime}\ \forall\ i,j\) and \(g_{\psi}(\mathbf{u},\mathbf{v})=(\mathbf{u}^{\prime},\mathbf{v}^{\prime})\).

Among commonly used rank tests for the two-sample problem (5.119) are the following.

_Example 5.16 (Wilcoxon,_ or _Mann-Whitney Test)._

\[\phi(r_{1},\ldots,r_{m})=\left\{\begin{array}{ll}1\ \mbox{if}&\sum_{i=1}^{m}r_{i}> c,\\ \gamma\ \mbox{if}&\sum_{i=1}^{m}r_{i}=c,\\ 0\ \mbox{if}&\sum_{i=1}^{m}r_{i}<c,\end{array}\right. \tag{5.124}\]

where \(r_{1}<\cdots<r_{m}\) are the values of \(R_{1}<R_{2}<\cdots<R_{m}\). The positive integer \(c\) and \(\gamma\in(0,1)\) are chosen so that the size is \(\alpha\).

_Example 5.17 (The Fisher-Yates Test)._

\[\phi(r_{1},\ldots,r_{m})=\left\{\begin{array}{ll}1\ \mbox{if}&\sum_{i=1}^{m} \eta_{(r_{i})}^{m+n}>c,\\ \gamma\ \mbox{if}&\sum_{i=1}^{m}\eta_{(r_{i})}^{m+n}=c,\\ 0\ \mbox{if}&\sum_{i=1}^{m}\eta_{(r_{i})}^{m+n}<c,\end{array}\right. \tag{5.125}\]

where \(\eta_{(r)}^{m+n}\) is the expected value of the \(r\)-th order statistic of a random sample of size \(m+n\) from a standard Normal distribution \(\mathbf{N}(0,1)\).

There is, of course, no uniformly most powerful test for (5.119). We may, however, look for _admissible tests_ in the class of all rank tests. Both tests (5.124), (5.125) may be shown to be admissible in this sense. We will later discuss asymptotic (as \(m\to\infty\), \(n\to\infty\)) superiority of these tests compared to the classical \(t\)-test based on \(\overline{X}-\overline{Y}\), in the location model: \(F(x)=F_{0}(x)\), \(G(x)=F_{0}(x+\theta)\), \(\theta<0\). Here \(F_{0}\) is a given continuous distribution function.

### 5.9 Linear Models

#### The Gauss-Markov Theorem

Consider the _linear model_

\[\mathbf{X}=A\boldsymbol{\theta}+\boldsymbol{\varepsilon}, \tag{5.126}\]

where \(\mathbf{X}=(X_{1},\ldots,X_{n})^{\prime}\) is the _observation vector, \(\boldsymbol{\theta}=(\theta_{1},\ldots,\theta_{k})^{\prime}\in\mathbb{R}^{k}\)_ is the vector of _unknown parameters, \(A\)_ is the \((n\times k)\) known _coefficient matrix_

\[A=\begin{pmatrix}a_{11}&a_{12}&\cdots&a_{1k}\\ a_{21}&a_{22}&\cdots&a_{2k}\\ \vdots&\vdots\\ a_{n1}&a_{n2}&\cdots&a_{nk}\end{pmatrix}=[\boldsymbol{\alpha}_{1}\boldsymbol{ \alpha}_{2}\cdots\boldsymbol{\alpha}_{k}]=\begin{bmatrix}\boldsymbol{\beta}_{ 1}\\ \boldsymbol{\beta}_{2}\\ \vdots\\ \boldsymbol{\beta}_{n}\end{bmatrix}, \tag{5.127}\]

and \(\boldsymbol{\varepsilon}=(\varepsilon_{1},\varepsilon_{2},\ldots,\varepsilon _{n})^{\prime}\) is the vector of _unobservable errors,_

\[E\varepsilon_{i}=0,\quad E\varepsilon_{i}\varepsilon_{j}=\sigma^{2}\delta_{ ij}\quad(1\leq i,\,j\leq n), \tag{5.128}\]

\(0<\sigma^{2}<\infty\), \(\sigma^{2}\) unknown.

**Definition 5.9**.: A linear parametric function \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}=\sum_{j=1}^{k}\ell_{j}\theta_{j}\) is said to be _estimable_ if there exists a linear function of the observations \(\mathbf{d}^{\prime}\mathbf{X}=\sum_{i=1}^{n}d_{i}X_{i}\) such that

\[E_{(\theta,\sigma^{2})}\mathbf{d}^{\prime}\mathbf{X}=\boldsymbol{\ell}^{\prime }\boldsymbol{\theta}\qquad\forall\ \boldsymbol{\theta}\in\mathbb{R}^{k},\ \sigma^{2}>0. \tag{5.129}\]

A linear unbiased estimator \(\mathbf{d}^{\prime}_{0}\mathbf{X}\) of \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\) is said to be the _best linear unbiased estimator_ or _BLUE,_ if

\[E_{(\boldsymbol{\theta},\sigma^{2})}(\mathbf{d}^{\prime}_{0}\mathbf{X}- \boldsymbol{\ell}^{\prime}\boldsymbol{\theta})^{2}=\inf E_{(\boldsymbol{ \theta},\sigma^{2})}(\mathbf{d}^{\prime}\mathbf{X}-\boldsymbol{\ell}^{\prime} \theta)^{2}\quad\forall\ \boldsymbol{\theta}\in\mathbb{R}^{k},\ \sigma^{2}>0, \tag{5.130}\]

where the infimum is taken over the class \(\mathbf{d}^{\prime}\mathbf{X}\) of all linear unbiased estimators of \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\).

For the present subsection, nothing is assumed about the distribution of the errors other than (5.128). Hence the notation \(E_{(\boldsymbol{\theta},\sigma^{2})}\) here simply means expectation under an arbitrary distribution satisfying (5.128) and a given \(\boldsymbol{\theta}\). In later subsections, \(\varepsilon\)'s are also assumed to be Normal, so that the pair \((\boldsymbol{\theta},\sigma^{2})\) specifies a distribution of \(\mathbf{X}\).

**Theorem 5.7** (Gauss-Markov Theorem).:
1. \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\) _is estimable if and only if_ \(\boldsymbol{\ell}\in\mathscr{L}_{r}\)_--the vector space spanned by the rows_ \(\boldsymbol{\beta}_{i}\)__\((1\leq i\leq n)\) _of_ \(A\)_._
2. _Suppose_ \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\) _is estimable and_ \(\mathbf{d}^{\prime}\mathbf{X}\) _is an unbiased estimator of_ \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\)_. Then the unique_ BLUE _of_ \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\) _is_ \(\mathbf{d}^{\prime}_{0}\mathbf{X}\)_, where_ \(\mathbf{d}_{0}\) _is the orthogonal projection of_ \(\mathbf{d}\) _on the vector space_ \(\mathscr{L}_{c}\) _spanned by the column vectors_ \(\boldsymbol{\alpha}_{j}\)__\((1\leq j\leq k)\) _of_ \(A\)_._

Proof.:
1. One has \(E_{(\boldsymbol{\theta},\sigma^{2})}\mathbf{d}^{\prime}\mathbf{X}=\boldsymbol{ \ell}^{\prime}\boldsymbol{\theta}\) for some \(\mathbf{d}\in\mathbb{R}^{n}\) if and only if \[\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\equiv\sum_{j=1}^{k}\ell_{j} \theta_{j}=E_{(\boldsymbol{\theta},\sigma^{2})}\mathbf{d}^{\prime}\mathbf{X}= \mathbf{d}^{\prime}A\boldsymbol{\theta}\equiv\sum_{j=1}^{k}\left(\sum_{i=1}^{n }d_{i}\boldsymbol{\beta}_{i}\right)_{j}\theta_{j}\quad\forall\ \boldsymbol{\theta}\in \mathbb{R}^{k},\] i.e., iff \(\boldsymbol{\ell}=\sum_{i=1}^{n}d_{i}\boldsymbol{\beta}_{i}\) for some \(\mathbf{d}\in\mathbb{R}^{n}\).
2. Let \(\mathbf{d}^{\prime}\mathbf{X}\) be an unbiased estimator of \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\), and \(\mathbf{d}_{0}\) the orthogonal projection of \(\mathbf{d}\) onto \(\mathscr{L}_{c}\). Then \(E_{(\boldsymbol{\theta},\sigma^{2})}(\mathbf{d}-\mathbf{d}_{0})^{\prime} \mathbf{X}=(\mathbf{d}-\mathbf{d}_{0})^{\prime}\ A\boldsymbol{\theta}=(\mathbf{ d}-\mathbf{d}_{0})^{\prime}\sum_{j=1}^{k}\theta_{j}\boldsymbol{\alpha}_{j}=0\), since \(\mathbf{d}-\mathbf{d}_{0}\) is orthogonal to \(\sum_{j=1}^{k}\theta_{j}\boldsymbol{\alpha}_{j}\in\mathscr{L}_{c}\). It follows that \(\mathbf{d}^{\prime}_{0}\mathbf{X}\) is an unbiased estimator of \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\): \[E_{(\boldsymbol{\theta},\sigma^{2})}\mathbf{d}^{\prime}_{0}\mathbf{X}=E_{( \boldsymbol{\theta},\sigma^{2})}[\mathbf{d}^{\prime}\mathbf{X}-(\mathbf{d}- \mathbf{d}_{0})^{\prime}\mathbf{X}]=E_{(\boldsymbol{\theta},\sigma^{2})} \mathbf{d}^{\prime}\mathbf{X}=\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}.\] Also, \(E_{(\boldsymbol{\theta},\sigma^{2})}(\mathbf{d}^{\prime}_{0}\mathbf{X}- \boldsymbol{\ell}^{\prime}\boldsymbol{\theta})(\mathbf{d}^{\prime}\mathbf{X}- \mathbf{d}^{\prime}_{0}\mathbf{X})=E_{(\boldsymbol{\theta},\sigma^{2})}( \mathbf{d}^{\prime}_{0}\mathbf{X})(\mathbf{d}^{\prime}\mathbf{X}-\mathbf{d}^{ \prime}_{0}\mathbf{X})=\sigma^{2}\mathbf{d}^{\prime}_{0}(\mathbf{d}-\mathbf{d} _{0})=0\), so that \[E_{(\boldsymbol{\theta},\sigma^{2})}(\mathbf{d}^{\prime}\mathbf{X}- \boldsymbol{\ell}^{\prime}\boldsymbol{\theta})^{2}=E_{(\boldsymbol{\theta}, \sigma^{2})}(\mathbf{d}^{\prime}_{0}\mathbf{X}-\boldsymbol{\ell}^{\prime} \boldsymbol{\theta}+\mathbf{d}^{\prime}\mathbf{X}-\mathbf{d}^{\prime}_{0} \mathbf{X})^{2}\] \[= E_{(\boldsymbol{\theta},\sigma^{2})}(\mathbf{d}^{\prime}_{0} \mathbf{X}-\boldsymbol{\ell}^{\prime}\boldsymbol{\theta})^{2}+E_{(\boldsymbol{ \theta},\sigma^{2})}(\mathbf{d}^{\prime}\mathbf{X}-\mathbf{d}^{\prime}_{0} \mathbf{X})^{2}\geq E_{(\boldsymbol{\theta},\sigma^{2})}(\mathbf{d}^{\prime}_{0} \mathbf{X}-\boldsymbol{\ell}^{\prime}\boldsymbol{\theta})^{2},\] with a strict inequality unless \(E_{(\boldsymbol{\theta},\sigma^{2})}(\mathbf{d}^{\prime}\mathbf{X}-\mathbf{d}^{ \prime}_{0}\mathbf{X})^{2}=0\), i.e., unless \(\sigma^{2}|\mathbf{d}-\mathbf{d}_{0}|^{2}=0\), i.e., unless \(\mathbf{d}_{0}=\mathbf{d}\), or \(\mathbf{d}\in\mathscr{L}_{c}\).

To prove uniqueness, let \(\boldsymbol{\gamma}^{\prime}\mathbf{X}\) be another unbiased estimator of \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\), and let \(\boldsymbol{\gamma}_{0}\) be the orthogonal projection of \(\boldsymbol{\gamma}\) on \(\mathscr{L}_{c}\). Then \(E_{(\boldsymbol{\theta},\sigma^{2})}(\boldsymbol{\gamma}^{\prime}_{0}\mathbf{X}- \mathbf{d}^{\prime}_{0}\mathbf{X})=0\ \forall\ \boldsymbol{\theta}\), i.e., \((\boldsymbol{\gamma}_{0}-\mathbf{d}_{0})\cdot\sum_{i=1}^{k}\theta_{i}\alpha_{i}=0 \ \forall\boldsymbol{\theta}\in\mathbb{R}^{k}\). This implies \(\boldsymbol{\gamma}_{0}-\mathbf{d}_{0}\) is orthogonal to \(\mathscr{L}_{c}\). But \(\boldsymbol{\gamma}_{0}-\mathbf{d}_{0}\in\mathscr{L}_{c}\). Hence \(\boldsymbol{\gamma}_{0}-\mathbf{d}_{0}=0\).

_Remark 5.10_.: _Suppose \(n\geq k\), and \(A\) is of full rank \(k\). Then the vector space \(\mathscr{L}_{r}\) is of rank \(k\), so that every \(\boldsymbol{\ell}\) in \(\mathbb{R}^{k}\) belongs to \(\mathscr{L}_{r}\). Therefore, all linear parametric functions are estimable, by part (a) of the Gauss-Markov Theorem (Theorem 5.7). Also, in this case, \(A^{\prime}A\) is a non-singular \(k\times k\) matrix, and_

\[E_{(\boldsymbol{\theta},\sigma^{2})}A^{\prime}\mathbf{X}=A^{\prime}A \boldsymbol{\theta},\quad\text{or }E_{(\boldsymbol{\theta},\sigma^{2})}(A^{ \prime}A)^{-1}A^{\prime}\mathbf{X}=\boldsymbol{\theta}\quad\forall\ \boldsymbol{ \theta}\in\mathbb{R}^{k}.\]

_Hence an unbiased estimator of the vector \(\boldsymbol{\theta}\) is given by_

\[\widehat{\boldsymbol{\theta}} = (A^{\prime}A)^{-1}A^{\prime}\mathbf{X}=D_{0}^{\prime}\mathbf{X}, \quad\text{cov}\,\widehat{\boldsymbol{\theta}}=\sigma^{2}D_{0}^{\prime}D_{0}= \sigma^{2}(A^{\prime}A)^{-1}, \tag{5.131}\] \[D_{0} := A(A^{\prime}A)^{-1}=[\alpha_{1}\alpha_{2}\cdots\alpha_{k}] \begin{bmatrix}b_{11}\ b_{12}\ \cdots\ b_{1k}\\ b_{21}\ b_{22}\ \cdots\ b_{2k}\\ \vdots\ \vdots\ \ \ \ \ \ \ \ \ \ \ \vdots\\ b_{k1}\ b_{k2}\ \cdots\ b_{kk}\end{bmatrix},\text{ say.}\]

_The \(j\)-th column of \(D_{0}\) is_

\[\mathbf{d}_{0,j}=\sum_{j=1}^{k}b_{j^{\prime}j}\alpha_{j^{\prime}}\in\mathscr{L }_{c},\]

_and_

\[\hat{\boldsymbol{\theta}}_{j}=\mathbf{d}_{0,j}^{\prime}\mathbf{X}\]

_is, therefore, the best linear unbiased estimator of \(\theta_{j}\) (\(1\leq j\leq k\)), as is \(\sum_{1}^{k}\ell_{j}\hat{\theta}_{j}=\sum_{1}^{k}(\ell_{j}d_{0,j})^{\prime} \mathbf{X}\) of \(\sum_{1}^{k}\ell_{j}\theta_{j}\), whatever be \(\boldsymbol{\ell}\in\mathbb{R}^{k}\)._

_In view of part (b) of the Gauss-Markov Theorem, the best linear unbiased estimator of an estimable function \(\boldsymbol{\ell}^{\prime}\boldsymbol{\theta}\) is also called its least squares estimator._

#### Testing in Linear Models

In addition to (5.126), (5.128), assume

\[\varepsilon_{i}\ \text{ are i.i.d. }\mathbf{N}(0,\sigma^{2})\qquad(1\leq i\leq n )\ (0<\sigma^{2}<\infty).\]

In fact, for simplicity let us assume for the time being that we have the _canonical linear model_ for the observations \(Y_{1},\ldots,Y_{n}\), i.e.,

\[\mathbf{Y}=\begin{pmatrix}\mu_{1}\\ \mu_{2}\\ \vdots\\ \mu_{k}\\ 0\\ 0\\ \vdots\\ 0\end{pmatrix}+\begin{pmatrix}\varepsilon_{1}\\ \varepsilon_{2}\\ \vdots\\ \varepsilon_{n}\end{pmatrix} \tag{5.132}\]where \((\mu_{1},\mu_{2},\ldots,\mu_{k})^{\prime}\in\mathbb{R}^{k}\), \(\Theta=\{(\mu_{1},\ldots,\mu_{k},\sigma^{2}):\mu_{i}\in\mathbb{R}\), \(1\leq i\leq k\), \(0<\sigma^{2}<\infty\}=\mathbb{R}^{k}\times(0,\infty)\) and \(\varepsilon_{i}\)'s are i.i.d. \(N(0,\sigma^{2})\). We want to test

\[H_{0}:\mu_{1}=\mu_{2}=\cdots=\mu_{r}=0\quad(\Theta_{0}=\{(0,\ldots,0)\}\times \mathbb{R}^{k-r}\times(0,\infty))\]

against

\[H_{1}:H_{0}\text{ is not true}\quad(\Theta_{1}=\Theta\backslash\Theta_{0}), \tag{5.133}\]

where \(1\leq r\leq k\). For \(r=k\), \(\Theta_{0}=\{\mathbf{O}\}\times(0,\infty)\).

The testing problem is invariant under each of the following three groups of transformations:

\[\mathscr{G}_{1}:g_{1}(\mathbf{y})=\mathbf{z},\quad z_{i} =y_{i}+c_{i}\quad(r+1\leq i\leq k)\] \[z_{i} =y_{i}\quad\text{for }i\not\in\{r+1,r+2,\ldots,r+k\}.\]

\(c_{i}\) (\(r+1\leq i\leq k\)) are arbitrary reals.

\[\mathscr{G}_{2}:g_{2}(\mathbf{y})=\mathbf{z},\quad\begin{pmatrix}z_{1}\\ z_{2}\\ \vdots\\ z_{r}\end{pmatrix}=O_{r}\begin{pmatrix}y_{1}\\ y_{2}\\ \vdots\\ y_{r}\end{pmatrix},\quad z_{i}=y_{i}\quad\text{for }r+1\leq i\leq n,\]

where \(O_{r}\) is an arbitrary \(r\times r\) orthogonal matrix.

\[\mathscr{G}_{3}:g_{3}(\mathbf{y})=b\mathbf{y},\quad b>0.\]

Let \(\mathscr{G}\) be the group of transformations generated by \(\varGamma=\mathscr{G}_{1}\cup\mathscr{G}_{2}\cup\mathscr{G}_{3}\).

**Proposition 5.5**.: _The statistic \(T(\mathbf{y})\equiv\frac{\sum_{i}^{r}y_{i}^{2}}{\sum_{k+1}^{k}y_{i}^{2}}\) is a maximal invariant function of the sufficient statistic_

\[S(\mathbf{y})\equiv\left(y_{1},\ldots,y_{k},\sum_{i=k+1}^{n}y_{i}^{2}\right). \tag{5.134}\]

Proof.: The joint p.d.f. of \(Y_{1},\ldots,Y_{n}\) is

\[\left(\frac{1}{\sqrt{2\pi\sigma^{2}}}\right)^{n}e^{-\frac{1}{2\sigma^{2}}\sum _{i=1}^{k}(y_{i}-\mu_{i})^{2}-\frac{1}{2\sigma^{2}}\sum_{i=k+1}^{n}y_{i}^{2}}, \tag{5.135}\]

implying \(S(\mathbf{y})\) is indeed a sufficient statistic for \((\mu_{1},\ldots,\mu_{k},\sigma^{2})\). (Use the Factorization Theorem). We need to show that a function \(T(\mathbf{y})\), of \(S(\mathbf{y})\), which is also invariant under \(\mathscr{G}\), is a function of \(T(\mathbf{y})\).

Now a maximal invariant under \(\mathscr{G}_{1}\) is

\[W_{1}(\mathbf{y})=(y_{1},\ldots,y_{r},y_{k+1},\ldots,y_{n}) \tag{5.136}\]

Observe that \(W_{1}(g_{1}\mathbf{y})=W_{1}(\mathbf{y})\), since \(W_{1}\) does not involve the coordinates \(y_{r+1},\ldots,y_{k}\) which are the only ones affected by \(g_{1}\); also, if \(W_{1}(\mathbf{y})=W_{1}(\mathbf{z})\), then \(\mathbf{z}=(y_{1},\ldots,y_{r},y_{r+1}+(z_{r+1}-y_{r+1})\), \(\ldots,y_{k}+(z_{k}-y_{k})\),\(y_{k+1},\ldots,y_{n})=g_{1}(\mathbf{y})\) with \(c_{i}=z_{i}-y_{i}\) for \(r+1\leq i\leq k\). Thus \(\mathbf{y}\) and \(\mathbf{z}\) belong to the same orbit under \(\mathscr{G}_{1}\) proving that \(W_{1}\) is a maximal invariant under \(\mathscr{G}_{1}\). Hence \(T_{1}\) must be a function of \(W_{1}\), say,

\[T_{1}(\mathbf{y})=f_{1}(y_{1},\ldots,y_{r},y_{k+1},\ldots,y_{n}). \tag{5.137}\]A maximal invariant under \(\mathscr{G}_{2}\) is

\[W_{2}(\mathbf{y})=\left(\sum_{i=1}^{r}y_{i}^{2},y_{k+1},\ldots,y_{n}\right) \tag{5.138}\]

For, \(W_{2}(g\mathbf{y})=W_{2}(\mathbf{y})\), since \(\left\|O_{r}\begin{pmatrix}y_{1}\\ \vdots\\ y_{r}\end{pmatrix}\right\|^{2}=\left\|\begin{pmatrix}y_{1}\\ \vdots\\ y_{r}\end{pmatrix}\right\|^{2}\). Also, given two vectors \((z_{1},\ldots,z_{r})^{\prime}\) and \((y_{1},y_{2},\ldots,y_{r})^{\prime}\) of equal length there exists an orthogonal transformation \(O_{r}\) such that \(O_{r}(z_{1},\ldots,z_{r})=(y_{1},\ldots,y_{r})\). Hence \(W_{2}(\mathbf{y})=W_{2}(\mathbf{z})\) implies that \(\mathbf{z}=g(\mathbf{y})\) for some \(g\in\mathscr{G}_{2}\). Therefore, \(T_{1}(\mathbf{y})\) depends on \(y_{1},\ldots,y_{r}\) only through \(\sum_{1}^{r}y_{i}^{2}\), i.e., \(T_{1}\) is of the form [see (5.137)]

\[T_{1}(\mathbf{y})=f_{2}\left(\sum_{i=1}^{r}y_{i}^{2},\,y_{k+1},\ldots,y_{n} \right). \tag{5.139}\]

Now since \(T_{1}(\mathbf{y})\) is a function of \(S(\mathbf{y})\), it must be of the form (using (5.134) and (5.139))

\[T_{1}(\mathbf{y})=f_{3}\left(\sum_{i=1}^{r}y_{i}^{2},\,\sum_{k+1}^{n}y_{i}^{2} \right). \tag{5.140}\]

In view of invariance under \(\mathscr{G}_{3}\), one must have

\[T_{1}(b\mathbf{y})=T_{1}(\mathbf{y})\qquad\forall\ b>0, \tag{5.141}\]

i.e.,

\[f_{3}\left(b^{2}\sum_{1}^{r}y_{i}^{2},b^{2}\sum_{k+1}^{n}y_{i}^{2}\right)=f_{3 }\left(\sum_{1}^{r}y_{i}^{2},\sum_{k+1}^{n}y_{i}^{2}\right). \tag{5.142}\]

This implies \(T_{1}\) is of the form

\[T_{1}(\mathbf{y})=f_{4}\left(\sum_{1}^{r}y_{i}^{2}\bigg{/}\sum_{k+1}^{n}y_{i}^ {2}\right)\quad\text{if}\ \sum_{k+1}^{n}y_{i}^{2}\neq 0, \tag{5.143}\]

where \(f_{4}\) is one-to-one, i.e., strictly monotone on \((0,\infty)\). For _if this is not the case_ then there exist \(\mathbf{y}\), \(\mathbf{z}\) such that \(\sum_{1}^{r}y_{i}^{2}/\sum_{k+1}^{n}y_{i}^{2}=\sum_{1}^{r}z_{i}^{2}/\sum_{k+1} ^{n}z_{i}^{2}\), but \(T_{1}(\mathbf{y})\neq T_{1}(\mathbf{z})\). Write \(b=\left(\sum_{k+1}^{n}z_{i}^{2}/\sum_{k+1}^{n}y_{i}^{2}\right)^{1/2}\) (assuming \(\sum_{k+1}^{n}z_{i}^{2}>0\), by (5.143)). Then, by invariance under \(\mathscr{G}_{3}\),

\[T_{1}(b\mathbf{y})=T_{1}(\mathbf{y})\]

so that

\[T_{1}(\mathbf{z}) =f_{3}\left(\sum_{1}^{r}z_{i}^{2},\sum_{k+1}^{n}z_{i}^{2}\right) =f_{3}\left(\sum_{1}^{r}z_{i}^{2}\over\sum_{k+1}^{n}z_{i}^{2}\,\cdot\,\sum_{k +1}^{n}z_{i}^{2},b^{2}\sum_{k+1}^{n}y_{i}^{2}\right)\] \[=f_{3}\left(\sum_{k+1}^{r}y_{i}^{2}\over\sum_{k+1}^{n}y_{i}^{2} \,\cdot\,\sum_{k+1}^{n}z_{i}^{2},\,b^{2}\sum_{k+1}^{n}y_{i}^{2}\right)=f_{3} \left(b^{2}\sum_{1}^{r}y_{i}^{2},\,b^{2}\sum_{k+1}^{n}y_{i}^{2}\right)\] \[=T_{1}(b\mathbf{y})=T_{1}(\mathbf{y}),\]

which is a contradiction. Hence (5.143) holds.

**Theorem 5.8**.: _For the canonical linear model, the U.M.P. invariant (under \(\mathscr{G}\)) test of \(H_{0}\) against \(H_{1}\) (of size \(\alpha\)) is given by_

\[\varphi(\mathbf{y})=\left\{\begin{array}{ll}1&\mbox{if $\frac{\sum_{1}^{r}y_{i}^{2}}{r}$} \bigg{/}\frac{\sum_{k+1}^{n}y_{i}^{2}}{n-k}>F_{\alpha}(r,n-k)\\ 0&\mbox{if $\frac{\sum_{1}^{r}y_{i}^{2}}{r}$}\bigg{/}\frac{\sum_{k+1}^{n}y_{i}^{2}}{n- k}\leq F_{\alpha}(r,n-k)\end{array}\right. \tag{5.144}\]

_where \(F_{1-\alpha}(r,n-k)\) is the \((1-\alpha)\)th quantile of the \(F\)-distribution with d.f.s \(r\) for the numerator, and \(n-k\) for the denominator._

Proof.: We know that the UMP invariant test must be based on \(\sum_{1}^{r}y_{i}^{2}/\sum_{k+1}^{n}y_{i}^{2}\). We need to prove that among all tests which are functions of this ratio, the test (5.144) is UMP. This follows from the Neyman-Pearson Lemma and the fact that the ratio \(h_{\gamma^{2}}(u)/h_{0}(u)\) of the p.d.f. of \(\left(\sum_{1}^{r}y_{i}^{2}/r\right)/\left(\sum_{k+1}^{n}y_{i}^{2}/(n-k)\right)\) under \(H_{1}\) and under \(H_{0}\) is monotone increasing in \(u\) (see Remark 5.4 and Example C.6). Here \(\gamma^{2}=\sum_{i=1}^{r}\,\mu_{i}^{2}/\sigma^{2}\). 

_Remark 5.11_.: For (5.143) note that \(\mbox{Prob}_{\mu}(\sum_{k+1}^{n}Y_{i}^{2}=0)=0\), so that one can restrict the observation space to exclude the set \(\{\sum_{k+1}^{n}y_{i}^{2}=0\}\).

_Reduction of the General Case to the Canonical Model_ The general linear hypothesis in the linear model (5.126) is of the form

\[H_{0}:B\boldsymbol{\theta}=0, \tag{5.145}\]

where \(B=((b_{ij}))\) is an \(r\times k\) matrix of full rank \(r\) (\(1\leq r\leq k\)). Without loss of generality one may assume that the first \(r\) columns of \(B\) are linearly independent. Then the matrix \(B_{r}\), which is \(r\times r\) and is composed of the first \(r\) columns of \(B\), is nonsingular, and one may rewrite (5.145) as

\[B_{r}\begin{pmatrix}\theta_{1}\\ \vdots\\ \theta_{r}\end{pmatrix}=-\begin{pmatrix}b_{1\,r+1}&\cdots&b_{1k}\\ \vdots&\vdots\\ b_{r\,r+1}&\cdots&b_{rk}\end{pmatrix}\begin{pmatrix}\theta_{r+1}\\ \vdots\\ \theta_{k}\end{pmatrix},\]

or,

\[\begin{pmatrix}\theta_{1}\\ \vdots\\ \theta_{r}\end{pmatrix}=-B_{r}^{-1}\begin{pmatrix}b_{1\,r+1}&\cdots&b_{1k}\\ \vdots&\vdots\\ b_{r\,r+1}&\cdots&b_{rk}\end{pmatrix}\begin{pmatrix}\theta_{r+1}\\ \vdots\\ \theta_{k}\end{pmatrix}=\begin{pmatrix}d_{1\,r+1}&\cdots&d_{1k}\\ \vdots&\vdots\\ d_{r\,r+1}&\cdots&d_{rk}\end{pmatrix}\begin{pmatrix}\theta_{r+1}\\ \vdots\\ \theta_{k}\end{pmatrix}, \tag{5.146}\]

say.

Observe that in the linear model (5.126), we assume that \(\mathbf{X}\) is Normal with mean (vector) \(A\boldsymbol{\theta}\) and dispersion matrix \(\sigma^{2}I_{n}\), and \(A\) is of full rank \(k\). Here \(\boldsymbol{\theta}\) is unknown (i.e., \(\theta_{i}\)'s can take arbitrary real values), and \(\sigma^{2}>0\) is unknown; and \(I_{n}\) is the \(n\times n\) identity matrix. Thus \(E\mathbf{X}\equiv\sum_{1}^{k}\theta_{i}\boldsymbol{\alpha}_{i}\) (\(\boldsymbol{\alpha}_{i}\) being the \(i\)th column of \(A\)) spans the subspace of \(\mathscr{L}_{c}\) of dimension \(k\) of an \(n\)-dimensional Euclidean space \(E_{n}\), say, which may be identified with \(\mathbb{R}^{n}\) (w.r.t. the standard orthonormal basis). Under \(H_{0}\), \(E\mathbf{X}\) lies in a \((k-r)\)-dimensional subspace, say \(\mathscr{L}_{c,0}\), of \(\mathscr{L}_{c}\) (expressing \(\theta_{1},\ldots,\theta_{r}\) in terms of \(\theta_{r+1},\ldots,\theta_{k}\), by (5.146)). We will now construct an \(n\times n\) orthogonal matrix \(\mathbf{O}\) such that \(\mathbf{Y}=\mathbf{OX}\) is in the form (5.132) of a canonicalmodel, with \(H_{0}\) and \(H_{1}\) given by (5.133). For this (1) choose the first \(r\) rows of \({\bf O}\) to lie in \(\mathscr{L}_{c}\), but orthogonal to \(\mathscr{L}_{c,0}\), (2) choose the next \(k-r\) rows of \({\bf O}\) to span \(\mathscr{L}_{c,0}\), and (3) choose the last \(n-k\) rows of \({\bf O}\) orthogonal to \(\mathscr{L}_{c}\). Because of the orthogonality of \({\bf O}\), the error \({\bf O}\,\mathbf{\epsilon}\) remains \(N({\bf O},\sigma^{2}I_{n})\), and \(\mathbf{\mu}\equiv E\mathbf{{\bf Y}}={\bf O}\,A\mathbf{\theta}\) span \(\mathscr{L}\). Also, under \(H_{0}\), the first \(r\) elements of \(\mu\) are zero, leaving the next \(k-r\) elements arbitrary (unknown).

Before turning to Examples, let us note that, because of orthogonality of \({\bf O}\) in the relation \({\bf Y}={\bf O}\,{\bf X}\), one has \(\|{\bf X}-E\mathbf{{\bf X}}\|^{2}=\|{\bf Y}-E\mbox{\boldmath${\bf Y}$ }\|^{2}\). Hence

\[\sum_{i=k+1}^{n}Y_{i}^{2} =\min_{\mathbf{\mu}}\left\{\sum_{i=1}^{k}(Y_{i}-\mu_{i} )^{2}+\sum_{i=k+1}^{n}Y_{i}^{2}\right\}=\min_{E\mathbf{{\bf Y}}}\|{ \bf Y}-E\mathbf{{\bf Y}}\|^{2}\] \[=\min_{\mathbf{{\bf m}}\equiv E\mathbf{{\bf X}} \in\mathscr{L}_{c}}\|{\bf X}-E\mathbf{{\bf X}}\|^{2}=\|{\bf X}-\hat{ \mathbf{{\bf m}}}\|^{2}, \tag{5.147}\]

where \(\hat{\mathbf{{\bf m}}}=\mathbf{A}\hat{\mathbf{\theta }}\) is the projection of \({\bf X}\) on \(\mathscr{L}_{c}\). Similarly, writing \(\mathbf{\mu}^{0}=(0,\ldots,0,\)\(\mu_{r+1}\), \(\ldots\), \(\mu_{k}\)), one has

\[\sum_{i=1}^{r}Y_{i}^{2}+\sum_{i=k+1}^{n}Y_{i}^{2}=\min_{\mathbf{\mu}^ {0}}\|{\bf Y}-E\mathbf{{\bf Y}}\|^{2}=\min_{\mathbf{{\bf m}} \in\mathscr{L}_{c,0}}\|{\bf X}-{\bf m}\|^{2}=\|{\bf X}-\hat{\mathbf{{ \bf m}}}\|^{2},\]

where \(\hat{\mathbf{{\bf m}}}\) is the projection of \({\bf X}\) on \(\mathscr{L}_{c,0}\), \(\hat{\mathbf{{\bf m}}}=\mathbf{A}\hat{\hat{\mathbf{ \theta}}}\), say. Hence, by Pythagoras,

\[\sum_{i=1}^{r}Y_{i}^{2}=\|{\bf X}-\hat{\hat{\mathbf{{\bf m}}}}\|^{2} -\|{\bf X}-\hat{\mathbf{{\bf m}}}\|^{2}=\|\hat{\mathbf{{\bf m }}}-\hat{\hat{\mathbf{{\bf m}}}}\|^{2}. \tag{5.148}\]

Therefore, the UMP invariant test of size \(\alpha\) for \(H_{0}:B\mathbf{\theta}=0\) against \(H_{1}:B\mathbf{\theta}\neq 0\) is given by (See Theorem 5.8)

\[\mbox{Reject $H_{0}$ iff}\quad\frac{\|\hat{\mathbf{{\bf m}}}-\hat{ \hat{\mathbf{{\bf m}}}}\|^{2}/r}{\|{\bf X}-\hat{\mathbf{{\bf m }}}\|^{2}/(n-k)}\equiv\frac{\|A(\hat{\mathbf{\theta}}-\hat{\hat{ \mathbf{\theta}}})\|^{2}/r}{\|{\bf X}-A\hat{\mathbf{\theta} }\|^{2}/(n-k)}>F_{1-\alpha}(r,n-k), \tag{5.149}\]

where \(\hat{\mathbf{\theta}}\), \(\hat{\hat{\mathbf{\theta}}}\) are, respectively, the least squares estimators of \(\theta\) under the linear model (5.126) and under \(H_{0}\).

_Example 5.18_ (One-Way Layout, or the \(k\)-Sample Problem).: For \(k\geq 2\), let \(X_{ji}\) be independent \(N(\theta_{i},\sigma^{2})\) random variables (\(1\leq j\leq n_{i}\), \(1\leq i\leq k\)). We wish to test \(H_{0}:\theta_{1}=\theta_{2}=\cdots=\theta_{k}\) against \(H_{1}\) which says \(H_{0}\) is not true. Here \(n=\sum_{1}^{k}n_{i}\), \(r=k-1\), and \({\bf X}=A\mathbf{\theta}+\mathbf{\epsilon}\), where \(a_{ji}=1\) for \(n_{1}+\cdots+n_{i-1}<j\leq n_{1}+\cdots+n_{i}\) (\(2\leq i\leq k\)), \(a_{j1}=1\) for \(1\leq j\leq n_{1}\): \(a_{ji}=0\) otherwise. The minimum of \(\|{\bf X}-A\mathbf{\theta}\|^{2}\equiv\sum_{i=1}^{k}\sum_{j=1}^{n_{i}} (X_{ji}-\theta_{i})^{2}\) over all \(\mathbf{\theta}\in\mathbb{R}^{k}\) is attained by taking \(\theta_{i}=\hat{\theta}_{i}=\overline{X}_{\cdot i}\equiv\frac{1}{n_{i}}\sum_ {j=1}^{n_{i}}X_{ji}\) (\(1\leq i\leq k\)). Under \(H_{0}\), letting \(\theta_{0}\) denote the common value of \(\theta_{i}\)'s, the minimum value of \(\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}\) (\(X_{ji}-\theta_{0}\))\({}^{2}\) is attained by \(\theta_{0}=\hat{\theta}_{0}=\frac{1}{n}\sum_{j,i}X_{ji}=\overline{X}_{\cdot\cdot}\), say. Hence the test (5.149) becomes, in view of (5.147), (5.148),

\[\mbox{Reject $H_{0}$ iff}\quad\frac{\sum_{i=1}^{k}n_{i}(\overline{X}_{\cdot i}- \overline{X}_{\cdot\cdot})^{2}/(k-1)}{\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(X_{ji} -\overline{X}_{\cdot i})^{2}/(n-k)}>F_{1-\alpha}(k-1,n-k). \tag{5.150}\]Example 5.19 (Two-Way Layout): Consider an agricultural field experiment in which certain _varieties_ of a crop, say wheat, is grown on plots of equal size treated with different _fertilizers_. The objective is to study the effect of variety and fertilizer on the yield of the crop. Let \(X_{ijq}\) denote the yield of the \(q\)-th plot treated with fertilizer \(j\) on which the \(i\)th variety is grown (\(q=1,\ldots,S\); \(j=1,2,\ldots,J\); \(i=1,2,\ldots,I\)), \(I>1\), \(J>1\), \(S>1\). Assume the linear model

\[X_{ijq}=\theta_{ij}+\varepsilon_{ijq}, \tag{5.151}\]

where \(\varepsilon_{ijq}\) are independent \(N(0,\sigma^{2})\) random variables. To test various hypotheses, it is useful to express \(\theta_{ij}\) as

\[\theta_{ij}=\mu+\delta_{i}+\gamma_{j}+\eta_{ij}\qquad(1\leq i\leq I,\,1\leq j \leq J), \tag{5.152}\]

where with \(\overline{\theta}_{..}=\frac{1}{IJ}\sum_{i=1}^{I}\sum_{j=1}^{J}\theta_{ij}\), \(\overline{\theta}_{i..}=\frac{1}{J}\sum_{j=1}^{J}\theta_{ij}\), \(\overline{\theta}_{.j}=\frac{1}{I}\sum_{i=1}^{I}\theta_{ij}\),

\[\mu=\overline{\theta}_{..},\quad\delta_{i}=\overline{\theta}_{i..}-\overline{ \theta}_{..},\quad\gamma_{j}=\overline{\theta}_{..j}-\overline{\theta}_{..}, \quad\eta_{ij}=\theta_{ij}-\overline{\theta}_{i..}-\overline{\theta}_{.j}+ \overline{\theta}_{..}. \tag{5.153}\]

The quantities \(\delta_{i}\), \(\gamma_{j}\) are called the _main effects_ (of variety \(i\) and fertilizer \(j\), respectively), and \(\eta_{ij}\) are the _interactions_. The presence of non-zero interactions indicate that some varieties yield more in the presence of certain fertilizers than with others even after the main effects are accounted for. In analogy with continuous variables \(x\), \(y\) (in place of \(i\), \(j\)), the presence of interactions indicates a _nonlinear dependence_ of the mean yield \(\theta\) on the two variables--variety and fertilizer. Note that, in the parametrization (5.152), the following restrictions hold:

\[\mu\in\mathbb{R},\quad\sum_{i=1}^{I}\delta_{i}=0,\qquad\sum_{j=1} ^{J}\gamma_{j}=0,\qquad\sum_{j=1}^{J}\eta_{ij}=0\qquad\forall\ i,\] \[\sum_{i=1}^{I}\eta_{ij}=0\qquad\forall\ j. \tag{5.154}\]

We consider several tests of interest.

1. _Test of Equality of all \(\theta_{ij}\):_\(H_{0}:\theta_{ij}=\mu\)\(\forall\)\(i,j\). In this case, under \(H_{0}\), the minimum value of \(\sum_{i,j,q}(X_{ijq}-\mu)^{2}\) is attained by setting \(\mu=\overline{X}_{...}=\frac{1}{n}\sum_{i,j,q}X_{ijq}\), where \(n=SIJ\). Also write \(\overline{X}_{i..}=\frac{1}{SJ}\sum_{j,q}X_{ijq}\), \(\overline{X}.j.=\frac{1}{SI}\sum_{i,q}X_{ijq}\). Under the general model (5.151) (or, (5.152)), the minimum value of \(\sum_{i,j,q}(X_{ijq}-\theta_{ij})^{2}\) is attained by taking \(\theta_{ij}=\hat{\theta}_{ij}=\overline{X}_{ij.}\equiv\frac{1}{S}\sum_{q}X_{ ijq}\). Hence, with \(k=IJ\) and \(r=IJ-1\), the test is to \[\text{Reject }H_{0}\text{ iff }\quad\frac{\sum_{i=1}^{I}\sum_{j=1}^{J}( \overline{X}_{ij..}-\overline{X}_{...})^{2}/(IJ-1)}{\sum_{i,j,q}(X_{ijq}- \overline{X}_{ij.})^{2}/(n-IJ)}>F_{1-\alpha}(IJ-1,n-IJ).\] (5.155) Note that this is the same test as the \(k\)-sample test of the preceding example, with \(k=IJ\).
2. _Test of Absence of Variety Main Effect:_\(H_{0}:\delta_{i}=0\)\(\forall\)\(i\). For this, and for the cases (c), (d) below, it is convenient to express \(\sum_{i,j,q}(X_{ijq}-\theta_{ij})^{2}\) as \[\|\mathbf{X}-\mathbf{m}\|^{2} =\sum_{i,j,q}(X_{ijq}-\mu-\delta_{i}-\gamma_{j}-\eta_{ij})^{2}\] \[=\sum_{i,j,q}(X_{ijq}-\overline{X}_{ij.})^{2}+\sum_{i,j,q}( \overline{X}_{ij.}-\overline{X}_{i..}-\overline{X}_{.j.}+\overline{X}_{...}- \eta_{ij})^{2}\] \[\quad+\sum_{i,j,q}(\overline{X}_{i..}-\overline{X}_{...}-\delta_{ i})^{2}+\sum_{i,j,q}(\overline{X}_{.j.}-\overline{X}_{...}-\gamma_{j})^{2}+\sum_{i,j, q}(\overline{X}_{...}-\mu)^{2}.\]

That (5.156) is an identity follows from the representation

\[X_{ijq}-\mu-\delta_{i}-\gamma_{j}-\eta_{ij} =(X_{ijq}-\overline{X}_{ij.})+(\overline{X}_{ij.}-\overline{X}_{ i..}-\overline{X}_{.j.}+\overline{X}_{...}-\eta_{ij})\] \[\quad+(\overline{X}_{i..}-\overline{X}_{...}-\delta_{i})+( \overline{X}_{.j.}-\overline{X}_{...}-\gamma_{j})+(\overline{X}_{...}-\mu), \tag{5.157}\]

noting that the sum (over \(i,j,q\)) of products of any two terms among the five terms on the right vanishes. Now the minimum of the sum of squares in (5.156), subject to \(\delta_{i}=0\)\(\forall\)\(i\), is attained by setting \(\eta_{ij}=\overline{X}_{ij.}-\overline{X}_{i..}-\overline{X}_{.j.}+\overline{ X}_{...}\), \(\gamma_{j}=\overline{X}_{.j.}-\overline{X}_{...}\), \(\mu=\overline{X}_{...}\). Thus the minimum sum of squares under \(H_{0}\) is attained by setting

\[\theta_{ij}=\hat{\hat{\theta}}_{ij} =\overline{X}_{...}+\overline{X}_{.j.}-\overline{X}_{...}+ \overline{X}_{ij.}-\overline{X}_{i..}-\overline{X}_{.j.}+\overline{X}_{...}\] \[=\overline{X}_{ij.}-\overline{X}_{i..}+\overline{X}_{...}\qquad( 1\leq i\leq I,\,1\leq j\leq J).\]

Hence

\[\|\hat{\boldsymbol{m}}-\hat{\hat{\boldsymbol{m}}}\|^{2}=\sum_{i,j,q}(\hat{ \theta}_{ij}-\hat{\hat{\theta}}_{ij})^{2}=SJ\sum_{i=1}^{I}(\overline{X}_{i..}- \overline{X}_{...})^{2}.\]

Thus the test is:

\[\text{Reject $H_{0}$ iff}\quad\frac{\sum_{i=1}^{I}SJ(\overline{X}_{i..}- \overline{X}_{...})^{2}/(I-1)}{\sum_{i,j,q}(X_{ijq}-\overline{X}_{ij.})^{2}/( n-IJ)}>F_{1-\alpha}(I-1,n-IJ). \tag{5.158}\]
* The test for \(H_{0}:\gamma_{j}=0\)\(\forall\)\(j\) is entirely analogous to the case (b): \[\text{Reject $H_{0}$ iff}\quad\frac{\sum_{j=1}^{J}SI(\overline{X}_{.j.}- \overline{X}_{...})^{2}/(J-1)}{\sum_{i,j,q}(X_{ijq}-\overline{X}_{ij.})^{2}/( n-IJ)}>F_{1-\alpha}(J-1,n-IJ).\] (5.159)
* Finally, we consider the test for the _absence of interactions:_\(H_{0}:\eta_{ij}=0\)\(\forall\)\(i,j\). The minimum value of the sum of squares (5.156) is attained under \(H_{0}\) by setting \(\delta_{i}=\overline{X}_{i..}-\overline{X}_{...}\), \(\gamma_{j}=\overline{X}_{.j..}-\overline{X}_{...}\), \(\mu=\overline{X}_{...}\). That is, \(\hat{\hat{\theta}}_{ij}=\overline{X}_{...}+\overline{X}_{i..}-\overline{X}_{...}+\overline{X}_{.j.}-\overline{X}_{...}=\overline{X}_{i..}+\overline{X}_{.j.}-\overline{X}_{...}\). Hence \[\|\hat{m}-\hat{\hat{m}}\|^{2}=S\sum_{i,j}(\overline{X}_{ij.}-\overline{X}_{i.. }-\overline{X}_{.j.}+\overline{X}_{...})^{2},\] so that the test is: \[\text{Reject $H_{0}$ iff}\quad\frac{S\sum_{i,j}(\overline{X}_{ij.}- \overline{X}_{i..}-\overline{X}_{.j.}+\overline{X}_{...})^{2}/(I-1)(J-1)}{\sum_{i,j,q}(X_{ijq}-\overline{X}_{ij.})^{2}/(n-IJ)}\.\] Note that there are \((I-1)(J-1)\) linearly independent functions of \(\boldsymbol{\theta}=(\theta_{ij}:1\leq i\leq I,\,1\leq j\leq J)\) among \(\{\eta_{ij},\,1\leq i\leq I,\,1\leq j\leq J\}\).

_Remark 5.12_.: If in the two-way layout above \(s=1\), then the denominators of the tests (a)-(d) all vanish. This is due to the fact that in this case the least squares estimators \(\hat{\theta}_{ij}=X_{ij}\) take up all the observations and it is not possible to estimate \(\sigma^{2}\). If one assumes \(\eta_{ij}=0\ \forall\ i,j\), then one can still find UMP invariant tests for (a)-(c) (Exercise 5.17).

_Example 5.20 (UMPU Invariant Tests for Regression)._ Consider the multiple regression model

\[X_{i}=\alpha+\sum_{j=1}^{p}\beta_{j}z_{ji}+\varepsilon_{i}\quad(i=1,\ldots,n), \ p+1<n, \tag{5.160}\]

where \(\beta_{1},\ldots,\beta_{p}\) are unknown regression coefficients, \(\beta_{i}\in\mathbb{R}\ \forall\,i\), and \(Z=((z_{ji}))_{1\leq j\leq p,1\leq i\leq n}\) a known design matrix of full rank \(p\). As usual, assume \(\varepsilon_{i}\)'s are i.i.d. \(N(0,\sigma^{2})\), \(\sigma^{2}>0\) is unknown. We wish to explore the effects of the \(p\)_regressors_ or _independent variables, \(z_{j}\)_, \(1\leq j\leq p\), on the _predictor_ or _dependent variable_\(X\), assuming that the relationship is linear.

1. **Test \(\boldsymbol{H_{0}:\beta_{1}=\cdots=\beta_{p}=0}\),** i.e., the regressors have no effect on \(X\). To simplify the computation it is best to rewrite the model (5.160) as the equivalent model \[X_{i}=\delta+\sum_{j=1}^{p}\beta_{j}(z_{ji}-\overline{z}_{j.})+ \varepsilon_{i},\] (5.161) \[\delta:=\alpha+\sum_{j=1}^{p}\beta_{j}\overline{z}_{j.},\overline{ z}_{j.}=\frac{1}{n}\sum_{i=1}^{n}z_{ji}\quad(1\leq j\leq p).\] Here \(\boldsymbol{k=p+1}\), and to minimize \[\|\mathbf{X}-E\mathbf{X}\|^{2}=\sum_{i=1}^{n}(X_{i}-\delta-\sum_{j=1}^{p}\beta _{j}(z_{ji}-\overline{z}_{j.}))^{2},\] differentiate with respect to these parameters to obtain \[\sum_{i=1}^{n}(X_{i}-\delta-\sum_{j=1}^{p}\beta_{j}(z_{ji}- \overline{z}_{j.}))=0,\] \[\sum_{i=1}^{n}(z_{j^{\prime}i}-\overline{z}_{j^{\prime}.})\left( \sum_{i=1}^{n}X_{i}-\delta-\sum_{j=1}^{p}\beta_{j}(z_{ji}-\overline{z}_{j.}) \right)=0,\ 1\leq j^{\prime}\leq p.\] (5.162) The first equation yields the solution \[\delta=\hat{\delta}=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i},\] (5.163) while the remaining \(p\) equations yield \[S_{xj} \equiv \sum_{i=1}^{n}(z_{j^{\prime}i}-\overline{z}_{j^{\prime}})(X_{i}- \overline{X})=\sum_{j=1}^{p}S_{j^{\prime}j}\beta_{j^{\prime}},\] \[S_{j^{\prime}j} := \sum_{i=1}^{n}(z_{j^{\prime}i}-\overline{z}_{j^{\prime}}.)(z_{ji} -\overline{z}_{j}.),\ \ 1\leq j^{\prime}\leq p. \tag{5.164}\]

Hence, in matrix notation, the solution \(\hat{\boldsymbol{\beta}}\) of the test \(p\) equations may be expressed as

\[\hat{\boldsymbol{\beta}} = \Big{(}\hat{\beta}_{1},\ldots,\hat{\beta}_{p}\Big{)}^{\prime}=S^{ -1}\mathbf{S}_{x},\] \[S=((S_{j^{\prime}j})),\quad\mathbf{S}_{x}=(S_{x1},\ldots,S_{xp}) ^{\prime}. \tag{5.165}\]

Together (5.161) and (5.165) provide the solution to \(\alpha\) as

\[\hat{\alpha}=\hat{\delta}-\sum_{j=1}^{p}\hat{\beta}_{j}\overline{z}_{j.}= \overline{X}-\sum_{j=1}^{p}\hat{\beta}_{j}\overline{Z}_{j.}. \tag{5.166}\]

Hence, writing \(S_{xx}=\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\),

\[\min_{E\mathbf{X}}\|\mathbf{X}-E\mathbf{X}\|^{2} = \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}+\sum_{j^{\prime},j=1}^{p} \hat{\beta}_{j^{\prime}}\hat{\beta}_{j}S_{j^{\prime}j}-2\sum_{j=1}^{p}\hat{ \beta}_{j}S_{xj} \tag{5.167}\] \[= \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}+\hat{\beta}^{\prime}S\hat {\beta}-2\hat{\beta}^{\prime}\mathbf{S}_{x}\] \[= \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}+\mathbf{S}_{x}^{\prime}S^{ -1}\mathbf{S}_{x}-2\mathbf{S}_{x}^{\prime}S^{-1}\mathbf{S}_{x}\] \[= S_{xx}-\mathbf{S}_{x}S^{-1}\mathbf{S}_{x}\] \[= \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}-\sum_{j,j^{\prime}=1}^{p} S_{jj^{\prime}}\hat{\beta}_{j}\hat{\beta}_{j^{\prime}}.\]

Next, under \(H_{0}\), \(EX_{x}=\delta=\alpha\), and \(\|\mathbf{X}-E\mathbf{X}\|^{2}=\sum_{i=1}^{n}(X_{i}-\delta)^{2}\) is minimized by \(\delta=\hat{\delta}=\overline{X}\), so that

\[\min_{E\mathbf{X}}\mbox{ under }H_{0}\left\|\mathbf{X}-E\mathbf{X}\right\|^{2}= \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=S_{xx}. \tag{5.168}\]

Hence the UMPU invariant test for (a) is to

\[\mbox{ reject }H_{0}\mbox{ iff }\ \frac{\mathbf{S}_{x}^{\prime}S^{-1}\mathbf{S}_{x}/ p}{[S_{xx}-\mathbf{S}_{x}^{\prime}S^{-1}\mathbf{S}_{x}]/(n-p-1)}\rightarrow\ F_{1-\alpha}(p,n-p-1). \tag{5.169}\]

It may be noted that \(\mathbf{S}_{x}^{\prime}S^{-1}\mathbf{S}_{x}=\sum_{j,j^{\prime}=1}^{p}S^{jj^{ \prime}}\hat{\beta}_{j}\hat{\beta}_{j^{\prime}}\), where \(((S^{jj^{\prime}}))=S^{-1}\).

(b) Test \(\boldsymbol{H_{0}:\beta_{q+1}=\cdots=\beta_{p}=0}\), where \(1\leq q\leq p-1\). This is to test if some of the independent variables \(Z_{j}\) have no effect on the predictor (or dependent) variable \(X\). Proceeding as in (5.160)-(5.167), but with \(p\) replaced by \(q\), one has the least squares estimate \(\hat{\hat{\hat{\beta}}}_{1}^{q}=(\hat{\hat{\beta}}_{1},\ldots,\hat{\hat{\beta} }_{q})^{\prime}\) given by

\[\hat{\hat{\beta}}_{1}^{q}=(S_{1}^{q})^{-1}\mathbf{S}_{x,1}^{q},\quad S_{1}^{q} :=((S_{j^{\prime}j}))_{1\leq j,j^{\prime}\leq q},\ \mathbf{S}_{x,1}^{q}:=(S_{x,1},\ldots,S_{x,q})^{\prime}. \tag{5.170}\]Also, \(\hat{\hat{\delta}}=\overline{X}\), and

\[\hat{\hat{\alpha}}=\overline{X}-\sum_{j=1}^{q}\hat{\hat{\beta}}_{j}\overline{z}_{ j}. \tag{5.171}\]

Then

\[\min_{E\mathbf{X}\ \text{under}\ H_{0}}\|\mathbf{X}-E\mathbf{X}\|^{2}=S_{xx}-( \mathbf{S}_{x,1}^{q})^{\prime}(S_{1}^{q})^{-1}\mathbf{S}_{x,1}^{q}. \tag{5.172}\]

The \(F\) test is now to

\[\text{reject}\ H_{0}\ \text{iff}\ \frac{[\mathbf{S}_{x}^{\prime}S^{-1}\mathbf{S}_{ x}-(\mathbf{S}_{x,1}^{q})^{\prime}(S_{1}^{q})^{-1}\mathbf{S}_{x,1}^{q}]/(p-q)}{[S_{ xx}-\mathbf{S}_{x}^{\prime}S^{-1}\mathbf{S}_{x}]/(n-p-1)}>F_{1-\alpha}(p-q,n-p-1).\] (c)

For the test \(\boldsymbol{H_{0}:\alpha=0}\), the sum of squares under \(H_{0}\) to be minimized is

\[\|\mathbf{X}-E\mathbf{X}\|^{2}=\sum_{i=1}^{n}(X_{i}-\sum_{j=1}^{p}\beta_{j}z_{ ji})^{2}, \tag{5.173}\]

and the minimizer is \(\hat{\hat{\beta}}=(\hat{\hat{\beta}}_{1},\ldots,\hat{\hat{\beta}}_{p})^{\prime}\) obtained by setting zero to the derivatives of (5.173) with respect to \(\beta_{j}\ (1\leq j\leq p)\). This gives

\[\hat{\hat{\beta}}=S_{0}^{-1}\mathbf{S}_{0x},\quad S_{0}:=((S_{0jj^{\prime}}))_ {1\leq j,j^{\prime}\leq p},\quad\mathbf{S}_{0x}:=(X_{0x_{1}},\ldots,S_{0x_{p} })^{\prime}, \tag{5.174}\]

where

\[S_{0jj^{\prime}}=\sum_{i=1}^{n}Z_{ji}Z_{j^{\prime}i},\quad S_{0xj}=\sum_{i=1}^ {n}X_{i}z_{ji}\ \ (1\leq j\leq p). \tag{5.175}\]

Therefore, writing \(S_{0xx}=\sum_{i=1}^{n}X_{i}^{2}\), \(S_{0}^{-1}=((S_{0}^{jj^{\prime}}))\), one has

\[\min_{E\mathbf{X}\ \text{under}\ H_{0}}\|\mathbf{X}-E\mathbf{X}\|^{2}=S_{Oxx}- \mathbf{S}_{Ox}^{\prime}S_{0}^{-1}\mathbf{S}_{Ox}=\sum_{i=1}^{n}X_{i}^{2}- \sum_{j,j^{\prime}=1}^{p}S_{O}^{jj^{\prime}}S_{Oxj}S_{Oxj^{\prime}}. \tag{5.176}\]

This leads to the UMPU invariant test for \(H_{0}\) which would

\[\text{reject}\ H_{0}\ \text{iff}\ \frac{n\overline{x}^{2}+\mathbf{S}_{x}^{\prime }S^{-1}\mathbf{S}_{x}-S_{Ox}^{\prime}S_{O}^{-1}\mathbf{S}_{Ox}}{[S_{xx}- \mathbf{S}_{x}^{\prime}S^{-1}\mathbf{S}_{x}]/(n-p-1)}>F_{1-\alpha}(1,n-p-1). \tag{5.177}\]

Note that one may similarly test (a)\({}_{\mathbf{e}}\ H_{0}:\beta_{j}=c_{j}\) for arbitrarily given constants \(c_{j}\ (1\leq j\leq p)\), (b)\({}_{\mathbf{d}}\ H_{0}:\beta_{j}=d_{j}\) for given \(d_{j}\ (1\leq j\leq q)\), and (c)\({}_{a_{0}}\ H_{0}:\alpha=a_{0}\) for given \(a_{0}\).

Remark 5.13 (ANOVA--Analysis of Variance): In Examples 5.18, 5.19, a convenient and intuitively meaningful tabulation of calculations of the various tests is by the so-called ANOVA-table, ANOVA being the abbreviation for analysis of variance. The idea is to consider the _total variability_ in the data, namely, the _total sum of squares_ (Total \(SS\)) of all the observed \(X\)-values around the _grand total_. This Total \(SS\) into sums of squares of orthogonal components. For example, for Example 5.18, the ANOVA Table is 

### Notes and References

Here \(DF\) is _degrees of freedom_, i.e., the number of linearly independent contrasts or comparisons which can be attributed to the particular source of variation; \(SS\) is the sum of squares and \(MS\) stands for mean squares. The _treatment sum of squares_ SSI equals \(\sum_{i=1}^{k}n_{i}(\overline{X}_{i.}\!-\!\overline{X}_{..})^{2}\), the Total sum of squares is \(\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(X_{ij}\!-\!\overline{X}_{..})^{2}\), and the error sum of squares is \(SSE=\) Total \(SS-SST=\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(X_{ij}\!-\!\overline{X}_{i.})^{2}\).

For the two-way layout of Example 5.19, the ANOVA table is

\begin{tabular}{l|l|l|l|l} \hline Source & DF & SS & MS & F \\ \hline Variety & \(I-1\) & \(SSV\) & \(SSV/(I-1)\) & \(MSV/MSE\) \\ \hline Fertilizer & \(J-I\) & \(SSF\) & \(SSF/(J-1)\) & \(MSF/MSE\) \\ \hline Interaction & \((I-1)\times(J-1)\) & \(SSI\) & \(SSI(I-1)(J-1)\) & \(MSI/MSE\) \\ \hline Error & \(n-IJ\) & \(SSE\) & \(SSE/(n-IJ)\) & \\ \hline Total & \(IFS-1=n-1\) & Total & & \\ \hline \end{tabular}

Here \(SSV=JS\sum_{i=1}^{I}(\overline{X}_{i..}-\overline{X}_{...})^{2}\), \(SSF=IS\sum_{j=1}^{J}(\overline{X}_{.j.}-\overline{X}_{...})^{2}\), \(SSI=S\sum_{i=1}^{I}\sum_{j=1}^{J}(\overline{X}_{ij.}-\overline{X}_{i..}-\overline {X}_{.j.}+\overline{X}_{...})^{2}\), Total \(SS=\sum_{i,j,q}(X_{ijq}-\overline{X}_{...})^{2}\), and \(SSE=\) Total \(SS-SSV-SSF-SSI\).

### Notes and References

The presentation in this chapter follows (Ferguson, 1967, Chap. 5), which in turn is strongly influenced by Lehmann (1959). The theory in Sects. 5.1-5.7 is mostly due to Neyman and Pearson (1933, 1936, 1938). Theorem 5.5 is due to Basu (1955). Pitman (1939) introduced the notion of invariance in problems of estimation of location and scale, and this was generalized in broader estimation problems by Kiefer (1957). As mentioned by Lehmann (1959, p. 261), the general theory of invariant tests is due to Hunt and Stein (1946). Analysis of variance introduced and presented in Fisher (1925, 1935) has had a profound impact on parametric statistics especially in the study of so-called linear models and multivariate analysis. Rao (1952), Scheffe (1959) and Anderson (1958) are early standard references on these topics. Fisher's efforts at construction of designs that allow proper and/or optimal statistical analysis of agricultural experiments in the 1930s and 1940s led to the creation of a theory by him and other statisticians with wide applications to the construction of error correcting codes in information theory, such as the famous _Bose-Chaudhuri-Hocquenghem code_ (Bose and Ray-Chaudhuri (1960), Hocquenghem (1959)). Many of its applications to combinatorial mathematics included the settling in 1959 in the negative by R.C. Bose, S.S. Shrikhande and, independently, by E.T. Parker (Bose et al. (1960)), of the famous conjecture of Euler on the construction of certain _Latin squares_.

### Exercises for Chap. 5

**Ex. 5.1**.: Using the theory of UMP tests for \(H_{0}:\pi\leq\pi_{0}\) against \(H_{1}:\pi>\pi_{0}\) in one-parameter exponential families (See Sect. 5.3, and Remarks 5.2, 5.3), show that the test (5.26) is UMP of its size for testing \(H_{0}:\pi\geq\pi_{0}\) against \(H_{1}:\pi<\pi_{0}\).

**Ex. 5.2**.: Given \(\mathbf{X}=(X_{1},\ldots,X_{n})\) with \(X_{j}\)'s i.i.d. gamma \(\mathscr{G}(\theta,\beta)\), \(\beta>0\) known (as in Example 5.4), find the UMP test of size \(\alpha\) (\(0<a<1\)) for \(H_{0}:\theta\leq\theta_{0}\) against \(H_{1}:\theta>\theta_{0}\), where \(\theta_{0}>0\) is a given (threshold) number.

**Ex. 5.3**.: Use Remark 5.4 to find UMP tests of size \(\alpha\) for testing \(H_{0}:\theta\leq\theta_{0}\) against \(H_{1}:\theta>\theta_{0}\) in the following examples, based on i.i.d. observations \(X_{1},\ldots,X_{n}\).

1. \(X_{j}\) has the double exponential distribution with p.d.f. \(f(x\mid\theta)=(2\alpha)^{-1}\exp\{-|x-\theta|/\alpha\}\) (\(x\in\mathbb{R}\)), \(\theta\in\mathbb{R}=\Theta\) (\(\alpha>0\) known).
2. \(X_{j}\) has the shifted exponential distribution with p.d.f. \(f(x\mid\theta)=\exp\{-x(x-\theta)\}\) (\(x>\theta\)), and \(f(x\mid\theta)=0\) (\(x\leq\theta\)). Here \(\Theta=\mathbb{R}\).
3. \(X_{j}\) has the uniform distribution \(\mathscr{U}(0,\theta)\) on \((0,\theta)\), \(\theta\in(0,\infty)=\Theta\).
4. \(X_{j}\) has the uniform distribution \(\mathscr{U}(\theta,\theta+1)\) on \((\theta,\theta+1)\), \(\theta\in\Theta=\mathbb{R}\).

**Ex. 5.4**.:
1. Justify the interchange of the order of differentiation (w.r.t. \(\pi\)) and integration (w.r.t. \(\mu\)) in (5.34).
2. Show that the UMP unbiased test in Example 5.1 for \[H_{0}:\theta=\theta_{0},\quad H_{1}:\theta\neq\theta_{0}\] (5.178) is given by \[\text{Reject }H_{0}\text{ iff }\quad\left|\frac{\sum_{1}^{n}X_{j}-n\theta_{0} }{\sqrt{n}}\right|>z_{1-\frac{\alpha}{2}}\,.\] More generally, if the model is \(\mathbf{N}(\theta,\sigma_{0}^{2})\), _with \(\sigma_{0}^{2}>0\) known,_ then the UMP unbiased test for (5.178) is given by \[\text{Reject }H_{0}\text{ iff }\quad\left|\frac{\sqrt{n}(\overline{X}-\theta_{0 })}{\sigma_{0}}\right|>z_{1-\frac{\alpha}{2}}\,.\]

[Hint: Take \(T=\frac{\sum_{1}^{n}X_{j}-n\theta_{0}}{\sigma_{0}}\), and apply (5.42), (5.43).]

**Ex. 5.5**.: In the Normal example \(\mathbf{N}(\mu_{0},\theta)\), show that the UMP unbiased test of size \(\alpha\) for \(H_{0}:\theta=\sigma_{0}^{2}\), \(H_{1}:\theta\neq\sigma_{0}^{2}\), is given by

\[\text{Reject }H_{0}\text{ iff }\quad\sum_{j=1}^{n}\frac{(X_{j}-\mu_{0})^{2}}{2 \sigma_{0}^{2}}<t_{1}\quad\text{or}\quad>t_{2},\]

where \(t_{1}\) and \(t_{2}\) are determined by the first condition in (5.47) and (5.48) (both with \(\beta=\frac{1}{2}\)). [Hint: Consider observations \((X_{j}-\mu_{0})/\sigma_{0}\) (\(1\leq j\leq n\)), and apply Example 5.8.]

**Ex. 5.6**.: Let \(X_{j}\) (\(1\leq j\leq n\)) be i.i.d. with common density (w.r.t. Lebesgue measure on \((0,1)\))

\[f(x\mid\theta)=\theta_{x}\theta^{-1}\qquad 0<x<1,\quad\theta\in\Theta=(0,\infty).\]Find the UMP unbiased test of size \(\alpha\), \(0<\alpha<1\), for testing \(H_{0}:\theta=1\) against \(H_{1}:\theta\neq 1\).

**Ex. 5.7.** In Example 5.6, show that

(a): the UMP unbiased test of size \(\alpha\) for \(H_{0}:\mu\leq\mu_{0}\), \(H_{1}:\mu>\mu_{0}\), is given by:

Reject \(H_{0}\) iff \(\frac{\sqrt{n(X}-\mu_{0})}{s}>t_{1-\alpha}(n-1)\),

(b): the UMP unbiased test of size \(\alpha\) for \(H_{0}:\mu\geq\mu_{0}\), \(H_{1}:\mu<\mu_{0}\), is given by:

Reject \(H_{0}\) iff \(\frac{\sqrt{n(X}-\mu_{0})}{s}<-t_{1-\alpha}(n-1)\),

(c): the UMP unbiased test of size \(\alpha\) for \(H_{0}:\mu=\mu_{0}\), \(H_{1}:\mu\neq\mu_{0}\), is given by:

Reject \(H_{0}\) if \(\left|\frac{\sqrt{n(X}-\mu_{0})}{s}\right|>t_{1-\frac{\alpha}{2}}(n-1)\).

**Ex. 5.8.**:
* Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. observations from \(N(\mu_{0},\sigma^{2})\), with \(\mu_{0}\) known. Find the UMP test of size \(\alpha\) for testing \(H_{0}:\sigma^{2}\leq\sigma_{0}^{2}\) against \(H_{1}:\sigma^{2}>\sigma_{0}^{2}\), where \(\sigma_{0}^{2}>0\) is given. In Example 5.7, show that
* the UMP unbiased test of size \(\alpha\) for \(H_{0}:\sigma^{2}\geq\sigma_{0}^{2}\), \(H_{1}:\sigma^{2}<\sigma_{0}^{2}\) is given by: Reject \(H_{0}\) iff \(\sum_{1}^{n}(X_{j}-\overline{X})^{2}/\sigma_{0}^{2}<\chi_{\alpha}^{2}\), the \(\alpha\)th quantile of the chi-square distribution with \(n-1\) d.f.
* the UMP unbiased test of size \(\alpha\) for \(H_{0}:\sigma^{2}=\sigma_{0}^{2}\), \(H_{1}:\sigma^{2}\neq\sigma_{0}^{2}\) is given by: Reject \(H_{0}\) iff \(\sum_{1}^{n}(X_{j}-\overline{X})^{2}<c_{1}\sigma_{0}^{2}\) or \(>c_{2}\sigma_{0}^{2}\) where \(0<c_{1}<c_{2}\) are determined by the equations (see Sect. 5.4, Examples 5.4, 5.5) \[\int_{C_{1}}^{C_{2}}g(t)dt=1-\alpha,\quad\left(\frac{c_{2}}{c_{1}}\right)^{(n- 1)/2}=e^{(c_{2}-c_{1})/2},\] (5.179) where \(g(t)\) is the density of the chi-square distribution with \(n-1\) d.f.

**Ex. 5.9.** Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},Y_{2},\ldots,Y_{n}\) be independent random samples from exponential distributions with means \(\theta_{1}\), \(\theta_{2}\) respectively (i.e., from \(\mathscr{G}(\theta_{1},1)\) and \(\mathscr{G}(\theta_{2},1)\)). Find the UMP unbiased test of size \(\alpha\) for \(H_{0}:\theta_{1}\leq\theta_{2}\) against \(H_{1}:\theta_{1}>\theta_{2}\).

**Ex. 5.10.** Let \(U_{1}\) and \(U_{2}\) be independent gamma random variables \(\mathscr{G}(\theta,m)\) and \(\mathscr{G}(\theta,n)\). Prove that \(Z_{1}\equiv U_{1}/(U_{1}+U_{2})\) and \(Z_{2}\equiv U_{1}+U_{2}\) are independent random variables with \(Z_{1}\) having the beta distribution \(\mathrm{Beta}(m,n)\) and \(Z_{2}\) have the gamma distribution \(\mathscr{G}(\theta,m+n)\).

**Ex. 5.11.**:
* Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},Y_{2},\ldots,Y_{n}\) be independent random samples from \(\mathbf{N}(\mu_{1},\sigma_{1}^{2})\) and \(\mathbf{N}(\mu_{2},\sigma_{2}^{2})\), respectively, where \(\sigma_{i}^{2}>0\) (\(i=1,2\)) are known, and \((\mu_{1},\mu_{2})\in\mathbb{R}^{2}\). Find a UMP unbiased test of size \(\alpha\in(0,1)\) for (5.78).
* In the context of Example 5.8, find the UMP unbiased test of size \(\alpha\) for (i) \(H_{0}:\mu_{1}\geq\mu_{2}\), \(H_{1}:\mu_{1}<\mu_{2}\), and for (ii) \(H_{0}:\mu_{1}=\mu_{2}\), \(H_{1}:\mu_{1}\neq\mu_{2}\).
* Extend (b) to the case (a).

**Ex. 5.12.** In Example 5.10, find the UMP unbiased test of size \(\alpha\in(0,1)\) for

\[H_{0}:\frac{\sigma_{2}^{2}}{\sigma_{1}^{2}}\leq\gamma_{0},\quad H_{1}:\frac{ \sigma_{2}^{2}}{\sigma_{1}^{2}}>\gamma_{0},\]

for a given \(\gamma_{0}>0\). [Hint: change \(Y_{j}\) to \(Y_{j}/\gamma_{0}\), \(1\leq j\leq n\), in Example 5.10.]

**Ex. 5.13**.: Let \(\mathbf{X}=(X_{1},\ldots,X_{20})\), where \(X_{j}\), \(1\leq j\leq 20\), are i.i.d. Bernoulli \(\mathscr{B}(\theta)\), \(\theta\in\Theta=(0,1)\). Construct the UMPU test of size \(\alpha\) for \(H_{0}:\theta=0.5\), against \(H_{1}:\theta\neq 0.5\). [Hint: By symmetry, the test should be of the form: \(\varphi(\mathbf{x})=1\) if \(\sum_{1}^{20}x_{j}<10-r\) or \(>10+r\), \(\varphi(\mathbf{x})=\gamma\) if \(\sum_{1}^{20}x_{j}=10-r\) or \(10+r\), and \(\varphi(\mathbf{x})=0\) if \(10-r<\sum_{1}^{20}x_{j}<10+r\). Here \(r\) is a positive integer, and \(0\leq\gamma<1\).]

**Ex. 5.14** (**Match Pair Test**).: Assume that \((X_{i},Y_{i})\), \(1\leq i\leq n\), are i.i.d. Normal \(N((\mu_{x},\mu_{y})^{t},\Sigma)\), where \(\Sigma\) is positive definite and all the parameters are unknown.

1. Prove that under the assumption \(\sigma_{x}^{2}=\sigma_{y}\), the match pair test for \(H_{0}:\mu_{x}=\mu_{y}\), \(H_{1}:\mu_{x}>\mu_{y}\) described in Example 5.11 is UMPU of size \(\alpha\).
2. Prove the assertion that the match pair design is more efficient than the independent samples design, at least for sufficiently large \(n\), provided \(\rho>0\).

**Ex. 5.15**.:
1. Show that the map \(\overline{g}:\Theta\to\Theta\) defined in Sect. 5.8 is one-to-one.
2. 1. Show that the two-sample problem in Example 5.8 is invariant under the group \(\mathscr{G}\) of all _translations_\(g_{a}:\mathbb{R}^{m}\times\mathbb{R}^{n}\longrightarrow\mathbb{R}^{m}\times \mathbb{R}^{n}\), given by \(g_{a}((x_{1}\),..., \(x_{m}\), \(y_{1}\),..., \(y_{n}))=(x_{1}+a,\ldots,x_{m}+a,y_{1}+a,\ldots,y_{n}+a)\), \(a\in\mathbb{R}\). 2. Also, show that the UMPU test is also an invariant test.
3. Show that the two-sample problem in Example 5.9 is invariant under the group \(\mathscr{G}\) of transformations \(g_{a,c}:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}^{m}\times\mathbb{R}^{n}\) given by \(g_{a,c}((x_{1},\ldots,x_{m},y_{1},\ldots,y_{n}))=\left(\frac{x_{1}+a}{c},\ldots,\frac{x_{m}+a}{c},\frac{y_{1}+a}{c},\ldots,\frac{y_{n}+a}{c}\right)\), \(a\in\mathbb{R}\), \(c>0\). Show also that the UMPU test is invariant.

**Ex. 5.16**.: Let \(\mathbf{X}\) be \(k\)-dimensional Normal \(N(\boldsymbol{\mu},\not{\Sigma})\) with \(\not{\Sigma}\) a known positive definite matrix.

1. Find the UMA invariant confidence region for \(\boldsymbol{\mu}\) using the UMP invariant test in Example 5.15.
2. Find a UMA invariant confidence region for \(\boldsymbol{\mu}\) (under the group as in (a)) based on \(n\) i.i.d. observations \(\mathbf{X}_{1},\ldots,\mathbf{X}_{n}\) with common distribution \(N(\boldsymbol{\mu},\not{\Sigma})\)).

**Ex. 5.17** (**Two-way Layout with One Observation Per Cell**).: In Example 5.19, let \(S=1\), and assume \(\eta_{ij}=0\)\(\forall\)\(i,j\) (in addition to the other assumptions). Find the UMP invariant tests (a)-(c).

**Ex. 5.18**.: In Example 5.19, let the number of replications, say \(S_{ij}\), vary for different pairs \((i,j)\), with \(S_{ij}\geq 2\)\(\forall\)\((i,j)\). Carry out the tests (a)-(d) in this case.

## References

* Anderson (1958) Anderson, T. W. (1958). _An introduction to multivariate analysis_ (1st ed.). New York: Wiley.
* Basu (1955) Basu, D. (1955). On statistics independent of a complete sufficient statistic. _Sankhya, 15_, 377-380.
* Basu (1959) Basu, D. (1959). The family of ancillary statistics. _Sankhya, 21_, 247-256.
* Bose & Ray-Chaudhuri (1960) Bose, R. C., & Ray-Chaudhuri, D. K. (1960). On a class of error-correcting binary codes. _Information and Control, 3_, 68-79.

Bose, R. C., Shrikhande, S. S., & Parker, E. T. (1960). Further results on the construction of mutually orthogonal Latin squares and the falsity of Euler's conjecture. _Canadian Journal of Mathematics, 12_, 189-203.
* Ferguson (1967) Ferguson, T. (1967). _Mathematical statistics: A decision theoretic approach_. Boston: Academic.
* Fisher (1925) Fisher, R. A. (1925). _Statistical methods for research workers_ (1st ed.). Edinburgh: Oliver and Boyd.
* Fisher (1935) Fisher, R. A. (1935). _The design of experiments_ (1st ed.). Edinburgh: Oliver and Boyd.
* Hocquenghem (1959) Hocquenghem, A. (1959). Codes correcteurs d'erreurs. _Chiffres (in French) (Paris), 2_, 147-156.
* Hunt & Stein (1946) Hunt, G., & Stein, C. (1946). Most stringent tests of statistical hypotheses. Unpublished manuscript.
* Kiefer (1957) Kiefer, J. (1957). Invariance, minimax sequential estimation, and continuous time processes. _Annals of Mathematical Statistics, 28_, 573-601.
* Lehmann (1959) Lehmann, E. L. (1959). _Testing statistical hypotheses_ (1st ed.). New York: Wiley.
* Neyman & Pearson (1933) Neyman, J., & Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. _Philosophical Transactions of the Royal Society of London. Series A, 231_, 289-337.
* Neyman & Pearson (1936) Neyman, J., & Pearson, E. S. (1936). Contributions to the theory of testing statistical hypotheses. Part I. Unbiased critical regions of type \(A\) and type \(A_{1}\). _Statistical Research Memoirs, 1_, 1-37.
* Neyman & Pearson (1938) Neyman, J., & Pearson, E. S. (1938). Contributions to the theory of testing statistical hypotheses. Part II. Certain theorems on unbiased critical regions of type \(A\). Part III. Unbiased tests of simple statistical hypotheses specifying the values of more than one unknown parameter. _Statistical Research Memoirs, 2_, 25-57.
* Pitman (1939) Pitman, E. J. G. (1939). The estimation of location and scale parameters of a continuous population of any given form. _Biometrika, 30_, 391-421.
* Rao (1952) Rao, C. R. (1952). _Advanced statistical methods in biometric research_. New York: Wiley.
* Scheffe (1959) Scheffe, H. (1959). _Analysis of variance_. New York: Wiley.

## Part II Mathematical Statistics: Large Sample Theory

## Chapter 6 Consistency and Asymptotic Distributions of Statistics

**Abstract** Notions of convergence for large sample theory are introduced: almost sure convergence, convergence in probability and convergence in distribution. Consistency of several classes of estimators and their asymptotic distributions are derived, including those of sample moments, quantiles and linear regression coefficients.

### 6.1 Introduction

Unlike the so-called exact sampling theory, where one needs to look for optimal estimators separately for each parametric function in every individual parametric family, in large sample theory there is a remarkable unity in the methodology for optimal estimators. Also, unlike the former, the optimality criterion is essentially uniquely and unambiguously specified and this (asymptotic) optimality is achieved under fairly general hypotheses. Indeed, one may say that the maximum likelihood estimator (MLE) is optimal in an asymptotic sense under broad assumptions. In addition, if nothing is known about the form of the distribution except perhaps that it has certain finite moments, or that it has a continuous and positive density over the effective range, one may still construct reasonably good estimators for important classes of population indices such as moments or quantiles. For example, one may use sample moments and sample quantiles as the respective estimators. The present chapter introduces the basic notions of convergence in large sample theory and develops some of its main tools. Asymptotics of sample moments and quantiles, and of semiparametric linear regression, are derived here, which allow one to construct nonparametric or semiparametric confidence regions or tests for the corresponding population parameters.

### 6.2 Almost Sure Convergence, Convergence in Probability and Consistency of Estimators

A minimal requirement of any reasonable estimator \(U_{n}:=U_{n}(X_{1},\ldots,X_{n})\) of a population parameter \(\gamma\) is that of _consistency_.

**Definition 6.1**.: A statistic \(U_{n}\) (\(n\geq 1\)) is said to be a _consistent estimator_ of a parameter \(\gamma\), if \(U_{n}\) converges to \(\gamma\) in probability: \(U_{n}\stackrel{{ P}}{{\longrightarrow}}\gamma\), i.e.,

\[P(|U_{n}-\gamma|>\varepsilon)\longrightarrow 0\quad\mbox{as $n\to\infty$, for every $\varepsilon>0$}. \tag{6.1}\]

More generally, a sequence of random variables \(Y_{n}\) is said to _converge to a random variable \(Y\) in probability_, \(Y_{n}\stackrel{{ P}}{{\longrightarrow}}Y\), if

\[P(|Y_{n}-Y|>\varepsilon)\longrightarrow 0\quad\mbox{as $n\to\infty$, for every $\varepsilon>0$}. \tag{6.2}\]

In (6.1), \(Y\) is the constant random variable \(Y=\gamma\).

A common method for proving consistency is the following.

**Proposition 6.1**.: _(a) If, for some \(r>0,E|U_{n}-\gamma|^{r}\to 0\), then \(U_{n}\) is a consistent estimator of \(\gamma\). (b) If \(U_{n}\) is an unbiased estimator of \(\gamma\) and \(\operatorname{var}(U_{n})\to 0\), then \(U_{n}\) is a consistent estimator of \(\gamma\)._

Proof.: (a) By Chebychev's Inequality (see note below), for every \(\varepsilon>0\),

\[P(|U_{n}-\gamma|\geq\varepsilon)\leq\frac{E|U_{n}-\gamma|^{r}}{\varepsilon^{r }}\longrightarrow 0\quad\mbox{as $n\to\infty$}. \tag{6.3}\]

(b) is a special case of (a) with \(r=2\), and \(\operatorname{var}(U_{n})=E|U_{n}-\gamma|^{2}\). 

_Note:_ Let \(X\) be a random variable (e.g., \(X=U_{n}-\gamma\)) such that \(E|X|^{r}<\infty\) for some \(r>0\), then writing \(E(Y:A)\) for the _expectation of \(Y\) on the set \(A\)_, i.e., \(E(Y:A)=\int_{A}YdP\),

\[E|X|^{r} = E\left[|X|^{r}:|X|<\varepsilon\right]+E\left[|X|^{r}:|X|\geq \varepsilon\right] \tag{6.4}\] \[\geq E\left[|X|^{r}:|X|\geq\varepsilon\right]\geq\varepsilon^{r}P(|X |\geq\varepsilon),\]

which gives _Chebyshev's Inequality_

\[P(|X|\geq\varepsilon)\leq\frac{E|X|^{r}}{\varepsilon^{r}}. \tag{6.5}\]

**Proposition 6.2**.: _Suppose \(U_{n}\) and \(V_{n}\) are two sequences of random variables such that \(U_{n}\stackrel{{ P}}{{\longrightarrow}}a,V_{n}\stackrel{{ P}}{{\longrightarrow}}b\). If \(g(u,v)\) is a function (of two variables) which is continuous at \((a,b)\), then \(g(U_{n},V_{n})\stackrel{{ P}}{{\longrightarrow}}g(a,b)\)._

Proof.: Fix \(\varepsilon>0\). There exists \(\delta=\delta(\varepsilon)\) such that if \(|u-a|\leq\delta\) and \(|v-b|\leq\delta\) then \(|g(u,v)-g(a,b)|\leq\varepsilon\). Now

\[P(|g(U_{n},V_{n})-g(a,b)|>\varepsilon)= \tag{6.6}\] \[= P(\{|U_{n}-a|>\delta\mbox{ or }|V_{n}-b|>\delta\}\cap\{|g(U_{n},V_{n})-g(a,b)|>\varepsilon\})\] \[+P(\{|U_{n}-a|\leq\delta\mbox{ and }|V_{n}-b|\leq\delta\}\cap\{|g(U_{n},V_{n})-g(a,b)|>\varepsilon\})\] \[\leq P(|U_{n}-a|>\delta)+P(|V_{n}-b|>\delta)\to 0.\]

Note that the set \(\{|U_{n}-a|\leq\delta,|V_{n}-b|\leq\delta\}\cap\{g(U_{n},V_{n})-g(a,b)|>\varepsilon\}\) is empty, and has therefore zero probability--a fact used for the last inequality.

**Definition 6.2**.: A statistic \(U_{n}\) (\(n\geq 1\)) is said to be a _strongly consistent estimator_ of a parameter \(\gamma\) if \(U_{n}\to\gamma\) a.s.

**Proposition 6.4**.: _If \(U_{n}\to a\) a.s. and \(V_{n}\to b\) a.s., then \(g(U_{n},V_{n})\longrightarrow g(a,b)\) a.s. for every function \(g\) of two variables which is continuous at the point \((a,b)\)._

We leave the proof of Proposition 6.4, as well as that of the a.s. convergence version of Corollary 6.1 to Exercise 6.2.

### Consistency of Sample Moments and Regression Coefficients

_Example 6.1_ (Consistency of the Sample Mean).: Let \(X_{1},\ldots,X_{n}\) be independent observations from an unknown distribution of which we assume a finite variance \(\sigma^{2}\)Let \(U_{n}(X)=\overline{X}=\frac{X_{1}+\ldots+X_{n}}{n}\) be used as an estimator of the unknown population mean \(\mu\). Since \(E\overline{X}=\mu\), and \(\operatorname{var}(\overline{X})=\sigma^{2}/n\to 0\) as \(n\to\infty\), it follows from Proposition 6.2, that \(\overline{X}\) is a consistent estimator of \(\mu\).

One may actually prove strong consistency of \(\overline{X}\) under the assumption of finiteness of \(\mu\), using the SLLN.

_Example 6.2 (Consistency of Sample Moments)._ Suppose a random sample \(X_{1}\), \(\ldots\), \(X_{n}\) is taken from a distribution with a finite \(k\)-th moment, for some \(k\geq 1\) (i.e., \(E|X_{j}|^{k}<\infty\)). Then it can be shown by the strong law of large numbers (SLLN) that the sample moments \(\hat{m}_{r}\) are strongly consistent estimators of population moments \(m_{r}\) for \(r=1,\ldots,k\),

\[\hat{m}_{r}=\frac{1}{n}\sum_{j=1}^{n}X_{j}^{r}\xrightarrow{\text{a.s.}}E(X_{1 }^{r}) =m_{r}\qquad\text{(the $r$-th `population moment')}\]

\[r =1,2,\ldots,k. \tag{6.8}\]

Note that \(\hat{m}_{r}\) is an unbiased estimator of \(m_{r}\) (\(r=1,2,\ldots,k\)). Hence if \(EX_{1}^{2k}\equiv m_{2k}<\infty\), then it follows from Proposition 6.1 that \(\hat{m}_{r}\) is a consistent estimator of \(m_{r}\) (\(r=1,2,\ldots,k\)). The SLLN implies that it is enough to assume \(E|X_{1}|^{k}<\infty\). Next consider the centered population moments \(\mu_{r}=E(X_{1}-m_{1})^{r}\), where \(m_{1}=\mu\) is the mean of the distribution (population). A natural estimator of \(\mu_{r}\) is the (corresponding) centered \(r\)-th sample moment \(\hat{\mu}_{r}=\frac{1}{n}\sum_{j=1}^{n}(X_{j}-\overline{X})^{r}\). Note that by the binomial expansion,

\[\hat{\mu}_{r} =\frac{1}{n}\sum_{j=1}^{n}\left\{X_{j}^{r}-\binom{r}{1}X_{j}^{r-1} \overline{X}+-\cdots+(-1)^{t}\binom{r}{t}X_{j}^{r-t}\overline{X}^{t}\pm\cdots +(-1)^{r}\overline{X}^{r}\right\}\] \[=\frac{1}{n}\sum_{j=1}^{n}\sum_{t=0}^{r}(-1)^{t}\binom{r}{t}X_{j }^{r-t}\overline{X}^{t}=\sum_{t=0}^{r}(-1)^{t}\binom{r}{t}\overline{X}^{t}\hat {m}_{r-t}\cdot\] \[=\sum_{t=0}^{r}(-1)^{t}\binom{r}{t}\hat{m}_{r-t}\hat{m}_{1}^{t}. \tag{6.9}\]

By repeated application of Proposition 6.2 or Corollary 6.1 it follows that the last sum converges in probability to

\[\sum_{t=0}^{r}(-1)^{t}\binom{r}{t}m_{r-t}m_{1}^{t}=E(X_{1}-m_{1})^{r}, \tag{6.10}\]

provided \(\hat{m}_{r^{\prime}}\xrightarrow{\mathcal{P}}m_{r^{\prime}}\) as \(n\to\infty\) (\(r^{\prime}=1,\ldots,r\)). The latter is assured for all \(r^{\prime}=1,\ldots,r\) if \(EX_{1}^{2r}<\infty\) (by Proposition 6.1). Once again this last requirement may be relaxed to \(E|X_{1}^{r}|<\infty\), by the SLLN. Thus _sample moments_, raw as well as centered, are strongly _consistent estimators_ of the _corresponding population moments_ if the corresponding population moments are finite.

_Remark 6.2_.: Although the 'raw' sample moments \(\hat{m}_{r}\) are unbiased estimators of the corresponding population moments \(m_{r}\) (if \(m_{r}\) is finite), this is not true for centered sample moments \(\mu_{r}\), \(r\geq 2\). For example, writing \(\mu=m_{1}\), \(\sigma^{2}=\mu_{2}\), and assuming \(\sigma^{2}>0\), one has\[E\hat{\mu}_{2} =E\left\{\frac{1}{n}\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}\right\}=E \left\{\frac{1}{n}\sum_{j=1}^{n}[X_{j}-\mu-(\overline{X}-\mu)]^{2}\right\}\] \[=E\left\{\frac{1}{n}\sum_{j=1}^{n}(X_{j}-\mu)^{2}+(\overline{X}- \mu)^{2}-2(\overline{X}-\mu)^{2}\right\}\] \[=E\left\{\frac{1}{n}\sum_{j=1}^{n}(X_{j}-\mu)^{2}-(\overline{X}- \mu)^{2}\right\}=\mu_{2}-\frac{\mu_{2}}{n}=\sigma^{2}-\frac{\sigma^{2}}{n}\] \[=\left(1-\frac{1}{n}\right)\sigma^{2}\neq\mu_{2}. \tag{6.11}\]

_Example 6.3 (Linear Regression)._ Consider the semiparametric regression model

\[Y_{j}=\alpha+\beta X_{j}+\varepsilon_{j}\qquad(1\leq j\leq n), \tag{6.12}\]

where the _response variable_\(Y\) and the nonstochastic _explanatory variable_\(X\) are observable, while the random errors \(\varepsilon_{j}\) are not. Assume that \(\varepsilon_{j}\) are i.i.d. with mean \(0\) and finite variance \(\sigma^{2}\). Consider the least squares estimators \(\hat{\alpha}\), \(\hat{\beta}\) of \(\alpha\), \(\beta\), i.e., values of \(\alpha\), \(\beta\) which minimize \(\sum_{1}^{n}(Y_{j}-\alpha-\beta X_{j})^{2}\). For ease of computation, let \(\delta=\alpha+\beta\overline{X}\), and express (6.12) in terms of the new parameters \(\delta\) and \(\beta\) as

\[Y_{j}=\delta+\beta(X_{j}-\overline{X})+\varepsilon_{j}. \tag{6.13}\]

Then, by calculus, \(\hat{\delta}=\overline{Y}\) and

\[\hat{\beta}=\frac{\sum_{j=1}^{n}(X_{j}-\overline{X})Y_{j}}{\sum_{j=1}^{n}(X_{ j}-\overline{X})^{2}}=\frac{\sum_{j=1}^{n}(X_{j}-\overline{X})(\alpha+\beta X_{j}+ \varepsilon_{j})}{\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}}=\beta+\frac{\sum_{j =1}^{n}\varepsilon_{j}(X_{j}-\overline{X})}{\sum_{j=1}^{n}(X_{j}-\overline{X })^{2}}\,. \tag{6.14}\]

In particular,

\[E\hat{\delta}=\delta,\qquad\operatorname{var}\hat{\delta}=\sigma ^{2}/n\,,\] \[E\hat{\beta}=\beta,\qquad\operatorname{var}\hat{\beta}=\frac{ \sigma^{2}\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}}{[\sum_{j=1}^{n}(X_{j}- \overline{X}^{2})]^{2}}=\frac{\sigma^{2}}{\sum_{j=1}^{n}(X_{j}-\overline{X})^ {2}}\,,\] \[\operatorname{cov}(\hat{\delta},\hat{\beta})=\operatorname{cov}( \overline{\varepsilon},\hat{\beta})=0. \tag{6.15}\]

Hence, by Proposition 6.1, \(\hat{\beta}\) is a consistent estimator of \(\beta\) if \(\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}\to\infty\) as \(n\to\infty\). Next,

\[\hat{\alpha} =\overline{Y}-\hat{\beta}\overline{X}=\alpha+\beta\overline{X}+ \frac{1}{n}\sum_{j=1}^{n}\varepsilon_{j}-\hat{\beta}\overline{X}=\alpha-(\hat{ \beta}-\beta)\overline{X}+\frac{1}{n}\sum_{j=1}^{n}\varepsilon_{j}\] \[E\hat{\alpha} =\alpha,\,\operatorname{var}\hat{\alpha}=\overline{X}^{2}( \operatorname{var}\hat{\beta})+\frac{\sigma^{2}}{n}-\frac{2\overline{X}}{n} \operatorname{cov}\left\{\frac{\sum_{j=1}^{n}\varepsilon_{j}(X_{j}-\overline{ X})}{\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}},\sum_{j=1}^{n}\varepsilon_{j}\right\}\] \[=\overline{X}^{2}(\operatorname{var}\hat{\beta})+\frac{\sigma^{ 2}}{n}-\frac{2}{n}(0)\] \[=\sigma^{2}\!\left\{\frac{1}{n}+\frac{\overline{X}^{2}}{\sum_{j=1 }^{n}(X_{j}-\overline{X})^{2}}\right\},\,\operatorname{cov}(\hat{\alpha},\hat{ \beta})=\operatorname{cov}(\hat{\delta}-\hat{\beta}\overline{X},\hat{\beta})=- \overline{X}\operatorname{var}\hat{\beta}. \tag{6.16}\]Hence \(\hat{\alpha}\), \(\hat{\beta}\) are consistent estimators of \(\alpha\), \(\beta\), respectively, if \(\sum_{1}^{n}(X_{j}-\overline{X})^{2}\to\infty\) and \(\overline{X}^{2}/\sum_{1}^{n}(X_{j}-\overline{X})^{2}\to 0\) as \(n\to\infty\) (also see Exercise 6.5).

_Remark 6.3_ (Consistency of \(\hat{\alpha}\), \(\hat{\beta}\) Under Dependence).: Note that the calculations (6.14)-(6.16) only required that

\[E\varepsilon_{j}=0,\qquad E\varepsilon_{j}^{2}=\sigma^{2},\qquad E\varepsilon _{j}\varepsilon_{j^{\prime}}=0\quad\text{for}\;j\neq j^{\prime}. \tag{6.17}\]

Thus \(\hat{\alpha}\), \(\hat{\beta}\) are consistent estimators of \(\alpha,\beta\) if \(\varepsilon_{j}\) are mean-zero uncorrelated (but not necessarily independent) random variables having a common variance \(\sigma^{2}\), and \(\sum_{1}^{n}(X_{j}-\overline{X})^{2}\to\infty\), \(\sum_{1}^{n}(X_{j}-\overline{X})^{2}/\overline{X}^{2}\to\infty\).

_Remark 6.4_ (Consistency of \(\hat{\alpha}\), \(\hat{\beta}\) under Heteroscedasticity).: Assume again that \(\varepsilon_{j}\) are mean-zero and uncorrelated, but heteroscedastic, i.e.,

\[E\varepsilon_{j}^{2}=\sigma_{j}^{2}\qquad(j=1,2,\dots). \tag{6.18}\]

Then \(\hat{\delta}=\overline{Y}\), \(\hat{\beta}\) are still unbiased estimators, as is \(\hat{\alpha}\). (This only requires \(E\varepsilon_{j}=0\) for all \(j\)). But

\[\operatorname{var}\hat{\delta} =\sum_{j=1}^{n}\sigma_{j}^{2}/n,\] \[\operatorname{var}\hat{\beta} =\frac{\Sigma_{j=1}^{n}\sigma_{j}^{2}(X_{j}-\overline{X})^{2}}{ \left[\Sigma_{j=1}^{n}(X_{j}-\overline{X})^{2}\right]^{2}}\,,\] \[\operatorname{var}\hat{\alpha} =\overline{X}^{2}(\operatorname{var}\hat{\beta})+\frac{\Sigma_{j =1}^{n}\sigma_{j}^{2}}{n^{2}}-\frac{2\Sigma\sigma_{j}^{2}(X_{j}-\overline{X}) }{n\Sigma_{j=1}^{n}(X_{j}-\overline{X})^{2}}\,\overline{X}\] \[\operatorname{cov}(\hat{\delta},\hat{\beta}) =\operatorname{cov}\left(\overline{\varepsilon},\frac{\sum_{1}^{ n}(X_{j}-\overline{X})\varepsilon_{j}}{\sum_{1}^{n}(X_{j}-\overline{X})^{2}} \right)=\frac{\sum_{1}^{n}\sigma_{j}^{2}(X_{j}-\overline{X})^{2}}{\sum_{1}^{n }(X_{j}-\overline{X})^{2}}\,. \tag{6.19}\]

Check that \(\operatorname{var}\hat{\beta}\) and \(\operatorname{var}\hat{\alpha}\) both go to zero as \(n\to\infty\), provided the following conditions hold:

\[\sigma_{j}^{2}\leq c<\infty\quad\text{for all}\;j,\qquad\sum_{j= 1}^{n}(X_{j}-\overline{X})^{2}\longrightarrow\infty,\] \[\sum_{1}^{n}(X_{j}-\overline{X})^{2}/\overline{X}^{2}\longrightarrow \infty\quad\text{as}\;n\to\infty. \tag{6.20}\]

For the last term in \(\operatorname{var}\hat{\alpha}\), use \(\lfloor\frac{1}{n}\sum_{j=1}^{n}\sigma_{j}^{2}(X_{j}-\overline{X})\rfloor\,\leq (\frac{1}{n}\sum_{j=1}^{n}\sigma_{j}^{4})^{1/2}(\frac{1}{n}\sum_{j=1}^{n}(X_{j }-\overline{X})^{2})^{1/2}\;\leq c^{2}(\frac{1}{n}\sum_{j=1}^{n}(X_{j}- \overline{X})^{2})^{1/2}\). One may relax the assumption of boundedness of \(\sigma_{j}^{2}\) in (6.18) even further (Exercise 6.5).

The estimators \(\hat{\delta}\), \(\hat{\alpha}\), \(\hat{\beta}\) here are called _ordinary least squares estimators_ or _OLS_ as opposed to _weighted least squares estimators_ considered later in Example 6.8.

Consider now the problem of _predicting \(Y\) from a future observation \(X\),_ based on past observations \((Y_{j},X_{j})\), \(1\leq j\leq n\), satisfying (6.12), with i.i.d. errors \(\varepsilon_{j}\). The natural predictor \(\hat{Y}=\hat{\alpha}+\hat{\beta}X\) has the prediction error \(\hat{Y}-Y\) with expectedsquared error \(\operatorname{var}\hat{\alpha}+X^{2}\operatorname{var}\hat{\beta}+2X\text{cov}( \hat{\alpha},\hat{\beta})+\sigma^{2}\) which may be estimated by replacing \(\sigma^{2}\) by \(\hat{\sigma}^{2}=\sum_{j=1}^{n}(Y_{j}-\hat{\alpha}-\hat{\beta}X_{j})^{2}/(n-2)\) [See (6.87), (6.88)].

_Example 6.4 (Autoregressive Model)._ A commonly used time series model is the linear autoregressive model of order \(k\geq 1\). For the case \(k=1\), it takes the form

\[Y_{j}=\alpha+\beta Y_{j-1}+\varepsilon_{j}\qquad(j=1,2,\ldots,n), \tag{6.21}\]

where \(Y_{0},Y_{1},\ldots,Y_{n}\) are observed, \(\alpha\) and \(\beta\) are unknown parameters to be estimated and \(\varepsilon_{j}\), \(1\leq j\leq n\), are uncorrelated mean zero random variables with common variance \(\sigma^{2}>0\) (unknown). Once again the least squares estimators are (see (6.14), (6.16), with \(X_{j}=Y_{j-1}\))

\[\hat{\alpha}=\overline{Y}_{1,n}-\hat{\beta}\overline{Y}_{0,n-1},\qquad\hat{ \beta}=\beta+\frac{\sum_{r=1}^{n}\varepsilon_{r}(Y_{r-1}-\overline{Y}_{0,n-1 })}{\sum_{r=1}^{n}(Y_{r-1}-\overline{Y}_{0,n-1})^{2}}, \tag{6.22}\]

where \(\overline{Y}_{1,n}=\frac{1}{n}\sum_{r=1}^{n}Y_{r}\), \(\overline{Y}_{0,n-1}=\frac{1}{n}\sum_{r=0}^{n-1}Y_{r}=\overline{Y}_{1,n}+(Y_{ 0}-Y_{n})/n\). Assume the stability condition

\[|\beta|<1. \tag{6.23}\]

One also assumes that \(Y_{0}\) is uncorrelated with of \(\{\varepsilon_{1},\ldots,\varepsilon_{n}\}\), and \(E\varepsilon_{j}^{4}<\infty\), \(EX_{0}^{2}<\infty\). It may be shown that, \(\hat{\alpha},\hat{\beta}\) are consistent estimators of \(\alpha,\beta\), respectively:

\[\hat{\beta}\stackrel{{ P}}{{\longrightarrow}}\beta,\;\hat{\alpha} \stackrel{{ P}}{{\longrightarrow}}\alpha\quad\text{as}\;\,n\to\infty. \tag{6.24}\]

_Proof of (6.24)._ First, by iteration of (6.21),

\[Y_{1} =\alpha+\beta Y_{0}+\varepsilon_{1},Y_{2}=\alpha+\beta Y_{1}+ \varepsilon_{2}=\alpha+\alpha\beta+\beta^{2}Y_{0}+\beta\varepsilon_{1}+ \varepsilon_{2},\ldots,\] \[Y_{r} =\alpha+\alpha\beta+\cdots+\alpha\beta^{r-1}+\beta^{r}Y_{0}+ \beta^{r-1}\varepsilon_{1}+\beta^{r-2}\varepsilon_{2}+\cdots+\beta\varepsilon _{r-1}+\varepsilon_{r}\] \[=\alpha(1-\beta^{r})/(1-\beta)+\beta^{r}Y_{0}+\sum_{j=1}^{r} \beta^{r-j}\varepsilon_{j}. \tag{6.25}\]

Hence

\[Y_{r}-\frac{\alpha}{1-\beta} =\frac{-\alpha\beta^{r}}{1-\beta}+\beta^{r}Y_{0}+\sum_{j=1}^{r} \beta^{r-j}\varepsilon_{j},\] \[\overline{Y}_{0,n-1}-\frac{\alpha}{1-\beta} =\frac{-\alpha(1-\beta)}{n(1-\beta)}+\frac{1-\beta^{n}}{n(1-\beta )}Y_{0}+\frac{1}{n}\sum_{r=1}^{n}\sum_{j=1}^{r}\beta^{r-j}\varepsilon_{j},\] \[E\left(\overline{Y}_{0,n-1}-\frac{\alpha}{1-\beta}\right)^{2} =E\,\frac{\left[\frac{-\alpha(1-\beta^{n})}{(1-\beta)}+\frac{1- \beta^{n}}{1-\beta}Y_{0}+\sum_{j=1}^{n-1}\frac{1-\beta^{n-j}}{1-\beta} \varepsilon_{j}\right]^{2}}{n^{2}}\] \[\leq\frac{3\left[\frac{\alpha^{2}(1-\beta^{n})^{2}}{(1-\beta)^{2 }}+\frac{(1-\beta^{n})^{2}}{(1x-\beta)^{2}}EY_{0}^{2}+E(\sum_{j=1}^{n-1}\frac {1-\beta^{n-j}}{1-\beta}\varepsilon_{j})^{2}\right]}{n^{2}}\] \[\qquad\text{(for, $(a+b+c)^{2}\leq 3a^{2}+3b^{2}+3c^{2}$)}\] \[=\text{O}\left(\frac{1}{n^{2}}\right)+\frac{3}{n^{2}}\sum_{j=1}^ {n-1}\frac{(1-\beta^{n-j})^{2}}{(1-\beta)^{2}}\sigma^{2}\] \[=\text{O}\left(\frac{1}{n^{2}}\right)+\text{O}\left(\frac{1}{n} \right)\longrightarrow 0. \tag{6.26}\]Next, writing \(Y_{r-1}-\overline{Y}_{0,n-1}=Y_{r-1}-\frac{\alpha}{1-\beta}-(\overline{Y}_{0,n-1}- \frac{\alpha}{1-\beta})\) one gets

\[\frac{1}{n}\sum_{r=1}^{n}(Y_{r-1}-\overline{Y}_{0,n-1})^{2}\] \[=\frac{1}{n}\sum_{r=1}^{n}\left(Y_{r-1}-\frac{\alpha}{1-\beta} \right)^{2}-\frac{2}{n}\sum_{r=1}^{n}\left(\overline{Y}_{0,n-1}-\frac{\alpha}{ 1-\beta}\right)\left(\overline{Y}_{0,n-1}-\frac{\alpha}{1-\beta}\right)\] \[\quad+\left(\overline{Y}_{0,n-1}-\frac{\alpha}{1-\beta}\right)^{2}\] \[=\frac{1}{n}\sum_{r=1}^{n}\left(Y_{r-1}-\frac{\alpha}{1-\beta} \right)^{2}-\left(\overline{Y}_{0,n-1}-\frac{\alpha}{1-\beta}\right)^{2},\]

so that, by (6.26),

\[\frac{1}{n}\sum_{r=1}^{n}(Y_{r-1}-\overline{Y}_{0,n-1})^{2}-\frac{1}{n}\sum_{ r=1}^{n}(Y_{r-1}-\frac{\alpha}{1-\beta})^{2}\stackrel{{ P}}{{\longrightarrow}}0. \tag{6.27}\]

Now, using the first relation in (6.26),

\[\left(Y_{r-1}-\frac{\alpha}{1-\beta}\right)^{2}-\left(\sum_{j=1}^ {r-1}\beta^{r-j-1}\varepsilon_{j}\right)^{2}\] \[=\left(\frac{-\alpha\beta^{r-1}}{1-\beta}+\beta^{r-1}Y_{0}\right) ^{2}+2\left(\frac{-\alpha\beta^{r-1}}{1-\beta}+\beta^{r-1}Y_{0}\right)\cdot \left(\sum_{j=1}^{r-1}\beta^{r-j-1}\varepsilon_{j}\right),\]

so that, by the Cauchy-Schwartz Inequality,

\[\left|\sum_{r=1}^{n}\left(Y_{r-1}-\frac{\alpha}{1-\beta}\right)^ {2}-\sum_{r=1}^{n}\left(\sum_{j=1}^{r-1}\beta^{r-j-1}\varepsilon_{j}\right)^{ 2}\right|\] \[\leq\sum_{r=1}^{n}\left(\frac{-\alpha\beta^{r-1}}{1-\beta}+\beta^ {r-1}Y_{0}\right)^{2}+2\left[\left(\sum_{r=1}^{n}\left\{\frac{-\alpha\beta^{r -1}}{1-\beta}+\beta^{r-1}Y_{0}\right\}^{2}\right)\right]^{1/2}\] \[\quad\cdot\left[\sum_{r=1}^{n}\left(\sum_{j=1}^{r-1}\beta^{r-j-1 }\varepsilon_{j}\right)^{2}\right]^{1/2},\] \[E\left|\left[\frac{\sum_{r=1}^{n}(Y_{r-1}-\frac{\alpha}{1-\beta} )^{2}}{n}-\frac{1}{n}\sum_{r=1}^{n}\left(\sum_{j=1}^{r-1}\beta^{r-j-1} \varepsilon_{j}\right)^{2}\right]\right|\] \[\leq\frac{2\alpha^{2}}{n(1-\beta)^{2}}\frac{1-\beta^{2n}}{1-\beta ^{2}}+\frac{2(1-\beta^{2n})}{n(1-\beta^{2})}\,EY_{0}^{2}\] \[\quad+\frac{2}{n}\left[E\left(\sum_{r=1}^{n}\left\{\frac{-\alpha \beta^{r-1}}{1-\beta}+\beta^{r-1}Y_{0}\right\}^{2}\right)\cdot E\left(\sum_{r =1}^{n}\sum_{j=1}^{r-1}\beta^{r-j-1}\varepsilon_{j}\right)^{2}\right]^{1/2}\]\[=\mathrm{O}\left(\frac{1}{n}\right)+\frac{2}{n}\cdot\mathrm{O}(1) \left(E\left[\sum_{j=1}^{n-1}\frac{1-\beta^{n-j}}{1-\beta}\varepsilon_{j}\right] ^{2}\right)^{1/2}\] \[=\mathrm{O}\left(\frac{1}{n}\right)+\frac{2}{n}\ \cdot\ \mathrm{O}(1) \left[\sum_{j=1}^{n-1}\frac{(1-\beta^{n-j})^{2}}{(1-\beta)^{2}}\sigma^{2} \right]^{1/2}=\mathrm{O}(n^{-1/2})\longrightarrow\,0. \tag{6.28}\]

Therefore, by Chebyshev's inequality (with \(p=1\)),

\[\frac{\sum_{r=1}^{n}(Y_{r-1}-\overline{Y}_{0,n-1})^{2}}{n}-\frac{1}{n}\sum_{r= 1}^{n}\left(\sum_{j=1}^{r-1}\beta^{r-j-1}\varepsilon_{j}\right)^{2}\stackrel{{ P}}{{\longrightarrow}}0. \tag{6.29}\]

Also,

\[E\,\frac{1}{n}\,\sum_{r=1}^{n}\left(\sum_{j=1}^{r-1}\beta^{r-j-1 }\varepsilon_{j}\right)^{2}\] \[=\frac{\sigma^{2}}{n}\,\sum_{r=1}^{n}\sum_{j=1}^{r-1}\beta^{2(r-j -1)}\] \[=\frac{\sigma^{2}}{n}\sum_{r=1}^{n}\frac{1-\beta^{2(r-1)}}{1-\beta ^{2}}\,\longrightarrow\,\frac{\sigma^{2}}{1-\beta^{2}}\,. \tag{6.30}\]

A little extra work shows that (Exercise 6.8)

\[\frac{1}{n}\sum_{r=1}^{n}\left(\sum_{j=1}^{r-1}\beta^{r-j-1}\varepsilon_{j} \right)^{2}\stackrel{{ P}}{{\longrightarrow}}\frac{\sigma^{2}}{1 -\beta^{2}}. \tag{6.31}\]

Also, from the above estimates (6.28), one gets

\[E\,\left[\frac{1}{n}\,\sum_{r=1}^{n}\varepsilon_{r}(Y_{r-1}-\overline{Y}_{0,n- 1})\right]^{2}\longrightarrow 0\quad\text{as}\,n\rightarrow\infty. \tag{6.32}\]

Combining (6.29), (6.31) and (6.32), we get

\[|\hat{\beta}-\beta|=\left|\frac{\frac{1}{n}\sum_{r=1}^{n}\varepsilon_{r}(Y_{r- 1}-\overline{Y}_{0,n-1})}{\frac{1}{n}\sum_{r=1}^{n}(Y_{r-1}-\overline{Y}_{0,n -1})^{2}}\right|\stackrel{{ P}}{{\longrightarrow}}\frac{0}{ \frac{\sigma^{2}}{1-\beta}}=0.\]

That is, \(\hat{\beta}\stackrel{{ P}}{{\longrightarrow}}\beta.\) Clearly then

\[\hat{\alpha}=\overline{Y}_{1,n}-\hat{\beta}\overline{Y}_{0,n-1}\stackrel{{ P}}{{\longrightarrow}}\frac{\alpha}{1-\beta}-\frac{\beta\alpha}{1-\beta}=\alpha.\]

_Remark 6.5_.: To prove (6.24) one may relax the assumption of common variance of \(\varepsilon_{j}\) by "\(\sigma_{j}^{2}:=E\varepsilon_{j}^{2}\) (\(j=1,2,\dots\)) is a bounded sequence".

_Remark 6.6_.: Suppose \(Y_{n}\stackrel{{ P}}{{\longrightarrow}}Y\). Although this does not in general imply that \(Y_{n}\stackrel{{\rm a.s.}}{{\longrightarrow}}Y\), it is a useful fact that there exists a subsequence \(Y_{n_{k}}\) (\(k\geq 1\)), \(n_{1}<n_{2}<\cdots\), such that \(Y_{n_{k}}\stackrel{{\rm a.s.}}{{\longrightarrow}}Y\).3

Footnote 3: See Bhattacharya and Waymire (2007, pp. 179, 180) or Billingsley (1986, p. 274).

### Consistency of Sample Quantiles

As a general definition, a _\(p\)-th quantile_ of a random variable \(X\), or of its distribution \(Q\), is a number \(c\) such that \(P(X\leq c)\geq p\), \(P(X<c)\leq p\), i.e., \(F(c)\geq p,F(c-)\leq p\), where \(F(c-)=\lim_{x\downarrow c\ (x<c)}F(x)\) is the _left-hand limit_ of \(F(x)\) as \(x\downarrow c\). Note that if \(F\) is continuous at \(x=c\), then \(F(c-)=F(c)\) but otherwise \(F(c-)<F(c)\), i.e., there is a jump in \(F\) at \(x=c\), \(P(X=c)>0\). One may also write the requirement as \(F(c)\geq p\), \(1-F(c-)\equiv P(X\geq c)\geq 1-p\).

Let \(X_{1},X_{2},\ldots,X_{n}\) be a random sample from \(Q\). Ordering them from the smallest to the largest, one may arrange them as \(X_{(1)}\leq X_{(2)},\leq\cdots\leq X_{(n)}\). Note that if we add another observation \(X_{n+1}\), then the orderings (even of the first \(n\) order statistics) generally change. For example, consider the samples \(\{2,7,5\}\) (\(n=3\)), and \(\{2,7,5,3\}\) (\(n=4\)). For the first sample \(X_{(1)}=2\), \(X_{(2)}=5\), \(X_{(3)}=7\). When a fourth observation is added one has \(X_{(1)}=2\), \(X_{(2)}=3\), \(X_{(3)}=5\), \(X_{(4)}=7\). For this reason, one should write the order statistics as \(X_{(1):n}\), \(X_{(2):n}\), \(X_{(n):n}\), indicating that this ordering is based on the first \(n\) observations. When there is little chance of confusion, we will continue to write \(X_{(j)}\) in place of \(X_{(j):n}\), keeping in mind that for any given \(n\), \(X_{(j)}\) depends on \(n\), as the sample size is increased.

**Definition 6.3**.: For a random sample \(\{X_{1},\ldots,X_{n}\}\) the _sample p-th quantile_\(\hat{\xi}_{p}\) (\(0<p<1\)) is defined either as \(X_{([np])}\) or \(X_{([np]+1)}\), where \([np]\) is the _integer part_ of \(np\). One may think of (or define) the sample \(p\)-th quantile as the \(p\)-th quantile of _the empirical distribution_\(Q_{n}\), which assigns mass \(\frac{1}{n}\) to each of the observed \(n\) points \(X_{1},X_{2},\ldots,X_{n}\). The empirical _distribution function_ (d.f.) of \(Q_{n}\) is:

\[F_{n}(x)=\frac{1}{n}\ \#\left\{j:1\leq j\leq n,\ X_{j}\leq x\right\}\qquad(x\in \mathbb{R}). \tag{6.33}\]

If \(F\) is continuous then, with probability one, \(X_{(1)}<X_{(2)}<\cdots<X_{(n)}\), i.e., there are no ties.

It follows from the SLLN that, for a given \(x\), \(\widehat{F}_{n}(x)\to F(x)\) almost surely, i.e., \(\widehat{F}_{n}(x)\) is a strongly consistent estimator of \(F(x)\). Indeed, by the so-called Gilvenko-Cantelli Theorem4\(\sup\{|\widehat{F}_{n}(x)-F(x)|:x\in\mathbb{R}\}\to 0\) almost surely as \(n\to\infty\).

Footnote 4: See Billingsley (1986, pp. 275, 276).

**Proposition 6.5** (Consistency of Sample Quantiles).: _Let \(X_{j}\), \(1\leq j\leq n\), be i.i.d. real-valued random variables with the common distribution function \(F\). Suppose, for a given \(p\in(0,1)\), the \(p\)-th quantile \(\xi_{p}\) is uniquely defined, i.e., there is a unique solution of \(F(x)=p\). Then \(\hat{\xi}_{p}\) is a consistent estimator of \(\xi_{p}\)._

Proof.: The hypothesis on \(F\) implies

\[F(x)<p\ \mbox{for all}\ x<\xi_{p}\ \mbox{and}\ F(x)>p\ \mbox{for all}\ x>\xi_{p}. \tag{6.34}\]Fix any \(\varepsilon>0\). Then, writing \(Y_{n}=\Sigma_{j=1}^{n}1_{\{X_{j}\leq\xi_{p}-\varepsilon\}}\),

\[P(\hat{\xi}_{p}\leq\xi_{p}-\varepsilon) = P(X_{([np])}\leq\xi_{p}-\varepsilon)=P(Y_{n}\geq[np]) \tag{6.35}\] \[= P\left(\frac{Y_{n}}{n}-F(\xi_{p}-\varepsilon)\geq\frac{[np]}{n} -F(\xi_{p}-\varepsilon)\right)\] \[\leq P\left(\left|\frac{Y_{n}}{n}-F(\xi_{p}-\varepsilon)\right| \geq\frac{[np]}{n}-F(\xi_{p}-\varepsilon)\right).\]

In view of (6.34) and the fact that \(\frac{[np]}{n}\to p\) as \(n\to\infty\) (indeed, \(|\frac{[np]}{n}-p|\leq\frac{1}{n}\)), it follows that for all sufficiently large \(n\) (e.g., \(n\geq 2/\delta(\varepsilon)\))

\[P(\hat{\xi}_{p}\leq\xi_{p}-\varepsilon)\leq P\left(\left|\frac{Y_{n}}{n}-F(\xi _{p}-\varepsilon)\right|\geq\frac{\delta(\varepsilon)}{2}\right), \tag{6.36}\]

where

\[\delta(\varepsilon)=p-F(\xi_{p}-\varepsilon)>0.\]

By Chebyshev's Inequality (6.5) applied to \(Y_{n}/n-F(\xi_{p}-\varepsilon)=Y_{n}/n-E(Y_{n}/n)\), one gets from (6.36) the inequality

\[P\left(\hat{\xi}_{p}\leq\xi_{p}-\varepsilon\right)\leq\frac{\text{var}(Y_{n}/n )}{(\delta(\varepsilon)/2)^{2}}=\frac{F(\xi_{p}-\varepsilon)(1-F(\xi_{p}- \varepsilon))}{n(\delta(\varepsilon/2)^{2})}\ \longrightarrow 0\ \text{as}\ n\to\infty. \tag{6.37}\]

Similarly, writing \(Z_{n}=\sum_{j=1}^{n}{\bf 1}_{\{X_{j}\leq\xi_{p}+\varepsilon\}}\),

\[P\left(\hat{\xi}_{p}>\xi_{p}+\varepsilon\right)=P(Z_{n}<[np]) \tag{6.38}\] \[= P\left(\frac{Z_{n}}{n}-F(\xi_{p}+\varepsilon)<\frac{[np]}{n}-F( \xi_{p}+\varepsilon)\right)\leq P\left(\frac{Z_{n}}{n}-F(\xi_{p}+\varepsilon) <-\delta^{\prime}(\varepsilon)\right)\] \[\leq P\left(\left|\frac{Z_{n}}{n}-F(\xi_{p}+\varepsilon\right|> \delta^{\prime}(\varepsilon)\right)\]

where, by (6.34),

\[\delta^{\prime}(\varepsilon):=F(\xi_{p}+\varepsilon)-p>0.\]

Note that \(\frac{[np]}{n}-F(\xi_{p}+\varepsilon)<p-F(\xi_{p}+\varepsilon)=-\delta^{ \prime}(\varepsilon)\). It now follows from (6.38) that, as \(n\to\infty\),

\[P\left(\hat{\xi}_{p}>\xi_{p}+\varepsilon\right)\leq\frac{\text{var}(\frac{Z_{n }}{n})}{(\delta^{\prime}(\varepsilon))^{2}}=\frac{F(\xi_{p}+\varepsilon)(1-F( \xi_{p}+\varepsilon))}{n(\delta^{\prime}(\varepsilon))^{2}}\longrightarrow 0. \tag{6.39}\]

The inequalities (6.37) and (6.39) imply \(P(|\hat{\xi}_{p}-\xi_{p}|>\varepsilon)\to 0\), as \(n\to\infty\).

Note that \(\xi_{1/2}\), if it is uniquely defined, is called the _median_ of \(F\).

Remark Remark 6.7: Suppose \(U_{n}\stackrel{{ P}}{{\longrightarrow}}U\), and \(U_{n}^{2}\) (\(n\geq 1\)) are _uniformly integrable_ i.e., \(\sup_{n}E[U_{n}^{2}:|U_{n}|\geq\lambda]\to 0\) as \(\lambda\to\infty\), then \(E(U_{n}-U)^{2}\to 0\).5 This allows one to sometimes derive \(E(U_{n}-\theta)^{2}\to 0\) from \(U_{n}\stackrel{{ P}}{{\longrightarrow}}\theta\).

### Convergence in Distribution or in Law (or Weak Convergence): The Central Limit Theorem

**Definition 6.4**.: A sequence of random variables \(Z_{n}\) (real-valued) is said to _converge in distribution_ to a probability measure \(P\) on \(\mathbb{R}\) if

\[Ef(Z_{n})\longrightarrow\int_{-\infty}^{\infty}fdP\quad\text{as }n\to\infty \tag{6.40}\]

for all bounded real-valued continuous functions \(f\) on \(\mathbb{R}.\) If the _distribution_ of \(Z_{n}\) is \(P_{n}\) (i.e., \(\operatorname{Prob}(Z_{n}\in A)=P_{n}(A)\) for all Borel sets, \(A\)), then one also describes this convergence as "\(P_{n}\)_converges weakly_ to \(P\)". Sometimes instead of "convergence in distribution" one uses the terminology "_convergence in law_," and uses the notation

\[Z_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}P. \tag{6.41}\]

Recall that the distribution function \(F\) of a random variable \(Z\) is defined by

\[F(x)=\operatorname{Prob}(Z\leq x)\ <-\infty<x<\infty. \tag{6.42}\]

One may show that the convergence in distribution (6.41) is equivalent to convergence of the distribution functions \(F_{n}(x)\) of \(Z_{n}\)_to_\(F(x)=P((-\infty,x])\) at all points of continuity of \(F\).

Convergence in distribution of \(Z_{n}\) to a probability law \(P\) is also equivalent to the convergence of characteristic functions of \(Z_{n}\) to the characteristic function of \(P\):

\[Ee^{i\xi Z_{n}}\longrightarrow\int_{-\infty}^{\infty}e^{i\xi z}dP(z),\qquad\xi \in\mathbb{R}. \tag{6.43}\]

The above definitions and results extend word for word when \(Z_{n}\) are vector-valued.6

Footnote 6: See, e.g., Bhattacharya and Waymire (2007, pp. 62, 63, 86).

The most important convergence theorem in law is the following theorem, abbreviated as _CLT_.7

Footnote 7: For a proof, see Bhattacharya and Waymire (2007, pp. 92, 93) or Billingsley (1986, pp. 398, 399).

**Proposition 6.6** (Central Limit Theorem).: _If \(Z_{n}\) is a sequence of i.i.d. random variables (or vectors) with common mean (or mean vector) zero and a finite common variance \(\sigma^{2}\) (or dispersion matrix \(\Sigma\)), then_

\[n^{-\frac{1}{2}}(Z_{1}+Z_{2}+\cdots+Z_{n})\stackrel{{\mathscr{L} }}{{\longrightarrow}}N(0,\sigma^{2})\quad(\text{or}\,N(0,\Sigma)). \tag{6.44}\]

Here \(N(0,\sigma^{2})\) is the _Normal distribution_ having mean zero and variance \(\sigma^{2}.\) For convenience we allow the possibility \(\sigma^{2}=0\) and in this case interpret \(N(0,0)\) as the probability measure _degenerate at_\(0\) (which assigns probability one to the singleton \(\{0\}\)). Similarly, \(\Sigma\) is in general a nonnegative definite matrix.

We will sometimes use the alternative notation \(\Phi_{0,\sigma^{2}}\) (or \(\Phi_{0,\Sigma}\)) for \(N(0,\sigma^{2})\) (or \(N(0,\Sigma)\)), and denote the corresponding distribution function by \(\Phi_{0,\sigma^{2}}(x),\) and the density function by \(\phi_{0,\sigma^{2}}(x).\) According to the criterion stated immediately after (6.42), the central limit theorem (CLT) says: if \(\sigma^{2}>0.\)\[Prob\left(\frac{Z_{1}+Z_{2}+\cdots+Z_{n}}{n^{\frac{1}{2}}}\leq x\right)\longrightarrow \Phi_{0,\sigma^{2}}(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\int_{-\infty}^{x}e^{-y^{2 }/2\sigma^{2}}dy \tag{6.45}\]

_for all \(x.\)_ An old result of Polya implies that actually the convergence in (6.45) _is uniform_ over all \(x\):

\[\sup_{x}\left|P\left(\frac{Z_{1}+\cdots+Z_{n}}{\sqrt{n}}\leq x\right)-\Phi_{0, \sigma^{2}}(x)\right|\longrightarrow 0\quad\text{as }n\rightarrow\infty. \tag{6.46}\]

For a precise statement and proof of Polya's Theorem, see the Appendix to this chapter (Appendix D).

An immediate application of the _CLT_ is that the \(r\)th _sample moment_\((X_{1}^{r}+\cdots+X_{n}^{r})/n\)_is asymptotically normal_ if \(EX_{1}^{2r}<\infty.\) (Here \(r\) is a positive integer). The italicized statement means:

\[\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{r}-EX_{1}^{r}\right)\overset{ \mathscr{L}}{\longrightarrow}N(0,\sigma^{2}), \tag{6.47}\]

where \(\sigma^{2}=\text{var}(X_{1}^{r})=EX_{1}^{2r}-(EX_{1}^{r})^{2}.\)

_Remark 6.8_.: The law of large numbers shows that for a sequence of i.i.d. random variables \(Z_{n},\)\(\frac{1}{n}\sum_{i=1}^{n}Z_{i}\simeq\mu\) (\(=EZ_{1}\)), i.e., the difference between the two sides in \(\simeq\) goes to zero as \(n\rightarrow\infty.\) Under the additional assumption \(EZ_{1}^{2}<\infty,\) Chebyshev's Inequality strengthens this approximation by showing that the difference is of the order of \(n^{\frac{1}{2}}\) in probability:

\[\frac{1}{n}\sum_{i=1}^{n}Z_{i}-\mu=O_{p}\left(\frac{1}{\sqrt{n}}\right). \tag{6.48}\]

This means that, given \(\varepsilon<0\) there exists \(A>0\) such that

\[\text{Prob}\left(\left|\frac{1}{n}\sum_{i=1}^{n}Z_{i}-\mu\right|>\frac{A}{ \sqrt{n}}\right)<\varepsilon \tag{6.49}\]

for all sufficiently large \(n.\) The CLT is of course a more precise statement than (6.48) [or (6.49)]. In particular, it provides a computation of the left side of (6.49) as

\[2\Phi_{0,1}\left(-\frac{A}{\sigma}\right)+\delta_{n}=\frac{2}{\sqrt{2\pi}} \int_{-\infty}^{-A/\sigma}e^{-y^{2}/2}dy+\delta_{n}, \tag{6.50}\]

where \(\delta_{n}\to 0\) as \(n\rightarrow\infty\) (uniformly w.r.t. \(A\)). (See Exercise 6.12.)

Before we may consider more important applications of the CLT we need some useful elementary facts.

First, we shall use the notation

\[Z_{n}\overset{\mathscr{L}}{\longrightarrow}Z \tag{6.51}\]

sometimes, in place of (6.41), and say that \(Z_{n}\)_converges in distribution_ (or _law_) to \(Z,\) with the understanding that \(Z\) is a random variable (or random vector) whose distribution is \(P.\) Although the definition (6.41) does not imply (and in most situations of interest it is not true) that there exists a limiting random variable in the sense of convergence in probability or a.s. convergence, the notation (6.51) and the corresponding language has some advantage--as the following results show.

**Proposition 6.7**.: _Suppose \(Z_{n}\ \stackrel{{\mathscr{L}}}{{\longrightarrow}}\ Z\) and \(g\) is a continuous function. Then \(g(Z_{n})\stackrel{{\mathscr{L}}}{{\longrightarrow}}g(Z)\)._

Proof.: Use Definition 6.4 (6.40) (Exercise 6.14). 

**Proposition 6.8** (Slutsky's Lemma).: _Suppose \(U_{n},V_{n},W_{n}\ (n\geq 1)\) are three sequences of real-valued random variables such that \(U_{n}\stackrel{{ P}}{{\longrightarrow}}a\), \(V_{n}\stackrel{{ P}}{{\longrightarrow}}b\), \(W_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}W\). Let \(h(u,v,w)\) be a function which is continuous on \([a-\delta_{1},a+\delta_{1}]\times[b-\delta_{2},b+\delta_{2}]\times(-\infty,\infty)\) for some \(\delta_{1}>0\), \(\delta_{2}>0\). Then \(h(U_{n},V_{n},W_{n})\stackrel{{\mathscr{L}}}{{\longrightarrow}}h \ (a,b,W)\)._

Proof.: We will first prove that \(h(U_{n},V_{n},W_{n})-h(a,b,W_{n})\stackrel{{ P}}{{\longrightarrow }}0\). The desired result would follow from this, using the continuity of \(w\to h(a,b,w)\) (see Proposition 6.4). Fix \(\varepsilon>0\) and \(\theta>0\), however small. In view of the convergence in distribution of \(W_{n}\) there exists \(A=A(\theta)\) such that \(P(|W_{n}|>A)<\theta/3\) for all \(n\) (Exercise 6.15). In view of (uniform) continuity of \(h\) on the compact set \([a-\delta_{1},a+\delta_{1}]\times[b-\delta_{2},b+\delta_{2}]\times[-A,A]\), there exists \(\delta=\delta(\varepsilon)>0\) such that \(|h(u,v,w)-h(a,b,w)|\leq\varepsilon\) for all \((u,v,w)\) satisfying \(|u-a|\leq\delta\), \(|v-b|\leq\delta\) and \(|w|\leq A\). Now since \(U_{n}\stackrel{{ P}}{{\longrightarrow}}a\), \(V_{n}\stackrel{{ P}}{{\longrightarrow}}b\), there exists a positive integer \(n(\theta,\varepsilon)\) such that

\[P(|U_{n}-a|>\delta)<\frac{\theta}{3},\quad P(|V_{n}-b|>\delta)<\frac{\theta}{3 }\quad\forall\ n\geq n(\theta,\varepsilon). \tag{6.52}\]

Hence

\[P(|h(U_{n},V_{n},W_{n})-h(a,b,W_{n})|>\varepsilon) \tag{6.53}\] \[\leq P(|U_{n}-a|>\delta)+P(|V_{n}-b|>\delta)+P(|W_{n}|>A)\] \[\quad+P(\{|U_{n}-a|\leq\delta,|V_{n}-b|\leq\delta,|W_{n}|\] \[\leq A,|h(U_{n},V_{n},W_{n})-h(a,b,W_{n})|>\varepsilon\})\] \[\leq 3\,\frac{\theta}{3}=\theta,\]

since the set within curly brackets in (6.53) is empty and has, therefore, probability zero. 

_Remark 6.9_.: Proposition 6.8 easily extends to the case of \(k\) sequences \(U_{ni}\stackrel{{ P}}{{\longrightarrow}}\), \(a_{i}\ (i=1,\ldots,k)\), \(W_{n}\ \stackrel{{\mathscr{L}}}{{\longrightarrow}}\ W\ \mathbb{R}^{p}\)-valued, \(h(u_{1},\ldots,u_{k},w)\) continuous on \(O\times\mathbb{R}^{p}\), where \(O\) is an open neighborhood of \((a_{1},\ldots,a_{k})\), \(h(U_{n1},\ldots,U_{nk},W_{n})\ \stackrel{{\mathscr{L}}}{{\longrightarrow}}h(a_{1},\ldots,a_{k},W)\). Indeed, the function \(h\) may also be vector-valued.

The following simple result is widely used in large sample theory.

**Theorem 6.1**.: _Suppose \(W_{n}\) is a sequence of random variables and \(g(n)\) a sequence of constants, \(g(n)\uparrow\infty\), such that \(g(n)(W_{n}-\mu)\stackrel{{\mathscr{L}}}{{\longrightarrow}}V\). Then for every function \(H\) which is continuously differentiable in a neighborhood of \(\mu\), one has_

\[g(n)[H(W_{n})-H(\mu)]\stackrel{{\mathscr{L}}}{{\longrightarrow}}H ^{\prime}(\mu)V. \tag{6.54}\]

_(b)_Proof: By the mean value theorem there exists \(\mu^{*}\) between \(\mu\) and \(W_{n}\) such that the left side of (6.54) is

\[g(n)(W_{n}-\mu)H^{\prime}(\mu^{*})=g(n)(W_{n}-\mu)\{H^{\prime}(\mu)+o_{p}(1)\} \tag{6.55}\]

where \(o_{p}(1)\to 0\) in probability. Note that this is a consequence of the fact that \(W_{n}-\mu\stackrel{{ P}}{{\longrightarrow}}0\) [since \(g(n)\uparrow\infty\) and \(g(n)(W_{n}-\mu)\stackrel{{\mathscr{L}}}{{\longrightarrow}}V\) (Exercise 6.13(c))]. The convergence (6.54) now follows from Proposition 6.8. 

The most commonly used consequence of Theorem 6.1 is the following, called the delta method.

**Corollary 6.2** (The Delta Method): _Suppose \(Z_{j}\), \(j\geq 1\), are i.i.d. random variables with common mean and variance \(\mu\) and \(\sigma^{2}<\infty\). If \(H\) is continuously differentiable in a neighborhood of \(\mu\), then_

\[\sqrt{n}[H(\overline{Z})-H(\mu)]\stackrel{{\mathscr{L}}}{{ \longrightarrow}}H^{\prime}(\mu)V\stackrel{{\mathscr{L}}}{{=}}N(0,(H^{\prime}(\mu))^{2}\sigma^{2}), \tag{6.56}\]

_where \(\overline{Z}=\overline{Z}_{n}=\sum_{j=1}^{n}Z_{j}/n\), and \(V\) is \(N(0,\sigma^{2})\)._

For future reference and notational convenience, the following definition is useful.

**Definition 6.5**: _A statistic \(T=T_{n}\) (i.e., a function of observations \(X_{1},\ldots,X_{n}\)) is said to be asymptotically Normal with mean \(\theta\) and variance \(\frac{\sigma^{2}}{n}\), or \(AN(\theta,\frac{\sigma^{2}}{n})\), if \(\sqrt{n}(T_{n}-\theta)\stackrel{{\mathscr{L}}}{{\longrightarrow} }N(0,\sigma^{2})\). A vector valued \(\mathbf{T}_{n}\) is said to be \(AN(\boldsymbol{\theta},\frac{1}{n}\Sigma)\) if \(\sqrt{n}(T_{n}-\theta)\stackrel{{\mathscr{L}}}{{\longrightarrow} }N(0,\Sigma)\)._

_Remark 6.10_: Corollary 6.2 (and, indeed, Theorem 6.1) extends to the case where \(Z_{j}\), \(j\geq 1\), are i.i.d. \(k\)-dimensional random vectors with mean vector \(\mu\) and covariance matrix \(\Sigma=((\sigma_{ij}))\), while \(H\) is real-valued and continuously differentiable (as a function of \(k\) variables) in a neighborhood of \(\mu\). In this case

\[\sqrt{n}[H(\overline{Z})-H(\mu)]\stackrel{{\mathscr{L}}}{{ \longrightarrow}}Grad\ H(\mu)\cdot V\stackrel{{\mathscr{L}}}{{ \cong}}N(0,\sum_{i,j=1}^{k}\ell_{i}\ell_{j}\sigma_{ij}), \tag{6.57}\]

with \(\ell_{i}:=(D_{i}H)(\mu)=(\partial H(z)/\partial z_{i})_{z=\mu}\), and \(V\) is \(N(0,\sigma)\).

_Example 6.5_ (\(t\)_-Statistic): _Let \(Y_{n}\) be a sequence of i.i.d. one-dimensional random variables and consider the \(t\)-statistic_

\[t_{n} =\frac{\sqrt{n}(\overline{Y}-\mu)}{[(\sum_{j=1}^{n}Y_{j}^{2}-n \overline{Y}^{2})/(n-1)]^{\frac{1}{2}}}=\frac{\sqrt{n-1}(\overline{Y}-\mu)}{( \frac{1}{n}\sum_{1}^{n}Y_{j}^{2}-\overline{Y}^{2})^{\frac{1}{2}}}\] \[=\sqrt{\frac{n-1}{n}}\,\overline{t}_{n},\qquad\qquad\overline{t} _{n}=\frac{\sqrt{n}(\overline{Y}-\mu)}{(\frac{1}{n}\sum_{1}^{n}Y_{j}^{2}- \overline{Y}^{2})^{\frac{1}{2}}}\,. \tag{6.58}\]

_Here \(EY_{n}=\mu\), \(\operatorname{var}Y_{n}=\sigma^{2}>0\) (finite). Note that_

\[\sqrt{n}(\overline{Y}-\mu) \stackrel{{\mathscr{L}}}{{\longrightarrow}}N(0,\sigma ^{2}),\] \[\frac{1}{n}\sum_{1}^{n}Y_{j}^{2}-\overline{Y}^{2}\stackrel{{ \mathrm{a.s.}}}{{\longrightarrow}}EY_{1}^{2}-\mu^{2}=\sigma^{2}>0,\]

_so that Proposition 6.8 applies to show that \(t_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}N(0,1)\)._

[MISSING_PAGE_FAIL:137]

Then, since \(\ell_{1}=0=\ell_{2}\),

\[\sigma^{2}=\sum_{i,j=1}^{5}\ell_{i}\ell_{j}\sigma_{ij}=\sum_{i,j=3}^{5}\ell_{i} \ell_{j}\sigma_{ij}. \tag{6.65}\]

Now

\[\begin{array}{ll}\sigma_{33}=\operatorname{var}(U_{j}^{2})=EU_{j}^{4}-1,& \sigma_{44}=\operatorname{var}(V_{j}^{2})=EV_{j}^{4}-1,\\ \sigma_{55}=\operatorname{var}(U_{j}V_{j})=EU_{j}^{2}V_{j}^{2}-\rho^{2},&\sigma _{34}=\sigma_{43}=\operatorname{cov}(U_{j}^{2},V_{j}^{2})=EU_{j}^{2}V_{j}^{2}- 1,\\ \sigma_{35}=\sigma_{53}=\operatorname{cov}(U_{j}^{2},U_{j}V_{j})&\sigma_{45}= \sigma_{54}=EV_{j}^{3}U_{j}-\rho.\\ =EU_{j}^{3}V_{j}-\rho,\end{array} \tag{6.66}\]

From (6.64) to (6.66) we get

\[\sigma^{2} = \left(-\frac{1}{2}\rho\right)^{2}(EU_{j}^{4}-1)+\left(-\frac{1}{2 }\rho\right)^{2}(EV_{j}^{4}-1)+1^{2}(EU_{j}^{2}V_{j}^{2}-\rho^{2}) \tag{6.67}\] \[+2\left(-\frac{1}{2}\rho\right)^{2}(EU_{j}^{2}V_{j}^{2}-1)+2\left( -\frac{1}{2}\rho\right)(1)(EU_{j}^{3}V_{j}-\rho)\] \[+2\left(-\frac{1}{2}\rho\right)(1)(EV_{j}^{3}U_{j}-\rho)\] \[= \rho^{2}\left[\frac{EU_{j}^{4}}{4}+\frac{EV_{j}^{4}}{4}+\frac{EU_ {j}^{2}V_{j}^{2}}{2}\right]-\rho\left[EU_{j}^{3}V_{j}+EV_{j}^{3}U_{j}\right]+EU _{j}^{2}V_{j}^{2}.\]

In particular, if \((X_{j},Y_{j})\) are normal, then (Exercise 6.16)

\[\sigma^{2}=\rho^{2}\left[\frac{3}{4}+\frac{3}{4}+\frac{1}{2}+\rho^{2}\right]- \rho[3\rho+3\rho]+1+2\rho^{2}=1-2\rho^{2}+\rho^{4}=(1-\rho^{2})^{2}, \tag{6.68}\]

using the facts \(EU^{2}=EV^{2}=1\), \(EU^{4}=EV^{4}=3\), the conditional distribution of \(U\), given \(V\), is \(N(\rho V,1-\rho^{2})\) and, similarly, the conditional distribution of \(V\), given \(U\), is \(N(\rho U,1-\rho^{2})\).

_Remark 6.11_.: If one attempts to derive (6.61) directly without using the delta method, then one arrives at

\[\sqrt{n}(r-\rho) = \frac{\sqrt{n}\left[\frac{1}{n}\sum_{j=1}^{n}(U_{j}V_{j}-\rho)- \overline{U}\ \overline{V}\right]}{\left[\frac{1}{n}\sum_{j=1}^{n}(U_{j}- \overline{U}^{2})\right]^{1/2}\left[\frac{1}{n}\sum_{j=1}^{n}(V_{j}-\overline {V})^{2}\right]^{1/2}}\] \[-\sqrt{n}\rho\left(1\,-\,\frac{1}{\left[\frac{1}{n}\sum_{j=1}^{n} (U_{j}-\overline{U})^{2}\right]^{1/2}\left[\frac{1}{n}\sum_{j=1}^{n}(V_{j}- \overline{V})^{2}\right]^{1/2}}\right).\]

Using the fact that \(\sqrt{n}\ \overline{U}\ \overline{V}\stackrel{{ P}}{{\longrightarrow}}0\), and the denominator of the first term on the right in (6.69) converges to \(1\) in probability, one arrives at the fact that this term converges in distribution to \(N(0,EU_{1}^{2}V_{1}^{2}-\rho^{2})\). One may similarly prove that the second term on the right converges in distribution to some Normal distribution \(N(0,\delta)\), say. However, from these facts alone one can not conclude convergence of the sum to a Normal law and/or derive the asymptotic variance of \(\sqrt{n}(r-\rho)\). For problems such as these, Efron's, percentile _bootstrap method_ for estimating the distribution of the statistic is very effective (see Chap. 9). Diaconis and Efron (1983) may be consulted for an example.

### Asymptotics of Linear Regression

Consider the semiparametric regression model described in Example 6.3, Sect. 6.3. Write

\[b_{n}=\left[\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}\right]^{\frac{1}{2}}. \tag{6.70}\]

We will first prove that

\[b_{n}(\hat{\beta}-\beta)\equiv\left(\sum_{j^{\prime}=1}^{n}(X_{j^{\prime}}- \overline{X})^{2}\right)^{\frac{1}{2}}(\hat{\beta}-\beta)\stackrel{{ \mathscr{L}}}{{\longrightarrow}}N(0,\sigma^{2})\quad\text{as $n\to\infty$}, \tag{6.71}\]

if

\[\delta_{n}:=\max_{1\leq j\leq n}\frac{(X_{j}-\overline{X})^{2}}{\sum_{j^{ \prime}\leq 1}^{n}(X_{j^{\prime}}-\overline{X})^{2}}\longrightarrow 0\quad\text{as $n\to\infty$}. \tag{6.72}\]

Now the left side of (6.71) may be expressed as [see (6.14)]

\[\sum_{j=1}^{n}\varepsilon_{j,n},\qquad\varepsilon_{j,n}=:\frac{(X_{j}- \overline{X})\varepsilon_{j}}{(\sum_{j^{\prime}=1}^{n}(X_{j^{\prime}}- \overline{X})^{2})^{\frac{1}{2}}}\,, \tag{6.73}\]

so that

\[E\varepsilon_{j,n}=0\qquad E\varepsilon_{j,n}^{2}=\frac{(X_{j}- \overline{X})^{2}}{\sum_{j^{\prime}=1}^{n}(X_{j^{\prime}}-\overline{X})^{2}} \,\sigma^{2}\leq\delta_{n}\sigma^{2},\] \[\sum_{j=1}^{n}E\varepsilon_{j,n}^{2}=\sigma^{2}. \tag{6.74}\]

By the Lindeberg central limit theorem (see Appendix D), it is enough to prove that for every \(\eta>0\),

\[\gamma_{n}:=E\sum_{j=1}^{n}\varepsilon_{j,n}^{2}\mathbf{1}_{\{|\varepsilon_{ j,n}|>\eta|\}}\longrightarrow 0\quad\text{as $n\to\infty$}. \tag{6.75}\]

But the expectation on the left satisfies

\[\gamma_{n} \leq\sum_{j=1}^{n}\frac{(X_{j}-\overline{X})^{2}}{\sum_{j^{ \prime}=1}^{n}(X_{j^{\prime}}-\overline{X})^{2}}\;E\varepsilon_{j}^{2} \mathbf{1}_{[\varepsilon_{j}^{2}>\eta^{2}/\delta_{n}]}\] \[=E\varepsilon_{1}^{2}\mathbf{1}_{[\varepsilon_{1}^{2}>\eta^{2}/ \delta_{n}]}\longrightarrow 0. \tag{6.76}\]

Note that \(\mathbf{1}_{[\varepsilon_{1}^{2}>\eta^{2}/\delta_{n}]}\stackrel{{ \text{a.s.}}}{{\longrightarrow}}0\) since \(\eta^{2}/\delta_{n}\to\infty\), and \(\varepsilon_{1}^{2}\mathbf{1}_{[\varepsilon_{1}^{2}>\eta^{2}/\delta_{n}]}\leq \varepsilon_{1}^{2}\), so that one may apply Lebesgue's dominated convergence theorem to obtain the last relation in (6.76). Using (6.16) one may write \[a_{n}(\hat{\alpha}-\alpha)=\sum_{j=1}^{n}\tilde{\varepsilon}_{j,n} \tag{6.77}\]

where

\[a_{n}=\left\{\frac{1}{n}+\frac{\overline{X}^{2}}{\sum_{j=1}^{n}(X_{j}-\overline {X})^{2}}\right\}^{-\frac{1}{2}}=n^{\frac{1}{2}}\left[\frac{\sum_{j=1}^{n}(X_ {j}-\overline{X})^{2}}{\sum_{j=1}^{n}X_{j}^{2}}\right]^{\frac{1}{2}}=\frac{b_{ n}}{m_{2}^{\frac{1}{2}}}\,, \tag{6.78}\]

so that

\[\tilde{\varepsilon}_{j,n}=a_{n}\left\{\frac{-(X_{j}-\overline{X})\overline{X} }{\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}}+\frac{1}{n}\right\}\varepsilon_{j}= \frac{1}{m_{2}^{\frac{1}{2}}}\left\{\frac{-(X_{j}-\overline{X})\overline{X}}{b _{n}}+\frac{b_{n}}{n}\right\}\varepsilon_{j}. \tag{6.79}\]

Here

\[m_{2}=\frac{1}{n}\sum_{j=1}^{n}X_{j}^{2}. \tag{6.80}\]

Then \(\tilde{\varepsilon}_{j,n}\), \(1\leq j\leq n\), are independent,

\[E\tilde{\varepsilon}_{j,n}=0,\quad\tilde{\varepsilon}_{j,n}^{2} \leq 2a_{n}^{2}\left\{\frac{(X_{j}-\overline{X})^{2}\overline{X}^{2}}{ \left[\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}\right]^{2}}+\frac{1}{n^{2}} \right\}\varepsilon_{j}^{2}=\theta_{j,n}\varepsilon_{j}^{2},\ \text{say},\] \[\sum_{j=1}^{n}E\tilde{\varepsilon}_{j,n}^{2}=\sigma^{2}. \tag{6.81}\]

Now, noting that \(n\overline{X}^{2}\leq\sum_{j=1}^{n}X_{j}^{2}\), one has

\[\theta_{j,n} =\frac{2n\overline{X}^{2}(X_{j}-\overline{X})^{2}}{\left(\sum_{j= 1}^{n}(X_{j}-\overline{X})^{2}\right)\left(\sum_{j=1}^{n}X_{j}^{2}\right)}+ \frac{2\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}}{n\sum_{j=1}^{n}X_{j}^{2}}\] \[\leq 2\delta_{n}+\frac{2}{n}\longrightarrow 0,\quad(1\leq j\leq n);\] \[\sum_{1}^{n}\theta_{jn} =\frac{2n\overline{X}^{2}}{\sum_{j=1}^{n}X_{j}^{2}}+\frac{2\sum_ {j=1}^{n}(X_{j}-\overline{X})^{2}}{\sum_{j=1}^{n}X_{j}^{2}}\] \[\leq 2+2=4. \tag{6.82}\]

Hence

\[E\sum_{j=1}^{n}\tilde{\varepsilon}_{j,n}^{2}\mathbf{1}_{\{\tilde {\varepsilon}_{j,n}^{2}>\eta^{2}\}} \leq E\sum_{j=1}^{n}\theta_{j,n}\varepsilon_{j}^{2}\mathbf{1}_{\{ \varepsilon_{j}^{2}>\eta^{2}/(2\delta_{n}+\frac{2}{n})\}}\] \[=\sum_{j=1}^{n}\theta_{j,n}E\varepsilon_{1}^{2}\mathbf{1}_{\{ \varepsilon_{j}^{2}>\eta^{2}/(2\delta_{n}+\frac{2}{n})\}}\longrightarrow 0\quad\text{as $n\to\infty$}, \tag{6.83}\]

for every \(\eta>0\). Hence, by the Lindeberg CLT,

\[a_{n}(\hat{\alpha}-\alpha)\ \stackrel{{\mathscr{L}}}{{\longrightarrow}}\ N(0,\sigma^{2}),\quad\text{as $n\to\infty$}. \tag{6.84}\]From (6.72), (6.84) one can construct confidence intervals for \(\alpha\) and \(\beta\), separately, for any desired asymptotic level (or, confidence coefficient), provided a consistent estimator of \(\sigma^{2}\) can be found. Such an estimator is

\[\hat{\sigma}^{2}=\frac{1}{n-2}\sum_{j=1}^{n}\left(Y_{j}-\hat{\alpha}-\hat{\beta }X_{j}\right)^{2}. \tag{6.85}\]

To see this, let \(Z_{n}=\sum_{j=1}^{n}(X_{j}-\overline{X})\varepsilon_{j}\) and express the right side of (6.85) as

\[\frac{1}{n-2}\sum_{j=1}^{n}\left[-\frac{Z_{n}}{b_{n}^{2}}(X_{j}- \overline{X})+\varepsilon_{j}-\overline{\varepsilon}\right]^{2}=\frac{1}{n-2 }\left[\frac{Z_{n}^{2}}{b_{n}^{2}}+\sum_{j=1}^{n}(\varepsilon_{j}-\overline{ \varepsilon})^{2}-2\frac{Z_{n}^{2}}{b_{n}^{2}}\right]\] \[=\frac{1}{n-2}\sum_{j=1}^{n}\varepsilon_{j}^{2}-\frac{n}{n-2} \overline{\varepsilon}^{2}-\frac{Z_{n}^{2}}{b_{n}^{2}(n-2)}\,. \tag{6.86}\]

By the SLLN, the first two terms on the extreme right side converge to \(\sigma^{2}\)and \(0\) a.s.; also, \(EZ_{n}^{2}/(b_{n}^{2}(n-2))=\sigma^{2}/(n-2)\to 0\), so that the last term in (6.86) converges in probability to \(0\). Hence

\[\hat{\sigma}^{2}\longrightarrow\sigma^{2}\quad\text{in probability, as }n\to\infty. \tag{6.87}\]

In addition, \(\hat{\sigma}^{2}\) is an _unbiased_ estimator of \(\sigma^{2}\). For

\[E\hat{\sigma}^{2} = \frac{1}{n-2}\,E\left[\sum_{j=1}^{n}(\varepsilon_{j}-\overline{ \varepsilon})^{2}-\frac{Z_{n}^{2}}{b_{n}^{2}}\right]=\frac{1}{n-2}\left[(n-1) \sigma^{2}-\sigma^{2}\right] \tag{6.88}\] \[= \sigma^{2}.\]

Thus

\[\hat{\alpha}\pm\frac{\hat{\sigma}}{a_{n}}\,z_{1-\frac{\hat{\sigma}}{2}},\quad \hat{\beta}\pm\frac{\hat{\sigma}}{b_{n}}\,z_{1-\frac{\hat{\sigma}}{2}} \tag{6.89}\]

are confidence intervals for \(\alpha\) and \(\beta\), each of asymptotic confidence coefficient \(1-\theta\). Here \(z_{\alpha}\) is the \(\alpha\)-th quantile of the standard Normal distribution \(N(0,1)\),--that is, the \(N(0,1)\)--probability of this set of values larger than \(z_{\theta/2}\) is \(\theta/2\). Although, a _Bonferroni-type confidence region_ for the pair \((\alpha,\beta)\) may be given by the rectangle

\[\left\{(\alpha,\beta):\hat{\alpha}-\frac{\hat{\sigma}}{a_{n}}\,z_{1-\frac{ \hat{\sigma}}{4}}\leq\alpha\leq\hat{\alpha}+\frac{\hat{\sigma}}{a_{n}}\,z_{1- \frac{\hat{\sigma}}{4}},\,\hat{\beta}-\frac{\hat{\sigma}}{b_{n}}\,z_{1-\frac{ \hat{\sigma}}{4}}\leq\beta\leq\hat{\beta}+\frac{\hat{\sigma}}{b_{n}}\,z_{1- \frac{\hat{\sigma}}{4}}\right\}, \tag{6.90}\]

having an asymptotic confidence coefficient of at least \((1-\theta)\), for a better and more precise confidence region for \((\alpha,\beta)\), we need to look at the joint distribution of \((\hat{\alpha},\hat{\beta})\). The \(k\times k\)_identity matrix_ is denoted by \(I_{k}\).

**Theorem 6.2** (Asymptotic Joint Distribution of Regression Coefficients): _Consider the linear regression model (6.12) with i.i.d. errors \(\varepsilon_{j}\) satisfying \(E\varepsilon_{j}=0\), \(0<E\varepsilon_{j}^{2}\equiv\sigma^{2}<\infty\). If the quantity \(\delta_{n}\) in (6.72) goes to zero, then \((\hat{\alpha},\hat{\beta})\) is asymptotically Normal, i.e.,_

\[(U_{n},V_{n})^{\prime}\equiv\Gamma_{n}(a_{n}(\hat{\alpha}-\alpha),b_{n}(\hat{ \beta}-\beta))^{\prime}\,\stackrel{{\mathscr{L}}}{{\longrightarrow }}\,N\left(\binom{0}{0},\sigma^{2}I_{2}\right), \tag{6.91}\]_where \(\Gamma_{n}=((\gamma_{ii^{\prime}}))\) is the symmetric positive definite matrix satisfying_

\[\Gamma_{n}^{2}=\begin{bmatrix}1&\frac{-\overline{X}}{m_{2}^{1/2}}\\ \frac{-\overline{X}}{m_{2}^{1/2}}&1\end{bmatrix}^{-1}\equiv\frac{1}{1-\frac{ \overline{X}}{m_{2}}}\begin{bmatrix}1&\frac{\overline{X}}{m_{2}^{1/2}}\\ \frac{\overline{X}}{m_{2}^{1/2}}&1\end{bmatrix}. \tag{6.92}\]

Proof.: Note that the covariance matrix of \((a_{n}(\hat{\alpha}-\alpha),b_{n}(\hat{\beta}-\beta))^{\prime}\) is

\[\Sigma_{n}=\sigma^{2}\begin{bmatrix}1&\frac{-\overline{X}}{m_{2}^{1/2}}\\ \frac{-\overline{X}}{m_{2}^{1/2}}&1\end{bmatrix}, \tag{6.93}\]

so that the covariance matrix of the left side of (6.91) is \(\sigma^{2}I_{2}\), where \(I_{2}\) is the \(2\times 2\) identity matrix. Let \(a,b\) be arbitrary reals. We will show that \(aU_{n}+bV_{n}\xrightarrow{\mathscr{L}}N(0,\sigma^{2})\). For this write

\[aU_{n}+bV_{n}=\sum_{j=1}^{n}\zeta_{j,n},\quad\zeta_{j,n}:=(a\gamma_{11}+b \gamma_{21})\tilde{\varepsilon}_{j,n}+(a\gamma_{12}+b\gamma_{22})\varepsilon_ {j,n} \tag{6.94}\]

Then \(\zeta_{j,n}\) are independent, \(E\zeta_{j,n}=0\), and [see (6.74), (6.90)]

\[E\zeta_{j,n}^{2} \leq 2\sigma^{2}\left[\left(a\gamma_{11}+b\gamma_{21}\right)^{2} \left(2\delta_{n}+\frac{2}{n}\right)+\left(a\gamma_{12}+b\gamma_{22}\right)^{2 }\delta_{n}\right]\] \[\leq 2\sigma^{2}\left(2\delta_{n}+\frac{2}{n}\right)\left|\Gamma _{n}\binom{a}{b}\right|^{2}\longrightarrow 0\,\text{ as }n\rightarrow\infty. \tag{6.95}\]

To prove the convergence to \(0\) in (6.95), check that

\[\gamma_{11} =\gamma_{22}=\frac{1}{2\sqrt{1-\frac{\overline{X}}{m_{2}}}} \left[\sqrt{1+\frac{\overline{X}}{m_{2}^{1/2}}}+\sqrt{1-\frac{\overline{X}}{m _{2}^{1/2}}}\right],\] \[\gamma_{12} =\gamma_{21}=\frac{\overline{X}}{2\sqrt{1-\frac{\overline{X}}{m_ {2}}}}\left[\sqrt{1+\frac{\overline{X}}{m_{2}^{1/2}}}-\sqrt{1-\frac{\overline{ X}}{m_{2}^{1/2}}}\right]. \tag{6.96}\]

Now it is easy to verify (6.95). Also, \(\sum_{j=1}^{n}E\zeta_{j,n}^{2}=\sigma^{2}(a^{2}+b^{2})\), using the fact that the covariance matrix of \((U_{n},V_{n})^{\prime}\) is \(\Gamma_{n}\Sigma_{n}I_{n}=\sigma^{2}I_{2}\). The proof of \(\sum_{j=1}^{n}E\zeta_{j,n}^{2}\mathbf{1}_{\{\zeta_{j,n}^{2}\succ\eta^{2}\}}\to 0\), as \(n\rightarrow\infty\), for every \(\eta>0\), now follows exactly as in (6.76), or (6.83). Thus for all \((a,b)\), \(aU_{n}+bV_{n}\xrightarrow{\mathscr{L}}N(0,(a^{2}+b^{2})\sigma^{2})\). By the so-called Cramer-Wold device,8 it now follows that \((U_{n},V_{n})^{\prime}\xrightarrow{\mathscr{L}}N\left(\binom{0}{0},\sigma^{2 }I_{2}\right)\). 

Footnote 8: See Bhattacharya and Waymire (2007, p. 105) or Billingsley (1986, p. 397).

It follows from Theorem 6.2 that, if (6.74) holds, _a confidence region for \((\alpha,\beta)\), with asymptotic confidence coefficient \(1-\theta\), is given by the ellipse_ (Exercise 6.21).

\[C_{n}:= \Bigg{\{}(\alpha,\beta):p(a_{n}(\hat{\alpha}-\alpha))^{2}\] \[+q(b_{n}(\hat{\beta}-\beta))^{2}+\gamma a_{n}b_{n}(\hat{\alpha}- \alpha)(\hat{\beta}-\beta)\leq\hat{\sigma}^{2}\chi_{1-\theta}^{2}(2)\Bigg{\}}, \tag{6.97}\]where

\[p=\frac{1}{1-\frac{\overline{X}^{2}}{m_{2}}}=\frac{m_{2}}{\frac{b_{n}^{2}}{n}}=q, \quad r=\frac{2\frac{\overline{X}}{m_{2}^{1/2}}}{1-\frac{\overline{X}^{2}}{m_{2 }}}\,, \tag{6.98}\]

and \(\chi_{\alpha}^{2}(2)\) is the \(\alpha\)-th quantile of the chi square distribution with 2 degrees of freedom.

_Remark 6.12_.: In Sect. 6.9, Theorem 6.2 is extended to the case of multiple linear regression, allowing more than one explanatory real-valued variable \(X\). It is also shown there in particular that the condition \(\delta_{n}\to 0\) in (6.72) is also _necessary_ from the asymptotic Normality of \((\hat{\alpha},\hat{\beta})\).

_Remark 6.13_ (Parametric Linear Regression with Normal Errors).: Assume that \(\varepsilon_{j}\)'s in (6.12) are i.i.d. \(N(0,\sigma^{2})\). Then the least squares estimators \(\hat{\alpha}\), \(\hat{\beta}\) above are also the maximum likelihood estimators of \(\alpha,\beta\). It follows, from the fact that \(\hat{\alpha}-\alpha\), \(\hat{\beta}-\beta\) are linear combinations of \(\varepsilon_{j}\)'s, that they are Normal, (individually and jointly, with zero means and variances given by (6.15), (6.16), and covariance \(-\sigma^{2}\overline{X}/b_{n}^{2}\). Note that this is _true for all \(n\geq 3\)_, assuming \(X_{j}\)'s (\(1\leq j\leq n\)) are not all the same:

\[\left(\hat{\alpha}-\alpha,\hat{\beta}-\beta\right)^{\prime}\,\stackrel{{ \varnothing}}{{=}}\,N\left(\begin{pmatrix}0\\ 0\end{pmatrix},\,\sigma^{2}\left[\begin{matrix}\frac{1}{n}+\frac{\overline{X}^ {2}}{b_{n}^{2}}-\frac{\overline{X}}{b_{n}^{2}}\\ -\frac{\overline{X}}{b_{n}^{2}}&b_{n}^{-2}\end{matrix}\right]\right) \tag{6.99}\]

From the theory of Linear Models in Part I, Sect. 5.9, it is known that \(\hat{\sigma}^{2}\) is independent of \((\hat{\alpha},\hat{\beta})\), and that the left side of (6.91) has the Normal distribution on the right side for all \(n\geq 3\). In particular, the quadratic form appearing on the left of the inequality within curly brackets in (6.97) is \(\sigma^{2}\)-times chi-square random variable with degrees of freedom 2, and

\[\widetilde{D}_{n}:=\left\{(\alpha,\beta):n\left[\hat{\alpha}-\alpha+(\hat{ \beta}-\beta)\overline{X}\right]^{2}+b_{n}^{2}(\hat{\beta}-\beta)^{2}\leq 2 \hat{\sigma}^{2}F_{1-\theta}(2,n-2)\right\} \tag{6.100}\]

is a confidence region for \((\alpha,\beta)\) of _exact_ confidence coefficient \(1-\theta\), for all \(n\geq 3\) (assuming \(X_{j}\), \(1\leq j\leq n\), are not all the same) (Exercise 6.22). Here \(F_{\alpha}(r,s)\) is the \(\alpha\)-th quantile of the \(F\)-distribution with numerator d.f. \(r\) and denominator d.f. \(s\).

_Example 6.7_ (_A Heteroscedastic Linear Regression Model with Known Error Variances).: Consider the linear regression

\[Y_{j}:=\alpha+\beta X_{j}+\varepsilon_{j}\quad(1\leq j\leq n),\]

\(\varepsilon_{j}\)'s are independent,

\[E\varepsilon_{j}=0,\quad 0<\sigma_{j}^{2}=E\varepsilon_{j}^{2}<\infty. \tag{6.101}\]

Assume that \(\sigma_{j}^{2}\) are known (\(1\leq j\leq n\)). If one also assumes that \(\varepsilon_{j}\)'s are Normal \(N(0,\sigma_{j}^{2})\), then the M.L.E.'s \(\hat{\alpha}\), \(\hat{\beta}\) are the solutions of

\[\sum_{j=1}^{n}\frac{1}{\sigma_{j}^{2}}(Y_{j}-\alpha-\beta X_{j})=0,\quad\sum_{ j=1}^{n}\frac{1}{\sigma_{j}^{2}}\,X_{j}(Y_{j}-\alpha-\beta X_{j})=0. \tag{6.102}\]Rewrite these as

\[\sum_{j=1}^{n}\frac{1}{\sigma_{j}^{2}}\left(Y_{j}-\{\alpha+\beta \overline{X}_{\omega}\}-\beta\{X_{j}-\overline{X}_{\omega}\}\right)=0,\] \[\sum_{j=1}^{n}\frac{1}{\sigma_{j}^{2}}X_{j}\left(Y_{j}-\{\alpha- \beta\overline{X}_{\omega}\}-\beta\{X_{j}-\overline{X}_{\omega}\}\right)=0, \tag{6.103}\]

where \(\overline{X}_{\omega}\) is the weighted mean of \(X_{j}\) (\(1\leq j\leq n\)),

\[\overline{X}_{\omega}=\frac{\sum_{j=1}^{n}\frac{1}{\sigma_{j}^{2}}X_{j}}{\sum_ {j=1}^{n}\frac{1}{\sigma_{j}^{2}}}. \tag{6.104}\]

Writing \(\alpha+\beta\overline{X}_{\omega}=\delta\), one obtains

\[\hat{\delta}=\overline{Y}_{\omega}\equiv\frac{\sum_{j=1}^{n}\frac{Y_{j}}{ \sigma_{j}^{2}}}{\sum_{j=1}^{n}\frac{1}{\sigma_{j}^{2}}}\,,\quad\hat{\beta}= \frac{\sum_{1}^{n}(Y_{j}-\overline{Y}_{\omega})X_{j}/\sigma_{j}^{2}}{\sum_{1} ^{n}(X_{j}-\overline{X}_{\omega})X_{j}/\sigma_{j}^{2}}\,, \tag{6.105}\]

\[\hat{\alpha}=\hat{\delta}-\hat{\beta}\overline{X}_{\omega}=\overline{Y}_{ \omega}-\hat{\beta}\overline{X}_{\omega}. \tag{6.106}\]

Note that

\[\hat{\beta} =\frac{\sum_{1}^{n}Y_{j}(X_{j}-\overline{X}_{\omega})/\sigma_{j} ^{2}}{\sum_{1}^{n}X_{j}(X_{j}-\overline{X}_{\omega})/\sigma_{j}^{2}}=\beta+ \sum_{j=1}^{n}\frac{(X_{j}-\overline{X}_{\omega})/\sigma_{j}^{2}}{\sum_{1}^{n }X_{j}(\overline{X}_{j}-\overline{X}_{\omega})/\sigma_{j}^{2}}\,\varepsilon_{j},\] \[\hat{\delta} =\alpha+\beta\overline{X}_{\omega}+\sum_{j=1}^{n}\frac{1/\sigma_ {j}^{2}}{\sum_{j=1}^{n}1/\sigma_{j}^{2}}\,\varepsilon_{j}\] \[=\delta+\sum_{j=1}^{n}\frac{1/\sigma_{j}^{2}}{\sum_{j=1}^{n}1/ \sigma_{j}^{2}}\,\varepsilon_{j}. \tag{6.107}\]

Thus, for Normal \(\varepsilon_{j}\)'s, \((\hat{\delta},\hat{\beta})^{\prime}\) is Normal \(N\left(\binom{\delta}{\beta},\widetilde{\Sigma}\right)\), where \(\widetilde{\Sigma}=((\tilde{\sigma}_{ii^{\prime}}))\) is given by

\[\sigma_{\hat{\delta}}^{2}=\tilde{\sigma}_{11}=\left(\sum_{j=1}^{ n}\frac{1}{\sigma_{j}^{2}}\right)^{-1},\quad\sigma_{\hat{\beta}}^{2}=\tilde{ \sigma}_{22}=\left(\sum_{1}^{n}(X_{j}-\overline{X}_{\omega})^{2}/\sigma_{j}^{ 2}\right)^{-1},\] \[\tilde{\sigma}_{12}=\tilde{\sigma}_{21}=0. \tag{6.108}\]

Also, \((\hat{\alpha},\hat{\beta})^{\prime}\) is Normal \(N\left(\binom{\alpha}{\beta},\Sigma\right)\) with \(\Sigma=((\sigma_{ii^{\prime}}))\), given by

\[\sigma_{11}=\tilde{\sigma}_{11}+\overline{X}_{\omega}^{2}\tilde{\sigma}_{22}, \quad\sigma_{22}=\tilde{\sigma}_{22},\quad\sigma_{12}=\sigma_{21}=-\overline{ X}_{\omega}\tilde{\sigma}_{22}. \tag{6.109}\]

Consider next the general case (6.101), \(\varepsilon_{j}\)'s not necessarily Normal, but \(\sigma_{j}^{2}\) (\(1\leq j\leq n\)) are known. Writing the _weighted least squares estimators_ as

\[\hat{\delta}=\delta+\sum_{1}^{n}\omega_{j}\varepsilon_{j},\quad\hat{\beta}= \beta+\sum_{1}^{n}\gamma_{j}\varepsilon_{j},\quad\omega_{j}=\frac{\frac{1}{ \sigma_{j}^{2}}}{\sum_{1}^{n}\frac{1}{\sigma_{j}^{2}}}\,. \tag{6.110}\]

one has\[\sigma_{\hat{\delta}}^{-1}(\hat{\delta}-\delta)=\sum_{1}^{n}\varepsilon_{j,n},\ \ \sigma_{\hat{\beta}}^{-1}(\hat{\beta}-\beta)=\sum_{1}^{n}\xi_{j,n},\] \[\varepsilon_{j,n}=\left\{\frac{\frac{1}{\sigma_{j}^{2}}}{\left( \sum_{1}^{n}\frac{1}{\sigma_{j}^{2}}\right)^{\frac{1}{2}}}\right\}\varepsilon_{j},\ \ \xi_{j,n}=\frac{\frac{(X_{j}-\overline{X}_{\omega})}{\sigma_{j}^{2}}}{\left[ \sum_{1}^{n}(X_{j}-\overline{X}_{\omega})^{2}/\sigma_{j}^{2}\right]^{\frac{1}{2}}}\, \varepsilon_{j}. \tag{6.111}\]

Note first that \(\varepsilon_{j,n}\) (\(1\leq j\leq n\)) are independent, and

\[E\varepsilon_{j,n}=0,\ \ \ \ E\varepsilon_{j,n}^{2}=(1/\sigma_{j}^{2})/\sum_{1}^{n}1/ \sigma_{j}^{2}=\omega_{j},\] \[\sum_{j=1}^{n}E\varepsilon_{j,n}^{2}=1. \tag{6.112}\]

If

(i) \(\varepsilon_{j}/\sigma_{j}\) (\(j=1,2,\dots\)) are uniformly integrable and

(ii) \(\theta_{n}:=\max\{\omega_{j}:1\leq j\leq n\}\longrightarrow 0\), (6.113)

then it is simple to check that the Lindeberg conditions hold for the (triangular) sequence \(\{\varepsilon_{j,n}:1\leq j\leq n,\,n\geq 1\}\) (Exercise 6.23). Hence, if (6.20) holds,

\[\sigma_{\hat{\delta}}^{-1}(\hat{\delta}-\delta)\,\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\,N(0,1). \tag{6.114}\]

Similarly, if

\[\tilde{\delta}_{n}:=\frac{\max\left\{(X_{j}-\overline{X}_{\omega})^{2}/\sigma_ {j}^{2}:1\leq j\leq n\right\}}{\sum_{1}^{n}(X_{j}-\overline{X}_{\omega})^{2}/ \sigma_{j}^{2}}\longrightarrow 0, \tag{6.115}\]

then one can show that (Exercise 6.23)

\[\sigma_{\hat{\beta}}^{-1}(\hat{\beta}-\beta)\,\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\,N(0,1). \tag{6.116}\]

More generally, if (6.20), (6.115) hold, then (Exercise 6.23)

\[\begin{pmatrix}\sigma_{\hat{\delta}}^{-1}(\hat{\delta}-\delta)\\ \sigma_{\hat{\beta}}^{-1}(\hat{\beta}-\beta)\end{pmatrix}\,\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\,N\left(\begin{pmatrix}0\\ 0\end{pmatrix},I_{2}\right). \tag{6.117}\]

Just as argued in Remark 6.12 in the case of homoscedastic linear regression (6.115), for the present heteroscedastic case, a confidence region for \((\alpha,\beta)\) with asymptotic confidence coefficient \(1-\theta\) is given by the ellipse (Exercise 6.23)

\[\widetilde{D}_{n}:=\left\{(\alpha,\beta):(\sigma_{\hat{\delta}}^{2})^{-1}\{( \hat{\alpha}-\alpha)+(\hat{\beta}-\beta)\overline{X}_{\omega}\}^{2}+\left( \sigma_{\hat{\beta}}^{2}\right)^{-1}(\hat{\beta}-\beta)^{2}\leq\chi_{1-\theta }^{2}(2)\right\}. \tag{6.118}\]

For an application, consider the commonly arising problem9 of estimating the postulated relationship \(D=\alpha+\beta T^{-1}\) between the diffusion coefficient \(D\) of a substance in a given medium and the temperature \(T\) of the medium (Ito and Ganguly 2006). Under isothermal conditions at each of temperatures \(T_{1},\dots,T_{n}\)the diffusion coefficient \(D\) is estimated in \(n\) independent experiments, perhaps by observing concentration profiles. These experiments also yield estimates of the standard deviations \(\sigma_{j}\) of the estimates \(y_{j}\) of \(D_{j}\) (\(1\leq j\leq n\)). Writing \(x=T^{-1}\), one then has the model (6.101).

_Remark 6.14_.: Suppose that \(\sigma_{j}^{2}\) in (6.101), is known up to a positive scalar, i.e., \(\sigma_{j}^{2}=\eta_{j}^{2}\sigma^{2}\), where \(\eta_{j}^{2}\) is known (\(1\leq j\leq n\)), but \(\sigma^{2}>0\) is not. For example, one may postulate that \(\sigma_{j}^{2}\) is proportional to \(X_{j}\) (for a positive explanatory variable), in which case \(\sigma_{j}^{2}=X_{j}\sigma^{2}(\eta_{j}^{2}=X_{j})\). If the \(\varepsilon_{j}\)'s are also Normal, then the Eqs. (6.102)-(6.107), (6.110), all hold if \(\sigma_{j}^{2}\)_is replaced by_\(\eta_{j}^{2}\). For the variances and covariances in (6.108), (6.109), a multiplier \(\sigma^{2}\) is to be used to each of \(\tilde{\sigma}_{ii^{\prime}}\), \(\sigma_{ii^{\prime}}\). If \(\varepsilon_{j}\)'s are not necessarily Normal, the arguments leading to (6.12), (6.15), (6.117) remain the same. However, for statistical inference using the CLT one needs to have a consistent estimator of \(\sigma^{2}\). This is given by

\[\hat{\sigma}^{2}=\frac{1}{n-2}\sum_{j=1}^{n}(Y_{j}-\hat{\alpha}-\hat{\beta}X_{ j})^{2}/\eta_{j}^{2}. \tag{6.119}\]

First note that \(\hat{\sigma}^{2}\) is _unbiased_ (Exercise 6.24). Next

\[\hat{\sigma}^{2} = \frac{1}{n-2}\sum_{j=1}^{n}\frac{1}{\eta_{j}^{2}}\left[\hat{\delta }-\delta+(\hat{\beta}-\beta)(X_{j}-\overline{X}_{\omega})-\varepsilon_{j} \right]^{2}\] \[= \frac{1}{n-2}\left(\sum_{j=1}^{n}\frac{1}{\eta_{j}^{2}}\right)( \hat{\delta}-\delta)^{2}+\frac{1}{n-2}(\hat{\beta}-\beta)^{2}\sum_{1}^{n}(X_{j }-\overline{X}_{\omega})^{2}/\eta_{j}^{2}\] \[+\frac{1}{n-2}\sum_{j=1}^{n}\varepsilon_{j}^{2}/\eta_{j}^{2}- \frac{2}{n-2}(\hat{\delta}-\delta)\sum_{j=1}^{n}\varepsilon_{j}/\eta_{j}^{2}\] \[-\frac{2}{n-2}(\hat{\beta}-\beta)\sum_{j=1}^{n}\varepsilon_{j}(X _{j}-\overline{X}_{\omega})/\eta_{j}^{2}-\frac{1}{n-2}(\hat{\delta}-\delta)( \hat{\beta}-\beta)\sum_{j=1}^{n}(X_{j}-\overline{X}_{\omega})/\eta_{j}^{2}\] \[= \frac{\sigma^{2}}{n-2}\sigma_{\delta}^{-2}(\hat{\delta}-\delta)^{ 2}+\frac{\sigma^{2}}{n-2}\sigma_{\hat{\beta}}^{-2}(\hat{\beta}-\beta)^{2}+ \frac{1}{n-2}\sum_{j=1}^{n}\varepsilon_{j}^{2}/\eta_{j}^{2}\] \[-\frac{2\sigma^{2}}{n-2}\,\sigma_{\delta}^{-2}(\hat{\delta}-\delta )^{2}-2\sigma^{2}\sigma_{\hat{\beta}}^{-2}\frac{(\hat{\beta}-\beta)^{2}}{n-2} -\frac{2\sigma^{2}}{n-2}\,\sigma_{\delta}^{-1}\sigma_{\hat{\beta}}^{1}(\hat{ \delta}-\delta)(\hat{\beta}-\beta).\]

Since \(\sigma_{\hat{\delta}}^{-1}(\hat{\delta}-\delta)\ \ \stackrel{{\mathscr{L}}}{{\longrightarrow}}\ N(0,1)\), \(\sigma_{\hat{\beta}}^{-1}(\hat{\beta}-\beta)\ \ \stackrel{{\mathscr{L}}}{{\longrightarrow}}N(0,1)\), these quantities are bounded in probability. Hence all the terms on the extreme right of (6.120) go to zero in probability, except for the term \(\frac{1}{n-2}\sum_{j=1}^{n}\varepsilon_{j}^{2}/\eta_{j}^{2}=J_{n}\), say. Now \(E\varepsilon_{j}^{2}/\eta_{j}^{2}=\sigma^{2}\). So if either \(\varepsilon_{j}/\eta_{j}\) are i.i.d. (as in the Normal case), or if their variances are bounded, i.e.,

\[\sup_{j}E\,(\varepsilon_{j}/\eta_{j})^{4}<\infty, \tag{6.121}\]

then \(J_{n}\) converges in probability to \(\sigma^{2}\). Hence \(\hat{\sigma}^{2}\) is an unbiased consistent estimator of \(\sigma^{2}\) if, in addition to (6.113) and (6.115), (6.121) holds. In particular, it then follows that a confidence region for \((\alpha,\beta)\) with asymptotic confidence coefficient \(1-\theta\) is given by the modified version of (6.118) obtained by replacing \(\sigma^{2}\) by \(\hat{\sigma}^{2}\) as given in (6.119), provided (6.113), (6.115), (6.121) hold.

The estimates in Example 6.7 are special cases of what are called _weighted least squares estimates_.

_Remark 6.15_.: If one simply assumes (6.101) and that the \(\varepsilon_{j}\)'s are uncorrelated (instead of being independent), then the OLS estimator (6.14) is unbiased with variance \(\sum[(X_{j}-\overline{X})^{2}\sigma_{j}^{2}/\sum(X_{j}-\overline{X})^{2}].\) If the latter goes to zero as \(n\to\infty,\) then the OLS is a consistent estimator of \(\beta.\)

_Remark 6.16_ (Confidence Region for Regression Lines).: Given a confidence region \(D\) for \((\alpha,\beta)\) with (asymptotic) confidence coefficient \(1-\theta,\) the set \(L\) of lines \(y=a+bx\) with \((a,b)\in D\) is a confidence region for the true regression line \(y=\alpha+\beta x\) with (asymptotic) confidence coefficient \(1-\theta.\) However, for a given \(x,\) the confidence bound for the regression is \(\hat{\alpha}+\hat{\beta}x\pm z_{1-\frac{\theta}{2}}SE(\hat{\alpha}+\hat{\beta}x),\) where the _standard error_\(SE(\hat{\alpha}+\hat{\beta}x)\) is obtained from (6.99) with \(\sigma^{2}\) replaced by \(\hat{\sigma}^{2}\) [see (6.85)]. By a _confidence band_ (with confidence level \(1-\theta\)) one usually means the regions between the upper and lower curves constructed as above (for all \(x\)).

The asymptotic probability that this confidence band contains the entire regression line is in general _smaller_ than \(1-\theta.\) On the other hand, an application of _Scheffe's method of multiple comparison10_ provides a confidence band containing the regression line with a probability at least \(1-\theta.\) Scheffe's method is based on the simple fact that for a random vector \(U\) and a constant \(c>0,\) the events \(\{\|\mathbf{U}\|^{2}\leq c^{2}\}\) and \(\{\langle\mathbf{U},\boldsymbol{\gamma}\rangle^{2}\leq\|\boldsymbol{\gamma}\|^ {2}c^{2}\ \forall\) vectors \(\boldsymbol{\gamma}\}\) are equivalent and, therefore, have the same probability.

Footnote 10: H. Scheffe (1959).

**Proposition 6.9** (Simultaneous Confidence Region for Regression).: _Under the hypothesis of Theorem 6.2, the asymptotic probability that_

\[|\hat{\alpha}+\hat{\beta}x-(\alpha+\beta x)|\leq\frac{\hat{\sigma}}{b_{n}} \left[m_{2}-2x\overline{X}+x^{2}\right]^{\frac{1}{2}}\sqrt{\chi_{1-\theta}^{2 }(2)}\quad\text{for all }x \tag{6.122}\]

_is at least \(1-\theta\)._

Proof.: By (6.93) or (6.16), the covariance matrix of \((\hat{\alpha},\hat{\beta})\) is

\[V_{n}=\frac{\sigma^{2}}{b_{n}^{2}}\begin{bmatrix}m_{2}&-\overline{X}\\ -\overline{X}&1\end{bmatrix}\,,\]

and, by Theorem 6.2, \(Q\equiv(\hat{\alpha}-\alpha,\hat{\beta}-\beta)V_{n}^{-1}(\hat{\alpha}-\alpha, \hat{\beta}-\beta)^{\prime}\) converges in distribution to the chi-square distribution with two degrees of freedom, as \(n\to\infty.\) Now \(Q=\|\mathbf{U}\|^{2}\) where \(\mathbf{U}=V_{n}^{-1/2}(\hat{\alpha}-\alpha,\hat{\beta}-\beta)^{\prime},\)\(V_{n}^{1/2}\) being the symmetric positive definite matrix such that \(V_{n}^{1/2}V_{n}^{1/2}=V_{n}\) and \(V_{n}^{-1/2}=(V_{n}^{1/2})^{-1}.\) One has \(P(\|\mathbf{U}\|^{2}\leq\chi_{1-\theta}^{2}(2))\longrightarrow 1-\theta\) as \(n\to\infty.\) Now \(\{\|\mathbf{U}\|^{2}\leq\chi_{1-\theta}^{2}(2)\}=\{\langle\mathbf{U},V_{n}^{1/ 2}\boldsymbol{\gamma}\rangle^{2}\leq\|V_{n}^{1/2}\boldsymbol{\gamma}\|^{2}\chi _{1-\theta}^{2}(2)\ \forall\ \boldsymbol{\gamma}\in\mathbb{R}^{2}\}=\{\langle(\hat{ \alpha}-\alpha,\hat{\beta}-\beta)^{\prime},\boldsymbol{\gamma}\rangle^{2}\leq \langle\boldsymbol{\gamma},V_{n}\boldsymbol{\gamma}\rangle\chi_{1-\theta}^{2}(2)\ \forall\ \boldsymbol{\gamma}\in\mathbb{R}^{2}\}.\) In particular taking \(\boldsymbol{\gamma}=(1,x)^{\prime},\) one gets \(\{\|\mathbf{U}\|^{2}\leq\chi_{1-\theta}^{2}(2)\}\subset\{|(\hat{\alpha}+\hat{ \beta}x-(\alpha+\beta x)|^{2}\leq\frac{\sigma^{2}}{b_{n}^{2}}[m_{2}-2n \overline{X}+x^{2}]\chi_{1-\theta}^{2}(2)\forall\ x\}.\) Hence the last event has asymptotic probability at least \(1-\theta.\) Replacing \(\sigma^{2}\) by \(\hat{\sigma}^{2}\) one obtains the desired result.

Often the explanatory variable \(X\) is random. In the so-called _correlation model_ one assumes that, in (6.12), \(X_{j}\)'s are i.i.d. with finite positive variance (\(j\geq 1\)) and that they are independent of the error sequence \(\{\varepsilon_{j}:j\geq 1\}\), the latter being i.i.d. with mean zero and variance \(\sigma^{2}>0\) as assumed above. One may then apply the above arguments verbatim, _conditionally given \(X_{j}\)'s_ (\(1\leq j\leq n\)), noting that in this case (6.72) holds with probability one. Hence (6.91) holds, conditionally given \(X_{j}\)'s (\(j\geq 1\)). Since the _nonrandom_ limit does not depend on the \(X_{j}\)'s it follows that (6.91) holds unconditionally as well. We state this as a corollary to Theorem 6.2. (Exercise 6.25).

**Corollary 6.3** (The Correlation Model): _Suppose that in (6.12), \(X_{j}\)'s are also i.i.d., independent of the i.i.d. \(\varepsilon_{j}\)'s, and that \(0<\operatorname{var}X_{j}<\infty\). Then (6.91) holds._

As a final remark on convergence in distribution, we state the following useful fact.11

Footnote 11: See Bhattacharya and Waymire (2007, Theorem 5.2, p. 62).

_Remark 6.17_.: Suppose \(P_{n}\) (\(n\geq 1\)), \(P\) are probability measures on \((\mathbb{R},\mathscr{B}(\mathbb{R}))\) such that \(P_{n}\) converges weakly to \(P\). If \(f\) is a bounded measurable function whose points of discontinuity comprise a set \(D\) with \(P(D)=0\), then \(\int fdP_{n}\to\int fdP\).

### Asymptotic Distribution of Sample Quantiles, Order Statistics

Throughout this section it will be assumed that \(X_{1},X_{2},\dots\) is an i.i.d. sequence of random variables whose common _distribution function_\(F\)_is continuous on_\((-\infty,\infty)\). For each fixed \(n\), let \(X_{(1)}<X_{(2)}<\dots<X_{(n)}\) be an ordering of \(X_{1},X_{2},\dots,X_{n}\). Note that \(\operatorname{Prob}(X_{i}=X_{j})=\int_{-\infty}^{\infty}\operatorname{Prob}(X _{i}=x/X_{j}=x)dF(x)=\int_{-\infty}^{\infty}\operatorname{Prob}(X_{i}=x)dF(x) =\int_{-\infty}^{\infty}\operatorname{0}dF(x)=0\), for each pair \((i,j)\) with \(i\neq j\). Hence one may assume _strict_ ordering among \(X_{1},\dots,X_{n}\). Observe also that one should write \(X_{(1):n}<X_{(2):n}<\dots<X_{(n):n}\), in order to emphasize that \(X_{(i)}\), for any \(i\) (\(1\leq i\leq n\)), depends on \(n\). However, we will write \(X_{(1)},\dots,X_{(n)}\) for the \(n\)_order statistics_ to simplify notations. With this notation the distribution function of the \(r\)th _order statistic_ is easily seen to be

\[\operatorname{Prob}(X_{(r)}\leq x) =\operatorname{Prob}(\text{At least $r$ of the $n$ random variables $X_{1},\dots,X_{n}$ are $\leq x$})\] \[=\sum_{j=r}^{n}\operatorname{Prob}(\text{Exactly $j$ of the random variables are $\leq x$})\] \[=\sum_{j=r}^{n}\binom{n}{j}(F(x))^{j}(1-F(x))^{n-j},\qquad[1\leq r \leq n]. \tag{6.123}\]

Fix \(0<p<1\). Define the \(p\)th _quantile_\(\xi_{p}\) of \(F\) as the solution of

\[F(x)=p \tag{6.124}\]

if this solution is unique; else let a \(p\)th quantile be any solution of (6.124). By the sample \(p\)th quantile \(\hat{\zeta}_{p}\) we shall mean the order statistic \(X_{([np])}\). (Once again, a more appropriate notation would be \(\hat{\zeta}_{p:n}\)).

**Theorem 6.3**.: _(a) Fix \(p\), \(0<p<1\). Assume that the solution \(\zeta_{p}\) of (6.124) is unique, \(F\) is continuously differentiable in a neighborhood of \(\zeta_{p}\) and \(F^{\prime}(\zeta_{p})=f(\zeta_{p})>0\). Then_

\[\sqrt{n}(\hat{\zeta}_{p}-\zeta_{p})\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N\left(0,\,\frac{p(1-p)}{f^{2}(\zeta_{p})}\right)\quad\text{ as }n\to\infty. \tag{6.125}\]

_(b) Let \(0<p_{1}<p_{2}<\cdots<p_{k}\), \(k>1\), and assume that the hypothesis of (a) holds for \(p=p_{i}\) for all \(i=1,2,\ldots,k\). Then_

\[\sqrt{n}\left(\hat{\xi}_{p_{1}}-\xi_{p_{1}},\hat{\xi}_{p_{2}}-\xi_{p_{2}}, \ldots,\hat{\xi}_{p_{k}}-\xi_{p_{k}}\right)\stackrel{{\mathscr{L }}}{{\longrightarrow}}\,N(\mathbf{0},\varSigma), \tag{6.126}\]

_where \(\varSigma=((\sigma_{ij}))\), \(\sigma_{ij}=p_{i}(1-p_{j})/f(\xi_{p_{i}})f(\xi_{p_{j}})\) for \(1\leq i\leq j<k\)._

Proof.: (a) Fix \(z\in(-\infty,\infty)\). Then, writing \(\mathbf{1}_{A}\) for the _indicator_ of \(A\),

\[\text{Prob}\left(\sqrt{n}(\hat{\zeta}_{p}-\zeta_{p})\leq z\right)\] \[= \text{Prob}\left(\hat{\zeta}_{p}\leq\zeta_{p}+\frac{z}{\sqrt{n}}\right)\] \[= \text{Prob}\left(\text{\# of observations among }\,X_{1},\ldots,X_{n}\,\text{which are}\leq\zeta_{p}+\frac{z}{\sqrt{n}}\,\,\,\,\text{is}\,\,\,\geq[np]\right)\] \[= \text{Prob}\left(\sum_{j=1}^{n}\mathbf{1}_{\{X_{j}\leq\zeta_{p}+z /\sqrt{n}\}}\geq[np]\right)\] \[= \text{Prob}\left[\frac{1}{\sqrt{n}}\left(\sum_{j=1}^{n}\mathbf{1} _{\{X_{j}\leq\zeta_{p}+\frac{z}{\sqrt{n}}\}}-nF\left(\zeta_{p}+\frac{z}{\sqrt {n}}\right)\right)\right.\] \[\geq \frac{1}{\sqrt{n}}\left([np]-nF\left(\zeta_{p}+\frac{z}{\sqrt{n}} \right)\right)\right]=P(Z_{n}\geq C_{n}), \tag{6.127}\]

where \(Z_{n}=\frac{1}{\sqrt{n}}(\sum_{j=1}^{n}\mathbf{1}_{\{X_{j}\leq\xi_{p}+z/\sqrt{ n}\}}-nF(\xi_{p}+z/\sqrt{n}))\), and \(c_{n}=\frac{1}{\sqrt{n}}([np]-nF(\xi_{p}+z/\sqrt{n}))\). Let

\[W_{n} =n^{\frac{1}{2}}(F_{n}(\xi_{p})-F(\xi_{p}))\] \[=\frac{1}{\sqrt{n}}\sum_{j=1}^{n}\left(\mathbf{1}_{\{X_{j}\leq\xi _{p}\}}-F(\xi_{p})\right). \tag{6.128}\]

Then \(EZ_{n}=0\), \(EW_{n}=0\), and

\[\text{var}(Z_{n}-W_{n}) = \text{var}\left[\frac{1}{\sqrt{n}}\sum_{j=1}^{n}\left(\mathbf{1}_ {\{\xi_{p}<X_{j}\leq\xi_{p}+z/\sqrt{n}\}}\right)\right] \tag{6.129}\] \[=\] \[\to 0\,\text{as }n\to\infty.\]

Therefore \(Z_{n}-W_{n}\) converges in probability to zero. By the classical CLT, \(W_{n}\) converges in distribution to \(N(0,p(1-p))\). By (6.129), \(Z_{n}\) converges in distribution to the same limit. Also, \(c_{n}\to-zf(\zeta_{p})\). Hence (6.127) yields

\[\lim_{n\to\infty}\text{Prob}(\sqrt{n}(\hat{\zeta}_{p}-\zeta_{p}) \leq z)=\text{Prob}(W\geq-zf(\zeta_{p}))\] \[= \text{Prob}(W\leq zf(\zeta_{p}))=\text{Prob}(W/f(\zeta_{p})\leq z),\]

where \(W\) has the Normal distribution \(N(0,p(1-p))\). This proves (6.125).

2. From the argument above applied to \(p=p_{i}\) (\(1\leq i\leq k\)), it follows that \[{\bf Z}_{n}-\tilde{W}_{n}=(Z_{n}^{(1)}-W_{n}^{(1)},\ldots,Z_{n}^{(k)}-W_{n}^{(k)}) \mbox{ converges in probability to }{\bf 0},\] (6.130) where \(Z_{n}^{(i)}\) and \(W_{n}^{(i)}\) are the same as \(Z_{n}\) and \(W_{n}\), but with \(p=p_{i}\), and \({\bf Z}_{n}=(Z_{n}^{(1)},\ldots,Z_{n}^{(k)})\), \({\bf W}_{n}=(W_{n}^{(1)},\ldots,W_{n}^{(k)})\). Now \[{\bf W}_{n} = \frac{1}{\sqrt{n}}\sum_{j=1}^{n}\Big{(}{\bf 1}_{\{X_{j}\leq\zeta_{p_{ 1}}\}}-F(\xi_{p_{1}}),\ldots,{\bf 1}_{\{X_{j}\leq\zeta_{p_{k}}\}}-F(\zeta_{p_{k}}) \Big{)}\] (6.131) \[\stackrel{{\mathscr{L}}}{{\longrightarrow}}N({\bf 0},((p_{i}(1-p_ {j})))_{1\leq i\leq j\leq k}),\quad\mbox{as }n\to\infty,\] by the classical multivariate CLT. Writing \(c_{n}^{(i)}\) for \(c_{n}\) and \(z^{(i)}\) for \(z\) in (a), one has \(c_{j}^{(i)}\to-z^{(i)}f(\zeta_{p_{i}})\). Therefore, as in the proof of (a), \[{\rm Prob}(\sqrt{n}(\hat{\zeta}_{p_{1}}-\zeta_{p})\leq z^{(1)}, \ldots,\sqrt{n}(\hat{\zeta}_{p_{k}}-\zeta_{p_{k}})\leq z^{(k)})\] \[\longrightarrow{\rm Prob}(W^{(1)}\leq z^{(1)}f(\zeta_{p_{1}}), \ldots,W^{(k)}\leq z^{(k)}f(\zeta_{p_{k}}),\] where \({\bf W}=(W^{(1)},\ldots,W^{(k)})\) is \(N({\bf 0},((p_{i}(1-p_{j})))\). Let \(0<p_{1}<p_{2}<\cdots<p_{k}\), \(k>1,\)_and assume that the hypothesis of (a) holds for \(p=p_{i}\) for all \(i=1,2,\ldots,k\). Then_ \[\sqrt{n}\left(\hat{\zeta}_{p_{1}}-\zeta_{p_{1}},\hat{\zeta}_{p_{2}}-\zeta_{p_{ 2}},\ldots,\hat{\zeta}_{p_{k}}-\zeta_{p_{k}}\right)\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\ N({\bf 0},\Sigma),\] (6.132) where \(\Sigma=((\sigma_{ij}))\), \(\sigma_{ij}=p_{i}(1-p_{j})/f(\zeta_{p_{i}})f(\zeta_{p_{j}})\) for \(1\leq i\leq j<k\). \[{\rm Prob}(\sqrt{n}(\hat{\zeta}_{p_{1}}-\zeta_{p})\leq z^{(1)}, \ldots,\sqrt{n}(\hat{\zeta}_{p_{k}}-\zeta_{p_{k}})\leq z^{(k)})\] \[= {\rm Prob}(Z_{n}^{(1)}\geq c_{n}^{(1)},\ldots,Z_{n}^{(k)}\geq c_{ n}^{(k)})\] \[\longrightarrow{\rm Prob}(W^{(1)}\leq z^{(1)}f(\zeta_{p_{1}}), \ldots,W^{(k)}\leq z^{(k)}f(\zeta_{p_{k}})),\] where \({\bf W}=(W^{(1)},\ldots,W^{(k)})\) is \(N({\bf 0},((p_{i}(1-p_{j})))_{1\leq i\leq j\leq k})\).

\(\sqcap\)\(\sqcup\)

Precise asymptotic analysis of sample quantiles was spurred by the so-called _Bahadur representation_

\[\hat{\zeta}_{p}=\zeta_{p}+\frac{p-F_{n}(\zeta_{p})}{f(\zeta_{p})}+R_{n} \tag{6.133}\]

where \(R_{n}\to 0\) almost surely as \(n\to\infty\). This was derived by Bahadur (1966) under the additional assumption of twice differentiability of \(F\) at \(\zeta_{p}\), but with an estimation of the remainder term \(R_{n}=O(n^{-3/4}(\log n)^{1/2}(\log\log n)^{1/4})\) a.s. as \(n\to\infty\), and he suggested the problem of finding the precise rate of convergence of \(R_{n}\) to zero. Kiefer (1967) derived the precise rate given by

\[\overline{\lim}_{n\to\infty}\frac{n^{3/4}R_{n}}{(\log\log n)^{3/4}}=\frac{z^{3 /4}[p(1-p)]^{1/4}}{3^{3/4}} \tag{6.134}\]with probability one. Our proof follows Ghosh (1971), who proved the representation (6.133) with the remainder satisfying \(n^{\frac{1}{2}}R_{n}\to 0\) in probability with the help of a simple but useful lemma.

### Asymptotics of Semiparametric Multiple Regression

In this section, we extend the results of Sect. 6.6 to _multiple regression_ with \(p\) explanatory variables, \(p\geq 1\). This is one of the most widely applied models in statistics. Consider the semiparametric regression model

\[{\bf y}=X\mathbf{\theta}+\mathbf{\epsilon}\quad\left[\mbox{i.e., }\ y_{i}=\sum_{j=1}^{p}x_{ij}\theta_{j}+\varepsilon_{i}\quad(1\leq i\leq n) \right], \tag{6.135}\]

where \({\bf y}\) is the _observation vector,_\(X=((x_{ij}))_{1\leq i\leq n,\,1\leq j\leq p}\) (\(p\leq n\)) is a known _design matrix_ of full rank \(p\) and \(\varepsilon\) (\(1\leq i\leq n\)) are i.i.d. (unobserved) _errors_ satisfying

\[E\varepsilon_{i}=0,\qquad E\varepsilon_{i}^{2}=\sigma^{2}\qquad(0<\sigma^{2} <\infty), \tag{6.136}\]

(\(\sigma^{2}\) unknown) and \(\mathbf{\theta}=(\theta_{1},\theta_{2},\ldots,\theta_{p})^{\prime}\) is the unknown _parameter vector_ to be estimated. The _least squares estimator_ of \(\theta\) is \(\hat{\mathbf{\theta}}\) which minimizes

\[f(\mathbf{\theta}):=\sum_{i=1}^{n}\left(y_{i}-\sum_{j=1}^{p}x_{ij} \theta_{j}\right)^{2}. \tag{6.137}\]

Differentiating w.r.t. \(\theta_{k}\) one gets the _normal equations_\(\partial f(\mathbf{\theta})/\partial\theta_{k}=0\), or

\[\sum_{i=1}^{n}x_{ik}\underbrace{\sum_{j=1}^{p}x_{ij}\theta_{j}}_{(X\mathbf{\theta})_{i}}=\underbrace{\sum_{i=1}^{n}x_{ik}y_{i}}_{(X^{\prime} y)_{k}}\qquad(1\leq k\leq p),\]

or, treating the left side as the \(k\)-th element of the column vector \(X^{\prime}X\mathbf{\theta}\) one has

\[X^{\prime}X\mathbf{\theta}=X^{\prime}{\bf y}, \tag{6.138}\]

so that the minimum of \(f(\mathbf{\theta})\) is attained as

\[\hat{\mathbf{\theta}}=(X^{\prime}X)^{-1}X^{\prime}{\bf y}. \tag{6.139}\]

Substituting from (6.135), one has

\[\hat{\mathbf{\theta}}=\mathbf{\theta}+(X^{\prime}X)^{-1}X^{ \prime}\mathbf{\epsilon}, \tag{6.140}\]

and

\[\int E\hat{\mathbf{\theta}}=\mathbf{\theta},\qquad\mbox{ cov}\hat{\mathbf{\theta}}=(X^{\prime}X)^{-1}X^{\prime}\sigma^{2}I_{n}X(X^{ \prime}X)^{-1}=\sigma^{2}(X^{\prime}X)^{-1}, \tag{6.141}\]where \(I_{n}\) is the \(n\times n\) identity matrix. It is well known, and not difficult to prove, that \(\hat{\mathbf{\theta}}\) has the smallest expected square error \((E|\hat{\mathbf{\theta}}-\mathbf{\theta}|^{2}=\sigma^{2}\) trace \((X^{\prime}X)^{-1})\) in the class of all linear unbiased estimators of \(\mathbf{\theta}\) (i.e., among all estimators of the form \(\mathbf{\theta}^{*}=b+A\mathbf{y}\)). In fact this latter property holds even under the milder assumption on \(\mathbf{\epsilon}\): \(\varepsilon_{i}\) satisfy (6.136) and are uncorrelated. (See the Gauss-Markov Theorem in Part I, Sect. 5.9.) Write \((X^{\prime}X)^{1/2}\) as the positive definite symmetric matrix whose square is \(X^{\prime}X\).

If, in addition to the assumptions made above, \(\varepsilon_{i}\)'s are Normal \(N(0,\sigma^{2})\), then it follows from (6.140) that \(\hat{\mathbf{\theta}}\) is Normal \(N(\mathbf{\theta},\sigma^{2}(X^{\prime}X)^{-1}))\). For this classical linear model an optimal confidence region for \(\mathbf{\theta}\) is based on the \(F\)-statistic \(\{(\hat{\mathbf{\theta}}-\mathbf{\theta})^{\prime}X^{\prime}X(\hat{\mathbf{\theta}}-\mathbf{ \theta})/p\}/\hat{\sigma}^{2}\) where \(\hat{\sigma}^{2}=\|Y-X\hat{\mathbf{\theta}}\|^{2}/(n-p)\) (see Chap. 5, Example 5.20).

**Theorem 6.4**.: _In the model (6.135), assume (6.136) where the \(\varepsilon_{i}\)'s are i.i.d. and that the nonstochastic matrix \(X\) is of full rank \(p\). Assume also that \(\varepsilon_{i}\)'s are non-Normal. Then \((X^{\prime}X)^{1/2}(\hat{\mathbf{\theta}}-\mathbf{\theta})\) converges in distribution to \(N(0,\sigma^{2}I_{p})\) if and only if the maximum among the diagonal elements of the matrix \(X(X^{\prime}X)^{-1}X^{\prime}\) goes to zero as \(n\to\infty\)._

We first prove a lemma.

**Lemma 6.1**.: _The \(n\times n\) matrix \(H=X(X^{\prime}X)^{-1}X^{\prime}\) is symmetric and idempotent, and it has the eigenvalue 1 of multiplicity \(p\) and the eigenvalue \(0\) of multiplicity \(n-p\). Also, \(0\leq H_{ii}\leq 1\)\(\forall\)\(i\)._

Proof.: Symmetry and idempotence are easy to check. If \(\lambda\) is an eigenvalue with a corresponding eigenvector \(\mathbf{x}\), then \(Hx=\lambda\mathbf{x}\) and \(Hx=HH\mathbf{x}=H\lambda\mathbf{x}=\lambda H\mathbf{x}=\lambda^{2}\mathbf{x}\), implying \(\lambda=\lambda^{2}\), i.e. \(\lambda(1-\lambda)=0\), or, \(\lambda=0\) or \(1\). Clearly, rank of \(H\) is no more than \(p\) (since \(H\mathbf{x}=0\)\(\forall\)\(x\) orthogonal to the rows of \(X^{\prime}\), i.e., columns of \(X\)). On the other hand, rank of \(H\) is no less than the rank of \(HX=X\), which is \(p\). Hence the rank of \(H\) is \(p\). Note that \(H\) has \(p\) linearly independent row vectors and, therefore, exactly an \((n-p)\)-dimensional subspace orthogonal to them. That is, the null space of \(H\) is of dimension \(n-p\). Hence the multiplicity of the eigenvalue zero is \(n-p\). It follows that the eigenvalue \(1\) is of multiplicity \(p\). Since \(H_{ii}\leq\max\) eigenvalue of \(H\), and \(H_{ii}\geq\min\) eigenvalue of \(H\), one gets \(0\leq H_{ii}\leq 1\). 

Proof of Theorem.: Let \(\mathbf{a}\in\mathbb{R}^{p}\), \(\mathbf{a}\neq 0\). Then

\[\mathbf{a}^{\prime}\hat{\mathbf{\theta}}-\mathbf{a}^{\prime}\mathbf{\theta} =\mathbf{a}^{\prime}(X^{\prime}X)^{-1}X^{\prime}\mathbf{\epsilon}= \mathbf{\epsilon}^{\prime}X(X^{\prime}X)^{-1}\mathbf{a}\] \[\text{var}(\mathbf{a}^{\prime}\hat{\mathbf{\theta}}) =\mathbf{a}^{\prime}(X^{\prime}X)^{-1}X^{\prime}\sigma^{2}I_{n}X (X^{\prime}X)^{-1}\mathbf{a}\] \[=\sigma^{2}\mathbf{a}^{\prime}(X^{\prime}X)^{-1}\mathbf{a}= \gamma^{2},\quad\text{say}. \tag{6.142}\] \[\frac{\mathbf{a}^{\prime}\hat{\mathbf{\theta}}-\mathbf{a}^{\prime} \mathbf{\theta}}{\gamma} =\mathbf{\epsilon}^{\prime}X(X^{\prime}X)^{-\frac{1}{2}}\frac{\mathbf{ b}}{\sigma}=\sum_{1}^{n}\frac{s_{i}}{\sigma}\,\varepsilon_{i}\]

where

\[\mathbf{b}=\frac{(X^{\prime}X)^{-\frac{1}{2}}\mathbf{a}}{\|(X^{\prime}X)^{- \frac{1}{2}}\mathbf{a}\|}\quad\text{is a unit vector in $\mathbb{R}^{p}$},\]

and \(s_{i}\) is the \(i\)-th element of

\[X(X^{\prime}X)^{-\frac{1}{2}}\mathbf{b}=G\mathbf{b},\ \ \text{say},\ \ \ s_{i}=\sum_{k=1}^{p}g_{ik}b_{k}. \tag{6.143}\]Note that

\[s_{i}^{2} =\left(\sum_{k=1}^{p}g_{ik}b_{k}\right)^{2}\leq\sum_{k}g_{ik}^{2} \cdot\sum_{k}b_{k}^{2}=\sum_{k}g_{ik}^{2}=(GG^{\prime})_{ii}\] \[=\left[X(X^{\prime}X)^{-\frac{1}{2}}(X^{\prime}X)^{-\frac{1}{2}}X^ {\prime}\right]_{ii}=H_{ii}. \tag{6.144}\]

Now

\[\frac{\mathbf{a}^{\prime}\hat{\boldsymbol{\theta}}-\mathbf{a}^{\prime}\hat{ \boldsymbol{\theta}}}{\gamma}=\sum_{i=1}^{n}\frac{s_{i}}{\sigma}\,\varepsilon_ {i},\qquad E\left(\sum_{i=1}^{n}\frac{s_{i}}{\sigma}\,\varepsilon_{i}\right)^{ 2}=\frac{\text{var}(\mathbf{a}^{\prime}\hat{\boldsymbol{\theta}})}{\gamma^{2} }=1, \tag{6.145}\]

and, for every \(\delta>0\),

\[\sum_{i=1}^{n}E\left(\frac{s_{i}^{2}}{\sigma^{2}}\,\varepsilon_{ i}^{2}\mathbf{1}_{\{|\frac{\delta_{i}}{\sigma}\varepsilon_{i}|>\delta\}}\right) =\frac{1}{\sigma^{2}}\sum_{i=1}^{n}s_{i}^{2}E\varepsilon_{i}^{2} \mathbf{1}_{\{|s_{i}\varepsilon_{i}|>\delta\sigma\}}\] \[\leq\frac{1}{\sigma^{2}}\sum_{i=1}^{n}s_{i}^{2}E\varepsilon_{i}^{ 2}\mathbf{1}_{\{|\varepsilon_{i}|>\frac{\delta\sigma}{s}\}}, \tag{6.146}\]

where

\[s^{2}=\max_{1\leq i\leq n}s_{i}^{2}\leq\max_{i}H_{ii}\longrightarrow 0\qquad \text{by hypothesis}. \tag{6.147}\]

Thus

\[\sum_{i=1}^{n}E\left(\frac{s_{i}^{2}}{\sigma^{2}}\,\varepsilon_{i }^{2}\mathbf{1}_{\{|\frac{\delta_{i}}{\sigma}\,\varepsilon_{i}|>\delta\}}\right)\] \[\leq\frac{1}{\sigma^{2}}\sum_{i=1}^{n}s_{i}^{2}E\varepsilon_{1}^{ 2}\mathbf{1}_{\{|\varepsilon_{1}|>\frac{\delta\sigma}{s}\}}\] \[=\frac{\sum_{1}^{n}s_{i}^{2}}{\sigma^{2}}\,E\varepsilon_{1}^{2} \mathbf{1}_{\{|\varepsilon_{1}|>\frac{\delta\sigma}{s}\}} \tag{6.148}\]

Since [see (6.143)],

\[\sum_{1}^{n}s_{i}^{2} =\langle G\mathbf{b},G\mathbf{b}\rangle=\mathbf{b}^{\prime}G^{ \prime}G\mathbf{b}=\mathbf{b}^{\prime}\mathbf{b}=1,\] \[(G^{\prime}G=(X^{\prime}X)^{-\frac{1}{2}}X^{\prime}X(X^{\prime} X)^{-\frac{1}{2}}=I_{p})\]

one gets

\[\sum_{i=1}^{n}E\left(\frac{s_{i}^{2}}{\sigma^{2}}\,\varepsilon_{i }^{2}\mathbf{1}_{\{\frac{|\varepsilon_{i}\varepsilon_{i}|>\delta\}}}\right) \leq\frac{1}{\sigma^{2}}\,E\varepsilon_{1}^{2}\mathbf{1}_{\{|\varepsilon_{1}|> \frac{\delta\sigma}{s}\}}\longrightarrow 0\] \[\text{as }n\rightarrow\infty\quad\left(\text{since }\frac{ \delta\sigma}{s}\rightarrow\infty\right). \tag{6.149}\]

This proves the "sufficiency" part of the theorem, applying the Lindeberg-Feller CLT.

To prove the "necessary" part, choose \(\mathbf{b}\) (and, therefore, \(\mathbf{a}\)) such that (6.144) is an equality, i.e., \(b_{k}=g_{i_{0}k}\) for the _particular_\(i_{0}\) for which \(\max_{i^{\prime}}H_{i^{\prime}i^{\prime}}=H_{i_{0}i_{0}}\). Then, if \(H_{i_{0}i_{0}}:\not\to 0\), \(E\left(\frac{s_{i_{0}}^{2}}{\sigma^{2}}\,\varepsilon_{i_{0}}^{2}\right)=s_{i_{0 }}^{2}\not\to 0\) (as \(n\rightarrow\infty\)). This violates Feller's necessary condition for the Lindeberg-Feller CLT.

You may directly consider \(\sum_{i=1}^{n}\frac{s_{i}}{\sigma}\,\varepsilon_{i}=\frac{s_{i_{0}}}{\sigma}\, \varepsilon_{i_{0}}+\sum_{i\neq i_{0}}\frac{s_{i}}{\sigma}\,\varepsilon_{i}\). There exists a subsequence \(n^{\prime}\) of integers \(n=1,2,\dots\), such that as \(n^{\prime}\to\infty\), \(\frac{s_{i_{0}}}{\sigma}\to\alpha>0\), so that \(\frac{s_{i_{0}}}{\sigma}\,\varepsilon_{i_{0}}\stackrel{{\mathscr{ L}}}{{\longrightarrow}}\alpha\varepsilon_{1}\) (which is _non-Normal_). What-ever may be the limit of \(\sum_{i\neq i_{0}}\frac{s_{i}}{\sigma}\,\varepsilon_{i}\), the sum of two independent random variables, one of which is non-Normal, cannot be Normal (Exercise 6.30).

Remark Remark 6.18: If \(\varepsilon_{i}\)'s are i.i.d. \(N(0,\sigma^{2})\), then of course \(\hat{\boldsymbol{\theta}}=\boldsymbol{\theta}+(X^{\prime}X)^{-1}X^{\prime} \boldsymbol{\epsilon}\) is normal, no matter what \(X\) is (assuming full rank).

Corollary 6.4: Let

\[s_{ij} = \frac{1}{n}\sum_{k=1}^{n}x_{ki}x_{kj}\qquad(1\leq i,\,j\leq p),\] \[S = ((s_{ij})),\] \[\Lambda = \text{largest eigenvalue of }S^{-1}=\frac{1}{\lambda}\,. \tag{6.150}\]

where \(\lambda=\) smallest eigenvalue of \(S\). Thus \(\sqrt{n}\,\mathbf{S}^{\frac{1}{2}}(\hat{\boldsymbol{\theta}}-\boldsymbol{\theta })\stackrel{{\mathscr{L}}}{{\longrightarrow}}N(\mathbf{0},\sigma^{ 2}I_{p})\) if

\[n^{-1}\Lambda\max_{i}\left(\sum_{j=1}^{p}x_{ij}^{2}\right)\longrightarrow 0 \quad\text{as }n\to\infty. \tag{6.151}\]

Demonstration Proof: Let \(e_{i}\) (\(\in\mathbb{R}^{n}\)) have 1 in the \(i\)-th coordinate and 0 elsewhere. Then, writing \(\langle\;,\;\rangle_{m}\) for the Euclidean inner product in \(\mathbb{R}^{m}\),

\[H_{ii} = \langle He_{i},e_{i}\rangle_{n}\equiv\langle X(X^{\prime}X)^{-1}X ^{\prime}e_{i},e_{i}\rangle_{n} \tag{6.152}\] \[= \langle(X^{\prime}X)^{-1}X^{\prime}e_{i},X^{\prime}e_{i}\rangle_ {p}\qquad\quad[\text{since}\,\langle\mathbf{B}x,y\rangle_{n}=\langle x,B^{ \prime}y\rangle]\] \[\leq \frac{\Lambda}{n}\|X^{\prime}e_{i}\|^{2}=\frac{\Lambda}{n}\left\| (x_{ij})_{1\leq j\leq p}\right\|^{2}=\frac{\Lambda}{n}\left(\sum_{j=1}^{p}x_{ ij}^{2}\right).\]

Corollary 6.5: Let

\[\mathbf{y}_{i}=\alpha+\sum_{j=1}^{r}\beta_{j}Z_{ij}+\varepsilon_{i}\qquad(1 \leq i\leq n), \tag{6.153}\]

where \(\mathbf{y}=(y_{1},\dots,y_{n})^{\prime}\) is observed, \(\mathbf{Z}_{i}\equiv(Z_{i1},\dots,Z_{ir})^{\prime}\), \(i\geq 1\), are i.i.d. \(r\)-dimensional random vectors with finite mean vector \(\boldsymbol{\mu}=(\mu_{1},\dots\mu_{r})^{\prime}\) and nonsingular covariance matrix \(\Sigma=((\sigma_{jk}))_{1\leq j,k\leq r}\). Assume \(\varepsilon_{i}\), \(i\geq 1\), are i.i.d. satisfying (6.136), and that the two families \(\{\mathbf{Z}_{i}:i\geq 1\}\) and \(\{\varepsilon_{i}:i\geq 1\}\) are independent.

Demonstration Proof: Then the least squares estimators of \(\alpha\), \(\beta_{1},\dots,\beta_{r}\) are

\[\begin{pmatrix}\hat{\beta_{1}}\\ \vdots\\ \hat{\beta_{r}}\end{pmatrix}=\;\widehat{\Sigma}^{-1}\begin{pmatrix}\hat{ \sigma}_{y1}\\ \vdots\\ \hat{\sigma}_{yr}\end{pmatrix}\begin{bmatrix}\widehat{\Sigma}:=((\hat{\sigma} _{jk})),\;\hat{\sigma}_{jk}:=\frac{1}{n}\sum_{i=1}^{n}(Z_{ij}-\overline{Z}_{.j})(Z_{ik}-Z_{.k}),\\ \hat{\sigma}_{yj}:=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\overline{y})(Z_{ij}- \overline{Z}_{.j})\end{bmatrix},\]\[\hat{\alpha}=\overline{y}-\sum_{j=1}^{r}\hat{\beta}_{j}\overline{Z}_{.j}\qquad( \overline{y}=\tfrac{1}{n}\sum_{i=1}^{n}y_{i},\ \ \overline{Z}_{.j}=\tfrac{1}{n}\sum_{i=1}^{n}Z_{ij}). \tag{6.154}\]

_(b) Also,_

\[\begin{pmatrix}\hat{\alpha}\\ \hat{\beta}_{1}\\ \vdots\\ \hat{\beta}_{r}\end{pmatrix}\quad\text{is}\quad AN\begin{pmatrix}\begin{pmatrix} \alpha\\ \beta_{1}\\ \vdots\\ \beta_{r}\end{pmatrix},\ \frac{\sigma^{2}}{n}\begin{pmatrix}\overline{a_{0}} \begin{matrix}a_{1}&\ldots&a_{r}\\ a_{1}&&\\ \vdots&\Sigma^{-1}&\\ a_{r}&&\end{matrix}\\ \end{pmatrix}, \tag{6.155}\]

\[[a_{0}:=1+\sum_{j,k}\sigma^{jk}\mu_{j}\mu_{k}\text{, }a_{k}:=-\sum_{j^{\prime}=1}^{r} \sigma^{kj^{\prime}}\mu_{j^{\prime}}\ (k\geq 1)].\]

Proof.:
1. One may directly prove (6.154), by rewriting (6.153) as \[y_{i}=\alpha_{1}+\sum_{j=1}^{r}\beta_{j}(Z_{ij}-\overline{Z}_{.j})+\varepsilon _{i}\qquad(\alpha_{1}:=\alpha+\sum_{j=1}^{r}\beta_{j}\overline{Z}_{.j}),\] (6.156) and solve for \(\alpha_{1},\beta_{1}\ldots\beta_{r}\) by differentiating w.r.t. \(\alpha_{1},\beta_{1},\ldots,\beta_{r}\) the quantity \(\sum_{i=1}^{n}(y_{i}-\alpha_{1}-\sum_{j=1}^{r}\beta_{j}(Z_{ij}-\overline{Z}_{. j}))^{2},\) and setting the derivatives equal to zero.
2. For the proof of (b), write (6.153) in the form (6.135) with \(p=r+1\), and \[X=\begin{bmatrix}1&Z_{11}&Z_{12}&\ldots&Z_{1r}\\ 1&Z_{21}&Z_{22}&\ldots&Z_{2r}\\ \vdots&\vdots&\vdots&\ldots&\vdots\\ 1&Z_{n1}&Z_{n2}&\ldots&Z_{nr}\end{bmatrix}.\] (6.157) Then \[X^{\prime}X=n\begin{bmatrix}1&\overline{Z}_{.1}&\overline{Z}_{.2}&\ldots& \overline{Z}_{.r}\\ \overline{Z}_{.1}&\tilde{s}_{11}&\tilde{s}_{12}&\ldots&\tilde{s}_{2r}\\ \vdots&\vdots&\vdots&\vdots\\ \overline{Z}_{.r}&\tilde{s}_{r1}&\tilde{s}_{r2}&\ldots&\tilde{s}_{rr}\end{bmatrix} =n\begin{bmatrix}1&\overline{Z}_{.1}&\ldots&\overline{Z}_{.r}\\ \overline{Z}_{.1}&&\\ \vdots&\vdots&\vdots&\vdots\\ \overline{Z}_{.r}&&\end{bmatrix},\] \[\frac{X^{\prime}X}{n}\xrightarrow{\text{a.s.}}\begin{bmatrix}1&\mu_{1}& \ldots&\mu_{r}\\ \mu_{1}&&\\ \vdots&\Gamma&\\ \mu_{r}&&\end{bmatrix}=B,\ \text{say, }[\Gamma:=((\gamma_{jk}))],\] (6.158) where \(\tilde{s}_{jk}=\frac{1}{n}\sum_{i=1}^{n}Z_{ij}Z_{ik}\), \(\gamma_{jk}=E(Z_{ij}Z_{ik})=E\tilde{s}_{jk}.\) It is simple to check that \(AB=I_{r+1}.\) In particular, \(B\) is nonsingular so that the probability that \(X^{\prime}X\) is nonsingular (\(\Longrightarrow X\) is of full rank) converges to \(1\) as \(n\to\infty.\) Thus, one may apply Corollary 6.4 above, conditionally given \(\{\mathbf{Z}_{i}:i\geq 1\}\), to show that \((\hat{\alpha},\hat{\beta}_{1},\ldots,\hat{\beta}_{r})^{\prime}\) is outside a set of sample points whose probability is zero. 

Remark 24: The model in Corollary 22 is called the _correlation model,_ to indicate that \(y_{i}\) and \(\mathbf{Z}_{i}\equiv(Z_{i1},\ldots,Z_{i,r})\) are correlated. The asymptotic normality can be proved directly in this case, using the expressions (233) for \(\hat{\alpha}\) and \(\hat{\boldsymbol{\beta}}\equiv(\hat{\beta}_{1},\ldots,\hat{\beta}_{r})^{\prime}\), without invoking the general result contained in Theorem 21.

### Asymptotic Relative Efficiency (ARE) of Estimators

We have seen in Sects. 22-23 that large classes of statistics \(T_{n}\) are asymptotically normal, i.e., \(\sqrt{n}(T_{n}-g(\theta))\,\stackrel{{\mathscr{L}}}{{ \longrightarrow}}\,N(0,\sigma^{2}(\theta))\), where \(T_{n}\) maybe taken to be an estimator of a parametric function \(g(\theta)\).

Here we consider an index of asymptotic comparison of estimators of \(g(\theta)\), \(\theta\) being an unknown parameter which identifies the underlying probability distribution, say, \(P_{\theta}\), from which independent observations \(X_{1},\ldots,X_{n}\) are drawn.

Definition 25: If two estimators \(T_{n}^{(1)},T_{n}^{(2)}\) of \(g(\theta)\) are both asymptotically normal with

\[\sqrt{n}(T_{n}^{(1)}-g(\theta))\,\stackrel{{\mathscr{ L}}}{{\longrightarrow}}\,N(0,\sigma_{1}^{2}(\theta)),\quad\text{when}\,\theta\,\,\text{is the true parameter value} \tag{234}\] \[\sqrt{n}(T_{n}^{(2)}-g(\theta))\,\stackrel{{\mathscr{ L}}}{{\longrightarrow}}\,N(0,\sigma_{2}^{2}(\theta)),\]

then the _asymptotic relative efficiency_ (ARE) of \(T_{n}^{(2)}\) with respect to \(T_{n}^{(1)}\) is defined by

\[e_{T^{(2)},T^{(1)}}=\frac{\sigma_{1}^{2}(\theta)}{\sigma_{2}^{2}(\theta)}, \qquad(T^{(i)}=\{T_{n}^{(i)}:n=1,2,\ldots\}). \tag{235}\]

Thus if (for some value \(\theta\) of the parameter) the above efficiency is \(2/3\) then \(\sigma_{1}^{2}\) is two-thirds of \(\sigma_{2}^{2}\), and adopting \(T^{(1)}\) with a sample size \((2/3)n\) leads to the same accuracy (asymptotically, as \(n\to\infty\)) in the estimation of \(g(\theta)\) as would be achieved by using \(T^{(2)}\) with \(n\) observations.

Example 26 (Mean Versus Median in Normal and Cauchy Models): If \(X_{1},X_{2},\ldots,\)\(X_{n}\) are i.i.d. \(N(\theta,\sigma^{2})\), \(\theta\in(-\infty,\infty)\), \(\sigma^{2}\in(0,\infty)\). Consider \(\overline{X}=\frac{X_{1}+\cdots+X_{n}}{n}\) and \(\hat{\zeta}_{\frac{1}{2}}=\) the sample median (\(=\frac{1}{2}\)-quantile) as estimators of \(\theta\). Now \(\sqrt{n}(\overline{X}-\theta)\) is \(N(0,\sigma^{2})\) (and, therefore, \(\sqrt{n}(\overline{X}-\theta)\,\stackrel{{\mathscr{L}}}{{ \longrightarrow}}\,N(0,\sigma^{2})\), trivially) and, by Theorem 21, \(\sqrt{n}(\hat{\zeta}_{\frac{1}{2}}-\theta)\stackrel{{\mathscr{L}}} {{\longrightarrow}}\,N(0,\frac{2\pi\sigma^{2}}{4})=N(0,\frac{\pi}{2}\sigma^{2})\). Hence

\[e_{\hat{\zeta}_{\frac{1}{2},\overline{X}}}=\,\,\frac{\sigma^{2}}{\frac{\pi}{2 }\sigma^{2}}=\,\frac{2}{\pi}\,\simeq\,0.637. \tag{236}\]

Thus \(\overline{X}\) is decidedly the better of the two estimators (no matter what \(\theta,\sigma^{2}\) may be).

When the same two estimators are used to estimate the median \(\theta\) of a Cauchy distribution with density

\[f(x;\theta)=\left(\frac{1}{\pi a}\right)\frac{1}{1+\left(\frac{x-\theta}{a} \right)^{2}},\quad-\infty<x<\infty,\,\,\,(-\infty<\theta<\infty,\,a>0), \tag{237}\]then \(\overline{X}\) is not even consistent, since the distribution of \(\overline{X}\) is the same as that of a single observation (Exercise). But Theorem 6.3 applies to yield

\[\sqrt{n}(\hat{\zeta}_{\frac{1}{2}}-\theta)\stackrel{{\mathscr{L}}}{ {\longrightarrow}}N\left(0,\frac{\pi^{2}a^{2}}{4}\right). \tag{6.163}\]

Although, strictly speaking, the ARE of \(\hat{\zeta}_{\frac{1}{2}}\) w.r.t. \(\overline{X}\) is not defined for this case, one may informally take it to be \(\infty\). No matter how large a number \(A\) may be, \(\overline{X}\) based on \(nA\) observations is worse than \(\hat{\zeta}_{\frac{1}{2}}\) based on \(n\) observations.

To compare the two statistics as estimators of the median \(\theta\) of a distribution which has a density symmetric about \(\theta\), let \(f(x)\) be a p.d.f. (with respect to Lebesgue measure) which is an even function: \(f(x)=f(-x)\). The common p.d.f. of the observations is \(f(x;\theta)=f(x-\theta)\), \(\theta\in(-\infty,\infty)\). (There may be other unknown parameters in \(f(x;\theta)\)). Here are some examples:

_Example 6.9_.: The Logistic \(L(0,a)\) has the density \(f(x)=\frac{1}{a}e^{-x/a}/(1+e^{-x/a})^{2}\), \(a>0\). For this case

\[\sqrt{n}(\hat{\zeta}_{\frac{1}{2}}-\theta)\stackrel{{ \mathscr{L}}}{{\longrightarrow}}N(0,4a^{2}),\] \[\sqrt{n}(\overline{X}-\theta)\stackrel{{\mathscr{L}}} {{\longrightarrow}}N(0,\sigma^{2}),\]

where

\[\sigma^{2} =\int_{-\infty}^{\infty}x^{2}f(x)dx=a^{2}\int_{-\infty}^{\infty} x^{2}e^{-x}/(1+e^{-x})^{2}dx\] \[=2a^{2}\int_{0}^{\infty}x^{2}e^{-x}/(1+e^{-x})^{2}dx=(4a^{2}) \frac{\pi^{2}}{12}\,.\]

(Exercise 6.34). Hence

\[e_{\hat{\zeta}_{\frac{1}{2}},\overline{X}}=\frac{\pi^{2}}{12}\simeq 0.82.\]

_Example 6.10 (Contaminated Normal or Tukey Model)._

\[f(x)=(1-\varepsilon)\varphi(x)+\varepsilon\varphi_{\tau}(x),\qquad(0< \varepsilon<1,\tau>0)\]

where \(\varphi\) is the Normal p.d.f. with mean zero and variance one and \(\varphi_{\tau}\) is the Normal p.d.f. with mean zero and variance \(\tau\). For \(f(x-\theta)\) the median \(\zeta_{\frac{1}{2}}\) is \(\theta\) and the mean is also \(\theta\).

\[f(\zeta_{\frac{1}{2}};\theta)=f(\theta-\theta)=f(0)=\frac{1}{\sqrt{2\pi}}(1- \varepsilon)+\varepsilon\left(\frac{1}{\sqrt{2\pi}\sqrt{\tau}}\right)=\frac{ 1}{\sqrt{2\pi}}\left(1-\varepsilon+\frac{\varepsilon}{\sqrt{\tau}}\right).\]

The variance is \(\sigma^{2}=\int_{-\infty}^{\infty}x^{2}f(x)dx=(1-\varepsilon)+\varepsilon\tau\). Hence

\[e_{\zeta_{\frac{1}{2}},\overline{X}}=\frac{(1-\varepsilon)+\varepsilon\tau}{ \frac{1}{4}/f^{2}(0)}=\frac{4\{(1-\varepsilon)+\varepsilon\tau\}(1- \varepsilon+\varepsilon/\sqrt{\tau})^{2}}{2\pi}\,.\]

### Constructing (Nonparametric) Confidence Intervals

In this section we briefly consider constructing confidence regions for nonparametric functional estimates. Sections 6.6 and 6.8 dealt with what one might call semiparametric estimation of regression.

Corollary 6.2 may be used to construct an asymptotic _confidence interval_ for the unknown quantity \(H(\mu)\): such as \([H(\overline{Z})-z_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt{n}},H(\overline{Z})+z_ {1-\alpha/2}\frac{\hat{\sigma}}{\sqrt{n}}]\), where \(z_{1-\alpha/2}\) is the \((1-\frac{\alpha}{2})\)th quantile of \(N(0,1)\), and

\[\hat{\sigma}^{2}=\sum_{i,j=1}^{k}\hat{\ell}_{i}\hat{\ell}_{j}\hat{b}_{ij}\]

with \(\hat{\ell}_{i}\) obtained by replacing \(\mu\) by \(\overline{Z}\) in the expression for \(\ell_{i}\) in (6.57), and letting \(\hat{b}_{ij}\) be the sample covariance between \(f_{i}(Y_{1})\) and \(f_{j}(Y_{1})\):

\[\hat{b}_{ij}=\frac{1}{n}\sum_{r=1}^{n}f_{i}(Y_{r})f_{j}(Y_{r})-\left(\frac{1} {n}\sum_{r=1}^{n}f_{i}(Y_{r})\right)\left(\frac{1}{n}\sum_{r=1}^{n}f_{j}(Y_{r} )\right).\]

By the SLLN, \(\hat{\ell}_{i}\) and \(\hat{b}_{ij}\) are consistent estimators of \(\ell_{i}\) and \(b_{ij}\), respectively (\(1\leq i\), \(j\leq k\)). Hence \(\hat{\sigma}^{2}\stackrel{{ P}}{{\longrightarrow}}\sigma^{2}\). It follows that (use Slutsky's Lemma and Corollary 6.2)

\[P\left(H(\overline{Z})-z_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt{n }}\leq H(\mu)\leq H(\overline{Z})+z_{1-\alpha/2}\frac{\tilde{\sigma}}{\sqrt{n} }\right)\\ =P\left(\left|\frac{\sqrt{n}(H(\overline{Z})-H(\mu))}{\hat{\sigma} }\right|\leq z_{1-\alpha/2}\right)\longrightarrow 1-\alpha. \tag{6.164}\]

One may similarly obtain an asymptotic confidence interval for \(\zeta_{p}\), under the hypotheses of Theorem 6.3, by estimating the density at \(\zeta_{p}\). A _nonparametric density estimation_ is carried out in a later chapter. It is possible, however, to provide a different nonparametric confidence interval for \(\zeta_{p}\) without resorting to density estimation. We describe this below.

For every \(s\), \(1\leq s\leq n\), one has

\[P(\zeta_{p}<X_{(s)}) =P(\text{\#of observations $X_{j},1\leq j\leq n$, which are less than}\] (6.165) or equal to

\[\zeta_{p}\]

 is less than

\[s\] \[=\sum_{m=0}^{s-1}\binom{n}{m}F^{m}(\zeta_{p})(1-F(\zeta_{p}))^{n-m}=\sum_ {m=0}^{s-1}\binom{n}{m}p^{m}(1-p)^{n-m}.\]For \(1\leq r<s\leq n\), one then has

\[P(X_{(r)} \leq\zeta_{p}<X_{(s)})=P(\zeta_{p}<X_{(s)})-P(\zeta_{p}<X_{(r)})\] \[=\sum_{m=r}^{s-1}{n\choose m}p^{m}(1-p)^{n-m}. \tag{6.166}\]

One now needs to look up _binomial tables_ to find \(r\) and \(s\) such that (6.166) is approximately \(1-\alpha\). For the approximately equal tailed confidence interval, one finds \(s\) such that (6.165) is closest to \(1-\alpha/2\), and \(r\) such that \(\sum_{m=0}^{r-1}{n\choose m}\,p^{m}(1-p)^{n-m}\) is closest to \(\alpha/2\). If \(n\) is large and \(np\), \(n(1-p)\) are both moderately large then one may use _normal approximation_ to the binomial:

\[s=\left[np+z_{1-\alpha/2}\sqrt{np(1-p)}\,\right],\qquad r=\left[ np-z_{1-\alpha/2}\sqrt{np(1-p)}\,\right],\] \[([y]:=\mbox{integer part of $y$}). \tag{6.167}\]

#### Addendum

The Berry-Esseen bound for the classical CLT says: If \(X_{j}\), \(1\leq j\leq n\), are i.i.d. with mean \(\mu\), variance \(\sigma^{2}\) and a finite third absolute moment \(\rho_{3}=E|X_{1}|^{3}\), then

\[\sup_{x\in\mathbb{R}}\left|P(\sqrt{n}(\overline{X}-\mu)\leq x)-\Phi_{0,\sigma^ {2}}(x)\right|\leq(0.5600)\,\frac{\rho_{3}}{\sigma^{3}\sqrt{n}}\,. \tag{6.168}\]

The constant \(0.7975\) is due to Shevtsova (2010). The same inequality holds also for non-identically distributed, but independent, \(X_{j}\) (\(1\leq j\leq n\)) with \(\mu=\sum_{j=1}^{n}EX_{j}/n\), \(\sigma^{2}=1/n\sum_{j=1}^{n}\mbox{var}(X_{j})\), and \(\rho_{3}=\frac{1}{n}\sum_{j=1}^{n}E|X_{j}-E(X_{j})|^{3}\).

For the multidimensional CLT there are Berry-Esseen type bounds available, although good numerical constants in the bound [such as given in (6.168)] are difficult to obtain. (Reference: Bhattacharya and Rang (2010).)

### Errors in Variables Models

In standard regression models the independent or explanatory variables are assumed to be observed exactly, i.e., without errors. However, in many situations the independent variables can be contaminated, mismeasured or they cannot be directly observed. Under these circumstances the model is referred to as an _errors in variables model_. Consider for simplicity the linear regression model

\[Y_{j}=\alpha+\beta X_{j}^{*}+\varepsilon_{j}\qquad(j=1,\ldots,n), \tag{6.169}\]

where \(\varepsilon_{j}\) has mean zero and a finite variance \(\sigma_{\varepsilon}^{2}>0\). The "true" value \(X_{j}^{*}\) of the independent variable cannot be observed and, instead, one observes \(X_{j}\) subject to a random error,

\[X_{j}=X_{j}^{*}+\eta_{j}\qquad(j=1,\ldots,n), \tag{6.170}\]

where \(\eta_{j}\) are mean zero i.i.d. random variables with finite variance \(\sigma_{\eta}^{2}>0\). Consider first \(X_{j}^{*}\) to be random with a finite variance \(\sigma_{x}^{2}>0\), and assume that \(X_{j}^{*}\), \(\eta_{j}\)and \(\varepsilon_{j}\) are independent. Also, assume the \(n\) observations \((X_{j},Y_{j})\), \(1\leq j\leq n\), are independent. The ordinary least squares (OLS) estimate of \(\beta\) based on these observations is (see (6.14))

\[\hat{\beta} =\frac{[\sum_{1\leq j\leq n}(X_{j}-\overline{X})(Y_{j}-\overline{ Y})]}{[\sum_{1\leq j\leq n}(X_{j}-\overline{X})^{2}]}\] \[=\frac{[\sum_{1\leq j\leq n}(X_{j}^{*}-\overline{X}^{*}+\eta_{j}- \overline{\eta})(\beta(X_{j}^{*}-\overline{X}^{*})+\varepsilon_{j}-\overline{ \varepsilon})]}{[\sum_{1\leq j\leq n}(X_{j}^{*}-\overline{X}^{*}+\eta_{j}- \overline{\eta})]^{2}}. \tag{6.171}\]

When divided by \(n\), the denominator converges in probability to \(\sigma_{x^{*}}^{2}+\sigma_{\eta}^{2}\), and the numerator to \(\beta\sigma_{x^{*}}^{2}\) (Exercise 6.37), so that

\[\hat{\beta}\stackrel{{ p}}{{\longrightarrow}}\frac{\beta}{(1+ \sigma_{\eta}^{2}/\sigma_{x^{*}}^{2})}=\kappa\beta, \tag{6.172}\]

where

\[\kappa:=\big{(}1+\sigma_{\eta}^{2}/\sigma_{x^{*}}^{2}\big{)}^{-1} \tag{6.173}\]

is the so-called _reliability ratio_ determined by the _noise-to-signal ratio_\(\sigma_{\eta}^{2}/\sigma_{x^{*}}^{2}\). Therefore, \(\hat{\beta}\) is an _inconsistent underestimate_ of \(\beta\). This errors in variables model with a stochastic \(X^{*}\) is called a _structural model_ in economics; here one stipulates an error free relation between the "true" random variables \(X^{*}\) and \(Y^{*}\) of the form

\[\theta_{1}Y^{*}+\theta_{2}X^{*}=\alpha,\qquad(\theta_{1}\neq 0,\,\theta_{2}\neq 0), \tag{6.174}\]

but with both \(X^{*}\) and \(Y^{*}\) subject to measurement errors.

In the so-called _functional model_, \(X^{*}\) is non-stochastic, while the other assumptions above remain intact. If \(n^{-1}\sum_{1\leq j\leq n}(X_{j}^{*}-\overline{X}^{*})^{2}\) converges to a positive quantity, say \(\sigma_{x^{*}}^{2}\), as \(n\to\infty\), then (6.172) still holds (see Exercise 6.37).

We now concentrate on the structural model. Suppose one knows, or has a consistent estimate of, \(\kappa\) (i.e. of the noise-to-signal ratio \(\sigma_{\eta}^{2}/\sigma_{x^{*}}^{2}\)). We assume that \(\kappa\) is known. Then

\[\tilde{\beta}=\left(\frac{1}{\kappa}\right)\hat{\beta}=\left(1+\sigma_{\eta}^{ 2}/\sigma_{x^{*}}^{2}\right)\hat{\beta} \tag{6.175}\]

is a consistent estimate of \(\beta\). Assume, for simplicity, that \(X^{*}\), \(\eta\) and \(\varepsilon\) are Normal. Then \((X,\eta)\) is bivariate Normal with \(EX=\mu\), say, \(\mathrm{var}(X)=\sigma_{x^{*}}^{2}+\sigma_{\eta}^{2}\), \(E\eta=0\), \(\mathrm{var}(\eta)=\sigma_{\eta}^{2}\), \(\mathrm{cov}(X,\eta)=\sigma_{\eta}^{2}\). Therefore, the conditional distribution of \(\eta\), given \(X\), is Normal with mean \(E\eta+\big{[}\sigma_{\eta}^{2}/(\sigma_{x^{*}}^{2}+\sigma_{\eta}^{2})\big{]}^{ 1/2}\rho(X-\mu)\) and variance \(\sigma_{\eta}^{2}(1-\rho^{2})\), where \(\rho=\mathrm{cov}(X,\eta)/(\mathrm{var}(X)\mathrm{var}(\eta))^{1/2}=(1-\kappa) ^{1/2}\) (see Appendix A.3, Exercise A.3). Now express \(\hat{\beta}\) as [see (6.171)]

\[\hat{\beta} =\sum(X_{j}-\overline{X})\frac{\big{[}\beta(X_{j}-\overline{X}) -\beta(\eta_{i}-\overline{\eta})+\varepsilon_{j}-\overline{\varepsilon}\big{]} }{\sum(X_{j}-\overline{X})^{2}}\] \[=\beta\left(1-\frac{\sum(X_{j}-\overline{X})\eta_{j}}{\sum(X_{j }-\overline{X})^{2}}\right)+\frac{\sum(X_{j}-\overline{X})\varepsilon_{j}}{ \sum(X_{j}-\overline{X})^{2}}\,. \tag{6.176}\]

Hence, conditionally given \(\mathbf{X}=(X_{j};1\leq j\leq n)\), \(\hat{\beta}\) is Normal with mean and variance given by (Exercise 6.38)

\[E(\hat{\beta}\mid\mathbf{X})=\kappa\beta,\quad\mathrm{var}(\tilde{\beta}\mid \mathbf{X})=\frac{\beta^{2}\sigma_{x^{*}}^{2}+\sigma_{\varepsilon}^{2}}{[\sum_{1 \leq j\leq n}(X_{j}-\overline{X})^{2}]}. \tag{6.177}\]Since \([\sum_{1\leq j\leq n}(X_{j}-\overline{X})^{2}]/n\stackrel{{ P}}{{ \longrightarrow}}\sigma_{x^{*}}^{2}+\sigma_{\eta}^{2}\), it follows that the conditional distribution of \(\sqrt{n}(\hat{\beta}-\kappa\beta)\) converges in distribution to \(N(0,\delta^{2})\), where (Exercise 6.38)

\[\delta^{2}=\frac{[\beta^{2}\sigma_{x^{*}}^{2}+\sigma_{\varepsilon}^{2}]}{[ \sigma_{x^{*}}^{2}+\sigma_{\eta}^{2}]}=\beta^{2}\kappa+\frac{\sigma_{ \varepsilon}^{2}}{\sigma_{x^{*}}^{2}+\sigma_{\eta}^{2}}=\beta^{2}\kappa+\frac{ \sigma_{\varepsilon}^{2}\kappa}{\sigma_{x^{*}}^{2}}\,. \tag{6.178}\]

This latter distribution does not involve \(\mathbf{X}\); hence we have shown that

\[\sqrt{n}(\hat{\beta}-\kappa\beta)\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N(0,\delta^{2}),\qquad\mbox{as $n\to\infty$}. \tag{6.179}\]

It follows that

\[\sqrt{n}(\tilde{\beta}-\beta)\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N\left(0,\frac{\delta^{2}}{\kappa^{2}}\right). \tag{6.180}\]

One may similarly derive the asymptotic distribution of \(\sqrt{n}(\tilde{\alpha}-\alpha,\tilde{\beta}-\beta)\).

Since the above argument is conditional on \(\mathbf{X}\), the corresponding results also hold for the functional model, if

\[\sum_{1\leq j\leq n}\frac{X_{j}^{*}}{n}\longrightarrow\mu,\quad\sum_{1\leq j \leq n}\frac{(X_{j}^{*}-\overline{X}^{*})^{2}}{n}\longrightarrow\sigma_{x^{*} }^{2}\quad\mbox{(for some $\mu,\sigma_{x^{*}}^{2}$; $0<\sigma_{x^{*}}^{2}<\infty$)}. \tag{6.181}\]

Fuller (1987, pp. 18-20), describes an agricultural experiment in which a consistent estimate of \(\sigma_{\eta}^{2}\) and, therefore, of \(\kappa\), is obtained. In the absence of such additional information the parameter \(\boldsymbol{\theta}=(\alpha,\beta,\mu=x^{*},\sigma_{x^{*}}^{2},\sigma_{\eta}^{2 },\sigma_{\varepsilon}^{2})\) is _not identifiable_ in the bivariate Normal model for \((X,Y)\) presented above [see (6.169), (6.170)]. In particular, \(\beta\) is _not identifiable_. That is, there are \(\boldsymbol{\theta}_{1}\) and \(\boldsymbol{\theta}_{2}\) with different values of \(\beta\) yielding the same bivariate Normal distribution for \((X,Y)\). We give a proof of this fact due to Riersol (1950). That \(\boldsymbol{\theta}\) is not identifiable in the Normal model, of course, is easy to see. For the bivariate Normal is entirely determined by five parameters--two means, two variances and a covariance. But \(\boldsymbol{\theta}\) has six functionally independent parameters, i.e., with a parameter space in \(\mathbb{R}^{6}\) with a non-empty interior. On the other hand five parametric functions are identifiable and, therefore, can be estimated consistently. Unfortunately, the most important parameter \(\beta\) is not. To see this assume that the errors \(\varepsilon\) and \(\eta\) are independent and Normal with zero means and variances \(\sigma_{\varepsilon}^{2}>0\), \(\sigma_{\eta}^{2}>0\). Also assume that \(X^{*}\) is independent of \(\varepsilon\) and \(\eta\). It will be shown that \(\beta\) is not identifiable precisely when \(X^{*}\) is Normal, i.e., if \((X,Y)\) is bivariate Normal. Let there exist \(\boldsymbol{\theta}_{j}=(\alpha_{j},\beta_{j},\mu_{j},\sigma_{x^{*}j}^{2}, \sigma_{\eta j}^{2},\sigma_{\varepsilon j}^{2})\), \(j=1,2\), such that \(\beta_{1}\neq\beta_{2}\), but the distributions of \((X,Y)\) the same under \(\boldsymbol{\theta}_{1}\) and \(\boldsymbol{\theta}_{2}\). The characteristic function of \((X,Y)\) under \(\boldsymbol{\theta}_{j}\) is

\[\varphi(t_{1},t_{2}) =E_{\boldsymbol{\theta}_{j}}\exp\left\{it_{1}X+it_{2}Y\right\}\] \[=E_{\boldsymbol{\theta}_{j}}\exp\left\{it_{1}X^{*}+it_{1}\eta_{j }+it_{2}\alpha_{j}+it_{2}\beta_{j}X^{*}+it_{2}\varepsilon_{j}\right\}\] \[=e^{it_{2}\alpha_{j}}\exp\left\{-\frac{1}{2}\left(t_{1}^{2}\sigma _{\eta_{j}}^{2}+t_{2}^{2}\sigma_{\varepsilon_{j}}^{2}\right)\right\}\psi_{ \boldsymbol{\theta}_{j}}(t_{1}+t_{2}\beta_{j})\;\;(j=1,2), \tag{6.182}\]

where \(\psi_{\boldsymbol{\theta}_{j}}\) is the characteristic function of \(X^{*}\) under \(\boldsymbol{\theta}_{j}\). Fix \(z\in\mathbb{R}\). There exist \(t_{1},t_{2}\) such that \(t_{1}+t_{2}\beta_{1}=z\) and \(t_{1}+t_{2}\beta_{2}=0\), namely, \(t_{1}=-\beta_{2}z/(\beta_{1}-\beta_{2})\), \(t_{2}=z/(\beta_{1}-\beta_{2})\). Then by (6.182), and equating \(\varphi_{1}\) and \(\varphi_{2}\), one has\[\psi_{\boldsymbol{\theta}_{1}}(z) =\exp\Bigl{\{}iz\frac{(\alpha_{2}-\alpha_{1})}{(\beta_{1}-\beta_{2}) }-\frac{1}{2}\left(\frac{\beta_{2}^{2}}{(\beta_{1}-\beta_{2})^{2}}\right)\left( \sigma_{\eta 2}^{2}-\sigma_{\eta 1}^{2}\right)z^{2}\] \[\qquad\qquad\qquad\qquad\qquad-\frac{1}{2}(\beta_{1}-\beta_{2})^{ -2}\left(\sigma_{\varepsilon 2}^{2}-\sigma_{\varepsilon 1}^{2}\right)z^{2}\Bigr{\}}\] \[=\exp\left\{icz-\frac{1}{2}dz^{2}\right\}\]

for some constants \(c\) and \(d\). In other words, the distribution of \(X^{*}\) under \(\boldsymbol{\theta}_{1}\) (and, therefore, also \(\boldsymbol{\theta}_{2}\)) is Normal (or a constant, which may be taken as Normal). We have arrived at the fact that if the errors \(\eta\) and \(\varepsilon\) in the variables are Gaussian, then \(\beta\)_is unidentifiable if and only if \(X^{*}\) is Gaussian, that is, if and only if \((X,Y)\) has the bivariate Normal._

Before moving away from the bivariate Normal model for \((X,Y)\) in the absence of additional information, note that the OLS \(\hat{\beta}_{xy}\) for the regression of \(X\) on \(Y\) provides a lower estimate of \(1/\beta\) and, therefore, \(\hat{\beta}_{xy}\) is an upper estimate of \(\beta\), just as \(\hat{\beta}=\hat{\beta}_{yx}\) is a lower estimate of \(\beta\).

It was pointed out by Berkson (1950) that in many applications observations \(X_{j}\) of the regressor are _controlled_. For example, in bioassay for different levels of _fixed_ dosages \(X_{j}\) one finds the response \(Y_{j}\). In this case there may still be some errors in the actual dosages \(x_{j}^{*}\) that are administered, but \(X_{j}\) is nonrandom, \(X_{j}=x_{j}^{*}+\eta_{j}\). Here \(x_{j}^{*}\) and \(\eta_{j}\) are negatively correlated, but \(X_{j}\) is uncorrelated with them. The OLS \(\hat{\beta}\) is now an unbiased and consistent estimator of \(\beta\) and the regression model is formally the same as the one without measuremental errors [see (6.14)-(6.16)]

\[Y_{j} =\alpha+\beta X_{j}=\gamma_{j},\qquad\qquad\gamma_{j}:=-\beta\eta _{j}+\varepsilon_{j},\] \[\hat{\beta} =\beta+\frac{\sum_{j=1}^{n}\gamma_{j}(X_{j}-\overline{X})}{\sum_{ j=1}^{n}(X_{j}-\overline{X})^{2}}\,,\quad\hat{\alpha}=\overline{Y}-\hat{\beta} \overline{X}. \tag{6.183}\]

Theorem 6.2 holds for the joint distribution of \(\hat{\alpha},\hat{\beta}\), where \(\sigma_{\gamma}^{2}=\beta^{2}\sigma_{\eta}^{2}=\sigma_{\varepsilon}^{2}\) replaces \(\sigma^{2}\).

As a final observation on the model (6.169), (6.170), express the equation (6.169) as

\[Y_{j}=\alpha+\beta X_{j}-\beta\eta_{j}+\varepsilon_{j}. \tag{6.184}\]

Since \(\eta\) is not observed, this can be thought of as a special case of linear regression with one regressor missing. See Bhattacharya and Bhattacharyya (1994).

### Notes and References

A general reference to this chapter is Ferguson (1996). Serfling (1980, Chaps. 1, 2) and Bickel and Doksum (2001, Chap. 5 and Appendices A.14, A.15), contain many basic results and fine exercises. Dasgupta (2008, Chaps. 1, 5 and 7) may be consulted for many additional facts and references as well as a wealth of exercises.

### Exercises for Chap. 6

#### Exercises for Sect. 6.2

**Ex. 6.1**.:
1. Extend Proposition 6.1 to \(k\) sequences \(U_{n}^{(i)}\stackrel{{ P}}{{\longrightarrow}}a_{i}\) (\(1\leq i\leq k\)) and a function \(g\) of \(k\) variables continuous at \((a_{1},a_{2},\ldots,a_{k})\), as stated in Remark 6.1.
2. Extend (a) to vector-valued sequences \(U_{n}^{(i)}\), \(1\leq i\leq k\), and vector-valued functions \(g(u,v)\).

**Ex. 6.2**.:
1. Show that Corollary 6.1 holds if \(\stackrel{{ P}}{{\longrightarrow}}\) is replaced by \(\stackrel{{\rm a.s.}}{{\longrightarrow}}\).
2. Prove Proposition 6.4.

**Ex. 6.3**.: Let \(X_{n}\) (\(n\geq 1\)) be a sequence of i.i.d. random variables, and assume that the infimum and supremum of values of \(X_{1}\) are \(m\) and \(M\), respectively. That is, \(P(m\leq X_{1}\leq M)=1\), \(P(X_{1}<a)>0\)\(\forall\)\(a>m\), \(P(X_{1}>b)>0\)\(\forall\)\(b<M\) (Here \(X_{n}\) real-valued, but \(m\) and/or \(M\) may be infinite). Prove that \(\max\{X_{1},\ldots,X_{n}\}\stackrel{{\rm a.s.}}{{\longrightarrow}}M\) and \(\min\{X_{1},\ldots,X_{n}\}\stackrel{{\rm a.s.}}{{\longrightarrow }}m\).

**Ex. 6.4**.: Let \(Y_{n}\stackrel{{ P}}{{\longrightarrow}}Y\). Prove that there exists a subsequence \(Y_{n_{k}}\) (\(k=1,2,\ldots\)) (\(n_{1}<n_{2}<\cdots\)) such that \(Y_{n_{k}}\stackrel{{\rm a.s.}}{{\longrightarrow}}Y\) as \(k\to\infty\).

#### Exercises for Sect. 6.3

**Ex. 6.5**.:
1. Let \(\delta_{n}=\max\{(X_{j}-\overline{X})^{2}:1\leq j\leq n\}/\sum_{1}^{n}(X_{j}- \overline{X})^{2}\). Prove that, under the assumptions of Example 6.3, \(\hat{\alpha}\) and \(\hat{\beta}\) are consistent estimators of \(\alpha\), \(\beta\) if \(\delta_{n}\to 0\).
2. Prove the statement in Remark 6.4 for consistency of \(\hat{\alpha}\), \(\hat{\beta}\) under the hypothesis (6.17), and assuming \(\delta_{n}\to 0\) as \(n\to\infty\).
3. Extend (b) to the heteroscedastic case (6.18) with bounded \(\sigma_{j}^{2}\) (\(j\geq 1\)).
4. Write \(\gamma_{n}=\max\{\sigma_{j}^{2}:1\leq j\leq n\}\). Extend (c) to the case of possibly unbounded sequences \(\sigma_{j}^{2}\) (\(j\geq 1\)), but satisfying \(\gamma_{n}/n\longrightarrow 0\), \(m_{2}\gamma_{n}/\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}\to 0\), where \(m_{2}=\sum_{1}^{n}X_{j}^{2}/n\).

**Ex. 6.6**.: Give simpler proofs of consistency of \(\hat{\alpha}\), \(\hat{\beta}\) assuming \(\varepsilon_{j}\) are i.i.d. \(N(0,\sigma^{2})\) in

1. Example 6.3,
2. Example 6.4.

**Ex. 6.7**.:
1. Let \(X_{n}\) (\(n\geq 1\)) be uncorrelated random variables, \(\sigma_{n}^{2}=\operatorname{var}X_{n}\). Suppose \(\frac{1}{n}\sum_{j=1}^{n}EX_{j}\to\mu\) as \(n\to\infty\). Prove that \(\overline{X}=\frac{1}{n}\sum_{j=1}^{n}\) is a consistent estimator of \(\mu\) if \(\frac{1}{n^{2}}\sum_{j=1}^{n}\sigma_{j}^{2}\to 0\).
2. In (a), assume \(EX_{j}=\mu\)\(\forall\)\(j\), and while \(\mu\) is unknown, \(\sigma_{j}^{2}>0\) are known. If \(X_{1},\ldots,X_{n}\) are observed, show that the linear unbiased estimator \(U_{n}=\sum_{1}^{n}\omega_{j}X_{j}\) of \(\mu\) with the minimum expected squared error is obtained by taking \(\omega_{j}=\frac{1}{\sigma_{j}^{2}}/\sum_{1}^{n}\frac{1}{\sigma_{i}^{2}}\) (\(1\leq j\leq n\)).
3. Show that the optimal estimator \(U_{n}\) (\(n\geq 1\)) of \(\mu\) in (b) is consistent if \(\sum_{1}^{n}\frac{1}{\sigma_{i}^{2}}\to\infty\) as \(n\to\infty\).

**Ex. 6.8**.: Prove (6.30) by showing that the second moment of the left side converges to \([\sigma^{2}/(1-\beta^{2})]^{2}\), and then using (6.31).

#### Exercises for Sect. 6.4

**Ex. 6.9**.: Using the general definition of a quantile, prove the following assertions for \(0<p<1\).

1. If \(F\) is continuous and strictly increasing on \((c,d)\) where \(F(c)<p\) and \(F(d)>p\), then \(F\) has a unique \(p\)-th quantile.
2. If \(F(x)=p\) for all \(x\in(c,d)\), \(F(x)<p\)\(\forall\)\(x<c\) and \(F(x)>p\)\(\forall\)\(x>d\), then the set of \(p\)-th quantiles is \([c,d]\).

**Ex. 6.10**.: Assume \(F\) is continuous.

1. If \(np\)_is not an integer,_ show that \(X_{([np]+1)}\) is the unique \(p\)-th quantile of the empirical distribution.
2. If \(np\) is an integer, show that the set of \(p\)-th quantiles is the interval \([X_{([np])},X_{([np]+1)}]\).

**Ex. 6.11**.: Let \(X\) be a discrete random variable with values \(a_{1}<a_{2}<\cdots<a_{k}<\cdots\) (finite or denumerable sequence), \(P(X=a_{k})=\pi_{k}>0\)\(\forall\)\(k\). Its d.f. is

\[F(x)=\left\{\begin{array}{ll}0&\mbox{if $x<a_{1}$}\\ p_{k}&\mbox{if $a_{k}\leq x<a_{k+1}$}\end{array}\right.\quad(k=1,2,\cdots) \tag{6.185}\]

where \(p_{k}=\pi_{1}+\pi_{2}+\cdots+\pi_{k}\)\((k\geq 1)\). Let \(p_{k}<p<p_{k+1}\). Show that \(X_{([np])}\)\((\equiv X_{([np]):n)}\) converges in probability to the population \(p\)-th quantile \(a_{k+1}\), as \(n\to\infty\). [Hint: Let \(N_{k}\)\((\equiv N_{k:n})=\#\{j:1\leq j\leq n\), \(X_{j}\leq a_{k}\}\). Then \(N_{k}\) is binomial \(B(n,p_{k})\), and \(P(X_{([np])}=a_{k+1})=P(N_{k}<[np]\leq N_{k+1})=P(N_{k}<[np])-P(N_{k+1}<[np]) \longrightarrow 1-0=1\).]

#### Exercises for Sect. 6.5

**Ex. 6.12** (Designing a Sampling Plan).: For taking an opinion poll one wishes to know the size \(n\) of the random sample needed to ensure that the error of estimating the population proportion \(p\) by the sample proportion \(\hat{p}\) be no more than \(0.03\) with a \(95\,\%\) probability.

1. Use Chebyshev's Inequality (6.5) with \(r=2\) to get \(n=5556\).
2. Use Chebyshev's Inequality with \(r=4\) to get \(n=2153\).
3. Use the CLT to get \(n=1067\). [Hint: Use the fact that \(p(1-p)\) has the maximum value \(\frac{1}{4}\) at \(p=\frac{1}{2}\).]

**Ex. 6.13**.: Let \(P_{n}\)\((n\geq 1)\), \(P\) be probability measures on \((\mathbb{R},\mathscr{B}(\mathbb{R}))\) such that \(P_{n}\) converges weakly to \(P\).

1. Give an example to show that \(P_{n}(B)\) need not converge to \(P(B)\) for all Borel sets \(B\).
2. Give an example to show that the distribution function \(F_{n}\) of \(P_{n}\) may not converge to the distribution function \(F\) of \(P\) at every point \(x\).
3. Suppose \(g_{n}Y_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}V\) for some random variable \(V\), where \(g_{n}\to\infty\) as \(n\to\infty\). Show that \(Y_{n}\stackrel{{ P}}{{\longrightarrow}}0\).

**Ex. 6.14**.: Extend (6.57) to vector valued \(H\).

**Ex. 6.15**.: Suppose \(P_{n}\)\((n\geq 1)\), \(P\) are probability measures on \((\mathbb{R},\mathscr{B}(\mathbb{R}))\) such that \(P_{n}\) converges weakly to \(P\). Show that \(\{P_{n}:n\geq 1\}\) is _tight: for every \(\varepsilon>0\) there exists \(A_{\varepsilon}>0\) such that \(P_{n}\)\((\{x:|x|>A_{\varepsilon}\})<\varepsilon\) for all \(n\)._

[Hint: (i) Find points of continuity \(-B_{\varepsilon}\), \(C_{\varepsilon}\) of the distribution function \(F\) of \(P\) such that \(F(-B_{\varepsilon})<\varepsilon/3\), \(F(C_{\varepsilon})>1-\varepsilon/3\).

(ii) Find \(N_{\varepsilon}\) such that \(F_{n}(-B_{\varepsilon})<\varepsilon/3\) and \(F_{n}(C_{\varepsilon})>1-\varepsilon/3\) for all \(n\geq N_{\varepsilon}\), where \(F_{n}\) is the distribution function of \(P_{n}\). Then \(P_{n}([-B_{\varepsilon}\), \(C_{\varepsilon}])>1-\frac{2\varepsilon}{3}\) for all \(n\geq N_{\varepsilon}\).

(iii) For \(n=1,\dots,N_{\varepsilon}\), find \(D_{\varepsilon}>0\) such that \(P_{n}([-D_{\varepsilon},D_{\varepsilon}])>1-\varepsilon\)\((1\leq n\leq N_{\varepsilon})\).

(iv) Let \(A_{\varepsilon}=\max B_{\varepsilon},C_{\varepsilon},D_{\varepsilon}\) to get \(P_{n}(\{x:|x|>A_{\varepsilon}\})<\varepsilon\) for all \(n\).]

**Ex. 6.16**.: Suppose \(\binom{U}{V}\) has the bivariate Normal distribution \(N\left(\binom{0}{0},\begin{bmatrix}1&\rho\\ \rho&1\end{bmatrix}\right)\).

(a) Prove that the conditional distributions of \(V\), given \(U\), is \(N(\rho U,1-\rho^{2})\).

(b) Show that \(EU^{2}V^{2}=1+2\rho^{2}\), \(EU^{3}V=EVU^{3}=3\rho\).

**Ex. 6.17**.: Assume \(X_{j}\), \(j\geq 1\), are i.i.d. real-valued, with \(EX_{j}=\mu\), \(\text{var}(X_{j})=\sigma^{2}>0\), \(EX_{j}^{4}<\infty\). Prove that

(a) \(\sqrt{n}(s^{2}-\sigma^{2})\xrightarrow{\mathscr{L}}N(0,E(X_{1}-\mu)^{4}-\sigma ^{4})\), and

(b) \(\sqrt{n}(\frac{1}{s}-\frac{1}{\sigma})\xrightarrow{\mathscr{L}}N(0,[E(X_{1}- \mu)^{4}-\sigma^{4}]\cdot[1/4\sigma^{6}])\).

[Hint: (a) Consider \(U_{j}=X_{j}-\mu\), \(j\geq 1\), \(s^{2}=(\frac{n}{n-1})\frac{1}{n}\sum_{j=1}^{1}(U_{j}-\overline{U})^{2}=(\frac{ n}{n-1})\left[\frac{1}{n}(\sum_{j=1}^{n})U_{j}^{2}-\overline{U}^{2}\right]\), so that \(\sqrt{n}(s^{2}-\sigma^{2})-\sqrt{n}(\frac{1}{n}\Sigma_{j=1}^{n}(U_{j}^{2}- \sigma^{2}))\xrightarrow{P}0\).

(b) \(\sqrt{n}(\frac{1}{s}-\frac{1}{\sigma})-\sqrt{n}[H(\overline{z})-H(\sigma^{2})] \xrightarrow{P}0\), where \(z_{j}=U_{j}^{2}\), \(EZ_{j}=\sigma^{2}\), \(H(z)=z^{-1/2}\), \(H(\overline{Z})=(\frac{1}{n}\Sigma_{j=1}^{n}U_{j}^{2})^{-1/2}\), \(H(\sigma^{2})=1/\sigma\). Apply Corollary 6.2.]

**Ex. 6.18**.: (a) Let \(X_{n}\) have the discrete uniform distribution on \(\{0,\frac{1}{n},\frac{2}{n},\dots,1\}\) (i.e., \(P(X_{n}=\frac{k}{n})=\frac{1}{n+1}\)\((k=0,1,\dots,n)\). Show that \(X_{n}\) converges in distribution to the uniform distribution on \([0,1]\) (with constant density 1).

(b) Use (a) to prove that \((1/n+1)\sum_{k=0}^{n}f(k/n)\to\int_{0}^{1}f(x)dx\) for every continuous function \(f\) on \([0,1]\).

(c) Extend (b) to the case of all bounded measurable \(f\) on \([0,1]\) with a finite set of discontinuities.

**Ex. 6.19**.: \(Y_{n}=\min\{X_{i}:1\leq i\leq n\}\) where \(X_{1},X_{2},\dots\) are i.i.d. beta \(\mathscr{B}_{e}(\alpha,1)\) random variables.

(a) What is the distribution of \(Y_{n}\)?

(b) Find a value of \(\alpha\) such that \(Y_{n}\) converges in distribution to a nondegenerate law.

**Ex. 6.20** (**Fieller's Method for the Estimation of a Ratio).: Let \(\theta_{1}\), \(\theta_{2}\) be unknown parameters, \(\theta_{2}>0\). The problem is to obtain a confidence interval for \(\rho=\frac{c+d\theta_{1}}{\theta_{2}}\)\((d\neq 0)\), based on asymptotically jointly Normal estimators \(\hat{\theta}_{1}\), \(\hat{\theta}_{2}\)of \(\theta_{1}\), \(\theta_{2}\), with available consistent estimates \(\widehat{W}_{11}\), \(\widehat{W}_{22}\), \(\widehat{W}_{12}\) of \(W_{11}=\operatorname{var}(\hat{\theta}_{1})\), \(W_{22}=\operatorname{var}(\hat{\theta}_{2})\), \(W_{12}=\operatorname{cov}(\hat{\theta}_{1},\hat{\theta}_{2})\).

1. Use the delta method to obtain a confidence interval for \(\rho\) of confidence level \(1-\alpha\).
2. _(Fieller method.)_ Let \(W_{2}=\operatorname{var}(c+d\hat{\theta}_{1}-\rho\hat{\theta}_{2})=d^{2}W_{11} +\rho^{2}W_{22}-2d\rho W_{12}\). Let \(\widehat{W}^{2}=d^{2}\widehat{W}_{11}+\rho^{2}\widehat{W}_{22}-2d\rho\widehat {W}_{12}\). Show that \((c+d\hat{\theta}_{1}-\rho\hat{\theta}_{2})/\widehat{W}\) is AN \(N(0,1)\), so that \(\{(c+d\hat{\theta}_{1}-\rho\hat{\theta}_{2})^{2}\leq\widehat{W}^{2}z_{1-\frac {\alpha}{2}}^{2}\}\) has asymptotic probability \(1-\alpha\) (\(z_{\beta}\) being the \(\beta\)-th quantile of \(N(0,1)\)). Use the last relation to obtain a quadratic equation in \(\rho\) with roots \(\hat{\rho}_{\ell}<\hat{\rho}_{u}\), and argue that \([\hat{\rho}_{\ell},\hat{\rho}_{u}]\) is a confidence interval for \(\rho\) with asymptotic level \(1-\alpha\). [See Fieller 1940.]

**Exercises for Sect. 6.6**

**Ex. 6.21**.: (a) Prove that \(\delta_{n}\to 0\) implies \(m_{2}/b_{n}^{2}\to 0\) (see (6.70)). [Hint: Let \(|X_{j}-X_{j^{\prime}}|=c>0\) for some \(j\), \(j^{\prime}\). Then \(\delta_{n}\geq(c/2)^{2}/b_{n}^{2}\)\(\forall\)\(n\geq\max\{j,j^{\prime}\}\), since either \(|X_{j}-\overline{X}|\geq c/2\) or \(|X_{j^{\prime}}-\overline{X}|\geq c/2\). Also, \(X_{i}^{2}\leq 2X_{1}^{2}+4\delta_{n}b_{n}^{2}\) (\(1\leq i\leq n\)), so that \(m_{2}/b_{n}^{2}\leq 2X_{1}^{2}/b_{n}^{2}+4\delta_{n}\to 0\).]
2. Verify (6.95).
3. Prove that the random ellipse given by (6.97) is a confidence region for \((\alpha,\beta)\) with asymptotic confidence coefficient \(1-\theta\), if (6.72) holds for the linear regression model (6.12).

**Ex. 6.22** (Normal Correlation Model).: Show that, under Normal errors \(\varepsilon_{j}\) in (6.12),

1. \(\hat{\alpha}\), \(\hat{\beta}\) are M.L.E.'s, and
2. the confidence region (6.100) for \((\alpha,\beta)\) has exact confidence coefficient \(1-\theta\), for all \(n\geq 3\), assuming that \(X_{j}\)'s, \(1\leq j\leq n\), are not all the same.

**Ex. 6.23**.: For the heteroscedastic linear regression model (6.101) of Example 6.7,

1. prove (6.114) under the assumption (6.113),
2. prove (6.116) under the assumption (6.115),
3. derive (6.117), assuming (6.113) and (6.115), and show that \(\widetilde{D}_{n}\) in (6.118) is a confidence region for \((\alpha,\beta)\) with asymptotic confidence coefficient \(1-\theta\).

**Ex. 6.24**.: For the heteroscedastic linear regression model considered under Remark 6.14 (i.e., in (6.101) with \(E\varepsilon_{j}^{2}=\eta_{j}^{2}\sigma^{2}\) (\(1\leq j\leq n\)), where \(\eta_{j}^{2}>0\) are known, but \(\sigma^{2}>0\) is unknown), prove that \(\hat{\sigma}^{2}\) defined in (6.119) is an unbiased estimator of \(\sigma^{2}\).

**Ex. 6.25**.: (a) Write out a detailed proof of Corollary 6.3.
2. Show that the _hypothesis of the correlation model holds if \((X_{j},Y_{j})\), \(j\geq 1\), are i.i.d. observations from a bivariate normal distribution,_ where \(\alpha+\beta_{j}X_{j}\) is the conditional mean of \(Y_{j}\), given \(X_{j}\). [_Hint:_ Use the fact that the conditional distribution of \(Y_{j}\), given \(X_{j}\), is Normal with mean \(\alpha+\beta_{j}X_{j}\) (for appropriate \(\alpha\), \(\beta\)) and \(\varepsilon_{j}:=Y_{j}-\alpha-\beta X_{j}\) uncorrelated with \(X_{j}\). This says that the conditional distribution of \(\varepsilon_{j}:=Y_{j}-\alpha-\beta X_{j}\), given \(X_{j}\) is \(N(0,\sigma^{2})\), so that \(\varepsilon_{j}\) is independent of \(X_{j}\).]

**Ex. 6.26** (Linear Regression Passing Through a Point):
* Consider the linear regression (6.12) with \(\alpha=0\) (i.e., the regression line passes through the origin \((0,0)\)). Show that the least squares estimator of \(\beta\) is \(\hat{\beta}=\sum_{1}^{n}X_{j}Y_{j}/\sum_{1}^{n}X_{j}^{2}\) and that \(\hat{\beta}\) is asymptotically Normal \(N(\beta,\sigma^{2}/\sum_{1}^{n}X_{j}^{2})\) if \(\hat{\delta}_{n}:=\max\{X_{j}^{2}/\sum_{1}^{n}X_{j}^{2}:1\leq j\leq n\}\to 0\) as \(n\to\infty\).
* Consider the linear regression passing through a given point \((x_{0},y_{0})\), i.e., \(Y_{j}-y_{0}=\beta(X_{j}-x_{0})\) (so that \(\alpha=y_{0}-\beta x_{0}\)). Show that the least squares estimator of \(\beta\) is \(\hat{\beta}=\sum_{1}^{n}(X_{j}-x_{0})(Y_{j}-y_{0})/\sum_{1}^{n}(X_{j}-x_{0})^{2}\) and that \(\hat{\beta}\) is asymptotically Normal \(N(\beta,\sigma^{2}/\sum_{1}^{n}(X_{j}-x_{0})^{2})\), provided \(\bar{\delta}_{n}:=\max\{(X_{j}-x_{0})^{2}/\sum_{1}^{n}(X_{i}-x_{0})^{2}:1\leq j \leq n\}\to 0\).

**Ex. 6.27**.: Suppose in (6.12) \(X\) represents blood pressure and \(Y\) platelet calcium in 38 people with normal blood pressure. Let \(\overline{X}=84.5\), \(\overline{Y}=105.8\), \(\sum(X_{j}-\overline{X})^{2}=2397.5\), \(\sum(X_{j}-\overline{X})Y_{j}=2792.5\).

* Find the usual \(90\,\%\) confidence band for the regression line.
* Find the simultaneous confidence bound with asymptotic confidence level at least \(90\,\%\) using Scheffe's method (See Proposition 6.9):

**Ex. 6.28** (Transformation to Linear Regression): Consider the relations (i) \(y=\beta_{0}\exp\{\beta_{1}x\}\) (\(\beta_{0}>0\), \(\beta_{1}\in\mathbb{R}\)), \(x\in\mathbb{R}\); (ii) \(y=\beta_{0}x^{\beta_{1}}\) (\(\beta_{0}>:0\), \(\beta_{1}\in\mathbb{R}\)), \(x>0\). In both (i) and (ii) \(y>0\) and a _multiplicative error_\(\zeta\) may be more appropriate than a linear one.

* Find appropriate transformations of observations \(Y_{j}\), \(X_{j}\), \(1\leq j\leq n\), to make them linear regression models.
* Assuming that \(\zeta_{j}\) are i.i.d. positive multiplicative errors, obtain appropriate estimates of \(\beta_{0}\), \(\beta_{1}\) and find their asymptotic distributions in each case (i), (ii).

**Ex. 6.29**.: This exercise shows that Theorem 6.3(a) breaks down if the density \(f\) is not continuous (and positive) at \(\zeta_{p}\). Consider a density whose right-hand limit \(f(\zeta_{\frac{1}{2}^{+}})\) and left-hand limit \(f(\zeta_{\frac{1}{2}^{-}})\) are positive but unequal (at the median \(\zeta_{\frac{1}{2}}\)). Show that \(\sqrt{n}(\hat{\zeta}_{\frac{1}{2}}-\zeta_{\frac{1}{2}})\) does not converge to a Normal distribution. [Hint: If \(z>0\), \(\mathbf{1}_{\{X_{i}\leq\zeta_{\frac{1}{2}+\frac{z}{\sqrt{n}}}\}}\)\((1\leq i\leq n)\) are i.i.d. Bernoulli, taking the value \(1\) with probability \(\frac{1}{2}+\int_{\zeta_{1/2}}^{\zeta_{1/2}+\frac{z}{\sqrt{n}}}f(x)dx=\frac{1} {2}+\frac{z}{\sqrt{n}}f\left(\zeta_{\frac{1}{2}^{+}}\right)+o(n^{-\frac{1}{2}})\). Hence, by following the steps (6.127)-(6.128), \(P\left(\sqrt{n}\left(\hat{\zeta}_{1/2}-\zeta_{1/2}\right)\leq z\right)\)\(\longrightarrow\)\(P\left(\frac{1}{2f\left(\zeta_{\frac{1}{2}^{+}}\right)}Z\leq z\right)\), where \(Z\) is standard Normal. But for \(z<0\), the same argument shows that \(P(\sqrt{n}(\hat{\zeta}_{1/2}-\zeta_{1/2})\leq z)\longrightarrow P(\frac{1}{2f( \zeta_{\frac{1}{2}^{-}})}Z\leq z)\).]

**Ex. 6.30**.: Fix \(p\in(0,1)\). Assume that \(\zeta_{p}\) is uniquely defined and \(F\) is three times continuously differentiable in a neighborhood of \(\zeta_{p}\), with \(f(\zeta_{p})\equiv F^{\prime}(\zeta_{p})=0\), and \(f^{\prime\prime}(\zeta_{p})\neq 0\). Show that \(P(n^{\frac{1}{6}}(\hat{\zeta}_{p}-\zeta_{p})\leq z)\to\Phi(cz^{3})\) where \(\Phi\) is the standard Normal distribution function and \(c=\frac{1}{6}f^{\prime\prime}(\zeta_{p})/\sqrt{p(1-p)}\).

**Ex. 6.31**.: Find the asymptotic distribution of the sample _inter-quartile range_\(\hat{\zeta}_{0.75}-\hat{\zeta}_{0.25}\) assuming that the hypothesis of Theorem 6.3(a) holds for \(p=0.75\) and \(p=0.25\).

### Exercises for Sect. 6.8

**Ex. 6.32**.: Prove that the sum of two independent mean-zero random variables, one of which is Normal while the other is non-Normal, can not be Normal. [Hint: Use characteristic functions.]

**Ex. 6.33**.: Derive Theorem 6.2 as a corollary of Theorem 6.4, and show that the condition \(\delta_{n}\to 0\) in (6.72) is also necessary for (6.91) to hold, if \(\varepsilon_{i}\)'s are not Normal.

### Exercises for Sect. 6.9

**Ex. 6.34**.: Verify the computations in Example 6.9.

**Ex. 6.35**.: In Example 6.10, compare the relative performances of \(\hat{\zeta}_{1/2}\) and \(\overline{X}\) as the parameters \(\varepsilon\) and \(\tau\) vary over their respective ranges (\(0<\varepsilon<1\), \(\tau>0\)). [Hint: Write \(\sigma=\sqrt{\tau}\).] Let \(h(\varepsilon,\sigma):=e_{\hat{\zeta}_{1/2},\overline{X}}\), as given in Example 6.10. Fix \(\varepsilon\in(0,1)\). Show that \(\partial h/\partial\sigma\) is negative for \(0<\sigma<1\), positive for \(\sigma>1\), and vanishes at \(\sigma=1\). Also, \(h(\varepsilon,\sigma)\to\infty\) as \(\sigma\downarrow 0\) and as \(\sigma\uparrow\infty\). Thus,

1. for every given \(\varepsilon\in(0,1)\), \(h(\varepsilon,\sigma)\) has the unique minimum value \(h(\varepsilon,1)=\frac{2}{\pi}<1\), and
2. there exist \(\sigma_{i}(\varepsilon)\) (\(i=1,2\)), \(0<\sigma_{1}(\varepsilon)<1<\sigma_{2}(\varepsilon)\) such that \(h(\varepsilon,\sigma_{i}(\varepsilon))=1\) (\(i=1,2\)), \(h(\varepsilon,\sigma)>1\) if \(\sigma\in(0,\sigma_{1}(\varepsilon))\cup(\sigma_{2}(\varepsilon),\infty)\), \(h(\varepsilon,\sigma)>1\) if \(\sigma_{1}(\varepsilon)<\sigma<\sigma_{2}(\varepsilon)\). In particular, \(h(\varepsilon,\sigma)\geq\frac{2}{\pi}\)\(\forall\)\(\varepsilon,\sigma\), but the lower bound of \(e_{\overline{X},\hat{\zeta}_{1/2}}=1/h(\varepsilon,\sigma)\) is zero.]

Conclusion: \(\hat{\xi}_{1/2}\) is _robust_ compared to \(\overline{X}\) in the Normal mixture model.

### Exercise for Sect. 6.10

**Ex. 6.36**.: Take a random sample of size \(n=50\) from the standard Normal distribution. Treating this as a random sample from an unknown population, find an approximate \(95\,\%\) confidence interval for \(\xi_{.25}\).

**Exercise for Sect. 6.11**.:
1. In the so-called _structural model_ as considered in (6.169), (6.170), (6.174) with a stochastic \(X^{*}\), prove (6.172). [Hint: Show that the quantities \(n^{-1}\sum(X^{*}_{j}-\overline{X}^{*})(\varepsilon_{j}-\overline{\epsilon})\), \(n^{-1}\sum(X^{*}_{j}-\overline{X}^{*})(\eta_{j}-\overline{\eta})\) and \(n^{-1}\sum(\eta_{j}-\overline{\eta})(\varepsilon_{j}-\overline{\varepsilon})\) have all zero means and variances converging to zero.]
2. Prove (6.172) for the _functional model,_ assuming \(n^{-1}\sum_{1\leq j\leq n}(X^{*}_{j}-\overline{X}^{*})^{2}\to\sigma_{x^{*}}^{2}>0\), as \(n\to\infty\).
3. In the functional model, show that \(\hat{\beta}\) is a consistent estimator of \(\beta\) if and only if \(n^{-1}\sum_{1\leq j\leq n}\) (\(X^{*}_{j}-\overline{X}^{*})^{2}\to\infty\) as \(n\to\infty\).

**Ex. 6.38**.: Derive (6.177), (6.178) under the given assumptions of the structural bivariate Normal model.

**Ex. 6.39**.: Carry out a simulation study of the structural bivariate Normal model with \(\kappa=0.9\), and compute \(\hat{\beta}=\hat{\beta}_{yx}\) and \(\hat{\beta}_{xy}^{-1}\).

## References

* Bahadur (1966) Bahadur, R. R. (1966). A note on quantiles in large samples. _Annals of Mathematical Statistics, 37_(3), 577-580.
* Berkson (1950) Berkson, J. (1950). Are there two regressions? _Journal of the American Statistical Association, 45_(250), 164-180.
* Bhattacharya and Bhattacharyya (1994) Bhattacharya, R., & Bhattacharyya, D. K. (1994). Proxy versus instrumental variable methods in regression with one regressor missing. _Journal of Multivariate Analysis, 47_, 123-138.
* Bhattacharya and Ranga Rao (2010) Bhattacharya, R., & Ranga Rao, R. (2010). _Normal approximation and as symptomatic expansions_. SIAM classics in applied mathematics (Vol. 64). Philadelphia: SIAM
* Bhattacharya and Waymire (2007) Bhattacharya, R., & Waymire, E. (2007). _A basic course in probability theory_. New York: Springer.
* Bickel and Doksum (2001) Bickel, P. J., & Doksum, K. (2001). _Mathematical statistics_ (2nd ed.). Upper Saddle River, NJ: Prentice Hall.
* Billingsley (1986) Billingsley, P. (1986). _Probability and measure_. Wiley.
* Dasgupta (2008) Dasgupta, A. (2008). _Asymptotic theory of statistics and probability_. New York: Springer.
* Diaconis and Efron (1983) Diaconis, P., & Efron, B. (1983). Computer intensive methods in statistics. _Division of Biostatistics, 248_(5), 116-126.
* Ferguson (1996) Ferguson, T. S. (1996). _A course in large sample theory_. London: Taylor & Francis.
* Feller (1940) Feller, E. C. (1940). The biological standardisation of insulin. _Journal of the Royal Statistical Society (Supplement), 1_, 1-54.
* Fuller (1987) Fuller, W. A. (1987). _Measurement error models_. Wiley.
* Ghosh (1971) Ghosh, J. K. (1971). A new proof of the Bahadur representation of quantiles and an application. _Annals of Mathematical Statistics, 42_(6), 1957-1961.
* Ito and Ganguly (2006) Ito, M., & Ganguly, J. (2006). Diffusion kinetics of Cr in olivine and 53Mn-53Cr thermochronology of early solar system objects. _Geochimica et Cosmochimica Acta, 70_, 799-806.
* Kiefer (1967) Kiefer, J. (1967). On Bahadur's representation of sample quantiles. _Annals of Mathematical Statistics, 38_(5), 1323-1342.
* Riersol (1950) Riersol, O. (1950). Identifiability of a linear relation between variables which are subject to error. _Econometrica, 18_, 375-389.
* Scheffe (1959) Scheffe, H. (1959). _The analysis of variance_. New York: Wiley.
* Serfling (1980) Serfling, R. (1980). _Approximation theorems of mathematical statistics_. New York: Wiley.
* Shevtsova (2010) Shevtsova, I. G. (2010). An improvement of convergence rate estimates in the Lyapunov theorem. _Doklady Mathematics, 82_(3), 862-864.

## Chapter 7 Large Sample Theory of Estimation in Parametric Models

**Abstract** The main focus of this chapter is the asymptotic Normality and optimality of the maximum likelihood estimator (MLE), under regularity conditions. The Cramer-Rao lower bound for the variance of unbiased estimators of parametric functions is shown to be achieved asymptotically by the MLE. Also derived are the asymptotic Normality of -estimators and the asymptotic behavior of the Bayes posterior.

### 7.1 Introduction

We begin with the derivation of an important inequality known as the _Cramer-Rao bound_ which gives a lower bound to the expected squared error (or, variance) of unbiased estimators of parametric functions under certain regularity conditions. This bound is attained by some estimators of special parametric functions in exponential families (see Chap. 4, Part I for definition of exponential families). In general the bound is rarely attained (exactly). We will see, however, that this lower bound is attained in an asymptotic sense by maximum likelihood estimators (MLEs) in large samples, provided certain regularity conditions hold. This shows that the MLEs are asymptotically optimal under these conditions.

**Notation** In order to avoid having to make the arguments separately for the discrete and absolutely continuous cases (or, for a mix of them), we will in this section write  for the density of the observed random vector \(\mathbf{X}\) w.r.t. a sigma finite measure \(\mu\) on the set \(\mathbf{x}\) of all values \(\mathbf{x}\) in the range \(\mathscr{X}\) of \(\mathbf{X}\). In the absolutely continuous case, \(g(\mathbf{x};\theta)\) is the _probability density function (p.d.f.)_ of \(\mathbf{X}\) (and \(\mu\) is Lebesgue measure: \(\mu(dx)=dx\)). In the discrete case \(g(\mathbf{x};\theta)\) is the _probability mass function (p.m.f.)_ of \(\mathbf{X}\) on a countable set \(\mathscr{X}\) (and \(\mu\) is the counting measure, \(\int_{\mathscr{X}}h(\mathbf{x})g(\mathbf{x};\theta)\mu(d\mathbf{x})=\sum_{ \mathbf{x}\in\mathscr{X}}h(\mathbf{x})g(\mathbf{x};\theta)\)). Sometimes, when \(\mathbf{X}=(X_{1},\ldots,X_{n})\) with \(X_{1},\ldots,X_{n}\) i.i.d. (or when \(X_{i}\)'s are \(n\) observations in a time series), we write \(f_{n}(\mathbf{x};\theta)\), instead of \(g(\mathbf{x};\theta)\), to indicate the _sample size_. In the i.i.d. case, \(f(x;\theta)\) always indicates the density of a single observation \(X_{i}\). The range of \(X_{i}\) is \(\mathscr{X}\) in this case, while that of \(\mathbf{X}\) is \(\mathscr{X}=\mathscr{X}^{n}\).

The random variables, or vectors, are all defined on a measurable space \((\Omega,\mathscr{F})\) and, for each value \(\theta\) of the parameter, they are governed by a probability law \(P_{\theta}\) on \((\Omega,\mathscr{F})\). The expectation under \(P_{\theta}\) is denoted \(E_{\theta}\). Also, \(Y\stackrel{{\mathscr{L}}}{{\sim}}Q\) denotes "\(Y\) has distribution \(Q\)".

### The Cramer-Rao Bound

**Theorem 7.1** (Cramer-Rao Information Inequality): _Suppose \(\mathbf{X}\) has p.d.f. \(g(\mathbf{x};\theta)\) (with respect to a sigma-finite measure \(\mu\)) on a space \(\mathscr{X}\) satisfying_

1. \(g(\mathbf{x};\theta)>0\)__\(\forall\)__\(\mathbf{x}\in\mathscr{X}\)_,_ \(\forall\)__\(\theta\in\Theta\)_--_ _an open interval,_
2. \(\int_{\mathscr{X}}\frac{d}{d\theta}g(\mathbf{x};\theta)\mu(d\mathbf{x})=\frac {d}{d\theta}\int_{\mathscr{X}}g(\mathbf{x};\theta)\mu(d\mathbf{x})\equiv 0\)_._

_Let_ \(T=t(\mathbf{X})\) _be a (real-valued) statistic with_ \(c(\theta):=E_{\theta}T\)_,_ \(E_{\theta}T^{2}<\infty\)__\(\forall\)__\(\theta\)_, satisfying_
3. \(\frac{d}{d\theta}\int_{\mathscr{X}}t(\mathbf{x})g(\mathbf{x};\theta)\mu(d \mathbf{x})\equiv c^{\prime}(\theta)=\int_{\mathscr{X}}t(\mathbf{x})\frac{d \theta(\mathbf{x};\theta)}{d\theta}\mu(d\mathbf{x})\)_._

_Then_

\[\mathrm{var}_{\theta}T\equiv E_{\theta}(T-c(\theta))^{2}\geq\frac{c^{\prime}( \theta)^{2}}{E_{\theta}\left(\frac{d\log g(\mathbf{X};\theta)}{d\theta}\right) ^{2}}\,. \tag{7.1}\]

_Proof._ Condition (ii) may be restated as

\[0=E_{\theta}\frac{d\log g(\mathbf{X};\theta)}{d\theta}\qquad\qquad\left(\equiv \int_{\mathscr{X}}\frac{\frac{d}{d\theta}g(\mathbf{x};\theta)}{g(\mathbf{x}; \theta)}g(\mathbf{x};\theta)\mu(d\mathbf{x})\right) \tag{7.2}\]

Similarly, condition (iii), together with (ii), says

\[\mathrm{cov}_{\theta}\left(T,\frac{d\log g(\mathbf{X};\theta)}{d\theta}\right) =c^{\prime}(\theta), \tag{7.3}\]

The inequality (7.1) now follows from (7.3) by the Cauchy-Schwartz inequality.

\(\sqcap\)\(\sqcup\)

_Remark 7.1_: Assumptions (ii), (iii) in the theorem concern the interchangeability of the order of differentiation and integration. If \(\mathscr{X}\) depends on \(\theta\), then these generally do not hold. For example, let \(\mathbf{X}=(X_{1},\ldots,X_{n})\) with \(X_{j}\)'s i.i.d. uniform on \((0,\theta)\). Then \(\mathscr{X}=(0,\theta)^{n}\). Take \(T=M_{n}\equiv\max\{X_{1},\ldots,X_{n}\}\). Note that \(g(\mathbf{x};\theta)=1/\theta^{n}\) on \(\mathscr{X}\), so that \(dg(\mathbf{x};\theta)/d\theta=-n/\theta^{n+1}\), \(\int_{\mathscr{X}}\frac{d}{d\theta}g(\mathbf{x};\theta)d\mathbf{x}\equiv\int_ {(0,\theta)^{n}}(-n/\theta^{n+1})d\mathbf{x}=-\frac{n}{\theta^{n+1}}\cdot \theta^{x}=-n/\theta\neq 0\). Also, letting \(c(\theta)=E_{\theta}M_{n}=\frac{n}{n+1}\theta\) (Exercise 7.1), one has \(c^{\prime}(\theta)=\frac{n}{n+1}\), while \(\int_{\mathscr{X}}t(\mathbf{x})(dg(\mathbf{x};\theta)/d\theta)d\mathbf{x}=E_{ \theta}(Td\log g\) (\(\mathbf{X};\theta)/d\theta)=E_{\theta}\left(M_{n}d[-n\log\theta]/d\theta \right)=E_{\theta}\!\left(M_{n}(-\frac{n}{\theta})\right)=-\frac{n^{2}}{n+1}\,.\)

_Remark 7.2_: The most common \(\mathbf{X}\) encountered in this course is that of a random vector \(\mathbf{X}=(X_{1},\ldots,X_{n})\) with \(X_{j}\)'s i.i.d. and having a (common) p.d.f. or p.m.f. \(f(x;\theta)\). In this case

\[g(\mathbf{x};\theta)=\prod_{j=1}^{n}f(x_{j};\theta),\quad\log g(\mathbf{x}; \theta)=\sum_{j=1}^{n}\log f(x_{j};\theta),\quad x=(x_{1},\ldots,x_{n}). \tag{7.4}\]so that (ii) or (7.2) is equivalent to

\[E_{\theta}\frac{d\log f(X_{1};\theta)}{d\theta}\qquad\left(\equiv\int_{\mathscr{X}} \frac{df(x;\theta)}{d\theta}dx,\text{ or }\sum_{x\in\mathscr{X}}\frac{df(x; \theta)}{dx}\right)=0\]

Also,

\[E_{\theta}\left(\frac{d\log g(\mathbf{X};\theta)}{d\theta}\right) ^{2}\equiv\operatorname{var}_{\theta}\left(\frac{d\log g(\mathbf{X};\theta)}{ d\theta}\right)\] \[=\operatorname{var}_{\theta}\left(\sum_{j=1}^{n}\frac{d\log f(X_{ j};\theta)}{d\theta}\right)=\sum_{j=1}^{n}\operatorname{var}_{\theta}\left( \frac{d\log f(X_{j};\theta)}{d\theta}\right)\] \[=n\operatorname{var}_{\theta}\left(\frac{d\log f(X_{1};\theta)}{ d\theta}\right)=nE_{\theta}\left(\frac{d\log f(X_{1};\theta)}{d\theta}\right)^{2}.\]

The quantity \(I(\theta):=E_{\theta}\left(\frac{d\log f(X_{1};\theta)}{d\theta}\right)^{2}\) is called, in the case of i.i.d. observations, the _information per observation,_ and \(E_{\theta}\left(\frac{d\log g(\mathbf{X};\theta)}{d\theta}\right)^{2}=nI(\theta)\) is the _information contained in the whole sample_. We have derived the following corollary of the theorem.

**Corollary 7.1**.: _Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. with a common density \(f(x;\theta)\) with respect to a sigma-finite measure \(\mu\), and let \(\mathbf{X}=(X_{1},\ldots,X_{n})\). Then, under the hypothesis of the theorem above, one has_

\[\operatorname{var}_{\theta}(T)\geq\frac{(c^{\prime}(\theta))^{2}}{nI(\theta)}, \tag{7.5}\]

_where \(I(\theta)=E_{\theta}\left(\frac{d\log f(X_{1};\theta)}{d\theta}\right)^{2}\)._

_Remark 7.3_.: The hypothesis of Corollary 7.1 holds if the following conditions hold:

\((R_{0})\quad\)\(f(\mathbf{x};\theta)>0\)\(\forall\)\(\mathbf{x}\in\mathscr{X}\), \(\theta\in\Theta\), where \(\mathscr{X}\) does not depend on \(\theta\).

\((R_{1})\quad\)\(\frac{df(\mathbf{x};\theta)}{d\theta}\) is continuous on \(\Theta\)\((\forall\)\(\mathbf{x}\ \in\mathscr{X})\), and for each \(\theta_{0}\in\Theta\) there exists \(h_{1}=h_{1}(\theta_{0})>0\) such that \(\sup\left\{|\frac{df(\mathbf{x};\theta)}{d\theta}|:|\theta-\theta_{0}|\leq h_ {1}\right\}\leq g_{1}(\mathbf{x})\), where \(\int_{\mathscr{X}}g_{1}(\mathbf{x})\mu(d\mathbf{x})<\infty\).

\((R_{2})\quad\)For each \(\theta_{0}\in\Theta\) there exists \(h_{2}=h_{2}(\theta_{0})>0\) such that \(\sup\left\{|\frac{df(\mathbf{x};\theta)}{d\theta}|:|\theta-\theta_{0}|\leq h_ {2}\right\}\leq g_{2}(\mathbf{x})\) where \(E_{\theta_{0}}g_{2}^{2}(\mathbf{X})\)\((\equiv\int_{\mathscr{X}}g_{2}^{2}(\mathbf{x})f(\mathbf{x};\theta_{0})\mu\)\((d\mathbf{x})<\infty)\).

_Remark 7.4_.: If one estimates some function \(h(\theta)\) by \(T\), and \(E_{\theta}T=c(\theta)\), then under the hypothesis of Theorem 7.1 one has

\[E_{\theta}(T-h(\theta))^{2} =E_{\theta}(T-c(\theta))^{2}+(c(\theta)-h(\theta))^{2}\] \[\geq\frac{(c^{\prime}(\theta))^{2}}{E_{\theta}\left(\frac{d\log g (\mathbf{X};\theta)}{d\theta}\right)}+(c(\theta)-h(\theta))^{2} \tag{7.6}\]_Remark 7.5_.: It may be shown in the case of i.i.d. observations \(X_{1},\ldots,X_{n}\) from a one-parameter exponential family (with p.d.f. or p.m.f. \(f(x;\theta)=c_{1}(\theta)h(x)e^{\theta t_{1}(x)},\theta\in\Theta\)--an open interval) that \((R_{0})\)-\((R_{2})\) hold. Hence (7.5) is valid in this case. However, the lower bound is attained only by \(T=t(X)=\sum_{j=1}^{n}t_{1}(X_{j})\) (and its linear functions). (Exercise 7.2(b).)

### 7.3 Maximum Likelihood: The One Parameter Case

As mentioned in Sect. 7.1, one of the main reasons why the Rao-Cramer bound is so important is that the MLE's, under appropriate regularity conditions, attain this bound in an asymptotic sense. In other words, the MLE is asymptotically optimal or, _asymptotically efficient_. The following result is a precise statement of this fact.

**Theorem 7.2** (CLT for the MLE).: _Let \(f(x;\theta)\) be the p.d.f. with respect to a sigma-finite measure \(\mu\) of the common distribution of i.i.d. random variables \(X_{1},X_{2},\ldots,\) for \(\theta\) belonging to an open interval \(\Theta\). Assume that \(f(x;\theta)>0\)\(\forall\)\(x\in\mathscr{X}\), \(\forall\)\(\theta\in\Theta\), where \(P_{\theta}(X_{1}\in\mathscr{X})=1\)\((\forall\)\(\theta\in\Theta)\). Assume also the following_

\((A_{1}^{*})\)  \(\theta\to f(x;\theta)\) _is three times continuously differentiable on_ \(\Theta\)_,_ \(\forall\)__\(x\in\mathscr{X}\)_;_

\((A_{2}^{*})\)  \(\int_{\mathscr{X}}\frac{d}{d\theta}f(x;\theta)\mu(dx)=0\)__\((\equiv\frac{d}{d\theta}\int_{\mathscr{X}}f(x;\theta)\mu(dx))\)_,_ \(\int_{\mathscr{X}}\frac{d^{2}}{d\theta^{2}}f(x;\theta)\mu(dx)=0\)__\((\equiv\frac{d^{2}}{d\theta^{2}}\int_{\mathscr{X}}f(x;\theta)\mu(dx))\)_;_

\((A_{3}^{*})\)  \(0<I(\theta):=E_{\theta}\left(\frac{d\log f(X_{1};\theta)}{d\theta}\right)^{2}< \infty\)__\(\forall\)__\(\theta\in\Theta\)_;_

\((A_{4}^{*})\)  _For each_ \(\theta_{0}\in\Theta\) _there exists an_ \(\varepsilon\)__\((=\varepsilon(\theta_{0}))>0\) _such that_ \(|\frac{d^{3}\log f(x;\theta)}{d\theta^{3}}|^{3}\leq g(x)\)__\(\forall\)__\(\theta\in[\theta_{0}-\varepsilon\)_,_ \(\theta_{0}+\varepsilon]\)_, where_ \(\int_{\mathscr{X}}g(x)f(x;\theta_{0})\mu(dx)<\infty\)_._

\((A_{5}^{*})\)  _The likelihood equation (writing_ \(\ell(\theta):=\sum_{j=1}^{n}\log f(X_{j};\theta)\))_

\[\frac{d\ell(\theta)}{d\theta}=0,\qquad\text{i.e., }\sum_{j=1}^{n}\frac{d\log f(X_{j}; \theta)}{d\theta}=0, \tag{7.7}\]

_has a consistent solution_ \(\hat{\theta}_{n}\)_._

_Then \(\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\) converges in distribution to \(N(0,1/I(\theta_{0}))\) if \(\theta_{0}\) is the true parameter value (i.e., under \(P_{\theta_{0}}\)), as \(n\to\infty\)._

Proof.: Using a Taylor expansion of \(d\ell(\theta)/d\theta\) around \(\theta=\theta_{0}\), one has

\[0=\frac{d\ell(\theta)}{d\theta}|_{\theta=\hat{\theta}_{n}}=\frac{d\ell(\theta) }{d\theta}|_{\theta=\theta_{0}}+(\hat{\theta}_{n}-\theta_{0})\frac{d^{2}\ell( \theta)}{d\theta^{2}}|_{\theta=\theta_{0}}+\frac{(\hat{\theta}_{n}-\theta_{0}) ^{2}}{2}\frac{d^{3}\ell(\theta)}{d\theta^{3}}|_{\theta=\theta^{*}} \tag{7.8}\]

where \(\theta^{*}\) lies in the line segment joining \(\theta_{0}\) and \(\hat{\theta}_{n}\). Thus,

\[\sqrt{n}(\hat{\theta}_{n}-\theta_{0})=\frac{-\frac{1}{\sqrt{n}}\left(\frac{d \ell(\theta)}{d\theta}\right)_{\theta=\theta_{0}}}{\frac{1}{n}\left(\frac{d^{2} \ell(\theta)}{d\theta^{2}}\right)_{\theta=\theta_{0}}+\frac{1}{n}\frac{\hat{ \theta}_{n}-\theta_{0}}{2}\frac{d^{3}\ell(\theta)}{d\theta^{3}}|_{\theta= \theta^{*}}}\;. \tag{7.9}\]First note that

\[E_{\theta}\frac{d^{2}\log f(X_{j};\ \theta)}{d\theta^{2}} =\int_{\mathscr{X}}\frac{d}{d\theta}\left(\frac{\frac{d}{d\theta}f( x;\theta)}{f(x;\theta)}\right)f(x;\theta)dx\] \[=\int_{\mathscr{X}}\frac{f^{\prime\prime}(x;\theta)f(x;\theta)-(f^ {\prime}(x;\theta))^{2}}{f^{2}(x;\theta)}f(x;\theta)dx\] \[=\int_{\mathscr{X}}f^{\prime\prime}(x;\theta)\mu(dx)-\int_{ \mathscr{X}}\left(\frac{d\log f(x;\theta)}{d\theta}\right)^{2}f(x;\theta)\mu(dx)\] \[=-I(\theta), \tag{7.10}\]

using the second relation in \((A_{2}^{*})\) for the last step. Thus by the strong law of large numbers (SLLN),

\[\frac{1}{n}\left(\frac{d^{2}\ell(\theta)}{d\theta^{2}}\right)_{\theta=\theta_ {0}}\equiv\frac{1}{n}\sum_{j=1}^{n}\left(\frac{d^{2}\log f(X_{j};\theta)}{d \theta^{2}}\right)_{\theta=\theta_{0}}\longrightarrow\ -I(\theta_{0}) \tag{7.11}\]

with probability one (under \(P_{\theta_{0}}\)). By \((A_{4}^{*})\), \(\frac{1}{n}|\frac{d^{3}\ell(\theta)}{d\theta^{3}}|\leq\frac{1}{n}\sum_{j=1}^{n }g(X_{j})\ \forall\ \theta\in[\theta_{0}-\varepsilon,\theta_{0}+\varepsilon]\), but by \((A_{5}^{*})\), \(P_{\theta_{0}}(|\theta_{0}-\theta^{*}|\leq\varepsilon)\leq P_{\theta_{0}}(| \hat{\theta}_{n}-\theta_{0}|\leq\varepsilon)\to 1\) as \(n\to\infty\). Using this and the fact that \(\frac{1}{n}\sum_{j=1}^{n}g(X_{j})\to E_{\theta_{0}}g(X_{1})<\infty\), one finds that \(\frac{1}{n}|\frac{d3\ell(\theta)}{d\theta^{3}}|_{\theta^{*}}\) remains bounded in probability as \(n\to\infty\) (See Definition 7.1 below). Since, also, \(\theta_{n}-\theta_{0}\to 0\) in probability, one gets

\[\frac{1}{n}\,\frac{\hat{\theta}_{n}-\theta_{0}}{2}\,\frac{d^{3}\ell(\theta)}{ d\theta^{3}}\longrightarrow 0\quad\text{in}\quad P_{\theta_{0}}\text{-probability as }\,n\to\infty.\]

Using this and (7.11) in (7.9), one gets

\[\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\approx\frac{\frac{1}{\sqrt{n}}\left( \frac{d\ell(\theta)}{d\theta}\right)_{\theta=\theta_{0}}}{I(\theta_{0})} \equiv\frac{\frac{1}{\sqrt{n}}\sum_{j=1}^{n}\left(\frac{d\log f(X_{j};\theta) }{d\theta}\right)_{\theta=\theta_{0}}}{I(\theta_{0})}\,, \tag{7.12}\]

where \(\approx\) indicates that the difference between its two sides goes to zero. But \(\left(\frac{d\log f(X_{j};\theta)}{d\theta}\right)_{\theta=\theta_{0}}\) (\(j=1,2,\dots\)) is an i.i.d. sequence of random variables having mean zero (by \((A_{2}^{*})\)) and finite variance \(I(\theta_{0})\) (by \((A_{3}^{*})\)). Therefore, by the classical CLT, writing \(Z\) for a Normal random variable \(N(0,I(\theta_{0}))\),

\[\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\stackrel{{\mathscr{X}}}{{ \longrightarrow}}\frac{Z}{-I(\theta_{0})}\stackrel{{\mathscr{X}}}{ {=}}N\left(0,\frac{I(\theta_{0})}{I^{2}(\theta_{0})}\right)=N\left(0,\frac{1} {I(\theta_{0})}\right).\]

Q.E.D. 

_Remark 7.6_.: In particular, the above theorem says that (under \(P_{\theta_{0}}\)) the "asymptotic variance" of \((\hat{\theta}_{n}-\theta_{0})\) is \(\frac{1}{nI(\theta_{0})}\), which is the Rao-Cramer lower bound for the expected squared error of any estimator of \(\theta\) (at \(\theta=\theta_{0}\)).

_Example 7.1_.: The hypothesis of Theorem 7.2 is satisfied by one-parameter exponential families with p.d.f.

\[f(x;\theta)=C(\theta)h(x)e^{\theta t(x)},\qquad x\in\mathscr{X} \tag{7.13}\]where \(\mathscr{X}\) does not depend on \(\theta\), \(h(x)>0\) for \(x\in\mathscr{X}\). This form of the exponential family is said to be in natural parametric form with \(\theta\) being the natural parameter (belonging to an open interval \(\Theta\)). The likelihood equation is

\[\frac{d\log}{d\theta}\left[C^{n}(\theta)\left(\prod_{j=1}^{n}h(X_{j})\right)exp \left\{\theta\sum_{j=1}^{n}t(X_{j})\right\}\right]=0, \tag{7.14}\]

i.e.,

\[-\frac{d\log C(\theta)}{d\theta}=\frac{1}{n}\sum_{j=1}^{n}t(X_{j}). \tag{7.15}\]

But

\[-\frac{d\log C(\theta)}{d\theta} =-\frac{C^{\prime}(\theta)}{C(\theta)}=-C^{\prime}(\theta)\int h (x)\exp\{\theta t(x)\}\mu(dx)\] \[=-\frac{d}{d\theta}\left(\int h(x)\exp\{\theta t(x)\}\mu(dx) \right)^{-1}\int h(x)\exp\{\theta t(x)\}\mu(dx)\] \[=\frac{\int t(x)h(x)\exp\{\theta t(x)\}\mu(dx)}{\int h(x)\exp\{ \theta t(x)\}\mu(dx)}=\int t(x)f(x;\theta)\mu(dx)\] \[=E_{\theta}t(X_{1}). \tag{7.16}\]

Hence, one may rewrite (7.15) as the equation

\[E_{\theta}t(X_{1})=\frac{1}{n}\sum_{j=1}^{n}t(X_{j}). \tag{7.17}\]

Now, the second derivative of the log likelihood function is (see the left side of the first equation in (7.15) and the relations (7.16))

\[n\frac{d^{2}\log C(\theta)}{d\theta^{2}}=\] \[= \frac{n[(\int t(x)\exp\{\theta t(x)\}\mu(dx))^{2}-(\int h(x)\exp\{ \theta t(u)\}\mu(dx))(\int t^{2}(x)h(x)e^{\theta t(x)}\mu(dx))]}{(\int h(x) \exp\{\theta t(x)\}\mu(dx))^{2}}\] \[=-n\ \mathrm{var}_{\theta}\ t(X_{1})<0. \tag{7.18}\]

Thus the log likelihood function is strictly concave, and, therefore, (7.15) cannot have more than one root \(\hat{\theta}\). We will show later (see Theorem 7.5) that in this case the likelihood equation has a unique solution \(\hat{\theta}\) on a set with \(P_{\theta_{0}}\)-probability tending to \(1\), as \(n\to\infty\), under \(\theta_{0}\), and that \(\hat{\theta}\) is consistent. Thus Theorem 7.2 applies.

_Example 7.2 (Logistic)._ In this case

\[f(x;\theta)=\frac{e^{-(x-\theta)}}{(1+e^{-(x-\theta)})^{2}},\qquad-\infty<x< \infty\quad(\Theta=(-\infty,\infty)),\]

so that the likelihood equation \((d\log f_{n}(\mathbf{X};\theta)/d\theta=0)\) is

\[\frac{d}{d\theta}\left\{-\sum_{j=1}^{n}(X_{j}-\theta)-2\sum_{j=1}^{n}\log \left(1+e^{-(X_{j}-\theta)}\right)\right\}=0, \tag{7.19}\]or

\[n-2\sum_{j=1}^{n}\frac{e^{-(X_{j}-\theta)}}{1+e^{-(X_{j}-\theta)}}=0\]

or,

\[n-2\sum_{j=1}^{n}\left\{1-\frac{1}{1+e^{-(X_{j}-\theta)}}\right\}=0,\]

or,

\[n-2n+2\sum_{j=1}^{n}\frac{1}{1+e^{-(X_{j}-\theta)}}=0 \tag{7.20}\]

or,

\[\sum_{j=1}^{n}\frac{1}{1+e^{-(X_{j}-\theta)}}=\frac{n}{2}\,. \tag{7.21}\]

Since the left hand side is strictly decreasing in \(\theta\), and goes to \(n\) as \(\theta\downarrow-\infty\) and to \(0\) as \(\theta\uparrow\infty\), there is a unique solution \(\hat{\theta}\) of (7.20). Also the left hand side of (7.19) (which gives the first derivative of \(\log f_{n}(\mathbf{X};\theta)\)) is positive for \(\theta<\hat{\theta}\) and negative for \(\theta>\hat{\theta}\). Hence \(\hat{\theta}\) is the MLE. The hypothesis of Theorem 7.2 can be easily verified for this case.

_Example 7.3 (Double Exponential)._ Here

\[f(x;\theta)=\frac{1}{2}e^{-|x-\theta|},\qquad-\infty<x<\infty,\quad\theta\in(- \infty,\infty).\]

\[\log f_{n}(\mathbf{X};\theta)=-n\log 2-\sum_{j=1}^{n}|X_{j}-\theta|, \tag{7.22}\]

which is maximized by that value of \(\theta\) for which \(\varphi(\theta):=\sum_{j=1}^{n}|X_{j}-\theta|\) is minimized. The minimizing value is the median of \(X_{1},X_{2},\ldots,X_{n}\); i.e., \(\hat{\theta}\) is the \(\frac{n+1}{2}\)th observation when \(X_{1},X_{2},\ldots,X_{n}\) are arranged in increasing order, for the case when \(n\) is odd. If \(n\) is even, then \(\hat{\theta}\) is any number between the \(\frac{n}{2}\)th and \((\frac{n}{2}+1)\)th observation (arranged in increasing order). To see this, let \(X_{(1)}<X_{(2)}<\cdots<X_{(n)}\) denote the ordering of the \(n\) observations. Let \(\theta\in[X_{(r)},X_{(r+1)})\), and let \(\delta>0\) be such that \(\theta+\delta\in[X_{(r)},X_{(r+1)}]\). Then \(\varphi(\theta+\delta)=\varphi(\theta)+r\delta-(n-r)\delta=\varphi(\theta)+(2r -n)\delta\). Thus \(\varphi(\theta+\delta)<\varphi(\theta)\) if and only if \(r<\frac{n}{2}\), and \(\varphi(\theta+\delta)=\varphi(\theta)\) if \(r=\frac{n}{2}\). In other words, if \(n\) is odd, then \(\varphi(\theta)\) is strictly decreasing on \((-\infty,X_{([\frac{n}{2}]+1)}]\), and strictly increasing on \((X_{([\frac{n}{2}]+1)},\infty)\), attaining its unique minimum at \(\theta=X_{([\frac{n}{2}]+1)}\). On the other hand, if \(n\) is even, \(\varphi(\theta)\) is strictly decreasing on \((-\infty,X_{(\frac{n}{2})}]\), strictly increasing on \((X_{(\frac{n}{2}+1)},\infty)\), and constant on \([X_{(\frac{n}{2})},X_{(\frac{n}{2}+1)}]\).

Although the hypothesis of Theorem 7.2 does not hold, the conclusion holds. Note that

\[\frac{d\log f(x;\theta)}{d\theta}=\begin{cases}-1&\text{for }x<\theta\\ 1&\text{for }x>\theta.\end{cases} \tag{7.23}\]

\[I(\theta)=E_{\theta}\left(\frac{d\log f(X_{1};\theta)}{d\theta}\right)^{2}=E_ {\theta}1=1. \tag{7.24}\]We know from Theorem 6.3 that, under \(P_{\theta}\),

\[\sqrt{n}(\hat{\theta}-\theta)\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N\left(0,\frac{1}{4f^{2}(\theta;\theta)}\right)=N(0,1).\]

Hence \(\hat{\theta}\) is asymptotically efficient. This also explains the poor performance of the mean relative to the median: \(e_{\theta,\overline{X}}=2\). (Note that \(\sqrt{n}(\overline{X}-\theta)\rightarrow\mathscr{N}(0,\sigma^{2})\) where \(\sigma^{2}=E_{\theta}(X_{1}-\theta)^{2}=\frac{1}{2}\int_{-\infty}^{\infty}x^{2 }e^{-|x|}dx=\int_{0}^{\infty}x^{2}e^{-x}dx=\varGamma_{3}=2\)).

_Example 7.4_.: \[f(x;\theta) =\left(\frac{1}{\sqrt{2\pi\theta^{2}}}\right)^{n}e^{-\frac{1}{2 \theta^{2}}(x-\theta)^{2}},\qquad-\infty<x<\infty,\] \[\Theta=\{\theta\in\mathbb{R}^{1},\theta\neq 0\}=\mathbb{R}^{1} \setminus\{0\}.\]

The likelihood equation \(d\log f_{n}(\mathbf{X};\theta)/d\theta=0\) may be expressed as

\[-\frac{n}{\theta}+\frac{\sum_{1}^{n}X_{i}^{2}}{\theta^{3}}-\frac{\sum X_{i}}{ \theta^{2}}=0,\]

or,

\[\theta^{2}+\overline{X}\theta-m_{2}=0,\qquad\left(\overline{X}:=\frac{\sum X_ {i}}{n},\ m_{2}:=\frac{\sum X_{i}^{2}}{n}\right), \tag{7.25}\]

whose roots are

\[\theta_{+}=\frac{-\overline{X}+\sqrt{\overline{X}^{2}+4m_{2}}}{2}\,,\qquad \theta_{-}=\frac{-\overline{X}-\sqrt{\overline{X}^{2}+4m_{2}}}{2}\,. \tag{7.26}\]

Note that (1) as \(\theta\to 0\), \(f_{n}(\mathbf{X};\theta)\to 0\) (except for \(\mathbf{X}=(\theta,\theta,\dots,\theta)\) which has zero probability), and (2) as \(\theta\rightarrow\pm\infty\), \(\max_{\mathbf{x}}f_{n}(\mathbf{x};\theta)=(2\pi\theta^{2})^{-n/2}\to 0\). There are, therefore, at least two extrema: at least one between \(-\infty\) and \(0\), and at least one between \(0\) and \(+\infty\). However, the only critical points are \(\theta_{+},\theta_{-}\). Hence there are _exactly_ two extrema. Note that \(\theta_{+}>0\) and \(\theta_{-}<0\). Now at \(\hat{\theta}=\theta_{+},\theta_{-}\) one has

\[\log f_{n}(\mathbf{X};\theta) =-\frac{n}{2}\log 2\pi-\frac{n}{2}\log\hat{\theta}^{2}-\frac{n}{2 \hat{\theta}^{2}}\left(\hat{\theta}^{2}-2\overline{X}\hat{\theta}+m_{2}\right)\] \[=-\frac{n}{2}\log 2\pi-\frac{n}{2}-\frac{n}{2}\log\hat{\theta}^{2} -\frac{n}{2\hat{\theta}^{2}}\left(\hat{\theta}^{2}-\overline{X}\hat{\theta}\right)\] \[=-\frac{n}{2}\log 2\pi-\frac{n}{2}\log\hat{\theta}^{2}-n+\frac{n}{2 \hat{\theta}}\,\overline{X}. \tag{7.27}\]

If \(\overline{X}>0\), then (7.2) is larger at \(\hat{\theta}=\theta_{+}\), and if \(\overline{X}<0\), then (7.2) is larger at \(\hat{\theta}=\theta_{-}\). Therefore, the _MLE_ is

\[\hat{\theta}_{n}=\begin{cases}\theta_{+}&\text{if }\overline{X}>0\\ \theta_{-}&\text{if }\overline{X}\leq 0.\end{cases} \tag{7.28}\]

Theorem 7.2 applies to _consistent_ solutions (i.e., an estimator (sequence) \(\hat{\theta}_{n}\) which is consistent and solves the likelihood Eq. (7.2) on a set whose probability goes to \(1\) as \(n\rightarrow\infty\)). How does one obtain such a solution? There is no general method that works in all cases, from the computational point of view. However,under the hypothesis of Theorem 7.2 one may show that there exists essentially one consistent sequence of solutions.1 A general numerical method which often works is the following: find a consistent estimator (sequence) \(\tilde{\theta}_{n}\) (e.g., by the method of moments if that is applicable, or by using the median in case of a symmetric location problem etc.); taking \(\tilde{\theta}_{n}\) as a trial solution, use iteration (or _Newton-Raphson method_):

Footnote 1: See Lehmann and Casella (1998, p. 448).

\[\tilde{\theta}_{n}^{(i+1)}({\bf x}) =\tilde{\theta}_{n}^{(i)}({\bf x})-\left(\frac{d\log f_{n}({\bf x} ;\theta)/d\theta}{d^{2}\log f_{n}({\bf x};\theta)/d\theta^{2}}\right)_{\tilde{ \theta}_{n}^{(i)}}\] \[\tilde{\theta}_{n}^{(0)} =\tilde{\theta}_{n},\qquad(i=0,1,2,\ldots). \tag{7.29}\]

As the following result shows, one may use (7.29) with \(i=0\) and use the estimator \(\tilde{\theta}_{n}^{(1)}\) in place of a consistent root, in situations involving multiple roots and computational difficulties.

**Definition 7.1**.: A sequence of random variables (or, vectors) \(Y_{n}\) is said to be _bounded in probability_ if for every \(\varepsilon>0\) there exists \(A=A(\varepsilon)\) such that

\[P(|Y_{n}|>A)<\varepsilon.\]

Note that \(Y_{n}\), \(n\geq 1\), is bounded in probability if \(E|Y_{n}|\) is a bounded sequence.

**Theorem 7.3**.: _Suppose that the hypothesis of Theorem 7.2 concerning \(f(x;\theta)\) holds. Assume that \(\tilde{\theta}_{n}\) is an estimator (sequence) such that for each \(\theta_{0}\in\Theta,\sqrt{n}(\tilde{\theta}_{n}-\theta_{0})\) is bounded in \(P_{\theta_{0}}\)-probability. Then the estimator (sequence)_

\[\delta_{n}=\tilde{\theta}_{n}-\left(\frac{d\log f_{n}({\bf X};\theta)/d\theta} {d^{2}\log f_{n}({\bf X};\theta)/d\theta^{2}}\right)_{\tilde{\theta}_{n}} \tag{7.30}\]

_is asymptotically efficient._

Proof.: One has, by a Taylor expansion of \((d\log f_{n}({\bf X};\theta)/d\theta)_{\tilde{\theta}}\)

\[\sqrt{n}(\delta_{n}-\theta_{0})=\sqrt{n}(\tilde{\theta}_{n}-\theta_{0})-\frac {\frac{1}{\sqrt{n}}(d\log f_{n}({\bf X};\theta/d\theta)_{\theta_{0}}}{\frac{1 }{n}(d^{2}\log f_{n}({\bf X};\theta)/d\theta^{2})_{\tilde{\theta}_{n}}}\]

\[-\frac{\sqrt{n}(\tilde{\theta}_{n}-\theta_{0})\frac{1}{n}(d^{2}\log f_{n}({\bf X };\theta)/d\theta^{2})_{\theta_{n}^{*}}}{\frac{1}{n}(d^{2}\log f_{n}({\bf X}; \theta)/d\theta^{2})_{\tilde{\theta}_{n}}} \tag{7.31}\]

where \(\theta_{n}^{*}\) lies between \(\theta_{0}\) and \(\tilde{\theta}_{n}\). Now, under \(P_{\theta_{0}}\),

\[\frac{\frac{1}{n}(d^{2}\log f_{n}({\bf X};\theta)/d\theta^{2})_{\theta_{n}^{*} }}{\frac{1}{n}(d^{2}\log f_{n}({\bf X};\theta)/d\theta^{2})_{\tilde{\theta}_{ n}}}-1\stackrel{{ P_{\theta_{0}}}}{{\longrightarrow}}0. \tag{7.32}\]

One may prove (7.32)) by expanding the numerator and the denominator around \(\theta_{0}\) and using \(A_{4}^{*}\).

Using (7.32) in (7.31) one gets

\[\sqrt{n}(\delta_{n}-\theta_{0})-\left\{-\frac{\frac{1}{\sqrt{n}}(d\log f_{n}( {\bf X};\theta)/d\theta)_{\theta_{0}}}{\frac{1}{n}(d^{2}\log f_{n}({\bf X}; \theta)/d\theta^{2})_{\tilde{\theta}_{n}}}\right\}\begin{array}{c}P_{\theta _{0}}\\ \longrightarrow\end{array}0. \tag{7.33}\]

But the expression within curly brackets converges in distribution to \(N\left(0,\frac{1}{I(\theta_{0})}\right)\) under \(P_{\theta_{0}}\).

_Remark 7.7_.: Suppose now that \(T_{n}\) is bounded in probability and \(U_{n}\) converges in probability to zero (written \(U_{n}\stackrel{{ P}}{{\longrightarrow}}0\) or, \(U_{n}=o_{p}(1)\)). Then one can easily show that \(U_{n}T_{n}\) converges in probability to zero as \(n\to\infty\). [Proof: \(P(|U_{n}T_{n}|>\delta)\leq P(|T_{n}|>A_{\varepsilon})+P(|U_{N}|>\delta/A_{ \varepsilon})<\varepsilon+\varepsilon_{n}\) for all sufficiently large \(n\), with \(\varepsilon_{n}\to 0\)]. This fact has been used in getting (7.33) from (7.31) and (7.32).

_Remark 7.8_.: A variant of (7.30) is

\[\delta^{\prime}_{n}=\tilde{\theta}_{n}+\frac{(d\log f_{n}({\bf X};\theta)/d \theta)_{\tilde{\theta}_{n}}}{nI(\tilde{\theta}_{n})}\,. \tag{7.34}\]

Since \(-(d^{2}\log F_{n}({\bf X}:\theta)/d\theta^{2})_{\tilde{\theta}_{n}}/nI(\tilde{ \theta}_{n})\) converges to \(1\) in probability, by Slutsky's Lemma, \(\delta_{n}\) and \(\delta^{\prime}_{n}\) converge asymptotically to the same distribution.

### The Multi-Parameter Case

We will now prove a multi-parameter analogue of the Cramer-Rao inequality.

**Theorem 7.4** (Multi-Parameter Cramer-Rao Information Inequality).: _Let \({\bf X}\) be a random quantity whose distribution has density \(g({\bf x};\theta)\) with respect to a sigma finite measure \(\nu\) (on a sigma field \(\mathscr{A}\) on the range space \(\mathscr{X}\) of \({\bf X}\)); here \(\theta\in\Theta\)--a nonempty open subset of \(\mathbb{R}^{p}\). Assume the following: (Notation: \(\boldsymbol{\theta}=(\theta_{1},\theta_{2},\ldots,\theta_{p})\))_

\((B_{1})\)__\(C=\{{\bf x};g({\bf x};\boldsymbol{\theta})>0\}\) _is independent of_ \(\theta\)_._ \((B_{2})\)__\(\boldsymbol{\theta}\to g({\bf x};\boldsymbol{\theta})\) _is once differentiable on_ \(\Theta\)_,_ \(\forall\;{\bf x}\) _(outside a_ \(\nu\)_-null set)._ \((B_{3})\)__\(\int_{C}(\partial/\partial\theta_{r})g({\bf x};\boldsymbol{\theta})\nu(dx)=( \partial/\partial\theta_{r})\int_{C}g({\bf x};\boldsymbol{\theta})\nu(dx)(=0)\)__\(\forall\)__\(r=1,2,\ldots,p\) _and_ \(\forall\)__\(\boldsymbol{\theta}\in\Theta\)_._ \((B_{4})\)__\(\text{ The matrix }\mathscr{I}(\boldsymbol{\theta})=((E_{\theta}[(\partial\log g({\bf X}; \boldsymbol{\theta})/\partial\theta_{r})\cdot(\partial\log g({\bf X}; \boldsymbol{\theta})/\partial\theta_{r^{\prime}})]))\) _is non-singular._

_Let now \(T=(T_{1},T_{2},\ldots,T_{p})\) be an unbiased estimator of \(\boldsymbol{\theta}\) (i.e., \(E_{\boldsymbol{\theta}}T_{r}=\theta_{r}\)\(\forall\)\(\boldsymbol{\theta}\in\Theta\)) such that \(E_{\boldsymbol{\theta}}(T_{r})^{2}<\infty\)\(\forall\)\(r\) and \(\forall\)\(\boldsymbol{\theta}\). Assume further that_

\((B_{5})\)__\(\int_{C}T_{r}({\bf x})(\partial/\partial\theta_{r^{\prime}})g({\bf x}; \boldsymbol{\theta})\nu(dx)=(\partial/\partial\theta_{r^{\prime}})\int_{C}T_{r }(x)g({\bf x};\boldsymbol{\theta})\nu(dx)\)__\((=\delta_{rr^{\prime}}\)_,_

_Kronecker's delta) \(\forall\)\(r,r^{\prime}\) and \(\forall\)\(\boldsymbol{\theta}\)._

_Then one has the inequality_

\[\Sigma(\boldsymbol{\theta})\geq\mathscr{I}^{-1}(\boldsymbol{\theta}), \tag{7.35}\]

_where \(\Sigma(\theta)=((\operatorname{cov}_{\boldsymbol{\theta}}(T_{r},T_{r^{\prime} })))\), and the inequality (7.35) means the \(\Sigma(\boldsymbol{\theta})-\mathscr{I}^{-1}(\boldsymbol{\theta})\) is a nonnegative definite matrix._

Proof.: It follows from \((B_{3})\) that

\[E_{\theta}(\partial\log g({\bf X};\theta)/\partial\theta_{r^{\prime}}=\int_{C} \frac{(\partial g({\bf x};\boldsymbol{\theta})/\partial\theta_{r^{\prime}})}{g( {\bf x}^{\prime};\boldsymbol{\theta})}g({\bf x};\boldsymbol{\theta})\nu(d{ \bf x})=0. \tag{7.36}\]

Hence, using \((B_{5})\),

\[\operatorname{cov}_{\boldsymbol{\theta}}(T_{r},(\partial/\partial \theta_{r^{\prime}})\log g({\bf X};\boldsymbol{\theta})) =E_{\theta}[T_{r}(\partial\log g({\bf X};\boldsymbol{\theta})/ \partial\theta_{r^{\prime}})]\] \[=\int_{C}T_{r}({\bf x})(\partial g({\bf x};\boldsymbol{\theta})/ \partial\theta_{r^{\prime}})\nu(d{\bf x})=\delta_{rr^{\prime}}. \tag{7.37}\]Let \({\bf a}=(a_{1},a_{2},\ldots,a_{p})\), \({\bf b}=(b_{1},b_{2},\ldots,b_{p})\) be arbitrary vectors in \({\mathbb{R}}^{p}\). Then, by (7.37),

\[{\rm cov}_{\boldsymbol{\theta}}\left(\sum_{1}^{p}a_{r}T_{r},\sum_{1}^{p}b_{r}( \partial\log g({\bf X};{\boldsymbol{\theta}})/\partial\theta_{r})\right)=\sum_ {1}^{p}a_{r}b_{r}. \tag{7.38}\]

Therefore, by Schwartz' inequality, writing \(\sigma_{rr^{\prime}}({\boldsymbol{\theta}})={\rm cov}_{\boldsymbol{\theta}}(T_ {r},T_{r^{\prime}})\), \({\mathscr{I}}({\boldsymbol{\theta}})=(({\mathscr{I}}_{rr^{\prime}}({ \boldsymbol{\theta}}))\),

\[\left(\sum_{1}^{p}a_{r}b_{r}\right)^{2} \leq {\rm var}_{\boldsymbol{\theta}}\left(\sum_{1}^{p}a_{r}T_{r} \right)\cdot{\rm var}_{\boldsymbol{\theta}}\left(\sum_{1}^{p}b_{r}(\partial \log g({\bf X};{\boldsymbol{\theta}})/\partial\theta_{r})\right) \tag{7.39}\] \[= \left(\sum_{r=1}^{p}\sum_{r^{\prime}=1}^{p}a_{r}a_{r^{\prime}}\ \ \sigma_{rr^{\prime}}({\boldsymbol{\theta}})\right)\left(\sum_{r=1}^{p}\sum_{r^{ \prime}=1}^{p}b_{r}b_{r^{\prime}}{\mathscr{I}}_{rr^{\prime}}({\boldsymbol{ \theta}})\right).\]

Writing \(\langle{\bf a},{\bf b}\rangle=\sum_{r=1}^{p}a_{r}b_{r}\) (the Euclidean inner product), one may rewrite (7.39) as

\[\langle{\bf a},{\bf b}\rangle^{2}\leq\langle{\bf a},\Sigma({\boldsymbol{ \theta}}){\bf a}\rangle\,\langle{\bf b},{\mathscr{I}}({\boldsymbol{\theta}}){ \bf b}\rangle\,. \tag{7.40}\]

Now choose \({\bf b}={\mathscr{I}}^{-1}({\boldsymbol{\theta}}){\bf a}\) to get

\[\left\langle{\bf a},{\mathscr{I}}^{-1}({\boldsymbol{\theta}}){\bf a}\right\rangle^{2}\leq\langle{\bf a},\Sigma({\boldsymbol{\theta}}){\bf a}\rangle\,\langle{\mathscr{I}}^{-1}({\boldsymbol{\theta}}){\bf a},{\bf a}\rangle\,,\]

or,

\[\langle{\bf a},\Sigma({\boldsymbol{\theta}}){\bf a}\rangle\geq\left\langle{ \bf a}{\mathscr{I}}^{-1}({\boldsymbol{\theta}}){\bf a}\right\rangle\]

or,

\[\left\langle{\bf a},(\Sigma({\boldsymbol{\theta}})-{\mathscr{I}}^{-1}({ \boldsymbol{\theta}})){\bf a}\right\rangle\geq 0\ \forall\ \,{\bf a}\in{ \mathbb{R}}^{p}. \tag{7.41}\]

_Remark 7.9_.: As in the one-parameter case, (see proof of Cramer-Rao Theorem 7.1) the following conditions ensure the validity of the inequality (7.35) at \({\boldsymbol{\theta}}={\boldsymbol{\theta}}_{0}:(B_{1}),(B_{2})\) hold, as well as the following conditions

\[(R_{1}^{\prime}):\quad\left|\frac{\partial g({\bf x};{\boldsymbol{\theta}})}{ \partial{\boldsymbol{\theta}}_{r}}\right|\leq g_{1}({\bf x})\ (\forall\ r=1,2,\ldots,p)\ {\rm in\ a\ neighborhood\ of}\ {\boldsymbol{\theta}}_{0},\ {\rm where}\ g_{1}({\bf x})\]

does not depend on values of \({\boldsymbol{\theta}}\) in this neighborhood, and \(\int g_{1}({\bf x})\nu(d{\bf x})<\infty\).

\[(R_{2}^{\prime}):\quad{\rm In\ a\ neighborhood\ of\ }{\boldsymbol{\theta}}={ \boldsymbol{\theta}}_{0}\ {\rm one\ has}\left|\frac{\partial g({\bf x};{\boldsymbol{\theta}})}{\partial{ \boldsymbol{\theta}}_{r}}\right|\leq g_{2}({\bf x})\ (\forall\ r=1,2,\ldots,p),\]

where \(g_{2}({\bf x})\) does not depend on \({\boldsymbol{\theta}}\) in this neighborhood, and \(\int\frac{g_{2}^{2}({\bf x})}{g({\bf x};{\boldsymbol{\theta}}_{0})}\nu(d{\bf x})\)\(<\infty\).

_Remark 7.10_.: In case \({\bf X}=(X_{1},X_{2},\ldots,X_{n})\) where \(X_{1},X_{2},\ldots,X_{n}\) are i.i.d. with common p.d.f. \(\mu)\)\(f(x;{\boldsymbol{\theta}})\), one takes \(g({\bf x};{\boldsymbol{\theta}})=f_{n}({\bf x};{\boldsymbol{\theta}})=\prod_{j= 1}^{n}f(x_{j};{\boldsymbol{\theta}})\). The information inequality then becomes

\[\Sigma({\boldsymbol{\theta}})\geq\tfrac{1}{n}{\mathscr{I}}_{1}^{-1}({ \boldsymbol{\theta}}), \tag{7.42}\]

where \({\mathscr{I}}_{1}({\boldsymbol{\theta}})=((E_{\boldsymbol{\theta}}\frac{ \partial\log f(X_{1};{\boldsymbol{\theta}})}{\partial\theta_{r}}\cdot\frac{ \partial\log f(X_{1};{\boldsymbol{\theta}})}{\partial\theta_{r^{\prime}}}))\).

_Remark 7.11_.: Letting \({\bf a}\) in (7.41) be the vector with \(1\) as the \(j\)th component and zeros elsewhere, one gets

\[{\rm var}_{\theta}(T_{j})\geq\left({\mathscr{I}}^{-1}({\boldsymbol{\theta}}) \right)_{jj}. \tag{7.43}\]In the context of Remark 7.10 this becomes

\[\operatorname{var}_{\theta}(T_{j})\geq\frac{1}{n}\left(\mathscr{I}_{1}^{-1}( \boldsymbol{\theta})\right)_{jj}. \tag{7.44}\]

Note also that (7.41) (and, therefore, (7.35)) says that _the variance of an unbiased estimator of a linear parametric function \(g(\theta)=\sum_{1}^{p}a_{r}\theta_{r}\) is at least \(\left\langle\mathbf{a},\mathscr{I}^{-1}(\boldsymbol{\theta})\mathbf{a}\right\rangle\)_ when \(\boldsymbol{\theta}\) is the true parameter value.

**Definition 7.2**.: Assume that \((B_{1}),(B_{2})\), hold for \(f(x;\theta)\) (in place of \(g(\mathbf{x};\boldsymbol{\theta})\)), and that \((R^{\prime}_{1}),(R^{\prime}_{2})\) hold for \(f_{n}(\mathbf{x};\boldsymbol{\theta})=\prod_{j=1}^{n}f(x_{j};\boldsymbol{ \theta})\) (in place of \(g(\mathbf{x};\boldsymbol{\theta})\)). Then a sequence of estimators \(T_{n}\) of \(\boldsymbol{\theta}\) is said to be _asymptotically efficient_ (in Fisher's sense) if, \(\forall\;\boldsymbol{\theta}\in\Theta\),

\[\sqrt{n}(T_{n}-\boldsymbol{\theta})\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N(\mathbf{o},\mathscr{I}_{1}^{-1}(\boldsymbol{\theta})), \qquad\text{under }P_{\theta}. \tag{7.45}\]

_Remark 7.12_.: Apart from the motivation for the above definition provided in Remark 7.11, one may show that _an asymptotically efficient estimator \(T_{n}\) concentrates more probability around the true parameter value_, _asymptotically, than a competing asymptotically normal estimator \(\delta_{n}\) which is not asymptotically efficient_. More precisely,

\[\lim_{n\to\infty}P_{\boldsymbol{\theta}}(\sqrt{n}(T_{n}- \boldsymbol{\theta})\in C) =\Phi_{\mathscr{I}^{-1}(\boldsymbol{\theta})}(C)\geq\Phi_{V( \boldsymbol{\theta})}(C)\] \[\equiv\lim_{n\to\infty}P_{\boldsymbol{\theta}}(\sqrt{n}(\delta_{n }-\boldsymbol{\theta})\in C) \tag{7.46}\]

for all _symmetrically convex sets_ \(C\) (i.e., convex \(C\) satisfying: \(C=-C=\{-\mathbf{x}:\mathbf{x}\in C\}\)). Here \(V(\boldsymbol{\theta})\) is the dispersion matrix of the limiting normal distribution of \(\sqrt{n}(\delta_{n}-\boldsymbol{\theta})\) under \(P_{\boldsymbol{\theta}}\). The two equalities in (7.46) are consequences of convergence in distribution, while the inequality follows from a theorem of T.W. Anderson (_Proc. Amer. Math. Soc._**6**, 170-176): _If \(V_{1}(\boldsymbol{\theta})\geq V_{2}(\boldsymbol{\theta})\), \(V_{1}(\boldsymbol{\theta})\), \(V_{2}(\boldsymbol{\theta})\) nonnegative definite, then \(\Phi_{V_{1}(\boldsymbol{\theta})}(C)\leq\Phi_{V_{2}(\boldsymbol{\theta})}(C)\)\(\forall\) symmetric convex sets \(C\)._

To provide an alternative justification in terms of risk functions, define a loss function \(L(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime})\) (loss when \(\boldsymbol{\theta}\) is the true parameter value and \(\boldsymbol{\theta}^{\prime}\) is the estimated value) to be _negative unimodal_ (or _bowl shaped_) if \(L\geq 0\) and \(\{\mathbf{z}:\mathbf{z}\in\mathbb{R}^{p}\), \(L(\mathbf{z})\leq r\}\) is convex for all \(r\geq 0\); a loss function \(L(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime})\) is _symmetric_ if \(L(\mathbf{z})=L(-\mathbf{z})\)\(\forall\;\mathbf{z}\in\mathbb{R}^{p}\). Let \(\mathscr{L}_{0}\) denote the class of all negative unimodal symmetric and bounded loss functions. For each \(L\in\mathscr{L}_{0}\) define the loss function (sequence) \(L_{n}(\boldsymbol{\theta},\boldsymbol{\theta}^{\prime})=L(\sqrt{n}( \boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}))\). Then one can show, using T.W. Anderson's inequality and weak convergence, that

\[\lim_{n\to\infty}E_{\theta}L_{n}(\boldsymbol{\theta},T_{n}) =\int_{\mathbb{R}^{p}}L(\mathbf{z})d\Phi_{\mathscr{I}^{-1}( \boldsymbol{\theta})}(\mathbf{z})\leq\int_{\mathbf{R}^{p}}L(\mathbf{z})d\Phi _{V(\boldsymbol{\theta})}(\mathbf{z})\] \[=\lim_{n\to\infty}E_{\theta}L_{n}(\boldsymbol{\theta},\delta_{n}) \tag{7.47}\]

\(\forall\;L\in\mathscr{L}_{0}\) (provided \(V(\boldsymbol{\theta})\geq I^{-1}(\boldsymbol{\theta})\)).

Finally note that the loss function \(L_{n}\) is a "normalization" of \(L\), which is in the class \(\mathscr{L}_{0}\), and such that the limits in (7.47) become _discriminating_. On the other hand,

\[\lim_{n\to\infty}E_{\boldsymbol{\theta}}L(\boldsymbol{\theta}-T_{n})=L(0)=\lim _{n\to\infty}E_{\boldsymbol{\theta}}L(\boldsymbol{\theta}-\delta_{n}). \tag{7.48}\]

[MISSING_PAGE_EMPTY:12593]

where \(\theta^{*}\) lies on the line segment joining \(\theta\) and \(\theta_{0},\) and therefore \(|\theta^{*}-\theta_{0}|\leq\delta.\) The second summand on the right side of (7.51) may be expressed, by the strong law of large numbers, as

\[\sum_{r=1}^{p}(\theta_{r}-\theta_{0r})\eta_{r,n},\quad\eta_{r,n}\to{\rm a.s.} \quad E_{\theta_{0}}\left(\frac{\partial\log f(X_{1};\boldsymbol{\theta})}{ \partial\theta_{r}}\right)_{\theta_{0}}=0. \tag{7.52}\]

\[({\rm under}\ \ P_{\boldsymbol{\theta}_{0}})\]

The third summand converges a.s. (under \(P_{\boldsymbol{\theta}_{0}}\)) to

\[-\frac{1}{2!}\sum_{r,r^{\prime}=1}^{p}(\theta_{r}-\theta_{0r})(\theta_{r^{ \prime}}-\theta_{0r^{\prime}})I_{rr^{\prime}}(\theta_{0}); \tag{7.53}\]

for one may show, as in (7.10), that

\[E_{\theta_{0}}\left(\frac{\partial^{2}\log f(X_{1};\boldsymbol{\theta})}{ \partial\theta_{r}\partial\theta_{r^{\prime}}}\right)_{\theta_{0}}=-E_{\theta _{0}}\left(\frac{\partial\log f(X_{1};\boldsymbol{\theta})}{\partial\theta_{r }}\cdot\frac{\partial\log f(X_{1};\boldsymbol{\theta})}{\partial\theta_{r^{ \prime}}}\right)_{\boldsymbol{\theta}_{0}}. \tag{7.54}\]

The last summand in (7.51) is, by (A5), bounded by

\[\frac{p^{\frac{3}{2}}}{3!}|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|^{3} \frac{1}{n}\sum_{j=1}^{n}g(X_{j}). \tag{7.55}\]

(Note: \((\sum_{r=1}^{p}|\theta_{r}-\theta_{0r}|)^{3}\leq p^{\frac{3}{2}}|\boldsymbol{ \theta}-\boldsymbol{\theta}_{0}|^{3}).\))

By the strong law of large numbers the expression (7.55) converges a.s., under \(P_{\boldsymbol{\theta}_{0}},\) to

\[\frac{p^{\frac{3}{2}}}{3!}|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|^{3} \cdot E_{\theta_{0}}g(X_{1}). \tag{7.56}\]

Let \(\lambda\) be the smallest eigenvalue of \(\mathscr{I}_{1}(\boldsymbol{\theta}_{0}).\) Then

\[\frac{1}{2!}\sum_{r,r^{\prime}=1}^{p}(\theta_{r}-\theta_{0r})(\theta_{r^{ \prime}}-\theta_{0r^{\prime}})I_{rr^{\prime}}(\boldsymbol{\theta}_{0})\geq \frac{1}{2!}\lambda_{1}|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|^{2}. \tag{7.57}\]

Choose \(\delta\) such that

\[\frac{p^{\frac{3}{2}}}{3!}\,\delta E_{\boldsymbol{\theta}_{0}}g(X_{1})<\frac{ \lambda_{1}}{8}, \tag{7.58}\]

so that the expression (7.56) is less than \(\lambda_{1}|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|^{2}/8\) for \(|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|\leq\delta.\) Consider the set

\[B_{n_{1}}=\bigg{\{}\mathbf{x}:x_{j}\in C\ \forall\,j=1,2,\ldots,n,\text{ and the right side of \eqref{eq:1} is less than} \tag{7.59}\]

\[\frac{1}{n}\sum_{j=1}^{n}\log f(X_{j};\boldsymbol{\theta}_{0})+\sum_{r=1}^{p} (\theta_{r}-\theta_{0r})\eta_{r,r^{\prime}}-\frac{1}{4}\lambda_{1}|\boldsymbol {\theta}-\boldsymbol{\theta}_{0}|^{2}\ \forall\ \boldsymbol{\theta}\text{ s.t.}\ |\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|\leq\delta\bigg{\}}.\]

It follows from (7.41)-(7.58) that

\[P_{\boldsymbol{\theta}_{0}}(B_{n1})\to 1\qquad\text{as }n\to\infty. \tag{7.60}\]Now on the (_surface_ of the) sphere \(\{\theta:|\theta_{r}-\theta_{\theta_{0}}|=\delta\}\)

\[\sum_{r=1}^{p}(\theta_{r}-\theta_{0r})\eta_{r,n}-\frac{1}{4}\lambda _{1}|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|^{2}\leq|\boldsymbol{\theta}- \boldsymbol{\theta}_{0}|\cdot\left(\sum_{r=1}^{p}\eta_{r,n}^{2}\right)^{\frac{1 }{2}}-\frac{1}{4}\lambda_{1}|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|^{2}\] \[=\delta\left(\sum_{r=1}^{p}\eta_{r,n}^{2}\right)^{\frac{1}{2}}- \frac{1}{4}\lambda_{1}\delta^{2}. \tag{7.61}\]

Consider the set

\[B_{n2}=\left\{\mathbf{x}:\left(\sum_{r=1}^{p}\eta_{r,n}^{2}\right)^{\frac{1}{2 }}<\frac{\lambda_{1}\delta}{8}\right\}. \tag{7.62}\]

On \(B_{n2}\) the expression (7.61) is strictly negative. Therefore for \(\mathbf{x}\) belonging to

\[B_{n}=B_{n1}\cap B_{n2} \tag{7.63}\]

one has

\[\frac{1}{n}\sum_{j=1}^{n}\log f(X_{j};\boldsymbol{\theta})<\frac{1}{n}\sum_{j =1}^{n}\log f(X_{j};\boldsymbol{\theta}_{0})\quad\forall\;\boldsymbol{\theta} \in\{|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|=\delta\}. \tag{7.64}\]

i.e.,

\[f_{n}(\mathbf{x};\boldsymbol{\theta})<f_{n}(\mathbf{x};\boldsymbol{\theta}_{0} )\quad\forall\;\boldsymbol{\theta}\in\{|\boldsymbol{\theta}-\boldsymbol{\theta }_{0}|=\delta\}. \tag{7.65}\]

But (7.65) implies that the maximum of the function \(f_{n}(\mathbf{x};\boldsymbol{\theta})\) on the closed ball \(\{|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|\leq\delta\}\) is attained in the interior \(\{|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|<\delta\}\). Therefore, inside this ball there exists a (at least one) point \(\boldsymbol{\theta}\) satisfying (7.50). On the other hand by (7.60) \(P_{\boldsymbol{\theta}_{0}}(B_{n1})\to 1\) as \(n\to\infty\). Therefore, by (7.52),

\[P_{\boldsymbol{\theta}_{0}}(B_{n})\longrightarrow 1\qquad\text{as}\;\;n\to\infty. \tag{7.66}\]

Since \(\mathbf{x}\in B_{n}\) imples \(S_{\mathbf{x},n}\) is nonempty and \(|\hat{\boldsymbol{\theta}}_{n}(\mathbf{x})-\boldsymbol{\theta}_{0}|<\delta\), and since the definition of \(\hat{\boldsymbol{\theta}}_{n}\) does not involve \(\delta\), the proof of (1), (2) is complete. (Note that (7.66) holds for all sufficiently small \(\delta\)).

In order to prove part (3) we proceed as in the proof of Theorem 7.2 i.e., write for \(\mathbf{x}\in B_{n}\)

\[0 =\left(\frac{\partial\log f_{n}(\mathbf{x};\boldsymbol{\theta})} {\partial\theta_{r}}\right)_{\hat{\theta}_{n}(\mathbf{x})}\] \[=\left(\frac{\partial\log f_{n}(\mathbf{x};\boldsymbol{\theta})}{ \partial\theta_{r}}\right)_{\theta_{0}}+\sum_{r^{\prime}=1}^{p}(\hat{\theta}_{n,r^{\prime}}(\mathbf{x})-\theta_{0r^{\prime}})\left(\frac{\partial^{2}\log f_ {n}(\mathbf{x};\boldsymbol{\theta})}{\partial\theta_{r}\partial\theta_{r^{ \prime}}}\right)_{\boldsymbol{\theta}_{0}} \tag{7.67}\] \[\quad+\frac{1}{2!}\sum_{r^{\prime},r^{\prime\prime}=1}^{p}(\hat{ \theta}_{n,r^{\prime}}(\mathbf{x})-\theta_{0r^{\prime}})(\hat{\theta}_{n,r^{ \prime\prime}}(\mathbf{x})-\theta_{0r^{\prime\prime}})\left(\frac{\partial^{3} \log f_{n}(\mathbf{x};\boldsymbol{\theta})}{\partial\theta_{r}\partial\theta_{r^ {\prime}}\partial\theta_{r^{\prime\prime}}}\right)_{\overline{\theta}(\mathbf{ x},r))},\,1\leq r\leq p,\]

where \(\overline{\theta}(\mathbf{x};r)\) lies on the line segment joining \(\hat{\theta}_{n}(\mathbf{x})\) and \(\theta_{0}\). Express (7.67) as

\[\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}(\mathbf{x})-\boldsymbol{ \theta}_{0})=\] \[=-\left(\left(\left(\frac{1}{n}\frac{\partial^{2}\log f_{n}( \mathbf{x};\boldsymbol{\theta})}{\partial\theta_{r}\partial\theta_{r^{\prime}} }\right)_{\boldsymbol{\theta}_{0}}+\eta_{n,r,r^{\prime}}(\mathbf{x})\right) \right)^{-1}\left(\frac{1}{\sqrt{n}}\text{grad}\log f_{n}(\mathbf{x}; \boldsymbol{\theta})\right)_{\boldsymbol{\theta}_{0}} \tag{7.68}\]where

\[\eta_{n,r,r^{\prime}}({\bf x})=\frac{1}{2}\sum_{r^{\prime\prime}=1}^{p}\left(\hat{ \theta}_{n,r^{\prime\prime}}({\bf x})-\theta_{0r^{\prime\prime}}\right)\frac{1} {n}\left(\frac{\partial^{3}\log f_{n}({\bf x};\boldsymbol{\theta})}{\partial \theta_{r}\partial\theta_{r^{\prime}}\partial\theta_{r^{\prime\prime}}}\right)_ {\overline{\boldsymbol{\theta}}({\bf x};r)}, \tag{7.69}\]

and (7.68) is defined only for those \({\bf x}\) for which the matrix \(((\quad))\) (whose \((r,r^{\prime})\) element is given in (7.68)) is nonsingular. Now, by (A5),

\[\left|\frac{1}{n}\left(\frac{\partial^{3}\log f_{n}({\bf x};\boldsymbol{\theta })}{\partial\theta_{r}\partial\theta_{r^{\prime}}\partial\theta_{r^{\prime \prime}}}\right)_{\overline{\boldsymbol{\theta}}({\bf x},r)}\right|\leq\frac {1}{n}\sum_{j=1}^{n}g(x_{j}) \tag{7.70}\]

which is bounded in \(P_{\boldsymbol{\theta}_{0}}\) probability (since the right side converges a.s. \((P_{\boldsymbol{\theta}_{0}})\) to a constant). But \(\hat{\theta}_{n,r^{\prime\prime}}-\tilde{\theta}_{0r^{\prime\prime}}\to 0\) in \(P_{\boldsymbol{\theta}_{0}}\)-probability (\(\forall\ r^{\prime\prime}=1,2,\ldots,p\)) as \(n\to\infty\). Hence

\[\eta_{r,r,r^{\prime}}\stackrel{{ P_{\boldsymbol{\theta}_{0}}}}{{ \longrightarrow}}0\,\,\,\text{as}\,\,n\to\infty. \tag{7.71}\]

Also, writing \(\mathscr{I}_{1}=((I_{rr^{\prime}}))\), one has

\[\frac{1}{n}\left(\frac{\partial^{2}\log f_{n}({\bf X};\boldsymbol{\theta})}{ \partial\theta_{r}\partial\theta_{r^{\prime}}}\right)_{\theta_{0}}\stackrel{{ \text{a.s.}(P_{\boldsymbol{\theta}_{0}})}}{{\longrightarrow}}-I_{rr^{\prime} }(\boldsymbol{\theta}_{0}),\qquad\text{as}\quad n\to\infty. \tag{7.72}\]

Therefore, the matrix \(((\quad))\) in (7.68) converges a.s. to \(-\mathscr{I}_{1}(\boldsymbol{\theta}_{0})\), and its inverse converges a.s. to \(-\mathscr{I}_{1}^{-1}(\boldsymbol{\theta}_{0})\), as \(n\to\infty\). By the multidimensional classical central limit theorem,

\[\frac{1}{\sqrt{n}}(\text{grad}\,\log f_{n}({\bf X};\boldsymbol{\theta}))_{ \boldsymbol{\theta}_{0}}=\frac{1}{\sqrt{n}}\sum_{j=1}^{n}(\text{grad}\,\log f (X_{j};\boldsymbol{\theta}))_{\boldsymbol{\theta}_{0}}\stackrel{{ \mathscr{L}}}{{\longrightarrow}}N(\boldsymbol{0},\mathscr{I}( \boldsymbol{\theta}_{0})). \tag{7.73}\]

Therefore, by (a vector version of) Slutsky's theorem,

\[\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\mathscr{I}_{1}^{-1}(\boldsymbol{ \theta}_{0}){\bf Z},\quad Z\stackrel{{\mathscr{L}}}{{\sim}}N(0, \mathscr{I}_{1}(\boldsymbol{\theta}_{0})), \tag{7.74}\]

from which part (3) of the theorem follows. To be completely precise one needs to write

\[\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})=\sqrt{n}(\hat{ \boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})\cdot\mathbf{1}_{D_{n}}+ \sqrt{n}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})\mathbf{1}_{D_{n }^{c}}, \tag{7.75}\]

where \(D_{n}\) is the subset of \(B_{n}\) on which the matrix \(((\cdot))\) in (7.68) is nonsingular. Then use the representation (7.68) for the first summand in (7.75). Since \(\mathbf{1}_{D_{n}}\stackrel{{ P_{\boldsymbol{\theta}_{0}}}}{{ \longrightarrow}}1\), and, therefore, \(\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})\mathbf{1}_{D_{n }^{c}}\stackrel{{ P_{\boldsymbol{\theta}_{0}}}}{{\longrightarrow }}0\), the proof is complete. 

Example 7.5 (Multi-Parameter Exponential Family): Here

\[f(x;\boldsymbol{\theta})=C(\boldsymbol{\theta})e^{\sum_{r=1}^{p}\theta_{r}t_{r} (x)}h(x), \tag{7.76}\]

\[\Theta=\left\{\boldsymbol{\theta}=(\theta_{1},\theta_{2},\ldots,\theta_{p}) \in\mathbb{R}^{p}:\int_{\mathscr{X}}e^{\sum_{r=1}^{p}\theta_{r}t_{r}(x)}h(x) \mu(dx)<\infty\right\}.\]

Assume that \(\Theta\) is a nonempty open subset of \(\mathbb{R}^{p}\). In this set up \(\boldsymbol{\theta}\) is called a natural parameter. Note that

\[C(\boldsymbol{\theta})=\left(\int_{\mathscr{X}}e^{\sum_{r=1}^{p}\theta_{r}t_{r }(x)}h(x)\mu(dx)\right)^{-1}. \tag{7.77}\]A family of distributions \(\{f(x;\boldsymbol{\theta}):\boldsymbol{\theta}\in\Theta\}\) given by (7.76) is called a multi-parameter exponential family in the natural parameter form. One may easily show, by Holder's inequality, that \(\Theta\) is convex (See Sect. 4.2 and Theorem 4.4). Also under the assumption underlined above \(\int_{\mathscr{X}}\exp\{\sum_{r=1}^{p}\theta_{r}t_{r}(x)\}h(x)\mu(dx)\) is analytic (in particular, infinitely differentiable) in \(\theta\). One has the likelihood equations

\[-\frac{\partial\log C(\boldsymbol{\theta})}{\partial\theta_{r}}=\frac{1}{n} \sum_{j=1}^{n}t_{r}(X_{j}),\qquad(1\leq r\leq p), \tag{7.78}\]

(See Exercise 7.12). As in Theorem 7.1 (iii), one may easily show that

\[E_{\boldsymbol{\theta}}t_{r}(X_{1})=-\frac{\partial\log C(\boldsymbol{\theta} )}{\partial\theta_{r}},\qquad(1\leq r\leq p), \tag{7.79}\]

and

\[\frac{\partial^{2}\log C(\boldsymbol{\theta})}{\partial\theta_{r}\partial \theta_{r^{i}}}=-\text{cov}_{\boldsymbol{\theta}}(t_{r}(X_{1}),t_{r^{\prime}} (X_{1})). \tag{7.80}\]

Since

\[\frac{\partial^{2}\log f_{n}(\mathbf{x},\boldsymbol{\theta})}{\partial\theta _{r}\partial\theta_{r^{\prime}}}=n\frac{\partial^{2}\log C(\boldsymbol{\theta} )}{\partial\theta_{r}\partial\theta_{r^{\prime}}}=-n\;\text{cov}_{\boldsymbol {\theta}}(t_{r}(X_{1}),t_{r^{\prime}}(X_{1})), \tag{7.81}\]

the matrix of second derivatives of \(\log f_{n}(\mathbf{x};\boldsymbol{\theta})\) is a strictly concave function of \(\theta\). Hence there can not be more than one solution to the likelihood Eq. (7.78). If there is a solution it is the MLE. Since the assumptions \((A_{1})-(A_{5})\) are all satisfied in this case it follows that the MLE (equivalently, the unique solution of the likelihood equations) exists on a set whose probability goes to one. When extended arbitrarily (but measurably) to these \(\mathbf{x}\) for which a solution to (7.78) does not exist, the MLE is, therefore, asymptotically efficient.

The following special multi-parameter families are important in applications.

1. _Univariate Normal_\(N(\mu,\sigma^{2})\). In this case write \(\boldsymbol{\theta}=(\theta_{1},\theta_{2})\) with \(\theta_{1}=\mu/\sigma^{2}\) and \(\theta_{2}=-\frac{1}{2\sigma^{2}}\). Then \[f(x;\mu,\sigma^{2}) =\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-(x-\mu)^{2}/2\sigma^{2}}=\frac {1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{x^{2}}{2\sigma^{2}}+\frac{\mu x}{\sigma^{ 2}}-\frac{\mu^{2}}{2\sigma^{2}}}\] \[=C(\boldsymbol{\theta})e^{\theta_{1}t_{1}(x)+\theta_{2}t_{2}(x)}= f(x;\boldsymbol{\theta})\qquad\theta_{1}=\frac{\mu}{\sigma^{2}},\theta_{2}=- \frac{1}{2\sigma^{2}}\,,\] \[C(\boldsymbol{\theta}) =\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\mu^{2}/2\sigma^{2}}=\left(- \frac{\theta_{2}}{\pi}\right)^{\frac{1}{2}}e^{\frac{1}{4}\theta_{1}^{2}/\theta _{2}};\quad t_{1}(x)=x,t_{2}(x)=x^{2}.\] From (7.78) we get the MLE as \[\hat{\boldsymbol{\theta}}=(\hat{\theta}_{1},\hat{\theta}_{2})=\left(\frac{ \overline{X}}{s^{2}},-\frac{1}{2s^{2}}\right),\] where \(\overline{X}=\frac{1}{n}\sum_{j=1}^{n}X_{j}\) and \(s^{2}=\frac{1}{n}\sum_{j=1}^{n}(X_{j}-\overline{X})^{2}\). The information matrix is given by \(\mathscr{I}_{1}(\boldsymbol{\theta})=((I_{ij}(\boldsymbol{\theta})))\), where \[I_{11}(\mathbf{\theta}) = -\frac{\partial^{2}\log C(\mathbf{\theta})}{\partial\theta_{ 1}^{2}}=-\frac{1}{2\theta_{2}}=\sigma^{2},\] \[I_{12}(\mathbf{\theta}) = I_{21}(\theta)=-\frac{\partial^{2}\log C(\mathbf{\theta$ })}{\partial\theta_{1}\partial\theta_{2}}=-\frac{\theta_{1}}{2\theta_{2}^{2}}=- 2\mu\sigma^{2},\] \[I_{22}(\mbox{\boldmath$\theta}) = -\frac{\partial^{2}\log C(\mathbf{\theta})}{\partial\theta _{2}^{2}}=\frac{1}{2\theta_{2}^{2}}+\frac{\theta_{1}^{2}}{2\theta_{2}^{3}}=2 \sigma^{4}+4\mu^{2}\sigma^{4}.\] (7.82) Since \((\theta_{1},\theta_{2})\rightarrow(\mu,\sigma^{2})\) is one-to-one (and differentiable), the MLE of \((\mu,\sigma^{2})\) is (Note: \(\mu=-\theta_{1}/2\theta_{2},\sigma^{2}=-\frac{1}{2\theta_{2}}\).)

\[\hat{\mu}=-\frac{\hat{\theta}_{1}}{2\hat{\theta}_{2}}=\overline{X},\qquad\hat {\sigma}^{2}=s^{2}. \tag{7.83}\]

The information matrix in terms of \(\mu,\sigma^{2}\) is then given by

\[I_{11}(\mu,\sigma^{2}) = E_{\mu,\sigma^{2}}\left(\frac{\partial\log f(X_{1};\mu,\sigma^{2 })}{\partial\mu}\right)^{2}\] \[= E_{\mu,\sigma^{2}}\left(\frac{X_{1}}{\sigma^{2}}-\frac{\mu}{ \sigma^{2}}\right)^{2}=\frac{1}{\sigma^{4}}E_{\mu,\sigma^{2}}(X_{1}-\mu)=\frac {1}{\sigma^{2}},\]

\[I_{12}(\mu,\sigma^{2}) = E_{\mu,\sigma^{2}}\left(\frac{\partial\log f(X_{1};\mu,\sigma^{2 })}{\partial\mu}\cdot\frac{\partial\log f(X_{1};\mu,\sigma^{2})}{\partial \sigma^{2}}\right)\] \[= E_{\mu,\sigma^{2}}\left(\frac{X_{1}-\mu}{\sigma^{2}}\cdot\left\{ -\frac{1}{2\sigma^{2}}+\frac{X_{1}^{2}}{2\sigma^{4}}-\frac{\mu X_{1}}{\sigma^ {4}}\right\}\right)\] \[= E_{\mu,\sigma^{2}}\left(\frac{X_{1}-\mu}{\sigma^{2}}\left\{ \frac{(X_{1}-\mu)^{2}}{2\sigma^{4}}-\frac{1}{2\sigma^{2}}-\frac{\mu^{2}}{2 \sigma^{4}}\right\}\right)\] \[= E_{\mu,\sigma^{2}}\frac{(X_{1}-\mu)^{3}}{2\sigma^{6}}=0,\] \[I_{22}(\mu,\sigma^{2}) = E_{\mu,\sigma^{2}}\left(\frac{\partial\log f(X_{1};\mu,\sigma^{ 2})}{\partial\sigma^{2}}\right)^{2}=\mbox{var}\left(\frac{\partial\log f(X_{1 };\mu,\sigma^{2})}{\partial\sigma^{2}}\right)\] \[= \mbox{var}\left\{\frac{(X_{1}-\mu)^{2}}{2\sigma^{4}}\right\}= \frac{2\sigma^{4}}{4\sigma^{8}}=\frac{1}{2\sigma^{4}}.\] \[I_{11}^{-1}(\mu,\sigma^{2}) = \sigma^{2},\qquad I_{12}^{-1}=I_{21}^{-1}(\mu,\sigma^{2})=0, \qquad I_{22}^{-1}(\mu,\sigma^{2})=2\sigma^{4}.\]

Therefore, \((\sqrt{n}(\overline{X}-\mu),\sqrt{n}(s^{2}-\sigma^{2}))\stackrel{{ \mathscr{L}}}{{\longrightarrow}}N\left(0,\begin{pmatrix}\sigma^{2}&0\\ 0&2\sigma^{4}\end{pmatrix}\right)\).
2. _Multivariate Normal_\(N\left(\mathbf{\mu}=\begin{pmatrix}\mu_{1}\\ \vdots\\ \mu_{k}\end{pmatrix},\Sigma=((\sigma_{ij}))\right).\) Let \(X_{1}=(X_{11},X_{12},\ldots,X_{1k}),\ldots,X_{n}=(X_{n1},X_{n2},\ldots,X_{nk})\) be i.i.d. \(N(\mathbf{\mu},\Sigma)\), \(\mathbf{\mu}\in\mathbb{R}^{k}\), \(\Sigma\) set of all positive definite symmetric \(k\times k\) matrices. One has, with \(\mathbf{\theta}=(\mathbf{\mu},\Sigma)\),\[f_{n}({\bf x};\theta)=(2\pi)^{-nk/2}\left(\det\Sigma\right)^{-n/2}\exp\left\{- \frac{1}{2}\sum_{j=1}^{n}\left[\sum_{i,i^{\prime}=1}^{k}\sigma^{ii^{\prime}}(x_{ ji}-\mu_{i})\cdot(x_{ji^{\prime}}-\mu_{i^{\prime}})\right]\right\} \tag{7.84}\]

where \(((\sigma^{ii^{\prime}}))=\Sigma^{-1}\). If \(\mu\) is known then (7.84) is a \(p(=k(k+1)/2)\)-parameter exponential family with _natural parameters_\(\{\sigma^{ii^{\prime}}:1\leq i\leq i^{\prime}\leq p\}\). By (7.78), (7.79) one has the likelihood equations (with \(t_{ii^{\prime}}(x_{j})=-(x_{ji}-\mu_{i})(x_{ji^{\prime}}-\mu_{i^{\prime}})\) for \(i<i^{\prime}\), and \(t_{ii}(x_{j})=-\frac{1}{2}(x_{ji}-\mu_{i})^{2}\)).

\[E_{\theta}t_{ii^{\prime}}(X_{1})=\frac{1}{n}\sum_{j=1}^{n}t_{ii^{\prime}}(X_{j }), \tag{7.85}\]

or, _the MLE_\(\hat{\mathbf{\theta}}_{n}=\{\hat{\sigma}_{ii^{\prime}}:1\leq i\leq i^{ \prime}\leq k\}\) is given by

\[\hat{\sigma}_{ii^{\prime}}=s_{ii^{\prime}}\qquad(1\leq i\leq i^{\prime}\leq k), \tag{7.86}\]

where

\[s_{ii^{\prime}}=\frac{1}{n}\sum_{j=1}^{n}(X_{ji}-\mu_{i})(X_{ji^{\prime}}-\mu_ {i^{\prime}}). \tag{7.87}\]

The \(p=(\frac{k(k+1)}{2})\)-dimensional random vectors \(\sqrt{n}(\hat{\mathbf{\theta}}_{n}-\mathbf{\theta})\) converges in distribution to a \(p\)-variate Normal distribution \(N({\bf 0},V(\mathbf{\theta}))\), where \(V(\mathbf{\theta})=((v_{(ii^{\prime}),(\ell,\ell^{\prime})}(\theta)))\)

\[v_{(i,i^{\prime}),(\ell,\ell^{\prime})}(\mathbf{\theta}) = \mbox{cov}_{\theta}((X_{1i}-\mu_{i})(X_{1i^{\prime}}-\mu_{i^{ \prime}}),(X_{1\ell}-\mu_{\ell})(X_{1\ell^{\prime}}-\mu_{\ell^{\prime}})) \tag{7.88}\] \[= E_{\theta}(X_{1i}-\mu_{i})(X_{1i^{\prime}}-\mu_{i^{\prime}})(X_ {1\ell}-\mu_{\ell})(X_{1\ell^{\prime}}-\mu_{\ell^{\prime}})-\sigma_{ii^{\prime }}\sigma_{\ell\ell^{\prime}}\] \[= \left(\frac{\partial^{4}}{\partial\xi_{i}\partial\xi_{i^{\prime} }\partial\xi_{\ell}\partial\xi_{\ell^{\prime}}}\exp\left\{\frac{1}{2}\sum_{r,r ^{\prime}=1}^{k}\sigma_{rr^{\prime}}\xi_{r}\xi_{r^{\prime}}\right\}\right)_{ \xi=0}-\sigma_{ii^{\prime}}\sigma_{\ell\ell^{\prime}}.\]

In particular,

\[v_{(i,i^{\prime}),(i,i^{\prime})}=\sigma_{ii}\sigma_{i^{\prime}i^{\prime}}+ \sigma_{ii^{\prime}}^{2}. \tag{7.89}\]

In case \(\mu_{i}\)'s are also unknown, one has

\[f(x;\mathbf{\theta}) = (2\pi)^{-k/2}\left(\det\Sigma\right)^{-\frac{1}{2}}\exp\left\{- \frac{1}{2}\sum_{i,i^{\prime}=1}^{k}\sigma^{ii^{\prime}}(x_{i}-\mu_{i})(x_{i^ {\prime}}-\mu_{i^{\prime}})\right\}\] \[= C(\mathbf{\theta})\exp\left\{-\frac{1}{2}\sum_{i,i^{ \prime}=1}^{k}x_{i}x_{i^{\prime}}\sigma^{ii^{\prime}}+\sum_{i=1}^{k}x_{i} \left(\sum_{i^{\prime}=1}^{k}\mu_{i^{\prime}}\sigma^{ii^{\prime}}\right) \right\},\]

with \(\mathbf{\theta}=\{\sigma^{ii^{\prime}},1\leq i\leq i^{\prime}\leq k; \sum_{i^{\prime}=1}^{k}\mu_{i^{\prime}}\sigma^{ii^{\prime}},1\leq i\leq k\}\). Hence \(f(x;\mathbf{\theta})\) belongs to the \(p\)-parameter exponential family with \(p=\frac{k(k+1)}{2}+k=k(k+3)/2\). The _natural parameters_ are the coordinates of \(\theta\). The likelihood equations are (see (7.78), (7.79)).

\[E_{\theta}\tilde{t}_{ii^{\prime}}(X_{1})=\frac{1}{n}\sum_{j=1}^{n}\tilde{t}_{ ii^{\prime}}(X_{j}),\]\[E_{\vec{\theta}}T_{i}(X_{1})=\frac{1}{n}\sum_{j=1}^{n}T_{i}(X_{j}), \tag{7.90}\]

with \(\tilde{t}_{ii^{\prime}}(x)=-x_{i}x_{i^{\prime}}\) (if \(i<i^{\prime}\)), \(\tilde{t}_{ii}(x)=-\frac{1}{2}x_{i}^{2}\), \(T_{i}(x)=x_{i}\). Hence one has the equations

\[\sigma_{ii^{\prime}}+\mu_{i}\mu_{i^{\prime}}=\frac{1}{n}\sum_{j=1}^{n}X_{ji}X_{ ji^{\prime}}=s^{\prime}_{ii^{\prime}},\]

\[\mu_{i}=\frac{1}{n}\sum_{j=1}^{n}X_{ji}=\overline{X}_{i}\quad(1\leq i\leq i^{ \prime}\leq k), \tag{7.91}\]

whose solution is

\[\hat{\mu}_{i}=\overline{X}_{i},\qquad\tilde{\sigma}_{ii^{\prime}}=s^{\prime}_{ ii^{\prime}}-\overline{X}_{i}\overline{X}_{i^{\prime}}=\frac{1}{n}\sum_{j=1}^ {n}(X_{ji}-\overline{X}_{i})(X_{ji^{\prime}}-\overline{X}_{i}) \tag{7.92}\]

It may be shown (See Problem 5.6, Lehmann, _Theory of Point Estimation_, Chap. 6) that \(\{\tilde{\sigma}_{ii^{\prime}};\,1\leq i\leq i^{\prime}\leq k\}\) are independent of \(\{\hat{\mu}_{i};\,1\leq i\leq k\}\) and that the joint distribution of \(\{\tilde{\sigma}_{ii^{\prime}};\,1\leq i\leq i^{\prime}\leq k\}\) is the same as that of \(\{s_{ii^{\prime}};\,1\leq i\leq i^{\prime}\leq k\}\) based on \(n-1\) observations. Thus the asymptotic distribution of \(\{\sqrt{n}(\tilde{\sigma}_{ii^{\prime}}-\sigma_{ii^{\prime}});\,1\leq i\leq i^{ \prime}\leq k\}\) is the same as that of \(\{\sqrt{n}(s_{ii^{\prime}}-\sigma_{ii^{\prime}});\,1\leq i\leq i\leq k\}\). The (asymptotic) distribution of \(\{\sqrt{n}(\hat{\mu}_{i}-\mu_{i});\,1\leq i\leq k\}\) is \(N(\vec{0},\Sigma)\). In view of the independence mentioned above the limiting distribution of \(\sqrt{n}(\tilde{\vec{\theta}}-\vec{\theta})\) is now completely specified (\(\tilde{\vec{\theta}}=\{\tilde{\sigma}_{ii^{\prime}},\,1\leq i\leq i^{\prime}\leq k\); \(\hat{\mu}_{i};\,1\leq i\leq k\}\)). (See Exercise 7.16).
3. _The multinomial distribution_. A population is divided into \(k\) categories, with proportion \(p_{i}\) belonging to the \(i\)th category, \(0<p_{i}<1\) (\(1\leq i\leq k\)): \(\sum_{i=1}^{k}p_{i}=1\). Consider \(n\) observations taken at random (with replacement) from this population. Let us code an observation as the unit \(k\)-dimensional vector \(e_{i}\) with \(1\) as the \(i\)th coordinate and zeros elsewhere, if the observation belongs to the \(i\)th category. Let \(X_{1},X_{2},\ldots,X_{n}\) (\(X_{j}=(X_{j1},\ldots,X_{jk})\)) be the random observations. Then \(X_{1},X_{2},\ldots,X_{n}\) are i.i.d. with common p.d.f. \[f(x;\vec{\theta})= \prod_{i=1}^{k}p_{i}^{x_{i}}=p_{1}^{x_{1}}p_{2}^{x_{2}}\ldots p_{ k-1}^{x_{k-1}}(1-p_{1}-\cdots-p_{k-1})^{x_{k}}\] \[\mbox{for }x=(x_{1},\ldots,x_{k})\in\{e_{i};1\leq i\leq k\}= \mathscr{X}.\] (7.93) Write \[f(x;\vec{\theta}) = e^{\sum_{i=1}^{k-1}x_{i}\log p_{i}+(1-\sum_{1}^{k-1}x_{i})\log(1 -\sum_{1}^{k-1}p_{i})}\] (7.94) where \(\theta_{i}=\log(p_{i}/(1-\sum_{1}^{k-1}p_{i}))\), \(1\leq i\leq k-1\), are natural parameters. The likelihood equations are \[p_{i}=E_{\vec{\theta}}X_{1i}=\frac{1}{n}\sum_{j=1}^{n}X_{ji},\qquad(1\leq i \leq k-1),\] (7.95)or, the MLE of \((p_{1},\ldots,p_{k-1})\) is given by

\[\hat{p}_{i}=\frac{1}{n}\sum_{j=1}^{n}X_{ji}= \text{ proportion of the sample belonging to $ith$ category }(1\leq i\] \[\leq k-1),\] \[\hat{\mathbf{p}}=(\hat{p}_{1},\ldots\hat{p}_{k-1}).\]

Of course, in this case one may represent

\[\sqrt{n}(\hat{\mathbf{p}}-\mathbf{p})=\sqrt{n}\sum_{j=1}^{n}(Y_{j}-\mathbf{p}),\qquad\mathbf{p}=\begin{pmatrix}p_{1}\\ \vdots\\ p_{k-1}\end{pmatrix} \tag{7.96}\]

where \(Y_{1},\ldots,Y_{n}\) are i.i.d. \((k-1)\)-dimensional random vectors with

\[\text{Prob}\left(Y_{1}=\begin{pmatrix}0\\ \vdots\\ 0\\ 1\\ 0\\ \vdots\\ 0\end{pmatrix}\right)i\] \[=p_{i}\quad(1\leq i\leq k-1),\quad P(Y_{1}=\mathbf{0})=1-\sum_{1}^{k-1}p_{i} \tag{7.97}\]

The classical multidimensional CLT then yields (Exercise 7.12)

\[\sqrt{n}(\hat{\mathbf{p}}-\mathbf{p})\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N(\mathbf{0},V),\quad V=((v_{ij^{\prime}})),\]

\[v_{ii}=p_{i}(1-p_{i})(1\leq i\leq k-1),\quad v_{ii^{\prime}}=-p_{i}p_{i^{ \prime}}\ (i\neq i^{\prime}), \tag{7.98}\]

although one could also use Theorem 7.5.

### Method of Moments

Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. real-valued with common p.d.f. \(f(x;\boldsymbol{\theta})\) where \(\boldsymbol{\theta}=(\theta_{1},\theta_{2},\ldots,\theta_{p})\in\varTheta\) which is an open subset of \(\mathbb{R}^{p}\). Assume that

\[E|X_{1}|^{p}<\infty. \tag{7.99}\]

The method of moments consists in solving the equations

\[E_{\boldsymbol{\theta}}X_{1}^{\tau}=\frac{1}{n}\sum_{j=1}^{n}X_{j}^{\tau}\qquad (1\leq r\leq p) \tag{7.100}\]

for \(\theta_{1},\theta_{2},\ldots,\theta_{p}\). In case (7.99) does not hold, or \(X_{j}\)'s are not real-valued, or (7.100) are difficult to solve, one may take some suitable real-valued functions \(g_{1}(x),g_{2}(x),\ldots,g_{p}(x)\) such that

\[E_{\boldsymbol{\theta}}|g_{r}(X_{1})|<\infty,\qquad 1\leq r\leq p, \tag{7.101}\]and solve the equations

\[M_{r}(\boldsymbol{\theta}):=E_{\theta}g_{r}(X_{1})=\frac{1}{n}\sum_{j=1}^{n}g_{r}(X _{j}),\qquad 1\leq r\leq p, \tag{7.102}\]

for \(\theta_{1},\theta_{2},\ldots,\theta_{p}\).

**Proposition 7.1**.: _Let \(\Theta\) be an open subset of \(\mathbb{R}^{p}\). Let \(g_{r}\), \(1\leq r\leq p\), be real-valued functions on \(\mathscr{X}\) such that_

\[E_{\theta}g_{r}^{2}(X_{1})<\infty\quad\text{for}\quad 1\leq r\leq p,\quad \forall\;\boldsymbol{\theta}\in\Theta. \tag{7.103}\]

_Assume (i) (7.102) has a unique solution \(\tilde{\boldsymbol{\theta}}=(\tilde{\theta}_{1},\tilde{\theta}_{2},\ldots, \tilde{\theta}_{p})\) (a.s. \(P_{\boldsymbol{\theta}_{0}}\)\(\forall\;\boldsymbol{\theta}_{0}\in\Theta\)), (ii) the map \(\theta\to M(\theta)=(M_{1}(\theta),\ldots,M_{p}(\theta))\) is a diffeomorphism, i.e., \(M\) and its inverse \(H\) (say) are both continuously differentiable. Then, for each \(\theta_{0}\in\Theta\),_

\[\sqrt{n}(\tilde{\boldsymbol{\theta}}-\boldsymbol{\theta}_{0})\stackrel{{ \mathscr{L}}}{{\longrightarrow}}N(0,V(\boldsymbol{\theta}_{0}))\qquad\text{ under }P_{\boldsymbol{\theta}_{0}}, \tag{7.104}\]

_where_

\[V(\boldsymbol{\theta}_{0})=((v_{ii^{\prime}}(\theta_{0}))),\] \[v_{ii^{\prime}}(\boldsymbol{\theta}_{0})=\sum_{r,r^{\prime}=1}^{ p}\sigma_{rr^{\prime}}(\boldsymbol{\theta}_{0})\ell_{ir}(\theta_{0})\ell_{i^{ \prime}r^{\prime}}(\boldsymbol{\theta}_{0}),\] \[\sigma_{rr^{\prime}}(\boldsymbol{\theta}_{0})=E_{\theta_{0}}[(g_ {r}(X_{1})-M_{r}(\boldsymbol{\theta}_{0}))(g_{r^{\prime}}(X_{1})-M_{r^{\prime }}(\boldsymbol{\theta}_{0}))],\] \[\ell_{ir}(\boldsymbol{\theta}_{0})=\left(\frac{\partial H_{i}(M) }{\partial M_{r}}\right)_{M=M(\boldsymbol{\theta}_{0})} \tag{7.105}\]

Proof.: \[\sqrt{n}(\tilde{\boldsymbol{\theta}}-\boldsymbol{\theta}_{0})^{\prime} =\sqrt{n}\left[H\left(\frac{1}{n}\sum_{j=1}^{n}g_{1}(X_{j}),\ldots,\frac{1}{n}\sum_{j=1}^{n}g_{p}(X_{j})\right)-H(M(\theta_{0}))\right]^{\prime}\] \[=\sqrt{n}\;(\text{Grad }H(M(\boldsymbol{\theta}_{0}))+\varepsilon_{n })\begin{pmatrix}\frac{1}{n}\sum_{j=1}^{n}g_{1}(X_{j})-M_{1}(\theta_{0})\\ \vdots\\ \frac{1}{n}\sum_{j=1}^{n}g_{p}(X_{j})-M_{p}(\theta_{0})\end{pmatrix},\] (7.106)

where \(\text{Grad }H(M(\boldsymbol{\theta}_{0}))\) is the \(p\times p\) matrix who \((i,r)\) element is \(\ell_{ir}(\boldsymbol{\theta}_{0})\), and \(\varepsilon_{n}\) is a \(p\times p\) matrix whose elements converge in probability \((P_{\boldsymbol{\theta}_{0}})\) to zero. 

Example 7.6.: Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. with common p.d.f. \(f(x;\mu,\alpha)=\frac{1}{\mu^{\alpha}T(\alpha)}e^{-x/\mu}x^{\alpha-1}\), \(0<x<\infty\); \(\mu\in(0,\infty)\), \(\alpha\in(0,\infty)\). Note that \(EX_{1}=\alpha\mu\), \(EX_{1}^{2}=(\alpha+1)\alpha\mu^{2}\). Therefore, solve

\[\alpha\mu=\frac{1}{n}\sum_{j=1}^{n}X_{j}=\overline{X},\qquad(\alpha+1)\alpha \mu^{2}=\frac{1}{n}\sum_{j=1}^{n}X_{j}^{2}=m_{2}^{\prime}, \tag{7.107}\]

for \(\alpha,\mu\) to get \[\tilde{\alpha}=\frac{\overline{X}^{2}}{m^{\prime}_{2}-\overline{X}^{2}}=\frac{ \overline{X}^{2}}{s^{2}},\qquad\quad\left(s^{2}=\frac{1}{n}\sum_{j=1}^{n}(X_{j} -\overline{X})^{2}\right),\]

\[\tilde{\mu}=\frac{\overline{X}}{\tilde{\alpha}}=\frac{s^{2}}{\overline{X}}. \tag{7.108}\]

Here (check)

\[M_{1}(\mu,\alpha)=\alpha\mu,\quad M_{2}(\mu,\alpha)=\alpha^{2}\mu^{2}+\alpha\mu ^{2};\]

\[H_{1}(z_{1},z_{2})=\frac{z_{2}-z_{1}^{2}}{z_{1}},\quad H_{2}(z_{1},z_{2})= \frac{z_{1}^{2}}{z_{2}-z_{1}^{2}}. \tag{7.109}\]

Therefore, with \(\boldsymbol{\theta}=(\mu,\alpha)\),

\[\ell_{11}(\boldsymbol{\theta}_{0}) =-\left(\frac{z_{2}+z_{1}^{2}}{z_{1}^{2}}\right)_{z_{1}=\alpha_{0 }\mu_{0},z_{2}=(\alpha_{0}+1)\alpha_{0}\mu_{0}^{2}}=-\frac{2\alpha_{0}^{2}\mu_ {0}^{2}+\alpha_{0}\mu_{0}^{2}}{\alpha_{0}^{2}\mu_{0}^{2}}\] \[=-\left(2+\frac{1}{\alpha_{0}}\right),\]

\[\ell_{12}(\boldsymbol{\theta}_{0})=\frac{1}{\alpha_{0}\mu_{0}}, \quad\ell_{22}(\theta_{0})=-\frac{\alpha_{0}^{2}\mu_{0}^{2}}{\alpha_{0}^{2}\mu_ {0}^{4}}=-\frac{1}{\mu_{0}^{2}},\] \[\ell_{21}(\boldsymbol{\theta}_{0})=\frac{2\alpha_{0}\mu_{0}( \alpha_{0}\mu_{0}^{2})-\alpha_{0}^{2}\mu_{0}^{2}(-2\alpha_{0}\mu_{0})}{\alpha _{0}^{2}\mu_{0}^{4}}=\frac{2(1+\alpha_{0})}{\mu_{0}};\] \[\sigma_{11}(\boldsymbol{\theta}_{0})=\text{var}_{\theta_{0}}(X_{1 })=E_{\theta_{0}}X_{1}^{2}-(E_{\theta_{0}}X_{1})^{2}=\alpha_{0}\mu_{0}^{2},\]

\[\sigma_{12}(\boldsymbol{\theta}_{0}) =E_{\theta_{0}}X_{1}^{3}-(E_{\theta_{0}}X_{1})(E_{\theta_{0}}X_{1 }^{2})=(\alpha_{0}+2)(\alpha_{0}+1)\alpha_{0}\mu_{0}^{3}-(\alpha_{0}+1)\alpha _{0}^{2}\mu_{0}^{3}\] \[=2(\alpha_{0}+1)\alpha_{0}\mu_{0}^{3}=\sigma_{21}(\theta_{0}),\]

\[\sigma_{22}(\boldsymbol{\theta}_{0}) =E_{\theta_{0}}X_{1}^{4}-(E_{\theta_{0}}X_{1}^{2})^{2}=(\alpha_{0} +3)(\alpha_{0}+2)(\alpha_{0}+1)\alpha_{0}\mu_{0}^{4}-(\alpha_{0}+1)^{2}\alpha _{0}^{2}\mu_{0}^{4}\] \[=2(2\alpha_{0}+3)(\alpha_{0}+1)\alpha_{0}\mu_{0}^{4}. \tag{7.110}\]

\(V(\boldsymbol{\theta})\) can be computed from (7.110). For example,

\[v_{11}(\boldsymbol{\theta}_{0}) =\sigma_{11}(\theta_{0})\ell_{11}^{2}(\theta_{0})+\sigma_{12}( \theta_{0})(\ell_{11}(\theta_{0})\ell_{12}(\theta_{0})\] \[\qquad+\ell_{12}(\theta_{0})+\ell_{11}(\theta_{0}))+\sigma_{22}( \theta_{0})\ell_{12}^{2}(\theta_{0}). \tag{7.111}\]

One may show that \(\tilde{\boldsymbol{\theta}}\) is not asymptotically efficient in the above example. However, since\(\sqrt{n}(\tilde{\boldsymbol{\theta}}-\boldsymbol{\theta}_{0})\) is bounded in probability (under \(P_{\boldsymbol{\theta}_{0}}\)) one may obtain an asymptotically efficient estimator by adding a "correction" term, as in the one-parameter case (See Theorem 7.3). You may also check that the likelihood equations are intractable for this example.

**Theorem 7.6**.: _Suppose the hypothesis of Theorem 7.1 holds, and that \(\tilde{\boldsymbol{\theta}}_{n}\) is an estimator (sequence) of \(\boldsymbol{\theta}\) such that \(\sqrt{n}(\tilde{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})\) is bounded in probability under \(P_{\boldsymbol{\theta}_{0}}\) (for each \(\boldsymbol{\theta}_{0}\in\Theta\)). Then_\[\delta_{n}=\tilde{\boldsymbol{\theta}}_{n}-\left(\left(\frac{\partial^{2}\log f_{n}( \mathbf{X};\boldsymbol{\theta})}{\partial\theta_{r}\partial\theta_{r^{\prime}}} \right)\right)_{\tilde{\theta}_{n}}^{-1}(\operatorname{Grad}\,\log f_{n}( \mathbf{X};\boldsymbol{\theta}))_{\tilde{\theta}_{n}} \tag{7.112}\]

_is an asymptotically efficient estimator of \(\theta\). Here_

\[\operatorname{Grad}\,\log f_{n}(\mathbf{x};\boldsymbol{\theta})=\begin{pmatrix} \partial\log f_{n}(\mathbf{x};\boldsymbol{\theta})/\partial\theta_{1}\\ \vdots\\ \partial\log f_{n}(\mathbf{x};\boldsymbol{\theta})/\partial\theta_{p}\end{pmatrix}. \tag{7.113}\]

Proof: Entirely analogous to the proof of Theorem 7.3. 

_Example 7.7_.: To find an asymptotically efficient estimator of \(\theta=(\mu,\alpha)\) in the above example, write \(f(x;\mu,\alpha)=c(\mu,\alpha)e^{-x(1/\mu)+(\log x)\alpha}x^{-1}\), \(0<x<\infty\). Thus this is a two-parameter exponential family with \(\theta_{1}=1/\mu\), \(\theta_{2}=\alpha\), \(t_{1}(x)=-x\), \(t_{2}(x)=\log x\). The likelihood equations, therefore, may be expressed as (See (7.78), (7.79))

\[EX_{1}=\frac{1}{n}\sum_{j=1}^{n}X_{j},\quad E\log X_{1}=\frac{1}{n}\sum_{j=1}^ {n}\log X_{j},\]

or,

\[\alpha\mu=\frac{1}{n}\sum_{j=1}^{n}X_{j},\quad E\log X_{1}=\frac{1}{n}\sum_{j=1 }^{n}\log X_{j}. \tag{7.114}\]

But \(E\log X_{1}\) cannot be computed in a tractable form. Hence the likelihood equations are impossible to solve explicitly. One may, therefore, take recourse to Theorem 7.6 with \(\tilde{\theta}_{n}\) as the method-of-moments estimator given by (7.114): \(\tilde{\theta}_{n1}=\frac{1}{\tilde{\mu}}=\frac{\overline{X}}{s^{2}},\tilde{ \theta}_{n2}=\tilde{\alpha}=\frac{\overline{X}}{s^{2}}\). One has

\[\partial\log f_{n}(\mathbf{x};\boldsymbol{\theta})/\partial\theta_{1}=n\frac{ \partial\log C(\boldsymbol{\theta})}{\partial\theta_{1}}-\sum_{j=1}^{n}X_{j}= n\alpha\mu-\sum_{j=1}^{n}X_{j},\]

\[\partial\log f_{n}(\mathbf{x};\boldsymbol{\theta})/\partial\theta_{2}=n\frac{ \partial\log C(\boldsymbol{\theta})}{\partial\theta_{2}}+\sum_{j=1}^{n}\log X_ {j}, \tag{7.115}\]

with \(\log C(\boldsymbol{\theta})=\theta_{2}\log\theta_{1}-\log\Gamma(\theta_{2})\). Hence \(\partial\log C(\boldsymbol{\theta})/\partial\theta_{1}=\theta_{2}/\theta_{1}\),

\[\partial\log C(\boldsymbol{\theta})/\partial\theta_{2}=\log\theta_{1}-\frac{1 }{\Gamma(\theta_{2})}\int_{0}^{\infty}e^{-x}x^{\theta_{2}-1}\log x\,dx,\]

\[\partial^{2}\log C(\boldsymbol{\theta})/\partial\theta_{1}^{2}=-\theta_{2}/ \theta_{1}^{2},\]

\[\partial^{2}\log C(\boldsymbol{\theta})/\partial\theta_{1}\partial\theta_{2}= \frac{1}{\theta_{1}},\]

\[\partial^{2}\log C(\boldsymbol{\theta})/\partial\theta_{2}^{2}= \left(\frac{1}{\Gamma(\theta_{2})}\right)^{2}\left(\int_{0}^{\infty}e^{-x}x^{ \theta_{2}-1}\log xdx\right)^{2}\] \[-\frac{1}{\Gamma(\theta_{2})}\left(\int_{0}^{\infty}e^{-x}x^{ \theta_{2}-1}(\log x)^{2}dx\right).\]Since

\[\frac{\partial^{2}\log f_{n}({\bf x};\boldsymbol{\theta})}{\partial\theta_{i} \partial\theta_{i^{\prime}}}=n\,\frac{\partial^{2}\log C(\boldsymbol{\theta})} {\partial\theta_{i}\partial\theta_{i^{\prime}}}\,. \tag{7.116}\]

The estimator \(\delta_{n}\) in (7.115) may now be computed numerically. Note that \(\delta_{n}\) is a first approximation to the solution of the likelihood Eq. (7.114).

### Asymptotic Efficiency of Bayes Estimators

Suppose that the observation vector \({\bf X}\) in a statistical experiment has a p.d.f. \(g({\bf x};\boldsymbol{\theta})\) (w.r.t. a sigma-finite measure \(\nu(d{\bf x})\)), and \(\boldsymbol{\theta}\in\Theta\)--an open subset of \(\mathbb{R}^{p}\). Let \(\pi(d\boldsymbol{\theta})\) be a probability measure (the _prior_) on the Borel sigma-field of \(\Theta\). The Bayesian thinks of \(\boldsymbol{\theta}\) as a random variable with distribution \(\pi(d\boldsymbol{\theta})\), and \(g({\bf x};\boldsymbol{\theta})\) is regarded as the _conditional p.d.f. of \({\bf X}\) given \(\boldsymbol{\theta}\)_. Hence the joint distribution of \({\bf X}\) and \(\boldsymbol{\theta}\) is

\[g({\bf x};\boldsymbol{\theta})\nu(d{\bf x})\pi(d\boldsymbol{\theta}). \tag{7.117}\]

Let \(\pi(d\theta|{\bf x})\) denote the _conditional distribution of \(\theta\) given \({\bf X}={\bf x}\)_, i.e.,

\[\pi(d\theta|{\bf x})=\frac{g({\bf x};\theta)\pi(d\theta)}{\int_{\Theta}g({\bf x };\theta^{\prime})\pi(d\theta^{\prime})}=\frac{g({\bf x};\boldsymbol{\theta}) \pi(d\boldsymbol{\theta})}{\overline{g}({\bf x})}. \tag{7.118}\]

where \(\overline{g}({\bf x})\) is the _marginal p.d.f. of_ \({\bf X}\):

\[\overline{g}({\bf x})=\int_{\Theta}g({\bf x};\boldsymbol{\theta})\pi(d \boldsymbol{\theta}). \tag{7.119}\]

Then \(\pi(d\theta|{\bf x})\) is called the _posterior distribution_ of \(\theta\) given \({\bf X}={\bf x}\).

In this section it will be shown that, under the regularity conditions of Theorem 7.5, and with respect to a prior \(\pi(d\theta)\) having a positive density on \(\Theta\) (with respect to Lebesgue measure on \(\Theta\)) the posterior distribution of \(\theta\) is asymptotically normal with mean \(\hat{\theta}({\bf x})\) (the _maximum likelihood estimator_, i.e., the consistent solution of the likelihood equation) and covariance matrix \(g^{\prime-1}(\boldsymbol{\theta}^{\prime})/n\), a.s. \((P_{\boldsymbol{\theta}^{\prime}})\) for every \(\boldsymbol{\theta}^{\prime}\). Thus, irrespective of the prior density, the Bayes estimator is asymptotically the same as the MLE. We will outline the proof whose details may be found in Bickel and Doksum (2001, pp. 337-345).

Assume for simplicity that \(p=1\) and \(\Theta=(a,b)\), \(-\infty\leq a<b\leq\infty\), and \(\pi(d\theta)\) has a positive and continuous density on \((a,b)\). Let \(X_{1},\ldots,X_{n}\) be i.i.d. with common p.d.f. \(f(x;\theta)\), \({\bf X}=(X_{1},\ldots,X_{n})\) and \(f_{n}({\bf x};\theta)=\prod_{i=1}^{n}f(x_{i};\theta)\)\(\forall\)\({\bf x}=(x_{1},\ldots,x_{n})\). The posterior density of \(\sqrt{n}(\theta-\hat{\theta}_{n})\) at \(t\) is

\[\pi(\hat{\theta}_{n}+\frac{t}{\sqrt{n}}\mid{\bf X})=\frac{\pi(\hat{\theta}_{n }+\frac{t}{\sqrt{n}})f_{n}({\bf X};\hat{\theta}_{n}j+\frac{t}{\sqrt{n}})}{ \int\pi(\hat{\theta}_{n}+\frac{s}{\sqrt{n}})f_{n}({\bf X};\hat{\theta}_{n}+ \frac{s}{\sqrt{n}})ds}\,, \tag{7.120}\]

making the change of variables \(\theta\to s=(\theta-\hat{\theta}_{n})/\sqrt{n}\). Now, given \(\theta=\theta^{\prime}\),

\[\log f_{n}({\bf X};\hat{\theta}_{n}+\frac{s}{\sqrt{n}}) =\log f_{n}({\bf X};\hat{\theta}_{n})+\frac{s^{2}}{2n}\sum_{i=1} ^{n}\frac{d^{2}\log f(X_{i};\theta)}{d\theta^{2}}\bigg{|}_{\hat{\theta}_{n}} +R_{n}\] \[=\log f_{n}({\bf X};\hat{\theta}_{n})+\frac{s^{2}}{2}\,\mathscr{I }_{1}(\hat{\theta}_{n})+R^{\prime}_{n}, \tag{7.121}\]where \(R_{n}\), \(R^{\prime}_{n}\) go to zero in \(P_{\theta^{\prime}}\)-probability as \(n\to\infty\), uniformly for \(|s|\leq M\sqrt{n}\), for every \(M>0\). For simplicity assume this convergence is a.s. \((P_{\theta^{\prime}})\). In particular, \(R^{\prime}_{n}\to 0\) for every \(s\) a.s. \((P_{\theta^{\prime}})\). One may now express (7.121) as

\[f_{n}\left({\bf X};\hat{\theta}_{n}+\frac{s}{\sqrt{n}}\right)=f_{n}({\bf X}; \hat{\theta}_{n})\exp\left\{\frac{s^{2}}{2}\mathscr{I}_{1}(\hat{\theta}_{n}) \right\}(1+o(1))\;\;\forall\,s, \tag{7.122}\]

a.s. \((P_{\theta^{\prime}})\). Applying this to (7.120) one arrives at

\[\pi\left(\hat{\theta}_{n}+\frac{t}{\sqrt{n}}\bigg{|}\,{\bf X}\right) = \frac{\pi(\hat{\theta}_{n}+\frac{t}{\sqrt{n}})f_{n}({\bf X};\hat{ \theta}_{n})\exp\{-\frac{t^{2}}{2}\mathscr{I}_{1}(\hat{\theta}_{n})\}(1+o(1)) }{\int\pi(\hat{\theta}_{n}+\frac{s}{\sqrt{n}})f_{n}({\bf X};\hat{\theta}_{n}) \exp\{-\frac{s^{2}}{2}\mathscr{I}_{1}(\hat{\theta}_{n})\}(1+o(1))ds} \tag{7.123}\] \[= \frac{\pi(\hat{\theta}_{n}+\frac{t}{\sqrt{n}})\exp\{-\frac{t^{2}} {2}\mathscr{I}_{1}(\hat{\theta}_{n})\}(1+o(1))}{\int\pi(\hat{\theta}_{n}+\frac {s}{\sqrt{n}})\exp\{-\frac{s^{2}}{2}\mathscr{I}_{1}(\theta_{n})\}(1+o(1))ds}\] \[\longrightarrow \frac{\exp\{-\frac{t^{2}}{2}\mathscr{I}_{1}(\theta^{\prime})\}}{ \int\exp\{-\frac{s^{2}}{2}\mathscr{I}_{1}(\theta^{\prime})\}ds}=\frac{\exp\{- \frac{t^{2}}{2}\mathscr{I}_{1}(\theta^{\prime})\}}{\sqrt{2\pi/\mathscr{I}_{1}( \theta^{\prime})}}\] \[\approx \sqrt{\frac{\mathscr{I}_{1}(\hat{\theta}_{n})}{2\pi}}\,\exp\left\{ -\frac{t^{2}}{2}\mathscr{I}_{1}(\hat{\theta}_{n})\right\},\]

where the difference between the two sides of \(\approx\) goes to zero a.s. \((P_{\theta^{\prime}})\).

Thus the total variation distance between the posterior distribution of \(\theta\) and the Normal distribution \(N(\hat{\theta}_{n},\frac{1}{n}\mathscr{I}_{1}(\hat{\theta}_{n}))\), or \(N(\hat{\theta}_{n},\frac{1}{n}\mathscr{I}_{1}(\theta^{\prime}))\), goes to zero as \(n\to\infty\), a.s. \(P_{\theta^{\prime}}\), for every (true) parameter value \(\theta^{\prime}\). The posterior distribution concentrates most of its mass near the maximum likelihood estimator \(\hat{\theta}_{n}\). In particular, the asymptotic mean and the asymptotic median of the posterior are both \(\hat{\theta}_{n}\).

We state the result for the general multi-parameter case for which the proof is not substantially different from that outlined above.

**Theorem 7.7** (Bernstein-von Mises Theorem).: _If the assumptions (\(A_{1}\))-(\(A_{5}\)) of Theorem 7.5 hold and the prior \(\pi\) of \(\theta\) has a continuous and positive density on \(\Theta\) then, under \(P_{\boldsymbol{\theta}^{\prime}}\), the total variation distance between the posterior distribution of \(\theta\) and the Normal distribution \(N(\hat{\boldsymbol{\theta}}_{n},\mathscr{I}^{-1}(\boldsymbol{\theta}^{\prime} )/n)\) converges to zero as \(n\to\infty\), a.s. \((P_{\boldsymbol{\theta}^{\prime}})\)._

_Example 7.8_.: Let \(X_{1},\ldots,X_{n}\) be i.i.d. Bernoulli with \(P_{\theta}(X_{i}=1)=\theta\), \(P_{\theta}(X_{i}=0)=1-\theta\), \(\theta\in\Theta=(0,1)\). Let \(\pi(\theta)=\) Beta density with parameters \(\alpha,\beta:\pi(\theta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta )}\theta^{\alpha-1}(1-\theta)^{\beta-1}\) (\(\alpha>0\), \(\beta>0\)). Then

\[f_{n}({\bf x};\theta)=\theta^{\sum_{1}^{n}x_{i}}(1-\theta)^{n- \sum_{1}^{n}x_{i}},\] \[\pi_{n}(\theta|{\bf x})=\frac{\theta^{\sum_{1}^{n}x_{i}+\alpha-1} (1-\theta)^{n-\Sigma_{1}^{n}x_{i}+\beta-1}}{\Gamma(\sum_{1}^{n}x_{i}+\alpha-1) \Gamma(n-\sum_{1}^{n}x_{i}+\beta-1)/\Gamma(n+\alpha+\beta-2)}\,,\]

i.e., the posterior distribution is a beta distribution with parameters \(\sum x_{i}+\alpha\) and \(n-\sum x_{i}+\beta\), so that one can directly show that a beta random variable \(Y\) with this distribution is asymptotically Normal \(N(\hat{\theta}_{n},1/n\,I_{1}(\theta))\) under \(P_{\theta}\), a.s. as \(n\to\infty\) (Exercise 7.12).

### Asymptotic Normality of \(M\)-estimators

Let \(X_{i}\) (\(i=1,2,\ldots,n\)) _be i.i.d. with values in \((\mathscr{X},\mathscr{S})\) defined on \((\varOmega,\mathscr{F},P_{\emptyset})\)_.

An \(M\)_-estimator_ (or a maximum likelihood type estimator), a term coined by Huber (1981), may be defined to be an estimator \(\hat{\theta}_{n}=\hat{\theta}_{n}\) (\(X_{1},\ldots,X_{n}\)) which maximizes (on \(\Theta\)) a real-valued function of the form

\[g_{n}(\theta):=\frac{1}{n}\sum_{i=1}^{n}g(X_{i},\theta),\qquad(\theta\in \Theta\ \text{open}\ \subset\mathbb{R}^{k}) \tag{7.124}\]

where \(g(x,\theta):\mathscr{X}\times\Theta\to\mathbb{R}\) is measurable (w.r.t. the given sigma-field \(\mathscr{S}\) on \(\mathscr{X}\)) for each \(\theta\in\Theta\). Just as in the case of the MLE, it is more common to define an \(M\)-estimator as a critical point of \(g_{n}(\theta)\), i.e., an estimator which satisfies

\[\frac{\partial g_{n}(\theta)}{\partial\theta_{r}}=0,\qquad 1\leq r\leq k. \tag{7.125}\]

Writing \(h(x,\theta)=\operatorname{grad}g(x,\theta)\equiv(\partial g(x,\theta)/ \partial\theta_{1},\ldots,\ \partial g(x,\theta)/\partial\theta_{k})\), one may rewrite (7.125) as

\[h_{n}^{(r)}(\theta)=0\qquad(1\leq r\leq k), \tag{7.126}\]

where

\[h_{n}^{(r)}(\theta)=\frac{1}{n}\sum_{i=1}^{n}h^{(r)}(X_{i},\theta),\qquad 1\leq r \leq k. \tag{7.127}\]

More generally, we define _an \(M\)-estimator \(\hat{\theta}_{n}\) to be a solution of a vector equation_

\[h_{n}(\theta)=0\qquad[\text{i.e., }h_{n}^{(r)}(\theta)=0\ \ \ \ (1\leq r\leq k)] \tag{7.128}\]

where \(h_{n}(\theta)\equiv(h_{n}^{(1)}(\theta,\ldots,\)\(h_{n}^{(k)}(\theta))^{\prime}\) is defined by (7.127), \(h^{(r)}(x,\theta)\) being, for each \(r\ (=1,2,\ldots,k)\), a real-valued (measurable) function on \(\mathscr{X}\times\Theta\).

**Theorem 7.8**.: _Let \(\Theta\) be an open subset of \(\mathbb{R}^{k}\), and let \(\hat{\theta}_{n}\) be a consistent solution of (7.128). Assume (i) \(E_{\theta}h(X_{1},\theta)=0\ \forall\ \theta\in\Theta\), (ii) \(\theta\to h(x,\theta)\) is twice continuously differentiable on \(\Theta\), for every \(x\in\mathscr{X}\), (iii) \(\varGamma(\theta)\equiv E_{\theta}\mathrm{Grad}\,h(X_{1},\theta)\) is a nonsingular \(k\times k\) matrix \((\forall\ \theta\in\Theta)\), whose \(r\)-th row is given by_

\[E_{\theta}(\operatorname{grad}h^{(r)}(X_{1},\theta))^{\prime}\equiv(E_{\theta }(\partial h^{(r)}(X_{1},\theta)/\partial\theta_{1}),\ldots,E_{\theta}( \partial h^{(r)}(X_{1},\theta)/\partial\theta_{k})),\]

_(iv) \(V(\theta)\equiv\text{Cov}_{\emptyset}h(X_{1},\theta)\) is finite and nonsingular \(\forall\ \theta\in\Theta\), and (v) for each \(\theta\in\Theta\), there exists \(\delta(\theta)>0\) such that \(\sup_{\{\theta^{\prime}:|\theta^{\prime}-\theta|\leq\delta(\theta)\}}\left| \frac{\partial^{2}h^{(r)}(x,\theta)}{\partial\theta_{i}\partial\theta_{j}} \right|_{\theta^{\prime}}\leq b_{ij}^{(r)}(x)\) where_

\[E_{\theta}b_{ij}^{(r)}(X_{1})<\infty\qquad\forall\ 1\leq r,i,j\leq k. \tag{7.129}\]

_Then_

\[\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\xrightarrow{\mathscr{L}}N(0,\varGamma^{ -1}(\theta_{0})V(\theta_{0})(\varGamma^{-1}(\theta_{0}))^{\prime})\]

_under \(P_{\theta_{0}}\ (\forall\ \theta_{0}\in\Theta)\)._

Proof.: For simplicity of notation, let us first consider the case \(k=1\). Then \(h\) is real-valued and \(\theta\) is one-dimensional. One has, by a Taylor expansion,\[0=h_{n}(\hat{\theta}_{n})=h_{n}(\theta_{0})+(\hat{\theta}_{n}-\theta_{0})\left( \frac{dh_{n}(\theta)}{d\theta}\right)_{\theta=\theta_{0}}+\frac{1}{2}(\hat{ \theta}_{n}-\theta_{0})^{2}\left(\frac{d^{2}h_{n}(\theta)}{d\theta^{2}}\right)_ {\theta=\theta_{n}^{*}} \tag{7.130}\]

on the set \(A_{n}=\{|\hat{\theta}_{n}-\theta_{0}|<\delta(\theta_{0})\},\) where \(|\theta_{n}^{*}-\theta_{0}|\leq|\hat{\theta}_{n}-\theta_{0}|<\delta(\theta_{0}).\) Let \(B_{n}=\left\{\left(\frac{dh_{n}(\theta)}{d\theta}\right)_{\theta_{0}}+\frac{1} {2}(\hat{\theta}_{n}-\theta_{0})\left(\frac{d^{2}h_{n}(\theta)}{d\theta^{2}} \right)_{\theta_{n}^{*}}\neq 0\right\}.\) Then \(P_{\theta_{0}}(A_{n})\to 1,\) by consistency of \(\hat{\theta}_{n}.\) Also, under \(P_{\theta_{0}},\)

\[\left(\frac{dh_{n}(\theta)}{d\theta}\right)_{\theta_{0}}\equiv\frac{1}{n}\sum_ {i=1}^{n}\left(\frac{dh(X_{i},\theta)}{d\theta}\right)_{\theta_{0}}\longrightarrow E _{\theta_{0}}\left(\frac{dh(X_{1},\theta)}{d\theta}\right)_{\theta_{0}}= \Gamma(\theta_{0})\quad\text{a.s.} \tag{7.131}\]

Moreover, on \(A_{n}\) (using (v) and writing \(b\) for \(b_{11}\)),

\[\left|\left(\frac{d^{2}h_{n}(\theta)}{d\theta^{2}}\right)_{\theta_{n}^{*}} \right|\leq\frac{1}{n}\sum_{i=1}^{n}b(X_{i}),\]

so that, under \(P_{\theta_{0}},\)

\[\varlimsup_{n\to\infty}\left|\left(\frac{d^{2}h_{n}(\theta)}{d\theta^{2}} \right)_{\theta_{n}^{*}}\right|\leq\varlimsup_{n\to\infty}\frac{1}{n}\sum_{i= 1}^{n}b(X_{i})=E_{\theta_{0}}b(X_{1})<\infty.\]

It follows that, under \(P_{\theta_{0}},\)

\[\left(\frac{dh_{n}(\theta)}{d\theta}\right)_{\theta_{0}}+\frac{1}{2}(\hat{ \theta}_{n}-\theta_{0})\left(\frac{d^{2}h_{n}(\theta)}{d\theta^{2}}\right)_{ \theta_{n}^{*}}\text{ converges in }(P_{\theta_{0}^{-}})\text{ probability to }\Gamma( \theta_{0}). \tag{7.132}\]

In particular, \(P_{\theta_{0}}(B_{n})\to 1,\) and (7.130) yields

\[\sqrt{n}(\hat{\theta}_{n}-\theta_{0})=\frac{-\sqrt{n}\,h_{n}(\theta_{0})}{ \Gamma(\theta_{0})+o_{p}(1)}\ \stackrel{{\mathscr{L}}}{{\longrightarrow}}\ N(0,\Gamma^{-2}( \theta_{0})\,V\,(\theta_{0})), \tag{7.133}\]

since \(-\sqrt{n}\,h_{n}(\theta_{0})=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}(-h(X_{i},\theta_ {0}))\) and \(-h(X_{i},\theta_{0})\) has mean zero and variance \(V(\theta_{0}),\) under \(P_{\theta_{0}}.\)

For \(k>1\) one may write, in place of (7.130)

\[0 = h_{n}^{(r)}(\hat{\boldsymbol{\theta}}_{n})\] \[= h_{n}^{(r)}(\boldsymbol{\theta}_{0})+(\text{grad}\,h_{n}^{(r)}( \boldsymbol{\theta}_{0}))^{\prime}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol {\theta}_{0})\] \[+\frac{1}{2}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0 })^{\prime}H_{n}^{(r)}(\boldsymbol{\zeta}_{n}^{(r)})(\hat{\boldsymbol{\theta }}_{n}-\boldsymbol{\theta}_{0})\qquad(1\leq r\leq k),\]

where \(|\boldsymbol{\zeta}_{n}^{(r)}-\boldsymbol{\theta}_{0}|\leq\delta(\boldsymbol {\theta}_{0}),\) and \(H_{n}^{(r)}(\boldsymbol{\theta})=\left(\left(\frac{\partial^{2}}{\partial \theta_{i}\partial\theta_{j}}\,h_{n}^{(r)}(\boldsymbol{\theta})\right)\right)_ {1\leq i,j\leq k}\). In matrix notation one may express (7.134) as

\[0=h_{n}(\boldsymbol{\theta}_{0})+\text{Grad }h_{n}(\boldsymbol{\theta}_{0})( \hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})+\left[\frac{1}{2}(\hat{ \boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})^{\prime}H_{n}^{(r)}(\zeta_{n }^{(r)})\right]_{1\leq r\leq k}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{ \theta}_{0}) \tag{7.135}\]

where \([\quad]_{1\leq r\leq k}\) is a \(k\times k\) matrix whose \(r\)-th column is \(\frac{1}{2}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})^{\prime}H_{n}^ {(r)}(\boldsymbol{\zeta}_{n}^{(r)}).\) By assumption (v) and the consistency of \(\hat{\boldsymbol{\theta}}_{n},\) all elements of this matrix converge to zero in probability, by the same argument as in the case \(k=1.\) Hence one may rewrite (7.135) as \[-\sqrt{n}\,h_{n}(\boldsymbol{\theta}_{0})=\left\{\operatorname{Grad}h_{n}( \boldsymbol{\theta}_{0})+\left[\frac{1}{2}(\hat{\boldsymbol{\theta}}_{n}- \boldsymbol{\theta}_{0})^{\prime}H_{n}^{(r)}(\boldsymbol{\zeta}_{n}^{(r)}) \right]_{1\leq r\leq k}\right\}\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}- \boldsymbol{\theta}_{0}), \tag{7.136}\]

or, on the set \(A_{n}\cap B_{n}\), with \(B_{n}=\) the expression \(\{\quad\}\) within curly brackets in (7.136) is nonsingular,

\[\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})=\left\{ \qquad\right\}^{-1}(-\sqrt{n}\,h_{n}(\boldsymbol{\theta}_{0})). \tag{7.137}\]

Since (7.124) \(-\sqrt{n}\,h_{n}(\boldsymbol{\theta}_{0})\to N(0,V(\boldsymbol{\theta}_{0}))\), under \(P_{\boldsymbol{\theta}_{0}}\) (by CLT), and \(\{\quad\}\) converges to \(\varGamma(\boldsymbol{\theta}_{0})\) in \(P_{\theta_{0}}\)-probability, it follows that

\[\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta}_{0})\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\varGamma^{-1}(\boldsymbol{\theta}_{0}) \mathbf{Z},\qquad\mathbf{Z}\stackrel{{\mathscr{L}}}{{\sim}}N( \mathbf{0},V(\boldsymbol{\theta}_{0})).\]

_Remark 7.13_.: The hypothesis of Theorem 7.8 may be relaxed. Instead of assuming \(\theta\to h(x,\theta)\) is twice continuously differentiable (and (7.129) holds), it is enough to assume that \(\theta\to h(x;\theta)\) is (once) continuously differentiable, and that for each \(\theta_{0}\in\Theta\), and every \(\varepsilon>0\), there exists \(\delta(\varepsilon,\theta_{0})>0\) such that

\[\sup\left\{\left|\left(\frac{\partial h^{(r)}(x,\theta^{n})}{\partial\theta_{i }}\right)_{\theta^{\prime}}-\left(\frac{\partial h^{(r)}(x,\theta)}{\partial \theta_{i}}\right)_{\theta_{0}}\right|:|\theta^{\prime}-\theta_{0}|\leq\delta (\varepsilon,\theta_{0})\right\}\leq\varepsilon b_{i}(x),\]

with \(E_{\theta_{0}}b_{i}(X_{1})<\infty\)\(\forall\)\(i\).

_Example 7.9 (MLE)._ Let \(f(x;\boldsymbol{\theta})\) be the p.d.f. of \(X_{i}\) (under \(P_{\boldsymbol{\theta}}\)) w.r.t. a \(\sigma\)-finite measure \(\mu(dx)\). The log likelihood equations for the MLE \(\hat{\boldsymbol{\theta}}_{n}\) are

\[\sum_{i=1}^{n}\frac{\partial\log f(X_{i};\boldsymbol{\theta})}{\partial \theta_{r}}=0\qquad(1\leq r\leq k), \tag{7.138}\]

which one may write as (7.126) or (7.128), with \(h^{(r)}(x,\boldsymbol{\theta})=\partial\log f(x;\boldsymbol{\theta})/\partial \theta_{r}\) (\(1\leq r\leq k\)).

_Example 7.10 (Method of Moments)._ Let \(X_{i}\) be real-valued, and

\[m_{r}(\boldsymbol{\theta})=EX_{i}^{r}\qquad(r=1,2,\ldots,k) \tag{7.139}\]

finite. The _method of moments_ provides an estimator \(\hat{\boldsymbol{\theta}}_{n}\) which solves the equations

\[h_{n}^{(r)}(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{r}-m_{r}( \boldsymbol{\theta})=0\qquad(1\leq r\leq k). \tag{7.140}\]

Here \(h^{(r)}(x,\boldsymbol{\theta})=X^{r}-m_{r}(\boldsymbol{\theta})\), \(1\leq r\leq k\). More generally, one may choose \(k\) functions \(\psi_{r}(x)\), \(1\leq r\leq k\), such that \(E_{\boldsymbol{\theta}}|\psi_{r}(X_{1})|<\infty\), and solve (for \(\theta\))

\[h_{n}^{(r)}(\boldsymbol{\theta})\equiv\frac{1}{n}\sum_{i=1}^{n}\psi_{r}(X_{i} )-E_{\boldsymbol{\theta}}\psi_{r}(X_{1})=0\qquad(1\leq r\leq k), \tag{7.141}\]

so that \(h^{(n)}(x,\boldsymbol{\theta})=\psi_{r}(x)-E_{\boldsymbol{\theta}}\psi_{r}(X_{1})\).

### Asymptotic Efficiency and Super Efficiency

_When does the variance of an unbiased estimator \(T_{n}\) attain the lower bound given by the information inequality?_ Under \((R_{0})\)-\((R_{2})\), this happens if and only if the coefficient of correlation between \(T_{n}-E_{\theta}T_{n}\) and \(\frac{d\log f_{n}(\mathbf{X};\theta)}{d\theta}\) is \(+1\) or \(-1\), i.e.,

\[T_{n}(\mathbf{x})-E_{\theta}T_{n}=\lambda_{n}(\theta)\,\frac{d\log f_{n}( \mathbf{x};\theta)}{d\theta}\]

with \(P_{\theta}\)-probability one for some \(\lambda_{n}\). Thus

\[\log f_{n}(\mathbf{x};\theta)=\int\frac{(T_{n}(\mathbf{x})-g_{n}(\theta))}{ \lambda_{n}(\theta)}\,d\theta=\pi_{n}(\theta)T_{n}(\mathbf{x})-\gamma_{n}( \theta)+\varphi_{n}(\mathbf{x}).\]

In view of the information inequality (7.1) or (7.5), the following definition is reasonable.

**Definition 7.3**.: Under \((R_{0})\)-\((R_{2})\) a sequence of estimators \(T_{n}\) (\(n=1,2,\dots\)) of \(g(\theta)\) is _asymptotically efficient (in Fisher's sense)_ if

\[\sqrt{n}(T_{n}-g(\theta))\,\stackrel{{\mathscr{L}}}{{\longrightarrow }}\,N(0,\sigma^{2}(\theta)) \tag{7.142}\]

with

\[\sigma^{2}(\theta)=\frac{1}{E_{\theta}\left(\frac{d\log f(X_{1};\theta)}{d \theta}\right)^{2}}\,. \tag{7.143}\]

_Remark 7.14_.: Note that an asymptotically efficient \(T_{n}\) in the above sense is asymptotically normal with mean \(g(\theta)\) and variance equal to the information lower bound. This _does not_ necessarily _imply that_\(g(\theta)\) is the mean of \(T_{n}\), i.e., \(T_{n}\)_is unbiased._ But it implies that \(T_{n}\)_is consistent._ Suppose \(E_{\theta}T_{n}=g_{n}(\theta)\) and \(\operatorname{var}E_{\theta}T_{n}^{2}\to 0\). Then one may show by Chebyshev's inequality that \(T_{n}-g_{n}(\theta)\,\stackrel{{ P}}{{\longrightarrow}}\,0\). But by consistency of \(T_{n}\) as an estimator of \(g(\theta)\), \(T_{n}-g(\theta)\,\stackrel{{ P}}{{\longrightarrow}}\,0\). Therefore, the _bias_\(g_{n}(\theta)-g(\theta)\to 0\) as \(n\to\infty\). Note that (7.5) also implies for biased estimators \(T_{n}\)

\[E_{\theta}(T_{n}-g(\theta))^{2} \geq\operatorname{var}_{\theta}T_{n}\geq\frac{\left(\frac{d}{d \theta}E_{\theta}T_{n}\right)^{2}}{nE_{\theta}\left(\frac{d\log f(X_{1};\theta )}{d\theta}\right)^{2}}\] \[=\frac{\left[\frac{d}{d\theta}(g(\theta)+E_{\theta}T_{n}-g(\theta ))\right]^{2}}{nE_{\theta}\left(\frac{d\log f(X_{1};\theta)}{d\theta}\right)^ {2}}=\frac{(g^{\prime}(\theta)+b_{n}^{\prime}(\theta))^{2}}{nE_{\theta}\left( \frac{d\log f(X_{1};\theta)}{d\theta}\right)^{2}}\,, \tag{7.144}\]

where \(b_{n}(\theta)=E_{\theta}T_{n}-g(\theta)\) is the _bias_. Since in the present case \(b_{n}(\theta)\to 0\) as \(n\to\infty\), it is usually true that \(b_{n}^{\prime}(\theta)\to 0\) as \(n\to\infty\). Hence, from that asymptotic point of view, one need not restrict attention only to unbiased estimators in order to compare efficiencies in Fisher's sense.

_Remark 7.15_.: Relation (7.142) _does not imply_

\[E_{\theta}\left(\sqrt{n}(T_{n}-g(\theta))\right)^{2}\longrightarrow\sigma^{2} (\theta)\quad\text{as }n\to\infty. \tag{7.145}\]

For the function \(f(x)=x^{2}\) is _not bounded_. Thus \(\frac{\sigma^{2}(\theta)}{n}\) is _not,_ in general, _the asymptotic variance of \(T_{n}\); it is the variance of the asymptotic distribution of_\(T_{n}\). In general one can only prove\[\underline{\lim}_{n\to\infty}\ E_{\theta}\left(\sqrt{n}(T_{n}-g(\theta))\right)^{2} \geq\sigma^{2}(\theta), \tag{7.146}\]

if (7.142) holds. (See Lehmann, _Theory of Point Estimation,_ Lemma 5.1.2).

_Remark 7.16_.: For statistical experiments satisfying \((R_{0})\)-\((R_{2})\) the information inequality (7.5) holds. However, the equality in (7.5) rarely holds. We have seen that the equality in (7.5), under mild additional conditions, implies that \(f(x;\theta)\) belongs to the exponential family. Even in the exponential case, however, the equality holds only for the estimation of \(\frac{1}{n}E_{\theta}T_{n}(\mathbf{x})=(\frac{d}{d\pi})\log\int h(x)e^{\pi T_{ 1}(x)}\mu(dx)\) or some linear function of it, and _not_ for other parametric functions. On the other hand, it will be shown that there are asymptotically efficient estimators of every'smooth' parametric function in the exponential family case and, more generally, when \((R_{0})\)-\((R_{2})\) and some additional conditions are satisfied.

_Remark 7.17_ (_Superefficiency)_.: Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. \(N(\theta,1)\). Hodges pointed out that the estimator

\[T_{n}=\left\{\begin{array}{ll}\overline{X}=\frac{1}{n}\sum_{j=1}^{n}X_{j}& \mbox{for }|\overline{X}|>n^{-\frac{1}{4}}\\ \alpha\overline{X}&\mbox{for }|\overline{X}|\leq n^{-\frac{1}{4}},\end{array}\right. \tag{7.147}\]

where \(\alpha\) is a constant, \(0\leq\alpha<1\), satisfies (where \(\theta\) is the true value)

\[\begin{array}{ll}\sqrt{n}(T_{n}-\theta)&\stackrel{{\mathscr{L}} }{{\longrightarrow}}\ N(0,1)&\mbox{if }\theta\neq 0,\\ \sqrt{n}(T_{n}-0)&\longrightarrow N(0,\alpha^{2})&\mbox{if }\theta=0.\end{array} \tag{7.148}\]

In other words, at all \(\theta\neq 0\) the estimator \(T_{n}\) is asymptotically efficient; and at \(\theta=0\) the variance of the asymptotic distribution of \(\sqrt{n}(T_{n}-0)\) is \(\alpha^{2}\), which is _smaller than_ the information lower bound 1 (note that \(\sqrt{n}(\overline{X}-\theta)\to N(0,1)\) for all \(\theta\)). Such an estimator has come to be known as _superefficient_, and the point \(\theta=0\) as a _point of superefficient_ of \(T_{n}\). To check (7.148) note that, for \(\theta\neq 0\),

\[P_{\theta}\left(|\sqrt{n}(T_{n}-\theta)-\sqrt{n}(\overline{X}- \theta)|>0\right)\] \[= P_{\theta}\left(|\overline{X}|\leq n^{-\frac{1}{4}}\right)=\int _{-n^{-\frac{1}{4}}}^{-\frac{1}{4}}\frac{\sqrt{n}}{\sqrt{2\pi}}\,e^{-n(x- \theta)^{2}/2}dx\] \[= \frac{1}{\sqrt{2\pi}}\int_{-\sqrt{n}(-n^{-\frac{1}{4}}-\theta)}^{ \sqrt{n}(n^{-\frac{1}{4}}-\theta)}e^{-y^{2}/2}dy\leq\frac{1}{\sqrt{2\pi}}\left( 2\sqrt{n}\,n^{\frac{1}{4}}e^{-(|\theta\sqrt{n}|-n^{\frac{1}{4}})^{2}/2}\right)\] \[= \frac{2n^{\frac{1}{2}}}{\sqrt{2\pi}}\,e^{\frac{n^{\frac{1}{2}}}{2 }(|n^{\frac{1}{4}}\theta|-1)^{2}}=n^{\frac{1}{4}}\cdot o\left(e^{-n^{\frac{1}{ 2}/2}}\right)\longrightarrow 0\mbox{ as }n\to\infty. \tag{7.149}\]

This means that

\[\sqrt{n}(T_{n}-\theta)=\sqrt{n}(\overline{X}-\theta)+R_{n}\quad\mbox{with }R_{n} \stackrel{{ P}}{{\longrightarrow}}\,0. \tag{7.150}\]

Since \(\sqrt{n}(\overline{X}-\theta)\) is \(N(0,1)\), the first relation in (7.148) follows (by Theorem 6.1). Now, if \(\theta=0\), then

\[\sqrt{n}\,T_{n}=\alpha\sqrt{n}\ \overline{X}\cdot I_{\{|\overline{X}|\leq n^{- \frac{1}{4}}\}}+R^{\prime}_{n},\quad R^{\prime}_{n}=\sqrt{n}\ \overline{X}\cdot I_{\{|\overline{X}|>n^{-\frac{1}{4}}\}}\,. \tag{7.151}\]

Since \(\theta=0\), \(\sqrt{n}\ \overline{X}\) is \(N(0,1)\). Also,\[\text{Prob}\left(I_{\{|\overline{X}|\leq n^{-\frac{1}{4}}\}}=1\right) =\text{Prob}\left(|\overline{X}|\leq n^{-\frac{1}{4}}\right)= \frac{1}{\sqrt{2\pi}}\int_{-n^{\frac{1}{4}}}^{n^{\frac{1}{4}}}e^{-y^{2}/2}dy \longrightarrow 1\] \[\text{as }n\to\infty;\] \[\text{Prob}(R^{\prime}_{n}=0) =\text{Prob}\left(|\overline{X}|\leq n^{-\frac{1}{4}}\right) \longrightarrow 1\text{ as }n\to\infty. \tag{7.152}\]

Therefore, (by Theorem 6.1) \(\sqrt{n}\,T_{n}\to\alpha Z\cdot 1+0=\alpha Z\) where \(Z\) is \(N(0,1)\), proving (7.148). Hence the first term on the right side in (7.151) converges in distribution to \(\alpha Z\), where \(Z\) is \(N(0,1)\), while the second term goes to \(Z\cdot 0=0\) in probability. Thus, under \(\theta=0\), \(\sqrt{n}\,T_{n}\ \xrightarrow[]{\mathscr{L}}\ N(0,\alpha^{2})\).

A calculation of the risk function \(\mathscr{R}_{n}(\theta,T_{n})\equiv E_{\theta}(T_{n}-\theta)^{2}\) shows, however, that (Exercise 7.22)

\[\mathscr{R}_{n}(\theta,T_{n})=\frac{A_{n}(\theta)}{n}\quad\text{ with }\quad\sup_{|\theta|\leq n^{-\frac{1}{4}}}A_{n}(\theta)=\infty, \tag{7.153}\]

whereas

\[\mathscr{R}_{n}(\theta,\overline{X})=\frac{1}{n}\quad\forall\ \theta. \tag{7.154}\]

Thus an event near the point of superefficiency \(T_{n}\) behaves poorly compared to \(\overline{X}\), if one would like to control the risk uniformly for all \(\theta\) in at least a small neighborhood of each parameter point.

Remark Remark 7.18: Under slightly stronger conditions than \((R_{0})\)-\((R_{2})\) it has been proved by Le Cam (1953), and by Bahadur (1958), that if an estimator \(T_{n}\) of \(\theta\) is asymptotically \(N(\theta,\frac{\sigma^{2}(\theta)}{n})\) (i.e., \(\sqrt{n}(T_{n}-\theta)\ \xrightarrow[]{\mathscr{L}}\ N(0,\sigma^{2}(\theta))\)), then the points of superefficiency (i.e., \(\theta\) such that \(\sigma^{2}(\theta)<1/E_{\theta}((d\log f(X_{1};\theta)/d\theta)^{2})\) constitute at most a set of Lebesgue measure zero). If one requires that the normalized risk function \(n\mathscr{R}_{n}(\theta,T_{n})\) perform _uniformly_ well in an interval of width \(O(n^{-\frac{1}{2}})\) around each parameter point \(\theta_{0}\), then under the same regularity conditions LeCam and Hajek proved that there do not exist estimators which are superior to asymptotically normal estimators.

## Exercises for Chap. 7

### Ex. 7.1

Let \(\mathbf{X}=(X_{1},\ldots,X_{n})\) where \(X_{i}\)'s are i.i.d. uniform on \((0,\theta)\) (\(\theta\in\Theta=(0,\infty)\)). Show that

1. \(M_{n}\equiv\max\{X_{1},\ldots,X_{n}\}\) is the MLE of \(\theta\), and
2. \(E_{\theta}M_{n}=\frac{n}{n+1}\,\theta\), \(\text{var}_{\theta}M_{n}=\frac{n}{(n+2)(n+1)^{2}}\theta^{2}\), \(E_{\theta}\left(\frac{n+1}{n}\,M-\theta\right)^{2}=\frac{1}{n(n+2)}\,\theta^{2}\).

**Ex. 7.2**.:
1. Show that the hypothesis of Theorem 7.1 holds if \((R_{0})\), \((R_{1})\), \((R_{2})\) stated in Remark 7.3 hold.
2. Show that the hypothesis of Theorem 7.1 holds for one-parameter exponential families as stated in Remark 7.5.

3. In the context of Remark 7.5 show that the Cramer-Rao lower bound is only attained by linear functions of \(T\).

**Ex. 7.3**.: Show that _truncated exponential families_ are also exponential.

**Ex. 7.4** (Dasgupta 2008).: _(Missing data)_. In a city the number of traffic accidents per day is assumed to follow the Poisson distribution \(\mathscr{P}(\lambda)\).

1. Suppose records are available only for days with more than \(r\) accidents, and the number of days with \(r\) or less accidents is not known. Show that Theorem 7.2 applies for the estimation of \(\lambda\), and find the asymptotic distribution of the MLE \(\hat{\lambda}\). Do not attempt to find closed form expressions. [Hint: Use the truncated \(\mathscr{P}(\lambda)\).]
2. Suppose the over a period of \(n\) days \(n-m\) days have more than \(r\) accidents and the numbers of accidents on these days are recorded. For the remaining \(m\) days with \(r\) or less accidents the actual numbers of accidents are not recorded. Regarding \(m\) as random write down the likelihood equation. Show that Theorem 7.2 applies for the MLE.
3. One way of dealing with the missing observations in (b) is to compute \(\hat{\lambda}\) as in (a) based on the \(n-m\) observations fully recorded, and then replace the missing data by a random sample of size \(m\) drawn from \(\mathscr{P}(\hat{\lambda})\). Considering the \(n\) observations so obtained as a random sample from \(\mathscr{P}(\lambda)\), the "MLE" \(\lambda^{*}\) is computed. As an example, compute numerically the estimates of \(\lambda\) in (a) (with \(n-m\) observations), (b), (c), from a random sample of size \(n=100\) from \(\mathscr{P}(10)\), letting \(r=2\). Do the same with \(r=1\).

**Ex. 7.5** (Bhattacharyya Inequality).: Suppose the pd.f. \(g(\mathbf{x};\theta)\) of \(\mathbf{X}\) belongs to a one-parameter exponential family. Let \(T=t(\mathbf{X})\) be as in Theorem 7.1, and write \(u_{r}=c^{(r)}(\theta)\), the \(r\)-th derivative of \(c(\theta)=E_{\theta}T\), \(\mathbf{u}=(u_{1},\ldots,u_{k})^{\prime}\), \(k\geq 1\). Also write \(a_{rs}=E_{\theta}((g^{(r)}(\mathbf{X};\theta)/g)(g^{(s)}(\mathbf{X};\theta)/g))\), \(A=((a_{rs}))_{1\leq r,s\leq k}\).

1. Derive the inequality (Bhattacharyya 1946) \[\mathrm{var}_{\theta}T\geq\mathbf{u}^{\prime}A^{-1}\mathbf{u}.\] [Hint: \(\mathrm{cov}_{\theta}(T,\sum_{r=1}^{k}b_{r}g^{(r)}/g)=\sum_{r}b_{r}\mathrm{ cov}_{\theta}(T,g^{(r)}/g)=\sum_{r}b_{r}u_{r}\), so that \(\mathrm{var}_{\theta}T\geq(\mathbf{b}^{\prime}\mathbf{u})^{2}/\sum_{r,s}b_{r }b_{s}a_{rs}=(\mathbf{b}^{\prime}\mathbf{u})^{2}/\mathbf{b}^{\prime}A\mathbf{ b}=(\mathbf{\gamma}^{\prime}A^{-\frac{1}{2}})^{2})/\|\mathbf{\gamma}\|^{2}\) (\(\boldsymbol{\gamma}:=A^{-\frac{1}{2}}\mathbf{b}\)). The supremum of the last quantity over all \(\mathbf{b}\neq\mathbf{0}\) equals \(\sup_{\{\|\boldsymbol{\gamma}\|=1\}}\) (\(\boldsymbol{\gamma}^{\prime}A^{-\frac{1}{2}})^{2}=((\mathbf{u}^{\prime}/\| \mathbf{u}\|)A^{-\frac{1}{2}}\mathbf{u})^{2}=\mathbf{u}^{\prime}A^{-1}\mathbf{ u}\).]
2. Let \(\mathbf{X}=(X_{1},\ldots,X_{n})\), \(X_{i}\)'s i.i.d. \(N(\mu,1)\), \(\mu\in\mathbb{R}\). Let the MLE \(T=\overline{X}^{2}\) estimate \(\mu^{2}\). Find the lower bound of \(\mathrm{var}_{\theta}T\) using \(k=1\) (Cramer-Rao) and \(k=2\) (Bhattacharyya), and compare these with the true variance of \(\overline{X}^{2}\). Note that the UMVU estimator of \(\mu^{2}\) is \(\overline{X}^{2}-\frac{1}{n}\), having the same asymptotic distribution around \(\mu^{2}\) as the MLE.

**Ex. 7.6** (Hardy-Weinberg Model).: The Hardy-Weinberg formula for probabilities of the three genotypes of a single gene with two alleles is \(p_{1}=\theta^{2}\), \(p_{2}=2\theta(1-\theta)\), \(p_{3}=(1-\theta)^{2}\), \(0<\theta<1\). A random sample of size \(n\) from the populations yields frequencies \(n_{1}\), \(n_{2}\), \(n_{3}\) for the three genotypes, \(n_{1}+n_{2}+n_{3}=n\)1. Show that this is a one-parameter exponential family. Find the UMVU estimator of \(\theta\) and show that it coincides with the MLE \(\hat{\mathbf{\theta}}\).
2. Compare \(\hat{\theta}\) with the naive estimator \(\sqrt{n_{1}/n}\).

**Ex. 7.7.** Instead of the Hardy-Weinberg formula in Exercise 7.6, assume \(p_{1}=\pi\), \(p_{2}=\pi^{2}\), \(P_{3}=1-\pi-\pi^{2}\) (\(0<\pi<(\sqrt{5}-1)/2\)).

1. Show that the (log-) likelihood equation has a consistent solution \(\hat{\pi}\).
2. Find the asymptotic distribution of \(\hat{\pi}\).
3. Find the asymptotic distribution of \(\hat{p}_{1}\) and compute the asymptotic relative efficiency \(e_{\hat{p}_{1},\hat{\pi}}\).

**Ex. 7.8.** In Example 7.4 let the parameter space \(\Theta\) be \((0,\infty)\).

1. Find the asymptotic distribution of the MLE.
2. Compare the estimate in (a) with \(\overline{X}^{+}\equiv\max\{\overline{X},0\}\).

**Ex. 7.9.** To study if animals are free of a toxin after being fed a contaminated feed, a veterinarian combines blood samples from \(m\) different animals to determine if the combined specimen is free of the toxin. Let \(X=0\) if the combined specimen indicates presence of the toxin and \(X=1\) otherwise. Let \(\pi\) be the probability that an animal is free of the toxin. What is the distribution of \(X\)? Suppose a random sample \(\{X_{1},\ldots,X_{n}\}\) of such combined samples is drawn. Find the MLE \(\hat{\pi}\) of \(\pi\) and the asymptotic distribution of \(\hat{\pi}\).

**Ex. 7.10.** Let \(X_{1},\ldots,X_{n}\) be i.i.d. binomial \(\mathscr{B}(m,p)\), \(0<p<1\).

1. Find the UMVU estimate \(\tilde{\pi}\) of \(\pi(p)=(1-p)^{m}\) (= probability of "no success").
2. Find the MLE \(\hat{\pi}\) of \(\pi(p)\) and show that \(\sqrt{n}(\tilde{\pi}-\hat{\pi})\to 0\) in probability as \(n\to\infty\).
3. Find the asymptotic distributions of \(\tilde{\pi}\), \(\hat{\pi}\).

**Ex. 7.11.** Suppose \(X_{1},\ldots,X_{n}\) are independent random variables with \(X_{i}\) having the Poisson distribution \(\mathscr{P}(c_{i}\theta)\) where \(c_{i}\)'s are known positive constants (\(i=1,2,\ldots\)), and \(\theta>0\) is an unknown parameter.

1. Find the MLE \(\hat{\theta}\) of \(\theta\) and prove that it is also UMVU.
2. Find a reasonably broad criterion (i.e., condition on the sequence of constants \(c_{i}\), \(i\geq 1\)) for the consistency of \(\hat{\theta}\). [Hint: Calculate \(\mbox{var}_{\theta}(\hat{\theta})\).]
3. Prove that \(\hat{\theta}\) is inconsistent if \(\sum c_{i}\) converges to a finite limit \(\delta\). [Hint: In this case \(\hat{\theta}-\theta\) is approximately of the form \(\sum_{1}^{n}Y_{i}-\delta\) where \(Y_{1}\), \(Y_{2},\ldots\), are independent positive random variables.]
4. Use the Lindeberg CLT to find a broad condition for the asymptotic Normality of \(\hat{\theta}\) around \(\theta\).

**Ex. 7.12 (Neyman-Scott).** Let \(X_{ij}\) be independent \(N(\mu_{i},\sigma^{2})\) random variables (\(i=1,\ldots,n\); \(j=1,\ldots,k\)).

1. Show that the MLE of the parameter \(\mathbf{\theta}=(\mu_{1},\ldots,\mu_{n},\sigma^{2})\) is given by \(\hat{\mu}_{i}=\overline{X}_{i.}=\frac{1}{k}\sum_{j=1}^{k}X_{ij}\), \(1\leq i\leq n\), and \(\hat{\sigma}^{2}=\frac{1}{nk}\sum_{i,j}(X_{ij}-\overline{X}_{i.})^{2}\).

2. Show that \(\hat{\sigma}^{2}\) is not consistent. [Hint: \(nk\hat{\sigma}^{2}/\sigma^{2}\) has the chi-square distribution with \(n(k-1)\) degrees of freedom.]
3. Show that the _bias-corrected MLE_\(\tilde{\sigma}^{2}=nk\hat{\sigma}^{2}/n(k-1)\) is a consistent estimator of \(\sigma^{2}\).

**Ex. 7.13**.: Suppose the hypothesis of Theorem 7.5 holds. Let \(\boldsymbol{\gamma}:\Theta\to\mathbb{R}^{s}\) be a continuously differentiable function in a neighborhood of \(\boldsymbol{\theta}_{0}\) contained in \(\Theta\). Find the asymptotic distribution of the MLE of \(\boldsymbol{\gamma}(\boldsymbol{\theta}_{0})\) under \(P_{\theta_{0}}\).

**Ex. 7.14**.: Consider the multivariate Normal \(N(\boldsymbol{\mu},\Sigma)\) distribution of Example 7.5(b).

1. Prove that \(\{\hat{\sigma}_{ii^{\prime}}:1\leq i\leq i^{\prime}\leq k\}\) and \(\{\hat{\mu}_{i}:1\leq i\leq k\}\) defined by (7.92) are independent.
2. Prove that the joint distribution of \(\{n\hat{\sigma}_{ii^{\prime}}:1\leq i\leq i^{\prime}\leq k\}\) based on \(n\) independent observations from \(N(\boldsymbol{\mu},\Sigma)\) is the same as that of \(\{(n-1)\sigma_{ii^{\prime}}\equiv\sum_{j=1}^{n-1}X_{ji}X_{ji^{\prime}}:1\leq i \leq i^{\prime}\leq k\}\) based on \((n-1)\) observations from \(N(\boldsymbol{0},\Sigma)\). [Hint: Proceed as in the case of \(N(\mu,\sigma^{2})\) considered in Proposition 2.1, Chap. 2, Part I.]

### Exercises for Sect. 7.4

**Ex. 7.15**.: Let \(X_{j}=(X_{j1},\ldots,X_{jk})\), \(j\geq 1\), be i.i.d. with \(P(X_{j}=e_{i})=p_{i}\), \(0<p_{i}<1\) for \(1\leq i\leq k\), \(\sum_{i=1}^{k}p_{i}=1\), where \(e_{i}\)'s (\(i=1,\ldots,k\)) are the standard unit vectors in \(\mathbb{R}^{k}\) as defined in Example 7.5(c).

1. Show that \(\sqrt{n}(\hat{\mathbf{p}}-\mathbf{p})\,\stackrel{{\mathscr{L}}}{{ \longrightarrow}}\,N(0,\Sigma)\), where \(\hat{p}_{i}=\frac{1}{n}\sum_{j=1}^{n}X_{ji}\), \(\hat{\mathbf{p}}=(\hat{p}_{1},\ldots,\hat{p}_{k})^{\prime}\), \(\mathbf{p}=(p_{1},\ldots,p_{k})^{\prime}\), and \(\sigma_{ii}=p_{i}(1-p_{i})\), \(\sigma_{ii^{\prime}}=-p_{i}p_{i^{\prime}}\) (\(i\neq i^{\prime}\)).
2. Restricting the result (a) to the first \(k-1\) coordinates of \(\hat{\mathbf{p}}\), derive (7.98).

**Ex. 7.16**.: Suppose \(\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\) converges in distribution, under \(P_{\theta_{0}}\). Check that \(\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\) is bounded in probability.

**Ex. 7.17**.: Assume \(\Theta\) in Example 7.5, defined by (7.76) is a nonempty open subset of \(R^{p}\).

1. Show that \(C(\boldsymbol{\theta})\), \(1/C(\boldsymbol{\theta})\) and \(f(\mathbf{x},\boldsymbol{\theta})\) are analytic functions of \(\boldsymbol{\theta}\) in \(\Theta\). [Hint: Fix \(\boldsymbol{\theta}_{0}\in\Theta\) and let \(\delta>0\) be such that \(B(\boldsymbol{\theta}_{0}:\delta)\equiv\{\boldsymbol{\theta}:|\boldsymbol{ \theta}-\boldsymbol{\theta}_{0}|<\delta\}\subset\Theta\). Express \(1/C(\boldsymbol{\theta})\) as \((1/C(\boldsymbol{\theta}_{0}))\)-times the m.g.f. of \(\mathbf{t}(\mathbf{X})=(t_{1}(\mathbf{X}),\ldots,t_{p}(X))\) in a neighborhood of the origin \(\mathbf{0}\in\mathbb{R}^{p}\), under the distribution of \(X\) given by \(f(x;\boldsymbol{\theta}_{0})\mu(dx)\).]
2. Check that the hypothesis of Theorem 7.5 is satisfied by the family (7.76).

### Exercises for 7.5

**Ex. 7.18**.: Let \(X_{1},\ldots,X_{n}\) be i.i.d. observations from the gamma distribution \(\mathscr{G}(\mu,\alpha)\), \(\mu>0\), \(\alpha>0\).

1. Suppose \(\mu\) is known. Show that both \(T_{1}=\overline{X}/\mu\) and \(T_{2}=(\sum X_{i}^{2}/n-\overline{X}^{2})/(1-\frac{1}{n})\mu^{2}\) are unbiased estimates of \(\alpha\), and find the asymptotic relative efficiency \(e_{T_{1},T_{2}}\).

2. Suppose \(\alpha\) is known. Show that both \(T_{3}=\overline{X}/\alpha\) and \(T_{4}=[\sum X_{i}^{2}/(n\alpha(\alpha+1))]^{\frac{1}{2}}\) are consistent estimators of \(\mu\). Find the ARE \(e_{T_{3},T_{4}}\).

#### Exercises for 7.6

**Ex. 7.19**. Directly check the conclusion of the Bernstein-von Mises Theorem for the following examples: (i) Example 7.8, (ii) \(X_{1},\ldots,X_{n}\) i.i.d. Poisson \(\mathscr{P}(\theta)\) with \(\theta\in(0,\infty)\) and prior given by Gamma \(\mathscr{G}(\alpha,\beta)\), (iii) Example 3.6, Chap. 3, Part I, where \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\theta,\sigma^{2})\), \(\sigma^{2}>0\) known, \(\theta\in\mathbb{R}\), and the prior is \(N(0,\beta^{2})\).

**Ex. 7.20**. Show that in Examples (i)-(iii) in the preceding Exercise (7.19) the Bayes estimator under squared error, or absolute error, loss is asymptotically efficient.

#### Exercises for 7.7, 7.8

**Ex. 7.21**. Let \(X_{1},\ldots,X_{n}\), be independent observations with the common distribution \(Q\) on \(\mathbb{R}\), having a finite sixth moment. Define \(g(\theta)=E(X_{1}-\theta)^{4}\) and \(g_{n}(\theta)=n^{-1}\sum_{j=1}^{n}(X_{j}-\theta)^{4}\), \(\theta\in\mathbb{R}\).

1. Prove that \(g(\theta)\) has a unique minimizer \(\theta_{0}\), and \(g_{n}(\theta)\) has a unique minimizer \(\hat{\theta}\). [Hint: \(g\), \(g_{n}\) are strictly convex.]
2. First assume \(\hat{\theta}\to\theta_{0}\) a.s. as \(n\to\infty\), and obtain a Taylor expansion of \(\theta\equiv g_{n}^{\prime}(\hat{\theta})\) around \(\theta_{0}\) to prove that \(\sqrt{n}(\hat{\theta}-\theta_{0})\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N(0,\gamma^{2})\), where \(\gamma^{2}=E(X_{1}-\theta_{0})^{6}/\{(6(E(X_{1}-\theta_{0})^{2})^{2}\}\).
3. Prove \(\hat{\theta}\to\theta_{0}\) a.s. [Hint: On a compact interval \(g_{n}(\theta)\to g(\theta)\) uniformly, a.s.]

**Ex. 7.22**. Prove (7.153), (7.154).

## References

* Bahadur (1958) Bahadur, R.R. (1958). Examples of inconsistency of maximum likelihood estimates. _Sankhya, 20_, 207-210.
* Bhattacharyya (1946) Bhattacharyya, A. (1946). On some analogues of the amount of information and their use in statistical estimation. _Sankhya, 8_, 1-14.
* Bickel and Doksum (2001) Bickel, P. J., & Doksum, K. (2001). _Mathematical statistics_ (2nd ed.). Englewood Cliffs, NJ: Prentice Hall.
* Cramer (1946) Cramer, H. (1946). _Mathematical methods of statistics_. Princeton: Princeton University Press.
* Dasgupta (2008) Dasgupta, A. (2008). _Asymptotic theory of statistics and probability_. New York: Springer.
* Ferguson (1996) Ferguson, T. S. (1996). _A course in large sample theory_. London: Taylor and Francis.
* Huber (1981) Huber, P. J. (1981). _Robust statistics_. New York, Wiley.
* Le Cam (1953) Le Cam, L. (1953). On some asymptotic properties of maximum likelihood estimates and related Bayes estimates. _University of California Publications in Statistics, 1_, 277-320.
* Le Cam (1954)* Lehmann & Casella (1998) Lehmann, E., & Casella, G. (1998). _Theory of point estimation._ New York: Springer.
* Prakasa Rao (1987) Prakasa Rao, B. L. S. (1987). _Asymptotic theory of statistical inference._ New York: Wiley.
* Rao (1945) Rao, C. R. (1945). Information and accuracy attainable in the estimation of statistical parameters. _Bulletin of the Calcutta Mathematical Society, 37_(3), 81-91.
* Rao (1973) Rao, C. R. (1973). _Linear statistical inference and its applications_ (2nd ed.). New York: Wiley.
* Serfling (1980) Serfling, R. (1980). _Approximation theorems of mathematical statistics._ New York: Wiley.
* Stigler et al. (2002) Stigler, S. M., Wong, W. H., & Xu, D. (2002). _R. R. Bahadur's lectures on the theory of estimation_. Institute of Mathematical Statistics. Beachwood.

## Chapter 8 Tests in Parametric and Nonparametric Models

### 8.1 Pitman ARE (Asymptotic Relative Efficiency)

_Example 8.1._ Let \(X_{1},X_{2},\dots\) be i.i.d. real valued with common distribution function \(F(x-\theta)\), where \(F^{\prime}(x)=f(x)\) is symmetric about \(x=0\) and is continuous at \(x=0\). Consider the problem of testing \(H_{0}:\theta=0\) against \(H_{1}:\theta>0\), by the following procedures:

1. **Mean Test:** Assume \[\sigma_{f}^{2}\equiv\int_{-\infty}^{\infty}x^{2}f(x)dx<\infty.\] Reject \(H_{0}\) iff \(\overline{X}>a_{n}\), where \(\overline{X}=\frac{1}{n}\sum_{j=1}^{n}X_{j}\) and (the sequence) \(a_{n}\) is so chosen that the test has (asymptotic) size \(\alpha\) (\(0<\alpha<1\)) : \(P_{0}(\overline{X}>a_{n})\to\alpha\). This may be expressed as \[P_{0}(\overline{X}>a_{n})=P_{0}\left(\sqrt{n}\ \overline{X}>\sqrt{n}\,a_{n} \right)\longrightarrow\alpha,\] (8.1) which implies by the CLT that \[\frac{\sqrt{n}\,a_{n}}{\sigma_{f}}=\Phi_{1-\alpha}+o(1),\] (8.2) \(\Phi(x)\) denoting the distribution function of \(N(0,1)\), and \(\Phi_{\beta}\) its \(\beta\)-th quantile.

2. **Sign Test:** Reject \(H_{0}\) iff \(T_{n}>b_{n}\), when \(T_{n}=\frac{1}{n}\sum_{j=1}^{n}{\bf 1}_{\{X_{j}>0\}}\) and (the sequence) \(b_{n}\) is so chosen that the test has (asymptotic) size \(\alpha:P_{0}(T_{n}>b_{n})\to\alpha\). This yields, by the CLT (and the fact that \({\bf 1}_{\{X_{j}>0\}}\) are i.i.d. Bernoulli which have, under \(H_{0}\), common mean \(\frac{1}{2}\) and common variance \(\frac{1}{2}(1-\frac{1}{2})=\frac{1}{4}\)), \[P_{0}\left(T_{n}-\frac{1}{2}>b_{n}-\frac{1}{2}\right)=P_{0}\left(2\sqrt{n} \left(T_{n}-\frac{1}{2}\right)>2\sqrt{n}\left(b_{n}-\frac{1}{2}\right)\right) \longrightarrow\alpha,\] or, \[2\sqrt{n}\left(b_{n}-\frac{1}{2}\right)=\Phi^{-1}(1-\alpha)+o(1).\] (8.3)

Now if one fixes an alternative \(\theta_{1}>0\), then it is simple to check that the _probability of the type \(\Pi\) error_ of each of the above tests converges to zero, i.e., the _power_ goes to one. (This property is referred to as _consistency_ of a test.) In order to make a comparison among such tests one may choose (a sequence of) alternatives \(\theta_{n}\downarrow\theta_{0}=0\) as \(n\uparrow\infty\). Suppose that \(\theta_{n}\downarrow 0\) are so chosen that the probability of type \(\Pi\) error \(\beta_{n}^{(1)}\) of test (1) converges to a desired level \(\beta\), \(0<\beta<1-\alpha\) (\(\beta<1-\alpha\) means _unbiasedness_ for a test; hence we are requiring _asymptotic unbiasedness_). Thus

\[\beta_{n}^{(1)}\equiv P_{\theta_{n}}\left(\overline{X}\leq a_{n}\right) \longrightarrow\beta. \tag{8.4}\]

This means

\[P_{\theta_{n}}(\overline{X}-\theta_{n}\leq a_{n}-\theta_{n}) =P_{0}(\overline{X}\leq a_{n}-\theta_{n})\] \[=P_{0}\left(\sqrt{n}\ \overline{X}/\sigma_{f}\leq\sqrt{n}(a_{n}- \theta_{n})/\sigma_{f}\right)\longrightarrow\beta, \tag{8.5}\]

or, by the CLT,

\[\sqrt{n}(a_{n}-\theta_{n})/\sigma_{f}=\Phi^{-1}(\beta)+o(1)\]

\[\theta_{n} =a_{n}-\sigma_{f}\Phi^{-1}(\beta)n^{-\frac{1}{2}}+o\left(n^{- \frac{1}{2}}\right)\] \[=\sigma_{f}\left(\Phi^{-1}(1-\alpha)-\Phi^{-1}(\beta)\right)n^{- \frac{1}{2}}+o\left(n^{-\frac{1}{2}}\right). \tag{8.6}\]

A fruitful way of comparing test (2) with test (1) is to find the sample size \(h(n)\) required for test (2) (which has asymptotic size \(\alpha\)--the same as that of test (1)) to have the same limiting power \(1-\beta\) or probability of the type \(\Pi\) error \(\beta\) under the alternative \(\theta_{n}\). One may then define the _Pitman Asymptotic Relative Efficiency of test_ (2) _relative to test_ (1) as

\[\lim_{n\to\infty}\frac{n}{h(n)}=e_{p}(T_{n},\overline{X}) \tag{8.7}\]

provided this limit exists (and is independent of \(\alpha\), \(\beta\) for \(0<\beta<1-\alpha\)). One must then find \(h(n)\) such that

\[\beta_{h(n)}^{(2)}\equiv P_{\theta_{n}}\left(T_{h(n)}\leq b_{h(n)}\right) \longrightarrow\beta. \tag{8.8}\]

Now the distribution of (the finite sequence) \({\bf 1}_{\{X_{j}>0\}}\equiv{\bf 1}_{\{X_{j}-\theta_{n}>-\theta_{n}\}}\) (\(j=1,2,\ldots,\)\(h(n)\)) under \(\theta=\theta_{n}\) is the same as that of \({\bf 1}_{\{X_{j}>-\theta_{n}\}}\) under \(H_{0}:\theta=0\). Hence (by an application of Liapounov's CLT to triangular arrays)\[\beta^{(2)}_{h(n)} = P_{\theta_{n}}\left(\frac{1}{h(n)}\sum_{j=1}^{h(n)}{\bf 1}_{\{X_{j}>0 \}}\leq b_{h(n)}\right)=P_{0}\left(\frac{1}{h(n)}\sum_{j=1}^{h(n)}{\bf 1}_{\{X_{j}>- \theta_{n}\}}\leq b_{h(n)}\right) \tag{8.9}\] \[= P_{0}\left(2\sqrt{h(n)}\left(\frac{1}{h(n)}\sum_{j=1}^{h(n)}\left( {\bf 1}_{\{X_{j}>-\theta_{n}\}}\!-\!F(\theta_{n})\right)\right)\leq 2\sqrt{h(n)}(b_{h(n)} \!-\!F(\theta_{n}))\right)\] \[= \varPhi\left(2\sqrt{h(n)}\left(b_{h(n)}\!-\!F(\theta_{n})\right) \right)\!+\!o(1)\longrightarrow\beta.\]

Here we have made use of the facts

\[P_{0}(X_{j}>-\theta_{n})=P_{0}(X_{j}<\theta_{n})=F(\theta_{n}),\]

\[{\rm var}_{0}{\bf 1}_{\{X_{j}>-\theta_{n}\}}=F(\theta_{n})(1-F(\theta_{n}))= \frac{1}{4}+o(1). \tag{8.10}\]

Thus, by (8.9), (8.3), and (8.6)

\[\mathbf{\Phi}^{-1}(\beta)+o(1) = 2\sqrt{h(n)}\left(b_{h(n)}-F(\theta_{n})\right)=2\sqrt{h(n)} \left(b_{h(n)}-\frac{1}{2}+\frac{1}{2}-F(\theta_{n})\right)\] \[= \mathbf{\Phi}^{-1}(1-\alpha)-2\sqrt{h(n)}\left(F(\theta_ {n})-\frac{1}{2}\right)+o(1)\] \[= \mathbf{\Phi}^{-1}(1-\alpha)-2\sqrt{h(n)}\left(\theta_{n }(f(0)+o(1))\right)+o(1)\] \[= \mathbf{\Phi}^{-1}(1-\alpha)-2\sqrt{\frac{h(n)}{n}}\ \sigma_{f}\left(\mathbf{\Phi}^{-1}(1-\alpha)-\mathbf{\Phi}^{-1}(\beta)\right)f(0)\] \[\qquad+o\left(\sqrt{\frac{h(n)}{n}}\right)+o(1),\]

which yields

\[2\sigma_{f}f(0)\sim\sqrt{\frac{n}{h(n)}}. \tag{8.11}\]

The symbol "\(\backsim\)" indicates that the ratio of the two sides (of \(\sim\)) goes to one.

It follows from (8.11) (and (8.7)) that

\[e_{P}(T_{n},\overline{X})=4\sigma_{f}^{2}f^{2}(0). \tag{8.12}\]

Consider now the following special cases:

\[\begin{array}{|c|c|}\hline F&e_{P}(T_{n},\overline{X})\\ \hline N(0,\sigma^{2})&2/\pi\\ \hline\mbox{Double exponential}&2\\ \hline\mbox{Uniform on }[-\frac{1}{2},\frac{1}{2}]&1/3\\ \hline\end{array}\]

A more realistic version of the mean test is the \(t\)_-test:_

\[{\rm Reject}\ H_{0}\ {\rm iff}\ t\equiv\frac{\overline{X}}{s}>a^{\prime}_{n}, \tag{8.13}\]where \(s=[\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}]^{\frac{1}{2}}.\) Since, under \(H_{0}\), \(\sqrt{n}\ \overline{X}/s\to N(0,1)\) (provided \(\sigma_{f}^{2}<\infty\)), one has

\[\sqrt{n}\ a_{n}^{\prime} =\boldsymbol{\Phi}^{-1}(1-\alpha)+o(1),\] \[a_{n}^{\prime} =n^{-\frac{1}{2}}\left[\boldsymbol{\Phi}^{-1}(1-\alpha)+o(1) \right]. \tag{8.14}\]

Usually, the p.d.f. \(f\) may not be known, so that \(\sigma_{f}^{2}\) is unknown. Proceeding as above one shows that \(e_{P}(T_{n},t)=4\sigma_{f}^{2}f^{2}(0)\), the same as in (8.12). (Exercise 8.1.)

_Remark 8.1._ The reason why one may use the sign test and not the \(t\)-test is that one may not be sure of the underlying distribution. The \(t\)-test is not nearly as "robust" (against model variation) as the sign test. In particular, \(e_{p}(T_{n},\overline{X})=\infty\) if \(\sigma_{f}^{2}=\infty\). Although one may look at the asymptotic relative efficiency _(ARE)_ of two tests in a parametric model also, there do exist under broad assumptions asymptotically optimal tests in parametric models--for example, the likelihood ratio test is asymptotically optimal under appropriate regularity conditions. We shall consider such tests in the next section.

The theorem below allows one to compute Pitman ARE of tests of the type in Example 8.1. Consider two tests \(\delta_{i,n}\ (i=1,2)\) which may be expressed as

\[\delta_{i,n}:\text{Reject }H_{0}\text{ if }T_{i,n}>a_{i,n}\qquad(i=1,2), \tag{8.15}\]

where \(a_{i,n}\) are such that

\[\lim_{n\to\infty}P_{\theta_{0}}\left(T_{i,n}>a_{i,n}\right)=\alpha\qquad(i=1,2), \tag{8.16}\]

for a given \(\alpha\in(0,1)\). Fix \(\beta\in(0,1)\) such that \(\beta<1-\alpha\).

**Theorem 8.1**.: \((A_{1})\) _Assume that there exists functions \(\mu_{i,n}(\theta)\), \(\sigma_{i,n}(\theta)>0\ (i=1,2)\) and \(\delta>0\) such that_

\[\sup_{\theta\in[\theta_{0}-\delta,\theta_{0}+\delta]}\ \sup_{x\in(a,b)} \left|P_{\theta}\left(\frac{T_{i,n}-\mu_{i,n}(\theta)}{\sigma_{i,n}(\theta)} \leq x\right)-G(x)\right|\longrightarrow 0 \tag{8.17}\]

_as \(n\to\infty\), where \(G\) is a continuous distribution function which is strictly increasing on an interval \((a,b)\) with \(G^{-1}(a)=0\), \(G^{-1}(b)=1\ (-\infty\leq a<b\leq\infty)\). \((A_{2})\) Assume that \(\theta\to\mu_{i,n}(\theta)\) is \(k\) times continuously differentiable in a neighborhood of \(\theta_{0}\), where \(k\) is the smallest positive integer such that \(\mu_{i,n}^{(k)}(\theta_{0})\neq 0\ (i=1,2)\). \((A_{3})\) Assume that_

\[\lim_{n\to\infty}\frac{\sigma_{i,n}(\theta_{n}^{\prime})}{\sigma_{i,n}(\theta_ {0})}=1,\quad\lim_{n\to\infty}\frac{\mu_{i,n}^{(k)}(\theta_{n}^{\prime})}{\mu_ {i,n}^{(k)}(\theta_{0})}=1\qquad(i=1,2) \tag{8.18}\]

_for every sequence \(\{\theta_{n}^{\prime}\}\) converging to \(\theta_{0}\). Finally assume \((A_{4})\) that there exist positive constants \(c_{i}\ (i=1,2)\), \(\gamma\) such that_

\[\lim_{n\to\infty}n^{-\gamma}\ \frac{\mu_{i,n}^{(k)}(\theta_{0})}{\sigma_{i,n}( \theta_{0})}=c_{i},\qquad(i=1,2). \tag{8.19}\]_Under assumptions \((A_{1})\)-\((A_{4})\), for the test \(H_{0}:\theta=\theta_{0}\), \(H_{1}:\theta>\theta_{0}\), one has_

\[e_{P}(\delta_{2,n},\delta_{1,n})=\left(\frac{c_{2}}{c_{1}}\right)^{\frac{1}{ \gamma}}. \tag{8.20}\]

Proof.: Let

\[\theta_{n} =\theta_{0}+\left(\frac{k!}{c_{1}}\right)^{\frac{1}{k}}\big{(}G^{ -1}(1-\alpha)-G^{-1}(\beta)\big{)}^{\frac{1}{k}}\,n^{-\frac{\gamma}{k}}(1+o(1)),\] \[h(n) =\left[n\left(\frac{c_{1}}{c_{2}}\right)^{\frac{1}{\gamma}}\right] +1\qquad([x]:=\text{integer part of $x$}). \tag{8.21}\]

By assumptions \((A_{2})\), \((A_{3})\), \((A_{4})\), there exists \(\theta_{n,i}^{*}\) lying between \(\theta_{n}\) and \(\theta_{0}\) such that

\[\mu_{i,n}(\theta_{n})-\mu_{i,n}(\theta_{0}) =\frac{(\theta_{n}-\theta_{0})^{k}}{k!}\,\mu_{i,n}^{(k)}(\theta_{ n,i}^{*})\] \[=\frac{1}{c_{1}}\left\{G^{-1}(1-\alpha)-G^{-1}(\beta)\right\}n^{ -\gamma}\mu_{i,n}^{(k)}(\theta_{0})(1+o(1))\] \[=\frac{c_{i}}{c_{1}}\left\{G^{-1}(1-\alpha)-G^{-1}(\beta)\right\} \sigma_{i,n}(\theta_{0})(1+o(1))\;\;(i=1,2). \tag{8.22}\]

Now (8.16) may be expressed as

\[P_{\theta_{0}}\left(\frac{T_{i,n}-\mu_{i,n}(\theta_{0})}{\sigma_{i,n}(\theta_{ 0})}>\frac{a_{i,n}-\mu_{i,n}(\theta_{0})}{\sigma_{i,n}(\theta_{0})}\right) \longrightarrow\alpha\qquad(i=1,2). \tag{8.23}\]

Therefore, by \((A_{1})\),

\[\lim_{n\to\infty}\frac{a_{i,n}-\mu_{i,n}(\theta_{0})}{\sigma_{i,n}(\theta_{0}) }==G^{-1}(1-\alpha)\qquad(i=1,2). \tag{8.24}\]

Now

\[P_{\theta_{0}}\left(T_{1,n}\leq a_{1,n}\right)=P_{\theta_{0}}\left(\frac{T_{1,n}-\mu_{1,n}(\theta_{n})}{\sigma_{1,n}(\theta_{n})}\leq\frac{a_{1,n}-\mu_{1, n}(\theta_{n})}{\sigma_{1,n}(\theta_{n})}\right), \tag{8.25}\]

and, by \((A_{3})\) and (8.22), (8.24),

\[\frac{a_{1,n}-\mu_{1,n}(\theta_{n})}{\sigma_{1,n}(\theta_{n})}\] \[=\frac{a_{1,n}-\mu_{1,n}(\theta_{0})+\mu_{1,n}(\theta_{0})-\mu_{1,n}(\theta_{n})}{\sigma_{1,n}(\theta_{0})}\,(1+o(1))\] \[=\left[G^{-1}(1-\alpha)-\left\{G^{-1}(1-\alpha)-G^{-1}(\beta) \right\}\right](1+o(1))\longrightarrow G^{-1}(\beta). \tag{8.26}\]

Similarly,

\[P_{\theta_{n}}\left(T_{2,h(n)}\leq a_{2,h(n)}\right)=P_{\theta_{n}}\left( \frac{T_{2,h(n)}-\mu_{2,h(n)}(\theta_{n})}{\sigma_{2,h(n)}(\theta_{n})}\leq \frac{a_{2,h(n)}-\mu_{2,h(n)}(\theta_{n})}{\sigma_{2,h(n)}(\theta_{n})}\right)\]and \(\mu_{2,h(n)}(\theta_{0})-\mu_{2,h(n)}(\theta_{n})=\{-(\theta_{n}-\theta_{0})^{k}/k! \}[\mu_{2,h(n)}^{(k)}(\theta_{0})+o(1)],\) so that by (8.19), (8.21), one has

\[\frac{a_{2,h(n)}\!-\!\mu_{2,h(n)}(\theta_{n})}{\sigma_{2,h(n)}( \theta_{n})}= \bigg{\{}\frac{a_{2,h(n)}\!-\!\mu_{2,h(n)}(\theta_{0})}{\sigma_{2, h(n)}(\theta_{0})}\!+\!\frac{\mu_{2,h(n)}(\theta_{0})\!-\!\mu_{2,h(n)}(\theta_{n})}{ \sigma_{2,h(n)}(\theta_{0})}\bigg{\}}\,(1\!+\!o(1))\] \[= \bigg{[}G^{-1}(1-\alpha)-\frac{1}{c_{1}}\left\{G^{-1}(1-\alpha)- G^{-1}(\beta)\right\}\frac{(1/n)^{\gamma}}{\sigma_{2,h(n)}(\theta_{0})}\] \[\qquad\qquad\cdot\mu_{2,h(n)}^{(k)}(\theta_{0})\bigg{]}(1+o(1))\] \[\longrightarrow G^{-1}(1-\alpha)-\left\{G^{-1}(1-\alpha)-G^{-1}( \beta)\right\}=G^{-1}(\beta).\ \square\]

### 8.2 CLT for \(U\)-Statistics and Some Two-Sample Rank Tests

Let \(X_{1},X_{2},\ldots,X_{m}\) be a random sample drawn from a population with p.d.f. \(f\left(\frac{x}{\sigma}\right)\), and let \(Y_{1},Y_{2},\ldots,Y_{n}\) be a random sample (independent of \(X_{j}\)'s) from another population with p.d.f. \(f\left(\frac{x-\theta}{\sigma}\right)\). We will suppress \(\sigma\). Consider

\[H_{0}:\theta=0,\qquad H_{1}:\theta>0. \tag{8.27}\]

Here \(\sigma\) is an unknown scale parameter.

The _Wilcoxon_ (or _Mann-Whitney_) test rejects \(H_{0}\) iff

\[\sum_{j=1}^{n}R_{j}>c_{m,n}, \tag{8.28}\]

where \(R_{j}\) is the rank of \(Y_{j}\) among the \(m+n\) observations \(X_{1},X_{2},\ldots,X_{m}\), \(Y_{1},Y_{2},\ldots,Y_{n}\). Assume that \(m\to\infty\), \(n\to\infty\) in such a way that

\[\frac{m}{m+n}\longrightarrow\lambda\quad\text{for some }\lambda\in(0,1). \tag{8.29}\]

One chooses \(c_{m,n}\) in such a way that (\(P_{\theta}\) denotes probability under \(f(\frac{x-\theta}{\sigma})\)),

\[\lim P_{0}\left(\sum_{i=1}^{n}R_{j}>c_{m,n}\right)=\alpha, \tag{8.30}\]

where the limit is through sequences of values of \(m,n\) satisfying (8.29), and \(\alpha\in(0,1)\). Now, under \(H_{0}\), the distribution of \((R_{1},R_{2},\ldots,R_{n})\) is

\[P_{0}\left(R_{1}=r_{1},R_{2}=r_{2},\ldots,R_{n}=r_{n}\right)=\frac{m!}{(m+n)!} \tag{8.31}\]

for every \(m\)-tuple of distinct integers \((r_{1},r_{2},\ldots,r_{n})\) from \(\{1,2,\ldots,m+n\}\). Hence the null distribution of the Wilcoxon statistic \(\sum_{1}^{n}R_{j}\) does not depend on the underlying \(f\).

The _Fisher-Yates test_ rejects \(H_{0}\) iff

\[\sum_{i=1}^{n}Z_{(R_{i})}>d_{m,n} \tag{8.32}\]

where \(Z_{(r)}\) is the expected value of the \(r^{th}\)_order statistic_ of a sample of size \(m+n\) from a standard normal distribution. This test is also called the _normal scores test_. Once again, since the distribution of the statistic on the left (8.32) depends only the distribution of \((R_{1},\ldots,R_{m})\), the null distribution of the normal scores statistic is independent of the underlying p.d.f. \(f\), and \(d_{m,n}\) depends only on \(m\), \(n\) and \(\alpha\).

In order to compute the ARE's of the above tests (with respect to each other, or with respect to their parametric competitors) we will use the following central limit theorem for \(U\)-statistics.

**Theorem 8.2**.: _Let \(\varphi(x,y)\) be a real-valued measurable function of \((x,y)\). Assume_

\[E\varphi(X_{i},Y_{j}) =0,\ \ \ \ 0<\sigma_{1}^{2}:=Eg^{2}(X_{1})<\infty,\] \[0 <\sigma_{2}^{2}:=Eh^{2}(Y_{1})<\infty, \tag{8.33}\]

_where \(g(x):=E\varphi(x,Y_{1})\), \(h(y):=E\varphi(X_{1},y)\). Then under the assumption (8.29),_

\[\frac{\sum_{i=1}^{m}\sum_{j=1}^{n}\varphi(X_{i},Y_{j})}{\sqrt{\operatorname{ var}\sum_{i=1}^{m}\sum_{j=1}^{n}\varphi(X_{i},Y_{j})}}\ \xrightarrow{\mathscr{L}}\ N(0,1). \tag{8.34}\]

Proof.: Write \(U=\sum\sum\varphi(X_{i},Y_{j})\). Then, writing \(\sigma_{0}^{2}=E\varphi^{2}(X_{1},Y_{1})=\operatorname{var}\varphi(X_{1},Y_{1})\),

\[EU =mnE\varphi(X_{1},Y_{1})=0,\] \[\operatorname{var}U =mn\operatorname{var}\varphi(X_{1},Y_{1})+mn(n-1)\sigma_{1}^{2}+ mn(m-1)\sigma_{2}^{2}\] \[=mn\sigma_{0}^{2}+mn(n-1)\sigma_{1}^{2}+mn(m-1)\sigma_{2}^{2}, \tag{8.35}\]

since (taking conditional expectation given \(X_{1}\) first)

\[\operatorname{cov}(\varphi(X_{1},Y_{1}),\varphi(X_{1},Y_{2}))=Eg^{2}(X_{1})= \sigma_{1}^{2},\]

and (taking conditional expectation given \(Y_{1}\) first)

\[\operatorname{cov}(\varphi(X_{1},Y_{1}),\varphi(X_{2},Y_{1}))=Eh^{2}(Y_{1})= \sigma_{2}^{2}.\]

In the computation of variance \((U)\) in (8.35) we have also used the fact that \(\operatorname{cov}(\varphi(X_{i},Y_{j})\), \(\varphi(X_{i^{\prime}},Y_{j^{\prime}}))=0\) if \(i\neq i^{\prime}\)_and_\(j\neq j^{\prime}\).

Now consider the following approximation of \(U\):

\[S :=\sum_{i=1}^{m}E(U|X_{i})+\sum_{j=1}^{n}E(U|Y_{j})\] \[=\,n\sum_{i=1}^{m}g(X_{i})+m\sum_{j=1}^{n}h(Y_{j}). \tag{8.36}\]Note that \(S\) is the projection of \(U\) onto the subspace (of \(L^{2}\)-functions on the probability space) \((\oplus\mathscr{L}_{i})\oplus(\oplus\mathscr{L}_{j}^{\prime})\), where \(\mathscr{L}_{i}\) is the space of (\(L^{2}\)-) functions of \(X_{i}\), \(\mathscr{L}_{j}^{\prime}\) is the space of (\(\mathscr{L}^{2}\)-) functions of \(Y_{j}\). _In any case_

\[\operatorname{var}S=mn^{2}\sigma_{1}^{2}+m^{2}n\sigma_{2}^{2}, \tag{8.37}\]

\[\operatorname{cov}(S,U) = n\sum_{i=1}^{m}\operatorname{cov}(g(X_{i}),U)+m\sum_{j=1}^{n} \operatorname{cov}(h(Y_{j}),U) \tag{8.38}\] \[= n\sum_{i=1}^{m}n\operatorname{cov}(g(X_{i}),\varphi(X_{i},Y_{1} ))+m\sum_{j=1}^{n}m\operatorname{cov}(h(Y_{j}),\varphi(X_{1},Y_{j}))\] \[= mn^{2}\sigma_{1}^{2}+m^{2}n\sigma_{2}^{2}=\operatorname{var}S,\] \[\operatorname{cov}(S,U-S) = \operatorname{cov}(S,U)-\operatorname{var}S=0.\]

Also,

\[\operatorname{var}(U-S)=\operatorname{var}(U)-\operatorname{var}S=mn(\sigma_ {0}^{2}-\sigma_{1}^{2}-\sigma_{2}^{2}). \tag{8.39}\]

Therefore,

\[\operatorname{var}\left(\frac{U-S}{\sqrt{\operatorname{var}U}}\right) \longrightarrow 0,\qquad\frac{\operatorname{var}U}{\operatorname{var}S} \longrightarrow 1. \tag{8.40}\]

Hence it is enough to prove that

\[\frac{S}{\sqrt{\operatorname{var}S}}\ \stackrel{{\mathscr{L}}}{{ \longrightarrow}}\ N(0,1). \tag{8.41}\]

But (8.41) is an immediate consequence of the classical CLT applied separately to the two components of (8.36), using their independence and (6.29). 

_Remark 8.2_.: Suppose, under \(P_{\theta}\), \(\varphi(X_{i},Y_{j})\) is of the form

\[\varphi(X_{i},Y_{j})=\psi(X_{i},Y_{j})-E_{\theta}\psi(X_{1},Y_{1}),\]

Also, write \(\sigma_{0}^{2}(\theta)\), \(\sigma_{1}^{2}(\theta)\), \(\sigma_{2}^{2}(\theta)\) to indicate the dependence of the variances on \(\theta\). Write

\[\ell_{m,n}(\theta):=\frac{n^{3}mE_{\theta}|g(X_{1})|^{3}+nm^{3}E_{\theta}|h(Y_ {1})|^{3}}{(n^{2}m\sigma_{1}^{2}(\theta)+nm^{2}\sigma_{2}^{2}(\theta))^{\frac {3}{2}}}\,. \tag{8.42}\]

By the _Berry-Esseen Theorem1_

Footnote 1: See Bhattacharya and Rao Ranga (2010), pp. 104, 186.

\[\sup_{x}\left|P_{\theta}\left(\frac{S}{\sqrt{mn^{2}\sigma_{1}^{2}(\theta)+m^{ 2}n\sigma_{2}^{2}(\theta)}}\leq x\right)-\boldsymbol{\Phi}(x)\right|\leq c \ell_{m,n}(\theta) \tag{8.43}\]

where \(\boldsymbol{\Phi}\) is the standard normal distribution function, and \(c\) is an absolute constant (\(c=1\) will do). Also,

\[\Delta_{m,n}(\theta) := E_{\theta}\left(\frac{U}{\sqrt{\operatorname{var}_{\theta}(U)}} -\frac{S}{\sqrt{\operatorname{var}_{\theta}(S)}}\right)^{2} \tag{8.44}\] \[= \frac{E_{\theta}(U-S)^{2}}{\operatorname{var}_{\theta}(U)}+ \operatorname{var}_{\theta}(S)\left(\frac{1}{\operatorname{var}_{\theta}(S)}- \frac{1}{\operatorname{var}_{\theta}(U)}\right)^{2}.\]Now assume

\[\text{(B)}:\left\{\begin{array}{ll}\text{(i)}&E_{\theta}|\varphi(X_{1},Y_{1})|^{3 }\text{ is bounded away from infinity in a neighborhood of }\theta_{0},\\ \text{(ii)}&\sigma_{1}^{2}(\theta),\,\sigma_{2}^{2}(\theta)\text{ are bounded away from zero in a neighborhood of }\theta_{0}.\end{array}\right.\]

Then both \(\ell_{m,n}(\theta)\) and \(\Delta_{m,n}(\theta)\) (see (8.42), (8.44)) go to zero uniformly for \(\theta\) in a neighborhood \([\theta_{0}-\delta,\theta_{0}+\delta],\) as \(m,n\to\infty\) (still assuming (8.29)). It may be shown from this that (Exercise 8.2)

\[\sup_{\theta\in[\theta_{0}-\delta,\theta_{0}+\delta]}\,\,\sup_{x\in\mathbb{R} ^{1}}\left|P_{\theta}\left(\frac{\sum_{i,j}\psi(X_{i},Y_{j})-mnE_{\theta}\psi (X_{1},Y_{1})}{\sqrt{\operatorname{var}_{\theta}U}}\leq x\right)-\boldsymbol{ \Phi}(x)\right|\,\,\to 0, \tag{8.45}\]

_Example 8.2_.: Let us now compute the ARE of the Wilcoxon test (8.28) with respect to the two-sample \(t\)-test. First note that, writing \(Y_{(1)}<Y_{(2)}<\dots<Y_{(n)}\) as the _order statistics_ of \(Y_{j}\)'s,

\[\sum_{i=1}^{m}\sum_{j=1}^{n}\mathbf{1}_{\{X_{i}<Y_{j}\}}=\sum_{j=1}^{n}\left( \#\{i:X_{i}<Y_{j}\}\right)=\sum_{j=1}^{n}\left(\#\{i:X_{i}<Y_{(j)}\}\right),\]

since each \(Y_{j}\) is a unique \(Y_{(k)}\). But \(\#\{i:X_{i}<Y_{(j)}\}\) equals the rank of \(Y_{(j)}\) among \(\{X_{1},\dots,X_{m},Y_{1},\dots,Y_{n}\}\) minus \(j\) (since there are \(j\)\(Y_{k}\)'s \(\leq Y_{(j)}\)). Hence

\[\sum_{i=1}^{m}\sum_{j=1}^{n}\mathbf{1}_{\{X_{i}<Y_{j}\}}=\sum_{j=1}^{n}R_{(j)} -\sum_{j=1}^{n}j=\sum_{j=1}^{n}R_{j}-\frac{n(n+1)}{2}\,. \tag{8.46}\]

Here \(R_{(j)}\) is the \(j\)-th order statistic among \(R_{1},\dots,R_{n}\) (which is the same as the rank of \(Y_{(j)}\) among \(\{X_{1},\dots,X_{m},Y_{1},\dots,Y_{n}\}\)). Hence the Wilcoxon test (8.28) may be expressed as

\[\text{Reject }H_{0}\text{ iff }\sum_{i=1}^{m}\sum_{j=1}^{n}\mathbf{1}_{\{X_{i}<Y _{j}\}}>a_{m,n} \tag{8.47}\]

where \(a_{m,n}\) is so chosen that

\[\lim_{m,n\to\infty}P_{0}\left(\sum_{i=1}^{m}\sum_{j=1}^{n}\mathbf{1}_{\{X_{i}< Y_{j}\}}>a_{m,n}\right)=\alpha. \tag{8.48}\]

Now write \(\psi(x,y)=\mathbf{1}_{\{x<y\}}\), \(\varphi(x,y)=\psi(x,y)-E_{\theta}(\mathbf{1}_{\{X_{i}<Y_{j}\}})\), to get, from Theorem 8.2,

\[\lim_{m,n\to\infty}P_{0}\left(\frac{U}{\sqrt{\frac{mn}{12}(m+n+1)}}>\frac{a_{m,n}-\frac{1}{2}\,mn}{\sqrt{\frac{mn}{12}(m+n+1)}}\right)=\alpha. \tag{8.49}\]

Note that, \(\forall\)\(\theta>0\),

\[p_{\theta} := E_{\theta}\left(\mathbf{1}_{\{X_{i}<Y_{j}\}}\right)=P_{\theta}( X_{i}<Y_{j})=\int_{-\infty}^{\infty}F(y)f(y-\theta)dy\] \[= \int_{-\infty}^{\infty}F(z+\theta)f(z)dz\geq\int_{-\infty}^{ \infty}F(z)f(z)dz\]\[\sigma_{m,n}(0)=\sqrt{{\rm var}_{0}(U)}=\sqrt{\frac{mn}{12}\left(m+n+1 \right)}\,, \tag{8.54}\]so that \(k=1\) (in the application of Theorem 8.1) and

\[n^{-\frac{1}{2}}\frac{\mu^{(1)}_{2,m,n}(0)}{\sigma_{m,n}(0)}=n^{- \frac{1}{2}}\frac{(n^{2}\sqrt{\lambda}\ )(1+o(1))\int_{-\infty}^{\infty}f^{2}(z)dz}{n^{ \frac{3}{2}}\left(\frac{1}{12}\right)^{\frac{1}{2}}(1+o(1))}\] \[\longrightarrow\sqrt{\lambda}\sqrt{12}\int_{-\infty}^{\infty}f^{2 }(z)dz=c_{2}. \tag{8.55}\]

Thus \(\gamma=\frac{1}{2}.\) (See (8.18).)

On the other hand for the Student's \(t\),

\[t=\frac{\overline{Y}-\overline{X}}{s\sqrt{\frac{1}{m}+\frac{1}{n}}}\qquad\left[ s^{2}:=\frac{\sum_{i=1}^{m}(X_{i}-\overline{X})^{2}+\sum_{j=1}^{n}(Y_{j}- \overline{Y})^{2}}{m+n-2}\right], \tag{8.56}\]

the test of asymptotic size \(\alpha\) is

\[\text{Reject }H_{0}\text{ iff }\overline{Y}-\overline{X}>z_{\alpha}s\sqrt{ \frac{1}{m}+\frac{1}{n}}\,=b_{m,n}. \tag{8.57}\]

Writing \(\sigma_{f}^{2}=\text{var}_{\theta}X=\text{var}_{\theta}Y=\int(x-\mu_{x})^{2}f (x)dx\) (\(\mu_{x}=\int xf(x)d\mu\)), one has

\[\mu_{1,m,n}(\theta) =E_{\theta}Y_{1}-EX_{1}=\int_{-\infty}^{\infty}xf(x-\theta)dx-\int _{-\infty}^{\infty}xf(x)dx\] \[=\int_{-\infty}^{\infty}(y+\theta)f(y)dy-\int_{-\infty}^{\infty}xf (x)dx=\theta,\] \[\sigma_{m,n}(\theta) =\sqrt{\text{var}(\overline{Y})+\text{var}(\overline{X})}=\sqrt{ \sigma_{f}^{2}/n+\sigma_{f}^{2}/m}\] \[=\sigma_{f}\sqrt{1+\frac{1-\lambda}{\lambda}}\ n^{-\frac{1}{2}}(1 +o(1))=\left(\frac{\sigma_{f}}{\sqrt{\lambda}}\right)n^{-\frac{1}{2}}(1+o(1)). \tag{8.58}\]

Thus

\[\mu^{(1)}_{1,m,n}(\theta) =\frac{d}{d\theta}\,\theta=1, \tag{8.59}\] \[\sigma_{m,n}(0) =n^{-\frac{1}{2}}\frac{\sigma_{f}}{\sqrt{\lambda}}\,(1+o(1)). \tag{8.60}\]

Thus, as before, \(\gamma=\frac{1}{2}\), \(k=1\), and

\[c_{1} =\lim_{n\to\infty}n^{-\frac{1}{2}}\,\frac{\mu^{(1)}_{1,m,n}(0)}{ \sigma_{m,n}(0)}=\frac{1}{\sigma_{f}\sqrt{\frac{1}{\lambda}}}\] \[=\frac{\sqrt{\lambda}}{\sigma_{f}}\,. \tag{8.61}\]

From (8.55) and (8.61), we get

\[e_{P}(W,t)\equiv e_{P}(\text{Wilcoxon},t)=\left(\frac{c_{2}}{c_{1}}\right)^{ \frac{1}{1/2}}=\left(\frac{c_{2}}{c_{1}}\right)^{2}\]\[=12\sigma_{f}^{2}\left(\int_{-\infty}^{\infty}f^{2}(x)dx\right)^{2} \tag{8.62}\] \[=\left\{\begin{array}{ll}\frac{3}{\pi}\approx 0.95&\mbox{if $f$ is normal density,}\\ \frac{3}{2}&\mbox{if $f$ is double exponential,}\\ 1&\mbox{if $f$ is uniform on $[-1,1].$}\end{array}\right.\]

It has been shown by Hodges and Lehmann (1956) that \(e_{P}(W,t)\geq 108/125=0.864\), whatever be \(f\). On the other hand, \(e_{P}(W,t)=\infty\) if \(\sigma_{f}^{2}=\infty\). Even in the class of \(f\) with \(\sigma_{f}^{2}<\infty\), the supremum of \(e_{P}(W,t)\) is infinity. Thus the \(t\)-test can be very bad for certain \(f\)'s.

Similarly, one may compute the ARE of the Fisher-Yates test NS with respect to the two-sample \(t\)-test. Indeed,

\[e_{P}(NS,t)=\sigma_{f}^{2}\left(\int_{-\infty}^{\infty}\frac{f^{2}(x)}{\varphi \{\Phi^{-1}[F(x)]\}}\,dx\right)^{2}.\]

One may show that \(e_{P}(NS,t)>1\) for all \(f\neq\varphi\), and equals \(1\) when \(f=\varphi\). It also follows that

\[e_{P}(W,NS)=\frac{3}{\pi}\quad\mbox{if $f$ is normal.}\]

See Hodges and Lehmann (1960).

_Remark 8.3_.: The two-sample rank tests are often used to test \(H_{0}:F(x)=G(x)\;\forall\;x\) against \(H_{1}:F(x)\geq G(x)\;\forall\;x\) with strict inequality for some \(x\in\mathbb{R}^{1}\). Here \(F,G\) are the (common) distribution functions of the \(X\)'s and the \(Y\)'s, respectively, and it is assumed that \(F,G\) are continuous. (Note that \(H_{1}\) says that \(Y\)'s are _stochastically larger_ than \(X\)'s: \(P(Y>x)\geq P(X>x)\;\forall\;x\), with strict inequality for some \(x\).) Consider now the group \(\mathscr{G}\) (under composition of maps) of all continuous homeomorphisms of \(\mathbb{R}^{1}\). An element \(\varphi\) of \(\mathscr{G}\) transforms an observation vector \((x_{1},\ldots,x_{m},y_{1},\ldots,y_{n})\in\mathbb{R}^{m+n}\) into the vector \((\varphi(x_{1}),\ldots,\varphi(x_{m}),\varphi(y_{1}),\ldots,\varphi(y_{n}))\). Thus \(\varphi\) induces a transformation \(\tilde{\varphi}\) on \(\mathbb{R}^{m+n}\) (onto \(\mathbb{R}^{m+n}\equiv\) observation space \(\mathscr{X}\)). Let \(\tilde{\mathscr{G}}\) denote the group of these transformations on \(\mathbb{R}^{m+n}\). Because \(\tilde{\varphi}\) does not change the orders (or _ranks_) among the observations, it is reasonable to require that the test based on \(\tilde{\varphi}_{\mathbf{x}}\) rejects \(H_{0}\)_iff_ the same test based on \(\mathbf{x}\equiv(x_{1},\ldots,x_{m},y_{1},\ldots,y_{n})\) does so. For the form of \(H_{0}\) and \(H_{1}\) remain unchanged if \(F,G\) are replaced by the distributions of \(\varphi(X_{1}),\varphi(Y_{1})\), respectively. (Also, \(F,G\) are completely unknown, except for the properties assumed.) The only tests which are _invariant_ under every \(\tilde{\varphi}\) in \(\tilde{\mathscr{G}}\) are the _rank tests,_ i.e., tests based on the ranks of \(\{X_{1},\ldots,X_{m},Y_{1},\ldots,Y_{n}\}\). (See Proposition 5.4.)

The _power_ of a rank test of course depends on specific pairs \((F,G)\). For example, if \(F,G\) have densities \(f(x)\), \(f(x-\theta)\), \(\theta>0\), and \(f(x)\)_is the normal p.d.f._ with mean \(0\), then the most powerful rank test is the Fisher-Yates test. If, on the other hand \(f(x)\)_is the logistic with mean zero,_ then the most powerful rank test is the Wilcoxon test. For a simple derivation of these facts, see T. Ferguson (1967), pp. 250-257.

_Remark 8.4_.: There is a one-sample version of Theorem 8.2, which is even simpler to prove (Exercise 8.4):

**Proposition 8.1** (CLT for One-Sample \(U\)-Statistics): \[\sum_{1\leq i\neq j\leq n}\frac{\varphi(X_{i},X_{j})}{\sqrt{\operatorname{var} \sum_{i\neq j}\varphi(X_{i},X_{j})}}\ \xrightarrow{\mathscr{L}}\ N(0,1)\quad\text{if}\]

_(i)_ \(E\varphi(X_{1},X_{2})=0\)_,_

_(ii)_ \(0<\sigma_{1}^{2}:=Eg^{2}(X_{1})<\infty\)_._

_Example 8.3: One may write_

\[\sum_{1}^{n}(X_{i}-E(X_{i}))=\frac{1}{n-1}\sum_{i\neq j}\left[(X_{i}-E(X_{i})) +(X_{j}-EX_{j})\right], \tag{8.63}\]

\[\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{2n}\sum_{1\leq i\neq j\leq n}( X_{i}-X_{j})^{2}. \tag{8.64}\]

_Remark 8.5: Let \(T=T(\mathbf{X})\) be an unbiased estimator of some parameter \(\theta=ET(\mathbf{X})\), where \(\mathbf{X}=(X_{1},\ldots,X_{n})\) and \(X_{1},X_{2},\ldots,X_{n}\) are i.i.d. The \(U\)-statistic \(\varphi(\mathbf{X})=\frac{1}{n!}\sum\varphi(X_{i_{1}},\)\(X_{i_{2}},\ldots,X_{i_{n}})\) is also an unbiased estimator of \(\theta\); here the sum is over all \(n!\) permutations (\(i_{1}\),\(i_{2}\),\(\ldots,i_{n}\)) of the indices \((1,2,\ldots,n)\). If \(E|T|^{p}<\infty\) for some \(p\geq 1\), then \(E|\varphi(\mathbf{X})-\theta|^{p}\leq E|T-\theta|^{p}\) (See Exercise 8.1)._

### Asymptotic Distribution Theory of Parametric Large Sample Tests

Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d with common p.d.f. \(f(x;\theta)\) (w.r.t. a \(\sigma\)-finite measure \(\mu\)), with the parameter space \(\Theta\) an open subset of \(\mathbb{R}^{p}\). Assume that the hypothesis of Theorem 7.2, Chap. 7, holds. Suppose that the _null hypothesis_ may be expressed in the form

\[H_{0}:\pi_{i}(\boldsymbol{\theta})=0,\qquad 1\leq i\leq k, \tag{8.65}\]

where \(1\leq k\leq p\). To test this, against the alternative that \(H_{0}\) is not true (i.e., \(H_{1}:\theta\not\in\Theta_{0}\equiv\{\boldsymbol{\theta}\in\Theta:\pi_{i}( \boldsymbol{\theta})=0\ \forall\ i\}\)), a natural procedure would be to see if \(\pi_{i}(\tilde{\boldsymbol{\theta}}_{n})\) are close to zero (for \(1\leq i\leq k\)) or not; here \(\tilde{\boldsymbol{\theta}}_{n}\) is an asymptotically efficient estimator of \(\boldsymbol{\theta}\). If \(\boldsymbol{\theta}_{0}\) is the true parameter value, then under \(H_{0}\) (i.e., for \(\boldsymbol{\theta}_{0}\in\Theta_{0}\)) one has

\[\pi_{i}(\tilde{\boldsymbol{\theta}}_{n}) =\pi_{i}(\boldsymbol{\theta}_{0})+\sum_{r=1}^{p}\left(\tilde{ \theta}_{n}^{(r)}-\theta_{0}^{(r)}\right)\left(\frac{\partial\pi_{i}}{ \partial\theta_{r}}\right)_{\boldsymbol{\theta}^{*}}\] \[=\sum_{r=1}^{p}\left(\tilde{\boldsymbol{\theta}}_{n}^{(r)}-\theta _{0}^{(r)}\right)\left[\left(\frac{\partial\pi_{i}}{\partial\theta^{(r)}} \right)_{\boldsymbol{\theta}_{0}}+o_{p}(1)\right], \tag{8.66}\]

assuming \(\pi_{i}(\boldsymbol{\theta})\) is continuously differentiable (in \(\boldsymbol{\theta}\)) on \(\Theta\). One may express (8.66) for all \(i\) compactly as

\[\sqrt{n}\,\pi(\tilde{\boldsymbol{\theta}}_{n})=\Big{[}(\operatorname{Grad}\pi)_ {\boldsymbol{\theta}_{0}}+o_{p}(1)\Big{]}\sqrt{n}(\tilde{\boldsymbol{\theta} }_{n}-\boldsymbol{\theta}_{0}) \tag{8.67}\]

[MISSING_PAGE_EMPTY:12631]

The expression within curly brackets in (8.74) converges to \(-\pi(\boldsymbol{\theta})^{\prime}V^{-1}(\boldsymbol{\theta})\pi(\boldsymbol{ \theta})\) almost surely (\(P_{\boldsymbol{\theta}}\)). Since this last quantity is strictly negative, \(\chi^{2}_{1-\alpha}(k)+n\{\dots\}\) converges to \(-\infty\) almost surely as \(n\to\infty\). Hence \(\beta_{n}\to 0\).

Since all reasonable tests are consistent, to discriminate among them one must consider a sequence of alternatives \(\boldsymbol{\theta}_{n}\) such that \(\boldsymbol{\theta}_{n}\not\in\Theta_{0}\), \(\boldsymbol{\theta}_{n}\to\boldsymbol{\theta}_{0}\in\Theta_{0}\), and

\[\beta_{n}(\boldsymbol{\theta}_{n})\longrightarrow\beta. \tag{8.75}\]

for some \(\beta<1-\alpha\). This requires that one takes

\[\boldsymbol{\theta}_{n}=\boldsymbol{\theta}_{0}+n^{-\frac{1}{2}}\boldsymbol{ \delta}\qquad(\boldsymbol{\delta}=(\delta_{1},\dots,\delta_{p})^{\prime}) \tag{8.76}\]

such that \(\theta_{0}\) is an element of \(\Theta_{0}\) and \(\boldsymbol{\delta}\neq\boldsymbol{0}\) is an element of \(\mathbb{R}^{p}\), \(\boldsymbol{\theta}_{0}+n^{-\frac{1}{2}}\boldsymbol{\delta}\not\in\Theta_{0}\). Since

\[\pi_{i}(\boldsymbol{\theta}_{n}) =\pi_{i}(\boldsymbol{\theta}_{0})+n^{-\frac{1}{2}}\sum_{r=1}^{p} \delta_{r}\left(\frac{\partial\pi_{i}(\boldsymbol{\theta})}{\partial\theta^{(r )}}\right)_{\theta_{0}}+o\left(n^{-\frac{1}{2}}\right), \tag{8.77}\] \[\pi(\boldsymbol{\theta}_{n}) =o+n^{-\frac{1}{2}}\cdot\operatorname{Grad}\pi(\boldsymbol{ \theta}_{0})\boldsymbol{\delta}+o\left(n^{-\frac{1}{2}}\right),\]

one ought to choose \(\boldsymbol{\delta}\) so that it is not orthogonal to all the vectors \(\operatorname{Grad}\pi_{i}(\boldsymbol{\theta}_{0})\), \(1\leq i\leq k\). Then under (8.74)-(8.77) we have

\[\beta_{n}(\boldsymbol{\theta}_{n}) =P_{\boldsymbol{\theta}_{n}}(W_{n}\leq\chi^{2}_{1-\alpha}(k))\] \[=P_{\boldsymbol{\theta}_{n}}(n(\pi(\tilde{\boldsymbol{\theta}}_{ n})-\pi(\boldsymbol{\theta}_{n}))^{\prime}V^{-1}(\tilde{\boldsymbol{\theta}}_{ n})(\pi(\tilde{\boldsymbol{\theta}}_{n})-\pi(\boldsymbol{\theta}_{n}))\] \[\leq\chi^{2}_{1-\alpha}(k)-\boldsymbol{\delta}^{\prime} \operatorname{Grad}\pi(\boldsymbol{\theta}_{0})V^{-1}(\boldsymbol{\theta}_{0} )(\operatorname{Grad}\pi(\boldsymbol{\theta}_{0}))^{\prime}\boldsymbol{\delta }+o_{p}(1)).\]

Assume now that there exists \(\varepsilon>0\) such that for all \(x>0\)

\[\sup_{|\boldsymbol{\theta}-\boldsymbol{\theta}_{0}|\leq\varepsilon}\Big{|}P_{ \boldsymbol{\theta}}(n(\pi(\tilde{\boldsymbol{\theta}}_{n})-\pi(\boldsymbol{ \theta}))^{\prime}V^{-1}(\boldsymbol{\theta})(\pi(\tilde{\boldsymbol{\theta}} _{n})-\pi(\boldsymbol{\theta}))\leq x)-G_{k}(x)\Big{|}\to 0\,\text{as }n\to\infty,\] (A)

where \(G_{k}\) is the distribution function of a chi-square random variable with \(k\) degrees of freedom. Then

\[\lim_{n\to\infty}\beta_{n}(\boldsymbol{\theta}_{n})=G_{k}\left(\chi^{2}_{1- \alpha}(k)-\boldsymbol{\delta}^{\prime}\big{(}\operatorname{Grad}\pi( \boldsymbol{\theta}_{0}))^{\prime}V^{-1}(\boldsymbol{\theta}_{0})\text{Grad}\, \pi(\boldsymbol{\theta}_{0})\boldsymbol{\delta}\right). \tag{8.78}\]

In order that this limit be \(\beta\), one must choose \(\delta\) so that \(G_{k}(\chi^{2}_{1-\alpha}(k)-\gamma(\boldsymbol{\delta}))=\beta\), where

\[\gamma(\boldsymbol{\delta}):=\boldsymbol{\delta}^{\prime}(\operatorname{Grad} \pi(\boldsymbol{\theta}_{0}))^{\prime}V^{-1}(\boldsymbol{\theta}_{0}) \text{Grad}\,\pi(\boldsymbol{\theta}_{0})\boldsymbol{\delta}. \tag{8.79}\]

This is possible since \(G_{k}(\chi^{2}_{1-\alpha}(k))=1-\alpha\) and \(\beta<1-\alpha\).

_Remark 8.6_.: Theorem 8.3 is due to Wald (1943). To motivate the test (8.72), suppose (as is often the case) that _the MLE_\(\hat{\boldsymbol{\theta}}_{n}\) is a sufficient statistic for \(\theta\). It is then enough to confine one's attention to tests based on \(\hat{\boldsymbol{\theta}}_{n}\). By Theorem 7.5, Chap. 7, the asymptotic distribution of \(\hat{\boldsymbol{\theta}}_{n}\) is the \(k\)-dim. Normal distribution \(N(\theta,\frac{1}{n}I^{-1}(\boldsymbol{\theta}))\). If the latter was the exact distribution of \(\hat{\boldsymbol{\theta}}_{n}\), and if \(I(\boldsymbol{\theta})\) was known, then an optimal test for \(H_{0}:\theta_{1}=\theta_{2}=\dots=\theta_{k}=0\) would be given by (See Lehmann, _Testing Statistical Hypotheses_)

\[\text{Reject }H_{0}\text{ if }n\left(\hat{\boldsymbol{\theta}}_{n}^{(1)} \right)^{\prime}V^{-1}\hat{\boldsymbol{\theta}}_{n}^{(1)}>\chi^{2}_{1-\alpha}(k), \tag{8.80}\]where \(\hat{\boldsymbol{\theta}}_{n}^{(1)}=(\hat{\theta}_{n1},\ldots,\hat{\theta}_{nk})^{\prime}\) and \(V=\) the matrix formed by the elements in the first \(k\) rows and the first \(k\) columns of the information matrix. In case \(I=I(\boldsymbol{\theta})\) depends on \(\boldsymbol{\theta}\), one may replace \(\boldsymbol{\theta}\) by \(\hat{\boldsymbol{\theta}}_{n}\). This yields test (8.72). The case (8.65) simply needs a reparametrization.

An alternative test of (8.65) is the _likelihood ratio test_ originally proposed by Neyman and Pearson (1928) and explored by Wilks (1938). According to this procedure one calculates the _likelihood ratio statistic_

\[\Lambda_{n}=\frac{\max_{\boldsymbol{\theta}\in\Theta_{0}}f_{n}(\mathbf{X}; \boldsymbol{\theta})}{\max_{\boldsymbol{\theta}\in\Theta}f_{n}(\mathbf{X}; \boldsymbol{\theta})}=\frac{f_{n}(\mathbf{X};\hat{\hat{\boldsymbol{\theta}}}_{ n})}{f_{n}(\mathbf{X};\hat{\boldsymbol{\theta}}_{n})}\,. \tag{8.81}\]

The test statistic is then

\[\lambda_{n}=-2\log\Lambda_{n}=2\log f_{n}(\mathbf{X};\hat{\boldsymbol{\theta} }_{n})-2\log f_{n}(\mathbf{X};\hat{\hat{\boldsymbol{\theta}}}_{n}) \tag{8.82}\]

and the _likelihood ratio test_ is

\[\text{Reject }H_{0}\text{ iff }\lambda_{n}>\chi_{1-\alpha}^{2}(k). \tag{8.83}\]

**Theorem 8.4**.: _Assume the hypothesis of Theorem 8.3._

1. _If_ \(H_{0}\) _holds, then_ \[\lambda_{n}=W_{n}+o_{p}(1),\] (8.84) _so that_ \(\lambda_{n}\) _converges in law to a chi-square distribution with_ \(k\) _d.f., and the test (_8.83_) has asymptotic size_ \(\alpha\)_._
2. _The likelihood ratio test (_8.83_) is consistent. Also, for alternatives_ \(\boldsymbol{\theta}_{n}\) _given by (_8.76_) (with_ \(\boldsymbol{\delta}\) _not orthogonal to the linear span of_ \(\operatorname{Grad}\pi_{i}(\boldsymbol{\theta})\)__\((1\leq i\leq k)\) _on_ \(\Theta_{0}\)_),_ \[\lim_{n\to\infty}P_{\boldsymbol{\theta}_{n}}\left(\lambda_{n}\leq\chi_{1- \alpha}^{2}(k)\right)=\beta,\] (8.85) _provided_ \(\beta<1-\alpha\)_,_ \(\gamma(\boldsymbol{\delta})\) _in (_8.79_) satisfies_ \(G_{k}(\chi_{1-\alpha}^{2}(k)=\gamma(\delta))=\beta\)_, and an analogue of (_A_) holds for_ \(\lambda_{n}\)_._

Proof.:
1. Fix \(\boldsymbol{\theta}_{0}\in\Theta_{0}\). All \(o_{p}(1)\) errors below are under \(P_{\boldsymbol{\theta}_{0}}\). Assume for the sake of simplicity that \(\pi_{i}(\boldsymbol{\theta})=\boldsymbol{\theta}^{(i)}\) (\(\boldsymbol{\theta}:=(\theta^{(1)},\ldots,\theta^{(p)})^{\prime}\in\Theta\)), \(1\leq i\leq k\). This may be achieved, at least in a neighborhood of \(\boldsymbol{\theta}_{0}\), by reparametrization. Write \[I(\boldsymbol{\theta}_{0})=\begin{bmatrix}I_{11}&I_{12}\\ I_{21}&I_{22}\end{bmatrix},\qquad I^{-1}(\boldsymbol{\theta}_{0})=\begin{bmatrix} I^{11}&I^{12}\\ I^{21}&I^{22}\end{bmatrix},\] (8.86) where \(I_{11}\) comprises the elements of \(I(\boldsymbol{\theta}_{0})\) belonging to the first \(k\) rows and the first \(k\) columns, etc. Also write \((x)_{r^{\prime}}^{r}=(x^{(r)},\ldots,x^{(r^{\prime})})^{\prime}\) for \(x=(x^{(1)},\ldots,x^{(p)})^{\prime}\in\mathbb{R}^{p}\). With this notation, Wald's statistic (see (8.70), (8.71)) becomes \[W_{n}=n(\hat{\boldsymbol{\theta}}_{n})_{k}^{1^{\prime}}(I^{11})^{-1}(\hat{ \boldsymbol{\theta}}_{n})_{k}^{1}.\] (8.87) Since \((\hat{\boldsymbol{\theta}}_{n})_{k}^{1}=0=(\hat{\hat{\boldsymbol{\theta}}}_{ n})_{k}^{1}\), \(\lambda_{n}\) may be expressed as \[\lambda_{n} =n(\hat{\boldsymbol{\theta}}_{n}-\hat{\hat{\boldsymbol{\theta}}}_{ n})^{\prime}I(\boldsymbol{\theta}_{0})(\hat{\boldsymbol{\theta}}_{n}-\hat{\hat{ \boldsymbol{\theta}}}_{n})+o_{p}(1)\] \[=(I(\boldsymbol{\theta}_{0})\sqrt{n}(\hat{\boldsymbol{\theta}}_{ n}-\hat{\hat{\boldsymbol{\theta}}}_{n}))^{\prime}\sqrt{n}(\hat{\boldsymbol{\theta}}_{ n}-\hat{\hat{\boldsymbol{\theta}}}_{n})+o_{p}(1)\]\[I_{k}-I_{12}I^{21}+I_{12}I_{22}^{-1}I_{22}I^{21}=I_{k}. \tag{8.95}\]
2. The proof of part (b) follows along the lines of the computations (8.77)-(8.79).

Remark Remark 8.7 (Amplification of (8.87)-(8.88)): Let the \(k\times k\)-matrix

\[V(\boldsymbol{\theta}_{0})=\overbrace{\text{Grad}\,\pi(\boldsymbol{\theta}_{0} )}^{(k\times p)\text{-matrix}}\overbrace{I^{-1}(\boldsymbol{\theta}_{0})}^{p \times p}(\text{Grad}\,\pi(\boldsymbol{\theta}_{0}))^{\prime}.\]If \(\pi_{i}(\boldsymbol{\theta})=\theta_{i}\), \(1\leq i\leq k\), then \(\operatorname{Grad}\pi(\boldsymbol{\theta}_{0})=\begin{bmatrix}I_{k}&0\end{bmatrix}_ {k\times(p-k)}\)

\[V(\boldsymbol{\theta}_{0}) =\begin{bmatrix}I_{k}&0\end{bmatrix}\begin{bmatrix}I^{11}&I^{12}\\ I^{21}&I^{22}\end{bmatrix}\begin{bmatrix}I_{k}\\ 0\end{bmatrix}_{(p-k)\times k}=\begin{bmatrix}I^{11}&I^{12}\end{bmatrix} \begin{bmatrix}I_{k}\\ 0\end{bmatrix}\] \[=I^{11}.\]

\[V^{-1}(\boldsymbol{\theta}_{0})=\begin{pmatrix}I^{11}\end{pmatrix}^{-1}.\]

Then

\[W_{n}=n\left[\begin{pmatrix}\hat{\boldsymbol{\theta}}_{n}\end{pmatrix}_{1}^{k} \right]^{\prime}(I^{11})^{-1}(\hat{\boldsymbol{\theta}}_{n})_{1}^{k}.\]

The log of the likelihood ratio

\[\lambda_{n} =2\log f_{n}(\mathbf{X};\hat{\boldsymbol{\theta}}_{n})-2\log f_{ n}(\mathbf{X};\hat{\hat{\boldsymbol{\theta}}}_{n}))\] \[=2\underbrace{(\hat{\boldsymbol{\theta}}_{n}-\hat{\hat{\boldsymbol {\theta}}}_{n})^{\prime}\text{grad}\,\ell(\boldsymbol{\theta})|_{\theta=\hat{ \hat{\theta}}_{n}}}_{=0}+\sum_{i,j=1}^{k}n(\hat{\boldsymbol{\theta}}_{n}-\hat{ \hat{\boldsymbol{\theta}}}_{n})_{i}(\hat{\boldsymbol{\theta}}_{n}-\hat{\hat{ \boldsymbol{\theta}}}_{n})_{j}\] \[\quad\cdot\underbrace{\frac{1}{n}\frac{\partial^{2}\ell( \boldsymbol{\theta})}{\partial\theta_{i}\partial\theta_{j}}\Big{|}_{\theta_{0} }}_{=I(\theta_{0})+o_{p}(1)}+\text{(under $P_{\theta_{0}}$)}\] \[=n(\hat{\boldsymbol{\theta}}_{n}-\hat{\hat{\boldsymbol{\theta}}}_ {n})^{\prime}I(\boldsymbol{\theta}_{0})(\hat{\boldsymbol{\theta}}_{n}-\hat{ \hat{\boldsymbol{\theta}}}_{n})+o_{p}(1).\]

**Corollary 8.1**.: _The Pitman ARE of the likelihood ratio test (8.83) relative to Wald's test (8.72) is one._

_Example 8.4 (Multinomial Models and the Chi-square Test)._ A population is divided into \(M+1\) categories and a random sample of size \(n\) is drawn from it (without replacement). Let \(\theta^{(j)}\) denote the probability of an observation to belong to the \(j\)-th category (\(j=1,2,\ldots,M+1\)), and assume \(\theta^{(j)}>0\) for all \(j\). Write \(\boldsymbol{\theta}=(\theta^{(1)},\ldots,\theta^{(M)})^{\prime}\). From Example 7.1, Chap.7, we know that (i) the MLE of \(\boldsymbol{\theta}\) is \(\hat{\boldsymbol{\theta}}_{n}=(\hat{\theta}_{n}^{(1)},\ldots,\hat{\theta}_{n} ^{(M)})\), \(\hat{\theta}_{n}^{(M+1)}:=1-\sum_{1}^{M}\hat{\theta}_{n}^{(j)}\), where \(\hat{\theta}_{n}^{(j)}\) is the proportion in the sample belonging to the \(j\)-th category (\(1\leq j\leq M\)), and (ii) \(\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}-\boldsymbol{\theta})\ \xrightarrow{\mathscr{L}}\ N(0,I^{-1}(\boldsymbol{\theta}))\), where the \((i,j)\) element of \(I^{-1}(\boldsymbol{\theta})\) is

\[\sigma_{ij}(\boldsymbol{\theta}):=\left\{\begin{aligned} -\theta^{(i)}\theta^{(j)}&\text{if $i\neq j$},\\ \theta^{(i)}(1-\theta^{(i)})&\text{if $i=j$}\end{aligned}\right. \tag{8.96}\]

A widely used alternative to (8.72) or (8.83) for testing (8.65) in this case is the so-called _frequency chi-square test,_ originally due to Karl Pearson:

\[\text{Reject $H_{0}$ iff }\quad\sum_{j=1}^{M+1}\frac{n^{2}(\hat{\theta}_{n}^{(j)}-\hat{\hat{\theta}}_{n}^{(j)})^{2}}{n\hat{\hat{\theta}}_{n}^{(j)}}>\chi_{1-\alpha}^{2}(M). \tag{8.97}\]

We will show that the _statistic on the left differs from the likelihood ratio statistic \(\lambda_{n}\) by a quantity which is \(o_{p}(1)\), under \(H_{0}\)_. First note that

\[f_{n}(\mathbf{X};\boldsymbol{\theta})=\prod_{j=1}^{M}\left(\theta^{(j)}\right)^ {\nu_{j}},\qquad\nu_{j}:=n\hat{\theta}_{n}^{(j)}.\]Hence

\[\lambda_{n} = 2L_{n}(\hat{\boldsymbol{\theta}}_{n})-2L_{n}(\hat{\hat{\boldsymbol{ \theta}}}_{n})=2\sum_{j=1}^{M}\nu_{j}\log\frac{\hat{\boldsymbol{\theta}}_{n}^{(j )}}{\hat{\hat{\boldsymbol{\theta}}}_{n}^{(j)}} \tag{8.98}\] \[= 2\sum_{j=1}^{M}\nu_{j}\log\left(1+\frac{\hat{\boldsymbol{\theta} }_{n}^{(j)}}{\hat{\hat{\boldsymbol{\theta}}}_{n}^{(j)}}-1\right)\] \[= 2\sum_{j=1}^{M}\nu_{j}\left\{\frac{\hat{\boldsymbol{\theta}}_{n}^ {(j)}}{\hat{\boldsymbol{\theta}}_{n}^{(j)}}-1-\frac{1}{2}\left(\frac{\hat{ \boldsymbol{\theta}}_{n}^{(j)}}{\hat{\hat{\boldsymbol{\theta}}}_{n}^{(j)}}-1 \right)^{2}+o_{p}(n^{-1})\right\},\]

since \(|\log(1+x)-(x-\frac{1}{2}x^{2})|\leq(\frac{4}{3})|x|^{3}\) if \(|x|\leq\frac{1}{2}\), and

\[\left|\frac{\hat{\boldsymbol{\theta}}_{n}^{(j)}}{\hat{\hat{\boldsymbol{\theta }}}_{n}^{(j)}}-1\right|^{3}\leq 4n^{-\frac{3}{2}}\left\{\left|\frac{\sqrt{n}( \hat{\boldsymbol{\theta}}_{n}^{(j)}-\boldsymbol{\theta}_{0}^{(j)})}{\hat{ \hat{\boldsymbol{\theta}}}_{n}^{(j)}}\right|^{3}+\left|\frac{\sqrt{n}(\hat{ \hat{\boldsymbol{\theta}}}_{n}^{(j)}-\boldsymbol{\theta}_{0}^{(j)})}{\hat{ \hat{\boldsymbol{\theta}}}_{n}^{(j)}}\right|^{3}\right\}. \tag{8.99}\]

Since \(\sqrt{n}(\hat{\boldsymbol{\theta}}_{n}^{(j)}-\boldsymbol{\theta}_{0}^{(j)})/ \hat{\hat{\boldsymbol{\theta}}}_{n}^{(j)}\), \(\sqrt{n}(\hat{\hat{\boldsymbol{\theta}}}_{n}^{(j)}-\boldsymbol{\theta}_{0}^{( j)}/\hat{\hat{\boldsymbol{\theta}}}_{n}^{(j)}\) converges in distribution under \(P_{\theta_{0}}\), the expression within curly brackets in (8.99) is bounded in probability, so that (8.99) is \(o_{p}(n^{-1})\).

From (8.98) we get

\[\lambda_{n} = 2\sum_{j=1}^{M}n\boldsymbol{\hat{\boldsymbol{\theta}}}_{n}^{(j )}\left\{\frac{\hat{\boldsymbol{\theta}}_{n}^{(j)}}{\hat{\hat{\boldsymbol{ \theta}}}_{n}^{(j)}}-1-\frac{1}{2}\left(\frac{\hat{\boldsymbol{\theta}}_{n}^{( j)}}{\hat{\hat{\boldsymbol{\theta}}}_{n}^{(j)}}-1\right)^{2}\right\}+o_{p}(1) \tag{8.100}\] \[= \sum_{j=1}^{M}\left[2n\left(\hat{\boldsymbol{\theta}}_{n}^{(j)}- \hat{\boldsymbol{\theta}}_{n}^{(j)}\right)\left(\frac{\hat{\boldsymbol{\theta }}_{n}^{(j)}-\hat{\boldsymbol{\theta}}_{n}^{(j)}}{\hat{\boldsymbol{\theta}}_{n }^{(j)}}\right)\right.\] \[\qquad\qquad+\left.2n\left(\hat{\boldsymbol{\theta}}_{n}^{(j)}- \hat{\boldsymbol{\theta}}_{n}^{(j)}\right)-\frac{\hat{\boldsymbol{\theta}}_{n} ^{(j)}}{\hat{\boldsymbol{\theta}}_{n}^{(j)}}\frac{n\left(\hat{\boldsymbol{ \theta}}_{n}^{(j)}-\hat{\boldsymbol{\theta}}_{n}^{(j)}\right)^{2}}{\hat{ \boldsymbol{\theta}}_{n}^{(j)}}\right]+o_{p}(1)\] \[= 2\sum_{j=1}^{M}\frac{n\left(\hat{\boldsymbol{\theta}}_{n}^{(j)}- \hat{\boldsymbol{\theta}}_{n}^{(j)}\right)^{2}}{\hat{\boldsymbol{\theta}}_{n}^ {(n)}}\] \[\quad+2n\sum_{j=1}^{M}\left(\hat{\boldsymbol{\theta}}_{n}^{(j)}- \hat{\boldsymbol{\theta}}_{n}^{(j)}\right)-\sum_{j=1}^{M}\frac{n\left(\hat{ \boldsymbol{\theta}}_{n}^{(j)}-\hat{\boldsymbol{\theta}}_{n}^{(j)}\right)^{2}}{ \hat{\boldsymbol{\theta}}_{n}^{(j)}}\left(1+o_{p}(1)\right)+o_{p}(1)\] \[= \sum_{j=1}^{M}\frac{n\left(\hat{\boldsymbol{\theta}}_{n}^{(j)}- \hat{\boldsymbol{\theta}}_{n}^{(j)}\right)^{2}}{\hat{\boldsymbol{\theta}}_{n} ^{(j)}}+0+o_{p}(1),\]using the facts (i) \(\sum\hat{\boldsymbol{\theta}}_{n}^{(j)}=1=\sum\hat{\boldsymbol{\theta}}_{n}^{(j)}\), (ii) \(\hat{\boldsymbol{\theta}}_{n}^{(j)}/\hat{\boldsymbol{\theta}}_{n}^{(j)}\to 1\) in probability, and (iii) \(n(\hat{\boldsymbol{\theta}}_{n}^{(j)}-\hat{\bar{\boldsymbol{\theta}}}_{n}^{(j)} )^{2}/\hat{\bar{\boldsymbol{\theta}}}_{n}^{(j)}\) is bounded in probability. This completes the proof of the italicized statement above.

With a little extra effort it may be proved that, for every \(\varepsilon>0\),

\[P_{\boldsymbol{\theta}_{n}}\left(\left|\lambda_{n}-\sum_{j=1}^{M}\frac{n\left( \hat{\boldsymbol{\theta}}_{n}^{(j)}-\hat{\bar{\boldsymbol{\theta}}}_{n}^{(j)} \right)^{2}}{\hat{\bar{\boldsymbol{\theta}}}_{n}^{(j)}}\right|>\varepsilon \right)\longrightarrow 0\quad\text{as }n\rightarrow\infty. \tag{8.101}\]

From this it follows that _the Pitman ARE of the frequency chi-square test relative to the likelihood ratio test is one_. Thus for this example, the Wald test, the likelihood ratio test, and the frequency chi-square test are all asymptotically equivalent from the point of view of the Pitman ARE.

We now discuss _Rao's scores test_ which is an important alternative to Wald's and likelihood ratio tests under the hypothesis of Theorem 8.3. First consider a simple null hypothesis: \(H_{0}:\boldsymbol{\theta}=\boldsymbol{\theta}_{0}\) (\(H_{1}:\boldsymbol{\theta}\neq\boldsymbol{\theta}_{0}\)). Under \(H_{0}\), as in the proof of Theorem 7.5,

\[U_{n}:=\frac{1}{\sqrt{n}}\text{grad}\log f_{n}(\mathbf{X};\boldsymbol{\theta}) \big{|}_{\boldsymbol{\theta}=\boldsymbol{\theta}_{0}}\stackrel{{ \mathscr{L}}}{{\longrightarrow}}N(\boldsymbol{0},I(\boldsymbol{\theta}_{0})), \tag{8.102}\]

where \(N\) is \(k\)-dimensional normal. Hence

\[Q_{n}:=U_{n}^{\prime}\,I^{-1}(\boldsymbol{\theta}_{0})U_{n}\stackrel{{ \mathscr{L}}}{{\longrightarrow}}Z_{k}^{0}, \tag{8.103}\]

where \(Z_{k}^{0}\) has a \(\chi_{k}^{2}\) distribution, and \(H_{0}\) is rejected if \(Q_{n}>\chi_{1-\alpha}^{2}(k)\). Unlike the likelihood ratio and Wald's tests, this test does not require computation of the MLE. For testing a composite \(H_{0}:\boldsymbol{\theta}\in\boldsymbol{\Theta}_{0}\), \(\boldsymbol{\theta}_{0}\) in (8.102), (8.103) is replaced by the MLE \(\hat{\bar{\theta}}_{n}\) under \(H_{0}\). As in Corollary 8.1, one can prove that the ARE of Rao's scores test relative to Wald's test (and, therefore, to the likelihood ratio test) is one. For details see Rao (1973), Serfling (1980), Sen and Singer (1979) and van der Vaart (1998). For higher order comparisons among the three tests see Mukherjee and Reid (2001). Also see Brown et al. (2001) for a comparison of coverage errors for confidence intervals for the binomial proportion based on Wald's and Rao's (score) tests.

### Tests for Goodness-of-Fit

It is a common practice in statistics to see if a random sample of observations \(X_{1},\ldots,X_{n}\) may fit a distribution \(F\)--a parametric model specified up to perhaps an unknown finite dimensional parameter \(\theta\). That is, test if the observations may be considered to have been drawn from \(F\). For continuous data, one may, e.g., test if \(F\) is Normal \(N(\mu,\sigma^{2})\), with \(\boldsymbol{\theta}=(\mu,\sigma^{2})\) unknown. Similarly, one may test if the number of accidents per week at a particular traffic intersection follows the Poisson distribution \(\mathscr{P}(\lambda)\), \(\theta=\lambda>0\) unknown.

Before considering a number of goodness-of-fit tests, we recall the follow widely used notion or index.

**Definition 8.2**.: The \(p\)_-value_ of a test is the smallest level of significance (or size) at which the null hypothesis \(H_{0}\) would be rejected by the observed value of the test statistic.

Informally, the \(p\)-value of a test is the probability of having a discrepancy from the null hypothesis as much (or more) as observed, if the null hypothesis were true. Thus a very small \(p\)-value may be taken as a strong evidence against \(H_{0}\), whereas a relatively large \(p\)-value provides some evidence in support of \(H_{0}\).

A classical goodness-of-fit procedure is by the _frequency chi-square_. Here the range of the distribution is divided into a finite number, say \(k\), of disjoint classes, or consecutive intervals, and the number of observations \(n_{j}\) in the \(j\)th class is compared with the 'expected' number of observations \(E_{j}\) in the class under \(H_{0}\). Here \(E_{j}\) equals \(n\hat{\hat{\theta}}\,_{n}^{(j)}\), where \(\hat{\hat{\theta}}\,_{n}^{(j)}\) is the estimated probability of the \(j\)th class computed by using a good estimate, usually the MLE of the unknown parameters of the model. _Pearson's chi-square statistic_\(\sum_{j=1}^{k}(n_{j}-E_{j})^{2}/E_{j}^{2}\) has asymptotically a chi-square distribution \(\chi_{q}^{2}\), as \(n\to\infty\), where the degrees of freedom _(d.f.)_\(q\) equals \(k-1-r\), \(r\) being the number of unknown parameters required to specify \(F\). (See the derivation of the frequency chi-square test in the preceding section.) For details see Chernoff and Lehmann (1954), Rao (1973) and Sen and Singer (1979). If the chi-square statistic has a large observed value, i.e., if the \(p\)-value is small, one rejects \(H_{0}\); otherwise the model is not rejected. Generally, this test is _not consistent;_ for two different distributions may assign the same probability to each of the \(k\) classes, unless the model is discrete with only \(k\) different values.

To present one of the earliest _consistent_ goodness-of-fit tests due to Kolmogorov and Smirnov, and also derive later tests due to Cramer and von Mises and by Anderson and Darling, we now provide an informal introduction to the _functional central limit theorem and Brownian motion_.2

Footnote 2: See, e.g., Bhattacharya and Waymire (2007), Chap. 11, or Billingsley (1968), Sects. 10, 11.

Consider a sequence of i.i.d. random variables \(X_{n}\) (\(n=1,2,\dots\)) with mean \(\mu\) and finite variance \(\sigma^{2}>0\). Changing to \(Z_{n}=X_{n}-\mu\), and writing \(S_{n}=Z_{1}+\dots+Z_{n}\) and \(S_{0}=0\), one obtains, by the classical CLT,

\[S_{n}/\sqrt{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}N(0, \sigma^{2})\quad\text{as $n\to\infty$.} \tag{8.104}\]

Consider now the stochastic processes \(Y_{n}\) (\(n=1,2,\dots\)) on \([0,\infty)\) defined by

\[Y_{n}(t)=S_{[nt]}/\sqrt{n},\quad t\in[0,\infty),\,([nt]:=\text{integer part of $nt$}). \tag{8.105}\]

It is a (random) _step function,_ with \(Y_{n}(0)=0\), \(Y_{n}(1/n)=S_{1}/\sqrt{n}\), \(Y_{n}(j/n)=S_{j}/\sqrt{n}\), and \(Y_{n}(t)\) is constant for \(j/n\leq t<(j+1)/n\). Then \(Y_{n}(0)=0\), and for any fixed \(t\), \(0<t<\infty\), \(Y_{n}(t)\) is asymptotically Normal with mean zero and variance \([nt]\sigma^{2}/n\approx t\sigma^{2}\). That is,

\[Y_{n}(t)\stackrel{{\mathscr{L}}}{{\longrightarrow}}N(0,t\sigma^{ 2}),\quad\text{as $n\to\infty$,}\]

and if \(0<t_{1}<t_{2}<\dots<t_{k}\), then as \(n\to\infty\), for \(i\leq j\),

\[\text{cov}(Y_{n}(t_{i}),Y_{n}(t_{j})) =\left(\frac{1}{n}\right)\text{cov}(Z_{1}+\dots+Z_{[nt_{i}]},Z_{1 }+\dots+Z_{[nt_{j}]})=\] \[=\left(\frac{1}{n}\right)\text{cov}(Z_{1}+\dots+Z_{[nt_{i}]},Z_{1 }+\dots+Z_{[nt_{j}]})=\] \[=\left(\frac{1}{n}\right)\text{var}(Z_{1}+\dots+Z_{[nt_{i}]})=([ nt_{i}]/n)\sigma^{2}\longrightarrow t_{i}\sigma^{2}.\]Therefore, by the multivariate CLT (by taking linear combinations of \(Y_{n}(t_{i})\), \(1\leq i\leq k\), and applying Lindeberg's CLT),

\[(Y_{n}(t_{1}),Y_{n}(t_{2}),\ldots,Y_{n}(t_{k}))\stackrel{{\mathscr{ L}}}{{\longrightarrow}}N(0,((\min\{t_{i},t_{j}\}\sigma^{2})),\quad\text{as $n\to\infty$}. \tag{8.106}\]

Since \(k\) and \(0<t_{1}<t_{2}<\cdots<t_{k}\) are arbitrary, one may think of this as convergence of the processes \(Y_{n}=\{Y_{n}(t):t\in[0,\infty)\}\) to a _Gaussian process_\(B\) on \([0,\infty)\) satisfying

\[E(B(t))=0,\quad\text{cov}(B(s),B(t))=E(B(s)B(t))=\min\{s,t\}\sigma^{2}\quad( \text{for all $s,t\geq 0$}). \tag{8.107}\]

We will call such a \(B\) the _Brownian motion with zero mean and dispersion_\(\sigma^{2}\). It is simple to check either using the corresponding fact for \(Y_{n}\), or directly using (8.107) that Brownian motion has _independent increments,_ i.e., if \(0<t_{1}<t_{2}<\cdots<t_{k}\) then

\[B(t_{i})-B(t_{i-1}),\,i=1,\ldots,k,\] \[\text{are $k$ independent $N(0,(t_{i}-t_{j-1})\sigma^{2})$ random variables ($t_{0}=0$)}. \tag{8.108}\]

Indeed, (8.108) is equivalent to (8.107) for a Gaussian process, since a Gaussian process is determined by its mean and covariance (functions). If \(\sigma^{2}=1\), then the Brownian motion is said to be a _standard Brownian motion._ It was proved by Wiener in 1923 that _Brownian paths are_ (or, can be taken to be) _continuous._ The distribution of \(B\) on \(C[0,\infty)\) is called the _Weiner measure._ Here \(C(0,\infty)\) is the space of all real-valued continuous functions on the half line \([0,\infty)\). It is a metric space in which convergence \(f_{n}\to f\) means uniform convergence of \(f_{n}\) to \(f\) on bounded intervals. The usual sigma-field on it is the Borel sigma-field generated by its open sets. Although \(Y_{n}\) is a step function since it has the value \(S_{j}/\sqrt{n}\) for \(j/n\leq t<(j+1)/n)\), one may still write (in an appropriate sense)

\[Y_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}B\quad\text{as $n\to\infty$}. \tag{8.109}\]

Another possibility is to linearly interpolate \(Y_{n}\) on \(j/n<t<(j+1)/n\), for all \(j=0,1,\ldots,\) yielding a continuous process \(\widetilde{Y}_{n}(t)\) (with polygonal paths) which agrees with \(Y_{n}(t)\) at points \(t=j/n\),

\[\widetilde{Y}_{n}(t) =S_{j}/\sqrt{n}+n((t-j/n)(S_{j+1}/\sqrt{n}-S_{j}/\sqrt{n})\] \[=S_{j}/\sqrt{n}+n((t-j/n)Z_{j+1}/\sqrt{n}\text{ for $j/n\leq t \leq(j+1)/n$ }\,(j=0,1,2,\ldots).\]

Then, in the usual sense of weak convergence of probability measures on metric spaces,

\[\widetilde{Y}_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}B, \quad\text{as $n\to\infty$}. \tag{8.110}\]

That is, if \(h\) is a real-valued bounded continuous function on \(C[0,\infty)\), then

\[E(h(\widetilde{Y}_{n}))\longrightarrow E(h(B))\quad\text{as $n\to\infty$}. \tag{8.111}\]

Also, if \(h\) is a continuous real-valued function on \(C[0,\infty)\), then

\[h(\widetilde{Y}_{n}))\stackrel{{\mathscr{L}}}{{\longrightarrow} }h(B)\quad\text{as $n\to\infty$}. \tag{8.112}\]As an example,

\[\max\{Y_{n}(t);0\leq t\leq 1\}=\max\{\widetilde{Y}_{n}(t);0\leq t\leq 1\}\] \[=\max\{S_{j}/\sqrt{n}:j=0,1,\ldots,n\}\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\max\{B(t):0\leq t\leq 1\}. \tag{8.113}\]

Using a simple symmetric random walk \(S_{n}\) (with \(Z_{j}=+1\) or \(-1\) with probabilities \(1/2\) each), the limiting distribution function on the left can be computed directly (from binomial probabilities), thus determining the distribution of \(\max\{B(t):0\leq t\leq 1\}\). On the other hand, the convergence (8.113) holds for partial sums \(S_{j}\) of an arbitrary mean zero sequence \(Z_{j}\). Hence the limiting distribution for \(\max\{S_{j}/\sqrt{n}:j=0,1,\ldots,n\}\) is determined for arbitrary partial sum processes (up to a scalar factor \(\sigma^{2}\), which can be taken into account simply by standardizing). The second assertion is referred to as the _invariance principle,_ while the convergence (8.110) is called the _functional central limit theorem._

We will apply the functional central limit theorem (FCLT) to derive the Kolmogorov-Smirnov goodness-of-fit test: \(H_{0}:Q=Q_{0}\), based on i.i.d. real-valued observations \(X_{1},\ldots,X_{n}\) from an unknown continuous distribution \(Q\) (on the real line \(\mathbb{R}\)). Here \(Q_{0}\) is a given hypothesized distribution for the observations.

**Theorem 8.5** (Kolmogorov-Smirnov One-Sample Statistic).: _Let \(F_{n}(t)=n^{-1}\ \sum_{1\leq j\leq n}\ \mathbf{1}_{\{X_{j}\leq t\}}\) be the empirical distribution function. Also let \(F\) be the distribution function of \(Q\). If \(F\) is continuous on \(\mathbb{R}\), then_

\[\sqrt{n}\sup\{|F_{n}(t)-F(t)|:t\in\mathbb{R}\}\stackrel{{\mathscr{ L}}}{{\longrightarrow}}\max\{|B(t)-tB(1)|:t\in[0,1]\},\quad\text{as $t\to\infty$.} \tag{8.114}\]

_Here \(B\) is a standard Brownian motion._

Proof.: First, by the CLT for proportions (or sums of Bernoulli random variables), \(\mathbf{1}_{\{X_{j}\leq t\}}\), \(\sqrt{n}(F_{n}(t)-F(t))\stackrel{{\mathscr{L}}}{{\longrightarrow }}N(0,F(t)(1-F(t))\). If \(t_{1}<t_{2},\cdots<t_{k}\), then

\[\sqrt{n}(F_{n}(t_{1})-F(t_{1}),F_{n}(t_{2})-F(t_{2}),\ldots,F_{n}(t_{k})-F(t_{ k}))\stackrel{{\mathscr{L}}}{{\longrightarrow}}N(0,\Gamma(t_{1},t_{2}, \ldots,t_{k})),\]

where the \((i,i)\)-element of the symmetric matrix \(\Gamma\) is \(\operatorname{var}(\mathbf{1}_{\{X_{j}\leq t_{i}\}})=F(t_{i})(1-F(t_{i}))\), and the \((i,j)\)-elements are \(\operatorname{Cov}(\mathbf{1}_{\{X_{j}\leq t_{i}\}},\mathbf{1}_{\{X_{j}\leq t _{j}\}})=F(t_{i})(1-F(t_{j}))\) for \(i<j\). Note that the above equation may be checked by taking a linear combination of the \(k\) components on the left and applying the classical one-dimensional CLT to the summands \(c_{1}\mathbf{1}_{\{X_{j}\leq t_{1}\}}+c_{2}\mathbf{1}_{\{X_{j}\leq t_{2}\}}+ \cdots+c_{k}\mathbf{1}_{\{X_{j}\leq t_{k}\}}\) (\(j=1,\ldots,n\)). By the arguments leading to (8.109), one now has

\[\sqrt{n}(F_{n}(\cdot)-F(\cdot))\stackrel{{\mathscr{L}}}{{ \longrightarrow}}W, \tag{8.115}\]

where \(W\) is a Gaussian process on \(\mathbb{R}\), with \(E(W(t))=0\) for all \(t\), \(\operatorname{var}(W(t))=F(t)(1-F(t))\), and \(\operatorname{Cov}(W(s),W(t))=F(s)(1-F(t))\) for \(s<t\). In particular,

\[\sup\big{\{}\sqrt{n}|F_{n}(t)-F(t)|:t\in\mathbb{R}\big{\}}\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\sup\{|W(t)|:t\in\mathbb{R}\}. \tag{8.116}\]

To simplify (8.116) for computational as well as theoretical purposes, we now consider the random variables \(U_{j}=F(X_{j})\), \(1\leq j\leq n\), and show that

\[U_{j}=F(X_{j}),\ \ 1\leq j\leq n,\ \text{are i.i.d. uniform on}\ [0,1]. \tag{8.117}\]To see this first assume that the continuous function \(F\) is strictly increasing on the range of \(X_{j}\) (i.e., on the smallest interval \((a,b)\) (finite or infinite) such that \(P(a<X_{j}<b))=1\)). Then \(P(U_{j}\leq u)=P(F(X_{j})\leq u)=P(X_{j}\leq F^{-1}(u))=F(F^{-1}(u))=u\), for all \(0<u<1\) and, clearly, \(P(U<0)=0\) and \(P(U>1)=0\). This argument extends to the case where \(F\) may not be strictly increasing on \((a,b)\). For if \(u\) is such that \(F^{-1}(u)\) is an interval \([c,d]\), so that \(F(t)=u\) on \([c,d]\) and \(F(t)<u\) for \(t<c\) and \(F(t)>u\) for \(t>b\), then one has the equality of the sets \(\{F(X_{j})\leq u\}\) and \(\{X_{j}\leq d\}\), so that \(P(F(X_{j})\leq u)=P(X_{j}\leq d)=F(d)=u\).

Applying the same arguments to \(U_{j}\) (\(1\leq j\leq n\)) as we used for \(X_{j}\) (\(1\leq j\leq n\)) above, and writing the empirical distribution function of \(U_{j}\) (\(1\leq j\leq n\)) as \(G_{n}\), it follows that

\[\sqrt{n}\{(G_{n}(u)-u):0\leq u\leq 1\}\stackrel{{\mathscr{L}}}{{ \longrightarrow}}B^{*}, \tag{8.118}\]

where \(B^{*}\) is a Gaussian process on \([0,1]\), with \(E(B^{*}(u))=0\) for all \(0\leq u\leq 1\), and \(\text{Cov}(B^{*}(s),B^{*}(t))=s(1-t)\) for \(0\leq s\leq t\leq 1\). Checking means, variances and covariances, we easily see that the process \(B^{*}\) has the same distribution as the Gaussian process

\[\{B(u)-sB(1):0\leq s\leq 1\}, \tag{8.119}\]

where \(B(\cdot)\) is the _standard Brownian motion._ Hence \(B^{*}\) is identified as the process (8.119) and is known as the _Brownian Bridge._

The analog of (8.116) is then

\[D_{n}\equiv\sup\{\sqrt{n}|G_{n}(u)-u|:u\in[0,1]\}\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\sup\{|B^{*}(u)|:u\in[0,1]\}=D,\ \text{say}. \tag{8.120}\]

Now, in view of (8.117), one has

\[F_{n}(t)=n^{-1}\sum_{1\leq j\leq n}\mathbf{1}_{\{X_{j}\leq t\}}=n^{-1}\sum_{1 \leq j\leq n}\mathbf{1}_{\{F(X_{j})\leq F(t)\}}=n^{-1}\sum_{1\leq j\leq n} \mathbf{1}_{\{U_{j}\leq F(t)\}}=G_{n}(F(t)),\]

so that

\[\sup\{\sqrt{n}|F_{n}(t)-F(t)|:t\in\mathbb{R}\} =\sup\sqrt{n}|G_{n}(F(t))-F(t)|:t\in\mathbb{R}\}\] \[=\sup\{\sqrt{n}|G_{n}(u)-u|:u\in[0,1]\}. \tag{8.121}\]

From (8.120) and (8.121) we arrive at (8.114). 

The distribution of \(D\) on the right side of (8.120) is known, and is given by3

Footnote 3: See Billingsley (1968), p. 85.

\[P(D\leq d)=1-2\sum_{1\leq k<\infty}(-1)^{k+1}\exp\{-2k^{2}d^{2}\},\quad d\in[ 0,\infty). \tag{8.122}\]

The following Corollary is an immediate consequence of the theorem above.

Corollary 8.2 (Kolmogorov-Smirnov Goodness-of-Fit Test): Let \(F_{0}\) be the distribution function of \(Q_{0}\). Consider the Kolmogorov-Smirnov test for \(H_{0}:Q=Q_{0}\) to reject \(H_{0}\) iff

\[D_{n}>d_{1-\alpha} \tag{8.123}\]_where \(D_{n}\) is as given by the left side of (8.114) with \(F=F_{0}\), and \(d_{1-\alpha}\) is the \((1-\alpha)\)th quantile of the distribution (8.122). Then the test has asymptotic level of significance \(\alpha\). _

_Remark 8.8_.: Note that the limiting distribution (8.122) under \(H_{0}\) does not depend on \(F_{0}\).

By similar arguments one may derive the following results (See Serfling 1980, Chap. 6, and van der Vaart 1998, Chaps. 12, 19) for the _Cramer-von Mises statistic_

\[C_{n}=n\int(F_{n}(t)-F(t))^{2}dF(t), \tag{8.124}\]

and the _Anderson-Darling statistic_

\[A_{n}=n\int\frac{(F_{n}(t)-F(t))^{2}}{F(t)(1-F(t))}\,dF(t). \tag{8.125}\]

**Theorem 8.6**.: _Under the hypothesis of Theorem 8.5 the following hold:_

\[C_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}}\int_{0}^{1}B^{ *2}(t)dt,\quad A_{n}\stackrel{{\mathscr{L}}}{{\longrightarrow}} \int_{0}^{1}\frac{B^{*2}(t)}{t(1-t)}\,dt, \tag{8.126}\]

_where \(B^{*}(t)\), \(0\leq t\leq 1\), is the Brownian bridge. _

The limiting distributions in (8.126) do not depend on \(F\) provided \(F\) is continuous. Let \(c_{1-\alpha}\) and \(a_{1-\alpha}\) be the \((1-\alpha)\)-th quantiles of \(\int_{0}^{1}(B^{*}(t))^{2}dt\) and \(\int_{0}^{1}\{(B^{*}(t))^{2}/t(1-t)\}dt\), respectively.

**Corollary 8.3**.: _Let \(F_{0}\) be a continuous distribution function. For the null hypothesis \(H_{0}:F=F_{0}\), (a) the Cramer-von Mises test: Reject \(H_{0}\) iff \(C_{n}>C_{1-\alpha}\) and (b) the Anderson-Darling test: Reject \(H_{0}\) iff \(A_{n}>a_{1-\alpha}\), are of asymptotic level \(\alpha\), where \(C_{n}\) and \(A_{n}\) are computed with \(F=F_{0}\)._

The following expressions for the statistics \(D_{n}\), \(C_{n}\) and \(A_{n}\) facilitate their computation:

\[D_{n} =\sqrt{n}\,\max_{1\leq i\leq n}\max\left\{\frac{i}{n}-U_{(i)},\,U_ {(i)}-\frac{i-1}{n}\right\},\] \[C_{n} =\frac{1}{12n}+\sum_{i=1}^{n}\left(U_{(i)}-\frac{2i-1}{n}\right)^ {2},\] \[A_{n} =-n-\frac{1}{n}\sum_{i=1}^{n}(2i-1)\left\{\log U_{(i)}+\log(1-U_{ (n-i+1)})\right\} \tag{8.127}\]

where \(U_{(i)}=F_{0}(X_{(i)})\), and \(X_{(1)}<X_{(2)}<\cdots<X_{(n)}\) is the ordering of the observations \(X_{1},X_{2},\ldots,X_{n}\) (See Exercise 8.9 for the verification of the expression for \(D_{n}\) in (8.127)). For the expressions for \(C_{n}\) and \(D_{n}\) and much more see D'Agostino and Stephens (1986).

Usually, the models one is required to check for validity are parametric models with unknown parameters, e.g., the Normal model \(N(\mu,\sigma^{2})\) with \(\mu\) and \(\sigma^{2}\) unknown. For such _composite goodness-of-fit_ tests, in the expression \(F(t)=F(t;\theta)\) one replaces the unknown parameter(s) \(\theta\) by a good estimator \(\hat{\theta}_{n}\), which we take to be the MLE (See (8.121), (8.124), (8.125)). This changes the asymptotic distributions, as may be seen from the following expansion around the true parameter value \(\theta_{0}\):

\[\sqrt{n}(F_{n}(t)-F(t;\hat{\theta}_{n}))=\sqrt{n}(F_{n}(t)-F(t;\theta_{0}))\]

\[-\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\frac{\partial F(t;\theta)}{\partial \theta}\bigg{|}_{\theta=\theta_{0}}+o_{p}(1). \tag{8.128}\]

Under regularity conditions, the second term on the right is asymptotically Normal and therefore cannot be ignored. For a detailed discussion of and comprehensive references to the goodness-of-fit literature, see Dasgupta (2008), Chaps. 26-28.

The Cramer-von Mises and Anderson-Darling tests and more recent tests such as the one due to Shabiro and Wilk (1965), mostly outperform the Kolmogorov-Smirnov test since the latter is less sensitive to probabilities at the tail than the other tests.

Finally, note that the above procedures based on the empirical process cannot test the goodness-of-fit of discrete distributions such as the Poisson. Hence, although the frequency chi-square test is not consistent, it provides a reasonable and widely used procedure in this case.

### Nonparametric Inference for the Two-Sample Problem

One of the most important problems in statistics is to decide if two populations, or distributions, are different, based on random samples from them. Does one brand of a certain commodity last longer than another? Has the position of the earth's magnetic poles, say the South Pole, shifted from the Quaternary period (2.59 million years ago) to the modern era? To answer the first type of questions on one-dimensional distributions, in Sect. 8.2 procedures such as those based on ranks were compared to the nonparametric procedure based on the two-sample \(t\)-statistic. The second type of questions concern multivariate distributions for which no natural ordering of data is available. The particular example mentioned here involves observations lying on the unit sphere \(S^{2}\); a surface of dimension two. The analysis based on Frechet means appears in Chap. 12.

In general, a multi-dimensional version of \(t^{2}\), namely a chi-square statistic, may be used to test effectively if two distributions have different mean vectors.

Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) be two independent random samples from two populations with distributions \(Q_{x}\) and \(Q_{y}\) with means \(\mu_{x}\) and \(\mu_{y}\), and finite variances \(\sigma_{x}^{2}\) and \(\sigma_{y}^{2}\). For testing a hypothesis concerning \(\mu_{x}-\mu_{y}\), or estimating it we use the two-sample \(t\)-statistic

\[t=\{\overline{X}-\overline{Y}-(\mu_{x}-\mu_{y})\}/\sqrt{(s_{x}^{ 2}/m+s_{y}^{2}/n)},\ s_{x}^{2}=\sum_{1\leq j\leq m}(X_{j}-\overline{X})^{2}/(m -1),\\ s_{y}^{2}=\sum_{1\leq j\leq n}(Y_{j}-\overline{Y})^{2}/(n-1). \tag{8.129}\]

We will denote by \(t^{\sim}\) the statistic obtained from (8.129) on replacing \(s_{x}^{2}\), \(s_{y}^{2}\) by \(\sigma_{x}^{2}\) and \(\sigma_{y}^{2}\), respectively.

**Proposition 8.2**.: _The statistic \(t\) in (8.129) converges in distribution to \(N(0,1)\), as \(m\to\infty\) and \(n\to\infty\)._

Proof.: Write

\[t^{\sim}=U-V=\sum_{1\leq j\leq m+n}\zeta_{j,m,n}\]

where

\[\zeta_{j,m,n} = (1/m)\{X_{j}-\mu_{x}\}/\sqrt{(\sigma_{x}^{2}/m+\sigma_{y}^{2}/n)} \text{for }1\leq j\leq m,\] \[\zeta_{m+j,m,n} = -(1/n)\{Y_{j}-\mu_{y}\}/\sqrt{(\sigma_{x}^{2}/m+\sigma_{y}^{2}/n) } \text{for }1\leq j\leq n.\]

Let \(\gamma_{j,n}^{2}=E\zeta_{j,m,n}^{2}\), so that \(\sum_{1\leq j\leq m+n}\gamma_{j,n}^{2}=1\). To apply Lindeberg's CLT, we need to show that, for every given \(\varepsilon>0\),

\[\sum_{1\leq j\leq m+n}E\left[\zeta_{j,m,n}^{2}:|\zeta_{j,m,n}|>\varepsilon \right]\longrightarrow 0\quad\text{as }m\to\infty\text{ and }n\to\infty, \tag{8.130}\]

where \(E(Z:A)\) denotes \(E(Z\mathbf{1}_{A})\). Now the left side of (8.130) equals

\[\sum_{1\leq j\leq m}E\left[\left\{\frac{1}{m}(X_{j}-\mu_{x})/ \sqrt{\frac{\sigma_{x}^{2}}{m}+\frac{\sigma_{y}^{2}}{n}}\,\right\}^{2}:\left| \frac{1}{m}(X_{j}-\mu_{x})/\sqrt{\frac{\sigma_{x}^{2}}{m}+\frac{\sigma_{y}^{2} }{n}}\,\right|>\varepsilon\right]\] \[+\sum_{1\leq j\leq n}E\left[\left\{\frac{1}{m}(Y_{j}-\mu_{y})/ \sqrt{\frac{\sigma_{x}^{2}}{m}+\frac{\sigma_{y}^{2}}{n}}\,\right\}^{2}:\left| \frac{1}{n}(Y_{j}-\mu_{y})/\sqrt{\frac{\sigma_{x}^{2}}{m}+\frac{\sigma_{y}^{2} }{n}}\,\right|>\varepsilon\right]\] \[\leq \sum_{1\leq j\leq m}E\left[\left\{\frac{1}{m}(X_{j}-\mu_{x})/ \sqrt{(\sigma_{x}^{2}/m)}\,\right\}^{2}:\left|\frac{1}{m}(X_{j}-\mu_{x})/\sqrt {(\sigma_{x}^{2}/m)}\,\right|>\varepsilon\right]\] \[+\sum_{m+1\leq j\leq m+n}E\left[\left\{\frac{1}{n}(Y_{j}-\mu_{y}) /\sqrt{(\sigma_{y}^{2}/n)}\,\right\}^{2}:\left|\frac{1}{n}(Y_{j}-\mu_{y})/ \sqrt{(\sigma_{y}^{2}/n)}\,\right|>\varepsilon\right]\] \[\leq mE\left[\left\{\frac{1}{m}(X_{1}-\mu_{x})/\sqrt{(\sigma_{x}^{2} /m)}\,\right\}^{2}:\left|\frac{1}{m}(X_{1}-\mu_{x})/\sqrt{(\sigma_{x}^{2}/m)} \,\right|>\varepsilon\right]\] \[+nE\left[\left\{\frac{1}{m}(Y_{1}-\mu_{y})/\sqrt{(\sigma_{x}^{2} /m+\sigma_{y}^{2}/n)}\,\right\}^{2}:\left|\frac{1}{n}(Y_{j}-\mu_{y})/\sqrt{( \sigma_{y}^{2}/n)}\,\right|>\varepsilon\right]\] \[=E\left[\{(X_{1}-\mu_{x}/\sigma_{x})^{2}:|(X_{1}-\mu_{x})/\sigma_ {x}|>\sqrt{m}\,\varepsilon\}\right]\] \[+E\left[\{(Y_{1}-\mu_{y}/\sigma_{y})^{2}:|(Y_{1}-\mu_{y}|>\sqrt{n }\,\varepsilon)\}\right]\] \[=\int_{\{|x|>\sqrt{m}\varepsilon\}}x^{2}Q_{x}^{\sim}(dx)+\int_{ \{|y|>\sqrt{n}\varepsilon\}}y^{2}Q_{y}^{\sim}(dy)\longrightarrow 0\text{ as }m\to\infty\text{ and }n\to\infty,\]

where \(Q_{x}^{\sim}\) is the distribution of \((X_{1}-\mu_{x})/\sigma_{x}\) and \(Q_{y}^{\sim}\) is the distribution of \((Y_{1}-\mu_{y})/\sigma_{y})\). This shows that \(t^{\sim}\xrightarrow{\mathscr{L}}N(0,1)\), as \(m\to\infty\) and \(n\to\infty\). Application of Slutsky's Lemma now shows that \(t\xrightarrow{\mathscr{L}}N(0,1)\), as \(m\to\infty\) and \(n\to\infty\). 

One rejects \(H_{0}:\mu_{x}=\mu_{y}\) if \(|t|>z_{1-\alpha}\) where \(z_{1-\alpha}\) is the \((1-\alpha)\)-th quantile of \(N(0,1)\). One may also reject \(H_{0}:Q_{x}=Q_{y}\) if \(|t|>z_{1-\alpha}\).

We next turn to the case of _the two-sample multi-dimensional problem._ Here \(X_{1},\ldots,X_{m},\) and \(Y_{1},\ldots,Y_{n}\) are independent random samples from two distributions \(Q_{x}\) and \(Q_{y}\) on \(\mathbb{R}^{k}\) having respective means (vectors) \(\mu_{x}\) and \(\mu_{y},\) and finite non-singular covariance matrices \(\sum_{x}=((\sigma_{r,s,x}))_{1\leq r,s\leq k}\) and \(\sum_{y}=((\sigma_{r,s,y}))_{1\leq r,s\leq k}.\) Then one has the following result. We regard all vectors as column vectors, unless specified otherwise. Recall the notation \(\chi^{2}(k)\) for the chi-square distribution with \(k\) degrees of freedom, and \(\chi^{2}_{1-\alpha}(k)\) for the \((1-\alpha)\)th quantile of a \(\chi^{2}_{k}\) distribution.

**Proposition 8.3**.: _As \(m\to\infty\) and \(n\to\infty\), one has_

\[[\overline{X}-\overline{Y}-(\mu_{x}-\mu_{y})]\widehat{\sum}_{m,n}^{-1}[ \overline{X}-\overline{Y}-(\mu_{x}-\mu_{y})]\xrightarrow{\mathscr{L}}\chi^{2} _{k},\]

_where \(\widehat{\sum}_{m,n}=[(1/m)\widehat{\sum}_{x}+(1/n)\widehat{\sum}_{y}]\), and \(\widehat{\sum}_{x}=((\hat{\sigma}_{r,s,x}))_{1\leq r,s\leq k}\) and \(\widehat{\sum}_{y}=((\hat{\sigma}_{r,s,y}))_{1\leq r,s\leq k}\) are the sample covariance matrices with elements_

\[\hat{\sigma}_{r,s,x} =(m-1)^{-1}\sum_{1\leq j\leq m}(X^{(r)}_{j}-\overline{X}^{(r)})( X^{(s)}_{j}-\overline{X}^{(s)}),\] \[X_{j} =(X^{(1)}_{j},\ldots,X^{(k)}_{j})^{\prime},\quad\overline{X}^{(r )}=m^{-1}\sum_{1\leq j\leq m}X^{(r)}_{j};\] \[\hat{\sigma}_{r,s,y} =(n-1)^{-1}\sum_{1\leq j\leq n}(Y^{(r)}_{j}-\overline{Y}^{(r)})( Y^{(s)}_{j}-\overline{Y}^{(s)}),\] \[Y_{j} =(Y^{(1)}_{j},\ldots,Y^{(k)}_{j})^{\prime},\quad\overline{Y}^{(r )}=n^{-1}\sum_{1\leq j\leq n}Y^{(r)}_{j}.\]

Proof.: Note that, if \(Z=(Z^{(1)},\ldots,Z^{(k)})^{\prime}\) is a \(k\)-dimensional standard Normal random vector \(N(0,I_{k}),\) where \(I_{k}\) is the \(k\times k\) identity matrix, then \(|Z|^{2}=(Z^{(1)})^{2}+\cdots+(Z^{(k)})^{2}\) has the chi-square distribution \(\chi^{2}_{k}.\) More generally, if \(Z=(Z^{(1)},\ldots,Z^{(k)})\) is a \(k\)-dimensional Normal distribution \(N(0,\sum),\) where \(\sum=((\sigma_{rs}))\) is a \(k\times k\) positive definite (covariance) matrix, and \(\sum^{-1}=((\sigma^{rs})),\) then

\[Z^{\prime}\sum^{-1}Z=\sum_{1\leq r,s\leq k}\sigma^{rs}Z^{(r)}Z^{(s)}\quad\text {has a $\chi^{2}_{k}$ distribution.}\]

Now, as in the case of \(k=1,\) one has the multidimensional CLT

\[\left(\frac{\sum_{x}}{m}+\frac{\sum_{y}}{n}\right)^{-\frac{1}{2}}[\overline{X }-\overline{Y}-(\mu_{x}-\mu_{y})]\xrightarrow{\mathscr{L}}N(0,I_{k}),\quad \text{as $m\to\infty$ and $n\to\infty.$} \tag{8.131}\]

Here, for a symmetric positive definite matrix \(A,\)\(A^{-1/2}\) is the symmetric positive definite matrix satisfying \(A^{-1/2}A^{-1/2}=A^{-1}.\) To prove (8.131), denote the random vector on the left side of (8.131) by \(W_{n}=(W^{(1)}_{n},\ldots,W^{(k)}_{n})^{\prime}[.\) It is enough to prove that every linear combination \(\sum_{1\leq r\leq k}c_{r}W^{(r)}_{n}\) of the \(k\) coordinates of \(W_{n}\) converges in distribution to the corresponding Normal distribution of \(\sum_{1\leq r\leq k}c_{r}Z^{(r)},\) where \(Z=(Z^{(1)},\ldots,Z^{(k)})\) is Normal \(N(0,I_{k}).\) The last convergence follows from Lindeberg's CLT the same way as in the case of \(k=1.\) Hence

\[W^{\prime}_{n}W_{n}=[\overline{X}-\overline{Y}-(\mu_{x}-\mu_{y})]^{\prime} \left(\frac{1}{m}\Sigma_{x}+\frac{1}{n}\Sigma_{y}\right)^{-1}[\overline{X}- \overline{Y}-(\mu_{x}-\mu_{y})]\xrightarrow{\mathscr{L}}\chi^{2}_{k}. \tag{8.132}\]Finally, applying the general version of Slutsky's Lemma, using the fact that \(\hat{\sigma}_{r,s,x}\to\ \sigma_{r,s,x}\) and \(\hat{\sigma}_{r,s,y}\to\sigma_{r,s,y}\) almost surely for all \(r,s\), as \(m\to\infty\) and \(n\to\infty\), one obtains the desired result. It may be noted that the elements of the inverse \((\sum_{x}/m+\sum_{y}/n)^{-1}\) are continuous functions of the elements of \((\sum_{x}/m+\sum_{y}/n)\) (on the set where this matrix is nonsingular). Thus the Proposition follows from (8.132) and this consistency. 

The above proposition may be used to obtain an ellipsoidal confidence region \(D\) for \(\mu_{x}-\mu_{y}\) of asymptotic level \(1-\alpha\) given by

\[P[\mu_{x}-\mu_{y}\in D=\{c\in\mathbb{R}^{k}:[\overline{X}-\overline{Y}-c]^{ \prime}\hat{\sum}_{m,n}^{-1}[\overline{X}-\overline{Y}-c]\leq\chi_{1-\alpha}^{ 2}(k)]\longrightarrow 1-\alpha,\]

as \(m\to\infty\) and \(n\to\infty\).

A test, of asymptotic size \(\alpha\), rejects \(H_{0}:\mu_{x}=\mu_{y}\) iff the origin \(0\) does not belong to \(D\), i.e., iff

\[[\overline{X}-\overline{Y}]^{\prime}\hat{\sum}_{m,n}^{-1}[\overline{X}- \overline{Y}]>\chi_{1-\alpha}^{2}(k). \tag{8.133}\]

Note that in the case \(k=1\), \(T^{2}=t^{2}\) is an asymptotic chi-square statistic with degrees of freedom \(1\). We used \(t\) there since it can be used to obtain one-sided tests and confidence intervals as well.

Although two different (multivariate) distributions may have the same means, i.e., the test (8.133) is not consistent for testing \(Q_{x}=Q_{y}\), in many high-dimensional problems the test is usually quite effective in discriminating two distributions with different features (See the section on Frechet means in Chap. 12, for example).

We next turn to the two-sample Kolmogorov-Smirnov test, which is consistent. Suppose \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) are real-valued independent random samples from distributions \(Q_{x}\) and \(Q_{y}\), respectively. We wish to test \(H_{0}:Q_{x}=Q_{y}\), (or, equivalently, that the distribution function \(F_{x}\) of \(X_{j}\) equals the distribution function \(F_{y}\) of \(Y_{j}\)).

**Theorem 8.7** (Kolmogorov-Smirnov Two-Sample Statistic).: _Suppose \(F_{x}\) and \(F_{y}\) are continuous, \(m\to\infty\) and \(n\to\infty\), and \(m/(m+n)\to\theta\), \(0<\theta<1\). Let \(F_{m,x}(t)\), \(F_{n,y}(t)\) be the empirical distribution functions of \(X_{j}s\)\((1\leq j\leq m)\) and \(Y_{j}s\)\((1\leq j\leq n)\), respectively. If \(F_{x}=F_{y}\), then_

\[D_{m,n}\!\!\equiv\!\!\left(\frac{mn}{(m+n)}\right)^{\frac{1}{2}}\!\!\sup\{|F_ {m,x}(t)\!\!-\!\!F_{n,y}(t)|\!:\!t\in\mathbb{R}\}\stackrel{{ \mathscr{L}}}{{\longrightarrow}}\sup\{|B^{*}(u)|:u\in[0,1]\}\!=\!\!D, \tag{8.134}\]

_say, where \(B^{*}\) is a Brownian bridge._

Proof.: Assume \(F_{x}=F_{y}=F\), say. By the proof of Theorem 8.5,

\[\sqrt{m}(F_{m,x}(t)-F(t)) =\sqrt{m}(G_{m,x}(F(t))-F(t))\stackrel{{\mathscr{L }}}{{\longrightarrow}}B_{1}^{*}(F(t)),\] \[\sqrt{n}(F_{n,y}(t)-F(t)) =\sqrt{n}(G_{n,y}(F(t))-F(t))\stackrel{{\mathscr{L }}}{{\longrightarrow}}B_{2}^{*}(F(t)),\ (t\in\mathbb{R}), \tag{8.135}\]

where (i) \(F_{m,x}(t)=m^{-1}\sum_{1\leq j\leq m}\mathbf{1}\{U_{j,x}\leq F(t)\}\), \(U_{j,x}=F(X_{j})\)\((1\leq j\leq m)\) are independent uniform on \([0,1]\), (ii) \(F_{n,y}(t)=n^{-1}\sum_{1\leq j\leq n}\mathbf{1}\{U_{j,y}\leq F(t)\}\), \(U_{j,y}=F(Y_{j})\)\((1\leq j\leq n)\) are independent uniform on \([0,1]\), (iii) \(B_{1}^{*}\) and \(B_{2}^{*}\) are independent Brownian bridges.

Hence

\[\left(\frac{mn}{(m+n)}\right)^{\frac{1}{2}}((F_{m,x}(t))-F_{n,y}(t))\] \[=\left(\frac{n}{(m+n)}\right)^{\frac{1}{2}}m^{\frac{1}{2}}(F_{m,x}( t))-F(t))-\left(\frac{m}{(m+n)}\right)^{\frac{1}{2}}n^{\frac{1}{2}}(F_{n,y}(t))-F(t))\] \[\stackrel{{\mathscr{L}}}{{\longrightarrow}}\theta B _{1}^{*}(F(t))-(1-\theta)B_{2}^{*}(F(t))=B^{*}(F(t)),\ \text{say}, \tag{8.136}\]

where, by checking means and covariances, it is seen that \(B^{*}=\theta B_{1}^{*}-(1-\theta)B_{2}^{*}\) is a Brownian bridge. Arguing as above, this says

\[\left(\frac{mn}{(m+n)}\right)^{\frac{1}{2}}((F_{m,x}(\cdot)-F_{n,y}(\cdot)) \stackrel{{\mathscr{L}}}{{\longrightarrow}}B^{*}(F(\cdot)), \tag{8.137}\]

in the sense of convergence in distribution of the stochastic process on \([0,\infty)\) on the left to the one on the right. Therefore, writing \(u=F(t)\), one has

\[D_{m,n} \equiv \sup\left\{\left|\left(\frac{mn}{(m+n)}\right)^{\frac{1}{2}}((F_ {m,x}(t))-F_{n,y}(t))\right|:t\in\mathbb{R}\right\} \tag{8.138}\] \[\stackrel{{\mathscr{L}}}{{\longrightarrow}}\sup\{|B ^{*}(F(t))|:t\in\mathbb{R}\}\] \[= \sup\{|B^{*}(u)|:u\in[0,1]\}=D,\quad\text{say}.\]

**Corollary 8.4**.: _Suppose \(F_{x}\) and \(F_{y}\) are continuous, and \(m\) and \(n\to\infty\) as specified in Theorem 8.7. Then a test of asymptotic level of significance \(\alpha\) for \(H_{0}:F_{x}=F_{y}\) is given by_

\[\text{Reject }H_{0}\text{ iff }D_{m,n}>d_{1-\alpha},\]

_where \(d_{1-\alpha}\), is the \((1-\alpha)th\) quantile of the distribution of \(D\)._

_Remark 8.9_.: Note that (8.138) provides a nonparametric test for the equality of two continuous distributions.

_Remark 8.10_.: Since the distribution function of \(D\) in Theorem 8.7 is continuous, one can dispense with the assumption in this theorem and in Corollary 8.4 that \(m/(m+n)\to\theta\in(0,1)\), and simply require \(m\to\infty\) and \(n\to\infty\).

We conclude this section with a description of a simple method for multiple testing known as the _Bonferroni method_.

To illustrate the Bonferroni method, consider the testing of the equality of two multivariate means \(\mu_{x}=(\mu_{x}^{(1)},\ldots,\mu_{x}^{(k)})^{\prime}\) and \(\mu_{y}=(\mu_{y}^{(1)},\ldots,\mu_{y}^{(k)})^{\prime}\) of two populations based on random samples taken from them. Suppose the level of significance for the test of the null hypothesis \(H_{0}:\mu_{x}=\mu_{y}\) is set at \(\alpha\). One may try to test the component null hypotheses \(\mu_{x}^{(i)}=\mu_{y}^{(i)}\) (\(i=1,\ldots,k\)) by \(k\) separate tests. Then the _Bonferroni principle says that \(H_{0}\) is to be rejected if at least one of the null hypotheses \(H_{0}^{(i)}:\mu_{x}^{(i)}=\mu_{y}^{(i)}\), is rejected at a level of significance \(\alpha/k\)_(\(i=1,\ldots,k\))_. To see that under this procedure the level of significance for \(H_{0}\) is no more than \(\alpha\), let \(R^{(i)}\) be the event that the null hypothesis \(H_{0}^{(i)}\) is rejected, and suppose that its level of significance is set at \(\alpha^{(i)}\). Then the probability (under \(H_{0}\)) that \(H_{0}\) is rejected equals \(P_{H_{0}}\) (at least one \(R^{(i)}\) occurs among \(i=1,\ldots,k)\leq\sum_{1\leq i\leq k}P_{H_{0}}(R^{(i)})=\sum_{1\leq i\leq k}\alpha^{(i )}=\alpha\), if \(\alpha^{(i)}=\alpha/k\) for each \(i\). Thus the Bonferroni principle is _conservative_ in that the actual level of significance is less than or equal to \(\alpha\) under this procedure. As an illustration, if \(k=3\) and \(\alpha=0.05\), then the Bonferroni procedure is to test each of the three component hypotheses at a level of significance \((0.05)/3=0.01666\ldots\), and reject \(H_{0}\) if at least one of the component hypotheses is rejected at level \(0.01666\ldots\).

For estimating the \(p\)-value using the Bonferroni principle, let \(p\)-min denote the smallest \(p\)-value of the \(k\) tests (for \(H_{0}^{(i)}\), \(i=1,\ldots,k\)). Then the \(p\)_-value for \(H_{0}\) is no more than \(k\) (\(p\)-min). That is, \(p\)-value of \(H_{0}\leq k(p\)-min)._ For suppose one sets \(\alpha=k\) (\(p-\min\)), and applies the procedure described in the preceding paragraph, namely, to reject \(H_{0}\) if at least one of the tests is rejected at a level of significance \(\alpha/k\); then the test with the smallest \(p\)-value (\(=p\)-min) meets that rejection criterion.

There are modifications/corrections of the Bonferroni principle which provide sharper estimates of the actual \(p\)-value. But we will consider those in Chap. 13.

### Large Sample Theory for Stochastic Processes

This section provides an introduction to semiparametric inference for an important class of time series models, and to an extension of the asymptotic properties of the maximum likelihood estimators for stationary ergodic processes. Because the linear time series here may be viewed as coordinates of Markov processes, we begin with a brief review of ergodic Markov processes.

Recall that a _Markov process_\(X_{n}\) in discrete time \(n\) (\(n=0,1\ldots\)) is a sequence of random variables defined on some probability space \((\Omega,\mathscr{F},P)\) taking values on a (measurable) _state space_\(S\) (with a sigma-field \(\mathscr{S}\)) and governed by a _transition probability function_\(p(x,B)\), which is the probability that the process moves to a set \(B\) in one step (or one unit of time), starting from a state \(x\) in \(S\). The _Markov property_ is the following:

\[p(X_{n},B)=P(X_{n+1}\in B\mid X_{0},\ldots,X_{n})=P(X_{n+1}\in B\mid X_{n}) \quad(\forall\,B\in\mathscr{S},\text{ and }\forall\,n). \tag{8.139}\]

_Thus the conditional distribution of the "future" \(X_{n+1}\) given the "past" \(X_{0},\ldots,X_{n-1}\) and "present" \(X_{n}\) depends only on the "present" state \(X_{n}\)._ This may also be expressed as

\[E[f(X_{n+1}) \mid X_{0},\ldots,X_{n}]=E[f(X_{n+1})\mid X_{n}]\] \[=\int f(y)p(X_{n},dy)\ \forall\text{ bounded measurable function }f\text{ on }S.\]

Here \(p(x,dy)\) denotes the _distribution of \(X_{n+1}\), given \(X_{n}=x\)_. One may check by iteration that the Markov property (8.139) or (8.140) implies (and is equivalent to) the more general property

\[P(X_{n+k}\in B\mid X_{0},\ldots,X_{n})=P(X_{n+k}\in B\mid X_{n})\quad\forall \ k=1,2,\ldots,\text{and }\forall\ n. \tag{8.141}\]The probability on the right provides the _\(k\)-step transition probability_\(p^{(k)}(x,B)\), which may be iteratively obtained from the (one-step) transition probability \(p(x,dy)\). For example,

\[p^{(2)}(x,B) =P(X_{n+2}\in B\mid X_{n}=x)=E[P(X_{n+2}\in B\mid X_{n},X_{n+1}) \mid X_{n})]_{X_{n}=x}=\] \[=E[P(X_{n+2}\in B\mid X_{n+1})\mid X_{n}=x]\] \[=E[p(X_{n+1},B)\mid X_{n}=x]=\int p(y,B)p(x,dy). \tag{8.142}\]

The second equality is a property of the conditional expectation, while the Markov property is used for the third equality. One may express \(p^{(k)}(x,B)\) similarly as

\[p^{(k)}(x,B)=\int p^{(k-1)}(y,B)p(x,dy). \tag{8.143}\]

Also, the Markov property implies that, for every (measurable) subset \(C\) of \(S^{k+1}\).

\[P[(X_{n},X_{n+1},\ldots,X_{n+k})\in C\mid X_{n}=x]=P[(X_{0},X_{1},\ldots,X_{k} )\in C\mid X_{0}=x]\quad\forall\ n. \tag{8.144}\]

That is, the conditional distribution of \((X_{n},X_{n+1},\ldots,X_{n+k})\), given \(X_{n}=x\), does not depend on \(n\).

Remark Remark 8.11: In the case \(p(x,dy)\) has, for every \(x\), a density \(p(x,y)\) with respect to some sigma-finite measure \(\mu\) one may express the Markov property by simply writing down the joint probability density of \((X_{n+1},X_{n+2},\ldots,X_{n+k})\) at a point \((y_{1},y_{2},\ldots,y_{k})\), given \((X_{0},X_{1},\ldots,X_{n})\), as \(p(x,y_{1})p(y_{1},y_{2})\ldots p(y_{k-1},y_{k})\) on \(\{X_{n}=x\}\).

Definition 8.3: A probability measure \(\pi\) on \(S\) is said to be an _invariant probability,_ or a _steady state distribution,_ for a Markov process with transition probability \(p(x,dy)\) if

1. \(\int p(y,B)\pi(dy)=\pi(B)\quad\forall B\in\mathscr{S}\), or (equivalently),
2. \(\int\limits_{\mbox{on }}Tf(y)\pi(dy)\equiv\int f(z)p(y,dz)\pi(dy)=\int f(z)\pi( dz)\,\forall\) bounded measurable \(f\) on \(S\).

Note that in the case \(p(x,dy)\) has a density \(p(x,y)\), an _invariant probability density_\(\pi(y)\) satisfies

\[\int p(x,y)\pi(x)\mu(dx)=\pi(y)\quad\mbox{for all $y$ (outside a set of $\mu$-measure zero).}\]

In Definition 8.3 (ii), we have used the notation \(Tf\) to denote the function \(Tf(y)=\int f(z)p(y,dz)\). Note that the left side of Definition 8.3 (i) says that \(X_{1}\) has the same distribution as \(X_{0}\)_if \(X_{0}\) has distribution \(\pi\)_. Similarly, the left side of Definition 8.3 (ii) equals \(\int[Ef(X_{1}\mid X_{0}=y)]\pi(dy)\) which is \(Ef(X_{1})\) if \(X_{0}\) has distribution \(\pi\), while the right side is \(Ef(X_{0}))\). By the same argument if \(X_{1}\) has distribution \(\pi\), then \(X_{2}\) has distribution \(\pi\), and so on, implying that \(X_{n}\) has distribution \(\pi\) for all \(n\), if \(X_{0}\) has distribution \(\pi\). Indeed more is true. If \(X_{0}\) has distribution \(\pi\), then the distribution of \((X_{0},X_{1},\ldots,X_{k})\) is the same as the distribution of \((X_{n},X_{n+1},\dots,X_{n+k})\)\(\forall\,n\). This follows from (8.144) and the fact that \(X_{n}\) has the same distribution as \(X_{0}\), namely, \(\pi\). Hence, if \(X_{0}\) has distribution \(\pi\), then the _after-\(n\) process_\(X_{n}^{+}=(X_{n},X_{n+1},\dots,X_{n+k},\dots)\) has the same distribution as \(X_{0}^{+}=(X_{0},X_{1},\dots,X_{k},\dots)\). This last property is referred to as the _stationarity of the process_\(\{X_{n}:n=0,1,\dots\}\).

_Example 8.5 ((AR(1) Model)._ Let a stochastic process \(X_{n}\) (\(n=0,1,\dots\)) be defined on the state space \(S=R\) by the recursion

\[X_{n+1}=g(X_{n})+\varepsilon_{n+1}\quad(n=0,1,\dots),\text{ and }X_{0}\text{ is given,} \tag{8.145}\]

where \(g\) is a given (measurable) function on \(\mathbb{R}\), and \(\varepsilon_{n}\) (\(n=1,2,\dots\)) is an i.i.d. sequence. Assume \(X_{0}\) is independent of \(\{\varepsilon_{n}:n=1,2,\dots\}\). Then \(\{X_{n}:n=0,1,\dots\}\) is a Markov process with transition probability function given by

\[p(x,B)=P(X_{1}\in B\mid X_{0}=x)=P(g(x)+\varepsilon_{1}\in B)=P(\varepsilon_{1 }\in B-g(x)),\]

where \(B-g(x)=\{y-g(x):y\in B\}\). A special case of (8.145) is the _autoregressive model of order 1, or AR(1) model,_

\[X_{n+1}=\alpha+\beta X_{n}+\varepsilon_{n+1}\quad(n=0,1,\dots),\quad X_{0} \text{ is independent of }\{\varepsilon_{n}:n=1,2,\dots\}, \tag{8.146}\]

where \(\{\varepsilon_{n}:n=1,2,\dots\}\) is an i.i.d. sequence satisfying

\[E\varepsilon_{n}=0,\quad 0<E\varepsilon_{n}^{2}=\sigma^{2}<\infty. \tag{8.147}\]

We now show that this Markov process has a unique steady state or invariant probability if the following _stability condition_ is satisfied:

\[|\beta|<1. \tag{8.148}\]

To prove the existence of a unique invariant probability, we now demonstrate that

\[X_{n}\xrightarrow{\mathscr{L}}\frac{\alpha}{(1-\beta)}+Z,\quad\text{where }Z= \sum_{0\leq j<\infty}\beta^{j}\varepsilon_{j+1}. \tag{8.149}\]

To see this, use (8.146) to get successively,

\[X_{1} =\alpha+\beta X_{0}+\varepsilon_{1},\,\,\,X_{2}=\alpha+\beta X_{1 }+\varepsilon_{2}=\alpha+\beta\alpha+\beta^{2}X_{0}+\beta\varepsilon_{1}+ \varepsilon_{2},\] \[X_{3} =\alpha+\beta X_{2}+\varepsilon_{3}=\alpha+\beta\alpha+\beta^{2 }\alpha+\beta^{3}X_{0}+\beta^{2}\varepsilon_{1}+\beta\varepsilon_{2}+ \varepsilon_{3},\dots\] \[X_{j} =\alpha+\beta\alpha+\beta^{2}\alpha+\dots+\beta^{j-1}\alpha+ \beta^{j}X_{0}+\beta^{j-1}\varepsilon_{1}+\beta^{j-2}\varepsilon_{2}+\dots+ \beta\varepsilon_{j-1}+\varepsilon_{j},\] \[\stackrel{{\mathscr{L}}}{{=}}\alpha+\beta\alpha+ \beta^{2}\alpha+\dots+\beta^{j-1}\alpha+\beta^{j}X_{0}+\sum_{0\leq r\leq j-1} \beta^{r}\varepsilon_{r+1}. \tag{8.150}\]

The last equality in distribution follows from the fact \(\{\varepsilon_{n}:n=1,2,\dots\}\) is an i.i.d. sequence (independent of \(X_{0}\)). Now (8.149) follows from (8.150), since \(\{\varepsilon_{n}:n=1,2,\dots\}\) is an i.i.d. sequence. We now appeal to the following general result.

**Proposition 8.4**.: _Let \(\{X_{n}:n=0,1,\dots\}\) be a Markov process on a metric space \(S\) with a transition probability \(p(x,dy)\) such that \(X_{n}\) converges in distribution to the same probability \(\pi\), as \(n\to\infty\), whatever be the initial sate \(X_{0}\). Assume also that \(x\to p(x,dy)\) is weakly continuous, i.e., the function \(Tf(x)=\int f(y)p(x,dy)\) is continuous for every bounded continuous \(f\). Then \(\pi\) is the unique invariant probability for the Markov process._

Proof.: Whatever the initial state \(X_{0}\), one has

\[Ef(X_{n+1})=E[E(f(X_{n+1})\mid X_{n})]=ETf(X_{n})\longrightarrow\int Tf(y)\pi( dy)\,\,\,\text{as}\,\,n\to\infty. \tag{8.151}\]

This follows from the definition of convergence in distribution, since \(X_{n}\) converges to \(\pi\) in distribution and \(Tf\) is a bounded continuous function. On the other hand, \(X_{n+1}\) converges in distribution to \(\pi\), so that the left side of (8.151) converges to \(\int f(y)\pi(dy)\). We, therefore, have

\[\int Tf(y)\pi(dy)=\int f(y)\pi(dy)\quad\forall\,\,\text{bounded continuous}\,\,f.\]

From this the criterion (ii) in Definition 8.3 follows (for all bounded measurable \(f\)). For (a) the set of all bounded continuous functions is dense in \(L^{1}(\pi)\)--the set of all (equivalence classes of) functions on \(\mathbb{R}\) integrable with respect to \(\pi\), which includes all bounded measurable functions,4 and (b) \(f\to Tf\) is a contraction on \(L^{1}(\pi)\), i.e., if \(f\), \(g\) are in \(L^{1}(\pi)\) then by the invariance of \(\pi\),

Footnote 4: See, e.g., Bhattacharya and Waymire (2007), p. 180.

\[\|Tf-Tg\|_{1} \equiv\int|Tf(y)-Tg(y)|\pi(dy)=E|E(f(X_{1})-g(X_{1})\mid X_{0})|\] \[\leq E\,E(|f(X_{1})-g(X_{1})|\mid X_{0}))\] \[=E|f(X_{1})-g(X_{1})|=\int|f(y)-g(y)|\pi(dy)\equiv\|f-g\|_{1}.\]

To prove _uniqueness_ of the invariant probability \(\pi\), let \(\gamma\) be any invariant probability of the Markov process, which implies that if \(X_{0}\) has distribution \(\gamma\) then \(X_{n}\) has distribution \(\gamma\) for all \(n\). But, no matter what \(X_{0}\) is, \(X_{n}\) converges in distribution to \(\pi\) as \(n\to\infty\). This implies that \(\gamma\) cannot be anything other than \(\pi\). 

**Corollary 8.5**.: _Under the hypotheses (8.147) and (8.148), the Markov process \(X_{n}\) in (8.146) has the unique invariant probability given by the distribution \(\pi\) of the random variable \(\alpha/(1-\beta)+Z\), where \(Z=\sum_{0\leq j<\infty}\beta^{j}\varepsilon_{j+1}\). The mean of \(\pi\) is \(\mu=\alpha/(1-\beta)\), and the variance of \(\pi\) is \(\delta=\sigma^{2}/(1-\beta^{2})\)._

For purposes of inference the following simple calculations are needed (see (8.150)).

\[E(X_{n}) =\alpha+\beta\alpha+\beta^{2}\alpha+\cdots+\beta^{n-1}\alpha+ \beta^{n}E(X_{0})\] \[=\beta^{n}E(X_{0})+\alpha(1-\beta^{n})/(1-\beta);\] \[\text{var}(X_{n}) =\text{var}(\alpha+\beta\alpha+\beta^{2}\alpha+\cdots+\beta^{n-1 }\alpha+\beta^{n}X_{0}+\beta^{n-1}\varepsilon_{1}+\beta^{n-2}\varepsilon_{2}\] \[\quad+\cdots+\beta\varepsilon_{n-1}+\varepsilon_{n})=\] \[=\beta^{2n}\text{var}(X_{0})+(\beta^{2(n-1)}+\beta^{2(n-2)}+ \cdots+\beta^{2}+1)\sigma^{2}\] \[=\beta^{2n}\text{var}(X_{0})+\sigma^{2}(1-\beta^{2n})/(1-\beta^{ 2});\]\[\begin{split}\mathrm{Cov}(X_{n},X_{n+j})&=\mathrm{Cov}(X_ {n},\alpha+\beta\alpha+\beta^{2}\alpha+\cdots+\beta^{j-1}\alpha+\beta^{j}X_{n}+ \beta^{j-1}\varepsilon_{n+1}\\ &\quad+\beta^{j-2}\varepsilon_{n+2}\cdots+\beta\varepsilon_{n+j-1} +\varepsilon_{n+j})\\ &=\beta^{j}\mathrm{var}(X_{n})=\beta^{j}\{\beta^{2n}\mathrm{var}(X _{0})+\sigma^{2}(1-\beta^{2n})/(1-\beta^{2}\}.\end{split} \tag{8.152}\]

We now turn to the problem of estimating \(\alpha\) and \(\beta\) from observations \(X_{0},X_{1},\ldots,X_{n}\). The least squares estimates are given, as usual, by

\[\operatorname*{arg\,min}_{\alpha,\beta}\sum_{1\leq j\leq n}[X_{j}-(\alpha+ \beta X_{j-1})]^{2}, \tag{8.153}\]

whose solution is (See (6.14), (6.16))

\[\hat{\beta} =\left[\sum_{1\leq j\leq n}X_{j}(X_{j-1}-\overline{X})\right] \bigg{/}\left[\sum_{1\leq j\leq n}(X_{j-1}-\overline{X})^{2}\right],\quad\hat{ \alpha}=\overline{Y}-\hat{\beta}\overline{X},\ \mathrm{where}\] \[\overline{X} =\sum_{1\leq j\leq n}X_{j-1}/n,\ \mathrm{and}\ \ \overline{Y}=\sum_{1\leq j\leq n}X_{j}/n. \tag{8.154}\]

**Theorem 8.8**.: _For the AR(1) model (8.146) assume in addition to (8.147) and (8.148) that either (a) \(X_{0}\) has the invariant distribution, or (b) \(E\varepsilon_{j}^{4}<\infty\) and \(E(X_{0})^{4}<\infty\). Then_

\[\sqrt{n}(\hat{\beta}-\beta)\xrightarrow{\mathscr{L}}N(0,1-\beta^{2}), \tag{8.155}\]

_and_

\[\sqrt{n}(\hat{\alpha}-\alpha,\hat{\beta}-\beta)^{\prime}\xrightarrow{\mathscr{ L}}N((0,0)^{\prime},\varGamma), \tag{8.156}\]

_where_

\[\varGamma=\quad\left[\sigma^{2}+\alpha^{2}(1+\beta)/(1-\beta)\ -\alpha\sigma^{2}(1+\beta)\right] \tag{8.157}\]

Proof.: We will first prove (8.155). By (8.154),

\[\sqrt{n}(\hat{\beta}-\beta)=\left[n^{-\frac{1}{2}}\sum_{1\leq j\leq n}(X_{j-1} -\overline{X})\varepsilon_{j}\right]\bigg{/}\left[n^{-1}\sum_{1\leq j\leq n}(X_{j-1}- \overline{X})^{2}\right]. \tag{8.158}\]

We will first show that the denominator on the right converges in probability to the variance of the invariant distribution \(\pi\),

\[\left[n^{-1}\sum_{1\leq j\leq n}(X_{j-1}-\overline{X})^{2}\right]\longrightarrow \delta=\sigma^{2}/(1-\beta^{2})\quad\text{in probability, as $n\to\infty$}. \tag{8.159}\]

First, writing \(\mu=\alpha/(1-\beta)\), one has (See (8.152)),

\[\begin{split} E\overline{X}&=n^{-1}\sum_{0\leq j \leq n-1}\{\beta^{j}EX_{0}+\alpha(1-\beta^{j})/(1-\beta)\}\\ &=\mu+\{n^{-1}(1-\beta^{n})/(1-\beta)\}EX_{0}-n^{-1}\alpha(1- \beta^{n})/(1-\beta)^{2}\\ &=\mu+O(1/n),\quad E\overline{X}-\mu=O(1/n);\end{split}\]

[MISSING_PAGE_FAIL:242]

If \(X_{0}\) has the invariant distribution, the process \(\{(X_{n},\,\varepsilon_{n}):n=0,\dots\}\) is stationary, and \((X_{j-1}-\mu)\varepsilon_{j}\) are stationary ergodic martingale differences. In this case the martingale CLT applies and (8.155) is proved. Otherwise we may check the Lindeberg-type conditions for the _martingale CLT_ to hold.5 First,

Footnote 5: See, e.g., Bhattacharya and Waymire (2009), pp. 507511.

\[\sum_{1\leq j\leq n}E\left[(n^{-\frac{1}{2}}(X_{j-1}-\mu)\varepsilon _{j})^{2}\mid\mathscr{F}_{j-1}\right]=n^{-1}\sum_{1\leq j\leq n}(X_{j-1}-\mu)^{ 2}E(\varepsilon_{j}^{2}\mid\mathscr{F}_{j-1})=\] \[=\sigma^{2}n^{-1}\sum_{1\leq j\leq n}(X_{j-1}-\mu)^{2}\longrightarrow \sigma^{2}\delta\,\text{ in probability as }n\to\infty, \tag{8.162}\]

using (8.159) in the last step. It remains to show that, for every \(\theta>0\),

\[\sum_{1\leq j\leq n}\hskip-14.226378ptE\left[(n^{-\frac{1}{2}}(X_{j-1}-\mu) \varepsilon_{j})^{2}\mathbf{1}\{|n^{-\frac{1}{2}}(X_{j-1}-\mu)\varepsilon_{j} |>\theta\}\mid\mathscr{F}_{j-1}\right]\to 0\text{ in probability.} \tag{8.163}\]

By Chebyshev's inequality, the left side of (8.163) is bounded above by

\[n^{-1}\hskip-14.226378pt\sum_{1\leq j\leq n}(E[(((X_{j-1}-\mu) \varepsilon_{j})^{2})^{2}\mid\mathscr{F}_{j-1}])^{\frac{1}{2}}\] \[\qquad\cdot E[(\mathbf{1}\{|n^{-\frac{1}{2}}(X_{j-1}-\mu) \varepsilon_{j}\}|>\theta\}\mid\mathscr{F}_{j-1})]^{\frac{1}{2}}\] \[\leq n^{-1}\sum_{1\leq j\leq n}\{(X_{j-1}-\mu)^{4}\}^{\frac{1}{2} }\gamma\cdot\{E[(X_{j-1}-\mu)^{2}\varepsilon_{j}^{2}\mid\mathscr{F}_{j-1}]\}^ {\frac{1}{2}}/(\theta\sqrt{n})\] \[\qquad\leq n^{-1}\sum_{1\leq j\leq n}(X_{j-1}-\mu)^{2}\gamma \sigma|X_{j-1}-\mu|/(\theta\sqrt{n})\] \[\qquad=\left(n^{-1}\sum_{1\leq j\leq n}|X_{j-1}-\mu|^{3}\right) \gamma\sigma/(\theta\sqrt{n})\] \[\qquad\leq\left\{n^{-1}\sum_{1\leq j\leq n}(X_{j-1}-\mu)^{4} \right\}^{\frac{3}{4}}\gamma\sigma/(\theta\sqrt{n})\quad[\gamma:=(E\varepsilon _{j}^{4})^{\frac{1}{2}}]. \tag{8.164}\]

By calculations using the expression for \(X_{j}\) in (8.150) (Exercise 8.14), one has

\[\overline{\lim_{n}}\,E\left(n^{-1}\sum_{1\leq j\leq n}(X_{j-1}-\mu)^{4}\right) <\infty. \tag{8.165}\]

Now (8.163) follows from (8.164), (8.165). This proves (8.155). The proof of (8.156) may be given by expressing an arbitrary linear combination of \(\sqrt{n}(\hat{\alpha}-\alpha)\) and \(\sqrt{n}(\hat{\beta}-\beta)\) as a martingale (plus a negligible term), and appealing to the martingale central limit theorem, noting that \(\sqrt{n}(\hat{\alpha}-\alpha)=n^{-\frac{1}{2}}\sum_{1\leq j\leq n}\varepsilon_ {j}+\sqrt{n}(\hat{\beta}-\beta)\mu+o_{p}(1)\) (Exercise 8.14). 

Under the assumptions of Theorem 8.8, one can obtain CLT-based classical confidence intervals and tests for the parameters, using sample estimates of the dispersions (see (8.154), (8.155), (8.159)) (Exercise 8.15). Bootstrap confidence intervals and tests for the parameters may be derived by resampling from the estimated residuals

\[\{\hat{\varepsilon}_{j}:=X_{j}-\hat{\alpha}-\hat{\beta}X_{j-1}:j=1,\ldots,n\}.\]

See Bose (1988) and Lahiri (2003).

_Example 8.6 (\(Ar(p)\) Model)._ We next consider the more general _autoregressive model of order \(p\geq 1\)_, or the _AR(p) model_. Here the sequence of real-valued random variables \(\{X_{n}:n=0,1,2,\dots\}\) are recursively defined by

\[X_{n}=\alpha+\sum_{1\leq r\leq p}\beta_{r}X_{n-r}+\varepsilon_{n}\quad(n=p,p+1,\dots),\quad(\beta_{p}\neq 0), \tag{8.166}\]

given the _initial set of \(p\) values_\((X_{0},\dots,X_{p-1})\). The sequence \(\{\varepsilon_{n}:n=p,p+1,\dots\}\) is i.i.d. and independent of \((X_{0},\dots,X_{p-1})\), and satisfy (8.147). Since \(X_{n}\) depends on the past \(p\) values in the sequence, in addition to the random error \(\varepsilon_{n}\), it is not Markov. However, the vector sequence \(\{Y_{n}=(X_{n-p+1},X_{n-p+2},\dots,X_{n})^{\prime}:n=p-1,p,\dots\}\) is Markov and satisfies the recursion

\[Y_{n}=\alpha^{\sim}+BY_{n-1}+\varepsilon_{n}^{\sim}\quad(n=p,p+1,\dots), \tag{8.167}\]

where the \(p\)-dimensional vectors \(\alpha^{\sim}\) and \(\varepsilon_{n}^{\sim}\) and the \(p\times p\) matrix \(B\) are defined as \(\alpha^{\sim}=(0,0,\dots,0,\alpha)^{\prime}\), \(\varepsilon_{n}^{\sim}=(0,0,\dots,0,\varepsilon_{n})^{\prime}\)\((n=p,p+1,\dots)\),

\[B=\begin{bmatrix}0&1&0&0&0&0&0\\ 0&0&1&0&0&0&0\\ \dots&\dots&\dots&\dots&\dots&\dots&\dots\\ 0&0&0&&&0&1\\ \beta_{p}&\beta_{p-1}&&&\dots&\beta_{2}&\beta_{1}\end{bmatrix} \tag{8.168}\]

Note that the initial value \(Y_{p-1}=(X_{0},X_{1},\dots,X_{p-1})^{\prime}\) is independent of the i.i.d. sequence \(\{\varepsilon_{n}^{\sim}:n=p,p+1,\dots\}\). The state space of the Markov process \(\{Y_{n}:n=p,p+1,\dots\}\) is \(S=\mathbb{R}^{p}\). A convenient way of expressing (8.167), for purposes of the analysis that follows, is

\[Y_{n}=BY_{n-1}+\zeta_{n},\quad\zeta_{n}:=\alpha^{\sim}+\varepsilon_{n}^{\sim} \quad(n=p,p+1,\dots), \tag{8.169}\]

The random vectors \(\zeta_{n}\) are still i.i.d. and independent of \(Y_{p-1}\), although their mean vector is \(\alpha^{\sim}\) which is not zero. For stability, i.e., for convergence of the Markov process to a unique invariant probability irrespective of the initial state, a necessary and sufficient condition turns out to be

\[\text{Maximum modulus of the eigenvalues of $B$ is less than $1$}. \tag{8.170}\]

Expanding the determinant \(\det(B-\lambda I_{p})\) by its last row, one gets the polynomial (in \(\lambda\)) \(\det(B-\lambda I_{p})=(-1)^{p}\{\lambda^{p}-\beta_{1}\lambda^{p-1}-\beta_{2} \lambda^{p-2}-\dots-\beta_{p}\}\). Thus (8.170) says that all its roots lie in the interior of the unit circle of the complex plane. One can show that (8.170) is equivalent to (see, e.g., Bhattacharya and Waymire 2009, pp. 168-172):

There exists a positive integer \(m\) such that \(\|B^{m}\|<1\). (8.171)The relations (8.170) and (8.171) are called _stability conditions_. Successive iterations of (8.169) lead to

\[Y_{n}=B^{n-p+1}Y_{p-1}+\sum_{p\leq r\leq n}B^{n-r}\zeta_{r}. \tag{8.172}\]

Using and proceeding as the case AR(1), one arrives at (Exercise 8.16).

**Proposition 8.5**.: _Assume, as before, that the i.i.d. \(\varepsilon_{n}\) satisfy (8.147) and are independent of \(Y_{p-1}\), and that (8.170) (or, equivalently, (8.171) holds. Then the Markov process converges in distribution to a unique invariant distribution \(\pi\), irrespective of its initial state \(Y_{p-1}\), given by the distribution of_

\[\mathbf{Z}=\sum_{0\leq n<\infty}B^{n}\zeta_{n}=b^{\sim}+\sum_{0\leq n<\infty}B^ {n}\varepsilon_{n}^{\sim}, \tag{8.173}\]

_where all the elements of \(b^{\sim}\) are \(\alpha(1-\sum_{r}\beta_{r})^{-1}\). Each coordinate of \(E\mathbf{Z}\) equals \(\alpha(1-\sum\beta_{r})^{-1}\), while the covariance matrix of \(\mathbf{Z}\) is \(\sigma^{2}V\), where \(V=((\sum_{0\leq n<\infty}b_{ip}^{(n)}b_{i^{\prime}p}^{(n)}))_{i,i^{\prime}=1, \ldots,p}\), and \(b_{i.i^{\prime}}^{(n)}\) is the \((i,i^{\prime})\) element of the matrix \(B^{n}\)._

Given observations \(X_{0},\ldots,X_{n}\) from the AR(\(p\)) model (8.166), the least square estimators of the parameters are obtained algebraically the same way as in linear regression models (see (6.152), (6.153)) and one has

\[\hat{\beta}=C_{n}^{-1}c_{0,n},\quad\hat{\alpha}=\overline{X}_{p,n}-\sum_{1\leq r \leq p}\hat{\beta}_{r}\overline{X}_{p-r,n}, \tag{8.174}\]

where \(C_{n}\) is a \(p\times p\) matrix, \(c_{0,n}\) is a \(p\times 1\) (column) vector, and \(\overline{X}_{p,n}\), \(\overline{X}_{p-r,n}\) are lag averages, define by

\[\overline{X}_{p-r,n} = (n-p+1)^{-1}\sum_{p\leq j\leq n}X_{j-r}(r=1,\ldots,p);\ \overline{X}_{p,n}=(n-p+1)^{-1}\sum_{p\leq j\leq n}X_{j}.\] \[C_{n} = ((c_{r,s,n}))_{1\leq r,s\leq p},\] \[c_{r,s,n} = (n-p+1)^{-1}\sum_{p\leq j\leq n}(X_{j-r}-\overline{X}_{p-r,n})(X_ {j-s}-\overline{X}_{p-s,n}),\] \[c_{0,n} = (c_{0,1,n},c_{0,2,n},\ldots,c_{0,p,n})^{\prime}. \tag{8.175}\]

Proceeding as in the case \(p=1\), one shows that \(C_{n}\) converges to \(\operatorname{Cov}(Z)=\sigma^{2}V\) (Exercise 8.17), and

\[\sqrt{n}(\hat{\beta}-\beta)=\] \[= C_{n}^{-1}\left(\frac{1}{\sqrt{n}}\right)\sum_{p\leq j\leq n}((X_ {j-1}-\mu)\varepsilon_{j},(X_{j-2}-\mu)\varepsilon_{j},\ldots,(X_{j-p}-\mu) \varepsilon_{j})^{\prime}+o_{p}(1)\] \[= (\sigma^{2}V)^{-1}\left(\frac{1}{\sqrt{n}}\right)\sum_{p\leq j \leq n}((X_{j-1}-\mu)\varepsilon_{j},(X_{j-2}-\mu)\varepsilon_{j},\ldots,(X_{j- p}-\mu)\varepsilon_{j})^{\prime}+\] \[\quad+o_{p}(1)\stackrel{{\mathscr{L}}}{{\longrightarrow }}N(0,V^{-1}), \tag{8.176}\]

by the martingale CLT, noting that every linear combination of \((X_{j-1}-\mu)\varepsilon_{j}\), \((X_{j-2}-\mu)\varepsilon_{j},\ldots,(X_{j-p}-\mu)\varepsilon_{j}\) is a martingale difference sequence, and the covariance matrix of its components is \(\sigma^{2}\text{Cov}(Z)=\sigma^{4}V\). One may also show that the least squares estimate of \(\sigma^{2}\), namely,

\[\hat{\sigma}^{2}=(n-p+1)^{-1}\sum_{p\leq j\leq n}\hat{\varepsilon}_{j}^{2},\quad \hat{\varepsilon}_{j}:=X_{j}-\hat{\alpha}-\sum_{1\leq r\leq p}\hat{\beta}_{r}X_ {j-r}, \tag{8.177}\]

is consistent (Exercise 8.17). We have arrived at the following useful result.

**Theorem 8.9**.: _Assume (8.170) holds for the AR\((p)\) model (8.166) with i.i.d. errors satisfying (8.147), and that \(X_{0},\ldots,X_{p-1}\) and \(\varepsilon_{n}\) have finite fourth moments, or that their joint distribution is the invariant distribution (of \(Z\)). Then \(\sqrt{n}C_{n}^{1/2}((\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})/\hat{\sigma}\) converges in distribution to \(N(0,I_{p})\) as \(n\to\infty\)._

Note that \((\hat{\alpha},\hat{\boldsymbol{\beta}})\) as given by (8.174) is now easily shown to be jointly asymptotically Normal, and by computing variances and covariances of \(\hat{\alpha}\) and the components of \(\hat{\boldsymbol{\beta}}\), one can find the (joint) Normal distribution of \(\sqrt{n}(\hat{\alpha}-\alpha,\hat{\boldsymbol{\beta}}-\boldsymbol{\beta})\). Classical CLT-based confidence regions for \(\beta\) and for \((\alpha,\boldsymbol{\beta})\) may now be constructed (Exercise 8.18). One may also obtain bootstrap confidence regions by resampling from the estimated residuals in (8.177) (see Bose 1988; Lahiri 2003).

_Remark 8.12_.: If one assumes that the errors \(\varepsilon_{n}\) are Normal \(N(0,\sigma^{2})\), a common assumption in time series analysis (see Brockwell and Davis 2002), then the least squares estimates \(\hat{\alpha}\), \(\hat{\beta}\) are the MLEs, conditionally given the initial values (Exercise 8.20).

_Remark 8.13_.: It is customary in texts on time series to express the polynomial equation: determinant \((B-\lambda I_{p})=0\), for the AR\((p)\) model, namely, \(\lambda^{p}=\beta_{1}\lambda^{p-1}+\beta_{2}\lambda^{p-2}+\cdots+\beta_{p}\), in terms of the variable \(\zeta=\lambda^{-1}\). The stability condition (8.170) then becomes: _the polynomial \(\beta(\zeta)=1-\sum_{1\leq r\leq p}\beta_{r}\zeta^{r}\) (in \(\zeta\)) has all its roots in \(|\zeta|>1\), i.e., outside the unit circle in the complex plane._

Our final example is the _autoregressive-moving average model ARMA\((p,q)\),_ where to the new innovation (error) in each period is added a linear combination of the past \(q\) errors.

_Example 8.7 (ARMA\((p,q)\) Model)._ For \(p\geq 1\), \(q\geq 1\), and a sequence of mean zero (and finite variance) i.i.d. sequence \(\{\varepsilon_{n}:n\geq 1\}\) define

\[X_{n}=\alpha+\sum_{1\leq r\leq p}\beta_{r}X_{n-r}+\sum_{1\leq r\leq q}\theta_{ r}\varepsilon_{n-r}+\varepsilon_{n},\quad(n\geq p),\ (\beta_{p}\neq 0+\theta_{q} \neq 0). \tag{8.178}\]

If the initial states \(\{X_{0},X_{1},\ldots,X_{p-1}\}\) are independent of \(\{\varepsilon_{n}:n\geq p\}\), the sequence \(\{Y_{n}=(X_{n-p+1},X_{n-p+2},\ldots,X_{n},\varepsilon_{n-q+1},\varepsilon_{n- q},\ldots,\varepsilon_{n})^{\prime}:\ n=p-1,p,p+1,\ldots\}\) is Markov, as follows from the representation

\[Y_{n}=\alpha^{\sim}+HY_{n-1}+\zeta_{n}, \tag{8.179}\]

where \(\alpha^{\sim}=(0,\ldots,0,\alpha,0,\ldots,0)^{\prime}\) with \(p-1\) zero's before \(\alpha\) and \(q\) zero's after it; \(\zeta_{n}=(0,\ldots,0,\varepsilon_{n},0,\ldots,0,\varepsilon_{n})^{\prime}\), and the \((p+q)\times(p+q)\) matrix \(H\) is of the form

\[H=\begin{bmatrix}B&C\\ O_{q\times p}&D\end{bmatrix}\,.\]Here \(B\) is the \(p\times p\) matrix (8.168); the \(p\times q\) matrix \(C\) has all elements zero, except for its last (\(p\)-th) row which is \((\theta_{q},\ldots,\theta_{1})\); the \(q\times p\) matrix \(O_{q\times p}\) has all zero elements; the \(i\)-th row of the \(q\times q\) matrix \(D\) has \(1\) as its \((i+1)\)-th element and zeros for the rest (\(i=1,\ldots,q-1\)), its last (\(q\)-th) row has all zeros. For example, if \(p=3\), \(q=3\), then \(H\) is given by

\[\left[\begin{matrix}0&1&0&0&0&0\\ 0&0&1&0&0&0\\ \beta_{3}&\beta_{2}&\beta_{1}&\theta_{3}&\theta_{2}&\theta_{1}\\ 0&0&0&0&1&0\\ 0&0&0&0&0&1\\ 0&0&0&0&0&0\end{matrix}\right]\]

Expanding by the last row, determinant \((H-\lambda I_{p+q})=(-\lambda)^{q}\times\) determinant of \((B-\lambda I_{p})\). Therefore the eigenvalues of \(H\) all lie inside the unit circle in the complex plane, iff the same is true of \(B\). Iterations of (8.179) yield

\[Y_{n}=H^{n-p+1}Y_{p-1}+\sum_{p\leq r\leq n}H^{n-r}\xi_{r},\qquad(\xi_{r}=\zeta _{r}+\alpha^{\sim}). \tag{8.180}\]

**Proposition 8.6**.: _If the eigenvalues of the matrix \(B\) in (8.168) all lie in the interior of the unit circle in the complex plane, then the Markov process (8.179) has a unique invariant distribution \(\pi\) given by the distribution of \(Z=\sum_{0\leq n<\infty}H^{n}\xi_{n}\), whose mean vector is the constant vector equal to \(\alpha/(1-\sum_{r}\beta_{r})\) in each coordinate, and the covariance matrix is \(\sigma^{2}V\), \(V=\sum_{0\leq n<\infty}H^{n}A(H^{\prime})^{n}\) with \(\sigma^{2}A\) as the covariance matrix of \(\zeta_{r}\) and \(A\) has \((p,p)\), \((p+q,p+q)\), \((p,p+q)\) and \((p+q,p)\) elements equal to \(1\) and all other elements zero. Also, no matter what the initial state \(Y_{p-1}\) is, the process converges in distribution to \(\pi\)._

Note that the variance \(\gamma(0)\), say, of \(X_{n}\) under the stationary distribution is \(\sigma^{2}V_{pp}\) (or \(\sigma^{2}V_{ii}\), for any \(i\), \(1\leq i\leq p\)), and one obtains \(\gamma(r):=\operatorname{cov}(X_{n-r},X_{n})\) as the element \(\sigma^{2}V_{1,1+r}\) for \(1\leq r\leq p-1\). Also, \(\rho(q):=\operatorname{cov}(X_{n},\varepsilon_{n})=\sigma^{2}\) and \(\rho(r):=\operatorname{cov}(X_{n},\varepsilon_{n-q+r})=\sigma^{2}V_{p,p+r}\) for \(1\leq r<q\). Clearly, \(\operatorname{cov}(X_{n},\varepsilon_{n+r})=0\) for \(r>0\). One may derive \(\gamma(r):=\operatorname{cov}(X_{n-r},X_{n})\) for \(r>p-1\) recursively, using (8.178), namely,

\[\gamma(s)=\sum_{1\leq r\leq p}\beta_{r}\gamma(s-r)+\sum_{1\leq r\leq\min\{s,q \}}\theta_{r}\rho(q+s-r)\quad(s>p). \tag{8.181}\]

However, as illustrated by Example 8.8 below (also see Exercise 8.21), it is generally much simpler to compute the covariances \(\gamma(s)\) using (8.178) (and (8.181)) for all \(s\geq 0\), than by using the expression for \(V\) in Proposition 8.6 for the computation of \(\gamma(r)\) for \(r=0,\ldots,p-1\) and \(\rho(r)\) for \(r=0,\ldots,q\) first and then (8.181). The function \(r\to\gamma(r)\) is referred to as the _autocovariance function_ or _ACVF_.

In the stable AR\((p)\) and, more generally, ARMA\((p,q)\) models _one may subtract the mean from the original variables \(X_{n}\)_ or from the augmented Markovian \(Y_{n}\) to simplify the equations. For example, (8.178), (8.179) may then be expressed as

\[X_{n} = \sum_{1\leq r\leq p}\beta_{r}X_{n-r}+\sum_{1\leq r\leq q}\theta_{ r}\varepsilon_{n-r}+\varepsilon_{n}\qquad(n\geq p-q),\ (\beta_{p}\neq 0+\theta_{q} \neq 0),\] \[Y_{n} = HY_{n-1}+\zeta_{n}. \tag{8.182}\]If one also assumes that the initial data \(Y_{0}=(X_{0},\ldots,X_{p-1})\) has the distribution \(\pi\), then iterations such as (8.172) lead to the so-called (infinite order) _moving average representations_ of stationary time series:

\[X_{n}=\sum_{0\leq r<\infty}\psi_{r}\varepsilon_{n-r}. \tag{8.183}\]

The coefficients \(\psi_{r}\) may be computed from (8.181) in different ways. One convenient formalism is to use the _one-step backward operator_\(\boldsymbol{B}:\boldsymbol{B}U_{n}=U_{n-1}\). (We use slanted and bold \(\boldsymbol{B}\) to distinguish it from the matrix \(B\) in the models above). Then (8.181) may be expressed as

\[X_{n}=\sum_{1\leq r\leq p}\beta_{r}\boldsymbol{B}^{r}X_{n}+\sum_{1\leq r\leq q }\theta_{r}\boldsymbol{B}^{r}\varepsilon_{n}+\varepsilon_{n}. \tag{8.184}\]

Write \(\beta(z)\), \(\theta(z)\) for the polynomials \(\beta(z)=1-\sum_{1\leq r\leq p}\beta_{r}z^{r}\), \(\theta(z)=1+\sum_{1\leq r\leq q}\theta_{r}z^{r}\). By Remark 8.13, the stability condition of the process means that the zeros of \(\beta(z)\) all lie outside the unit circle in the complex plane, and \(\psi(z):=\theta(z)/\beta(z)\) may be expanded in a convergent series expansion around \(z=0\), cancelling out common zeros of \(\beta(z)\) and \(\theta(z)\), namely, \(\psi(z)=\sum_{0\leq r<\infty}\psi_{r}z^{r}\). One then has \(\beta(\boldsymbol{B})X_{n}=\theta(\boldsymbol{B})\varepsilon_{n}\), or

\[X_{n}=\psi(\boldsymbol{B})\varepsilon_{n}=\sum_{0\leq r<\infty}\psi_{r} \varepsilon_{n-r}. \tag{8.185}\]

If one assumes the _invertibility_ condition that _all the zeros of \(\theta(z)\) also lie outside the unit circle,_ one may similarly invert (8.185) to write

\[\sum_{0\leq r<\infty}\chi_{r}z^{r}=\beta(z)/\theta(z),\] \[\varepsilon_{n}=[\beta(\boldsymbol{B})/\theta(\boldsymbol{B})]X_ {n}=\sum_{0\leq r<\infty}\chi_{r}X_{n-r},\] \[\frac{1}{\theta(z)}=\sum_{0\leq j<\infty}a_{j}z^{j},\] \[\varepsilon_{n}=\sum_{0\leq j<\infty}a_{j}\boldsymbol{B}^{j} \beta(\boldsymbol{B})X_{n}=\sum_{0\leq j<\infty}a_{j}(X_{n-j}-\beta_{1}X_{n-j -1}-\cdots-\beta_{p}X_{n-j-p}). \tag{8.186}\]

For the estimation of the stationary ergodic ARMA\((p,q)\) model, one may first estimate the mean \(\mu\) by \(\overline{X}\) and the autocovariances \(\gamma(r)\) by

\[\hat{\gamma}(r)=n^{-1}\sum_{0\leq j\leq n-r}(X_{j}-\overline{X})(X_{j+r}- \overline{X}),\quad(r=0,1,2,\ldots),\ \overline{X}=n^{-1}\sum_{0\leq j\leq n}X_{j}. \tag{8.187}\]

In general, one requires \(\hat{\gamma}(r)\) (\(r=0,1,\ldots,p+q\)) values to estimate the \(p+q+1\) parameters \(\beta_{r}\) (\(r=1,\ldots,p\)), \(\theta_{r}\) (\(r=1,\ldots,q\)) and \(\sigma^{2}\) by this rather elementary method. The estimates (8.187) are easily shown to be consistent, and, therefore, the estimates of the \(p+q+1\) parameters, using (8.181) are consistent. This "method of moments" is also known as the _Yule-Walker method_. Assume now that \(E\varepsilon_{n}^{4}<\infty\). By using general central limit theorems under dependence, such as that for martingales with stationary ergodic differences (See Bhattacharya and Waymire 2009, pp. 507-513), one may show that the estimates \(\hat{\gamma}(r)\), for any finite set of values of \(r\), are (jointly) Normal. By the delta method one may then derive the asymptotic (joint) Normality of \(\hat{\beta}_{r}\) (\(r=1,\ldots,p\)), \(\hat{\theta}_{r}\) (\(r=1,\ldots,q\)). This method is rather messy, and generally inefficient, and there exist more efficient, but computationally elaborate procedures in the literature (see Brockwell and Davis 2002). For a comprehensive account of bootstrap methods we refer to Lahiri (2003), especially Sect. 8.5. Here is a brief outline of the latter section. By (8.186), writing \(\beta_{0}=-1\), \(\theta_{0}=1\), and \(-\sum_{0\leq k\leq p}\beta_{k}X_{r-k}=\sum_{0\leq k\leq q}\theta_{k}\varepsilon _{r-k}\), one may represent the innovations \(\varepsilon_{n}\) as

\[\varepsilon_{i} = \sum_{0\leq j<\infty}a_{j}(X_{i-j}-\beta_{1}X_{i-j-1}-\cdots- \beta_{p}X_{i-j-p})\] \[= \sum_{0\leq j<i_{0}+1}a_{j}(X_{i-j}-\beta_{1}X_{i-j-1}-\cdots- \beta_{p}X_{i-j-p})+R_{i_{0},i}\] \[R_{i_{0},i} := \sum_{j\geq i_{0}+1}a_{j}(X_{i-j}-\beta_{1}X_{i-j-1}-\cdots- \beta_{p}X_{i-j-p})\] \[= \sum_{j\geq i_{0}+1}a_{j}\sum_{0\leq r\leq q}\theta_{r}\varepsilon _{i-j-r}. \tag{8.188}\]

Note that \(a_{i}\to 0\) exponentially fast with \(i\), in view of \(\|H^{m}\|<1\) for some \(m\) (see (8.171)), so that one may ignore the term \(R_{i_{0},i}\) in (8.188) for large enough \(i\). Since the coefficients \(a_{i}\) are polynomial in \(\theta_{j}\) (use the identity \((1+\sum_{1\leq r\leq q}\theta_{r}z^{r})(\sum_{0\leq j<\infty}a_{j}z^{j})=1\)), consistent and asymptotically Normal estimates (for example, the Yule-Walker estimates \(\hat{\theta}_{r}\) of \(\theta_{r}\)) lead to consistent and asymptotically Normal estimates \(\hat{a}_{j}\) of \(a_{j}\) (\(j=1,\ldots,i\)). Together with similar estimates \(\hat{\beta}_{k}\) of \(\beta_{k}\) one uses (8.188), after deleting the term \(R_{i_{0},i}\), to obtain estimates \(\hat{\varepsilon}_{i}\) of the residuals \(\varepsilon_{i}\), \(i=1,2,\ldots,i_{0}\) for a large enough \(i_{0}\). To make its mean zero (in bootstrap sampling), let \(\hat{\varepsilon}_{i,n}=\hat{\varepsilon}_{i}-\overline{\varepsilon}\), where \(\overline{\varepsilon}=(1/i_{0})\sum_{1\leq i\leq i_{0}}\hat{\varepsilon}_{i}\). The bootstrap observations \(X_{i}^{*}\) are now recursively obtained using the relation \(X_{i}^{*}=\sum_{1\leq k\leq p}\hat{\beta}_{k}X_{i-k}^{*}+\sum_{0\leq k\leq q} \hat{\beta}_{k}\hat{\varepsilon}_{i-k,n}+\varepsilon_{i}^{*}\), for \(i\geq 1-\max\{p,q\}\) and setting \(X_{i}^{*}=0\), \(\varepsilon_{i}^{*}=0\) for \(i\leq-\max\{p,q\}\).

_Example 8.8_ (_ARMA\((1,1)\)).: Assume the stability condition \(|\beta_{1}|<1\) and the invertibility condition \(|\theta_{1}|<1\). Here, after subtracting the mean \(\mu=\alpha/(1-\beta_{1})\) from the original sequence, the mean zero stationary AR\((1,1)\) sequence satisfies

\[X_{n}=\beta_{1}X_{n-1}+\theta_{1}\varepsilon_{n-1}+\varepsilon_{n},\]

where, the i.i.d. \(\varepsilon_{n}\) have mean zero, variance \(\sigma^{2}>0\) and a finite fourth moment. The covariance \(\gamma(r)\) of \(X_{n-r}\) and \(X_{n}\) is obtained by taking covariances of both sides of (8.185) with \(X_{n-r}\), yielding \(\gamma(0)=\beta_{1}\gamma(1)+\sigma^{2}(1+\theta_{1}(\beta_{1}+\theta_{1}))\), and \(\gamma(1)=\beta_{1}\gamma(0)+\theta_{1}\sigma^{2}\), to solve for both \(\gamma(0)\) and \(\gamma(1)\). One thus has

\[\gamma(0) = \sigma^{2}(1+2\beta_{1}\theta_{1}+\theta_{1}^{2})/(1-\beta_{1}^{ 2}),\quad\gamma(1)=\beta_{1}\gamma(0)+\theta_{1}\sigma^{2};\] \[\gamma(r) = \beta_{1}\gamma(r-1)=\beta_{1}^{r-1}\gamma(1)\text{ for }r>1. \tag{8.189}\]

One may use the sample estimates \(\hat{\gamma}(r)\) of \(\gamma(r)\) in (8.187) to estimate the three parameters \(\beta_{1}\), \(\theta_{1}\), \(\sigma^{2}\) by the "method of moments":\[\hat{\sigma}^{2}(1+2\hat{\beta}_{1}\hat{\theta}_{1}+\hat{\theta}_{1}^{2})/(1-\hat{ \beta}_{1}^{2})=\hat{\gamma}(0),\quad\hat{\beta}_{1}\hat{\gamma}(0)+\hat{\theta} _{1}\hat{\sigma}^{2}=\hat{\gamma}(1),\ \ \hat{\beta}_{1}\hat{\gamma}(1)=\hat{ \gamma}(2).\]

As mentioned above, \((\hat{\gamma}(0),\,\hat{\gamma}(1)\,\,\hat{\gamma}(2))\) has a three-dimensional asymptotic Normal distribution. It then follows, by the delta method, that \((\hat{\beta}_{1},\hat{\theta}_{1})\) is asymptotically bivariate Normal, and its covariance matrix may be computed using that of \((\hat{\gamma}(0),\,\hat{\gamma}(1),\,\hat{\gamma}(2))\). Also, for purposes of bootstrapping as described above, one has

\[\frac{1}{\theta(z)}=(1+\theta_{1}z)^{-1}=1-\theta_{1}z+\theta_{1}^{2}z^{2}+ \cdots+(-1)^{j}\theta_{1}^{j}z^{j}+\ldots;\]

\[a_{0}=1,\qquad a_{j}=(-1)^{j}\theta_{1}^{j}\ (j\geq 1).\]

We next turn to an extension of Theorem 7.5 to maximum likelihood estimators in parametric models for stationary ergodic stochastic processes.

Let \(\{X_{n}:n\geq 1\}\) be a stochastic process with values in a measurable space \(\mathscr{X}\) (with \(\sigma\)-field \(\mathscr{S}\)), defined on a probability space \((\Omega,\mathscr{F},P_{\theta})\). Here \(\theta\in\Theta\)--an open subset of \(\mathbb{R}^{p}\). Assume that \(\mu_{n}\) (\(n\geq 1\)) are \(\sigma\)-finite measures on \((\mathscr{X},\mathscr{S})\) such that under \(P_{\theta}\) the distribution of \((X_{1},\ldots,X_{n})\) is absolutely continuous with respect to the product measure \(\mu_{1}\times\cdots\times\mu_{n}\) with density \(f_{n}(\mathbf{x}_{1}^{n};\theta)\) (\(\mathbf{x}_{1}^{n}:=(x_{1},\ldots,x_{n})^{\prime}\)). Write \(h_{n}\) for the conditional p.d.f. of \(X_{n}\), given \(\mathbf{X}_{1}^{n-1}\equiv(X_{1},\ldots,X_{n-1})^{\prime}\). That is,

\[h_{n}(x_{n};\mathbf{x}_{1}^{n-1},\theta) =f_{n}(\mathbf{x}_{1}^{n};\theta)/f_{n-1}(\mathbf{x}_{1}^{n-1}; \theta),\qquad(n\geq 2)\] \[h_{1}(x_{1};\theta) =f_{1}(x_{1};\theta).\]

For \(n\geq 2\), \(h_{n}\) is defined whenever the denominator in (8.192) is positive. If \(f_{n-1}(\mathbf{x}_{1}^{n-1};\theta)=0\), then \(f_{n}(\mathbf{x}_{1}^{n};\theta)=0\) (for all \(x_{n}\) outside a set of \(\mu_{n}\)-measure zero). Therefore, one may define \(h_{n}(x_{n};\mathbf{x}_{1}^{n-1},\theta)\) arbitrarily in this case. We make the following assumptions:

**(D\({}_{1}\))**: \(\theta\to f_{n}(\mathbf{x}_{1}^{n};\theta)\) is thrice continuously differentiable on \(\Theta\), for all \(\mathbf{x}_{1}^{n}\) outside a set of zero \(\mu_{1}\times\cdots\times\mu_{n}\)-measure (\(n\geq 1\)).

**(D\({}_{2}\))**:

\[\text{(i)}\quad 0 \equiv\frac{\partial}{\partial\theta_{r}}\int h_{n}(x_{n}; \mathbf{x}_{1}^{n-1},\theta)\mu_{n}(dx_{n})\] \[=\int\frac{\partial}{\partial\theta_{r}}h_{n}(x_{n}:\mathbf{x}_{1 }^{n-1},\theta)\mu_{n}(dx_{n})\qquad(1\leq r\leq p,\,n\geq 1).\] \[\text{(ii)}\quad 0 \equiv\frac{\partial^{2}}{\partial\theta_{r}\partial\theta_{r^ {\prime}}}\int h_{n}(x_{n};\mathbf{x}_{1}^{n-1},\theta)\mu_{n}(dx_{n})\] \[=\int\frac{\partial^{2}}{\partial\theta_{r}\partial\theta_{r^{ \prime}}}\,h_{n}(x_{n};\mathbf{x}_{1}^{n-1};\theta)\mu_{n}(dx_{n})\qquad(1 \leq r,r^{\prime}\leq p;\,n\geq 1).\]
**(D\({}_{3}\))**: \(E_{\theta}\left(\frac{\partial\log f_{n}(\mathbf{X}_{1}^{n};\theta)}{\partial \theta_{r}}\right)^{2}<\infty\qquad(1\leq r\leq p,\,n\geq 1).\)
**(D\({}_{4}\))**: For each \(\theta_{0}\in\Theta\) there exists \(\delta=\delta(\theta_{0})>0\) such that

\[\sup\left\{\left|\frac{1}{n}\sum_{j=1}^{n}\frac{\partial^{3}\log h_{j}(X_{j}; \mathbf{X}_{1}^{j-1},\theta)}{\partial\theta_{r}\partial\theta_{r^{\prime}} \partial\theta_{r^{\prime\prime}}}\right|:|\theta-\theta_{0}|\leq\delta\right\}\]

is bounded by a function \(g_{n}(\mathbf{X}_{1}^{n})\) such that \(\overline{\lim}_{n\to\infty}\,E_{\theta_{0}}g_{n}(\mathbf{X}_{1}^{n})<\infty\).

**(D\({}_{5}\))**:

\[\text{(i)}\quad\quad\frac{1}{n}\sum_{j=1}^{n}\frac{\partial}{ \partial\theta_{r}}\log h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta)\ \stackrel{{ P_{\theta}}}{{\longrightarrow}}0,\qquad(1\leq r\leq p);\] \[\text{(ii)}\quad\quad\frac{1}{n}\sum_{j=1}^{n}\frac{\partial^{2}}{ \partial\theta_{r}\partial\theta_{r^{\prime}}}\log h_{j}(X_{j};{\bf X}_{1}^{j-1}, \theta)\stackrel{{ P_{\theta}}}{{\longrightarrow}}-I_{r,r^{\prime}}( \theta),\quad(1\leq r,r^{\prime}\leq p),\]

where \(I(\theta):=((I_{r,r^{\prime}}(\theta)))\) is, for each \(\theta\in\Theta\), a positive definite matrix.

**Theorem 8.10**.: _Under the assumptions \((D_{1})\)-\((D_{5})\), for each \(\theta_{0}\in\Theta\) there exists a measurable sequence \(\hat{\theta}_{n}:\Omega\to\Theta\) such that \(\hat{\theta}_{n}\) is \(\mathscr{F}_{n}\)-measurable and_

_(i) \(P_{\theta_{0}}(\hat{\theta}_{n}\) is a solution of the likelihood equation \(\partial f_{n}({\bf x}_{1}^{n};\theta)/\partial\theta_{r}=0\), \(1\leq r\leq p)\longrightarrow 1\) as \(n\to\infty\),_

_(ii) \(\hat{\theta}_{n}\stackrel{{ P_{\theta_{0}}}}{{\longrightarrow}} \theta_{0}\)._

Proof.: The proof follows exactly as the first part of the proof of Theorem 7.5. (See (7.49)-(7.66).) 

In addition to the above assumptions if one assumes also that

**(D\({}_{6}\))**: \(\quad\quad\frac{1}{\sqrt{n}}\operatorname{Grad}\log f_{n}({\bf X}_{1}^{n}; \theta_{0})\quad\equiv\quad\frac{1}{\sqrt{n}}\sum_{j=1}^{n}\operatorname{Grad} \log h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta_{0})\quad\stackrel{{ \mathscr{L}}}{{\longrightarrow}}N(0,I(\theta_{0}))\)

under \(P_{\theta}\), then the proof of the second part of Theorem 7.5 also goes over.

**Theorem 8.11**.: _Let \(\theta_{0}\) be the true parameter value. Under the assumptions \((D_{1})\)-\((D_{6})\) every \(\hat{\theta}_{n}\) satisfying (i), (ii) of Theorem 8.10 converges in law to \(N(0,I^{-1}\theta_{0}))\)._

**Definition 8.4**.: Assume (D\({}_{1}\))-(D\({}_{6}\)). An estimator \(\hat{\theta}_{n}\) of \(\theta\) is said to be _asymptotically efficient_ if \(\sqrt{n}(\hat{\theta}_{n}-\theta_{0})\stackrel{{\mathscr{L}}}{{ \longrightarrow}}N(0,I^{-1}(\theta_{0}))\), under \(P_{\theta_{0}}\), for every \(\theta_{0}\in\Theta\).

_Remark 8.14_.: Suppose \(\widetilde{\theta}_{n}\) is an unbiased estimator of \(\theta\), and \(E_{\theta}|\widetilde{\theta}_{n}|^{2}<\infty\). Then Theorem 7.4 yields

\[\sideset{}{{}_{n}}{\sum}_{n}(\theta)\geq I_{n}^{-1}(\theta), \tag{8.193}\]

where \(\sum_{n}(\theta)\) is the dispersion matrix of \(\widetilde{\theta}_{n}\) (under \(P_{\theta}\)), and

\[\frac{1}{n}I_{n}(\theta)\] \[=\frac{1}{n}\left(\left(E_{\theta}\left[\left\{\sum_{j=1}^{n} \frac{\partial\log h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta)}{\partial\theta_{r} }\right\}\left\{\sum_{j=1}^{n}\frac{\partial\log h_{j}(X_{j};{\bf X}_{1}^{j- 1},\theta)}{\partial\theta_{r^{\prime}}}\right\}\right]\right)\right)\] \[=\frac{1}{n}\left(\left(\sum_{j=1}^{n}E_{\theta}\left[\frac{ \partial\log h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta)}{\partial\theta_{r}}\cdot \frac{\partial\log h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta)}{\partial\theta_{r^{ \prime}}}\right]\right)\right), \tag{8.194}\]since \(E_{\theta}\left(\frac{\partial\log h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta)}{\partial \theta_{r}}\big{|}{\mathscr{F}}_{j-1}\right)=0\;\forall\;j\). Suppose now that \(\left(\frac{\partial\log h_{j}(X_{j}{\bf X}_{1}^{j-1},\theta)}{\partial\theta_ {r}}\right)^{2}\) (\(j\geq 1\)) is a uniformly integrable sequence for every \(r\). Then (D\({}_{6}\)) implies that the sum in (8.194) converges to \(I(\theta)\). Together with (8.193), this leads to

\[\varliminf_{n\to\infty}\;n\big{\rgroup}\sum\nolimits_{n}(\theta)\geq I^{-1}( \theta). \tag{8.195}\]

This justifies the definition of asymptotic efficiency. It may be noted that, under uniform integrability of \(\sqrt{n}(\tilde{\theta}_{n}-\theta)\), \(E_{\theta}\sqrt{n}(\tilde{\theta}_{n}-\theta)\to 0\) (by (D\({}_{5}\))). Therefore, \(E_{\theta}\tilde{\theta}_{n}=\theta+o(n^{-\frac{1}{2}})\), and (8.195) holds without the requirement of unbiasedness of \(\tilde{\theta}_{n}\).

_Remark 8.15_.: Observe that \(\partial\log h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta)/\partial\theta_{r}\) (\(j\geq 1\)) is a _martingale difference sequence_ (under \(P_{\theta}\)), by (D\({}_{2}\)). Therefore one may often apply martingale limit theorems to verify (D\({}_{4}\))-(D\({}_{6}\)). For example, if \(\{X_{n}:n\geq 0\}\) is a _stationary ergodic Markov process_ then \(h_{j}(X_{j};{\bf X}_{1}^{j-1},\theta)\equiv g(X_{j},X_{j-1};\theta)\) for an appropriate \(g\) (the so-called transition probability density of the Markov process), so that \(\partial\log g(X_{j},X_{j-1};\theta)/\partial\theta_{r}\) (\(j\geq 1\)) is a stationary ergodic martingale difference sequence, and (D\({}_{5}\)) holds by _Birkhoff's ergodic theorem_ and (D\({}_{6}\)) holds by the _Billingsley-Ibragimov martingale CLT_.6

Footnote 6: See Billingsley (1968), p. 206, or Bhattacharya and Waymire (2009), p. 511.

_Example 8.9 (AR\((p)\) Model)._ Let \(p>1\), \(\{\varepsilon_{j}:j\geq p\}\) an _i.i.d._ sequence of mean zero squared integrable random variables, \(X_{0},X_{1},\ldots,X_{p-1}\) square integrable random variables independent of the sequence \(\{\varepsilon_{j}:j\geq p\}\). Define, recursively, the \(p\)-th order autoregressive process (or, the AR\((p)\) process)

\[X_{n}=\alpha+\beta_{1}X_{n-1}+\beta_{2}X_{n-2}+\cdots+\beta_{p}X_{n-p}+ \varepsilon_{n}\qquad(n\geq p). \tag{8.196}\]

Assume first that \(\varepsilon_{j}\) are i.i.d. \(N(0,\sigma^{2})\). Then the log of the conditional p.d.f. of \(X_{j}\) given \(X_{j-p},\ldots,X_{j-i}\) is

\[\log\,h_{j}=\frac{1}{2}\log 2\pi-\frac{1}{2}\log\sigma^{2}-\frac{1}{2\sigma^{2} }\left(X_{j}-\alpha-\beta_{1}X_{j-1}-\cdots-\beta_{p}X_{j-p}\right)^{2},\]

and

\[\frac{\partial\log h_{j}}{\partial\alpha} =\frac{1}{\sigma^{2}}\left(X_{j}-\alpha-\beta_{1}X_{j-1}-\cdots- \beta_{p}X_{j-p}\right),\] \[\frac{\partial\log h_{j}}{\partial\beta_{r}} =\frac{1}{\sigma^{2}}X_{j-r}\left(X_{j}-\alpha-\beta_{1}X_{j-1}- \cdots-\beta_{p}X_{j-p}\right),\quad 1\leq r\leq p,\] \[\frac{\partial\log h_{j}}{\partial\sigma^{2}} =-\frac{1}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\left(X_{j}-\alpha- \beta_{1}X_{j-1}-\cdots-\beta_{p}X_{j-p}\right)^{2}. \tag{8.197}\]

The likelihood equations are (for observations \(X_{p},\ldots,X_{n}\), given \(X_{0},\ldots,X_{p-1}\))

\[0 =\frac{1}{\sigma^{2}}\sum_{j=p}^{n}\left(X_{j}-\alpha-\beta_{1}X_ {j-1}-\cdots-\beta_{p}X_{j-p}\right)\quad\left(=\frac{1}{\sigma^{2}}\sum_{j=p} ^{n}\varepsilon_{j}\right);\] \[0 =\frac{1}{\sigma^{2}}\sum_{j=p}^{n}X_{j-r}\left(X_{j}-\alpha- \beta_{1}X_{j-1}-\cdots-\beta_{p}X_{j-p}\right)\quad\left(=\frac{1}{\sigma^{2}} \sum_{p}^{n}X_{j-r}\varepsilon_{j}\right),\]\[(1\leq r\leq p);\] \[0=-\frac{n-p+1}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{p}^{n}{(X_{j }-\alpha-\beta_{1}X_{j-1}-\cdots-\beta_{p}X_{j-p})^{2}}\] \[\qquad\Big{(}=-\tfrac{1}{2\sigma^{2}}+\tfrac{1}{2\sigma^{4}}\sum_ {p}^{n}\varepsilon_{j}^{2}\Big{)}.\]

The solutions are given by

\[\begin{pmatrix}\hat{a}\\ \hat{\beta}_{1}\\ \vdots\\ \hat{\beta}_{p}\end{pmatrix}=\begin{pmatrix}1&s_{1}s_{2}\ldots s_{p}\\ s_{1}&s_{11}s_{12}\ldots s_{1p}\\ s_{2}&s_{21}s_{22}\ldots s_{2p}\\ \cdot&\cdots\\ s_{p}&s_{p1}s_{p2}\ldots s_{pp}\end{pmatrix}^{-1}\begin{pmatrix}s_{0}\\ s_{01}\\ s_{02}\\ \vdots\\ s_{0p}\end{pmatrix}, \tag{8.199}\] \[\hat{\sigma}^{2}=\sum_{j=p}^{n}{\Big{(}X_{j}-\hat{a}-\hat{b}_{1}X_ {j-1}-\cdots-\hat{b}_{j}X_{j-p}\Big{)}^{2}}/(n-p+1),\]

where

\[s_{r} =\frac{1}{n-p+1}\sum_{j=p}^{n}{X_{j-r}}\qquad(0\leq r\leq p),\] \[s_{rr^{\prime}} =\frac{1}{n-p+1}\sum_{j=p}^{n}{X_{j-r}X_{j-r^{\prime}}}\qquad(0 \leq r,\,r^{\prime}\leq p). \tag{8.200}\]

Write

\[\varepsilon_{j}^{\prime}=a+\varepsilon_{j},\quad\gamma_{j}(0,0,\ldots,0, \varepsilon_{j})^{\prime}\in{\mathbb{R}}^{p}\qquad(j\geq p), \tag{8.201}\]

and

\[X_{j} =\sum_{r=1}^{p}{\beta_{r}X_{j-r}}+\varepsilon_{j}^{\prime},\] \[Y_{j} =(X_{j-p+1},X_{j-p+2},\ldots,X_{j})^{\prime}\qquad(j\geq p). \tag{8.202}\]

Then \(Y_{j}\), \(j\geq p\), is a Markov process,

\[Y_{j}=BY_{j-1}+\gamma_{j}, \tag{8.203}\]

where \(B\) is the \(p\times p\) matrix (8.168). _All eigenvalues of \(B\) have magnitude less than one_. Now the eigenvalues of \(B\) are the roots of the polynomial equation \(\det(B-\lambda I)=0\), i.e.,

\[-\lambda^{p}+\beta_{1}\lambda^{p-1}+\beta_{2}\lambda^{p-2}+\cdots+\beta_{p}=0. \tag{8.204}\]

In the stable case the Markov process converges in distribution to a unique invariant distribution \(\pi\), whatever the initial state \(Y_{j}=(X_{0},X_{1},\ldots,X_{p-1})^{\prime}\). Clearly \(\pi\) is Normal whose mean vector and dispersion matrix are given in Proposition 8.5. Then all the conditions (D\({}_{1}\))-(D\({}_{6}\)) hold. For example (see (8.198)),\[\left(\frac{1}{\sqrt{n-p-1}}\sum_{j=p}^{n}\varepsilon_{j},\right. \left.\frac{1}{\sqrt{n-p+1}}\sum_{j=p}^{n}X_{j-1}\varepsilon_{j},\ldots,\right.\] \[\left.\frac{1}{\sqrt{n-p+1}}\sum_{j=p}^{n}X_{j-p}\varepsilon_{j}, \frac{1}{\sqrt{n-p+1}}\sum_{j=p}^{n}(\varepsilon_{j}^{2}-\sigma^{2})\right)\]

is asymptotically Normal, by the martingale central limit theorem. Thus \(\sqrt{n}(\hat{\alpha}-\alpha\), \(\hat{\beta}-\beta\), \(\hat{\sigma}^{2}-\sigma^{2})\to N(0,I^{-1}(\theta))\), where \(I(\theta)\) is the information matrix.

Remark Remark 8.16: Finally, consider a _nonlinear autoregressive model of order p,_ or NLAR(\(p\)),

\[X_{n+1}=f(X_{n-p+1},\ldots,X_{n})+g(X_{n-p+1},\ldots,X_{n})\varepsilon_{n+1}, \quad(n=p-1,p,\ldots), \tag{8.205}\]

where \(\varepsilon_{n}\), \(n\geq p\), are i.i.d. with mean zero and variance \(\sigma^{2}>0\), and independent of \(X_{0},X_{1},\ldots,X_{p-1}\). The unknown real-valued functions \(f\) and \(g\) on \(\mathbb{R}^{p}\) are such that the Markov process \(Y_{n}=(X_{n-p+1},\ldots,X_{n})^{\prime}\), \(n\geq p\), (on the state space \(\mathbb{R}^{p}\)) is ergodic. For the case \(p=1\) an asymptotically optimal estimate of \(f\) by the so-called kernel method (See Chap. 10), and a consistent bootstrap estimate of its distribution, were obtained by Frankel et al. (2002). Independently of this, and more generally, Hwang (2002) derived an asymptotically optimal estimate of \(f\) and a consistent bootstrap estimate of the distribution of this estimate. She also found interesting conditions on a misspecified order \(p^{\prime}>p\) such that the estimate of \(f\) is still consistent. The problem of validity of approximating long memory processes by short memory ARMA processes are considered in Hosking (1984), Chan and Palma (1998), and Basak et al. (2001).

## Notes and References

* Ferguson (1996) and Serfling (1980) may be used as general references for this chapter. Pitman asymptotic relative efficiency (ARE) is due to Pitman (1948) and our treatment follows Serfling (1980), Chap. 10. A broader, more sophisticated and elegant approach to Pitman ARE is due to LeCam based on his notion of _contiguity_. See LeCam and Yang 1990 or van der Vaart 1998). An entirely different notion of asymptotic relative efficiency is due to Bahadur (1960). Here one looks at the asymptotic exponential rate at which the \(p\)-value under \(H_{1}:\theta>\theta_{0}\) goes to zero in probability (as \(n\to\infty\)) for a test statistic \(T_{n}\) which rejects \(H_{0}:\theta=\theta_{0}\) for large values of \(T_{n}\) (e.g., \(T_{n}=\sqrt{(X-\theta_{0})}\,\)). While the Pitman efficiency looks at alternatives at a distance \(O(n^{-1/2})\) of \(\theta_{0}\) and compares the rejection probabilities of two tests under such alternatives within the range of Normal approximation, the _Bahadur ARE_ looks at rejection probabilities under fixed alternatives \(\theta\) in the large deviation domain. A fine exposition of this is given in Serfling (1980), Chap. 10.

A standard reference for inference for time series is Brockwell and Davis (2002). For results and procedures for bootstrapping under dependence and for time series, see Lahiri (2003). For many econometric time series modeled as AR(1), AR(\(p\)) or ARMA(\(p,q\)), an appropriate assumption would be to have eigenvalues of the matrix \(B\) in (8.168) to be inside the circle, but with some very close to the boundary of the circle. For example, in AR(1) one may think of scaling \(\beta_{1}\) as \(1-b/n\) for some \(b>0\). The asymptotic distribution theory of the ordinary least squares estimate (OLS) then presents challenges. Basic work on this phenomenon under broad assumptions is due to Chan and Wei (1987, 1988). Also, see Phillips (1987). This so-called _unit root problem_ is now an important area in econometrics.

### Exercises for Chap. 8

#### Exercises for Sects. 8.1, 8.2

**Ex. 8.1**.: Consider the family of p.d.f.'s \(\{f(x-\theta):\theta\in\mathbb{R}^{1}\}\) in Example 8.1.

1. For the one-sided alternative \(H_{1}:\theta>0\), show that \(e_{P}(T_{n},t)=4\sigma_{f}^{2}f^{2}(0)\).
2. Consider the tests of \(H_{0}:\theta=0\), \(H_{1}:\theta\neq 0\). \[\delta_{1,n} :\text{Reject }H_{0}\text{ iff }\left|t\right|\equiv\left|\frac{ \overline{X}}{s}\right|>a_{n},\] \[\delta_{2,n} :\text{Reject }H_{0}\text{ iff }\left|\frac{1}{n}\sum_{j=1}^{n} \mathbf{1}_{\{X_{j}>0\}}-\frac{1}{2}\right|>b_{n},\] having asymptotic size \(\alpha\). Prove that \[e_{P}(\delta_{2,n},\delta_{1,n})=4\sigma_{f}^{2}f^{2}(0).\]

**Ex. 8.2**.:
1. Show that for every pair of random variables \(X,Y\), every distribution function \(G(x)\), and every constant \(\varepsilon>0\), one has \[\sup_{x\in\mathbb{R}^{1}}\left|\text{Prob}(Y\leq x)-\text{Prob}(X \leq x)\right|\] \[\leq P(|X-Y|>\varepsilon)+2\sup_{x}\left|P(X\leq x)-G(x)\right|+ \sup_{x}\left|G(x+\varepsilon)-G(x)\right|.\] (8.206)
2. Use (8.206), (8.43) and (8.44) to prove (8.45), under the assumptions (B) and (8.29).

**Ex. 8.3**.: Let \(\varphi(\mathbf{X})\) be the _symmetrization_ of \(T=T(\mathbf{X})\) as in Remark 8.2, with \(\mathbf{X}=(X_{1},\ldots,X_{n})^{\prime}\) and \(X_{i}\)'s i.i.d. If \(E|T|^{p}<\infty\) for some \(p\geq 1\), then show that \(E|\varphi(\mathbf{X})-\theta|^{p}\leq E|T-\theta|^{p}\), where \(\theta=ET\). [Hint: \(\varphi(\mathbf{X})=E(T\mid\mathscr{F})\), where \(\mathscr{F}\) is the sigma-field generated by permutations of indices \((1,2,\ldots,n)\) of \(X_{1}\), \(X_{2}\), \(\ldots,X_{n}\).]

**Ex. 8.4**.: Prove Proposition 8.1.

#### Exercises for Sect. 8.3

**Ex. 8.5** (Mendel's Experiment).: In Mendel's experiment in pea breeding, possible types of progeny were (1) round-yellow, (2) round green, (3) wrinkled yellow and (4) wrinkled green. According to Mendel's theory these were to occur in respective proportions \((H_{0})\)\(p_{1}=9/16\), \(p_{2}=p_{3}=3/16\), \(p_{4}=1/16\). In his experiment Mendel observed the respective numbers of progeny (out of a total \(n=556\)), \(n_{1}=315\), \(n_{2}=108\), \(n_{3}=101\), \(n_{4}=32\). Find the \(p\)-values using

1. Pearson's frequency chi-square test,
2. Wald's test,
3. the likelihood ratio test,
4. Rao's scores test.

Ex. 8.6 (Hardy-Weinberg Model).Test the Hardy-Weinberg model for probabilities of the three genotypes of a single gene with two alleles: \(p_{1}=\theta^{2}\), \(p_{2}=2\theta(1-\theta)\), \(p_{3}=(1-\theta)^{2}\), \(0<\theta<1\), and observed frequencies \(n_{1}\), \(n_{2}\), \(n_{3}\), using

1. Pearson's frequency chi-square test,
2. Wald's test,
3. the likelihood ratio test,
4. Rao's scores test.

Ex. 8.7.Test the _Fisher linkage model_ of Example 4.15, using

1. Pearson's frequency chi-square test,
2. Wald's test,
3. the likelihood ratio test,
4. Rao's scores test.

Ex. 8.8 (Chi-square Test for Independence in Two-Way Contingency Tables).A population is classified according to two categorical variables \(A\) and \(B\). The variable \(A\) has \(k\) classes \(A_{1},\ldots,A_{k}\) while \(B\) has \(m\) classes \(B_{1},\ldots,B_{m}\). One wishes to test if the two classifications are independent of each other, namely, \(H_{0}:p_{ij}=p_{i.}p_{.j}\) (\(1\leq i\leq k\), \(1\leq j\leq m\)), where \(p_{ij}\) is the proportion in the population belonging to class \(A_{i}\) and class \(B_{j}\), \(p_{i.}=\sum_{j}p_{ij}\) is the proportion belonging to class \(A_{i}\) of the categorical variable \(A\), \(p_{.j}=\sum_{i}p_{ij}\) is the proportion belonging to class \(B_{j}\) of the categorical variable \(B\).

1. Write down the frequency chi-square test.
2. Apply the test (a) to the following data concerning heart disease in male federal employees. Researchers classified 356 volunteer subjects according to their socio-economic status (\(SES\))--\(A\) and their smoking habits--\(B\).

Exercises for Sects. 8.4, 8.5Ex.8.1.Verify the expression (8.127) for \(D_{n}\).

**Ex. 8.10**.: The numbers \(k\) of micro-organisms of a certain type found within each of \(n=60\) squares of a hemocytometer are as follows.

\[\begin{array}{l|c|c|c|c|c}\hline k&0&1&2&3&4\\ \hline\text{Observed frequency}&28&20&8&3&1\\ \hline\end{array}\]

Test if the data follow a Poisson distribution (Example taken from the website vassarstats. net).

**Ex. 8.11**.: It is common in insurance to assume that the times of arrival of claims follow a homogeneous Poisson process. Given \(n\) successive arrivals of claims \(0<t_{1}<t_{2}<\cdots<t_{n}\), derive the following tests for this model.

1. Kolmogorov-Smirnov
2. Carmer-von Mises
3. Anderson-Darling

**Ex. 8.12**.: Suppose the intervals between successive arrivals in the preceding exercise are i.i.d. _Pareto_ with p.d.f. \(f(u;\alpha)=\alpha(1+u)^{-\alpha-1}\) (\(\alpha>0\)), \(\alpha\) unknown.

1. Find the MLE of \(\alpha\).
2. Draw a random sample of size \(n=50\) from this _Pareto distribution_ with \(\alpha=2\), and carry out the goodness-of-fit tests (a)-(c) above for the data to have come from a Pareto distribution (with \(\alpha\) unknown).
3. Use tests (a)-(c) for the data in (b) to have come from an exponential distribution..

**Ex. 8.13**.: The following data on 32 skulls are taken from the book _A Handbook of Small Data Sets_ (Hand et al. 1994), which reproduced them from Moran (1923). The 17 type \(A\) skulls came from Sikkim and neighboring areas of Tibet. The 15 type \(B\) skulls were picked up on a battlefield in the Lhasa district and were believed to be those of native soldiers from the eastern province of Khams. It was thought at the time that the Tibetans from Khams might be survivors of a particular fundamental human type, unrelated to the Mongolian and Indian types which surrounded them. The five measurements on each skull are as follows:

\(X_{1}=\) greatest length of skull, \(X_{2}=\) greatest horizontal breadth of skull, \(X_{3}=\) height of skull, \(X_{4}=\) upper face height, \(X_{5}=\) face breadth, between outermost points of cheekbones.

Test if the skulls \(A\) and Skulls \(B\) belonged to the same type of humans. [Hint: Use Proposition 8.3.]

**Exercises for Sect. 8.6**

**Ex. 8.14**.: To complete the proof of Theorem 8.8,

1. prove (8.165) under the hypothesis of Theorem 8.8(b),
2. prove (8.166) using the martingale CLT as suggested at the end of the proof of Theorem 8.8.

**Ex. 8.15**.: Under the hypothesis of Theorem 8.8 construct a confidence interval for \(\beta\) with asymptotic level \(1-\theta(0<\theta<1)\).

**Ex. 8.16**.: Verify Proposition 8.5 using (8.172).

**Ex. 8.17**.: Assume the hypothesis of Theorem 8.9 for the AR\((p)\) model.

1. Prove that \(\hat{\gamma}(r)\) is a consistent estimate of \(\gamma(r)\) for \(r=0,1,\ldots,p\).
2. Prove that \(\hat{\sigma}^{2}\) in (8.177) is a consistent estimator of \(\sigma^{2}\).
3. Prove that \(C_{n}\) converges in probability to \(\sigma^{2}V\).

**Ex. 8.18**.: In addition to the hypothesis of Theorem 8.9, assume \(\varepsilon_{n}\) are Normal \(N(0,\sigma^{2})\), and then prove that \(Z\) in (8.173) is Normal.

**Ex. 8.19**.: Construct a confidence region for \(\boldsymbol{\beta}\) of asymptotic level \(1-\theta\) in the stable AR\((p)\) model.

**Ex. 8.20**.: Assume that the error \(\varepsilon_{n}\) in the AR\((p)\) model (8.166) are i.i.d. \(N(0,\sigma^{2})\). Show that the least squares estimates \(\hat{\alpha}\), \(\hat{\beta}\) are then the MLEs, conditionally given \(X_{0},\ldots,X_{p-1}\).

**Ex. 8.21**.: Consider a stable and invertible stationary ARMA\((1,2)\) model.

1. Show that \(\hat{\gamma}(r)\) is a consistent estimate of \(\gamma(r)\) for \(r=0\), \(1,\ldots\), and that (\(\hat{\gamma}(0)\), \(\hat{\gamma}(1)\), \(\hat{\gamma}(2)\), \(\hat{\gamma}(3)\)) is asymptotically Normal.
2. Derive the analogs of (8.189) for \(\gamma(0)\), \(\gamma(1)\), \(\gamma(2)\), \(\gamma(3)\), and show that the Yule-Walker ("method of moments") estimator of \((\beta_{1},\theta_{1},\theta_{2})^{\prime}\) derived using these equations is asymptotically Normal.

**Ex. 8.22** (**Trend Removal**).: Let \(X_{n}=f(n)+Z_{n}\), where \(\{Z_{n}:n\geq 0\}\) is a stationary time series, and \(f\) is a deterministic function on the set of integers \(\mathbb{Z}_{t}=\{0,1,\ldots\}\). Show that if \(f\) is linear then \(\Delta X_{n}\equiv X_{n}-X_{n-1}\), \(n\geq 1\), is a stationary time series and, more generally, if \(f\) is a polynomial of order \(k\) then \(\Delta^{k}X_{n}\), \(n\geq k\), is stationary.

**Ex. 8.23** (**Trend Estimation)**.: Let \(X_{n}=f(n)+Z_{n}\), as in Exercise 8.22. Use the Dow Jones Utilities Index for 100 days from the internet, or from August 28 to December 18 of 1972 as given in Brockwell and Davis (1987), p. 499, for the problems below.

1. Assuming \(f\) is linear, use the method of least squares to estimate \(f\), and plot \(\widehat{Z}_{n}:=X_{n}-\hat{f}(n)\) to see if it looks stationary.
2. Assume \(f\) is quadratic, estimate it by the method of least squares, and plot \(\widehat{Z}_{n}=X_{n}-\hat{f}(n)\) to check visually if this "trend-removed" process looks stationary.
3. Assuming \(\{Z_{n},n\geq 0\}\) is a stable mean zero stationary AR(1) process, show that the estimated coefficients of \(f\) in (a) and (b) are consistent and asymptotically Normal.

## References

* Bahadur (1960) Bahadur, R. R. (1960). On the asymptotic efficiency of tests and estimates. _Sankhya, 22_, 229-252.
* Basak et al. (2001) Basak, G. K., Chan, N. H., & Palma, W. (2001). The approximation of long memory processes by an ARMA model. _Journal of Forecasting, 20_, 367-389.
* Bhattacharya & Ranga Rao (2010) Bhattacharya, R., & Ranga Rao, R. (2010). _Stochastic processes with applications_. SIAM Classics in Applied Mathematics (Vol. 61). Philadelphia: SIAM.
* Bhattacharya & Waymire (2009) Bhattacharya, R., & Waymire, E. C. (2009). _Stochastic processes with applications_. SIAM classics in applied mathematics series.
* Billingsley (1968) Billingsley, P. (1968). _Convergence of probability measures_. New York: Wiley.
* Bose (1988) Bose, A. (1988). Edgeworth correction by bootstrap in autoregressions. _Annals of Statistics, 16_, 1709-1722.
* Brockwell & Davis (2002) Brockwell, P.J., & Davis, R. A. (2002). _Introduction to time series and forecasting_. New York: Springer.
* Brown et al. (2001) Brown, L., Cai, T., & DevsGupta, A. (2001). Interval estimation for a binomial proportion. _Statistical Science, 16_(2), 101-133.
* Chan & Palma (1998) Chan, N. H., & Palma, W. (1998). State space modeling of long-memory processes. _The Annals of Statistics, 26_, 719-740.
* Chan & Wei (1987) Chan, N. H., & Wei, C. Z. (1987). Asymptotic inference for nearly nonstationary \(AR(1)\) processes. _The Annals of Statistics, 15_(3), 1050-1063.
* Chan & Wei (1988) Chan, N. H., & Wei, C. Z. (1988). Limiting distributions of least squares estimates of unstable autoregressive processes. _The Annals of Statistics, 16_(1), 367-401.
* Chernoff & Lehmann (1954) Chernoff, H., & Lehmann, E. L. (1954). The use of maximum likelihood estimates in chisquare tests for goodness of fit. _The Annals of Mathematical Statistics_, 25, 579-586.
* D'Agostino & Stephens (1986) D'Agostino, R., & Stephens, M. (1986). _Goodness of fit techniques_. New York: Marcel Dekker
* Ferguson (1967) Ferguson, T. S. (1967). _Mathematical statistics: A decision theoretic approach_. New York: Academic Press.
* Ferguson (1996) Ferguson, T. S. (1996). _A course in large sample theory_. London: Taylor & Francis.
* Frankel et al. (2002) Frankel, J., Kreiss, J. P., & Mammen, E. (2002). Bootstrap kernel smoothing in nonlinear time series. _Bernoulli, 8_, 1-37.

* Hand et al. (1994) Hand, D. J., Daly, F., Lunn, A. D., McConway, K. J., & Ostrowski, E. (1994). _A handbook of small data sets_. London: Chapman & Hall.
* Hodges & Lehmann (1956) Hodges, J. L., & Lehmann, E. L. (1956). The efficiency of some nonparametric competitors of the t-test. _Annals of Mathematical Statistics, 27_(2), 324-335.
* Hodges & Lehmann (1960) Hodges, J. L., & Lehmann, E. L. (1960). Comparison of the normal scores and Wilcoxon tests. _Proceedings of Fourth Berkeley Symposium on Mathematical Statistics and Probability_ (Vol. 1, pp. 307-317).
* Hosking (1984) Hosking, J. R. M. (1984). Modelling persistence in hydrological time series using fractional differencing. _Water Resources Research, 20_, 1898-1908.
* Hwang (2002) Hwang, E. (2002). _Nonparametric estimation for nonlinear autoregressive processes_. Ph.D. Thesis, Indiana University, Bloomington.
* Lahiri (2003) Lahiri, S. N. (2003). _Resampling methods for dependent data_. New York: Springer.
* LeCam & Yang (1990) LeCam, L., & Yang, G. (1990). _Asymptotics in statistics. Some basic concepts_. New York: Springer.
* Moran (1923) Moran, G. M. (1923). A first study of the Tibetan skull. _Biometrika, 14_, 193-260.
* Mukherjee & Reid (2001) Mukherjee, R., & Reid, N. (2001). Comparison of test statistics via expected lengths of associated confidence intervals. _Journal of Statistical Planning and Inference, 97_(1), 141-151.
* Neyman & Pearson (1928) Neyman, J., & Pearson, E. S. (1928). On the use and interpretation of certain test criteria for purposes of statistical inference: Part I. _Biometrika, 20A_, 175-240.
* Phillips (1987) Phillips, P. C. B. (1987). Towards a unified asymptotic theory for autoregression. _Biometrika, 74_(3), 535-547.
* Pitman (1948) Pitman, E. J. G. (1948). _Lecture notes on nonparametric statistical inference_. New York: Columbia University.
* Sen & Singer (1979) Sen, P. R., & Singer, J. (1979). _Large simple methods in statistics: An introduction with applications_. New York: Chapman and Hall.
* Serfling (1980) Serfling, R. (1980). _Approximation theorems of mathematical statistics_. New York: Wiley.
* Shapiro & Wilk (1965) Shapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality. _Biometrika, 52_, 591-611.
* van der Vaart (1998) van der Vaart, A. (1998). _Asymptotic statistics_. Cambridge: Cambridge University Press.
* Wald (1943) Wald, A. (1943). Tests of statistical hypotheses concerning several parameters when the number of observations is large. _Transactions of the American Mathematical Society, 54_(3), 426-482.
* Wilks (1938) Wilks, S. S. (1938). The large-sample distribution of the likelihood ratios for testing composite hypotheses. _The Annals of Mathematical Statistics, 9_, 60-62.

## Chapter 9 The Nonparametric Bootstrap

### 9.1 What is "Bootstrap"? Why Use it?

We describe in this section an important methodology due to Efron (1979) to estimate distributions of statistics from data by resampling.

Suppose one needs to construct a confidence interval for a parameter \(\theta\) based on an estimator \(\hat{\theta}_{n}\) constructed from i.i.d. observations \(X_{1},X_{2},\ldots,X_{n}\) from a distribution \(P\). (That is, \(X_{i}\)'s have a common distribution \(P\).) If \(\sqrt{n}(\hat{\theta}_{n}-\theta)\stackrel{{\mathcal{L}}}{{ \longrightarrow}}N(0,\sigma^{2})\) as \(n\rightarrow\infty\) and \(\hat{\sigma}_{n}^{2}\) is a consistent estimator of \(\sigma^{2}\) based on \(X_{1},\ldots,X_{n}\), then a confidence interval of _approximate size_\(1-\alpha\) is given by

\[\left[\hat{\theta}_{n}-z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_{n}}{\sqrt{n}} \,,\,\hat{\theta}_{n}+z_{1-\frac{\alpha}{2}}\frac{\hat{\sigma}_{n}}{\sqrt{n}} \right], \tag{9.1}\]

where \(z_{\delta}\) is such that \(\text{Prob}(Z\leq z_{\delta})=\delta\), \(Z\) having the standard normal distribution \(N(0,1)\). The targeted size, or coverage probability, \(1-\alpha\) is called the _nominal coverage._ Since (9.1) is based on an asymptotic result, the _coverage error_ (\(=\) nominal coverage \(-\) true coverage probability) may be significant, especially when the sample size \(n\) in an actual experiment is not very large.

Let \(T_{n}(P)\) be a function of i.i.d. observations \(X_{1},\ldots,X_{n}\) (from a distribution \(P\)) and \(P\). For example, in the case of real-valued \(X_{i}\), \(T_{n}(P)\) may be \(\sqrt{n}(\overline{X}_{n}-\theta)/\hat{\sigma}_{n}\), which not only involves \(X_{i}\)'s (\(1\leq i\leq n\)), but also \(P\) via \(\theta:=\int xP(dx)\equiv\) mean of \(P\). First estimate \(P\) by its consistent and unbiased estimator \(\widehat{P}_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{X_{i}}\equiv\) the _empirical distribution_ (based on \(X_{1},\ldots,X_{n}\)). Note that \(\delta_{X_{i}}\) is the point mass at \(X_{i}\). Hence \(\widehat{P}_{n}(B)=\frac{1}{n}\sum_{i=1}^{n}1_{B}(X_{i})\equiv\) proportion of those observations which lie in \(B\) (for all measurable \(B\)): (1) \(E\widehat{P}_{n}(B)=E1_{B}(X_{i})=O\cdot(1-P(B))+1\cdot P(B)=P(B)\)_(unbiasedness)_, (2) \(\widehat{P}_{n}(B)\to E1_{B}(X_{i})=P(B)\) a.s. as \(n\rightarrow\infty\), by the strong law of large numbers _(consistency)_. We first consider the _percentile bootstrap._ To arrive at the bootstrap estimate follow the steps below.

**Step 1:**: Take a _random sample with replacement of size \(n\) from the empirical \(\widehat{P}_{n}\)_. Denote this by \(X_{1}^{*},X_{2}^{*},\ldots,X_{n}^{*}\).
**Step 2:**: Calculate \(T_{n}^{*}(\widehat{P}_{n})\), substituting \(X_{1}^{*},\ldots,X_{n}^{*}\) for \(X_{1},\ldots,X_{n}\), and \(P\) by \(\widehat{P}_{n}\) in the functional form \(T_{n}(P)\). For example, if \(T_{n}(P)=(\overline{X}_{n}-\theta)\), \(T_{n}^{*}(\widehat{P}_{n})=(\overline{X}_{n}^{*}-\overline{X}_{n})\) where \(\overline{X}_{n}^{*}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{*}\). Note that here \(\theta=\int xP(dx)\), \(\overline{X}_{n}=\int x\widehat{P}_{n}(dx)\).
**Step 3.**: Repeat independently Steps 1 and 2 a very large number of times, say \(M\), obtaining \(M\) independent values of the _bootstrapped statistic_\(T_{n}^{*}(\hat{P}_{n})\)l. (Typically \(M\) is between 500 and 2000).
**Step 4.**: Find the lower and upper \(\frac{\alpha}{2}\)-quantiles of the \(M\) values in Step 3. Call these \(q_{\frac{\alpha}{2}}^{*}\), \(q_{1-\frac{\alpha}{2}}^{*}\), respectively.
**Step 5.**: The bootstrap confidence interval for \(\theta\) is

\[\left[q_{\frac{\alpha}{2}}^{*},q_{1-\frac{\alpha}{2}}^{*}\right]. \tag{9.2}\]

To see that (9.2) is a confidence interval for \(\theta\) with an asymptotic level \(1-\alpha\) (almost surely, as \(n\to\infty\)), note that a classical confidence interval is given by the interval \([\hat{\theta}_{n}-z_{1-\alpha/2}\hat{\sigma}_{n},\hat{\theta}_{n}-z_{\alpha/2} \hat{\sigma}_{n}]=[\hat{\theta}_{n}+z_{\alpha/2}\hat{\sigma}_{n},\hat{\theta}_ {n}+z_{1-\alpha/2}\hat{\sigma}_{n}]=[l,u]\), say. Now the bootstrap version \(\theta_{n}^{*}\) of \(\hat{\theta}_{n}\) is, under the empirical \(P^{*}=\widehat{P}_{n}\), asymptotically Normal \(N(\hat{\theta}_{n},\hat{\sigma}_{n}^{2})\), so that the \(\alpha/2\)-th and \((1-\alpha/2)\)-th quantiles of \(\theta_{n}^{*}\), \(q_{\alpha/2}^{*}\) and \(q_{1-\alpha/2}^{*}\) say, are asymptotically equal to \(\hat{\theta}_{n}+z_{\alpha/2}\hat{\sigma}_{n}=l\) and \(\hat{\theta}_{n}+z_{1-\alpha/2}\hat{\sigma}_{n}=u\), respectively.

When the standard error \(\hat{\sigma}_{n}\) of \(\hat{\theta}_{n}\) is known in closed form, one may use the studentized or pivoted statistic \(T_{n}=(\hat{\theta}_{n}-\theta)/\hat{\sigma}_{n}\), which is asymptotically standard Normal \(N(0,1)\). The usual CLT-based symmetric confidence interval for \(\theta\) is given by

\[[\hat{\theta}_{n}+z_{\frac{\alpha}{2}}\hat{\sigma}_{n},\hat{\theta}_{n}+z_{1- \frac{\alpha}{2}}\hat{\sigma}_{n}]=[\hat{\theta}_{n}-z_{1-\frac{\alpha}{2}} \hat{\sigma}_{n},\hat{\theta}_{n}-z_{\frac{\alpha}{2}}\hat{\sigma}_{n}], \tag{9.3}\]

using \(P(|T_{n}|\leq z_{1-\alpha/2})=1-\alpha\). The corresponding _pivotal bootstrap confidence interval_ is based on the resampled values of \(T_{n}^{*}=(\hat{\theta}_{n}^{*}-\hat{\theta}_{n})/\hat{\sigma}_{n}^{*}\), where \(\hat{\sigma}_{n}^{*}\) is the bootstrap estimate of the standard error obtained by steps analogous to those described in the preceding paragraph. Let \(c_{\alpha/2}^{*}\) be such that \(P^{*}(|T_{n}^{*}|\leq c_{\alpha/2}^{*})=1-\alpha\). The bootstrap pivotal confidence interval for \(\theta\) is then

\[\left[\hat{\theta}_{n}-c_{\frac{\alpha}{2}}^{*}\hat{\sigma}_{n}^{*},\,\hat{ \theta}_{n}+c_{\frac{\alpha}{2}}^{*}\hat{\sigma}_{n}^{*}\right]. \tag{9.4}\]

Suppose \(\hat{\theta}_{n}\) is based on i.i.d. observations \(X_{1},\ldots,X_{n}\), whose common distribution has a density (or a nonzero density component), and that it is a smooth function of sample means of a finite number of characteristics of \(X\), or has a stochastic expansion (Taylor expansion) in terms of these sample means (such as the MLE in regular cases). It may then be shown that the _coverage error_ of the CLT-based interval (9.3) is \(O(n^{-1})\), while that based on (9.4) is \(O(n^{-3/2})\), a major advantage of the bootstrap procedure. The coverage error of the percentile interval (9.2) is \(O(n^{-1/2})\), irrespective of whether the distribution of \(X\) is continuous or discrete. Chapter 11, Part III provides a rigorous treatment of coverage errors.

One of the compelling arguments in favor of using the percentile bootstrap is that it does not require an analytical computation of the standard error \(\hat{\sigma}_{n}\) of the estimator \(\hat{\theta}_{n}\). In many problems such analytic computations are complex and difficult. Moreover, one may obtain a bootstrap estimate of the standard error simply as the standard deviation of the bootstrap estimates \(\hat{\theta}^{*}\).

### 9.2 When Does Bootstrap Work?

#### Linear Statistics, or Sample Means

Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. observations from a distribution \(P\). Consider a _linear statistic_\(T_{n}=\frac{1}{n}\sum_{i=1}^{n}g(X_{i})\) for some real-valued measurable function \(g\), which is used to estimate \(Eg(X_{1})=T(P)\). Let \(\widehat{P}_{n}\) denote the empirical, \(\widehat{P}_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{X_{i}}\). Let \(X_{n1}^{*},\ldots,X_{nn}^{*}\) be a bootstrap sample, i.e., conditionally given \(\widehat{P}_{n}\), \(X_{n1}^{*},X_{n2}^{*},\ldots,X_{nn}^{*}\) are i.i.d. with common distribution \(\widehat{P}_{n}\). For simplicity of notation, we will drop \(n\) from the subscript of the bootstrap observations and write \(X_{i}^{*}\) in place of \(X_{ni}^{*}\). Let \(\mathscr{L}(V)\) denote the _law, or distribution,_ of a random variable \(V\). Let \(d_{\infty}(Q_{1},Q_{2})\) denote the _Kolmogorov distance_ between two probability measures \(Q_{1},Q_{2}\) on \(\mathbb{R}\) with distribution functions \(F_{1},F_{2}\),

\[d_{\infty}(Q_{1},Q_{2})=\sup_{x}|F_{1}(x)-F_{2}(x)|. \tag{9.5}\]

**Theorem 9.1**.: _Assume \(0<\sigma^{2}\equiv\operatorname{var}g(X_{1})<\infty\). Then, with probability one,_

\[d_{\infty}\left(\mathscr{L}^{*}\left(\sqrt{n}(T_{n}-T(P))\right),\,\mathscr{L} \left(\sqrt{n}(T_{n}^{*}-T(\widehat{P}_{n}))\right)\right)\longrightarrow 0 \quad\text{as $n\to\infty$},\]

_where \(\mathscr{L}^{*}\) denotes law under \(\widehat{P}_{n}\)._

Proof.: We will prove the result under the additional assumption \(E|g(X_{1})|^{3}<\infty\). (The general case is proved under Remark 9.1.) It is enough to consider the case \(g(x)=x\). Write \(\mu=EX_{1}\), \(\sigma^{2}=\operatorname{var}X_{1}\), \(\rho_{3}=E|X_{1}-\mu|^{3}\). We will show that the \(\widehat{P}_{n}\)-distribution of \(\sqrt{n}(\overline{X}^{*}-\overline{X})\) approximates the \(P\)-distribution of \(\sqrt{n}(\overline{X}-\mu)\) (almost surely). Since the latter distribution converges to \(\Phi_{\sigma^{2}}(x)\), it is enough to show that the \(\widehat{P}_{n}\)-distribution of \(\sqrt{n}(\overline{X}^{*}-\overline{X})\) converges to \(\Phi_{\sigma^{2}}\) (almost surely, or in probability). Now, by the Berry-Esseen Theorem,1

Footnote 1: See, e.g., Bhattacharya and Rao (1976, pp. 110).

\[\left|\widehat{P}_{n}\left(\sqrt{n}(\overline{X}^{*}-\overline{X})\leq x \right)-\Phi_{\hat{\sigma}^{2}}(x)\right|\leq\ell_{n}^{*}, \tag{9.6}\]

where \(\hat{\sigma}^{2}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\), and

\[\ell_{n}^{*}=\frac{n^{-\frac{1}{2}}\frac{1}{n}\sum_{i=1}^{n}|X_{i}-\overline {X}|^{3}}{\hat{\sigma}^{3}}\ \longrightarrow\ 0\quad\text{a.s. as $n\to\infty$}.\]

To see this note that \(\hat{\sigma}^{2}=\frac{1}{n}\sum_{i=1}^{n}[(X_{i}-\mu)^{2}+(\overline{X}-\mu) ^{2}-2(X_{i}-\mu)(\overline{X}-\mu)]=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2} -(\overline{X}-\mu)^{2}\to E(X_{1}-\mu)^{2}=\sigma^{2}\) a.s. Also, \(\frac{1}{n}\sum_{i=1}^{n}|X_{i}-\overline{X}|^{3}\leq\frac{1}{n}\sum_{i=1}^{n} 2^{3-1}(|X_{i}-\mu|^{3}+|\overline{X}-\mu|^{3})\to 4\rho_{3}\) a.s., using the elementary inequality

\[|a+b|^{p}\leq 2^{p-1}\left(|a|^{p}+|b|^{p}\right)\quad\forall\ p\geq 1.\]

Hence \(\ell_{n}^{*}\to 0\) a.s. Next, \(\frac{1}{\sqrt{2\pi\hat{\sigma}^{2}}}\,e^{-z^{2}/2\hat{\sigma}^{2}}\longrightarrow \frac{1}{\sqrt{2\pi\hat{\sigma}^{2}}}\,e^{-z^{2}/2\sigma^{2}}\) as \(n\to\infty\), (for all \(\omega\), outside a set \(N\) of probability zero). Hence, by Scheffe's Theorem, \(\Phi_{\hat{\sigma}^{2}}(x)\to\Phi_{\sigma^{2}}(x)\) uniformly for all \(x\), outside \(N\)).

_Remark 9.1_.: Write \(\overline{g}=\frac{1}{n}\sum_{i=1}^{n}g(X_{i})\), \(s_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}(g(X_{i})-\overline{g})^{2}\). Now, conditionally given \(\widehat{P}_{n}\), \(Y_{n,i}^{*}:=(g(X_{ni}^{*})-\overline{g})\) is a triangular array of i.i.d. random variables with mean zero and variance \(s_{n}^{2}\) (\(1\leq i\leq n\); \(n\geq 1\)). Also, for each \(\varepsilon>0\),

\[E^{*}\sum_{i=1}^{n}\left(\left(\frac{Y_{n,i}^{*}}{\sqrt{n}}\right) ^{2}\cdot\mathbf{1}_{\left\{\left|\frac{Y_{n,i}^{*}}{\sqrt{n}}\right|> \varepsilon\right\}}\right) =E^{*}Y_{n,1}^{*^{2}}\mathbf{1}_{\{|Y_{n,1}^{*}|>\varepsilon \sqrt{n}\}}\] \[=\frac{1}{n}\sum_{i=1}^{n}(g(X_{i})-\overline{g})^{2}\mathbf{1}_{ \{|g(X_{i})-\overline{g}|>\varepsilon\sqrt{n}\}} \tag{9.7}\]

whose expectation

\[E\left[(g(X_{1})-\overline{g})^{2}\cdot\mathbf{1}_{\{|g(X_{1})-\overline{g}|> \varepsilon\sqrt{n}\}}\right]\]

goes to zero. This implies that there exists a sequence \(\varepsilon_{n}\downarrow 0\) such that the last expectation goes to zero with \(\varepsilon_{n}\) in place of \(\varepsilon\). One may now apply the Lindeberg-Feller central limit theorem. Since \(s_{n}^{2}\to\sigma^{2}\) by SLLN, it follows that the Kolmogorov distance between the (bootstrap) distribution of \(T_{n}^{*}=\Sigma_{i=1}^{n}Y_{n,i}^{*}\) (under \(\widehat{P}_{n}\)) and the distribution (under \(P\)) of \(T_{n}=\overline{g}-Eg(X_{1})\) goes to zero, a.s.

_Remark 9.2_.: It may be shown that for linear statistics \(T_{n}\), the _bootstrap approximation_ of the distribution is valid (i.e., _consistent_ in the Kolmogorov distance) if and only if \(T_{n}\) is asymptotically normal. A similar phenomenon holds for more general statistics (See Gine and Zinn 1989, 1990). Thus, roughly speaking, bootstrap works only when classical Gaussian approximation works (with relatively minor exceptions). Various counterexamples highlight the inconsistency of the bootstrap estimate when the conditions for the validity of classical asymptotics break down (See, e.g., Athreya 1987).

#### Smooth Functions of Sample Averages

Let \(Z_{i}=(f_{1}(X_{i}),f_{2}(X_{i}),\ldots,f_{k}(X_{i}))\) be a vector of sample characteristics of the \(i\)th observation (\(1\leq i\leq n\)). Here \(f_{j}\) are real-valued measurable functions such that \(Ef_{j}^{2}(X_{1})<\infty\) for \(1\leq j\leq k\). Let

\[\overline{Z} = \frac{1}{n}\sum_{i=1}^{n}Z_{i}=\left(\frac{1}{n}\sum_{i=1}^{n}f_{ 1}(X_{i}),\ldots,\frac{1}{n}\sum_{i=1}^{n}f_{k}(X_{i})\right),\] \[\mu := EZ_{i}=(Ef_{1}(X_{1}),\ldots,Ef_{k}(X_{1})). \tag{9.8}\]

Consider a statistic \(T_{n}=H(\overline{Z})\), where \(H\) is a continuously differentiable function in a neighborhood of \(\mu\) in \(\mathbb{R}^{k}\). We have seen earlier (using the delta method) that \(\widehat{T}_{n}-H(\mu)\) equals \((\overline{Z}-\mu)\cdot(\operatorname{grad}H)(\mu)+o_{p}(n^{-\frac{1}{2}})\). Therefore, one needs to look only at the linear statistic \((\overline{Z}-\mu)\cdot(\operatorname{grad}H)(\mu)\). The same is true for \(T_{n}^{*}-T_{n}=H(\overline{Z}^{*})-H(\overline{Z})\), conditionally given the empirical \(\widehat{P}_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{Z_{i}}\). Hence, by Remark 9.1 above,

\[d_{\infty}(\mathscr{L}^{*}(T_{n}^{*}-T_{n}),\mathscr{L}(T_{n}-H(\mu)))\longrightarrow 0 \text{ in probability}. \tag{9.9}\]

#### Linear Regression

Consider the regression model

\[Y\equiv\begin{pmatrix}Y_{1}\\ Y_{2}\\ \vdots\\ Y_{n}\end{pmatrix}=X\beta+\varepsilon, \tag{9.10}\]

where \(X\) is the known (nonrandom) \(n\times p\)_design matrix,_

\[X=\begin{bmatrix}X_{11}&X_{12}&\cdots&X_{1p}\\ X_{21}&X_{22}&\cdots&X_{2p}\\ &\cdots&\\ X_{n1}&X_{n2}&\cdots&X_{np}\end{bmatrix} \tag{9.11}\]

\(\beta=(\beta_{1}\beta_{2}\ldots\beta_{p})^{\prime}\) is the \(p\times 1\) column vector of unknown parameters to be estimated, and \(\boldsymbol{\varepsilon}=(\varepsilon_{1},\varepsilon_{2},\ldots, \varepsilon_{n})^{\prime}\) is the \(n\times 1\) column vector of i.i.d. random variables satisfying

\[E\varepsilon_{i}=0,\quad 0<\sigma^{2}\equiv E\varepsilon_{i}^{2}<\infty. \tag{9.12}\]

We will make the standard assumption that \(X\) is of full rank \(p\) (\(n\geq p\)). Then the least square estimator \(\hat{\beta}\) of \(\beta\) is given by

\[\hat{\beta} =(X^{\prime}X)^{-1}X^{\prime}Y\equiv(X^{\prime}X)^{-1}X^{\prime}( X\beta+\varepsilon)\] \[=\beta+(X^{\prime}X)^{-1}X^{\prime}\varepsilon, \tag{9.13}\]

so that \(\hat{\beta}-\beta=(X^{\prime}X)^{-1}X^{\prime}\varepsilon\), and the covariance matrix of \(\hat{\beta}\) is given by

\[\operatorname{cov}\hat{\beta}=(X^{\prime}X)^{-1}X^{\prime}\sigma^{2}IX(X^{ \prime}X)^{-1}=\sigma^{2}(X^{\prime}X)^{-1}. \tag{9.14}\]

The estimated residuals are given by

\[\hat{\varepsilon}_{is}:=\hat{\varepsilon}_{i}-\overline{\hat{ \varepsilon}},\quad\text{with}\enspace\hat{\varepsilon}_{i} =Y_{i}-X_{i}\hat{\beta}\quad(1\leq i\leq n)\text{ and}\] \[\overline{\hat{\varepsilon}} =\frac{1}{n}\sum_{i=1}^{n}\hat{\varepsilon}_{i}, \tag{9.15}\]

where \(X_{i}\) is the \(i\)th row vector of \(X\)

\[X_{i}=(X_{i1},X_{i2},\ldots,X_{ip})\qquad(1\leq i\leq n). \tag{9.16}\]

The _shifted empirical distribution of the residuals_ is \(\widetilde{P}_{n}:=\frac{1}{n}\sum_{i=1}^{n}\delta_{\varepsilon_{is}}\). Let

\[T_{n} =(X^{\prime}X)^{\frac{1}{2}}(\hat{\beta}-\beta)\equiv(X^{\prime}X )^{-\frac{1}{2}}X^{\prime}\varepsilon\] \[T_{n}^{*} =(X^{\prime}X)^{\frac{1}{2}}(\beta^{*}-\hat{\beta})=(X^{\prime}X) ^{-\frac{1}{2}}X^{\prime}\varepsilon^{*}, \tag{9.17}\]

Where \(\boldsymbol{\varepsilon}^{*}=(\varepsilon_{1}^{*},\ldots,\varepsilon_{n}^{*} )^{\prime}\) with \(\varepsilon_{i}^{*}\) (\(1\leq i\leq n\)) i.i.d. with common distribution \(\widetilde{P}_{n}\) (conditionally given the estimated residuals \(\hat{\varepsilon}_{i}\) (\(1\leq i\leq n\))), and

\[\beta^{*}=\hat{\beta}+(X^{\prime}X)^{-1}X^{\prime}\boldsymbol{\varepsilon}^{*}. \tag{9.18}\]We will show that the bootstrapped distribution of \(T_{n}^{*}\) is consistent for the distribution of \(T_{n}\), under an appropriate condition on \(X\).

For this let us introduce the _Mallows distance_\(d_{2}\) on the space \(\mathscr{P}\) (\(\mathbb{R}^{k}\)) of probability measures on (the Borel sigma-field of) \(\mathbb{R}^{k}\):

\[d_{2}(P_{1},P_{2})=(\inf E\|U-V\|^{2})^{\frac{1}{2}}\qquad(P_{1},P_{2}\in \mathscr{P}(\mathbb{R}^{k})), \tag{9.19}\]

where the infimum is over all random vectors \(U,V\) (defined on some probability space, arbitrary) such that \(U\) has distribution \(P_{1}\) and \(V\) has distribution \(P_{2}\). Equivalently, one could define \(d_{2}^{2}(P_{1},P_{2})=\inf\int_{\mathbb{R}^{2k}}\|u-v\|^{2}Q(dudv)\), where the infimum is over all distributions \(Q\) on \(\mathbb{R}^{2k}=\mathbb{R}^{k}\times\mathbb{R}^{k}\), such that the distribution of the vectors of the first \(k\) coordinates \(u\) is \(P_{1}\) and that of the last \(k\) coordinates \(v\) is \(P_{2}\). It is known (See Bickel and Freedman 1981) that

\[d_{2}(P_{n},P)\longrightarrow 0\Longleftrightarrow \text{(i) }P_{n}\to P\text{ weakly,}\] \[\text{and}\quad\text{(ii) all first and second order moments of }P_{n}\text{ converge}\] \[\text{to those of }P. \tag{9.20}\]

For the following results due to Bickel and Freedman (1983), let \(\pi_{n}(P)\) denote the distribution of \(T_{n}\) under a common distribution \(P\) of the i.i.d. residuals \(\varepsilon_{i}\), and \(\pi_{n,c}(P)\) the corresponding distribution of \(c^{\prime}T_{n}\), for \(c\in\mathbb{R}^{p}\).

**Proposition 9.1**.: _Let \(X^{\prime}X\) be nonsingular, and \(P\) and \(Q\) two probability measures for the residuals \(\varepsilon_{i}\) such that \(E\varepsilon_{i}=0\), \(E\varepsilon_{i}^{2}<\infty\) under both \(P\) and \(Q\). Then_

\[d_{2}(\pi_{n}(P),\pi_{n}(Q))\leq\sqrt{p}\,d_{2}(P,Q), \tag{9.21}\]

_and_

\[d_{2}(\pi_{n,c}(P),\pi_{n,c}(Q))\leq\|c\|d_{2}(P,Q).\]

Proof.: Let \((U_{i},V_{i})\), \(1\leq i\leq n\), be i.i.d. with \(\mathscr{L}(U_{i})=P\), \(\mathscr{L}(V_{i})=Q\). Write \(U=(U_{1},\ldots,U_{n})^{\prime}\), \(V=(V_{1},\ldots,V_{n})^{\prime}\), \(Z=X(X^{\prime}X)^{-\frac{1}{2}}\). Let 'inf' below be the infimum over all such \(U,V\). Then

\[d_{2}^{2}(\pi_{n}(P),\pi_{n}(Q))\leq\inf E\|Z^{\prime}(U-V)\|^{2}\] \[= \inf\sum_{r=1}^{p}E\left(\sum_{i=1}^{n}Z_{ir}(U_{i}-V_{i})\right) ^{2}=\left(\sum_{r=1}^{p}\sum_{i=1}^{n}Z_{ir}^{2}\right)\inf\,E(U_{i}-V_{i})^ {2}\] \[= (\text{Trace of }Z^{\prime}Z)d_{2}^{2}(P,Q)=pd_{2}^{2}(P,Q),\]

since \(Z^{\prime}Z\) is the \(p\times p\) identity matrix. This proves (9.21). Similarly,

\[d_{2}^{2}(\pi_{n,c}(P),\pi_{n,c}(Q)) \leq \inf\,E(c^{\prime}Z^{\prime}(U-V))^{2}\] \[= \inf E(c^{\prime}Z^{\prime}(U-V)\cdot(U-V)^{\prime}Zc)\] \[= \inf(c^{\prime}Z^{\prime}\{E(U-V)(U-V)^{\prime}\}Zc)\] \[= \inf\,c^{\prime}Z^{\prime}E(U_{1}-V_{1})^{2}I_{n\times n}Zc=\inf \,c^{\prime}Z^{\prime}E(U_{1}-V_{1})^{2}Zc\] \[= (c^{\prime}Zc)d_{2}^{2}(P,Q)=c^{\prime}cd_{2}^{2}(P,Q).\]

**Theorem 9.2**.: _Suppose that \(X^{\prime}X\) is nonsingular for all sufficiently large \(n\), \(E\varepsilon_{i}=0\), \(0<\sigma^{2}=E\varepsilon_{i}^{2}<\infty\). Then_

\[d_{2}(\pi_{n}(\widetilde{P}_{n}),\pi_{n}(P))\longrightarrow 0\text{ in probability, if }\frac{p+1}{n}\to 0. \tag{9.22}\]

Proof.: First note that, writing \(\overline{\varepsilon}=\frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}\), \(\overline{\varepsilon}=\frac{1}{n}\sum_{i=1}^{n}\hat{\varepsilon}_{i}\),

\[d_{2}^{2}(\widetilde{P}_{n},\widehat{P}_{n}) \leq n^{-1}\sum_{i=1}^{n}\left[(\hat{\varepsilon}_{i}-\overline{ \hat{\varepsilon}})-\varepsilon_{i}\right]^{2}\] \[=n^{-1}\sum_{i=1}^{n}\left[\hat{\varepsilon}_{i}-\varepsilon_{i}- (\overline{\hat{\varepsilon}}-\overline{\varepsilon})\right]^{2}=n^{-1}\sum_{ i=1}^{n}\left[\hat{\varepsilon}_{i}-\varepsilon_{i}-(\overline{\hat{\varepsilon}}- \overline{\varepsilon})\right]^{2}+\overline{\varepsilon}^{2}\] \[\leq n^{-1}\sum_{i=1}^{n}(\hat{\varepsilon}_{i}-\varepsilon_{i}) ^{2}+\overline{\varepsilon}^{2}\] \[=n^{-1}\sum_{i=1}^{n}(X_{i}(\hat{\beta}-\beta))^{2}+\overline{ \varepsilon}^{2}\] \[=n^{-1}\sum_{i=1}^{n}\sum_{r,r^{\prime}=1}^{p}X_{ir}X_{ir^{\prime }}(\hat{\beta}_{r}-\beta_{r})(\hat{\beta}_{r^{\prime}}-\beta_{r^{\prime}})+ \overline{\varepsilon}^{2}\] \[=\sum_{r,r^{\prime}=1}^{p}\left(n^{-1}\sum_{i=1}^{n}X_{ir}X_{ir^ {\prime}}\right)(\hat{\beta}_{r}-\beta_{r})(\hat{\beta}_{r^{\prime}}-\beta_{r^ {\prime}})+\overline{\varepsilon}^{2}\] \[=\frac{1}{n}(\hat{\beta}-\beta)^{\prime}X^{\prime}X(\hat{\beta}- \beta)+\overline{\varepsilon}^{2}=\frac{1}{n}\varepsilon^{\prime}X(X^{\prime} X)^{-1}X^{\prime}X(X^{\prime}X)^{-1}X^{\prime}\varepsilon+\overline{\varepsilon}^{2}\] \[=\frac{1}{n}\,\varepsilon^{\prime}X(X^{\prime}X)^{-1}X^{\prime} \varepsilon+\overline{\varepsilon}^{2}. \tag{9.23}\]

Therefore,

\[Ed_{2}^{2}(\widetilde{P}_{n},\widehat{P}_{n})\leq\frac{\sigma^{2}}{n}(\text{ Trace of }X(X^{\prime}X)^{-1}X^{\prime})+\frac{\sigma^{2}}{n}=\frac{\sigma^{2}}{n}(p+1). \tag{9.24}\]

To see this, let \(A=X(X^{\prime}X)^{-1}X^{\prime}\). Then \(A^{\prime}=A\) and \(A^{2}=A\). Hence the eigenvalues of \(A\) are \(0\) or \(1\), and the number of \(1\)'s equals the rank of \(A\). Clearly Rank \(A\leq p\). But \(AX=X\), the \(p\) column vectors of \(X\) are eigenvectors of \(A\) with eigenvalues and \(X\) has rank \(p\). Hence Rank \(A=p\).

Using (9.24) and the fact (See (9.20))

\[d_{2}(\widehat{P}_{n},P)\longrightarrow 0\text{ in probability,} \tag{9.25}\]

one arrives at

\[d_{2}(\widetilde{P}_{n},P)\longrightarrow 0\text{ in probability if }\frac{p+1}{n}\longrightarrow 0. \tag{9.26}\]

_Remark 9.3_.: If one lets \(p\) be fixed (as is usually the case), then the bootstrap estimate of the distribution of \(T_{n}=(X^{\prime}X)^{\frac{1}{2}}(\hat{\theta}-\theta)\) is consistent if \(X^{\prime}X\) is nonsingular. On the other hand, \(T_{n}\) is asymptotically Normal if and only if (i) \(X^{\prime}X\) is nonsingular _and_ (ii) the maximum diagonal element of \(A=X(X^{\prime}X)^{-1}X^{\prime}\) goes to zero as\(n\to\infty\) (See Theorem 6.4, Chap. 6). Thus the bootstrap estimate may be consistent even in the case the CLT does not hold. This is a rather exceptional example of the consistency of the bootstrap in the absence of a valid Normal approximation (See Remark 9.2).

### Notes and References

The main sources for this chapter are Efron (1979), who introduced the bootstrap, and Efron and Tibshirani (1994) where one can find detailed procedures for various bootstrapping techniques and many applications with real data. Section 9.2.3 is due to Bickel and Freedman (1981, 1983). More extensive references to the bootstrap appear in Chap. 11.

### Exercises for Chap. 9

#### Exercises for Sects. 9.1, 9.2

**Ex. 9.1**.: We reproduce from Diaconis and Efron (1983) the following data of \(Y=LSAT\) (the average score of the entering class of the law school on a national test), and \(X=GPA\) (the average grade-point average for the class), of \(n=15\) randomly chosen law schools from a population of \(N=82\) participating law schools in the U.S.

* Compute the sample correlation coefficient \(r=r_{X,Y}\).
* Take \(M=200\) (bootstrap) samples of size \(n=15\) each from the above data and compute the corresponding \(200\) bootstrap values \(r_{j}^{*}\) (\(j=1,\ldots,200\)) of the correlation coefficient.
* Construct a \(90\,\%\) percentile bootstrap confidence interval for the population coefficient of correlation \(\rho=\rho_{X,Y}\). [True value of \(\rho\) is \(0.761\).]
* Find the bootstrap estimate of the standard error of \(r\).
* Construct a classical CLT based confidence interval for \(\rho\) based on the above sample of size \(n=15\) [Hint: Use (6.67) to estimate the standard error of \(r\)].

**Ex. 9.2**.: Consider the data in Exercise 9.1, and assume that the regression of \(Y\) on \(X\) is linear: \(Y=\alpha+\beta x+\varepsilon\), with \(\varepsilon\) having mean zero and finite variance.

* Find the least squares estimates \(\hat{\alpha}\), \(\hat{\beta}\).
* Find a \(90\,\%\) percentile bootstrap confidence interval for \(\beta\), and one for \(\alpha\).

**Ex. 9.3**.: Consider the AR(2) model \(X_{n}=1+\frac{3}{2}X_{n-1}-\frac{9}{16}X_{n-2}+\varepsilon_{n}\) (\(n\geq 2\)), with \(\varepsilon_{n}\) (\(n\geq 2\)) i.i.d. \(N(0,1)\) and independent of \((X_{0},X_{1})\).

1. Show that the model satisfies the stability condition (8.170) and find the stationary distribution \(\pi\) of the Markov process \(Y_{n}=(X_{n-1},X_{n})^{\prime}\), \(n\geq 1\). In particular, show that \(\pi\) is Gaussian.
2. Show that under \(\pi\), \(\{X_{n}:n\geq 0\}\) is a stationary Gaussian process.
3. Draw a random sample \((X_{0},X_{1},\ldots,X_{100})\) of the time series from the above AR(2) model.
4. Suppose one is told that the data in (c) came from a stable stationary A(2) model \(X_{n}=\alpha+\beta_{1}X_{n-1}+\beta_{2}X_{n-2}+\varepsilon_{n}\) with an i.i.d. Gaussian \(N(0,\sigma^{2})\). Compute the maximum likelihood estimators \(\hat{\alpha}\), \(\hat{\beta}_{1}\), \(\hat{\beta}_{2}\), \(\hat{\sigma}^{2}\) of \(\alpha\), \(\beta_{1}\), \(\beta_{2}\), \(\sigma^{2}\).
5. Construct a classical (i.e., CLT based) \(95\,\%\) confidence interval for \(\beta_{2}\) in (d) above.
6. Construct a \(95\,\%\) percentile bootstrap confidence interval for \(\beta_{2}\), by (i) resampling from the _estimated residuals_\(\hat{\varepsilon}_{j}\) (\(2\leq j\leq 100\)) given by (8.177), with sample size \(90\), (ii) plugging the bootstrap sample \(\varepsilon_{j}^{*}\) (\(2\leq n\leq 100\)) in the formula \(X_{n}^{*}=\hat{\alpha}+\hat{\beta}_{1}X_{n-1}^{*}+\varepsilon_{n}^{*}\) (\(2\leq n\leq 100\)), beginning with \(X_{0}^{*}=X_{0}\), \(X_{1}^{*}=X_{1}\), to obtain a bootstrap sample \((X_{0}^{*},X_{1}^{*},\ldots,X_{100}^{*})\) of the sequence \((X_{0},X_{1},\ldots,X_{100})\), (iii) computing the bootstrap value of \(\beta_{2}^{*}\) of the statistic \(\hat{\beta}_{2}\) using (d), and (iv) repeating (i)-(iii) many times (say \(M=500\) times) to obtain the quantiles \(q_{\alpha/2}^{*}\), \(q_{1-\alpha/2}^{*}\) (\(\alpha=0.025\)) of the \(M\) values of \(\beta_{2}^{*}\) so obtained.

## References

* Athreya (1987) Athreya, K. B. (1987). Bootstrap of the mean in the infinite variance case. _Annals of Statistics, 15_(2), 724-731.
* Bhattacharya & Waymire (1990) Bhattacharya, R., & Waymire, E. C. (1990). _Stochastic processes with applications_. Philadelphia: SIAM.
* Bickel & Freedman (1981) Bickel, P. J., & Freedman, D. A. (1981). Some asymptotic theory for the Bootstrap. _Annals of Statistics, 9_(6), 1196-1217.
* Bickel & Freedman (1983) Bickel, P. J., & Freedman, D. A. (1983). Bootstrapping regression models with many parameters. In _A festschrift for Erich L. Lehmann_ (pp. 28-48). Belmont: Wadsworth.
* Efron (1979) Efron, B. (1979). Bootstrap methods: Another look at the jackknife. _Annals of Statistics, 7_, 1-26.
* Efron & Tibshirani (1994) Efron, B., & Tibshirani, T. (1994). _An introduction to the bootstrap_. New York: Chapman and Hall.
* Gine & Zinn (1989) Gine, E., & Zinn, J. (1989). Necessary conditions for the bootstrap of the mean. _Annals of Statistics, 17_(2), 684-691.
* Gine & Zinn (1990) Gine, E., & Zinn, J. (1990). Bootstrapping general empirical measures. _Annals of Statistics, 18_(2), 851-869.

## Chapter 10 Nonparametric Curve Estimation

### 10.1 Nonparametric Density Estimation

Let \(X_{1},X_{2},\ldots,X_{n}\) be a random sample from a distribution \(P\) on the real line. A consistent estimator of \(P\) is the _empirical_\(\widehat{P}_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{X_{i}}\), assigning mass \(\frac{1}{n}\) to each of the points \(X_{1},X_{2},\ldots,X_{n}\). By the Strong Law of Large Numbers, \(\widehat{P}_{n}(B)\equiv\frac{1}{n}\sum_{i=1}^{n}\mathbf{1}_{B}(X_{i})\to E \mathbf{1}_{B}(X_{1})=P(B)\) almost surely, as \(n\to\infty\), for each Borel set \(B\). With a little effort one can prove (the _Glivenko-Cantelli Theorem_):

\[\sup_{x\in\mathbb{R}}\left|\widehat{F}_{n}(x)-F(x)\right|\ \longrightarrow\ 0 \quad\text{as $n\to\infty$, almost surely.} \tag{10.1}\]

Here \(\widehat{F}_{n}(x)\equiv\widehat{P}_{n}((-\infty,x])\) is the cumulative distribution function of \(\widehat{P}_{n}\) and \(F(x)\) that of \(P\). If \(F\) is continuous on \(\mathbb{R}\) then one can prove, in addition to (10.1), that

\[\sup_{x\in\mathbb{R}}\sqrt{n}\left|\widehat{F}_{n}(x)-F(x)\right|\stackrel{{ \mathscr{L}}}{{\longrightarrow}}W, \tag{10.2}\]

where \(W=\max\{|B_{t}^{*}|:0\leq t\leq 1\}\), \(\{B_{t}^{*}:0\leq t\leq 1\}\) being the so-called _Brownian Bridge_ (See, e.g., Bhattacharya and Waymire 1990, pp. 37-39). One may use (10.2) to obtain a confidence band for \(F(\cdot)\).

Suppose now that \(F\) is absolutely continuous with a density \(f\). Since \(\widehat{P}_{n}\) is discrete, to estimate \(f\) one may use the density of the random variable \(\widehat{X}+hZ\) where \(\widehat{X}\) has the distribution \(\widehat{P}_{n}\) (conditionally, given \(X_{1},\ldots,X_{n}\)) and \(Z\) is independent of \(\widehat{X}\) and has a nice density, say, \(K\), and \(h\) is a small positive number, called the _bandwidth_ satisfying

\[h\equiv h_{n}\longrightarrow 0\quad\text{as $n\to\infty$.} \tag{10.3}\]

Note that \(\widehat{X}+hZ\) has the density

\[\hat{f}_{n}(x)=\frac{1}{n}\sum_{i=1}^{n}K_{h}(x-X_{i}), \tag{10.4}\]where \(K_{h}\) is the density of \(hZ\), namely,

\[K_{h}(y)=\frac{1}{h}K\left(\frac{y}{h}\right). \tag{10.5}\]

To see that \(\hat{f}_{n}\) is the density of \(\widehat{X}+hZ\), note that the latter has the distribution function

\[\widetilde{\rm Prob}(\widehat{X}+hZ\leq x)=\widetilde{E}[\widetilde {\rm Prob}(hZ\leq x-\widehat{X}|\widehat{X})]\] \[=\frac{1}{n}\sum_{i=1}^{n}\int_{-\infty}^{x-X_{i}}K_{h}(u)du, \tag{10.6}\]

where the superscript \(\widetilde{\phantom{a}}\) indicates that the probabilities are computed given \(X_{1},\ldots,X_{n}\). Differentiating (10.6) w.r.t. \(x\) one arrives at (10.4).

Alternatively, one may think of (10.4) as obtained by spreading out the point mass \(\delta_{X_{i}}\) with a density centered at \(X_{i}\) and concentrating most of the density near \(X_{i}\) (\(1\leq i\leq n\)). Now we show that the _bias_\(E\hat{f}_{n}(x)-f(x)\to 0\). For this write

\[E\hat{f}_{n}(x) =E\,\frac{1}{n}\,\sum_{i=1}^{n}\frac{1}{h}K\left(\frac{x-X_{i}}{h }\right)=E\,\frac{1}{h}\,K\left(\frac{x-X_{1}}{h}\right)\] \[=\frac{1}{h}\int_{-\infty}^{\infty}K\left(\frac{x-y}{h}\right)f(y)dy\] \[=\int_{-\infty}^{\infty}K(v)f(x-vh)dv\longrightarrow f(x)\ {\rm as }\ h\downarrow 0, \tag{10.7}\]

_if \(f\) is bounded and is continuous at \(x\)_ (by the Lebesgue Dominated Convergence Theorem). Also,

\[{\rm var}(\hat{f}_{n}(x)) =\frac{1}{n}\,{\rm var}\,K_{h}(x-X_{1})\] \[=\frac{1}{n}\left\{\frac{1}{h^{2}}\,EK^{2}\left(\frac{x-X_{1}}{h }\right)-\left(E\,\frac{1}{h}\,K\left(\frac{x-X_{1}}{h}\right)\right)^{2}\right\}\] \[=\frac{1}{nh}\int_{-\infty}^{\infty}\!\!\!K^{2}(v)f(x-vh)dv-\frac {1}{n}\left(\int_{-\infty}^{\infty}\!\!\!K(v)f(x-vh)dv\right)^{2}\longrightarrow 0, \tag{10.8}\]

if (1) \(f\) is bounded, (2) \(f\) is continuous at \(x\), (3) \(K^{2}(v)\) is integrable,

\[nh\longrightarrow\infty\quad{\rm and}\quad h\longrightarrow 0\qquad({\rm as }\ n\to\infty). \tag{10.9}\]

Thus, under the hypothesis (1)-(3) and (10.9), one has (by (10.7) and (10.8))

\[E\left(\hat{f}_{n}(x)-f(x)\right)^{2}={\rm var}\,\hat{f}_{n}(x)+({\rm Bias}\, \hat{f}_{n}(x))^{2}\longrightarrow 0\quad{\rm as}\ n\to\infty. \tag{10.10}\]

In other words, under the above assumptions, \(\hat{f}_{n}(x)\to f(x)\)_in probability_ as \(n\to\infty\).

Note that the convergences (10.7) and (10.8) do not really require boundedness of \(f\) on all of \(\mathbb{R}\). For example, if one takes \(K\) to have a compact support then it is enough to require that \(f\) is continuous at \(x.\) We have proved that under mild assumptions the so-called _kernel estimator_ (with _kernel_\(K\)) is a consistent estimator of \(f\) at every point of continuity of \(f\).

By choosing an appropriately smooth and _symmetric_ kernel \(K\) one may make the error of approximation \(\hat{f}_{n}(x)-f(x)\) reasonably small.

A measure of the (squared) error of approximation is provided by the so-called _mean integrated squared error,_ or MISE, given by

\[\text{MISE}(\hat{f}_{n})=\int_{\mathbb{R}}E[\hat{f}_{n}(x)-f(x)]^{2}dx=\int_{ \mathbb{R}}[\text{var}\,\hat{f}_{n}(x)+(\text{Bias}\,\,\hat{f}_{n}(x)]^{2}dx. \tag{10.11}\]

Write

\[c_{1}=\int v^{2}K(v)dv,\quad c_{2}=\int K^{2}(v)dv,\quad c_{3}=\int(f^{\prime \prime}(x))^{2}dx, \tag{10.12}\]

and assume \(c_{1},c_{2},c_{3}\) are finite and that

\[\int K(v)dv=1,\quad\int vK(v)dv=0. \tag{10.13}\]

Now it follows from (10.7) that

\[E\hat{f}_{n}(x) =\int_{\mathbb{R}}K(v)\left[f(x)-vhf^{\prime}(x)+\frac{v^{2}h^{2 }}{2}\,f^{\prime\prime}(x)+o(h^{2})\right]dv\] \[=f(x)+\frac{c_{1}h^{2}}{2}\,f^{\prime\prime}(x)+o(h^{2}),\] \[(\text{Bias}\,\hat{f}_{n})(x) =\frac{c_{1}h^{2}}{2}\,f^{\prime\prime}(x)+o(h^{2}), \tag{10.14}\]

if \(f^{\prime\prime}\) is continuous and bounded. Then

\[\int_{\mathbb{R}}(\text{Bias}\,\hat{f}_{n})^{2}(x)dx=\frac{c_{3}c_{1}^{2}h^{4 }}{4}+o(h^{4}). \tag{10.15}\]

Next, by (10.8) and (10.9),

\[\text{var}\,\hat{f}_{n}(x) =\frac{c_{2}f(x)}{nh}+O\left(\frac{1}{n}\right),\] \[\int_{\mathbb{R}}\text{var}\,\hat{f}_{n}(x)dx =\frac{c_{2}}{nh}+O\left(\frac{1}{n}\right),\text{ as }n\to\infty. \tag{10.16}\]

Hence

\[\text{MISE}(\hat{f}_{n})=\frac{c_{1}^{2}c_{3}}{4}\,h^{4}+\frac{c_{2}}{nh}+o(h^ {4})+O\left(\frac{1}{n}\right). \tag{10.17}\]

Neglecting the two smaller order terms, the asymptotically optimal choice of the bandwidth \(h\), for a given kernel \(K\) as above, is obtained by

\[h_{n}=\arg\min_{h}\left\{\frac{c_{1}^{2}c_{3}}{4}\,h^{4}+\frac{c_{2}}{nh} \right\}=\left(\frac{c_{2}}{c_{1}^{2}c_{3}}\right)^{\frac{1}{5}}n^{-\frac{1}{5 }}. \tag{10.18}\]The corresponding asymptotically minimal MISE is

\[\operatorname{MISE}\hat{f}_{n}=\frac{c_{4}}{n^{4/5}}+o\left(n^{-4/5}\right)\quad c _{4}:=\frac{5}{4}\left(c_{1}^{2/5}c_{2}^{4/5}c_{3}^{1/5}\right). \tag{10.19}\]

We have arrived at the following result.

**Theorem 10.1**.: _Assume \(f^{\prime\prime}\) is continuous and bounded. Then for any choice of a symmetric kernel \(K\) satisfying (10.13), and \(0<c_{i}<\infty\)\((i=1,2,3)\), the asymptotically optimal bandwidth \(h\) is given by the extreme right side of (10.18), and the asymptotically minimal MISE is given by (10.19)._

From the expression (10.4) it follows that \(\hat{f}_{n}(x)\) is, for each \(n\), a sum of i.i.d. random variables. By the Lindeberg CLT it now follows that, under the hypothesis of Theorem 10.1 one has (Exercise 10.1)

\[\frac{\hat{f}_{n}(x)-E\hat{f}_{n}(x)}{\sqrt{\operatorname{var}\hat{f}_{n}(x)} }\ \stackrel{{\mathscr{L}}}{{\longrightarrow}}\ N(0,1),\quad \text{if }f(x)>0. \tag{10.20}\]

Also check, using (10.14), (10.15) and (10.18), that

\[\frac{E\hat{f}_{n}(x)-f(x)}{\sqrt{\operatorname{var}\hat{f}_{n}(x)}}\ \longrightarrow\ c_{3}^{-\frac{1}{2}}\,\frac{f^{\prime\prime}(x)}{\sqrt{f(x)}}=\gamma,\quad\text{say, if }f(x)>0. \tag{10.21}\]

Hence

\[\frac{\hat{f}_{n}(x)-f(x)}{\sqrt{\operatorname{var}\hat{f}_{n}(x)}}\ \stackrel{{\mathscr{L}}}{{\longrightarrow}}\ N(\gamma,1)\quad \text{if }f(x)>0. \tag{10.22}\]

To remove the asymptotic bias \(\gamma\), one may choose a slightly sub-optimal bandwidth \(h_{n}=o(n^{-\frac{1}{5}})\) (Exercise 10.2). Since \(\operatorname{var}\hat{f}_{n}(x)\) involves \(f^{\prime\prime}(x)\), for setting confidence regions for \(f(x)\), one may resort to bootstrapping (See Hall 1992).

_Remark 10.1_.: It has been shown by Epanechnikov (1969) that the constant \(c_{4}\) in MISE is minimized (under the hypothesis of Theorem 10.1) by the kernel

\[K(v)=\frac{3}{4\sqrt{5}}\left(1-\frac{1}{5}\,v^{2}\right)\mathbf{1}_{\{|v| \leq\sqrt{5}\}}. \tag{10.23}\]

However, the loss of efficiency is rather small if, instead of (10.23), one chooses any symmetric kernel with high concentration, such as the (standard) Normal density or the triangular density (See Lehmann 1999, pp. 415, 416).

_Remark 10.2_ (Optimal Choice of Bandwidth and Cross Validation).: The "optimal" \(h_{n}\) given by (10.18) is not usable in practice because it involves the second derivative of the unknown density \(f\) (see (10.12)). Following (Tsybakov 2009, pp. 28, 29), we now describe a practical choice of an optimal bandwidth \(h(CV)\) given by

\[h(CV) = \arg\min_{h>0}CV(h),\] \[CV(h) := \int\hat{f}_{n}^{2}(x)dx-\frac{2}{n}\sum_{i=1}^{n}\hat{f}_{n,-i}( X_{i}), \tag{10.24}\]where \(\hat{f}_{n,-i}(x)\equiv((n-1)h)^{-1}\sum_{j\neq i}K(\frac{x-X_{j}}{h})\) is the kernel estimate of \(f(x)\) using the \(n-1\) observations \(\{X_{j}:j\neq i\}\) omitting \(X_{i}\). The quantity \(CV(h)\) in (10.24) is called the _cross-validation_. To prove this optimality observe that \(MISE(\hat{f}_{n})=E\int(\hat{f}_{n}(x)-f(x))^{2}dx=E\int\hat{f}_{n}^{2}(x)dx-2E \int f(x)\hat{f}_{n}(x)dx+\int f^{2}(x)dx\). Of the three terms, \(\int f^{2}(x)dx\) does not involve \(h\), and \(\int\hat{f}_{n}^{2}(x)dx\) is an unbiased estimate of its expectation. We now seek an unbiased estimate of the middle term \(E\int f(x)\hat{f}_{n}(x)dx\), and this is provided by \(\widehat{A}=n^{-1}\sum_{i=1}^{n}\hat{f}_{n,-i}(X_{i})\) (See Exercise 10.4). Thus \(E(CV(h))+\int f^{2}(x)dx=MISE(\hat{f}_{n})\), so that the functions \(E(CV(h))\) and \(MISE(\hat{f}_{n})\) have the same minimizing \(h\).

The asymptotic theory presented above has extensions to the multidimensional case. We provide a brief sketch of the arguments here, leaving the details to Exercise 10.3. Let \(f\) be a probability density function on \(\mathbb{R}^{d}\), and let \(K\) be a symmetric kernel density with finite second moments \(c_{1,i,j}=\int v_{i}v_{j}K(\mathbf{v})d\mathbf{v}\), and with \(c_{2}=\int K^{2}(\mathbf{v})d\mathbf{v}\). Then if \(f^{\prime\prime}\) is continuous and bounded, one uses the kernel estimate

\[\hat{f}_{n}(\mathbf{x})=\frac{1}{nh^{d}}\sum_{i=1}^{n}K\left(\frac{\mathbf{x}-X _{i}}{h}\right), \tag{10.25}\]

based on i.i.d. observations \(X_{1},\ldots,X_{n}\) with p.d.f. \(f\). Then

\[E\hat{f}_{n}(x) =E\frac{1}{h^{d}}K\left(\frac{\mathbf{x}-X_{1}}{h}\right)=\frac{1 }{h^{d}}\int_{\mathbb{R}^{d}}K\left(\frac{\mathbf{x}-\mathbf{v}}{h}\right)f( \mathbf{v})dv_{1}\ldots dv_{d}\] \[=\int_{\mathbb{R}^{d}}K(\mathbf{u})f(\mathbf{x}-h\mathbf{u})d \mathbf{u}\] \[=\int_{\mathbb{R}^{d}}K(\mathbf{u})\left[f(\mathbf{x})-h\mathbf{u }\cdot\operatorname{grad}f(\mathbf{x})+\frac{h^{2}}{2}\sum u_{i}u_{j}\frac{ \partial^{2}f(\mathbf{x})}{\partial x_{i}\partial x_{j}}+o(h^{2})\right]d \mathbf{u}\] \[=f(\mathbf{x})+\frac{h^{2}}{2}\sum c_{1,i,j}\frac{\partial^{2}f( \mathbf{x})}{\partial x_{i}\partial x_{j}}+o(h^{2}),\]

so that

\[\operatorname{Bias}\hat{f}_{n}(\mathbf{x})=E\hat{f}_{n}(\mathbf{x})-f(\mathbf{ x})=\frac{h^{2}}{2}\sum c_{1,i,j}\frac{\partial^{2}f(\mathbf{x})}{\partial x _{i}\partial x_{j}}+o(h^{2}). \tag{10.26}\]

Also,

\[E\left(\frac{1}{h^{d}}\,K\left(\frac{\mathbf{x}-X_{1}}{h}\right) \right)^{2} =\frac{h^{d}}{h^{2d}}\int_{\mathbb{R}^{d}}K^{2}(\mathbf{u})\bigg{[}f( \mathbf{x})-h\mathbf{u}\cdot\operatorname{grad}f(\mathbf{x})\] \[\quad+\frac{h^{2}}{2}\sum u_{i}u_{j}\frac{\partial^{2}f(\mathbf{ x})}{\partial x_{i}\partial x_{j}}+o(h^{2})\bigg{]}d\mathbf{u}\] \[=\frac{1}{h^{d}}\,c_{2}f(\mathbf{x})+o\left(\frac{h^{2}}{h^{d}} \right),\] \[\operatorname{var}\left(\frac{1}{h^{d}}K\left(\frac{\mathbf{x}-X _{1}}{h}\right)\right) =\frac{1}{h^{d}}\,c_{2}f(\mathbf{x})+O(1)\] \[\operatorname{var}(\hat{f}_{n}(\mathbf{x})) =\frac{1}{nh^{d}}\,c_{2}f(\mathbf{x})+o\left(\frac{1}{nh^{d}} \right). \tag{10.27}\]Hence

\[E(\hat{f}_{n}({\bf x})-f({\bf x}))^{2} = ({\rm Bias}\,\hat{f}_{n}({\bf x}))^{2}+{\rm var}(\hat{f}_{n}({\bf x})) \tag{10.28}\] \[= \frac{1}{nh^{d}}\,c_{2}f({\bf x})+\frac{h^{4}}{4}\left(\sum_{i,j}c _{1,i,j}\frac{\partial^{2}f({\bf x})}{\partial x_{i}\partial x_{j}}\right)^{2}\] \[+o(h^{4})+o\left(\frac{1}{nh^{d}}\right).\]

Therefore,

\[{\rm MISE}\,(\hat{f}_{n})=\int_{{\mathbb{R}}^{d}}E(\hat{f}_{n}({\bf x})-f({\bf x }))^{2}d{\bf x}=\frac{c_{2}}{nh^{d}}+\frac{h^{4}}{4}\,\tilde{c}_{3}+o(h^{4})+ o\left(\frac{1}{nh^{d}}\right), \tag{10.29}\]

where

\[\tilde{c}_{3}=\int_{{\mathbb{R}}^{d}}\left(\sum_{i,j}c_{1,i,j}\frac{\partial^{ 2}f({\bf x})}{\partial x_{i}\partial x_{j}}\right)^{2}d{\bf x}.\]

As before, the asymptotically optimal bandwidth is given by

\[h_{n}=\arg\min_{h}\left\{\frac{h^{4}}{4}\,\tilde{c}_{3}+\frac{c_{2}}{nh^{d}} \right\}=\left(\frac{c_{2}d}{\tilde{c}_{3}}\right)^{\frac{1}{4+4}}n^{-\frac{1} {d+4}}, \tag{10.30}\]

and the asymptotically minimal MISE is

\[{\rm MISE}\,(\hat{f}_{n})=C_{2}^{\frac{4}{d+4}}\widetilde{C}_{3}^{\frac{d}{d+4 }}\left(\frac{1}{4}\,d^{\frac{4}{d+4}}+d^{-\frac{d}{d+4}}\right)n^{-\frac{4}{d +4}}+o\left(n^{-\frac{4}{d+4}}\right). \tag{10.31}\]

Multi-dimensional versions of (10.20)-(10.22) may now be derived (Exercise 10.3).

### Nonparametric Regression-Kernel Estimation

We now turn to the problem of nonparametric estimation of a regression function \(f(x)=E[Y\mid X=x]\), based on observations \((X_{j},Y_{j})\), \(j=1,\ldots,n\). Assume first that the regressor \(X\) is also stochastic and that \((X_{j},Y_{j})\), \(j=1,\ldots,n\), are i.i.d. with a density \(g_{1}(x)\) of \(X\) and a conditional distribution \(G_{2}(dy\mid x)\) of \(Y\), given \(X=x\). Then, with a suitable symmetric kernel \(K\), one has, by the SLLN, the following convergence almost surely as \(n\to\infty\), for every \(x\) and every \(h>0\):

\[n^{-1}h^{-1}\sum_{1\leq j\leq n}Y_{j}K\left(\frac{x-X_{j}}{h} \right)\longrightarrow h^{-1}EY_{j}K\left(\frac{x-X_{j}}{h}\right)\] \[= \int yh^{-1}K\left(\frac{x-u}{h}\right)g_{1}(u)G_{2}(dy\mid u)du. \tag{10.32}\]

As \(h\downarrow 0\), under appropriate continuity conditions the last integral converges to \(\int yg_{1}(x)G_{2}(dy\mid x)\), since \(h^{-1}K((x-u)/h)du\) converges in distribution to the point mass at \(x\), namely, \(\delta_{x}\). Thus the left side of (10.32) approximates \(g_{1}(x)E[Y\mid X=x]=g_{1}(x)f(x)\) for small \(h\) and large \(n\). Since, by the preceding section,\(n^{-1}h^{-1}\sum_{1\leq j\leq n}K((x-X_{j})/h)\to g_{1}(x)\) as \(n\to\infty\) and \(h\downarrow 0\), one obtains the _Nadaraya-Watson estimator_ of \(f(x)\), namely,

\[n^{-1}h^{-1}\sum_{1\leq j\leq n}Y_{j}K\left(\frac{x-X_{j}}{h} \right)/n^{-1}h^{-1}\sum_{1\leq j\leq n}K\left(\frac{x-X_{j}}{h}\right)\] \[=\sum_{1\leq j\leq n}Y_{j}K\left(\frac{x-X_{j}}{h}\right)/\sum_{1 \leq j\leq n}K\left(\frac{x-X_{j}}{h}\right). \tag{10.33}\]

Note that the argument goes through if \(X_{j}\)s satisfy the convergence of the empirical measure \(n^{-1}\sum\delta x_{j}\) to a probability distribution \(g_{1}(x)dx\) as \(n\to\infty\). This is also satisfied by a _non-stochastic design of \(X\)_, with the same convergence. For example, if one takes \(X_{j}\)s to be equidistributed in \([0,1]\), \(X_{j}=j/n\) (\(j=1,\ldots,n\)), then the denominator on the left in (10.32) converges to \(g_{1}(x)=1\), while the numerator converges to \(f(x)g_{1}(x)=f(x)\).

We will now derive in detail the properties of a kernel estimator due to Gasser and Muller (1984) instead of the Nadaraya-Watson estimator. Both kernel estimators are asymptotically optimal among nonparametric estimators in a sense to be defined later. However, the former seems to perform a little better in simulation studies, and is more suitable with non-equidistant design points of (a non-stochastic) \(X\). The proof of the asymptotic optimality of the Nadaraya-Watson estimator is similar, and is left as an exercise (Exercise 10.5).

First consider a non-stochastic \(X\) with equidistant design points labeled as \(x_{j}=j/n\) (\(j=1,\ldots,n\)). Also denote \(x_{0}=0\). The domain over which the function \(f\) is estimated is taken to be \([0,1]\) without any essential loss of generality. Let \(K\) be a smooth symmetric probability density with support \([-1,1]\) (e.g., the rescaled Epanechnikov kernel (see Remark 10.1)). Assume the nonparametric regression model

\[Y_{j}=f(x_{j})+\varepsilon_{j}\quad(j=1,\ldots,n),\ (x_{j}=j/n,\,(j=1,\ldots,n) \in[0,1]), \tag{10.34}\]

where \(\varepsilon_{j}\) are i.i.d., satisfying

\[E\varepsilon_{j}=0,\quad 0<E\varepsilon_{j}^{2}=\sigma^{2}<\infty. \tag{10.35}\]

**Theorem 10.2**.: _Let \(f\) be twice continuously differentiable on \([0,1]\). Consider the estimator of \(f\) given by_

\[f_{h}(x)=h^{-1}\sum_{1\leq j\leq n}Y_{j}\int_{(x_{j-1},x_{j}]}K\left(\frac{x-u }{h}\right)du, \tag{10.36}\]

_where the kernel \(K\) is a probability density which is symmetric and twice continuously differentiable with support \([-1,1]\). (a) Then, as \(h\downarrow 0\) and \(n\to\infty\), one has_

\[(i)\quad Ef_{h}(x)=f(x)+\left(\frac{c_{1}}{2}\right)h^{2}f^{\prime \prime}(x)+o(h^{2})+O(n^{-1}),\quad(h<x<1-h),\] \[(ii)\quad\mathrm{Var}(f_{h}(x))\leq\sigma^{2}c_{4}(nh)^{-1},\quad (0<x<1). \tag{10.37}\]

_Here \(c_{1}\) is as in (10.12), and \(c_{4}=2(\max\{K^{2}(u):u\in[-1,1]\})\)._

* _The expected squared error of_ \(f_{h}(x)\)_, for_ \(h<x<1-h\)_, attains its minimal rate_ \(O(n^{-4/5})\) _with_ \(h=b_{3}n^{-1/5}\) _for any_ \(b_{3}>0\)Proof.: (a) By the labeling used, \(EY_{j}=f(x_{j})\), and

\[Ef_{h}(x)=h^{-1}\sum_{1\leq j\leq n}f(x_{j})\int_{(x_{j-1},x]}K((x-u)/h)du,\]

and

\[\left|Ef_{h}(x)-h^{-1}\int_{[0,1]}K((x-u)/h)f(u)du\right|\] \[=h^{-1}\left|\sum_{1\leq j\leq n}\int_{(x_{j-1},x_{j}]}K((x-u)/h)(f (x_{j})-f(u))du\right|\] \[\leq ch^{-1}n^{-1}\int_{[0,1]}K((x-u)/h)du\] \[=ch^{-1}n^{-1}h\int_{[-1,1]}K(v)dv=cn^{-1}\ \left[c=\max\{|f^{ \prime}(u)|:u\in[0,1]\}\right]\!. \tag{10.38}\]

Now, as in the calculations (10.14), and (10.15), with \(c_{1}\), \(c_{2}\), \(c_{3}\) as in (10.12), one has

\[f(x)-h^{-1}\int_{[0,1]}K((x-u)/h)f(u)du=(c_{1}h^{2}/2)f^{\prime\prime}(x)+o(h^{ 2}), \tag{10.39}\]

which, together with (10.38), yields the first relation in (10.37).

Next, by the mean value theorem, there exist \(v_{j}\in[x_{j-1},x_{j}]\) such that

\[\text{Var}(f_{h}(x)) =h^{-2}\sigma^{2}\sum_{1\leq j\leq n}\left[\int_{(x_{j-1},x_{j}]}K ((x-u)/h)du\right]^{2}\] \[=h^{-2}\sigma^{2}\sum_{1\leq j\leq n}K^{2}((x-v_{j})/h)(x_{j}-x_{ j-1})^{2}\] \[=n^{-2}h^{-2}\sigma^{2}\sum_{1\leq j\leq n}K^{2}((x-v_{j})/h).\]

Since \((x-v_{j})/h\in[-1,1]\) only if \(v_{j}\in[x-h,x+h]\), and there are at most \(2nh\) nonzero summands in the last sum, one derives the desired inequality

\[\text{Var}(f_{h}(x))\leq 2n^{-1}h^{-1}\sigma^{2}\ (\max\{K^{2}(u):u\in[-1,1]\}). \tag{10.40}\]

(b) It follows from (10.37) that for \(h<x<1-h\), if one takes \(h=b_{3}n^{-1/5}\) for some \(b_{3}>0\), then

\[\text{Expected squared error of}\ f_{h}(x)=(\text{Bias of}\ f_{h}(x))^{2}+\text{Var }(f_{h}(x))=O\left(n^{-\frac{4}{3}}\right). \tag{10.41}\]

With a more precise estimation of \(\text{Var}(f_{h}(x))\) one can show that (See Eubank 1999, pp. 165-166) that

\[\text{Var}(f_{h}(x))=c_{2}h^{-1}h^{-1}\sigma^{2}+O(n^{-2}h^{-2}). \tag{10.42}\]Hence the right side of (10.42) is minimized by \(h=b(x)n^{-1/5}\) for some appropriate constant \(b(x)\). This completes the proof of (b). 

Remark Remark 10.3: The proof of Theorem 10.2 is easily extended to the case of _arbitrary design_ points \(x_{j}\) such that \(a/n<x_{i+1}-x_{i}<b/n\) for all \(i=0,1,\ldots,n\) (\(x_{0}=0\)), for any pair of positive constants \(a<b\). If the _regressor_\(X\)_is stochastic,_ then the proof (with \(X\) and \(\varepsilon\) independent) extends to the case where \(X\) has a density which is continuous and strictly positive on \((0,1)\).

Remark Remark 10.4: The arguments above show that, under the hypothesis of Theorem 10.2, the asymptotic optimal rate is given by \(O(n^{-4/5})\), over the class of all kernels that may be used. It has actually been proved by Stone (1980), that nonparametric estimators of the regression function \(f\) cannot have an integrated squared error of smaller rate than \(O(n^{-4/5})\), no matter whether kernel or other methods are used, provided \(f\) is required to be only twice continuously differentiable.

As in the case of density estimation, one can establish the CLT for \(f_{h}(x)\), namely,

\[[\hat{f}_{n}(x)-E\hat{f}_{n}(x)]/[\text{Var}(\hat{f}_{n}(x)]^{\frac{1}{2}} \longrightarrow N(0,1)\text{ in distribution as }n\to\infty, \tag{10.43}\]

where \(\hat{f}_{n}(x):=f_{h}(x)\) with \(h=b_{3}n^{-1/5}\) for some \(b_{3}>0\), and

\[[\hat{f}_{n}(x)-f(x)]/[\text{Var}(\hat{f}_{n}(x)]^{\frac{1}{2}}\longrightarrow N (\gamma(x),1)\text{ in distribution as }n\to\infty, \tag{10.44}\]

where \(\gamma(x)=c^{\prime}f^{\prime\prime}(x)\) arises from the bias term (10.37) and the variance term (10.42). Also, just as in the case of density estimation (See Exercise 10.2), by choosing the bandwidth to go to zero at a slightly faster rate than the optimal rate, namely, \(h=o(n^{-1/5})\), one may prove the useful result

\[[f_{h}(x)-f(x)]/[\text{Var}(f_{h}(x)]^{\frac{1}{2}}\longrightarrow N(0,1)\, \text{ in distribution if }\ h=o(n^{-1/5}),\,n\to\infty. \tag{10.45}\]

Remark Remark 10.5: In the case of multiple regression, i.e., with a \(d\)_-dimensional covariate_\(X\), one can show in the same manner as in Sect. 10.1, that the optimal rate of the integrated squared error is \(O(n^{-4/(d+4)})\), attained with bandwidth \(h=c^{\prime\prime}n^{-1/(d+4)}\). Simulation studies exhibit dramatic deteriorations in the performance of nonparametric estimators of density and regression as \(d\) increases, a phenomenon often referred to as the _curse of dimensionality_ (See, e.g., Wasserman 2003, p. 319).

Remark Remark 10.6: It may be noted that the bias term in (10.37) is computed only for \(h<x<1-h\). Unfortunately, for \(x\) beyond this range one does not get the expected squared error to be as small as \(O(n^{-4/5})\), but rather \(O(n^{-3/4})\). For a discussion of this so-called _boundary effect,_ see Eubank (1999, p. 170).

Finally, we turn to the important problem of the data-driven choice of the optimal bandwidth \(h\), which is not immediately available from the above discussion. The most popular procedure for this is known as _cross-validation,_ analogous to that for density estimation (see Remark 10.2). In this method one seeks \(h\) which minimizes the sum of squares \(\sum_{1\leq i\leq n}[Y_{i}-f_{h}^{(-i)}(x_{i})]^{2}=CV(h)\), where \(f_{h}^{(-i)}\) is the kernel estimate of \(f\) using \(n-1\) observation pairs \((x_{j},Y_{j})\), omitting the \(i\)-thone. Note that \(Y_{i}-f_{h}^{(-i)}(x_{i})\) is the error of predicting \(Y\) by the kernel method at the design point (or covariate value) \(x_{i}\), using the remaining \(n-1\) observations. Also, \(n^{-1}CV(h)\) is the estimate of the expected (integrated) prediction error using bandwidth \(h\). Thus \(h=h_{n}\) aims at minimizing this error. In Eubank (1999, pp. 37-46), one can find a modified version called _generalized cross-validation,_ or GCV, as well. That this choice is asymptotically optimal is also shown by the following result of C. Stone for the case of density estimation: If \(\hat{f}_{n,h}\) denotes the estimate (10.4) for a given \(h\), and \(\hat{f}_{n,h_{n}}\) that with \(h=h_{n}\), then

\[MISE(\hat{f}_{n,h_{n}})/\inf_{h}\,MISE(\hat{f}_{n,h})\longrightarrow 1\quad \text{in probability as }n\rightarrow\infty. \tag{10.46}\]

### Notes and References

A very readable text on the subject of this chapter is Eubank (1999). A more comprehensive account of nonparametric density estimation is given in Silverman (1986). A precise study of error rates may be found in Tsybakov (2009).

For _monotone increasing_ regression in bioassay, Bhattacharya and Kong (2007) study the asymptotics of nonparametric quantile estimates using a continuous extension of the so-called _pool adjusted violators algorithm (PAV)_. Bhattacharya and Lin (2010, 2011, 2013) improve upon this by providing asymptotically optimal procedures for bioassay and environmental risk assessment, and also provide extensive data analysis and simulations for a comparative study of a number of different estimation methods. Lin (2012) develops a general nonparametric theory of regression in this context under order restrictions. Dette et al. (2005) and Dette and Scheder (2010) use an _inverse kernel method_ to design an asymptotically optimal procedure for nonparametric quantile estimation in the order restricted context.

We have not discussed in this chapter the important method of nonparametric regression using _splines,_ because of some complications involving the statistical inference involved. See Eubank (1999) for a fine treatment of optimal smoothing splines, proving in particular the following result. Let the true regression curve belong to the Sobolev class \(W_{2}^{2}[0,1]\), which is the completion of \(C^{2}[0,1]\) in \(L^{2}[0,1]\). Then the optimal estimate in \(W_{2}^{2}[0,1]\) is the cubic spline with knots at the data points (See Wahba 1990). Kong and Eubank (2006) derived the optimal monotone cubic spline estimate and applied it to the quantal bioassay problem.

### Exercises for Chap. 10

#### Exercises for Sects. 10.1, 10.2

**Ex. 10.1.** Let the hypothesis of Theorem 10.1 hold.

1. Derive (10.19) and (10.22).
2. Show that \(\{(\hat{f}_{n}(x)-f(x))/\sqrt{\operatorname{var}\hat{f}_{n}(x)}:x\) such that \(f(x)>0\}\) converges in law to a Gaussian process, and compute the mean and covariance function of this Gaussian process.

**Ex. 10.2**.: Assume that \(h\equiv h_{n}=o(n^{-1/5})\) and (10.9) holds, and prove that with this bandwidth (10.22) holds with \(\gamma=0\), under the hypothesis of Theorem 10.1.

**Ex. 10.3**.: Consider a pdf \(f\) on \(\mathbb{R}^{d}\), having continuous and bounded second derivatives.

1. State and prove the analog of Theorem 10.1.
2. Derive the multidimensional versions of (10.20)-(10.22).

**Ex. 10.4**.: Prove that, in Remark 10.2, \(E\widehat{A}=E\int f(x)\hat{f}_{n}(x)dx\), where \(\widehat{A}=n^{-1}\sum_{i=1}^{n}\hat{f}_{n,-i}(X_{i})\). [Hint: \(E\widehat{A}=E\hat{f}_{n,-1}(X_{1})=\frac{1}{n}EK((X_{1}-X_{2})/h)=\frac{1}{ \hbar}\int K((x-y)/h)f(x)f(y)dxdy\). Also, \(E\int f(x)\hat{f}_{n}(x)dx=\frac{1}{\hbar}\int f(x)EK\left(\frac{x-X_{1}}{h} \right)dx=\frac{1}{\hbar}\int f(x)K\left(\frac{x-y}{h}\right)f(y)dxdy\).]

**Ex. 10.5**.: Extend Theorem 10.2 to the Nadaraya-Watson estimator (10.33).

## References

* Bhattacharya & Kong (2007) Bhattacharya, R., & Kong, M. (2007). Consistency and asymptotic normality of the estimated effective dose in bioassay. _Journal of Statistical Planning and Inference, 137_, 643-658.
* Bhattacharya & Lin (2010) Bhattacharya, R., & Lin, L. (2010). An adaptive nonparametric method in benchmark analysis for bioassay and environmental studies. _Statistics and Probability Letters, 80_, 1947-1953.
* Bhattacharya & Lin (2011) Bhattacharya, R., & Lin, L. (2011). Nonparametric benchmark analysis in risk assessment: A comparative study by simulation and data analysis. _Sankhya Series B, 73_(1), 144-163.
* Bhattacharya & Lin (2013) Bhattacharya, R., & Lin, L. (2013). Recent progress in the nonparametric estimation of monotone curves--with applications to bioassay and environmental risk assessment. _Computational Statistics and Data Analysis, 63_, 63-80.
* Dette et al. (2005) Dette, H., Neumeyer, N., & Pliz, K. F. (2005). A note on nonparametric estimation of the effective dose in quantal bioassay. _Journal of the American Statistical Association, 100_, 503-510.
* Dette & Scheder (2010) Dette, H., & Scheder, R. (2010). A finite sample comparison of nonparametric estimates of the effective dose in quantal bioassay. _Journal of Statistical Computation and Simulation, 80_(5), 527-544.
* Epanechnikov (1969) Epanechnikov, V. A. (1969). Non-parametric estimation of a multivariate probability density. _Theory of Probability and its Applications, 14_, 153-158.
* Eubank (1999) Eubank, R. (1999). _Nonparametric regression and spline smoothing_. New York: Chapman and Hall.
* Gasser & Muller (1984) Gasser, T., & Muller, R. G. (1984). Estimating regression functions and their derivatives by the kernel method. _Scandinavian Journal of Statistics, 11_, 171-185.
* Hall (1992) Hall, P. (1992). On bootstrap confidence intervals in nonparametric regression. _Annals of Statistics, 20_(2), 695-711.
* Kong & Eubank (2006) Kong, M., & Eubank, R. (2006). Monotone smoothing with application to dose-response curve. _Communications in Statistics Simulation and Computation, 35_(4), 991-1004.

* Lehmann (1999) Lehmann, E. L. (1999). _Element of large-sample theory._ Springer, New York.
* Lin (2012) Lin, L. (2012). Nonparametric inference for bioassay (Ph.D Thesis). University of Arizona.
* Silverman (1986) Silverman, B. (1986). _Density estimation for statistics and data analysis._ New York: Chapman and Hall.
* Stone (1980) Stone, C. (1980). Optimal rates of convergence for nonparametric estimators. _Annals of Statistics, 8_(6), 1348-1360.
* Tsybakov (2009) Tsybakov, A. B. (2009). _Introduction to nonparametric estimation._ New York: Springer.
* Wahba (1990) Wahba, G. (1990). _Spline models for observational data._ CBMS-NSF Series (Vol. 59). Philadelphia: SIAM.
* Wasserman (2003) Wasserman, L. (2003). _All of statistics: A concise course in statistical inference._ New York: Springer.

[MISSING_PAGE_EMPTY:12693]

## Chapter 11 Edgeworth Expansions and the Bootstrap

### 11.1 Cramer Type Expansion for the Multivariate CLT

Let \(X_{j}\) (\(j\geq 1\)) be a sequence of \(k\)-dimensional random vectors with (common) distribution \(Q\), mean (vector) \(0\) and a nonsingular covariance matrix \(\mathbf{V}\). Assume that \(\rho_{s}:=E||X_{1}||^{s}<\infty\) for some integer \(s\geq 3\). The \(X_{j}\)'s have the common characteristic function (c.f.) \(\widehat{Q}(\xi):=E(\exp\{i\xi\cdot X_{j}\})\), \(\xi\in\mathbb{R}^{k}\). Let \(S_{n}=X_{1}+\cdots+X_{n}\). Then the c.f. of the distribution \(Q_{n}\) of \(n^{-\frac{1}{2}}S_{n}\) is

\[\widehat{Q}_{n}(\xi)=\widehat{Q}^{n}\left(\xi/\sqrt{n}\,\right), \tag{11.1}\]

The _cumulant generating function (c.g.f.)_ of \(Q\) in a neighborhood of \(\xi=0\) may be expressed by a Taylor expansion as

\[\log\widehat{Q}(\xi)=-\xi\cdot V\xi/2+\sum_{r=3}^{s}i^{r}\lambda_{r}(\xi)/r!+o( |\xi|^{s}), \tag{11.2}\]

where \(\lambda_{r}(\xi)\) is the \(r\)-th cumulant of the random variable \(\xi\cdot X_{1}\). Note that

\[\lambda_{r}(\xi)=\sum_{|\beta|=r}(\lambda_{\beta}/\beta!)\xi^{\beta}. \tag{11.3}\]

Here \(\beta=(\beta_{1},\ldots,\beta_{k})\subset Z_{+}^{k}\), \(|\beta|=\beta_{1}+\cdots+\beta_{k}\). Also, \(\lambda_{\beta}\) is the \(\beta\)-th cumulant of \(X_{1}\) (i.e., \(\lambda_{\beta}=i^{-|\beta|}(\mathbf{D}^{\beta}\log\hat{Q})(0)\)), \(\beta!=\beta_{1}!\beta_{2}!\ldots\beta_{k}!\) and \((i\xi)^{\beta}=(i\xi_{1})^{\beta_{1}}\ldots(i\xi_{k})^{\beta_{k}}\). Note that for, \(|\beta|=3\), \(\lambda_{\beta}=\mu_{\beta}=EX_{1}^{\beta}=EX_{1}^{\beta_{1}}\ldots X_{k}^{ \beta_{k}}\), where \(\mathbf{D}=(D_{1},D_{2},\ldots,D_{k})\), \(D_{j}=\partial/\partial x_{j}\), \((-\mathbf{D})^{\beta}=(-1)^{|\beta|}D_{1}^{\beta_{1}}\ldots D_{k}^{\beta_{k}}\)\(\forall\beta\in\mathbf{Z}_{+}^{k}\), and \(\varphi_{\mathbf{V}}\) is the density of the \(k\)-dimensional Normal distribution with mean \(0\) and covariance matrix \(\mathbf{V}\).

The c.g.f. of \(Q_{n}\) may then be expressed as

\[\log\widehat{Q}_{n}(\xi) =n\log\widehat{Q}(\xi/\sqrt{n}\,)\] \[=-\xi\cdot V\xi/2+\sum_{r=3}^{s}n^{-(r-2)/2}i^{r}\lambda_{r}(\xi)/ r!+o\left(n^{-(s-2)/2}\right),\]

so that

\[\widehat{Q}_{n}(\xi)=\exp\{-\xi\cdot V_{\xi}\}\exp\left\{\sum_{r=3}^{s}n^{-( \gamma-2)/2}\,i^{r}\frac{\lambda_{r}(\xi)}{r!}\right\}+o\left(n^{-(s-2)/2} \right). \tag{11.4}\]

Expanding the second factor on the right in powers of \(n^{-\frac{1}{2}}\), one has

\[\widehat{Q}_{n}(\xi)=\exp\{-\xi\cdot V_{\xi}/2\}\left[1+\sum_{r=1}^{s-2}n^{-r/ 2}\widetilde{\mathbf{P}}_{r}(i\xi)\right]+o\left(n^{-(s-2)/2}\right), \tag{11.5}\]

where \(P_{r}(i\xi)\) is a polynomial in \(i\xi\). For example,

\[\widetilde{\mathbf{P}}_{1}(i\xi)=\frac{i^{3}\lambda_{3}(\xi)}{3!}=\frac{i^{3} }{3!}E(\xi\cdot X_{1})^{3}=\sum_{|\beta|=3}\frac{\lambda_{\beta}}{\beta!}(i\xi )^{\beta}, \tag{11.6}\]

The 'formal' density of \(Q_{n}\) is then, by Fourier inversion,

\[\psi_{s-2,n}(x)=\left[1+\sum_{r=1}^{s-2}n^{-r/2}\widetilde{\mathbf{P}}_{r}(- \mathbf{D})\right]\varphi_{\mathbf{V}}(x). \tag{11.7}\]

In particular,

\[\widetilde{\mathbf{P}}_{1}(-\underset{\sim}{D})\varphi_{\mathbf{V}}(x)=-\sum_ {|\beta|=3}\frac{\mu_{\beta}}{\beta!}\,\mathbf{D}^{\beta}\varphi_{\mathbf{V}}(x). \tag{11.8}\]

In general, one has the following theorem. Here \(\mathscr{B}(\mathbb{R}^{k})\) is the Borel sigma-field of \(\mathbb{R}^{k}\) (See Theorem 19.4 and Corollary 19.6 in Bhattacharya and Rao 1976).

**Theorem 11.1**.: _Suppose the \(p\)-fold convolution \(Q^{sp}\) (i.e., the distribution of \(X_{1}+\cdots+X_{p}\)) has a nonzero absolutely continuous component for some positive integer \(p\). Then, if \(\rho_{s}<\infty\) for some integer \(s\geq 3\), one has_

\[\sup_{B\in\mathscr{B}(\mathbb{R}^{k})}|Q_{n}(B)-\Psi_{s-2,n}(B)|=o(n^{-(s-2)/ 2}), \tag{11.9}\]

_where \(\Psi_{s-2,n}\) is the finite signed measure whose density is \(\psi_{s-2,n}\) given by (11.7)._

### The Formal Edgeworth Expansion and Its Validity

A crucial factor underlying the expansion of the distribution \(Q_{n}\) of the normalized mean of i.i.d. random variables (or, vectors) is that the \(r\)-th cumulant (or \(v\)-th cumulant with \(|v|=r\)) of \(Q_{n}\) is \(\circ(n^{-(r-2)/2})\). Nearly a hundred years ago, Edgeworth (1905) proposed an analogous expansion of distributions of more general statistics. In this section we sketch the main ideas needed to validate a properly formulated version of Edgeworth' s formal expansion. The details may be found in Bhattacharya (1977), Bhattacharya and Ghosh (1978), or Bhattacharya and Denker (1990).

Let \(X_{j},\ 1\leq j\leq n,\) be i.i.d. observations with values in \(\boldsymbol{R}^{m}.\) Many important statistics may be expressed as, or approximated adequately by, a smooth function of a finite number, say, \(k\) of averaged sample characteristics \((1/n)\sum_{j=1}^{n}f_{i}(X_{j}),\)\(1\leq i\leq k.\) For simplicity we will consider only real-valued statistics, in which case \(f_{i}\)' s are real-valued Borel measurable functions on \(\mathbb{R}^{m}.\)

As simple examples, one may consider (1) the \(t\)-statistic, which is a function of \(\overline{X}\) and \((1/n)\sum_{1\leq j\leq n}X_{j}^{2}\) (Here \(m=1,k=2,f_{1}(x)=x,\ f_{2}(x)=x^{2}\)), and (2) the sample correlation coefficient, which is a function of \(\overline{X}^{(1)},\)\(\overline{X}^{(2)},\)\((1/n)\sum_{1\leq j\leq n}X_{j}^{(1)}X_{j}^{(2)},(1/n)\sum_{1\leq j\leq n}(X_{j}^{(1)}) ^{2},\)\((1/n)\sum_{1\leq j\leq n}(X_{j}^{(2)})^{2}.\) (Here \(X_{j}=(X_{j}^{(1)},X_{j}^{(2)}),\)\(m=2,\)\(k=5,\)\(f_{1}(x)=x^{(1)},\)\(f_{2}(x)=x^{(2)},\)\(f_{3}(x)=x^{(1)}x^{(2)},\)\(f_{4}(x)=(x^{(1)})^{2},\)\(f_{5}(x)=(x^{(2)})^{2}.\)) Other important statistics to which the present theory applies include \(M\)-estimators (e.g., maximum likelihood estimators) under appropriate smoothness conditions. (See Bhattacharya and Ghosh 1978).

In general, define

\[\overline{Z} =(1/n)\sum_{1\leq j\leq n}Z_{j},\ Z_{j}:=(f_{1}(X_{j}),\ f_{2}(X_{ 2}),\ldots,f_{k}(X_{j}),\] \[\mu =EZ_{n}=E\overline{Z}=(Ef_{1}(X_{j}),\ Ef_{2}(X_{j}),\ldots,Ef_{k} (X_{j})). \tag{11.10}\]

We will consider statistics of the form \(H(\overline{Z}),\) where \(H\) is a real-valued, \((s-1)\)-times continuously differentiable function on a neighborhood of \(\mu\in\mathbb{R}^{k}\) containing a ball \(\{z\in\mathbb{R}^{k}:||z-\mu||\leq\delta\}\) for some \(\delta>0.\) As in the preceding section, \(s\geq 3.\) By a Taylor expansion around \(\mu,\)

\[H(\overline{Z})-H(\mu) =(\operatorname{grad}H)(\mu)\cdot(\overline{Z}-\mu)\] \[\qquad+\sum_{|\beta|=2}^{s-1}(\mathbf{D}^{\beta}H)(\mu)(\overline {Z}-\mu)^{\beta}/\beta!+R(\overline{Z},\mu)\] \[=H_{s-1}(\overline{Z}-\mu)+R(\overline{Z},\mu),\ \text{say,} \tag{11.11}\]

where the remainder satisfies

\[|R(\overline{Z},\mu)|\leq c_{13}||\overline{Z}-\mu||^{s-1}\ \ \text{on}\ \{||\overline{Z}-\mu||\leq\delta\},\] \[|R(\overline{Z},\mu)|=o(||\overline{Z}-\mu||^{s-1})\ \ \text{as}\ ||\overline{Z}-\mu||\to 0. \tag{11.12}\]

Consider the normalized statistics

\[W_{n} :=\sqrt{n}\,(H(\overline{Z})-H(\mu))=W_{n}^{\prime}+\sqrt{n}\,R(\overline{Z},\mu),\] \[W_{n}^{\prime} :=\sqrt{n}\,H_{s-1}(\overline{Z}-\mu)\] \[=\operatorname{grad}H(\mu)\cdot\sqrt{n}\,(\overline{Z}-\mu)+\sum_{| \beta|=2}^{s-1}n^{-(|\beta|-2)/2}(\mathbf{D}^{\beta}H)(\mu)\frac{(\sqrt{n}\,(\overline{Z}-\mu))^{\beta}}{\beta!}. \tag{11.13}\]An important fact concerning polynomials in \(\overline{Z}-\mu\) is that their cumulants decay at the same asymptotic rate as those of \(\overline{X}-\mu\) (James 1958 and Leonov and Shiryaev 1959). In particular, the \(r\)-th cumulant of \(H_{s-1}(\overline{Z}-\mu)\) is \(O(n^{-r})\), and that of \(W^{\prime}_{n}\), is therefore \(O(n^{-r/2})\), if sufficiently many moments of \(||\overline{Z}-\mu||\) are finite. Omitting terms of smaller order than \(n^{-(s-2)/2}\), the "approximate" cumulants \(K_{r,n}\), say, of \(W^{\prime}_{n}\) may then be expressed as (See Bhattacharya and Ghosh 1978, or Bhattacharya and Denker 1990)

\[\begin{split} K_{1,n}&=\sum_{r^{\prime}=0}^{[(s-3)/2 ]}n^{-(2r^{\prime}+1)/2}b_{1,r^{\prime}}\\ K_{2,n}&=b_{2,0}+\sum_{r^{\prime}=1}^{[(s-2)/2]}n^{- r^{\prime}}b_{2,r^{\prime}}\\ K_{r,n}&=\begin{cases}\sum_{r^{\prime}=(r-3)/2]}^{[( s-3)/2]}n^{(-2r^{\prime}+1)/2}b_{r,r^{\prime}}&\text{if $r$ is odd, $3\leq r\leq s$},\\ \sum_{r^{\prime}=(r-2)/2}^{[(s-2)/2]}n^{-r^{\prime}}b_{r,r^{\prime}}&\text{if $r$ is even, $4\leq r\leq s$}.\end{split} \tag{11.14}\]

Here \(b_{r,r^{\prime}}\) are constants involving derivatives of \(H\) at \(\mu\) and cumulants (or moments) of \(Z_{1}-\mu\). Under the assumptions \((\text{grad}\ H)(\mu)\neq 0\) and covariance matrix \(\mathbf{V}\) of \(Z_{1}\) is nonsingular, one has

\[\sigma^{2}:=b_{2,0}=(\text{grad}\ H)(\mu)\cdot\mathbf{V}(\text{grad}\ H)(\mu)>0. \tag{11.15}\]

The 'formal' approximation to the characteristic function of \(W_{n}\) (or of \(W^{\prime}_{n}\)) is then obtained in a manner quite analogous to that for \(\sqrt{n}(\overline{X}-\mu)\) (See, e.g., Bhattacharya and Ranga Rao 1976), keeping only terms up to order \(n^{-(s-2)/2}\). We write this expansion as

\[\exp\{-\sigma^{2}\xi^{2}/2\}\exp\left\{K_{1,n}(i\xi)-(K_{2,n}- \sigma^{2})\xi^{2}/2+\sum_{3\leq r\leq s}\frac{K_{r,n}}{r!}(i\xi)^{r}\right\}\] \[=\exp\{-\sigma^{2}\xi^{2}/2\}\left[1+\sum_{r=1}^{s-2}n^{-r/2}\pi_ {r}(i\xi)\right]+o(n^{-(s-2)/2}). \tag{11.16}\]

The Fourier inversion of the last expansion is the 'formal' Edgeworth expansion of the density of \(W_{n}\) (or \(W^{\prime}_{n}\)), which we write as

\[\tilde{\psi}_{s-2,n}(x)=\left[1+\sum_{1\leq r\leq s-2}n^{-r/2}\pi_{r}(-\mathbf{ D})\right]\varphi_{\sigma^{2}}(x). \tag{11.17}\]

We now make the following assumption.

**(A)**: The distribution of the underlying observations \(X_{j}\)_has a nonzero absolutely continuous component whose density is positive in a ball_ \(B\) _in_ \(\mathbb{R}^{m}\) _such that the functions_ \(f_{1},f_{2},\ldots,f_{k}\) _are continuously differentiable on_ \(B\) _and_ \(1,f_{1},f_{2},\ldots,f_{k}\) _are linearly independent as elements of the vector space of real-valued continuous functions on_ \(B\)_Remark 11.1_.: Under (A), \(Q^{*k}\) (the distribution of \(Z_{1}+Z_{2}+\cdots+Z_{k}\)) has a nonzero absolutely continuous component (w.r.t. Lebesgue measure on \(\mathbb{R}^{k}\)), so that Theorem 11.1 applies (See Bhattacharya and Ghosh 1978; Bhattacharya and Denker 1990).

We can now state the main result of this section.

**Theorem 11.2**.: _Suppose (1) \(E|f_{i}(X_{1})|^{s}<\infty\), \(1\leq i\leq k\), for some integer \(s\geq 3\), (2) \(H\) is \((s-1)\)-times continuously differentiable in a neighborhood of \(\mu=EZ_{1}\), (3) \(\operatorname{grad}H(\mu)\neq 0\) and \(V=\operatorname{Cov}Z_{1}\) is nonsingular, (4) (A) holds. Then_

\[\sup_{B\in\mathscr{B}(\mathbb{R})}|P(W_{n}\in B)-\int_{B}\tilde{\psi}_{s-2,n}(x )\,dx|=o(n^{-(s-2)/2}). \tag{11.18}\]

Proof (Sketch).: We sketch the proof of Theorem 11.2 in a number of steps.

Step 1.: First, \(P(||\overline{Z}-\mu||>\delta)\equiv P(||\sqrt{n}\,(\overline{Z}-\mu)||>\delta \sqrt{n}))\), is easily shown to be \(o(n^{-(s-2)/2})\). One may then restrict attention to the set \(B_{n}:=\{z\in\mathbb{R}^{k}:||z||\leq\delta\sqrt{n}\}\) to integrate \(\psi_{s-2,n}(z)\) in (11.7) over the two sets \(\{g_{n}(z)\in B\}\cap B_{n}\) and \(\{h_{n}(z)\in B\}\cap B_{n}\), where \(g_{n}(z)=\sqrt{n}\,(H(\mu+n^{-1/2}z)-H(\mu))\), \(h_{n}(z)=\sqrt{n}\,H_{s-1}(n^{-1/2}z)\) so that \(\{W_{n}\in B\}=\{g_{n}(\sqrt{n}(\overline{Z}-\mu))\in B\}\) and \(\{W^{\prime}_{n}\in B\}=\{h_{n}(\sqrt{n}(\overline{Z}-\mu))\in B\}\). From multivariate calculus, using (11.11)-(11.13), it then follows that

\[\sup_{B\in\mathscr{B}(\mathbb{R})}|P(W_{n}\in B)-P(W^{\prime}_{n}\in B)|=o(n^{ -(s-2)/2}) \tag{11.19}\]

Step 2.: In view of (11.19), it is enough to consider \(W^{\prime}_{n}\) instead of \(W_{n}\) in (11.13). To apply Theorem 11.1, consider the class \(\mathscr{A}\) (which depends on \(n\)) \(\subset\mathscr{B}(\mathbb{R}^{k})\) comprising sets of the form \(A:=\{z\in\mathbb{R}^{k}:h_{n}(z)\in B\}\), \(B\in\mathscr{B}(\ \mathbf{R})\). By a change of variables \(z{\rightarrow}y=(y_{1},\ldots,y_{k})\) where \(y_{1}=h_{n}(z)\) and integrating out \(y_{2},\ldots,y_{k}\) one obtains (See Remark 11.1).

\[\sup_{B\in\mathscr{B}(\mathbb{R})}\left|P(W^{\prime}_{n}\in B)- \int_{\{z\in\mathbb{R}^{k}:h_{n}(z)\in B\}}\psi_{s-2,n}(z)\,dz\right|\] \[\leq\sup_{A\in\mathscr{B}(\mathbb{R}^{k})}\left|Q_{n}(A)-\int_{A} \psi_{s-2,n}(z)\,dz\right|=o(n^{-(s-2)/2}). \tag{11.20}\]

Step 3.: In view of (11.20), in order to prove (11.18) it remains to show that (uniformly \(\forall\)\(B\in\mathscr{B}(\mathbb{R})\))

\[\int_{A}\psi_{s-2,n}(z)\,dz=\int_{B}\tilde{\psi}_{s-2,n}(x)\,dx+o(n^{-(s-2)/2}), \tag{11.21}\]

where \(A=\{z\in\mathbb{R}^{k}:h_{n}(z)\in B\}\). By a change of variables \(z{\rightarrow}T(z):=y\) as before, with \(y_{1}\equiv T_{1}(z)=h_{n}(z)\), the density of \(y_{1}\), induced from \(\psi_{s-2,n}(z)\,dz\), is given in powers of \(n^{-1/2}\) by \(\overline{\psi}_{s-2,n}(x)+o(n^{-(s-2)/2})\),

\[\overline{\psi}_{s-2,n}(x):=\left[1+\sum_{r=1}^{s-2}n^{-r/2}q_{r}(x)\right] \varphi_{\sigma^{2}}(x), \tag{11.22}\]where \(q_{r}(x),\ 1\leq r\leq s-2,\) are certain polynomials in \(x.\) Note that since the dominant term in \(\psi_{s-2,n}(z)\) is \(\varphi_{{}_{\mathbf{V}}}(z)\) and the dominant term in \(T_{1}(z)\equiv h_{n}(z)\) is \(z\cdot(\text{grad}\ H)(\mu),\) the density \(\varphi_{{}_{\sigma^{2}}}\) of \(Z\cdot(\text{grad}\ H)(\mu)\) (under \(\varphi_{{}_{\mathbf{V}}}(z)\,dz\)) is the dominant term in (11.22). More generally, since \(\psi_{s-2,n}\) and \(\overline{\psi}_{s-2,n}\) both have Gaussian decays at the tails, one has (by the same transformation argument),

\[\int_{\mathbb{R}}x^{r}\overline{\psi}_{s-2,n}(x)\,dx=\int_{\mathbb{R}^{k}}h_{n} ^{r}(z)\psi_{s-2,n}(z)\,dz+o(n^{-(s-2)/2})\ \forall\ r=0,1,2,\ldots. \tag{11.23}\]

Assume for the moment that \(Z_{j}\) has finite moments of all orders. Then, by construction, the \(\beta\)-th cumulant of \(\psi_{t-2,n}(z)\,dz\) (namely, \(i^{-|\beta|}(\mathbf{D}^{\beta}\log\tilde{\psi}_{t-2,n})(0)\)) matches the \(\beta\)-th cumulant of \(Q_{n}\) (i.e., of \(\sqrt{n}(\overline{Z}-\mu)\)) up to order \(n^{-(t-2)/2},\)\(|\beta|\leq t.\) This implies

\[\int_{\mathbb{R}^{k}}z^{\beta}\psi_{t-2,n}(z)\,dz=\int_{\mathbb{R}^{k}}z^{ \beta}Q_{n}(dz)+o(n^{-(t-2)/2}),\ \forall\ |\beta|\leq t. \tag{11.24}\]

Letting \(t=s,\) and \(t>s,\) in turn, (11.24) leads to

\[\int z^{\beta}\psi_{s-2,n}(z)\,dz=\int z^{\beta}Q_{n}(dz)+o(n^{-(s-2)/2})\quad \forall\ \beta, \tag{11.25}\]

since \(\psi_{t-2,n}\) equals \(\psi_{s-2,n}\) up to a polynomial multiple of \(\varphi_{{}_{\mathbf{V}}}\) of order \(0(n^{-(s-1)/2}),\) if \(t>s.\) Using (11.25) in (11.23), and noting that \(h_{n}^{r}(z)\) is a polynomial in \(z,\) one then has

\[\int_{\mathbb{R}}x^{r}\overline{\psi}_{s-2,n}(x)\,dx =\int_{\mathbb{R}^{k}}h_{n}^{r}(z)Q_{n}(dz)+o(n^{-(s-2)/2})\] \[\equiv EW_{n}^{\prime}+o(n^{-(s-2)/2})\quad\forall\ r=0,1,2,\ldots. \tag{11.26}\]

By construction of \(\tilde{\psi}_{s-2,n}\) on the other hand,

\[\int_{\mathbb{R}}x^{r}\tilde{\psi}_{s-2,n}(x)\,dx=EW_{n}^{\prime r}+o(n^{-(s-2) /2})\quad\forall\ r=0,1,2,\ldots, \tag{11.27}\]

using the same argument as above, namely, the \(r\)-th cumulant of \(\tilde{\psi}_{t-2,n}(z)\,dz\) matches that of \(W_{n}^{\prime}\) up to order \(n^{-(t-2)/2}\) for all \(r,\ 0\leq r\leq t.\) Comparing (11.26) and (11.27), and noting that the left sides have no terms which are \(o(n^{-(s-2)/2}),\) we have

\[\int_{\mathbb{R}}x^{r}\tilde{\psi}_{s-2,n}(x)\,dx=\int_{\mathbb{R}}x^{r} \overline{\psi}_{s-2,n}(x)\,dx\quad\forall\ r=0,1,2,\ldots, \tag{11.28}\]

which yields \(\tilde{\psi}_{s-2,n}\equiv\overline{\psi}_{s-2,n}\) and implies, in particular, (11.21). 

In order to apply Theorem 11.2, one needs first to compute the "approximate" cumulants of \(W_{n}^{\prime}=\sqrt{n}\,H_{s-1}(\overline{Z}-\mu).\) In the Appendix we carry this out for approximations with errors of the order \(O(n^{-3/2}).\) Here we consider two simple examples.

Example 11.1 (The Sample Mean).: Here \(m=1,k=1,f_{1}(x)=x,H(z)=z,W_{n}=\sqrt{n}\,(\overline{X}-\mu)=W_{n}^{\prime}.\) In this case the \(r\)-th cumulant of \(W_{n}=W_{n}^{\prime}\) is \(K_{r,n}=n^{-(r-2)/2}K_{r}\) (\(r\geq 2\)), where \(K_{r}\) is the \(r\)-th cumulant of \(X_{j};K_{1,n}=0,\ K_{2,n}=\sigma^{2}=EX_{j}^{2},\)\(K_{3,n}=n^{-1/2}\mu_{3},\)\(K_{4,n}=n^{-1}(\mu_{4}-3\sigma^{2}),\) etc., with \(\mu_{r}:=E(X_{j}-\mu)^{r}.\) The Edgeworth expansion in this case is the same as the Cramer expansion (11.7) (with \(k=1\)) and Theorem 11.2 is valid.

_Example 11.2 (Student's \(t\))_. Here \(m=1,\ k=2,\ f_{1}(x)=x,\ f_{2}(x)=x^{2},\)\(EX_{j}=0,\)\(EX_{j}^{2}=\sigma^{2},\) and \(H(z^{(1)},z^{(2)})=z^{(1)}/\sqrt{z^{(2)}-(z^{(1)})^{2}}.\) Then (See, e.g., Hall 1983, or Qunnsiyeh 1989), \(K_{1,n}=(-1/2\mu_{3})n^{-1/2}+O(n^{-3/2}),\)\(K_{2,n}=1+((7/4)\mu_{3}^{2}+3)n^{-1}+O(n^{-2}),\)\(K_{3,n}=(-2\mu_{3})n^{-1/2}+O(n^{-3/2}),\)\(K_{4,n}=-2(\mu_{4}-6\mu_{3}^{2}-6)n^{-1}+O(n^{-2}),\) with \(\mu_{r}:=E(X_{j}/\sigma)^{r}.\) The expansion (11.17) up to order \(n^{-1}\) (and error \(O(n^{-3/2})\)) is

\[\tilde{\psi}_{2,n}(x) =\varphi(x)+n^{-1/2}\left(\frac{\mu_{3}}{6}\right)\frac{d}{dx}[(2 x^{2}+1)\varphi(x)]\] \[\quad+n^{-1}\frac{d}{dx}\big{\{}\big{[}(-3-2\mu_{3}^{2})x+(6-\mu _{4}+8\mu_{3}^{2})(3x-x^{3})\] \[\qquad+\frac{\mu_{3}^{2}}{18}(-15x+10x^{3}-x^{5})\big{]}\varphi(x )\big{\}}. \tag{11.29}\]

The expansion of the distribution function of \(t=\sqrt{n}\,\overline{X}/s,\) with \(s^{2}=\frac{1}{n}\sum X_{j}^{2}-\overline{X}^{2},\) is obtained by removing the \(d/dx\) symbol from (11.29) and replacing the standard normal density \(\varphi(x)\equiv\varphi_{1}(x)\) by its distribution function \(\Phi(x).\) Note that the usual \(t\)-statistic is \(\sqrt{\frac{n-1}{n}}\,t,\) for which the expansion (with error \(O(n^{-3/2})\)) is obtained by a simple change of variables.

The final result of this section applies to statistics such as \((H(\overline{Z})-H(\mu))^{2}=G(\overline{Z}),\) say, for which \((\operatorname{grad}H)(\mu)\neq 0,\) but \((\operatorname{grad}G)(\mu)=0.\) More generally, consider the statistic

\[{\bf U}_{n} :=2nG(\overline{Z})\equiv 2n(H(\overline{Z})-H(\mu))^{2}\] \[=\sum_{1\leq r,r^{\prime}\leq k}(D_{r}D_{r^{\prime}}G)(\mu)(\sqrt {n}\,(\overline{Z}-\mu))^{(r)}(\sqrt{n}\,(\overline{Z}-\mu))^{(r^{\prime})}\] \[\quad+\sum_{|\beta|=3}^{s}n^{-(|\beta|-2)/2}({\bf D}^{\beta}G)( \mu)\frac{(\sqrt{n}\,(\overline{Z}-\mu))^{\beta}}{\beta!}+2nR^{\prime}( \overline{Z},\mu)\] \[={\bf U}_{n}^{\prime}+2nR^{\prime}(\overline{Z},\mu),\ \text{say,} \tag{11.30}\]

where \(|R^{\prime}(z,\mu)|=o(||z-\mu||^{s})\) as \(||z-\mu||\!\to\!\!0,\) so that \(2nR^{\prime}(\overline{Z},\mu)=o_{p}(n^{-(s-2)/2}),\) and \(Z^{(r)}\) is the \(r\)-th coordinate of \(Z.\) Here we assume \(G(\mu)=0\) and \((\operatorname{grad}G)(\mu)=0,\)\(G\) is \(s\)-times continuously differentiable and \(E||Z_{j}||^{s}<\infty\) for some integer \(s\geq 3.\) It is important to note that the moments and, therefore, cumulants of \({\bf U}_{n}^{\prime}\) involve only powers of \(n^{-1}.\) If the Hessian matrix of \(H\) at \(\mu,\) namely, \((((D_{r}D_{r^{\prime}}H)(\mu)))_{1\leq r,r^{\prime}\leq k}\) is nonzero, then the limiting distribution of \({\bf U}_{n}\) (or \({\bf U}_{n}^{\prime}\)) is that of

\[{\bf U}:=\sum_{1\leq r,r^{\prime}\leq k}(D_{r}D_{r^{\prime}}G)(\mu)Y^{(r)}Y^{(r )^{\prime}}\]

where \(Y\) is \(k\)-dimensional normal with mean zero and covariance \({\bf V}\equiv\operatorname{Cov}Z_{j}.\) For likelihood ratio tests, Wald' s tests and Rao' s score tests, etc., the asymptotic distribution of \({\bf U}_{n}\) is the chi-square distribution with \(d\) degrees of freedom (d.f.),\(1\leq d\leq k\). In such cases, one may proceed as in the case of \(W_{n},W^{\prime}_{n}\) to derive a formal Edgeworth expansion

\[\eta_{r,n}(u) :=f_{\chi_{d}^{2}}(u)[1+\sum_{r^{\prime}=1}^{r}n^{-r^{\prime}}g_{r^ {\prime}}(u)],\] \[r :=[(s-2)/2]\text{ (integer part of $(s-2)/2$)}. \tag{11.31}\]

where \(f_{\chi_{d}^{2}}\) is the density of the chi-square distribution with \(d\) degrees of freedom, and \(g_{r^{\prime}},1\leq r^{\prime}\leq r\), are polynomials. The following result is due to Chandra and Ghosh (1979). Let \(\mathscr{B}(\mathbb{R}^{+})\) denote the Borel sigma-field on \(\mathbb{R}^{+}:=\{u\in\mathbb{R}:u\geq 0\}\).

**Theorem 11.3**.: _Let \(E||Z_{j}||^{s}<\infty\) for some integer \(s\geq 3\), and let \(V:=\text{Cov}Z_{j}\) be nonsingular. Suppose \(G\) is \(s\)-times continuously differentiable in a neighborhood of \(\mu=EZ_{j}\), \(G(\mu)=0\), \(\operatorname{grad}G(\mu)=0\), and the limiting distribution of \(\mathbf{U}_{n}\) is \(f_{\chi_{d}^{2}}(u)\,du\). (a) If, in addition, the hypothesis (A) stated before Remark 11.1 holds, then_

\[\sup_{B\in\mathscr{B}(\mathbb{R}^{+})}\left|P(U_{n}\in B)-\int_{B}\eta_{[(s-2) /2],n}(u)\,du\right|=o(n^{-(s-2)/2}). \tag{11.32}\]

The proof of Theorem 11.3 follows the line of proof of Theorem 11.2, noting that the "approximate" cumulants of \(\mathbf{U}_{n}\) and \((U^{\prime}_{n})\) only involve powers of \(n^{-1}\) (See Chandra and Ghosh 1979 and Bhattacharya and Denker 1990).

_Remark 11.2_.: For many purposes, if not most, the asymptotic expansions in Theorems 11.2 and 11.3, are needed only for distribution functions of \(W_{n}\) and \(U_{n}\), which hold under _Cramer's condition:_

\[\limsup_{|\xi|\to\infty}\left|Ee^{i\xi\cdot Z_{1}}\right|<1. \tag{11.33}\]

More generally, we have the following version of Theorem 11.2 (See Bhattacharya and Rao 1976; Bhattacharya 1987).

**Theorem 11.4**.: _Suppose (1) \(E|f_{i}(X_{1})|^{s}<\infty\), \(1\leq i\leq k\), for some integer \(s\geq 3\), (2) \(H\) is \((s-1)\)-times continuously differentiable in a neighborhood of \(\mu=EZ_{1}\), (3) \(\operatorname{grad}H(\mu)\neq 0\) and \(V=\operatorname{Cov}Z_{1}\) is nonsingular, (4) the distribution \(Q\) of \(Z_{1}\) satisfies Cramer's condition (11.33). Then_

\[\sup_{B\in\mathscr{C}}|P(W_{n}\in B)-\int_{B}\tilde{\psi}_{s-2,n}(x)\,dx|=o(n^{ -(s-2)/2}). \tag{11.34}\]

_for every class \(\mathscr{C}\subset\mathscr{B}(\mathbb{R})\) such that, for some \(a>0\),_

\[\sup_{B\in\mathscr{C}}\int_{(\partial B)^{*}}\varphi_{\sigma^{2}}(x)dx=0( \varepsilon^{\alpha})\quad\text{as $\varepsilon\downarrow 0$}. \tag{11.35}\]

_Remark 11.3_.: Consider the problem of constructing a symmetric confidence interval for a real valued parameter \(\theta\), of the form \(|\hat{\theta}_{n}-\theta|\leq n^{-1/2}c\) (for some \(c>0\)), where \(\hat{\theta}_{n}\) is of the form \(H(\overline{Z})\), or adequately approximated by such, one may use Theorem 11.4 to show that

\[P(|\hat{\theta}_{n}-\theta|\leq n^{-1/2}c) \equiv P((\sqrt{n}\,(\hat{\theta}_{n}-\theta))^{2}\leq c^{2})=P(Y^ {2}\leq c^{2})+O(n^{-1})\] \[\equiv P(|Y|\leq c)+O(n^{-1}), \tag{11.36}\]where \(Y\) is a standard normal random variable. Thus with \(c=z_{\gamma/2}\) (the upper \(\gamma/2\)-point of the standard normal distribution), the coverage error of a nominal \((1-\gamma)\)-confidence interval is \(O(n^{-1})\), provided \(s\geq 4\), and the other hypotheses of Theorem 11.3 are satisfied.

The theory presented in this section applies to the estimation and testing in parametric families under appropriate regularity conditions (Bhattacharya and Ghosh 1978; Bhattacharya and Denker 1990; Chandra and Ghosh 1979), in providing more accurate coverage probabilities of confidence regions and levels of significance of tests. If one desires to adjust the confidence bounds or the critical points of tests so as to have specified levels, (up to an error \(O(n^{-1}),\ O(n^{-3/2})\), etc.) one may resort to the inversion of the expansions leading to the so-called _Cornish-Fisher expansions_ (See Hall 1983, or Bhattacharya and Denker 1990). See Sect. 8.5 for nonparametric two-sample tests based on sample mean vectors and an asymptotic chi-square statistic.

Expansions for regression statistics based on independent observations are given in Qumsiyeh (1989) and Ivanov and Zwanzig (1983).

In conclusion, we remark that Chibishov (1973) also obtained asymptotic expansions such as \(\overline{\psi}\) (See (11.22)), but not the validity of the formal Edgeworth expansion.

## 11.3 Bootstrap and Edgeworth Expansion

The nonparametric bootstrap due to Efron (1979) is now an essential tool in statistical applications. Its two most important impacts have been (1) in the setting of confidence bounds and critical levels for estimation and testing in nonparametric models, even in the absence of an estimate of the standard deviation of the statistic, and (2) in the estimation of the above standard deviation. In both cases the computer based procedures are simple and automatic. It was first pointed out by Singh (1981) that, in addition to the simplicity of the procedure, bootstrap provides an asymptotically better estimate of the distribution of the standardized mean than its classical normal approximation, provided the observations are from a nonlattice distribution. Later authors have shown that bootstrap performs better in estimating the distribution of more general studentized, or pivotal, statistics, if the observations come from a distribution with some smoothness satisfying Cramer's condition, or having a density (Beran 1987; Bhattacharya 1987; Hall 1988). In this section we present a precise analysis of the efficacy of the bootstrap estimation of distributions of pivotal statistics.

Consider statistics \(H(\overline{Z})\) as in the preceding section, with \(\overline{Z}\) as given in (11.10). Let \(F_{n}=(1/n)\sum_{1\leq j\leq n}\delta_{Z_{j}}\) denote the empirical distribution of the random vector \(Z\), given \(\{Z_{j}:1\leq j\leq n\}\), while \(F=Q\) denotes its true distribution. The bootstrap distribution of the statistic \(W_{n}=\sqrt{n}\left(H(\overline{Z})-H(\mu)\right)\) is the distribution (conditionally given \(Z_{j},1\leq j\leq n\)) of \(W_{n}^{*}:=\sqrt{n}\left(H(\overline{Z}^{*})-H(\overline{Z})\right)\) where \(\overline{Z}^{*}=(1/n)\sum_{1\leq j\leq n}Z_{j}^{*}\), \(\{Z_{j}^{*};1\leq j\leq n\}\) being a random sample from the empirical \(F_{n}\). We will write \(P\) and \(P^{*}\) for probabilities under \(F\) and \(F_{n}\), respectively. We will consider _studentized_ or pivotal statistics, i.e., assume

\[\sigma^{2}:=(\operatorname{grad}H)(\mu)\cdot V(\operatorname{grad}H)(\mu)=1. \tag{11.37}\]The densities of the (formal) empirical Edgeworth expansions of the distributions of \(\sqrt{n}\,(\overline{Z}-\mu)\) and \(W_{n}=\sqrt{n}\,(H(\overline{Z})-H(\mu))\) are denoted by \(\psi_{s-2,n}\) and \(\tilde{\psi}_{s-2,n}\), respectively (See (11.7), (11.17)), while we define

\[\psi_{s-2,n}^{*}(z) =\left[1+\sum_{r=1}^{s-2}n^{-r/2}\mathbf{P}_{r}^{*}(-\mathbf{D}) \right]\varphi_{\mathbf{V}^{*}}(z)\] \[\tilde{\psi}_{s-2,n}(x) =\left[1+\sum_{r=1}^{s-2}n^{-r/2}\pi_{r}^{*}(-\mathbf{D})\right] \varphi(x), \tag{11.38}\]

where the superscript \(*\) indicates that all population moments are replaced by corresponding sample moments.

**Theorem 11.5**.: _Under the hypothesis of Theorem 11.2, almost surely,_

\[\sup_{B\in\mathscr{C}}|P^{*}(W_{n}^{*}\in B)-\int_{B}\tilde{\psi}_{s-2,n}^{*}( x)\,dx|=o(n^{-(s-2)/2}), \tag{11.39}\]

_where \(\mathscr{C}\) satisfies (11.19) (with \(\sigma^{2}=1\))._

Proof (Sketch).: The main difference between the proof of this theorem and that of Theorem 11.2 lies in deriving an expansion of the distribution \(Q_{n}^{*}\) of \(\sqrt{n}\,(\overline{Z}^{*}-\overline{Z})\) under \(F_{n}\). The rest of the proof showing the validity of the formal Edgeworth expansion for the distribution of \(W_{n}^{*}\) is entirely analogous to the corresponding proof for Theorem 11.2. Now because \(F_{n}\) is discrete, \(\lim_{||\xi||\to\infty}|\hat{F}_{n}(\xi)|=1\), and Cramer's condition does not hold for \(F_{n}\). But as we show now (See (11.43)), \(|\hat{F}_{n}(\xi)|\) is bounded away from one on \([c,e^{n\delta}]\) for every \(c>0\) and \(\delta>0\) depending on \(c\). For this, first one may use Bernstein' s inequality (See Serfling 1980, p. 95) to get

\[P(|\hat{F}_{n}(\xi)-\hat{F}(\xi)|\geq a)\leq 4\exp\{-na^{2}/10\}. \tag{11.40}\]

By approximating points in the set \(A_{n}=\{||\xi||\leq e^{n\delta}\}\) by the set \(B_{n}\) of lattice points in \(A_{n}\) of the form \(\xi=\gamma m\) for some \(m\in\boldsymbol{Z}^{k}\) and some \(\gamma>0\), one has for every \(\delta>0\),

\[P\left(\sup_{\xi\in B_{n}}|\hat{F}_{n}(\xi)-\hat{F}(\xi)|\geq a\right) \leq\left\{\frac{e^{n\delta}}{\gamma}+2\right\}^{k}4\exp\{-na^{2}/10\}\] \[\leq c_{18}(k,\gamma)\exp\left\{-n\left(\frac{a^{2}}{10}-k\delta \right)\right\},\] \[\sup_{||\xi||\leq\exp\{n\delta\}}\left|\hat{F}_{n}(\xi)-\hat{F}( \xi)\right| \leq\sup_{\xi\in B_{n}}|\hat{F}_{n}(\xi)-\hat{F}(\xi)|\] \[\quad+\sqrt{k}\,\gamma\left(E||Z_{1}||+\frac{1}{n}\sum_{1}^{n}|| Z_{j}||\right). \tag{11.41}\]

For \(0<\delta<a^{2}/10k\), using the Borel-Cantelli Lemma it follows from the first inequality that with probability one, \(\sup\{|\hat{F}_{n}(\xi)-\hat{F}(\xi)|:=\xi\in B_{n}\}<a\) for all sufficiently large \(n\). From the second inequality in (11.41) it now follows that

\[\limsup_{n\to 0}\,\sup_{||\xi||\leq e^{n\delta}}|\hat{F}_{n}(\xi)-\hat{F}(\xi)| \leq a+2\sqrt{k}\,\gamma E||Z_{1}||\text{ a.s.} \tag{11.42}\]Let now \(a:=(1/4)(1-\sup\{|\hat{F}(\xi)|:||\xi||\geq c\}),\ \gamma=a/(2\sqrt{k}\,E||Z_{1}||)\), to get

\[\limsup_{n\to\infty}\sup_{c\leq||\xi||\leq\exp\{n\delta\}}|\hat{F}_ {n}(\xi)|\leq a+2\sqrt{k}\gamma E||Z_{1}||+\sup_{||\xi||\geq c}|\hat{F}(\xi)|\] \[=\frac{1}{2}(1-\sup\{|\hat{F}(\xi)|:||\xi||\geq c\})+\sup_{||\xi|| \geq c}|\hat{F}(\xi)|=\delta^{\prime},\ \text{say, a.s.} \tag{11.43}\]

Since \(\delta^{\prime}<1\), this estimate of Babu and Signh (1984) enables one to proceed as in the proof of Cramer's Theorem 11.4 (See Remark 11.2), replacing \(\hat{Q}\) by \(\hat{F}_{n}\), \(\hat{Q}_{n}\) by the c.f. of \(\sqrt{n}\,(\overline{Z}^{*}-\overline{Z})\) under \(F_{n}\), and population moments by sample moments throughout. Since the estimate (11.43) holds only up to the upper bound \(e^{n\delta}\) for \(||\xi||\), one needs to show that

\[\int_{||\xi||>\exp\{n\delta\}}|\hat{K}(\varepsilon\xi)|\,d\xi=\varepsilon^{-k} \int_{||\xi||>\varepsilon e^{n\delta}}|\hat{K}(\xi)|\,d\xi=o(n^{-(s-2)/2}) \tag{11.44}\]

by choosing a kernel \(K\) (with support in the unit ball) such that \(\hat{K}(\xi)\) decays fast as \(||\xi||\to\infty\). For example, one may have \(|\hat{K}(\xi)|\leq c\exp\{-c^{\prime}||\xi||^{1/2}\}\) (See Bhattacharya and Rao 1976, Corollary 10.4). For a detailed proof see Bhattacharya (1987) or Bhattacharya and Denker (1990).

Theorem 11.5 allows us to determine the precise asymptotic accuracy of the bootstrap estimate of the distribution of \(W_{n}\). To see this, write the formal Edgeworth expansion \(\tilde{\psi}_{s-2,n}\) of the 'density' of \(W_{n}\) and the corresponding expansion \(\tilde{\psi}^{*}\) for \(W_{n}^{*}\) as

\[\tilde{\psi}_{s-2,n(x)} =\left[1+\sum_{r=1}^{s-2}n^{-r/2}q_{r}(x)\right]\varphi(x)\] \[\tilde{\psi}_{s-2,n(x)}^{*} =\left[1+\sum_{r=1}^{s-2}n^{-r/2}q_{r}^{*}(x)\right]\varphi(x). \tag{11.45}\]

Then, by Theorems 11.2, 11.5, and noting that sample moments converge to population moments a.s.,

\[{\bf P}^{*}(W_{n}^{*}\leq x)-{\bf P}(W_{n}\leq x) =\frac{1}{\sqrt{n}}\int_{-\infty}^{x}(q_{1}^{*}(y)-q_{1}(y)) \varphi(y)\,dy\] \[\quad+\frac{1}{n}\int_{-\infty}^{\infty}(q_{2}^{*}(y)-q_{2}(y)) \varphi(y)\,dy+o(n^{-1})\ \text{a.s.}\] \[=\frac{1}{\sqrt{n}}(p_{1}(x)-p_{1}^{*}(x))\varphi(x)+o(n^{-1})\ \text{a.s.} \tag{11.46}\]

where \(p_{1}(x)\) is a polynomial (in \(x\)) whose coefficients are polynomials in the moments of \(Z_{1}\), while \(p_{1}^{*}(x)\) is obtained on replacing population moments by corresponding sample moments. Hence \(p_{1}^{*}(x)-p_{1}(x)\) is of the form \(H(\overline{Y};x)-H(v;x)\) where \(v\) is the vector of population moments of \(Z_{1}\) and \(\overline{Y}\) is the corresponding vector of sample moments. By Theorem 11.2 (or, just the delta method), if sufficiently many moments of \(Z_{1}\) are finite,

\[\sqrt{n}[p_{1}^{*}(x)-p_{1}(x)]\varphi(x)\smash{\mathop{\longrightarrow}^{ \mathscr{L}}\limits}N(0,\sigma_{b}^{2}(x))\quad\text{as $n\to\infty$} \tag{11.47}\]where \(\sigma_{b}^{2}(x)\geq 0\) decays to zero fast as \(x{\rightarrow}\infty\), in view of the presence of the exponential factor \(\varphi^{2}(x)\). Hence we have

\[n(\mathbf{P}^{*}(W_{n}^{*}\leq x)-\mathbf{P}(W_{n}\leq x)){\buildrel\mathscr{L} \over{\rightarrow}}N(0,\sigma_{b}^{2}(x)) \tag{11.48}\]

uniformly for all \(x\). We have then proved the following result.

**Theorem 11.6**.: _Assume the hypothesis of Theorem 11.2 with \(s=4\). Also assume that \(W_{n}\) is pivotal, i.e., (11.37) holds, and that \(E||Z_{1}||^{8}<\infty\). Then (11.48) holds._

_Remark 11.4_.: The relation (11.48) implies that the error of bootstrap approximation is \(O(n^{-1})\) a.s. In the same manner, one can show that

\[n^{3/2}(P^{*}(\mathbf{U}_{n}^{*}\leq u)-P(\mathbf{U}_{n}\leq u)){\buildrel \mathscr{L}\over{\rightarrow}}N(0,\theta_{b}^{2}(u)) \tag{11.49}\]

for some \(\theta_{b}^{2}(u)\) which decay fast to zero as \(u{\rightarrow}\infty\). For this one uses Theorem 11.3 and the corresponding version for the expansion \(\eta_{r,n}^{*}\) for \(\mathbf{U}_{n}^{*}\), with \(s=5\), to get (See (11.31), (11.32))

\[P^{*}(U_{n}^{*}\leq u)-P(U_{n}\leq u)=n^{-1}(g_{1}(u)-g_{1}^{*}(u))f_{\chi_{2} ^{2}}(u)+O_{p}(n^{-3/2}). \tag{11.50}\]

Hence (11.49) holds, arguing as in the case of (11.48). In particular, we have the following result.

**Corollary 11.1**.: _(a) Under the hypothesis of Theorem 11.6, the coverage error of the bootstrap for symmetric confidence intervals is \(O_{p}(n^{-3/2})\). (b) Assume the hypothesis of Theorem 11.3 with \(s=5\), and also assume \(E||Z_{1}||^{10}<\infty\). Then the coverage error of the bootstrap with test statistics \(U_{n}\) is \(O_{p}(n^{-3/2})\)._

A similar analysis shows that for nonpivotal statistics \(W_{n}=\sqrt{n}\left(H(\overline{Z})-H(\mu)\right)\) having a normal asymptotic distribution with mean \(0\) and variance \(\sigma^{2}>0\),

\[n^{1/2}(P^{*}(W_{n}^{*}\leq x)-P(W_{n}\leq x)){\buildrel\mathscr{L}\over{ \rightarrow}}N(0,\delta^{2}(x)) \tag{11.51}\]

for an appropriate \(\delta^{2}(x)\). For this, one assumes the hypothesis of Theorem 11.2 with \(s=3\), so that

\[P(W_{n}\leq x) =\Phi_{\sigma^{2}}(x)+n^{-1/2}p_{1}(x)\varphi_{\sigma^{2}}(x)+o(n ^{-1/2})\] \[P^{*}(W_{n}^{*}\leq x) =\Phi_{\hat{\sigma}^{2}}(x)+n^{-1/2}p_{1}^{*}(x)\varphi_{\hat{ \sigma}^{2}}(x)+o_{p}(n^{-1/2}). \tag{11.52}\]

If \(z_{\lambda}^{*}\) is the \(\lambda\)-th quantile of the bootstrap distribution of \(W_{n}^{*}\) then, by (11.51),

\[1-\lambda =P^{*}(z_{\lambda/2}^{*}\leq W_{n}^{*}\leq z_{1-\lambda/2}^{*})\] \[=P(z_{\lambda/2}^{*}\leq W_{n}\leq z_{1-\lambda/2}^{*})+o_{p}(n^{ -1/2}), \tag{11.53}\]

i.e.,

\[P(H(\overline{z})-n^{-1/2}z_{1-\lambda/2}^{*}, \leq H(\mu)\leq H(\overline{Z})-n^{-1/2}z_{\lambda/2}^{*})\] \[=1-\lambda+o_{p}(n^{-1/2}). \tag{11.54}\]

In other words, \([H(\overline{Z})-n^{-1/2}z_{1-\lambda/2}^{*},\ H(\overline{Z})-n^{-1/2}z_{ \lambda/2}^{*}]\) is a confidence interval for \(H(\mu)\) with a coverage error \(o_{p}(n^{-1/2})\). Note that, without the knowledge of the asymptotic variance \(\sigma^{2}\) and studentization, one can not use \(H(\overline{Z})\) to obtain a valid classical confidence interval for \(H(\mu)\).

### Miscellaneous Applications

In this section we briefly touch upon a number of applications of the theory, in addition to applications already discussed.

#### Cornish-Fisher Expansions

By inverting the expansions of the distribution function

\[\tilde{\Psi}_{s-2,n}(x):=\int_{-\infty}^{x}\tilde{\psi}_{s-2,n}(y)\,dy\]

of \(W_{n}\), and of \(\int_{-\infty}^{u}\eta_{r,n}(u)\,du\) of \({\bf U}_{n}\), one may refine critical points for tests and confidence regions to provide a desired level (of significance or confidence) up to an error \(O(n^{-r})\) for \(r=1,3/2\), etc. This idea goes back to Cornish and Fisher (1937). See Hall (1983) and Bhattacharya and Denker (1990) for general derivations. As an example, consider the \(t\)-statistic of Example 11.1 in Sect. 3. Write \(G_{n}\) for its distribution function and \(\Phi\) for that of the standard normal, and let \(z_{p,n}\) and \(z_{p}\) be their respective \(p\)-th quantities, \(G_{n}(z_{p,n})=p,\;\Phi(z_{p})=p\). Then \(z_{p,n}=z_{p,n}^{(1)}+O(n^{-1})\), and \(z_{p,n}=z_{p,n}^{(2)}+O(n^{-3/2})\), where (See Bhattacharya and Denker 1990, p. 46)

\[z_{p,n}^{(1)} =z_{p}-n^{-1/2}\frac{\mu_{3}}{6}(2z_{p}^{2}+1),\] \[z_{p,n}^{(2)} =z_{p,n}^{(1)}+n^{-1}\frac{\mu_{3}^{2}}{18}z_{p}^{5}-\left(\frac{ \mu_{4}}{12}+\frac{\mu_{3}^{2}}{9}+\frac{1}{2}\right)z_{p}^{3}\] \[\qquad-\left(\frac{\mu_{3}^{3}}{6}+\frac{\mu_{4}}{4}\right)z_{p}+ \frac{7\mu_{3}^{2}}{72}(2z_{p}^{3}+z_{p}),\] \[\mu_{r}:=E\left(\frac{X_{1}-\mu}{\sigma}\right)^{r}.\]

Now write, \(\hat{\mu}_{r}\) for the sample moment \(1/n\sum_{j=1}(X_{j}-\overline{X})^{r}/s^{r}\), and

\[y_{p,n}^{(1)} :=z_{p}-n^{-1/2}\frac{\hat{\mu}_{3}}{6}(2z_{p}^{2}+1)=\hat{z}_{p,n }^{(1)},\;\text{say},\] \[P(W_{n}\leq y_{p,n}^{(1)})=p+O(n^{-1}).\]

Next let \(\hat{z}_{p,n}^{(2)}\) denote \(z_{p,n}^{(2)}\) with \(\mu_{r}\) replaced by \(\hat{\mu}_{r}\,(2\leq r\leq 4)\). Then define

\[y_{p,n}^{(2)}:=y_{p,n}^{(1)}-n^{-1}(\hat{z}_{p,n}^{(2)}-\hat{z}_{p,n}^{(1)}),\]

to get

\[{\bf P}(W_{n}\leq y_{p,n}^{(2)})=p+O(n^{-3/2}).\]

In this manner one can approximate the \(p\)-th quantile by a sample estimate to provide any level of accuracy desired, if sufficiently many moments of \(X_{j}\) are finite.

#### Higher Order Efficiency

In regular parametric models, the MLE (maximum likelihood estimator) of a parameter \(\theta\) attains the Cramer-Rao lower bound for variance of unbiased estimators in an asymptotic sense, and is therefore called _efficient,_ a terminology due to R.A. Fisher. There are, however, other estimators of \(\theta\) which are also efficient in this sense. To discriminate among all such estimators, a notion of _second order efficiency_ was introduced by Fisher (1925) and further developed by Rao (1961, 1962, 1963) and other later authors. See Ghosh (1994), and Bhattacharya and Denker (1990) for a detailed account. Using Edgeworth expansions it may be shown that a bias correction of order \(n^{-1/2}\) makes a first order efficient estimator second order efficient, and a bias correction of order \(n^{-1}\) of the latter yields a third order efficient estimator, and so on (Pfanzagl 1980, 1985). Similar results for tests are provided in Bickel et al. (1981).

#### Computation of Power in Parametric Models

Asymptotic expansions of power under contiguous alternatives, similar to the expansion in Theorem 11.3 but with the chi-square density term replaced by the density of a noncentral chi-square distribution, was derived in Chandra and Ghosh (1980). Also see Groeneboom and Oosterhoff (1981).

#### Convergence of Markov Processes to Diffusions

Konakov and Mammen (2005) have obtained an asymptotic expansion of the density of a Markov chain converging to a diffusion on \(\mathbb{R}^{k}\), with an error \(O(n^{-1-\delta})\) for some \(\delta>0\). Here a version of Theorem 11.1 which holds for independent but non i.i.d. summands is needed, as given in Bhattacharya and Rao (1976, Theorem 19.3).

#### Asymptotic Expansions in Analytic Number Theory

A famous classical problem in number theory considered by Gauss and Landau is to provide precise asymptotic rates of the errors in estimating by the volume of a ball (or, ellipsoid) in \(\mathbb{R}^{k}\) the number of lattice points (in \(\boldsymbol{Z}^{k}\)) lying in the ball as the radius increases to infinity (\(k>1\)). Esseen (1945) showed that Landau' s estimates may be derived by using Cramer type expansions for lattice random vectors. See, e.g., Bhattacharya and Rao (1976, Chap. 5), for expansions in the lattice case, and Bhattacharya (1977) for a simple exposition of Landau's estimates. Important progress has been recently made by Bentkus and Gotze (1999) in this problem using a combination of probabilistic and analytic techniques.

In a different direction, Kubilius (1964) derived an asymptotic expansion, similar to Cramer' s of the number of prime factors (counting multiplicity) of an integer \(m\), as \(m{\to}\infty\).

#### Asymptotic Expansions for Time Series

Cramer-type expansions have been extended to the case of dependent sequences, with an exponentially decaying dependence over time, by Gotze and Hipp (1983). The Edgeworth expansions for smooth statistics based on such time series follow more or less the same way as in the independent case considered in Sect. 11.2 (See, e.g., Gotze and Hipp 1994; Bose 1988). An excellent exposition of the Gotze-Hipp theory, with complete but simplified proofs, is given in Jensen (1986). It should be mentioned that for Markovian sequences a fairly complete extension of Cramer-type expansions was given earlier by Nagaev (1957) (Also see Jensen 1989). An extension of the Gotze-Hipp results under less restrictive polynomial decay rates of dependence was obtained by Lahiri (1996). Second order accuracy of general pivotal bootstrap estimates have been derived by Gotze and Kunsch (1996) and others (See Lahiri 1996 for an up to date account).

### Notes and References

Asymptotic expansions for distributions of normalized sums \(\sqrt{n}(\overline{X}-\mu)/\sigma\) of i.i.d. random variables were stated formally without proof by Chebyshev (1890). Cramer (1928), (1937) provided the first rigorous derivation of Chebyshev type expansions under the so-called _Cramer's condition_. One dimensional expansions for the lattice case were obtained by Esseen (1945). A fine account of these may be found in the monograph by Guedenko and Kolmogorov (1954). Multidimensional expansions of this theory and later developments are detailed in Bhattacharya and Ranga Rao (1976).

Independently of Chebyshev, Edgeworth (1905) proposed asymptotic expansions for distribution functions of general statistics. Edgeworth's innovative idea and corresponding expansions were validated in Bhattacharya and Ghosh (1978).

Efron (1979) provided his novel simulation methodology termed the (nonparametric) _bootstrap,_ originally conceived as a convenient way in the computer age for the construction of critical points of tests and confidence intervals. His percentile bootstrap does not require the computation of the standard error of the statistic, often an onerous task for applied statisticians. But the bootstrap may be used separately to compute the standard error, and when used to pivot the statistic it yielded a more accurate estimate of the distribution. It was shown by Singh (1981) that in the non-lattice case the bootstrap approximation of the distribution function of \(\sqrt{n}(\overline{X}-\mu)/\sigma\) had an error \(o_{p}(n^{-1/2})\) as opposed to the \(O(n^{-1/2})\) error of the classical CLT-based approximation. Since \(\sigma\) is generally unknown, for statistical purposes one needs to estimate the distribution of the studentized statistic \(\sqrt{n}(\overline{X}-\mu)/s\). Babu and Signh (1984) extended Singh's result to this case. That bootstrap approximation of distributions of general smooth pivotal statistics for continuous data improve upon the CLT-based approximation was proved independently, using the expansions in Bhattacharya and Ghosh (1978), by Beran (1987), Bhattacharya (1987), Hall (1988) and Bhattacharya and Qumsiyeh (1989). There does not seem to be much evidence, theoretical or otherwise, that the bootstrap does better than the classical procedure if the underlying distribution is lattice. For precise comparisons for the standardized sample mean in the lattice case, one may compare the relation (23.12) in Bhattacharya and Ranga Rao (1976) and the asymptotic limit in the lattice case in Singh (1981). See Bhattacharya and Chan (1996) for some numerical results in the lattice case; the Appendix in the present chapter is also due to them. Hall (1992) is a good source for the mathematical theory of Edgeworth expansions in relation to the bootstrap. Athreya (1987) showed that the usual bootstrap estimate of the distribution function of a properly normalized \(\overline{X}-\mu\) is not consistent unless the i.i.d. observations belonged to the domain of Normal attraction, a result extended further by Gine and Zinn (1989). This may be contrasted with the result of Bickel and Freedman (1981) in Chap. 9, Remark 9.3, showing that in the linear multiple regression problem there are cases where the bootstrap estimate of the distribution of the vector of regression coefficients is consistent although the latter is not asymptotically Normal.

Efron and Tibshirani (1994) present a readable and very useful account of the bootstrap with diverse applications illustrated by data analyses. Theoretical extensions of the methodology to dependent data and time series are given in Lahiri (1996). Finally, we mention Yoshida (1997) for a novel asymptotic expansion for martingales using Malliavin calculus.

## Appendix: Approximate Moments and Cumulants of \(W_{n}\)

Write \(H(\overline{Z})-H(\mu)=G(\overline{Z})+O_{p}(n^{-3/2})\), where

\[G(\overline{Z})= (\overline{Z}-\mu)\cdot\operatorname{grad}H(\mu)+\frac{1}{2!}\sum _{1\leq i_{1},i_{2}\leq k}D_{i_{1}i_{2}}(\overline{Z}^{(i_{1})}-\mu^{(i_{1})}) (\overline{Z}^{(i_{2})}-\mu^{(i_{2})})\] \[+\frac{1}{3!}\sum_{1\leq i_{1},i_{2},i_{3}\leq k}D_{i_{1}i_{2}i_{ 3}}(\overline{Z}^{(i_{1})}-\mu^{(i_{1})})(\overline{Z}^{(i_{2})}-\mu^{(i_{2})}) (\overline{Z}^{(i_{3})}-\mu^{(i_{3})}).\] \[[(\overline{Z}-\mu)\cdot\operatorname{grad}H(\mu)=\sum_{i}(D_{i}H) (\mu)(\overline{Z}^{(i)}-\mu^{(i)})].\]

**Notation**\((D_{i}H)(z)=(\partial H/\partial z^{(i)})(z)\), \((D_{i_{1}i_{2}}H)(z)=(D_{i_{1}}D_{i_{2}}H)(z)\), \(D_{i}=(D_{i}H)(\mu)\), \(D_{i_{1}i_{2}}=(D_{i_{1}i_{2}}H)(\mu)\), \(D_{i_{1}i_{2}i_{3}}=(D_{i_{1}i_{2}i_{3}}H)(\mu)\), etc., \(\sigma_{i_{1}i_{2}}=E(Z_{j}^{(i_{1})}-\mu^{(i_{1})})(Z_{j}^{(i_{2})}-\mu^{(i_{ 2})})=\mu_{i_{1}i_{2}}\), \(\mu_{i_{1}i_{2}i_{3}}=E(Z_{j}^{(i_{1})}-\mu^{(i_{1})})\cdot(Z_{j}^{(i_{2})}-\mu^ {(i_{2})})(Z_{n}^{(i_{3})}-\mu^{(i_{3})})\), etc., \(m_{r,n}:=EG(\overline{Z})^{r}\), \(\mu_{r,n}=E(\sqrt{n}G(\overline{Z}))^{r}=n^{r/2}m_{r,n}\). We will compute \(\mu_{r,n}\) up to \(O(n^{-3/2})\)\((1\leq r\leq 6)\).

\[m_{1,n} =\frac{a_{1}}{n}+O(n^{-2}),\ a_{1}=\frac{1}{2!}\sum_{i_{1},i_{2}}D _{i_{1}i_{2}}\sigma_{i_{1}i_{2}}, \tag{11.55}\] \[m_{2,n} =\frac{b_{1}}{n}+\frac{b_{2}}{n^{2}}+O(n^{-3}),\] \[b_{1} =\sum_{i_{1},i_{2}}D_{i_{1}}\cdot D_{i_{2}}\sigma_{i_{1}i_{2}}=E( (Z_{j}-\mu)\text{grad}\,H(\mu))^{2},\]

[MISSING_PAGE_EMPTY:12710]

\[+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_{1})}-\mu^{(i _{1})}))\] \[\quad\cdot\bigg{\{}E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{ 1}^{(i_{2})}-\mu^{(i_{2})}))\sigma_{i_{3}i_{4}}\] \[\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_{3} )}-\mu^{(i_{3})}))\sigma_{i_{2}i_{4}}\] \[\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_{4} )}-\mu^{(i_{4})})\sigma_{i_{2}i_{3}}\bigg{\}}\] \[\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_{2} )}-\mu^{(i_{2})}))\] \[\qquad\cdot\bigg{\{}E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))( Z_{1}^{(i_{1})}-\mu^{(i_{1})}))\sigma_{i_{3}i_{4}}\] \[\qquad\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{ (i_{3})}-\mu^{(i_{3})}))\sigma_{i_{1}i_{4}}\] \[\qquad\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}^{(i_{4})}-\mu ^{(i_{4})}))\sigma_{i_{1}i_{3}}\bigg{\}}\] \[\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_{3} )}-\mu^{(i_{3})}))\] \[\quad\cdot\bigg{\{}E(((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))( Z_{1}^{(i_{1})}-\mu^{(i_{1})}))\sigma_{i_{2}i_{4}}\] \[\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_{2} )}-\mu^{(i_{2})}))\sigma_{i_{1}i_{4}}\] \[\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_{4} )}-\mu^{(i_{4})}))\sigma_{i_{1}i_{2}}\bigg{\}}\] \[\quad+E((\operatorname{grad}H(\mu)\cdot Z_{1}-\mu))(Z_{1}^{(i_{4} )}-\mu^{(i_{4})}))\] \[\qquad\cdot\bigg{\{}E((\operatorname{grad}H(\mu)\cdot Z_{1}-\mu))( Z_{1}^{(i_{1})}-\mu^{(i_{1})}))\sigma_{i_{2}i_{3}}\] \[\qquad\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}-\mu))(Z_{1}^{(i_ {2})}-\mu^{(i_{2})}))\sigma_{i_{1}i_{3}}\] \[\qquad\quad+E((\operatorname{grad}H(\mu)\cdot(Z_{1}^{(i_{3})}-\mu ^{(i_{3})}))\sigma_{i_{1}i_{2}}\bigg{\}}\bigg{]}, \tag{11.58}\] \[m_{5,n} =\frac{e_{1}}{n^{3}}+O(n^{-4}),\] \[e_{1} =10E((Z_{1}-\mu)\cdot\operatorname{grad}H(\mu))^{3}\cdot E((Z_{1} -\mu)\cdot\operatorname{grad}H(\mu))^{2}\] \[\quad+\frac{5}{2}\sum_{i_{1},i_{2}}D_{i_{1}i_{2}}E(Z_{1}-\mu)\cdot \operatorname{grad}H(\mu))^{2}\] \[\qquad\cdot\bigg{\{}3\sigma_{i_{1}i_{2}}(E(Z_{1}-\mu)\cdot \operatorname{grad}H(\mu))^{2}\] \[\qquad\quad+12E((Z_{1}-\mu)\cdot\operatorname{grad}H(\mu))(Z_{1}^ {(i_{1})}-\mu^{(i_{1})}))\] \[\qquad\quad\cdot E((Z_{1}-\mu)\cdot\operatorname{grad}H(\mu))(Z_{1} ^{(i_{2})}-\mu^{(i_{2})}))\bigg{\}},\] (11.59) \[m_{6,n} =\frac{f_{1}}{n^{3}}+O(n^{-4}),\] \[f_{1} =15[E(Z_{1}-\mu)\cdot\operatorname{grad}H(\mu))^{2}]^{3}. \tag{11.60}\]

"Approximate" cumulants of \(W_{n}\) are then given by

\[k_{1,n}=\mu_{1,n}=\sqrt{n}\,m_{1,n}=n^{-1/2}a_{1}+O(n^{-3/2})\]\[=\frac{n^{-1/2}}{2}\sum_{i_{1},i_{2}}D_{i_{1}i_{2}}\sigma_{i_{1}i_{2}}+O(n^{-3/2})\] \[k_{2,n} =\mu_{2,n}-\mu_{1,n}^{2}=n(m_{2,n}-m_{1,n}^{2})\] \[=b_{1}+\frac{b_{2}-a_{1}^{2}}{n}+O(n^{-2}),\] \[k_{3,n} =\mu_{3,n}-3\mu_{2,n}\mu_{1,n}+2\mu_{1,n}^{3}=n^{-1/2}(c_{1}-3b_{1 }a_{1})+O(n^{-3/2}),\] \[k_{4,n} =\mu_{4,n}-4\mu_{3,n}\mu_{1,n}-3\mu_{2,n}^{2}+12\mu_{2,n}\mu_{1,n} ^{2}-6\mu_{1,n}^{4}\] \[=d_{1}+\frac{d_{2}}{n}-\frac{4a_{1}c_{1}}{n}-3\left(b_{1}^{2}+ \frac{2b_{1}b_{2}}{n}\right)+12\frac{a_{1}^{2}b_{1}}{n}+O(n^{-2})\] \[=n^{-1}(d_{2}-4a_{1}c_{1}-6b_{1}b_{2}+12a_{1}^{2}b_{1})+O(n^{-2}). \tag{11.61}\]

As an example for Student's \(t\),

\[k_{1,n} =-\frac{1}{2}\mu_{3}n^{-1/2}+O(n^{-3/2}),\] \[k_{2,n} =1+n^{-1}(2\mu_{3}^{2}+3)-n^{-1}\left(\frac{1}{4}\mu_{3}^{2}\right) +O(n^{-2})\] \[=1+n^{-1}\left(\frac{7}{4}\mu_{3}^{2}+3\right)+O(n^{-2})\] \[k_{3,n} =n^{-1/2}\left(-\frac{7}{2}\mu_{3}+\frac{3}{2}\mu_{3}\right)+O(n^ {-3/2})=-2n^{-1/2}\mu_{3}+O(n^{-3/2})\] \[k_{4,n} =n^{-1}[-2\mu_{4}+28\mu_{3}^{2}+30-7\mu_{3}^{2}-6(2\mu_{3}^{2}+3) +3\mu_{3}^{2}]+O(n^{-2})\] \[=n^{-1}(-2\mu_{4}+12\mu_{3}^{2}+12)+O(n^{-2})\] \[=-2n^{-1}(\mu_{4}-6\mu_{3}^{2}-6)+O(n^{-2}). \tag{11.62}\]

### Exercises for Chap. 11

**Ex. 11.1.** Derive (a) the Edgeworth expansion for the distribution function of the (nonparametric) Student's \(t\), using (11.62), and under appropriate conditions, and (b) prove the analog of (11.49) for the coverage error of the bootstrap approximation for a symmetric confidence interval for the mean based on \(t\).

## References

* Babu & Signh (1984) Babu, G. J., & Signh, K. (1984). On one term Edgeworth correction by Efron's bootstrap. _Sankhya Series A, 46_, 219-232.
* Bentkus & Gotze (1999) Bentkus, V., & Gotze, F. (1999). Lattice point problems and distribution of values of quadratic forms. _Annals of Mathematics, 150_, 977-1027.
* Beran (1987) Beran, R. (1987). Prepivoting to reduce level error of confidence sets. _Biometrika, 74_, 457-468.
* Bhattacharya (1977) Bhattacharya, R. N. (1977) Refinements of the multidimensional central limit theorem and applications. _Annals of Probability, 5_, 1-28.

* Bhattacharya (1987) Bhattacharya, R. N. (1987) Some aspects of Edgeworth expansions in statistics and probability. In M. L. Puri, J. P. Vilaplana, & W. Wertz (Eds.), _New perspectives in theoretical and applied statistics_. New York: Wiley.
* Bhattacharya & Chan (1996) Bhattacharya, R. N., & Chan, N. H. (1996). Edgeworth expansions and bootstrap approximations to the distributions of the frequency chisquare. _Sankhya Series A, 58_, 57-68.
* Bhattacharya & Denker (1990) Bhattacharya, R. N., & Denker, M. (1990). _Asymptotic statistics_. Boston: Birkhauser.
* Bhattacharya & Ghosh (1978) Bhattacharya, R. N., & Ghosh, J. K. (1978). On the validity of the formal Edgeworth expansion. _Annals of statistics, 6_, 434-451.
* Bhattacharya & Qumsiyeh (1989) Bhattacharya, R. N., & Qumsiyeh, M. (1989). Second order and \(L^{p}\)-comparisons between the bootstrap and empirical Edgeworth expansion methodologies. _Annals of Statistics, 17_, 160-169.
* Bhattacharya & Ranga Rao (1976) Bhattacharya, R. N., & Ranga Rao, R. (1976). _Normal approximation and asymptotic expansions._ New York: Wiley; Revised Reprint, SIAM Reprint with a new chapter in SIAM No.64. Malabar: Krieger (2010).
* Bickel et al. (1981) Bickel, P. J., Chibishov, D. N., & van Zwet, W. R. (1981). On efficiency of first and second order. _International Statistics Revised, 49_, 169-175.
* Chandra & Ghosh (1979) Chandra, T. K., & Ghosh, J. K. (1979) Valid asymptotic expansions for the likelihood ratio statistic and other perturbed chi-square alternatives. _Sankhya Series A, 41_, 22-47.
* Chandra & Ghosh (1980) Chandra, T. K., & Ghosh, J. K. (1980). Valid asymptotic expansions for the likelihood ratio and other statistics under contiguous alternatives. _Sankhya Series A, 42_, 170-184.
* Chebyshev (1890) Chebyshev, P. L. (1890). Sur deux theoremes relatifs aux probabilities. _Acta Mathematica, 14_, 305-315.
* Chibishov (1973) Chibishov, D. M. (1973). An asymptotic expansion for a class of estimators containing maximum likelihood estimators. _Theory of Probability and Its Applications, 18_, 295-303.
* Cornish & Fisher (1937) Cornish, E. A., & Fisher, R. A. (1937). Moments and cumulants in the specification of distributions. _Review of the Institute of Mathematical Statistics, 5_, 307-322.
* Cramer (1928) Cramer, H. (1928). On the composition of elementary errors. _Skandinaviak Aktuarietidskr, 11_, 13-74, 141-180.
* Cramer (1937) Cramer, H. (1937). _Random variables and probability distribution._ Cambridge: Cambridge University Press
* Edgeworth (1905) Edgeworth, F. Y. (1905). The law of error. _Proceedings of the Cambridge Philosophical Society, 20_, 36-65.
* Efron & Tibshirani (1994) Efron, B., & Tibshirani, R. (1994). _An introduction to the bootstrap_. New York: Chapman and Hall.
* Esseen (1945) Esseen, C. G. (1945). Fourier analysis of distribution functions: A mathematical study of the Laplace-Gaussian law. _Acta Mathematica, 77_, 1-125.
* Fisher (1925) Fisher, R. A. (1925). Theory of statistical estimation. _Proceedings of the Cambridge Philological Society, 22_, 700-725.
* Ghosh (1994) Ghosh, J. K. (1994). _Higher order asymptotics_. NSF-CBMS Regional Conference Series in Probability and Statistics (Vol. 4). Hayward: Institute of Mathematical Statistics.
* Gine & Zinn (1989) Gine, E., & Zinn, J. (1989). Necessary conditions for the bootstrap of the mean. _Annals of Statistics, 17_(2), 684-691.

* Gotze & Hipp (1983) Gotze, F., & Hipp, C. (1983). Asymptotic expansions for sums of weakly dependent random vectors. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete, 64_, 211-239.
* Gotze & Hipp (1994) Gotze, F., & Hipp, C. (1994). Asymptotic distribution of statistics in time series. _Annals of Statistics, 22_, 2062-2088.
* Gotze & Kunsch (1996) Gotze, F., & Kunsch, H. R. (1996). Second order correctness of the blockwise bootstrap for stationary observations. _Annals of Statistics, 24_, 1914-1933.
* Groeneboom & Oosterhoff (1981) Groeneboom, P., & Oosterhoff, J. (1981). Bahadur efficiency and small sample efficiency. _International Statistical Review, 49_, 127-142.
* Guedenko & Kolmogorov (1954) Guedenko, B. V., & Kolmogorov, A. N. (1954). _Limit distributions of sums of independent random variables_ (K.L. Chung, Trans.). London: Addison-Wesley.
* Hall (1983) Hall, P. (1983). Inverting an Edgeworth expansion. _Annals of Statistics, 11_, 569-576.
* Hall (1988) Hall, P. (1988). Theoretical comparison of bootstrap confidence intervals. _Annals of Statistics, 16_, 927-985.
* Hall (1992) Hall, P. (1992). _The bootstrap and Edgeworth expansions._ New York: Springer.
* Ivanov & Zwanzig (1983) Ivanov, A. V., & Zwanzig, S. (1983). An asymptotic expansion of the distribution of least squares estimators in the nonlinear regression model. _Mathematische Operationsforschung und Statistik, Series Statistics, 14_, 7-27.
* James (1958) James, G. S. (1958). On moments and cumulants of system of statistics. _Sankhya, 20_, 1-30.
* Jensen (1986) Jensen, J. L. (1986). A note on the work of Gotze and Hibb concerning asymptotic expansions for sums of weakly dependent random vectors (Memoirs no. 10). Department of Mathematics, Aarhus University.
* Jensen (1989) Jensen, J. L. (1989). Asymptotic expansions for strongly mixing Harris recurrent Markov chains. _Scandinavian Journal of Statistics, 16_, 47-64.
* Konakov & Mammen (2005) Konakov, V., & Mammen, E. (2005). Edgeworth-type expansions for transition densities of Markov chains converging to diffutions. _Bernoulli, 11_, 591-641.
* Kubilius (1964) Kubilius, J. (1964). _Probabilistic methods in the theory of numbers._ Translations for Mathematical Monographs. Providence: American Mathematical Society.
* Lahiri (1996) Lahiri, S. N. (1996). Asymptotic expansions for sums of random vectors under polynomial mixing rates. _Sankhya Series A, 58_, 206-224.
* Leonov & Shiryaev (1959) Leonov, V. P., & Shiryaev, A. N. (1959). On a method of calculation of semi-invariants. _Theory of Probability and Its Applications, 4_, 319-329.
* Nagaev (1957) Nagaev, S. V. (1957). Some limit theorems for stationary Markov chains. _Theory of Probability and Its Applications, 71_, 378-405.
* Pfanzagl (1980) Pfanzagl, J. (1980). Asymptotic expansions in parametric statistical theory. _Developments in Statistics (Krishnaiah, P.R., Ed.), 3_, 1-97.
* Pfanzagl (1985) Pfanzagl, J. (1985). _Asymptotic expansions for general statistical models._ Lecture Notes in Statistics (Vol. 31). Berlin: Springer.
* Qumsiyeh (1989) Qumsiyeh, M. (1989). Edgeworth expansion in regression models. _Journal of Multivariate Analysis_
* Rao (1961) Rao, C. R. (1961). Asymptotic efficiency and limiting information. _Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, 1_, 531-546.
* Rao (1962) Rao, C. R. (1962). Efficient estimates and optimum inference procedures in large samples. _Journal of the Royal Statistical Society, Series B, 24_, 45-63.
* Rao (1963) Rao, C. R. (1963). Criteria of estimation in large samples. _Sankhya Series A, 25_, 189-206.

* Singh (1981) Singh, K. (1981). On the asymptotic accuracy of Efron's bootstrap. _Annals of Statistics, 9_, 1187-1195.
* Yoshida (1997) Yoshida, N. (1997). Malliavin calculus and asymptotic expansion for martingales. _Probability Theory and Related Fields, 190_, 301-312.

## Chapter 12 Frechet Means and Nonparametric Inference on Non-Euclidean Geometric Spaces

### 12.1 Introduction

Among statistical analysis on non-Euclidean spaces, statistics on spheres \(S^{d}=\{x\in\mathbb{R}^{d+1}:|x|^{2}=1\}\), \(d\geq 1\), has a long history (See, e.g., Watson 1983 and Mardia and Jupp 2000). Its growth was especially spurred on by the seminal 1953 paper of R.A. Fisher providing conclusive statistical evidence that the earth's magnetic poles had dramatically shifted positions over geological time scales. This was a fundamental contribution to paleomagnetism--a field of earth science devoted to the study of fossil magnetism (See Irving 1964). This theory also has important consequences in the field of tectonics, especially to the older theory that the continents had changed their relative positions over a period of several hundred millions of years. If rock samples in different continents dating back to the same period exhibit different magnetic polarities, that would be a confirmation of the theory of continental drift. See Fisher et al. (1987) for examples of data on so-called remanent magnetism in fossilized rock samples. In Sect. 12.3 we discuss the parametric theory of R.A. Fisher (1953) and compare that with the nonparametric theory based on Frechet means developed in Sect. 12.2.

Due to advances in modern technology in recent decades, digital images are now available and extensively used in biology, medicine and many other areas of science and technology. An important class of examples are landmarks based images whose analysis was pioneered by D.G. Kendall (1977, 1984) and F. Bookstein (1978). As described in Sect. 12.2, such an image on the plane may be viewed as an orbit under rotation of a point on a sphere of high dimension known as the preshape sphere. The present chapter is devoted to the nonparametric statistical analysis of data on such non-Euclidean spaces \(M\) by means of Frechet means of the samples, whose asymptotic distribution theory is described in the next section.

### Frechet Means on Metric Spaces

Let \((M,\rho)\) be a metric space and \(Q\) a probability measure on the Borel sigma-field of \(M\). Consider a _Frechet function_ of \(Q\) defined by

\[F(x)=\int\rho^{\alpha}(x,y)Q(dy),\quad x\in M. \tag{12.1}\]

for some \(\alpha\geq 1\). We will be mostly concerned with the case \(\alpha=2\). Assume that \(F\) is finite. A minimizer of \(F\), if unique, serves as a measure of location of \(Q\). In general, the set \(C_{Q}\) of minimizers of \(F\) is called the _Frechet mean set_ of \(Q\). In the case the minimizer is unique, one says that the _Frechet mean exists_ and refers to it as the _Frechet mean_ of \(Q\). If \(X_{1},\ldots,X_{n}\) are i.i.d. observations with common distribution \(Q\), the Frechet mean set and the Frechet mean of the empirical \(Q_{n}=1/n\sum_{1\leq j\leq n}\delta_{X_{j}}\) are named the _sample Frechet mean set_ and the _sample Frechet mean,_ respectively. For a reason which will be clear from the result below, in the case the Frechet mean of \(Q\) exists, a (every) measurable selection from \(C_{Q_{n}}\) is taken to be a sample Frechet mean.

Remark Remark 12.1: For \(M=\mathbb{R}^{m}\) with the Euclidean norm \(|\cdot|\) and distance \(\rho(x,y)=|x-y|\), the Frechet mean for the case \(\alpha=2\) is the same as the usual mean \(\int yQ(dy)\), provided \(\int|y|^{2}Q(dy)<\infty\). The generalization considered here is due to Frechet (1948).

The following is a general result on Frechet mean sets \(C_{Q}\) and \(C_{Q_{n}}\) of \(Q\) and \(Q_{n}\) and _consistency_ of the sample Frechet mean. It is due to Ziezold (1977) and Bhattacharya and Patrangenaru (2003).

Theorem 12.1: _Let \(M\) be a metric space such that every closed and bounded subset of \(M\) is compact. Suppose \(\alpha\geq 1\) in (12.1) and \(F(x)\) is finite. Then (a) the Frechet mean set \(C_{Q}\) is nonempty and compact, and (b) given any \(\varepsilon>0\), there exists a positive integer valued random variable \(N=N(\omega,\varepsilon)\) and a \(P\)-null set \(\Omega(\varepsilon)\) such that_

\[C_{Q_{n}}\subseteq C_{Q^{c}}=\{x\in M:\rho(x,C_{Q})<\varepsilon\}\forall\,n \geq N,\;\forall\,\omega\in(\Omega(\varepsilon))^{c}. \tag{12.2}\]

_(c) In particular, if the Frechet mean of \(Q\) exists then the sample Frechet mean, taken as a measurable selection from \(C_{Q_{n}}\), converges almost surely to it._

Proof: We give a proof of Theorem 12.1 for a compact metric \(M\), which is the case in many of the applications of interest here. Part (a) is then trivially true. For part (b), for each \(\varepsilon>0\), write

\[\eta=\inf\{F(x):x\in M\}\equiv F(q)\,\forall\,q\in C_{Q},\] \[\eta+\delta(\varepsilon)=\inf\{F(x):x\in M\backslash C_{Q^{ \varepsilon}}\}. \tag{12.3}\]

If \(C^{\varepsilon}_{Q}=M\), then (12.2) trivially holds. Consider the case \(C^{\varepsilon}_{Q}\neq M\), so that \(\delta(\varepsilon)>0\). Let \(F_{n}(x)\) be the Frechet function of \(Q_{n}\), namely,

\[F_{n}(x)=\frac{1}{n}\sum_{1\leq j\leq n}\rho^{\alpha}(x,X_{j}).\]Now use the elementary inequality,

\[|\rho^{\alpha}(x,y)-\rho^{\alpha}(x^{\prime},y)|\leq\alpha\rho(x,x^{\prime})\left[ \rho^{\alpha-1}(x,y)+\rho^{\alpha-1}(x^{\prime},y)\right]\leq c\alpha\rho(x,x^{ \prime}),\]

with \(c=2\max\{\rho^{\alpha-1}(x,y),\,x,y\in M\}\), to obtain

\[|F(x)-F(x^{\prime})|\leq c\alpha\rho(x,x^{\prime}),\;|F_{n}(x)-F_{n}(x^{\prime })|\leq c\alpha\rho(x,x^{\prime}),\;\forall\,x,x^{\prime}. \tag{12.4}\]

For each \(x\in M\setminus C_{Q^{\varepsilon}}\) find \(r=r(x,\varepsilon)>0\) such that \(c\alpha\rho(x,x^{\prime})<\delta(\varepsilon)/4\;\forall\,x^{\prime}\) within a distance \(r\) from \(x\). Let \(m=m(\varepsilon)\) of balls with centers \(x_{1},\cdots,x_{m}\) and radii \(r(x_{1}),\ldots,r(x_{m})\) (in \(M\setminus C_{Q^{\varepsilon}}\)) cover \(M\setminus C_{Q^{\varepsilon}}\). By the SLLN, there exist integers \(N_{i}=N_{i}(\omega)\) such that, outside a \(P\)-null set \(\Omega_{i}(\varepsilon)\), \(|F_{n}(x_{i})-F(x_{i})|<\delta(\varepsilon)/4\;\forall\,n\geq N_{i}\) (\(i=1,\ldots,m\)). Let \(N^{\prime}=\max\{N_{i}:i=1,\ldots,m\}\). If \(n>N^{\prime}\), then for every \(i\) and all \(x\) in the ball with center \(x_{i}\) and radius \(r(x_{i},\varepsilon)\),

\[F_{n}(x) > F_{n}(x_{i})-\delta(\varepsilon)/4>F(x_{i})-\delta(\varepsilon)/4 -\delta(\varepsilon)/4\] \[\geq \eta+\delta(\varepsilon)-\delta(\varepsilon)/2=\eta+\delta( \varepsilon)/2.\]

Next choose a point \(q\in C_{Q}\) and find \(N^{\prime\prime}=N^{\prime\prime}(\omega)\), again by the SLLN, such that, if \(n\geq N^{\prime\prime}\) then \(|F_{n}(q)-F(q)|<\delta(\varepsilon)/4\) and, consequently, \(F_{n}(q)<\eta+\delta(\varepsilon)/4\), outside of a \(P\)-null set \(\Omega^{\prime\prime}(\varepsilon)\). Hence (12.2) follows with \(N=\max\{N^{\prime},N^{\prime\prime}\}\) and \(\Omega(\varepsilon)=\{\cup\,\Omega_{i}(\varepsilon):i=1,\ldots,m\}\cup\Omega^ {\prime\prime}(\varepsilon)\). Part (c) is an immediate consequence of part (b).

For noncompact \(M\), the proof of Theorem 12.1 is a little more elaborate and may be found in Bhattacharya and Bhattacharya (2012) or, for the case \(\alpha=2\), in Bhattacharya and Patrangenaru (2003).

In the applications considered here, the space \(M\) of observations is a manifold, i.e., a space which is locally like an Euclidean space of dimension \(d\) with a differentiable structure. Here is the formal definition.

**Definition 12.1**.: A \(d\)_-dimensional differentiable manifold_\(M\) is a separable metric space with the following properties:

1. Every point \(p\in M\) has an open neighborhood \(U_{p}\) with a homeomorphism \(\psi_{p}:U_{p}\longrightarrow B_{p}\), where \(B_{p}\) is an open subset of \(\mathbb{R}^{d}\).
2. The maps \(\psi_{p}\) are smoothly compatible; that is, if \(U_{p}\cap U_{q}\neq\emptyset\), then \(\psi_{p}\circ\psi_{q}^{-1}\) is an infinitely differentiable diffeomorphism on \(\psi_{q}(U_{p}\cap U_{q})\subset B_{q}\) onto \(\psi_{p}(U_{p}\cap U_{q})\subset B_{p}\).

The pair \((U_{p},\psi_{p})\) is called a _coordinate neighborhood_ of \(p\) and \(\psi_{p}(p^{\prime})=(p^{\prime}_{1},p^{\prime}_{2},\)\(\ldots,p^{\prime}_{d})\) are the _local coordinates_ of \(p^{\prime}\in U_{p}\), and the collection of all coordinate neighborhoods is an _atlas_ for \(M\).

_Example 12.1_ (The Sphere \(S^{d}\)).: Consider the sphere \(M=S^{d}=\{p\in\mathbb{R}^{d+1}|:|p|^{2}=1\}\). For \(p\in S^{d}\), the tangent space at \(p\) is \(T_{p}=T_{p}(S^{d})=\{x\in\mathbb{R}^{d+1}:p.x=0\}\), where \(p.x=\langle p,x\rangle\) is the Euclidean inner product. Note that \(T_{p}\) is isomorphic to \(\mathbb{R}^{d}\), with \(y=(y_{1},y_{2},\ldots,y_{d})\in\mathbb{R}^{d}\) corresponding to \(\sum y_{i}e_{i}\) where \(e_{1},\ldots,e_{d}\) comprise an orthonormal basis of \(T_{p}\). Let \(U_{p}=\{q\in S^{d}:|q-p|<1/2\}\) and \(\psi_{p}\) the projection of \(U_{p}\) into \(T_{p}:q\to q-(q.p)p=\sum y_{i}e_{i}\), so that \(B_{p}=\psi_{p}(U_{p})\) considered as a subset of \(\mathbb{R}^{d}\).

An important task in the use of Frechet means for inference is to choose a proper distance \(\rho\) for \(M\). If \(M\) is a submanifold of an Euclidean space \(E^{N}\) (of dimension \(N>d\)) such as a (hyper-)surface, then one may just use the distance inherited from \(E^{N}\). For example, the sphere \(M=S^{d}=\{p\in\mathbb{R}^{d+1}|:|p|^{2}=1\}\) is a submanifold of \(E^{d+1}=\mathbb{R}^{d+1}\), and a natural distance on it may be the _chord distance_ inherited from \(\mathbb{R}^{d+1}\) as a subset. One may, alternatively, use the geodesic distance on \(S^{d}\). The _geodesic distance_\(\rho=d_{g}\) between two points \(p\) and \(q\) is the arc length (of the smaller of the two arcs) along the big circle connecting \(p\) and \(q\). For general manifolds the notion of the geodesic distance depends on endowing \(M\) with a Riemannian structure, i.e., a metric tensor, which is not considered here. Instead, we will consider the following analog of the chord distance.

**Definition 12.2**.: An _embedding_\(J\) of a \(d\)-dimensional manifold \(M\) is a map of \(M\) into an Euclidean space \(E^{N}\) which is (i) a homeomorphism of \(M\) onto its image \(J(M)\) which is given the relative topology of \(E^{N}\), and (ii) in local coordinates, infinitely differentiable with the Jacobian (of \(J\)) of rank \(d\) at every point of \(M\). The _extrinsic distance_\(\rho=\rho^{J}\) on \(M\) under \(J\) is the (Euclidean) distance on \(E^{N}\) restricted to \(J(M)\); that is, \(\rho(p,q)=|J(p)-J(q)|\)\(\forall\,p,q\in M\), where \(|x|^{2}=\langle x,x\rangle\) is the squared norm of \(x\) in \(E^{N}\) with inner product \(\langle\,,\,\rangle\). The Frechet mean \(\mu_{E}\), say, if it exists as the unique minimizer under the extrinsic distance is called the _extrinsic mean_ of \(Q\). In order that the extrinsic mean be well defined we assume that \(J(M)\) is _closed,_ which is automatic if \(M\) is compact.

Consider the case \(\alpha=2\). Letting \(Q^{J}=Q\circ J^{-1}\) denote the distribution induced on \(E^{N}\) from \(Q\) by \(J\), and \(\mu^{J}\) the usual (Euclidean) mean \(\int yQ^{J}(dy)\) of \(Q^{J}\), the Frechet function \(F^{J}\) under the _extrinsic distance under the embedding \(J\) may be expressed at a point_\(p=J^{-1}(x)\), \(x\in J(M)\), as

\[F^{J}(p) =\int_{EN}|x-y|^{2}Q^{J}(dy)=\int_{EN}|x-\mu^{J}+\mu^{J}-y|^{2}Q^ {J}(dy)\] \[=|x-\mu^{J}|^{2}+\int_{EN}|y-\mu^{J}|^{2}Q^{J}(dy)\quad(x=J(p)). \tag{12.5}\]

The following is a simple but very useful result concerning the Frechet mean under the extrinsic distance. Let \(P=P^{J}\) denote the orthogonal projection on \(E^{N}\) onto \(J(M)\). That is, \(P(x)=P^{J}(x)\) is the point of \(J(M)\) closest to \(x\) in the Euclidean distance, provided there is a unique such point of \(J(M)\). From (12.5) it immediately follows that the minimum of \(F^{J}(p)\) is attained at \(p\) whose image \(x\), if unique, minimizes \(|x-\mu^{J}|^{2}\) over \(J(M)\). We have then proved the following result (Bhattacharya and Patrangenaru 2003). In particular, the uniqueness of the projection is the necessary and sufficient condition for the existence of the extrinsic mean.

**Proposition 12.1**.: _Assume that \(\int_{E^{N}}|y|^{2}Q^{J}(dy)<\infty\) and there is a unique point in \(J(M)\) closest to \(\mu^{J}\), namely, \(P(\mu^{J})\). Then \(P(\mu^{J})\) is the image of the Frechet mean \(\mu_{E}\) in \(J(M)\); that is, \(\mu_{E}=J^{-1}(P(\mu^{J}))\)._

The next task is to find the asymptotic distribution of the sample extrinsic Frechet mean \(\mu_{nE}\), whose image \(J(\mu_{nE})\) under \(J\) is \(P(\mu^{J}_{n})\) with \(\mu^{J}_{n}=\overline{Y}\) as the mean of the empirical \(Q^{J}_{n}\) of \(Y_{i}=J(X_{i})\) based on i.i.d. observations \(X_{i}\) on \(M\) with common distribution \(Q\) (\(1\leq i\leq n\)). Denote by \(d_{\mu^{J}}P\) the \(N\times N\) Jacobian matrix at \(\mu^{J}\) of the projection \(P\) on a neighborhood of \(\mu^{J}\in E^{N}\approx R^{N}\) into \(J(M)\subset E^{N}\) considered as a map on \(E^{N}\) into \(E^{N}\). Since \(J(M)\) is of dimension \(d<N\), \(d_{\mu^{J}}P\) is singular and has rank \(d\). Indeed, \(d_{\mu^{J}}P\) maps the tangent space \(T_{\mu^{J}}(E^{N})\approx R^{N}\) onto the tangent space of \(J(M)\) at the point \(P(\mu^{J})\), namely, \(T_{P(\mu^{J})}(J(M))\). Let \(F_{1},\ldots,F_{d}\) be an orthonormal basis of the latter tangent space. Then one has

\[P(\mu^{J}_{n})-P(\mu^{J}) =(d_{\mu^{J}}P)(\overline{Y}-\mu^{J})+o\left(|\overline{Y}-\mu^{J} |\right)\] \[=\sum_{1\leq i\leq d}\langle(d_{\mu^{J}}P)(\overline{Y}-\mu^{J}), F_{i}\rangle F_{i}+o(|\overline{Y}-\mu^{J}|), \tag{12.6}\]

where \(\langle\,,\,\rangle\) is the inner product in \(E^{N}\). The asymptotic distribution of the image (under \(J\)) of the sample extrinsic mean \(P(\overline{Y})\) on the tangent space of \(J(M)\) at \(P(\mu^{J})\), given in terms of its coordinates (with respect to the basis \(\{F_{1},F_{2},\ldots,F_{d}\}\)), now follows from (12.6) using the classical CLT for \(\sqrt{n}(\overline{Y}-\mu^{J})\) (See Bhattacharya and Patrangenaru 2003 and Bhattacharya and Bhattacharya 2012, pp. 38, 39).

**Theorem 12.2**.: _Suppose that the projection \(P\) is uniquely defined and continuously differentiable in a neighborhood of \(\mu^{J}\), and that the \(N\times N\) covariance matrix \(V\) of \(Y_{i}\) is nonsingular. Then_

\[\left(\langle\sqrt{n}(d_{\mu^{J}}P)(\overline{Y}-\mu^{J}),F_{i}\rangle:i=1, \ldots,d\right)\stackrel{{\mathscr{L}}}{{\longrightarrow}}N(0, \varGamma), \tag{12.7}\]

_where \(\varGamma\) is the nonsingular \(d\times d\) covariance matrix given by \(\varGamma=F(d_{\mu^{J}}P)\)\(V(d_{\mu^{J}}P)^{\prime}F^{\prime}\), with the rows of the \(d\times N\) matrix \(F\) being \(F_{1},\ldots,F_{d}\)._

This theorem then has the following corollary, using a Slutsky type argument in replacing \(d_{\mu^{J}}P\) by \(d_{\overline{Y}}P\) and \(\varGamma\) by \(\widehat{\varGamma}=[\widehat{F}(d_{\overline{Y}}P)]^{\prime}\widehat{V}[ \widehat{F}(d_{\overline{Y}}P)]\), where the rows of \(\widehat{F}\) are the orthonormal basis \(\{\widehat{F}_{1},\widehat{F}_{2},\ldots,\widehat{F}_{d}\}\) of the tangent space of \(J(M)\) at \(P(\overline{Y})\) corresponding smoothly to \(\{F_{1},F_{2},\cdots,F_{d}\}\), and \(\widehat{V}\) is the sample covariance matrix of \(Y_{i}\) (\(i=1,\ldots,n\)).

**Corollary 12.1**.: _Under the hypothesis of Theorem 12.2, one has_

\[n\left[(d_{\overline{Y}}P)(\overline{Y}-\mu^{J})\right]^{\prime}\widehat{ \varGamma}^{-1}\left[(d_{\overline{Y}}P)(\overline{Y}-\mu^{J})\right] \stackrel{{\mathscr{L}}}{{\longrightarrow}}\chi^{2}(d)\text{ as }n\to\infty \text{,} \tag{12.8}\]

_and a confidence region for the extrinsic mean \(\mu_{E}\) of asymptotic confidence level \(1-\alpha\) is given by_

\[\left\{\mu_{E}=J^{-1}P(\mu^{J}):n\left[(d_{\mu^{J}}P)(\overline{Y}-\mu^{J}) \right]^{\prime}\widehat{\varGamma}^{-1}\left[(d_{\mu^{J}}P)(\overline{Y}-\mu ^{J})\right]\leq\chi^{2}_{1-\alpha}(d)\right\}, \tag{12.9}\]

_where one may replace \(d_{\mu^{J}}\) by \(d_{\overline{Y}}\)._

We next consider the two-sample problem of distinguishing two distributions \(Q_{1}\) and \(Q_{2}\) on \(M\), based on two independent samples of sizes \(n_{1}\) and \(n_{2}\), respectively: \(\{Y_{j_{1}}=J(X_{j_{1}}):j=1,\ldots,n_{1}\}\), \(\{Y_{j_{2}}=J(X_{j_{2}}):j=1,\ldots,n_{2}\}\). Hence the proper null hypothesis is \(H_{0}:Q_{1}=Q_{2}\). For high dimensional \(M\) it is often sufficient to test if the two Frechet means are equal. For the extrinsic procedure, again consider an embedding \(J\) into \(E^{N}\). Write \(\mu_{i}\) for \(\mu_{i}^{J}\) for the population means and \(\overline{Y}_{i}\) for the corresponding sample means on \(E^{N}\) (\(i=1,2\)). Let \(n=n_{1}+n_{2}\), and assume \(n_{1}/n\to p_{1}\), \(n_{2}/n\to p_{2}=1-p_{1}\), \(0<p_{i}<1\) (\(i=1,2\)), as \(n\to\infty\). If \(\mu_{1}\neq\mu_{2}\) then \(Q_{1}\neq Q_{2}\). One may then test \(H_{0}:\mu_{1}=\mu_{2}\) (\(=\mu\), say). Since \(N\) is generally quite large compared to \(d\), the direct test for \(H_{0}:\mu_{1}=\mu_{2}\) based on \(\overline{Y}_{1}-\overline{Y}_{2}\) is generally not a good test. Instead, we compare the two extrinsic means \(\mu_{E_{1}}\) and \(\mu_{E_{2}}\) of \(Q_{1}\) and \(Q_{2}\) and test for their equality. This is equivalent to testing if \(P(\mu_{1})=P(\mu_{2})\). Then, assuming \(H_{0}\),

\[n^{\frac{1}{2}}d_{\overline{Y}}P(\overline{Y}_{1}-\overline{Y}_{2})\longrightarrow N(0,B(p_{1}V_{1}+p_{2}V_{2})B^{\prime}) \tag{12.10}\]

in distribution, as \(n\to\infty\). Here \(\overline{Y}=\frac{n1}{n}\overline{Y}_{1}+\frac{n2}{n}\overline{Y}_{2}\) is the _pooled estimate_ of the common mean \(\mu_{1}=\mu_{2}=\mu\), say, \(B=B(\mu)=F(d_{\mu}P)\) and \(V_{1},V_{2}\) are the covariance matrices of \(Y_{j_{1}}\) and \(Y_{j_{2}}\). This leads to the asymptotic chi-square statistic below:

\[n\left[d_{\overline{Y}}P(\overline{Y}_{1}-\overline{Y}_{2})\right]^{\prime} \left[\widehat{B}(\hat{p}_{1}\widehat{V}_{1}+\hat{p}_{2}\widehat{V}_{2}) \widehat{B}^{t}\right]^{-1}\left[d_{\overline{Y}}P(\overline{Y}_{1}-\overline{Y}_{2})\right]\longrightarrow\chi^{2}(d) \tag{12.11}\]

in distribution, as \(n\to\infty\). Here \(\widehat{B}=B(\overline{Y})\), \(\widehat{V}_{i}\) is the sample covariance matrix of \(Y_{ji}\), and \(\hat{p}_{i}=n_{i}/n\) (\(i=1,2\)). One rejects the null hypothesis \(H_{0}\) at a level of significance \(1-\alpha\) if and only if the observed value of the left side of (12.11) exceeds \(\chi^{2}_{1-\alpha}(d)\).

Example 12.2: Consider again the sphere \(S^{d}\) of Example 12.1, and let \(J\) be the inclusion map. Then \(P(x)=x/|x|\) (\(x\neq 0\)). It is not difficult to check that the Jacobian matrix \(d_{x}P\) is given by

\[d_{x}P=|x|^{-1}\left[I_{d+1}-|x|^{-2}(xx^{t})\right],\;x\neq 0. \tag{12.12}\]

Let \(F(x)\) be a \(d\times(d+1)\) matrix whose rows form an orthonormal basis of \(T_{x}(S^{d})=\{q\in R^{d+1}:x\cdot q=0\}\). One may apply Theorem 12.2 and Corollary 12.1. For \(d=2\), and \(x=(x_{1},x_{2},x_{3})^{\prime}\neq(0,0,\pm 1)^{\prime}\) and \(x_{3}\neq 0\), one may choose the two rows of \(F(x)\) as \((-x_{2},x_{1},0)/\sqrt{x_{1}^{2}+x_{2}^{2}}\) and \((x_{1},x_{2},-(x_{1}^{2}+x_{2}^{2})/x_{3})c\) with \(c=[x_{1}^{2}+x_{2}^{2}+(x_{1}^{2}+x_{2}^{2})^{2}/x_{3}^{2}]^{1/2}\). For \(x=(0,0,1)\) one may simply take the two rows of \(F(x)\) as \((1,0,0)\) and \((0,1,0)\). If \(x_{3}=0\) and \(x_{1}\neq 0\), \(x_{2}\neq 0\), then take the two rows as \((-x_{2},x_{1},0)\) and \((0,0,1)\). Permuting the indices all cases are now covered.

Example 12.3 (Kendall's Planar Shape Space \(\Sigma^{k}_{2}\)): In landmarks based image analysis on the plane one chooses, with expert help, \(k>2\) points or landscapes, not all the same, on an image in the plane \(\mathbb{R}^{2}\). The ordered set of \(k\) points--a \(k\)-_ad_--(\((x_{1},y_{1}),\ldots,(x_{k},y_{k})\)) is more conveniently viewed as a \(k\)-tuple \(\mathbf{z}=(z_{1},\ldots,z_{k})\) in the complex plane \(\mathbb{C}\), \(z_{j}=x_{j}+iy_{j}\) (\(j=1,\ldots,k\)). The _shape_\(\sigma(\mathbf{z})\) of a \(k\)-ad is defined to be \(\mathbf{z}\) modulo translation, scaling and rotation, in order that images of the same object taken from different locations, different distances and different angles are not distinguished. To rid the \(k\)-ad of the effects of location and distance, or scale, one considers \(\mathbf{u}=(\mathbf{z}-\langle\mathbf{z}\rangle)/|(\mathbf{z}-\langle\mathbf{z }\rangle)|\), where \(\langle\mathbf{z}\rangle=(\frac{1}{k}\sum_{j=1}^{k}z_{j})(1,1,\ldots,1)\) and the norm \(|(c_{1},c_{2},\ldots,c_{k})|\) of a \(k\)-tuple of complex numbers \(c_{j}=a_{j}+ib_{j}\) (\(j=1,\ldots,k\)) is given by \((\sum_{j=1}^{1}|c_{j}|^{2})^{1/2}\) with \(|c_{j}|^{2}=a_{j}^{2}+b_{j}^{2}\). Note that this would be the same as the Euclidean norm of the \(k\)-tuple of points \(((a_{1},b_{1}),\ldots,(a_{k},b_{k}))^{\prime}\) in \(\mathbb{R}^{2}\). The normed \(k\)-tuple \(\mathbf{u}\) lies in the complex \((k-1)\)-dimensional hyperplane \(L\) in \(\mathbb{C}^{k}\) defined by \(\sum_{j=1}^{k}u_{j}=0\), which is also the real \(2(k-1)\)-dimensional subspace of \((\mathbb{R}^{2})^{k}\) defined by \(\sum_{1}^{k}v_{j}=0\), \(\sum_{1}^{k}w_{j}=0\) where \(u_{j}=v_{j}+iw_{j}\). Due to norming, \(|\mathbf{u}|=1\) so that \(\mathbf{u}\) lies on the unit sphere in \(L\approx\mathbb{C}^{k-1}\) (or, \(\approx\mathbb{R}^{2(k-1)}\)), called the _preshape sphere_ denoted \(\mathbb{C}S^{k-1}\approx S^{2k-3}\). In order to rid the \(k\)-ad of the angle of observation as well, one considers the _shape_\(\sigma({\bf z})\) of a \(k\)-ad \({\bf z}\) to be its preshape \({\bf u}\) modulo rotations in the plane. This is conveniently expressed as the _orbit of \({\bf u}\) under rotations,_ i.e.,

\[\sigma({\bf z})=\left\{e^{i\theta}u:-\pi<\theta\leq\pi\right\}. \tag{12.13}\]

Since the preshape sphere has (real) dimension \(2k-3\), and \(\{e^{i\theta}:-\pi<\theta\leq\pi\}\) is one-dimensional, _Kendall's planar shape space_ denoted \(\Sigma_{2}^{k}\) is a manifold which has dimension \(2k-4\) (\(k>2\)) (See, e.g., Bhattacharya and Bhattacharya 2012, Lemma A3, p. 214). It is preferable to represent the preshape \({\bf u}\) as a point \((u_{1},u_{2},\ldots,u_{k-1})\ \in{\mathbb{C}}^{k-1}\) (or \({\mathbb{R}}^{2k-2}\)). This may be achieved by premultiplying the \(k\)-ad by a \(k\times(k-1)\)_Helmert matrix_\(H\) comprising \(k-1\) column vectors forming an orthonormal basis of the subspace \(1^{\perp}\) of \({\mathbb{R}}^{k}\) orthogonal to \((1,1,\ldots,1)\). For example, one may take the \(j\)-th column of \(H\) to be \((a(j),\ldots,a(j))\), \(-ja(j),0,\ldots,0)\) where \(a(j)=[j(j+1)]^{-1/2}\) (\(j=1,\ldots,k-1\)). That is, \({\bf x}=(x_{1},\ldots,x_{k})\) is changed to \({\bf x}H=(x_{1}^{0},\ldots,x_{k-1}^{0})\), say, and, similarly, \({\bf y}=(y_{1},\ldots,y_{k})\) is changed to \({\bf y}H=(y_{1}^{0},\ldots,y_{k-1}^{0})\). This translated \(k\)-ad \(((x_{1}^{0},y_{1}^{0}),\ldots,(x_{k-1}^{0},y_{k-1}^{0}))\) then has the complex representation \({\bf z}^{0}=(z_{1}^{0},\ldots,z_{k-1}^{0})\) with \(z_{j}^{0}=x_{j}^{0}+iy_{j}^{0}\), and the preshape is then \({\bf u}^{0}={\bf z}^{0}/|{\bf z}^{0}|\). Note that we only consider \(k\)-ads whose \(k\) points are not all the same, so that \({\bf z}^{0}\neq{\bf 0}\). To avoid excessive notation we will drop the superscript \(0\) from \({\bf z}^{0}\) and \({\bf u}^{0}\) and write \((k-1)\)-tuples as \({\bf z}\) and \({\bf u}\), respectively.

A good embedding of the shape space \(\Sigma_{2}^{k}\) is the so-called _Veronese-Whitney embedding_\(J\) into the (real) vector space \(S(k-1,{\mathbb{C}})\) of all \((k-1)\times(k-1)\) Hermitian matrices \(B=B^{*}\) (\(B^{*}\) is the transpose of the matrix \(\overline{B}\) of complex conjugates of elements of \(B\)), given by

\[J\sigma({\bf z})={\bf u}^{*}{\bf u}. \tag{12.14}\]

Note that \((e^{i\theta}{\bf u})^{*}(e^{i\theta}{\bf u})={\bf u}^{*}{\bf u}\ \forall\, \theta\in(-\pi,\pi]\), so that \(J\) is a one-to-one map of \(\Sigma_{2}^{k}\) into \(S(k-1,{\mathbb{C}})\). The space \(S(k-1,{\mathbb{C}})\) is a vector space with respect to real scalars and with the inner product given by \(\langle B,C\rangle=Re(\mbox{Trace}\ \ BC^{*})\). If \(B=((\alpha_{jj^{\prime}}+i\beta_{jj^{\prime}}))\) and \(C=((\delta_{jj^{\prime}}+i\gamma_{jj^{\prime}}))\), then \(\langle B,C\rangle=\sum_{j,j^{\prime}}(\alpha_{jj^{\prime}}\delta_{jj^{\prime }}+\beta_{jj^{\prime}}\gamma_{jj^{\prime}})\). One may think of an element \(B\) of \(S(k-1,{\mathbb{C}})\) to be represented by a real \(2(k-1)\times(k-1)\) matrix with the first \(k-1\) rows \(\{\alpha_{jj^{\prime}}:1\leq j^{\prime}\leq k-1,\,1\leq j\leq k-1\}\) comprising a symmetric \((k-1)\times(k-1)\) matrix, and the last \(k-1\) rows \(\{\beta_{jj^{\prime}}:1\leq j^{\prime}\leq k-1,\)\(1\leq j\leq k-1\}\) comprising a \((k-1)\times(k-1)\) skew symmetric matrix. The inner product \(\langle B,C\rangle\) is then simply the Euclidean inner product on this space of \(2(k-1)\times(k-1)\) real matrices considered as a \(2(k-1)\times(k-1)\)-dimensional vector. Note that the dimension of \(S(k-1,{\mathbb{C}})\) (or of its representation as real \(2(k-1)\times(k-1)\) matrices as described) is \(k(k-1)\). Hence \(S(k-1,{\mathbb{C}})\approx E^{N}\) with \(N=k(k-1)\).

We next turn to extrinsic analysis on \(\Sigma_{2}^{k}\), using the embedding (12.14). Let \(\mu^{J}\) be the mean of \(Q\circ J^{-1}\) on \(S(k-1,{\mathbb{C}})\). Denote by \(SU(k-1)\) the special unitary group of \((k-1)\times(k-1)\) complex matrices \(T\) such that \(TT^{*}=I_{k-1}\) and \(det(T)=1\). To compute the projection \(P(\mu^{J})\), let \(T\) be a unitary matrix, \(T\in SU(k-1)\) such that \(T\mu^{J}T^{*}=D=diag(\lambda_{1},\ldots,\lambda_{k-1})\), \(\lambda_{1}\leq\cdots\leq\lambda_{k-2}\leq\lambda_{k-1}\). For \(u\in{\mathbb{C}}S^{k-1}\), \(u^{*}u\in J(\Sigma_{2}^{k})\), write \(v=Tu^{*}\). Then \(Tu^{*}uT^{*}=vv^{*}\), and \[\|u^{*}u-\mu_{J}\|^{2} =\|vv^{*}-D\|^{2}=\sum_{i,j}|v_{i}v_{j}-\lambda_{j}\delta_{ij}|^{2} \tag{12.15}\] \[=\sum_{j}(|v_{j}|^{2}+\lambda_{j}^{2}-2\lambda_{|}v_{j}|^{2})\] \[=\sum_{j}\lambda_{j}^{2}+1-2\sum_{j}\lambda_{j}|v_{j}|^{2},\]

which is minimized on \(J(\Sigma_{2}^{k})\) by \(v=(v_{1},\ldots,v_{k-1})\) for which \(v_{j}=0\) for \(j=1,\ldots,k-2\), and \(|v_{k-1}|=1\). That is, the minimizing \(u^{*}\) in (12.15) is a unit eigenvector of \(\mu^{J}\) with the largest eigenvalue \(\lambda_{k-1}\), and \(P(\mu^{J})=u^{*}u\). _This projection is unique if and only if the largest eigenvalue of \(\mu^{J}\) is simple,_ i.e., \(\lambda_{k-2}<\lambda_{k-1}\).

Assuming that the largest eigenvalue of \(\mu^{J}\) is simple, one may now obtain the asymptotic distribution of the sample extrinsic mean \(\mu_{n,E}\), namely, that of \(J(\mu_{n,E})=v_{n}^{*}v_{n}\), where \(v_{n}\) is a unit eigenvector of \(\widetilde{\overline{X}}=\sum\widetilde{X}_{j}/n\) corresponding to its largest eigenvalue. Here \(\widetilde{X}_{j}=J(X_{j})\), for i.i.d. observations \(X_{1},\ldots,X_{n}\) on \(\Sigma_{2}^{k}\). For this purpose, a convenient orthonormal basis (frame) of \(T_{p}S(k-1,\mathbb{C})\approx S(k-1,\mathbb{C})\) is the following

\[v_{a,b} =2^{-\frac{1}{2}}(e_{a}e_{b}^{\prime}+e_{b}e_{a}^{\prime})\ \ \mbox{for}\ a<b,\,v_{a,a}=e_{a}e_{a}^{\prime}; \tag{12.16}\] \[w_{a,b} =i2^{-\frac{1}{2}}(e_{a}e_{b}^{\prime}-e_{b}e_{a}^{\prime})\ \ \mbox{for}\ b<a\ (a,b=1,\ldots,k-1),\]

where \(e_{a}\) is the column vector with all entries zero other than the \(a\)-th, and the \(a\)-th entry is \(1\). Let \(U_{1},\ldots,U_{k-1}\) be orthonormal unit eigenvectors corresponding to the eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{k-2}<\lambda_{k-1}\). Then choosing \(T=(U_{1},\ldots,U_{k-1})\in SU(k-1)\), \(T\mu^{J}T^{*}=D=\mbox{\it diag}(\lambda_{1},\ldots,\lambda_{k-1})\), the columns of \(Tv_{a,b}T^{*}\) and \(Tw_{a,b}T^{*}\) together constitute an orthonormal basis of \(S(k-1,\mathbb{C})\). It is not difficult to check that the differential of the projection operator \(P\) satisfies

\[(d_{\mu^{J}}P)Tv_{a,b}T^{*}=\left\{\begin{array}{ll}0&\mbox{if}\,1\leq a\leq b<k-1,\\ &\mbox{or}\ \ a=b=k-1,\\ (\lambda_{k-1}-\lambda_{a})^{-1}Tv_{a,k-1}T^{*}&\mbox{if}\,1\leq a<k-1,\,b=k-1; \end{array}\right. \tag{12.17}\]

\[(d_{\mu^{J}}P)Tw_{a,b}T^{*}=\left\{\begin{array}{ll}0&\mbox{if}\,1\leq a\leq b<k-1,\\ (\lambda_{k-1}-\lambda_{a})^{-1}Tw_{a,k-1}T^{*}&\mbox{if}\,1\leq a<k-1.\end{array}\right.\]

To check these, take the projection of a linear curve \(c(s)\) in \(S(k-1,\mathbb{C})\) such that \(\dot{c}(0)\) is one of the basis elements \(v_{a,b}\), or \(w_{a,b}\), and differentiate the projected curve with respect to \(s\). It follows that \(\{Tw_{a,k-1}T^{*},Tw_{a,k-1}T^{*}:\ a=1,\ldots,k-2\}\) form an orthonormal basis of \(T_{P(\mu^{J})}J(\Sigma_{2}^{k})\). Expressing \(\widetilde{\overline{X}}-\mu^{J}\) in the orthonormal basis of \(S(k-1,\mathbb{C})\), and \(d_{\mu^{J}}P(\widetilde{\overline{X}}-\mu^{J})\) with respect to the above basis of \(T_{P(\mu^{J})}J(\Sigma_{2}^{k})\), one may now apply Theorem 12.2 and Corollary 12.1.

For a two-sample test for \(H_{0}:Q_{1}=Q_{2}\), one may use (12.11).

### 12.3 Data Examples

In this section we apply the theory to a number of data sets available in the literature.

Example 12.4 (Paleomagnetism): The first statistical confirmation of the shifting of the earth's magnetic poles over geological times, theorized by paleontologists based on observed fossilised magnetic rock samples, came in a seminal paper by R.A. Fisher (1953). Fisher analyzed two sets of data--one recent (1947-1948) and another old (Quaternary period), using the so-called _von Mises-Fisher model_

\[f(x;\mu,\tau)=c(\tau)\exp\{\tau x.\mu\}\;(x\in S^{2}), \tag{12.18}\]

Here \(\mu(\in S^{2})\), is the extrinsic _mean direction,_ under the inclusion map \(J\) (Exercise 12.1) (\(\mu=\mu_{E}\)), and \(\tau>0\) is the concentration parameter. The maximum likelihood estimate of \(\mu\) is \(\hat{\mu}=\overline{X}/|\overline{X}|\), which is also our sample extrinsic mean (Exercise 12.1). The value of the MLE for the first data set of \(n=9\) observations turned out to be \(\hat{\mu}-\hat{\mu}_{E}=(0.2984,\,0.1346,\,0.9449)\), where \((0,0,1)\) is the geographic north pole. Fisher's 95 % confidence region for \(\mu\) is \(\{\mu\in S^{2}:d_{g}(\hat{\mu},\mu)\leq 0.1536\}\), where \(d_{g}\) is the geodesic distance on \(S^{2}\). The nonparametric confidence region based on \(\hat{\mu}_{E}\), is given by (12.9) and is about 10 % smaller in area than Fisher's region (See Bhattacharya and Bhattacharya 2012, Chap. 2).

The second data set based on \(n=29\) observations from the Quaternary period that Fisher analyzed, using the same parametric model as above, had the MLE \(\hat{\mu}_{E}=\overline{X}/|\overline{X}|=(0.0172,\,-0.2978,\,-0.9545)\), almost antipodal of that for the first data set, and with a confidence region of geodesic radius 0.1475 around the MLE. Note that the two confidence regions are not only disjoint, they also lie far away from each other. This provided the first statistical confirmation of the hypothesis of shifts in the earth's magnetic poles, a result hailed by paleontologists (See Irving 1964). Because of the difficulty in accessing the second data set, the nonparametric procedures could not be applied to it. But the analysis of another data set dating from the Jurassic period, with \(n=33\), once again yielded a nonparametric extrinsic confidence region, and about 10 % smaller than the region obtained by Fisher's parametric method (See Bhattacharya and Bhattacharya, Chap. 5, for details).

Example 12.5 (Brain Scan of Schizophrenic and Normal Patients): We consider an example from Bookstein (1991) in which 13 landmarks were recorded on a mid-sagittal two-dimensional slice from magnetic brain scans of each of 14 schizophrenic patients and 14 normal patients. The object is to detect the deformation, if any, in the shape of the \(k\)-ad due to the disease, and to use it for diagnostic purposes. The shape space is \(\Sigma_{2}^{13}\). The extrinsic test based on (12.11) has an observed value 95.5476 of the chi-square statistic and a \(p\)-value \(3.8\times 10^{-11}\). The calculations made use of the analytical computations carried out in Example 12.3. For details of these calculations and others we refer to Bhattacharya and Bhattacharya (2012). This may also be contrasted with the results of parametric inference in the literature for the same data, as may be found in (Dryden and Mardia, 1998, pp. 146, 162-165). Using a isotropic Normal model for the original landmarks data, and after removal of "nuisance" parameters for translation, size and rotation, an \(F\)-test known as Goodall's \(F\)-test (See Goodall 1991) gives a \(p\)-value 0.01. A Monte Carlo test based permutation test obtained by 999 random assignments of the data into two groups and computing Goodall's \(F\)-statistic, gave a \(p\)-value 0.04. A Hotellings's \(T^{2}\) testin the tangent space of the pooled sample mean had a \(p\)-value 0.834. A likelihood ratio test based on the isotropic offset Normal distribution on the shape space has the value 43.124 of the chi-square statistic with 22 degrees of freedom, and a \(p\)-value 0.005.

Example 12.6 (Corpus Callosum Shapes of Normal and ADHD Children): We consider a planar shape data set, which gives measurements on a group of typically developing children and a group of children suffering the ADHD (Attention deficit hyperactivity disorder). ADHD is one of the most common psychiatric disorders for children that can continue through adolescence and adulthood. Symptoms include difficulty staying focused and paying attention, difficulty controlling behavior, and hyperactivity (over-activity). ADHD in general has three subtypes: (1) ADHD hyperactive-impulsive, (2) ADHD-inattentive; (3) Combined hyperactive-impulsive and inattentive (ADHD-combined) (Ramsay 2007). ADHD-200 Dataset ([http://fcon_1000.projects.nitrc.org/indi/adhd200/](http://fcon_1000.projects.nitrc.org/indi/adhd200/)) is a data set that records both anatomical and resting-state functional MRI data of 776 labeled subjects across 8 independent imaging sites, 491 of which were obtained from typically developing individuals and 285 in children and adolescents with ADHD (ages: 7-21 years old). The data was further processed by UNC BIAS lab (see Huang et al. 2015) to extract the planar Corpus Callosum shape data, which contains 50 landmarks on the contour of the Corpus Callosum of each subject (see Fig. 12.1 for a plot of the raw landmarks of a normal developing child and a ADHD child). After quality control, 647 CC shape data out of 776 subjects were obtained, which included 404 (\(n_{1}\)) typically developing children, 150 (\(n_{2}\)) diagnosed with ADHD-Combined, 8 (\(n_{3}\)) diagnosed with ADHD-Hyperactive-Impulsive, and 85 (\(n_{4}\)) diagnosed with ADHD-Inattentive. Therefore, the data lie in the space

Figure 12.1: Raw landmarks from the contour of the Corpus Callosum of a typically developing child and an ADHD child

\(\Sigma_{2}^{50}\), which has a high dimension of \(2\times 50-4=96\). We carry out _extrinsic two sample testings_ based on (12.11) between the group of typically developing children and the group of children diagnosed with ADHD-Combined, and also between the group of typically developing children and ADHD-Inattentive children. We construct testing statistics that base on the asymptotic distribution of the extrinsic mean for the planar shapes.

The \(p\)-value of the two-sample test between the group of typically developing children and the group of children diagnosed with ADHD-Combined is \(5.1988\times 10^{-11}\), which is based on the asymptotic chi-squared distribution given in (12.11). The \(p\)-value of the test between the group of typically developing children and the group ADHD-Inattentive children is smaller than \(10^{-50}\).

More details of the above two examples can be found in [https://stat.duke.edu/~ll162/research/](https://stat.duke.edu/~ll162/research/).

### Notes and References

In addition to the early seminal work of R.A. Fisher (1953) and books by Watson (1983) and N. Fisher et al. (1987) on directional statistics on spheres \(S^{d}\) and axial spaces \(\mathbb{R}^{p^{d}}\) mentioned in this chapter, Dryden and Mardia (1998) give a comprehensive account of parametric inference on shape manifolds of D.G. Kendall (See Kendall et al. 1999 for a detailed account of these manifolds). Nonparametric inference based on Frechet means on general manifolds was introduced in the Indiana University Ph.D. dissertation of Vic Patrangenaru (1998), and further developed in Bhattacharya and Patrangenaru (2003, 2005). This theory is much more general than, and was done independently of, the work of Hendriks and Landsman (1996, 1998) on Euclidean submanifolds such as spheres and hypersurfaces of an Euclidean space with the inclusion map as the embedding. The Frechet mean on a non-Euclidean space depends on the distance chosen. The role of a proper choice of the distance in analyzing complex data was recently emphasized by Holmes (2015).

The data analysis on \(S^{2}\) in Sect. 12.3 follows Bhattacharya and Bhattacharya (2012), and that on the planar shape space may be found in Bhattacharya and Lin (2016). Earlier, nonparametric tests for uniformity on compact Riemannian manifolds were developed by Beran (1968) and Gine (1975). For certain functional data analysis on manifolds, especially projective shape spaces, one may refer to Musse et al. (2008).

A very recent source on the subject matter of this chapter is the book by Ellingson and Patrangenaru (2015).

### Exercises for Chap. 12

**Ex. 12.1**.:
1. (a) Prove that the extrinsic mean of the von Mises-Fisher distribution (12.18) is \(\mu\).
2. Prove that the MLE of \(\mu\) in (12.18) is the sample extrinsic mean \(\overline{X}/|\overline{X}|\).
3. Using the first data set of 9 observations in Fisher (1953) construct the asymptotic 95 % confidence region for \(\mu=\mu_{E}\) using (12.9).

## References

* Beran (1968) Beran, R. J. (1968). Testing for uniformity on a compact homogeneous space. _Journal of Applied Probability,__5_, 177-195.
* Bhattacharya & Bhattacharya (2012) Bhattacharya, A., & Bhattacharya, R. (2012). _Nonparametric inference on manifolds: With applications to shape spaces_. IMS Monograph (Vol. #2). Cambridge: Cambridge University Press.
* Bhattacharya & Lin (2016) Bhattacharya, R., & Lin, L. (2016). Omnibus CLTs for Frechet means and nonparametric inference on non-Euclidean spaces. The Proceedings of the American Mathematical Society (in Press).
* Bhattacharya & Patrangenaru (2003) Bhattacharya, R., & Patrangenaru, V. (2003). Large sample theory of intrinsic and extrinsic sample means on manifolds. _Annals of Statistics,__31_, 1-29.
* Bhattacharya & Patrangenaru (2005) Bhattacharya, R., & Patrangenaru, V. (2005). Large sample theory of intrinsic and extrinsic sample means on manifolds-II. _Annals of Statistics,__33_, 1225-1259.
* Bookstein (1978) Bookstein, F. (1978). _The measurement of biological shape and shape change_. Lecture Notes in Biomathematics. New York: Springer.
* Bookstein (1991) Bookstein, F. (1991). _Morphometric tools for landmark data: Geometry and biology_. Cambridge: Cambridge University Press.
* Dryden & Mardia (1998) Dryden, I., & Mardia, K. V. (1998). _Statistical analysis of shape_. New York: Wiley.
* Ellingson & Patrangenaru (2015) Ellingson, L., & Patrangenaru, V. (2015). _Nonparametric statistics on manifolds and their applications to object data analysis_. New York: Chapman and Hall.
* Fisher (1953) Fisher, R. A. (1953). Dispersion on a sphere. _Proceedings of the Royal Society of London, Series A,__217_, 295-305.
* Fisher et al. (1987) Fisher, N. I., Lewis, T., & Embleton, B. J. J. (1987). _Statistical analysis of spherical data_. Cambridge: Cambridge University Press.
* Frechet (1948) Frechet, M. (1948). Les elements aleatoires de nature quelconque dans un espace distancie. _Annales de l'Institute Henri Poincare,__10_, 215-310.
* Gine (1975) Gine, E. (1975). Invariant tests for uniformity on compact Riemannian manifolds based on Sobolev norms. _Annals of Statistics,__3_, 1243-1266.
* Goodall (1991) Goodall, C. (1991). Procrustes methods in the statistical analysis of shape. _Journal of the Royal Statistical Society, Series B,__53_, 285-339.
* Hendriks & Landsman (1996) Hendriks, H., & Landsman, Z. (1996). Asymptotic tests for mean location on manifolds. _Comptes Rendus de l'Academie des Sciences. Serie I, Mathematique,__322_, 773-778.
* Hendriks & Landsman (1998) Hendriks, H., & Landsman, Z. (1998). Mean location and sample mean location on manifolds: Asymptotics, tests, confidence regions. _Journal of Multivariate Analysis,__67_, 227-243.
* Holmes (2015) Holmes, S. (2015). Statistically relevant metrics for complex data. _Joint Mathematics Meetings,__2015_, San Antonio.
* Huang et al. (2015) Huang, C., Styner, M., & Zhu, H. T. (2015). Penalized mixtures of offset-normal shape factor analyzers with application in clustering high-dimensional shape data. _Journal of the American Statistical Association_ (to appear).
* Irving (1964) Irving, E. (1964). _Paleomagnetism and its application to geological and geographical problems_. New York: Wiley.
* Kendall (1977) Kendall, D. G. (1977). The diffusion of shape. _Advances in Applied Probability,__9_, 428-430.
* Kendall (1984) Kendall, D. G. (1984). Shape Manifolds, Procrustean metrics, and complex projective spaces. _Bulletin of the London Mathematical Society,__16_, 81-121.

* Kendall et al. (1999) Kendall, D. G., Barden, D., Carne, T. K., & Le, H. (1999). _Shape and shape theory._ New York: Wiley.
* Mardia & Jupp (2000) Mardia, K. V., & Jupp, P. E. (2000). _Directional statistics_. New York: Wiley.
* Munk et al. (2008) Munk, A., Paige, R., Pang, J., Patrangenaru, V., & Ruymgaart, F. H. (2008). The one and multisample problem for functional data with applications to projective shape analysis. _Journal of Multivariate Analysis, 99_, 815-833.
* Patrangenaru (1998) Patrangenaru, V. (1998). Asymptotic statistics on manifolds and their applications (Ph.D. Thesis). Indiana University, Bloomington.
* Ramsay (2007) Ramsay, J. R. (2007). Current status of cognitive-behavioral therapy as a psychosocial treatment for adult attention-deficit/hyperactivity disorder. _Current Psychiatry Reports, 9_(5), 427-433.
* Watson (1983) Watson, G. S. (1983). _Statistics on spheres_. University Arkansas Lecture Notes in the Mathematical Sciences. New York: Wiley.
* Ziezold (1977) Ziezold, H. (1977). On expected figures and a strong law of large numbers for random elements in quasi-metric spaces. In _Transactions of the Seventh Prague Conference on Information Theory, Statistical Functions, Random Processes and of the Eighth European Meeting of Statisticians_ (Vol. A, pp. 591-602).

## Chapter 13 Multiple Testing and the False Discovery Rate

**Abstract** Here is an introduction to the theory of the false discovery rates (FDR) developed by Benjamini and Hochberg (Journal of the Royal Statistical Society, Series B, 57, 289-300, 1995), Benjamini and Yekatieli (Annals of Statistics, 29(4), 1165-1188, 2001) and others, dealing with the problem of testing a large number of hypotheses often based on relatively small or moderate sample sizes.

### 13.1 Introduction

Statisticians are often confronted with the problem of testing simultaneously \(m\) null hypotheses \(H_{01}\), \(H_{02},\ldots,\)\(H_{0m}\), \(m>1\), based on some data. If the goal is to determine if all these hypotheses are right, one may take the _global null hypothesis_ to be \(H_{0}=\cap_{1\leq i\leq m}H_{0i}\). This may be the case in a two-sample problem in which the only objective is to see if the two underlying distributions are the same. For a test with a level of significance \(\alpha\), the classical Bonferroni procedure is to reject \(H_{0}\) if and only if at least one of the \(m\)\(p\)-values \(p_{1},\ldots,p_{m}\) is smaller than \(\alpha/m\). This, of course, is a very conservative test in protecting \(H_{0}\), i.e., the actual level of significance of the test is probably far smaller than \(\alpha\). For a test of size \(\alpha\), the Bonferroni test has then a small power. A much improved procedure was suggested by Simes (1986), in which the \(p\)-values are ordered as \(p_{(1)}\leq p_{(2)}\leq\cdots\leq p_{(m)}\) and \(H_{0}\) is rejected if and only if \(p_{(i)}\leq(i/m)\alpha\) for at least one \(i\). He proved that this test is conservative (i.e., it has size \(\leq\alpha\)) if the test statistics are i.i.d. and their common distribution (function) is continuous, and he conjectured that this is true more generally. Sarkar (1998) proved the conjecture for a class of positively dependent joint, or multivariate, distributions of the test statistics, with common marginals. Also see Sen (1999a,b) for additional facts and some history on multiple testing. From results due to Benjamini and Yekutieli (2001) derived in Sect. 13.2, Simes' conjectured inequality follows for test statistics \({\bf T}=(T_{1},T_{2},\ldots,T_{m})\), whose joint distribution has the property of _positive regression dependency on each element of a subset_ of these tests, or _PRDS_, defined as follows. A set \(D\) of \(m\)-tuples \({\bf t}=(t_{1},t_{2},\ldots,t_{m})\) of values of the test statistics is said to be _increasing,_ provided \({\bf t}\in D\) and \({\bf t}\leq{\bf s}\) (i.e., \(t_{i}\leq s_{i}\) for all \(i\)) implies \({\bf s}\in D\).

_PRDS holds for a given subset \(I_{0}\) of \(\{1,\ldots,m\}\)_

_if for every measurable increasing set \(D\),_

\[Prob({\bf T}\in D\mid T_{i}=t_{i})\mbox{ is increasing in }t_{i}\mbox{ for every }i\in I_{0}. \tag{13.1}\]

This includes the case of independent test statistics \(T_{i}\), \(i=1,\ldots,m\), as well as many other positively dependent ones.

Of greater interest in this chapter is the determination, with a limited statistical error rate, of those among the \(m\) null hypotheses which are false. For example, in microarray experiments a single observation records gene expression levels of thousands of genes for the purpose of locating those genes which may contribute to a certain disease. The experiments are generally expensive and the number of observations made is not large. Here \(m\) is of the order of several thousands, and the Bonferroni procedure is obviously inadequate for this purpose only pointing to those genes for which the \(p\)-values are less than \(\alpha/m\). In contrast, the procedure for independent test statistics due to Benjamini and Hochberg (1995) and, more general procedures due to Benjamini and Yekutieli (2001) discussed in the next section are more effective. For an example, one may look at the article of Reiner et al. (2003), Identifying differentially expressed genes using false discovery rate controlling procedures, Bioinformatics 19:368-375. The _Benjamini-Hochberg procedure_ looks somewhat analogous to that of Simes, but it rejects all \(k\) null hypotheses with the smallest \(p\)-values, where \(k=\max\{i:p_{(i)}\leq(i/m)\alpha\}\) (See Theorem 13.1 in the next section), whereas the Benjamini-Yekutieli procedure, valid without any restriction on the nature of the joint distribution of the test statistics, rejects the \(k\) null hypotheses with the lowest \(p\)-values with \(k=\max\{i:p_{(i)}\leq(i/c_{m}m)\alpha\}\), where \(c_{m}=\sum_{1\leq j\leq m}(1/j)\) is, for large \(m\), approximately \(\log m\) (Theorem 13.2).

In Sect. 13.3, the theory is applied to a set of real data for \(m=75\) two-sample tests with 28 HIV+ patients and 18 controls.

### False Discovery Rate

As mentioned above, in many problems involving multiple tests it is important to identify, within a small statistical error, those hypotheses \(H_{0i}\) which are false among the \(m\) null hypotheses that are presented (\(i=1,2,\ldots,m\)). As a measure of the effectiveness of such multiple testing the following notion was introduced by Benjamini-Hochberg. Denote by \(m_{0}\) the number of true hypotheses among the \(m\) null hypotheses \(H_{0i}\) (\(1\leq i\leq m\)). Let the true null hypotheses be labeled as \(H_{0i}\) (\(1\leq i\leq m_{0}\)), unless this set is empty.

**Definition 13.1**.: Suppose that of the \(m\) null hypotheses tested, \(V\) true null hypotheses and \(S\) false null hypotheses are rejected, with a proportion \(Q=V/(V+S)\) of true hypotheses among all hypotheses rejected. The quantity \(E(Q)\) is called _the false discovery rate._

_Remark 13.1_.: If \(m_{0}=m\), then \(Q=1\), provided \(V>0\). If \(V=0\), then \(Q=0\). Hence in the case all null hypotheses are true, the false discovery rate is the same as the type 1 error, i.e., it is the probability of rejection under the global null hypothesis \(H_{0}=\cap_{1\leq i\leq m}H_{0i}\).

Recall the _Benjamini-Hochberg procedure:_

_Reject only the \(k\) null hypotheses with the smallest \(p\)-values,_

_where \[k=\max\{i:p_{(i)}\leq\left(\frac{i}{m}\right)\alpha\}.\]_

The proofs below are along the lines of those in Benjamini and Yekutieli (2001).

**Theorem 13.1**.: _(a) (Benjamini-Hochberg) Assume that the \(m\) test statistics \(T_{i}\), \(i=1,\ldots,m\), are independent. Then the false discovery rate \(E(Q)\) for the procedure (13.2) is no more than \(\alpha m_{0}/m\). (b) (Benjamini-Yekutieli) if the PDRS (13.1) holds with \(I_{0}\) indexing the set of all true null hypotheses, then also the procedure (13.2) has a false discovery rate no more than \(\alpha m_{0}/m\)._

Proof.: We only give the proof of (a). For part (b), See Benjamini and Yekutieli (2001).

_Step 1._ Consider the distribution of the \(p\)-values, \(P_{1},P_{2},\ldots,P_{m}\), on the probability space \([0,1]^{m}\). Let \(A(v,s)\) denote the event that \(v\) of the true null hypotheses and \(s\) false null hypotheses are rejected by the procedure (13.2), and \(A(v,s;J)\) the subset of \(A(v,s)\) with a specific set of \(v\) true null hypotheses indexed by \(J\subset\{1,\ldots,m_{0}\}\). Writing \(q_{i}=(i/m)\alpha\), note that on \(A(v,s)\) the \(p\)-values of only the \(v+s\) hypotheses rejected are less than, or equal to, \(q_{v+s}\). Then on the set \(A(v,s;J)\), and for \(1\leq i\leq m_{0}\), \(\{P_{i}\leq q_{v+s}\}\) holds if and only if \(i\in J\). That is, \(\{P_{i}\leq q_{v+s}\}\cap A(v,s;J)=A(v,s;J)\) or \(\emptyset\) according as \(i\in J\) or \(i\notin J\) (\(1\leq i\leq m_{0}\)). Hence

\[\sum_{1\leq i\leq m_{0}}\mathrm{Prob}(\{P_{i}\leq q_{v+s}\}\cap A( v,s)) \tag{13.3}\] \[= \sum_{1\leq i\leq m_{0}}\sum_{J}\mathrm{Prob}(\{P_{i}\leq q_{v+s} \}\cap A(v,s;J))\] \[= \sum_{J}\sum_{i\in J}\mathrm{Prob}(A(v,s;J))=\sum_{J}v\mathrm{ Prob}(A(v,s;J))=v\mathrm{Prob}(A(v,s)).\]

Writing \(\tilde{m}=m-m_{0}\), it follows from (13.3) that

\[E(Q) = \sum_{0\leq s\leq\bar{n}}\sum_{1\leq v\leq m_{0}}\frac{v}{v+s} \mathrm{Prob}(A(v,s)) \tag{13.4}\] \[= \sum_{0\leq s\leq\bar{n}}\sum_{1\leq v\leq m_{0}}\sum_{1\leq i \leq m_{0}}\mathrm{Prob}(\{P_{i}\leq q_{v+s}\}\cap A(v,s))/(v+s)\] \[= \sum_{1\leq i\leq m_{0}}\left[\sum_{0\leq s\leq\bar{n}}\sum_{1 \leq v\leq m_{0}}\sum(v+s)^{-1}\mathrm{Prob}(\{P_{i}\leq q_{v+s}\}\cap A(v,s)) \right].\]

_Step 2._ The event \(\{P_{i}\leq q_{v+s}\}\cap A(v,s)\) may be expressed as \(\{P_{i}\leq q_{v+s}\}\cap C^{i}(v,s)\), where \(C^{i}(v,s)\) is the event that \(s\) false null hypotheses and \(v-1\) true null hypotheses \(H_{0j}\) with \(j\in\{1,\ldots,m_{0}\}\backslash\{i\}\) are rejected. For each \(k\), the \(m_{0}\) sets \(C(i|k)=\cup\{C^{i}(v,s)\): all \(v,s\) such that \(v+s=k\}\) are disjoint. Now (13.4) may be expressed as

\[E(Q)=\sum_{1\leq i\leq m_{0}}\left[\sum_{1\leq k\leq m}k^{-1}\mathrm{Prob}(\{P _{i}\leq q_{k}\}\cap C(i|k))\right]. \tag{13.5}\]_Step 3._ By the hypothesis of independence of \(T_{i}\) (\(i=1,\ldots,m\)), the probability within square brackets on the right side of (13.5) equals the product \(\mbox{Prob}(P_{i}\leq q_{k})\mbox{Prob}(C(i|k))\). For \(i=1,\ldots,m_{0}\), \(\mbox{Prob}(P_{i}\leq q_{k})\leq q_{k}=(k/m)\alpha\) (Exercise 13.1)). Therefore, the right side of (13.5) is no more than \((\alpha/m)\) times \(\sum_{1\leq i\leq m_{0}}\,\,[\sum_{1\leq k\leq m}\,\,\mbox{ Prob}(C(i|k))]\). By disjointness of the \(m\) events \(C(i|k)\), for each \(i\), the inner sum (of the double sum) equals the probability of the union of the sets over \(k\) and is, therefore, no more than one. Hence the double sum is no more than \(m_{0}\), and we arrive at the desired result. 

Observe that the proof above does not make use of the hypothesis of independence of the \(T_{i}\)'s for Steps 1 and 2. In particular, (13.5) holds generally, without any dependency restriction on the distribution of \({\bf T}=(T_{1},T_{2},\ldots,T_{m})\). To bound the right side properly, split the event \(\{P_{i}\leq q_{k}\}\) as the union \(\cup\{P_{i}\in(\alpha(j-1)/m,\alpha j/m]:j=1,\ldots,k\}\), and note that, if the \(T_{i}\) have continuous distribution functions, then

\[\sum_{1\leq k\leq m}\mbox{Prob}(\{P_{i}\in(\alpha(j-1)/m,\alpha j /m]\}\cap C(i,k))\] \[= \mbox{Prob}(\{P_{i}\in(\alpha(j-1)/m,\alpha j/m]\}\cap\{\cup C(i, k):k=1,\ldots,m\})\] \[\leq \mbox{Prob}(\{P_{i}\in(\alpha(j-1)/m,\alpha j/m]\})=\alpha/m. \tag{13.6}\]

The equality in (13.6) is a consequence of the disjointness of the sets \(C(i,k)\), \(k=1,\ldots,m\). The inequality follows from the fact that for each \(i=1,\ldots,m_{0}\), \(\mbox{Prob}(\{P_{i}\in(\alpha(j-1)/m,\alpha j/m]\})=\alpha/m\). The relations (13.5), (13.6) then lead to

\[E(Q) = \sum_{1\leq i\leq m_{0}}\left[\sum_{1\leq k\leq m}k^{-1}\mbox{ Prob}(\{P_{i}\leq q_{k}\}\cap C(i,k))\right] \tag{13.7}\] \[= \sum_{1\leq i\leq m_{0}}\sum_{1\leq k\leq m}\sum_{1\leq j\leq k}k ^{-1}\mbox{Prob}(\{P_{i}\in(\alpha(j-1)/m,\alpha j/m\}\cap C(i,k))\] \[\leq \sum_{1\leq i\leq m_{0}}\sum_{1\leq k\leq m}\sum_{1\leq j\leq k} j^{-1}\mbox{Prob}(\{P_{i}\in(\alpha(j-1)/m,\alpha j/m\}\cap C(i,k))\] \[= \sum_{1\leq i\leq m_{0}}\sum_{1\leq j\leq m}j^{-1}\sum_{j\leq k \leq m}\mbox{Prob}(\{P_{i}\in(\alpha(j-1)/m,\alpha j/m\}\cap C(i,k))\] \[\leq \sum_{1\leq i\leq m_{0}}\sum_{1\leq j\leq m}j^{-1}\alpha/m=m_{0} c_{m}\alpha/m.\]

Since this holds for arbitrary \(\alpha\), replacing \(\alpha\) by \(\alpha/c_{m}\), one arrives at the following result, under an additional assumption of continuity of the test statistics.

**Theorem 13.2** (Benjamini-Yekutieli).: _Consider the procedure: Reject all null hypotheses \(H_{0i}\) with the smallest \(k\)\(p\)-values where \(k=\max\{i:p_{(i)}\leq i\alpha/(c_{m}m)\}\), and \(c_{m}=\sum_{1\leq j\leq m}(1/j)\). The false discovery rate of this procedure is no more than \((m_{0}/m)\alpha\leq\alpha\)._

### An Application to a Diffusion Tensor Imaging Data Set

In this section, we consider an application of multiple testing to a diffusion tensor imaging (DTI) data set from a HIV study. The DTI data set consist of 46 subjects with 28 HIV+ subjects and 18 healthy controls. Diffusion tensors were extracted along the fiber tract of the splenium of the corpus callosum of each subject. The DTI for all the subjects are registered in the same atlas space based on arc lengths, with tensors extracted from 75 locations along the fiber tract of each subject. This data set have been studied in a regression setting in Yuan et al. (2012). We instead consider the problem of multiple testing with \(m=75\) null hypotheses in testing whether there is a difference between control group and the HIV+ group at each of the 75 locations where the DTI are extracted. We carry out the tests using the two procedures introduced in the previous section.

The DTI data are represented by the diffusion matrices which are 3 by 3 positive definite matrices. At each location, a nonparametric testing statistics is constructed. Let \(X_{i1},\ldots,X_{in_{1}}\) be the sample of DTI data from control group at location \(i\) (\(i=1,\ldots,75\)) and \(Y_{i1},\ldots,Y_{in_{2}}\) be an i.i.d. sample from the HIV positive group at location \(i\), with \(\overline{X}_{i}\) and \(\overline{Y}_{i}\) their corresponding sample means. That is, \(\overline{X}_{i}\) and \(\overline{Y}_{i}\) are the sample mean vectors of dimension 6 for the 6 distinct values of the vectorized data. Let \(\Sigma_{X_{i}}\) and \(\Sigma_{Y_{i}}\) be the corresponding sample covariance matrices at location \(i\). Then, for testing the two-sample hypothesis \(H_{0i}\) we use the test statistic \((\overline{X}_{i}-\overline{Y}_{i})\Sigma^{-1}(\overline{X}_{i}-\overline{Y}_ {i})^{T}\) with \(\Sigma=(1/n_{1}\Sigma_{X_{i}}+1/n_{2}\Sigma_{Y_{i}})\), which has the asymptotic chi-square distribution \(\chi^{2}(6)\).

Set the significant level \(\alpha=0.05\). We first carry out the test using the _Benjamini-Hochberg procedure_. That is, reject only the \(k\) null hypothesis with the smallest \(p\)-values, where \(k=\max\{i:p_{(i)}\leq\frac{1}{m}\alpha\}\). We first order the 75 \(p\)-values corresponding to the tests carried out at all the locations (see Fig. 13.1 for a plot of the \(p\)-values as a function of the arc length (location) before the ordering). The ordered \(p\)-values are compared with the vector \(\{0.05/75,0.1/75,\ldots,0.05\}\), which gives the result \(k=58\). Therefore we reject the 58 null hypotheses corresponding to the first 58 ordered \(p\)-values.

We now carry out the tests using the _Benjamini-Yekutieli procedure_. The ordered vector of \(p\)-values are compared with the vector \(\{i\alpha/(c_{m}m),i=1,\ldots,m\}\), where \(\alpha=0.05\), \(m=75\), and \(c_{m}=\sum_{1\leq j\leq m}1/j=4.0021\). With \(k=\max\{i:p_{(i)}\leq i\alpha/(c_{m}m)\}\). The procedure yields \(k=50\). Thus we reject the 50 null hypotheses corresponding to the first 50 ordered \(p\)-values.

### Notes and References

The presentation in this chapter depends much on the article by Benjamini and Yekutieli (2001). For a history of multiple testing we refer to this article, and to Sarkar (1998), and Sen (1999a,b). In Sen (1999a,b) one may also find the history of earlier procedures suited especially in clinical trials and their connections with the classic work of Scheffe (1959) on Normal models and the early work of S.N. Roy detailed in Roy et al. (1971). The data example in Sect. 13.3 is taken from Bhattacharya and Lin (2016).

## Exercises for Chap. 13

**Ex. 13.1**.: Let \(P\) denote the \(p\)-value of a test of a null hypothesis \(H^{0}\) of size no more than \(\alpha\).

1. Show that \(\mathrm{Prob}(P\leq\alpha|H^{0})\leq\alpha\), with equality if the test is of exact size \(\alpha\), i.e., \(\mathrm{Prob}(\mathrm{Reject}\ H^{0}\mid H^{0})=\alpha\).
2. Prove that, if the test statistic \(T\) which rejects \(H^{0}\) if \(T>c\) (for \(c\) depending on \(\alpha\)) has a continuous distribution under \(H^{0}\), then the \(p\)-value has the uniform distribution on \([0,1]\).

## References

* Benjamini & Hochberg (1995) Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. _Journal of the Royal Statistical Society, Series B,__57_, 289-300.
* Benjamini & Yekutieli (2001) Benjamini, Y., & Yekutieli, D. (2001). The control of the false discovery rate in multiple testing under dependency. _Annals of Statistics,__29_(4), 1165-1188.
* Bhattacharya & Lin (2016) Bhattacharya, R., & Lin, L. (2016). Omnibus CLTs for Frechet means and nonparametric inference on non-Euclidean spaces. _Proceedings of the American Mathematicasl Society_ (in Press).
* Reiner et al. (2003) Reiner, A., Yekutieli, D., & Benjamini, Y. (2003). Identifying differentially expressed genes using false discovery rate controlling procedures. _Bioinformatics,__19_(3), 368-375.
* Reiner et al. (2004)

Figure 13.1: \(p\)-values along the fiber trackRoy, S. N., Gnanadesikan, R., & Srivastava, J. N. (1971). _Analysis and design of certain quantitative multiresponse experiments._ New York: Pergamon Press.
* Sarkar (1998) Sarkar, S. K. (1998). Some probability inequalities for ordered MTP2 random variables: A proof of Simes conjecture. _Annals of Statistics, 26_, 494-504.
* Scheffe (1959) Scheffe, H. (1959). _The analysis of variance._ New York: Wiley.
* Sen (1999a) Sen, P. K. (1999a). Some remarks on Simes-type multiple tests of significance. _Journal of Statistical Planning and Inference, 82_, 139-145.
* Sen (1999b) Sen, P. K. (1999b). Multiple comparisons in interim analysis. _Journal of Statistical Planning and Inference, 82_, 5-23.
* Simes (1986) Simes, R. J. (1986). An improved Bonferroni procedure for multiple tests of significance. _Biometrika, 73_, 751-754.
* Yuan _et al._ (2012) Yuan, Y., Zhu, H., Lin, W., & Marron, J. S. (2012). Local polynomial regression for symmetric positive definite matrices. _Journal of the Royal Statistical Society: Series B, 74_, 697-719.

## Chapter 14 Markov Chain Monte Carlo (MCMC) Simulation and Bayes Theory

### 14.1 Metropolis-Hastings Algorithm

The topic of this chapter is the use of Markov chain theory in the computation of posterior distributions in Bayes theory. First consider the problem of computing the expectation \(Eg(Z)\) when \(Z\) has a pdf \(f\) with respect to a sigma-finite measure \(\mu\) on a state space \((S,\mathscr{S})\), and \(g\) is a measurable function with a finite expectation,

\[Eg(Z)=\int g(y)f(y)\mu(dy). \tag{14.1}\]

Suppose \(f\) is not explicitly known, but the _ratio_\(f(x)/f(y)\) is tractable. This occurs in mathematical physics where \(Z\) has the Gibbs distributions and \(f\) is given as \(f(y)=r\exp\{h(x)\}\), with \(h\) known explicitly (for example, as the Hamiltonian of a system of a large number of particles), but the normalizing constant \(r\), the so-called _partition function_, is not known analytically in computable form, being simply given as the reciprocal of the multiple integral of \(f\) over a very large number of coordinates. Here \(f(x)/f(y)=\exp\{h(x)\}/\exp\{h(y)\}\) is known explicitly and it is computable. Another example occurs in Bayes theory, where \(f(\theta)=\pi(\theta\mid\mathbf{X})\) is the _posterior density_ of \(\theta\), i.e., the conditional density of \(\theta\), given the observation \(\mathbf{X}\). Given a parametric family \(g(\mathbf{x}\mid\theta)\), \(\theta\in\boldsymbol{\Theta}\)\((=S)\), of densities of \(\mathbf{X}\) with respect to some sigma-finite measure \(v\) (e.g., \(\mathbf{X}=(X_{1},\ldots,X_{n})\)--a random sample from some distribution parametrized by \(\theta\)), the Bayesian views \(g(\mathbf{x}\mid\theta)\) as the conditional density of \(\mathbf{X}\) (at \(\mathbf{x}\)), given the value of the random variable \(\theta\) which has a density \(\pi(\theta)\), say, with respect to Lebesgue measure \(\mu\) on the \(p\)-dimensional parameter space \(\boldsymbol{\Theta}\subset\mathbb{R}^{p}\). One calls \(\pi d\mu\) the _prior distribution_ of the random variable \(\theta\). In this formulation, the joint density of \((\theta,\mathbf{X})\) is given by \(\pi(\theta)g(\mathbf{x}\mid\theta)\), and the posterior density of \(\theta\) is \(\pi(\theta\mid\mathbf{x}):=\pi(\theta)g(\mathbf{x}\mid\theta)/c(\mathbf{x})\), \(c(\mathbf{x})\) being the marginaldensity of \(\mathbf{X}\). That is, \(c(\mathbf{x})=\int\pi(\theta)g(\mathbf{x}\mid\theta)\mu(d\theta)\) which is usually very difficult to compute. On the other hand, \(\pi(\theta_{1}\mid\mathbf{x})/\pi(\theta_{2}\mid\mathbf{x})=\pi(\theta_{1})g( \mathbf{x}\mid\theta_{1})/\pi(\theta_{2})g(\mathbf{x}\mid\theta_{2})\) is explicitly given in an analytical and easily computable form.

For an introduction to Markov chains, also referred to as Markov processes in discrete time, see Chap. 8, Sect. 8.6. While this is enough for an understanding of the results in this chapter, for proofs and a more detailed account, especially suited for the present topic, see Robert and Casella (2004, Chap. 6), which is also a standard reference for MCMC as a whole. Our presentation is also influenced by an exposition given in Wasserman (2003, Chap. 24).

In general for the computation of (14.1), the _Metropolis-Hastings algorithm_ constructs a Markov chain \(\{X_{j}:j=0,1,2,\dots\}\) with a transition probability density function \(p(x,y)\) which satisfies the following _detailed balance condition:_

\[f(x)p(x,y)=f(y)p(y,x)\quad\text{for all }x,y. \tag{14.2}\]

On integrating both sides with respect to \(x\), one gets

\[\int f(x)p(x,y)\mu(dx)=f(y)\quad\text{for all }y, \tag{14.3}\]

which implies that \(f\) is an invariant density for the Markov process. By ensuring that the choice of \(p(x,y)\) is such that this invariant probability is unique and the strong law of large numbers (ergodic theorem) for Markov processes holds, one gets

\[N^{-1}\sum_{1\leq j\leq N}g(X_{j})\longrightarrow\int g(y)f(y)\mu(dy)\,\text{ with probability one, as }N\to\infty. \tag{14.4}\]

Since the number \(N\) of realizations of successive states of the Markov chain can be taken to be as large as needed by the simulation procedure, the objective of computing the desired expectation is achieved. We now describe the method for constructing \(p(x,y)\). For simplicity, take \(S\) to be an open subset of \(\mathbb{R}^{d}\), and \(\mu\) Lebesgue measure.

**Metropolis-Hastings Algorithm**

_Step 1:_: _Choose a transition probability density_ \(q(x,y)\) _(with respect to_ \(\mu\)_) of a Markov process on_ \(S\)_, i.e., (i) for each_ \(x\in S\)_,_ \(y\to q(x,y)\) _is a probability density and (ii) for each_ \(y\in S\)_,_ \(x\to q(x,y)\) _is measurable._

_Step 2:_: _Starting with some initial state_ \(X_{0}=x_{0}\)_, pick_ \(Y_{0}\) _with density_ \(q(x_{0},\cdot)\) _and choose_ \(X_{1}\) _according to the rule_

\[X_{1}=\left\{\begin{array}{ll}Y_{0}\text{ with probability }a(x_{0},Y_{0}),\\ X_{0}\text{ with probability }1-a(x_{0},Y_{0}),\end{array}\right. \tag{14.5}\]

_where the_ acceptance ratio \(a(x,y)\) _is defined by_

\[a(x,y)=\min\left\{\frac{f(y)}{f(x)}\cdot\frac{q(y,x)}{q(x,y)}\,,1\right\}. \tag{14.6}\]

_Step 3:_: _In general, after_ \(X_{n}\) _is chosen, pick_ \(Y_{n}\overset{\mathscr{L}}{\sim}q(X_{n},y)\mu(dy)\) _and pick_ \(X_{n+1}\) _according to (_14.5_), (_14.6_), with_ \(X_{n}\) _in places of_ \(X_{0}\) _and_ \(Y_{n}\) _in place of_ \(Y_{0}\)__\((n=1,2,\dots)\)

**Theorem 14.1**.: _Assume \(q(x,y)>0\) for all \(x,y\in S\). Then (a) the transition probability density \(p(x,y)\) of the Markov chain \(\{X_{j}:j=0,1,\dots\}\) has \(f\) as its invariant probability density, and (b) the convergence (14.4) holds for every \(g\) such that\(\int|g(y)|f(y)\mu(dy)<\infty\)._

Proof.: (a). We only need to establish the detailed balance relation (14.2). For this fix \(x,y\in S\) (\(x\neq y\)), and suppose that \(f(x)q(x,y)<f(y)q(y,x)\), so that \(a(x,y)=1\) and \(a(y,x)=f(x)q(x,y)/f(y)q(y,x)\). Therefore, the transition probability density \(p(x,y)\) of the Markov chain \(\{X_{j}:j=0,1,\dots\}\) is given by

\[p(x,y) =q(x,y)a(x,y)=q(x,y),\] \[p(y,x) =q(y,x)a(y,x)=q(y,x)f(x)q(x,y)/(f(y)q(y,x))\] \[=\frac{f(x)}{f(y)}\,q(x,y).\] Therefore, \(f(x)p(x,y)=f(y)p(y,x)\). The case \(f(x)q(x,y)>f(y)q(y,x)\) is treated similarly. In the case of equality, (14.2) holds trivially.

(b). This is a standard result for positive Harris recurrent Markov chains, for which we refer to Robert and Casella (2004, Theorems 6.63, 7.4).

_Remark 14.1_.: To facilitate numerical simulation it is important to choose the _proposal density_\(q(x,y)\) well, as well as a good initial state \(x_{0}=X_{0}\). One choice is \(q(x,y)=g(y)\) where \(g(y)\) is a positive density on \(S\). This is referred to as _independent Metropolis-Hastings algorithm_. One assumes here that it is simple to draw observations from the distribution \(g(y)\mu(dy)\). This method leads to fast convergence to the limit (14.4) if there is a constant \(M\) such that \(f(x)\leq Mg(x)\) for all \(x\). Another choice is to have a _symmetric proposal density_\(q(x,y)=q(y,x)\), so that it is simple to draw from \(q(x,y)\mu(dy)\). In this case the acceptance ratio takes the simpler form \(a(x,y)=\min\left\{\frac{f(y)}{f(x)},1\right\}\). For example, if \(S=\mathbb{R}^{d}\) one may choose \(q(x,y)=\varphi(x-y:c)\) where \(\varphi(z:c)\) is the \(d\)-dimensional Normal density with mean zero and dispersion matrix \(cI_{d}\) for some properly chosen \(c>0\). This is referred to as the _random walk Metropolis-Hastings algorithm_. If \(S\) is an open interval (or an open rectangle) one may make a transformation of it diffeomorphic to \(\mathbb{R}^{d}\) and apply the algorithm. For these we refer to Robert and Casella (2004, Chap. 7).

### Gibbs Sampler

The Metropolis-Hastings algorithm is difficult to apply directly to \(S\) of dimension \(d>1\), partly because of the problem with directly generating random vectors and partly because of the slow rate of convergence to stationarity of multidimensional chains. The _Gibbs sampler_ alleviates these problems by using several one-dimensional problems to deal with a multidimensional problem. To illustrate this, consider the two-dimensional problem with \(f(x,y)\) as the density of \((X,Y)\) on \(S\subset\mathbb{R}^{2}\). Let \(f_{Y|X}(y\mid x)\) denote the conditional density of \(Y\) (at \(y\)), given \(X=x\). Similarly define \(f_{X|Y}(x\mid y)\).

Suppose it is possible to simulate from the one-dimensional, conditional distributions \(f_{X|Y}(\cdot\mid y)\) and \(f_{Y|X}(\cdot\mid x).\) The following algorithm for simulating \((X_{n},Y_{n})\) converging to the distribution \(f(x,y)\) is then used in the two-dimensional case, and it can be generalized in an obvious way to higher dimensions.

**Two-Stage Gibbs Sampling Algorithm**

_Step 1._: _Begin with a suitable initial state_ \((x_{0},y_{0})\) _and generate_ \(X_{1}\) _with density_ \(f_{X|Y}(\cdot\mid y_{0})\)_._ _Step 2._: _Generate_ \(Y_{1}\) _with density_ \(f_{Y|X}(\cdot\mid X_{1})\)_._ _Step 3._: _Given_ \((X_{n},Y_{n})\)_, generate_ \(X_{n+1}\) _with density_ \(f_{X|Y}(\cdot\mid Y_{n})\)_, and generate_ \(Y_{n+1}\) _with density_ \(f_{Y|X}(\cdot\mid X_{n+1})\)__\((n=1,2,\dots)\)_._

**Theorem 14.2**.: _Assume \(f(x,y)>0\)\(\forall\,(x,y)\) belonging to an open rectangle \(S\subset\mathbb{R}^{2}\). Then (a) the Markov chain \((X_{n},Y_{n})\), \(n\geq 0\), has the invariant density \(f(x,y)\) and (b) the two-dimensional analog of (14.4) holds._

Proof.:
1. The transition probability density of the Markov chain \((X_{n},Y_{n})\), \(n\geq 0\), is given by \[q(x_{1},y_{1}\mid x_{0},y_{0})=f_{X|Y}(x_{1}\mid y_{0})f_{Y|X}(y_{1}\mid x_{1 })=\frac{f(x_{1},y_{0})}{f_{Y}(y_{0})}\cdot\frac{f(x_{1},y_{1})}{f_{X}(x_{1})}\] so that \[\int_{S}q(x_{1},y_{1}\mid x_{0},y_{0})f(x_{0},y_{0})\mu(dx_{0}) \mu(dy_{0})\] \[=\int_{S}\frac{f(x_{1},y_{0})f(x_{1},y_{1})f(x_{0},y_{0})}{f_{Y}(y _{0})f_{X}(x_{1})}\mu(dx_{0})\mu(dy_{0})\] \[=\int\left(\int f(x_{0},y_{0})\mu(dx_{0})\right)\left\{f(x_{1},y_ {0})f(x_{1},y_{1})/f_{Y}(y_{0})f_{X}(x_{1})\right\}\mu(dy_{0})\] \[=\int\frac{f_{Y}(y_{0})f(x_{1},y_{0})f(x_{1},y_{1})}{f_{Y}(y_{0}) f_{X}(x_{1})}\mu(dy_{0})=f(x_{1},y_{1}),\] establishing the claim.
2. The proof of convergence again follows from Theorem 6.63 in Robert and Casella (2004). 

An efficient way of implementing the Gibbs sampler is to use the following simulation procedure.

**Metropolis-Hastings with Gibbs Sampling**

_Step 1._: _Choose symmetric proposal distributions_ \(q\) _and_ \(\tilde{q}\) _for drawing from_ \(f_{X|Y}\) _and_ \(f_{Y|X}\)_, respectively:_ \(q(x,x^{\prime})=q(x^{\prime},x)\)_,_ \(\tilde{q}(y,y^{\prime})=\tilde{q}(y^{\prime},y)\)_._

_Step 2._: _Choose some initial_ \(X_{0}=x_{0}\)_,_ \(Y_{0}=y_{0}\)_. Draw_ \(W_{0}\) _from_ \(q(\cdot,X_{0})\)_, and let_ \[X_{1}=\left\{\begin{array}{ll}W_{0}&\mbox{with probability $a(X_{0},W_{0} \mid Y_{0})$}\\ X_{0}&\mbox{with probability $1-a(X_{0},W_{0}\mid Y_{0})$},\end{array}\right.\] _and draw_ \(Z_{0}\) _from_ \(\tilde{q}(\cdot,Y_{0})\) _and let_ \[Y_{1}=\left\{\begin{array}{ll}Z_{0}&\mbox{with probability $\tilde{a}(Y_{0},Z_{0} \mid X_{1})$}\\ Y_{0}&\mbox{with probability $1-\tilde{a}(Y_{0},Z_{0}\mid X_{1})$},\end{array}\right.\]_where_

\[a(x,x^{\prime}\mid y) =\min\left\{\frac{f_{X\mid Y}(x^{\prime}\mid y)}{f_{X\mid Y}(x\mid y )}\,,1\right\}=\min\left\{\frac{f(x^{\prime},y)}{f(x,y)}\,,1\right\},\] \[\tilde{a}(y,y^{\prime}\mid x) =\min\left\{\frac{f_{Y\mid X}(y^{\prime}\mid x)}{f_{Y\mid X}(y\mid x )}\,,1\right\}=\min\left\{\frac{f(x,y^{\prime})}{f(x,y)}\,,1\right\}.\]

_Having drawn \((X_{n},Y_{n})\), draw \((X_{n+1},Y_{n+1})\) following the procedure in Step 2, but with \(X_{0}\) replaced by \(X_{n}\) and \(Y_{0}\) by \(Y_{n}\), and with \(W_{0}\) replaced by \(W_{n}\) and \(Z_{0}\) by \(Z_{n}\). Here \(W_{n}\) is drawn afresh from \(q(\cdot,X_{n})\) and \(Z_{n}\) from \(\tilde{q}(\cdot\mid Y_{n})\)\((n\geq 1)\)._

The above procedure extends to higher dimensional problems. The following section is devoted to an application.

### 14.3 Bayes Estimation in the Challenger Disaster Problem: A Project for Students

Consider the space shuttle disaster problem described in Chap. 4, pp. 58-60. Assume the same logistic regression model.

**Suggested Model** Let \(Y\) denote the failure status (response variable), and \(X\) the temperature in degrees F at launch time (explanatory variable). Use the _logistic regression model,_

\[P(Y=1\mid X=x) =\frac{\exp\{\alpha+\beta x\}}{[1+\exp\{\alpha+\beta x\}]}=p(x), \mbox{ say, and}\] \[P(Y=0\mid X=x) =1-p(x).\]

Note that one may express the model as

\[\log\left[\frac{p(x)}{(1-p(x))}\right]=\alpha+\beta x.\]

Hence the name logistic regression.

Assume that the regressor \(x\) is _stochastic_ and \((X_{i},Y_{i})\) are i.i.d. random vectors.

For 23 independent \(Y\) observations \((y_{1},\ldots,y_{23})\) the conditional likelihood function (i.e., the conditional p.d.f. of \(Y_{i}\), given \(X_{i}=x_{i}\)\((i=1,\ldots,23)\)), is

\[\ell(\mathbf{y}\mid\mathbf{x};\alpha,\beta)=\prod_{i=1,\ldots,23}\left[p(x_{i} )^{y_{i}}(1-p(x_{i}))^{1-y_{i}}\right],\]

and the (conditional) log likelihood is

\[\log\ell=\sum_{i}[y_{i}(\alpha+\beta x_{i})]-\sum_{i}\log[1+\exp\{\alpha+\beta x _{i}\}].\]

Assume that the distribution of \(X_{i}\) does not involve \(\alpha\), \(\beta\).

1. Use a suitable prior distribution of \((\alpha,\beta)\) to compute the posterior distribution of \((\alpha,\beta)\) on a sufficiently fine grid of points. Do this numerically, by MCMC. For example, you may assume \(\alpha\) and \(\beta\) to be independent random variables with \(\alpha\) having the Normal distribution \(N(\alpha_{0},\sigma_{0}^{2})\) and \(-\beta\) having a log-normal distribution, i.e., \(\xi\equiv\log(-\beta)\) has the distribution \(N(\xi_{0},\eta_{0}^{2})\). Observe that in the present context \(\beta<0\). For this prior, you may choose the parameters \(\alpha_{0}=10\), \(\sigma_{0}^{2}=20\), \(\xi_{0}=1\), \(\eta_{0}^{2}=1\), or use the MLEs from Chap. 4, pp. 65, 66. [Hint: First compute the posterior distribution of \((\alpha,\xi)\) and then obtain that of \((\alpha,\beta)\) by simply noting that \(\xi=\log(-\beta)\). Denoting by \(\pi_{1}(\alpha)\pi_{2}(\xi)\) the prior density of \((\alpha,\xi)\), its posterior density is \[\pi(\alpha,\xi\mid\mathbf{x},\mathbf{y})=\frac{\pi_{1}(\alpha)\pi_{2}(\xi)/ \ell(\mathbf{y}\mid\mathbf{x};\alpha,\beta)}{\int\ell(\mathbf{y}\mid\mathbf{x };\alpha,\beta)\pi_{1}(\alpha)\pi_{2}(\xi)d\alpha d\xi}=\frac{\ell(\mathbf{y} \mid\mathbf{x};\alpha,\beta)\pi_{1}(\alpha)\pi_{2}(\xi)}{h(\mathbf{x},\mathbf{ y})}\] [Recall \(\ell(\mathbf{y}\mid\mathbf{x};\alpha,\beta)=\prod_{i=1,\ldots,23}[p(x_{i})^{y_{i}}(1-p(x_{i}))^{1-y_{i}}\); \(p(x)=\exp\{\alpha+\beta x\}\), \(\beta=-e^{\xi}\).] Hence \(\pi(\alpha_{1},\xi_{1}\mid\mathbf{x},\mathbf{y})/\pi(\alpha_{2},\xi_{2}\mid \mathbf{x},\mathbf{y})=\pi_{1}(\alpha_{1})\pi_{2}(\xi_{1})\ell(\mathbf{y}\mid \mathbf{x};\alpha_{1},\beta_{1})/\ell(\mathbf{y}\mid\mathbf{x};\alpha_{2}, \beta_{2})\) is computable for all \((\alpha_{i}\xi_{i})\)\((i=1,2)\). Now the conditional (posterior) densities are denoted \(f_{\alpha|\xi}\) and \(f_{\xi|\alpha}\), and one has \[\frac{f_{\alpha|\xi}(\alpha_{1}\mid\xi)}{f_{\alpha|\xi}(\alpha_{2 }\mid\xi)} =\frac{\ell(\mathbf{y}\mid\mathbf{x};\alpha_{1},\beta)\pi_{1}( \alpha_{1})}{\ell(\mathbf{y}\mid\mathbf{x};\alpha_{2},\beta)\pi_{1}(\alpha_{2} )}\,,\] \[\frac{f_{\xi|\alpha}(\xi_{1}\mid\alpha)}{f_{\xi|\alpha}(\xi_{2}\mid\alpha)} =\frac{\ell(\mathbf{y}\mid\mathbf{x};\alpha,\beta_{1})\pi_{2}(\xi_{1})}{\ell( \mathbf{y}\mid\mathbf{x};\alpha,\beta_{2})\pi_{2}(\xi_{2})}\,.\] (14.7) Use the _random walk Metropolis-Hastings algorithm_ with the proposal density for \(\alpha\), namely \(q(\alpha_{1},\alpha_{2})\), as the Normal density (for \(\alpha_{1}\)) with mean \(\alpha_{2}\), and some variance, say \(\sigma_{0}^{2}=20\). Similarly, let the proposal density for \(\xi=\log(-\beta)\), \(q^{\sim}(\xi_{1},\xi_{2})\) as the Normal density (for \(\xi_{1}\)) with mean \(\xi_{2}\) and variance \(\eta_{0}^{2}=1\) (You may, of course, choose different variances for these if you like). Now follow the steps of the _Metropolis-Hastings with Gibbs Sampling_ steps.] 2. (i) Use the posterior distribution of \((\alpha,\beta)\) in (a) to find the (histograms of the) posterior distributions of the failure probabilities \(p(x)\) at launch time temperatures \(x=31\,^{\circ}\)F, and \(x=65\,^{\circ}\)F. (ii) Locate the tenth percentile \(\delta_{0.10}\), say, of the histogram for the posterior distribution in (i) for the failure probability at \(x=31\,^{\circ}\)F (i.e., \(10\,\%\) of the points of the histogram lie below \(\delta_{0.10}\) while \(90\,\%\) lie above it). Observe that, in the Bayesian paradigm, the statistician has a \(90\,\%\) faith (or confidence) that the probability of failure at \(31\,^{\circ}\)F is at least \(\delta_{0.10}\).

## Appendix A Project for Students

#### Project: Space Shuttle Disaster

In 1986, the space shuttle Challenger exploded during take off, killing the seven astronauts aboard. It was determined that the explosion was the result of an O-ring failure, a splitting of a ring of rubber that seals different parts of the ship together. The flight accident was believed to be caused by the unusually cold weather (\(31\,^{\circ}\)F) at the time of the launch.

The past O-ring failure data along with temperature at launch time are given below (in increasing order of temperature) for 23 prior flights. The flight numbers denote the (unimportant) time order of launch. The numbers 0 and 1 indicate "no O-ring failure" and "O-ring failure", respectively.

#### Project Objective

Estimate the probability of O-ring failure at temperature 31 \({}^{\circ}\)F and at 65 \({}^{\circ}\)F.

\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|r|r|} \hline Flight\# & 14 & 9 & 23 & 10 & 1 & 5 & 13 & 15 & 4 & 3 & 8 & 17 & 2 \\ \hline Failure & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\ \hline Temp. in Degrees F & 53 & 57 & 58 & 63 & 66 & 67 & 67 & 67 & 68 & 69 & 70 & 70 & 70 \\ \hline Flight\# & 11 & 6 & 7 & 16 & 21 & 19 & 22 & 12 & 20 & 18 & \\ \hline Failure & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline Temp. in Degrees F & 70 & 72 & 73 & 75 & 75 & 76 & 76 & 78 & 79 & 81 & \\ \hline \end{tabular}

### Notes and References

Original sources for the material of this chapter may be traced to Metropolis et al. (1953), Hastings (1970), Geman and Geman (1984) and Gelfand and Smith (1990). A standard reference for the subject is Robert and Casella (2004), where one also finds a detailed history of the development of the subject and extensive references. Our presentation is much influenced by Wasserman (2003, Chap. 24), which gives a very readable introduction. For the Challenger disaster problem in Sect. 14.3 we refer to Robert and Casella (2004, pp. 15-19, 281, 282).

For a modern and exciting perspective of the Metropolis-Hastings algorithm, including a dramatic application to cryptography, we refer to Diaconis (2009).

### Exercises for Chap. 14

#### Ex. 14.1

Do the Project.

## References

* Diaconis (2009) Diaconis, P. (2009). The Markov Chain Monte Carlo revolution. _Bulletin of the American Mathematical Society,__46_(2), 179-20.
* Gelfand and Smith (1990) Gelfand, A. E., & Smith, A. F. M. (1990). Sampling-based approaches to calculating marginal densities. _Journal of the American Statistical Association,__85_(410), 398-409.
* Geman and Geman (1984) Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. _IEEE Transactions on Pattern Analysis and Machine Intelligence,__6_(6), 721-741.

* [HastingsHastings1970] Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications. _Biometrika, 57_(1), 97-109.
* [Metropolis, Rosenbluth, Rosenbluth, Teller, Teller, Teller, 1953] Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., Teller, E. (1953). Equations of state calculations by fast computing machines. _Journal of Chemical Physics, 21_(6), 1087-1092.
* [Robert CasellaRobert Casella2004] Robert, C. Casella, G. (2004). _Monte Carlo statistical methods._ New York: Springer.
* [WassermanWasserman2003] Wasserman, L. (2003). _All of statistics: A concise course in statistical inference._ New York: Springer.

## Chapter Miscellaneous Topics

### 15.1 Classification/Machine Learning

The problem of classification is that of assigning an observation \(X\in\mathscr{X}\) (observation space) to one of a finite number of distributions indexed as \(Y\in\mathscr{Y}\) (finite set of _classes_) from which it came. In the computer science literature \(X\) is referred to as the _input_ and \(Y\) the _output_ and an algorithm \(h:\mathscr{X}\rightarrow\mathscr{Y}\) that assigns an observation to a class is sometimes described as _machine learning_. One may think of diagnosing a patient based on her symptoms \(X\) [e.g., \(X\) may be a vector of measurements of her systolic pressure, cholesterol level (LDL), heart rate, etc.] as having one, say \(Y\), of a set \(\mathscr{Y}\) of possible ailments. Assume first that the distributions indexed by \(\mathscr{Y}=\{1,\ldots,K\}\) have known densities \(f_{i}\), (\(i\in\mathscr{Y}\)), and that in the general population the proportion of the \(i\)th class is \(\pi_{i}\). An optimal procedure which minimizes the probability of misclassification is the following \(h^{*}\) called the _Bayes classifier_

\[h^{*}(x)=\operatorname*{argmax}_{j}\pi_{j}f_{j}(x). \tag{15.1}\]

If the set on the right has more than one element, choose any element from it. To prove optimality of \(h^{*}\), note that the probability of misclassification for any classifier \(h\) is

\[\sum_{j=1}^{K}\pi_{j}\int_{\mathscr{H}}\mathbf{1}_{\{h(x)\neq j\}} f_{j}(x)dx=1-\sum_{j=1}^{K}\int_{\mathscr{H}}\mathbf{1}_{\{h(x)=j\}}\pi_{j}f_{j}( x)dx\] \[=1-\sum_{i=1}^{K}\sum_{j=1}^{K}\int_{\mathscr{H}}\mathbf{1}_{\{h( x)=j,h^{*}(x)=i\}}\pi_{j}f_{j}(x)dx\]\[\leq 1-\sum_{i=1}^{K}\sum_{j=1}^{K}\int_{\mathscr{H}}\mathbf{1}_{\{h(x)= i\}}\pi_{i}f_{i}(x)dx\] \[=1-\sum_{i=1}^{K}\int_{\mathscr{H}}\mathbf{1}_{\{h^{*}(x)=i\}}\pi_{i }f_{i}(x)dx\] \[=\sum_{i=1}^{K}\pi_{i}\int_{\mathscr{H}}\mathbf{1}_{\{h^{*}(x)\neq i \}}\pi_{i}f_{i}(x)dx\] \[=\text{Probability of misclassification for $h^{*}$}.\]

Usually one does not know \(\pi_{j}\), \(f_{j}\) (\(1\leq j\leq K\)) and estimates them from a random sample, called _training data_ in the computer science literature.

As an example, suppose that \(X\) is \(d\)-dimensional Normal \(N(\mu_{j},\Sigma_{j})\), \(1\leq j\leq K\). Then a straightforward calculation shows (Exercise 15.1)

\[h^{*}(x)=\operatorname*{argmax}_{j}\left\{-\frac{1}{2}\log|\Sigma_{j}|-\frac{ 1}{2}(x-\mu_{j})^{\prime}\Sigma_{j}^{-1}(x-\mu_{j})+\log\pi_{j}\right\}, \tag{15.2}\]

where \(|\Sigma_{j}|\) is the determinant of \(\Sigma_{j}\). This _quadratic classifier_ is known as _Fisher's quadratic discriminant_ originally due to R.A. Fisher. The distance \(d(x,\mu)=[(x-\mu)^{\prime}\Sigma^{-1}(x-\mu)]^{\frac{1}{2}}\) is known as the _Mahalanobis distance._ In the case the dispersion matrices \(\Sigma_{j}\), are all the same, namely \(\Sigma\), it is simple to check that

\[h^{*}(x)=\operatorname*{argmax}_{j}\left\{x^{\prime}\Sigma_{j}^{-1}\mu_{j}- \tfrac{1}{2}\mu_{j}^{\prime}\Sigma_{j}^{-1}\mu_{j}+\log\pi_{j}\right\}. \tag{15.3}\]

The function within curly brackets is linear (in \(x\)) and is called the _linear discriminant_ and the Bayes procedure is referred to as the _linear discriminant analysis,_ or _LDA_.

A more widely used classifier is based on the regression function \(\ell(x)=E(Y\mid X=x)\). Although \(\ell(x)\) generally does not lie in the finite set \(\mathscr{Y}\), it is still used for its simplicity. Indeed, one often assumes \(\ell(x)\) to be linear in \(x:Y=\beta_{0}+\sum_{j=1}^{p}\beta_{j}X_{j}+\varepsilon\) with the usual assumptions (See Sect. 6.8, Theorem 6.4, Corollary 6.5). We will consider the case \(\mathscr{Y}=\{0,1\}\), i.e., \(K=2\) and write \(\ell(x)=P(Y=1\mid X=x)\), \(x=(x_{1},\ldots,x_{p})^{\prime}\). Given a training \(\operatorname*{sample}(Y_{i},X_{i1},\ldots,X_{ip})^{\prime}\), \(1\leq i\leq n\), the least squares estimate of \(\boldsymbol{\beta}=(\beta_{0},\beta_{1},\ldots,\beta_{p})^{\prime}\) is \(\hat{\boldsymbol{\beta}}=(\mathbf{X}^{\prime}\mathbf{X})^{-1}\mathbf{X}^{ \prime}\mathbf{Y}\) where

\[\mathbf{X}=\begin{bmatrix}1&X_{11}&\ldots&X_{1p}\\ 1&&\\ \vdots&\vdots&\\ 1&X_{n1}&\ldots&X_{np}\end{bmatrix},\quad\mathbf{Y}=(Y_{1},\ldots,Y_{n})^{ \prime}. \tag{15.4}\]

The classification rule for an observation \(X=(X_{1},\ldots,X_{p})^{\prime}\) is

\[\hat{h}(x)=\left\{\begin{array}{ll}1&\text{if $\hat{\ell}(x):=X^{\prime} \hat{\boldsymbol{\beta}}>\tfrac{1}{2}$},\\ 0&\text{otherwise}.\end{array}\right. \tag{15.5}\]

The rationale behind the rule is that \(P(Y=1\mid X=x)+P(Y=0\mid X=x)=1\). For much more on this issue refer to Hastie et al. (2001, Chap. 4).

A more appropriate model, compared to the classical linear model, is perhaps _logistic regression:_\(\ell(x)\equiv P(Y=1\mid X=x)=\exp\{\beta_{0}+\boldsymbol{\beta}^{\prime}x\}/[1+ \exp\{\beta_{0}+\boldsymbol{\beta}^{\prime}x\}]\). For estimation of the parameters of the model we refer to Chap. 4 (Project) and Chap. 14, Sect. 14.3. The classifier is then the analog of (15.5), namely:

\[\hat{h}(x)=\left\{\begin{array}{ll}1&\mbox{if }\hat{\ell}(x)>\frac{1}{2}\,, \\ 0&\mbox{otherwise.}\end{array}\right. \tag{15.6}\]

See Hastie et al. (2001, Chap. 4, Sect. 4).

For general parametric models with densities \(f_{j}\), \(1\leq j\leq K\), governing the \(K\) classes one may use the MLE for estimating the parameters from sufficiently large random samples, or training data, in order to compute a classifier \(\hat{h}\). If parametric models are not particularly reliable, one may resort to nonparametric density estimation by the _kernel method_ described in Chap. 10. If the observation \(X\) is high-dimensional then the convergence to the true density is generally very slow. In many data examples and simulations the _nonparametric Bayes_ estimation of the density, following Ferguson (1973, 1974), and using appropriate MCMC, seem to yield substantially better approximations. One may also use nonparametric regression for classification (See Ghosh and Ramamoorthi (2002), Chaps. 5, 7, Hjort et al. 2010, and Bhattacharya and Bhattacharya 2012, Chaps. 13, 14).

Finally, we mention the linear classifier known as the _support vector machines_ introduced by Vapnik and his co-authors (See Vapnik 1998). This theory provides the precise criterion for separating the training data by a hyperplane, and provides optimal separating hyperplane maximizing its distance from the data. One then uses this to classify an observation into one of two classes (\(K=2\)).

### Principal Component Analysis (PCA)

Let \(\mathbf{X}\) be a \(d\)-dimensional random vector with distribution \(Q\), mean \(\boldsymbol{\mu}=(\mu_{1},\ldots,\mu_{d})^{\prime}\) and covariance matrix \(\Sigma\) (\(d>1\)). Let \(u_{1},u_{2},\ldots,u_{d}\) be unit length (orthonormal) eigenvectors of \(\Sigma\) with eigenvalues \(\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{d}\). One may express \(\mathbf{X}\) as

\[\mathbf{X}=\sum_{i=1}^{d}\langle\mathbf{X},u_{i}\rangle u_{i}=\boldsymbol{\mu} +\sum_{i=1}^{d}\langle\mathbf{X}-\boldsymbol{\mu},u_{i}\rangle u_{i}, \tag{15.7}\]

where \(\langle\,,\,\rangle\) denotes Euclidean inner product (and \(\|v\|=\langle v,v\rangle\) is the Euclidean norm). The quantities \(\langle\mathbf{X}-\boldsymbol{\mu},u_{i}\rangle u_{i}\) are called _principal components_ of \(\mathbf{X}-\boldsymbol{\mu}\), or of \(\mathbf{X}\). Note that \(\mathrm{var}(\mathbf{X},u_{i})=\mathrm{var}\langle\mathbf{X}-\boldsymbol{\mu},u_{i}\rangle=\lambda_{i}\) (\(i=1,\ldots,d\)). The coefficient \(\langle\mathbf{X}-\boldsymbol{\mu},u_{1}\rangle\) of the first principal component has the largest variance \(\lambda_{1}\). The first principal component is considered to be the most important, followed by the next important component, namely the second principal component, and so on.

The most important use of PCA is in _dimension reduction_ especially when \(d\) is large. The following result says that the \(r\)-dimensional hyperplane passing through \(\boldsymbol{\mu}\) and generated by the first \(r\) principal components provides the best \(r\)-dimensional subspace approximation for \(\mathbf{X}\) for the criterion of minimizing expected squared distance.

**Theorem 15.1**.: _Consider the orthogonal projection of \(\mathbf{X}\) onto an \(r\)-dimensional hyperplane passing through \(\boldsymbol{\mu}\). The expected squared distance between \(\mathbf{X}\) and such a projection is minimized over the class of all such hyperplanes when the hyperplane is the translation by \(\bolds{\mu}\) of the \(r\)-dimensional subspace spanned by \(u_{1},u_{2},\ldots,u_{r}\). In particular, the optimal projection is \(\sum_{i=1}^{r}\langle\mathbf{X}-\bolds{\mu},u_{i}\rangle u_{i}+\bolds{\mu}\)._

Proof: An \(r\)-dimensional hyperplane passing through \(\bolds{\mu}\) may be expressed as \(\bolds{\mu}+H\), where \(H\) is spanned by \(r\) orthonormal vectors \(v_{1},v_{2},\ldots,v_{r}\). The projection of \(\mathbf{X}\) on this hyperplane is \(\bolds{\mu}+\sum_{i=1}^{r}\langle\mathbf{X}-\bolds{\mu},v_{i}\rangle v_{i}\) whose expected squared distance from \(\mathbf{X}\) is

\[\mathbb{E}\|\mathbf{X}-\bolds{\mu}-\sum_{i=1}^{r}\langle\mathbf{X} -\bolds{\mu},v_{i}\rangle v_{i}\|^{2}\] \[=E\|\mathbf{X}-\bolds{\mu}\|^{2}+\sum_{i=1}^{r}E\langle\mathbf{X} -\bolds{\mu},v_{i}\rangle^{2}-2\sum_{i=1}^{r}E\langle\mathbf{X}-\bolds{\mu}, v_{i}\rangle^{2}\] \[=E\|\mathbf{X}-\bolds{\mu}\|^{2}-\sum_{i=1}^{r}E\langle\mathbf{X} -\bolds{\mu},v_{i}\rangle^{2}\] \[=E\|\mathbf{X}-\bolds{\mu}\|^{2}-\sum_{i=1}^{r}v_{i}^{\prime} \Sigma v_{i}. \tag{15.8}\]

The minimizer of this over all orthonormal \(r\)-tuples \(\{v_{1},v_{2},\ldots,v_{r}\}\) is the maximizer

\[\operatorname*{argmax}_{\{v_{1},v_{2},\ldots,v_{r}\}\atop\text{orthonormal}} \sum_{i=1}^{r}v_{i}^{\prime}\Sigma v_{i}. \tag{15.9}\]

First choose \(v_{1}\) to maximize \(v_{1}^{\prime}\Sigma v_{1}\) over the class of all vectors of norm \(1\). The maximum value is \(\lambda_{1}\) attained by \(u_{1}\). Next choose \(v_{2}\) to minimize \(v_{2}^{\prime}\Sigma v_{2}\) over the class of all vectors of norm \(1\) orthogonal to \(u_{1}\); this maximum is \(\lambda_{2}\) attained by \(v_{2}=u_{2}\). In this manner we arrive at \(\{u_{1},u_{2},\ldots,u_{r}\}\) as the maximizer of (15.9), and the minimizer of (15.8). 

Remark 15.1: One estimates \(\bolds{\mu}\) and \(\Sigma\) and, consequently, eigenvalues and eigenvectors from random samples taken from underlying distributions \(Q\). These estimates are then used for constructing principal components and dimension reduction.

Remark 15.2: A word of caution in the use of PCA for purposes of inference such as classification and two-sample tests with high-dimensional data. The idea of dimension reduction via PCA is not that one ignores data pertaining to those principal components with small variances. It simply means that those components may simply be represented by their means. Notice that the optimal \(r\)-dimensional hyperplane in Theorem 15.1 passes through \(\bolds{\mu}\), which is the mean vector of _all_ the coordinates. Indeed, if one wishes to discriminate one distribution \(Q_{1}\) from another, \(Q_{2}\), based on a few principal components estimated from random samples from the two distributions, it would be prudent to compare the means of the lowest principal components. For even small differences in means of these components are more likely to be detected, because of small variances, than similar differences in higher level principal components. See Bhattacharya and Bhattacharya (2012, pp. 16, 17).

For applications of PCA in image analysis and pattern recognition we refer to Bishop (2006, Chap. 4) and Hastie et al. (2001, Chap. 4).

### Sequential Probability Ratio Test (SPRT)

The _sequential probability ratio test_, or _SPRT,_ was introduced by Wald (1947). We provide here a brief outline of the procedure and its properties.

Usual statistical inference procedures, such as considered in this book so far, are based on fixed sample sizes, which may be either too small or inadequate or too large and wasteful for the specific goals one has. A sequential procedure tries to achieve the desired goals with the smallest possible sample sizes by drawing observations one at a time until the accumulated evidence becomes sufficient for the desired level of accuracy. To illustrate Wald's SPRT in this context we consider the problem of testing a simple null hypothesis \(H^{0}\) against a simple alternative \(H^{1}\) with prescribed probabilities \(\alpha\), \(\beta\) of Type 1 and Type 2 errors.

Let \(U_{1},U_{2},\dots,\) be i.i.d. observations with values in some measurable space \((S,\mathscr{S})\). Assume that their common distribution has a probability density \(f(u;\theta)\) (with respect to some measure \(\nu(du)\)). There are two hypotheses concerning the value of \(\theta\), namely, \(H^{0}:\theta=\theta_{0}\), \(H^{1}:\theta=\theta_{1}\), \((\theta_{0}\neq\theta_{1})\). Assume for simplicity that \(f(u;\theta_{0})\), \(f(u;\theta_{1})\) are both strictly positive for all \(u\) (outside perhaps a set of zero \(v\)-measure). Let \(X_{n}\) be the likelihood ratio: \(X_{n}:=\prod_{1}^{n}(f(U_{j};\theta_{1})/f(U_{j};\theta_{0}))\), then, under \(H^{0}\), \(\{X_{n}\}_{n=1}^{\infty}\) is a \(\{\mathscr{F}_{n}\}_{n=1}^{\infty}\)_-martingale_ with \(\mathscr{F}_{n}:=\sigma\{U_{1},\cdots,U_{n}\}\). The sequential probability ratio test (SPRT) of A. Wald may be described as follows. Let \(0<A<1<B\) be two positive numbers, and let \(\tau\) be the first time \(\{X_{n}\}_{n=1}^{\infty}\) escapes from the interval \((A,B)\)

\[\tau:=\inf\left\{n\geq 1:X_{n}\leq A\quad\text{ or }\quad X_{n}\geq B\right\}. \tag{15.10}\]

Then _accept_\(H^{0}\) if \(X_{\tau}\leq A\), and _accept_\(H^{1}\) if \(X_{\tau}\geq B\). Assuming a _parameter identifiability condition_\(\nu(\{u:f(u;\theta_{0})\neq f(u;\theta_{1})\})>0\), one may check that \(\mathbb{E}^{0}\tau<\infty\) and \(\mathbb{E}^{1}\tau<\infty\), where \(P^{i}\) denotes probability, and \(\mathbb{E}^{i}\) expectation, under \(H^{i}\) (\(i=0,1\)).1 Now

Footnote 1: See Proposition 3.7 in Bhattacharya and Waymire (2007), noting that \(\log X_{n}\) is a sum of i.i.d. random variables under both \(H^{0}\) and \(H^{1}\).

\[\mathbb{E}^{i}X_{m}\mathbf{1}_{[\tau>m]}\leq BP^{i}(\tau>m)\longrightarrow 0 \quad\text{ as }\quad m\rightarrow\infty\quad(i=0,1). \tag{15.11}\]

It follows from the _Optional Stopping Theorem_2 that

Footnote 2: Theorem 3.6 in Bhattacharya and Waymire (2007).

\[1=\mathbb{E}^{0}X_{1}=\mathbb{E}^{0}X_{\tau}=\mathbb{E}^{0}X_{\tau}\mathbf{1} _{[X_{\tau}\geq B]}+\mathbb{E}^{0}X_{\tau}\mathbf{1}_{[X_{\tau}\leq A]}. \tag{15.12}\]

Now, expressing \(\mathbf{1}_{[\tau=n]}\) as a function \(g_{n}(U_{1},\dots,U_{n})\), one has

\[\mathbb{E}^{0}X_{\tau}\mathbf{1}_{[X_{\tau}\leq A]}\mathbf{1}_{[ \tau=n]} =\mathbb{E}^{0}X_{n}\mathbf{1}_{[X_{n}\leq A]}\mathbf{1}_{[\tau=n ]}=\mathbb{E}^{1}\mathbf{1}_{[X_{n}\leq A]}\mathbf{1}_{[\tau=n]}\] \[=\mathbb{E}^{1}\mathbf{1}_{[X_{\tau}\leq A]}\mathbf{1}_{[\tau=n]}. \tag{15.13}\]

The second equality in (15.13) holds as a special case of the general equality \(\mathbb{E}^{0}X_{n}f(U_{1},\cdots,U_{n})=\mathbb{E}^{1}f(U_{1},\cdots,U_{n})\) for every nonnegative (or bounded) measurable \(f\) on \(S^{n}\). Summing (15.13) over \(n=1,2,\dots,\) we get

\[1=\mathbb{E}^{0}X_{\tau}\geq BP^{0}(X_{\tau}\geq B)+P^{1}(X_{\tau}\leq A). \tag{15.14}\]Writing \(\alpha:=P^{0}(X_{\tau}\geq B)\) as the probability of accepting \(H^{1}\) when \(H^{0}\) is true, and \(\beta:=P^{1}(X_{\tau}\leq A)\) as the probability of accepting \(H^{0}\) when \(H^{1}\) is true, we get

\[B\alpha+\beta\leq 1. \tag{15.15}\]

Similarly, \(\{1/X_{n}\}_{n=1}^{\infty}\) is a \(\{\mathscr{F}_{n}\}_{n=1}^{\infty}\)-martingale under \(P^{1}\), and the same argument as above yields

\[1 =\mathbb{E}^{1}X_{\tau}^{-1}=\mathbb{E}^{1}X_{\tau}^{-1}\mathbf{1 }_{[X_{\tau}\leq A]}+\mathbb{E}^{1}X_{\tau}^{-1}\mathbf{1}_{[X_{\tau}\geq B]}\] \[\geq A^{-1}P^{1}(X_{\tau}\leq A)+P^{0}(X_{\tau}\geq B), \tag{15.16}\]

leading to the inequality

\[(A^{-1})\beta+\alpha\leq 1. \tag{15.17}\]

For small values of \(\alpha\) and \(\beta\) (i.e., for large \(B\) and small \(A\)), (15.15) and (15.17) are often treated as (approximate) equalities, and then one has

\[B\simeq\frac{1-\beta}{\alpha},\quad A\simeq\frac{\beta}{1-\alpha};\quad\alpha \simeq\frac{1-A}{B-A},\quad\beta\simeq A\left(\frac{B-1}{B-A}\right)\,. \tag{15.18}\]

This approximation is often applied, but is not always good.

It may be shown that the _SPRT_ is _optimal_. This means that in the class of all tests whose error probabilities are no more than the corresponding probabilities of the _SPRT,_ the _expected_ sample sizes \(\mathbb{E}^{i}\tau\) (\(i=0,1\)) are the smallest for the _SPRT_. See Ferguson (1967, Sect. 7.6, Theorem 2), for a detailed proof of this result. Note that the fixed sample size procedure of finding the smallest \(n\) such that the Type 1 and Type 2 errors are no more than \(\alpha\) and \(\beta\), respectively, is also a stopping rule. In order to obtain an approximate value of \(\mathbb{E}^{i}\tau\) (\(i=0,1\)) we consider the \(\{\mathscr{F}_{n}\}_{n=1}^{\infty}\)-martingale \(\{S_{n}-n\mu_{i}\}\), where

\[S_{n}:=\sum_{j=1}^{n}\left(\log f(U_{j};\theta_{1})-\log f(U_{j} ;\theta_{0})\right)=\sum_{j=1}^{n}\log\left(f(U_{j};\theta_{1})/f(U_{j};\theta _{0})\right),\] \[\qquad\qquad\qquad\qquad\mu_{i}:=\mathbb{E}^{i}\log\left(f(U_{j} ;\theta_{1})/f(U_{j};\theta_{0})\right),\quad(i=0,1). \tag{15.19}\]

Since \(x\to\log x\) is strictly _concave_ on \((0,\infty)\), it follows from Jensen's inequality that

\[\mu_{i} <\log\mathbb{E}^{i}\left(f(U_{j};\theta_{1})/f(U_{j};\theta_{0})\right)\] \[=\left\{\begin{array}{ll}0&\mbox{if}\,\,\,i=0,\\ \log\left\{\int\frac{f^{2}(u;\theta_{1})}{f(u;\theta_{0})}\nu(du)\right\}>0& \mbox{if}\,\,\,i=1,\end{array}\right. \tag{15.20}\]

since

\[\int\frac{f^{2}(u;\theta_{1})}{f(u;\theta_{0})}\,\nu(du) =\int\left(\frac{f(u;\theta_{1})}{f(u;\theta_{0})}\right)^{2}f(u; \theta_{0})\nu(du)\] \[>\left(\int\frac{f(u;\theta_{1})}{f(u;\theta_{0})}f(u;\theta_{0}) \nu(du)\right)^{2}=1. \tag{15.21}\]

The last inequality follows from the inequality \(\mathbb{E}Y^{2}\geq(\mathbb{E}Y)^{2}\), with strict inequality unless \(Y\) has all its mass at one point. To rule out the possibility \(\mu_{0}=-\infty\) and/or \(\mu_{1}=\infty\), _assume_\(\mu_{0},\mu_{1}\) to be finite.

The _SPRT_ may be expressed as

\[\text{Accept }H_{0}\text{ if }S_{\tau}\leq-a, \text{ Accept }H_{1}\text{ if }S_{\tau}\geq b, \tag{15.22}\] \[a:=-\log A, b:=\log B.\]

We have already checked that \(\mathbb{E}_{i}\tau<\infty\) (\(i=0,1\)). Further,

\[\big{|}\mathbb{E}^{i}\left(S_{m}\mathbf{1}_{[\tau>m]}\right)\big{|}\leq\max\{a,b\}P^{i}(\tau>m)\longrightarrow 0\quad\text{ as }\ m\rightarrow\infty. \tag{15.23}\]

Therefore, it follows from Wald's identity,3 applied to the sequence \(\{S_{n}-n\mu_{i}\}\), that

Footnote 3: See (Bhattacharya and Waymire, 2007, p. 47).

\[\mathbb{E}^{i}S_{\tau}=\mu_{i}\mathbb{E}^{i}\tau\qquad(i=0,1). \tag{15.24}\]

Hence

\[\mathbb{E}^{i}\tau=\frac{\mathbb{E}^{i}S_{\tau}}{\mu_{i}}\qquad(i=0,1). \tag{15.25}\]

Again one may 'approximately' calculate \(E^{i}S_{\tau}\) as follows:

\[\mathbb{E}^{i}S_{\tau} \simeq bP^{i}(S_{\tau}\geq b)-aP^{i}(S_{\tau}\leq-a)\] \[=bP^{i}(X_{\tau}\geq B)-aP^{i}(X_{\tau}\leq A),\] \[\mathbb{E}^{0}S_{\tau} \simeq b\alpha-a(1-\alpha),\quad\mathbb{E}^{1}S_{\tau}\simeq b(1- \beta)-a\beta. \tag{15.26}\]

The values of \(a,b,\alpha,\beta\) are then substituted from (15.18), (15.22).

_Remark 15.3_.: Let \(A<1<B\). The approximations \(\alpha^{\prime}=\frac{1-A}{B-A}\), \(\beta^{\prime}=A\left(\frac{B-1}{B-A}\right)\) of \(\alpha\), \(\beta\) are, in most applications conservative, i.e., \(\alpha^{\prime}<\alpha\), \(\beta^{\prime}<\beta\). In general, one can show that \(\alpha^{\prime}+\beta^{\prime}\leq\alpha+\beta\) (See Rao, 1965, pp. 401, 402).

_Remark 15.4_.: Sequential procedures for estimation may be derived using the duality between tests and confidence regions (See Sect. 5.9).

_Remark 15.5_.: When the density \(f(\,;\theta)\) has a monotone likelihood ratio the SPRT is effective under composite hypotheses \(H^{0}:\theta\leq\theta_{0}\), \(H^{1}:\theta>\theta_{1}\) (\(\theta_{0}\leq\theta_{1}\)) (See Remarks 5.2-5.4 in Chap. 5). See (Siegmund, 1985, pp. 14-19), for a detailed discussion.

Sequential procedures such as the SPRT are especially important in clinical trials with patients for drug testing, and for those cases where sampling is extremely expensive as in the case of car crash safety experiments and in other destructive testing. In such cases the optimality criterion should involve the cost \(c\) per item as well as the Type 1, Type 2 errors. An appropriate objective function to minimize in such cases is the _Bayes risk_\(\delta\{\text{Prob}(\text{Reject }H_{0}\mid\theta_{0})+cE(\tau\mid\theta_{0})\}+(1- \delta)\{\text{Prob}(\text{Reject }H_{1}\mid\theta_{1})+cE(\tau\mid\theta_{1})\}\) (\(0<\delta<1\)). A sequential probability ratio test with appropriate boundary points \(A\), \(B\), minimizes the Bayes risk. See (Lehmann, 1959, pp. 104-110). Both the SPRT minimizing expected sample size for given levels of \(\alpha\), \(\beta\) and the SPRT minimizing appropriate Bayes risks yield substantial reduction in sample sizes and costs.

### Notes and References

Our main sources for Sect. 15.1 on classification are an exposition in Wasserman (2003, Chap. 22), and the book by Hastie et al. (2001). For a theoretical overview of Bayesian nonparametrics we refer to Ghosal (2010). The original basic theory of nonparametric Bayes inference is due to Ferguson (1973, 1974). A useful construction of Ferguson's Dirichlet priors is by the so-called _stick breaking_ of Sethuraman (1994). For extensions of nonparametric Bayes theory to manifolds, see Bhattacharya and Dunson (2010, 2012), and Bhattacharya and Bhattacharya (2012, Chaps. 13, 14).

For applications of principal components analysis (Sect. 15.2) to image analysis, see Hastie et al. (2001, Chap. 4), and Bishop (2006, Chap. 4).

Standard texts on sequential analysis (Sect. 15.3) include Wald (1947), Siegmund (1992), and Chernoff (1972). The last two books also provide continuous time versions of the SPRT and Brownian motion approximations of the discrete time SPRT considered here. Nonparametric sequential analysis is considered in the monograph by Sen (1981).

### Exercises for Chap. 15

**Exercises for Sect. 15.1**

**Ex. 15.1.**: (a) Check (15.2) and (15.3).

(b) Refer to the data on skulls in Chap. 8, Exercise 8.13. Assume that the distributions are five-dimensional Normal \(N(\mu_{A},\Sigma_{A})\), \(N(\mu_{B},\Sigma_{B})\). Use the first 15 \(A\) skulls and the first 13 \(B\) skulls to estimate the unknown parameters (with \(\pi_{A}=15/28\), \(\pi_{B}=13/28\)). Then apply the quadratic classifier (15.2) to classify the remaining 4 skulls.

(c) Carry out the classification in (b) assuming \(\Sigma_{A}=\Sigma_{B}\) and using the linear classifier (15.3).

**Ex. 15.2.**: Compute the Bayes classifier (15.1) for \(K\) Bernoulli populations with parameters \(p_{1},p_{2},\ldots,p_{K}\).

**Exercises for Sect. 15.2**

**Ex. 15.3.**: Refer to the data on skulls in Chap. 8, Exercise 8.13.

(a) Assume \(\Sigma_{A}=\Sigma_{B}\). From the pooled estimate of the common covariance matrix calculate the eigenvalues and the principal components.

(b) For each of the five principal components in (a) carry out the two-sample \(t\)-test for the equality of the means in the population, and list the \(p\)-values.

**Exercises for Sect. 15.3**

**Ex. 15.4.**: Let \(U_{j}\)'s be i.i.d. Bernoulli taking values \(+1\) and \(-1\) with probabilities \(p\) and \(q=1-p\), respectively.

(a) Compute the sequential probability ratio test for \(H_{0}:p=\frac{1}{2}\) against \(H_{1}:p=2/3\), with the nominal probabilities of error \(\alpha=.1\), \(\beta=.2\). [i.e., with \(A\) and \(B\) given by equalities in (15.18)].

2. Calculate approximate values of \(\mathbb{E}^{i}\tau\) (\(i=0,1\)) using (15.25), (15.26).
3. Given successive sample observations \(-1,1,1,-1,-1,1,1,1,-1,1,1,-1,1,\)\(-1,1,1,1,1,1,\) decide when to stop and what action to take.

## References

* Bhattacharya & Bhattacharya (2012) Bhattacharya, A., & Bhattacharya, R. (2012). _Nonparametric inference on manifolds: With applications to shape spaces_. IMS monograph (Vol. 2). Cambridge: Cambridge University Press.
* Bhattacharya & Dunson (2010) Bhattacharya, A., & Dunson, D. B. (2010). Nonparametric Bayesian density estimation on manifolds with applications to planar shapes. _Biometrika, 97_, 851-865.
* Bhattacharya & Dunson (2012) Bhattacharya, A., & Dunson, D. B. (2012). Nonparametric Bayes classification and hypothesis testing on manifolds. _Journal of Multivariate Analysis, 111_, 1-19.
* Bhattacharya & Waymire (2007) Bhattacharya, A., & Waymire, E. C. (2007). _A Basic course in probability theory_. New York, Springer.
* Bishop (2006) Bishop, C. (2006). Principal component analysis. In _Pattern recognition and machine learning (information science and statistics)_ (pp. 561-570). New York: Springer.
* Chernoff (1972) Chernoff, H. (1972). Sequential analysis and optimal design. _Regional conferences in applied mathematics_ (Vol. 8). Philadelphia: SIAM.
* Ferguson (1967) Ferguson, T. S. (1967). _Mathematical statistics: A decision theoretic approach_. New York: Academic Press.
* Ferguson (1973) Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. _Annals of Statistics, 1_(2), 209-230.
* Ferguson (1974) Ferguson, T. S. (1974). Prior distributions on spaces of probability measures. _Annals of Statistics, 4_(4), 615-629.
* Ghosal (2010) Ghosal, S. (2010). Dirichlet process, related priors and posterior asymptotics. In N. L. Hjort et al. (Eds.), _Bayesian nonparametrics_ (pp. 36-83). Cambridge: Cambridge University Press.
* Ghosh & Ramamoorthi (2002) Ghosh, J. K., & Ramamoorthi, R. V. (2002). _Bayesian nonparametrics_. New York: Springer.
* Hastie et al. (2001) Hastie, T., Tibshirani, R., & Friedman, J. H., (2001). _The elements of statistical learning_. New York: Springer.
* Hjort et al. (2010) Hjort, N. L., Holmes, C., Muller, P., & Walker, S. G., (Eds.). (2010). _Bayesian nonparametrics_. Cambridge: Cambridge University Press.
* Lehmann (1959) Lehmann, E. L. (1959). _Testing statistical hypothesis_. New York: Wiley.
* Rao (1965) Rao, C. R. (1965). _Linear statistical inference and its applications_. New York: Wiley.
* Sen (1981) Sen, P. K. (1981). _Sequential nonparametrics_. New York: Wiley.
* Sethuraman (1994) Sethuraman, J. (1994). A constructive definition of Dirichlet priors. _Statistica Sinica, 4_, 639-650.
* Siegmund (1985) Siegmund, D. (1985). _Sequential analysis: Tests and confidence intervals_. New York: Springer.
* Siegmund (1992) Siegmund, D. (1992). _Sequential analysis: Tests and confidence intervals_. New York: Springer.
* Vapnik (1998) Vapnik, V. N. (1998). _Statistical learning theory_. New York: Springer.
* Wald (1947) Wald, A. (1947). _Sequential analysis_. New York: Wiley.
* Wasserman (2003) Wasserman, L. (2003). _All of statistics: A concise course in statistical inference_. New York: Springer.

## Appendix A Standard Distributions

### A.1 Standard Univariate Discrete Distributions

_I. Binomial Distribution_ \(\mathscr{B}(n,p)\) has the probability mass function

\[f(x)=\binom{n}{x}p^{x}\qquad(x=0,1,\ldots,n).\]

The mean is \(np\) and variance \(np(1-p)\).

_II. The Negative Binomial Distribution_\(\mathscr{N}\mathscr{B}(r,p)\) arises as the distribution of \(X\equiv\{\)number of failures until the \(r\)-th success\(\}\) in independent trials each with probability of success \(p\). Thus its probability mass function is

\[f_{r}(x)=\binom{r+x-1}{x}p^{r}(1-p)^{x}\qquad(x=0,1,\cdots,\cdots).\]

Let \(X_{i}\) denote the number of failures between the \((i-1)\)-th and \(i\)-th successes (\(i=2,3,\ldots,r\)), and let \(X_{1}\) be the number of failures before the first success. Then \(X_{1},X_{2},\ldots,X_{r}\) and \(r\) independent random variables each having the distribution \(\mathscr{N}\mathscr{B}(1,p)\) with probability mass function

\[f_{1}(x)=p(1-p)^{x}\qquad(x=0,1,2,\ldots).\]

Also,

\[X=X_{1}+X_{2}+\cdots+X_{r}.\]

Hence

\[E(X) =rEX_{1}=r\left(p\sum_{x=0}^{\infty}x(1-p)^{x}\right)=rp(1-p)\sum _{x=1}^{\infty}x(1-p)^{x-1}\] \[=rp(1-p)\sum_{x=1}^{\infty}\left(-\frac{d}{dp}(1-p)^{x}\right)=- rp(1-p)\frac{d}{dp}\sum_{x=1}^{\infty}(1-p)^{x}\]\[=-rp(1-p)\frac{d}{dp}\sum_{x=0}^{\infty}(1-p)^{x}=-rp(1-p)\frac{d}{dp} \underbrace{\frac{1}{1-(1-p)}}_{=p}\] \[=\frac{r(1-p)}{p}\,.\] (A.1)

Also, one may calculate \(\operatorname{var}(X)\) using (Exercise A.1)

\[\operatorname{var}(X)=r\operatorname{var}(X_{1}).\] (A.2)

_III. The Poisson Distribution_\(\mathscr{P}(\lambda)\) has probability mass function

\[f(x)=e^{-\lambda}\frac{\lambda^{x}}{x!}\qquad(x=0,1,2,\dots),\]

where \(\lambda>0\) is the mean. To see this let \(X\) be a random variable with this distribution. Then

\[E(X) =\sum_{x=0}^{\infty}xe^{-\lambda}\frac{\lambda^{x}}{x!}=e^{- \lambda}\sum_{x=1}^{\infty}\frac{\lambda^{x}}{(x-1)!}=\lambda e^{-\lambda}\sum _{y=0}^{\infty}\frac{\lambda^{y}}{y!}\] \[=\lambda e^{-\lambda}\cdot e^{\lambda}=\lambda.\qquad\qquad(y=x-1)\]

Also,

\[E(X(X-1)) =\sum_{x=0}^{\infty}x(x-1)e^{-\lambda}\frac{\lambda^{x}}{x!}=e^{- \lambda}\sum_{x=2}^{\infty}x(x-1)\frac{\lambda^{x}}{x!}\] \[=e^{-\lambda}\lambda^{2}\sum_{x=2}^{\infty}\frac{\lambda^{x-2}}{( x-2)!}=e^{-\lambda}\lambda^{2}\cdot e^{\lambda}=\lambda^{2},\]

so that

\[E(X^{2}) =\lambda^{2}+E(X)=\lambda^{2}+\lambda,\] \[\operatorname{var}(X) =E(X^{2})-\left(E(X)\right)^{2}=\lambda^{2}+\lambda-\lambda^{2}=\lambda.\] (A.3)

_IV. The Beta-Binomial Distribution_\(\mathscr{B}\mathscr{B}(\alpha,\beta,n)\) is the marginal distribution of \(X\) when the conditional distribution of \(X\) given (another random variable) \(Y=y\) (with values in \([0,1]\)) is \(\mathscr{B}(n,y)\), where \(Y\) has the beta distribution \(\mathscr{B}_{e}(\alpha,\beta)\) (see Sect. A.2). Hence the probability mass function of \(X\) is

\[f(x) =\int_{0}^{1}\binom{n}{x}y^{x}(1-y)^{n-x}\frac{1}{B(\alpha,\beta) }\,y^{\alpha-1}(1-y)^{\beta-1}dy\] \[=\frac{\binom{n}{x}}{B(\alpha,\beta)}\int_{0}^{1}y^{x+\alpha-1}(1 -y)^{n-x+\beta-1}dy=\frac{\binom{n}{x}B(x+\alpha,n-x+\beta)}{B(\alpha,\beta)}\] \[=\binom{n}{x}\frac{\Gamma(x+\alpha)\Gamma(n-x+\beta)\Gamma(\alpha +\beta)}{\Gamma(n+\alpha+\beta)\Gamma(\alpha)\Gamma(\beta)}\qquad(x=0,1,\dots,n)\][See below for the relation \(B(\alpha,\beta)=\Gamma(\alpha)\Gamma(\beta)/\Gamma(\alpha+\beta)\).] Here if \(X\sim\mathscr{B}\mathscr{B}(\alpha,\beta,n)\),

\[E(X) = EE(X\mid Y)=EnY=nEY=\frac{n\alpha}{\alpha+\beta}\,,\] \[E(X^{2}) = EE(X^{2}\mid Y)=E[nY(1-Y)+n^{2}Y^{2}]\] \[= (n^{2}-1)EY^{2}+nEY=(n^{2}-1)\frac{\alpha(\alpha+1)}{(\alpha+ \beta)(\alpha+\beta+1)}+n\,\frac{\alpha}{\alpha+\beta},\] \[\text{var}\,(X) = E(X^{2})-(E(X))^{2}.\]

(See below for the computation of the moments of the beta distribution.)

### Some Absolutely Continuous Distributions

_I. The Uniform Distribution_ \(\mathscr{U}(\alpha,\beta)\) on \([\alpha,\beta]\) has the probability density function (p.d.f.)

\[f(x) = \frac{1}{\beta-\alpha}\quad\text{for $\alpha\leq x\leq\beta$},\] \[= 0\qquad\text{elsewhere.}\]

_II. The Beta Distribution_\(\mathscr{B}_{e}(\alpha,\beta)\) has p.d.f.

\[f(x) = \frac{1}{B(\alpha,\beta)}\,x^{\alpha-1}(1-x)^{\beta-1},\qquad 0<x <1,\] \[= 0\qquad\text{elsewhere.}\]

Here \(\alpha>0\), \(\beta>0\) and \(B(\alpha,\beta)\) is the normalizing constant _(beta function)_

\[B(\alpha,\beta)=\int_{0}^{1}x^{\alpha-1}(1-x)^{\beta-1}dx.\]

Clearly, if \(X\sim\mathscr{B}_{e}(\alpha,\beta)\), then

\[E(X) = \frac{1}{B(\alpha,\beta)}\int_{0}^{1}x^{\alpha}(1-x)^{\beta-1}dx= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)}\,,\] \[E(X^{2}) = \frac{B(\alpha+2,\beta)}{B(\alpha,\beta)}\,,\ldots,\,E(X^{k})= \frac{B(\alpha+k,\beta)}{B(\alpha,\beta)}\.\] (A.4)

Recall that the _gamma function_\(\Gamma(\alpha)\) is defined by

\[Gamma(\alpha)=\int_{0}^{\infty}e^{-x}x^{\alpha-1}dx\qquad(\alpha>0).\]

On integration by parts one gets

\[\Gamma(\alpha+1)=\int_{0}^{\infty}e^{-x}x^{\alpha}dx=-e^{-x}x^{\alpha}\Big{|} _{0}^{\infty}+\int_{0}^{\infty}\alpha x^{\alpha-1}e^{-x}dx=0+\alpha\Gamma( \alpha)=\alpha\Gamma(\alpha).\] (A.5)We now prove

\[B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}\,,\qquad \forall\ \alpha>0,\ \beta>0.\] (A.6)

Now

\[\Gamma(\alpha)\Gamma(\beta) =\int_{0}^{\infty}e^{-x}x^{\alpha-1}dx\int_{0}^{\infty}e^{-y}y^{ \beta-1}dy\] \[=\int_{0}^{\infty}\int_{0}^{\infty}e^{-(x+y)}x^{\alpha-1}y^{\beta -1}dxdy.\]

Change variables: \(z=x+y\), \(y=y\), to get \(\left\{\begin{array}{l}x=z-y\\ y=y\end{array}\right.\), with

\[\text{Jacobian =}\Bigg{|}\det\begin{bmatrix}\frac{\partial x}{\partial t}& \frac{\partial x}{\partial y}\\ \frac{\partial y}{\partial z}&\frac{\partial y}{\partial y}\end{bmatrix} \Bigg{|}=1\]

\[\Gamma(\alpha)\Gamma(\beta) =\int_{0}^{\infty}e^{-z}\left(\int_{0}^{z}(z-y)^{\alpha-1}y^{ \beta-1}dy\right)dz\] \[=\int_{0}^{\infty}e^{-z}z^{\alpha-1}z^{\beta-1}\left(\int_{0}^{z }\left(1-\frac{y}{z}\right)^{\alpha-1}\left(\frac{y}{z}\right)^{\beta-1}dy \right)dz\] \[=\int_{0}^{\infty}e^{-z}z^{\alpha+\beta-2}\left(z\int_{0}^{1}(1-u )^{\alpha-1}u^{\beta-1}du\right)dz\quad\left[u=\frac{y}{z},\,du=\frac{1}{z}\, dy\right]\] \[=\int_{0}^{\infty}e^{-z}z^{\alpha+\beta-1}B(\beta,\alpha)dz= \Gamma(\alpha+\beta)B(\beta,\alpha).\] (A.7)

But

\[B(\beta,\alpha) =\int_{0}^{1}u^{\beta-1}(1-u)^{\alpha-1}du=\int_{0}^{1}x^{\alpha- 1}(1-x)^{\beta-1}dx\] \[=B(\alpha,\beta),\qquad(x=1-u).\] (A.8)

Hence (A.7) and (A.6). Using (A.4)-(A.6), one gets the \(k\)-th moment of a beta (\(\mathscr{B}_{e}(\alpha,\beta)\)) random variable \(X\) as

\[E(X^{k})=\frac{\Gamma(\alpha+k)\Gamma(\beta)\cdot\Gamma(\alpha+\beta)}{\Gamma( \alpha+\beta+k)\cdot\Gamma(\alpha)\Gamma(\beta)}=\frac{(\alpha+k-1)\cdots( \alpha+1)\alpha}{(\alpha+\beta+k-1)\cdots(\alpha+\beta+1)(\alpha+\beta)}\.\]

In particular,

\[E(X)\text{=}\frac{\alpha}{\alpha+\beta}\,,\quad E(X^{2})=\frac{(\alpha+1) \alpha}{(\alpha+\beta+1)(\alpha+\beta)}\,\quad\text{var}(X)\text{=}\frac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}\.\]

_III. The Gamma Distribution \(\mathscr{G}(\alpha,\beta)\)_ has the p.d.f.

\[f(x) =\frac{1}{\Gamma(\beta)\alpha^{\beta}}\ e^{-x/\alpha}x^{\beta-1},\qquad 0<x<\infty\] \[=0\quad\text{elsewhere,}\] (A.9)where \(\alpha>0,\,\beta>0.\) Here \(\alpha\) is a _scale parameter,_ i.e., if \(X\) is \(\sim\mathscr{G}(\alpha,\beta),\) then \(X/\alpha\) is \(\mathscr{G}(1,\beta),\) Note that

\[E\left(\frac{X}{\alpha}\right)^{k} =\frac{1}{\Gamma(\beta)}\int_{0}^{\infty}z^{k}e^{-z}z^{\beta-1}dz\] \[=\frac{\Gamma(\beta+k)}{\Gamma(\beta)}=(\beta+k-1)\cdots(\beta+1 )\beta,\]

so that

\[E(X^{k})=\alpha^{k}(\beta+k-1)\cdots(\beta+1)\beta. \tag{10}\]

Hence \(EX=\alpha\beta,\)\(\text{var}(X)=\alpha^{2}\beta.\)

#### The Normal Distribution \(\mathbf{N}(\mu,\sigma^{2})\)

has p.d.f.

\[f_{\mu,\sigma^{2}}(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\,e^{-(x-\mu)^{2}/2 \sigma^{2}},\qquad-\infty<x<\infty, \tag{11}\]

where \(\mu\in(-\infty,\infty),\)\(\sigma^{2}>0.\) The _standard normal distribution_ \(\mathbf{N}(0,1)\) has p.d.f.

\[f(x)=\frac{1}{\sqrt{2\pi}}\,\,e^{-x^{2}/2},\qquad-\infty<x<\infty. \tag{12}\]

To show that (12) (and hence (11), by transformation \(y=\frac{x-\mu}{\sigma}\)) is a p.d.f., one needs to show that

\[\int_{0}^{\infty}e^{-x^{2}/2}dx=\sqrt{\frac{\pi}{2}} \tag{13}\]

For this use the transformation \(z=x^{2}/2\) to get

\[\int_{0}^{\infty}e^{-x^{2}/2}dx=\int_{0}^{\infty}e^{-z}\sqrt{2}\,\left(\frac{1} {2}\right)z^{-\frac{1}{2}}dz=\frac{1}{\sqrt{2}}\,\Gamma\left(\frac{1}{2}\right). \tag{14}\]

Now, by (7) (with \(\alpha=\beta=\frac{1}{2}\))

\[\Gamma^{2}\left(\frac{1}{2}\right) =\Gamma(1)B\left(\frac{1}{2}\,,\,\frac{1}{2}\right)\!=\!B\left( \frac{1}{2}\,,\,\frac{1}{2}\right)\quad\text{(since $\Gamma(1)\!=\!\int_{0}^{ \infty}e^{-x}dx$=\!-e^{-x}|_{0}^{\infty}=1,$)}\] \[=\int_{0}^{1}x^{-1/2}(1-x)^{-1/2}dx=\int_{0}^{1}z^{-1}(1-z^{2})^{ -1/2}2zdz\quad(z=x^{1/2},\,\,dx=2zdz)\] \[=2\int_{0}^{1}(1-z^{2})^{-1/2}dz=2\int_{0}^{\pi/2}\frac{\cos \theta d\theta}{\cos\theta}\quad(z=\sin\theta,\,\,dz=\cos\theta d\theta)\] \[=2\left(\frac{\pi}{2}\right)=\pi.\]

Hence

\[\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}\,, \tag{15}\]

which when used in (14) yields (13).

If \(X\) is \({\bf N}(\mu,\sigma^{2})\), then \(Z=\frac{X-\mu}{\sigma}\) has the p.d.f. (A.12) (by change of variables). Therefore,

\[\begin{array}{ll}E\left(\frac{X-\mu}{\sigma}\right)=&E(Z)=\frac{1}{\sqrt{2\pi }}\int_{-\infty}^{\infty}xe^{-x^{2}/2}dx=0\quad\mbox{(since $xe^{-x^{2}/2}$ is $odd)},\\ E\left(\frac{X-\mu}{\sigma}\right)^{2}=&E(Z^{2})=\frac{1}{\sqrt{2\pi}}\int_{- \infty}^{\infty}x^{2}e^{-x^{2}/2}dx=\frac{2}{\sqrt{2\pi}}\int_{0}^{\infty}x^{2} e^{-x^{2}/2}dx\\ &=\sqrt{\frac{2}{\pi}}\int_{0}^{\infty}(2z)^{\frac{1}{2}}e^{-z}dz \qquad(z=\frac{z^{2}}{2}\,,\quad dz=xdx)\\ &=\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}e^{-z}z^{1/2}dz=\frac{2}{\sqrt{\pi}}\, \Gamma\left(\frac{3}{2}\right)=\frac{2}{\sqrt{\pi}}\left(\frac{1}{2}\right) \Gamma\left(\frac{1}{2}\right)=\frac{2}{\sqrt{\pi}}\left(\frac{1}{2}\right) \sqrt{\pi}\\ &=1.\end{array}\]

Hence

\[\begin{array}{ll}E(X-\mu)=0,\mbox{ or }&E(X)=\mu,\\ E\left(\left(\frac{X-\mu}{\sigma}\right)^{2}\right)=1,\mbox{ or, var}(X)=\sigma^{2}.\end{array}\] (A.16)

_V. The Chi-Square Distribution \(\chi_{k}^{2}\,\)_ with \(k\) Degrees of Freedom is defined to be the distribution of the _sum of squares of \(k\) independent standard normal random variables._ To derive its p.d.f. let \(X_{1},X_{2},\ldots,X_{k}\) be \(k\) independent \({\bf N}(0,1)\) random variables. Then define the chi-square random variable

\[Z=X_{1}^{2}+X_{2}^{2}+\cdots+X_{k}^{2},\]

and note that, as \(\Delta z\downarrow 0\),

\[\begin{array}{ll}P(z<Z\leq z+\Delta z)=\int\cdots\int\left(\frac{1}{\sqrt{2 \pi}}\right)^{k}e^{-(x_{1}^{2}+\cdots+x_{k}^{2})/2}dx_{1}\cdots dx_{k}\\ \{(x_{1},\ldots,x_{k}):z<\sum_{1}^{k}x_{i}^{2}\leq z+\Delta z\}\\ =\left(\frac{1}{\sqrt{2\pi}}\right)^{k}\left(e^{-z/2}+O(\Delta z)\right)\\ \times\mbox{volume of the annulus }\{(x_{1},\ldots,x_{k}):z<\sum x_{i}^{2}<z+ \Delta z\}\\ =\left(\frac{1}{\sqrt{2\pi}}\right)^{k}\left(e^{-z/2}+O(\Delta z)\right) \left(c_{k}\left(\sqrt{z+\Delta z}\right)^{k}-c_{k}\left(\sqrt{z}\right)^{k} \right),\end{array}\]

writing the volume of a ball of radius \(r\) in \(k\)-dimension as \(c_{k}r^{k}\). [Note that \(\int_{\{\sum x_{i}^{2}\leq r\}}\cdots\,\int dx_{1}\ldots dx_{k}=r^{k}\int_{\{ \sum x_{i}^{2}\leq 1\}}\cdots\int dx_{1}\ldots dx_{k}\), by change of variables \(z_{i}=x_{i}/r\) (\(1\leq i\leq k\)).] Since \((\sqrt{z+\Delta z}\,)^{k}-(\sqrt{z}\,)^{k}=\frac{d}{dz}(\sqrt{z}\,)^{k}\cdot \Delta z+O(\Delta z)^{2}=\frac{k}{2}\,z^{\frac{k}{2}-1}\Delta z+(\Delta z)^{2}\), one has

\[P(z<Z\leq z+\Delta z)=c_{k}\left(\frac{1}{\sqrt{2\pi}}\right)^{k}\frac{k}{2} \,e^{-\frac{z}{2}z^{\frac{k}{2}-1}}\Delta z+O(\Delta z)^{2}.\] (A.17)Hence the p.d.f. of \(Z\) is

\[f(z) = c^{\prime}_{k}z^{\frac{k}{2}-1}e^{-\frac{z}{2}},\quad 0<z<\infty,\] (A.18) \[= 0\qquad\text{elsewhere}\]

where \(c^{\prime}_{k}\) is the normalized constant,

\[c^{\prime}_{k} = \left(\int_{0}^{\infty}z^{\frac{k}{2}-1}e^{-\frac{z}{2}}dz\right) ^{-1}=\left(\int_{0}^{\infty}u^{\frac{k}{2}-1}e^{-u}du\left(2^{\frac{k}{2}} \right)\right)^{-1}\quad\left(u=\frac{z}{2}\right)\] (A.19) \[= \frac{1}{2^{k/2}\Gamma\left(\frac{k}{2}\right)}\,\,.\]

Since \(c^{\prime}_{k}\) may be directly calculated from (A.17), one has

\[c^{\prime}_{k}=c_{k}\left(\frac{1}{\sqrt{2\pi}}\right)^{k}\frac{k}{2}\,.\] (A.20)

Comparing (A.19), (A.20) the constant \(c_{k}\) (= volume of the unit ball in \(\mathbb{R}^{k}\)) may also be obtained (\(c_{k}=\frac{2\pi^{k/2}}{k\Gamma(k/2)}\)). Note also that \(\chi^{2}_{k}\)_is_\(\mathscr{G}(2,k/2)\) and hence \(Z/2\sim\mathscr{G}(1,\frac{k}{2})\). In particular, if \(X\sim\mathbf{N}(0,1)\), \(X^{2}\sim\chi^{2}_{1}\), \(\frac{X^{2}}{2}\sim\mathscr{G}(1,\frac{1}{2})\). This also shows that the sum of independent chi-square random variables \(Z_{1},Z_{2},\ldots,Z_{m}\) with degrees of freedom \(k_{1},k_{2},\ldots,k_{m}\), respectively, is a chi-square random variable with degrees of freedom \(k=k_{1}+\cdots+k_{m}\) (Exercise A.2). Also, if \(Z\sim\chi^{2}_{k}\) then using (A.10) [with \(\alpha=1\), \(k=1\), \(\beta=k/2\) or the fact that \(EX^{2}=1\) if \(X\sim\mathbf{N}(0,1)\)],

\[EZ=2E\frac{Z}{2}=2\frac{k}{2}=k.\]

_VI. The Student's \(t\)-Distribution \(t_{k}\) with \(k\) Degrees of Freedom_ is defined to be the distribution of \(T=X/\sqrt{Z/k}\), where \(X\sim\mathbf{N}(0,1)\), \(Z\sim\chi^{2}_{k}\), and \(X\) and \(Z\) are _independent._ The (cumulative) distribution function of \(T\) is given by

\[P(T\leq t) = P\left(X\leq t\sqrt{Z/k}\right)=EP\left(X\leq t\sqrt{Z/k}\bigg{|} Z\right)\] \[= \int_{0}^{\infty}P\left(X\leq\frac{t}{\sqrt{k}}\,\,\,\sqrt{z} \bigg{|}Z=z\right)\cdot\frac{1}{2^{k/2}\Gamma(k/2)}\,z^{k/2-1}e^{-z/2}dz\] \[= \frac{1}{2^{k/2}\Gamma(k/2)}\int_{0}^{\infty}\left[\int^{\sqrt{z /k}\,\,t}\frac{1}{\sqrt{2\pi}}\,e^{x^{2}/2}dx\right]z^{k/2-1}e^{-z/2}dz\]

Differentiating w.r.t. \(t\) under the integral sign one gets the p.d.f. of \(T\) as

\[f(t) = \frac{1}{2^{k/2}\Gamma\left(\frac{k}{2}\right)}\int_{0}^{\infty} \left(\frac{1}{\sqrt{2\pi}}\,e^{-\frac{z^{2}}{w\kappa}}z^{\frac{k}{2}-1}e^{- \frac{z}{2}}\right)\sqrt{\frac{z}{k}}\,dz\] \[= \frac{1}{\sqrt{k}\,2^{\frac{k+1}{2}}\sqrt{\pi}\Gamma\left(\frac{k }{2}\right)}\int_{0}^{\infty}e^{-\frac{z}{2}\left(1+\frac{z^{2}}{\kappa} \right)}z^{\frac{k}{2}-\frac{1}{2}}dz\]

[MISSING_PAGE_FAIL:349]

IX. Logistic Distribution

The distribution function of a _standard logistic random variable_\(X\) is given by

\[F(x)=\frac{e^{x}}{1+e^{x}}\,,\quad-\infty<x<\infty,\]

with density

\[f(x)=\frac{e^{x}}{(1+e^{x})^{2}}\,,\quad-\infty<x<\infty.\]

It is easy to check that the density is symmetric about \(x=0\), i.e., \(f(x)=f(-x)\) for all \(x\). Hence all odd order moments are zero. The even order moments are given as follows.1 First note that, for \(y>0\),

Footnote 1: See Balakrishnan, N. and Nevzorov, V.B. (2003). _A Primer on Statistical Distributions_, Chap. 22. Wiley, New York.

\[(1+y)^{-2}=\sum_{n=0}^{\infty}{-2\choose n}y^{n}=\sum_{n=0}^{\infty}(-1)^{n}(n +1)^{y^{n}}.\]

Using this one obtains

\[EX^{2m} = 2\int_{[0,\infty)}x^{2m}\frac{e^{-x}}{(1+e^{-x})^{2}}\,dx\] \[= 2\sum_{n=0}^{\infty}(-1)^{n}(n+1)\int_{[0,\infty)}x^{2m}e^{-(n+1 )x}dx\] \[= 2\Gamma(2m+1)\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(n+1)^{2m}}\] \[= 2(2m)!\sum_{n=1}^{\infty}\frac{(-1)^{n-1}}{n^{2m}}=2(2m)!\left\{ \sum_{r=1}^{\infty}\frac{(-1)^{2r-2}}{(2r-1)^{2m}}+\sum_{r=1}^{\infty}\frac{(- 1)^{2r-1}}{(2r)^{2m}}\right\}\] \[= 2(2m)!\left\{\sum_{r=1}^{\infty}(2r-1)^{-2m}-\sum_{r=1}^{\infty} (2\gamma)^{-2m}\right\}\] \[= 2(2m)!\left\{\sum_{r=1}^{\infty}\gamma^{-2m}-2\sum_{r=1}^{\infty }(2\gamma)^{-2m}\right\}=2(2m)!\left(1-2^{-(2m-1)}\right)\sum_{r=1}^{\infty} \gamma^{-2m}\] \[= 2(2m)\left(1-2^{-(2m-1)}\right)\zeta(2m),\]

where \(\zeta(r)=\sum_{r=1}^{\infty}r^{-2}\) is the _Riemann zeta function._ In particular, \(EX^{2}=\operatorname{var}(X)=2\zeta(2)=\pi^{2}/3\), since2\(\sum_{r=1}^{\infty}r^{-2}=\pi^{2}/6\).

Footnote 2: See Titchmarsh, E.C. (1939). _The Theory of Functions_, 2nd ed., p. 35. Oxford University Press, London.

Next, if \(X\) has the standard logistic distribution, then for any real \(\mu\) and \(h>0\), the pdf of \(Z=hX+\mu\) is

\[f_{Z}(z)=hf_{X}\left(\frac{z-\mu}{h}\right)=h^{-1}\frac{\exp\left\{\frac{z- \mu}{h}\right\}}{\left(1+\exp\left\{\frac{z-\mu}{h}\right\}\right)^{2}}\,\quad- \infty<z<\infty.\]The mean of \(Z\) is \(\mu\) and its variance is \(h^{2}\,\pi^{2}/3\). The logistic regression in its simplest form used in the student projects in Chaps. 4 and 14, has a binary response variable, say \(Y\), with values \(0\), \(1\), and a continuous regressor \(Z\). When \(Z=z\), then \(Y=1\) with probability \(F_{Z}(z)=\operatorname{Prob}(Z\leq z)\) and \(Y=0\) with probability \(1-F_{Z}(z)\). One may also apply this in the case of a non-stochastic \(Z\) using the same relations formally. For many applications other than logistic regression see, e.g., Balakrishnan and Nevzorov (2003).

### The Multivariate Normal Distribution

(Notation: \(\langle x,y\rangle=x^{\prime}y=\sum_{j=1}^{k}x^{(j)}y^{(j)}\forall x,y\in \mathbb{R}^{k}\).)

**Definition A.1**.: A random vector \(\mathbf{X}=(X^{(1)},X^{(2)},\ldots,X^{(k)})^{\prime}\) with values in \(\mathbb{R}^{k}\) has the \(k\)_-dimensional normal distribution_\(\mathbf{N}(\boldsymbol{\mu},\,\Sigma)\) with _mean_ (vector) \(\boldsymbol{\mu}=(\mu^{(1)},\)\(\mu^{(2)},\)\(\ldots,\)\(\mu^{(k)})^{\prime}\) and _nonsingular dispersion matrix_\(\Sigma\) if it has the probability density function

\[f(\mathbf{x}) =\frac{1}{(2\pi)^{k/2}(\det\,\Sigma)^{1/2}}\,e^{-\frac{1}{2} \langle\mathbf{x}-\boldsymbol{\mu},\,\Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})\rangle}\] \[=\frac{1}{(2\pi)^{k/2}(\det\Sigma)^{1/2}}\,e^{-\frac{1}{2}\Sigma \sum_{i,j=1}^{k}\sigma^{ij}(x^{(i)}-\mu^{(i)})(x^{(j)}-\mu^{(j)})}.\] (A.24)

Here \(\sigma^{ij}\) is the \((i,j)\) element of \(\Sigma^{-1}\).

_Remark A.1_.: To show that \(f\) is a p.d.f., make the change of variables: \(\mathbf{y}=B^{-1}(\mathbf{x}-\boldsymbol{\mu})\), where \(B\) is a symmetric positive definite matrix satisfying \(B^{2}=\Sigma\). Then \(\mathbf{x}-\boldsymbol{\mu}=B\mathbf{y}\), \(\Sigma^{-1}=B^{-1}B^{-1}\) (since \((BB)^{-1}=B^{-1}B^{-1}\)), and \(\langle\mathbf{x}-\boldsymbol{\mu},\Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu}) \rangle=\langle B\mathbf{y},B^{-1}B^{-1}B\mathbf{y}\rangle=\langle B\mathbf{y}, B^{-1}\mathbf{y}\rangle=\langle\mathbf{y},BB^{-1}\mathbf{y}\rangle\) (since \(B^{\prime}=B\)) \(=\langle\mathbf{y},\mathbf{y}\rangle=\|\mathbf{y}\|^{2}=\sum_{1}^{k}(y^{(j)})^ {2}\). Thus,

\[\int_{\mathbb{R}^{k}}f(x)dx=\frac{\det B}{(2\pi)^{k/2}(\det\Sigma)^{1/2}}\int_{ \mathbb{R}^{k}}e^{-\frac{1}{2}\sum_{j=1}^{k}y_{j}^{2}}dy_{1}\ldots dy_{k},\]

since the Jacobian matrix is

\[J\left(\frac{\mathbf{x}}{\mathbf{y}}\right)=\begin{bmatrix}\frac{\partial x^{ (1)}}{\partial y^{(1)}}&\cdots&\frac{\partial x^{(1)}}{\partial y^{(k)}}\\ \vdots&\vdots\\ \frac{\partial x^{(k)}}{\partial y^{(1)}}&\cdots&\frac{\partial x^{(k)}}{ \partial y^{(k)}}\end{bmatrix}=B,\]

one gets the change of volume elements

\[dx_{1}dx_{2}\,\ldots\,dx_{k}=\det B\,dy_{1}dy_{2}\,\ldots\,dy_{k}.\]

Also, \(\det\Sigma=\det BB=\det B\cdot\det B\) (since \(\det(AB)=\det A\cdot\det B\) for \(k\times k\) matrices \(A\), \(B\)). Hence \(\det B=\sqrt{\det\Sigma}\). One has the positive square root, since \(B\) is positive definite. Hence\[\int_{{\mathbb{R}}^{k}}f(x)dx = \frac{1}{(2\pi)^{k/2}}\int_{{\mathbb{R}}^{k}}e^{-\frac{1}{2}\sum_{j =1}^{k}(y^{(j)})^{2}}dy_{1}dy_{2}\,\ldots\,dy_{k}\] \[= \prod_{j=1}^{k}\left(\frac{1}{\sqrt{2\pi}}\int_{{\mathbb{R}}^{k}}e^ {-\frac{y_{2}^{2}}{2}}dy_{j}\right)=\prod_{j=1}^{k}1=1.\]

This proof also shows that if \({\bf X}\) is distributed as \({\bf N}(\mathbf{\mu},\Sigma)\), then \({\bf Y}\equiv B^{-1}({\bf X}-\mathbf{\mu})\) is distributed as \({\bf N}(\mathbf{0},I)\) where \(I\) is the \(k\times k\)_identity matrix_ (having \(1\)'s on the diagonal, an zero's off the diagonal). The distribution \({\bf N}(\mathbf{0},I)\) is called the \(k\)_-dimensional standard normal distribution_. Notice that \({\bf Y}=(Y^{(1)},Y^{(2)},\ldots,Y^{(k)})^{\prime}\) is distributed as \({\bf N}(\mathbf{0},I)\), if and only if \(Y^{(1)},Y^{(2)},\ldots,Y^{(k)}\) are \(k\) independent \(1\)-dimensional standard normal random variables.

Conversely, given any positive definite matrix \(\Sigma\) (\(k\times k\)), and any vector \(\mathbf{\mu}\in{\mathbb{R}}^{k}\), the random vector \({\bf X}=(X^{(1)},X^{(2)},\ldots,X^{(k)})^{\prime}\) defined by

\[{\bf X}=B{\bf Y}+\mathbf{\mu}\] (A.25)

is distributed as \({\bf N}(\mathbf{\mu},\Sigma)\), if \({\bf Y}\) is \(k\)-dimensional standard normal. Here \(B\) is a symmetric positive definite matrix satisfying \(B^{2}=\Sigma\). If \(\Sigma\) is merely _nonnegative definite,_ then also the definition (A.25) makes sense and defines a random vector \({\bf X}\) whose distribution is denoted by \({\bf N}(\mathbf{\mu},\Sigma)\). If \(\Sigma\) is nonnegative definite and of rank less than \(k\) (i.e., at least one eigenvalue of \(\Sigma\) is zero), then \({\bf N}(\mathbf{\mu},\Sigma)\) defined above via (A.25) is called a _singular \(k\)-dimensional normal distribution_. The representation (A.25) yields

\[E({\bf X}) = BE{\bf Y}+\mathbf{\mu}=B\mathbf{0}+\mathbf{\mu}=\mathbf{\mu},\] \[{\rm cov}(X^{(i)},X^{(j)}) = {\rm cov}\left(\sum_{r=1}^{k}b_{ir}Y^{(r)}+\mu^{(i)},\sum_{r^{ \prime}=1}^{k}b_{jr^{\prime}}Y^{(r^{\prime})}+\mu^{(j)}\right)\] \[= {\rm cov}\left(\sum_{r=1}^{k}b_{ir}Y^{(r)},\sum_{r^{\prime}=1}^{ k}b_{jr^{\prime}}Y^{(r^{\prime})}\right)\] \[= \sum_{r,r^{\prime}=1}^{k}b_{ir}b_{jr^{\prime}}{\rm cov}(Y^{(r)}, Y(r^{\prime}))=\sum_{r=1}^{k}b_{ir}b_{jr}\] \[= (i,j)\mbox{ element of }BB=(i,j)\mbox{ element of }\Sigma=\sigma_{ij}.\] (A.26)

This justifies the name _mean vector_ for \(\mu\) and _dispersion_ (i.e., covariance _matrix_) for \(\Sigma\).

In general, if \({\bf X}\) is a \(k\)-variate Normal random vector (singular, or non-singular), distributed as \({\bf N}(\mathbf{\mu},\Sigma)\), and if \({\bf Y}=\mathbf{\nu}+C{\bf X}\), where \(\mathbf{\nu}\in{\mathbb{R}}^{d}\) and \(C\) is a \(d\times k\) matrix, then \({\bf Y}\) has the Normal distribution \({\bf N}(\mathbf{\beta},\Gamma)\), whose mean \(\beta\) and covariance (or, dispersion) matrix are given by

\[\mathbf{\beta} = E{\bf Y}=\mathbf{\nu}+CE{\bf X}=\mathbf{\nu}+C \mathbf{\mu},\] \[\Gamma = {\rm cov}{\bf Y}={\rm cov}(C{\bf X})=C\Sigma C^{\prime}.\] (A.27)

Note that if the \(d\times d\) matrix \(C\Sigma C^{\prime}\) is of rank \(k_{0}\) (\(\leq\min\{k,d\}\)), then \({\bf Y}\) may be represented as \({\bf Y}=B{\bf Z}+\mathbf{\beta}\), where \(B\) is the non-negative definite symmetric matrix satisfying \(B^{2}=C\Sigma C^{\prime}\), and \(Z\) is a \(d\)-dimensional standard Normal \(N({\bf O},I)\).

One may prove this directly using (A.25), or using moment generating functions that we discuss in Appendix B.

Let now \({\bf X}\) be \({\bf N}(\mathbf{\mu},\Sigma)\), a \(k\)-dimensional Normal distribution with \(\Sigma\) nonsingular. Then, for any given \(m\), \(1<m\leq k\), \({\bf Y}=(X_{m},\ldots,X_{k})^{\prime}\) has the Normal distribution \({\bf N}(\mathbf{\mu}_{m}^{k},\Sigma_{m}^{k})\), where

\[\mathbf{\mu}_{m}^{k}=(\mu_{m},\ldots,\mu_{k})^{\prime},\quad\Sigma_{m} ^{k}=((\sigma_{ij}))_{m\leq i,\,j\leq k}.\] (A.28)

This follows as a special case of the transformation in the preceding paragraph, with \(d=k-m+1\), \(\mathbf{\nu}={\bf 0}\), \(C=\begin{bmatrix}0&0\\ 0&I_{d}\end{bmatrix}\) (\(I_{d}\) is the \(d\times d\) identity matrix). Indeed, since \({\bf Y}\) is a linear transformation of \({\bf X}\) and, hence, Normal, its mean and covariance are easily seen to be given by (A.28). _The (marginal) density of \({\bf Y}=(X_{m},\ldots,X_{k})^{\prime}\) is given by_

\[f_{{\bf Y}}(x_{m},\ldots,x_{k})=2\pi^{-d/2}\left(\det\Sigma_{m}^{k}\right)^{-1 /2}\exp\left\{-\frac{1}{2}\sum_{i,j=m}^{k}\gamma^{ij}(x_{i}-\mu_{i})(x_{j}-\mu _{j})\right\}\] (A.29)

where \(((\gamma^{ij}))_{m\leq i,j\leq k}=\Gamma^{-1}\). Hence the conditional density of \(W=(X_{1},\ldots,\break X_{m-1})^{\prime}\), given \(\bar{X}_{m}=x_{m},\ldots,X_{k}=x_{k}\), is

\[\begin{array}{l}f_{W|x_{m},\ldots,x_{k}}(x_{1},\ldots,x_{m-1})=\frac{f_{{\bf X }}(x_{1},\ldots,x_{k})}{f_{{\bf Y}}(x_{m},\ldots,x_{k})}\\ =c\,\exp\!\left\{-\frac{1}{2}\sum_{1\leq i,j\leq m-1}\sigma^{ij}(x_{i}-\mu_{i})( x_{j}-\mu_{j})\right.\\ \left.\quad-\sum_{i=1}^{m-1}\left(\sum_{j=m}^{k}\sigma^{ij}((x_{j}-\mu_{j}))(x_ {i}-\mu_{i})\right)\!+\!\frac{1}{2}\sum_{m\leq i,j\leq k}\gamma^{ij}(x_{i}\!- \!\mu_{i})(x_{j}\!-\!\mu_{j})\right\}\\ =c_{1}(x_{m},\ldots,x_{k})\exp\left\{-\frac{1}{2}\sum_{1\leq i,j\leq m-1}\sigma ^{ij}(x_{i}-\ell_{i}(x_{m},\ldots,x_{k}))(x_{j}-\ell_{j}(x_{m},\ldots,x_{k})) \right\},\end{array}\] (A.30)

where \(c_{1}(x_{m},\ldots,x_{k})\) depends (possibly) on \(x_{m},\ldots,x_{k}\) and \(\mu\) and \(\Sigma\), and \(\ell_{i}(x_{m},\ldots,x_{k})\) is the affine linear function of \(x_{m},\ldots,x_{k}\), given by

\[\ell_{i}(x_{m},\ldots,x_{k})=\mu_{i}-A_{i}\left(\Sigma^{-1}\right)_{m-1}^{k-m+ 1}\begin{pmatrix}x_{m}-\mu_{m}\\ \vdots\\ x_{k}-\mu_{k}\end{pmatrix}.\] (A.31)

Here \(\left(\Sigma^{-1}\right)_{m-1}^{k-m+1}\) is the \((m-1)\times(k-m+1)\) matrix comprising the first \(m-1\) rows and the last \(k-m+1\) columns of \(\Sigma^{-1}\), and \(A_{i}\) is the \(i\)-th row of the \((m-1)\times(m-1)\) matrix

\[A=\left(((\sigma^{ij}))_{1\leq i,j\leq m-1}\right)^{-1}.\] (A.32)Thus _the conditional distribution of \(W=(X_{1},\ldots,X_{m-1})^{\prime}\), given \(X_{m}=x_{m},\ldots,X_{k}=x_{k}\), is an \((m-1)\)-dimensional Normal distribution \({\bf N}(\mathbf{\ell}(x_{m},\ldots,x_{k}),{\bf A})\), with the mean vector \(\ell\), and covariance (or, dispersion) matrix \({\bf A}\)._

As a special case, with \(m=2\), the conditional distribution of \(X_{1}\), given \(X_{2}=x_{2},\ldots,X_{k}=x_{k}\), is Normal \({\bf N}(\ell_{1},1/\sigma^{11})\), where (Exercise A.3)

\[\ell_{1}=\ell_{1}(x_{2},\ldots,x_{k})=\mu_{1}-\frac{1}{\sigma^{11}}\sum_{j=2}^ {k}\sigma^{1j}(x_{j}-\mu_{j}).\] (A.33)

### A Useful Property of the Multivariate Normal Distribution \(N(\mu,\Sigma)\)

Consider all vectors \({\bf x}\) in the following as column vectors, with \({\bf x}^{\prime}\) as the transpose of \({\bf x}\), a row vector.

Proposition A.1: _Let \({\bf X}\) be a \(k\)-dimensional Normal random vector \(N(\mathbf{\mu},\Sigma)\), where \(\Sigma\) is positive definite. Then \(({\bf X}-\mathbf{\mu})^{\prime}\Sigma^{-1}({\bf X}-\mathbf{ \mu})\) has the chi-square distribution with \(k\) degrees of freedom._

Proof: Let \({\bf Y}={\bf X}-\mathbf{\mu}\). Then \(Y\) is \(N(0,\Sigma)\). Let \(B\) be a \(k\times k\) matrix such that \(BB^{\prime}=\Sigma^{-1}\). (See the Lemma below.) Then \(\Sigma=(B^{\prime})^{-1}B^{-1}\). Now the random vector \({\bf Z}={\bf B}^{\prime}{\bf Y}\) is \(N(0,B^{\prime}\Sigma B)\). But \(B^{\prime}\Sigma B=B^{\prime}(B^{\prime})^{-1}B^{-1}B=I_{k}\), where \(I_{k}\) is the \(k\times k\) identity matrix. Thus \({\bf Z}\) is a standard \(k\)-dimensional Normal random vector whose coordinates \(Z_{1},\ldots,Z_{k}\) are one-dimensional independent standard Normal random variables. Hence \(({\bf X}-\mathbf{\mu})^{\prime}\Sigma^{-1}({\bf X}-\mathbf{ \mu})={\bf Y}^{\prime}\Sigma^{-1}{\bf Y}={\bf Y}^{\prime}BB^{\prime}{\bf Y}= {\bf Z}^{\prime}{\bf Z}=Z_{1}^{2}+\cdots+Z_{k}^{2}\) has the chi-square distribution with \(k\) degrees of freedom. Q.E.D. 

### Note

If \({\bf Y}\) is a \(k\)-dimensional random vector with covariance matrix \(\Sigma=((\sigma_{ij}))\), and \({\bf Z}=c{\bf Y}\) for some \(m\times k\) matrix \(C\), then the \(m\times m\) covariance matrix of \({\bf Z}\) is \(C\Sigma C^{\prime}\). For, the covariance between the \(i\)-th and \(j\)-th components of \(Z\) is \(\mbox{cov}(Z_{i},Z_{j})=\mbox{cov}(\sum_{1\leq r\leq k}C_{ir}Y_{r}\), \(\sum_{1\leq s\leq k}C_{js}Y_{s})=\sum_{r,s}C_{ir}C_{js}\sigma_{rs}=\sum_{s}( \sum_{r}C_{ir}\sigma_{rs})C_{js}=\sum_{s}(C\Sigma)_{is}\widehat{C}_{js}\), which is the \((i,j)\) element of the matrix \(C\Sigma C^{\prime}\).

Lemma A.1: _Let \(\Gamma\) be a \(k\times k\) symmetric and positive definite matrix. Then there exists a \(k\times k\) nonsingular matrix \(B\) such that \(\mathbf{\Gamma}=BB^{\prime}\)._

Proof: Let \(\lambda_{1},\ldots,\lambda_{k}\) be the (positive) eigenvalues of \(\Gamma\), counting multiplicities, and let \(a_{1},\ldots,a_{k}\) be corresponding eigenvectors of unit length each. Then \(\mathbf{\Gamma}a_{i}=\lambda_{i}a_{i}\) (\(i=1,\ldots,k\)), and the matrix \(A\) comprising the \(k\) (column) vectors \(a_{1},\ldots,a_{k}\) is orthonormal, i.e., \(AA^{\prime}={\bf I}_{k}\), and satisfies \(\mathbf{\Gamma}A=A(\mbox{Diag}(\lambda_{1},\ldots\lambda_{k}))\). Define \(B=A(\mbox{Diag}(\sqrt{\lambda_{1}},\ldots,\sqrt{\lambda_{k}}))\). Then \(BB^{\prime}=A(\mbox{Diag}(\lambda_{1},\ldots,\lambda_{k}))A^{\prime}=\Gamma AA^ {\prime}=\Gamma\). Q.E.D.

## Exercises for Appendix A

### Ex. A.1

Calculate \(\operatorname{var}(X_{1})\) and \(\operatorname{var}(X)\) in (A.2).

**Ex. A.2**.: Show that the sum of independent chi-square random variables \(Z_{1},Z_{2},\ldots,Z_{m}\) with degrees of freedom \(k_{1},k_{2},\ldots,k_{m}\), respectively, is a chi-square random variable with degrees of freedom \(k_{1}+k_{2}+\cdots+k_{m}\).

### Ex. A.3

For the case of the Normal distribution (A.24), with \(k=2\), show that \(\sigma^{11}=\frac{\sigma_{22}}{\sigma_{11}\sigma_{22}-\sigma_{12}^{2}}=\frac{1 }{\sigma_{11}(1-\rho^{2})}\), \(\sigma^{12}=\sigma^{21}=-\frac{\sigma_{12}}{\sigma_{11}\sigma_{22}-\sigma_{12 }^{2}}=-\frac{1}{\sqrt{\sigma_{11}\sigma_{22}}}\left(\frac{\rho}{1-\rho^{2}}\right)\), so that the conditional distribution of \(X_{1}\), given \(X_{2}=x_{2}\), is Normal with mean \(\ell_{1}(x_{2})=\mu_{1}+\sqrt{\frac{\sigma_{11}}{\sigma_{22}}}\,\rho(x_{2}-\mu_ {2})\) and variance \(\sigma_{11}(1-\rho^{2})\).

## Appendix B Moment Generating Functions (M.G.F.)

Definition B.1: The _m.g.f. of a random variable_\(X\) (or of its _distribution_) is defined as

\[\varphi(z)=Ee^{zX}\qquad z\in\mathbb{R}^{1}.\]

Theorem B.1: _Suppose the m.g.f. \(\varphi(\xi)\) (or, \(\varphi(\xi)\)) of a random variable (vector) \(X\) is finite in a neighborhood of zero (origin \(=\mathbf{0}\)). Then \(\varphi\) determines the distribution of \(X\)._

Proof: See Proposition 4.2 in Chap. 4.

Theorem B.2: _Suppose the m.g.f. \(\varphi\) of a random variable \(X\) is finite in an interval nondegenerate) around zero. (a) Then all moments of \(X\) are finite and_

\[EX^{k}=\frac{d^{k}\varphi(z)}{dz^{k}}\Bigg{|}_{z=0}.\]

_(b) \(\varphi(z)=\sum_{0}^{\infty}\mu_{k}\left(\frac{z^{k}}{k!}\right)\), where \(\mu_{k}=EX^{k}\)._

Proof: Follows from the proof of Proposition 4.2 in Chap. 4. 

Theorem B.3: _If \(X_{1},X_{2},\ldots,X_{n}\) are independent with finite m.g.f.'s \(\varphi_{1},\varphi_{2},\ldots,\varphi_{n}\) in a neighborhood of zero, then the m.g.f. \(\varphi\) of \(X_{1}+X_{2}+\cdots+X_{n}\) is_

\[\varphi(z)=\varphi_{1}(z)\varphi_{2}(z)\ldots\varphi_{n}(z),\]

_in a neighborhood of zero._

Proof: \[\varphi(z) =Ee^{z(X_{1}+X_{2}+\cdots+X_{n})}=(Ee^{zX_{1}})(Ee^{zX_{2}})\cdots (Ee^{zX_{n}})\] \[=\varphi_{1}(z)\varphi_{2}(z)\cdots\varphi_{n}(z).\]

_Example B.1._\(X\) is \(\mathscr{P}(\lambda)\). Then

\[\varphi(z) =\sum_{x=0}^{\infty}e^{zx}e^{-\lambda}\lambda^{x}/x!=e^{-\lambda} \sum_{x=0}^{\infty}(\lambda e^{z})^{x}/x!\] \[=e^{-\lambda}e^{\lambda e^{z}}=e^{\lambda(e^{z}-1)}<\infty,\quad- \infty<z<\infty.\]

_Example B.2._\(X\) is \(\mathscr{B}(n,p)\):

\[\varphi(z) =\sum_{x=0}^{n}e^{zx}{n\choose x}p^{x}q^{n-x}=\sum_{x=0}^{n}{n \choose x}(pe^{z})^{x}q^{n-x}\] \[=(pe^{z}+q)^{n}<\infty,\qquad-\infty<z<\infty.\]

_Example B.3._\(X\) is \(\mathscr{N}\mathscr{B}(r,p)\):

\[\varphi(z)=\sum_{x=0}^{\infty}e^{zx}{r+x-1\choose r-1}p^{r}q^{x}.\]

Alternatively, \(\varphi(z)=Ee^{z(X_{1}+X_{2}+\cdots+X_{r})}\), where \(X_{i}\) is the number of failures between the \((i-1)\)-th and \(i\)-th successes. Because \(X_{1},X_{2},\ldots,X_{r}\) are i.i.d., one has

\[\varphi(z) =Ee^{zX_{1}}e^{zX_{2}}\ldots e^{zX_{r}}=(Ee^{zX_{1}})(Ee^{zX_{2}} )\cdots(Ee^{zX_{r}})\] \[=\varphi_{1}(z)\varphi_{1}(z)\cdots\varphi_{1}(z)=(\varphi_{1}(z) )^{r},\]

where

\[\varphi_{1}(z) =Ee^{zX_{1}}=\sum_{x=0}^{\infty}e^{zx}pq^{x}=p\sum_{x=0}^{\infty} (qe^{z})^{x}=\frac{p}{1-qe^{z}}<\infty\] \[\quad\text{if $e^{z}<\frac{1}{q}$, i.e., $z<\ln\frac{1}{q}$}.\] \[(\ln\frac{1}{q}>0).\]

Therefore,

\[\varphi(z)=\left(\frac{p}{1-qe^{z}}\right)^{r}.\]

_Example B.4._ Let \(X\) be gamma \(\mathscr{G}(\alpha,\beta)\) with p.d.f.

\[f(x) =\frac{1}{\Gamma(\beta)\alpha^{\beta}}\,e^{-x/\alpha}x^{\beta-1},\quad 0<x<\infty\] \[=0,\qquad\text{elsewhere}.\]

Its m.g.f. is

\[\varphi(z) =Ee^{zX}=\int_{0}^{\infty}e^{zx}f(x)dx=\frac{1}{\Gamma(\beta) \alpha^{\beta}}\,e^{-\left(\frac{1}{\alpha}-z\right)x}x^{\beta-1}dx\] \[=\infty\qquad\text{if $\,z\geq\frac{1}{\alpha}$}.\]For \(z<\frac{1}{\alpha}\), change variables \(y=\left(\frac{1}{\alpha}-z\right)x\), to get

\[\varphi(z) = \frac{1}{\Gamma(\beta)\alpha^{\beta}}\int_{0}^{\infty}e^{-y}y^{ \beta-1}dy\bigg{/}\left(\frac{1}{\alpha}-z\right)^{\beta}\] \[= \frac{\alpha^{\beta}}{\Gamma(\beta)\alpha^{\beta}(1-\alpha z)^{ \beta}}\cdot\Gamma(\beta)=\frac{1}{(1-\alpha z)^{\beta}}\,,\quad-\infty<z< \frac{1}{\alpha}\,.\]

Example B.5: Let \(X_{1},X_{2},\ldots,X_{n}\) be i.i.d. gamma \(\mathscr{G}(\alpha,\beta)\). Then \(Y=X_{1}+X_{2}+\cdots+X_{n}\) is gamma \(\mathscr{G}(\alpha,n\beta)\). For the m.g.f. of \(Y\) is, by Theorem B.3,

\[\varphi_{Y}(z)=\frac{1}{(1-\alpha z)^{n\beta}}\]

which is the m.g.f. of a gamma \(\mathscr{G}(\alpha,n\beta)\).

Example B.6: \(X\sim N(\mu,\sigma^{2})\). First let \(\mu=0\), \(\sigma^{2}=1\). Then

\[\varphi(z) = Ee^{zX}=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{zx}e^{-x^ {2}/2}dx=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{1}{2}(x-z)^{2}+ \frac{z^{2}}{2}}dx\] \[= \frac{1}{\sqrt{2\pi}}\,e^{z^{2}/2}\int_{-\infty}^{\infty}e^{- \frac{1}{2}y^{2}}dy\qquad(y=x-z)\] \[= e^{z^{2}/2}.\]

Now consider the general case \(X\sim\mathbf{N}(\mu,\sigma^{2})\), and let \(Y=\frac{X-\mu}{\sigma}\,\). Then \(Y\sim\mathbf{N}(0,1)\) and

\[Ee^{zX} = Ee^{z(\sigma Y+\mu)}=Ee^{z\mu+\sigma zY}=e^{\mu z}Ee^{\sigma zY}\] \[= e^{\mu Z}e^{\sigma^{2}z^{2}/2}=e^{\mu z+\sigma^{2}z^{2}/2}.\]

Example B.7: Consider \(X\sim N(\mu,\sigma^{2})\). By Example B.6 above, for the case \(\mu=0\), \(\sigma^{2}=1\), expanding the m.g.f. \(\varphi\) of \(X\) in a power series,

\[\varphi(z)=e^{z^{2}/2}=1+\frac{z^{2}}{2}+\frac{1}{2!}\left(\frac{z^{2}}{2} \right)^{2}+\frac{1}{3!}\left(\frac{z^{2}}{2}\right)^{3}+\frac{1}{4!}\left( \frac{z^{2}}{2}\right)^{4}+\cdots\,\]

one has \(EX^{k}=0\) for all odd integers \(k\), and

\[EX^{2}=\varphi^{\prime\prime}(0)=1,\quad EX^{4}=\varphi^{(iv)}(0)=\frac{1}{8} \left(\frac{d}{dz^{4}}\,z^{4}\right)_{z=0}=\frac{4!}{8}=3.\]

In general, the term-by-term differentiation of the power series for \(\varphi(z)\), evaluated at \(z=0\), yields

\[EX^{2k}=\left[\frac{d}{dz^{zk}}\cdot\frac{\left(\frac{z^{2}}{2}\right)^{k}}{k!}\right]_{z=0}=\frac{(2k)!}{k!2^{k}}=(2k-1)(2k-3)\cdots 3.1.\] (B.1)

A Normal random variable \(\mathbf{N}(\mu,\sigma^{2})\), say \(X\), may be expressed as \(X=\mu+\sigma Y\) where \(Y\) is \(\mathbf{N}(0,1)\). Hence, the _raw moments_ of \(X\) are \[\mu^{\prime}_{1}=EX=\mu,\quad\mu^{\prime}_{2}=EX^{2}=\mu^{2}+\sigma^{2}, \quad\mu^{\prime}_{3}=EX^{3}=\mu^{3}+3\mu\sigma^{2},\] \[\mu^{\prime}_{4}=EX^{4}=\mu^{4}+6\mu^{2}\sigma^{2}+3\sigma^{4},\]

and the _centered moments_ of \(X\), \(\mu_{k}\equiv E(X-\mu)^{k}\), are given by

\[\mu_{k}=0\qquad\forall\text{ odd integers }k,\] \[\mu_{2k}=E(X-\mu)^{2k}=\sigma^{2k}EY^{2k}=(2k-1)(2k-3)\cdots 3.1. \sigma^{2k},\ (k=1,2,\dots).\] (B.2)

_II. Multivariate M.G.F._ Let \({\bf X}=(X^{(1)},X^{(2)},\dots,X^{(k)})^{\prime}\) be a \(k\)-dimensional random vector. Then the m.g.f. of \({\bf X}\) is

\[\varphi({\boldsymbol{\xi}})=Ee^{\langle{\boldsymbol{\xi}},{\bf X}\rangle}=Ee^{ {\boldsymbol{\xi}},{\bf X}}=Ee^{\sum_{j=1}^{k}-\xi^{(j)}X^{(j)}}\]

for \(\xi=(\xi^{(1)},\dots,\xi^{(k)})^{\prime}\in{\mathbb{R}}^{k}\).

**Theorem B.4**.: _Suppose that the m.g.f. \(\varphi({\boldsymbol{\xi}})\) of a random vector \({\bf X}=(X^{(1)},\dots,\)\(X^{(k)})\) is finite in a neighborhood of \({\boldsymbol{\xi}}=0\) (i.e., for \(|\xi|<\delta\), for some \(\delta>0\)). (a) Then all moments of \(X\) are finite and, using multi-indices \({\boldsymbol{\nu}}\),_

\[\mu_{\nu} \equiv EX^{\nu}\equiv E\left[(X^{(1)})^{\nu^{(1)}}(X^{(2)})^{\nu^ {(2)}}\cdots(X^{(k)})^{\nu^{(k)}}\right]\] \[=\left[D^{\nu}\varphi({\boldsymbol{\xi}})\right]_{\xi=0}\equiv \left[\left(\frac{\partial}{\partial\xi^{(1)}}\right)^{\nu^{(1)}}\left(\frac{ \partial}{\partial\xi^{(2)}}\right)^{\nu^{(2)}}\cdots\left(\frac{\partial}{ \partial\xi^{(k)}}\right)^{\nu^{(k)}}\varphi({\boldsymbol{\xi}})\right]_{{ \boldsymbol{\xi}}=0}\]

_for all \(\nu=(\nu^{(1)},\dots,\nu^{(k)})\in({\mathbb{Z}}^{+})^{k}\) (= the set of all \(k\)-tuples of nonnegative integers)._

_(b)_ \[\varphi({\boldsymbol{\xi}})=\sum_{\nu}\frac{\mu_{\nu}}{\nu!}\,{ \boldsymbol{\xi}}^{\nu}\qquad|\xi|<\delta\text{,}\] _where_ \({\boldsymbol{\xi}}^{\nu}=(\xi^{(1)})^{\nu^{(1)}}(\xi^{(2)})^{\nu^{(2)}}\cdots (\xi^{(k)})^{\nu^{(k)}}\)_,_ \(\nu!=\nu^{(1)}!\nu^{(2)}!\cdots\nu^{(k)}!\)_._

_(c) If \({\bf X}_{1},{\bf X}_{2},\dots,{\bf X}_{n}\) are independent \(k\)-dimensional random vectors whose m.g.f.'s \(\varphi_{1}(\xi),\varphi_{2}(\xi),\dots,\varphi_{k}(\xi)\) are finite for \(|\xi|<\delta\), for some \(\delta>0\), then the m.g.f. \(\varphi(\xi)\) of \({\bf X}={\bf X}_{1}+{\bf X}_{2}+\cdots+{\bf X}_{n}\) is finite for \(|\xi|<\delta\) and is given by_

\[\varphi({\boldsymbol{\xi}})=\varphi_{1}({\boldsymbol{\xi}})\varphi_{2}({ \boldsymbol{\xi}})\cdots\varphi_{n}({\boldsymbol{\xi}}).\]

Proof.: The term-by-term differentiation below is justified by the proof of Proposition 4.2 in Chap. 4. (a) Note that, if \(Q\) is the distribution of \({\bf X}\),

\[\frac{\partial}{\partial\xi^{(1)}}\,\varphi({\boldsymbol{\xi}}) =\frac{\partial}{\partial\xi^{(1)}}\int_{{\mathbb{R}}^{k}}e^{{ \boldsymbol{\xi}}^{\prime}{\boldsymbol{\times}}}dQ({\bf x})\] \[=\int_{{\mathbb{R}}^{k}}\frac{\partial}{\partial\xi^{(1)}}\,e^{ \xi^{(1)}x^{(1)}+\cdots+\xi^{(i)}x^{(i)}+\cdots+\xi^{(k)}x^{(k)}}dQ({\bf x})\] \[=\int_{{\mathbb{R}}^{k}}x^{(1)}e^{\xi^{(1)}x^{(1)}+\cdots+\xi^{( i)}x^{(i)}+\cdots+\xi^{(k)}x^{(k)}}dQ({\bf x}).\]

Continuing in this manner

\[\left(\frac{\partial}{\partial\xi^{(1)}}\right)^{\nu^{(1)}}\varphi({ \boldsymbol{\xi}})=\int^{{\mathbb{R}}^{k}}(x^{(1)})^{\nu^{(1)}}e^{\xi^{(1)}x^ {(1)}+\cdots+\xi^{(i)}x^{(i)}+\cdots+\xi^{(k)}x^{(k)}}dQ({\bf x}).\]Differentiating w.r.t. \(\xi^{(2)}\), \(\nu^{(2)}\)-times, one gets

\[\left(\frac{\partial}{\partial\xi^{(1)}}\right)^{\nu^{(2)}}\left( \frac{\partial}{\partial\xi^{(1)}}\right)^{\nu^{(1)}}\varphi(\boldsymbol{\xi})\] \[=\int_{\mathbb{R}^{k}}\left(\frac{\partial}{\partial\xi^{(2)}} \right)^{\nu^{(2)}}\left[\left(x^{(1)}\right)^{\nu^{(1)}}e^{\boldsymbol{\xi}^{ \prime}\mathbf{x}}\right]dQ(\mathbf{x})\] \[=\int_{\mathbb{R}^{k}}\left(x^{(1)}\right)^{\nu^{(1)}}\left(x^{(2) }\right)^{\nu^{(2)}}e^{\boldsymbol{\xi}^{\prime}\mathbf{x}}dQ(\mathbf{x}).\]

Continuing in this manner one has

\[D^{\nu}\varphi(\boldsymbol{\xi})\Big{|} \equiv\left(\frac{\partial}{\partial\xi^{(1)}}\right)^{\nu^{(1)}} \left(\frac{\partial}{\partial\xi^{(2)}}\right)^{\nu^{(2)}}\cdots\left(\frac{ \partial}{\partial\xi^{(k)}}\right)^{\nu^{(k)}}\varphi(\boldsymbol{\xi})\] \[=\int_{\mathbb{R}^{k}}(x^{(1)})^{\nu^{(1)}}(x^{(2)})^{\nu^{(2)}} \cdots(x^{(k)})^{\nu^{(k)}}\cdot e^{\boldsymbol{\xi}^{\prime}\mathbf{x}}dQ( \mathbf{x}).\]

Hence

\[D^{\nu}\varphi(\boldsymbol{\xi})\Big{|}_{\xi=0}=\int_{\mathbb{R}^{k}}(x^{(1)}) ^{\nu^{(1)}}\cdots(x^{(k)})^{\nu^{(k)}}dQ(\mathbf{x})=\mu_{\nu}=E\mathbf{X}^{( \boldsymbol{\nu})}.\]

(b)

\[\varphi(\boldsymbol{\xi}) =Ee^{\boldsymbol{\xi}^{\prime}\mathbf{X}}=Ee^{\sum_{i=1}^{k}\xi^{ (i)}X^{(i)}}=\sum_{r=0}^{\infty}\frac{1}{r!}E\left(\sum_{i=1}^{k}\xi^{(i)}X^{( i)}\right)^{r}\] \[=\sum_{r=0}^{\infty}\frac{1}{r!}E\Bigg{(}\sum_{|\boldsymbol{\nu} |=r}\binom{r}{\nu^{(1)}}\binom{r-\nu^{(1)}}{\nu^{(2)}}\cdots\binom{\nu^{(k)}} {\nu^{(k)}}\times\] \[\qquad\times\left(\xi^{(1)}X^{(1)}\right)^{\nu^{(1)}}\left(\xi^{ (2)}X^{(2)}\right)^{\nu^{(2)}}\cdots\left(\xi^{(k)}X^{(k)}\right)^{\nu^{(k)}} \Bigg{)},\]

where \(|\boldsymbol{\nu}|=\nu^{(1)}+\cdots+\nu^{(k)}\) and the second sum is over all \(\nu\in(\mathbb{Z}^{+})^{k}\) such that \(|\boldsymbol{\nu}|=r\).

\[\binom{r}{\nu^{(1)}}\binom{r-\nu^{(1)}}{\nu^{(2)}}\cdots\binom{\nu^{(k)}}{\nu ^{(k)}}=\frac{r!}{\nu^{(1)}!\nu^{(2)}!\cdots\nu^{(k)}!}\.\]

Hence, writing \(\boldsymbol{\nu}!=\nu^{(1)}!\cdots\nu^{(k)}!\), one has

\[\varphi(\boldsymbol{\xi}) =\sum_{r=0}^{\infty}\sum_{|\boldsymbol{\nu}|=r}\frac{\boldsymbol {\xi}^{\prime}}{\boldsymbol{\nu}!}\,E\left[\left(X^{(1)}\right)^{\nu^{(1)}}(X ^{(2)})^{\nu^{(2)}}\cdots(X^{(k)})^{\nu^{(k)}}\right]\] \[=\sum_{r=0}^{\infty}\sum_{|\boldsymbol{\nu}|=r}\frac{\mu_{ \boldsymbol{\nu}}}{\boldsymbol{\nu}!}\,\boldsymbol{\xi}^{\nu}=\sum_{\boldsymbol {\nu}\in(\mathbb{Z}^{+})^{k}}\frac{\mu_{\boldsymbol{\nu}}}{\boldsymbol{\nu}!} \,\boldsymbol{\xi}^{\nu}.\]

(c) Essentially the same proof as that of Theorem B.3.

_Example B.8_.: \(\mathbf{X}\sim(\mathbf{N}(\boldsymbol{\mu},\Sigma))\). Assume \(\Sigma\) is \(k\times k\) symmetric positive definite. Let \(B\) be a symmetric positive definite matrix such that \(BB=\Sigma\). Then define \(Y=B^{-1}(X-\mu)\). We have seen that \(Y\sim\mathbf{N}(\mathbf{0},I)\), i.e., the coordinates of \(Y\) are \(k\) independent standard normal random variables. The m.g.f. of \(\mathbf{Y}\) is

\[\psi(\boldsymbol{\xi}) \equiv Ee^{\boldsymbol{\xi}^{\prime}\mathbf{Y}}=Ee^{\sum_{i=1}^{k} \xi^{(i)}Y^{(i)}}=\prod_{i=1}^{k}Ee^{\xi^{(i)}Y^{(i)}}\] \[=\prod_{i=1}^{k}e^{(\xi^{(i)})^{2}/2}=e^{|\boldsymbol{\xi}|^{2}/2}.\]

Thus the m.g.f. of \(\mathbf{X}\) is

\[\varphi(\boldsymbol{\xi}) =Ee^{\boldsymbol{\xi}^{\prime}\mathbf{X}}=Ee^{\boldsymbol{\xi}^{ \prime}(B\mathbf{Y}+\boldsymbol{\mu})}=e^{\boldsymbol{\xi}^{\prime}\boldsymbol {\mu}}E_{\boldsymbol{\xi}}e^{\boldsymbol{\xi}^{\prime}B\mathbf{Y}}\] \[=e^{\boldsymbol{\xi}^{\prime}\boldsymbol{\mu}}Ee^{(B\boldsymbol{ \xi})^{\prime}\mathbf{Y}}=e^{\boldsymbol{\xi}^{\prime}\boldsymbol{\mu}}\psi(B \boldsymbol{\xi})=e^{\boldsymbol{\xi}^{\prime}\boldsymbol{\mu}}e^{|B\boldsymbol {\xi}|^{2}/2}\] \[=e^{\boldsymbol{\xi}^{\prime}\boldsymbol{\mu}+(B\boldsymbol{\xi} )^{\prime}(B\boldsymbol{\xi})/2}=e^{\boldsymbol{\xi}^{\prime}\boldsymbol{\mu}+ \boldsymbol{\xi}^{\prime}B^{\prime}B\boldsymbol{\xi}/2}=e^{\boldsymbol{\xi}^{ \prime}\boldsymbol{\mu}+\boldsymbol{\xi}^{\prime}BB\boldsymbol{\xi}/2}\] \[=e^{\boldsymbol{\xi}^{\prime}\boldsymbol{\mu}+\frac{1}{2} \boldsymbol{\xi}^{\prime}\Sigma\boldsymbol{\xi}}.\]

## Appendix C Computation of Power of Some Optimal Tests: Non-central \(t\), \(\chi^{2}\) and \(F\)

_Example C.1_.: Consider the UMP test \(\varphi^{*}\) of size \(\alpha\in(0,1)\) for \(H_{0}:\mu\leq\mu_{0}\) against \(H_{1}:\mu>\mu_{0}\) based on \(\mathbf{X}=(X_{1},\ldots,X_{n})\) where \(X_{i}\)'s are i.i.d. \(\mathbf{N}(\mu,\sigma_{0}^{2})\), with \(\sigma_{0}^{2}>0\) known and \(\mu\in\mathbb{R}=\Theta:\varphi^{*}(\mathbf{x})=1\) if \(\sqrt{n}(\overline{x}-\mu_{0})/\sigma_{0}>z_{\alpha}\) and \(\varphi^{*}(\mathbf{x})=0\) otherwise. The power function is

\[\gamma(u)\equiv\gamma_{\varphi^{*}}(\mu) =P_{\mu}(\sqrt{n}(\overline{X}-\mu_{0})/\sigma_{0}>z_{\alpha})\] \[=P_{\mu}\left(\sqrt{n}(\overline{X}-\mu)/\sigma_{0}>z_{\alpha}- \frac{\sqrt{n}(\mu-\mu_{0})}{\sigma_{0}}\right)\] \[=1-\Phi\left(z_{\alpha}-\frac{\sqrt{n}(\mu-\mu_{0})}{\sigma_{0}} \right)\qquad(\mu\in\Theta_{1}=(\mu_{0},\infty)),\] (C.1)

where \(\Phi\) is the (cumulative) distribution function of a standard normal random variable.

Similarly, the power function of the UMP unbiased test for \(H_{0}:\mu=\mu_{0}\) against \(H_{1}:\mu\neq\mu_{0}\) is

\[\gamma(u) =P_{\mu}\left(\left|\frac{\sqrt{n}(\overline{X}-\mu_{0})}{\sigma _{0}}\right|>\frac{z_{\alpha}}{2}\right)\] \[=1-P_{\mu}\left(-\frac{z_{\alpha}}{2}\leq\frac{\sqrt{n}(\overline {X}-\mu_{0})}{\sigma_{0}}\leq\frac{z_{\alpha}}{2}\right)\] \[=1-P_{\mu}\left(-\frac{z_{\alpha}}{2}-\frac{\sqrt{n}(\mu-\mu_{0}) }{\sigma_{0}}\leq\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma_{0}}\leq z_{\alpha/ 2}-\frac{\sqrt{n}(\mu-\mu_{0})}{\sigma_{0}}\right)\] \[=1-\left\{\Phi\left(z_{\alpha/2}-\frac{\sqrt{n}(\mu-\mu_{0})}{ \sigma_{0}}\right)-\Phi\left(-z_{\alpha/2}-\frac{\sqrt{n}(\mu-\mu_{0})}{\sigma _{0}}\right)\right\}\quad(\mu\neq\mu_{0}).\] (C.2)

_Example C.2_.: The UMP test for \(H_{0}:\sigma^{2}\leq\sigma_{0}^{2}\) against \(H_{1}:\sigma^{2}>\sigma_{0}^{2}\) based on i.i.d. \(\mathbf{N}(\mu_{0},\sigma^{2})\) random variables \(X_{i}\) (\(1\leq i\leq n\)) is to reject \(H_{0}\) iff \(\sum_{i=1}^{n}(X_{i}-\mu_{0})^{2}/\sigma_{0}^{2}>\chi_{1-\alpha}^{2}(n)\) [upper \(\alpha\)-point of chi-square distribution with \(n\) d.f.]. Its power is\[\gamma(\sigma^{2}) =P_{\sigma^{2}}\left(\sum_{i=1}^{n}(X_{i}-\mu_{0})^{2}/\sigma_{0}^{2} >\chi_{1-\alpha}^{2}(n)\right)\] \[\equiv P_{\sigma^{2}}\left(\sum_{i=1}^{n}(X_{i}-\mu_{0})^{2}/\sigma^{2} >\frac{\sigma_{0}^{2}}{\sigma^{2}}\chi_{1-\alpha}^{2}(n)\right)=\int_{\frac{ \sigma_{0}^{2}}{\sigma^{2}}\chi_{1-\alpha}^{2}(n)}^{\infty}k_{n}(u)du\] \[=1-K_{n}\left(\frac{\sigma_{0}^{2}}{\sigma^{2}}\,\chi_{1-\alpha}^{ 2}(n)\right)\qquad(\sigma^{2}>\sigma_{0}^{2}),\] (C.3)

where \(k_{n}\) is the p.d.f., and \(K_{n}\) the cumulative distribution function of the chi-square distribution with \(n\) d.f.

_Example C.3_.: Consider now the test for \(H_{0}:\mu\leq\mu_{0}\) against \(H_{1}:\mu>\mu_{0}\) based on i.i.d. \(X_{i}\) (\(1\leq i\leq n\)) which are \(\mathbf{N}(\mu,\sigma^{2})\) with \(\sigma^{2}\) unknown. The UMP unbiased test of size \(\alpha\) rejects \(H_{0}\) iff \(\sqrt{n}(\overline{X}-\mu_{0})/s>t_{1-\alpha}(n-1)\) [the upper \(\alpha\)-point of Student's \(t\) with \(n-1\) d.f.]. Here \(s^{2}=\sum(X_{i}-\overline{X})^{2}/(n-1)\); and \(\frac{(n-1)s^{2}}{\sigma^{2}}\) has a chi-square distribution with \(n-1\) d.f. and is independent of \(\sqrt{n}(\overline{X}-\mu)/\sigma\) which is \(\mathbf{N}(0,1)\) under \(P_{\mu,\sigma^{2}}\). Thus the power of the test is

\[\gamma(\mu;\sigma^{2}) =P_{\mu,\sigma^{2}}\left(\frac{\sqrt{n}(\overline{X}-\mu_{0})}{s}> t_{1-\alpha}(n-1)\right)\] \[=E_{\mu,\sigma^{2}}\left[P_{\mu,\sigma^{2}}\left(\frac{\sqrt{n}( \overline{X}-\mu_{0})}{\sigma}>\frac{s}{\sigma}\,t_{1-\alpha}(n-1)\bigg{|}s \right)\right]\] \[=E_{\mu,\sigma^{2}}\left[P_{\mu,\sigma^{2}}\left(\frac{\sqrt{n}( \overline{X}-\mu)}{\sigma}>\frac{\sqrt{n}(\mu-\mu_{0})}{\sigma}+\frac{s}{ \sigma}\,t_{1-\alpha}(n-1)\bigg{|}s\right)\right]\] \[=1-\int_{0}^{\infty}\Phi\left(-\frac{\sqrt{n}(\mu-\mu_{0})}{ \sigma}+\frac{(t_{1-\alpha}(n-1))}{\sqrt{n-1}}\,\sqrt{u}\right)k_{n-1}(u)du.\] (C.4)

In obtaining (C.4) we used (i) the independence of \(\overline{X}\) and \(s\) and (ii) the fact that \(U\equiv(n-1)\left(\frac{s}{\sigma}\right)^{2}\) is a chi-square random variable with \(n-1\) d.f. (As before, \(k_{\nu}\) is the p.d.f. of a chi-square with \(\nu\) d.f.)

Replacing \(t_{1-\alpha}(n-1)\) by a general argument \(t>0\), and differentiating (C.4) w.r.t. \(t\) (and changing the sign of the derivative) one arrives at the p.d.f. of the so-called _non-central \(t\)-distribution with the noncentrality parameter \(\Delta=\frac{\sqrt{n}(\mu-\mu_{0})}{\sigma}\) and d.f. \(\nu=n-1\)_, as

\[f_{\nu,\Delta}(t)\equiv f(t)=\int_{0}^{\infty}\varphi\left(-\Delta+\frac{t}{ \sqrt{\nu}}\,\sqrt{u}\right)\cdot\frac{\sqrt{u}}{\sqrt{\nu}}\,k_{\nu}(u)du\] (C.5)

where \(\varphi\) is the standard normal density. Simplifying a little, this density is (Recall: \(k_{\nu}(u)=\frac{1}{2^{\nu/2}\Gamma\left(\frac{\nu}{2}\right)}\,e^{-\frac{u}{2} }u^{\frac{\nu}{2}-1}\))

\[f(t) =\frac{1}{\sqrt{2\pi}\,2^{\nu/2}\Gamma\left(\frac{\nu}{2}\right) \sqrt{\nu}}\int_{0}^{\infty}u^{\frac{\nu-1}{2}}\exp\left\{-\frac{u\left(1+ \frac{t^{2}}{\nu}\right)+\Delta^{2}-\frac{2\lambda_{\ell}}{\sqrt{\nu}}\,\sqrt{ u}}{2}\right\}\,du\] \[=\frac{e^{-\frac{\nu\Delta^{2}}{2(\nu+t^{2})}}}{\sqrt{\nu}\, \sqrt{2\pi}\,2^{\nu/2}\Gamma\left(\frac{\nu}{2}\right)}\int_{0}^{\infty}u^{ \frac{\nu-1}{2}}\exp\left\{-\frac{1}{2}\left(\sqrt{u}\sqrt{\frac{\nu+t^{2}}{ \nu}}-\frac{\Delta t}{\sqrt{\nu+t^{2}}}\right)^{2}\right\}du.\]So, one has the _non-central \(t\) p.d.f._

\[f(t)=c_{\nu}(t^{2}+\nu)^{-(\nu+1)/2}\exp\left\{-\frac{\nu\Delta^{2}}{2(\nu{+}t^{2} )}\right\}\int_{0}^{\infty}\exp\left\{-\frac{1}{2}\left(x-\frac{\Delta t}{\sqrt {\nu{+}t^{2}}}\right)\right\}^{2}dx,\] (C.6)

with a change of variables \(u\to x=\sqrt{\frac{\nu+t^{2}}{\nu}}\,x\). Here

\[c_{\nu}=\frac{\nu^{\nu/2}}{\sqrt{\pi}\Gamma\left(\frac{\nu}{2}\right)\,2^{(\nu -1)/2}}\;.\]

The power function (C.4) may now be simply expressed as

\[\gamma(\mu;\sigma^{2})=\int_{t_{1-\alpha}(n-1)}^{\infty}f_{\nu,\Delta}(t)dt\quad \text{with }\nu=n-1,\,\Delta=\frac{\sqrt{n}(\mu-\mu_{0})}{\sigma}\,,\] (C.7)

(\(\mu>\mu_{0},\,\sigma^{2}>0\)).

_Example C.4_.: To test \(H_{0}:\sigma^{2}\leq\sigma_{0}^{2}\) against \(H_{1}:\sigma^{2}>\sigma_{0}^{2}\) based on i.i.d. \(\mathbf{N}(\mu,\sigma^{2})\) random variables \(X_{i}\) (\(\mu\) not known), the UMP unbiased test is to reject \(H_{0}\) iff \(\frac{\sum(X_{i}-\overline{X})^{2}}{\sigma_{0}^{2}}\equiv\frac{(n-1)s^{2}}{ \sigma_{0}^{2}}>\chi_{1-\alpha}^{2}(n-1)\). Its power is

\[P_{\mu,\sigma^{2}}\left(\frac{(n-1)s^{2}}{\sigma_{0}^{2}}>\chi_{ 1-\alpha}^{2}(n-1)\right) \equiv P_{\mu,\sigma^{2}}\left(\frac{(n-1)s^{2}}{\sigma^{2}}>\frac {\sigma_{0}^{2}}{\sigma^{2}}\,\chi_{1-\alpha}^{2}(n-1)\right)\] \[=\int_{\frac{\sigma_{0}^{2}}{\sigma^{2}}\chi_{1-\alpha}^{2}(n-1)}k _{n-1}(u)du\qquad(\sigma^{2}>\sigma_{0}^{2}).\]

_Example C.5 (The Non-Central Chi-Square Distribution)._ Finally, we consider the noncentral chi-square distribution. If \(Y\) is \(\mathbf{N}(\mu,1)\) then the distribution of \(Y^{2}\) is said to be a noncentral chi-square distribution with \(1\) d.f. The sum of squares \(\sum Y_{i}^{2}\) of \(n\) independent \(N(\mu_{i},1)\) normal random variables \(Y_{i}\) s said to have the _noncentral chi-square distribution with_ d.f. \(n\) and _noncentrality parameter_\(\sum_{1}^{n}\mu_{i}^{2}\).

**Proposition C.1**.: _Let \(Y_{i}\) be \(\mathbf{N}(\mu_{i},1)\), \(1\leq i\leq n\), and \(Y_{1},Y_{2},\ldots,Y_{n}\) independent. Then the p.d.f. of \(V:=\sum_{1}^{n}Y_{i}^{2}\) is given by the noncentral chi-square p.d.f. with d.f. \(n\) and noncentrality parameter \(\Delta=\sum_{1}^{n}\mu_{i}^{2}\), namely,_

\[f(v)\equiv f(v;n,\Delta)=\sum_{j=0}^{\infty}p\left(j;\frac{\Delta}{2}\right)k_ {n+2j}(v)\] (C.8)

_where_

\[p\left(j;\frac{\Delta}{2}\right)=e^{-\frac{\Delta}{2}}\left(\frac{\Delta}{2} \right)^{j}\frac{1}{j!}\qquad(j=0,1,2,\ldots)\] (C.9)

_is the probability (mass) function of a Poisson random variable with mean (parameter) \(\frac{\Delta}{2}\), and \(k_{n+2j}\) is the chi-square p.d.f. with d.f. \(n+2j\)._

Proof.: The p.d.f. of \(V_{i}:=Y_{i}^{2}\) is (use a two-to-one change of variables \(\{y,-y\}\to v=y^{2}\), or differentiate (w.r.t. \(\nu\)) the cumulative distribution function of \(V_{i}\), namely, \(\int_{-\sqrt{v}}^{\sqrt{v}}\frac{1}{\sqrt{2\pi}}\,e^{-\frac{(y-\mu_{i})^{2}}{ 2}}dy\)\[f_{i}(v) =\frac{1}{2\sqrt{2\pi}}\,v^{-\frac{1}{2}}\left(e^{-\frac{1}{2}v-\frac {1}{2}\mu_{i}^{2}}\right)\left\{e^{-\mu_{i}\sqrt{v}}+e^{\mu_{i}\sqrt{v}}\right\}\] \[=\frac{1}{2\sqrt{2\pi}}\,v^{-\frac{1}{2}}\left(e^{-\frac{1}{2}v} \cdot e^{-\frac{1}{2}\mu_{i}^{2}}\right)\left\{\sum_{j=0}^{\infty}\frac{2(\mu_ {i}^{2}v)^{j}}{(2j)!}\right\}\] \[=\sum_{j=0}^{\infty}\frac{1}{j!}\,e^{-\frac{\mu_{i}^{2}}{2}}\left( \frac{\mu_{i}^{2}}{2}\right)^{j}\cdot\frac{1}{\sqrt{2\pi}}\frac{j!}{(2j)!}\,2 ^{j}\cdot 2^{j-\frac{1}{2}}\left(\frac{v}{2}\right)^{j-\frac{1}{2}}e^{-\frac{v}{ 2}}\] \[=\sum_{j=0}^{\infty}p\left(j;\frac{\mu_{i}^{2}}{2}\right)\cdot \frac{1}{\sqrt{2\pi}}\,\frac{2^{j-\frac{1}{2}}}{(2j-1)(2j-3)\cdots 3.1}\,e^{-\frac{v}{ 2}}\left(\frac{v}{2}\right)^{j-\frac{1}{2}}\] \[=\sum_{j=0}^{\infty}p\left(j;\frac{\mu_{i}^{2}}{2}\right)\frac{1}{ 2\Gamma\left(j+\frac{1}{2}\right)}\left(\frac{v}{2}\right)^{\frac{2j-1}{2}}e^{ -\frac{v}{2}}\] \[=\sum_{j=0}^{\infty}p\left(j;\frac{\mu_{i}^{2}}{2}\right)k_{2j+1}( v).\]

The p.d.f. of \(Y_{1}^{2}+Y_{2}^{2}\) is the convolution of \(f_{1}(v):=f(v;1,\mu_{1}^{2})\) and \(f_{2}(v):=f(v;1,\mu_{2}^{2})\), namely,

\[(f_{1}*f_{2})(v) =\sum_{j=0}^{\infty}\sum_{j^{\prime}=0}^{\infty}p\left(j;\frac{ \mu_{1}^{2}}{2}\right)p\left(j^{\prime};\frac{\mu_{2}^{2}}{2}\right)(k_{1+2j}* k_{1+2j^{\prime}})\,(v)\] \[=\sum_{j=0}^{\infty}\sum_{j^{\prime}=0}^{\infty}\frac{e^{-(\mu_{1 }^{2}+\mu_{2}^{2})/2}}{j!j^{\prime}!}\left(\frac{\mu_{1}^{2}}{2}\right)^{j} \left(\frac{\mu_{2}^{2}}{2}\right)^{j^{\prime}}k_{2+2(j+j^{\prime})}(v).\]

For the convolution of two (central) chi-square p.d.f. s is a chi-square p.d.f. whose d.f. is the sum of the d.f.'s of its two components. This equals

\[\sum_{s=0}^{\infty}\left\{\sum_{(j,j^{\prime}):j+j^{\prime}=s}e^ {-(\mu_{1}^{2}+\mu_{2}^{2})/2}\cdot\frac{1}{j!j^{\prime}!}\left(\frac{\mu_{1}^ {2}}{2}\right)\left(\frac{\mu_{2}^{2}}{2}\right)^{j^{\prime}}\right\}k_{2+2s}(v)\] \[=\sum_{s=0}^{\infty}e^{-(\mu_{1}^{2}+\mu_{2}^{2})/2}\frac{(\mu_{1 }^{2}+\mu_{2}^{2})^{s}}{2^{s}s!}\,k_{2+2s}(v)=f(v;2,\mu_{1}^{2}+\mu_{2}^{2}).\] (C.10)

For the last step in (C.10), one uses the combinatorial identity:

\(\sum_{\{(j,j^{\prime})\in(\mathbb{Z}^{+})^{2}:j+j^{\prime}=s\}}a^{j}\,b^{j^{ \prime}}/j!j^{\prime}!=\frac{(a+b)^{s}}{s!}\). _(Binomial expansion of \((a+b)^{s}\))._

The general result follows by induction. 

Remark Remark C.1: The non-central chi-square appears in the _power function of \(F\)-tests_ in linear models.

The final example in this section enables one to compute the power of \(F\) tests in linear models.

Example C.6 (The Non-Central \(F\) Distribution \(\mathscr{F}_{r,s}(\Delta)\)): Recall the \(F\)-distribution \(\mathscr{F}_{r,s}\) in A.2, VIII. This is the distribution of the \(F\)-statistic

\[F=\frac{U/r}{V/s}\,,\] (C.11)where \(U\) and \(V\) are independent chi-square random variables with degrees of freedom \(r\), \(s\), respectively. If we let \(U\) be a _noncentral chi-square distribution_ with d.f. \(r\) and noncentrality parameter \(\varDelta\), then the Proposition above (Proposition C.1) says that \(U\) may be thought of as a chi-square random variable with a _random degree of freedom_\(r+2\gamma\), \(\gamma\) having the Poisson distribution \(\mathscr{P}(\varDelta/2)\) (with mean \(\varDelta/2\)). Therefore, conditionally given \(\gamma=j\), the distribution of \((U/(r+2j))/(V/s)\) is the (central) \(F\)-distribution \(\mathscr{F}_{r+2j,s}\), with p.d.f. \(f_{r+2j,s}\) (see Sect. A.2). Hence the p.d.f. of \((U/r)/(V/s)=\left(\frac{r+2j}{r}\right)[(U/(r+2j))/(V/s)]\), conditionally given \(\gamma=j\), is

\[f_{r+2j,s}\left(\frac{r}{r+2j}\,x\right)\left(\frac{r}{r+2j}\right)\qquad(j=0, 1,\dots).\] (C.12)

Therefore, the density of the non-central \(F\)-statistic (C.11) is

\[f_{r,s,\varDelta}(x)=\sum_{j=0}^{\infty}e^{-\varDelta/2}\left(\frac{\varDelta} {2}\right)^{j}\frac{1}{j!}\left(\frac{r}{r+2j}\right)f_{r+2j,s}\left(\frac{r} {r+2j}\,x\right),\quad 0<x<\infty.\] (C.13)

By using the formula for \(f_{r+2j,s}\) one gets

\[f_{r,s,\varDelta}(x)=e^{-\varDelta/2}\sum_{j=0}^{\infty}\left(\frac{\varDelta} {2}\right)^{j}\frac{1}{j!}\,\frac{r^{\frac{r}{2}+j}s^{\frac{s}{2}}\varGamma \left(\frac{r+s}{2}+j\right)}{\varGamma\left(\frac{r}{2}+j\right)\varGamma \left(\frac{s}{2}\right)}\cdot\frac{x^{\frac{r}{2}+j-1}}{(s+rx)^{\frac{r+s}{2} +j}}\.\] (C.14)

For the use of the N-P Lemma to an observed random variable \(X\) given by (C.11), one considers the ratio

\[\frac{f_{r,s,\varDelta}(x)}{f_{r,s}(x)}=e^{-\frac{\varDelta}{2}}\sum_{j=0}^{ \infty}\left(\frac{\varDelta}{2}\right)^{j}\frac{1}{j!}\,\frac{\varGamma\left( \frac{r+s}{2}+j\right)\varGamma\left(\frac{r}{2}\right)}{\varGamma\left( \frac{r}{2}+j\right)\varGamma\left(\frac{r+s}{2}\right)}\left(\frac{rx}{s+rx} \right)^{j}.\] (C.15)

Each summand is an increasing function of \(x\) and, hence, so is the sum.

## Appendix D Liapounov's, Lindeberg's and Polya's Theorems

#### 1.0.1 Liapounov's Central Limit Theorem

_Let \(X_{j,n}\)\((1\leq j\leq k_{n}\); \(n=1,2,\dots)\) be a triangular array of independent random variables each with zero mean. Write \(s_{n}^{2}=\Sigma_{j=1}^{k_{n}}\mathrm{var}(X_{j,n})\), \(\rho_{3,n}=\Sigma_{j=1}^{k_{n}}E|X_{j,n}|^{3}\). If \(\rho_{3,n}/s_{n}^{3}\to 0\) as \(n\to\infty\), then_

\[\sum_{j=1}^{k_{n}}\frac{X_{j,n}}{s_{n}}\xrightarrow{\mathscr{L}}N(0,1).\] (D.1)

Liapounov's Theorem follows from a more general theorem due to Lindeberg:

#### 1.0.2 Lindeberg's Central Limit Theorem

1 Let the triangular array \(X_{j,n}\)\((1\leq j\leq k_{n}\); \(n=1,2,\dots)\) have mean zero and finite variance. Write \(s_{n}^{2}=\sum_{j=1}^{k_{n}}\mathrm{var}\,X_{j,n}\), \(\varepsilon_{j,n}=X_{j,n}/s_{n}\). If the _Lindeberg condition_

\[\sum_{j=1}^{k_{n}}E\varepsilon_{j,n}^{2}\mathbf{1}_{[|\varepsilon_{j,n}|> \eta]}\longrightarrow 0\quad\text{as $n\to\infty$}\] (D.2)

holds for every constant \(\eta>0\), then

\[\sum_{j=1}^{k_{n}}\varepsilon_{j,n}\xrightarrow{\mathscr{L}}N(0,1).\] (D.3)

#### 1.0.3 Polya's Theorem

_Suppose \(F_{n}\xrightarrow{\mathscr{L}}F\), and \(F(x)\) is continuous on \((-\infty,\infty)\). Then \(\sup_{x}|F_{n}(x)-F(x)|\to 0\) as \(n\to\infty\)._

Proof.: We need to show that, under the given hypotheses, given any \(\varepsilon>0\) there exists an integer \(n(\varepsilon)\) such that

\[\sup_{x}|F_{n}(x)-F(x)|<\varepsilon\qquad\text{for all $n>n(\varepsilon)$}.\]Find \(A_{\varepsilon}>0\) such that \(F(-A_{\varepsilon})<\varepsilon/4,\,1-F(A_{\varepsilon})<\varepsilon/4.\) Since \(F_{n}(-A_{\varepsilon})\to F(-A_{\varepsilon})\) and \(F_{n}(A_{\varepsilon})\to F(A_{\varepsilon}),\) there exists a positive integer \(N_{1}\) such that \(\forall\ n>N_{1},\)\(|F_{n}(-A_{\varepsilon})-F(-A_{\varepsilon})|<\varepsilon/4,\)\(|1-F_{n}(A_{\varepsilon})-\{1-F(A_{\varepsilon})\}|=|F_{n}(A_{\varepsilon})-F(A_{ \varepsilon})|<\varepsilon/4.\) This implies,

\[F_{n}(-A_{\varepsilon})<\frac{\varepsilon}{4}+\frac{\varepsilon}{4}=\frac{ \varepsilon}{2},\quad 1-F_{n}(+A_{\varepsilon})<\frac{\varepsilon}{4}+\frac{ \varepsilon}{4}=\frac{\varepsilon}{2}\quad\forall\ n>N_{1}.\] (D.4)

Since \(F\) is continuous, it follows that it is _uniformly_ continuous on \([-A_{\varepsilon},A_{\varepsilon}].\) Hence there exists \(\delta>0\) such that \(|F(x)-F(y)|<\varepsilon/2,\) whenever \(x,y\in[-A_{\varepsilon},A_{\varepsilon}]\) and \(|x-y|<\delta.\) Choose points \(x_{0}=-A_{\varepsilon}<x_{1}<x_{2}<\cdots<x_{k}=A_{\varepsilon}\) such that \(|x_{j+1}-x_{j}|<\delta.\) There exists, for each \(j,\) an integer \(M_{j}\) such that \(|F_{n}(x_{j})-F(x_{j})|<\varepsilon/2\) for all \(n>M_{j}\) (\(j=0,1,\ldots,k).\) Now let

\[n(\varepsilon)=\max\{N_{1},M_{0},M_{1},\ldots,M_{k}\}.\] (D.5)

Then if \(x\in[x_{j},x_{j+1}],\) one has \(\forall\ n>M_{j+1}\)

\[F_{n}(x)-F(x) \leq F_{n}(x_{j+1})-F(x_{j})\] \[= F_{n}(x_{j+1})-F(x_{j+1})+F(x_{j+1})-F(x_{j})<\frac{\varepsilon }{2}+\frac{\varepsilon}{2}=\varepsilon,\]

and \(\forall\ n>M_{j}\) one has

\[F(x)-F_{n}(x)\leq F(x_{j+1})-F_{n}(x_{j})=F(x_{j+1})-F(x_{j})+F(x_{j})-F_{n}(x _{j})<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon,\]

i.e.

\[|F_{n}(x)-F(x)|<\varepsilon,\quad\forall\ n>n(\varepsilon),\ \text{if}\ x\in[-A_{ \varepsilon},A_{\varepsilon}].\] (D.6)

On the other hand, if \(x<-A_{\varepsilon},\) then \(\forall\ n>N_{1}\)

\[|F_{n}(x)-F(x)|\leq F_{n}(x)+F(x)\leq F_{n}(-A_{\varepsilon})+F(-A_{ \varepsilon})<\frac{\varepsilon}{2}+\frac{\varepsilon}{4}<\varepsilon,\] (D.7)

while if \(x>A_{\varepsilon},\) then for all \(n>N_{1}\)

\[|F_{n}(x)-F(x)| = |1-F_{n}(x)-\{1-F(x)\}|\leq 1-F_{n}(x)+1-F(x)\] (D.8) \[\leq 1-F_{n}(A_{\varepsilon})+1-F(A_{\varepsilon})<\frac{ \varepsilon}{2}+\frac{\varepsilon}{4}<\varepsilon.\]

Combining (D.6)-(D.8) one has

\[|F_{n}(x)-F(x)|<\varepsilon\quad\forall\ x\in(-\infty,\infty)\ \text{if}\ n>n( \varepsilon).\]

## Solutions of Selected Exercises in Part I

Let us index the \(N_{i}\) members of the \(i^{th}\) stratum as \(\{x_{ij}:1\leq j\leq N_{i}\}\), \(i=1,2,\ldots,k\). Also, let \(X_{ij}\) (\(j=1,\ldots,n_{i}\)) denote a random sample of size \(n_{i}\) (with replacement) from the \(i^{th}\) stratum (\(i=1,\ldots,k\)). Then, \(E(\overline{X}_{i})=m_{i}\equiv\frac{1}{N_{i}}\sum_{j=1}^{N_{i}}x_{ij}=\) mean of the \(i^{th}\) stratum, \(\operatorname{var}(\overline{X}_{i})=\frac{v_{i}}{n_{i}}\) where \(v_{i}=\frac{1}{N_{i}}\sum_{j=1}^{N_{i}}(x_{ij}-m_{i})^{2}=\) variance of the \(i^{th}\) stratum.

1. Let \(\overline{Y}=\sum_{i=1}^{k}w_{i}\overline{X}_{i}\) (\(w_{i}=\frac{N_{i}}{N}\)). Then (i) \(E\overline{Y}=\sum_{i=1}^{k}w_{i}E(\overline{X}_{i})=\sum_{i=1}^{k}w_{i}m_{i}= \sum_{i=1}^{k}\frac{N_{i}}{N}\left(\frac{1}{N_{i}}\sum_{j=1}^{N_{i}}x_{ij} \right)=\frac{1}{N}\sum_{i=1}^{k}\sum_{j=1}^{N_{i}}x_{ij}=\frac{x_{1}+x_{2}+ \cdots+x_{N}}{N}=m\) and (ii) \[\operatorname{var}(\overline{Y})=\sum_{i=1}^{k}w_{i}^{2}\operatorname{var}( \overline{X}_{i})=\sum_{i=1}^{k}w_{i}^{2}\frac{v_{i}}{n_{i}}\,.\] (S.1)
2. One may express the population variance as \[v =\frac{1}{N}\sum_{i=1}^{N}(x_{i}-m)^{2}=\frac{1}{N}\sum_{i=1}^{k} \sum_{j=1}^{N_{i}}(x_{ij}-m)^{2}\] \[=\frac{1}{N}\sum_{i=1}^{k}\sum_{j=1}^{N_{i}}(x_{ij}-m_{i}+m_{i}-m )^{2}\] \[=\frac{1}{N}\sum_{i=1}^{k}\xi\sum_{j=1}^{N_{i}}(x_{ij}-m_{i})^{2} +\sum_{j=1}^{N_{i}}(m_{i}-m)^{2}+2(m_{i}-m)\sum_{j=1}^{N_{i}}(x_{ij}-m_{i})\] \[=\frac{1}{N}\sum_{i=1}^{k}\left\{N_{i},v_{i}+N_{i}(m_{i}-m)^{2}+ 0\right\}=\sum_{i=1}^{k}\frac{N_{i}}{N}v_{i}+\sum_{i=1}^{k}\frac{N_{i}}{N}(m_ {i}-m)^{2}\] \[=\sum_{i=1}^{k}w_{i}v_{i}+\sum_{i=1}^{k}w_{i}(m_{i}-m)^{2}.\] (S.2)3. Let \(n_{i}=nw_{i}\)\(\forall\)\(i=1,\ldots,k\). Then, using (S.1) and (S.2). \[E(\overline{Y}-m)^{2}=\sum_{i=1}^{k}w_{i}^{2}.\frac{v_{i}}{nw_{i}}=\frac{1}{n} \sum_{i=1}^{k}w_{i}v_{i},\] \[E(\overline{X}-m)^{2} =\frac{v}{n}=\frac{1}{m}\sum_{i=1}^{k}w_{i}v_{i}+\frac{1}{n}\sum_ {i=1}^{k}w_{i}(m_{i}-m)^{2}\] \[>E(\overline{Y}-m)^{2},\] unless \(m_{i}=m\)\(\forall\)\(i=1,\ldots,k\). Only in the latter case \(E(\overline{Y}-m)^{2}=E(\overline{X}-m)^{2}\).
4. Suppose \(v_{i}\)'s are known. Then \(E(\overline{Y}-m)^{2}\) is minimized (with respect to \(n_{i}\)'s) by solving the equations [see (S.1)] \[\frac{\partial}{\partial n_{i}}\left\{\sum_{i=1}^{k}w_{i}^{2}\frac{v_{i}}{n_{ i}}+\lambda\left(\sum_{i^{\prime}=1}^{k}n_{i^{\prime}}\right)\right\}=0\quad(i=1, \ldots,k),\] where \(\lambda\) is the so-called Lagrange multiplier. That is, \(-w_{i}^{2}\frac{v_{i}}{n_{i}^{2}}+\lambda=0\), or \(w_{i}^{2}\frac{v_{i}}{n_{i}^{2}}=\lambda\), or, \(n_{i}^{2}=\frac{w_{i}^{2}v_{i}}{\lambda}\), or, \(n_{i}=\frac{w_{i}\sqrt{v_{i}}}{\sqrt{x}}\)\((i=1,2,\ldots,k)\). Summing over \(i\), one gets \(n=\frac{\sum_{i=1}^{k}w_{i}\sqrt{v_{i}}}{\sqrt{\lambda}}\), or, \(\sqrt{\lambda}=\frac{\sum_{i=1}^{k}w_{i}\sqrt{v_{i}}}{n}\). Hence the optimal choice for \(n_{i}\) is \[n_{i}=\frac{nw_{i}\sqrt{v_{i}}}{\sum_{i=1}^{k}w_{i}\sqrt{v_{i}}}\quad(i=1,2, \ldots,k).\] Thus, \(n_{i}\) is proportional to the size of the stratum and to the standard deviation of the stratum. 

## Chapter 2

In Example 2.1, p. 12, prove (2.4) and (2.6).

**Proof of (2.4).** Recall that one can express \(\frac{(n-1)s^{2}}{\sigma^{2}}\) as

\[\frac{(n-1)s^{2}}{\sigma^{2}}=\sum_{i=2}^{n}Y_{i}^{2}\quad(Y_{i}\ \ \mbox{i.i.d.}\ N(0,1)).\]

Then

\[E\left(\frac{(n-1)s^{2}}{\sigma^{2}}\right)^{2}=E\frac{(n-1)^{2}s^{4}}{\sigma^ {4}}=\frac{(n-1)^{2}}{\sigma^{4}}\,Es^{4}=E\left(\sum_{i=2}^{n}Y_{i}^{2}\right) ^{2}.\] (S.3)But the best expectation equals (using \(EY_{1}^{4}=3\)),

\[E\left(\sum_{i=2}^{n}Y_{i}^{2}\right)^{2} =E\left(\sum_{i=2}^{n}Y_{i}^{4}\right)+E\left(\sum_{2\leq i\neq j \leq n}Y_{i}^{2}Y_{j}^{2}\right)\] \[=\sum_{i=2}^{n}EY_{i}^{4}+\sum_{2\leq i\neq j\leq n}(EY_{i}^{2})(EY _{j}^{2})=3(n-1)+(n-1)(n-2)\] \[=(n-1)(3+n-2)=(n-1)(n+1).\]

Hence the last equality in (S.3) yields

\[Es^{4}=\frac{\sigma^{4}}{(n-1)^{2}}(n-1)(n+1)=\frac{n+1}{n-1}\sigma^{4},\]

so that

\[E_{\theta}(s^{2}-\sigma^{2})=E_{\theta}s^{4}=\sigma^{4}-2\sigma^{2}E_{\theta}s ^{2}=E_{\theta}s^{4}-\sigma^{4}=\frac{2\sigma^{4}}{n-1}.\]

**Proof of (2.6).** Here \(\theta=(\mu,\sigma^{2})\), \(d({\bf X})=(\overline{X},s^{2})\). Therefore, \(R(\theta,d)=E_{\theta}|\theta-d({\bf X})|^{2}=E_{\theta}[(\overline{X}-\mu)^{2 }+(s^{2}-\sigma^{2})^{2}]=\frac{\sigma^{2}}{n}+\frac{2\sigma^{4}}{n-1}\).

## Chapter 3

**3.6**: \(X_{1},\ldots,X_{n}\) i.i.d. Bernoulli, \(P_{\theta}(X_{i}=1)=\theta\), \(P_{\theta}(X_{i}=0)=1-\theta\), \(\theta\in[0,1]=\Theta\). the (joint) distribution of \({\bf X}=(X_{1},\ldots,X_{n})\) is

\[P_{\theta}({\bf X}={\bf x})=\theta^{r}(1-\theta)^{n-r}\quad(r=\sum_{j=1}^{n}x_{ j},\,{\bf x}=(x_{1},\ldots,x_{n})\in\{0,1\}^{n}).\]

By Example 3.5, the Bayes estimator for the beta prior \(\mathscr{B}_{e}(\alpha,\beta)\) under loss \(L(\theta,a)=C(\theta-a)^{2}\) (\(c>0\) does not depend on \(\theta\)) is given by

\[d_{o}({\bf x})=\frac{r+\alpha}{n+\alpha+\beta}\,.\]

Its risk function is

\[R(\theta,d_{0}) =cE_{\theta}\left(\varepsilon-\frac{\sum_{1}^{n}X_{j}+\alpha}{n+ \alpha+\beta}\right)^{2}\] \[=c\left[\operatorname{var}_{\theta}(d_{0}({\bf X}))+(E_{\theta}d_ {0}({\bf X})-\theta)^{2}\right]\] \[=c\left[\frac{n\theta(1-\theta)}{n+\alpha+\beta^{2}}+\left(\frac {n\theta+\alpha}{n+\alpha+\beta}-\theta\right)^{2}\right]\] \[=c\left[\frac{n\theta(1-\theta)}{(n+\alpha+\beta)^{2}}+\frac{ \alpha+\theta(\alpha+\beta)^{2}}{(n+\alpha+\beta)^{2}}\right]\] \[=\frac{c}{(n+\alpha+\beta)^{2}}[n\theta(1-\theta)+(\alpha-\theta (\alpha+\beta))^{2}]\]For \(\alpha=\beta=\frac{\sqrt{n}}{2}\), this simplifies to

\[R(\theta,d_{0})=\frac{c\frac{n}{4}}{(n+\sqrt{n})^{2}}=\frac{c}{4(\sqrt{n}+1)^{2} }\,,\]

a Bayes rule which is an equalizer rule. Hence it is **minimax** (Theorem 3.7). Also, the property P1 (p. 29) holds with \(\widetilde{\Theta}=(0,1)\). Hence \(d_{0}\) is admissible,

\[d_{0}(\mathbf{X})=\left(\dot{\sum_{1}}\,X_{j_{0}}+\frac{\sqrt{n}}{2}/(n+\sqrt{ n})\right)=\frac{\overline{X}+\frac{1}{2\sqrt{n}}}{1+\frac{1}{\sqrt{n}}}\,.\]

**3.7**: \(X_{1},\ldots,X_{n}\) i.i.d. Poisson, with common pmf \(P_{\theta}(X_{i}=x)=e^{-\theta}\theta^{x}/x!\), \(x\in\{0,1,2,\ldots\}=\mathbb{Z}_{+}\), \(\theta\in(0,\infty)=\Theta\). The (joint) pmf of \(\mathbf{X}=(X_{1},\ldots,X_{n})\) is

\[f(\mathbf{x}\mid\theta)=e^{-n\theta}\theta^{\sum_{1}^{n}x_{j}}/\prod_{j=1}^{n }x_{j!}\quad(\mathbf{x}=(x_{1},\ldots,x_{n})\in\mathbb{Z}_{+}^{n}=\mathscr{H}).\]

1. Let \(L(\theta,a)=\frac{e^{a}}{\theta}(\theta-a)^{2}\). For the gamma prior \(\tau=\mathscr{G}(\alpha,\beta)\), the Bayes risk of an estimator \(d\) is \[r(\tau,d) =\sum_{\mathbf{x}\in\mathscr{X}}\frac{1}{\prod x_{j!}}\int_{0}^{ \infty}\frac{e^{\theta}}{\theta}(\theta-d(\mathbf{x}))^{2}e^{-n\theta}\theta^{ \sum x_{j}}\] \[=\sum_{\mathbf{x}\in\mathscr{X}}\frac{1}{\prod x_{j}!}\int_{0}^{ \infty}\frac{1}{\Gamma(\beta)\alpha^{\beta}}(\theta^{2}-2\theta d(\mathbf{x})+ d^{2}(\mathbf{x}))e^{-\theta/\frac{\alpha}{(n-1)\alpha+1}}\cdot\theta^{\sum x_{j}+ \beta-2}d\theta.\] If \(\mathbf{x}=(0,\ldots,\theta)\) then the integral diverges (at \(0\)) if \(\beta\leq 1\) (\(\beta>0\)), unless one sets \(d((0,\ldots,0))=0\). For all other \(\mathbf{x}\) (and for \(\mathbf{x}=(0,\ldots,0)\) if \(\beta>1\)), one has the integral equal to \[\frac{1}{\Gamma(\beta)\alpha^{\beta}}\cdot\Gamma(\Sigma x_{j}+\beta-1)\alpha^ {\prime\beta^{\prime}}\cdot\int_{0}^{\infty}(\theta-d(\mathbf{x}))^{2}g( \theta\mid\alpha^{\prime},\beta^{\prime})d\theta,\] where \(g(\theta\mid\alpha^{\prime},\beta^{\prime})\) is the pdf of the gamma distribution \(\mathscr{G}(\alpha^{\prime},\beta^{\prime})\), with \(\alpha^{\prime}=\frac{\alpha}{(n-1)\alpha+1}\), \(\beta^{\prime}=\sum_{1}^{n}x_{j}+\beta-1\). This is minimized by the mean of this gamma distribution (for each \(\mathbf{x}\), if \(\beta>1\)), i.e., the Bayes estimator is given by \[d_{0}(\mathbf{x})=\alpha^{\prime}\beta^{\prime}=\frac{\alpha}{(n-1)\alpha+1} \left(\sum_{1}^{n}x_{j}+\beta-1\right).\]
2. For the case \(\beta=1\), this becomes \[d_{0}(\mathbf{x})=\frac{\alpha}{(n-1)\alpha+1}\sum_{1}^{n}x_{j},\quad\mathbf{ x}\in\mathscr{X}=\mathbb{Z}_{+}^{n},\] (S.4) which automatically satisfies the restriction \(d_{0}((0,\ldots,0))=0\) imposed earlier. If one takes \(\alpha=1\), \(\beta=1\), then (S.4) becomes \[d_{0}(\mathbf{x})=\overline{x}.\]Hence \(\overline{x}\) is admissible under the loss function \(L(\theta,a)=\frac{e^{\theta}}{\theta}(\theta-a)^{2}\). [Note that \(P_{1}\) holds with \(\widetilde{\Theta}=\Theta\), since \(P_{\theta}(\mathbf{x})>0\ \forall\ \mathbf{x}\), and \(P_{\theta_{0}}(A)=0\) implies \(A\) is empty, whatever be \(\theta_{0}\in\Theta\).]
3. We now show that \(d_{0}=\overline{X}\) is admissible under squared error loss \(L(\theta,a)\)=(\(\theta-a)^{2}\). Let \(\widetilde{L}(\theta,a)=\frac{e^{\theta}}{\theta}(\theta-a)^{2}\equiv\frac{e^{ \theta}}{\theta}L(\theta,a)\). Suppose \(d_{0}=\overline{X}\) is not admissible under \(L\). Then there exists an estimator \(d_{1}\) such that \[R(\theta,d_{1})\leq R(\theta,d_{0})\ \forall\ \theta,\quad R(\theta_{0},d_{1})<R( \theta_{0},d_{0})\] for some \(\theta_{0}>0\). Multiplying both sides of the inequalities by \(\frac{e^{\theta}}{\theta}\), one obtains the risk function \(\widetilde{R}(\theta,d_{1})\), \(\widetilde{R}(\theta,d_{0})\) under \(\widetilde{L}\) satisfying the inequalities \[\widetilde{R}(\theta,d_{1})\leq\widetilde{R}(\theta,d_{0})\ \forall\ \theta, \quad\widetilde{R}(\theta_{0},d_{1})<\widetilde{R}(\theta_{0},d_{0}).\] But this implies \(d_{0}\) is inadmissible under loss \(\widetilde{L}\)--a contradiction.
4. We now show that \(\overline{X}\) is minimax under the loss function \(L(\theta,a)=(\theta-a)^{2}/\theta\). The risk function of \(\overline{X}\) is \[R(\theta,\overline{X})=E_{\theta}(\theta-\overline{X})^{2}/\theta=\frac{1}{ \theta}\operatorname{var}_{\theta}(\overline{X})=\frac{1}{\theta}n\theta= \frac{1}{\theta}\ \forall\ \theta\in\Theta=(0,\infty).\] [Note that the variance of \(X_{1}\) is \(\theta\).] Thus \(\overline{X}\) is an equalizer rule. We will now apply Theorem 3.6 to find a sequence of priors \(\tau_{N}\) such that the Bayes risks of the corresponding Bayes estimators \(d_{N}\), say, satisfy \[\lim_{N\to\infty}r(\tau_{N},d_{N})=\frac{1}{n}\,.\] (S.5) Now for the gamma prior \(\mathscr{G}(\alpha,\beta)\), the Bayes estimator is obtained by minimizing the Bayes risk \(r(\tau,d)\) over the class of all estimators \(d\): \[r(\tau,d) =\sum_{\mathbf{x}\in\mathscr{X}}\frac{1}{\prod_{1}^{n}x_{j!}} \int_{0}^{\infty}\frac{(\theta-d(\mathbf{x}))^{2}}{\theta}\,e^{-n\theta} \theta^{\sum_{i}x_{j}}\frac{\theta^{\beta-1}e^{-\theta/d}}{\Gamma(\beta)\alpha ^{\beta}}\,d\theta\] \[=\frac{1}{\Gamma(\beta)\alpha^{\beta}}\sum_{\mathbf{x}\in \mathscr{X}}\frac{1}{\prod_{1}^{n}x_{j!}}\int_{0}^{\infty}(\theta-d(\mathbf{x} ))^{2}e^{-\theta/\frac{\alpha}{n\alpha+1}}\theta^{\sum x_{j}+\beta-2}d\theta\] \[=\frac{1}{\Gamma(\beta)\alpha^{\beta}}\cdot\Gamma(\beta^{\prime} )\alpha^{\prime\beta^{\prime}}\sum_{\mathbf{x}\in\mathscr{X}}\frac{1}{\prod_{ 1}^{n}x_{j!}}\int_{0}^{\infty}(\theta-d(\mathbf{x}))^{2}g(\theta\mid\alpha^{ \prime},\beta^{\prime})d\theta,\] where \(g(\theta\mid\alpha^{\prime},\beta^{\prime})\) is the pdf of the gamma distribution \(\mathscr{G}(\alpha^{\prime},\beta^{\prime})\), and \(\alpha^{\prime}=\frac{\alpha}{n\alpha+1}\), \(\beta^{\prime}=\sum_{1}^{n}x_{j}+\beta-1\).

Let us choose \(\beta>1\). Then, as in (b), the last integral is minimized (for each \(\mathbf{x}\in\mathscr{X}\)) by \(\alpha^{\prime}\beta^{\prime}=\frac{\alpha(\sum_{1}^{n}x_{j}+\beta-1)}{n\alpha +1}\). Hence the Bayes estimator for the prior \(\mathscr{G}(\alpha,\beta)\), \(\beta>1\), is

\[d_{0}(\mathbf{x})=\frac{\alpha(\sum_{j=1}^{n}x_{j}+\beta-1)}{n\alpha+1}\,.\]

We now show that for \(\alpha=\alpha_{N}\), \(\beta=\beta_{N}\) with \(\alpha_{N}\uparrow\infty\) and \(\beta_{N}\downarrow 1\) as \(N\uparrow\infty\), the Bayes estimator \(d_{N}\) for the prior \(\tau_{N}=\mathscr{G}(\alpha_{N},\beta_{N})\) has Bayes risks \(r(\tau_{N},d_{N})\) satisfying (S.5). Now for the prior \(\mathscr{G}(\alpha,\beta)\), \(\beta>1\), the Bayes risk is \[r(\tau,d_{0}) =E\left[\left(\frac{\alpha(\sum_{1}^{n}X_{j}+\beta-1)}{n\alpha+1}- \mathscr{O}\right)^{2}/\mathscr{O}\right]=E\left[\mathscr{O}-2d_{0}(\mathbf{X})+ \frac{d_{0}^{2}(\mathbf{X})}{\mathscr{O}}\right]\] \[=\alpha\beta-2E\left[\frac{\alpha(n\theta+\beta-1)}{n\alpha+1} \right]+E\left[\frac{\text{var}_{\mathscr{O}}(d_{0}(\mathbf{X}))+(E_{\mathscr{ O}}d_{0}(\mathbf{X}))^{2}}{\mathscr{O}}\right]\] \[=\alpha\beta-2\left[\frac{\alpha(n\alpha+1\beta-1)}{n\alpha+1}\right]\] \[\quad\quad+E\left[\left(\frac{\alpha}{n\alpha+1}\right)^{2}\frac{ n\mathscr{O}}{\mathscr{O}}+\left(\frac{\alpha}{n\alpha+1}\right)^{2}\frac{(n \mathscr{O}+\beta-1)^{2}}{\mathscr{O}}\right]\] \[=\alpha\beta-2\alpha[(n\alpha+1)\beta-1]\] \[\quad\quad+n\left(\frac{\alpha}{n\alpha+1}\right)^{2}+\left(\frac {\alpha}{n\alpha+1}\right)^{2}\left[n^{2}E(\mathscr{O})+2(\beta-1)+\frac{( \beta-1)^{2}}{\mathscr{O}}\right]\] \[=\alpha\beta-2\alpha\beta+\frac{2\alpha}{n\alpha+1}+n\left(\frac {\alpha}{n\alpha+1}\right)^{2}\] \[\quad\quad+\left(\frac{\alpha}{n\alpha+1}\right)^{2}\left[n^{2} \alpha\beta+2(\beta-1)+\frac{\beta-1}{\alpha}\right].\]

Note that, for \(\beta>1\), \(E(1/\mathscr{O})=\frac{1}{\Gamma(\beta)\alpha^{\beta}}\int_{0}^{\infty}\frac{1 }{\theta}e^{-\theta/\alpha}\theta^{\beta-1}d\theta=\frac{\Gamma(\beta-1)\alpha ^{\beta-1}}{\Gamma(\beta)\alpha^{\beta}}=\frac{1}{\alpha(\beta-1)}\). Also use \((1+\frac{1}{n\alpha})^{-2}=1-\frac{2}{n-\alpha}+\theta(\frac{1}{\alpha^{2}})\) as \(\alpha\rightarrow\infty\) to get

\[r(\tau,d_{0}) = \alpha\beta-2\alpha\beta+\frac{2\alpha}{n\alpha+1}+n\left(\frac{1 }{n+\frac{1}{\alpha}}\right)^{2}+\left(1+\frac{1}{n\alpha}\right)^{2}\left[ \alpha\beta+\frac{2(\beta-1)}{n^{2}}+\frac{\beta-1}{n^{2}\alpha}\right]\] \[= \alpha\beta-2\alpha\beta+\frac{2}{n}(1+o(1))+n\left(\frac{1}{n^{2 }}+o(1)\right)\] \[\quad+\alpha\beta-\frac{2\beta}{n}+\frac{2(\beta-1)}{n^{2}}+o(1) \text{ (as }\alpha\rightarrow\infty)\] \[= \frac{2}{n}+\frac{1}{n}-\frac{2\beta}{n}+\frac{2(\beta-1)}{n^{2}} +o(1)\text{ as }\alpha\rightarrow\infty.\] \[\longrightarrow\frac{1}{n}\text{ as }\,\alpha\rightarrow\infty, \beta\downarrow 1.\]

## Chapter 4

1. Find the UMVU estimator of \(\theta^{i}\) in Example 4.5 (\(k=1,2,\ldots,n\)).

**Solution.** The complete sufficient statistic for \(\theta\) is \(T=\sum_{1\leq j\leq n}X_{j}\). A simple unbiased estimator of \(\theta^{k}\) is \(d(\mathbf{X})=X_{1}X_{2},\ldots,X_{k}\), which takes the value \(1\) with probability \(\theta^{k}\) (when \(=X_{1}=1\), \(X_{2}=1,\ldots,X_{k}=1\)), and \(0\) with probability \(1-\theta^{k}\). The UMVU estimate is then \[d^{*}({\bf X})=E(X_{1},X_{2},\ldots,X_{k}\mid T)=P(X_{1},X_{2},\ldots,X_{k}=1\mid T).\]

The last probability is zero if \(T<k\), and, for \(T=r\geq k\), it equals

\[\begin{array}{l}\frac{P(X_{1},X_{2},\ldots,X_{k}{=}1,T{=}r)}{P(T{=}r)}=\theta ^{k}\ ^{n-k}C_{r-k}\theta^{r-k}(1{-}\theta)^{n-k-(r-k)}/\left[{}^{n}C_{r}\theta^{r}( 1-\theta)^{n-r}\right]\\ =\frac{{}^{n-k}C_{r-k}}{{}^{n}C_{r}}.\end{array}\]

Hence, \(d^{*}({\bf X})=0\) for \(T=\sum_{1\leq j\leq n}X_{j}=0,1,\ldots,k-1\), and it equals \(r(r-1)\ldots(r-k+1)/[n(n-1)\ldots(n-k+1)]\), for \(T=r\) where \(r=k\), \(k+1,\ldots,n\). This can be succinctly expressed as

\[d^{*}({\bf X})=\frac{T(T-1)\ldots(T-k+1)}{[n(n-1)\ldots(n-k+1)]}.\]

(b) (i): For \(X_{1},\ldots,X_{n}\) i.i.d. uniform on \((0,\theta)\), it is shown in Example 4.6 that \(M=\max\{X_{1},\ldots,X_{n}\}\) is a complete sufficient statistic. Hence to find the UMVU estimator \(g(M)\) of \(\sin\,\theta\), we may solve for \(g\) satisfying the following. (Note that the pdf of \(M\) is \((n/\theta^{n})t^{n-1}.1_{\{0<t<\theta\}}\).)

\[\sin\,\theta=E_{\theta}g(M)=\frac{n}{\theta^{n}}\int_{(0,\theta)}g(t)t^{n-1}dt, \mbox{ or, }\left(\frac{\theta^{n}}{n}\right)\sin\,\theta=\int_{(0,\theta)}g(t)t^{n-1}dt.\]

Differentiation with respect to \(\theta\) leads to the equation \(\theta^{n-1}\sin\,\theta+(\theta^{n}/n)\cos\,\theta=g(\theta)\theta^{n-1}\). Hence \(g(\theta)=\sin\,\theta+(\theta/n)\cos\,\theta\). That is, the UMVU estimator of \(\sin\,\theta\) is \(\sin\,M+(M/n)\cos\,M\). (ii): In the same manner the UMVU estimator \(g(M)\) of \(e^{\theta}\) satisfies the equation \(\theta^{n}e^{\theta}/n=\int_{(0,\theta)}g(t)t^{n-1}dt\), and differentiation leads to \(\theta^{n-1}e^{\theta}+\theta^{n}e^{\theta}/n=g(\theta)\theta^{n-1}\), i.e., \(g(\theta)=e^{\theta}+\theta e^{\theta}/n\). Therefore, the UMVU estimator of \(e^{\theta}\) is \(e^{M}(1+M/n)\).

**4.7**: Let \({\bf X}=(X)_{ij}:1\leq i\leq m\), \(1\leq j\leq n)=({\bf X}_{j}\), \(1\leq j\leq n)\) be \(n\geq 2\) i.i.d. random vectors \({\bf X}_{j}=(X_{1j},X_{2j},\ldots,X_{mj})\), \(1\leq j\leq n\), from the \(m\)-dimensional Normal distribution \(N(\mathbf{\mu},\mathbf{\Sigma})\) where \(\mathbf{\mu}=(\mu_{1},\ldots,\mu_{m})\) is the mean vector, and \(\mathbf{\Sigma}\) is the \(m\times m\) symmetric positive definite covariance matrix of \({\bf X}_{j}\) (\(j=1,\ldots,n\)). From Example 4.12 it follows that \(T\equiv[(\overline{X}_{i.}=\frac{1}{n}\sum_{j=1}^{n}X_{ij}\), \(1\leq i\leq m)\), \((m_{ii}=\frac{1}{n}\sum_{j=1}^{n}X_{ij}^{2}\), \(1\leq i\leq m)\), \((m_{ii^{\prime}}=\frac{1}{n}\sum_{j=1}^{n}X_{ij}X_{i^{\prime}j}\), \(1\leq i<i^{\prime}\leq m)]\) is a complete sufficient statistic for \((\mathbf{\mu},\mathbf{\Sigma})\).

(a): Since \(E_{\theta}\overline{X}_{i.}=\mu_{i}\), \(1\leq i\leq m\), the UMVU estimator of \(\mu_{i}\) is \(\overline{X}_{i.}\) (a function of \(T\)). Hence the UMVU estimator of \(\mu\) is \({\bf m}=(\overline{X}_{1.},\ldots,\overline{X}_{m.})\). (b): One has \(E_{\theta}(\sum_{j=1}^{n}X_{ij}^{2}-n\overline{X}_{i.}^{2})=(n-1)\sigma_{ii.}\). For \(X_{ij}\), \(1\leq j\leq n\)), are i.i.d. Normal \(N(\mu_{i},\sigma_{ii})\), so that it follows from Example 1.1 in Chap. 1 that the _sample variance_\(\mathscr{S}_{ii}=\sum_{j=1}^{n}(X_{ij}-\overline{X}_{i.})^{2}/(n-1)\) is an unbiased estimator of the population variance \(\sigma_{ii}\). Hence \(\frac{\sum_{i=1}^{n}(\sum_{j=1}^{n}X_{ij}^{2}-n\overline{X}_{ij}^{2})}{n-1}= \mathscr{S}_{ii}\), a function \(g_{i}(T)\), say, is the UMVU estimator of \(\sigma_{ii.}\). Similarly, for \(i<i^{\prime}\), writing \(\mu_{ii^{\prime}}=EX_{ij}X_{i^{\prime}j}\),\[E_{\theta}\left[\sum_{j=1}^{n}X_{ij}X_{i^{\prime}j}-n\overline{X}_{i }.\overline{X}_{i^{\prime}}.\right]=E_{\theta}\left[n\mu_{ii^{\prime}}-\sum_{j=1 }^{n}X_{ij}\sum_{j^{\prime}=1}^{\lambda}X_{i^{\prime}j^{\prime}}/n\right]\] \[=n\mu_{ii^{\prime}}-\frac{1}{n}E_{\theta}\sum_{j=1}^{n}X_{ij}X_{i^ {\prime}j}-\sum_{\begin{array}{c}j=1\\ j^{\prime}\neq j\end{array}}^{n}(E_{\theta}X_{ij})(E_{\theta}X_{i^{\prime}j^{ \prime}})/n\] \[=n\mu_{ii^{\prime}}-\mu_{ii^{\prime}}-(n-1)\mu_{i}\mu_{i^{\prime}}\] \[=(n-1)(\mu_{ii^{\prime}}-\mu_{i}\mu_{i^{\prime}})=(n-1)E(X_{ij}- \mu_{i})(X_{i^{\prime}j}-\mu_{i^{\prime}})\] \[=(n-1)\sigma_{ii^{\prime}}.\]

[Note: \(E(X_{ij}-\mu_{i})(X_{i^{\prime}j}-\mu_{i^{\prime}})=EX_{ij}X_{i^{\prime}j}-\mu _{i}\mu_{i^{\prime}}-\mu_{i}\mu_{i^{\prime}}-\mu_{i}\mu_{i^{\prime}}+\mu_{i} \mu_{i^{\prime}}=\mu_{ii^{\prime}}-\mu_{i}\mu_{i^{\prime}}.\)] Hence the _sample covariance_

\[s_{ii^{\prime}} \equiv\frac{1}{n-1}\sum_{j=1}^{n}(X_{ij}-\overline{X}_{i}.)(X_{i^ {\prime}j}-\overline{X}_{i^{\prime}}.)\] \[=\frac{1}{n-1}\left[\sum_{j=1}^{n}X_{ij}X_{ij^{\prime}}-n \overline{X}_{i}.\overline{X}_{i^{\prime}}.\right]\] \[=\frac{n}{n-1}\left[m_{ii^{\prime}}-\overline{X}_{i}.\overline{X }_{i^{\prime}}.\right]=g(T),\text{ say,}\]

is the UMVU estimator of the population covariance \(\sigma_{ii^{\prime}}.\) It follows that

\[S=((r_{ii^{\prime}}))_{1\leq i,i^{\prime}\leq m}\text{ is the UMVU estimator of }\not{X}.\]

[Here \(\text{var}(S)\equiv\sum_{1\leq i,i^{\prime}\leq m}E(s_{ii^{\prime}}-\sigma_{ ii^{\prime}})^{2}.\)]

**4.8**: Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(N(\mu,\sigma^{2}),\)\(\theta=(\mu,\sigma^{2})\in\mathbb{R}\times(0,\infty).\) Find the UMVU estimator of \(\mu/\sigma.\)

**Solution.** Since \(\overline{X}\) and \(s^{2}\) are independent, one has

\[E_{\theta}\frac{\overline{X}}{s}=(E_{\theta}\overline{X})(E_{\theta}(1/s))=\mu /E_{\theta}(1/s).\]

Now one may write

\[\frac{1}{s}=\left[\frac{(n-1)/\sigma^{2}}{(n-1)s^{2}/\sigma^{2}}\right]^{ \frac{1}{2}}=\frac{(n-1)^{\frac{1}{2}}}{\sigma}\,\cdot\frac{1}{U^{1/2}}\,,\]

where \(U\) has the chisquare distribution with \(n-1\) degrees of freedom, i.e., a gamma a distribution \(\mathscr{G}(2,\frac{n-1}{2}).\) Hence

\[E_{\theta}\frac{1}{s} =\frac{(n-1)^{\frac{1}{2}}}{\sigma}E_{\theta}\frac{1}{U^{\frac{1}{ 2}}}=\frac{(n-1)^{\frac{1}{2}}}{\sigma}\int_{0}^{\infty}u^{-\frac{1}{2}}\frac {1}{\Gamma(\frac{n-1}{2})2^{(n-1)/2}}e^{-u/2}u^{\frac{n-1}{2}-1}du\] \[=\frac{(n-1)^{\frac{1}{2}}}{\sigma\Gamma(\frac{n-1}{2})2^{\frac {n-1}{2}}}\int_{0}^{\infty}e^{-u/2}u^{\frac{n-2}{2}-1}du\]\[=\frac{(n-1)^{\frac{1}{2}}\Gamma(\frac{n-2}{2})2^{\frac{n-2}{2}}}{\sigma \Gamma(\frac{n-1}{2})2^{\frac{(n-1)}{2}}}\] \[=\frac{(n-1)^{\frac{1}{2}}\Gamma(\frac{n-2}{2})}{\sigma\sqrt{2} \,\Gamma(\frac{n-1}{2})}=\frac{d_{n}}{\sigma},\,\,\,d_{n}=\frac{(n-1)^{\frac{1 }{2}}\Gamma(\frac{n-2}{2})}{\sqrt{2}\,\Gamma(\frac{n-1}{2})}\,.\]

Hence \(E_{\theta}\frac{1}{d_{n}s}=\frac{1}{\sigma},\) and \(\frac{1}{d_{n}}(\overline{X}/s)\) is the UMVU estimator of \(\mu/\sigma.\)_This requires \(n>\,2.\)_]

**4.11**: Let \(X_{1},\ldots,X_{n}\) be i.i.d. beta random variables \(Be(\theta,1),\) with common p.d.f.

\[f(x\mid\theta)=\theta x^{\theta-1},\,\,0<x<1,\,\,\,\theta\in(0,\infty).\]

Find the UMVU estimators of (a) \(\theta,\) (b) \(\frac{1}{\theta}.\)
**Solution.**: (a) \(f(x\mid\theta)=\theta\cdot\frac{1}{x}\cdot e^{\theta\ln x},\) belongs to the 1-parameter exponential family with natural parameter \(\theta\in(0,\infty).\) The complete sufficient statistic for \(\theta\) is \(T=\sum_{j=1}^{n}\ln X_{j}.\)
**(b) We need to find a function \(g(T)\) such that \(E_{\theta}g(T)=\theta.\) Now consider the random variable \(Y_{j}=-\ln X_{j},\) then the p.d.f. of \(Y_{j}\) is (Note: \(x=e^{-y}.\))**

\[f_{Y}(y\mid\theta)=\theta e^{y}e^{-\theta y}e^{-y}=\theta e^{-\theta y},\quad y \in(0,\infty).\]

That is, \(Y_{j}\) is gamma \(\Gamma(\frac{1}{\theta}),\) and (with

\[EY_{j}=\frac{1}{\theta}\,.\]

Hence \(\sum_{j=1}^{n}Y_{j}/n\equiv-\sum_{j=1}^{n}\ln X_{j}/n\) is the UMVU estimator of \(\frac{1}{\theta}.\) 

## Chapter 5

Let \(X_{j},\,1\leq j\leq n,\) be i.i.d. with density

\[f(x\mid\theta)=\theta x^{\theta-1},\quad 0<x<1\,\,(\theta\in\Theta=(0,\infty)).\]

Find the UMPU test of size \(\alpha\in(0,1)\) for \(H_{0}:\theta=1\) against \(H_{1}:\theta\neq 1.\)

**Solution.**: The joint density is

\[f_{n}({\bf x}\mid\theta)=\theta^{n}\prod_{j=1}^{n}x_{j}^{\theta-1}=\theta^{n} \left(\prod_{j=1}^{n}x_{j}\right)^{-1}e^{\theta\sum_{j=1}^{n}\ln x_{j}}\]

which is a one-parameter exponential family with natural parameter \(\pi=\theta\) and a complete sufficient statistic \(T=-\sum_{j=1}^{n}\ln x_{j}.\) From HW Set 4 (Problem #4), we know that \(T\) has the gamma distribution \(\mathscr{G}(\frac{1}{\theta},n).\) By Example 5.4, the UMPU test for \(N_{0}:\theta=1\) against \(H_{1}:\theta\neq 1,\) (or, \(H_{0}:\pi=-1,\)\(H_{1}:\pi\neq-1\)) is given by

\[\varphi^{*}({\bf x})=\left\{\begin{array}{l}1\mbox{ if }T<t_{1}\mbox{ or }T>t_{2}\\ 0\mbox{ otherwise,}\end{array}\right.\]where \(t_{1}<t_{2}\) are determined by

\[\int_{t_{1}}^{t_{2}}\frac{1}{\Gamma(n)}\,e^{-t}t^{n-1}dt=1-\alpha,\quad\left(\frac {t_{2}}{t_{1}}\right)^{n}=e^{t_{2}-t_{1}}.\]

**5.9**: Let \(X_{1},\ldots,X_{m}\) and \(Y_{1},\ldots,Y_{n}\) be independent random samples from exponential distributions with means \(\theta_{1}\), \(\theta_{2}\), respectively [i.e., from \(\mathscr{G}(\theta_{1},1)\) and \(\mathscr{G}(\theta_{2},1)\)]. Find the UMPU test of size \(\alpha\) for \(H_{0}:\theta_{1}\leq\theta_{2}\) against \(H_{1}:\theta_{1}>\theta_{2}\).

**Solutions.** The (joint) pdf of the observation vector \((X_{1},\ldots,X_{m},Y_{1},\ldots,Y_{n})\) is

\[f_{n}(\mathbf{x},\mathbf{y}\mid\theta_{1},\theta_{2}) =\frac{1}{\theta_{1}^{m}\theta_{2}^{n}}\,e^{-\sum_{1}^{m}x_{i}/ \theta_{1}-\sum_{n}^{1}y_{j}/\theta_{2}}\] \[=\frac{1}{\theta_{1}^{m}\theta_{2}^{n}}e^{-\left(\frac{1}{\theta_ {1}}-\frac{1}{\theta_{2}}\right)\sum_{1}^{m}x_{i}}-\frac{1}{\theta_{2}}\left( \sum_{1}^{m}x_{i}+\sum_{1}^{n}y_{j}\right)\]

which is a 2-parameter exponential family with natural parameter \(\pi_{1}=\frac{1}{\theta_{2}}-\frac{1}{\theta_{1}}\), \(\pi_{2}=-\frac{1}{\theta_{2}}\), and complete sufficient statistic \(T=(T_{1},T_{2})\), where \(T_{1}=\sum_{i=1}^{m}X_{i}\), \(T_{2}=\sum_{1}^{m}x_{i}+\sum_{1}^{n}y_{j}\). The null hypothesis is \(H_{0}:\pi_{1}\leq 0\) and the alternative is \(H_{1}:\pi_{1}>0\). Here the natural parameter space is \(\Pi=(-\infty,\infty)\times(-\infty,0)\), and the boundary is \(\Pi_{B}\equiv\{(0,\pi_{2}),\pi_{2}\in(-\infty,0)\}=\{0\}\times(-\infty,0)\). The UMPU test of size \(\alpha\) is given by

\[\varphi^{*}(t)=\left\{\begin{array}{ll}1&\mbox{if $T_{1}>t_{1}(t_{2})$}\\ 0&\mbox{if $T_{1}\leq t_{1}(t_{2})$},\end{array}\right.\]

where \(t_{2}\) is determined by

\[P_{(0,\pi_{2})}(T_{1}>t_{1}(t_{2})\mid T_{2}=t_{2})=\alpha\,\,\,\forall\,\,\pi _{2}\in(-\infty,0),\]

i.e.,

\[P_{(0,\pi_{2})}\left(\frac{T_{1}}{T_{2}}>\eta(t_{2})\mid T_{2}=t_{2}\right)= \alpha\,\,\,\forall\,\,\pi_{2}\in(-\infty,0),\]

where \(\eta(t_{2})=t_{1}(t_{2})/t_{2}\). Now the distribution of \(U\equiv\frac{T_{1}}{T_{2}}\) under \(\Pi_{B}\), i.e., under \(\pi_{1}=0\) (or, \(\theta_{1}=\theta_{2}\)) is that of a ratio \(U=\frac{V_{1}}{V_{1}+V_{2}}\) where \(V_{1}=\sum_{1}^{m}X_{i}/\theta_{1}\) is \(\mathscr{G}(1,m)\), \(V_{2}=\sum_{1}^{n}Y_{j}/\theta_{1}\) is \(\mathscr{G}(1,n)\), with \(V_{1}\) and \(V_{2}\) independent. This _distribution of \(U\) does not depend on \(\pi_{2}\)_ (i.e., it is the same for all \(\boldsymbol{\pi}=(0,\pi_{2})\in\Pi_{B}\)). Hence, by Basu's Theorem, \(T_{1}/T_{2}\) is independent of \(T_{2}\) under \(\Pi_{B}\). Therefore, the UMPU test is to _reject \(H_{0}\) iff_

\[\frac{T_{1}}{T_{2}}>\eta,\]

where \(T_{1}/T_{2}=V_{1}/(V_{1}+V_{2})\)_has the beta distribution function_ Beta \((m,n)\). (See the next Exercise.) Hence \(\eta\) is the \((1-\alpha)^{th}\) quantile of this Beta \((m,n)\) distribution.

**5.10**: Let \(U_{1}\) and \(U_{2}\) be independent gamma random variables \(\mathscr{G}(\theta,m)\) and \(\mathscr{G}(\theta,n)\). Prove that \(Z_{1}\equiv U_{1}/(U_{1}+U_{2})\) and \(Z_{2}\equiv U_{1}+U_{2}\) are independent random variables with \(Z_{1}\) having the beta distribution Beta\((m,n)\) and \(Z_{2}\) have the gamma distribution \(\mathscr{G}(\theta,m+n)\).

**Solution.** The joint density of \((U_{1},U_{2})\) is

\[f_{(U_{1},U_{2})}(u_{1},u_{2})\!=\!\frac{1}{\Gamma(m)\Gamma(n)\theta^{m+n}}\,e^{-( u_{1}+u_{2})/\theta}u_{1}^{m-1}u_{2}^{n-1}\ (0<u_{1}<\infty,\,0<u_{2}<\infty).\]

The Jacobian of the transformation \((u_{1},u_{2})\rightarrow(\frac{u_{1}}{u_{1}+u_{2}})=z_{1}\), \(u_{1}+u_{2}=z_{2}\), is

\[J\left(\frac{z_{1},z_{2}}{u_{1},u_{2}}\right)=\begin{bmatrix}\frac{\partial z_{ 1}}{\partial u_{1}}&\frac{\partial t_{1}}{\partial u_{2}}\\ \frac{\partial z_{2}}{\partial u_{1}}&\frac{\partial z_{2}}{\partial u_{2}} \end{bmatrix}=\begin{bmatrix}\frac{u_{2}}{(u_{1}+u_{2})^{2}}&\frac{-u}{(u_{1}+ u_{2})^{2}}\\ 1&1\end{bmatrix},\]

whose determinant is \(1/(u_{1}+u_{2})=1/z_{2}\). Hence the joint density of \(Z_{1}\) and \(Z_{2}\) is given by [Note: \(u_{1}=z_{1}z_{2}\), \(u_{2}=z_{2}(1-z_{1})\).]

\[g_{(Z_{1},Z_{2})}(z_{1},z_{2}) = f_{(U_{1},U_{2})}(u_{1},u_{2})\cdot z_{2}\mid_{z_{1},z_{2}}=\frac {e^{-z_{2}/\theta}(z_{1}z_{2})^{m-1}}{\Gamma(m)\Gamma(n)\theta^{m+1}}[z_{2}(1-z _{1})]^{n-1}z_{2}\] \[= \frac{1}{\Gamma(m)\Gamma(n)}\,z_{1}^{m-1}(1-z_{1})^{n-1}\cdot \frac{e^{-z_{2}/\theta}}{\theta^{m+n}}\ z_{2}^{m+n-1}\] \[= \frac{\Gamma(m_{n})}{\Gamma(m)\Gamma(n)}\,z_{1}^{m-1}(1-z_{1})^{n -1}\cdot\frac{1}{\Gamma(m+n)\theta^{m+n}}\,e^{-z_{2}/\theta}z_{2}^{m+n-1},\] \[(0<z_{1}<1,\,0<z_{2}<\infty).\]

**5.18** In this _two-way layout with one observation per cell,_ the model is given by (5.147) and (5.148), but with \(\eta_{ij}=0\ \forall\ i,j\), i.e.,

\[X_{ij}=\mu+\delta_{i}+\gamma_{j}+\varepsilon_{ij}\ (\varepsilon_{ij}\mbox{'s are i.i.d.}\ N(0,\sigma^{2})),\ 1\leq i\leq I,\,1\leq j\leq J,\] (S.6)

where \(\mu\in\mathbb{R}\), \(\sum_{i=1}^{I}\delta_{i}=0\), \(\sum_{j=1}^{J}\gamma_{j}=0\). Hence

\[E{\bf X}=\mu+\delta_{i}+\gamma_{j}\mbox{ lies in (and spans) an Euclidean space}\] \[\mbox{of }\mbox{\bf dimension }1+I-1+J-1=I+J-1=k.\] (S.7)

Note that in this model, if one writes \(\theta_{ij}=EX_{ij}\) then \(\mu=\overline{\theta}_{..}=\frac{1}{IJ}\sum_{i,j}\theta_{ij}\), \(\delta_{i}=\overline{\theta}_{i.}-\overline{\theta}_{..}\), \(\gamma_{j}=\overline{\theta}_{.j}-\overline{\theta}_{..}\), where a dot \((\cdot)\) indicates averaging over the corresponding (missing) index. To find the minimum of \(\|{\bf X}-E{\bf X}\|^{2}\) in this model, it is algebraically convenient to write

\[X_{ij}-EX_{ij} = X_{ij}-\mu-\delta_{i}-\gamma_{j}=(\overline{X}_{..}-\mu)+( \overline{X}_{i.}-\overline{X}_{..}-\delta_{i})\] (S.8) \[+(\overline{X}_{.j}-\overline{X}_{..}-\gamma_{j})+(X_{ij}- \overline{X}_{i.}-\overline{X}_{.j}+\overline{X}_{..}),\]

and check, as in Example 5.19, that the sum over \(i,j\) of the products of the \(\binom{4}{2}=6\) pairs of the four terms on the right are all zero. Thus

\[\|{\bf X}-E{\bf X}\|^{2} = IJ(\overline{X}_{..}-\mu)^{2}+J\sum_{i=1}^{I}(\overline{X}_{i.}- \overline{X}_{..}-\delta_{i})^{2}+I\sum_{j=1}^{J}(\overline{X}_{.j}-\overline{X}_{..}-\gamma_{j})\] (S.9) \[+\sum_{i=1}^{I}\sum_{j=1}^{J}(X_{ij}-\overline{X}_{i.}-\overline {X}_{.j}+\overline{X}_{..})^{2},\]so that the minimum of (S.9) over \(E{\bf X}\) is attained by taking \(\mu=\overline{X}_{..}\), \(\delta_{i}=\overline{X}_{i.}-\overline{X}_{..}\), \(\gamma_{j}=\overline{X}_{..j}-\overline{X}_{..}\), which are the least squares estimators of the corresponding parameters. One then has

\[\min_{E{\bf X}}\|{\bf X}-E{\bf X}\|^{2}=\sum_{i=1}^{I}\sum_{j=1}^{J}(X_{ij}- \overline{X}_{i.}-\overline{X}_{.j}+\overline{X}_{..})^{2},\] (S.10)

and the \(IJ=n\) coordinates \(\hat{m}_{ij}\) of \(\hat{\bf m}\) are given by

\[\hat{m}_{ij}=\overline{X}_{..}+(\overline{X}_{i.}-\overline{X}_{..})+( \overline{X}_{.j}-\overline{X}_{..})=\overline{X}_{i.}+\overline{X}_{.j}- \overline{X}_{..}\ (1\leq i\leq I,1\leq j\leq J).\] (S.11)

1. To test \(H_{0}:\)_All the \(IJ\) means \(\theta_{ij}\equiv EX_{ij}\) are equal,_ that is, \(\delta_{i}=0\)\(\forall\)_i_ and \(\eta_{j}=0\)\(\forall\,j\)_ one has [see (S.8)] \[X_{ij}-EX_{ij}=X_{ij}-\mu=(\overline{X}_{..}-\mu)+(X_{ij}-\overline{X}_{..}).\] (S.12) The sum over \(i,j\) of the product of the two terms on the right vanishes, so that the minimum of \[\|{\bf X}-E{\bf X}\|^{2}=IJ(\overline{X}_{..}-\mu)^{2}+\sum_{i=1}^{I}\sum_{j=1 }^{J}(X_{ij}-\overline{X}_{..})^{2},\] (S.13) is attained by setting \(\mu=\overline{X}_{..}\), and then \[\min_{E{\bf X}\ \mbox{under}\ H_{0}}\|{\bf X}-E{\bf X}\|^{2}=\sum_{i=1}^{I} \sum_{j=1}^{J}(X_{ij}-\overline{X}_{..})^{2}.\] (S.14) The least squares estimator of the mean vector \({\bf m}=E{\bf X}\) is given by \[\hat{\hat{\hat{m}}}_{ij}=\hat{\mu}=\overline{X}_{..}\quad\forall\ i,j.\] (S.15) Hence, by (S.11) and (S.15), \[\|\hat{\bf m}-\hat{\bf m}\|^{2}=\sum_{i,j}\left\{(\overline{X}_{i.}-\overline {X}_{..})+(\overline{X}_{.j}-\overline{X}_{..})\right\}^{2}=J\sum_{i=1}^{I}( \overline{X}_{i.}-\overline{X}_{..})^{2}+I\sum_{j=1}^{J}(\overline{X}_{.j}- \overline{X}_{..})^{2}.\] From (S.10), (S.15), the UMP unbiased and invariant test is to \[\mbox{\it reject }H_{0}\ \mbox{\it iff }\ \frac{\{J\sum_{i=1}^{I}( \overline{X}_{i.}-\overline{X}_{..})^{2}+I\sum_{j=1}^{J}(\overline{X}_{.j}- \overline{X}_{..})^{2}\}/(I+J-2)}{\sum_{i=1}^{I}\sum_{j=1}^{J}(X_{ij}-\overline {X}_{i.}-\overline{X}_{.j}+\overline{X}_{..})^{2}/(I-1)(J-1)}\] \[>F_{1-\alpha}(I+J-1,(I-1)(J-1)),\] (S.16) for \(k=1+I-1+J-1=I+J-1\), and \(n=IJ\), so that \(n-k=(I-1)(J-1)\), and the number of linearly independent constraints under \(H_{0}\) is \(I-1+J-1=I+J-2=4\).
2. To test \(H_{0}:\delta_{i}=0\)\(\forall\,i\), [i.e., the \(\overline{\theta}_{i.}=\cdots=\overline{\theta}_{I.}\) under the model (S.6)] note that, under \(H_{0}\), \(EX_{ij}=\mu+\gamma_{j}\), so that [see (S.8)]\[X_{ij}-EX_{ij}=(\overline{X}_{..}-\mu)+(\overline{X}_{i.1}-\overline{X}_{..})+( \overline{X}_{.j}-\overline{X}_{..}-\gamma_{j})+(X_{ij}-\overline{X}_{i.}- \overline{X}_{.j}+\overline{X}_{..}),\] (S.17)

and

\[\|{\bf X}-E{\bf X}\|^{2} = IJ(\overline{X}_{..}-\mu)^{2}+J\sum_{i=1}^{I}(\overline{X}_{i.}- \overline{X}_{..})^{2}+I\sum_{j=1}^{J}(\overline{X}_{.j}-\overline{X}_{..}- \gamma_{j})^{2}\] (S.18) \[+\sum_{i=1}^{I}\sum_{j=1}^{J}(X_{ij}-\overline{X}_{i.}-\overline{ X}_{.j}+\overline{X}_{..})^{2},\]

which is minimized by taking \(\mu=\overline{X}_{..}\), \(\gamma_{j}=\overline{X}_{.j}-\overline{X}_{..}\), to yield

\[\min_{E{\bf X}\ {\rm under}\ H_{0}}\|{\bf X}-E{\bf X}\|^{2}-J\sum_{i=1}^{I}( \overline{X}_{i.}-\overline{X}_{..})^{2}+\sum_{i=1}^{I}\sum_{j=1}^{J}(X_{ij}- \overline{X}_{.i}-\overline{X}_{.j}+\overline{X}_{..}).\] (S.19)

Hence the numerator sum of squares of the \(F\)-statistic is the difference between (S.19) and (S.18), namely,

\[J\sum_{i=1}^{I}(\overline{X}_{i.}-\overline{X}_{..})^{2}.\] (S.20)

One may obtain (S.20) also as \(\|\hat{\hat{m}}-\hat{m}\|^{2}\), where \(\hat{m}_{ij}\)'s are given by (S.11), and \(\hat{\hat{m}}_{ij}=\overline{X}_{..}+\overline{X}_{.j}-\overline{X}_{..}= \overline{X}_{.j}\), (\(1\leq i\leq I\), \(1\leq j\leq J\)). Hence the UMPU invariant test for \(H_{0}:\delta_{i}=0\ \forall\,i\) is

\[{\rm Reject}\ H_{0}\ {\rm iff}\ {J\sum_{i=1}^{I}(\overline{X}_{i.}- \overline{X}_{..})^{2}/(I-1)\over\sum_{i=1}^{I}\sum_{j=1}^{J}(X_{ij}-\overline {X}_{i.}-\overline{X}_{.j}+\overline{X}_{..})^{2}/(I-1)(J-1)}\] \[>F_{1-\alpha}(I-1,(I-1)(J-1)).\] (S.21)

For there are \(r=I-1\) linearly independent constraints under \(H_{0}\).
3. The UMPU invariant test for \({\bf H_{0}:\gamma_{j}=0}\ \forall\,j\) is entirely analogous to the case (b):

\[{\rm Reject}\ H_{0}\ {\rm iff}\ {I\sum_{j=1}^{J}(\overline{X}_{.j}-\overline{X}_{..})^{2}/(J-1)\over\sum_{i=1}^{I}\sum_{j=1}^{J}(X_{ij}-\overline{X}_{i.}-\overline{X}_{.j}+\overline{X}_{..})^{2}/(I-1)(J-1)}\] \[>F_{1-\alpha}(J-1,(I-1)(J-1)).\] (S.22)

## Index

**Symbols**

\(F\)-distribution, 350

\(M\)-estimator, 191

\(p\)-th quantile, 126

\(t\)-statistic, 131

\(t\)-test, 205

**A**

absence of interactions, 105

acceptance region, 68

action space, 12

admissible, 16

Anderson's inequality, 176

ANOVA, 108

AR(\(p\)) model, 240, 248

AR(1) model, 235

ARE, 151

ARMA(\(1,1\)), 245

ARMA(\(p,q\)) model, 242

asymptotic relative efficiency, 151, 203, 204

asymptotic unbiasedness, 204

asymptotically efficient, 176, 247

asymptotically normal, 131

autocovariance function, 243

autoregressive model, 123

autoregressive process, 248

**B**

bandwidth, 267

Basu's Theorem, 84

Bayes classifier, 333

Bayes estimator, 22

Bayes risk, 22

Bayes rule, 22

generalized, 32

unique, 28

unique up to equivalence, 27

Behrens-Fisher problem, 87

Benjamini-Hochberg procedure, 319

Berry-Esseen Theorem, 210

best linear unbiased estimator, 98

beta distribution, 25

bias, 268

Bickel and Freedman

proposition, 262

theorem, 262

binomial expansion, 366

BLUE, 98

Blyth's method, 32

Bonferroni method, 232

Bootstrap, 289

bootstrap, 281

approximation, 260

percentile, 257

bootstrap confidence interval, 258

bootstrapped statistic, 258

Bootstrapping, 63

boundedly complete, 44

Brownian Bridge, 226, 267

Brownian motion

with zero mean, 224

**C**

canonical model, 102

Cauchy distribution, 350

median, 151

centered moments, 360

Central Limit Theorem, 128

Liapounov, 369

Lindeberg, 134, 369

central limit theorem

functional, 225

Chebyshev's Inequality, 118

chi-square

frequency, 223

chi-square distribution, 216

chi-square test, 220

classical density, 19

classification, 6

**D**
CLT, 128

for \(U\)-statistics, 215

for the MLE, 168

composite alternative hypothesis, 69

composite null hypothesis, 69

conditional density, 23, 26

conditional distribution, 189, 355

conditional p.d.f., 189

confidence interval, 153

confidence level, 89

confidence region, 89

Bonferroni, 136

regression lines, 142

UMA, 91

confidence regions, 153

consistency, 117

of a test, 204

of sample mean, 119

of sample moments, 120

consistent, 172, 216

contaminated normal, 152

Control group, 88

converge in distribution, 128

convergence

almost sure, 119

convergence in law, 128

convex set, 176

Cornish-Fisher expansions, 293

correlation model, 143, 151

coverage error, 64, 257

Cramer-Rao bound, 165

Cramer-Rao Inequality, 55

Cramer-Rao Information Inequality, 56, 166

multiparameter, 174

critical region, 68

cross-validation, 275

generalized, 276

cumulant generating function, 281

curse of dimensionality, 275

**D**

decision rule, 11, 12

deductive inference, 3

delta method, 131

design matrix, 146, 261

differentiable manifold, 305

dimension reduction, 335

dispersion matrix, 352

distribution

\(F\), 350

beta, 25

Cauchy, 350

empirical, 257

logistic, 351

multivariate normal, 352

negative binomial, 61

normal, 347

shifted empirical, 261

standard normal, 347

distribution function, 126

distributions

absolutely continuous, 345

duality

between confidence regions, 90

**E**

Edgeworth expansion, 290

Edgeworth expansions, 281

efficient

asymptotically, 168

error

coverage, 257

Type I, 14

Type II, 14

Errors in variables models, 154

estimable, 45, 98

estimate

pooled, 86

estimation, 6

estimator

asymptotically efficient, 176

best linear unbiased, 98

consistent, 118

least squares, 99

Maximum Likelihood, 19

unbiased, 45

uniformly minimum variance unbiased, 45

expectation, 118

explanatory variable, 121

exponential families, 47

exponential family

one-parameter, 50, 51

two-parameter, 50, 51

exponential family: one-parameter, 47

**F**

false discovery rate, 318

Fieller's method, 160

Fisher information, 58

Fisher Linkage Model, 59

Fisher-Yates test, 97, 209, 214

Frechet function, 304

Frechet mean, 304

frequency chi-square test, 220

function

estimable, 98

maximal invariant, 100

moment generating, 357

functional model, 155

**G**

gamma density, 21

distribution, 346

function, 345

Gauss-Markov Theorem, 98Gibbs sampler, 327

Glivenko-Cantelli Theorem, 267

goodness-of-fit, 222

group of transformations, 92

**H**

heteroscedastic linear regression model, 138

Hotelling's \(T^{2}\) test, 94

hypotheses

testing, 6

hypothesis

alternative, 14

null, 14

**I**

i.i.d., 4

identifiability, 92

identity map, 12, 39

inadmissible, 16

independent and identically distributed, 4

indicator, 144

inductive inference, 3

information, 167

invariance, 92

invariant, 214

maximal, 93

under \(g\), 92

invariant probability, 234

**J**

James-Stein estimator, 33

Jensen's Inequality, 42

**K**

Kendall's planar shape space, 309

kernel estimator, 269

Kolmogorov distance, 259

Kolmogorov-Smirnov

one-sample statistic, 225

Kolmogorov-Smirnov Goodness-of-Fit Test, 226

Kolmogorov-Smirnov two-sample statistic, 231

**L**

law, 259

least squares estimates

weighted, 142

least squares estimator, 99, 146

Lebesgue-Stieltjes measure, 60

Lehman-Scheffe Theorem, 45

level of significance, 68

Liapounov's Central Limit Theorem, 369

likelihood equations, 20

Likelihood function, 20

likelihood function, 19

likelihood ratio statistic, 218

likelihood ratio test, 218

Lindeberg Central Limit Theorem, 134, 369

linear regression, 121, 134

parametric, 138

linear regression model

known error variances, 138

linear statistic, 259

log-likelihood function, 20

loss function, 12, 14

**M**

Mahalanobis distance, 334

main effects, 104

Mallows distance, 262

Mann-Whitney test, 97

marginal distribution, 23

marginal p.d.f., 189

Markov process, 233

stationary ergodic, 248

Markov property, 233

martingale difference sequence, 248

matrix

nonsingular dispersion, 352

maximal invariant, 92

maximal invariant function, 100

maximum likelihood estimator, 19, 189, 191

maximum value

smallest, 30

mean test, 203

mean vector, 353

mean vs. median, 151

median, 24, 127

method of moments, 193

Metropolis-Hastings algorithm, 326

mgf, 54

minimal sufficient, 44

minimax, 30

MSE, 269

MLE, 19, 189

model

canonical, 102

correlation, 143

moment generating function, 54

moment generating functions, 357

monotone increasing, 73

multinomial distribution, 184

multinomial model, 53, 220

multiparameter exponential family, 180

multiparametric case, 174

multiple regression, 146

multivariate

normal distribution, 352

normal model, 52

multivariate m.g.f., 360

multivariate normal, 182

**N**

Nadaraya-Watson estimator, 273

natural parameter, 49

natural parameter space, 47

negative binomial distribution, 61

negative unimodal, 176

Neyman structure, 80

Neyman-Pearson Lemma, 69

nominal coverage, 257

non-uniqueness, 24

nonparametric, 3

normal

asymptotically, 131

normal correlation model, 161

normal distribution, 347

normal equations, 146

normal scores test, 209

nuisance parameters, 3

**O**

observation, 12

observation space, 12, 95

odd function, 77

One-Way Layout, 103

orbit

constant on the, 92

order statistic, 209

ordinary least squares estimators, 122

Ornstein-Uhlenbeck process, 60

**P**

p.d.f., 165

p.m.f., 165

Parameter identifiability condition, 337

parameter space, 12

parameter vector, 146

parametric, 3

parametric statistical inference, 3

partition function, 325

Pearson's chi-square statistic, 223

Pitman ARE, 203, 204

Polya's Theorem, 369

pooled estimate, 86

population, 3

distribution, 3

posterior density, 26, 325

posterior distribution, 189

power, 214

power of the test, 68

PRDS, 317

preshape sphere, 308

Principal Component Analysis, 335

prior

distribution, 22

improper, 32

probability density function, 165

probability mass function, 19, 165

problem

Behrens-Fisher, 87

two-sample, 84

problems, 371

R
random, 4

random sample, 6

simple, 4

stratified, 5

rank test, 214

Rao's scores test, 222

Rao-Blackwell, 44

Theorem, 42

raw moments, 359

rejection region, 68

response variable, 121

risk function, 11, 12, 68

**S**

sample, 4

sample correlation coefficient, 132

sample mean, 286

sample quantiles, 126

semi-parametric, 3

sequential probability ratio test, 337

Shapiro and Wilk test, 228

sign test, 204

simple alternative hypothesis, 69

simple null hypothesis, 69

simple random sample, 4

with replacement, 4

without replacement, 4

size of the test, 68

SLLN, 119

Slutsky Theorem, 130

solutions, 371

space

observation, 95

SPRT, 338

squared error

mean integrated, 269

stability conditions, 241

standard normal distribution, 347

statistic, 39

Anderson-Darling, 227

Cramer-von Mises, 227

statistical decision problem, 6

steady state distribution, 234

strata, 5

stratified random sample, 5

Strong Law of Large Numbers, 119

strongly consistent estimator, 119

Student \(t\) test, 213, 287

sufficient, 39

minimal, 44

superficient, 195

symmetric convex sets, 176

symmetric difference, 44

**T**

test

\(\alpha\)-similar, 80

Fisher-Yates, 97

goodness-of-fit, 222

Kolmogorov-Smirnov, 231

level of significance, 68

Mann-Whitney, 97, 208

most powerful of size \(\alpha\), 73

nonrandomized, 67, 68

randomized, 68

size of the, 68

UMP unbiased, 91

uniformly least powerful, 74

uniformly most powerful, 73

uniformly most powerful of size \(\alpha\), 73

Wilcoxon, 97, 208

Test of Absence of Variety Main Effect, 104

Test of Equality, 104

theorem

asymptotic joint distribution of regression coefficients, 136

Basu's, 84

Central Limit, 128

Gauss-Markov, 98

Glivenko-Cantelli, 267

Polya's, 369

Rao-Blackwell, 42

time series, 295

transformation, 92

Treatment group, 88

Tukey Model, 152

two-sample

multi-dimensional problem, 230

two-sample problem, 84, 94

two-sided alternative, 74

Two-Way Layout, 104

one observation per cell, 112

Type I Error, 14, 68

Type II Error, 14, 68

**U**

UMA, 89

confidence region, 91

UMP, 68, 73

invariant test, 102

unbiased test, 91

UMPU, 75

UMVU, 45

unbiasedness, 73, 75

of test, 204

uniform distribution, 46

uniformly most accurate, 89

uniformly most powerful, 68

tests, 73

uniformly most powerful unbiased, 75

univariate normal, 181

unobservable errors, 97

**W**

Wilcoxon

rank test, 7

test, 97, 214

**Y**

Yule-Walker method, 244