[MISSING_PAGE_EMPTY:1]

_Springer Texts in Statistics (STS)_ includes advanced textbooks from 3rd- to 4th-year undergraduate courses to 1st- to 2nd-year graduate courses. Exercise sets should be included. The series editors are currently Geneva I. Allen, Richard D. De Veaux, and Rebecca Nugent. Stephen Fienberg, George Casella, and Ingram Olkin were editors of the series for many years.

More information about this series at [http://www.springer.com/series/417](http://www.springer.com/series/417)Bing Li G. Jogesh Babu

## A Graduate Course

on Statistical InferenceBing Li

Department of Statistics

Penn State University

University Park, PA, USA

G. Jogesh Babu

Department of Statistics

Penn State University

University Park, PA, USA

###### Abstract

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

This Springer imprint is published by the registered company Springer Science+Business Media, LLC part of Springer Nature.

The registered company address is: 233 Spring Street, New York, NY 10013, U.S.A.

**To our parents:**

**Jianmin Li and Liji Du**

**Nagarathmann and Mallayya**

**and to our families:**

**Yanling.** Ann. and** **Terrence**

**Sudha, Vinay, and Vinay**

## Preface

It is our goal to write a compact, rigorous, self-contained, and accessible graduate textbook on statistical estimation and inference that reflects the current trends in statistical research.

The book contains three main themes: the finite-sample theory, the asymptotic theory, and Bayesian statistics. Chapters 2 through 4 are devoted to the finite-sample theory, which includes the classical theory of optimal estimation and hypothesis test, sufficiency, completeness, ancillarity, and exponential families. Chapters 5 to 6 are devoted to Bayesian statistics, covering prior and posterior distributions, Bayesian decision theory for estimation, hypothesis testing, and classification, empirical Bayes, shrinkage estimates. Chapters 8 through 11 are devoted to asymptotic theory, covering consistency and asymptotic normality of maximum likelihood estimation and estimating equations, the Le Cam-Hajek convolution theorem for regular estimates, and the asymptotic analysis of a wide variety of hypothesis testing procedures. Two chapters on preliminaries are included to make the book self-contained: Chapter 1 contains preliminaries for the finite-sample theory and Bayesian statistics; Chapter 7 for the asymptotic theory.

The topics and treatment of some material are different from a typical textbook on statistical inference, which we regard as a special feature of this book. For example, we devoted a chapter on estimating equations and used it as a unifying mechanism to cover some useful methodologies such as the generalized linear models, generalized estimation equations, quasi likelihood estimation, and conditional inference. We include a systematic exposition of the theory of regular estimates, from regularity, contiguity, the convolution theory, to asymptotic efficiency. This theory was then used in conjunction with the Local Asymptotic Normal (LAN) assumption to develop asymptotic local alternative distributions and the optimal properties for a wide variety of hypothesis testing procedures that can be written as quadratic forms in the limit.

One of the features of the book is the systematic use of a parsimonious set of assumptions and mathematical tools to streamline some recurring regularity conditions, and theoretical results that are fundamentally similar. This makes the development of the methodology more transparent and interconnected, and the book a coherent whole. For example, the conditions "differentiable under the integral sign (DUI)", and "stochastic equicontinuity" are repeatedly used throughout many chapters of the book; the geometric projection and the multivariate Cauchy-Schwarz inequality are used to unify different types of optimal theories; the structures of asymptotic estimation and hypothesis testing echo their counterparts in the finite-sample theory.

This book can be used either as a one-semester or a two-semester textbook on statistical inference. For the two-semester courses, the first six chapters can be used for the first semester to cover finite-sample estimation and Bayesian statistics, and the last five for the second semester to cover asymptotic statistics. For a one-semester course, there are several pathways depending on the instructor's emphasis. For example, one possibility is to use Chapters 1, 3, 4, 7, 10, 11 for an advanced course on hypothesis testing; another possibility is to use Chapters 1, 2, 5, part of 6, 7, 8, 9 as an advanced course on point estimation and Bayesian statistics.

The book grew out of the lecture notes for two graduate-level courses that we have taught for more than two decades at the Pennsylvania State University. Over this period we have revamped the courses several times to adapt to the evolving trends, emphases, and demands in theoretical and methodological research. The authors are grateful to the Department of Statistics of the Pennsylvania State University for its constant support and the stimulating research and education environment it provides. The authors also gratefully acknowledge the support from the National Science Foundation grants.

State College

April 2019

_Bing Li_

_G. Jogesh Babu_

[MISSING_PAGE_FAIL:9]

###### Contents

* 3 Testing Hypotheses for a Single Parameter
	* 3.1 Basic concepts
	* 3.2 The Neyman-Pearson Lemma
	* 3.3 Uniformly Most Powerful test for one-sided hypotheses
		* 3.3.1 Definition and examples of UMP tests
		* 3.3.2 Monotone Likelihood Ratio
		* 3.3.3 The general form of UMP tests
		* 3.3.4 Properties of the one-sided UMP test
	* 3.4 Uniformly Most Powerful Unbiased test and two-sided hypotheses
		* 3.4.1 Uniformly Most Powerful Unbiased tests
		* 3.4.2 More properties of the exponential family
		* 3.4.3 Generalized Neyman-Pearson Lemma
		* 3.4.4 Quantile transformation and construction of two-sided tests
		* 3.4.5 UMP test for hypothesis III
		* 3.4.6 UMPU tests for hypotheses I and II Problems
* 4 Testing Hypotheses in the Presence of Nuisance Parameters
	* 4.1 Unbiased and Similar tests
	* 4.2 Sufficiency and completeness for a part of the parameter vector
	* 4.3 UMPU tests in the presence of nuisance parameters
	* 4.4 Invariant family and ancillarity
	* 4.5 Using Basu's theorem to construct UMPU test
	* 4.6 UMPU test for a linear function of \(\theta\)
	* 4.7 UMPU test for nonregular family
	* 4.8 Confidence sets
* 5 Basic Ideas of Bayesian Methods
	* 5.1 Prior, posterior, and likelihood
	* 5.2 Conditional independence and Bayesian sufficiency
	* 5.3 Conjugate families
	* 5.4 Two-parameter normal family
	* 5.5 Multivariate Normal likelihood
	* 5.6 Improper prior
		* 5.6.1 The motivation idea of improper prior
		* 5.6.2 Haar measures
		* 5.6.3 Jeffreys prior
	* 5.7 Statistical decision theory
* 6 Bayesian Inference
	* 6.1 Estimation
	* 6.2 Bayes rule and unbiasedness
	* 6.3 Error assessment of estimators
	* 6.4 Credible sets
	* 6.5 Hypothesis test
	* 6.6 Classification
	* 6.7 Stein's phenomenon
	* 6.8 Empirical Bayes Problems
	* 6.9 References
* 7 Asymptotic tools and projections
	* 7.1 Laws of Large Numbers
	* 7.2 Convergence in distribution
	* 7.3 Argument via subsequences
	* 7.4 Argument via simple functions
	* 7.5 The Central Limit Theorems
	* 7.6 The \(\delta\)-method
	* 7.7 Mann-Wald notation for order of magnitude
	* 7.8 Hilbert spaces
	* 7.9 Multivariate Cauchy-Schwarz inequality
* 7.10 Projections
* 7.11 Applications Problems
* 7.12 References
* 8 Asymptotic theory for Maximum Likelihood Estimation
	* 8.1 Maximum Likelihood Estimation
	* 8.2 Cramer's approach to consistency
	* 8.3 Almost everywhere uniform convergence
	* 8.4 Wald's approach to consistency
	* 8.5 Asymptotic normality
	* 8.6 Problems
* 9 Estimating equations
	* 9.1 Optimal Estimating Equations
	* 9.2 Quasi likelihood estimation
	* 9.3 Generalized Estimating Equations
	* 9.4 Other optimal estimating equations
	* 9.5 Asymptotic properties
	* 9.6 One-step Newton-Raphson estimate
	* 9.7 Asymptotic linear form
	* 9.8 Efficient score for parameter of interest Problems
* 10 Convolution Theorem and Asymptotic Efficiency
	* 10.1 Contiguity
	* 10.2 Le Cam's first lemma
	* 10.3 Le Cam's third lemma
	* 10.4 Local asymptotic Normality
	* 10.5 The convolution theorem
	* 10.6 Asymptotically efficient estimates
	* 10.7 Augmented LAN
	* 10.8 Le Cam's third lemma under ALAN
	* 10.9 Superefficiency Problems
* 11 Asymptotic Hypothesis Test
	* 11.1 Quadratic Form test
	* 11.2 Wilks's likelihood ratio test
	* 11.3 Wald's, Rao's, and Neyman's tests
		* 11.3.1 Wald's test
		* 11.3.2 Rao's test
		* 11.3.3 Neyman's \(C(\alpha)\) test
	* 11.4 Asymptotically efficient test
	* 11.5 Pitman efficiency
	* 11.6 Hypothesis specified by an arbitrary constraint
		* 11.6.1 Asymptotic analysis of constrained MLE
		* 11.6.2 Likelihood ratio test for general hypotheses
		* 11.6.3 Wald's test and Rao's test for general hypotheses
		* 11.6.4 Neyman's \(C(\alpha)\) test for general hypotheses
		* 11.6.5 Asymptotic efficiency
	* 11.7 QF tests for estimating equations
		* 11.7.1 Wald's, Rao's, and Neyman's tests for estimating equations
		* 11.7.2 QF tests for canonical estimating equations
		* 11.7.3 Wilks's test for conservative estimating equations
* Problems
* 11.7.3

## 1 Probability and Random Variables

A brief outline of the important ideas and results from classical theory of measure and probability are presented in this chapter. This is not intended for the first reading of the subject, but rather as a review and a reference. Occasionally some proofs are presented, but in most cases they are omitted, and such omissions are indicated by saying "it is true...". These proofs are easily found in standard texts on measure theory and probability, such as Billingsley (1995), Rudin (1987), and Vestrup (2003). In the last section we lay out some basic notations that will be repeatedly used throughout the book.

### 1.1 Sample space, events, and probability

Probability theory has three basic elements: outcomes, events, and probability. An outcome is a result of an experiment. An experiment here means any action that can have a number of possible results, but which result will actually occur cannot be predicted with certainty before the experiment is performed. For example, tossing a coin is an experiment, and the coin turning up heads is an outcome; rolling a die is an experiment, and the die turns up 6 is an outcome. The set of all outcomes of an experiment is the sample space, which is denoted by \(\Omega\). For example, in the experiment of tossing a pair of dice, the sample space is the set of 36 possible combinations of \((i,j)\), \(i,j=1,\ldots,6\). A set of outcomes, or a subset of \(\Omega\), is an event. Of course a single outcome is itself an event, but when we think of it as an event, we think of it as a subset, rather than an element, of the sample space.

### 1.2 \(\sigma\)-field and measure

In probability we are concerned with a class of events, which is formulated as an algebraic structure called the \(\sigma\)-field. Intuitively, a \(\sigma\)-field describes the world in which events occur, whose likelihood we would like to assess.

**Definition 1.1**: _A \(\sigma\)-field (or \(\sigma\)-algebra) over a nonempty set \(\Omega\) is any collection of subsets in \(\Omega\) that satisfies the following three conditions:_

1. \(\Omega\in\mathcal{F}\)_,_
2. _If_ \(A\in\mathcal{F}\)_, then_ \(A^{c}\in\mathcal{F}\)_,_
3. _If_ \(A_{1},A_{2},\ldots\) _is a sequence of sets in_ \(\mathcal{F}\)_, then_ \(\mathop{\cup}\limits_{n=1}^{\infty}A_{n}\in\mathcal{F}\)_._

A set \(A\) in a \(\sigma\)-field \(\mathcal{F}\) is called a \(\mathcal{F}\)-measurable set, a measurable set, or an \(\mathcal{F}\)-set. It is easy to show that the collection of all subsets of \(\Omega\) is a \(\sigma\)-field, and the collection \(\{\varnothing,A,A^{c},\Omega\}\), where \(A\subset\Omega\), is a \(\sigma\)-field. Another simple example is \(\{\varnothing,\Omega\}\), which is, in fact, the smallest \(\sigma\)-field.

It is true that the intersection of any collection of \(\sigma\)-fields is itself a \(\sigma\)-field. Let \(\mathcal{A}\) be a collection of subsets of \(\Omega\). Then the \(\sigma\)-field generated by \(\mathcal{A}\) is defined as the intersection of all \(\sigma\)-fields that contain \(\mathcal{A}\). This is well defined because the collection of all subsets of \(\Omega\), which must contain \(\mathcal{A}\), is a \(\sigma\)-field. If \(\Omega=\mathbb{R}^{k}\), the \(k\)-dimensional Euclidean space, and if \(\mathcal{A}\) is the collection of all open sets in \(\Omega\), then the \(\sigma\)-field generated by \(\mathcal{A}\) is written as \(\mathcal{R}^{k}\). Members of \(\mathcal{R}^{k}\) are called the Borel sets. (Therefore \(\mathcal{R}^{k}\) is the collection of all Borel sets).

A set \(\Omega\), together with a \(\sigma\)-field \(\mathcal{F}\) of its subsets, is called a measurable space, and is written as \((\Omega,\mathcal{F})\).

A measure is a mechanism that enable us to assign probability to each event in a \(\sigma\)-field. It can also be understood as the length, the area, or the volume, and so on, of a set.

**Definition 1.2**: _A measure \(\mu\) defined on a measurable space \((\Omega,\mathcal{F})\) is a mapping \(\mu:\mathcal{F}\rightarrow[0,\infty]\) such that_

1. \(\mu(\varnothing)=0\)_,_
2. _If_ \(A_{1},A_{2},\ldots\in\mathcal{F}\) _and_ \(A_{i}\cap A_{j}=\emptyset\) _whenever_ \(i\neq j\)_, then_

\[\mu\Big{(}\bigcup_{n=1}^{\infty}A_{n}\Big{)}=\sum_{n=1}^{\infty}\mu(A_{n}).\]

A measure \(\mu\) is called a \(\sigma\)-finite measure if there is a sequence of \(\mathcal{F}\)-sets \(\{A_{n}\}\) such that \(\Omega=\cup_{n=1}^{\infty}A_{n}\) and \(\mu(A_{n})<\infty\). It is called a finite measure if \(\mu(\Omega)<\infty\). It is called a probability measure if \(\mu(\Omega)=1\). There is no essential difference between a finite measure and a probability measure; the latter is introduced simply to conform to our daily convention that the largest probability is \(100\%\).

A set \(\Omega\), together with a \(\sigma\)-field \(\mathcal{F}\) of its subsets, and a measure \(\mu\) defined on \((\Omega,\mathcal{F})\), is called a measure space. In the special case where \(\mu\) is a probability measure on \(\mathcal{F}\), \((\Omega,\mathcal{F},\mu)\) is called a probability space. Often \(P\) is used, instead of \(\mu\), to represent a probability measure.

**Example 1.1**: It is true that there exists a unique measure \(\lambda\) on \((\mathbb{R}^{k},\mathcal{R}^{k})\) such that for each open rectangle \(A\) in \(\mathbb{R}^{k}\), \(\lambda(A)\) is the volume of the rectangle. This measure is called the Lebesgue measure.

**Example 1.2** Consider the measure space \((\mathbb{R},\mathcal{R},\lambda)\), where \(\lambda\) is the Lebesgue measure. Then \(\lambda\) is \(\sigma\)-finite: let \(A_{n}=(-n,n)\), then \(\mathop{\cup}\limits_{n=1}^{\infty}A_{n}=\mathbb{R}\), \(\lambda(A_{n})<\infty\).

**Example 1.3** Let \(\Omega=\{\omega_{1},\omega_{2},\ldots\}\) be a countable set. Let \(\mathcal{F}\) be the class of all subsets of \(\Omega\). Then \(\mathcal{F}\) is a \(\sigma\)-field. Let \(\mu:\mathcal{F}\rightarrow\mathbb{R}\) be defined as follows: for any subset \(A\) of \(\Omega\), \(\mu(A)=\) the number of elements in \(A\). Then it is true that \(\mu\) is a measure on \(\mathcal{F}\). This measure is called the counting measure.

### 1.3 Measurable function and random variable

Let \((\Omega,\mathcal{F})\) and \((\Omega^{\prime},\mathcal{F}^{\prime})\) be two measurable spaces. Let \(f:\Omega\rightarrow\Omega^{\prime}\) be a mapping from \(\Omega\) to \(\Omega^{\prime}\). For any set \(A^{\prime}\subseteq\Omega^{\prime}\), let \(f^{-1}(A^{\prime})\) denotes the set \(\{\omega\in\Omega:f(\omega)\in A^{\prime}\}\). Suppose we have a measure \(\mu\) on \((\Omega,\mathcal{F})\). Then we could measure any set \(A\) in \(\mathcal{F}\) by \(\mu(A)\). But can we somehow use \(\mu\) to measure any set \(A^{\prime}\) in \(\mathcal{F}^{\prime}\)? One possibility is to use the measure \(\mu\) of the set in \(\Omega\) that maps to \(A^{\prime}\). To do so we need this set, \(f^{-1}(A^{\prime})\), to be in \(\mathcal{F}\). This motivates the following definition of the measurability of \(f\).

**Definition 1.3**: _The mapping \(f\) is measurable \(\mathcal{F}/\mathcal{F}^{\prime}\) if, whenever \(A^{\prime}\in\mathcal{F}^{\prime}\), \(f^{-1}(A^{\prime})\in\mathcal{F}\)._

Suppose \((\Omega^{\prime},\mathcal{F}^{\prime})=(\mathbb{R}^{k},\mathcal{R}^{k})\). If \(f:\Omega\rightarrow\mathbb{R}^{k}\) is measurable \(\mathcal{F}/\mathcal{R}^{k}\), then we will say \(f\) is measurable with respect to \(\mathcal{F}\), or measurable \(\mathcal{F}\), or, simply, measurable.

As mentioned at the beginning of this section, if \(\mu\) is a measure on \((\Omega,\mathcal{F})\), then any mapping \(f:\Omega\rightarrow\Omega^{\prime}\) that is measurable with respect to \(\mathcal{F}/\mathcal{F}^{\prime}\) induces a measure on \(\mathcal{F}^{\prime}\), in the following way. For any \(A^{\prime}\in\mathcal{F}^{\prime}\), define the set function

\[\nu(A^{\prime})=\mu(f^{-1}(A^{\prime})).\]

It is true that \(\nu\) is a measure in \((\Omega^{\prime},\mathcal{F}^{\prime})\). This measure is written as \(\mu\circ f^{-1}\).

Suppose that \((\Omega,\mathcal{F},\mu)\) is a measure space, and \((\Omega^{\prime},\mathcal{F}^{\prime})\) is a measurable space. Suppose that the function \(f:\Omega\rightarrow\Omega^{\prime}\) is measurable \(\mathcal{F}/\mathcal{F}^{\prime}\). Let \(B\in\mathcal{F}^{\prime}\). We say that \(f\in B\) almost everywhere \(\mu\) (a.e. \(\mu\)) if

\[\mu(\{\omega\in\Omega:f(\omega)\notin B)\})=(\mu\circ f^{-1})(B^{c})=0.\]

If \(P\) is a probability measure, then \(f\in B\) almost everywhere \(P\) is also called \(f\in B\) almost everywhere \(P\). Note that in this case, \(P(\{\omega:f(\omega)\notin B\})=0\) is equivalent to \(P(\{\omega:f(\omega)\in B\})=1\).

For convenience, we abbreviate sets such as

\[\{\omega:f(\omega)\notin B\},\ \ \{\omega:f(\omega)\in B\}\]

as \(\{f\in B\}\) and \(\{f\notin B\}\).

[MISSING_PAGE_EMPTY:5849]

\[\begin{cases}0\cdot\infty=\infty\cdot 0=0,\\ x\cdot\infty=\infty\cdot x=\infty,&\text{if }0<x<\infty\\ \infty\cdot\infty=\infty\end{cases} \tag{1.2}\]

The supremum of this sum (1.1) over all finite measurable partitions is defined to be the integral \(\int fd\mu\). That is

\[\int fd\mu=\sup\sum_{i}\left[\inf_{\omega\in A_{i}}f(\omega)\right]\mu(A_{i}).\]

If this number is finite, we say that \(f\) is integrable. If it is \(\infty\), then we say that \(f\) is not integrable, but has integral \(\infty\).

For an arbitrary measurable function \(f:\Omega\to\mathbb{R}\), let \(f^{+}\) be defined by

\[f^{+}(\omega)=\begin{cases}f(\omega)&\text{if }\ f(\omega)\geq 0\\ 0&\text{otherwise}\end{cases}\]

and let \(f^{-}=(-f)^{+}\). Then \(f^{+}\) and \(f^{-}\) are nonnegative measurable functions, and \(f=f^{+}-f^{-}\). The integral of \(f\) is defined as

\[\int fd\mu=\int f^{+}d\mu-\int f^{-}d\mu,\]

if at least one of the terms on the right is finite. \(f\) is said to be integrable if both terms on the right are finite. If \(\int f^{+}d\mu=\infty\) and \(\int f^{-}d\mu<\infty\), then \(f\) is not integrable, but has definite integral \(\infty\). If \(\int f^{-}d\mu=\infty\) and \(\int f^{+}d\mu<\infty\), then \(f\) is not integrable but its definite integral is \(-\infty\). If both terms are \(\infty\), then integral of \(f\) is not defined.

Let \(A\) be a measurable set. Let \(I_{A}\) be the indicator function of \(A\); that is \(I_{A}:\Omega\to\{0,1\}\) is defined by

\[I_{A}(\omega)=\begin{cases}1&\text{if }\omega\in A\\ 0&\text{if }\omega\notin A\end{cases}.\]

It is true that \(fI_{A}\) is a measurable function if \(f\) is measurable. Then \(\int fI_{A}d\mu\) will be written as \(\int_{A}fd\mu\), and is called the integral of \(f\) with respect to measure \(\mu\) over the set \(A\), whenever it exists.

An integral has the following properties.

**Theorem 1.1**:
1. _Let_ \(\{A_{1},\ldots,A_{k}\}\) _be a finite measurable partition of_ \(\Omega\)_, and_ \(f:\Omega\to\mathbb{R}\) _be a nonnegative simple function; that is,_ \(f(\omega)=\sum_{i}x_{i}I_{A_{i}}(\omega)\) _for some nonnegative numbers_ \(x_{1},\ldots,x_{k}\)_. Then_ \(\int fd\mu=\sum_{i}x_{i}\mu(A_{i})\)_._
2. _If_ \(f\) _and_ \(g\) _are integrable and_ \(f\leq g\) _almost everywhere, then_ \(\int fd\mu\leq\int gd\mu\)3. _If_ \(f\) _and_ \(g\) _are integrable and_ \(\alpha\) _and_ \(\beta\) _are real numbers, then_ \(\alpha f+\beta g\) _is integrable and_ \[\int\alpha fd\mu+\beta gd\mu=\alpha\int fd\mu+\beta\int gd\mu.\]

A property is said to hold almost everywhere with respect to a measure \(\mu\) (a.e. \(\mu\)), if it holds for \(\omega\) outside a measurable set of \(\mu\)-measure zero.

Note that, part 2 implies that if \(f=g\) a.e. \(\mu\), then \(\int fd\mu=\int gd\mu\). An important consequence of part 1 is that, for any \(A\in\mathcal{F}\),

\[\mu(A)=\int I_{A}d\mu=\int_{A}d\mu.\]

That is, the measure of a set \(A\) is the integral of the measure over that set.

**Theorem 1.2**: _Suppose that \(f:\Omega\to\mathbb{R}\) is measurable and nonnegative. Then 1. \(f=0\) a.e. \(\mu\) if and only if \(\int fd\mu=0\). 2. If \(\int fd\mu<\infty\) then \(f<\infty\) a.e. \(\mu\)._

**Corollary 1.1**: _If \(f\) and \(g\) are measurable \(\mathcal{F}\) and integrable \(\mu\) and if \(\int_{A}fd\mu=\int_{A}gd\mu\) for all \(A\in\mathcal{F}\), then \(f=g\) a.e. \(\mu\)._

Proof: Note that

\[\int|f-g|d\mu=\int_{\{f>g\}}(f-g)\,d\mu+\int_{\{f<g\}}(g-f)\,d\mu\]

Because \(\{f>g\}\in\mathcal{F}\) and \(\{f<g\}\in\mathcal{F}\), the right hand side is zero. Hence \(\int|f-g|d\mu=0\) which, by part 1 of Theorem 1.2, implies that \(f=g\) a.e. \(\mu\). \(\Box\)

**Corollary 1.2**: _If \(f:\Omega\to\mathcal{F}\) is measurable \(\mathcal{F}\) and integrable \(\mu\), and \(\int_{A}fd\mu\leq 0\) for all \(A\in\mathcal{F}\), then \(f\leq 0\) almost everywhere \(\mu\)._

Proof: Since \(\int_{A}fd\mu\leq 0\) for every \(A\in\mathcal{F}\), we have \(\int_{f>0}fd\mu\leq 0\). Hence \(\int I(f>0)fd\mu\leq 0\). But we know that \(I(f>0)f\geq 0\). So \(\int I(f>0)fd\mu\geq 0\). Therefore this integral must be 0. By part 1 of Theorem 1.2, \(I(f>0)f=0\) a.e. \(\mu\). However, we know that

\[\{\omega:I(f>0)f=0\}=\{\omega:f=0\}\cup\{\omega:f\leq 0\}=\{\omega:f\leq 0\}.\]

Therefore, \(f\leq 0\) a.e. \(\mu\). \(\Box\)

Suppose that \((\Omega,\mathcal{F},P)\) is a probability space, and \(X:\Omega\to\mathbb{R}\) is a random variable. Suppose that \(f:\mathbb{R}\to\mathbb{R}\) is measurable \(\mathcal{R}/\mathcal{R}\) and that \(f\circ X\) is integrable with respect to \(P\). Then

\[\int f\circ XdP=\int f[X(\omega)]P(d\omega)\]

is called the expectation of \(f(X)\), and is written as \(E[f(X)]\). If \(X^{2}\) is integrable, then \(E[X-E(X)]^{2}\) is called the variance of \(X\), and is written as \(\mathrm{var}(X)\).

### 1.5 Some inequalities

A set \(A\) in a vector space is convex if, for any \(a_{1},a_{2}\in A\), the line segment

\[\{(1-\lambda)a_{1}+\lambda a_{2}:\,\lambda\in[0,1]\}\]

is in \(A\). A real-valued function \(f\) defined on a convex set \(A\) is convex if, for any \(a_{1},a_{2}\in A\) and \(\lambda\in(0,1)\),

\[f((1-\lambda)a_{1}+\lambda a_{2})\leq(1-\lambda)f(a_{1})+\lambda f(a_{2}).\]

Such a function is strictly convex if the above inequality is strict whenever \(a_{1}\neq a_{2}\). Let \((\Omega,\mathcal{F},P)\) be a probability space. We say that a probability measure is degenerate if it is concentrated on a single point. The next theorem is taken from Perlman (1974).

**Theorem 1.3** (Jensen's inequality): _Let \(f\) be a convex function defined on a convex subset \(C\) of \(\mathbb{R}^{p}\), and let \(X\in\mathbb{R}^{p}\) be an integrable random vector such that \(P(X\in C)=1\). Then_

_1. \(E(X)\in C\);_

_2. \(Ef(X)\) exists and \(f(E(X))\leq E(f(X))\);_

_3. if \(f\) is strictly convex and the distribution of \(X\) is not degenerate then \(f(E(X))<E(f(X))\)._

Two positive integers, \(p\) and \(q\), are called a conjugate pair if \(1/p+1/q=1\). If \(p=1\), then \((1,\infty)\) is also defined as a conjugate pair. Let \((\Omega,\mathcal{F},\mu)\) be a measure space.

**Theorem 1.4**: _Suppose \((p,q)\) is a conjugate pair, and \(f\) and \(g\) are measurable functions on \(\Omega\). Then the following inequalities hold:_

\[\int|fg|d\mu\leq\left(\int|f|^{p}d\mu\right)^{1/p}\left(\int|g|^{q}d\mu\right) ^{1/q}, \tag{1.3}\]

_and_

\[\left(\int|f+g|^{p}d\mu\right)^{1/p}\leq\left(\int|f|^{p}d\mu\right)^{1/p}+ \left(\int|g|^{p}d\mu\right)^{1/p}. \tag{1.4}\]

The first inequality is the Holder's inequality; the second is the Minkowski's inequality.

### 1.6 Logical statements modulo a measure

Recall that a statement holds almost everywhere \(\mu\) if it holds everywhere outside a measurable set of \(\mu\) measure zero. This convention induces a logical deduction system modulo a measure, which is now developed further for lateruse. Let \((\Omega,\mathcal{F},\mu)\) be a measure space. Let \(S\) be a logical statement. We say \(S\) is a measurable statement if \(\{\omega:S\}\in\mathcal{F}\). Here, we should always understand \(\{\omega:S\}\) as \(\{\omega\in\Omega:S\}\). We say that a statement holds for \(\omega^{\prime}\) if \(\omega^{\prime}\in\{\omega:S\}\). We say that \(S\) holds on a subset \(A\) of \(\Omega\) if \(S\) holds for every \(\omega\in A\); that is, \(A\subseteq\{\omega:S\}\). We say that a statement \(S\) holds if it holds for every \(\omega\in\Omega\); that is \(\{\omega:S\}=\Omega\). If a statement holds for \(\omega\), then we write \(S(\omega)\). As an example, let \(f:\Omega\rightarrow\mathbb{R}\) be a measurable function. Let \(S\) be the statement that \(f>0\). Because \(f\) is measurable \(\mathcal{F}\), \(f>0\) is a measurable statement. The sentence "\(f>0\) holds for \(\omega\)" means \(f(\omega)>0\). So the symbol, \((f>0)(\omega)\) means \(f(\omega)>0\).

In measure theory, many statements hold not for \(\Omega\) but for a subset \(A\) of \(\Omega\) with \(\mu(A^{c})=0\) for a measure \(\mu\) on \(\Omega\). For example, later on we will learn that if \(E(X^{2})=0\), then all we can conclude is \(P(X\neq 0)=0\). We cannot conclude \(X(\omega)=0\) for every \(\omega\in\Omega\). If a statement \(S\) only holds for \(\omega\) on a set \(A\) with \(\mu(A^{c})=0\), then we say \(S\) holds modulo \(\mu\), and write

\[S\ \ \ \ [\mu].\]

For example, \(f>0\ \ [\mu]\) means \(\mu(f\leq 0)=0\). What is interesting -- and extremely convenient -- is that modulo \(\mu\) statements obey the usual logical laws, in the sense of the following proposition.

**Proposition 1.1**: _Let \(S_{1},\ldots,S_{k}\) be \(k\) logical statements. If, for every \(\omega\in\Omega\),_

\[S_{1}(\omega),\ldots,S_{k}(\omega)\implies S(\omega), \tag{1.5}\]

_then \(S_{1}\ [\mu],\ldots,S_{k}\ [\mu]\implies S\ [\mu].\)_

Proof: The expression (1.5) means that

\[\{\omega:S_{1}\}\cap\cdots\cap\{\omega:S_{k}\}\subseteq\{\omega:S\},\]

which is equivalent to \(\{\omega:S\}^{c}\subseteq\{\omega:S_{1}\}^{c}\cup\cdots\cup\{\omega:S_{k}\}^ {c}\). Consequently,

\[\mu(\{\omega:S\}^{c})\leq\mu(\{\omega:S_{1}\}^{c})+\cdots+\mu(\{\omega:S_{k} \}^{c}).\]

So if each term on the right is \(0\) then the term on the left is also \(0\). \(\Box\)

A practical implication of this proposition is that, when we are dealing with a finite set of statements each of which holds modulo \(\mu\), we can make logical deductions ignoring \(\mu\), and at the end state the conclusion modulo \(\mu\). The premise of this simplification is that the modulus measure to be the same for every statement involved. For different measures, the following proposition is helpful.

Let \(\mu\) and \(\nu\) be measures on \((\Omega,\mathcal{F})\). We say that \(\nu\) is absolutely continuous with respect to \(\mu\) if, for every \(A\in\mathcal{F}\), \(\mu(A)=0\implies\nu(A)=0\). In this case we write \(\nu\ll\mu\). We say \(\nu\) and \(\mu\) are equivalent if \(\nu\ll\mu\) and \(\mu\ll\nu\). We write \(\mu\equiv\nu\).

**Proposition 1.2**: _Suppose \(\nu\ll\mu\) and \(S\) is a logical statement. Then \(S\ [\mu]\Rightarrow S\ [\nu].\) In particular, if \(\nu\equiv\mu\), then \(S\ [\mu]\ \Leftrightarrow\ S\ [\nu].\)_

The next example illustrates a typical logical deduction modulo a measure -- the type in which we will often be engaged.

**Example 1.4**: Let \(\nu\) and \(\mu\) be two measures defined on a measurable space \((\Omega,\mathcal{F})\) such that \(\nu\ll\mu.\) Let \(f_{1},f_{2},f_{3}\) be measurable functions from \(\Omega\rightarrow\mathbb{R}.\) Suppose we know \(f_{1}f_{3}=f_{2}f_{3}\ \ [\nu]\) and \(f_{3}\neq 0\ \ [\mu].\) Then

\[f_{1}f_{3}=f_{2}f_{3}\ \ [\nu],\ \ \ \ f_{3}\neq 0\ \ [\nu],\]

which implies \(f_{1}=f_{2}\ \ [\nu].\) \(\Box\)

### 1.7 Integration to the limit

Suppose that \((\Omega,\mathcal{F},\mu)\) is a measure space and \(f\) and \(\{f_{n}:n=1,2,\ldots\}\) are real-valued measurable functions on \(\Omega.\) If \(f_{n}\) converges (a.e. \(\mu\)) to a function \(f\), will the integral \(\int f_{n}d\mu\) also converge to the integral \(\int fd\mu?\) This is not always true. Here is a counter example.

**Example 1.5**: Consider the measure space \((\mathbb{R},\mathcal{R},\lambda)\), where \(\lambda\) is the Lebesgue measure. Let

\[f_{n}(x)=nI_{(0,1/n)}(x).\]

Then \(f_{n}(x)\to 0\) for all \(x\in\mathbb{R}\) but

\[\int f_{n}d\lambda=1\]

for all \(n.\) Thus \(\lim_{n}\int f_{n}d\lambda=1\) and \(\int\lim_{n}f_{n}d\lambda=0.\)\(\Box\)

We see that the point-wise convergence does not always imply the convergence of the integral. Nevertheless, under reasonable conditions the above situation can be ruled out. We now give several sufficient conditions under which integration to the limit is valid.

**Theorem 1.5** (Monotone Convergence Theorem): _Suppose that \(\{f_{n}\}\) and \(f\) are measurable \(\mathcal{F}.\) If \(0\leq f_{n}\uparrow f\) a.e. \(\mu\), then \(\int f_{n}d\mu\uparrow\int fd\mu.\)_

This is the basic result for integration to the limit, from which other sufficient conditions can be reasonably easily deduced.

**Theorem 1.6** (Fatou's Lemma): _Suppose that \(\{f_{n}\}\) are measurable \(\mathcal{F}\) and \(f_{n}\geq 0\). Then_

\[\int\liminf_{n}f_{n}d\mu\leq\liminf_{n}\int f_{n}d\mu. \tag{1.6}\]

Proof: Let \(g_{n}=\inf_{k\geq n}f_{k}\). Then \(0\leq g_{n}\uparrow\liminf_{n}f_{n}\equiv g\). Hence \(\int g_{n}d\mu\uparrow\int gd\mu\). But \(g_{n}\leq f_{n}\). So \(\int g_{n}d\mu\leq\int f_{n}d\mu\). Hence

\[\liminf_{n}\int f_{n}d\mu\geq\liminf_{n}\int g_{n}d\mu=\int gd\mu\equiv\int \liminf_{n}f_{n}d\mu,\]

as desired. \(\Box\)

Note that, under the conditions of Fatou's lemma alone, it is not true that

\[\int\limsup_{n}f_{n}d\mu\geq\limsup_{n}\int f_{n}d\mu. \tag{1.7}\]

However, if \(f_{n}\leq g\) for some integrable \(g\), then \(g-f_{n}\) satisfies the conditions of Fatou's lemma, and we have

\[\int\liminf_{n}(g-f_{n})d\mu\leq\liminf_{n}\int(g-f_{n})d\mu,\]

which implies

\[\int gd\mu-\int\limsup_{n}f_{n}d\mu\leq\int gd\mu-\limsup_{n}\int f_{n}d\mu.\]

Since \(\int gd\mu\) is finite, we can cancel it out from both sides of the equality, which then reduces to (1.7). The directions of the inequalities (1.6) and (1.7) can be easily memorized if we notice that bringing limit (\(\limsup\) or \(\liminf\)) inside an integral makes the integral more extreme (bearing in mind that the \(\limsup\) case requires a dominating functions).

Note that, if \(\liminf_{n}f_{n}=\limsup_{n}f_{n}=\lim_{n}f_{n}\), then (1.6) and (1.7) imply

\[\liminf_{n}\int f_{n}d\mu=\limsup_{n}\int f_{n}d\mu=\int\lim_{n}f_{n}d\mu.\]

That is, \(\lim_{n}\int f_{n}d\mu\) exists and coincides with \(\int\lim_{n}f_{n}d\mu\). Essentially, this is the argument of the Lebesgue's Dominated Convergence Theorem, though a more careful treatment than outlined above would allow us to remove the requirement \(f_{n}\geq 0\). See, for example, Billingsley (1995, page 209).

**Theorem 1.7** (Lebesgue's Dominated Convergence Theorem): _Let \(\{f_{n}\}\) be a sequence of measurable functions such that \(|f_{n}|\leq g\) a.e. \(\mu\), where \(g\) measurable \(\mathcal{F}\) and is integrable \(\mu\). If \(f_{n}\to f\) a.e. \(\mu\), then \(f\) and \(f_{n}\) are integrable \(\mu\) and \(\int f_{n}d\mu\to\int fd\mu\)._The following theorem is an immediate consequence of Lebesgue' Dominated Convergence Theorem.

**Theorem 1.8** (Bounded Convergence Theorem): _Suppose that \(\mu(\Omega)<\infty\) and that \(\{f_{n}\}\) is a uniformly bounded sequence of measurable functions; that is, \(|f_{n}|\leq C\) for some \(C>0\). Then \(f_{n}\to f\) a.e. \(\mu\) implies that \(\int f_{n}d\mu\to\int fd\mu\)._

### 1.8 Differentiation under integral

Closely related to passing the limit inside an integral is passing a derivative inside an integral. After all, derivative is a form of limit. So, inevitably, the verification of the validity of this operation relies on the Dominated Convergence Theorem (Theorem 1.7), whose sufficient condition in this case is the Lipschitz condition. Differentiation under the integral sign will be used heavily in the rest of the book. Instead of having to state the complicated sufficient conditions involved every time we use this, we devote this section to streamlining the condition of passing a derivative through an integral.

Let \((\Omega,\mathcal{F},\mu)\) a measure space. Let \(\Theta\) be an open subset of \(\mathbb{R}^{k}\). Let \(g:\Omega\times\Theta\to\mathbb{R}\), and \(g(\cdot,\theta)\) is measurable for each \(\theta\in\Theta\). Let \(B\) be a measurable set in \(\Omega\).

**Definition 1.4**: _Suppose_

1. _for each_ \(x\in\Omega\)_,_ \(g(\theta,x)\) _is differentiable with respect to_ \(\theta\)_, and_ \([\partial g(\theta,x)/\partial\theta]I_{B}(x)\) _is integrable with respect to_ \(\mu\)_;_
2. _for each_ \(\theta\in\Theta\)_,_ \(g(\theta,x)I_{B}(x)\) _is integrable with respect to_ \(\mu\) _the function_ \(\theta\mapsto\int_{B}g(\theta,x)d\mu(x)\) _is differentiable;_
3. \[\frac{\partial}{\partial\theta}\int_{B}g(\theta,x)d\mu(x)=\int_{B}\frac{ \partial g(\theta,x)}{\partial\theta}d\mu(x).\] (1.8)

_Then we say that \(g\) is differentiable with respect to \(\theta\) under the integral over \(B\) with respect to \(\mu\), and state this as "\(g\) satisfies DUI\((\theta,B,\mu)\)"._

The following theorem gives sufficient conditions for DUI\((\theta,B,\mu)\). It is essentially the dominated convergence theorem applied to quotient. In this case dominating function is related to the \(L_{1}\)-Lipschitz condition. Let \(e_{i}\) denote the \(p\)-dimensional vector whose \(i\)th component is \(1\) and the rest of the components are \(0\).

**Theorem 1.9**: _Let \(B\) be a measurable set. Suppose_

1. \(g(\theta,x)\) _is differentiable with respect to_ \(\theta\)_, and_ \([\partial g(\theta,x)/\partial\theta]I_{B}(x)\) _is integrable_ \(\mu\)_2. there is a function \(g_{0}(x)\) integrable \(\mu\) such that, for each \(\theta_{1},\theta_{2}\) in \(\Theta\)_

\[|g(\theta_{2},x)-g(\theta_{1},x)|\leq g_{0}(x)\|\theta_{2}-\theta_{1}\|\]

_for each \(x\in B\)._

_Then \(g\) satisfies DUI\((\theta,B,\mu)\)._

The second condition is simply the \(L_{1}\)-Lipschitz condition for the variable \(\theta\).

Proof.: Let \(f(\theta)=\int_{B}g(\theta,x)d\mu(x)\). Recall that a \(p\)-variate function, say \(f(\theta)\), is differentiable at \(\theta_{0}\) if and only if, for every \(\theta\) in a neighborhood of \(\theta_{0}\), the function \(f((1-t)\theta_{0}+t\theta)\) is differentiable with respect to \(t\) at \(t=0\). Now for any \(t\in\mathbb{R}\),

\[\frac{f((1-t)\theta_{0}+t\theta)-f(\theta_{0})}{t}=\int_{B}\frac{g((1-t)\theta _{0}+t\theta,x)-g(\theta_{0},x)}{t}d\mu(x).\]

Since

\[\left|\frac{g((1-t)\theta_{0}+t\theta,x)-g(\theta_{0},x)}{t}\right|\leq g_{0} (x)\|\theta-\theta_{0}\|\]

for all \(x\in B\), by the dominated convergence theorem (Theorem 1.7),

\[[f^{\prime}_{t}((1-t)\theta_{0}+t\theta)]_{t=0}=\int_{B}[g^{\prime}_{t}((1-t) \theta_{0}+t\theta,x)]_{t=0}d\mu(x)\]

This shows that \(f(\theta)\) is differentiable at \(\theta_{0}\). Now take \(\theta\) to be \(e_{i}\), \(i=1,\ldots,p\), to prove the equality (1.8), where \(e_{i}\) is the \(k\)-dimensional vector with its \(i\) component being \(1\) and other components being \(0\). 

### Change of variables

Suppose \((\Omega,\mathcal{F})\) and \((\Omega^{\prime},\mathcal{F}^{\prime})\) are measurable spaces, and \(T:\Omega\rightarrow\Omega^{\prime}\) is a mapping measurable \(\mathcal{F}/\mathcal{F}^{\prime}\).

**Theorem 1.10**: _A function \(f:\Omega^{\prime}\rightarrow\mathbb{R}\) is measurable \(\mathcal{F}^{\prime}\) and integrable \(\mu\circ T^{-1}\) if and only if \(f\circ T\) is measurable \(\mathcal{F}\) and integrable \(\mu\), in which case_

\[\int_{A^{\prime}}f(\omega^{\prime})(\mu\circ T^{-1})(d\omega^{\prime})=\int_{ T^{-1}A^{\prime}}(f\circ T)(\omega)\mu(d\omega). \tag{1.9}\]

So, the expectation of \(f(T)\) can be represented in various ways:\[E[f(T)]= \int f(T(\omega))P(d\omega)\] \[= \int f(x)(P\circ T^{-1})(dx) \tag{1.10}\] \[= \int y(P_{T}\circ f^{-1})(dy),\]

where \(P_{T}=P\circ T^{-1}\)

**Example 1.6** Let \((\Omega,\mathcal{F},P)\) be a probability space and let \(X:\Omega\to[0,1]\) be a random variable. Suppose that the distribution \(P_{X}=P\circ X^{-1}\) has the following probability density with respect to the Lebesgue measure

\[f(x)=\begin{cases}1&0\leq x\leq 1\\ 0&\text{otherwise}\end{cases}\]

We can find the expectation \(E(e^{X})\) by definition or using the above change of variable theorem. By definition,

\[E(e^{X})=\int_{0}^{1}e^{x}f(x)dx=\int_{0}^{1}e^{x}dx=e-1.\]

Alternatively, let \(Y=e^{X}\). The range of \(Y\) is \(\Omega_{Y}=[1,e]\). For each \(a\in\Omega_{Y}\),

\[F_{Y}(a)=P(Y\leq a)=P(e^{X}\leq a)=P(X\leq\log a)=\int_{0}^{\log a}dx=\log a.\]

Thus applying the third line of (1.10) we have

\[E(Y)=\int_{1}^{e}y(1/y)dy=e-1.\]

We see that both methods give the same answer. \(\Box\)

### The Radon-Nikodym Theorem

Suppose \((\Omega,\mathcal{F},\mu)\) is a measure space and \(\delta\geq 0\) is a measurable function. Then it can be shown that the set function

\[\nu(A)=\int_{A}\delta d\mu,\quad\text{for all }A\in\mathcal{F} \tag{1.11}\]

defines a measure on \((\Omega,\mathcal{F})\). The above equation is often abbreviated as

\[d\nu=\delta d\mu.\]

Note that (1.11) implies that if \(\mu(A)=0\), then \(\nu(A)=0\). This leads to the following definition.

**Definition 1.5**: _Let \(\mu\) and \(\nu\) be two measures on a measurable space \((\Omega,\mathcal{F})\). Then \(\nu\) is said to be absolutely continuous with respect to \(\mu\) if, whenever \(\mu(A)=0\), \(A\in\mathcal{F}\), we have \(\nu(A)=0\). In this case we write \(\nu\ll\mu\)._

The next theorem is the Radon-Nikodym theorem, which says that not only (1.11) implies \(\nu\ll\mu\), but the converse implication is also true provided that \(\mu\) and \(\nu\) are \(\sigma\)-finite.

**Theorem 1.11** (Radon-Nikodym Theorem): _Suppose that \(\mu\) and \(\nu\) are \(\sigma\)-finite measures. Then the following two statements are equivalent:_

_1. \(\nu\ll\mu\),_

_2. there exists a measurable \(\delta\geq 0\) such that \(\nu(A)=\int_{A}\delta d\mu\) for all \(A\in\mathcal{F}\)._

By Corollary 1.1, if \(\delta\geq 0\) and \(\delta^{\prime}\geq 0\) both satisfy \(2\) then \(\delta=\delta^{\prime}\) a.e. \(\mu\). The function \(\delta\) is called the Radon-Nikodym derivative of \(\nu\) with respect to \(\mu\), and is written as \(d\nu/d\mu\). The Radon-Nikodym derivative \(d\nu/d\mu\) is also known as the density of \(\nu\) with respect to \(\mu\). Radon-Nikodym theorem is the key to many important probability concepts, such as conditional probability and conditional expectation. The next theorem generalizes the equality \(\int I_{A}d\nu=\int I_{A}\delta d\mu\) to arbitrary measurable functions.

**Theorem 1.12**: _Suppose \(\delta\) is the density of \(\nu\) with respect to \(\mu\) and \(f\) is measurable \(\mathcal{F}\). Then, \(f\) is integrable with respect to \(\nu\) if and only if \(f\delta\) is integrable with respect to \(\mu\), in which case_

\[\int_{A}fd\nu=\int_{A}f\delta d\mu.\]

_for all \(A\in\mathcal{F}\)._

The next theorem connects three probability measures.

**Theorem 1.13**: _Let \(P,Q,\mu\) be probability measures on a measurable space \((\Omega,\mathcal{F})\). If \(P\ll Q\ll\mu\ll P\), then_

\[\mu\{p=0\}+\mu\{q=0\}=0 \tag{1.12}\] \[P\left(\frac{q}{p}\frac{dP}{dQ}=1\right)=1, \tag{1.13}\]

_where \(p=\frac{dP}{d\mu},q=\frac{dQ}{d\mu}\)._

Proof: By Theorems 1.11, and 1.12, we have (1.12), and for all \(A\in\mathcal{F}\),

\[P(B)=\int_{B}\frac{dP}{dQ}\,dQ=\int_{B}\frac{dP}{dQ}\,q\,d\mu.\]

By (1.12), the right-hand side can be rewritten as

\[\int_{B,p>0}\frac{dP}{dQ}\,q\,d\mu=\int_{B,p>0}\frac{dP}{dQ}\,\frac{q}{p}\,p\, d\mu=\int_{B,p>0}\frac{dP}{dQ}\,\frac{q}{p}\,dP=\int_{B}\frac{dP}{dQ}\,\frac{q}{p} \,dP.\]

The result now follows by another application of Theorem 1.11.

[MISSING_PAGE_FAIL:27]

**Theorem 1.16** (Tonelli's Theorem): _Suppose that \((\Omega_{1},\mathcal{F}_{1},\mu_{1})\) and \((\Omega_{2},\mathcal{F}_{2},\)\(\mu_{2})\) are \(\sigma\)-finite measure spaces and \(f:\Omega\rightarrow\mathbb{R}\) is a nonnegative and measurable (\(\mathcal{F}/\mathcal{R}\)) function. Then the functions_

\[\int_{\Omega_{2}}f(\omega_{1},\omega_{2})\mu_{2}(d\omega_{2}),\ \ \int_{\Omega_{1}}f(\omega_{1},\omega_{2})\mu_{1}(d\omega_{1}) \tag{1.14}\]

_are measurable with respect to \(\mathcal{F}_{1}\) and \(\mathcal{F}_{2}\), respectively, and_

\[\int_{\Omega_{1}}\left[\int_{\Omega_{2}}f(\omega_{1},\omega_{2}) \mu_{2}(d\omega_{2})\right]\mu_{1}(d\omega_{1})= \int_{\Omega_{2}}\left[\int_{\Omega_{1}}f(\omega_{1},\omega_{2}) \mu_{1}(d\omega_{1})\right]\mu_{2}(d\omega_{2})\] \[= \int_{\Omega}f(\omega_{1},\omega_{2})\pi(d(\omega_{1},\omega_{2})). \tag{1.15}\]

Thus, when \(f\) is nonnegative and measurable, its integration with respect to the product measure can always be computed iteratively with respect to one measure at a time, and the order of the iterative integration does not matter. If \(f\) is not nonnegative, this is still true but requires the additional condition that \(f\) be integrable with respect to \(\pi\). This is called Fubini's Theorem.

**Theorem 1.17** (Fubini's Theorem): _Suppose \((\Omega_{1},\mathcal{F}_{1},\mu_{1})\) and \((\Omega_{2},\mathcal{F}_{2},\mu_{2})\) are \(\sigma\)-finite measure spaces and \(f:\Omega\rightarrow\mathbb{R}\) is measurable (\(\mathcal{F}/\mathcal{R}\)) and integrable with respect to \(\pi\). Then the functions defined in (1.14) are finite and measurable on \(A_{1}\) and \(A_{2}\), respectively, with \(\mu_{1}(\Omega_{1}\setminus A_{1})=0\) and \(\mu_{2}(\Omega_{2}\setminus A_{2})=0\). Moreover, equality (1.15) still holds._

### Conditional probability

Let \((\Omega,\mathcal{F},P)\) be a probability space, and let \(A\) be a member of \(\mathcal{F}\). From the elementary probability theory we know that if \(B\) is a member of \(\mathcal{F}\) such that \(P(B)\neq 0\), then the conditional probability of \(A\) given \(B\) is

\[P(A|B)=\frac{P(A\cap B)}{P(B)}.\]

We now define the conditional probability of \(A\) given a \(\sigma\)-field, which is a generalization of the conditional probability given a set.

Let \(\mathcal{G}\subset\mathcal{F}\) be a \(\sigma\)-field. Let \(\nu\) be the set function on \(\mathcal{G}\) given by

\[\nu(G)=P(A\cap G)\ \ \text{for}\ \ G\in\mathcal{G}\]

It is true that \(\nu\) is a measure on \(\mathcal{G}\). The measure \(P\), being a measure on \(\mathcal{F}\), is also a measure on \(\mathcal{G}\). Moreover, if \(P(G)=0\), then

\[\nu(G)=P(G\cap A)=0.\]Hence \(\nu\ll P\). By the Radon-Nikodym Theorem there is a nonnegative function \(f\) that is measurable with respect to \(\mathcal{G}\) such that for all \(G\in\mathcal{G}\), \(\nu(G)=\int_{G}fdP\). That is

\[P(G\cap A)=\int_{G}fdP\ \ \mbox{for all}\ \ G\in\mathcal{G}.\]

Furthermore, by Corollary 1.1, if there is a nonnegative function \(g\) that is measurable with respect to \(\mathcal{G}\) satisfying the above relation, then \(g=f\) a.e. \(P\). This function \(f\) is a version of conditional probability of \(A\) given \(\mathcal{G}\). More generally, we have the following definition.

**Definition 1.6**: _Let \(A\in\mathcal{F}\), and \(\mathcal{G}\) be a sub-\(\sigma\)-field of \(\mathcal{F}\). Then any function \(f:\Omega\rightarrow\mathbb{R}\) that satisfies the following conditions_

_1. \(f\) is measurable \(\mathcal{G}\) and integrable \(P\),_

_2. for each \(G\in\mathcal{G}\),_

\[\int_{G}fdP=P(A\cap G),\]

_is called the conditional probability of \(A\) given \(\mathcal{G}\), and is written as \(P(A|\mathcal{G})\)._

We emphasize that the conditional probability \(P(A|\mathcal{G})\) is defined as a \(\mathcal{G}\)-measurable function rather than a number. Following Billingsley (1995, Section 33) we use \(P(A|\mathcal{G})_{\omega}\) to evaluation of this function at \(\omega\). The next theorem shows that this function _resembles_ a probability even thought it is not a probability.

**Theorem 1.18**: _The function \(P(A|\mathcal{G})\) has the following properties almost everywhere \(P\):_

_1. \(0\leq P(A|\mathcal{G})\leq 1\),_

_2. \(P(\varnothing|\mathcal{G})=0\),_

_3. If \(A_{n}\) is a sequence of disjoint \(\mathcal{F}\)-sets, then \(P(\cup_{n}A_{n}|\mathcal{G})=\sum_{n}P(A_{n}|\mathcal{G})\)._

Proof: 1. We know, for any \(G\in\mathcal{G}\), \(\int_{G}P(A|\mathcal{G})dP=P(A\cap G)\). Hence, for any \(G\in\mathcal{G}\),

\[0\leq\int_{G}P(A|\mathcal{G})dP\leq P(G).\]

By the first inequality and Corollary 1.2, \(P(A|\mathcal{G})\geq 0\) a.e. \(P\). The second inequality implies that \(\int_{G}(P(A|\mathcal{G})-1)dP\leq 0\) for all \(G\in\mathcal{G}\). By Corollary 1.2 again, \(P(A|\mathcal{G})-1\leq 0\) a.e. \(P\). This proves part 1.

2. Since \(f=0\) satisfies

\[\int_{G}0dP=P(\varnothing\cap G)\]for all \(G\in{\cal G}\), \(f\) is a version of \(P(\varnothing|G)\).

3. Let \(f=\sum_{n}P(A_{n}|{\cal G})\). Then

\[\int_{G}fdP=\int_{G}\sum_{n}P(A_{n}|{\cal G})dP=\sum_{n}\int_{G}P(A _{n}|{\cal G})dP\] \[= \sum_{n}P(A_{n}\cap G)=P(A\cap G).\]

Hence \(\sum_{n}P(A_{n}|{\cal G})\) is a version of \(P(A|{\cal G})\). 

Now let \(X:\Omega\to{\mathbb{R}}\) be a random variable. Let \(B\in{\cal R}\). We write

\[P(X^{-1}(B)|{\cal G})=P(\{\omega:X(\omega)\in B\}|{\cal G})\]

as \(P(X\in B|{\cal G})\). So far we have defined the conditional probability as a \({\cal G}\)-measurable function on \(\Omega\) for a fixed set \(B\in{\mathbb{R}}\). Since this construction can be carried out for each \(B\in{\cal R}\), \(P(X\in B|{\cal G})\) can also be viewed as a mapping from \({\cal R}\times\Omega\) to \([0,1]\). By intuition, as \(B\) varies in \({\cal R}\), and for a fixed \(\omega\in\Omega\), \(P(X\in B|{\cal G})\) should behave like a probability measure on \(\Omega\). If fact, if it were not for the qualification "almost everywhere P", Theorem 1.18 amounts exactly to this statement. This statement is valid in the following sense.

### Conditional expectation

Let \((\Omega,{\cal F},P)\) be a probability space and \({\cal G}\subset{\cal F}\) be a sub \(\sigma\)-field. The definition of conditional expectation has similar motivation as that of conditional probability, as demonstrated in the proof of the next theorem.

**Theorem 1.19**: _Suppose that \(f:\Omega\to{\mathbb{R}}\) is measurable \({\cal F}\) and integrable \(P\). Suppose \({\cal G}\) is a sub \(\sigma\)-field of \({\cal F}\). Then there is a function \(f_{0}:\Omega\to{\mathbb{R}}\) such that_

_1. \(f_{0}\) is measurable \({\cal G}\) and integrable \(P\),_

_2. \(\int_{G}f_{0}dP=\int_{G}fdP\) for all \(G\in{\cal G}\)._

Proof: First, suppose that \(f\geq 0\). Define \(\nu:{\cal G}\to{\mathbb{R}}\) as the set function

\[\nu(G)=\int_{G}fdP,\ \ G\in{\cal G}.\]

Then, it can be shown that \(\nu\) is a measure on \({\cal G}\). Moreover, if \(P(G)=0\), then \(\nu(G)=\int_{G}fdP=0\). Hence \(\nu\ll P\). By the Radon-Nikodym Theorem, there is a nonnegative function \(f_{0}:\Omega\to{\mathbb{R}}\), measurable \({\cal G}\), \(f_{0}\geq 0\), such that

\[\nu(G)=\int_{G}f_{0}dP\ \ \mbox{for all}\ G\in{\cal G}. \tag{1.16}\]More generally, suppose that \(f\) is measurable \(\mathcal{F}\) and integrable \(P\). Let \(f=f^{+}-f^{-}\). Then \(f^{+}\) and \(f^{-}\) are also measurable \(\mathcal{F}\) and integrable \(P\). Let \(f_{0}^{+}\) and \(f_{0}^{-}\) be constructed as above, then \(f_{0}\) is measurable \(\mathcal{G}\) and integrable \(P\) such that (1.16) is satisfied. \(\Box\)

This leads to the definition of conditional expectation.

**Definition 1.7**: _Let \(f:\Omega\to\mathbb{R}\) be a mapping that is measurable \(\mathcal{F}\) and integrable \(P\). Suppose \(f_{0}:\Omega\to\mathbb{R}\) is a mapping satisfying the following conditions:_

_1. \(f_{0}\) is measurable \(\mathcal{G}\) and integrable \(P\),_

_2. For all \(G\in\mathcal{G}\) we have \(\int_{G}fdP=\int_{G}f_{0}dP\)._

_Then the function \(f_{0}\) is called a version of the conditional expectation of \(f\) given \(\mathcal{G}\), and is written as \(E(f|\mathcal{G})\)._

Again, if the sub \(\sigma\)-field \(\mathcal{G}\) is generated by some random element \(T\), then the \(E(f|\sigma(T))\) is abbreviated as \(E(f|T)\). Also notice that \(E(I_{A}|\mathcal{G})\) coincides with the definition of \(P(A|\mathcal{G})\).

We now describe several useful properties of the conditional expectation. The next theorem and corollary connect the conditional expectation and the projection in \(L_{2}(P)\)-space.

**Theorem 1.20**: _Let \((\Omega,\mathcal{F},P)\) be a probability space, and \(\mathcal{G}\) be a sub \(\sigma\)-field of \(\mathcal{F}\). Suppose \(f:\Omega\to\mathbb{R}\) is measurable \(\mathcal{F}/\mathcal{R}\) and integrable with respect to \(P\). Then the following statements are equivalent:_

_1. \(h=E(f|\mathcal{G})\)\([P]\);_

_2. for any function \(g:\Omega\to\mathcal{R}\) measurable \(\mathcal{G}/\mathcal{R}\) such that \(fg\) is integrable \(P\), we have_

\[\int(f-h)gdP=0. \tag{1.17}\]

_Proof._\(1\Rightarrow 2\). By 1, for any \(G\in\mathcal{G}\), \(\int I_{ G}hdP=\int I_{ G}fdP\). In other words, equality (1.17) holds for all indicator functions measurable \(\mathcal{G}\). Use three-step argument to complete the proof of this part.

\(2\Rightarrow 1\). Any \(I_{ G}\) is measurable \(\mathcal{G}\) such that \(fI_{ G}\) is integrable. \(\Box\)

Let \(L_{2}(P)\) be the collection of all functions that are measurable \(\mathcal{F}/\mathcal{R}\) and square-integrable \(P\); that is,

\[\{f:\ f\ \text{measurable}\ \mathcal{F}/\mathcal{R},\ \int f^{2}dP<\infty\}.\]

**Corollary 1.3**: _Let \((\Omega,\mathcal{F},P)\) be a probability space, and \(\mathcal{G}\) be a sub \(\sigma\)-field of \(\mathcal{F}\). Suppose \(f\in L_{2}(P)\). Then the following statements are equivalent:_

_1. \(h=E(f|\mathcal{G})\)\([P]\);__2. for any function \(g:\Omega\to{\cal R}\), \(g\in L_{2}(P)\), we have_

\[\int(f-h)gdP=0. \tag{1.18}\]

Proof: We only need to show \(fg\) is integrable. This follows from Holder's inequality (1.3). \(\Box\)

Corollary 1.3 can be interpreted as \(E(f|{\cal G})\) is the projection of \(f\) on to the subspace of \(L_{2}(P)\) consisting of members of \(L_{2}(P)\) that are measurable with respect to \({\cal G}\). Define the inner product in \(L_{2}(P)\) as

\[\langle f_{1},f_{2}\rangle=\int f_{1}f_{2}dP.\]

Then \(L_{2}(P)\), along with this inner product, is a Hilbert space. Let \(L_{2}(P|{\cal G})\) be the subset of \(L_{2}(P)\) consisting of measurable functions of \({\cal G}\) that are \(P\)-square integrable. Then, it can be shown that \(L_{2}(P|{\cal G})\) is a subspace of \(L_{2}(P)\). The identity (1.18) can be rewritten in the form

\[\langle f-E(f|{\cal G}),h\rangle=0\]

for all \(h\in L_{2}(P|{\cal G})\). This is precisely the definition of projection of \(f\) on to the subspace \(L_{2}(P|{\cal G})\). See, for example, Conway (1990, page 10). The next Proposition is another useful property of conditional expectation.

**Proposition 1.3**_Suppose \(U,V\) are members of \(L_{2}(P)\) and \({\cal G}\) is a sub \(\sigma\)-field of \({\cal F}\). Then_

\[E[E(U|{\cal G})V]=E[UE(V|{\cal G})]. \tag{1.19}\]

Proof: We have

\[E[E(U|{\cal G})V] = E\{E[(U|{\cal G})V]|{\cal G}\}\] \[= E[E(U|{\cal G})E(V|{\cal G})]\] \[= E\{E[E(V|{\cal G})U|{\cal G}]\}\] \[= E[UE(V|{\cal G})].\]

This completes the proof. \(\Box\)

The identity (1.19) can be generalized to random vectors in an obvious way. This proposition can be interpreted as "the conditional expectation is a self-adjoint operator." We can regard \(A:U\mapsto E(U|{\cal G})\) as a mapping from \(L_{2}(P)\) to \(L_{2}(P)\) that transforms any member \(U\) of \(L_{2}(P)\) to the member \(E(U|{\cal G})\) of \(L_{2}(P|{\cal G})\). Thus, the identity (1.19) can be rewritten as

\[\langle AU,V\rangle=\langle U,AV\rangle,\]

which is the defining relation of a self-adjoint operator, see for example Conway (1990, page 33).

### 1.14 Conditioning on a random element

In the last two sections we considered conditional probability and expectation conditioned on a sub \(\sigma\)-field \(\mathcal{G}\) of \(\mathcal{F}\). We now consider the special case where \(\mathcal{G}\) is generated by random elements. Let \((\Omega_{1},\mathcal{F}_{1},P)\) be a probability space and \((\Omega_{2},\mathcal{F}_{2})\) be a measurable space. Let \(T:\Omega_{1}\to\Omega_{2}\) be a measurable function. Let \(T^{-1}(\mathcal{F}_{1})\) denote the collection of sets \(\{T^{-1}(A)\in\mathcal{F}_{1}:A\in\mathcal{F}_{2}\}\). Recall that \(\sigma(T)\) is the intersection of all sub \(\sigma\)-fields of \(\mathcal{F}_{1}\) with respect to which \(T\) is measurable. The following fact would be useful.

**Lemma 1.1**: \(T^{-1}(\mathcal{F}_{2})=\sigma(T)\)_._

Proof: It suffices to show the following three statements

1. \(T^{-1}(\mathcal{F}_{2})\) is a \(\sigma\)-field;
2. \(T\) is measurable \(T^{-1}(\mathcal{F}_{2})/\mathcal{F}_{2}\);
3. for each \(A\in\mathcal{F}_{2}\), \(T^{-1}(A)\in\sigma(T)\).

The details are left as an exercise. 

For a function \(f:\Omega_{1}\to\mathbb{R}\) measurable \(\mathcal{F}_{1}/\mathcal{R}\), we would like to further investigate the special conditional expectation \(E(f|\sigma(T))\), where \(\sigma(T)\) is the intersection of all sub \(\sigma\)-field of \(\mathcal{F}_{1}\) with respect to which \(T\) is measurable. Recall that \(E(f|\sigma(T))\) is abbreviate as \(E(f|T)\).

**Lemma 1.2**: _A function \(h:\Omega_{1}\to\mathbb{R}\) is measurable \(T^{-1}(\mathcal{F}_{2})/\mathcal{R}\) if and only if there is a mapping \(g:\Omega_{2}\to\mathbb{R}\) that is measurable with respect to \(\mathcal{F}_{2}/\mathcal{R}\) such that \(h=g\circ T\)._

A proof can be found in Halmos and Savage (1949). Recall that \(E(h|T)\) is a function from \(\Omega_{1}\) to \(\mathbb{R}\) that is measurable with respect to \(T^{-1}(\mathcal{F}_{2})/\mathcal{R}\). By the above lemma it can be written as \(g\circ T\) where \(g:\Omega_{2}\to\mathbb{R}\) is measurable \(\mathcal{F}_{2}/\mathcal{R}\). Two functions are of interest here': one is the composite function \(g\circ T\), and the other is the function \(g\) itself. We use \(E(f|\circ T)\) to denote the function \(g\). Thus \(E(f|T)\) is defined on \(\Omega_{1}\), but \(E(f|\circ T)\) is defined on \(\Omega_{2}\).

Halmos and Savage (1949) gave the following construction of the conditional expectation \(E(f|\circ T)\). See also Kolmogorov (1933).

**Theorem 1.21**: _Let \(h:\Omega_{1}\to\mathbb{R}\) be a measurable, nonnegative function that is integrable with respect to \(P\). Let \(Q\) be the measure on \((\Omega_{1},\mathcal{F}_{1})\) defined by \(dQ=hdP\). Then_

1. \(Q\circ T^{-1}\ll P\circ T^{-1}\)_;_
2. \(d(Q\circ T^{-1})/d(P\circ T^{-1})=E(h|\circ T)\)_._

Proof: 1. Suppose \(A\in\mathcal{F}_{2}\) and \((P\circ T^{-1})(A)=0\). Then \(P(T^{-1}(A))=0\). Since, by definition, \(Q\ll P\), we have \(Q(T^{-1}(A))=(Q\circ T^{-1})(A)=0\).

2. Let \(g=d(Q\circ T^{-1})/d(P\circ T^{-1})\). Then \(g\) is a function from \(\Omega_{2}\) to \(\mathbb{R}\) measurable \(\mathcal{F}_{2}/\mathcal{R}\). Moreover, for any \(A\in\mathcal{F}_{2}\) we have\[\int_{A}d(Q^{\circ}T^{-1})=\int_{A}gd(P^{\circ}T^{-1}).\]

By change of variable theorem we have

\[\int_{A}d(Q^{\circ}T^{-1})=\int_{T^{-1}(A)}dQ,\ \ \int_{A}gd(P^{\circ}T^{-1})= \int_{T^{-1}(A)}g^{\circ}TdP\]

So we have

\[\int_{T^{-1}(A)}dQ=\int_{T^{-1}(A)}g^{\circ}TdP\]

But by definition of \(Q,\)\(\int_{T^{-1}(A)}dQ=\int_{T^{-1}(A)}hdP.\) In other words for any \(B\in T^{-1}(\mathcal{F}_{2})\) we have

\[\int_{B}hdP=\int_{B}g^{\circ}TdP\]

So \(g^{\circ}T\) is a version of \(E(h|T).\) That is, \(E(h|^{\circ}T)=g.\)\(\Box\)

To familiarize ourselves with the notation \(E(h|^{\circ}T),\) the following relations are helpful:

\[[E(h|^{\circ}T)](T(\omega))=[E(h|T)](\omega),\] \[E(g^{\circ}T|T)=g^{\circ}T,\] \[E(g^{\circ}T|^{\circ}T)=g,\]

where, in the second and third equality, \(g\) is a function from \(\mathcal{F}_{2}\rightarrow\mathbb{R}\) measurable \(\mathcal{F}_{2}/\mathcal{R}.\)

### Conditional distributions and densities

Let \((\Omega,\mathcal{F},P)\) be a probability space, where \(P\ll\mu\) for some \(\sigma\)-finite measure \(\mu\) on \((\Omega,\mathcal{F}).\) Suppose \(X\) and \(Y\) are random elements defined on \((\Omega,\mathcal{F},P),\) with \(X\) taking values in \((\Omega_{X},\mathcal{F}_{X})\) and \(Y\) taking values in \((\Omega_{Y},\mathcal{F}_{Y}).\) Then the mapping

\[\mathcal{F}_{Y}\times\Omega_{X}\rightarrow\mathbb{R},\quad(A,x)\mapsto P(Y^{-1 }(A)|^{\circ}X)_{x}\]

is called the conditional distribution of \(Y\) given \(X.\) This mapping is written as \(P_{Y|X}.\) The conditional distribution \(P_{X|Y}\) is defined in the same way.

Because \(P\ll\mu,\) we have \(P^{\circ}X^{-1}\ll\mu^{\circ}X^{-1},\)\(P^{\circ}Y^{-1}\ll\mu^{\circ}Y^{-1}\) (Problem 1.8). Let

\[P_{X}=P^{\circ}X^{-1},\quad P_{Y}=P^{\circ}Y^{-1},\quad\mu_{X}=P^{\circ}X^{-1 },\quad\mu_{Y}=\mu^{\circ}Y^{-1}.\]Let \(\Omega_{XY}=\Omega_{X}\times\Omega_{Y}\) and \(\mathcal{F}_{XY}=\mathcal{F}_{X}\times\mathcal{F}_{Y}\). Let \((X,Y)\) be the mapping

\[(X,Y):\Omega\to\Omega_{X}\times\Omega_{Y},\quad\omega\mapsto(X(\omega),Y(\omega)).\]

Then \((X,Y)\) is measurable with respect to \(\mathcal{F}/(\mathcal{F}_{X}\times\mathcal{F}_{Y})\) (Problem 1.10). Let \(P_{XY}=P^{\circ}(X,Y)^{-1}\). Then \((\Omega_{XY},\mathcal{F}_{XY},P_{XY})\) is a probability space, \(\mu_{XY}\) is a \(\sigma\)-finite measure on \((\Omega_{XY},\mathcal{F}_{XY})\), and \(P_{XY}\ll\mu_{XY}\). Let

\[f_{XY}=dP_{XY}/d\mu_{XY},\quad f_{X}=dP_{X}/d\mu_{X},\quad f_{Y}=dP_{Y}/d\mu_{ Y}.\]

The function \(f_{XY}\) is called the joint density of \((X,Y)\); \(f_{X}\) the marginal density of \(X\); \(f_{Y}\) the marginal density of \(Y\). Finally, let

\[f_{Y|X}(x|y)=\begin{cases}f_{XY}(x,y)/f_{X}(x)&\text{ if }f_{X}(x)\neq 0\\ 0&\text{ if }f_{X}(x)=0\end{cases}\]

This function is called the conditional density of \(Y\) given \(X\). It can be shown (Problem 1.22) for each \(A\in\mathcal{F}_{Y}\), the function

\[x\mapsto\int_{A}f_{Y|X}(y|x)d\mu_{Y}(y) \tag{1.20}\]

is a version of the conditional probability \(P(A|X)\). That is, the two mappings

\[(A,x)\mapsto P(Y^{-1}(A)|\circ X)_{x},\quad(A,x)\mapsto\int_{A}f_{Y|X}(y|x)d\mu _{Y}(y)\]

are the same function modulo \(P\).

### Dynkin's \(\pi\)-\(\lambda\) theorem

Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(\mathcal{G}\subseteq\mathcal{F}\) be a sub-\(\sigma\)-field. Let \(f\) and \(g\) be \(\mathcal{G}\)-measurable function and are integrable with respect to \(\mu\). We often need to prove that \(f=g\)\([\mu]\). By Corollary 1.1 it suffices to show that

\[\int_{A}fd\mu=\int_{A}gd\mu,\text{ for all }A\in\mathcal{G}. \tag{1.21}\]

Dynkin's \(\pi\)-\(\lambda\) theorem is very useful for this purpose. A class \(\mathcal{P}\) of subsets of \(\Omega\) is called a \(\pi\)-system if

\[A_{1}\in\mathcal{P},\ A_{2}\in\mathcal{P}\ \implies\ A_{1}\cap A_{2}\subseteq \mathcal{P}.\]

A class of subsets \(\mathcal{L}\) of \(\Omega\) is called a \(\lambda\)-system if

1. \(\Omega\in\mathcal{L}\);
2. \(A\in\mathcal{L}\implies A^{c}\in\mathcal{L}\);
3. if \(A_{1},A_{2},\cdots\) are disjoint members of \(\mathcal{L}\), then \(\cup_{n=1}^{\infty}A_{n}\in\mathcal{L}\).

Note that a \(\lambda\)-system is almost a \(\sigma\)-field except that the former requires \(A_{1},A_{2},\ldots\) to be disjoint. So a \(\sigma\)-field is always a \(\lambda\)-system but not vice-versa. The following theorem is the \(\pi\)-\(\lambda\) theorem; its proof can be found in Billingsley (1995, page 42).

**Theorem 1.22** (Dynkin's \(\pi\)-\(\lambda\) Theorem): _If \(\mathcal{P}\) is a \(\pi\)-system, \(\mathcal{L}\) is a \(\lambda\)-system, and \(\mathcal{P}\subseteq\mathcal{L}\), then \(\sigma(\mathcal{P})\subseteq\mathcal{L}\)._

We can use the \(\pi\)-\(\lambda\) theorem to prove \(f=g\) [\(\mu\)] in the following way. Let \(\mathcal{P}\subseteq\mathcal{G}\) be a \(\pi\)-system that generates \(\sigma\)-field \(\mathcal{G}\) and let

\[\mathcal{L}=\big{\{}A\in\mathcal{F}:\int_{A}fd\mu=\int_{A}gd\mu\big{\}} \tag{1.22}\]

If we can show \(\int_{A}fd\mu=\int_{A}gd\mu\) holds on \(\mathcal{P}\) (that is, \(\mathcal{P}\subseteq\mathcal{L}\)) and \(\mathcal{L}\) is a \(\lambda\)-system, then \(\mathcal{G}\subseteq\mathcal{L}\). That is, (1.21) also holds on \(\mathcal{G}\), which implies \(f=g\) [\(\mu\)].

**Corollary 1.4**: _Suppose \(\mathcal{G}\subseteq\mathcal{F}\) is a sub \(\sigma\)-field, \(f\) and \(g\) are real-valued functions on \(\Omega\) that are measurable with respect to \(\mathcal{G}\) and integrable with respect to \(\mu\). Suppose \(\mathcal{P}\subseteq\mathcal{G}\) is a \(\pi\)-system generating \(\mathcal{G}\) and \(\Omega\in\mathcal{P}\). Then \(\int_{A}fd\mu=\int_{A}gd\mu\) for all \(A\in\mathcal{P}\) implies \(f=g\) [\(\mu\)]._

Proof: It suffices to show that \(\mathcal{L}\) defined in (1.22) is a \(\lambda\)-system. Since \(\Omega\in\mathcal{P}\), \(\Omega\in\mathcal{L}\). If \(B\in\mathcal{L}\), then

\[\int_{B^{c}}fd\mu=\int_{\Omega}fd\mu-\int_{B}fd\mu=\int_{\Omega}gd\mu-\int_{B} gd\mu=\int_{B^{c}}gd\mu.\]

So \(B^{c}\in\mathcal{L}\). If \(A_{1},A_{2},\ldots\) are disjoint members of \(\mathcal{L}\), then

\[\int_{\cup_{n=1}^{\infty}A_{n}}fd\mu=\sum_{n=1}^{\infty}\int_{A_{n}}fd\mu=\sum _{n=1}^{\infty}\int_{A_{n}}gd\mu=\int_{\cup_{n=1}^{\infty}A_{n}}gd\mu.\]

So \(\cup_{n=1}^{\infty}A_{n}\in\mathcal{L}\). \(\Box\)

### Derivatives and other notations

We will frequently need to take derivative of a vector-valued function with respect to a vector-valued variable. Let \(A\) be a subset of \(\mathbb{R}^{p}\), \(B\) a subset of \(\mathbb{R}^{q}\), and \(f:A\to B\) a differentiable function. Denoting the argument of \(f\) by \(\theta=(\theta_{1},\ldots,\theta_{p})\) and the components of \(f(\theta)\) by \(f_{1}(\theta),\ldots,f_{q}(\theta)\), we adopt the following convention:

\[\frac{\partial f(\theta)}{\partial\theta^{T}}=\begin{pmatrix}\partial f_{1}( \theta)/\partial\theta_{1}&\cdots&\partial f_{1}(\theta)/\partial\theta_{p} \\ \vdots&\ddots&\vdots\\ \partial f_{q}(\theta)/\partial\theta_{1}&\cdots&\partial f_{q}(\theta)/ \partial\theta_{p}\end{pmatrix}.\]

The transpose of the above matrix will be denoted by \[\left(\frac{\partial f(\theta)}{\partial\theta^{T}}\right)^{T}=\frac{\partial f^{T} (\theta)}{\partial\theta}.\]

This convention also applies to the situation where \(f\) depends on another variable, say \(x\in\mathbb{R}^{k}\); that is, \(f=f(\theta,x)\).

The 3-dimensional array of the second derivatives

\[\{\partial^{2}f_{i}(\theta)/\partial\theta_{j}\partial\theta_{k}:\ i=1,\ldots, p,\ j,k=1,\ldots,p\}\]

will be denoted by \(\partial f(\theta)/\partial\theta\partial\theta^{T}\), which is a \(p\times q\times q\) array.

We will use the letter \(I\) or \(I(\theta)\) to denote the Fisher information (as defined in Section 2.4). We will use \(I_{p}\) to represent the \(p\) by \(p\) identity matrix. The difference between \(I\) and \(I_{p}\) should be emphasized: the symbol with a integer subscript always denotes the identity matrix. In addition, for a set \(A\) and a variable \(x\) we use \(I_{A}(x)\) to represent the indicator function. We should also pay attention to the difference between \(I_{p}\) and \(I_{A}\): the former is indexed by an integer represented by a lowercase letter; the latter is the indicator of a set represented by an uppercase letter.

## Problems

**1.1.** Let \(\Omega\) be a non-empty set. Show that the classes of subsets in \(\Omega\)

\[\mathcal{F}_{1}= \{\text{all subsets of }\Omega\},\] \[\mathcal{F}_{2}= \{\varnothing,\Omega\},\] \[\mathcal{F}_{3}= \{\varnothing,\Omega,A,A^{c}\},\ \text{ where }A\subset\Omega\]

are \(\sigma\)-fields.

**1.2.** Let \(\Omega\) be a set. Let \(\mathcal{F}\) be a finite class of subsets of \(\Omega\). Suppose that \(\mathcal{F}\) satisfies

1. \(\Omega\in\mathcal{F}\);
2. If \(A\in\mathcal{F}\) then \(A^{c}\in\mathcal{F}\);
3. If \(A,B\in\mathcal{F}\) then \(A\cup B\in\mathcal{F}\).

Show that \(\mathcal{F}\) is a \(\sigma\)-field.

**1.3.** Let \(\Omega=\mathbb{R}\). Let \(a\in\mathbb{R}\). Show that the set \(\{a\}\) is a Borel set. Let \(a_{1},\ldots,a_{k}\) be numbers in \(\mathbb{R}\). Show that \(\{a_{1},\ldots,a_{k}\}\) is a Borel set. Show that the set of all rational numbers and the set of all irrational numbers are Borel sets. Show that any countable set in \(\mathbb{R}\) is a Borel set. A set is said to be cocountable if its complement is countable. Show that any cocountable set in \(\mathbb{R}\) is a Borel set.

[MISSING_PAGE_FAIL:38]

**1.11**.: Show that any open set in \(\mathbb{R}\) can be written as a countable union of open intervals, and that the class of all open intervals also generates the Borel \(\sigma\)-field.

**1.12**.: Repeat the above problem with open intervals replaced by sets of the form \((-\infty,a]\), where \(a\in\mathbb{R}\).

**1.13**.: Let \((\Omega,\mathcal{F},\mu)\) be a measure space and \(\{f_{n}\}\) is a sequence of measurable functions from \(\Omega\) to \(\mathbb{R}\). Suppose that \(f_{n}\) is decreasing and \(f_{n}\to f\) a.e. \(\mu\). Suppose, furthermore, that \(f_{n}\) is \(\mu\)-integrable for some \(n\). Use the Monotone Convergence Theorem to show that \(\int f_{n}d\mu\to\int fd\mu\).

**1.14**.: Let \((\Omega,\mathcal{F},P)\) be a probability space. Let \(\{A_{n}:n=1,2,\ldots\}\) be a sequence of \(\mathcal{F}\)-sets. The symbols \(\liminf_{n\to\infty}A_{n}\) and \(\limsup_{n\to\infty}A_{n}\) stand, respectively, for the sets

\[\bigcap_{n=1}^{\infty}\bigcup_{m=n}^{\infty}A_{m}\ \ \text{and}\ \ \bigcup_{n=1}^{ \infty}\bigcap_{m=n}^{\infty}A_{m}.\]

If \(\liminf_{n\to\infty}A_{n}=\limsup_{n\to\infty}f_{n}\), then we call this common set the limit of \(\{A_{n}\}\) and write it as \(\lim_{n\to\infty}A_{n}\).

1. Show that \(\liminf_{n\to\infty}A_{n}\) and \(\limsup_{n\to\infty}f_{n}\) are \(\mathcal{F}\)-sets.
2. Suppose that \(\{A_{n}\}\) has a limit \(\lim_{n}A_{n}=A\). Show that inequalities (1.6) and (1.7) hold for \(f_{n}\) and \(f\) defined by \[f_{n}(\omega)=I_{A_{n}}(\omega),\ \ f(\omega)=I_{A}(\omega).\] Then use these inequalities to show that \(\lim_{n\to\infty}P(A_{n})\) exists and \[\lim_{n}P(A_{n})=P(\lim_{n\to\infty}A_{n}).\] Thus \(P\), as a set function defined on \(\mathcal{F}\), is continuous. The above relation is called the continuity of probability. Evidently, this continuity also applies to any finite measure \(\mu\).

**1.15**.: Deduce the Bounded Convergence Theorem from Lebesgue's Dominated Convergence Theorem.

**1.16**.: Let \(f_{n}(x)\) be the probability density of \(N(0,1/n)\), and let \(\lambda\) be the Lebesgue measure. Show that \(\lim_{n}\int f_{n}d\lambda\neq\int\lim_{n}f_{n}d\lambda\).

**1.17**.: Suppose \((\Omega,\mathcal{F},\mu)\) is a measure space and \(\delta\geq 0\) is integrable with respect to \(\mu\). Then the set function

\[\nu(A)=\int_{A}\delta d\mu\]

defines a measure on \((\Omega,\mathcal{F})\).

**1.18**.: Suppose that \((\Omega,\mathcal{F})=(\Omega^{\prime},\mathcal{F}^{\prime})=(\mathbb{R},\mathcal{R})\). Let \(T:\Omega\to\Omega^{\prime}\) be defined by \(T(x)=x^{2}\). Define on \((\Omega,\mathcal{F})\) the measure

\[\mu(A)=\int_{A}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}d\lambda(x),\ \ \mbox{for any}\ A\in\mathcal{F},\]

where \(\lambda\) is the Lebesgue measure. How is \(\mu\circ T^{-1}\) defined in this case? Express it in terms of its density with respect to the Lebesgue measure. Suppose \(f:\Omega^{\prime}\to\mathbb{R}\) is defined by \(f(y)=\sin(y)\). What does the general formula (1.9) reduce to in this case?

**1.19**.: Let \((\Omega,\mathcal{F},P)\) be a probability space and \(X\) be a nonnegative random variable. Use Tonelli's theorem to show that

\[\int_{0}^{\infty}P(X\geq t)dt=E(X).\]

If \(X\) is not necessarily nonnegative but has an integral, show that

\[E(X)=\int_{0}^{\infty}P(X^{+}\geq t)dt-\int_{0}^{\infty}P(X^{-}\geq t)dt.\]

**1.20**.: Suppose that \((\Omega,\mathcal{F},\mu)\) is a measure space and \(\mathcal{G}\subset\mathcal{F}\) is a \(\sigma\)-field. Show that \(\mu\) is a measure on \(\mathcal{G}\).

**1.21**.: Let \((\Omega,\mathcal{F},P)\) be a probability space and \(\mathcal{G}\) be a sub \(\sigma\)-field of \(\mathcal{F}\). Let \(A\in\mathcal{F}\). Use the definition of conditional probability to show that \(E(I_{A}|\mathcal{G})\) is a version of \(P(A|\mathcal{G})\).

**1.22**.: Use the definition of conditional probability to show that the set function defined in (1.20) is a version of the conditional probability \(P(A|X)\).

**1.23**.: Let \((\Omega,\mathcal{F},P)\) be a probability space and \(X\) be a random variable. Suppose that \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) are sub \(\sigma\)-fields of \(\mathcal{F}\) such that \(\mathcal{G}_{1}\subset\mathcal{G}_{2}\). Use the definition of conditional expectation to show that

\[E[E(X|\mathcal{G}_{2})|\mathcal{G}_{1}]=E(X|\mathcal{G}_{1})\ \mbox{a.e.}\ P.\]

**1.24**.: Let \(P_{1}\) and \(P_{2}\) be two probability measures defined on a measurable space \((\Omega,\mathcal{F})\). Suppose that \(P_{2}\ll P_{1}\). Let \(\delta=dP_{2}/dP_{1}\) be the Radon-Nikodym derivative of \(P_{2}\) with respect to \(P_{1}\). Suppose that \(\delta>0\) a.e. \(P_{1}\). Show that \(\int_{\Omega}\log(\delta)dP_{1}\leq 0\) unless \(\delta=1\) a.e. \(P_{1}\).

**1.25**.: Let \((\Omega_{1},\mathcal{F}_{1})\) and \((\Omega_{2},\mathcal{F}_{2})\) be measurable spaces and \(T:\Omega_{1}\to\Omega_{2}\) is measurable \(\mathcal{F}_{1}/\mathcal{F}_{2}\). Show that \(T^{-1}(\mathcal{F}_{2})\) is the smallest sub \(\sigma\)-field of \(\mathcal{F}_{1}\) with respect to which \(T\) is measurable.

**1.26**.: If \(V\geq 0\) is a random variable and \(U\) is \(k\)-dimensional random vector, then \(\mu(B)=E\left(I_{B}(U)V\right)\) defines a measure on \(\mathbb{R}^{k}\).

## References

* [BillingsleyBillingsley1995] Billingsley, P. (1995). _Probablity and Measure_. Third Edition. Wiley.
* [ConwayConway1990] Conway, J. B. (1990). _A course in functional analysis_. Second edition. Springer, New York.
* [Halmos and SavageHalmos and Savage1949] Halmos, P. R. and Savage, L. J. (1949). Application of the Radon-Nikodym Theorem to the Theory of Sufficient Statistics. _The Annals of Mathematical Statistics_, **20**, 225-241.
* [KolmogorovKolmogorov1933] Kolmogorov, A. N. (1933). _Grundbegriffe der Wahrscheinlichkeitsrechnung_ (in German). Berlin: Julius Springer. Translation: _Foundations of the Theory of Probability_ (2nd ed.). New York: Chelsea. (1956).
* [PerlmanPerlman1974] Perlman, M. D. (1974). Jensen's inequality for a convex vector-valued function on an infinite-dimensional space. _Journal of Multivariate Analysis_, **4**, 52-65.
* [RudinRudin1987] Rudin, W. (1987). _Real and Complex Analysis_. Third Edition. McGraw-Hill, Inc.
* [VestrupVestrup2003] Vestrup, E. M. (2003). _The Theory of Measures and Integration_. Wiley.

[MISSING_PAGE_EMPTY:5875]

_We write this statement as \(\mathfrak{M}\ll\mathfrak{N}\). If \(\mathfrak{M}\ll\mathfrak{N}\) and \(\mathfrak{N}\ll\mathfrak{M}\), then we write \(\mathfrak{M}\equiv\mathfrak{N}\)._

If \(\mathfrak{M}\ll\mathfrak{N}\) and \(\mathfrak{N}\) is a singleton \(\{\lambda\}\), then we also say \(\mathfrak{M}\) is dominated by \(\lambda\). Furthermore, if both \(\mathfrak{M}\) and \(\mathfrak{N}\) are singletons, say \(\{\lambda_{1}\}\) and \(\{\lambda_{2}\}\), then we also say \(\lambda_{1}\) is dominated by \(\lambda_{2}\) (which agrees with our definition of \(\lambda_{1}\ll\lambda_{2}\) in Chapter 1). Here, a useful fact is that a \(\sigma\)-finite measure is always equivalent to a finite measure. Indeed, let \(\lambda\) be a \(\sigma\)-finite measure on \((\Omega,\mathcal{F})\), and let \(\{A_{1},A_{2},\ldots\}\) of members of \(\mathcal{F}\) such that \(\cup A_{n}=\ \Omega\) and \(\lambda(A_{n})<\infty\). Without loss of generality, assume \(\lambda(A_{n})>0\) for all \(n\in\mathbb{N}\). Let \(c_{n}\) be a sequence of positive number such that \(\sum_{n=1}^{\infty}c_{n}=1\). Let \(\lambda^{*}\) be the set function defined by

\[\lambda^{*}(A)=\sum_{n=1}^{\infty}c_{n}\lambda(A\cap A_{n})/\lambda(A_{n}),\]

It is left as an exercise to show that \(\lambda^{*}\) is a probability measure, and \(\lambda^{*}\equiv\lambda\).

**Definition 2.2**: _Let \((\Omega,\mathcal{F})\) be a measurable space. We say that a family of measures \(\mathfrak{M}\) on \((\Omega,\mathcal{F})\) is dominated if it is dominated by a \(\sigma\)-finite measure._

A property of a dominated family is that it has an equivalent countable subfamily. See, for example, Halmos and Savage (1949) and Lin'kov (2005).

**Proposition 2.1**: _Let \(\mathfrak{M}\) be a family of probability measures on a measurable space \((\Omega,\mathcal{F})\). Then \(\mathfrak{M}\) is dominated if and only if it contains a countable set of probability measures \(\mathfrak{N}\) such that \(\mathfrak{N}\equiv\mathfrak{M}\)._

Proof: Since any \(\sigma\)-finite measure is equivalent to a finite measure, we can, without loss of generality, assume the dominating measure \(\lambda\) to be a finite measure. For each \(\mu\in\mathfrak{M}\), let \(f_{\mu}=d\mu/d\lambda\), and let \(K_{\mu}=\{f_{\mu}>0\}\). An \(\mathcal{F}\)-set \(K\) is called a kernel if there is a \(\mu\in\mathfrak{M}\) such that \(\mu(K)>0\) and \(K\subseteq K_{\mu}\). Any countable, disjoint union of kernels is called a chain.

We note that a union of countably many chains is a chain. This is because such a set is the union of countably many kernels, which need not be disjoint. Let us write this union as \(\cup_{i\in\mathbb{N}}K_{i}\). Let

\[M_{1}=K_{1},\ M_{2}=K_{2}\setminus K_{1},\ M_{3}=K_{3}\setminus(K_{1}\cup K_{ 2}),\ldots\]

Then \(M_{i}\) are disjoint kernels satisfying \(\cup_{i\in\mathbb{N}}M_{i}=\cup_{i\in\mathbb{N}}K_{i}\). Thus we see that \(\cup K_{i}=\cup M_{i}\) is indeed a chain.

Now let \(\mathcal{C}\) be the collection of all chains and let \(\lambda^{\circ}=\sup\{\lambda(C):C\in\mathcal{C}\}\). Then there is a sequence \(\{C_{n}:n\in\mathbb{N}\}\) such that \(\lambda(C_{n})\rightarrow\lambda^{\circ}\). Let \(C^{\circ}=\cup_{n\in\mathbb{N}}C_{n}\). Since \(C_{n}\subseteq C^{\circ}\), \(\lambda(C_{n})\leq\lambda(C^{\circ})\), and hence \(\lambda(C^{\circ})\geq\lambda^{\circ}\). Since \(C^{\circ}\) is a chain, \(\lambda(C^{\circ})\leq\lambda^{\circ}\). So \(\lambda(C^{\circ})=\lambda^{\circ}\).

Since \(C^{\circ}\) is a chain, it can be written as a disjoint union \(\cup_{i=1}^{\infty}K_{i}\), where \(K_{i}\subseteq K_{\mu_{i}}\) for some \(\mu_{i}\in\mathfrak{M}\). Let \(\mathfrak{N}=\{\mu_{1},\mu_{2},\ldots\}\). Note that \(\mu_{i}\) are probability measures. We now show that \(\mathfrak{N}\equiv\mathfrak{M}\). Since \(\mathfrak{N}\subseteq\mathfrak{M}\), we have \(\mathfrak{N}\ll\mathfrak{M}\).

To show \(\mathfrak{M}\ll\mathfrak{N}\), let \(E\) be a member of \(\mathcal{F}\) such that \(\mu_{i}(E)=0\) for all \(i\in\mathbb{N}\). Let \(\mu\) be a member of \(\mathfrak{M}\). We need to show that \(\mu(E)=0\). Note that

\[\mu(E)=\mu(EK_{\mu}^{c})+\mu(EK_{\mu})=\mu(EK_{\mu}).\]

We will show that \(\mu(EK_{\mu})=0\). This measure can be decomposed as

\[\mu(EK_{\mu})=\mu(EK_{\mu}C^{\circ})+\mu(EK_{\mu}(C^{\circ})^{c}). \tag{2.1}\]

Suppose \(\mu(EK_{\mu}(C^{\circ})^{c})>0\). Then \(\lambda(EK_{\mu}(C^{\circ})^{c})>0\), and hence

\[\lambda(EK_{\mu}(C^{\circ})^{c}\cup C^{\circ})=\lambda(EK_{\mu}(C^{\circ})^{c} )+\lambda(C^{\circ})>\lambda^{\circ}. \tag{2.2}\]

However, because \(EK_{\mu}\subseteq K_{\mu}\), and

\[\mu(EK_{\mu})\geq\mu(EK_{\mu}(C^{\circ})^{c})>0,\]

(as implied by \(\lambda(EK_{\mu}(C^{\circ})^{c})>0\)), the set \(EK_{\mu}(C^{\circ})^{c}\cup C^{\circ}\) is itself a chain. Thus the inequality (2.2) is impossible, which implies \(\lambda(EK_{\mu}(C^{\circ})^{c})=0\), which implies that the second term on the right hand side of (2.1) is \(0\).

Next, we show that the first term on the right hand side of (2.1) is also \(0\). Since \(\mu_{i}(E)=0\), we have \(\mu_{i}(EK_{\mu}K_{i})=0\). In other words

\[\int_{EK_{\mu}K_{i}}f_{\mu_{i}}d\lambda=0\]

Because \(EK_{\mu}K_{i}\subseteq K_{\mu_{i}}\), the density \(f_{\mu_{i}}\) is positive on this set. Then the above equality implies \(\lambda(EK_{\mu}K_{i})=0\). However, because this is true for all \(i\in\mathbb{N}\), we have \(\lambda(EK_{\mu}C^{\circ})=0\). Hence \(\mu(EK_{\mu}C^{\circ})=0\). \(\Box\)

The next example illustrate the meaning of this proposition.

**Example 2.1** Let \(\Omega=(0,\infty)\) and \(\mathcal{F}=\mathcal{R}\cap(0,\infty)\), and consider the family of distributions \(\mathfrak{M}=\{P_{a}:a>0\}\) defined on \((\Omega,\mathcal{F})\), where \(P_{a}\) is the uniform distribution \(U(0,a)\). That is, for any \(B\in\mathcal{F}\),

\[P_{a}(B)=a^{-1}\lambda(B\cap(0,a)),\]

where \(\lambda\) is the Lebesgue measure. Let \(\lambda_{0}\) be the Lebesgue measure on \((0,\infty)\). Then it is easy to see that \(\mathfrak{M}\ll\lambda_{0}\). Let \(\mathfrak{N}=\{P_{n}:n=1,2,\ldots\}\). Let us show that \(\mathfrak{N}\equiv\mathfrak{M}\). Since \(\mathfrak{N}\subseteq\mathfrak{M}\), we have \(\mathfrak{N}\ll\mathfrak{M}\). To prove the opposite direction, let \(B\) be a member of \(\mathcal{F}\) such that \(P_{a}(B)>0\) for some \(a\). Let \(n\) be an integer greater than \(a\). Then

\[P_{a}(B)>0\Rightarrow\lambda_{0}(B\cap(0,a))>0\Rightarrow\lambda_{0}(B\cap(0, n))>0\Rightarrow P_{n}(B)>0.\]

This shows \(\mathfrak{M}\ll\mathfrak{N}\).

Let us create a probability measure \(P\) that is equivalent to \(\mathfrak{M}\). For any \(B\in\mathcal{F}\), let \[P(B)=\sum_{n=1}^{\infty}2^{-n}P_{n}(B).\]

Then clearly \(P\) is a probability measure on \((\Omega,\mathcal{F})\). Furthermore, \(P(B)>0\) if and only if \(P_{n}(B)>0\) for some \(n\). Thus \(P\equiv\mathfrak{N}\equiv\mathfrak{M}\). \(\Box\)

A special case of the dominated family is the homogeneous family. A family of distribution \(\mathfrak{M}\) on \((\Omega,\mathcal{F})\) is _homogeneous_ if any pair of members of \(\mathfrak{M}\) are equivalent. For example, The family of distributions \(\{N(\mu,1):\mu\in\mathbb{R}\}\) is a homogeneous family.

#### 2.1.2 Parametric families

Let \(\Theta\) be a subset of \(\mathbb{R}^{p}\), and \(\mathfrak{M}^{*}\) be the family of all distributions on \((\Omega,\mathcal{F})\). Let \(P:\Theta\rightarrow\mathfrak{M}^{*}\) be a function. Then the range of \(P\), \(\mathfrak{M}=\{P_{\theta}:\theta\in\Theta\}\), is called a parametric family. A basic assumption for a parametric family is identifiability.

**Definition 2.3**: _A parametric family \(\{P_{\theta}:\theta\in\Theta\}\) is said to be identifiable if \(P:\Theta\rightarrow\mathfrak{M}^{*}\) is an injection. That is, whenever \(\theta_{1}\neq\theta_{2}\), we have \(P_{\theta_{1}}\neq P_{\theta_{2}}\), or equivalently, there is a set \(B\in\mathcal{F}\) such that \(P_{\theta_{1}}(B)\neq P_{\theta_{2}}(B)\)._

We say that a family of distributions \(\mathfrak{M}\) on \((\Omega,\mathcal{F})\) is a model if the true distribution belongs to the family \(\mathfrak{M}\). An important property of an identifiable parametric family is the likelihood inequality, as given in the next theorem. In the following, for a random variable \(Y\) defined on \((\Omega,\mathcal{F})\), we use \(E_{\theta}(Y)\) to denote \(\int YdP_{\theta}\).

**Theorem 2.1**: _Let \(\{P_{\theta}:\theta\in\Theta\}\), \(\Theta\subseteq\mathbb{R}^{p}\), be a homogeneous and identifiable parametric family dominated by a \(\sigma\)-finite measure \(\lambda\). Let \(f_{\theta}=dP_{\theta}/d\lambda\). Let \(\theta_{0}\in\Theta\). Suppose \(\log f_{\theta}\) is integrable with respect to \(P_{\theta_{0}}\) for each \(\theta\in\Theta\). Then, for any \(\theta\neq\theta_{0}\),_

\[E_{\theta_{0}}(\log f_{\theta})<E_{\theta_{0}}(\log f_{\theta_{0}}).\]

Proof: Since \(P_{\theta}\ll P_{\theta_{0}}\), \(P_{\theta}\ll\lambda\), and \(P_{\theta_{0}}\ll\lambda\), the Radon-Nikodym derivative \(dP_{\theta}/dP_{\theta_{0}}\) is defined and

\[dP_{\theta}/dP_{\theta_{0}}=f_{\theta}/f_{\theta_{0}}.\]

By Jensen's inequality,

\[E_{\theta_{0}}\log(f_{\theta}/f_{\theta_{0}})\leq\log[E_{\theta_{0}}(f_{ \theta}/f_{\theta_{0}})]=\log\int\frac{dP_{\theta}}{dP_{\theta_{0}}}\;dP_{ \theta_{0}}=\log 1=0. \tag{2.3}\]

By identifiability, whenever \(\theta\neq\theta_{0}\), there is a set \(B\in\mathcal{F}\) such that \[\int_{B}f_{\theta_{0}}d\lambda\neq\int_{B}f_{\theta}d\lambda.\]

This implies \(\lambda(f_{\theta_{0}}\neq f_{\theta})>0\). If \(f_{\theta}/f_{\theta_{0}}=c\) a.e. \(\lambda\) for some constant \(c\), then \(f_{\theta}c=f_{\theta_{0}}\) a.e. \(\lambda\). Integrating both sides of this equation with respect to \(\lambda\) gives \(c=1\), which implies \(f_{\theta}=f_{\theta_{0}}\) a.e. \(\lambda\), which is impossible. Hence \(f_{\theta}/f_{\theta_{0}}\) is nondegenerate under \(\lambda\), and hence also nondegenerate under \(P_{\theta_{0}}\). By Theorem 1.3, the inequality in (2.3) is strict whenever \(\theta\neq\theta_{0}\). \(\Box\)

#### Exponential families

A parametric family of special interest is the exponential family, which was introduced by Darmois (1935); Pitman (1936); Koopman (1936). For more information see also Barndorff-Nielsen (1978); McCullagh and Nelder (1989), and Lehmann and Casella (1998).

Let \((\Omega,\mathcal{F})\) be a measurable space, and \((\Omega_{X},\mathcal{F}_{X},\mu)\) a \(\sigma\)-finite measure space with \(\Omega_{X}\subseteq\mathbb{R}^{m}\) and \(\mathcal{F}_{X}\subseteq\mathcal{R}^{m}\). Let \(X:\Omega\to\Omega_{X}\) be a random vector. Let \(t:\Omega_{X}\to\mathbb{R}^{p}\) be a function measurable with respect to \(\mathcal{F}_{X}/\mathcal{R}^{p}\), such that \(\int_{\Omega_{X}}e^{\theta^{T}t(x)}d\mu(x)<\infty\) for some \(\theta\in\mathbb{R}^{p}\). We say that a measurable function \(t:\Omega_{X}\to\mathbb{R}^{p}\) is of full dimension with respect to a measure \(\mu\) on \(\mathcal{F}_{X}\) if for all \(a\in\mathbb{R}^{p}\), \(a\neq 0\), and all \(c\in\mathbb{R}\), we have

\[\mu(\{x:a^{T}t(x)\neq c\})>0.\]

In other words, \(t\) has full dimension with respect to \(\mu\) if the range of \(t\) does not stay within a proper affine subspace of \(\mathbb{R}^{p}\) almost everywhere \(\mu\).

**Definition 2.4**: _Suppose \((\Omega_{X},\mathcal{F}_{X},\mu)\) is a \(\sigma\)-finite measure space and \(t:\Omega_{X}\to\mathbb{R}^{p}\) is a function of full dimension with respect to \(\mu\). Let_

\[\Theta=\left\{\theta\in\mathbb{R}^{p}:\int_{\Omega_{X}}e^{\theta^{T}t(x)}d\mu (x)<\infty\right\}.\]

_For each \(\theta\in\Theta\), let \(P_{\theta}\) be the probability measure on \(\mathcal{F}_{X}\) defined by_

\[P_{\theta}(B)=\int_{B}\left(\int_{\Omega_{X}}e^{\theta^{T}t(x)}d\mu(x)\right) ^{-1}e^{\theta^{T}t(x)}d\mu(x),\ \ B\in\mathcal{F}_{X}. \tag{2.4}\]

_The family of measures \(\{P_{\theta}:\theta\in\Theta\}\) is called an exponential family._

Since an exponential family is determined by \(\mu,t\), we denote it by \(\mathfrak{E}_{p}(\mu,t)\), where the subscript \(p\) indicates the dimension of \(\theta\). Several properties follow immediately.

**Theorem 2.2**: _An exponential family is identifiable and homogenous._Proof: Let \(P_{\theta_{1}}\) and \(P_{\theta_{2}}\) be two members of \(\mathfrak{E}_{p}(t,\mu)\), and \(B\in\mathcal{F}_{X}\). If \(P_{\theta_{1}}(B)=0\), then

\[\int_{B}e^{\theta_{1}^{T}t(x)}d\mu(x)=0.\]

Since \(e^{\theta_{1}^{T}t(x)}>0\), we have \(\mu(B)=0\). Since, by (2.4), \(P_{\theta_{2}}\ll\mu\), we have \(P_{\theta_{2}}(B)=0\). Hence \(P_{\theta_{1}}\ll P_{\theta_{2}}\). By the same argument \(P_{\theta_{2}}\ll P_{\theta_{1}}\). So \(\mathfrak{E}_{p}(t,\mu)\) is homogenous.

If \(P_{\theta_{1}}=P_{\theta_{2}}\), then \(P_{\theta_{1}}(B)=P_{\theta_{2}}(B)\) for all \(B\in\mathcal{F}_{X}\). Then

\[\int_{B}\biggl{(}\int_{\Omega_{X}}e^{\theta_{1}^{T}t(x)}d\mu(x) \biggr{)}^{-1}e^{\theta_{1}^{T}t(x)}d\mu(x)\!=\!\int_{B}\biggl{(}\int_{\Omega _{X}}\!e^{\theta_{2}^{T}t(x)}d\mu(x)\biggr{)}^{-1}e^{\theta_{2}^{T}t(x)}d\mu( x).\]

Let \(b(\theta)=\log\int_{\Omega_{X}}e^{\theta^{T}t(x)}d\mu(x)\). The above equation implies

\[e^{\theta_{1}^{T}t(x)-b(\theta_{1})}=e^{\theta_{2}^{T}t(x)-b(\theta_{2})}\ \ [\mu].\]

This implies \((\theta_{1}-\theta_{2})^{T}t(x)=b(\theta_{1})-b(\theta_{2})\ [\mu]\). But since \(t\) has full dimension with respect to \(\mu\), we have \(\theta_{1}=\theta_{2}\). \(\Box\)

**Theorem 2.3**: _The parameter space \(\Theta\) for an exponential family \(\mathfrak{E}_{p}(t,\mu)\) is convex._

Proof: Let \(\lambda\in(0,1)\). Then \(((1-\lambda)^{-1},\lambda^{-1})\) is a conjugate pair. Let \(\theta_{1},\theta_{2}\in\Theta\). Let \(f=e^{(1-\lambda)\theta_{1}^{T}t}\) and \(g=e^{\lambda\theta_{2}^{T}t}\). By Holder's inequality,

\[\int_{\Omega_{X}}\!e^{\{(1-\lambda)\theta_{1}+\lambda\theta_{2} \}^{T}t(x)}d\mu(x)= \int_{\Omega_{X}}fgd\mu\] \[\leq \biggl{(}\int_{\Omega_{X}}f^{(1-\lambda)^{-1}}d\mu\biggr{)}^{1- \lambda}\left(\int_{\Omega_{X}}g^{\lambda^{-1}}d\mu\right)^{\lambda}\] \[= \biggl{(}\int_{\Omega_{X}}e^{\theta_{1}^{T}t(x)}d\mu(x)\biggr{)}^ {1-\lambda}\left(\int_{\Omega_{X}}e^{\theta_{2}^{T}t(x)}d\mu(x)\right)^{ \lambda}\!\!,\]

where both factors on the right hand side are finite because both \(\theta_{1}\) and \(\theta_{2}\) belong to \(\Theta\). Hence \((1-\lambda)\theta_{1}+\lambda\theta_{2}\in\Theta\). \(\Box\)

A slightly more general definition of an exponential family involves a bijective transformation of the parameter \(\theta\). Suppose the conditions in Definition 2.4 hold. Let \(\Upsilon\) be another subset of \(\mathbb{R}^{p}\), and \(\psi:\Upsilon\rightarrow\Theta\) a bijection. Then, for any \(\theta\in\Upsilon\),

\[\int_{\Omega_{X}}e^{\psi^{\Upsilon}(\theta)t(u)}d\mu(u)<\infty.\]

Let \(P_{\theta}\) be the probability measure on \((\Omega_{X},\mathcal{F}_{X})\) defined by \[dP_{\theta}=c(\theta;\psi,t,\mu)e^{\psi^{T}(\theta)t(x)}d\mu, \tag{2.5}\]

where

\[c(\theta;\psi,t,\mu)=\left(\int_{\Omega_{X}}e^{\psi^{T}(\theta)t(x)}d\mu\right)^{ -1}.\]

Then we call the family \(\{P_{\theta}:\theta\in\Upsilon\}\) the exponential family with respect to \(\psi,t,\mu\), and write this family as

\[\mathfrak{E}_{p}(\psi,t,\mu).\]

Henceforth, whenever the first argument of \(\mathfrak{E}_{p}(\cdot,\cdot,\cdot)\) is missing we always mean the special case defined by Definition 2.4, where \(\psi\) is the identity mapping. We will consistently use \(c(\theta;\psi,t,\mu)\) to represent the proportional constant for \(\mathfrak{E}_{p}(\psi,t,\mu)\). Again, if the argument \(\psi\) is missing, then \(c(\theta;t,\mu)\) means the proportional constant for \(\mathfrak{E}_{p}(t,\mu)\).

### Sufficient, complete, and ancillary statistics

Let \((\Omega_{X},\mathcal{F}_{X})\) and \((\Omega_{T},\mathcal{F}_{T})\) be measurable spaces. Let \(\mathfrak{M}\) be a family of probability measures on \((\Omega_{X},\mathcal{F}_{X})\). Let \(X:\Omega\to\Omega_{X}\) and \(T:\Omega_{X}\to\Omega_{T}\) be mappings measurable with respect to \(\mathcal{F}/\mathcal{F}_{X}\) and \(\mathcal{F}_{X}/\mathcal{F}_{T}\), respectively. Usually, \(\Omega_{X}\) and \(\Omega_{T}\) are Euclidean spaces and \(\mathcal{F}_{X}\) and \(\mathcal{F}_{T}\) are corresponding Borel \(\sigma\)-fields, but we do not need to make such assumptions. The random element \(X\) represents the data. The random element \(T\) represents a statistic. Formally, a statistic is any measurable mapping defined on \(\Omega_{X}\). Implicit in this definition is that \(T\) does not depend on the measure \(P\in\mathfrak{M}\), because otherwise \(T\) would be a mapping from \(\Omega_{X}\times\mathfrak{M}\) to \(\Omega_{T}\). A statistic \(T\) is sufficient with respect to the family \(\mathfrak{M}\) if, for any \(B\in\mathcal{F}_{X}\), the conditional probability \(P(B|T)=P(B|T)\) is the same for all \(P\in\mathfrak{M}\).

**Definition 2.5**: _A statistic \(T=T(X)\) is sufficient for \(\mathfrak{M}\) if, for each \(B\in\mathcal{F}_{X}\), there is a function \(\kappa_{B}:\Omega_{X}\to\mathbb{R}\), measurable \(T^{-1}(\mathcal{F}_{T})/\mathcal{R}\), such that for each \(P\in\mathfrak{M}\),_

\[P(B|T)=\kappa_{B}\ \ a.e.\ P.\]

The point of this definition is, of course, that \(\kappa_{B}\) is the same for all \(P\in\mathfrak{M}\). Note that, we do not require the measure zero set under each \(P\) to be the same for all \(P\). In other words, our sufficiency is not defined in terms of conditional distribution but in terms of conditional probability. A common way to find sufficient statistic is to use Fisher-Neyman factorization theorem, of which the following theorem is a special case.

**Lemma 2.1**: _Let \(P\ll P_{\circ}\) be two probability measures on \((\Omega_{X},\mathcal{F}_{X})\) and \(T:\Omega_{X}\to\Omega_{T}\) be a function measurable \(\mathcal{F}_{X}/\mathcal{F}_{T}\). Then the following assertions are equivalent._1. \(E_{P_{0}}\Big{(}\frac{dP}{dP_{0}}\Big{|}T\Big{)}=\frac{dP}{dP_{0}}\)\([P_{ 0}]\);
2. \(P(B|T)=P_{0}(B|T)\)\([P]\) for any \(B\in{\cal F}_{X}\).

Proof: 1\(\Rightarrow\) 2. Since both \(P(B|T)\) and \(P_{ 0}(B|T)\) are measurable \(T^{-1}({\cal F}_{T})\), it suffices to show that, for any \(G\in T^{-1}({\cal F}_{T})\),

\[\int_{G}P(B|T)dP=\int_{G}P_{ 0}(B|T)dP.\]

We have

\[\int_{G}P(B|T)dP=\int_{G}I_{ B}dP=\int I_{ B}\Big{(}\frac{dP}{dP_{ 0}}\Big{)}dP_{ 0}\]

By 1, the right hand side is

\[\int_{G}I_{ B}E_{P_{0}}\Big{(}\frac{dP}{dP_{ 0}}\Big{|}T\Big{)}SdP_{ 0}=\int_{G}E_{P_{0}}(I_{ B}|T)\frac{dP}{dP_{ 0}}\,dP_{ 0}=\int_{G}P_{ 0}(B|T)dP.\]

2\(\Rightarrow\) 1. It suffices to show that, for any \(B\in{\cal F}_{X}\),

\[\int_{B}\frac{dP}{dP_{ 0}}\,dP_{ 0}=\int_{B}E_{P_{0}}\Big{(}\frac{dP}{dP_{ 0}}\Big{|}T\Big{)}dP_{ 0}.\]

The right-hand side is

\[\int_{B}E_{P_{0}}\Big{(}\frac{dP}{dP_{ 0}}\Big{|}T\Big{)}dP_{ 0} = \int I_{B}E_{P_{0}}\Big{(}\frac{dP}{dP_{ 0}}\Big{|}T\Big{)}dP_{ 0}\] \[= \int E_{P_{0}}(I_{B}|T)\frac{dP}{dP_{ 0}}\,dP_{ 0}\] \[= \int E_{P_{0}}(I_{B}|T)dP\] \[= \int_{B}dP\] \[= \int_{B}\frac{dP}{dP_{ 0}}\,dP_{ 0},\]

as desired. \(\Box\)

**Theorem 2.4**: _Suppose \(\mathfrak{M}\) is a family of probability measures on \((\Omega_{X},{\cal F}_{X})\) that is dominated by a \(\sigma\)-finite measure \(\lambda\). Then the following statements are equivalent:_

1. \(T\) _is sufficient with respect to_ \(\mathfrak{M}\)_;_
2. _There is a probability measure_ \(P_{ 0}\) _such that_ \(\mathfrak{M}\equiv P_{ 0}\)_, and for every_ \(P\in\mathfrak{M}\)_, there is an_ \(h_{ P}:\Omega_{X}\to\mathbb{R}\) _measurable_ \(T^{-1}({\cal F}_{T})/{\cal R}\) _such that_ \(\frac{dP}{dP_{ 0}}=h_{ P}\)__\([P_{ 0}]\)

[MISSING_PAGE_EMPTY:5883]

\(3\Rightarrow 2\). Suppose \(dP/d\lambda=(g_{{}_{P}}{{}^{\circ}T})u\) [\(\lambda\)]. Since \(dP/d\lambda\geq 0\) [\(\lambda\)], we have \((g_{{}_{P}}{{}^{\circ}T})u=(|g_{{}_{P}}{{}^{\circ}T})|u|\). Let \(dP_{{}_{0}}=|u|d\lambda\). Then \(dP=(|g_{{}_{P}}{{}^{\circ}T})dP_{{}_{0}}\), as desired. \(\Box\)

If \(S\) is another statistic such that \(T\) is measurable \(\sigma(S)\) (that is, \(\sigma(T)\subseteq\sigma(S)\)), then we say that \(S\) refines \(T\). It is intuitively clear that if \(T\) is sufficient and \(S\) refines \(T\), then \(S\) is also sufficient, because \(T\) is a more concise summary of the data. This is proved in the next corollary.

**Corollary 2.1**: _Under the conditions in Theorem 2.4, if \(T\) is sufficient for \(\mathfrak{M}\) and \(S\) refines \(T\), then \(S\) is sufficient for \(\mathfrak{M}\)._

Proof: By Theorem 2.4, \(T(X)\) is sufficient for \(\mathfrak{M}\) if and only if \(E_{P_{0}}(dP/dP_{0}|T)=dP/dP_{0}\,\mbox{a.e.}\,P_{0}\). Then

\[E_{P_{0}}(dP/dP_{0}|S) = E[E_{P_{0}}(dP/dP_{0}|T)|S]\] \[= E_{P_{0}}(dP/dP_{0}|T)\] \[= dP/dP_{0},\]

which, by Theorem 2.4 again, is equivalent to \(S(X)\) being sufficient. \(\Box\)

Sufficient statistic means we can use \(T\) as the data without losing any information about \(\theta\). This is a reduction of the original \(X\). Naturally, if \(S\) refines \(T\) and \(T\) is sufficient we prefer \(T\), because the latter is a greater reduction of the original data. It is then natural introduce the concept the minimal sufficient statistic.

**Definition 2.6**: _A statistic \(T:\Omega_{X}\rightarrow\Omega_{T}\) is minimal sufficient for \(\mathfrak{M}\) if_

_1. \(T\) is sufficient for \(\mathfrak{M}\);_

_2. it is refined by any other sufficient statistic for \(\mathfrak{M}\)._

Another useful concept is the complete statistic.

**Definition 2.7**: _Let \(T:\Omega_{X}\rightarrow\Omega_{T}\) be measurable \(\mathcal{F}_{X}/\mathcal{F}_{T}\). We say that \(T\) is complete for \(\mathfrak{M}\) if, for any \(g:\Omega_{T}\rightarrow\mathbb{R}\) measurable \(\mathcal{F}_{T}/\mathcal{R}\) such that \(g{{}^{\circ}T}\) is \(P\)-integrable, we have_

\[\int g{{}^{\circ}T}dP=0\ \ \mbox{for all}\ P\in\mathfrak{M}\Rightarrow g{{}^{ \circ}T}=0\ [P]\ \ \mbox{for all}\ P\in\mathfrak{M}.\]

_The statistic \(T\) is bounded complete for \(\mathfrak{M}\) if the above implication holds for all \(\mathcal{F}_{T}\)-measurable bounded function \(g\)._

Note that bounded completeness is a weaker assumption than completeness. These concepts are important in several ways. As we will see shortly, under mild conditions, if a sufficient statistic is complete, then it is a minimal sufficient statistic. Also, in Chapter 3, we will see that the notion of (bounded) completeness is helpful for constructing uniformly most powerful unbiased tests in the presence of nuisance parameters.

Suppose that \(\mathfrak{M}\) is a dominated family of probability measures on \((\Omega_{X},\mathcal{F}_{X})\). Then there is a probability measure \(P_{\circ}\) such that \(P_{\circ}\equiv\mathfrak{M}\). In the following, we say that a random vector belongs to \(L_{2}(P_{\circ})\) if each of its component belongs to \(L_{2}(P_{\circ})\).

**Theorem 2.5**: _Suppose there exists a minimal sufficient statistic for \(\mathfrak{M}\) in \(L_{2}(P_{\circ})\). Then any complete and sufficient statistic for \(\mathfrak{M}\) in \(L_{2}(P_{\circ})\) is minimal sufficient for \(\mathfrak{M}\)._

Note that the statistics in the above theorem are allowed to be vectors. In the following proof, for a random variable \(W,\ E_{P}(W)\) denotes the expectation of \(W\) under \(P\); that is \(E_{P}(W)=\int WdP\).

Proof: Let \(T\in L_{2}(P_{\circ})\) be a minimal sufficient statistic for \(\mathfrak{M}\), and \(U\in L_{2}(P_{\circ})\) a complete sufficient statistic for \(\mathfrak{M}\). It suffices to show that \(U\) is measurable with respect to \(T\), for which it suffices to show \(U=E_{P_{0}}(U|T)\). We note that

\[E_{P}(U-E_{P}(E_{P}(U|T)|U))=E_{P}(U)-E_{P}(U)=0\]

for all \(P\in\mathfrak{M}\). Because both \(U\) and \(T\) are sufficient, the conditional expectations given \(U\) or \(T\) are the same for all \(P\). In particular,

\[E_{P}(E_{P}(U|T)|U)=E_{P_{0}}(E_{P_{0}}(U|T)|U).\]

Hence

\[E_{P}(U-E_{P_{0}}(E_{P_{0}}(U|T)|U))=0\]

for all \(P\in\mathfrak{M}\). By the completeness of \(U\), we have \(U=E_{P_{0}}(E_{P_{0}}(U|T)|U)\). However, because \(T\) is minimal sufficient and \(U\) is sufficient, \(T\) is measurable with respect to \(\sigma(U)\). Hence \(E_{P_{0}}(E_{P_{0}}(U|T)|U)=E_{P_{0}}(U|T)\). \(\Box\)

A statistic \(U:\Omega_{ X}\rightarrow\Omega_{U}\) is said to be an _ancillary_ statistic with respect to a family \(\mathfrak{M}\) of probability distributions over \((\Omega_{ X},\mathcal{F}_{ X})\) if \(P\circ U^{-1}\) is the same for all \(P\in\mathfrak{M}\). Basu (1955) proved the following well known result.

**Theorem 2.6** (Basu's Theorem): _Suppose \(T:\Omega_{X}\rightarrow\Omega_{T}\) is complete and sufficient for \(\mathfrak{M}\) and \(U:\Omega_{X}\rightarrow\Omega_{U}\) is ancillary for \(\mathfrak{M}\). Then \(T\) and \(U\) are independent under each \(P\in\mathfrak{M}\)._

Proof: We shall show that for any \(B\in\mathcal{F}_{U}\) we have

\[P(U\in B|T)-P(U\in B)=0\ \ [P].\]

Since \(T\) sufficient, the first term does not depend on \(P\), and because \(U\) is ancillary, neither does the second term. Moreover, we have \[\int[P(U\in B|T)-P(U\in B)]dP=0\]

for all \(P\in\mathfrak{M}\). Since the integrand is a function of \(T\) independent of \(P\), by the completeness of \(T\) we have

\[P(U\in B|T)=P(U\in B)\ \ [P]\]

This means \(U\) and \(T\) are independent. \(\Box\)

### Complete sufficient statistics for exponential family

In this section we derive the complete and sufficient statistic for an exponential family. For this purpose it is useful to review the basic properties of analytic functions.

**Definition 2.8**: _A function \(f:\mathbb{R}\rightarrow\mathbb{R}\) is real analytic on an open set \(G\subseteq\mathbb{R}\) if for any \(x\in G\), there is an open neighborhood \(N\) of \(x\) such that, for all \(x^{\prime}\in N\),_

\[f(x^{\prime})=\sum_{n=0}^{\infty}a_{n}(x^{\prime}-x)^{n},\]

_in which the coefficients \(a_{0},a_{1},...\) are real numbers._

In other words, an analytic function can be expanded as a power series locally at each point in the region in which it is analytic. Thus, equivalently, a real analytic function can be defined as a function that is infinitely differentiable and that can be expanded as a Taylor series locally at any point in \(G\). The above definition can be generalized to functions with several variables, say \(x=(x_{1},\ldots,x_{p})\). In that case the power series is replaced by

\[f(x^{\prime})=\sum a_{i_{1}\ldots i_{p}}(x^{\prime}_{1}-x_{1})^{i_{1}}\cdots(x ^{\prime}_{p}-x_{p})^{i_{p}},\]

where the summation is over the index set

\[\{(i_{1},\ldots,i_{p}):\ i_{1},\ldots,i_{p}=0,1,2,\ldots\}.\]

A complex analytic function is defined in the same way by replacing \(\mathbb{R}\) by \(\mathbb{C}\), the set of all complex numbers, and \(G\) by an open set in \(\mathbb{C}\). It is true that any complex function from \(\mathbb{C}\) to \(\mathbb{C}\) that is differentiable is analytic. An analytic function is global, in the sense that the overall property of the function can be determined by its local property near a point. In this sense it behaves rather like a polynomial. We know that a \(k\)th order polynomial cannot have more than \(k\) solutions unless all the coefficients of the polynomial are \(0\). Similarly, if the collection of roots of an analytic function contains a limit point, then it is identically \(0\). This fact will be used in several places in later discussions. See, for example, Rudin (1987, page 209).

**Theorem 2.7**: _If \(f\) is a real analytic function on an open set \(G\) in \(\mathbb{R}^{p}\) and if \(\{x:f(x)=0\}\) has a limit point in \(G\), then \(f(x)=0\) on \(G\)._

The following is an important property of an exponential family of distributions. It implies that if \(g(x)\) integrable then \(E_{\theta}g(X)\) is an analytic function of \(\theta\) provided that \(f_{\theta}\) is an exponential family.

**Lemma 2.2**: _Let \((\Omega_{X},\mathcal{F}_{X},\mu)\) be a measure space, where \(\Omega_{X}\subseteq\mathbb{R}^{p}\), and \(\Theta\) is a subset of \(\mathbb{R}^{p}\) with nonempty interior. Suppose \(g(x)e^{\theta^{T}x}\) is integrable with respect to \(\mu\) for each \(\theta\in\Theta\), then the integral \(\int g(x)e^{\theta^{T}x}\mu(dx)\) is analytic function of \(\theta\) in the interior of \(\Theta\). Furthermore, the derivatives of all orders with respect to \(\theta\) can be taken inside the integral._

For a proof, see, for example, Lehmann and Romano (2005, page 49).

**Theorem 2.8**: _Suppose \(\Theta\subseteq\mathbb{R}^{p}\) has a nonempty interior (-- that is, it has a nonempty open subset). Then the statistic \(t(X)\) is sufficient and complete for the exponential family \(\mathfrak{E}_{p}(\lambda,t)\)._

Since

\[\frac{dP_{\theta}}{d\lambda}=e^{\theta^{T}t(x)}\left(\int e^{\theta^{T}t(x)}d \lambda(x)\right)^{-1}\]

is measurable with respect to \(t^{-1}(\mathcal{R}^{p})\), by Theorem 2.4, \(t(X)\) is sufficient with respect to \(\mathfrak{E}_{p}(\lambda,t)\). Let \(g\) be an integrable function of \(t\) such that

\[E_{\theta}g(t(X))=0\ \ \mbox{for all}\ \ \theta\in\Theta.\]

Then, by the change of variable theorem,

\[\int g(t)e^{\theta^{T}t}d\nu(t)=0\ \ \mbox{for all}\ \ \theta\in\Theta,\]

where \(\nu=\lambda\circ t^{-1}\). Because this integral is an analytic function of \(\theta\), and because \(\Theta\) contains an open set (and hence limit point), it is \(0\) over \(\mathbb{R}^{k}\). By the uniqueness of Laplace transformation, \(g(t)=0\)\([\nu]\). By the change of variable theorem, \(g(t(x))=0\)\([\lambda]\). Because \(P_{\theta}\ll\lambda\), \(g(X)=0\)\([P_{\theta}]\) for all \(\theta\in\Theta\). \(\Box\)

### Unbiased estimator and Cramer-Rao lower bound

Let \(\mathfrak{M}=\{P_{\theta}:\theta\in\Theta\}\), where \(\Theta\subseteq\mathbb{R}^{p}\), be a parametric family of probability measures on \((\Omega_{X},\mathcal{F}_{X})\). In this section, we will always assume that \(\mathfrak{M}\) is dominated by a \(\sigma\)-finite measure \(\lambda\), and that \(\mathfrak{M}\) is identifiable. For a random vector \(U=(U_{1},\ldots,U_{k})\) defined on \(\Omega_{X}\), we use \(E(U)\) to denotethe vector \((EU_{1},\ldots,EU_{2})\). For two random vectors \(U=(U_{1},\ldots,U_{k})\) and \(V=(V_{1},\ldots,V_{\ell})\) defined on \((\Omega_{X},\mathcal{F}_{X})\), we use \(\mathrm{cov}(U,V)\) to denote the matrix whose \((i,j)\)th entry is \(\mathrm{cov}(U_{i},V_{j})\). Moreover, we define \(\mathrm{var}(U)\) as \(\mathrm{cov}(U,U)\).

A few more words about notation for expectation. In most of this book, there is no need to make a distinction between the underlying probability space \((\Omega,\mathcal{F})\) and the induced probability space \((\Omega_{X},\mathcal{F}_{X})\). That is, we will simply equate these two probability spaces. In this case, a random variable \(X\) is just the identity mapping:

\[X:\Omega\rightarrow\Omega_{X},\ \ x\mapsto x.\]

So \(E(X)\) means the integral \(\int XdP=\int X(x)dP(dx)=\int xdP(dx)\). To be consistent with the previous notation, it is helpful to still regard \(P\) as a probability measure defined on \((\Omega,\mathcal{F})\).

We now introduce the unbiased estimator of a parameter. In the following, \(X\) is a random vector representing the data. Typically, \(X\) is a sample \(X_{1},\ldots,X_{n}\), where each \(X_{i}\) is a \(k\)-dimensional random vector. In this case \(X\) is an \(nk\)-dimensional random vector, \(\Omega_{X}\) is \(\mathbb{R}^{nk}\) and \(\mathcal{F}_{X}\) is \(\mathcal{R}^{nk}\). Intuitively, if we want to use a random vector \(u(X)\) to estimate a parameter \(\theta\), then we would like the distribution of \(u(X)\) to be centered at the target we want to estimate. This is formulated mathematically as the next definition. In the following we will use \(E_{\theta}\) and \(\mathrm{var}_{\theta}\) to denote the mean (vector) and variance (matrix) of a random variable (vector) under \(P_{\theta}\).

**Definition 2.9**: _Let \(u:\Omega_{X}\rightarrow\mathbb{R}^{p}\) be a function measurable \(\mathcal{F}_{X}/\mathcal{R}^{p}\). We say that \(u(X)\) is an unbiased estimator of \(\theta\) if, for all \(\theta\in\Theta\),_

\[E_{\theta}[u(X)]=\theta.\]

For two symmetric matrices \(A\) and \(B\), we write \(A\succeq B\) if \(A-B\) is positive semidefinite; we write \(A\succ B\) if \(A-B\) is positive definite. The partial ordering represented by \(\succeq\) or \(\succ\) among matrices is sometimes called positive semidefinite ordering, positive definite ordering, or Louwner's ordering. The following proposition is well known. See Horn and Johnson (1985, page 465).

**Proposition 2.2**: _If \(A\) and \(B\) are positive definite, then_

\[A\succeq B\Leftrightarrow B^{-1}\succeq A^{-1},\ \ A\succ B\Leftrightarrow B^{ -1}\succ A^{-1}.\]

The next lemma will be called the multivariate Cauchy-Schwarz inequality. It is the basis of the famous Cramer-Rao inequality. We say that a function \(f\) is square integrable with respect to a measure \(\mu\) if \(f\) is measurable and \(f^{2}\) is integrable \(\mu\).

**Lemma 2.3** (Multivariate Cauchy-Schwarz inequality): _Let \((\Omega,\mathcal{F},\mu)\) be a measure space, and \(f:\Omega\rightarrow\mathbb{R}^{p}\) and \(g:\Omega\rightarrow\mathbb{R}^{p}\) be \(\mathbb{R}^{p}\)-valued functions whose components are square-integrable with respect to \(\mu\). Suppose that the matrix \(\int gg^{T}d\mu\) is nonsingular. Then_\[\left(\int fg^{T}d\mu\right)\left(\int gg^{T}d\mu\right)^{-1}\left(\int gf^{T}d\mu \right)\leq\int ff^{T}d\mu. \tag{2.6}\]

_The equality in (2.6) holds if and only if there is a constant matrix \(A\in\mathbb{R}^{p\times p}\) such that \(f=Ag\ \ [\mu]\). Moreover, if \(\int ff^{T}d\mu\) is nonsingular then so is \(A\)._

Proof.: Let

\[\delta=f-\left(\int fg^{T}d\mu\right)\left(\int gg^{T}d\mu\right)^{-1}g.\]

Then

\[\int\delta\delta^{T}d\mu=\int ff^{T}d\mu-\left(\int fg^{T}d\mu\right)\left(\int gg ^{T}d\mu\right)^{-1}\left(\int gf^{T}d\mu\right).\]

This implies the inequality (2.6) because the left-hand side is positive semidefinite. The equality in (2.6) holds if and only if \(\int\delta\delta^{T}d\mu=0\), which happens if and only if \(\delta=0\ \ [\mu]\). That is,

\[f=\left(\int fg^{T}d\mu\right)\left(\int gg^{T}d\mu\right)^{-1}g\equiv Ag\ \ [\mu].\]

Then \(\int ff^{T}d\mu=A\int gg^{T}d\mu A^{T}\). Since \(\int ff^{T}d\mu\) is nonsingular \(A\) must also be nonsingular. 

Let \(f_{\theta}=dP_{\theta}/d\lambda\). If \(\log f_{\theta}(x)\) is differentiable with respect to \(\theta\), then the partial derivative \(\partial\log f_{\theta}(x)/\partial\theta\) is called the score function (as a function of \(\theta\) and \(x\)). We denote the score function by \(s_{\theta}(x)\). Let \(I(\theta)=\text{var}_{\theta}(s_{\theta}(X))\). This matrix is called the Fisher information. Let \(I_{p}\) be the \(p\times p\) identity matrix.

In the rest of the book we will frequently assume that \(f_{\theta}(x)\) has a common support, say \(A\), for all \(\theta\in\Theta\) and that, for some function \(g(\theta,x)\), \(g(\theta,x)f_{\theta}(x)\) satisfies \(\text{DUI}(\theta,A,\mu)\). For convenience, this lengthy statement is abbreviated in the definition below.

**Definition 2.10**: _We say that \(g(\theta,x)f_{\theta}(x)\) satisfies \(\text{DUI}^{+}(\theta,\mu)\), if the support \(A\) of \(f_{\theta}(x)\) is independent of \(\theta\) and \(g(\theta,x)f_{\theta}(x)\) satisfies \(\text{DUI}(\theta,A,\mu)\)._

We usually regard the Fisher information \(I(\theta)\) as appropriately defined only when both \(f_{\theta}(x)\) and \(s_{\theta}(x)f_{\theta}(x)\) satisfy \(\text{DUI}^{+}(\theta,\mu)\). In this case, it is easy to verify (by passing first two derivatives of \(\theta\) through the integral with respect to \(\mu\))

\[\begin{split} E_{\theta}[s_{\theta}(X)]&=0\\ \text{var}_{\theta}[s_{\theta}(x)]&=-E[\partial s_{ \theta}(X)/\partial\theta^{T}].\end{split} \tag{2.7}\]

More specifically, the first identity requires \(f_{\theta}(x)\) to satisfy \(\text{DUI}^{+}(\theta,\mu)\); the second requires both \(f_{\theta}(x)\) and \(s_{\theta}(x)f_{\theta}(x)\) to satisfy \(\text{DUI}^{+}(\theta,\mu)\). These identities are called the information identities. We will have a closer look at them in Chapter 8, where they play a critical role.

The next two theorems, however, only require the first equality in (2.7) to hold. So in this chapter, we define the Fisher information \(I(\theta)\) as \(\mathrm{var}_{\theta}[s_{\theta}(X)]\) under the first equality in (2.7) without regard of the second equality in (2.7).

We now prove the Cramer-Rao's inequality, which asserts that, under some conditions, no unbiased estimator can have variance smaller than \(I^{-1}(\theta)\). This is but a special case of a very general phenomenon. Later on we will see that \(I^{-1}(\theta)\) is in fact the lower bound of the asymptotic variances of all regular estimates, which cover a very wide range of estimates used in statistics.

**Theorem 2.9**: _Let \(\mathfrak{M}=\{P_{\theta}:\theta\in\Theta\}\) be a dominated identifiable parametric family on a measurable space \((\Omega_{X},\mathcal{F}_{X})\). Let \(f_{\theta}=dP_{\theta}/d\lambda\), where \(\lambda\) is the \(\sigma\)-finite dominating measure. Suppose_

_1. \(U=u(X)\) is an unbiased estimator of \(\theta\) such that all entries of the matrix \(\mathrm{var}_{\theta}(U)\) are finite for each \(\theta\in\Theta\);_

_2. \(f_{\theta}(x)\) and \(u(x)f_{\theta}(x)\) satisfy \(\mathrm{DUI}^{+}(\theta,\lambda)\);_

_3. \(I(\theta)\) is nonsingular for each \(\theta\in\Theta\)._

_Then, for all \(\theta\in\Theta\),_

\[\mathrm{var}_{\theta}(U)\succeq I^{-1}(\theta). \tag{2.8}\]

_Moreover, the equality holds if and only if \(u(X)=\theta+I^{-1}(\theta)s_{\theta}(X)\)._

Proof: Since \(f_{\theta}\) satisfies \(\mathrm{DUI}^{+}(\theta,\lambda)\), we have

\[0=\partial(1)/\partial\theta=\int\partial f_{\theta}(x)/\partial\theta d \lambda(x)=E_{\theta}[s_{\theta}(X)]. \tag{2.9}\]

This implies \(\mathrm{var}_{\theta}[s_{\theta}(X)]=E_{\theta}[s_{\theta}(X)s_{\theta}^{T}(X)] =I(\theta)\). Let \(\delta_{\theta}(X)=u(X)-\theta\). By Lemma 2.3,

\[\mathrm{var}_{\theta}[u(X)]\succeq E_{\theta}[\delta_{\theta}(X)s_{\theta}^{T}( X)][E_{\theta}(s_{\theta}(X)s_{\theta}(X))]^{-1}E_{\theta}[s_{\theta}(X)\delta_{ \theta}^{T}(X)]. \tag{2.10}\]

By (2.9),

\[E_{\theta}[s_{\theta}(X)\delta_{\theta}^{T}(X)]=E_{\theta}[s_{\theta}(X)u^{T}( X)].\]

Moreover, because \(u(x)f_{\theta}(x)\) satisfies \(\mathrm{DUI}^{+}(\theta,\mu)\), we have

\[E_{\theta}[s_{\theta}(X)u^{T}(X)]=\int u(x)(\partial f_{\theta}(x)/\partial \theta^{T})d\mu(x)=\partial\theta/\partial\theta^{T}=I_{p}.\]

Therefore the right hand side of (2.10) is \(I^{-1}(\theta)\). This proves the inequality (2.8).

By the equality part of Lemma 2.3, \(\mathrm{var}_{\theta}[u(X)]=I^{-1}(\theta)\) if and only if

\[\delta_{\theta}(X)=A_{\theta}s_{\theta}(X)\]for some matrix \(A_{\theta}\). It follows that

\[E_{\theta}[\delta_{\theta}(X)\delta_{\theta}^{T}(X)]=A_{\theta}E_{\theta}[s_{ \theta}(X)\delta_{\theta}^{T}(X)]=A_{\theta}.\]

Hence \(A_{\theta}=I^{-1}(\theta)\). 

We now discuss two problems related to this inequality. The first is about reaching the lower bound. The second is about the practical situations where the \(\text{DUI}^{+}(\theta,\lambda)\) condition is violated and the lower bound is exceeded. The first problem is often referred to as the attainment problem in the literature. See, for example, Fend (1959). The next theorem gives a solution to this problem.

**Theorem 2.10**: _Suppose \((\Omega_{X},\mathcal{F}_{X})\) is a measurable space and \(\mathfrak{M}=\{P_{\theta}:\theta\in\Theta\}\) is an identifiable parametric family dominated by a \(\sigma\)-finite measure \(\lambda\). Let \(f_{\theta}=dP_{\theta}/d\lambda\). Further, suppose that_

1. \(f_{\theta}(x)\) _satisfies_ \(\text{DUI}^{+}(\theta,\mu)\)_;_
2. \(I(\theta)\) _is positive definite and its entries are finite._

_Then the following statements are equivalent:_

1. _there is an unbiased estimator_ \(u(X)\) _of_ \(\theta\) _such that_ \(\text{\rm var}_{\theta}[u(X)]=I^{-1}(\theta)\)_;_
2. \(X\) _has the exponential-family distribution of the form_

\[f_{\theta}(x)=e^{\psi^{T}(\theta)u(x)}\left(\int e^{\psi^{T}(\theta)u(x)}d\nu( x)\right)^{-1} \tag{2.11}\]

_for some measure_ \(\nu\) _on_ \(\Omega_{X}\) _dominated by_ \(\lambda\)_, and some function_ \(\psi(\theta)\) _differentiable with respect to_ \(\theta\) _satisfying_

\[\frac{\partial\psi(\theta)}{\partial\theta^{T}}=\frac{\partial\psi^{T}(\theta) }{\partial\theta}=I(\theta).\]

Proof: \(1\Rightarrow 2\). By Lemma 2.3, the equality in (2.8) holds if and only if \(s_{\theta}(x)=A_{\theta}(u(x)-\theta)\) for some nonsingular matrix \(A_{\theta}\). This means

\[\partial\log f_{\theta}(x)/\partial\theta=A_{\theta}u(x)-A_{\theta}\theta\]

Hence there exist \(\psi:\Theta\rightarrow\mathbb{R}^{p}\) and \(\phi:\Theta\rightarrow\mathbb{R}\) such that

\[\partial\psi^{T}/\partial\theta=A_{\theta},\ \ \partial\phi/\partial\theta=A_{ \theta}\theta,\ \ \log f_{\theta}(x)=\psi^{T}(\theta)u(x)+\phi(\theta)+v(x).\]

Thus

\[f_{\theta}(x)=e^{\psi^{T}(\theta)u(x)}c_{\theta}w(x).\]

So the density of \(X\) has the form (2.11) with \(d\nu=wd\lambda\).

\(2\Rightarrow 1\). Since \(\psi\rightarrow\int u(x)e^{\psi^{T}u(x)}d\nu(x)\) and \(\psi\rightarrow\int e^{\psi^{T}u(x)}d\nu(x)\) are analytic, \(u(x)f_{\theta}(x)\) satisfies \(\text{DUI}^{+}(\theta,\nu)\). Applying the fact that \(u(X)\) is unbiased we find \[E_{\theta}(s_{\theta}(X)u^{T}(X))=E_{\theta}(s_{\theta}(X)(u(X)-\theta)^{T})=I_{p}.\]

We also know that

\[s_{\theta}(X)=\frac{\partial\psi^{T}(\theta)}{\partial\theta}u(X)-\frac{ \partial}{\partial\theta}\log\int e^{\psi^{T}(\theta)u(x)}d\nu(x)=A_{\theta}u( X)+b_{\theta}.\]

Because \(f_{\theta}\) satisfies \(\mbox{DUI}^{+}(\theta,\nu)\), we have \(E_{\theta}s_{\theta}(X)=0\). Hence \(b_{\theta}=-A_{\theta}\theta\); that is,

\[s_{\theta}(X)=A_{\theta}(u(X)-\theta).\]

It follows that

\[E_{\theta}(s_{\theta}(X)(u(X)-\theta)^{T})=A_{\theta}\mbox{var}_{\theta}(u(X)) =A_{\theta}I^{-1}(\theta).\]

Hence \(A_{\theta}=I(\theta)\). But this implies

\[\mbox{var}_{\theta}[u(X)]=I^{-1}(\theta)\mbox{var}_{\theta}[s_{\theta}(X)]I^{- 1}(\theta)=I^{-1}(\theta),\]

as desired. \(\Box\)

This theorem shows that the Cramer-Rao lower bound is achieved by an unbiased estimator \(u(X)\) if and only if \(X\) has an exponential family distribution with \(u(X)\) as its sufficient statistic. For any other distribution that satisfies the relevant \(\mbox{DUI}^{+}(\theta,\lambda\) assumption, this bound cannot be reached exactly.

When the \(\mbox{DUI}^{+}(\theta,\lambda)\) assumption is not satisfied, an unbiased estimator can have smaller variance than \(I^{-1}(\theta)\), as the following example shows.

**Example 2.2** Consider the case where \(P_{\theta}\) is the uniform \(U(0,\theta)\), \(\theta>0\). That is,

\[f_{\theta}(x)=\theta^{-1}I_{(0,\theta)}(x),\ \ \theta>0.\]

This family does not satisfy \(\mbox{DUI}^{+}(\theta,\mu)\) because the support of \(f_{\theta}(x)\) depends on \(\theta\). Since

\[\partial(\theta^{-1}I_{(0,\theta)(x)})/\partial\theta=-\theta^{-2}I_{(0,\theta )(x)}\]

almost everywhere with respect to the Lebesgue measure, we have

\[\int\partial f_{\theta}(x)/\partial\theta\,dx=-\theta^{-1}.\]

On the other hand \(\partial[\int f_{\theta}(x)\,dx]/\partial\theta=0\). So

\[\frac{\partial}{\partial\theta}\int f_{\theta}(x)dx\neq\int\frac{\partial f_{ \theta}(x)}{\partial\theta}dx.\]

The score function for this density is \[s_{\theta}(x)=\partial\log f_{\theta}(x)/\partial\theta=-\partial\log(\theta)/ \partial\theta+\partial\log I_{(0,\theta)}(x)/\partial\theta=-1/\theta,\]

which is defined almost everywhere with respect to the Lebesgue measure. Hence \(\mathrm{var}_{\theta}[s_{\theta}(X)]=0\). So the inequality

\[\mathrm{var}_{\theta}[u(X)]\geq 1/\mathrm{var}_{\theta}[s_{\theta}(X)]\]

does not hold for any unbiased estimator of \(\theta\) with a finite variance.

In the above we have considered a single observation from \(U(0,\theta)\) for convenience, but the conclusion for the situation where we have an i.i.d. sample from \(U(0,\theta)\) is essentially the same. \(\Box\)

### Conditioning on complete and sufficient statistics

Unbiasedness is but one way of assessing the quality of an estimator -- that a good estimator should be centered around the target it intends to estimate. Another way to assess the quality of an estimator is by its variance: a good estimator should be more closely clustered around the target. In this section we study how to find estimator with small variance. We first introduced a lemma, often called the EV-VE formula.

**Lemma 2.4**: _Suppose that the components of \(U\) belong to \(L_{2}(P)\). Then_

\[\mathrm{var}(U)=E[\mathrm{var}(U|\mathcal{G})]+\mathrm{var}[E(U|\mathcal{G})].\]

Proof: We have

\[\mathrm{var}(U)= E(U-EU)(U-EU)^{T}\] \[= E[(U-E(U|\mathcal{G})+E(U|\mathcal{G})-EU)(U-E(U|\mathcal{G})+E( U|\mathcal{G})-EU)^{T}]\]

Note that

\[E[(U-E(U|\mathcal{G}))(E(U|\mathcal{G})-EU)^{T}]= E[(U-E(U|\mathcal{G}))E(U^{T}-EU^{T}|\mathcal{G})]\] \[= E[E(U-E(U|\mathcal{G})|\mathcal{G})(U-EU)^{T}]=0.\]

Hence

\[\mathrm{var}(U)= E(U-EU)(U-EU)^{T}\] \[= E[(U-E(U|\mathcal{G}))(U-E(U|\mathcal{G}))^{T}]\] \[\qquad\qquad+E[(E(U|\mathcal{G})-EU)(E(U|\mathcal{G})-EU)^{T}]\] \[= \mathrm{var}(E(U|\mathcal{G}))+E(\mathrm{var}(U|\mathcal{G}))\]

as desired.

Let \((\Omega_{X},\mathcal{F}_{X})\) be a measurable space and \(\mathfrak{M}=\{P_{\theta}:\theta\in\Theta\}\) be a dominated and identifiable parametric family of probability distributions on \((\Omega_{X},\mathcal{F}_{X})\). Let \(\mathfrak{U}\) be the class of all unbiased estimators \(U=u(X)\) of \(\theta\) defined on \(\Omega_{X}\) such that \(\mathrm{var}_{\theta}(U)<\infty\) for each \(\theta\in\Theta\). The following theorem shows that, given an unbiased estimator \(U\in\mathfrak{U}\) and a sufficient statistic \(T\) for \(\mathfrak{M}\), one can reduce the variance of \(U\) by taking the conditional expectation on \(T\). This is called Rao-Blackwell's inequality. See Rao (1945) and Blackwell (1947).

**Theorem 2.11**: _Suppose \(U\in\mathfrak{U}\) and \(T\) is a sufficient statistic for \(\mathfrak{M}\). Then \(E(U|T)\in\mathfrak{U}\) and_

\[\mathrm{var}_{\theta}(U)\succeq\mathrm{var}_{\theta}[E(U|T)].\]

Proof: We have

\[\mathrm{var}_{\theta}(U)=\mathrm{var}_{\theta}[E(U|T)]+E_{\theta}[\mathrm{var} (U|T)]\succeq\mathrm{var}_{\theta}[E(U|T)].\]

But \(E(U|T)\) is an unbiased estimator of \(\theta\). \(\Box\)

If, in addition to being sufficient, \(T\) is also complete for \(\mathfrak{M}\). Then taking the conditional expectation of an unbiased estimator given \(T\) actually brings the variance to the minimum. This is called the Lehmann-Scheffe theorem (Lehmann and Scheffe, 1950, 1955).

**Theorem 2.12**: _Suppose \(T=t(X)\) is complete and sufficient statistic for \(\mathfrak{M}\), and \(U\) is a statistic in \(\mathfrak{U}\) that is measurable \(\sigma(t(X))\). Then for any \(U^{\prime}\in\mathfrak{U}\), we have \(\mathrm{var}_{\theta}(U)\preceq\mathrm{var}_{\theta}(U^{\prime})\) for each \(\theta\in\Theta\)._

Proof: Since \(U^{\prime}\in\mathfrak{U}\), we have \(E_{\theta}[E(U^{\prime}|T)]=\theta\) for all \(\theta\). Moreover, \(U=E(U|T)\) and \(E_{\theta}[E(U|T)]=\theta\). Hence

\[E_{\theta}[E(U^{\prime}|T)-E(U|T)]=0\ [P_{\theta}]\ \mathrm{for\ all}\ \theta\in\Theta.\]

Since \(T\) is complete,

\[E(U^{\prime}|T)=E(U|T)\ [P_{\theta}]\ \mathrm{for\ all}\ \theta\in\Theta.\]

Hence \(\mathrm{var}_{\theta}(U^{\prime})\succeq\mathrm{var}_{\theta}[E(U^{\prime}|T) ]=\mathrm{var}_{\theta}[E(U|T)]=\mathrm{var}_{\theta}(U)\) as desired. \(\Box\)

So, if an unbiased estimator in \(\mathfrak{U}\) is measurable with respect to a complete and sufficient statistic its variance reaches the lower bound among \(\mathfrak{U}\). This leads us to introduce the following notion of the optimal estimator among unbiased estimators.

**Definition 2.11**: _A statistic \(U\in\mathfrak{U}\) is called Uniformly Minimum Variance Unbiased Estimator (UMVUE) if for any member \(U^{\prime}\) of \(\mathfrak{U}\) we have \(\mathrm{var}_{\theta}(U)\leq\mathrm{var}_{\theta}(U^{\prime})\) for all \(U^{\prime}\in\mathfrak{U}\)._The following result follows directly from Theorem 2.12 and Definition 2.11.

**Corollary 2.2**: _Suppose there is a complete and sufficient statistic \(T\) for \(\mathfrak{M}\). Then for any \(U_{1},U_{2}\in\mathfrak{U}\), we have \(E(U_{1}|T)=E(U_{2}|T)\)\([P_{\theta}]\) for all \(\theta\in\Theta\). Moreover, \(E(U_{1}|T)\) is the UMVUE._

The above corollary implies that there can be only one measurable function of \(\sigma(T)\) that is UMVUE. In fact, the uniqueness of UMVUE need not be restricted to measurable functions of \(T\). We now show that UMVUE is unique. We first introduce a lemma.

**Lemma 2.5**: _Suppose \(X\) and \(Y\) are \(p\)-dimensional random vectors defined on a probability space \((\Omega,\mathcal{F},P)\) whose entries belong to \(L_{2}(P)\). Then the following statements are equivalent:_

1. _For any_ \(A\in\mathbb{R}^{p\times p}\)_,_ \(\mathrm{var}(X)\preceq\mathrm{var}(X+AY)\)_._
2. \(\mathrm{cov}(X,Y)=0\)_._

Assertion 1 holds if and only if, for all \(t\in\mathbb{R}^{p}\), \(t\neq 0\), we have

\[\mathrm{var}(t^{T}X)\leq\mathrm{var}(t^{T}X+t^{T}AY).\]

Since \(A\) can take any value in \(\mathbb{R}^{p\times p}\), if we let \(\tau=At\), then \(\tau\) can take any value in \(\mathbb{R}^{p}\) for any fixed \(t\neq 0\). Thus, assertion 1 holds if and only if, for any fixed \(t\neq 0\), \(f(\tau)=\mathrm{var}(t^{T}X+\tau^{T}Y)\) is minimized at \(\tau=0\), which happens if and only if \(\partial f(0)/\partial\tau=0\) for any \(t\neq 0\). Since

\[f(\tau)=\mathrm{var}(t^{T}X)+2\mathrm{cov}(t^{T}X,Y)\tau+\tau^{T}\mathrm{var}( Y)\tau,\]

assertion 1 holds if and only if \(\mathrm{cov}(t^{T}X,Y)=0\) for any \(t\neq 0\), which happens if and only if \(\mathrm{cov}(X,Y)=0\). \(\Box\)

**Theorem 2.13**: _Suppose \(\mathfrak{M}=\{P_{\theta}:\theta\in\Theta\}\) is a dominated and identifiable parametric family. A UMVUE, if it exists, is unique \([P_{\theta}]\) for each \(\theta\in\Theta\)._

Suppose \(U_{1}\) and \(U_{2}\) are both UMVUE. Then

\[\mathrm{var}_{\theta}(U_{1}-U_{2})=\mathrm{var}_{\theta}(U_{1})-\mathrm{cov} _{\theta}(U_{1},U_{2})-\mathrm{cov}_{\theta}(U_{2},U_{1})+\mathrm{var}_{ \theta}(U_{2}).\]

Since both \(U_{1}\) and \(U_{2}\) are UMVUE, their variances are the same. So the right-hand side of the above equality becomes

\[2\mathrm{var}_{\theta}(U_{1})-\mathrm{cov}_{\theta} (U_{1},U_{2})-\mathrm{cov}_{\theta}(U_{2},U_{1})\] \[=\mathrm{cov}_{\theta}(U_{1},U_{1}-U_{2})+\mathrm{cov}_{\theta}( U_{1}-U_{2},U_{1}).\]

If \(A\in\mathbb{R}^{p\times p}\), then \(U_{1}+A(U_{2}-U_{1})\) is an unbiased estimate, and hence \[\operatorname{var}_{\theta}(U_{1})\preceq\operatorname{var}_{\theta}(U_{1}+A(U_{2}- U_{1}))\]

By Lemma 2.5, \(\operatorname{cov}_{\theta}(U_{1},U_{2}-U_{1})=0\). This also implies \(\operatorname{cov}_{\theta}(U_{2}-U_{1},U_{1})=0\). Therefore

\[\operatorname{var}_{\theta}(U_{1}-U_{2})=0.\]

Because \(E_{\theta}(U_{1})=E_{\theta}(U_{2})=\theta\), the above equality implies \(E_{\theta}\|U_{1}-U_{2}\|^{2}=0\). Hence \(U_{1}=U_{2}\ [P_{\theta}]\). \(\Box\)

**Example 2.3**: In this example we develop the UMVU estimate for \(\theta\) using the Lehmann-Scheffe theorem under the setting of Example 2.2. Consider an i.i.d. sample \(\{X_{1},\ldots,X_{n}\}\) from \(U(0,\theta)\). Thus \((X_{1},\ldots,X_{n})\) has density

\[\theta^{-n}\prod_{i=1}^{n}I_{(0,\theta)}(X_{i}).\]

By the factorization theorem, the sufficient statistic is \(T=\max(X_{1},\ldots,X_{n})\). The distribution of \(T\) is \(P_{\theta}(T<t)=\prod P_{\theta}(X_{i}<t)=(t/\theta)^{n}\), and its density is \(n(t/\theta)^{n-1}(1/\theta)\). Hence

\[E_{\theta}T=\int_{0}^{\theta}tn(t/\theta)^{n-1}(1/\theta)\,dt=n\theta\int_{0}^ {1}s^{n}\,ds=\frac{n}{n+1}\theta.\]

Let \(U=(n+1)T/n\). Then \(U\) is an unbiased estimate. We now compute its variance:

\[\operatorname{var}_{\theta}U=\left(\frac{n+1}{n}\right)^{2}\operatorname{var} _{\theta}T=\left(\frac{n+1}{n}\right)^{2}(E_{\theta}T^{2}-(E_{\theta}T)^{2}).\]

The first two moments on the right hand side are computed to be

\[E_{\theta}T = \frac{n}{n+1}\theta\] \[E_{\theta}(T^{2}) = \int_{0}^{\theta}t^{2}n(t/\theta)^{n-1}(1/\theta)dt=n\theta^{2} \int_{0}^{1}(s)^{n+1}ds=\frac{n}{n+2}\theta^{2}\]

Therefore

\[\operatorname{var}_{\theta}(U)=\left[\frac{1}{n(n+2)}\right]\theta^{2}.\]

By Lehmann-Scheffe's theorem, \(U\) is the UMVU estimate if \(T\) is complete. Let \(g\) be a function such that \(E_{\theta}g(T)=0\) for all \(\theta>0\). Then

\[\int_{0}^{\theta}g(t)t^{n-1}dt=0\]

Take derivative with respect to \(\theta\) to obtain

\[g(\theta)\theta^{n-1}=0\]

for all \(\theta>0\). Thus \(g(\theta)=0\) for all \(\theta>0\), and \(T\) is complete. \(\Box\)

### Fisher consistency and two classical estimators

In this section we introduce another important criterion, known as Fisher consistency, to assess an estimator, besides unbiasedness and minimal variance. Strict unbiasedness is often too strong and can exclude many useful estimators. Fisher consistency is a more practical criterion, and, more importantly, it often suggests the form of an estimator. Let \(\mathfrak{M}\) be a class of all probability measures on \((\Omega_{X},\mathcal{F}_{X})\). A parameter can be viewed as a mapping from \(\mathfrak{M}\) to \(\mathbb{R}^{p}\). For example, the mean parameter \(\theta=E_{P}(X)\) can be viewed as the mapping

\[\theta:\mathfrak{M}\rightarrow\mathbb{R}^{p},\ \ P\mapsto\int XdP.\]

Let \(P_{\circ}\) be the true probability model. Let \(X_{1},\ldots,X_{n}\) be an independent sample from \(X\). For \(x\in\Omega_{X}\), let \(\delta_{x}\) be the probability measure on \((\Omega_{X},\mathcal{F}_{X})\) defined by the set function \(\delta_{x}:\mathcal{F}_{X}\rightarrow\mathbb{R}\) by

\[\delta_{x}(A)=\begin{cases}1&x\in A\\ 0&x\notin A\end{cases}\]

It is left as an exercise to show that \(\delta_{x}\) is a probability measure on \((\Omega_{X},\mathcal{F}_{X})\). It is called the point mass at \(x\). The set function defined by

\[P_{n}=n^{-1}\sum_{i=1}^{n}\delta_{X_{i}}\]

is called the empirical distribution of \(X\). It is left as an exercise to show that \(P_{n}\) is a probability measure and for any measurable function \(f:\Omega_{X}\rightarrow\mathbb{R}\),

\[\int fdP_{n}=n^{-1}\sum_{i=1}^{n}f(X_{i}).\]

**Definition 2.12**: _Let \(T=t(X_{1},\ldots,X_{n})\) be an \(\mathbb{R}^{p}\)-valued statistic and \(\theta_{\circ}\) be a \(p\)-dimensional vector. We say that \(T\) is a Fisher consistent estimate of \(\theta_{\circ}\) if there is a mapping \(\theta:\mathfrak{M}\rightarrow\mathbb{R}^{p}\) such that_

\[T=\theta(P_{n}),\ \ \theta_{\circ}=\theta(P_{\circ}).\]

We next introduce two widely used estimators in statistics: the method of moment (Pearson, 1902) and the maximum likelihood estimator (Fisher, 1922). Let \(\mathfrak{M}_{\circ}=\{P_{\theta}:\theta\in\Theta\}\) be a dominated, identifiable parametric family on \((\Omega_{X},\mathcal{F}_{X})\). Suppose \(X^{1},\ldots,X^{p}\) are integrable with respect to \(P_{\theta}\). We define the solution to the following system of equations

\[\int X^{k}dP_{\theta}=\int X^{k}dP_{n},\ \ k=1,\ldots,p \tag{2.12}\]

is the method-of-moment estimator of \(\theta_{\circ}\). Let \(\mu(\theta)=(\mu_{1}(\theta),\ldots,\mu_{p}(\theta))^{T}\).

**Theorem 2.14**: _Suppose that the mapping \(\theta\mapsto\mu(\theta)\) is injective and the vector_

\[\left(\int XdP_{n},\ldots,\int X^{p}dP_{n}\right)^{T}.\]

_is in the range of \(\mu\). Then the (unique) solution to (2.12) is Fisher consistent for \(\theta_{{}_{0}}\), the parameter corresponding to the true distribution in \(\mathfrak{M}_{{}_{0}}\)._

Proof: Let \(\theta:\mathfrak{M}\rightarrow\mathbb{R}^{p}\) be a mapping that satisfies

\[\theta(P)=\mu^{-1}\left(\int XdP,\ldots,\int X^{p}dP\right)\text{ for any }P \in\mathfrak{M}_{{}_{0}}\cup\{P_{n}\}.\]

Then

\[\theta(P_{\theta_{0}})=\mu^{-1}\left(\int XdP_{\theta_{0}},\ldots,\int X^{p}dP _{\theta_{0}}\right)=\mu^{-1}\left(\mu_{1}(\theta_{{}_{0}}),\ldots,\mu_{p}( \theta_{{}_{0}})\right)=\theta_{{}_{0}}.\]

Meanwhile, if we let \(T\) be the solution to (2.12), then

\[T=\mu^{-1}\left(\int XdP_{n},\ldots,\int X^{p}dP_{n}\right)=\theta(P_{n}).\]

Hence \(T\) is Fisher consistent. \(\Box\)

Let us now turn to the maximum likelihood estimate. Let

\[\ell(\theta;X_{1},\ldots,X_{n})=\sum_{i=1}^{n}\log f_{\theta}(X).\]

The random function \(\theta\mapsto\ell(\theta;X_{1},\ldots,X_{n})\) is called the likelihood function. The maximum likelihood estimator (MLE) is defined as the maximizer of the likelihood function. That is,

\[T=\operatorname{argmax}\left\{\ell(\theta:X_{1},\ldots,X_{n}):\theta\in\Theta \right\}.\]

Clearly, it is equivalent to maximize the function \(n^{-1}\ell(\theta,X_{1},\ldots,X_{n})\). Thus the maximum likelihood estimator can be defined alternatively as

\[T=\operatorname{argmax}\left\{\int\log f_{\theta}(X)dP_{\theta}:\theta\in \Theta\right\}.\]

**Theorem 2.15**: _Suppose \(\mathfrak{M}_{{}_{0}}\) is a dominated, identifiable parametric family and the likelihood function \(\ell(\theta,X_{1},\ldots,X_{n})\) has a unique maximizer over \(\Theta\) for each \(X_{1}(\omega),\ldots,X_{n}(\omega)\). Then the maximum likelihood estimator is Fisher consistent._Proof.: Since \(\mathfrak{M}_{0}\) is identifiable, by Theorem 2.1, for each \(\theta^{\prime}\in\Theta\), the maximizer of

\[\int\log f_{\theta}(X)dP_{\theta^{\prime}}\]

over \(\Theta\) is unique and is equal to \(\theta^{\prime}\) itself. Let \(\theta:\mathfrak{M}\to\mathbb{R}^{p}\) be a mapping such that

\[\theta(P)\mapsto\operatorname{argmax}\left\{\int\log f_{\theta}(X)dP:\theta \in\Theta\right\}\ \text{for any}\ P\in\mathfrak{M}_{0}\cup\{P_{n}\}.\]

Let \(T\) be the maximum likelihood estimator. Then,

\[\theta(P_{\theta_{0}}) = \operatorname{argmax}\left\{\int\log f_{\theta}(X)dP_{\theta_{0}}: \theta\in\Theta\right\}=\theta_{0}\] \[\theta(P_{n}) = \operatorname{argmax}\left\{\int\log f_{\theta}(X)dP_{n}:\theta \in\Theta\right\}=T.\]

Thus \(T\) is Fisher consistent. 

## Problems

**2.1**.: Let \((\Omega,\mathcal{F})\) be a measurable space, and \(A\) be a nonempty set in \(\mathcal{F}\). Show that \(\{F\cap A:F\in\mathcal{F}\}\) is a \(\sigma\)-field of subsets of \(A\).

**2.2**.: Let \((\Omega,\mathcal{F},\lambda)\) be a \(\sigma\)-finite measure space. Let \(A_{1},A_{2},\ldots\) be a sequence of \(\mathcal{F}\)-sets such that \(\lambda(A_{n})<\infty\) and \(\cup_{n}A_{n}=\Omega\). Let \(c_{n}\) be a sequence of positive numbers such that \(\sum_{n}c_{n}=1\). Define a set function \(\lambda^{*}:\mathcal{F}\to\mathbb{R}\) as

\[\lambda^{*}(A)=\sum_{n}c_{n}\lambda(A\cup A_{n})/\lambda(A_{n}),\]

where the quotients on the right are defined to be \(0\) if \(\lambda(A_{n})=0\). Show that \(\lambda^{*}\) is a probability measure and \(\lambda^{*}\equiv\lambda\).

**2.3**.: Let \((\Omega,\mathcal{F})\) be a measurable space. Let \(\{P_{n}:n\in\mathbb{N}\}\) be a sequence of probability measures on \((\Omega,\mathcal{F})\). Let \(\{c_{n}:n\in\mathbb{N}\}\) be a sequence of positive numbers such that \(\sum_{n\in\mathbb{N}}c_{n}=1\). For any \(A\in\mathcal{F}\), define

\[P(A)=\sum_{n\in\mathbb{N}}c_{n}P_{n}(A).\]

Prove:

1. the set function \(P:\mathcal{F}\to\mathbb{R}\) is a probability measure on \((\Omega,\mathcal{F})\);

2. \(\{P\}\equiv\{P_{n}:n\in\mathbb{N}\}\).

**2.4.** Let \((\Omega_{1},\mathcal{F}_{1})\) and \((\Omega_{2},\mathcal{F}_{2})\) be measurable spaces. Let \(T:\Omega_{1}\to\Omega_{2}\) be a mapping that is measurable \(\mathcal{F}_{1}/\mathcal{F}_{2}\). Prove:

1. \(T^{-1}(\mathcal{F}_{2})\) is a \(\sigma\)-field;
2. if \(f:\Omega_{1}\to\Omega_{2}\) is measurable with respect to \(\mathcal{F}_{1}/\mathcal{F}_{2}\), then \(f\) is measurable with respect to \(T^{-1}(\mathcal{F}_{2})\);
3. conclude that \(T^{-1}(\mathcal{F}_{2})=\sigma(T)\), where \(\sigma(T)\) is the intersection of all \(\sigma\)-field with respect to which \(f\) is measurable.

**2.5.** Let \((\Omega_{1},\mathcal{F}_{1})\) and \((\Omega_{2},\mathcal{F}_{2})\) be measurable spaces, and \(T:\Omega_{1}\to\Omega_{2}\) be a mapping that is measurable with respect to \(\mathcal{F}_{1}/\mathcal{F}_{2}\). Suppose \(Q\ll P\) are two measures on \((\Omega_{1},\mathcal{F}_{1})\). Show that

\[E(dQ/dP|\circ T)=dQ\circ T^{-1}/dP\circ T^{-1}.\]

**2.6.** Let \((\Omega_{X},\mathcal{F}_{X})\) and \((\Omega_{T},\mathcal{F}_{T})\) be measurable spaces and \(T:\Omega_{X}\to\Omega_{T}\) be measurable \(\mathcal{F}_{X}/\mathcal{F}_{T}\). Let \(f:\Omega_{X}\to\mathbb{R}\) be measurable \(T^{-1}(\mathcal{F}_{T})/\mathcal{R}\). Let \(t_{0}\in\Omega_{T}\), and let \(x_{0}\in T^{-1}(\{t_{0}\})\).

1. Prove that \(f^{-1}(\{f(x_{0})\})\) is a member of \(T^{-1}(\mathcal{F}_{T})\); that is \(f^{-1}(\{f(x_{0})\})=T^{-1}(F)\) for some \(F\in\mathcal{F}_{T}\);
2. From \(x_{0}\in f^{-1}(\{f(x_{0})\})=T^{-1}(F)\), prove that \(t_{0}\in F\), and hence that \(T^{-1}(\{t_{0}\})\subseteq T^{-1}(F)\);
3. From \(T^{-1}(\{t_{0}\})\subseteq f^{-1}(\{f(x_{0})\})\), prove that \(f\) is constant on \(T^{-1}(\{t_{0}\})\);
4. Conclude that, whenever \(T(x)=T(x^{\prime})\) for \(x,x^{\prime}\in\Omega_{X}\), we have \(f(x)=f(x^{\prime})\). That is, \(f\) depends on \(x\) only through \(T(x)\).

**2.7.** Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(f_{\theta}(x)=\theta^{-1}e^{-\frac{x-\theta}{\theta}}\), \(x>\theta\).

1. Find the MLE.

2. Find the method of moment estimator.

3. Find \(\alpha\) and \(\beta\) so that \(\alpha X_{(1)}\) and \(\beta\bar{X}\) are unbiased.

4. Find the a linear combination of \(X_{(1)}\) and \(\bar{X}\) so that it is unbiased and have smallest variance among all such linear combinations.

5. Find an unbiased estimate based on \(X_{1}\) and Rao-Blackwellize it. What is the improvement in the variance?

**2.8.** Suppose \(u(X)\) is an unbiased estimate of \(\theta\). \(f_{\theta}(x)\) is the density of \(X\). Let \(f_{\theta}^{(k)}(x)\) be the \(k\)th derivative of \(f_{\theta}\) with respect to \(\theta\). Then

\[\int uf_{\theta}^{(k)}d\mu=\begin{cases}1&k=1\\ 0&k=2,\ldots,r\end{cases}\]

So for any constants \(\alpha_{1},\ldots,\alpha_{r}\), we have

\[\int u(\alpha_{1}f_{\theta}^{(1)}+\cdots+\alpha_{r}f_{\theta}^{(r)})d\mu= \alpha_{1}\]Let \(b_{\theta}^{k}=f_{\theta}^{(k)}/f_{\theta}\). These form the so called Bhattacharyya basis. (Bhattacharyya, 1946). Then the above can be rewritten as

\[\int u(\alpha_{1}b_{\theta}^{1}+\cdots+\alpha_{r}b_{\theta}^{r})f_{\theta}d\mu= \alpha_{1}\]

So use the Cauchy Schwarz inequality, to get

\[\alpha_{1}^{2}\leq\text{var}_{\theta}(u)\text{var}_{\theta}(\alpha_{1}b_{\theta }^{1}+\cdots+\alpha_{r}b_{\theta}^{r})\]

Hence, for any constants \(\alpha_{1},\ldots,\alpha_{r}\) we have

\[\text{var}_{\theta}(u)\geq\frac{\alpha_{1}^{2}}{\text{var}_{\theta}(\alpha_{1}b _{\theta}^{1}+\cdots+\alpha_{r}b_{\theta}^{r})}\]

**2.9.** Stein's Lemma: Suppose \(X\) is a random variable having exponential family density

\[e^{\theta^{T}t(x)}h(x)c(\theta)\]

with respect to the Lebesgue measure. Suppose \(x\) is defined on an interval \((a,b)\) such that \(\lim_{x\to x^{\prime}}e^{\theta^{T}t(x)}h(x)=0\) for \(x^{\prime}=a,b\). \((a,b)\) is allowed to be \((-\infty,\infty)\). Then for any differentiable function \(g\) of \(x\) with \(E|g^{\prime}(X)|<\infty\). we have

\[E_{\theta}\left\{\left[\frac{h^{\prime}(X)}{h(X)}+\theta^{T}t(X)\right]g(X) \right\}=-E_{\theta}g^{\prime}(X).\]

In particular, if \(X\) has a normal distribution, then

\[\text{cov}[g(X),X]=\text{var}(X)E[g^{\prime}(X)].\]

A more general version of equality will be further studied in Chapter 5.

**2.10.** Let \(\{f_{\theta}:\theta\in\Theta\}\) be a family of densities of \(X\). Let \(u(X)\) be an unbiased estimator of \(\theta\). Consider the function

\[\psi_{\theta}(x)=\frac{f_{\theta+\Delta}(x)-f_{\theta}(x)}{f_{\theta}(x)},\]

where \(\Delta\) is any constant. Suppose both \(u(X)\) and \(\psi_{\theta}(X)\) have finite second moment. Show that

\[\text{var}_{\theta}(u(X))\geq\frac{\Delta^{2}}{\text{var}_{\theta}(\psi_{ \theta}(X))}.\]

This result is due to Hammersley (1950) and Chapman and Robbins (1951).

**2.11.** Let \(\{P_{\theta}:\theta\in\Theta\}\) be a family of distributions of \(X\). Let \(\mathcal{N}\) be the class of all statistics \(\delta(X)\) satisfying the following conditions:1. \(E_{\theta}\delta^{2}(X)<\infty\) for all \(\theta\in\Theta\);
2. \(E_{\theta}\delta(X)=0\) for all \(\theta\in\Theta\).

Use Lemma 2.5 to show that, an unbiased estimator \(u(X)\) is a UMVU estimator if and only if

\[\mathrm{cov}_{\theta}(u(X),\delta(X))=0,\text{ for all }\theta\in\Theta\text{ and all }\delta\in\mathcal{N}.\]

Let \((\Omega,\mathcal{F},\mu)\) be a measure space, and \(\Theta\) be a subset of \(\mathbb{R}^{p}\). We say that a function \(g:\Theta\times\Omega\to\mathbb{R}\) is \(L_{k}(\mu)\)-Lipschitz with dominating slope \(g_{0}\geq 0\) if \(\int g_{0}^{k}d\mu\) and

\[|g(\theta_{2},x)-g(\theta_{1},x)|\leq g_{0}(x)\|\theta_{2}-\theta_{1}\|\]

for any \(\theta_{1},\theta_{2}\in\Theta\). Suppose

1. \(g:\Omega\times\Theta\to\mathbb{R}\) is differentiable with respect to \(\theta\) modulo \(\mu\);
2. \(g\) is \(L_{2}(\mu)\)-Lipschitz with dominating slope \(g_{0}\in L_{2}(\mu)\);

Then, for any \(h\in L_{2}(\mu)\), the function \(h(x)g(\theta,x)\) is \(L_{1}(\mu)\)-Lipschitz with dominating slope \(hg_{0}\).

**2.12.** Let \((\Omega,\mathcal{F},\mu)\) be a measure space, and \(\Theta\) be a subset of \(\mathbb{R}^{p}\). We say that a function \(g:\Theta\times\Omega\to\mathbb{R}\) is \(L_{k}(\mu)\)-Lipschitz with dominating slope \(g_{0}\geq 0\) if \(\int g_{0}^{k}d\mu\) and

\[|g(\theta_{2},x)-g(\theta_{1},x)|\leq g_{0}(x)\|\theta_{2}-\theta_{1}\|\]

for any \(\theta_{1},\theta_{2}\in\Theta\). Suppose

1. \(g:\Omega\times\Theta\to\mathbb{R}\) is differentiable with respect to \(\theta\) modulo \(\mu\);
2. \(g\) is \(L_{1}(\mu)\)-Lipschitz with dominating slope \(g_{0}\in L_{1}(\mu)\);

Then, for any bounded function \(h:\Omega\to\mathbb{R}\), the function \(h(x)g(\theta,x)\) is \(L_{1}(\mu)\)-Lipschitz with dominating slope \(|h|g_{0}\).

**2.13.** Suppose \(T\) is complete and sufficient with respect to \(\mathfrak{M}=\{P_{\theta}:\theta\in\Theta\}\). Let \(U_{1}\) and \(U_{2}\) be two members of \(\mathfrak{U}\). Show that \(E(U_{1}|T)=E(U_{2}|T)\)\([P_{\theta}]\), for all \(\theta\in\Theta\).

**2.14.** Show that if \(U\) be the UMVUE for \(\theta\), and \(V\) is any statistic whose components are in \(L_{2}(P_{\theta})\) and \(E_{\theta}V=0\) for all \(\theta\in\Theta\), then \(\mathrm{cov}_{\theta}(U,V)=0\) for all \(\theta\in\Theta\).

## References

* Barndorff-Nielsen (1978) Barndorff-Nielsen, O. E. (1978). Information and Exponential Families in Statistical Theory. Wiley.
* Basu (1955) Basu, D. (1955). On statistics independence of a complete sufficient statistic. _Sankhya_. **15**, 377-380.
* Bhattacharyya (1946) Bhattacharyya, A. (1946). On some analogues of the amount of information and their use in statistical estimation. _Sankhya: The Indian Journal of Statistics_. **8**, 1-14.
* Blackwell (1947) Blackwell, D. (1947). Conditional expectation and unbiased sequential estimation. _The Annals of Mathematical Statistics_, **18**, 105-110.
* Chapman and Robbins (1951) Chapman, D. G. and Robbins, H. (1951). Minimum variance estimation without regularity assumptions. _The Annals of Mathematical Statistics_, **22**, 581-586.
* Darmois (1935) Darmois, G. (1935). Surles lois de probabilites a estimation exhaustive. _C. R. Acad. Sci Paris_ (in French) **200**, 1265-1266.
* Fend (1959) Fend, A. V. (1959). On the attainment of Cramer-Rao and Bhattacharyya bounds for the variance of an estimate. _The Annals of Mathematical Statistics_, **30**, 381-388.
* Fisher (1922) Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. _Philosophical Transactions of the Royal Society A_, **222**, 594-604.
* Halmos and Savage (1949) Halmos, P. R. and Savage, L. J. (1949). Application of the Radon-Nikodym Theorem to the Theory of Sufficient Statistics. _The Annals of Mathematical Statistics_, **20**, 225-241.
* Hammersley (1950) Hammersley, J. M. (1950). On estimating restricted parameters. _Journal of the Royal Statistical Society, Series B_, **12**, 192-240.
* Horn and Johnson (1985) Horn, R. A. and Johnson, C. R. (1985). _Matrix Analysis_. Cambridge University Press.
* Koopman (1936) Koopman, B. O. (1936). On distributions admitting a sufficient statistic. _Transactions of the American Mathematical Society_, **39**, 399-409.
* Lehmann and Romano (2005) Lehmann, E. L. and Romano, J. P. (2005). _Testing Statistical Hypotheses_. Third edition. Springer.
* Lehmann and Casella (1998) Lehmann, E. L. and Casella, G. (1998). _Theory of Point Estimation_. Second edition. Springer, New York.
* Lehmann and Scheffe (1950) Lehmann, E. L. and Scheffe, H. (1950). Completeness, similar regions, and unbiased estimation, I. _Sankya_, **10**, 305-340.
* Lehmann and Scheffe (1955) Lehmann, E. L. and Scheffe, H. (1955). Completeness, similar regions, and unbiased estimation, II. _Sankya_, **15**, 219-236.
* Lin'kov (2005) Lin'kov, Y. N. (2005). Lectures in Mathematical Statistics, Parts 1 and 2. In _Transactions of Mathematical Monographs_, **229**. American Mathematical Society.
* McCullagh and Nelder (1989) McCullagh, P. and Nelder, J. A. (1989). _Generalized Linear Models, Second Edition_, Chapman & Hall.
* Pearson (1902) Pearson, K. (1902). On the systematic fitting of curves to observations and measurements. _Biometrika_, **1**, 265-303.

Pitman, E. J. G. (1936). Sufficient statistics and intrinsic accuracy. _Proceedings of the Cambridge Philosophical Society_, _32_, 567-579.
* Rao (1945) Rao, C. R. (1945). Information and the accuracy attainable in the estimation of statistical parameters. _Bulletin of the Calcutta Mathematical Society_. **37**, 81-89.
* Rudin (1987) Rudin, W. (1987). _Real and Complex Analysis_. Third Edition. McGraw-Hill, Inc.

## 3 Testing Hypotheses for a Single Parameter

The central idea of optimal hypothesis test is the Neyman-Pearson Lemma (see Neyman and Pearson, 1933), which gives the form of the Most Powerful test for simple hypotheses. The basic idea of the Neyman-Pearson Lemma can be used to construct optimal tests for composite hypotheses, including one-sided and two-sided hypotheses. This is achieved by applying the Neyman-Pearson Lemma pointwise in the parameter spaces specified by composite hypotheses. To do so we require special assumptions on the forms of the distribution of the data, such as Monotone Likelihood Ratio and exponential family. The discussion of this chapter will be focussed on testing a scalar parameter. Vector-valued parameters will be treated in the next chapter.

### 3.1 Basic concepts

Scientific theories are posed as hypotheses; they uphold until refuted by sufficient evidence. While no amount of data can "prove" a scientific theory, a single instance can refute it. Suppose a hypothesis \(H\) implies an assertion \(A\) whose truth or falsehood can be determined by observation (say by experiments). If the observed facts are against \(A\), then the hypothesis \(H\) is false. New hypotheses are then to be formulated in the hope to accommodate the observed facts that are inconsistent with the old hypothesis.

In a perfectly deterministic world, whether \(A\) is false can be determined definitely, so that we can decide whether or not to reject \(H\) with certainty -- if \(H\) implies something that is false, then \(H\) itself must be false. In reality, however, the falsehood of \(A\) can in most cases only be determined with a degree of uncertainty. The need to make a decision in the face of uncertainty is the chief motivation for statistical hypothesis testing. The basic logic underlying statistical hypothesis testing is this: if \(H\) implies something unlikely, then \(H\) itself is unlikely to be true.

Let \((\Omega,\mathcal{F})\) be a measurable space, and \(X\) be a random element, which has range \(\Omega_{X}\), together with a \(\sigma\)-field \(\mathcal{F}_{X}\). Thus \(X\) is measurable \(\mathcal{F}/\mathcal{F}_{X}\). Let \(\mathcal{P}_{0}\)and \({\cal P}_{1}\) be two disjoint families of distributions defined on \((\Omega,{\cal F})\). Statistical hypotheses are formulated as

\[H_{0}:P\in{\cal P}_{0}\ \ \mbox{versus}\ \ H_{1}:P\in{\cal P}_{1}, \tag{3.1}\]

where \(H_{0}\) is called the null hypothesis, and \(H_{1}\) the alternative hypothesis. Note that the formulation (3.1) implicitly assumes that true distribution \(P\) must be in one of the two families. That is, \(P\in{\cal P}_{0}\cup{\cal P}_{1}\). If \(X\) falls into a region that has small probability under \(P\in{\cal P}_{0}\) -- that is, if \(X\) is unlikely whenever its distribution were from \({\cal P}_{0}\), then we can make a decision to reject \(H\).

Our action of rejecting or not rejecting \(H_{0}\) can be described by rejection region, also called critical region. A rejection region is any set \(C\in{\cal F}_{X}\) such that we reject \(H_{0}\) whenever \(X\) falls into \(C\). That is, the region \(C\) describes a rule of when to reject \(H_{0}\). This rule is called a nonrandomized test.

**Definition 3.1**: _For a nonrandomized test described by \(C\), let_

\[\alpha=\sup_{P\in{\cal P}_{0}}P(X\in C),\ \ \beta(P)=P(X\in C),\ \ P\in{\cal P}_{ 1}.\]

_Then \(\alpha\) is called the type I error, or size, of the test, and \(\beta(P)\) is called the power of the test at probability \(P\), and \(1-\beta(P)\) is called the type II error of the test at \(P\)._

Naturally, if \(H_{0}\) is true, we would like to reject \(H_{0}\) with a small probability, and if \(H_{0}\) is false (\(H_{1}\) is true), we would like to reject \(H_{0}\) with a large probability. That is, ideally, we want to choose \(C\) so that the both the type I and the type II errors are minimized. However, this is typically impossible, as the following example shows.

**Example 3.1**: Suppose that \(X\) has a binomial distribution

\[f_{\theta}(x)={n\choose x}\theta^{x}(1-\theta)^{n-x},\ \ x=0,1,\ldots,n,\ \ 0\leq \theta\leq 1.\]

We abbreviate this statement as \(X\sim b(n,\theta)\). Take \(n=2\). Then \(X\) has range \(\{0,1,2\}\). Suppose we are interested in testing the hypothesis

\[H_{0}:\theta=1/2\ \ \mbox{versus}\ \ H_{1}:\theta=1/6.\]

The table below gives type-I and type-II errors and power of all possible subsets of \(\{0,1,2\}\).

We see that there is no critical region for which \(\alpha\) and \(\beta\) are both minimized. \(\Box\)

From this table we also see that using a nonrandomized test we cannot control \(\alpha\) at an arbitrary level. For example, no critical region has type-I error exactly equal to 0.05. For a technical reason it is desirable to be able to control \(\alpha\) at an arbitrary level. This leads us to consider the following general form of test

\[\phi:\Omega_{X}\to[0,1],\;\;\;\phi\;\;\mbox{is measurable}\,{\cal F }_{X}/{\cal R}. \tag{3.2}\]

The evaluation of \(\phi\) at \(x\) is the conditional probability of rejecting \(H_{0}\) given the observation \(X=x\); \(1-\phi(x)\) is the conditional probability of not rejecting \(H_{0}\).

**Definition 3.2**: _A function \(\phi\) of the form (3.2) is called a test. A test \(\phi\) is called a randomized test if, for some \(x\in\Omega_{X}\), \(0<\phi(x)<1\); it is a nonrandomized test if \(\phi\) only takes 0 or 1 as its values._

This general definition is consistent with Definition 3.1: If \(\phi\) only takes 0 and 1 as its values, then the set \(C=\{x:\phi(x)=1\}\) is the rejection region in Definition 3.1. If \(X\in C\), we reject \(H_{0}\) (with probability 1) otherwise we do not reject \(H_{0}\) (or reject \(H_{0}\) with probability 0). Because \(\phi\) is assumed measurable, \(C\) is necessarily a set in \({\cal F}_{X}\).

**Definition 3.3**: _The test \(\phi\) for \(H_{0}:P\in{\cal P}_{0}\) versus \(H_{1}:P\in{\cal P}_{1}\) is said to have level of significance \(\alpha\), if_

\[\sup_{P\in{\cal P}_{0}}\,\int\phi(x)dP(x)\leq\alpha. \tag{3.3}\]

_The left side of (3.3) is called the size of the test._

As a special case, a nonrandomized test \(\phi\) is of significance level \(\alpha\) if \(P(C)\leq\alpha\) for all \(P\in{\cal P}_{0}\). Also note the subtle difference between the size and level of a test: the level of a test is an upper bound of the test. In other words the level of a test whose size is \(\alpha^{\prime}\) can be any number \(\alpha\) satisfying \(\alpha^{\prime}\leq\alpha\leq 1\).

**Definition 3.4**: _The function \(\beta_{\phi}\) on \({\cal P}\) given by \(\beta_{\phi}(P)=\int\phi(x)dP(x)\) is called the power function of \(\phi\)._

### The Neyman-Pearson Lemma

The simplest hypotheses \(H_{0}\) and \(H_{1}\) are ones in which \(\mathcal{P}_{0}\) and \(\mathcal{P}_{1}\) each contains only one distribution on \((\Omega,\mathcal{F})\). That is, \(\mathcal{P}_{0}=\{P_{0}\}\) and \(\mathcal{P}_{1}=\{P_{1}\}\). A test that contains only one distribution is called a simple hypothesis. A test that contains more than one distributions is called a composite hypothesis. Testing simple hypotheses is only of limited practical interest, but it lays the theoretical foundation for tests of composite hypotheses.

Consider the problem of testing a simple null hypothesis versus a simple alternative hypothesis

\[H_{0}:P=P_{0}\;\;\text{versus}\;H_{1}:P=P_{1}. \tag{3.4}\]

Let \(0\leq\alpha\leq 1\). We seek a test \(\phi\) of size \(\alpha\) such that its power at \(P_{1}\) is greater than or equal to the power at \(P_{1}\) of any other test of significance level \(\alpha\).

**Definition 3.5**: _A test \(\phi\) for simple-versus-simple hypotheses is a Most Powerful (MP) test of size \(\alpha\) if_

1. \(\beta_{\phi}(P_{0})=\alpha\)_,_
2. _for any_ \(\phi^{\prime}\) _with_ \(\beta_{\phi^{\prime}}(P_{0})\leq\alpha\) _we have_ \(\beta_{\phi}(P_{1})\geq\beta_{\phi^{\prime}}(P_{1})\)_._

Note that \(1-\beta_{\phi}(P_{1})\leq 1-\beta_{\phi^{\prime}}(P_{1})\) and hence an MP has minimum type-II error among all tests of significance level \(\alpha\).

Does such a test exist? If so, is it unique? A fundamental result by Neyman and Pearson (1933) will help in answering this question. See also Lehmann and Romano (2005, Chapter 3) and Ferguson (1967, Chapter 5). Without loss of generality, we can assume that there is a common measure \(\mu\) on \((\Omega,\mathcal{F})\) that dominates both \(P_{0}\) and \(P_{1}\) -- for example, we can simply take \(\mu=P_{0}+P_{1}\).

**Lemma 3.1** (Neyman-Pearson Lemma): _Let \(f_{0}\) and \(f_{1}\) be densities of \(P_{0}\) and \(P_{1}\) with respect to \(\mu\). Then any test \(\phi\) of the form_

\[\phi(x)=\begin{cases}1&\text{if}\;\;f_{1}(x)>kf_{0}(x)\\ \gamma(x)&\text{if}\;\;f_{1}(x)=kf_{0}(x)\\ 0&\text{if}\;\;f_{1}(x)<kf_{0}(x)\end{cases} \tag{3.5}\]

_where \(0\leq\gamma(x)\leq 1\) and \(k\geq 0\) is the MP of size \(\int\phi f_{0}d\mu\)._

Proof: Let \(\phi^{\prime}\) be any test of level \(\int\phi f_{0}d\mu\). We want to show that \(\int\phi dP_{1}\geq\int\phi^{\prime}dP_{1}\). Since \(\phi^{\prime}\) is a test,

\[(\phi(x)-\phi^{\prime}(x))(f_{1}(x)-kf_{0}(x))\geq 0\text{ for all }x.\]

Thus

\[\int(\phi-\phi^{\prime})(f_{1}-kf_{0})d\mu=\int(\phi-\phi^{\prime})f_{1}d\mu- k\int(\phi-\phi^{\prime})f_{0}d\mu\geq 0.\]Since \(\phi^{\prime}\) is of level \(\alpha\), \(\int\phi f_{0}d\mu\geq\int\phi^{\prime}f_{0}d\mu\). Hence

\[\int\phi f_{1}d\mu\geq\int\phi^{\prime}f_{1}d\mu,\]

as desired. \(\Box\)

The following proposition is a generalization of the intermediate value theorem.

**Proposition 3.1**: _Suppose \(\rho:\mathbb{R}\rightarrow\mathbb{R}\) is a right continuous, nonincreasing function satisfying_

_1. \(\rho(0-)=1\);_

_2. \(\lim_{x\rightarrow\infty}\rho(x)=0\)._

_Then for any \(0<\alpha\leq 1\) there exists \(k\in[0,\infty)\) such that \(\alpha\in[\rho(k),\rho(k-)]\)._

_Proof._ If \(\alpha=1\) then \(\alpha\in[\rho(0-),\rho(0)]\). Now suppose \(1>\alpha>0\). Let \(S=\{x:\rho(x)\leq\alpha\}\). This set is nonempty because \(\rho(x)\to 0\) as \(x\rightarrow\infty\). Let \(k=\inf S\). It is easy to see that \(k<\infty\). Since \(\rho(0-)=1\), we have \(k\geq 0\). Let \(x_{n}\) be a sequence in \(S\) approaching \(k\). Then, by right continuity, \(\rho(x_{n})\rightarrow\rho(k)\), and we have \(\rho(k)\leq\alpha\). Since \(\rho(x)>\alpha\) for all \(x<k\), we have \(\rho(k-)\geq\alpha\). \(\Box\)

**Theorem 3.1**: _For any \(0<\alpha\leq 1\), there exists a test \(\phi\) of size \(\alpha\) of the form (3.5) with \(\gamma(x)=\gamma\), a constant, and \(0\leq k<\infty\). Furthermore, if \(\phi^{\prime}\) is MP of size \(0<\alpha\leq 1\), then it has the form (3.5) a.e. \(P_{0}\) and \(P_{1}\). That is,_

\[P_{0}(\phi\neq\phi^{\prime},f_{1}\neq kf_{0})=0,\ \ \ P_{1}(\phi\neq\phi^{\prime},f_{1}\neq kf_{0})=0.\]

Note that the form (3.5) does not specify \(\gamma(x)\). In other words as long as \(\phi\) and \(\phi^{\prime}\) are the same on \(\{f_{1}\neq kf_{0}\}\), they are both of the form (3.5).

_Proof._ (Existence). For a test \(\phi\) of the type (3.5) with \(\gamma(x)=\gamma\), we have

\[\int\phi f_{0}d\mu=P_{0}(f_{1}>kf_{0})+\gamma P_{0}(f_{1}=kf_{0}).\]

Fix \(0<\alpha\leq 1\), and define

\[\rho(k)=P_{0}(f_{1}>kf_{0})=P_{0}(f_{1}/f_{0}>k,f_{0}>0).\]

Then \(\rho(\cdot)\) is a nonincreasing, right continuous function with left limit \(\rho(k-)=P_{0}(f_{1}\geq kf_{0})\). It is then clear that

\[\rho(0-)=1,\ \ \rho(0)=P_{0}(f_{1}>0),\ \ \lim_{k\rightarrow\infty}\rho(k)=0.\]It follows that \((0,1]\subseteq\cup\{[\rho(k),\rho(k-)]:\,0\leq k<\infty\}\). Hence for any \(0<\alpha\leq 1\) there is a \(0\leq k_{0}<\infty\) such that

\[P_{0}(f_{1}>k_{0}f_{0})\leq\alpha\leq P_{0}(f_{1}\geq k_{0}f_{0}).\]

If we take \(k=k_{0}\) in (3.5) and

\[\gamma=\begin{cases}\dfrac{\alpha-P_{0}(f_{1}>k_{0}f_{0})}{P_{0}(f_{1}=k_{0}f _{0})}&\text{ if }\,P_{0}(x:f_{1}=k_{0}f_{0})\neq 0\\ 0&\text{ otherwise,}\end{cases}\]

then \(\int\phi f_{0}d\mu=\alpha\).

(Uniqueness). Let \(0<\alpha\leq 1\), let \(\phi\) be a test of the type (3.5) of size \(\alpha\), and let \(\phi^{\prime}\) be another MP of size \(\alpha\). Then \(\int\phi f_{i}d\mu=\int\phi^{\prime}f_{i}d\mu\) for \(i=0,1\). Consequently,

\[\int(\phi-\phi^{\prime})(f_{1}-kf_{0})d\mu=\int(\phi-\phi^{\prime})f_{1}d\mu- k\int(\phi-\phi^{\prime})f_{0}d\mu=0.\]

Since \((\phi-\phi^{\prime})(f_{1}-kf_{0})\geq 0\), it is \(0\) a.e. \(\mu\). This implies

\[\mu(\{\phi-\phi^{\prime}\neq 0,f_{1}-kf_{0}\neq 0\})=0.\]

Since \(P_{0}\ll\mu\) and \(P_{1}\ll\mu\) we have the desired result. \(\Box\)

Existence and uniqueness of the MP test can also be established when \(\alpha=0\) if we adopt the convention (1.2). Let

\[\phi(x)=\begin{cases}1&\text{ if }\,f_{0}(x)=0\\ 0&\text{ if }\,f_{0}(x)>0.\end{cases} \tag{3.6}\]

By (1.2),

\[\{f_{1}>\infty f_{0}\}\subset\{f_{0}=0\},\ \ \{f_{1}=\infty f_{0}\}\subset\{f_{0}= 0\},\ \ \{f_{1}<\infty f_{0}\}\subset\{f_{0}>0\}.\]

Thus \(\phi\) in (3.6) has the form (3.5) with \(\gamma(x)=1\) and \(k=\infty\) and satisfies

\[\beta_{\phi}(P_{0})=\int\phi f_{0}d\mu=\int_{f_{0}>0}\phi f_{0}d\mu=0.\]

To prove uniqueness let \(\phi^{\prime}\) be another MP test of size \(\alpha=0\). Since \(\int\phi^{\prime}f_{0}d\mu=0\), \(\phi^{\prime}=0\) a.e. \(\mu\) on \(\{f_{0}>0\}\). Since \(\phi^{\prime}\) is the most powerful

\[0\geq\ \int(\phi-\phi^{\prime})f_{1}d\mu= \int_{\{f_{0}=0\}}(1-\phi^{\prime})f_{1}d\mu-\int_{\{f_{0}>0\}} \phi^{\prime}f_{1}d\mu\] \[= \int_{\{f_{0}=0\}}(1-\phi^{\prime})f_{1}d\mu\geq 0.\]Thus \(\phi^{\prime}=1\) a.e. \(\mu\) on \(\{f_{0}=0\}\cap\{f_{1}>0\}\). Since \(P_{i}(f_{0}=f_{1}=0)=0\) for \(i=0,1\), we see that \(\phi^{\prime}=1\) a.e. \(P_{0}\), \(P_{1}\) on \(\{f_{0}=0\}\). Thus, for \(i=0,1\),

\[P_{i}(\phi^{\prime}\neq\phi)= P_{i}(\phi^{\prime}\neq I_{\{f_{0}=0\}})\] \[= P_{i}(\phi^{\prime}\neq 0,f_{0}>0)+P_{i}(\phi^{\prime}\neq 1,f_ {0}=0)=0.\]

In other words \(\phi^{\prime}=\phi\) a.e. \(P_{0}\), \(P_{1}\).

### 3.3 Uniformly Most Powerful test for one-sided hypotheses

We now begin the process of generalizing the basic idea of the Neyman-Pearson Lemma to composite hypotheses. A family of distributions indexed by a parameter in a Euclidean space is called a parametric family. Let \(\Theta\) be a subset of the Euclidean space \(\mathbb{R}^{k}\). A parametric family is a set \(\mathcal{P}=\{P_{\theta}:\theta\in\Theta\}\), where each \(P_{\theta}\) is a probability measure on \((\Omega,\mathcal{F})\). Let \(\Theta_{0}\) and \(\Theta_{1}\) be a partition of \(\Theta\). That is, \(\Theta_{0}\cap\Theta_{1}=\varnothing\) and \(\Theta_{0}\cup\Theta_{1}=\Theta\). Hypotheses (3.1) in this context reduce to

\[H_{0}:\theta\in\Theta_{0}\ \ \text{versus}\ \ H_{1}:\theta\in\Theta_{1}.\]

For a parametric family, the power function \(\beta_{\phi}(P_{\theta})\) is a function of \(\theta\). We will write \(\beta_{\phi}(P_{\theta})\) as \(\beta_{\phi}(\theta)\). For a measurable function \(f\) of \(X\), we write its expectation \(\int fdP_{\theta}\) as \(E_{\theta}f(X)\).

In this section we consider the one-sided hypotheses. See Allen (1953), Karlin and Rubin (1956), Pfanzagl (1967), and Lehmann and Romano (2005). If \(\Theta\) is an interval in \(\mathbb{R}\), and \(\theta_{0}\in\Theta\), and if \(\Theta_{0}=\{\theta\in\Theta:\theta\leq\theta_{0}\}\) and \(\Theta_{1}=\{\theta\in\Theta:\theta>\theta_{0}\}\), then the above hypothesis takes the special form

\[H_{0}:\,\theta\leq\theta_{0}\ \ \text{versus}\ \ H_{1}:\,\theta>\theta_{0}. \tag{3.7}\]

This is called one-sided hypotheses. In this section we develop the UMP-\(\alpha\) test for one-sided hypotheses, and give the sufficient conditions under which such a test exists.

#### 3.3.1 Definition and examples of UMP tests

As in the case of simple versus simple hypotheses, we would like to find a size \(\alpha\) test that has the most power among all level \(\alpha\) tests. When \(\mathcal{P}_{0}\) and \(\mathcal{P}_{1}\) are composite, however, the set of powers, \(\{\beta_{\phi}(P):P\in\mathcal{P}_{1}\}\), is no longer a number and we would like it to be large for all \(P\in\mathcal{P}_{1}\). This is formulated rigorously as the Uniformly Most Powerful test.

**Definition 3.6**: _A test \(\phi\) of size \(\alpha\), that is, \(\sup_{P\in\mathcal{P}_{0}}\beta_{\phi}(P)=\alpha,\) for testing_\[H_{0}:P\in\mathcal{P}_{0}\ \text{versus}\ H_{1}:P\in\mathcal{P}_{1} \tag{3.8}\]

_is called a Uniformly Most Powerful test of size \(\alpha\) if, for any \(\phi^{\prime}\) of level \(\alpha\), that is,_

\[\sup_{P\in\mathcal{P}_{0}}\beta_{\phi^{\prime}}(P)\leq\alpha,\]

_we have \(\beta_{\phi}(P)\geq\beta_{\phi^{\prime}}(P)\) for all \(P\in\mathcal{P}_{1}\),_

A Uniformly Most Powerful test of size \(\alpha\) is abbreviated as a UMP-\(\alpha\) test.

A natural way to find a UMP-\(\alpha\) test is to apply the MP test for simple versus simple hypotheses \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta=\theta^{\prime}\) for each fixed \(\theta^{\prime}>\theta_{0}\). However, the test \(\phi\) obtained by such a procedure would in general depend on \(\theta^{\prime}\), and would therefore not be suitable for hypothesis (3.7), as it is not specific to any particular point in \(\Theta_{1}\).

However, in an important special case we can construct a test as described above to obtain an MP test not specific to \(\theta^{\prime}\). We first illustrate this by two examples. For illustration, we first construct UMP-\(\alpha\) test for the simple versus composite hypothesis

\[H_{0}:\theta=\theta_{0}\ \ \text{versus}\ \ H_{1}:\theta>\theta_{0}. \tag{3.9}\]

**Example 3.2** Let \(X\) be a \(b(n,\theta)\) random variable and suppose we are interested in testing (3.9) for a \(\theta_{0}\in(0,1)\). We first pick any fixed \(\theta^{\prime}>\theta_{0}\) and consider the simple versus simple hypotheses \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta=\theta^{\prime}\). Let

\[L(x)=\frac{f_{\theta^{\prime}}(x)}{f_{\theta_{0}}(x)}=\bigg{(}\frac{\theta^{ \prime}}{\theta_{0}}\bigg{)}^{x}\bigg{(}\frac{1-\theta^{\prime}}{1-\theta_{0}} \bigg{)}^{n-x}.\]

By the Neyman-Pearson Lemma, the MP test for \(\theta_{0}\) versus \(\theta^{\prime}\) is

\[\phi(x)=\begin{cases}1&\text{ if }\,L(x)>k\\ \gamma&\text{ if }\,L(x)=k\\ 0&\text{ if }\,L(x)<k\end{cases}\]

for some \(k\) and \(\gamma\). Since \(\theta^{\prime}>\theta_{0}\), the likelihood ratio \(L(x)\) is increasing in \(x\). Hence \(\phi\) is equivalent to

\[\phi(x)=\begin{cases}1&\text{ if }\,x>m\\ \gamma&\text{ if }\,x=m\\ 0&\text{ if }\,x<m\end{cases}\]

where \(\gamma\) and \(m\) are determined by \(E_{\theta_{0}}(\phi(X))=\alpha\); that is

\[P_{\theta_{0}}(X>m)+\gamma P_{\theta_{0}}(X=m)=\alpha.\]To be precise, we first find \(m\) such that \(P_{\theta_{0}}(X>m)\leq\alpha\leq P_{\theta_{0}}(X\geq m),\) and define \(\gamma(m)=[\alpha-P_{\theta_{0}}(X>m)]/P_{\theta_{0}}(X=m).\)

Clearly, the test \(\phi\) depends only on \(\theta_{0}\) and hence it is a size \(\alpha\) MP test for \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta=\theta^{\prime}\) for every \(\theta^{\prime}>\theta_{0}.\) This implies that it is a UMP-\(\alpha\) test for \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta>\theta_{0}.\)\(\Box\)

**Example 3.3** Let \(X\) denote a Gaussian random variable with density

\[f_{\theta}(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x-\theta)^{2}},\]

and let \(\theta^{\prime}>\theta_{0}.\) As in the previous example, the likelihood ratio,

\[L(x)=f_{\theta^{\prime}}(x)/f_{\theta_{0}}(x) =\exp[(x-\theta_{0})^{2}/2-(x-\theta^{\prime})^{2}/2]\] \[=\exp[(\theta_{0}^{2}-\theta^{\prime 2})/2+x(\theta^{\prime}- \theta_{0})],\]

is an increasing function of \(x.\) Fix a \(\theta^{\prime}>\theta_{0}.\) By the Neyman-Pearson Lemma, the MP test for \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta=\theta^{\prime}\) of size \(\alpha\) is of the form

\[\phi(x)=\begin{cases}1&\text{ if }\,f_{\theta_{1}}(x)>kf_{\theta_{0}}(x)\\ \gamma(x)&\text{ if }\,f_{\theta_{1}}(x)=kf_{\theta_{0}}(x)\\ 0&\text{ if }\,f_{\theta_{1}}(x)<kf_{\theta_{0}}(x)\end{cases}\]

which, because \(L(x)\) is monotone increasing in \(x,\) is equivalent to

\[\phi(x)=\begin{cases}1&\text{ if }\,x>k^{\prime}\\ \gamma(x)&\text{ if }\,x=k^{\prime}\\ 0&\text{ if }\,x<k^{\prime}\end{cases}.\]

Furthermore, since \(X\) is a continuous random variable, \(P_{\theta_{0}}(X=k^{\prime})=0.\) Hence we can choose \(\gamma(x)\) arbitrarily without changing the size of \(\phi.\) Choose \(\gamma(x)=0,\) and the above test reduces to

\[\phi(x)=\begin{cases}1&\text{ if }\,x>k^{\prime}\\ 0&\text{ if }\,x\leq k^{\prime}\end{cases},\]

where \(k^{\prime}\) is determined by

\[E_{\theta_{0}}(\phi(X))=\frac{1}{\sqrt{2\pi}}\int_{k^{\prime}}^{\infty}e^{- \frac{1}{2}(x-\theta_{0})^{2}}dx=\frac{1}{\sqrt{2\pi}}\int_{k^{\prime}-\theta_ {0}}^{\infty}e^{-\frac{1}{2}x^{2}}dx=\alpha.\]

Again, \(\phi\) is completely determined by \(\theta_{0}\) and \(\alpha,\) and is not specific to the \(\theta^{\prime}\) we started with. Thus \(\phi\) is UMP-\(\alpha\) for testing \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta>\theta_{0}.\)\(\Box\)What makes it possible for us to construct UMP test from a collection of MP simple versus simple tests in the foregoing examples is that the likelihood ratio is monotone in \(X\) in both cases. In fact, such a construction is always possible under the condition of monotone likelihood ratio, which we now formally define. Let \(\mathcal{P}=\{P_{\theta}:\theta\in\Theta\subset\mathbb{R}\}\) be a family of probability measures. Suppose that each \(P_{\theta}\) in \(\mathcal{P}\) is dominated by a common density \(\mu\). Let \(f_{\theta}(x)\) denote the density of \(P_{\theta}\) with respect to \(\mu\).

#### 3.3.2 Monotone Likelihood Ratio

The assumption of Monotone Likelihood Ratio for constructing one-sided UMP tests, whose importance we have seen in the last two examples, is crystallized by Karlin and Rubin (1956); a more general form is given by Pfanzagl (1967). See also Ferguson (1967) and Lehmann and Romano (2005).

**Definition 3.7**: _Let \(\mathcal{P}=\{P_{\theta}:\theta\in\Theta\}\) be a one-parameter family of probability measures dominated by a measure \(\mu\), and let \(f_{\theta}=dP_{\theta}/d\mu\). The family of densities \(\{f_{\theta}:\theta\in\Theta\}\) is said to have monotone likelihood ratio (MLR) in \(Y(x)\) if for any \(\theta_{1}<\theta_{2}\), \(\theta_{1},\theta_{2}\in\Theta\), the likelihood ratio \(L(x)=f_{\theta_{2}}(x)/f_{\theta_{1}}(x)\) is a monotone (nondecreasing or nonincreasing) function in \(Y(x)\) on a set on which \(L(x)\) is defined._

Here, we say that \(L(x)\) is defined if \(f_{\theta_{2}}(x)\) and \(f_{\theta_{1}}(x)\) are not both \(0\). If \(f_{\theta_{1}}(x)=0\) and \(f_{\theta_{2}}(x)>0\), we say that \(L(x)\) takes the value \(\infty\). Also note that according to this definition, if \(\mathcal{P}\) has MLR in \(Y(x)\), then \(Y(X)\) is sufficient for \(\mathcal{P}\). MLR can also be defined more generally without using densities; see Pfanzagl (1967).

Since \(L(x)\) is nondecreasing in \(Y(x)\) if and only if it is nonincreasing in \(-Y(x)\), for convenience we shall always assume \(L(x)\) to be nondecreasing in \(Y(x)\) in the rest of this chapter.

**Example 3.4**: Let \(U(a,b)\) denote the uniform distribution on an interval \((a,b)\), and suppose that the distribution of \(X\) belongs to the family \(\{U(0,\theta):\theta>0\}\). This family has MLR in \(Y(x)=x\). To see this, let \(\theta_{2}>\theta_{1}>0\). Then \(f(x|\theta_{i})=\theta_{i}^{-1}I_{(0,\theta_{i})}(x)\), \(i=1,2\). Thus \(L(x)\) is \(\theta_{2}/\theta_{1}\) on \((0,\theta_{1})\); it is \(\infty\) on \([\theta_{1},\theta_{2})\); it is not defined on \([\theta_{2},\infty)\). Thus \(L(x)\) is nondecreasing on the set on which it is defined.

Similarly, the family \(\{U(\theta,\theta+1);\theta>0\}\) has MLR in \(x\). Let \(\theta_{2}>\theta_{1}\). If \(\theta_{2}\geq\theta_{1}+1\), then \(L(x)\) is defined on \((\theta_{1},\theta_{1}+1)\cup(\theta_{2},\theta_{2}+1)\) and not defined elsewhere. Note that \(L(x)=0\) on \((\theta_{1},\theta_{1}+1)\) and \(L(x)=\infty\) on \((\theta_{2},\theta_{2}+1)\). Hence the family has MLR in \(x\). Suppose \(\theta_{1}<\theta_{2}<\theta_{1}+1\). Then \(L(x)\) is defined on \((\theta_{1},\theta_{2}+1)\) and is not defined elsewhere. Note that \(L(x)=0\) on \((\theta_{1},\theta_{2}]\); \(L(x)=1\) on \((\theta_{2},\theta_{1}+1)\); \(L(x)=\infty\) on \([\theta_{1}+1,\theta_{2}+1)\). Thus the family has MLR in \(x\).

**Example 3.5** For the double exponential

\[f_{\theta}(x)=\frac{1}{2\beta}\exp\big{(}-|x-\theta|/\beta\big{)},\]

where the scale parameter \(\beta>0\) is known, the likelihood ratio

\[L(x)=f_{\theta_{2}}(x)/f_{\theta_{1}}(x)=\exp\left\{\frac{1}{\beta}\big{(}|x- \theta_{1}|-|x-\theta_{2}|\big{)}\right\},\]

is given by

\[L(x)=\begin{cases}\exp\big{(}(\theta_{1}-\theta_{2})/\beta\big{)}&\text{if \ \ }x<\theta_{1}\\ \exp\big{(}(2x-\theta_{1}-\theta_{2})/\beta\big{)}&\text{if \ \ }\theta_{1}\leq x<\theta_{2}\\ \exp\big{(}(\theta_{2}-\theta_{1})/\beta\big{)}&\text{if \ \ }\theta_{2}\geq x\end{cases}\]

So if \(\theta_{1}<\theta_{2}\), then \(L\) is continuous and nondecreasing in \(x\). Hence the family has MLR in \(x\). \(\Box\)

However, Cauchy family does not have MLR in \(x\).

**Example 3.6** For Cauchy distribution with density

\[f_{\theta}(x)=\frac{\theta}{\pi(x^{2}+\theta^{2})},\quad\theta>0,\]

the likelihood ratio is

\[L(x)=\frac{f_{\theta_{2}}(x)}{f_{\theta_{1}}(x)}=\frac{\theta_{2}}{\theta_{1} }\left(\frac{\theta_{1}^{2}+x^{2}}{\theta_{2}^{2}+x^{2}}\right)=\frac{\theta_ {2}}{\theta_{1}}\bigg{(}\frac{\theta_{1}^{2}-\theta_{2}^{2}}{\theta_{2}^{2}+x^ {2}}+1\bigg{)}.\]

Obviously \(L(x)\) is symmetric in \(x\). We know it is not a constant. Hence it is not monotone. \(\Box\)

The next lemma is useful in deriving UMP tests for one sided tests.

**Lemma 3.2**: _Suppose that the family of densities \(\{f_{\theta}(x):\theta\in\Theta\}\) has MLR (nondecreasing) in \(Y(x)\). If \(\phi\) is a non-decreasing and integrable function of \(Y\), then \(\beta_{\phi}(\theta)=\int\phi f_{\theta}d\mu\) is non-decreasing in \(\theta\)._

Proof: Let \(\theta_{1}<\theta_{2}\), and let

\[A=\{x:f_{\theta_{2}}(x)<f_{\theta_{1}}(x)\}\ \ \text{and}\ \ B=\{x:f_{\theta_{2}}(x)>f _{\theta_{1}}(x)\}.\]

By definition, \(L(a)<1<L(b)\) whenever \(a\in A,\,b\in B\). This implies that \(Y(b)>Y(a)\) and hence that \(\phi(b)\geq\phi(a)\). Therefore\[\beta_{\phi}(\theta_{2})-\beta_{\phi}(\theta_{1})= \int\phi(f_{\theta_{2}}-f_{\theta_{1}})d\mu\] \[= \int_{A}\phi(f_{\theta_{2}}-f_{\theta_{1}})d\mu+\int_{B}\phi(f_{ \theta_{2}}-f_{\theta_{1}})d\mu\] \[\geq \sup_{a\in A}\phi(a)\int_{A}(f_{\theta_{2}}-f_{\theta_{1}})d\mu+ \inf_{b\in B}\phi(b)\int_{B}(f_{\theta_{2}}-f_{\theta_{1}})d\mu\geq 0,\]

where the last inequality holds because

\[\inf_{b\in B}\phi(b)\geq\sup_{a\in A}\phi(a),\]

\[\int_{A}(f_{\theta_{2}}-f_{\theta_{1}})d\mu+\int_{B}(f_{\theta_{2}}-f_{\theta_ {1}})d\mu=\int(f_{\theta_{2}}-f_{\theta_{1}})d\mu=0,\]

and \(f_{\theta_{2}}-f_{\theta_{1}}<0\) on \(A\). \(\Box\)

#### 3.3.3 The general form of UMP tests

We now state the main result of this section. The theorem states, in essence, that the construction similar to those in Examples 3.2 and 3.3 always gives valid UMP test if the MLR assumption is satisfied.

**Theorem 3.2**: _Suppose that the family \(\{f_{\theta}(x):\theta\in\Theta\}\) has MLR (nondecreasing) in \(Y(x)\)._

_1. If \(\alpha>0\), then the test \(\phi\) defined by_

\[\phi(x)=\begin{cases}1&\text{if }\,Y(x)>k\\ \gamma&\text{if }\,Y(x)=k\\ 0&\text{if }\,Y(x)<k\end{cases},\qquad\int\phi f_{\theta_{0}}d\mu=\alpha \tag{3.10}\]

_is a UMP test for_

\[H_{0}:\theta\leq\theta_{0}\,\,\,versus\,\,\,H_{1}:\theta>\theta_{0}. \tag{3.11}\]

_2. If \(\alpha<1\), then the test \(\phi^{\prime}\) defined by_

\[\phi^{\prime}(x)=\begin{cases}1&\text{if }\,Y(x)<k^{\prime}\\ \gamma^{\prime}&\text{if }\,Y(x)=k^{\prime}\\ 0&\text{if }\,Y(x)>k^{\prime}\end{cases},\qquad\int\phi^{\prime}f_{\theta_{0}}d \mu=\alpha\]

_is UMP test for_

\[H^{\prime}_{0}:\theta\geq\theta_{0}\,\,\,versus\,\,\,H^{\prime}_{1}:\theta< \theta_{0}. \tag{3.12}\]The proof follows roughly the argument used in Examples 3.2 and 3.3. Additional care must be taken, however, to extend the null hypothesis from \(H_{0}:\theta=\theta_{0}\) in the examples to \(H_{0}:\theta\leq\theta_{0}\) here, which is achieved using the monotonicity of \(\beta_{\phi}(\theta)\), as shown in Lemma 3.2. By the MLR assumption, the likelihood ratio \(L(x)\) is a function of \(Y(x)\). We write \(L(x)\) as \(L_{0}(Y(x))\).

Proof of Theorem 3.2.: First, we show that test (3.10) is UMP for testing \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta>\theta_{0}\). To do so it suffices to show that, for any fixed \(\theta_{1}>\theta_{0}\), (3.10) is of the form

\[\phi_{1}(x)=\begin{cases}1&\text{ if }\,f_{\theta_{1}}(x)>k_{1}f_{\theta_{0}}(x) \\ \gamma(x)&\text{ if }\,f_{\theta_{1}}(x)=k_{1}f_{\theta_{0}}(x)\\ 0&\text{ if }\,f_{\theta_{1}}(x)<k_{1}f_{\theta_{0}}(x)\end{cases}\]

for some \(k_{1}\geq 0\) and some function \(0\leq\gamma(x)\leq 1\). Since \(\gamma(x)\) is arbitrary, any test that takes value \(1\) on \(\{f_{\theta_{1}}>k_{1}f_{\theta_{0}}\}\) and \(0\) on \(\{f_{\theta_{1}}<k_{1}f_{\theta_{0}}\}\) has the above form. Because the family \(\{f_{\theta}:\theta\in\Theta\}\) has MLR,

\[\{x:Y(x)\leq k\}\subset\{x:L_{0}(Y(x))\leq L_{0}(k)\}.\]

Hence

\[\{x:f_{\theta_{2}}(x)>L_{0}(k)f_{\theta_{1}}(x)\}\subset\{x:Y(x)>k\}.\]

Thus \(\phi\) in (3.10) takes the value \(1\) on \(\{f_{\theta_{2}}>k_{1}f_{\theta_{1}}\}\) where \(k_{1}=L(k)\). For a similar reason, it takes the value \(0\) on \(\{f_{\theta_{2}}<k_{1}f_{\theta_{1}}\}\). Thus we have proved \(\phi\) is UMP for testing \(\theta=\theta_{0}\) versus \(\theta>\theta_{0}\).

Next, we show that \(\phi\) is a UMP test of size \(\beta_{\phi}(\theta_{0})\) for testing \(\theta\leq\theta_{0}\) versus \(\theta>\theta_{0}\). Since, by Lemma 3.2, \(\beta_{\phi}(\cdot)\) is monotone nondecreasing, \(\phi\) has size \(\beta_{\phi}(\theta_{0})\). Let \(\Psi\) be the class of all tests of size \(\beta_{\phi}(\theta_{0})\). That is,

\[\Psi=\{\psi:\sup_{\theta\leq\theta_{0}}\beta_{\psi}(\theta)\leq\beta_{\phi}( \theta_{0})\}.\]

We need to show that \(\beta_{\phi}(\theta)\geq\beta_{\psi}(\theta)\) for all \(\theta>\theta_{0}\) and all \(\psi\in\Psi\). Let \(\Psi^{\prime}\) be the class of all tests whose power at \(\theta_{0}\) is no more than \(\beta_{\phi}(\theta_{0})\). That is, \(\Psi^{\prime}=\{\psi^{\prime}:\beta_{\psi^{\prime}}(\theta_{0})\leq\beta_{\phi }(\theta_{0})\}\). Clearly, \(\Psi\subset\Psi^{\prime}\). But we have already shown that \(\beta_{\phi}(\theta)\geq\beta_{\psi^{\prime}}(\theta)\) for all \(\psi^{\prime}\in\Psi^{\prime}\).

The second part of the theorem follows by considering \(1-\psi\), where \(\psi\) is a size \(1-\alpha\) test of the type (3.10). 

The next theorem establishes the existence of UMP test for one-sided hypotheses.

**Theorem 3.3**: _Suppose that the family \(\{f_{\theta}(x):\theta\in\Theta\}\) has MLR in \(Y(x)\). Then for any given \(0<\alpha\leq 1\) and \(\theta_{0}\in\Theta\), there exist \(-\infty\leq k\leq\infty\) and \(0\leq\gamma\leq 1\) such that \(\phi\) in (3.10) has size \(\alpha\)._Proof: Let \(P_{\theta_{0}}\) denote the distribution corresponding go \(f_{\theta_{0}}\). Choose \(k\) such that

\[P_{\theta_{0}}(Y(X)>k)\leq\alpha\leq P_{\theta_{0}}(Y(X)\geq k)\]

and define

\[\gamma=\begin{cases}\dfrac{\alpha-P_{\theta_{0}}(Y(X)>k)}{P_{\theta_{0}}(Y(X)=k )},&\text{ if }\ P_{\theta_{0}}(Y(X)=k)\neq 0\\ 0&\text{ otherwise.}\end{cases}\]

Clearly, a test of the form (3.10) with \(k\) and \(\gamma\) chosen above satisfies \(\beta_{\phi}(\theta_{0})=\alpha\). By Lemma 3.2, then, the size of the test is \(\alpha\). \(\Box\)

#### 3.3.4 Properties of the one-sided UMP test

We now further study the properties of the one-sided UMP test as given in (3.10). The next corollary shows that the UMP test (3.10) not only has the most power for \(\theta>\theta_{0}\), but also has the _least_ power for \(\theta<\theta_{0}\).

**Corollary 3.1**: _Suppose \(0<\beta_{\phi}(\theta_{0})<1\). If \(\phi\) is a test of the form (3.10), then, for any test \(\psi\) satisfying \(\beta_{\psi}(\theta_{0})\geq\beta_{\phi}(\theta_{0})\), we have \(\beta_{\psi}(\theta)\geq\beta_{\phi}(\theta)\) for all \(\theta\leq\theta_{0}\)._

Proof: From the proof of Theorem 3.2, \(1-\phi\) is the UMP test of size \(1-\beta_{\phi}(\theta_{0})\) for testing \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta<\theta_{0}\). Since \(\beta_{1-\psi}(\theta_{0})\leq\beta_{1-\phi}(\theta_{0})\), we have

\[1-\beta_{\psi}(\theta)=\beta_{1-\psi}(\theta)\leq\beta_{1-\phi}(\theta)=1- \beta_{\phi}(\theta)\]

for all \(\theta\leq\theta_{0}\). Consequently, \(\beta_{\psi}(\theta)\geq\beta_{\phi}(\theta)\) for all \(\theta\leq\theta_{0}\). \(\Box\)

A typical comparison between the power of the UMP test \(\phi\) and any other test \(\psi\) of the same size is presented in Figure 3.1.

Recall that Lemma 3.2 states that if \(\phi\) is a monotone function of \(Y(x)\) then it has a nondecreasing power function. The next lemma shows that \(\phi\) has a strictly increasing power function if the parametric family \(\{P_{\theta}:\theta\in\Theta\}\) is identifiable and if \(\phi\) is of the form (3.10). We say that a parametric family of probability measures \(\{P_{\theta}:\theta\in\Theta\}\) is identifiable if, whenever \(\theta_{1}\neq\theta_{2}\), \(P_{\theta_{1}}\neq P_{\theta_{2}}\), where the latter inequality means that there is a set \(A\) in \((\Omega,\mathcal{F})\) such that \(P_{\theta_{1}}(X\in A)\neq P_{\theta_{2}}(X\in A)\). Thus, identifiability means different parameters correspond to different probability measures. That is, the mapping \(\theta\mapsto P_{\theta}\) is injective.

**Theorem 3.4**: _Suppose that the family \(\{f_{\theta}(x):\theta\in\Theta\}\) have (nondecreasing) MLR in \(Y(x)\) and the corresponding family of probability measures \(\{P_{\theta}:\theta\in\Theta\}\) is identifiable. Let \(\phi\) be the test defined in (3.10). Then \(\beta_{\phi}(\theta)\) is strictly increasing over \(\{\theta:0<\beta_{\phi}(\theta)<1\}\)._Proof: Let \(\theta_{1}<\theta_{2}\) be numbers in \(\Theta\). As seen earlier in the proof of Theorem 3.2, the test \(\phi\) defined in (3.10) is equivalent to

\[\phi(x)=\begin{cases}1&\text{ if }\,f_{\theta_{2}}(x)>k^{\prime}f_{\theta_{1}}(x) \\ \gamma(x)&\text{ if }\,f_{\theta_{2}}(x)=k^{\prime}f_{\theta_{1}}(x)\\ 0&\text{ if }\,f_{\theta_{2}}(x)<k^{\prime}f_{\theta_{1}}(x)\end{cases}\]

for some \(k^{\prime}\) and \(\gamma(x)\). By the Neyman-Pearson Lemma, \(\phi\) is a size \(\beta_{\phi}(\theta_{1})\) MP test for \(H_{0}:\theta=\theta_{1}\) versus \(H_{1}:\theta=\theta_{2}\). If \(\beta_{\phi}(\theta_{1})=\beta_{\phi}(\theta_{2})\), then the test \(\phi^{*}(x)\equiv\beta_{\phi}(\theta_{1})\) satisfies

\[(\phi-\phi^{*})(f_{\theta_{2}}-k^{\prime}f_{\theta_{1}})\geq 0,\ \ \text{and}\ \ \ \int(\phi-\phi^{*})(f_{\theta_{2}}-k^{\prime}f_{\theta_{1}})d\mu=0.\]

So \(\mu\{(\phi-\phi^{*})(f_{\theta_{2}}-k^{\prime}f_{\theta_{1}})\neq 0\}=0\). Because \(\beta_{\phi}(\theta_{1})\neq 0\), whenever \(f_{\theta_{2}}\neq k^{\prime}f_{\theta_{1}}\), we have \(\phi\neq\phi^{*}\). And so

\[\{x:f_{\theta_{2}}(x)\neq f_{\theta_{1}}(x)\}\subset\{x:(\phi(x)-\phi^{*}(x))(f _{\theta_{2}}(x)-k^{\prime}f_{\theta_{1}}(x))\neq 0\}.\]

Thus we see that \(\mu(f_{\theta_{2}}\neq k^{\prime}f_{\theta_{1}})=0\). In other words \(f_{\theta_{2}}(x)=k^{\prime}f_{\theta_{1}}(x)\) a.e. \(\mu\), which implies \(k^{\prime}=1\). This leads to a contradiction \(P_{\theta_{1}}=P_{\theta_{2}}\). \(\Box\)

### 3.4 Uniformly Most Powerful Unbiased test and two-sided hypotheses

In this section we consider two-sided hypotheses, which include three types

Figure 3.1: Comparison of power functions

1. \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta\neq\theta_{0}\);
2. \(H_{0}:\theta_{1}\leq\theta\leq\theta_{2}\) versus \(H_{1}:\theta<\theta_{1}\) or \(\theta>\theta_{2}\);
3. \(H_{0}:\theta\leq\theta_{1}\) or \(\theta\geq\theta_{2}\) versus \(H_{1}:\theta_{1}<\theta<\theta_{2}\).

The way these tests are ordered above follows roughly according to how common they are in practice: for example hypotheses of type I are most commonly seen. Technically, however, it is easier to develop the optimal tests following the order of III\(\rightarrow\)II\(\rightarrow\)I, which will be the route we take.

Unlike the one-sided hypotheses, there are in general no UMP tests for two-sided hypotheses. UMP tests always exists for hypotheses III, but they do not exist for hypotheses I and II. This point is illustrated by the following example.

**Example 3.7** Suppose \(X\) is distributed as \(b(n,\theta),0<\theta<1\), and \(0<\alpha<1\). To test the null hypothesis \(H_{0}:\theta=\frac{1}{2}\) versus the two sided alternative \(H_{1}:\theta\neq\frac{1}{2}\) we first consider the one-sided alternative hypothesis \(H_{+}:\theta>\frac{1}{2}\). Then

\[\phi_{+}(x)=\begin{cases}1&\text{ if }\,x>c_{+}\\ \gamma_{+}(x)&\text{ if }\,x=c_{+}\\ 0&\text{ if }\,x<c_{+},\end{cases}\]

with \(E_{\frac{1}{2}}(\phi_{+}(X))=\alpha\) is UMP-\(\alpha\) test for testing \(H_{0}\) versus \(H_{+}\). Similarly,

\[\phi_{-}(x)=\begin{cases}1&\text{ if }\,x<c_{-}\\ \gamma_{-}(x)&\text{ if }\,x=c_{-}\\ 0&\text{ if }\,x>c_{-},\end{cases}\]

with \(E_{\frac{1}{2}}(\phi_{-}(X))=\alpha\), is UMP test of size \(\alpha\) for testing \(H_{0}\) versus \(H_{-}:\theta<\frac{1}{2}\).

Suppose \(\phi_{0}\) is a UMP-\(\alpha\) test for \(H_{0}\) versus \(H_{1}\). Then it is also UMP-\(\alpha\) for \(H_{0}\) versus \(H_{+}\). Consequently \(E_{\theta}(\phi_{0}(X))=E_{\theta}(\phi_{+}(X))\), for all \(\theta\geq\frac{1}{2}\). Let \(g(j)=\phi_{0}(j)-\phi_{+}(j)\). Then

\[0=E_{\theta}[g(X)]=\sum_{j=0}^{n}g(j)\binom{n}{j}\bigg{(}\frac{\theta}{1- \theta}\bigg{)}^{j}(1-\theta)^{n}.\]

If we let \(\eta=\theta/(1-\theta)\), then the above equality implies

\[\sum_{j=1}^{n}g(j)\binom{n}{j}\eta^{j}=0\]

for all \(\eta=\theta/(1-\theta)\geq 1\). So \(g(j)=0\) for all \(j\). In other words \(\phi_{0}(j)=\phi_{+}(j)\) for all \(j\). Using the same argument we can show that \(\phi_{-}\equiv\phi_{0}\). Thus \(\phi_{-}(j)=\phi_{+}(j)\) for all \(j\), which is impossible.

For example if \(n=4,\ \alpha=\frac{1}{16}\), then \[\phi_{+}(j)=\begin{cases}1&\text{ if }\,j>3\\ 0&\text{ if }\,j\leq 3,\end{cases}\qquad\phi_{-}(j)=\begin{cases}1&\text{ if }\,j<4\\ 0&\text{ if }\,j\geq 4,\end{cases}\]

but \(\phi_{+}\neq\phi_{-}\). So there is no UMP test for the 2-sided hypotheses. 

#### 3.4.1 Uniformly Most Powerful Unbiased tests

From Example 3.7 we see that UMP tests do not in general exist for two-sided composite hypotheses, because one can always sacrifice the power for one side and make the power for the other side as large as possible. Apparently, a certain restriction should be imposed. A simple condition to impose is that the power function, for \(\theta\) in the alternative, should take values greater than or equal to the size of the test. That is, the probability of rejecting the null hypothesis when false is never smaller than the probability of rejecting the null hypothesis when true. A test satisfying this condition is called an unbiased test (Neyman and Pearson, 1936). The following definition formulates the concept of unbiasedness in a more general setting than scalar parameter, which will become important for later discussions. Again, \(\mathcal{P}_{0}\) and \(\mathcal{P}_{1}\) denote two disjoint families of distributions.

**Definition 3.8**: _A size \(\alpha\) test \(\phi\) for testing \(H_{0}:P\in\mathcal{P}_{0}\) versus \(H_{1}:P\in\mathcal{P}_{1}\) is said to be unbiased if \(\alpha\leq\inf_{P\in\mathcal{P}_{1}}\beta_{\phi}(P)\)._

**Definition 3.9**: _A test \(\phi\) is called Uniformly Most Powerful Unbiased test of size \(\alpha\) (UMPU-\(\alpha\)) if_

_1. it has size \(\alpha\);_

_2. for any size \(\alpha\) unbiased test \(\psi\) we have \(\beta_{\phi}(P)\geq\beta_{\psi}(P)\) for all \(P\in\mathcal{P}_{1}\)._

If a UMP-\(\alpha\) exists, then its power cannot fall below that of the test \(\psi(x)\equiv\alpha\), for \(P\in\mathcal{P}_{1}\). So a UMP tests are unbiased. For a large class of testing problems, where UMP tests fail to exist, there do exist UMPU tests. By a similar argument, a UMPU-\(\alpha\) test must be itself unbiased. To see this, let \(\phi\) be an UMPU-\(\alpha\) test and let \(\psi\equiv\alpha\). Then \(\psi\) is unbiased of size \(\alpha\), and hence \(\beta_{\phi}(P)\geq\beta_{\psi}(P)=\alpha\) for all \(P\in\mathcal{P}_{1}\).

A fact about size-\(\alpha\) unbiased tests that will be useful in our discussion is that, if the power is continuous, then their power functions are constant on the boundary of the two sets of probability measures specified by the null and alternative hypotheses.

**Proposition 3.2**: _Suppose that on \(\mathcal{P}=\mathcal{P}_{0}\cup\mathcal{P}_{1}\) is defined a metric, say \(\rho\), with respect to which \(\beta_{\phi}(\cdot)\) is continuous. If \(\phi\) is a size-\(\alpha\) unbiased test, then \(\beta_{\phi}(P)=\alpha\) for all \(P\in\bar{\mathcal{P}}_{0}\cap\bar{\mathcal{P}}_{1}\), where \(\bar{\mathcal{P}}_{0}\) and \(\bar{\mathcal{P}}_{1}\) are the closures of \(\mathcal{P}_{0}\) and \(\mathcal{P}_{1}\) with respect to the metric._Proof: Let \(P\in\bar{\mathcal{P}}_{0}\cap\bar{\mathcal{P}}_{1}\). Then there is a sequence \(\{P^{\prime}_{k}\}\subseteq\mathcal{P}_{0}\) and \(\{P^{\prime\prime}_{k}\}\subseteq\mathcal{P}_{1}\) such that \(\rho(P^{\prime}_{k},P)\to 0\) and \(\rho(P^{\prime\prime}_{k},P)\to 0\). Thus

\[\alpha\geq\lim_{k\to\infty}\beta_{\phi}(P^{\prime}_{k})=\beta_{\phi}(P)=\lim_{k \to\infty}\beta_{\phi}(P^{\prime\prime}_{k})\geq\alpha,\]

as desired. 

The rest of this section is devoted to constructing UMPU-\(\alpha\) tests for hypotheses II and III and the UMP-\(\alpha\) test for hypotheses I. We first introduce some technical mechanism needed to construct optimal tests for two-sided hypotheses.

#### More properties of the exponential family

We have seen that the families with MLR property play an important role in constructing UMP test for one-sided hypotheses. Exponential families play a similar role in constructing UMPU tests for two-sided hypotheses.

Specializing to the current context, the exponential family (2.5) becomes

\[c(\theta)e^{\eta(\theta)Y(x)}, \tag{3.13}\]

where \(\eta\) is a monotone function of the parameter \(\theta\). Clearly, this family has the MLR property. Examples of one-parameter exponential families include normal \(\{N(\mu,1):\mu\in\mathbb{R}\}\), and binomial \(\{b(n,\theta):0<\theta<1\}\). For example, in the binomial case, the probability mass function is given by

\[f_{\theta}(x)=\binom{n}{x}\theta^{x}(1-\theta)^{n-x}=c(\theta)e^{\eta(\theta)Y( x)},\]

where \(c(\theta)=(1-\theta)^{n}\binom{n}{x}\), \(Y(x)=x\), and \(\eta(\theta)=\log(\theta/(1-\theta))\). In this section we study the properties of the exponential family that will be used frequently in the subsequent exposition.

By Lemma 3.2, Theorem 2.7, and Lemma 2.2, if \(f_{\theta}\) has the form (3.13), if \(\phi\) is a nondecreasing function of \(Y(x)\) where \(Y(x)\) is as it appears in (3.13), then \(\beta_{\phi}(\theta)\) is either strictly monotone increasing in \(\theta\) or constant in \(\theta\).

For a one-parameter exponential family (3.13), since \(E_{\theta}\psi(X)<\infty\) for \(\theta\in\Theta\), the power function \(\beta_{\psi}(\theta)\) is an analytic function for any test \(\psi\), and the derivative with respect to \(\theta\) can be brought inside the integral \(\int\psi f_{\theta}d\mu\). (Here and in what follows, we use the dot notation to represent derivatives. For example \(\dot{f}_{\theta}(x)\) or simply \(\dot{f}_{\theta}\) represents \(\partial f_{\theta}(x)/\partial\theta\), and \(\dot{\beta}_{\phi}(\theta)\) or simply \(\dot{\beta}_{\phi}\) represents \(\partial\beta_{\phi}(\theta)/\partial\theta\).) Hence

\[\dot{\beta}_{\psi}(\theta)=\int\psi\dot{f}_{\theta}d\mu=\int\psi[\dot{c}_{1}( \theta)/c_{1}(\theta)+Y]f_{\theta}d\mu. \tag{3.14}\]

If, in the above, we take \(\psi\equiv 1\), then \(\beta_{\psi}(\theta)\equiv 1\), and \(\dot{\beta}_{\psi}(\theta)=0\). It follows that 

[MISSING_PAGE_FAIL:90]

\[\int(\phi^{*}-\phi)f_{m+1}d\mu\geq\sum c_{i}\int(\phi^{*}-\phi)f_{i}d\mu.\]

If (3.18) holds then the right hand side is \(0\), proving assertion 1. If \(c_{1}\geq 0,\ldots c_{m}\geq 0\) and (3.19) holds, then each summand on the right hand is nonnegative. So the right hand side is nonnegative, proving assertion 2. \(\Box\)

The following is another variation of the Neyman-Pearson lemma that will be useful.

**Lemma 3.4**: _Suppose that \(f_{1}\) and \(f_{2}\) are two functions integrable with respect to \(\mu\). Let \(\phi^{*}\) be a test satisfying_

\[\phi^{*}(x)=\begin{cases}1&\text{if }f_{2}(x)>kf_{1}(x)\\ 0&\text{if }f_{2}(x)<kf_{1}(x)\end{cases} \tag{3.20}\]

_for some \(-\infty\leq k\leq\infty\). Then, for any test \(\phi\) that satisfies \(\int\phi f_{1}d\mu=\int\phi^{*}f_{1}\mu\) we have_

\[\int_{f_{1}\neq 0}\phi^{*}f_{2}d\mu\geq\int_{f_{1}\neq 0}\phi f_{2}d\mu.\]

_Proof._ By construction

\[(\phi^{*}(x)-\phi(x))(f_{2}(x)-kf_{1}(x))\geq 0\]

for all \(x\in\Omega_{X}\). Hence \(\int_{f_{1}\neq 0}(\phi^{*}-\phi)(f_{2}-kf_{1})d\mu\geq 0\), which implies

\[\int_{f_{1}\neq 0}(\phi^{*}-\phi)f_{2}d\mu\geq k\int_{f_{1}\neq 0}(\phi^{*}- \phi)f_{1}d\mu=\int(\phi^{*}-\phi)f_{1}d\mu=0.\]

\(\Box\)

#### 3.4.4 Quantile transformation and construction of two-sided tests

Our construction of two-sided optimal tests hinges on a type of quantile transformation, which we discuss in detail in this subsection. For a similar construction, see Ferguson (1967, Section 5.3). Let \(F\) be the distribution function of a random variable \(Y\). For \(0<\omega<1\), define

\[F^{-1}(\omega)=\inf\{y:F(y)\geq\omega\}.\]

This function is called the quantile function of \(Y\). Its definition is illustrated by Figure 3.2. We will use \(F(a-)\) to denote the left limit of \(F\) at \(a\); that is, \(F(a-)=P(Y<a)\). The following properties of \(F^{-1}\) will prove useful.

**Lemma 3.5**: _Let \(0<\omega<1\). Then:_

_1. \(F(y)<\omega\) if and only if \(y<F^{-1}(\omega)\)._

_2. \(F(y)\geq\omega\) if and only if \(y\geq F^{-1}(\omega)\)._

_3. \(F(F^{-1}(\omega)-)\leq\omega\leq F(F^{-1}(\omega))\)._

_4. If \(F\) is continuous at \(F^{-1}(\omega)\), then \(F(F^{-1}(\omega))=\omega\)._

Proof.: 1. By the definition of \(F^{-1}\), if \(F(y)\geq\omega\), then \(y\geq F^{-1}(\omega)\). This shows that \(y<F^{-1}(\omega)\) implies \(F(y)<\omega\). Now suppose \(y\geq F^{-1}(\omega)\). Then \(F(y)\geq F(F^{-1}(\omega))\). Let \(A_{\omega}=\{y:F(y)\geq\omega\}\). Then there is a sequence \(\{y_{k}\}\subseteq A_{\omega}\) such that \(\lim_{k}y_{k}=\inf A_{\omega}=F^{-1}(\omega)\). Because \(y_{k}\in A_{\omega}\), \(y_{k}\geq F^{-1}(\omega)\). By right continuity of \(F\), we have \(\lim_{k}F(y_{k})=F(F^{-1}(\omega))\). But we also know that \(F(y_{k})\geq\omega\) for each \(k\). So \(F(y)\geq F(F^{-1}(\omega))\geq\omega\).

2. This statement is equivalent to statement 1.

3. That \(F(F^{-1}(\omega))\geq\omega\) has been proved in the proof of assertion 1. Also by assertion 1, whenever \(y<F^{-1}(\omega)\), we have \(F(y)<\omega\). So \(F(F^{-1}(\omega)-)\leq\omega\).

4. This is a direct consequence of assertion 3. \(\Box\)

Let \(\gamma\) be a number in \((0,1)\) and let \(\Delta(y)\) denote the jump of \(F\) at \(y\); that is \(\Delta(y)=F(y)-F(y-)\). Let

\[G(y,\gamma)=F(y)-(1-\gamma)\Delta(y)=F(y-)+\gamma\Delta(y),\]

Thus, \(G(y,0)=F(y-)\), \(G(y,1)=F(y)\), and

\[F(y-)\leq G(y,\gamma)\leq F(y),\ \ \mbox{for}\ 0\leq\gamma\leq 1. \tag{3.21}\]

Figure 3.2: Quantile function

[MISSING_PAGE_EMPTY:5926]

Now let \(X\) be a random variable (or vector) defined on \((\Omega,\mathcal{F})\) and \(Y:x\mapsto Y(x)\) be a real-valued measurable function. Let \(F\) denote the distribution of \(Y\).

**Lemma 3.7**: _Let \(\gamma(\omega)\) be as in (3.22). For each \(0<\omega<1\), the function defined by_

\[\phi_{\omega}(x)=\begin{cases}1&\text{if }\,Y(x)<F^{-1}(\omega)\\ \gamma(\omega)&\text{if }\,Y(x)=F^{-1}(\omega)\\ 0&\text{if }\,Y(x)>F^{-1}(\omega)\end{cases} \tag{3.25}\]

_satisfies_

\[\phi_{\omega}(x)=E(I_{[0,\omega)}(G(Y(x),V)))=E(I_{[0,\omega)}(W)|X=x), \tag{3.26}\]

_for all \(x\), consequently \(E(\phi_{\omega}(X))=\omega\)._

Let \(y=Y(x)\). Then by Lemma 3.5, part 1, and (3.21), if \(y<F^{-1}(\omega)\), then \(G(y,\gamma)\leq F(y)<\omega\) for all \(0\leq\gamma\leq 1\). So

\[E[I_{[0,\omega)}(G(Y(x),V))]=1.\]

If \(y>F^{-1}(\omega)\), then, by Lemma 3.5, part 3, \(F(y-)\geq F(F^{-1}(\omega))\geq\omega\). So by (3.21), \(G(y,\gamma)\geq\omega\) for all \(0\leq\gamma\leq 1\), which implies

\[E[I_{[0,\omega)}(G(Y(x),V))]=0.\]

Finally, suppose \(y=F^{-1}(\omega)\). When \(\Delta(y)\neq 0\) we have

\[G(y,\gamma)<\omega\Leftrightarrow\gamma\Delta(y)<\omega-F(y-)=\gamma(\omega) \Delta(y)\Leftrightarrow\gamma<\gamma(\omega).\]

So \(P(G(y,V)<\omega)=P(V<\gamma(\omega))=\gamma(\omega)\). When \(\Delta(y)=0\), \(G(y,\gamma)<\omega\) does not hold for any \(\gamma\). So in this case \(P(G(y,V)<\omega)=0=\gamma(\omega)\). This completes the proof. \(\Box\)

**Lemma 3.8**: _Let \(f\) denote the density of \(X\) with respect to a measure \(\mu\) and \(h\) a function of \(x\) such that \(\int\limits_{f>0}|h|d\mu<\infty\). Let \(\phi_{\omega}\) be as defined in (3.25) with \(F\) being the distribution corresponding to \(Y(X)\). Then the function \(g:[0,1]\to[0,1]\) defined by_

\[g(\omega)=\begin{cases}0&\text{if }\,\,\omega=0\\ \int\limits_{f>0}\,\,\phi_{\omega}hd\mu&\text{if }\,\,0<\omega<1\\ 1&\text{if }\,\,\omega=1\end{cases}\]

_is continuous on \([0,1]\)._Proof: We have

\[g(\omega) =\int\limits_{f>0}\phi_{\omega}hd\mu=\int\limits_{f>0}\ \phi_{\omega} \frac{h}{f}\,fd\mu\] \[=\int\limits_{f>0}\ E[I_{[0,\omega)}(W)|X=x][h(x)/f(x)]\,f(x)\mu(dx)\] \[=E[I_{[0,\omega)}(W)(h(X)/f(X))].\]

Let \(\epsilon>0\) be such that \(0<\omega-\epsilon<\omega<\omega+\epsilon<1\). Then

\[|g(\omega+\epsilon)-g(\omega-\epsilon)|\leq E[I_{[\omega-\epsilon,\omega+ \epsilon)}(W)|h(X)/f(X)|].\]

The random variable \(I_{[\omega-\epsilon,\omega+\epsilon)}(W)|h(X)/f(X)|\) is dominated by \(|h(X)/f(X)|\), whose expectation is finite. So by Lebesgue's Dominated Convergence Theorem,

\[\lim_{\epsilon\to 0}E[I_{[\omega-\epsilon,\omega+\epsilon)}(W)|h(X)/f(X)|]=E[I_{\{ \omega\}}(W)|h(X)/f(X)|].\]

The right hand side is zero because \(I_{\{\omega\}}(W)=0\) almost everywhere. Hence \(g\) is continuous in \((0,1)\).

By a similar argument it can be shown that \(\lim_{\omega\to 1}g(\omega)=1\) and \(\lim_{\omega\to 0}g(\omega)=0\). Thus \(g(\omega)\) is continuous in \([0,1]\). \(\Box\)

**Lemma 3.9**: _Let \(f\) and \(h\) be densities of two probability distributions of \(X\) with respect to \(\mu\) such that \(\mu\{f=0,h>0\}=0\). Suppose that \(h(x)/f(x)\) is a non-decreasing function of \(Y(x)\). Then, for any \(0<\alpha<1\), there exist \(0\leq\gamma_{1},\ \gamma_{2}\leq 1,\ -\infty\leq t_{1}<t_{2}\leq+\infty\) such that the test_

\[\phi(x)=\begin{cases}1&\text{if}\ \,t_{1}<Y(x)<t_{2}\\ \gamma_{i}&\text{if}\ \,Y(x)=t_{i},\,i=1,2\\ 0&\text{if}\ \,Y(x)<t_{1}\,\text{or}\ Y(x)>t_{2},\end{cases} \tag{3.27}\]

_satisfies \(\int\phi fd\mu=\int\phi hd\mu=\alpha\)._

Proof: Let \(\phi_{\omega}\) be as defined in (3.25) with \(F\) therein being the distribution of \(Y(X)\). For \(0\leq u\leq 1-\alpha\), let

\[\psi_{u}(x)=\begin{cases}\phi_{\alpha}(x)&\text{if}\ \ u=0\\ \phi_{\alpha+u}(x)-\phi_{u}(x)&\text{if}\ \ 0<u<1-\alpha\\ 1-\phi_{1-\alpha}(x)&\text{if}\ \ u=1-\alpha\end{cases}\]

Clearly \(0\leq\psi_{u}(x)\leq 1\), and by Lemma 3.7, \(\int\psi_{u}fd\mu=\alpha\) for all \(u\in[0,1-\alpha]\).

That \(h(x)/f(x)\) is nondecreasing in \(Y(x)\) implies that the ratio is a function of \(Y(x)\). Write this ratio as \(L(Y(x))\). Then\[L(Y(x))>L(F^{-1}(\omega))\Rightarrow Y(x)>F^{-1}(\omega),\ \ \mbox{and}\] \[L(Y(x))<L(F^{-1}(\omega))\Rightarrow Y(x)<F^{-1}(\omega).\]

Hence \(1-\phi_{\omega}\) is of form (3.20) with \(f_{1}=f\) and \(f_{2}=h\). So by Lemma 3.4, taking \(\phi^{*}\) and \(\phi\) therein to be \(1-\phi_{\alpha}\) and \(1-\alpha\), we have

\[\int\limits_{f>0}\phi_{\alpha}hd\mu\leq\alpha,\ \ \mbox{and similarly},\ \ \int\limits_{f>0}\phi_{1-\alpha}hd\mu\leq 1-\alpha.\]

This implies, as \(\psi_{1-\alpha}=1-\phi_{1-\alpha}\) and \(\psi_{0}=\phi_{\alpha}\), that

\[\int\limits_{f>0}\psi_{0}hd\mu\leq\alpha\leq\int\limits_{f>0}\psi_{1-\alpha}hd\mu.\]

The case that \(h(x)/f(x)\) is nonincreasing in \(Y(x)\) can be treated similarly.

Now by construction \(s(u)=\int\limits_{f>0}\psi_{u}hd\mu=g(\alpha+u)-g(u)\) for \(u\in[0,1-\alpha]\). Hence by Lemma 3.8, \(s(u)\) is continuous in \([0,1]\). So there exists \(0\leq u_{0}\leq 1-\alpha\) such that \(s(u_{0})=\alpha\). But because \(\mu(\{f=0,h>0\})=0\), \(s(u_{0})=\alpha\) implies \(\int\psi_{u_{0}}hd\mu=\alpha\), as to be demonstrated. \(\Box\)

**Lemma 3.10**: _Suppose that \(f\) is the density of \(X\) with respect to a measure \(\mu\), that \(Y(x)\) is a measurable function, and that there is an interval \((-\epsilon,\epsilon)\) such that, for each \(\zeta\) in this interval, \(\int e^{\zeta Y(x)}f(x)\mu(dx)<\infty\). Then, for any \(0<\alpha<1\), there exist \(0\leq\gamma_{1},\gamma_{2}\leq 1\), \(-\infty\leq t_{1}<t_{2}\leq\infty\) such that the test (3.27) satisfies_

\[\int\phi fd\mu=\alpha,\ \ \int\phi Yfd\mu=\alpha\int Yfd\mu. \tag{3.28}\]

_Proof._ For \(\zeta\in(-\epsilon,\epsilon)\), define

\[f_{\zeta}(x)=c(\zeta)f(x)e^{\zeta Y(x)},\ \ \mbox{where}\ \ c(\zeta)=\left(\int f (x)e^{\zeta Y(x)}\mu(dx)\right)^{-1}.\]

Then \(\{f_{\zeta}:\zeta\in(-\epsilon,\epsilon)\}\) is an exponential family. Let \(\dot{f}(x)\) denote \(\partial f_{\zeta}(x)/\partial\zeta|_{\zeta=0}\). Then, by (3.15),

\[\dot{f}(x)/f(x)=Y(x)-\int Y(s)f(s)\mu(ds). \tag{3.29}\]

Thus \(\dot{f}(x)/f(x)\) is monotone increasing in \(Y(x)\). Let \(L(Y(x))=\dot{f}(x)/f(x)\). Then

\[L(Y(x))<L(F^{-1}(\omega))\Rightarrow Y(x)<F^{-1}(\omega)\] \[L(Y(x))>L(F^{-1}(\omega))\Rightarrow Y(x)>F^{-1}(\omega).\]It follows that \(\phi_{\alpha}\), as defined by (3.25), is of the form

\[\phi_{\alpha}(x)=\begin{cases}1&\dot{f}(x)<L(F^{-1})(\alpha)f(x)\\ 0&\dot{f}(x)>L(F^{-1})(\alpha)f(x)\end{cases}\]

By Lemma 3.4, \(\int\limits_{f>0}(1-\phi_{\alpha})\dot{f}d\mu\geq\int\limits_{f>0}\phi\dot{f}d\mu\) for any test \(\phi\) that satisfies \(\int\phi fd\mu=1-\alpha\). In particular,

\[\int\limits_{f>0}(1-\phi_{\alpha})\dot{f}d\mu\geq(1-\alpha)\int\limits_{f>0} \dot{f}d\mu=0,\]

implying \(\int\limits_{f>0}\phi_{\alpha}\dot{f}d\mu\leq 0\). For the same reason, \(\int\limits_{f>0}\phi_{1-\alpha}\dot{f}d\mu\leq 0\). Now define \(\psi_{u}(x)\), for \(0\leq u\leq 1-\alpha\), as in the proof of Lemma 3.9. Then

\[\int\limits_{f>0}\psi_{0}\dot{f}d\mu\leq 0\leq\int\limits_{f>0}\psi_{1-\alpha} \dot{f}d\mu.\]

Because \(f_{\zeta}\) is an exponential family, \(Y(X)\) has finite expectation. Consequently \(\int\limits_{f>0}|\dot{f}|d\mu<\infty\). Therefore, by Lemma 3.8, \(\int\psi_{u}\dot{f}d\mu\) is continuous in \([0,1-\alpha]\). So there is a \(0\leq u_{0}\leq 1-\alpha\) such that \(\int\limits_{f>0}\psi_{u_{0}}\dot{f}d\mu=0\) which, by (3.29), implies the second equality in (3.28). \(\Box\)

#### 3.4.5 UMP test for hypothesis III

We are now ready to derive the UMP test for hypothesis III: \(H_{0}:\theta\leq\theta_{1}\) or \(\theta\geq\theta_{2}\) versus \(H_{1}:\theta_{1}<\theta<\theta_{2}\).

**Theorem 3.5**: _Let \(X\) be a random variable with a density given by (2.4)._

_(1) For_ \(\theta_{1}<\theta_{2}\) _and_ \(0<\alpha<1\)_, there exist_ \(-\infty<t_{1}<t_{2}<\infty\)_,_ \(0\leq\gamma_{1},\gamma_{2}\leq 1\)_, such that_ \(\phi\) _defined by (_3.27_) satisfies_

\[E_{\theta_{1}}\phi(X)=E_{\theta_{2}}\phi(X)=\alpha. \tag{3.30}\]

_(2) Let_ \(\phi\) _be a test satisfying the conditions in part 1. Then, for any_ \(\psi\) _that satisfies_ \(E_{\theta_{1}}\psi(X)\leq\alpha\)_,_ \(E_{\theta_{2}}\psi(X)\leq\alpha\)_, we have_ \(E_{\theta}\phi(X)\geq E_{\theta}\psi(X)\) _for all_ \(\theta_{1}<\theta<\theta_{2}\)_._

_(3) Let_ \(\phi\) _be a test satisfying the conditions in part 1. Then, for any test_ \(\psi\) _satisfying_ \(E_{\theta_{1}}\psi(X)=E_{\theta_{2}}\psi(X)=\alpha\)_, we have_ \(E_{\theta}\psi(X)\geq E_{\theta}\phi(X)\) _for_ \(\theta<\theta_{1}\) _or_ \(\theta>\theta_{2}\)_._

_(4) Any test_ \(\phi\) _that satisfies conditions in part 1 is a UMP-_\(\alpha\) _test for testing_ \(H_{0}:\theta\leq\theta_{1}\) _or_ \(\theta\geq\theta_{2}\) _versus_ \(H_{1}:\theta_{1}<\theta<\theta_{2}\)

[MISSING_PAGE_EMPTY:5931]

Thus by (3.32), \(\phi(x)=1\) if and only if \(f_{\theta}(x)>c_{1}f_{\theta_{1}}(x)+c_{2}f_{\theta_{0}}(x)\) and \(\phi(x)=0\) if and only if \(f_{\theta}(x)<c_{1}f_{\theta_{1}}(x)+c_{2}f_{\theta_{2}}(x)\). So by Lemma 3.3,

\[E_{\theta}(\psi(X))\leq E_{\theta}(\phi(X))\]

for any test rule \(\psi\) satisfying \(E_{\theta_{i}}(\psi(X))\leq\alpha\).

(3) Let \(\theta<\theta_{1}\). In this case \(a_{1}>0\) and \(a_{2}<0\). So

\[\lim_{t\rightarrow-\infty}g(t)=0,\ \ \lim_{t\rightarrow\infty}g(t)=-\infty.\]

Moreover, \(g^{\prime}(t)=0\) has a unique solution \(t=t_{0}\); in fact:

\[t_{0}=\frac{1}{\theta_{1}-\theta_{2}}\log\left[-\frac{a_{2}(\theta_{2}-\theta )}{a_{1}(\theta_{1}-\theta)}\right].\]

These facts, together with \(g(t_{1})=g(t_{2})=1\), imply that \(g\) is strictly increasing for \(t<t_{0}\) and strictly decreasing for \(t>t_{0}\) for some \(t_{0}\in(t_{1},t_{2})\). Consequently, \(g(t)>1\) if and only if \(t_{1}<t<t_{2}\). Thus, as above by (3.32), we have for some \(c_{1}>0>c_{2}\) that

\[1-\phi(x)=1 \Leftrightarrow g(Y(x))<1 \Leftrightarrow f_{\theta}(x)>c_{1}f_{\theta_{1}}(x)+c_{2}f_{ \theta_{2}}(x)\] \[1-\phi(x)=0 \Leftrightarrow g(Y(x))>1 \Leftrightarrow f_{\theta}(x)<c_{1}f_{\theta_{1}}(x)+c_{2}f_{ \theta_{2}}(x).\]

Hence by Lemma 3.3,

\[E_{\theta}(1-\psi(X))\leq E_{\theta}(1-\phi(X)),\]

whenever \(E_{\theta_{i}}(1-\psi(X))=1-\alpha\). A similar result holds for \(\theta>\theta_{2}\).

(4) Let \(\psi\) be any test of level \(\alpha\). Then \(\beta_{\psi}(\theta_{1})\leq\alpha,\ \beta_{\psi}(\theta_{2})\leq\alpha\). Let \(\phi\) be a test satisfying the conditions in part 1. Then, by part 2, \(\beta_{\phi}(\theta)\geq\beta_{\psi}(\theta)\) for all \(\theta\in(\theta_{1},\theta_{2})\). It suffices to show that \(\phi\) is of size \(\alpha\). Let \(\psi_{1}(x)\equiv\alpha\). By part 3, \(\beta_{\phi}(\theta)\leq\alpha\) for \(\theta\in(-\infty,\theta_{1})\cup(\theta_{2},\infty)\). Hence \(\phi\) has size \(\alpha\). \(\Box\)

Figure 3.3 illustrates the relation between the power functions of \(\phi\) and \(\psi\) that appear in Theorem 3.5. This behavior is similar to that mentioned in Corollary 3.1.

#### 3.4.6 UMPU tests for hypotheses I and II

Let us now turn to the UMPU tests for hypotheses I and II. First, consider hypothesis II.

**Theorem 3.6**: _Suppose that the density of \(X\) belong to the exponential family (3.13). Let \(\theta_{1}<\theta_{2}\) and \(0<\alpha<1\). Then_(1) There exists a test of the form_

\[\phi(x)=\begin{cases}1&\text{if}\;\;Y(x)<t_{1}\;\;\text{or}\;\;Y(x)>t_{2}\\ \gamma_{i}&\text{if}\;\;Y(x)=t_{i},\;i=1,2\\ 0&\text{if}\;\;t_{1}<Y(x)<t_{2},\end{cases} \tag{3.33}\]

_where \(-\infty<t_{1}<t_{2}<\infty\) that satisfies_

\[E_{\theta_{1}}\phi(X)=E_{\theta_{2}}\phi(X)=\alpha.\]

_(2) Any test \(\phi\) that satisfies the conditions in part 1 is a UMPU-\(\alpha\) for testing \(H_{0}:\theta_{1}\leq\theta\leq\theta_{2}\) versus \(H_{1}:\theta<\theta_{1}\) or \(\theta>\theta_{2}\)._

Proof: (1) Let \(\phi^{0}\) be a test that satisfies the conditions in part 1 of Theorem 3.5 with \(\alpha\) in (3.30) replaced by \(1-\alpha\). Then \(\phi=1-\phi^{0}\) has the desired form.

(2) Let \(\psi\) be any unbiased test of size \(\alpha\). Because the power function \(\beta_{\psi}(\theta)\) is continuous, we have \(\beta_{\psi}(\theta_{1})=\beta_{\psi}(\theta_{2})=\alpha\). Let \(\psi^{0}=1-\psi\). Then \(\beta_{\psi^{0}}(\theta_{i})=1-\alpha\), \(i=1,2\). Let \(\phi\) be a test that satisfies the conditions in part 1. Then \(\phi^{0}\) is a test that satisfies the conditions in part 1 of Theorem 3.5 with \(\alpha\) in (3.30) replaced by \(1-\alpha\). By Theorem 3.5, \(\beta_{\phi^{0}}(\theta)\leq\beta_{\psi^{0}}(\theta)\) for all \(\theta\in\Theta_{1}\) and \(\beta_{\phi^{0}}(\theta)\geq\beta_{\psi^{0}}(\theta)\) for all \(\theta\in\Theta_{0}\). Therefore \(\beta_{\phi}(\theta)\geq\beta_{\psi}(\theta)\) for all \(\theta\in\Theta_{1}\) and \(\beta_{\phi}(\theta)\leq\beta_{\psi}(\theta)\) for all \(\theta\in\Theta_{0}\). Thus \(\phi\) is a UMPU-\(\alpha\) test. \(\Box\)

Next, consider hypothesis I.

**Theorem 3.7**_Suppose that the density of \(X\) belongs to the exponential family (3.13). Let \(\theta_{0}\in\Theta^{0}\) and \(0<\alpha<1\). Then_

_(1) There is a test \(\phi\) of the form (3.33) such that_

\[E_{\theta_{0}}\phi(X)=\alpha\;\;\text{and}\;\;E_{\theta_{0}}[Y(X)\phi(X)]= \alpha E_{\theta_{0}}Y(X). \tag{3.34}\]

Figure 3.3: Comparison of the power functions

_._
2. _Any test_ \(\phi\) _that satisfies the conditions in part_ 1 _is a UMPU-_\(\alpha\) _test for testing_ \(H_{0}:\theta=\theta_{0}\) _versus_ \(H_{1}:\theta\neq\theta_{0}\)_._

Proof.: (1) By Lemma 3.10 there is a \(\phi^{0}\) of the form (3.27) with \(-\infty\leq t_{1}<t_{2}\leq\infty\) such that (3.34) holds with \(\alpha\) replace by \(1-\alpha\). Exclude the possibilities of \(t_{1}=-\infty\) and \(t_{2}=\infty\) using the similar argument as in the proof of part 1 of Theorem 3.5. Then \(\phi=1-\phi^{0}\) is the desired test.

(2) Suppose \(\theta<\theta_{0}\). By the generalized Neyman-Pearson lemma, any test having the form

\[\phi^{\prime}(x)=\begin{cases}1&f_{\theta}(x)>k_{1}f_{\theta_{0}}(x)+k_{2}\dot {f}_{\theta_{0}}(x)\\ 0&f_{\theta}(x)<k_{1}f_{\theta_{0}}(x)+k_{2}\dot{f}_{\theta_{0}}(x)\end{cases} \tag{3.35}\]

that satisfies \(\beta_{\phi^{\prime}}(\theta_{0})=\alpha\) and \(\dot{\beta}_{\phi^{\prime}}(\theta_{0})=0\) has maximum power at \(\theta\) out of all tests \(\psi\) satisfying \(\beta_{\psi}(\theta_{0})=\alpha\) and \(\dot{\beta}_{\psi}(\theta_{0})=0\). Thus it has maximum power at \(\theta\) out of all unbiased tests of size \(\alpha\). We now show that any \(\phi\) that satisfies the conditions in part 1 is of the above form for some \(k_{1}\) and \(k_{2}\).

Let \(a_{1},a_{2}\) be the solution to the system of equations

\[a_{1}+a_{2}t_{i}=e^{(\theta-\theta_{0})t_{i}},\ i=1,2.\]

Then,

\[a_{1} =\big{(}t_{2}e^{(\theta-\theta_{0})t_{1}}-t_{1}e^{(\theta-\theta_{ 0})t_{2}}\big{)}/(t_{2}-t_{1})\] \[a_{2} =\big{(}e^{(\theta-\theta_{0})t_{2}}-e^{(\theta-\theta_{0})t_{1}} \big{)}/(t_{2}-t_{1}).\]

Clearly \(a_{1}>0>a_{2}\) and hence as in the proof of Theorem 3.5, it follows that

\[a_{1}+a_{2}t\begin{cases}<e^{(\theta-\theta_{0})t}&\text{if $t<t_{1}$ or $t>t_{2}$}\\ >e^{(\theta-\theta_{0})t}&\text{if $t_{1}<t<t_{2}$.}\end{cases}\]

From the exponential family form (3.13) it can be deduced that there exist \(k_{1}\), \(k_{2}\) such that

\[f_{\theta}(x)>k_{1}f_{\theta_{0}}(x)+k_{2}\dot{f}_{\theta_{0}}(x)\Leftrightarrow a_{1}+a_{2}Y(x)<e^{(\theta-\theta_{0})Y(x)}\] \[f_{\theta}(x)<k_{1}f_{\theta_{0}}(x)+k_{2}\dot{f}_{\theta_{0}}(x) \Leftrightarrow a_{1}+a_{2}Y(x)>e^{(\theta-\theta_{0})Y(x)}.\]

It follows then that

\[f_{\theta}(x)>k_{1}f_{\theta_{0}}(x)+k_{2}\dot{f}_{\theta_{0}}(x) \Leftrightarrow Y(x)<t_{1}\text{ or }Y(x)>t_{2}\] \[f_{\theta}(x)<k_{1}f_{\theta_{0}}(x)+k_{2}\dot{f}_{\theta_{0}}(x) \Leftrightarrow t_{1}<Y(x)<t_{2}\]

Thus \(\phi\) is of the form (3.35). A similar result holds when \(\theta>\theta_{0}\). Thus we see that, for any unbiased test of size \(\alpha\), \(E_{\theta}\phi(X)\geq E_{\theta}\psi(X)\) for all \(\theta\). Finally, since the null space \(\Theta_{0}\) is the singleton \(\{\theta_{0}\}\), \(\phi\) has size \(\alpha\). This completes the proof.

Theorems 3.5 and 3.6 give the general forms of the UMP or UMPU tests for the two sided hypothesis. In practice, all we are left to determine are the constants \(t_{1},t_{2},\gamma_{1},\gamma_{2}\). For hypotheses III and II, these can be determined by

\[E_{\theta_{1}}\phi(X)=E_{\theta_{2}}\phi(X)=\alpha.\]

For hypothesis I, they can be determined by (3.34). If \(Y(X)\) has a continuous distribution, then the values of \(\gamma_{i}\) are unimportant, and we usually take them to be \(0\) or \(1\). If discrete \(Y(X)\), \(t_{1}\) and \(t_{2}\) are first determined so that the solution for \(\gamma_{1}\) and \(\gamma_{2}\) are between \(0\) and \(1\). Then \(\gamma_{1}\) and \(\gamma_{2}\) are determined. Typically numerical calculations are involved. The following example illustrate the procedure.

**Example 3.8** Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(N(\theta,1)\) random variables. The joint density of \(X_{1},\ldots,X_{n}\) is of the form (3.13), with \(Y(X_{1},\ldots,X_{n})=\sum_{i=1}^{n}X_{i}\). For hypotheses III and II, \(t_{1}\) and \(t_{2}\) in (3.27) and (3.33) are determined by the following equations

\[P_{\theta_{i}}(Y<t_{1})+P_{\theta_{i}}(Y>t_{2})=\alpha,\ i=1,2.\]

Since, under \(\theta\), \(Y\sim N(n\theta,n)\). The above equations reduce to

\[\Phi\left(\frac{t_{1}-n\theta_{i}}{\sqrt{n}}\right)+1-\Phi\left(\frac{t_{2}-n \theta_{i}}{\sqrt{n}}\right)=\alpha,\ i=1,2,\]

where \(\Phi\) denotes the c.d.f. of a standard normal random variable. For example, if \(n=10\), \(\theta_{1}=0\), \(\theta_{2}=1\), and \(\alpha=0.05\). Then \((t_{1},t_{2})\) is the solution to the following system of equations

\[\begin{cases}\Phi(t_{1}/\sqrt{10})-\Phi(t_{2}/\sqrt{10})+0.95=0\\ \Phi((t_{1}-10)/\sqrt{10})-\Phi((t_{2}-10)/\sqrt{10})+0.95=0\end{cases}\]

We can either solve this equation directly by, for example, the Newton-Raphson algorithm or simplify this equation using the specific symmetric structure of this problem. Note that \(N(0,10)\) is symmetric about \(0\) and \(N(10,10)\) is symmetric about \(10\), and the two distributions have the same variance. Therefore \(t_{1}\) and \(t_{2}\) are symmetrically placed about \(t=5\). In other words \(t_{1}=5-t_{0}\) and \(t_{2}=5+t_{0}\) for some \(t_{0}\). Thus the above system of two equations reduce the following equation

\[\Phi\left(\frac{5-t_{0}}{\sqrt{10}}\right)-\Phi\left(\frac{5+t_{0}}{\sqrt{10}} \right)+0.95=0.\]

Solving this equation numerically we find \(t_{0}\approx 10.18\). Thus, for testing III, we reject the \(H_{0}\) if \(-5.18<Y<15.18\), and for testing II, we reject \(H_{0}\) if \(Y\) falls outside this region.

For testing I, \(t_{1}\) and \(t_{2}\) are determined by solving equation (3.34), which is \(E_{\theta_{0}}(Y)=n\theta_{0}\) in this example. So (3.34) reduces to

\[\begin{split}&\Phi\left(\frac{t_{1}-n\theta_{0}}{\sqrt{n}}\right)+1 -\Phi\left(\frac{t_{2}-n\theta_{0}}{\sqrt{n}}\right)=\alpha\\ &\int_{-\infty}^{t_{1}}t\frac{1}{\sqrt{n}}\varphi\left(\frac{t-n \theta_{0}}{\sqrt{n}}\right)dt+\int_{t_{2}}^{\infty}t\frac{1}{\sqrt{n}}\varphi \left(\frac{t-n\theta_{0}}{\sqrt{n}}\right)dt=\alpha n\theta_{0},\end{split} \tag{3.36}\]

where, in the second equation, \(\varphi\) denotes the p.d.f. of a standard normal random variable.

Again, one can either solve these equations directly by a numerical methods or further explore the symmetric structure specific to this problem. Note that the second equation is equivalent to \(\text{cov}_{\theta_{0}}(\phi,Y)\). Since \(\phi\) is a function of \(Y\) we will write it as \(\phi(Y)\). Since the distribution is symmetric about \(n\theta_{0}\), this covariance is \(0\) if \(\phi\) is symmetric about \(n\theta_{0}\). Thus, if we take \(t_{1}=n\theta_{0}-t_{0}\) and \(t_{2}=n\theta_{0}+t_{0}\) for some \(t_{0}>0\) then the second equation is automatically satisfied. Thus all we need to solve is the equation

\[\Phi\left(\frac{-t_{0}}{\sqrt{n}}\right)+1-\Phi\left(\frac{t_{0}}{\sqrt{n}} \right)=\alpha\]

The solution to this equation is \(t_{0}=\sqrt{n}\Phi^{-1}(1-\alpha/2)\). That is, we reject the hypothesis \(H_{0}\) in I if

\[Y>n\theta_{0}+\sqrt{n}\Phi^{-1}(1-\alpha/2)\ \ \text{or}\ \ Y>n\theta_{0}- \sqrt{n}\Phi^{-1}(1-\alpha/2).\]

For example, if \(\theta_{0}=1\), \(n=10\), \(\alpha=0.05\). Then we reject the null hypothesis in I if \(Y<3.80\) or \(Y>16.20\). \(\Box\)

**Example 3.9** Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. with p.d.f.

\[f_{\theta}(x)=\theta x^{\theta-1}I_{(0,1)}(x),\ \ \text{where}\ \theta>0,\]

and we are interested in testing hypothesis I with \(\theta_{0}=1\) and \(0<\alpha<1\). The joint p.d.f. of \(X_{1},\ldots,X_{n}\) belongs to the exponential family, and is given by

\[\theta^{n}e^{(\theta-1)Y(x_{1},\ldots,x_{n})},\ \ \text{where}\ \ Y(x_{1}, \ldots,x_{n})=\sum_{i=1}^{n}\log x_{i}.\]

Let us derive the distribution of \(Y\) under \(\theta_{0}=1\). We know that \(Y=Y_{1}+\cdots+Y_{n}\), where \(-Y_{i}\) are i.i.d. \(\text{Exp}(1)\). Therefore \(-Y_{1}-\cdots-Y_{n}=-Y\) is distributed as \(\text{Gamma}(n,1)\). So \(Y\) has p.d.f.

\[(-t)^{n-1}e^{t}/\Gamma(n),\ \ t<0.\]

Since \(Y\) has a continuous distribution, we can ignore \(\gamma_{1}\) and \(\gamma_{2}\). The constants \(t_{1}\), \(t_{2}\) in (3.33) are determined by \[\int_{-\infty}^{t_{1}}t^{n-1}e^{t}dt+\int_{t_{2}}^{0}t^{n-1}e^{t}dt=(-1)^{n-1} \Gamma(n)\alpha,\] \[\int_{-\infty}^{t_{1}}(-t)^{n-1}te^{t}dt+\int_{t_{2}}^{0}(-t)^{n-1 }te^{t}dt=\alpha\int_{-\infty}^{0}(-t)^{n-1}te^{t}dt.\]

Let \(\Gamma_{n}\) denote the cumulative distribution of a Gamma\((n,1)\) random variable. The above equations can now be represented as

\[\Gamma_{n}(-t_{1})+1-\Gamma_{n}(-t_{2})=\alpha\] \[\Gamma_{n+1}(-t_{1})+1-\Gamma_{n+1}(-t_{2})=\alpha.\]

where \(t_{1}<t_{2}<0\). One can then use a numerical method to find \(t_{1}\) and \(t_{2}\). For example, if \(n=10\), \(\alpha=0.05\), then \((t_{1},t_{2})\approx(-17.61,-4.98)\). \(\Box\)

### Problems

**3.1.** Let \((\Omega,\mathcal{F},P)\) be a probability space space and let \(X:\Omega\to[a,b]\) be a random variable on \((\Omega,\mathcal{F},P)\). Let \(A\in\mathcal{F}\). Use the continuity of probability measure to show that \(\rho(x)=P(\{X>x\}\cap A)\) is a right continuous function with \(\rho(x-)=P(\{X\geq x\}\cap A)\). Show that for any \(\alpha\in[\rho(a-),\rho(b)]\) there is an \(x_{0}\in[a,b]\) such that

\[P(\{X>x_{0}\}\cap A)\leq\alpha\leq P(\{X\geq x_{0}\}\cap A).\]

Here, the set \(A\) plays the role of \(\{f_{0}>0\}\) in the proof of Lemma 3.1.

**3.2.** Let \(f\) and \(g\) be measurable functions on \((\Omega,\mathcal{F},\mu)\). Let \(A\in\mathcal{F}\). We say that \(f=0\) a.e. \(\mu\) on \(A\) if \(\mu(\{f\neq 0\}\cap A)=0\). Show that \(fg=0\) a.e. \(\mu\) implies \(f>0\) a.e. \(\mu\) on \(\{g\neq 0\}\).

**3.3.** Let \(f_{0}\) and \(f_{1}\) denote densities of uniform distribution on (0,1) and \(\left(\frac{1}{2},\frac{3}{2}\right)\) respectively. Find the most powerful test for \(H_{0}:f_{0}\) versus \(H_{1}:f_{1}\) for each \(\alpha\in[0,1]\).

**3.4.** Let \(X\) be a \(b(2,q)\) (binomial) random variable and \(f_{0}\) and \(f_{1}\) denote the probability mass functions corresponding to \(b(2,\frac{1}{2})\) and \(b(2,\frac{2}{3})\). Find the most powerful test for \(H_{0}:q=\frac{1}{2}\) versus \(H_{1}:q=\frac{2}{3}\) for each \(\alpha\in[0,1]\).

**3.5.** Suppose \(X\) is a random variable defined on \((0,\infty)\). Find the most powerful test for the hypothesis

\[H_{0}:f_{0}(x)=\frac{1}{2}e^{-x/2}\quad\text{versus}\quad H_{1}:f_{1}(x)=e^{-x}\]

for each significance level \(\alpha\in[0,1]\).

**3.6.** Let \(X_{1},\ldots,X_{n}\) be an i.i.d. sample from the density \(f_{\theta}(x)\). Construct UMPs test of size \(\alpha\) when \(f_{\theta}(x)\) takes the following forms.

1. \(f_{\theta}(x)=\theta^{-1}e^{-x/\theta},\quad x>0,\ \ \theta>0\);

2. \(f_{\theta}(x)=\theta x^{\theta-1},\quad 0<x<1,\ \ \theta>0\);

3. \(f_{\theta}(x)=\frac{1}{\sqrt{2\pi\theta}}\,\exp\Big{\{}-\frac{1}{2\theta}(x-1)^ {2}\Big{\}},\ -\infty<x<\infty,\ \theta>0\);

4. \(f_{\theta}(x)=4\theta^{-4}x^{3}e^{-(x/\theta)^{4}},\quad x>0,\ \ \theta>0\).

**3.7.** Suppose \(X\) has density

\[f(x|\theta)=c(\theta)h(x)e^{\theta x},\ \ \theta\in\Theta\]

with respect to a measure \(\mu\). Here \(\Theta\) is an open interval and \(c(\theta)>0\) for all \(\theta\in\Theta\). Now let \(\theta\in\Theta\). Show that there is an interval \((-a,a)\) on which the moment generating function \(M_{X}(t)=E_{\theta}\left(e^{tX}\right)\) is finite, and, furthermore,

\[M_{X}(t)=c(\theta)/c(\theta+t).\]

**3.8.** Suppose that \(Y\) has density:

\[f_{\theta}(y)=c(\theta)h(y)e^{\eta(\theta)y}\]

for some one-to-one differentiable function \(\eta\). Let \(\phi(t)\) a function of \(t\). Show that

\[(\partial/\partial\theta)E_{\theta}\phi(Y)=\dot{\eta}(\theta){\rm cov}_{\theta} (\phi(Y),Y),\]

where \(\dot{\eta}(\theta)\) is the derivative of \(\eta\) with respect to \(\theta\).

**3.9.** Suppose \(X\) is a random variable having density \(f_{\theta}\) where \(\theta\in\Theta\subseteq\mathbb{R}\). Let \(\pi\) be a probability measure defined on \(\Theta\). For a \(\theta_{0}\in\Theta\), we are interested in testing the hypotheses \(H_{0}:\theta=\theta_{0}\) against the alternative \(H_{1}:\theta\) is distributed as \(\ \pi\). Here, we have treated \(\theta\) as random. Let us say that a test \(\phi\) is best of size \(\alpha\) if (i) \(E_{\theta_{0}}\phi(X)=\alpha\), and (ii) for any other test \(\phi^{\prime}\) satisfying \(E_{\theta_{0}}\phi^{\prime}(X)\leq E_{\theta_{0}}\phi(X)\) we have

\[E_{\pi}E_{\theta}\phi(X)\geq E_{\pi}E_{\theta}\phi^{\prime}(X),\]

where, for example, \(E_{\pi}E_{\theta}\phi(X)\) is the integral \(\int_{\Theta}E_{\theta}\{\phi(X)\}\pi(\theta)d\theta\). Show that any test of the form:

\[\phi(x)=\begin{cases}1&\text{if}\ \ \int_{\Theta}f_{\theta}(x)\pi(\theta)d \theta>kf_{\theta_{0}}(x)\\ 0&\text{if}\ \ \int_{\Theta}f_{\theta}(x)\pi(\theta)d\theta<kf_{\theta_{0}}(x) \end{cases}\]

for some \(k>0\), is best of its size.

[MISSING_PAGE_EMPTY:5939]

1. \(\int\phi_{1}fd\mu\geq\int\phi_{2}fd\mu\)
2. For some \(k\geq 0\), \(\phi_{1}\geq\phi_{2}\) whenever \(g>kf\) and \(\phi_{1}\leq\phi_{2}\) whenever \(g<kf\).

Show that \(\int\phi_{1}gd\mu\geq\int\phi_{2}gd\mu\).

Show that the logistic distribution with location parameter \(\theta\), having density

\[f(x|\theta)=\frac{e^{x-\theta}}{(1+e^{x-\theta})^{2}},\ \ x\in\mathbb{R},\ \ \theta\in\mathbb{R},\]

has monotone likelihood ratio. Write down the general form of UMPU-\(\alpha\) test for testing \(H_{0}:\theta<\theta_{0}\) against \(H_{1}:\theta\geq\theta_{0}\).

Let \(X_{i}\) be independent and distributed as \(N(i\theta,1)\), \(i=1,...,n\). Show that there exists a UMP test of \(H_{0}:\theta\leq 0\) against \(H_{1}:\theta>0\), and determine the test for a given \(\alpha\).

Suppose that \(F\) is a cdf and \(0<\omega<1\). Show that

\[\inf\{y:F(y)\geq\omega\}=\sup\{y:F(y)<\omega\}.\]

So \(F^{-1}(\omega)\) can be equivalently defined as either side of this equality. Give an example in which

\[\inf\{y:F(y)\geq\omega\}\neq\inf\{y:F(y)>\omega\}\]

for some \(\omega\).

Let \(X\) be distributed as \(\text{Gamma}(\theta,1)\); That is, its p.d.f. is given by

\[f_{X}(x;\theta)=\frac{1}{\Gamma(\theta)}\,x^{\theta-1}\,e^{-x},\ \ x>0,\ \ \theta>0.\]

Suppose \(0<\alpha<1\).

i. Derive the UMP-\(\alpha\) test for \(H_{0}:\theta\leq 1\) versus \(H_{1}:\theta>1\).

ii. Derive the UMP-\(\alpha\) test for \(H_{0}:\theta\leq 1\) or \(\theta\geq 2\) versus \(H_{1}:1<\theta<2\).

iii. Derive the UMPU-\(\alpha\) test for \(H_{0}:\theta=1\) versus \(H_{1}:\theta\neq 1\).

Suppose \(X\) has a binomial distribution \(b(7,p)\). Let \(\alpha=0.20\). Construct the following tests:

i. The UMP-\(\alpha\) test for \(H_{0}:p\leq 0.5\) against \(H_{1}:p>0.5\);

ii. The UMPU-\(\alpha\) test for \(H_{0}:0.25\leq p\leq 0.75\) against \(H_{1}:p<0.25\) or \(p>0.75\).

iii. The UMPU-\(\alpha\) test for \(H_{0}:p=0.5\) against \(H_{1}:p\neq 0.5\).

**3.19.** Suppose that \(X\) is distributed as \(\mathrm{Exp}(\theta)\); that is,

\[f_{\theta}(x)=\frac{1}{\theta}e^{-x/\theta}I_{[0,\infty)}(x),\ \ \theta>0.\]

i. Find UMP-\(\alpha\) test for testing \(H_{0}:\theta\leq\theta_{0}\) versus \(H_{1}:\theta>\theta_{0}\).

ii. Find UMPU-\(\alpha\) test for testing \(H_{0}:\theta\in[\theta_{0},2\theta_{0}]\) versus \(H_{1}:\theta\notin[\theta_{0},2\theta_{0}]\).

iii. Find the UMPU-\(\alpha\) test for testing \(H_{0}:\theta=\theta_{0}\) versus \(H_{1}:\theta\neq\theta_{0}\).

**3.20.** Mary needs to interview 5 people. Suppose each person (independently) agrees to be interviewed with probability \(\theta\), and let \(X\) be the minimal number of people she needs to ask in order to obtain the 5 interviews she needs.

i. Derive the probability mass function of \(X\).

ii. Show that \(X\) has an exponential family distribution.

iii. Find the UMP-\(\alpha\) test for the hypothesis \(H_{0}:\theta\leq 1/2\) versus \(H_{1}:\theta>1/2\), where \(\alpha=0.05\).

**3.21.** Suppose that \(X\) is a random variable with density belonging to a parametric family \(\{f_{\theta}:\theta\in\Theta\}\), where \(\Theta\subset\mathbb{R}\). Suppose that this family has monotone likelihood ratio with respect to \(Y(X)\). We are interested in testing the one-sided hypothesis \(H_{0}:\theta\leq\theta_{0}\) versus \(H_{1}:\theta>\theta_{0}\). Suppose that \(\phi_{1}\) and \(\phi_{2}\) are two tests satisfying the following conditions:

a. They are both functions of a statistic \(Y(X)\) and they are monotone non-decreasing in \(Y(X)\);

b. \(E_{\theta_{0}}\phi_{1}(Y(X))=E_{\theta_{0}}\phi_{2}(Y(X))=\alpha\);

c. There is a \(k\) such that whenever \(Y(x)>k\), \(\phi_{1}(Y(x))\geq\phi_{2}(Y(x))\) and whenever \(Y(x)<k\), \(\phi_{1}(Y(x))\leq\phi_{2}(Y(x))\).

d. The family \(\{f_{\theta}:\theta\in\Theta\}\) has a common support; that is, \(\{x:f_{\theta}(x)>0\}\) is the same set for all \(\theta\in\Theta\).

Show that

i. \(\beta_{\phi_{1}}(\theta)\geq\beta_{\phi_{2}}(\theta)\) for all \(\theta\geq\theta_{0}\);

ii. \(\beta_{\phi_{1}}(\theta)\leq\beta_{\phi_{2}}(\theta)\) for all \(\theta\leq\theta_{0}\);

iii. Both tests are of size \(\alpha\);

iv. Both tests are unbiased.

## References

* Allen (1953) Allen, S. G. (1953). A class of minimax tests for one-sided composite hypothesis. _The Annals of Mathematical Statistics_. **24**, 295-298.
* Ferguson (1967) Ferguson, T. S. (1967). _Mathematical Statistics: A Decision Theoretic Approach_. Academic.

Karlin, S. and Rubin, H. (1956). The theory of decisin procedures for distributions with monotone likelihood ratio. _Annals of Mathematical Statistics_, **27**, 272-299.
* Lehmann and Romano (2005) Lehmann, E. L. and Romano, J. P. (2005). _Testing Statistical Hypotheses_. Third edition. Springer.
* Neyman and Pearson (1933) Neyman, J. and Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. _Philosophy Transaction of the Royal Society of London. Series A._**231**, 289-337.
* Neyman and Pearson (1936) Neyman, J. and Pearson, E. S. (1936). Contributions to the theory of testing statistical hypothesis. _Statistical Research Memois_, **1**, 1-37.
* Pfanzagl (1967) Pfanzagl, J. (1967). A technical lemma for monotone likelihood ratio families. _The Annals of Mathematical Statistics_, **38**, 611-612.

## 4 Testing Hypotheses in the Presence of Nuisance Parameters

In this chapter we consider the hypothesis tests where more than one parameter is involved. Let \(\mathcal{P}=\{P_{\theta}:\theta\in\Theta\}\) be a parametric family of distributions of a random vector \(X\), where \(\Theta\) is a subset of \(\mathbb{R}^{k}\). The families \(\mathcal{P}_{0}\) and \(\mathcal{P}_{1}\) are \(\{P_{\theta}:\theta\in\Theta_{0}\}\) and \(\{P_{\theta}:\theta\in\Theta_{1}\}\) where \(\{\Theta_{0},\Theta_{1}\}\) is a partition of \(\Theta\). Thus the type of hypotheses we are concerned with is

\[H_{0}:\theta\in\Theta_{0}\quad\text{vs}\quad H_{1}:\theta\in\Theta_{1}.\]

More specifically, we will discuss how to construct test for one of the \(k\) parameters that has optimal power for that parameter and all the rest of the parameters. Without loss of generality, we assume that component to be the first component, \(\theta_{1}\), of \(\theta\). For example, a typical hypothesis we will consider in this chapter is

\[H_{0}:\theta_{1}=a\quad\text{vs}\quad H_{1}:\theta_{1}\neq a. \tag{4.1}\]

where \(a\) is a specific value of \(\theta_{1}\). In this setting

\[\Theta_{0}=\{\theta\in\Theta:\theta_{1}=a\},\quad\Theta_{1}=\{\theta\in\Theta: \theta_{1}\neq a\}.\]

Our goal is to find a test that maximizes the power over all \(\Theta_{1}\) among a reasonably wide class of tests. The component of \(\theta\) to be tested is called the parameter of interest; the rest of the components are called the nuisance parameters. Tests such as (4.1) are called hypothesis tests in the presence of nuisance parameters.

Because of the uniform nature of this optimality, it is possible only under rather restrictive conditions: unlike in the single parameter case, even the one-sided hypothesis does not in general permit a UMP test. Thus in this chapter we will be dealing with UMPU tests.

In our exposition we will frequently need to divide a vector \((c_{1},c_{2},\ldots,c_{p})\) as its first component \(c_{1}\) and the rest of the components \((c_{2},\ldots,c_{p})\). To simplify notation we use \(c_{2:p}\) to represent \((c_{2},\ldots,c_{p})\).

For more information on this topic, see Ferguson (1967); Lehmann and Casella (1998).

### Unbiased and Similar tests

In Section 3.4.1 we have already noticed that, if \(\phi\) is an unbiased test and if the function \(P\mapsto\beta_{\phi}(P)\) is continuous, then the power function \(\beta_{\phi}(P)\) is constant on the boundary \(\bar{\mathcal{P}}_{0}\cap\bar{\mathcal{P}}_{1}\). Under a parametric model the power function is \(\beta_{\phi}(P_{\theta})\), is abbreviated as \(\beta_{\phi}(\theta)\). The next proposition is the parametric counterpart of Proposition 3.2.

**Proposition 4.1**: _If \(\beta_{\phi}(\theta)\) is continuous in \(\theta\) and \(\phi\) is unbiased then \(\beta_{\phi}(\theta)\) is constant on \(\Theta_{B}=\bar{\Theta}_{0}\cap\bar{\Theta}_{1}\)._

Thus, under the continuity of the power function, the class of all tests whose powers are constant on \(\Theta_{B}\) contains the class of unbiased test. This means if we can find UMP test among the latter class, then we can find the UMP test among unbiased tests.

**Definition 4.1**: _A test \(\phi\) satisfying \(\beta_{\phi}(\theta)=\alpha\) on \(\Theta_{B}\) is an \(\alpha\)-similar test. A test \(\phi\) is called uniformly most powerful \(\alpha\)-similar (UMP \(\alpha\)-similar) test for testing (4.1), if_

_1. \(\phi\) is \(\alpha\)-similar;_

_2. for any \(\alpha\)-similar test \(\psi\), \(\beta_{\phi}(\theta)\geq\beta_{\psi}(\theta)\) for all \(\theta\in\Theta_{1}\)._

If we let \(\mathcal{U}_{\alpha}\) be the class of all unbiased tests of size \(\alpha\), and \(\mathcal{S}_{\alpha}\) be the class of all \(\alpha\)-similar tests. By Proposition 4.1, if \(\beta_{\phi}\) is continuous for all \(\phi\), then \(\mathcal{U}_{\alpha}\subseteq\mathcal{S}_{\alpha}\). Hence, if \(\phi\) is UMP \(\alpha\)-similar, then it is also UMPU test of size \(\alpha\) as long as we can guarantee \(\phi\) itself is in \(\mathcal{U}_{\alpha}\). This is proved in the next theorem.

**Theorem 4.1**: _If the power function is continuous in \(\theta\) for every test, and if \(\phi\) is UMP \(\alpha\)-similar test for testing (4.1) with size \(\alpha\), then \(\phi\) is a UMPU-\(\alpha\) test._

Proof: A UMP \(\alpha\)-similar test \(\phi\) is unbiased because the power \(\beta_{\phi}(\theta)\geq\beta_{\psi}(\theta)=\alpha\) for all \(\theta\in\Theta_{1}\), for the \(\alpha\)-similar test \(\psi(x)\equiv\alpha\). By assumption, \(\phi\) has size \(\alpha\). Therefore \(\phi\in\mathcal{U}_{\alpha}\). \(\Box\)

In Section 2.2, we introduced the notions of sufficient, complete, and boundedly complete statistics. In the parametric context, it takes the following form. A statistic \(T\) is sufficient for a subset \(A\subseteq\Theta\) if, for any \(B\in\mathcal{F}_{X}\), there is a function \(\kappa_{B}\) such that

\[P_{\theta}(B|T)=\kappa_{B}\quad[P_{\theta}]\quad\text{for all }\theta\in A.\]

A statistic \(T\) is complete for \(A\) if, for any \(\sigma(T)\)-measurable function \(g\),

\[\int gdP_{\theta}=0\quad\text{for all }\theta\in A\ \Rightarrow\ g=0\ [P_{\theta}]\ \text{for all }\theta\in A. \tag{4.2}\]A statistic \(T\) is boundedly complete for \(A\) if the above implication holds for all bounded \(\sigma(T)\)-measurable function \(g\).

The basic idea underlying the construction of UMPU tests in the presence of nuisance parameters is the following. Let \(\Lambda_{1}\) denote the parameter space of \(\theta_{1}\); that is,

\[\Lambda_{1}=\{\theta_{1}:(\theta_{1},\theta_{2:p})\in\Theta\}.\]

Suppose we want to test the hypotheses in (4.1) and suppose there is a sufficient statistic \(S\) for \(\theta_{2:p}\). Then the conditional distribution \(P_{\theta}(\cdot|S)\) depends only on \(\theta_{1}\). So let us write it as \(P_{\theta_{1}}(\cdot|S)\). If this conditional distribution belongs to a one-parameter exponential family, then we can construct UMP (or UMPU) tests for the one-parameter family

\[\{P_{\theta_{1}}(\cdot|S):\theta_{1}\in\Lambda_{1}\}.\]

using the mechanism studied in Chapter 3. However, the optimal power derived in this way is in terms of the conditional distribution \(P_{\theta_{1}}(\cdot|S)\), not the original unconditional distribution \(P_{\theta}\) of \(X\). To link this conditional optimality to the original unconditional optimality we assume that \(S\) is boundedly complete for \(\theta_{2:p}\), and employ the notion of Neyman structure, which in some sense aligns a conditional distribution with an unconditional distribution through bounded completeness.

To do so, we first introduce the concept of Neyman structure that is closely tied to bounded completeness. Let \(\alpha\) be a value in \((0,1)\), and \(T\) is a statistic.

**Definition 4.2**: _A test \(\phi(X)\) is said to have an \(\alpha\)-Neyman structure with respect to a statistic \(T\) if_

\[E_{\theta}(\phi(X)|T)=\alpha\]

_almost everywhere \(P_{\theta}\) for all \(\theta\in\Theta_{B}\)._

Obviously, if a test \(\phi\) has an \(\alpha\)-Neyman structure with respect to \(S\), then it is \(\alpha\)-similar, because, for all \(\theta\in\Theta_{B}\),

\[E_{\theta}\phi(X)=E_{\theta}E_{\theta}(\phi(X)|S)=\alpha.\]

The next theorem shows that, if \(S\) is sufficient and boundedly complete, then the reversed implication is true.

**Theorem 4.2**: _If \(S\) is sufficient and boundedly complete for \(\theta\in\Theta_{B}\), then every test \(\alpha\)-similar on \(\Theta_{B}\) has an \(\alpha\)-Neyman structure._

Proof: Let \(\phi\) be an \(\alpha\)-similar test. Then \(E_{\theta}(\phi(X))=\alpha\) for all \(\theta\in\Theta_{B}\), which implies

\[E_{\theta}\{E[\phi(X)|S]\}=\alpha\]for all \(\theta\in\Theta_{B}\). By sufficiency of \(S\) for \(\theta\in\Theta_{B}\), the conditional expectation \(E_{\theta}[\phi(X)|S]\) does not depend on \(\theta\in\Theta_{B}\). Hence we write \(E[\phi(X)|S]\) instead of \(E_{\theta}[\phi(X)|S]\) for \(\theta\in\Theta_{B}\). The above equality can be equivalently written as

\[E_{\theta}\{E[\phi(X)|S]-\alpha\}=0\]

for all \(\theta\in\Theta_{B}\). Since \(S\) is boundedly complete and since \(E[\phi(X)|X]-\alpha\) is a bounded function of \(S\), we have

\[E[\phi(X)|S]-\alpha=0\]

for all \(\theta\in\Theta_{B}\), which means that \(\phi\) has an \(\alpha\)-Neyman structure with respect to \(S\). 

Sometimes we encounter the situations where \(\Theta_{B}\) is the union of finite number of sets, and \(S\) is not complete sufficient for the whole boundary \(\Theta_{B}\) but rather for each member of the union. More specifically, for any \(a\in\Lambda_{1}\), let

\[\Theta(a)=\{\theta\in\Theta:\theta_{1}=a\}.\]

Let \(a_{1},\ldots,a_{s}\in\Lambda_{1}\). Suppose \(\Theta_{B}\) is of the form

\[\Theta_{B}=\cup_{r=1}^{s}\Theta(a_{r}) \tag{4.3}\]

and \(S\) is complete and sufficient relative to each \(\Theta(a_{r})\) but not necessarily on \(\Theta_{B}\). In this case the conclusion of Theorem 4.2 still holds, as shown in the following corollary.

**Corollary 4.1**: _If \(S\) is sufficient and boundedly complete for each \(\Theta(a_{r})\), \(r=1,\ldots,s\), then any test that is \(\alpha\)-similar on \(\Theta_{B}\) has an \(\alpha\)-Neyman structure with respect to \(S\)._

Proof.: Suppose \(\phi\) is \(\alpha\)-similar on \(\Theta_{B}\). Then it is \(\alpha\)-similar on each \(\Theta(a_{r})\). Because \(S\) is sufficient and complete on each \(\Theta(a_{r})\), by Theorem 4.2 we have

\[E_{\theta}[\phi(X)|S]=\alpha\]

almost everywhere \(P_{\theta}\) all for \(\theta\in\Theta(a_{r})\) and for all \(r=1,\ldots,s\). This means

\[E_{\theta}[\phi(X)|S]=\alpha \tag{4.4}\]

almost everywhere \(P_{\theta}\) for all \(\theta\in\Theta_{B}\). Hence \(\phi\) has an \(\alpha\)-Neyman structure with respect to \(S\). 

Note that the equality (4.4) justifies writing \(E_{\theta}[\phi(X)|S]\) as \(E[\phi(X)|S]\) in this setting as the former does not depend on \(\theta\) when \(\theta\) varies over \(\Theta_{B}\).

Thus, if we let \(\mathcal{N}_{\alpha}\) to be the class of all tests with \(\alpha\)-Neyman structure, then we have the following relations among the classes of unbiased tests of size \(\alpha\), \(\alpha\)-similar tests, and tests with \(\alpha\)-Neyman structures:1. \(\mathcal{U}_{\alpha}{\subseteq}\mathcal{S}_{\alpha}\) under condition (A),
2. \(\mathcal{S}_{\alpha}\subseteq\mathcal{N}_{\alpha}\) under condition (B),
3. \(\mathcal{N}_{\alpha}\subseteq\mathcal{S}_{\alpha}\),

where (A) and (B) are the conditions:

1. the power function \(\beta_{\phi}(\theta)\) is continuous for all tests \(\phi\);
2. the set \(\Theta_{B}\) is the union (4.3), and \(S\) is sufficient and boundedly complete for each \(\Theta(a_{r})\).

Thus, under conditions (A) and (B), \(\mathcal{U}_{\alpha}\subseteq\mathcal{N}_{\alpha}=\mathcal{S}_{\alpha}\). It turns out that finding the UMPU test among \(\mathcal{N}_{\alpha}\) is inherently a one-parameter problem. In the next section we investigate under what circumstances does there exist a sufficient and boundedly complete statistic \(S\) for \(\theta_{2:p}\).

### 4.2 Sufficiency and completeness for a part of the parameter vector

In section 2.1.3 we introduced the exponential family for a single random variable \(X\). We now extend it to multiple random variables. Let \(X=(X_{1},\ldots,X_{n})\), where \(X_{1},\ldots,X_{n}\) are i.i.d., with each \(X_{i}\) having its distribution in \(\mathfrak{E}_{p}(t_{0},\mu_{0})\), where \(t_{0}:\Omega_{X_{1}}\to\mathbb{R}^{p}\) and \(\mu_{0}\) is a \(\sigma\)-finite measure on \(\Omega_{X_{1}}\), a subset of \(\mathbb{R}\). For brevity, we write this as

\[X\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0}).\]

The joint density of \((X_{1},\ldots,X_{n})\) with respect to the product measure \(\mu=\mu_{0}\times\cdots\times\mu_{0}\) is

\[\prod_{i=1}^{n}e^{\theta^{T}t_{0}(x_{i})}/\int e^{\theta^{T}t_{0} (x_{i})}d\mu_{0}(x_{i})\] \[= e^{\theta^{T}\sum_{i=1}^{n}t_{0}(x_{i})}/[\int e^{\theta^{T}t_{0 }(x_{i})}d\mu_{0}(x_{i})]^{n}.\]

Let

\[t(x_{1},\ldots,x_{n})=\sum_{i=1}^{n}t_{0}(x_{i}). \tag{4.5}\]

Then the joint density of \((X_{1},\ldots,X_{n})\) with respect to \(\mu\) can be written as

\[e^{\theta^{T}t(x_{1},\ldots,x_{n})}/\int e^{\theta^{T}t(x_{1},\ldots,x_{n})}d \mu(x_{1},\ldots,x_{n}).\]

Using essentially the same proof as that of Theorem 2.8, we can establish the following theorem.

**Theorem 4.3**: _Suppose \((X_{1},\ldots,X_{n})\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\), and \(\Theta\) has a nonempty interior. Then \(t(X_{1},\ldots,X_{n})\) is complete and sufficient statistic for \(\Theta\)._For the rest of the section, the symbol \(X\) will be used to denote an i.i.d. sample \((X_{1},\ldots,X_{n})\).

An important property about the exponential family is that it is closed under conditioning and marginalization: that is, the conditional distributions and marginal distributions derived from the components of \(T\) also belong to the exponential family. This fact allows us to extend Theorem 2.8 the marginal and conditional distributions, so that we can speak of complete and sufficient statistics for a part of the parameter \(\theta\). This is crucial for reducing a multi-parameter problem to a one-parameter problem, so that we can use the results from Chapter 3 to tackle the new problems in this chapter.

We first introduce a lemma. Let \((U,V)\) be a pair of random vectors defined on \((\Omega_{U}\times\Omega_{V},\mathcal{F}_{U}\times\mathcal{F}_{V})\), and let \(P\) and \(Q\) be the two distributions of \((U,V)\) with \(P\ll Q\). Let \(P_{U}\) and \(P_{V|U}\) be the marginal distribution of \(U\) and conditional distribution of \(V|U\) under \(P\); let \(Q_{U}\) and \(Q_{V|U}\) be the marginal distribution of \(U\) and conditional distribution of \(V|U\) under \(Q\).

**Lemma 4.1**: _Let \(P\) be the probability measure defined by_

\[dP=a(u)b(v)dQ\]

_where \(a\) and \(b\) are nonnegative functions such that \(\int a(u)b(v)dQ(u,v)=1\). Then_

1. \(dP_{U}(u)=a(u)\left(\int b(v)dQ_{V|U}(v|u)\right)dQ_{U}(u)\)_,_
2. \(dP_{V|U}(v|u)=\frac{b(v)dQ_{V|U}(v|u)}{\int b(v^{\prime})dQ_{V|U}(v^{ \prime}|u)}\)_._

_Proof._ (a) By Theorem 1.21 we have

\[P^{\circ}U^{-1}\ll Q^{\circ}U^{-1},\quad d(P^{\circ}U^{-1})/d(Q^{\circ}U^{-1})= E[a(U)b(V)|U=u].\]

Hence

\[dP_{U}/dQ_{U}=a(u)E(b(V)|U),\]

which proves (a).

(b) Let \(B\in\mathcal{F}_{U}\times\mathcal{F}_{V}\). Let \(P^{*}\) be the measure defined by \(dP^{*}=I_{B}dP\). Then, by Theorem 1.21,

\[P^{*}\ll P,\quad P(B|U)=d(P^{*}{\circ}U^{-1})/d(P^{\circ}U^{-1}).\]

Also, by Theorem 1.21,

\[d(P^{*}{\circ}U^{-1}) = E_{Q}(I_{B}(U,V)a(U)b(V)|U)d(Q^{\circ}U^{-1})\] \[d(P^{\circ}U^{-1}) = E_{Q}(a(U)b(V)|U)d(Q^{\circ}U^{-1}).\]

So\[P(B|U)=\frac{E_{Q}(I_{B}(U,V)a(U)b(V)|U)}{E_{Q}(a(U)v(V)|U)}=\frac{E_{Q}(I_{B}(U,V )b(V)|U)}{E_{Q}(b(V)|U)}.\]

In particular, for any \(B\in{\cal F}_{V}\) we have

\[E(I_{B}(V)|U)=E(I_{\Omega_{U}\times B}(U,V)|U)=\ \frac{E_{Q}(I_{B}(V)b(V)|U)}{E_{Q}( b(V)|U)}.\]

Another way of writing this is

\[P_{V|U}(B|u)=\ \int_{B}\frac{b(v)}{E_{Q}(b(V)|U)}dQ_{V|U}(v|u),\]

which is the desired equality. \(\Box\)

We now apply this lemma to show that the marginal and conditional distributions associated with an exponential family remain to be from an exponential family. This result is important in establishing the Neyman Structure. Suppose \(t(X)\) is partitioned in \(t(X)=(u(X),v(X))\) and, correspondingly, \(\theta\) is partitioned into \((\eta,\xi)\), so that

\[\theta^{T}t(X)=\eta^{T}u(X)+\xi^{T}v(X).\]

Let \(U\), \(V\) denote the random vectors \(u(X)\) and \(v(X)\) and let \(u,v\) denote the specific values of \(U,V\). Recall that \(\mu\) is the \(n\)-fold product measure \(\mu_{0}\times\cdots\times\mu_{0}\).

**Theorem 4.4**: _Suppose \(X=(X_{1},\ldots,X_{n})\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\). Then:_

_(a) For each fixed \(\xi\), there is measure \(\mu_{\xi}\) on \((\Omega_{U},{\cal F}_{U})\) such that_

\[dP_{U}/d\mu_{\xi}=e^{\eta^{T}u}/\int e^{\eta^{T}u}d\mu_{\xi}(u)\]

_(b) For each fixed \(u\), there is a measure \(\mu_{u}\) on \((\Omega_{V},{\cal F}_{V})\) such that_

\[dP_{V|U}(v|u)/\mu_{u}=e^{\xi^{T}v}/\int e^{\xi^{T}v}dQ_{V|U}(v|u).\]

Proof.: (a). Let \(\nu=\mu\circ(u,v)^{-1}\). Then, the density of \((U,V)\) with respect to \(\nu\) is

\[f_{\theta}(u,v)=e^{\eta^{T}u+\xi^{T}v}/\int e^{\eta^{T}u+\xi^{T}v}d\nu(u,v).\]

Let \(\theta_{0}=(\eta_{0},\xi_{0})\in\Theta,\ \theta=(\eta,\xi)\in\Theta\). Define \(P\) and \(Q\) by

\[dQ(u,v)=f_{\theta_{0}}(u,v)d\nu(u,v),\quad dP(u,v)=f_{\theta}(u,v)d\nu(u,v).\]

Let \(Q_{U}\) and \(P_{U}\) be the marginal distributions of \(U\) under \(Q\) and \(P\), respectively, and let \(Q_{V|U}\) and \(P_{V|U}\) be the conditional distributions of \(V|U\) under \(Q\) and \(P\), respectively. Then,\[dP(u,v) =\,f_{\theta}(u,v)d\nu(u,v)\] \[=\,\frac{f_{\theta}(u,v)}{f_{\theta_{0}}(u,v)}f_{\theta_{0}}(u,v)d \nu(u,v)\] \[=\,c(\theta)e^{(\eta-\eta_{0})^{T}u}e^{(\xi-\xi_{0})^{T}v}dQ(u,v),\]

where

\[c(\theta)=\frac{\int e^{\eta_{0}^{T}u}+\xi_{0}^{T}v}d\nu(u,v)}{ \int e^{\eta^{T}u+\xi^{T}v}d\nu(u,v)}.\]

Then, by Lemma 4.1,

\[dP_{U}(u)=c(\theta)e^{\eta^{T}u}e^{-\eta_{0}^{T}u}\left(\int e^{( \xi-\xi_{0})^{T}v}dQ_{V|U}(v|u)\right)dQ_{U}(u).\]

The assertion of part (a) follows if we let \(c_{\xi}(\eta)=c(\theta)\) and

\[d\mu_{\xi}(u)=e^{-\eta_{0}^{T}u}\left(\int e^{(\xi-\xi_{0})^{T}v}dQ_{V|U}(v|u) \right)dQ_{U}(u).\]

(b) By Lemma 4.1 again,

\[dP_{V|U}(v|u) =\,\frac{e^{\xi^{T}v}e^{-\xi_{0}^{T}v}dQ_{V|U}(v|u)}{\int e^{(\xi -\xi_{0})^{T}v}dQ_{V|U}(v|u)}\] \[=\,\frac{e^{\xi^{T}v}}{\int e^{\xi^{T}v}dQ_{V|U}(v|u)}\frac{\int e ^{\xi^{T}v}dQ_{V|U}(v|u)e^{-\xi_{0}^{T}v}dQ_{V|U}(v|u)}{\int e^{(\xi-\xi_{0})^{ T}v}dQ_{V|U}(v|u)}.\]

Now let

\[d\mu_{u}(v)=\frac{\int e^{\xi^{T}v}dQ_{V|U}(v|u)e^{-\xi_{0}^{T}v}dQ_{V|U}(v|u) }{\int e^{(\xi-\xi_{0})^{T}v}dQ_{V|U}(v|u)}\]

to complete the proof. 

In the next few sections, we will be concerned with the case where \(\eta=\theta_{1}\) and \(\xi=\theta_{2:p}\). The above theorem, together with Theorem 2.8, implies that, when \(\theta_{1}\) is fixed at \(a\), the statistic \(t_{2:p}(X)\) is sufficient and complete for \(\theta_{2:p}\). For easy reference we summarize this result as a corollary. The proof follows directly from Theorem 4.4 and Theorem 2.8.

**Corollary 4.2**_Suppose \(X=(X_{1},\ldots,X_{n})\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\). Let \(a\) be a real number such that_

1. \((a,\theta_{2:p})\in\Theta\)_;_
2. _the set_ \(\{\theta_{2:p}:(a,\theta_{2:p})\in\Theta\}\) _has a nonempty interior in_ \(\mathbb{R}^{p-1}\)_Then \(t_{2:p}(X)\) is sufficient and complete for \(\Theta(a)\)._

We will frequently need to make assumptions similar to 2 in the corollary. To simplify discussion, let \(\Psi\) be the collection of all \(\theta_{1}\in\mathbb{R}\) for which there exists a \(\theta_{2:p}\in\mathbb{R}^{p-1}\) such that \((\theta_{1},\theta_{2:p})\) is an interior point of \(\Theta\). Obviously, if the interior of \(\Theta\) is nonempty, then \(\Psi\) is nonempty. Furthermore, it is easy to see that, for every \(\theta_{1}\in\Psi\), \(\{\theta_{2:p}:(\theta_{1},\theta_{2:p})\in\Theta\}\) has a nonempty interior in \(\mathbb{R}^{p-1}\).

### 4.3 UMPU tests in the presence of nuisance parameters

In this section we construct the UMPU tests for several hypothesis about \(\theta_{1}\) in the presence of the nuisance parameters \(\theta_{2:p}\). First, let us show that we can restrict our attention exclusively on those tests that depend on sufficient statistics so long as the purpose is to maximize the power.

**Lemma 4.2**: _Suppose \(T=t(X_{1},\ldots,X_{n})\) is sufficient for \(\Theta\). Then, for any test \(\phi(X)\), there exists a \(\psi\circ t(X)\) such that_

\[\beta_{\phi}(\theta)=\beta_{\psi\circ t}(\theta)\quad\text{for all}\;\;\theta \in\Theta. \tag{4.6}\]

Proof.: The test \(\psi\circ t(X)=E[\phi(X)|T]\) satisfies the asserted property. 

For the rest of this section, any test we consider will be a function of a sufficient statistic.

**Theorem 4.5**: _Suppose_

_1. \(T=t(X_{1},\ldots,X_{n})\) is a sufficient statistic for \(\Theta\),_

_2. \(\Theta_{B}\) can be written as \(\cup_{r=1}^{s}\Theta(a_{r})\) such that \(T_{2:p}\) is sufficient and complete on each \(\Theta(a_{r})\),_

_3. The power function \(\beta_{\phi}(\theta)\) is continuous in \(\theta\) for all tests \(\phi\)._

_Moreover, suppose that there is a test \(\phi_{0}(T)\) such that_

_4. \(\phi_{0}\) has \(\alpha\)-Neyman structure with respect to \(T_{2:p}\),_

_5. \(E_{\theta}(\phi_{0}(T)|T_{2:p})\leq\alpha\)\([P_{\theta}]\) for all \(\theta\in\Theta_{0}\),_

_6. For any test \(\phi(T)\) with \(\alpha\)-Neyman structure with respect to \(T_{2:p}\), we have_

\[E_{\theta}(\phi_{0}(T)|T_{2:p})\geq E_{\theta}(\phi(T)|T_{2:p})\quad[P_{\theta }]\quad\text{for all}\;\theta\in\Theta_{1}. \tag{4.7}\]

_Then \(\phi_{0}\) is a UMPU-\(\alpha\) test for testing \(H_{0}:\theta\in\Theta_{0}\) versus \(H_{1}:\theta\in\Theta_{1}\)._

Proof.: Let \(\phi(T)\) be an unbiased test of size \(\alpha\). By assumption 2 and Corollary 4.2, \(\phi\) has \(\alpha\)-Neyman structure with respect to \(T_{2:p}\). Hence, by assumption 6, (4.7) holds. Then

\[E_{\theta}(\phi_{0}(T))\geq E_{\theta}(\phi(T))\]

[MISSING_PAGE_EMPTY:5952]

\[E_{\theta_{1}}(\phi_{0}(T)|T_{2:p}=t_{2:p})\geq E_{\theta_{1}}(\phi(T)|T_{2:p}=t_{2: p})\quad\text{for all $\theta_{1}>a$.} \tag{4.11}\]

By construction, both \(\phi_{0}(T)\) and \(\phi(T)\) have \(\alpha\)-Neyman structure with respect to \(T_{2:p}\) for the set \(\Theta_{B}\). Moreover, note that any test with an \(\alpha\)-Neyman structure must be in the form of \(\phi(T)\) as constructed above. Thus conditions 4 and 6 in Theorem 4.5 are satisfied. By Theorem 3.2,

\[E_{\eta}(\phi_{0}(T)|T_{2:p}=t_{2:p})\leq\alpha\quad\text{for all $\theta_{1}\leq a$.}\]

This implies condition 5 in Theorem 4.5. To summarize, we have the following theorem.

**Theorem 4.6**: _Suppose_

_1. \(X\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\);_

_2. \(\Theta\) has a nonempty interior in \(\mathbb{R}^{p}\);_

_3. \(a\in\Psi\)._

_Then \(\phi_{0}\) in (4.10) is a UMPU-\(\alpha\) test for hypothesis (4.9)._

The next example illustrates how to construct a UMPU-\(\alpha\) test.

**Example 4.1** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\), and we are interested in testing

\[H_{0}:\mu\leq 0\quad\text{vs}\quad H_{1}:\mu>0. \tag{4.12}\]

Note that

\[f(x|\mu,\sigma^{2}) = \frac{1}{\sqrt{2\pi}\sigma}\exp\{-(x-\mu)^{2}/(2\sigma^{2})\}\] \[= \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^{2}}{2\sigma^{2}} \right)\exp\left(\frac{\mu}{\sigma^{2}}x-\frac{1}{2\sigma^{2}}x^{2}\right)\] \[= c(\theta)e^{\theta^{T}t_{0}(x)}\]

where \(\theta=(\theta^{1},\theta^{2})\), \(t_{0}(x)=(x,x^{2})\) and

\[\theta_{1}=\mu/\sigma^{2},\quad\theta_{2}=-1/(2\sigma^{2}).\]

The hypothesis (4.12) is equivalent to

\[H_{0}:\theta_{1}\leq 0\quad\text{vs}\quad H_{1}:\theta_{1}>0.\]

Let

\[T_{1}=\sum_{i=1}^{n}X_{i},\quad T_{2}=\sum_{i=1}^{n}X_{i}^{2}.\]

By Theorem 4.6, we should look for UMP-\(\alpha\) one sided test for the one-parameter exponential family \(\{f_{T_{1}|T_{2}}(t_{1}|t_{2};\mu):\mu\in\mathbb{R}\}\). That is \[\phi_{0}(t_{1},t_{2})=\begin{cases}1&\text{if}\ \ t_{1}>z(t_{2})\\ 0&\text{otherwise,}\end{cases} \tag{4.13}\]

where \(z(t_{2})\) is to be determined by \(E_{\theta_{1}=0}(\phi(T_{1})|T_{2}=t_{2})=\alpha\). Here and in what follows, the symbol \(E_{\theta=a}\) is used to denote \(E_{\theta}\) where \(\theta\) is evaluated at \(a\).

Later on in section 4.5 we will describe how to further simplify the test using the Basu's theorem (Theorem 2.6), but for now, let us carry through the principle provided by Theorem 4.5. For simplicity, consider the case \(n=2\). The critical point \(z(t_{2})\) is the solution to the equation

\[P_{\theta_{1}=0}(T_{1}>c|T_{2}=t_{2})=\alpha.\]

Here and in what follows we use the standard notation \(P_{\theta=a}\) to denote \(P_{\theta}\), where \(\theta\) is evaluated at \(a\). Under \(H_{0}\), \((X_{1},X_{2})\) is distributed as \(N(0_{2},\sigma^{2}I_{2})\), where \(0_{2}\) is the vector \((0,0)\) and \(I_{2}\) is the \(2\) by \(2\) identity matrix. Then, conditioning on \(X_{1}^{2}+X_{2}^{2}=t_{2}\), \((X_{1},X_{2})\) is uniformly distributed on the circle \(\{x:\|x\|=\sqrt{t_{2}}\}\). The inequality \(T_{1}\leq t_{1}\) corresponds to an arc on the circle if \(-\sqrt{2t_{2}}<t_{1}\leq\sqrt{2t_{2}}\). Specifically, the distribution of \(T_{1}|T_{2}\) when \(\mu=0\) is

\[F_{T_{1}|T_{2}}(t_{1}|t_{2})=\begin{cases}0&\text{if}\ \ t_{1}\leq-\sqrt{2t_{2}} \\ \pi^{-1}\cos^{-1}(-t_{1}/\sqrt{2t_{2}})&\text{if}\ \ -\sqrt{2t_{2}}<t_{1}\leq 0\\ 1-\pi^{-1}\cos^{-1}(t_{1}/\sqrt{2t_{2}})&\text{if}\ \ 0<t_{1}\leq\sqrt{2t_{2}} \\ 1&\text{if}\ \ \sqrt{2t_{2}}<t_{1}.\end{cases}\]

Thus, if \(\alpha<1/2\), then

\[P_{\theta_{1}=0}(T_{1}>c|T_{2}=t_{2})=\pi^{-1}\cos^{-1}(c/\sqrt{2t_{2}})= \alpha\Rightarrow c=\sqrt{2t_{2}}\cos(\pi\alpha).\]

For \(n\geq 2\) the development is essentially the same except that the arc length in a circle is replaced by the corresponding area in a sphere in the \(n\)-dimensional Euclidean space. However, this problem can be simplified and solved explicitly using Basu's theorem. \(\Box\)

The UMPU test for the hypothesis

\[H_{0}:\theta_{1}\geq a,\quad H_{1}:\theta_{1}<a\]

can be constructed using the above procedure by treating \(\zeta_{1}=-\theta_{1}\) as the parameter to be tested.

#### Two-sided UMPU tests

Now let us carry out the generalization for the three hypotheses I, II, III considered in section 3.2. Thus consider the following hypotheses:\[\begin{array}{ll}\mathrm{I}^{\prime}&H_{0}:\theta_{1}=a\quad\text{vs}\quad H_{1}: \theta_{1}\neq a\\ \mathrm{II}^{\prime}&H_{0}:a\leq\theta_{1}\leq b\quad\text{vs}\quad H_{1}:\theta _{1}<a\text{ or }\theta_{1}>b\\ \mathrm{III}^{\prime}&H_{0}:\theta_{1}\leq a\text{ or }\theta_{1}\geq b\quad\text{ vs}\quad H_{1}:a<\theta_{1}<b,\end{array}\]

where \(a<b\). Since the procedures for developing these tests are similar, we will focus on \(\mathrm{II}^{\prime}\). The development is parallel to the one-sided case except that the boundary set \(\Theta_{B}\) is now the union of two sets, and the completeness and sufficiency do not apply to the union but rather to each set in the union. This aspect will be highlighted in the following development.

For hypothesis \(\mathrm{II}^{\prime}\) we have

\[\begin{array}{ll}\Theta_{0}&=&\{\theta:a\leq\theta_{1}\leq b\},\\ \Theta_{1}&=&\{\theta:\theta_{1}<a\}\cup\{\theta:\theta_{1}>b\},\\ \Theta_{B}&=&\Theta(a)\cup\Theta(b).\end{array}\]

As before, \(T\) is sufficient for \(\Theta\) and the power function for any test is continuous. However, in this case \(T_{2:p}\) is complete and sufficient \(\Theta(a)\) and \(\Theta(b)\) separately, but not necessarily for their union. Nevertheless, as we have shown in Corollary 4.1, this is enough to guarantee the Neyman structure. We have then verified conditions 1, 2, 3 in Theorem 4.5.

Let \(\Psi\) and \(\alpha\) be as defined previously, but with \(\Theta_{B}\) replaced by the new boundary set. By Theorem 3.7, as applied to the one-parameter conditional family \(\{P_{T_{1}|T_{2:p}}(t_{1}|t_{2:p};\theta_{1}):\theta_{1}\in\Psi\}\), there is a test

\[\phi_{0}(t_{1},t_{2:p})=\begin{cases}1&\text{if}\;\;t_{1}<k_{1}(t_{2:p})\text{ or }t_{1}>k_{2}(t_{2:p})\\ \gamma_{i}(t_{2:p})&\text{if}\;\;t_{1}=k_{i}(t_{2:p}),\;i=1,2\\ 0&\text{if}\;\;u<k_{1}(v)\text{ or }t_{1}>k_{2}(t_{2:p}),\end{cases} \tag{4.14}\]

where \(-\infty<k_{1}(t_{2:p})<k_{2}(t_{2:p})<\infty\) are determined by

\[E_{\theta_{1}=a}(\phi_{0}(T)|T_{2:p}=t_{2:p})=E_{\theta_{1}=b}(\phi_{0}(T)|T_{ 2:p}=t_{2:p})=\alpha. \tag{4.15}\]

Moreover, for any test \(\phi(T)\) that satisfies the above relation, we have

\[E_{\eta}(\phi_{0}(T)|T_{2:p}=t_{2:p})\geq E_{\eta}(\phi(T)|T_{2:p}=t_{2:p}), \text{ for all }\eta\notin[a,b]. \tag{4.16}\]

By construction, \(\phi_{0}(T)\in\mathcal{N}_{\alpha}\) and \(\phi(T)\) is an arbitrary member of \(\mathcal{N}_{\alpha}\). Thus conditions 4 and 6 of Theorem 4.5 are satisfied. By Theorem 3.7, part 2, we also know that \(\phi_{0}(T)\) has size \(\alpha\) for the conditional problem. That is,

\[E_{\theta_{1}}(\phi_{0}(T)|T_{2:p}=t_{2:p})\leq\alpha\quad\text{for all }\eta\in[a,b].\]

Thus condition 5 of Theorem 4.5 is also satisfied, leading to the next theorem.

**Theorem 4.7**: _Suppose_

1. \(X=(X_{1},\ldots,X_{n})\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\)2. \(\Theta\) has a nonempty interior in \(\mathbb{R}^{p}\);
3. \(a,b\in\Psi\).

Then the test specified by (4.14) and (4.15) is a UMPU-\(\alpha\) for testing \(\mathrm{II}^{\prime}\).

We now state the forms of UMPU tests for Hypotheses \(\mathrm{I}^{\prime}\) and \(\mathrm{III}^{\prime}\) without proof.

**Theorem 4.8**: _Suppose the three assumptions in Theorem 4.7 are satisfied. Let_

\[\phi_{0}(t)=\begin{cases}0&\text{if}\ \ t_{1}<k_{1}(t_{2:p})\ \text{or}\ t_{1}>k_{2}(t_{2:p})\\ \gamma_{i}(t_{2:p})&\text{if}\ \ t_{1}=k_{i}(t_{2:p}),\ i=1,2\\ 1&\text{if}\ \ t_{1}<k_{1}(t_{2:p})\ \text{or}\ t_{1}>k_{2}(t_{2:p})\end{cases},\]

_where \(-\infty<k_{1}(t_{2:p})<k_{2}(t_{2:p})<\infty\) that satisfies_

\[E_{\theta_{1}=a}(\phi_{0}(T_{1},T_{2:p})|T_{2:p}=t_{2:p})=E_{\theta_{1}=b}( \phi_{0}(T_{1},T_{2:p})|T_{2:p}=t_{2:p})=\alpha.\]

_Then \(\phi_{0}(T_{1},T_{2:p})\) is a UMPU-\(\alpha\) test for hypothesis \(\mathrm{III}^{\prime}\)._

**Theorem 4.9**: _Suppose \(X\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\), \(\mathrm{int}(\Theta)\neq\varnothing\), and \(a\in\Psi\). Let_

\[\phi_{0}(t_{1},t_{2:p})=\begin{cases}1&\text{if}\ \ t_{1}<k_{1}(t_{2:p})\ \text{or}\ t_{1}>k_{2}(t_{2:p})\\ \gamma_{i}(t_{2:p})&\text{if}\ \ t_{1}=k_{i}(t_{2:p}),\ i=1,2\\ 0&\text{if}\ \ t_{1}<k_{1}(t_{2:p})\ \text{or}\ t_{1}>k_{2}(t_{2:p})\end{cases},\]

_where \(-\infty<k_{1}(t_{2:p})<k_{2}(t_{2:p})<\infty\) satisfy_

\[E_{\theta_{1}=a}(\phi_{0}(T_{1},T_{2:p})|T_{2:p}=t_{2:p})=\alpha,\] \[E_{\theta_{1}=a}(\phi_{0}(T_{1},T_{2:p})T_{1}|T_{2:p}=t_{2:p})= \alpha E_{\theta_{1}=a}(T_{1}|T_{2:p}=t_{2:p}).\]

_Then \(\phi_{0}(T_{1},T_{2:p})\) is a UMPU-\(\alpha\) test for hypothesis \(\mathrm{I}^{\prime}\)._

The next example illustrates the construction of the UMPU test for hypothesis \(I^{\prime}\) for the mean parameter of the Normal distribution.

**Example 4.2**: Suppose, in Example 4.1, we would like to test the hypothesis

\[H_{0}:\mu=0\quad\text{vs}\quad H_{1}:\mu\neq 0.\]

Then, by Theorem 4.9, the UMPU-\(\alpha\) test has the form

\[\phi_{0}(t_{1},t_{2})=\begin{cases}1&\text{if}\ \ t_{1}<k_{1}(t_{2}),\ t_{1}>k_{1}(t_{2})\\ 0&\text{otherwise},\end{cases}\]

where \(k_{1}(t_{2})<k_{2}(t_{2})\) are determined by 

[MISSING_PAGE_FAIL:124]

\[\phi_{0}(T_{1},T_{2})=\begin{cases}1&\text{if }\ T_{1}>k(T_{2})\\ 0&\text{otherwise}\end{cases},\quad E_{\theta_{1}=0}(\phi_{0}(T_{1},T_{2})|T_{2})=\alpha\]

is equivalent to

\[\phi_{1}(T_{1},T_{2})=\begin{cases}1&\text{if }\ g(T_{1},T_{2})>c\\ 0&\text{otherwise}\end{cases},\quad E_{\theta_{1}=0}(\phi_{1}(T_{1},T_{2}))=\alpha.\]

Many classical tests, such as the chi-square test, the student \(t\)-tests, and the \(F\)-tests can be shown using this mechanism to be UMPU tests. The critical step in the above procedure is to find the transformation \(g\). For this purpose, we first introduce the notion of an invariant family of distributions.

Let \((\Omega,\mathcal{F})\) be a measurable space and \(\mathcal{P}\) be a family of probability measures on \((\Omega,\mathcal{F})\). Let \(\mathcal{G}\) be a set of bijections \(g:\Omega\to\Omega\). Let \(\circ\) denote composition of functions. Suppose \(\mathcal{G}\) is a group with respect to \(\circ\). That is:

1. if \(g_{1}\in\mathcal{G}\), \(g_{2}\in\mathcal{G}\), then \(g_{2}\circ g_{1}\in\mathcal{G}\);
2. for all \(g_{1},g_{2},g_{3}\in\mathcal{G}\), \((g_{1}\circ g_{2})\circ g_{3}=g_{1}\circ(g_{2}\circ g_{3})\);
3. there exists \(e\in\mathcal{G}\) such that \(g\circ e=e\circ g\) for all \(g\in\mathcal{G}\);
4. for each \(g\in\mathcal{G}\) there is a \(f\in\mathcal{G}\) such that \(g\circ f=f\circ g=e\).

**Definition 4.3**: _A family of distributions \(\mathcal{P}\) is said to be invariant under \(\mathcal{G}\) if for every \(g\in\mathcal{G}\) and \(P\in\mathcal{P}\), we have \(P\circ g^{-1}\in\mathcal{P}\)._

If \(\mathcal{P}\) is invariant under \(\mathcal{G}\) then each \(g\in\mathcal{G}\) induces a function

\[\tilde{g}:\mathcal{P}\to\mathcal{P},\quad P\mapsto P\circ g^{-1}.\]

It is left as an exercise to show that \(\tilde{g}\) is bijective and the set \(\tilde{\mathcal{G}}=\{\tilde{g}:g\in\mathcal{G}\}\) is itself a group. For a member \(P\) of \(\mathcal{P}\), we call the set

\[\mathcal{M}(P)=\{\tilde{g}(P):\tilde{g}\in\tilde{\mathcal{G}}\}\]

an orbit of \(\tilde{\mathcal{G}}\). If \(\mathcal{M}(P)=\mathcal{P}\), then we say the group \(\tilde{\mathcal{G}}\) is transitive.

**Theorem 4.10**: _Let \(V\) be a random element defined on \((\Omega,\mathcal{F})\). Suppose:_

1. \(\mathcal{P}\) _is invariant under_ \(\mathcal{G}\)_;_
2. _For each_ \(P\in\mathcal{P}\)_,_ \(g\in\mathcal{G}\)_,_ \(P\circ(V\circ g)^{-1}=P\circ V^{-1}\)_;_
3. _the group_ \(\tilde{\mathcal{G}}\) _is transitive._

_Then \(V\) is ancillary for \(\mathcal{P}\)._

Proof: Let \(P\in\mathcal{P}\). Since \(\mathcal{M}(P)=\mathcal{P}\), it suffices to show that the distribution of \(V\) is the same for all \(Q\in\mathcal{M}(P)\). Let \(Q_{1},Q_{2}\in\mathcal{M}(P)\). Then \(Q_{1}=P\circ g_{1}^{-1}\), \(Q_{2}=P\circ g_{2}^{-1}\) for some \(g_{1},g_{2}\in\mathcal{G}\). By Exercise 4.6 we have

\[Q_{1}\circ V^{-1}= (P\circ g_{1}^{-1})\circ V^{-1}=P\circ(V\circ g_{1})^{-1}\] \[Q_{2}\circ V^{-1}= (P\circ g_{2}^{-1})\circ V^{-1}=P\circ(V\circ g_{2})^{-1}.\]By assumption 2,

\[P^{\circ}(V^{\circ}g_{1})^{-1}=P^{\circ}V^{-1}=P^{\circ}(V^{\circ}g_{2})^{-1}.\]

That is, \(Q_{1}{}^{\circ}V^{-1}=Q_{2}{}^{\circ}V^{-1}\), hence \(V\) has the same distribution under \(Q_{1}\) and \(Q_{2}\). \(\Box\)

Our treatment of this topic slightly differs from the treatment in classical texts such as Ferguson (1967); Lehmann and Romano (2005), in that

1. we do not assume \(\mathcal{P}\) to be parametric -- not for generality but for greater clarity;
2. we require \(P^{\circ}(V^{\circ}g)^{-1}=P^{\circ}V^{-1}\) rather than the stronger assumption \(V^{\circ}g=V\).

**Example 4.3**: Let \(X=(X_{1},\ldots,X_{n})\) be an \(n\)-dimensional random vector whose density with Lebesgue measure on \((\mathbb{R}^{n},\mathcal{R}^{n})\) is of the form

\[f(x_{1}-\mu,\ldots,x_{n}-\mu),\quad\mu\in\mathbb{R},\]

where \(f\) is a known density function on \(\mathbb{R}^{n}\). Let \(\mathcal{P}\) denote the class of distributions corresponding to these densities. Consider the group \(\mathcal{G}\) consisting of transformations

\[g_{c}:\mathbb{R}\to\mathbb{R},\quad(x_{1},\ldots,x_{n})\mapsto(x_{1}+c,\ldots, x_{n}+c),\quad c\in\mathbb{R}.\]

This group is called the translation group. The random vector \(Y=g_{c}(X)\) has density

\[f(y_{1}-(\mu+c),\ldots,y_{n}-(\mu+c)),\]

which belongs to \(\mathcal{P}\). Hence the family \(\mathcal{P}\) is invariant under \(\mathcal{G}\). Now let \(V(x)\) be a function such that \(V(x+c)=V(x)\) for all \(c\). For example, \(V(x)\) could be \(x_{1}-x_{2}\). In addition, for any \(P_{\mu}\in\mathcal{P}\), \(\mathcal{M}(P)\) corresponds to the following family of densities

\[\{f(y_{1}-(\mu+c),\ldots,y_{n}-(\mu+c)),\quad c\in\mathbb{R}\},\]

which is \(\mathcal{P}\) itself. Therefore, \(V(X)\) is ancillary for \(\mu\). In the special case where \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,1)\), \(T=\sum_{i=1}^{n}X_{i}\) is complete and sufficient for \(\mu\). Therefore \(T\mathop{\mathchoice{\hbox{\hbox to 0.0pt{\kern 2.999954pt\vrule height 6.299904pt wid th 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{\kern 2.999954pt \vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 2.099968pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 1.049984pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}}V(X)\). \(\Box\)

**Example 4.4**: Suppose that \((X_{1},\ldots,X_{n})\) has density of the form

\[\sigma^{-n}f(x_{1}/\sigma,\ldots,x_{n}/\sigma),\quad\sigma>0.\]

Write this family of distributions as \(\mathcal{P}\). Consider the group of transformations\[g_{c}:\ (x_{1},\ldots,x_{n})\mapsto(cx_{1},\ldots,cx_{n}),\quad c>0.\]

If \(Y=cX\) then the density of \(Y\) is

\[(c\sigma)^{-n}f(x_{1}/(c\sigma),\ldots,x_{n}/(c\sigma)),\]

which belongs to \(\mathcal{P}\). Thus \(\mathcal{P}\) is invariant under \(\mathcal{G}\). Furthermore, it is easy to see that \(\mathcal{M}(P)=\mathcal{P}\) for any \(P\in\mathcal{P}\). Let \(V(x)\) be a function that satisfies \(V(x)=V(cx)\) for all \(c>0\). For example \(V(x)=x_{1}/x_{2}\) satisfies this condition. Then \(V(X)\) is ancillary for \(\sigma\). In this special case where \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(0,\sigma^{2})\), the statistic \(T=\sum_{i=1}^{n}X_{i}^{2}\) is complete and sufficient for \(\sigma\). Hence, by Basu's theorem \(T(X)\leavevmode\hbox{\rm 1\kern-3.8pt\normalsize 1}\kern-3.8pt\normalsize 1 }V(X)\). \(\Box\)

**Example 4.5** Suppose that \((X_{1},\ldots,X_{n})\) has density of the form

\[\sigma^{-n}f((x_{1}-\mu)/\sigma,\ldots,(x_{n}-\mu)/\sigma),\quad\sigma>0,\ \mu \in\mathbb{R}.\]

Write this family of distributions as \(\mathcal{P}\). Consider the group of transformations

\[g_{c}:\ (x_{1},\ldots,x_{n})\mapsto(cx_{1}+d,\ldots,cx_{n}+d),\quad c>0,\ d\in \mathbb{R}.\]

Similar to the previous two examples, we can show that \(\mathcal{M}(P)=\mathcal{P}\) for each \(P\in\mathcal{P}\) and \(\mathcal{P}\) is invariant under \(\mathcal{G}\). Therefore any statistic \(V(X)\) satisfying \(V(cX+d)=V(X)\) is ancillary for \((\mu,\sigma)\). If \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\), then such \(V(X)\)'s are independent of the complete and sufficient statistic \((\sum_{i=1}^{n}X_{i},\sum_{i=1}^{n}X_{i}^{2})\). For example

\[\left(\frac{X_{1}-\bar{X}}{S},\ldots,\frac{X_{n}-\bar{X}}{S}\right) \leavevmode\hbox{\rm 1\kern-3.8pt\normalsize 1}\kern-3.8pt\normalsize 1 }(\bar{X},S^{2}).\]

where \(S^{2}\) is the sample variance \(\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}/(n-1)\) The statistic on the left is called the standardized (or studentized) residuals. \(\Box\)

We now give an example where the invariant family is nonparametric.

**Example 4.6** Let \(X\) be a \(p\)-dimensional random vector. The distribution of \(X\) is said to be spherical if, for any orthogonal matrix \(A\),

\[AX\overset{\mathcal{D}}{=}X,\ \text{that is,}\ AX\ \text{and}\ X\ \text{have the same distribution}. \tag{4.17}\]

If \(X\) has a density with respect to the Lebesgue measure, then (4.17) means that the density of \(X\) is \(cf(\|x\|)\) for some probability density function \(f\) defined on \([0,\infty)\) and some constant \(c>0\). Let \(\tau\) be the transformation

\[\mathbb{R}^{p}\rightarrow\mathbb{R},\quad x\mapsto\|x\|.\]Let \(\lambda\) be the \(n\)-fold product Lebesgue measure. Then by the change of variable theorem

\[\int cf(\|x\|)d\lambda(x) = \int cf(s)d\lambda^{\circ}\tau^{-1}(s)\] \[= c\int f(s)\frac{d\lambda^{\circ}\tau^{-1}(s)}{d\lambda(s)}\,d \lambda(s)=1.\]

Hence

\[c=\left(\int f(s)\frac{d\lambda^{\circ}\tau^{-1}(s)}{d\lambda(s)}\,d\lambda(s) \right)^{-1}\equiv c(f).\]

Let \(\mathcal{D}\) be the class of all probability density functions defined on \([0,\infty)\). Consider the family of densities for \(X\):

\[\{c(f)f(\|x\|):f\in\mathcal{D}\}.\]

Denote the corresponding family of distributions of \(X\) by \(\mathcal{P}\). Let \(\mathcal{G}\) be the class of all functions of the form

\[x\mapsto h(\|x\|)x/\|x\|,\quad h\in\mathcal{G}_{0},\]

where \(\mathcal{G}_{0}\) is the class all bijective, positive, functions from \([0,\infty)\) to \([0,\infty)\). It is clear that \(\mathcal{G}\) is a group. For example, if \(y=h(\|x\|)x/\|x\|\), then \(\|x\|=h^{-1}(\|y\|)\) and hence

\[x=y\|x\|/h(\|x\|)=yh^{-1}(\|y\|)/\|y\|\equiv g^{-1}(y).\]

For any \(P\in\mathcal{P}\) and \(g\in\mathcal{G}\), the density of \(Y=g(X)\) also has a spherical distribution, because, if \(A\) is an orthogonal matrix,

\[AY=h(\|X\|)AX/\|X\|=h(\|AX\|)AX/\|AX\|\stackrel{{\mathcal{D}}}{{= }}h(\|X\|)X/\|X\|\]

where the last equality follows from \(AX\stackrel{{\mathcal{D}}}{{=}}X\). Since \(Y\) is also dominated by \(\lambda\), its density is also in \(\mathcal{P}\). So \(\mathcal{P}\) is invariant under \(\mathcal{G}\). To see that and \(\mathcal{M}(P)=\mathcal{P}\) for any \(P\in\mathcal{P}\) (that is, \(\tilde{\mathcal{G}}\) is transitive). Hence, any function \(V(x)\) satisfying \(V=V^{\circ}g\) for all \(g\in\mathcal{G}\) is ancillary for \(\mathcal{P}\). One such function is \(V(x)=x/\|x\|\).

We now show that \(\|X\|\) is complete sufficient on \(\mathcal{P}\). \(\|X\|\) is a sufficient statistic, because the conditional distribution of \(X\) given \(\|X\|=r\) is uniform on the sphere \(\{x:\|x\|=r\}\). Now let \(u\) be a function of \(\|X\|\), independent of \(f\), such that

\[E_{f}u(\|X\|)=0\ \ \mbox{for all}\ f\in\mathcal{D}.\]

Take \(f\) to be the exponential distribution \(te^{-st}\), where \(t>0\). Then we have

\[\int_{0}^{\infty}u(s)e^{-st}tds=0\Rightarrow\int_{0}^{\infty}u(s)e^{-st}ds=0\]Consequently \(v(t)=\int_{0}^{\infty}g(s)e^{-ts}ds=0\) for all \(t>0\). Because \(v(t)\) is analytic we see that \(v(t)=0\) for all \(t\in\mathbb{C}\). Hence, by the uniqueness of inverse Laplace transformation, we see that \(u(s)=0\). Thus \(\|X\|\) is complete. By Basu's theorem, any \(V(x)\) such that \(V\circ g=V\) is independent of \(\|X\|\). In particular, \((X/\|X\|)\,\hbox{\rm 1\kern-2.5pt\rm l}\,\|X\|\). \(\Box\)

### Using Basu's theorem to construct UMPU test

The development in the last section gives a general principle for constructing UMPU test using statistics that is ancillary to the nuisance parameters. The implementation is this principle is straightforward for the one-sided tests and the two-sided tests \(\Pi^{\prime},\Pi^{\prime}\), where \(\phi_{0}(t_{1})\) alone appears in the conditional expectations: we simply replace \(\phi_{0}(t_{1})\) by \(\phi_{0}\circ V(t)\), and remove conditioning. The situation is slightly more complicated for the two-sided test \(\hbox{\rm I}^{\prime}\), where we encounter an additional constraint of the form

\[E_{\theta_{1}=a}[\phi_{0}(T_{1})T_{1}|T_{2:p}=t_{2:p}]=\alpha E_{\theta_{1}=a} (T_{1}|T_{2:p}=t_{2:p}). \tag{4.18}\]

In this case, we replace \(\phi_{0}(T_{1})\) by \(\phi_{0}(V)\) and \(T_{1}\) by \(t_{1}(V,T_{2:p})\), which is the solution in \(t_{1}\) of the equation \(V(t_{1},t_{2:p})=v\). Since \(V\,\hbox{\rm 1\kern-2.5pt\rm l}\,\ T_{2:p}\), the above constraint (4.18) becomes

\[E_{\theta_{1}=a}[\phi_{0}(V)t_{1}(V,t_{2:p})]=\alpha E_{\theta_{1}=a}[t_{1}(V,t _{2:p})].\]

In the special case where \(t_{1}(V,t_{2:p})\) has the linear form \(a(t_{2:p})V+b(t_{2:p})\), we have

\[E_{\theta_{1}=a}[\phi_{0}(V)t_{1}(V,t_{2:p})]=\alpha E_{\theta_{1}=a}[t_{1}(V,t _{2:p})].\]

This is equivalent to

\[a(t_{2:p})E[\phi_{0}(V)V]+\alpha b(t_{2:p})=\alpha[a(t_{2:p})E\phi_{0}(V)+b(t_{ 2:p})].\]

This happens if and only if

\[E_{\theta_{1}=a}[\phi(V)V]=\alpha E_{\theta_{1}=a}(V).\]

We now use several examples to illustrate how to use Basu's theorem to construct UMPU test.

**Example 4.7** In Example 4.1, let

\[V(t_{1}(X),t_{2}(X))=\frac{\sqrt{n-1}}{n}\frac{t_{1}(X)}{\sqrt{t_{2}(X)-t_{1} ^{2}(X)/n}}=\frac{\bar{X}}{S}.\]Write \(t(x)=(t_{1}(x),t_{2}(x))\). Then \(V\circ t(x)=V\circ t(cx)\) for any \(c>0\). By Example 4.4, \(V\circ t(X)\) is ancillary with respect to \(\Theta_{B}=\{(0,\sigma^{2}):\sigma^{2}>\infty\}\). By Basu's theorem, \(V\circ t(X)\operatorname{\text{\rm 1\kern-2.27622ptl}}t_{2}(X)\). In the meantime, it is easy to check by differentiation that \(V(t_{1},t_{2})\) is an increasing function of \(t_{1}\) for each fixed \(t_{2}\). Therefore the UMPU test (4.13) is equivalent to

\[\phi(t_{1},t_{2})=\begin{cases}1&\text{if }\;V(t_{1},t_{2})>k(t_{2})\\ 0&\text{otherwise,}\end{cases}\]

where \(k(t_{2})\) is determined by the equation

\[E_{\mu=0}[\phi(T_{1},T_{2})|T_{2}=t_{2}]=P_{\mu=0}(V(T_{1},T_{2})>k)|T_{2}=t_{ 2})=\alpha.\]

Because \(V(T_{1},T_{2})\operatorname{\text{\rm 1\kern-2.27622ptl}}T_{2}\), the above equation is equivalent to

\[P_{\mu=0}(V(T_{1},T_{2})>k)=\alpha.\]

Because \(V(T_{1},T_{2})\sim t_{n-1}\). We have \(k=t_{n-1}(\alpha)\), which is the one-sided \(t\) test.

The UMPU test for the two-sided hypothesis in Example 4.2 in its original form is

\[\phi(t_{1},t_{2})=\begin{cases}1&t_{1}\leq k_{1}(t_{2}),\ t_{1}>k_{2}(t_{2})\\ 0&\text{otherwise.}\end{cases}\]

We cannot directly write the two constraints in terms of \(V\) because \(V\) is not in the linear form \(a(t_{2})t_{1}+b(t_{2})\), and the second constraint

\[E_{\mu=0}(\phi(T)T_{1}|t_{2})=\alpha E_{\mu=0}(T_{1}|t_{2}) \tag{4.19}\]

is no longer equivalent to

\[E_{\mu=0}(\phi(V)V)=\alpha E_{\mu=0}(V).\]

However, because \(T_{1}\operatorname{\text{\rm 1\kern-2.27622ptl}}T_{2}\), the null distribution of \(T_{1}\) given \(T_{2}\) is the same as the marginal distribution of \(T_{1}\), which is symmetric about \(0\). Hence, as argued earlier, the constraint (4.19) is automatically satisfied if we take \(k_{1}(t_{2})=-k_{2}(t_{2})\). In other words the UMPU test is of the form

\[\phi(t_{1},t_{2})=\begin{cases}0&\text{if }\;-k(t_{2})<t_{1}<k(t_{2})\\ 1&\text{otherwise,}\end{cases}\]

where \(k(t_{2})\) is determined by \(E_{\mu=0}(\phi(T)|t_{2})=\alpha\). This can then be equivalently written in terms of \(V\) as

\[\phi(v)=\begin{cases}0&\text{if }\;V(-k(t_{2}),t_{2})<V<V(k(t_{2}),t_{2})\\ 0&\text{otherwise,}\end{cases}\]

where, note that \(V(-k(t_{2}),t_{2})=-V(k(t_{2}),t_{2})\). Because \(V\operatorname{\text{\rm 1\kern-2.27622ptl}}T_{2}\), \(V(k(t_{2}),t_{2})\) is a nonrandom constant determined by \(E_{\mu=0}(\phi(V))=\alpha\). This constant then must be \(t_{n-1}(\alpha/2)\). \(\Box\)The next example is concerned with testing the variance in the presence of the mean as the nuisance parameter.

**Example 4.8** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\). We are interested in testing the hypothesis

\[H_{0}:\sigma^{2}\leq 1\ \ \text{versus}\ \ H_{1}:\sigma^{2}>1. \tag{4.20}\]

In the notation of Example 4.1, we can equivalently state the hypothesis (4.20) as

\[H_{0}:\theta_{2}\leq-1/2\ \ \text{versus}\ \ H_{1}:\theta_{2}>-1/2.\]

The UMPU test is of the form

\[\phi(t_{1},t_{2})=\begin{cases}1&\text{if}\ \ t_{2}>k(t_{1})\\ 0&\text{otherwise},\end{cases} \tag{4.21}\]

where \(k(t_{1})\) is determined by

\[P_{\theta_{1}=-1/2}\left(T_{2}>k(T_{1})|T_{1}\right)=\alpha.\]

Let

\[V\circ t(x)=t_{2}(x)-t_{1}^{2}(x)/n.\]

Then \(V\circ t(x+c)=V\circ t(x)\) for all \(c\in\mathbb{R}\). Hence, by Example 4.3, \(V\circ t(X)\) is ancillary for \(\Theta_{B}=\{(\theta_{1},-1/2):\theta_{1}\in\mathbb{R}\}\). Since \(T_{1}\) is complete and sufficient for \(\Theta_{B}\), by Basu's theorem \(V\circ t(X)\amalg t_{1}(X)\) for all \(\theta\in\Theta_{B}\). As \(V(t)\) is increasing in \(t_{2}\) for each fixed \(t_{1}\), the test (4.21) can be equivalently written as

\[\phi(t_{1},t_{2})=\begin{cases}1&\text{if}\ \ V(t_{1},t_{2})>k\\ 0&\text{otherwise},\end{cases}\]

where \(k\) is determined by the equation

\[P_{\theta_{2}=-1/2}\left(V(T_{1},T_{2})>k|T_{1}\right)=P_{\theta_{2}=-1/2} \left(V(T_{1},T_{2})>k\right)=\alpha\]

Since, under \(\theta_{2}=-1/2\), \(V(T_{1},T_{2})\sim\chi_{(n-1)}^{2}\), \(k=\chi_{(n-1)}^{2}(\alpha)\).

Now suppose we want to test the two-sided hypothesis

\[H_{0}:\sigma^{2}=1\quad\text{vs}\quad H_{1}:\sigma^{2}\neq 1.\]

The UMPU-\(\alpha\) test is of the form

\[\phi(t)=\begin{cases}1&\text{if}\ \ t_{2}<k_{1}(t_{1}),\ t_{2}>k_{2}(t_{2})\\ 0&\text{otherwise},\end{cases}\]where \(k_{1}(t_{2})\), \(k_{2}(t_{2})\) are determined by

\[E_{\sigma^{2}=1}[\phi_{0}(T)|T_{2}]=\alpha,\quad E_{\sigma^{2}=1}(\phi_{0}(T)T_{1 }|T_{2})=\alpha E_{\sigma^{2}=1}(T_{1}|T_{2}).\]

Because \(t_{2}\) is of the linear form \(V+t_{1}^{2}/n\), by the discussion at the beginning of this section, the constraints can be replaced by

\[E_{\sigma^{2}=1}(\phi_{0}(V))=\alpha,\quad E_{\sigma^{2}=1}(\phi_{0}(V)V)= \alpha E_{\sigma^{2}=1}(V)=\alpha(n-1)\]

where the last equality follows from \(V\sim\chi_{(n-1)}^{2}\). The values of \(k_{1},k_{2}\) can be obtained by solving the above equation numerically. 

### UMPU test for a linear function of \(\theta\)

Many important statistical tests can be written as the test for a linear combination of the components of the parameter \(\theta\). Let \(c_{1}=(\alpha_{1},\ldots,\alpha_{p})\in\mathbb{R}^{p}\), and let \(c_{1}\neq 0\) be a nonzero vector. Suppose we are interested in the parameter \(\eta_{1}=c_{1}^{T}\theta\). Let \(c_{2},\ldots,c_{p}\) be vectors in \(\mathbb{R}^{p}\) such that \(C=(c_{1},\ldots,c_{p})\) is a nonsingular matrix. Then

\[\theta^{T}t_{0}=(C^{-T}C^{T}\theta)^{T}t_{0}=(C^{T}\theta)^{T}C^{-1}t_{0}\equiv \eta^{T}u_{0}.\]

Let \(L\) represent the linear transformation \(\theta\mapsto C^{T}\theta\). Then the exponential family \(\mathfrak{E}_{p}(t_{0},\mu_{0})\) can be written as \(\mathfrak{E}_{p}(u_{0},\mu_{0}{}^{\circlearrowleft}L^{-1})=\mathfrak{E}_{p}(u_ {0},\nu_{0})\). So testing hypothesis about \(\eta_{1}\) reduces to the problem in the last section.

The choice of \(c_{2},\ldots,c_{p}\) does not affect the result. A convenient choice is as follows. Without loss of generality, assume the first component \(\alpha_{1}\) of \(c_{1}\) is nonzero. Then let \(c_{k}=e_{k}\), \(k=2,\ldots p\), where \(e_{k}\) is the \(p\)-dimensional vector whose \(k\)th entry is \(1\) and all the other entries are \(0\). In this case,

\[C^{-1}=\begin{pmatrix}1/\alpha_{1}&0&\cdots&0\\ -\alpha_{2}/\alpha_{1}&1&0\\ \vdots&&\ddots\\ -\alpha_{p}/\alpha_{1}&0&1\end{pmatrix}.\]

The two examples are concerned with the well known two-sample problem for gaussian observations: the first is concerned with comparison of the variances; the second is concerned with the comparison of the means. Both examples are special cases of UMPU tests for linear combinations of \(\theta\) as described above. It is somewhat surprising that, in some sense, the comparison of the means is more difficult than the comparison of the variances.

**Example 4.9**: Suppose that \(X_{1},\ldots,X_{m}\) are i.i.d. \(N(\mu_{1},\sigma_{1}^{2})\) and that \(Y_{1},\ldots,Y_{n}\) are i.i.d. \(N(\mu_{2},\sigma_{2}^{2})\). We are interested in testing \[H_{0}:\sigma_{2}^{2}/\sigma_{1}^{2}\leq\tau\ \ \mbox{versus}\ \ H_{1}:\sigma_{2}^{2}/ \sigma_{1}^{2}>\tau. \tag{4.22}\]

Let \(X=(X_{1},\ldots,X_{m})\) and \(Y=(Y_{1},\ldots,Y_{n})\). By simple algebra we deduce the joint density of \((X,Y)\) in the form of

\[c(\theta)e^{\theta^{T}t(x,y)}\]

with respect to some measure on \((\Omega_{X}\times\Omega_{Y},\mathcal{F}_{X}\times\mathcal{F}_{Y})\), where

\[\theta=(\theta_{1},\ldots,\theta_{4})=(\mu_{1}/\sigma_{1}^{2},\mu_{2}/\sigma_{ 2}^{2},-1/(2\sigma_{1}^{2}),-1/(2\sigma_{2}^{2}))\]

and

\[t(x,y) = (t_{1}(x),t_{2}(y),t_{3}(x),t_{4}(x))\] \[= (\sum_{i=1}^{m}x_{i},\sum_{j=1}^{n}y_{j},\sum_{i=1}^{m}x_{i}^{2}, \sum_{j=1}^{n}y_{j}^{2}).\]

In terms of \(\theta\), the hypothesis (4.22) can be rewritten as

\[H_{0}:\theta_{4}\leq\theta_{3}/\tau\ \ \ \mbox{vs}\ \ \ H_{1}:\theta_{4}>\theta_{3 }/\tau.\]

Let \(\eta_{4}=\theta_{4}-\theta_{3}/\tau\), \(\eta_{3}=\theta_{3}\), \(\eta_{2}=\theta_{2}\), and \(\eta_{1}=\theta_{1}\). Then, in terms of \(\eta=(\eta_{1},\ldots,\eta_{4})\), the hypothesis further reduces to

\[H_{0}:\eta_{4}\leq 0\ \ \ \mbox{vs}\ \ \ H_{1}:\eta_{4}>0.\]

The parameter space is

\[\Lambda=\{(\eta_{1},\eta_{2},\eta_{3},\eta_{4}):\eta_{1}\in\mathbb{R},\ \eta_{2}\in\mathbb{R},\ \eta_{3}<0,\ \eta_{4}\in\mathbb{R}\}.\]

The boundary space is

\[\Lambda_{B}=\{(\eta_{1},\eta_{2},\eta_{3},0):\eta_{1}\in\mathbb{R},\ \eta_{2}\in\mathbb{R},\ \eta_{3}<0\}.\]

Note that

\[\theta_{1}t_{1}+\cdots\theta_{4}t_{4} = \theta_{1}t_{1}+\theta_{2}t_{2}+\theta_{3}(t_{3}+t_{4}/\tau)+( \theta_{4}-\theta_{3}/\tau)t_{4}\] \[= \eta_{1}u_{1}+\cdots+\eta_{4}u_{4}.\]

So the UMPU-\(\alpha\) test is of the form

\[\phi(u)=\begin{cases}1&\mbox{if}\ \ u_{4}>k(u_{1:3})\\ 0&\mbox{otherwise}.\end{cases} \tag{4.23}\]

Let \(u\) be the function \((x,y)\mapsto(u_{1}(x),u_{2}(y),u_{3}(x,y),u_{4}(y))\). Now consider the statistic \[V\circ u(X,Y)=\frac{1}{\tau}\frac{\sum_{j=1}^{n}(Y_{j}-\bar{Y})^{2}/(n-1)}{\sum_{i =1}^{m}(X_{i}-\bar{X})^{2}/(m-1)}.\]

We write the above function as \(V\circ u(x,y)\) because the right hand side is indeed such a composite function:

\[\frac{\sum_{j=1}^{n}(Y_{j}-\bar{Y})^{2}}{\sum_{i=1}^{m}(X_{i}-\bar{X})^{2}}= \frac{u_{4}-u_{2}^{2}/n}{u_{3}-u_{4}/\tau-u_{1}^{2}/m}. \tag{4.24}\]

Note that we have, by construction,

\[u_{4}-u_{2}^{2}/n\geq 0,\quad u_{3}-u_{4}/\tau-u_{1}^{2}/m\geq 0.\]

We will show

1. \(V(u)\) is increasing in \(u_{4}\) for each fixed \((u_{1},u_{2},u_{3})\);
2. \(V\circ u(X,Y)\) is ancillary for \(\Lambda_{B}\).

The validity of the first assertion is easily seen from (4.24). To show the second assertion, let \(\mathcal{P}=\{P_{\eta}:\eta\in\Lambda_{B}\}\), and consider the group \(\mathcal{G}\) of transformations:

\[(x_{1},\ldots,x_{m},y_{1},\ldots,y_{n})\mapsto(cx_{1}+d_{1},\ldots,cx_{m}+d_{1 },cy_{1}+d_{2},\ldots,cy_{n}+d_{2}),\]

where \(c>0\) and \(d_{1},d_{2}\in\mathbb{R}\). Denote this transformation as \(g_{c,d_{1},d_{2}}\). Suppose the distribution of \((X,Y)\) belongs to \(\Lambda_{B}\). Then the distribution of \((\tilde{X},\tilde{Y})=g_{c,d_{1},d_{2}}(X,Y)\) is

\[N(\tilde{\mu}_{1},\tilde{\sigma}_{1}^{2})\times\cdots\times N(\tilde{\mu}_{1}, \tilde{\sigma}_{1}^{2})\times N(\tilde{\mu}_{2},\tilde{\sigma}_{2}^{2})\times \cdots\times N(\tilde{\mu}_{2},\tilde{\sigma}_{2}^{2})\]

with parameters

\[\begin{cases}\tilde{\mu}_{1}=c\mu_{1}+d_{1}\\ \tilde{\mu}_{2}=c\mu_{2}+d_{2}\\ \tilde{\sigma}_{1}^{2}=c^{2}\sigma_{1}^{2}\\ \tilde{\sigma}_{2}^{2}=c^{2}\sigma_{2}^{2}\end{cases}\Rightarrow\tilde{\eta}_{ 4}=\tilde{\theta}_{4}-\tilde{\theta}_{3}/\tau=\frac{1}{c^{2}}(\theta_{4}- \theta_{3}/\tau)=0.\]

In other words, the distribution of \((\tilde{X},\tilde{Y})\) stays in the family indexed by \(\Lambda_{B}\). That is, \(\mathcal{P}\) is invariant under \(\mathcal{G}\). In the meantime it is easy to see that \(V\circ u(\tilde{x},\tilde{y})\)=\(V\circ u(x,y)\). Finally, let \((\eta_{1},\eta_{2},\eta_{3},0)\) be any fixed point in \(\Lambda_{B}\). Then the distribution of \((\tilde{X},\tilde{Y})=g_{c,d_{1},d_{2}}(X,Y)\) corresponds to the parameter

\[\tilde{\eta}_{1}=\eta_{1}/c-(2d_{1}/c^{2})\eta_{3},\quad\tilde{\eta}_{2}=\eta_ {2}/c-(2d_{2}/c^{2})\theta_{3}/\tau,\quad\tilde{\eta}_{3}=\eta_{3}/c^{2}.\]

Clearly, when \((d_{1},d_{2},c)\) varies freely in \(\mathbb{R}\times\mathbb{R}\times(0,\infty)\), the above parameters occupy the whole space \(\mathbb{R}\times\mathbb{R}\times(-\infty,0)\). This means \(\tilde{G}\) is a transitive group. Hence, by Theorem 4.10, \(V\circ u(X,Y)\) is ancillary for \(\Lambda_{B}\). By Basu's theorem, \(V\circ u(X,Y)\mathop{\hbox{\rm 1\kern-2.5pt\rm 1}}\nolimits\left(u_{1}(X),u_{2}(Y),u_{3}(X,Y)\right)\). Therefore, the test (4.23) is equivalent to \[\phi(u)=\begin{cases}1&\text{if}\ \ V\circ u(X,Y)>k\\ 0&\text{otherwise},\end{cases}\]

where \(k\) is determined by

\[P_{\eta_{4}=0}(V\circ u(X,Y)>k)=\alpha.\]

Since, under any \(\eta\in\Lambda_{B}\), \(V\circ u(X,Y)\sim F_{(m-1),(n-1)}\), the critical point \(k\) is \(F_{(m-1),(n-1)}(\alpha)\). 

Now let us turn to the comparison of two normal means.

**Example 4.10**: Consider the same setting as the above example with \(\sigma_{1}=\sigma_{2}\). Under this assumption \((X,Y)\) follows a 3-parameter exponential family

\[\theta_{1}t_{1}+\theta_{2}t_{2}+\theta_{3}(t_{3}+t_{4}),\]

where \(t_{1},\ldots,t_{4}\) are defined as in the above example. We are now interested in testing \(H_{0}:\mu_{1}=\mu_{2}\) versus \(H_{1}:\mu_{1}\neq\mu_{2}\), which is equivalent to

\[H_{0}:\theta_{1}=\theta_{2}\quad\text{vs}\quad H_{1}:\theta_{1}\neq\theta_{2}.\]

Let \(\eta_{1}=\theta_{1}-\theta_{2}\), and

\[\theta_{1}t_{1}+\theta_{2}t_{2}+\theta_{3}(t_{3}+t_{4})= \ (\theta_{1}-\theta_{2})t_{1}+\theta_{2}(t_{1}+t_{2})+\theta_{3}(t_{3}+t_{4})\] \[\equiv \eta_{1}u_{1}+\eta_{2}u_{2}+\eta_{3}u_{3}.\]

The UMPU test is of the form

\[\phi(u)=\begin{cases}1&\text{if}\ \ u_{1}>k(u_{2},u_{3})\\ 0&\text{otherwise}.\end{cases}\]

Now consider the statistic

\[\frac{(\bar{X}-\bar{Y})/\sqrt{m^{-1}+n^{-1}}}{\sqrt{[\sum_{i=1}^{m}(X_{i}-\bar{ X})^{2}+\sum_{i=1}^{n}(Y_{i}-\bar{Y})^{2}]/(m+n-2)}}. \tag{4.25}\]

Because

\[\bar{X}= \ t_{1}/m=u_{1}/m,\] \[\bar{Y}= \ t_{2}/n=(u_{2}-u_{1})/n,\] \[\sum_{i=1}^{m}(X_{i}-\bar{X})^{2}+\sum_{i=1}^{n}(Y_{i}-\bar{Y})^{ 2}= \ u_{3}-u_{1}^{2}/m-(u_{2}-u_{1})^{2}/n,\]

the statistic (4.25) can be rewritten as \[\frac{[(m^{-1}+n^{-1})u_{1}(X)-n^{-1}u_{2}(Y)]/\sqrt{m^{-1}+n^{-1}}}{\sqrt{[u_{3}-u _{1}^{2}/m-(u_{2}-u_{1})^{2}/n]/(m+n-2)}}\equiv V\circ u(X,Y).\]

Ignoring constants, this function is

\[[(m^{-1}+n^{-1})u_{1}(X)-n^{-1}u_{2}(Y)][u_{3}-u_{1}^{2}/m-(u_{2}-u_{1})^{2}/n]^ {-1/2}.\]

To show that this is an increasing function of \(u_{1}\), we differentiate it with respect to \(u_{1}\) to obtain

\[[u_{3}-u_{1}^{2}/m-(u_{2}-u_{1})^{2}/n]^{-3/2}[(m^{-1}+n^{-1})u_{1 }-n^{-1}u_{2}]^{2}\] \[+[u_{3}-u_{1}^{2}/m-(u_{2}-u_{1})^{2}/n]^{-1/2}(m^{-1}+n^{-1})>0.\]

Use similar argument as before, the distribution of \(V\circ u(X,Y)\) is symmetric about \(0\) given \((u_{2},u_{3})\). Hence the second condition is automatically satisfied.

For this example, the full parameter space is \(\Lambda=\mathbb{R}\times\mathbb{R}\times(-\infty,0)\). The boundary of the parameter space is \(\Lambda_{B}=\{0\}\times\mathbb{R}\times(-\infty,0)\). Consider the group of transformations

\[g_{c,d}:(x,y)\mapsto(cx+d,cy+d),\]

where \(c>0\) and \(d\in\mathbb{R}\). Let \(\eta\) be a fixed point in \(\Lambda_{B}\). For \(c>0,d\in\mathbb{R}\), the random vector \((\tilde{X},\tilde{Y})\) has distribution \(P_{\tilde{\eta}}\), where

\[\tilde{\eta}_{1}=0,\quad\tilde{\eta}_{2}=\eta_{2}/c-(2d/c^{2})\eta_{3},\quad \tilde{\eta}_{3}=\eta_{3}/c^{2}.\]

From this we can see that \(\mathcal{P}=\{P_{\eta}:\eta\in\Lambda_{B}\}\) is invariant under \(\mathcal{G}\) and \(\tilde{\mathcal{G}}\) is a transitive group. Finally, it is easy to see that \(V\circ u(g_{c,d}(x,y)=V\circ u(x,y)\) for all \(c>0\) and \(d\in\mathbb{R}\). Hence, by Theorem 4.10, \(V(X,Y)\) is ancillary for \(\mathcal{P}\). Thus the UMPU-\(\alpha\) test has the form

\[\phi(u)=\begin{cases}1&\text{if }\ V(u)>k\\ 0&\text{otherwise}\,\end{cases}\]

where \(k\) is determined by

\[P_{\eta_{1}=0}(V\circ u(X,Y)>k)=\alpha.\]

Because \(V\circ u(X,Y)\sim t_{(m+n-2)}\), the critical point \(k\) is \(t_{(m+n-2)}(\alpha)\). \(\Box\)

### UMPU test for nonregular family

So far we have been concerned with constructing the UMPU tests for exponential families. In some special cases, UMPU-tests also exist for distributions not in the exponential family. One such special case is the so called nonregular family, where the support of \(X_{i}\) may depend on the parameter values. Rather than discussing this family generally, we will use an example to illustrate how to construct UMPU tests for these problems. For a more general discussion about nonregular family, see Ferguson (1967, page 130).

**Example 4.11**: Let \(X_{1},\ldots,X_{n}\) be i.i.d. random variables uniformly distributed on \((\theta_{1},\theta_{2})\). We are interested in testing

\[H_{0}:\theta_{1}\leq 0\quad\text{vs}\quad H_{1}:\theta_{1}>0.\]

Let \(S=\min_{1\leq i\leq n}X_{i}\) and \(T=\max_{1\leq i\leq n}X_{i}\). The joint density of \(X_{1},\ldots,X_{n}\) is

\[(\theta_{2}-\theta_{1})^{n}{\prod_{i=1}^{n}}I_{(\theta_{1},\theta_{2})}(X_{i}) =(\theta_{2}-\theta_{1})^{n}I_{(-\infty,\theta_{2})}(T)I_{(\theta_{1},\infty)} (S).\]

By the factorization theorem (Theorem 2.4), \((S,T)\) is sufficient for \(X_{1},\ldots,X_{n}\). This tells us that any optimal test can be based on \((S,T)\). The full parameter space is \(\Theta=\{(\theta_{1},\theta_{2}):\theta_{1}<\theta_{2}\}\); the boundary parameter space is \(\Theta_{B}=\{(0,\theta_{2}):\theta_{2}>0\}\). We shall now show that \(T\) is complete and sufficient for \(\Theta_{B}\).

Note that, for any \(s<t\) satisfying \(s>\theta_{1}\) and \(t<\theta_{2}\) we have

\[P(s<S<T<t)=\big{(}(t-s)/(\theta_{2}-\theta_{1})\big{)}^{n}.\]

From this we deduce the conditional density of \(S|T\) and the marginal density of \(T\) as follows:

\[f_{S|T}(s|t) = (n-1)(t-s)^{n-2}(t-\theta_{1})^{-(n-1)}I_{(\theta_{1},t)}(s),\quad \theta_{1}<s<t<\theta_{2}\] \[f_{T}(t) = n(t-\theta_{1})^{n-1}/(\theta_{2}-\theta_{1})^{n},\quad\theta_{1 }<t<\theta_{2}.\]

From the first expression we see that, for any fixed \(\theta_{1}\), \(T\) is sufficient for \(\theta_{2}\). Now let \(g(t)\) be a function of \(t\) such that \(E_{\theta}g(T)=0\). Then

\[\int_{\theta_{1}}^{\theta_{2}}g(t)n(t-\theta_{1})^{n-1}/(\theta_{2 }-\theta_{1})^{n}dt=0\] \[\Rightarrow \int_{\theta_{1}}^{\theta_{2}}g(t)(t-\theta_{1})^{n-1}dt=0.\]

Since this is true for all \(\theta_{2}>\theta_{1}\), we have

\[(\partial/\partial\theta_{2})\int_{\theta_{1}}^{\theta_{2}}g(t)(t-\theta_{1}) ^{n-1}dt=g(\theta_{2})(\theta_{2}-\theta_{1})^{n-1}=0\Rightarrow g(\theta_{2}) =0.\]

This means \(g(t)=0\) for all \(t>\theta_{1}\). That is, for each fixed \(\theta_{1}\), \(T\) is complete for \(\theta_{2}\).

The UMPU-\(\alpha\) test for this problem is

\[\phi(s,t)=\begin{cases}1&\text{ if }\,s>k(t)\\ 0&\text{ otherwise,}\end{cases}\]

where

\[\alpha=E_{\theta_{1}=0}(\phi(S,T)|T=t)=(1-(k(t)/t))^{n-1}.\]

Thus \(k(t)=t(1-\alpha^{1/(n-1)})\). \(\Box\)

### 4.8 Confidence sets

In this section we show that there is a direct association between the confidence sets and the tests of hypotheses, and use it to convert an optimal testing procedure to an optimal confidence set. We first consider the case where \(\theta\) is a scalar, with no nuisance parameters involved. We will confine our discussion to nonrandomized tests. Suppose the distribution of \(X\) belongs to a parametric family \(\mathcal{P}=\{P_{\theta}:\theta\in\Theta\}\), where \(\Theta\subseteq\mathbb{R}\) is a Borel set. Let \(\mathcal{F}_{\Theta}\) be the Borel \(\sigma\)-field on \(\Theta\).

**Definition 4.4**: _A mapping \(C:\Omega_{X}\to\mathcal{F}_{\Theta}\) is called a confident set at confidence level \(1-\alpha\), if_

\[P_{\theta}(\theta\in C(X))=1-\alpha\text{ for all }\,\theta\in\Theta.\]

In the above definition, the probability \(P_{\theta}(\theta\in C(X))\) is to be understood as

\[P_{\theta}(X\in\{x:\theta\in C(x)\}),\]

where \(X\) is random, but \(\theta\) is non-random. To emphasize this point we will say "the probability of \(C(X)\) covering \(\theta\)" rather than "the probability of \(\theta\) belonging to \(C(X)\)", though these two statements are logically equivalent. For each \(a\in\Theta\), let \(\phi_{a}:\Omega_{X}\to\{0,1\}\) be a size \(\alpha\) nonrandomized test for the simple versus composite hypothesis

\[H_{0}:\theta=a\quad\text{vs}\quad H_{1}:\theta\neq a. \tag{4.26}\]

Assume that, for each \(x\in\Omega_{X}\), \(a\mapsto\phi_{a}(x)\) is measurable \(\mathcal{F}_{\Theta}\). Consider the following mappings

\[A:\Theta\to\mathcal{F}_{X},\quad A(a)=\{x\in\Omega_{X}:\phi_{a}( x)=0\},\] \[C:\Omega_{X}\to\mathcal{F}_{\Theta},\quad C(x)=\{a\in\Theta:\phi _{a}(x)=0\}.\]

That is, \(A(a)\) is the 'acceptance region' of the test \(\phi_{a}\), and \(C(X)\) is the collection of \(a\) such that \(H_{0}:\theta=a\) is not rejected based on the observation \(X\). The following relation follows from construction.

**Proposition 4.2**: _For each \(\theta\in\Theta\),_

\[P_{\theta}(\theta\in C(X))=P_{\theta}(X\in A(\theta)).\]

This proposition implies that \(A(\theta)\) is an 'acceptance region' of a size-\(\alpha\) test for (4.26) if and only if \(C\) is a confidence set for \(\theta\) with level \(1-\alpha\).

Obviously there are infinitely many confidence sets of level \(1-\alpha\). Then, how to evaluate the performances of two confidence sets? Intuitively, a higher performing confidence set should have smaller probability of covering a wrong parameter value -- if there were a confidence set that never covers a wrongparameter value, then it could tell us the correct value of \(\theta\) perfectly. Thus it is reasonable to require \(P_{\theta^{\prime}}(\theta\in C(X))\) to be small when \(\theta^{\prime}\neq\theta\), so that \(C(X)\) has stronger discriminating power against incorrect parameter values. This discriminating power is directly related to the power of a test, and the UMPU property can be passed on to confidence set through the relation between a test and a confidence set.

**Definition 4.5**: \(A\)__\((1-\alpha)\)_-level confidence set \(C:\Omega_{X}\rightarrow\mathcal{F}_{\Theta}\) is unbiased if, for any \(\theta^{\prime}\neq\theta\), \(P_{\theta^{\prime}}(\theta\in C(X))\leq 1-\alpha\)._

The next theorem shows that a confidence set constructed from a UMPU test inherits its optimal property.

**Theorem 4.11**: _Let \(A(\theta)\) be the acceptance region of UMPU test of size \(\alpha\) of the hypothesis (4.26). Let \(C(x)=\{\theta:x\in A(\theta)\}\). Then_

_1. \(C\) is a \((1-\alpha)\)-level confidence set;_

_2. If \(D\) is another \((1-\alpha)\)-level confidence set, then_

\[P_{\theta_{1}}(\theta\in C(X))\leq P_{\theta^{\prime}}(\theta\in D(X))\]

_for all \(\theta^{\prime}\neq\theta\)._

1. That \(C(X)\) has level \(1-\alpha\) follows directly from the definition of \(C\). Let \(\theta^{\prime}\neq\theta\). Because \(A(\theta)\) is unbiased, we have

\[P_{\theta^{\prime}}(\theta\in C(X))=P_{\theta^{\prime}}(X\in A(\theta))\leq P_ {\theta}(X\in A(\theta))=P_{\theta}(\theta\in C(X)).\]

Hence \(C\) is unbiased.

2. Let \(D:\Omega_{X}\rightarrow\mathcal{F}_{\Theta}\) be another level \(1-\alpha\) confidence set and let

\[B(\theta)=\{x\in\Omega_{X}:\theta\in D(x)\}.\]

Then \(\phi(x)=1-I_{B(\theta)}(x)\) is a size \(\alpha\) unbiased test. Hence

\[P_{\theta^{\prime}}(\theta\in C(X))=P_{\theta^{\prime}}(X\in A(\theta))\leq P_ {\theta^{\prime}}(X\in B(\theta))=P_{\theta^{\prime}}(X\in B(\theta)),\]

as desired. \(\Box\)

**Example 4.12**: Consider the situation in Example 3.9, where \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\theta,1)\). As we showed in that example, the UMPU-\(\alpha\) test for \(H_{0}:\theta=a\) vs \(H_{1}:\theta\neq a\) has acceptance region

\[A(a)=\{(x_{1},\ldots,x_{n}):a-n^{-1/2}\Phi^{-1}(1-\alpha/2)\leq\bar{x}\leq a+n ^{-1/2}\Phi^{-1}(1-\alpha/2)\}\]

So corresponding confidence set is

\[C(x)=\{a:\bar{x}-n^{-1/2}\Phi^{-1}(1-\alpha/2)\leq\theta\leq\bar{x}+n^{-1/2} \Phi^{-1}(1-\alpha/2)\}.\]

This is a \((1-\alpha)\)-level unbiased confidence interval, and is optimal among all \((1-\alpha)\)-level unbiased confidence intervals. \(\Box\)Now let us consider the problem of obtaining confidence sets for one parameter in the presence of nuisance parameters. Let \(\theta=(\theta_{1},\ldots,\theta_{p})\). Without loss of generality, we assume \(\theta_{1}\) is the parameter of interest. In this section, we assume that all tests have continuous power functions, which holds for exponential families.

**Definition 4.6**: _A \((1-\alpha)\)-level confidence set for \(\theta_{1}\) is any mapping \(C:\Omega_{X}\to\mathcal{F}_{\Lambda_{1}}\) such that_

\[P_{\theta}(a\in C(X))=1-\alpha\]

_for all \(a\in\Lambda_{1}\), \(\theta\in\Theta(a)\). A \((1-\alpha)\)-level confidence set \(C\) for \(\theta_{1}\) is said to be unbiased if_

\[P_{\theta}(a\in C(X))\leq 1-\alpha\]

_for all \(a\in\Lambda_{1}\) and \(\theta\notin\Theta(a)\)._

Let \(A(a)\) be the acceptance region of a size \(\alpha\) test for

\[H_{0}:\theta_{1}=a\quad\text{vs}\quad H_{1}:\theta_{1}\neq a \tag{4.27}\]

The following result is similar to Theorem 4.11; its proof is omitted.

**Theorem 4.12**: _Let \(A(a)\) be the acceptance region of the UMPU test of size \(\alpha\) for hypothesis (4.27). Let \(C(X)=\{a:x\in A(a)\}\). Then_

_1. \(C\) is a \((1-\alpha)\)-level confidence set for \(\theta_{1}\);_

_2. if \(D\) is another \((1-\alpha)\)-level confidence set for \(\theta_{1}\), then_

\[P_{\theta}(a\in C(X))\leq P_{\theta}(a\in D(X))\]

_for all \(a\in\Lambda_{1}\) and \(\theta\notin\Theta(a)\)._

The type of optimality in Theorems 4.11 and 4.12 are called Uniformly Most Accurate (UMA) in the literature, see for example, Ferguson (1967, page 260). In particular, the optimal confidence sets in the two theorems are called the UMA unbiased confidence sets (or UMAU confidence sets). We now discuss specifically how to construct construct UMAU confidence sets for exponential family \(X\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\), where \(\mu_{0}\) is dominated by the Lebesgue measure. By Theorem 4.9, the acceptance region is in the form of an interval

\[A(\theta_{1})=\{(t_{1},\ldots,t_{p}):k_{1}(\theta_{1},t_{2:p})<t_{1}<k_{2}( \theta_{1},t_{2:p})\},\]

where we have added \(\theta_{1}\) as an argument of \(k_{1},k_{2}\) because we are considering the family of all hypothesis of the form (4.27). Suppose \(k_{1}(\theta_{1},t_{2:p})\) and \(k_{2}(\theta_{2},t_{2:p})\) are increasing functions of \(\theta_{1}\) for each fixed \(t_{2:p}\). Let \(k_{1}^{-1}(t_{1},t_{2:p})\) and \(k_{2}^{-1}(t_{1},t_{2:p})\) be the inverses of \(k_{1}(\theta_{1},t_{2:p})\) and \(k_{2}(\theta_{1},t_{2:p})\) for each fixed \(t_{2:p}\). Then, by Theorem 4.12, the UMAU confidence set of level \(1-\alpha\) is the interval

\[S(t_{1},\ldots,t_{n})=[k_{1}^{-1}(t_{1},t_{2:p}),\ k_{2}^{-1}(t_{1},t_{2:p})].\]

The next proposition shows that under some conditions (that are satisfied by exponential families) \(k_{1}(\theta_{1},t_{2:p})\) and \(k_{2}(\theta_{1},t_{2:p})\) are increasing functions of \(\theta_{1}\) for fixed \(t_{2:p}\). Since the setting we have in mind is \(X\sim\mathfrak{E}_{p}^{n}(t_{0},\mu_{0})\), where \(T_{1}|T_{2:p}=t_{2:p}\) has a one-parameter exponential family distribution, we only state the result for the one-parameter case.

**Proposition 4.3**: _Suppose:_

1. _for each_ \(a\in\Theta\)_,_ \[A(a)=\{x:k_{1}(a)<T<k_{2}(a)\}\] (4.28) _defines a size_ \(\alpha\) _UMPU test,_ \(\phi_{a}=1-I_{A(a)}\)_, for the hypothesis (_4.26_);_
2. \(\phi_{a}\) _is a strictly unbiased test in the sense that_ \(E_{b}\phi_{a}>E_{a}\phi_{a}=\alpha\) _whenever_ \(b\neq a\)_;_
3. _the family of distribution of_ \(T\)_, say_ \(\{P_{\theta}:\theta\in\Theta\}\)_, has monotone (nondecreasing) likelihood ratio._

_Then \(k_{1}(a)\) and \(k_{2}(a)\) are strictly increasing in \(a\)._

Let \(a<b\), and \(a,b\in\Theta_{1}\). Let \(\phi_{a}\) and \(\phi_{b}\) be UMPU tests of size \(\alpha\) for testing

\[H_{0}:\theta=a\quad\text{vs}\quad H_{1}:\theta\neq a\] \[H_{0}:\theta=b\quad\text{vs}\quad H_{1}:\theta\neq b,\]

respectively. Since \(\phi_{a}\) and \(\phi_{b}\) are strictly unbiased, we have

\[E_{a}(\phi_{b}(T)-\phi_{a}(T))>0,\quad E_{b}(\phi_{b}(T)-\phi_{a}(T))<0.\]

These rule out the possibilities \(A(a)\subseteq A(b)\) or \(A(b)\subseteq A(a)\). In other words, among the 4 possibilities

\[k_{1}(a)<k_{1}(b)\begin{cases}k_{2}(a)<k_{2}(b)\\ k_{2}(a)\geq k_{2}(b)\end{cases}\quad\text{and }k_{1}(a)\geq k_{1}(b) \begin{cases}k_{2}(a)<k_{2}(b)\\ k_{2}(a)\geq k_{2}(b)\end{cases},\]

we are left with two possibilities

\[k_{1}(a)<k_{1}(b),\ k_{2}(a)<k_{2}(b)\quad\text{or}\quad k_{1}(a)\geq k_{1}(b ),\ k_{2}(a)\geq k_{2}(b). \tag{4.29}\]

Let us further rule out the second possibility of the above two.

In this case the set \(A(b)\) is positioned to the left of \(A(a)\), which implies there is a point \(t_{0}\) such that \[\phi_{b}(t)-\phi_{a}(t)=I_{A(a)}(t)-I_{A(b)}(t)\begin{cases}\leq 0&\text{if}\ \ t\leq t_{0}\\ \geq 0&\text{otherwise.}\end{cases}\]

Let \(r\) denote the likelihood ratio \(dP_{b}/dP_{a}\). Then

\[E_{b}(\phi_{b}-\phi_{a})=\int_{t\leq t_{0}}(\phi_{b}(t)-\phi_{a}(t))r(t)dP_{a} (t)+\int_{t>t_{0}}(\phi_{b}(t)-\phi_{a}(t))r(t)dP_{a}(t).\]

However, because \(\phi_{b}-\phi_{a}\leq 0\) on \(\{t\leq t_{0}\}\), the first term on the right is greater than or equal to \(r(t_{0})\int_{t\leq t_{0}}(\phi_{b}-\phi_{a})dP_{a}\). Similarly, because \(\phi_{b}-\phi_{a}\geq 0\) on \(\{t>t_{0}\}\), the second term on the right is greater than or equal to \(r(t_{0})\int_{t>t_{0}}(\phi_{b}-\phi_{a})dP_{a}\). Hence

\[E_{b}(\phi_{b}-\phi_{a})\geq r(t_{0})E_{a}(\phi_{b}-\phi_{a})<0,\]

which is a contradiction. 

We have learned in Chapter 3 that all the conditions except the strict unbiasedness are satisfied by exponential families. In fact, the strict unbiasedness also holds for exponential families. See Lehmann and Romano (2005, page 112) for a proof of this result.

## Problems

### Prove Proposition 4.1

**4.2.** Let \((\Omega,\mathcal{F})\) be a measurable space. Let \(\mathcal{G}\) be a group of bijections from \(\Omega\) to \(\Omega\), and \(\mathcal{P}\) be an invariant family of probability measures on \((\Omega,\mathcal{F})\). Let \(\tilde{g}\) be the mapping \(P\mapsto P\circ g^{-1}\).

1. Show that \(\tilde{g}:\mathcal{P}\rightarrow\mathcal{P}\) is a bijection;

2. Show that \(\tilde{\mathcal{G}}=\{\tilde{g}:g\in\mathcal{G}\}\) is a group.

**4.3.** Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(U(0,\theta)\) random variables with \(\theta>0\). Show that \(X_{(n)}=\max_{1\leq i\leq n}X_{i}\) is complete and sufficient for \(\{\theta:\theta>0\}\).

**4.4.** Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(U(\theta,\theta+1)\) and \(X_{(k)}\) be the \(k\)th order statistic. Show that \((X_{(1)},X_{(n)})\) is sufficient but not complete for \(\{\theta:\theta\in\mathbb{R}\}\).

**4.5.** Let \(X_{1},\ldots,X_{n}\) be i.i.d. \(N(\theta,\theta^{2})\). Let

\[T=\sum_{i=1}^{n}X_{i},\ S=\sum_{i=1}^{n}X_{i}^{2}.\]

Show that \((T,S)\) is sufficient but not complete for \(\{\theta:\theta\in\mathbb{R}\}\).

**4.6.** Suppose \((\Omega_{1},{\cal F}_{1})\), \((\Omega_{2},{\cal F}_{2})\), and \((\Omega_{3},{\cal F}_{3})\) be measurable spaces and \(P_{1}\) is a probability measure on \((\Omega_{1},{\cal F}_{1})\). Let \(f_{1}:\Omega_{1}\to\Omega_{2}\) be a function measurable with respect to \({\cal F}_{1}/{\cal F}_{2}\) and \(f_{2}:\Omega_{2}\to\Omega_{3}\) be a function measurable with respect to \({\cal F}_{2}/{\cal F}_{3}\). Show that

\[(P_{1}{{}^{\circ}}f_{1}^{-1}){{}^{\circ}}f_{2}^{-1}=P_{1}{{}^{\circ}}(f_{2}{{} ^{\circ}}f_{1})^{-1}.\]

**4.7.** Suppose \(X\sim\mbox{Gamma}(2,\theta)\), \(Y\sim\mbox{Exp}(\eta)\), that is

\[f_{X}(x;\theta)=\theta^{-1}\,x\,e^{-x/\theta},\ \ \ \ x>0,\ \theta>0,\] \[f_{Y}(y;\eta)=\eta^{-1}\,e^{-y/\eta},\ \ \ y>0,\ \eta>0,\]

and \(X\) and \(Y\) are independent.

1. Derive the explicit form of the UMPU size \(\alpha\) test for \(H_{0}:\theta\geq\eta\) versus \(H_{1}:\theta<\eta\).

2. Derive the explicit form of the UMPU size \(\alpha\) test for \(H_{0}:\theta\geq 2\eta\) versus \(H_{1}:\theta<2\eta\).

3. Derive the UMPU size \(\alpha\) test for the hypothesis \(H_{0}:\theta=\eta\) versus \(H_{1}:\theta\neq\eta\). Express the critical point as the solution to an equation.

**4.8.** Suppose \(X\sim b(n,p_{1})\), \(Y\sim b(m,p_{2})\), and \(X\) and \(Y\) are independent. Derive the UMPU size \(\alpha\) test for

\[H_{0}:p_{1}\leq p_{2}\ \ \ \mbox{vs}\ \ \ H_{1}:p_{1}>p_{2}.\]

Derive the conditional distribution involved in the test.

**4.9.** Suppose \(Y_{1},\ldots,Y_{n}\) are random variables defined by

\[Y_{i}=X_{i}\beta+\varepsilon_{i},\]

where \(X_{1},\ldots,X_{n}\) are numbers, \(\beta\) is the regression parameter, and \(\varepsilon_{1},\ldots\varepsilon_{n}\) are an i.i.d. sample from \(N(0,\sigma^{2})\). Here \(X_{1},\ldots,X_{n}\) are treated as fixed numbers (rather than random variables).

1. Find the canonical parameter \(\theta=(\theta_{1},\theta_{2})\), and the complete and sufficient statistics, say \((T_{1},T_{2})\), for the canonical parameter \(\theta\).

2. Write down the generic form of the UMPU-\(\alpha\) test for the hypothesis \(H_{0}:\beta=0\) versus \(H_{1}:\beta\neq 0\), as well as the constraint(s). State the hypotheses \(H_{0}\) and \(H_{1}\) in terms of the canonical parameter.

3. Write down the boundary of the parameter space \(\Theta_{{}^{B}}\). Which statistic is sufficient and complete for \(\theta\in\Theta_{{}^{B}}\)?

4. Let \(Y\) and \(X\) denote the vectors \((Y_{1},\ldots,Y_{n})\) and \((X_{1},\ldots,X_{n})\). Show that the statistic

\[u(Y)=\frac{X^{T}Y}{\sqrt{Y^{T}(I_{n}-XX^{T}/X^{T}X)Y}}\]

is ancillary for \(\Theta_{{}^{B}}\). Here \(I_{n}\) is the \(n\times n\) identity matrix. Comment on the relation between this statistic and the statistic in part 3.

5. Show that the test in part 2 is equivalent to \[\phi(Y)=\begin{cases}1&\text{if}\ \ u(Y)>c_{2},\ \ u(Y)<c_{1}\\ 0&\text{otherwise}\end{cases}\] for some constants \(c_{1}\) and \(c_{2}\) that do not depend on \(Y\).
6. Write down the generic form of the UMPU-\(\alpha\) test for testing \(H_{0}:\sigma^{2}=1\) versus \(H_{1}:\sigma^{2}\neq 1\), as well as the constraint(s). State the hypotheses in terms of the canonical parameter.
7. Write down the boundary of the parameter space \(\Theta_{ B}\) for the hypotheses in part 6. Which statistic is sufficient and complete for \(\theta\in\Theta_{ B}\).
8. Show that the \[v(Y)=Y^{T}(I_{n}-XX^{T}/X^{T}X)Y.\] is ancillary with respect to the boundary \(\Theta_{ B}\) in part 7. Comment on the relation between this statistic and the statistic in part 7.
9. Show that the test in part 6 is equivalent to \[\phi(Y_{1},\ldots,Y_{n})=\begin{cases}1&\text{if}\ \ v(Y)>c_{2},\ \ v(Y)<c_{1}\\ 0&\text{otherwise}\end{cases}\] for some constants \(c_{1}\) and \(c_{2}\) that do not depend on \(Y\).

**4.10**.: Let \(X_{1},\ldots\), \(X_{n}\) be an i.i.d. sample from a bivariate normal distribution

\[N\left(\begin{pmatrix}\mu_{1}\\ \mu_{2}\end{pmatrix},\quad\begin{pmatrix}\sigma_{1}^{2}&\sigma_{1}\sigma_{2} \rho\\ \sigma_{1}\sigma_{2}\rho&\sigma_{2}^{2}\end{pmatrix}\right).\]

We are interested in testing

\[H_{0}:\rho=0\quad\text{vs}\quad H_{1}:\rho\neq 0.\]

1. Express the joint distribution of \(X\) as an exponential family; express the above hypothesis in terms of a canonical parameter.
2. Give the general form of the UMPU size \(\alpha\) test.
3. Show that the UMPU size \(\alpha\) test can be equivalently expressed in terms of \[V=\sqrt{(n-2)}\,R/\sqrt{1-R^{2}},\] where \(R\) is the sample correlation coefficient \[R=\frac{\sum_{i=1}^{n}(X_{i1}-\bar{X}_{1})(X_{i2}-\bar{X}_{2})}{\sqrt{\sum_{i= 1}^{n}(X_{i1}-\bar{X}_{1})^{2}\sum_{i=1}^{n}(X_{i2}-\bar{X}_{2})^{2}}}.\]
4. Show that, under \(\rho=0\), \(V\sim t_{(n-2)}\).

**4.11**.: Suppose \(X\sim\text{Poisson}(\lambda_{1})\), \(Y\sim\text{Poisson}(\lambda_{2})\), and \(X\) and \(Y\) are independant. Derive the UMPU size \(\alpha\) test for

\[H_{0}:\lambda_{1}\leq\lambda_{2}\quad\text{vs}\quad H_{1}:\lambda_{1}>\lambda_ {2}.\]

## References

* Ferguson (1967) Ferguson, T. S. (1967). _Mathematical Statistics: A Decision Theoretic Approach_. Academic.
* Lehmann and Romano (2005) Lehmann, E. L. and Romano, J. P. (2005). _Testing Statistical Hypotheses_. Third edition. Springer.
* Lehmann and Casella (1998) Lehmann, E. L. and Casella, G. (1998). _Theory of point estimation_. Second edition. Springer, New York.

## 5 Basic Ideas of Bayesian Methods

In the Bayesian approach to statistical inference, the parameter \(\theta\) is treated as a random variable, which in this chapter will be written as \(\Theta\), and is assigned a distribution. This distribution represents our prior knowledge - or ignorance - about this parameter before observing the data. Once the data, as represented by a random vector \(X\), is observed, we draw inference about \(\Theta\) by the conditional distribution of \(\Theta|X\). This conditional distribution is called the posterior distribution.

The term "Bayesian" comes from the well known Bayes theorem, which is a formula for computing the probability of several causes after a specific outcome is observed. The approach to inference considered in Chapters 2-4, where \(\theta\) is a fixed number, is called the "frequentist" approach to distinguish it from the new "Bayesian" approach.

Bayesian analysis is a vast area and in this and the next chapter, we can only cover some basic ideas and machineries. For more extensive and specialized discussions, see Berger (1985), Lee (2012), and O'Hagan (1994).

### Prior, posterior, and likelihood

Let

\[(\Omega_{X},\mathcal{F}_{X},\mu_{X}),\quad(\Omega_{\Theta},\mathcal{F}_{\Theta },\mu_{\Theta})\]

be two \(\sigma\)-finite measure spaces, where \(\Omega_{X}\) is a Borel set in \(\mathbb{R}^{n}\), \(\mathcal{F}_{X}\) is the Borel \(\sigma\)-field of subsets in \(\Omega_{X}\), \(\Omega_{\Theta}\) is a Borel set in \(\mathbb{R}^{p}\), \(\mathcal{F}_{\Theta}\) is the Borel \(\sigma\)-field of subsets in \(\Omega_{\Theta}\), and \(\mu_{X}\), \(\mu_{\Theta}\) are \(\sigma\)-finite measures. Let

\[\Omega=\Omega_{X}\times\Omega_{\Theta},\quad\mathcal{F}=\mathcal{F}_{X}\times \mathcal{F}_{\Theta}.\]

Let \(P\) be a probability measure on the measurable space \((\Omega,\mathcal{F})\) that is dominated by \(\mu_{X}\times\mu_{\Theta}\). Although we have used indices such as \(X\) and \(\Theta\), up to this point we have not introduced the random vectors \(X\) and \(\Theta\), and the aboveconstruction are completely independent of any random vectors. In fact, it is equally reasonable to use \(\Omega_{1},\Omega_{2}\) in place of \(\Omega_{X}\), \(\Omega_{\Theta}\). Conceptually, it is clearer to think about measurable spaces and their product before introducing random elements.

Now let \(X\) be the random vector defined by

\[X:\Omega\to\Omega_{X},\quad(x,\theta)\mapsto x,\]

and let \(\Theta\) be the random vector defined by

\[\Theta:\Omega\to\Omega_{\Theta},\quad(x,\theta)\mapsto\theta.\]

The random vector \(X\) represents the data, which in this book is usually a set of \(n\) i.i.d. random variables or random vectors. The random vector \(\Theta\) represents a vector-valued parameter.

The joint probability measure \(P\) determines the marginal distributions \(P_{X}=P\circ X^{-1}\), \(P_{\Theta}=P\circ\Theta^{-1}\), and conditional distributions \(P_{X|\Theta}\) and \(P_{\Theta|X}\). As discussed in Chapter 1, conditional distributions such as \(P_{\Theta|X}\) are to be understood as the mapping

\[P_{\Theta|X}:\ \mathcal{F}_{\Theta}\times\Omega_{X}\to\mathbb{R},\quad(G,x) \mapsto P(\Theta^{-1}(G)|X)_{x}.\]

In the Bayesian context, \(P_{\Theta}\) is called the prior distribution for \(\Theta\); \(P_{X}\) is called the marginal distribution of \(X\); \(P_{\Theta|X}\) is called the posterior distribution; \(P_{X|\Theta}\) is called the likelihood. We are usually given the prior distribution \(P_{\Theta}\) and the likelihood \(P_{X|\Theta}\). Our goal is to compute the posterior distribution \(P_{\Theta|X}\) and extract a variety of information about \(\Theta\) from the posterior distribution.

Since \(P\ll\mu_{X}\times\mu_{\Theta}\), we have

\[P_{X}=P\circ X^{-1}\ll(\mu_{X}\times\mu_{\Theta})\circ X^{-1}=\mu_{X},\]

and similarly \(P_{\Theta}\ll\mu_{\Theta}\). Let

\[f_{X}=dP_{X}/d\mu_{X},\quad\pi_{\Theta}=dP_{\Theta}/d\mu_{\Theta},\quad f_{X \Theta}=dP/d(\mu_{X}\times\mu_{\Theta}).\]

Define

\[f_{X|\Theta}(x|\theta)= \begin{cases}f_{X\Theta}(x,\theta)/\pi_{\Theta}(\theta)&\text{if }\pi_{\Theta}(\theta)\neq 0\\ 0&\text{if }\pi_{\Theta}(\theta)=0,\end{cases}\] \[\pi_{\Theta|X}(\theta|x)= \begin{cases}f_{X\Theta}(x,\theta)/f_{X}(x)&\text{if }f_{X}(x)\neq 0\\ 0&\text{if }f_{X}(x)=0.\end{cases}\]

It can be shown that \(f_{X|\Theta}(x|\theta)\) is the density of \(P_{X|\Theta}\) in the sense that, for each \(A\in\mathcal{F}_{X}\), the mapping \(\theta\mapsto\int_{A}f_{X|\Theta}(x|\theta)d\mu_{X}\) is a version of \(P(A|\Theta)\). See Problem 5.8. The same can be said about \(\pi_{\Theta|X}(\theta|x)\).

The functions \(\pi_{\Theta},\ \pi_{\Theta|X},\ f_{X},\ f_{X|\Theta}\) are called, respectively, the prior density, the posterior density, the marginal density of \(X\), and the likelihood function. If we denote the probability measure \(P_{X|\Theta}(\cdot|\theta)\) by \(P_{\theta}\), then the likelihood \(P_{X|\Theta}\) gives rise to a family of distributions

\[\{P_{\theta}:\ \theta\in\Omega_{\Theta}\}. \tag{5.1}\]

This corresponds to a parametric family of distributions of \(X\) in the frequentist setting. Similarly, the likelihood function \(f_{X|\Theta}(\cdot|\theta)\) is simply the density of \(f_{\theta}(x)\) of \(X\) in the frequentist context.

By construction, the posterior density can be expressed as

\[\pi_{\Theta|X}(\theta|x)=\frac{f_{X\Theta}(x,\theta)}{f_{X}(x)}=\frac{f_{X| \Theta}(x|\theta)\pi_{\Theta}(\theta)}{f_{X}(x)}\propto f_{X|\Theta}(x|\theta) \pi_{\Theta}(\theta).\]

In other words, the posterior density is the product of the prior density and the likelihood function. This fact will be useful in later discussions.

The well known Bayes theorem can be derived as follows. Let \(G\in\mathcal{F}_{\Theta}\), \(A\in\mathcal{F}_{X}\). Then

\[P(\Theta^{-1}(G)\cap X^{-1}(A))=\int_{X^{-1}(A)}P(\Theta^{-1}(G)|X)dP\]

In the special case where \(A=\Omega_{X}\), we have

\[P(\Theta^{-1}(G))=\int_{\Omega}P(\Theta^{-1}(G)|X)dP\]

Hence

\[\frac{P(X^{-1}(A)\cap\Theta^{-1}(G))}{P(\Theta^{-1}(G))}=\frac{\int_{X^{-1}(A) }P(\Theta^{-1}(G)|X)dP}{\int_{\Omega}P(\Theta^{-1}(G)|X)dP}\]

Another way of writing this is

\[P(X\in A|\Theta\in G)=\frac{\int_{X\in A}P(\Theta\in G|X)dP}{\int_{X\in\Omega_{ X}}P(\Theta\in G|X)dP}\]

which is the Bayes Theorem.

### 5.2 Conditional independence and Bayesian sufficiency

Conditional conditional independence plays a prominent role in Bayesian analysis as in many other areas of statistics. In this section we give a careful treatment of this subject. In connection with conditional independence we will also discuss the role played by sufficiency in the Bayesian approach.

Recall that we say \(X\) and \(\Theta\) are independent, and write \(X\mathop{\mathchoice{\hbox{\hbox to 0.0pt{\kern 2.999954pt\vrule height 6.299904pt wid th 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{\kern 2.999954pt \vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 2.099968pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 1.049984pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}}\Theta\), if, for any \(A\in\mathcal{F}_{X}\) and \(G\in\mathcal{F}_{\Theta}\) we have 

[MISSING_PAGE_EMPTY:5982]

\(3\Rightarrow 2\). By Corollary 1.1, it suffices to show that, for any \(B\in\sigma(X)=X^{-1}(\mathcal{F}_{X})\), we have

\[E[I_{B}E(I_{\Theta^{-1}(G)}|X)]=E[I_{B}E(I_{\Theta^{-1}(G)}|T^{ \circ}X)]. \tag{5.3}\]

Because \(B\in X^{-1}(\mathcal{F}_{X})\), there is an \(A\in\mathcal{F}_{X}\) such that \(B=X^{-1}(A)\). So

\[E[I_{B}E(I_{\Theta^{-1}(G)}|X)] = E[I_{X^{-1}(A)}E(I_{\Theta^{-1}(G)}|X)]\] \[= E[E(I_{X^{-1}(A)}I_{\Theta^{-1}(G)}|X)]\] \[= E(I_{X^{-1}(A)}I_{\Theta^{-1}(G)})\] \[= E[E(I_{X^{-1}(A)}I_{\Theta^{-1}(G)}|T^{\circ}X)].\]

By 3, the right hand side is

\[E[E(I_{X^{-1}(A)}|T^{\circ}X)E(I_{\Theta^{-1}(G)}|T^{\circ}X)] = E[E(I_{X^{-1}(A)}E(I_{\Theta^{-1}(G)}|T^{\circ}X)|T^{\circ}X)]\] \[= E[I_{X^{-1}(A)}E(I_{\Theta^{-1}(G)}|T^{\circ}X)]\] \[= E[I_{B}E(I_{\Theta^{-1}(G)}|T^{\circ}X)].\]

Thus (5.3) holds.

\(1\Rightarrow 3\). Let \(A\in\mathcal{F}_{X}\) and \(G\in\mathcal{F}_{\Theta}\). Then

\[E[I_{X^{-1}(A)}I_{\Theta^{-1}(G)}|T^{\circ}X] = E[E(I_{X^{-1}(A)}I_{\Theta^{-1}(G)}|T^{\circ}X,\Theta)|T^{\circ}X]\] \[= E[I_{\Theta^{-1}(G)}E(I_{X^{-1}(A)}|T^{\circ}X,\Theta)|T^{\circ}X]\] \[= E[I_{\Theta^{-1}(G)}E(I_{X^{-1}(A)}|T^{\circ}X)|T^{\circ}X]\] \[= E(I_{\Theta^{-1}(G)}|T^{\circ}X)E(I_{X^{-1}(A)}|T^{\circ}X).\]

Hence 3 holds.

\(3\Rightarrow 1\). Again, by Corollary 1.1, it suffices to show that, for any \(B\in\sigma(T^{\circ}X,\Theta)\),

\[E[I_{B}E(X^{-1}(A)|T^{\circ}X,\Theta)]=E[I_{B}E(X^{-1}(A)|T^{ \circ}X)]. \tag{5.4}\]

Define two set functions

\[Q_{1}(B) = E[I_{B}E(I_{X^{-1}(A)}|T^{\circ}X,\Theta)]\] \[Q_{2}(B) = E[I_{B}E(I_{X^{-1}(A)}|T^{\circ}X)].\]

By Problem 5.6, \(\sigma(T^{\circ}X,\Theta)\) is of the form \(\sigma(\mathcal{P})\), where \(\mathcal{P}\) is the collection of sets

\[\{T^{-1}(C)\times D:C\in\mathcal{F}_{T},D\in\mathcal{F}_{\Theta}\}.\]

Moreover \(\mathcal{P}\) is a \(\pi\)-system, and \(T^{-1}(\Omega_{T})\times\Omega_{\Theta}=\Omega\in\mathcal{P}\).

By Corollary 1.4, it suffices to show that \(Q_{1}(B)=Q_{2}(B)\) for all \(B\in\mathcal{P}\). Let \(B\in\mathcal{P}\). Then \(B=T^{-1}(C)\times D\) for some \(C\in\mathcal{F}_{T}\), \(D\in\mathcal{F}_{\Theta}\). An alternative way of writing the set \(B\) is \[B=(T^{-1}(C)\times\Omega_{\Theta})\cap(\Omega_{X}\times D)=X^{-1}(T^{-1}(C))\cap \Theta^{-1}(D).\]

Hence

\[Q_{1}(B) = E[I_{X^{-1}(T^{-1}(C))}I_{\Theta^{-1}(D)}E(I_{X^{-1}(A)}|T\circ X, \Theta)]\] \[= E[E(I_{X^{-1}(T^{-1}(C))}I_{\Theta^{-1}(D)}I_{X^{-1}(A)}|T\circ X, \Theta)]\] \[= E(I_{X^{-1}(T^{-1}(C))}I_{\Theta^{-1}(D)}I_{X^{-1}(A)}).\]

Because \(I_{X^{-1}(T^{-1}(C))}I_{X^{-1}(A)}=I_{X^{-1}(T^{-1}(C)\cap A)}\), the right hand side above can be rewritten as

\[E(I_{\Theta^{-1}(D)}I_{X^{-1}(T^{-1}(C)\cap A)})=E[E(I_{\Theta^{-1}(D)}I_{X^{-1 }(T^{-1}(C)\cap A)}|T\circ X)]. \tag{5.5}\]

However, by 3,

\[E(I_{\Theta^{-1}(D)}I_{X^{-1}(T^{-1}(C)\cap A)}|T\circ X)=E(I_{\Theta^{-1}(D)} |T\circ X)E(I_{X^{-1}(T^{-1}(C)\cap A)}|T\circ X),\]

where, because \(X^{-1}(T^{-1}(C))\in\sigma(T\circ X)\), the second conditional expectation on the right is

\[E(I_{X^{-1}(T^{-1}(C)\cap A)}|T\circ X) = E(I_{X^{-1}(T^{-1}(C))}I_{X^{-1}(A)}|T\circ X)\] \[= I_{X^{-1}(T^{-1}(C))}E(I_{X^{-1}(A)}|T\circ X).\]

Hence we arrive at

\[E[E(I_{\Theta^{-1}(D)}I_{X^{-1}(T^{-1}(C)\cap A)}|T\circ X)]\] \[=I_{X^{-1}(T^{-1}(C))}E(I_{X^{-1}(A)}|T\circ X)E(I_{\Theta^{-1}(D) }|T\circ X)\] \[=E(I_{X^{-1}(T^{-1}(C))}I_{\Theta^{-1}(D)}|T\circ X)E(I_{X^{-1}(A )}|T\circ X)\]

Substitute this into (5.5) to obtain

\[E(I_{\Theta^{-1}(D)}I_{X^{-1}(T^{-1}(C)\cap A)}) = E[E(I_{B}|T\circ X)E(I_{X^{-1}(A)}|T\circ X)]\] \[= E[I_{B}E(I_{X^{-1}(A)}|T\circ X)]=Q_{2}(B),\]

as desired. \(\Box\)

The first statement of Theorem 5.1 is essentially the same as sufficiency in the frequentist context, as we described in Chapter 2. In fact, if the statement were made pointwise in \(\theta\) rather than almost everywhere in the unconditional probability \(P\), then it is exactly the same as the frequentist sufficiency. The second statement is what one might call "Bayesian sufficiency", which means that, if statistical inference is to be based on posterior probability, then we can replace \(X\) with \(T\circ X\) without changing anything. Both of these statements can be interpreted through conditional independence in statement 3.

The next theorem shows rigorously that frequentist sufficiency implies Bayesian sufficiency. Let \(P_{\theta}\) denote the probability measure \(P_{X|\Theta}(\cdot|\theta)\).

[MISSING_PAGE_EMPTY:5985]

**Example 5.1** Suppose that \(X\sim N(\theta,\sigma^{2})\) and \(\Theta\sim N(\mu,\tau^{2})\), where \(\sigma^{2},\mu\) and \(\tau^{2}\) are treated as known constants. A convenient way of finding \(\pi(\theta|x)\) is to view \(f(x|\theta)\pi_{\Theta}(\theta)\) as a functional of \(\theta\) and identify its functional form with a known density. Since \(f_{X}(x)\) does not depend on \(\theta\) we can ignore it in this process. Specifically, \(f(x|\theta)\pi_{\Theta}(\theta)\) is proportional to (ignoring any multiplicative constant that does not depend on \(\theta\)):

\[\exp\left[-\frac{(x-\theta)^{2}}{2\sigma^{2}}-\frac{(\theta-\mu)^ {2}}{2\tau^{2}}\right] \tag{5.6}\] \[\propto\exp\left[-\left(\frac{1}{2\sigma^{2}}+\frac{1}{2\tau^{2} }\right)\theta^{2}+\left(\frac{x}{\sigma^{2}}+\frac{\mu}{\tau^{2}}\right) \theta\right].\]

The right hand side is of the form \(\exp(-(a\theta^{2}+b\theta))\), where \(a>0\). Let \(c\) and \(c_{1}\) be constants such that

\[(\sqrt{a}\theta+c)^{2}=a\theta^{2}+b\theta+c_{1}.\]

Then, \(2\sqrt{a}c=b\), \(c_{1}=c^{2}\). Hence the right hand side of (5.6) is proportional to

\[\exp[-(\sqrt{a}\theta+b/(2\sqrt{a}))^{2}]= \exp(-(\sqrt{a})^{2}(\theta+b/(2a))^{2}]\] \[= \exp(-(\theta+b/(2a))^{2}/(2(1/\sqrt{2a})^{2})].\]

This corresponds to a normal density with mean

\[E(\theta|x)=-b/(2a)=\left(\frac{x}{\sigma^{2}}+\frac{\mu}{\tau^{2}}\right) \left(\frac{1}{\sigma^{2}}+\frac{1}{\tau^{2}}\right)^{-1} \tag{5.7}\]

and variance

\[\mathrm{var}(\theta|x)=\frac{1}{2a}=\left(\frac{1}{\sigma^{2}}+\frac{1}{\tau^{ 2}}\right)^{-1}. \tag{5.8}\]

A more interpretable way of writing the posterior mean \(E(\theta|x)\) is

\[E(\theta|x)=\left(\frac{\tau^{2}}{\sigma^{2}+\tau^{2}}\right)x+\left(\frac{ \sigma^{2}}{\sigma^{2}+\tau^{2}}\right)\mu.\]

Thus the posterior mean is the weighted average of \(x\) and \(\mu\), the former is the maximum likelihood estimate based on \(\{f(x|\theta):\theta\in\Omega_{\Theta}\}\); the latter is the prior mean based on \(\pi_{\Theta}(\theta)\). If \(\tau^{2}\) is large compared with \(\sigma^{2}\), which means we have little prior information about \(\Theta\), then we give more weight to the maximum likelihood estimate; if \(\tau^{2}\) is small compare with \(x\), then we have more prior information about \(\Theta\), and give more weight to \(\mu\).

To compute the marginal density \(f_{X}(x)\), notice that

\[f_{X}(x)=f(x|\theta)\pi(\theta)/\pi(\theta|x).\]Treating \(x\) as the variable and everything else as constants, the right hand side of the above is proportional to

\[\exp\left[-\frac{(x-\theta)^{2}}{2\sigma^{2}}-\frac{(\theta-\mu)^{2}}{2\tau^{2}} \right]/\exp\left[-\frac{(\theta-E(\theta|x))^{2}}{2\mathrm{var}(\theta|x)}\right]\]

We know that the expression must not involve \(\theta\) -- it is canceled out one way or another. Discarding all the terms depending on \(\theta\), and all constants, the above reduces to

\[\exp\left[-\frac{1}{2}\left(\frac{x^{2}}{\sigma^{2}}-\frac{E^{2}(\theta|x)}{ \mathrm{var}(\theta|x)}\right)\right].\]

By straight forward computation, we obtain that

\[\frac{x^{2}}{\sigma^{2}}-\frac{E^{2}(\theta|x)}{\mathrm{var}(\theta |x)} =\frac{1}{\sigma^{2}+\tau^{2}}\left(x^{2}-2x\mu\right)+\mathrm{ constant}\] \[=\frac{1}{\sigma^{2}+\tau^{2}}\left(x-\mu\right)^{2}+\mathrm{ constant}.\]

From this we see that \(X\) is distributed as \(N(\mu,\sigma^{2}+\tau^{2})\). \(\Box\)

The next example shows how to use sufficient statistic to simplify the computation of posterior distribution.

**Example 5.2** Suppose, conditioning on \(\Theta=\theta\), \(X_{1},\ldots,X_{n}\) is an i.i.d. sample from \(N(\theta,\sigma^{2})\), and \(\Theta\) is distributed as \(N(\mu,\tau^{2})\) for some \(\mu\in\mathcal{R}\) and \(\tau>0\). To find the posterior distribution \(\pi(\theta|X_{1},\ldots,X_{n})\), note that \(\bar{X}\) is sufficient for \(\{N(\theta,\sigma^{2}):\theta\in\mathbb{R}\}\), and by Theorem 5.1,

\[\pi(\theta|X_{1},\ldots,X_{n})=\pi(\theta|\bar{X}).\]

But we know that \(\bar{X}|\Theta=\theta\sim(\theta,\sigma^{2}/n)\) and \(\Theta\sim N(\mu,\tau^{2})\). So by Example 5.1, we have \(\theta|\bar{X}\sim N(E(\theta|\bar{X}),\mathrm{var}(\theta|\bar{X}))\), where

\[E(\theta|\bar{X}) =\] \[\mathrm{var}(\theta|\bar{X}) = \left(\frac{1}{\sigma^{2}/n}+\frac{1}{\tau^{2}}\right)^{-1}.\]

From this we see that

\[E(\theta|\bar{X})\to\bar{X}\ \ \mathrm{and}\ \ \mathrm{var}(\theta|\bar{X})= \sigma^{2}/n+o(1/n),\ \mathrm{as}\ n\to\infty.\]

This result is a special case of a general fact -- in a later chapter we will show that, as \(n\to\infty\), the posterior distribution is concentrated at the maximum likelihood estimate \(\dot{\theta}\) with asymptotic variance \(1/I(\theta)\), where \(I(\theta)\) is the Fisher information. This example also shows that, as the sample size increases, the effect of the prior distribution vanishes, and Bayesian estimate becomes approximately the same as the frequentist estimate. \(\Box\)

### Conjugate families

The posterior density \(P_{\Theta|X}\) is typically difficult to compute explicitly. But in a some special cases explicit and relatively simple solutions exist. One such special case is when the prior and posterior distributions are of the same form.

**Definition 5.2**: _We say that a family \(\mathcal{P}\) of distributions on \((\Omega_{\Theta},\mathcal{F}_{\Theta})\) is conjugate to a likelihood \(P_{X|\Theta}\) if_

\[P_{\Theta}\in\mathcal{P}\Rightarrow P_{\Theta|X}(\cdot|x)\in\mathcal{P}\text{ for each }x\in\Omega_{X}\]

_where \(P_{\Theta|X}\) is derived from \((P_{\Theta},P_{X|\Theta})\)._

By a posterior distribution derived from \((P_{\Theta},P_{X|\Theta})\) we mean

\[dP_{\Theta|X}(\cdot|x)=[f(x|\theta)\pi(\theta)/f_{X}(x)]d\mu_{\Theta}.\]

Of course, if we let \(\mathcal{P}\) to be sufficiently large, then it will always be conjugate to \(P_{X|\Theta}\). So the concept is useful only when \(\mathcal{P}\) is relatively small and is easily manipulated. The next example illustrates the idea.

**Example 5.3**: Suppose that \(X_{1},\ldots,X_{n}|\Theta=\theta\) are i.i.d. Poisson(\(\theta\)) random variables. Then

\[f(X_{1},\ldots,X_{n}|\theta)=\prod_{i=1}^{n}\frac{\theta^{X_{i}}}{X_{i}!}e^{- \theta}=\frac{\theta^{\sum X_{i}}e^{-n\theta}}{\prod(X_{i}!)}. \tag{5.9}\]

Suppose that \(\Theta\) has a Gamma\((\alpha,\beta)\) distribution; that is

\[\pi(\theta)\propto\theta^{\alpha-1}e^{-\theta/\beta},\ \ \beta>0,\ \alpha>1. \tag{5.10}\]

Then the posterior density \(\pi(\theta|X_{1},\ldots,X_{n})\) is proportional to

\[\theta^{\sum X_{i}+\alpha-1}e^{-(n+1/\beta)\theta}.\]

Thus \(\theta|X_{1},\ldots,X_{n}\) has a Gamma\((\alpha^{*},\beta^{*})\) distribution with

\[\alpha^{*}=\alpha+\sum X_{i}\ \ \beta^{*}=\frac{1}{n+1/\beta}.\]

Hence the Gamma family (5.10) is conjugate to the Poisson family (5.9). The advantage of conjugate prior is that (1) the posterior is easy to compute and (2) prior and posterior have the same distribution with parameters bearing the same interpretation. In other words, conditioning on \(X\) simply means updating the parameter of the prior distribution. \(\Box\)

The phenomenon can be generalized to all exponential family distributions. Let us first expand the notation of an exponential family to accommodate transformation of parameters. We now evoke the more general definition (2.5) of an exponential family, \(\mathfrak{E}_{p}(\psi,t,\mu)\) that was introduced in Chapter 2.

**Theorem 5.3**: _If \(P_{\Theta}\in\mathfrak{E}_{p}(\zeta,\psi,\nu)\) and \(P_{X|\Theta}(\cdot|\theta)\in\mathfrak{E}_{p}(\psi,t,\mu)\), then \(P_{\Theta|X}(\cdot|x)\in\mathfrak{E}_{p}(\zeta_{x},\psi,\gamma)\), where_

\[\zeta_{x}(\alpha)=\zeta(\alpha)+t(x),\quad d\gamma(\theta)=d\nu(\theta)/\int e^{ \psi^{T}(\theta)t(x)}d\mu(x).\]

_Proof._ Since \(P_{\Theta}\in\mathfrak{E}_{p}(\zeta,\psi,\nu)\), \(P_{X|\Theta}(\cdot|\theta)\in\mathfrak{E}_{p}(\psi,t,\mu)\), we have

\[\pi_{\Theta}(\theta)=e^{\zeta^{T}(\alpha)\psi(\theta)}/\int e^{\zeta^{T}( \alpha)\psi(\theta)}d\nu(\theta),\quad f(x|\theta)=e^{\psi^{T}(\theta)t(x)}/ \int e^{\psi^{T}(\theta)t(x)}d\mu(x)\]

for some \(\alpha\in\mathbb{R}^{p}\) and \(\theta\in\Theta\). Consequently,

\[\pi(\theta|x)\propto \,e^{(\zeta(\alpha)+t(x))^{T}\psi(\phi)}/[\int e^{\zeta^{T}( \alpha)\psi(\theta)}d\nu(\theta)\int e^{\psi^{T}(\theta)t(x)}d\mu(x)]\] \[\propto \,e^{(\zeta(\alpha)+t(x))^{T}\psi(\phi)}/\int e^{\psi^{T}(\theta) t(x)}d\mu(x).\]

Hence

\[\pi(\theta|x)=\frac{e^{(\zeta(\alpha)+t(x))^{T}\psi(\phi)}/\int e^{\psi^{T}( \theta)t(x)}d\mu(x)}{\int e^{(\zeta(\alpha)+t(x))^{T}\psi(\phi)}/\int e^{\psi^ {T}(\theta)t(x)}d\mu(x)d\nu(\theta)}\]

So if we let

\[\zeta_{x}(\alpha)=\zeta(\alpha)+t(x),\quad d\gamma(\theta)=d\nu(\theta)/\int e ^{\psi^{T}(\theta)t(x)}d\mu(x)d\nu(\theta)\]

then the posterior density belongs to \(\mathfrak{E}_{p}(\zeta_{x},\psi,\gamma)\). \(\Box\)

The algebraic manipulation employed in Example 5.3 and Theorem 5.3 to construct conjugate families can be summarized as the following general scheme. We first inspect the functional form of \(\theta\mapsto f_{\Theta|X}(\theta|x)\) and identify a (often parametric) family of functions of \(\theta\), say \(\mathcal{F}=\{\theta\mapsto g_{\alpha}(\theta):\alpha\in A\}\). If \(\mathcal{F}\) is closed under multiplication; that is, for any \(g_{\alpha},g_{\beta}\in A\), their product \(g_{\alpha}g_{\beta}=g_{\gamma}\) for some \(\gamma\in A\), then any prior density in \(\mathcal{F}\) would be conjugate to \(f_{\Theta|X}\). The resulting posterior density is of the form \(g_{\gamma}\).

**Example 5.4**: Suppose that \(X|\theta\) is distributed as \(N(\theta,\sigma^{2})\), where \(\sigma^{2}\) is treated as a known constant. We want to assign a conjugate prior for \(\theta\). Ignoring any constant (quantities not dependent on \(\theta\)), the function \(\theta\mapsto f_{X|\Theta}(x|\theta)\) is of the form

\[\theta\mapsto\exp\left[\left(-\frac{1}{2\sigma^{2}}\right)\theta^{2}+\left( \frac{\mu}{\sigma^{2}}\right)\theta\right]\]

So the family \(\mathcal{F}\) takes the form

\[\theta\mapsto\exp\left[\left(-\frac{1}{2\alpha_{2}}\right)\theta^{2}+\left( \frac{\alpha_{1}}{\alpha_{2}}\right)\theta\right],\quad\alpha_{1}\in\mathbb{ R},\ \alpha_{2}>0.

\[\exp\left[\left(-\frac{1}{2\alpha_{2}}-\frac{1}{2\beta_{2}}\right)\theta^{2}+ \left(\frac{\alpha_{1}}{\alpha_{2}}+\frac{\beta_{1}}{\beta_{2}}\right)\theta\right]\]

This function also belongs to \(\mathcal{F},\) because

\[-\frac{1}{2\alpha_{2}}-\frac{1}{2\beta_{2}}=-\frac{1}{2\gamma_{2}},\quad\frac {\alpha_{1}}{\alpha_{2}}+\frac{\beta_{1}}{\beta_{2}}=\frac{\gamma_{1}}{\gamma_ {2}},\]

where

\[\gamma_{2}=\left(\frac{1}{\alpha_{2}}+\frac{1}{\beta_{2}}\right)^{-1},\quad \gamma_{1}=\left(\frac{\alpha_{1}}{\alpha_{2}}+\frac{\beta_{1}}{\beta_{2}} \right)\left(\frac{1}{\alpha_{2}}+\frac{1}{\beta_{2}}\right)^{-1}.\]

So any prior of the form \(\theta\sim N(\mu,\tau^{2})\) is conjugate, and the posterior density is of the form derived in Example 5.1. \(\Box\)

The next theorem shows that, if a family \(\mathcal{P}\) is conjugate to \(\mathcal{F}.\) Then the convex hull \(\operatorname{conv}(\mathcal{P})\) is also conjugate to \(\mathcal{F}.\) Recall that, for a generic set \(S,\) the convex hull of \(S\) is the intersection of all convex sets that contains \(S.\) Alternatively, \(\operatorname{conv}(S)\) can be equivalently defined as the set

\[\{\alpha_{1}s_{1}+\cdots+\alpha_{k}s_{k}: \alpha_{1}+\cdots+\alpha_{k}=1,\] \[\alpha_{1}\geq 0,\ldots,\alpha_{k}\geq 0,\ s_{1},\ldots,s_{k} \in S\}.\]

If \(S\) is a class of probability measures, then, for any \(P\in S,\)\(\alpha\in\mathbb{R},\) the product \(\alpha P\) is the measure defined by \(A\mapsto\alpha P(A),\) where \(A\) is a set in a relevant \(\sigma\)-field. A convex combination of a number of probability measures defined as such is called a mixture.

**Theorem 5.4**: _If \(\mathcal{P}\) is conjugate to \(P_{X|\Theta}\), then so is \(\operatorname{conv}(\mathcal{P}).\)_

Proof: Suppose \(P_{\Theta}\in\mathcal{P}.\) Then there exist

\[\alpha_{1}\geq 0,\ldots,\alpha_{k}\geq 0,\quad\sum_{i=1}^{k}\alpha_{i}=1, \quad P_{\Theta}^{1},\ldots,P_{\Theta}^{k}\in\mathcal{P}\]

such that \(P_{\Theta}=\sum_{i=1}^{k}\alpha_{i}P_{\Theta}^{i}.\) It follows that

\[f(x|\theta)dP_{\Theta}=\sum_{i=1}^{k}\alpha_{i}f(x|\theta)dP_{\Theta}^{i}.\]

Integrating both sides over \(\Omega_{\Theta},\) we find

\[f_{X}(x)=\sum_{i=1}^{k}\alpha_{i}m_{i}(x),\]where

\[f_{X}(x)=\int_{\Omega_{\Theta}}f(x|\theta)dP_{\Theta},\quad m_{i}(x)=\int_{\Omega_{ \Theta}}f(x|\theta)dP_{\Theta}^{i},\quad i=1,\ldots,k.\]

Then

\[dP_{\Theta|X}(\cdot|x) = [f(x|\theta)/f_{X}(x)]dP_{\Theta}\] \[= \sum_{i=1}^{k}\alpha_{i}[m_{i}(x)/f_{X}(x)][f(x|\theta)/m_{i}(x) ]dP_{\Theta}^{i}\] \[= \sum_{i=1}^{k}\alpha_{i}(x)dP_{\Theta|X}^{i}(\cdot|x),\]

where \(\alpha_{i}(x)=\alpha_{i}m_{i}(x)/f_{X}(x)\), \(i=1,\ldots,k\). Since

\[\sum_{i=1}^{k}\alpha_{i}(x)=\sum_{i=1}^{m}\alpha_{i}m_{i}(x)/f_{X} (x)=1,\quad\alpha_{i}(x)\geq 0,\ i=1,\ldots,k,\] \[P_{\Theta|X}^{i}(\cdot|x)\in\mathcal{P},\quad i=1,\ldots,k,\]

we have \(P_{\Theta|X}(\cdot|x)\in\operatorname{conv}(\mathcal{P})\). \(\Box\)

### 5.4 Two-parameter normal family

As with frequentist approach, the normal model is a classical and fundamental component of Bayesian analysis. It is important on its own but also provides intuitions to analyze other models and illuminates connections with frequentist methods. In this section we discuss specifically how to assign conjugate prior for the two-parameter normal likelihood \(X|\theta,\sigma^{2}\sim N(\theta,\sigma^{2})\). Our approach is similar to the one used by Lee (2012), but with a more systematic use of notations, such as the Normal Inverse-Chi square distribution. We first introduce the inverse chi-square distribution.

**Definition 5.3**: _A random variable \(T\) is said to have an inverse \(\chi^{2}_{(\kappa)}\) distribution with \(\kappa\) degrees of freedom if \(1/T\) is distributed as \(\chi^{2}_{\kappa}\). In this case we will write \(T\sim\chi^{-2}_{\kappa}\)._

If \(\tau>0\), then we write \(T\sim\tau\chi^{-2}_{(\kappa)}\) if \(T/\tau\sim\chi^{-2}_{(\kappa)}\). The next theorem gives the density of the \(\chi^{-2}_{(\kappa)}\) distribution.

**Theorem 5.5**: _Suppose that \(T\) has an inverse chi-square distribution of \(\kappa\) degrees of freedom, then \(T\) has density_

\[f(t)=\frac{1}{\Gamma(\nu/2)2^{\nu/2}}t^{-\nu/2-1}e^{-1/(2t)}.\]Proof: Let \(U=1/T\). Then \(U\sim\chi_{(\kappa)}^{2}\), which has density

\[g(u)=\frac{1}{\Gamma(\nu/2)2^{\nu/2}}u^{\nu/2-1}e^{-u/2}. \tag{5.11}\]

Therefore

\[f_{T}(t)=g(1/t)|d(1/t)/dt|=g(1/t)/t^{2}. \tag{5.12}\]

Now combine (5.11) and (5.12) to obtain the desired density. 

We now use the inverse chi-square combined with normal distribution to construct a conjugate prior for the likelihood \(N(\theta,\sigma^{2})\). It will prove convenient to introduce the following family of distributions.

**Definition 5.4**: _Suppose \(\lambda\) and \(\phi\) are random variables that take values in \(\mathbb{R}\) and \((0,\infty)\), respectively. If \(\lambda|\phi\sim N(a,\phi/m)\) and \(\phi\sim\tau\chi_{(k)}^{-2}\), then the random vector \((\lambda,\phi)\) is said to have a Normal Inverse Chi-square distribution with parameters \(a,m,\tau,k\), where \(a\in\mathbb{R}\), \(\tau>0\), and \(m,k\) are positive integers. In this case we write_

\[(\lambda,\phi)\sim\mathrm{NICH}(a,m,\tau,k),\]

The parameter \(a\) is interpreted as the mean of \(\lambda\), \(m\) the sample size in the prior distribution of \(\lambda\), \(\tau\) the scale of \(\phi\), and \(k\) the degrees of freedom of inverse chi-square distribution. The next proposition gives the form of the p.d.f. of an NICH random vector.

**Proposition 5.1**: _If \((\lambda,\phi)\sim\mathrm{NICH}(a,m,\tau,k)\), then its p.d.f. is of the following form up to a proportional constant:_

\[\phi^{-1/2}\exp\left[\left(-\frac{1}{2(\phi/m)}\right)\lambda^{2}+\left(\frac{ a}{\phi/m}\right)\lambda\right]\phi^{-k/2-1}\exp\left(-\frac{\tau+ma^{2}}{2 \phi}\right). \tag{5.13}\]

The proof is straightforward and is left as an exercise. It turns out that the NICH family is closed under multiplication, which is very convenient for deriving the conjugate prior and the posterior distribution.

**Proposition 5.2**: _The NICH family is closed under multiplication. That is, if \(a_{1},a_{2}\in\mathbb{R}\), \(m_{1},m_{2},k_{1},k_{2}\) are positive integers, and \(\tau_{1},\tau_{2}\) are positive numbers, then_

\[\mathrm{NICH}(a_{1},m_{1},\tau_{1},k_{1})\times\mathrm{NICH}(a_{2},m_{2},\tau_{ 2},k_{2})\propto\mathrm{NICH}(a_{3},m_{3},\tau_{3},k_{3}), \tag{5.14}\]

_where_

\[m_{3}= \,m_{1}+m_{2},\] \[a_{3}= \,(m_{1}a_{1}+m_{2}a_{2})/m_{3},\] \[\tau_{3}= \,\tau_{1}+\tau_{2}+m_{1}a_{1}^{2}+m_{2}a_{2}^{2}-m_{3}a_{3}^{2},\] \[k_{3}= \,k_{1}+k_{2}+3.\]

[MISSING_PAGE_EMPTY:5993]

[MISSING_PAGE_EMPTY:5994]

An equivalent way of writing the conclusion of the above theorem is

\[\phi|x\sim\tau(x)\chi_{(n+k)}^{-2},\quad\lambda|\phi,x\sim N\left(\mu(x),\ \frac{\phi}{n+m}\right). \tag{5.18}\]

Assigning the prior for \((\lambda,\phi)\) in this way also gives very nice interpretations of the various parameters in the prior distribution. If we imagine our prior knowledge about \((\phi,\lambda)\) is in the form of a sample, then \(m\) would be the sample size, \(a\) would be the sample average, and \(\tau/k\) would be the sample mean squared error.

We now develop the marginal posterior distribution of \(\lambda|x\) based on the above joint posterior distribution of \((\lambda,\phi)|x\). By (5.18) we have

\[\frac{\lambda-\mu(x)}{\sqrt{\phi/(n+m)}}|(x,\phi)\sim N(0,1),\quad\frac{\tau(x )}{\phi}|x\sim\chi_{(n-1+\kappa)}^{2}. \tag{5.19}\]

Since the density of \(N(0,1)\) does not depend on \((x,\phi)\), we have from the first relation that

\[\frac{\lambda-\mu(X)}{\sqrt{\phi/(n+m)}}\,\hbox{$1\!\!1$}\,(\phi,X), \tag{5.20}\]

which implies

\[\frac{\lambda-\mu(X)}{\sqrt{\phi/(n+m)}}\,\hbox{$1\!\!1$}\,\frac{\tau(X)}{\phi}. \tag{5.21}\]

By relations (5.19) and (5.21), if we let

\[V=\frac{\sqrt{n+m}(\lambda-\mu(X))}{\sqrt{\tau(X)/(n-1+\kappa)}},\]

then \(V\sim t_{(n-1+k)}\). Moreover, (5.20) also implies

\[\frac{\lambda-\mu(X)}{\sqrt{\phi/(n+m)}}\,\hbox{$1\!\!1$}\,\phi|X,\ \hbox{which in turn implies}\ \ \frac{\lambda-\mu(X)}{\sqrt{\phi/(n+m)}}\,\hbox{$1\!\!1$}\,\frac{\tau(X)}{\phi}|X.\]

So the posterior distribution of \(V\) given \(X\) is also distributed as \(t_{(n-1+\kappa)}\). In Chapter 6 we will discuss how to use this fact to draw Bayesian inference about \(\lambda\).

### 5.5 Multivariate Normal likelihood

We now consider the more general situation where \(X\) is a \(p\)-dimensional random vector distributed as multivariate Normal \(N(a,\Phi)\), where \(a\in\mathbb{R}^{p}\), and \(\Phi\in\mathbb{R}^{p\times p}\) is a positive definite matrix. When \(\Phi\) is known, the situation is similar to Example 5.1, and it is relatively easy to generalize that example to derive the conjugate prior, posterior density, and the marginal density of \(X\). We leave this as an exercise (see Problem 5.11).

When \(\Phi\) is unknown and treated as a random parameter, we can develop the conjugate prior and posterior distribution in a parallel manner as Section 5.4. We first define the inverse Wishart distribution, which is a generalization of the inverse \(\chi^{2}\)-distribution.

**Definition 5.5**: _Suppose \(Z_{1},\ldots,Z_{k}\) are \(p\)-dimensional random vectors that are i.i.d. \(N(0,\Sigma)\), where \(\Sigma\) is a positive definite matrix. Then the distribution of \(U=Z_{1}Z_{1}^{T}+\cdots+Z_{k}Z_{k}^{T}\) is called the Wishart distribution with scale parameter \(\Sigma\) and degrees of freedom \(p\). We write this as_

\[U\sim W_{p}(\Sigma,k).\]

_If \(V\) is a \(p\)-dimension vector whose inverse \(V^{-1}\) is distributed as \(W_{p}(\Sigma,k)\), then we say \(V\) has an inverse Wishart distribution and write this as_

\[V\sim W_{p}^{-1}(\Sigma,k).\]

The density functions of the Wishart and the inverse Wishart distribution are given by the next Proposition. For more information about these distributions, see, for example, Mardia, Kent, and Bibby (1979).

**Proposition 5.4**: _The density of \(U\sim W_{p}(\Sigma,k)\) is given by_

\[\frac{1}{2^{kp/2}\Gamma_{p}(k/2)}|\Sigma|^{-k/2}|U|^{(n-k-1)/2}\exp\left[- \frac{1}{2}\mbox{tr}(\Sigma^{-1}U)\right],\]

_and that of \(V\sim W_{p}^{-1}(\Sigma,k)\) is given by_

\[\frac{1}{2^{kp/2}\Gamma_{p}(k/2)}|\Sigma|^{k/2}|V|^{-(k+p+1)/2}\exp\left[- \frac{1}{2}\mbox{tr}(\Sigma V^{-1})\right],\]

_where \(|\cdot|\) represents the determinant of a matrix, and \(\Gamma_{p}\) represents the multivariate Gamma function, defined by_

\[\Gamma_{p}(a)=\pi^{p(p-1)/2}{\prod_{j=1}^{p}\Gamma[a+(1-j)/2]}.\]

As in Section 5.4, for developing the conjugate prior and posterior distribution for a multivariate Normal likelihood, it is convenient to introduce the following class of distributions.

**Definition 5.6**: _Suppose \(\lambda\) is a random vector that takes values in \(\mathbb{R}^{p}\), and \(\Phi\) is a random matrix that takes values in the set of all positive definite matrices. If \(\lambda|\Phi\sim N(a,\Sigma/m)\) and \(\Phi\sim W_{p}^{-1}(\Sigma,k)\), then the random element \((\lambda,\Phi)\) is said to have a Normal Inverse Wishart distribution with parameters \(a,m,\Sigma,k\), where \(a\in\mathbb{R}^{p}\), \(\Sigma\) is a positive definite matrix, and \(m,k\) are positive integers. We write this as_

\[(\lambda,\Phi)\sim\mathrm{NIW}(a,m,\Sigma,k).\]The form of the density of \(\mathrm{NIW}(a,m,\Sigma,k)\) can be derived from the product of \(N(a,\Sigma/m)\) and \(W_{p}^{-1}(\Sigma,k)\). This is stated as the next proposition. The proof is left as an exercise.

**Proposition 5.5**: _If \((\lambda,\Phi)\sim\mathrm{NIW}(a,m,\Sigma,k)\), then the p.d.f. of \((\lambda,\Phi)\) is proportional to_

\[|\Phi|^{-(k+p+2)/2}\exp[-m\lambda^{T}\Phi^{-1}\lambda/2+ma^{T}\Phi^{-1}\lambda] \exp\{-\mbox{tr}[\Phi^{-1}(\Sigma+maa^{T})/2]\}.\]

Similar to the NICH family, the NIW family is also closed under multiplication, as detailed by the next proposition. The proof is left as an exercise.

**Proposition 5.6**: _Suppose \(m_{1},k_{1},m_{2},k_{2}\) are positive integers, \(a_{1},a_{2}\) are vectors in \(\mathbb{R}^{p}\), and \(\Sigma_{1}\), \(\Sigma_{2}\) are positive definite matrices, then_

\[\mathrm{NIW}(a_{1},m_{1},\Sigma_{1},k_{1})\times\mathrm{NIW}(a_{2},m_{2}, \Sigma_{2},k_{2})\propto\mathrm{NIW}(a_{3},m_{3},\Sigma_{3},k_{3}),\]

_where_

\[m_{3}= \,m_{1}+m_{2},\] \[a_{3}= \,(m_{1}a_{1}+m_{2}a_{2})/m_{3},\] \[\Sigma_{3}= \,\Sigma_{1}+\Sigma_{2}+m_{1}a_{1}a_{1}^{T}+m_{2}a_{2}a_{2}^{T}-m _{3}a_{3}a_{3}^{T},\] \[k_{3}= \,k_{1}+k_{2}+p+2.\]

Note that, in the univariate case, \(p=1\), so \(k_{3}=p+3\), agreeing with Proposition 5.2.

We now develop the conjugate prior and posterior distribution for the multivariate Normal likelihood. Suppose, conditioning on \((\lambda,\Phi)\), \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\lambda,\Phi)\). Let

\[T=n^{-1}\sum_{i=1}^{n}X_{i},\quad S=\sum_{i=1}^{n}(X_{i}-\bar{X})(X_{i}-\bar{X} )^{T}.\]

Then the following statements about \((T,S)\) hold true.

**Proposition 5.7**: _If \((T,S)\) are as defined in the last paragraph, then_

1. \((T,S)\) _is sufficient for_ \((\lambda,\Phi)\)_;_
2. \(T\coprod S|(\lambda,\Phi)\)_;_
3. \(S|\Phi\sim W_{p}(\Phi,n-1)\)_,_ \(T|(\lambda,\Phi)\sim N(\lambda,\Phi/n)\)_._

Again, abbreviating \((X_{1},\ldots,X_{n})\) as \(X\). From part 1 of this proposition we see that the conditional distribution of \((\lambda,\Phi)|X\) is the same as the conditional distribution of \((\lambda,\Phi)|(T,S)\). So we only need to consider the likelihood \((T,S)|(\lambda,\Phi)\). By part 2 and part 3 of the proposition, this likelihood is of the form \(N(\lambda,\Phi/n)\times W_{p}(\Sigma,n-1)\). The next proposition asserts that, as a function of \((\lambda,\Phi)\), this likelihood is of the NIW form.

**Proposition 5.8**: _Let \(f(t,s|\lambda,\Phi)\) be the density of \(N(\lambda,\Phi/m)\) and \(f(s|\Phi)\) be the density of \(W_{p}(\Phi,n-1)\). Then the function \((\lambda,\Phi)\mapsto f(t,s|\lambda,\Phi)f(s|\Phi)\) is proportional to the density of \(\mathrm{NIW}(T,n,S,n-p-2)\)._

The proof is similar to that of Proposition 5.3 and so it is left as an exercise. When \(p=1\), the degrees of freedom in \(\mathrm{NIW}(T,n,S,n-p-2)\) reduces to \(p-3\), agreeing with Proposition 5.3. From Propositions 5.5 to 5.8, we can easily derive the posterior distribution of \((\lambda,\Phi)\).

**Theorem 5.7**: _Suppose, conditioning on \((\lambda,\phi)\), \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\lambda,\Phi)\), and the prior distribution of \((\lambda,\Phi)\) is \(\mathrm{NIW}(a,m,\Sigma,k)\). Then the posterior distribution of \((\lambda,\Phi)\) given \(X=x\) is_

\[\mathrm{NIW}\left(\mu(x),m+n,\Sigma(x),n+k\right),\]

_where_

\[\mu(x)= \frac{ma+nt(x)}{m+n},\] \[\Sigma(x)= \Sigma+s(x)+ma^{2}+nt^{2}(x)-(m+n)\mu^{2}(x),\]

_where \(t(x)\) and \(s(x)\) are the observed values of \(T\) and \(S\)._

### Improper prior

#### 5.6.1 The motivation idea of improper prior

Sometimes we do not have much prior knowledge about the parameter \(\Theta\) and would like to reflect this uncertainty in the Bayesian analysis. In this case it is desirable to use a prior distribution that is in some sense "flat." Unless the sample space \(\Omega_{\Theta}\) is a bounded set, a flat distribution cannot be a finite measure. This leads us to the notion improper priors.

**Definition 5.7**: _An improper prior is an infinite but \(\sigma\)-finite measure on \(\Omega_{\Theta}\)._

One might ask if there is no prior information about the parameter, then why should we use the Bayesian method in the first place? Under some circumstances the Bayesian method has some technical advantages over the frequentist method. For example, when dealing with nuisance parameters we can simply integrate them out from the posterior distribution, rather than trying to condition on a sufficient statistic for the nuisance parameter, which may not be available.

A frequently used improper prior is the Lebesgue measure, which can be viewed as the uniform distribution on the whole line. The posterior distribution corresponding to an improper prior is typically a probability measure, and it often leads to similar estimates to those given by the frequentist method. However, the marginal distribution \(f_{X}(x)\) corresponding to an improper prior is typically also improper, as illustrated by the following example.

**Example 5.5**: Suppose that \(X|\theta\sim N(\theta,\phi)\) where \(\phi\) is treated as a known constant. Let \(\pi_{\Theta}(\theta)=1\) for all \(\theta\in\mathbb{R}\). That is, the improper prior is the Lebesgue measure. In this case, we have

\[\pi_{\Theta|X}(\theta|x)\propto 1\times e^{-\frac{1}{2}\frac{(x-\theta)^{2}}{ \phi}}.\]

We see that \(\Theta|x\sim N(x,\sigma^{2})\). The marginal distribution of \(X\) can be obtained formally from the equation

\[\pi_{\Theta|X}(\theta|x)f_{X}(x)=f_{X|\Theta}(x|\theta)\pi_{\Theta}(\theta)=f_{ X|\Theta}(x|\theta).\]

Therefore, \(f_{X}(x)=f_{X|\Theta}(x|\theta)/\pi_{\Theta|X}(\theta|x)=1\), which is an improper distribution. \(\Box\)

As with proper priors, we can also assign improper priors sequentially when there are several parameters.

**Example 5.6**: Suppose that \(X_{1},\ldots,X_{n}|\lambda,\phi\) are i.i.d. \(N(\lambda,\phi)\), where both \(\lambda\) and \(\phi\) are unknown. Let \((S,T)\) be defined as in Section 5.4. Recall that the likelihood \((\lambda,\phi)\mapsto f(s,t|\lambda,\phi)\) is of the form

\[\phi^{-1/2}\exp\left[\left(-\frac{1}{2(\phi/n)}\right)\lambda^{2}+\left(\frac {t}{\phi/n}\right)\lambda\right]\phi^{-(n-3)/2-1}\exp\left(-\frac{s}{2\phi}\right)\]

If we assign \((\lambda,\phi)\) the improper prior \(\pi(\phi)=1/\phi\) and \(\pi(\lambda|\phi)=1\), the posterior density is of the form

\[\phi^{-1/2}\exp\left[\left(-\frac{1}{2(\phi/n)}\right)\lambda^{2}+\left(\frac{ t}{\phi/n}\right)\lambda\right]\phi^{-(n-1)/2-1}\exp\left(-\frac{s}{2\phi} \right).\]

Equivalently, the joint posterior distribution of \((\lambda,\phi)\) can be expressed as

\[\phi|t,s\sim s\chi_{(n-1)}^{-2},\quad\lambda|\phi,t,s\sim N(t,\phi/n)\]

These imply

\[\frac{\lambda-t}{\sqrt{\phi/n}}\Bigg{|}\,x\sim N(0,1),\quad\left.\frac{s}{ \phi}\right|x\sim\chi_{(n-1)}^{2},\quad\left.\frac{\lambda-t}{\sqrt{\phi/n}} \right.\hbox{$1\!\!1$}\,\frac{s}{\phi}\Bigg{|}\,x.\]

So if we want to make posterior inference about the mean parameter \(\lambda\), then we can use the fact

\[\frac{\sqrt{n}(\lambda-t)}{\sqrt{s/(n-1)}}\Bigg{|}\,x\sim t_{(n-1)}. \tag{5.22}\]

Since the right hand side does not depend on \(x\), this relation also implies\[\frac{\sqrt{n}(\lambda-t)}{\sqrt{s/(n-1)}}\,{\rm 1\mskip-4.5mu l}\,X,\quad\frac{ \sqrt{n}(\lambda-t)}{\sqrt{s/(n-1)}}\sim t_{(n-1)}.\]

Interestingly, the statistic \(\sqrt{n}(\lambda-t)/\sqrt{s/(n-1)}\) has the same form as the \(t\) statistic in the frequentist setting.

If we want to draw posterior inference about \(\phi\), then we can use the fact

\[\frac{s}{\phi}\bigg{|}\,x\sim\chi^{2}_{(n-1)} \tag{5.23}\]

Again, this has the same form as the chi-square test in the frequentist setting. In a later chapter we will see that posterior inference based on (5.22) and (5.23) are exactly the same as discussed in Chapter 4. \(\Box\)

It is not always clear what is the real meaning of being noninformative. Note that in the above example we assigned \(P_{\Phi}(\phi)=1/\phi\) to the variance parameter \(\Phi\), which is not "flat". Is there any reason to use such priors? Also, suppose we assign a uniform prior to a parameter \(\Theta\), then any nonlinear monotone transformation of \(\Theta\) would have a nonuniform distribution. Given that a one-to-one transformation of parameter does not change the family of distributions, it is not clear to which transformation should we assign the uniform prior.

#### Haar measures

The Haar measures are generalizations of the Lebesgue measure, or the (improper) uniform distribution over the whole real line. Lebesgue measure is the Haar measure under location transformations (or translations): \(\theta\mapsto\theta+c\). The set of all translations form a group, and the Lebesgue is invariant under this group of transformations. This leads naturally to the question: can we construct improper distributions that are invariant under other group of transformation. In general, are there something like Lebesgue measure for _any_ group of transformations? If so, then that would be a good candidate for improper prior in the more general setting.

It turns out that there are actually two types of improper distributions that are invariant under a group of transformations: the left and right Haar measure. Let \(\mathcal{G}\) be a group of transformations from \(\Omega_{\Theta}\) on to \(\Omega_{\Theta}\), indexed by members of \(\Omega_{\Theta}\); that is

\[\mathcal{G}=\{g_{t}:t\in\Omega_{\Theta}\}. \tag{5.24}\]

In a statistical problem, the group \(\mathcal{G}\) is induced by an invariant family of distributions, as discussed in Section 4.4. Specifically, suppose \(\mathcal{F}=\{P_{\theta}:\theta\in\Omega_{\Theta}\}\) is parametric family of distributions of \(X\). We assume that there is a group, say \(\mathcal{H}\), of transformations from \(\Omega_{X}\) to \(\Omega_{X}\) such that, for each \(h\in\mathcal{H}\)the induced measure \(P_{\theta}{\circ}h^{-1}\) is a member of \({\cal F}\). That is, there exists a \(\tilde{\theta}\in\Omega_{\Theta}\) such that \(P_{\tilde{\theta}}=P_{\theta}{\circ}h^{-1}\). This induces a transformation from \(\Omega_{\Theta}\) to \(\Omega_{\Theta}\) that maps \(\theta\) to \(\tilde{\theta}\). The collections of all these mappings can be shown to be a group, and this is the group \({\cal G}\) in (5.24), which is our starting point for constructing the Haar measures.

Each \(g_{t}\in{\cal G}\) induces two types of transformations of \(\theta\):

\[L_{t}(\theta)=g_{t}(\theta),\quad R_{t}(\theta)=g_{\theta}(t),\]

where \(L_{t}\) is called the left transformation, and \(R_{t}\) the right transformation.

**Definition 5.8**: \(A\) _measure \(\Pi\) on \(\Omega_{\Theta}\) is the left (right) Haar measure if it satisfies_

\[\Pi=\Pi{\circ}L_{t}^{-1}\quad(\Pi=\Pi{\circ}R_{t}^{-1}) \tag{5.25}\]

_for all \(t\in\Omega_{\Theta}\)._

We can use the equations in (5.25) to determine the forms of the left and right Haar measures. Take the left Haar measure as an example. Suppose \(\pi(\theta)\) is the density of \(\Pi\), and assume that it is differentiable. Then the distribution of \(\tilde{\theta}=L_{t}(\theta)\) is \(\Pi{\circ}L_{t}^{-1}\), with density

\[\tilde{\pi}(\tilde{\theta})=\pi(L_{t}^{-1}(\tilde{\theta}))|\det(\partial L_{t }^{-1}(\tilde{\theta})/\partial\tilde{\theta})|\]

where \(\det(\cdot)\) denotes the determinant of a matrix and \(|\det(\cdot)|\) its absolute value. The first equation in (5.25) implies that \(\tilde{\pi}\) and \(\pi\) are the same density functions; that is,

\[\pi(L_{t}^{-1}(\theta))|\det(\partial L_{t}^{-1}(\theta)/\partial\theta^{T})|= \pi(\theta),\]

for all \(\theta,t\in\Omega_{\Theta}\). Fix \(\theta\) at any \(\theta_{0}\in\Omega_{\Theta}\), and we have

\[\pi(L_{t}^{-1}(\theta_{0}))=\frac{\pi(\theta_{0})}{|\det(\partial L_{t}^{-1}( \theta_{0})/\partial\theta^{T})|}\equiv g(t),\]

for all \(t\in\Omega_{\Theta}\). Let \(h(t)=L_{t}^{-1}(\theta_{0})\). Then it can be shown that \(h:\Omega_{\Theta}\to\Omega_{\Theta}\) is a bijection. So we have \(\pi(h(t))=g(t)\) for all \(t\in\Omega_{\Theta}\). Putting \(\theta=h(t)\), we have

\[\pi(\theta)=g(h^{-1}(\theta)).\]

The right Haar measure can be derived in exactly the same way. In the next three examples we use this method to develop the left and right Haar measures for three groups of transformations: the location transformation group, the scale transformation group, and the location-scale transformation group.

**Example 5.7** Let \(\Omega_{\Theta}=\mathbb{R}\), and let \(\mathcal{G}\) be the group of transformations

\[g_{c}(\theta)=\theta+c,\quad c\in\mathbb{R}.\]

Then

\[L_{c}(\theta)=g_{c}(\theta)=\theta+c,\quad R_{c}(\theta)=g_{\theta}(c)=\theta+c.\]

Let us first find the left Haar measure. Let \(\tilde{\theta}=L_{c}(\theta)=\theta+c\). Then \(\theta=L_{c}^{-1}(\tilde{\theta})=\tilde{\theta}-c\). If the density of \(\theta\) is \(\pi\), then the density of \(\tilde{\theta}\) is \(\tilde{\pi}=\pi(\tilde{\theta}-c)\). So we want \(\tilde{\pi}\) and \(\pi\) to be the same function for all \(c\); that is,

\[\pi(\theta-c)=\pi(\theta)\]

for all \(\theta\in\mathbb{R}\), \(c\in\mathbb{R}\). Taking \(\theta=0\), we have \(\pi(-c)=\pi(0)\) for all \(c\in\mathbb{R}\), implying \(\pi(\theta)=\pi(0)\) for all \(\theta\in\mathbb{R}\). That is, the left Haar measure is proportional to the Lebesgue measure. Since \(L_{t}=R_{t}\), the right Haar measure is also proportional to the Lebesgue measure. \(\Box\)

**Example 5.8** Let \(\Omega_{\Theta}=(0,\infty)\), and let \(\mathcal{G}\) be the group of transformations

\[g_{a}(\theta)=a\theta,\quad a\in(0,\infty).\]

Then

\[L_{a}(\theta)=g_{a}(\theta)=a\theta,\quad R_{a}(\theta)=g_{a}(\theta)=a\theta.\]

To determine the left Haar measure, let \(\tilde{\theta}=L_{a}(\theta)=a\theta\), then \(\theta=L_{a}^{-1}(\tilde{\theta})=\tilde{\theta}/a\). If the density of \(\theta\) is \(\pi\), then the density of \(\tilde{\theta}\) is

\[\tilde{\pi}=\pi(\tilde{\theta}/a)\frac{\partial(\tilde{\theta}/a)}{\partial \tilde{\theta}}=\pi(\tilde{\theta}/a)/a.\]

The relation \(\Pi^{\circ}L_{c}^{-1}=\Pi\) implies that \(\tilde{\pi}\) and \(\pi\) are the same density function. So we want \(\tilde{\pi}\) and \(\pi\) to be the same function for all \(a\); that is,

\[\pi(\theta/a)/a=\pi(\theta)\]

for all \(\theta\in(0,\infty)\), \(a\in(0,\infty)\). Take \(\theta=1\). Then we have \(\pi(1/a)=a\pi(1)\) for all \(a\in(0,\infty)\), implying \(\pi(\theta)=\pi(1)/\theta\) for all \(\theta\in\mathbb{R}\). That is, the left Haar measure has density proportional to \(1/\theta\). Since \(L_{t}=R_{t}\), the right Haar measure also has density proportional to \(1/\theta\). \(\Box\)

**Example 5.9** Let \(\Omega_{\Theta}\) be the parameter space of two parameters, a real number \(\mu\) and a positive number \(\sigma\). That is \(\Omega_{\Theta}=\mathbb{R}\times(0,\infty)\). Let \(\mathcal{G}\) be the group of transformations\[g_{b,c}(\mu,\sigma)=(c\mu+b,c\sigma),\quad(b,c)\in\mathbb{R}\times(0,\infty).\]

Then

\[L_{b,c}(\mu,\sigma)= g_{b,c}(\mu,\sigma)=(c\mu+b,c\sigma),\] \[R_{b,c}(\mu,\sigma)= g_{\mu,\sigma}(b,c)=(\sigma b+\mu,\sigma c).\]

To determine the left Haar measure, let \((\tilde{\mu},\tilde{\sigma})=L_{b,c}(\mu,\sigma)=(c\mu+b,c\sigma)\). Then

\[(\mu,\sigma)=L_{b,c}^{-1}(\tilde{\mu},\tilde{\sigma})=\left(\frac{\tilde{\mu}- b}{c},\frac{\tilde{\sigma}}{c}\right).\]

If the density of \((\mu,\sigma)\) is \(\pi(\mu,\sigma)\), then the density of \((\tilde{\mu},\tilde{\sigma})\) is

\[\tilde{\pi}(\tilde{\mu},\tilde{\sigma})=\pi\left(\frac{\tilde{\mu}-b}{c},\frac {\tilde{\sigma}}{c}\right)\left|\det\left(\frac{\partial((\tilde{\mu}-b)/c, \tilde{\sigma}/c)}{\partial(\tilde{\mu},\tilde{\sigma})^{T}}\right)\right|.\]

The determinant on the right-hand side is

\[\det\begin{pmatrix}1/c&0\\ 0&1/c\end{pmatrix}=c^{-2}.\]

Hence we have the equation

\[\tilde{\pi}(\tilde{\mu},\tilde{\sigma})=c^{-2}\pi\left(\frac{\tilde{\mu}-b}{c},\frac{\tilde{\sigma}}{c}\right).\]

The relation \(\Pi\circ L_{t}^{-1}=\Pi\) implies \(\tilde{\pi}\) and \(\pi\) are the same function for all \(b,c\); that is,

\[c^{-2}\pi\left(\frac{\mu-b}{c},\frac{\sigma}{c}\right)=\pi(\mu,\sigma)\]

for all \(\mu,b\in\mathbb{R}\), \(c,\sigma\in(0,\infty)\). Take \(\mu=0,c=1\), and we have

\[c^{-2}\pi\left(-\frac{b}{c},\frac{1}{c}\right)=\pi(0,1).\]

Let \(\mu=-b/c\), \(\sigma=1/c\). Then \(c=1/\sigma\), \(b=-\mu/\sigma\), and

\[\sigma^{2}\pi\left(\mu,\sigma\right)=\pi(0,1)\ \Rightarrow\ \pi\left(\mu, \sigma\right)=\pi(0,1)/\sigma^{2}.\]

So, the left Haar measure has density proportional to \(1/\sigma^{2}\).

To determine the right Haar measure, let

\[(\tilde{\mu},\tilde{\sigma})=R_{b,c}(\mu,\sigma)=g_{\mu,\sigma}(b,c)=(\sigma b +\mu,1/\sigma).\]

Then\[(\mu,\sigma)=R_{b,c}^{-1}(\tilde{\mu},\tilde{\sigma})=\left(\tilde{\mu}-\frac{b \tilde{\sigma}}{c},\frac{\tilde{\sigma}}{c}\right).\]

The density of \((\tilde{\mu},\tilde{\sigma})\) is

\[\tilde{\pi}(\tilde{\mu},\tilde{\sigma})=\pi\left(\tilde{\mu}-\frac{b\tilde{ \sigma}}{c},\frac{\tilde{\sigma}}{c}\right)\left|\det\left(\frac{\partial( \tilde{\mu}-b\tilde{\sigma}/c,\tilde{\sigma}/c)}{\partial(\tilde{\mu},\tilde{ \sigma})^{T}}\right)\right|,\]

where the determinant on the right-hand side is

\[\det\begin{pmatrix}1&-b/c\\ 0&1/c\end{pmatrix}=c^{-1}.\]

Hence the density of \((\tilde{\mu},\tilde{\sigma})\) reduces to

\[\tilde{\pi}(\tilde{\mu},\tilde{\sigma})=c^{-1}\pi\left(\tilde{\mu}-\frac{b \tilde{\sigma}}{c},\frac{\tilde{\sigma}}{c}\right).\]

The relation \(\Pi{{}_{\circ}}R_{t}^{-1}=\Pi\) implies \(\tilde{\pi}\) and \(\pi\) are the same function for all \(b,c\); that is,

\[c^{-1}\pi\left(\mu-\frac{b\sigma}{c},\frac{\sigma}{c}\right)=\pi(\mu,\sigma)\]

for all \(\mu,b\in\mathbb{R}\), \(c,\sigma\in(0,\infty)\). Taking \(\mu=0,c=1\), we have

\[c^{-1}\pi\left(-\frac{b}{c},\frac{1}{c}\right)=\pi(0,1).\]

Let \(\mu=-b/c\), \(\sigma=1/c\). Then

\[\pi\left(\mu,\sigma\right)=\pi(0,1)/\sigma.\]

So, the right Haar measure has density proportional to \(1/\sigma\). \(\Box\)

#### Jeffreys prior

Another important class of noninformative prior is the Jeffreys priors, which is defined as

\[\pi_{\Theta}(\theta)\propto[\det I(\theta)]^{1/2},\]

where \(I(\theta)\) is the Fisher information

\[I(\theta)=-E_{\theta}\left[\frac{\partial^{2}\log f_{X|\Theta}(x|\theta)}{ \partial\theta\partial\theta^{T}}\right].\]A useful feature of Jeffreys prior is that it satisfies the usual transformation law of measure. Suppose that \(\phi=h(\theta)\) is a one-to-one and differentiable transformation of \(\theta\). If \(P_{\Theta}\) is a noninformative prior distribution we assign to \(\Theta\) and \(P_{\Phi}\) is the noninformative prior distribution we assign to \(\Phi=h(\Theta)\). Then it is desirable to have

\[P_{\Phi}=P_{\Theta}{}^{\circ}h^{-1}.\]

Contrary to intuition, this is not automatically satisfied. This is because \(P_{\Phi}\) is not defined inherently by \(P_{\Theta}{}^{\circ}h^{-1}\), but rather it is subjectively assigned, possibly without regard to this transformation law. Let \(\pi_{\Theta}\) and \(\pi_{\Phi}\) be the densities of \(P_{\Theta}\) and \(P_{\Phi}\) with respect to the Lebesgue measure. Then, in terms of these densities, the above transformation rule is

\[\pi_{\Phi}(\phi)=\pi_{\Theta}(h^{-1}(\phi))|\det(\partial h^{-1}(\phi)/\partial \phi^{T})|. \tag{5.26}\]

The next theorem shows that Jeffreys prior satisfies the above transformation rule.

**Theorem 5.8**_Let \(\pi_{\Theta}\) be Jeffreys prior density for \(\Theta\). Let \(\Phi=h(\Theta)\), where \(h\) is a one-to-one differentiable function. Let \(\pi_{\Phi}\) be the Jeffreys prior density for \(\Phi\). Then \(\pi_{\Theta}\) and \(\pi_{\Phi}\) satisfy the tranformation rule (5.26)._

Proof.: Let \(f_{X|\Phi}(x|\phi)\) be the conditional density of \(X\) expressed in \(\phi\). That is

\[f_{X|\Phi}(x|\phi)=f_{X|\Theta}(x|h^{-1}(\phi)).\]

Take the second partial derivatives with respect to \(\phi_{i}\), \(\phi_{j}\), \(i,j=1,\ldots,p\), to obtain

\[\frac{\partial^{2}\log f_{X|\Phi}(x|\phi)}{\partial\phi_{i} \partial\phi_{j}}\] \[=\sum_{k=1}^{p}\sum_{\ell=1}^{p}\frac{\partial^{2}\log f_{X| \Theta}(x|\theta)}{\partial\theta_{k}\partial\theta_{\ell}}\frac{\partial \theta_{k}}{\partial\phi_{i}}\frac{\partial\theta_{\ell}}{\partial\phi_{j}}+ \sum_{k=1}^{p}\frac{\partial\log f_{X|\Theta}(x|\theta)}{\partial\theta_{k}} \frac{\partial^{2}\theta_{k}}{\partial\phi_{i}\partial\phi_{j}}.\]

Thus, taking conditional expectation \(E(\cdot|\theta)\) (or equivalently \(E(\cdot|\phi)\)), and noticing that the second term on the right hand side vanishes after taking this conditional expectation, we have

\[E\left[\left.\frac{\partial^{2}\log f_{X|\Phi}(X|\phi)}{\partial\phi_{i} \partial\phi_{j}}\right|\phi\right]=\sum_{k=1}^{p}\sum_{\ell=1}^{p}E\left[ \left.\frac{\partial^{2}\log f_{X|\Theta}(X|\theta)}{\partial\theta_{k} \partial\theta_{\ell}}\right|\theta\right]\frac{\partial\theta_{k}}{\partial \phi_{i}}\frac{\partial\theta_{\ell}}{\partial\phi_{j}}.\]

If we let \(\partial\theta^{T}/\partial\phi\) denote the matrix \(A_{ik}=\partial\theta_{k}/\partial\phi_{i}\) and let \(\partial\theta/\partial\phi^{T}\) denote the transpose of \(\partial\theta^{T}/\partial\phi\), then the above equation becomes

\[I(\phi)=(\partial\theta^{T}/\partial\phi)I(\theta)(\partial\theta/\partial \phi^{T}).\]It follows that

\[\det(I(\phi))=\det(\partial\theta^{T}/\partial\phi)\det(I(\theta))\det(\partial \theta/\partial\phi^{T})=\det(I(\theta))\left[\det(\partial\theta/\partial\phi^ {T})\right]^{2}.\]

Now take square root on both sides the equation to verify (5.26). 

**Example 5.10** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\theta,\sigma^{2})\) variables where \(\sigma^{2}\) is known. Then the Fisher information is \(I(\theta)=n/\sigma^{2}\). Hence Jeffreys prior is \(\sqrt{n}/\sigma\), which is proportional to the Lebesgue measure. If both \(\mu\) and \(\sigma^{2}\) are unknown, then the Fisher information is the matrix

\[I(\theta)=\begin{pmatrix}n/\sigma^{2}&0\\ 0&n/(2\sigma^{4})\end{pmatrix}\]

So \(\det(I(\theta))=n^{2}/(2\sigma^{6})\), and the Jeffreys prior is proportional to \(1/\sigma^{3}\). 

### Statistical decision theory

Many topics in Bayesian statistical inference, such estimation, testing, and classification, can be efficiently described within the framework of statistical decision theory Berger (1985).

The statistical decision theory consists of several elements. Let \((\Omega_{A},\mathcal{F}_{A})\) be a measurable space, where \(\Omega_{A}\) is called the action space. For example, if we estimate a parameter \(\Theta\), then \(\Omega_{A}\) is typically the same space as \(\Omega_{\Theta}\). If our goal is to test a hypothesis, then \(\Omega_{A}\) is \(\{0,1\}\), where \(1\) represents rejection. If our goal is classification, then \(\Omega_{A}\) is a list of categories.

Any mapping \(d:\Omega_{X}\rightarrow\Omega_{A}\) that is measurable with respect to \(\mathcal{F}_{X}/\mathcal{F}_{A}\), and such that \(L(\theta,d(X))\) is integrable with respect to \(P_{X|\Theta}(\cdot|\theta)\) for any \(\theta\in\Omega_{\Theta}\) is called a decision rule. A decision rule is a statistic -- typically an estimate, a test, or a classifier. The class of all decision rules is written as \(\mathcal{D}\).

A mapping \(L:\Omega_{\Theta}\times\Omega_{A}\rightarrow\mathbb{R}\) is called a loss function. It represents the error one makes by taking a certain action in \(\Omega_{A}\). For example, if our action is to reject or accept a hypothesis, then it is either right or wrong; In this case \(L\) takes values in \(\{0,1\}\), representing right and wrong. If our action is to estimate \(\Theta\), then the loss may be Euclidean distance \(\|\Theta-a\|\).

A loss is something that has already happened: if we took an action \(a\in\Omega_{A}\), and it turned out the value \(\theta\) of \(\Theta\) is quite different from our action \(a\), and then we lose, or lose a certain amount. Of course a statistician's job is to prevent or to minimize the loss _before_ it happens. That means we need to be able to access a loss before it happens. This leads us to the notion of the risk, which is the expectation of loss. There are three ways of taking this expectation. The first is to condition on \(\Theta\):

\[R(\theta,d)=E[L(\Theta,d(X)|\Theta)]_{\theta}.\]This is called the frequentist risk. It is a mapping from \(\Omega_{\Theta}\times\mathcal{D}\to\mathbb{R}\). The second is to take expectation conditioning on \(X\):

\[\rho(x,a)=E[L(\Theta,a)|X]_{x}.\]

This is called the posterior expected loss. It is a mapping from \(\Omega_{X}\times\Omega_{A}\to\mathbb{R}\). Finally we can take the unconditional expectation

\[r(d)=E[L(\Theta,d(X))]=E[ER(\Theta,d(X)|\Theta)].\]

This is called the Bayes risk. It is a mapping from \(\mathcal{D}\to\mathbb{R}\).

One of the most important principles for choosing a decision is the Bayes rule, defined as follows.

**Definition 5.9**: _The Bayes rule \(d_{B}\) is defined as_

\[d_{B}=\operatorname{argmin}\{r(d):d\in\mathcal{D}\}.\]

_If \(P_{\Theta}\) is improper, then this rule is called the generalized Bayes rule._

In appearance the Bayes rule is the minimizer of \(r(d)\) over a class of functions \(\mathcal{D}\), which is in general a difficult problem. However, under mild conditions this can be converted to a finite dimensional minimization problem using Fubini's theorem.

**Theorem 5.9**: _If \(L(\theta,a)\geq C\) for some \(C>-\infty\) for all \(\theta\in\Omega_{\Theta}\) and \(a\in\Omega_{A}\), then the decision rule_

\[\Omega_{X}\to\Omega_{A},\quad x\mapsto\operatorname{argmin}\{\rho(x,a):a\in \Omega_{A}\} \tag{5.27}\]

_is a Bayes rule._

Proof: By definition,

\[r(d)=\int_{\Omega_{\Theta}}\int_{\Omega_{X}}L(\theta,d(x))f_{X|\Theta}(x| \theta)d\mu_{X}(x)\pi(\theta)d\mu_{\Theta}(\theta).\]

Since the loss function is bounded from below, we interchange the order of the integrals by Tonelli's theorem. That is

\[r(d) =\int_{\Omega_{X}}\int_{\Omega_{\Theta}}L(\theta,d(x))\pi_{\Theta |X}(\theta|x)d\mu_{\Theta}(\theta)f_{X}(x)d\mu_{X}(x)\] \[=\int_{\Omega_{X}}\rho(x,d(x))f_{X}(x)d\mu_{X}(x).\]

Now let \(d_{0}:\Omega_{X}\to\Omega_{A}\) be the decision rule

\[d_{0}(x)=\operatorname{argmin}\{\rho(x,a):a\in\Omega_{A}\}.\]

Then for any \(d\in\mathcal{D}\),\[r(d_{0}) =\int_{\Omega_{X}}\rho(x,d_{0}(x))f_{X}(x)d\mu_{X}(x)\] \[\leq\int_{\Omega_{X}}\rho(x,d(x))f_{X}(x)d\mu_{X}(x)=r(d).\]

In other words, \(d_{0}\) is a Bayes rule. \(\Box\)

Note that, to compute the posterior expected loss, we do not need the marginal density \(f_{X}(x)\), because

\[E(L(\Theta,a)|X)_{x}\propto\int_{\Omega_{X}}L(\theta,a)f_{X|\Theta}(x|\theta) \pi_{\Theta}(\theta)d\mu_{\Theta}(\theta)\]

with a proportional constant \((1/f_{X}(x))\) that does not depend on \(a\). So it is equivalent to define the \(d_{0}\) in Theorem 5.9 as

\[\mbox{argmin}\{\int_{\Omega_{X}}L(\theta,a)f_{X|\Theta}(x|\theta)\pi_{\Theta}( \theta)d\mu_{\Theta}(\theta):\ a\in\Omega_{A}\}. \tag{5.28}\]

The generalized Bayes rule can also be computed using (5.28).

Another optimal criterion in decision theory is admissibility. In the following we assume that every decision rule in \(\mathcal{D}\) has an integrable risk \(R(\theta,d)\) with respect to \(\pi_{\Theta}\), where \(\pi_{\Theta}\) can be a proper or improper.

**Definition 5.10**: _A decision rule is \(d\in\mathcal{D}\) is inadmissible if there is a decision rule \(d_{1}\) such that_

\[R(\theta,d_{1})\leq R(\theta,d)\mbox{ for all }\theta\in\Omega_{ \Theta},\] \[R(\theta,d_{1})<R(\theta,d)\mbox{ for some }\theta\in\Omega_{ \Theta}.\]

_A decision rule is admissible if it is not inadmissible._

Admissibility is a uniform (in \(\theta\)) property; whereas Bayes is an average property. Admissibility is a weak optimality property, because it only presents itself from being uniformly worse than any other decision rules. Bayes rule, on the other hand, does require itself to be better than all other rules, albeit according to a criterion weaker than the uniform criterion used in admissibility. It is then not surprising that under some mild conditions, a Bayes rule is admissible.

**Theorem 5.10**: _Suppose_

_1. for each_ \(d\in\mathcal{D}\)_,_ \(R(\theta,d)\) _is integrable with respect to_ \(P_{\Theta}\)_;_

_2. for any_ \(d_{1},d_{2}\in\mathcal{D}\)_,_

\[R(\theta,d_{2})-R(\theta,d_{1})<0\mbox{ for some }\theta\in\Omega_{ \Theta}\] \[\Rightarrow\ P_{\Theta}(R(\theta,d_{2})-R(\theta,d_{1})<0)>0.\]

_Then any Bayes or generalized Bayes rule is admissible._Proof.: Suppose \(d_{1}\) is an inadmissible Bayes rule in \(\mathcal{D}\). Then there is \(d_{2}\in\mathcal{D}\) such that \(R(\theta,d_{2})\) is no more than \(R(\theta,d_{1})\) for all \(\theta\in\Omega_{\Theta}\) and is strictly less than \(R(\theta,d_{1})\) for some \(\theta\). Hence

\[\int_{\Omega_{\Theta}}[R(\theta,d_{2})-R(\theta,d_{1})]dP_{\Theta}=\int_{R( \theta,d_{2})-R(\theta,d_{1})<0}[R(\theta,d_{2})-R(\theta,d_{1})]dP_{\Theta}\]

Since \(P_{\Theta}(R(\theta,d_{2})-R(\theta,d_{1})<0)>0\), the right hand side is negative, which contradicts the assumption that \(d_{1}\) is Bayes. 

Here, we have implicitly used assumption 1, because without it the difference

\[\int_{\Omega_{\Theta}}[R(\theta,d_{2})-R(\theta,d_{1})]dP_{\Theta}\]

may not be defined. Assumption 2 of the theorem covers two interesting cases. First, suppose \(\Omega_{\Theta}=\{\theta_{1},\theta_{2},\ldots\}\) is countable and \(\pi_{\Theta}\) is positive for each \(\theta_{i}\), then this assumption is obviously satisfied. Second, if \(R(\theta,d)\) is continuous in \(\theta\) for each \(d\), and \(P_{\Theta}(A)>0\) for any nonempty open set, then it is also satisfied. See Problem 5.35.

### Problems

**5.1.** Let \((\Omega_{X},\mathcal{F}_{X})\), \((\Omega_{\Theta},\mathcal{F}_{\Theta})\) be measurable spaces. Let

\[\Omega=\Omega_{X}\times\Omega_{\Theta},\quad\mathcal{F}=\mathcal{F}_{X}\times \mathcal{F}_{\Theta}.\]

So \((\Omega,\mathcal{F})\) is a measurable space. Let \(X\) be the random element

\[X:\Omega\to\Omega,\quad(x,\theta)\mapsto x.\]

Show that \(\sigma(X)=\{A\times\Omega_{\Theta}:A\in\mathcal{F}_{X}\}\).

**5.2.** Let \(\Omega_{1}\) and \(\Omega_{2}\) be two sets, and \(T:\Omega_{1}\to\Omega_{2}\) be any function. Show that

1. \(T^{-1}(\Omega_{2})=\Omega_{1}\);

2. for any \(A\subseteq\Omega_{2}\), \(T^{-1}(A^{c})=(T^{-1}(A))^{c}\);

3. if \(A_{1},A_{2},\ldots\) is a sequence of subsets of \(\Omega_{2}\), then

\[\cup_{n=1}^{\infty}T^{-1}(A_{n})=T^{-1}(\cup_{n=1}^{\infty}A_{n}).\]

**5.3.** Let \(\Omega_{1}\), \(\Omega_{2}\) be two sets, and \(\mathcal{G}_{1}\) be a \(\sigma\)-field in \(\Omega_{1}\). Let \(T:\Omega_{1}\to\Omega_{2}\) be any function. Let \(\mathcal{B}=\{B:T^{-1}(B)\subseteq\mathcal{G}_{1}\}\). Show that \(\mathcal{B}\) is a \(\sigma\)-field in \(\Omega_{2}\).

**5.4.** Let \((\Omega_{1},\mathcal{F}_{1})\) and \((\Omega_{2},\mathcal{F}_{2})\) be measurable space, and \(T:\Omega_{1}\to\Omega_{2}\) be a surjection that is measurable \(\mathcal{F}_{1}/\mathcal{F}_{2}\). Let \(\mathcal{A}\) be a subclass of \(\mathcal{F}_{2}\) such that \(\sigma(\mathcal{A})=\mathcal{F}_{2}\). Show that \(\sigma(T^{-1}(\mathcal{A}))=T^{-1}(\sigma(\mathcal{A}))\).

**5.5.** Let \((\Omega_{X},{\cal F}_{X})\), \((\Omega_{\Theta},{\cal F}_{\Theta})\) be measurable spaces. Let

\[\Omega=\Omega_{X}\times\Omega_{\Theta},\quad{\cal F}={\cal F}_{X}\times{\cal F}_ {\Theta}.\]

So \((\Omega,{\cal F})\) is a measurable space. Let \(X\) be the random element

\[X:\Omega\to\Omega_{X},\quad(x,\theta)\mapsto x.\]

Show that \(\sigma(X)=\{A\times\Omega_{\Theta}:A\in{\cal F}_{X}\}\).

**5.6.** Let \((\Omega_{X},{\cal F}_{X})\), \((\Omega_{\Theta},{\cal F}_{\Theta})\), \((\Omega,{\cal F})\), and \(X\) be as defined in the previous problem. Let \((\Omega_{T},{\cal F}_{T})\) be another measurable space. Let \(T:\Omega_{X}\to\Omega_{T}\) be a function measurable \({\cal F}_{X}/{\cal F}_{T}\).

1. Show that

\[\sigma(T^{\circ}X)=\{T^{-1}(A)\times\Omega_{\Theta}:A\in{\cal F}_{X}\}.\]

2. Suppose, in addition, \(T\) is surjective. Let \(\Theta\) be the random element

\[\Theta:\Omega\to\Omega_{\Theta},\quad(x,\theta)\mapsto\theta.\]

Let \((T^{\circ}X,\Theta)\) be the random element

\[(T^{\circ}X,\Theta):\Omega\to\Omega_{T}\times\Omega_{\Theta},\quad(x,\theta) \mapsto(T(x),\theta).\]

Show that

\[\sigma(T^{\circ}X,\Theta)=\sigma\{T^{-1}(C)\times D:C\in{\cal F}_{T},D\in{\cal F }_{\Theta}\}.\]

3. Let

\[{\cal P}=\{T^{-1}(C)\times D:C\in{\cal F}_{T},D\in{\cal F}_{\Theta}\}.\]

Sow that \({\cal P}\) is a \(\pi\)-system.

**5.7.** Let \((\Omega_{X},{\cal F}_{X})\), \((\Omega_{\Theta},{\cal F}_{\Theta})\), \((\Omega_{T},{\cal F}_{T})\), \((\Omega,{\cal F})\), \(X\), \(\Theta\), \(T\), and \({\cal P}\) be as defined in the previous problem. Let \(A\in{\cal F}_{X}\). Define

\[Q_{1}(B)=E[I_{B}E(X^{-1}(A)|T^{\circ}X,\Theta)],\quad Q_{2}(B)=E[I_{B}E(X^{-1} (A)|T^{\circ}X)].\]

Show that \({\cal L}=\{B\in\sigma({\cal P}):Q_{1}(B)=Q_{2}(B)\}\) is a \(\lambda\)-system.

**5.8.** Let \((\Omega_{X},{\cal F}_{X},\mu_{X})\), \((\Omega_{\Theta},{\cal F}_{\Theta},\mu_{\Theta})\) be \(\sigma\)-finite measure spaces. Let \(P\) be a probability measure defined on \((\Omega_{X}\times\Omega_{\Theta},{\cal F}_{X}\times{\cal F}_{\Theta})\) with \(P\ll\mu_{X}\times\mu_{\Theta}\).

1. Show that \(P^{\circ}X^{-1}\ll\mu_{X}\) and \(P^{\circ}\Theta^{-1}\ll\mu_{\Theta}\).

2. Let \[f_{X|\Theta}(x|\theta)=\begin{cases}[dP/d(\mu_{X}\times\mu_{\Theta})]/[dP\circ X^{-1 }/d\mu_{X}]&\text{if }dP\circ X^{-1}/d\mu_{X}=0\\ 0&\text{if }dP\circ X^{-1}/d\mu_{X}=0\end{cases}\] Show that, for each \(A\in\mathcal{F}_{X}\), the function \[\theta\mapsto\int_{A}f_{X|\Theta}(x|\theta)d\mu_{X}(x)\] is (a version of) \(P(A|\Theta)\).

**5.9**.: Suppose that \(\Theta=(\Psi,\Lambda)\). Suppose \(T(X)\) is a statistic that is sufficient for \(\lambda\) for each fixed \(\psi\). Show that, for any \(G\in\mathcal{F}_{\Theta}\),

\[P(\Theta^{-1}(G)|X,\Psi)=\pi(\Theta^{-1}(G)|T(X),\Psi).\]

**5.10**.: Let \(P_{\theta}=P_{X|\Theta}(\cdot|\theta)\). Show that the mapping

\[(x,\theta)\mapsto P_{\theta}(A|T\circ X)_{x}\]

is a version of the conditional probability

\[(x,\theta)\mapsto P(A|T\circ X,\Theta)_{(x,\theta)}.\]

**5.11**.: Suppose that \(X|\theta\) is a \(p\)-dimensional random vector with distribution \(N(\theta,\Sigma)\), where \(\Sigma\) is a \(p\) by \(p\) positive definite matrix. This matrix is treated as the non-random parameter. The random parameter \(\Theta\) is also distributed as \(p\)-dimensional multivariate normal \(N(\mu,\Omega)\), where \(\Omega\) is a \(p\) by \(p\) positive definite matrix. Here \(\mu\) and \(\Omega\) are treated as non-random parameters. Find the distribution of \(\Theta|X\) as well as the marginal distribution of \(X\).

**5.12**.: Suppose that \(X_{1},\ldots,X_{n}|\theta\) are independent \(p\)-dimensional random vectors distributed as \(N(\theta,\Sigma)\), where \(\Sigma>0\) (this means \(\Sigma\) is positive definite). Suppose that \(\theta\) is distributed as \(N(\mu,\Omega)\) where \(\Omega>0\). Find the posterior distribution \(\theta|X_{1},\ldots,X_{n}\). Write your result in its most interpretable form.

**5.13**.: Let \(X\) and \(Y\) be two random elements. Suppose that the conditional density of \(Y|X\) does not depend on \(X\); that is \(f(y|x)=h(y)\) for some function \(h\). Then \(X\) and \(Y\) are independent.

**5.14**.: Suppose that \(\Phi\sim\chi_{\nu}^{-2}\) and \(\Lambda|\phi\sim N(0,\phi)\).

1. Show that \(\sqrt{\nu}\Lambda\sim t_{(\nu)}\).

2. Show that \(\Phi|\lambda\sim(1+\lambda^{2})\chi_{\nu+1}^{-2}\) and that \(\Phi/(1+\Lambda^{2})\mathop{\mathchoice{\hbox{\hbox to 0.0pt{\kern 2.999954pt\vrule height 6.299904pt wid th 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{\kern 2.999954pt \vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{\kern 2.999968pt \vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\hbox{\hbox to 0.0pt{ \kern 1.049984pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}}\Lambda\).

**5.15**.: Show that, if

\[T|\phi\sim\phi\chi_{(m)}^{2},\quad\Phi\sim\tau\chi_{(k)}^{-2},\]

[MISSING_PAGE_EMPTY:6012]

3. For each fixed \(\phi\in\Omega_{\Phi}\), find a conjugate prior for the conditional density \((t,\lambda)\mapsto f_{T|\Phi\Lambda}(t|\phi,\lambda)\).
4. Find the joint posterior distribution of \((\Lambda,\Phi)\).
5. Find the marginal posterior distribution of \(\Lambda\).

**5.24.** Suppose

\[\Phi|(S=s)\sim s\chi_{(m)}^{-2},\quad T|(\Phi=\phi)\sim N(a,c\phi),\]

where \(a\in\mathbb{R}\), \(c>0\). Then

\[\pi_{\Phi|S}(\phi|s)f_{T|\Phi}(t|\phi)\propto[s+(t-a)^{2}/c]g(\phi),\]

where \(g(\phi)\) is the density of \(\chi_{(m+1)}^{-2}\), and the proportionality constant may depend on \(s,t\) but does not depend on \(\phi\).

**5.25.** Suppose \(X\) is a random variable with density of the form

\[\frac{1}{\sqrt{\phi}}f\left(\frac{x^{2}}{\phi}\right),\]

where \(x\in\mathbb{R}=\Omega_{X}\) and \(\phi\in(0,\infty)=\Omega_{\Phi}\). Consider the set of transformations

\[h_{c}(x)=cx,\quad c\in(0,\infty).\]

1. Show that \(\mathcal{H}=\{h_{c}:\,c\in(0,\infty)\}\) is a group of transformations from \(\Omega_{X}\) to \(\Omega_{X}\).
2. Determine the transformation \(g_{c}\) from \(\Omega_{\Phi}\) to \(\Omega_{\Phi}\) induced by the transformation \(h_{c}\).
3. Show that the set of transformations \(\mathcal{G}=\{g_{c}:c\in(0,\infty)\}\) is a group.
4. Find the left and right Haar measures for the group \(\mathcal{G}\).

**5.26.** Suppose \(X\) is a random variable with density of the form

\[\frac{1}{\sqrt{\phi}}f\left(\frac{(x-\mu)^{2}}{\phi}\right).\]

Let \(\theta=(\mu,\phi)\) and \(\Omega_{\Theta}=\mathbb{R}\times\mathbb{R}^{+}\), where \(\mathbb{R}^{+}=(0,\infty)\), be the parameter space. Consider the set of transformations

\[\mathcal{H}=\{h_{b,c}(x)=cx+b:\quad b\in\mathbb{R},\ c\in\mathbb{R}^{+}\}.\]

1. Show that \(\mathcal{H}\) is a group of transformations from \(\Omega_{X}\) to \(\Omega_{X}\).

2. Determine the transformation \(g_{b,c}\) from \(\Omega_{\Theta}\) to \(\Omega_{\Theta}\) induced by the transformation \(h_{b,c}\in\mathcal{H}\).

3. Show that \(\mathcal{G}=\{g_{b,c}:b\in\mathbb{R},c\in\mathbb{R}^{+}\}\) is a group of transformations from \(\Omega_{\Theta}\) to \(\Omega_{\Theta}\).

4. Find the left and right Haar measures for the group \(\mathcal{G}\).

**5.27.** Suppose \(X\) is a random vector of dimension \(p\) with density of the form

\[\frac{1}{\det(\Sigma)^{1/2}}f\left(x^{T}\Sigma^{-1}x\right),\]

where \(\Sigma\) is a member of \(\mathbb{R}_{+}^{p\times p}\), the set of all positive definite matrices. Consider the set of transformations of the form

\[\mathcal{H}=\{h_{C}(x)=Cx:\,C\in\mathbb{R}_{+}^{p\times p}\}.\]

1. Show that \(\mathcal{H}\) is a group of transformations from \(\Omega_{X}\) to \(\Omega_{X}\).

2. Determine the transformation \(g_{C}\) induced by a transformation \(h_{C}\in\mathcal{H}\).

3. Show that the set of transformations \(\mathcal{G}=\{g_{C}:\,C\in\mathbb{R}_{+}^{p\times}\}\) is a group.

4. Find the left and right Haar measures for this group.

(Hint: To solve this problem, we need to take derivative of \(\mathrm{vec}(ABC)\) with respect to \(\mathrm{vec}(B)\), where \(A\), \(B\), \(C\) are matrices, and \(\mathrm{vec}\) is the vectorization operator that stacks the columns of the argument matrix. Since \(\mathrm{vec}(ABC)=(C\otimes A^{T})\mathrm{vec}(B)\), where \(\otimes\) is the Kronecker product between matrices, we have \(\partial\mathrm{vec}(ABC)/\partial\mathrm{vec}(B)^{T}=(C\otimes A^{T})\). Also, the following identity will be useful: if \(A\) and \(B\) are square matrices with dimensions \(n_{1}\) and \(n_{2}\), respectively, then \(\det(A\otimes B)=\det(A)^{n_{1}}\times\det(B)^{n_{2}}\).)

**5.28.** Suppose \(X\) is a random vector of dimension \(p\) with density of the form

\[\frac{1}{\det(\Sigma)^{1/2}}\,f\left((x-\mu)^{T}\Sigma^{-1}(x-\mu)\right),\]

where \(\mu\in\mathbb{R}\), \(\Sigma\in\mathbb{R}_{+}^{p\times p}\). Consider the set of transformations

\[\mathcal{H}=\{h_{b,C}(x)=Cx+b:\,b\in\mathbb{R}^{p},\ C\in\mathbb{R}_{+}^{p \times p}\}.\]

Let \(\theta=(\mu,\Sigma)\) be the whole parameter and let \(\Omega_{\Theta}=\mathbb{R}^{p}\times\mathbb{R}_{+}^{p\times p}\) be the parameter space.

1. Show that \(\mathcal{H}\) is a group of transformations from \(\Omega_{X}\) to \(\Omega_{X}\).

2. Determine the transformation \(g_{b,C}\) from \(\Omega_{\Theta}\) to \(\Omega_{\Theta}\) induced by a transformation \(h_{b,C}\in\mathcal{H}\).

3. Show that the set of transformations \(\mathcal{G}=\{g_{b,C}:b\in\mathbb{R},C\in\mathbb{R}_{+}^{p\times p}\}\) is a group.

4. Calculate the left Haar measure for this group.

**5.29.** Let \(\Omega_{\Gamma}=\mathbb{R}_{+}^{p\times p}\), the set of all positive definite matrices in \(\mathbb{R}^{p\times p}\). Consider the set of transformations on \(\Omega_{\Gamma}\) defined as

\[\mathcal{G}=\{(\Gamma\mapsto C^{1/2}\Gamma C^{1/2}):\,C\in\mathbb{R}_{+}^{p \times p}\}.\]

1. Show that \(\mathcal{G}\) is a group of transformations on \(\Omega_{\Gamma}\);

2. Find the left Haar measure on \(\Omega_{\Gamma}\) with respect to \(\mathcal{G}\);3. Find the right Haar measure on \(\Omega_{\Gamma}\) with respect to \(\mathcal{G}\).

**5.30.** Let \(\Omega_{\Gamma}=\{(\mu,\Gamma):\,\mu\in\mathbb{R}^{p},\ \Gamma\in\mathbb{R}^{p \times p}_{+}\}\). Consider the set of transformations on \(\Omega_{\Gamma}\) defined as

\[\mathcal{G}=\{(\mu,\Gamma)\mapsto(C\mu+d),C^{1/2}\Gamma C^{1/2}):\ d\in\mathbb{ R},C\in\mathbb{R}^{p\times p}_{+}\}.\]

1. Show that \(\mathcal{G}\) is a group of transformations on \(\Omega_{\Gamma}\);

2. Find the left Haar measure on \(\Omega_{\Gamma}\) with respect to \(\mathcal{G}\);

3. Find the right Haar measure on \(\Omega_{\Gamma}\) with respect to \(\mathcal{G}\).

**5.31.** Let \(X\) be a random vector defined on \((\mathbb{R}^{p},\mathcal{R}^{p})\). Suppose the family of distributions of \(X\) has densities of the form

\[[1/\det(\Lambda)]f(\Gamma\Lambda\Gamma^{T}(x-\mu)),\quad\mu\in\mathbb{R}^{p}, \ \Lambda\in\mathbb{D}^{p\times p},\ \Sigma>0,\]

where \(\mathbb{D}^{p\times p}\) is the class of all \(p\times p\) diagonal matrices with positive diagonal entries, and \(\Gamma\) is a known orthogonal matrix. The parameter of this model is \(\Theta=(\mu,\mathrm{diag}(\Lambda))\), where \(\mathrm{diag}(\Lambda)\) is the vector of diagonal entries of \(\Lambda\). Let \(\mathcal{A}\) be a class of matrices of the form

\[\{\Gamma\Delta\Gamma^{T}:\ \Delta\in\mathbb{D}\}.\]

Let \(\mathcal{G}\) be the class of all transformations from \(\mathbb{R}^{p}\) to \(\mathbb{R}^{p}\) defined

\[x\mapsto Ax+b,\quad A\in\mathcal{A},\ b\in\mathbb{R}^{p}.\]

1. Show that \(\mathcal{G}\) is a group.

2. Show that the transformation \(\tilde{g}:\Omega_{\Theta}\to\Omega_{\Theta}\) induced by the transformation \(g:\Omega_{X}\to\Omega_{X}\), \(g(x)=cx+b\), \(c>0\), is of the form

\[\tilde{g}(\mu,\mathrm{diag}(\Lambda))=(A\mu+b,\mathrm{diag}(D)\odot\mathrm{ diag}(\Lambda)),\]

where \(\odot\) is the Hadamard, or entry-wise, product.

3. Derive the left and right Haar measures for the group of transformations in part 2.

**5.32.** Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\mu,\sigma^{2})\) where \(\mu\) is known. Derive Jeffreys prior for \(\sigma^{2}\).

**5.33.** Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. \(p\)-dimensional multivariate Normal \(N(\mu,\Sigma)\) where \(\mu\in\mathbb{R}^{p}\) and \(\Sigma\in\mathbb{R}^{p\times p}_{+}\). Derive Jeffreys prior for \((\mu,\Sigma)\).

**5.34.** 1. If \(X\) has a Gamma distribution with parameterization

\[\frac{1}{\Gamma(k)\theta]^{k}}x^{k-1}e^{-x/\theta},\quad x>0,\ \theta>0,\ k>0,\]

find the Jeffreys prior for \((\theta,k)\).

2. If \(X\) has a Gamma distribution with parameterization

\[\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x},\quad x>0,\ \alpha>0,\ \beta>0,\]

find the Jeffreys prior for \((\alpha,\beta)\).

**5.35.** Show that, if \(R(\theta,d)\) is continuous in \(\theta\) for each \(d\in\mathcal{D}\), and \(P_{\Theta}(A)>0\) for any nonempty open set \(A\in\mathcal{F}_{\Theta}\), then, for any \(d_{1},d_{2}\in\mathcal{D}\),

\[R(\theta,d_{2})-R(\theta,d_{1})<0\ \mbox{for some}\ \theta\in \Omega_{\Theta}\] \[\Rightarrow\ P_{\Theta}(R(\theta,d_{2})-R(\theta,d_{1})<0)>0.\]

**References**

Berger, J. O. (1985). _Statistical decision theory and Bayesian analysis_. Second edition. Springer, New York.

Lee, P. M. (2012). _Bayesian Statistics: An Introduction, Fourth Edition_. Wiley.

Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979). _Multivariate Analysis._ Academic.

O'Hagan, A. (1994). _Kendall's Advanced Theory of Statistics: Bayesian Inference, Volume 2B_. Edward Arnold.

## 6 Bayesian Inference

Based on the concepts and preliminary results introduced in the last chapter, we develop the Bayesian methods for statistical inference, including estimation, testing, and classification, in this chapter. We will focus on Bayesian rules under different settings. All three problems can be formulated as decision theoretic problems described in Section 5.7, with different parameter spaces, action space, and loss functions. For estimation, we take \(\Omega_{A}=\Omega_{\Theta}=\mathbb{R}^{p}\); for hypothesis testing, we take \(\Omega_{A}\) as a set of two elements -- accept or reject; for classification, we take \(\Omega_{\Theta}=\Omega_{A}\) as a finite set representing a list of categories. We will also explore some important special topics in Bayesian analysis, such as empirical Bayes and Stein's estimator.

### 6.1 Estimation

In a Bayesian estimation problem, the parameter \(\Theta\) is typically a random vector whose distribution \(P_{\Theta}\) is dominated by the Lebesgue measure. Also, it is natural to assume \(\Omega_{\Theta}=\Omega_{A}\). The most commonly used loss function is the \(L_{2}\)-loss function, as defined below.

**Definition 6.1**: _The \(L_{2}\)-loss function is defined as_

\[L(\Theta,a)=(\Theta-a)^{T}W(\Theta)(\Theta-a), \tag{6.1}\]

_where \(W(\theta)\in\mathbb{R}^{p\times p}\) is a positive definite matrix for all \(\theta\in\Omega_{\Theta}\)._

Recall that a Bayes rule can be calculated by minimizing the posterior expected loss. For \(L_{2}\) loss this has an explicit solution, described in the next Theorem.

**Theorem 6.1**: _Suppose_

1. \(E[\Theta^{T}W(\Theta)\Theta|X]<\infty\)__\([P]\)_;_
2. _Springer Science+Business Media, LLC, part of Springer Nature_ 2019_

B. Li and G. J. Babu, _A Graduate Course on Statistical Inference_, Springer Texts in Statistics, [https://doi.org/10.1007/978-1-4939-9761-9_6](https://doi.org/10.1007/978-1-4939-9761-9_6)_2. \(E[W(\Theta)|X]>0\)_: \([P]\)_._

_Then the Bayes rule with respect to loss function (6.1) is_

\[d_{B}(X)=[E(W(\Theta)|X)]^{-1}E[W(\Theta)\Theta|X]. \tag{6.2}\]

Proof.: By definition,

\[E[W(\Theta)(\Theta-d_{B}(X))|X]=E[W(\Theta)\Theta|X]-E[W(\Theta)|X]d_{B}(X)=0.\]

Hence, for any \(a\in\Omega_{A}\),

\[E[(d_{B}(X)-a)^{T} W(\Theta)(\Theta-d(X))|X]\] \[=(d_{B}(X)-a)^{T}E[W(\Theta)(\Theta-d_{B}(X))|X]=0.\]

Consequently,

\[E[L(\Theta,a)|X]\] \[=E[(\Theta-d_{B}(X))^{T}W(\Theta)(\Theta-d_{B}(X))|X]\] \[\quad+E[(d_{B}(X)-a)^{T}W(\Theta)(d_{B}(X)-a)|X]\geq E[L(\Theta,d _{B}(X))|X].\]

That is, \(d_{B}(X)\) is a Bayes rule. 

It can be shown that the Bayes rule for the \(L_{2}\)-loss (6.1) is unique modulo \(P\). See Problem 6.3. Now let us turn to the \(L_{1}\)-loss function. We first consider the one-dimensional case.

**Definition 6.2**: _If \(p=1\), then the \(L_{1}\)-loss is the function_

\[L(\theta,a)=|\theta-a|. \tag{6.3}\]

We would like to minimize the posterior expected loss

\[E(|\Theta-a|\,|X).\]

We will show that the minimizer is the posterior median. We first give a general definition of median.

**Definition 6.3**: _Let \(U\) be a random variable with a cumulative distribution function \(F\). Then any number \(m\) satisfying_

\[F(m)\geq 1/2,\quad F(m-)\leq 1/2\]

_is called a median of \(U\)._

In the next theorem \(U\) is a generic random variable that takes values in \(\Omega_{U}\).

**Theorem 6.2** _If \(U\) is integrable and \(m\) is a median of \(U\), then_

\[\int|U-m|dP\leq\int|U-a|dP\]

_for all \(a\in\Omega_{U}\)._

_Proof._ Suppose \(a>m\), \(a\in\Omega_{U}\). Then

\[\int_{\Omega_{U}}\left(|U-m|-|U-a|\right)dP\] \[\qquad=\int_{U\leq m}\left(\left(m-U\right)-\left(a-U\right) \right)dP+\int_{m<U\leq a}\left(\left(U-m\right)-\left(a-U\right)\right)dP\] \[\qquad\qquad+\int_{U>a}\left(\left(U-m\right)-\left(U-a\right) \right)dP\] \[\qquad=(m-a)P(U\leq m)+(a-m)P(U>a)+\int_{m<U\leq a}\left(2U-m-a \right)dP.\]

Because the last term on the right side is no more than \((a-m)P(m<U\leq a)\), \(E|U-m|-E|U-a|\) is no more than

\[\left(m-a\right)\left[P(U\leq m)-P(U>a)-P(m<U\leq a)\right]\] \[\qquad\qquad=\left(m-a\right)\left[P(U\leq m)-P(U>m)\right]=\left( m-a\right)\left[2F(m)-1\right].\]

Similarly, for \(a<m\), \(a\in\Omega_{U}\), we have

\[\int_{\Omega_{U}}\left(|U-m|-|U-a|\right)dP\] \[\qquad=\left(\int_{U<a}+\int_{a\leq U<m}+\int_{U\geq m}\right) \left(|U-m|-|U-a|\right)dP\] \[\qquad=\left(m-a\right)P(U<a)+(a-m)P(U\geq m)+\int_{a\leq U<m} \left(m+a-2U\right)dP\] \[\qquad\leq\left(a-m\right)\left[1-2F(m-)\right].\]

To summarize, we have

\[E|U-m|-E|U-a|\leq\begin{cases}\left(m-a\right)\left[2F(m)-1\right]&\text{if } \ m<a\\ \left(a-m\right)\left[1-2F(m-)\right]&\text{if }\ a<m\end{cases}\]

Because \(m\) is a median of \(U\),

\[2F(m)-1\geq 0,\ \ 1-2F(m-)\geq 0.\]

Hence \(E|U-m|-E|U-a|\leq 0\) for all \(a\in\Omega_{U}\). \(\Box\)

Since the \(L_{1}\)-loss is bounded from below, by Theorem 5.8 the posterior median \(m(\Theta|X)\) is a Bayes rule.

**Corollary 6.1**: _The posterior median \(m(\theta|X)\) is a Bayes rule with respect to the loss \(L(\theta,a)=|\theta-a|\)._

When \(\Theta\) is a vector, there are more than one way to define a \(L_{1}\)-loss. The one possibility is to use the Euclidean norm

\[L(\theta,a)=\|\theta-a\|.\]

The minimizer of the expectation of this loss is called the _geometric median_. Thus the Bayes rule based on this loss is the posterior geometric median. Another possibility is

\[L(\theta,a)=|\theta_{1}-a_{1}|+\cdots+|\theta_{p}-a_{p}|.\]

Since the function is additive, with each term involving on \(\theta_{i}\), the minimization of the posterior expectation is equivalent to the minimization of each \(E(|\theta_{i}-a_{i}|\,|X)\), so that the Bayes rule is simply the stacked marginal posterior medians. For other choices of multivariate \(L_{1}\)-loss, see Oja (1983); Hettmansperger and Randles (2002).

In the univariate case, one can generalize \(L_{1}\)-loss to the "check function", defined as

\[L(\theta,a)=\begin{cases}\alpha_{1}(\theta-a)&\text{if}\ \ \theta-a\geq 0\\ \alpha_{2}(a-\theta)&\text{if}\ \ \theta-a<0\end{cases} \tag{6.4}\]

where \(\alpha_{1}>0\) and \(\alpha_{2}>0\). Note that the \(L_{1}\)-loss function is the special case where \(\alpha_{1}=\alpha_{2}=1\). It can be shown, using the similar argument for Theorem 6.2, that the \(\alpha_{1}/(\alpha_{1}+\alpha_{2})\)th percentile is the Bayes rule with respect to this loss function. See Problem 6.4.

Another commonly used Bayesian estimator is the mode of the posterior distribution, which is also called the generalized maximum likelihood estimator (Berger, 1985, Chapter 4).

**Definition 6.4**: _The generalized maximum likelihood estimator is_

\[\hat{\theta}=\operatorname{argmax}\{\pi_{\Theta|X}(\theta|x):\theta\in\Omega_{ \Theta}\}.\]

The generalized maximum likelihood estimator is the point in the parameter space that is most likely to happen according to the posterior distribution.

We now illustrate these estimators by the constrained linear regression. In some cases we know a priori that there should be some restrictions on the parameter \(\Theta\). For example, if \(\Theta\) represents height then we know it is nonnegative. In Bayesian analysis this is handled by restricting the support of the prior distribution according the desired constraint.

**Example 6.1**: Consider the linear regression model

\[Y_{i}=\Theta x_{i}+\varepsilon_{i},\ i=1,\ldots,n,\]

[MISSING_PAGE_FAIL:188]

Solve this equation to obtain

\[m=\tau(x)\Phi^{-1}(1-\Phi(\nu(x,y)/\tau(x))/2)+\nu(x,y).\]

The generalized maximum likelihood estimate of \(\Theta\) is the maximizer of

\[N(\nu(x,y),\tau^{2}(x))\]

subject to \(\theta\geq 0\). So it is \(\max(\gamma(x,y),0)\). 

### Bayes rule and unbiasedness

Recall from Chapter 3 that an estimator \(d(X)\) of \(\theta\) (lower case, as in the frequentist setting) is unbiased if \(E_{\theta}d(X)=\theta\) for all \(\theta\). In the Bayesian setting, this means \(E(d(X)|\Theta)_{\theta}=\theta\) for all \(\theta\in\Omega_{\Theta}\). In fact, since conditional expectation is unique modulo \(P\), it is more accurate to write unbiasedness in the Bayesian setting as

\[E(d(X)|\Theta)=\Theta\;\;[P].\]

It is then natural to ask: is the Bayes rule unbiased? The answer is, somewhat surprisingly, no.

**Theorem 6.3**: _Suppose that \(d_{B}(X)\) is a Bayes rule with respect to the \(L_{2}\)-loss (6.1) and suppose \(d_{B}(X)\) is unbiased. Then_

\[d_{B}(X)=\Theta\;\;[P].\]

This theorem says that, unless \(d_{B}(X)=\Theta\) modulo \(P\), a Bayes rule with respect to the \(L_{2}\)-loss cannot be unbiased. But \(d_{B}(X)=\Theta\;\;[P]\) means we can estimate \(\Theta\) perfect modulo \(P\) -- a clearly unrealistic premise. Thus, the theorem implies that, for all practical purposes, the Bayes rule is always biased.

Proof: By Theorem 6.1 and Problem 6.3, \(d_{B}\) takes the form

\[d_{B}(X)=\left[E(W(\Theta)|X)\right]^{-1}E\left[W(\Theta)\Theta|X \right]\;\;[P]. \tag{6.5}\]

It suffices to show that \(r(d_{B})=0\). In the following we will abbreviate \(d(X)\) by \(d\) and \(W(\Theta)\) by \(W\). Using the iterative law of conditional expectations, we have

\[r(d_{B}) =E\{E[(\Theta-d_{B})^{T}W(\Theta-d_{B})|X]\}\] \[=E[E(\Theta^{T}W\Theta|X)-2d_{B}^{T}E(W\Theta|X)+d_{B}^{T}E(W|X)d _{B}]\]

Substitute (6.5) for the second \(d_{B}\) in the last term on the right hand side, to obtain \[r(d_{B})=E[E(\Theta^{T}W\Theta|X)-d_{B}^{T}E(W\Theta|X)]=E(\Theta^{T}W\Theta-d_{B}^{ T}W\Theta).\]

Now apply the iterative law of conditional expectations in the reversed order, to obtain

\[r(d_{B})=E[\Theta^{T}W\Theta-E(d_{B}^{T}|\Theta)W\Theta]=E(\Theta^{T}W\Theta- \Theta^{T}W\Theta)=0.\]

where the second equality holds because, by unbiasedness of \(d_{B}\), \(E(d_{B}|\Theta)=\Theta\). \(\Box\)

This theorem is by no means suggests that the Bayes rule with respect to the \(L_{2}\)-loss is undesirable. In fact, many useful estimators are not unbiased, and the unbiasedness seems to be a too strict requirement for many practical purposes.

### Error assessment of estimators

Conceptually, the error of an estimate has very different meanings in the Bayesian and the frequentist setting. In the frequentist setting, \(\theta\) is fixed and the error of \(d(X)\) is measured by how closely the distribution of \(d(X)\) clusters around \(\theta\). In Bayesian analysis, \(d(x)\) is fixed, and its error is measured by how closely the posterior distribution of \(\Theta|X\) clusters around \(d(x)\). One such measure is the posterior variance \(\mbox{var}(\Theta|X)\). As usual, when \(\Theta\) is a vector, \(\mbox{var}(\Theta|X)\) is a matrix. Another measurement of error is the posterior mean squared error, which is the matrix

\[\mbox{mse}_{d}(\Theta|X)=E[(d(X)-\Theta)(d(X)-\Theta)^{T}|X].\]

Note that the posterior variance is not estimator specific -- it only applies to the Bayes rule \(d_{B}\) for the \(L_{2}\)-loss. In fact, we have

\[\mbox{var}(\Theta|X)=\mbox{mse}(\Theta,d_{B}|X).\]

The posterior mean squared error is specific to the estimator \(d\), and is used as a measurement of error of \(d(X)\). It can be shown that (Problem 6.6) the posterior mean squared error and the posterior variance are related by

\[\mbox{mse}_{d}(\Theta|X)=\mbox{var}(\Theta|X)+(d(X)-d_{B}(X))(d(X)-d_{B}(X))^{ T}. \tag{6.6}\]

The next example illustrates the calculation of \(\mbox{var}(\Theta|X)\) and \(\mbox{mse}_{d}(\Theta|X)\).

**Example 6.2** Suppose \(X_{1},\ldots,X_{n}|\phi\) are i.i.d. \(N(0,\phi)\) variables and \(\Phi\sim\tau\chi_{(\nu)}^{-2}\). Then, by Problem 5.14,

\[\Phi|x\sim(S+\tau)\chi_{(\nu+n)}^{2}\]where \(S=\sum_{i=1}^{n}x_{i}^{2}\). By Problem 6.5,

\[E(\Phi|X)=\frac{S+\tau}{\nu+n-2},\quad\mathrm{var}(\Phi|X)=\frac{2(S+\tau)^{2}}{( \nu+n-2)^{2}(\nu+n-4)}.\]

Let \(d_{1}(X)\) be the usual unbiased estimator of \(\Phi\); that is, \(d_{1}(X)=S/(n-1)\). Then

\[\mathrm{mse}_{d_{1}}(\Phi|X)=\frac{2(S+\tau)^{2}}{(\nu+n-2)^{2}(\nu+n-4)}+\left( \frac{S}{n-1}-\frac{S+\tau}{\nu+n-2}\right)^{2}.\]

Let \(d_{2}(X)\) be the generalized maximum likelihood estimate, which, by Problem 6.5 is of the form \((S+\tau)/(n+\nu+2)\). Then

\[\mathrm{mse}_{d_{2}}(\Phi|X)=\frac{2(S+\tau)^{2}}{(\nu+n-2)^{2}(\nu+n-4)}+ \left(\frac{S+\tau}{n+\nu+2}-\frac{S+\tau}{\nu+n-2}\right)^{2}.\]

It is often easier to compute mse through relation (6.6) than to compute it directly. \(\Box\)

### Credible sets

The credible set is the Bayesian counterpart of the frequentist confidence set, and has a more direct interpretation. Recall that, in the frequentist setting, a confidence set is a random set that covers a nonrandom parameter with certain probability. In Bayesian analysis, since \(\Theta\) is random, the credible set is directly defined as a set in \(\mathcal{F}_{\Theta}\) such that \(\Theta\) falls into it with a certain probability.

**Definition 6.5**: \(A\)__\(100(1-\alpha)\%\) _credible set for \(\Theta\) is any \(C\in\mathcal{F}_{\Theta}\) such that \(P(\Theta^{-1}(C)|x)\geq 1-\alpha\)._

According to this definition there can be infinitely many \(100(1-\alpha)\%\) credible sets. But, just as in the frequentist setting (Section 4.8), we would like to minimize the size of the credible set. That gives rise to another criterion for constructing credible sets - the measure of the credible set. The smaller the measure of the credible set, the better. It turns out that the credible set that has the highest posterior density has the smallest measure. We first define the highest posterior density credible set and then prove its optimality.

**Definition 6.6**: _The \(100(1-\alpha)\%\) highest posterior density (HPD) credible set for \(\Theta\) is a \(C\in\mathcal{F}_{\Theta}\) of the form_

\[C=\{\theta\in\Omega_{\Theta}:\pi_{\Theta|X}(\theta|x)\geq\kappa_{\alpha}\}\]

_where \(\kappa_{\alpha}\) is the largest constant such that \(P(\Theta^{-1}(C)|x)\geq 1-\alpha\); that is,_

\[\kappa_{\alpha}=\sup\big{\{}\kappa:P_{\Theta|X}(C(\kappa)|x)\geq 1-\alpha\big{\}}\]

_where \(C(\kappa)\) is the set \(\{\theta:\pi_{\Theta|X}(\theta|x)\geq\kappa\}\)._Intuitively, when \(\kappa\) increases, the posterior probability of \(C\) decreases. The critical point \(\kappa_{\alpha}\) is the largest value of \(\kappa\) before \(P(C|x)\) drops below \(1-\alpha\). The following theorem shows that the measure of the HPD credible set is minimal. The argument is somewhat similar to the Neyman Pearson Lemma.

**Theorem 6.4**: _Suppose that \(\pi_{\Theta}(\theta)>0\) on \(\Omega_{\Theta}\). Let \(C^{*}_{\alpha}\) be a \(100(1-\alpha)\%\) HPD credible set and \(C_{\alpha}\) be any \(100(1-\alpha)\%\) credible set. Furthermore, assume that \(P_{\Theta|X}(C^{*}_{\alpha}|x)=1-\alpha\). Then \(\mu_{\Theta}(C^{*}_{\alpha})\leq\mu_{\Theta}(C_{\alpha})\)._

Proof.: It suffices to show that any \(C\in\mathcal{F}_{\Theta}\) with \(\mu_{\Theta}(C)<\mu_{\Theta}(C^{*}_{\alpha})\) cannot be a \(100(1-\alpha)\%\) credible set. That is,

\[\mu_{\Theta}(C)<\mu_{\Theta}(C^{*}_{\alpha})\Rightarrow P_{\Theta|X}(C|x)<1-\alpha.\]

Let \(C\) be a set in \(\mathcal{F}_{\Theta}\) such that \(\mu_{\Theta}(C)<\mu_{\Theta}(C^{*}_{\alpha})\). Then

\[\mu_{\Theta}(C^{*}_{\alpha})=\mu_{\Theta}(C^{*}_{\alpha}\cap C)+ \mu_{\Theta}(C^{*}_{\alpha}\setminus C),\] \[\mu_{\Theta}(C)=\mu_{\Theta}(C^{*}_{\alpha}\cap C)+\mu_{\Theta}(C \setminus C^{*}_{\alpha}).\]

Because \(\mu_{\Theta}(C)<\mu_{\Theta}(C^{*}_{\alpha})\) we see that \(\mu_{\Theta}(C\setminus C^{*}_{\alpha})<\mu_{\Theta}(C^{*}_{\alpha}\setminus C)\). Moreover, by construction, \(\pi_{\Theta|X}(\theta|x)\geq\kappa_{\alpha}\) on \(C^{*}_{\alpha}\setminus C\) and \(\pi_{\Theta|X}(\theta|x)\leq\kappa_{\alpha}\) on \(C\setminus C^{*}_{\alpha}\). Hence

\[P_{\Theta|X}(C^{*}_{\alpha}\setminus C|x)= \int_{C^{*}_{\alpha}\setminus C}\pi_{\Theta|X}(\theta|x)d\mu_{ \Theta}(\theta)\] \[\geq \kappa_{\alpha}\mu_{\Theta}(C^{*}_{\alpha}\setminus C)\] \[> \kappa_{\alpha}\mu_{\Theta}(C\setminus C^{*}_{\alpha})\] \[\geq \int_{C\setminus C^{*}_{\alpha}}\pi_{\Theta|X}(\theta|x)d\mu_{ \Theta}(\theta)\] \[= P_{\Theta|X}(C\setminus C^{*}_{\alpha}|x). \tag{6.7}\]

Because

\[P_{\Theta|X}(C^{*}_{\alpha}|x)= P_{\Theta|X}(C^{*}_{\alpha}\cap C|x)+P_{\Theta|X}(C^{*}_{\alpha} \setminus C|x),\] \[P_{\Theta|X}(C|x)= P_{\Theta|X}(C^{*}_{\alpha}\cap C|x)+P_{\Theta|X}(C \setminus C^{*}_{\alpha}|x),\]

the inequality (6.7) implies \(P_{\Theta|X}(C|x)<P_{\Theta|X}(C^{*}_{\alpha}|x)=1-\alpha\). So \(C\) is not a \((1-\alpha)\times 100\%\) credible set. 

**Example 6.3**: Consider the model in Example 5.5, where \(X_{1},\ldots,X_{n}|\lambda,\phi\) are i.i.d. \(N(\lambda,\phi)\) random variables, and the prior distribution of \((\Lambda,\Phi)\) is determined by

\[\Lambda|\phi\sim N(a,\phi/m),\quad\Phi\sim\tau\chi_{(\kappa)}^{-2}.\]In Example 5.5 we showed that

\[\frac{\sqrt{n+m}(\Lambda-m(X))}{\sqrt{\tau(X)/(n+\kappa)}}|x\sim t_{(n+k)},\]

where

\[\tau(x) = \sum_{i=1}^{n}(x_{i}-\bar{x})^{2}+\tau+(\bar{x}-a)^{2}(m^{-1}+n^{- 1})\] \[m(x) = (n\bar{x}+ma)/(n+m).\]

So the \(100(1-\alpha)\%\) HPD credible set is

\[\left\{\lambda:-t_{(n+\kappa)}(\alpha/2)<\frac{\sqrt{n+m}(\Lambda-m(X))}{\sqrt {\delta(X)/(n+\kappa)}}<t_{(n+\kappa)}(\alpha/2)\right\}.\]

It can be written as the interval

\[\left(m(x)-t_{(n+\kappa)}(\alpha/2)\sqrt{\delta(x)/[(n+\kappa)(n+m)]},\right.\] \[\left.m(x)+t_{(n+\kappa)}(\alpha/2)\sqrt{\delta(x)/[(n+\kappa)(n+ m)]}\,\right).\]

Also, in Example 5.5 we derived that

\[\frac{\Phi}{\tau(X)}|x\sim\chi_{(n+\kappa)}^{-2}\]

So, if we let \(h(\phi)\) denote the density of \(\chi_{(n+\kappa)}^{-2}\), then the \(100(1-\alpha)\) percent credible set has the form

\[(c_{1}\delta(x),c_{2}\tau(x))\]

where \(c_{1}\), \(c_{2}\) are the solutions to the equations

\[h(c_{1})=h(c_{2}),\quad\int_{c_{1}}^{c_{2}}h(\phi)d\phi=1-\alpha,\]

which can be solved numerically. \(\Box\)

### Hypothesis test

For hypothesis test the action space \(\Omega_{A}\) consists of only two actions, to accept or to reject, which we denote by \(\{a_{0},a_{1}\}\). Similar to the frequentist setting, the hypotheses are \[H_{0}:\Theta\in\Omega^{(0)}_{\Theta}\quad\text{vs}\quad H_{1}:\Theta\in\Omega^{(1)}_ {\Theta},\]

where \(\Omega^{(0)}_{\Theta}\in\mathcal{F}_{\Theta}\) and \(\Omega^{(0)}_{\Theta}\cap\Omega^{(1)}_{\Theta}=\Omega_{\Theta}\). A commonly used loss function is the 0-1 loss. That is, the loss is 1 if we make a wrong decision, and 0 if we make a right decision. In symbols,

\[L(\theta,a)=\begin{cases}0\ \ \text{if}\ \ (\theta,a)\in(\Omega^{(0)}_{\Theta} \times\{a_{0}\})\cup(\Omega^{(1)}_{\Theta}\times\{a_{1}\})\\ 1\ \ \text{if}\ \ (\theta,a)\in(\Omega^{(0)}_{\Theta}\times\{a_{1}\})\cup(\Omega^{(1)}_ {\Theta}\times\{a_{0}\})\end{cases} \tag{6.8}\]

A more nuanced loss function assigns different costs for two types of errors: rejecting \(H_{0}\) when it is right (false positive), or accepting \(H_{0}\) when it is wrong (false negative). In symbols,

\[L(\theta,a)=\begin{cases}0&\text{if}\ \ (\theta,a)\in(\Omega^{(0)}_{\Theta} \times\{a_{0}\})\cup(\Omega^{(1)}_{\Theta}\times\{a_{1}\})\\ c_{0}&\text{if}\ \ (\theta,a)\in(\Omega^{(0)}_{\Theta}\times\{a_{1}\})\\ c_{1}&\text{if}\ \ (\theta,a)\in(\Omega^{(1)}_{\Theta}\times\{a_{0}\})\end{cases} \tag{6.9}\]

where \(c_{0}>0\) and \(c_{1}>0\) represent, respectively, the false positive cost and false negative cost. Note that (6.8) is a special case of (6.9) with \(c_{0}=c_{1}=1\). The next theorem gives the Bayes rule for the loss function (6.9).

**Theorem 6.5**: _Suppose \(0<P_{\Theta|X}(\Omega^{(0)}_{\Theta}|x)<1\) for all \(x\in\Omega_{X}\). Then the Bayes rule for loss function (6.9) is_

\[d_{B}(x)=\begin{cases}a_{0}&\text{if}\ c_{1}P_{\Theta|X}(\Omega^{(1)}_{\Theta} |x)\leq c_{0}P_{\Theta|X}(\Omega^{(0)}_{\Theta}|x)\\ a_{1}&\text{if}\ c_{1}P_{\Theta|X}(\Omega^{(1)}_{\Theta}|x)>c_{0}P_{\Theta|X}( \Omega^{(0)}_{\Theta}|x)\end{cases} \tag{6.10}\]

_Proof._ Since the loss is bounded, by Theorem 5.8 we can calculate the Bayes rule by minimizing the posterior expected loss (Section 5.6). For \(a=a_{0}\),

\[\rho(x,a_{0}) = E[L(\Theta,a_{0})|X]_{x}\] \[= \int_{\Omega^{(0)}_{\Theta}}0dP_{\Theta|X}(\theta|x)+\int_{\Omega ^{(1)}_{\Theta}}c_{1}dP_{\Theta|X}(\theta|x)\] \[= c_{1}P_{\Theta|X}(\Omega^{(1)}_{\Theta}|x).\]

Similarly, \(\rho(x,a_{1})=c_{0}P_{\Theta|X}(\Omega^{(0)}_{\Theta}|x)\). So the Bayes rule for this problem is

\[d_{B}(x)=\text{argmin}\{\rho(x,a):\ a\in\{a_{0},a_{1}\}\},\]

which is the same as (6.10). \(\Box\)

Since \(P_{\Theta|X}(\Theta_{0}|x)=1-P_{\Theta|X}(\Theta_{1}|x)\), we can rewrite the Bayes rule (6.10) as

\[d_{B}(x)=\begin{cases}a_{0}&\text{if}\ P_{\Theta|X}(\Omega^{(1)}_{\Theta}|x) \leq c_{0}/(c_{0}+c_{1})\\ a_{1}&\text{if}\ P_{\Theta|X}(\Omega^{(1)}_{\Theta}|x)>c_{0}/(c_{0}+c_{1}) \end{cases} \tag{6.11}\]In some cases, \(\Omega^{(0)}_{\Theta}\) is a singleton or a finite set. For example for the two sided hypothesis

\[H_{0}:\Theta=\theta_{0}\quad\text{vs}\quad H_{1}:\Theta\neq\theta_{0}, \tag{6.12}\]

\(\Omega^{(0)}_{\Theta}\) is the singleton \(\{\theta_{0}\}\). Any measure \(\mu_{\Theta}\) dominated by the Lebesgue measure will entail \(P_{\Theta|X}(\Omega^{(0)}_{\Theta}|x)=0\), which violates the assumptions in Theorem 6.5. Thus to avoid this difficulty we must use a prior distribution not dominated by the Lebesgue measure.

**Definition 6.7**: _Let \((\Omega,\mathcal{F})\) be a measurable space and \(a\in\Omega\). The Dirac measure for \(a\) is the set function_

\[\delta_{a}(B)=\begin{cases}0&\text{ if }a\notin B\\ 1&\text{ if }a\in B\end{cases}\]

It is left as an exercise to show that \(\delta_{a}\) is indeed a measure on \((\Omega,\mathcal{F})\), and for any \(\mathcal{F}\)-measurable function we have

\[\int_{\Omega}f(\omega)d\delta_{a}(\omega)=f(a).\]

Using the Dirac measure, we can construct a prior distribution \(P_{\Theta}\) with nonzero mass at \(\theta_{0}\), which gives rise to a posterior distribution that gives nonzero mass at \(\theta_{0}\). Let \(\nu_{\Theta}\) be a \(\sigma\)-finite measure on \((\Omega_{\Theta},\mathcal{F}_{\Theta})\): the measure we have in mind is one that is dominated by the Lebesgue measure. Suppose \(Q_{\Theta}\) is a probability measure on \((\Omega_{\Theta},\mathcal{F}_{\Theta})\) dominated by \(\nu_{\Theta}\). Let \(\pi_{\Theta}=dQ_{\Theta}/d\nu_{\Theta}\). Let \(P_{\Theta}\) be the measure on \((\Omega_{\Theta},\mathcal{F}_{\Theta})\) defined by

\[dP_{\Theta}=(1-\epsilon)dQ_{\Theta}+\epsilon d\delta_{\theta_{0}}. \tag{6.13}\]

As we will show in the next theorem, this prior distribution gives rise to a posterior distribution that assign nonzero mass at \(\theta_{0}\).

**Theorem 6.6**: _Suppose the prior distribution \(P_{\Theta}\) is defined by (6.13), where \(\nu_{\Theta}\) is dominated by the Lebesgue measure. Then the posterior probability of the null set in the hypothesis (6.12) is_

\[P_{\Theta|X}(\{\theta_{0}\}|x)=\frac{\epsilon f_{X|\Theta}(x|\theta_{0})}{(1- \epsilon)\int_{\Omega_{\Theta}}f_{X|\Theta}(x|\theta)\pi_{\Theta}(\theta)d\nu _{\Theta}(\theta)+\epsilon f_{X|\Theta}(x|\theta_{0})}\]

_Proof._ Let

\[\tau_{\Theta}(\theta)=\begin{cases}\pi_{\Theta}(\theta)&\text{ if }\theta\neq \theta_{0}\\ 1&\text{ if }\theta=\theta_{0}\end{cases},\quad\mu_{\Theta}=(1-\epsilon)\nu_{ \Theta}+\epsilon\delta_{\theta_{0}}.\]

Then, for any \(B\in\mathcal{F}_{\Theta}\),\[\int_{B}\tau_{\Theta}(\theta)d\mu_{\Theta}(\theta)=\,(1-\epsilon)\int_{B}\tau_{ \Theta}(\theta)d\nu_{\Theta}(\theta)+\epsilon\delta_{\theta_{0}}(B).\]

Since \(\nu_{\Theta}\) is dominated by the Lebesgue measure, \(\int_{B}\tau_{\Theta}d\nu_{\Theta}=\int_{B}\pi_{\Theta}d\nu_{\Theta},\) and the right hand side above becomes

\[(1-\epsilon)\int_{B}\pi_{\Theta}(\theta)d\nu_{\Theta}(\theta)+\epsilon\delta_{ \theta_{0}}(B)=P_{\Theta}(B).\]

This means \(P_{\Theta}\ll\mu_{\Theta}\) and \(dP_{\Theta}/d\mu_{\Theta}=\tau_{\Theta}.\) That is, \(\tau_{\Theta}\) is the prior density of \(P_{\Theta}\) with respect to \(\mu_{\Theta}.\) The posterior density derived from \(\tau_{\Theta}\) and the likelihood \(f_{X|\Theta}(\theta|x)\) is then

\[\tau_{\Theta|X}(\theta|x)=\frac{f_{X|\Theta}(\theta|x)\tau_{\Theta}(\theta)}{ \int_{\Omega_{\Theta}}f_{X|\Theta}(\theta|x)\tau_{\Theta}(\theta)d\mu_{\Theta} (\theta)}, \tag{6.14}\]

from which it follows that

\[P_{\Theta|X}(\{\theta_{0}\}|x)=\frac{\int_{\{\theta_{0}\}}f_{X|\Theta}(\theta |x)\tau_{\Theta}(\theta)d\mu_{\Theta}(\theta)}{\int_{\Omega_{\Theta}}f_{X| \Theta}(\theta|x)\tau_{\Theta}(\theta)d\mu_{\Theta}(\theta)}. \tag{6.15}\]

Because \(\tau_{\Theta}\) is dominated by the Lebesgue measure we have the following equalities:

\[\begin{split}\int_{\Omega_{\Theta}}f_{X|\Theta}(\theta|x)\tau_{ \Theta}(\theta)d\mu_{\Theta}(\theta)\\ =(1-\epsilon)\int_{\Omega_{\Theta}}f_{\Theta|X}(\theta|x)\pi_{ \Theta}(\theta)d\nu_{\Theta}(\theta)+\epsilon f_{X|\Theta}(x|\theta_{0})\end{split} \tag{6.16}\]

and

\[\int_{\{\theta_{0}\}}f_{X|\Theta}(\theta|x)\tau_{\Theta}(\theta)d\mu_{\Theta}( \theta)=\epsilon f_{\Theta|X}(x|\theta_{0}). \tag{6.17}\]

Substitute (6.16) and (6.17) into (6.15) to complete the proof. \(\Box\)

We can now construct the Bayes rule for the prior distribution defined by (6.13) and the two-sided hypothesis (6.12).

**Corollary 6.2**_If the loss function is defined by (6.9) and the prior distribution \(P_{\Theta}\) is defined by (6.13) where \(\nu_{\Theta}\) is dominated by the Lebesgue measure, then the Bayes for testing hypothesis (6.12) is defined by the rejection region_

\[\frac{\epsilon f_{X|\Theta}(x|\theta_{0})}{(1-\epsilon)\int_{\Omega_{\Theta}}f_ {X|\Theta}(\theta|x)\pi_{\Theta}(\theta)d\nu_{\Theta}(\theta)+\epsilon f_{X| \Theta}(x|\theta_{0})}<\frac{c_{1}}{c_{0}+c_{1}} \tag{6.18}\]

We can also express the rejection rule (6.18) in terms of the posterior density. Let \(\pi_{\Theta|X}\) denote the posterior density derived from \(\pi_{\Theta}\) and \(f_{X|\Theta}\)Note that this is not the true posterior density, but rather the posterior density derived from the continuous component of \(\tau_{\Theta}\). In practice, this is usually the continuous prior we assign when probability at \(\theta_{0}\) being \(0\) is not of a concern. We can now express the rejection rule (6.18) in terms on the \(\pi_{\Theta|X}\).

**Corollary 6.3**: _If \(\pi(\theta_{0})\neq 0\), then the rejection region (6.18) can be expressed as_

\[\frac{\epsilon\pi_{\Theta|X}(\theta_{0}|x)}{(1-\epsilon)\pi_{ \Theta}(\theta_{0})+\epsilon\pi_{\Theta|X}(\theta_{0}|x)}<\frac{c_{0}}{c_{0}+ c_{1}}. \tag{6.19}\]

_Proof._ Rewrite the density (6.14) as

\[\tau_{\Theta|X}(\theta|x)=\frac{f_{X|\Theta}(\theta|x)\pi_{ \Theta}(\theta)(1-I_{\{\theta_{0}\}})(\theta)+f_{X|\Theta}(x|\theta_{0})I_{\{ \theta_{0}\}}}{(1-\epsilon)\int_{\Omega_{\Theta}}f_{X|\Theta}(\theta|x)\pi_{ \Theta}(\theta)d\nu_{\Theta}(\theta)+\epsilon f_{X|\Theta}(x|\theta_{0})}. \tag{6.20}\]

Let \(f_{X}\) be the marginal density of \(X\) derived from \(\pi_{\Theta}\) and \(f_{X|\Theta}\); that is,

\[f_{X}(x)=\int_{\Omega_{\Theta}}f_{X|\Theta}(\theta|x)\tau_{ \Theta}(\theta)d\mu_{\Theta}(\theta).\]

Again, this is not the true marginal density, which is based on \(\tau_{\Theta}\) and \(f_{X|\Theta}\). Divide the numerator and denominator of (6.20) by \(f_{X}(x)\), to obtain

\[\tau_{\Theta|X}(\theta|x)=\frac{\pi_{\Theta|X}(\theta|x)(1-I_{\{ \theta_{0}\}}(\theta))+[\pi_{\Theta|X}(\theta_{0}|x)/\pi_{\Theta}(\theta_{0})] I_{\{\theta_{0}\}}(\theta)}{(1-\epsilon)+\epsilon\pi_{\Theta|X}(\theta_{0}|x)/ \pi_{\Theta}(\theta_{0})}.\]

Now take the integral \(\int_{\{\theta_{0}\}}(\cdots)d\tau_{\Theta}\) on both sides of the equation to complete the proof. \(\Box\)

The next example illustrates the construction of one-sided and two-sided tests.

**Example 6.4**: Suppose that \(X_{1},\ldots,X_{n}|\theta\) are i.i.d. \(\mathrm{Exp}(\theta)\) variables. Then the likelihood function is

\[f_{X|\Theta}(x|\theta)=\theta^{-n}e^{-t(x)/\theta},\quad\text{ where }\theta>0\text{ and }t(x)=\sum_{i=1}^{n}x_{i}.\]

Suppose \(\Theta\sim\tau\chi_{(m)}^{-2}\). Then it can be shown that (Problem 6.7)

\[\Theta|x\sim(2t(x)+\tau)\chi_{(2n+m)}^{-2}.\]

Suppose we want to test the one-sided hypothesis

\[H_{0}:\Theta\leq a\quad\text{vs}\quad H_{1}:\Theta>a.\]

By (6.11), the Bayes rule has rejection region \[1-F\left(\frac{a}{2t(x)+\tau}\right)>\frac{c_{1}}{c_{0}+c_{1}},\]

where \(F\) is the c.d.f. of \(\chi_{(2n+m)}^{-2}\). An alternative way to write this rule is

\[a<(\tau+2t(x))\chi_{(2n+m)}^{-2}(c_{1}/(c_{0}+c_{1})).\]

To test the two sided hypothesis

\[H_{0}:\Theta=a\quad\text{vs}\quad H_{1}:\Theta\neq a,\]

it is more convenient to use (6.19) than (6.18), because the form of \(\pi_{\theta|X}\) is known. To use this rule we simply substitute \(\pi_{\Theta}(\theta_{0})\) and \(\pi_{\Theta|X}\) by the densities of \(\tau\chi_{(m)}^{-2}\) and \((2t(x)+\tau)\chi_{(2n+m)}^{-2}\). \(\Box\)

### 6.6 Classification

In a classification problem both the parameter space and the action space are finite:

\[\Omega_{\Theta}=\{1,\ldots,k\},\quad\Omega_{A}=\{a_{1},\ldots,a_{k}\}.\]

As before, \(X\) is a \(p\)-dimensional random vector. Conditioning on \(\Theta=\theta\), \(X\) has distribution \(P_{X|\Theta}(\cdot|\theta)\) for \(\theta=1,\ldots,k\). The likelihoods \(P_{X|\Theta}(\cdot|\theta)\), \(\theta=1,\ldots,k\), represent "classes" or "sub-populations". These distributions describe \(k\) clusters or data clouds in a \(p\)-dimensional space. Estimating \(\Theta\) amounts to determining to which cluster a newly observed \(X\) belongs. An action \(a_{\theta}\in\Omega_{A}\) is the action of choosing class \(\theta\). For simplicity, we write

\[\Omega_{A}=\{1,\ldots,k\}\]

where each \(a\in\Omega_{A}\) is to be interpreted as "choosing class \(a\)".

A distinct feature of the classification problem is that both the prior distribution and the likelihood function are to be estimated from a training sample or training data set, whereas the decision rule is to be applied to a separate testing sample or testing data set. Because of this one might argue that a classification procedure described here is not a truly Bayesian procedure. Nevertheless, if we regard the training data as prior knowledge, then it is Bayesian relative to this prior knowledge. Also note that this feature (of having to estimate prior and likelihood function) should not be confused with the empirical Bayes procedure to be discussed in the next section: in the latter case, the prior is estimated by the same sample that is used for parameter estimation.

The loss function in this setting can be represented by a matrix:\[L(i,j)=c_{ij},\quad\text{where}\quad c_{ij}\begin{cases}=0&\text{ if }\ i=j\\ >0&\text{ if }\ i\neq j\end{cases} \tag{6.21}\]

The next theorem gives the Bayes rule for this problem.

**Theorem 6.7**: _The Bayes rule with respect to the loss function (6.21) is_

\[d_{B}(x)=\operatorname{argmin}\{a:\sum_{\theta=1}^{k}c_{\theta a}f_{X|\Theta}(x |\theta)\pi_{\Theta}(\theta)\}. \tag{6.22}\]

_Proof._ Because the loss function is bounded, by Theorem 5.8\(d_{B}(x)\) is the minimizer of the posterior expected loss \(\rho(x,a)\), which in this context takes the form

\[\rho(x,a)=\,E(L(\Theta,a)|X)_{\theta}=\sum_{\theta=1}^{k}c_{\theta a}\pi_{ \Theta|X}(\theta|x).\]

Thus

\[d_{B}(x)=\operatorname{argmin}\{a:\rho(x,a)\}=\operatorname{argmin}\{a:\sum_{ \theta=1}^{k}c_{\theta a}\pi_{\Theta|X}(\theta|x)\}.\]

Because \(\pi_{\Theta|X}(\theta|x)\) is proportional to \(f_{X|\Theta}(x|\theta)\pi_{\Theta}(\theta)\), where the proportionality constant is independent of \(a\), the above minimizer can be rewritten as (6.22). \(\Box\)

To use the Bayes rule (6.22) we need to know the likelihood function \(f_{X|\Theta}\) and the prior density \(\pi_{\Theta}\). This is where the classification problem differs from a typical Bayesian procedure. In actual classification problems the class label does not fully specify the distribution of that class. We can think of \(k\) data clouds situated in a \(p\)-dimensional space, with each cloud having a different shape. The labels can only tell us cloud A, cloud B,..., but does not carry the information about the shape of each cloud. Thus the distribution \(P_{X|\Theta}(\cdot|\theta)\) requires an additional parameter (real- or vector-valued), say \(\Psi\), to specify itself. Let us then denote the likelihood as \(P_{X|\Theta\Psi}\), with \(\Theta\) representing class label and \(\Psi\) representing the additional parameter.

A training data set is one for which the class label for each \(X\) is known. Thus, we have

\[X_{\theta 1},\ldots,X_{\theta n_{\theta}}\text{ is an i.i.d. sample from }P_{X|\Theta\Psi}(\cdot|\theta,\psi),\ \theta=1,\ldots,k.\]

We can then use the training data set

\[\{X_{\theta 1},\ldots,X_{\theta n_{\theta}}\},\quad\theta=1,\ldots,k,\]

to estimate \(\Psi\), by either a Bayesian or a frequentist method. Denote this estimate by \(\hat{\psi}\). The training sample also allows us to estimate the prior probability of each class. For example, a natural estimate is the proportion of the sample size of a class relative to the total sample size:

\[\hat{\pi}_{\Theta}(\theta)=n_{\theta}/(n_{1}+\cdots+n_{k}).\]

We can then substitute \(P_{X|\Theta\Psi}(\cdot|\theta,\hat{\psi})\) and \(\hat{\pi}_{\Theta}(\theta)\) into the Bayes rule (6.22) to perform classification. That is,

\[\hat{d}_{B}(x)=\arg\!\min\{a:\sum_{\theta=1}^{k}c_{\theta a}f_{X|\Theta\Psi}(x| \theta,\hat{\psi})\hat{\pi}_{\Theta}(\theta)\}, \tag{6.23}\]

where \(f_{X|\Theta\Psi}\) is the density of \(P_{X|\Theta\Psi}\).

Commonly used models for \(f_{X|\Theta\Psi}\) are multivariate Normal distributions with equal or unequal covariance matrices. That is,

\[\begin{cases}N(\mu_{\theta},\varSigma)\\ N(\mu_{\theta},\varSigma_{\theta})\end{cases},\quad\theta=1,\ldots,k, \tag{6.24}\]

where \(\mu_{\theta}\) are \(p\)-dimensional vectors and \(\varSigma_{\theta},\varSigma\) are \(p\times p\) positive definite matrices. The Bayes rule based on the first model (with common variance matrix) is called _linear discriminant analysis_ (LDA); that based on the second model (with different variance matrices) is called _quadratic discriminant analysis_ (QDA). For example, if we use the UMVU estimates, then the estimates of \(\mu_{\theta}\) and \(\varSigma\) for LDA are:

\[\begin{split}\hat{\mu}_{\theta}&=\,n_{\theta}^{-1} \sum_{i=1}^{n_{\theta}}X_{\theta i},\\ \hat{\varSigma}&=\,\left(\sum_{\theta=1}^{k}n_{\theta }-k\right)^{-1}\sum_{\theta=1}^{k}\sum_{i=1}^{n_{\theta}}(X_{\theta i}-\hat{ \mu}_{\theta})(X_{\theta i}-\hat{\mu}_{\theta})^{T},\end{split} \tag{6.25}\]

and the estimates of \(\mu_{\theta}\) and \(\varSigma_{\theta}\) for QDA are:

\[\begin{split}\hat{\mu}_{\theta}&=\,n_{\theta}^{-1} \sum_{i=1}^{n_{\theta}}X_{\theta i},\\ \hat{\varSigma}_{\theta}&=\,(n_{\theta}-1)^{-1}\sum_ {i=1}^{n_{\theta}}(X_{\theta i}-\hat{\mu}_{\theta})(X_{\theta i}-\hat{\mu}_{ \theta})^{T}.\end{split} \tag{6.26}\]

It is left as an exercise (6.10) to show that these two sets of estimators are indeed the UMVU estimators for the two models in (6.24). The reason for the qualifiers _linear_ and _quadratic_, is that, for LDA, \(\hat{d}_{B}(x)\) can be expressed in terms of a linear function of \(x\) when \(k=2\); whereas for QDA, \(\hat{d}_{B}(x)\) can be expressed in terms of a quadratic function of \(x\) when \(k=2\) (see Problems 6.8 and 6.9).

LDA and QDA each have their own advantages for different distribution shapes. If we imagine the \(k\) data clouds are all watermelon (or football) shaped, then LDA works the best when these watermelons all have the same orientation and shape (but they can have different sizes), and QDA works the best when these watermelons have different orientations, and have different shapes (some longer, some rounder); they can also have different sizes.

## 6.7 Stein's phenomenon

A phenomenon discovered by Stein in a short paper (Stein, 1956) turned out to be highly influential in a wide range of subsequent developments, such as shrinkage estimation and empirical Bayes. A lemma implicitly used in that paper (see also Stein, 1981), the famous Stein's lemma, is frequently used in the area of sufficient dimension reduction (Li, 1992) and several other areas. We devote this section to that result.

The phenomenon concerns the inadmissibility of a maximum likelihood estimate based on the multivariate Normal distribution. We first state Stein's Lemma Stein (1981), which is a convenient method to demonstrate Stein's phenomenon. Although the lemma can be proved by a single line of integration by parts, such a proof requires conditions quite strong. We will instead use the original argument of Stein (1981) that relies on Fubini's theorem, which requires weaker assumptions than those used in the proof via integration by parts.

Since the discussion of this section is largely in the realm of frequentist decision theory, we tentatively revert to the frequentist notation. For example, we use \(P_{\theta}\) instead of \(P_{X|\Theta}(\cdot|\theta)\) to denote the conditional distribution of \(X\) given \(\theta\).

**Lemma 6.1**: _Suppose \(X\) is a Normally distributed random variable. Let \(g:real\to\mathbb{R}\) be a function differentiable \([\lambda]\), \(\lambda\) being the Lebesgue measure, and its derivative \(\dot{g}\) satisfies \(E|\dot{g}(X)|<\infty\). Then_

\[\mathrm{cov}(X,g(X))=\mathrm{var}(X)E\dot{g}(X). \tag{6.27}\]

_Proof._ First, assume \(X\sim N(0,1)\). Let \(\phi\) denote the standard Normal density. Let \(a\) be a point in \(\mathbb{R}\) at which \(g\) is differentiable. Recall that \(\dot{\phi}(y)=-y\phi(y)\). We have

\[\int_{-\infty}^{\infty}\dot{g}\phi dx = \int_{-\infty}^{a}\dot{g}\phi dx+\int_{a}^{\infty}\dot{g}\phi dx\] \[= \int_{-\infty}^{a}\dot{g}(x)\int_{-\infty}^{x}\dot{\phi}(z)dzdx- \int_{a}^{\infty}\dot{g}(x)\int_{x}^{\infty}\dot{\phi}(z)dzdx.\]

By Fubini's theorem, the right hand side is \[\int_{-\infty}^{a}\dot{\phi}(z)\int_{z}^{a}\dot{g}(x)dxdz-\int_{a}^{ \infty}\dot{\phi}(z)\int_{a}^{z}\dot{g}(x)dxdz\] \[=\int_{-\infty}^{a}\dot{\phi}(z)(g(a)-g(z))dz-\int_{a}^{\infty} \dot{\phi}(z)(g(z)-g(a))dz\] \[=-\int_{-\infty}^{\infty}\dot{\phi}(z)g(z)dz\] \[=\int_{-\infty}^{\infty}z\phi(z)g(z)dz.\]

This proves the theorem for the standard Normal case. Now suppose \(X\sim N(\mu,\sigma^{2})\). Then \(Z=(X-\mu)/\sigma\) has a standard Normal distribution. Hence, if we let \(g_{1}(z)=g(\sigma z+\mu)\), then

\[\mathrm{cov}(X,g(X))= \,\mathrm{cov}(\sigma Z+\mu,g_{1}(Z))\] \[= \,\sigma\mathrm{cov}(Z,g_{1}(Z))=\sigma E\dot{g}_{1}(Z)=\sigma^{ 2}E\dot{g}(X)=\mathrm{var}(X)E\dot{g}(X),\]

as to be demonstrated. \(\Box\)

We now generalize this lemma to the multivariate case.

**Lemma 6.2**: _Suppose \(X\) is a \(p\)-dimensional random vector distributed as \(N(\theta,I_{p})\). Suppose \(g:\Omega_{X}\to\mathbb{R}^{q}\) is a differentiable function such that the components of \(\partial g/\partial x^{T}\) are integrable. Then_

\[E[(X-EX)g^{T}(X)]=E[\partial g^{T}(X)/\partial x].\]

Proof: The \((i,j)\)th entry of the matrix on the left is

\[E[(X_{i}-EX_{i})^{T}g_{j}(X)]=E\{E[(X_{i}-EX_{i})^{T}g_{j}(X)|X_{\ell},\ell\neq i ]\}.\]

Consider the conditional expectation

\[E[(X_{i}-EX_{i})g_{j}(X)|X_{\ell}=x_{\ell},\ell\neq i].\]

Because \(X_{i}\) is independent of \(\{X_{\ell}:\ell\neq i\}\), the above conditional expectation is the unconditional expectation

\[E[(X_{i}-EX_{i})g_{j}(x_{1},\ldots,x_{i-1},X_{i},x_{i+1},\ldots,x_{p})],\]

which only involves one random variable \(X_{i}\). By Lemma 6.1, the above expectation is the same as

\[E[\partial g_{j}(x_{1},\ldots,x_{i-1},X_{i},x_{i+1},\ldots,x_{p})/\partial x_{ i}]=E[\partial g_{j}(X)/\partial x_{i}|X_{\ell}=x_{\ell},\ell\neq i].\]

In other words,

\[E[(X_{i}-EX_{i})g_{j}(X)|X_{\ell},\ell\neq i]=E[(\partial g_{j}(X)/\partial x _{i}|X_{\ell},\ell\neq i].\]Now take expectation on both sides to complete the proof. 

We are now ready to state Stein's paradox. Recall that, if \(X\sim N(\theta,\Sigma)\), then \(X\) is the maximum likelihood estimate of \(\theta\). In fact, \(X\) is also the UMVU estimator of \(\theta\). Stein (1956) showed that there is an estimator that is uniformly better than \(X\) in terms of the frequentist risk \(R(\theta,d)\) for the \(L_{2}\)-loss.

**Theorem 6.8**_Suppose \(X\sim N(\theta,I_{p})\), \(p\geq 3\), \(L(\theta,a)=\|\theta-a\|^{2}\), and \(d(x)=x\). Then there is a decision rule \(d_{1}\in\mathcal{D}\) such that_

\[R(\theta,d_{1})<R(\theta,d),\quad\text{for all }\theta\in\mathbb{R}.\]

Proof.: Consider the alternative estimate

\[d_{1}(x)=[1-h(x)]x,\]

where \(h:\mathbb{R}^{p}\to\mathbb{R}\) is a function to be specified later. For now we assume nothing about \(h\) except that it must make \(d_{1}\) square integrable \(P_{\theta}\), and that \(h(x)x\) must satisfy the conditions for \(g\) in Lemma 6.2. We have

\[\begin{split} R(\theta,d_{1})=&\,E\|X-\theta-h(X)X \|^{2}\\ =&\,R(\theta,d)-2E[(X-\theta)^{T}h(X)X]+E[h^{2}(X)\| X\|^{2}].\end{split} \tag{6.28}\]

By Lemma 6.2,

\[\begin{split} E[(X-\theta)^{T}h(X)X]=&\,\text{tr} [E(X-\theta)h(X)X^{T}]\\ =&\,\text{tr}\{XE[\partial h(X)/\partial x^{T}+h(X)I _{p}]\}\\ =&\,E[(\partial h(X)/\partial x^{T})X]+pEh(X).\end{split}\]

Hence

\[R(\theta,d_{1})=R(\theta,d)-2E[(\partial h/\partial x^{T})X]-2pEh(X)+E[h^{2}(X )\|X\|^{2}].\]

Let \(h(x)=\alpha/\|x\|^{2}\). Then \(h\) is differentiable and the derivative is integrable, so that the above equality holds for this particular choice. Using the form of \(h\) we find, after some elementary computation,

\[-2(\partial h/\partial x^{T})x-2ph(x)+h^{2}(x)\|x\|^{2}=\frac{\alpha(4-2p+ \alpha)}{\|x\|^{2}}.\]

Substitute this into the (6.28) to obtain

\[R(\theta,d_{1})-R(\theta,d)=\alpha(4-2p+\alpha)E(\|X\|^{-2}).\]

The right hand side is negative if \(0<\alpha<2p-4\), which is possible because \(p\geq 3\). For example, if we take \(\alpha=p-2\), then \(R(\theta,d_{1})<R(\theta,d)\) for all \(\theta\in\mathbb{R}^{p}\). 

The choice \(\alpha=p-2\) in the proof leads to the estimator

\[d_{1}(x)=\left(1-\frac{p-2}{\|x\|^{2}}\right)x, \tag{6.29}\]

which is called the James-Stein estimator (James and Stein, 1961).

### Empirical Bayes

In the Bayes procedures described in the previous sections the prior distribution \(P_{\Theta}\) is assumed known -- except for the classification problem in Section 6.6 where it is estimated from the training set. Even if we think of the training set as prior knowledge, \(P_{\Theta}\) has a known form. In the empirical Bayes method the prior distribution is estimated from the same sample that will be used for estimation of the parameter \(\Theta\). Specifically, suppose

\[\{P_{\Theta,b}:\ b\in\Omega_{B}\}\]

is a parametric family of prior distributions defined on the sample space \(\Omega_{B}\). Then, for a \(b\in\Omega_{B}\), the density of \(X\) is

\[f_{X,b}(x)=\int_{\Omega_{\Theta}}f_{X|\Theta}(x|\theta)\pi_{\Theta,b}(\theta)d \mu_{\Theta}(\theta).\]

Thus we have a parametric family of probability distributions indexed by \(b\):

\[\{f_{X,b}(x):\ b\in\Omega_{B}\}.\]

We can then use a frequentist method to estimate \(b\) from this family. For example, we can estimate \(b\) by the maximum likelihood estimate, the moment estimate, or the UMVU estimate described in Chapter 2. Once an estimate \(\hat{b}\) is obtained in this way, we then draw statistical inference about \(\Theta\) using the posterior density

\[\pi_{\Theta|X,b}=\frac{f_{X|\Theta}(\theta|x)\pi_{\Theta,b}(\theta)}{f_{X,b}(x )},\]

with \(b\) replaced by \(\hat{b}\). This procedure is called the Empirical Bayes procedure. See Robbins (1955); Efron and Morris (1973), and Berger (1985, Section 4.5).

The empirical Bayes method is especially useful when we have a large number of parameters -- as many parameters as the sample size \(n\) or even more. These parameters cannot be estimated accurately unless we introduce some structures to them. Assigning these parameters a prior distribution with a common parameter \(b\) is an effective way of building such a structure.

The next example shows that the James-Stein estimator described in the last section can be derived as an empirical Bayes estimator with \(b\) estimated by the UMVUE (Efron and Morris, 1973).

**Example 6.5** Suppose that \(X_{1},\ldots,X_{p}|\Theta\) are independent random variables, where \(\Theta=(\Theta_{1},\ldots\Theta_{p})^{T}\). Also assume:

1. \(\Theta_{1}\leavevmode\hbox{\rm 1\kern-3.8pt\rm l}\leavevmode\nobreak\ \cdots \leavevmode\hbox{\rm 1\kern-3.8pt\rm l}\leavevmode\nobreak\ \Theta_{p}\);
2. \(X_{i}|\Theta=\theta\sim N(\theta_{i},1)\);
3. \(\Theta_{i}\sim N(0,b)\).

Note that assumption 2 also implies \(X_{i}\perp\!\!\!\!\perp\Theta_{(-i)}|\Theta_{i}\), where \(\Theta_{(-i)}\) denote the vector \(\Theta\) with its \(i\)-th component removed. Under these assumptions the likelihood function and the prior density can be simplified as

\[f_{X|\Theta}(x|\theta)=\prod_{i=1}^{p}f_{X_{i}|\Theta}(x_{i}|\theta)=\prod_{i=1 }^{p}f_{X_{i}|\Theta_{i}}(x_{i}|\theta_{i}),\]

\[\pi_{\Theta,b}=\prod_{i=1}^{p}\pi_{\Theta_{i},b}(\theta_{i}).\]

Hence the marginal density \(f_{X,b}(x)\) is

\[f_{X,b}(x)=\prod_{i=1}^{p}\int_{\mathbb{R}}f_{X_{i}|\Theta_{i}}(x_{i}|\theta_{i })\pi_{\Theta_{i},b}(\theta_{i})dP_{\Theta_{i},b}=\prod_{i=1}^{p}f_{X_{i},b}(x _{i}).\]

The posterior density is

\[\pi_{\Theta|X,b}(\theta|X)=\prod_{i=1}^{p}f_{X_{i}|\Theta_{i}}(x_{i}|\theta_{i })\pi_{\Theta_{i},b}(\theta_{i})/f_{X_{i},b}(x_{i})=\prod_{i=1}^{p}\pi_{\Theta |X,b}(\theta_{i}|x_{i},b).\]

The Bayes rule with respect to the \(L_{2}\)-loss \(L(\theta,a)=\|\theta-a\|^{2}\) loss is

\[d_{B}(x)=E(\Theta|X)_{x}=(E(\Theta_{1}|X)_{x},\ldots,E(\Theta_{p}|X)_{x}),\]

where each component is

\[E(\Theta_{j}|X)_{x} = \int_{\mathbb{R}^{p}}\theta_{j}\prod_{i=1}^{p}\pi_{\Theta|X,b}( \theta_{i}|x_{i})d\theta_{1}\cdots d\theta_{p}\] \[= \int_{\mathbb{R}}\theta_{j}\pi_{\Theta_{j}|X_{j},b}(\theta_{j}|x_ {j},b)d\theta_{j}\] \[= E(\Theta_{j}|X_{j})_{x_{j}}.\]

By Example 5.1,

\[\Theta_{j}|x_{j}\sim N\left(\frac{x_{j}}{b^{-1}+1},\frac{1}{b^{-1}+1}\right).\]

So the Bayes rule is

\[d_{B}(x)=\left(\frac{1}{b^{-1}+1}\right)x=\left(1-\frac{1}{1+b}\right)x. \tag{6.30}\]

Now let us derive the UMVU estimate for \(b\). By Example 5.1 again, the marginal distribution of \(X_{i}\) is \(N(0,b+1)\). Hence \(S=\sum_{i=1}^{p}X_{i}^{2}\) is complete and sufficient for \(b\). Moreover, \(S/(b+1)\sim\chi_{(p)}^{2}\), which implies \((b+1)/S\sim\chi_{(p)}^{-2}\). Hence, by Problem 6.5,\[E((b+1)/S)=1/(p-2)\Rightarrow E((p-2)/S)=1/(p-2).\]

Because \(S\) is complete and sufficient, by Corollary 2.2, \((p-2)/S\) is the UMVUE for \(1/(b+1)\). Substituting this estimate for \(1/(b+1)\) into the Bayes rule (6.30) yields the James-Stein estimate (6.29). 

This method can be generalized to \(\theta_{i}\sim N(b_{1},b_{2})\), where \(b_{1}\in\mathbb{R}\), \(b_{2}>0\) are unknown, as well as to the regression setting. Interested readers can consult Morris (1983) and Berger (1985, Section 4.5). Some of these generalizations are given as exercises.

### Problems

**6.1.** Suppose \(X_{1},\ldots,X_{n}\) are an independent sample from \(U(0,\theta)\). Let \(\pi(\theta)\) be the Pareto(\(\theta_{0},\alpha\)) distribution, defined by

\[\pi(\theta)=\frac{\alpha}{\theta_{0}}\left(\frac{\theta_{0}}{\theta}\right)^{ \alpha+1}I_{(\theta_{0},\infty)}(\theta),\ \ \alpha>0,\ \ \theta_{0}>0.\]

1. Find the posterior density \(\pi(\theta|X_{1},\ldots,X_{n})\).

2. Assuming \(\alpha>2\), find the Bayes estimate of \(\theta\) with respect to the \(L_{2}\) loss \(L(\theta,a)=(\theta-a)^{2}\).

3. Assuming \(\alpha>1\), find the Bayes estimate of \(\theta\) with respect to the \(L_{1}\)-loss \(L(\theta,a)=|\theta-a|\).

4. Find the Generalized MLE for \(\theta\).

5. For what values of \(\alpha\) are the Generalized MLE, the Bayes estimates based on \(L_{1}\) and \(L_{2}\) losses approximately the same?

6. Let \(0<\gamma<1\). Find the \((1-\gamma)\)-level HPD credible set for \(\theta\).

7. Derive the Bayes rule (in the form of rejection region) for testing the hypothesis

\[H_{0}:\theta\leq 2\theta_{0}\quad\text{vs}\quad H_{1}:\theta>2\theta_{0}.\]

Use the loss function (6.9) with \(c_{0}=0\), \(c_{1}=1\), \(c_{2}=2\). Make the answer as explicit as possible.

8. Derive the Bayes rule for testing the hypothesis

\[H_{0}:\theta=2\theta_{0}\quad\text{vs}\quad H_{1}:\theta\neq 2\theta_{0}\]

use the same loss function and the prior distribution (6.13).

**6.2.** Let \(X_{1},\ldots,X_{n}\) be an i.i.d. sample from \(N(\mu,\phi)\), and let \(Y_{1},\ldots,Y_{m}\) be an i.i.d. sample from \(N(\nu,\psi)\). Here, \(\mu\) and \(\nu\) are treated as parameters, and \(\phi\) and \(\psi\) as known constants. Suppose we know \(\mu\leq\nu\), and would like to incorporate this prior knowledge into the estimation process. For example,if we are to estimate the population means of men's heights and women's heights, as represented by \(\mu\) and \(\nu\) respectively, then it is quite reasonable to assume \(\mu\geq\nu.\) Assign \((\mu,\nu)\) the improper prior

\[\pi_{\Theta}(\mu,\nu)=I(\mu\leq\nu).\]

Derive the Bayes rule for estimating \((\mu,\nu).\)

Hint: Let \(\delta=\nu-\mu.\) Then the problem becomes that of deriving \(E(\mu|X)\) and \(E(\mu+\delta|X).\) The corresponding improper prior is

\[\pi_{\Theta}(\mu,\delta)=I(\delta\geq 0).\]

**6.3.** Prove that, under the assumptions of Theorem 6.1, the Bayes estimator (6.2) is unique modulo \(P\).

**6.4.** Suppose \(U\) is a random variable defined on \((\Omega,\mathcal{F},P)\) taking values in \(\Omega_{U},\) and \(U\) is integrable with respect to \(P.\) Let \(L(\theta,a)\) be the loss function (6.4). Show that, if \(q\) satisfies

\[F(q)\geq\alpha_{1}/(\alpha_{1}+\alpha_{2}),\quad F(q-)\leq\alpha_{1}/(\alpha_{ 1}+\alpha_{2}),\]

then

\[\int_{\Omega_{U}}|q-U|dP\leq\int_{\Omega_{U}}|a-U|dP\]

for all \(a\in\Omega_{U}.\)

**6.5.** Suppose \(X\sim\chi_{(m)}^{-2}.\)

1. Show that if \(m>2\) then \(X\) is integrable and \(E(X)=(m-2)^{-1}.\)

2. Show that if \(m>4\) then \(X\) has finite variance and

\[\mbox{var}(X)=2(m-2)^{-2}(m-4)^{-1}.\]

3. Show that the mode of the density of \(X\) is \((m+2)^{-1}.\)

**6.6.** Suppose \(\Theta\) is square integrable with respect to \(P\) and \(d\in\mathcal{D}.\) Show that (6.6) holds.

**6.7.** Suppose \(X_{1},\ldots,X_{n}|\theta\) are i.i.d. Exp\((\theta)\) random variables and \(\Theta\sim\tau\chi_{(m)}^{-2}.\) Show that

\[\Theta|x\sim(2t(x)+\tau)\chi_{(2n+m)}^{-2},\]

where \(t(x)=\sum_{i=1}^{n}x_{i}.\)

**6.8.** Show that, under the first model in (6.24) and \(k=2\), the Bayes rule \(\hat{d}_{B}\) based on (6.25) can be rewritten as

\[\hat{d}_{B}(x)=\begin{cases}1&\text{ if }(x-(\hat{\mu}_{1}+\hat{\mu}_{2})/2)^{T} \hat{\Sigma}^{-1}(\hat{\mu}_{2}-\hat{\mu}_{1})\leq\log[c_{21}n_{1}/(c_{12}n_{2})] \\ 2&\text{ if }(x-(\hat{\mu}_{1}+\hat{\mu}_{2})/2)^{T}\hat{\Sigma}^{-1}(\hat{\mu}_{2}- \hat{\mu}_{1})>\log[c_{21}n_{1}/(c_{12}n_{2})].\end{cases}\]

Note that, when

\[c_{21}n_{1}=c_{12}n_{2}, \tag{6.31}\]

the Bayes rule reduces to

\[\hat{d}_{B}(x)=1\ \ \text{iff }\ g(x)\leq g((\hat{\mu}_{1}+\hat{\mu}_{2})/2),\]

where

\[g(x)=x^{T}\hat{\Sigma}^{-1}(\hat{\mu}_{2}-\hat{\mu}_{1}).\]

This function is linear in \(x\), an is called Fisher's linear discriminant function (Fisher, 1935).

**6.9.** Show that, under the second model in (6.24), \(k=2\), and (6.31), the Bayes rule \(\hat{d}_{B}(x)\) based on (6.26) takes action \(1\) if and only

\[(x-\hat{\mu}_{1})^{T}\hat{\Sigma}_{1}^{-1}(x-\hat{\mu}_{1})-(x-\hat{\mu}_{2})^{ T}\hat{\Sigma}_{1}^{-1}(x-\hat{\mu}_{2})\leq\det(\hat{\Sigma}_{2})-\det(\hat{ \Sigma}_{2}).\]

The left hand side is a quadratic function of \(x\), and is called the quadratic discriminant function.

**6.10.** Let

\[X_{i1},\ldots,X_{in_{i}},\quad i=1,\ldots,k\]

be \(p\) dimensional random vectors. Assume:

a. for each \(i\), \(X_{i1},\ldots,X_{in_{i}}\) are an i.i.d. \(N(\mu_{i},\Sigma_{i})\);

b. for \(i\neq j\),

\[\{X_{i1},\ldots,X_{in_{i}}\}\,\hbox{\rm 1\kern-2.8pt\rm l}\,\{X_{j1},\ldots,X_{jn _{j}}\}.\]

For each \(i=1,\ldots,k\), let

\[\hat{\mu}_{i}=n_{i}^{-1}\sum_{\ell=1}^{n_{i}}X_{i\ell},\quad\hat{\Sigma}_{i}=( n_{i}-1)^{-1}\sum_{\ell=1}^{n_{i}}(X_{i\ell}-\hat{\mu}_{i})(X_{i\ell}-\hat{ \mu}_{i})^{T}.\]

Also, let \(n=n_{1}+\cdots+n_{k}\). Prove the following statements:

1. The statistic \[\{(\hat{\mu}_{i},\hat{\Sigma}_{i}):i=1,\ldots k\}\] is the UMVU estimator of \(\{(\mu_{i},\Sigma_{i}):i=1,\ldots,k\}\).

2. If \(\Sigma_{1}=\cdots=\Sigma_{k}\), then the statistic \[\left(\hat{\mu}_{1},\ldots,\hat{\mu}_{k},\quad\frac{\sum_{\theta=1}^{k}(n_{\theta }-1)\hat{\Sigma}_{\theta}}{\sum_{\theta=1}^{k}n_{\theta}-1}\right)\] is the UMVU estimator for \((\mu_{1},\ldots,\mu_{k},\Sigma)\).

**6.11.** Prove the following variations of Stein's lemma. Suppose that \(X\) is a random variable distributed as \(N(\theta,1)\). Then the following equalities hold.

1. If \(g:\mathbb{R}\rightarrow\mathbb{R}\) is twice differentiable \([\lambda]\), \(\lambda\) being the Lebesgue measure, and \(|g(X)|\) and \(|g^{{(2)}}|\) have finite expectations, then \[E[(X-EX)^{2}g(X)]=E[g(X)+g^{{(2)}}(X)].\]
2. If, in addition, \(g\) is four times differentiable \([\lambda]\) and \(|g^{{(4)}}|\) has a finite expectation, then \[E[(X-\theta)^{{4}}g(X)]=E[3g(X)+6g^{{(2)}}+g^{{(4)}}(X)].\]

**6.12.** Suppose \(X\) is a \(p\)-dimensional multivariate Normal random vector and \(g:\mathbb{R}^{p}\rightarrow\mathbb{R}^{p}\) is differentiable modulo \(\lambda\), the Lebesgue measure on \((\mathbb{R}^{p},\mathcal{R}^{p})\). Suppose the entries of \(\partial g^{T}/\partial x\) are integrable. Then

\[\mathrm{cov}[X,g(X)]=\mathrm{var}(X)E[\partial g^{T}(X)/\partial x].\]

**6.13.** The Beta prime distribution is defined by the density function

\[f(x)=x^{\alpha-1}(x+1)^{-\alpha-\beta},\quad x>0,\ \alpha>0,\ \beta>0,\]

which we write as \(\mathrm{Beta}^{\prime}(\alpha,\beta)\).

1. Show that if \(X\sim\mathrm{Beta}^{\prime}(\alpha,\beta)\) and \(\beta>1\), then \(X\) is integrable and \[E(X)=\alpha/(\beta-1).\]
2. Show that if \(X\sim\mathrm{Beta}^{\prime}(\alpha,\beta)\) and \(\beta>2\) then \(X\) has finite variance and \[\mathrm{var}(X)=\frac{\alpha(\alpha+\beta-1)}{(\beta-2)(\beta-1)^{2}}.\]
3. Show that, if \(S|\phi\sim\phi\chi_{(n)}^{2}\) and \(\Phi\sim\tau\chi_{(m)}^{-2}\), then \[\frac{S}{\tau}\sim\mathrm{Beta}^{\prime}(n/2,m/2).\]

**6.14.** Suppose \(\phi=(\phi_{1},\ldots,\phi_{n})\), where the \(n\) components are i.i.d. \(\tau\chi_{(m)}^{-2}\) random variables. Suppose \(S_{1},\ldots,S_{n}|\phi\) are independent random variables with \(S_{i}|\phi\sim\phi_{i}\chi_{(n_{i}-1)}^{-2}\). Here \(S_{i}\) may be the sum of the squared errors of an i.i.d. sample \(X_{i1},\ldots,X_{in_{i}}\sim N(\mu_{i},\phi_{i})\) from the \(i\)th group.

1. Assuming \(\tau\) is known, derive a Bayes rule for estimating \(\phi\).
2. Use the result of Problem 6.13 to find an unbiased estimate of \(\tau\) of the form \[\hat{\tau}=\sum_{i=1}^{n}w_{i}S_{i}.\]
3. Construct an empirical Bayes estimate of \(\phi\) based on parts 1 and 2.

**6.15.** Suppose \(\theta_{1},\ldots,\theta_{n}\) are i.i.d. \(N(b_{1},b_{2})\) random variables, where \(b_{1}\in\mathbb{R}\) and \(b_{2}>0\). Let \(\theta=(\theta_{1},\ldots,\theta_{n})\). Suppose \(X_{1},\ldots,X_{n}|\theta\) are independent and \(X_{i}|\theta\sim N(\theta_{i},1)\).

1. Assuming \(b_{1}\) and \(b_{2}\) are known, derive a Bayes rule for estimating \(\theta\).
2. Find the marginal distribution of \((X_{1},\ldots,X_{n})\) for a fixed \((b_{1},b_{2})\). Based on this distribution find the UMVU estimates of \((b_{1},b_{2})\).
3. Construct an empirical Bayes rule for estimating \(\theta\) based on the above two parts.

**6.16.** Suppose \(\theta_{1},\ldots,\theta_{n}\) are independent random variables with \(\theta_{i}{\sim}N(\beta x_{i},\tau)\), where \(\beta\) is a parameter, \(\tau>0\) is a known constants, and \(x_{1},\ldots,x_{n}\) are constants. Suppose \(Y_{1},\ldots,Y_{n}|\theta\) are independent with \(Y_{i}|\theta\sim N(\theta_{i},\phi)\), where \(\phi>0\) is known.

1. Pretending \(\beta\) is known, derive a Bayes rule for estimating \(\theta\).
2. For each fixed \(\beta\), derive the marginal distribution of \((Y_{1},\ldots,Y_{n})\). Based on this distribution derive the UMVU estimate for \(\beta\).
3. Construct an empirical Bayes estimate for \(\theta\) based on Parts 1 and 2.
4. Construct a test for the hypothesis \(H_{0}:\theta_{1}\leq 0\) vs \(H_{1}:\theta_{1}>0\).
5. Construct a test for the hypothesis \(H_{0}:\theta_{1}=0\) vs \(H_{1}:\theta_{1}\neq 0\).

**6.17.** Suppose

1. \(\lambda|\phi\sim N(b_{1},\phi/m),\quad b_{1}\in\mathbb{R}\).

2. \(\phi\sim b_{2}\chi_{(k)}^{-2},\;\;b_{2}>0\).

3. \(T|\phi,\lambda\sim N(\lambda,\phi/n)\).

4. \(S|\phi,\lambda\sim\phi\chi_{(n-1)}^{2}\).

5. \(S\,\hbox{$1\!\!\!\!\!\!\perp$}\,T|\lambda,\phi\).

Prove the following statements:

1. \(T\,\hbox{$1\!\!\!\!\!\perp$}\,S|\phi\).

2. \(T|s\sim t_{(n-1+k)}(b_{1},(n^{-1}+m^{-1})s/(n-1))\).

3. \(S\sim b_{2}\,\hbox{Beta}^{\prime}((n-1)/2,k/2)\).

**6.18.** Suppose \(\theta_{1},\ldots,\theta_{n}\) are independent 2-dimensional random vectors. Let \(\theta_{i}=(\lambda_{i},\phi_{i})\). Let

\[\theta=(\theta_{1},\ldots,\theta_{n}),\quad\phi=(\phi_{1},\ldots,\phi_{n}), \quad\lambda=(\lambda_{1},\ldots,\lambda_{n}).\]Suppose

\[\Lambda_{i}|\phi_{i}\sim N(b_{1},\phi_{i}/m),\quad\Phi_{i}\sim b_{2}\chi_{(k)}^{-2},\quad b_{1}\in\mathbb{R},\ b_{2}>0.\]

Suppose \((S_{1},T_{1}),\ldots,(S_{n},T_{n})\) are 2-dimensional random vectors satisfying

1. \((S_{1},T_{1})\,\hbox{$1\!\!\!1\,$}\,\cdots\,\hbox{$1\!\!\!1\,$}\,(S_{n},T_{n})|\theta\).

2. \((S_{i},T_{i})\,\hbox{$1\!\!\!1\,$}\,\theta_{(-i)}|\theta_{i}\).

3. \(S_{i}|\theta_{i}\sim\phi_{i}\chi_{(n_{i}-1)}^{2}\).

4. \(T_{i}|\theta_{i}\sim N(\lambda_{i},\phi_{i}/n_{i})\).

Solve the following problems.

1. Assuming \(b_{1},b_{2}\) are known, derive a Bayes rule for estimating \(\theta\).

2. Use the result of Problem 6.17 to derive the joint distribution of

\[(S_{1},T_{1}),\ldots,(S_{1},T_{1})\]

(this is a distribution not conditioned on \(\theta\)).

3. Use part 2 to derive an unbiased estimate of \(b_{1},b_{2}\).

4. Derive the empirical Bayes estimate of \(\theta\).

## References

* Berger (1985) Berger, J. O. (1985). _Statistical decision theory and Bayesian analysis_. Second edition. Springer, New York.
* an empirical Bayes approach. _Journal of the American Statistical Association_, **68**, 117-130.
* Fisher (1935) Fisher, R. A. (1935). Theory of Statistical Estimation. _Mathematical Proceedings of the Cambridge Philosophical Society_, **22**, 700-725.
* Hettmansperger and Randles (2002) Hettmansperger, T. P. and Randles, R. H. (2002). A practical affine equivariant multivariate median. _Biometrika_, **89**, 851-860.
* James and Stein (1961) James, W. and Stein, C. (1961). Estimation with quadratic loss. _Proceeding of the Fourth Berkeley Symposium on Mathematical Statistics and Probability_, 361-379.
* Li (1992) Li, K.-C. (1992). On Principal Hessian Directions for Data Visualization and Dimension Reduction: AnotherApplication of Stein's Lemma. _Journal of the American Statistical Association_, **87**, 1025-1039.
* Morris (1983) Morris, C. N. (1983). Parametric empirical Bayes inference: theory and applications. _Journal of the American Statistical Association_, **78**, 47-55.
* Oja (1983) Oja, H. (1983). Descriptive statistics for multivariate distributions. _Statistics and Probability Letters_, **1**, 327-332.
* Robbins (1955) Robbins, H. (1955). An empirical Bayes approach to Statistics. _Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volumn 1: Contributions to the Theory of Statistics_, 157-163.

* [SteinSteinStein1956] Stein, C. (1956). Inadmissibility of the usual estimator for the mean of a multivariate Normal distribution. _Berkeley Symposium on Mathematical Statistics and Probability,_ 197-206.
* [SteinStein1981] Stein, C. (1981). Estimation of the mean of a multivariate Normal distribution. _The Annals of Statistics_, **9**, 1135-1151.

## Asymptotic tools and projections

In this chapter we review some crucial results about limit theorems in probability theory, which will be used repeatedly in the rest of the book. The limit theorems include those about convergence in probability, almost everywhere convergence, and convergence in distribution. For further information about the content of this chapter, see Serfling (1980) and Billingsley (1995).

We shall also review some basic results on projections in Hilbert spaces.

### 7.1 Laws of Large Numbers

Let \(\|\cdot\|\) denote the Euclidean norm.

**Definition 7.1**: _The sequence \(X_{n}\) of random vectors is said to converge in probability to a random vector \(X\) if for any \(\epsilon>0\)_

\[\lim_{n\to\infty}P(\|X_{n}-X\|>\epsilon)=0.\]

_This convergence is expressed as \(X_{n}\stackrel{{ P}}{{\to}}X\)._

Thus convergence in probability means that, as \(n\to\infty\), the distributions of the random vectors \(X_{n}-X\) are increasingly concentrated at the origin: the probability of \(\|X_{n}-X\|\) escaping outside of any interval \((-\epsilon,\epsilon)\) goes to \(0\).

In most cases, the limit random vector \(X\) is a constant vector. Convergence in probability in these cases is often proved by the Weak Law of Large Numbers (WLLN). A proof of this depends on few useful probability inequalities, which are presented below.

**Lemma 7.1** (Markov's inequality): _If \(U\geq 0\) is a random variable with \(E(U)<\infty\), then for any \(\epsilon>0\),_

\[P(U>\epsilon)\leq\epsilon^{-1}E(U). \tag{7.1}\]Proof: For any \(\epsilon>0\),

\[P(U>\epsilon)=\int_{U>\epsilon}dP\leq\int_{U>\epsilon}(U/\epsilon)\,dP\leq \epsilon^{-1}E(U),\]

as desired. \(\Box\)

Chebyshev's inequality given in the next lemma is an easy consequence of lemma 7.1.

**Lemma 7.2** (Chebyshev's inequality): _Suppose \(X\) is a random variable with finite variance. Then \(\ P(|X-E(X)|>\epsilon)\leq\epsilon^{-2}\ \mathrm{var}(X).\)\(\Box\)_

From this inequality we can easily derive the Chebyshev's Weak Law of Large Numbers for random variables. A similar result for random vectors follow easily by applying the result on random variables coordinate-wise.

**Theorem 7.1** (Weak Law of Large Numbers): _Let \(\{X_{1},X_{2},\ldots\}\) be a sequence of uncorrelated random variables with finite second moments; that is,_

\[E(X_{i}^{2})<\infty,\quad\mathrm{cov}(X_{i},X_{j})=0,\quad i\neq j.\]

_If \(\lim_{n\to\infty}n^{-2}\sum_{i=1}^{n}\mathrm{var}(X_{i})=0,\) then_

\[\frac{1}{n}\sum_{i=1}^{n}(X_{i}-E(X_{i}))\stackrel{{ P}}{{\to}}0.\]

Proof: Let \(Y_{i}=X_{i}-E(X_{i})\). Then

\[\left(\frac{1}{n}\sum_{i=1}^{n}Y_{i}\right)^{2}=\frac{1}{n^{2}}\sum_{i=1}^{n}Y _{i}^{2}+\frac{2}{n^{2}}\sum_{i<j}^{n}Y_{i}Y_{j}.\]

Since \(X_{i}\) and \(X_{j}\) are uncorrelated, \(Y_{i}\) and \(Y_{j}\) are also uncorrelated. Thus

\[\mathrm{var}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n^{2}}\sum_{i =1}^{n}\mathrm{var}(X_{i})\to 0.\]

The theorem now follows from Lemma 7.2. \(\Box\)

A stronger mode of convergence of sequence of random variables is the almost everywhere convergence.

**Definition 7.2**: _A sequence of random variables \(X_{n}\) converges almost everywhere to a random variable \(X\) if \(\ P(\lim_{n\to\infty}|X_{n}-X|=0)=1\)._

If this definition is satisfied then we write \(X_{n}\to X\ \left[P\right]\).

Before focusing on almost everywhere convergence results for sample means, let us first review limits of sets and the Borel-Cantelli Lemma.

**Definition 7.3**: _For a sequence \(A_{n}\) of measurable sets,_

\[\limsup_{n\to\infty}A_{n} =\{\omega:\omega\in A_{k},\mbox{ for infinitely many }k\},\] \[\liminf_{n\to\infty}A_{n} =\{\omega:\omega\in A_{k},\mbox{ for all but finitely many }k\}.\]

It is easy to establish that

\[\liminf_{n\to\infty}A_{n}=\bigcup_{n=1}^{\infty}\bigcap_{k=n}^{\infty}A_{k}\ \subset\ \limsup_{n\to\infty}A_{n}=\bigcap_{n=1}^{\infty}\bigcup_{k=n}^{\infty}A_{k}.\]

**Lemma 7.3** (Borel-Cantelli Lemma): _If \(\sum_{n=1}^{\infty}P(A_{n})<\infty\), then_

\[P\left(\limsup_{n\to\infty}A_{n}\right)=0. \tag{7.2}\]

_Proof._ Note that the probability on the left hand side of (7.2) is no more than \(P(\cup_{k=n}^{\infty}A_{k})\) for any \(n\), which is bounded from above by the sum \(\sum_{k=n}^{\infty}P(A_{k})\). This sum converges to \(0\) provided \(\sum_{n=1}^{\infty}P(A_{n})<\infty\). \(\Box\)

Results concerning almost everywhere convergence of sums of random elements are called strong laws of large numbers (SLLN). A version of SLLN can be proved along the lines of the proof for the weak law, but it is not the most general one. Because the method is simple and exemplifies the use of Markov inequality combined with Borel-Cantelli Lemma, we choose to prove this version. We will state a more general version without proof later.

**Theorem 7.2** (Strong Law of Large Numbers): _If \(X_{1},X_{2},\ldots\) is a sequence of independent and identically distributed (i.i.d.) random variables and \(X_{i}\) has finite fourth moment, then \(n^{-1}\sum_{i=1}^{n}X_{i}\to E(X_{1})\) almost everywhere._

Henceforth, we will use \(E_{n}X\) to denote the sample average \(n^{-1}\sum_{i=1}^{n}X_{i}\). This notation is motivated by the fact that the sample average is in fact the expectation of \(X\) with respect to the empirical distribution that assign probability mass \(1/n\) at each observation \(X_{i}\).

_Proof._ If necessary by subtracting its expected value from \(X_{i}\), we assume, without loss of generality that \(E(X_{i})=0\). Note that a sequence of numbers, say \(x_{n}\), converges to \(0\) if there is a nonnegative sequence of numbers, say \(\alpha_{n}\) such that \(\alpha_{n}\to 0\) and, for all large \(n\), \(|x_{n}|\leq\alpha_{n}\). So it suffices to show that there is a nonnegative sequence \(\alpha_{n}\), which converges to \(0\), such that

\[P(|E_{n}(X)|>\alpha_{n}\mbox{ for infinitely many }n)=0. \tag{7.3}\]

By Markov's inequality, we have

\[P(|E_{n}(X)|>\alpha_{n})\leq P(|(E_{n}(X))^{4}>\alpha_{n}^{4})\leq\alpha_{n}^ {-4}E(E_{n}(X))^{4}. \tag{7.4}\]To get an upper bound for \(E(E_{n}(X))^{4}\), let \(S_{n}=\sum_{i=1}^{n}X_{i}\). Since \(X_{i}\) are i.i.d. with mean 0, we have \(E(X_{n}S_{n-1}^{3})=0=E(X_{n}^{3}S_{n-1})\),

\[E(X_{n}^{2}S_{n-1}^{2})=E(X_{n}^{2})E(S_{n-1}^{2}),\mbox{ and for any }m,E(S_{m}^{2})=mE(X_{1}^{2}).\]

Hence for some constant \(K>0\),

\[E(S_{n}^{4}) =E(S_{n}S_{n}^{3})=\sum_{i=1}^{n}E(X_{i}S_{n}^{3})=nE(X_{n}S_{n}^{3})\] \[=nE(X_{n}(X_{n}+S_{n-1})^{3})\] \[=nE(X_{n}(X_{n}^{3}+3X_{n}S_{n-1}^{2}+3X_{n}^{2}S_{n-1}+S_{n-1}^{3})\] \[=nE(X_{n}^{4})+3nE(X_{n}^{2})E(S_{n-1}^{2})\] \[=nE(X_{1}^{4})+3nE(X_{1}^{2})(n-1)E(X_{1}^{2})\] \[=nE(X_{1}^{4})+3n(n-1)(E(X_{1}^{2}))^{2}<Kn^{2}. \tag{7.5}\]

By taking \(\alpha_{n}=n^{-1/8}\), it follows from equations (7.4) and (7.5) that

\[\sum_{n=1}^{\infty}P(|E_{n}(X)|>\alpha_{n})\leq\sum_{n=1}^{\infty}K\alpha_{n}^{ -4}n^{-2}\leq K\sum_{n=1}^{\infty}n^{-3/2}<\infty.\]

The theorem now follows from Borel-Cantelli lemma and (7.3). \(\Box\)

Theorem 7.2 can be established under a weaker assumption of finite first moment. A proof of the next theorem, called Kolmogorov's SLLN, can be found in Billingsley (1995, page 282).

**Theorem 7.3**_If \(X_{1},X_{2},\ldots\) are i.i.d. random variables with \(E|X_{i}|<\infty\), then \(E_{n}(X)\to E(X)\) almost everywhere._

By considering coordinate-wise convergence, the above theorem can be extended to random vectors

**Theorem 7.4**_If \(X_{1},X_{2},\ldots\) are i.i.d. random vectors with \(E\|X_{i}\|<\infty\), then \(E_{n}(X)\to E(X)\) almost everywhere._

It is easy to see that convergence almost everywhere implies convergence in probability (see Problem 7.3). We record this fact below for future reference.

**Corollary 7.1**_Let \(X_{1},X_{2},\ldots\) be an i.i.d. sequence or random vectors, where \(E\|X_{1}\|<\infty\). Then \(E_{n}X\stackrel{{ P}}{{\to}}E(X_{1})\)._

### Convergence in distribution

Convergence of a sequence random vectors means, roughly, that the distribution of \(X_{n}\) converges to that of \(X\). However, this intuitive statement hassevere limitations. Consider the sequence of random variables \(\{X_{n}\}\) that is distributed as \(N(0,1/n)\). Here, it is intuitively clear that these distributions converge to a probability measure assigning mass \(1\) at \(0\). Hence the limiting distribution should be \(F(x)=I_{[0,\infty)}(x)\). However, if we denote by \(F_{n}\) the cumulative distribution function of \(N(0,1/n)\), then \(F_{n}(0)=1/2\) for all \(n\), and therefore does not tend to \(F(0)\), which equals \(1\). So a precise definition of convergence in distribution or weak convergence should take care of this caveat.

We first define weak convergence of a sequence of probability measures. As before, let \(\mathbb{N}\) denote the set of positive integers \(\{1,2,\ldots\}\).

**Definition 7.4**: _Let \(\{\mu_{n}:n\in\mathbb{N}\}\) and \(\mu\) be probability measures on \((\mathbb{R}^{p},\mathcal{R}^{p})\). We say that \(\mu_{n}\) converges weakly to \(\mu\) and write \(\mu_{n}\Rightarrow\mu\) if, for any bounded and continuous function \(f\) on \(\mathbb{R}^{p}\), \(\int fd\mu_{n}\rightarrow\int fd\mu\)._

For a set \(A\), let \(\partial A=\bar{A}\setminus A^{\circ}\) denote the boundary of \(A\), where \(\bar{A}\) is the closure of \(A\), and \(A^{\circ}\) is the interior of \(A\). As will be seen in the Portman-teau Theorem stated later, \(\mu_{n}\) converges weakly to \(\mu\) if and only if, for any measurable set \(A\) with \(\mu(\partial A)=0\), \(\mu_{n}(A)\rightarrow\mu(A)\). This successfully avoids the caveat we mentioned at the beginning of this section: if \(A=\{0\}\), then \(\partial A=\{0\}\), and \(\mu(\partial A)=1\). It is easy to check in that example that for any set \(A\in\mathcal{R}\) except \(A=\{0\}\) we have \(\mu_{n}(A)\rightarrow\mu(A)\). Thus \(N(0,1/n)\) indeed converges weakly to the point mass at \(0\) even though the c.d.f. of \(N(0,1/n)\) at \(0\) does not converge to \(1\).

A sequence of \(p\)-dimensional random vectors \(\{X_{n}\}\) defined on probability spaces \((\Omega_{n},\mathcal{F}_{n},P_{n})\) converges in distribution to a random vector \(X\) defined on a probability space \((\Omega,\mathcal{F},P)\) if and only if the probability distributions \(P_{n}\circ X_{n}^{-1}\) on \((\mathbb{R}^{p},\mathcal{R}^{p})\) converges weakly to \(P\circ X^{-1}\). In symbols, \(X_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}X\) if and only if \(P_{n}\circ X_{n}^{-1}\Rightarrow P\circ X^{-1}\). If \(Q\) is a probability measure on \((\mathbb{R}^{p},\mathcal{R}^{p})\), and if \(P_{n}\circ X_{n}^{-1}\Rightarrow Q\), then we also use the notation \(X_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}Q\).

A special case of convergence in distribution is \(X_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}X\), where \(X\) is a degenerate random variable; that is, \(X=a\) almost everywhere for some constant \(a\). Then it can be shown that \(P(X=a)=1\) if and only if the distribution \(P\circ X^{-1}\) is \(\delta_{a}\), the Dirac measure at \(a\). Thus, the meaning of \(X_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}X\) in this case is simply \(X_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}\delta_{a}\). Furthermore, it can be shown that \(X_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}\delta_{a}\) if and only if \(X_{n}\stackrel{{ P}}{{\rightarrow}}a\). See Problem 7.4.

The spaces \((\Omega_{n},\mathcal{F}_{n},P_{n})\) and \((\Omega,\mathcal{F},P)\) do not appear directly, but only by way of the distributions they induce on the range space. In view of this, there is no source of confusion if we drop the subscripts from \(P_{n}\) and \(E_{P_{n}}\). So from now on, for example, we simply write \(E(X_{n})\) for expectation instead of \(E_{P_{n}}(X_{n})\). We return to explicit mention of \(P_{n}\) and \(E_{P_{n}}(X_{n})\) in Chapter 10.

In general, convergence in distribution is weaker than convergence in probability. That is, if \(X_{n}\stackrel{{ P}}{{\rightarrow}}X\) for some random vector \(X\), then \(X_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}X\). See Problem 7.5.

Often, results on weak convergence of random variables can be simplified by using the next theorem, known as Skorohod's Theorem. See Billingsley (1995, Theorem 25.6).

Theorem 7.5 (Skorohod Theorem): _Suppose a sequence of probability measures \(\{\nu_{n}\}\) on the real-line converges weakly to a probability measure \(\nu\). Then there exist random variables \(Y_{n}\) and \(Y\) defined on a common probability space \((\Omega,\mathcal{F},P)\) such that \(Y_{n}\) has distribution \(\nu_{n}\), \(Y\) has distribution \(\nu\), and \(Y_{n}(\omega)\to Y(\omega)\) for all \(\omega\in\Omega\)._

In particular, if a sequence of random variables \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\), where \(X_{n}\) has distribution \(F_{n}\) and \(X\) has distribution \(F\), then there exist random variables \(Y_{n}\) and \(Y\) defined on a common space \(\Omega\) such that \(Y_{n}\) has distribution \(F_{n}\), \(Y\) has distribution \(F\), and \(Y_{n}(\omega)\to Y(\omega)\) for all \(\omega\in\Omega\). Combining this with Fatou's Lemma 1.6 and Bounded Convergence Theorem (Theorem 1.8), we get the following result. Proof is left as an exercise.

Lemma 7.4: _Let \(\{X_{n}\}\) be a sequence of random variables such that \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\). Then_

\[E(|X|)\leq\liminf_{n\to\infty}E(|X_{n}|).\]

_If, in addition, \(|X_{n}|\) are uniformly bounded (that is, for some \(M>0\), then by \(|X_{n}|\leq M\) for all \(n\)), then_

\[E(X_{n})\to E(X).\]

Skorohod's Theorem also holds for random vectors.

There are several equivalent conditions to the definition of weak convergence, and depending on context, each may be more useful than the others, either as a tool to work with, or as a goal to work towards. These statements are collectively called the Portmanteau theorem (See Billingsley, 1999, pages 16 and 26). Let \(h\) be a real-valued function. Let \(D_{h}\) be the collection of points at which \(h\) is discontinuous, that is, \(D_{h}=\{x:h\mbox{ is discontinuous at }x\}\). We say that a set \(A\in\mathcal{R}^{p}\) is a \(P^{\circ}X^{-1}\)-continuity set of \(P^{\circ}X^{-1}(\partial A)=0\). We first define the upper and lower semi-continuity.

Definition 7.5: A function \(f\) is said to be upper semi-continuous at \(x\) if, for any sequence \(x_{n}\to x\), \(\limsup_{n\to\infty}f(x_{n})\leq f(x)\).

A function \(f\) is called lower semi-continuous at \(x\) if, for any sequence \(x_{n}\to x\), \(\liminf_{n\to\infty}f(x_{n})\geq f(x)\).

Theorem 7.6 (Portmanteau Theorem): _Let \(\{X_{n}:n\in\mathbb{N}\}\) be a sequence of random vectors, and \(X\) a random vector. The following statements are equivalent:_

1. \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\)_;_
2. \(Eh(X_{n})\to Eh(X)\) _for any bounded and uniformly continuous function_ \(h\)3. For any \(P\circ X^{-1}\)-continuity set \(A\), \(\lim_{n\to\infty}P(X_{n}\in A)=P(X\in A)\);
4. For any closed set \(F\), \(\limsup_{n\to\infty}P(X_{n}\in F)\leq P(X\in F)\);
5. For any open set \(G\), \(\liminf_{n\to\infty}P(X_{n}\in G)\geq P(X\in G)\);
6. for any upper semi-continuous function \(h\) that is bounded from above, we have \(\limsup_{n\to\infty}Eh(X_{n})\leq Eh(X)\);
7. For any lower semi-continuous function \(h\) that is bounded from below, we have \(\liminf_{n\to\infty}Eh(X_{n})\geq Eh(X)\).

Statements 5 and 7 are easy consequences of Lemma 7.4 under condition 1. The statements in the above theorem are in terms of convergence in distribution of \(X_{n}\) to \(X\), but they can be equivalently stated in terms of weak convergence of probability measures \(P_{n}\) to \(P\) defined on \((\mathbb{R}^{p},\mathcal{R}^{p})\). For example, the first five statements can be reformulated as

1. \(P_{n}\Rightarrow P\);
2. \(\int fdP_{n}\to\int fdP\) for every bounded and uniformly continuous function \(f\);
3. \(\lim_{n\to\infty}P_{n}(A)=P(A)\) for any \(A\in\mathcal{R}^{p}\) with \(P(\partial A)=0\);
4. \(\limsup_{n\to\infty}P_{n}(F)\leq P(F)\) for any closed set \(F\) in \(\mathbb{R}^{p}\);
5. \(\liminf_{n\to\infty}P_{n}(G)\geq P(G)\) for any open set \(G\) in \(\mathbb{R}^{p}\).

The rest of the theorem can be similarly translated in terms of probability measures.

The Portmanteau theorem is a fundamental result and is extremely useful. Many important results in asymptotic analysis can be derived from them. Below we derive several of these results, both because of their importance in future discussion and as exercises to practice the use of the Portmanteau theorem.

The first result is the Continuous Mapping Theorem, which says that if \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\) and \(h\) is a continuous function then \(h(X_{n})\stackrel{{\mathcal{D}}}{{\to}}h(X)\). This is easily seen using the Portmanteau theorem. Since \(h\) is continuous, \(h^{-1}(G)\) is open whenever \(G\) is open. Thus

\[\liminf_{n\to\infty}P(h(X_{n})\in G) =\liminf_{n\to\infty}P(X_{n}\in h^{-1}(G))\] \[\geq P(X\in h^{-1}(G))=P(h(X)\in G).\]

Hence \(h(X_{n})\stackrel{{\mathcal{D}}}{{\to}}h(X)\) by the Portmanteau theorem.

We record below without proof a more general version of the Continuous Mapping Theorem. The proof is in the same spirit as the last paragraph.

**Proposition 7.1** (Continuous Mapping Theorem): _Suppose \(h\) is a vector valued function such that \(P(X\in D_{h})=0\)._

1. _If_ \(X_{n}\to X\) _almost everywhere, then_ \(h(X_{n})\to h(X)\) _almost everywhere._
2. _If_ \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\)_, then_ \(h(X_{n})\stackrel{{\mathcal{D}}}{{\to}}h(X)\)_._
3. _If_ \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\)_, then_ \(h(X_{n})\stackrel{{\mathcal{D}}}{{\to}}h(X)\)_._The proof of part 1 of this proposition is relatively straightforward, and is left as an exercise. Part 2 follows from part 1 and \(k\)-dimensional version of Skorohod Theorem (see Theorem 7.5). A complete proof of the above theorem can be found in Serfling (1980).

Sometimes we want to know whether

\[X_{n}\stackrel{{\mathcal{D}}}{{\to}}X,\quad Y_{n}\stackrel{{ \mathcal{D}}}{{\to}}Y\quad\Rightarrow\quad(X_{n},Y_{n})\stackrel{{ \mathcal{D}}}{{\to}}(X,Y).\]

Obviously this cannot be true generally. In fact, we don't even know what \((X,Y)\) means -- even if \(X\) and \(Y\) are well defined individually, it does not specify anything about what happens between them. However, in some special cases \(X\) and \(Y\) does specify \((X,Y)\). For example, if \(Y\) is a constant vector then \((X,Y)\) is well defined. The question now is whether \((X_{n},Y_{n})\stackrel{{\mathcal{D}}}{{\to}}(X,Y)\) in this case. We are interested in this because many statistics we use, such as the studentized statistics, involve two sequences with one converging to a random vector and another converging to a constant vector. Using the Portmanteau Theorem we can answer this question reasonably easily.

**Theorem 7.7**: _Let \(\{X_{n}\}\) be a sequence of random vectors in \(\mathbb{R}^{p}\) and let \(\{Y_{n}\}\) be a sequence of random vectors in \(\mathbb{R}^{q}\)._

_1. If \(p=q\), \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\) and \(\|X_{n}-Y_{n}\|\stackrel{{ P}}{{\to}}0\), then \(Y_{n}\stackrel{{\mathcal{D}}}{{\to}}X\)._

_2. If \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\) and \(Y_{n}\stackrel{{ P}}{{\to}}c\) for some constant \(c\in\mathbb{R}^{q}\), then \((X_{n},Y_{n})\stackrel{{\mathcal{D}}}{{\to}}(X,c)\)._

Proof.: _1._ We will use the third assertion of the Portmanteau theorem. Let \(F\) be a closed set, we will show that \(\limsup_{n\to\infty}P(Y_{n}\in F)\leq P(X\in F)\). Let \(\epsilon>0\). Then

\[P(Y_{n}\in F)=P(Y_{n}\in F,\|X_{n}-Y_{n}\|<\epsilon)+P(Y_{n}\in F,\|X_{n}-Y_{n }\|\geq\epsilon).\]

The second probability on the right hand side goes to \(0\) as \(n\to\infty\). The event inside the first probability on the right hand side implies \(X_{n}\in A(\epsilon)\), where \(A(\epsilon)=\{x:\sup_{y\in F}\|x-y\|<\epsilon\}\). Hence the first term on the right is no more than \(P(X_{n}\in A(\epsilon))\). Because \(F\) is closed, \(\cap_{k=1}^{\infty}A(1/k)=F\). By continuity of probability, for any \(\delta>0\) we can select a \(k\) so large that \(P(X_{n}\in A(1/k))\) is no more than \(P(X_{n}\in F)+\delta\). Take \(\epsilon<1/k\). Then

\[P(Y_{n}\in F)\leq P(X_{n}\in F)+\delta+P((\|X_{n}-Y_{n}\|\geq\epsilon).\]

Consequently \(\limsup_{n\to\infty}P(Y_{n}\in F)\leq P(X\in F)+\delta\) for any \(\delta\), which implies \(\limsup_{n\to\infty}P(Y_{n}\in F)\leq P(X\in F)\).

_2._ We will use the first assertion of the Portmanteau theorem. Note that \(\|(X_{n},Y_{n})-(X_{n},c)\|=\|Y_{n}-c\|\stackrel{{ P}}{{\to}}0\). By the first part of this theorem it suffices to show that \((X_{n},c)\stackrel{{\mathcal{D}}}{{\to}}(X,c)\). Let \(h\) be a bounded and real-valued continuous function. Then \(g\) defined by \(g(x)=h(x,c)\) is a bounded continuous function. Because \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\), we have \(Eg(X_{n})\to Eg(X)\), and hence \(Eh(X_{n},c)\to Eh(X,c)\). \(\Box\)

The next result is the well known Slutsky's theorem, which combines the second assertion of the above theorem with the continuous mapping theorem.

**Corollary 7.2** (Slutsky's Theorem): _Suppose that \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\), \(Y_{n}\stackrel{{\mathcal{D}}}{{\to}}c\), and \(h\) is a vector-valued function with \(P((X,c)\in D_{h})=0\). Then \(h(X_{n},Y_{n})\stackrel{{\mathcal{D}}}{{\to}}h(X,c)\)._

In some text books Slutsky's theorem refers to the special case of the above theorem when \(h\) is a rational function and \(X_{n}\) and \(Y_{n}\) are random variables.

**Corollary 7.3**: _Suppose that \(\{(X_{n},Y_{n})\}\) is a sequence of bivariate random vectors such that \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\), and \(Y_{n}\stackrel{{\mathcal{P}}}{{\to}}c\). Then_

1. \(X_{n}+Y_{n}\stackrel{{\mathcal{D}}}{{\to}}X+c\)_._
2. \(X_{n}Y_{n}\stackrel{{\mathcal{D}}}{{\to}}cX\)_._
3. _If_ \(c\neq 0\)_, then_ \(X_{n}/Y_{n}\stackrel{{\mathcal{D}}}{{\to}}X/c\)_._

Besides the equivalent statements of convergence in distribution given in the Portmanteau theorem, there is another such statement using characteristic functions. A characteristic function of a random vector \(X\) is its Fourier transform with respect to the measure \(P\); that is \(\phi_{X}(t)=E(e^{it^{T}X})\), where \(i=\sqrt{-1}\) and \(t\) is any vector in \(\mathbb{R}^{p}\).

**Proposition 7.2**: _A sequence of random vectors \(X_{n}\) converges in distribution to \(X\) if and only if \(\phi_{X_{n}}(t)\to\phi_{X}(t)\) for all \(t\in\mathbb{R}^{p}\)._

Characteristic functions can be used to prove the Central Limit Theorems. They can also be used to extend convergence in distribution for a sequence of random variables to a sequence of random vectors. This is called the Cramer-Wold device, introduced by Cramer and Wold (1936), see also Billingsley (1995, page 383). For example, in the next section, we will use this device to extend the Lindeberg Theorem to sequences of random vectors.

**Theorem 7.8** (Cramer-Wold Device): _Let \(X\), \(\{X_{n}\,:\,n=1,2,\ldots\}\) be random random vectors of dimension \(p\). Suppose that, for any \(a\in\mathbb{R}^{p}\), \(a^{T}X_{n}\stackrel{{\mathcal{D}}}{{\to}}a^{T}X\). Then \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\)._

Proof: Because \(a^{T}X_{n}\stackrel{{\mathcal{D}}}{{\to}}a^{T}X\), the characteristic function of \(a^{T}X_{n}\) converges to that of \(a^{T}X\). That is, for any \(t\in\mathbb{R}\),

\[\phi_{a^{T}X_{n}}(t)=Ee^{it(a^{T}X_{n})}\to Ee^{it(a^{T}X)}=\phi_{a^{T}X}(t).\]

Take \(t=1\). Then \(Ee^{ia^{T}X_{n}}\to Ee^{ia^{T}X}\). In other words, \(\phi_{X_{n}}(a)\to\phi_{X}(a)\) for all \(a\in\mathbb{R}^{p}\). Hence, by Proposition 7.2, \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\). \(\Box\)

### Argument via subsequences

In this section we introduce two theorems that are useful for proving weak convergence. Recall that, in calculus, a sequence of numbers \(\{a_{n}\}\) converges to a number \(a\) if and only if every subsequence of \(\{a_{n}\}\) contains a further subsequence that converges to \(a\). This equivalence also applies to convergence in distribution.

**Theorem 7.9**: _A sequence of random vectors \(X_{n}\) converges in distribution to \(X\) if and only if every subsequence \(X_{n^{\prime}}\) contains a further subsequence \(X_{n^{\prime\prime}}\) that converges in distribution to \(X\)._

A weaker condition than "every subsequence of \(\{X_{n}\}\) contains a further subsequence that converges to \(X\)" is "every subsequence of \(\{X_{n}\}\) contains a further subsequence that converges to _some_ random vector". In the latter statement, the random vector that the further subsequence converges to may depends on the subsequence and further subsequence involved. This property is known as relative compactness. Prohorov's theorem below gives a sufficient condition for relative compactness: tightness.

Tightness is an extension of the boundedness in probability to metric spaces. Since in this book we focus on random vectors in Euclidean spaces, we do not need the more general meaning of tightness. Nevertheless, following the standard usage in this area we will use the term tightness rather than boundedness in probability. We first give the formal definition of tightness.

**Definition 7.6**: _A family \(\Pi\) of probability measures on \((\mathbb{R}^{p},\mathcal{R}^{p})\) is tight, if for every \(\epsilon>0\), there exists a compact set \(K_{\epsilon}\subseteq\mathbb{R}^{p}\) such that \(P(K_{\epsilon})>1-\epsilon\) for all \(P\in\Pi\)._

Since a compact set is a bounded and closed set in a Euclidean space, tightness is equivalent to boundedness in probability in the case of Euclidean spaces. For a sequence \(\{U_{n}\}\) of random vectors, tightness translates to the property that for every \(\epsilon>0\), there exists a real number \(M_{\epsilon}\), sucht that \(P(\|U_{n}\|>M_{\epsilon})<\epsilon\) for all \(n\). The next lemma asserts that marginal tightness implies joint tightness, and marginal stochastic smallness implies joint stochastic smallness.

**Lemma 7.5**: _If \(\{U_{n}\}\) and \(\{V_{n}\}\) are tight, then \(\{(U_{n}^{T},V_{n}^{T})^{T}\}\) is tight. Moreover, if \(U_{n}\stackrel{{ P}}{{\rightarrow}}0\) and \(V_{n}\stackrel{{ P}}{{\rightarrow}}0\), then \((U_{n}^{T},V_{n}^{T})^{T}\stackrel{{ P}}{{\rightarrow}}0\)._

Proof: We note that

\[\|(U_{n}^{T},V_{n}^{T})^{T}\|^{2}=\|U_{n}\|^{2}+\|V_{n}\|^{2}.\]

Thus, if \(\|U_{n}\|\leq K\) and \(\|V_{n}\|\leq K\) then \(\|(U_{n}^{T},V_{n}^{T})^{T}\|\leq\sqrt{2}K\). Let \(\epsilon>0\) be any fixed constant. Let \(K\) be sufficiently large so that \(P(\|U_{n}\|>K)<\epsilon/2\) and \(P(\|V_{n}\|>K)<\epsilon/2\). Then\[P(\|(U_{n}^{T},V_{n}^{T})^{T}\|>\sqrt{2}K)\leq P(\|U_{n}\|>K)+P(\|V_{n}\|>K)<\epsilon.\]

Thus \((U_{n}^{T},V_{n}^{T})\) is tight. The second statement can be proved similarly. 

Argument via subsequenes is implied by tightness. This is the Prohorov's Theorem. See, for example, Billingsley (1999, page 57).

**Theorem 7.10** (Prohorov's theorem): _If a sequence of random vectors \(\{U_{n}\}\) is tight, then every subsequence \(U_{n^{\prime}}\), contains a further subsequence \(\{U_{n^{\prime\prime}}\}\) such that \(U_{n^{\prime\prime}}\) converges in distribution to a random vector._

Theorems 7.9 and 7.10 are often used together to show that a tight sequence of random vectors converges in distribution. If we are given a tight sequence, say \(\{U_{n}\}\), then, by Theorem 7.10, every subsequence \(\{U_{n^{\prime}}\}\) contains a further subsequence \(\{U_{n^{\prime\prime}}\}\) that converges in distribution to some random vector \(U\). If we can further show that this \(U\) does not depend on the subsequence \(\{n^{\prime}\}\) and the further subsequence \(\{n^{\prime\prime}\}\), then, by Theorem 7.9, the entire sequence \(\{U_{n}\}\) converges in distribution to \(U\). For easy reference, we refer to this method as the _argument via subsequences_.

Sometimes we have two sequences of random vectors that are tight under different distributions. Specifically, suppose, for each \(n\), \(U_{n}\) is a random vector distributed as \(P_{n}\), and \(V_{n}\) is a random vector distributed as \(Q_{n}\). If \(\{U_{n}\}\) is tight with respect to \(\{P_{n}\}\) and \(\{V_{n}\}\) is tight with respect to \(\{Q_{n}\}\) then by Theorem 7.10, for any subsequence \(n^{\prime}\), there is a subsequence \(n^{\prime\prime}\) such that \(U_{n^{\prime\prime}}\) converges in distribution under \(P_{n^{\prime\prime}}\), and there is another subsequence \(n^{\prime\prime\prime}\) such that \(V_{n^{\prime\prime\prime}}\) converge in distribution under \(Q_{n^{\prime\prime\prime}}\). The question is, can \(n^{{}^{\prime\prime}}\) and \(n^{\prime\prime\prime}\) be taken as the same subsequence? The next lemma answers this question.

**Lemma 7.6**: _If \(\{U_{n}\}\) is tight under \(\{P_{n}\}\) and \(\{V_{n}\}\) is tight under \(\{Q_{n}\}\), then, for any subsequence \(\{n^{\prime}\}\), there exist a further subsequence \(\{n^{\prime\prime}\}\), and random vectors \(U\) and \(V\), such that_

\[U_{n^{\prime\prime}}\stackrel{{\mathcal{D}}}{{\rightarrow}}U,\quad V _{n^{\prime\prime}}\stackrel{{\mathcal{D}}}{{\rightarrow}}V. \tag{7.6}\]

Proof: Let \(n^{\prime}\) be any subsequence. Because \(\{U_{n}\}\) is tight under \(\{P_{n}\}\), there is a subsequence \(m^{\prime}\) of \(n^{\prime}\) and a random vector \(U\) such that

\[U_{m^{\prime}}\stackrel{{\mathcal{D}}}{{\rightarrow}}U.\]

Because \(V_{n}\) is tight under \(Q_{n}\) and \(\{m^{\prime}\}\) is a subsequence of \(\{n\}\), there is a further subsequence \(\{n^{\prime\prime}\}\) of \(\{m^{\prime}\}\) and a random vector \(V\) such that

\[V_{n^{\prime\prime}}\stackrel{{\mathcal{D}}}{{\rightarrow}}V.\]

Because \(U_{m^{\prime}}\) converges in distribution to \(U\) under \(P_{m^{\prime}}\), it also converges in distribution to \(U\) along the subsequence \(\{n^{\prime\prime}\}\). Thus we have (7.6).

### Argument via simple functions

Let \(\mu_{1}\) and \(\mu_{2}\) be two measures defined on a measurable space \((\Omega,\mathcal{F})\). Let \(g_{1}\geq 0\) and \(g_{2}\geq 0\) be measurable functions on the same space. In this section we develop a general method to show that

\[\int fg_{1}\,d\mu_{1}=\int fg_{2}\,d\mu_{2} \tag{7.7}\]

holds for an arbitrary \(f\).

For this we need the following fact: for any nonnegative and \(\mathcal{F}\)-measurable function \(f\), there is a sequence of nonnegative simple functions \(\{f_{n}\}\) such that \(0\leq f_{n}\uparrow f\). Such a sequence can be constructed as follows. For each \(n=1,2,\ldots\), we divide \([0,\infty)\) into \(n2^{n}+1\) left closed and right open intervals. The first \(n2^{n}\) intervals are of length \(2^{-n}\), equally spaced between \(0\) and \(n\); the last interval is \([n,\infty)\). If \(f(\omega)\) falls in one of these intervals, say \([a,b)\), then the value \(f_{n}(\omega)\) is defined as \(b\). Specifically,

\[f_{n}(\omega)=\begin{cases}0&\text{if }0\leq f(\omega)<2^{-n}\\ 2^{-n}&\text{if }2^{-n}\leq f(\omega)<2\cdot 2^{-n}\\ \vdots\\ (k-1)2^{-n}&\text{if }(k-1)2^{-n}\leq f(\omega)<k2^{-n}\\ \vdots\\ n-2^{-n}&\text{if }n-2^{-n}\leq f(\omega)<n\\ n&\text{if }n\leq f(\omega)<\infty\end{cases}\]

For convenience, the collection \([0,2^{-n}),\ldots,[n-2^{-n},n),[n,\infty)\) is referred to as the \(n\)th generation intervals. For each \(n\), the collection of \((n+1)\)th generation intervals is a refinement of the collection of \(n\)th generation intervals; that is, each \((n+1)\)th generation interval is contained in an \(n\)th generation interval. Consequently, if \(f(\omega)\) is contained in an \((n+1)\)th generation interval \([a,b)\), then \([a,b)\subseteq[c,d)\) for some \(n\)th generation interval \([c,d)\). Thus \(f_{n+1}(\omega)=a\geq c=f_{n}(\omega)\). Also, by construction, for \(f(\omega)<n\),

\[|f_{n}(\omega)-f(\omega)|\leq 2^{-n},\]

and for any \(\omega\in\Omega\), \(f(\omega)<n\) for large enough \(n\). From this we see that \(\lim_{n\to\infty}f_{n}(\omega)=f(\omega)\) for all \(\omega\in\Omega\). Thus \(\{f_{n}\}\) is a sequence of simple functions satisfying \(0\leq f_{n}\uparrow f\). These preliminaries help in establishing the next theorem.

**Theorem 7.11**: _If (7.7) holds for all \(\mathcal{F}\)-measurable indicator functions, then_

1. _it holds for all nonnegative measurable functions_ \(f\)_;_
2. _it holds for all measurable_ \(f\) _such that the integrals on both sides of (_7.7_) are finite._Proof.: _1._ Because equation (7.7) holds for all measurable indicator functions, it holds for all simple functions, and in particular all nonnegative simple functions. Now let \(f\) be a nonnegative measurable function and \(f_{n}\) a sequence of nonnegative simple functions such that \(f_{n}\uparrow f\). Then \(0\leq f_{n}g_{1}\uparrow fg_{1}\) and \(0\leq f_{n}g_{2}\uparrow fg_{2}\). By the Monotone Convergence Theorem (see Theorem 1.5), as \(n\to\infty\),

\[\int f_{n}g_{1}\,d\mu_{1}\to\int fg_{1}\,d\mu_{1},\quad\int f_{n}g_{2}\,d\mu_{2 }\to\int fg_{2}\,d\mu_{1}.\]

Because \(\int f_{n}g_{1}\,d\mu_{1}=\int f_{n}g_{2}\,d\mu_{2}\) holds for all \(n\), equality (7.7) holds for \(f\).

_2._ Let \(f\) be a measurable function such that \(\int f^{\pm}g_{1}\,d\mu_{1}\) and \(\int f^{\pm}g_{2}\,d\mu_{2}\) are finite. Then

\[\int fg_{1}\,d\mu_{1} = \int f^{+}g_{1}\,d\mu_{1}-\int f^{-}g_{1}\,d\mu_{1}\] \[\int fg_{2}\,d\mu_{2} = \int f^{+}g_{2}\,d\mu_{2}-\int f^{-}g_{2}\,d\mu_{2}.\]

By part 1,

\[\int f^{+}g_{1}\,d\mu_{1}=\int f^{+}g_{2}\,d\mu_{2},\quad\int f^{-}g_{1}\,d\mu_ {1}=\int f^{-}g_{2}\,d\mu_{2}.\]

Hence (7.7) holds for \(f\). 

### The Central Limit Theorems

Convergence in distribution is usually established using the Central Limit Theorems. In the independent case, the most general version is the Lindeberg Theorem. This is concerned with a triangular array of random vectors:

\[\{X_{nk}:\ k=1,\ldots,k_{n},n=1,2,\ldots\} \tag{7.8}\]

Typically, \(k_{n}=n\), in which case this array does look like a triangle. We first consider the scalar case; that is, \(X_{nk}\) are random variables.

**Theorem 7.12** (Lindeberg Theorem): _Suppose_

1. \(X_{n1},\ldots,X_{nk_{n}}\) _are independent for each_ \(n\)_;_ \(S_{n}=X_{n1}+\cdots+X_{nk_{n}}\)_;_
2. \(E(X_{nk}^{2})<\infty\) _for each_ \(n,k\)_;_ \(\operatorname{var}(S_{n})>0\)_;_ \(U_{nk}=[X_{nk}-E(X_{nk})]/\sqrt{\operatorname{var}(S_{n})}\)_;_
3. _and for any_ \(\epsilon>0\)_,_ \[L_{n}(\epsilon)=\sum_{k=1}^{k_{n}}\int_{|U_{nk}|>\epsilon}U_{nk}^{2}dP\to 0.\] (7.9)_Then \((S_{n}-ES_{n})/\sqrt{\operatorname{var}(S_{n})}\stackrel{{\mathcal{D}}} {{\rightarrow}}N(0,1)\)._

We will omit the proof of this theorem. Interested readers can find a proof in Billingsley (1995, page 359). The sequence \(L_{n}(\epsilon)\) in (7.9) is called the Lindeberg sequence, and the condition \(L_{n}(\epsilon)\to 0\) is called the Lindeberg condition. The meaning of this condition is best seen through its special cases. The simplest special case is concerned with a sequence of independent and identically distributed random variables.

**Corollary 7.4** (The Lindeberg-Levy Theorem): _Suppose that \(X_{1},X_{2},\ldots\) is an i.i.d. sequence of random variables with a finite nonzero variance \(\sigma^{2}\). Then_

\[\sqrt{n}(E_{n}(X)-E(X_{1}))/\sigma\stackrel{{\mathcal{D}}}{{ \rightarrow}}N(0,1).\]

Proof: Let \(\mu=E(X_{1})\). Consider the triangular array \(\{X_{nk}:\,k=1,\ldots,n,\,n=1,2,\ldots\}\) defined by \(X_{nk}=(X_{k}-\mu)/\sigma\). For this triangular array,

\[s_{n}^{2}=\operatorname{var}(X_{n1})+\cdots+\operatorname{var}(X_{nn})=1+ \cdots+1=n.\]

Since \(X_{nk}\) are identically distributed as \(Z=(X_{1}-\mu)/\sigma\), the Lindeberg number \(L_{n}(\epsilon)\) is

\[\frac{1}{n}\sum_{k=1}^{n}\int_{|X_{nk}|>\epsilon\sqrt{n}}X_{nk}^{2}\,dP=\frac{ 1}{n}\sum_{k=1}^{n}\int_{|Z|>\epsilon\sqrt{n}}Z^{2}\,dP=\int_{|Z|>\epsilon\sqrt {n}}Z^{2}\,dP.\]

Because \(E(Z^{2})<\infty\), the right hand side of the above expression tends to \(0\) as \(n\rightarrow\infty\). \(\Box\)

The next special case applies to triangular arrays where each \(|X_{nk}|\) has slightly higher than second moment.

**Corollary 7.5** (The Lyapounov Theorem): _Let \(X_{nk}\) be a triangular array, where the random variables in each row are independent. Suppose that, \(E|X_{nk}|^{2+\delta}<\infty\) for some \(\delta>0\) and suppose, without loss of generality, \(E(X_{nk})=0\). If_

\[M_{n}(\epsilon)=\sum_{k=1}^{k_{n}}\frac{1}{s_{n}^{2+\delta}}E\left(|X_{nk}|^{2 +\delta}\right)\to 0,\mbox{ then }S_{n}/s_{n}\stackrel{{ \mathcal{D}}}{{\rightarrow}}N(0,1).\]

Proof: It suffices to show that \(L_{n}(\epsilon)\leq cM_{n}(\epsilon)\) for some \(c>0\) that does not depend on \(n\). We have \[L_{n}(\epsilon) =a_{n}\sum_{k=1}^{k_{n}}\int_{|X_{nk}|>\epsilon s_{n}}|X_{nk}/s_{n}|^ {2}dP\] \[\leq\sum_{k=1}^{k_{n}}\int_{|X_{nk}|>\epsilon s_{n}}\left(|X_{nk}/s _{n}|\,/\epsilon\right)^{\delta}|X_{nk}/s_{n}|^{2}dP\] \[\leq\sum_{k=1}^{k_{n}}\int\left(|X_{nk}/s_{n}|\,/\epsilon\right)^ {\delta}|X_{nk}/s_{n}|^{2}dP=M_{n}(\epsilon)/\epsilon^{\delta},\]

as desired. \(\Box\)

Now consider the triangular arrays in which \(X_{nk}\) are \(p\)-dimensional vectors. We will use the Cramer-Wold device described in Theorem 7.8, which allows us to pass from the a central limit theorem for scalar random variables to random vectors. Let \(\text{var}(X_{nk})=\Sigma_{nk}\) and without loss of generality assume \(E(X_{nk})=0\). Let \(S_{n}=X_{n1}+\cdots+X_{nk_{n}}\) and \(\Sigma_{n}=\Sigma_{n1}+\cdots+\Sigma_{nk_{n}}\). Define

\[L_{n}^{(p)}(\epsilon)=\sum_{k=1}^{k_{n}}\int_{(X_{nk}^{T}\Sigma_{n}^{-1}X_{nk} )^{1/2}>\epsilon}\left(X_{nk}^{T}\Sigma_{n}^{-1}X_{nk}\right)\,dP.\]

Here, the superscript \((p)\) of \(L_{n}^{(p)}(\epsilon)\) indicates the dimension of \(X_{nk}\). Note that when \(p=1\), this reduces to the usual Lindeberg sequence.

**Theorem 7.13**: _Suppose that \(\{X_{nk}\}\) is a triangular array of \(p\)-dimensional random vectors with \(E(X_{nk})=0\) and positive definite variance matrices \(\Sigma_{nk}\). Suppose that the random vectors in each row are independent. If \(L_{n}^{(p)}(\epsilon)\to 0\), then \(\Sigma_{n}^{-1/2}S_{n}\overset{\mathcal{D}}{\to}N(0,I_{p})\), where \(I_{p}\) is the \(p\times p\) identity matrix._

_Proof._ Applying the Cramer-Wold device, it suffices to show that for any \(t\in\mathbb{R}^{p}\), \(t\neq 0\),

\[t^{T}\Sigma_{n}^{-1/2}S_{n}\overset{\mathcal{D}}{\to}N(0,\|t\|^{2}).\]

To do so we need to verify that the Lindeberg sequence \(L_{n}(\epsilon)\) for the triangular array \(\{t^{T}X_{nk}\}\) converges to \(0\), where

\[L_{n}(\epsilon)=\sum_{k=1}^{k_{n}}\int_{|t^{T}\Sigma_{n}^{-1/2}X_{nk}|> \epsilon}\left(t^{T}\Sigma_{n}^{-1/2}X_{nk}\right)^{2}\,dP. \tag{7.10}\]

Applying the Cauchy-Schwarz inequality (see Lemma 2.3), we obtain

\[\left(t^{T}\Sigma_{n}^{-1/2}X_{nk}\right)^{2}\leq\|t\|^{2}\left(X_{nk}^{T} \Sigma_{n}^{-1}X_{nk}\right). \tag{7.11}\]

Consequently, we can replace the inequality that specifies the integral in (7.10) by \((X_{nk}^{T}\Sigma_{n}^{-1}X_{nk})^{1/2}>\epsilon/\|t\|\) and replace the integrand of (7.10) by the quantity on the right hand side of (7.11) without making the integral smaller. In other words,\[L_{n}(\epsilon)\leq\|t\|^{2}\sum_{k=1}^{k_{n}}\int_{(X_{nk}^{T}\Sigma_{n}^{-1}X_{nk )^{1/2}}>\epsilon/\|t\|}\left(X_{nk}^{T}\Sigma_{n}^{-1}X_{nk}\right)\,dP.\]

However, right hand side is just \(\|t\|^{2}L_{n}^{(p)}(\epsilon/\|t\|)\), which converges to \(0\) by assumption. \(\Box\)

From this theorem we can easily generalize the Lyapounov and Lindeberg-Levy theorems from random variables to random vectors. The proofs will be left as exercises.

**Corollary 7.6**: _Suppose that \(X_{1},\ldots,X_{n}\) are independent and identically distributed random vectors in \(\mathbb{R}^{p}\) with finite mean \(\mu\) and positive definite variance matrix \(\Sigma\). Then, \(\sqrt{n}E_{n}(X-\mu)\stackrel{{\mathcal{D}}}{{\rightarrow}}N(0,\Sigma)\)._

**Corollary 7.7**: _Let \(\{X_{nk}:\,k=1,\ldots,k_{n},n=1,2,\ldots\}\) be a triangular array of \(p\)-dimensional random vectors in which the random vectors in each row are independent. Suppose, without loss of generality, that \(E(X_{nk})=0\). Let_

\[M_{n}(\epsilon)=\sum_{k=1}^{k_{n}}E\left(X_{nk}^{T}\Sigma_{n}^{-1}X_{nk} \right)^{1+\delta}.\]

_If \(M_{n}(\epsilon)\to 0\) for some \(\delta>0\), then \(\Sigma_{n}^{-1/2}S_{n}\stackrel{{\mathcal{D}}}{{\rightarrow}}N(0,1)\)._

### The \(\delta\)-method

The \(\delta\)-method is a convenient device that allows us to find the asymptotic distribution of a function of a random vector that converges in distribution to some random vector. Specifically, suppose \(X_{n}\) and \(U\) are \(p\)-dimensional random vectors, and \(g\) is a differentiable function taking values in \(\mathbb{R}^{m}\), where \(m\leq p\). The question is: if, for some positive sequence \(a_{n}\) and some \(\mu\in\mathbb{R}^{p}\), \(a_{n}(X_{n}-\mu)\) converges in distribution to a random vector \(U\), then what is the limit of \(a_{n}[g(X_{n})-g(\mu)]\)? The most commonly used form of this result is the case where \(U\) is the multivariate Normal random vector, \(a_{n}=\sqrt{n}\), and \(\mu=E(X)\). However, it is actually easier to prove the theorem in the general case.

In the following, we denote the \(i\)th component of \(g\) by \(g_{i}\), and \(j\)th component of \(X_{n}\) by \(X_{n}^{j}\). Furthermore, we let

\[\frac{g(x)-g(\mu)}{(x-\mu)^{T}}\]

denote the \(m\times p\) matrix whose \((i,j)\)th entry is \([g_{i}(x)-g_{i}(\mu)]/(x^{j}-\mu^{j})\). Note that, in this notation, we have \[g(x)-g(\mu)=\frac{g(x)-g(\mu)}{(x-\mu)^{T}}\,(x-\mu).\]

Moreover, if \(g\) is differentiable at \(\mu,\) then

\[\lim_{x\to\mu}\frac{g(x)-g(\mu)}{(x-\mu)^{T}}=\frac{\partial g(\mu)}{\partial\mu ^{T}}. \tag{7.12}\]

**Theorem 7.14** (The \(\delta\)-method): _Suppose \(\{X_{n}:n=1,2,\ldots\}\) is a sequence of \(p\)-dimensional random vectors such that, for a positive sequence \(a_{n}\to\infty\) and a vector \(\mu\in\mathbb{R}^{p}\),_

\[a_{n}(X_{n}-\mu)\stackrel{{\mathcal{D}}}{{\to}}U. \tag{7.13}\]

_If \(g\) is a differentiable function taking values in \(\mathbb{R}^{m}\), where \(m\leq p\), then_

\[a_{n}[g(X_{n})-g(\mu)]\stackrel{{\mathcal{D}}}{{\to}}[\partial g (\mu)/\partial\mu^{T}]U.\]

Proof.: Note that

\[a_{n}[g(X_{n})-g(\mu)]=\frac{g(X_{n})-g(\mu)}{(X_{n}-\mu)^{T}}\,[a_{n}(X_{n}- \mu)]. \tag{7.14}\]

Define \(h:\mathbb{R}^{p}\to\mathbb{R}^{m\times p}\) to be the following function

\[h(t)=\begin{cases}[g(t)-g(\mu)]/(t-\mu)^{T}&\quad t\neq\mu\\ \partial g(\mu)/\partial\mu^{T}&\quad t=\mu\end{cases}\]

Then we can rewrite the identity (7.14) as

\[a_{n}[g(X_{n})-g(\mu)]=h(X_{n})\,[a_{n}(X_{n}-\mu)],\]

which also holds obviously when \(X_{n}=\mu.\) By (7.12), \(h\) is continuous at \(\mu.\) By (7.13), \(X_{n}\stackrel{{ P}}{{\to}}\mu.\) Hence, by the Continuous Mapping Theorem,

\[h(X_{n})\stackrel{{ P}}{{\to}}h(\mu)=\partial g(\mu)/\partial\mu ^{T}.\]

By Slutsky's Theorem (see Corollary 7.2),

\[a_{n}[g(X_{n})-g(\mu)]=h(X_{n})a_{n}(X_{n}-\mu)\stackrel{{ \mathcal{D}}}{{\to}}[\partial g(\mu)/\partial\mu^{T}]\,U,\]

as desired. 

In the multivariate Normal case the above reduces to the following familiar form.

**Corollary 7.8**: _Suppose \(\{X_{n}:n=1,2,\ldots\}\) is a sequence of \(p\)-dimensional random vectors such that_

\[\sqrt{n}(X_{n}-\mu)\stackrel{{\mathcal{D}}}{{\to}}N(0,\Sigma),\]

_where \(\mu=E(X).\) If \(g\) is a differentiable function taking values in \(\mathbb{R}^{m}\), where \(m\leq p\), then_

\[\sqrt{n}[g(X_{n})-g(\mu)]\stackrel{{\mathcal{D}}}{{\to}}N\left(0, \frac{g(\mu)}{\partial\mu^{T}}\,\Sigma\frac{g^{T}(\mu)}{\partial\mu}\right).\]

### Mann-Wald notation for order of magnitude

Recall that, in calculus, the magnitude of a sequence of numbers is denoted by the little o or the big O notation. If a sequence of numbers \(\{x_{n}\}\) is bounded, then we write \(x_{n}=O(1)\); if the sequence converges to \(0\), then we write \(x_{n}=o(1)\). Furthermore, if \(\{a_{n}\}\) is another sequence such that \(x_{n}/a_{n}=O(1)\), then we write \(x_{n}=O(a_{n})\) and say that the magnitude of \(x_{n}\) is no greater than that of \(a_{n}\). If \(x_{n}/a_{n}=o(1)\), then we write \(x_{n}=o(a_{n})\) and say that \(x_{n}\) is ignorable compared with \(a_{n}\). A similar notational system can be applied to a sequence of random variables or random vectors. This notational system was introduced by Mann and Wald (1943). In the following, \(X_{n}\) are \(p\)-dimensional random vectors.

**Definition 7.7**: _A sequence of random vectors \(\{X_{n}\}\) is said to be bounded in probability if, for any \(\epsilon>0\), there is a \(K>0\), such that_

\[P(\|X_{n}\|>K)<\epsilon. \tag{7.15}\]

_for all \(n\)._

There are two more equivalent conditions for this definition: the first requires (7.15) to hold for all sufficiently large \(n\). That is, there exists an \(n_{0}\) such that (7.15) holds for all \(n>n_{0}\); the second is

\[\limsup_{n\to\infty}P(\|X_{n}\|>K)<\epsilon.\]

This is a generalization of the notion of bounded sequence of numbers to a sequence of random variables. Using this notion of boundedness, we can extend the big O notation to sequence of random vectors.

**Definition 7.8**: _If a sequence of random vectors \(\{X_{n}:n=1,2,\ldots\}\) is bounded in probability, then we write \(X_{n}=O_{P}(1)\). Furthermore, if \(\{a_{n}\}\) is a sequence of non-random positive constants, and if \(X_{n}/a_{n}=O_{P}(1)\), then we \(X_{n}=O_{P}(a_{n})\)._

The interpretation of \(X_{n}=O_{P}(a_{n})\) is that the order of magnitude of the random sequence \(\{X_{n}\}\) is not greater than that of the nonrandom sequence \(\{a_{n}\}\). Similarly, we replace the deterministic convergence \(x_{n}\to 0\) with the stochastic convergence \(X_{n}\stackrel{{ P}}{{\to}}0\) extend the little o notation to a sequence of random vectors.

**Definition 7.9**: _If a sequence of random vectors \(\{X_{n}:n=1,2,\ldots\}\) converges in probability to zero, then we write \(X_{n}=o_{P}(1)\). Furthermore, \(X_{n}=o_{P}(a_{n})\) if \(\{a_{n}\}\) is a sequence of non-random positive constants, and \(X_{n}/a_{n}=o_{P}(1)\)._The interpretation of \(X_{n}=o_{P}(a_{n})\) is that the random sequence \(\{X_{n}\}\) is ignorable compared with nonrandom sequence \(\{a_{n}\}\).

If we think of \(O\) as a nonzero constant such as \(1\), and \(o\) as \(0\), then the product between different types of O's obeys the same rules of the product of \(0\) and \(1\). That is, similar to

\[1\times 1=1,\quad 1\times 0=0,\quad 0\times 0=0.\]

Thus, for two positive nonrandom sequences \(\{a_{n}\}\) and \(\{b_{n}\}\):

\[O(a_{n})O(b_{n}) = O(a_{n}b_{n}),\] \[O(a_{n})o(b_{n}) = o(a_{n}b_{n}),\] \[o(a_{n})o(b_{n}) = o(a_{n}b_{n}).\]

These equalities should be interpreted in the following way. For example, the first equality means that if \(x_{n}=O(a_{n})\), \(y_{n}=O(b_{n})\), then \(x_{n}y_{n}=O(a_{n}b_{n})\). The above rules can be easily proved by the definitions of \(O(a_{n})\) and \(o(a_{n})\). A similar set of rules apply to \(O_{P}\) and \(o_{P}\), as summarized by the next theorem.

**Theorem 7.15** (The rules of Os): \[O_{P}(a_{n})O_{P}(b_{n}) = O_{P}(a_{n}b_{n})\] \[O_{P}(a_{n})o_{P}(b_{n}) = o_{P}(a_{n}b_{n})\] \[o_{P}(a_{n})o_{P}(b_{n}) = o_{P}(a_{n}b_{n}).\]

Again, these equalities should be interpreted in terms of the underlying sequences of random random variables. For example, the first equality should be interpreted as: if \(X_{n}=O_{P}(a_{n})\), \(Y_{n}=O_{P}(b_{n})\), then \(X_{n}Y_{n}=O_{P}(a_{n}b_{n})\). Note that, here, we assume \(X_{n}\) and \(Y_{n}\) to be numbers rather than vectors.

Proof: 1. If \(X_{n}=O_{P}(a_{n})\) and \(Y_{n}=O_{P}(b_{n})\), then, for any \(\epsilon>0\), there exist \(K_{1}>0\) and \(K_{2}>0\) such that

\[\limsup_{n}P(|X_{n}|>K_{1})<\epsilon/2,\quad\limsup_{n}P(|Y_{n}|>K_{2})< \epsilon/2.\]

Because \(|X_{n}Y_{n}|/(a_{n}b_{n})>K_{1}K_{2}\) implies that at least one of the following inequalities hold

\[\frac{|X_{n}|}{a_{n}}>K_{1},\quad\frac{|Y_{n}|}{b_{n}}>K_{2},\]

we have

\[P\left(\frac{|X_{n}Y_{n}|}{a_{n}b_{n}}>K_{1}K_{2}\right)\leq P\left(\frac{|X_ {n}|}{a_{n}}>K_{1}\right)+P\left(\frac{|Y_{n}|}{b_{n}}>K_{2}\right).\]Hence

\[\limsup_{n\to\infty}P\left(\frac{|X_{n}Y_{n}|}{a_{n}b_{n}}>K_{1}K_{2}\right) \leq\limsup_{n\to\infty}P\left(\frac{|X_{n}|}{a_{n}}>K_{1}\right)+ \limsup_{n\to\infty}P\left(\frac{|Y_{n}|}{b_{n}}>K_{2}\right)\] \[<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon,\]

which means \(|X_{n}Y_{n}|/(a_{n}b_{n})\) is bounded in probability.

_2._ Suppose that \(X_{n}=O_{P}(a_{n})\) and \(Y_{n}=o_{P}(b_{n})\). Let \(\epsilon>0,\;\delta>0\) be constants. Let \(K>0\) be such that

\[P\left(\frac{|X_{n}|}{a_{n}}\geq K\right)<\delta.\]

Then

\[P\left(\frac{|X_{n}Y_{n}|}{a_{n}b_{n}}>K\right) =P\left(\frac{|X_{n}Y_{n}|}{a_{n}b_{n}}>\epsilon,\,\frac{|X_{n}|}{ a_{n}}>K\right)+P\left(\frac{|X_{n}Y_{n}|}{a_{n}b_{n}}>\epsilon,\,\frac{|X_{n}|}{ a_{n}}\leq K\right)\] \[\leq P\left(\frac{|X_{n}|}{a_{n}}>K\right)+P\left(\frac{|Y_{n}|}{ b_{n}}>\frac{\epsilon}{K}\right)\] \[\leq P\left(\frac{|Y_{n}|}{b_{n}}>\frac{\epsilon}{K}\right)+\delta.\]

Therefore,

\[\limsup_{n\to\infty}P\left(\frac{X_{n}Y_{n}}{a_{n}b_{n}}>\epsilon\right)\leq \limsup_{n\to\infty}P\left(\frac{|Y_{n}|}{b_{n}}>\frac{\epsilon}{K}\right)+ \delta=\delta.\]

Since \(\delta>0\) is arbitrary, we have

\[\limsup_{n\to\infty}P\left(\frac{X_{n}Y_{n}}{a_{n}b_{n}}>\epsilon\right)=0,\]

as desired.

_3._ Let \(X_{n}/a_{n}=o_{P}(1)\) and \(Y_{n}/b_{n}=o_{P}(1)\). It suffices to show \(X_{n}/a_{n}=O_{P}(1)\) because, by part 2,

\[\frac{X_{n}Y_{n}}{a_{n}b_{n}}=O_{P}(1)o_{P}(1)=o_{P}(1).\]

If \(\epsilon>0\), then

\[P\left(\frac{|X_{n}|}{a_{n}}>1\right)=0<\epsilon.\]

So \(X_{n}/a_{n}=O_{P}(1)\). \(\Box\)

We can also use Theorem 7.15 to evaluate the order of products such as \(a_{n}X_{n}\), where \(a_{n}\) is fixed and \(X_{n}\) is random. This is because \(o\) or \(O\) are special cases of \(o_{P}\) or \(O_{P}\), as the next proposition shows.

**Proposition 7.3**: _If \(X_{n}=O(a_{n})\), then \(X_{n}=O_{P}(a_{n})\); if \(X_{n}=o(a_{n})\), then \(X_{n}=o_{P}(a_{n})\)._

If \(X_{n}=O(a_{n})\), then \(|X_{n}/a_{n}|\leq K\) for some constant \(K\). Let \(\epsilon>0\), then \(P(|X_{n}/a_{n}|>K+1)=0<\epsilon\). If \(X_{n}=o(a_{n})\), then \(X_{n}/a_{n}\to 0\). Let \(\epsilon>0\). Then, for sufficiently large \(n\), \(|X_{n}/a_{n}|\leq\epsilon\). So, for sufficiently large \(n\), \(P(|X_{n}/a_{n}|>\epsilon)=0\). \(\Box\)

For example, by Theorem 7.15 and Proposition 7.3 we have the following relations:

\[O_{P}(a_{n})O(b_{n})= O_{P}(a_{n}b_{n}),\] \[O_{P}(a_{n})o(b_{n})= o_{P}(a_{n}b_{n}),\] \[O(a_{n})o_{P}(b_{n})= o_{P}(a_{n}b_{n}),\] \[o_{P}(a_{n})o(b_{n})= o_{P}(a_{n}b_{n}).\]

## 7.8 Hilbert spaces

The notion of projections in Hilbert spaces is used frequently later in this book. In this section we outline basic properties of Hilbert spaces. A Hilbert space is an extension of the Euclidean space \(\mathbb{R}^{p}\). We begin with the definition of a vector space defined on the field of real numbers.

**Definition 7.10** (Vector space): _A vector space is a set \(\mathcal{V}\), together with an operation \(+\) between elements in \(\mathcal{V}\), and an operation \(\cdot\) between numbers in \(\mathbb{R}\) and elements \(\mathcal{V}\) satisfying the following conditions._

1. _Operation_ \(+\)_:_ 1. _(closure)_ _If_ \(v_{1},v_{2}\in\mathcal{V}\) _then_ \(v_{1}+v_{2}\in\mathcal{V}\)_;_ 1. _(commutative law)_ \(v_{1}+v_{2}=v_{2}+v_{1}\)_;_ 1. _(associative law)_ \(v_{1}+(v_{2}+v_{3})=(v_{1}+v_{2})+v_{3}\)_;_ 1. _(zero element)_ _There is a unique element_ \(0\in\mathcal{V}\) _such that, for all_ \(v\in\mathcal{V}\)_,_ \(v+0=v\)_;_ 1. _(negative element)_ _For each_ \(v\in\mathcal{V}\)_, there is a_ \((-v)\in\mathcal{V}\) _such that_ \(v+(-v)=0\)_._
2. _Operation_ \(\cdot\) _between the members of_ \(\mathbb{R}\) _and_ \(\mathcal{V}\)_:_ 2. _(closure)_ _If_ \(\lambda\in\mathbb{R}\)_,_ \(v\in\mathcal{V}\)_, then_ \(\lambda\cdot v\in\mathcal{V}\)_;_ 2. _(distributive law 1)_ _If_ \(\lambda\in\mathbb{R}\)_,_ \(u,v\in\mathcal{V}\)_, then_ \(\lambda\cdot(u+v)=\lambda\cdot u+\lambda\cdot v\)_;_ 2. _(distributive law 2)_ _If_ \(\lambda,\mu\in\mathbb{R}\) _and_ \(v\in\mathcal{V}\)_, then_ \((\lambda+\mu)\cdot v=\lambda\cdot v+\mu\cdot v\)_;_ 2. _(associative law)_ _If_ \(\lambda,\mu\in\mathbb{R}\) _and_ \(v\in\mathcal{V}\) _then_ \(\lambda\cdot(\mu\cdot v)=(\lambda\mu)\cdot v\)_;_ 2. _(unit element)_ _For any_ \(v\in\mathcal{V}\)_,_ \(1\cdot v=v\)_._

Note that \(0\cdot v=0\) for any \(v\in\mathcal{V}\), because

\[v+0\cdot v=1\cdot v+0\cdot v=(1+0)\cdot v=1\cdot v=v.\]Thus, \(0\cdot v\) is the zero element in \(\mathcal{V}\). Also note that the zero element is unique. In fact, let \(0_{1},0_{2}\) be members of \(\mathcal{H}\) that satisfies \(0_{1}+v=v\) and \(0_{2}+v=v\) for all \(v\in\mathcal{V}\). By taking \(v=0_{1}\) and \(v=0_{2}\) separately in \(1d\) above, we get \(0_{1}+0_{2}=0_{2}\), and \(0_{1}+0_{2}=0_{2}+0_{1}=0_{1}\). Therefore, \(0_{1}=0_{2}\).

For the rest of the book we will omit the dot in \(\lambda\cdot v\) and write it simply as \(\lambda v\). To sum up, a vector space consists of four ingredients: a set \(\mathcal{V}\), an operation \(+\) between members of \(\mathcal{V}\), an operation \(\cdot\) between members of \(\mathbb{R}\) and members of \(\mathcal{V}\), and finally a zero element in \(\mathcal{V}\). Thus, a rigorous notation of a vector space is \(\{\mathcal{V},+,\cdot,0\}\). However, in most cases we simply denote a vector space by the set \(\mathcal{V}\) without causing ambiguity. We now give some examples of a vector space.

**Example 7.1** The space \(\mathbb{R}^{n}\). Let \(a,b\in\mathbb{R}^{n}\) and \(\lambda\in\mathbb{R}\). Define

\[\begin{pmatrix}a_{1}\\ \vdots\\ a_{n}\end{pmatrix}+\begin{pmatrix}b_{1}\\ \vdots\\ b_{n}\end{pmatrix}=\begin{pmatrix}a_{1}+b_{1}\\ \vdots\\ a_{n}+b_{n}\end{pmatrix}\!\!,\ \ \text{and}\ \ \lambda\begin{pmatrix}a_{1}\\ \vdots\\ a_{n}\end{pmatrix}=\begin{pmatrix}\lambda a_{1}\\ \vdots\\ \lambda a_{n}\end{pmatrix}\!\!.\]

Furthermore, define the zero element in \(\mathbb{R}^{n}\) to be the vector \((0,\ldots,0)^{T}\). Then, it is easy to verify that the conditions in Definition 7.10 are satisfied, which means \(\mathbb{R}^{n}\) is a vector space. \(\Box\)

**Example 7.2** Space of square-integrable functions. Let \((\Omega,\mathcal{F},P)\) be a probability space. Let \(L_{2}(P)\) be the set of all real-valued functions \(f\) such that \(\int f^{2}dP<\infty\). For \(f_{1},f_{2}\in L_{2}(P)\) and \(\lambda\in\mathbb{R}\) define \(f_{1}+f_{2}\) and \(\lambda f_{1}\) to be the following members of \(L_{2}(P)\):

\[f_{1}+f_{2}:x\mapsto f_{1}(x)+f_{2}(x),\ \ \lambda f_{1}:x\mapsto\lambda f_{1}(x).\]

Furthermore, define the zero element in \(L_{2}(P)\) to be the function \(f(x)=0\) almost everywhere \(P\). Then it is easy to verify that the conditions in Definition 7.10 are satisfied. This space is called the \(L_{2}\) space with respect to \((\Omega,\mathcal{F},P)\), and is written as \(L_{2}(P)\). \(\Box\)

An inner product space \(\mathcal{V}\), or pre-Hilbert space, is a vector space together with a mapping \(u:\mathcal{V}\times\mathcal{V}\rightarrow\mathbb{R}\) that is symmetric, bilinear, and positive definite.

**Definition 7.11**: _Suppose that \(\mathcal{V}\) is a vector space. An inner product is a function \(u:\mathcal{V}\times\mathcal{V}\mapsto\mathbb{R}\) such that for any \(x,y,z\in\mathcal{V}\) and \(\alpha,\beta\in\mathbb{R}\) we have_

1. _(symmetric)_ \(u(x,y)=u(y,x)\)_,_
2. _(bilinear)_ \(u(\alpha x+\beta y,z)=\alpha u(x,z)+\beta u(y,z)\)_,_
3. _(positive)_ \(u(x,x)\geq 0\) _for all_ \(x\in\mathcal{V}\)_,_
4. _(definite)_ \(u(x,x)=0\) _implies_ \(x=0\)_A vector space \(\mathcal{V}\), together with an inner product \(u:\mathcal{V}\times\mathcal{V}\to\mathbb{R}\), is called an inner product space._

Note that, if \(x=0\), then it can be written as \(0\cdot y\) for some \(y\in\mathcal{V}\). Therefore, \(u(x,x)=u(x,0\cdot y)=0\times u(x,y)=0\). Thus an inner product also satisfies the following condition

_v._\(x=0\)_implies_\(u(x,x)=0\).

Also, by properties \(i\) and _ii_ we see that

\[u(x,\alpha y+\beta z)=u(\alpha y+\beta z,x)=\alpha u(y,x)+\beta u(z,x)=\alpha u (x,y)+\beta u(x,z).\]

That is, \(u\) is in fact a bilinear function. We record this as the sixth property of an inner product:

_vi._\(u(x,\alpha y+\beta z)=\alpha u(x,y)+\beta u(x,z)\).

For the rest of the book we will write \(u(x,y)\) as \(\langle x,y\rangle_{\mathcal{V}}\). If there is no source of confusion, the subscript \(\mathcal{V}\) is dropped and the inner product in \(\mathcal{V}\) is simply written as \(\langle x,y\rangle\). Two examples of inner product spaces are given below.

**Example 7.3**: Let \(\mathcal{V}\) be the vector space in Example 7.1, and let \(A\) be an \(n\) by \(n\) positive definite matrix. Let \(u\) be the mapping

\[u:\mathbb{R}^{n}\times\mathbb{R}^{n}\to\mathbb{R},\quad(x,y)\mapsto x^{T}Ay.\]

Then it can be easily verified that \(u\) defines an inner product. This inner product space is called the \(n\)-dimensional Euclidean space. \(\Box\)

**Example 7.4**: Let \(\mathcal{V}\) be the vector space \(L_{2}(P)\) in Example 7.2. Define the mapping \(u:\mathcal{V}\times\mathcal{V}\to\mathbb{R}\) by

\[(f,g)\mapsto\int fg\,dP.\]

The right-hand side is a finite number because, by Holder's inequality,

\[\int|fg|dP\leq\left(\int f^{2}dP\right)^{1/2}\left(\int g^{2}dP\right)^{1/2}.\]

See, for example, Billingsley (1995). It is easy to check that the function \(u\) thus defined satisfies properties _i, ii, iii_ in Definition 7.11. However, condition _iv_ is in general not satisfied, because \(\int f^{2}dP=0\) only implies \(f=0\) almost everywhere \(P\).

To make \(u\) an inner product, we introduce the following equivalence relation \(\sim\) in \(\mathcal{V}\):

\[f\sim g\ \ \text{if and only if}\ \ f=g\ \text{almost everywhere}\ P.\]It can be easily verified that \(\sim\) thus defined is indeed an equivalence relation. Let \(\mathcal{V}/\sim\) be the quotient space with respect to \(\sim\) (see Kelley, 1955). For two members \(F\) and \(G\) of \(\mathcal{V}/\sim\), let \(F+G\) be the equivalent class of \(f+g\), where \(f\) is any member of \(F\) and \(g\) is any member of \(G\). For \(\lambda\in\mathbb{R}\) and \(F\in\mathcal{V}/\sim\), let \(\lambda\cdot F\) be the equivalent class of \(\lambda\cdot f\), where \(f\) is any member of \(F\). Furthermore, define the zero element of \(\mathcal{V}/\sim\) as the equivalent class of any function that is almost everywhere \(0\). It can then be shown that, with these definitions of \(+\), \(\cdot\), \(0\), \(\mathcal{V}/\sim\) is indeed a vector space relative to \(\mathbb{R}\).

Furthermore, we introduce the mapping \(\tilde{u}:(\mathcal{V}/\sim)\times(\mathcal{V}/\sim)\rightarrow\mathbb{R}\) as follows. If \(F,G\) are members of \(\mathcal{V}/\sim\), then

\[\tilde{u}(F,G)=u(f,g),\]

where \(f\) is any member of \(F\) and \(g\) is any member of \(G\). Note that \(u(f,g)\) is not affected by the choices of \(f\) and \(g\). It can be shown that \(\tilde{u}\) does satisfy all four conditions in Definition 7.11. Thus, it is a well defined inner product. In other words,

\[\{\mathcal{V}/\sim,+,\cdot,0,\tilde{u}\}\]

forms an inner product space.

For our purpose, however, it will not cause serious ambiguity if we simply treat almost everywhere equal functions as the same function, and treat (simply) \(\mathcal{V}/\sim\) as \(\mathcal{V}\). This we will do throughout the rest of the book. \(\Box\)

A _norm_ in a vector space \(\mathcal{V}\) is a mapping \(\rho:\mathcal{V}\rightarrow\mathbb{R}\) such that

1. \(\rho(f)\geq 0\) for all \(f\in\mathcal{V}\),
2. for any \(a\in\mathbb{R}\) and \(f\in\mathcal{V}\), \(\rho(af)=|a|\rho(f)\),
3. for any \(f,g\in\mathcal{V}\), \(\rho(f+g)\leq\rho(f)+\rho(g)\),
4. \(\rho(f)=0\) implies \(f=0\).

A norm is a generalization of the absolute value. In particular, \(\rho(f-g)\) is a measure of distance between two members of \(\mathcal{V}\), just like \(|a-b|\) is a measure of distance between two numbers. For the rest of the book, we write \(\rho(f)\) as \(\|f\|_{\mathcal{V}}\) or simply \(\|f\|\). A vector space \(\mathcal{V}\), together with a norm \(\rho:\mathcal{V}\rightarrow\mathbb{R}\), is called a normed space.

It can be shown that, if \((\mathcal{V},u)\) is an inner product space, then the function

\[\rho:\mathcal{V}\rightarrow\mathbb{R},\ f\mapsto[u(f,f)]^{1/2}\]

is a norm. Thus, an inner product space is also a normed space.

Using this norm we can define the notions of limit and completeness in an inner product space. A sequence \(\{f_{n}:n=1,2,\ldots\}\) of elements of \(\mathcal{V}\) is a _Cauchy sequence_ if, for any \(\epsilon>0\), there exists an \(n_{0}\), such that for all \(m,n>n_{0}\) we have \[\|f_{n}-f_{m}\|<\epsilon.\]

We say that a sequence \(f_{n}\) converges to a member \(f\) of \(\mathcal{V}\) if \(\lim_{n\to\infty}\|f_{n}-f\|=0\).

**Definition 7.12**: _An inner product space \(\mathcal{V}\) is complete if every Cauchy sequence \(\{f_{n}\}\) in \(\mathcal{V}\) converges to a member of \(\mathcal{V}\). That is, \(\lim_{n\to\infty}\|f_{n}-f\|=0\) for some \(f\in\mathcal{V}\). A complete inner product space is called a Hilbert space._

If an inner product \(\mathcal{V}\) is finite dimensional, then it is always complete. Also, the \(L_{2}\)-space with respect to a measure is always complete. In other words, the inner product spaces in Example 7.3 and Example 7.4 are Hilbert spaces.

Recall that, one of the defining assumptions of a norm is the triangular inequality \(\|f+g\|\leq\|f\|+\|g\|\). There is an inequality for inner product of similar importance, but, unlike the triangular inequality, it is a consequence of the four defining assumptions of the inner product. This is the Cauchy-Schwarz inequality.

**Theorem 7.16** (The Cauchy-Schwarz inequality): _If \(\langle\cdot,\cdot\rangle\) is an inner product in an inner product space \(\mathcal{V}\), then, for any \(f,g\in\mathcal{V}\),_

\[\langle f,g\rangle^{2}\leq\langle f,f\rangle\langle g,g\rangle. \tag{7.16}\]

_Moreover, the equality holds if and only if \(f\) and \(g\) are proportional to each other._

Proof: First note that, if \(\langle g,g\rangle=0\), then \(g=0\) and consequently the inequality (7.16) holds. Now assume \(\langle g,g\rangle\neq 0\). Let \(\alpha\in\mathbb{R}\), \(f,g\in\mathcal{V}\). Then

\[0\leq\langle f-\alpha g,f-\alpha g\rangle=\langle f,f\rangle-2\alpha\langle f,g \rangle+\alpha^{2}\langle g,g\rangle\equiv F(\alpha).\]

Since \(F(\alpha)\) is a quadratic polynomial, it can be easily verified that it achieves its minimum at \(\alpha^{*}=\langle f,g\rangle/\langle g,g\rangle\). So we have

\[0\leq F(\alpha^{*}) =\langle f,f\rangle-2\alpha^{*}\langle f,g\rangle+{\alpha^{*}}^{ 2}\langle g,g\rangle\] \[=\langle f,f\rangle-2\frac{\langle f,g\rangle}{\langle g,g \rangle}\langle f,g\rangle+\frac{\langle f,g\rangle^{2}}{\langle g,g\rangle^{ 2}}\langle g,g\rangle\] \[=\langle f,f\rangle-\frac{\langle f,g\rangle^{2}}{\langle g,g \rangle},\]

which is the inequality (7.16).

Now suppose the equality in (7.16) holds, then \(F(\alpha^{*})=0\). Hence \(\langle f-\alpha^{*}g,f-\alpha^{*}g\rangle=0\), which implies \(f=\alpha^{*}g\). Thus \(f\) and \(g\) are proportional to each other. Conversely, if \(f\) and \(g\) are proportional then it is obvious that the equality in (7.16) holds. \(\Box\)

Using the Cauchy-Schwarz inequality we can easily show that the mapping \(\rho(f)=\langle f,f\rangle^{1/2}\) indeed satisfies the triangular inequality.

**Corollary 7.9**: _If \(\langle\cdot,\cdot\rangle\) is an inner product in an inner product space \(\mathcal{V}\) and \(\rho(f)=\langle f,f\rangle^{\frac{1}{2}}\), then, for any \(f,g\in\mathcal{V}\),_

\[\rho(f+g)\leq\rho(f)+\rho(g).\]

_Proof._ Note that

\[\rho(f+g)^{2}= \,\rho(f)^{2}+2\langle f,g\rangle+\rho(g)^{2}\] \[\leq \,\rho(f)^{2}+2|\langle f,g\rangle|+\rho(g)^{2}\] \[\leq \,\rho(f)^{2}+2\rho(f)\,\rho(g)+\rho(g)^{2}\] \[= \,(\rho(f)+\rho(g))^{2},\]

where the second inequality follows from the Cauchy-Schwarz inequality. Now take square root on both sides to complete the proof. \(\Box\)

### Multivariate Cauchy-Schwarz inequality

Recall that, in Lemma 2.3, we stated a version of the multivariate Cauchy-Schwarz inequality to establish the Cramer-Rao lower bound. In this section we further extend this inequality in terms of inner product matrices, which is useful for developing optimal estimating equations among other things. Let \(\mathcal{H}\) be a Hilbert space, and let \(\mathcal{H}^{p}\) be the \(p\)-fold Cartesian product:

\[\mathcal{H}^{p}=\underbrace{\mathcal{H}\times\ldots\times\mathcal{H}}_{p}.\]

For any two members \(S,G\) of \(\mathcal{H}^{p}\), define their inner product matrix as

\[[S,G]=\begin{pmatrix}\langle s_{1},g_{1}\rangle&\cdots&\langle s_{1},g_{p} \rangle\\ \vdots&\ddots&\vdots\\ \langle s_{p},g_{1}\rangle&\cdots&\langle s_{p},g_{p}\rangle\end{pmatrix}.\]

The inner product matrix shares similar properties with an inner product, as shown by the next Proposition. The proof is left as an exercise.

**Proposition 7.4**: _Let \(\mathcal{H}\) be a Hilbert space and \(\mathcal{H}^{p}\) be its \(p\)-fold Cartesian product. Let \([\cdot,\cdot]:\mathcal{H}^{p}\times\mathcal{H}^{p}\to\mathbb{R}^{p\times p}\) be the inner product matrix in \(\mathcal{H}^{p}\). Then_

1. _(symmetry after transpose)_ \([G_{1},G_{2}]=[G_{2},G_{1}]^{T}\)_;_
2. _(bilinear) If_ \(G_{1},G_{2},G_{3}\in\mathcal{H}^{p}\) _and_ \(a_{1},a_{2},a_{3}\in\mathbb{R}\)_, then_ \[[a_{1}G_{1}+a_{2}G_{2},G_{3}]= a_{1}[G_{1},G_{3}]+a_{2}[G_{2},G_{3}],\] \[[G_{1},a_{2}G_{2}+a_{3}G_{3}]= a_{2}[G_{1},G_{3}]+a_{3}[G_{1},G_{3}];\]3. (positivity) for any \(G\in\mathcal{H}^{p}\), \([G,G]\) is positive semidefinite;
4. (definiteness) \([G,G]=0\) implies \(G=0\).

Another useful property for the inner product matrix is that, if \(A,B\in\mathbb{R}^{p\times p}\) and \(F,G\in\mathcal{H}^{p}\), then

\[[AG,BF]=A[G,F]B^{T}. \tag{7.17}\]

To see this, let \((AG)_{i}\) and \((BF)_{i}\) be the \(i\)th component of \(AG\) and \(BF\). Then \((AG)_{i}=\operatorname{row}_{i}(A)G\) and \((BF)_{i}=\operatorname{row}_{i}(B)F\), where, for example, \(\operatorname{row}_{i}(A)\) means the \(i\)th row of \(A\). We have

\[[AG,BF] =\begin{pmatrix}\langle\operatorname{row}_{1}(A)G,\operatorname{ row}_{1}(B)F\rangle&\cdots&\langle\operatorname{row}_{1}(A)G,\operatorname{row}_{p}(B) F\rangle\\ \cdots&\ddots&\vdots\\ \langle\operatorname{row}_{p}(A)G,\operatorname{row}_{1}(B)F\rangle&\cdots& \langle\operatorname{row}_{p}(A)G,\operatorname{row}_{p}(B)F\rangle\end{pmatrix}\] \[=\begin{pmatrix}\operatorname{row}_{1}(A)\\ \vdots\\ \operatorname{row}_{p}(A)\end{pmatrix}[G,F]\left(\operatorname{row}_{1}(B)^{T}, \cdots,\operatorname{row}_{p}(B)\right)=A[G,F]B^{T}.\]

**Definition 7.13**: _For any two symmetric square matrices \(A\) and \(B\), write \(A\succeq B\) if \(A-B\) is positive semi-definite. This partial ordering is called the Loewner ordering._

In the special case where \(p=1\), \(G\) and \(S\) are simply members of \(\mathcal{H}\), and the matrix inequality in Theorem 7.17 below reduces to the classical Cauchy-Schwarz inequality:

\[\langle G,S\rangle^{2}\leq\langle G,G\rangle\langle S,S\rangle.\]

**Theorem 7.17** (Multivariate Cauchy-Schwarz inequality): _If \(S\) and \(G\) are members of \(\mathcal{H}^{p}\) and the matrix \([G,G]\) is invertible, then_

\[[S,S]\succeq[S,G][G,G]^{-1}[G,S].\]

Proof: By Proposition 7.4, \([G,G]\succeq 0\). Hence the matrix

\[[S-G[G,G]^{-1}G,S-G[G,G]^{-1}G]\]

is positive semi-definite. This matrix can be decomposed as

\[[S,S]-[S,G][G,G]^{-1}[G,S]-[S,G][G,G]^{-1}[G,S]\] \[+[S,G][G,G]^{-1}[G,G][G,G]^{-1}[G,S]\] \[=[S,S]-[S,G][G,G]^{-1}[G,S].\]

Hence the desired inequality. \(\Box\)

The condition that \([G,G]\) is invertible in the above Proposition can be relaxed using the Moore-Penrose inverse - see Problem 7.22.

### Projections

One of the most useful tools related to a Hilbert space is the notion of projection, which is based on orthogonality, as defined below.

Definition 7.14 (Orthogonality): If \(\mathcal{H}\) is a Hilbert space and if \(f,g\in\mathcal{H}\), then \(f\) and \(g\) are orthogonal if \(\langle f,g\rangle=0\). We write this as \(g\perp f\).

An immediate consequence of orthogonality is the Pythagaras theorem, as given below. The proof is left as an exercise.

Proposition 7.5: _If \(f_{1},\ldots,f_{n}\) are pairwise orthogonal vectors in \(\mathcal{H}\) then_

\[\|f_{1}+\cdots+f_{n}\|^{2}=\|f_{1}\|^{2}+\cdots+\|f_{n}\|^{2}.\]

A more general version of Pythagoras theorem the parallelogram law. Again, the proof is left as an exercise.

Proposition 7.6: _If \(\mathcal{H}\) is a Hilbert space and \(f,g\in\mathcal{H}\), then_

\[\|f+g\|^{2}+\|f-g\|^{2}=2(\|f\|^{2}+\|g\|^{2}).\]

We now define the linear subspace. Intuitively, it is any hyperplane that passes through the origin.

Definition 7.15: A subset \(\mathcal{G}\) of a Hilbert space is called a linear manifold if it is closed under linear operation. That is, for any \(f_{1},f_{2}\in\mathcal{G}\) and \(c_{1},c_{2}\in\mathbb{R}\), we have \(c_{1}f_{1}+c_{2}f_{2}\in\mathcal{G}\). A closed linear manifold is called a linear subspace.

Note that a subspace \(\mathcal{G}\) must contain the zero element of \(\mathcal{H}\). This is because, for any \(g\in\mathcal{G}\), \(0\cdot g=0\) must be a member of \(\mathcal{G}\). We now define projection. For a member \(f\) of \(\mathcal{H}\) and a subspace \(\mathcal{G}\) of \(\mathcal{H}\), the member of \(\mathcal{G}\) that is nearest to \(f\) is the projection of \(f\) on to \(\mathcal{G}\). Intuitively, if \(f^{*}\) is the projection of \(f\) on to \(\mathcal{G}\), then the vector \(f-f^{*}\) should be orthogonal to \(\mathcal{G}\).

Theorem 7.18 (Projection theorem): _If \(\mathcal{H}\) is a Hilbert space and \(\mathcal{G}\) is a linear subspace then, for any \(f\in\mathcal{H}\), there is a unique element \(f_{0}\in\mathcal{G}\) such that_

\[\|f-f_{0}\|\leq\|f-g\|\]

_for all \(g\) in \(\mathcal{G}\). furthermore, a vector \(f_{0}\) satisfies the above relation if and only if it satisfies_

\[\langle f-f_{0},g\rangle=0 \tag{7.18}\]

_for all \(g\in\mathcal{G}\)._The vector \(f_{0}\) is called the orthogonal projection of \(f\) on to \(\mathcal{G}\), and is written as \(P_{\mathcal{G}}(f)\). Or, if there is no ambiguity we will simply write this as \(Pf\). The operator \(P:\mathcal{H}\to\mathcal{H}\) thus defined is called a projection operator. It can be shown that \(P\) is a idempotent and self-adjoint linear operator; that is, for any \(f,g\in\mathcal{H}\), \(\alpha,\beta\in\mathbb{R}\), we have

\[P(Pf)=Pf,\quad\langle f,Pg\rangle=\langle Pf,g\rangle,\quad P(\alpha f+\beta g )=\alpha Pf+\beta Pg.\]

Conversely, if \(P:\mathcal{H}\to\mathcal{H}\) is a self-adjoint and idempotent linear operator, and if \(\mathcal{S}=\{Pf:f\in\mathcal{H}\}\), then \(\mathcal{S}\) is necessarily a linear subspace of \(\mathcal{H}\) and, for any \(f\in\mathcal{H}\), \(Pf\) is the orthogonal projection of \(f\) on to the subspace \(\mathcal{S}\). The proofs of this theorem and the above statements can be found in Conway (1990). Equation (7.18) provides a way to find the projection, as illustrated by the next example.

**Example 7.5** (Projection on to a finite dimensional subspace): Let \(\mathcal{H}\) be a Hilbert space and \(f\in\mathcal{H}\). Let \(\mathcal{M}\) be the subspace of \(\mathcal{H}\) spanned by the vectors \(f_{1},\ldots,f_{p}\) in \(\mathcal{H}\). We want to find the projection of \(f\) on to \(\mathcal{M}\). Let \(f_{0}\) be this projection; that is, \(f_{0}=P_{\mathcal{M}}f\). Because \(f_{0}\in\mathcal{M}\), it is a linear combination of \(f_{1},\ldots,f_{p}\), say \(f_{0}=a_{1}f_{1}+\cdots+a_{p}f_{p}\). By the projection formula (7.18), we have \(\langle f-f_{0},g\rangle=0\) for all \(g\in\mathcal{M}\). In particular, this holds for \(f_{1},\ldots,f_{p}\):

\[\langle f-f_{0},f_{i}\rangle=0,\ \ i=1,\ldots,p.\]

Since the left-hand side is

\[\langle f,f_{i}\rangle-\left\langle\sum_{j=1}^{p}a_{j}f_{j},f_{i}\right\rangle= \langle f,f_{i}\rangle-\sum_{j=1}^{p}\langle f_{j},f_{i}\rangle a_{j},\]

the coefficients \(a_{1},\ldots,a_{p}\) satisfy the following \(p\) equations

\[\sum_{j=1}^{p}\langle f_{j},f_{i}\rangle a_{j}=\langle f,f_{i}\rangle.\]

In matrix notation,

\[\left(\begin{array}{c}a_{1}\\ \vdots\\ a_{p}\end{array}\right)=\left(\begin{array}{cc}\langle f_{1},f_{1}\rangle& \cdots&\langle f_{1},f_{p}\rangle\\ \vdots&\cdots&\vdots\\ \langle f_{p},f_{1}\rangle&\cdots&\langle f_{p},f_{p}\rangle\end{array}\right) ^{-1}\left(\begin{array}{c}\langle f,f_{1}\rangle\\ \vdots\\ \langle f,f_{p}\rangle\end{array}\right)\]

The projection of \(f\) onto \(\mathcal{M}\) is therefore

\[P_{\mathcal{M}}f=\left(\langle f,f_{1}\rangle,\cdots,\langle f,f_{p}\rangle \right)\left(\begin{array}{cc}\langle f_{1},f_{1}\rangle&\cdots&\langle f_{ 1},f_{p}\rangle\\ \vdots&\cdots&\vdots\\ \langle f_{p},f_{1}\rangle&\cdots&\langle f_{p},f_{p}\rangle\end{array} \right)^{-1}\left(\begin{array}{c}f_{1}\\ \vdots\\ f_{p}\end{array}\right), \tag{7.19}\]provided that the matrix \(\{\langle f_{i},f_{j}\rangle\}_{i,j=1}^{p}\) is invertible. This matrix is called the Gram matrix with respect to the set \(\{f_{1},\ldots,f_{n}\}\), and will be written as \(G(f_{1},\ldots,f_{p})\). \(\Box\)

**Example 7.6** (Ordinary Least Squares): This is a specialization of Example 7.5 to \({\cal H}=\mathbb{R}^{n}\), which gives the formula for the Ordinary Least Squares. Let \(x_{1}\),..., \(x_{p}\), \(p\leq n\), be a set of linear independent vectors in \(\mathbb{R}^{n}\), and let \({\cal L}\) be the linear subspace spanned by \(x_{1}\),..., \(x_{p}\). Let \(y\) be another vector in \(\mathbb{R}^{n}\). Define the inner product in \(\mathbb{R}^{n}\) by \(\langle x,y\rangle=x^{T}y\). In this case, the Gram matrix \(G(x_{1},\ldots,x_{p})\) can be expressed as \(X^{T}\Sigma X\), where \(X\) is the \(n\) by \(p\) matrix whose \(i\)th column is \(x_{i}^{T}\). The vector \(\{\langle f,f_{i}\rangle\}\) can be expressed as \(X^{T}y\). Hence, the projection of \(y\) onto \({\cal L}\) can be expressed in matrix form as

\[\hat{y}=X(X^{T}X)^{-1}X^{T}y\equiv X\hat{\beta},\]

where \(\hat{\beta}=(X^{T}X)^{-1}X^{T}y\) is just the Ordinary Least Squares estimate and \(\hat{y}\) is the prediction vector of \(y\). \(\Box\)

**Example 7.7** (Conditional Expectation): Let \((X,Y)\) be a random element. Suppose \(P_{XY}\) and \(P_{Y}\) denote the probability measures induced by \((X,Y)\) and \(Y\) respectively. Then, it can be shown that \(L_{2}(P_{Y})\) is a subspace of \(L_{2}(P_{XY})\). Let \(f\) be a member of \(L_{2}(P_{XY})\). We now show that \(f_{0}(Y)=E[f(X,Y)|Y]\) is the projection of \(f\) on to \(L_{2}(P_{Y})\) using the projection formula (7.18). If \(g\) is an arbitrary member of \(L_{2}(P_{Y})\), then

\[\langle f-f_{0},g\rangle= \int(f-f_{0})g\,dP_{XY}\] \[= E\{[f(X,Y)-f_{0}(Y)]g(Y)\}\] \[= E\{[f(X,Y)-E(f(X,Y)|Y)]g(Y)\}\] \[= E[f(X,Y)g(Y)]-E[E(f(X,Y)|Y)g(Y)].\]

Since the second term in the last line is

\[E[E(f(X,Y)|Y)g(Y)]=E[E(g(Y)f(X,Y)|Y)]=E[g(Y)f(X,Y)],\]

we have

\[\langle f-f_{0},g\rangle=0,\]

which means \(f_{0}\) is the projection of \(f\) on to \(L_{2}(P_{Y})\). \(\Box\)

Before proceeding to the next example, we first introduce the concept of the Moore-Penrose inverse (See, for example, Kollo and von Rosen, 2005).

**Definition 7.16**: _Let \(A\) be a matrix. The Moore-Penrose inverse \(A^{+}\) is defined to be the (unique) matrix that satisfies_

\[AA^{+}A=A,\quad A^{+}AA^{+}=A^{+},\quad(AA^{+})^{T}=AA^{+},\quad(A^{+}A)^{T}=A^{ +}A.\]

**Example 7.8**: Let \(\Sigma\) be a positive definite matrix in \(\mathbb{R}^{p\times p}\) and consider the Hilbert space consisting of the linear space \(\mathbb{R}^{p}\) and the inner product defined by \(\langle x,y\rangle=x^{T}\Sigma y\). Let \(\mathcal{G}\) be a linear subspace of \(\mathbb{R}^{p}\) of dimension \(q\leq p\) and let \(\{v_{1},\ldots,v_{r}\}\), \(r\geq q\), be a set of vectors in \(\mathbb{R}^{p}\) that span \(\mathcal{G}\). Let \(V\) be the \(p\times r\) matrix \((v_{1},\ldots,v_{r})\). Note that \(v_{1},\ldots,v_{r}\) need not be linearly independent, and hence \(V^{T}\Sigma V\) need not be invertible. Let

\[P_{\mathcal{G}}(\Sigma)=V(V^{T}\Sigma V)^{+}V^{T}\Sigma.\]

We now show that this matrix is the projection operator with range \(\mathcal{G}\).

We first note that

\[P_{\mathcal{G}}(\Sigma)P_{\mathcal{G}}(\Sigma)= V(V^{T}\Sigma V)^{+}V^{T}\Sigma V(V^{T}\Sigma V)^{+}V^{T}\Sigma\] \[= V(V^{T}\Sigma V)^{+}V^{T}\Sigma=P_{\mathcal{G}}(\Sigma).\]

Thus \(P_{\mathcal{G}}(\Sigma)\) is idempotent. Moreover, for any \(x,y\in\mathbb{R}^{p}\),

\[\langle x,P_{\mathcal{G}}(\Sigma)y\rangle=x^{T}\Sigma V(V^{T}\Sigma V)^{+}V^{T }\Sigma y=\langle P_{\mathcal{G}}(\Sigma)x,y\rangle.\]

Thus \(P_{\mathcal{G}}(\Sigma)\) is self-adjoint. Since, for any \(x\in\mathbb{R}^{p}\), \(P_{\mathcal{G}}(\Sigma)x=Vz\) for some \(z\in\mathbb{R}^{p}\), the range of \(P_{\mathcal{G}}(\Sigma)\) is contained in \(\operatorname{span}(V)=\mathcal{G}\). Since \(\operatorname{span}(V)\) is a \(q\)-dimensional subspace, \(V\) has rank \(q\). Because \(\Sigma\) is nonsingular, \(V(V^{T}\Sigma V)^{+}V^{T}\Sigma\) also has rank \(q\). Thus the range of \(P_{\mathcal{G}}(\Sigma)\) is not a proper subset of \(\mathcal{G}\). \(\Box\)

### Problems

**7.1.** Suppose \(X_{n}=(Y_{n1},\ldots,Y_{nk})^{T}\), \(n=1,2,\ldots\), are \(k\)-dimensional random vectors. Show that \(X_{n}\) converges almost everywhere to a random vector \(X=(Y_{1},\ldots,Y_{k})^{T}\) if and only if \(Y_{ni}\) converges almost everywhere to \(Y_{i}\) for each \(i=1,\ldots,k\).

**7.2.** Suppose \(X_{n}=(Y_{n1},\ldots,Y_{nk})^{T}\), \(n=1,2,\ldots\), are \(k\)-dimensional random vectors. Show that \(X_{n}\) converges in probability to a random vector \(X=(Y_{1},\ldots,Y_{k})^{T}\) if and only if \(Y_{ni}\) converge to \(Y_{i}\) in probability for each \(i=1,\ldots,k\).

**7.3.** Show that, if \(X_{n}\) is a sequence of random vectors that converges almost everywhere to a random vector, then the sequence converges to \(X\) in probability.

**7.4.** Show that a random vector \(X\) is degenerate at \(a\) -- that is, \(P(X=a)=1\) -- if and only if \(P\circlearrowleft X^{-1}=\delta_{a}\). Furthermore, use the Portmanteau theorem to show that \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}\delta_{a}\) if and only if \(X_{n}\stackrel{{ P}}{{\to}}a\).

**7.5.** Suppose \(X_{n}\) is a sequence of random vectors that converges in probability to a random vector \(X\); that is, \(P(\|X_{n}-X\|>\epsilon)\to 0\) for any \(\epsilon>0\). Show that, for any open set \(G\in\mathcal{F}_{X}\), \(\liminf P(X_{n}\in G)\geq P(X\in G)\). Use the Portmanteau theorem to conclude that \(X_{n}\stackrel{{\mathcal{D}}}{{\to}}X\).

**7.6.** Prove part 3 of Proposition 7.1.

**7.7.** Suppose that \(X_{1},X_{2},\ldots\) are independent and uniformly bounded random variables with mean \(E(X_{n})=0\) for all \(n\). Let \(S_{n}=X_{1}+\cdots+X_{n}\) and let \(s_{n}^{2}=\mbox{var}(S_{n})\). Show that, if \(s_{n}\to\infty\), then \(S_{n}/s_{n}\stackrel{{\mathcal{D}}}{{\to}}N(0,1)\).

**7.8.** Let \(X_{1},\cdots,X_{n}\) be uncorrelated random variables with means \(\mu_{1},\cdots,\mu_{n}\) and variances \(\sigma_{1}^{2},\cdots,\sigma_{n}^{2}\). Suppose that \((\sigma_{1}^{2}+\cdots+\sigma_{n}^{2})/n^{2}\to 0\) as \(n\to\infty\). Prove that

\[n^{-1}\sum_{i=1}^{n}(X_{i}-\mu_{i})\stackrel{{ P}}{{\longrightarrow }}0.\]

**7.9.** Let \(X_{1},X_{2},\ldots\) be a sequence of i.i.d. random variables with finite first four moments. Denote \(E(X_{i})\) by \(\mu\) and \(\mbox{var}(X_{i})\) by \(\sigma^{2}\). Let \(S_{n}=\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}/(n-1)\) be the unbiased estimate of \(\sigma^{2}\).

(a) Show that \(S_{n}\stackrel{{ P}}{{\longrightarrow}}\sigma^{2}\).

(b) Show that

\[\frac{\sum_{i=1}^{n}(X_{i}-\mu)}{\sqrt{n\,S_{n}}}\stackrel{{ \mathcal{D}}}{{\longrightarrow}}N(0,1).\]

**7.10.** Let \(X_{1},\cdots,X_{n}\) be an i.i.d. sequence with \(E(X_{i})=\mu\neq 0\) and \(\mbox{var}(X_{i})=\sigma^{2}<\infty\). Find the asymptotic distribution of

(a) \(\sqrt{n}\left(\overline{X}^{-1}-\mu^{-1}\right)\),

(b) \(\sqrt{n}\left(\overline{X}^{2}-\mu^{2}\right)\),

(c) \(\sqrt{n}\log(\overline{X}/\mu)\),

(d) \(\sqrt{n}\left(e^{\overline{X}}-e^{\mu}\right)\).

**7.11.** Suppose that \(\sqrt{n}(X_{n}-\mu)\stackrel{{\mathcal{D}}}{{\to}}N(0,\sigma^{2})\), and that \(g\) is a function of \(X_{n}\) with continuous second derivative such that \(g^{\prime}(\mu)=0\) and \(g^{\prime\prime}(\mu)\neq 0\). Find the asymptotic distribution of \(n[g(X_{n})-g(\mu)]\). (Hint: a version of the Taylor's theorem states that if \(f\) has continuous \(k\)th derivative, then

\[f(x)=f(x_{0})+f^{\prime}(x_{0})(x-x_{0})+\cdots+f^{(k)}(\xi)(x-x_{0})^{k}/k!\]

for some \(\xi\) satisfying \(|\xi-x_{0}|\leq|x-x_{0}|\). )

**7.12**.: Show that Definition 7.6 is equivalent to boundedness in probability.

**7.13**.: Use the Dominated Convergence Theorem to show that \(X\) is \(P\)-integrable (i.e. \(E_{P}(|X|)<\infty\)) if and only if

\[\lim_{\alpha\to\infty}E_{P}[|X|I(|X|\geq\alpha)]=0.\]

**7.14**.: Show that the following rules hold for \(O_{P}\) and \(o_{P}\). For any positive sequences \(a_{n}\) and \(b_{n}\), we have

\[O_{P}(a_{n})+O_{P}(b_{n}) = O_{P}(\max(a_{n},b_{n})),\] \[o_{P}(a_{n})+o_{P}(b_{n}) = o_{P}(\max(a_{n},b_{n})),\] \[o_{P}(a_{n})+O_{P}(a_{n}) = O_{P}(a_{n}).\]

**7.15**.: Show that, if a sequence of random vectors \(X_{n}\) converges in distribution, or in probability, or almost everywhere, to a random vector \(X\), then \(X_{n}=O_{P}(1)\).

**7.16**.: Show that, if \(X_{n}\) is a sequence of integrable random vectors with \(E\|X_{n}\|\) being a bounded sequence, then \(X_{n}=O_{P}(1)\).

**7.17**.: Suppose that \(X_{n}\) is a sequence of random vectors taking values in \(A\subseteq\mathbb{R}^{k}\), and \(f:A\to\mathbb{R}^{m}\) satisfies the following Lipschitz condition:

\[\frac{\|f(x_{1})-f(x_{2})\|}{\|x_{1}-x_{2}\|}<K\]

for some \(K>0\) and for all \(x_{1},x_{2}\in A,x_{1}\neq x_{2}\). Show that, if \(X_{n}=O_{P}(1)\), then \(f(X_{n})=O_{P}(1)\).

**7.18**.: Show that, if \(f_{1},\ldots,f_{n}\) are orthogonal elements of a Hilbert space \(\mathcal{H}\), then

\[\|f_{1}+\cdots+f_{n}\|^{2}=\|f_{1}\|^{2}+\cdots+\|f_{n}\|^{2}.\]

**7.19**.: Show that a finite-dimensional Hilbert space is complete using the fact that the real number system is complete. That is, if \(\{x_{n}\}\) is a sequence of numbers such that, for any \(\epsilon>0\), there exists \(n_{\epsilon}\) such that

\[|x_{n}-x_{m}|<\epsilon,\text{ for all }n,m\geq n_{\epsilon},\]

then \(x_{n}\) converges to a real number.

**7.20**.: Show that, if \(f\) and \(g\) are members of a Hilbert space \(\mathcal{H}\), then

\[\|f+g\|^{2}+\|f-g\|^{2}=2(\|f\|^{2}+\|g\|^{2}).\]

**7.21.** Let \(A\) be a \(p\times p\) symmetric and positive semidefinite matrix. Let \(G\) and \(B\) be \(p\times q\) matrices. Suppose that \(B^{T}AB\) is non-singular. Show that the following inequality holds

\[G^{T}AG\succeq(G^{T}AB)(B^{T}AB)^{-1}(B^{T}AG).\]

**7.22.** Show that Theorem 7.17 can be generalized to the case where \([G,G]\) is not invertible using the Moore-Penrose inverse. That is, if \(S\) and \(G\) are member of \(\mathcal{H}^{p}\), then

\[[S,S]\succeq[S,G][G,G]^{+}[G,S].\]

**7.23.** Prove Proposition 7.4.

**7.24.** Let \(P_{\mathcal{M}}:\mathcal{H}\rightarrow\mathcal{H}\) be the operator defined by (7.19). Show that \(P_{\mathcal{M}}\) is an idempotent and self adjoint linear operator.

**7.25.** In the setting of Example 7.7, show that \(L_{2}(P_{Y})\) is a linear subspace of \(L_{2}(P_{XY})\).

**7.26.** In the setting of Example 7.7, define \(P\) to be the operator

\[L_{2}(P_{XY})\to L_{2}(P_{XY}),\quad f\mapsto E[f(X,Y)|Y].\]

Show that \(P\) is an idempotent, and self adjoint linear operator.

## References

* [BillingsleyBillingsley1995] Billingsley, P. (1995). _Probablity and Measure_. Third Edition. Wiley.
* [BillingsleyBillingsley1999] Billingsley, P. (1999). _Convergence of Probablity Measures._ Second Edition. Wiley.
* [ConwayConway1990] Conway, J. B. (1990). _A course in functional analysis_. Second edition. Springer, New York.
* [Cramer and WoldCramer and Wold1936] Cramer, H. and Wold, H. (1936). Some theorems on distriutions functions. _Journal of the London Mathematical Society_, **s1-11**, 290-294.
* [KelleyKelley1955] Kelley, J. L. (1955). _General Topology_. D. Van Nostrand Company, Inc. Princeton.
* [Kollo and von RosenKollo and von Rosen2005] Kollo, T. and von Rosen, D. (2005). _Advanced Multivariate Statistics with Matrices_. Springer
* [Mann and WaldMann and Wald1943] Mann, H. B. and Wald, A. (1943). On stochastic limit and order relationships. _The Annals of Mathematical Statistics_, **14**, 217-226.
* [SerflingSerfling1980] Serfling, R. J. (1980). _Approximation Theorems of Mathematical Statistics_. Wiley.

## Asymptotic theory for Maximum Likelihood Estimation

The theoretical properties of the Maximum Likelihood Estimate introduced in Section 2.6 will be discussed in this chapter. This is one of the most commonly used estimators. Suppose \(X\) is a random element whose distribution belongs to a parametric family \(\{P_{\theta}:\theta\in\Theta\subseteq\mathbb{R}^{p}\}\). Intuitively, if the data \(X(\omega)=x\) is observed, then a reasonable estimate of true parameter \(\theta_{0}\) would be the \(\theta\in\Theta\) that makes the observed data \(x\) most likely to be detected, because, after all, it is \(x\), and not some other values \(x^{\prime}\) of \(X\), that has occurred. Thus, the Maximum Likelihood Estimate seems to be derived from the following dictum: "only the most likely to occur, occurs". This is, of course, not true in a literal sense: sometimes the least likely does occur. However, as a general tendency this seems plausible. Indeed, without any prior knowledge about \(\theta\), we seem to have no reason to think otherwise. In this chapter we systematically develop the theoretical properties for the Maximum Likelihood Estimate: its consistency, its asymptotic normality, and its optimality.

### 8.1 Maximum Likelihood Estimation

Let \(X_{1:n}=(X_{1},\ldots,X_{n})\) be a sample of random vectors of dimension \(m\) that take values in a measurable space \((\Omega_{n},\mathcal{F}_{n})\), with a joint distribution belonging to a parametric family, say \(\{P_{\theta}:\theta\in\Theta\subseteq\mathbb{R}^{p}\}\). Suppose that \(P_{\theta}\) is dominated by a \(\sigma\)-finite measure \(\mu\), and let \(f_{\theta}=dP_{\theta}/d\mu\) be the density of \(X_{1:n}\).

**Definition 8.1**: _Suppose that, for each \(x_{1:n}\in\Omega_{n}\), \(\sup_{\theta\in\Theta}f_{\theta}(x_{1:n})\) can be reached within \(\Theta\). Then the Maximum Likelihood Estimate is defined as_

\[\hat{\theta}_{n}=\operatorname*{argmax}_{\theta\in\Theta}\left[f_{\theta}(X_{1 :n})\right]=\operatorname*{argmax}_{\theta\in\Theta}\left[\log f_{\theta}(X_{1 :n})\right].\]

In the above definition, the two argmax are the same because logarithm is a strictly increasing function, and strictly increasing transformations do not affect the maximizer of a function. Taking logarithm brings great convenience,as it transforms a product into a sum, to which the Law of Large Numbers and the Central Limit Theorem can be applied. The function \(\theta\mapsto f_{\theta}(X_{1:n})\) is called the likelihood; the function \(\theta\mapsto\log f_{\theta}(X_{1:n})\) is called the log likelihood.

Because the MLE \(\hat{\theta}\) is the maximizer of the log likelihood, it satisfies the equation

\[\partial\log f_{\theta}(X_{1:n})/\partial\theta=0, \tag{8.1}\]

provided that the function \(\theta\mapsto f_{\theta}(X_{1:n})\) is differentiable. This equation is called the likelihood equation. The function on the left is called the score function, and will be denoted by

\[s(\theta,X_{1:n})=\partial\log f_{\theta}(X_{1:n})/\partial\theta.\]

Sometimes equation (8.1) is also called the score equation. The score function has some interesting properties, as described in the next proposition.

**Proposition 8.1**: _If \(f_{\theta}(x_{1:n})\) and \(s(\theta,x_{1:n})f_{\theta}(x_{1:n})\) satisfy \(\text{DUI}^{+}(\theta,\mu)\), then for all \(\theta\in\Theta\),_

\[E_{\theta}\left[s(\theta,X_{1:n})\right]=0, \tag{8.2}\] \[E_{\theta}\left[s(\theta,X_{1:n})s(\theta,X_{1:n})^{T}\right]=-E _{\theta}\left[\partial s(\theta,X_{1:n})/\partial\theta^{T}\right]. \tag{8.3}\]

Proof: Because \(f_{\theta}\) is the density of \(X_{1:n}\), we have

\[\int_{\Omega_{n}}f_{\theta}d\mu=1.\]

Differentiating both sides of the equation and evoking the first \(\text{DUI}^{+}\) condition, we have

\[\int_{\Omega_{n}}\frac{\partial f_{\theta}}{\partial\theta}\;d\mu=0. \tag{8.4}\]

Because \(f_{\theta}(x_{1:n})>0\) on \(\Omega_{n}\), we have

\[\frac{\partial f_{\theta}(x_{1:n})}{\partial\theta}=\frac{\partial\log f_{ \theta}(x_{1:n})}{\partial\theta}f_{\theta}(x_{1:n})=s(\theta,x_{1:n})f_{ \theta}(x_{1:n}). \tag{8.5}\]

Hence the left-hand side of (8.4) is simply \(E_{\theta}[s(\theta,X_{1:n})]\). This proves the first identity.

Differentiating the equation

\[\int_{\Omega_{n}}s(\theta,x_{1:n})f_{\theta}(x_{1:n})d\mu(x_{1:n})=0\]

with respect to \(\theta^{T}\) and evoking the second \(\text{DUI}^{+}\) condition, we have

\[\int_{\Omega_{n}}\frac{\partial s(\theta,x_{1:n})}{\partial\theta^{T}}f_{ \theta}(x_{1:n})d\mu(x_{1:n})+\int_{\Omega_{n}}s(\theta,x_{1:n})\frac{f_{ \theta}(x_{1:n})}{\partial\theta^{T}}d\mu(x_{1:n})=0.\]The first term on the left-hand side is simply \(E_{\theta}[\partial s(\theta,X_{1:n})/\partial\theta^{T}]\). The second term, by relation (8.5) again, can be rewritten as

\[E_{\theta}[s(\theta,X_{1:n})s(\theta,X_{1:n})^{T}].\]

\(\Box\)

The matrix on the left-hand side of (8.3) is called the Fisher information contained in \(X_{1:n}\), denoted by \(I_{1:n}(\theta)\); the identity (8.3) is called the information identity. The relation (8.2) is known as the unbiasedness of the score (see for example, Godambe, 1960).

We will focus on the case where \(X_{1},\ldots,X_{n}\) are i.i.d. with a density function \(h_{\theta}(x)\), where \(\theta\in\Theta\subseteq\mathbb{R}^{p}\). In this case,

\[f_{\theta}(X_{1:n})=\prod_{i=1}^{n}h_{\theta}(X_{i}).\]

The log-likelihood is

\[\log f_{\theta}(X_{1:n})=\sum_{i=1}^{n}\log h_{\theta}(X_{i})\propto E_{n}[ \log h_{\theta}(X)].\]

Obviously, for the purpose of maximizing \(\log f_{\theta}(X_{1:n})\), it is equivalent to use the sum version \(\sum_{i=1}^{n}\log h_{\theta}(X_{i})\) or average version \(E_{n}\log h_{\theta}(X)\), as they only differ by a proportionality constant \(n^{-1}\). As we will see, the latter is in many ways more convenient to use than the former.

By an abuse of notation, let \(s(\theta,X_{i})\) be the score for a single observation; that is, \(s(\theta,X_{i})=\partial h_{\theta}(X_{i})/\partial\theta\). Then Proposition 8.1 holds for a single observation as well. That is,

\[E_{\theta}\left[s(\theta,X_{i})=0,\right.\] \[\left.E_{\theta}\left[s(\theta,X_{i})s(\theta,X_{i})^{T}\right] =-\left[\partial s(\theta,X_{i})/\partial\theta^{T}\right].\right.\]

The matrix \(E_{\theta}\left[s(\theta,X_{i})s(\theta,X_{i})^{T}\right]\) is called the Fisher information contained in a single observation, denoted by \(I(\theta)\). It can be easily shown that

\[I_{1:n}(\theta)=nI(\theta),\quad s(\theta,X_{1:n})=\sum_{i=1}^{n}s(\theta,X_{i })=nE_{n}[s(\theta,X)].\]

The assumption in Proposition 8.1 that the support of \(P_{\theta}\) does not depend on \(\theta\) is quite important. Without it, even the definition of score function is problematic. For example, consider the density \(h_{\theta}(x)\) for the uniform distribution on \((0,\theta)\):

\[h_{\theta}(x)=\theta^{-1}I(x<\theta).\]This function is not differentiable at \(\theta=x\). The log likelihood is only defined for \(\theta>x\). If we define the derivatives of \(\log h_{\theta}(x)\) only in the region \(\theta>x\), then

\[\partial\log h_{\theta}(x)/\partial\theta=-\theta^{-1},\ \partial^{2}\log h_{ \theta}(x)/\partial\theta^{2}=\theta^{-2},\ [\partial\log h_{\theta}(x)/\partial \theta]^{2}=\theta^{-2}.\]

Thus none of the identities in Proposition 8.1 is satisfied.

Even in the case where the support of \(P_{\theta}\) does not depend on \(\theta\), it is still possible that the rest of the conditions in Proposition 8.1 are violated -- see Problem 8.3. Nevertheless, conditions such as those in Proposition 8.1 are satisfied for a wide variety of commonly encountered statistical problems, and are often evoked in statistical inference.

### Cramer's approach to consistency

In Section 2.6 we proved the Fisher consistency of the MLE. There are two other types of consistency, strong consistency and consistency, which we define below. Let \(T=T(X_{1:n})\) be a statistic.

**Definition 8.2**: _A statistic \(T(X_{1:n})\) is a weakly consistent estimate (or simply consistent estimate) of the parameter \(\theta_{0}\) if, under \(P_{\theta_{0}}\), \(T(X_{1:n})\stackrel{{ P}}{{\rightarrow}}\theta_{0}\). It is a strongly consistent estimate of \(\theta_{0}\) if, under \(P_{\theta_{0}}\), \(T(X_{1:n})\rightarrow\theta_{0}\) almost everywhere._

It turns out that the MLE is consistent in both senses, under different sets of conditions. The methods for proving these two types of consistency are also quite different. The weak consistency was proved in Cramer (1946); the strong consistency was proved by Wald (1949). Each result reveals a different nature of the MLE: the first one reveals the property of the score function; the second reveals the property of the likelihood function. Both approaches are used widely in statistical research. In this section we focus on Cramer's approach.

Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. with the common density belonging to a parametric family \(\{f_{\theta}(x):\theta\in\Theta\subseteq\mathbb{R}^{p}\}\). As before, let \(s(\theta,x)\) be the score of a single observation. Cramer's statement of consistency does not directly state "the MLE is consistent". Rather, it states, roughly, that there is a sequence of consistent solutions to the likelihood equation. The existence part of the statement is established by a fixed point theorem (see Conway, 1990), which is stated below.

**Proposition 8.2** (Brouwer's Fixed Point Theorem): _Let \(B\) be a closed unit ball in \(\mathbb{R}^{p}\). Suppose that \(h:B\mapsto B\) is a continuous function. Then there is an \(x\in B\) such that \(h(x)=x\)_

The next theorem is Cramer's version of consistency of the MLE.

[MISSING_PAGE_FAIL:251]

where \(\|\cdot\|\) denotes the Euclidean norm.

By condition 1 and Proposition 8.1 (as applied to a single observation), the first term on the right-hand side is \(0\), and the second term on the right-hand side is \(-(\theta-\theta_{0})^{T}I(\theta_{0})(\theta-\theta_{0})\). Hence,

\[(\theta-\theta_{0})^{T}E[s(\theta,X)]=-(\theta-\theta_{0})^{T}I(\theta_{0})( \theta-\theta_{0})+o(\|\theta-\theta_{0}\|^{2}). \tag{8.6}\]

By condition 3, \(I(\theta_{0})\) is a positive definite matrix. Consequently for sufficiently small \(\epsilon>0\), \((\theta-\theta_{0})^{T}E[s(\theta,X)]<0\) whenever \(\|\theta-\theta_{0}\|\leq\epsilon\). Because \((\theta-\theta_{0})^{T}E[s(\theta,X)]\) is continuous on the compact set \(B(\theta_{0},\epsilon)\), it attains its maximum within the closed ball. Hence for some \(\delta>0\),

\[-\delta>\sup_{\theta\in B(\theta_{0},\epsilon)}\{(\theta-\theta_{0})^{T}Es( \theta,X)\}\geq\sup_{\theta\in\partial B(\theta_{0},\epsilon)}\{(\theta- \theta_{0})^{T}Es(\theta,X)\} \tag{8.7}\]

where \(\partial B(\theta_{0},\epsilon)\) is the boundary \(\{\theta:\|\theta-\theta_{0}\|=\epsilon\}\).

Next, note that

\[\sup_{\theta\in\partial B(\theta_{0},\epsilon)}\{(\theta-\theta_{ 0})^{T}E_{n}s(\theta,X)\}\leq\sup_{\theta\in\partial B(\theta_{0},\epsilon)} \{(\theta-\theta_{0})^{T}Es(\theta,X)\}\\ +\sup_{\theta\in\partial B(\theta_{0},\epsilon)}\left\{(\theta- \theta_{0})^{T}\left[E_{n}s(\theta,X)-Es(\theta,X)\right]\right\}.\]

By (8.7), the first term on the right is smaller than \(-\delta\). By Cauchy-Schwarz inequality, the second term is no greater than

\[\sup_{\theta\in\partial B(\theta_{0},\epsilon)}\left\{\|\theta- \theta_{0}\|\left\|E_{n}s(\theta,X)-Es(\theta,X)\right\|\right\}\\ \leq\epsilon\ \sup_{\theta\in\partial B(\theta_{0},\epsilon)}\left\|E_ {n}s(\theta,X)-Es(\theta,X)\right\|,\]

where, by condition 4, the right-hand side converges to \(0\) in probability. Therefore,

\[P(B_{n})\to 1,\ \text{where}\ \ B_{n}=\left\{\sup_{\theta\in\partial B( \theta_{0},\epsilon)}\{(\theta-\theta_{0})E_{n}[s(\theta,X)]\}\leq 0\right\}. \tag{8.8}\]

Let \(A_{n}=\{\omega:R_{n}\cap B(\theta_{0},\epsilon)\neq\varnothing\}\). Then, on the event \(A_{n}^{c}\), \(E_{n}[s(\theta,X)]\)=0 has no solution on \(B(\theta_{0},\epsilon)\). In other words, \(E_{n}[s(\theta,X)]\neq 0\) for all \(\theta\in B(\theta_{0},\epsilon)\). Consequently the mapping \(h\) defined on the unit ball by

\[h(\eta)=E_{n}[s(\theta_{0}+\epsilon\eta,X)]/\|E_{n}s(\theta_{0}+\epsilon\eta,X )\|\]

is continuous. By Brouwer's Fixed Point Theorem, there is an \(\eta^{*}\) such that \(\|\eta^{*}\|\leq 1\) and \(h(\eta^{*})=\eta^{*}\). That is,

\[E_{n}s(\theta_{0}+\epsilon\eta^{*},X)/\|E_{n}s(\theta_{0}+\epsilon\eta^{*},X )\|=\eta^{*}. \tag{8.9}\]From this equality we also see that \(\|\eta^{*}\|=1\), which implies \({\eta^{*}}^{T}h(\eta^{*})={\eta^{*}}^{T}\eta^{*}=1\).

Now let \(\theta^{*}=\theta_{0}+\epsilon\eta^{*}\). Then \(\theta^{*}\in\partial B(\theta_{0},\epsilon)\) and

\[(\theta^{*}-\theta_{0})^{T}E_{n}s(\theta^{*},X)=\epsilon{\eta^{*}}^{T}E_{n}s( \theta^{*},X)=\epsilon\|E_{n}s(\theta_{0}+\epsilon{\eta^{*}},X)\|>0,\]

where, for the second equality, we have used the relation (8.9). Hence, on \(A_{n}^{c}\),

\[\sup_{\theta\in\partial B(\theta_{0},\epsilon)}\{(\theta-\theta_{0})^{T}E_{n}s (\theta,X)\}>0.\]

Consequently, \(B_{n}\subseteq A_{n}\). So by (8.8), \(P(A_{n})\to 1\). But since

\[R_{n}\cap B(\theta_{0},\epsilon)\neq\varnothing\,\Rightarrow\,R_{n}\neq \varnothing\;\;\text{and}\;\;\|\hat{\theta}_{0,n}-\theta_{0}\|\leq\epsilon,\]

we have

\[P(E_{n}[s(\hat{\theta}_{n},X)]=0)\to 1,\quad P(\|\hat{\theta}_{n}-\theta_{0}\| \leq\epsilon)\to 0.\]

\(\Box\)

Cramer's consistency result does not guarantee any specific solution to be consistent when there are multiple solutions. It merely asserts that consistent solution or solutions exist with probability tending to 1. Nevertheless, if the likelihood equation only has one solution, then Cramer's consistency statement can guarantee that solution to be consistent.

The sufficient conditions for the uniform convergence condition 4 will be further discussed in the next section. In the special case where \(p=1\), the uniform convergence is unnecessary, because the boundary of the closed ball \(B(\theta_{0},\epsilon)\) is simply the set of two points \(\{\theta_{0}-\epsilon,\theta_{0}+\epsilon\}\). The convergence of \(E_{n}[s(\theta,X)]\) to \(E[s(\theta,X)]\) is guaranteed by the law of large numbers. In the next example we verify the sufficient conditions of Theorem 8.1 for \(p=1\) in a Poisson regression problem.

**Example 8.1**: Suppose \(X\) is a random variable with density \(f_{X}\), and the conditional distribution \(Y\) given \(X=x\) is Poisson(\(e^{\theta x}\)). For simplicity, we assume \(\Theta=\mathbb{R}\), and \(\Omega_{X}=\mathbb{R}\). Suppose, for all \(\theta\in\Theta\),

\[0<\int_{-\infty}^{\infty}x^{2}e^{\theta x}f_{X}(x)dx<\infty. \tag{8.10}\]

The goal here is to verify the first three conditions in Theorem 8.1.

Since the joint density of \((X,Y)\) is

\[f_{\theta}(x,y)=(e^{\theta xy}/y!)\,e^{-e^{\theta x}}f_{X}(x),\]

the log likelihood function is \(\log f_{\theta}(x,y)=\theta xy-e^{\theta x}+\text{constant}\), and the score function for a single observation is \[s(\theta,x,y)=x(y-e^{\theta x}). \tag{8.11}\]

Conditions 1 and 2 can now be verified by straighforward calculations. Condition 3 is simply assumption (8.10). \(\Box\)

The Cramer's type consistency for all generalized linear models (McCullagh and Nelder, 1989) can be verified in a similar way, where the predictor \(X\) can be a vector. Here, we have made the simplifying assumption that the predictor \(X\) is random. This is not an unreasonable assumption since the estimation is based on the conditional distribution of \(Y|X\), and the marginal density \(f_{X}\) plays no role. The proof of the case where \(X_{1},\ldots,X_{n}\) are fixed can be carried out in the same spirit, but requires more careful treatment of details.

### Almost everywhere uniform convergence

In this section we further explore the condition in Theorem 8.1 that requires \(E_{n}[s(\theta,X)]\) to converge to \(Es[(\theta,X)]\) uniformly over the boundary set \(\partial B(\theta_{0},\epsilon)\), that is

\[\sup_{\theta\in\partial B(\theta_{0},\epsilon)}|E_{n}s(\theta,X)-E_{n}s(\theta,X)|\stackrel{{ P}}{{\longrightarrow}}0.\]

For a set \(\mathcal{F}\) of integrable functions, we are interested in whether the convergence

\[\sup_{f\in\mathcal{F}}|E_{n}f(X)-Ef(X)|\to 0\quad[P], \tag{8.12}\]

holds. In the case of Theorem 8.1, the set of functions is \(\{s(\theta,X):\theta\in\partial B(\theta_{0},\epsilon)\}\). This type of uniform convergence is also important for the Wald-type consistency that will be discussed in Section 8.4.

If \(\mathcal{F}\) consists of a single function, then convergence (8.12) reduces to the strong law of large numbers. But if \(\mathcal{F}\) contains too many functions, then uniform convergence over \(\mathcal{F}\) will not hold. Then, what is the "appropriate size" for \(\mathcal{F}\) to ensure uniform convergence? To answer this question we first have to define the "size of a family of functions". Let \(\mathcal{S}\) denote the class of functions \(f\) on \(\Omega_{X}\) such that \(\|f\|_{1}=E(|f(X)|)<\infty\).

**Definition 8.3**: _Given two members \(\ell,u\) of \(\mathcal{S}\), the bracket \([\ell,u]\) is the set_

\[\{f\in\mathcal{S}:\,\ell(x)\leq f(x)\leq u(x)\text{ for all }x\in\Omega_{X}\}.\]

The next theorem provides sufficient conditions under which a class \(\mathcal{F}\) of functions satisfies (8.12).

**Theorem 8.2** (Glivenko-Cantelli Theorem): _Let \(X_{1},X_{2},\ldots\) be an i.i.d. sequence with distribution \(P\). Suppose, for every \(\epsilon>0\), there exists an integer \(m_{\epsilon}\geq 1\) and a set of functions \(\{g_{ij}\in\mathcal{S}:\,i=1,\ldots,m_{\epsilon},\ j=1,2\}\) such that \(\|g_{i2}-g_{i1}\|_{1}<\epsilon\) and \(\mathcal{F}\subset\cup_{i=1}^{m_{\epsilon}}[g_{i1},g_{i2}]\). Then_

\[\sup_{f\in\mathcal{F}}|E_{n}f(X)-Ef(X)|\to 0\quad[P].\]

Proof: If \(f\in[\ell,u]\) and \(\|u-\ell\|<\epsilon\), then

\[E_{n}\ell(X)\leq E_{n}f(X)\leq E_{n}u(X),\quad E\ell(X)\leq Ef(X)\leq Eu(X).\]

It follows that

\[E_{n}f(X)-Ef(X) \leq E_{n}u(X)-E\ell(X)\] \[\leq(E_{n}u(X)-Eu(X))+(Eu(X)-E\ell(X))\] \[\leq(E_{n}u(X)-Eu(X))+\epsilon,\] \[E_{n}f(X)-Ef(X) \geq E_{n}\ell(X)-Eu(X)\] \[\geq(E_{n}\ell(X)-E\ell(X))-(Eu(X)-E\ell(X))\] \[\geq(E_{n}\ell(X)-E\ell(X))-\epsilon.\]

Hence

\[|E_{n}f(X)-Ef(X)|\leq\max\left\{\left|E_{n}\ell(X)-E\ell(X)\right|,\left|E_{n}u (X)-Eu(X)\right|\right\}+\epsilon.\]

Therefore,

\[\sup_{f\in\mathcal{F}}|E_{n}f(X)-Ef(X)|\leq\max_{i=1,\ldots,m_{\epsilon};\,j=1, 2}\left|E_{n}g_{ij}(X)-Eg_{ji}(X)\right|+\epsilon.\]

By the strong law of large numbers each term inside the maximum on the right converges to \(0\) almost everywhere. By Problem 8.6, the maximum itself converges to \(0\) almost everywhere. It follows that, for each \(\epsilon>0\),

\[P\left(\limsup_{n\to\infty}\sup_{f\in\mathcal{F}}|E_{n}f(X)-Ef(X)|\leq\epsilon \right)=1.\]

By Problem 8.7, the above implies

\[P\left(\limsup_{n\to\infty}\sup_{f\in\mathcal{F}}|E_{n}f(X)-Ef(X)|=0\right)=1.\]

\(\Box\)

Thus, a sufficient condition for almost everywhere uniform convergence over a class \(\mathcal{F}\) is that for each \(\epsilon>0\), \(\mathcal{F}\) is covered by finite number of \([\ell,u]\) with \(\|\ell-u\|<\epsilon\). The next proposition gives a sufficient condition for this.

**Proposition 8.3**: _Let \(\mathcal{F}=\{g(\theta,x):\theta\in A\}\). Suppose that \(A\) is a compact set in \(\mathbb{R}^{p}\), that \(g(\theta,x)\) is continuous in \(\theta\) for every \(x\), and that, for all \(\theta\in A\), \(|g(\theta,x)|\) is dominated by an integrable function \(G(x)\). Then, for any \(\epsilon>0\), there exists an integer \(m_{\epsilon}\geq 1\) and a set of functions \(\{g_{ij}\in\mathcal{S}:i=1,\ldots,m_{\epsilon},j=1,2\}\) such that \(\|g_{i2}-g_{i1}\|_{1}<\epsilon\) and \(\mathcal{F}\subseteq\cup_{i=1}^{m_{\epsilon}}[g_{i1},g_{i2}]\)._

Proof: Let \(\epsilon>0\). For any \(\theta\in A\), let \(B^{\circ}(\theta,\delta)\) be the open ball centered at \(\theta\) with radius \(\delta\); that is, \(B^{\circ}(\theta,\delta)=\{\theta^{\prime}:\|\theta^{\prime}-\theta\|<\delta\}\). Let

\[u(\theta,x,\delta)=\sup_{\theta^{\prime}\in B^{\circ}(\theta,\delta)}g(\theta ^{\prime},x)\ \ \text{and}\ \ \ell(\theta,x,\delta)=\inf_{\theta^{\prime}\in B^ {\circ}(\theta,\delta)}g(\theta^{\prime},x).\]

Because for each \(x\), \(g(\theta,x)\) is continuous in \(\theta\), for any \(\epsilon>0\) there is a \(\delta>0\) such that \(\theta^{\prime}\in B^{\circ}(\theta,\delta)\Rightarrow|g(\theta^{\prime},x)-g( \theta,x)|<\epsilon\). Hence, for such a \(\theta^{\prime}\),

\[g(\theta^{\prime},x)= g(\theta,x)+g(\theta^{\prime},x)-g(\theta,x)\] \[\leq g(\theta,x)+\sup_{\theta^{\prime}\in B^{\circ}(\theta,\delta)}|g (\theta^{\prime},x)-g(\theta,x)|\] \[\leq g(\theta,x)+\epsilon.\]

Taking supremum, we have

\[u(\theta,x,\delta)=\sup_{\theta^{\prime}\in B^{\circ}(\theta,\delta)}g(\theta^ {\prime},x)\leq g(\theta,x)+\epsilon.\]

Therefore \(\limsup_{\delta\to 0}u(\theta,x,\delta)\ \leq\ g(\theta,x)\). Similarly, \(\liminf_{\delta\to 0}\ell(\theta,x,\delta)\ \geq g(\theta,x)\). Thus we have shown that

\[\lim_{\delta\to 0}u(\theta,x,\delta)=\lim_{\delta\to 0}\ell(\theta,x,\delta)=g( \theta,x).\]

We next use this result to construct a finite collection of \([\ell,u]\) to cover \(\mathcal{F}\). Note that

\[E\left\{u(\theta,X,\delta)-\ell(\theta,X,\delta)\right\}\] \[=E\left\{u(\theta,X,\delta)-g(\theta,X)+E\left\{g(\theta,X)-\ell( \theta,X,\delta)\right\}.\right.\]

By assumption \(g(\theta,x)\) is dominated by \(G(x)\), and hence \(|u(\theta,x,\delta)|\) and \(|\ell(\theta,x,\delta)|\) are both dominated by \(G(x)\). By the Dominated Convergence Theorem, both of the two terms on the right-hand side converge to \(0\) as \(\delta\to 0\). Consequently,

\[\lim_{\delta\to 0}E\left\{u(\theta,X,\delta)-\ell(\theta,X,\delta)\right\}=0.\]

Hence, for any \(\epsilon>0\), there is a \(\delta_{\theta}>0\) (which may depend on \(\theta\)), \(\ell_{\theta},u_{\theta}\) such that \(\|\ell_{\theta}-u_{\theta}\|_{1}<\epsilon\), where \(\ell_{\theta}(x)=\ell(\theta,x,\delta_{\theta})\), and \(u_{\theta}(x)=u(\theta,x,\delta_{\theta})\). Now consider the class of open balls\[\mathcal{O}=\left\{B^{\circ}(\theta,\delta_{\theta}):\theta\in A\right\}.\]

This is an open cover of \(A\). Because \(A\) is compact, there is a finite subcover, say, \(\{B^{\circ}(\theta_{i},\delta_{\theta_{i}}):i=1,\ldots,m\}\) of \(A\). Then, the collection of \(\{[\ell_{\theta_{i}},u_{\theta_{i}}]:i=1,\ldots,m\}\) must cover \(\mathcal{F}\) because, if \(f\in\mathcal{F}\), then \(f=f(\theta,x)\) for some \(\theta\in A\), which must belong to one of the open balls, say \(\theta\in B^{\circ}(\theta_{i},\delta_{\theta_{i}})\). Then,

\[\ell(\theta_{i},x,\delta_{\theta_{i}})\leq\sup_{\theta^{\prime}\in B^{\circ}( \hat{\theta}_{i},\delta_{\theta_{i}})}f(\theta^{\prime},x)\leq f(\theta,x)\leq \sup_{\theta^{\prime}\in B^{\circ}(\hat{\theta}_{i},\delta_{\theta_{i}})}f( \theta^{\prime},x)=u(\theta_{i},x,\delta_{\theta_{i}}).\]

Thus \(f\) must be in the bracket \([\ell_{\theta_{i}},u_{\theta_{i}}]\). 

### Wald's approach to consistency

The Wald approach to consistency (Wald, 1949) is quite different from Cramer's approach in that it relies on the properties of the log likelihood, rather than the score function. Also, it asserts that the MLE is consistent, rather than the existence of a solution to the score equation.

The structure of the proof is similar to those given in Wong (1986) and van der Vaart (1998). The intuition is the following. Suppose again \(X_{1},\ldots,X_{n}\) are an i.i.d. sample from a density \(f_{\theta}(X)\). The log likelihood is proportional to \(E_{n}\log f_{\theta}(X)\). As shown in Theorem 2.1, if the family \(\{P_{\theta}:\theta\in\Theta\}\) is identifiable, then \(E[\log f_{\theta}(X)]\) is uniquely maximized at the true parameter \(\theta_{0}\). If

\[\sup_{\theta\in\Theta}|E_{n}\log f_{\theta}(X)-E\log f_{\theta}(X)|\to 0\quad[P]\]

then we would expect that the maximizer \(\hat{\theta}_{n}\) of \(E_{n}[\log f_{\theta}(X)]\) to be close to \(\theta_{0}\), which is the maximizer of \(E[\log f_{\theta}(X)]\). Let \(R_{n}(\theta)=E_{n}[\log f_{\theta}(X)]\) and \(R(\theta)=E[\log f_{\theta}(X)]\). The next theorem makes the above intuition rigorous.

**Theorem 8.3**: _Suppose \(R\) has a unique maximizer \(\theta_{0}\) and \(R_{n}\) has a unique maximizer \(\hat{\theta}_{n}\in\Theta\) modulo \(P\). Suppose, furthermore,_

1. \(\lim_{n\to\infty}\sup_{\theta\in\Theta}|R_{n}(\theta)-R(\theta)|=0\quad[P]\)_;_
2. _For every_ \(\epsilon>0\)_,_ \(\sup_{\|\theta-\theta_{0}\|>\epsilon}R(\theta)<R(\theta_{0})\)_._

_Then \(\hat{\theta}_{n}\to\theta_{0}\)\([P]\)._

Assumption \(1\) says that \(R_{n}\) converges uniformly to \(R\) almost everywhere, which would be true if \(\Theta\) is a compact set and \(\log f_{\theta}(X)\) satisfies the conditions in Proposition 8.3. Condition \(2\) is called "\(R(\theta)\) has a well-separated maximum", which rules out the situations where, although \(R(\theta)\) is less than \(R(\theta_{0})\), the former can get arbitrarily close to the latter (van der Vaart, 1998).

Proof of Theorem8.3.: Since a probability \(0\) set does not affect our argument, we can assume \(R_{n}\) has a unique maximizer without loss of generality. We first show \(R(\hat{\theta}_{n})\to R(\theta_{0})\)\([P]\). Since \(R(\hat{\theta}_{n})\leq R(\theta_{0})\), it suffices to show that

\[\liminf_{n\to\infty}R(\hat{\theta}_{n})-R(\theta_{0})\geq 0\quad[P].\]

Since

\[R(\hat{\theta}_{n})-R(\theta_{0})=R(\hat{\theta}_{n})-R_{n}(\hat{\theta}_{n})+R _{n}(\hat{\theta}_{n})-R_{n}(\theta_{0})+R_{n}(\theta_{0})-R(\theta_{0}),\]

we have

\[\liminf_{n\to\infty}[R(\hat{\theta}_{n})-R(\theta_{0})]\geq\liminf _{n\to\infty}[R(\hat{\theta}_{n})-R_{n}(\hat{\theta}_{n})]\\ +\liminf_{n\to\infty}[R_{n}(\hat{\theta}_{n})-R_{n}(\theta_{0})] +\liminf_{n\to\infty}[R_{n}(\theta_{0})-R(\theta_{0})].\]

By condition \(1\), the first term and last term on the right-hand side are \(0\). Therefore,

\[\liminf_{n\to\infty}[R(\hat{\theta}_{n})-R(\theta_{0})]\geq\liminf_{n\to\infty} [R_{n}(\hat{\theta}_{n})-R_{n}(\theta_{0})],\]

where the right-hand side is nonnegative because \(\hat{\theta}_{n}\) is the maximizer of \(R_{n}(\theta)\).

Next, we show the desired relation

\[P(\lim_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|=0)=P(\limsup_{n\to\infty}\| \hat{\theta}_{n}-\theta_{0}\|=0)=1.\]

By Problem8.7, it suffices to show that, for any \(\epsilon>0\),

\[P(\limsup_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|<\epsilon)=1. \tag{8.13}\]

If \(\limsup_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|\geq\epsilon\), then there a subsequence \(\{\hat{\theta}_{n_{k}}:k=1,2,\ldots\}\) such that \(\|\hat{\theta}_{n_{k}}-\theta_{0}\|>\epsilon/2\). By condition \(2\), there is a \(\delta>0\) such that

\[\sup_{\|\theta-\theta_{0}\|>\epsilon/2}R(\theta)\leq R(\theta_{0})-\delta.\]

Thus, along the subsequence \(\{\hat{\theta}_{n_{k}}:k=1,2,\ldots\}\), \(R(\hat{\theta}_{n_{k}})\leq R(\theta_{0})-\delta\), making it impossible for \(R(\hat{\theta}_{n})\) to converge to \(R(\theta_{0})\). Hence

\[P\left(\limsup_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|\geq\epsilon\right) \leq P\left(\lim_{n\to\infty}R(\hat{\theta}_{n})\neq R(\theta_{0})\right),\]

where the right-hand side is \(0\) because we already established

\[R(\hat{\theta}_{n})\to R(\theta_{0})\ [P].\]

This proves (8.13).

With slightly more effort the uniform convergence condition (condition _1_) in the above Theorem can be relaxed. Instead of assuming \(R_{n}(\theta)\) converges uniformly to \(R(\theta)\) over \(\Theta\), we can assume \(R_{n}(\theta)\) converges uniformly to \(R(\theta)\) over a subset \(K\) of \(\Theta\), and assume the maximizer of \(R_{n}(\theta)\) is always in \(K\). This condition is known as essential compactness (Wong, 1986), and would be satisfied if, for example, \(R_{n}(\theta)\) concave.

**Corollary 8.1**: _Suppose \(R\) has a unique maximizer \(\theta_{0}\) and, with probability 1, \(R_{n}\) has a unique maximizer \(\hat{\theta}_{n}\). Suppose, furthermore,_

1. _there is a subset_ \(K\subseteq\Theta\)_, whose interior contains_ \(\theta_{0}\)_, such that_ \[\sup_{\theta\in K}|R_{n}(\theta)-R(\theta)|\to 0\quad[P];\]
2. _with probability 1,_ \(R_{n}\) _has a unique maximizer_ \(\tilde{\theta}_{n}\) _over_ \(K\)_;_
3. \(P\left(\liminf_{n\to\infty}\left\{\sup_{\theta^{\prime}\notin K}R_{n}(\theta)< \sup_{\theta\in\Theta}R_{n}(\theta)\right\}\right)=1;\)__
4. _For every_ \(\epsilon>0\)_,_ \[\sup\left\{R(\theta):\|\theta-\theta_{0}\|\geq\epsilon,\theta\in K\right\}<R( \theta_{0}).\]

_Then \(\hat{\theta}_{n}\to\theta_{0}\quad[P]\)._

_Proof._ Again, we can assume, without loss of generality, that \(R_{n}\) has a unique maximizer \(\hat{\theta}_{n}\) over \(\Theta\) and a unique maximizer \(\tilde{\theta}_{n}\) over \(K\). By condition \(3\),

\[P\left(\lim_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|=0\right)\] \[=P\left(\left\{\lim_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|=0 \right\}\cap\liminf_{n\to\infty}\left\{\sup_{\theta^{\prime}\notin K}R_{n}( \theta)<\sup_{\theta\in\Theta}R_{n}(\theta)\right\}\right).\]

The event

\[\liminf_{n\to\infty}\left\{\sup_{\theta^{\prime}\notin K}R_{n}(\theta)<\sup_{ \theta\in\Theta}R_{n}(\theta)\right\}\]

happens if and only if, for all sufficiently large \(n\), \(\hat{\theta}_{n}=\tilde{\theta}_{n}\). Hence

\[P\left(\lim_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|=0\right)\] \[=P\left(\lim_{n\to\infty}\|\hat{\theta}_{n}-\theta_{0}\|=0,\ \hat{\theta}_{n}=\tilde{\theta}_{n}\ \mbox{for sufficiently large $n$}\right)\] \[=P\left(\lim_{n\to\infty}\|\tilde{\theta}_{n}-\theta_{0}\|=0,\ \hat{\theta}_{n}=\tilde{\theta}_{n}\ \mbox{for sufficiently large $n$}\right)\] \[=P\left(\lim_{n\to\infty}\|\tilde{\theta}_{n}-\theta_{0}\|=0 \right).\]

From the proof of Theorem 8.3, we see that the probability on the right is 1. \(\Box\)The well-separated maximum condition (condition _1_) of Theorem 8.3 is guaranteed if \(K\) is a compact set, \(f\) is upper semi-continuous, and \(f\) has a unique maximizer over \(K\). Recall from Chapter 2 that, in the maximum likelihood estimation context, the function \(R(\theta)=E[\log f_{\theta}(X)]\) has a unique maximizer if the family \(\{f_{\theta}:\theta\in\Theta\}\) is identifiable.

The next proposition gives a set of sufficient conditions for a function \(f\) to have a well separated maximum on a compact set.

**Proposition 8.4**: _Suppose \(K\) is a compact set and \(f(\theta)\) is an upper semi-continuous function on \(K\). Suppose \(f\) has a unique maximum over \(K\) and the maximizer is an interior point of \(K\). Then \(f\) has a well-separated maximum on \(K\)._

Proof: If \(f\) is not well-separated on \(K\) then there is an \(\epsilon>0\) such that

\[\sup\{f(\theta):\|\theta-\theta_{0}\|>\epsilon,\theta\in K\}=f(\theta_{0}).\]

Then there is a sequence \(\{\theta_{n}\}\subseteq K\setminus B^{\circ}(\theta_{0},\epsilon)\) such that \(f(\theta_{n})\to f(\theta_{0})\). But because the set \(K\setminus B^{\circ}(\theta_{0},\epsilon)\) is compact this sequence has a subsequence say \(\{\theta_{n_{k}}\}\) such that \(\theta_{n_{k}}\to\theta^{*}\) with \(\theta^{*}\) in \(K\). By upper semi-continuity of \(f\) we have \(\limsup_{k\to\infty}f(\theta_{n_{k}})\leq f(\theta^{*})\). However, by the uniqueness of maximum \(f(\theta^{*})<f(\theta_{0})\), contradicting to \(f(\theta_{n})\to f(\theta_{0})\). \(\Box\)

The next proposition gives a set of sufficient conditions for essential compactness.

**Proposition 8.5**: _Suppose that \(R\) has a unique maximuzer \(\theta_{0}\). Suppose_

1. _There is a subset_ \(K\subseteq\Theta\) _whose interior contains_ \(\theta_{0}\) _such that_ \[\lim_{n\to\infty}\sup_{\theta\in K}|R_{n}(\theta)-R(\theta)|=0\quad[P];\]
2. \(R\) _has a well-separated maximum over_ \(K\)_;_
3. \(R_{n}(\theta)\) _is concave with probability 1._

_Then,_

\[P\left(\liminf_{n\to\infty}\left\{\sup_{\theta\notin K}R_{n}(\theta)<\sup_{ \theta\in\Theta}R_{n}(\theta)\right\}\right)=1. \tag{8.14}\]

Note that, in condition \(3\), we do not need \(R_{n}\) to be strictly concave with probability 1.

Proof of Proposition 8.5.: Let \(\epsilon>0\) be such that \(B^{\circ}(\theta_{0},\epsilon)\) is contained in the interior of \(K\). By assumptions \(1\) and \(2\) there is a \(\delta>0\) such that

\[P\left(\limsup_{n\to\infty}\sup_{K\setminus B^{\circ}(\theta_{0},\epsilon)}R_{ n}(\theta)<R(\theta_{0})-\delta\right)=1.\]By condition \(1\), we have \(R_{n}(\theta_{0})\to R(\theta_{0})\)\([P]\), which implies

\[P\left(\liminf_{n\to\infty}R_{n}(\theta_{0})>R(\theta_{0})-\delta/2 \right)=1\] \[\Rightarrow \,P\left(\liminf_{n\to\infty}\sup_{B^{\circ}(\theta_{0},\epsilon)} R_{n}(\theta)>R(\theta_{0})-\delta/2\right)=1.\]

Let \(A_{n}\), \(B_{n}\) and \(C_{n}\) be the sets

\[A_{n} = \left\{\limsup_{n\to\infty}\sup_{K\setminus B^{\circ}(\theta_{0}, \epsilon)}R_{n}(\theta)<R(\theta_{0})-\delta\right\},\] \[B_{n} = \left\{\liminf_{n\to\infty}\sup_{B^{\circ}(\theta_{0},\epsilon)} R_{n}(\theta)>R(\theta_{0})-\delta/2\right\},\] \[C_{n} = \{R_{n}\text{ is concave }\}.\]

Then,

\[P(A_{n}B_{n}C_{n})=1.\]

However, \(A_{n}B_{n}C_{n}\) implies that, for sufficiently large \(n\) the supremum of \(R_{n}\) outside \(K\) cannot be greater than \(R(\theta_{0})-\delta\), which implies (8.14). \(\Box\)

Problems 8.8 through 8.11 contain step-to-step verifications of all the conditions in Theorem 8.3 in the Poisson regression setting.

### Asymptotic normality

In this section we show that, if \(\hat{\theta}\) is the MLE, then \(\sqrt{n}(\hat{\theta}-\theta_{0})\) converges in distribution to a Normal random vector. Here, we have omitted the subscript \(n\) of \(\hat{\theta}_{n}\) for simplicity. As before, let \(X_{1},\ldots,X_{n},\ldots\) be an i.i.d. sequence of random variables or vectors with probability density function \(f_{\theta}(x)\), where \(\theta\in\Theta\subseteq\mathbb{R}^{p}\). Let \(s(\theta,X)\) be the score function and \(I(\theta)\) be the Fisher information matrix as defined previously. Let \(J(\theta)\) and \(K(\theta)\) denote the matrices

\[E_{\theta}\left[\partial s(\theta,X)/\partial\theta^{T}\right],\quad E_{\theta} [s(\theta,X)s^{T}(\theta,X)]\,, \tag{8.15}\]

respectively. Although under the mild conditions in Proposition 8.1 we have \(K(\theta)=-J(\theta)=I(\theta)\), it makes the proof clearer if we use two separate symbols.

For finite-dimensional spaces, all matrix norms are equivalent so long as convergence is concerned. For definiteness, we use the Frobenius matrix norm:

\[\|A\|_{\mathrm{F}}=\sqrt{\sum_{i,j}a_{ij}^{2}}.\]

We will also use the matrix version of the rules of \(O_{P}\) and \(o_{P}\) in Theorem 7.15. In particular, we say a sequence of random matrices \(A_{n}\) is of the order \(O_{P}(a_{n})\)

[MISSING_PAGE_FAIL:262]

We now give a further consequence of stochastic equicontinuity when \(g_{n}(\theta,X_{1:n})\) takes the special form \(E_{n}[g(\theta,X)]\). It follows directly from the above proposition and the law of large numbers.

**Corollary 8.2** _If \(g(\theta,X)\) is \(P_{\theta_{0}}\)-integrable, \(\{E_{n}g(\theta,X):n\in\mathbb{N}\}\) is stochastically equicontinuous, and \(\hat{\theta}\stackrel{{ P}}{{\rightarrow}}\theta_{0}\), then \(E_{n}[g(\hat{\theta},X)]\stackrel{{ P}}{{\rightarrow}}E[g(\theta_ {0},X)]\)._

The next proposition gives a sufficient condition for stochastic equicontinuity when \(g_{n}(\theta,X_{1:n})\) takes the form \(E_{n}[g(\theta,X)]\).

**Proposition 8.7** _Suppose \(X_{1},\cdots,X_{n}\) are i.i.d. random vectors with distribution \(P_{\theta_{0}}\). If \(g(\theta,X)\) is differentiable with respect to \(\theta\), and there is a \(P_{\theta_{0}}\)-integrable function \(M(X)\) such that_

\[\sup_{\theta\in\Theta}\|\partial g(\theta,X)/\partial\theta^{T}\|_{\rm F}\leq M (X),\]

_then the sequence \(\{E_{n}[g(\theta,X)]:n\in\mathbb{N}\}\) is stochastically equicontinuous._

Proof.: By Taylor's mean value theorem, for any \(\theta_{1},\theta_{2}\in\Theta\), there is a \(\xi\) on the line joining \(\theta_{1}\) and \(\theta_{2}\) such that

\[g(\theta_{2},X)=g(\theta_{1},X)+[\partial g(\xi,X)/\partial\theta^{T}]\,( \theta_{2}-\theta_{1}).\]

Hence

\[\|g(\theta_{2},X)-g(\theta_{1},X)\|\leq \,\|[\partial g(\xi,X)/\partial\theta^{T}]\,(\theta_{2}-\theta_{1 })\|\] \[\leq \,\|\partial g(\xi,X)/\partial\theta^{T}\|_{\rm F}\,\,\|\theta_{2 }-\theta_{1}\|\] \[\leq M(X)\,\|\theta_{2}-\theta_{1}\|,\]

where the second equality follows from Problem 8.15. It follows that

\[\|E_{n}[g(\theta_{2},X)]-E_{n}[g(\theta_{1},X)]\|\leq E_{n}[M(X)]\,\|\theta_{2}-\theta_{1}\|.\]

Then, for any \(\delta>0\),

\[\sup_{\|\theta_{1}-\theta_{2}\|<\delta}\|E_{n}[g(\theta_{2},X)]-E_{n}[g(\theta _{1},X)]\|\leq E_{n}[M(X)]\,\delta.\]

Let \(\epsilon>0\) and \(\eta>0\). Because \(E_{n}[M(X)]\stackrel{{ P}}{{\rightarrow}}E[M(X)]\), \(E_{n}[M(X)]\) is bounded in probability (Problem 8.12). Hence there is a \(K>0\) such that

\[\limsup_{n\rightarrow\infty}P(E_{n}M(X)>K)<\eta.\]

Let \(\delta>0\) be so small that \(\epsilon/\delta>K\). Then\[\limsup_{n\to\infty}P\left(E_{n}[M(X)]>\epsilon/\delta\right)<\eta.\]

Hence

\[\limsup_{n\to\infty}P\left(\sup_{\|\theta_{1}-\theta_{2}\|<\delta} \|E_{n}[g(\theta_{2},X)]-E_{n}[g(\theta_{1},X)]\|>\epsilon\right)\] \[\leq\limsup_{n\to\infty}P\left(E_{n}M(X)\delta>\epsilon\right)<\eta.\]

\(\Box\)

We are now ready to derive the asymptotic distribution of the maximum likelihood estimate.

**Theorem 8.4**: _Suppose that \(\hat{\theta}\) is a consistent solution to likelihood equation \(E_{n}[s(\theta,X)]=0\) and that \(\theta_{0}\) is an interior point of \(\Theta\). Suppose, furthermore,_

1. \(f_{\theta}(x)\) _satisfies_ \(\mbox{DUI}^{+}(\theta,\mu)\)_;_
2. \(s(\theta,x)f_{\theta}(x)\) _satisfies_ \(\mbox{DUI}^{+}(\theta,\mu)\)_;_
3. _the sequence_ \(\{E_{n}[\partial s(\theta,X)/\partial\theta^{T}]:n\in\mathbb{N}\}\) _is stochastically equicontinuous over_ \(B(\theta_{0},\epsilon)\) _for some_ \(\epsilon>0\)_._

_Then \(\sqrt{n}(\hat{\theta}-\theta_{0})\stackrel{{\mathcal{D}}}{{ \longrightarrow}}N(0,I^{-1}(\theta_{0}))\), provided that \(I(\theta_{0})\) is positive definite._

Note that by the first two conditions and Proposition 8.1, the information identities (8.2) and (8.3) hold -- particularly for \(X_{1:n}=X_{1}\). The assumption that \(K(\theta_{0})=I(\theta_{0})\) is positive definite amounts to assuming the score \(s(\theta_{0},X)\) is non-degenerate under \(P_{\theta_{0}}\), which is also quite mild. Condition 3 is local in nature. Although we do not know \(\theta_{0}\), in a particular problem we can check whether this condition is satisfied for every interior point of \(\Theta\).

Proof: By Taylor's theorem,

\[0=E_{n}[s(\hat{\theta},X)]= E_{n}[s(\theta_{0},X)]+[E_{n}\dot{s}(\xi,X)](\hat{\theta}- \theta_{0}),\]

for some \(\xi\) on the line joining \(\theta_{0}\) and \(\hat{\theta}\). Because \(\xi\stackrel{{ P}}{{\to}}\theta_{0}\) and \(\{E_{n}\dot{s}(\theta,X):n=1,2,\ldots\}\) is stochastically equicontinuous, we have, by Corollary 8.2,

\[E_{n}\dot{s}(\xi,X)=J(\theta_{0})+o_{P}(1),\]

where \(o_{P}(1)\) means a matrix sequence whose Frobenius norm tends to \(0\). Hence

\[0=E_{n}[s(\theta_{0},X)]+J(\theta_{0})(\hat{\theta}-\theta_{0})+o_{P}(1)(\hat{ \theta}-\theta_{0}). \tag{8.17}\]

Since \(J(\theta_{0})\) is invertible, we have

\[(\hat{\theta}-\theta_{0})= -J(\theta_{0})^{-1}E_{n}[s(\theta_{0},X)]+J(\theta_{0})^{-1}o_{P} (1)(\hat{\theta}-\theta_{0}).\]Rearranging the above equality, we have

\[\sqrt{n}[I_{p}+o_{P}(1)](\hat{\theta}-\theta_{0})=-J^{-1}(\theta_{0})\{\sqrt{n}E_{ n}[s(\theta_{0},X)]\}.\]

By the central limit theorem,

\[\sqrt{n}E_{n}[s(\theta_{0},X)]\stackrel{{\mathcal{D}}}{{ \longrightarrow}}N(0,K(\theta_{0})),\]

where \(K(\theta_{0})=E[s(\theta_{0},X)s^{T}(\theta_{0},X)].\) Hence, by Slutsky's theorem,

\[-[J^{-1}(\theta_{0})+o_{P}(1)][\sqrt{n}E_{n}s(\theta_{0},X)]\stackrel{{ \mathcal{D}}}{{\longrightarrow}}N(0,J^{-1}(\theta_{0})K(\theta_{0})J^{-1}( \theta_{0})).\]

Since, by condition \(2\), \(K(\theta_{0})=-J(\theta_{0})=I(\theta_{0})\), the right-hand side is simply \(N(0,I^{-1}(\theta_{0}))\), leading to

\[\sqrt{n}[I_{p}+o_{P}(1)](\hat{\theta}-\theta_{0})\stackrel{{ \mathcal{D}}}{{\longrightarrow}}N(0,I^{-1}(\theta_{0})),\]

which implies \(\sqrt{n}(\hat{\theta}-\theta_{0})\stackrel{{\mathcal{D}}}{{ \longrightarrow}}N(0,I^{-1}(\theta_{0}))\) by Slutsky's Theorem (Problem 8.13). \(\Box\)

### Problems

**8.1.** Suppose that \(X\) is a random variable that takes values in \(\Omega_{X}\subseteq\mathbb{R}\) having an exponential family density (with respect to a \(\sigma\)-finite measure \(\mu\)) of the form

\[f_{\theta}(x)=\frac{e^{\theta t(x)}}{\int_{\Omega_{X}}e^{\theta t(x)}d\mu}, \quad\theta\subseteq\Theta\in\mathbb{R}. \tag{8.18}\]

1. Show that the identities in Proposition 8.1 hold.

2. Show that the Fisher information \(I(\theta)\) is of the form \(\mathrm{var}_{\theta}[t(X)]\).

3. Repeat 1 and 2 if \(\theta\) in (8.18) is replaced by a monotone increasing function of \(\psi(\theta).\) That is,

\[f_{\theta}(x)=\frac{e^{\psi(\theta)t(x)}}{\int_{\Omega_{X}}e^{\psi(\theta)t(x) }d\mu}.\]

**8.2.** Suppose that \(X\) is a random variable that takes values in \(\Omega_{X}\subseteq\mathbb{R}\) whose density with respect to a \(\sigma\)-finite measure \(\mu\) is given by (8.18), where \(\mathrm{var}_{\theta}[t(X)]>0.\)

1. Show that the function \(\mu(\theta)=E_{\theta}[t(X)]\) is strictly increasing.

2. Show that the maximum likelihood estimate is given by

\[\hat{\theta}=\mu^{-1}(E_{n}[t(X)]).\]

**8.3.** Suppose that \(X\) is a random variable in \((0,1)\) whose density is of the form

\[f_{\theta}(x)=\frac{\theta}{\theta-1}x^{-1/\theta},\quad\theta>1.\]

1. Show that \(f_{\theta}(x)\) does not satisfy \(\mathrm{DUI}^{+}(\theta,\lambda)\), \(\lambda\) being the Lebesgue measure.

2. Show that \(E_{\theta}[s(\theta,X)]\neq 0\).

**8.4.** Suppose \(X\) is a random variable whose density belongs to a parametric family \(\{f_{\theta}(X):\theta\in\Theta\subseteq\mathbb{R}^{p}\}\). Suppose the support of \(f_{\theta}\) does not depend on \(\theta\). The Kullback-Leibler divergence between \(f_{\theta}\) and \(f_{\theta_{0}}\) is defined as

\[K(\theta)=E_{\theta}\left\{\log\left[\frac{f_{\theta}(X)}{f_{\theta_{0}}(X)} \right]\right\}.\]

Derive the second-order Taylor polynomial for \(K(\theta)\) and express it in terms of the Fisher information \(I(\theta_{0})\).

**8.5.** Suppose that \(\{A_{1,n}\},\ldots,\{A_{k,n}\}\) are \(k\) sequences of events and that, for each \(i=1,\ldots,k\), \(P(A_{i,n})\to 1\) as \(n\) goes to infinity. Show that

\[\lim_{n\to\infty}P\left(\bigcap_{i=1}^{k}A_{i,n}\right)=1.\]

**8.6.** Suppose that \(\{X_{1,n}\},\ldots,\{X_{k,n}\}\) are \(k\) sequences of random variables.

1. Show that if \(X_{i,n}\stackrel{{ P}}{{\to}}0\) for each \(i=1,\ldots,k\), then

\[\max_{i=1,\ldots,k}\left(|X_{1,n}|,\ldots,|X_{k,n}|\right)\stackrel{{ P}}{{\longrightarrow}}0.\]

2. Show that if \(X_{i,n}\to 0\)\([P]\) for each \(i=1,\ldots,k\), then

\[\max_{i=1,\ldots,k}\left(|X_{1,n}|,\ldots,|X_{k,n}|\right)\to 0\;\;[P].\]

**8.7.** Let \(X\geq 0\) be a random variable. Show that if \(P(X\leq\epsilon)=1\) for all \(\epsilon>0\), then \(P(X=0)=1\).

**8.8.** Suppose \(Y\) conditioning on \(X=x\) is distributed as \(\mathrm{Poisson}(e^{\theta^{T}x})\), \(\theta\in\Theta=\mathbb{R}^{p}\), and \(X\) has a density \(h\) with respect to the Lebesgue measure. Let \(\Omega_{X}=\{x\in\mathbb{R}^{p}:f_{X}(x)>0\}\) be the support of \(X\), and suppose that \(\Omega_{X}\) contains a nonempty open set in \(\mathbb{R}^{p}\). Let \(f_{\theta}(x,y)\) be the joint density of \((X,Y)\) under \(\theta\). Prove that \(\{f_{\theta}(x,y):\theta\in\Theta\}\) is identifiable.

**8.9.** Suppose the joint distribution of \((X,Y)\) is as defined in Problem 8.8. Furthermore, suppose that

[MISSING_PAGE_EMPTY:6100]

**8.18**.: Show that the condition "\(\hat{\theta}\) is a consistent solution to \(E_{n}[s(\theta,X)]=0\)" in Theorem 8.4 can be replaced by "\(\hat{\theta}\) is consistent and with probability tending to \(1\) is a solution to \(E_{n}[s(\theta,X)]=0\)." Notice that the latter is the conclusion of the Cramer-type consistency in Theorem 8.1.

**8.19**.: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. random variables whose common distribution is \(P_{\theta_{0}}\), where \(\theta_{0}\) is an interior point of a \(p\)-dimensional parameter space \(\Theta\). Let \(\hat{\theta}\) be a \(p\)-dimensional statistic, and let \(f\) be a function from \(\Theta\times\Omega_{X}\) to \(\mathbb{R}\). Suppose

1. \(\sqrt{n}(\hat{\theta}-\theta_{0})\stackrel{{\mathcal{D}}}{{ \rightarrow}}N(0,\Sigma)\) for some positive semidefinite matrix \(\Sigma\);

2. for each \(x\in\Omega_{X}\), the function \(\theta\mapsto f(\theta,x)\) is differentiable;

3. for any \(\theta\in\Theta\), the components of \(\partial f(\theta,X)/\partial\theta\) are \(P_{\theta_{0}}\)-integrable;

4. the sequence of random functions \(\{E_{n}[\partial f(\theta,X)/\partial\theta]:n=1,2,\ldots\}\) is stochastically equicontinuous over \(\Theta\).

Derive the asymptotic distribution of \(\sqrt{n}[E_{n}f(\hat{\theta},X)-Ef(\theta_{0},X)]\).

**8.20**.: Suppose \(X\) is a random variable whose distribution belongs to a multi-parameter exponential family:

\[f_{\theta}(x)=\frac{e^{\theta^{T}t(x)}}{\int e^{\theta^{T}t(x)}d\mu(x)},\quad \theta\in\Theta\subseteq\mathbb{R}^{p},\ x\in\Omega_{X}\subseteq\mathbb{R},\]

where \(\mu\) is a \(\sigma\)-finite measure on \(\Omega_{X}\) and \(t\) is a function from \(\Omega_{X}\) to \(\mathbb{R}^{p}\). Suppose \(X_{1},\ldots,X_{n}\) are an i.i.d. sample from \(X\). Let \(\hat{\theta}\) be the maximum likelihood estimate.

1. Derive the asymptotic distribution of \(\sqrt{n}(\hat{\theta}-\theta_{0})\).

2. Suppose we estimate the moment generating function of \(t(X)\), \(\varphi(u)=E_{\theta_{0}}[e^{u^{T}t(X)}]\), by \(\hat{\varphi}(u)=E_{\hat{\theta}}[e^{u^{T}t(X)}]\). Prove that

\[\sqrt{n}[\hat{\varphi}(u)-\varphi(u)]\stackrel{{\mathcal{D}}}{{ \rightarrow}}N(0,\Lambda),\]

where \(\Lambda\) is

\[\varphi^{2}(u)[E_{\theta_{0}+u}t(X)-E_{\theta_{0}}t(X)]^{T}\{\mathrm{var}_{ \theta_{0}}[t(X)]\}^{-1}[E_{\theta_{0}+u}t(X)-E_{\theta_{0}}t(X)].\]

**8.21**.: Suppose \(X_{1},\ldots,X_{n}\) are an i.i.d. sample from the uniform distribution \(U[0,\theta]\); that is,

\[f_{\theta}(x)=\theta^{-1},\quad 0\leq x\leq\theta.\]

1. Derive the maximum likelihood estimate \(\hat{\theta}\) of \(\theta_{0}\).

2. Derive the asymptotic distribution of \(n(\hat{\theta}-\theta_{0})\).

3. Explain the discrepancy between this asymptotic distribution and that given by Theorem 8.4.

## References

* Conway (1990) Conway, J. B. (1990). _A course in functional analysis_. Second edition. Springer, New York.
* Cramer (1946) Cramer, H. (1946). _Mathematical Methods of Statistics_. Princeton University Press.
* Godambe (1960) Godambe, V. P. (1960). An optimum property of regular maximum likelihood estimation. _The Annals of Mathematical Statistics_, **31**, 1208-1211.
* McCullagh and Nelder (1989) McCullagh, P. and Nelder, J. A. (1989). _Generalized Linear Models, Second Edition_, Chapman & Hall.
* van der Vaart (1998) van der Vaart, A. W. (1998). _Asymptotic Statistics_. Cambridge University Press.
* Wald (1949) Wald, A. (1949). Note on the consistency of maximum likelihood estimate. _Annals of Mathematical Statistics_, **20**, 595-601.
* Wong (1986) Wong, W. H. (1986). Theory of partial likelihood. _The Annals of Statistics_. **14**, 88-123.

## Chapter Estimating equations

In this chapter we develop the theory of estimating equations. Estimating equations are a generalization of the maximum likelihood method, but they do not require a fully specified probability model. Instead, they only require the functional forms of their first two moments. Estimating equations have wide range of applications: for example, the quasi likelihood method (Wedderburn, 1974; McCullagh, 1983; Godambe and Thompson, 1989; Heyde, 1997), and the Generalized Estimating Equations (Liang and Zeger, 1986; Zeger and Liang, 1986) are two important types of estimating equations that are widely used in Generalized Linear Models and longitudinal data analysis. (Yes, a Generalized Estimating Equation is indeed a special type of estimating equation due to the commonly adopted convention, even though this sounds like an oxymoronic statement). In addition, estimating equations are related to the Generalized Method of Moments (Hansen, 1982), which is popular in econometrics. Estimating equations have also been developed in conjunction with the Martingale theory, and are a powerful method for statistical inference for stochastic processes (Heyde, 1997). As a natural extension of Maximum Likelihood Estimation and Method of Moments, estimating equations have their combined flavors and advantages.

In addition to their wide applications in data analysis, estimating equations are also a convenient theoretical framework to develop many aspects of statistical inference, such as conditional inference (Godambe, 1976; Lindsay, 1982), nuisance parameters, efficient estimator, and the information bound. They make some optimal results in statistical inference transparent via projections in Hilbert spaces.

### 1.1 Optimal Estimating Equations

The basic theory of optimal estimating equations are introduced and developed by Godambe (1960, 1976), Durbin (1960), and Crowder (1987). See also Morton (1981) and Jarrett (1984). An estimating equation is any measurableand \(P_{\theta}\)-integrable function of \(\theta\) and \(X\). We usually assume its expectation under \(\theta\) is either \(0\) or statistically ignorable, so that under mild conditions its solution is asymptotically Normal with a variance matrix depending on the estimating equation. The optimal estimating equation in a class of estimating equations is the one whose solution has the smallest asymptotic variance among the class in terms of Louwner's ordering. The explicit form of the optimal estimating equation can be derived by the multivariate Cauchy-Schwarz inequality developed in Section 7.9.

As before, let \(X_{1},\ldots,X_{n}\) be an i.i.d. sample with probability distribution \(P_{\theta}\), where \(\theta\) is a \(p\)-dimensional parameter in a parametric space \(\Theta\subseteq\mathbb{R}^{p}\). Throughout this chapter, we assume that the parametric family \(\{P_{\theta}:\theta\in\Theta\}\) is dominated by a \(\sigma\)-finite measure \(\mu\), and denote the density of \(P_{\theta}\) with respect to \(\mu\) by \(f_{\theta}\).

**Definition 9.1**: _A function \(g:\Theta\times\Omega_{X}\rightarrow\mathbb{R}^{p}\) is called an unbiased estimating equation if_

\[E_{\theta}[g(\theta,X)]=0\]

_for all \(\theta\in\Theta\)._

An unbiased estimating equation is a generalization of the score function \(s(\theta,X)\), which satisfies \(E_{\theta}[s(\theta,X)]=0\) under the assumptions of Proposition 8.1. It also includes many other estimates, such as the method of moments, the least squares estimate, and the quasi likelihood estimate (Wedderburn, 1974; McCullagh, 1983). Given an estimating equation \(g(\theta,X)\), the parameter \(\theta\) is estimated by solving the equation

\[E_{n}[g(\theta,X)]=0.\]

Let \(L_{2}(P_{\theta})\) be the class of all \(P_{\theta}\)-square-integrable random variables, and let

\[L_{2}^{p}(P_{\theta})=L_{2}(P_{\theta})\times\cdots\times L_{2}(P_{\theta})\]

be the \(p\)-fold Cartesian product of \(L_{2}(P_{\theta})\). We often assume that, for each \(\theta\), \(g(\theta,X)\) is a member of \(L_{2}^{p}(P_{\theta})\). This is stated as the following formal definition for easy reference.

**Definition 9.2**: _An estimating equation \(g(\theta,X)\) is \(P_{\theta}\)-square-integrable if \(g(\theta,X)\in L_{2}^{p}(P_{\theta})\) for each \(\theta\in\Theta\). Furthermore, a class of estimating equations \(\mathcal{G}\) is said to be \(P_{\theta}\)-square-integrable if its members are \(P_{\theta}\)-square-integrable._

The optimal estimating equation is defined in terms of the amount of information contained in an estimating equation, as introduced by Godambe (1960). We now give a general definition of this information.

**Definition 9.3**: _Suppose that \(g(\theta,X)\) is an unbiased and \(P_{\theta}\)-square-integrable estimating equation, and \(g(\theta,x)f_{\theta}(x)\) satisfies \(\mbox{DUI}^{+}(\theta,\mu)\). Then the following matrix_\[I_{g}(\theta)=E_{\theta}[\partial g^{T}(\theta,X)/\partial\theta]\{E_{\theta}[g( \theta,X)g^{T}(\theta,X)]\}^{+}E_{\theta}[\partial g(\theta,X)/\partial\theta^{ T}].\]

_is called the information contained in \(g(\theta,X)\)._

To understand the intuition behind this definition, consider the special case where \(\theta\) is a scalar, and the information \(I_{g}(\theta)\) takes the form

\[I_{g}(\theta)=\frac{\{E_{\theta}[\dot{g}(\theta,X)]\}^{2}}{E_{\theta}[g^{2}( \theta,X)]}.\]

Intuitively, since \(\hat{\theta}\) is solved from \(E_{n}[g(\theta,X)]=0\), a larger slope of \(E_{\theta_{0}}[g(\theta,X)]\) at \(\theta_{0}\) would benefit estimation, because a slight departure from \(\theta_{0}\) would cause a large change in \(E_{n}[g(\theta,X)]\), forcing its root to be close to \(\theta_{0}\). On the other hand, a smaller variance \(\text{var}_{\theta_{0}}[g(\theta_{0},X)]\) would benefit estimation, because it will make the random function \(E_{n}[g(\theta,X)]\) packed tightly around \(0\) when \(\theta\) is near \(\theta_{0}\), which makes the variation of the solution small. Thus the ratio of these two quantities characterizes the tendency of an estimating equation having a root close to the true parameter value. Also, as we shall see in Section 9.5, \(I_{g}^{-1}(\theta)\) is the asymptotic variance of \(\sqrt{n}(\hat{\theta}_{g}-\theta_{0})\), where \(\hat{\theta}_{g}\) is any consistent solution to the estimating equation \(E_{n}[g(\theta,X)]=0\). Thus maximizing the information of an estimating equation amounts to minimizing the asymptotic variance of its solution.

We now define the optimal estimating equation in a class \(\mathcal{G}\) of estimating equations.

**Definition 9.4**: _Suppose \(\mathcal{G}\) is a class of unbiased and \(P_{\theta}\)-square-integrable estimating equations such that, for each \(g\in\mathcal{G}\), \(g(\theta,x)f_{\theta}(x)\) satisfies \(DUI^{+}(\theta,\mu)\). If there is a member \(g^{*}\) of \(\mathcal{G}\) such that \(I_{g^{*}}\succeq I_{g}\) for all \(g\in\mathcal{G}\), then \(g^{*}\) is called the optimal estimating equation in \(\mathcal{G}\)._

We next develop a general method for constructing the optimal estimating equation. For all practical purposes a class \(\mathcal{G}\) of estimating equations may be assumed to be a linear manifold. That is, for each \(\theta\), the set \(\{g(\theta,X):g\in\mathcal{G}\}\) is a linear manifold in \(L_{2}^{p}(P_{\theta})\). As we will see in the subsequent development, if this linear manifold is closed, then the optimal estimating equation can be obtained by projecting the score function \(s(\theta,X)\) on to the linear subspace. However, the set \(\{g(\theta,X):g\in\mathcal{G}\}\) may not be closed: for example the DUI assumption may not be preserved after taking an \(L_{2}(P_{\theta})\)-limit. While it might be possible to get around this problem by using a more general definition of a derivative, we take the simpler approach to assume that \(\mathcal{G}\) contains the optimal estimating equations, which is sufficient for the discussions here. The method for constructing optimal estimating equations by projecting the score function on to a class of estimating equations is referred to as the projected score method by Small and McLeish (1989).

The next lemma is a generalization of the information identity (8.3) in Proposition 8.1.

**Lemma 9.1**: _If \(g(\theta,x)\) is an unbiased estimating equation such that \(g(\theta,x)f_{\theta}(x)\) satisfies \(\text{DUI}^{+}(\theta,\mu)\), then_

\[E_{\theta}\left[\frac{\partial g(\theta,X)}{\partial\theta^{T}}\right]=-E_{ \theta}[g(\theta,X)s^{T}(\theta,X)]. \tag{9.1}\]

_Proof._ By unbiasedness of \(g(\theta,X)\), we have

\[\int_{A}g(\theta,X)f_{\theta}(X)d\mu=0,\]

where \(A\) is the common support of \(f_{\theta}(x)\). Because \(g(\theta,X)f_{\theta}(X)\) satisfies \(\text{DUI}^{+}(\theta,\mu)\), by differentiating both sides of the equation we have

\[\int_{A}\frac{\partial g(\theta,X)}{\partial\theta^{T}}f_{\theta}(X)d\mu+\int_ {A}g(\theta,X)\frac{\partial f_{\theta}(X)/\partial\theta^{T}}{f_{\theta}(X)} f_{\theta}(X)d\mu=0.\]

The asserted result follows because first term on the left is \(E_{\theta}[\partial g(\theta,X)/\partial\theta^{T}]\), and the second term on the left is \(E_{\theta}[g(\theta,X)s^{T}(\theta,X)]\). \(\Box\)

Note that, if we take \(g(\theta,X)\) to be \(s(\theta,X)\), then the identity (9.1) reduces to the information identity (8.3) in Proposition 8.1. Before stating the next theorem, we define the inner product matrix of two members of \(L^{p}_{2}(P_{\theta})\), \(h_{1}(\theta,X)\) and \(h_{2}(\theta,X)\), to be

\[[h_{1},h_{2}]=E_{\theta}[h_{1}(\theta,X)h_{2}^{T}(\theta,X)].\]

**Theorem 9.1**: _Suppose \(\mathcal{G}\) is a class of unbiased and \(P_{\theta}\)-square-integrable-estimating equations such that for each \(g\)\(\in\)\(\mathcal{G}\), \(g(\theta,x)f_{\theta}(x)\) satisfies \(\text{DUI}^{+}(\theta,\mu)\). If there is a member \(g^{*}\) of \(\mathcal{G}\) satisfying \([s-g^{*},g]=0\) for all \(g\in\mathcal{G}\), then \(I_{g^{*}}\succeq I_{g}\) for all \(g\in\mathcal{G}\)._

_Proof._ By the multivariate Cauchy-Schwarz inequality (the more general version in Problem 7.22), we have, for any \(g\in\mathcal{G}\),

\[[g^{*},g^{*}]\succeq[g^{*},g][g,g]^{+}[g,g^{*}].\]

By assumption, \([g^{*},g]=[s,g]\). As shown in Lemma 9.1, under the DUI and the unbiasedness assumption, we have

\[[g,s]=-E_{\theta}[\partial g(\theta,X)/\partial\theta^{T}]. \tag{9.2}\]

Hence

\[[g^{*},g][g,g]^{+}[g,g^{*}]=E[\partial g^{T}/\partial\theta][E(gg^{T})]^{+}E( \partial g/\partial\theta^{T})=I_{g}.\]

Applying the above relation to \(g=g^{*}\), we have

\[[g^{*},g^{*}]=[g^{*},g^{*}][g^{*},g^{*}]^{+}[g^{*},g^{*}]=I_{g^{*}}.\]Therefore \(I_{g^{*}}\succeq I_{g}\). \(\Box\)

It follows immediately from the above theorem that the score function \(s(\theta,x)\) is the optimal estimating equation among a class of estimating equations \(\mathcal{G}\), as long as it belongs to \(\mathcal{G}\). In particular, we can take \(\mathcal{G}\) to be the class of all unbiased, \(P_{\theta}\)-square-integrable estimating equations that satisfy the DUI condition. This optimal property is stated in the next theorem, whose simple proof is omitted. The core idea of this result is due to Godambe (1960) and Durbin (1960).

**Corollary 9.1**: _Suppose \(\mathcal{G}\) is the class of all unbiased, \(P_{\theta}\)-square-integrable estimating equations such that, for each \(g\in\mathcal{G}\), \(f_{\theta}(x)\) and \(g(\theta,x)f_{\theta}(x)\) satisfy DUI\({}^{+}(\theta,\mu)\). If \(s(\theta,x)\) is a member of \(\mathcal{G}\), then, for any \(g\in\mathcal{G}\), we have \(I_{s}(\theta)\succeq I_{g}(\theta)\) for all \(\theta\in\Theta\)._

The optimality of maximum likelihood estimation can be stated at several levels. The above result stated in terms of estimating equations is an intuitive and relatively simple optimal property. We will revisit this issue to give a more general form of this optimality in Chapter 10.

By itself, Corollary 9.1 does not lead to any new method; it merely states that the maximum likelihood estimate is optimal in this sense. A more interesting case is when \(s(\theta,X)\) is not among the estimating equations considered, in which case it leads to new methods such as the quasi likelihood method and the generalized estimating equations.

The next proposition shows that the condition \([s-g^{*},g]=0\) for all \(g\in\mathcal{G}\) uniquely determines an optimal estimating equation in \(\mathcal{G}\) provided that \(\mathcal{G}\) is a linear space.

**Proposition 9.1**: _Suppose that \(\mathcal{G}\) is a linear manifold of unbiased and \(P_{\theta}\)-square-integrable estimating equations such that, for each \(g\in\mathcal{G}\), \(g(\theta,x)f_{\theta}(x)\) satisfies DUI\({}^{+}(\theta,\mu)\). If there exist \(g_{1}^{*}\) and \(g_{2}^{*}\) in \(\mathcal{G}\) satisfying \([s-g_{i}^{*},g]=0\), for \(i=1,2\) and for all \(g\in\mathcal{G}\), then \(g_{1}^{*}=g_{2}^{*}\)._

Proof: By assumption,

\[[s-g_{1}^{*},g]=0,\quad[s-g_{2}^{*},g]=0\]

for all \(g\in\mathcal{G}\). Since \(\mathcal{G}\) is a linear manifold, \(g_{1}^{*}-g_{2}^{*}\) is a member of \(\mathcal{G}\). Hence

\[[s-g_{1}^{*},g_{1}^{*}-g_{2}^{*}]=0,\quad[s-g_{2}^{*},g_{1}^{*}-g_{2}^{*}]=0.\]

Subtracting the first equation from the second, we have

\[[g_{1}^{*}-g_{1}^{*},g_{1}^{*}-g_{2}^{*}]=0,\]

which implies \(g_{1}^{*}-g_{2}^{*}=0\) by Proposition 7.4.

In the next three sections we develop some special optimal estimating equations, including the quasi likelihood estimating equation, and the Generalized Estimating Equation. They are the foundations of some important statistical methodologies, and also serve to illustrate how to construct optimal estimating equations using the general method introduced in this section under different settings. Ideally all the results in the next three sections can be written rigorously as lemmas, theorems, and corollaries, but doing so would involve lengthy regularity conditions that may obscure otherwise simple ideas. We will instead develop them somewhat informally without stating all the regularity conditions.

### Quasi likelihood estimation

Suppose that \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) are i.i.d. with a joint distribution from a parametric family \(\{P_{\theta}:\theta\in\Theta\subseteq\mathbb{R}^{p}\}\). We assume \(X_{i}\) is a \(p\)-dimensional random vector, and \(Y_{i}\) is a random variable. Again, assuming \(X_{i}\) to be random is purely for convenience. The following estimating equation is based entirely on conditional moments given \(X\). That is, we are in effect treating \(X\) as fixed. The optimal estimating equation we get is the same as that of assuming \(X_{i}\) to be fixed.

Here, we do not assume the full parametric form of \(P_{\theta}\), but instead, we only assume the forms of the first two conditional moments of \(Y\) given \(X\):

\[E_{\theta}(Y|X)=\mu(\theta^{T}X),\quad\text{var}_{\theta}(Y|X)=V(\theta^{T}X),\]

where \(\mu\) and \(V\) are known functions. For example, in a log linear model for the Poisson data,

\[\mu(\theta^{T}X)=e^{\theta^{T}X},\quad V(\theta^{T}X)=e^{\theta^{T}X}.\]

Denote the conditional density of \(Y|X\) by \(f_{\theta}(y|x)\) and the marginal density of \(X\) by \(f_{X}(x)\). Neither \(f_{\theta}(y|x)\) nor \(f_{X}\) needs to be specified except the conditional moments \(\mu(\cdot)\) and \(V(\cdot)\).

Consider the class of unbiased estimating equations of the form

\[\mathcal{G}=\{a_{\theta}(X)(Y-\mu(\theta^{T}X)):\,a\text{ is a function from }\Theta\times\Omega_{X}\text{ to }\mathbb{R}^{p}\}.\]

Our goal is to find the optimal estimating equation in \(\mathcal{G}\) that satisfies \([s-g^{*},g]\)= 0 for all \(g\in\mathcal{G}\), where, as before, \(s=s(\theta,x,y)\) represents the true score function. Despite its appearance, this process does not require the form of \(s\). Our optimal estimating equation is derived from the defining relation

\[E_{\theta}(g^{*}g^{T})=E_{\theta}(sg^{T})=-E_{\theta}(\partial g^{T}/\partial \theta).\]

In our setting, \(g^{*}=a_{\theta}^{*}(X)(Y-\mu(\theta^{T}X))\), \(g=a_{\theta}(X)(Y-\mu(\theta^{T}X))\). For convenience, we abbreviate \(a_{\theta}^{*}(X)\), \(a_{\theta}(X)\), and \(\mu(\theta^{T}X)\) by \(a^{*}\), \(a\), and \(\mu\), respectively. The above relation specializes to \[E_{\theta}[a^{*}a^{T}(Y-\mu)^{2}]=-E_{\theta}\left[\frac{\partial a^{T}}{\partial \theta}(Y-\mu)\right]+E_{\theta}\left(\frac{\partial\mu}{\partial\theta}a^{T} \right). \tag{9.3}\]

The left-hand side is

\[E_{\theta}\{a^{*}a^{T}E[(Y-\mu)^{2}|X]\}=E_{\theta}(a^{*}a^{T}V).\]

The first term on the right-hand side of (9.3) is

\[-E_{\theta}\left[\frac{\partial a^{T}}{\partial\theta}E(Y-\mu|X)\right]=0.\]

Thus, \(a^{*}\) satisfies the following functional equation

\[E_{\theta}(a^{*}a^{T}V)=E_{\theta}\left(\frac{\partial\mu}{\partial\theta}a^{T }\right).\]

By inspection, we see that if we take \(a^{*}=(\partial\mu/\partial\theta)/V\), then the above equation is satisfied for all \(a\). Thus we arrive at the following optimal estimating equation

\[g^{*}(\theta,X,Y)=[\partial\mu(\theta^{T}X)/\partial\theta]\,[Y-\mu(\theta^{T} X)]/V(\theta^{T}X).\]

This optimal estimating equation is called the quasi score function. See Wedderburn (1974), McCullagh (1983), and Li and McCullagh (1994). The maximum quasi likelihood estimate is defined as the solution to the estimating equation

\[E_{n}[g^{*}(\theta,X,Y)]=0.\]

The information contained in the quasi score is

\[I_{g^{*}}=E[(\partial\mu/\partial\theta)(\partial\mu/\partial\theta)^{T}/V].\]

The above construction can be easily extended to vector-valued \(Y_{i}\), say, of dimension \(q\). In this case, \(\mu(\theta^{T}X)\) is a \(q\)-dimensional vector and \(V(\theta^{T}X)\) is a \(q\times q\) matrix. Consider the class \(\mathcal{G}\) of estimating equations of the form

\[A(\theta,X)[Y-\mu(\theta^{T}X)],\]

where \(A(\theta,X)\) is a \(p\times q\) random matrix that may depend on \(\theta\) and \(X\) but does not depend on \(Y\). By Theorem 9.1, if there is an \(A^{*}(\theta,X)\) such that \(g^{*}=A^{*}(\theta,X)[Y-\mu(\theta^{T}X)]\) satisfies

\[\begin{split}[A(\theta,X)(Y-\mu(\theta^{T}X)),s]\\ =[A(\theta,X)(Y-\mu(\theta^{T}X)),A^{*}(\theta,X)(Y-\mu(\theta^{T }X))],\end{split} \tag{9.4}\]

for all \(A(\theta,X)\), then \(g^{*}\) is an optimal estimating equation. The left-hand side is \[[A(\theta,X)(Y-\mu(\theta^{T}X)),s]= E_{\theta}\{\partial[A(\theta,X)(Y-\mu(\theta^{T}X))]/\partial \theta^{T}\}\] \[= -E_{\theta}[A(\theta,X)\partial\mu(\theta^{T}X)/\partial\theta^{T}].\]

The right-hand side of (9.4) is

\[E_{\theta}[A(\theta,X)V(\theta^{T}X)A^{*T}(\theta,X)]\]

So, if we let

\[A^{*T}(\theta,X)=V^{-1}(\theta^{T}X)\partial\mu(\theta^{T}X)/\partial\theta^{T},\]

then (9.4) is satisfied for all \(A(\theta,X)\). Thus, the quasi score function in this case is

\[[\partial\mu^{T}(\theta^{T}X)/\partial\theta]V^{-1}(\theta^{T}X)[Y-\mu(\theta^ {T}X)].\]

This is the form given in McCullagh (1983).

## 0.3 Generalized Estimating Equations

Generalized Estimating Equations (GEE) were introduced by Liang and Zeger (1986), and Zeger and Liang (1986) to deal with the situations where each subject has multiple observations that may be dependent. They are a flexible and effective method for handling longitudinal data, and are widely used in that area. Suppose we have observations

\[\{(X_{ik},Y_{ik}):\,k=1,\ldots,n_{i},\ i=1,\ldots,n\},\]

where \(X_{ik}\) are \(p\)-dimensional predictors, and \(Y_{ik}\) are 1-dimensional response. We write

\[Y_{i}=(Y_{i1},\ldots,Y_{in_{i}})^{T},\quad X_{i}=(X_{i1},\ldots,X_{in_{i}}).\]

Note that \(Y_{i}\) (and also \(X_{i}\)) may have different dimensions for different \(i\). The responses within the same subject, \(\{Y_{i1},\ldots,Y_{in_{i}}\}\), may be dependent, but the responses for different subjects are assumed to be independent.

As in quasi likelihood, we assume the functional forms of the conditional mean and variance:

\[E(Y_{ik}|X_{ik})=\mu(X_{ik}^{T}\beta),\quad\text{var}(Y_{ik}|X_{ik})=V(X_{ik}^{ T}\beta),\]

where \(\mu\) and \(V\) are known functions. We write

\[\mu_{i}(X_{i},\beta)= \left(\mu(\beta^{T}X_{i1}),\ldots,\mu(\beta^{T}X_{in_{i}})\right) ^{T},\] \[V_{i}(X_{i},\beta)= \operatorname{diag}\left(V(\beta^{T}X_{i1}),\ldots,V(\beta^{T}X_{ in_{i}})\right).\]

Within the same subject, we assume

\[\text{var}(Y_{i}|X_{i})=V_{i}(X_{i},\beta)^{1/2}R_{i}(\alpha)V_{i}(X_{i},\beta) ^{1/2},\]where, for each \(i\), \(R_{i}(\alpha)\) is a known matrix-valued function of \(\alpha\). These \(R_{i}(\alpha)\) are called the working correlation matrices. An important feature of the GEE is that \(R_{i}(\alpha)\) need not be correctly specified for GEE to yield \(\sqrt{n}\)-consistent estimates, but if \(R_{i}(\alpha)\) are correctly specified, then GEE is the optimal estimating equation, as described below.

Consider the following class of linear (in \(Y\)) and unbiased estimating equations

\[\mathcal{G}=\left\{\sum_{i=1}^{n}W_{i}(X_{i},\alpha,\beta)(Y_{i}-\mu_{i}(X_{i},\beta)):\,W_{i}(X_{i},\beta)\in\mathbb{R}^{p\times n_{i}}\right\}.\]

It is natural to consider this class of estimating equations because their inner products can be completely specified by the form of the first two moments that we assume. We seek the optimal estimating equation within this class. Let

\[S(\alpha,\beta,X_{1:n},Y_{1:n})=\sum_{i=1}^{n}s(\alpha,\beta,X_{i},Y_{i})\]

be the score function. Let \(G\) be an arbitrary member of \(\mathcal{G}\), and \(G^{*}\) be the optimal estimating equation in \(\mathcal{G}\). That is,

\[G(\alpha,\beta,X_{1:n},Y_{1:n}) =\sum_{i=1}^{n}W_{i}(X_{i},\alpha,\beta)(Y_{i}-\mu_{i}(X_{i}, \beta)),\] \[G^{*}(\alpha,\beta,X_{1:n},Y_{1:n}) =\sum_{i=1}^{n}W_{i}^{*}(X_{i},\alpha,\beta)(Y_{i}-\mu_{i}(X_{i}, \beta)).\]

For convenience, we omit the arguments of the functions and write

\[S=S(\alpha,\beta,X_{1:n},Y_{1:n}),\ s_{i}=s(\alpha,\beta,X_{i},Y_ {i}),\] \[G=G(\alpha,\beta,X_{1:n},Y_{1:n}),\] \[W_{i}=W_{i}(X_{i},\alpha,\beta),\ R_{i}=R_{i}(\alpha),\ \mu_{i}= \mu_{i}(\beta,X_{i}),\ V_{i}=V_{i}(\beta,X_{i}).\]

The optimal estimating equation \(G^{*}\) is determined by the equation

\[[S,G]=[G^{*},G] \tag{9.5}\]

for all \(G\in\mathcal{G}\). The right-hand side is

\[E[G^{*}G^{T}] =\sum_{i=1}^{n}E\left[W_{i}^{*}(Y_{i}-\mu_{i})(Y_{i}-\mu_{i})^{T} W_{i}^{T}\right]\] \[=\sum_{i=1}^{n}E\left\{W_{i}^{*}E[(Y_{i}-\mu_{i})(Y_{i}-\mu_{i})^ {T}|X_{i}]W_{i}^{T}\right\} \tag{9.6}\] \[=\sum_{i=1}^{n}E\left(W_{i}^{*}V_{i}^{1/2}R_{i}V_{i}^{1/2}W_{i}^{ T}\right).\]The left-hand side of (9.5) is

\[E(SG^{T})=\sum_{i=1}^{n}E\left[s_{i}(Y_{i}-\mu_{i})^{T}W_{i}^{T}\right].\]

By Lemma 9.1, the summand in the right-hand side is

\[-E\left\{(\partial/\partial\beta)[(Y_{i}-\mu_{i})^{T}W_{i}^{T}]\right\}\] \[=-E\left\{[\partial(Y_{i}-\mu_{i})^{T}/\partial\beta]W_{i}^{T} \right\}-E\left\{(Y_{i}-\mu_{i})^{T}(\partial W_{i}^{T}/\partial\beta)\right\}.\]

In the second term, \((Y_{i}-\mu_{i})^{T}\) is a row vector, and \(W_{i}^{T}\) is an \(n_{i}\times p\) matrix. The expression \((Y_{i}-\mu_{i})^{T}(\partial W_{i}^{T}/\partial\beta)\) simply means the \(p\times p\) matrix whose \(j\)th row is \((Y_{i}-\mu_{i})^{T}(\partial W_{i}^{T}/\beta_{j})\). Since

\[E\left\{[\partial(Y_{i}-\mu_{i})^{T}/\partial\beta]W_{i}^{T}\right\} = -E\left[(\partial\mu_{i}^{T}/\partial\beta)W_{i}^{T}\right]\] \[E\left\{(Y_{i}-\mu_{i})^{T}(\partial W_{i}^{T}/\partial\beta) \right\} = E[E(Y_{i}-\mu_{i}|X_{i})^{T}(\partial W_{i}^{T}/\partial\beta)]=0,\]

we have

\[[S,G]=E\left[(\partial\mu_{i}^{T}/\partial\beta)W_{i}^{T}\right]. \tag{9.7}\]

By (9.6) and (9.7), the defining relation (9.5) for an optimal estimating equation specializes to the following form in the GEE setting:

\[\sum_{i=1}^{n}E\left(W_{i}^{*}V_{i}^{1/2}R_{i}V_{i}^{1/2}W_{i}^{T}\right)=\sum _{i=1}^{n}E\left[(\partial\mu_{i}^{T}/\partial\beta)W_{i}^{T}\right]. \tag{9.8}\]

If we let

\[W_{i}^{*}=(\partial\mu_{i}^{T}/\partial\beta)V_{i}^{-1/2}R_{i}^{-1}V_{i}^{-1/2},\]

then (9.8) holds for all \(W_{i}\). Thus we arrive at the following optimal estimating equation

\[\sum_{i=1}^{n}[\partial\mu_{i}(X_{i},\beta)^{T}/\partial\beta][V_{i}^{1/2}(X_{ i},\beta)R_{i}(\alpha)V_{i}^{1/2}(X_{i},\beta)]^{-1}[Y_{i}-\mu_{i}(X_{i},\beta)]=0.\]

At the sample level, the parameters \(\alpha\) and \(\beta\) are estimated by the following iterative regime. For a fixed \(\hat{\alpha}\), we estimate \(\beta\) by solving the above equation. For a fixed \(\hat{\beta}\), we define the residuals as

\[\hat{r}_{ik}=\frac{Y_{ik}-\mu_{i}(X_{i},\hat{\beta})}{V_{i}^{1/2}(X_{i},\hat{ \beta})}.\]

We then estimate \((k,k^{\prime})\)th component of \(R_{i}\) as \[(\hat{R}_{i})_{kk^{\prime}}=\frac{1}{N-p}\sum_{i=1}^{n}\hat{r}_{ik}\hat{r}_{ik^{ \prime}}.\]

In this case, we do not assume any further structure in the correlation matrix \(R_{i}\), so that the distinct entries of \(R_{i}\) themselves constitute \(\alpha\). But it is possible to build more structure into the \(R_{i}(\alpha)\), such as the autoregressive model, the exchangeable correlation model. See Liang and Zeger (1986) for further information.

### 9.4 Other optimal estimating equations

The above two sections are concerned with the quasi score function and the generalized estimating equation, both of which can be regarded as the optimal estimating equations among the linear estimating equations of the form

\[A(\theta,X)[Y-\mu(\theta^{T}X)],\]

where \(A(\theta,X)\) is a scalar or a vector. In this section we make a brief exposition of several other types of optimal estimating equations to get a broader view of the methodology of estimating equations.

Crowder (1987) considered the following general class of estimating equations

\[g(\theta,X,Y)=W(\theta,X)u(\theta,X,Y),\]

where \(u\) is a function on \(\Omega\times\Theta\) to \(\mathbb{R}^{r}\), whose components are unbiased and \(P_{\theta}\)-square-integrable estimating equations. The components of \(u\) may be any fixed set of functions of \(X,Y,\theta\). In particular, Crowder (1987) considered the special case where \(u\) is the vector of linear and quadratic polynomials of \(Y\), as a modification of the quasi score function when the fourth moment of \(Y\) is available.

Another type of optimal estimating equations are the ones used for conditional inference, which were developed by Waterman and Lindsay (1996). Suppose \((X_{1},\ldots,X_{n})\) is distributed as \(P_{\theta}\), where \(\theta\) consists of a parameter of interest \(\psi\) and a nuisance parameter \(\lambda\). For simplicity, we will focus on the case where both \(\psi\) and \(\lambda\) are scalars. But the following development can be extended in a straightforward (albeit quite tedious) manner to the vector-valued \(\psi\) and \(\lambda\). The vector-valued \(\psi\) and \(\lambda\) will be considered in Section 9.8, but there we will only consider the first-order projection, a special case of the \(m\)th-order projection developed below.

We are interested in statistical inference about \(\psi\). The parameter \(\lambda\) is not of interest, but it is needed to specify a meaningful statistical model. Ideally, if we have a statistic \(T\) that is sufficient for \(\lambda\), then the conditional distribution of \((X_{1},\ldots,X_{n})|T\) does not depend on the nuisance parameter \(\lambda\), and we can infer about \(\psi\) using this conditional distribution. This is, in fact, the approach we took in Chapter 4 to develop the UMPU test for a parameter of interest for an exponential family distribution, for which such a statistic \(T\) does exit. However, existence of such a \(T\) is a very stringent requirement, which a majority of distributions outside the exponential family do not meet. Waterman and Lindsay (1996) proposed a generalization of the conditional score \(s^{(c)}(\psi,X_{1},\ldots,X_{n})=\partial\log f_{X_{1},\ldots,X_{n}|T}(x_{1}, \ldots,x_{n}|t;\psi)/\partial\psi\) using the idea of optimal estimating equations for the situations where the sufficient statistic \(T\) for \(\lambda\) does not exist. For convenience, only for the rest of this section, we will use \(X\) to abbreviate \(X_{1},\ldots,X_{n}\), so that, for example, \(s^{(c)}(\psi,X_{1},\ldots,X_{n})\) is abbreviated by \(s^{(c)}(\psi,X)\). After this section, we will resume the convention that \(X_{1},\ldots,X_{n}\) are random copies of \(X\) -- a convention that has been used throughout the rest of the book.

To develop the intuition, let us first derive an alternative representation of the conditional score \(s^{(c)}(\psi,X)\) when \(T\) exists. Since

\[f_{X}(x;\theta)=f_{X|T}(x|t;\psi)f_{T}(t;\theta), \tag{9.9}\]

we have

\[s_{\psi}(\theta,X)=s^{(c)}(\psi,X)-s^{(m)}(\theta,T),\]

where \(s_{\psi}(\theta,x)=\partial\log f_{X}(x;\theta)/\partial\psi\) is the score function for \(\psi\), and \(s^{(m)}(\theta,T)\) is the marginal score \(\partial\log f_{T}(t;\theta)/\partial\psi\). Next, consider the linear subspace \(\mathcal{B}\) of \(L_{2}(P_{\theta})\) spanned by

\[b_{k}(x)=\frac{\partial^{k}f_{X}(x;\theta)/\partial\lambda^{k}}{f_{X}(x;\theta )},\quad k=1,2,\ldots\]

These functions are called the Bhattacharyya basis (Bhattacharyya, 1946) (and hence the notation \(\mathcal{B}\)). By (9.9), it is easy to check that

\[b_{k}(x)=\frac{\partial^{k}f_{T}(t;\theta)/\partial\lambda^{k}}{f_{T}(t;\theta )}\equiv c_{k}(t).\]

If the set of functions \(\{c_{k}(t):k=1,2,\ldots\}\) is rich enough so that \(s^{(m)}(\theta,t)\) is contained in \(\mathcal{B}\), then \(s^{(m)}(\theta,T)\) is, in fact, the projection of \(s_{\psi}(\theta,X)\) on to \(\mathcal{B}\) in terms of the \(L_{2}(P_{\theta})\) inner product. To see this, take any basis function \(c_{k}(t)\) from \(\mathcal{B}\), and we have

\[E_{\theta}\{c_{k}(T)[s_{\psi}(\theta,X)-s^{(m)}(\theta,T)]\}= E_{\theta}[c_{k}(T)s^{(c)}(\psi,X)]\] \[= E_{\theta}\{c_{k}(T)E_{\psi}[s^{(c)}(\psi,X)|T]\}=0.\]

Consequently, \(s^{(c)}(\psi,X)=s_{\psi}(\theta,X)-s^{(m)}(\theta,T)\) is the projection of \(s_{\psi}(\theta,X)\) on to \(\mathcal{B}^{\perp}\). In other words, \(s^{(c)}(\psi,X)\) is the optimal estimating equation in \(\mathcal{B}^{\perp}\).

The point of the above derivation is that, while the conditional score \(s^{(c)}(\psi,X)\) is defined only when there is a sufficient statistic \(T\) for \(\lambda\), the projection \(s_{\psi}(\theta,X)\) on to \(\mathcal{B}^{\perp}\) is not restricted by the existence of \(T\). Motivated by this, Waterman and Lindsay (1996) proposed to use the projection of \(s_{\psi}(\theta,X)\)on to \(\mathcal{B}_{m}^{\perp}\) as the generalized conditional score to conduct statistical inference about \(\psi\), where \(\mathcal{B}_{m}=\text{span}\{b_{1}(x),\ldots,b_{m}(x)\}\).

We can now derive the explicit form of this projection. Let \(P_{\mathcal{B}_{m}}\) be the projection operator on to the subspace of \(L_{2}(P_{\theta})\) spanned by the set \(\mathcal{B}_{m}\). By Example 7.5, the projection of \(s_{\psi}=s_{\psi}(\theta,X)\) on to \(\mathcal{B}_{m}\) is

\[P_{\mathcal{B}_{m}}s_{\psi}=\left(\langle s_{\psi},b_{1}\rangle,\ldots,\langle s _{\psi},b_{m}\rangle\right)\begin{pmatrix}\langle b_{1},b_{1}\rangle&\cdots& \langle b_{1},b_{m}\rangle\\ \vdots&\ddots&\vdots\\ \langle b_{m},b_{1}\rangle&\cdots&\langle b_{m},b_{m}\rangle\end{pmatrix} \begin{pmatrix}b_{1}\\ \vdots\\ b_{m}\end{pmatrix}.\]

Thus, the projection of \(s_{\psi}\) on to \(\mathcal{B}_{m}^{\perp}\) is \(s_{\psi}-P_{\mathcal{B}_{m}}s_{\psi}\).

An important -- and most commonly used -- special case is \(m=1\), where the above projection becomes

\[s_{\psi}-\frac{E_{\theta}(s_{\psi}s_{\lambda})}{E_{\theta}(s_{\lambda}s_{ \lambda})}s_{\lambda}=s_{\psi}-\frac{I_{\psi\lambda}}{I_{\lambda\lambda}}\,s_ {\lambda},\]

where \(s_{\lambda}=\partial\log f_{X}(x;\theta)/\partial\lambda\) is the score function for \(\lambda\); \(I_{\psi\lambda}\) and \(I_{\lambda\lambda}\) are the \((\psi,\psi)\)- and \((\psi,\lambda)\)-components of the Fisher information. This estimating equation is commonly known as the efficient score, to which we will return in Section 9.8.

### Asymptotic properties

Let \(X_{1},\ldots,X_{n}\) be independent copies of \(X\), where \(X\) is distributed as \(P_{\theta}\). Let \(g\) be an unbiased estimating equation. We estimate the parameter \(\theta\) by solving the estimating equation

\[E_{n}[g(\theta,X)]=0.\]

The existence of consistent solutions, as stated in the next theorem, can be proved using a similar method as that used in Theorem 8.1.

**Theorem 9.2**_Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. random variables or vectors having a density \(f_{\theta_{0}}\) belonging to a parametric family \(\{f_{\theta}:\theta\in\Theta\subseteq\mathbb{R}^{p}\}\), and the support \(B\) of \(f_{\theta}\) does not depend on \(\theta\). Suppose \(g:\Theta\times\Omega_{X}\rightarrow\mathbb{R}^{p}\) is an unbiased estimating equation such that_

_1. for all \(\theta\in\Theta\), the matrix \(A(\theta_{0})=E[\partial g(\theta,X)/\partial\theta^{T}]\) is negative definite;_

_2. \(g(\theta,x)\) satisfies DUI\((\theta,B,P_{\theta_{0}})\);_

_3. in a neighborhood of \(\theta_{0}\), \(E_{n}[g(\theta,X)]\) converges in probability uniformly to \(E[g(\theta,X)]\)._

_Then, there is a sequence of estimators \(\{\hat{\theta}_{n}\}\) such that_

_i. with probability tending to 1, \(\hat{\theta}_{n}\) is a solution to \(E_{n}[g(\theta,X)]=0\),__ii._\(\hat{\theta}_{n}\stackrel{{ P}}{{\rightarrow}}\theta_{0}\)_._

The assumption that \(A(\theta_{0})\) is negative definite is not unreasonable. For example, if \(g=g^{*}\) is the optimal estimating equation, then we have

\[E[\partial g^{*}(\theta,X)/\partial\theta^{T}]=-E[g^{*}(\theta,X)g^{*T}(\theta, X)],\]

which is negative definite if \(g^{*}(\theta,X)\) is not degenerate for any \(\theta\). Another example is when \(E_{n}[g(\theta,X)]\) is derived from the derivative of a concave objective function, in which case, \(\partial g(\theta,X)/\partial\theta^{T}\) is the Hessian matrix of a concave function, which is negative semi-definite. The proof of this theorem is similar to the proof of Theorem 8.1, and is left as an exercise.

Since the estimating equation method does not start with an objective function to maximize or minimize, Wald's approach to consistency cannot be directly applied. So a commonly used consistency statement is existence of a consistent solution to an estimating equation as in Theorem 9.2, which is not as specific as the Wald-type consistency statement. However, it is possible to construct an objective function such that a Wald-type consistency statement can be obtained. For example, Li (1993, 1996) introduced a deviance function for the quasi-likelihood method as a function of the first two moments. It was shown that the minimax of this deviance function is consistent under mild assumptions.

Next, we derive the asymptotic distribution of a consistent solution to an estimating equation. Let \(J_{g}(\theta)\) and \(K_{g}(\theta)\) denote the matrices

\[J_{g}(\theta)=E_{\theta}[\partial g(\theta,X)/\partial\theta^{T}],\quad K_{g} (\theta)=E_{\theta}[g(\theta,X)g^{T}(\theta,X)]. \tag{9.10}\]

The information contained in the estimating equation \(g\) is simply

\[I_{g}(\theta)=J_{g}^{T}(\theta)K_{g}^{-1}(\theta)J_{g}(\theta). \tag{9.11}\]

As before, let \(B^{\circ}(\theta_{0},\epsilon)\) denote the open ball centered at \(\theta_{0}\) with radius \(\epsilon\).

**Theorem 9.3**: _Suppose_

1. \(g(\theta,x)\) _is an unbiased,_ \(P_{\theta}\)_-square-integrable estimating equation such that_ \(g(\theta,x)f_{\theta}(x)\) _satisfies_ \(\mbox{DUI}^{+}(\theta,\mu)\)_;_
2. _the sequence_ \(\{E_{n}[\partial g(\theta,X)/\partial\theta^{T}]:n=1,2,\ldots\}\) _is stochastically equicontinuous over_ \(B^{\circ}(\theta_{0},\epsilon)\)_;_
3. _the true parameter_ \(\theta_{0}\) _is an interior point of_ \(\Theta\)_._

_If \(\hat{\theta}\) is a consistent solution to likelihood equation \(E_{n}[g(\theta,X)]=0\), then \(\sqrt{n}(\hat{\theta}-\theta_{0})\stackrel{{\mathcal{D}}}{{ \longrightarrow}}N(0,I_{g}^{-1}(\theta_{0}))\)._

The similar arguments in the proof of Theorem 8.4 lead to

\[\sqrt{n}(\hat{\theta}-\theta_{0})\stackrel{{\mathcal{D}}}{{ \rightarrow}}N(0,J_{g}^{-1}(\theta_{0})K_{g}(\theta_{0})J_{g}^{-T}(\theta_{0}))\]But here, we no longer have

\[K_{g}(\theta)=-J_{g}(\theta)=I_{g}(\theta).\]

So, by (9.11), the asymptotic distribution of \(\sqrt{n}(\hat{\theta}-\theta)\) is \(N(0,I_{g}(\theta_{0}))\). 

The finite-sample optimality in Corollary 9.1, combined with the asymptotic normality in Theorem 9.3, shows that the maximum likelihood estimate has the smallest asymptotic variance (in terms of Louyner's ordering) among the solutions to estimating equations \(E_{n}[g(\theta,X)]=0\), where \(g\) satisfies the conditions in Theorem 9.3. This property is a form of asymptotic efficiency of the maximum likelihood estimate.

Similarly, in the situations where \(\mathcal{G}\) does not include the true score function, if \(g^{*}\) is the optimal estimating equation in \(\mathcal{G}\), and if \(\hat{\theta}_{g^{*}}\) is the solution to the estimating equation \(E_{n}[g^{*}(\theta,X)]=0\), then \(\sqrt{n}(\hat{\theta}_{g^{*}}-\theta_{0})\) converges in distribution to \(N(0,I_{g^{*}}^{-1}(\theta_{0}))\). Since \(I_{g^{*}}\geq I_{g}\) for any \(g\in\mathcal{G}\), \(\sqrt{n}(\hat{\theta}_{g^{*}}-\theta_{0})\) has the the smallest asymptotic variance among the solutions to the estimating equations in \(\mathcal{G}\).

### 9.6 One-step Newton-Raphson estimate

The maximum likelihood estimate is not the unique estimate that achieves asymptotic efficiency. In fact, if we are given a \(\sqrt{n}\)-consistent estimator, then it is quite easy to construct an asymptotically efficient estimator. We first give a formal definition of \(\sqrt{n}\)-consistency.

**Definition 9.5**: _We say that \(\tilde{\theta}\) is a \(\sqrt{n}\)-consistent estimate of \(\theta_{0}\) if \(\tilde{\theta}-\theta_{0}=O_{P}(n^{-1/2})\)._

Before introducing the one-step Newton-Raphson estimate, we now briefly review the Newton-Raphson algorithm. Consider a generic equation

\[F(\theta)=0,\]

where \(F:\Theta\rightarrow\mathbb{R}^{p}\) is a differentiable function. Suppose we have an initial value \(\theta^{(0)}\). Then, near \(\theta^{(0)}\), we can approximate \(F(\theta)\) by its first-order Taylor polynomial \(\hat{F}(\theta)=\theta^{(0)}+[\partial F(\theta^{(0)})/\partial\theta^{T}]( \theta-\theta^{(0)})\). Instead of solving \(F(\theta)=0\), we solve the linear equation \(\hat{F}(\theta)=0\), which gives

\[\theta=\theta^{(0)}-[\partial F(\theta^{(0)})/\partial\theta^{T}]^{-1}F( \theta^{(0)}).\]

Motivated by this approximation, the Newton-Raphson algorithm consists of the iterations

\[\theta^{(k+1)}=\theta^{(k)}-[\partial F(\theta^{(k)})/\partial\theta^{T}]^{-1} F(\theta^{(k)})\]until convergence.

In our context, the equation to solve is \(E_{n}[s(\theta,X)]=0\). So the Newton-Raphson algorithm for computing the maximum likelihood estimate is

\[\theta^{(k+1)}=\theta^{(k)}-[E_{n}\partial s(\theta^{(k)},X)/\partial\theta^{T}] ^{-1}E_{n}s(\theta^{(k)},X).\]

By the central limit theorem, \(E_{n}[\partial s(\theta_{0},X)/\partial\theta^{T}]\) is a \(\sqrt{n}\)-consistent estimate of \(-I(\theta_{0})\). So it is reasonable to replace \(E_{n}[\partial s(\theta^{(k)},X)/\partial\theta^{T}]\) by \(-I(\theta^{(k)})\) in the above formula, resulting in

\[\theta^{(k+1)}=\theta^{(k)}+I^{-1}(\theta^{(k)})E_{n}[s(\theta^{(k)},X)].\]

This is called the Fisher scoring algorithm (Cox and Hinkley, 1974).

The one-step Newton-Raphson estimate is derived from this, except that the initial value is a \(\sqrt{n}\)-consistent estimate, and that we only need to apply the formula (9.12) once. That is, if \(\tilde{\theta}\) is a \(\sqrt{n}\)-consistent estimate of \(\theta_{0}\), then the one-step Newton-Raphson estimate is

\[\hat{\theta}=\tilde{\theta}+I^{-1}(\tilde{\theta})E_{n}[s(\tilde{\theta},X)]. \tag{9.12}\]

The next theorem implies that even though the updated \(\theta\) has not converged yet after one iteration, this estimate is fully efficient: it has the same asymptotic distribution as the maximum likelihood estimate.

In fact, we shall prove a more general result. Let \(g\) be an unbiased, \(P_{\theta}\)-square-integrable estimating equation such that \(g(\theta,x)f_{\theta}(x)\) satisfies \(\text{DUI}^{+}(\theta,\mu)\). If we define \(\tilde{\theta}_{g}\) as the one-step Newton-Raphson estimator

\[\tilde{\theta}_{g}=\tilde{\theta}-J_{g}^{-1}(\tilde{\theta})E_{n}[g(\tilde{ \theta},X)], \tag{9.13}\]

then \(\sqrt{n}(\tilde{\theta}_{g}-\theta_{0})\) has the same asymptotic distribution as \(\sqrt{n}(\tilde{\theta}_{g}-\theta_{0})\), where \(\tilde{\theta}_{g}\) is a consistent solution to \(E_{n}[g(\theta,X)]=0\). Consequently, if \(g^{*}\) is the optimal estimating equation in \(\mathcal{G}\), then \(\tilde{\theta}_{g^{*}}\) has the smallest asymptotic variance among the solutions to estimating equations in \(\mathcal{G}\). In particular, for \(g(\theta,X)=s(\theta,X)\), the estimate in (9.12) is an asymptotically efficient estimate.

**Theorem 9.4**: _Suppose_

1. \(\tilde{\theta}\) _is a_ \(\sqrt{n}\)_-consistent estimator of_ \(\theta_{0}\)_;_
2. \(g\) _is an unbiased,_ \(P_{\theta}\)_-square-integrable estimating equation such that_ \(g(\theta,x)\)__\(f_{\theta}(x)\) _satisfies_ \(\text{DUI}^{+}(\theta,\mu)\)_;_
3. _the sequence of random functions_ \(\{E_{n}[\partial g(\theta,X)/\partial\theta^{T}]:n=1,2,\ldots\}\) _is stochastic equicontinuous in a neighborhood of_ \(\theta_{0}\)_;_
4. \(J_{g}(\theta)\) _is continuous in_ \(\theta\)_._

_If \(\tilde{\theta}_{g}\) is defined as (9.13), then_

\[\sqrt{n}(\tilde{\theta}_{g}-\theta_{0})\overset{\mathcal{D}}{\to}N\left(0,I_{ g}^{-1}(\theta_{0})\right).\]

[MISSING_PAGE_FAIL:286]

### Asymptotic linear form

In Section 9.5 we have shown that, if \(\hat{\theta}_{g}\) is consistent, then \(\sqrt{n}(\hat{\theta}_{g}-\theta_{0})\) converges in distribution to a Normal random vector, which, in turn, implies that \(\hat{\theta}_{g}\) is \(\sqrt{n}\)-consistent. In this section we develop a stronger asymptotic property for \(\hat{\theta}_{g}\) than the asymptotic normality.

**Theorem 9.5**: _If the conditions in Theorem 9.3 hold, then_

\[\hat{\theta}_{g}=\theta_{0}-J_{g}^{-1}(\theta_{0})\,E_{n}[g(\theta_{0},X)]+o_{ P}(n^{-1/2}). \tag{9.17}\]

Note that, by straightforward applications of the Central Limit Theorem and Slutsky's theorem, we can deduce from (9.17) the asymptotic Normality

\[\sqrt{n}(\hat{\theta}_{g}-\theta_{0})\stackrel{{\mathcal{D}}}{{ \rightarrow}}N(0,I_{g}^{-1}(\theta_{0})).\]

Proof of Theorem 9.5.: Since, by Theorem 9.3, \(\hat{\theta}_{g}\) is \(\sqrt{n}\)-consistent, and by the proof of Theorem 9.4, the relation (9.16) holds for \(\hat{\theta}_{g}\). In this case, \(E_{n}[g(\hat{\theta}_{g},X)]=0\) because \(\hat{\theta}_{g}\) is a solution to the equation \(E_{n}[g(\theta,X)]=0\). Hence

\[0=E_{n}[g(\theta_{0},X)]+J_{g}(\theta_{0})(\hat{\theta}_{g}-\theta_{0})+o_{P}( n^{-1/2}),\]

which implies the desired result. 

An estimate that satisfies a relation such as (9.17) is said to be asymptotically linear. See, for example, Bickel, Klaassen, Ritov, and Wellner (1993). More generally, we have the following definition.

**Definition 9.6**: _Suppose \(X_{1},\ldots,X_{n}\) are independent copies of a random vector \(X\), whose \(P_{\theta_{0}}\) belongs to a parametric family \(\{P_{\theta}:\theta\in\Theta\subseteq\mathbb{R}^{p}\}\). An estimate \(\hat{\theta}\) of \(\theta_{0}\) is asymptotically linear if_

\[\hat{\theta}=\theta_{0}+E_{n}[\psi(\theta_{0},X)]+o_{P}(n^{-1/2}), \tag{9.18}\]

_where_

1. \(E[\psi(\theta_{0},X)]=0\)_;_
2. _the covariance matrix_ \(\mbox{var}[\psi(\theta_{0},X)]\) _has finite elements._

Note that, because \(\psi(\theta_{0},X)\) is a mean \(0\) function, the term \(E_{n}[\psi(\theta_{0},X)]\) in (9.18) is of the order \(O_{P}(n^{-1/2})\) by the Central Limit Theorem. This condition is satisfied by a large number of statistics. The term "asymptotic linear" refers to the fact that, after ignoring the term \(o_{P}(n^{-1/2})\), the leading term is a linear functional of the empirical distribution \(F_{n}\). A typical estimate is asymptotically linear; whereas a typical test statistic is asymptotically quadratic. The function \(\psi\) in (9.18) is called the influence function. So, for example,the influence function of \(\hat{\theta}_{g}\) is \(-J_{g}^{-1}(\theta_{0})g(\theta_{0},X)\). By the Central Limit Theorem, an asymptotically linear estimate \(\hat{\theta}\) has the following asymptotic Normal distribution:

\[\sqrt{n}(\hat{\theta}-\theta_{0})\stackrel{{\mathcal{D}}}{{ \rightarrow}}N\left(0,\text{var}[\psi(\theta_{0},X)]\right).\]

Two important special cases are when \(g\) is the score function \(s\) or the optimal estimating equation \(g^{*}\in\mathcal{G}\). In both cases,

\[I_{g^{*}}(\theta)= K_{g^{*}}(\theta)=-J_{g^{*}}(\theta),\quad I(\theta)=K(\theta)=-J (\theta).\]

Thus, the maximum likelihood estimate \(\hat{\theta}\) and a consistent solution \(\hat{\theta}_{g^{*}}\) of an optimal estimating equation \(g^{*}\) have the following asymptotic linear forms:

\[\hat{\theta}= \,\theta_{0}+I^{-1}(\theta_{0})E_{n}[s(\theta_{0},X)]+o_{P}(n^{- 1/2}),\] \[\hat{\theta}_{g^{*}}= \,\theta_{0}+I_{g^{*}}^{-1}(\theta_{0})E_{n}[g^{*}(\theta_{0},X)] +o_{P}(n^{-1/2}).\]

The asymptotic linear form provides more information about an estimate than the asymptotic distribution. For example, if we know the asymptotic linear forms of two estimates are

\[\hat{\theta}= \,\theta_{0}+E_{n}\psi_{1}(\theta_{0},X)+o_{P}(n^{-1/2}),\] \[\tilde{\theta}= \,\theta_{0}+E_{n}\psi_{2}(\theta_{0},X)+o_{P}(n^{-1/2}),\]

then we know the joint asymptotic distribution of \([\sqrt{n}(\hat{\theta}-\theta_{0}),\sqrt{n}(\tilde{\theta}-\theta_{0})]\) is

\[N\left(0,\begin{pmatrix}E[\psi_{1}(\theta_{0},X)\psi_{1}^{T}(\theta_{0},X)]&E[ \psi_{1}(\theta_{0},X)\psi_{2}^{T}(\theta_{0},X)]\\ E[\psi_{2}(\theta_{0},X)\psi_{1}^{T}(\theta_{0},X)]&E[\psi_{2}(\theta_{0},X) \psi_{2}^{T}(\theta_{0},X)]\end{pmatrix}\right).\]

However, if we only know the asymptotic distributions of the random vectors \(\sqrt{n}(\hat{\theta}-\theta_{0})\) and \(\sqrt{n}(\hat{\theta}-\theta_{0})\), then we cannot deduce the joint asymptotic distribution of the two random vectors. Fortunately, in most cases where we know a statistic is asymptotically Normal, its asymptotic linear form is readily available.

## 9.8 Efficient score for parameter of interest

In this section we take a closer look at the efficient score described at the end of Section 9.4. We will consider the more general case where \(\theta\), \(\psi\), and \(\lambda\) are vectors. That is, \(\theta\) is a \(p\)-dimensional parameters consisting of an \(r\)-dimensional subvector \(\psi\), which is the parameter of interest, and an \(s\)-dimensional subvector \(\lambda\), which is the nuisance parameter.

For the purpose of estimating \(\psi\), we need to extend the notion of the information about the whole parameter \(\theta\) contained in an estimating equation \(g(\theta,x)\), as defined in Definition 9.3, to that of the information about a part (\(\psi\)) of the parameter \(\theta\) contained in the estimating equation \(g(\theta,x)\).

**Definition 9.7**: _Suppose \(g:\Theta\times\Omega_{X}\rightarrow\mathbb{R}^{r}\) is an unbiased, \(P_{\theta}\)-square-integrable estimating equation such that \(g(\theta,x)f_{\theta}(x)\) satisfies DUI\({}^{+}(\psi,\mu)\). Then the matrix_

\[I_{g}(\psi|\theta)=E_{\theta}\left(\frac{\partial g^{T}(\theta,X)}{\partial\psi }\right)\left\{E_{\theta}\left[g(\theta,X)g^{T}(\theta,X)\right]\right\}^{+}E _{\theta}\left(\frac{\partial g(\theta,X)}{\partial\psi^{T}}\right)\]

_is called the information about \(\psi\) contained in \(g(\theta,X)\)._

This is a generalization of the information matrix \(I_{g}(\theta)\) in Definition 9.3 because, when \(\psi\) is the entire parameter \(\theta\), \(I_{g}(\theta|\theta)=I_{g}(\theta)\). The optimal estimating equation for \(\psi\) can be found in the same way as that for \(\theta\). Recall that \(s_{\psi}\) stands for the \(\psi\)-component of \(s(\theta,X)\). That is, \(s_{\psi}(\theta,x)=\partial\log f_{\theta}(x)/\partial\psi\). The proof of the next theorem is similar to that of Theorem 9.1 and is left as an exercise.

**Theorem 9.6**: _Suppose \(\mathcal{G}\) is a class of unbiased, \(P_{\theta}\)-square-integrable estimating equations of dimension \(r\) such that, for each \(g\in\mathcal{G}\), \(g(\theta,x)f_{\theta}(x)\) satisfies DUI\({}^{+}(\theta,\mu)\). If there is a member \(g^{*}\) of \(\mathcal{G}\) such that \([s_{\psi}-g^{*},g]=0\) for all \(g\in\mathcal{G}\), then \(I_{g^{*}}(\psi|\theta)\succeq I_{g}(\psi|\theta)\) for all \(g\in\mathcal{G}\)._

When estimating \(\psi\), it is natural to consider a class of estimating equations that are, in some sense, insensitive to the nuisance parameter \(\lambda\). We now give a formal definition for such estimating equations.

**Definition 9.8**: _An unbiased and \(P_{\theta}\)-square-integrable estimating equation \(g:\Theta\times\Omega_{X}\rightarrow\mathbb{R}^{r}\) is said to be insensitive to \(\lambda\) to the first order if \(g(\theta,x)f_{\theta}(x)\) satisfies DUI\({}^{+}(\theta,\mu)\), and_

\[E\left(\frac{\partial g(\theta,X)}{\partial\lambda^{T}}\right)=0. \tag{9.19}\]

_Let \(\mathcal{G}_{\psi\cdot\lambda}\) denote the class of such estimating equations._

The next proposition gives a characterization of the space \(\mathcal{G}_{\psi\cdot\lambda}\).

**Proposition 9.2**: _Let \(g:\Theta\times\Omega_{X}\rightarrow\mathbb{R}^{r}\) be an unbiased, \(P_{\theta}\)-square-integrable estimating equation such that \(g(\theta,x)f_{\theta}(x)\) satisfies DUI\({}^{+}(\theta,\mu)\). Then \(g\) belongs to \(\mathcal{G}_{\psi\cdot\lambda}\) if and only if \([g,s_{\lambda}]=0\)._

Proof: By the DUI\({}^{+}\) assumption,

\[\int(\partial g/\partial\lambda^{T})f_{\theta}d\mu+\int gs_{\lambda}f_{\theta} d\mu=0.\]

Hence \([g,s_{\lambda}]=0\) if and only if \(g\in\mathcal{G}_{\psi\cdot\lambda}\). \(\Box\)

By Problem 9.14 we see that the optimal estimating equation in \(\mathcal{G}_{\psi\cdot\lambda}\) that maximizes \(I_{g}(\psi|\theta)\) is\[s_{\psi}-[s_{\psi},s_{\lambda}][s_{\lambda},s_{\lambda}]^{-1}s_{\lambda}.\]

Note that the Fisher information for \(\theta\) can be written as the matrix

\[I(\theta)=\begin{pmatrix}[s_{\psi},s_{\psi}]&[s_{\psi},s_{\lambda}]\\ &[s_{\lambda},s_{\psi}]&[s_{\lambda},s_{\lambda}]\end{pmatrix}\equiv\begin{pmatrix} I_{\psi\psi}&I_{\psi\lambda}\\ I_{\lambda\psi}&I_{\lambda\lambda}\end{pmatrix}.\]

Thus the optimal estimating equation in \(\mathcal{G}_{\psi\cdot\lambda}\) can be written as

\[g^{*}=s_{\psi}-I_{\psi\lambda}I_{\lambda\lambda}^{-1}s_{\lambda}. \tag{9.20}\]

Note that the information for \(\psi\) contained in \(g^{*}(\theta,X)\) is

\[I_{g^{*}}(\psi|\theta)=I_{\psi\psi}-I_{\psi\lambda}I_{\lambda\lambda}^{-1}I_{ \lambda\psi}. \tag{9.21}\]

Because of the special importance of the optimal estimating equation \(g^{*}\) in (9.20) and the information \(I_{g^{*}}(\psi|\theta)\) in statistical inference, we give them special names and notations.

**Definition 9.9**: _The optimal estimating equation \(g^{*}(\theta,X)\) in (9.20) is called the efficient score for \(\psi\), and is written as \(s_{\psi\cdot\lambda}(\theta,X)\); the information \(I_{g^{*}}(\psi|\theta)\) in (9.21) is called the efficient information for \(\psi\), and is written as \(I_{\psi\cdot\lambda}(\theta)\)._

Before proceeding further, let us review some formulas about inversion of a block matrix. The next proposition can be proved by straightforward matrix multiplication.

**Proposition 9.3** (Inversion of a block matrix): _Let \(B\) be a \(p\) by \(p\) nonsingular and symmetric matrix, which is partitioned into_

\[B=\begin{pmatrix}B_{11}&B_{12}\\ B_{21}&B_{22}\end{pmatrix},\]

_where \(B_{11}\) is an \(r\times r\) matrix, \(B_{12}=B_{21}^{T}\) is an \(r\times s\) matrix, and \(B_{22}\) is an \(s\times s\) matrix, with \(r+s=p\). Let the inverse \(B^{-1}\) of \(B\) be partitioned, in accordance with the above dimensions, as_

\[B^{-1}=\begin{pmatrix}(B^{-1})_{11}&(B^{-1})_{12}\\ (B^{-1})_{21}&(B^{-1})_{22}\end{pmatrix}.\]

_Then_

\[(B^{-1})_{11}= (B_{11}-B_{12}B_{22}^{-1}B_{21})^{-1}\] \[(B^{-1})_{22}= (B_{22}-B_{21}B_{11}^{-1}B_{12})^{-1}\] \[(B^{-1})_{12}= -(B^{-1})_{11}B_{12}B_{22}^{-1}=-B_{11}^{-1}B_{12}(B^{-1})_{22}=( B^{-1})_{21}^{T}.\]

Partition the inverse Fisher information \(I^{-1}(\theta)\) into block matrix 

[MISSING_PAGE_FAIL:291]

\[\hat{\psi}-\psi_{0}= I^{-1}_{\psi\cdot\lambda}E_{n}\left[s_{\psi}(\theta_{0},X)-I_{\psi \lambda}I^{-1}_{\lambda\lambda}s_{\lambda}(\theta_{0},X)\right]+o_{P}(n^{-1/2})\] \[= I^{-1}_{\psi\cdot\lambda}E_{n}s_{\psi\cdot\lambda}(\theta_{0},X)+ o_{P}(n^{-1/2}),\]

as desired. 

From the above expansion we can easily write down the asymptotic distribution of the MLE for the parameter of interest.

**Corollary 9.2**: _Under the assumptions in Theorem 9.7, we have_

\[\sqrt{n}(\hat{\psi}-\psi_{0})\stackrel{{\mathcal{D}}}{{\to}}N(0,I ^{-1}_{\psi\cdot\lambda}(\theta_{0})).\]

Recall that \(s_{\psi\cdot\lambda}\) is the optimal estimating equation in \(\mathcal{G}_{\psi\cdot\lambda}\). Thus \(I_{\psi\cdot\lambda}(\theta)\) is the upper bound of the information \(I_{g}(\psi|\theta)\) for any estimating equation in \(\mathcal{G}_{\psi\cdot\lambda}\). Meanwhile, if we pretend \(\lambda_{0}\) to be known, then Theorem 9.3 implies that any consistent solution \(\hat{\psi}_{g}\) to the estimating equation

\[E_{n}[g(\psi,\lambda_{0},X)]=0\]

has asymptotic distribution \(\sqrt{n}(\hat{\psi}_{g}-\psi_{0})\stackrel{{\mathcal{D}}}{{\to}}N (0,I^{-1}_{g}(\psi_{0}|\theta_{0}))\). Thus, intuitively, we can say that \(\hat{\psi}\) has the smallest asymptotic variance among the solutions to all estimating equations in \(\mathcal{G}_{\psi\cdot\lambda}\). Of course, this statement is not rigorous as we pretend \(\lambda_{0}\) to be known. This statement will be made rigorous by Theorem 9.9.

The efficient score and efficient information share some similarities with the score \(s(\theta,X)\) and the information \(I(\theta)\) when there is no nuisance parameter. For example, the information identity has its analogy for the efficient score.

**Theorem 9.8**: _Suppose \(s_{\psi\cdot\lambda}(\theta,X)\) is \(P_{\theta}\)-square-integrable, and \(f_{\theta}(x)\) and \(s_{\psi\cdot\lambda}(\theta,x)f_{\theta}(x)\) satisfy DUI\({}^{+}\)(\(\theta,\mu\)). Then_

\[E_{\theta}[s_{\psi\cdot\lambda}(\theta,X)]= \,0, \tag{9.23}\] \[E_{\theta}\left[\frac{\partial s_{\psi\cdot\lambda}(\theta,X)}{ \partial\lambda^{T}}\right]= \,0,\] (9.24) \[E_{\theta}\left[\frac{\partial s_{\psi\cdot\lambda}(\theta,X)}{ \partial\psi^{T}}\right]= \,-E\left[s_{\psi\cdot\lambda}(\theta,X)s^{T}_{\psi\cdot\lambda}( \theta,X)\right]=-I_{\psi\cdot\lambda}(\theta). \tag{9.25}\]

Proof: The equality (9.23) follows from \(E_{\theta}[s(\theta,X)]=0\), as can be verified by differentiating the equation \(\int f_{\theta}(x)d\mu(x)=1\) with respect to \(\theta\).

To establish (9.24), first observe that by the definition of \(s_{\psi\cdot\lambda}(\theta,X)\), we have

\[E_{\theta}\left[\frac{\partial s_{\psi\cdot\lambda}(\theta,X)}{\partial \lambda^{T}}\right]=E_{\theta}\left[\frac{\partial s_{\psi}(\theta,X)}{ \partial\lambda^{T}}\right]-E_{\theta}\left[\frac{\partial(I_{\psi\lambda}I^{- 1}_{\lambda\lambda}s_{\lambda}(\theta,X))}{\partial\lambda^{T}}\right]. \tag{9.26}\]

The second term on the right is \[-E_{\theta}\left[\frac{\partial(I_{\psi\lambda}I_{\lambda\lambda}^{-1})}{\partial \lambda^{T}}s_{\lambda}(\theta,X)+I_{\psi\lambda}I_{\lambda\lambda}^{-1}\frac{ \partial s_{\lambda}(\theta,X)}{\partial\lambda^{T}}\right],\]

where the notation \([\partial(I_{\psi\lambda}I_{\lambda\lambda}^{-1})/\partial\lambda^{T}]s_{ \lambda}(\theta,X)\) simply means the matrix

\[\sum_{i=1}^{s}\frac{\partial(I_{\psi\lambda}I_{\lambda\lambda}^{-1})}{\partial \lambda_{i}}s_{\lambda_{i}}(\theta,X),\]

\(s_{\lambda_{i}}(\theta,X)\) being the score function for \(\lambda_{i}\), \(\partial\log f_{\theta}(X)/\partial\lambda_{i}\). Since the above term has expectation \(0\),

\[E_{\theta}\left[\frac{\partial(I_{\psi\lambda}I_{\lambda\lambda}^{-1}s_{ \lambda}(\theta,X))}{\partial\lambda^{T}}\right]=E_{\theta}\left[I_{\psi \lambda}I_{\lambda\lambda}^{-1}\frac{\partial s_{\lambda}(\theta,X)}{\partial \lambda^{T}}\right]=-I_{\psi\lambda}I_{\lambda\lambda}^{-1}I_{\lambda\lambda}= -I_{\psi\lambda}.\]

Hence the right-hand side of (9.26) reduces to \(-I_{\psi\lambda}+I_{\psi\lambda}=0\), proving (9.24).

Finally, to establish (9.25), first differentiate the equation

\[\int s_{\psi\cdot\lambda}(\theta,x)f_{\theta}(x)d\mu(x)=0, \tag{9.27}\]

with respect to \(\lambda\) to obtain

\[E_{\theta}\left[\frac{\partial s_{\psi\cdot\lambda}(\theta,X)}{\partial\lambda ^{T}}\right]=-E\left[s_{\psi\cdot\lambda}(\theta,X)s_{\lambda}^{T}(\theta,X) \right].\]

By (9.24), the left-hand side is \(0\), and hence \(E\left[s_{\psi\cdot\lambda}(\theta,X)s_{\lambda}^{T}(\theta,X)\right]=0\), leading to

\[E\left[s_{\psi\cdot\lambda}(\theta,X)s_{\psi}^{T}(\theta,X)\right]=E\left[s_{ \psi\cdot\lambda}(\theta,X)s_{\psi\cdot\lambda}^{T}(\theta,X)\right].\]

Next, differentiate the equation (9.27) with respect to \(\psi\) to obtain

\[E_{\theta}\left[\frac{\partial s_{\psi\cdot\lambda}(\theta,X)}{\partial\psi^{ T}}\right]=-E\left[s_{\psi\cdot\lambda}(\theta,X)s_{\psi}^{T}(\theta,X)\right]=-E \left[s_{\psi\cdot\lambda}(\theta,X)s_{\psi\cdot\lambda}^{T}(\theta,X)\right],\]

proving (9.25). \(\Box\)

One way to use estimating equations in \(\mathcal{G}_{\psi\cdot\lambda}\), such as the efficient score \(s_{\psi\cdot\lambda}(\theta,X)\), is to estimate \(\psi\) by solving the equation

\[E_{n}[g(\psi,\tilde{\lambda},X)]=0, \tag{9.28}\]

where \(\tilde{\lambda}\) is some estimate of the nuisance parameter \(\lambda_{0}\). Since an estimating equation in \(\mathcal{G}_{\psi\cdot\lambda}\) is insensitive to the nuisance parameter \(\lambda_{0}\), intuitively, it should be able to tolerate a relatively poor estimate of \(\lambda_{0}\) while still producing an accurate estimate of \(\psi\). Indeed, the next theorem shows that even if \(\tilde{\lambda}-\lambda_{0}=o_{P}(n^{-1/4})\), which can be much slower than the parametric rate \(O_{P}(n^{-1/2})\), the solution to (9.28) produces an estimate for \(\psi\) that is asymptotically equivalent to the solution to \(E_{n}[g(\psi,\lambda_{0},X)]=0\), where \(\lambda_{0}\) is treated as known.

We need to introduce some additional notations. Suppose \(h(\lambda)\) is a function from \(\mathbb{R}^{s}\) to \(\mathbb{R}^{r}\) with components \(h_{1}(\lambda),\ldots,h_{r}(\lambda)\). We use \(\partial^{2}h(\lambda)/\partial\lambda\partial\lambda^{T}\) denote the \(r\times s\times s\) array

\[\left\{\frac{\partial^{2}h_{i}(\lambda)}{\partial\lambda_{j}\partial\lambda_{k }}:i=1,\ldots,r,j,k=1,\ldots,s\right\}.\]

Furthermore, if \(a,b\in\mathbb{R}^{s}\), then the notation \(a^{T}\left[\partial^{2}h(\lambda)/\partial\lambda\partial\lambda^{T}\right]b\) represents the \(r\)-dimensional vector whose \(i\)th component is

\[\sum_{j=1}^{s}\sum_{k=1}^{s}a_{j}b_{k}\,\frac{\partial^{2}h_{i}(\lambda)}{ \partial\lambda_{j}\partial\lambda_{k}}.\]

For an estimating equation \(g\in\mathcal{G}_{\psi\cdot\lambda}\), let

\[J_{g}(\psi|\theta)=E_{\theta}\left[\frac{\partial g(\theta,X)}{\partial\psi^{ T}}\right],\quad K_{g}(\psi|\theta)=E_{\theta}\left[g(\theta,X)g^{T}(\theta,X) \right],\]

so that we have

\[I_{g}(\psi|\theta)=J_{g}(\psi|\theta)^{T}K_{g}^{-1}(\psi|\theta)J_{g}(\psi| \theta).\]

As before, we use \(\mathbb{N}\) to denote the set of natural numbers \(\{1,2,\ldots\}\). To our knowledge, the following result has not been recorded in the statistical literature.

**Theorem 9.9**: _Suppose that \(g\) is an estimating equation in \(\mathcal{G}_{\psi\cdot\lambda}\) satisfying the following additional conditions:_

1. \(g(\theta,X)\) _is twice differentiable with respect to_ \(\lambda\) _and the entries of the_ \(r\times s\times s\) _array;_
2. _in a neighborhood of_ \(\theta_{0}\)_, the sequences of random elements_ \[\left\{E_{n}\left[\frac{\partial g(\psi,\lambda,X)}{\partial\psi^{T}}\right]: \,n\in\mathbb{N}\right\},\quad\left\{E_{n}\left[\frac{\partial^{2}g(\psi, \lambda,X)}{\partial\lambda\partial\lambda^{T}}\right]:\,n\in\mathbb{N}\right\}\] _are stochastically equicontinuous;_
3. _the matrices_ \(J_{g}(\psi|\theta)\) _and_ \(K_{g}(\psi|\theta)\) _are nonsingular._

_If \(\tilde{\lambda}\) is an estimate of \(\lambda_{0}\) such that \(\tilde{\lambda}-\lambda_{0}=o_{P}(n^{-1/4})\), and \(\hat{\psi}\) is a consistent solution to the estimating equation \(E_{n}[g(\psi,\tilde{\lambda},X)]=0,\) then_

\[\sqrt{n}(\hat{\psi}-\psi_{0})\stackrel{{\mathcal{D}}}{{\to}}N \left(0,I_{g}^{-1}(\psi_{0}|\theta_{0})\right).\]Proof: Applying Taylor's mean value theorem to the function \(\psi\mapsto E_{n}[g(\psi,\tilde{\lambda},X)]\), we have

\[0=E_{n}[g(\hat{\psi},\tilde{\lambda},X)]=E_{n}[g(\psi_{0},\tilde{\lambda},X)]+E_{ n}\left[\frac{g(\xi,\tilde{\lambda},X)}{\partial\psi^{T}}\right](\hat{\psi}-\psi_{0})\]

for some \(\xi\) between \(\psi_{0}\) and \(\hat{\psi}\). Because \((\xi,\tilde{\lambda})\) converges in probability to \((\psi_{0},\lambda_{0})\), by the first equicontinuity assumption in \(2\) and Corollary 8.2, we see that \(E_{n}[g(\xi,\tilde{\lambda},X)/\partial\psi^{T}]\) differs from \(J(\psi_{0}|\theta_{0})\) by \(o_{P}(1)\). Hence

\[0=E_{n}[g(\psi_{0},\tilde{\lambda},X)]+[J_{g}(\psi_{0}|\theta_{0})+o_{P}(1)]( \hat{\psi}-\psi_{0}). \tag{9.29}\]

Next, applying (the second-order) Taylor's mean-value theorem to the function \(\lambda\mapsto E_{n}[g(\psi_{0},\lambda,X)]\), we have

\[\begin{split} E_{n}[g(\psi_{0},\tilde{\lambda},X)]=& \,E_{n}[g(\psi_{0},\lambda_{0},X)]+E_{n}\left[\frac{\partial g(\psi_{0}, \lambda_{0},X)}{\partial\lambda^{T}}\right](\tilde{\lambda}-\lambda_{0})\\ &+\frac{1}{2}(\tilde{\lambda}-\lambda_{0})^{T}E_{n}\left[\frac{ \partial^{2}g(\psi_{0},\lambda_{1},X)}{\partial\lambda\partial\lambda^{T}} \right](\tilde{\lambda}-\lambda_{0}),\end{split} \tag{9.30}\]

for some \(\lambda_{1}\) on the line joining \(\tilde{\lambda}\) and \(\lambda_{0}\). By the second equicontinuity assumption in \(2\) and Corollary 8.2,

\[E_{n}\left[\frac{\partial^{2}g(\psi_{0},\lambda^{\dagger},X)}{\partial\lambda \partial\lambda^{T}}\right]\stackrel{{ P}}{{\to}}E\left[\frac{ \partial^{2}g(\psi_{0},\lambda_{0},X)}{\partial\lambda\partial\lambda^{T}} \right].\]

Therefore, the term on the left-hand side above is \(O_{P}(1)\), and hence the third term on the right-hand side of (9.30) is of the order \(o_{P}(n^{-1/2})\). Moreover, because \(g\in\mathcal{G}_{\psi\cdot\lambda}\), we have \(E[\partial g(\theta_{0},X)/\partial\lambda^{T}]=0\). Hence, by the central limit theorem,

\[E_{n}\left[\frac{\partial g(\psi_{0},\lambda_{0},X)}{\partial\lambda^{T}} \right]=O_{P}(n^{-1/2}),\]

which implies that the second term in (9.30) is of the order \(o_{P}(n^{-3/4})\). So the following approximation holds:

\[E_{n}[g(\psi_{0},\tilde{\lambda},X)]=E_{n}[g(\psi_{0},\lambda_{0},X)]+o_{P}(n^ {-1/2}).\]

Substituting this into the right-hand side of (9.29) results in

\[0=E_{n}[g(\psi_{0},\lambda_{0},X)]+[J_{g}(\psi_{0}|\theta_{0})+o_{P}(1)](\hat{ \psi}-\psi_{0})+o_{P}(n^{-1/2}).\]

Multiplying both sides of the above equation by the matrix \(J_{g}^{-1}(\psi_{0}|\theta_{0})\) from the left, we obtain

\[[I_{r}+o_{P}(1)](\hat{\psi}-\psi_{0})=-J_{g}^{-1}(\psi_{0}|\theta_{0})E_{n}[g( \psi_{0},\lambda_{0},X)]+o_{P}(n^{-1/2}).\]Hence, by the central limit theorem and Slutsky's theorem,

\[\sqrt{n}[I_{r}+o_{P}(1)](\hat{\psi}-\psi_{0})\stackrel{{\mathcal{D}}} {{\to}}N(0,I_{g}(\psi_{0}|\theta_{0})),\]

which implies the asserted result by Problem 8.13. 

An important special case of Theorem 9.9 is when \(g\) is the efficient score, which says that for any \(\tilde{\lambda}\) that converges to \(\lambda_{0}\) at a rate faster than \(n^{-1/4}\), a consistent solution to

\[E_{n}[s_{\psi\cdot\lambda}(\psi,\tilde{\lambda},X)]=0\]

has the asymptotic distribution \(N(0,I_{\psi\cdot\lambda}(\theta_{0})^{-1})\). This result also makes precise the optimal statement following Corollary 9.2. That is, for any estimating equation \(g\in\mathcal{G}_{\psi\cdot\lambda}\), and any estimate \(\tilde{\lambda}\) satisfying \(\tilde{\lambda}=\lambda_{0}+o_{P}(n^{-1/4})\), the asymptotic variance of the solution to

\[E_{n}[g(\psi,\tilde{\lambda},X)]=0\]

reaches its lower bound (in terms of Louwner's ordering) when \(g\) is the efficient score \(s_{\psi\cdot\lambda}\).

## Problems

**9.1.** Prove Theorem 9.2 by following the proof of Theorem 8.1.

**9.2.** Let \(X_{1},\cdots,X_{n}\) be i.i.d. with density \(f(x;\mu)=x^{\mu-1}e^{-x}/\Gamma(\mu)\).

1. Find all the solutions to the estimating equation \[\sum_{i=1}^{n}(X_{i}^{2}-\mu-\mu^{2})=0,\] (9.31) and decide which one is consistent. Derive the asymptotic distribution of this solution.
2. Find the maximum likelihood estimate of \(\mu\) and derive its asymptotic distribution.
3. Let \(\hat{\mu}^{(1)}=\bar{X}\), \(\hat{\mu}^{(2)}\) be the consistent solution to the estimating equation (9.31), and \(\hat{\mu}^{(3)}\) be the maximum likelihood estimate of \(\mu\). Let \(V_{1}(\mu)\), \(V_{2}(\mu)\), and \(V_{3}(\mu)\) be the asymptotic variances of the above three estimators of \(\mu\). Using a computer to plot them against \(\mu\). What is your conclusion?

**9.3.** Suppose that \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) are an i.i.d. sample from \((X,Y)\), where \(X\) is a random vector in \(\mathbb{R}^{p}\), and \(Y\) is a random variable. Suppose that conditional distribution of \(Y|X\) is given by \(N(e^{\beta^{T}x},e^{\beta^{T}x})\) for some \(\beta\in\mathbb{R}^{p}\), and that the marginal distribution of \(X\) does not depend on \(\beta\).

1. Derive the score function \(s(\beta,X,Y)\).

2. Derive the Fisher information matrix \(I(\beta)\).
3. Derive the quasi score function \(g^{*}(\beta,X,Y)\).
4. Derive the information contained in \(g^{*}\), and show that this information is smaller (in terms of Louwner's ordering) than obtained in part 2.
5. Write down Fisher scoring iterative algorithms for estimating the maximum likelihood estimate and maximum quasi likelihood estimate.

**9.4.** In the setting of Problem 9.3, suppose we estimate \(\beta\) by the Least Squares method -- that is, by minimizing

\[E_{n}(Y-e^{\beta^{T}X})^{2}\]

over \(\beta\in\mathbb{R}^{p}\) to estimate \(\beta\).

1. Derive the estimating equation for \(\beta\).
2. Compute the information contained in this estimating equation, and show that it is smaller (in terms of Louwner's ordering) than that obtained in part 4 of Problem 9.3.

**9.5.** Suppose \(\theta\in\Theta\subseteq\mathbb{R}^{p}\) is a \(p\)-dimensional parameter and \(g_{1},\ldots,g_{m}\) are \(p\)-dimensional unbiased, \(P_{\theta}\)-square-integrable estimating equations such that \(g_{i}(x)f_{\theta}(x)\), \(i=1,\ldots,m\), satisfy \(\mbox{DUI}^{+}(\theta,\mu)\). Consider the following class of unbiased estimating equations

\[\mathcal{G}=\{A_{1}(\theta)g_{1}(\theta,X)+\cdots+A_{m}(\theta)g_{m}(\theta,X) :\,A_{1}(\theta),\ldots,A_{m}(\theta)\in\mathbb{R}^{p\times p}\},\]

where \(A_{i}(\theta)\) are \(p\times p\) nonrandom matrices that may depend on \(\theta\). Derive the optimal equation \(g^{*}\) in \(\mathcal{G}\).

**9.6.** Consider the generalized estimating equation described in Section 9.3 with the following simplifications: \(n_{i}=m\) are the same for all \(i=1,\ldots,n\), and

\[\mu_{i}(X_{i},\beta)=\mu(X_{i}^{T}\beta),\quad V_{i}(X_{i},\beta)=V(X_{i}^{T} \beta),\quad R_{i}(\alpha)=R(\alpha).\]

Furthermore, assume that \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) are an i.i.d. sample from \((X,Y)\). These lead the following generalized estimating equation

\[E_{n}\left\{[\partial\mu(\beta^{T}X)^{T}/\partial\beta][V^{1/2}(\beta^{T}X)R( \alpha)V^{1/2}(\beta^{T}X)]^{-1}[Y-\mu(\beta^{T}X)]\right\}=0.\]

Denote the term in \(E_{n}\{\cdots\}\) by \(g(\beta,\alpha,X,Y)\). Assume that \(\hat{\alpha}\) converges in probability to a fixed vector \(\alpha_{0}\) with the rate \(o_{P}(n^{-1/4})\). Let \(\hat{\beta}\) be a consistent solution to the equation

\[E_{n}[g(\beta,\hat{\alpha},X,Y)]=0.\]

Let \(\beta_{0}\) be the true parameter.

1. Derive the asymptotic distribution of \(\sqrt{n}(\hat{\beta}-\beta_{0})\) when the form of \(R(\alpha)\) might be misspecified.
2. Derive the asymptotic distribution of \(\sqrt{n}(\hat{\beta}-\beta_{0})\) when the form of \(R(\alpha)\) is correctly specified.
3. Show that, when \(R\) is correctly specified, \(g(\beta,\alpha,X,Y)\) is the optimal estimating equation among estimating equations of the form \[A(X,\alpha,\beta)[Y-\mu(\beta^{T}X)],\] where \(A(X,\alpha,\beta)\) is an \(m\times m\) random matrix that may depend on \(X,\alpha,\beta\) but does not depend on \(Y\).

### 9.7.

Prove Theorem 9.6.

**9.8.** Let \(X=(X_{1},\ldots,X_{n})\) be a sample with joint distribution \(P_{\theta}\), where \(\theta\) is a two dimensional vector with a parameter of interest \(\psi\) and a nuisance parameter \(\lambda\). Suppose \(P_{\theta}\) has a density \(f_{\theta}\) with respect to a \(\sigma\)-finite measure \(\mu\). Suppose there is a sufficient statistic \(T=T(X_{1:n})\) for the nuisance parameter \(\lambda\) -- that is, the conditional density \(f_{X|T}(x|t;\psi)\) does not depend on \(\lambda\). Let \(\mathcal{G}_{\psi}\) be a class of estimating equations for \(\psi\) satisfying the following conditions:

1. each \(g(\psi,X)\) in \(\mathcal{G}_{\psi}\) is a function of \(X\) and \(\psi\);
2. \(E_{\psi,\lambda}[g(\psi,X)]=0\) for all values of \(\lambda\);
3. \(g(\psi,X)\) is \(P_{\theta}\)-square-integrable, and \(g(\psi,x)f_{\theta}(x)\) is differentiable with respect to \(\psi\) under the integral with respect to \(\mu\).

Let \(s_{X|T}(\psi,X)\) be the conditional score function \(\partial\log f_{X|T}(x|t;\psi)/\partial\psi\) and assume that it satisfies condition (iii). Note that this conditional score depends on both \(X\) and \(T\), but since \(T\) is a function of \(X\), we can write it as \(s_{X|T}(\psi,X)\).

1. Show that the conditional score satisfies (ii).
2. Assuming \(s_{X|T}(\psi;X)\) belongs to \(\mathcal{G}_{\psi}\), show that it is the optimal estimating equation in that class in terms of the information \(I_{g}(\psi|\theta)\).
3. Let \(s_{\psi}(\theta;X)\) be the unconditional score for \((\partial/\partial\psi)\log f_{\theta}(X)\). Show that

\[s_{X|T}(\psi,X)=s_{\psi}(\theta;X)-E(s_{\psi}(\theta;X)|T).\]

(This problem is inspired by Godambe (1976)).

**9.9.** Suppose \(X\sim\mbox{Poisson}(\lambda_{1})\), \(Y\sim\mbox{Poisson}(\lambda_{2})\), and \(X,Y\) are independent. We are interested in estimating \(\psi=\lambda_{1}/\lambda_{2}\), treating \(\lambda_{2}\) as the nuisance parameter. For simplicity, we denote \(\lambda_{2}\) by \(\lambda\). Let \(T=X+Y\).

1. Show that, for each fixed \(\psi\), \(T\) is sufficient for \(\lambda\).
2. Derive the conditional score function \(s_{X|T}(\psi,X)\).
3. Derive the information about \(\psi\) contained in the conditional score. Express it in terms of both the original parameter \((\lambda_{1},\lambda_{2})\) and the transformed parameter \((\psi,\lambda)\).

[MISSING_PAGE_EMPTY:6132]

2. Let \(\hat{\psi}\) be a consistent solution to the estimating equation \[E_{n}s_{\psi\cdot\lambda}(\psi,\tilde{\lambda},X)=0.\]

Derive the asymptotic distribution of \(\sqrt{n}(\hat{\psi}-\psi_{0})\).

3. Compare the asymptotic variances of \(\sqrt{n}(\tilde{\psi}-\psi_{0})\) and \(\sqrt{n}(\hat{\psi}-\psi_{0})\).

**9.17.** Suppose \(\theta\) is a \(p\)-dimensional parameter, consisting of a parameter of interest \(\psi\in\mathbb{R}^{r}\), and a nuisance parameter \(\lambda\in\mathbb{R}^{s}\), where \(r+s=p\). Let \(s_{\psi\cdot\lambda}(\psi,\lambda,X)\) be the efficient score, and let \(I_{\psi\cdot\lambda}(\psi,\lambda)\) be the efficient information. Suppose \(\tilde{\psi}\) and \(\tilde{\lambda}\) are estimates of \(\psi_{0}\) and \(\lambda_{0}\) such that

\[\tilde{\psi}-\psi_{0}=O_{P}(n^{-1/2}),\quad\tilde{\lambda}-\lambda_{0}=o_{P}(n ^{-1/4}).\]

Moreover, suppose:

1. \(s_{\psi\cdot\lambda}(\psi,\lambda,X)\) is differentiable with respect to \(\psi\) and twice differentiable with respect to \(\lambda\);
2. the entries of the arrays \[A(\psi,\lambda)=\frac{\partial s_{\psi\cdot\lambda}(\psi,\lambda,X)}{\partial \psi^{T}},\quad B(\psi,\lambda)=\frac{\partial^{2}s_{\psi\cdot\lambda}(\psi, \lambda,X)}{\partial\lambda\partial\lambda^{T}}\] are \(P_{\theta}\)-integrable;
3. the sequences of random arrays \[\{E_{n}[A(\psi,\lambda)]:n=1,2,\ldots\},\quad\{E_{n}[B(\psi,\lambda)]:n=1,2, \ldots\}\] are stochastic equicontinuous in an open ball centered at \(\theta_{0}=(\psi_{0}^{T},\lambda_{0}^{T})^{T}\).

Let \(\hat{\psi}\) be the one-step Newton-Raphson estimate for the parameter of interest \(\psi_{0}\), defined as

\[\hat{\psi}=\tilde{\psi}-I_{\psi\cdot\lambda}^{-1}(\tilde{\psi},\tilde{\lambda}) E_{n}[s_{\psi\cdot\lambda}(\tilde{\psi},\tilde{\lambda},X)].\]

1. Show that \[\hat{\psi}=\psi_{0}-I_{\psi\cdot\lambda}^{-1}(\psi_{0},\lambda_{0})E_{n}[s_{ \psi\cdot\lambda}(\psi_{0},\lambda_{0},X)]+o_{P}(n^{-1/2}).\]
2. Derive the asymptotic distribution of \(\sqrt{n}(\hat{\psi}-\psi_{0})\).

**9.18.** Suppose that the parameter \(\theta\in\mathbb{R}^{p}\) consists of a parameter of interest \(\psi\in\mathbb{R}^{r}\) and a nuisance parameter \(\lambda\in\mathbb{R}^{s}\), with \(r+s=p\). Let \(g(\theta,X)\) be a \(p\)-dimensional unbiased and \(P_{\theta}\)-square-integrable estimating equation such that \(g(\theta,x)f_{\theta}(x)\) satisfies \(\text{DUI}^{+}(\theta,\mu)\). Let \(g_{\psi}\) be the first \(r\) components of \(g\) and \(g_{\lambda}\) be the last \(s\) components of \(g\). Let \(J_{g}(\theta)\) and \(K_{g}(\theta)\) be the matrices defined in (9.10). Decompose \(J_{g}\) and \(J_{g}^{-1}\) as block matrices according to the dimensions of \(\psi\) and \(\lambda\), as follows:\[J_{g}=\begin{pmatrix}(J_{g})_{\psi\psi}&(J_{g})_{\psi\lambda}\\ (J_{g})_{\lambda\psi}&(J_{g})_{\lambda\lambda}\end{pmatrix},\quad J_{g}^{-1}= \begin{pmatrix}(J_{g}^{-1})_{\psi\psi}&(J_{g}^{-1})_{\psi\lambda}\\ (J_{g}^{-1})_{\lambda\psi}&(J_{g}^{-1})_{\lambda\lambda}\end{pmatrix}.\]

Let \(K_{g}\) and \(K_{g}^{-1}\) be decomposed similarly. Let

\[g_{\psi\cdot\lambda}(\theta,X)=g_{\psi}(\theta,X)-(J_{g})_{\psi\lambda}[(J_{g} )_{\lambda\lambda}]^{-1}g_{\lambda}(\theta,X).\]

Let \(\hat{\theta}\) be a consistent solution to \(E_{n}[g_{\psi\cdot\lambda}(\theta,X)]=0\), and let \(\hat{\psi}\) be its first \(r\) components. You may impose further regularity conditions such as stochastic equicontinuity.

1. Show that \[\hat{\psi}=\psi_{0}+(J_{g}^{-1})_{\psi\psi}E_{n}[g_{\psi\cdot\lambda}(\theta,X )]+o_{P}(n^{-1/2}).\]
2. Derive the asymptotic distribution of \(\sqrt{n}(\hat{\psi}-\psi_{0})\), and express the asymptotic variance in terms of the sub-matrices of \(J_{g}\) and \(K_{g}\).

**9.19**.: In the setting of Problem 9.18. Suppose \(\tilde{\lambda}\) is an estimate of \(\lambda_{0}\) satisfying \(\tilde{\lambda}-\lambda_{0}=o_{P}(n^{-1/4})\). Let \(\tilde{\psi}\) be a consistent solution to the estimating equation

\[E_{n}[g_{\psi\cdot\lambda}(\psi,\tilde{\lambda},X)]=0.\]

Show that \(\sqrt{n}(\hat{\psi}-\psi_{0})\) has the same asymptotic distribution as the one obtained in Problem 9.18.

## References

* [1]
* [2]Bhattacharyya1946] Bhattacharyya, A. (1946). On some analogues of the amount of information and their use in statistical estimation. _Sankhya: The Indian Journal of Statistics_. **8**, 1-14.
* [3]
* [4]Bickel, Klaassen, Ritov, Wellner, 1993] Bickel, P. J., Klaassen, C. A. J., Ritov, Y. Wellner, J. A. (1993). _Efficient and Adaptive Estimation for Semiparametric Models_. The Johns Hopkins University Press.
* [5]
* [6]Cox and Hinkley1974] Cox, D. R. and Hinkley, D. (1974). _Theoretical Statistics_. Chapman & Hall.
* [7]
* [8]Crowder1987] Crowder, M. (1987). On linear and quadratic estimating functions. _Biometrika_, **74**, 591-597.
* [9]
* [10]Durbin1960] Durbin, J. (1960). Estimation of parameters in time-series regression models. _Journal of the Royal Statistical Society, Series B_, **22**, 139-153.
* [11]
* [12]Godambe1960] Godambe, V. P. (1960). An optimum property of regular maximum likelihood estimation. _The Annals of Mathematical Statistics_, **31**, 1208-1211.
* [13]
* [14]Godambe1976] Godambe, V. P. (1976). Conditional likelihood and unconditional optimum estimating equations. _Biometrikca_, **63**, 277-284.
* [15]
* [16]Godambe and Thompson1989] Godambe, V. P. and Thompson, M. E. (1989). An extension of quasi-likelihood estimation. _Journal of Statistical Planning and Inference_, **22**, 137-152.
* [17]
* [18]* Hansen (1982) Hansen, L. P. (1982). Large sample properties of Generalized Method of Moments Estimators. _Econometrica_, **50**, 1029-1054.
* Heyde (1997) Heyde, C. C. (1997). _Quasi-Likelihood and its Application: a General Approach to Optimal Parameter Estimation._ Springer.
* Jarrett (1984) Jarrett, R. G. (1984). Bounds and expansions for Fisher information when the moments are known. _Biometrika_, **71**, 101-113.
* Li (1993) Li, B. (1993). A deviance function for the quasi-likelihood method. _Biometrika_, **80**, 741-753.
* Li (1996) Li, B. (1996). A minimax approach to consistency and efficiency for estimating equations. _The Annals of Statistics_, **24**, 1283-1297.
* Li and McCullagh (1994) Li, B. and McCullagh, P. (1994). Potential functions and conservative estimating functions. _The Annals of Statistics_, **22**, 340-356.
* Liang and Zeger (1986) Liang, K. Y. and Zeger, S. L. (1986). Longitudinal data analysis using generalized linear models. _Biometrika_, **73**, 13-22.
* Lindsay (1982) Lindsay, B. (1982). Conditional score functions: some optimality results. _Biometrika_, **69**, 503-512.
* McCullagh (1983) McCullagh, P. (1983). Quasi-likelihood functions. _The Annals of Statistics_, **11**, 59-67.
* Morton (1981) Morton, R. (1981). Efficiency of estimating equations and the use of pivots. _Biometrika_, **68**, 227-233.
* Small and McLeish (1989) Small, C. G. and McLeish, D. L. (1989). Projection as a method for increasing sensitivity and eliminating nuisance parameters. _Biometrika_, **76**, 693-703.
* Waterman and Lindsay (1996) Waterman, R. P and Lindsay, B. G. (1996). Projected score methods for approximating conditional scores. _Biometrika_, **83**, 1-13.
* Wedderburn (1974) Wedderburn, R. W. M. (1974). Quasi-likelihood functions, Generalized Linear Models, and the Gauss-Newton method. _Biometrika_, **61**, 439-447.
* Zeger and Liang (1986) Zeger, S. L. and Liang, K. Y. (1986). Longitudinal data analysis for discrete and continuous outcomes. _Biometrics_, **42**, 121-130.

## 10 Convolution Theorem and Asymptotic Efficiency

In Chapters 8 and 9 we have developed optimality of the maximum likelihood estimate among the class of solutions to estimating equations in terms of the information the asymptotic variance. The optimality of the maximum likelihood estimate, in fact, goes much deeper. In this chapter we show that the maximum likelihood estimate, as well as estimates that are asymptotically equivalent to it, have the smallest asymptotic variance among all regular estimates. This is a wide class of estimates that includes not only the asymptotically linear estimates such as the solutions to estimating equations, but also asymptotically nonlinear (and therefore asymptotically non-Gaussian) estimates. We systematically develop the theory underlying this general result: the framework of Local Asymptotic Normality and the Convolution Theorem (Le Cam 1953, 1960; Hajek 1970). This is an amazingly logical system that leads to far-reaching results with a small set of assumptions. Some techniques introduced in this chapter, such as Le Cam's third lemma and the convolution theorem, will also be useful for developing local alternative distributions for asymptotic hypothesis tests in the next chapter.

As a historical note, it had been known since Fisher (1922, 1925) that the maximum likelihood estimate has the smallest asymptotic variance among, roughly, all estimates that are asymptotically normal. But Fisher did not give a rigorous proof and counterexamples were found, the first of which by J. L. Hodges, Jr. in an unpublished paper, which was cited by Le Cam (1953). This led to intensive research in the ensuing years on what kind of estimates can reach Fisher's asymptotic variance lower bound, how wide a class of estimates the lower bound applies to, as well as how meaningful the counterexamples are. Hall and Mathiason (1990), van der Vaart (1997, 1998), and Le Cam and Yang (2000) are excellent references on this topic. Some of the developments here echo the logic lines in these works.

## 10.1 Contiguity

Recall that a probability measure \(P\) is absolutely continuous with respect to a probability measure \(Q\) if, for any measurable set \(A,\,Q(A)=0\) implies \(P(A)=0\). Contiguity is an analogue of this condition for two sequences of probability measures.

**Definition 10.1**: _Let \(\{P_{n}\}\) and \(\{Q_{n}\}\) be two sequences of probability measures. \(P_{n}\) is said to be contiguous with respect to \(Q_{n}\) if, for any sequence \(A_{n}\), \(Q_{n}(A_{n})\to 0\) implies \(P_{n}(A_{n})\to 0\). This property is written as \(P_{n}\lhd Q_{n}\). If \(P_{n}\lhd Q_{n}\) and \(Q_{n}\lhd P_{n}\), then \(P_{n}\) and \(Q_{n}\) are said to be mutually contiguous, and this property is expressed as \(P_{n}\lhd Q_{n}\)._

Even though contiguity between sequences of probability measures is analogous to absolute continuity between two probability measures, the latter does not imply the former logically. In particular, even if \(P_{n}\ll Q_{n}\) for every \(n\), that does not imply \(P_{n}\lhd Q_{n}\). For example, let \(P_{n}\) be the distribution of \(N(0,1/n)\) and \(Q_{n}\) be the distribution of \(N(1,1/n)\). In this case \(P_{n}\ll Q_{n}\) for each \(n\). The limiting distribution of \(P_{n}\) is a point mass at \(0\) and the limiting distribution of \(Q_{n}\) is a point mass at \(1\). So if, for each \(n\), \(A_{n}\) is the set \((-1/2,-1/2)\), then \(Q_{n}(A_{n})\to 0\) and yet \(P_{n}(A_{n})\to 1\). Proof of the next Proposition is left as an exercise.

**Proposition 10.1**: _The following statements are equivalent:_

_1. \(P_{n}\lhd Q_{n}\)._

_2. Whenever \(Q_{n}(A_{n})\to 1\), we have \(P_{n}(A_{n})\to 1\)._

_3. If \(T_{n}\) is any sequence of random variables with \(Q_{n}(|T_{n}|\geq\epsilon)\to 0\), then \(P_{n}(|T_{n}|\geq\epsilon)\to 0\)._

We now focus on two results known as Le Cam's first and third lemmas.

## 10.2 Le Cam's first lemma

Le Cam's first lemma is concerned with a set of sufficient and necessary conditions for contiguity. Recall that, for two probability measures \(P\) and \(Q\), if \(P\ll Q\), then

\[E_{Q}\left(\frac{dP}{dQ}\right)=\int\frac{dP}{dQ}dQ=\int dP=1, \tag{10.1}\]

where \(E_{Q}\) denotes the expectation with respect to the probability measure \(Q\). Le Cam's first Lemma is analogous to this result when \(P\) and \(Q\) are replaced by sequences of probability measures \(\{P_{n}\}\) and \(\{Q_{n}\}\) and absolute continuity \(P\ll Q\) is replaced by contiguity \(P_{n}\lhd Q_{n}\).

The following technical lemma is established first.

**Lemma 10.1**: _Suppose that \(g_{n}:\mathbb{R}\to\mathbb{R}\) is a sequence of functions such that, for any \(\epsilon>0\), \(\liminf_{n\to\infty}g_{n}(\epsilon)\geq 0\). Then there is a sequence \(\epsilon_{n}\downarrow 0\) such that \(\liminf_{n\to\infty}g_{n}(\epsilon_{n})\geq 0\)._

Proof: Since for each integer \(k\geq 1\), \(\liminf_{n\to\infty}g_{n}(1/k)\geq 0\), there is a positive integer \(n_{k}\) such that, for all \(n\geq n_{k}\), \(g_{n}(1/k)>-1/k\). Without loss of generality, we can assume that \(n_{k+1}>n_{k}\) for all \(k=1,2,\ldots\). Let \(\epsilon_{n}=1/k\) for \(n_{k}\leq n<n_{k+1}\). Then \(g_{n}(\epsilon_{n})>-\epsilon_{n}\) for all \(n\geq n_{1}\). Clearly \(\epsilon_{n}\downarrow 0\) and \(\liminf_{n\to\infty}g_{n}(\epsilon_{n})\geq 0\). \(\Box\)

In the following discussion, multiple probability measures are considered on single measurable spaces. So for clarity of the exposition, we revamp the notation for convergence in distribution and convergence in probability. For \(n\in\mathbb{N}\), let \(X_{n}\) and \(X\) be random variables defined, respectively, on the probability spaces \((\Omega_{n},\mathcal{F}_{n},P_{n})\) and \((\Omega,\mathcal{F},P)\) taking values in \((\mathbb{R}^{k},\mathcal{R}^{k})\).

We write

\[X_{n}\xrightarrow[P_{n}]{\mathcal{D}}X,\]

if \(X_{n}\) converges in distribution to \(X\) under the sequence \(\{P_{n}\}\); that is, for every bounded and continuous \(f\) on \(\mathbb{R}^{k}\),

\[\int f(X_{n})dP_{n}\to\int f(X)dP.\]

Similarly, if \(X_{n}\) converges in \(P_{n}\)-probability to a constant \(a\); that is,

\[P_{n}(\|X_{n}-a\|>\epsilon)\to 0,\]

for every \(\epsilon>0\), then we write \(X_{n}\xrightarrow[P_{n}]{P_{n}}a\).

The first statement of the next theorem is analogous to (10.1), and the second statement is analogous to \(P\ll Q\).

**Theorem 10.1** (Le Cam's first lemma): _Let \(P_{n}\) and \(Q_{n}\) be sequences of probability measures on measurable spaces \((\Omega_{n},\mathcal{F}_{n})\), and assume \(P_{n}\equiv Q_{n}\). Then the following statements are equivalent:_

1. _If_ \(dP_{n}/dQ_{n}\xrightarrow[Q_{n}]{\mathcal{D}}V\) _along a subsequence, then_ \(E(V)=1\)_._
2. \(P_{n}\triangleleft Q_{n}\)_._
3. _If_ \(dQ_{n}/dP_{n}\xrightarrow[P_{n}]{\mathcal{D}}U\) _along a subsequence, then_ \(P(U>0)=1\)_._

Proof: \(1\Rightarrow 2\). Suppose \(Q_{n}(A_{n})\to 0\) for a sequence of measurable sets \(\{A_{n}\}\). Then we shall show that \(P_{n}(A_{n})\to 0\), proving statement 2. First note that, since \(P_{n}\ll Q_{n}\), the sequence of densities \(\{dP_{n}/dQ_{n}\}\) is tight under the sequence of measures \(\{Q_{n}\}\). By Prohorov's Theorem (Theorem 7.10), every subsequence \(\{n^{\prime}\}\) has a further subsequence \(\{n^{\prime\prime}\}\), and a random variable \(V\), such that \(dP_{n^{\prime\prime}}/dQ_{n^{\prime\prime}}\xrightarrow[Q_{n^{\prime\prime}}] {\mathcal{D}}V\).

[MISSING_PAGE_FAIL:306]

\[\begin{split} 0\leq p_{n},q_{n}\leq 2,\ \mu_{n}\{p_{n}=0\}& =\mu_{n}\{q_{n}=0\}=0,\ p_{n}+q_{n}=2,\\ \mu_{n}\bigg{\{}\frac{p_{n}}{q_{n}}=\frac{dP_{n}}{dQ_{n}}\bigg{\}} &=1,\ \text{and}\ \ \mu_{n}\bigg{\{}\frac{q_{n}}{p_{n}}=\frac{dQ_{n}}{dP_{ n}}\bigg{\}}=1.\end{split} \tag{10.5}\]

Clearly for any \(c>0\),

\[\{(p_{n}/q_{n})<c\}\Leftrightarrow\{(2-q_{n})/q_{n}<c\}\Leftrightarrow\{q_{n}> 2/(1+c)\},\]

and hence

\[\begin{split} E_{Q_{n}}\left(\frac{p_{n}}{q_{n}}I_{\{p_{n}/q_{n} \leq c\}}\right)&=E_{\mu_{n}}\left(p_{n}I_{\{p_{n}/q_{n}\leq c\}} \right)\\ &\geq E_{\mu_{n}}\left(p_{n}I_{\{p_{n}/q_{n}<c\}}\right)\\ &=E_{\mu_{n}}\left((2-q_{n})I_{\{q_{n}>2/(1+c)\}}\right).\end{split} \tag{10.6}\]

Now suppose \(dP_{n}/dQ_{n}\xrightarrow[Q_{n}]{\mathcal{D}}V\) along a subsequence \(\{n^{\prime}\}\). By (10.5), we have \((p_{n^{\prime}}/q_{n^{\prime}})\xrightarrow[Q_{n^{\prime}}]{\mathcal{D}}V\). Since, for any \(K>0\),

\[\begin{split}\mu_{n}(q_{n}>K)\leq(1/K)E_{\mu_{n}}(dQ_{n}/d\mu_{n} )=1/K,\\ P_{n}(q_{n}/p_{n}>K)=P_{n}(dQ_{n}/dP_{n}>& K)\leq(1/K )E_{P_{n}}(dQ_{n}/dP_{n})=1/K,\end{split}\]

the sequence \(\{q_{n}\}\) is tight under \(\{\mu_{n}\}\), and the sequence \(\{(q_{n}/p_{n})\}\) is tight under \(\{P_{n}\}\). So, by Lemma 7.6, there is a further subsequence \(\{n^{\prime\prime}\}\) of \(\{n^{\prime}\}\) such that

\[\frac{q_{n^{\prime\prime}}}{p_{n^{\prime\prime}}}\xrightarrow[P_{n^{\prime \prime}}]{\mathcal{D}}U, \tag{10.7}\] \[\frac{p_{n^{\prime\prime}}}{q_{n^{\prime\prime}}}\xrightarrow[Q_{ n^{\prime\prime}}]{\mathcal{D}}V,\ \ \text{and}\ \ q_{n^{\prime\prime}} \xrightarrow[\mu_{n^{\prime\prime}}]{\mathcal{D}}W, \tag{10.8}\]

for some random variables \(U,\,W\). Hence by Fatou's lemma (Lemma 7.4), (10.5), and bounded convergence theorem (Theorem 1.8),

\[E(V)\leq 1,\quad E(W)=1. \tag{10.9}\]

In view of (10.6) and (10.8), by applying Portmanteau Theorem (Theorem 7.6) to the upper bounded upper semi-continuous function \(f(x)=xI_{\{x\leq c\}}\) and the lower bounded lower semi-continuous function \(g(x)=(2-x)I_{\{x>2/(1+c)\}}\), we get for any \(c>0\),

\[\begin{split} E(V)\geq E\left(VI_{\{V\leq c\}}\right)& \geq\limsup_{n^{\prime\prime}\to\infty}E_{Q_{n^{\prime\prime}}} \bigg{(}\frac{p_{n^{\prime\prime}}}{q_{n^{\prime\prime}}}I_{\{p_{n^{\prime \prime}}/q_{n^{\prime\prime}}\leq c\}}\bigg{)}\\ &\geq\liminf_{n^{\prime\prime}\to\infty}E_{\mu_{n^{\prime\prime} }}\left((2-q_{n^{\prime\prime}})I_{\{q_{n^{\prime\prime}}>2/(1+c)\}}\right) \\ &\geq E\left((2-W)I_{\{W>(2/(1+c)\}}\right).\end{split} \tag{10.10}\]Now let \(c\to\infty\) in (10.10), and use (10.9) to get

\[1\geq E(V)\geq E\left((2-W)I_{\{W>0\}})\right)\] \[= 2P(W>0)-E(W)\] \[= 1-2P(W=0). \tag{10.11}\]

To complete the proof it is sufficient to prove \(P(W=0)=0\). Toward this end, let \(0<\epsilon<1\), and apply Portmanteau Theorem 7.6 to (10.7) and (10.8) to conclude

\[P(W=0)\leq P(W<\epsilon) \leq \liminf_{n^{\prime\prime}\to\infty}\mu_{n^{\prime\prime}}(q_{n^{ \prime\prime}}<\epsilon)\] \[\leq \limsup_{n^{\prime\prime}\to\infty}P_{n^{\prime\prime}}((q_{n^{ \prime\prime}}/p_{n^{\prime\prime}})\leq\epsilon)\] \[\leq P(U\leq\epsilon).\]

By the right continuity of probability distribution functions and Statement 3, it follows that \(P(W=0)\leq P(U\leq\epsilon)\downarrow P(U=0)=0\), as \(\epsilon\downarrow 0\). \(\Box\)

### Le Cam's third lemma

Le Cam's third lemma establishes the limit form of the local alternative distributions \(P_{n}\) based on the limit form of the null distributions \(Q_{n}\). This is useful for proving the convolution theorem and for developing the asymptotic power of a test statistic under the local alternative distributions. To understand the intuition behind Le Cam's third lemma, it is again helpful to make an analogy with the situation involving two probability measures \(P\), \(Q\). If \(P\ll Q\) and \(U\) is a random vector, then for any measurable set \(B\),

\[P(U\in B)=E_{P}[I_{B}(U)]=E_{Q}\left[I_{B}(U)\frac{dP}{dQ}\right].\]

Le Cam's third lemma is an analogous statement with probability measures \(P\) and \(Q\) replaced by sequences of probability measures \(P_{n}\) and \(Q_{n}\), and the absolute continuity \(P\ll Q\) with contiguity \(P_{n}\lhd Q_{n}\).

**Theorem 10.2** (Le Cam's third lemma): _Suppose that \(P_{n},Q_{n}\) are probability measures defined on \((\Omega_{n},\mathcal{F}_{n})\) such that \(P_{n}\equiv Q_{n}\) and \(P_{n}\lhd Q_{n}\). If \(\{U_{n}\}\) is a sequence of random vectors in \(\mathbb{R}^{k}\) such that_

\[(U_{n},dP_{n}/dQ_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(U,V), \tag{10.12}\]

_then \(L(B)=E(I_{B}(U)V)\) defines a probability measure on the Borel sets of \(\mathbb{R}^{k}\), and \(U_{n}\xrightarrow[P_{n}]{\mathcal{D}}L\)._Proof.: Because \(P_{n}\triangleleft Q_{n}\) and \(dP_{n}/dQ_{n}\xrightarrow[Q_{n}]{\mathcal{D}}V\), by Le Cam's first lemma \(L(\mathbb{R}^{k})=E(V)=1\). As \(V\geq 0\), clearly \(L(B)\geq 0\) for all Borel sets \(B\). By Problem 1.26, \(L\) is a probability measure on \(\mathbb{R}^{k}\). By the definition of \(L\), \(E[f(U)V]=\int fdL\) holds for any measurable indicator function \(f\). Thus, by Theorem 7.11,

\[E[f(U)V]=\int fdL, \tag{10.13}\]

for all non-negative measurable functions \(f\), and hence clearly (10.13) holds for all measurable functions that are bounded below.

We shall now show that \(U_{n}\xrightarrow[P_{n}]{\mathcal{D}}L\). Since \(P_{n}\equiv Q_{n}\), for any measurable function \(f\) that is bounded below, we have

\[E_{P_{n}}(f(U_{n}))=E_{Q_{n}}\left(f(U_{n})\frac{dP_{n}}{dQ_{n}}\right).\]

Thus for any lower semi-continuous function \(f\) that is bounded from below, the function that maps \((u,v)\) to \(f(u)v\) is a also lower semi-continuous and is bounded from below. Thus, by the Portmanteau Theorem and the convergence \((U_{n},dP_{n}/dQ_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(U,V)\), we have

\[\liminf_{n\to\infty}\int f(U_{n})dP_{n}=\liminf_{n\to\infty}\int f(U_{n})\frac {dP_{n}}{dQ_{n}}dQ_{n}\geq E(f(U)V). \tag{10.14}\]

Hence by (10.13) and (10.14),

\[\liminf_{n\to\infty}\int f(U_{n})dP_{n}\geq\int fdL.\]

Now another application of Portmanteau Theorem yields \(U_{n}\xrightarrow[P_{n}]{\mathcal{D}}L\). 

In the above proof we have used \(\int fdL=E[f(U)V]\) for any measurable function bounded from below. We now expand this result somewhat and state it as a corollary for future reference. This corollary is a direct consequence of Theorem 7.11.

**Corollary 10.1**_Suppose \(U\) is a random vector and \(V\) is a nonnegative random variable with \(E(V)=1\). Let \(L\) be the set function defined by \(L(B)=E[I_{B}(U)V]\) for all Borel sets \(B\) of \(\mathbb{R}^{k}\). Then \(L\) defines a probability measure and the equality_

\[E[f(U)V]=\int f(U)dL\]

_holds (i) for any nonnegative measurable function \(f\); (ii) for any measurable function \(f(u)\) such that \(E[f(U)V]<\infty\) and \(\int|f(U)|dL<\infty\).__In particular, the moment generating function (if it exists) and the characteristic function of \(L\) are, respectively,_

\[\phi_{L}(t)=E(e^{t^{T}U}V),\quad\kappa_{L}(t)=E(e^{it^{T}U}V).\]

The next corollary gives the local alternative distribution of \((U_{n},L_{n})\) under the conditions in Le Cam's third lemma.

**Corollary 10.2**: _Suppose that the assumptions in Theorem 10.2 hold. Then \(L(B)=E[I_{B}(U,\log V)V]\) defines a probability measure and_

\[(U_{n},L_{n})\xrightarrow[P_{n}]{\mathcal{D}}L.\]

By (10.12) and the continuous mapping theorem,

\[(U_{n},\log(dP_{n}/dQ_{n}),dP_{n}/dQ_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(U, \log V,V).\]

By Theorem 10.2 (with \(U_{n}\) replaced by the random vector \((U_{n},\log(dP_{n}/dQ_{n}))\)), we have

\[(U_{n},\log(dP_{n}/dQ_{n}))\xrightarrow[P_{n}]{\mathcal{D}}L,\]

as desired. \(\Box\)

## 0.4 Local asymptotic Normality

We now set up the assumptions and notations for a framework known as the local asymptotic Normality. See Le Cam (1960), Hall and Mathiason (1990), van der Vaart (1998), and Le Cam and Yang (2000). This framework will be important for the development of both the convolution theorem in Section 0.5 and the local alternative distributions of hypothesis testing in Chapter 11. We first make some assumptions about the parametric family that is the basis of our discussions.

**Assumption 10.1** (parametric model): _1. \((\Omega_{n},\mathcal{F}_{n})\), \(n\in\mathbb{N}\), is a sequence of measurable spaces;_

_2. for each \(n\), \(\{P_{n\theta}:\theta\in\Theta\subseteq\mathbb{R}^{p}\}\) is a homogeneous parametric family of probability measures on \((\Omega_{n},\mathcal{F}_{n})\) dominated by a \(\sigma\)-finite measure \(\mu_{n}\)._

_3. The density \(f_{n\theta}=dP_{n\theta}/d\mu_{n}\) is differentiable with respect to \(\theta\)._Recall from Section 2.1.1, a parametric family \(\{P_{n\theta}:\theta\in\Theta\}\) being homogeneous means that \(P_{n\theta^{\prime}}\equiv P_{n\theta^{\prime\prime}}\) for all \(\theta^{\prime},\theta^{\prime\prime}\in\Theta\). This assumption is stronger than necessary and is introduced to simplify the subsequent technical developments. Note that above definition makes no reference to the random vectors underlying the distributions \(P_{n\theta}\).

To facilitate the asymptotic analysis, we localize the above parametric family around an interior point \(\theta_{0}\) in the parameter space, as described by the next definition.

**Definition 10.2** (local parametric model): _Suppose Assumption 10.1 holds. Let \(\theta_{0}\) be an interior point of \(\Theta\), and let \(\theta_{n}(\delta)=\theta_{0}+n^{-1/2}\delta\in\Theta\)._

1. _The set_ \(\{P_{n\theta_{n}(\delta)}:\delta\in\mathbb{R}^{p}\}\) _is called the local parametric family around_ \(\theta_{0}\)_._
2. _The sequence_ \(\{P_{n\theta_{0}}:n\in\mathbb{N}\}\) _is called the null sequence and is denoted by_ \(\{Q_{n}:n\in\mathbb{N}\}\)_._
3. _The sequence_ \(\{P_{n\theta_{n}(\delta)}:n\in\mathbb{N}\}\) _is called the local alternative sequence and is denoted by_ \(\{P_{n}(\delta):n\in\mathbb{N}\}\)_._
4. \(L_{n}(\delta)=\log(dP_{n}(\delta)/dQ_{n})\) _is called the local log likelihood ratio._
5. \(S_{n}=n^{-1/2}\partial(\log f_{n\theta})/\partial\theta|_{\theta=\theta_{0}}=n ^{-1/2}\partial L_{n}(\delta)/\partial\delta|_{\delta=0}\) _is called the standardized score._

When it causes no ambiguity, we will write \(\theta_{n}(\delta)\), \(P_{n}(\delta)\), and \(L_{n}(\delta)\) simply as \(\theta_{n}\), \(P_{n}\), and \(L_{n}\). The vector \(\delta\) serves as the localized parameter -- localized within a neighborhood with size of the order \(n^{-1/2}\). In a hypothesis test setting, the sequence \(Q_{n}\) can be regarded as the null hypothesis, and the sequence \(P_{n}(\delta)\) the local alternative hypothesis. In the estimation setting, \(\{P_{n}(\delta):\delta\in\mathbb{R}^{p}\}\) is simply a family of distributions indexed by the local parameter \(\delta\). The localization scale \(n^{-1/2}\) is chosen to guarantee contiguity of \(P_{n}(\delta)\) with respect to \(Q_{n}\).

The next assumption is the local asymptotic normal assumption (LAN). It assumes that the standard score function \(S_{n}\) is asymptotically Normal, and the second-order Taylor expansion of \(L_{n}\) has a desired remainder term. The conditions for these are quite mild, which are satisfied not only for the independent case but also for some stochastic processes.

**Assumption 10.2** (Lan): _Under Assumption 10.1, we further assume_

\[S_{n} \stackrel{{\mathcal{D}}}{{\longrightarrow}}N(0,I( \theta_{0})) \tag{10.15}\] \[L_{n}(\delta) \stackrel{{ Q_{n}}}{{=}}\delta^{T}S_{n}-\delta^{T}I( \theta_{0})\delta/2+o_{P}(1), \tag{10.16}\]

_where \(I(\theta_{0})\) is a positive definite matrix, called the Fisher information. If these conditions hold, then we say \((S_{n},L_{n})\) satisfies LAN._

The notation \(\stackrel{{ Q_{n}}}{{=}}\) in (10.16) indicates the probability underlying \(o_{P}(1)\) is \(Q_{n}\): thus, \(X_{n}\stackrel{{ Q_{n}}}{{=}}Y_{n}+o_{P}(1)\) means that for any \(\epsilon>0\),\[Q_{n}(\|X_{n}-Y_{n}\|>\epsilon)\to 0.\]

This notation will be used repeatedly throughout the rest of the Chapter. The next lemma shows that \(P_{n}(\delta)\) is contiguous with respect to \(Q_{n}\) under LAN.

**Lemma 10.2**: _If \((S_{n},L_{n}(\delta))\) satisfies LAN, then \(P_{n}(\delta)\lhd Q_{n}\)._

Proof.: By Slutsky's theorem, \(L_{n}(\delta)\xrightarrow[Q_{n}]{\mathcal{D}}L(\delta)\), where

\[L(\delta)\sim N(-\delta^{T}I(\theta_{0})\delta/2,\delta^{T}I(\theta_{0})\delta).\]

By the continuous mapping theorem,

\[dP_{n}/dQ_{n}=e^{L_{n}(\delta)}\xrightarrow[Q_{n}]{\mathcal{D}}e^{L(\delta)}.\]

The expectation of \(e^{L(\delta)}\) is simply the moment generating function of \(L(\delta)\) evaluated at \(1\), which is

\[E(e^{L(\delta)})=\phi_{L(\delta)}(1)=\exp[(-\delta^{T}I\delta/2)+(\delta^{T}I \delta)1^{2}/2]=1.\]

Here, we used the fact that the moment generating function of normal distribution with mean \(\mu\) and variance \(\sigma^{2}\) is \(\exp(\mu t+\sigma^{2}t^{2}/2)\). Hence, by Theorem 10.1, \(P_{n}(\delta)\lhd Q_{n}\). 

We next develop a set of sufficient conditions for LAN under the i.i.d. parametric model. To do so, we first give a rigorous definition of the i.i.d. parametric model and the regularity conditions needed.

**Assumption 10.3** (i.i.d. parametric model): _1. \(X_{1},X_{2},\ldots\) are i.i.d. random vectors with distribution belonging to a homogeneous parametric family \(\{P_{\theta}:\theta\in\Theta\}\) defined on a measurable space \((\Omega,\mathcal{F})\)._

_2. \(P_{\theta}\) is dominated by a \(\sigma\)-finite measure \(\mu\), with its density denoted by \(f_{\theta}=dP_{\theta}/d\mu\)._

_3. \(f_{\theta}\) is differentiable with respect to \(\theta\)._

Under the i.i.d. model, the various quantities in Assumption 10.1 and Definition 10.2 reduce to the following:

* \((\Omega_{n},\mathcal{F}_{n},\mu_{n})=(\Omega\times\cdots\times\Omega,\mathcal{ F}\times\cdots\times\mathcal{F},\mu\times\cdots\times\mu)\);
* \(P_{n\theta}=P_{\theta}\times\cdots\times P_{\theta}\);
* \(L_{n}(\delta)=nE_{n}[\ell(\theta_{n},X)-\ell(\theta_{0},X)]\), where \(\ell(\theta,X)=\log[f_{\theta}(X)]\);
* \(S_{n}=n^{1/2}E_{n}[s(\theta,X)]\), where \(s(\theta,X)=\partial[\log f_{\theta}(X)]/\partial\theta\).

The next proposition gives the sufficient conditions for LAN. Following the notations in Chapter 9 (see equation (9.10)), let

\[J(\theta)=E_{\theta}[\partial s(\theta,X)/\partial\theta^{T}],\quad K(\theta) =E_{\theta}[s(\theta,X)s^{T}(\theta,X)].\]

**Proposition 10.2**: _Suppose Assumption 10.3 holds and_

_1. \(\ell(\theta,x)\) is twice differentiable;_

_2. \(f_{\theta}(X)\) and \(s(\theta,X)f_{\theta}(X)\) satisfy \(\mbox{DUI}^{+}(\theta,\mu)\);_

_3. \(s(\theta,X)\) is \(P_{\theta}\)-square integrable;_

_4. the sequence of random matrices \(\{E[\partial^{2}\ell(\theta,X)/\partial\theta\partial\theta^{T}]:n\in\mathbb{N}\}\) is stochastically equicontinuous in a neighborhood of \(\theta_{0}\)._

_Then \(J(\theta)=-K(\theta)\), and LAN is satisfied with \(I(\theta)=-J(\theta)=K(\theta)\)._

Proof: Under Assumption 10.3, \(S_{n}=\sqrt{n}E_{n}[s(\theta_{0},X)]\). Because \(f_{\theta}(X)\) satisfies \(\mbox{DUI}^{+}(\theta,\mu)\), \(s(\theta,X)\) is an unbiased estimating equation. Because \(s(\theta,X)\) is \(P_{\theta}\)-square-integrable, by the Central Limit Theorem,

\[S_{n}\stackrel{{\mathcal{D}}}{{\longrightarrow}}N(0,K(\theta_{0 })), \tag{10.17}\]

By Taylor's mean value theorem,

\[E_{n}[\ell(\theta_{n},X)]= \,E_{n}[\ell(\theta_{0},X)]+n^{-1/2}E_{n}[\partial\ell(\theta_{0 },X)/\partial\theta^{T}]\delta\] \[+n^{-1}\delta^{T}E_{n}[\partial\ell(\theta^{\dagger},X)/\partial \theta\partial\theta^{T}]\delta\]

for some \(\theta^{\dagger}\) between \(\theta_{0}\) and \(\theta_{n}\). By \(\theta_{n}\to\theta_{0}\), the stochastic equicontinuity condition \(4\), and Corollary 8.2, we have

\[E_{n}[\partial\ell(\theta^{\dagger},X)/\partial\theta\partial \theta^{T}]\stackrel{{ Q_{n}}}{{=}}J(\theta_{0})+o_{P}(1).\]

Hence

\[L_{n}\stackrel{{ Q_{n}}}{{=}}\delta^{T}S_{n}-\delta ^{T}(J+o_{P}(1))\delta/2=\delta^{T}S_{n}-\delta^{T}J\delta/2+o_{P}(1), \tag{10.18}\]

where \(J\) is the abbreviation of \(J(\theta_{0})\). Finally, because \(s(\theta,X)f_{\theta}(X)\) satisfies \(\mbox{DUI}^{+}(\theta,\mu)\), we have

\[-J(\theta)=K(\theta)=I(\theta). \tag{10.19}\]

Now the proposition follows from (10.17), (10.18), and (10.19). \(\Box\)

### 10.5 The convolution theorem

Now we are ready to prove the Le Cam-Hajek convolution theorem (see, for example, Bickel, Klaassen, Ritov, and Wellner (1993)). This theorem asserts, roughly, if \(\tilde{\theta}\) is a regular estimate (see below for a definition) of \(\theta_{0}\), then \(\sqrt{n}(\tilde{\theta}-\theta_{0})\) can be written as the sum of two asymptotically independent random vectors, the first of which has asymptotic distribution \(N(0,I^{-1}(\theta_{0}))\). This is significant because it implies that the asymptotic variance of anyregular estimate is greater than or equal to the asymptotic variance of the maximum likelihood estimate. This form of optimality of the MLE is much stronger than the one stated in Section 9.5 following the proof of Theorem 9.3, because a regular estimate need not be asymptotically linear or asymptotically normal.

We first introduce the concept of a regular estimate. Let \(\theta_{0}\in\Theta\) be the true parameter. Let \(h:\Theta\to\mathbb{R}^{r}\), \(r\leq p\), be a differentiable function.

**Definition 10.3**: _Under the local parametric model in Definition 10.2, we say that \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\) if_

\[\sqrt{n}\{\hat{\psi}-h[\theta_{n}(\delta)]\}\xrightarrow[P_{n}(\delta)]{ \mathcal{D}}Z,\]

_where \(Z\) is a random vector whose distribution does not depend on \(\delta\)._

Of special importance are the following two scenarios:

1. \(h(\theta)=\theta\) for all \(\theta\in\Theta\). In this case \(\hat{\theta}\) is a regular estimate of \(\theta_{0}\) if and only if \[\sqrt{n}[\hat{\theta}-\theta_{n}(\delta)]\xrightarrow[P_{n}(\delta)]{\mathcal{ D}}Z,\] where the distribution of \(Z\) does not depend on \(\delta\).
2. \(\theta=(\psi^{T},\lambda^{T})^{T}\), where \(\psi\in\mathbb{R}^{r}\) is the parameter of interest, and \(\lambda\in\mathbb{R}^{s}\) is the nuisance parameter. In this case \(\hat{\psi}\) is a regular estimate of \(\psi_{0}\) if and only if \[\sqrt{n}[\hat{\psi}-\psi_{n}(\delta)]\xrightarrow[P_{n}(\delta)]{\mathcal{D}}Z,\] where \(\psi(\delta)\) is the first \(r\) components of \(\theta_{n}(\delta)\), and the distribution of \(Z\) does not depend on \(\delta\).

Intuitively, regularity means that in the vicinity of \(\theta_{0}\), the asymptotic distribution of \(\sqrt{n}[\hat{\psi}-h(\theta)]\) under \(P_{n\theta}\) is essentially the same. Thus it is a type of smoothness of the limiting distribution. Technically, since regularity concerns the asymptotic distribution under the local alternative distribution \(P_{n}(\delta)\), it links \(\sqrt{n}(\hat{\psi}-h(\theta_{0}))\) with the likelihood ratio \(L_{n}\). Indeed, a main point of the convolution theorem is that, if \(\hat{\psi}\) is regular and \((S_{n},L_{n})\) satisfies LAN, then the joint distribution of \(\sqrt{n}(\hat{\psi}-h(\theta_{0}))\) and \(L_{n}\) converges weakly. We are now ready to prove the convolution theorem.

**Theorem 10.3** (Convolution Theorem): _If \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\) and \((S_{n},L_{n})\) satisfies LAN, then_

\[\sqrt{n}\{\hat{\psi}-h[\theta_{n}(\delta)]\}\xrightarrow[P_{n}(\delta)]{ \mathcal{D}}\hat{h}^{T}I^{-1}S+R, \tag{10.20}\]

_where \(S\leavevmode\hbox{\small 1\kern-3.8pt\normalsize 1}\kern-3.8pt\normalsize 1\)\(R\), and \(S\), \(I\) are as defined in LAN - that is, \(S_{n}\xrightarrow[Q_{n}]{\mathcal{D}}S\) and \(E(SS^{T})=I\)._Proof.: Let \(U_{n}=\sqrt{n}[\hat{\psi}-h(\theta_{0})].\) Since \(\hat{\psi}\) is regular,

\[\sqrt{n}\{\hat{\psi}-h[\theta_{n}(\delta)]\}\underset{P_{n}(\delta)}{\overset{ \mathcal{D}}{\longrightarrow}}U\]

where the distribution of \(U\) does not depend on \(\delta.\) Taking \(\delta=0,\) we have \(U_{n}\underset{Q_{n}}{\overset{\mathcal{D}}{\longrightarrow}}U.\) By the LAN assumption, we also have \(S_{n}\underset{Q_{n}}{\overset{\mathcal{D}}{\longrightarrow}}N(0,I).\) Therefore both sequences \(\{U_{n}\}\) and \(\{S_{n}\}\) are tight under \(Q_{n},\) implying that \((U_{n},S_{n})\) is jointly tight under \(Q_{n}.\) By Prohorov's theorem, for any subsequence \(\{n^{\prime}\}\) there is a further subsequence \(\{n^{\prime\prime}\}\) such that \((U_{n^{\prime\prime}},S_{n^{\prime\prime}})\underset{Q_{n}}{\overset{ \mathcal{D}}{\longrightarrow}}W,\) where the first \(r\) arguments of \(W\) has the same distribution as \(U\) and the last \(p\) arguments of \(W\) has the same distribution as \(S.\) Naturally, we denote this fact by \((U_{n^{\prime\prime}},S_{n^{\prime\prime}})\underset{Q_{n}}{\overset{ \mathcal{D}}{\longrightarrow}}(U,S).\)

Now fix a subsequence \(\{n^{\prime}\}\) and for convenience write the further subsequence \(\{n^{\prime\prime}\}\) as \(\{k\}.\) By the LAN assumption and Slutsky's theorem,

\[(U_{k},L_{k}(\delta))\overset{Q_{k}}{\,=\,}(U_{k},\delta^{T}S_{k}-\delta^{T}I \delta/2)+o_{P}(1)\underset{Q_{k}}{\overset{\mathcal{D}}{\longrightarrow}}(U, \delta^{T}S-\delta^{T}I\delta/2).\]

By Continuous Mapping Theorem,

\[(U_{k},dP_{k}(\delta)/dQ_{k})\underset{Q_{k}}{\overset{\mathcal{D}}{ \longrightarrow}}(U,e^{\delta^{T}S-\delta^{T}I\delta/2}).\]

By Le Cam's third Lemma (Theorem 10.2), if \(L\) is the set function \(L(B)=E[I_{B}(U)e^{\delta^{T}S-\delta^{T}I\delta/2}],\) then \(L\) is a probability measure, and \(U_{k}\underset{P_{k}(\delta)}{\overset{\mathcal{D}}{\longrightarrow}}L.\) By Corollary 10.1, the characteristic function of \(L\) is

\[\kappa_{L}(t)=E\left(e^{it^{T}U+\delta^{T}S-\delta^{T}I\delta/2}\right). \tag{10.21}\]

The same characteristic function can be deduced from the regularity of \(\hat{\psi}.\) Because \(h\) is differentiable, \(U_{k}\) can be expressed as

\[U_{k}=\sqrt{k}[\hat{\psi}-h(\theta_{k}(\delta))]+\dot{h}^{T}\delta+o(1).\]

Hence \(U_{k}\underset{P_{k}(\delta)}{\overset{\mathcal{D}}{\longrightarrow}}U+\dot{ h}^{T}\delta,\) and an alternative expression for the characteristic function of the limit law of \(U_{k}\) under \(P_{k}(\delta)\) is

\[\kappa_{L}(t)=E\left(e^{it^{T}U+it^{T}\dot{h}^{T}\delta}\right). \tag{10.22}\]

From (10.21) and (10.22) we have

\[E\left(e^{it^{T}U+\delta^{T}S-\delta^{T}I\delta/2}\right)=E\left(e^{it^{T}U+ it^{T}\dot{h}^{T}\delta}\right).\]By Lemma 2.2, both sides of the above equality are analytic functions of \(\delta\in\mathbb{R}^{p}\). Hence, by the analytic continuation theorem (Theorem 2.7), the equality holds for all \(\delta\in\mathbb{C}^{p}\), the \(p\)-fold Cartesian product of the complex plane \(\mathbb{C}\). Take \(\delta=iu\), where \(u\in\mathbb{R}^{p}\). Then

\[\begin{split}& E(e^{it^{T}U+iu^{T}S+u^{T}Iu/2})=E(e^{it^{T}U-t^{T} \dot{h}^{T}u})\\ \Rightarrow& E(e^{it^{T}U+iu^{T}S})=e^{-t^{T}\dot{h}^ {T}u-u^{T}Iu/2}E(e^{it^{T}U}).\end{split} \tag{10.23}\]

Since the right hand side depends only on \(U\), the joint distribution of \((U,S)\) does not depend on the subsequence \(k\). Therefore, \((U_{n},S_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(U,S)\) along the whole sequence. By continuous mapping theorem

\[(U_{n}-\dot{h}^{T}I^{-1}S_{n},S_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(U-\dot{h }^{T}I^{-1}S,S)\equiv(R,S).\]

Next, let us show that \(R\) and \(S\) are independent. The characteristic function of \((R,S)\) is

\[\kappa_{(R,S)}(t,u)= E\left(e^{it^{T}(U-\dot{h}^{T}I^{-1}S)+iu^{T}S}\right)\] \[= E\left(e^{it^{T}U+i(-I^{-1}ht+u)^{T}S}\right)=\kappa_{(U,S)}(t,- \dot{h}^{T}I^{-1}t+u)\]

By (10.23),

\[\begin{split}&\kappa_{(U,S)}(t,-\dot{h}^{T}I^{-1}t+u)\\ &=e^{-t^{T}\dot{h}^{T}(-I^{-1}\dot{h}t+u)-(-I^{-1}\dot{h}t+u)^{T}I _{\theta}(-I^{-1}\dot{h}t+u)/2}E(e^{it^{T}U})\\ &=e^{t^{T}\dot{h}^{T}I^{-1}\dot{h}t/2-u^{T}Iu/2}E(e^{it^{T}U}). \end{split}\]

Therefore,

\[\kappa_{(R,S)}(t,u)=\left(e^{-u^{T}Iu/2}\right)\left(e^{t^{T}\dot{h}^{T}I^{-1} \dot{h}t/2}Ee^{it^{T}U}\right).\]

Since the characteristic function of \((R,S)\) factorizes into the product of a function of \(t\) and a function of \(u\), \(R\) and \(S\) are independent. Therefore, \(U-\dot{h}^{T}I^{-1}S\) and \(\dot{h}^{T}I^{-1}S\) are independent. So

\[U_{n}\xrightarrow[Q_{n}]{\mathcal{D}}U=(U-\dot{h}^{T}I^{-1}S)+\dot{h}^{T}I^{-1 }S,\]

where \(U-\dot{h}^{T}I^{-1}S\) and \(\dot{h}^{T}I^{-1}S\) are independent.

Finally, because \(\hat{\psi}\) is regular, the asymptotic distribution of \(\sqrt{n}\{\hat{\psi}-h[\theta_{n}(\delta)]\}\) under \(P_{n}(\delta)\) is the same as the asymptotic distribution of \(U_{n}\) under \(Q_{n}\). Thus we have (10.20). \(\Box\)

The next corollary gives an alternative form of the convolution theorem.

**Corollary 10.3**: _Suppose \((S_{n},L_{n})\) satisfies LAN and \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\). Then_

\[\sqrt{n}[\hat{\psi}-h(\theta_{0})]=\dot{h}^{T}I^{-1}S_{n}+R_{n},\]

_where \((S_{n},R_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(S,R)\) and \(S\operatorname{\leavevmode\hbox{\small 1\kern-3.8pt\normalsize 1}}R\)._

_Proof._ By the proof of Theorem 10.3, \((S_{n},R_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(S,R)\), where \(S\operatorname{\leavevmode\hbox{\small 1\kern-3.8pt\normalsize 1}}R\) and \(R_{n}=\sqrt{n}[\hat{\psi}-h(\theta_{0})]-\dot{h}^{T}I^{-1}S_{n}\). \(\Box\)

The name "convolution" is motivated by the fact that, because of the independence between \(S\) and \(R\), the distribution of \(U\) is the convolution of the distribution of \(R\) and the distribution of \(\dot{h}^{T}I^{-1}S\). An important fact emerged in the proof of the above theorem -- that is, \((U_{n},S_{n})\) converges in distribution to a random vector \((U,S)\). This is not automatically implied by \(U_{n}\xrightarrow[Q_{n}]{\mathcal{D}}U\), \(S_{n}\xrightarrow[Q_{n}]{\mathcal{D}}S\), and \(L_{n}\xrightarrow[Q_{n}]{\mathcal{D}}\log(V)\). Instead it was deduced from the regularity of \(\hat{\psi}\) and the LAN condition using the argument via subsequences. This result is of importance in its own and we record it below as a corollary.

**Corollary 10.4**: _If \((S_{n},L_{n})\) satisfies LAN and \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\), then \((U_{n},S_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(U,S)\) for some random vector in \(\mathbb{R}^{k+p}\), where \(U_{n}=\sqrt{n}[\hat{\psi}-h(\theta_{0})]\). The characteristic function of \((U,S)\) is_

\[\kappa_{U,S}(t,u)=e^{-t^{T}\dot{h}^{T}u-u^{T}Iu/2}E(e^{it^{T}U}).\]

To complete the picture of regular estimate and convolution theorem, we show that the convolution form, in fact, characterizes a regular estimate; that is, an estimate that can be written as the convolution form must be regular. We first prove a lemma.

**Lemma 10.3**: _If \(X\sim N(\mu,\Sigma)\), then, for any \(s\in\mathbb{C}^{p}\),_

\[E(e^{s^{T}X})=\exp(\mu^{T}s+s^{T}\Sigma s/2). \tag{10.24}\]

_Proof._ Since the moment generating function of \(X\) is the right-hand side of (10.24) with \(s\in\mathbb{R}^{p}\), the equality (10.24) holds for all \(s\in\mathbb{R}^{p}\). Because the functions on both sides of (10.24) are analytic functions of \(s\), by the analytic continuation theorem (Theorem 2.7), the equality holds for all \(s\in\mathbb{C}^{p}\). \(\Box\)

**Theorem 10.4**: _Suppose that \((S_{n},L_{n})\) satisfies LAN. Then the following statements are equivalent:_

_1. \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\);_2. \(\sqrt{n}[\hat{\psi}-h(\theta_{0})]=\dot{h}^{T}I^{-1}S_{n}+R_{n}\) where \((S_{n},R_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(S,R)\) with \(S\,\hbox{\rm 1\kern-2.5pt\hbox{l}}\,R\).

Proof.: \(1\Rightarrow 2\). This is Corollary 10.3.

\(2\Rightarrow 1\). By the differentiability of \(h\) and the LAN assumption,

\[\left(\sqrt{n}\{\hat{\psi}-h[\theta_{n}(\delta)]\}\right)\stackrel{{ \mathcal{D}}}{{=}}\left(\dot{\dot{h}}^{T}I^{-1}S_{n}+R_{n}-\dot{h}^{T}\delta \right)+o_{P}(1).\]

Because \((S_{n},R_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(S,R)\), the above implies

\[\left(\sqrt{n}\{\hat{\psi}-h[\theta_{n}(\delta)]\}\right)\xrightarrow[Q_{n}]{ \mathcal{D}}\left(\dot{\dot{h}}^{T}I^{-1}S+R-\dot{h}^{T}\delta\right).\]

By Le Cam's third lemma and Corollary 10.1, \(\sqrt{n}\{\hat{\psi}-h[\theta_{n}(\delta)]\}\xrightarrow[P_{n}]{\mathcal{D}}L\), where the characteristic function of \(L\) is

\[\kappa_{L}(t)= E\left\{\exp[i(\dot{h}^{T}I^{-1}S+R-\dot{h}^{T}\delta)^{T}t]\exp( \delta^{T}S-\delta^{T}I\delta/2)\right\}\] \[= E\left\{\exp[it^{T}\dot{h}^{T}I^{-1}S-it^{T}\dot{h}^{T}\delta+ \delta^{T}S-\delta^{T}I\delta/2]\right\}\kappa_{R}(t) \tag{10.25}\] \[= E\left\{\exp[(iI^{-1}\dot{h}t+\delta)^{T}S]\right\}\exp(-it^{T} \dot{h}^{T}\delta-\delta^{T}I\delta/2)\kappa_{R}(t),\]

where the second equality holds because \(S\,\hbox{\rm 1\kern-2.5pt\hbox{l}}\,R\), and \(\kappa_{R}\) is the characteristic function of \(R\). Because \(S\sim N(0,I)\), by Lemma 10.3,

\[E\left\{\exp[(iI^{-1}\dot{h}t+\delta)^{T}S]\right\} = \exp\left[(iI^{-1}\dot{h}t+\delta)^{T}I(iI^{-1}\dot{h}t+\delta)/2 \right]\] \[= \exp\left(-t^{T}\dot{h}^{T}I^{-1}\dot{h}t/2+it^{T}\dot{T}\delta+ \delta^{T}I\delta/2\right).\]

Substituting this into the right-hand side of (10.25), we have

\[\kappa_{L}(t)=\exp\left(-t^{T}\dot{h}^{T}I^{-1}\dot{h}t/2\right)\kappa_{R}(t).\]

Since this characteristic function is independent of \(\delta\), the probability measure \(L\) does not depend on \(\delta\). Hence \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\). 

### Asymptotically efficient estimates

Equipped with the convolution theorem, we can now answer the question raised at the beginning of this chapter: the maximum likelihood estimate -- or any estimate that is asymptotically equivalent to it -- is optimal among how large a class of estimates? The convolution theorem implies that if \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\) and \((S_{n},L_{n})\) satisfies LAN, then \[\sqrt{n}[\hat{\psi}-h(\theta_{0})]\xrightarrow[Q_{n}]{\mathcal{D}}h^{T}I^{-1}S+R, \quad S\,\mathchoice{\mathrel{\hbox to 0.0pt{\kern 2.999954pt\vrule height 6.299904pt wid th 1px\hss}\hbox{$\rm 1$}}}{\mathrel{\hbox to 0.0pt{\kern 2.999954pt \vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\mathrel{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\mathrel{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\mathrel{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}{\mathrel{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}}{\mathrel{\hbox to 0.0pt{ \kern 1.499977pt\vrule height 6.299904pt width 1px\hss}\hbox{$\rm 1$}}}}R.\]

Thus the asymptotic variance of \(\sqrt{n}[\hat{\psi}-h(\theta_{0})]\) is bounded from below by \(\dot{h}^{T}I^{-1}\dot{h}\) in terms of Louwner's ordering. In other words, any regular estimate of \(h(\theta_{0})\) that is asymptotically normal with asymptotic variance \(\dot{h}^{T}I^{-1}\dot{h}\) is optimal among all regular estimates. This leads to the following formal definition.

**Definition 10.4**: _Under the LAN assumption, an estimate \(\hat{\psi}\) of \(h(\theta_{0})\) is asymptotically efficient if it is regular with asymptotic distribution_

\[\sqrt{n}[\hat{\psi}-h(\theta_{0})]\xrightarrow[Q_{n}]{\mathcal{D}}N(0,\dot{h}^{ T}I^{-1}\dot{h}), \tag{10.26}\]

_where \(I=E(SS^{T})\), and \(S\) is as defined in LAN._

Note that, because \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\), expression (10.26) is equivalent to

\[\sqrt{n}[\hat{\psi}-h(\theta_{n}(\delta))]\xrightarrow[P_{n}(\delta)]{ \mathcal{D}}N(0,\dot{h}^{T}I^{-1}\dot{h})\quad\text{for all }\delta\in\mathbb{R}^{p}.\]

Moreover, the above convergence implies the regularity of \(\hat{\psi}\) and the convergence (10.26). Hence we have the following equivalent definition of an asymptotically efficient estimate.

**Definition 10.5**: _Under the LAN assumption, an estimate \(\hat{\psi}\) of \(h(\theta_{0})\) is asymptotically efficient if_

\[\sqrt{n}[\hat{\psi}-h(\theta_{n}(\theta))]\xrightarrow[P_{n}(\delta)]{ \mathcal{D}}N(0,\dot{h}^{T}I^{-1}\dot{h})\quad\text{for all }\delta\in \mathbb{R}^{p}. \tag{10.27}\]

In the case of \(h(\theta)=\theta\), the right-hand side of (10.26) is \(N(0,I^{-1})\). As shown in Chapter 8, this is the asymptotic distribution of the maximum likelihood estimate in the i.i.d. case. Hence the maximum likelihood estimate is asymptotically efficient estimate of \(\theta_{0}\) in that case. In the case of \(h(\theta)=\psi\), the right-hand side of (10.26) is \(N(0,\dot{h}^{T}I^{-1}\dot{h})\), where \(\dot{h}(\theta_{0})=(I_{r},0)\). Following the notations in Section 9.8, let \(I_{\psi\psi}\) be the \(r\times r\) upper left block \(I\) and \(I_{\psi\lambda}\) be the upper right block of dimension \(r\times s\), and so on, and make the similar partition to \(I^{-1}\). Then

\[\dot{h}^{T}I^{-1}\dot{h}=(U_{r},0)I^{-1}(U_{r},0)^{T}=(I^{-1})_{\psi\psi}=(I_{ \psi\psi}-I_{\psi\lambda}I_{\lambda\lambda}^{-1}I_{\lambda\psi})^{-1}\equiv I _{\psi\cdot\lambda}^{-1}.\]

As in Section 9.8, we call \(I_{\psi\cdot\lambda}\) the efficient information. Note that, here, \(I\) has a much more general meaning than the Fisher information in the i.i.d. case, as the LAN assumption accommodates far more probability models than the i.i.d. model. In this i.i.d. case, this definition reduces to the definition in Section 9.8 because \(E(SS^{T})\) is precisely \(E[s(\theta_{0},X)s(\theta_{0},X)^{T}]\). We have shown in Chapter 9 that, if \(\hat{\psi}\) is the \(\psi\)-component of the maximum likelihood estimate \(\hat{\theta}\), then \(\sqrt{n}(\hat{\psi}-\psi)\) has asymptotic normal with variance \(I_{\psi.\lambda}^{-1}\). Thus, \(\hat{\psi}\) is an asymptotically efficient estimate of \(\psi_{0}\) among all regular estimates of \(\psi_{0}\).

More generally, if \(\hat{\theta}\) is the maximum likelihood estimate, then, by the \(\delta\)-method, \(\sqrt{n}(h(\hat{\theta})-h(\theta_{0}))\) has asymptotic distribution \(N(0,\dot{h}^{T}I^{-1}\dot{h})\). Thus \(\hat{\psi}=h(\hat{\theta})\) is an asymptotically efficient estimate of \(h(\theta_{0})\). Furthermore, as we have shown in Chapter 9, there is a wide class of estimates, such as the one-step Newton-Raphson estimates, that have the same asymptotic distribution as the maximum likelihood estimate. All these estimates are asymptotically efficient.

The next theorem gives a sufficient and necessary condition for an estimate to be asymptotically efficient.

**Theorem 10.5**: _If \((S_{n},L_{n})\) satisfies LAN, then the following conditions are equivalent: 1. \(\hat{\psi}\) is an asymptotically efficient estimate of \(h(\theta_{0})\); 2. \(\sqrt{n}[\hat{\psi}-h(\theta_{0})]\ \stackrel{{\Omega_{n}}}{{=}}\dot{h}^{T}I^{-1}S_{n}+o_{P}(1)\)._

Proof: 2 \(\Rightarrow\) 1. Obviously, statement 2 implies \(\sqrt{n}[\hat{\psi}-h(\theta_{0})]\ \stackrel{{\mathcal{D}}}{{ \underset{Q_{n}}{\longrightarrow}}}N(0,\dot{h}^{T}I\dot{h})\). To see that \(\hat{\psi}\) is regular, note that statement 2 means

\[\sqrt{n}[\hat{\psi}-h(\theta_{0})]=\dot{h}^{T}I^{-1}S_{n}+R_{n}\]

where \(R_{n}\stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}}0\). Because \(S_{n}\stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}}S\) and 0 and \(S\) are independent, by Theorem 10.4, \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\).

1 \(\Rightarrow\) 2. Let \(U_{n}=\sqrt{n}[\hat{\psi}-h(\theta_{0})]\). Because \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\),

\[U_{n}=\dot{h}^{T}I^{-1}S_{n}+R_{n},\]

where \(R_{n}\) and \(S_{n}\) are asymptotically independent. Hence the asymptotic variance of \(U_{n}\) is the sum of the asymptotic variance of \(\dot{h}^{T}I^{-1}S_{n}\) and the asymptotic variance of \(R_{n}\). By assumption, the asymptotic variance of \(U_{n}\) is \(\dot{h}^{T}I^{-1}\dot{h}\). Since \(S_{n}\stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}}N (0,I)\), the asymptotic variance of \(\dot{h}^{T}I^{-1}S_{n}\) is also \(\dot{h}^{T}I^{-1}\dot{h}\). So the asymptotic variance of \(R_{n}\) is 0, implying \(R_{n}\ \stackrel{{ Q_{n}}}{{=}}\ o_{P}(1)\). \(\Box\)

### Augmented LAN

An important special case of Le Cam's third lemma is when \((U_{n},S_{n})\) has a joint asymptotic normal distribution in addition to the LAN assumption on \((S_{n},L_{n})\). This not only provides us concrete examples and verification criteria for regular estimates, but also plays a critical role in the development of local alternative distribution for hypothesis testing that will be done in the next chapter. Following Hall and Mathiason (1990), we refer to the LAN assumption together with the joint asymptotic normal assumption on \((U_{n},S_{n})\) as the augmented local asymptotic normal assumption, or ALAN (Hall and Mathiason abbreviated this assumption as LAN\({}^{\#}\)).

**Assumption 10.4** (Alan): _Let \(\{U_{n}:n\in\mathbb{N}\}\) be a sequence random vectors on \((\Omega_{n},\mathcal{F}_{n})\). We say that \((U_{n},S_{n},L_{n})\) satisfies ALAN if (10.16) holds and_

\[\begin{pmatrix}U_{n}\\ S_{n}\end{pmatrix}\xrightarrow[Q_{n}]{\mathcal{D}}N\left[\begin{pmatrix}0\\ 0\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\Sigma_{US}\\ \Sigma_{SU}&I\end{pmatrix}\right]. \tag{10.28}\]

From Assumption 10.4, we can easily derive the asymptotic joint distribution of \((U_{n},L_{n})\) under \(Q_{n}\).

**Proposition 10.3**: _If \((U_{n},S_{n},L_{n})\) satisfies ALAN, then_

\[\begin{pmatrix}U_{n}\\ L_{n}(\delta)\end{pmatrix}\xrightarrow[Q_{n}]{\mathcal{D}}N\left[\begin{pmatrix} 0\\ -\delta^{T}I\delta/2\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\Sigma_{US}\delta\\ \delta^{T}\Sigma_{SU}&\delta^{T}I\delta\end{pmatrix}\right]. \tag{10.29}\]

Proof.: Let \((U,S)\) be the random vector whose joint distribution is the multivariate Normal distribution on the right-hand side of (10.28). By the continuous mapping theorem,

\[\begin{pmatrix}U_{n}\\ \delta^{T}S_{n}-\delta^{T}I\delta/2\end{pmatrix}\xrightarrow[Q_{n}]{\mathcal{D} }\begin{pmatrix}U\\ \delta^{T}S-\delta^{T}I\delta/2\end{pmatrix}.\]

By (10.16) and Slutsky's Theorem,

\[\begin{pmatrix}U_{n}\\ L_{n}(\delta)\end{pmatrix}\xrightarrow[Q_{n}]{\mathcal{D}}\begin{pmatrix}U\\ \delta^{T}S-\delta^{T}I\delta/2\end{pmatrix}.\]

Because

\[E(U)=0,\quad E(\delta^{T}S-\delta^{T}I\delta/2)=-\delta^{T}I \delta/2\] \[\text{var}(U)=\Sigma_{U},\quad\text{cov}(U,\delta^{T}S-\delta^{T} I\delta/2)=\Sigma_{US}\delta,\] \[\text{var}(\delta^{T}S-\delta^{T}I\delta/2)=\delta^{T}I\delta,\]

the distribution of \((U^{T},\delta^{T}S-\delta^{T}I\delta/2)^{T}\) is the right-hand side of (10.29). 

Recall from Corollary 10.4 that regularity of \(\hat{\psi}\) and LAN together implies the joint convergence in distribution of \((U_{n},S_{n})\). From this and the above proposition we can easily deduce the following sufficient and necessary condition for ALAN.

**Proposition 10.4**: _Suppose \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\), and let \(U_{n}=\sqrt{n}[\hat{\psi}-h(\theta_{0})]\). Then the following conditions are equivalent:_1. \((S_{n},L_{n})\) satisfies LAN and \(U_{n}\xrightarrow[Q_{n}]{\mathcal{D}}N(0,\Sigma_{U})\);
2. \((U_{n},S_{n},L_{n})\) satisfies ALAN with \(\Sigma_{US}=\dot{h}^{T}\).

Proof.: \(2\Rightarrow 1\). If 2 holds, then \((U_{n},S_{n})\) converges to a multivariate normal random vector. Hence \(S_{n}\) converges marginally to a multivariate normal vector, which, together with the assumption on \(L_{n}\) in ALAN, implies that \((S_{n},L_{n})\) satisfies LAN. Since \(U_{n}\) also converges marginally to a multivariate normal vector, \(U_{n}\xrightarrow[Q_{n}]{\mathcal{D}}N(0,\Sigma_{U})\) holds.

\(1\Rightarrow 2\). Since \(\hat{\psi}\) is regular and \((S_{n},L_{n})\) satisfies LAN, by Corollary 10.4, \((U_{n},S_{n})\xrightarrow[Q_{n}]{\mathcal{D}}(U,S)\) with characteristic function

\[\kappa_{U,S}(t,u)=e^{-t^{T}\dot{h}^{T}u-u^{T}Iu/2}E(e^{it^{T}U}).\]

Because \(U\sim N(0,\Sigma_{U})\),

\[\kappa_{U,S}(t,u)= e^{-t^{T}\dot{h}^{T}u-u^{T}Iu/2}e^{-t^{T}\Sigma_{U}t/2}\] \[= e^{-\frac{1}{2}(u^{T}Iu-2t^{T}h^{T}u+t^{T}\Sigma_{U}t)},\]

which is the characteristic function of a multivariate normal distribution with mean 0 and variance matrix

\[\begin{pmatrix}\Sigma_{U}&\dot{h}^{T}\\ \dot{h}&I\end{pmatrix}.\]

Thus statement 2 holds. 

We now develop sufficient conditions for ALAN under the i.i.d. parametric model. Let \(\hat{\theta}\) be an estimate of \(\theta_{0}\) and let \(U_{n}=\sqrt{n}(\hat{\theta}-\theta_{0})\). The next proposition gives the sufficient conditions for (10.28).

**Proposition 10.5**: _Suppose Assumption 10.3 holds and 1. \(\hat{\theta}\) is an asymptotically linear estimate of \(\theta_{0}\) with influence function \(\psi\); 2. \(s(\theta,X)\) is \(P_{\theta}\)-square integrable and \(s(\theta,x)f_{\theta}(x)\) satisfies DUI\({}^{+}(\theta,\mu)\); Then condition (10.28) is satisfied with \(U_{n}=\sqrt{n}(\hat{\theta}-\theta_{0})\)._

Proof.: By Definition 9.6, \(\hat{\theta}\) being an asymptotically linear estimate of \(\theta_{0}\) means

\[\sqrt{n}(\hat{\theta}-\theta_{0})=\sqrt{n}E_{n}[\psi(\theta_{0},X)]+o_{P}(1),\]

where \(\psi(\theta_{0},X)\) is \(P_{\theta}\)-square integrable and \(E[\psi(\theta_{0},X)]=0\). Also, under Assumption 10.3, \(S_{n}=\sqrt{n}E_{n}[s(\theta_{0},X)]\). By the Central Limit Theorem and Slutsky's Theorem,

\[\begin{pmatrix}U_{n}\\ S_{n}\end{pmatrix}\xrightarrow[Q_{n}]{\mathcal{D}}N\left[\begin{pmatrix}0\\ 0\end{pmatrix},\ \begin{pmatrix}E[\psi(\theta_{0},X)\psi^{T}(\theta_{0},X)]&E[ \psi(\theta_{0},X)s^{T}(\theta_{0},X)]\\ E[s(\theta_{0},X)\psi^{T}(\theta_{0},X)]&E[s(\theta_{0},X)s^{T}(\theta_{0},X)] \end{pmatrix}\right]\]

By condition 2, the Fisher information \(I(\theta_{0})\) is well defined and it is the matrix \(E[s(\theta_{0},X)s^{T}(\theta_{0},X)]\).

## 10.8 Le Cam's third lemma under ALAN

Le Cam's third lemma reduces to a particularly convenient form under ALAN, which will be used heavily in the next chapter. We first develop the specific forms of the measure \(L(B)\) in Theorem 10.2 and Corollary 10.2.

**Lemma 10.4**: _Suppose \(U\) is a \(p\)-dimensional random vector and \(V\) is a positive random variable and their joint distribution is determined by_

\[\begin{pmatrix}U\\ \log V\end{pmatrix}\sim N\left[\begin{pmatrix}\mu_{U}\\ -\sigma^{2}/2\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\beta\\ \beta^{T}&\sigma^{2}\end{pmatrix}\right]. \tag{10.30}\]

_If \(L\) is the probability measure on \((\mathbb{R}^{p},\mathcal{R}^{p})\) defined by_

\[L(B)=E[I_{B}(U)V],\]

_then the c.d.f. of \(L\) is \(N(\mu_{U}+\beta,\Sigma_{U})\)._

Proof: Let \(W=\log V\). Then \(L(B)\) can be rewritten as \(E[I_{B}(U)e^{W}]\). By Corollary 10.1, the characteristic function of \(L\) is

\[\phi_{L}(t)= E(e^{it^{T}U}e^{W})=\exp\left[\begin{pmatrix}it\\ 1\end{pmatrix}^{T}\begin{pmatrix}U\\ W\end{pmatrix}\right]\]

By (10.30) and Lemma 10.3, the right-hand side is

\[\exp\left[\begin{pmatrix}it\\ 1\end{pmatrix}^{T}\begin{pmatrix}\mu_{U}\\ -\sigma^{2}/2\end{pmatrix}^{T}+\frac{1}{2}\begin{pmatrix}it\\ 1\end{pmatrix}^{T}\begin{pmatrix}\Sigma_{U}&\beta\\ \beta^{T}&\sigma^{2}\end{pmatrix}\begin{pmatrix}it\\ 1\end{pmatrix}\right]\] \[=\exp\left(it^{T}(\mu_{U}+\beta)-t^{T}\Sigma_{U}t/2\right),\]

which is the characteristic function of \(N(\mu_{U}+\beta,\Sigma_{U})\). \(\Box\)

If we replace \(U\) in the lemma by \((U^{T},\log V)^{T}\) then we get the following result.

**Corollary 10.5**: _Suppose \(U\) is a \(p\)-dimensional random vector and \(V\) is a positive random variable with their joint distribution determined by (10.30). If \(L\) is the probability measure on \((\mathbb{R}^{p+1},\mathcal{R}^{p+1})\) defined by_

\[L(B)=E[I_{B}(U,\log V)V] \tag{10.31}\]

_then the c.d.f. of \(L\) is_

\[N\left[\begin{pmatrix}\mu_{U}+\beta\\ \sigma^{2}/2\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\beta\\ \beta^{T}&\sigma^{2}\end{pmatrix}\right]. \tag{10.32}\]Proof.: If (10.30) holds then

\[\begin{pmatrix}U\\ \log V\\ \log V\end{pmatrix}\sim N\left[\begin{pmatrix}\mu_{U}\\ -\sigma^{2}/2\\ -\sigma^{2}/2\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\beta&\beta\\ \beta^{T}&\sigma^{2}&\sigma^{2}\\ \beta^{T}&\sigma^{2}&\sigma^{2}\end{pmatrix}\right]. \tag{10.33}\]

By Lemma 10.4 (with \(U\) replaced by \((U^{T},\log V)^{T}\)), the probability measure \(L\) defined by (10.31) corresponds to the distribution

\[N\left[\begin{pmatrix}\mu_{U}+\beta\\ -\sigma^{2}/2+\sigma^{2}\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\beta\\ \beta^{T}&\sigma^{2}\end{pmatrix}\right]=N\left[\begin{pmatrix}\mu_{U}+\beta\\ \sigma^{2}/2\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\beta\\ \beta^{T}&\sigma^{2}\end{pmatrix}\right],\]

as desired. 

The next theorem gives the special form of Le Cam's third lemma under the ALAN assumption.

**Theorem 10.6**_If \((U_{n},S_{n},L_{n})\) satisfies ALAN, then_

\[\begin{pmatrix}U_{n}\\ L_{n}\end{pmatrix}\xrightarrow[P_{n}(\delta)]{\mathcal{D}}N\left[\begin{pmatrix} \Sigma_{US}\delta\\ \delta^{T}I\delta/2\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\Sigma_{US}\delta\\ \delta^{T}\Sigma_{SU}&\delta^{T}I\delta\end{pmatrix}\right]. \tag{10.34}\]

_In particular, \(U_{n}\xrightarrow[P_{n}(\delta)]{\mathcal{D}}N(\Sigma_{US}\delta,\Sigma_{U})\)._

Proof.: By Proposition 10.3, \((U_{n}^{T},L_{n})^{T}\xrightarrow[Q_{n}]{\mathcal{D}}(U^{T},W)^{T}\), where \((U^{T},W)^{T}\) is the random vector whose joint distribution is the right-hand side of (10.29). By the continuous mapping theorem,

\[\begin{pmatrix}U_{n}\\ dP_{n}/dQ_{n}\end{pmatrix}\xrightarrow[Q_{n}]{\mathcal{D}}\begin{pmatrix}U\\ e^{W}\end{pmatrix}.\]

By Corollary 10.2, \((U_{n}^{T},L_{n})^{T}\xrightarrow[P_{n}]{\mathcal{D}}L\), where \(L\) is defined by (10.31). By Corollary 10.5 this measure is, in fact, the distribution on the right-hand side of (10.34). 

A variation of Le Cam's third lemma under the ALAN assumption concerns the joint distribution of \(U_{n}\) and the standardized score function \(S_{n}\), which is given by the next corollary.

**Corollary 10.6**_If \((U_{n},S_{n},L_{n})\) satisfies ALAN, then_

\[\begin{pmatrix}U_{n}\\ S_{n}\end{pmatrix}\xrightarrow[P_{n}(\delta)]{\mathcal{D}}N\left[\begin{pmatrix} \Sigma_{US}\delta\\ I\delta\end{pmatrix},\begin{pmatrix}\Sigma_{U}&\Sigma_{US}\\ \Sigma_{SU}&I\end{pmatrix}\right]. \tag{10.35}\]Proof.: Let \((U^{T},S^{T})^{T}\) represent the random vector whose joint distribution is (10.28). By the continuous mapping theorem,

\[\begin{pmatrix}U_{n}\\ S_{n}\\ \delta^{T}S_{n}-\delta^{T}I\delta/2\end{pmatrix}\xrightarrow{\mathcal{D}}_{Q_{n}} \begin{pmatrix}U\\ S\\ \delta^{T}S-\delta^{T}I\delta/2\end{pmatrix}.\]

Hence, by (10.16) and Slutsky's theorem,

\[\begin{pmatrix}U_{n}\\ S_{n}\\ L_{n}\end{pmatrix}\xrightarrow{\mathcal{D}}_{Q_{n}}\begin{pmatrix}U\\ S\\ \delta^{T}S-\delta^{T}I\delta/2\end{pmatrix}.\]

Because

\[\operatorname{cov}\left[\begin{pmatrix}U\\ S\end{pmatrix},\delta^{T}S-\delta^{T}I\delta/2\right]=\begin{pmatrix}\Sigma_{US }\delta\\ I\delta\end{pmatrix},\]

by Theorem 10.6, (10.35) holds. 

### 10.9 Superefficiency

In the last two sections we have shown that \(\dot{h}^{T}I^{-1}\dot{h}\) is the lower bound of the asymptotic variances of all regular estimates. An estimate whose asymptotic variance reaches this lower bound is asymptotically efficient. In this section we use an example to show that it is possible for an estimate that is not regular to have a smaller asymptotic variance than an asymptotically efficient estimate. We call such estimates _superefficient estimates_. More specifically, let \(\hat{\theta}\) be an estimate such that \(\sqrt{n}(\hat{\theta}-\theta)\) converges in distribution under \(P_{n\theta}\). Let \(\operatorname{AV}_{\hat{\theta}}(\theta)\) be the asymptotic variance of \(\sqrt{n}(\hat{\theta}-\theta)\) under \(P_{n\theta}\). Let \(I(\theta)\) be the Fisher information.

**Definition 10.6**: _An estimate \(\hat{\theta}\) is superefficient if \(\operatorname{AV}_{\hat{\theta}}(\theta)\leq I^{-1}(\theta)\) for all \(\theta\in\Theta\), and \(\operatorname{AV}_{\hat{\theta}}(\theta)<I^{-1}(\theta)\) for some \(\theta\in\Theta\). Here, when \(\theta\) is a vector, the inequality is in terms of Louwner's ordering._

For convenience, let us refer to an estimate that is not regular as an _irregular estimate_. We first prove a lemma.

**Lemma 10.5**: _Suppose \(X_{n}=O_{P}(1)\). Then for any sequences \(\{a_{n}\}\) and \(\{b_{n}\}\) such that \(a_{n}\to\infty\) and \(b_{n}>0\), we have \(I(X_{n}>a_{n})=o_{P}(b_{n})\)._

The point of this lemma is that if \(I(X_{n}>a_{n})=o_{P}(1)\), then its order of magnitude is arbitrarily small -- for example \(I(X_{n}>a_{n})=o_{P}(n^{-100})\). This fact will prove convenient for the discussions in this section.

Proof.: Since \(X_{n}\leq|X_{n}|\), it suffices to show that \(I(|X_{n}|>a_{n})=o_{P}(b_{n})\). This means, for any \(\epsilon>0\), \(P(b_{n}^{-1}I(|X_{n}|>a_{n})>\epsilon)\to 0\). Because \(b_{n}>0\), \(b_{n}^{-1}I(|X_{n}|>a_{n})>\epsilon\) if and only if \(I(|X_{n}|>a_{n})=1\). Hence we only need to show \(P(|X_{n}|>a_{n})\to 0\). For any fixed \(\epsilon>0\), let \(K>0\) be such that \(P(|X_{n}|>K)<\epsilon\) for all \(n\). Because \(a_{n}\to\infty\), \(a_{n}>K\) for all sufficiently large \(n\). Therefore, for sufficiently large \(n\), \(P(|X_{n}|>a_{n})<\epsilon\). Because \(\epsilon\) is arbitrary we have \(\limsup_{n\to\infty}P(|X_{n}|>a_{n})=0\), as desired. 

The next example describes an estimate, called Hodges-Lehmann estimate, that is irregular and superefficient.

**Example 10.1** Suppose that \(X_{1},X_{2},\ldots\) are i.i.d. \(N(\theta,1)\) where \(\theta\in\mathbb{R}\). Let \(\hat{\theta}\) be the estimator

\[\hat{\theta}=\begin{cases}\bar{X}&\text{ if }\ |\bar{X}|>n^{-\frac{1}{4}}\\ a\bar{X}&\text{ if }\ |\bar{X}|\leq n^{-\frac{1}{4}}\end{cases}\]

where \(0\leq a<1\). We first show that the above estimate is superefficient. Let \(\text{AV}_{\hat{\theta}}(\theta)\) denote the asymptotic variance of \(\sqrt{n}(\hat{\theta}-\theta)\). We will show that

\[\text{AV}_{\hat{\theta}}(\theta)\begin{cases}\leq I^{-1}(\theta)&\text{ for all }\theta\neq 0\\ <I^{-1}(\theta)&\text{ for all }\theta=0.\end{cases} \tag{10.36}\]

Note that \(\sqrt{n}\,(\bar{X}-\theta)\,\stackrel{{\mathcal{D}}}{{=}}\,Z\), where \(Z\) has normal distribution with mean zero and unit variance.

Case I: \(\theta=0\). In this case,

\[\sqrt{n}(\hat{\theta}-0) =\sqrt{n}\,\bar{X}I(|\bar{X}|>n^{-\frac{1}{4}})+\sqrt{n}\,a\bar{X }I(|\bar{X}|\leq n^{-\frac{1}{4}}).\] \[\stackrel{{\mathcal{D}}}{{=}}ZI(|Z|>n^{\frac{1}{4}}) +aZI(|Z|\leq n^{\frac{1}{4}})\] \[\stackrel{{\mathcal{D}}}{{\to}}aZ\sim N(0,a^{2}).\]

Case II: \(\theta\neq 0\). In this case

\[\sqrt{n}(\hat{\theta}-\theta)=\sqrt{n}\,\bar{X}I(|\bar{X}|>n^{-\frac{1}{4}})+ \sqrt{n}\,a\bar{X}I(|\bar{X}|\leq n^{-\frac{1}{4}})-\sqrt{n}\,\theta.\]

Without loss of generality, assume \(\theta>0\). Note that

\[|\bar{X}|\leq n^{-\frac{1}{4}} \Leftrightarrow -n^{-\frac{1}{4}}\leq\bar{X}\leq n^{-\frac{1}{4}}\] \[\Leftrightarrow \sqrt{n}(-n^{-\frac{1}{4}}-\theta)\leq\sqrt{n}(\bar{X}-\theta) \leq\sqrt{n}(n^{-\frac{1}{4}}-\theta)\] \[\Rightarrow \sqrt{n}(\theta-\bar{X})\geq\sqrt{n}(\theta-n^{-\frac{1}{4}}).\]

Because \(\theta>0\), the right-hand side of the last line goes to \(\infty\). But we also know that, under \(P_{\theta}\), the term \(\sqrt{n}(\theta-\bar{X})=O_{P}(1)\) (in fact, \(\sqrt{n}(\theta-\bar{X})\sim N(0,1)\)). Therefore, by Lemma 10.5, for any sequence \(b_{n}>0\), we have \[I\left(\sqrt{n}(\theta-\bar{X})\geq\sqrt{n}(\theta-n^{-\frac{1}{4}})\right)=o(b_{n}),\]

which implies \(I(|\bar{X}|\leq n^{-\frac{1}{4}})=o_{P}(b_{n}).\) Take \(b_{n}=n^{-1/2},\) then we have \(I(|\bar{X}|\leq n^{-\frac{1}{4}})=o_{P}(n^{-1/2}).\) Hence

\[\sqrt{n}(\hat{\theta}-\theta)=\sqrt{n}\bar{X}(1+o_{P}(n^{-1/2}))+\sqrt{n}a\bar{ X}\ o_{P}(n^{-1/2})-\sqrt{n}\theta.\]

Because \(\bar{X}\stackrel{{ P}}{{\to}}\theta,\)\(\sqrt{n}\bar{X}=O_{P}(n^{1/2}).\) Therefore

\[\sqrt{n}(\hat{\theta}-\theta)=\sqrt{n}(\bar{X}-\theta)+o_{P}(1)\stackrel{{ \mathcal{D}}}{{\longrightarrow}}N(0,1).\]

Meanwhile, it is easy to see that the Fisher information in this case is \(I(\theta)\equiv 1.\) Therefore (10.36) holds.

Next, we show that \(\hat{\theta}\) is irregular at \(\theta=0.\) Since \(\theta_{n}(\delta)=\delta/\sqrt{n},\) under \(P_{\theta_{n}(\delta)},\)\(\sqrt{n}(\bar{X}-\delta/\sqrt{n})\) is distributed as \(N(0,1)\) and therefore has order of magnitude \(O_{P}(1).\) In the meantime,

\[|\bar{X}|>n^{-\frac{1}{4}} \Leftrightarrow \bar{X}>n^{-\frac{1}{4}}\text{ or }\bar{X}<-n^{-\frac{1}{4}}\] \[\Leftrightarrow \bar{X}-n^{-\frac{1}{2}}\delta>n^{-\frac{1}{4}}-n^{-\frac{1}{2}} \delta\text{ or }\bar{X}-n^{-\frac{1}{2}}\delta<-n^{-\frac{1}{4}}-n^{-\frac{1}{2}}\delta\] \[\Leftrightarrow \sqrt{n}(\bar{X}-n^{-\frac{1}{2}}\delta)>n^{\frac{1}{4}}-\delta \text{ or }-\sqrt{n}(\bar{X}-n^{-\frac{1}{2}}\delta)>n^{\frac{1}{4}}+\delta.\]

Hence

\[I(|\bar{X}|>n^{-\frac{1}{4}})\] \[\leq I\left(\sqrt{n}(\bar{X}-n^{-\frac{1}{2}}\delta)>n^{\frac{1}{ 4}}-\delta\right)+I\left(-\sqrt{n}(\bar{X}-n^{-\frac{1}{2}}\delta)>n^{\frac{1} {4}}+\delta\right).\]

Because

\[\sqrt{n}(\bar{X}-n^{-\frac{1}{2}}\delta)= O_{P}(1),\quad n^{\frac{1}{4}}-\delta\to\infty,\] \[-\sqrt{n}(\bar{X}-n^{-\frac{1}{2}}\delta)= O_{P}(1),\quad n^{\frac{1}{4}}+\delta\to\infty,\]

we have, by Lemma 10.5, for any \(b_{n}>0,\)

\[I(|\bar{X}|>n^{-\frac{1}{4}})=o_{P}(b_{n}),\quad\text{and in particular, }\ I(|\bar{X}|>n^{-\frac{1}{4}})=o_{P}(1).\]

Hence, under \(P_{\theta_{n}(\delta)},\)

\[\hat{\theta}=\bar{X}I(|\bar{X}|>n^{-\frac{1}{4}})+a\bar{X}I(|\bar{X}|\leq n^{- \frac{1}{4}})=a\bar{X}+o_{P}(n^{-1/2}).\]

It follows that

\[\sqrt{n}(\hat{\theta}-n^{-\frac{1}{2}}\delta) = \sqrt{n}(a\bar{X}-n^{-\frac{1}{2}}\delta)+o_{P}(1)\] \[= \sqrt{n}(a(\bar{X}-n^{-\frac{1}{2}}\delta+n^{-\frac{1}{2}}\delta) -n^{-\frac{1}{2}}\delta)+o_{P}(1)\] \[= \sqrt{n}a(\bar{X}-n^{-\frac{1}{2}}\delta)+\sqrt{n}(an^{-\frac{1}{ 2}}\delta-n^{-\frac{1}{2}}\delta)+o_{P}(1)\] \[= \sqrt{n}a(\bar{X}-n^{-\frac{1}{2}}\delta)+(a-1)\delta+o_{P}(1) \stackrel{{\mathcal{D}}}{{\longrightarrow}}N((a-1)\delta,a^{2}).\]

Since this distribution depends on \(\delta,\)\(\hat{\theta}\) is irregular at \(\theta=0.\)\(\Box\)The existence of superefficient estimates does not diminish the importance of asymptotically efficient estimates, as superefficient estimates are somewhat pathological. Suppose \(\hat{\theta}\) is a superefficient estimate. Let us say that \(\theta\in\Theta\) is a superefficient point if \(\mbox{AV}_{\hat{\theta}}(\theta)<I^{-1}(\theta)\). Let \(S\) be the set of all superefficient points. Then it can be shown that \(S\) has Lebesgue measure \(0\). See Le Cam (1953, 1960); Bahadur (1964); van der Vaart (1997). This issue will be further explored in a series problems in the Problems section.

## Problems

**10.1.** Let \(P_{n}\) and \(Q_{n}\) probability measures defined by the following distributions:

1. \(Q_{n}=N(0,1/n)\), \(P_{n}=N(0,1/n^{2})\);

2. \(Q_{n}=N(0,1/n)\), \(P_{n}=N(1/\sqrt{n},1/n)\);

3. \(Q_{n}=U(0,2/n)\), \(P_{n}=U(0,1/n)\);

4. \(Q_{n}=U(0,1/n)\), \(P_{n}=U(0,1/n^{2})\).

In each of the above scenarios,

1. Show that \(dP_{n}/dQ_{n}\stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}}V\) for some \(V\), and find the distribution of \(V\);

2. Compute \(E(V)\);

3. Prove or disprove \(P_{n}\lhd Q_{n}\).

**10.2.** Show that, if \(c>0\), then

1. \(f(x)=xI_{\{x\leq c\}}\) is an upper semi-continuous function bounded from above;

2. \(g(x)=(2-x)I_{\{x>2/(1+c)\}}\) is a lower semi-continuous function bounded from below.

**10.3.** Under the assumptions of Theorem 10.2, show that the following statements are equivalent:

1. \(P_{n}\lhd Q_{n}\);

2. If \(dP_{n}/dQ_{n}\stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}}V\) along a subsequence, then \(E(V)=1\), \(P(V>0)=1\).

**10.4.** Suppose that \(\hat{\theta}\) is a regular estimator of \(\theta_{0}\) and \(h\) is a differentiable function. Show that \(h(\hat{\theta})\) is a regular estimator of \(h(\theta_{0})\).

**10.5.** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. with \(E_{\mu}(X)=\mu\) and \(\mbox{var}_{\mu}(X)=1\). We are interested in estimating \(\mu^{2}\) by \(\bar{X}^{2}\). Let us say \(\{n^{a}\}\) is normalizing sequence if \(a\) is so chosen that \(n^{a}(\bar{X}^{2}-\mu^{2})=O_{P}(1)\) and \(n^{a}(\bar{X}^{2}-\mu^{2})\neq o_{P}(1)\).

1. If \(\mu\neq 0\), find the normalizing sequence \(n^{a}\) and the asymptotic distribution of \(n^{a}(\bar{X}^{2}-\mu^{2})\).

2. If \(\mu=0\), find the normalizing sequence \(n^{a}\) and the asymptotic distribution of \(n^{a}(\bar{X}^{2}-\mu^{2})\). What is the asymptotic variance of this distribution?3. If \(\mu_{n}=n^{-1/2}\delta\) where \(\delta\neq 0\), find the normalizing sequence \(n^{a}\) and the asymptotic distribution of \(n^{a}(\bar{X}^{2}-\mu_{n}^{2})\). What is the asymptotic variance of this distribution?
4. Is \(\bar{X}^{2}\) a regular estimator of \(\mu^{2}\) if at \(\mu=0\)?

Suppose that \(\hat{\theta}\) is a regular estimate of a scalar parameter \(\theta_{0}\), and that \((\sqrt{n}(\hat{\theta}-\theta_{0}),S_{n},L_{n})\) satisfies ALAN. Denote the limiting distribution of \(\sqrt{n}(\hat{\theta}-\theta_{n}(\delta))\) under \(P_{n}(\delta)\) by \(N(0,\sigma^{2})\). Let \(h\) be a differentiable function of \(\theta\). Define the notion of a normalizing sequence as in the last problem.

1. Suppose that \(\dot{h}(\theta_{0})\neq 0\). Find the normalizing sequence \(n^{a}\) in \(n^{a}[h(\hat{\theta})-h(\theta_{0})]\) and derive the asymptotic distribution of \(n^{a}[h(\hat{\theta})-h(\theta_{n}(\delta))]\) under \(P_{n}(\delta)\). Is \(h(\hat{\theta})\) regular at \(\theta_{0}\)?
2. Suppose that \(h\) is twice differential at \(\theta_{0}\) and that \(\dot{h}(\theta_{0})=0\). Find the normalizing sequence \(n^{a}\) in \(n^{a}[h(\hat{\theta})-h(\theta_{0})]\) and derive the asymptotic distribution of \(n^{a}[h(\hat{\theta})-h(\theta_{n})]\) under \(P_{n}(\delta)\). Is \(h(\hat{\theta})\) regular at \(\theta_{0}\)?

Let \(\theta\) be a \(p\)-dimensional vector, and suppose that \(\hat{\theta}\) is a regular estimate of \(\theta\), and that \((\sqrt{n}(\hat{\theta}-\theta_{0}),S_{n},L_{n})\) satisfies ALAN. Let

\[\tilde{\theta}=\begin{cases}\hat{\theta}&\text{ if }\|\hat{\theta}-\theta_{0} \|>n^{-\frac{1}{4}}\\ \theta_{0}+a(\hat{\theta}-\theta_{0})&\text{ if }\|\hat{\theta}-\theta_{0}\|\leq n ^{-\frac{1}{4}}\end{cases}\]

where \(0<a<1\). Denote \(U_{n}=\sqrt{n}(\tilde{\theta}-\theta_{0})\). Show that \(U_{n}\) satisfies ALAN at under \(\theta_{0}\), and derive the asymptotic covariance \(\Sigma_{US}\) between \(U_{n}\) and \(S_{n}\) under \(\theta_{0}\). Is \(\tilde{\theta}\) a regular estimate of \(\theta_{0}\)?

Suppose \(T_{n}\) is a regular estimate of \(\theta_{0}\in\mathbb{R}^{p}\). Assuming \(p\geq 0\), the James-Stein-type estimator of \(\theta\) based on \(T_{n}\) can be defined as follows

\[U_{n}=T_{n}-(p-2)\frac{T_{n}}{\|\sqrt{n}T_{n}\|^{2}},\]

where \(\|\cdot\|\) is the Euclidean norm. In the following, let \(Z\) represent the standard Normal random variable.

1. Write down the local alternative asymptotic distribution of \(\sqrt{n}(U_{n}-\theta_{n}(\delta))\) at \(\theta_{0}\neq 0\) in terms of \(Z\). Is \(U_{n}\) regular at \(\theta\neq 0\)?
2. Write down the local alternative asymptotic distribution of \(\sqrt{n}(U_{n}-\theta_{n}(\delta))\) at \(\theta_{0}=0\) in terms of \(Z\). Is \(U_{n}\) regular at \(\theta_{0}=0\)?

Suppose \(Q_{n}\) is the probability measure \(N(0,\sigma_{n}^{2})\) and \(P_{n}\) is the probability measure \(N(\theta_{n},\sigma_{n}^{2})\). Prove the following statements:

1. If \(\lim_{n\to\infty}\mu_{n}/\sigma_{n}\to\rho\) for some \(\rho\neq 0\), then \(P_{n}\lhd Q_{n}\);
2. If \(\lim_{n\to\infty}\mu_{n}/\sigma_{n}=0\), then \(P_{n}\lhd Q_{n}\);
3. If \(\lim_{n\to\infty}\mu_{n}/\sigma_{n}=\infty\), then \(P_{n}\) is not contiguous with respect to \(Q_{n}\).

**10.10**.: A distance between two probability measures, say \(P\) and \(Q\), is defined by

\[\|P-Q\|=\sup_{A}|P(A)-Q(A)|,\]

where the supremum is taken all measurable sets. Let \(\{P_{n}\}\) and \(\{Q_{n}\}\) be two sequences of probability measures. Show that if \(\|P_{n}-Q_{n}\|\to 0\) as \(n\to\infty\) then \(P_{n}\lhd Q_{n}\).

**10.11**.: Let \(K:R^{1}\mapsto R^{1}\) be a function such that (a) \(0<K(0)<1\), (b) \(\lim_{|u|\to\infty}K(u)=1\), and (c) \(K\) is differentiable and has bounded derivative; that is, \(\dot{K}(u)<C\) for some \(C>0\). Let \(\hat{\theta}\) be a regular estimator of \(\theta\).

1. Show that \(K(n^{1/4}\hat{\theta})\overset{P}{\to}K(0)\) under \(\theta=0\) and \(K(n^{1/4}\hat{\theta})\overset{P}{\to}1\) under \(\theta\neq 0\).

2. Show that \(K(n^{1/4}\hat{\theta})\overset{P}{\to}K(0)\) under \(\theta_{n}=n^{-1/2}\delta\) and \(K(n^{1/4}\hat{\theta})\overset{P}{\to}1\) under \(\theta_{n}=\theta+n^{-1/2}\delta\), with \(\theta_{0}\neq 0\).

3. Let \(\tilde{\theta}=K(n^{1/4}\tilde{\theta})\tilde{\theta}\). Derive the asymptotic distribution of \(\sqrt{n}(\tilde{\theta}-\theta_{n}(\delta))\) under \(\theta_{n}(\delta)\), where \(\theta_{n}(\delta)=n^{-1/2}\delta\). Is \(\tilde{\theta}\) regular at \(0\)?

Remarks on Problems 10.12 through 10.15

An alternative definition of a regular estimate is the following: \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\) if, for any sequence of parameters \(\theta_{n}\) such that \(\sqrt{n}(\theta_{n}-\theta_{0})\) is bounded, we have \(\sqrt{n}[\hat{\psi}-h(\theta_{0})]\overset{\mathcal{D}}{\underset{P_{n}\theta _{n}}{\longrightarrow}}Z\), where the distribution of \(Z\) does not depend on the sequence \(\theta_{n}\) chosen. This alternative definition obviously implies Definition 10.3; it is equivalent to Definition 10.3 under the following assumption: if \(\delta_{n}\to\delta\), then

\[L_{n}(\delta_{n})\overset{Q_{n}}{\,\,\,\,=\,\,\,}L_{n}(\delta)+o_{P}(1). \tag{10.37}\]

Problems 10.12 through 10.15 provide a proof of the equivalence, and also give a sufficient condition for (10.37). These problems touch on many aspects of this chapter.

**10.12**.: Suppose that \((S_{n},L_{n}(\delta))\) satisfies LAN, and \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\) in the sense of Definition 10.3. Show that

\[\begin{pmatrix}U_{n}\\ L_{n}(\delta)\end{pmatrix}\overset{\mathcal{D}}{\underset{Q_{n}}{\longrightarrow}} \begin{pmatrix}U\\ \delta^{T}S-\delta^{T}I\delta/2\end{pmatrix}.\]

**10.13**.: Suppose the conditions in Problem 10.12 are satisfied. Consider a sequence of local alternative parameters \(\theta_{n}\) such that

\[\sqrt{n}(\theta_{n}-\theta_{0})\equiv\delta_{n}\to\delta\]

for some \(\delta\in\mathbb{R}^{p}\). Note that \(\theta_{n}\) can be written as \(\theta_{0}+n^{-1/2}\delta_{n}\), and \(P_{n\theta_{n}}\) can be written as \(P_{n}(\delta_{n})\). Let

[MISSING_PAGE_EMPTY:6164]

where \(E[\rho(\theta_{0},X)]=0\) and \(\rho(\theta_{0},X)\) is \(P_{\theta_{0}}\)-square-integrable. Suppose that \(\rho(\theta,X)f_{\theta}(X)\) satisfies \(\mbox{DUI}^{+}(\theta,\mu)\). Use Theorem 10.4 to show that \(\hat{\psi}\) is a regular estimate of \(h(\theta_{0})\) if and only if

\[E\left[\frac{\partial\rho(\theta_{0},X)}{\partial\theta^{T}}\right]=-\dot{h}( \theta_{0}).\]

**10.19.** Recall from Theorem 9.7 that, under the assumptions of Theorem 9.5, a consistent solution \(\hat{\theta}\) to the estimating equation \(E_{n}[g(\theta,X)]=0\) can be expanded as the form

\[\hat{\theta}=\theta_{0}-J_{g}^{-1}(\theta_{0})E[g(\theta_{0},X)]+o_{P}(1),\]

where

\[J_{g}(\theta_{0})=E\left[\frac{\partial g(\theta_{0},X)}{\partial\theta^{T}} \right].\]

Show that, under the conditions in Theorem 9.5, \(\hat{\theta}\) is a regular estimate of \(\theta_{0}\).

**10.20.** Suppose that \((S_{n},L_{n})\) satisfies LAN, and \(\theta=(\psi^{T},\lambda^{T})\), where \(\psi\in\mathbb{R}^{r}\) is the parameter of interest, and \(\lambda\in\mathbb{R}^{s}\) is the nuisance parameter. Suppose \(\hat{\psi}\) is a regular estimate of \(\psi\). Show that \(\sqrt{n}(\hat{\psi}-\psi_{0})\mathop{\hbox{\rm 1\kern-2.27622ptl}}S_{\lambda}\), where \(S_{\lambda}\) is the last \(s\) components of \(S\), and \(S\) is as defined in the LAN assumption.

**10.21.** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. with density \(f(x-\theta)\) where \(f\) is a known symmetric p.d.f. defined on \(\mathbb{R}\) satisfying

\[\int_{-\infty}^{\infty}[\dot{f}(t)/f(t)]^{2}f(t)dt<\infty,\ \ \lim_{t\to\infty}f(t)=0,\]

where \(\dot{f}\) denote the derivative of \(f\). Let \(T_{1}=\bar{X}\) and \(T_{2}\) be the sample median of \(X\). It can be shown that \(T_{2}\) satisfies

\[T_{2}\ \stackrel{{ Q_{n}}}{{=}}\ \theta_{0}-\frac{1}{f(0)}E_{n}[I(X \leq\theta_{0})-1/2]+o_{P}(1).\]

1. Derive the asymptotic distribution of \(\sqrt{n}(T_{2}-\theta_{0})\) under \(Q_{n}\).

2. Show that \(T_{2}\) satisfies LAN and is a regular estimate of \(\theta_{0}\).

3. Derive the asymptotic distribution of \(\sqrt{n}(T_{2}-\theta_{0})\) under \(P_{n}(\delta)\).

4. Derive the asymptotic distribution of \(\sqrt{n}(T_{1}-\theta_{0},T_{2}-\theta_{0})\) under \(Q_{n}\).

5. Derive the asymptotic distribution of \(\sqrt{n}(T_{1}-\theta_{0},T_{2}-\theta_{0})\) under \(P_{n}(\delta)\).

_Remarks on Problems 10.22 through 10.28_

Bahadur (1964) gave a relatively simple proof that the collection of superefficient points have Lebesgue measure \(0\). This method uses the Neyman-Pearson Lemma to derive an inequality concerning the null and local alternative distributions. The next few problems walk through his proof (with adaptation to our context, assumptions, and notations). Proving the various steps of this result turns out to be excellent exercises, as it involves many techniques developed in this chapter. In the context of \(1\)-dimensional \(\theta\), it suffices to consider one \(\delta\) value, say \(\delta=1\). Thus in the following we use \(\theta_{n}(1)\), \(L_{n}(1)\), and \(P_{n}(1)\) and so on. Also, note that \(P_{n}(1)\) and \(P_{n\theta_{n(1)}}\) mean the same probability measure. Throughout these problems, we will always make the following assumptions

1. \((S_{n},L_{n}(1))\) satisfies LAN;
2. \(\hat{\theta}\) is an estimate of \(\theta_{0}\) such that \(\sqrt{n}(\hat{\theta}-\theta_{0})\xrightarrow[Q_{n}]{\mathcal{D}}N(0,v(\theta_ {0}))\).

Our goal is to show that the collection of superefficient points \(\{\theta_{0}:v(\theta_{0})<I^{-1}(\theta_{0})\}\) has Lebesgue measure \(0\).

**10.22.** Use Corollary 10.1 to show that \(L_{n}(1)\xrightarrow[P_{n}(1)]{\mathcal{D}}N(I/2,I)\).

**10.23.** Let \(K_{n}=[L_{n}(1)+I/2]/\sqrt{I}\). Show that

1. \(K_{n}\xrightarrow[Q_{n}]{\mathcal{D}}N(0,1),\quad K_{n}\xrightarrow[P_{n}(1)] {\mathcal{D}}N(\sqrt{I},1)\);
2. for any \(k>\sqrt{I}\), \(\lim_{n\to\infty}P_{n\theta_{n}(1)}(K_{n}\geq k)<1/2\).

**10.24.** Use the Neyman-Pearson lemma to show that, if \(C\) is the event \(K_{n}{\geq}k\), and \(D\) is any other event such that

\[P_{n\theta_{n}(1)}(C)<P_{n\theta_{n}(1)}(D),\]

then \(P_{n\theta_{0}}(C)<P_{n\theta_{0}}(D)\).

**10.25.** Show that, if

\[\limsup_{n}P_{n\theta_{n}(1)}(\hat{\theta}\geq\theta_{n}(1))\geq 1/2,\]

then, for any \(k>\sqrt{I}\), there is a subsequence \(n^{\prime}\) such that

\[P_{n^{\prime}\theta_{n^{\prime}}(1)}(K_{n^{\prime}}\geq k)<P_{n^{\prime}\theta _{n^{\prime}}(1)}(\hat{\theta}\geq\theta_{n^{\prime}}(1)),\]

whence use the result of Problem 10.24 to conclude that

\[P_{n^{\prime}\theta_{0}}(K_{n^{\prime}}\geq k)<P_{n^{\prime}\theta_{0}}(\hat{ \theta}\geq\theta_{0}). \tag{10.38}\]

**10.26.** By taking limits on both sides of (10.38), show that \(v(\theta_{0})\geq k^{-2}\) for any \(k>I^{1/2}(\theta_{0})\). Conclude that \(v(\theta_{0})\geq I^{-1}(\theta_{0})\), and whence that

\[\{\theta_{0}:\limsup_{n}P_{n\theta_{n}(1)}(\hat{\theta}\geq\theta_{n}(1))\geq 1 /2\}\subseteq\{\theta_{0}:v(\theta_{0})\geq I^{-1}(\theta_{0})\}.\]

**10.27.** Let \(\Delta_{n}(\theta)=|P_{\theta}(\hat{\theta}<\theta)-1/2|\), and \(\Phi\) the c.d.f. of \(N(0,1)\). Use the Bounded Convergence Theorem to show that

\[\lim_{n\to\infty}\int\Delta_{n}(\theta+n^{1/2})d\Phi(\theta)=0.\]

Using this to show that \(\Delta_{n}\smash{\mathop{\longrightarrow}\limits^{\Phi}}0\) (i.e. \(\Delta_{n}\) converges in \(\Phi\)-probability to \(0\)).

**10.28.** Using the fact that, if \(U_{n}\stackrel{{ P}}{{\to}}a\), then \(U_{n}\to a\) almost surely along some subsequence of \(\{n\}\), to show that

\[\Phi\left(\{\theta_{0}:\liminf_{n}\Delta_{n}(\theta_{n})=0\}\right)=1,\]

whence conclude, in turn,

\[\begin{array}{l}1.\;\Phi(\{\theta_{0}:\limsup_{n}P_{n\theta_{n}(1)}(\hat{ \theta}\geq\theta_{n}(1))\geq 1/2\})=1;\\ 2.\;\Phi(\{\theta_{0}:v(\theta_{0})\geq I^{-1}(\theta_{0})\})=1.\\ 3.\;\mbox{the set $\{\theta_{0}:v(\theta_{0})<I^{-1}(\theta_{0})\}$ has Lebesgue measure $0$.}\end{array}\]

## References

* [1] Bahadur, R. R. (1964). On Fisher's bound for asymptotic variances. _The Annals of Mathematical Statistics_. **35**, 1545-1552.
* [2] Bickel, P. J., Klaassen, C. A. J., Ritov, Y. Wellner, J. A. (1993). _Efficient and Adaptive Estimation for Semiparametric Models_. The Johns Hopkins University Press.
* [3] Fisher, R. A. (1922). On the mathematical foundations of theoretical statistics. _Philosophical Transactions of the Royal Society A_, **222**, 594-604.
* [4] Fisher, R. A. (1925). Theory of statistical estimation. _Proc. Cambridge Phil. Soc._, **22**, 700-725.
* [5] Hajek, J. (1970). A characterization of limiting distributions of regular estimates. _Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete_. **14**, 323-330.
* [6] Hall, W. J. and Mathiason, D. J. (1990). On large-sample estimation and testing in parametric models. _Int. Statist. Rev._, **58**, 77-97.
* [7] Le Cam, L. (1953). On some asymptotic Properties of maximum likelihood estimates and related Bayes estimates. _Univ. California Publ. Statistic._**1**, 277-330.

* [14]Le Cam, L. (1960). Locally asymptotically normal families of distributions. _Univ. California Publ. Statistic._**3**, 370-98.
* [15]Le Cam, L. and Yang, G. L. (2000). _Asymptotics in Statistics: Some Basic Concepts_. Second Edition. Springer.
* [16]van der Vaart, A. W. (1997). Superefficiency. _Festschrift for Lucien Le Cam: Research Papers in Probability and Statistics,_ 397-410. Springer.
* [17]van der Vaart, A. W. (1998). _Asymptotic Statistics_. Cambridge University Press.

## Asymptotic Hypothesis Test

In this chapter we develop various asymptotic methods for testing statistical hypotheses under the general framework of Quadratic Form tests (QF test, Hall and Mathiason 1990), a class of statistics that are asymptotically equivalent to quadratic forms in statistics that satisfy the ALAN assumption in Chapter 10. The asymptotic null and local alternative distributions of a QF test can be easily derived from Le Cam's third lemma. Several commonly used test statistics will be shown to be special cases of QF tests, including Wilks's likelihood ratio test, Wald's test, Rao's score test, Neyman's \(C(\alpha)\) test, the Lagrangian multiplier test, as well as tests based on estimating equations. We first consider the testing problem that involves an explicit parameter of interest and an explicit nuisance parameter, and then the more general testing problem where the null hypothesis is specified by an arbitrary nonlinear equation of parameters. We will also introduce the concept of asymptotically efficient QF test whose local power is the greatest among the collection of all QF tests, and Pitman's efficiency that can be used to numerically compare the powers of two tests.

### Quadratic Form test

Consider the setting where \(\theta\) is a \(p\)-dimensional parameter consisting of an \(r\)-dimensional parameter of interest \(\psi\) and an \(s\)-dimensional nuisance parameter \(\lambda\); that is, \(\theta=(\psi,\lambda)\). We allow \(r=p\), so as to accommodate the special case \(\psi=\theta\) - that is, the entire parameter \(\theta\) is of interest. Thus, the null hypothesis is

\[H_{0}:\psi=\psi_{0}. \tag{11.1}\]

Here, in the asymptotic approach to hypothesis testing, we consider the following sequence of local alternative hypotheses:

\[H_{1}^{(n)}:\theta=\theta_{n}(\delta),\text{ where }\theta_{n}(\delta)=\theta_{0 }+n^{-1/2}\delta.\]Note that the local alternative hypothesis involves the nuisance parameter \(\lambda_{0}\), which does not appear in the null hypothesis. However, \(\lambda_{0}\) will always remain offstage, and will not affect the further development in anyway. As before, let \(Q_{n}\) denote the null probability measure \(P_{n\theta_{0}}\) and \(P_{n}(\delta)\) the local alternative probability measure \(P_{n\theta_{n}(\delta)}\).

A more general testing problem than (11.1) is

\[H_{0}:\,h(\theta)=0, \tag{11.2}\]

where \(h\) is an arbitrary differentiable function. This will be taken up in a later section. Although the setting (11.1) is a special case of (11.2), and all the related theories under (11.1) are special cases of their counterparts under (11.2), we will nevertheless first develop the special case and then move on to the more general case. We choose this somewhat inefficient way of presentation because (11.1) is the most commonly used form of hypothesis, and also because this special case helps to develop intuition, especially that related to the efficient score and efficient information. We now give the definition of a Quadratic Form test for the hypothesis (11.1). Let \(L_{n}\) and \(S_{n}\) be the local log likelihood ratio and the standardized score as defined in Definition 10.2 and the note immediately following it. For random vectors \(V_{n}\) and \(W_{n}\), recall that we write \(V_{n}\ \stackrel{{ Q_{n}}}{{=}}\ W_{n}+o_{P}(1)\) to mean \(Q_{n}(\|V_{n}-W_{n}\|>\epsilon)\to 0\) for every \(\epsilon>0\).

**Definition 11.1**: _A test statistic \(T_{n}\in\mathbb{R}\) is a Quadratic Form (QF) test if there is an \(r\)-dimensional random vector \(U_{n}\) such that \((U_{n},S_{n},L_{n})\) satisfies ALAN with \(\Sigma_{U}\succ 0\), and such that_

\[T_{n}\ \stackrel{{ Q_{n}}}{{=}}\ U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P }(1). \tag{11.3}\]

Using Le Cam's third lemma under ALAN (Corollary 10.6), we can easily derive the asymptotic null and alternative distribution of a QF test.

**Theorem 11.1**: _If \(T_{n}\) is a QF test of the form (11.3), then, for any \(\delta\in\mathbb{R}^{p}\),_

\[T_{n}\ \stackrel{{\mathcal{D}}}{{\underset{P_{n}(\delta)}{ \longrightarrow}}}\chi_{r}^{2}\left(\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_ {US}\delta\right).\]

_In particular, when \(\delta=0\), we have \(T_{n}\ \stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}} \chi_{r}^{2}\)._

Proof: Because \(T_{n}\) is a QF test, it can be written as (11.3) where

\[\begin{pmatrix}U_{n}\\ S_{n}\end{pmatrix}\ \stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{ \longrightarrow}}}N\left[\begin{pmatrix}0\\ 0\end{pmatrix},\ \begin{pmatrix}\Sigma_{U}&\Sigma_{US}\\ \Sigma_{SU}&I\end{pmatrix}\right].\]

By Corollary 10.6, \(U_{n}\ \stackrel{{\mathcal{D}}}{{\underset{P_{n}(\delta)}{ \longrightarrow}}}N(\Sigma_{US}\delta,\Sigma_{U})\). Hence

\[\Sigma_{U}^{-1/2}U_{n}\ \stackrel{{\mathcal{D}}}{{\underset{P_{n}( \delta)}{\longrightarrow}}}N(\Sigma_{U}^{-1/2}\Sigma_{US}\delta,I_{r}),\]which implies

\[U_{n}^{T}\Sigma_{U}^{-1}U_{n}\xrightarrow[P_{n}(\delta)]{\mathcal{D}}\chi_{r}^{2}( \delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US}\delta).\]

Also, because \(P_{n}(\delta)\lhd Q_{n}\), by Proposition 10.3,

\[T_{n}\stackrel{{ P_{n}(\delta)}}{{=}}U_{n}^{T}\Sigma_{U}^{-1}U_{n} +o_{P}(1).\]

Now apply Slutsky's theorem to prove the asserted convergence. 

The above theorem applies to all QF tests, which include many well known test statistics. Thus, as long as we can show that a particular statistic is a QF test, then we can automatically write down their null and local alternative distributions using the the above theorem. In the following few sections we shall show that a set of most commonly used test statistics are QF tests.

### 11.2 Wilks's likelihood ratio test

First, consider the parametric model as specified by Assumption 10.1. Suppose \(\hat{\theta}\) is the maximum likelihood estimate, and \(\tilde{\theta}=(\psi_{0}^{T},\tilde{\lambda}^{T})^{T}\) is the maximum likelihood estimate under the constraint \(\psi=\psi_{0}\). Then the likelihood ratio test is defined as

\[T_{n}=2\log(dP_{n\hat{\theta}}/dP_{n\tilde{\theta}}),\]

which is also known as Wilks's test (Wilks, 1938). Under the i.i.d. parametric model (Assumption 10.3), the above reduces to

\[T_{n}=2nE_{n}[\ell(\hat{\theta},X)-\ell(\tilde{\theta},X)].\]

The next theorem shows that \(T_{n}\) is a QF test under the i.i.d. model. Recall the notations

\[J(\theta)=E_{\theta}[\partial s(\theta,X)/\partial\theta^{T}],\quad K(\theta)= E_{\theta}[s(\theta,X)s^{T}(\theta,X)],\]

where \(s(\theta,X)\) is the score function \(\partial\log f_{\theta}(X)/\partial\theta\) for an individual observation \(X\). Also recall that, if \(f_{\theta}(X)\) and \(s(\theta,X)f_{\theta}(X)\) satisfy \(\text{DUI}^{+}(\theta,\mu)\), and \(s(\theta,X)\) is \(P_{\theta}\)-square integrable, then \(K(\theta)=-J(\theta)\), and the common matrix \(I(\theta)\) is known as the Fisher information. Let \(I_{\psi\cdot\lambda}(\theta)\) and \(s_{\psi\cdot\lambda}(\theta,X)\) be the efficient information and efficient score defined in Section 9.8. Let

\[J_{n}(\theta)=E_{n}[\partial s(\theta,X)/\partial\theta^{T}]. \tag{11.4}\]

**Theorem 11.2**_Suppose Assumption 10.3 holds and_

1. \(\ell(\theta,x)\) _is twice differentiable;_
2. \(f_{\theta}(X)\) _and_ \(s(\theta,X)f_{\theta}(X)\) _satisfy_ \(\text{DUI}^{+}(\theta,\mu)\)

[MISSING_PAGE_FAIL:339]

\[T_{n}^{(1)}/(2n) \stackrel{{ Q_{n}}}{{=}}[(\hat{\theta}-\theta_{0})^{T}I( \theta_{0})+o_{P}(n^{-1/2})](\hat{\theta}-\theta_{0})\] \[\quad+\frac{1}{2}(\hat{\theta}-\theta_{0})^{T}[-I(\theta_{0})+o_{P }(1)](\hat{\theta}-\theta_{0}).\]

Because (11.6) also implies \(\hat{\theta}-\theta_{0}\stackrel{{ Q_{n}}}{{=}}O_{P}(n^{-1/2})\), we have

\[T_{n}^{(1)}/(2n)\stackrel{{ Q_{n}}}{{=}}\frac{1}{2}(\hat{\theta}- \theta_{0})^{T}I(\theta_{0})(\hat{\theta}-\theta_{0})+o_{P}(n^{-1}). \tag{11.7}\]

By a similar argument we can show that

\[T_{n}^{(2)}/(2n)\stackrel{{ Q_{n}}}{{=}}\frac{1}{2}(\hat{ \lambda}-\lambda_{0})^{T}I_{\lambda\lambda}(\theta_{0})(\hat{\lambda}-\lambda_ {0})+o_{P}(n^{-1}). \tag{11.8}\]

Next, we establish an asymptotic linear relation between \(\hat{\theta}-\theta_{0}\) and \(\tilde{\lambda}-\lambda_{0}\). Applying Theorem 9.5 to the estimating equation \(E_{n}[s_{\lambda}(\psi_{0},\lambda,X)]=0\) where \(\psi_{0}\) is fixed and \(\lambda\) alone is the argument, under the assumption that \(\tilde{\lambda}\) is a consistent solution to this estimating equation (condition 5), we have

\[\begin{array}{rl}\tilde{\lambda}-\lambda_{0}&\stackrel{{ Q_{n}}}{{=}}I_{ \lambda\lambda}^{-1}(\theta_{0})E_{n}[s_{\lambda}(\theta_{0},X)]+o_{P}(1)\\ &\stackrel{{ Q_{n}}}{{=}}[0,I_{\lambda\lambda}^{-1}(\theta_{0})]E_ {n}[s(\theta_{0},X)]+o_{P}(1)\\ &\stackrel{{ Q_{n}}}{{=}}[0,I_{\lambda\lambda}^{-1}(\theta_{0})]I( \theta_{0})(\hat{\theta}-\theta_{0})+o_{P}(1),\end{array} \tag{11.9}\]

where, for the third equality we used again the relation (11.6). Substituting this into the right-hand side of (11.8), we have

\[\begin{array}{rl}T_{n}^{(2)}/(2n)&\stackrel{{ Q_{n}}}{{=}}\frac{1}{2}(\hat{\theta}-\theta_{0})^{T}I\left(\begin{matrix}0\\ I_{\lambda\lambda}^{-1}\end{matrix}\right)I_{\lambda\lambda}\left(0\;\;I_{\lambda\lambda}^{-1}\right)I(\hat{\theta}-\theta_{0})+o_{P}(1)\\ &\stackrel{{ Q_{n}}}{{=}}\frac{1}{2}(\hat{\theta}-\theta_{0})^{T} \binom{I_{\psi\lambda}I_{\lambda\lambda}^{-1}I_{\lambda\psi}\;\;I_{\psi\lambda}}{I_{\lambda\psi}}(\hat{\theta}-\theta_{0})+o_{P}(1).\end{array}\]

Hence

\[\begin{array}{rl}T_{n}/(2n)=&T_{n}^{(1)}/(2n)-T_{n}^{(2)}/(2n)\\ &\stackrel{{ Q_{n}}}{{=}}\frac{1}{2}(\hat{\theta}-\theta_{0})^{T} \binom{I_{\psi\cdot\lambda}\;0}{0}(\hat{\theta}-\theta_{0})+o_{P}(n^{-1})\\ &\stackrel{{ Q_{n}}}{{=}}\frac{1}{2}(\hat{\psi}-\psi_{0})^{T}I_{ \psi\cdot\lambda}(\hat{\psi}-\psi_{0})+o_{P}(n^{-1}).\end{array} \tag{11.10}\]

By Theorem 9.7 we have

\[\hat{\psi}-\psi_{0}\stackrel{{ Q_{n}}}{{=}}I_{\psi\cdot\lambda}^{-1}\,E_{n}[s_{\psi\cdot\lambda}(\theta_{0},X)]+o_{P}(n^{-1/2}).\]

Substituting the above into the right-hand side of (11.10), we have \[\begin{array}{rl}T_{n}&\stackrel{{ Q_{n}}}{{=}}nE_{n}[s_{\psi\cdot \lambda}^{T}(\theta_{0},X)]I_{\psi\cdot\lambda}^{-1}(\theta_{0})E_{n}[s_{\psi \cdot\lambda}(\theta_{0},X)]+o_{P}(1)\\ &\equiv U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1).\end{array} \tag{11.11}\]

It remains to show that \((U_{n},S_{n})\) converges to a multivariate normal distribution with asserted forms of \(\Sigma_{U}\) and \(\Sigma_{US}\). For convenience, we abbreviate \(s(\theta_{0},X)\), \(s_{\lambda}(\theta_{0},X)\), \(s_{\psi}(\theta_{0},X)\), and \(s_{\psi\cdot\lambda}(\theta_{0},X)\) by \(s\), \(s_{\psi}\), \(s_{\lambda}\), and \(s_{\psi\cdot\lambda}\). By the central limit theorem

\[\begin{pmatrix}n^{1/2}E_{n}(s_{\psi\cdot\lambda})\\ n^{1/2}E_{n}(s)\end{pmatrix}\stackrel{{\mathcal{D}}}{{\longrightarrow}}N\left[\begin{pmatrix}0\\ 0\end{pmatrix},\;\begin{pmatrix}E(s_{\psi\cdot\lambda}s_{\psi\cdot\lambda}^{T})& E(s_{\psi\cdot\lambda}s^{T})\\ E(ss_{\psi\cdot\lambda}^{T})&E(ss^{T})\end{pmatrix}\right],\]

where

\[E(s_{\lambda}s_{\psi\cdot\lambda}^{T})= I_{\lambda\psi}-I_{\lambda\lambda}I_{\lambda\lambda}^{-1}I_{\lambda \psi}=0,\] \[E(s_{\psi}s_{\psi\cdot\lambda}^{T})= I_{\psi\psi}-I_{\psi\lambda}I_{\lambda\lambda}^{-1}I_{\lambda\psi}=I_{ \psi\cdot\lambda}.\]

Hence \(\Sigma_{SU}=(I_{\psi\cdot\lambda},0)^{T}\). Also, by Theorem 9.8,

\[\Sigma_{U}=E(s_{\psi\cdot\lambda}s_{\psi\cdot\lambda}^{T})=I_{\psi\cdot\lambda},\]

thus proving the case of \(r<p\).

The case of \(r=p\) can be proved by substituting (11.6) into (11.7). \(\Box\)

From Theorems 11.1 and 11.2 we can easily derive the null and local alternative distributions of the likelihood ratio test.

**Theorem 11.3**_Under the conditions in Theorem 11.2, we have, for any \(\delta\in\mathbb{R}^{p}\),_

\[T_{n}\stackrel{{\mathcal{D}}}{{\longrightarrow}}\chi_{r}^{2}( \delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta_{\psi}),\]

_where \(\delta_{\psi}\) is the first \(r\) components of \(\delta\). In particular, when \(\delta=0\), we have \(T_{n}\stackrel{{\mathcal{D}}}{{\longrightarrow}}\chi_{r}^{2}\)._

Proof.: By Theorem 11.1, \(T_{n}\stackrel{{\mathcal{D}}}{{\longrightarrow}}\chi_{r}^{2}( \delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US}\delta)\). Substituting into the noncentrality parameter the forms of \(\Sigma_{U}\) and \(\Sigma_{US}\) as given in Theorem 11.2, we have

\[\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US}\delta=\delta^{T}\begin{pmatrix} I_{\psi\cdot\lambda}\\ 0\end{pmatrix}I_{\psi\cdot\lambda}^{-1}\left(I_{\psi\cdot\lambda}\;0\right) \delta=\delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta_{\psi},\]

as desired. \(\Box\)

It is interesting to note that the limiting distribution of \(T_{n}\) under \(P_{n}(\delta)\) only depends on \(\delta_{\psi}\). In the local parametric family \(\{P_{n}(\delta):\delta\in\mathbb{R}^{p}\}\),plays the role of the parameter of interest \(\psi\), and \(\delta_{\lambda}\) plays the role of the nuisance parameter \(\lambda\). Thus, the limiting distribution of \(T_{n}\) only depends on the parameter of interest in the local parametric family.

In practice, we use the null distribution in Theorem 11.1 to determine the critical value of the rejection region, and use the local alternative distribution to determine the power at an arbitrary \(\theta_{0}+n^{-1/2}\delta\). Since \(\lambda_{0}\) is not specified in the null hypothesis, we can replace it by \(\tilde{\lambda}\). The estimated power is \(\sqrt{n}\)-consistent estimate of the true power at \(\theta_{0}+n^{-1/2}\delta\).

### Wald's, Rao's, and Neyman's tests

In this section we introduce three more commonly used tests which turn out to be QF-tests having the same form of \(U_{n}^{T}\Sigma_{U}^{-1}U_{n}\) as the likelihood ratio test.

#### Wald's test

Wald (1943) introduced the following statistic

\[W_{n}=n(\hat{\theta}-\theta_{0})^{T}I(\theta_{0})(\hat{\theta}-\theta_{0})\]

for testing the hypothesis \(H_{0}:\theta=\theta_{0}\) asymptotically. He showed that this statistic converges in distribution to \(\chi_{p}^{2}\). For the more general hypothesis (11.1), Wald's test takes the following form.

**Definition 11.2**: _The Wald test for hypothesis (11.1) takes the form_

\[W_{n}=n(\hat{\psi}-\psi_{0})^{T}I_{\psi\cdot\lambda}(\bar{\theta})(\hat{\psi}- \psi_{0}),\]

_where \(\bar{\theta}\) is any consistent estimate of \(\theta_{0}\)._

The often-used \(\bar{\theta}\) is the global MLE \(\hat{\theta}\), or the constrained MLE \((\psi_{0},\tilde{\lambda})\). Since Wald's test itself is already in a quadratic form, it is no surprise that it is a QF test, as shown in the next Theorem.

**Theorem 11.4**: _If the conditions in Theorem 9.7 hold and \(I(\theta)\) is a continuous function of \(\theta\), then \(W_{n}\) in Definition 11.2 is a QF test with the same quadratic form as \(T_{n}\)._

Proof.: By Theorem 9.7,

\[\sqrt{n}(\hat{\psi}-\psi_{0})\ \stackrel{{ Q_{n}}}{{=}}\ \sqrt{n}I_{\psi\cdot\lambda}^{-1}E_{n}[s_{\psi\cdot\lambda}(\theta_{0},X)]+o_{P}(1).\]

Since \(I(\theta)\) is continuous, \(I(\bar{\theta})\ \stackrel{{ Q_{n}}}{{=}}\ I(\theta_{0})+o_{P}(1)\). Hence \(W_{n}\) is of the form (11.11). 

As a consequence, the asymptotic null and local alternative distributions of \(W_{n}\) are the same as those of \(T_{n}\), as given in Theorem 11.2.

#### Rao's test

Rao's test, also known as the score test, was introduced by Rao (1948). In the case where \(\psi=\theta\), Rao's statistic is of the form

\[R_{n}=nE_{n}[s^{T}(\theta_{0},X)]I^{-1}(\theta_{0})E_{n}[s(\theta_{0},X)].\]

This, by definition, is in the form of a QF test with \(U_{n}=n^{1/2}E_{n}[s(\theta_{0},X)]\), \(\Sigma_{U}=I(\theta_{0})\), and \(\Sigma_{US}=I(\theta_{0})\). The general form of Rao's test is given by the following definition (Rao, 2001).

**Definition 11.3**: _Suppose \(\tilde{\theta}=(\psi_{0}^{T},\tilde{\lambda}^{T})^{T}\) is the constrained MLE under the null hypothesis \(H_{0}:\psi=\psi_{0}\). Then Rao's statistic is_

\[R_{n}=nE_{n}[s^{T}(\tilde{\theta},X)]I^{-1}(\tilde{\theta})E_{n}[s(\tilde{ \theta},X)].\]

An interesting feature of Rao's statistic is that it only involves the constrained maximum likelihood estimate \((\psi_{0},\tilde{\lambda})\) under the null hypothesis; the global MLE appears nowhere in this statistic. In particular, in the case where \(\psi=\theta\), no estimate is needed to perform this test. This gives a numerical advantage to Rao's test, because we only need to perform the maximization over an \(s\)-dimensional space. That Rao's test is a QF test will be proved in the next subsection along with the Neyman's \(C(\alpha)\) test.

#### Neyman's \(C(\alpha)\) test

This test was introduced by Neyman (1959). One way to motivate Neyman's \(C(\alpha)\) test is through its relation with Rao's test, as explained in Kocherlakota and Kocherlakota (1991). Since \(\tilde{\lambda}\) is a solution to \(E_{n}[s_{\lambda}(\psi_{0},\lambda,X)]=0\), we have

\[E_{n}[s(\tilde{\theta},X)]=\begin{pmatrix}E_{n}[s_{\psi}(\tilde{\theta},X)]\\ 0\end{pmatrix}.\]

Hence Rao's test can be equivalently written as

\[\begin{split} R_{n}=& n(E_{n}s_{\psi}^{T}(\tilde{\theta},X),0)I^{-1}(\tilde{\theta})(E_{n}s_{\psi}^{T}(\tilde{\theta},X),0)^{T}\\ =& nE_{n}[s_{\psi}^{T}(\tilde{\theta},X)][I^{-1}(\tilde{\theta})]_{\psi\psi}E_{n}[s_{\psi}(\tilde{\theta},X)]\\ =& nE_{n}[s_{\psi\cdot\lambda}^{T}(\psi_{0},\tilde{\lambda},X)]I^{-1}_{\psi \cdot\lambda}(\psi_{0},\tilde{\lambda})E_{n}[s_{\psi\cdot\lambda}(\psi_{0}, \tilde{\lambda},X)],\end{split} \tag{11.12}\]

where, for the third equality, we used \(E_{n}[s_{\psi}(\tilde{\theta},X)]=E_{n}[s_{\psi\cdot\lambda}(\tilde{\theta},X)]\), which holds because \(E_{n}[s_{\lambda}(\tilde{\theta},X)]=0\), and \([I^{-1}(\tilde{\theta})]_{\psi\psi}=I^{-1}_{\psi\cdot\lambda}(\tilde{\theta})\), which follows from the formula of the inverse of block matrix given in Proposition 9.3. The right-hand side of (11.12) is precisely the form of Neyman's \(C(\alpha)\) test introduced by Neyman (1959) except that the latter does not require \(\tilde{\lambda}\) to be the MLE under the null hypothesis \(H_{0}:\psi=\psi_{0}\). Instead, Neyman's \(C(\alpha)\) test allows \(\tilde{\lambda}\) to be any \(\sqrt{n}\)-consistent estimate of \(\lambda_{0}\).

**Definition 11.4**: _Let \(\tilde{\lambda}\) be any \(\sqrt{n}\) consistent estimate of \(\lambda_{0}\), Neyman's \(C(\alpha)\) test is the statistic_

\[N_{n}=nE_{n}[s_{\psi\cdot\lambda}^{T}(\psi_{0},\tilde{\lambda},X)]I_{\psi\cdot \lambda}^{-1}(\psi_{0},\tilde{\lambda})E_{n}[s_{\psi\cdot\lambda}(\psi_{0}, \tilde{\lambda},X)].\]

Rao's test is a special case of Neyman's \(C(\alpha)\) test when \(\tilde{\lambda}\) is the constrained MLE under the null hypothesis. We next prove that \(N_{n}\) -- and hence also \(R_{n}\) -- is a QF test.

**Theorem 11.5**: _Suppose Assumption 10.3 holds and_

1. \(\ell(\theta,x)\) _is twice differentiable;_
2. \(f_{\theta}(X)\) _and_ \(s(\theta,X)f_{\theta}(X)\) _satisfy_ \(\text{DUI}^{+}(\theta,\mu)\)_;_
3. \(s(\theta,X)\) _is_ \(P_{\theta}\)_-square integrable and_ \(K(\theta)\) _is positive definite and continuous;_
4. _the sequence of random matrices_ \[\{E_{n}[\partial s_{\psi\cdot\lambda}(\psi_{0},\lambda,X)/\partial\lambda^{T} ]:n\in\mathbb{N}\}\] (11.13) _is stochastically equicontinuous in a neighborhood of_ \[\lambda_{0}\] _;_
5. \(\tilde{\lambda}\) _is a_ \(\sqrt{n}\)_-consistent estimate of_ \(\lambda_{0}\)_._

_Then \(N_{n}\) is a QF test with the same quadratic form as \(T_{n}\)._

The assumptions in this theorem are essentially the same as those in Theorem 11.2 except conditions 4 and 5: we only require these conditions for the \(\lambda\)-component. Nevertheless, we do need the derivatives of \(\ell\) with respect to \(\theta\) (not just with respect to \(\lambda\)) because both the efficient score and the efficient information involve derivatives with respect to \(\theta\).

_Proof of Theorem 11.5._ By Taylor's theorem,

\[\begin{split}& E_{n}[s_{\psi\cdot\lambda}(\psi_{0},\tilde{\lambda},X)] \\ &=E_{n}[s_{\psi\cdot\lambda}(\theta_{0},X)]+E_{n}\left[\frac{ \partial s_{\psi\cdot\lambda}(\psi_{0},\lambda^{\dagger},X)}{\partial\lambda^ {T}}\right](\tilde{\lambda}-\lambda_{0}),\end{split} \tag{11.14}\]

for some \(\lambda^{\dagger}\) between \(\lambda_{0}\) and \(\tilde{\lambda}\). By the equicontinuity condition 4, and Corollary 8.2,

\[E_{n}\left[\frac{\partial s_{\psi\cdot\lambda}(\psi_{0},\lambda^{\dagger},X)}{ \partial\lambda^{T}}\right]\stackrel{{ Q_{n}}}{{=}}E\left[\frac{ \partial s_{\psi\cdot\lambda}(\theta_{0},X)}{\partial\lambda^{T}}\right]+o_{P} (1).\]

However, by Theorem 9.8, the expectation on the right-hand side is \(0\), leading to

\[E_{n}\left[\frac{\partial s_{\psi\cdot\lambda}(\psi_{0},\lambda^{\dagger},X)}{ \partial\lambda^{T}}\right]\stackrel{{ Q_{n}}}{{=}}o_{P}(1).\]

This, together with (11.14) and condition 5, implies\[\begin{array}{rl}E_{n}s_{\psi\cdot\lambda}(\psi_{0},\tilde{\lambda},X)&\stackrel{{ Q_{n}}}{{=}}E_{n}s_{\psi\cdot\lambda}(\theta_{0},X)+o_{P}(1)\,O_{P}(n^{-1/2})\\ &\stackrel{{ Q_{n}}}{{=}}E_{n}s_{\psi\cdot\lambda}(\theta_{0},X)+o_{P}(n^{-1/2}).\end{array} \tag{11.15}\]

Meanwhile, by the continuity and nonsingularity of \(I(\theta)\), we have

\[I_{\psi\cdot\lambda}^{-1}(\tilde{\theta})\,\stackrel{{ Q_{n}}}{{=}}\,I_{\psi\cdot\lambda}^{-1}(\theta_{0})+o_{P}(1). \tag{11.16}\]

Now substitute (11.15) and (11.16) into the right-hand side of (11.12) to complete the proof. \(\Box\)

### Asymptotically efficient test

The local asymptotic distribution we have developed for the QF tests allows us to compare the local asymptotic power among these tests. Since, as we will show below, the local asymptotic power of a QF test is an increasing function of the noncentrality parameter of its asymptotic noncentral \(\chi^{2}\) distribution, a QF test with the greatest noncentrality parameter achieves maximum local asymptotic power. We first define the class of regular QF tests, which is the platform for developing the asymptotically efficient QF tests.

**Definition 11.5**: _A test \(T_{n}\) for \(H_{0}:\psi=\psi_{0}\) is regular if \(T_{n}\,\stackrel{{\cal D}}{{P_{n}(\delta)}}F(\delta)\), where \(F(\delta)\) depends on, and only on, \(\delta_{\psi}\) in the sense that_

_1. if \(\delta_{\psi}\neq 0\), then \(F(\delta)\neq F(0)\);_

_2. if \(\delta_{\psi}^{(1)}=\delta_{\psi}^{(2)}\), then \(F(\delta^{(1)})=F(\delta^{(2)})\)._

In the case where \(\psi=\theta\), the above definition requires that the limiting distribution \(F(\delta)\) genuinely depends on \(\delta\); that is, \(F(\delta)\neq F(0)\) whenever \(\delta\neq 0\). This seems to contradict with the definition of a regular estimate, which requires that the limiting distribution of \(\sqrt{n}(\hat{\psi}-\psi_{0}-n^{-1/2}\delta_{\psi})\) under \(P_{n}(\delta)\) to be the same for all \(\delta\). This apparent inconsistency of terminologies comes from the fact that \(T_{n}\) is usually of the form

\[T_{n}\,\stackrel{{ Q_{n}}}{{=}}\,n(\hat{\psi}-\psi_{0})^{T}I_{ \psi\cdot\lambda}(\theta_{0})(\hat{\psi}-\psi_{0})+o_{P}(1),\]

where \(\hat{\psi}\) is a regular estimate of \(\psi_{0}\), and asymptotic distribution of \(\sqrt{n}(\hat{\psi}-\psi_{0})\) under \(P_{n}(\delta)\), unlike that of \(\sqrt{n}(\hat{\psi}-\psi_{0}-n^{-1/2}\delta_{\psi})\) under \(P_{n}(\delta)\), should indeed depend on \(\delta_{\psi}\). The next theorem gives a sufficient and necessary condition for a QF test to be regular. In the following, we use \(S\) to denote the limit of \(S_{n}\); that is, \(S_{n}\,\stackrel{{\cal D}}{{\longrightarrow}}\,S\), where \(S\sim N(0,I)\), \(I\) being the Fisher information. This is guaranteed by the ALAN assumption. We use \(S_{\psi}\) to denote the first \(r\) components of \(S\), and \(S_{\lambda}\) the last \(s\) components of \(S\).

[MISSING_PAGE_FAIL:346]

Proof.: Because \(T_{n}\) is a regular QF test, it can be written as \(T_{n}\stackrel{{ Q_{n}}}{{=}}V_{n}^{T}\Sigma_{V}^{-1}V_{n}+o_{P}(1)\) for some \(V_{n}\in\mathbb{R}^{r}\) such that \((V_{n},S_{n},L_{n})\) satisfies ALAN with \(\Sigma_{VS_{\psi}}\) being a nonsingular matrix and \(\Sigma_{VS_{\lambda}}=0\). Let \(U_{n}=\Sigma_{VS_{\psi}}^{-1}V_{n}\). Then \((U_{n},S_{n},L_{n})\) satisfies ALAN with \(\Sigma_{US}=(I_{r},0)\), and

\[T_{n}\stackrel{{ Q_{n}}}{{=}}U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{p}( 1).\]

The limiting random vector \(U\) of \(U_{n}\) can be written as \(I_{\psi\cdot\lambda}^{-1}S_{\psi\cdot\lambda}+R\) where \(R=U-I_{\psi\cdot\lambda}^{-1}S_{\psi\cdot\lambda}\) and \(S_{\psi\cdot\lambda}=S_{\psi}-I_{\psi\lambda}I_{\lambda\lambda}^{-1}S_{\lambda}\). To show that \(S\) and \(R\) are independent, note that

\[\mathrm{cov}(R,S)= \,\mathrm{cov}(U,S)-\mathrm{cov}(I_{\psi\cdot\lambda}^{-1}S_{\psi \cdot\lambda},S)\] \[= \,\Sigma_{US}-I_{\psi\cdot\lambda}^{-1}\mathrm{cov}(S_{\psi\cdot \lambda},S).\]

By construction, \(\Sigma_{US}=(I_{r},0)\). Also,

\[\mathrm{cov}(S_{\psi\cdot\lambda},S)=(\mathrm{cov}(S_{\psi\cdot\lambda},S_{ \psi}),\mathrm{cov}(S_{\psi\cdot\lambda},S_{\lambda}))=(I_{\psi\cdot\lambda},0).\]

Hence \(\mathrm{cov}(R,S)=0\). Because \(R\) and \(S\) are jointly Normal, we have \(R\leavevmode\hbox{\small 1\kern-3.8pt\normalsize 1}\kern-3.8pt\normalsize 1}S\). The reverse implication is obvious. 

Since \(U=I_{\psi\cdot\lambda}^{-1}S_{\psi\cdot\lambda}+R\) with \(R\leavevmode\hbox{\small 1\kern-3.8pt\normalsize 1}\kern-3.8pt\normalsize 1}S\), the variance of \(U\) is always greater than or equal to \(I_{\psi\cdot\lambda}^{-1}\) in terms of Louwner's ordering. Since the noncentrality parameter for the distribution of \(U^{T}\Sigma_{U}^{-1}U\) is

\[\delta^{T}\begin{pmatrix}I_{r}\\ 0\end{pmatrix}\Sigma_{U}^{-1}\big{(}I_{r}\leavevmode\nobreak\ 0\big{)} \delta=\delta_{\psi}^{T}\Sigma_{U}^{-1}\delta_{\psi},\]

\(\delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta_{\psi}\) is the upper bound of the noncentrality parameter of any regular QF test. We summarize this optimal result in the next corollary.

**Corollary 11.1** _Suppose \(T_{n}\) is a regular QF test. Then the following statements hold:_

1. \(T_{n}\stackrel{{\mathcal{D}}}{{\longrightarrow}}\chi_{r}^{2}( \delta_{\psi}^{T}\Sigma_{U}^{-1}\delta_{\psi})\)_, where_ \(\Sigma_{U}^{-1}\preceq I_{\psi\cdot\lambda}\)_;_
2. _in the above statement,_ \(\Sigma_{U}=I_{\psi\cdot\lambda}^{-1}\) _if and only if_ \(T_{n}\) _can be represented as_ \(T_{n}\stackrel{{ Q_{n}}}{{=}}U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1)\)_, where_ \(U_{n}\stackrel{{ Q_{n}}}{{=}}I_{\psi\cdot\lambda}^{-1}S_{n,\psi \cdot\lambda}+o_{P}(1)\)_._

Proof.: 1. By Theorem 11.7, \(T_{n}\) can be written as

\[T_{n}\stackrel{{ Q_{n}}}{{=}}U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}( 1),\]

where \(U_{n}=I_{\psi\cdot\lambda}^{-1}S_{n,\psi\cdot\lambda}+R_{n}\) with \((R_{n},S_{n},L_{n})\) satisfying ALAN and \(\Sigma_{RS}=0\). Hence \(\Sigma_{U}=I_{\psi\cdot\lambda}^{-1}+\Sigma_{R}\), implying \(\Sigma_{U}^{-1}\preceq I_{\psi\cdot\lambda}\).

2. \(\Sigma_{U}^{-1}=I_{\psi\cdot\lambda}\) if and only if \(\Sigma_{U}=I_{\psi\cdot\lambda}^{-1}\) which, by part 1, holds if and only if \(\Sigma_{R}=0\), which in turn holds if and only if \(R_{n}\ \stackrel{{ Q_{n}}}{{=}}\ o_{P}(1)\). \(\Box\)

We now define the the asymptotically efficient QF test. A natural definition would be in terms of the local asymptotic power, which is indeed the approach we adopt. Let \(\mathcal{Q}\) denote the collection of all sequences of QF tests. So a member of \(\mathcal{Q}\) is \(\{T_{n}:n\in\mathbb{N}\}\) where \(T_{n}\) is a QF test.

**Definition 11.6**: _An asymptotically efficient QF test is a member \(\{T_{n}^{*}:n\in\mathbb{N}\}\) of \(\mathcal{Q}\) such that for any member \(\{T_{n}:n\in\mathbb{N}\}\) of \(\mathcal{Q}\), \(\delta\in\mathbb{R}^{p}\), and any \(c>0\),_

\[P(T^{*}(\delta)\geq c)\geq P(T(\delta)\geq c),\]

_where \(T^{*}(\delta)\) is the limit of \(T_{n}^{*}\) under \(P_{n}(\delta)\) and \(T(\delta)\) is the limit of \(T_{n}\) under \(P_{n}(\delta)\); that is,_

\[T_{n}\stackrel{{\mathcal{D}}}{{\longrightarrow}}T(\delta)\ and\ \ T_{n}^{*}\stackrel{{\mathcal{D}}}{{\longrightarrow}}T^{*}(\delta).\]

This definition is an asymptotic analogue of the UMPU test described in Chapters 3 and 4. Mathew and Nordstrom (1997) showed that a noncentral chi-squared distribution with a larger noncentrality parameter is always stochastically larger than a noncentral chi-squared distribution with the same degrees of freedom and a smaller noncentrality parameter. A rigorous statement of this is given below without proof.

**Proposition 11.1**: _If \(K_{1}\sim\chi_{r}^{2}(d_{1})\) and \(K_{2}\sim\chi_{r}^{2}(d_{2})\) and \(d_{2}\geq d_{1}\), then, for any \(c>0\),_

\[P(K_{2}\geq c)\geq P(K_{1}\geq c)\]

Using this proposition we immediately arrive at the following equivalent definition of an asymptotically efficient QF test.

**Definition 11.7**: _An asymptotically efficient QF test is a member \(\{T_{n}^{*}:n\in\mathbb{N}\}\) of \(\mathcal{Q}\) such that \(T_{n}^{*}\stackrel{{\mathcal{D}}}{{\longrightarrow}}\chi_{r}^{2}( \delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta_{\psi})\) for any \(\delta\in\mathbb{R}^{p}\)._

Since all the four test statistics \(T_{n}\), \(W_{n}\), \(R_{n}\), and \(N_{n}\) developed in the previous sections are QF tests with the same quadratic form

\[S_{n,\psi\cdot\lambda}^{T}I_{\psi\cdot\lambda}^{-1}S_{n,\psi\cdot\lambda}+o_{P }(1)\]

under \(Q_{n}\), where \(S_{n,\psi\cdot\lambda}=n^{1/2}E_{n}[s_{\psi\cdot\lambda}(\theta_{0},X)]\), their noncentrality parameters all reach the upper bound \(\delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta_{\psi}\). Hence they are all asymptotically efficient.

In the rest of this section we develop a relation between a regular estimate that satisfies ALAN and a regular QF test.

**Proposition 11.2**: _Suppose that \(\hat{\psi}\) is a regular estimate of \(\psi_{0}\) and \((U_{n},S_{n},L_{n})\) satisfies ALAN with \(U_{n}=\sqrt{n}(\hat{\psi}-\psi_{0})\). Then \(T_{n}\) of the form_

\[T_{n}\ \stackrel{{ Q_{n}}}{{=}}\ U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P} (1) \tag{11.17}\]

_is a regular QF test. Furthermore, \(T_{n}\) is asymptotically efficient if and only if \(\hat{\psi}\) is asymptotically efficient._

Because \(\hat{\psi}\) is regular and \((U_{n},S_{n},L_{n})\) satisfies ALAN, we have, by Proposition 10.4, \(\Sigma_{US}=\dot{h}^{T}=(I_{r},0)\), where \(h(\theta)=\psi=(I_{r},0)\theta\). Hence by Theorem 11.6, \(T_{n}\) is regular.

By Corollary 11.1, \(T_{n}\ \stackrel{{\mathcal{D}}}{{\underset{P_{n}(\delta)}{\longrightarrow}}}\chi_{r}^{2}(\delta_{\psi}^{T}\Sigma_{U}^{-1}\delta_{\psi})\). Therefore, \(T_{n}\) is asymptotically efficient QF test if and only if \(\Sigma_{U}^{-1}=I_{\psi\cdot\lambda}\), which holds, in turn, if and only if \(\hat{\psi}\) is asymptotically efficient estimate. \(\Box\)

The converse statement of Proposition 11.2 is not true: Problem 11.13 shows that it is possible to construct a regular QF test based on an estimate that is not regular.

### 11.5 Pitman efficiency

This section is concerned with a numerical measurement of asymptotic relative efficiency between two test statistics introduced by Pitman (1948) in an unpublished lecture notes. See, for example, Neother (1950), Neother (1955), and van Eeden (1963). The general definition given here is from Hall and Mathiason (1990). Recall that each member of \(\mathcal{Q}\) converges in distribution to a noncentral \(\chi^{2}\) distribution under \(P_{n}(\delta)\), which characterizes the local asymptotic power in the direction of \(\delta\): the larger the noncentrality parameter the greater the asymptotic power in the direction of \(\delta\). The relative Pitman efficiency of one member of \(\mathcal{Q}\) with respect to another is defined as the ratio of the respective noncentrality parameters. Let \(\{T_{n}\}\) and \(\{T_{n}^{*}\}\) be two members of \(\mathcal{Q}\) with

\[T_{n}\ \stackrel{{ Q_{n}}}{{=}}\ U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P} (1),\quad T_{n}^{*}\ \stackrel{{ Q_{n}}}{{=}}\ U_{n}^{*\,T}\Sigma_{U^{*}}^{-1}U_{n}^{*}+o_{P} (1),\]

where \((U_{n},S_{n})\ \stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}}(U,S)\), \((U_{n}^{*},S_{n})\ \stackrel{{\mathcal{D}}}{{\underset{Q_{n}}{\longrightarrow}}}(U^{*},S)\). Let

\[\Sigma_{US}=\mathrm{cov}(U,S),\quad\Sigma_{U^{*}S}=\mathrm{cov}(U^{*},S).\]

**Definition 11.8**: _The relative Pitman efficiency of \(\{T_{n}\}\) with respect to \(\{T_{n}^{*}\}\) in the direction of \(\delta\) is_\[\mathcal{E}_{T,T^{*}}(\delta)=\frac{\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US }\delta}{\delta^{T}\Sigma_{SU^{*}}\Sigma_{U^{*}}^{-1}\Sigma_{U^{*}S}\delta}.\]

The Pitman efficiency of \(\{T_{n}\}\) in the direction of \(\delta\) is its relative Pitman efficiency with respect to any Asymptotically Efficient QF test; that is

\[\mathcal{E}_{T}(\delta)=\frac{\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US} \delta}{\delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta_{\psi}}.\]

Note that if \(T_{n}\ \stackrel{{ Q_{n}}}{{=}}\ U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1)\) is a regular QF test for the parameter of interest \(\psi\), then \(\Sigma_{US}=(\Sigma_{US_{\psi}},0)\). Thus \(\Sigma_{US}\delta=\Sigma_{US_{\psi}}\delta_{\psi}\), and the Pitman efficiency for any regular QF test is of the form

\[\mathcal{E}_{T}(\delta)=\frac{\delta_{\psi}^{T}\Sigma_{S_{\psi}U}\Sigma_{U}^{- 1}\Sigma_{US_{\psi}}\delta_{\psi}}{\delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta _{\psi}}.\]

Pitman efficiency can be equivalently defined using the correlation matrix between two random vectors. If \(U\) and \(V\) are random vectors with finite and positive definite variance matrices \(\Sigma_{U}=\mbox{var}(U)\) and \(\Sigma_{V}=\mbox{var}(V)\), then their correlation matrix is defined to be

\[R_{UV}=\Sigma_{U}^{-1/2}\Sigma_{UV}\Sigma_{V}^{-1/2},\]

where \(\Sigma_{UV}=\mbox{cov}(U,V)\). Let

\[\rho^{2}(U,V)=\mbox{tr}(R_{UV}R_{VU})=\mbox{tr}(R_{VU}R_{UV}).\]

Then, the Pitman efficiency of \(\{T_{n}\}\) can be rewritten as

\[\mathcal{E}_{T}(\delta)=\rho^{2}(\delta_{\psi}^{T}S_{\psi},U).\]

### Hypothesis specified by an arbitrary constraint

We now turn to the general hypothesis testing problem where the null hypothesis is specified by one or a set of equations. Suppose that \(h:\Theta\mapsto\mathbb{R}^{r}\) is a mapping from \(\Theta\subseteq\mathbb{R}^{p}\) to \(\mathbb{R}^{r}\), where \(r\leq p\). We are interested in testing

\[H_{0}:h(\theta)=0.\]

The test with an explicit parameter of interest, \(H_{0}:\psi=\psi_{0}\), can be regarded as a special case of the above test with \(h(\theta)=\psi-\psi_{0}\). All the tests we developed in the previous sections can be generalized to this setting. The likelihood ratio test and the score test take almost exactly the same form as before, with \(\tilde{\theta}\) being the MLE under the constraint \(h(\theta)=0\). However, because there is no explicit parameter of interest in this case, the forms of Wald's test and Neyman's \(C(\alpha)\) test have to be modified.

#### Asymptotic analysis of constrained MLE

Let \(\tilde{\theta}\) be the maximizer of the likelihood \(E_{n}[\ell(\theta,X)]\) under the constraint \(h(\theta)=0\). In this subsection we derive the asymptotic linear form of \(\tilde{\theta}\) under standard regularity conditions. Let \(F(\theta)\) be the Lagrangian

\[F(\theta)=E_{n}[\ell(\theta,X)]-h^{T}(\theta)\rho,\]

where \(\rho\in\mathbb{R}^{r}\) is the Lagrangian multiplier. It is well known in Calculus that, if \(\ell\) and \(h\) are continuously differentiable with respect to \(\theta\), then, for some \(\tilde{\rho}\in\mathbb{R}^{r}\), \((\tilde{\theta},\tilde{\rho})\) satisfies the system of equations

\[\begin{cases}E_{n}[s(\theta,X)]-H(\theta)\rho=0\\ h(\theta)=0.\end{cases} \tag{11.18}\]

where \(H(\theta)\) is the \(p\times r\) matrix \(\partial h^{T}(\theta)/\partial\theta\).

In the following development, we will frequently encounter sequences of random matrices, say \(A_{n}\), which converges in probability to an invertible matrix \(A\), but which themselves may not be invertible. The next lemma shows that, in this case, the Moore-Penrose inverse of \(A_{n}\) converges in probability to the inverse of \(A\).

We need to use two facts from linear algebra. First, if a matrix \(A\in\mathbb{R}^{p\times p}\) is invertible, then there is an open ball

\[B(A,\epsilon)=\{M\in\mathbb{R}^{p\times p}:\|M-A\|<\epsilon\}\]

such that every \(M\in B(A,\epsilon)\) is invertible, where the norm is the Frobenius norm. This is because \(M\) is invertible if and only if \(\det(M)\) is nonzero, and \(\det(M)\), being a linear combination of products of entries of \(M\), is continuous in \(M\) in the Frobenius norm. This implies there is an open all \(\mathrm{B}(A,\epsilon)\) in which every \(M\) has \(\det(M)\neq 0\).

Second, on any open set \(G\) of \(\mathbb{R}^{p\times p}\) of invertible matrices, the function \(M\mapsto M^{-1}\) is continuous. This is because

\[M^{-1}=[\det(M)]^{-1}\mathrm{adj}(M),\]

where \(\mathrm{adj}(A)\) is the adjugate of \(A\) (see, for example, Horn and Johnson, 1985, page 22). Since both \([\det(M)]^{-1}\) and \(\mathrm{adj}(M)\) are continuous in \(M\) on \(G\), \(M^{-1}\) is continuous in \(M\) on \(G\).

**Lemma 11.1**: _If \(A_{n}\overset{P}{\rightarrow}A\) and \(A\) is invertible, then_

1. \(P(A_{n}^{+}=A_{n}^{-1})\to 1\)_;_
2. \(A_{n}^{+}\overset{P}{\rightarrow}A^{-1}\)_._

Proof: 1. As discussed above, since \(A\) is invertible, there is an open ball \(B(A,\epsilon)\) such that all \(M\in B(A,\epsilon)\) are invertible. Because \(A_{n}\overset{P}{\rightarrow}A\), \(P(A_{n}\in B(A,\epsilon))\to 1\). The assertion holds because \(A_{n}\in B(A,\epsilon)\) implies \(A_{n}^{+}=A_{n}^{-1}\).

_2._ Let \(g:\mathbb{R}^{p\times p}\to\mathbb{R}^{p\times p}\) be the function \(g(M)=M^{+}\). Since every \(M\) in \(B(A,\epsilon)\) is invertible, \(g(M)=M^{+}=M^{-1}\) on \(B(A,\epsilon)\). By the discussion preceding this Lemma, \(g(M)\) is continuous on \(B(A,\epsilon)\). By the Continuous Mapping Theorem,

\[A_{n}^{+}=g(A_{n})\stackrel{{ P}}{{\to}}g(A)=A^{+}=A^{-1},\]

where the last equality holds because \(A\) is invertible. \(\Box\)

The next theorem gives an asymptotic linear form of the Lagrangian multiplier \(\tilde{\rho}\) and the constrained MLE \(\tilde{\theta}\) under \(h(\theta)=0\). In the following, we abbreviate the gradient matrix \(H(\theta_{0})\) by \(H\) and the Fisher information \(I(\theta_{0})\) by \(I\). As before, the symbol \(I\) should be differentiated from \(I_{k}\), the \(k\times k\) identity matrix.

**Theorem 11.8**: _Suppose Assumption 10.3 holds and_

_1. \(\ell(\theta,x)\) is twice differentiable;_

_2. \(f_{\theta}(X)\) and \(s(\theta,X)f_{\theta}(X)\) satisfy \(\mbox{DUI}^{+}(\theta,\mu)\);_

_3. \(s(\theta,X)\) is \(P_{\theta}\)-square integrable, and \(I(\theta)\) is invertible;_

_4. \(h(\theta)\) is continuously differentiable, and \(H^{T}(\theta)I^{-1}(\theta)H(\theta)\) is invertible;_

_5. the sequence of random matrices \(\{J_{n}(\theta):n\in\mathbb{N}\}\) in (11.4) is stochastically equicontinuous in a neighborhood of \(\theta_{0}\)._

_If \(\tilde{\theta}\) is a consistent solution to \(E_{n}[s(\theta,X)]=0\) under the constraint \(h(\theta)=0\), then_

\[\begin{split}\tilde{\rho}=&\,(H^{T}I^{-1}H)^{-1}H^{ T}I^{-1}E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/2}),\\ \tilde{\theta}=&\,\theta_{0}+I^{-1}Q_{H}(I^{-1})E_{ n}[s(\theta_{0},X)]+o_{P}(n^{-1/2}),\end{split} \tag{11.19}\]

_where \(Q_{H}(I^{-1})=I_{p}-P_{H}(I^{-1})\), and_

\[P_{H}(I^{-1})=H(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}.\]

Recall from Example 7.8 that the matrix \(P_{H}(I^{-1})\) is simply the projection on to the subspace \(\mbox{span}(H)\) of \(\mathbb{R}^{p}\) with respect to the inner product \(\langle x,y\rangle_{I^{-1}}=x^{T}I^{-1}y\). Hence \(Q_{H}(I^{-1})\) is the projection on to \(\mbox{span}(H)^{\perp}\) with respect to the inner product \(\langle\cdot,\cdot\rangle_{I^{-1}}\).

Before proving the theorem, we note the following fact: if \(a_{n}\) is a sequence of constants that goes to \(0\) as \(n\to\infty\), and \(U_{n}\), \(V_{n}\), \(W_{n}\) are random vectors, then

\[P(U_{n}=V_{n})\to 1,\ V_{n}=W_{n}+o_{P}(a_{n})\ \Rightarrow\ U_{n}=W_{n}+o_{P}(a _{n}). \tag{11.20}\]

The proof of this is left as an exercise.

_Proof of Theorem 11.8_. To prove the first relation in (11.19), note that, by (11.18),\[H(\tilde{\theta})\tilde{\rho}=E_{n}[s(\tilde{\theta},X)].\]

By the above equality and Taylor's theorem,

\[\begin{split} H(\tilde{\theta})\tilde{\rho}=&\,E_{n}[ s(\theta_{0},X)]+J_{n}(\theta^{\dagger})(\tilde{\theta}-\theta_{0})\\ h(\tilde{\theta})=&\,h(\theta_{0})+H^{T}(\theta^ {\ddagger})(\tilde{\theta}-\theta_{0})\end{split} \tag{11.21}\]

for some \(\theta^{\dagger}\) and \(\theta^{\ddagger}\) between \(\theta_{0}\) and \(\tilde{\theta}\). Because \(h(\tilde{\theta})=0\) by definition and \(h(\theta_{0})=0\) under the null hypothesis \(H_{0}\), the second equation in (11.21) reduces to \(H^{T}(\theta^{\ddagger})(\tilde{\theta}-\theta_{0})=0\) under \(H_{0}\). Multiplying the first equation in (11.21) by \(H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})\) from the left, we have

\[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{\theta})\tilde{ \rho}=H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})E_{n}[s(\theta_{0},X) ]+R_{n}, \tag{11.22}\]

where \(R_{n}=H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})J_{n}(\theta^{\dagger })(\tilde{\theta}-\theta_{0})\).

Because \(\{J_{n}(\theta):n\in\mathbb{N}\}\) is stocahstically equicontinuous in a neighborhood of \(\theta_{0}\) and \(\theta^{\dagger}\stackrel{{ P}}{{\rightarrow}}\theta_{0}\), we have, by Corollary 8.2, \(J_{n}(\theta^{\dagger})\stackrel{{ P}}{{\rightarrow}}-I\), where \(I\) is invertible. Hence, by Lemma 11.1, part 1

\[P(J_{n}^{+}(\theta^{\dagger})J_{n}(\theta^{\dagger})=I_{p})\to 1,\]

which, in view of \(H^{T}(\theta^{\ddagger})(\tilde{\theta}-\theta_{0})=0\), implies \(P(R_{n}=0)\to 1\). Since \(R_{n}=0\) implies

\[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{\theta})\tilde{ \rho}=H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})E_{n}[s(\theta_{0},X)],\]

we have

\[\begin{split}& P\left([H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{ \dagger})H(\tilde{\theta})]^{+}H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{ \dagger})H(\tilde{\theta})\tilde{\rho}\right.\\ &\left.=[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H( \tilde{\theta})]^{+}H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})E_{n}[s (\theta_{0},X)]\right)\to 1.\end{split} \tag{11.23}\]

Because \(J_{n}(\theta^{\dagger})\stackrel{{ P}}{{\rightarrow}}-I\) and \(I\) is invertible, by Lemma 11.1, part 2, we have

\[J_{n}^{+}(\theta^{\dagger})\stackrel{{ P}}{{\rightarrow}}-I^{-1}. \tag{11.24}\]

Because \(H(\theta)\) is continuous, we have

\[H(\tilde{\theta})\stackrel{{ P}}{{\rightarrow}}H\text{ and }H( \theta^{\ddagger})\stackrel{{ P}}{{\rightarrow}}H. \tag{11.25}\]

Consequently,

\[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{\theta})\stackrel{{ P}}{{\rightarrow}}-H^{T}I^{-1}H. \tag{11.26}\]

Because \(H^{T}I^{-1}H\) is invertible, by part 1 of Lemma 11.1, we have

\[P\left([H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{\theta})]^ {+}=[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{\theta})]^{-1 }\right)\to 1. \tag{11.27}\]By (11.23) and (11.27),

\[P\left(\tilde{\rho}=[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{ \theta})]^{+}H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})E_{n}[s(\theta_ {0},X)]\right)\to 1. \tag{11.28}\]

Now by (11.26) and part 2 of Lemma 11.1,

\[[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{\theta})]^{+} \stackrel{{ P}}{{\rightarrow}}-(H^{T}I^{-1}H)^{-1}. \tag{11.29}\]

Hence by (11.24), (11.25), (11.29), and

\[E_{n}[s(\theta_{0},X)]=O_{P}(n^{-1/2}), \tag{11.30}\]

we have

\[[H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})H(\tilde{ \theta})]^{+}H^{T}(\theta^{\ddagger})J_{n}^{+}(\theta^{\dagger})E_{n}[s( \theta_{0},X)]\] \[\qquad=(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}E_{n}[s(\theta_{0},X)]+o_{P} (n^{-1/2}).\]

By (11.20), the above relation and (11.28) imply

\[\tilde{\rho}=(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/ 2}). \tag{11.31}\]

proving the first expression in (11.19).

Next, substitute (11.31) into the first equation in (11.21) to obtain

\[J_{n}(\theta^{\dagger})(\tilde{\theta}-\theta_{0})\] \[=-E_{n}[s(\theta_{0},X)]+H(\tilde{\theta})\{(H^{T}I^{-1}H)^{-1}H^{ T}I^{-1}E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/2})\}\] \[=-E_{n}[s(\theta_{0},X)]+H(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}E_{n}[s( \theta_{0},X)]+o_{P}(n^{-1/2})\] \[=-E_{n}[s(\theta_{0},X)]+P_{H}(I^{-1})E_{n}[s(\theta_{0},X)]+o_{P} (n^{-1/2}),\]

where the second equality follows from (11.25) and (11.30), and the third from the definition of \(P_{H}(I^{-1})\). Hence

\[J_{n}^{+}(\theta^{\dagger})J_{n}(\theta^{\dagger})(\tilde{\theta }-\theta_{0})= -J_{n}^{+}(\theta^{\dagger})E_{n}[s(\theta_{0},X)]\] \[+J_{n}^{+}(\theta^{\dagger})P_{H}(I^{-1})E_{n}[s(\theta_{0},X)] \tag{11.32}\] \[+J_{n}^{+}(\theta^{\dagger})o_{P}(n^{-1/2}),\]

Because \(J_{n}^{+}(\theta^{\dagger})=O_{P}(1)\), the last term on the right-hand side of (11.32) is \(o_{P}(n^{-1/2})\). By (11.24) and (11.30), the first and second terms on the right-hand side of (11.32) are \(I^{-1}E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/2})\) and\(-I^{-1}P_{H}(I^{-1})E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/2})\), respectively. Because \(P(J_{n}^{+}(\theta^{\dagger})J_{n}(\theta^{\dagger})=I_{p})\to 1\), the left-hand side of (11.32) is \(\tilde{\theta}-\theta_{0}\) with probability tending to 1. Consequently, by (11.20),

\[\tilde{\theta}-\theta_{0}=I^{-1}E_{n}[s(\theta_{0},X)]-I^{-1}P_{H}(I^{-1})E_{n }[s(\theta_{0},X)]+o_{P}(n^{-1/2}),\]

proving the second expression in (11.19).

It is informative to investigate the forms of the various quantities in (11.19) in the special case where \(h(\theta)=\psi\). In this case, \(H^{T}=(I_{r},0)\) and, as shown in Problem 11.19,

\[I^{-1}Q_{H}(I^{-1})=\begin{pmatrix}0&0\\ 0&I_{\lambda\lambda}^{-1}\end{pmatrix},\quad I^{-1}Q_{H}(I^{-1})E_{n}[s(\theta,X)]=\begin{pmatrix}0\\ E_{n}[s_{\lambda}(\theta,X)]\end{pmatrix}.\]

So the second equation in (11.19) reduces to

\[\tilde{\lambda}=\lambda_{0}+I_{\lambda\lambda}^{-1}E_{n}[s_{\lambda}(\theta_{0},X)]+o_{P}(n^{-1/2}),\]

which is implied by Theorem 9.5 when \(g(\theta,X)\) therein is taken to be the estimating equation \((\lambda,x)\mapsto s_{\lambda}(\psi_{0},\lambda,x)\). The Lagrangian multiplier \(\tilde{\rho}\) also has an interesting interpretation in the special case of \(h(\theta)=\psi\): since \(\tilde{\theta}=(0^{T},\tilde{\lambda}^{T})^{T}\) and \(h(\theta)=\psi\), we have

\[E_{n}[s(\tilde{\theta},X)]=\begin{pmatrix}E_{n}[s(\psi_{0},\tilde{\lambda},X)] \\ 0\end{pmatrix},\quad H(\tilde{\theta})=\begin{pmatrix}I_{r}\\ 0\end{pmatrix}.\]

Hence, \(\tilde{\rho}\) is simply the plugged-in score \(E_{n}[s_{\psi}(\psi_{0},\tilde{\lambda},X)]\). Finally, the quantities on the right-hand side of the first equation in (11.19) generalizes the efficient score and efficient information when \(h(\theta)=\psi\). As shown in Problem 11.19,

\[(H^{T}I^{-1}H)^{-1}=I_{\psi\psi}-I_{\psi\lambda}I_{\lambda\lambda}^{-1}I_{\psi \lambda}=I_{\psi\cdot\lambda}. \tag{11.33}\]

Thus \((H^{T}I^{-1}H)^{-1}\) is the generalization of the efficient information. Also, as shown in Problem 11.19,

\[(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}s(\theta,X)=s_{\psi\cdot\lambda}(\theta,X). \tag{11.34}\]

Thus, \((H^{T}I^{-1}H)^{-1}H^{T}I^{-1}s(\theta,X)\) is the generalization of the efficient score. This motivates the following definition.

**Definition 11.9**: _The efficient information and efficient score for the hypothesis \(H_{0}:h(\theta)=0\) is_

\[I_{(H)}(\theta)= \,[H^{T}(\theta)I^{-1}(\theta)H(\theta)]^{-1}\] \[s_{(H)}(\theta,X)= \,I_{(H)}(\theta)H^{T}(\theta)I^{-1}(\theta)s(\theta,X).\]

In terms of the efficient score, the first equation in (11.19) can be reexpressed as

\[\tilde{\rho}= E_{n}[s_{(H)}(\theta_{0},X)]+o_{P}(n^{-1/2}). \tag{11.35}\]

Thus, the Lagrangian multiplier is asymptotically equivalent to the efficient score. This point will be useful later.

#### Likelihood ratio test for general hypotheses

In this subsection we derive the asymptotic distribution of Wilks likelihood test statistic for testing the hypothesis \(H_{0}:h(\theta)=0\), which is defined as

\[T_{n}=2n[E_{n}\ell(\hat{\theta},X)-E_{n}\ell(\tilde{\theta},X)], \tag{11.36}\]

where \(\hat{\theta}\) is the global MLE and \(\tilde{\theta}\) is the MLE under the constraint \(h(\theta)=0\).

**Theorem 11.9**: _If the assumptions in Theorem 11.8 are satisfied and \(\hat{\theta}\) is a consistent solution to the likelihood equation \(E_{n}[s(\theta,X)]=0\), then \(T_{n}\) in (11.36) is a QF test of the form_

\[T_{n}\ \overset{Q_{n}}{=}\ U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1), \tag{11.37}\]

_where_

\[U_{n}=n^{1/2}E_{n}[s_{(H)}(\theta_{0},X)],\quad\Sigma_{U}=I_{(H)},\quad\Sigma_ {US}=I_{(H)}H^{T}. \tag{11.38}\]

Proof: From the proof of Theorem 11.2, we have

\[2[E_{n}\ell(\hat{\theta},X)-E_{n}\ell(\theta_{0},X)]= \,(\hat{\theta}-\theta_{0})^{T}I(\hat{\theta}-\theta_{0})+o_{P}(n^ {-1})\] \[= \,(E_{n}s)^{T}\,I^{-1}\,(E_{n}s)+o_{P}(n^{-1}),\]

where \(s\) is the abbreviation of \(s(\theta_{0},X)\). Also, similar to the argument used in that proof, by Taylor's theorem we can show that

\[2[E_{n}\ell(\tilde{\theta},X)-E_{n}\ell(\theta_{0},X)]\] \[=2\,(E_{n}s)^{T}\,(\tilde{\theta}-\theta_{0})-(\tilde{\theta}- \theta_{0})^{T}I(\tilde{\theta}-\theta_{0})+o_{P}(n^{-1}).\]

Substituting the second equation in (11.19) into the right hand side, we have

\[2[E_{n}\ell(\tilde{\theta},X)-E_{n}\ell(\theta_{0},X)]\] \[=2\,(E_{n}s)^{T}\,I^{-1}Q_{H}(I^{-1})\,(E_{n}s)-\left(E_{n}s \right)^{T}I^{-1}[Q_{H}(I^{-1})]^{2}\,(E_{n}s)+o_{P}(n^{-1})\] \[=\left(E_{n}s\right)^{T}I^{-1}Q_{H}(I^{-1})\,(E_{n}s)+o_{P}(n^{-1 }),\]

where, for the second equality, we have used the fact that, as a projection, \(Q_{H}(I^{-1})\) is idempotent. Therefore

\[2[E_{n}\ell(\hat{\theta},X)-E_{n}\ell(\tilde{\theta}_{0},X)]\] \[=\left(E_{n}s\right)^{T}I^{-1}P_{H}(I^{-1})\,(E_{n}s)+o_{P}(n^{-1 })\] \[=\left(E_{n}s\right)^{T}I^{-1}H(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}\,(E _{n}s)+o_{P}(n^{-1})\] \[=\left(E_{n}s_{(H)}\right)^{T}I_{(H)}^{-1}\left(E_{n}s_{(H)} \right)+o_{P}(n^{-1}).\]Hence \(T_{n}\stackrel{{{\cal D}_{n}}}{{=}}U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1)\) with \(U_{n}=n^{1/2}E_{n}(s_{(H)})\) and \(\Sigma_{U}=I_{(H)}.\) By the central limit theorem,

\[\left(\begin{matrix}n^{1/2}E_{n}(s_{(H)})\\ n^{1/2}E_{n}(s)\end{matrix}\right)\stackrel{{{\cal D}}}{{\longrightarrow }}N\left[\left(\begin{matrix}0\\ 0\end{matrix}\right),\ \left(\begin{matrix}E(s_{(H)}s_{(H)}^{T})&E(s_{(H)}s^{T})\\ E(ss_{(H)}^{T})&E(ss^{T})\end{matrix}\right)\right],\]

It is easy to verify that \(E(s_{(H)}s^{T})=I_{(H)}H^{T}.\) Hence \(\Sigma_{SU}=I_{(H)}H^{T}.\)\(\Box\)

This theorem implies that, for any \(\delta\in\mathbb{R}^{p},\)

\[T_{n}\stackrel{{{\cal D}}}{{\longrightarrow}}\chi_{r}^{2}( \delta^{T}HI_{(H)}H\delta).\]

When \(h(\theta)=\psi,\) the noncentrality parameter reduces to \(\delta_{\psi}^{T}I_{\psi\cdot\lambda}\delta_{\psi};\) when \(\delta=0,\)\(P_{n}(\delta)\) reduces to \(Q_{n}\) and the noncentral chi-squared distribution reduces to the central chi-squared distribution.

#### Wald's test and Rao's test for general hypotheses

Wald's and Rao's tests for the general hypothesis \(H_{0}:h(\theta)=0\) are defined as follows:

\[W_{n}= \,nh^{T}(\hat{\theta})I_{(H)}(\hat{\theta})h(\hat{\theta}) \tag{11.39}\] \[R_{n}= \,nE_{n}[s^{T}(\tilde{\theta},X)]I^{-1}(\tilde{\theta})E_{n}[s( \tilde{\theta},X)]. \tag{11.40}\]

In (11.39), \(h(\hat{\theta})\) is a generalization of \(\hat{\psi}-\psi_{0},\) and \(I_{(H)}\) is a generalization of \(I_{\psi\cdot\lambda}.\) In (11.40), \(R_{n}\) takes the same form as Definition 11.3 except that here \(\tilde{\theta}\) is the constrained maximizer of \(E_{n}\ell(\theta,X)\) subject to \(h(\theta)=0\) rather than \((\psi_{0}^{T},\tilde{\lambda}^{T})^{T}.\) Since, by (11.18), \(H(\tilde{\theta})\tilde{\rho}=E_{n}[s(\tilde{\theta},X)],\)\(R_{n}\) can be equivalently written as

\[R_{n}=\tilde{\rho}^{T}H^{T}(\tilde{\theta})I^{-1}(\tilde{\theta})H(\tilde{ \theta})\tilde{\rho}. \tag{11.41}\]

For this reason, Rao's score test is also known as the Lagrangian multiplier test in the econometrics literature (see Engle, 1984; Bera and Bilias, 2001). The next lemma gives the asymptotic linear form of \(h(\hat{\theta}).\)

**Lemma 11.2**: _Suppose Assumption 10.3 and the assumptions 1\(\sim\)5 in Theorem 11.9 are satisfied. If \(\hat{\theta}\) is a consistent solution to the likelihood equation \(E_{n}[s(\theta,X)]=0\), then_

\[h(\hat{\theta})=h(\theta_{0})+I_{(H)}^{-1}E_{n}[s_{(H)}(\theta_{0},X)]+o_{P}(n ^{-1/2}).\]

_Proof._ By Taylor's theorem,

\[h(\hat{\theta})=h(\theta_{0})+H^{T}(\theta^{\dagger})(\hat{\theta}-\theta_{0})\]for some \(\theta^{\dagger}\) between \(\theta_{0}\) and \(\hat{\theta}\). By Theorem 9.5, as applied to \(g(\theta,x)=s(\theta,x)\), we have \(\hat{\theta}-\theta_{0}=I^{-1}E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/2})\). Because \(H(\theta)\) is continuous we have \(H(\theta^{\dagger})=H+o_{P}(1)\). Hence

\[h(\hat{\theta})= \,h(\theta_{0})+[H+o_{P}(1)]^{T}\{I^{-1}E_{n}[s(\theta_{0},X)]+o_ {P}(n^{-1/2})\}\] \[= \,h(\theta_{0})+H^{T}I^{-1}E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/2})\] \[= \,h(\theta_{0})+I^{-1}_{(H)}E_{n}[s_{(H)}(\theta_{0},X)]+o_{P}(n^ {-1/2}),\]

where, for the second equality, we have used \(E_{n}[s(\theta_{0},X)]=O_{P}(n^{-1/2})\). 

We now show that \(W_{n}\) and \(R_{n}\) in (11.39) and (11.40) are QF tests with the same quadratic form \(U_{n}^{T}\Sigma_{U}^{-1}U_{n}\) as in \(T_{n}\) in (11.36).

**Theorem 11.10**: _Suppose Assumption 10.3 and the assumptions 1\(\sim\)5 in Theorem 11.9 are satisfied. Suppose, in addition, \(I(\theta)\) is continuous._

1. _If_ \(\hat{\theta}\) _is a consistent solution to_ \(E_{n}[s(\theta,X)]=0\) _then_ \(W_{n}\) _is a QF test of the form specified by (_11.37_) and (_11.38_);_
2. _If_ \(\tilde{\theta}\) _is a consistent solution to_ \(E_{n}[s(\theta,X)]=0\) _subject to the constraint_ \(h(\theta)=0\)_, then_ \(R_{n}\) _is a QF test of the form specified by (_11.37_) and (_11.38_)._

_Proof. 1._ By Lemma 11.2,

\[W_{n} \stackrel{{ Q_{n}}}{{=}} n[H^{T}I^{-1}E_{n}s+o_{P}(n^{-1/2})]^{T}I_{(H)}(\hat{\theta})[H^{T}I^{-1}E_{ n}s+o_{P}(n^{-1/2})],\]

where \(E_{n}s\) is the abbreviation of \(E_{n}[s(\theta_{0},X)]\). Because \(I(\theta)\) and \(H(\theta)\) are continuous, \(I\) is invertible, and \(H^{T}I^{-1}H\) is invertible, we have, by Lemma 11.1, \(I_{(H)}(\hat{\theta})\stackrel{{ P}}{{\to}}I_{(H)}\). Hence,

\[W_{n} \stackrel{{ Q_{n}}}{{=}} n[H^{T}I^{-1}E_{n}s+o_{P}(n^{-1/2})]^{T}[I_{(H)}+o_{P}(1)][H^{T}I^{-1}E_{ n}s+o_{P}(n^{-1/2})]\] \[\stackrel{{ Q_{n}}}{{=}} n(H^{T}I^{-1}E_{n}s)^{T}I_{(H)}(H^{T}I^{-1}E_{n}s)+o_{P}(1)\] \[= \,n(E_{n}s_{(H)})^{T}I^{-1}_{(H)}(E_{n}s_{(H)})+o_{P}(1),\]

where, for the second equality, we have used \(E_{n}s=O_{P}(n^{-1/2})\), and in the third line, \(s_{(H)}\) is the abbreviation of \(s_{(H)}(\theta_{0},X)\).

2. By (11.35), (11.41) and the first equation in (11.19),

\[R_{n} \stackrel{{ Q_{n}}}{{=}} [E_{n}s_{(H)}+o_{P}(n^{-1/2})]^{T}H^{T}(\tilde{\theta})I^{-1}( \tilde{\theta})H(\tilde{\theta})[E_{n}s_{(H)}+o_{P}(n^{-1/2})].\]

Because \(\tilde{\theta}\) is consistent, \(H(\theta)\) and \(I(\theta)\) are continuous and \(I(\theta)\) is invertible, we have

\[H^{T}(\tilde{\theta})I^{-1}(\tilde{\theta})H(\tilde{\theta}) \stackrel{{ Q_{n}}}{{\longrightarrow}}\!H^{T}I^{-1}H.\]

Hence\[R_{n} \stackrel{{ Q_{n}}}{{=}} n[E_{n}s_{(H)}+o_{P}(n^{-1/2})]^{T}[H^{T}I^{-1}H+o_{P}(1)][E_{n}s_{(H)}+o_{P}(n^{-1/2})]\] \[= \,n(E_{n}s_{(H)})^{T}I_{(H)}^{-1}(E_{n}s_{(H)})+o_{P}(1),\]

where, for the second equality, we used \(E_{n}(s_{(H)})=O_{P}(n^{-1/2})\). 

#### Neyman's \(C(\alpha)\) test for general hypotheses

Next, we extend Neyman's \(C(\alpha)\) test in Section 11.3.3 to the general hypothesis \(H_{0}:h(\theta)=0\). Replacing the efficient score function and efficient information matrix in Definition 11.4 by \(s_{(H)}\) and \(I_{(H)}\) leads to the following definition of the extended Neyman's \(C(\alpha)\) statistic.

**Definition 11.10**: _Neyman's \(C(\alpha)\) statistic for the general hypothesis \(H_{0}:h(\theta)=0\) is defined as_

\[N_{n}=nE_{n}[s_{(H)}^{T}(\tilde{\theta},X)]I_{(H)}^{-1}(\tilde{\theta})E_{n}[ s_{(H)}(\tilde{\theta},X)]. \tag{11.42}\]

_where \(\tilde{\theta}\) is any \(\sqrt{n}\)-consistent estimate of \(\theta_{0}\) that satisfies \(h(\tilde{\theta})=0\)._

We next show that the Neyman's \(C(\alpha)\) thus defined is a QF-test with the same asymptotic quadratic form as \(T_{n}\), \(R_{n}\), and \(W_{n}\).

**Theorem 11.11**: _Suppose Assumption 10.3 and the assumptions 1\(\sim\)5 in Theorem 11.9 are satisfied. Suppose, in addition, \(I(\theta)\) is continuous and \(\tilde{\theta}\) is a \(\sqrt{n}\)-consistent estimate of \(\theta_{0}\) satisfying the constraint \(h(\tilde{\theta})=0\). Then \(N_{n}\) is a QF test of the form specified by (11.37) and (11.38)._

_Proof._ Abbreviate \(I(\tilde{\theta})\) and \(H(\tilde{\theta})\) by \(\tilde{I}\) and \(\tilde{H}\). By definition,

\[E_{n}[s_{(H)}(\tilde{\theta},X)]=(\tilde{H}^{T}\tilde{I}^{-1}\tilde{H})^{-1} \tilde{H}^{T}\tilde{I}^{-1}E_{n}[s(\tilde{\theta},X)].\]

By Taylor's theorem,

\[E_{n}[s(\tilde{\theta},X)]=E_{n}[s(\theta_{0},X)]+J_{n}(\theta^{\dagger})( \tilde{\theta}-\theta_{0})\]

for some \(\theta^{\dagger}\) between \(\theta_{0}\) and \(\tilde{\theta}\). Since \(\{J_{n}(\theta):n\in\mathbb{N}\}\) is stochastic equicontinuous and \(\theta^{\dagger}\smash{\mathop{\longrightarrow}\limits^{Q_{n}}}\theta_{0}\), we have by Corollary 8.2\(J_{n}(\theta^{\dagger})\stackrel{{ Q_{n}}}{{=}}-I+o_{P}(1)\). Hence

\[E_{n}[s(\tilde{\theta},X)]\stackrel{{ Q_{n}}}{{=}}E_{n}[s(\theta _{0},X)]-I(\tilde{\theta}-\theta_{0})+o_{P}(n^{-1/2}). \tag{11.43}\]

Because both \(E_{n}[s(\theta_{0},X)]\) and \(\tilde{\theta}-\theta_{0}\) are of the order \(O_{P}(n^{-1/2})\) under \(Q_{n}\), we have \(E_{n}[s(\tilde{\theta},X)]\stackrel{{ Q_{n}}}{{=}}O_{P}(n^{-1/2})\). Furthermore, because \(H(\theta)\), \(I(\theta)\) are continuous, and \(I(\theta)\) and \(H^{T}(\theta)I^{-1}(\theta)H(\theta)\) are invertible, we have, by Lemma 11.1,\[(\tilde{H}^{T}\tilde{I}^{-1}\tilde{H})^{-1}\tilde{H}^{T}\tilde{I}^{-1}\ \stackrel{{ \mathbb{Q}_{n}}}{{=}}\ (H^{T}I^{-1}H)^{-1}H^{T}I^{-1}+o_{P}(1).\]

Therefore,

\[E_{n}[s_{(H)}(\tilde{\theta},X)]\ \stackrel{{ Q_{n}}}{{=}}\ (H^{T}I^{-1}H)^{-1}H^{T}I^{-1}E_{n}[s(\tilde{\theta},X)]+o_{P}(n^{-1/2}). \tag{11.44}\]

Now substituting (11.43) into (11.44), we have

\[\begin{split} E_{n}[s_{(H)}(\tilde{\theta},X)]\ \stackrel{{ Q_{n}}}{{=}}&(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}E_{n}[s(\theta_{0},X)]\\ &-(H^{T}I^{-1}H)^{-1}H^{T}(\tilde{\theta}-\theta_{0})+o_{P}(n^{-1/2}).\end{split} \tag{11.45}\]

By Taylor's theorem

\[\begin{split} h(\tilde{\theta})=&\,h(\theta_{0})+H^{T}( \theta^{\ddagger})(\tilde{\theta}-\theta_{0})\\ =&\,h(\theta_{0})+H^{T}(\tilde{\theta}-\theta_{0})+o_{P}(n^{-1/2}),\end{split}\]

where the second equality follows from the continuity of \(H(\theta)\) and the \(\sqrt{n}\)-consistency of \(\tilde{\theta}\). Because both \(\tilde{\theta}\) and \(\theta_{0}\) satisfy the constraint \(h(\theta)=0\), we have \(H^{T}(\tilde{\theta}-\theta_{0})=o_{P}(n^{-1/2})\). Hence the second term on the right hand side of (11.45) is of the order \(o_{P}(n^{-1/2})\), resulting in

\[\begin{split} E_{n}[s_{(H)}(\tilde{\theta},X)]=&\,(H^{T}I^{-1}H)^{-1}H^{T}I^{-1}E_{n}[s(\theta_{0},X)]+o_{P}(n^{-1/2})\\ =&\,E_{n}[s_{(H)}(\theta_{0},X)]+o_{P}(n^{-1/2}). \end{split}\]

Substituting the above relation as well as \(I_{(H)}^{-1}(\tilde{\theta})\ \stackrel{{ Q_{n}}}{{=}}\ I_{(H)}^{-1}(\theta_{0})+o_{P}(1)\) into the right hand side of (11.42), we have

\[N_{n}=nE_{n}[s_{(H)}(\theta_{0},X)]^{T}I_{(H)}^{-1}(\theta_{0})E_{n}[s_{(H)}( \theta_{0},X)]+o_{P}(1),\]

as desired. \(\Box\)

#### Asymptotic efficiency

In this subsection we extend the concept of an asymptotically efficient test to the general hypothesis \(H_{0}:h(\theta)=0\). We first extend the concept of a regular test for such a hypothesis. Let \(\delta_{H}\) be the projection of \(\delta\) on to \(\operatorname{span}(H)\) with respect to the \(I^{-1}\)-inner product, and \(\delta_{H^{\perp}}\) the projection on to the orthogonal complement of \(\operatorname{span}(H)\). That is,

\[\delta_{H}=P_{H}(I^{-1})\delta,\quad\delta_{H^{\perp}}=Q_{H}(I^{-1})\delta.\]

These vectors play the roles of \(\delta_{\psi}\) and \(\delta_{\lambda}\) when \(\psi\) and \(\lambda\) are explicitly defined.

**Definition 11.11**: _A statistic \(T_{n}\) for testing \(H_{0}:h(\theta)=0\) is regular if \(T_{n}\stackrel{{\mathcal{D}}}{{\longrightarrow}}F(\delta)\), where \(F(\delta)\) depends on, and only on, \(\delta_{H}\) in the sense that_1. _if_ \(\delta_{H}\neq 0\)_, then_ \(F(\delta)\neq F(0)\)_;_
2. _if_ \(\delta_{H}^{(1)}=\delta_{H}^{(2)}\)_, then_ \(F(\delta^{(1)})=F(\delta^{(2)})\)_._

The next theorem, which is a generalization of Theorem 11.6, gives a necessary and sufficient condition for a QF test to be regular. Let \(S_{n},U_{n},\Sigma_{SU}\) be as defined in Assumption 10.4.

**Theorem 11.12**: _A QF test \(T_{n}\) for the hypothesis \(H_{0}:h(\theta)=0\) is regular if and only if \(\operatorname{span}(\Sigma_{SU})=\operatorname{span}(H)\)._

Proof: By Theorem 11.1, \(T_{n}\xrightarrow[P_{n}(\delta)]{\mathcal{D}}\chi_{r}^{2}[f(\delta)]\), where \(f(\delta)=\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US}\delta\). Decomposing \(\Sigma_{SU}\) as \(P_{H}(I^{-1})\Sigma_{SU}+Q_{H}(I^{-1})\Sigma_{SU}\), we have

\[\begin{split} f(\delta)=&\,\delta_{H}^{T}\Sigma_{SU }\Sigma_{U}^{-1}\Sigma_{US}\delta_{H}+2\delta_{H}^{T}\Sigma_{SU}\Sigma_{U}^{- 1}\Sigma_{US}\delta_{H^{\perp}}\\ &+\delta_{H^{\perp}}^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US} \delta_{H^{\perp}}.\end{split} \tag{11.46}\]

If \(\operatorname{span}(\Sigma_{SU})=\operatorname{span}(H)\), then there is a nonsingular \(A\in\mathbb{R}^{r\times r}\) such that \(\Sigma_{SU}=HA\) and

\[f(\delta)= \,\delta_{H}^{T}HA\Sigma_{U}^{-1}A^{T}H^{T}\delta_{H}, \tag{11.47}\]

which depends on and only on \(\delta_{H}\). Conversely, suppose \(f(\delta)\) satisfies 1 and 2 of Definition 11.11. Then, for any \(\delta\in\operatorname{span}(H)\), \(\delta\neq 0\), we have \(\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US}\delta\neq 0\), implying

\[\operatorname{span}(H)\subseteq\operatorname{span}(\Sigma_{SU}\Sigma_{U}^{-1} \Sigma_{US})=\operatorname{span}(\Sigma_{SU}).\]

For any \(\delta\perp\operatorname{span}(H)\), \(\delta\neq 0\), we have \(\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US}\delta=f(\delta)=f(0)=0\), implying

\[\operatorname{span}(H)^{\perp}\subseteq\operatorname{span}(\Sigma_{SU}\Sigma_{ U}^{-1}\Sigma_{US})^{\perp}=\operatorname{span}(\Sigma_{SU})^{\perp}.\]

Hence \(\operatorname{span}(\Sigma_{SU})=\operatorname{span}(H)\). \(\Box\)

The next theorem is a generalization of Theorem 11.7. It says that a QF test for \(H_{0}:h(\theta)=0\) is regular if and only if its \(U_{n}\) can be so chosen as to obey the convolution theorem for a regular estimate. We will use \(S_{n,(H)}\) and \(S_{(H)}\) to represent \(I_{(H)}H^{T}I^{-1}S_{n}\) and \(I_{(H)}H^{T}I^{-1}S\), respectively, where \(S\) is the limit of \(S_{n}\) and \(S\sim N(0,I)\). Note also that \(S_{n,(H)}\) reduces to the rescaled efficient score \(\sqrt{n}E_{n}[s_{(H)}(\theta_{0},X)]\) under the i.i.d. assumption.

**Theorem 11.13**: _A QF test \(T_{n}\) for \(H_{0}:h(\theta)=0\) is regular if and only if it can be written as the form_

\[T_{n}\ \stackrel{{ Q_{n}}}{{=}}\ U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P }(1),\]

_where \(U_{n}=I_{(H)}^{-1}S_{n,(H)}+R_{n}\), with \((R_{n},S_{n},L_{n})\) satisfying ALAN and \(R\operatorname{1\!\!\!1}\,S\)._Proof.: Because \(T_{n}\) is a QF test, it can be written as \(T_{n}\stackrel{{ Q_{n}}}{{=}}V_{n}^{T}\Sigma_{V}^{-1}V_{n}+o_{P}(1)\) for some \(V_{n}\in\mathbb{R}^{r}\) such that \((V_{n},S_{n},L_{n})\) satisfies ALAN. Furthermore, because \(T_{n}\) is regular, by Theorem 11.12, \(\Sigma_{VS}\) satisfies \(\mathrm{span}(\Sigma_{VS})=\mathrm{span}(H)\). Hence \(\Sigma_{VS}=HA\) for some nonsingular \(A\in\mathbb{R}^{r\times r}\). Let \(U_{n}=A^{-T}V_{n}\). Then \((U_{n},S_{n},L_{n})\) satisfies ALAN with \(\Sigma_{SU}=\Sigma_{SV}A^{-1}=H\), and

\[T_{n}\stackrel{{ Q_{n}}}{{=}}U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1).\]

The limiting random vector \(U\) can be written as \(I_{(H)}^{-1}S_{(H)}+R\) where \(R=U-I_{(H)}^{-1}S_{(H)}\) and \(S_{(H)}=I_{(H)}H^{T}I^{-1}S\). Note that

\[\mathrm{cov}(R,S)= \,\mathrm{cov}(U,S)-\mathrm{cov}(I_{(H)}^{-1}S_{(H)},S)\] \[= \,\Sigma_{US}-I_{(H)}^{-1}\mathrm{cov}(S_{(H)},S)\] \[= \,H^{T}-I_{(H)}^{-1}I_{(H)}H^{T}=0,\]

where, for the third equality, we have used \(\Sigma_{SS}=I\). which implies \(R\stackrel{{ 1}}{{\mathchoice{\hbox{\hbox to 0.0pt{\kern 2.999954pt \vrule height 6.299904pt width 1px\hss}\hbox{$ \rm l$}}}{\hbox{\hbox to 0.0pt{\kern 2.999968pt\vrule height 6.299904pt width 1px \hss}\hbox{$\rm l$}}}{\hbox{\hbox to 0.0pt{\kern 1.499977pt\vrule h eight 6.299904pt width 1px\hss}\hbox{$\rm l$}}}{\hbox{ \hbox to 0.0pt{\kern 1.049977pt\vrule height 6.299904pt width 1px\hss}\hbox{$ \rm l$}}}}S\) because \(R\) and \(S\) are jointly Normal by the ALAN assumption. 

By this theorem and its proof, if \(T_{n}\) is any regular QF test for \(H_{0}:h(\theta)=0\), then it can be represented by \(U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1)\), where

\[\Sigma_{U}=I_{(H)}^{-1}I_{(H)}I_{(H)}^{-1}+\Sigma_{R}\succeq I_{(H)}^{-1},\]

and \(\Sigma_{SU}=H\). Hence the noncentrality parameter of the asymptotic noncentral chi-squared distribution of \(T_{n}\) under \(P_{n}(\delta)\) is

\[\delta^{T}\Sigma_{SU}\Sigma_{U}^{-1}\Sigma_{US}\delta=\delta^{T}H\Sigma_{U}^{- 1}H^{T}\delta.\]

This implies that the upper bound of the noncentrality parameter is \(\delta^{T}HI_{(H)}\)\(H^{T}\delta\). Furthermore, for any regular QF test that reaches this upper bound, its \(U_{n}\) is asymptotically equivalent to \(I_{(H)}^{-1}S_{n,(H)}\). We summarize this result in the next corollary.

**Corollary 11.2**: _Suppose \(T_{n}\) is any regular QF test. Then the following statements hold._

1. \(T_{n}\stackrel{{\mathcal{D}}}{{\underset{P_{n}(\delta)}{\mathcal{ P}}}}\chi_{r}^{2}(\delta^{T}H\Sigma_{U}^{-1}H^{T}\delta)\)_, where_ \(\Sigma_{U}^{-1}\preceq I_{(H)}\)_._
2. \(T_{n}\stackrel{{\mathcal{D}}}{{\underset{P_{n}(\delta)}{\mathcal{ P}}}}\chi_{r}^{2}(\delta^{T}HI_{(H)}H^{T}\delta)\) _if and only if_ \(T_{n}\) _can be written in the form_ \(T_{n}\stackrel{{ Q_{n}}}{{=}}U_{n}^{T}\Sigma_{U}^{-1}U_{n}+o_{P}(1)\)_, where_ \(U_{n}\stackrel{{ Q_{n}}}{{=}}I_{(H)}^{-1}S_{n,(H)}+o_{P}(1)\)_._

Naturally, we define those QF tests for \(H_{0}:h(\theta)=0\) with largest noncentrality parameter \(\delta^{T}HI_{(H)}H^{T}\delta\) in their asymptotic distribution under \(P_{n}(\delta)\) as the asymptotically efficient tests. All the four tests \(T_{n}\), \(W_{n}\), \(R_{n}\), and \(N_{n}\) are Asymptotically Efficient tests.

## 11.7 QF tests for estimating equations

In this section we develop QF-tests for estimating equations. To our knowledge, the results presented here have not all been recorded in the statistics literature. The most relevant publications are Rotnisky and Jewel (1990), where a Wald-type statistic was proposed for testing hypothesis for parameters in a Generalized Estimating Equation, and Boos (1992), which developed score tests based estimating equations rather than the likelihood score functions. Of course, the general strategy in Hall and Mathiason (1990) also plays a critical role in the following development, as it has throughout this chapter. We will only consider the case where the parameter of interest and the nuisance parameter are explicitly defined, omitting the general hypothesis \(H_{0}:h(\theta)=0\), which can be developed by making analogies to the steps in Section 11.6. Problems 11.24 and 11.25 are devoted to this further generalization.

### Wald's, Rao's, and Neyman's tests for estimating equations

Let \(\theta\), \(\psi\), and \(\lambda\) be as defined in Section 11.1. Let \(X_{1},\ldots,X_{n}\) be an i.i.d. sample from an unspecified distribution \(P_{\theta}\), where \(\theta\in\Theta\subseteq\mathbb{R}^{p}\). Suppose we are interested in testing the hypothesis

\[H_{0}:\psi=\psi_{0}.\]

Let \(g:\Theta\times\Omega_{X}\rightarrow\mathbb{R}^{p}\) be an unbiased and \(P_{\theta}\)-square-integrable estimating equation. We estimate the true parameter \(\theta_{0}=(\psi_{0}^{T},\lambda_{0}^{T})^{T}\) by solving the equation

\[E_{n}[g(\theta,X)]=0. \tag{11.48}\]

Let \(g_{\psi}\) be the first \(r\) components of \(g\), and \(g_{\lambda}\) the last \(s\) components of \(g\), and, as in Chapter 9, let

\[J_{g}(\theta)=E_{\theta}[\partial g(\theta,X)/\partial\theta^{T}],\quad K_{g}( \theta)=E_{\theta}[g(\theta,X)g^{T}(\theta,X)].\]

The information contained in \(g\) is \(I_{g}(\theta)=J_{g}^{T}(\theta)K_{g}^{-1}(\theta)J_{g}(\theta)\). Let \(J_{g}(\theta)\) and \(K_{g}(\theta)\) be partitioned, in obvious ways, into the following block matrices

\[J_{g}(\theta)=\begin{pmatrix}J_{g,\psi\psi}(\theta)&J_{g,\psi\lambda}(\theta) \\ J_{g,\lambda\psi}(\theta)&J_{g,\lambda\lambda}(\theta)\end{pmatrix},\quad K_{g} (\theta)=\begin{pmatrix}K_{g,\psi\psi}(\theta)&K_{g,\psi\lambda}(\theta)\\ K_{g,\lambda\psi}(\theta)&K_{g,\lambda\lambda}(\theta)\end{pmatrix}\!.\]

Here, we do not assume \(J_{g}(\theta)\) to be symmetric: for example \(J_{g,\psi\psi}^{T}(\theta)\) need not be the same as \(J_{g,\psi\psi}(\theta)\) and \(J_{g,\psi\lambda}^{T}(\theta)\) need not be the same as \(J_{g,\lambda\psi}(\theta)\). Mimicking the efficient score, let

\[g_{\psi\cdot\lambda}(\theta,X)=g_{\psi}(\theta,X)-J_{g,\psi\lambda}(\theta)J_{ g,\lambda\lambda}^{-1}(\theta)g_{\lambda}(\theta,X).\]Notice that, in the above definition, the coefficient matrix for \(g_{\lambda}(\theta,X)\) is not \(I_{g,\psi\lambda}(\theta)I_{g,\lambda\lambda}(\theta)\), as one might anticipate. We abbreviate \(J_{g}(\theta_{0})\), \(K_{g}(\theta_{0})\) and \(I_{g}(\theta_{0})\) by \(J_{g}\), \(K_{g}\), and \(I_{g}\), respectively, and abbreviate \(g_{\psi}(\theta_{0},X)\), \(g_{\lambda}(\theta_{0},X)\), and \(g_{\psi\cdot\lambda}(\theta_{0},X)\) by \(g_{\psi}\), \(g_{\lambda}\), and \(g_{\psi\cdot\lambda}\). Let \((J_{g}^{-1})_{\psi\theta}\) denote the \(r\times p\) matrix consisting of the first \(r\) rows of \(J_{g}^{-1}\), \((J_{g}^{-1})_{\psi\psi}\) the upper-left \(r\times r\) block of the matrix \(J_{g}^{-1}\), and \((J_{g}^{-1})_{\psi\lambda}\) the upper-right \(r\times s\) block of \(J_{g}^{-1}\). The following lemma gives some properties about \(g_{\psi\cdot\lambda}(\theta_{0},X)\) that will be useful further on.

**Lemma 11.3**: _Suppose \(g(\theta,X)\) is an unbiased and \(P_{\theta}\)-square-integrable estimating equation. If the derivatives, inverses, and moments involved are defined, then_

_1. \([J_{g}^{-1}(\theta)]_{\psi\psi}\,g_{\psi\cdot\lambda}(\theta,X)=[J_{g}^{-1}( \theta)]_{\psi\theta}\,g(\theta,X)\);_

_2. \(\operatorname{var}[(J_{g}^{-1})_{\psi\psi}\,g_{\psi\cdot\lambda}]=(I_{g}^{-1}) _{\psi\psi}\);_

_3. \(E[\partial g_{\psi\cdot\lambda}(\theta_{0},X)/\partial\psi^{T}]=(J_{g}^{-1})_{ \psi\psi}^{-1}\);_

_4. \(E[\partial g_{\psi\cdot\lambda}(\theta_{0},X)/\partial\lambda^{T}]=0\),_

_where \(I_{g}=I_{g}(\theta_{0}),J_{g}=J_{g}(\theta_{0}),g_{\psi,\lambda}=g_{\psi, \lambda}(\theta_{0},X)\) in parts 2, 3, and 4._

Proof: _1._ By construction,

\[[J_{g}^{-1}(\theta)]_{\psi\theta}= ([J_{g}^{-1}(\theta)]_{\psi\psi},\ [J_{g}^{-1}(\theta)]_{\psi\lambda})\] \[= [J_{g}^{-1}(\theta)]_{\psi\psi}(I_{r},\ [J_{g}^{-1}(\theta)]_{\psi \psi}^{-1}[J_{g}^{-1}(\theta)]_{\psi\lambda}),\]

where the second equality is obtained by factoring out the term \([J_{g}^{-1}(\theta)]_{\psi\psi}\). By Proposition 9.3, \([J_{g}^{-1}(\theta)]_{\psi\psi}^{-1}[J_{g}^{-1}(\theta)]_{\psi\lambda}=-J_{g, \psi\lambda}(\theta)J_{g,\lambda\lambda}^{-1}(\theta)\). Hence

\[[J_{g}^{-1}(\theta)]_{\psi\theta}\ g(\theta,X)= [J_{g}^{-1}(\theta)]_{\psi\psi}(I_{r},\ [J_{g}^{-1}(\theta)]_{\psi\psi}^{-1}[J_{g}^{-1}(\theta)]_{\psi\lambda})\] \[= [J_{g}^{-1}(\theta)]_{\psi\psi}\,[I_{r},-J_{g,\psi\lambda}(\theta )J_{g,\lambda\lambda}^{-1}(\theta)]\ g(\theta,X)\] \[= [J_{g}^{-1}(\theta)]_{\psi\psi}\,[g_{\psi}(\theta,X)-J_{g,\psi \lambda}(\theta)J_{g,\lambda\lambda}^{-1}(\theta)g_{\lambda}(\theta,X)]\] \[= [J_{g}^{-1}(\theta)]_{\psi\psi}\,g_{\psi\cdot\lambda}(\theta,X).\]

_2._ Because, by part 1, \((J_{g}^{-1})_{\psi\psi}\,g_{\psi\cdot\lambda}=(J_{g}^{-1})_{\psi\theta}\,g\), we have

\[\operatorname{var}[(J_{g}^{-1})_{\psi\psi}\,g_{\psi\cdot\lambda}]= (J_{g}^{-1})_{\psi\theta}\,K_{g}\,[(J_{g}^{-1})_{\psi\theta}]^{T}. \tag{11.49}\]

Note that, for any \(p\times p\) matrix \(A\), if \(A_{\psi\theta}\) represents the first \(r\) rows of \(A\), then \((A_{\psi\theta})^{T}\) is simply the first \(r\) columns of \(A^{T}\). That is, \((A_{\psi\theta})^{T}=(A^{T})_{\theta\psi}\). Consequently, the right-hand side of (11.49) is

\[(J_{g}^{-1})_{\psi\theta}\,K_{g}\,(J_{g}^{-T})_{\theta\psi}.\]

This matrix is simply the upper-left \(r\times r\) block of the matrix \(J_{g}^{-1}K_{g}J_{g}^{-T}\); that is, \((J_{g}^{-1}K_{g}J_{g}^{-T})_{\psi}\). Because \(J_{g}^{-1}K_{g}J_{g}^{-T}\) is the inverse of the information matrix \(I_{g}\), we have \[(J_{g}^{-1})_{\psi\theta}\,K_{g}\,(J_{g}^{-T})_{\theta\psi}=(I_{g}^{-1})_{\psi\psi}.\]

Thus we have the identity in \(2\).

\(3\). By definition, for each \(i=1,\ldots r\),

\[E[\partial g_{\psi\cdot\lambda}(\theta_{0},X)/\partial\psi_{i}]= E[\partial g_{\psi}(\theta_{0},X)/\partial\psi_{i}]\] \[-\partial[J_{g,\psi\lambda}(\theta_{0})J_{g,\lambda\lambda}^{-1}( \theta_{0})]/\partial\psi_{i}\,E[g_{\lambda}(\theta_{0},X)]\] \[-J_{g,\psi\lambda}J_{g,\lambda\lambda}^{-1}E[\partial g_{\lambda }(\theta_{0},X)/\partial\psi_{i}].\]

Because \(g(\theta,X)\) is unbiased, the second term on the right-hand side is 0, resulting in

\[E[\partial g_{\psi\cdot\lambda}(\theta_{0},X)/\partial\psi^{T}]\] \[=E[\partial g_{\psi}(\theta_{0},X)/\partial\psi^{T}]-J_{g,\psi \lambda}J_{g,\lambda\lambda}^{-1}E[\partial g_{\lambda}(\theta_{0},X)/ \partial\psi^{T}]\] \[=J_{g,\psi\psi}-J_{g,\psi\lambda}J_{g,\lambda\lambda}^{-1}J_{g, \lambda\psi}=(J_{g}^{-1})_{\psi\psi}.\]

\(4\). Similarly, \(E[\partial g_{\psi\cdot\lambda}(\theta_{0},X)/\partial\lambda^{T}]=J_{g,\psi \lambda}-J_{g,\psi\lambda}J_{g,\lambda\lambda}^{-1}J_{g,\lambda\lambda}=0.\)\(\Box\)

Let \(\hat{\theta}=(\hat{\psi}^{T},\hat{\lambda}^{T})^{T}\) be a solution to the estimating equation (11.48). Let \(\tilde{\lambda}\) be a solution to the estimating equation

\[E_{n}[g_{\lambda}(\psi_{0},\lambda)]=0.\]

Let \(\tilde{\theta}=(\psi_{0}^{T},\tilde{\lambda}^{T})^{T}\). Let \(\bar{\lambda}\) be any \(\sqrt{n}\)-consistent estimate of \(\lambda_{0}\), and \(\bar{\theta}=(\psi_{0}^{T},\bar{\lambda}^{T})^{T}\). We now give the formal definitions of Wald's, Rao's, and Neyman's test statistics for an estimating equation \(g\).

**Definition 11.12**: _The Wald's, Rao's, and Neyman's \(C(\alpha)\) test statistics for \(H_{0}:\psi=\psi_{0}\) based on the estimating equation \(g\) are defined, respectively, as_

\[W_{n}(g)= n(\hat{\psi}-\psi_{0})^{T}[I_{g}^{-1}(\hat{\theta})]_{\psi\psi}^{-1 }(\hat{\psi}-\psi_{0}),\] \[R_{n}(g)= nE_{n}[g_{\psi}^{T}(\bar{\theta},X)][J_{g}^{-1}(\bar{\theta})]_{ \psi\psi}[I_{g}^{-1}(\tilde{\theta})]_{\psi\psi}^{-1}[J_{g}^{-1}(\bar{\theta}) ]_{\psi\psi}E_{n}[g_{\psi}(\bar{\theta},X)],\] \[N_{n}(g)= nE_{n}[g_{\psi\cdot\lambda}^{T}(\bar{\theta},X)][J_{g}^{-1}( \bar{\theta})]_{\psi\psi}[I_{g}^{-1}(\bar{\theta})]_{\psi\psi}^{-1}[J_{g}^{-1}( \bar{\theta})]_{\psi\psi}E_{n}[g_{\psi\cdot\lambda}(\bar{\theta},X)].\]

These statistics are generalizations of \(W_{n}\), \(R_{n}\) and \(N_{n}\) defined in (11.39), (11.40), (11.42): if we take \(g\) to be the score function \(s\), then

\[W_{n}=W_{n}(s),\quad R_{n}=R_{n}(s),\quad N_{n}=N_{n}(s).\]

Note that, similar to \(W_{n}\), \(R_{n}\), and \(N_{n}\), \(W_{n}(g)\) requires \(\hat{\theta}\), the solution to the full estimating equation \(E_{n}[g(\theta,X)]=0\); \(R_{n}(g)\) requires \(\bar{\lambda}\), the solution to \(\lambda\)-component of the full estimating equation, \(E_{n}[g_{\lambda}(\psi_{0},\lambda,X)]=0\); whereas \(N_{n}(g)\) just requires any \(\sqrt{n}\)-consistent estimate \(\bar{\lambda}\) of \(\lambda_{0}\).

In the following, We use \(J_{g,n}(\theta)\) to denote the matrix \[E_{n}[\partial g(\theta,X)/\partial\theta^{T}].\]

The next theorem shows that \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\) are QF tests with the same asymptotic quadratic form.

**Theorem 11.14**: _Suppose Assumption 10.3 holds and_

1. \(g(\theta,X)\) _is an unbiased,_ \(P_{\theta}\)_-square-integrable estimating equation and_ \(g(\theta,x)f_{\theta}(x)\) _satisfies_ \(\text{DUI}^{+}(\theta,\mu)\)_;_
2. \(J_{g}(\theta)\) _and_ \(K_{g}(\theta)\) _are invertible and continuous;_
3. _the sequence of random matrices_ \(\{J_{g,n}(\theta):n\in\mathbb{N}\}\) _is stochastically equicontinuous in a neighborhood of_ \(\theta_{0}\)_;_
4. _the true parameter_ \(\theta_{0}\) _is an interior point of_ \(\Theta\)_._

_Then the following assertions hold._

1. _If_ \(\hat{\theta}\) _is a consistent solution of_ \(E_{n}[g(\theta,X)]=0\)_, then_ \[W_{n}(g)\ \overset{Q_{n}}{=}\ U_{n}^{T}(g)\Sigma_{U(g)}^{-1}U_{n}(g)+o_{P}(1),\] _where_ \((U_{n}(g),S_{n},L_{n})\) _satisfies_ \(\text{ALAN}\) _with_ \[U_{n}(g)=\sqrt{n}(J_{g}^{-1})_{\psi\psi}E_{n}(g_{\psi\cdot\lambda}),\quad\Sigma _{U(g)}=(I_{g}^{-1})_{\psi\psi},\quad\Sigma_{U(g)S}=-\left(I_{r},0\right).\]
2. _If_ \(\tilde{\lambda}\) _is a consistent solution to_ \(E_{n}[g_{\psi}(\psi_{0},\lambda,X)]=0\)_, then_ \(R_{n}(g)\) _is a QF test of the same form as_ \(W_{n}(g)\)_._
3. _If_ \(\tilde{\lambda}\) _is any_ \(\sqrt{n}\)_-consistent estimate of_ \(\lambda_{0}\)_,_ \(g_{\psi\cdot\lambda}(\psi_{0},\lambda)\) _is differentiable with respect to_ \(\lambda\)_, and_ \[\{E_{n}[\partial g_{\psi\cdot\lambda}(\psi_{0},\lambda)/\partial\lambda^{T}]:n \in\mathbb{N}\}\] _is stochastically equicontinuous with respect to_ \(\lambda\)_, then_ \(N_{n}(g)\) _is a QF test of the same form as_ \(W_{n}(g)\)_._

_Proof. 1._ By Theorem 9.5 we have

\[\hat{\theta}=\theta_{0}-J_{g}^{-1}(\theta_{0})E_{n}[g(\theta_{0},X)]+o_{P}(n^{ -1/2}).\]

Read off the first \(r\) lines of this equation to obtain

\[\sqrt{n}(\hat{\psi}-\psi_{0})=-\sqrt{n}(J_{g}^{-1})_{\psi\theta}E_{n}[g(\theta _{0},X)]+o_{P}(n^{-1/2}). \tag{11.50}\]

Because \(I_{g}(\theta)\) is continuous and \(\hat{\theta}\) is consistent, \(I_{g}(\hat{\theta})\overset{Q_{n}}{\longrightarrow}I_{g}\), By the invertibility of \(I_{g}\) and Lemma 11.1, this convergence implies \(I_{g}^{-1}(\hat{\theta})\overset{Q_{n}}{\longrightarrow}I_{g}^{-1}\), which, by the continuous mapping theorem, implies \([I_{g}^{-1}(\hat{\theta})]_{\psi\psi}\overset{Q_{n}}{\longrightarrow}(I_{g}^{ -1})_{\psi\psi}\). Because \((I_{g}^{-1})_{\psi\psi}\) is invertible, we have, by Lemma 11.1 again,

\[[I_{g}^{-1}(\hat{\theta})]_{\psi\psi}^{-1}\overset{Q_{n}}{\longrightarrow}(I_{ g}^{-1})_{\psi\psi}^{-1}. \tag{11.51}\]

[MISSING_PAGE_EMPTY:6200]

where, for the second equality we used \(E_{n}(g_{\lambda})=O_{P}(n^{-1/2})\). Using arguments similar to the proof of part 1, we can show that

\[[I_{g}^{-1}(\tilde{\theta})]_{\psi\psi}^{-1}\ \stackrel{{ Q_{n}}}{{=}}\ (I_{g}^{-1})_{\psi\psi}+o_{P}(1),\quad[J_{g}^{-1}(\tilde{\theta})]_{\psi\psi}\ \stackrel{{ Q_{n}}}{{=}}\ (J_{g}^{-1})_{\psi\psi}+o_{P}(1).\]

Hence

\[R_{n}(g)= \,[E_{n}(g_{\psi\cdot\lambda})+o_{P}(n^{-1/2})]^{T}[(J_{g}^{-1})_{ \psi\psi}+o_{P}(1)][(I_{g}^{-1})_{\psi\psi}+o_{P}(1)]\] \[[(J_{g}^{-1})_{\psi\psi}+o_{P}(1)]^{T}[E_{n}(g_{\psi\cdot\lambda}) +o_{P}(n^{-1/2})]\] \[= \,n[(J_{g}^{-1})_{\psi\psi}E_{n}(g_{\psi\cdot\lambda})]^{T}(I_{g}^ {-1})_{\psi\psi}^{-1}[(J_{g}^{-1})_{\psi\psi}E_{n}(g_{\psi\cdot\lambda})]+o_{P}(1),\]

where the right-hand side is the same as (11.52). The rest of the proof of part 2 is same as that of part 1.

3. By Taylor's theorem,

\[E_{n}[g_{\psi\cdot\lambda}(\psi_{0},\bar{\lambda},X)]=E_{n}[g_{\psi\cdot\lambda }(\theta_{0},X)]+E_{n}[\partial g_{\psi\cdot\lambda}(\psi_{0},\lambda^{\ddagger},X)/ \partial\lambda^{T}](\bar{\lambda}-\lambda_{0})\]

for some \(\lambda^{\ddagger}\) between \(\lambda_{0}\) and \(\bar{\lambda}\). Because \(\{E_{n}[\partial g_{\psi\cdot\lambda}(\psi_{0},\lambda,X)]:n\in\mathbb{N}\}\) is stochastically equicontinuous and \(\lambda^{\ddagger}\smash{\mathop{\longrightarrow}\limits^{Q_{n}}}\lambda_{0}\), we have by Corollary 8.2

\[E_{n}[\partial g_{\psi\cdot\lambda}(\psi_{0},\lambda^{\ddagger},X)/\partial \lambda^{T}]\ \stackrel{{ Q_{n}}}{{=}}E[\partial g_{\psi\cdot\lambda}(\theta_{0},X)/\partial\lambda^{T}]+o_{P}(1)=o_{P}(1),\]

where the second equality follows from Lemma 11.3, part 4. Therefore,

\[E_{n}[g_{\psi\cdot\lambda}(\psi_{0},\bar{\lambda},X)]\ \stackrel{{ Q_{n}}}{{=}}\ E_{n}[g_{\psi\cdot\lambda}(\theta_{0},X)]+o_{P}(n^{-1/2}).\]

Using arguments similar to those in the proof of part 1, we can show that

\[[I_{g}^{-1}(\tilde{\theta})]_{\psi\psi}^{-1}\ \stackrel{{ Q_{n}}}{{=}}\ (I_{g}^{-1})_{\psi\psi}+o_{P}(1),\quad[J_{g}^{-1}(\tilde{\theta})]_{\psi\psi}\ \stackrel{{ Q_{n}}}{{=}}\ (J_{g}^{-1})_{\psi\psi}+o_{P}(1).\]

Hence

\[N_{n}(g)\ \stackrel{{ Q_{n}}}{{=}}\ n[(J_{g}^{-1})_{\psi\psi}E_{n}(g_{ \psi\cdot\lambda})]^{T}(I_{g}^{-1})_{\psi\psi}^{-1}[(J_{g}^{-1})_{\psi\psi}E_{n}(g_{\psi\cdot\lambda})]+o_{P}(1),\]

where the right-hand side is the same as (11.52). The rest of the proof of this part is the same as that of part 1. \(\Box\)

From Theorem 11.14 we can immediately derive the asymptotic distributions of \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\) under \(P_{n}(\delta)\) for any \(\delta\in\mathbb{R}^{p}\). The proof is straightforward and is omitted.

**Corollary 11.3**: _Under the conditions in Theorem 11.14, \(W_{n}(g)\), \(R_{n}(g)\) and \(N_{n}(g)\) each converges in distribution to \(\chi_{r}^{2}(\delta_{\psi}^{T}(I_{g}^{-1})_{\psi\psi}^{-1}\delta_{\psi})\) under the local alternative distribution \(P_{n}(\delta)\)._Note that \((I_{g}^{-1})_{\psi\psi}^{-1}\) is monotone nondecreasing with respect to \(I_{g}\) in the sense that, if \(I_{g^{*}}\succeq I_{g}\), then \((I_{g^{*}}^{-1})_{\psi\psi}^{-1}\succeq(I_{g}^{-1})_{\psi\psi}^{-1}\). This is because

\[I_{g^{*}}\succeq I_{g}\Rightarrow I_{g^{*}}^{-1}\preceq I_{g}^{-1}\Rightarrow(I _{g^{*}}^{-1})_{\psi\psi}\preceq(I_{g}^{-1})_{\psi\psi}\Rightarrow(I_{g^{*}}^{- 1})_{\psi\psi}^{-1}\succeq(I_{g}^{-1})_{\psi\psi}^{-1}.\]

Hence, if \(g^{*}\) is the optimal estimating equation among a class of estimating equations \(\mathcal{G}\), then the local alternative asymptotic distribution of \(W_{n}(g^{*})\), \(R_{n}(g^{*})\), and \(N_{n}(g^{*})\) under \(P_{n}(\delta)\) have the largest noncentrality parameter among that class, implying that they are asymptotically most powerful compared with any \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\) for \(g\in\mathcal{G}\). In other words, an optimal estimating equation leads to an optimal QF test.

#### 11.7.2 QF tests for canonical estimating equations

The QF tests described above are applicable to arbitrary unbiased and \(P_{\theta}\)-square-integrable estimations that satisfy some mild additional assumptions. These statistics take simpler forms when the identity \(J_{g}=-K_{g}\) holds. Recall that this relation does hold for the score function \(s(\theta,x)\) under mild conditions; that is,

\[-J(\theta)=K(\theta)=I(\theta)\]

for \(J(\theta)\) and \(K(\theta)\) defined in (8.15). This relation does not hold for a general estimating equation \(g\), but it is always possible to find an equivalent transformation of \(g\) that satisfies this relation.

Specifically, let \(g(\theta,X)\) be any unbiased and \(P_{\theta}\)-square-integrable estimating equation such that \(f_{\theta}(x)g(\theta,x)\) satisfies \(\text{DUI}^{+}(\theta,\mu)\). All the estimating equations in the class

\[\mathcal{G}_{g}=\{B(\theta)g(\theta,X):B(\theta)\in\mathbb{R}^{p \times p},B(\theta)\ \ \text{is differentiable and invertible}\}\]

are equivalent. That is, \(E_{n}[h(\theta,X)]=0\) produces the same solution(s) for any \(h\in\mathcal{G}_{g}\). Adopt again the notation

\[E_{\theta}[g_{1}(\theta,X)g_{2}^{T}(\theta,X)]=[g_{1},g_{2}].\]

In this notation, and in view of Lemma 9.1, we have

\[J_{g}(\theta)=-[g,s],\quad K_{g}(\theta)=[g,g],\]

where \(s=s(\theta,x)\) is the score function. Let \(\tilde{g}(\theta,X)=B(\theta)g(\theta,X)\), and consider the equation \([\tilde{g},s]=[\tilde{g},\tilde{g}]\). If this holds then

\[[s,g]B^{T}=B[g,g]B^{T}\] \[\Rightarrow[s,g]=B[g,g]\] \[\Rightarrow B=[s,g][g,g]^{-1}\] \[\Rightarrow B=-J_{g}^{T}(\theta)K_{g}^{-1}(\theta).\]So, if we let

\[\tilde{g}(\theta,X)=-J_{g}^{T}(\theta)K_{g}^{-1}(\theta)\,g(\theta,X),\]

then \(\tilde{g}\) satisfies \(J_{\tilde{g}}=-K_{\tilde{g}}\), and \(\tilde{g}\) is equivalent to \(g\). This motivates the following definition of canonical form of an estimating equation.

**Definition 11.13**: _Let \(g\) be an unbiased, \(P_{\theta}\)-square-integrable estimating function such that \(g(\theta,x)f_{\theta}(x)\) satisfies DUI\({}^{T}(\theta,\mu)\) with \(J_{g}(\theta)\) and \(K_{g}(\theta)\) invertible. The canonical form of \(g\) is \(-J_{g}(\theta)^{T}K_{g}^{-1}(\theta)g(\theta,X)\)._

Since the canonical form of an estimating equation is equivalent to the estimating equation, we can assume, without loss of generality, any estimating equation satisfies \(J_{g}=-K_{g}\). With this in mind, we can redefine \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\) in the canonical form of \(g\).

**Definition 11.14**: _Suppose \(g\) is a canonical estimating equation. The Wald's, Rao's, and Neyman's \(C(\alpha)\) tests are defined as_

\[W_{n}(g)= n(\hat{\psi}-\psi_{0})^{T}[I_{g}^{-1}(\hat{\theta})]_{\psi\psi}^{-1} (\hat{\psi}-\psi_{0}),\] \[R_{n}(g)= nE_{n}[g_{\psi}^{T}(\tilde{\theta},X)][I_{g}^{-1}(\tilde{\theta})] _{\psi\psi}E_{n}[g_{\psi}(\tilde{\theta},X)].\] \[N_{n}(g)= nE_{n}[g_{\psi\cdot\lambda}^{T}(\tilde{\theta},X)][I_{g}^{-1}( \tilde{\theta})]_{\psi\psi}E_{n}[g_{\psi\cdot\lambda}(\tilde{\theta},X)].\]

Note that \(W_{n}(g)\) takes the same form as that in Definition 11.12, but here \(I_{g}=-J_{g}=K_{g}\), which is not the case for \(W_{n}(g)\) in Definition 11.12. The forms of \(R_{n}(g)\) and \(N_{n}(g)\) are simplified due to the relation \(-J_{g}=I_{g}\). The Rao's statistic for a canonical estimating equation takes a particularly simple form. Since \(g_{\lambda}(\psi_{0},\tilde{\lambda})=0\), \(R_{n}(g)\) reduces to

\[nE_{n}[g^{T}(\tilde{\theta},X)]I_{g}^{-1}(\tilde{\theta})E_{n}[g(\tilde{\theta },X)],\]

which is of the same form as the score test for likelihood in Definition 11.3 except that \(s\) is replaced by \(g\). The quadratic form of \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\) when \(g\) is in the canonical form is simplified correspondingly, which is recorded in the next corollary.

**Corollary 11.4**: _If the conditions in Theorem 11.14 hold and \(g\) is in its canonical form, then under \(Q_{n}\), the statistics \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\) are of the following asymptotic quadratic form_

\[U_{n}^{T}(g)\Sigma_{U(g)}^{-1}U_{n}(g)+o_{P}(1),\]

_where \((U_{n}(g),S_{n},L_{n})\) satisfies ALAN with_

\[U_{n}(g)=\sqrt{n}E_{n}(g_{\psi\cdot\lambda}),\quad\Sigma_{U(g)}=(I_{g}^{-1})_ {\psi\psi}^{-1},\quad\Sigma_{U(g)S}=-((I_{g}^{-1})_{\psi\psi}^{-1},0).\]

_Consequently, they converge in distribution to \(\chi_{r}^{2}(\delta_{\psi}(I_{g}^{-1})_{\psi\psi}^{-1}\delta_{\psi})\) under \(P_{n}(\delta)\)._

Interestingly, the asymptotic distribution of \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\) are the same whether or not \(g\) is in its canonical form, which is not surprising because an estimating equation is equivalent to its canonical form in the sense that they have the same solution(s).

#### 11.7.3 Wilks's test for conservative estimating equations

Up to this point we haven't mentioned the generalization of the likelihood ratio test for estimating equations. This is because an estimating equation \(g(\theta,X)\) does not in general correspond to a "likelihood"; that is, there need not be a function \(\ell(\theta,X)\) such that

\[\partial\ell(\theta,X)/\partial\theta^{T}=g(\theta,X).\]

A set of sufficient conditions for \(g(\theta,X)\) to possess such a function \(\ell(\theta,X)\) are

1. \(g(\theta,X)\) is continuously differentiable with \(\partial g(\theta,X)/\partial\theta^{T}\) being a symmetric matrix;
2. \(\Theta\) is a convex set in \(\mathbb{R}^{p}\).

The convex assumption \(\Theta\) is not the weakest possible, but is good enough for our purpose. Li and McCullagh (1994) call such estimating equations conservative estimating equations because \(\{g(\theta,X):\theta\in\Theta\}\) forms a conservative vector field. For such estimating equations the line integral

\[\int_{C}g^{T}(\theta,X)d\theta,\]

where \(C\) is a smooth curve from a fixed point \(a\in\Theta\) to an arbitrary point \(\theta\in\Theta\), does not depend on the curve \(C\). As such, the integral is a function of \(\theta\). We define this integral as the "quasilikelihood function" \(\ell(\theta,X)\) for the estimating equation \(g(\theta,X)\). McCullagh (1983) introduced such a definition for linear estimating equations. A convenient choice of \(C\) is the straight line. Specifically, fix any point \(a\in\Theta\), and let \(\theta\in\Theta\) be an arbitrary point. Because \(\Theta\) is assumed convex, the straight line \(\{(1-t)a+t\theta:t\in[0,1]\}\) is contained in \(\Theta\), and we can define \(\ell(\theta,X)\) as the line integral

\[\ell(\theta,X)=\int_{0}^{1}g^{T}[(1-t)a+t\theta,X](\theta-a)dt. \tag{11.53}\]

It can be easily checked (Problem 11.20) that \(\partial\ell(\theta,X)/\partial\theta=g(\theta,X)\). This motivates the following definition of the Wilks likelihood ratio statistics based on a conservative and canonical estimating equation \(g\).

**Definition 11.15**: _Suppose that \(g(\theta,X)\) is a canonical and conservative estimating equation. Let \(\ell(\theta,X)\) be the line integral (11.53). Let \(\hat{\theta}\) be a solution to \(E_{n}[g(\theta,X)]=0\) and \(\hat{\theta}\) be a solution to \(E_{n}[g_{\lambda}(\psi_{0},\lambda,X)]=0\). The Wilks's statistic for the estimating equation \(g\) is_

\[T_{n}(g)=2nE_{n}[\ell(\hat{\theta},X)-\ell(\tilde{\theta},X)].\]

The next theorem shows that \(T_{n}(g)\) is a QF-test with of the same asymptotic quadratic form as \(W_{n}(g)\), \(R_{n}(g)\), and \(N_{n}(g)\). Of course, we should keep in mind that \(T_{n}(g)\) requires \(g\) to be a conservative estimating equation, whereas the other tests do not.

**Theorem 11.15**: _Suppose Assumption 10.3 holds and_

1. \(g(\theta,X)\) _is an unbiased,_ \(P_{\theta}\)_-square-integrable, canonical, and conservative estimating equation;_
2. \(f_{\theta}(X)\) _and_ \(g(\theta,X)f_{\theta}(X)\) _satisfy_ \(\text{DUI}^{+}(\theta,\mu)\) _with_ \(J_{g}(\theta)\) _invertible;_
3. _the sequence_ \(\{E_{n}[\partial g(\theta,X)/\partial\theta^{T}]:n\in\mathbb{N}\}\) _is stochastically equicontinuous;_
4. \(\hat{\theta}\) _is a consistent solution to_ \(E_{n}[g(\theta,X)]=0\) _and_ \(\tilde{\lambda}\) _is a consistent solution to_ \(E_{n}[g(\psi_{0},\lambda,X)]=0\)_._

_Then \(T_{n}(g)\) is a QF test of the form given in Corollary 11.4. Consequently,_

\[T_{n}(g)\overset{\mathcal{D}}{\underset{P_{n}(\delta)}{\longrightarrow}}\chi _{r}^{2}\left(\delta_{\psi}^{T}(I_{g}^{-1})^{-1}_{\psi\psi}\delta_{\psi}\right).\]

The proof is similar to that of Theorem 11.2 and is left as an exercise (Problem 11.23).

## Problems

Many of the following problems require regularity assumptions that are too tedious to be stated completely. We therefore leave it to the readers to impose them as appropriate. In particular, these types of assumptions will be made without mentioning:

1. integrability: certain moments involved, such as means, variances, and third moments, are finite;
2. differentiability and DUI: certain functions of \(\theta\) are differentiable to a required order, and when needed, the derivatives can be exchanged with integral over a random variable;
3. stochastic continuity: certain sequences of random functions of \(\theta\) are stochastic equicontinuous.

It is usually obvious where these assumptions should be imposed.

**11.1**: The notion of a QF test can be extended to the case where \(\Sigma_{U}\) is singular. Suppose \((U_{n},S_{n},L_{n})\) satisfies ALAN with a singular \(\Sigma_{U}\), where \(U_{n}\in\mathbb{R}^{r}\), \(S_{n}\in\mathbb{R}^{p}\), and \(\Sigma_{U}\in\mathbb{R}^{p\times p}\). Suppose \(\Sigma_{U}^{-}\) is a reflexive generalized inverse of \(\Sigma_{U}\); that is,

\[\Sigma_{U}\Sigma_{U}^{-}\Sigma_{U}=\Sigma_{U},\quad\Sigma_{U}^{-}\Sigma_{U} \Sigma_{U}^{-}=\Sigma_{U}^{-}.\]

See, for example, Kollo and von Rosen (2005). Suppose, furthermore, \(\Sigma_{U}^{-}\) is a symmetric matrix. We define a corresponding QF test as any statistic \(T_{n}\) that satisfies

\[T_{n}\ \overset{Q_{n}}{=}\ U_{n}^{T}\Sigma_{U}^{-}U_{n}+o_{P}(1).\]

Show that\[T_{n}\underset{P_{n}(\delta)}{\overset{\mathcal{D}}{\longrightarrow}}\chi_{d}^{2}( \delta^{T}\Sigma_{SU}\Sigma_{U}^{-}\Sigma_{US}\delta),\]

where \(d\) is the rank of \(\Sigma_{U}\). (Here, we have used a more general definition of inverse than the Moore-Penrose inverse to accommodate Pearson's test, which is discussed in the next problem).

**11.2.** This problem concerns Pearson's Chi-square test. Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. multinomial variables with \(k\) categories and probabilities \((p_{1},\ldots,p_{k})\). Let \(\theta=(p_{1},\ldots,p_{k})^{T}\).

1. Show that the MLE of \(\theta\) is \[\hat{\theta}=(n_{1}/n,\ldots,n_{k}/n)^{T},\] where \(n_{i}=\sum_{\ell=1}^{n}I(X_{\ell}=i)\).
2. Since the MLE is obtained by maximizing \(E_{n}[\ell(\theta,X)]\) subject to \(h(\theta)=1_{p}^{T}\theta=1\), the score function is to be derived from \[\begin{cases}E_{n}\ell(\theta,X)-[\partial h(\theta)/\partial\theta]\lambda= 0\\ h(\theta)=1\end{cases}\] From this deduce that \[S_{n}=\sqrt{n}\,Q\,\text{diag}(\theta)^{-1}E_{n}(Z),\] where \(Q=I_{p}-1_{p}1_{p}^{T}\), \(Z=(Z_{1},\ldots,Z_{k})^{T}\) and \(Z_{i}=I(X=i)\).
3. Let \(U_{n}=\sqrt{n}(\hat{\theta}-\theta_{0})\). Show that \((U_{n},S_{n},L_{n})\) satisfies ALAN with \[\Sigma_{U}=\text{diag}(\theta)-\theta\theta^{T},\quad\Sigma_{US}=Q.\]
4. Show that \(\Sigma_{U}^{-}=\text{diag}(\theta)^{-1}-1_{k}1_{k}^{T}\) is a reflexive and symmetric generalized inverse of \(\Sigma_{U}\).
5. In the setting of Problem 11.2, show that Wald's statistic \(W_{n}=n(\hat{\theta}-\theta_{0})^{T}\Sigma_{U}^{-}(\hat{\theta})(\hat{\theta} -\theta_{0})\) in this case reduces to the Pearson's chi-square test \[W_{n}=\sum_{i=1}^{k}\frac{(n_{i}-np_{i})^{2}}{np_{i}}.\] (11.54)
6. Show that \(W_{n}\underset{P_{n}(\delta)}{\overset{\mathcal{D}}{\longrightarrow}}\chi_{k- 1}^{2}(\delta^{T}Q\text{diag}(\theta)^{-1}Q\delta)\).

**11.3.** In the setting of Problem 11.2, prove the following statements.

1. The Fisher information is \(I(\theta)=Q\text{diag}(\theta)^{-1}Q\).

2. A reflexive generalize inverse of \(I(\theta)\) is \[I^{-}(\theta)=\text{diag}(\theta)-\theta\theta^{T}.\]3. \(I^{-}(\theta)=QI^{-}(\theta)=I^{-}(\theta)Q=QI^{-}(\theta)Q\).
4. Rao's score test, \(R_{n}=nS_{n}^{T}I^{-}(\theta_{0})S_{n}\), also takes the form of Pearson's chi-square test in (11.54).

**11.4.** In the setting of Problem 11.2, show that the Wilks's likelihood ratio test takes the form

\[T_{n}=\sum_{i=1}^{k}n_{i}\log\left(\frac{n_{i}}{np_{i}}\right).\]

Derive the asymptotic distribution of \(T_{n}\) under \(P_{n}(\delta)\).

**11.5.** Suppose \(X_{1},\ldots,X_{n}\) are an i.i.d. sample from \(N(\theta,\theta)\), where \(\theta>0\). Let \(\hat{\theta}\) be the maximum likelihood estimate of the true parameter \(\theta_{0}\).

1. Let \(T_{n}\) be the Wilks's test statistic for testing \(H_{0}:\theta=1\). If \(n=100\), find the local asymptotic alternative distribution of \(T_{n}\) at \(\theta=1.1\).
2. Let \(U_{n}=n(\overline{X}-\theta_{0})^{2}/\theta_{0}\). Derive the asymptotic null distribution (under \(Q_{n}\)) and local alternative distribution of \(W_{n}\) (under \(P_{n}(\delta)\)).
3. Derive the Pitman efficiency of \(U_{n}\) and show that it is no greater than 1.
4. Let \(M_{n}=n^{-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\), and \(V_{n}=n(M_{n}-\theta_{0})^{2}/(2\theta_{0}^{2})\). Derive the asymptotic null and local alternative distribution of \(V_{n}\) for testing \(H_{0}:\theta=\theta_{0}\).
5. Derive the Pitman efficiency of \(V_{n}\).
6. For which region of \(\theta\) is \(V_{n}\) more efficient than \(U_{n}\)?

**11.6.** Let \(X_{1},\ldots,X_{n}\) be an i.i.d. sample from probability density function \(f_{\theta}\), which is supported on \((-\infty,\infty)\) and dominated by the Lebesgue measure. For \(0<p<1\), let \(\tau_{p}(\theta)\) be the \(p\)th quantile of \(X\). That is,

\[\int_{-\infty}^{\tau_{p}(\theta)}f_{\theta}(x)d\lambda(x)=p.\]

Let \(T_{n}\) be the sample \(p\)th quantile (the exact definition of this statistic is not important for our purpose). It is known that \(T_{n}\) has expansion (Bahadur, 1966)

\[T_{n}\stackrel{{\theta}}{{=}}\tau_{p}(\theta)+cE_{n}[I(X\leq\tau_ {p}(\theta))-p]+o_{P}(1/\sqrt{n}).\]

It is also known that \(T_{n}\) is a regular estimate.

1. Use the fact that \(T_{n}\) is regular to derive the value of the constant \(c\).
2. Find the asymptotic null and local alternative distributions of \(\sqrt{n}(T_{n}-\tau_{p}(\theta))\).
3. Construct a QF test based on \(T_{n}\) for testing \(H_{0}:\theta=\theta_{0}\), and derive its asymptotic local alternative distribution.

4. Suppose that the distribution of \(X_{i}\) is \(N(\theta,1)\), and that the \(p\) in \(\tau_{p}\) is \(1/2\). Find the asymptotic local alternative distribution of the QF test constructed in part 2, for testing the hypothesis \(H_{0}:\theta=0\), and derive Pitman's efficiency.

**11.7.** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. \(N(\theta_{1},\theta_{2})\) where \(\theta_{1}\in\mathbb{R}\) and \(\theta_{2}>0\). Let \(\theta=(\theta_{1},\theta_{2})^{T}\). We are interested in testing the null hypothesis

\[H_{0}:\theta_{1}=\theta_{2}.\]

Let \(\hat{\theta}\) be the global MLE and \(\tilde{\theta}\) be the MLE under \(H_{0}\). Let \(T_{n}\) be the Wilks's test statistic:

\[T_{n}=2\sum_{i=1}^{n}[\ell(\hat{\theta},X_{i})-\ell(\tilde{\theta},X_{i})].\]

Derive the asymptotic distribution of \(T_{n}\) under \(P_{n}(\delta)\).

**11.8.** Let \(X_{1},\ldots,X_{n}\) be i.i.d. with p.d.f. \(f_{\theta}(x)\), where \(\theta\in\Theta\subseteq\mathbb{R}^{p}\) is a parameter. We are interested in testing the null hypothesis \(H_{0}:\theta=\theta_{0}\). Let \(\hat{\theta}\) be a consistent maximum likelihood estimate and define

\[R(\theta_{0},\theta)=\sum_{i=1}^{n}\left[\frac{f_{\theta}(X_{i})}{f_{\theta_{0 }}(X_{i})}-\frac{f_{\theta_{0}}(X_{i})}{f_{\theta}(X_{i})}\right].\]

Show that the statistic \(R(\theta_{0},\hat{\theta})\) is a QF test, and derive its asymptotic distribution under \(P_{n}(\delta)\). Is this an asymptotically efficient test? (See Li, 1993).

**11.9.** Let \(X_{1},\ldots,X_{n}\) be independent copies of \(X\), where \(X\) is a random variable with finite fourth moments. Assume that \(X\) has distribution \(P_{\theta}\), where \(\theta\in\Theta\subseteq\mathbb{R}^{p}\) (here, the dimension of \(\theta\) is irrelevant). Let \(\theta_{0}\) be the true parameter value of \(\theta\). Let \(\mu_{1}(\theta),\mu_{2}(\theta),\mu_{3}(\theta),\mu_{4}(\theta)\) denote the first four moments of \(X\). Let \(\sigma^{2}(\theta)\) denote the variance of \(X\). Let \(\rho(\theta)\) be the signal-to-noise ratio defined as follows

\[\rho(\theta)=\frac{\mu_{1}(\theta)}{\sigma(\theta)}.\]

Estimate \(\rho(\theta_{0})\) by \(\hat{\rho}=\hat{\mu}_{1}/\hat{\sigma}\), where \(\hat{\mu}_{1}\), \(\hat{\sigma}\) are the sample mean and sample standard deviation, respectively.

1. Derive the asymptotic distribution of \(\sqrt{n}(\hat{\rho}-\rho(\theta_{0}))\) under \(P_{n}(\delta)\).

2. Based on part 1 construct a QF test \(T_{n}\), and derive its asymptotic distribution under \(P_{n}(\delta)\).

**11.10.** Let \(X_{1},...,X_{n}\) be an i.i.d. sample from a distribution whose density is \(f_{\theta}(x)\), with \(\theta=(\psi^{T},\lambda^{T})^{T}\in\mathbb{R}^{p}\), \(\psi\in\mathbb{R}^{r}\), \(\lambda\in\mathbb{R}^{s}\). For testing the null hypothesis \(H_{0}:\psi=\psi_{0}\), let \(\hat{\theta}=(\hat{\psi}^{T},\hat{\lambda}^{T})^{T}\) be a consistent solution to \(E_{n}[s(\theta,X)]=0\)and \(\tilde{\lambda}\) a consistent solution to the estimating equation \(E_{n}[s_{\lambda}(\psi_{0},\lambda,X)]=0\). Let \(S_{n,\psi}(\theta)=\sqrt{n}E_{n}[s_{\psi}(\theta,X)]\) be the rescaled score for \(\psi\). Let

\[B_{n}=\sqrt{n}(\hat{\psi}-\psi_{0})^{T}\,S_{n,\psi}(\psi_{0},\tilde{\lambda},X).\]

This is a hybrid between the Wald's and Rao's score statistic similar to the statistic proposed by Li and Lindsay (1996).

1. Show that \(B_{n}\) is a QF test, derive its asymptotic quadratic form, and its asymptotic distribution under \(P_{n}(\delta)\).

2. Show that \(B_{n}\) can be rewritten as the more compact form

\[B_{n}=\sqrt{n}(\hat{\theta}-\tilde{\theta})^{T}S_{n}(\tilde{\theta},X),\]

where \(\tilde{\theta}=(\psi_{0}^{T},\tilde{\lambda}^{T})^{T}\).

**11.11**.: Under the setting of Problem 11.10, let \(S_{n,\psi\cdot\lambda}(\theta,X)\) be the rescaled efficient score \(\sqrt{n}E_{n}[s_{\psi\cdot\lambda}(\theta,X)]\). Let \(\tilde{\lambda}\) be any \(\sqrt{n}\)-consistent estimate of \(\lambda_{0}\). Derive the asymptotic distribution of

\[\sqrt{n}(\hat{\psi}-\psi_{0})^{T}S_{n,\psi\cdot\lambda}(\tilde{\theta};X)\]

under \(P_{n}(\delta)\).

**11.12**.: Under the setting of Problem 11.10, for testing the null hypothesis \(H_{0}:h(\theta)=0\), let \(\tilde{\theta}\) be a consistent solution to \(E_{n}[s(\theta,X)]=0\) and \(\tilde{\theta}\) be a consistent solution to \(E_{n}[s(\theta,X)]=0\) subject to \(h(\theta)=0\). Show that

\[\sqrt{n}(\hat{\theta}-\tilde{\theta})^{T}S_{n}(\tilde{\theta},X)\]

is a regular QF test. Derive its asymptotic distribution under \(P_{n}(\delta)\). Is this test an Asymptotically Efficient test?

**11.13**.: Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. random variables with density \(f_{\theta}(x)\), where \(\theta\in\Theta\subseteq\mathbb{R}^{p}\), \(\theta=(\psi^{T},\lambda^{T})^{T}\), \(\psi\in\mathbb{R}^{r}\) and \(\lambda\in\mathbb{R}^{s}\), \(r+s=p\). We are interested in testing the null hypothesis \(H_{0}:\psi=\psi_{0}\). Consider the following procedure for estimating \(\psi_{0}\). First, estimate \(\lambda_{0}\) by \(\tilde{\lambda}\), which is the solution to estimating equation \(E_{n}[s_{\lambda}(\psi_{0},\lambda)]=0\). Second, estimate \(\psi_{0}\) by \(\tilde{\psi}\), which is the solution in \(\psi\) to the estimating equation \(E_{n}[s_{\psi}(\psi,\tilde{\lambda})]=0\).

1. Show that \(\tilde{\psi}\) is not regular at \(\theta_{0}\) unless \(I_{\psi\lambda}=O_{r\times s}\).
2. Compare the asymptotic variances of \(\sqrt{n}(\tilde{\psi}-\psi)\) and \(\sqrt{n}(\hat{\psi}-\psi)\), where \(\hat{\psi}\) is the first \(r\) components of the MLE \(\hat{\theta}\).
3. Construct a QF test based on the asymptotic distribution of \(\sqrt{n}(\tilde{\psi}-\psi)\), and show that this test is a regular test despite the fact that \(\hat{\psi}\) in not regular at \(\theta_{0}\). Is this test asymptotically efficient?

**11.14**.: Under the setting of Problem 11.13, suppose \(\hat{\theta}\) is the unconstrained MLE, and \(\hat{\psi}\) is the first \(r\) components of \(\hat{\theta}\).

1. Derive the asymptotic linear form of \(\sqrt{n}E_{n}[s_{\psi}(\psi_{0},\hat{\lambda};X)]\), and construct a QF test based this form.
2. Derive the asymptotic distribution of this QF test under \(P_{n}(\delta)\).
3. Is this QF test regular? Is it asymptotically efficient?

**11.15.** Under the setting of Problem 11.13, suppose \(\hat{\theta}\) is the unconstrained MLE, and \(\hat{\lambda}\) is the last \(s\) components of \(\hat{\theta}\). Let \(\tilde{\lambda}\) be the MLE for \(\lambda_{0}\) under the null hypothesis \(H_{0}:\psi=\psi_{0}\). Assume \(r=s\).

1. Derive the asymptotic linear form of \(\sqrt{n}(\hat{\lambda}-\tilde{\lambda})\).
2. Assuming \((I^{-1})_{\lambda\lambda}-I^{-1}_{\lambda\lambda}\) is nonsingular, derive a QF test based on the asymptotic linear form obtained in part 1.
3. Show that the QF test in part 2 is regular if and only if \(I_{\lambda\psi}\) is nonsingular.
4. Show that, when this QF test is regular, it is asymptotically efficient.

**11.16.** Under the setting of Problem 11.13, suppose \(\hat{\theta}\) is the unconstrained MLE, and \(\hat{\psi}\) is the first \(r\) components of \(\hat{\theta}\). Let \(\tilde{\lambda}\) be the MLE for \(\lambda_{0}\) under the null hypothesis \(H_{0}:\psi=\psi_{0}\).

1. Derive the asymptotic linear form of \(\sqrt{n}E_{n}[s_{\psi}(\hat{\psi},\tilde{\lambda},X)]\), and based on this result construct a QF test.
2. Give a necessary and sufficient condition for this QF test to be regular.
3. Show that, if this QF test is regular and \(r=s\), then it is asymptotically efficient.

**11.17.** Let \(X_{1},\ldots,X_{n}\) be an i.i.d. sample from a distribution in \(\{P_{\theta}:\theta\in\Theta\}\), where \(\Theta\in\mathbb{R}^{p}\). Consider testing the implicit hypothesis \(H_{0}:h(\theta_{0})=0\) versus \(H_{1}:h(\theta_{0})\neq 0\) where \(h\) is a mapping from \(\Theta\) to \(\mathbb{R}^{r}\). Let \(\hat{\theta}\) be the global MLE and \(\hat{\theta}\) be the MLE subject to the constraint \(h(\theta)=0\). Suppose the conditions in Theorem 11.8 are satisfied.

1. Derive the asymptotic linear form of \(\sqrt{n}(\hat{\theta}-\tilde{\theta})\) under \(\theta_{0}\).
2. Based on part 1 construct a QF test, and derive its asymptotic distribution under \(P_{n}(\delta)\).
3. Is this QF test asymptotically efficient?

**11.18.** Prove the implication in (11.20).

**11.19.** In the special case where \(\theta=(\psi^{T},\lambda^{T})^{T}\) and \(h(\theta)=\psi\), prove the following statements:

1. the first equality in (11.19) reduces to \[E_{n}[s_{\psi}(\psi_{0},\tilde{\lambda},X)]=E_{n}[s_{\psi\cdot\lambda}(\theta _{0},X)]+o_{P}(n^{-1/2}).\]
2. the second equality in (11.19) reduces to \[\tilde{\lambda}=\lambda_{0}+I^{-1}_{\lambda\lambda}E_{n}[s_{\lambda}(\theta_{0 },X)]+o_{P}(n^{-1/2})\]
3. the efficient information \(I_{(H)}\) in Definition 11.9 reduces to \(I_{\psi\cdot\lambda}\).

4. the efficient score \(s_{(H)}\) in Definition 11.9 reduces to \(s_{\psi\cdot\lambda}\).

**11.20.** Let \(\ell(\theta,X)\) be the function defined in (11.53). Show that

\[\partial\ell(\theta,X)/\partial\theta=g(\theta,X).\]

**11.21.** Suppose that \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) are i.i.d. random vectors with an unknown p.d.f. \(f_{\theta}(x,y)\), where \(\theta\in\Theta\in\mathbb{R}^{p}\). Suppose the parametric forms of the conditional mean and variance are given:

\[E_{\theta}(Y|X)=\mu_{\theta}(X),\ \ \mathrm{var}_{\theta}(Y|X)=V(\mu_{\theta}(X)),\]

for some known functions \(\mu(\cdot)\) and \(V(\cdot)\). Consider the class of estimating equations of the form \(a_{\theta}(X)(Y-\mu_{\theta}(X))\). Recall from Section 9.2 that the optimal estimating function among this class is

\[g^{*}(\theta,X,Y)=\frac{\partial\mu_{\theta}(X)}{\partial\theta}\times\frac{Y- \mu_{\theta}(X)}{V(\mu_{\theta}(X))}.\]

This is a special case of the optimal estimating equation in Section 9.2 because the function \(V\) here is assume to depend on \(\theta^{T}X\) through \(\mu\). Suppose \(\theta=(\psi^{T},\lambda^{T})^{T}\), where \(\psi\in\mathbb{R}^{r}\) is the parameter of interest and \(\lambda\in\mathbb{R}^{r}\) is the nuisance parameter. We are interested in testing \(H_{0}:\psi=\psi_{0}\). Assume regularity conditions such as differentiability, integrability and stochastic continuity as appropriate.

1. Let \(\hat{\theta}\) is a consistent solution to \(E_{n}[g^{*}(\theta,X,Y)]=0\), and \(\tilde{\lambda}\) a consistent solution to \(E_{n}[g^{*}_{\lambda}(\psi_{0},\lambda,X)]=0\). Find the asymptotic linear forms of \(\sqrt{n}(\hat{\theta}-\theta_{0})\) and \(\sqrt{n}(\tilde{\theta}-\theta_{0})\).
2. For a fixed vector \(a\in\Theta\), let \(\ell(\mu,Y)\) be the function \[\ell(\mu,Y)=\int_{a}^{\mu}\frac{Y-\nu}{V(\nu)}d\nu,\] and let \[T_{n}=2n\{E_{n}[\ell(\mu_{\tilde{\theta}}(X),Y)]-E_{n}[\ell(\mu_{\tilde{\theta }}(X),Y)]\},\] where \(\tilde{\theta}=(\psi_{0}^{T},\tilde{\lambda})^{T}\). Show that \(T_{n}\) is a QF test and derive its asymptotic distribution under \(P_{n}(\delta)\).

**11.22.** Under the setting of Problem 11.21, assume that

\[\mathrm{var}_{\theta}(Y|X)=V(\theta^{T}X).\]

The difference from Problem 11.21 is that, here, we do not assume \(V(\cdot)\) depends on \(\theta^{T}X\) through \(\mu(\theta^{T}X)\). In this case, the quasi score function

\[g^{*}(\theta,X,Y)=\frac{\partial\mu^{T}(\theta^{T}X)}{\partial\theta}\,\frac{Y- \mu(\theta^{T}X)}{V(\theta^{T}X)}\]need not be a conservative estimating equation. For testing \(H_{0}:\psi=\psi_{0}\), let \(\hat{\theta}\) be a consistent solution to \(E_{n}[g^{*}(\theta,X,Y)]=0\), and \(\tilde{\lambda}\) a consistent solution to \(E_{n}[g^{*}(\psi_{0},\lambda,X,Y)]=0\). Let

\[c(\theta,\eta)=\frac{\mu(\eta^{T}X)-\mu(\theta^{T}X)}{V(\theta^{T}X)}[Y-\mu( \theta^{T}X)].\]

and

\[C(\theta,\eta)=nE_{n}[c(\theta,\eta)-c(\eta,\theta)].\]

Show that \(C(\tilde{\theta},\hat{\theta})\) is a QF test, and derive its asymptotic distribution under \(P_{n}(\delta)\).

**11.23.** Prove Theorem 11.15.

**11.24.** Suppose that \(X_{1},\ldots,X_{n}\) are i.i.d. with p.d.f. \(f_{\theta}(x)\), where \(\theta\in\Theta\subseteq\mathbb{R}^{p}\). Let \(g:\Theta\times\Omega_{X}\rightarrow\mathbb{R}^{p}\) be an unbiased and \(P_{\theta}\)-square-integrable function. For an integer \(1\leq r<p\), let \(h:\Theta\mapsto\mathbb{R}^{r}\) be a differentiable function. We are interested in testing the hypothesis

\[H_{0}:h(\theta)=0\]

based on the estimating equation \(g(\theta,x)\). For convenience, and without loss of generality, assume that \(g\) is in its canonical form. Let \(H(\theta)=\partial h^{T}(\theta)/\partial\theta\), and suppose it has full column rank for all \(\theta\in\Theta\). Let \(I_{g}(\theta)\) be the information contained in \(g(\theta,X)\) and assume that it is nonsingular for all \(\theta\in\Theta\). Let

\[I_{(H),g}= \left[H^{T}(\theta)I_{g}^{-1}(\theta)H(\theta)\right]^{-1}\] \[g_{(H)}(\theta,X)= \,I_{(H),g}(\theta)H^{T}(\theta)I^{-1}(\theta)g(\theta,X).\]

Let \(\hat{\theta}\) be a consistent solution to \(E_{n}[g(\theta,X)]=0\) and \(\tilde{\theta}\) a consistent solution to \(E_{n}[g(\theta,X)]=0\) subject to \(h(\theta)=0\). Let

\[W_{n}(g)= \,nh^{T}(\hat{\theta})I_{(H),g}(\hat{\theta})h(\hat{\theta})\] \[R_{n}(g)= \,nE_{n}[g^{T}(\tilde{\theta},X)]I_{g}^{-1}(\tilde{\theta})E_{n} [g(\tilde{\theta},X)].\]

Show that \(W_{n}(g)\) and \(R_{n}(g)\) are QF test, derive their asymptotic quadratic forms, and their asymptotic distributions under \(P_{n}(\delta)\).

**11.25.** Under the setting of Problem 11.10, suppose we want to test the hypothesis \(H_{0}:h(\theta)=0\) based on an estimating equation \(g(\theta,X)\). Suppose \(g\) is in its canonical form. Let \(\hat{\theta}\) be a consistent solution to \(E_{n}[g(\theta,X)]=0\) and \(\tilde{\theta}\) be a consistent solution to \(E_{n}[g(\theta,X)]=0\) subject to \(h(\theta)=0\). Show that

\[n(\hat{\theta}-\tilde{\theta})^{T}E_{n}[g(\tilde{\theta},X)]\]

is a regular QF test for testing \(H_{0}:h(\theta)=0\), and derive its asymptotic distribution under \(P_{n}(\delta)\).

## References

* Bahadur (1966) Bahadur, R. R. (1966). A note on quantiles in large samples. _The Annals of Mathematical Statistics_. **37**, 37 577-581.
* Bera and Bilias (2001) Bera, A. K. and Bilias, Y. (2001). Rao's score, Neyman's \(C(\alpha)\) and Silvey's LM tests: an essay on historical developments and some new results. _Journal of Statistical Planning and Inference_**97**, 9-44.
* Boos (1992) Boos, D. D. (1992). On Generalized Score Tests. _The American Statistician_. **46**, 327-333.
* Engle (1984) Engle, R. F. (1984). Wald, likelihood ratio, and Lagrange multiplier tests in econometrics. _Handbook of Econometrics_, **2**, 775-826.
* Hall and Mathiason (1990) Hall, W. J. and Mathiason, D. J. (1990). On large-sample estimation and testing in parametric models. _Int. Statist. Rev._, **58**, 77-97.
* Horn and Johnson (1985) Horn, R. A. and Johnson, C. R. (1985). _Matrix Analysis_. Cambridge University Press.
* Kocherlakota and Kocherlakota (1991) Kocherlakota, S. and Kocherlakota, K. (1991). Neyman's \(C(\alpha)\) test and Rao's efficient score test for composite hypotheses. _Statistics and Probability Letters_, **11**, 491-493.
* Kollo and von Rosen (2005) Kollo, T. and von Rosen, D. (2005). _Advanced Multivariate Statistics with Matrices_. Springer.
* Li (1993) Li, B. (1993). A deviance function for the quasi-likelihood method. _Biometrika_, **80**, 741-753.
* Li and Lindsay (1996) Li, B. and Lindsay, B. (1996). Chi-square tests for Generalized Estimating Equations with possibly misspecified weights. _Scandinavian Journal of Statistics_, **23**, 489-509.
* Li and McCullagh (1994) Li, B. and McCullagh, P. (1994). Potential functions and conservative estimating functions. _The Annals of Statistics_, **22**, 340-356.
* Mathew and Nordstrom (1997) Mathew, T. and Nordstrom, K. (1997). Inequalities for the probability content of a rotated ellipse and related stochastic domination results. _The Annals of Applied Probability_, **7**, 1106-1117.
* McCullagh (1983) McCullagh, P. (1983). Quasi-likelihood functions. _The Annals of Statistics_, **11**, 59-67.
* Neother (1950) Neother, G. E. (1950). Asymptotic properties of the Wald-Wolfowitz test of randomness. _Ann. Math. Statist._, **21**, 231-246.
* Neother (1955) Neother, G. E. (1955). On a theorem by Pitman. _Ann. Math. Statist._, **26**, 64-68.
* Neyman (1959) Neyman, J. (1959). Optimal asymptotic test of composite statistical hypothesis. In: _Grenander, U. (Ed.), Probability and Statistics, the Harald Cramer Volume. Almqvist and Wiksell, Uppsala_, 213-234.
* Pitman (1948) Pitman, E. J. G. (1948). Unpublished lecture notes. Columbia Univ.
* Rao (1948) Rao, C. R. (1948). Large sample tests of statistical hypotheses concerning several parameters with applications to problems of estimation. _Mathematical Proceedings of the Cambridge Philosophical Society_, **44**, 50-57.
* Rao (2001) Rao, C. R. (2001). _Linear Statistical Inference and Its Applications, Second Edition_. Wiley.

Rotnisky, A. and Jewel, N. P. (1990). Hypothesis testing of regression parameters in semiparametric generalized linear models for cluster correlated data. _Biometrika_, **77**, 485-497.
* (21) van Eeden, C. (1963). The relation between Pitman's asymptotic relative efficiency of two tests and the correlation coefficient between their test statistics. _The Annals of Mathematical Statistics_. **34**, 1442-1451.
* (22) Wald, A. (1943). Tests of statistical hypotheses concerning several parameters when the number of observations is large. _Trans. Am. Math. Soc._, **54**, 462-482.
* (23) Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. _The Annals of Mathematical Statistics_, **9**, 60-62.

## Index

**Symbols**

\(L_{2}\) space, 224

\(L_{2}\)-loss function, 173

\(S\) refines \(T\), 40

\(\alpha\)-Neyman, 101

\(\alpha\)-similar test, 100

\(\delta\)-method, 218

\(\sigma\)-algebra, 2

\(\sigma\)-field, 1

\(\sqrt{n}\)-consistent estimate, 275

\(n\)-dimensional Euclidean space, 225

Wilks's likelihood ratio test, 331

**A**

absolutely continuous, 8, 31

action space, 162

admissibility, 164

almost everywhere, 6

almost everywhere convergence, 204

analytic function, 42

asymptotic efficiency, 275

asymptotic normality, 237

asymptotically efficient estimator, 275

asymptotically efficient QF test, 341

asymptotically linear, 278

Augmented LAN, 312

**B**

Bayes risk, 163

Bayes rule, 163

Bayes theorem, 137

Bayesian analysis, 135

Bayesian approach, 135

Bayesian statistical inference, 162

Bayesian sufficiency, 140

Beta prime distribution, 198

bivariate normal distribution, 133

block matrix, 281

Borel-Cantelli Lemma, 204

Bounded Convergence Theorem, 11

bounded in probability, 220

bracket \([\ell,u]\), 244

Brouwer's Fixed Point Theorem, 240

**C**

canonical form, 363

Cauchy sequence, 226

Central Limit Theorem, 215

chain, 32

characteristic function, 211

Chebyshev's inequality, 204

check function, 176

classification, 162

classifier, 162

completeness, 31

conditional density, 22

distribution, 22

expectation, 14

probability, 14

Conditional expectation, 18

conditional inference, 261

conditionally independent, 138

confidence set at level \(1-\alpha\), 127

confidence set, 127

Conjugate families, 144

\(\copyright\) Springer Science+Business Media, LLC, part of Springer Nature 2019

B. Li and G. J. Babu, _A Graduate Course on Statistical Inference_,

Springer Texts in Statistics, [https://doi.org/10.1007/978-1-4939-9761-9](https://doi.org/10.1007/978-1-4939-9761-9)conjugate pair, 7 conjugate prior, 144 consistency, 237 consistent estimate, 240 contiguous, 296 Continuous Mapping Theorem, 209 convergence in distribution, 206, 207 convex function, 7 set, 7 convex hull, 146 Convolution Theorem, 295, 306 Cramer-Rao lower bound, 31 Cramer-Wold device, 211 credible set, 180 critical region, 62 cumulative distribution function, 4

**D**

decision rule, 162 definite integral, 5 degenerate probability, 7 Dirac, 184 Dominated Convergence Theorem, 10 dominated family, 32 Dynkin's \(\pi-\lambda\) theorem, 23

**E**

efficient estimator, 261 Empirical Bayes, 173, 193 Empirical Bayes procedure, 193 equivalence relation, 226 equivalent class, 226 estimating equation, 261 Euclidean norm, 203 Euclidean space, 2 event, 1 expectation, 6 expectation of loss, 162 exponential family, 31

**F**

Fatou's Lemma, 10 Fisher consistency, 53 consistent estimate, 53 information, 31 Fisher information, 239 Fisher scoring algorithm, 276 Fisher's linear discriminant function, 197 Fisher-Neyman factorization theorem, 37 fixed point theorem, 240 frequentist risk, 163 Frobenius norm, 254 Fubini's Theorem, 15, 16 **G**

Gaussian random variable, 69 GEE, 268 generalized Bayes rule, 164 Generalized Estimating Equations, 268 Generalized Linear Models, 261 generalized maximum likelihood estimator, 176 Generalized Method of Moments, 261 Generalized Neyman-Pearson Lemma, 79 geometric median, 176 Glivenko-Cantelli Theorem, 245 Gram matrix, 232

**H**

Holder's inequality, 7 Haar measure, 156 Hadamard product, 171 highest posterior density credible set, 180 Hilbert space, 20, 223, 227 Hodges-Lehmann estimate, 318 homogeneous family, 34 HPD credible set, 180 hypothesis alternative, 62 composite, 64 null, 62 one-sided, 61 simple, 64 statistical, 62 two-sided, 61

**I**

i.i.d., 205 idempotent, 231 identifiable parametric family, 34 improper prior, 154 inadmissible, 164independent identically distributed, 205 inequality

Cauchy-Schwarz, 44

Cramer-Rao, 44

Rao-Blackwell, 50

information bound, 261

information contained in, 263

information identity, 239

inner product matrix, 228

inner product space, 225

insensitive to \(\lambda\) to the first order, 280

integrable, 5

intermediate value theorem, 65

invariant, 114

inverse chi-square distribution, 147

inverse Wishart distribution, 152

irregular estimate, 317

**J**

James-Stein estimator, 192

Jeffreys prior, 161

joint density, 23

joint posterior distribution, 151

**K**

Kolmogorov's SLLN, 206

**L**

Lagrangian, 344

Lagrangian multiplier, 344

Lagrangian multiplier test, 350

Laplace transformation, 43

Le Cam-Hajek convolution theorem, 305

least squares estimate, 262

left Haar measure, 156

left transformation, 157

level of a test, 63

level of significance, 63

likelihood, 136, 238

likelihood equation, 238

likelihood function, 54, 137

likelihood inequality, 34

likelihood ratio, 69

Lindeberg condition, 216

Lindeberg sequence, 216

Lindeberg Theorem, 211

Lindeberg-Levy Theorem, 216

linear discriminant analysis, 189

linear manifold, 230

linear operation, 230

linear regression model, 176

linear space, 263

linear subspace, 230

Lipschitz with dominating slope, 58

Local Asymptotic Normality, 295

location transformation group, 157

location-scale transformation group, 157

Loewner ordering, 229

log likelihood, 238

longitudinal data analysis, 261

loss function, 162

lower semi-continuous function, 209

Lyapounov Theorem, 216

**M**

Mann-Wald notation, 220

marginal density, 23

marginal distribution, 136

marginal posterior distribution, 151

matrix

positive definite, 44

positive semidefinite, 44

Maximum Likelihood Estimate, 237

maximum likelihood estimator, 53

measurable

function, 3

mapping, 3

partition, 4

set, 2

space, 2

statement, 8

measurable rectangle, 15

measure, 2, 184

\(\sigma\)-finite, 2

counting, 3

Lebesgue, 2

probability, 2

measure space, 2

median, 174

method of moment, 53, 262

minimal sufficient statistic, 40

Minkowski's inequality, 7

mixture, 146

MLE, 54

model, 34moment generating function, 94 Monotone Convergence Theorem, 9 monotone likelihood ratio (MLR), 70 Most Powerful (MP) test, 64 MP, 64 multivariate Gamma function, 152 multivariate Normal likelihood, 152 mutually contiguous, 296

**N**

Newton-Raphson algorithm, 91, 275 Newton-Raphson estimate, 275 Neyman structure, 101 Neyman's \(C(\alpha)\) test, 336 Neyman-Pearson Lemma, 61 NICH family, 148 NIW family, 153 noninformative prior, 160 nonrandomized test, 62 nonregular family, 125 norm, 226 Normal Inverse Chi-square distribution, 148 Normal Inverse Wishart distribution, 152 normed space, 226 nuisance parameters, 41, 107

**O**

optimal

estimating equation, 263 estimator, 50 tests, 61 optimal estimating equation, 261 optimality, 237 ordering

Louwner's, 44 positive definite, 44 positive semidefinite, 44 orthogonal projection, 231 orthogonal vectors, 230 outcome, 1

**P**

parallelogram law, 230 parametric family, 34 parametric family of probability measures, 74 Pitman efficiency, 342 Portmanteau theorem, 208 posterior density, 137 posterior distribution, 136 posterior expected loss, 163 posterior geometric median, 176 posterior mean squared error, 179 posterior median, 174 power function, 63 power of the test, 62 pre-Hilbert space,, 224 prior density, 137 prior distribution, 136 probability, 13 density, 2 probability space, 2 product measure, 15 projected score method, 263 projection, 230 projection operator, 231 Pythagaras theorem, 230

**Q**

QF test, 330 quadratic discriminant analysis, 189 Quadratic Form test, 330 quasi likelihood estimate, 262, 267 quasi likelihood method, 261 quasi score function, 267 quasilikelihood function, 364 quotient space, 226

**R**

Radon-Nikodym

derivative, 14

Theorem, 13 random element, 4 Rao's score test, 336 real analytic function, 42 regular estimate, 306 regular test, 338 rejection region, 62 relative compactness, 212 right Haar measure, 156 right transformation, 157 risk, 162

**S**

scalar parameter, 61 scale transformation group, 157score equation, 238 score function, 45, 238 self-adjoint linear operator, 231 significance level, 63 size of the test, 62 Skorohod Theorem, 208 Slutsky's theorem, 211 square integrable function, 44 stacked marginal posterior medians, 176 standard normal random variable, 91 statement holds modulo \(\mu\), 8 statistic ancillary, 37 bounded complete, 37 sufficient, 37 statistical decision theory, 162 Stein's estimator, 173 Stein's paradox, 192 stochastic smallness, 212 strictly unbiased test, 130 strong law of large numbers, 205 strongly consistent estimate, 240 sufficiency, 31 minimal, 31 sufficient dimension reduction, 190 superefficient estimate, 317 **T** tightness, 212 Tonelli's Theorem, 15 translation group, 115 type I error, 62 type II error, 62 **U** UMAU confidence sets, 129 UMP \(\alpha\)-similar, 100 UMP test, 67 UMP-\(\alpha\) test, 68 UMPU test, 75 UMPU-\(\alpha\), 77 UMPU-\(\alpha\) test, 100 UMVUE, 50 unbiased confidence set, 129 unbiased estimating equation, 262 unbiased estimator, 31 unbiasedness of the score, 239 Uniformly Minimum Variance Unbiased Estimator, 50 Uniformly Most Accurate, 129 Uniformly Most Powerful test, 67 uniformly most powerful unbiased, 41 Uniformly Most Powerful Unbiased test, 77 upper semi-continuous function, 209 **V** variance, 6 vector space, 223 vectorization operator, 170 **W** Wald test, 335 weak convergence, 207 Weak Law of Large Numbers, 203 weakly consistent estimate, 240 Wilks's test, 331 Wishart distribution, 152